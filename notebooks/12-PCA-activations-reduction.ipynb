{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from scipy.stats import sem\n",
    "import matplotlib.cm as cm\n",
    "import pathlib\n",
    "import traceback\n",
    "import gc\n",
    "import configs\n",
    "\n",
    "from data.ValidationDataset import NoisyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arg\n",
    "conv_idx = 1\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "netname = 'pnet'\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "activations_dir = f'{engram_dir}3_activations/{netname}/'\n",
    "pca_activations_dir = f'{engram_dir}4_activations_pca/{netname}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_types = ['pinkNoise', 'AudScene', 'Babble8Spkr']\n",
    "snr_types = [-9.0, -6.0, -3.0, 0.0, 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sample_size=None, transform=None):\n",
    "    X = []\n",
    "    bgs = []\n",
    "    snrs = []\n",
    "    dset_idxs = []\n",
    "    n_total_data = 0\n",
    "    n_sampled_data = 0\n",
    "    for bg in bg_types:\n",
    "        for snr in snr_types:\n",
    "            activ_dir = f'{activations_dir}{bg}_snr{int(snr)}/'\n",
    "            for results_file in os.listdir(activ_dir):\n",
    "                results_filepath = f'{activ_dir}{results_file}'\n",
    "                results = h5py.File(results_filepath, 'r')\n",
    "            if conv_idx > 3:\n",
    "                activ = np.array(results[f'conv{conv_idx}_W_{t}_activations'])\n",
    "            else:\n",
    "                activ = np.array(results[f'conv{conv_idx}_{t}_activations'])\n",
    "            n_data = activ.shape[0]\n",
    "            n_total_data += n_data\n",
    "            if sample_size != None:\n",
    "                sample_idxs = np.random.choice(n_data, size=sample_size)\n",
    "                activ = activ[sample_idxs]\n",
    "                n_sampled_data += sample_size\n",
    "                _dset_idxs = list(sample_idxs)\n",
    "            else:\n",
    "                n_sampled_data += n_data\n",
    "                _dset_idxs = list(range(n_data))\n",
    "            new_n_data = activ.shape[0]\n",
    "            activ = list(activ.reshape((new_n_data, -1)))\n",
    "            X.extend(activ)\n",
    "            snrs.extend([snr]*new_n_data)\n",
    "            bgs.extend([bg]*new_n_data)\n",
    "            dset_idxs.extend(_dset_idxs)\n",
    "            \n",
    "            del results\n",
    "            del activ\n",
    "            gc.collect()\n",
    "\n",
    "    idxs = np.arange(len(X))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    X = np.array(X)[idxs]\n",
    "    bgs = np.array(bgs)[idxs]\n",
    "    snrs = np.array(snrs)[idxs]\n",
    "    dset_idxs = np.array(dset_idxs)[idxs]\n",
    "    \n",
    "    print(f'Sampled {n_sampled_data}/{n_total_data} data')\n",
    "    print(f'with {X.shape[1]} features')\n",
    "    \n",
    "    return X, bgs, snrs, dset_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_usage():\n",
    "    total_memory, used_memory, free_memory = map(\n",
    "        int, os.popen('free -t --giga').readlines()[-1].split()[1:])\n",
    "\n",
    "    # Memory usage\n",
    "    p_used = round((used_memory/total_memory) * 100, 2)\n",
    "    print(f\"RAM {used_memory} GB, {p_used}% used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine n_components via random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data\n",
      "Sampled 1500/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "317\n",
      "RAM memory % used: 10.56\n",
      "Retrieving data\n",
      "Sampled 1500/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "311\n",
      "RAM memory % used: 10.56\n",
      "Retrieving data\n",
      "Sampled 1500/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "310\n",
      "RAM memory % used: 10.56\n",
      "Retrieving data\n",
      "Sampled 1500/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "315\n",
      "RAM memory % used: 10.56\n",
      "Retrieving data\n",
      "Sampled 1500/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "315\n",
      "RAM memory % used: 10.57\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print('Retrieving data')\n",
    "    X, _, _, _ = get_data(sample_size=sample_size)\n",
    "    print('Fitting Model:')\n",
    "    pca = PCA(n_components=0.8)\n",
    "    pca.fit(X)\n",
    "    print(pca.n_components_)\n",
    "    get_cpu_usage()\n",
    "    del X\n",
    "    del pca\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract n_components from small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3750/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "0.7493509\n",
      "RAM 19 GB, 9.95% used\n"
     ]
    }
   ],
   "source": [
    "X, bgs, snrs, dset_idxs = get_data(sample_size=250)\n",
    "print('Fitting Model:')\n",
    "pca = PCA(n_components=315)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "get_cpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract n_components from all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 8279/8279 data\n",
      "with 707520 features\n",
      "Fitting Model:\n",
      "1334\n",
      "0.9000441\n",
      "RAM 69 GB, 27.27% used\n"
     ]
    }
   ],
   "source": [
    "X, bgs, snrs, dset_idxs = get_data()\n",
    "print('Fitting Model:')\n",
    "pca = PCA(n_components=0.9)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(pca.n_components_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "get_cpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save components and PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pca_activations_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_filename = f'PCAmodel_conv{conv_idx}_t{t}'\n",
    "with open(f'{pca_activations_dir}{pca_filename}.p', 'wb') as f:\n",
    "    pickle.dump(pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = f'data_conv{conv_idx}_t{t}'\n",
    "with h5py.File(f'{pca_activations_dir}{data_filename}.hdf5', 'x') as f_out:\n",
    "    data_dict = {}\n",
    "    data_dict['X_pca'] = f_out.create_dataset('X_pca', data=X_pca)\n",
    "    bgs_ascii = [n.encode(\"ascii\", \"ignore\") for n in bgs]\n",
    "    data_dict['bgs'] = f_out.create_dataset('bgs', data=bgs_ascii, dtype='S10')\n",
    "    data_dict['snrs'] = f_out.create_dataset('snrs', data=snrs)\n",
    "    data_dict['dset_idxs'] = f_out.create_dataset('dset_idxs', data=dset_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hcnn]",
   "language": "python",
   "name": "conda-env-.conda-hcnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
