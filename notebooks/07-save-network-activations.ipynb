{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96d4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from models.networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe5e29",
   "metadata": {},
   "source": [
    "# PNet parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a3c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'pnet'\n",
    "chckpt = 50\n",
    "n_timesteps = 5\n",
    "layers = ['conv1', 'conv2', 'conv3', 'conv4_W', 'conv5_W', 'fc6_W']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f32ec5",
   "metadata": {},
   "source": [
    "# Paths to relevant directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7152498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "activations_dir = f'{engram_dir}activations_pnet/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96689593",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tf_dir = f'{tensorboard_dir}randomInit_lr_0.01x/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f2cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3174fa",
   "metadata": {},
   "source": [
    "# Helper functions to load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ace6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams(tf_filepath, bg, snr, shared=False):\n",
    "    if shared:\n",
    "        raise ValueError('Not implemented for shared hyperparameters.')\n",
    "    \n",
    "    hyperparams = []\n",
    "    ea = event_accumulator.EventAccumulator(tf_filepath)\n",
    "    ea.Reload()\n",
    "    for i in range(1, 6): \n",
    "        hps = {}\n",
    "        ffm = ea.Scalars(f'Hyperparam/pcoder{i}_feedforward')[-1].value\n",
    "        fbm = ea.Scalars(f'Hyperparam/pcoder{i}_feedback')[-1].value\n",
    "        erm = ea.Scalars(f'Hyperparam/pcoder{i}_error')[-1].value\n",
    "        if np.isnan(ffm) or np.isnan(fbm) or np.isnan(erm):\n",
    "            return None\n",
    "        hps['ffm'] = ffm\n",
    "        hps['fbm'] = fbm\n",
    "        hps['erm'] = erm\n",
    "        hyperparams.append(hps)\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487e4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(PNetClass, pnet_name, chckpt, hyperparams=None):\n",
    "    net = BranchedNetwork(track_encoder_representations=True)\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = PNetClass(net, build_graph=False)\n",
    "    pnet.load_state_dict(torch.load(\n",
    "        f\"{checkpoints_dir}{pnet_name}/{pnet_name}-{chckpt}-regular.pth\",\n",
    "        map_location='cpu'\n",
    "        ))\n",
    "    if hyperparams is not None:\n",
    "        pnet.set_hyperparameters(hyperparams)\n",
    "    pnet.to(DEVICE)\n",
    "    pnet.eval();\n",
    "    print(f'Loaded Pnet: {pnet_name}')\n",
    "    print_hps(pnet)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda98679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hps(pnet):\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        print (f\"PCoder{pc+1} : ffm: {getattr(pnet,f'ffm{pc+1}'):0.3f} \\t fbm: {getattr(pnet,f'fbm{pc+1}'):0.3f} \\t erm: {getattr(pnet,f'erm{pc+1}'):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1718c",
   "metadata": {},
   "source": [
    "# Helper functions to save activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c486a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units_per_layer = {\n",
    "    'conv1': (96, 55, 134), 'conv2': (256, 14, 34),\n",
    "    'conv3': (512, 7, 17), 'conv4_W': (1024, 7, 17),\n",
    "    'conv5_W': (512, 7, 17), 'fc6_W': (4096,)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58884191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pnet(pnet, _input):\n",
    "    pnet.reset()\n",
    "    reconstructions = []\n",
    "    activations = []\n",
    "    logits = []\n",
    "    output = []\n",
    "    for t in range(n_timesteps):\n",
    "        _input_t = _input if t == 0 else None\n",
    "        logits_t, _ = pnet(_input_t)\n",
    "        reconstructions.append(pnet.pcoder1.prd[0,0].cpu().numpy())\n",
    "        activations.append(pnet.backbone.encoder_repr)\n",
    "        logits.append(logits_t.cpu().numpy().squeeze())\n",
    "        output.append(logits_t.max(-1)[1].item())\n",
    "    return reconstructions, activations, logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b123dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_activations(pnet, dset, hdf5_path):\n",
    "    \n",
    "    with h5py.File(hdf5_path, 'x') as f_out:\n",
    "        data_dict = {}\n",
    "        data_dict['label'] = f_out.create_dataset(\n",
    "            'label', dset.n_data, dtype='float32'\n",
    "            )\n",
    "        data_dict['clean_correct'] = f_out.create_dataset(\n",
    "            'clean_correct', dset.n_data, dtype='float32'\n",
    "            )\n",
    "        data_dict['pnet_correct'] = f_out.create_dataset(\n",
    "            'pnet_correct', dset.n_data, dtype='float32'\n",
    "            )\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            reconstr_dim = (dset.n_data, 164, 400)\n",
    "            activ_dim = (dset.n_data,) + n_units_per_layer[layer]\n",
    "            logit_dim = (dset.n_data, 531)\n",
    "            for timestep in range(n_timesteps):\n",
    "                data_dict[f'{layer}_{timestep}_activations'] = f_out.create_dataset(\n",
    "                    f'{layer}_{timestep}_activations', activ_dim, dtype='float32'\n",
    "                    )\n",
    "                data_dict[f'{layer}_{timestep}_clean_activations'] = f_out.create_dataset(\n",
    "                    f'{layer}_{timestep}_clean_activations', activ_dim, dtype='float32'\n",
    "                    )\n",
    "                if layer_idx == 0:\n",
    "                    data_dict[f'{timestep}_reconstructions'] = f_out.create_dataset(\n",
    "                        f'{timestep}_reconstructions', reconstr_dim, dtype='float32'\n",
    "                        )\n",
    "                    data_dict[f'{timestep}_logits'] = f_out.create_dataset(\n",
    "                        f'{timestep}_logits', logit_dim, dtype='float32'\n",
    "                        )\n",
    "                    data_dict[f'{timestep}_output'] = f_out.create_dataset(\n",
    "                        f'{timestep}_output', dset.n_data, dtype='float32'\n",
    "                        )\n",
    "                    data_dict[f'{timestep}_clean_logits'] = f_out.create_dataset(\n",
    "                        f'{timestep}_clean_logits', logit_dim, dtype='float32'\n",
    "                        )\n",
    "                    data_dict[f'{timestep}_clean_output'] = f_out.create_dataset(\n",
    "                        f'{timestep}_clean_output', dset.n_data, dtype='float32'\n",
    "                        )\n",
    "    \n",
    "        for idx in range(10): #dset.n_data):\n",
    "            # Noisy input\n",
    "            noisy_in, label = dset[idx]\n",
    "            data_dict['label'][idx] = label\n",
    "            noisy_in = noisy_in.to(DEVICE)\n",
    "            reconstructions, activations, logits, output = run_pnet(pnet, noisy_in)\n",
    "            data_dict['pnet_correct'][idx] = label == output[-1]\n",
    "            for timestep in range(n_timesteps):\n",
    "                for layer in layers:\n",
    "                    data_dict[f'{layer}_{timestep}_activations'][idx] = \\\n",
    "                        activations[timestep][layer]\n",
    "                data_dict[f'{timestep}_reconstructions'][idx] = \\\n",
    "                    reconstructions[timestep]\n",
    "                data_dict[f'{timestep}_logits'][idx] = \\\n",
    "                    logits[timestep]\n",
    "                data_dict[f'{timestep}_output'][idx] = output[timestep]\n",
    "\n",
    "            # Clean input\n",
    "            clean_in = torch.tensor(\n",
    "                dset.clean_in[idx].reshape((1, 1, 164, 400))\n",
    "                ).to(DEVICE)\n",
    "            reconstructions, activations, logits, output = run_pnet(pnet, clean_in)\n",
    "            data_dict['clean_correct'][idx] = label == output[0]\n",
    "            for timestep in range(n_timesteps):\n",
    "                for layer in layers:\n",
    "                    data_dict[f'{layer}_{timestep}_clean_activations'][idx] = \\\n",
    "                        activations[timestep][layer]\n",
    "                data_dict[f'{timestep}_clean_logits'][idx] = \\\n",
    "                    logits[timestep]\n",
    "                data_dict[f'{timestep}_clean_output'][idx] = output[timestep]\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b16d6",
   "metadata": {},
   "source": [
    "# Run activation-saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc0a5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs = ['pinkNoise'] #, 'AudScene', 'Babble8Spkr']\n",
    "snrs = [-9.0] #, -6.0, -3.0, 0.0, 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c206980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Pnet: pnet\n",
      "PCoder1 : ffm: 0.508 \t fbm: 0.013 \t erm: -0.004\n",
      "PCoder2 : ffm: 0.553 \t fbm: 0.011 \t erm: 0.015\n",
      "PCoder3 : ffm: 0.022 \t fbm: 0.135 \t erm: -0.012\n",
      "PCoder4 : ffm: 0.017 \t fbm: 0.629 \t erm: 0.021\n",
      "PCoder5 : ffm: 0.274 \t fbm: 0.000 \t erm: -0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([164, 400])) that is different to the input size (torch.Size([1, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-17-083dfe8a75d9>\u001b[0m(51)\u001b[0;36msave_activations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     49 \u001b[0;31m            \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pnet_correct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 51 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m                    \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{layer}_{timestep}_activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(output[-1])\n",
      "229\n",
      "ipdb> print(label==output)\n",
      "False\n",
      "ipdb> print(len(output))\n",
      "5\n",
      "ipdb> c\n",
      "> \u001b[0;32m<ipython-input-17-083dfe8a75d9>\u001b[0m(68)\u001b[0;36msave_activations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     66 \u001b[0;31m            \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_correct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     67 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 68 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     69 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     70 \u001b[0;31m                    \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{layer}_{timestep}_clean_activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(len(output))\n",
      "5\n",
      "ipdb> print(label==output[0])\n",
      "tensor(False)\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a29a6c444a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mhdf5_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{activ_dir}{tf_file}.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0msave_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-083dfe8a75d9>\u001b[0m in \u001b[0;36msave_activations\u001b[0;34m(pnet, dset, hdf5_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_correct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{layer}_{timestep}_clean_activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-083dfe8a75d9>\u001b[0m in \u001b[0;36msave_activations\u001b[0;34m(pnet, dset, hdf5_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_correct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{layer}_{timestep}_clean_activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for bg in bgs:\n",
    "    for snr in snrs:\n",
    "        tf_dir = f'{main_tf_dir}hyper_{bg}_snr{snr}/'\n",
    "        activ_dir = f'{activations_dir}{bg}_snr{int(snr)}/'\n",
    "        for tf_file in os.listdir(tf_dir):\n",
    "            tf_filepath = f'{tf_dir}{tf_file}'\n",
    "            tf_file = tf_file.split('edu.')[-1]\n",
    "            hyperparams = get_hyperparams(tf_filepath, bg, snr)\n",
    "            if hyperparams is None:\n",
    "                continue\n",
    "            pnet = load_pnet(PNetClass, pnet_name, chckpt, hyperparams)\n",
    "            dset = NoisyDataset(bg, snr)\n",
    "            hdf5_path = f'{activ_dir}{tf_file}.hdf5'\n",
    "            save_activations(pnet, dset, hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc5059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
