{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96d4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from scipy.stats import pearsonr\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from models.networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe5e29",
   "metadata": {},
   "source": [
    "# PNet parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a3c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'pnet'\n",
    "chckpt = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0593f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 5\n",
    "layers = ['conv1', 'conv2', 'conv3', 'conv4_W', 'conv5_W', 'fc6_W']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f32ec5",
   "metadata": {},
   "source": [
    "# Paths to relevant directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7152498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "activations_dir = f'{engram_dir}activations_pnet_all/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f2cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3174fa",
   "metadata": {},
   "source": [
    "# Helper functions to load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7296d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams(tf_dir, bg, snr, shared=False):\n",
    "    if shared:\n",
    "        raise ValueError('Not implemented for shared hyperparameters.')\n",
    "        \n",
    "    hyperparams = []\n",
    "    tf_file_dir = f'{tf_dir}hyper_{bg}_snr{snr}/'\n",
    "    for tf_file in os.listdir(tf_file_dir):\n",
    "        tf_file = f'{tf_file_dir}{tf_file}'\n",
    "        ea = event_accumulator.EventAccumulator(tf_file)\n",
    "        ea.Reload()\n",
    "        for i in range(1, 6):\n",
    "            hps = {}\n",
    "            ffm = ea.Scalars(f'Hyperparam/pcoder{i}_feedforward')[-1].value\n",
    "            fbm = ea.Scalars(f'Hyperparam/pcoder{i}_feedback')[-1].value\n",
    "            erm = ea.Scalars(f'Hyperparam/pcoder{i}_error')[-1].value\n",
    "            hps['ffm'] = ffm\n",
    "            hps['fbm'] = fbm\n",
    "            hps['erm'] = erm\n",
    "            hyperparams.append(hps)\n",
    "        break\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "487e4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(PNetClass, pnet_name, chckpt, hyperparams=None):\n",
    "    net = BranchedNetwork(track_encoder_representations=True)\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = PNetClass(net, build_graph=False)\n",
    "    pnet.load_state_dict(torch.load(\n",
    "        f\"{checkpoints_dir}{pnet_name}/{pnet_name}-{chckpt}-regular.pth\",\n",
    "        map_location='cpu'\n",
    "        ))\n",
    "    if hyperparams is not None:\n",
    "        pnet.set_hyperparameters(hyperparams)\n",
    "    pnet.to(DEVICE)\n",
    "    pnet.eval();\n",
    "    print(f'Loaded Pnet: {pnet_name}')\n",
    "    print_hps(pnet)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda98679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hps(pnet):\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        print (f\"PCoder{pc+1} : ffm: {getattr(pnet,f'ffm{pc+1}'):0.3f} \\t fbm: {getattr(pnet,f'fbm{pc+1}'):0.3f} \\t erm: {getattr(pnet,f'erm{pc+1}'):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1718c",
   "metadata": {},
   "source": [
    "# Helper functions to save activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c486a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units_per_layer = {\n",
    "    'conv1': (96, 55, 134), 'conv2': (256, 14, 34),\n",
    "    'conv3': (512, 7, 17), 'conv4_W': (1024, 7, 17),\n",
    "    'conv5_W': (512, 7, 17), 'fc6_W': (4096,)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58884191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pnet(pnet, _input):\n",
    "    pnet.reset()\n",
    "    reconstructions = []\n",
    "    activations = []\n",
    "    logits = []\n",
    "    output = []\n",
    "    for t in range(n_timesteps):\n",
    "        _input_t = _input if t == 0 else None\n",
    "        logits_t, _ = pnet(_input_t)\n",
    "        x = pnet.pcoder1.prd\n",
    "        reconstructions.append(pnet.pcoder1.prd[0,0].cpu().numpy())\n",
    "        activations.append(pnet.backbone.encoder_repr)\n",
    "        logits.append(logits_t.cpu().numpy().squeeze())\n",
    "        output.append(logits_t.max(-1)[1].item())\n",
    "    return reconstructions, activations, logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b123dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_activations(pnet, data, hdf5_path):\n",
    "    n_data = data.shape[0]\n",
    "    \n",
    "    with h5py.File(hdf5_path, 'x') as f_out:\n",
    "        data_dict = {}\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            activ_dim = (n_data,) + n_units_per_layer[layer]\n",
    "            for timestep in range(n_timesteps):\n",
    "                data_dict[f'{layer}_{timestep}_activations'] = f_out.create_dataset(\n",
    "                    f'{layer}_{timestep}_activations', activ_dim, dtype='float32'\n",
    "                    )\n",
    "        for idx in range(data.shape[0]):\n",
    "            # Noisy input\n",
    "            noisy_in = data[idx]\n",
    "            reconstructions, activations, logits, output = run_pnet(pnet, noisy_in)\n",
    "            for timestep in range(n_timesteps):\n",
    "                for layer in layers:\n",
    "                    data_dict[f'{layer}_{timestep}_activations'][idx] = \\\n",
    "                        activations[timestep][layer]\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b16d6",
   "metadata": {},
   "source": [
    "# Run activation-saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc0a5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['fg', 'bg_july22']\n",
    "snrs = [-9.0, -6.0, -3.0, 0.0, 3.0]\n",
    "tf_dir = f'{tensorboard_dir}lr_0.01x/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c206980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Pnet: pnet\n",
      "PCoder1 : ffm: 0.300 \t fbm: 0.300 \t erm: 0.010\n",
      "PCoder2 : ffm: 0.300 \t fbm: 0.300 \t erm: 0.010\n",
      "PCoder3 : ffm: 0.300 \t fbm: 0.300 \t erm: 0.010\n",
      "PCoder4 : ffm: 0.300 \t fbm: 0.300 \t erm: 0.010\n",
      "PCoder5 : ffm: 0.300 \t fbm: 0.300 \t erm: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([164, 400])) that is different to the input size (torch.Size([1, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    }
   ],
   "source": [
    "hdf5_path = f'{activations_dir}natural_sounds.hdf5'\n",
    "#hyperparams = get_hyperparams(tf_dir, bg, snr)\n",
    "pnet = load_pnet(PNetClass, pnet_name, chckpt, hyperparams=None)\n",
    "loaded_f = h5py.File(f'{engram_dir}165_natural_sounds_not_resampled.hdf5', 'r')\n",
    "data = np.array(loaded_f['data']).reshape((-1, 164, 400))\n",
    "data = torch.tensor(data).to(DEVICE)\n",
    "save_activations(pnet, data, hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc5059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
