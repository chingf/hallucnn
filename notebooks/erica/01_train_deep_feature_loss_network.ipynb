{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a417066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "sys.path.append(\"/mnt/smb/locker/abbott-locker/es3773/from_home/hallucnn/src/data\")\n",
    "sys.path.append('/mnt/smb/locker/abbott-locker/es3773/from_home/hallucnn_orig/src/models/')\n",
    "from networks_2022 import BranchedNetwork\n",
    "from MergedNoisyDataset import MergedNoisyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ebbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_name = 'pnet_deep_feature_loss_merged_noisy_dataset_v3_with_eval_loss'\n",
    "_train_datafile = 'clean_reconstruction_training_set'\n",
    "SoundsDataset = MergedNoisyDataset\n",
    "dset_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3a87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/es3773/hallucnn/src/models\")\n",
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9622e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 30\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "lr = 1E-5\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}{_train_datafile}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05bd34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/smb/locker/abbott-locker/es3773/from_home/hallucnn_orig/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/mnt/smb/locker/abbott-locker/es3773/from_home/hallucnn_orig/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87ff5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pnet = PNetClass(net, build_graph=True)\n",
    "\n",
    "pnet.eval()\n",
    "\n",
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71afd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MergedNoisyDataset(subset=0.9, train=True)\n",
    "        \n",
    "test_dataset = MergedNoisyDataset(subset=0.9, train=False)\n",
    "train_dataset.n_data\n",
    "test_dataset.n_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94518e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb416d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fe697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d84594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import torch\n",
    "\n",
    "def train_pcoders(net: torch.nn.Module, optimizer: torch.optim.Optimizer, loss_function: Callable, epoch: int, train_loader: torch.utils.data.DataLoader, device: str, writer: torch.utils.tensorboard.SummaryWriter=None):\n",
    "    r\"\"\"\n",
    "    Trains the feedback modules of PCoders using a distance between the prediction of a PCoder and the\n",
    "    representation of the PCoder below.\n",
    "    Args:\n",
    "        net (torch.nn.Module): Predified network including all the PCoders\n",
    "        optimizer (torch.optim.Optimizer): PyTorch-compatible optimizer object\n",
    "        loss_function (Callable): A callable function that receives two tensors and returns the distance between them\n",
    "        epoch (int): Training epoch number\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for training samples\n",
    "        writer (torch.utils.tensorboard.SummaryWrite, optional): Tensorboard summary writer to track training history. Default: None\n",
    "        device (str): Training device (e.g. 'cpu', 'cuda:0')\n",
    "    \"\"\"\n",
    "    layers = ['conv1', 'conv2', 'conv3', 'conv4_W', 'conv5_W', 'fc6_W']\n",
    "    net.train()\n",
    "    net.backbone.eval()\n",
    "    activations_dir = '/mnt/smb/locker/abbott-locker/hcnn/3_activations/FF/'\n",
    "    hdf5_outpath = f'{activations_dir}clean_hyperparameter_matched_training_set_activations.hdf5'\n",
    "    with h5py.File(hdf5_outpath, 'r') as f_in: \n",
    "        nb_trained_samples = 0\n",
    "        for batch_index, (images, _) in enumerate(train_loader):\n",
    "            batch_size = np.shape(images)[0]\n",
    "            net.reset()\n",
    "            images = images.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            for i in range(net.number_of_pcoders):\n",
    "                if i == 0:\n",
    "                    a = loss_function(net.pcoder1.prd, images)\n",
    "                    loss = a\n",
    "                else:\n",
    "                    pcoder_pre =f_in[layers[i-1]+'_activations'][batch_index*batch_size: (batch_index+1)*batch_size] #getattr(net, f\"pcoder{i}\")\n",
    "                    pcoder_pre = torch.tensor(pcoder_pre).to(device)\n",
    "                    \n",
    "                    pcoder_curr =  getattr(net, f\"pcoder{i+1}\")\n",
    "                    a = loss_function(pcoder_curr.prd, pcoder_pre)\n",
    "                    loss += a\n",
    "                if writer is not None:\n",
    "                    writer.add_scalar(f\"MSE Train/PCoder{i+1}\", a.item(), (epoch-1) * len(train_loader) + batch_index)\n",
    "\n",
    "            nb_trained_samples += images.shape[0]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}'.format(\n",
    "                loss.item(),\n",
    "                epoch=epoch,\n",
    "                trained_samples=nb_trained_samples,\n",
    "                total_samples=len(train_loader.dataset)\n",
    "            ))\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(f\"MSE Train/Sum\", loss.item(), (epoch-1) * len(train_loader) + batch_index)\n",
    "\n",
    "\n",
    "def eval_pcoders(offset: int, net: torch.nn.Module, loss_function: Callable, epoch: int, eval_loader: torch.utils.data.DataLoader, device: str, writer: torch.utils.tensorboard.SummaryWriter=None):\n",
    "    \"\"\"\n",
    "    Evaluates the feedback modules of PCoders using a distance between the prediction of a PCoder and the\n",
    "    representation of the PCoder below.\n",
    "    Args:\n",
    "        net (torch.nn.Module): Predified network including all the PCoders\n",
    "        loss_function (Callable): A callable function that receives two tensors and returns the distance between them\n",
    "        epoch (int): Evaluation epoch number\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for evaluation samples\n",
    "        writer (torch.utils.tensorboard.SummaryWrite, optional): Tensorboard summary writer to track evaluation history. Default: None\n",
    "        device (str): Training device (e.g. 'cpu', 'cuda:0')\n",
    "    \"\"\"\n",
    "    layers = ['conv1', 'conv2', 'conv3', 'conv4_W', 'conv5_W', 'fc6_W']\n",
    "    activations_dir = '/mnt/smb/locker/abbott-locker/hcnn/3_activations/FF/'\n",
    "    hdf5_outpath = f'{activations_dir}clean_hyperparameter_matched_training_set_activations.hdf5'\n",
    "    with h5py.File(hdf5_outpath, 'r') as f_in: \n",
    "        net.eval()\n",
    "        \n",
    "        \n",
    "        ## NEED TO ADD OFFSET TO ACCOUNT FOR \n",
    "\n",
    "        final_loss = [0 for i in range(net.number_of_pcoders)]\n",
    "        for batch_index, (images, _) in enumerate(eval_loader):\n",
    "            batch_size = np.shape(images)[0]\n",
    "            net.reset()\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = net(images)\n",
    "            for i in range(net.number_of_pcoders):\n",
    "                print(i)\n",
    "                if i == 0:\n",
    "                    final_loss[i] += loss_function(net.pcoder1.prd, images).item()\n",
    "                else:\n",
    "                    \n",
    "                    start = ((batch_index*batch_size) + offset) \n",
    "                    end = (((batch_index+1)*batch_size) + offset) \n",
    "                    \n",
    "                    pcoder_pre =f_in[layers[i-1]+'_activations'][start:end] #getattr(net, f\"pcoder{i}\")\n",
    "                 \n",
    "                    pcoder_curr = getattr(net, f\"pcoder{i+1}\")\n",
    "                    final_loss[i] += loss_function(pcoder_curr.prd, pcoder_pre).item()\n",
    "\n",
    "        loss_sum = 0\n",
    "        for i in range(net.number_of_pcoders):\n",
    "            final_loss[i] /= len(eval_loader)\n",
    "            loss_sum += final_loss[i]\n",
    "            print(final_loss)\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(f\"MSE Eval/PCoder{i+1}\", final_loss[i], epoch-1)\n",
    "\n",
    "\n",
    "        print('Training Epoch: {epoch} [{evaluated_samples}/{total_samples}]\\tLoss: {:0.4f}'.format(\n",
    "            loss_sum,\n",
    "            epoch=epoch,\n",
    "            evaluated_samples=len(eval_loader.dataset),\n",
    "            total_samples=len(eval_loader.dataset)\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f\"MSE Eval/Sum\", loss_sum, epoch-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315a99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e3fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4c542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efb6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d44a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [30/67482]\tLoss: 556.5466\n",
      "Training Epoch: 1 [60/67482]\tLoss: 554.0670\n",
      "Training Epoch: 1 [90/67482]\tLoss: 550.2391\n",
      "Training Epoch: 1 [120/67482]\tLoss: 544.7906\n",
      "Training Epoch: 1 [150/67482]\tLoss: 535.3878\n",
      "Training Epoch: 1 [180/67482]\tLoss: 523.2587\n",
      "Training Epoch: 1 [210/67482]\tLoss: 518.9567\n",
      "Training Epoch: 1 [240/67482]\tLoss: 500.9839\n",
      "Training Epoch: 1 [270/67482]\tLoss: 506.0135\n",
      "Training Epoch: 1 [300/67482]\tLoss: 503.4045\n",
      "Training Epoch: 1 [330/67482]\tLoss: 482.3760\n",
      "Training Epoch: 1 [360/67482]\tLoss: 489.2063\n",
      "Training Epoch: 1 [390/67482]\tLoss: 478.5609\n",
      "Training Epoch: 1 [420/67482]\tLoss: 498.2364\n",
      "Training Epoch: 1 [450/67482]\tLoss: 472.9398\n",
      "Training Epoch: 1 [480/67482]\tLoss: 467.1927\n",
      "Training Epoch: 1 [510/67482]\tLoss: 457.7351\n",
      "Training Epoch: 1 [540/67482]\tLoss: 444.4552\n",
      "Training Epoch: 1 [570/67482]\tLoss: 449.5471\n",
      "Training Epoch: 1 [600/67482]\tLoss: 454.9693\n",
      "Training Epoch: 1 [630/67482]\tLoss: 444.1888\n",
      "Training Epoch: 1 [660/67482]\tLoss: 451.5621\n",
      "Training Epoch: 1 [690/67482]\tLoss: 427.7655\n",
      "Training Epoch: 1 [720/67482]\tLoss: 423.3318\n",
      "Training Epoch: 1 [750/67482]\tLoss: 400.6292\n",
      "Training Epoch: 1 [780/67482]\tLoss: 415.6029\n",
      "Training Epoch: 1 [810/67482]\tLoss: 414.7844\n",
      "Training Epoch: 1 [840/67482]\tLoss: 401.2933\n",
      "Training Epoch: 1 [870/67482]\tLoss: 376.9296\n",
      "Training Epoch: 1 [900/67482]\tLoss: 367.8061\n",
      "Training Epoch: 1 [930/67482]\tLoss: 394.5459\n",
      "Training Epoch: 1 [960/67482]\tLoss: 374.8412\n",
      "Training Epoch: 1 [990/67482]\tLoss: 371.7715\n",
      "Training Epoch: 1 [1020/67482]\tLoss: 362.8797\n",
      "Training Epoch: 1 [1050/67482]\tLoss: 346.0389\n",
      "Training Epoch: 1 [1080/67482]\tLoss: 349.8931\n",
      "Training Epoch: 1 [1110/67482]\tLoss: 354.0511\n",
      "Training Epoch: 1 [1140/67482]\tLoss: 338.7374\n",
      "Training Epoch: 1 [1170/67482]\tLoss: 334.2561\n",
      "Training Epoch: 1 [1200/67482]\tLoss: 341.6867\n",
      "Training Epoch: 1 [1230/67482]\tLoss: 327.1161\n",
      "Training Epoch: 1 [1260/67482]\tLoss: 350.3215\n",
      "Training Epoch: 1 [1290/67482]\tLoss: 304.0120\n",
      "Training Epoch: 1 [1320/67482]\tLoss: 312.5671\n",
      "Training Epoch: 1 [1350/67482]\tLoss: 322.3161\n",
      "Training Epoch: 1 [1380/67482]\tLoss: 295.8218\n",
      "Training Epoch: 1 [1410/67482]\tLoss: 309.4943\n",
      "Training Epoch: 1 [1440/67482]\tLoss: 302.0883\n",
      "Training Epoch: 1 [1470/67482]\tLoss: 296.5617\n",
      "Training Epoch: 1 [1500/67482]\tLoss: 280.0077\n",
      "Training Epoch: 1 [1530/67482]\tLoss: 286.4787\n",
      "Training Epoch: 1 [1560/67482]\tLoss: 264.3105\n",
      "Training Epoch: 1 [1590/67482]\tLoss: 265.1812\n",
      "Training Epoch: 1 [1620/67482]\tLoss: 273.9911\n",
      "Training Epoch: 1 [1650/67482]\tLoss: 237.4700\n",
      "Training Epoch: 1 [1680/67482]\tLoss: 261.9530\n",
      "Training Epoch: 1 [1710/67482]\tLoss: 248.1299\n",
      "Training Epoch: 1 [1740/67482]\tLoss: 252.9569\n",
      "Training Epoch: 1 [1770/67482]\tLoss: 228.6540\n",
      "Training Epoch: 1 [1800/67482]\tLoss: 234.5031\n",
      "Training Epoch: 1 [1830/67482]\tLoss: 234.7053\n",
      "Training Epoch: 1 [1860/67482]\tLoss: 227.2051\n",
      "Training Epoch: 1 [1890/67482]\tLoss: 204.1052\n",
      "Training Epoch: 1 [1920/67482]\tLoss: 237.7785\n",
      "Training Epoch: 1 [1950/67482]\tLoss: 239.3052\n",
      "Training Epoch: 1 [1980/67482]\tLoss: 193.3175\n",
      "Training Epoch: 1 [2010/67482]\tLoss: 203.0500\n",
      "Training Epoch: 1 [2040/67482]\tLoss: 186.4543\n",
      "Training Epoch: 1 [2070/67482]\tLoss: 216.9568\n",
      "Training Epoch: 1 [2100/67482]\tLoss: 212.4767\n",
      "Training Epoch: 1 [2130/67482]\tLoss: 208.6383\n",
      "Training Epoch: 1 [2160/67482]\tLoss: 189.3862\n",
      "Training Epoch: 1 [2190/67482]\tLoss: 178.9156\n",
      "Training Epoch: 1 [2220/67482]\tLoss: 185.3911\n",
      "Training Epoch: 1 [2250/67482]\tLoss: 190.5261\n",
      "Training Epoch: 1 [2280/67482]\tLoss: 181.5268\n",
      "Training Epoch: 1 [2310/67482]\tLoss: 195.3677\n",
      "Training Epoch: 1 [2340/67482]\tLoss: 163.1276\n",
      "Training Epoch: 1 [2370/67482]\tLoss: 173.9955\n",
      "Training Epoch: 1 [2400/67482]\tLoss: 189.1192\n",
      "Training Epoch: 1 [2430/67482]\tLoss: 171.1840\n",
      "Training Epoch: 1 [2460/67482]\tLoss: 190.1846\n",
      "Training Epoch: 1 [2490/67482]\tLoss: 166.1609\n",
      "Training Epoch: 1 [2520/67482]\tLoss: 171.2204\n",
      "Training Epoch: 1 [2550/67482]\tLoss: 168.3348\n",
      "Training Epoch: 1 [2580/67482]\tLoss: 170.3360\n",
      "Training Epoch: 1 [2610/67482]\tLoss: 161.5835\n",
      "Training Epoch: 1 [2640/67482]\tLoss: 172.1590\n",
      "Training Epoch: 1 [2670/67482]\tLoss: 170.8621\n",
      "Training Epoch: 1 [2700/67482]\tLoss: 156.7623\n",
      "Training Epoch: 1 [2730/67482]\tLoss: 160.7341\n",
      "Training Epoch: 1 [2760/67482]\tLoss: 158.0741\n",
      "Training Epoch: 1 [2790/67482]\tLoss: 147.2452\n",
      "Training Epoch: 1 [2820/67482]\tLoss: 159.7848\n",
      "Training Epoch: 1 [2850/67482]\tLoss: 158.9348\n",
      "Training Epoch: 1 [2880/67482]\tLoss: 154.0458\n",
      "Training Epoch: 1 [2910/67482]\tLoss: 149.5297\n",
      "Training Epoch: 1 [2940/67482]\tLoss: 154.0380\n",
      "Training Epoch: 1 [2970/67482]\tLoss: 155.2758\n",
      "Training Epoch: 1 [3000/67482]\tLoss: 153.9358\n",
      "Training Epoch: 1 [3030/67482]\tLoss: 152.9964\n",
      "Training Epoch: 1 [3060/67482]\tLoss: 158.0035\n",
      "Training Epoch: 1 [3090/67482]\tLoss: 158.6830\n",
      "Training Epoch: 1 [3120/67482]\tLoss: 151.6535\n",
      "Training Epoch: 1 [3150/67482]\tLoss: 154.0775\n",
      "Training Epoch: 1 [3180/67482]\tLoss: 151.9208\n",
      "Training Epoch: 1 [3210/67482]\tLoss: 158.8613\n",
      "Training Epoch: 1 [3240/67482]\tLoss: 151.2788\n",
      "Training Epoch: 1 [3270/67482]\tLoss: 150.5881\n",
      "Training Epoch: 1 [3300/67482]\tLoss: 149.9863\n",
      "Training Epoch: 1 [3330/67482]\tLoss: 152.1627\n",
      "Training Epoch: 1 [3360/67482]\tLoss: 155.2827\n",
      "Training Epoch: 1 [3390/67482]\tLoss: 154.0673\n",
      "Training Epoch: 1 [3420/67482]\tLoss: 150.9492\n",
      "Training Epoch: 1 [3450/67482]\tLoss: 144.5038\n",
      "Training Epoch: 1 [3480/67482]\tLoss: 149.9552\n",
      "Training Epoch: 1 [3510/67482]\tLoss: 153.4117\n",
      "Training Epoch: 1 [3540/67482]\tLoss: 149.7624\n",
      "Training Epoch: 1 [3570/67482]\tLoss: 149.7101\n",
      "Training Epoch: 1 [3600/67482]\tLoss: 142.6784\n",
      "Training Epoch: 1 [3630/67482]\tLoss: 151.1487\n",
      "Training Epoch: 1 [3660/67482]\tLoss: 152.2794\n",
      "Training Epoch: 1 [3690/67482]\tLoss: 147.4190\n",
      "Training Epoch: 1 [3720/67482]\tLoss: 153.1598\n",
      "Training Epoch: 1 [3750/67482]\tLoss: 142.8653\n",
      "Training Epoch: 1 [3780/67482]\tLoss: 146.9074\n",
      "Training Epoch: 1 [3810/67482]\tLoss: 142.4540\n",
      "Training Epoch: 1 [3840/67482]\tLoss: 147.4930\n",
      "Training Epoch: 1 [3870/67482]\tLoss: 140.3565\n",
      "Training Epoch: 1 [3900/67482]\tLoss: 149.1336\n",
      "Training Epoch: 1 [3930/67482]\tLoss: 148.9512\n",
      "Training Epoch: 1 [3960/67482]\tLoss: 152.3014\n",
      "Training Epoch: 1 [3990/67482]\tLoss: 149.9646\n",
      "Training Epoch: 1 [4020/67482]\tLoss: 145.4377\n",
      "Training Epoch: 1 [4050/67482]\tLoss: 145.5455\n",
      "Training Epoch: 1 [4080/67482]\tLoss: 147.6598\n",
      "Training Epoch: 1 [4110/67482]\tLoss: 145.6120\n",
      "Training Epoch: 1 [4140/67482]\tLoss: 139.6046\n",
      "Training Epoch: 1 [4170/67482]\tLoss: 140.2880\n",
      "Training Epoch: 1 [4200/67482]\tLoss: 144.8497\n",
      "Training Epoch: 1 [4230/67482]\tLoss: 152.1037\n",
      "Training Epoch: 1 [4260/67482]\tLoss: 142.7419\n",
      "Training Epoch: 1 [4290/67482]\tLoss: 146.2762\n",
      "Training Epoch: 1 [4320/67482]\tLoss: 141.1704\n",
      "Training Epoch: 1 [4350/67482]\tLoss: 143.0041\n",
      "Training Epoch: 1 [4380/67482]\tLoss: 146.2240\n",
      "Training Epoch: 1 [4410/67482]\tLoss: 139.6797\n",
      "Training Epoch: 1 [4440/67482]\tLoss: 145.1863\n",
      "Training Epoch: 1 [4470/67482]\tLoss: 140.0526\n",
      "Training Epoch: 1 [4500/67482]\tLoss: 141.7727\n",
      "Training Epoch: 1 [4530/67482]\tLoss: 140.5027\n",
      "Training Epoch: 1 [4560/67482]\tLoss: 136.0362\n",
      "Training Epoch: 1 [4590/67482]\tLoss: 142.5228\n",
      "Training Epoch: 1 [4620/67482]\tLoss: 144.2827\n",
      "Training Epoch: 1 [4650/67482]\tLoss: 143.6622\n",
      "Training Epoch: 1 [4680/67482]\tLoss: 143.6975\n",
      "Training Epoch: 1 [4710/67482]\tLoss: 146.3241\n",
      "Training Epoch: 1 [4740/67482]\tLoss: 145.1660\n",
      "Training Epoch: 1 [4770/67482]\tLoss: 144.2902\n",
      "Training Epoch: 1 [4800/67482]\tLoss: 142.2097\n",
      "Training Epoch: 1 [4830/67482]\tLoss: 145.9846\n",
      "Training Epoch: 1 [4860/67482]\tLoss: 143.0291\n",
      "Training Epoch: 1 [4890/67482]\tLoss: 131.6246\n",
      "Training Epoch: 1 [4920/67482]\tLoss: 137.8097\n",
      "Training Epoch: 1 [4950/67482]\tLoss: 136.4711\n",
      "Training Epoch: 1 [4980/67482]\tLoss: 137.8418\n",
      "Training Epoch: 1 [5010/67482]\tLoss: 137.8517\n",
      "Training Epoch: 1 [5040/67482]\tLoss: 135.1565\n",
      "Training Epoch: 1 [5070/67482]\tLoss: 139.1741\n",
      "Training Epoch: 1 [5100/67482]\tLoss: 139.7549\n",
      "Training Epoch: 1 [5130/67482]\tLoss: 144.5921\n",
      "Training Epoch: 1 [5160/67482]\tLoss: 133.0746\n",
      "Training Epoch: 1 [5190/67482]\tLoss: 141.1647\n",
      "Training Epoch: 1 [5220/67482]\tLoss: 138.0609\n",
      "Training Epoch: 1 [5250/67482]\tLoss: 137.1050\n",
      "Training Epoch: 1 [5280/67482]\tLoss: 141.9823\n",
      "Training Epoch: 1 [5310/67482]\tLoss: 130.0777\n",
      "Training Epoch: 1 [5340/67482]\tLoss: 138.8657\n",
      "Training Epoch: 1 [5370/67482]\tLoss: 141.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [5400/67482]\tLoss: 135.0438\n",
      "Training Epoch: 1 [5430/67482]\tLoss: 134.0845\n",
      "Training Epoch: 1 [5460/67482]\tLoss: 135.0941\n",
      "Training Epoch: 1 [5490/67482]\tLoss: 134.4748\n",
      "Training Epoch: 1 [5520/67482]\tLoss: 129.8926\n",
      "Training Epoch: 1 [5550/67482]\tLoss: 136.0229\n",
      "Training Epoch: 1 [5580/67482]\tLoss: 136.4851\n",
      "Training Epoch: 1 [5610/67482]\tLoss: 129.1438\n",
      "Training Epoch: 1 [5640/67482]\tLoss: 136.4847\n",
      "Training Epoch: 1 [5670/67482]\tLoss: 140.2888\n",
      "Training Epoch: 1 [5700/67482]\tLoss: 127.4764\n",
      "Training Epoch: 1 [5730/67482]\tLoss: 140.9365\n",
      "Training Epoch: 1 [5760/67482]\tLoss: 133.6520\n",
      "Training Epoch: 1 [5790/67482]\tLoss: 130.1189\n",
      "Training Epoch: 1 [5820/67482]\tLoss: 129.0533\n",
      "Training Epoch: 1 [5850/67482]\tLoss: 136.3812\n",
      "Training Epoch: 1 [5880/67482]\tLoss: 138.1537\n",
      "Training Epoch: 1 [5910/67482]\tLoss: 139.5058\n",
      "Training Epoch: 1 [5940/67482]\tLoss: 132.3652\n",
      "Training Epoch: 1 [5970/67482]\tLoss: 128.4603\n",
      "Training Epoch: 1 [6000/67482]\tLoss: 136.4644\n",
      "Training Epoch: 1 [6030/67482]\tLoss: 133.5090\n",
      "Training Epoch: 1 [6060/67482]\tLoss: 134.3649\n",
      "Training Epoch: 1 [6090/67482]\tLoss: 136.9390\n",
      "Training Epoch: 1 [6120/67482]\tLoss: 132.6805\n",
      "Training Epoch: 1 [6150/67482]\tLoss: 124.9575\n",
      "Training Epoch: 1 [6180/67482]\tLoss: 131.2378\n",
      "Training Epoch: 1 [6210/67482]\tLoss: 133.9919\n",
      "Training Epoch: 1 [6240/67482]\tLoss: 134.4417\n",
      "Training Epoch: 1 [6270/67482]\tLoss: 129.4088\n",
      "Training Epoch: 1 [6300/67482]\tLoss: 131.5041\n",
      "Training Epoch: 1 [6330/67482]\tLoss: 133.6211\n",
      "Training Epoch: 1 [6360/67482]\tLoss: 132.0335\n",
      "Training Epoch: 1 [6390/67482]\tLoss: 134.0757\n",
      "Training Epoch: 1 [6420/67482]\tLoss: 132.8560\n",
      "Training Epoch: 1 [6450/67482]\tLoss: 130.8846\n",
      "Training Epoch: 1 [6480/67482]\tLoss: 132.5866\n",
      "Training Epoch: 1 [6510/67482]\tLoss: 133.9626\n",
      "Training Epoch: 1 [6540/67482]\tLoss: 133.6983\n",
      "Training Epoch: 1 [6570/67482]\tLoss: 133.1271\n",
      "Training Epoch: 1 [6600/67482]\tLoss: 133.4269\n",
      "Training Epoch: 1 [6630/67482]\tLoss: 129.2682\n",
      "Training Epoch: 1 [6660/67482]\tLoss: 128.7629\n",
      "Training Epoch: 1 [6690/67482]\tLoss: 128.8921\n",
      "Training Epoch: 1 [6720/67482]\tLoss: 129.3842\n",
      "Training Epoch: 1 [6750/67482]\tLoss: 132.7928\n",
      "Training Epoch: 1 [6780/67482]\tLoss: 128.8478\n",
      "Training Epoch: 1 [6810/67482]\tLoss: 126.2552\n",
      "Training Epoch: 1 [6840/67482]\tLoss: 131.3832\n",
      "Training Epoch: 1 [6870/67482]\tLoss: 133.6915\n",
      "Training Epoch: 1 [6900/67482]\tLoss: 128.4129\n",
      "Training Epoch: 1 [6930/67482]\tLoss: 127.5181\n",
      "Training Epoch: 1 [6960/67482]\tLoss: 125.6619\n",
      "Training Epoch: 1 [6990/67482]\tLoss: 124.6124\n",
      "Training Epoch: 1 [7020/67482]\tLoss: 128.7730\n",
      "Training Epoch: 1 [7050/67482]\tLoss: 137.4249\n",
      "Training Epoch: 1 [7080/67482]\tLoss: 127.1259\n",
      "Training Epoch: 1 [7110/67482]\tLoss: 125.4768\n",
      "Training Epoch: 1 [7140/67482]\tLoss: 125.7842\n",
      "Training Epoch: 1 [7170/67482]\tLoss: 133.9098\n",
      "Training Epoch: 1 [7200/67482]\tLoss: 130.2553\n",
      "Training Epoch: 1 [7230/67482]\tLoss: 131.3830\n",
      "Training Epoch: 1 [7260/67482]\tLoss: 131.1815\n",
      "Training Epoch: 1 [7290/67482]\tLoss: 128.0230\n",
      "Training Epoch: 1 [7320/67482]\tLoss: 122.6467\n",
      "Training Epoch: 1 [7350/67482]\tLoss: 132.4808\n",
      "Training Epoch: 1 [7380/67482]\tLoss: 129.0295\n",
      "Training Epoch: 1 [7410/67482]\tLoss: 121.1295\n",
      "Training Epoch: 1 [7440/67482]\tLoss: 126.9071\n",
      "Training Epoch: 1 [7470/67482]\tLoss: 129.4671\n",
      "Training Epoch: 1 [7500/67482]\tLoss: 126.2630\n",
      "Training Epoch: 1 [7530/67482]\tLoss: 124.2590\n",
      "Training Epoch: 1 [7560/67482]\tLoss: 123.5331\n",
      "Training Epoch: 1 [7590/67482]\tLoss: 127.3062\n",
      "Training Epoch: 1 [7620/67482]\tLoss: 126.1672\n",
      "Training Epoch: 1 [7650/67482]\tLoss: 120.9850\n",
      "Training Epoch: 1 [7680/67482]\tLoss: 125.1547\n",
      "Training Epoch: 1 [7710/67482]\tLoss: 123.6986\n",
      "Training Epoch: 1 [7740/67482]\tLoss: 124.2737\n",
      "Training Epoch: 1 [7770/67482]\tLoss: 120.3154\n",
      "Training Epoch: 1 [7800/67482]\tLoss: 127.2327\n",
      "Training Epoch: 1 [7830/67482]\tLoss: 123.3526\n",
      "Training Epoch: 1 [7860/67482]\tLoss: 129.4458\n",
      "Training Epoch: 1 [7890/67482]\tLoss: 123.9792\n",
      "Training Epoch: 1 [7920/67482]\tLoss: 120.2753\n",
      "Training Epoch: 1 [7950/67482]\tLoss: 127.1230\n",
      "Training Epoch: 1 [7980/67482]\tLoss: 124.2178\n",
      "Training Epoch: 1 [8010/67482]\tLoss: 125.5987\n",
      "Training Epoch: 1 [8040/67482]\tLoss: 121.4681\n",
      "Training Epoch: 1 [8070/67482]\tLoss: 120.2093\n",
      "Training Epoch: 1 [8100/67482]\tLoss: 122.1248\n",
      "Training Epoch: 1 [8130/67482]\tLoss: 121.9665\n",
      "Training Epoch: 1 [8160/67482]\tLoss: 121.9054\n",
      "Training Epoch: 1 [8190/67482]\tLoss: 123.3350\n",
      "Training Epoch: 1 [8220/67482]\tLoss: 122.8572\n",
      "Training Epoch: 1 [8250/67482]\tLoss: 118.6783\n",
      "Training Epoch: 1 [8280/67482]\tLoss: 123.3038\n",
      "Training Epoch: 1 [8310/67482]\tLoss: 119.0506\n",
      "Training Epoch: 1 [8340/67482]\tLoss: 123.7324\n",
      "Training Epoch: 1 [8370/67482]\tLoss: 122.1158\n",
      "Training Epoch: 1 [8400/67482]\tLoss: 124.7257\n",
      "Training Epoch: 1 [8430/67482]\tLoss: 123.6315\n",
      "Training Epoch: 1 [8460/67482]\tLoss: 118.2586\n",
      "Training Epoch: 1 [8490/67482]\tLoss: 127.3099\n",
      "Training Epoch: 1 [8520/67482]\tLoss: 122.3866\n",
      "Training Epoch: 1 [8550/67482]\tLoss: 121.1527\n",
      "Training Epoch: 1 [8580/67482]\tLoss: 117.5531\n",
      "Training Epoch: 1 [8610/67482]\tLoss: 119.1571\n",
      "Training Epoch: 1 [8640/67482]\tLoss: 116.4954\n",
      "Training Epoch: 1 [8670/67482]\tLoss: 117.9102\n",
      "Training Epoch: 1 [8700/67482]\tLoss: 121.2687\n",
      "Training Epoch: 1 [8730/67482]\tLoss: 123.1589\n",
      "Training Epoch: 1 [8760/67482]\tLoss: 116.1129\n",
      "Training Epoch: 1 [8790/67482]\tLoss: 121.7037\n",
      "Training Epoch: 1 [8820/67482]\tLoss: 123.4654\n",
      "Training Epoch: 1 [8850/67482]\tLoss: 119.6219\n",
      "Training Epoch: 1 [8880/67482]\tLoss: 122.8710\n",
      "Training Epoch: 1 [8910/67482]\tLoss: 116.0683\n",
      "Training Epoch: 1 [8940/67482]\tLoss: 113.0081\n",
      "Training Epoch: 1 [8970/67482]\tLoss: 121.3655\n",
      "Training Epoch: 1 [9000/67482]\tLoss: 123.9625\n",
      "Training Epoch: 1 [9030/67482]\tLoss: 118.1763\n",
      "Training Epoch: 1 [9060/67482]\tLoss: 119.1793\n",
      "Training Epoch: 1 [9090/67482]\tLoss: 113.7359\n",
      "Training Epoch: 1 [9120/67482]\tLoss: 120.1274\n",
      "Training Epoch: 1 [9150/67482]\tLoss: 116.5288\n",
      "Training Epoch: 1 [9180/67482]\tLoss: 123.1638\n",
      "Training Epoch: 1 [9210/67482]\tLoss: 116.8148\n",
      "Training Epoch: 1 [9240/67482]\tLoss: 119.9038\n",
      "Training Epoch: 1 [9270/67482]\tLoss: 120.2053\n",
      "Training Epoch: 1 [9300/67482]\tLoss: 117.5785\n",
      "Training Epoch: 1 [9330/67482]\tLoss: 119.5322\n",
      "Training Epoch: 1 [9360/67482]\tLoss: 118.7564\n",
      "Training Epoch: 1 [9390/67482]\tLoss: 122.0442\n",
      "Training Epoch: 1 [9420/67482]\tLoss: 115.6665\n",
      "Training Epoch: 1 [9450/67482]\tLoss: 112.6928\n",
      "Training Epoch: 1 [9480/67482]\tLoss: 121.9619\n",
      "Training Epoch: 1 [9510/67482]\tLoss: 112.5426\n",
      "Training Epoch: 1 [9540/67482]\tLoss: 118.1803\n",
      "Training Epoch: 1 [9570/67482]\tLoss: 121.5329\n",
      "Training Epoch: 1 [9600/67482]\tLoss: 117.5280\n",
      "Training Epoch: 1 [9630/67482]\tLoss: 115.7915\n",
      "Training Epoch: 1 [9660/67482]\tLoss: 116.3439\n",
      "Training Epoch: 1 [9690/67482]\tLoss: 114.5574\n",
      "Training Epoch: 1 [9720/67482]\tLoss: 116.8027\n",
      "Training Epoch: 1 [9750/67482]\tLoss: 110.9785\n",
      "Training Epoch: 1 [9780/67482]\tLoss: 119.0971\n",
      "Training Epoch: 1 [9810/67482]\tLoss: 120.0128\n",
      "Training Epoch: 1 [9840/67482]\tLoss: 112.9421\n",
      "Training Epoch: 1 [9870/67482]\tLoss: 113.1142\n",
      "Training Epoch: 1 [9900/67482]\tLoss: 110.6311\n",
      "Training Epoch: 1 [9930/67482]\tLoss: 118.2170\n",
      "Training Epoch: 1 [9960/67482]\tLoss: 116.7397\n",
      "Training Epoch: 1 [9990/67482]\tLoss: 116.1928\n",
      "Training Epoch: 1 [10020/67482]\tLoss: 114.6699\n",
      "Training Epoch: 1 [10050/67482]\tLoss: 120.0952\n",
      "Training Epoch: 1 [10080/67482]\tLoss: 108.3926\n",
      "Training Epoch: 1 [10110/67482]\tLoss: 118.6480\n",
      "Training Epoch: 1 [10140/67482]\tLoss: 116.7246\n",
      "Training Epoch: 1 [10170/67482]\tLoss: 111.9078\n",
      "Training Epoch: 1 [10200/67482]\tLoss: 113.6698\n",
      "Training Epoch: 1 [10230/67482]\tLoss: 113.8059\n",
      "Training Epoch: 1 [10260/67482]\tLoss: 111.7337\n",
      "Training Epoch: 1 [10290/67482]\tLoss: 109.6832\n",
      "Training Epoch: 1 [10320/67482]\tLoss: 110.0408\n",
      "Training Epoch: 1 [10350/67482]\tLoss: 116.5075\n",
      "Training Epoch: 1 [10380/67482]\tLoss: 117.3612\n",
      "Training Epoch: 1 [10410/67482]\tLoss: 112.7336\n",
      "Training Epoch: 1 [10440/67482]\tLoss: 113.7925\n",
      "Training Epoch: 1 [10470/67482]\tLoss: 110.2880\n",
      "Training Epoch: 1 [10500/67482]\tLoss: 110.0949\n",
      "Training Epoch: 1 [10530/67482]\tLoss: 116.7539\n",
      "Training Epoch: 1 [10560/67482]\tLoss: 115.0554\n",
      "Training Epoch: 1 [10590/67482]\tLoss: 112.4782\n",
      "Training Epoch: 1 [10620/67482]\tLoss: 107.4854\n",
      "Training Epoch: 1 [10650/67482]\tLoss: 111.0094\n",
      "Training Epoch: 1 [10680/67482]\tLoss: 111.5822\n",
      "Training Epoch: 1 [10710/67482]\tLoss: 114.4285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [10740/67482]\tLoss: 114.9808\n",
      "Training Epoch: 1 [10770/67482]\tLoss: 110.3417\n",
      "Training Epoch: 1 [10800/67482]\tLoss: 111.2141\n",
      "Training Epoch: 1 [10830/67482]\tLoss: 110.8032\n",
      "Training Epoch: 1 [10860/67482]\tLoss: 106.8683\n",
      "Training Epoch: 1 [10890/67482]\tLoss: 109.6098\n",
      "Training Epoch: 1 [10920/67482]\tLoss: 112.2244\n",
      "Training Epoch: 1 [10950/67482]\tLoss: 109.5504\n",
      "Training Epoch: 1 [10980/67482]\tLoss: 106.3094\n",
      "Training Epoch: 1 [11010/67482]\tLoss: 104.9230\n",
      "Training Epoch: 1 [11040/67482]\tLoss: 114.9384\n",
      "Training Epoch: 1 [11070/67482]\tLoss: 110.2971\n",
      "Training Epoch: 1 [11100/67482]\tLoss: 108.5905\n",
      "Training Epoch: 1 [11130/67482]\tLoss: 111.5316\n",
      "Training Epoch: 1 [11160/67482]\tLoss: 113.6264\n",
      "Training Epoch: 1 [11190/67482]\tLoss: 109.6748\n",
      "Training Epoch: 1 [11220/67482]\tLoss: 107.7052\n",
      "Training Epoch: 1 [11250/67482]\tLoss: 110.5952\n",
      "Training Epoch: 1 [11280/67482]\tLoss: 112.3052\n",
      "Training Epoch: 1 [11310/67482]\tLoss: 108.6384\n",
      "Training Epoch: 1 [11340/67482]\tLoss: 108.7230\n",
      "Training Epoch: 1 [11370/67482]\tLoss: 105.7840\n",
      "Training Epoch: 1 [11400/67482]\tLoss: 107.4548\n",
      "Training Epoch: 1 [11430/67482]\tLoss: 107.9708\n",
      "Training Epoch: 1 [11460/67482]\tLoss: 104.3571\n",
      "Training Epoch: 1 [11490/67482]\tLoss: 108.7659\n",
      "Training Epoch: 1 [11520/67482]\tLoss: 107.8279\n",
      "Training Epoch: 1 [11550/67482]\tLoss: 109.8298\n",
      "Training Epoch: 1 [11580/67482]\tLoss: 108.2680\n",
      "Training Epoch: 1 [11610/67482]\tLoss: 105.7864\n",
      "Training Epoch: 1 [11640/67482]\tLoss: 103.7192\n",
      "Training Epoch: 1 [11670/67482]\tLoss: 104.7385\n",
      "Training Epoch: 1 [11700/67482]\tLoss: 111.5550\n",
      "Training Epoch: 1 [11730/67482]\tLoss: 103.6074\n",
      "Training Epoch: 1 [11760/67482]\tLoss: 108.0780\n",
      "Training Epoch: 1 [11790/67482]\tLoss: 112.0061\n",
      "Training Epoch: 1 [11820/67482]\tLoss: 110.2778\n",
      "Training Epoch: 1 [11850/67482]\tLoss: 109.5325\n",
      "Training Epoch: 1 [11880/67482]\tLoss: 104.1036\n",
      "Training Epoch: 1 [11910/67482]\tLoss: 112.4996\n",
      "Training Epoch: 1 [11940/67482]\tLoss: 110.6505\n",
      "Training Epoch: 1 [11970/67482]\tLoss: 101.6274\n",
      "Training Epoch: 1 [12000/67482]\tLoss: 101.3946\n",
      "Training Epoch: 1 [12030/67482]\tLoss: 100.5491\n",
      "Training Epoch: 1 [12060/67482]\tLoss: 104.9980\n",
      "Training Epoch: 1 [12090/67482]\tLoss: 105.6356\n",
      "Training Epoch: 1 [12120/67482]\tLoss: 103.7116\n",
      "Training Epoch: 1 [12150/67482]\tLoss: 99.8993\n",
      "Training Epoch: 1 [12180/67482]\tLoss: 105.0424\n",
      "Training Epoch: 1 [12210/67482]\tLoss: 104.3485\n",
      "Training Epoch: 1 [12240/67482]\tLoss: 102.5146\n",
      "Training Epoch: 1 [12270/67482]\tLoss: 112.5571\n",
      "Training Epoch: 1 [12300/67482]\tLoss: 101.2805\n",
      "Training Epoch: 1 [12330/67482]\tLoss: 103.4821\n",
      "Training Epoch: 1 [12360/67482]\tLoss: 104.6113\n",
      "Training Epoch: 1 [12390/67482]\tLoss: 106.8985\n",
      "Training Epoch: 1 [12420/67482]\tLoss: 104.5213\n",
      "Training Epoch: 1 [12450/67482]\tLoss: 104.1540\n",
      "Training Epoch: 1 [12480/67482]\tLoss: 99.7503\n",
      "Training Epoch: 1 [12510/67482]\tLoss: 105.0875\n",
      "Training Epoch: 1 [12540/67482]\tLoss: 103.1789\n",
      "Training Epoch: 1 [12570/67482]\tLoss: 102.4246\n",
      "Training Epoch: 1 [12600/67482]\tLoss: 105.0307\n",
      "Training Epoch: 1 [12630/67482]\tLoss: 103.6465\n",
      "Training Epoch: 1 [12660/67482]\tLoss: 102.8798\n",
      "Training Epoch: 1 [12690/67482]\tLoss: 101.9855\n",
      "Training Epoch: 1 [12720/67482]\tLoss: 101.3939\n",
      "Training Epoch: 1 [12750/67482]\tLoss: 104.3769\n",
      "Training Epoch: 1 [12780/67482]\tLoss: 103.3618\n",
      "Training Epoch: 1 [12810/67482]\tLoss: 102.0267\n",
      "Training Epoch: 1 [12840/67482]\tLoss: 98.8386\n",
      "Training Epoch: 1 [12870/67482]\tLoss: 100.6448\n",
      "Training Epoch: 1 [12900/67482]\tLoss: 104.5465\n",
      "Training Epoch: 1 [12930/67482]\tLoss: 103.1924\n",
      "Training Epoch: 1 [12960/67482]\tLoss: 104.0842\n",
      "Training Epoch: 1 [12990/67482]\tLoss: 104.8961\n",
      "Training Epoch: 1 [13020/67482]\tLoss: 100.4811\n",
      "Training Epoch: 1 [13050/67482]\tLoss: 102.4961\n",
      "Training Epoch: 1 [13080/67482]\tLoss: 101.3823\n",
      "Training Epoch: 1 [13110/67482]\tLoss: 100.4581\n",
      "Training Epoch: 1 [13140/67482]\tLoss: 99.3848\n",
      "Training Epoch: 1 [13170/67482]\tLoss: 105.3364\n",
      "Training Epoch: 1 [13200/67482]\tLoss: 100.4249\n",
      "Training Epoch: 1 [13230/67482]\tLoss: 102.5252\n",
      "Training Epoch: 1 [13260/67482]\tLoss: 101.5159\n",
      "Training Epoch: 1 [13290/67482]\tLoss: 100.4538\n",
      "Training Epoch: 1 [13320/67482]\tLoss: 101.5251\n",
      "Training Epoch: 1 [13350/67482]\tLoss: 104.2114\n",
      "Training Epoch: 1 [13380/67482]\tLoss: 101.1215\n",
      "Training Epoch: 1 [13410/67482]\tLoss: 104.2102\n",
      "Training Epoch: 1 [13440/67482]\tLoss: 100.9516\n",
      "Training Epoch: 1 [13470/67482]\tLoss: 104.8104\n",
      "Training Epoch: 1 [13500/67482]\tLoss: 102.1602\n",
      "Training Epoch: 1 [13530/67482]\tLoss: 95.4331\n",
      "Training Epoch: 1 [13560/67482]\tLoss: 98.3831\n",
      "Training Epoch: 1 [13590/67482]\tLoss: 96.8789\n",
      "Training Epoch: 1 [13620/67482]\tLoss: 98.2079\n",
      "Training Epoch: 1 [13650/67482]\tLoss: 98.7701\n",
      "Training Epoch: 1 [13680/67482]\tLoss: 96.7719\n",
      "Training Epoch: 1 [13710/67482]\tLoss: 96.2120\n",
      "Training Epoch: 1 [13740/67482]\tLoss: 97.6735\n",
      "Training Epoch: 1 [13770/67482]\tLoss: 97.4293\n",
      "Training Epoch: 1 [13800/67482]\tLoss: 95.4471\n",
      "Training Epoch: 1 [13830/67482]\tLoss: 100.9509\n",
      "Training Epoch: 1 [13860/67482]\tLoss: 98.3634\n",
      "Training Epoch: 1 [13890/67482]\tLoss: 98.4125\n",
      "Training Epoch: 1 [13920/67482]\tLoss: 98.6540\n",
      "Training Epoch: 1 [13950/67482]\tLoss: 96.7454\n",
      "Training Epoch: 1 [13980/67482]\tLoss: 97.6871\n",
      "Training Epoch: 1 [14010/67482]\tLoss: 100.1995\n",
      "Training Epoch: 1 [14040/67482]\tLoss: 94.2381\n",
      "Training Epoch: 1 [14070/67482]\tLoss: 96.3409\n",
      "Training Epoch: 1 [14100/67482]\tLoss: 97.0263\n",
      "Training Epoch: 1 [14130/67482]\tLoss: 98.0665\n",
      "Training Epoch: 1 [14160/67482]\tLoss: 97.4160\n",
      "Training Epoch: 1 [14190/67482]\tLoss: 97.4511\n",
      "Training Epoch: 1 [14220/67482]\tLoss: 95.8584\n",
      "Training Epoch: 1 [14250/67482]\tLoss: 97.2663\n",
      "Training Epoch: 1 [14280/67482]\tLoss: 97.3958\n",
      "Training Epoch: 1 [14310/67482]\tLoss: 97.4838\n",
      "Training Epoch: 1 [14340/67482]\tLoss: 94.9744\n",
      "Training Epoch: 1 [14370/67482]\tLoss: 93.2806\n",
      "Training Epoch: 1 [14400/67482]\tLoss: 90.4557\n",
      "Training Epoch: 1 [14430/67482]\tLoss: 99.5772\n",
      "Training Epoch: 1 [14460/67482]\tLoss: 95.9997\n",
      "Training Epoch: 1 [14490/67482]\tLoss: 94.2674\n",
      "Training Epoch: 1 [14520/67482]\tLoss: 96.8512\n",
      "Training Epoch: 1 [14550/67482]\tLoss: 93.7692\n",
      "Training Epoch: 1 [14580/67482]\tLoss: 96.2485\n",
      "Training Epoch: 1 [14610/67482]\tLoss: 98.6474\n",
      "Training Epoch: 1 [14640/67482]\tLoss: 98.9881\n",
      "Training Epoch: 1 [14670/67482]\tLoss: 97.5798\n",
      "Training Epoch: 1 [14700/67482]\tLoss: 95.2673\n",
      "Training Epoch: 1 [14730/67482]\tLoss: 93.1522\n",
      "Training Epoch: 1 [14760/67482]\tLoss: 94.2099\n",
      "Training Epoch: 1 [14790/67482]\tLoss: 98.5959\n",
      "Training Epoch: 1 [14820/67482]\tLoss: 94.5202\n",
      "Training Epoch: 1 [14850/67482]\tLoss: 93.7610\n",
      "Training Epoch: 1 [14880/67482]\tLoss: 91.3144\n",
      "Training Epoch: 1 [14910/67482]\tLoss: 94.6705\n",
      "Training Epoch: 1 [14940/67482]\tLoss: 93.0129\n",
      "Training Epoch: 1 [14970/67482]\tLoss: 97.1554\n",
      "Training Epoch: 1 [15000/67482]\tLoss: 93.4707\n",
      "Training Epoch: 1 [15030/67482]\tLoss: 90.3071\n",
      "Training Epoch: 1 [15060/67482]\tLoss: 93.3080\n",
      "Training Epoch: 1 [15090/67482]\tLoss: 96.0571\n",
      "Training Epoch: 1 [15120/67482]\tLoss: 89.4890\n",
      "Training Epoch: 1 [15150/67482]\tLoss: 90.2065\n",
      "Training Epoch: 1 [15180/67482]\tLoss: 96.3584\n",
      "Training Epoch: 1 [15210/67482]\tLoss: 97.8981\n",
      "Training Epoch: 1 [15240/67482]\tLoss: 90.4409\n",
      "Training Epoch: 1 [15270/67482]\tLoss: 92.6474\n",
      "Training Epoch: 1 [15300/67482]\tLoss: 91.1573\n",
      "Training Epoch: 1 [15330/67482]\tLoss: 93.6796\n",
      "Training Epoch: 1 [15360/67482]\tLoss: 93.0318\n",
      "Training Epoch: 1 [15390/67482]\tLoss: 92.6992\n",
      "Training Epoch: 1 [15420/67482]\tLoss: 96.0678\n",
      "Training Epoch: 1 [15450/67482]\tLoss: 88.6426\n",
      "Training Epoch: 1 [15480/67482]\tLoss: 89.7568\n",
      "Training Epoch: 1 [15510/67482]\tLoss: 89.4916\n",
      "Training Epoch: 1 [15540/67482]\tLoss: 92.0706\n",
      "Training Epoch: 1 [15570/67482]\tLoss: 89.7562\n",
      "Training Epoch: 1 [15600/67482]\tLoss: 93.3653\n",
      "Training Epoch: 1 [15630/67482]\tLoss: 91.1284\n",
      "Training Epoch: 1 [15660/67482]\tLoss: 90.3014\n",
      "Training Epoch: 1 [15690/67482]\tLoss: 90.6377\n",
      "Training Epoch: 1 [15720/67482]\tLoss: 89.2166\n",
      "Training Epoch: 1 [15750/67482]\tLoss: 89.7738\n",
      "Training Epoch: 1 [15780/67482]\tLoss: 91.0823\n",
      "Training Epoch: 1 [15810/67482]\tLoss: 89.5737\n",
      "Training Epoch: 1 [15840/67482]\tLoss: 90.0577\n",
      "Training Epoch: 1 [15870/67482]\tLoss: 85.4291\n",
      "Training Epoch: 1 [15900/67482]\tLoss: 89.0981\n",
      "Training Epoch: 1 [15930/67482]\tLoss: 91.6219\n",
      "Training Epoch: 1 [15960/67482]\tLoss: 90.9720\n",
      "Training Epoch: 1 [15990/67482]\tLoss: 91.1318\n",
      "Training Epoch: 1 [16020/67482]\tLoss: 91.4204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [16050/67482]\tLoss: 91.8521\n",
      "Training Epoch: 1 [16080/67482]\tLoss: 87.4816\n",
      "Training Epoch: 1 [16110/67482]\tLoss: 90.1961\n",
      "Training Epoch: 1 [16140/67482]\tLoss: 90.3085\n",
      "Training Epoch: 1 [16170/67482]\tLoss: 88.1848\n",
      "Training Epoch: 1 [16200/67482]\tLoss: 91.6718\n",
      "Training Epoch: 1 [16230/67482]\tLoss: 89.2986\n",
      "Training Epoch: 1 [16260/67482]\tLoss: 89.6663\n",
      "Training Epoch: 1 [16290/67482]\tLoss: 91.7166\n",
      "Training Epoch: 1 [16320/67482]\tLoss: 86.0477\n",
      "Training Epoch: 1 [16350/67482]\tLoss: 88.3860\n",
      "Training Epoch: 1 [16380/67482]\tLoss: 89.1201\n",
      "Training Epoch: 1 [16410/67482]\tLoss: 87.1873\n",
      "Training Epoch: 1 [16440/67482]\tLoss: 90.9019\n",
      "Training Epoch: 1 [16470/67482]\tLoss: 87.4648\n",
      "Training Epoch: 1 [16500/67482]\tLoss: 87.9325\n",
      "Training Epoch: 1 [16530/67482]\tLoss: 87.1435\n",
      "Training Epoch: 1 [16560/67482]\tLoss: 87.0647\n",
      "Training Epoch: 1 [16590/67482]\tLoss: 85.5185\n",
      "Training Epoch: 1 [16620/67482]\tLoss: 92.0339\n",
      "Training Epoch: 1 [16650/67482]\tLoss: 94.7238\n",
      "Training Epoch: 1 [16680/67482]\tLoss: 88.5314\n",
      "Training Epoch: 1 [16710/67482]\tLoss: 88.6409\n",
      "Training Epoch: 1 [16740/67482]\tLoss: 85.7824\n",
      "Training Epoch: 1 [16770/67482]\tLoss: 85.5246\n",
      "Training Epoch: 1 [16800/67482]\tLoss: 87.2012\n",
      "Training Epoch: 1 [16830/67482]\tLoss: 90.0993\n",
      "Training Epoch: 1 [16860/67482]\tLoss: 88.9299\n",
      "Training Epoch: 1 [16890/67482]\tLoss: 88.0757\n",
      "Training Epoch: 1 [16920/67482]\tLoss: 85.7175\n",
      "Training Epoch: 1 [16950/67482]\tLoss: 88.9738\n",
      "Training Epoch: 1 [16980/67482]\tLoss: 89.5377\n",
      "Training Epoch: 1 [17010/67482]\tLoss: 88.6581\n",
      "Training Epoch: 1 [17040/67482]\tLoss: 85.3582\n",
      "Training Epoch: 1 [17070/67482]\tLoss: 88.3047\n",
      "Training Epoch: 1 [17100/67482]\tLoss: 84.9631\n",
      "Training Epoch: 1 [17130/67482]\tLoss: 83.5807\n",
      "Training Epoch: 1 [17160/67482]\tLoss: 88.0026\n",
      "Training Epoch: 1 [17190/67482]\tLoss: 88.9415\n",
      "Training Epoch: 1 [17220/67482]\tLoss: 84.8063\n",
      "Training Epoch: 1 [17250/67482]\tLoss: 86.9387\n",
      "Training Epoch: 1 [17280/67482]\tLoss: 85.9525\n",
      "Training Epoch: 1 [17310/67482]\tLoss: 85.0619\n",
      "Training Epoch: 1 [17340/67482]\tLoss: 88.1729\n",
      "Training Epoch: 1 [17370/67482]\tLoss: 84.0894\n",
      "Training Epoch: 1 [17400/67482]\tLoss: 85.7867\n",
      "Training Epoch: 1 [17430/67482]\tLoss: 88.8132\n",
      "Training Epoch: 1 [17460/67482]\tLoss: 83.1941\n",
      "Training Epoch: 1 [17490/67482]\tLoss: 82.2904\n",
      "Training Epoch: 1 [17520/67482]\tLoss: 84.1637\n",
      "Training Epoch: 1 [17550/67482]\tLoss: 86.5886\n",
      "Training Epoch: 1 [17580/67482]\tLoss: 86.9498\n",
      "Training Epoch: 1 [17610/67482]\tLoss: 83.4714\n",
      "Training Epoch: 1 [17640/67482]\tLoss: 85.8818\n",
      "Training Epoch: 1 [17670/67482]\tLoss: 82.9252\n",
      "Training Epoch: 1 [17700/67482]\tLoss: 86.2642\n",
      "Training Epoch: 1 [17730/67482]\tLoss: 83.3121\n",
      "Training Epoch: 1 [17760/67482]\tLoss: 83.0009\n",
      "Training Epoch: 1 [17790/67482]\tLoss: 81.2383\n",
      "Training Epoch: 1 [17820/67482]\tLoss: 84.5763\n",
      "Training Epoch: 1 [17850/67482]\tLoss: 83.6312\n",
      "Training Epoch: 1 [17880/67482]\tLoss: 83.1120\n",
      "Training Epoch: 1 [17910/67482]\tLoss: 85.9096\n",
      "Training Epoch: 1 [17940/67482]\tLoss: 84.3464\n",
      "Training Epoch: 1 [17970/67482]\tLoss: 82.9044\n",
      "Training Epoch: 1 [18000/67482]\tLoss: 84.5285\n",
      "Training Epoch: 1 [18030/67482]\tLoss: 80.3408\n",
      "Training Epoch: 1 [18060/67482]\tLoss: 82.5161\n",
      "Training Epoch: 1 [18090/67482]\tLoss: 78.0389\n",
      "Training Epoch: 1 [18120/67482]\tLoss: 82.5976\n",
      "Training Epoch: 1 [18150/67482]\tLoss: 79.7549\n",
      "Training Epoch: 1 [18180/67482]\tLoss: 84.6674\n",
      "Training Epoch: 1 [18210/67482]\tLoss: 83.5169\n",
      "Training Epoch: 1 [18240/67482]\tLoss: 83.8894\n",
      "Training Epoch: 1 [18270/67482]\tLoss: 82.2472\n",
      "Training Epoch: 1 [18300/67482]\tLoss: 78.9306\n",
      "Training Epoch: 1 [18330/67482]\tLoss: 79.7285\n",
      "Training Epoch: 1 [18360/67482]\tLoss: 83.0555\n",
      "Training Epoch: 1 [18390/67482]\tLoss: 83.0747\n",
      "Training Epoch: 1 [18420/67482]\tLoss: 80.3351\n",
      "Training Epoch: 1 [18450/67482]\tLoss: 83.0054\n",
      "Training Epoch: 1 [18480/67482]\tLoss: 80.8893\n",
      "Training Epoch: 1 [18510/67482]\tLoss: 75.4505\n",
      "Training Epoch: 1 [18540/67482]\tLoss: 76.9153\n",
      "Training Epoch: 1 [18570/67482]\tLoss: 79.5332\n",
      "Training Epoch: 1 [18600/67482]\tLoss: 82.9900\n",
      "Training Epoch: 1 [18630/67482]\tLoss: 78.6134\n",
      "Training Epoch: 1 [18660/67482]\tLoss: 79.4743\n",
      "Training Epoch: 1 [18690/67482]\tLoss: 82.6775\n",
      "Training Epoch: 1 [18720/67482]\tLoss: 76.2863\n",
      "Training Epoch: 1 [18750/67482]\tLoss: 79.2593\n",
      "Training Epoch: 1 [18780/67482]\tLoss: 79.3518\n",
      "Training Epoch: 1 [18810/67482]\tLoss: 79.5279\n",
      "Training Epoch: 1 [18840/67482]\tLoss: 82.4642\n",
      "Training Epoch: 1 [18870/67482]\tLoss: 79.4529\n",
      "Training Epoch: 1 [18900/67482]\tLoss: 82.7465\n",
      "Training Epoch: 1 [18930/67482]\tLoss: 81.3491\n",
      "Training Epoch: 1 [18960/67482]\tLoss: 78.8956\n",
      "Training Epoch: 1 [18990/67482]\tLoss: 80.1282\n",
      "Training Epoch: 1 [19020/67482]\tLoss: 82.1296\n",
      "Training Epoch: 1 [19050/67482]\tLoss: 78.2505\n",
      "Training Epoch: 1 [19080/67482]\tLoss: 76.7247\n",
      "Training Epoch: 1 [19110/67482]\tLoss: 77.3083\n",
      "Training Epoch: 1 [19140/67482]\tLoss: 75.9722\n",
      "Training Epoch: 1 [19170/67482]\tLoss: 78.5536\n",
      "Training Epoch: 1 [19200/67482]\tLoss: 78.9946\n",
      "Training Epoch: 1 [19230/67482]\tLoss: 79.4103\n",
      "Training Epoch: 1 [19260/67482]\tLoss: 78.0309\n",
      "Training Epoch: 1 [19290/67482]\tLoss: 79.9479\n",
      "Training Epoch: 1 [19320/67482]\tLoss: 73.2198\n",
      "Training Epoch: 1 [19350/67482]\tLoss: 73.2908\n",
      "Training Epoch: 1 [19380/67482]\tLoss: 73.7823\n",
      "Training Epoch: 1 [19410/67482]\tLoss: 75.5597\n",
      "Training Epoch: 1 [19440/67482]\tLoss: 75.9175\n",
      "Training Epoch: 1 [19470/67482]\tLoss: 73.3764\n",
      "Training Epoch: 1 [19500/67482]\tLoss: 77.9647\n",
      "Training Epoch: 1 [19530/67482]\tLoss: 80.4512\n",
      "Training Epoch: 1 [19560/67482]\tLoss: 78.3339\n",
      "Training Epoch: 1 [19590/67482]\tLoss: 75.7514\n",
      "Training Epoch: 1 [19620/67482]\tLoss: 78.1807\n",
      "Training Epoch: 1 [19650/67482]\tLoss: 77.4603\n",
      "Training Epoch: 1 [19680/67482]\tLoss: 79.6681\n",
      "Training Epoch: 1 [19710/67482]\tLoss: 77.3925\n",
      "Training Epoch: 1 [19740/67482]\tLoss: 75.1348\n",
      "Training Epoch: 1 [19770/67482]\tLoss: 73.2436\n",
      "Training Epoch: 1 [19800/67482]\tLoss: 76.1574\n",
      "Training Epoch: 1 [19830/67482]\tLoss: 75.0697\n",
      "Training Epoch: 1 [19860/67482]\tLoss: 76.1890\n",
      "Training Epoch: 1 [19890/67482]\tLoss: 75.7694\n",
      "Training Epoch: 1 [19920/67482]\tLoss: 76.0871\n",
      "Training Epoch: 1 [19950/67482]\tLoss: 75.5573\n",
      "Training Epoch: 1 [19980/67482]\tLoss: 75.4608\n",
      "Training Epoch: 1 [20010/67482]\tLoss: 76.6203\n",
      "Training Epoch: 1 [20040/67482]\tLoss: 74.1913\n",
      "Training Epoch: 1 [20070/67482]\tLoss: 74.7809\n",
      "Training Epoch: 1 [20100/67482]\tLoss: 75.5119\n",
      "Training Epoch: 1 [20130/67482]\tLoss: 77.1576\n",
      "Training Epoch: 1 [20160/67482]\tLoss: 77.2780\n",
      "Training Epoch: 1 [20190/67482]\tLoss: 73.6059\n",
      "Training Epoch: 1 [20220/67482]\tLoss: 75.6763\n",
      "Training Epoch: 1 [20250/67482]\tLoss: 75.5689\n",
      "Training Epoch: 1 [20280/67482]\tLoss: 76.0403\n",
      "Training Epoch: 1 [20310/67482]\tLoss: 73.8702\n",
      "Training Epoch: 1 [20340/67482]\tLoss: 75.9836\n",
      "Training Epoch: 1 [20370/67482]\tLoss: 74.7438\n",
      "Training Epoch: 1 [20400/67482]\tLoss: 75.7689\n",
      "Training Epoch: 1 [20430/67482]\tLoss: 76.5458\n",
      "Training Epoch: 1 [20460/67482]\tLoss: 76.2009\n",
      "Training Epoch: 1 [20490/67482]\tLoss: 73.3714\n",
      "Training Epoch: 1 [20520/67482]\tLoss: 74.0550\n",
      "Training Epoch: 1 [20550/67482]\tLoss: 72.5191\n",
      "Training Epoch: 1 [20580/67482]\tLoss: 70.6145\n",
      "Training Epoch: 1 [20610/67482]\tLoss: 73.6320\n",
      "Training Epoch: 1 [20640/67482]\tLoss: 74.9245\n",
      "Training Epoch: 1 [20670/67482]\tLoss: 74.4754\n",
      "Training Epoch: 1 [20700/67482]\tLoss: 69.9772\n",
      "Training Epoch: 1 [20730/67482]\tLoss: 76.5753\n",
      "Training Epoch: 1 [20760/67482]\tLoss: 76.4097\n",
      "Training Epoch: 1 [20790/67482]\tLoss: 71.7383\n",
      "Training Epoch: 1 [20820/67482]\tLoss: 68.1102\n",
      "Training Epoch: 1 [20850/67482]\tLoss: 72.9366\n",
      "Training Epoch: 1 [20880/67482]\tLoss: 74.1586\n",
      "Training Epoch: 1 [20910/67482]\tLoss: 71.7037\n",
      "Training Epoch: 1 [20940/67482]\tLoss: 73.9484\n",
      "Training Epoch: 1 [20970/67482]\tLoss: 74.2074\n",
      "Training Epoch: 1 [21000/67482]\tLoss: 73.7420\n",
      "Training Epoch: 1 [21030/67482]\tLoss: 73.3154\n",
      "Training Epoch: 1 [21060/67482]\tLoss: 73.9289\n",
      "Training Epoch: 1 [21090/67482]\tLoss: 68.8645\n",
      "Training Epoch: 1 [21120/67482]\tLoss: 75.6521\n",
      "Training Epoch: 1 [21150/67482]\tLoss: 72.1156\n",
      "Training Epoch: 1 [21180/67482]\tLoss: 72.1219\n",
      "Training Epoch: 1 [21210/67482]\tLoss: 72.8787\n",
      "Training Epoch: 1 [21240/67482]\tLoss: 73.0078\n",
      "Training Epoch: 1 [21270/67482]\tLoss: 73.2772\n",
      "Training Epoch: 1 [21300/67482]\tLoss: 71.8790\n",
      "Training Epoch: 1 [21330/67482]\tLoss: 69.3924\n",
      "Training Epoch: 1 [21360/67482]\tLoss: 72.8944\n",
      "Training Epoch: 1 [21390/67482]\tLoss: 73.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [21420/67482]\tLoss: 71.1836\n",
      "Training Epoch: 1 [21450/67482]\tLoss: 70.0055\n",
      "Training Epoch: 1 [21480/67482]\tLoss: 71.5196\n",
      "Training Epoch: 1 [21510/67482]\tLoss: 69.9345\n",
      "Training Epoch: 1 [21540/67482]\tLoss: 73.5457\n",
      "Training Epoch: 1 [21570/67482]\tLoss: 70.6588\n",
      "Training Epoch: 1 [21600/67482]\tLoss: 68.6595\n",
      "Training Epoch: 1 [21630/67482]\tLoss: 70.7238\n",
      "Training Epoch: 1 [21660/67482]\tLoss: 69.8450\n",
      "Training Epoch: 1 [21690/67482]\tLoss: 69.1971\n",
      "Training Epoch: 1 [21720/67482]\tLoss: 70.5422\n",
      "Training Epoch: 1 [21750/67482]\tLoss: 69.8813\n",
      "Training Epoch: 1 [21780/67482]\tLoss: 70.3521\n",
      "Training Epoch: 1 [21810/67482]\tLoss: 70.0435\n",
      "Training Epoch: 1 [21840/67482]\tLoss: 71.2357\n",
      "Training Epoch: 1 [21870/67482]\tLoss: 69.9645\n",
      "Training Epoch: 1 [21900/67482]\tLoss: 66.0049\n",
      "Training Epoch: 1 [21930/67482]\tLoss: 69.1803\n",
      "Training Epoch: 1 [21960/67482]\tLoss: 68.9389\n",
      "Training Epoch: 1 [21990/67482]\tLoss: 72.0743\n",
      "Training Epoch: 1 [22020/67482]\tLoss: 69.4739\n",
      "Training Epoch: 1 [22050/67482]\tLoss: 73.2117\n",
      "Training Epoch: 1 [22080/67482]\tLoss: 71.2377\n",
      "Training Epoch: 1 [22110/67482]\tLoss: 70.3522\n",
      "Training Epoch: 1 [22140/67482]\tLoss: 71.4156\n",
      "Training Epoch: 1 [22170/67482]\tLoss: 69.8442\n",
      "Training Epoch: 1 [22200/67482]\tLoss: 69.6872\n",
      "Training Epoch: 1 [22230/67482]\tLoss: 67.6134\n",
      "Training Epoch: 1 [22260/67482]\tLoss: 70.2089\n",
      "Training Epoch: 1 [22290/67482]\tLoss: 68.9787\n",
      "Training Epoch: 1 [22320/67482]\tLoss: 71.6216\n",
      "Training Epoch: 1 [22350/67482]\tLoss: 70.1962\n",
      "Training Epoch: 1 [22380/67482]\tLoss: 69.8840\n",
      "Training Epoch: 1 [22410/67482]\tLoss: 66.5907\n",
      "Training Epoch: 1 [22440/67482]\tLoss: 66.4784\n",
      "Training Epoch: 1 [22470/67482]\tLoss: 65.9496\n",
      "Training Epoch: 1 [22500/67482]\tLoss: 68.0928\n",
      "Training Epoch: 1 [22530/67482]\tLoss: 69.0520\n",
      "Training Epoch: 1 [22560/67482]\tLoss: 68.4556\n",
      "Training Epoch: 1 [22590/67482]\tLoss: 65.9226\n",
      "Training Epoch: 1 [22620/67482]\tLoss: 67.4415\n",
      "Training Epoch: 1 [22650/67482]\tLoss: 66.9056\n",
      "Training Epoch: 1 [22680/67482]\tLoss: 67.9399\n",
      "Training Epoch: 1 [22710/67482]\tLoss: 65.6363\n",
      "Training Epoch: 1 [22740/67482]\tLoss: 66.1676\n",
      "Training Epoch: 1 [22770/67482]\tLoss: 64.4676\n",
      "Training Epoch: 1 [22800/67482]\tLoss: 65.6433\n",
      "Training Epoch: 1 [22830/67482]\tLoss: 68.6023\n",
      "Training Epoch: 1 [22860/67482]\tLoss: 66.3058\n",
      "Training Epoch: 1 [22890/67482]\tLoss: 66.1186\n",
      "Training Epoch: 1 [22920/67482]\tLoss: 65.9866\n",
      "Training Epoch: 1 [22950/67482]\tLoss: 66.9886\n",
      "Training Epoch: 1 [22980/67482]\tLoss: 68.2828\n",
      "Training Epoch: 1 [23010/67482]\tLoss: 66.2090\n",
      "Training Epoch: 1 [23040/67482]\tLoss: 67.4509\n",
      "Training Epoch: 1 [23070/67482]\tLoss: 65.8460\n",
      "Training Epoch: 1 [23100/67482]\tLoss: 68.6663\n",
      "Training Epoch: 1 [23130/67482]\tLoss: 65.1915\n",
      "Training Epoch: 1 [23160/67482]\tLoss: 65.9140\n",
      "Training Epoch: 1 [23190/67482]\tLoss: 64.8695\n",
      "Training Epoch: 1 [23220/67482]\tLoss: 65.4379\n",
      "Training Epoch: 1 [23250/67482]\tLoss: 63.6206\n",
      "Training Epoch: 1 [23280/67482]\tLoss: 65.0656\n",
      "Training Epoch: 1 [23310/67482]\tLoss: 65.0758\n",
      "Training Epoch: 1 [23340/67482]\tLoss: 62.7442\n",
      "Training Epoch: 1 [23370/67482]\tLoss: 62.2827\n",
      "Training Epoch: 1 [23400/67482]\tLoss: 65.0264\n",
      "Training Epoch: 1 [23430/67482]\tLoss: 63.9146\n",
      "Training Epoch: 1 [23460/67482]\tLoss: 62.2018\n",
      "Training Epoch: 1 [23490/67482]\tLoss: 64.5647\n",
      "Training Epoch: 1 [23520/67482]\tLoss: 62.8678\n",
      "Training Epoch: 1 [23550/67482]\tLoss: 63.3985\n",
      "Training Epoch: 1 [23580/67482]\tLoss: 65.9484\n",
      "Training Epoch: 1 [23610/67482]\tLoss: 63.1771\n",
      "Training Epoch: 1 [23640/67482]\tLoss: 65.8101\n",
      "Training Epoch: 1 [23670/67482]\tLoss: 64.8203\n",
      "Training Epoch: 1 [23700/67482]\tLoss: 63.8973\n",
      "Training Epoch: 1 [23730/67482]\tLoss: 63.3483\n",
      "Training Epoch: 1 [23760/67482]\tLoss: 64.1218\n",
      "Training Epoch: 1 [23790/67482]\tLoss: 63.3543\n",
      "Training Epoch: 1 [23820/67482]\tLoss: 66.0201\n",
      "Training Epoch: 1 [23850/67482]\tLoss: 64.0385\n",
      "Training Epoch: 1 [23880/67482]\tLoss: 64.0644\n",
      "Training Epoch: 1 [23910/67482]\tLoss: 64.4219\n",
      "Training Epoch: 1 [23940/67482]\tLoss: 63.8074\n",
      "Training Epoch: 1 [23970/67482]\tLoss: 65.2776\n",
      "Training Epoch: 1 [24000/67482]\tLoss: 62.7331\n",
      "Training Epoch: 1 [24030/67482]\tLoss: 62.1389\n",
      "Training Epoch: 1 [24060/67482]\tLoss: 63.6306\n",
      "Training Epoch: 1 [24090/67482]\tLoss: 65.7841\n",
      "Training Epoch: 1 [24120/67482]\tLoss: 63.0092\n",
      "Training Epoch: 1 [24150/67482]\tLoss: 63.1634\n",
      "Training Epoch: 1 [24180/67482]\tLoss: 64.5863\n",
      "Training Epoch: 1 [24210/67482]\tLoss: 58.8692\n",
      "Training Epoch: 1 [24240/67482]\tLoss: 60.4740\n",
      "Training Epoch: 1 [24270/67482]\tLoss: 60.7208\n",
      "Training Epoch: 1 [24300/67482]\tLoss: 64.4289\n",
      "Training Epoch: 1 [24330/67482]\tLoss: 63.1473\n",
      "Training Epoch: 1 [24360/67482]\tLoss: 63.3037\n",
      "Training Epoch: 1 [24390/67482]\tLoss: 61.2525\n",
      "Training Epoch: 1 [24420/67482]\tLoss: 63.5762\n",
      "Training Epoch: 1 [24450/67482]\tLoss: 61.5627\n",
      "Training Epoch: 1 [24480/67482]\tLoss: 60.9506\n",
      "Training Epoch: 1 [24510/67482]\tLoss: 60.9322\n",
      "Training Epoch: 1 [24540/67482]\tLoss: 62.3978\n",
      "Training Epoch: 1 [24570/67482]\tLoss: 60.6291\n",
      "Training Epoch: 1 [24600/67482]\tLoss: 61.6301\n",
      "Training Epoch: 1 [24630/67482]\tLoss: 63.4117\n",
      "Training Epoch: 1 [24660/67482]\tLoss: 64.0976\n",
      "Training Epoch: 1 [24690/67482]\tLoss: 63.9122\n",
      "Training Epoch: 1 [24720/67482]\tLoss: 61.0265\n",
      "Training Epoch: 1 [24750/67482]\tLoss: 62.1698\n",
      "Training Epoch: 1 [24780/67482]\tLoss: 63.0034\n",
      "Training Epoch: 1 [24810/67482]\tLoss: 63.2830\n",
      "Training Epoch: 1 [24840/67482]\tLoss: 62.4958\n",
      "Training Epoch: 1 [24870/67482]\tLoss: 64.5358\n",
      "Training Epoch: 1 [24900/67482]\tLoss: 63.1262\n",
      "Training Epoch: 1 [24930/67482]\tLoss: 61.4251\n",
      "Training Epoch: 1 [24960/67482]\tLoss: 61.1155\n",
      "Training Epoch: 1 [24990/67482]\tLoss: 61.2873\n",
      "Training Epoch: 1 [25020/67482]\tLoss: 62.7719\n",
      "Training Epoch: 1 [25050/67482]\tLoss: 62.0070\n",
      "Training Epoch: 1 [25080/67482]\tLoss: 62.1017\n",
      "Training Epoch: 1 [25110/67482]\tLoss: 59.9528\n",
      "Training Epoch: 1 [25140/67482]\tLoss: 58.3629\n",
      "Training Epoch: 1 [25170/67482]\tLoss: 63.1005\n",
      "Training Epoch: 1 [25200/67482]\tLoss: 59.2442\n",
      "Training Epoch: 1 [25230/67482]\tLoss: 60.4125\n",
      "Training Epoch: 1 [25260/67482]\tLoss: 60.6525\n",
      "Training Epoch: 1 [25290/67482]\tLoss: 60.8240\n",
      "Training Epoch: 1 [25320/67482]\tLoss: 62.3589\n",
      "Training Epoch: 1 [25350/67482]\tLoss: 60.9686\n",
      "Training Epoch: 1 [25380/67482]\tLoss: 59.2361\n",
      "Training Epoch: 1 [25410/67482]\tLoss: 59.6245\n",
      "Training Epoch: 1 [25440/67482]\tLoss: 58.8036\n",
      "Training Epoch: 1 [25470/67482]\tLoss: 59.3639\n",
      "Training Epoch: 1 [25500/67482]\tLoss: 59.4669\n",
      "Training Epoch: 1 [25530/67482]\tLoss: 61.1107\n",
      "Training Epoch: 1 [25560/67482]\tLoss: 58.9863\n",
      "Training Epoch: 1 [25590/67482]\tLoss: 60.5288\n",
      "Training Epoch: 1 [25620/67482]\tLoss: 63.1410\n",
      "Training Epoch: 1 [25650/67482]\tLoss: 58.5268\n",
      "Training Epoch: 1 [25680/67482]\tLoss: 60.2932\n",
      "Training Epoch: 1 [25710/67482]\tLoss: 57.2489\n",
      "Training Epoch: 1 [25740/67482]\tLoss: 61.2141\n",
      "Training Epoch: 1 [25770/67482]\tLoss: 58.2822\n",
      "Training Epoch: 1 [25800/67482]\tLoss: 57.1148\n",
      "Training Epoch: 1 [25830/67482]\tLoss: 58.7210\n",
      "Training Epoch: 1 [25860/67482]\tLoss: 60.4368\n",
      "Training Epoch: 1 [25890/67482]\tLoss: 60.0409\n",
      "Training Epoch: 1 [25920/67482]\tLoss: 60.9738\n",
      "Training Epoch: 1 [25950/67482]\tLoss: 57.3522\n",
      "Training Epoch: 1 [25980/67482]\tLoss: 58.4990\n",
      "Training Epoch: 1 [26010/67482]\tLoss: 57.9773\n",
      "Training Epoch: 1 [26040/67482]\tLoss: 58.4406\n",
      "Training Epoch: 1 [26070/67482]\tLoss: 59.1670\n",
      "Training Epoch: 1 [26100/67482]\tLoss: 59.2280\n",
      "Training Epoch: 1 [26130/67482]\tLoss: 59.7273\n",
      "Training Epoch: 1 [26160/67482]\tLoss: 60.0212\n",
      "Training Epoch: 1 [26190/67482]\tLoss: 59.0110\n",
      "Training Epoch: 1 [26220/67482]\tLoss: 60.0390\n",
      "Training Epoch: 1 [26250/67482]\tLoss: 57.6138\n",
      "Training Epoch: 1 [26280/67482]\tLoss: 58.3211\n",
      "Training Epoch: 1 [26310/67482]\tLoss: 57.7116\n",
      "Training Epoch: 1 [26340/67482]\tLoss: 58.6437\n",
      "Training Epoch: 1 [26370/67482]\tLoss: 59.9179\n",
      "Training Epoch: 1 [26400/67482]\tLoss: 60.6199\n",
      "Training Epoch: 1 [26430/67482]\tLoss: 58.1153\n",
      "Training Epoch: 1 [26460/67482]\tLoss: 56.6002\n",
      "Training Epoch: 1 [26490/67482]\tLoss: 58.2994\n",
      "Training Epoch: 1 [26520/67482]\tLoss: 57.5187\n",
      "Training Epoch: 1 [26550/67482]\tLoss: 58.9528\n",
      "Training Epoch: 1 [26580/67482]\tLoss: 57.1234\n",
      "Training Epoch: 1 [26610/67482]\tLoss: 56.3672\n",
      "Training Epoch: 1 [26640/67482]\tLoss: 57.1248\n",
      "Training Epoch: 1 [26670/67482]\tLoss: 55.9841\n",
      "Training Epoch: 1 [26700/67482]\tLoss: 56.9233\n",
      "Training Epoch: 1 [26730/67482]\tLoss: 54.7021\n",
      "Training Epoch: 1 [26760/67482]\tLoss: 56.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [26790/67482]\tLoss: 58.2194\n",
      "Training Epoch: 1 [26820/67482]\tLoss: 56.8514\n",
      "Training Epoch: 1 [26850/67482]\tLoss: 56.5717\n",
      "Training Epoch: 1 [26880/67482]\tLoss: 55.4524\n",
      "Training Epoch: 1 [26910/67482]\tLoss: 55.6440\n",
      "Training Epoch: 1 [26940/67482]\tLoss: 55.5832\n",
      "Training Epoch: 1 [26970/67482]\tLoss: 55.9259\n",
      "Training Epoch: 1 [27000/67482]\tLoss: 56.5553\n",
      "Training Epoch: 1 [27030/67482]\tLoss: 58.0639\n",
      "Training Epoch: 1 [27060/67482]\tLoss: 56.0693\n",
      "Training Epoch: 1 [27090/67482]\tLoss: 56.9625\n",
      "Training Epoch: 1 [27120/67482]\tLoss: 57.2258\n",
      "Training Epoch: 1 [27150/67482]\tLoss: 56.1491\n",
      "Training Epoch: 1 [27180/67482]\tLoss: 58.1421\n",
      "Training Epoch: 1 [27210/67482]\tLoss: 56.9554\n",
      "Training Epoch: 1 [27240/67482]\tLoss: 57.5479\n",
      "Training Epoch: 1 [27270/67482]\tLoss: 57.1936\n",
      "Training Epoch: 1 [27300/67482]\tLoss: 56.5594\n",
      "Training Epoch: 1 [27330/67482]\tLoss: 54.3352\n",
      "Training Epoch: 1 [27360/67482]\tLoss: 55.7553\n",
      "Training Epoch: 1 [27390/67482]\tLoss: 57.0289\n",
      "Training Epoch: 1 [27420/67482]\tLoss: 53.6631\n",
      "Training Epoch: 1 [27450/67482]\tLoss: 56.0470\n",
      "Training Epoch: 1 [27480/67482]\tLoss: 56.5241\n",
      "Training Epoch: 1 [27510/67482]\tLoss: 54.6562\n",
      "Training Epoch: 1 [27540/67482]\tLoss: 55.9277\n",
      "Training Epoch: 1 [27570/67482]\tLoss: 56.9297\n",
      "Training Epoch: 1 [27600/67482]\tLoss: 54.9577\n",
      "Training Epoch: 1 [27630/67482]\tLoss: 53.3258\n",
      "Training Epoch: 1 [27660/67482]\tLoss: 54.2815\n",
      "Training Epoch: 1 [27690/67482]\tLoss: 55.0070\n",
      "Training Epoch: 1 [27720/67482]\tLoss: 54.4907\n",
      "Training Epoch: 1 [27750/67482]\tLoss: 54.3558\n",
      "Training Epoch: 1 [27780/67482]\tLoss: 54.7368\n",
      "Training Epoch: 1 [27810/67482]\tLoss: 54.6850\n",
      "Training Epoch: 1 [27840/67482]\tLoss: 54.4169\n",
      "Training Epoch: 1 [27870/67482]\tLoss: 53.8126\n",
      "Training Epoch: 1 [27900/67482]\tLoss: 55.5305\n",
      "Training Epoch: 1 [27930/67482]\tLoss: 53.2925\n",
      "Training Epoch: 1 [27960/67482]\tLoss: 55.4930\n",
      "Training Epoch: 1 [27990/67482]\tLoss: 54.5086\n",
      "Training Epoch: 1 [28020/67482]\tLoss: 55.5969\n",
      "Training Epoch: 1 [28050/67482]\tLoss: 55.3929\n",
      "Training Epoch: 1 [28080/67482]\tLoss: 52.4983\n",
      "Training Epoch: 1 [28110/67482]\tLoss: 54.5037\n",
      "Training Epoch: 1 [28140/67482]\tLoss: 54.7843\n",
      "Training Epoch: 1 [28170/67482]\tLoss: 54.8546\n",
      "Training Epoch: 1 [28200/67482]\tLoss: 54.6218\n",
      "Training Epoch: 1 [28230/67482]\tLoss: 54.1647\n",
      "Training Epoch: 1 [28260/67482]\tLoss: 53.9047\n",
      "Training Epoch: 1 [28290/67482]\tLoss: 53.1565\n",
      "Training Epoch: 1 [28320/67482]\tLoss: 53.5693\n",
      "Training Epoch: 1 [28350/67482]\tLoss: 53.8183\n",
      "Training Epoch: 1 [28380/67482]\tLoss: 54.7479\n",
      "Training Epoch: 1 [28410/67482]\tLoss: 54.9150\n",
      "Training Epoch: 1 [28440/67482]\tLoss: 53.5066\n",
      "Training Epoch: 1 [28470/67482]\tLoss: 56.4293\n",
      "Training Epoch: 1 [28500/67482]\tLoss: 55.0568\n",
      "Training Epoch: 1 [28530/67482]\tLoss: 55.3590\n",
      "Training Epoch: 1 [28560/67482]\tLoss: 56.5927\n",
      "Training Epoch: 1 [28590/67482]\tLoss: 54.0718\n",
      "Training Epoch: 1 [28620/67482]\tLoss: 52.5197\n",
      "Training Epoch: 1 [28650/67482]\tLoss: 55.1372\n",
      "Training Epoch: 1 [28680/67482]\tLoss: 53.0169\n",
      "Training Epoch: 1 [28710/67482]\tLoss: 54.5028\n",
      "Training Epoch: 1 [28740/67482]\tLoss: 52.5209\n",
      "Training Epoch: 1 [28770/67482]\tLoss: 54.1941\n",
      "Training Epoch: 1 [28800/67482]\tLoss: 53.4612\n",
      "Training Epoch: 1 [28830/67482]\tLoss: 53.3581\n",
      "Training Epoch: 1 [28860/67482]\tLoss: 53.6217\n",
      "Training Epoch: 1 [28890/67482]\tLoss: 52.5897\n",
      "Training Epoch: 1 [28920/67482]\tLoss: 53.5340\n",
      "Training Epoch: 1 [28950/67482]\tLoss: 54.4502\n",
      "Training Epoch: 1 [28980/67482]\tLoss: 54.0159\n",
      "Training Epoch: 1 [29010/67482]\tLoss: 52.9138\n",
      "Training Epoch: 1 [29040/67482]\tLoss: 52.5562\n",
      "Training Epoch: 1 [29070/67482]\tLoss: 53.5979\n",
      "Training Epoch: 1 [29100/67482]\tLoss: 53.3456\n",
      "Training Epoch: 1 [29130/67482]\tLoss: 51.8913\n",
      "Training Epoch: 1 [29160/67482]\tLoss: 51.6752\n",
      "Training Epoch: 1 [29190/67482]\tLoss: 53.3309\n",
      "Training Epoch: 1 [29220/67482]\tLoss: 55.2123\n",
      "Training Epoch: 1 [29250/67482]\tLoss: 52.6183\n",
      "Training Epoch: 1 [29280/67482]\tLoss: 54.4041\n",
      "Training Epoch: 1 [29310/67482]\tLoss: 52.5505\n",
      "Training Epoch: 1 [29340/67482]\tLoss: 52.4426\n",
      "Training Epoch: 1 [29370/67482]\tLoss: 53.3746\n",
      "Training Epoch: 1 [29400/67482]\tLoss: 52.9273\n",
      "Training Epoch: 1 [29430/67482]\tLoss: 51.6237\n",
      "Training Epoch: 1 [29460/67482]\tLoss: 51.3827\n",
      "Training Epoch: 1 [29490/67482]\tLoss: 52.9237\n",
      "Training Epoch: 1 [29520/67482]\tLoss: 52.7003\n",
      "Training Epoch: 1 [29550/67482]\tLoss: 52.3663\n",
      "Training Epoch: 1 [29580/67482]\tLoss: 51.1280\n",
      "Training Epoch: 1 [29610/67482]\tLoss: 52.9165\n",
      "Training Epoch: 1 [29640/67482]\tLoss: 53.2240\n",
      "Training Epoch: 1 [29670/67482]\tLoss: 52.4810\n",
      "Training Epoch: 1 [29700/67482]\tLoss: 51.4981\n",
      "Training Epoch: 1 [29730/67482]\tLoss: 53.9537\n",
      "Training Epoch: 1 [29760/67482]\tLoss: 51.2458\n",
      "Training Epoch: 1 [29790/67482]\tLoss: 50.6498\n",
      "Training Epoch: 1 [29820/67482]\tLoss: 52.6695\n",
      "Training Epoch: 1 [29850/67482]\tLoss: 51.6973\n",
      "Training Epoch: 1 [29880/67482]\tLoss: 52.4269\n",
      "Training Epoch: 1 [29910/67482]\tLoss: 53.2948\n",
      "Training Epoch: 1 [29940/67482]\tLoss: 49.9238\n",
      "Training Epoch: 1 [29970/67482]\tLoss: 51.5543\n",
      "Training Epoch: 1 [30000/67482]\tLoss: 50.9856\n",
      "Training Epoch: 1 [30030/67482]\tLoss: 52.2646\n",
      "Training Epoch: 1 [30060/67482]\tLoss: 52.7265\n",
      "Training Epoch: 1 [30090/67482]\tLoss: 52.4933\n",
      "Training Epoch: 1 [30120/67482]\tLoss: 50.6835\n",
      "Training Epoch: 1 [30150/67482]\tLoss: 50.5089\n",
      "Training Epoch: 1 [30180/67482]\tLoss: 51.8758\n",
      "Training Epoch: 1 [30210/67482]\tLoss: 49.5236\n",
      "Training Epoch: 1 [30240/67482]\tLoss: 50.2348\n",
      "Training Epoch: 1 [30270/67482]\tLoss: 51.3667\n",
      "Training Epoch: 1 [30300/67482]\tLoss: 51.4810\n",
      "Training Epoch: 1 [30330/67482]\tLoss: 50.6926\n",
      "Training Epoch: 1 [30360/67482]\tLoss: 51.2955\n",
      "Training Epoch: 1 [30390/67482]\tLoss: 51.8932\n",
      "Training Epoch: 1 [30420/67482]\tLoss: 52.2445\n",
      "Training Epoch: 1 [30450/67482]\tLoss: 51.0497\n",
      "Training Epoch: 1 [30480/67482]\tLoss: 50.8037\n",
      "Training Epoch: 1 [30510/67482]\tLoss: 50.5965\n",
      "Training Epoch: 1 [30540/67482]\tLoss: 51.3430\n",
      "Training Epoch: 1 [30570/67482]\tLoss: 50.8810\n",
      "Training Epoch: 1 [30600/67482]\tLoss: 49.5000\n",
      "Training Epoch: 1 [30630/67482]\tLoss: 50.5892\n",
      "Training Epoch: 1 [30660/67482]\tLoss: 50.4678\n",
      "Training Epoch: 1 [30690/67482]\tLoss: 51.3447\n",
      "Training Epoch: 1 [30720/67482]\tLoss: 49.9331\n",
      "Training Epoch: 1 [30750/67482]\tLoss: 49.9071\n",
      "Training Epoch: 1 [30780/67482]\tLoss: 50.4230\n",
      "Training Epoch: 1 [30810/67482]\tLoss: 49.5351\n",
      "Training Epoch: 1 [30840/67482]\tLoss: 50.2980\n",
      "Training Epoch: 1 [30870/67482]\tLoss: 49.3689\n",
      "Training Epoch: 1 [30900/67482]\tLoss: 50.1709\n",
      "Training Epoch: 1 [30930/67482]\tLoss: 49.8096\n",
      "Training Epoch: 1 [30960/67482]\tLoss: 51.4417\n",
      "Training Epoch: 1 [30990/67482]\tLoss: 50.7200\n",
      "Training Epoch: 1 [31020/67482]\tLoss: 51.1092\n",
      "Training Epoch: 1 [31050/67482]\tLoss: 50.7042\n",
      "Training Epoch: 1 [31080/67482]\tLoss: 50.7590\n",
      "Training Epoch: 1 [31110/67482]\tLoss: 49.1673\n",
      "Training Epoch: 1 [31140/67482]\tLoss: 49.7769\n",
      "Training Epoch: 1 [31170/67482]\tLoss: 50.5816\n",
      "Training Epoch: 1 [31200/67482]\tLoss: 51.0403\n",
      "Training Epoch: 1 [31230/67482]\tLoss: 48.4888\n",
      "Training Epoch: 1 [31260/67482]\tLoss: 49.0420\n",
      "Training Epoch: 1 [31290/67482]\tLoss: 50.1017\n",
      "Training Epoch: 1 [31320/67482]\tLoss: 48.6707\n",
      "Training Epoch: 1 [31350/67482]\tLoss: 49.7091\n",
      "Training Epoch: 1 [31380/67482]\tLoss: 50.4166\n",
      "Training Epoch: 1 [31410/67482]\tLoss: 49.8048\n",
      "Training Epoch: 1 [31440/67482]\tLoss: 49.4143\n",
      "Training Epoch: 1 [31470/67482]\tLoss: 48.1107\n",
      "Training Epoch: 1 [31500/67482]\tLoss: 49.9735\n",
      "Training Epoch: 1 [31530/67482]\tLoss: 49.7692\n",
      "Training Epoch: 1 [31560/67482]\tLoss: 51.4155\n",
      "Training Epoch: 1 [31590/67482]\tLoss: 48.6936\n",
      "Training Epoch: 1 [31620/67482]\tLoss: 48.3299\n",
      "Training Epoch: 1 [31650/67482]\tLoss: 47.1253\n",
      "Training Epoch: 1 [31680/67482]\tLoss: 48.2535\n",
      "Training Epoch: 1 [31710/67482]\tLoss: 49.2522\n",
      "Training Epoch: 1 [31740/67482]\tLoss: 49.2558\n",
      "Training Epoch: 1 [31770/67482]\tLoss: 48.9026\n",
      "Training Epoch: 1 [31800/67482]\tLoss: 50.4031\n",
      "Training Epoch: 1 [31830/67482]\tLoss: 49.6192\n",
      "Training Epoch: 1 [31860/67482]\tLoss: 50.8981\n",
      "Training Epoch: 1 [31890/67482]\tLoss: 48.5657\n",
      "Training Epoch: 1 [31920/67482]\tLoss: 49.6105\n",
      "Training Epoch: 1 [31950/67482]\tLoss: 49.1748\n",
      "Training Epoch: 1 [31980/67482]\tLoss: 50.1474\n",
      "Training Epoch: 1 [32010/67482]\tLoss: 48.5966\n",
      "Training Epoch: 1 [32040/67482]\tLoss: 49.2815\n",
      "Training Epoch: 1 [32070/67482]\tLoss: 48.1759\n",
      "Training Epoch: 1 [32100/67482]\tLoss: 49.2911\n",
      "Training Epoch: 1 [32130/67482]\tLoss: 49.0216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [32160/67482]\tLoss: 48.1547\n",
      "Training Epoch: 1 [32190/67482]\tLoss: 48.9146\n",
      "Training Epoch: 1 [32220/67482]\tLoss: 49.2791\n",
      "Training Epoch: 1 [32250/67482]\tLoss: 48.9784\n",
      "Training Epoch: 1 [32280/67482]\tLoss: 49.1673\n",
      "Training Epoch: 1 [32310/67482]\tLoss: 47.7083\n",
      "Training Epoch: 1 [32340/67482]\tLoss: 50.1835\n",
      "Training Epoch: 1 [32370/67482]\tLoss: 50.4576\n",
      "Training Epoch: 1 [32400/67482]\tLoss: 47.7796\n",
      "Training Epoch: 1 [32430/67482]\tLoss: 49.1076\n",
      "Training Epoch: 1 [32460/67482]\tLoss: 48.2294\n",
      "Training Epoch: 1 [32490/67482]\tLoss: 47.1699\n",
      "Training Epoch: 1 [32520/67482]\tLoss: 48.2209\n",
      "Training Epoch: 1 [32550/67482]\tLoss: 47.6171\n",
      "Training Epoch: 1 [32580/67482]\tLoss: 47.8750\n",
      "Training Epoch: 1 [32610/67482]\tLoss: 47.9419\n",
      "Training Epoch: 1 [32640/67482]\tLoss: 49.0693\n",
      "Training Epoch: 1 [32670/67482]\tLoss: 48.3703\n",
      "Training Epoch: 1 [32700/67482]\tLoss: 48.2216\n",
      "Training Epoch: 1 [32730/67482]\tLoss: 47.8462\n",
      "Training Epoch: 1 [32760/67482]\tLoss: 48.9074\n",
      "Training Epoch: 1 [32790/67482]\tLoss: 49.0578\n",
      "Training Epoch: 1 [32820/67482]\tLoss: 46.8774\n",
      "Training Epoch: 1 [32850/67482]\tLoss: 48.1134\n",
      "Training Epoch: 1 [32880/67482]\tLoss: 47.7298\n",
      "Training Epoch: 1 [32910/67482]\tLoss: 48.4875\n",
      "Training Epoch: 1 [32940/67482]\tLoss: 48.6902\n",
      "Training Epoch: 1 [32970/67482]\tLoss: 47.1376\n",
      "Training Epoch: 1 [33000/67482]\tLoss: 47.6804\n",
      "Training Epoch: 1 [33030/67482]\tLoss: 48.2687\n",
      "Training Epoch: 1 [33060/67482]\tLoss: 47.8521\n",
      "Training Epoch: 1 [33090/67482]\tLoss: 49.0745\n",
      "Training Epoch: 1 [33120/67482]\tLoss: 47.4807\n",
      "Training Epoch: 1 [33150/67482]\tLoss: 47.2600\n",
      "Training Epoch: 1 [33180/67482]\tLoss: 47.4954\n",
      "Training Epoch: 1 [33210/67482]\tLoss: 47.8434\n",
      "Training Epoch: 1 [33240/67482]\tLoss: 48.3705\n",
      "Training Epoch: 1 [33270/67482]\tLoss: 48.5079\n",
      "Training Epoch: 1 [33300/67482]\tLoss: 46.7109\n",
      "Training Epoch: 1 [33330/67482]\tLoss: 48.2331\n",
      "Training Epoch: 1 [33360/67482]\tLoss: 47.7865\n",
      "Training Epoch: 1 [33390/67482]\tLoss: 47.6579\n",
      "Training Epoch: 1 [33420/67482]\tLoss: 47.5677\n",
      "Training Epoch: 1 [33450/67482]\tLoss: 46.7632\n",
      "Training Epoch: 1 [33480/67482]\tLoss: 45.6438\n",
      "Training Epoch: 1 [33510/67482]\tLoss: 47.6239\n",
      "Training Epoch: 1 [33540/67482]\tLoss: 46.7102\n",
      "Training Epoch: 1 [33570/67482]\tLoss: 46.1141\n",
      "Training Epoch: 1 [33600/67482]\tLoss: 47.2439\n",
      "Training Epoch: 1 [33630/67482]\tLoss: 46.3612\n",
      "Training Epoch: 1 [33660/67482]\tLoss: 47.4753\n",
      "Training Epoch: 1 [33690/67482]\tLoss: 46.6940\n",
      "Training Epoch: 1 [33720/67482]\tLoss: 47.5207\n",
      "Training Epoch: 1 [33750/67482]\tLoss: 46.0063\n",
      "Training Epoch: 1 [33780/67482]\tLoss: 46.8143\n",
      "Training Epoch: 1 [33810/67482]\tLoss: 47.1027\n",
      "Training Epoch: 1 [33840/67482]\tLoss: 47.0817\n",
      "Training Epoch: 1 [33870/67482]\tLoss: 47.0703\n",
      "Training Epoch: 1 [33900/67482]\tLoss: 47.3707\n",
      "Training Epoch: 1 [33930/67482]\tLoss: 46.8639\n",
      "Training Epoch: 1 [33960/67482]\tLoss: 47.7988\n",
      "Training Epoch: 1 [33990/67482]\tLoss: 46.5827\n",
      "Training Epoch: 1 [34020/67482]\tLoss: 46.0044\n",
      "Training Epoch: 1 [34050/67482]\tLoss: 46.9401\n",
      "Training Epoch: 1 [34080/67482]\tLoss: 45.4373\n",
      "Training Epoch: 1 [34110/67482]\tLoss: 46.5905\n",
      "Training Epoch: 1 [34140/67482]\tLoss: 46.7426\n",
      "Training Epoch: 1 [34170/67482]\tLoss: 46.5982\n",
      "Training Epoch: 1 [34200/67482]\tLoss: 46.4761\n",
      "Training Epoch: 1 [34230/67482]\tLoss: 47.1041\n",
      "Training Epoch: 1 [34260/67482]\tLoss: 46.6155\n",
      "Training Epoch: 1 [34290/67482]\tLoss: 45.1411\n",
      "Training Epoch: 1 [34320/67482]\tLoss: 46.0957\n",
      "Training Epoch: 1 [34350/67482]\tLoss: 46.9042\n",
      "Training Epoch: 1 [34380/67482]\tLoss: 46.9156\n",
      "Training Epoch: 1 [34410/67482]\tLoss: 45.3983\n",
      "Training Epoch: 1 [34440/67482]\tLoss: 45.3314\n",
      "Training Epoch: 1 [34470/67482]\tLoss: 46.3791\n",
      "Training Epoch: 1 [34500/67482]\tLoss: 46.6569\n",
      "Training Epoch: 1 [34530/67482]\tLoss: 46.3965\n",
      "Training Epoch: 1 [34560/67482]\tLoss: 46.3622\n",
      "Training Epoch: 1 [34590/67482]\tLoss: 45.7976\n",
      "Training Epoch: 1 [34620/67482]\tLoss: 45.3321\n",
      "Training Epoch: 1 [34650/67482]\tLoss: 46.2672\n",
      "Training Epoch: 1 [34680/67482]\tLoss: 46.7779\n",
      "Training Epoch: 1 [34710/67482]\tLoss: 46.1480\n",
      "Training Epoch: 1 [34740/67482]\tLoss: 47.3598\n",
      "Training Epoch: 1 [34770/67482]\tLoss: 45.9535\n",
      "Training Epoch: 1 [34800/67482]\tLoss: 45.5384\n",
      "Training Epoch: 1 [34830/67482]\tLoss: 45.8271\n",
      "Training Epoch: 1 [34860/67482]\tLoss: 46.7856\n",
      "Training Epoch: 1 [34890/67482]\tLoss: 45.4605\n",
      "Training Epoch: 1 [34920/67482]\tLoss: 46.0240\n",
      "Training Epoch: 1 [34950/67482]\tLoss: 46.7799\n",
      "Training Epoch: 1 [34980/67482]\tLoss: 46.0407\n",
      "Training Epoch: 1 [35010/67482]\tLoss: 45.5824\n",
      "Training Epoch: 1 [35040/67482]\tLoss: 45.3674\n",
      "Training Epoch: 1 [35070/67482]\tLoss: 46.1360\n",
      "Training Epoch: 1 [35100/67482]\tLoss: 45.1418\n",
      "Training Epoch: 1 [35130/67482]\tLoss: 45.4297\n",
      "Training Epoch: 1 [35160/67482]\tLoss: 44.4730\n",
      "Training Epoch: 1 [35190/67482]\tLoss: 45.4844\n",
      "Training Epoch: 1 [35220/67482]\tLoss: 46.9025\n",
      "Training Epoch: 1 [35250/67482]\tLoss: 45.3426\n",
      "Training Epoch: 1 [35280/67482]\tLoss: 45.4963\n",
      "Training Epoch: 1 [35310/67482]\tLoss: 45.8946\n",
      "Training Epoch: 1 [35340/67482]\tLoss: 45.5319\n",
      "Training Epoch: 1 [35370/67482]\tLoss: 45.5576\n",
      "Training Epoch: 1 [35400/67482]\tLoss: 45.1337\n",
      "Training Epoch: 1 [35430/67482]\tLoss: 45.3225\n",
      "Training Epoch: 1 [35460/67482]\tLoss: 45.1425\n",
      "Training Epoch: 1 [35490/67482]\tLoss: 44.7504\n",
      "Training Epoch: 1 [35520/67482]\tLoss: 45.5790\n",
      "Training Epoch: 1 [35550/67482]\tLoss: 45.7627\n",
      "Training Epoch: 1 [35580/67482]\tLoss: 44.9630\n",
      "Training Epoch: 1 [35610/67482]\tLoss: 45.7633\n",
      "Training Epoch: 1 [35640/67482]\tLoss: 45.9258\n",
      "Training Epoch: 1 [35670/67482]\tLoss: 45.0103\n",
      "Training Epoch: 1 [35700/67482]\tLoss: 45.4452\n",
      "Training Epoch: 1 [35730/67482]\tLoss: 45.3636\n",
      "Training Epoch: 1 [35760/67482]\tLoss: 45.4197\n",
      "Training Epoch: 1 [35790/67482]\tLoss: 46.0170\n",
      "Training Epoch: 1 [35820/67482]\tLoss: 45.5887\n",
      "Training Epoch: 1 [35850/67482]\tLoss: 45.1803\n",
      "Training Epoch: 1 [35880/67482]\tLoss: 43.8893\n",
      "Training Epoch: 1 [35910/67482]\tLoss: 44.5394\n",
      "Training Epoch: 1 [35940/67482]\tLoss: 45.0834\n",
      "Training Epoch: 1 [35970/67482]\tLoss: 44.7660\n",
      "Training Epoch: 1 [36000/67482]\tLoss: 45.1479\n",
      "Training Epoch: 1 [36030/67482]\tLoss: 45.9934\n",
      "Training Epoch: 1 [36060/67482]\tLoss: 45.6139\n",
      "Training Epoch: 1 [36090/67482]\tLoss: 43.9899\n",
      "Training Epoch: 1 [36120/67482]\tLoss: 44.3322\n",
      "Training Epoch: 1 [36150/67482]\tLoss: 44.9726\n",
      "Training Epoch: 1 [36180/67482]\tLoss: 44.3849\n",
      "Training Epoch: 1 [36210/67482]\tLoss: 45.3488\n",
      "Training Epoch: 1 [36240/67482]\tLoss: 45.3661\n",
      "Training Epoch: 1 [36270/67482]\tLoss: 44.9001\n",
      "Training Epoch: 1 [36300/67482]\tLoss: 45.2702\n",
      "Training Epoch: 1 [36330/67482]\tLoss: 43.9740\n",
      "Training Epoch: 1 [36360/67482]\tLoss: 44.6767\n",
      "Training Epoch: 1 [36390/67482]\tLoss: 44.4520\n",
      "Training Epoch: 1 [36420/67482]\tLoss: 44.6507\n",
      "Training Epoch: 1 [36450/67482]\tLoss: 45.0277\n",
      "Training Epoch: 1 [36480/67482]\tLoss: 43.8945\n",
      "Training Epoch: 1 [36510/67482]\tLoss: 44.8075\n",
      "Training Epoch: 1 [36540/67482]\tLoss: 45.3352\n",
      "Training Epoch: 1 [36570/67482]\tLoss: 44.5827\n",
      "Training Epoch: 1 [36600/67482]\tLoss: 44.1257\n",
      "Training Epoch: 1 [36630/67482]\tLoss: 45.2846\n",
      "Training Epoch: 1 [36660/67482]\tLoss: 44.7949\n",
      "Training Epoch: 1 [36690/67482]\tLoss: 45.1496\n",
      "Training Epoch: 1 [36720/67482]\tLoss: 44.4979\n",
      "Training Epoch: 1 [36750/67482]\tLoss: 43.4097\n",
      "Training Epoch: 1 [36780/67482]\tLoss: 44.1967\n",
      "Training Epoch: 1 [36810/67482]\tLoss: 44.6312\n",
      "Training Epoch: 1 [36840/67482]\tLoss: 44.7508\n",
      "Training Epoch: 1 [36870/67482]\tLoss: 45.3434\n",
      "Training Epoch: 1 [36900/67482]\tLoss: 43.9279\n",
      "Training Epoch: 1 [36930/67482]\tLoss: 43.6942\n",
      "Training Epoch: 1 [36960/67482]\tLoss: 44.8839\n",
      "Training Epoch: 1 [36990/67482]\tLoss: 45.1313\n",
      "Training Epoch: 1 [37020/67482]\tLoss: 44.6880\n",
      "Training Epoch: 1 [37050/67482]\tLoss: 44.6102\n",
      "Training Epoch: 1 [37080/67482]\tLoss: 44.1205\n",
      "Training Epoch: 1 [37110/67482]\tLoss: 43.1648\n",
      "Training Epoch: 1 [37140/67482]\tLoss: 45.2377\n",
      "Training Epoch: 1 [37170/67482]\tLoss: 44.3597\n",
      "Training Epoch: 1 [37200/67482]\tLoss: 44.4876\n",
      "Training Epoch: 1 [37230/67482]\tLoss: 44.7683\n",
      "Training Epoch: 1 [37260/67482]\tLoss: 44.0348\n",
      "Training Epoch: 1 [37290/67482]\tLoss: 44.1011\n",
      "Training Epoch: 1 [37320/67482]\tLoss: 44.0389\n",
      "Training Epoch: 1 [37350/67482]\tLoss: 43.7006\n",
      "Training Epoch: 1 [37380/67482]\tLoss: 44.0145\n",
      "Training Epoch: 1 [37410/67482]\tLoss: 43.5774\n",
      "Training Epoch: 1 [37440/67482]\tLoss: 44.0845\n",
      "Training Epoch: 1 [37470/67482]\tLoss: 44.7499\n",
      "Training Epoch: 1 [37500/67482]\tLoss: 44.1155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [37530/67482]\tLoss: 44.0758\n",
      "Training Epoch: 1 [37560/67482]\tLoss: 43.5917\n",
      "Training Epoch: 1 [37590/67482]\tLoss: 43.6650\n",
      "Training Epoch: 1 [37620/67482]\tLoss: 43.5276\n",
      "Training Epoch: 1 [37650/67482]\tLoss: 43.5766\n",
      "Training Epoch: 1 [37680/67482]\tLoss: 43.8860\n",
      "Training Epoch: 1 [37710/67482]\tLoss: 43.5808\n",
      "Training Epoch: 1 [37740/67482]\tLoss: 43.3974\n",
      "Training Epoch: 1 [37770/67482]\tLoss: 44.1185\n",
      "Training Epoch: 1 [37800/67482]\tLoss: 43.3031\n",
      "Training Epoch: 1 [37830/67482]\tLoss: 43.2447\n",
      "Training Epoch: 1 [37860/67482]\tLoss: 44.5105\n",
      "Training Epoch: 1 [37890/67482]\tLoss: 44.5308\n",
      "Training Epoch: 1 [37920/67482]\tLoss: 43.9460\n",
      "Training Epoch: 1 [37950/67482]\tLoss: 43.7863\n",
      "Training Epoch: 1 [37980/67482]\tLoss: 43.4628\n",
      "Training Epoch: 1 [38010/67482]\tLoss: 43.2648\n",
      "Training Epoch: 1 [38040/67482]\tLoss: 43.9378\n",
      "Training Epoch: 1 [38070/67482]\tLoss: 43.3513\n",
      "Training Epoch: 1 [38100/67482]\tLoss: 42.9002\n",
      "Training Epoch: 1 [38130/67482]\tLoss: 43.2166\n",
      "Training Epoch: 1 [38160/67482]\tLoss: 42.5481\n",
      "Training Epoch: 1 [38190/67482]\tLoss: 43.7253\n",
      "Training Epoch: 1 [38220/67482]\tLoss: 43.6817\n",
      "Training Epoch: 1 [38250/67482]\tLoss: 43.6079\n",
      "Training Epoch: 1 [38280/67482]\tLoss: 43.0938\n",
      "Training Epoch: 1 [38310/67482]\tLoss: 42.7654\n",
      "Training Epoch: 1 [38340/67482]\tLoss: 42.8343\n",
      "Training Epoch: 1 [38370/67482]\tLoss: 42.5427\n",
      "Training Epoch: 1 [38400/67482]\tLoss: 43.3919\n",
      "Training Epoch: 1 [38430/67482]\tLoss: 42.6419\n",
      "Training Epoch: 1 [38460/67482]\tLoss: 43.5132\n",
      "Training Epoch: 1 [38490/67482]\tLoss: 43.6380\n",
      "Training Epoch: 1 [38520/67482]\tLoss: 42.6358\n",
      "Training Epoch: 1 [38550/67482]\tLoss: 43.9615\n",
      "Training Epoch: 1 [38580/67482]\tLoss: 42.9991\n",
      "Training Epoch: 1 [38610/67482]\tLoss: 43.1123\n",
      "Training Epoch: 1 [38640/67482]\tLoss: 43.5486\n",
      "Training Epoch: 1 [38670/67482]\tLoss: 43.1970\n",
      "Training Epoch: 1 [38700/67482]\tLoss: 42.4600\n",
      "Training Epoch: 1 [38730/67482]\tLoss: 43.8501\n",
      "Training Epoch: 1 [38760/67482]\tLoss: 42.6274\n",
      "Training Epoch: 1 [38790/67482]\tLoss: 42.8166\n",
      "Training Epoch: 1 [38820/67482]\tLoss: 43.7953\n",
      "Training Epoch: 1 [38850/67482]\tLoss: 43.5606\n",
      "Training Epoch: 1 [38880/67482]\tLoss: 42.8125\n",
      "Training Epoch: 1 [38910/67482]\tLoss: 42.8993\n",
      "Training Epoch: 1 [38940/67482]\tLoss: 43.0076\n",
      "Training Epoch: 1 [38970/67482]\tLoss: 43.1183\n",
      "Training Epoch: 1 [39000/67482]\tLoss: 43.4225\n",
      "Training Epoch: 1 [39030/67482]\tLoss: 42.8071\n",
      "Training Epoch: 1 [39060/67482]\tLoss: 42.7776\n",
      "Training Epoch: 1 [39090/67482]\tLoss: 42.6816\n",
      "Training Epoch: 1 [39120/67482]\tLoss: 42.9528\n",
      "Training Epoch: 1 [39150/67482]\tLoss: 43.2320\n",
      "Training Epoch: 1 [39180/67482]\tLoss: 43.1319\n",
      "Training Epoch: 1 [39210/67482]\tLoss: 42.9869\n",
      "Training Epoch: 1 [39240/67482]\tLoss: 42.9691\n",
      "Training Epoch: 1 [39270/67482]\tLoss: 42.3131\n",
      "Training Epoch: 1 [39300/67482]\tLoss: 42.3231\n",
      "Training Epoch: 1 [39330/67482]\tLoss: 42.6969\n",
      "Training Epoch: 1 [39360/67482]\tLoss: 43.3891\n",
      "Training Epoch: 1 [39390/67482]\tLoss: 43.2157\n",
      "Training Epoch: 1 [39420/67482]\tLoss: 42.9647\n",
      "Training Epoch: 1 [39450/67482]\tLoss: 42.9953\n",
      "Training Epoch: 1 [39480/67482]\tLoss: 42.1145\n",
      "Training Epoch: 1 [39510/67482]\tLoss: 43.3470\n",
      "Training Epoch: 1 [39540/67482]\tLoss: 42.9622\n",
      "Training Epoch: 1 [39570/67482]\tLoss: 42.9493\n",
      "Training Epoch: 1 [39600/67482]\tLoss: 42.6466\n",
      "Training Epoch: 1 [39630/67482]\tLoss: 42.4656\n",
      "Training Epoch: 1 [39660/67482]\tLoss: 42.4968\n",
      "Training Epoch: 1 [39690/67482]\tLoss: 42.9046\n",
      "Training Epoch: 1 [39720/67482]\tLoss: 42.1194\n",
      "Training Epoch: 1 [39750/67482]\tLoss: 43.0362\n",
      "Training Epoch: 1 [39780/67482]\tLoss: 42.5588\n",
      "Training Epoch: 1 [39810/67482]\tLoss: 42.1475\n",
      "Training Epoch: 1 [39840/67482]\tLoss: 43.1458\n",
      "Training Epoch: 1 [39870/67482]\tLoss: 42.2147\n",
      "Training Epoch: 1 [39900/67482]\tLoss: 43.2239\n",
      "Training Epoch: 1 [39930/67482]\tLoss: 42.8158\n",
      "Training Epoch: 1 [39960/67482]\tLoss: 41.7445\n",
      "Training Epoch: 1 [39990/67482]\tLoss: 42.2246\n",
      "Training Epoch: 1 [40020/67482]\tLoss: 41.8898\n",
      "Training Epoch: 1 [40050/67482]\tLoss: 42.4571\n",
      "Training Epoch: 1 [40080/67482]\tLoss: 42.6742\n",
      "Training Epoch: 1 [40110/67482]\tLoss: 42.2717\n",
      "Training Epoch: 1 [40140/67482]\tLoss: 42.4267\n",
      "Training Epoch: 1 [40170/67482]\tLoss: 42.6730\n",
      "Training Epoch: 1 [40200/67482]\tLoss: 41.6775\n",
      "Training Epoch: 1 [40230/67482]\tLoss: 42.5153\n",
      "Training Epoch: 1 [40260/67482]\tLoss: 42.5830\n",
      "Training Epoch: 1 [40290/67482]\tLoss: 42.0361\n",
      "Training Epoch: 1 [40320/67482]\tLoss: 42.2215\n",
      "Training Epoch: 1 [40350/67482]\tLoss: 43.3971\n",
      "Training Epoch: 1 [40380/67482]\tLoss: 41.5929\n",
      "Training Epoch: 1 [40410/67482]\tLoss: 42.0734\n",
      "Training Epoch: 1 [40440/67482]\tLoss: 42.2249\n",
      "Training Epoch: 1 [40470/67482]\tLoss: 42.0138\n",
      "Training Epoch: 1 [40500/67482]\tLoss: 42.2684\n",
      "Training Epoch: 1 [40530/67482]\tLoss: 42.0898\n",
      "Training Epoch: 1 [40560/67482]\tLoss: 42.3002\n",
      "Training Epoch: 1 [40590/67482]\tLoss: 41.9730\n",
      "Training Epoch: 1 [40620/67482]\tLoss: 42.2211\n",
      "Training Epoch: 1 [40650/67482]\tLoss: 42.2477\n",
      "Training Epoch: 1 [40680/67482]\tLoss: 41.6885\n",
      "Training Epoch: 1 [40710/67482]\tLoss: 41.7519\n",
      "Training Epoch: 1 [40740/67482]\tLoss: 42.5314\n",
      "Training Epoch: 1 [40770/67482]\tLoss: 41.5875\n",
      "Training Epoch: 1 [40800/67482]\tLoss: 42.0484\n",
      "Training Epoch: 1 [40830/67482]\tLoss: 41.8974\n",
      "Training Epoch: 1 [40860/67482]\tLoss: 41.2858\n",
      "Training Epoch: 1 [40890/67482]\tLoss: 41.7341\n",
      "Training Epoch: 1 [40920/67482]\tLoss: 41.9344\n",
      "Training Epoch: 1 [40950/67482]\tLoss: 42.0519\n",
      "Training Epoch: 1 [40980/67482]\tLoss: 41.8672\n",
      "Training Epoch: 1 [41010/67482]\tLoss: 41.6841\n",
      "Training Epoch: 1 [41040/67482]\tLoss: 42.3413\n",
      "Training Epoch: 1 [41070/67482]\tLoss: 41.5793\n",
      "Training Epoch: 1 [41100/67482]\tLoss: 41.0873\n",
      "Training Epoch: 1 [41130/67482]\tLoss: 42.4139\n",
      "Training Epoch: 1 [41160/67482]\tLoss: 42.0932\n",
      "Training Epoch: 1 [41190/67482]\tLoss: 41.3411\n",
      "Training Epoch: 1 [41220/67482]\tLoss: 41.3033\n",
      "Training Epoch: 1 [41250/67482]\tLoss: 41.5179\n",
      "Training Epoch: 1 [41280/67482]\tLoss: 41.3937\n",
      "Training Epoch: 1 [41310/67482]\tLoss: 41.5402\n",
      "Training Epoch: 1 [41340/67482]\tLoss: 41.5944\n",
      "Training Epoch: 1 [41370/67482]\tLoss: 41.2491\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 \"epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(train_dataset.n_data, pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fad6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1e277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3b2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
