{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755c21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 17 03:05:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:08:00.0 Off |                  N/A |\n",
      "|  0%   31C    P0    71W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:09:00.0 Off |                  N/A |\n",
      "|  0%   40C    P0    73W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:85:00.0 Off |                  N/A |\n",
      "|  0%   38C    P0    75W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:86:00.0 Off |                  N/A |\n",
      "|  0%   36C    P0    74W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f10aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a122d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key_path = f'PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META_key.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb68c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51daeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key = np.load(f'{engram_dir}{f_key_path}').tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1affbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'above',\n",
       " b'according',\n",
       " b'account',\n",
       " b'acquire',\n",
       " b'acquired',\n",
       " b'acquisition',\n",
       " b'acquisitions',\n",
       " b'action',\n",
       " b'addition',\n",
       " b'additional',\n",
       " b'adds',\n",
       " b'administration',\n",
       " b'advantage',\n",
       " b'again',\n",
       " b'agency',\n",
       " b'agreed',\n",
       " b'agreement',\n",
       " b'airline',\n",
       " b'allow',\n",
       " b'almost',\n",
       " b'along',\n",
       " b'although',\n",
       " b'always',\n",
       " b'america',\n",
       " b'amount',\n",
       " b'analyst',\n",
       " b'announced',\n",
       " b'anything',\n",
       " b'apparently',\n",
       " b'approval',\n",
       " b'approved',\n",
       " b'april',\n",
       " b'areas',\n",
       " b'around',\n",
       " b'asked',\n",
       " b'asset',\n",
       " b'association',\n",
       " b'attorney',\n",
       " b'august',\n",
       " b'auto',\n",
       " b'away',\n",
       " b'bankers',\n",
       " b'banking',\n",
       " b'basis',\n",
       " b'become',\n",
       " b'began',\n",
       " b'behind',\n",
       " b'believe',\n",
       " b'below',\n",
       " b'best',\n",
       " b'bids',\n",
       " b'biggest',\n",
       " b'bill',\n",
       " b'bills',\n",
       " b'black',\n",
       " b'bond',\n",
       " b'book',\n",
       " b'bought',\n",
       " b'british',\n",
       " b'brokerage',\n",
       " b'brokers',\n",
       " b'budget',\n",
       " b'build',\n",
       " b'building',\n",
       " b'bush',\n",
       " b'businesses',\n",
       " b'buying',\n",
       " b'cable',\n",
       " b'california',\n",
       " b'call',\n",
       " b'called',\n",
       " b'calls',\n",
       " b'came',\n",
       " b'campaign',\n",
       " b\"can't\",\n",
       " b'canada',\n",
       " b'canadian',\n",
       " b'carry',\n",
       " b'cars',\n",
       " b'cases',\n",
       " b'central',\n",
       " b'certain',\n",
       " b'change',\n",
       " b'changes',\n",
       " b'charge',\n",
       " b'children',\n",
       " b'city',\n",
       " b'clear',\n",
       " b'come',\n",
       " b'coming',\n",
       " b'comment',\n",
       " b'commercial',\n",
       " b'commission',\n",
       " b'committee',\n",
       " b'common',\n",
       " b'communist',\n",
       " b'compared',\n",
       " b'competition',\n",
       " b'completed',\n",
       " b'composite',\n",
       " b'computer',\n",
       " b'concern',\n",
       " b'congressional',\n",
       " b'considered',\n",
       " b'considering',\n",
       " b'construction',\n",
       " b'consumer',\n",
       " b'continue',\n",
       " b'continued',\n",
       " b'contract',\n",
       " b'corporate',\n",
       " b'costs',\n",
       " b\"couldn't\",\n",
       " b'countries',\n",
       " b'country',\n",
       " b'course',\n",
       " b'crash',\n",
       " b'credit',\n",
       " b'critics',\n",
       " b'currently',\n",
       " b'customers',\n",
       " b'data',\n",
       " b'david',\n",
       " b'deal',\n",
       " b'dealers',\n",
       " b'december',\n",
       " b'decision',\n",
       " b'decline',\n",
       " b'declined',\n",
       " b'defense',\n",
       " b'deficit',\n",
       " b'demand',\n",
       " b'democratic',\n",
       " b'democrats',\n",
       " b'department',\n",
       " b'despite',\n",
       " b'development',\n",
       " b'difficult',\n",
       " b'directly',\n",
       " b'director',\n",
       " b'doctor',\n",
       " b'does',\n",
       " b'doing',\n",
       " b'domestic',\n",
       " b'done',\n",
       " b'drexel',\n",
       " b'drop',\n",
       " b'drug',\n",
       " b'during',\n",
       " b'early',\n",
       " b'earnings',\n",
       " b'economists',\n",
       " b'effect',\n",
       " b'effort',\n",
       " b'efforts',\n",
       " b'eighteen',\n",
       " b'eighths',\n",
       " b'either',\n",
       " b'election',\n",
       " b'eleven',\n",
       " b'employees',\n",
       " b'ended',\n",
       " b'energy',\n",
       " b'enough',\n",
       " b'equipment',\n",
       " b'equity',\n",
       " b'especially',\n",
       " b'estate',\n",
       " b'estimated',\n",
       " b'estimates',\n",
       " b'european',\n",
       " b'eventually',\n",
       " b'ever',\n",
       " b'every',\n",
       " b'evidence',\n",
       " b'example',\n",
       " b'executives',\n",
       " b'expect',\n",
       " b'expects',\n",
       " b'export',\n",
       " b'exports',\n",
       " b'face',\n",
       " b'fact',\n",
       " b'fall',\n",
       " b'family',\n",
       " b'farm',\n",
       " b'farmers',\n",
       " b'filed',\n",
       " b'finance',\n",
       " b'financing',\n",
       " b'find',\n",
       " b'firms',\n",
       " b'food',\n",
       " b'force',\n",
       " b'ford',\n",
       " b'former',\n",
       " b'found',\n",
       " b'fourteen',\n",
       " b'fourth',\n",
       " b'free',\n",
       " b'friday',\n",
       " b'funds',\n",
       " b'further',\n",
       " b'future',\n",
       " b'futures',\n",
       " b'gain',\n",
       " b'gains',\n",
       " b'generally',\n",
       " b'given',\n",
       " b'global',\n",
       " b'gold',\n",
       " b'great',\n",
       " b'groups',\n",
       " b'growing',\n",
       " b'hard',\n",
       " b\"hasn't\",\n",
       " b\"haven't\",\n",
       " b\"he's\",\n",
       " b'head',\n",
       " b'health',\n",
       " b'heavy',\n",
       " b'held',\n",
       " b'helped',\n",
       " b'himself',\n",
       " b'hold',\n",
       " b'holding',\n",
       " b'holdings',\n",
       " b'home',\n",
       " b'hostile',\n",
       " b'housing',\n",
       " b'huge',\n",
       " b'human',\n",
       " b'idea',\n",
       " b'important',\n",
       " b'include',\n",
       " b'included',\n",
       " b'including',\n",
       " b'income',\n",
       " b'increased',\n",
       " b'increases',\n",
       " b'increasingly',\n",
       " b'industrial',\n",
       " b'industries',\n",
       " b'information',\n",
       " b'instance',\n",
       " b'instead',\n",
       " b'insurance',\n",
       " b'interests',\n",
       " b'investigation',\n",
       " b'involved',\n",
       " b'issued',\n",
       " b'itself',\n",
       " b'january',\n",
       " b'japan',\n",
       " b'john',\n",
       " b'judge',\n",
       " b'july',\n",
       " b'june',\n",
       " b'keep',\n",
       " b'kind',\n",
       " b'known',\n",
       " b'korean',\n",
       " b'labor',\n",
       " b'land',\n",
       " b'largest',\n",
       " b'late',\n",
       " b'later',\n",
       " b'latest',\n",
       " b'lead',\n",
       " b'leaders',\n",
       " b'leave',\n",
       " b'left',\n",
       " b'level',\n",
       " b'levels',\n",
       " b'life',\n",
       " b'likely',\n",
       " b'limited',\n",
       " b'line',\n",
       " b'loan',\n",
       " b'loans',\n",
       " b'local',\n",
       " b'london',\n",
       " b'longer',\n",
       " b'look',\n",
       " b'looking',\n",
       " b'loss',\n",
       " b'losses',\n",
       " b'magazine',\n",
       " b'makers',\n",
       " b'makes',\n",
       " b'making',\n",
       " b'management',\n",
       " b'managers',\n",
       " b'manufacturing',\n",
       " b'marketing',\n",
       " b'mean',\n",
       " b'meanwhile',\n",
       " b'media',\n",
       " b'meeting',\n",
       " b'member',\n",
       " b'members',\n",
       " b'merger',\n",
       " b'military',\n",
       " b'monday',\n",
       " b'morgan',\n",
       " b'must',\n",
       " b'name',\n",
       " b'named',\n",
       " b\"nation's\",\n",
       " b'near',\n",
       " b'nearly',\n",
       " b'need',\n",
       " b'needed',\n",
       " b'needs',\n",
       " b'never',\n",
       " b'news',\n",
       " b'night',\n",
       " b'north',\n",
       " b'notes',\n",
       " b'nothing',\n",
       " b'november',\n",
       " b'nuclear',\n",
       " b'number',\n",
       " b'october',\n",
       " b'offering',\n",
       " b'office',\n",
       " b'officer',\n",
       " b'often',\n",
       " b'once',\n",
       " b'open',\n",
       " b'opening',\n",
       " b'operation',\n",
       " b'opportunity',\n",
       " b'option',\n",
       " b'options',\n",
       " b'order',\n",
       " b'orders',\n",
       " b'others',\n",
       " b'outside',\n",
       " b'outstanding',\n",
       " b'owned',\n",
       " b'paid',\n",
       " b'party',\n",
       " b'payments',\n",
       " b'percentage',\n",
       " b'period',\n",
       " b'personal',\n",
       " b'place',\n",
       " b'plant',\n",
       " b'plants',\n",
       " b'plus',\n",
       " b'points',\n",
       " b'policies',\n",
       " b'position',\n",
       " b'positions',\n",
       " b'possible',\n",
       " b'post',\n",
       " b'potential',\n",
       " b'power',\n",
       " b'presidential',\n",
       " b'press',\n",
       " b'pressure',\n",
       " b'previous',\n",
       " b'previously',\n",
       " b'private',\n",
       " b'probably',\n",
       " b'problem',\n",
       " b'problems',\n",
       " b'process',\n",
       " b'product',\n",
       " b'products',\n",
       " b'program',\n",
       " b'proposal',\n",
       " b'proposed',\n",
       " b'provide',\n",
       " b'purchase',\n",
       " b'quarters',\n",
       " b'question',\n",
       " b'quickly',\n",
       " b'quite',\n",
       " b'raise',\n",
       " b'raised',\n",
       " b'rather',\n",
       " b'reached',\n",
       " b'reagan',\n",
       " b'reason',\n",
       " b'received',\n",
       " b'recently',\n",
       " b'recession',\n",
       " b'record',\n",
       " b'related',\n",
       " b'relations',\n",
       " b'remain',\n",
       " b'report',\n",
       " b'reported',\n",
       " b'reports',\n",
       " b'representative',\n",
       " b'require',\n",
       " b'reserve',\n",
       " b'reserves',\n",
       " b'response',\n",
       " b'rest',\n",
       " b'restructuring',\n",
       " b'result',\n",
       " b'results',\n",
       " b'retail',\n",
       " b'return',\n",
       " b'revenue',\n",
       " b'review',\n",
       " b'rights',\n",
       " b'rise',\n",
       " b'risk',\n",
       " b'robert',\n",
       " b'rule',\n",
       " b'ruling',\n",
       " b'running',\n",
       " b'same',\n",
       " b'secretary',\n",
       " b'security',\n",
       " b'seeking',\n",
       " b'seem',\n",
       " b'seems',\n",
       " b'seen',\n",
       " b'selling',\n",
       " b'senate',\n",
       " b'senator',\n",
       " b'senior',\n",
       " b'september',\n",
       " b'services',\n",
       " b'settlement',\n",
       " b'shareholders',\n",
       " b'shearson',\n",
       " b'show',\n",
       " b'side',\n",
       " b'similar',\n",
       " b'situation',\n",
       " b'small',\n",
       " b'social',\n",
       " b'software',\n",
       " b'something',\n",
       " b'soon',\n",
       " b'source',\n",
       " b'south',\n",
       " b'soviet',\n",
       " b'soviets',\n",
       " b'special',\n",
       " b'spending',\n",
       " b'spokeswoman',\n",
       " b'staff',\n",
       " b'stake',\n",
       " b'stanley',\n",
       " b'start',\n",
       " b'started',\n",
       " b'statement',\n",
       " b'states',\n",
       " b'stocks',\n",
       " b'store',\n",
       " b'stores',\n",
       " b'street',\n",
       " b'study',\n",
       " b'subject',\n",
       " b'suit',\n",
       " b'supply',\n",
       " b'support',\n",
       " b'sure',\n",
       " b'system',\n",
       " b'systems',\n",
       " b'taken',\n",
       " b'takeover',\n",
       " b'taking',\n",
       " b'talks',\n",
       " b'taxes',\n",
       " b'team',\n",
       " b'technology',\n",
       " b'television',\n",
       " b'terms',\n",
       " b'test',\n",
       " b'texas',\n",
       " b\"that's\",\n",
       " b\"there's\",\n",
       " b\"they're\",\n",
       " b\"they've\",\n",
       " b'thing',\n",
       " b'things',\n",
       " b'thought',\n",
       " b'thus',\n",
       " b'times',\n",
       " b'today',\n",
       " b'told',\n",
       " b'took',\n",
       " b'total',\n",
       " b'traders',\n",
       " b'treasury',\n",
       " b'true',\n",
       " b'trust',\n",
       " b'trying',\n",
       " b'tuesday',\n",
       " b'turn',\n",
       " b'turned',\n",
       " b'united',\n",
       " b'used',\n",
       " b'using',\n",
       " b'utility',\n",
       " b'various',\n",
       " b'volume',\n",
       " b'vote',\n",
       " b'wall',\n",
       " b'wants',\n",
       " b'washington',\n",
       " b\"wasn't\",\n",
       " b'weeks',\n",
       " b'went',\n",
       " b'west',\n",
       " b'western',\n",
       " b'whether',\n",
       " b'white',\n",
       " b'whole',\n",
       " b'within',\n",
       " b'without',\n",
       " b'women',\n",
       " b'worked',\n",
       " b'workers',\n",
       " b\"world's\",\n",
       " b'worth',\n",
       " b\"wouldn't\",\n",
       " b'wrong',\n",
       " b\"year's\",\n",
       " b'yield',\n",
       " b'young',\n",
       " b'zero']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18575d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from models.networks_2022 import BranchedNetwork\n",
    "from data.ReconstructionTrainingDataset import CleanSoundsDataset\n",
    "from data.ReconstructionTrainingDataset import NoisySoundsDataset\n",
    "from data.ReconstructionTrainingDataset import GammaNoiseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d62c53",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccca1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_name = 'pnet'\n",
    "_train_datafile = 'clean_reconstruction_training_set'\n",
    "SoundsDataset = CleanSoundsDataset\n",
    "dset_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e55df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = 'neg9'\n",
    "bg = 'pink_noise'\n",
    "pnet_name = 'pnet_snr-9_pinkNoise'\n",
    "_train_datafile = 'hyperparameter_pooled_training_dataset_random_order_noNulls'\n",
    "SoundsDataset = NoisySoundsDataset\n",
    "dset_kwargs = {'snr': snr, 'bg': bg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ddc187",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_name = 'pnet_snr-9_pt2'\n",
    "_train_datafile = 'hyperparameter_pooled_training_dataset_random_order_noNulls'\n",
    "SoundsDataset = NoisySoundsDataset\n",
    "dset_kwargs = {'snr': 'neg9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f21e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_name = 'pnet_gammaNoise'\n",
    "_train_datafile = 'gammaNoise_reconstruction_training_set'\n",
    "SoundsDataset = GammaNoiseDataset\n",
    "dset_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4656af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e231cb4",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6da65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 30\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "lr = 1E-5\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}{_train_datafile}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "806f8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  2 13:21:02 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:8A:00.0 Off |                  N/A |\r\n",
      "|  0%   20C    P8     7W / 250W |   3016MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     24658      C   ...onda/envs/hcnn/bin/python     3012MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85498379",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6d7dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/ctn/users/cf2794/Code/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/ctn/users/cf2794/Code/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "231f267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35993b",
   "metadata": {},
   "source": [
    "#### Load checkpoint if desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa112c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.load_state_dict(torch.load(\n",
    "    f'{engram_dir}checkpoints/pnet/pnet-150-regular.pth'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d95e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2096dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3a0ba",
   "metadata": {},
   "source": [
    "# Set up train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e911f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SoundsDataset(\n",
    "    train_datafile, subset=.9, **dset_kwargs)\n",
    "test_dataset = SoundsDataset(\n",
    "    train_datafile, subset=.9,\n",
    "    train = False, **dset_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d10c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e92aed",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb071bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a303771",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [30/36045]\tLoss: 496.1280\n",
      "Training Epoch: 1 [60/36045]\tLoss: 538.0731\n",
      "Training Epoch: 1 [90/36045]\tLoss: 467.9078\n",
      "Training Epoch: 1 [120/36045]\tLoss: 498.6032\n",
      "Training Epoch: 1 [150/36045]\tLoss: 489.8661\n",
      "Training Epoch: 1 [180/36045]\tLoss: 473.2509\n",
      "Training Epoch: 1 [210/36045]\tLoss: 447.1912\n",
      "Training Epoch: 1 [240/36045]\tLoss: 572.2451\n",
      "Training Epoch: 1 [270/36045]\tLoss: 686.7305\n",
      "Training Epoch: 1 [300/36045]\tLoss: 640.4139\n",
      "Training Epoch: 1 [330/36045]\tLoss: 621.3936\n",
      "Training Epoch: 1 [360/36045]\tLoss: 586.4219\n",
      "Training Epoch: 1 [390/36045]\tLoss: 620.4456\n",
      "Training Epoch: 1 [420/36045]\tLoss: 577.4561\n",
      "Training Epoch: 1 [450/36045]\tLoss: 588.5890\n",
      "Training Epoch: 1 [480/36045]\tLoss: 526.9437\n",
      "Training Epoch: 1 [510/36045]\tLoss: 540.8100\n",
      "Training Epoch: 1 [540/36045]\tLoss: 535.0945\n",
      "Training Epoch: 1 [570/36045]\tLoss: 531.2734\n",
      "Training Epoch: 1 [600/36045]\tLoss: 523.5986\n",
      "Training Epoch: 1 [630/36045]\tLoss: 550.3634\n",
      "Training Epoch: 1 [660/36045]\tLoss: 539.7185\n",
      "Training Epoch: 1 [690/36045]\tLoss: 515.2935\n",
      "Training Epoch: 1 [720/36045]\tLoss: 490.7481\n",
      "Training Epoch: 1 [750/36045]\tLoss: 494.5753\n",
      "Training Epoch: 1 [780/36045]\tLoss: 487.0080\n",
      "Training Epoch: 1 [810/36045]\tLoss: 512.0861\n",
      "Training Epoch: 1 [840/36045]\tLoss: 490.7302\n",
      "Training Epoch: 1 [870/36045]\tLoss: 471.6927\n",
      "Training Epoch: 1 [900/36045]\tLoss: 452.8763\n",
      "Training Epoch: 1 [930/36045]\tLoss: 444.8215\n",
      "Training Epoch: 1 [960/36045]\tLoss: 432.9356\n",
      "Training Epoch: 1 [990/36045]\tLoss: 428.9868\n",
      "Training Epoch: 1 [1020/36045]\tLoss: 434.3885\n",
      "Training Epoch: 1 [1050/36045]\tLoss: 432.4354\n",
      "Training Epoch: 1 [1080/36045]\tLoss: 420.6981\n",
      "Training Epoch: 1 [1110/36045]\tLoss: 432.4291\n",
      "Training Epoch: 1 [1140/36045]\tLoss: 431.9996\n",
      "Training Epoch: 1 [1170/36045]\tLoss: 446.5099\n",
      "Training Epoch: 1 [1200/36045]\tLoss: 465.4524\n",
      "Training Epoch: 1 [1230/36045]\tLoss: 523.1169\n",
      "Training Epoch: 1 [1260/36045]\tLoss: 518.6468\n",
      "Training Epoch: 1 [1290/36045]\tLoss: 531.0054\n",
      "Training Epoch: 1 [1320/36045]\tLoss: 546.9909\n",
      "Training Epoch: 1 [1350/36045]\tLoss: 507.1392\n",
      "Training Epoch: 1 [1380/36045]\tLoss: 550.2557\n",
      "Training Epoch: 1 [1410/36045]\tLoss: 543.3454\n",
      "Training Epoch: 1 [1440/36045]\tLoss: 533.8658\n",
      "Training Epoch: 1 [1470/36045]\tLoss: 507.0468\n",
      "Training Epoch: 1 [1500/36045]\tLoss: 468.2626\n",
      "Training Epoch: 1 [1530/36045]\tLoss: 495.3686\n",
      "Training Epoch: 1 [1560/36045]\tLoss: 485.9808\n",
      "Training Epoch: 1 [1590/36045]\tLoss: 513.8698\n",
      "Training Epoch: 1 [1620/36045]\tLoss: 496.3465\n",
      "Training Epoch: 1 [1650/36045]\tLoss: 480.9832\n",
      "Training Epoch: 1 [1680/36045]\tLoss: 477.6335\n",
      "Training Epoch: 1 [1710/36045]\tLoss: 554.6767\n",
      "Training Epoch: 1 [1740/36045]\tLoss: 550.7065\n",
      "Training Epoch: 1 [1770/36045]\tLoss: 531.0428\n",
      "Training Epoch: 1 [1800/36045]\tLoss: 535.6533\n",
      "Training Epoch: 1 [1830/36045]\tLoss: 547.5452\n",
      "Training Epoch: 1 [1860/36045]\tLoss: 522.2668\n",
      "Training Epoch: 1 [1890/36045]\tLoss: 509.1870\n",
      "Training Epoch: 1 [1920/36045]\tLoss: 543.0428\n",
      "Training Epoch: 1 [1950/36045]\tLoss: 512.5823\n",
      "Training Epoch: 1 [1980/36045]\tLoss: 460.1971\n",
      "Training Epoch: 1 [2010/36045]\tLoss: 455.1635\n",
      "Training Epoch: 1 [2040/36045]\tLoss: 459.6135\n",
      "Training Epoch: 1 [2070/36045]\tLoss: 489.9058\n",
      "Training Epoch: 1 [2100/36045]\tLoss: 478.6436\n",
      "Training Epoch: 1 [2130/36045]\tLoss: 471.3779\n",
      "Training Epoch: 1 [2160/36045]\tLoss: 472.4736\n",
      "Training Epoch: 1 [2190/36045]\tLoss: 429.2267\n",
      "Training Epoch: 1 [2220/36045]\tLoss: 433.4499\n",
      "Training Epoch: 1 [2250/36045]\tLoss: 419.9891\n",
      "Training Epoch: 1 [2280/36045]\tLoss: 437.5356\n",
      "Training Epoch: 1 [2310/36045]\tLoss: 449.0023\n",
      "Training Epoch: 1 [2340/36045]\tLoss: 416.2101\n",
      "Training Epoch: 1 [2370/36045]\tLoss: 445.0438\n",
      "Training Epoch: 1 [2400/36045]\tLoss: 426.3297\n",
      "Training Epoch: 1 [2430/36045]\tLoss: 528.2380\n",
      "Training Epoch: 1 [2460/36045]\tLoss: 586.6326\n",
      "Training Epoch: 1 [2490/36045]\tLoss: 588.6870\n",
      "Training Epoch: 1 [2520/36045]\tLoss: 550.7078\n",
      "Training Epoch: 1 [2550/36045]\tLoss: 593.5223\n",
      "Training Epoch: 1 [2580/36045]\tLoss: 587.9870\n",
      "Training Epoch: 1 [2610/36045]\tLoss: 589.2385\n",
      "Training Epoch: 1 [2640/36045]\tLoss: 720.4153\n",
      "Training Epoch: 1 [2670/36045]\tLoss: 842.1196\n",
      "Training Epoch: 1 [2700/36045]\tLoss: 772.7639\n",
      "Training Epoch: 1 [2730/36045]\tLoss: 901.5612\n",
      "Training Epoch: 1 [2760/36045]\tLoss: 872.5034\n",
      "Training Epoch: 1 [2790/36045]\tLoss: 867.3793\n",
      "Training Epoch: 1 [2820/36045]\tLoss: 765.3190\n",
      "Training Epoch: 1 [2850/36045]\tLoss: 585.8867\n",
      "Training Epoch: 1 [2880/36045]\tLoss: 592.0909\n",
      "Training Epoch: 1 [2910/36045]\tLoss: 580.7621\n",
      "Training Epoch: 1 [2940/36045]\tLoss: 591.2772\n",
      "Training Epoch: 1 [2970/36045]\tLoss: 558.0744\n",
      "Training Epoch: 1 [3000/36045]\tLoss: 578.9659\n",
      "Training Epoch: 1 [3030/36045]\tLoss: 600.7381\n",
      "Training Epoch: 1 [3060/36045]\tLoss: 569.7604\n",
      "Training Epoch: 1 [3090/36045]\tLoss: 565.9303\n",
      "Training Epoch: 1 [3120/36045]\tLoss: 447.0916\n",
      "Training Epoch: 1 [3150/36045]\tLoss: 412.4375\n",
      "Training Epoch: 1 [3180/36045]\tLoss: 443.2759\n",
      "Training Epoch: 1 [3210/36045]\tLoss: 410.2210\n",
      "Training Epoch: 1 [3240/36045]\tLoss: 398.9879\n",
      "Training Epoch: 1 [3270/36045]\tLoss: 410.4761\n",
      "Training Epoch: 1 [3300/36045]\tLoss: 378.7056\n",
      "Training Epoch: 1 [3330/36045]\tLoss: 404.1805\n",
      "Training Epoch: 1 [3360/36045]\tLoss: 412.8145\n",
      "Training Epoch: 1 [3390/36045]\tLoss: 420.9389\n",
      "Training Epoch: 1 [3420/36045]\tLoss: 474.2678\n",
      "Training Epoch: 1 [3450/36045]\tLoss: 443.7242\n",
      "Training Epoch: 1 [3480/36045]\tLoss: 437.4896\n",
      "Training Epoch: 1 [3510/36045]\tLoss: 456.7371\n",
      "Training Epoch: 1 [3540/36045]\tLoss: 417.5039\n",
      "Training Epoch: 1 [3570/36045]\tLoss: 442.9288\n",
      "Training Epoch: 1 [3600/36045]\tLoss: 460.9849\n",
      "Training Epoch: 1 [3630/36045]\tLoss: 525.1838\n",
      "Training Epoch: 1 [3660/36045]\tLoss: 525.7213\n",
      "Training Epoch: 1 [3690/36045]\tLoss: 534.1827\n",
      "Training Epoch: 1 [3720/36045]\tLoss: 534.8148\n",
      "Training Epoch: 1 [3750/36045]\tLoss: 496.9492\n",
      "Training Epoch: 1 [3780/36045]\tLoss: 514.9560\n",
      "Training Epoch: 1 [3810/36045]\tLoss: 510.7912\n",
      "Training Epoch: 1 [3840/36045]\tLoss: 508.6775\n",
      "Training Epoch: 1 [3870/36045]\tLoss: 528.9152\n",
      "Training Epoch: 1 [3900/36045]\tLoss: 524.3774\n",
      "Training Epoch: 1 [3930/36045]\tLoss: 503.0636\n",
      "Training Epoch: 1 [3960/36045]\tLoss: 515.0197\n",
      "Training Epoch: 1 [3990/36045]\tLoss: 503.1131\n",
      "Training Epoch: 1 [4020/36045]\tLoss: 500.1942\n",
      "Training Epoch: 1 [4050/36045]\tLoss: 443.6328\n",
      "Training Epoch: 1 [4080/36045]\tLoss: 452.2343\n",
      "Training Epoch: 1 [4110/36045]\tLoss: 461.0638\n",
      "Training Epoch: 1 [4140/36045]\tLoss: 456.5139\n",
      "Training Epoch: 1 [4170/36045]\tLoss: 471.7750\n",
      "Training Epoch: 1 [4200/36045]\tLoss: 451.1643\n",
      "Training Epoch: 1 [4230/36045]\tLoss: 470.2741\n",
      "Training Epoch: 1 [4260/36045]\tLoss: 446.1349\n",
      "Training Epoch: 1 [4290/36045]\tLoss: 456.5844\n",
      "Training Epoch: 1 [4320/36045]\tLoss: 469.4767\n",
      "Training Epoch: 1 [4350/36045]\tLoss: 446.7034\n",
      "Training Epoch: 1 [4380/36045]\tLoss: 437.5718\n",
      "Training Epoch: 1 [4410/36045]\tLoss: 433.4699\n",
      "Training Epoch: 1 [4440/36045]\tLoss: 462.2433\n",
      "Training Epoch: 1 [4470/36045]\tLoss: 535.9722\n",
      "Training Epoch: 1 [4500/36045]\tLoss: 509.4545\n",
      "Training Epoch: 1 [4530/36045]\tLoss: 522.7839\n",
      "Training Epoch: 1 [4560/36045]\tLoss: 513.8065\n",
      "Training Epoch: 1 [4590/36045]\tLoss: 559.0094\n",
      "Training Epoch: 1 [4620/36045]\tLoss: 524.3045\n",
      "Training Epoch: 1 [4650/36045]\tLoss: 528.1019\n",
      "Training Epoch: 1 [4680/36045]\tLoss: 510.3892\n",
      "Training Epoch: 1 [4710/36045]\tLoss: 457.4616\n",
      "Training Epoch: 1 [4740/36045]\tLoss: 471.3894\n",
      "Training Epoch: 1 [4770/36045]\tLoss: 490.5560\n",
      "Training Epoch: 1 [4800/36045]\tLoss: 482.8037\n",
      "Training Epoch: 1 [4830/36045]\tLoss: 482.0386\n",
      "Training Epoch: 1 [4860/36045]\tLoss: 474.2363\n",
      "Training Epoch: 1 [4890/36045]\tLoss: 465.7328\n",
      "Training Epoch: 1 [4920/36045]\tLoss: 458.1166\n",
      "Training Epoch: 1 [4950/36045]\tLoss: 496.9035\n",
      "Training Epoch: 1 [4980/36045]\tLoss: 510.3373\n",
      "Training Epoch: 1 [5010/36045]\tLoss: 527.4368\n",
      "Training Epoch: 1 [5040/36045]\tLoss: 499.9215\n",
      "Training Epoch: 1 [5070/36045]\tLoss: 489.8744\n",
      "Training Epoch: 1 [5100/36045]\tLoss: 511.8015\n",
      "Training Epoch: 1 [5130/36045]\tLoss: 493.9046\n",
      "Training Epoch: 1 [5160/36045]\tLoss: 499.3720\n",
      "Training Epoch: 1 [5190/36045]\tLoss: 474.7956\n",
      "Training Epoch: 1 [5220/36045]\tLoss: 472.8535\n",
      "Training Epoch: 1 [5250/36045]\tLoss: 492.9871\n",
      "Training Epoch: 1 [5280/36045]\tLoss: 477.2518\n",
      "Training Epoch: 1 [5310/36045]\tLoss: 491.5757\n",
      "Training Epoch: 1 [5340/36045]\tLoss: 500.4165\n",
      "Training Epoch: 1 [5370/36045]\tLoss: 482.2662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [5400/36045]\tLoss: 476.3601\n",
      "Training Epoch: 1 [5430/36045]\tLoss: 460.3575\n",
      "Training Epoch: 1 [5460/36045]\tLoss: 446.1279\n",
      "Training Epoch: 1 [5490/36045]\tLoss: 492.0411\n",
      "Training Epoch: 1 [5520/36045]\tLoss: 470.4036\n",
      "Training Epoch: 1 [5550/36045]\tLoss: 467.1649\n",
      "Training Epoch: 1 [5580/36045]\tLoss: 536.0987\n",
      "Training Epoch: 1 [5610/36045]\tLoss: 528.0709\n",
      "Training Epoch: 1 [5640/36045]\tLoss: 532.6179\n",
      "Training Epoch: 1 [5670/36045]\tLoss: 453.4260\n",
      "Training Epoch: 1 [5700/36045]\tLoss: 500.4660\n",
      "Training Epoch: 1 [5730/36045]\tLoss: 456.1789\n",
      "Training Epoch: 1 [5760/36045]\tLoss: 481.3965\n",
      "Training Epoch: 1 [5790/36045]\tLoss: 492.4836\n",
      "Training Epoch: 1 [5820/36045]\tLoss: 488.9148\n",
      "Training Epoch: 1 [5850/36045]\tLoss: 485.9364\n",
      "Training Epoch: 1 [5880/36045]\tLoss: 543.8331\n",
      "Training Epoch: 1 [5910/36045]\tLoss: 562.1533\n",
      "Training Epoch: 1 [5940/36045]\tLoss: 563.1474\n",
      "Training Epoch: 1 [5970/36045]\tLoss: 545.7425\n",
      "Training Epoch: 1 [6000/36045]\tLoss: 561.7643\n",
      "Training Epoch: 1 [6030/36045]\tLoss: 513.3063\n",
      "Training Epoch: 1 [6060/36045]\tLoss: 556.7824\n",
      "Training Epoch: 1 [6090/36045]\tLoss: 543.4517\n",
      "Training Epoch: 1 [6120/36045]\tLoss: 511.2977\n",
      "Training Epoch: 1 [6150/36045]\tLoss: 557.9877\n",
      "Training Epoch: 1 [6180/36045]\tLoss: 551.7556\n",
      "Training Epoch: 1 [6210/36045]\tLoss: 560.3751\n",
      "Training Epoch: 1 [6240/36045]\tLoss: 545.7671\n",
      "Training Epoch: 1 [6270/36045]\tLoss: 617.2703\n",
      "Training Epoch: 1 [6300/36045]\tLoss: 568.6277\n",
      "Training Epoch: 1 [6330/36045]\tLoss: 625.2865\n",
      "Training Epoch: 1 [6360/36045]\tLoss: 619.6352\n",
      "Training Epoch: 1 [6390/36045]\tLoss: 483.3291\n",
      "Training Epoch: 1 [6420/36045]\tLoss: 479.9460\n",
      "Training Epoch: 1 [6450/36045]\tLoss: 445.5325\n",
      "Training Epoch: 1 [6480/36045]\tLoss: 469.9105\n",
      "Training Epoch: 1 [6510/36045]\tLoss: 465.2768\n",
      "Training Epoch: 1 [6540/36045]\tLoss: 484.3029\n",
      "Training Epoch: 1 [6570/36045]\tLoss: 480.9428\n",
      "Training Epoch: 1 [6600/36045]\tLoss: 488.5059\n",
      "Training Epoch: 1 [6630/36045]\tLoss: 552.5675\n",
      "Training Epoch: 1 [6660/36045]\tLoss: 556.9289\n",
      "Training Epoch: 1 [6690/36045]\tLoss: 572.2148\n",
      "Training Epoch: 1 [6720/36045]\tLoss: 541.1859\n",
      "Training Epoch: 1 [6750/36045]\tLoss: 565.8683\n",
      "Training Epoch: 1 [6780/36045]\tLoss: 555.2205\n",
      "Training Epoch: 1 [6810/36045]\tLoss: 553.3841\n",
      "Training Epoch: 1 [6840/36045]\tLoss: 561.2695\n",
      "Training Epoch: 1 [6870/36045]\tLoss: 493.3328\n",
      "Training Epoch: 1 [6900/36045]\tLoss: 473.2693\n",
      "Training Epoch: 1 [6930/36045]\tLoss: 451.7189\n",
      "Training Epoch: 1 [6960/36045]\tLoss: 456.2289\n",
      "Training Epoch: 1 [6990/36045]\tLoss: 481.9778\n",
      "Training Epoch: 1 [7020/36045]\tLoss: 502.6887\n",
      "Training Epoch: 1 [7050/36045]\tLoss: 499.6226\n",
      "Training Epoch: 1 [7080/36045]\tLoss: 479.8536\n",
      "Training Epoch: 1 [7110/36045]\tLoss: 499.7825\n",
      "Training Epoch: 1 [7140/36045]\tLoss: 483.8045\n",
      "Training Epoch: 1 [7170/36045]\tLoss: 503.9942\n",
      "Training Epoch: 1 [7200/36045]\tLoss: 489.5195\n",
      "Training Epoch: 1 [7230/36045]\tLoss: 486.1447\n",
      "Training Epoch: 1 [7260/36045]\tLoss: 491.0904\n",
      "Training Epoch: 1 [7290/36045]\tLoss: 491.2815\n",
      "Training Epoch: 1 [7320/36045]\tLoss: 471.0102\n",
      "Training Epoch: 1 [7350/36045]\tLoss: 478.7707\n",
      "Training Epoch: 1 [7380/36045]\tLoss: 460.2211\n",
      "Training Epoch: 1 [7410/36045]\tLoss: 439.1383\n",
      "Training Epoch: 1 [7440/36045]\tLoss: 452.6579\n",
      "Training Epoch: 1 [7470/36045]\tLoss: 435.3321\n",
      "Training Epoch: 1 [7500/36045]\tLoss: 452.9140\n",
      "Training Epoch: 1 [7530/36045]\tLoss: 425.1179\n",
      "Training Epoch: 1 [7560/36045]\tLoss: 439.3754\n",
      "Training Epoch: 1 [7590/36045]\tLoss: 467.3988\n",
      "Training Epoch: 1 [7620/36045]\tLoss: 512.0772\n",
      "Training Epoch: 1 [7650/36045]\tLoss: 496.2013\n",
      "Training Epoch: 1 [7680/36045]\tLoss: 497.0494\n",
      "Training Epoch: 1 [7710/36045]\tLoss: 467.0522\n",
      "Training Epoch: 1 [7740/36045]\tLoss: 504.9283\n",
      "Training Epoch: 1 [7770/36045]\tLoss: 477.9765\n",
      "Training Epoch: 1 [7800/36045]\tLoss: 477.7493\n",
      "Training Epoch: 1 [7830/36045]\tLoss: 463.2865\n",
      "Training Epoch: 1 [7860/36045]\tLoss: 479.0212\n",
      "Training Epoch: 1 [7890/36045]\tLoss: 480.1985\n",
      "Training Epoch: 1 [7920/36045]\tLoss: 489.2708\n",
      "Training Epoch: 1 [7950/36045]\tLoss: 480.4294\n",
      "Training Epoch: 1 [7980/36045]\tLoss: 514.8008\n",
      "Training Epoch: 1 [8010/36045]\tLoss: 482.5938\n",
      "Training Epoch: 1 [8040/36045]\tLoss: 468.6458\n",
      "Training Epoch: 1 [8070/36045]\tLoss: 492.3357\n",
      "Training Epoch: 1 [8100/36045]\tLoss: 491.0279\n",
      "Training Epoch: 1 [8130/36045]\tLoss: 570.6005\n",
      "Training Epoch: 1 [8160/36045]\tLoss: 574.7983\n",
      "Training Epoch: 1 [8190/36045]\tLoss: 575.3141\n",
      "Training Epoch: 1 [8220/36045]\tLoss: 514.6404\n",
      "Training Epoch: 1 [8250/36045]\tLoss: 541.4224\n",
      "Training Epoch: 1 [8280/36045]\tLoss: 595.4703\n",
      "Training Epoch: 1 [8310/36045]\tLoss: 557.2512\n",
      "Training Epoch: 1 [8340/36045]\tLoss: 543.7637\n",
      "Training Epoch: 1 [8370/36045]\tLoss: 498.7118\n",
      "Training Epoch: 1 [8400/36045]\tLoss: 460.4984\n",
      "Training Epoch: 1 [8430/36045]\tLoss: 445.4881\n",
      "Training Epoch: 1 [8460/36045]\tLoss: 457.3128\n",
      "Training Epoch: 1 [8490/36045]\tLoss: 462.4474\n",
      "Training Epoch: 1 [8520/36045]\tLoss: 513.1074\n",
      "Training Epoch: 1 [8550/36045]\tLoss: 444.2050\n",
      "Training Epoch: 1 [8580/36045]\tLoss: 463.0409\n",
      "Training Epoch: 1 [8610/36045]\tLoss: 458.5438\n",
      "Training Epoch: 1 [8640/36045]\tLoss: 492.6841\n",
      "Training Epoch: 1 [8670/36045]\tLoss: 496.5977\n",
      "Training Epoch: 1 [8700/36045]\tLoss: 519.5255\n",
      "Training Epoch: 1 [8730/36045]\tLoss: 485.8152\n",
      "Training Epoch: 1 [8760/36045]\tLoss: 501.1170\n",
      "Training Epoch: 1 [8790/36045]\tLoss: 521.2429\n",
      "Training Epoch: 1 [8820/36045]\tLoss: 507.4133\n",
      "Training Epoch: 1 [8850/36045]\tLoss: 485.7328\n",
      "Training Epoch: 1 [8880/36045]\tLoss: 438.1104\n",
      "Training Epoch: 1 [8910/36045]\tLoss: 485.3225\n",
      "Training Epoch: 1 [8940/36045]\tLoss: 441.5997\n",
      "Training Epoch: 1 [8970/36045]\tLoss: 492.0371\n",
      "Training Epoch: 1 [9000/36045]\tLoss: 459.7244\n",
      "Training Epoch: 1 [9030/36045]\tLoss: 470.4684\n",
      "Training Epoch: 1 [9060/36045]\tLoss: 490.3301\n",
      "Training Epoch: 1 [9090/36045]\tLoss: 509.7904\n",
      "Training Epoch: 1 [9120/36045]\tLoss: 483.4657\n",
      "Training Epoch: 1 [9150/36045]\tLoss: 282.8550\n",
      "Training Epoch: 1 [9180/36045]\tLoss: 274.2736\n",
      "Training Epoch: 1 [9210/36045]\tLoss: 278.7979\n",
      "Training Epoch: 1 [9240/36045]\tLoss: 288.2724\n",
      "Training Epoch: 1 [9270/36045]\tLoss: 303.9823\n",
      "Training Epoch: 1 [9300/36045]\tLoss: 311.7390\n",
      "Training Epoch: 1 [9330/36045]\tLoss: 293.2756\n",
      "Training Epoch: 1 [9360/36045]\tLoss: 336.6753\n",
      "Training Epoch: 1 [9390/36045]\tLoss: 559.4954\n",
      "Training Epoch: 1 [9420/36045]\tLoss: 580.7754\n",
      "Training Epoch: 1 [9450/36045]\tLoss: 576.7974\n",
      "Training Epoch: 1 [9480/36045]\tLoss: 562.6145\n",
      "Training Epoch: 1 [9510/36045]\tLoss: 595.3522\n",
      "Training Epoch: 1 [9540/36045]\tLoss: 578.5499\n",
      "Training Epoch: 1 [9570/36045]\tLoss: 529.9723\n",
      "Training Epoch: 1 [9600/36045]\tLoss: 451.3997\n",
      "Training Epoch: 1 [9630/36045]\tLoss: 465.8175\n",
      "Training Epoch: 1 [9660/36045]\tLoss: 458.9156\n",
      "Training Epoch: 1 [9690/36045]\tLoss: 449.0465\n",
      "Training Epoch: 1 [9720/36045]\tLoss: 450.4381\n",
      "Training Epoch: 1 [9750/36045]\tLoss: 452.1357\n",
      "Training Epoch: 1 [9780/36045]\tLoss: 550.3864\n",
      "Training Epoch: 1 [9810/36045]\tLoss: 602.2938\n",
      "Training Epoch: 1 [9840/36045]\tLoss: 604.4642\n",
      "Training Epoch: 1 [9870/36045]\tLoss: 614.2708\n",
      "Training Epoch: 1 [9900/36045]\tLoss: 623.1846\n",
      "Training Epoch: 1 [9930/36045]\tLoss: 580.8119\n",
      "Training Epoch: 1 [9960/36045]\tLoss: 617.8436\n",
      "Training Epoch: 1 [9990/36045]\tLoss: 581.1051\n",
      "Training Epoch: 1 [10020/36045]\tLoss: 434.7116\n",
      "Training Epoch: 1 [10050/36045]\tLoss: 471.2535\n",
      "Training Epoch: 1 [10080/36045]\tLoss: 460.6936\n",
      "Training Epoch: 1 [10110/36045]\tLoss: 469.6612\n",
      "Training Epoch: 1 [10140/36045]\tLoss: 478.0609\n",
      "Training Epoch: 1 [10170/36045]\tLoss: 455.6909\n",
      "Training Epoch: 1 [10200/36045]\tLoss: 458.4971\n",
      "Training Epoch: 1 [10230/36045]\tLoss: 569.2610\n",
      "Training Epoch: 1 [10260/36045]\tLoss: 542.2525\n",
      "Training Epoch: 1 [10290/36045]\tLoss: 553.1380\n",
      "Training Epoch: 1 [10320/36045]\tLoss: 563.5016\n",
      "Training Epoch: 1 [10350/36045]\tLoss: 570.1354\n",
      "Training Epoch: 1 [10380/36045]\tLoss: 564.1863\n",
      "Training Epoch: 1 [10410/36045]\tLoss: 568.1404\n",
      "Training Epoch: 1 [10440/36045]\tLoss: 532.5300\n",
      "Training Epoch: 1 [10470/36045]\tLoss: 453.2163\n",
      "Training Epoch: 1 [10500/36045]\tLoss: 433.6763\n",
      "Training Epoch: 1 [10530/36045]\tLoss: 446.6682\n",
      "Training Epoch: 1 [10560/36045]\tLoss: 421.9307\n",
      "Training Epoch: 1 [10590/36045]\tLoss: 461.9292\n",
      "Training Epoch: 1 [10620/36045]\tLoss: 454.4714\n",
      "Training Epoch: 1 [10650/36045]\tLoss: 458.9575\n",
      "Training Epoch: 1 [10680/36045]\tLoss: 495.1350\n",
      "Training Epoch: 1 [10710/36045]\tLoss: 575.4151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [10740/36045]\tLoss: 580.1423\n",
      "Training Epoch: 1 [10770/36045]\tLoss: 550.4819\n",
      "Training Epoch: 1 [10800/36045]\tLoss: 523.7025\n",
      "Training Epoch: 1 [10830/36045]\tLoss: 566.7348\n",
      "Training Epoch: 1 [10860/36045]\tLoss: 569.9726\n",
      "Training Epoch: 1 [10890/36045]\tLoss: 583.8038\n",
      "Training Epoch: 1 [10920/36045]\tLoss: 498.6703\n",
      "Training Epoch: 1 [10950/36045]\tLoss: 415.2055\n",
      "Training Epoch: 1 [10980/36045]\tLoss: 396.3951\n",
      "Training Epoch: 1 [11010/36045]\tLoss: 461.2235\n",
      "Training Epoch: 1 [11040/36045]\tLoss: 461.7989\n",
      "Training Epoch: 1 [11070/36045]\tLoss: 457.2346\n",
      "Training Epoch: 1 [11100/36045]\tLoss: 455.7219\n",
      "Training Epoch: 1 [11130/36045]\tLoss: 499.8760\n",
      "Training Epoch: 1 [11160/36045]\tLoss: 514.7093\n",
      "Training Epoch: 1 [11190/36045]\tLoss: 561.9329\n",
      "Training Epoch: 1 [11220/36045]\tLoss: 534.0110\n",
      "Training Epoch: 1 [11250/36045]\tLoss: 538.6942\n",
      "Training Epoch: 1 [11280/36045]\tLoss: 534.0537\n",
      "Training Epoch: 1 [11310/36045]\tLoss: 538.8496\n",
      "Training Epoch: 1 [11340/36045]\tLoss: 511.6454\n",
      "Training Epoch: 1 [11370/36045]\tLoss: 525.0283\n",
      "Training Epoch: 1 [11400/36045]\tLoss: 477.4865\n",
      "Training Epoch: 1 [11430/36045]\tLoss: 478.5234\n",
      "Training Epoch: 1 [11460/36045]\tLoss: 450.7592\n",
      "Training Epoch: 1 [11490/36045]\tLoss: 458.8319\n",
      "Training Epoch: 1 [11520/36045]\tLoss: 471.9930\n",
      "Training Epoch: 1 [11550/36045]\tLoss: 474.5320\n",
      "Training Epoch: 1 [11580/36045]\tLoss: 489.4040\n",
      "Training Epoch: 1 [11610/36045]\tLoss: 580.0267\n",
      "Training Epoch: 1 [11640/36045]\tLoss: 578.7545\n",
      "Training Epoch: 1 [11670/36045]\tLoss: 601.8212\n",
      "Training Epoch: 1 [11700/36045]\tLoss: 555.9606\n",
      "Training Epoch: 1 [11730/36045]\tLoss: 591.3462\n",
      "Training Epoch: 1 [11760/36045]\tLoss: 609.2925\n",
      "Training Epoch: 1 [11790/36045]\tLoss: 635.5690\n",
      "Training Epoch: 1 [11820/36045]\tLoss: 640.9557\n",
      "Training Epoch: 1 [11850/36045]\tLoss: 731.0898\n",
      "Training Epoch: 1 [11880/36045]\tLoss: 854.3771\n",
      "Training Epoch: 1 [11910/36045]\tLoss: 949.1009\n",
      "Training Epoch: 1 [11940/36045]\tLoss: 904.5499\n",
      "Training Epoch: 1 [11970/36045]\tLoss: 910.0394\n",
      "Training Epoch: 1 [12000/36045]\tLoss: 937.1589\n",
      "Training Epoch: 1 [12030/36045]\tLoss: 864.4637\n",
      "Training Epoch: 1 [12060/36045]\tLoss: 961.6110\n",
      "Training Epoch: 1 [12090/36045]\tLoss: 413.1559\n",
      "Training Epoch: 1 [12120/36045]\tLoss: 403.9998\n",
      "Training Epoch: 1 [12150/36045]\tLoss: 387.5226\n",
      "Training Epoch: 1 [12180/36045]\tLoss: 384.9234\n",
      "Training Epoch: 1 [12210/36045]\tLoss: 397.0069\n",
      "Training Epoch: 1 [12240/36045]\tLoss: 416.4313\n",
      "Training Epoch: 1 [12270/36045]\tLoss: 418.4555\n",
      "Training Epoch: 1 [12300/36045]\tLoss: 580.6924\n",
      "Training Epoch: 1 [12330/36045]\tLoss: 581.4985\n",
      "Training Epoch: 1 [12360/36045]\tLoss: 564.4073\n",
      "Training Epoch: 1 [12390/36045]\tLoss: 592.7275\n",
      "Training Epoch: 1 [12420/36045]\tLoss: 568.3679\n",
      "Training Epoch: 1 [12450/36045]\tLoss: 580.6879\n",
      "Training Epoch: 1 [12480/36045]\tLoss: 593.0876\n",
      "Training Epoch: 1 [12510/36045]\tLoss: 608.3130\n",
      "Training Epoch: 1 [12540/36045]\tLoss: 579.8920\n",
      "Training Epoch: 1 [12570/36045]\tLoss: 501.6304\n",
      "Training Epoch: 1 [12600/36045]\tLoss: 506.1517\n",
      "Training Epoch: 1 [12630/36045]\tLoss: 515.4163\n",
      "Training Epoch: 1 [12660/36045]\tLoss: 502.7600\n",
      "Training Epoch: 1 [12690/36045]\tLoss: 522.5460\n",
      "Training Epoch: 1 [12720/36045]\tLoss: 526.3210\n",
      "Training Epoch: 1 [12750/36045]\tLoss: 528.9673\n",
      "Training Epoch: 1 [12780/36045]\tLoss: 508.5149\n",
      "Training Epoch: 1 [12810/36045]\tLoss: 545.6465\n",
      "Training Epoch: 1 [12840/36045]\tLoss: 549.5707\n",
      "Training Epoch: 1 [12870/36045]\tLoss: 531.6791\n",
      "Training Epoch: 1 [12900/36045]\tLoss: 536.3782\n",
      "Training Epoch: 1 [12930/36045]\tLoss: 528.2169\n",
      "Training Epoch: 1 [12960/36045]\tLoss: 515.5081\n",
      "Training Epoch: 1 [12990/36045]\tLoss: 542.7066\n",
      "Training Epoch: 1 [13020/36045]\tLoss: 501.2909\n",
      "Training Epoch: 1 [13050/36045]\tLoss: 492.7157\n",
      "Training Epoch: 1 [13080/36045]\tLoss: 491.1111\n",
      "Training Epoch: 1 [13110/36045]\tLoss: 498.2962\n",
      "Training Epoch: 1 [13140/36045]\tLoss: 499.9493\n",
      "Training Epoch: 1 [13170/36045]\tLoss: 478.1966\n",
      "Training Epoch: 1 [13200/36045]\tLoss: 471.1832\n",
      "Training Epoch: 1 [13230/36045]\tLoss: 482.6914\n",
      "Training Epoch: 1 [13260/36045]\tLoss: 504.7389\n",
      "Training Epoch: 1 [13290/36045]\tLoss: 544.9066\n",
      "Training Epoch: 1 [13320/36045]\tLoss: 522.1627\n",
      "Training Epoch: 1 [13350/36045]\tLoss: 504.2341\n",
      "Training Epoch: 1 [13380/36045]\tLoss: 520.3916\n",
      "Training Epoch: 1 [13410/36045]\tLoss: 505.6958\n",
      "Training Epoch: 1 [13440/36045]\tLoss: 514.9061\n",
      "Training Epoch: 1 [13470/36045]\tLoss: 514.3663\n",
      "Training Epoch: 1 [13500/36045]\tLoss: 536.9250\n",
      "Training Epoch: 1 [13530/36045]\tLoss: 687.7676\n",
      "Training Epoch: 1 [13560/36045]\tLoss: 643.4536\n",
      "Training Epoch: 1 [13590/36045]\tLoss: 703.9320\n",
      "Training Epoch: 1 [13620/36045]\tLoss: 785.6591\n",
      "Training Epoch: 1 [13650/36045]\tLoss: 797.1008\n",
      "Training Epoch: 1 [13680/36045]\tLoss: 696.8111\n",
      "Training Epoch: 1 [13710/36045]\tLoss: 669.4034\n",
      "Training Epoch: 1 [13740/36045]\tLoss: 490.1418\n",
      "Training Epoch: 1 [13770/36045]\tLoss: 465.9771\n",
      "Training Epoch: 1 [13800/36045]\tLoss: 501.5547\n",
      "Training Epoch: 1 [13830/36045]\tLoss: 466.6473\n",
      "Training Epoch: 1 [13860/36045]\tLoss: 476.3765\n",
      "Training Epoch: 1 [13890/36045]\tLoss: 479.8328\n",
      "Training Epoch: 1 [13920/36045]\tLoss: 482.1689\n",
      "Training Epoch: 1 [13950/36045]\tLoss: 541.5950\n",
      "Training Epoch: 1 [13980/36045]\tLoss: 547.5792\n",
      "Training Epoch: 1 [14010/36045]\tLoss: 550.8503\n",
      "Training Epoch: 1 [14040/36045]\tLoss: 518.0941\n",
      "Training Epoch: 1 [14070/36045]\tLoss: 511.9254\n",
      "Training Epoch: 1 [14100/36045]\tLoss: 525.5973\n",
      "Training Epoch: 1 [14130/36045]\tLoss: 511.6529\n",
      "Training Epoch: 1 [14160/36045]\tLoss: 515.1003\n",
      "Training Epoch: 1 [14190/36045]\tLoss: 527.7115\n",
      "Training Epoch: 1 [14220/36045]\tLoss: 615.9157\n",
      "Training Epoch: 1 [14250/36045]\tLoss: 588.7209\n",
      "Training Epoch: 1 [14280/36045]\tLoss: 588.4669\n",
      "Training Epoch: 1 [14310/36045]\tLoss: 613.4354\n",
      "Training Epoch: 1 [14340/36045]\tLoss: 569.2783\n",
      "Training Epoch: 1 [14370/36045]\tLoss: 568.0706\n",
      "Training Epoch: 1 [14400/36045]\tLoss: 565.3954\n",
      "Training Epoch: 1 [14430/36045]\tLoss: 603.2006\n",
      "Training Epoch: 1 [14460/36045]\tLoss: 575.6651\n",
      "Training Epoch: 1 [14490/36045]\tLoss: 527.3483\n",
      "Training Epoch: 1 [14520/36045]\tLoss: 532.0928\n",
      "Training Epoch: 1 [14550/36045]\tLoss: 569.7654\n",
      "Training Epoch: 1 [14580/36045]\tLoss: 539.6216\n",
      "Training Epoch: 1 [14610/36045]\tLoss: 538.1812\n",
      "Training Epoch: 1 [14640/36045]\tLoss: 552.7380\n",
      "Training Epoch: 1 [14670/36045]\tLoss: 552.8811\n",
      "Training Epoch: 1 [14700/36045]\tLoss: 487.2659\n",
      "Training Epoch: 1 [14730/36045]\tLoss: 432.8926\n",
      "Training Epoch: 1 [14760/36045]\tLoss: 451.3832\n",
      "Training Epoch: 1 [14790/36045]\tLoss: 447.8152\n",
      "Training Epoch: 1 [14820/36045]\tLoss: 439.2183\n",
      "Training Epoch: 1 [14850/36045]\tLoss: 456.4359\n",
      "Training Epoch: 1 [14880/36045]\tLoss: 434.9029\n",
      "Training Epoch: 1 [14910/36045]\tLoss: 444.2687\n",
      "Training Epoch: 1 [14940/36045]\tLoss: 448.6598\n",
      "Training Epoch: 1 [14970/36045]\tLoss: 479.0715\n",
      "Training Epoch: 1 [15000/36045]\tLoss: 443.9168\n",
      "Training Epoch: 1 [15030/36045]\tLoss: 467.4083\n",
      "Training Epoch: 1 [15060/36045]\tLoss: 434.6370\n",
      "Training Epoch: 1 [15090/36045]\tLoss: 436.1001\n",
      "Training Epoch: 1 [15120/36045]\tLoss: 467.8336\n",
      "Training Epoch: 1 [15150/36045]\tLoss: 407.2817\n",
      "Training Epoch: 1 [15180/36045]\tLoss: 410.7620\n",
      "Training Epoch: 1 [15210/36045]\tLoss: 411.1743\n",
      "Training Epoch: 1 [15240/36045]\tLoss: 422.6625\n",
      "Training Epoch: 1 [15270/36045]\tLoss: 404.0006\n",
      "Training Epoch: 1 [15300/36045]\tLoss: 422.8847\n",
      "Training Epoch: 1 [15330/36045]\tLoss: 416.5016\n",
      "Training Epoch: 1 [15360/36045]\tLoss: 418.9157\n",
      "Training Epoch: 1 [15390/36045]\tLoss: 398.8850\n",
      "Training Epoch: 1 [15420/36045]\tLoss: 401.0561\n",
      "Training Epoch: 1 [15450/36045]\tLoss: 385.9489\n",
      "Training Epoch: 1 [15480/36045]\tLoss: 403.8758\n",
      "Training Epoch: 1 [15510/36045]\tLoss: 390.1324\n",
      "Training Epoch: 1 [15540/36045]\tLoss: 397.1252\n",
      "Training Epoch: 1 [15570/36045]\tLoss: 445.7957\n",
      "Training Epoch: 1 [15600/36045]\tLoss: 452.1885\n",
      "Training Epoch: 1 [15630/36045]\tLoss: 489.4109\n",
      "Training Epoch: 1 [15660/36045]\tLoss: 450.1236\n",
      "Training Epoch: 1 [15690/36045]\tLoss: 455.3757\n",
      "Training Epoch: 1 [15720/36045]\tLoss: 477.9223\n",
      "Training Epoch: 1 [15750/36045]\tLoss: 442.4294\n",
      "Training Epoch: 1 [15780/36045]\tLoss: 453.2945\n",
      "Training Epoch: 1 [15810/36045]\tLoss: 460.1901\n",
      "Training Epoch: 1 [15840/36045]\tLoss: 457.8861\n",
      "Training Epoch: 1 [15870/36045]\tLoss: 462.7965\n",
      "Training Epoch: 1 [15900/36045]\tLoss: 487.7089\n",
      "Training Epoch: 1 [15930/36045]\tLoss: 506.9323\n",
      "Training Epoch: 1 [15960/36045]\tLoss: 490.9784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [15990/36045]\tLoss: 453.0525\n",
      "Training Epoch: 1 [16020/36045]\tLoss: 427.8663\n",
      "Training Epoch: 1 [16050/36045]\tLoss: 436.9943\n",
      "Training Epoch: 1 [16080/36045]\tLoss: 405.3495\n",
      "Training Epoch: 1 [16110/36045]\tLoss: 397.5897\n",
      "Training Epoch: 1 [16140/36045]\tLoss: 385.5065\n",
      "Training Epoch: 1 [16170/36045]\tLoss: 453.7315\n",
      "Training Epoch: 1 [16200/36045]\tLoss: 477.3825\n",
      "Training Epoch: 1 [16230/36045]\tLoss: 465.6151\n",
      "Training Epoch: 1 [16260/36045]\tLoss: 569.4929\n",
      "Training Epoch: 1 [16290/36045]\tLoss: 550.5336\n",
      "Training Epoch: 1 [16320/36045]\tLoss: 539.1475\n",
      "Training Epoch: 1 [16350/36045]\tLoss: 579.8715\n",
      "Training Epoch: 1 [16380/36045]\tLoss: 538.3758\n",
      "Training Epoch: 1 [16410/36045]\tLoss: 539.5995\n",
      "Training Epoch: 1 [16440/36045]\tLoss: 527.6057\n",
      "Training Epoch: 1 [16470/36045]\tLoss: 528.7730\n",
      "Training Epoch: 1 [16500/36045]\tLoss: 518.8847\n",
      "Training Epoch: 1 [16530/36045]\tLoss: 485.7346\n",
      "Training Epoch: 1 [16560/36045]\tLoss: 498.8284\n",
      "Training Epoch: 1 [16590/36045]\tLoss: 518.0984\n",
      "Training Epoch: 1 [16620/36045]\tLoss: 511.8159\n",
      "Training Epoch: 1 [16650/36045]\tLoss: 528.3480\n",
      "Training Epoch: 1 [16680/36045]\tLoss: 506.6624\n",
      "Training Epoch: 1 [16710/36045]\tLoss: 509.0365\n",
      "Training Epoch: 1 [16740/36045]\tLoss: 499.6115\n",
      "Training Epoch: 1 [16770/36045]\tLoss: 493.1481\n",
      "Training Epoch: 1 [16800/36045]\tLoss: 505.6906\n",
      "Training Epoch: 1 [16830/36045]\tLoss: 467.4344\n",
      "Training Epoch: 1 [16860/36045]\tLoss: 496.2687\n",
      "Training Epoch: 1 [16890/36045]\tLoss: 471.6573\n",
      "Training Epoch: 1 [16920/36045]\tLoss: 531.4628\n",
      "Training Epoch: 1 [16950/36045]\tLoss: 510.8901\n",
      "Training Epoch: 1 [16980/36045]\tLoss: 510.9986\n",
      "Training Epoch: 1 [17010/36045]\tLoss: 496.4537\n",
      "Training Epoch: 1 [17040/36045]\tLoss: 514.7717\n",
      "Training Epoch: 1 [17070/36045]\tLoss: 526.1467\n",
      "Training Epoch: 1 [17100/36045]\tLoss: 516.9613\n",
      "Training Epoch: 1 [17130/36045]\tLoss: 460.4161\n",
      "Training Epoch: 1 [17160/36045]\tLoss: 414.7224\n",
      "Training Epoch: 1 [17190/36045]\tLoss: 394.3696\n",
      "Training Epoch: 1 [17220/36045]\tLoss: 426.7863\n",
      "Training Epoch: 1 [17250/36045]\tLoss: 434.0426\n",
      "Training Epoch: 1 [17280/36045]\tLoss: 460.6228\n",
      "Training Epoch: 1 [17310/36045]\tLoss: 430.3651\n",
      "Training Epoch: 1 [17340/36045]\tLoss: 452.3496\n",
      "Training Epoch: 1 [17370/36045]\tLoss: 454.0898\n",
      "Training Epoch: 1 [17400/36045]\tLoss: 463.8540\n",
      "Training Epoch: 1 [17430/36045]\tLoss: 481.1854\n",
      "Training Epoch: 1 [17460/36045]\tLoss: 477.6788\n",
      "Training Epoch: 1 [17490/36045]\tLoss: 464.4852\n",
      "Training Epoch: 1 [17520/36045]\tLoss: 454.1843\n",
      "Training Epoch: 1 [17550/36045]\tLoss: 473.8697\n",
      "Training Epoch: 1 [17580/36045]\tLoss: 451.3592\n",
      "Training Epoch: 1 [17610/36045]\tLoss: 462.8495\n",
      "Training Epoch: 1 [17640/36045]\tLoss: 475.7505\n",
      "Training Epoch: 1 [17670/36045]\tLoss: 447.2045\n",
      "Training Epoch: 1 [17700/36045]\tLoss: 453.1503\n",
      "Training Epoch: 1 [17730/36045]\tLoss: 460.7497\n",
      "Training Epoch: 1 [17760/36045]\tLoss: 465.2920\n",
      "Training Epoch: 1 [17790/36045]\tLoss: 453.2314\n",
      "Training Epoch: 1 [17820/36045]\tLoss: 466.2782\n",
      "Training Epoch: 1 [17850/36045]\tLoss: 484.3221\n",
      "Training Epoch: 1 [17880/36045]\tLoss: 529.0522\n",
      "Training Epoch: 1 [17910/36045]\tLoss: 490.7256\n",
      "Training Epoch: 1 [17940/36045]\tLoss: 520.5257\n",
      "Training Epoch: 1 [17970/36045]\tLoss: 530.2936\n",
      "Training Epoch: 1 [18000/36045]\tLoss: 518.2509\n",
      "Training Epoch: 1 [18030/36045]\tLoss: 560.6580\n",
      "Training Epoch: 1 [18060/36045]\tLoss: 555.4402\n",
      "Training Epoch: 1 [18090/36045]\tLoss: 541.1641\n",
      "Training Epoch: 1 [18120/36045]\tLoss: 557.2463\n",
      "Training Epoch: 1 [18150/36045]\tLoss: 584.2049\n",
      "Training Epoch: 1 [18180/36045]\tLoss: 548.6588\n",
      "Training Epoch: 1 [18210/36045]\tLoss: 567.1906\n",
      "Training Epoch: 1 [18240/36045]\tLoss: 590.1767\n",
      "Training Epoch: 1 [18270/36045]\tLoss: 524.8837\n",
      "Training Epoch: 1 [18300/36045]\tLoss: 542.1220\n",
      "Training Epoch: 1 [18330/36045]\tLoss: 613.9775\n",
      "Training Epoch: 1 [18360/36045]\tLoss: 594.3955\n",
      "Training Epoch: 1 [18390/36045]\tLoss: 596.1398\n",
      "Training Epoch: 1 [18420/36045]\tLoss: 557.5215\n",
      "Training Epoch: 1 [18450/36045]\tLoss: 586.6747\n",
      "Training Epoch: 1 [18480/36045]\tLoss: 556.3163\n",
      "Training Epoch: 1 [18510/36045]\tLoss: 576.6439\n",
      "Training Epoch: 1 [18540/36045]\tLoss: 558.1361\n",
      "Training Epoch: 1 [18570/36045]\tLoss: 527.8593\n",
      "Training Epoch: 1 [18600/36045]\tLoss: 555.7729\n",
      "Training Epoch: 1 [18630/36045]\tLoss: 592.1031\n",
      "Training Epoch: 1 [18660/36045]\tLoss: 575.5687\n",
      "Training Epoch: 1 [18690/36045]\tLoss: 641.3326\n",
      "Training Epoch: 1 [18720/36045]\tLoss: 598.0526\n",
      "Training Epoch: 1 [18750/36045]\tLoss: 614.7684\n",
      "Training Epoch: 1 [18780/36045]\tLoss: 649.1407\n",
      "Training Epoch: 1 [18810/36045]\tLoss: 578.7675\n",
      "Training Epoch: 1 [18840/36045]\tLoss: 576.5807\n",
      "Training Epoch: 1 [18870/36045]\tLoss: 599.7072\n",
      "Training Epoch: 1 [18900/36045]\tLoss: 624.9318\n",
      "Training Epoch: 1 [18930/36045]\tLoss: 597.6619\n",
      "Training Epoch: 1 [18960/36045]\tLoss: 483.5679\n",
      "Training Epoch: 1 [18990/36045]\tLoss: 427.9628\n",
      "Training Epoch: 1 [19020/36045]\tLoss: 434.4299\n",
      "Training Epoch: 1 [19050/36045]\tLoss: 419.5855\n",
      "Training Epoch: 1 [19080/36045]\tLoss: 432.6342\n",
      "Training Epoch: 1 [19110/36045]\tLoss: 431.5674\n",
      "Training Epoch: 1 [19140/36045]\tLoss: 415.1399\n",
      "Training Epoch: 1 [19170/36045]\tLoss: 435.9956\n",
      "Training Epoch: 1 [19200/36045]\tLoss: 465.8089\n",
      "Training Epoch: 1 [19230/36045]\tLoss: 473.4427\n",
      "Training Epoch: 1 [19260/36045]\tLoss: 490.7304\n",
      "Training Epoch: 1 [19290/36045]\tLoss: 479.0797\n",
      "Training Epoch: 1 [19320/36045]\tLoss: 469.7352\n",
      "Training Epoch: 1 [19350/36045]\tLoss: 475.7205\n",
      "Training Epoch: 1 [19380/36045]\tLoss: 495.3089\n",
      "Training Epoch: 1 [19410/36045]\tLoss: 479.3477\n",
      "Training Epoch: 1 [19440/36045]\tLoss: 473.8228\n",
      "Training Epoch: 1 [19470/36045]\tLoss: 460.9662\n",
      "Training Epoch: 1 [19500/36045]\tLoss: 489.8543\n",
      "Training Epoch: 1 [19530/36045]\tLoss: 470.7046\n",
      "Training Epoch: 1 [19560/36045]\tLoss: 478.3800\n",
      "Training Epoch: 1 [19590/36045]\tLoss: 488.0280\n",
      "Training Epoch: 1 [19620/36045]\tLoss: 694.6564\n",
      "Training Epoch: 1 [19650/36045]\tLoss: 704.3220\n",
      "Training Epoch: 1 [19680/36045]\tLoss: 661.9655\n",
      "Training Epoch: 1 [19710/36045]\tLoss: 656.4831\n",
      "Training Epoch: 1 [19740/36045]\tLoss: 677.3598\n",
      "Training Epoch: 1 [19770/36045]\tLoss: 699.9000\n",
      "Training Epoch: 1 [19800/36045]\tLoss: 669.2111\n",
      "Training Epoch: 1 [19830/36045]\tLoss: 418.1794\n",
      "Training Epoch: 1 [19860/36045]\tLoss: 424.1148\n",
      "Training Epoch: 1 [19890/36045]\tLoss: 418.2768\n",
      "Training Epoch: 1 [19920/36045]\tLoss: 418.4469\n",
      "Training Epoch: 1 [19950/36045]\tLoss: 392.9659\n",
      "Training Epoch: 1 [19980/36045]\tLoss: 408.7757\n",
      "Training Epoch: 1 [20010/36045]\tLoss: 425.9116\n",
      "Training Epoch: 1 [20040/36045]\tLoss: 459.8289\n",
      "Training Epoch: 1 [20070/36045]\tLoss: 456.9649\n",
      "Training Epoch: 1 [20100/36045]\tLoss: 469.4268\n",
      "Training Epoch: 1 [20130/36045]\tLoss: 468.1964\n",
      "Training Epoch: 1 [20160/36045]\tLoss: 456.3625\n",
      "Training Epoch: 1 [20190/36045]\tLoss: 463.1465\n",
      "Training Epoch: 1 [20220/36045]\tLoss: 455.3690\n",
      "Training Epoch: 1 [20250/36045]\tLoss: 534.0887\n",
      "Training Epoch: 1 [20280/36045]\tLoss: 549.2833\n",
      "Training Epoch: 1 [20310/36045]\tLoss: 528.3546\n",
      "Training Epoch: 1 [20340/36045]\tLoss: 550.5609\n",
      "Training Epoch: 1 [20370/36045]\tLoss: 564.4906\n",
      "Training Epoch: 1 [20400/36045]\tLoss: 579.1262\n",
      "Training Epoch: 1 [20430/36045]\tLoss: 530.8040\n",
      "Training Epoch: 1 [20460/36045]\tLoss: 534.2733\n",
      "Training Epoch: 1 [20490/36045]\tLoss: 532.4745\n",
      "Training Epoch: 1 [20520/36045]\tLoss: 480.6562\n",
      "Training Epoch: 1 [20550/36045]\tLoss: 451.0021\n",
      "Training Epoch: 1 [20580/36045]\tLoss: 466.6540\n",
      "Training Epoch: 1 [20610/36045]\tLoss: 456.6559\n",
      "Training Epoch: 1 [20640/36045]\tLoss: 446.4405\n",
      "Training Epoch: 1 [20670/36045]\tLoss: 441.3055\n",
      "Training Epoch: 1 [20700/36045]\tLoss: 455.4749\n",
      "Training Epoch: 1 [20730/36045]\tLoss: 459.7075\n",
      "Training Epoch: 1 [20760/36045]\tLoss: 520.8956\n",
      "Training Epoch: 1 [20790/36045]\tLoss: 519.3536\n",
      "Training Epoch: 1 [20820/36045]\tLoss: 510.0381\n",
      "Training Epoch: 1 [20850/36045]\tLoss: 512.4280\n",
      "Training Epoch: 1 [20880/36045]\tLoss: 545.0633\n",
      "Training Epoch: 1 [20910/36045]\tLoss: 536.7163\n",
      "Training Epoch: 1 [20940/36045]\tLoss: 514.9330\n",
      "Training Epoch: 1 [20970/36045]\tLoss: 518.9495\n",
      "Training Epoch: 1 [21000/36045]\tLoss: 474.1842\n",
      "Training Epoch: 1 [21030/36045]\tLoss: 423.1275\n",
      "Training Epoch: 1 [21060/36045]\tLoss: 410.4432\n",
      "Training Epoch: 1 [21090/36045]\tLoss: 432.2722\n",
      "Training Epoch: 1 [21120/36045]\tLoss: 442.1586\n",
      "Training Epoch: 1 [21150/36045]\tLoss: 457.2507\n",
      "Training Epoch: 1 [21180/36045]\tLoss: 459.5563\n",
      "Training Epoch: 1 [21210/36045]\tLoss: 450.5760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [21240/36045]\tLoss: 430.5455\n",
      "Training Epoch: 1 [21270/36045]\tLoss: 468.0368\n",
      "Training Epoch: 1 [21300/36045]\tLoss: 504.3957\n",
      "Training Epoch: 1 [21330/36045]\tLoss: 490.6345\n",
      "Training Epoch: 1 [21360/36045]\tLoss: 498.6601\n",
      "Training Epoch: 1 [21390/36045]\tLoss: 500.2559\n",
      "Training Epoch: 1 [21420/36045]\tLoss: 503.0526\n",
      "Training Epoch: 1 [21450/36045]\tLoss: 496.1414\n",
      "Training Epoch: 1 [21480/36045]\tLoss: 495.3603\n",
      "Training Epoch: 1 [21510/36045]\tLoss: 542.2305\n",
      "Training Epoch: 1 [21540/36045]\tLoss: 596.7069\n",
      "Training Epoch: 1 [21570/36045]\tLoss: 614.3982\n",
      "Training Epoch: 1 [21600/36045]\tLoss: 591.5840\n",
      "Training Epoch: 1 [21630/36045]\tLoss: 623.2941\n",
      "Training Epoch: 1 [21660/36045]\tLoss: 590.6331\n",
      "Training Epoch: 1 [21690/36045]\tLoss: 637.8439\n",
      "Training Epoch: 1 [21720/36045]\tLoss: 567.3512\n",
      "Training Epoch: 1 [21750/36045]\tLoss: 601.4503\n",
      "Training Epoch: 1 [21780/36045]\tLoss: 421.8167\n",
      "Training Epoch: 1 [21810/36045]\tLoss: 423.8259\n",
      "Training Epoch: 1 [21840/36045]\tLoss: 421.2581\n",
      "Training Epoch: 1 [21870/36045]\tLoss: 427.5162\n",
      "Training Epoch: 1 [21900/36045]\tLoss: 403.4455\n",
      "Training Epoch: 1 [21930/36045]\tLoss: 418.1073\n",
      "Training Epoch: 1 [21960/36045]\tLoss: 424.3679\n",
      "Training Epoch: 1 [21990/36045]\tLoss: 420.1716\n",
      "Training Epoch: 1 [22020/36045]\tLoss: 438.5712\n",
      "Training Epoch: 1 [22050/36045]\tLoss: 443.9106\n",
      "Training Epoch: 1 [22080/36045]\tLoss: 436.5171\n",
      "Training Epoch: 1 [22110/36045]\tLoss: 425.1272\n",
      "Training Epoch: 1 [22140/36045]\tLoss: 422.9466\n",
      "Training Epoch: 1 [22170/36045]\tLoss: 421.6266\n",
      "Training Epoch: 1 [22200/36045]\tLoss: 444.7999\n",
      "Training Epoch: 1 [22230/36045]\tLoss: 446.2700\n",
      "Training Epoch: 1 [22260/36045]\tLoss: 433.1728\n",
      "Training Epoch: 1 [22290/36045]\tLoss: 479.7698\n",
      "Training Epoch: 1 [22320/36045]\tLoss: 525.8617\n",
      "Training Epoch: 1 [22350/36045]\tLoss: 525.5735\n",
      "Training Epoch: 1 [22380/36045]\tLoss: 528.2681\n",
      "Training Epoch: 1 [22410/36045]\tLoss: 512.0609\n",
      "Training Epoch: 1 [22440/36045]\tLoss: 506.4223\n",
      "Training Epoch: 1 [22470/36045]\tLoss: 524.8357\n",
      "Training Epoch: 1 [22500/36045]\tLoss: 501.9515\n",
      "Training Epoch: 1 [22530/36045]\tLoss: 536.8162\n",
      "Training Epoch: 1 [22560/36045]\tLoss: 529.1362\n",
      "Training Epoch: 1 [22590/36045]\tLoss: 560.7503\n",
      "Training Epoch: 1 [22620/36045]\tLoss: 597.7822\n",
      "Training Epoch: 1 [22650/36045]\tLoss: 603.4509\n",
      "Training Epoch: 1 [22680/36045]\tLoss: 609.0837\n",
      "Training Epoch: 1 [22710/36045]\tLoss: 624.6800\n",
      "Training Epoch: 1 [22740/36045]\tLoss: 642.3051\n",
      "Training Epoch: 1 [22770/36045]\tLoss: 643.6662\n",
      "Training Epoch: 1 [22800/36045]\tLoss: 663.6970\n",
      "Training Epoch: 1 [22830/36045]\tLoss: 536.4122\n",
      "Training Epoch: 1 [22860/36045]\tLoss: 526.1847\n",
      "Training Epoch: 1 [22890/36045]\tLoss: 546.3400\n",
      "Training Epoch: 1 [22920/36045]\tLoss: 541.6494\n",
      "Training Epoch: 1 [22950/36045]\tLoss: 503.7583\n",
      "Training Epoch: 1 [22980/36045]\tLoss: 508.7534\n",
      "Training Epoch: 1 [23010/36045]\tLoss: 513.7329\n",
      "Training Epoch: 1 [23040/36045]\tLoss: 464.2400\n",
      "Training Epoch: 1 [23070/36045]\tLoss: 457.1006\n",
      "Training Epoch: 1 [23100/36045]\tLoss: 489.1833\n",
      "Training Epoch: 1 [23130/36045]\tLoss: 480.2489\n",
      "Training Epoch: 1 [23160/36045]\tLoss: 446.4465\n",
      "Training Epoch: 1 [23190/36045]\tLoss: 433.0860\n",
      "Training Epoch: 1 [23220/36045]\tLoss: 471.6686\n",
      "Training Epoch: 1 [23250/36045]\tLoss: 432.0877\n",
      "Training Epoch: 1 [23280/36045]\tLoss: 441.5171\n",
      "Training Epoch: 1 [23310/36045]\tLoss: 435.5064\n",
      "Training Epoch: 1 [23340/36045]\tLoss: 462.5942\n",
      "Training Epoch: 1 [23370/36045]\tLoss: 476.0807\n",
      "Training Epoch: 1 [23400/36045]\tLoss: 501.7539\n",
      "Training Epoch: 1 [23430/36045]\tLoss: 471.7578\n",
      "Training Epoch: 1 [23460/36045]\tLoss: 505.0955\n",
      "Training Epoch: 1 [23490/36045]\tLoss: 461.1522\n",
      "Training Epoch: 1 [23520/36045]\tLoss: 481.9933\n",
      "Training Epoch: 1 [23550/36045]\tLoss: 521.2995\n",
      "Training Epoch: 1 [23580/36045]\tLoss: 586.6583\n",
      "Training Epoch: 1 [23610/36045]\tLoss: 571.4912\n",
      "Training Epoch: 1 [23640/36045]\tLoss: 603.7846\n",
      "Training Epoch: 1 [23670/36045]\tLoss: 560.0394\n",
      "Training Epoch: 1 [23700/36045]\tLoss: 604.4315\n",
      "Training Epoch: 1 [23730/36045]\tLoss: 572.8966\n",
      "Training Epoch: 1 [23760/36045]\tLoss: 525.8948\n",
      "Training Epoch: 1 [23790/36045]\tLoss: 461.8929\n",
      "Training Epoch: 1 [23820/36045]\tLoss: 492.9218\n",
      "Training Epoch: 1 [23850/36045]\tLoss: 485.8005\n",
      "Training Epoch: 1 [23880/36045]\tLoss: 473.2422\n",
      "Training Epoch: 1 [23910/36045]\tLoss: 503.3368\n",
      "Training Epoch: 1 [23940/36045]\tLoss: 437.4884\n",
      "Training Epoch: 1 [23970/36045]\tLoss: 448.6981\n",
      "Training Epoch: 1 [24000/36045]\tLoss: 449.8342\n",
      "Training Epoch: 1 [24030/36045]\tLoss: 410.2217\n",
      "Training Epoch: 1 [24060/36045]\tLoss: 407.5201\n",
      "Training Epoch: 1 [24090/36045]\tLoss: 434.4696\n",
      "Training Epoch: 1 [24120/36045]\tLoss: 436.1120\n",
      "Training Epoch: 1 [24150/36045]\tLoss: 420.8578\n",
      "Training Epoch: 1 [24180/36045]\tLoss: 422.2709\n",
      "Training Epoch: 1 [24210/36045]\tLoss: 411.7845\n",
      "Training Epoch: 1 [24240/36045]\tLoss: 419.5380\n",
      "Training Epoch: 1 [24270/36045]\tLoss: 415.4160\n",
      "Training Epoch: 1 [24300/36045]\tLoss: 457.4317\n",
      "Training Epoch: 1 [24330/36045]\tLoss: 461.9161\n",
      "Training Epoch: 1 [24360/36045]\tLoss: 449.4589\n",
      "Training Epoch: 1 [24390/36045]\tLoss: 458.2900\n",
      "Training Epoch: 1 [24420/36045]\tLoss: 457.6414\n",
      "Training Epoch: 1 [24450/36045]\tLoss: 438.8549\n",
      "Training Epoch: 1 [24480/36045]\tLoss: 473.3034\n",
      "Training Epoch: 1 [24510/36045]\tLoss: 483.3734\n",
      "Training Epoch: 1 [24540/36045]\tLoss: 572.8210\n",
      "Training Epoch: 1 [24570/36045]\tLoss: 516.2917\n",
      "Training Epoch: 1 [24600/36045]\tLoss: 557.6929\n",
      "Training Epoch: 1 [24630/36045]\tLoss: 509.2451\n",
      "Training Epoch: 1 [24660/36045]\tLoss: 528.0070\n",
      "Training Epoch: 1 [24690/36045]\tLoss: 524.8342\n",
      "Training Epoch: 1 [24720/36045]\tLoss: 533.0121\n",
      "Training Epoch: 1 [24750/36045]\tLoss: 456.5130\n",
      "Training Epoch: 1 [24780/36045]\tLoss: 374.4403\n",
      "Training Epoch: 1 [24810/36045]\tLoss: 411.6312\n",
      "Training Epoch: 1 [24840/36045]\tLoss: 395.0695\n",
      "Training Epoch: 1 [24870/36045]\tLoss: 397.3243\n",
      "Training Epoch: 1 [24900/36045]\tLoss: 399.8158\n",
      "Training Epoch: 1 [24930/36045]\tLoss: 401.2156\n",
      "Training Epoch: 1 [24960/36045]\tLoss: 395.1895\n",
      "Training Epoch: 1 [24990/36045]\tLoss: 400.5296\n",
      "Training Epoch: 1 [25020/36045]\tLoss: 376.3703\n",
      "Training Epoch: 1 [25050/36045]\tLoss: 355.1123\n",
      "Training Epoch: 1 [25080/36045]\tLoss: 342.9297\n",
      "Training Epoch: 1 [25110/36045]\tLoss: 320.1976\n",
      "Training Epoch: 1 [25140/36045]\tLoss: 297.2974\n",
      "Training Epoch: 1 [25170/36045]\tLoss: 306.0486\n",
      "Training Epoch: 1 [25200/36045]\tLoss: 306.4026\n",
      "Training Epoch: 1 [25230/36045]\tLoss: 296.9057\n",
      "Training Epoch: 1 [25260/36045]\tLoss: 390.8228\n",
      "Training Epoch: 1 [25290/36045]\tLoss: 432.5071\n",
      "Training Epoch: 1 [25320/36045]\tLoss: 435.8414\n",
      "Training Epoch: 1 [25350/36045]\tLoss: 419.8232\n",
      "Training Epoch: 1 [25380/36045]\tLoss: 405.9421\n",
      "Training Epoch: 1 [25410/36045]\tLoss: 392.5156\n",
      "Training Epoch: 1 [25440/36045]\tLoss: 392.5748\n",
      "Training Epoch: 1 [25470/36045]\tLoss: 448.4602\n",
      "Training Epoch: 1 [25500/36045]\tLoss: 429.2493\n",
      "Training Epoch: 1 [25530/36045]\tLoss: 512.0336\n",
      "Training Epoch: 1 [25560/36045]\tLoss: 495.8225\n",
      "Training Epoch: 1 [25590/36045]\tLoss: 509.7002\n",
      "Training Epoch: 1 [25620/36045]\tLoss: 493.9846\n",
      "Training Epoch: 1 [25650/36045]\tLoss: 495.0392\n",
      "Training Epoch: 1 [25680/36045]\tLoss: 495.1399\n",
      "Training Epoch: 1 [25710/36045]\tLoss: 508.7263\n",
      "Training Epoch: 1 [25740/36045]\tLoss: 507.8750\n",
      "Training Epoch: 1 [25770/36045]\tLoss: 330.8282\n",
      "Training Epoch: 1 [25800/36045]\tLoss: 307.7709\n",
      "Training Epoch: 1 [25830/36045]\tLoss: 317.8911\n",
      "Training Epoch: 1 [25860/36045]\tLoss: 311.4406\n",
      "Training Epoch: 1 [25890/36045]\tLoss: 290.8075\n",
      "Training Epoch: 1 [25920/36045]\tLoss: 322.3152\n",
      "Training Epoch: 1 [25950/36045]\tLoss: 313.5677\n",
      "Training Epoch: 1 [25980/36045]\tLoss: 300.7168\n",
      "Training Epoch: 1 [26010/36045]\tLoss: 504.1537\n",
      "Training Epoch: 1 [26040/36045]\tLoss: 522.8378\n",
      "Training Epoch: 1 [26070/36045]\tLoss: 507.1500\n",
      "Training Epoch: 1 [26100/36045]\tLoss: 544.4513\n",
      "Training Epoch: 1 [26130/36045]\tLoss: 508.1180\n",
      "Training Epoch: 1 [26160/36045]\tLoss: 562.5424\n",
      "Training Epoch: 1 [26190/36045]\tLoss: 506.3980\n",
      "Training Epoch: 1 [26220/36045]\tLoss: 532.5911\n",
      "Training Epoch: 1 [26250/36045]\tLoss: 534.1785\n",
      "Training Epoch: 1 [26280/36045]\tLoss: 509.2401\n",
      "Training Epoch: 1 [26310/36045]\tLoss: 510.0906\n",
      "Training Epoch: 1 [26340/36045]\tLoss: 535.5560\n",
      "Training Epoch: 1 [26370/36045]\tLoss: 533.8682\n",
      "Training Epoch: 1 [26400/36045]\tLoss: 490.1774\n",
      "Training Epoch: 1 [26430/36045]\tLoss: 430.9240\n",
      "Training Epoch: 1 [26460/36045]\tLoss: 423.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [26490/36045]\tLoss: 538.5019\n",
      "Training Epoch: 1 [26520/36045]\tLoss: 509.5439\n",
      "Training Epoch: 1 [26550/36045]\tLoss: 503.6223\n",
      "Training Epoch: 1 [26580/36045]\tLoss: 498.3443\n",
      "Training Epoch: 1 [26610/36045]\tLoss: 506.3431\n",
      "Training Epoch: 1 [26640/36045]\tLoss: 504.1042\n",
      "Training Epoch: 1 [26670/36045]\tLoss: 503.3051\n",
      "Training Epoch: 1 [26700/36045]\tLoss: 497.2494\n",
      "Training Epoch: 1 [26730/36045]\tLoss: 506.4750\n",
      "Training Epoch: 1 [26760/36045]\tLoss: 375.8683\n",
      "Training Epoch: 1 [26790/36045]\tLoss: 347.4253\n",
      "Training Epoch: 1 [26820/36045]\tLoss: 332.1331\n",
      "Training Epoch: 1 [26850/36045]\tLoss: 242.6212\n",
      "Training Epoch: 1 [26880/36045]\tLoss: 277.5939\n",
      "Training Epoch: 1 [26910/36045]\tLoss: 290.7234\n",
      "Training Epoch: 1 [26940/36045]\tLoss: 309.6776\n",
      "Training Epoch: 1 [26970/36045]\tLoss: 421.6880\n",
      "Training Epoch: 1 [27000/36045]\tLoss: 564.4392\n",
      "Training Epoch: 1 [27030/36045]\tLoss: 547.9296\n",
      "Training Epoch: 1 [27060/36045]\tLoss: 540.1994\n",
      "Training Epoch: 1 [27090/36045]\tLoss: 526.6652\n",
      "Training Epoch: 1 [27120/36045]\tLoss: 541.2632\n",
      "Training Epoch: 1 [27150/36045]\tLoss: 591.3142\n",
      "Training Epoch: 1 [27180/36045]\tLoss: 430.2057\n",
      "Training Epoch: 1 [27210/36045]\tLoss: 365.7891\n",
      "Training Epoch: 1 [27240/36045]\tLoss: 396.4802\n",
      "Training Epoch: 1 [27270/36045]\tLoss: 385.1299\n",
      "Training Epoch: 1 [27300/36045]\tLoss: 369.8333\n",
      "Training Epoch: 1 [27330/36045]\tLoss: 375.8127\n",
      "Training Epoch: 1 [27360/36045]\tLoss: 373.4837\n",
      "Training Epoch: 1 [27390/36045]\tLoss: 374.5562\n",
      "Training Epoch: 1 [27420/36045]\tLoss: 411.2942\n",
      "Training Epoch: 1 [27450/36045]\tLoss: 505.9724\n",
      "Training Epoch: 1 [27480/36045]\tLoss: 507.1331\n",
      "Training Epoch: 1 [27510/36045]\tLoss: 515.9379\n",
      "Training Epoch: 1 [27540/36045]\tLoss: 501.4317\n",
      "Training Epoch: 1 [27570/36045]\tLoss: 525.2927\n",
      "Training Epoch: 1 [27600/36045]\tLoss: 508.0364\n",
      "Training Epoch: 1 [27630/36045]\tLoss: 513.3911\n",
      "Training Epoch: 1 [27660/36045]\tLoss: 511.6302\n",
      "Training Epoch: 1 [27690/36045]\tLoss: 516.4633\n",
      "Training Epoch: 1 [27720/36045]\tLoss: 555.0216\n",
      "Training Epoch: 1 [27750/36045]\tLoss: 529.9713\n",
      "Training Epoch: 1 [27780/36045]\tLoss: 525.1744\n",
      "Training Epoch: 1 [27810/36045]\tLoss: 524.9473\n",
      "Training Epoch: 1 [27840/36045]\tLoss: 519.5432\n",
      "Training Epoch: 1 [27870/36045]\tLoss: 528.6547\n",
      "Training Epoch: 1 [27900/36045]\tLoss: 452.9337\n",
      "Training Epoch: 1 [27930/36045]\tLoss: 429.1451\n",
      "Training Epoch: 1 [27960/36045]\tLoss: 398.5272\n",
      "Training Epoch: 1 [27990/36045]\tLoss: 397.3003\n",
      "Training Epoch: 1 [28020/36045]\tLoss: 389.5378\n",
      "Training Epoch: 1 [28050/36045]\tLoss: 407.2725\n",
      "Training Epoch: 1 [28080/36045]\tLoss: 399.0344\n",
      "Training Epoch: 1 [28110/36045]\tLoss: 383.8029\n",
      "Training Epoch: 1 [28140/36045]\tLoss: 418.7573\n",
      "Training Epoch: 1 [28170/36045]\tLoss: 408.9533\n",
      "Training Epoch: 1 [28200/36045]\tLoss: 410.0973\n",
      "Training Epoch: 1 [28230/36045]\tLoss: 390.8785\n",
      "Training Epoch: 1 [28260/36045]\tLoss: 415.1697\n",
      "Training Epoch: 1 [28290/36045]\tLoss: 382.0366\n",
      "Training Epoch: 1 [28320/36045]\tLoss: 370.0839\n",
      "Training Epoch: 1 [28350/36045]\tLoss: 390.8149\n",
      "Training Epoch: 1 [28380/36045]\tLoss: 675.9474\n",
      "Training Epoch: 1 [28410/36045]\tLoss: 742.2670\n",
      "Training Epoch: 1 [28440/36045]\tLoss: 674.8893\n",
      "Training Epoch: 1 [28470/36045]\tLoss: 525.3612\n",
      "Training Epoch: 1 [28500/36045]\tLoss: 579.7986\n",
      "Training Epoch: 1 [28530/36045]\tLoss: 579.7549\n",
      "Training Epoch: 1 [28560/36045]\tLoss: 446.9292\n",
      "Training Epoch: 1 [28590/36045]\tLoss: 535.2670\n",
      "Training Epoch: 1 [28620/36045]\tLoss: 559.7317\n",
      "Training Epoch: 1 [28650/36045]\tLoss: 550.8625\n",
      "Training Epoch: 1 [28680/36045]\tLoss: 543.5969\n",
      "Training Epoch: 1 [28710/36045]\tLoss: 552.5185\n",
      "Training Epoch: 1 [28740/36045]\tLoss: 542.6958\n",
      "Training Epoch: 1 [28770/36045]\tLoss: 554.5132\n",
      "Training Epoch: 1 [28800/36045]\tLoss: 550.9087\n",
      "Training Epoch: 1 [28830/36045]\tLoss: 519.1842\n",
      "Training Epoch: 1 [28860/36045]\tLoss: 409.5218\n",
      "Training Epoch: 1 [28890/36045]\tLoss: 392.9830\n",
      "Training Epoch: 1 [28920/36045]\tLoss: 375.2048\n",
      "Training Epoch: 1 [28950/36045]\tLoss: 421.1188\n",
      "Training Epoch: 1 [28980/36045]\tLoss: 395.0585\n",
      "Training Epoch: 1 [29010/36045]\tLoss: 388.7151\n",
      "Training Epoch: 1 [29040/36045]\tLoss: 389.7950\n",
      "Training Epoch: 1 [29070/36045]\tLoss: 413.2491\n",
      "Training Epoch: 1 [29100/36045]\tLoss: 414.1396\n",
      "Training Epoch: 1 [29130/36045]\tLoss: 400.3451\n",
      "Training Epoch: 1 [29160/36045]\tLoss: 400.3431\n",
      "Training Epoch: 1 [29190/36045]\tLoss: 413.9254\n",
      "Training Epoch: 1 [29220/36045]\tLoss: 367.4077\n",
      "Training Epoch: 1 [29250/36045]\tLoss: 390.2439\n",
      "Training Epoch: 1 [29280/36045]\tLoss: 350.2455\n",
      "Training Epoch: 1 [29310/36045]\tLoss: 525.9608\n",
      "Training Epoch: 1 [29340/36045]\tLoss: 488.6437\n",
      "Training Epoch: 1 [29370/36045]\tLoss: 467.5584\n",
      "Training Epoch: 1 [29400/36045]\tLoss: 500.4596\n",
      "Training Epoch: 1 [29430/36045]\tLoss: 518.0146\n",
      "Training Epoch: 1 [29460/36045]\tLoss: 499.7492\n",
      "Training Epoch: 1 [29490/36045]\tLoss: 526.6475\n",
      "Training Epoch: 1 [29520/36045]\tLoss: 506.6952\n",
      "Training Epoch: 1 [29550/36045]\tLoss: 495.2029\n",
      "Training Epoch: 1 [29580/36045]\tLoss: 422.3450\n",
      "Training Epoch: 1 [29610/36045]\tLoss: 423.0065\n",
      "Training Epoch: 1 [29640/36045]\tLoss: 400.7506\n",
      "Training Epoch: 1 [29670/36045]\tLoss: 373.8497\n",
      "Training Epoch: 1 [29700/36045]\tLoss: 360.7924\n",
      "Training Epoch: 1 [29730/36045]\tLoss: 342.8225\n",
      "Training Epoch: 1 [29760/36045]\tLoss: 373.1936\n",
      "Training Epoch: 1 [29790/36045]\tLoss: 390.8690\n",
      "Training Epoch: 1 [29820/36045]\tLoss: 502.5681\n",
      "Training Epoch: 1 [29850/36045]\tLoss: 492.3831\n",
      "Training Epoch: 1 [29880/36045]\tLoss: 490.4802\n",
      "Training Epoch: 1 [29910/36045]\tLoss: 466.6493\n",
      "Training Epoch: 1 [29940/36045]\tLoss: 531.6464\n",
      "Training Epoch: 1 [29970/36045]\tLoss: 509.0981\n",
      "Training Epoch: 1 [30000/36045]\tLoss: 468.5808\n",
      "Training Epoch: 1 [30030/36045]\tLoss: 466.9530\n",
      "Training Epoch: 1 [30060/36045]\tLoss: 544.7748\n",
      "Training Epoch: 1 [30090/36045]\tLoss: 583.8346\n",
      "Training Epoch: 1 [30120/36045]\tLoss: 591.1191\n",
      "Training Epoch: 1 [30150/36045]\tLoss: 576.4515\n",
      "Training Epoch: 1 [30180/36045]\tLoss: 551.4709\n",
      "Training Epoch: 1 [30210/36045]\tLoss: 552.5245\n",
      "Training Epoch: 1 [30240/36045]\tLoss: 602.9439\n",
      "Training Epoch: 1 [30270/36045]\tLoss: 559.1579\n",
      "Training Epoch: 1 [30300/36045]\tLoss: 574.8454\n",
      "Training Epoch: 1 [30330/36045]\tLoss: 406.5825\n",
      "Training Epoch: 1 [30360/36045]\tLoss: 401.8351\n",
      "Training Epoch: 1 [30390/36045]\tLoss: 412.9944\n",
      "Training Epoch: 1 [30420/36045]\tLoss: 392.5779\n",
      "Training Epoch: 1 [30450/36045]\tLoss: 403.4848\n",
      "Training Epoch: 1 [30480/36045]\tLoss: 393.5315\n",
      "Training Epoch: 1 [30510/36045]\tLoss: 333.2124\n",
      "Training Epoch: 1 [30540/36045]\tLoss: 350.2667\n",
      "Training Epoch: 1 [30570/36045]\tLoss: 364.8032\n",
      "Training Epoch: 1 [30600/36045]\tLoss: 336.0749\n",
      "Training Epoch: 1 [30630/36045]\tLoss: 334.4751\n",
      "Training Epoch: 1 [30660/36045]\tLoss: 351.4269\n",
      "Training Epoch: 1 [30690/36045]\tLoss: 354.8477\n",
      "Training Epoch: 1 [30720/36045]\tLoss: 346.9571\n",
      "Training Epoch: 1 [30750/36045]\tLoss: 333.9417\n",
      "Training Epoch: 1 [30780/36045]\tLoss: 370.3432\n",
      "Training Epoch: 1 [30810/36045]\tLoss: 377.3571\n",
      "Training Epoch: 1 [30840/36045]\tLoss: 362.8119\n",
      "Training Epoch: 1 [30870/36045]\tLoss: 367.3482\n",
      "Training Epoch: 1 [30900/36045]\tLoss: 371.7665\n",
      "Training Epoch: 1 [30930/36045]\tLoss: 377.1672\n",
      "Training Epoch: 1 [30960/36045]\tLoss: 418.5317\n",
      "Training Epoch: 1 [30990/36045]\tLoss: 395.4459\n",
      "Training Epoch: 1 [31020/36045]\tLoss: 320.0814\n",
      "Training Epoch: 1 [31050/36045]\tLoss: 324.6882\n",
      "Training Epoch: 1 [31080/36045]\tLoss: 317.0703\n",
      "Training Epoch: 1 [31110/36045]\tLoss: 310.3578\n",
      "Training Epoch: 1 [31140/36045]\tLoss: 320.7977\n",
      "Training Epoch: 1 [31170/36045]\tLoss: 330.3671\n",
      "Training Epoch: 1 [31200/36045]\tLoss: 441.2159\n",
      "Training Epoch: 1 [31230/36045]\tLoss: 509.4805\n",
      "Training Epoch: 1 [31260/36045]\tLoss: 508.4058\n",
      "Training Epoch: 1 [31290/36045]\tLoss: 494.5109\n",
      "Training Epoch: 1 [31320/36045]\tLoss: 483.3280\n",
      "Training Epoch: 1 [31350/36045]\tLoss: 514.3683\n",
      "Training Epoch: 1 [31380/36045]\tLoss: 486.0176\n",
      "Training Epoch: 1 [31410/36045]\tLoss: 466.8144\n",
      "Training Epoch: 1 [31440/36045]\tLoss: 514.5280\n",
      "Training Epoch: 1 [31470/36045]\tLoss: 516.7855\n",
      "Training Epoch: 1 [31500/36045]\tLoss: 495.4018\n",
      "Training Epoch: 1 [31530/36045]\tLoss: 522.5576\n",
      "Training Epoch: 1 [31560/36045]\tLoss: 494.2478\n",
      "Training Epoch: 1 [31590/36045]\tLoss: 481.7358\n",
      "Training Epoch: 1 [31620/36045]\tLoss: 498.1429\n",
      "Training Epoch: 1 [31650/36045]\tLoss: 513.3931\n",
      "Training Epoch: 1 [31680/36045]\tLoss: 413.2338\n",
      "Training Epoch: 1 [31710/36045]\tLoss: 322.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [31740/36045]\tLoss: 306.6272\n",
      "Training Epoch: 1 [31770/36045]\tLoss: 300.3414\n",
      "Training Epoch: 1 [31800/36045]\tLoss: 298.2700\n",
      "Training Epoch: 1 [31830/36045]\tLoss: 300.5493\n",
      "Training Epoch: 1 [31860/36045]\tLoss: 307.2237\n",
      "Training Epoch: 1 [31890/36045]\tLoss: 502.8581\n",
      "Training Epoch: 1 [31920/36045]\tLoss: 632.9091\n",
      "Training Epoch: 1 [31950/36045]\tLoss: 657.2278\n",
      "Training Epoch: 1 [31980/36045]\tLoss: 761.5096\n",
      "Training Epoch: 1 [32010/36045]\tLoss: 723.5193\n",
      "Training Epoch: 1 [32040/36045]\tLoss: 695.1630\n",
      "Training Epoch: 1 [32070/36045]\tLoss: 700.0427\n",
      "Training Epoch: 1 [32100/36045]\tLoss: 698.8508\n",
      "Training Epoch: 1 [32130/36045]\tLoss: 528.8324\n",
      "Training Epoch: 1 [32160/36045]\tLoss: 503.1494\n",
      "Training Epoch: 1 [32190/36045]\tLoss: 492.2804\n",
      "Training Epoch: 1 [32220/36045]\tLoss: 521.2157\n",
      "Training Epoch: 1 [32250/36045]\tLoss: 510.8430\n",
      "Training Epoch: 1 [32280/36045]\tLoss: 508.9939\n",
      "Training Epoch: 1 [32310/36045]\tLoss: 487.1083\n",
      "Training Epoch: 1 [32340/36045]\tLoss: 493.2984\n",
      "Training Epoch: 1 [32370/36045]\tLoss: 516.8719\n",
      "Training Epoch: 1 [32400/36045]\tLoss: 435.5808\n",
      "Training Epoch: 1 [32430/36045]\tLoss: 388.3517\n",
      "Training Epoch: 1 [32460/36045]\tLoss: 385.9260\n",
      "Training Epoch: 1 [32490/36045]\tLoss: 383.5292\n",
      "Training Epoch: 1 [32520/36045]\tLoss: 367.6482\n",
      "Training Epoch: 1 [32550/36045]\tLoss: 380.4960\n",
      "Training Epoch: 1 [32580/36045]\tLoss: 380.5081\n",
      "Training Epoch: 1 [32610/36045]\tLoss: 376.9890\n",
      "Training Epoch: 1 [32640/36045]\tLoss: 511.1369\n",
      "Training Epoch: 1 [32670/36045]\tLoss: 547.3435\n",
      "Training Epoch: 1 [32700/36045]\tLoss: 542.4800\n",
      "Training Epoch: 1 [32730/36045]\tLoss: 502.5216\n",
      "Training Epoch: 1 [32760/36045]\tLoss: 531.0443\n",
      "Training Epoch: 1 [32790/36045]\tLoss: 528.6155\n",
      "Training Epoch: 1 [32820/36045]\tLoss: 523.4816\n",
      "Training Epoch: 1 [32850/36045]\tLoss: 464.7956\n",
      "Training Epoch: 1 [32880/36045]\tLoss: 386.4248\n",
      "Training Epoch: 1 [32910/36045]\tLoss: 390.3611\n",
      "Training Epoch: 1 [32940/36045]\tLoss: 395.0190\n",
      "Training Epoch: 1 [32970/36045]\tLoss: 393.5273\n",
      "Training Epoch: 1 [33000/36045]\tLoss: 399.5793\n",
      "Training Epoch: 1 [33030/36045]\tLoss: 386.0390\n",
      "Training Epoch: 1 [33060/36045]\tLoss: 370.8835\n",
      "Training Epoch: 1 [33090/36045]\tLoss: 382.7654\n",
      "Training Epoch: 1 [33120/36045]\tLoss: 619.8622\n",
      "Training Epoch: 1 [33150/36045]\tLoss: 591.9700\n",
      "Training Epoch: 1 [33180/36045]\tLoss: 569.7906\n",
      "Training Epoch: 1 [33210/36045]\tLoss: 603.6344\n",
      "Training Epoch: 1 [33240/36045]\tLoss: 593.0891\n",
      "Training Epoch: 1 [33270/36045]\tLoss: 593.3680\n",
      "Training Epoch: 1 [33300/36045]\tLoss: 666.1104\n",
      "Training Epoch: 1 [33330/36045]\tLoss: 585.7296\n",
      "Training Epoch: 1 [33360/36045]\tLoss: 338.8256\n",
      "Training Epoch: 1 [33390/36045]\tLoss: 347.8134\n",
      "Training Epoch: 1 [33420/36045]\tLoss: 367.2498\n",
      "Training Epoch: 1 [33450/36045]\tLoss: 332.4604\n",
      "Training Epoch: 1 [33480/36045]\tLoss: 361.1324\n",
      "Training Epoch: 1 [33510/36045]\tLoss: 352.4384\n",
      "Training Epoch: 1 [33540/36045]\tLoss: 367.5446\n",
      "Training Epoch: 1 [33570/36045]\tLoss: 357.9679\n",
      "Training Epoch: 1 [33600/36045]\tLoss: 371.2505\n",
      "Training Epoch: 1 [33630/36045]\tLoss: 488.7660\n",
      "Training Epoch: 1 [33660/36045]\tLoss: 487.1455\n",
      "Training Epoch: 1 [33690/36045]\tLoss: 479.3874\n",
      "Training Epoch: 1 [33720/36045]\tLoss: 485.6157\n",
      "Training Epoch: 1 [33750/36045]\tLoss: 492.4236\n",
      "Training Epoch: 1 [33780/36045]\tLoss: 484.5360\n",
      "Training Epoch: 1 [33810/36045]\tLoss: 501.8602\n",
      "Training Epoch: 1 [33840/36045]\tLoss: 481.2646\n",
      "Training Epoch: 1 [33870/36045]\tLoss: 506.8152\n",
      "Training Epoch: 1 [33900/36045]\tLoss: 496.6329\n",
      "Training Epoch: 1 [33930/36045]\tLoss: 509.3446\n",
      "Training Epoch: 1 [33960/36045]\tLoss: 523.3014\n",
      "Training Epoch: 1 [33990/36045]\tLoss: 483.6776\n",
      "Training Epoch: 1 [34020/36045]\tLoss: 503.7737\n",
      "Training Epoch: 1 [34050/36045]\tLoss: 505.7669\n",
      "Training Epoch: 1 [34080/36045]\tLoss: 492.2363\n",
      "Training Epoch: 1 [34110/36045]\tLoss: 448.7284\n",
      "Training Epoch: 1 [34140/36045]\tLoss: 452.2749\n",
      "Training Epoch: 1 [34170/36045]\tLoss: 456.9203\n",
      "Training Epoch: 1 [34200/36045]\tLoss: 407.6094\n",
      "Training Epoch: 1 [34230/36045]\tLoss: 433.1096\n",
      "Training Epoch: 1 [34260/36045]\tLoss: 437.1436\n",
      "Training Epoch: 1 [34290/36045]\tLoss: 351.0193\n",
      "Training Epoch: 1 [34320/36045]\tLoss: 377.9037\n",
      "Training Epoch: 1 [34350/36045]\tLoss: 396.7480\n",
      "Training Epoch: 1 [34380/36045]\tLoss: 389.5746\n",
      "Training Epoch: 1 [34410/36045]\tLoss: 376.7301\n",
      "Training Epoch: 1 [34440/36045]\tLoss: 372.9868\n",
      "Training Epoch: 1 [34470/36045]\tLoss: 362.8770\n",
      "Training Epoch: 1 [34500/36045]\tLoss: 405.5992\n",
      "Training Epoch: 1 [34530/36045]\tLoss: 382.6664\n",
      "Training Epoch: 1 [34560/36045]\tLoss: 383.8576\n",
      "Training Epoch: 1 [34590/36045]\tLoss: 396.0530\n",
      "Training Epoch: 1 [34620/36045]\tLoss: 446.9358\n",
      "Training Epoch: 1 [34650/36045]\tLoss: 539.3507\n",
      "Training Epoch: 1 [34680/36045]\tLoss: 555.4478\n",
      "Training Epoch: 1 [34710/36045]\tLoss: 510.4130\n",
      "Training Epoch: 1 [34740/36045]\tLoss: 462.9828\n",
      "Training Epoch: 1 [34770/36045]\tLoss: 487.8319\n",
      "Training Epoch: 1 [34800/36045]\tLoss: 588.9804\n",
      "Training Epoch: 1 [34830/36045]\tLoss: 562.7463\n",
      "Training Epoch: 1 [34860/36045]\tLoss: 557.1193\n",
      "Training Epoch: 1 [34890/36045]\tLoss: 589.4934\n",
      "Training Epoch: 1 [34920/36045]\tLoss: 543.2414\n",
      "Training Epoch: 1 [34950/36045]\tLoss: 548.9555\n",
      "Training Epoch: 1 [34980/36045]\tLoss: 569.1674\n",
      "Training Epoch: 1 [35010/36045]\tLoss: 529.2126\n",
      "Training Epoch: 1 [35040/36045]\tLoss: 555.9927\n",
      "Training Epoch: 1 [35070/36045]\tLoss: 489.1450\n",
      "Training Epoch: 1 [35100/36045]\tLoss: 507.1781\n",
      "Training Epoch: 1 [35130/36045]\tLoss: 487.1672\n",
      "Training Epoch: 1 [35160/36045]\tLoss: 452.2185\n",
      "Training Epoch: 1 [35190/36045]\tLoss: 408.2936\n",
      "Training Epoch: 1 [35220/36045]\tLoss: 425.7674\n",
      "Training Epoch: 1 [35250/36045]\tLoss: 437.3558\n",
      "Training Epoch: 1 [35280/36045]\tLoss: 469.5288\n",
      "Training Epoch: 1 [35310/36045]\tLoss: 454.8876\n",
      "Training Epoch: 1 [35340/36045]\tLoss: 507.4035\n",
      "Training Epoch: 1 [35370/36045]\tLoss: 500.6092\n",
      "Training Epoch: 1 [35400/36045]\tLoss: 510.7008\n",
      "Training Epoch: 1 [35430/36045]\tLoss: 490.9313\n",
      "Training Epoch: 1 [35460/36045]\tLoss: 487.7800\n",
      "Training Epoch: 1 [35490/36045]\tLoss: 475.0894\n",
      "Training Epoch: 1 [35520/36045]\tLoss: 445.5024\n",
      "Training Epoch: 1 [35550/36045]\tLoss: 465.8656\n",
      "Training Epoch: 1 [35580/36045]\tLoss: 474.4245\n",
      "Training Epoch: 1 [35610/36045]\tLoss: 598.5699\n",
      "Training Epoch: 1 [35640/36045]\tLoss: 574.1121\n",
      "Training Epoch: 1 [35670/36045]\tLoss: 551.6603\n",
      "Training Epoch: 1 [35700/36045]\tLoss: 510.2659\n",
      "Training Epoch: 1 [35730/36045]\tLoss: 537.6409\n",
      "Training Epoch: 1 [35760/36045]\tLoss: 613.6421\n",
      "Training Epoch: 1 [35790/36045]\tLoss: 585.8150\n",
      "Training Epoch: 1 [35820/36045]\tLoss: 576.9031\n",
      "Training Epoch: 1 [35850/36045]\tLoss: 535.5126\n",
      "Training Epoch: 1 [35880/36045]\tLoss: 557.5696\n",
      "Training Epoch: 1 [35910/36045]\tLoss: 561.7756\n",
      "Training Epoch: 1 [35940/36045]\tLoss: 560.1342\n",
      "Training Epoch: 1 [35970/36045]\tLoss: 544.1754\n",
      "Training Epoch: 1 [36000/36045]\tLoss: 553.7743\n",
      "Training Epoch: 1 [36030/36045]\tLoss: 532.3295\n",
      "Training Epoch: 1 [36045/36045]\tLoss: 559.7679\n",
      "Training Epoch: 1 [4004/4004]\tLoss: 499.9490\n",
      "Training Epoch: 2 [30/36045]\tLoss: 496.1051\n",
      "Training Epoch: 2 [60/36045]\tLoss: 481.6413\n",
      "Training Epoch: 2 [90/36045]\tLoss: 464.1083\n",
      "Training Epoch: 2 [120/36045]\tLoss: 469.6102\n",
      "Training Epoch: 2 [150/36045]\tLoss: 460.7830\n",
      "Training Epoch: 2 [180/36045]\tLoss: 462.4548\n",
      "Training Epoch: 2 [210/36045]\tLoss: 439.0365\n",
      "Training Epoch: 2 [240/36045]\tLoss: 557.3866\n",
      "Training Epoch: 2 [270/36045]\tLoss: 671.1403\n",
      "Training Epoch: 2 [300/36045]\tLoss: 631.7911\n",
      "Training Epoch: 2 [330/36045]\tLoss: 616.1058\n",
      "Training Epoch: 2 [360/36045]\tLoss: 578.2142\n",
      "Training Epoch: 2 [390/36045]\tLoss: 609.2319\n",
      "Training Epoch: 2 [420/36045]\tLoss: 570.0002\n",
      "Training Epoch: 2 [450/36045]\tLoss: 585.5278\n",
      "Training Epoch: 2 [480/36045]\tLoss: 524.7004\n",
      "Training Epoch: 2 [510/36045]\tLoss: 536.3194\n",
      "Training Epoch: 2 [540/36045]\tLoss: 527.7756\n",
      "Training Epoch: 2 [570/36045]\tLoss: 525.8627\n",
      "Training Epoch: 2 [600/36045]\tLoss: 520.6572\n",
      "Training Epoch: 2 [630/36045]\tLoss: 548.9503\n",
      "Training Epoch: 2 [660/36045]\tLoss: 537.4192\n",
      "Training Epoch: 2 [690/36045]\tLoss: 511.2501\n",
      "Training Epoch: 2 [720/36045]\tLoss: 487.9154\n",
      "Training Epoch: 2 [750/36045]\tLoss: 492.9669\n",
      "Training Epoch: 2 [780/36045]\tLoss: 486.0361\n",
      "Training Epoch: 2 [810/36045]\tLoss: 511.4629\n",
      "Training Epoch: 2 [840/36045]\tLoss: 489.2077\n",
      "Training Epoch: 2 [870/36045]\tLoss: 470.4582\n",
      "Training Epoch: 2 [900/36045]\tLoss: 452.5022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [930/36045]\tLoss: 444.0562\n",
      "Training Epoch: 2 [960/36045]\tLoss: 431.9564\n",
      "Training Epoch: 2 [990/36045]\tLoss: 427.9522\n",
      "Training Epoch: 2 [1020/36045]\tLoss: 433.8761\n",
      "Training Epoch: 2 [1050/36045]\tLoss: 432.1668\n",
      "Training Epoch: 2 [1080/36045]\tLoss: 420.5672\n",
      "Training Epoch: 2 [1110/36045]\tLoss: 431.6301\n",
      "Training Epoch: 2 [1140/36045]\tLoss: 430.8210\n",
      "Training Epoch: 2 [1170/36045]\tLoss: 445.9658\n",
      "Training Epoch: 2 [1200/36045]\tLoss: 465.5165\n",
      "Training Epoch: 2 [1230/36045]\tLoss: 522.6809\n",
      "Training Epoch: 2 [1260/36045]\tLoss: 517.6641\n",
      "Training Epoch: 2 [1290/36045]\tLoss: 530.8632\n",
      "Training Epoch: 2 [1320/36045]\tLoss: 546.6030\n",
      "Training Epoch: 2 [1350/36045]\tLoss: 506.9172\n",
      "Training Epoch: 2 [1380/36045]\tLoss: 550.1671\n",
      "Training Epoch: 2 [1410/36045]\tLoss: 543.0601\n",
      "Training Epoch: 2 [1440/36045]\tLoss: 533.3284\n",
      "Training Epoch: 2 [1470/36045]\tLoss: 506.7535\n",
      "Training Epoch: 2 [1500/36045]\tLoss: 468.3958\n",
      "Training Epoch: 2 [1530/36045]\tLoss: 494.9658\n",
      "Training Epoch: 2 [1560/36045]\tLoss: 485.7058\n",
      "Training Epoch: 2 [1590/36045]\tLoss: 513.7474\n",
      "Training Epoch: 2 [1620/36045]\tLoss: 496.0480\n",
      "Training Epoch: 2 [1650/36045]\tLoss: 480.7250\n",
      "Training Epoch: 2 [1680/36045]\tLoss: 477.4043\n",
      "Training Epoch: 2 [1710/36045]\tLoss: 554.7427\n",
      "Training Epoch: 2 [1740/36045]\tLoss: 550.7749\n",
      "Training Epoch: 2 [1770/36045]\tLoss: 531.0425\n",
      "Training Epoch: 2 [1800/36045]\tLoss: 535.4279\n",
      "Training Epoch: 2 [1830/36045]\tLoss: 547.3915\n",
      "Training Epoch: 2 [1860/36045]\tLoss: 522.0947\n",
      "Training Epoch: 2 [1890/36045]\tLoss: 509.1349\n",
      "Training Epoch: 2 [1920/36045]\tLoss: 542.9147\n",
      "Training Epoch: 2 [1950/36045]\tLoss: 512.5652\n",
      "Training Epoch: 2 [1980/36045]\tLoss: 460.6738\n",
      "Training Epoch: 2 [2010/36045]\tLoss: 455.4488\n",
      "Training Epoch: 2 [2040/36045]\tLoss: 459.7371\n",
      "Training Epoch: 2 [2070/36045]\tLoss: 489.7743\n",
      "Training Epoch: 2 [2100/36045]\tLoss: 478.6222\n",
      "Training Epoch: 2 [2130/36045]\tLoss: 471.2057\n",
      "Training Epoch: 2 [2160/36045]\tLoss: 472.3318\n",
      "Training Epoch: 2 [2190/36045]\tLoss: 429.3671\n",
      "Training Epoch: 2 [2220/36045]\tLoss: 433.6909\n",
      "Training Epoch: 2 [2250/36045]\tLoss: 420.1959\n",
      "Training Epoch: 2 [2280/36045]\tLoss: 437.5477\n",
      "Training Epoch: 2 [2310/36045]\tLoss: 449.0784\n",
      "Training Epoch: 2 [2340/36045]\tLoss: 416.2000\n",
      "Training Epoch: 2 [2370/36045]\tLoss: 445.0880\n",
      "Training Epoch: 2 [2400/36045]\tLoss: 426.4717\n",
      "Training Epoch: 2 [2430/36045]\tLoss: 528.1849\n",
      "Training Epoch: 2 [2460/36045]\tLoss: 586.3442\n",
      "Training Epoch: 2 [2490/36045]\tLoss: 588.3433\n",
      "Training Epoch: 2 [2520/36045]\tLoss: 550.1625\n",
      "Training Epoch: 2 [2550/36045]\tLoss: 592.9652\n",
      "Training Epoch: 2 [2580/36045]\tLoss: 587.6730\n",
      "Training Epoch: 2 [2610/36045]\tLoss: 588.9423\n",
      "Training Epoch: 2 [2640/36045]\tLoss: 720.6041\n",
      "Training Epoch: 2 [2670/36045]\tLoss: 841.8996\n",
      "Training Epoch: 2 [2700/36045]\tLoss: 773.4449\n",
      "Training Epoch: 2 [2730/36045]\tLoss: 901.4748\n",
      "Training Epoch: 2 [2760/36045]\tLoss: 872.4146\n",
      "Training Epoch: 2 [2790/36045]\tLoss: 867.3680\n",
      "Training Epoch: 2 [2820/36045]\tLoss: 765.2945\n",
      "Training Epoch: 2 [2850/36045]\tLoss: 585.6810\n",
      "Training Epoch: 2 [2880/36045]\tLoss: 591.8560\n",
      "Training Epoch: 2 [2910/36045]\tLoss: 580.3920\n",
      "Training Epoch: 2 [2940/36045]\tLoss: 591.0741\n",
      "Training Epoch: 2 [2970/36045]\tLoss: 557.9683\n",
      "Training Epoch: 2 [3000/36045]\tLoss: 578.4658\n",
      "Training Epoch: 2 [3030/36045]\tLoss: 600.5864\n",
      "Training Epoch: 2 [3060/36045]\tLoss: 569.4364\n",
      "Training Epoch: 2 [3090/36045]\tLoss: 565.5523\n",
      "Training Epoch: 2 [3120/36045]\tLoss: 447.2045\n",
      "Training Epoch: 2 [3150/36045]\tLoss: 412.3452\n",
      "Training Epoch: 2 [3180/36045]\tLoss: 443.0848\n",
      "Training Epoch: 2 [3210/36045]\tLoss: 410.1961\n",
      "Training Epoch: 2 [3240/36045]\tLoss: 398.9346\n",
      "Training Epoch: 2 [3270/36045]\tLoss: 410.1465\n",
      "Training Epoch: 2 [3300/36045]\tLoss: 378.6131\n",
      "Training Epoch: 2 [3330/36045]\tLoss: 404.1513\n",
      "Training Epoch: 2 [3360/36045]\tLoss: 412.8957\n",
      "Training Epoch: 2 [3390/36045]\tLoss: 420.6949\n",
      "Training Epoch: 2 [3420/36045]\tLoss: 474.5320\n",
      "Training Epoch: 2 [3450/36045]\tLoss: 443.4159\n",
      "Training Epoch: 2 [3480/36045]\tLoss: 437.3651\n",
      "Training Epoch: 2 [3510/36045]\tLoss: 456.4748\n",
      "Training Epoch: 2 [3540/36045]\tLoss: 417.3530\n",
      "Training Epoch: 2 [3570/36045]\tLoss: 442.6268\n",
      "Training Epoch: 2 [3600/36045]\tLoss: 460.8503\n",
      "Training Epoch: 2 [3630/36045]\tLoss: 525.0101\n",
      "Training Epoch: 2 [3660/36045]\tLoss: 525.2919\n",
      "Training Epoch: 2 [3690/36045]\tLoss: 534.0513\n",
      "Training Epoch: 2 [3720/36045]\tLoss: 534.3455\n",
      "Training Epoch: 2 [3750/36045]\tLoss: 496.5521\n",
      "Training Epoch: 2 [3780/36045]\tLoss: 514.9274\n",
      "Training Epoch: 2 [3810/36045]\tLoss: 510.4544\n",
      "Training Epoch: 2 [3840/36045]\tLoss: 508.4940\n",
      "Training Epoch: 2 [3870/36045]\tLoss: 529.3667\n",
      "Training Epoch: 2 [3900/36045]\tLoss: 524.4489\n",
      "Training Epoch: 2 [3930/36045]\tLoss: 502.7554\n",
      "Training Epoch: 2 [3960/36045]\tLoss: 515.2748\n",
      "Training Epoch: 2 [3990/36045]\tLoss: 503.4247\n",
      "Training Epoch: 2 [4020/36045]\tLoss: 500.0958\n",
      "Training Epoch: 2 [4050/36045]\tLoss: 443.2021\n",
      "Training Epoch: 2 [4080/36045]\tLoss: 452.7218\n",
      "Training Epoch: 2 [4110/36045]\tLoss: 460.9243\n",
      "Training Epoch: 2 [4140/36045]\tLoss: 456.3699\n",
      "Training Epoch: 2 [4170/36045]\tLoss: 471.8422\n",
      "Training Epoch: 2 [4200/36045]\tLoss: 450.8659\n",
      "Training Epoch: 2 [4230/36045]\tLoss: 469.9872\n",
      "Training Epoch: 2 [4260/36045]\tLoss: 446.3207\n",
      "Training Epoch: 2 [4290/36045]\tLoss: 456.3980\n",
      "Training Epoch: 2 [4320/36045]\tLoss: 469.2090\n",
      "Training Epoch: 2 [4350/36045]\tLoss: 446.6026\n",
      "Training Epoch: 2 [4380/36045]\tLoss: 437.5135\n",
      "Training Epoch: 2 [4410/36045]\tLoss: 433.2163\n",
      "Training Epoch: 2 [4440/36045]\tLoss: 462.1488\n",
      "Training Epoch: 2 [4470/36045]\tLoss: 535.7167\n",
      "Training Epoch: 2 [4500/36045]\tLoss: 509.1955\n",
      "Training Epoch: 2 [4530/36045]\tLoss: 522.3865\n",
      "Training Epoch: 2 [4560/36045]\tLoss: 513.5954\n",
      "Training Epoch: 2 [4590/36045]\tLoss: 558.5708\n",
      "Training Epoch: 2 [4620/36045]\tLoss: 524.1899\n",
      "Training Epoch: 2 [4650/36045]\tLoss: 527.9835\n",
      "Training Epoch: 2 [4680/36045]\tLoss: 510.2610\n",
      "Training Epoch: 2 [4710/36045]\tLoss: 458.2546\n",
      "Training Epoch: 2 [4740/36045]\tLoss: 471.9437\n",
      "Training Epoch: 2 [4770/36045]\tLoss: 490.6656\n",
      "Training Epoch: 2 [4800/36045]\tLoss: 483.4334\n",
      "Training Epoch: 2 [4830/36045]\tLoss: 481.8767\n",
      "Training Epoch: 2 [4860/36045]\tLoss: 474.8105\n",
      "Training Epoch: 2 [4890/36045]\tLoss: 465.4280\n",
      "Training Epoch: 2 [4920/36045]\tLoss: 458.4987\n",
      "Training Epoch: 2 [4950/36045]\tLoss: 496.9543\n",
      "Training Epoch: 2 [4980/36045]\tLoss: 510.8940\n",
      "Training Epoch: 2 [5010/36045]\tLoss: 528.0408\n",
      "Training Epoch: 2 [5040/36045]\tLoss: 500.2974\n",
      "Training Epoch: 2 [5070/36045]\tLoss: 490.3093\n",
      "Training Epoch: 2 [5100/36045]\tLoss: 512.0331\n",
      "Training Epoch: 2 [5130/36045]\tLoss: 494.2629\n",
      "Training Epoch: 2 [5160/36045]\tLoss: 499.1068\n",
      "Training Epoch: 2 [5190/36045]\tLoss: 474.7719\n",
      "Training Epoch: 2 [5220/36045]\tLoss: 472.5567\n",
      "Training Epoch: 2 [5250/36045]\tLoss: 492.9488\n",
      "Training Epoch: 2 [5280/36045]\tLoss: 477.1489\n",
      "Training Epoch: 2 [5310/36045]\tLoss: 491.4839\n",
      "Training Epoch: 2 [5340/36045]\tLoss: 500.2830\n",
      "Training Epoch: 2 [5370/36045]\tLoss: 482.2044\n",
      "Training Epoch: 2 [5400/36045]\tLoss: 476.2688\n",
      "Training Epoch: 2 [5430/36045]\tLoss: 460.4594\n",
      "Training Epoch: 2 [5460/36045]\tLoss: 446.2507\n",
      "Training Epoch: 2 [5490/36045]\tLoss: 491.8040\n",
      "Training Epoch: 2 [5520/36045]\tLoss: 470.3776\n",
      "Training Epoch: 2 [5550/36045]\tLoss: 466.7912\n",
      "Training Epoch: 2 [5580/36045]\tLoss: 535.8439\n",
      "Training Epoch: 2 [5610/36045]\tLoss: 527.8728\n",
      "Training Epoch: 2 [5640/36045]\tLoss: 532.5568\n",
      "Training Epoch: 2 [5670/36045]\tLoss: 453.6671\n",
      "Training Epoch: 2 [5700/36045]\tLoss: 500.0785\n",
      "Training Epoch: 2 [5730/36045]\tLoss: 456.7335\n",
      "Training Epoch: 2 [5760/36045]\tLoss: 481.4286\n",
      "Training Epoch: 2 [5790/36045]\tLoss: 492.3235\n",
      "Training Epoch: 2 [5820/36045]\tLoss: 488.9772\n",
      "Training Epoch: 2 [5850/36045]\tLoss: 486.1269\n",
      "Training Epoch: 2 [5880/36045]\tLoss: 544.0160\n",
      "Training Epoch: 2 [5910/36045]\tLoss: 562.3498\n",
      "Training Epoch: 2 [5940/36045]\tLoss: 562.7744\n",
      "Training Epoch: 2 [5970/36045]\tLoss: 545.6382\n",
      "Training Epoch: 2 [6000/36045]\tLoss: 561.8365\n",
      "Training Epoch: 2 [6030/36045]\tLoss: 512.9422\n",
      "Training Epoch: 2 [6060/36045]\tLoss: 556.8967\n",
      "Training Epoch: 2 [6090/36045]\tLoss: 543.2281\n",
      "Training Epoch: 2 [6120/36045]\tLoss: 511.1968\n",
      "Training Epoch: 2 [6150/36045]\tLoss: 558.3195\n",
      "Training Epoch: 2 [6180/36045]\tLoss: 551.8294\n",
      "Training Epoch: 2 [6210/36045]\tLoss: 560.8931\n",
      "Training Epoch: 2 [6240/36045]\tLoss: 546.0284\n",
      "Training Epoch: 2 [6270/36045]\tLoss: 617.6044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [6300/36045]\tLoss: 568.6374\n",
      "Training Epoch: 2 [6330/36045]\tLoss: 625.4351\n",
      "Training Epoch: 2 [6360/36045]\tLoss: 619.6085\n",
      "Training Epoch: 2 [6390/36045]\tLoss: 483.3222\n",
      "Training Epoch: 2 [6420/36045]\tLoss: 480.3186\n",
      "Training Epoch: 2 [6450/36045]\tLoss: 445.6674\n",
      "Training Epoch: 2 [6480/36045]\tLoss: 469.8312\n",
      "Training Epoch: 2 [6510/36045]\tLoss: 464.9219\n",
      "Training Epoch: 2 [6540/36045]\tLoss: 484.4069\n",
      "Training Epoch: 2 [6570/36045]\tLoss: 480.7814\n",
      "Training Epoch: 2 [6600/36045]\tLoss: 488.4799\n",
      "Training Epoch: 2 [6630/36045]\tLoss: 553.2121\n",
      "Training Epoch: 2 [6660/36045]\tLoss: 557.1851\n",
      "Training Epoch: 2 [6690/36045]\tLoss: 572.4532\n",
      "Training Epoch: 2 [6720/36045]\tLoss: 541.6931\n",
      "Training Epoch: 2 [6750/36045]\tLoss: 566.1258\n",
      "Training Epoch: 2 [6780/36045]\tLoss: 555.1273\n",
      "Training Epoch: 2 [6810/36045]\tLoss: 553.6849\n",
      "Training Epoch: 2 [6840/36045]\tLoss: 561.1826\n",
      "Training Epoch: 2 [6870/36045]\tLoss: 493.2423\n",
      "Training Epoch: 2 [6900/36045]\tLoss: 473.2225\n",
      "Training Epoch: 2 [6930/36045]\tLoss: 451.8217\n",
      "Training Epoch: 2 [6960/36045]\tLoss: 456.0049\n",
      "Training Epoch: 2 [6990/36045]\tLoss: 482.0432\n",
      "Training Epoch: 2 [7020/36045]\tLoss: 503.1356\n",
      "Training Epoch: 2 [7050/36045]\tLoss: 499.4850\n",
      "Training Epoch: 2 [7080/36045]\tLoss: 480.1715\n",
      "Training Epoch: 2 [7110/36045]\tLoss: 500.2405\n",
      "Training Epoch: 2 [7140/36045]\tLoss: 484.7160\n",
      "Training Epoch: 2 [7170/36045]\tLoss: 504.1666\n",
      "Training Epoch: 2 [7200/36045]\tLoss: 490.4595\n",
      "Training Epoch: 2 [7230/36045]\tLoss: 486.2667\n",
      "Training Epoch: 2 [7260/36045]\tLoss: 491.4218\n",
      "Training Epoch: 2 [7290/36045]\tLoss: 491.8307\n",
      "Training Epoch: 2 [7320/36045]\tLoss: 471.0098\n",
      "Training Epoch: 2 [7350/36045]\tLoss: 479.1732\n",
      "Training Epoch: 2 [7380/36045]\tLoss: 460.3101\n",
      "Training Epoch: 2 [7410/36045]\tLoss: 439.6252\n",
      "Training Epoch: 2 [7440/36045]\tLoss: 452.3710\n",
      "Training Epoch: 2 [7470/36045]\tLoss: 435.5676\n",
      "Training Epoch: 2 [7500/36045]\tLoss: 453.0146\n",
      "Training Epoch: 2 [7530/36045]\tLoss: 425.0959\n",
      "Training Epoch: 2 [7560/36045]\tLoss: 439.5091\n",
      "Training Epoch: 2 [7590/36045]\tLoss: 467.5051\n",
      "Training Epoch: 2 [7620/36045]\tLoss: 511.6835\n",
      "Training Epoch: 2 [7650/36045]\tLoss: 495.9777\n",
      "Training Epoch: 2 [7680/36045]\tLoss: 496.7360\n",
      "Training Epoch: 2 [7710/36045]\tLoss: 466.6808\n",
      "Training Epoch: 2 [7740/36045]\tLoss: 504.7146\n",
      "Training Epoch: 2 [7770/36045]\tLoss: 477.5630\n",
      "Training Epoch: 2 [7800/36045]\tLoss: 477.4422\n",
      "Training Epoch: 2 [7830/36045]\tLoss: 463.0972\n",
      "Training Epoch: 2 [7860/36045]\tLoss: 478.5616\n",
      "Training Epoch: 2 [7890/36045]\tLoss: 480.0131\n",
      "Training Epoch: 2 [7920/36045]\tLoss: 489.1095\n",
      "Training Epoch: 2 [7950/36045]\tLoss: 480.0037\n",
      "Training Epoch: 2 [7980/36045]\tLoss: 514.5466\n",
      "Training Epoch: 2 [8010/36045]\tLoss: 482.5223\n",
      "Training Epoch: 2 [8040/36045]\tLoss: 468.3355\n",
      "Training Epoch: 2 [8070/36045]\tLoss: 492.2511\n",
      "Training Epoch: 2 [8100/36045]\tLoss: 490.8911\n",
      "Training Epoch: 2 [8130/36045]\tLoss: 570.4631\n",
      "Training Epoch: 2 [8160/36045]\tLoss: 574.5624\n",
      "Training Epoch: 2 [8190/36045]\tLoss: 575.1115\n",
      "Training Epoch: 2 [8220/36045]\tLoss: 514.3888\n",
      "Training Epoch: 2 [8250/36045]\tLoss: 541.1763\n",
      "Training Epoch: 2 [8280/36045]\tLoss: 595.1749\n",
      "Training Epoch: 2 [8310/36045]\tLoss: 557.1774\n",
      "Training Epoch: 2 [8340/36045]\tLoss: 543.5521\n",
      "Training Epoch: 2 [8370/36045]\tLoss: 498.7548\n",
      "Training Epoch: 2 [8400/36045]\tLoss: 460.3336\n",
      "Training Epoch: 2 [8430/36045]\tLoss: 445.4102\n",
      "Training Epoch: 2 [8460/36045]\tLoss: 457.1338\n",
      "Training Epoch: 2 [8490/36045]\tLoss: 462.2227\n",
      "Training Epoch: 2 [8520/36045]\tLoss: 512.9250\n",
      "Training Epoch: 2 [8550/36045]\tLoss: 444.1712\n",
      "Training Epoch: 2 [8580/36045]\tLoss: 462.7624\n",
      "Training Epoch: 2 [8610/36045]\tLoss: 458.6165\n",
      "Training Epoch: 2 [8640/36045]\tLoss: 492.2546\n",
      "Training Epoch: 2 [8670/36045]\tLoss: 496.7330\n",
      "Training Epoch: 2 [8700/36045]\tLoss: 519.2795\n",
      "Training Epoch: 2 [8730/36045]\tLoss: 485.2491\n",
      "Training Epoch: 2 [8760/36045]\tLoss: 501.0463\n",
      "Training Epoch: 2 [8790/36045]\tLoss: 520.9844\n",
      "Training Epoch: 2 [8820/36045]\tLoss: 507.1418\n",
      "Training Epoch: 2 [8850/36045]\tLoss: 485.5467\n",
      "Training Epoch: 2 [8880/36045]\tLoss: 438.0009\n",
      "Training Epoch: 2 [8910/36045]\tLoss: 485.2101\n",
      "Training Epoch: 2 [8940/36045]\tLoss: 441.4075\n",
      "Training Epoch: 2 [8970/36045]\tLoss: 491.8293\n",
      "Training Epoch: 2 [9000/36045]\tLoss: 459.7874\n",
      "Training Epoch: 2 [9030/36045]\tLoss: 470.1964\n",
      "Training Epoch: 2 [9060/36045]\tLoss: 490.1433\n",
      "Training Epoch: 2 [9090/36045]\tLoss: 509.6690\n",
      "Training Epoch: 2 [9120/36045]\tLoss: 483.2598\n",
      "Training Epoch: 2 [9150/36045]\tLoss: 282.6871\n",
      "Training Epoch: 2 [9180/36045]\tLoss: 274.2476\n",
      "Training Epoch: 2 [9210/36045]\tLoss: 278.8112\n",
      "Training Epoch: 2 [9240/36045]\tLoss: 288.0297\n",
      "Training Epoch: 2 [9270/36045]\tLoss: 304.0225\n",
      "Training Epoch: 2 [9300/36045]\tLoss: 311.6413\n",
      "Training Epoch: 2 [9330/36045]\tLoss: 293.2772\n",
      "Training Epoch: 2 [9360/36045]\tLoss: 336.4891\n",
      "Training Epoch: 2 [9390/36045]\tLoss: 558.8065\n",
      "Training Epoch: 2 [9420/36045]\tLoss: 580.5897\n",
      "Training Epoch: 2 [9450/36045]\tLoss: 576.2925\n",
      "Training Epoch: 2 [9480/36045]\tLoss: 562.0891\n",
      "Training Epoch: 2 [9510/36045]\tLoss: 595.0944\n",
      "Training Epoch: 2 [9540/36045]\tLoss: 578.1744\n",
      "Training Epoch: 2 [9570/36045]\tLoss: 529.7125\n",
      "Training Epoch: 2 [9600/36045]\tLoss: 451.4013\n",
      "Training Epoch: 2 [9630/36045]\tLoss: 465.9227\n",
      "Training Epoch: 2 [9660/36045]\tLoss: 458.7673\n",
      "Training Epoch: 2 [9690/36045]\tLoss: 448.8937\n",
      "Training Epoch: 2 [9720/36045]\tLoss: 450.4639\n",
      "Training Epoch: 2 [9750/36045]\tLoss: 451.9506\n",
      "Training Epoch: 2 [9780/36045]\tLoss: 550.1390\n",
      "Training Epoch: 2 [9810/36045]\tLoss: 602.0717\n",
      "Training Epoch: 2 [9840/36045]\tLoss: 604.1589\n",
      "Training Epoch: 2 [9870/36045]\tLoss: 613.8765\n",
      "Training Epoch: 2 [9900/36045]\tLoss: 622.6498\n",
      "Training Epoch: 2 [9930/36045]\tLoss: 580.4864\n",
      "Training Epoch: 2 [9960/36045]\tLoss: 617.3348\n",
      "Training Epoch: 2 [9990/36045]\tLoss: 580.7134\n",
      "Training Epoch: 2 [10020/36045]\tLoss: 434.6385\n",
      "Training Epoch: 2 [10050/36045]\tLoss: 471.0038\n",
      "Training Epoch: 2 [10080/36045]\tLoss: 460.6194\n",
      "Training Epoch: 2 [10110/36045]\tLoss: 469.2553\n",
      "Training Epoch: 2 [10140/36045]\tLoss: 477.7575\n",
      "Training Epoch: 2 [10170/36045]\tLoss: 455.4840\n",
      "Training Epoch: 2 [10200/36045]\tLoss: 458.2973\n",
      "Training Epoch: 2 [10230/36045]\tLoss: 569.4128\n",
      "Training Epoch: 2 [10260/36045]\tLoss: 542.1561\n",
      "Training Epoch: 2 [10290/36045]\tLoss: 553.1750\n",
      "Training Epoch: 2 [10320/36045]\tLoss: 563.2424\n",
      "Training Epoch: 2 [10350/36045]\tLoss: 570.0079\n",
      "Training Epoch: 2 [10380/36045]\tLoss: 564.0999\n",
      "Training Epoch: 2 [10410/36045]\tLoss: 567.8077\n",
      "Training Epoch: 2 [10440/36045]\tLoss: 532.5431\n",
      "Training Epoch: 2 [10470/36045]\tLoss: 453.3307\n",
      "Training Epoch: 2 [10500/36045]\tLoss: 433.6322\n",
      "Training Epoch: 2 [10530/36045]\tLoss: 446.6758\n",
      "Training Epoch: 2 [10560/36045]\tLoss: 421.7053\n",
      "Training Epoch: 2 [10590/36045]\tLoss: 461.9198\n",
      "Training Epoch: 2 [10620/36045]\tLoss: 454.2474\n",
      "Training Epoch: 2 [10650/36045]\tLoss: 458.8949\n",
      "Training Epoch: 2 [10680/36045]\tLoss: 495.3164\n",
      "Training Epoch: 2 [10710/36045]\tLoss: 575.2406\n",
      "Training Epoch: 2 [10740/36045]\tLoss: 580.4698\n",
      "Training Epoch: 2 [10770/36045]\tLoss: 550.1246\n",
      "Training Epoch: 2 [10800/36045]\tLoss: 523.9310\n",
      "Training Epoch: 2 [10830/36045]\tLoss: 566.7600\n",
      "Training Epoch: 2 [10860/36045]\tLoss: 569.7228\n",
      "Training Epoch: 2 [10890/36045]\tLoss: 583.9661\n",
      "Training Epoch: 2 [10920/36045]\tLoss: 498.5831\n",
      "Training Epoch: 2 [10950/36045]\tLoss: 415.3347\n",
      "Training Epoch: 2 [10980/36045]\tLoss: 396.2122\n",
      "Training Epoch: 2 [11010/36045]\tLoss: 461.8885\n",
      "Training Epoch: 2 [11040/36045]\tLoss: 461.5715\n",
      "Training Epoch: 2 [11070/36045]\tLoss: 457.2897\n",
      "Training Epoch: 2 [11100/36045]\tLoss: 455.6956\n",
      "Training Epoch: 2 [11130/36045]\tLoss: 499.8987\n",
      "Training Epoch: 2 [11160/36045]\tLoss: 514.7559\n",
      "Training Epoch: 2 [11190/36045]\tLoss: 561.9759\n",
      "Training Epoch: 2 [11220/36045]\tLoss: 534.3767\n",
      "Training Epoch: 2 [11250/36045]\tLoss: 538.5820\n",
      "Training Epoch: 2 [11280/36045]\tLoss: 534.2176\n",
      "Training Epoch: 2 [11310/36045]\tLoss: 538.6742\n",
      "Training Epoch: 2 [11340/36045]\tLoss: 511.6508\n",
      "Training Epoch: 2 [11370/36045]\tLoss: 524.8373\n",
      "Training Epoch: 2 [11400/36045]\tLoss: 477.9774\n",
      "Training Epoch: 2 [11430/36045]\tLoss: 478.6185\n",
      "Training Epoch: 2 [11460/36045]\tLoss: 451.0500\n",
      "Training Epoch: 2 [11490/36045]\tLoss: 458.7400\n",
      "Training Epoch: 2 [11520/36045]\tLoss: 472.0266\n",
      "Training Epoch: 2 [11550/36045]\tLoss: 474.3252\n",
      "Training Epoch: 2 [11580/36045]\tLoss: 489.2650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [11610/36045]\tLoss: 579.9512\n",
      "Training Epoch: 2 [11640/36045]\tLoss: 578.9963\n",
      "Training Epoch: 2 [11670/36045]\tLoss: 601.8132\n",
      "Training Epoch: 2 [11700/36045]\tLoss: 556.0808\n",
      "Training Epoch: 2 [11730/36045]\tLoss: 591.2645\n",
      "Training Epoch: 2 [11760/36045]\tLoss: 609.1748\n",
      "Training Epoch: 2 [11790/36045]\tLoss: 635.3881\n",
      "Training Epoch: 2 [11820/36045]\tLoss: 640.8660\n",
      "Training Epoch: 2 [11850/36045]\tLoss: 731.0140\n",
      "Training Epoch: 2 [11880/36045]\tLoss: 854.4921\n",
      "Training Epoch: 2 [11910/36045]\tLoss: 948.9555\n",
      "Training Epoch: 2 [11940/36045]\tLoss: 904.4485\n",
      "Training Epoch: 2 [11970/36045]\tLoss: 909.9544\n",
      "Training Epoch: 2 [12000/36045]\tLoss: 936.8904\n",
      "Training Epoch: 2 [12030/36045]\tLoss: 864.3895\n",
      "Training Epoch: 2 [12060/36045]\tLoss: 961.3657\n",
      "Training Epoch: 2 [12090/36045]\tLoss: 413.2813\n",
      "Training Epoch: 2 [12120/36045]\tLoss: 403.7952\n",
      "Training Epoch: 2 [12150/36045]\tLoss: 387.8820\n",
      "Training Epoch: 2 [12180/36045]\tLoss: 384.6813\n",
      "Training Epoch: 2 [12210/36045]\tLoss: 397.0682\n",
      "Training Epoch: 2 [12240/36045]\tLoss: 416.2709\n",
      "Training Epoch: 2 [12270/36045]\tLoss: 418.6477\n",
      "Training Epoch: 2 [12300/36045]\tLoss: 580.4139\n",
      "Training Epoch: 2 [12330/36045]\tLoss: 581.6959\n",
      "Training Epoch: 2 [12360/36045]\tLoss: 563.9705\n",
      "Training Epoch: 2 [12390/36045]\tLoss: 592.7344\n",
      "Training Epoch: 2 [12420/36045]\tLoss: 568.2394\n",
      "Training Epoch: 2 [12450/36045]\tLoss: 580.5240\n",
      "Training Epoch: 2 [12480/36045]\tLoss: 592.9714\n",
      "Training Epoch: 2 [12510/36045]\tLoss: 608.1516\n",
      "Training Epoch: 2 [12540/36045]\tLoss: 579.6796\n",
      "Training Epoch: 2 [12570/36045]\tLoss: 501.6740\n",
      "Training Epoch: 2 [12600/36045]\tLoss: 506.2447\n",
      "Training Epoch: 2 [12630/36045]\tLoss: 515.1909\n",
      "Training Epoch: 2 [12660/36045]\tLoss: 502.7138\n",
      "Training Epoch: 2 [12690/36045]\tLoss: 522.3158\n",
      "Training Epoch: 2 [12720/36045]\tLoss: 526.0077\n",
      "Training Epoch: 2 [12750/36045]\tLoss: 528.6615\n",
      "Training Epoch: 2 [12780/36045]\tLoss: 508.2488\n",
      "Training Epoch: 2 [12810/36045]\tLoss: 545.4692\n",
      "Training Epoch: 2 [12840/36045]\tLoss: 549.3822\n",
      "Training Epoch: 2 [12870/36045]\tLoss: 531.6929\n",
      "Training Epoch: 2 [12900/36045]\tLoss: 536.3107\n",
      "Training Epoch: 2 [12930/36045]\tLoss: 528.0817\n",
      "Training Epoch: 2 [12960/36045]\tLoss: 515.4207\n",
      "Training Epoch: 2 [12990/36045]\tLoss: 542.4940\n",
      "Training Epoch: 2 [13020/36045]\tLoss: 501.0683\n",
      "Training Epoch: 2 [13050/36045]\tLoss: 492.3268\n",
      "Training Epoch: 2 [13080/36045]\tLoss: 491.0487\n",
      "Training Epoch: 2 [13110/36045]\tLoss: 497.9399\n",
      "Training Epoch: 2 [13140/36045]\tLoss: 499.9643\n",
      "Training Epoch: 2 [13170/36045]\tLoss: 477.9651\n",
      "Training Epoch: 2 [13200/36045]\tLoss: 471.2603\n",
      "Training Epoch: 2 [13230/36045]\tLoss: 482.4156\n",
      "Training Epoch: 2 [13260/36045]\tLoss: 504.7938\n",
      "Training Epoch: 2 [13290/36045]\tLoss: 544.8638\n",
      "Training Epoch: 2 [13320/36045]\tLoss: 521.6608\n",
      "Training Epoch: 2 [13350/36045]\tLoss: 504.2123\n",
      "Training Epoch: 2 [13380/36045]\tLoss: 520.0748\n",
      "Training Epoch: 2 [13410/36045]\tLoss: 505.1788\n",
      "Training Epoch: 2 [13440/36045]\tLoss: 514.8713\n",
      "Training Epoch: 2 [13470/36045]\tLoss: 514.1142\n",
      "Training Epoch: 2 [13500/36045]\tLoss: 536.7062\n",
      "Training Epoch: 2 [13530/36045]\tLoss: 687.5147\n",
      "Training Epoch: 2 [13560/36045]\tLoss: 643.9913\n",
      "Training Epoch: 2 [13590/36045]\tLoss: 703.6089\n",
      "Training Epoch: 2 [13620/36045]\tLoss: 786.1664\n",
      "Training Epoch: 2 [13650/36045]\tLoss: 797.4056\n",
      "Training Epoch: 2 [13680/36045]\tLoss: 696.6501\n",
      "Training Epoch: 2 [13710/36045]\tLoss: 669.5315\n",
      "Training Epoch: 2 [13740/36045]\tLoss: 489.7986\n",
      "Training Epoch: 2 [13770/36045]\tLoss: 465.7314\n",
      "Training Epoch: 2 [13800/36045]\tLoss: 501.1098\n",
      "Training Epoch: 2 [13830/36045]\tLoss: 466.5681\n",
      "Training Epoch: 2 [13860/36045]\tLoss: 475.9381\n",
      "Training Epoch: 2 [13890/36045]\tLoss: 479.5697\n",
      "Training Epoch: 2 [13920/36045]\tLoss: 481.8858\n",
      "Training Epoch: 2 [13950/36045]\tLoss: 541.2127\n",
      "Training Epoch: 2 [13980/36045]\tLoss: 547.2311\n",
      "Training Epoch: 2 [14010/36045]\tLoss: 550.3524\n",
      "Training Epoch: 2 [14040/36045]\tLoss: 517.9838\n",
      "Training Epoch: 2 [14070/36045]\tLoss: 511.4420\n",
      "Training Epoch: 2 [14100/36045]\tLoss: 525.2491\n",
      "Training Epoch: 2 [14130/36045]\tLoss: 511.3336\n",
      "Training Epoch: 2 [14160/36045]\tLoss: 514.6118\n",
      "Training Epoch: 2 [14190/36045]\tLoss: 527.4437\n",
      "Training Epoch: 2 [14220/36045]\tLoss: 615.4986\n",
      "Training Epoch: 2 [14250/36045]\tLoss: 588.4370\n",
      "Training Epoch: 2 [14280/36045]\tLoss: 588.1195\n",
      "Training Epoch: 2 [14310/36045]\tLoss: 613.1727\n",
      "Training Epoch: 2 [14340/36045]\tLoss: 568.9642\n",
      "Training Epoch: 2 [14370/36045]\tLoss: 567.7069\n",
      "Training Epoch: 2 [14400/36045]\tLoss: 565.4546\n",
      "Training Epoch: 2 [14430/36045]\tLoss: 602.8749\n",
      "Training Epoch: 2 [14460/36045]\tLoss: 575.6231\n",
      "Training Epoch: 2 [14490/36045]\tLoss: 527.4911\n",
      "Training Epoch: 2 [14520/36045]\tLoss: 531.9880\n",
      "Training Epoch: 2 [14550/36045]\tLoss: 569.6676\n",
      "Training Epoch: 2 [14580/36045]\tLoss: 539.6104\n",
      "Training Epoch: 2 [14610/36045]\tLoss: 537.9691\n",
      "Training Epoch: 2 [14640/36045]\tLoss: 552.4601\n",
      "Training Epoch: 2 [14670/36045]\tLoss: 553.0378\n",
      "Training Epoch: 2 [14700/36045]\tLoss: 486.9892\n",
      "Training Epoch: 2 [14730/36045]\tLoss: 433.0838\n",
      "Training Epoch: 2 [14760/36045]\tLoss: 451.4326\n",
      "Training Epoch: 2 [14790/36045]\tLoss: 447.7699\n",
      "Training Epoch: 2 [14820/36045]\tLoss: 439.1828\n",
      "Training Epoch: 2 [14850/36045]\tLoss: 456.4980\n",
      "Training Epoch: 2 [14880/36045]\tLoss: 435.0110\n",
      "Training Epoch: 2 [14910/36045]\tLoss: 444.2564\n",
      "Training Epoch: 2 [14940/36045]\tLoss: 448.5966\n",
      "Training Epoch: 2 [14970/36045]\tLoss: 478.8339\n",
      "Training Epoch: 2 [15000/36045]\tLoss: 443.7077\n",
      "Training Epoch: 2 [15030/36045]\tLoss: 467.1552\n",
      "Training Epoch: 2 [15060/36045]\tLoss: 434.3998\n",
      "Training Epoch: 2 [15090/36045]\tLoss: 435.8144\n",
      "Training Epoch: 2 [15120/36045]\tLoss: 467.7151\n",
      "Training Epoch: 2 [15150/36045]\tLoss: 407.0548\n",
      "Training Epoch: 2 [15180/36045]\tLoss: 410.7959\n",
      "Training Epoch: 2 [15210/36045]\tLoss: 410.7827\n",
      "Training Epoch: 2 [15240/36045]\tLoss: 422.7331\n",
      "Training Epoch: 2 [15270/36045]\tLoss: 403.7109\n",
      "Training Epoch: 2 [15300/36045]\tLoss: 422.6362\n",
      "Training Epoch: 2 [15330/36045]\tLoss: 416.4007\n",
      "Training Epoch: 2 [15360/36045]\tLoss: 418.4496\n",
      "Training Epoch: 2 [15390/36045]\tLoss: 398.7881\n",
      "Training Epoch: 2 [15420/36045]\tLoss: 400.8505\n",
      "Training Epoch: 2 [15450/36045]\tLoss: 385.5698\n",
      "Training Epoch: 2 [15480/36045]\tLoss: 403.6132\n",
      "Training Epoch: 2 [15510/36045]\tLoss: 389.5815\n",
      "Training Epoch: 2 [15540/36045]\tLoss: 397.0504\n",
      "Training Epoch: 2 [15570/36045]\tLoss: 444.9154\n",
      "Training Epoch: 2 [15600/36045]\tLoss: 452.3284\n",
      "Training Epoch: 2 [15630/36045]\tLoss: 488.5242\n",
      "Training Epoch: 2 [15660/36045]\tLoss: 449.8263\n",
      "Training Epoch: 2 [15690/36045]\tLoss: 454.8112\n",
      "Training Epoch: 2 [15720/36045]\tLoss: 477.3862\n",
      "Training Epoch: 2 [15750/36045]\tLoss: 442.0146\n",
      "Training Epoch: 2 [15780/36045]\tLoss: 454.4417\n",
      "Training Epoch: 2 [15810/36045]\tLoss: 460.6299\n",
      "Training Epoch: 2 [15840/36045]\tLoss: 458.7726\n",
      "Training Epoch: 2 [15870/36045]\tLoss: 463.1826\n",
      "Training Epoch: 2 [15900/36045]\tLoss: 488.4353\n",
      "Training Epoch: 2 [15930/36045]\tLoss: 507.2785\n",
      "Training Epoch: 2 [15960/36045]\tLoss: 491.2634\n",
      "Training Epoch: 2 [15990/36045]\tLoss: 452.5809\n",
      "Training Epoch: 2 [16020/36045]\tLoss: 428.6767\n",
      "Training Epoch: 2 [16050/36045]\tLoss: 436.2584\n",
      "Training Epoch: 2 [16080/36045]\tLoss: 406.2798\n",
      "Training Epoch: 2 [16110/36045]\tLoss: 397.5213\n",
      "Training Epoch: 2 [16140/36045]\tLoss: 385.5752\n",
      "Training Epoch: 2 [16170/36045]\tLoss: 453.7797\n",
      "Training Epoch: 2 [16200/36045]\tLoss: 477.1553\n",
      "Training Epoch: 2 [16230/36045]\tLoss: 465.6816\n",
      "Training Epoch: 2 [16260/36045]\tLoss: 569.2277\n",
      "Training Epoch: 2 [16290/36045]\tLoss: 550.6771\n",
      "Training Epoch: 2 [16320/36045]\tLoss: 538.7444\n",
      "Training Epoch: 2 [16350/36045]\tLoss: 579.8759\n",
      "Training Epoch: 2 [16380/36045]\tLoss: 538.0360\n",
      "Training Epoch: 2 [16410/36045]\tLoss: 539.4210\n",
      "Training Epoch: 2 [16440/36045]\tLoss: 527.3923\n",
      "Training Epoch: 2 [16470/36045]\tLoss: 528.4599\n",
      "Training Epoch: 2 [16500/36045]\tLoss: 518.6578\n",
      "Training Epoch: 2 [16530/36045]\tLoss: 485.4885\n",
      "Training Epoch: 2 [16560/36045]\tLoss: 498.5368\n",
      "Training Epoch: 2 [16590/36045]\tLoss: 517.8773\n",
      "Training Epoch: 2 [16620/36045]\tLoss: 511.5023\n",
      "Training Epoch: 2 [16650/36045]\tLoss: 528.1452\n",
      "Training Epoch: 2 [16680/36045]\tLoss: 506.1921\n",
      "Training Epoch: 2 [16710/36045]\tLoss: 508.8471\n",
      "Training Epoch: 2 [16740/36045]\tLoss: 499.2721\n",
      "Training Epoch: 2 [16770/36045]\tLoss: 492.8242\n",
      "Training Epoch: 2 [16800/36045]\tLoss: 505.4244\n",
      "Training Epoch: 2 [16830/36045]\tLoss: 467.1770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [16860/36045]\tLoss: 495.9269\n",
      "Training Epoch: 2 [16890/36045]\tLoss: 471.3844\n",
      "Training Epoch: 2 [16920/36045]\tLoss: 531.3173\n",
      "Training Epoch: 2 [16950/36045]\tLoss: 510.6346\n",
      "Training Epoch: 2 [16980/36045]\tLoss: 510.8799\n",
      "Training Epoch: 2 [17010/36045]\tLoss: 496.2642\n",
      "Training Epoch: 2 [17040/36045]\tLoss: 514.5751\n",
      "Training Epoch: 2 [17070/36045]\tLoss: 525.8785\n",
      "Training Epoch: 2 [17100/36045]\tLoss: 516.7512\n",
      "Training Epoch: 2 [17130/36045]\tLoss: 460.2379\n",
      "Training Epoch: 2 [17160/36045]\tLoss: 414.4893\n",
      "Training Epoch: 2 [17190/36045]\tLoss: 394.1818\n",
      "Training Epoch: 2 [17220/36045]\tLoss: 426.6390\n",
      "Training Epoch: 2 [17250/36045]\tLoss: 433.8892\n",
      "Training Epoch: 2 [17280/36045]\tLoss: 460.5475\n",
      "Training Epoch: 2 [17310/36045]\tLoss: 430.2480\n",
      "Training Epoch: 2 [17340/36045]\tLoss: 452.4840\n",
      "Training Epoch: 2 [17370/36045]\tLoss: 453.9222\n",
      "Training Epoch: 2 [17400/36045]\tLoss: 463.7976\n",
      "Training Epoch: 2 [17430/36045]\tLoss: 480.9719\n",
      "Training Epoch: 2 [17460/36045]\tLoss: 477.5343\n",
      "Training Epoch: 2 [17490/36045]\tLoss: 464.3168\n",
      "Training Epoch: 2 [17520/36045]\tLoss: 454.0706\n",
      "Training Epoch: 2 [17550/36045]\tLoss: 473.7928\n",
      "Training Epoch: 2 [17580/36045]\tLoss: 451.3416\n",
      "Training Epoch: 2 [17610/36045]\tLoss: 462.7106\n",
      "Training Epoch: 2 [17640/36045]\tLoss: 475.5579\n",
      "Training Epoch: 2 [17670/36045]\tLoss: 447.0023\n",
      "Training Epoch: 2 [17700/36045]\tLoss: 452.9003\n",
      "Training Epoch: 2 [17730/36045]\tLoss: 460.5859\n",
      "Training Epoch: 2 [17760/36045]\tLoss: 465.0192\n",
      "Training Epoch: 2 [17790/36045]\tLoss: 453.0407\n",
      "Training Epoch: 2 [17820/36045]\tLoss: 466.0366\n",
      "Training Epoch: 2 [17850/36045]\tLoss: 484.1475\n",
      "Training Epoch: 2 [17880/36045]\tLoss: 528.8326\n",
      "Training Epoch: 2 [17910/36045]\tLoss: 490.3745\n",
      "Training Epoch: 2 [17940/36045]\tLoss: 520.5034\n",
      "Training Epoch: 2 [17970/36045]\tLoss: 530.1195\n",
      "Training Epoch: 2 [18000/36045]\tLoss: 518.1643\n",
      "Training Epoch: 2 [18030/36045]\tLoss: 559.9886\n",
      "Training Epoch: 2 [18060/36045]\tLoss: 555.4944\n",
      "Training Epoch: 2 [18090/36045]\tLoss: 540.3983\n",
      "Training Epoch: 2 [18120/36045]\tLoss: 556.9706\n",
      "Training Epoch: 2 [18150/36045]\tLoss: 583.5787\n",
      "Training Epoch: 2 [18180/36045]\tLoss: 548.0930\n",
      "Training Epoch: 2 [18210/36045]\tLoss: 566.6940\n",
      "Training Epoch: 2 [18240/36045]\tLoss: 589.5585\n",
      "Training Epoch: 2 [18270/36045]\tLoss: 524.6852\n",
      "Training Epoch: 2 [18300/36045]\tLoss: 542.2206\n",
      "Training Epoch: 2 [18330/36045]\tLoss: 614.0085\n",
      "Training Epoch: 2 [18360/36045]\tLoss: 594.6635\n",
      "Training Epoch: 2 [18390/36045]\tLoss: 596.1354\n",
      "Training Epoch: 2 [18420/36045]\tLoss: 557.5590\n",
      "Training Epoch: 2 [18450/36045]\tLoss: 586.6454\n",
      "Training Epoch: 2 [18480/36045]\tLoss: 556.2927\n",
      "Training Epoch: 2 [18510/36045]\tLoss: 576.6127\n",
      "Training Epoch: 2 [18540/36045]\tLoss: 558.0051\n",
      "Training Epoch: 2 [18570/36045]\tLoss: 527.7428\n",
      "Training Epoch: 2 [18600/36045]\tLoss: 555.6133\n",
      "Training Epoch: 2 [18630/36045]\tLoss: 591.9916\n",
      "Training Epoch: 2 [18660/36045]\tLoss: 575.3255\n",
      "Training Epoch: 2 [18690/36045]\tLoss: 641.1681\n",
      "Training Epoch: 2 [18720/36045]\tLoss: 597.9201\n",
      "Training Epoch: 2 [18750/36045]\tLoss: 614.5831\n",
      "Training Epoch: 2 [18780/36045]\tLoss: 648.9869\n",
      "Training Epoch: 2 [18810/36045]\tLoss: 578.6487\n",
      "Training Epoch: 2 [18840/36045]\tLoss: 576.4747\n",
      "Training Epoch: 2 [18870/36045]\tLoss: 599.5535\n",
      "Training Epoch: 2 [18900/36045]\tLoss: 624.8627\n",
      "Training Epoch: 2 [18930/36045]\tLoss: 597.4483\n",
      "Training Epoch: 2 [18960/36045]\tLoss: 483.3159\n",
      "Training Epoch: 2 [18990/36045]\tLoss: 427.7744\n",
      "Training Epoch: 2 [19020/36045]\tLoss: 434.1441\n",
      "Training Epoch: 2 [19050/36045]\tLoss: 419.4281\n",
      "Training Epoch: 2 [19080/36045]\tLoss: 432.3324\n",
      "Training Epoch: 2 [19110/36045]\tLoss: 431.3389\n",
      "Training Epoch: 2 [19140/36045]\tLoss: 414.7812\n",
      "Training Epoch: 2 [19170/36045]\tLoss: 435.8301\n",
      "Training Epoch: 2 [19200/36045]\tLoss: 465.5515\n",
      "Training Epoch: 2 [19230/36045]\tLoss: 473.2815\n",
      "Training Epoch: 2 [19260/36045]\tLoss: 490.5939\n",
      "Training Epoch: 2 [19290/36045]\tLoss: 478.9651\n",
      "Training Epoch: 2 [19320/36045]\tLoss: 469.5827\n",
      "Training Epoch: 2 [19350/36045]\tLoss: 475.5453\n",
      "Training Epoch: 2 [19380/36045]\tLoss: 495.1784\n",
      "Training Epoch: 2 [19410/36045]\tLoss: 479.1031\n",
      "Training Epoch: 2 [19440/36045]\tLoss: 473.5930\n",
      "Training Epoch: 2 [19470/36045]\tLoss: 460.7614\n",
      "Training Epoch: 2 [19500/36045]\tLoss: 489.5966\n",
      "Training Epoch: 2 [19530/36045]\tLoss: 470.4843\n",
      "Training Epoch: 2 [19560/36045]\tLoss: 478.1478\n",
      "Training Epoch: 2 [19590/36045]\tLoss: 487.8048\n",
      "Training Epoch: 2 [19620/36045]\tLoss: 694.6088\n",
      "Training Epoch: 2 [19650/36045]\tLoss: 704.2277\n",
      "Training Epoch: 2 [19680/36045]\tLoss: 661.8121\n",
      "Training Epoch: 2 [19710/36045]\tLoss: 656.6874\n",
      "Training Epoch: 2 [19740/36045]\tLoss: 677.2042\n",
      "Training Epoch: 2 [19770/36045]\tLoss: 699.8933\n",
      "Training Epoch: 2 [19800/36045]\tLoss: 669.0517\n",
      "Training Epoch: 2 [19830/36045]\tLoss: 418.0788\n",
      "Training Epoch: 2 [19860/36045]\tLoss: 423.9241\n",
      "Training Epoch: 2 [19890/36045]\tLoss: 418.0400\n",
      "Training Epoch: 2 [19920/36045]\tLoss: 418.3108\n",
      "Training Epoch: 2 [19950/36045]\tLoss: 392.6575\n",
      "Training Epoch: 2 [19980/36045]\tLoss: 408.7754\n",
      "Training Epoch: 2 [20010/36045]\tLoss: 425.6963\n",
      "Training Epoch: 2 [20040/36045]\tLoss: 459.6515\n",
      "Training Epoch: 2 [20070/36045]\tLoss: 456.7637\n",
      "Training Epoch: 2 [20100/36045]\tLoss: 469.2370\n",
      "Training Epoch: 2 [20130/36045]\tLoss: 468.0220\n",
      "Training Epoch: 2 [20160/36045]\tLoss: 456.1364\n",
      "Training Epoch: 2 [20190/36045]\tLoss: 462.9501\n",
      "Training Epoch: 2 [20220/36045]\tLoss: 455.1959\n",
      "Training Epoch: 2 [20250/36045]\tLoss: 533.9288\n",
      "Training Epoch: 2 [20280/36045]\tLoss: 549.2078\n",
      "Training Epoch: 2 [20310/36045]\tLoss: 528.1515\n",
      "Training Epoch: 2 [20340/36045]\tLoss: 550.4088\n",
      "Training Epoch: 2 [20370/36045]\tLoss: 564.2536\n",
      "Training Epoch: 2 [20400/36045]\tLoss: 578.9031\n",
      "Training Epoch: 2 [20430/36045]\tLoss: 530.6588\n",
      "Training Epoch: 2 [20460/36045]\tLoss: 534.1704\n",
      "Training Epoch: 2 [20490/36045]\tLoss: 532.3521\n",
      "Training Epoch: 2 [20520/36045]\tLoss: 480.8880\n",
      "Training Epoch: 2 [20550/36045]\tLoss: 450.8528\n",
      "Training Epoch: 2 [20580/36045]\tLoss: 467.0024\n",
      "Training Epoch: 2 [20610/36045]\tLoss: 456.4743\n",
      "Training Epoch: 2 [20640/36045]\tLoss: 446.3718\n",
      "Training Epoch: 2 [20670/36045]\tLoss: 441.1635\n",
      "Training Epoch: 2 [20700/36045]\tLoss: 455.4045\n",
      "Training Epoch: 2 [20730/36045]\tLoss: 459.6609\n",
      "Training Epoch: 2 [20760/36045]\tLoss: 520.9048\n",
      "Training Epoch: 2 [20790/36045]\tLoss: 519.3920\n",
      "Training Epoch: 2 [20820/36045]\tLoss: 509.9948\n",
      "Training Epoch: 2 [20850/36045]\tLoss: 512.5416\n",
      "Training Epoch: 2 [20880/36045]\tLoss: 544.9263\n",
      "Training Epoch: 2 [20910/36045]\tLoss: 536.6199\n",
      "Training Epoch: 2 [20940/36045]\tLoss: 514.7661\n",
      "Training Epoch: 2 [20970/36045]\tLoss: 518.7037\n",
      "Training Epoch: 2 [21000/36045]\tLoss: 474.1089\n",
      "Training Epoch: 2 [21030/36045]\tLoss: 423.0007\n",
      "Training Epoch: 2 [21060/36045]\tLoss: 410.2301\n",
      "Training Epoch: 2 [21090/36045]\tLoss: 432.1631\n",
      "Training Epoch: 2 [21120/36045]\tLoss: 441.9489\n",
      "Training Epoch: 2 [21150/36045]\tLoss: 457.0246\n",
      "Training Epoch: 2 [21180/36045]\tLoss: 459.2562\n",
      "Training Epoch: 2 [21210/36045]\tLoss: 450.3002\n",
      "Training Epoch: 2 [21240/36045]\tLoss: 430.2709\n",
      "Training Epoch: 2 [21270/36045]\tLoss: 467.7547\n",
      "Training Epoch: 2 [21300/36045]\tLoss: 504.0575\n",
      "Training Epoch: 2 [21330/36045]\tLoss: 490.3890\n",
      "Training Epoch: 2 [21360/36045]\tLoss: 498.4095\n",
      "Training Epoch: 2 [21390/36045]\tLoss: 499.9891\n",
      "Training Epoch: 2 [21420/36045]\tLoss: 502.8305\n",
      "Training Epoch: 2 [21450/36045]\tLoss: 495.8578\n",
      "Training Epoch: 2 [21480/36045]\tLoss: 495.0328\n",
      "Training Epoch: 2 [21510/36045]\tLoss: 542.0001\n",
      "Training Epoch: 2 [21540/36045]\tLoss: 596.1179\n",
      "Training Epoch: 2 [21570/36045]\tLoss: 614.4522\n",
      "Training Epoch: 2 [21600/36045]\tLoss: 590.8953\n",
      "Training Epoch: 2 [21630/36045]\tLoss: 623.4704\n",
      "Training Epoch: 2 [21660/36045]\tLoss: 590.0751\n",
      "Training Epoch: 2 [21690/36045]\tLoss: 637.8496\n",
      "Training Epoch: 2 [21720/36045]\tLoss: 566.9112\n",
      "Training Epoch: 2 [21750/36045]\tLoss: 601.4891\n",
      "Training Epoch: 2 [21780/36045]\tLoss: 423.0270\n",
      "Training Epoch: 2 [21810/36045]\tLoss: 423.6109\n",
      "Training Epoch: 2 [21840/36045]\tLoss: 421.9680\n",
      "Training Epoch: 2 [21870/36045]\tLoss: 427.3447\n",
      "Training Epoch: 2 [21900/36045]\tLoss: 403.7418\n",
      "Training Epoch: 2 [21930/36045]\tLoss: 418.1092\n",
      "Training Epoch: 2 [21960/36045]\tLoss: 424.3590\n",
      "Training Epoch: 2 [21990/36045]\tLoss: 420.5691\n",
      "Training Epoch: 2 [22020/36045]\tLoss: 438.6814\n",
      "Training Epoch: 2 [22050/36045]\tLoss: 444.4204\n",
      "Training Epoch: 2 [22080/36045]\tLoss: 436.4213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [22110/36045]\tLoss: 425.4095\n",
      "Training Epoch: 2 [22140/36045]\tLoss: 422.8157\n",
      "Training Epoch: 2 [22170/36045]\tLoss: 421.8476\n",
      "Training Epoch: 2 [22200/36045]\tLoss: 444.7056\n",
      "Training Epoch: 2 [22230/36045]\tLoss: 446.6365\n",
      "Training Epoch: 2 [22260/36045]\tLoss: 432.9251\n",
      "Training Epoch: 2 [22290/36045]\tLoss: 479.9900\n",
      "Training Epoch: 2 [22320/36045]\tLoss: 525.6042\n",
      "Training Epoch: 2 [22350/36045]\tLoss: 525.9236\n",
      "Training Epoch: 2 [22380/36045]\tLoss: 528.0861\n",
      "Training Epoch: 2 [22410/36045]\tLoss: 512.3497\n",
      "Training Epoch: 2 [22440/36045]\tLoss: 506.3732\n",
      "Training Epoch: 2 [22470/36045]\tLoss: 525.3086\n",
      "Training Epoch: 2 [22500/36045]\tLoss: 501.8947\n",
      "Training Epoch: 2 [22530/36045]\tLoss: 537.1650\n",
      "Training Epoch: 2 [22560/36045]\tLoss: 529.3901\n",
      "Training Epoch: 2 [22590/36045]\tLoss: 560.5935\n",
      "Training Epoch: 2 [22620/36045]\tLoss: 598.1628\n",
      "Training Epoch: 2 [22650/36045]\tLoss: 603.1258\n",
      "Training Epoch: 2 [22680/36045]\tLoss: 609.3254\n",
      "Training Epoch: 2 [22710/36045]\tLoss: 624.5020\n",
      "Training Epoch: 2 [22740/36045]\tLoss: 642.1473\n",
      "Training Epoch: 2 [22770/36045]\tLoss: 643.5229\n",
      "Training Epoch: 2 [22800/36045]\tLoss: 663.2963\n",
      "Training Epoch: 2 [22830/36045]\tLoss: 536.5641\n",
      "Training Epoch: 2 [22860/36045]\tLoss: 525.8467\n",
      "Training Epoch: 2 [22890/36045]\tLoss: 546.2137\n",
      "Training Epoch: 2 [22920/36045]\tLoss: 541.2294\n",
      "Training Epoch: 2 [22950/36045]\tLoss: 503.5739\n",
      "Training Epoch: 2 [22980/36045]\tLoss: 508.3711\n",
      "Training Epoch: 2 [23010/36045]\tLoss: 513.4057\n",
      "Training Epoch: 2 [23040/36045]\tLoss: 464.0007\n",
      "Training Epoch: 2 [23070/36045]\tLoss: 456.8932\n",
      "Training Epoch: 2 [23100/36045]\tLoss: 489.0307\n",
      "Training Epoch: 2 [23130/36045]\tLoss: 480.0360\n",
      "Training Epoch: 2 [23160/36045]\tLoss: 446.5140\n",
      "Training Epoch: 2 [23190/36045]\tLoss: 432.8413\n",
      "Training Epoch: 2 [23220/36045]\tLoss: 471.5379\n",
      "Training Epoch: 2 [23250/36045]\tLoss: 431.8065\n",
      "Training Epoch: 2 [23280/36045]\tLoss: 441.3379\n",
      "Training Epoch: 2 [23310/36045]\tLoss: 435.2420\n",
      "Training Epoch: 2 [23340/36045]\tLoss: 462.4038\n",
      "Training Epoch: 2 [23370/36045]\tLoss: 475.7549\n",
      "Training Epoch: 2 [23400/36045]\tLoss: 501.5079\n",
      "Training Epoch: 2 [23430/36045]\tLoss: 471.5867\n",
      "Training Epoch: 2 [23460/36045]\tLoss: 504.7914\n",
      "Training Epoch: 2 [23490/36045]\tLoss: 461.0771\n",
      "Training Epoch: 2 [23520/36045]\tLoss: 481.7905\n",
      "Training Epoch: 2 [23550/36045]\tLoss: 521.2067\n",
      "Training Epoch: 2 [23580/36045]\tLoss: 586.4175\n",
      "Training Epoch: 2 [23610/36045]\tLoss: 571.1525\n",
      "Training Epoch: 2 [23640/36045]\tLoss: 603.4918\n",
      "Training Epoch: 2 [23670/36045]\tLoss: 559.9114\n",
      "Training Epoch: 2 [23700/36045]\tLoss: 604.2020\n",
      "Training Epoch: 2 [23730/36045]\tLoss: 572.5482\n",
      "Training Epoch: 2 [23760/36045]\tLoss: 525.6613\n",
      "Training Epoch: 2 [23790/36045]\tLoss: 461.7680\n",
      "Training Epoch: 2 [23820/36045]\tLoss: 493.0023\n",
      "Training Epoch: 2 [23850/36045]\tLoss: 485.5807\n",
      "Training Epoch: 2 [23880/36045]\tLoss: 473.2621\n",
      "Training Epoch: 2 [23910/36045]\tLoss: 503.2570\n",
      "Training Epoch: 2 [23940/36045]\tLoss: 437.3516\n",
      "Training Epoch: 2 [23970/36045]\tLoss: 448.4431\n",
      "Training Epoch: 2 [24000/36045]\tLoss: 449.8280\n",
      "Training Epoch: 2 [24030/36045]\tLoss: 410.0762\n",
      "Training Epoch: 2 [24060/36045]\tLoss: 407.4395\n",
      "Training Epoch: 2 [24090/36045]\tLoss: 434.3156\n",
      "Training Epoch: 2 [24120/36045]\tLoss: 436.0795\n",
      "Training Epoch: 2 [24150/36045]\tLoss: 420.6129\n",
      "Training Epoch: 2 [24180/36045]\tLoss: 422.2305\n",
      "Training Epoch: 2 [24210/36045]\tLoss: 411.5313\n",
      "Training Epoch: 2 [24240/36045]\tLoss: 419.5507\n",
      "Training Epoch: 2 [24270/36045]\tLoss: 415.0299\n",
      "Training Epoch: 2 [24300/36045]\tLoss: 457.4255\n",
      "Training Epoch: 2 [24330/36045]\tLoss: 461.4196\n",
      "Training Epoch: 2 [24360/36045]\tLoss: 449.3727\n",
      "Training Epoch: 2 [24390/36045]\tLoss: 457.8702\n",
      "Training Epoch: 2 [24420/36045]\tLoss: 457.4102\n",
      "Training Epoch: 2 [24450/36045]\tLoss: 438.4891\n",
      "Training Epoch: 2 [24480/36045]\tLoss: 473.0022\n",
      "Training Epoch: 2 [24510/36045]\tLoss: 483.1617\n",
      "Training Epoch: 2 [24540/36045]\tLoss: 572.8359\n",
      "Training Epoch: 2 [24570/36045]\tLoss: 516.4329\n",
      "Training Epoch: 2 [24600/36045]\tLoss: 557.6352\n",
      "Training Epoch: 2 [24630/36045]\tLoss: 509.2581\n",
      "Training Epoch: 2 [24660/36045]\tLoss: 527.8788\n",
      "Training Epoch: 2 [24690/36045]\tLoss: 524.7964\n",
      "Training Epoch: 2 [24720/36045]\tLoss: 532.8166\n",
      "Training Epoch: 2 [24750/36045]\tLoss: 456.2428\n",
      "Training Epoch: 2 [24780/36045]\tLoss: 374.2133\n",
      "Training Epoch: 2 [24810/36045]\tLoss: 411.1512\n",
      "Training Epoch: 2 [24840/36045]\tLoss: 394.8924\n",
      "Training Epoch: 2 [24870/36045]\tLoss: 396.8808\n",
      "Training Epoch: 2 [24900/36045]\tLoss: 399.5483\n",
      "Training Epoch: 2 [24930/36045]\tLoss: 400.8045\n",
      "Training Epoch: 2 [24960/36045]\tLoss: 394.8958\n",
      "Training Epoch: 2 [24990/36045]\tLoss: 400.2188\n",
      "Training Epoch: 2 [25020/36045]\tLoss: 376.1311\n",
      "Training Epoch: 2 [25050/36045]\tLoss: 354.9443\n",
      "Training Epoch: 2 [25080/36045]\tLoss: 342.6042\n",
      "Training Epoch: 2 [25110/36045]\tLoss: 319.9665\n",
      "Training Epoch: 2 [25140/36045]\tLoss: 297.0352\n",
      "Training Epoch: 2 [25170/36045]\tLoss: 305.8657\n",
      "Training Epoch: 2 [25200/36045]\tLoss: 306.1801\n",
      "Training Epoch: 2 [25230/36045]\tLoss: 296.7342\n",
      "Training Epoch: 2 [25260/36045]\tLoss: 390.6302\n",
      "Training Epoch: 2 [25290/36045]\tLoss: 432.2674\n",
      "Training Epoch: 2 [25320/36045]\tLoss: 435.6300\n",
      "Training Epoch: 2 [25350/36045]\tLoss: 419.5875\n",
      "Training Epoch: 2 [25380/36045]\tLoss: 405.7322\n",
      "Training Epoch: 2 [25410/36045]\tLoss: 392.3760\n",
      "Training Epoch: 2 [25440/36045]\tLoss: 392.3239\n",
      "Training Epoch: 2 [25470/36045]\tLoss: 448.2216\n",
      "Training Epoch: 2 [25500/36045]\tLoss: 429.0353\n",
      "Training Epoch: 2 [25530/36045]\tLoss: 511.6033\n",
      "Training Epoch: 2 [25560/36045]\tLoss: 495.6154\n",
      "Training Epoch: 2 [25590/36045]\tLoss: 509.2051\n",
      "Training Epoch: 2 [25620/36045]\tLoss: 493.7822\n",
      "Training Epoch: 2 [25650/36045]\tLoss: 494.4629\n",
      "Training Epoch: 2 [25680/36045]\tLoss: 494.8035\n",
      "Training Epoch: 2 [25710/36045]\tLoss: 508.1774\n",
      "Training Epoch: 2 [25740/36045]\tLoss: 507.4139\n",
      "Training Epoch: 2 [25770/36045]\tLoss: 330.7903\n",
      "Training Epoch: 2 [25800/36045]\tLoss: 307.6588\n",
      "Training Epoch: 2 [25830/36045]\tLoss: 317.9189\n",
      "Training Epoch: 2 [25860/36045]\tLoss: 311.1134\n",
      "Training Epoch: 2 [25890/36045]\tLoss: 290.7203\n",
      "Training Epoch: 2 [25920/36045]\tLoss: 322.0592\n",
      "Training Epoch: 2 [25950/36045]\tLoss: 313.3600\n",
      "Training Epoch: 2 [25980/36045]\tLoss: 300.6888\n",
      "Training Epoch: 2 [26010/36045]\tLoss: 503.9940\n",
      "Training Epoch: 2 [26040/36045]\tLoss: 522.7001\n",
      "Training Epoch: 2 [26070/36045]\tLoss: 506.8722\n",
      "Training Epoch: 2 [26100/36045]\tLoss: 544.1356\n",
      "Training Epoch: 2 [26130/36045]\tLoss: 507.6156\n",
      "Training Epoch: 2 [26160/36045]\tLoss: 562.1627\n",
      "Training Epoch: 2 [26190/36045]\tLoss: 505.9138\n",
      "Training Epoch: 2 [26220/36045]\tLoss: 532.3151\n",
      "Training Epoch: 2 [26250/36045]\tLoss: 533.8509\n",
      "Training Epoch: 2 [26280/36045]\tLoss: 509.3348\n",
      "Training Epoch: 2 [26310/36045]\tLoss: 510.2173\n",
      "Training Epoch: 2 [26340/36045]\tLoss: 535.6310\n",
      "Training Epoch: 2 [26370/36045]\tLoss: 533.8772\n",
      "Training Epoch: 2 [26400/36045]\tLoss: 490.1382\n",
      "Training Epoch: 2 [26430/36045]\tLoss: 430.7145\n",
      "Training Epoch: 2 [26460/36045]\tLoss: 423.8273\n",
      "Training Epoch: 2 [26490/36045]\tLoss: 538.3795\n",
      "Training Epoch: 2 [26520/36045]\tLoss: 509.4704\n",
      "Training Epoch: 2 [26550/36045]\tLoss: 503.5816\n",
      "Training Epoch: 2 [26580/36045]\tLoss: 498.2598\n",
      "Training Epoch: 2 [26610/36045]\tLoss: 506.0883\n",
      "Training Epoch: 2 [26640/36045]\tLoss: 503.9436\n",
      "Training Epoch: 2 [26670/36045]\tLoss: 502.9681\n",
      "Training Epoch: 2 [26700/36045]\tLoss: 497.0357\n",
      "Training Epoch: 2 [26730/36045]\tLoss: 506.1809\n",
      "Training Epoch: 2 [26760/36045]\tLoss: 375.8596\n",
      "Training Epoch: 2 [26790/36045]\tLoss: 347.5371\n",
      "Training Epoch: 2 [26820/36045]\tLoss: 332.1928\n",
      "Training Epoch: 2 [26850/36045]\tLoss: 242.5984\n",
      "Training Epoch: 2 [26880/36045]\tLoss: 277.6180\n",
      "Training Epoch: 2 [26910/36045]\tLoss: 290.5424\n",
      "Training Epoch: 2 [26940/36045]\tLoss: 309.7268\n",
      "Training Epoch: 2 [26970/36045]\tLoss: 421.5676\n",
      "Training Epoch: 2 [27000/36045]\tLoss: 564.4655\n",
      "Training Epoch: 2 [27030/36045]\tLoss: 547.9713\n",
      "Training Epoch: 2 [27060/36045]\tLoss: 540.1192\n",
      "Training Epoch: 2 [27090/36045]\tLoss: 526.5742\n",
      "Training Epoch: 2 [27120/36045]\tLoss: 541.1597\n",
      "Training Epoch: 2 [27150/36045]\tLoss: 591.1177\n",
      "Training Epoch: 2 [27180/36045]\tLoss: 429.9016\n",
      "Training Epoch: 2 [27210/36045]\tLoss: 365.8290\n",
      "Training Epoch: 2 [27240/36045]\tLoss: 395.9572\n",
      "Training Epoch: 2 [27270/36045]\tLoss: 385.2585\n",
      "Training Epoch: 2 [27300/36045]\tLoss: 369.4737\n",
      "Training Epoch: 2 [27330/36045]\tLoss: 375.7738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [27360/36045]\tLoss: 373.0784\n",
      "Training Epoch: 2 [27390/36045]\tLoss: 374.4119\n",
      "Training Epoch: 2 [27420/36045]\tLoss: 410.9880\n",
      "Training Epoch: 2 [27450/36045]\tLoss: 505.6653\n",
      "Training Epoch: 2 [27480/36045]\tLoss: 506.7975\n",
      "Training Epoch: 2 [27510/36045]\tLoss: 515.5586\n",
      "Training Epoch: 2 [27540/36045]\tLoss: 501.0588\n",
      "Training Epoch: 2 [27570/36045]\tLoss: 524.7844\n",
      "Training Epoch: 2 [27600/36045]\tLoss: 507.8185\n",
      "Training Epoch: 2 [27630/36045]\tLoss: 512.8701\n",
      "Training Epoch: 2 [27660/36045]\tLoss: 511.4330\n",
      "Training Epoch: 2 [27690/36045]\tLoss: 515.8909\n",
      "Training Epoch: 2 [27720/36045]\tLoss: 554.6657\n",
      "Training Epoch: 2 [27750/36045]\tLoss: 529.4340\n",
      "Training Epoch: 2 [27780/36045]\tLoss: 524.7893\n",
      "Training Epoch: 2 [27810/36045]\tLoss: 524.4899\n",
      "Training Epoch: 2 [27840/36045]\tLoss: 519.1077\n",
      "Training Epoch: 2 [27870/36045]\tLoss: 528.2435\n",
      "Training Epoch: 2 [27900/36045]\tLoss: 452.8544\n",
      "Training Epoch: 2 [27930/36045]\tLoss: 428.9923\n",
      "Training Epoch: 2 [27960/36045]\tLoss: 398.4507\n",
      "Training Epoch: 2 [27990/36045]\tLoss: 397.2665\n",
      "Training Epoch: 2 [28020/36045]\tLoss: 389.4063\n",
      "Training Epoch: 2 [28050/36045]\tLoss: 407.3531\n",
      "Training Epoch: 2 [28080/36045]\tLoss: 398.9729\n",
      "Training Epoch: 2 [28110/36045]\tLoss: 383.8321\n",
      "Training Epoch: 2 [28140/36045]\tLoss: 418.7304\n",
      "Training Epoch: 2 [28170/36045]\tLoss: 408.7295\n",
      "Training Epoch: 2 [28200/36045]\tLoss: 410.1363\n",
      "Training Epoch: 2 [28230/36045]\tLoss: 390.7404\n",
      "Training Epoch: 2 [28260/36045]\tLoss: 415.2542\n",
      "Training Epoch: 2 [28290/36045]\tLoss: 381.9218\n",
      "Training Epoch: 2 [28320/36045]\tLoss: 370.0152\n",
      "Training Epoch: 2 [28350/36045]\tLoss: 390.7234\n",
      "Training Epoch: 2 [28380/36045]\tLoss: 675.8993\n",
      "Training Epoch: 2 [28410/36045]\tLoss: 742.1865\n",
      "Training Epoch: 2 [28440/36045]\tLoss: 674.8329\n",
      "Training Epoch: 2 [28470/36045]\tLoss: 525.3393\n",
      "Training Epoch: 2 [28500/36045]\tLoss: 579.6622\n",
      "Training Epoch: 2 [28530/36045]\tLoss: 579.6508\n",
      "Training Epoch: 2 [28560/36045]\tLoss: 446.6881\n",
      "Training Epoch: 2 [28590/36045]\tLoss: 535.2061\n",
      "Training Epoch: 2 [28620/36045]\tLoss: 559.8348\n",
      "Training Epoch: 2 [28650/36045]\tLoss: 550.7569\n",
      "Training Epoch: 2 [28680/36045]\tLoss: 543.5195\n",
      "Training Epoch: 2 [28710/36045]\tLoss: 552.1499\n",
      "Training Epoch: 2 [28740/36045]\tLoss: 542.4378\n",
      "Training Epoch: 2 [28770/36045]\tLoss: 554.1458\n",
      "Training Epoch: 2 [28800/36045]\tLoss: 550.7067\n",
      "Training Epoch: 2 [28830/36045]\tLoss: 518.9582\n",
      "Training Epoch: 2 [28860/36045]\tLoss: 409.9249\n",
      "Training Epoch: 2 [28890/36045]\tLoss: 393.1384\n",
      "Training Epoch: 2 [28920/36045]\tLoss: 375.6256\n",
      "Training Epoch: 2 [28950/36045]\tLoss: 421.1625\n",
      "Training Epoch: 2 [28980/36045]\tLoss: 395.0026\n",
      "Training Epoch: 2 [29010/36045]\tLoss: 388.5623\n",
      "Training Epoch: 2 [29040/36045]\tLoss: 389.5526\n",
      "Training Epoch: 2 [29070/36045]\tLoss: 413.0574\n",
      "Training Epoch: 2 [29100/36045]\tLoss: 414.0443\n",
      "Training Epoch: 2 [29130/36045]\tLoss: 400.2741\n",
      "Training Epoch: 2 [29160/36045]\tLoss: 400.4101\n",
      "Training Epoch: 2 [29190/36045]\tLoss: 413.9185\n",
      "Training Epoch: 2 [29220/36045]\tLoss: 367.4241\n",
      "Training Epoch: 2 [29250/36045]\tLoss: 390.1761\n",
      "Training Epoch: 2 [29280/36045]\tLoss: 350.0159\n",
      "Training Epoch: 2 [29310/36045]\tLoss: 525.8362\n",
      "Training Epoch: 2 [29340/36045]\tLoss: 489.0468\n",
      "Training Epoch: 2 [29370/36045]\tLoss: 467.1154\n",
      "Training Epoch: 2 [29400/36045]\tLoss: 500.8869\n",
      "Training Epoch: 2 [29430/36045]\tLoss: 517.8879\n",
      "Training Epoch: 2 [29460/36045]\tLoss: 499.7614\n",
      "Training Epoch: 2 [29490/36045]\tLoss: 527.0331\n",
      "Training Epoch: 2 [29520/36045]\tLoss: 506.3821\n",
      "Training Epoch: 2 [29550/36045]\tLoss: 495.3167\n",
      "Training Epoch: 2 [29580/36045]\tLoss: 422.3119\n",
      "Training Epoch: 2 [29610/36045]\tLoss: 422.6917\n",
      "Training Epoch: 2 [29640/36045]\tLoss: 400.5508\n",
      "Training Epoch: 2 [29670/36045]\tLoss: 373.4680\n",
      "Training Epoch: 2 [29700/36045]\tLoss: 360.4038\n",
      "Training Epoch: 2 [29730/36045]\tLoss: 342.5956\n",
      "Training Epoch: 2 [29760/36045]\tLoss: 373.0028\n",
      "Training Epoch: 2 [29790/36045]\tLoss: 390.7932\n",
      "Training Epoch: 2 [29820/36045]\tLoss: 502.0310\n",
      "Training Epoch: 2 [29850/36045]\tLoss: 492.4016\n",
      "Training Epoch: 2 [29880/36045]\tLoss: 489.6954\n",
      "Training Epoch: 2 [29910/36045]\tLoss: 466.8810\n",
      "Training Epoch: 2 [29940/36045]\tLoss: 530.5563\n",
      "Training Epoch: 2 [29970/36045]\tLoss: 509.5155\n",
      "Training Epoch: 2 [30000/36045]\tLoss: 467.7283\n",
      "Training Epoch: 2 [30030/36045]\tLoss: 467.2086\n",
      "Training Epoch: 2 [30060/36045]\tLoss: 544.1033\n",
      "Training Epoch: 2 [30090/36045]\tLoss: 583.8145\n",
      "Training Epoch: 2 [30120/36045]\tLoss: 590.9319\n",
      "Training Epoch: 2 [30150/36045]\tLoss: 576.4026\n",
      "Training Epoch: 2 [30180/36045]\tLoss: 551.7953\n",
      "Training Epoch: 2 [30210/36045]\tLoss: 552.3193\n",
      "Training Epoch: 2 [30240/36045]\tLoss: 603.5320\n",
      "Training Epoch: 2 [30270/36045]\tLoss: 558.7452\n",
      "Training Epoch: 2 [30300/36045]\tLoss: 575.0555\n",
      "Training Epoch: 2 [30330/36045]\tLoss: 406.1159\n",
      "Training Epoch: 2 [30360/36045]\tLoss: 401.5840\n",
      "Training Epoch: 2 [30390/36045]\tLoss: 412.5842\n",
      "Training Epoch: 2 [30420/36045]\tLoss: 392.3186\n",
      "Training Epoch: 2 [30450/36045]\tLoss: 403.2430\n",
      "Training Epoch: 2 [30480/36045]\tLoss: 393.3087\n",
      "Training Epoch: 2 [30510/36045]\tLoss: 332.9580\n",
      "Training Epoch: 2 [30540/36045]\tLoss: 350.0595\n",
      "Training Epoch: 2 [30570/36045]\tLoss: 364.5444\n",
      "Training Epoch: 2 [30600/36045]\tLoss: 335.8134\n",
      "Training Epoch: 2 [30630/36045]\tLoss: 334.2666\n",
      "Training Epoch: 2 [30660/36045]\tLoss: 351.1573\n",
      "Training Epoch: 2 [30690/36045]\tLoss: 354.5735\n",
      "Training Epoch: 2 [30720/36045]\tLoss: 346.6964\n",
      "Training Epoch: 2 [30750/36045]\tLoss: 333.7038\n",
      "Training Epoch: 2 [30780/36045]\tLoss: 370.2262\n",
      "Training Epoch: 2 [30810/36045]\tLoss: 377.1273\n",
      "Training Epoch: 2 [30840/36045]\tLoss: 362.6674\n",
      "Training Epoch: 2 [30870/36045]\tLoss: 367.1800\n",
      "Training Epoch: 2 [30900/36045]\tLoss: 371.6557\n",
      "Training Epoch: 2 [30930/36045]\tLoss: 376.9893\n",
      "Training Epoch: 2 [30960/36045]\tLoss: 418.3395\n",
      "Training Epoch: 2 [30990/36045]\tLoss: 395.2827\n",
      "Training Epoch: 2 [31020/36045]\tLoss: 319.9622\n",
      "Training Epoch: 2 [31050/36045]\tLoss: 324.5761\n",
      "Training Epoch: 2 [31080/36045]\tLoss: 316.9477\n",
      "Training Epoch: 2 [31110/36045]\tLoss: 310.2684\n",
      "Training Epoch: 2 [31140/36045]\tLoss: 320.7057\n",
      "Training Epoch: 2 [31170/36045]\tLoss: 330.2332\n",
      "Training Epoch: 2 [31200/36045]\tLoss: 441.1080\n",
      "Training Epoch: 2 [31230/36045]\tLoss: 509.3098\n",
      "Training Epoch: 2 [31260/36045]\tLoss: 508.1230\n",
      "Training Epoch: 2 [31290/36045]\tLoss: 494.3532\n",
      "Training Epoch: 2 [31320/36045]\tLoss: 483.1312\n",
      "Training Epoch: 2 [31350/36045]\tLoss: 514.1698\n",
      "Training Epoch: 2 [31380/36045]\tLoss: 485.8972\n",
      "Training Epoch: 2 [31410/36045]\tLoss: 466.7097\n",
      "Training Epoch: 2 [31440/36045]\tLoss: 514.6241\n",
      "Training Epoch: 2 [31470/36045]\tLoss: 516.6413\n",
      "Training Epoch: 2 [31500/36045]\tLoss: 495.2308\n",
      "Training Epoch: 2 [31530/36045]\tLoss: 522.6732\n",
      "Training Epoch: 2 [31560/36045]\tLoss: 493.9361\n",
      "Training Epoch: 2 [31590/36045]\tLoss: 481.6414\n",
      "Training Epoch: 2 [31620/36045]\tLoss: 497.8287\n",
      "Training Epoch: 2 [31650/36045]\tLoss: 513.2041\n",
      "Training Epoch: 2 [31680/36045]\tLoss: 413.2012\n",
      "Training Epoch: 2 [31710/36045]\tLoss: 321.8666\n",
      "Training Epoch: 2 [31740/36045]\tLoss: 306.6527\n",
      "Training Epoch: 2 [31770/36045]\tLoss: 300.1922\n",
      "Training Epoch: 2 [31800/36045]\tLoss: 298.2958\n",
      "Training Epoch: 2 [31830/36045]\tLoss: 300.3388\n",
      "Training Epoch: 2 [31860/36045]\tLoss: 307.2684\n",
      "Training Epoch: 2 [31890/36045]\tLoss: 502.6877\n",
      "Training Epoch: 2 [31920/36045]\tLoss: 632.6405\n",
      "Training Epoch: 2 [31950/36045]\tLoss: 657.0050\n",
      "Training Epoch: 2 [31980/36045]\tLoss: 761.1022\n",
      "Training Epoch: 2 [32010/36045]\tLoss: 723.2838\n",
      "Training Epoch: 2 [32040/36045]\tLoss: 694.8464\n",
      "Training Epoch: 2 [32070/36045]\tLoss: 699.8027\n",
      "Training Epoch: 2 [32100/36045]\tLoss: 698.6027\n",
      "Training Epoch: 2 [32130/36045]\tLoss: 528.8455\n",
      "Training Epoch: 2 [32160/36045]\tLoss: 503.0834\n",
      "Training Epoch: 2 [32190/36045]\tLoss: 492.1128\n",
      "Training Epoch: 2 [32220/36045]\tLoss: 521.1262\n",
      "Training Epoch: 2 [32250/36045]\tLoss: 510.4434\n",
      "Training Epoch: 2 [32280/36045]\tLoss: 508.8084\n",
      "Training Epoch: 2 [32310/36045]\tLoss: 486.8424\n",
      "Training Epoch: 2 [32340/36045]\tLoss: 492.9505\n",
      "Training Epoch: 2 [32370/36045]\tLoss: 516.7543\n",
      "Training Epoch: 2 [32400/36045]\tLoss: 435.4041\n",
      "Training Epoch: 2 [32430/36045]\tLoss: 388.1315\n",
      "Training Epoch: 2 [32460/36045]\tLoss: 385.8031\n",
      "Training Epoch: 2 [32490/36045]\tLoss: 383.5216\n",
      "Training Epoch: 2 [32520/36045]\tLoss: 367.5773\n",
      "Training Epoch: 2 [32550/36045]\tLoss: 380.3553\n",
      "Training Epoch: 2 [32580/36045]\tLoss: 380.5302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [32610/36045]\tLoss: 376.7801\n",
      "Training Epoch: 2 [32640/36045]\tLoss: 510.9744\n",
      "Training Epoch: 2 [32670/36045]\tLoss: 547.1160\n",
      "Training Epoch: 2 [32700/36045]\tLoss: 542.2655\n",
      "Training Epoch: 2 [32730/36045]\tLoss: 502.3091\n",
      "Training Epoch: 2 [32760/36045]\tLoss: 530.9139\n",
      "Training Epoch: 2 [32790/36045]\tLoss: 528.4348\n",
      "Training Epoch: 2 [32820/36045]\tLoss: 523.3292\n",
      "Training Epoch: 2 [32850/36045]\tLoss: 464.5369\n",
      "Training Epoch: 2 [32880/36045]\tLoss: 386.2833\n",
      "Training Epoch: 2 [32910/36045]\tLoss: 389.9991\n",
      "Training Epoch: 2 [32940/36045]\tLoss: 394.7760\n",
      "Training Epoch: 2 [32970/36045]\tLoss: 393.2508\n",
      "Training Epoch: 2 [33000/36045]\tLoss: 399.3047\n",
      "Training Epoch: 2 [33030/36045]\tLoss: 386.0124\n",
      "Training Epoch: 2 [33060/36045]\tLoss: 370.6916\n",
      "Training Epoch: 2 [33090/36045]\tLoss: 382.9461\n",
      "Training Epoch: 2 [33120/36045]\tLoss: 619.6291\n",
      "Training Epoch: 2 [33150/36045]\tLoss: 592.3224\n",
      "Training Epoch: 2 [33180/36045]\tLoss: 569.4686\n",
      "Training Epoch: 2 [33210/36045]\tLoss: 603.7356\n",
      "Training Epoch: 2 [33240/36045]\tLoss: 592.6334\n",
      "Training Epoch: 2 [33270/36045]\tLoss: 593.3351\n",
      "Training Epoch: 2 [33300/36045]\tLoss: 665.5694\n",
      "Training Epoch: 2 [33330/36045]\tLoss: 585.7087\n",
      "Training Epoch: 2 [33360/36045]\tLoss: 338.2238\n",
      "Training Epoch: 2 [33390/36045]\tLoss: 348.3518\n",
      "Training Epoch: 2 [33420/36045]\tLoss: 366.8049\n",
      "Training Epoch: 2 [33450/36045]\tLoss: 332.5110\n",
      "Training Epoch: 2 [33480/36045]\tLoss: 361.1946\n",
      "Training Epoch: 2 [33510/36045]\tLoss: 351.6802\n",
      "Training Epoch: 2 [33540/36045]\tLoss: 368.3757\n",
      "Training Epoch: 2 [33570/36045]\tLoss: 356.7635\n",
      "Training Epoch: 2 [33600/36045]\tLoss: 372.4348\n",
      "Training Epoch: 2 [33630/36045]\tLoss: 487.4091\n",
      "Training Epoch: 2 [33660/36045]\tLoss: 488.7444\n",
      "Training Epoch: 2 [33690/36045]\tLoss: 478.0085\n",
      "Training Epoch: 2 [33720/36045]\tLoss: 486.8940\n",
      "Training Epoch: 2 [33750/36045]\tLoss: 491.2024\n",
      "Training Epoch: 2 [33780/36045]\tLoss: 485.2856\n",
      "Training Epoch: 2 [33810/36045]\tLoss: 500.9717\n",
      "Training Epoch: 2 [33840/36045]\tLoss: 481.5841\n",
      "Training Epoch: 2 [33870/36045]\tLoss: 506.7412\n",
      "Training Epoch: 2 [33900/36045]\tLoss: 496.4979\n",
      "Training Epoch: 2 [33930/36045]\tLoss: 509.6425\n",
      "Training Epoch: 2 [33960/36045]\tLoss: 523.0742\n",
      "Training Epoch: 2 [33990/36045]\tLoss: 484.0016\n",
      "Training Epoch: 2 [34020/36045]\tLoss: 503.5005\n",
      "Training Epoch: 2 [34050/36045]\tLoss: 505.9409\n",
      "Training Epoch: 2 [34080/36045]\tLoss: 491.9596\n",
      "Training Epoch: 2 [34110/36045]\tLoss: 448.9681\n",
      "Training Epoch: 2 [34140/36045]\tLoss: 452.1275\n",
      "Training Epoch: 2 [34170/36045]\tLoss: 457.0571\n",
      "Training Epoch: 2 [34200/36045]\tLoss: 407.8188\n",
      "Training Epoch: 2 [34230/36045]\tLoss: 433.0425\n",
      "Training Epoch: 2 [34260/36045]\tLoss: 437.3510\n",
      "Training Epoch: 2 [34290/36045]\tLoss: 350.9066\n",
      "Training Epoch: 2 [34320/36045]\tLoss: 378.0830\n",
      "Training Epoch: 2 [34350/36045]\tLoss: 396.5285\n",
      "Training Epoch: 2 [34380/36045]\tLoss: 389.7742\n",
      "Training Epoch: 2 [34410/36045]\tLoss: 376.4878\n",
      "Training Epoch: 2 [34440/36045]\tLoss: 373.0496\n",
      "Training Epoch: 2 [34470/36045]\tLoss: 362.5610\n",
      "Training Epoch: 2 [34500/36045]\tLoss: 405.5298\n",
      "Training Epoch: 2 [34530/36045]\tLoss: 382.4426\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e1277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
