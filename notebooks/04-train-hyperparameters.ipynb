{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b84576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from models.networks_2022 import BranchedNetwork\n",
    "from data.ReconstructionTrainingDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset, LargeNoisyDataset\n",
    "from data.MergedNoisyDataset import MergedNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fbc7e",
   "metadata": {},
   "source": [
    "# Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb90b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main args\n",
    "SAME_PARAM = False           # to use the same parameters for all pcoders or not\n",
    "noise_types = ['AudScene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fae3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "snr_levels = [-9.]\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfb56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other training params\n",
    "EPOCH = 15\n",
    "FF_START = True             # to start from feedforward initialization\n",
    "MAX_TIMESTEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c99b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path names\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/fb_ablation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Load network arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7bb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAME_PARAM:\n",
    "    from pbranchednetwork_shared import PBranchedNetwork_SharedSameHP\n",
    "    PNetClass = PBranchedNetwork_SharedSameHP\n",
    "    pnet_name = 'pnet'\n",
    "    fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-shared-50-regular.pth'\n",
    "else:\n",
    "    from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "    PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "    pnet_name = 'pnet'\n",
    "    fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-50-regular.pth'\n",
    "fb_state_dict = torch.load(fb_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c46f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d783f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "        net, state_dict, build_graph, random_init,\n",
    "        ff_multiplier, fb_multiplier, er_multiplier,\n",
    "        same_param, device='cuda:0'):\n",
    "    \n",
    "    pnet = PNetClass(\n",
    "        net, build_graph=build_graph, random_init=random_init,\n",
    "        ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier\n",
    "        )\n",
    "\n",
    "    pnet.load_state_dict(state_dict)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e13f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, epoch, dataloader, timesteps, loss_function, writer=None, tag='Clean'):\n",
    "    test_loss = np.zeros((timesteps+1,))\n",
    "    correct   = np.zeros((timesteps+1,))\n",
    "    for (images, labels) in dataloader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tt in range(timesteps+1):\n",
    "                if tt == 0:\n",
    "                    outputs, _ = net(images)\n",
    "                else:\n",
    "                    outputs, _ = net()\n",
    "                \n",
    "                loss = loss_function(outputs, labels)\n",
    "                test_loss[tt] += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct[tt] += preds.eq(labels).sum()\n",
    "\n",
    "    print()\n",
    "    for tt in range(timesteps+1):\n",
    "        test_loss[tt] /= len(dataloader.dataset)\n",
    "        correct[tt] /= len(dataloader.dataset)\n",
    "        print('Test set t = {:02d}: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "            tt,\n",
    "            test_loss[tt],\n",
    "            correct[tt]\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"{tag}Perf/Epoch#{epoch}\",\n",
    "                correct[tt], tt\n",
    "                )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d32cdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epoch, dataloader, timesteps, loss_function, optimizer, writer=None):\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "        net.reset()\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        ttloss = np.zeros((timesteps+1))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for tt in range(timesteps+1):\n",
    "            if tt == 0:\n",
    "                outputs, _ = net(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = loss.item()\n",
    "            else:\n",
    "                outputs, _ = net()\n",
    "                current_loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = current_loss.item()\n",
    "                loss += current_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.update_hyperparameters()\n",
    "            \n",
    "        print(f\"Training Epoch: {epoch} [{batch_index * BATCH_SIZE + len(images)}/{len(dataloader.dataset)}]\\tLoss: {loss.item():0.4f}\\tLR: {optimizer.param_groups[0]['lr']:0.6f}\")\n",
    "        for tt in range(timesteps+1):\n",
    "            print(f'{ttloss[tt]:0.4f}\\t', end='')\n",
    "        print()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"TrainingLoss/CE\", loss.item(),\n",
    "                (epoch-1)*len(dataloader) + batch_index\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68603d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_parameters(net, epoch, sumwriter, same_param=True):\n",
    "    if same_param:\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedforward\", getattr(net,f'ff_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedback\",    getattr(net,f'fb_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/error\",       getattr(net,f'errorm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/memory\",      getattr(net,f'mem_part').item(), epoch)\n",
    "\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedforward\", getattr(net,f'ffm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedback\",    getattr(net,f'fbm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/error\",       getattr(net,f'erm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/memory\",      1-getattr(net,f'ffm').item()-getattr(net,f'fbm').item(), epoch)\n",
    "    else:\n",
    "        for i in range(1, net.number_of_pcoders+1):\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedforward\", getattr(net,f'ffm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", 0, epoch)\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_error\", getattr(net,f'erm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item()-getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item(), epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85955819",
   "metadata": {},
   "source": [
    "# Main hyperparameter optimization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf6488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(noise_type, snr_level):\n",
    "    if noise_type == 'Merged':\n",
    "        noisy_ds = MergedNoisyDataset(subset=0.9, train=True)\n",
    "        noise_loader = torch.utils.data.DataLoader(\n",
    "            noisy_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "        eval_ds = MergedNoisyDataset(subset=0.9, train=False)\n",
    "        eval_loader = torch.utils.data.DataLoader(\n",
    "            eval_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "    else:\n",
    "        noisy_ds = NoisyDataset(bg=noise_type, snr=snr_level)\n",
    "        noise_loader = torch.utils.data.DataLoader(\n",
    "            noisy_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "        eval_loader = noise_loader\n",
    "\n",
    "    # Set up logs and network for training\n",
    "    net_dir = f'hyper_{noise_type}_snr{snr_level}'\n",
    "    if FF_START:\n",
    "        net_dir += '_FFstart'\n",
    "    if SAME_PARAM:\n",
    "        net_dir += '_shared'\n",
    "\n",
    "    sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "    net = BranchedNetwork() # Load original network\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet_fw = load_pnet( # Load FF PNet\n",
    "        net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "        ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    evaluate(\n",
    "        pnet_fw, 0, eval_loader, 1,\n",
    "        loss_function,\n",
    "        writer=sumwriter, tag='FeedForward')\n",
    "    del pnet_fw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load PNet for hyperparameter optimization\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = load_pnet(\n",
    "        net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "        ff_multiplier=0.33, fb_multiplier=0.33, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "\n",
    "    # Set up loss function and hyperparameters\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    hyperparams = [*pnet.get_hyperparameters()]\n",
    "    import pdb; pdb.set_trace()\n",
    "    if SAME_PARAM:\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': hyperparams[:-1], 'lr':0.01},\n",
    "            {'params': hyperparams[-1:], 'lr':0.0001}], weight_decay=0.00001)\n",
    "    else:\n",
    "        fffbmem_hp = []\n",
    "        erm_hp = []\n",
    "        for pc in range(pnet.number_of_pcoders):\n",
    "            fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "            erm_hp.append(hyperparams[pc*4+3])\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': fffbmem_hp, 'lr':0.01},\n",
    "            {'params': erm_hp, 'lr':0.0001}], weight_decay=0.00001)\n",
    "\n",
    "    # Log initial hyperparameter and eval values\n",
    "    log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "    evaluate(\n",
    "        pnet, 0, eval_loader,\n",
    "        MAX_TIMESTEP, loss_function,\n",
    "        writer=sumwriter, tag='Noisy'\n",
    "        )\n",
    "\n",
    "    # Run epochs\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        train(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function, optimizer,\n",
    "            writer=sumwriter\n",
    "            )\n",
    "        log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "        hps = pnet.get_hyperparameters_values()\n",
    "        print(hps)\n",
    "\n",
    "        evaluate(\n",
    "            pnet, epoch, eval_loader,\n",
    "            MAX_TIMESTEP, loss_function,\n",
    "            writer=sumwriter, tag='Noisy'\n",
    "            )\n",
    "    sumwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba94982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "AudScene, for SNR -9.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/ctn/users/cf2794/Code/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/ctn/users/cf2794/Code/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n",
      "/home/cf2794/.conda/envs/hcnn/lib/python3.7/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([10, 164, 400])) that is different to the input size (torch.Size([10, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n",
      "/home/cf2794/.conda/envs/hcnn/lib/python3.7/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([3, 164, 400])) that is different to the input size (torch.Size([3, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.5165, Accuracy: 0.1812\n",
      "Test set t = 01: Average loss: 0.5002, Accuracy: 0.1865\n",
      "\n",
      "> \u001b[0;32m/tmp/ipykernel_36653/3147566134.py\u001b[0m(60)\u001b[0;36mtrain_and_eval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     58 \u001b[0;31m    \u001b[0mhyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     59 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 60 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mSAME_PARAM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     61 \u001b[0;31m        optimizer = torch.optim.Adam([\n",
      "\u001b[0m\u001b[0;32m     62 \u001b[0;31m            \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(hyperparams)\n",
      "[Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.4055, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(0.0100, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.4055, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(0.0100, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.4055, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(0.0100, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.4055, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(0.0100, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.8473, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(-0.4055, device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor(0.0100, device='cuda:0', requires_grad=True)]\n",
      "ipdb> print(pnet.get_named_hyperparameters())\n",
      "*** AttributeError: 'PBranchedNetwork_AllSeparateHP' object has no attribute 'get_named_hyperparameters'\n"
     ]
    }
   ],
   "source": [
    "for noise_type in noise_types:\n",
    "    for snr_level in snr_levels:\n",
    "        print(\"=====================\")\n",
    "        print(f'{noise_type}, for SNR {snr_level}')\n",
    "        print(\"=====================\")\n",
    "        train_and_eval(noise_type, snr_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b31d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
