{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from models.networks_2022 import BranchedNetwork\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset, LargeNoisyDataset\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "noise_types = ['AudScene']\n",
    "snr_levels = [-9.]\n",
    "LR_SCALING = 0.01\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other training params\n",
    "EPOCH = 15\n",
    "FF_START = True             # to start from feedforward initialization\n",
    "MAX_TIMESTEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path names\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}1_checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}2_hyperp/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = FullNoisyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.noisy_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.logical_and(x.bg=='Babble8Spkr', x.snr==3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_datafile = '/mnt/smb/locker/abbott-locker/hcnn/hyperparameter_pooled_training_dataset_random_order_noNulls.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.ReconstructionTrainingDataset import CleanSoundsDataset\n",
    "from data.ReconstructionTrainingDataset import NoisySoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NoisySoundsDataset(_train_datafile, subset=1., snr='neg9', bg='pink_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_ds = NoisyDataset(bg=noise_type, snr=snr_level)\n",
    "noise_loader = torch.utils.data.DataLoader(\n",
    "    noisy_ds,  batch_size=BATCH_SIZE,\n",
    "    shuffle=True, drop_last=False,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "eval_loader = noise_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'pnet'\n",
    "fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-50-regular.pth'\n",
    "fb_state_dict = torch.load(fb_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "        net, state_dict, build_graph, random_init,\n",
    "        ff_multiplier, fb_multiplier, er_multiplier,\n",
    "        same_param, device='cuda:0'):\n",
    "    \n",
    "    pnet = PNetClass(\n",
    "        net, build_graph=build_graph, random_init=random_init,\n",
    "        ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier\n",
    "        )\n",
    "\n",
    "    pnet.load_state_dict(state_dict)\n",
    "    hyperparams = []\n",
    "    for i in range(1, 6):\n",
    "        hps = {}\n",
    "        hps['ffm'] = ff_multiplier\n",
    "        hps['fbm'] = fb_multiplier\n",
    "        hps['erm'] = er_multiplier\n",
    "        hyperparams.append(hps)\n",
    "    pnet.set_hyperparameters(hyperparams)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, epoch, dataloader, timesteps, loss_function, writer=None, tag='Clean'):\n",
    "    test_loss = np.zeros((timesteps+1,))\n",
    "    correct   = np.zeros((timesteps+1,))\n",
    "    for (images, labels) in dataloader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tt in range(timesteps+1):\n",
    "                if tt == 0:\n",
    "                    outputs, _ = net(images)\n",
    "                else:\n",
    "                    outputs, _ = net()\n",
    "                \n",
    "                loss = loss_function(outputs, labels)\n",
    "                test_loss[tt] += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct[tt] += preds.eq(labels).sum()\n",
    "\n",
    "    print()\n",
    "    for tt in range(timesteps+1):\n",
    "        test_loss[tt] /= len(dataloader.dataset)\n",
    "        correct[tt] /= len(dataloader.dataset)\n",
    "        print('Test set t = {:02d}: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "            tt,\n",
    "            test_loss[tt],\n",
    "            correct[tt]\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"{tag}Perf/Epoch#{epoch}\",\n",
    "                correct[tt], tt\n",
    "                )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(noise_type, snr_level):\n",
    "    # Load noisy data\n",
    "    noisy_ds = LargeNoisyDataset(bg=noise_type, snr=snr_level)\n",
    "    noise_loader = torch.utils.data.DataLoader(\n",
    "        noisy_ds,  batch_size=BATCH_SIZE,\n",
    "        shuffle=True, drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    # Set up logs and network for training\n",
    "    net_dir = f'hyper_{noise_type}_snr{snr_level}'\n",
    "    if not FF_START:\n",
    "        net_dir += '_randomInit'\n",
    "    if SAME_PARAM:\n",
    "        net_dir += '_shared'\n",
    "\n",
    "    sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "    net = BranchedNetwork() # Load original network\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet_fw = load_pnet( # Load FF PNet\n",
    "        net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "        ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    evaluate(\n",
    "        pnet_fw, 0, noise_loader, 1,\n",
    "        loss_function,\n",
    "        writer=sumwriter, tag='FeedForward')\n",
    "    del pnet_fw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load PNet for hyperparameter optimization\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    ffm = np.random.uniform()\n",
    "    fbm = np.random.uniform(high=1.-ffm)\n",
    "    erm = np.random.uniform()*0.1\n",
    "    pnet = load_pnet(\n",
    "        net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "        ff_multiplier=ffm, fb_multiplier=fbm, er_multiplier=erm,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "\n",
    "    # Set up loss function and hyperparameters\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    hyperparams = [*pnet.get_hyperparameters()]\n",
    "    fffbmem_hp = []\n",
    "    erm_hp = []\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "        erm_hp.append(hyperparams[pc*4+3])\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': fffbmem_hp, 'lr':0.01*LR_SCALING},\n",
    "        {'params': erm_hp, 'lr':0.0001*LR_SCALING}], weight_decay=0.00001)\n",
    "\n",
    "    # Log initial hyperparameter and eval values\n",
    "    log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "    evaluate(\n",
    "        pnet, 0, noise_loader,\n",
    "        MAX_TIMESTEP, loss_function,\n",
    "        writer=sumwriter, tag='Noisy'\n",
    "        )\n",
    "\n",
    "    # Run epochs\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        train(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function, optimizer,\n",
    "            writer=sumwriter\n",
    "            )\n",
    "        log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "        hps = pnet.get_hyperparameters_values()\n",
    "        print(hps)\n",
    "\n",
    "        evaluate(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function,\n",
    "            writer=sumwriter, tag='Noisy'\n",
    "            )\n",
    "    sumwriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_parameters(net, epoch, sumwriter, same_param=True):\n",
    "    if same_param:\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedforward\", getattr(net,f'ff_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedback\",    getattr(net,f'fb_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/error\",       getattr(net,f'errorm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/memory\",      getattr(net,f'mem_part').item(), epoch)\n",
    "\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedforward\", getattr(net,f'ffm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedback\",    getattr(net,f'fbm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/error\",       getattr(net,f'erm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/memory\",      1-getattr(net,f'ffm').item()-getattr(net,f'fbm').item(), epoch)\n",
    "    else:\n",
    "        for i in range(1, net.number_of_pcoders+1):\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedforward\", getattr(net,f'ffm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", 0, epoch)\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_error\", getattr(net,f'erm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item()-getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item(), epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main hyperparameter optimization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(noise_type, snr_level):\n",
    "    if noise_type == 'Merged':\n",
    "        noisy_ds = MergedNoisyDataset(subset=0.9, train=True)\n",
    "        noise_loader = torch.utils.data.DataLoader(\n",
    "            noisy_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "        eval_ds = MergedNoisyDataset(subset=0.9, train=False)\n",
    "        eval_loader = torch.utils.data.DataLoader(\n",
    "            eval_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "    else:\n",
    "        noisy_ds = NoisyDataset(bg=noise_type, snr=snr_level)\n",
    "        noise_loader = torch.utils.data.DataLoader(\n",
    "            noisy_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "        eval_loader = noise_loader\n",
    "\n",
    "    # Set up logs and network for training\n",
    "    net_dir = f'hyper_{noise_type}_snr{snr_level}'\n",
    "    if FF_START:\n",
    "        net_dir += '_FFstart'\n",
    "\n",
    "    sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "    net = BranchedNetwork() # Load original network\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet_fw = load_pnet( # Load FF PNet\n",
    "        net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "        ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "        same_param=False, device='cuda:0'\n",
    "        )\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    evaluate(\n",
    "        pnet_fw, 0, eval_loader, 1,\n",
    "        loss_function,\n",
    "        writer=sumwriter, tag='FeedForward')\n",
    "    del pnet_fw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load PNet for hyperparameter optimization\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = load_pnet(\n",
    "        net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "        ff_multiplier=0.33, fb_multiplier=0.33, er_multiplier=0.0,\n",
    "        same_param=False, device='cuda:0'\n",
    "        )\n",
    "\n",
    "    # Set up loss function and hyperparameters\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    for param in pnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    hyperparams = [*pnet.get_hyperparameters()]\n",
    "    for hyperparam in hyperparams:\n",
    "        hyperparam.requires_grad = True\n",
    "    fffbmem_hp = []\n",
    "    erm_hp = []\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "        erm_hp.append(hyperparams[pc*4+3])\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': fffbmem_hp, 'lr':0.01},\n",
    "        {'params': erm_hp, 'lr':0.0001}], weight_decay=0.00001)\n",
    "\n",
    "    # Log initial hyperparameter and eval values\n",
    "    log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "    evaluate(\n",
    "        pnet, 0, eval_loader,\n",
    "        MAX_TIMESTEP, loss_function,\n",
    "        writer=sumwriter, tag='Noisy'\n",
    "        )\n",
    "\n",
    "    # Run epochs\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        train(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function, optimizer,\n",
    "            writer=sumwriter\n",
    "            )\n",
    "        log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "        hps = pnet.get_hyperparameters_values()\n",
    "        print(hps)\n",
    "\n",
    "        evaluate(\n",
    "            pnet, epoch, eval_loader,\n",
    "            MAX_TIMESTEP, loss_function,\n",
    "            writer=sumwriter, tag='Noisy'\n",
    "            )\n",
    "    sumwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for noise_type in noise_types:\n",
    "    for snr_level in snr_levels:\n",
    "        print(\"=====================\")\n",
    "        print(f'{noise_type}, for SNR {snr_level}')\n",
    "        print(\"=====================\")\n",
    "        train_and_eval(noise_type, snr_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
