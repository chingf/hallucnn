{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv3 import PBranchedNetwork_Conv3SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv3SeparateHP\n",
    "pnet_name = 'conv3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 20\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'\n",
    "train_datafile = f'{engram_dir}seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  7 13:15:06 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:1D:00.0 Off |                  N/A |\r\n",
      "| 27%   28C    P8    21W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_Conv3SeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(23.428571428571427, 23.529411764705884), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up TrainSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainCleanSoundsDataset(train_datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up PsychophysicsSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = h5py.File(f\"{engram_dir}PsychophysicsWord2017W_not_resampled.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metadata = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META_key.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPsychophysics2017WCleanCochleagrams():\n",
    "    \n",
    "    cochleagrams_clean = []\n",
    "   \n",
    "    cochleagrams = []\n",
    "    for batch_ii in range(0,15300,100):\n",
    "        hdf5_path = '/mnt/smb/locker/issa-locker/users/Erica/cgrams_for_noise_robustness_analysis/PsychophysicsWord2017W_clean/batch_'+str(batch_ii)+'_to_'+str(batch_ii+100)+'.hdf5'\n",
    "        with h5py.File(hdf5_path, 'r') as f_in:\n",
    "            cochleagrams += list(f_in['data'])\n",
    "\n",
    "    return cochleagrams\n",
    "clean_in = getPsychophysics2017WCleanCochleagrams()\n",
    "clean_in = np.array(clean_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for word in f_metadata['word']:\n",
    "    idx = np.argwhere(f_key == word)\n",
    "    if len(idx) == 0:\n",
    "        labels.append(-1)\n",
    "    else:\n",
    "        labels.append(idx.item())\n",
    "labels = np.array(labels)\n",
    "labels += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dset = []\n",
    "for _orig_dset in f_metadata['orig_dset']:\n",
    "    _orig_dset = str(_orig_dset, 'utf-8')\n",
    "    _orig_dset = 'WSJ' if 'WSJ' in _orig_dset else 'Timit'\n",
    "    orig_dset.append(_orig_dset)\n",
    "orig_dset = np.array(orig_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psycho_dataset = PsychophysicsCleanSoundsDataset(\n",
    "    clean_in, labels, orig_dset, exclude_timit=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del orig_dset\n",
    "del clean_in\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55188, 1, 164, 400])\n"
     ]
    }
   ],
   "source": [
    "full_dataset = CleanSoundsDataset(train_dataset, psycho_dataset)\n",
    "del train_dataset\n",
    "del psycho_dataset\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [20/49669]\tLoss: 3450874.7500\n",
      "Training Epoch: 1 [40/49669]\tLoss: 3454960.2500\n",
      "Training Epoch: 1 [60/49669]\tLoss: 2546383.5000\n",
      "Training Epoch: 1 [80/49669]\tLoss: 3489636.7500\n",
      "Training Epoch: 1 [100/49669]\tLoss: 3712058.7500\n",
      "Training Epoch: 1 [120/49669]\tLoss: 3743765.5000\n",
      "Training Epoch: 1 [140/49669]\tLoss: 3907733.2500\n",
      "Training Epoch: 1 [160/49669]\tLoss: 4056351.5000\n",
      "Training Epoch: 1 [180/49669]\tLoss: 4787789.5000\n",
      "Training Epoch: 1 [200/49669]\tLoss: 4600431.5000\n",
      "Training Epoch: 1 [220/49669]\tLoss: 4117565.2500\n",
      "Training Epoch: 1 [240/49669]\tLoss: 4583157.0000\n",
      "Training Epoch: 1 [260/49669]\tLoss: 5482876.0000\n",
      "Training Epoch: 1 [280/49669]\tLoss: 4907575.0000\n",
      "Training Epoch: 1 [300/49669]\tLoss: 4693817.5000\n",
      "Training Epoch: 1 [320/49669]\tLoss: 5633357.0000\n",
      "Training Epoch: 1 [340/49669]\tLoss: 5303913.0000\n",
      "Training Epoch: 1 [360/49669]\tLoss: 6537260.0000\n",
      "Training Epoch: 1 [380/49669]\tLoss: 5464949.0000\n",
      "Training Epoch: 1 [400/49669]\tLoss: 5855531.0000\n",
      "Training Epoch: 1 [420/49669]\tLoss: 5851848.5000\n",
      "Training Epoch: 1 [440/49669]\tLoss: 6027376.0000\n",
      "Training Epoch: 1 [460/49669]\tLoss: 6045353.5000\n",
      "Training Epoch: 1 [480/49669]\tLoss: 7401074.5000\n",
      "Training Epoch: 1 [500/49669]\tLoss: 6858241.5000\n",
      "Training Epoch: 1 [520/49669]\tLoss: 6268387.0000\n",
      "Training Epoch: 1 [540/49669]\tLoss: 6117005.5000\n",
      "Training Epoch: 1 [560/49669]\tLoss: 7717571.0000\n",
      "Training Epoch: 1 [580/49669]\tLoss: 7574903.0000\n",
      "Training Epoch: 1 [600/49669]\tLoss: 7534007.0000\n",
      "Training Epoch: 1 [620/49669]\tLoss: 8414837.0000\n",
      "Training Epoch: 1 [640/49669]\tLoss: 8481661.0000\n",
      "Training Epoch: 1 [660/49669]\tLoss: 7043442.0000\n",
      "Training Epoch: 1 [680/49669]\tLoss: 8738407.0000\n",
      "Training Epoch: 1 [700/49669]\tLoss: 9383802.0000\n",
      "Training Epoch: 1 [720/49669]\tLoss: 9257631.0000\n",
      "Training Epoch: 1 [740/49669]\tLoss: 9797048.0000\n",
      "Training Epoch: 1 [760/49669]\tLoss: 9542873.0000\n",
      "Training Epoch: 1 [780/49669]\tLoss: 9182114.0000\n",
      "Training Epoch: 1 [800/49669]\tLoss: 8764415.0000\n",
      "Training Epoch: 1 [820/49669]\tLoss: 11050945.0000\n",
      "Training Epoch: 1 [840/49669]\tLoss: 7758129.0000\n",
      "Training Epoch: 1 [860/49669]\tLoss: 9982795.0000\n",
      "Training Epoch: 1 [880/49669]\tLoss: 10007513.0000\n",
      "Training Epoch: 1 [900/49669]\tLoss: 10885534.0000\n",
      "Training Epoch: 1 [920/49669]\tLoss: 11901820.0000\n",
      "Training Epoch: 1 [940/49669]\tLoss: 8854160.0000\n",
      "Training Epoch: 1 [960/49669]\tLoss: 9702881.0000\n",
      "Training Epoch: 1 [980/49669]\tLoss: 10465325.0000\n",
      "Training Epoch: 1 [1000/49669]\tLoss: 11608991.0000\n",
      "Training Epoch: 1 [1020/49669]\tLoss: 13166338.0000\n",
      "Training Epoch: 1 [1040/49669]\tLoss: 11797000.0000\n",
      "Training Epoch: 1 [1060/49669]\tLoss: 12746287.0000\n",
      "Training Epoch: 1 [1080/49669]\tLoss: 10959171.0000\n",
      "Training Epoch: 1 [1100/49669]\tLoss: 11923846.0000\n",
      "Training Epoch: 1 [1120/49669]\tLoss: 12775254.0000\n",
      "Training Epoch: 1 [1140/49669]\tLoss: 10883633.0000\n",
      "Training Epoch: 1 [1160/49669]\tLoss: 12517535.0000\n",
      "Training Epoch: 1 [1180/49669]\tLoss: 13643238.0000\n",
      "Training Epoch: 1 [1200/49669]\tLoss: 12962992.0000\n",
      "Training Epoch: 1 [1220/49669]\tLoss: 10847515.0000\n",
      "Training Epoch: 1 [1240/49669]\tLoss: 10177355.0000\n",
      "Training Epoch: 1 [1260/49669]\tLoss: 14001758.0000\n",
      "Training Epoch: 1 [1280/49669]\tLoss: 12872357.0000\n",
      "Training Epoch: 1 [1300/49669]\tLoss: 12901142.0000\n",
      "Training Epoch: 1 [1320/49669]\tLoss: 16323352.0000\n",
      "Training Epoch: 1 [1340/49669]\tLoss: 15003562.0000\n",
      "Training Epoch: 1 [1360/49669]\tLoss: 14561536.0000\n",
      "Training Epoch: 1 [1380/49669]\tLoss: 14604587.0000\n",
      "Training Epoch: 1 [1400/49669]\tLoss: 14251934.0000\n",
      "Training Epoch: 1 [1420/49669]\tLoss: 13888296.0000\n",
      "Training Epoch: 1 [1440/49669]\tLoss: 16098413.0000\n",
      "Training Epoch: 1 [1460/49669]\tLoss: 13194406.0000\n",
      "Training Epoch: 1 [1480/49669]\tLoss: 15546402.0000\n",
      "Training Epoch: 1 [1500/49669]\tLoss: 15165887.0000\n",
      "Training Epoch: 1 [1520/49669]\tLoss: 14907050.0000\n",
      "Training Epoch: 1 [1540/49669]\tLoss: 15354057.0000\n",
      "Training Epoch: 1 [1560/49669]\tLoss: 13334868.0000\n",
      "Training Epoch: 1 [1580/49669]\tLoss: 18992328.0000\n",
      "Training Epoch: 1 [1600/49669]\tLoss: 17961662.0000\n",
      "Training Epoch: 1 [1620/49669]\tLoss: 14903159.0000\n",
      "Training Epoch: 1 [1640/49669]\tLoss: 13876637.0000\n",
      "Training Epoch: 1 [1660/49669]\tLoss: 15488442.0000\n",
      "Training Epoch: 1 [1680/49669]\tLoss: 15045476.0000\n",
      "Training Epoch: 1 [1700/49669]\tLoss: 15461711.0000\n",
      "Training Epoch: 1 [1720/49669]\tLoss: 15875506.0000\n",
      "Training Epoch: 1 [1740/49669]\tLoss: 15776412.0000\n",
      "Training Epoch: 1 [1760/49669]\tLoss: 14315565.0000\n",
      "Training Epoch: 1 [1780/49669]\tLoss: 16598779.0000\n",
      "Training Epoch: 1 [1800/49669]\tLoss: 16491411.0000\n",
      "Training Epoch: 1 [1820/49669]\tLoss: 15272479.0000\n",
      "Training Epoch: 1 [1840/49669]\tLoss: 13574013.0000\n",
      "Training Epoch: 1 [1860/49669]\tLoss: 16518435.0000\n",
      "Training Epoch: 1 [1880/49669]\tLoss: 16286159.0000\n",
      "Training Epoch: 1 [1900/49669]\tLoss: 15291607.0000\n",
      "Training Epoch: 1 [1920/49669]\tLoss: 19094702.0000\n",
      "Training Epoch: 1 [1940/49669]\tLoss: 16383258.0000\n",
      "Training Epoch: 1 [1960/49669]\tLoss: 17486716.0000\n",
      "Training Epoch: 1 [1980/49669]\tLoss: 17254248.0000\n",
      "Training Epoch: 1 [2000/49669]\tLoss: 18931570.0000\n",
      "Training Epoch: 1 [2020/49669]\tLoss: 16835674.0000\n",
      "Training Epoch: 1 [2040/49669]\tLoss: 17789382.0000\n",
      "Training Epoch: 1 [2060/49669]\tLoss: 17157238.0000\n",
      "Training Epoch: 1 [2080/49669]\tLoss: 16853816.0000\n",
      "Training Epoch: 1 [2100/49669]\tLoss: 17983622.0000\n",
      "Training Epoch: 1 [2120/49669]\tLoss: 16608833.0000\n",
      "Training Epoch: 1 [2140/49669]\tLoss: 17557460.0000\n",
      "Training Epoch: 1 [2160/49669]\tLoss: 15196431.0000\n",
      "Training Epoch: 1 [2180/49669]\tLoss: 18140092.0000\n",
      "Training Epoch: 1 [2200/49669]\tLoss: 16874642.0000\n",
      "Training Epoch: 1 [2220/49669]\tLoss: 17615728.0000\n",
      "Training Epoch: 1 [2240/49669]\tLoss: 15092629.0000\n",
      "Training Epoch: 1 [2260/49669]\tLoss: 17038138.0000\n",
      "Training Epoch: 1 [2280/49669]\tLoss: 18143704.0000\n",
      "Training Epoch: 1 [2300/49669]\tLoss: 18378566.0000\n",
      "Training Epoch: 1 [2320/49669]\tLoss: 16321015.0000\n",
      "Training Epoch: 1 [2340/49669]\tLoss: 18177586.0000\n",
      "Training Epoch: 1 [2360/49669]\tLoss: 17243218.0000\n",
      "Training Epoch: 1 [2380/49669]\tLoss: 17474752.0000\n",
      "Training Epoch: 1 [2400/49669]\tLoss: 15940195.0000\n",
      "Training Epoch: 1 [2420/49669]\tLoss: 17312284.0000\n",
      "Training Epoch: 1 [2440/49669]\tLoss: 16238941.0000\n",
      "Training Epoch: 1 [2460/49669]\tLoss: 13880854.0000\n",
      "Training Epoch: 1 [2480/49669]\tLoss: 18763842.0000\n",
      "Training Epoch: 1 [2500/49669]\tLoss: 17819000.0000\n",
      "Training Epoch: 1 [2520/49669]\tLoss: 16486968.0000\n",
      "Training Epoch: 1 [2540/49669]\tLoss: 17683514.0000\n",
      "Training Epoch: 1 [2560/49669]\tLoss: 18341548.0000\n",
      "Training Epoch: 1 [2580/49669]\tLoss: 19156680.0000\n",
      "Training Epoch: 1 [2600/49669]\tLoss: 18295854.0000\n",
      "Training Epoch: 1 [2620/49669]\tLoss: 15624917.0000\n",
      "Training Epoch: 1 [2640/49669]\tLoss: 19170804.0000\n",
      "Training Epoch: 1 [2660/49669]\tLoss: 13910150.0000\n",
      "Training Epoch: 1 [2680/49669]\tLoss: 17265264.0000\n",
      "Training Epoch: 1 [2700/49669]\tLoss: 16867998.0000\n",
      "Training Epoch: 1 [2720/49669]\tLoss: 17592000.0000\n",
      "Training Epoch: 1 [2740/49669]\tLoss: 16496101.0000\n",
      "Training Epoch: 1 [2760/49669]\tLoss: 16753213.0000\n",
      "Training Epoch: 1 [2780/49669]\tLoss: 17841108.0000\n",
      "Training Epoch: 1 [2800/49669]\tLoss: 16914424.0000\n",
      "Training Epoch: 1 [2820/49669]\tLoss: 19328804.0000\n",
      "Training Epoch: 1 [2840/49669]\tLoss: 16082278.0000\n",
      "Training Epoch: 1 [2860/49669]\tLoss: 16685555.0000\n",
      "Training Epoch: 1 [2880/49669]\tLoss: 16813152.0000\n",
      "Training Epoch: 1 [2900/49669]\tLoss: 17533280.0000\n",
      "Training Epoch: 1 [2920/49669]\tLoss: 15184956.0000\n",
      "Training Epoch: 1 [2940/49669]\tLoss: 14452587.0000\n",
      "Training Epoch: 1 [2960/49669]\tLoss: 16306184.0000\n",
      "Training Epoch: 1 [2980/49669]\tLoss: 15781674.0000\n",
      "Training Epoch: 1 [3000/49669]\tLoss: 14798494.0000\n",
      "Training Epoch: 1 [3020/49669]\tLoss: 16162990.0000\n",
      "Training Epoch: 1 [3040/49669]\tLoss: 16358078.0000\n",
      "Training Epoch: 1 [3060/49669]\tLoss: 17453528.0000\n",
      "Training Epoch: 1 [3080/49669]\tLoss: 16421374.0000\n",
      "Training Epoch: 1 [3100/49669]\tLoss: 17875766.0000\n",
      "Training Epoch: 1 [3120/49669]\tLoss: 16154940.0000\n",
      "Training Epoch: 1 [3140/49669]\tLoss: 16908626.0000\n",
      "Training Epoch: 1 [3160/49669]\tLoss: 15912482.0000\n",
      "Training Epoch: 1 [3180/49669]\tLoss: 13067299.0000\n",
      "Training Epoch: 1 [3200/49669]\tLoss: 16741903.0000\n",
      "Training Epoch: 1 [3220/49669]\tLoss: 13915009.0000\n",
      "Training Epoch: 1 [3240/49669]\tLoss: 15242153.0000\n",
      "Training Epoch: 1 [3260/49669]\tLoss: 14682946.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [3280/49669]\tLoss: 13178119.0000\n",
      "Training Epoch: 1 [3300/49669]\tLoss: 15081459.0000\n",
      "Training Epoch: 1 [3320/49669]\tLoss: 13454031.0000\n",
      "Training Epoch: 1 [3340/49669]\tLoss: 15307393.0000\n",
      "Training Epoch: 1 [3360/49669]\tLoss: 14293281.0000\n",
      "Training Epoch: 1 [3380/49669]\tLoss: 16566142.0000\n",
      "Training Epoch: 1 [3400/49669]\tLoss: 14625781.0000\n",
      "Training Epoch: 1 [3420/49669]\tLoss: 14826871.0000\n",
      "Training Epoch: 1 [3440/49669]\tLoss: 15117072.0000\n",
      "Training Epoch: 1 [3460/49669]\tLoss: 13698702.0000\n",
      "Training Epoch: 1 [3480/49669]\tLoss: 17190434.0000\n",
      "Training Epoch: 1 [3500/49669]\tLoss: 17750048.0000\n",
      "Training Epoch: 1 [3520/49669]\tLoss: 15230950.0000\n",
      "Training Epoch: 1 [3540/49669]\tLoss: 16791548.0000\n",
      "Training Epoch: 1 [3560/49669]\tLoss: 13841657.0000\n",
      "Training Epoch: 1 [3580/49669]\tLoss: 16858142.0000\n",
      "Training Epoch: 1 [3600/49669]\tLoss: 16879770.0000\n",
      "Training Epoch: 1 [3620/49669]\tLoss: 17158398.0000\n",
      "Training Epoch: 1 [3640/49669]\tLoss: 14611072.0000\n",
      "Training Epoch: 1 [3660/49669]\tLoss: 15592053.0000\n",
      "Training Epoch: 1 [3680/49669]\tLoss: 14084221.0000\n",
      "Training Epoch: 1 [3700/49669]\tLoss: 13314526.0000\n",
      "Training Epoch: 1 [3720/49669]\tLoss: 16653460.0000\n",
      "Training Epoch: 1 [3740/49669]\tLoss: 16167114.0000\n",
      "Training Epoch: 1 [3760/49669]\tLoss: 15069320.0000\n",
      "Training Epoch: 1 [3780/49669]\tLoss: 14752900.0000\n",
      "Training Epoch: 1 [3800/49669]\tLoss: 14449100.0000\n",
      "Training Epoch: 1 [3820/49669]\tLoss: 14201409.0000\n",
      "Training Epoch: 1 [3840/49669]\tLoss: 14179397.0000\n",
      "Training Epoch: 1 [3860/49669]\tLoss: 12272739.0000\n",
      "Training Epoch: 1 [3880/49669]\tLoss: 14053320.0000\n",
      "Training Epoch: 1 [3900/49669]\tLoss: 14842334.0000\n",
      "Training Epoch: 1 [3920/49669]\tLoss: 15223955.0000\n",
      "Training Epoch: 1 [3940/49669]\tLoss: 14707480.0000\n",
      "Training Epoch: 1 [3960/49669]\tLoss: 13201615.0000\n",
      "Training Epoch: 1 [3980/49669]\tLoss: 14143378.0000\n",
      "Training Epoch: 1 [4000/49669]\tLoss: 13979821.0000\n",
      "Training Epoch: 1 [4020/49669]\tLoss: 11793133.0000\n",
      "Training Epoch: 1 [4040/49669]\tLoss: 13362152.0000\n",
      "Training Epoch: 1 [4060/49669]\tLoss: 15310016.0000\n",
      "Training Epoch: 1 [4080/49669]\tLoss: 13941195.0000\n",
      "Training Epoch: 1 [4100/49669]\tLoss: 11457335.0000\n",
      "Training Epoch: 1 [4120/49669]\tLoss: 14565938.0000\n",
      "Training Epoch: 1 [4140/49669]\tLoss: 15157831.0000\n",
      "Training Epoch: 1 [4160/49669]\tLoss: 14646950.0000\n",
      "Training Epoch: 1 [4180/49669]\tLoss: 12312075.0000\n",
      "Training Epoch: 1 [4200/49669]\tLoss: 13256743.0000\n",
      "Training Epoch: 1 [4220/49669]\tLoss: 12584059.0000\n",
      "Training Epoch: 1 [4240/49669]\tLoss: 13235206.0000\n",
      "Training Epoch: 1 [4260/49669]\tLoss: 13497319.0000\n",
      "Training Epoch: 1 [4280/49669]\tLoss: 13863471.0000\n",
      "Training Epoch: 1 [4300/49669]\tLoss: 12844731.0000\n",
      "Training Epoch: 1 [4320/49669]\tLoss: 12787635.0000\n",
      "Training Epoch: 1 [4340/49669]\tLoss: 13666033.0000\n",
      "Training Epoch: 1 [4360/49669]\tLoss: 12766307.0000\n",
      "Training Epoch: 1 [4380/49669]\tLoss: 11382457.0000\n",
      "Training Epoch: 1 [4400/49669]\tLoss: 13743043.0000\n",
      "Training Epoch: 1 [4420/49669]\tLoss: 14650537.0000\n",
      "Training Epoch: 1 [4440/49669]\tLoss: 14501751.0000\n",
      "Training Epoch: 1 [4460/49669]\tLoss: 13646077.0000\n",
      "Training Epoch: 1 [4480/49669]\tLoss: 12335039.0000\n",
      "Training Epoch: 1 [4500/49669]\tLoss: 13298400.0000\n",
      "Training Epoch: 1 [4520/49669]\tLoss: 13783637.0000\n",
      "Training Epoch: 1 [4540/49669]\tLoss: 13528527.0000\n",
      "Training Epoch: 1 [4560/49669]\tLoss: 10885031.0000\n",
      "Training Epoch: 1 [4580/49669]\tLoss: 13042519.0000\n",
      "Training Epoch: 1 [4600/49669]\tLoss: 11809275.0000\n",
      "Training Epoch: 1 [4620/49669]\tLoss: 12379347.0000\n",
      "Training Epoch: 1 [4640/49669]\tLoss: 13508376.0000\n",
      "Training Epoch: 1 [4660/49669]\tLoss: 13152969.0000\n",
      "Training Epoch: 1 [4680/49669]\tLoss: 11860384.0000\n",
      "Training Epoch: 1 [4700/49669]\tLoss: 13024301.0000\n",
      "Training Epoch: 1 [4720/49669]\tLoss: 11737922.0000\n",
      "Training Epoch: 1 [4740/49669]\tLoss: 12514371.0000\n",
      "Training Epoch: 1 [4760/49669]\tLoss: 12987322.0000\n",
      "Training Epoch: 1 [4780/49669]\tLoss: 11039783.0000\n",
      "Training Epoch: 1 [4800/49669]\tLoss: 10677604.0000\n",
      "Training Epoch: 1 [4820/49669]\tLoss: 10772133.0000\n",
      "Training Epoch: 1 [4840/49669]\tLoss: 12605868.0000\n",
      "Training Epoch: 1 [4860/49669]\tLoss: 10606028.0000\n",
      "Training Epoch: 1 [4880/49669]\tLoss: 10278438.0000\n",
      "Training Epoch: 1 [4900/49669]\tLoss: 10972384.0000\n",
      "Training Epoch: 1 [4920/49669]\tLoss: 11195917.0000\n",
      "Training Epoch: 1 [4940/49669]\tLoss: 10932699.0000\n",
      "Training Epoch: 1 [4960/49669]\tLoss: 12200688.0000\n",
      "Training Epoch: 1 [4980/49669]\tLoss: 12930909.0000\n",
      "Training Epoch: 1 [5000/49669]\tLoss: 11138650.0000\n",
      "Training Epoch: 1 [5020/49669]\tLoss: 12842757.0000\n",
      "Training Epoch: 1 [5040/49669]\tLoss: 10340448.0000\n",
      "Training Epoch: 1 [5060/49669]\tLoss: 10542668.0000\n",
      "Training Epoch: 1 [5080/49669]\tLoss: 12340973.0000\n",
      "Training Epoch: 1 [5100/49669]\tLoss: 12190653.0000\n",
      "Training Epoch: 1 [5120/49669]\tLoss: 12160133.0000\n",
      "Training Epoch: 1 [5140/49669]\tLoss: 10226538.0000\n",
      "Training Epoch: 1 [5160/49669]\tLoss: 10491217.0000\n",
      "Training Epoch: 1 [5180/49669]\tLoss: 12337842.0000\n",
      "Training Epoch: 1 [5200/49669]\tLoss: 10879092.0000\n",
      "Training Epoch: 1 [5220/49669]\tLoss: 11122229.0000\n",
      "Training Epoch: 1 [5240/49669]\tLoss: 11601795.0000\n",
      "Training Epoch: 1 [5260/49669]\tLoss: 9884197.0000\n",
      "Training Epoch: 1 [5280/49669]\tLoss: 9937102.0000\n",
      "Training Epoch: 1 [5300/49669]\tLoss: 11104621.0000\n",
      "Training Epoch: 1 [5320/49669]\tLoss: 12332682.0000\n",
      "Training Epoch: 1 [5340/49669]\tLoss: 12527073.0000\n",
      "Training Epoch: 1 [5360/49669]\tLoss: 10521527.0000\n",
      "Training Epoch: 1 [5380/49669]\tLoss: 9662143.0000\n",
      "Training Epoch: 1 [5400/49669]\tLoss: 10380088.0000\n",
      "Training Epoch: 1 [5420/49669]\tLoss: 9392721.0000\n",
      "Training Epoch: 1 [5440/49669]\tLoss: 9636941.0000\n",
      "Training Epoch: 1 [5460/49669]\tLoss: 9274196.0000\n",
      "Training Epoch: 1 [5480/49669]\tLoss: 10431749.0000\n",
      "Training Epoch: 1 [5500/49669]\tLoss: 9860804.0000\n",
      "Training Epoch: 1 [5520/49669]\tLoss: 12063614.0000\n",
      "Training Epoch: 1 [5540/49669]\tLoss: 9721687.0000\n",
      "Training Epoch: 1 [5560/49669]\tLoss: 9599702.0000\n",
      "Training Epoch: 1 [5580/49669]\tLoss: 8913957.0000\n",
      "Training Epoch: 1 [5600/49669]\tLoss: 10117542.0000\n",
      "Training Epoch: 1 [5620/49669]\tLoss: 8855222.0000\n",
      "Training Epoch: 1 [5640/49669]\tLoss: 9882616.0000\n",
      "Training Epoch: 1 [5660/49669]\tLoss: 10832597.0000\n",
      "Training Epoch: 1 [5680/49669]\tLoss: 8789372.0000\n",
      "Training Epoch: 1 [5700/49669]\tLoss: 9733719.0000\n",
      "Training Epoch: 1 [5720/49669]\tLoss: 10024590.0000\n",
      "Training Epoch: 1 [5740/49669]\tLoss: 9801528.0000\n",
      "Training Epoch: 1 [5760/49669]\tLoss: 11673757.0000\n",
      "Training Epoch: 1 [5780/49669]\tLoss: 10631064.0000\n",
      "Training Epoch: 1 [5800/49669]\tLoss: 8908035.0000\n",
      "Training Epoch: 1 [5820/49669]\tLoss: 10425717.0000\n",
      "Training Epoch: 1 [5840/49669]\tLoss: 10763455.0000\n",
      "Training Epoch: 1 [5860/49669]\tLoss: 8910568.0000\n",
      "Training Epoch: 1 [5880/49669]\tLoss: 10130197.0000\n",
      "Training Epoch: 1 [5900/49669]\tLoss: 10834473.0000\n",
      "Training Epoch: 1 [5920/49669]\tLoss: 9185375.0000\n",
      "Training Epoch: 1 [5940/49669]\tLoss: 8799711.0000\n",
      "Training Epoch: 1 [5960/49669]\tLoss: 9276693.0000\n",
      "Training Epoch: 1 [5980/49669]\tLoss: 8852381.0000\n",
      "Training Epoch: 1 [6000/49669]\tLoss: 10344488.0000\n",
      "Training Epoch: 1 [6020/49669]\tLoss: 8779384.0000\n",
      "Training Epoch: 1 [6040/49669]\tLoss: 9447802.0000\n",
      "Training Epoch: 1 [6060/49669]\tLoss: 9595851.0000\n",
      "Training Epoch: 1 [6080/49669]\tLoss: 9788068.0000\n",
      "Training Epoch: 1 [6100/49669]\tLoss: 9610181.0000\n",
      "Training Epoch: 1 [6120/49669]\tLoss: 10425562.0000\n",
      "Training Epoch: 1 [6140/49669]\tLoss: 9803255.0000\n",
      "Training Epoch: 1 [6160/49669]\tLoss: 9636800.0000\n",
      "Training Epoch: 1 [6180/49669]\tLoss: 8164486.0000\n",
      "Training Epoch: 1 [6200/49669]\tLoss: 8324965.0000\n",
      "Training Epoch: 1 [6220/49669]\tLoss: 8390950.0000\n",
      "Training Epoch: 1 [6240/49669]\tLoss: 8965627.0000\n",
      "Training Epoch: 1 [6260/49669]\tLoss: 9183689.0000\n",
      "Training Epoch: 1 [6280/49669]\tLoss: 9612127.0000\n",
      "Training Epoch: 1 [6300/49669]\tLoss: 7092622.0000\n",
      "Training Epoch: 1 [6320/49669]\tLoss: 9923319.0000\n",
      "Training Epoch: 1 [6340/49669]\tLoss: 10390779.0000\n",
      "Training Epoch: 1 [6360/49669]\tLoss: 9412573.0000\n",
      "Training Epoch: 1 [6380/49669]\tLoss: 8455618.0000\n",
      "Training Epoch: 1 [6400/49669]\tLoss: 8448468.0000\n",
      "Training Epoch: 1 [6420/49669]\tLoss: 9499986.0000\n",
      "Training Epoch: 1 [6440/49669]\tLoss: 8971066.0000\n",
      "Training Epoch: 1 [6460/49669]\tLoss: 8600482.0000\n",
      "Training Epoch: 1 [6480/49669]\tLoss: 8458173.0000\n",
      "Training Epoch: 1 [6500/49669]\tLoss: 9568525.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [6520/49669]\tLoss: 8535856.0000\n",
      "Training Epoch: 1 [6540/49669]\tLoss: 7545051.0000\n",
      "Training Epoch: 1 [6560/49669]\tLoss: 7431613.0000\n",
      "Training Epoch: 1 [6580/49669]\tLoss: 8774845.0000\n",
      "Training Epoch: 1 [6600/49669]\tLoss: 8799614.0000\n",
      "Training Epoch: 1 [6620/49669]\tLoss: 6962742.5000\n",
      "Training Epoch: 1 [6640/49669]\tLoss: 7842856.0000\n",
      "Training Epoch: 1 [6660/49669]\tLoss: 9005071.0000\n",
      "Training Epoch: 1 [6680/49669]\tLoss: 9142933.0000\n",
      "Training Epoch: 1 [6700/49669]\tLoss: 7665729.5000\n",
      "Training Epoch: 1 [6720/49669]\tLoss: 9364698.0000\n",
      "Training Epoch: 1 [6740/49669]\tLoss: 8431398.0000\n",
      "Training Epoch: 1 [6760/49669]\tLoss: 7798402.5000\n",
      "Training Epoch: 1 [6780/49669]\tLoss: 7182206.5000\n",
      "Training Epoch: 1 [6800/49669]\tLoss: 7002081.0000\n",
      "Training Epoch: 1 [6820/49669]\tLoss: 6808742.5000\n",
      "Training Epoch: 1 [6840/49669]\tLoss: 7695544.5000\n",
      "Training Epoch: 1 [6860/49669]\tLoss: 9095296.0000\n",
      "Training Epoch: 1 [6880/49669]\tLoss: 8565974.0000\n",
      "Training Epoch: 1 [6900/49669]\tLoss: 6822889.0000\n",
      "Training Epoch: 1 [6920/49669]\tLoss: 7116990.5000\n",
      "Training Epoch: 1 [6940/49669]\tLoss: 7776281.0000\n",
      "Training Epoch: 1 [6960/49669]\tLoss: 6365971.0000\n",
      "Training Epoch: 1 [6980/49669]\tLoss: 7244958.0000\n",
      "Training Epoch: 1 [7000/49669]\tLoss: 8135212.0000\n",
      "Training Epoch: 1 [7020/49669]\tLoss: 7398112.0000\n",
      "Training Epoch: 1 [7040/49669]\tLoss: 8850531.0000\n",
      "Training Epoch: 1 [7060/49669]\tLoss: 7902166.0000\n",
      "Training Epoch: 1 [7080/49669]\tLoss: 7328861.0000\n",
      "Training Epoch: 1 [7100/49669]\tLoss: 7741804.0000\n",
      "Training Epoch: 1 [7120/49669]\tLoss: 8023782.5000\n",
      "Training Epoch: 1 [7140/49669]\tLoss: 7430104.0000\n",
      "Training Epoch: 1 [7160/49669]\tLoss: 8321309.5000\n",
      "Training Epoch: 1 [7180/49669]\tLoss: 7039193.0000\n",
      "Training Epoch: 1 [7200/49669]\tLoss: 7210013.0000\n",
      "Training Epoch: 1 [7220/49669]\tLoss: 7489698.5000\n",
      "Training Epoch: 1 [7240/49669]\tLoss: 6674732.5000\n",
      "Training Epoch: 1 [7260/49669]\tLoss: 7190033.5000\n",
      "Training Epoch: 1 [7280/49669]\tLoss: 6309611.0000\n",
      "Training Epoch: 1 [7300/49669]\tLoss: 6882405.0000\n",
      "Training Epoch: 1 [7320/49669]\tLoss: 6945340.5000\n",
      "Training Epoch: 1 [7340/49669]\tLoss: 6890862.5000\n",
      "Training Epoch: 1 [7360/49669]\tLoss: 7250493.0000\n",
      "Training Epoch: 1 [7380/49669]\tLoss: 7799979.5000\n",
      "Training Epoch: 1 [7400/49669]\tLoss: 8380897.0000\n",
      "Training Epoch: 1 [7420/49669]\tLoss: 6113819.0000\n",
      "Training Epoch: 1 [7440/49669]\tLoss: 6958022.5000\n",
      "Training Epoch: 1 [7460/49669]\tLoss: 7623421.5000\n",
      "Training Epoch: 1 [7480/49669]\tLoss: 8043789.5000\n",
      "Training Epoch: 1 [7500/49669]\tLoss: 6925174.5000\n",
      "Training Epoch: 1 [7520/49669]\tLoss: 7103696.0000\n",
      "Training Epoch: 1 [7540/49669]\tLoss: 6893769.0000\n",
      "Training Epoch: 1 [7560/49669]\tLoss: 6356258.0000\n",
      "Training Epoch: 1 [7580/49669]\tLoss: 6558292.0000\n",
      "Training Epoch: 1 [7600/49669]\tLoss: 6851590.5000\n",
      "Training Epoch: 1 [7620/49669]\tLoss: 7185945.5000\n",
      "Training Epoch: 1 [7640/49669]\tLoss: 6397914.5000\n",
      "Training Epoch: 1 [7660/49669]\tLoss: 6380046.0000\n",
      "Training Epoch: 1 [7680/49669]\tLoss: 7032332.5000\n",
      "Training Epoch: 1 [7700/49669]\tLoss: 7354919.5000\n",
      "Training Epoch: 1 [7720/49669]\tLoss: 5385957.5000\n",
      "Training Epoch: 1 [7740/49669]\tLoss: 6799189.0000\n",
      "Training Epoch: 1 [7760/49669]\tLoss: 6087488.5000\n",
      "Training Epoch: 1 [7780/49669]\tLoss: 6885423.0000\n",
      "Training Epoch: 1 [7800/49669]\tLoss: 6960529.0000\n",
      "Training Epoch: 1 [7820/49669]\tLoss: 7655829.0000\n",
      "Training Epoch: 1 [7840/49669]\tLoss: 6544289.5000\n",
      "Training Epoch: 1 [7860/49669]\tLoss: 5865889.0000\n",
      "Training Epoch: 1 [7880/49669]\tLoss: 7128249.0000\n",
      "Training Epoch: 1 [7900/49669]\tLoss: 5973808.0000\n",
      "Training Epoch: 1 [7920/49669]\tLoss: 6770676.0000\n",
      "Training Epoch: 1 [7940/49669]\tLoss: 6627007.5000\n",
      "Training Epoch: 1 [7960/49669]\tLoss: 6641869.0000\n",
      "Training Epoch: 1 [7980/49669]\tLoss: 5925837.5000\n",
      "Training Epoch: 1 [8000/49669]\tLoss: 6252549.5000\n",
      "Training Epoch: 1 [8020/49669]\tLoss: 6245177.0000\n",
      "Training Epoch: 1 [8040/49669]\tLoss: 6997453.5000\n",
      "Training Epoch: 1 [8060/49669]\tLoss: 4654625.5000\n",
      "Training Epoch: 1 [8080/49669]\tLoss: 6200110.0000\n",
      "Training Epoch: 1 [8100/49669]\tLoss: 6004370.5000\n",
      "Training Epoch: 1 [8120/49669]\tLoss: 6784122.0000\n",
      "Training Epoch: 1 [8140/49669]\tLoss: 5616312.0000\n",
      "Training Epoch: 1 [8160/49669]\tLoss: 6072303.0000\n",
      "Training Epoch: 1 [8180/49669]\tLoss: 4786265.5000\n",
      "Training Epoch: 1 [8200/49669]\tLoss: 6022737.5000\n",
      "Training Epoch: 1 [8220/49669]\tLoss: 6386629.0000\n",
      "Training Epoch: 1 [8240/49669]\tLoss: 6983622.5000\n",
      "Training Epoch: 1 [8260/49669]\tLoss: 5351838.0000\n",
      "Training Epoch: 1 [8280/49669]\tLoss: 6017086.0000\n",
      "Training Epoch: 1 [8300/49669]\tLoss: 6717347.0000\n",
      "Training Epoch: 1 [8320/49669]\tLoss: 5804959.0000\n",
      "Training Epoch: 1 [8340/49669]\tLoss: 7013810.5000\n",
      "Training Epoch: 1 [8360/49669]\tLoss: 5821328.5000\n",
      "Training Epoch: 1 [8380/49669]\tLoss: 6551695.5000\n",
      "Training Epoch: 1 [8400/49669]\tLoss: 5382864.0000\n",
      "Training Epoch: 1 [8420/49669]\tLoss: 6217747.5000\n",
      "Training Epoch: 1 [8440/49669]\tLoss: 5611287.5000\n",
      "Training Epoch: 1 [8460/49669]\tLoss: 6863513.0000\n",
      "Training Epoch: 1 [8480/49669]\tLoss: 5506720.5000\n",
      "Training Epoch: 1 [8500/49669]\tLoss: 5226619.5000\n",
      "Training Epoch: 1 [8520/49669]\tLoss: 5929080.0000\n",
      "Training Epoch: 1 [8540/49669]\tLoss: 5894913.5000\n",
      "Training Epoch: 1 [8560/49669]\tLoss: 5934109.5000\n",
      "Training Epoch: 1 [8580/49669]\tLoss: 5676220.0000\n",
      "Training Epoch: 1 [8600/49669]\tLoss: 5446908.0000\n",
      "Training Epoch: 1 [8620/49669]\tLoss: 5317947.0000\n",
      "Training Epoch: 1 [8640/49669]\tLoss: 5756794.5000\n",
      "Training Epoch: 1 [8660/49669]\tLoss: 5794781.0000\n",
      "Training Epoch: 1 [8680/49669]\tLoss: 5703098.5000\n",
      "Training Epoch: 1 [8700/49669]\tLoss: 5075502.5000\n",
      "Training Epoch: 1 [8720/49669]\tLoss: 6191557.5000\n",
      "Training Epoch: 1 [8740/49669]\tLoss: 5339565.5000\n",
      "Training Epoch: 1 [8760/49669]\tLoss: 5405487.5000\n",
      "Training Epoch: 1 [8780/49669]\tLoss: 5094786.0000\n",
      "Training Epoch: 1 [8800/49669]\tLoss: 5394039.5000\n",
      "Training Epoch: 1 [8820/49669]\tLoss: 5605087.0000\n",
      "Training Epoch: 1 [8840/49669]\tLoss: 6058307.5000\n",
      "Training Epoch: 1 [8860/49669]\tLoss: 4956834.5000\n",
      "Training Epoch: 1 [8880/49669]\tLoss: 5565160.0000\n",
      "Training Epoch: 1 [8900/49669]\tLoss: 5077167.5000\n",
      "Training Epoch: 1 [8920/49669]\tLoss: 5541776.5000\n",
      "Training Epoch: 1 [8940/49669]\tLoss: 4833437.0000\n",
      "Training Epoch: 1 [8960/49669]\tLoss: 5878709.5000\n",
      "Training Epoch: 1 [8980/49669]\tLoss: 4827309.0000\n",
      "Training Epoch: 1 [9000/49669]\tLoss: 4331279.5000\n",
      "Training Epoch: 1 [9020/49669]\tLoss: 5064958.5000\n",
      "Training Epoch: 1 [9040/49669]\tLoss: 5452765.5000\n",
      "Training Epoch: 1 [9060/49669]\tLoss: 4432387.5000\n",
      "Training Epoch: 1 [9080/49669]\tLoss: 4916023.0000\n",
      "Training Epoch: 1 [9100/49669]\tLoss: 4846133.5000\n",
      "Training Epoch: 1 [9120/49669]\tLoss: 5112397.5000\n",
      "Training Epoch: 1 [9140/49669]\tLoss: 5604602.0000\n",
      "Training Epoch: 1 [9160/49669]\tLoss: 5056849.0000\n",
      "Training Epoch: 1 [9180/49669]\tLoss: 5189484.5000\n",
      "Training Epoch: 1 [9200/49669]\tLoss: 4495974.0000\n",
      "Training Epoch: 1 [9220/49669]\tLoss: 4838444.0000\n",
      "Training Epoch: 1 [9240/49669]\tLoss: 5166849.5000\n",
      "Training Epoch: 1 [9260/49669]\tLoss: 4541791.5000\n",
      "Training Epoch: 1 [9280/49669]\tLoss: 5223903.0000\n",
      "Training Epoch: 1 [9300/49669]\tLoss: 4451415.0000\n",
      "Training Epoch: 1 [9320/49669]\tLoss: 5420211.5000\n",
      "Training Epoch: 1 [9340/49669]\tLoss: 4664847.5000\n",
      "Training Epoch: 1 [9360/49669]\tLoss: 4951016.0000\n",
      "Training Epoch: 1 [9380/49669]\tLoss: 5184125.0000\n",
      "Training Epoch: 1 [9400/49669]\tLoss: 5086364.0000\n",
      "Training Epoch: 1 [9420/49669]\tLoss: 4643193.5000\n",
      "Training Epoch: 1 [9440/49669]\tLoss: 6102838.0000\n",
      "Training Epoch: 1 [9460/49669]\tLoss: 5297047.5000\n",
      "Training Epoch: 1 [9480/49669]\tLoss: 4557883.0000\n",
      "Training Epoch: 1 [9500/49669]\tLoss: 4753748.5000\n",
      "Training Epoch: 1 [9520/49669]\tLoss: 5425944.5000\n",
      "Training Epoch: 1 [9540/49669]\tLoss: 4552204.0000\n",
      "Training Epoch: 1 [9560/49669]\tLoss: 4908058.0000\n",
      "Training Epoch: 1 [9580/49669]\tLoss: 4385841.5000\n",
      "Training Epoch: 1 [9600/49669]\tLoss: 5354492.5000\n",
      "Training Epoch: 1 [9620/49669]\tLoss: 4941360.5000\n",
      "Training Epoch: 1 [9640/49669]\tLoss: 4535343.0000\n",
      "Training Epoch: 1 [9660/49669]\tLoss: 4455091.0000\n",
      "Training Epoch: 1 [9680/49669]\tLoss: 4572717.5000\n",
      "Training Epoch: 1 [9700/49669]\tLoss: 4358881.0000\n",
      "Training Epoch: 1 [9720/49669]\tLoss: 4559559.0000\n",
      "Training Epoch: 1 [9740/49669]\tLoss: 4576283.5000\n",
      "Training Epoch: 1 [9760/49669]\tLoss: 4469537.5000\n",
      "Training Epoch: 1 [9780/49669]\tLoss: 4534206.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [9800/49669]\tLoss: 5001495.0000\n",
      "Training Epoch: 1 [9820/49669]\tLoss: 4819827.5000\n",
      "Training Epoch: 1 [9840/49669]\tLoss: 4676181.5000\n",
      "Training Epoch: 1 [9860/49669]\tLoss: 3773834.5000\n",
      "Training Epoch: 1 [9880/49669]\tLoss: 4217626.5000\n",
      "Training Epoch: 1 [9900/49669]\tLoss: 5049567.0000\n",
      "Training Epoch: 1 [9920/49669]\tLoss: 4411650.0000\n",
      "Training Epoch: 1 [9940/49669]\tLoss: 4836163.0000\n",
      "Training Epoch: 1 [9960/49669]\tLoss: 4381456.5000\n",
      "Training Epoch: 1 [9980/49669]\tLoss: 4213858.5000\n",
      "Training Epoch: 1 [10000/49669]\tLoss: 4450923.0000\n",
      "Training Epoch: 1 [10020/49669]\tLoss: 4699947.5000\n",
      "Training Epoch: 1 [10040/49669]\tLoss: 4312351.5000\n",
      "Training Epoch: 1 [10060/49669]\tLoss: 4153216.7500\n",
      "Training Epoch: 1 [10080/49669]\tLoss: 4610011.5000\n",
      "Training Epoch: 1 [10100/49669]\tLoss: 4249742.0000\n",
      "Training Epoch: 1 [10120/49669]\tLoss: 4499844.5000\n",
      "Training Epoch: 1 [10140/49669]\tLoss: 5074100.5000\n",
      "Training Epoch: 1 [10160/49669]\tLoss: 3476074.0000\n",
      "Training Epoch: 1 [10180/49669]\tLoss: 4098997.7500\n",
      "Training Epoch: 1 [10200/49669]\tLoss: 3718852.2500\n",
      "Training Epoch: 1 [10220/49669]\tLoss: 4631209.0000\n",
      "Training Epoch: 1 [10240/49669]\tLoss: 3813771.0000\n",
      "Training Epoch: 1 [10260/49669]\tLoss: 3202952.0000\n",
      "Training Epoch: 1 [10280/49669]\tLoss: 4294356.0000\n",
      "Training Epoch: 1 [10300/49669]\tLoss: 3533759.0000\n",
      "Training Epoch: 1 [10320/49669]\tLoss: 4701297.5000\n",
      "Training Epoch: 1 [10340/49669]\tLoss: 4672931.5000\n",
      "Training Epoch: 1 [10360/49669]\tLoss: 4552927.5000\n",
      "Training Epoch: 1 [10380/49669]\tLoss: 3190304.2500\n",
      "Training Epoch: 1 [10400/49669]\tLoss: 4619251.5000\n",
      "Training Epoch: 1 [10420/49669]\tLoss: 4376961.5000\n",
      "Training Epoch: 1 [10440/49669]\tLoss: 3415403.7500\n",
      "Training Epoch: 1 [10460/49669]\tLoss: 3924610.5000\n",
      "Training Epoch: 1 [10480/49669]\tLoss: 4712884.0000\n",
      "Training Epoch: 1 [10500/49669]\tLoss: 4086751.0000\n",
      "Training Epoch: 1 [10520/49669]\tLoss: 3588607.7500\n",
      "Training Epoch: 1 [10540/49669]\tLoss: 4457863.5000\n",
      "Training Epoch: 1 [10560/49669]\tLoss: 3478674.0000\n",
      "Training Epoch: 1 [10580/49669]\tLoss: 4150063.0000\n",
      "Training Epoch: 1 [10600/49669]\tLoss: 4255469.0000\n",
      "Training Epoch: 1 [10620/49669]\tLoss: 4089568.5000\n",
      "Training Epoch: 1 [10640/49669]\tLoss: 4373448.0000\n",
      "Training Epoch: 1 [10660/49669]\tLoss: 4383362.5000\n",
      "Training Epoch: 1 [10680/49669]\tLoss: 4113121.5000\n",
      "Training Epoch: 1 [10700/49669]\tLoss: 4292206.5000\n",
      "Training Epoch: 1 [10720/49669]\tLoss: 3616053.0000\n",
      "Training Epoch: 1 [10740/49669]\tLoss: 4260560.5000\n",
      "Training Epoch: 1 [10760/49669]\tLoss: 3761712.7500\n",
      "Training Epoch: 1 [10780/49669]\tLoss: 4225082.0000\n",
      "Training Epoch: 1 [10800/49669]\tLoss: 4071128.2500\n",
      "Training Epoch: 1 [10820/49669]\tLoss: 4153543.7500\n",
      "Training Epoch: 1 [10840/49669]\tLoss: 4194620.5000\n",
      "Training Epoch: 1 [10860/49669]\tLoss: 3489851.7500\n",
      "Training Epoch: 1 [10880/49669]\tLoss: 3668360.7500\n",
      "Training Epoch: 1 [10900/49669]\tLoss: 3734583.2500\n",
      "Training Epoch: 1 [10920/49669]\tLoss: 3617884.5000\n",
      "Training Epoch: 1 [10940/49669]\tLoss: 3529073.5000\n",
      "Training Epoch: 1 [10960/49669]\tLoss: 3668448.5000\n",
      "Training Epoch: 1 [10980/49669]\tLoss: 3733900.0000\n",
      "Training Epoch: 1 [11000/49669]\tLoss: 3583207.7500\n",
      "Training Epoch: 1 [11020/49669]\tLoss: 3712239.5000\n",
      "Training Epoch: 1 [11040/49669]\tLoss: 3797802.2500\n",
      "Training Epoch: 1 [11060/49669]\tLoss: 3819091.5000\n",
      "Training Epoch: 1 [11080/49669]\tLoss: 3990049.2500\n",
      "Training Epoch: 1 [11100/49669]\tLoss: 3768044.2500\n",
      "Training Epoch: 1 [11120/49669]\tLoss: 4248060.0000\n",
      "Training Epoch: 1 [11140/49669]\tLoss: 3381910.7500\n",
      "Training Epoch: 1 [11160/49669]\tLoss: 3587593.2500\n",
      "Training Epoch: 1 [11180/49669]\tLoss: 3392172.7500\n",
      "Training Epoch: 1 [11200/49669]\tLoss: 3268604.0000\n",
      "Training Epoch: 1 [11220/49669]\tLoss: 4203266.5000\n",
      "Training Epoch: 1 [11240/49669]\tLoss: 3187364.5000\n",
      "Training Epoch: 1 [11260/49669]\tLoss: 3245661.0000\n",
      "Training Epoch: 1 [11280/49669]\tLoss: 3906550.0000\n",
      "Training Epoch: 1 [11300/49669]\tLoss: 3919226.5000\n",
      "Training Epoch: 1 [11320/49669]\tLoss: 3661959.0000\n",
      "Training Epoch: 1 [11340/49669]\tLoss: 3266028.5000\n",
      "Training Epoch: 1 [11360/49669]\tLoss: 3803041.2500\n",
      "Training Epoch: 1 [11380/49669]\tLoss: 2782129.7500\n",
      "Training Epoch: 1 [11400/49669]\tLoss: 3436843.5000\n",
      "Training Epoch: 1 [11420/49669]\tLoss: 3220073.2500\n",
      "Training Epoch: 1 [11440/49669]\tLoss: 3591611.7500\n",
      "Training Epoch: 1 [11460/49669]\tLoss: 3425141.0000\n",
      "Training Epoch: 1 [11480/49669]\tLoss: 3228798.5000\n",
      "Training Epoch: 1 [11500/49669]\tLoss: 3975496.7500\n",
      "Training Epoch: 1 [11520/49669]\tLoss: 2960654.5000\n",
      "Training Epoch: 1 [11540/49669]\tLoss: 3928889.5000\n",
      "Training Epoch: 1 [11560/49669]\tLoss: 3438132.7500\n",
      "Training Epoch: 1 [11580/49669]\tLoss: 3247471.2500\n",
      "Training Epoch: 1 [11600/49669]\tLoss: 3537003.0000\n",
      "Training Epoch: 1 [11620/49669]\tLoss: 3649289.2500\n",
      "Training Epoch: 1 [11640/49669]\tLoss: 3083119.0000\n",
      "Training Epoch: 1 [11660/49669]\tLoss: 2777506.0000\n",
      "Training Epoch: 1 [11680/49669]\tLoss: 3332770.7500\n",
      "Training Epoch: 1 [11700/49669]\tLoss: 3599783.0000\n",
      "Training Epoch: 1 [11720/49669]\tLoss: 3220218.7500\n",
      "Training Epoch: 1 [11740/49669]\tLoss: 2968510.5000\n",
      "Training Epoch: 1 [11760/49669]\tLoss: 3153687.0000\n",
      "Training Epoch: 1 [11780/49669]\tLoss: 3231677.0000\n",
      "Training Epoch: 1 [11800/49669]\tLoss: 2892005.5000\n",
      "Training Epoch: 1 [11820/49669]\tLoss: 2840865.7500\n",
      "Training Epoch: 1 [11840/49669]\tLoss: 3044326.5000\n",
      "Training Epoch: 1 [11860/49669]\tLoss: 3020325.2500\n",
      "Training Epoch: 1 [11880/49669]\tLoss: 3473902.5000\n",
      "Training Epoch: 1 [11900/49669]\tLoss: 3439053.0000\n",
      "Training Epoch: 1 [11920/49669]\tLoss: 3650058.5000\n",
      "Training Epoch: 1 [11940/49669]\tLoss: 3092303.2500\n",
      "Training Epoch: 1 [11960/49669]\tLoss: 2923799.7500\n",
      "Training Epoch: 1 [11980/49669]\tLoss: 3379114.5000\n",
      "Training Epoch: 1 [12000/49669]\tLoss: 3220434.2500\n",
      "Training Epoch: 1 [12020/49669]\tLoss: 2892573.5000\n",
      "Training Epoch: 1 [12040/49669]\tLoss: 3289948.7500\n",
      "Training Epoch: 1 [12060/49669]\tLoss: 3538726.5000\n",
      "Training Epoch: 1 [12080/49669]\tLoss: 2776699.0000\n",
      "Training Epoch: 1 [12100/49669]\tLoss: 3147523.2500\n",
      "Training Epoch: 1 [12120/49669]\tLoss: 3402024.7500\n",
      "Training Epoch: 1 [12140/49669]\tLoss: 3320934.5000\n",
      "Training Epoch: 1 [12160/49669]\tLoss: 2988869.2500\n",
      "Training Epoch: 1 [12180/49669]\tLoss: 2962988.2500\n",
      "Training Epoch: 1 [12200/49669]\tLoss: 3234697.5000\n",
      "Training Epoch: 1 [12220/49669]\tLoss: 3041082.7500\n",
      "Training Epoch: 1 [12240/49669]\tLoss: 3279509.7500\n",
      "Training Epoch: 1 [12260/49669]\tLoss: 2753569.7500\n",
      "Training Epoch: 1 [12280/49669]\tLoss: 2691323.5000\n",
      "Training Epoch: 1 [12300/49669]\tLoss: 2754482.5000\n",
      "Training Epoch: 1 [12320/49669]\tLoss: 2943352.5000\n",
      "Training Epoch: 1 [12340/49669]\tLoss: 2921958.0000\n",
      "Training Epoch: 1 [12360/49669]\tLoss: 2934901.0000\n",
      "Training Epoch: 1 [12380/49669]\tLoss: 2715649.0000\n",
      "Training Epoch: 1 [12400/49669]\tLoss: 2885003.2500\n",
      "Training Epoch: 1 [12420/49669]\tLoss: 2734422.5000\n",
      "Training Epoch: 1 [12440/49669]\tLoss: 2968230.7500\n",
      "Training Epoch: 1 [12460/49669]\tLoss: 2844936.2500\n",
      "Training Epoch: 1 [12480/49669]\tLoss: 2935178.0000\n",
      "Training Epoch: 1 [12500/49669]\tLoss: 3164763.0000\n",
      "Training Epoch: 1 [12520/49669]\tLoss: 2973626.0000\n",
      "Training Epoch: 1 [12540/49669]\tLoss: 2996871.5000\n",
      "Training Epoch: 1 [12560/49669]\tLoss: 2364846.0000\n",
      "Training Epoch: 1 [12580/49669]\tLoss: 2945431.5000\n",
      "Training Epoch: 1 [12600/49669]\tLoss: 2776975.0000\n",
      "Training Epoch: 1 [12620/49669]\tLoss: 2801810.5000\n",
      "Training Epoch: 1 [12640/49669]\tLoss: 3016791.7500\n",
      "Training Epoch: 1 [12660/49669]\tLoss: 2765689.0000\n",
      "Training Epoch: 1 [12680/49669]\tLoss: 2771734.7500\n",
      "Training Epoch: 1 [12700/49669]\tLoss: 2769742.5000\n",
      "Training Epoch: 1 [12720/49669]\tLoss: 2476882.0000\n",
      "Training Epoch: 1 [12740/49669]\tLoss: 3004345.2500\n",
      "Training Epoch: 1 [12760/49669]\tLoss: 2729189.5000\n",
      "Training Epoch: 1 [12780/49669]\tLoss: 2875979.5000\n",
      "Training Epoch: 1 [12800/49669]\tLoss: 2676896.2500\n",
      "Training Epoch: 1 [12820/49669]\tLoss: 2759027.2500\n",
      "Training Epoch: 1 [12840/49669]\tLoss: 2768487.0000\n",
      "Training Epoch: 1 [12860/49669]\tLoss: 2649531.0000\n",
      "Training Epoch: 1 [12880/49669]\tLoss: 2636546.5000\n",
      "Training Epoch: 1 [12900/49669]\tLoss: 2674596.0000\n",
      "Training Epoch: 1 [12920/49669]\tLoss: 2778008.7500\n",
      "Training Epoch: 1 [12940/49669]\tLoss: 2586380.2500\n",
      "Training Epoch: 1 [12960/49669]\tLoss: 2686683.7500\n",
      "Training Epoch: 1 [12980/49669]\tLoss: 2822797.0000\n",
      "Training Epoch: 1 [13000/49669]\tLoss: 2532063.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [13020/49669]\tLoss: 2447263.5000\n",
      "Training Epoch: 1 [13040/49669]\tLoss: 2775290.5000\n",
      "Training Epoch: 1 [13060/49669]\tLoss: 2735651.2500\n",
      "Training Epoch: 1 [13080/49669]\tLoss: 2344188.7500\n",
      "Training Epoch: 1 [13100/49669]\tLoss: 2998329.5000\n",
      "Training Epoch: 1 [13120/49669]\tLoss: 2603781.2500\n",
      "Training Epoch: 1 [13140/49669]\tLoss: 2819983.7500\n",
      "Training Epoch: 1 [13160/49669]\tLoss: 2527757.5000\n",
      "Training Epoch: 1 [13180/49669]\tLoss: 2158467.2500\n",
      "Training Epoch: 1 [13200/49669]\tLoss: 2739459.7500\n",
      "Training Epoch: 1 [13220/49669]\tLoss: 2365269.7500\n",
      "Training Epoch: 1 [13240/49669]\tLoss: 2317116.2500\n",
      "Training Epoch: 1 [13260/49669]\tLoss: 2509412.2500\n",
      "Training Epoch: 1 [13280/49669]\tLoss: 2484181.5000\n",
      "Training Epoch: 1 [13300/49669]\tLoss: 2651497.7500\n",
      "Training Epoch: 1 [13320/49669]\tLoss: 2855108.7500\n",
      "Training Epoch: 1 [13340/49669]\tLoss: 2129930.7500\n",
      "Training Epoch: 1 [13360/49669]\tLoss: 2200918.2500\n",
      "Training Epoch: 1 [13380/49669]\tLoss: 2435139.7500\n",
      "Training Epoch: 1 [13400/49669]\tLoss: 2268809.0000\n",
      "Training Epoch: 1 [13420/49669]\tLoss: 2679150.0000\n",
      "Training Epoch: 1 [13440/49669]\tLoss: 2780538.5000\n",
      "Training Epoch: 1 [13460/49669]\tLoss: 2540333.7500\n",
      "Training Epoch: 1 [13480/49669]\tLoss: 2391858.5000\n",
      "Training Epoch: 1 [13500/49669]\tLoss: 2401836.7500\n",
      "Training Epoch: 1 [13520/49669]\tLoss: 2398043.7500\n",
      "Training Epoch: 1 [13540/49669]\tLoss: 2050513.1250\n",
      "Training Epoch: 1 [13560/49669]\tLoss: 2516198.2500\n",
      "Training Epoch: 1 [13580/49669]\tLoss: 2832937.7500\n",
      "Training Epoch: 1 [13600/49669]\tLoss: 2589315.5000\n",
      "Training Epoch: 1 [13620/49669]\tLoss: 2439721.2500\n",
      "Training Epoch: 1 [13640/49669]\tLoss: 2145877.2500\n",
      "Training Epoch: 1 [13660/49669]\tLoss: 2354655.2500\n",
      "Training Epoch: 1 [13680/49669]\tLoss: 2474692.0000\n",
      "Training Epoch: 1 [13700/49669]\tLoss: 2593322.5000\n",
      "Training Epoch: 1 [13720/49669]\tLoss: 2239775.7500\n",
      "Training Epoch: 1 [13740/49669]\tLoss: 2614750.7500\n",
      "Training Epoch: 1 [13760/49669]\tLoss: 2568121.7500\n",
      "Training Epoch: 1 [13780/49669]\tLoss: 2407650.0000\n",
      "Training Epoch: 1 [13800/49669]\tLoss: 1860867.8750\n",
      "Training Epoch: 1 [13820/49669]\tLoss: 2341482.5000\n",
      "Training Epoch: 1 [13840/49669]\tLoss: 2180217.0000\n",
      "Training Epoch: 1 [13860/49669]\tLoss: 2350068.5000\n",
      "Training Epoch: 1 [13880/49669]\tLoss: 2149188.2500\n",
      "Training Epoch: 1 [13900/49669]\tLoss: 2164939.5000\n",
      "Training Epoch: 1 [13920/49669]\tLoss: 2174047.7500\n",
      "Training Epoch: 1 [13940/49669]\tLoss: 1951890.3750\n",
      "Training Epoch: 1 [13960/49669]\tLoss: 2219851.0000\n",
      "Training Epoch: 1 [13980/49669]\tLoss: 2331478.7500\n",
      "Training Epoch: 1 [14000/49669]\tLoss: 2317208.7500\n",
      "Training Epoch: 1 [14020/49669]\tLoss: 2415697.2500\n",
      "Training Epoch: 1 [14040/49669]\tLoss: 2335075.0000\n",
      "Training Epoch: 1 [14060/49669]\tLoss: 2277274.7500\n",
      "Training Epoch: 1 [14080/49669]\tLoss: 2207825.5000\n",
      "Training Epoch: 1 [14100/49669]\tLoss: 2326508.2500\n",
      "Training Epoch: 1 [14120/49669]\tLoss: 2127975.5000\n",
      "Training Epoch: 1 [14140/49669]\tLoss: 2433601.0000\n",
      "Training Epoch: 1 [14160/49669]\tLoss: 1836483.2500\n",
      "Training Epoch: 1 [14180/49669]\tLoss: 1877529.2500\n",
      "Training Epoch: 1 [14200/49669]\tLoss: 2274151.0000\n",
      "Training Epoch: 1 [14220/49669]\tLoss: 2356694.5000\n",
      "Training Epoch: 1 [14240/49669]\tLoss: 2114183.0000\n",
      "Training Epoch: 1 [14260/49669]\tLoss: 2330151.2500\n",
      "Training Epoch: 1 [14280/49669]\tLoss: 2511567.2500\n",
      "Training Epoch: 1 [14300/49669]\tLoss: 2304458.2500\n",
      "Training Epoch: 1 [14320/49669]\tLoss: 2014063.5000\n",
      "Training Epoch: 1 [14340/49669]\tLoss: 1716591.2500\n",
      "Training Epoch: 1 [14360/49669]\tLoss: 2318629.7500\n",
      "Training Epoch: 1 [14380/49669]\tLoss: 2268823.0000\n",
      "Training Epoch: 1 [14400/49669]\tLoss: 1987845.8750\n",
      "Training Epoch: 1 [14420/49669]\tLoss: 1970557.1250\n",
      "Training Epoch: 1 [14440/49669]\tLoss: 2141384.7500\n",
      "Training Epoch: 1 [14460/49669]\tLoss: 1918880.2500\n",
      "Training Epoch: 1 [14480/49669]\tLoss: 2297165.5000\n",
      "Training Epoch: 1 [14500/49669]\tLoss: 1549959.3750\n",
      "Training Epoch: 1 [14520/49669]\tLoss: 2006679.7500\n",
      "Training Epoch: 1 [14540/49669]\tLoss: 1934682.0000\n",
      "Training Epoch: 1 [14560/49669]\tLoss: 2233644.2500\n",
      "Training Epoch: 1 [14580/49669]\tLoss: 1801500.1250\n",
      "Training Epoch: 1 [14600/49669]\tLoss: 2079563.5000\n",
      "Training Epoch: 1 [14620/49669]\tLoss: 1830406.8750\n",
      "Training Epoch: 1 [14640/49669]\tLoss: 2107969.7500\n",
      "Training Epoch: 1 [14660/49669]\tLoss: 2190329.7500\n",
      "Training Epoch: 1 [14680/49669]\tLoss: 2176200.2500\n",
      "Training Epoch: 1 [14700/49669]\tLoss: 2054916.3750\n",
      "Training Epoch: 1 [14720/49669]\tLoss: 2175819.7500\n",
      "Training Epoch: 1 [14740/49669]\tLoss: 2007078.7500\n",
      "Training Epoch: 1 [14760/49669]\tLoss: 1851269.8750\n",
      "Training Epoch: 1 [14780/49669]\tLoss: 1777184.3750\n",
      "Training Epoch: 1 [14800/49669]\tLoss: 1969624.1250\n",
      "Training Epoch: 1 [14820/49669]\tLoss: 2185308.7500\n",
      "Training Epoch: 1 [14840/49669]\tLoss: 1862681.3750\n",
      "Training Epoch: 1 [14860/49669]\tLoss: 1657606.3750\n",
      "Training Epoch: 1 [14880/49669]\tLoss: 2194489.7500\n",
      "Training Epoch: 1 [14900/49669]\tLoss: 1808941.6250\n",
      "Training Epoch: 1 [14920/49669]\tLoss: 1674060.3750\n",
      "Training Epoch: 1 [14940/49669]\tLoss: 2136832.7500\n",
      "Training Epoch: 1 [14960/49669]\tLoss: 2012093.2500\n",
      "Training Epoch: 1 [14980/49669]\tLoss: 2240870.7500\n",
      "Training Epoch: 1 [15000/49669]\tLoss: 1882360.7500\n",
      "Training Epoch: 1 [15020/49669]\tLoss: 2065449.5000\n",
      "Training Epoch: 1 [15040/49669]\tLoss: 2093976.2500\n",
      "Training Epoch: 1 [15060/49669]\tLoss: 1933074.1250\n",
      "Training Epoch: 1 [15080/49669]\tLoss: 2228278.7500\n",
      "Training Epoch: 1 [15100/49669]\tLoss: 2136716.5000\n",
      "Training Epoch: 1 [15120/49669]\tLoss: 2010810.8750\n",
      "Training Epoch: 1 [15140/49669]\tLoss: 1679685.2500\n",
      "Training Epoch: 1 [15160/49669]\tLoss: 1981657.5000\n",
      "Training Epoch: 1 [15180/49669]\tLoss: 2276303.5000\n",
      "Training Epoch: 1 [15200/49669]\tLoss: 2212111.2500\n",
      "Training Epoch: 1 [15220/49669]\tLoss: 1713947.0000\n",
      "Training Epoch: 1 [15240/49669]\tLoss: 1972339.8750\n",
      "Training Epoch: 1 [15260/49669]\tLoss: 1724118.8750\n",
      "Training Epoch: 1 [15280/49669]\tLoss: 1904600.6250\n",
      "Training Epoch: 1 [15300/49669]\tLoss: 1739781.3750\n",
      "Training Epoch: 1 [15320/49669]\tLoss: 1571494.2500\n",
      "Training Epoch: 1 [15340/49669]\tLoss: 1967051.3750\n",
      "Training Epoch: 1 [15360/49669]\tLoss: 1757669.6250\n",
      "Training Epoch: 1 [15380/49669]\tLoss: 1373497.7500\n",
      "Training Epoch: 1 [15400/49669]\tLoss: 1684083.3750\n",
      "Training Epoch: 1 [15420/49669]\tLoss: 1860384.3750\n",
      "Training Epoch: 1 [15440/49669]\tLoss: 1968104.7500\n",
      "Training Epoch: 1 [15460/49669]\tLoss: 1596924.2500\n",
      "Training Epoch: 1 [15480/49669]\tLoss: 1917701.2500\n",
      "Training Epoch: 1 [15500/49669]\tLoss: 1967374.8750\n",
      "Training Epoch: 1 [15520/49669]\tLoss: 1921209.0000\n",
      "Training Epoch: 1 [15540/49669]\tLoss: 1971189.0000\n",
      "Training Epoch: 1 [15560/49669]\tLoss: 2033937.1250\n",
      "Training Epoch: 1 [15580/49669]\tLoss: 1900665.2500\n",
      "Training Epoch: 1 [15600/49669]\tLoss: 1878897.6250\n",
      "Training Epoch: 1 [15620/49669]\tLoss: 1859300.8750\n",
      "Training Epoch: 1 [15640/49669]\tLoss: 1662857.6250\n",
      "Training Epoch: 1 [15660/49669]\tLoss: 1659317.1250\n",
      "Training Epoch: 1 [15680/49669]\tLoss: 1766042.5000\n",
      "Training Epoch: 1 [15700/49669]\tLoss: 1753818.3750\n",
      "Training Epoch: 1 [15720/49669]\tLoss: 1754806.2500\n",
      "Training Epoch: 1 [15740/49669]\tLoss: 1738656.0000\n",
      "Training Epoch: 1 [15760/49669]\tLoss: 1749918.6250\n",
      "Training Epoch: 1 [15780/49669]\tLoss: 1694945.8750\n",
      "Training Epoch: 1 [15800/49669]\tLoss: 1732625.5000\n",
      "Training Epoch: 1 [15820/49669]\tLoss: 1878861.8750\n",
      "Training Epoch: 1 [15840/49669]\tLoss: 2009801.6250\n",
      "Training Epoch: 1 [15860/49669]\tLoss: 1500692.3750\n",
      "Training Epoch: 1 [15880/49669]\tLoss: 1755692.6250\n",
      "Training Epoch: 1 [15900/49669]\tLoss: 1564585.8750\n",
      "Training Epoch: 1 [15920/49669]\tLoss: 1655877.7500\n",
      "Training Epoch: 1 [15940/49669]\tLoss: 1413222.5000\n",
      "Training Epoch: 1 [15960/49669]\tLoss: 1725935.2500\n",
      "Training Epoch: 1 [15980/49669]\tLoss: 1558092.2500\n",
      "Training Epoch: 1 [16000/49669]\tLoss: 1643367.6250\n",
      "Training Epoch: 1 [16020/49669]\tLoss: 1787486.5000\n",
      "Training Epoch: 1 [16040/49669]\tLoss: 1803426.6250\n",
      "Training Epoch: 1 [16060/49669]\tLoss: 1775022.3750\n",
      "Training Epoch: 1 [16080/49669]\tLoss: 1839613.2500\n",
      "Training Epoch: 1 [16100/49669]\tLoss: 1591168.3750\n",
      "Training Epoch: 1 [16120/49669]\tLoss: 1587493.8750\n",
      "Training Epoch: 1 [16140/49669]\tLoss: 1778593.5000\n",
      "Training Epoch: 1 [16160/49669]\tLoss: 1456401.2500\n",
      "Training Epoch: 1 [16180/49669]\tLoss: 1743510.1250\n",
      "Training Epoch: 1 [16200/49669]\tLoss: 1528581.6250\n",
      "Training Epoch: 1 [16220/49669]\tLoss: 1740089.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [16240/49669]\tLoss: 1945513.6250\n",
      "Training Epoch: 1 [16260/49669]\tLoss: 1782694.2500\n",
      "Training Epoch: 1 [16280/49669]\tLoss: 1452959.0000\n",
      "Training Epoch: 1 [16300/49669]\tLoss: 1557277.7500\n",
      "Training Epoch: 1 [16320/49669]\tLoss: 1541569.5000\n",
      "Training Epoch: 1 [16340/49669]\tLoss: 1715548.2500\n",
      "Training Epoch: 1 [16360/49669]\tLoss: 1471757.2500\n",
      "Training Epoch: 1 [16380/49669]\tLoss: 1742884.7500\n",
      "Training Epoch: 1 [16400/49669]\tLoss: 1264151.5000\n",
      "Training Epoch: 1 [16420/49669]\tLoss: 1662690.8750\n",
      "Training Epoch: 1 [16440/49669]\tLoss: 1403758.7500\n",
      "Training Epoch: 1 [16460/49669]\tLoss: 1533320.5000\n",
      "Training Epoch: 1 [16480/49669]\tLoss: 1691301.8750\n",
      "Training Epoch: 1 [16500/49669]\tLoss: 1584628.3750\n",
      "Training Epoch: 1 [16520/49669]\tLoss: 1450020.3750\n",
      "Training Epoch: 1 [16540/49669]\tLoss: 1383165.6250\n",
      "Training Epoch: 1 [16560/49669]\tLoss: 1448975.7500\n",
      "Training Epoch: 1 [16580/49669]\tLoss: 1398679.0000\n",
      "Training Epoch: 1 [16600/49669]\tLoss: 1720753.3750\n",
      "Training Epoch: 1 [16620/49669]\tLoss: 1657325.6250\n",
      "Training Epoch: 1 [16640/49669]\tLoss: 1639192.6250\n",
      "Training Epoch: 1 [16660/49669]\tLoss: 1506682.6250\n",
      "Training Epoch: 1 [16680/49669]\tLoss: 1578877.5000\n",
      "Training Epoch: 1 [16700/49669]\tLoss: 1638391.3750\n",
      "Training Epoch: 1 [16720/49669]\tLoss: 1676555.5000\n",
      "Training Epoch: 1 [16740/49669]\tLoss: 1393353.6250\n",
      "Training Epoch: 1 [16760/49669]\tLoss: 1383778.6250\n",
      "Training Epoch: 1 [16780/49669]\tLoss: 1535624.1250\n",
      "Training Epoch: 1 [16800/49669]\tLoss: 1280199.7500\n",
      "Training Epoch: 1 [16820/49669]\tLoss: 1604779.7500\n",
      "Training Epoch: 1 [16840/49669]\tLoss: 1555434.0000\n",
      "Training Epoch: 1 [16860/49669]\tLoss: 1691130.0000\n",
      "Training Epoch: 1 [16880/49669]\tLoss: 1516995.8750\n",
      "Training Epoch: 1 [16900/49669]\tLoss: 1612244.5000\n",
      "Training Epoch: 1 [16920/49669]\tLoss: 1521794.8750\n",
      "Training Epoch: 1 [16940/49669]\tLoss: 1338014.6250\n",
      "Training Epoch: 1 [16960/49669]\tLoss: 1594708.8750\n",
      "Training Epoch: 1 [16980/49669]\tLoss: 1337091.6250\n",
      "Training Epoch: 1 [17000/49669]\tLoss: 1407239.2500\n",
      "Training Epoch: 1 [17020/49669]\tLoss: 1462659.0000\n",
      "Training Epoch: 1 [17040/49669]\tLoss: 1298291.3750\n",
      "Training Epoch: 1 [17060/49669]\tLoss: 1157454.6250\n",
      "Training Epoch: 1 [17080/49669]\tLoss: 1432175.7500\n",
      "Training Epoch: 1 [17100/49669]\tLoss: 1308009.3750\n",
      "Training Epoch: 1 [17120/49669]\tLoss: 1580784.6250\n",
      "Training Epoch: 1 [17140/49669]\tLoss: 1602604.8750\n",
      "Training Epoch: 1 [17160/49669]\tLoss: 1476659.1250\n",
      "Training Epoch: 1 [17180/49669]\tLoss: 1448255.2500\n",
      "Training Epoch: 1 [17200/49669]\tLoss: 1516971.0000\n",
      "Training Epoch: 1 [17220/49669]\tLoss: 1478397.1250\n",
      "Training Epoch: 1 [17240/49669]\tLoss: 1370204.3750\n",
      "Training Epoch: 1 [17260/49669]\tLoss: 1249778.6250\n",
      "Training Epoch: 1 [17280/49669]\tLoss: 1523816.8750\n",
      "Training Epoch: 1 [17300/49669]\tLoss: 1401844.3750\n",
      "Training Epoch: 1 [17320/49669]\tLoss: 1256646.0000\n",
      "Training Epoch: 1 [17340/49669]\tLoss: 1526339.6250\n",
      "Training Epoch: 1 [17360/49669]\tLoss: 1489131.7500\n",
      "Training Epoch: 1 [17380/49669]\tLoss: 1248868.0000\n",
      "Training Epoch: 1 [17400/49669]\tLoss: 1135102.6250\n",
      "Training Epoch: 1 [17420/49669]\tLoss: 1479113.0000\n",
      "Training Epoch: 1 [17440/49669]\tLoss: 1310407.6250\n",
      "Training Epoch: 1 [17460/49669]\tLoss: 1336936.6250\n",
      "Training Epoch: 1 [17480/49669]\tLoss: 1485806.1250\n",
      "Training Epoch: 1 [17500/49669]\tLoss: 1427259.3750\n",
      "Training Epoch: 1 [17520/49669]\tLoss: 1085767.5000\n",
      "Training Epoch: 1 [17540/49669]\tLoss: 1511391.0000\n",
      "Training Epoch: 1 [17560/49669]\tLoss: 1467287.8750\n",
      "Training Epoch: 1 [17580/49669]\tLoss: 1363685.3750\n",
      "Training Epoch: 1 [17600/49669]\tLoss: 1560833.6250\n",
      "Training Epoch: 1 [17620/49669]\tLoss: 1320325.0000\n",
      "Training Epoch: 1 [17640/49669]\tLoss: 1286477.7500\n",
      "Training Epoch: 1 [17660/49669]\tLoss: 1472957.0000\n",
      "Training Epoch: 1 [17680/49669]\tLoss: 1186275.8750\n",
      "Training Epoch: 1 [17700/49669]\tLoss: 1103014.1250\n",
      "Training Epoch: 1 [17720/49669]\tLoss: 1193255.7500\n",
      "Training Epoch: 1 [17740/49669]\tLoss: 1299724.3750\n",
      "Training Epoch: 1 [17760/49669]\tLoss: 1293280.7500\n",
      "Training Epoch: 1 [17780/49669]\tLoss: 1556642.8750\n",
      "Training Epoch: 1 [17800/49669]\tLoss: 1513969.2500\n",
      "Training Epoch: 1 [17820/49669]\tLoss: 1249935.7500\n",
      "Training Epoch: 1 [17840/49669]\tLoss: 1109110.8750\n",
      "Training Epoch: 1 [17860/49669]\tLoss: 1115343.7500\n",
      "Training Epoch: 1 [17880/49669]\tLoss: 1282965.1250\n",
      "Training Epoch: 1 [17900/49669]\tLoss: 1148729.7500\n",
      "Training Epoch: 1 [17920/49669]\tLoss: 1329165.3750\n",
      "Training Epoch: 1 [17940/49669]\tLoss: 1288807.7500\n",
      "Training Epoch: 1 [17960/49669]\tLoss: 1364833.8750\n",
      "Training Epoch: 1 [17980/49669]\tLoss: 1308784.0000\n",
      "Training Epoch: 1 [18000/49669]\tLoss: 1242518.6250\n",
      "Training Epoch: 1 [18020/49669]\tLoss: 1432101.3750\n",
      "Training Epoch: 1 [18040/49669]\tLoss: 1263841.1250\n",
      "Training Epoch: 1 [18060/49669]\tLoss: 1345779.7500\n",
      "Training Epoch: 1 [18080/49669]\tLoss: 1374114.6250\n",
      "Training Epoch: 1 [18100/49669]\tLoss: 1285076.2500\n",
      "Training Epoch: 1 [18120/49669]\tLoss: 1283502.7500\n",
      "Training Epoch: 1 [18140/49669]\tLoss: 927366.3125\n",
      "Training Epoch: 1 [18160/49669]\tLoss: 1174527.1250\n",
      "Training Epoch: 1 [18180/49669]\tLoss: 1282907.1250\n",
      "Training Epoch: 1 [18200/49669]\tLoss: 1248210.0000\n",
      "Training Epoch: 1 [18220/49669]\tLoss: 1170959.0000\n",
      "Training Epoch: 1 [18240/49669]\tLoss: 1201617.1250\n",
      "Training Epoch: 1 [18260/49669]\tLoss: 1311324.6250\n",
      "Training Epoch: 1 [18280/49669]\tLoss: 978151.3750\n",
      "Training Epoch: 1 [18300/49669]\tLoss: 1274332.7500\n",
      "Training Epoch: 1 [18320/49669]\tLoss: 974817.8125\n",
      "Training Epoch: 1 [18340/49669]\tLoss: 1205323.1250\n",
      "Training Epoch: 1 [18360/49669]\tLoss: 1244218.1250\n",
      "Training Epoch: 1 [18380/49669]\tLoss: 1227619.3750\n",
      "Training Epoch: 1 [18400/49669]\tLoss: 1360320.6250\n",
      "Training Epoch: 1 [18420/49669]\tLoss: 1321034.2500\n",
      "Training Epoch: 1 [18440/49669]\tLoss: 1192568.0000\n",
      "Training Epoch: 1 [18460/49669]\tLoss: 1153184.3750\n",
      "Training Epoch: 1 [18480/49669]\tLoss: 1233875.0000\n",
      "Training Epoch: 1 [18500/49669]\tLoss: 1235480.5000\n",
      "Training Epoch: 1 [18520/49669]\tLoss: 1135870.3750\n",
      "Training Epoch: 1 [18540/49669]\tLoss: 911954.0000\n",
      "Training Epoch: 1 [18560/49669]\tLoss: 1201849.5000\n",
      "Training Epoch: 1 [18580/49669]\tLoss: 1122961.8750\n",
      "Training Epoch: 1 [18600/49669]\tLoss: 1213632.1250\n",
      "Training Epoch: 1 [18620/49669]\tLoss: 1154201.6250\n",
      "Training Epoch: 1 [18640/49669]\tLoss: 1079904.6250\n",
      "Training Epoch: 1 [18660/49669]\tLoss: 1140986.7500\n",
      "Training Epoch: 1 [18680/49669]\tLoss: 1174760.7500\n",
      "Training Epoch: 1 [18700/49669]\tLoss: 1318149.5000\n",
      "Training Epoch: 1 [18720/49669]\tLoss: 1242146.3750\n",
      "Training Epoch: 1 [18740/49669]\tLoss: 1176904.5000\n",
      "Training Epoch: 1 [18760/49669]\tLoss: 1265822.6250\n",
      "Training Epoch: 1 [18780/49669]\tLoss: 1169746.2500\n",
      "Training Epoch: 1 [18800/49669]\tLoss: 1107189.5000\n",
      "Training Epoch: 1 [18820/49669]\tLoss: 1274503.1250\n",
      "Training Epoch: 1 [18840/49669]\tLoss: 992152.2500\n",
      "Training Epoch: 1 [18860/49669]\tLoss: 1198369.5000\n",
      "Training Epoch: 1 [18880/49669]\tLoss: 986638.7500\n",
      "Training Epoch: 1 [18900/49669]\tLoss: 1115427.0000\n",
      "Training Epoch: 1 [18920/49669]\tLoss: 1214328.3750\n",
      "Training Epoch: 1 [18940/49669]\tLoss: 1084722.2500\n",
      "Training Epoch: 1 [18960/49669]\tLoss: 1087733.0000\n",
      "Training Epoch: 1 [18980/49669]\tLoss: 1290498.7500\n",
      "Training Epoch: 1 [19000/49669]\tLoss: 1128298.0000\n",
      "Training Epoch: 1 [19020/49669]\tLoss: 1271043.8750\n",
      "Training Epoch: 1 [19040/49669]\tLoss: 1046721.2500\n",
      "Training Epoch: 1 [19060/49669]\tLoss: 1042145.8750\n",
      "Training Epoch: 1 [19080/49669]\tLoss: 1119034.0000\n",
      "Training Epoch: 1 [19100/49669]\tLoss: 1182982.1250\n",
      "Training Epoch: 1 [19120/49669]\tLoss: 1042610.1250\n",
      "Training Epoch: 1 [19140/49669]\tLoss: 1236332.3750\n",
      "Training Epoch: 1 [19160/49669]\tLoss: 1171104.6250\n",
      "Training Epoch: 1 [19180/49669]\tLoss: 1178530.8750\n",
      "Training Epoch: 1 [19200/49669]\tLoss: 1111106.8750\n",
      "Training Epoch: 1 [19220/49669]\tLoss: 1050389.3750\n",
      "Training Epoch: 1 [19240/49669]\tLoss: 1087728.1250\n",
      "Training Epoch: 1 [19260/49669]\tLoss: 1075421.7500\n",
      "Training Epoch: 1 [19280/49669]\tLoss: 723080.2500\n",
      "Training Epoch: 1 [19300/49669]\tLoss: 863078.3125\n",
      "Training Epoch: 1 [19320/49669]\tLoss: 1101251.7500\n",
      "Training Epoch: 1 [19340/49669]\tLoss: 1109581.6250\n",
      "Training Epoch: 1 [19360/49669]\tLoss: 1120433.7500\n",
      "Training Epoch: 1 [19380/49669]\tLoss: 1079737.8750\n",
      "Training Epoch: 1 [19400/49669]\tLoss: 1051294.1250\n",
      "Training Epoch: 1 [19420/49669]\tLoss: 969282.6875\n",
      "Training Epoch: 1 [19440/49669]\tLoss: 1073727.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [19460/49669]\tLoss: 1148566.7500\n",
      "Training Epoch: 1 [19480/49669]\tLoss: 1131889.2500\n",
      "Training Epoch: 1 [19500/49669]\tLoss: 918265.2500\n",
      "Training Epoch: 1 [19520/49669]\tLoss: 906408.6250\n",
      "Training Epoch: 1 [19540/49669]\tLoss: 874077.1875\n",
      "Training Epoch: 1 [19560/49669]\tLoss: 1002301.6875\n",
      "Training Epoch: 1 [19580/49669]\tLoss: 1140582.6250\n",
      "Training Epoch: 1 [19600/49669]\tLoss: 1056811.0000\n",
      "Training Epoch: 1 [19620/49669]\tLoss: 1020208.6875\n",
      "Training Epoch: 1 [19640/49669]\tLoss: 1162589.8750\n",
      "Training Epoch: 1 [19660/49669]\tLoss: 963709.5625\n",
      "Training Epoch: 1 [19680/49669]\tLoss: 1108289.2500\n",
      "Training Epoch: 1 [19700/49669]\tLoss: 996264.2500\n",
      "Training Epoch: 1 [19720/49669]\tLoss: 913573.8125\n",
      "Training Epoch: 1 [19740/49669]\tLoss: 906274.9375\n",
      "Training Epoch: 1 [19760/49669]\tLoss: 1120326.1250\n",
      "Training Epoch: 1 [19780/49669]\tLoss: 911000.1250\n",
      "Training Epoch: 1 [19800/49669]\tLoss: 968928.1250\n",
      "Training Epoch: 1 [19820/49669]\tLoss: 998892.5000\n",
      "Training Epoch: 1 [19840/49669]\tLoss: 1087362.5000\n",
      "Training Epoch: 1 [19860/49669]\tLoss: 999433.1875\n",
      "Training Epoch: 1 [19880/49669]\tLoss: 1156592.2500\n",
      "Training Epoch: 1 [19900/49669]\tLoss: 930798.0625\n",
      "Training Epoch: 1 [19920/49669]\tLoss: 1047106.8750\n",
      "Training Epoch: 1 [19940/49669]\tLoss: 1055642.2500\n",
      "Training Epoch: 1 [19960/49669]\tLoss: 1031817.7500\n",
      "Training Epoch: 1 [19980/49669]\tLoss: 1175046.2500\n",
      "Training Epoch: 1 [20000/49669]\tLoss: 906451.7500\n",
      "Training Epoch: 1 [20020/49669]\tLoss: 989327.6250\n",
      "Training Epoch: 1 [20040/49669]\tLoss: 888561.4375\n",
      "Training Epoch: 1 [20060/49669]\tLoss: 1084348.1250\n",
      "Training Epoch: 1 [20080/49669]\tLoss: 899898.3750\n",
      "Training Epoch: 1 [20100/49669]\tLoss: 952944.2500\n",
      "Training Epoch: 1 [20120/49669]\tLoss: 1049869.8750\n",
      "Training Epoch: 1 [20140/49669]\tLoss: 775305.0000\n",
      "Training Epoch: 1 [20160/49669]\tLoss: 949876.2500\n",
      "Training Epoch: 1 [20180/49669]\tLoss: 828962.1250\n",
      "Training Epoch: 1 [20200/49669]\tLoss: 803789.1250\n",
      "Training Epoch: 1 [20220/49669]\tLoss: 930618.7500\n",
      "Training Epoch: 1 [20240/49669]\tLoss: 1065697.6250\n",
      "Training Epoch: 1 [20260/49669]\tLoss: 983693.7500\n",
      "Training Epoch: 1 [20280/49669]\tLoss: 984999.4375\n",
      "Training Epoch: 1 [20300/49669]\tLoss: 966956.0625\n",
      "Training Epoch: 1 [20320/49669]\tLoss: 962524.6875\n",
      "Training Epoch: 1 [20340/49669]\tLoss: 934837.3750\n",
      "Training Epoch: 1 [20360/49669]\tLoss: 928514.3125\n",
      "Training Epoch: 1 [20380/49669]\tLoss: 1038431.8750\n",
      "Training Epoch: 1 [20400/49669]\tLoss: 775988.6875\n",
      "Training Epoch: 1 [20420/49669]\tLoss: 946364.0000\n",
      "Training Epoch: 1 [20440/49669]\tLoss: 885450.0000\n",
      "Training Epoch: 1 [20460/49669]\tLoss: 980401.6250\n",
      "Training Epoch: 1 [20480/49669]\tLoss: 829029.9375\n",
      "Training Epoch: 1 [20500/49669]\tLoss: 1030805.5000\n",
      "Training Epoch: 1 [20520/49669]\tLoss: 999742.0625\n",
      "Training Epoch: 1 [20540/49669]\tLoss: 896660.8125\n",
      "Training Epoch: 1 [20560/49669]\tLoss: 978929.8750\n",
      "Training Epoch: 1 [20580/49669]\tLoss: 1005659.8750\n",
      "Training Epoch: 1 [20600/49669]\tLoss: 935159.7500\n",
      "Training Epoch: 1 [20620/49669]\tLoss: 877627.1250\n",
      "Training Epoch: 1 [20640/49669]\tLoss: 916118.4375\n",
      "Training Epoch: 1 [20660/49669]\tLoss: 783362.2500\n",
      "Training Epoch: 1 [20680/49669]\tLoss: 835637.9375\n",
      "Training Epoch: 1 [20700/49669]\tLoss: 1028242.7500\n",
      "Training Epoch: 1 [20720/49669]\tLoss: 934583.5000\n",
      "Training Epoch: 1 [20740/49669]\tLoss: 950013.0000\n",
      "Training Epoch: 1 [20760/49669]\tLoss: 814827.0000\n",
      "Training Epoch: 1 [20780/49669]\tLoss: 955453.3125\n",
      "Training Epoch: 1 [20800/49669]\tLoss: 827570.8750\n",
      "Training Epoch: 1 [20820/49669]\tLoss: 770210.9375\n",
      "Training Epoch: 1 [20840/49669]\tLoss: 873571.3750\n",
      "Training Epoch: 1 [20860/49669]\tLoss: 888585.3125\n",
      "Training Epoch: 1 [20880/49669]\tLoss: 973745.8750\n",
      "Training Epoch: 1 [20900/49669]\tLoss: 833861.4375\n",
      "Training Epoch: 1 [20920/49669]\tLoss: 865190.4375\n",
      "Training Epoch: 1 [20940/49669]\tLoss: 885760.5625\n",
      "Training Epoch: 1 [20960/49669]\tLoss: 916946.1875\n",
      "Training Epoch: 1 [20980/49669]\tLoss: 1025821.8750\n",
      "Training Epoch: 1 [21000/49669]\tLoss: 1004376.3125\n",
      "Training Epoch: 1 [21020/49669]\tLoss: 883628.2500\n",
      "Training Epoch: 1 [21040/49669]\tLoss: 946220.8125\n",
      "Training Epoch: 1 [21060/49669]\tLoss: 931950.2500\n",
      "Training Epoch: 1 [21080/49669]\tLoss: 901915.0000\n",
      "Training Epoch: 1 [21100/49669]\tLoss: 701382.8125\n",
      "Training Epoch: 1 [21120/49669]\tLoss: 905435.5625\n",
      "Training Epoch: 1 [21140/49669]\tLoss: 900332.4375\n",
      "Training Epoch: 1 [21160/49669]\tLoss: 877636.5000\n",
      "Training Epoch: 1 [21180/49669]\tLoss: 912755.6250\n",
      "Training Epoch: 1 [21200/49669]\tLoss: 877180.1250\n",
      "Training Epoch: 1 [21220/49669]\tLoss: 932055.0625\n",
      "Training Epoch: 1 [21240/49669]\tLoss: 762427.9375\n",
      "Training Epoch: 1 [21260/49669]\tLoss: 706288.4375\n",
      "Training Epoch: 1 [21280/49669]\tLoss: 850467.2500\n",
      "Training Epoch: 1 [21300/49669]\tLoss: 803194.6875\n",
      "Training Epoch: 1 [21320/49669]\tLoss: 978979.6250\n",
      "Training Epoch: 1 [21340/49669]\tLoss: 902873.2500\n",
      "Training Epoch: 1 [21360/49669]\tLoss: 941197.9375\n",
      "Training Epoch: 1 [21380/49669]\tLoss: 880863.7500\n",
      "Training Epoch: 1 [21400/49669]\tLoss: 847259.8750\n",
      "Training Epoch: 1 [21420/49669]\tLoss: 827206.3125\n",
      "Training Epoch: 1 [21440/49669]\tLoss: 872509.3125\n",
      "Training Epoch: 1 [21460/49669]\tLoss: 800624.3750\n",
      "Training Epoch: 1 [21480/49669]\tLoss: 901815.6875\n",
      "Training Epoch: 1 [21500/49669]\tLoss: 821498.4375\n",
      "Training Epoch: 1 [21520/49669]\tLoss: 853968.3125\n",
      "Training Epoch: 1 [21540/49669]\tLoss: 797047.7500\n",
      "Training Epoch: 1 [21560/49669]\tLoss: 839501.5625\n",
      "Training Epoch: 1 [21580/49669]\tLoss: 699278.0625\n",
      "Training Epoch: 1 [21600/49669]\tLoss: 727077.4375\n",
      "Training Epoch: 1 [21620/49669]\tLoss: 900412.8750\n",
      "Training Epoch: 1 [21640/49669]\tLoss: 797976.0625\n",
      "Training Epoch: 1 [21660/49669]\tLoss: 812892.0625\n",
      "Training Epoch: 1 [21680/49669]\tLoss: 810780.2500\n",
      "Training Epoch: 1 [21700/49669]\tLoss: 725659.5000\n",
      "Training Epoch: 1 [21720/49669]\tLoss: 727463.6875\n",
      "Training Epoch: 1 [21740/49669]\tLoss: 970005.6875\n",
      "Training Epoch: 1 [21760/49669]\tLoss: 687759.8750\n",
      "Training Epoch: 1 [21780/49669]\tLoss: 908654.1250\n",
      "Training Epoch: 1 [21800/49669]\tLoss: 904794.5625\n",
      "Training Epoch: 1 [21820/49669]\tLoss: 738047.1250\n",
      "Training Epoch: 1 [21840/49669]\tLoss: 868905.0625\n",
      "Training Epoch: 1 [21860/49669]\tLoss: 840287.5625\n",
      "Training Epoch: 1 [21880/49669]\tLoss: 903743.5000\n",
      "Training Epoch: 1 [21900/49669]\tLoss: 834256.3750\n",
      "Training Epoch: 1 [21920/49669]\tLoss: 776463.6875\n",
      "Training Epoch: 1 [21940/49669]\tLoss: 847679.9375\n",
      "Training Epoch: 1 [21960/49669]\tLoss: 705332.6875\n",
      "Training Epoch: 1 [21980/49669]\tLoss: 804886.2500\n",
      "Training Epoch: 1 [22000/49669]\tLoss: 851263.6875\n",
      "Training Epoch: 1 [22020/49669]\tLoss: 851464.9375\n",
      "Training Epoch: 1 [22040/49669]\tLoss: 699821.6875\n",
      "Training Epoch: 1 [22060/49669]\tLoss: 758733.9375\n",
      "Training Epoch: 1 [22080/49669]\tLoss: 878158.8125\n",
      "Training Epoch: 1 [22100/49669]\tLoss: 792574.0625\n",
      "Training Epoch: 1 [22120/49669]\tLoss: 817045.3750\n",
      "Training Epoch: 1 [22140/49669]\tLoss: 785511.1250\n",
      "Training Epoch: 1 [22160/49669]\tLoss: 653836.5000\n",
      "Training Epoch: 1 [22180/49669]\tLoss: 798352.8125\n",
      "Training Epoch: 1 [22200/49669]\tLoss: 790547.6875\n",
      "Training Epoch: 1 [22220/49669]\tLoss: 705675.0625\n",
      "Training Epoch: 1 [22240/49669]\tLoss: 753862.3750\n",
      "Training Epoch: 1 [22260/49669]\tLoss: 762504.1250\n",
      "Training Epoch: 1 [22280/49669]\tLoss: 697861.3750\n",
      "Training Epoch: 1 [22300/49669]\tLoss: 769209.1250\n",
      "Training Epoch: 1 [22320/49669]\tLoss: 732153.0625\n",
      "Training Epoch: 1 [22340/49669]\tLoss: 725076.5000\n",
      "Training Epoch: 1 [22360/49669]\tLoss: 734647.2500\n",
      "Training Epoch: 1 [22380/49669]\tLoss: 756808.7500\n",
      "Training Epoch: 1 [22400/49669]\tLoss: 813153.0625\n",
      "Training Epoch: 1 [22420/49669]\tLoss: 752228.8750\n",
      "Training Epoch: 1 [22440/49669]\tLoss: 751867.6875\n",
      "Training Epoch: 1 [22460/49669]\tLoss: 630161.0625\n",
      "Training Epoch: 1 [22480/49669]\tLoss: 794744.6875\n",
      "Training Epoch: 1 [22500/49669]\tLoss: 684793.4375\n",
      "Training Epoch: 1 [22520/49669]\tLoss: 626366.8125\n",
      "Training Epoch: 1 [22540/49669]\tLoss: 752477.8125\n",
      "Training Epoch: 1 [22560/49669]\tLoss: 685403.7500\n",
      "Training Epoch: 1 [22580/49669]\tLoss: 777738.4375\n",
      "Training Epoch: 1 [22600/49669]\tLoss: 865781.3125\n",
      "Training Epoch: 1 [22620/49669]\tLoss: 748513.3125\n",
      "Training Epoch: 1 [22640/49669]\tLoss: 725685.2500\n",
      "Training Epoch: 1 [22660/49669]\tLoss: 723390.7500\n",
      "Training Epoch: 1 [22680/49669]\tLoss: 747126.4375\n",
      "Training Epoch: 1 [22700/49669]\tLoss: 835222.0000\n",
      "Training Epoch: 1 [22720/49669]\tLoss: 673072.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [22740/49669]\tLoss: 747229.1875\n",
      "Training Epoch: 1 [22760/49669]\tLoss: 685294.1250\n",
      "Training Epoch: 1 [22780/49669]\tLoss: 729660.2500\n",
      "Training Epoch: 1 [22800/49669]\tLoss: 745283.4375\n",
      "Training Epoch: 1 [22820/49669]\tLoss: 643432.7500\n",
      "Training Epoch: 1 [22840/49669]\tLoss: 636494.8750\n",
      "Training Epoch: 1 [22860/49669]\tLoss: 577422.8750\n",
      "Training Epoch: 1 [22880/49669]\tLoss: 609448.0625\n",
      "Training Epoch: 1 [22900/49669]\tLoss: 741271.7500\n",
      "Training Epoch: 1 [22920/49669]\tLoss: 622881.4375\n",
      "Training Epoch: 1 [22940/49669]\tLoss: 740662.7500\n",
      "Training Epoch: 1 [22960/49669]\tLoss: 761929.6250\n",
      "Training Epoch: 1 [22980/49669]\tLoss: 800089.9375\n",
      "Training Epoch: 1 [23000/49669]\tLoss: 708566.5000\n",
      "Training Epoch: 1 [23020/49669]\tLoss: 670168.5000\n",
      "Training Epoch: 1 [23040/49669]\tLoss: 741921.4375\n",
      "Training Epoch: 1 [23060/49669]\tLoss: 729665.1875\n",
      "Training Epoch: 1 [23080/49669]\tLoss: 610451.7500\n",
      "Training Epoch: 1 [23100/49669]\tLoss: 704167.8125\n",
      "Training Epoch: 1 [23120/49669]\tLoss: 740860.6250\n",
      "Training Epoch: 1 [23140/49669]\tLoss: 717450.7500\n",
      "Training Epoch: 1 [23160/49669]\tLoss: 718003.1250\n",
      "Training Epoch: 1 [23180/49669]\tLoss: 694240.3125\n",
      "Training Epoch: 1 [23200/49669]\tLoss: 691567.2500\n",
      "Training Epoch: 1 [23220/49669]\tLoss: 627183.8125\n",
      "Training Epoch: 1 [23240/49669]\tLoss: 748280.3125\n",
      "Training Epoch: 1 [23260/49669]\tLoss: 763424.1250\n",
      "Training Epoch: 1 [23280/49669]\tLoss: 630119.1250\n",
      "Training Epoch: 1 [23300/49669]\tLoss: 595459.0000\n",
      "Training Epoch: 1 [23320/49669]\tLoss: 664244.8125\n",
      "Training Epoch: 1 [23340/49669]\tLoss: 661031.0625\n",
      "Training Epoch: 1 [23360/49669]\tLoss: 807128.5000\n",
      "Training Epoch: 1 [23380/49669]\tLoss: 652564.5000\n",
      "Training Epoch: 1 [23400/49669]\tLoss: 752575.8125\n",
      "Training Epoch: 1 [23420/49669]\tLoss: 653550.5625\n",
      "Training Epoch: 1 [23440/49669]\tLoss: 652292.0000\n",
      "Training Epoch: 1 [23460/49669]\tLoss: 719505.0625\n",
      "Training Epoch: 1 [23480/49669]\tLoss: 630842.5625\n",
      "Training Epoch: 1 [23500/49669]\tLoss: 571248.6250\n",
      "Training Epoch: 1 [23520/49669]\tLoss: 537184.1875\n",
      "Training Epoch: 1 [23540/49669]\tLoss: 666987.9375\n",
      "Training Epoch: 1 [23560/49669]\tLoss: 660453.8125\n",
      "Training Epoch: 1 [23580/49669]\tLoss: 635030.9375\n",
      "Training Epoch: 1 [23600/49669]\tLoss: 597937.6875\n",
      "Training Epoch: 1 [23620/49669]\tLoss: 749541.5000\n",
      "Training Epoch: 1 [23640/49669]\tLoss: 538638.6875\n",
      "Training Epoch: 1 [23660/49669]\tLoss: 669492.1875\n",
      "Training Epoch: 1 [23680/49669]\tLoss: 606591.0625\n",
      "Training Epoch: 1 [23700/49669]\tLoss: 718174.7500\n",
      "Training Epoch: 1 [23720/49669]\tLoss: 522581.9062\n",
      "Training Epoch: 1 [23740/49669]\tLoss: 615691.1875\n",
      "Training Epoch: 1 [23760/49669]\tLoss: 496135.9375\n",
      "Training Epoch: 1 [23780/49669]\tLoss: 591386.7500\n",
      "Training Epoch: 1 [23800/49669]\tLoss: 647477.6875\n",
      "Training Epoch: 1 [23820/49669]\tLoss: 677583.8750\n",
      "Training Epoch: 1 [23840/49669]\tLoss: 617578.7500\n",
      "Training Epoch: 1 [23860/49669]\tLoss: 576828.0625\n",
      "Training Epoch: 1 [23880/49669]\tLoss: 593724.5625\n",
      "Training Epoch: 1 [23900/49669]\tLoss: 582895.1875\n",
      "Training Epoch: 1 [23920/49669]\tLoss: 735713.1875\n",
      "Training Epoch: 1 [23940/49669]\tLoss: 552961.3125\n",
      "Training Epoch: 1 [23960/49669]\tLoss: 651088.0625\n",
      "Training Epoch: 1 [23980/49669]\tLoss: 605328.6250\n",
      "Training Epoch: 1 [24000/49669]\tLoss: 539715.1875\n",
      "Training Epoch: 1 [24020/49669]\tLoss: 603052.0625\n",
      "Training Epoch: 1 [24040/49669]\tLoss: 669312.3750\n",
      "Training Epoch: 1 [24060/49669]\tLoss: 682109.7500\n",
      "Training Epoch: 1 [24080/49669]\tLoss: 608841.6875\n",
      "Training Epoch: 1 [24100/49669]\tLoss: 630160.5625\n",
      "Training Epoch: 1 [24120/49669]\tLoss: 630626.0000\n",
      "Training Epoch: 1 [24140/49669]\tLoss: 678535.1250\n",
      "Training Epoch: 1 [24160/49669]\tLoss: 727161.5625\n",
      "Training Epoch: 1 [24180/49669]\tLoss: 584476.0625\n",
      "Training Epoch: 1 [24200/49669]\tLoss: 509357.5625\n",
      "Training Epoch: 1 [24220/49669]\tLoss: 594721.5000\n",
      "Training Epoch: 1 [24240/49669]\tLoss: 746917.8750\n",
      "Training Epoch: 1 [24260/49669]\tLoss: 576055.7500\n",
      "Training Epoch: 1 [24280/49669]\tLoss: 597344.5625\n",
      "Training Epoch: 1 [24300/49669]\tLoss: 484219.0625\n",
      "Training Epoch: 1 [24320/49669]\tLoss: 673133.8125\n",
      "Training Epoch: 1 [24340/49669]\tLoss: 670916.8125\n",
      "Training Epoch: 1 [24360/49669]\tLoss: 635146.2500\n",
      "Training Epoch: 1 [24380/49669]\tLoss: 566349.2500\n",
      "Training Epoch: 1 [24400/49669]\tLoss: 581353.8125\n",
      "Training Epoch: 1 [24420/49669]\tLoss: 578072.7500\n",
      "Training Epoch: 1 [24440/49669]\tLoss: 581969.3125\n",
      "Training Epoch: 1 [24460/49669]\tLoss: 593393.1250\n",
      "Training Epoch: 1 [24480/49669]\tLoss: 575257.1250\n",
      "Training Epoch: 1 [24500/49669]\tLoss: 597702.9375\n",
      "Training Epoch: 1 [24520/49669]\tLoss: 629293.9375\n",
      "Training Epoch: 1 [24540/49669]\tLoss: 613156.6250\n",
      "Training Epoch: 1 [24560/49669]\tLoss: 599112.4375\n",
      "Training Epoch: 1 [24580/49669]\tLoss: 600498.0000\n",
      "Training Epoch: 1 [24600/49669]\tLoss: 467283.9688\n",
      "Training Epoch: 1 [24620/49669]\tLoss: 640848.7500\n",
      "Training Epoch: 1 [24640/49669]\tLoss: 546362.5625\n",
      "Training Epoch: 1 [24660/49669]\tLoss: 644776.2500\n",
      "Training Epoch: 1 [24680/49669]\tLoss: 453068.2500\n",
      "Training Epoch: 1 [24700/49669]\tLoss: 538454.2500\n",
      "Training Epoch: 1 [24720/49669]\tLoss: 563796.9375\n",
      "Training Epoch: 1 [24740/49669]\tLoss: 579049.0625\n",
      "Training Epoch: 1 [24760/49669]\tLoss: 530624.4375\n",
      "Training Epoch: 1 [24780/49669]\tLoss: 542277.8750\n",
      "Training Epoch: 1 [24800/49669]\tLoss: 603244.2500\n",
      "Training Epoch: 1 [24820/49669]\tLoss: 526748.0000\n",
      "Training Epoch: 1 [24840/49669]\tLoss: 562362.7500\n",
      "Training Epoch: 1 [24860/49669]\tLoss: 576156.8125\n",
      "Training Epoch: 1 [24880/49669]\tLoss: 571207.4375\n",
      "Training Epoch: 1 [24900/49669]\tLoss: 590208.5625\n",
      "Training Epoch: 1 [24920/49669]\tLoss: 414777.0312\n",
      "Training Epoch: 1 [24940/49669]\tLoss: 571910.9375\n",
      "Training Epoch: 1 [24960/49669]\tLoss: 553207.4375\n",
      "Training Epoch: 1 [24980/49669]\tLoss: 540053.8750\n",
      "Training Epoch: 1 [25000/49669]\tLoss: 493568.9375\n",
      "Training Epoch: 1 [25020/49669]\tLoss: 552888.3750\n",
      "Training Epoch: 1 [25040/49669]\tLoss: 574933.7500\n",
      "Training Epoch: 1 [25060/49669]\tLoss: 566003.3125\n",
      "Training Epoch: 1 [25080/49669]\tLoss: 617158.8125\n",
      "Training Epoch: 1 [25100/49669]\tLoss: 519268.4375\n",
      "Training Epoch: 1 [25120/49669]\tLoss: 517757.2188\n",
      "Training Epoch: 1 [25140/49669]\tLoss: 600036.3750\n",
      "Training Epoch: 1 [25160/49669]\tLoss: 471346.0938\n",
      "Training Epoch: 1 [25180/49669]\tLoss: 630338.1250\n",
      "Training Epoch: 1 [25200/49669]\tLoss: 473489.0312\n",
      "Training Epoch: 1 [25220/49669]\tLoss: 548387.7500\n",
      "Training Epoch: 1 [25240/49669]\tLoss: 596825.3750\n",
      "Training Epoch: 1 [25260/49669]\tLoss: 601727.0625\n",
      "Training Epoch: 1 [25280/49669]\tLoss: 494415.7500\n",
      "Training Epoch: 1 [25300/49669]\tLoss: 616554.1875\n",
      "Training Epoch: 1 [25320/49669]\tLoss: 445211.4688\n",
      "Training Epoch: 1 [25340/49669]\tLoss: 597195.8750\n",
      "Training Epoch: 1 [25360/49669]\tLoss: 600945.5625\n",
      "Training Epoch: 1 [25380/49669]\tLoss: 588893.0625\n",
      "Training Epoch: 1 [25400/49669]\tLoss: 552904.4375\n",
      "Training Epoch: 1 [25420/49669]\tLoss: 544207.5625\n",
      "Training Epoch: 1 [25440/49669]\tLoss: 475742.5000\n",
      "Training Epoch: 1 [25460/49669]\tLoss: 499374.1562\n",
      "Training Epoch: 1 [25480/49669]\tLoss: 559102.8125\n",
      "Training Epoch: 1 [25500/49669]\tLoss: 523445.2188\n",
      "Training Epoch: 1 [25520/49669]\tLoss: 546242.1875\n",
      "Training Epoch: 1 [25540/49669]\tLoss: 514527.6562\n",
      "Training Epoch: 1 [25560/49669]\tLoss: 491208.7500\n",
      "Training Epoch: 1 [25580/49669]\tLoss: 481072.2500\n",
      "Training Epoch: 1 [25600/49669]\tLoss: 507316.5938\n",
      "Training Epoch: 1 [25620/49669]\tLoss: 438731.9375\n",
      "Training Epoch: 1 [25640/49669]\tLoss: 494159.1562\n",
      "Training Epoch: 1 [25660/49669]\tLoss: 595204.5000\n",
      "Training Epoch: 1 [25680/49669]\tLoss: 557475.0000\n",
      "Training Epoch: 1 [25700/49669]\tLoss: 504273.3438\n",
      "Training Epoch: 1 [25720/49669]\tLoss: 526986.5000\n",
      "Training Epoch: 1 [25740/49669]\tLoss: 403333.3125\n",
      "Training Epoch: 1 [25760/49669]\tLoss: 487824.6562\n",
      "Training Epoch: 1 [25780/49669]\tLoss: 527307.2500\n",
      "Training Epoch: 1 [25800/49669]\tLoss: 508958.9688\n",
      "Training Epoch: 1 [25820/49669]\tLoss: 563055.7500\n",
      "Training Epoch: 1 [25840/49669]\tLoss: 522329.1875\n",
      "Training Epoch: 1 [25860/49669]\tLoss: 535301.2500\n",
      "Training Epoch: 1 [25880/49669]\tLoss: 494525.6562\n",
      "Training Epoch: 1 [25900/49669]\tLoss: 528147.0000\n",
      "Training Epoch: 1 [25920/49669]\tLoss: 500600.7188\n",
      "Training Epoch: 1 [25940/49669]\tLoss: 541222.5625\n",
      "Training Epoch: 1 [25960/49669]\tLoss: 477008.1562\n",
      "Training Epoch: 1 [25980/49669]\tLoss: 455497.7500\n",
      "Training Epoch: 1 [26000/49669]\tLoss: 535738.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [26020/49669]\tLoss: 535094.5000\n",
      "Training Epoch: 1 [26040/49669]\tLoss: 535907.0625\n",
      "Training Epoch: 1 [26060/49669]\tLoss: 444510.4062\n",
      "Training Epoch: 1 [26080/49669]\tLoss: 536850.6250\n",
      "Training Epoch: 1 [26100/49669]\tLoss: 496349.0625\n",
      "Training Epoch: 1 [26120/49669]\tLoss: 507677.5938\n",
      "Training Epoch: 1 [26140/49669]\tLoss: 495346.9375\n",
      "Training Epoch: 1 [26160/49669]\tLoss: 518687.7500\n",
      "Training Epoch: 1 [26180/49669]\tLoss: 518968.0312\n",
      "Training Epoch: 1 [26200/49669]\tLoss: 490461.4062\n",
      "Training Epoch: 1 [26220/49669]\tLoss: 439523.4062\n",
      "Training Epoch: 1 [26240/49669]\tLoss: 508294.0938\n",
      "Training Epoch: 1 [26260/49669]\tLoss: 486870.7812\n",
      "Training Epoch: 1 [26280/49669]\tLoss: 547884.4375\n",
      "Training Epoch: 1 [26300/49669]\tLoss: 417762.3125\n",
      "Training Epoch: 1 [26320/49669]\tLoss: 531483.0000\n",
      "Training Epoch: 1 [26340/49669]\tLoss: 535825.6875\n",
      "Training Epoch: 1 [26360/49669]\tLoss: 553976.2500\n",
      "Training Epoch: 1 [26380/49669]\tLoss: 528221.1250\n",
      "Training Epoch: 1 [26400/49669]\tLoss: 513829.8125\n",
      "Training Epoch: 1 [26420/49669]\tLoss: 438920.3438\n",
      "Training Epoch: 1 [26440/49669]\tLoss: 487472.8438\n",
      "Training Epoch: 1 [26460/49669]\tLoss: 542573.8750\n",
      "Training Epoch: 1 [26480/49669]\tLoss: 502685.7812\n",
      "Training Epoch: 1 [26500/49669]\tLoss: 489490.0625\n",
      "Training Epoch: 1 [26520/49669]\tLoss: 443596.5938\n",
      "Training Epoch: 1 [26540/49669]\tLoss: 446469.4375\n",
      "Training Epoch: 1 [26560/49669]\tLoss: 447444.6250\n",
      "Training Epoch: 1 [26580/49669]\tLoss: 526570.5000\n",
      "Training Epoch: 1 [26600/49669]\tLoss: 478811.0000\n",
      "Training Epoch: 1 [26620/49669]\tLoss: 527372.8125\n",
      "Training Epoch: 1 [26640/49669]\tLoss: 476748.7188\n",
      "Training Epoch: 1 [26660/49669]\tLoss: 463025.7500\n",
      "Training Epoch: 1 [26680/49669]\tLoss: 451814.2188\n",
      "Training Epoch: 1 [26700/49669]\tLoss: 512550.8438\n",
      "Training Epoch: 1 [26720/49669]\tLoss: 478375.7188\n",
      "Training Epoch: 1 [26740/49669]\tLoss: 475533.5938\n",
      "Training Epoch: 1 [26760/49669]\tLoss: 476520.4688\n",
      "Training Epoch: 1 [26780/49669]\tLoss: 446260.5312\n",
      "Training Epoch: 1 [26800/49669]\tLoss: 460456.7188\n",
      "Training Epoch: 1 [26820/49669]\tLoss: 462485.0312\n",
      "Training Epoch: 1 [26840/49669]\tLoss: 453400.2188\n",
      "Training Epoch: 1 [26860/49669]\tLoss: 498464.3125\n",
      "Training Epoch: 1 [26880/49669]\tLoss: 538408.8125\n",
      "Training Epoch: 1 [26900/49669]\tLoss: 470330.4688\n",
      "Training Epoch: 1 [26920/49669]\tLoss: 508370.6875\n",
      "Training Epoch: 1 [26940/49669]\tLoss: 493432.8125\n",
      "Training Epoch: 1 [26960/49669]\tLoss: 490042.6875\n",
      "Training Epoch: 1 [26980/49669]\tLoss: 408440.7188\n",
      "Training Epoch: 1 [27000/49669]\tLoss: 455033.7188\n",
      "Training Epoch: 1 [27020/49669]\tLoss: 448403.2812\n",
      "Training Epoch: 1 [27040/49669]\tLoss: 467901.4062\n",
      "Training Epoch: 1 [27060/49669]\tLoss: 494795.0938\n",
      "Training Epoch: 1 [27080/49669]\tLoss: 413968.8750\n",
      "Training Epoch: 1 [27100/49669]\tLoss: 456119.4688\n",
      "Training Epoch: 1 [27120/49669]\tLoss: 431499.0000\n",
      "Training Epoch: 1 [27140/49669]\tLoss: 455787.0938\n",
      "Training Epoch: 1 [27160/49669]\tLoss: 408060.0625\n",
      "Training Epoch: 1 [27180/49669]\tLoss: 497285.2188\n",
      "Training Epoch: 1 [27200/49669]\tLoss: 430663.7188\n",
      "Training Epoch: 1 [27220/49669]\tLoss: 441470.3750\n",
      "Training Epoch: 1 [27240/49669]\tLoss: 403959.7812\n",
      "Training Epoch: 1 [27260/49669]\tLoss: 461345.5000\n",
      "Training Epoch: 1 [27280/49669]\tLoss: 451739.5938\n",
      "Training Epoch: 1 [27300/49669]\tLoss: 505869.7188\n",
      "Training Epoch: 1 [27320/49669]\tLoss: 463351.5938\n",
      "Training Epoch: 1 [27340/49669]\tLoss: 455264.5938\n",
      "Training Epoch: 1 [27360/49669]\tLoss: 389559.0000\n",
      "Training Epoch: 1 [27380/49669]\tLoss: 478414.7812\n",
      "Training Epoch: 1 [27400/49669]\tLoss: 479389.5938\n",
      "Training Epoch: 1 [27420/49669]\tLoss: 308304.2500\n",
      "Training Epoch: 1 [27440/49669]\tLoss: 427469.2500\n",
      "Training Epoch: 1 [27460/49669]\tLoss: 444788.8750\n",
      "Training Epoch: 1 [27480/49669]\tLoss: 448830.2812\n",
      "Training Epoch: 1 [27500/49669]\tLoss: 426888.3438\n",
      "Training Epoch: 1 [27520/49669]\tLoss: 445005.7188\n",
      "Training Epoch: 1 [27540/49669]\tLoss: 396382.5312\n",
      "Training Epoch: 1 [27560/49669]\tLoss: 453814.0625\n",
      "Training Epoch: 1 [27580/49669]\tLoss: 448825.3438\n",
      "Training Epoch: 1 [27600/49669]\tLoss: 450020.6250\n",
      "Training Epoch: 1 [27620/49669]\tLoss: 382861.5938\n",
      "Training Epoch: 1 [27640/49669]\tLoss: 413218.5625\n",
      "Training Epoch: 1 [27660/49669]\tLoss: 431235.0000\n",
      "Training Epoch: 1 [27680/49669]\tLoss: 452360.2188\n",
      "Training Epoch: 1 [27700/49669]\tLoss: 396311.1875\n",
      "Training Epoch: 1 [27720/49669]\tLoss: 461596.0938\n",
      "Training Epoch: 1 [27740/49669]\tLoss: 361909.0000\n",
      "Training Epoch: 1 [27760/49669]\tLoss: 418590.2188\n",
      "Training Epoch: 1 [27780/49669]\tLoss: 466304.1562\n",
      "Training Epoch: 1 [27800/49669]\tLoss: 367060.4062\n",
      "Training Epoch: 1 [27820/49669]\tLoss: 338469.0000\n",
      "Training Epoch: 1 [27840/49669]\tLoss: 410700.2812\n",
      "Training Epoch: 1 [27860/49669]\tLoss: 441780.5000\n",
      "Training Epoch: 1 [27880/49669]\tLoss: 373879.0625\n",
      "Training Epoch: 1 [27900/49669]\tLoss: 454231.6875\n",
      "Training Epoch: 1 [27920/49669]\tLoss: 430879.4062\n",
      "Training Epoch: 1 [27940/49669]\tLoss: 470442.7812\n",
      "Training Epoch: 1 [27960/49669]\tLoss: 338841.0938\n",
      "Training Epoch: 1 [27980/49669]\tLoss: 443752.7812\n",
      "Training Epoch: 1 [28000/49669]\tLoss: 439678.4688\n",
      "Training Epoch: 1 [28020/49669]\tLoss: 390004.7188\n",
      "Training Epoch: 1 [28040/49669]\tLoss: 374195.8750\n",
      "Training Epoch: 1 [28060/49669]\tLoss: 421609.9688\n",
      "Training Epoch: 1 [28080/49669]\tLoss: 382902.5000\n",
      "Training Epoch: 1 [28100/49669]\tLoss: 465177.7500\n",
      "Training Epoch: 1 [28120/49669]\tLoss: 457968.9062\n",
      "Training Epoch: 1 [28140/49669]\tLoss: 420400.5312\n",
      "Training Epoch: 1 [28160/49669]\tLoss: 377784.5938\n",
      "Training Epoch: 1 [28180/49669]\tLoss: 375806.5625\n",
      "Training Epoch: 1 [28200/49669]\tLoss: 431924.1250\n",
      "Training Epoch: 1 [28220/49669]\tLoss: 380655.4062\n",
      "Training Epoch: 1 [28240/49669]\tLoss: 423748.4688\n",
      "Training Epoch: 1 [28260/49669]\tLoss: 417632.0625\n",
      "Training Epoch: 1 [28280/49669]\tLoss: 355315.7812\n",
      "Training Epoch: 1 [28300/49669]\tLoss: 407547.1875\n",
      "Training Epoch: 1 [28320/49669]\tLoss: 401401.0312\n",
      "Training Epoch: 1 [28340/49669]\tLoss: 422143.5000\n",
      "Training Epoch: 1 [28360/49669]\tLoss: 372725.8125\n",
      "Training Epoch: 1 [28380/49669]\tLoss: 443965.8125\n",
      "Training Epoch: 1 [28400/49669]\tLoss: 382172.2500\n",
      "Training Epoch: 1 [28420/49669]\tLoss: 354719.5938\n",
      "Training Epoch: 1 [28440/49669]\tLoss: 344525.0000\n",
      "Training Epoch: 1 [28460/49669]\tLoss: 429645.2500\n",
      "Training Epoch: 1 [28480/49669]\tLoss: 398514.1562\n",
      "Training Epoch: 1 [28500/49669]\tLoss: 376432.8438\n",
      "Training Epoch: 1 [28520/49669]\tLoss: 371752.9375\n",
      "Training Epoch: 1 [28540/49669]\tLoss: 374640.0312\n",
      "Training Epoch: 1 [28560/49669]\tLoss: 423135.4062\n",
      "Training Epoch: 1 [28580/49669]\tLoss: 402360.9375\n",
      "Training Epoch: 1 [28600/49669]\tLoss: 431574.0312\n",
      "Training Epoch: 1 [28620/49669]\tLoss: 397249.8438\n",
      "Training Epoch: 1 [28640/49669]\tLoss: 443421.1562\n",
      "Training Epoch: 1 [28660/49669]\tLoss: 373020.0938\n",
      "Training Epoch: 1 [28680/49669]\tLoss: 375982.5312\n",
      "Training Epoch: 1 [28700/49669]\tLoss: 432539.7812\n",
      "Training Epoch: 1 [28720/49669]\tLoss: 391307.5938\n",
      "Training Epoch: 1 [28740/49669]\tLoss: 376561.5938\n",
      "Training Epoch: 1 [28760/49669]\tLoss: 362399.0938\n",
      "Training Epoch: 1 [28780/49669]\tLoss: 434391.2812\n",
      "Training Epoch: 1 [28800/49669]\tLoss: 354976.3125\n",
      "Training Epoch: 1 [28820/49669]\tLoss: 433197.5312\n",
      "Training Epoch: 1 [28840/49669]\tLoss: 366696.4375\n",
      "Training Epoch: 1 [28860/49669]\tLoss: 325251.7500\n",
      "Training Epoch: 1 [28880/49669]\tLoss: 401015.1250\n",
      "Training Epoch: 1 [28900/49669]\tLoss: 427246.8438\n",
      "Training Epoch: 1 [28920/49669]\tLoss: 415328.3438\n",
      "Training Epoch: 1 [28940/49669]\tLoss: 372620.8438\n",
      "Training Epoch: 1 [28960/49669]\tLoss: 368016.5000\n",
      "Training Epoch: 1 [28980/49669]\tLoss: 377376.8438\n",
      "Training Epoch: 1 [29000/49669]\tLoss: 345195.7188\n",
      "Training Epoch: 1 [29020/49669]\tLoss: 336311.5938\n",
      "Training Epoch: 1 [29040/49669]\tLoss: 406375.9375\n",
      "Training Epoch: 1 [29060/49669]\tLoss: 404302.3750\n",
      "Training Epoch: 1 [29080/49669]\tLoss: 415073.7188\n",
      "Training Epoch: 1 [29100/49669]\tLoss: 345014.2500\n",
      "Training Epoch: 1 [29120/49669]\tLoss: 327043.4062\n",
      "Training Epoch: 1 [29140/49669]\tLoss: 354956.8125\n",
      "Training Epoch: 1 [29160/49669]\tLoss: 361919.6875\n",
      "Training Epoch: 1 [29180/49669]\tLoss: 367495.7500\n",
      "Training Epoch: 1 [29200/49669]\tLoss: 379432.7812\n",
      "Training Epoch: 1 [29220/49669]\tLoss: 286852.6250\n",
      "Training Epoch: 1 [29240/49669]\tLoss: 353454.9062\n",
      "Training Epoch: 1 [29260/49669]\tLoss: 345372.1250\n",
      "Training Epoch: 1 [29280/49669]\tLoss: 371191.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [29300/49669]\tLoss: 364660.7812\n",
      "Training Epoch: 1 [29320/49669]\tLoss: 338352.3750\n",
      "Training Epoch: 1 [29340/49669]\tLoss: 346362.4375\n",
      "Training Epoch: 1 [29360/49669]\tLoss: 327257.0625\n",
      "Training Epoch: 1 [29380/49669]\tLoss: 351138.3750\n",
      "Training Epoch: 1 [29400/49669]\tLoss: 363892.7812\n",
      "Training Epoch: 1 [29420/49669]\tLoss: 401683.7812\n",
      "Training Epoch: 1 [29440/49669]\tLoss: 348956.7188\n",
      "Training Epoch: 1 [29460/49669]\tLoss: 345630.5312\n",
      "Training Epoch: 1 [29480/49669]\tLoss: 390043.5312\n",
      "Training Epoch: 1 [29500/49669]\tLoss: 372558.3125\n",
      "Training Epoch: 1 [29520/49669]\tLoss: 380576.7500\n",
      "Training Epoch: 1 [29540/49669]\tLoss: 337887.3750\n",
      "Training Epoch: 1 [29560/49669]\tLoss: 330496.4688\n",
      "Training Epoch: 1 [29580/49669]\tLoss: 302523.9062\n",
      "Training Epoch: 1 [29600/49669]\tLoss: 318791.6562\n",
      "Training Epoch: 1 [29620/49669]\tLoss: 339390.3438\n",
      "Training Epoch: 1 [29640/49669]\tLoss: 343417.4375\n",
      "Training Epoch: 1 [29660/49669]\tLoss: 340966.6562\n",
      "Training Epoch: 1 [29680/49669]\tLoss: 372660.6250\n",
      "Training Epoch: 1 [29700/49669]\tLoss: 391823.8750\n",
      "Training Epoch: 1 [29720/49669]\tLoss: 308915.9688\n",
      "Training Epoch: 1 [29740/49669]\tLoss: 336438.9375\n",
      "Training Epoch: 1 [29760/49669]\tLoss: 363905.3750\n",
      "Training Epoch: 1 [29780/49669]\tLoss: 327126.2812\n",
      "Training Epoch: 1 [29800/49669]\tLoss: 366637.8438\n",
      "Training Epoch: 1 [29820/49669]\tLoss: 279356.7812\n",
      "Training Epoch: 1 [29840/49669]\tLoss: 310884.1562\n",
      "Training Epoch: 1 [29860/49669]\tLoss: 335151.7500\n",
      "Training Epoch: 1 [29880/49669]\tLoss: 404838.2812\n",
      "Training Epoch: 1 [29900/49669]\tLoss: 339999.3438\n",
      "Training Epoch: 1 [29920/49669]\tLoss: 394186.8750\n",
      "Training Epoch: 1 [29940/49669]\tLoss: 362331.3750\n",
      "Training Epoch: 1 [29960/49669]\tLoss: 345916.4062\n",
      "Training Epoch: 1 [29980/49669]\tLoss: 330089.8125\n",
      "Training Epoch: 1 [30000/49669]\tLoss: 338405.9375\n",
      "Training Epoch: 1 [30020/49669]\tLoss: 352357.9688\n",
      "Training Epoch: 1 [30040/49669]\tLoss: 363500.8438\n",
      "Training Epoch: 1 [30060/49669]\tLoss: 385832.9375\n",
      "Training Epoch: 1 [30080/49669]\tLoss: 335241.2500\n",
      "Training Epoch: 1 [30100/49669]\tLoss: 337169.9375\n",
      "Training Epoch: 1 [30120/49669]\tLoss: 345961.8750\n",
      "Training Epoch: 1 [30140/49669]\tLoss: 350019.1875\n",
      "Training Epoch: 1 [30160/49669]\tLoss: 382221.9062\n",
      "Training Epoch: 1 [30180/49669]\tLoss: 286610.5312\n",
      "Training Epoch: 1 [30200/49669]\tLoss: 338613.0625\n",
      "Training Epoch: 1 [30220/49669]\tLoss: 319252.6562\n",
      "Training Epoch: 1 [30240/49669]\tLoss: 307840.0312\n",
      "Training Epoch: 1 [30260/49669]\tLoss: 321524.5625\n",
      "Training Epoch: 1 [30280/49669]\tLoss: 344682.2812\n",
      "Training Epoch: 1 [30300/49669]\tLoss: 308557.7500\n",
      "Training Epoch: 1 [30320/49669]\tLoss: 297605.0312\n",
      "Training Epoch: 1 [30340/49669]\tLoss: 332136.5625\n",
      "Training Epoch: 1 [30360/49669]\tLoss: 332262.0625\n",
      "Training Epoch: 1 [30380/49669]\tLoss: 297622.7812\n",
      "Training Epoch: 1 [30400/49669]\tLoss: 313629.6562\n",
      "Training Epoch: 1 [30420/49669]\tLoss: 316353.4062\n",
      "Training Epoch: 1 [30440/49669]\tLoss: 378810.9688\n",
      "Training Epoch: 1 [30460/49669]\tLoss: 335884.5625\n",
      "Training Epoch: 1 [30480/49669]\tLoss: 334377.6250\n",
      "Training Epoch: 1 [30500/49669]\tLoss: 276162.8438\n",
      "Training Epoch: 1 [30520/49669]\tLoss: 340443.5000\n",
      "Training Epoch: 1 [30540/49669]\tLoss: 322028.9062\n",
      "Training Epoch: 1 [30560/49669]\tLoss: 315142.2500\n",
      "Training Epoch: 1 [30580/49669]\tLoss: 359491.8125\n",
      "Training Epoch: 1 [30600/49669]\tLoss: 264101.0938\n",
      "Training Epoch: 1 [30620/49669]\tLoss: 307147.0000\n",
      "Training Epoch: 1 [30640/49669]\tLoss: 252335.6250\n",
      "Training Epoch: 1 [30660/49669]\tLoss: 326172.3438\n",
      "Training Epoch: 1 [30680/49669]\tLoss: 287739.6875\n",
      "Training Epoch: 1 [30700/49669]\tLoss: 321557.6562\n",
      "Training Epoch: 1 [30720/49669]\tLoss: 276903.3750\n",
      "Training Epoch: 1 [30740/49669]\tLoss: 360453.4688\n",
      "Training Epoch: 1 [30760/49669]\tLoss: 332099.9375\n",
      "Training Epoch: 1 [30780/49669]\tLoss: 281041.1250\n",
      "Training Epoch: 1 [30800/49669]\tLoss: 339235.4688\n",
      "Training Epoch: 1 [30820/49669]\tLoss: 294098.5625\n",
      "Training Epoch: 1 [30840/49669]\tLoss: 296195.6875\n",
      "Training Epoch: 1 [30860/49669]\tLoss: 357023.8125\n",
      "Training Epoch: 1 [30880/49669]\tLoss: 359202.6250\n",
      "Training Epoch: 1 [30900/49669]\tLoss: 331577.8750\n",
      "Training Epoch: 1 [30920/49669]\tLoss: 322733.4062\n",
      "Training Epoch: 1 [30940/49669]\tLoss: 311641.5938\n",
      "Training Epoch: 1 [30960/49669]\tLoss: 366309.9688\n",
      "Training Epoch: 1 [30980/49669]\tLoss: 316499.4062\n",
      "Training Epoch: 1 [31000/49669]\tLoss: 364672.1562\n",
      "Training Epoch: 1 [31020/49669]\tLoss: 349735.5625\n",
      "Training Epoch: 1 [31040/49669]\tLoss: 365762.1875\n",
      "Training Epoch: 1 [31060/49669]\tLoss: 274818.7500\n",
      "Training Epoch: 1 [31080/49669]\tLoss: 329674.9062\n",
      "Training Epoch: 1 [31100/49669]\tLoss: 305310.0000\n",
      "Training Epoch: 1 [31120/49669]\tLoss: 319484.0000\n",
      "Training Epoch: 1 [31140/49669]\tLoss: 323777.0625\n",
      "Training Epoch: 1 [31160/49669]\tLoss: 321473.3438\n",
      "Training Epoch: 1 [31180/49669]\tLoss: 276869.6250\n",
      "Training Epoch: 1 [31200/49669]\tLoss: 279507.3750\n",
      "Training Epoch: 1 [31220/49669]\tLoss: 243186.9531\n",
      "Training Epoch: 1 [31240/49669]\tLoss: 270529.8750\n",
      "Training Epoch: 1 [31260/49669]\tLoss: 318705.3125\n",
      "Training Epoch: 1 [31280/49669]\tLoss: 310051.3125\n",
      "Training Epoch: 1 [31300/49669]\tLoss: 322423.8438\n",
      "Training Epoch: 1 [31320/49669]\tLoss: 300675.7188\n",
      "Training Epoch: 1 [31340/49669]\tLoss: 328625.2188\n",
      "Training Epoch: 1 [31360/49669]\tLoss: 277802.8125\n",
      "Training Epoch: 1 [31380/49669]\tLoss: 348276.9375\n",
      "Training Epoch: 1 [31400/49669]\tLoss: 320422.6875\n",
      "Training Epoch: 1 [31420/49669]\tLoss: 315235.5938\n",
      "Training Epoch: 1 [31440/49669]\tLoss: 308997.6250\n",
      "Training Epoch: 1 [31460/49669]\tLoss: 282710.4688\n",
      "Training Epoch: 1 [31480/49669]\tLoss: 322142.2812\n",
      "Training Epoch: 1 [31500/49669]\tLoss: 315933.8125\n",
      "Training Epoch: 1 [31520/49669]\tLoss: 277334.7188\n",
      "Training Epoch: 1 [31540/49669]\tLoss: 308104.1875\n",
      "Training Epoch: 1 [31560/49669]\tLoss: 311820.8750\n",
      "Training Epoch: 1 [31580/49669]\tLoss: 315947.9375\n",
      "Training Epoch: 1 [31600/49669]\tLoss: 297678.1875\n",
      "Training Epoch: 1 [31620/49669]\tLoss: 287477.1250\n",
      "Training Epoch: 1 [31640/49669]\tLoss: 268995.5625\n",
      "Training Epoch: 1 [31660/49669]\tLoss: 293115.6562\n",
      "Training Epoch: 1 [31680/49669]\tLoss: 297837.8750\n",
      "Training Epoch: 1 [31700/49669]\tLoss: 262466.0625\n",
      "Training Epoch: 1 [31720/49669]\tLoss: 254735.0312\n",
      "Training Epoch: 1 [31740/49669]\tLoss: 260020.1094\n",
      "Training Epoch: 1 [31760/49669]\tLoss: 330185.1562\n",
      "Training Epoch: 1 [31780/49669]\tLoss: 323586.1562\n",
      "Training Epoch: 1 [31800/49669]\tLoss: 254731.8281\n",
      "Training Epoch: 1 [31820/49669]\tLoss: 300354.5000\n",
      "Training Epoch: 1 [31840/49669]\tLoss: 314960.9688\n",
      "Training Epoch: 1 [31860/49669]\tLoss: 282912.7500\n",
      "Training Epoch: 1 [31880/49669]\tLoss: 301235.9688\n",
      "Training Epoch: 1 [31900/49669]\tLoss: 329509.9688\n",
      "Training Epoch: 1 [31920/49669]\tLoss: 323812.4062\n",
      "Training Epoch: 1 [31940/49669]\tLoss: 317221.9688\n",
      "Training Epoch: 1 [31960/49669]\tLoss: 276218.6875\n",
      "Training Epoch: 1 [31980/49669]\tLoss: 283262.0625\n",
      "Training Epoch: 1 [32000/49669]\tLoss: 309792.6562\n",
      "Training Epoch: 1 [32020/49669]\tLoss: 282030.6562\n",
      "Training Epoch: 1 [32040/49669]\tLoss: 313774.0312\n",
      "Training Epoch: 1 [32060/49669]\tLoss: 310207.0938\n",
      "Training Epoch: 1 [32080/49669]\tLoss: 289870.9688\n",
      "Training Epoch: 1 [32100/49669]\tLoss: 296698.0625\n",
      "Training Epoch: 1 [32120/49669]\tLoss: 271893.5312\n",
      "Training Epoch: 1 [32900/49669]\tLoss: 253543.8906\n",
      "Training Epoch: 1 [32920/49669]\tLoss: 262633.5000\n",
      "Training Epoch: 1 [32940/49669]\tLoss: 282165.0312\n",
      "Training Epoch: 1 [32960/49669]\tLoss: 274952.7812\n",
      "Training Epoch: 1 [32980/49669]\tLoss: 288033.3750\n",
      "Training Epoch: 1 [33000/49669]\tLoss: 241568.6406\n",
      "Training Epoch: 1 [33020/49669]\tLoss: 294525.5625\n",
      "Training Epoch: 1 [33040/49669]\tLoss: 281906.0625\n",
      "Training Epoch: 1 [33060/49669]\tLoss: 207631.2344\n",
      "Training Epoch: 1 [33080/49669]\tLoss: 273704.0000\n",
      "Training Epoch: 1 [33100/49669]\tLoss: 232950.4844\n",
      "Training Epoch: 1 [33120/49669]\tLoss: 298474.3125\n",
      "Training Epoch: 1 [33140/49669]\tLoss: 273182.6562\n",
      "Training Epoch: 1 [33160/49669]\tLoss: 274475.6562\n",
      "Training Epoch: 1 [33180/49669]\tLoss: 281911.6250\n",
      "Training Epoch: 1 [33200/49669]\tLoss: 257726.1094\n",
      "Training Epoch: 1 [33220/49669]\tLoss: 243965.0469\n",
      "Training Epoch: 1 [33240/49669]\tLoss: 207766.5469\n",
      "Training Epoch: 1 [33260/49669]\tLoss: 234816.3750\n",
      "Training Epoch: 1 [33280/49669]\tLoss: 273654.8438\n",
      "Training Epoch: 1 [33300/49669]\tLoss: 244789.9844\n",
      "Training Epoch: 1 [33320/49669]\tLoss: 284321.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [33340/49669]\tLoss: 258039.0000\n",
      "Training Epoch: 1 [33360/49669]\tLoss: 269025.5625\n",
      "Training Epoch: 1 [33380/49669]\tLoss: 193335.9219\n",
      "Training Epoch: 1 [33400/49669]\tLoss: 225662.8438\n",
      "Training Epoch: 1 [33420/49669]\tLoss: 258010.2812\n",
      "Training Epoch: 1 [33440/49669]\tLoss: 299756.2500\n",
      "Training Epoch: 1 [33460/49669]\tLoss: 255834.9062\n",
      "Training Epoch: 1 [33480/49669]\tLoss: 272206.2500\n",
      "Training Epoch: 1 [33500/49669]\tLoss: 223649.7656\n",
      "Training Epoch: 1 [33520/49669]\tLoss: 244973.3125\n",
      "Training Epoch: 1 [33540/49669]\tLoss: 245393.9531\n",
      "Training Epoch: 1 [33560/49669]\tLoss: 271160.3125\n",
      "Training Epoch: 1 [33580/49669]\tLoss: 213301.3438\n",
      "Training Epoch: 1 [33600/49669]\tLoss: 255902.2656\n",
      "Training Epoch: 1 [33620/49669]\tLoss: 188707.5312\n",
      "Training Epoch: 1 [33640/49669]\tLoss: 293873.7812\n",
      "Training Epoch: 1 [33660/49669]\tLoss: 243436.8594\n",
      "Training Epoch: 1 [33680/49669]\tLoss: 268878.6875\n",
      "Training Epoch: 1 [33700/49669]\tLoss: 271840.7188\n",
      "Training Epoch: 1 [33720/49669]\tLoss: 244354.7344\n",
      "Training Epoch: 1 [33740/49669]\tLoss: 248992.6406\n",
      "Training Epoch: 1 [33760/49669]\tLoss: 243346.9062\n",
      "Training Epoch: 1 [33780/49669]\tLoss: 225061.5000\n",
      "Training Epoch: 1 [33800/49669]\tLoss: 250002.0000\n",
      "Training Epoch: 1 [33820/49669]\tLoss: 221515.3125\n",
      "Training Epoch: 1 [33840/49669]\tLoss: 252241.7188\n",
      "Training Epoch: 1 [33860/49669]\tLoss: 263538.8750\n",
      "Training Epoch: 1 [33880/49669]\tLoss: 245794.5781\n",
      "Training Epoch: 1 [33900/49669]\tLoss: 233901.6719\n",
      "Training Epoch: 1 [33920/49669]\tLoss: 282404.2500\n",
      "Training Epoch: 1 [33940/49669]\tLoss: 250186.8750\n",
      "Training Epoch: 1 [33960/49669]\tLoss: 280580.8750\n",
      "Training Epoch: 1 [33980/49669]\tLoss: 250670.6250\n",
      "Training Epoch: 1 [34000/49669]\tLoss: 269809.3438\n",
      "Training Epoch: 1 [34020/49669]\tLoss: 253676.8125\n",
      "Training Epoch: 1 [34040/49669]\tLoss: 242236.4375\n",
      "Training Epoch: 1 [34060/49669]\tLoss: 256154.6406\n",
      "Training Epoch: 1 [34080/49669]\tLoss: 229040.6250\n",
      "Training Epoch: 1 [34100/49669]\tLoss: 269571.1250\n",
      "Training Epoch: 1 [34120/49669]\tLoss: 232062.6719\n",
      "Training Epoch: 1 [34140/49669]\tLoss: 229598.1562\n",
      "Training Epoch: 1 [34160/49669]\tLoss: 211770.1562\n",
      "Training Epoch: 1 [34180/49669]\tLoss: 257975.4062\n",
      "Training Epoch: 1 [34200/49669]\tLoss: 231025.8750\n",
      "Training Epoch: 1 [34220/49669]\tLoss: 229393.3750\n",
      "Training Epoch: 1 [34240/49669]\tLoss: 252053.0781\n",
      "Training Epoch: 1 [34260/49669]\tLoss: 243421.2500\n",
      "Training Epoch: 1 [34280/49669]\tLoss: 228999.6094\n",
      "Training Epoch: 1 [34300/49669]\tLoss: 245747.9219\n",
      "Training Epoch: 1 [34320/49669]\tLoss: 249827.7188\n",
      "Training Epoch: 1 [34340/49669]\tLoss: 257494.4375\n",
      "Training Epoch: 1 [34360/49669]\tLoss: 174448.4062\n",
      "Training Epoch: 1 [34380/49669]\tLoss: 249985.7188\n",
      "Training Epoch: 1 [34400/49669]\tLoss: 278264.0938\n",
      "Training Epoch: 1 [34420/49669]\tLoss: 231370.0156\n",
      "Training Epoch: 1 [34440/49669]\tLoss: 201463.4844\n",
      "Training Epoch: 1 [34460/49669]\tLoss: 233189.7500\n",
      "Training Epoch: 1 [34480/49669]\tLoss: 240705.2812\n",
      "Training Epoch: 1 [34500/49669]\tLoss: 206103.4688\n",
      "Training Epoch: 1 [34520/49669]\tLoss: 240654.5781\n",
      "Training Epoch: 1 [34540/49669]\tLoss: 219851.0469\n",
      "Training Epoch: 1 [34560/49669]\tLoss: 222906.1406\n",
      "Training Epoch: 1 [34580/49669]\tLoss: 221902.5469\n",
      "Training Epoch: 1 [34600/49669]\tLoss: 198518.3594\n",
      "Training Epoch: 1 [34620/49669]\tLoss: 227324.0938\n",
      "Training Epoch: 1 [34640/49669]\tLoss: 222107.0000\n",
      "Training Epoch: 1 [34660/49669]\tLoss: 235482.8750\n",
      "Training Epoch: 1 [34680/49669]\tLoss: 238780.6562\n",
      "Training Epoch: 1 [34700/49669]\tLoss: 189933.0000\n",
      "Training Epoch: 1 [34720/49669]\tLoss: 225525.4844\n",
      "Training Epoch: 1 [34740/49669]\tLoss: 194733.3438\n",
      "Training Epoch: 1 [34760/49669]\tLoss: 214233.9531\n",
      "Training Epoch: 1 [34780/49669]\tLoss: 189231.9531\n",
      "Training Epoch: 1 [34800/49669]\tLoss: 227101.6719\n",
      "Training Epoch: 1 [34820/49669]\tLoss: 172041.3438\n",
      "Training Epoch: 1 [34840/49669]\tLoss: 260524.4219\n",
      "Training Epoch: 1 [34860/49669]\tLoss: 180795.0938\n",
      "Training Epoch: 1 [34880/49669]\tLoss: 225015.2969\n",
      "Training Epoch: 1 [34900/49669]\tLoss: 238445.2656\n",
      "Training Epoch: 1 [34920/49669]\tLoss: 165734.5469\n",
      "Training Epoch: 1 [34940/49669]\tLoss: 194658.2188\n",
      "Training Epoch: 1 [34960/49669]\tLoss: 220474.8438\n",
      "Training Epoch: 1 [34980/49669]\tLoss: 217096.8281\n",
      "Training Epoch: 1 [35000/49669]\tLoss: 255470.1562\n",
      "Training Epoch: 1 [35020/49669]\tLoss: 223571.2812\n",
      "Training Epoch: 1 [35040/49669]\tLoss: 213753.0000\n",
      "Training Epoch: 1 [35060/49669]\tLoss: 188762.3594\n",
      "Training Epoch: 1 [35080/49669]\tLoss: 219287.3906\n",
      "Training Epoch: 1 [35100/49669]\tLoss: 227845.6875\n",
      "Training Epoch: 1 [35120/49669]\tLoss: 231466.4531\n",
      "Training Epoch: 1 [35140/49669]\tLoss: 214336.9375\n",
      "Training Epoch: 1 [35160/49669]\tLoss: 226771.2812\n",
      "Training Epoch: 1 [35180/49669]\tLoss: 230794.3125\n",
      "Training Epoch: 1 [35200/49669]\tLoss: 219199.1875\n",
      "Training Epoch: 1 [35220/49669]\tLoss: 225977.4688\n",
      "Training Epoch: 1 [35240/49669]\tLoss: 243930.8750\n",
      "Training Epoch: 1 [35260/49669]\tLoss: 213178.0312\n",
      "Training Epoch: 1 [35280/49669]\tLoss: 231341.0469\n",
      "Training Epoch: 1 [35300/49669]\tLoss: 187448.3438\n",
      "Training Epoch: 1 [35320/49669]\tLoss: 216423.9844\n",
      "Training Epoch: 1 [35340/49669]\tLoss: 186455.0000\n",
      "Training Epoch: 1 [35360/49669]\tLoss: 195132.0781\n",
      "Training Epoch: 1 [35380/49669]\tLoss: 218605.1094\n",
      "Training Epoch: 1 [35400/49669]\tLoss: 188306.4062\n",
      "Training Epoch: 1 [35420/49669]\tLoss: 226523.5781\n",
      "Training Epoch: 1 [35440/49669]\tLoss: 218378.5312\n",
      "Training Epoch: 1 [35460/49669]\tLoss: 204164.1719\n",
      "Training Epoch: 1 [35480/49669]\tLoss: 175907.1250\n",
      "Training Epoch: 1 [35500/49669]\tLoss: 236991.4375\n",
      "Training Epoch: 1 [35520/49669]\tLoss: 218855.1406\n",
      "Training Epoch: 1 [35540/49669]\tLoss: 224023.8750\n",
      "Training Epoch: 1 [35560/49669]\tLoss: 182330.2188\n",
      "Training Epoch: 1 [35580/49669]\tLoss: 200346.6875\n",
      "Training Epoch: 1 [35600/49669]\tLoss: 206382.2188\n",
      "Training Epoch: 1 [35620/49669]\tLoss: 231402.2656\n",
      "Training Epoch: 1 [35640/49669]\tLoss: 224020.7031\n",
      "Training Epoch: 1 [35660/49669]\tLoss: 241715.4219\n",
      "Training Epoch: 1 [35680/49669]\tLoss: 233154.1562\n",
      "Training Epoch: 1 [35700/49669]\tLoss: 213364.3125\n",
      "Training Epoch: 1 [35720/49669]\tLoss: 203736.7812\n",
      "Training Epoch: 1 [35740/49669]\tLoss: 197331.9062\n",
      "Training Epoch: 1 [35760/49669]\tLoss: 203358.7500\n",
      "Training Epoch: 1 [35780/49669]\tLoss: 231095.0156\n",
      "Training Epoch: 1 [35800/49669]\tLoss: 190001.2031\n",
      "Training Epoch: 1 [35820/49669]\tLoss: 204976.9219\n",
      "Training Epoch: 1 [35840/49669]\tLoss: 211552.0938\n",
      "Training Epoch: 1 [35860/49669]\tLoss: 203463.7812\n",
      "Training Epoch: 1 [35880/49669]\tLoss: 219745.3750\n",
      "Training Epoch: 1 [35900/49669]\tLoss: 240894.3906\n",
      "Training Epoch: 1 [35920/49669]\tLoss: 190045.5312\n",
      "Training Epoch: 1 [35940/49669]\tLoss: 211630.8281\n",
      "Training Epoch: 1 [35960/49669]\tLoss: 187437.6719\n",
      "Training Epoch: 1 [35980/49669]\tLoss: 218001.6250\n",
      "Training Epoch: 1 [36000/49669]\tLoss: 207436.8594\n",
      "Training Epoch: 1 [36020/49669]\tLoss: 212844.9844\n",
      "Training Epoch: 1 [36040/49669]\tLoss: 211341.7500\n",
      "Training Epoch: 1 [36060/49669]\tLoss: 227891.1875\n",
      "Training Epoch: 1 [36080/49669]\tLoss: 221819.0312\n",
      "Training Epoch: 1 [36100/49669]\tLoss: 210494.9375\n",
      "Training Epoch: 1 [36120/49669]\tLoss: 208349.4219\n",
      "Training Epoch: 1 [36140/49669]\tLoss: 180940.1250\n",
      "Training Epoch: 1 [36160/49669]\tLoss: 170712.8906\n",
      "Training Epoch: 1 [36180/49669]\tLoss: 199577.3750\n",
      "Training Epoch: 1 [36200/49669]\tLoss: 189684.1094\n",
      "Training Epoch: 1 [36220/49669]\tLoss: 183092.8906\n",
      "Training Epoch: 1 [36240/49669]\tLoss: 205996.6562\n",
      "Training Epoch: 1 [36260/49669]\tLoss: 211836.8750\n",
      "Training Epoch: 1 [36280/49669]\tLoss: 192727.3125\n",
      "Training Epoch: 1 [36300/49669]\tLoss: 194565.7656\n",
      "Training Epoch: 1 [36320/49669]\tLoss: 237431.0781\n",
      "Training Epoch: 1 [36340/49669]\tLoss: 203285.4062\n",
      "Training Epoch: 1 [36360/49669]\tLoss: 178018.5312\n",
      "Training Epoch: 1 [36380/49669]\tLoss: 190521.6562\n",
      "Training Epoch: 1 [36400/49669]\tLoss: 232323.2188\n",
      "Training Epoch: 1 [36420/49669]\tLoss: 189966.9688\n",
      "Training Epoch: 1 [36440/49669]\tLoss: 220579.1094\n",
      "Training Epoch: 1 [36460/49669]\tLoss: 207952.6094\n",
      "Training Epoch: 1 [36480/49669]\tLoss: 208365.9062\n",
      "Training Epoch: 1 [36500/49669]\tLoss: 215997.0156\n",
      "Training Epoch: 1 [36520/49669]\tLoss: 193780.8594\n",
      "Training Epoch: 1 [36540/49669]\tLoss: 199874.3594\n",
      "Training Epoch: 1 [36560/49669]\tLoss: 218922.7031\n",
      "Training Epoch: 1 [36580/49669]\tLoss: 191425.5312\n",
      "Training Epoch: 1 [36600/49669]\tLoss: 206349.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [36620/49669]\tLoss: 211523.2969\n",
      "Training Epoch: 1 [36640/49669]\tLoss: 212605.0000\n",
      "Training Epoch: 1 [36660/49669]\tLoss: 157907.1406\n",
      "Training Epoch: 1 [36680/49669]\tLoss: 177631.4531\n",
      "Training Epoch: 1 [36700/49669]\tLoss: 201204.8125\n",
      "Training Epoch: 1 [36720/49669]\tLoss: 232577.8906\n",
      "Training Epoch: 1 [36740/49669]\tLoss: 210456.6250\n",
      "Training Epoch: 1 [36760/49669]\tLoss: 212755.3438\n",
      "Training Epoch: 1 [36780/49669]\tLoss: 195487.3125\n",
      "Training Epoch: 1 [36800/49669]\tLoss: 196439.4062\n",
      "Training Epoch: 1 [36820/49669]\tLoss: 172580.1875\n",
      "Training Epoch: 1 [36840/49669]\tLoss: 183722.6406\n",
      "Training Epoch: 1 [36860/49669]\tLoss: 178665.3594\n",
      "Training Epoch: 1 [36880/49669]\tLoss: 166833.8750\n",
      "Training Epoch: 1 [36900/49669]\tLoss: 217536.4062\n",
      "Training Epoch: 1 [36920/49669]\tLoss: 174857.4375\n",
      "Training Epoch: 1 [36940/49669]\tLoss: 181723.5312\n",
      "Training Epoch: 1 [36960/49669]\tLoss: 180685.7969\n",
      "Training Epoch: 1 [36980/49669]\tLoss: 205582.6562\n",
      "Training Epoch: 1 [37000/49669]\tLoss: 210504.2188\n",
      "Training Epoch: 1 [37020/49669]\tLoss: 204990.8594\n",
      "Training Epoch: 1 [37040/49669]\tLoss: 199654.9219\n",
      "Training Epoch: 1 [37060/49669]\tLoss: 186823.7344\n",
      "Training Epoch: 1 [37080/49669]\tLoss: 185389.5000\n",
      "Training Epoch: 1 [37100/49669]\tLoss: 153613.3750\n",
      "Training Epoch: 1 [37120/49669]\tLoss: 173962.1406\n",
      "Training Epoch: 1 [37140/49669]\tLoss: 181346.7656\n",
      "Training Epoch: 1 [37160/49669]\tLoss: 185626.6250\n",
      "Training Epoch: 1 [37180/49669]\tLoss: 209414.0312\n",
      "Training Epoch: 1 [37200/49669]\tLoss: 165863.0156\n",
      "Training Epoch: 1 [37220/49669]\tLoss: 181620.9062\n",
      "Training Epoch: 1 [37240/49669]\tLoss: 212333.1406\n",
      "Training Epoch: 1 [37260/49669]\tLoss: 195973.7656\n",
      "Training Epoch: 1 [37280/49669]\tLoss: 183910.4062\n",
      "Training Epoch: 1 [37300/49669]\tLoss: 192734.6719\n",
      "Training Epoch: 1 [37320/49669]\tLoss: 189608.7500\n",
      "Training Epoch: 1 [37340/49669]\tLoss: 167673.1562\n",
      "Training Epoch: 1 [37360/49669]\tLoss: 185523.7969\n",
      "Training Epoch: 1 [37380/49669]\tLoss: 195132.4844\n",
      "Training Epoch: 1 [37400/49669]\tLoss: 176231.5000\n",
      "Training Epoch: 1 [37420/49669]\tLoss: 191612.2969\n",
      "Training Epoch: 1 [37440/49669]\tLoss: 184585.2812\n",
      "Training Epoch: 1 [37460/49669]\tLoss: 206378.6719\n",
      "Training Epoch: 1 [37480/49669]\tLoss: 207075.2500\n",
      "Training Epoch: 1 [37500/49669]\tLoss: 187293.6406\n",
      "Training Epoch: 1 [37520/49669]\tLoss: 182268.7188\n",
      "Training Epoch: 1 [37540/49669]\tLoss: 192521.5000\n",
      "Training Epoch: 1 [37560/49669]\tLoss: 189449.7188\n",
      "Training Epoch: 1 [37580/49669]\tLoss: 167105.8125\n",
      "Training Epoch: 1 [37600/49669]\tLoss: 215441.4062\n",
      "Training Epoch: 1 [37620/49669]\tLoss: 180244.0312\n",
      "Training Epoch: 1 [37640/49669]\tLoss: 206707.7031\n",
      "Training Epoch: 1 [37660/49669]\tLoss: 179730.0781\n",
      "Training Epoch: 1 [37680/49669]\tLoss: 205048.6406\n",
      "Training Epoch: 1 [37700/49669]\tLoss: 180098.4219\n",
      "Training Epoch: 1 [37720/49669]\tLoss: 191220.2344\n",
      "Training Epoch: 1 [37740/49669]\tLoss: 175278.4531\n",
      "Training Epoch: 1 [37760/49669]\tLoss: 194696.6875\n",
      "Training Epoch: 1 [37780/49669]\tLoss: 212912.1250\n",
      "Training Epoch: 1 [37800/49669]\tLoss: 173145.0156\n",
      "Training Epoch: 1 [37820/49669]\tLoss: 160627.4375\n",
      "Training Epoch: 1 [37840/49669]\tLoss: 192097.8750\n",
      "Training Epoch: 1 [37860/49669]\tLoss: 178359.4531\n",
      "Training Epoch: 1 [37880/49669]\tLoss: 193742.8906\n",
      "Training Epoch: 1 [37900/49669]\tLoss: 170682.4688\n",
      "Training Epoch: 1 [37920/49669]\tLoss: 187560.0000\n",
      "Training Epoch: 1 [37940/49669]\tLoss: 181833.8281\n",
      "Training Epoch: 1 [37960/49669]\tLoss: 180568.2031\n",
      "Training Epoch: 1 [37980/49669]\tLoss: 174818.5938\n",
      "Training Epoch: 1 [38000/49669]\tLoss: 186686.3750\n",
      "Training Epoch: 1 [38020/49669]\tLoss: 170838.8281\n",
      "Training Epoch: 1 [38040/49669]\tLoss: 168318.2031\n",
      "Training Epoch: 1 [38060/49669]\tLoss: 160713.6406\n",
      "Training Epoch: 1 [38080/49669]\tLoss: 156923.9688\n",
      "Training Epoch: 1 [38100/49669]\tLoss: 187669.5781\n",
      "Training Epoch: 1 [38120/49669]\tLoss: 170331.7031\n",
      "Training Epoch: 1 [38140/49669]\tLoss: 162998.1875\n",
      "Training Epoch: 1 [38160/49669]\tLoss: 164875.3594\n",
      "Training Epoch: 1 [38180/49669]\tLoss: 155293.2031\n",
      "Training Epoch: 1 [38200/49669]\tLoss: 181988.3750\n",
      "Training Epoch: 1 [38220/49669]\tLoss: 180994.4844\n",
      "Training Epoch: 1 [38240/49669]\tLoss: 184547.6250\n",
      "Training Epoch: 1 [38260/49669]\tLoss: 153630.5625\n",
      "Training Epoch: 1 [38280/49669]\tLoss: 170623.7656\n",
      "Training Epoch: 1 [38300/49669]\tLoss: 159263.0312\n",
      "Training Epoch: 1 [38320/49669]\tLoss: 179072.3438\n",
      "Training Epoch: 1 [38340/49669]\tLoss: 164686.9375\n",
      "Training Epoch: 1 [38360/49669]\tLoss: 161739.5781\n",
      "Training Epoch: 1 [38380/49669]\tLoss: 178767.8281\n",
      "Training Epoch: 1 [38400/49669]\tLoss: 165432.2656\n",
      "Training Epoch: 1 [38420/49669]\tLoss: 148126.3906\n",
      "Training Epoch: 1 [38440/49669]\tLoss: 177569.0625\n",
      "Training Epoch: 1 [38460/49669]\tLoss: 163238.7500\n",
      "Training Epoch: 1 [38480/49669]\tLoss: 174156.2812\n",
      "Training Epoch: 1 [38500/49669]\tLoss: 183417.7812\n",
      "Training Epoch: 1 [38520/49669]\tLoss: 171470.9375\n",
      "Training Epoch: 1 [38540/49669]\tLoss: 172301.5938\n",
      "Training Epoch: 1 [38560/49669]\tLoss: 119699.1562\n",
      "Training Epoch: 1 [38580/49669]\tLoss: 168266.6875\n",
      "Training Epoch: 1 [38600/49669]\tLoss: 170733.1094\n",
      "Training Epoch: 1 [38620/49669]\tLoss: 160505.6094\n",
      "Training Epoch: 1 [38640/49669]\tLoss: 187487.8906\n",
      "Training Epoch: 1 [38660/49669]\tLoss: 195640.7500\n",
      "Training Epoch: 1 [38680/49669]\tLoss: 168350.9688\n",
      "Training Epoch: 1 [38700/49669]\tLoss: 194578.0000\n",
      "Training Epoch: 1 [38720/49669]\tLoss: 165450.4062\n",
      "Training Epoch: 1 [38740/49669]\tLoss: 178165.4219\n",
      "Training Epoch: 1 [38760/49669]\tLoss: 143664.4219\n",
      "Training Epoch: 1 [38780/49669]\tLoss: 148133.9531\n",
      "Training Epoch: 1 [38800/49669]\tLoss: 151311.5000\n",
      "Training Epoch: 1 [38820/49669]\tLoss: 182274.2656\n",
      "Training Epoch: 1 [38840/49669]\tLoss: 134181.5938\n",
      "Training Epoch: 1 [38860/49669]\tLoss: 154728.2344\n",
      "Training Epoch: 1 [38880/49669]\tLoss: 149220.7500\n",
      "Training Epoch: 1 [38900/49669]\tLoss: 178748.1406\n",
      "Training Epoch: 1 [38920/49669]\tLoss: 192449.7344\n",
      "Training Epoch: 1 [38940/49669]\tLoss: 167198.5312\n",
      "Training Epoch: 1 [38960/49669]\tLoss: 160684.7344\n",
      "Training Epoch: 1 [38980/49669]\tLoss: 166520.8750\n",
      "Training Epoch: 1 [39000/49669]\tLoss: 167284.9219\n",
      "Training Epoch: 1 [39020/49669]\tLoss: 173947.9219\n",
      "Training Epoch: 1 [39040/49669]\tLoss: 151389.4219\n",
      "Training Epoch: 1 [39060/49669]\tLoss: 163168.7969\n",
      "Training Epoch: 1 [39080/49669]\tLoss: 156882.8438\n",
      "Training Epoch: 1 [39100/49669]\tLoss: 167740.1562\n",
      "Training Epoch: 1 [39120/49669]\tLoss: 163609.0156\n",
      "Training Epoch: 1 [39140/49669]\tLoss: 160677.8906\n",
      "Training Epoch: 1 [39160/49669]\tLoss: 179854.5469\n",
      "Training Epoch: 1 [39180/49669]\tLoss: 169788.6875\n",
      "Training Epoch: 1 [39200/49669]\tLoss: 165070.9219\n",
      "Training Epoch: 1 [39220/49669]\tLoss: 145589.8906\n",
      "Training Epoch: 1 [39240/49669]\tLoss: 158877.0469\n",
      "Training Epoch: 1 [39260/49669]\tLoss: 150435.2188\n",
      "Training Epoch: 1 [39280/49669]\tLoss: 167377.9688\n",
      "Training Epoch: 1 [39300/49669]\tLoss: 137633.7812\n",
      "Training Epoch: 1 [39320/49669]\tLoss: 164180.5938\n",
      "Training Epoch: 1 [39340/49669]\tLoss: 147689.7188\n",
      "Training Epoch: 1 [39360/49669]\tLoss: 161355.2188\n",
      "Training Epoch: 1 [39380/49669]\tLoss: 167923.9531\n",
      "Training Epoch: 1 [39400/49669]\tLoss: 173578.6875\n",
      "Training Epoch: 1 [39420/49669]\tLoss: 167180.2031\n",
      "Training Epoch: 1 [39440/49669]\tLoss: 188770.5469\n",
      "Training Epoch: 1 [39460/49669]\tLoss: 154457.6875\n",
      "Training Epoch: 1 [39480/49669]\tLoss: 157853.0156\n",
      "Training Epoch: 1 [39500/49669]\tLoss: 174970.2188\n",
      "Training Epoch: 1 [39520/49669]\tLoss: 166545.3438\n",
      "Training Epoch: 1 [39540/49669]\tLoss: 167965.4688\n",
      "Training Epoch: 1 [39560/49669]\tLoss: 155862.4844\n",
      "Training Epoch: 1 [39580/49669]\tLoss: 193912.2031\n",
      "Training Epoch: 1 [39600/49669]\tLoss: 184833.5469\n",
      "Training Epoch: 1 [39620/49669]\tLoss: 160205.6406\n",
      "Training Epoch: 1 [39640/49669]\tLoss: 160595.4062\n",
      "Training Epoch: 1 [39660/49669]\tLoss: 162299.7500\n",
      "Training Epoch: 1 [39680/49669]\tLoss: 154072.8281\n",
      "Training Epoch: 1 [39700/49669]\tLoss: 146724.5781\n",
      "Training Epoch: 1 [39720/49669]\tLoss: 157144.2031\n",
      "Training Epoch: 1 [39740/49669]\tLoss: 160530.0938\n",
      "Training Epoch: 1 [39760/49669]\tLoss: 163406.0000\n",
      "Training Epoch: 1 [39780/49669]\tLoss: 144150.3438\n",
      "Training Epoch: 1 [39800/49669]\tLoss: 126710.9297\n",
      "Training Epoch: 1 [39820/49669]\tLoss: 159176.0312\n",
      "Training Epoch: 1 [39840/49669]\tLoss: 157482.5156\n",
      "Training Epoch: 1 [39860/49669]\tLoss: 157742.7031\n",
      "Training Epoch: 1 [39880/49669]\tLoss: 176079.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [39900/49669]\tLoss: 141178.7031\n",
      "Training Epoch: 1 [39920/49669]\tLoss: 158899.1406\n",
      "Training Epoch: 1 [39940/49669]\tLoss: 150651.9688\n",
      "Training Epoch: 1 [39960/49669]\tLoss: 171682.8906\n",
      "Training Epoch: 1 [39980/49669]\tLoss: 147824.8438\n",
      "Training Epoch: 1 [40000/49669]\tLoss: 153635.5156\n",
      "Training Epoch: 1 [40020/49669]\tLoss: 147870.5625\n",
      "Training Epoch: 1 [40040/49669]\tLoss: 142842.0312\n",
      "Training Epoch: 1 [40060/49669]\tLoss: 168872.1875\n",
      "Training Epoch: 1 [40080/49669]\tLoss: 143078.4219\n",
      "Training Epoch: 1 [40100/49669]\tLoss: 159630.0781\n",
      "Training Epoch: 1 [40120/49669]\tLoss: 148606.1250\n",
      "Training Epoch: 1 [40140/49669]\tLoss: 153140.2500\n",
      "Training Epoch: 1 [40160/49669]\tLoss: 157498.9688\n",
      "Training Epoch: 1 [40180/49669]\tLoss: 152892.7188\n",
      "Training Epoch: 1 [40200/49669]\tLoss: 155197.8594\n",
      "Training Epoch: 1 [40220/49669]\tLoss: 166170.8906\n",
      "Training Epoch: 1 [40240/49669]\tLoss: 143126.9531\n",
      "Training Epoch: 1 [40260/49669]\tLoss: 151282.4062\n",
      "Training Epoch: 1 [40280/49669]\tLoss: 120309.0938\n",
      "Training Epoch: 1 [40300/49669]\tLoss: 159827.9062\n",
      "Training Epoch: 1 [40320/49669]\tLoss: 134630.6719\n",
      "Training Epoch: 1 [40340/49669]\tLoss: 140696.9844\n",
      "Training Epoch: 1 [40360/49669]\tLoss: 156673.8750\n",
      "Training Epoch: 1 [40380/49669]\tLoss: 144595.1250\n",
      "Training Epoch: 1 [40400/49669]\tLoss: 164718.5938\n",
      "Training Epoch: 1 [40420/49669]\tLoss: 159063.7656\n",
      "Training Epoch: 1 [40440/49669]\tLoss: 173188.7344\n",
      "Training Epoch: 1 [40460/49669]\tLoss: 156173.8438\n",
      "Training Epoch: 1 [40480/49669]\tLoss: 154082.8281\n",
      "Training Epoch: 1 [40500/49669]\tLoss: 117931.4141\n",
      "Training Epoch: 1 [40520/49669]\tLoss: 155740.0625\n",
      "Training Epoch: 1 [40540/49669]\tLoss: 172237.1094\n",
      "Training Epoch: 1 [40560/49669]\tLoss: 155457.3750\n",
      "Training Epoch: 1 [40580/49669]\tLoss: 158018.6094\n",
      "Training Epoch: 1 [40600/49669]\tLoss: 137675.5312\n",
      "Training Epoch: 1 [40620/49669]\tLoss: 149171.6719\n",
      "Training Epoch: 1 [40640/49669]\tLoss: 137418.3906\n",
      "Training Epoch: 1 [40660/49669]\tLoss: 156971.0312\n",
      "Training Epoch: 1 [40680/49669]\tLoss: 153042.3906\n",
      "Training Epoch: 1 [40700/49669]\tLoss: 166420.7188\n",
      "Training Epoch: 1 [40720/49669]\tLoss: 140880.7188\n",
      "Training Epoch: 1 [40740/49669]\tLoss: 139647.5000\n",
      "Training Epoch: 1 [40760/49669]\tLoss: 124357.2188\n",
      "Training Epoch: 1 [40780/49669]\tLoss: 139783.2031\n",
      "Training Epoch: 1 [40800/49669]\tLoss: 160319.4844\n",
      "Training Epoch: 1 [40820/49669]\tLoss: 155912.1562\n",
      "Training Epoch: 1 [40840/49669]\tLoss: 126638.4141\n",
      "Training Epoch: 1 [40860/49669]\tLoss: 151502.0000\n",
      "Training Epoch: 1 [40880/49669]\tLoss: 150472.6562\n",
      "Training Epoch: 1 [40900/49669]\tLoss: 130948.9531\n",
      "Training Epoch: 1 [40920/49669]\tLoss: 154987.7812\n",
      "Training Epoch: 1 [40940/49669]\tLoss: 158919.0625\n",
      "Training Epoch: 1 [40960/49669]\tLoss: 153296.8438\n",
      "Training Epoch: 1 [40980/49669]\tLoss: 124576.3125\n",
      "Training Epoch: 1 [41000/49669]\tLoss: 167058.2656\n",
      "Training Epoch: 1 [41020/49669]\tLoss: 157367.6562\n",
      "Training Epoch: 1 [41040/49669]\tLoss: 117592.7812\n",
      "Training Epoch: 1 [41060/49669]\tLoss: 144030.2344\n",
      "Training Epoch: 1 [41080/49669]\tLoss: 145831.9531\n",
      "Training Epoch: 1 [41100/49669]\tLoss: 143629.1406\n",
      "Training Epoch: 1 [41120/49669]\tLoss: 158024.5312\n",
      "Training Epoch: 1 [41140/49669]\tLoss: 149763.2969\n",
      "Training Epoch: 1 [41160/49669]\tLoss: 127735.3828\n",
      "Training Epoch: 1 [41180/49669]\tLoss: 156951.8906\n",
      "Training Epoch: 1 [41200/49669]\tLoss: 151299.2812\n",
      "Training Epoch: 1 [41220/49669]\tLoss: 146474.7969\n",
      "Training Epoch: 1 [41240/49669]\tLoss: 132256.6875\n",
      "Training Epoch: 1 [41260/49669]\tLoss: 132554.0781\n",
      "Training Epoch: 1 [41280/49669]\tLoss: 144004.1875\n",
      "Training Epoch: 1 [41300/49669]\tLoss: 150106.9531\n",
      "Training Epoch: 1 [41320/49669]\tLoss: 153013.0938\n",
      "Training Epoch: 1 [41340/49669]\tLoss: 128225.5547\n",
      "Training Epoch: 1 [41360/49669]\tLoss: 139578.7656\n",
      "Training Epoch: 1 [41380/49669]\tLoss: 161078.1094\n",
      "Training Epoch: 1 [41400/49669]\tLoss: 141039.5625\n",
      "Training Epoch: 1 [41420/49669]\tLoss: 138444.1875\n",
      "Training Epoch: 1 [41440/49669]\tLoss: 139727.5000\n",
      "Training Epoch: 1 [41460/49669]\tLoss: 140897.5156\n",
      "Training Epoch: 1 [41480/49669]\tLoss: 130896.3203\n",
      "Training Epoch: 1 [41500/49669]\tLoss: 147766.1250\n",
      "Training Epoch: 1 [41520/49669]\tLoss: 156350.9531\n",
      "Training Epoch: 1 [41540/49669]\tLoss: 142734.7031\n",
      "Training Epoch: 1 [41560/49669]\tLoss: 144716.8281\n",
      "Training Epoch: 1 [41580/49669]\tLoss: 140268.1719\n",
      "Training Epoch: 1 [41600/49669]\tLoss: 135940.6875\n",
      "Training Epoch: 1 [41620/49669]\tLoss: 152598.4375\n",
      "Training Epoch: 1 [41640/49669]\tLoss: 154791.2812\n",
      "Training Epoch: 1 [41660/49669]\tLoss: 159022.0000\n",
      "Training Epoch: 1 [41680/49669]\tLoss: 141945.9062\n",
      "Training Epoch: 1 [41700/49669]\tLoss: 131758.4219\n",
      "Training Epoch: 1 [41720/49669]\tLoss: 112434.5469\n",
      "Training Epoch: 1 [41740/49669]\tLoss: 157641.2500\n",
      "Training Epoch: 1 [41760/49669]\tLoss: 143496.7344\n",
      "Training Epoch: 1 [41780/49669]\tLoss: 134251.6719\n",
      "Training Epoch: 1 [41800/49669]\tLoss: 122359.3828\n",
      "Training Epoch: 1 [41820/49669]\tLoss: 139297.9688\n",
      "Training Epoch: 1 [41840/49669]\tLoss: 126125.5234\n",
      "Training Epoch: 1 [41860/49669]\tLoss: 138407.7188\n",
      "Training Epoch: 1 [41880/49669]\tLoss: 149567.5781\n",
      "Training Epoch: 1 [41900/49669]\tLoss: 128425.1328\n",
      "Training Epoch: 1 [41920/49669]\tLoss: 142946.0469\n",
      "Training Epoch: 1 [41940/49669]\tLoss: 128797.9922\n",
      "Training Epoch: 1 [41960/49669]\tLoss: 128255.2734\n",
      "Training Epoch: 1 [41980/49669]\tLoss: 140820.0781\n",
      "Training Epoch: 1 [42000/49669]\tLoss: 143226.4531\n",
      "Training Epoch: 1 [42020/49669]\tLoss: 111536.8750\n",
      "Training Epoch: 1 [42040/49669]\tLoss: 142360.1250\n",
      "Training Epoch: 1 [42060/49669]\tLoss: 123211.8828\n",
      "Training Epoch: 1 [42080/49669]\tLoss: 133104.7812\n",
      "Training Epoch: 1 [42100/49669]\tLoss: 142878.2969\n",
      "Training Epoch: 1 [42120/49669]\tLoss: 117562.4609\n",
      "Training Epoch: 1 [42140/49669]\tLoss: 133667.4062\n",
      "Training Epoch: 1 [42160/49669]\tLoss: 133102.5312\n",
      "Training Epoch: 1 [42180/49669]\tLoss: 124918.9922\n",
      "Training Epoch: 1 [42200/49669]\tLoss: 139386.4219\n",
      "Training Epoch: 1 [42220/49669]\tLoss: 118849.6328\n",
      "Training Epoch: 1 [42240/49669]\tLoss: 114998.0547\n",
      "Training Epoch: 1 [42260/49669]\tLoss: 147518.2031\n",
      "Training Epoch: 1 [42280/49669]\tLoss: 124076.9531\n",
      "Training Epoch: 1 [42300/49669]\tLoss: 134317.0312\n",
      "Training Epoch: 1 [42320/49669]\tLoss: 117200.4297\n",
      "Training Epoch: 1 [42340/49669]\tLoss: 144841.8281\n",
      "Training Epoch: 1 [42360/49669]\tLoss: 149761.2969\n",
      "Training Epoch: 1 [42380/49669]\tLoss: 132664.6094\n",
      "Training Epoch: 1 [42400/49669]\tLoss: 140530.1875\n",
      "Training Epoch: 1 [42420/49669]\tLoss: 133022.9688\n",
      "Training Epoch: 1 [42440/49669]\tLoss: 135813.6406\n",
      "Training Epoch: 1 [42460/49669]\tLoss: 131857.4531\n",
      "Training Epoch: 1 [42480/49669]\tLoss: 148816.8594\n",
      "Training Epoch: 1 [42500/49669]\tLoss: 107530.5469\n",
      "Training Epoch: 1 [42520/49669]\tLoss: 124744.9141\n",
      "Training Epoch: 1 [42540/49669]\tLoss: 133883.6562\n",
      "Training Epoch: 1 [42560/49669]\tLoss: 123460.5547\n",
      "Training Epoch: 1 [42580/49669]\tLoss: 138540.9062\n",
      "Training Epoch: 1 [42600/49669]\tLoss: 115049.6406\n",
      "Training Epoch: 1 [42620/49669]\tLoss: 141495.3281\n",
      "Training Epoch: 1 [42640/49669]\tLoss: 112989.8516\n",
      "Training Epoch: 1 [42660/49669]\tLoss: 125581.9844\n",
      "Training Epoch: 1 [42680/49669]\tLoss: 149939.5625\n",
      "Training Epoch: 1 [42700/49669]\tLoss: 115604.7734\n",
      "Training Epoch: 1 [42720/49669]\tLoss: 102232.5078\n",
      "Training Epoch: 1 [42740/49669]\tLoss: 122900.9609\n",
      "Training Epoch: 1 [42760/49669]\tLoss: 131425.7500\n",
      "Training Epoch: 1 [42780/49669]\tLoss: 143988.7500\n",
      "Training Epoch: 1 [42800/49669]\tLoss: 125592.5469\n",
      "Training Epoch: 1 [42820/49669]\tLoss: 118760.6719\n",
      "Training Epoch: 1 [42840/49669]\tLoss: 126099.2188\n",
      "Training Epoch: 1 [42860/49669]\tLoss: 136600.8438\n",
      "Training Epoch: 1 [42880/49669]\tLoss: 144321.2812\n",
      "Training Epoch: 1 [42900/49669]\tLoss: 131432.0938\n",
      "Training Epoch: 1 [42920/49669]\tLoss: 132882.8750\n",
      "Training Epoch: 1 [42940/49669]\tLoss: 117293.4297\n",
      "Training Epoch: 1 [42960/49669]\tLoss: 140822.3750\n",
      "Training Epoch: 1 [42980/49669]\tLoss: 128337.6172\n",
      "Training Epoch: 1 [43000/49669]\tLoss: 144771.7969\n",
      "Training Epoch: 1 [43020/49669]\tLoss: 128173.1797\n",
      "Training Epoch: 1 [43040/49669]\tLoss: 129421.3828\n",
      "Training Epoch: 1 [43060/49669]\tLoss: 135198.8750\n",
      "Training Epoch: 1 [43080/49669]\tLoss: 111534.2031\n",
      "Training Epoch: 1 [43100/49669]\tLoss: 119989.9219\n",
      "Training Epoch: 1 [43120/49669]\tLoss: 127420.1094\n",
      "Training Epoch: 1 [43140/49669]\tLoss: 129140.3203\n",
      "Training Epoch: 1 [43160/49669]\tLoss: 129115.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [43180/49669]\tLoss: 118270.0000\n",
      "Training Epoch: 1 [43200/49669]\tLoss: 114090.2422\n",
      "Training Epoch: 1 [43220/49669]\tLoss: 126650.4922\n",
      "Training Epoch: 1 [43240/49669]\tLoss: 109325.2500\n",
      "Training Epoch: 1 [43260/49669]\tLoss: 126325.3047\n",
      "Training Epoch: 1 [43280/49669]\tLoss: 132816.7656\n",
      "Training Epoch: 1 [43300/49669]\tLoss: 136541.3281\n",
      "Training Epoch: 1 [43320/49669]\tLoss: 117755.2422\n",
      "Training Epoch: 1 [43340/49669]\tLoss: 130127.6719\n",
      "Training Epoch: 1 [43360/49669]\tLoss: 131204.1406\n",
      "Training Epoch: 1 [43380/49669]\tLoss: 122695.8906\n",
      "Training Epoch: 1 [43400/49669]\tLoss: 120499.6953\n",
      "Training Epoch: 1 [43420/49669]\tLoss: 133496.1562\n",
      "Training Epoch: 1 [43440/49669]\tLoss: 124819.5156\n",
      "Training Epoch: 1 [43460/49669]\tLoss: 120290.9219\n",
      "Training Epoch: 1 [43480/49669]\tLoss: 121026.5078\n",
      "Training Epoch: 1 [43500/49669]\tLoss: 134305.8281\n",
      "Training Epoch: 1 [43520/49669]\tLoss: 125666.8281\n",
      "Training Epoch: 1 [43540/49669]\tLoss: 120053.2109\n",
      "Training Epoch: 1 [43560/49669]\tLoss: 122883.8906\n",
      "Training Epoch: 1 [43580/49669]\tLoss: 116891.3203\n",
      "Training Epoch: 1 [43600/49669]\tLoss: 111025.6641\n",
      "Training Epoch: 1 [43620/49669]\tLoss: 126000.9453\n",
      "Training Epoch: 1 [43640/49669]\tLoss: 118543.7578\n",
      "Training Epoch: 1 [43660/49669]\tLoss: 111043.6328\n",
      "Training Epoch: 1 [43680/49669]\tLoss: 135064.5000\n",
      "Training Epoch: 1 [43700/49669]\tLoss: 118684.4453\n",
      "Training Epoch: 1 [43720/49669]\tLoss: 119451.4219\n",
      "Training Epoch: 1 [43740/49669]\tLoss: 118278.3438\n",
      "Training Epoch: 1 [43760/49669]\tLoss: 117300.1172\n",
      "Training Epoch: 1 [43780/49669]\tLoss: 134962.8438\n",
      "Training Epoch: 1 [43800/49669]\tLoss: 116949.9609\n",
      "Training Epoch: 1 [43820/49669]\tLoss: 105391.2734\n",
      "Training Epoch: 1 [43840/49669]\tLoss: 130321.9766\n",
      "Training Epoch: 1 [43860/49669]\tLoss: 133037.5938\n",
      "Training Epoch: 1 [43880/49669]\tLoss: 110738.5391\n",
      "Training Epoch: 1 [43900/49669]\tLoss: 104470.0938\n",
      "Training Epoch: 1 [43920/49669]\tLoss: 126904.2656\n",
      "Training Epoch: 1 [43940/49669]\tLoss: 103568.1719\n",
      "Training Epoch: 1 [43960/49669]\tLoss: 139084.3906\n",
      "Training Epoch: 1 [43980/49669]\tLoss: 123802.0312\n",
      "Training Epoch: 1 [44000/49669]\tLoss: 149621.6250\n",
      "Training Epoch: 1 [44020/49669]\tLoss: 114405.8203\n",
      "Training Epoch: 1 [44040/49669]\tLoss: 128310.3594\n",
      "Training Epoch: 1 [44060/49669]\tLoss: 129012.5547\n",
      "Training Epoch: 1 [44080/49669]\tLoss: 127412.3672\n",
      "Training Epoch: 1 [44100/49669]\tLoss: 131982.8125\n",
      "Training Epoch: 1 [44120/49669]\tLoss: 103719.5469\n",
      "Training Epoch: 1 [44140/49669]\tLoss: 103486.3750\n",
      "Training Epoch: 1 [44160/49669]\tLoss: 131011.7656\n",
      "Training Epoch: 1 [44180/49669]\tLoss: 107667.1562\n",
      "Training Epoch: 1 [44200/49669]\tLoss: 123002.1250\n",
      "Training Epoch: 1 [44220/49669]\tLoss: 114157.6172\n",
      "Training Epoch: 1 [44240/49669]\tLoss: 117505.1094\n",
      "Training Epoch: 1 [44260/49669]\tLoss: 111488.6562\n",
      "Training Epoch: 1 [44280/49669]\tLoss: 118651.2266\n",
      "Training Epoch: 1 [44300/49669]\tLoss: 103498.1406\n",
      "Training Epoch: 1 [44320/49669]\tLoss: 119991.7188\n",
      "Training Epoch: 1 [44340/49669]\tLoss: 119540.2969\n",
      "Training Epoch: 1 [44360/49669]\tLoss: 117751.7188\n",
      "Training Epoch: 1 [44380/49669]\tLoss: 116621.4062\n",
      "Training Epoch: 1 [44400/49669]\tLoss: 105346.6953\n",
      "Training Epoch: 1 [44420/49669]\tLoss: 129990.6172\n",
      "Training Epoch: 1 [44440/49669]\tLoss: 112964.4531\n",
      "Training Epoch: 1 [44460/49669]\tLoss: 108628.3047\n",
      "Training Epoch: 1 [44480/49669]\tLoss: 118713.1406\n",
      "Training Epoch: 1 [44500/49669]\tLoss: 130616.0938\n",
      "Training Epoch: 1 [44520/49669]\tLoss: 117521.1172\n",
      "Training Epoch: 1 [44540/49669]\tLoss: 109175.9297\n",
      "Training Epoch: 1 [44560/49669]\tLoss: 115449.7500\n",
      "Training Epoch: 1 [44580/49669]\tLoss: 125753.4922\n",
      "Training Epoch: 1 [44600/49669]\tLoss: 120333.7969\n",
      "Training Epoch: 1 [44620/49669]\tLoss: 106977.1172\n",
      "Training Epoch: 1 [44640/49669]\tLoss: 105273.5625\n",
      "Training Epoch: 1 [44660/49669]\tLoss: 120121.4375\n",
      "Training Epoch: 1 [44680/49669]\tLoss: 108391.4688\n",
      "Training Epoch: 1 [44700/49669]\tLoss: 114282.2812\n",
      "Training Epoch: 1 [44720/49669]\tLoss: 118353.8906\n",
      "Training Epoch: 1 [44740/49669]\tLoss: 100148.8672\n",
      "Training Epoch: 1 [44760/49669]\tLoss: 113362.5547\n",
      "Training Epoch: 1 [44780/49669]\tLoss: 116032.0625\n",
      "Training Epoch: 1 [44800/49669]\tLoss: 105736.5234\n",
      "Training Epoch: 1 [44820/49669]\tLoss: 123599.9688\n",
      "Training Epoch: 1 [44840/49669]\tLoss: 126081.4609\n",
      "Training Epoch: 1 [44860/49669]\tLoss: 117686.4219\n",
      "Training Epoch: 1 [44880/49669]\tLoss: 97292.5000\n",
      "Training Epoch: 1 [44900/49669]\tLoss: 88964.2266\n",
      "Training Epoch: 1 [44920/49669]\tLoss: 116480.9922\n",
      "Training Epoch: 1 [44940/49669]\tLoss: 101848.7969\n",
      "Training Epoch: 1 [44960/49669]\tLoss: 107625.0469\n",
      "Training Epoch: 1 [44980/49669]\tLoss: 107944.2344\n",
      "Training Epoch: 1 [45000/49669]\tLoss: 114016.2500\n",
      "Training Epoch: 1 [45020/49669]\tLoss: 110386.1875\n",
      "Training Epoch: 1 [45040/49669]\tLoss: 91891.4531\n",
      "Training Epoch: 1 [45060/49669]\tLoss: 123292.5703\n",
      "Training Epoch: 1 [45080/49669]\tLoss: 120644.9688\n",
      "Training Epoch: 1 [45100/49669]\tLoss: 103178.9141\n",
      "Training Epoch: 1 [45120/49669]\tLoss: 108313.7188\n",
      "Training Epoch: 1 [45140/49669]\tLoss: 107978.5078\n",
      "Training Epoch: 1 [45160/49669]\tLoss: 111145.5781\n",
      "Training Epoch: 1 [45180/49669]\tLoss: 110235.3672\n",
      "Training Epoch: 1 [45200/49669]\tLoss: 126394.1172\n",
      "Training Epoch: 1 [45220/49669]\tLoss: 103939.5156\n",
      "Training Epoch: 1 [45240/49669]\tLoss: 98250.3125\n",
      "Training Epoch: 1 [45260/49669]\tLoss: 115658.5391\n",
      "Training Epoch: 1 [45280/49669]\tLoss: 108295.9141\n",
      "Training Epoch: 1 [45300/49669]\tLoss: 118985.2734\n",
      "Training Epoch: 1 [45320/49669]\tLoss: 103101.6016\n",
      "Training Epoch: 1 [45340/49669]\tLoss: 103702.3047\n",
      "Training Epoch: 1 [45360/49669]\tLoss: 104349.1797\n",
      "Training Epoch: 1 [45380/49669]\tLoss: 122090.2891\n",
      "Training Epoch: 1 [45400/49669]\tLoss: 111825.4922\n",
      "Training Epoch: 1 [45420/49669]\tLoss: 111448.6094\n",
      "Training Epoch: 1 [45440/49669]\tLoss: 99522.8047\n",
      "Training Epoch: 1 [45460/49669]\tLoss: 94873.4531\n",
      "Training Epoch: 1 [45480/49669]\tLoss: 97808.7812\n",
      "Training Epoch: 1 [45500/49669]\tLoss: 109439.8359\n",
      "Training Epoch: 1 [45520/49669]\tLoss: 106591.9531\n",
      "Training Epoch: 1 [45540/49669]\tLoss: 103925.3047\n",
      "Training Epoch: 1 [45560/49669]\tLoss: 103009.3750\n",
      "Training Epoch: 1 [45580/49669]\tLoss: 98458.9297\n",
      "Training Epoch: 1 [45600/49669]\tLoss: 107389.0234\n",
      "Training Epoch: 1 [45620/49669]\tLoss: 95869.7656\n",
      "Training Epoch: 1 [45640/49669]\tLoss: 104548.7656\n",
      "Training Epoch: 1 [45660/49669]\tLoss: 116864.0703\n",
      "Training Epoch: 1 [45680/49669]\tLoss: 107377.2344\n",
      "Training Epoch: 1 [45700/49669]\tLoss: 112138.3984\n",
      "Training Epoch: 1 [45720/49669]\tLoss: 116010.9766\n",
      "Training Epoch: 1 [45740/49669]\tLoss: 103976.5312\n",
      "Training Epoch: 1 [45760/49669]\tLoss: 105738.3516\n",
      "Training Epoch: 1 [45780/49669]\tLoss: 118349.8203\n",
      "Training Epoch: 1 [45800/49669]\tLoss: 93989.1719\n",
      "Training Epoch: 1 [45820/49669]\tLoss: 100263.7422\n",
      "Training Epoch: 1 [45840/49669]\tLoss: 109115.4531\n",
      "Training Epoch: 1 [45860/49669]\tLoss: 112046.4766\n",
      "Training Epoch: 1 [45880/49669]\tLoss: 102268.7812\n",
      "Training Epoch: 1 [45900/49669]\tLoss: 97620.6797\n",
      "Training Epoch: 1 [45920/49669]\tLoss: 113550.4297\n",
      "Training Epoch: 1 [45940/49669]\tLoss: 103233.9844\n",
      "Training Epoch: 1 [45960/49669]\tLoss: 107879.7422\n",
      "Training Epoch: 1 [45980/49669]\tLoss: 94567.3594\n",
      "Training Epoch: 1 [46000/49669]\tLoss: 100222.3516\n",
      "Training Epoch: 1 [46020/49669]\tLoss: 97718.6719\n",
      "Training Epoch: 1 [46040/49669]\tLoss: 118563.1406\n",
      "Training Epoch: 1 [46060/49669]\tLoss: 107199.8828\n",
      "Training Epoch: 1 [46080/49669]\tLoss: 97532.9609\n",
      "Training Epoch: 1 [46100/49669]\tLoss: 107926.1484\n",
      "Training Epoch: 1 [46120/49669]\tLoss: 98005.1797\n",
      "Training Epoch: 1 [46140/49669]\tLoss: 97417.7891\n",
      "Training Epoch: 1 [46160/49669]\tLoss: 110111.0156\n",
      "Training Epoch: 1 [46180/49669]\tLoss: 113935.7578\n",
      "Training Epoch: 1 [46200/49669]\tLoss: 85363.6953\n",
      "Training Epoch: 1 [46220/49669]\tLoss: 114223.3750\n",
      "Training Epoch: 1 [46240/49669]\tLoss: 108449.7656\n",
      "Training Epoch: 1 [46260/49669]\tLoss: 98088.3438\n",
      "Training Epoch: 1 [46280/49669]\tLoss: 105240.3750\n",
      "Training Epoch: 1 [46300/49669]\tLoss: 83214.6875\n",
      "Training Epoch: 1 [46320/49669]\tLoss: 100695.1016\n",
      "Training Epoch: 1 [46340/49669]\tLoss: 107795.3516\n",
      "Training Epoch: 1 [46360/49669]\tLoss: 100800.2500\n",
      "Training Epoch: 1 [46380/49669]\tLoss: 113033.7891\n",
      "Training Epoch: 1 [46400/49669]\tLoss: 106805.9531\n",
      "Training Epoch: 1 [46420/49669]\tLoss: 98664.4141\n",
      "Training Epoch: 1 [46440/49669]\tLoss: 101077.4609\n",
      "Training Epoch: 1 [46460/49669]\tLoss: 98665.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [46480/49669]\tLoss: 110325.3047\n",
      "Training Epoch: 1 [46500/49669]\tLoss: 115688.2109\n",
      "Training Epoch: 1 [46520/49669]\tLoss: 109694.6953\n",
      "Training Epoch: 1 [46540/49669]\tLoss: 104858.0859\n",
      "Training Epoch: 1 [46560/49669]\tLoss: 109067.6641\n",
      "Training Epoch: 1 [46580/49669]\tLoss: 108738.4062\n",
      "Training Epoch: 1 [46600/49669]\tLoss: 95908.0469\n",
      "Training Epoch: 1 [46620/49669]\tLoss: 109478.2500\n",
      "Training Epoch: 1 [46640/49669]\tLoss: 88936.6328\n",
      "Training Epoch: 1 [46660/49669]\tLoss: 93531.5547\n",
      "Training Epoch: 1 [46680/49669]\tLoss: 97477.0703\n",
      "Training Epoch: 1 [46700/49669]\tLoss: 108296.0391\n",
      "Training Epoch: 1 [46720/49669]\tLoss: 110272.6094\n",
      "Training Epoch: 1 [46740/49669]\tLoss: 93571.1094\n",
      "Training Epoch: 1 [46760/49669]\tLoss: 117340.0312\n",
      "Training Epoch: 1 [46780/49669]\tLoss: 99044.5078\n",
      "Training Epoch: 1 [46800/49669]\tLoss: 89861.7266\n",
      "Training Epoch: 1 [46820/49669]\tLoss: 97298.3281\n",
      "Training Epoch: 1 [46840/49669]\tLoss: 95691.1328\n",
      "Training Epoch: 1 [46860/49669]\tLoss: 98385.4141\n",
      "Training Epoch: 1 [46880/49669]\tLoss: 96208.0781\n",
      "Training Epoch: 1 [46900/49669]\tLoss: 121891.2344\n",
      "Training Epoch: 1 [46920/49669]\tLoss: 112580.8281\n",
      "Training Epoch: 1 [46940/49669]\tLoss: 97303.4453\n",
      "Training Epoch: 1 [46960/49669]\tLoss: 94677.2344\n",
      "Training Epoch: 1 [46980/49669]\tLoss: 123731.2812\n",
      "Training Epoch: 1 [47000/49669]\tLoss: 105100.7344\n",
      "Training Epoch: 1 [47020/49669]\tLoss: 92258.5000\n",
      "Training Epoch: 1 [47040/49669]\tLoss: 99927.8828\n",
      "Training Epoch: 1 [47060/49669]\tLoss: 100361.1172\n",
      "Training Epoch: 1 [47080/49669]\tLoss: 92188.7969\n",
      "Training Epoch: 1 [47100/49669]\tLoss: 95071.9297\n",
      "Training Epoch: 1 [47120/49669]\tLoss: 112288.0938\n",
      "Training Epoch: 1 [47140/49669]\tLoss: 112346.5938\n",
      "Training Epoch: 1 [47160/49669]\tLoss: 101203.3125\n",
      "Training Epoch: 1 [47180/49669]\tLoss: 100503.1719\n",
      "Training Epoch: 1 [47200/49669]\tLoss: 94757.3203\n",
      "Training Epoch: 1 [47220/49669]\tLoss: 99889.0703\n",
      "Training Epoch: 1 [47240/49669]\tLoss: 81613.9453\n",
      "Training Epoch: 1 [47260/49669]\tLoss: 97004.6172\n",
      "Training Epoch: 1 [47280/49669]\tLoss: 99948.2500\n",
      "Training Epoch: 1 [47300/49669]\tLoss: 98996.7109\n",
      "Training Epoch: 1 [47320/49669]\tLoss: 94319.2031\n",
      "Training Epoch: 1 [47340/49669]\tLoss: 105248.9141\n",
      "Training Epoch: 1 [47360/49669]\tLoss: 101647.7656\n",
      "Training Epoch: 1 [47380/49669]\tLoss: 97703.2109\n",
      "Training Epoch: 1 [47400/49669]\tLoss: 91191.6953\n",
      "Training Epoch: 1 [47420/49669]\tLoss: 87464.9453\n",
      "Training Epoch: 1 [47440/49669]\tLoss: 109137.8516\n",
      "Training Epoch: 1 [47460/49669]\tLoss: 94757.6641\n",
      "Training Epoch: 1 [47480/49669]\tLoss: 86937.5000\n",
      "Training Epoch: 1 [47500/49669]\tLoss: 85025.8906\n",
      "Training Epoch: 1 [47520/49669]\tLoss: 87145.2344\n",
      "Training Epoch: 1 [47540/49669]\tLoss: 84399.4297\n",
      "Training Epoch: 1 [47560/49669]\tLoss: 87078.4219\n",
      "Training Epoch: 1 [47580/49669]\tLoss: 93159.3750\n",
      "Training Epoch: 1 [47600/49669]\tLoss: 100693.7500\n",
      "Training Epoch: 1 [47620/49669]\tLoss: 90650.2734\n",
      "Training Epoch: 1 [47640/49669]\tLoss: 98073.2266\n",
      "Training Epoch: 1 [47660/49669]\tLoss: 100170.8047\n",
      "Training Epoch: 1 [47680/49669]\tLoss: 81664.0234\n",
      "Training Epoch: 1 [47700/49669]\tLoss: 93558.7344\n",
      "Training Epoch: 1 [47720/49669]\tLoss: 102364.5156\n",
      "Training Epoch: 1 [47740/49669]\tLoss: 86155.6094\n",
      "Training Epoch: 1 [47760/49669]\tLoss: 102697.9219\n",
      "Training Epoch: 1 [47780/49669]\tLoss: 83561.1484\n",
      "Training Epoch: 1 [47800/49669]\tLoss: 86726.4766\n",
      "Training Epoch: 1 [47820/49669]\tLoss: 88353.2656\n",
      "Training Epoch: 1 [47840/49669]\tLoss: 97160.2734\n",
      "Training Epoch: 1 [47860/49669]\tLoss: 104533.3125\n",
      "Training Epoch: 1 [47880/49669]\tLoss: 107683.7656\n",
      "Training Epoch: 1 [47900/49669]\tLoss: 84590.7656\n",
      "Training Epoch: 1 [47920/49669]\tLoss: 82116.3203\n",
      "Training Epoch: 1 [47940/49669]\tLoss: 94445.5547\n",
      "Training Epoch: 1 [47960/49669]\tLoss: 93157.3594\n",
      "Training Epoch: 1 [47980/49669]\tLoss: 97833.2422\n",
      "Training Epoch: 1 [48000/49669]\tLoss: 76005.3516\n",
      "Training Epoch: 1 [48020/49669]\tLoss: 101477.7891\n",
      "Training Epoch: 1 [48040/49669]\tLoss: 78303.9219\n",
      "Training Epoch: 1 [48060/49669]\tLoss: 93286.9062\n",
      "Training Epoch: 1 [48080/49669]\tLoss: 75616.2188\n",
      "Training Epoch: 1 [48100/49669]\tLoss: 84134.7812\n",
      "Training Epoch: 1 [48120/49669]\tLoss: 80013.7500\n",
      "Training Epoch: 1 [48140/49669]\tLoss: 91133.6641\n",
      "Training Epoch: 1 [48160/49669]\tLoss: 103809.1172\n",
      "Training Epoch: 1 [48180/49669]\tLoss: 85575.2812\n",
      "Training Epoch: 1 [48200/49669]\tLoss: 82302.7500\n",
      "Training Epoch: 1 [48220/49669]\tLoss: 89300.9922\n",
      "Training Epoch: 1 [48240/49669]\tLoss: 88099.8594\n",
      "Training Epoch: 1 [48260/49669]\tLoss: 82538.3672\n",
      "Training Epoch: 1 [48280/49669]\tLoss: 84782.9609\n",
      "Training Epoch: 1 [48300/49669]\tLoss: 87350.6172\n",
      "Training Epoch: 1 [48320/49669]\tLoss: 92462.8359\n",
      "Training Epoch: 1 [48340/49669]\tLoss: 100558.2578\n",
      "Training Epoch: 1 [48360/49669]\tLoss: 85987.5000\n",
      "Training Epoch: 1 [48380/49669]\tLoss: 86339.0781\n",
      "Training Epoch: 1 [48400/49669]\tLoss: 93410.0859\n",
      "Training Epoch: 1 [48420/49669]\tLoss: 87016.8984\n",
      "Training Epoch: 1 [48440/49669]\tLoss: 97035.8125\n",
      "Training Epoch: 1 [48460/49669]\tLoss: 81984.1250\n",
      "Training Epoch: 1 [48480/49669]\tLoss: 93939.0000\n",
      "Training Epoch: 1 [48500/49669]\tLoss: 81243.3750\n",
      "Training Epoch: 1 [48520/49669]\tLoss: 84762.4688\n",
      "Training Epoch: 1 [48540/49669]\tLoss: 86660.6875\n",
      "Training Epoch: 1 [48560/49669]\tLoss: 91489.2891\n",
      "Training Epoch: 1 [48580/49669]\tLoss: 86220.3203\n",
      "Training Epoch: 1 [48600/49669]\tLoss: 91574.7656\n",
      "Training Epoch: 1 [48620/49669]\tLoss: 103307.0938\n",
      "Training Epoch: 1 [48640/49669]\tLoss: 97279.1016\n",
      "Training Epoch: 1 [48660/49669]\tLoss: 84207.8906\n",
      "Training Epoch: 1 [48680/49669]\tLoss: 81795.7656\n",
      "Training Epoch: 1 [48700/49669]\tLoss: 82939.9531\n",
      "Training Epoch: 1 [48720/49669]\tLoss: 103851.3047\n",
      "Training Epoch: 1 [48740/49669]\tLoss: 82368.0625\n",
      "Training Epoch: 1 [48760/49669]\tLoss: 106302.3125\n",
      "Training Epoch: 1 [48780/49669]\tLoss: 93848.0703\n",
      "Training Epoch: 1 [48800/49669]\tLoss: 95835.2188\n",
      "Training Epoch: 1 [48820/49669]\tLoss: 83936.2188\n",
      "Training Epoch: 1 [48840/49669]\tLoss: 88966.8438\n",
      "Training Epoch: 1 [48860/49669]\tLoss: 91824.9375\n",
      "Training Epoch: 1 [48880/49669]\tLoss: 85354.5703\n",
      "Training Epoch: 1 [48900/49669]\tLoss: 94867.3125\n",
      "Training Epoch: 1 [48920/49669]\tLoss: 96343.8516\n",
      "Training Epoch: 1 [48940/49669]\tLoss: 99819.4453\n",
      "Training Epoch: 1 [48960/49669]\tLoss: 96411.6562\n",
      "Training Epoch: 1 [48980/49669]\tLoss: 105401.7188\n",
      "Training Epoch: 1 [49000/49669]\tLoss: 83923.0156\n",
      "Training Epoch: 1 [49020/49669]\tLoss: 93389.6250\n",
      "Training Epoch: 1 [49040/49669]\tLoss: 87022.5234\n",
      "Training Epoch: 1 [49060/49669]\tLoss: 82504.2734\n",
      "Training Epoch: 1 [49080/49669]\tLoss: 88865.8984\n",
      "Training Epoch: 1 [49100/49669]\tLoss: 84003.4609\n",
      "Training Epoch: 1 [49120/49669]\tLoss: 91841.8672\n",
      "Training Epoch: 1 [49140/49669]\tLoss: 85933.4141\n",
      "Training Epoch: 1 [49160/49669]\tLoss: 90861.4141\n",
      "Training Epoch: 1 [49180/49669]\tLoss: 81211.9766\n",
      "Training Epoch: 1 [49200/49669]\tLoss: 81910.6328\n",
      "Training Epoch: 1 [49220/49669]\tLoss: 88303.9141\n",
      "Training Epoch: 1 [49240/49669]\tLoss: 93990.3594\n",
      "Training Epoch: 1 [49260/49669]\tLoss: 75054.2734\n",
      "Training Epoch: 1 [49280/49669]\tLoss: 98389.6016\n",
      "Training Epoch: 1 [49300/49669]\tLoss: 87770.2812\n",
      "Training Epoch: 1 [49320/49669]\tLoss: 87783.3438\n",
      "Training Epoch: 1 [49340/49669]\tLoss: 81555.7422\n",
      "Training Epoch: 1 [49360/49669]\tLoss: 75575.5547\n",
      "Training Epoch: 1 [49380/49669]\tLoss: 82606.2969\n",
      "Training Epoch: 1 [49400/49669]\tLoss: 76428.7578\n",
      "Training Epoch: 1 [49420/49669]\tLoss: 84294.9688\n",
      "Training Epoch: 1 [49440/49669]\tLoss: 79800.8516\n",
      "Training Epoch: 1 [49460/49669]\tLoss: 95571.1797\n",
      "Training Epoch: 1 [49480/49669]\tLoss: 82496.9375\n",
      "Training Epoch: 1 [49500/49669]\tLoss: 81796.1406\n",
      "Training Epoch: 1 [49520/49669]\tLoss: 76254.2656\n",
      "Training Epoch: 1 [49540/49669]\tLoss: 85789.6797\n",
      "Training Epoch: 1 [49560/49669]\tLoss: 90879.6250\n",
      "Training Epoch: 1 [49580/49669]\tLoss: 75549.8047\n",
      "Training Epoch: 1 [49600/49669]\tLoss: 75945.4219\n",
      "Training Epoch: 1 [49620/49669]\tLoss: 81967.3750\n",
      "Training Epoch: 1 [49640/49669]\tLoss: 78386.4531\n",
      "Training Epoch: 1 [49660/49669]\tLoss: 77095.3203\n",
      "Training Epoch: 1 [49669/49669]\tLoss: 88346.0391\n",
      "Training Epoch: 1 [5519/5519]\tLoss: 84326.3768\n",
      "Training Epoch: 2 [20/49669]\tLoss: 89447.7969\n",
      "Training Epoch: 2 [40/49669]\tLoss: 99009.3984\n",
      "Training Epoch: 2 [60/49669]\tLoss: 86478.4609\n",
      "Training Epoch: 2 [80/49669]\tLoss: 83430.0547\n",
      "Training Epoch: 2 [100/49669]\tLoss: 72466.2891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [120/49669]\tLoss: 80408.9141\n",
      "Training Epoch: 2 [140/49669]\tLoss: 71964.3672\n",
      "Training Epoch: 2 [160/49669]\tLoss: 73276.6016\n",
      "Training Epoch: 2 [180/49669]\tLoss: 84162.2266\n",
      "Training Epoch: 2 [200/49669]\tLoss: 73523.3672\n",
      "Training Epoch: 2 [220/49669]\tLoss: 87681.7578\n",
      "Training Epoch: 2 [240/49669]\tLoss: 89699.5156\n",
      "Training Epoch: 2 [260/49669]\tLoss: 90258.8438\n",
      "Training Epoch: 2 [280/49669]\tLoss: 84412.4141\n",
      "Training Epoch: 2 [300/49669]\tLoss: 89091.3438\n",
      "Training Epoch: 2 [320/49669]\tLoss: 89465.2891\n",
      "Training Epoch: 2 [340/49669]\tLoss: 77146.9688\n",
      "Training Epoch: 2 [360/49669]\tLoss: 76111.5078\n",
      "Training Epoch: 2 [380/49669]\tLoss: 77802.7969\n",
      "Training Epoch: 2 [400/49669]\tLoss: 88192.1172\n",
      "Training Epoch: 2 [420/49669]\tLoss: 93332.8125\n",
      "Training Epoch: 2 [440/49669]\tLoss: 87192.8828\n",
      "Training Epoch: 2 [460/49669]\tLoss: 87511.1406\n",
      "Training Epoch: 2 [480/49669]\tLoss: 87379.0859\n",
      "Training Epoch: 2 [500/49669]\tLoss: 78705.1953\n",
      "Training Epoch: 2 [520/49669]\tLoss: 83850.5859\n",
      "Training Epoch: 2 [540/49669]\tLoss: 84149.5703\n",
      "Training Epoch: 2 [560/49669]\tLoss: 83273.2734\n",
      "Training Epoch: 2 [580/49669]\tLoss: 85206.9766\n",
      "Training Epoch: 2 [600/49669]\tLoss: 82663.9062\n",
      "Training Epoch: 2 [620/49669]\tLoss: 70422.0000\n",
      "Training Epoch: 2 [640/49669]\tLoss: 84559.5156\n",
      "Training Epoch: 2 [660/49669]\tLoss: 77435.7031\n",
      "Training Epoch: 2 [680/49669]\tLoss: 83639.5469\n",
      "Training Epoch: 2 [700/49669]\tLoss: 84487.3516\n",
      "Training Epoch: 2 [720/49669]\tLoss: 78949.9141\n",
      "Training Epoch: 2 [740/49669]\tLoss: 83375.6797\n",
      "Training Epoch: 2 [760/49669]\tLoss: 77766.9219\n",
      "Training Epoch: 2 [780/49669]\tLoss: 76294.2422\n",
      "Training Epoch: 2 [800/49669]\tLoss: 90490.9453\n",
      "Training Epoch: 2 [820/49669]\tLoss: 69529.2188\n",
      "Training Epoch: 2 [840/49669]\tLoss: 88140.6562\n",
      "Training Epoch: 2 [860/49669]\tLoss: 89442.3906\n",
      "Training Epoch: 2 [880/49669]\tLoss: 74276.8906\n",
      "Training Epoch: 2 [900/49669]\tLoss: 79371.6875\n",
      "Training Epoch: 2 [920/49669]\tLoss: 80245.2969\n",
      "Training Epoch: 2 [940/49669]\tLoss: 85819.4609\n",
      "Training Epoch: 2 [960/49669]\tLoss: 91003.1953\n",
      "Training Epoch: 2 [980/49669]\tLoss: 80705.2812\n",
      "Training Epoch: 2 [1000/49669]\tLoss: 76541.2734\n",
      "Training Epoch: 2 [1020/49669]\tLoss: 70057.8047\n",
      "Training Epoch: 2 [1040/49669]\tLoss: 75615.0234\n",
      "Training Epoch: 2 [1060/49669]\tLoss: 92322.5000\n",
      "Training Epoch: 2 [1080/49669]\tLoss: 77273.7812\n",
      "Training Epoch: 2 [1100/49669]\tLoss: 73794.3516\n",
      "Training Epoch: 2 [1120/49669]\tLoss: 79170.9141\n",
      "Training Epoch: 2 [1140/49669]\tLoss: 77515.3125\n",
      "Training Epoch: 2 [1160/49669]\tLoss: 77175.9766\n",
      "Training Epoch: 2 [1180/49669]\tLoss: 71426.1719\n",
      "Training Epoch: 2 [1200/49669]\tLoss: 84287.7031\n",
      "Training Epoch: 2 [1220/49669]\tLoss: 79589.0781\n",
      "Training Epoch: 2 [1240/49669]\tLoss: 79089.9766\n",
      "Training Epoch: 2 [1260/49669]\tLoss: 87087.4297\n",
      "Training Epoch: 2 [1280/49669]\tLoss: 79971.9141\n",
      "Training Epoch: 2 [1300/49669]\tLoss: 71360.3516\n",
      "Training Epoch: 2 [1320/49669]\tLoss: 79561.9141\n",
      "Training Epoch: 2 [1340/49669]\tLoss: 64825.6797\n",
      "Training Epoch: 2 [1360/49669]\tLoss: 74698.6719\n",
      "Training Epoch: 2 [1380/49669]\tLoss: 79772.4297\n",
      "Training Epoch: 2 [1400/49669]\tLoss: 68290.5547\n",
      "Training Epoch: 2 [1420/49669]\tLoss: 81224.7266\n",
      "Training Epoch: 2 [1440/49669]\tLoss: 77917.7969\n",
      "Training Epoch: 2 [1460/49669]\tLoss: 82485.8594\n",
      "Training Epoch: 2 [1480/49669]\tLoss: 83373.5781\n",
      "Training Epoch: 2 [1500/49669]\tLoss: 74704.1250\n",
      "Training Epoch: 2 [1520/49669]\tLoss: 74687.1953\n",
      "Training Epoch: 2 [1540/49669]\tLoss: 70509.6719\n",
      "Training Epoch: 2 [1560/49669]\tLoss: 82618.3047\n",
      "Training Epoch: 2 [1580/49669]\tLoss: 72693.8828\n",
      "Training Epoch: 2 [1600/49669]\tLoss: 77637.5156\n",
      "Training Epoch: 2 [1620/49669]\tLoss: 71745.3125\n",
      "Training Epoch: 2 [1640/49669]\tLoss: 75737.8828\n",
      "Training Epoch: 2 [1660/49669]\tLoss: 75738.7734\n",
      "Training Epoch: 2 [1680/49669]\tLoss: 73204.6719\n",
      "Training Epoch: 2 [1700/49669]\tLoss: 80262.5391\n",
      "Training Epoch: 2 [1720/49669]\tLoss: 69428.6406\n",
      "Training Epoch: 2 [1740/49669]\tLoss: 95418.3750\n",
      "Training Epoch: 2 [1760/49669]\tLoss: 84731.6797\n",
      "Training Epoch: 2 [1780/49669]\tLoss: 72505.6875\n",
      "Training Epoch: 2 [1800/49669]\tLoss: 83864.6328\n",
      "Training Epoch: 2 [1820/49669]\tLoss: 77155.0859\n",
      "Training Epoch: 2 [1840/49669]\tLoss: 73173.1797\n",
      "Training Epoch: 2 [1860/49669]\tLoss: 68053.0938\n",
      "Training Epoch: 2 [1880/49669]\tLoss: 79403.6797\n",
      "Training Epoch: 2 [1900/49669]\tLoss: 67845.4062\n",
      "Training Epoch: 2 [1920/49669]\tLoss: 60568.7227\n",
      "Training Epoch: 2 [1940/49669]\tLoss: 77282.4531\n",
      "Training Epoch: 2 [1960/49669]\tLoss: 80208.8047\n",
      "Training Epoch: 2 [1980/49669]\tLoss: 70780.9297\n",
      "Training Epoch: 2 [2000/49669]\tLoss: 77789.0469\n",
      "Training Epoch: 2 [2020/49669]\tLoss: 62954.7734\n",
      "Training Epoch: 2 [2040/49669]\tLoss: 80247.0234\n",
      "Training Epoch: 2 [2060/49669]\tLoss: 73422.4844\n",
      "Training Epoch: 2 [2080/49669]\tLoss: 55653.2539\n",
      "Training Epoch: 2 [2100/49669]\tLoss: 71170.3828\n",
      "Training Epoch: 2 [2120/49669]\tLoss: 73804.3750\n",
      "Training Epoch: 2 [2140/49669]\tLoss: 86531.1875\n",
      "Training Epoch: 2 [2160/49669]\tLoss: 78533.3594\n",
      "Training Epoch: 2 [2180/49669]\tLoss: 84646.2109\n",
      "Training Epoch: 2 [2200/49669]\tLoss: 72512.0234\n",
      "Training Epoch: 2 [2220/49669]\tLoss: 86764.7812\n",
      "Training Epoch: 2 [2240/49669]\tLoss: 85788.3125\n",
      "Training Epoch: 2 [2260/49669]\tLoss: 79448.6406\n",
      "Training Epoch: 2 [2280/49669]\tLoss: 68709.0000\n",
      "Training Epoch: 2 [2300/49669]\tLoss: 78616.8516\n",
      "Training Epoch: 2 [2320/49669]\tLoss: 79493.0469\n",
      "Training Epoch: 2 [2340/49669]\tLoss: 78118.1953\n",
      "Training Epoch: 2 [2360/49669]\tLoss: 73089.1719\n",
      "Training Epoch: 2 [2380/49669]\tLoss: 75031.2500\n",
      "Training Epoch: 2 [2400/49669]\tLoss: 78424.8828\n",
      "Training Epoch: 2 [2420/49669]\tLoss: 76043.2344\n",
      "Training Epoch: 2 [2440/49669]\tLoss: 86131.7812\n",
      "Training Epoch: 2 [2460/49669]\tLoss: 63891.9297\n",
      "Training Epoch: 2 [2480/49669]\tLoss: 78738.6641\n",
      "Training Epoch: 2 [2500/49669]\tLoss: 75379.2578\n",
      "Training Epoch: 2 [2520/49669]\tLoss: 47529.3867\n",
      "Training Epoch: 2 [2540/49669]\tLoss: 59317.2461\n",
      "Training Epoch: 2 [2560/49669]\tLoss: 75650.8359\n",
      "Training Epoch: 2 [2580/49669]\tLoss: 73685.9219\n",
      "Training Epoch: 2 [2600/49669]\tLoss: 77570.0938\n",
      "Training Epoch: 2 [2620/49669]\tLoss: 59864.1484\n",
      "Training Epoch: 2 [2640/49669]\tLoss: 78070.4922\n",
      "Training Epoch: 2 [2660/49669]\tLoss: 63983.3945\n",
      "Training Epoch: 2 [2680/49669]\tLoss: 70754.8047\n",
      "Training Epoch: 2 [2700/49669]\tLoss: 73295.6484\n",
      "Training Epoch: 2 [2720/49669]\tLoss: 78963.8359\n",
      "Training Epoch: 2 [2740/49669]\tLoss: 81848.8594\n",
      "Training Epoch: 2 [2760/49669]\tLoss: 65783.2969\n",
      "Training Epoch: 2 [2780/49669]\tLoss: 70818.0625\n",
      "Training Epoch: 2 [2800/49669]\tLoss: 74661.8828\n",
      "Training Epoch: 2 [2820/49669]\tLoss: 67883.9375\n",
      "Training Epoch: 2 [2840/49669]\tLoss: 60971.3242\n",
      "Training Epoch: 2 [2860/49669]\tLoss: 69680.8594\n",
      "Training Epoch: 2 [2880/49669]\tLoss: 73831.6562\n",
      "Training Epoch: 2 [2900/49669]\tLoss: 82659.8750\n",
      "Training Epoch: 2 [2920/49669]\tLoss: 67930.2578\n",
      "Training Epoch: 2 [2940/49669]\tLoss: 81467.2734\n",
      "Training Epoch: 2 [2960/49669]\tLoss: 73009.5625\n",
      "Training Epoch: 2 [2980/49669]\tLoss: 56467.9297\n",
      "Training Epoch: 2 [3000/49669]\tLoss: 86571.9844\n",
      "Training Epoch: 2 [3020/49669]\tLoss: 62769.2266\n",
      "Training Epoch: 2 [3040/49669]\tLoss: 68757.2266\n",
      "Training Epoch: 2 [3060/49669]\tLoss: 75444.9922\n",
      "Training Epoch: 2 [3080/49669]\tLoss: 73971.8359\n",
      "Training Epoch: 2 [3100/49669]\tLoss: 66737.6562\n",
      "Training Epoch: 2 [3120/49669]\tLoss: 70236.6719\n",
      "Training Epoch: 2 [3140/49669]\tLoss: 78169.2734\n",
      "Training Epoch: 2 [3160/49669]\tLoss: 76240.5312\n",
      "Training Epoch: 2 [3180/49669]\tLoss: 61581.2930\n",
      "Training Epoch: 2 [3200/49669]\tLoss: 82699.5000\n",
      "Training Epoch: 2 [3220/49669]\tLoss: 65336.1055\n",
      "Training Epoch: 2 [3240/49669]\tLoss: 73936.0312\n",
      "Training Epoch: 2 [3260/49669]\tLoss: 87463.4688\n",
      "Training Epoch: 2 [3280/49669]\tLoss: 74454.6016\n",
      "Training Epoch: 2 [3300/49669]\tLoss: 82363.4609\n",
      "Training Epoch: 2 [3320/49669]\tLoss: 67570.9609\n",
      "Training Epoch: 2 [3340/49669]\tLoss: 72820.2734\n",
      "Training Epoch: 2 [3360/49669]\tLoss: 69254.0625\n",
      "Training Epoch: 2 [3380/49669]\tLoss: 69194.1250\n",
      "Training Epoch: 2 [3400/49669]\tLoss: 70869.1250\n",
      "Training Epoch: 2 [3420/49669]\tLoss: 61603.8086\n",
      "Training Epoch: 2 [3440/49669]\tLoss: 74434.0391\n",
      "Training Epoch: 2 [3460/49669]\tLoss: 69623.4531\n",
      "Training Epoch: 2 [3480/49669]\tLoss: 66295.5469\n",
      "Training Epoch: 2 [3500/49669]\tLoss: 70033.8906\n",
      "Training Epoch: 2 [3520/49669]\tLoss: 66562.2422\n",
      "Training Epoch: 2 [3540/49669]\tLoss: 78997.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [3560/49669]\tLoss: 66282.8594\n",
      "Training Epoch: 2 [3580/49669]\tLoss: 75377.9922\n",
      "Training Epoch: 2 [3600/49669]\tLoss: 55208.6719\n",
      "Training Epoch: 2 [3620/49669]\tLoss: 64698.3555\n",
      "Training Epoch: 2 [3640/49669]\tLoss: 73367.5156\n",
      "Training Epoch: 2 [3660/49669]\tLoss: 65640.2656\n",
      "Training Epoch: 2 [3680/49669]\tLoss: 66953.1797\n",
      "Training Epoch: 2 [3700/49669]\tLoss: 69939.4922\n",
      "Training Epoch: 2 [3720/49669]\tLoss: 75961.4531\n",
      "Training Epoch: 2 [3740/49669]\tLoss: 74179.1875\n",
      "Training Epoch: 2 [3760/49669]\tLoss: 77042.2969\n",
      "Training Epoch: 2 [3780/49669]\tLoss: 60806.3047\n",
      "Training Epoch: 2 [3800/49669]\tLoss: 63952.2188\n",
      "Training Epoch: 2 [3820/49669]\tLoss: 70337.0078\n",
      "Training Epoch: 2 [3840/49669]\tLoss: 62190.5547\n",
      "Training Epoch: 2 [3860/49669]\tLoss: 71696.3281\n",
      "Training Epoch: 2 [3880/49669]\tLoss: 74030.8594\n",
      "Training Epoch: 2 [3900/49669]\tLoss: 69302.0547\n",
      "Training Epoch: 2 [3920/49669]\tLoss: 69379.2812\n",
      "Training Epoch: 2 [3940/49669]\tLoss: 73849.1016\n",
      "Training Epoch: 2 [3960/49669]\tLoss: 71150.8750\n",
      "Training Epoch: 2 [3980/49669]\tLoss: 63375.6562\n",
      "Training Epoch: 2 [4000/49669]\tLoss: 72981.3125\n",
      "Training Epoch: 2 [4020/49669]\tLoss: 70966.5781\n",
      "Training Epoch: 2 [4040/49669]\tLoss: 68088.0078\n",
      "Training Epoch: 2 [4060/49669]\tLoss: 63088.6367\n",
      "Training Epoch: 2 [4080/49669]\tLoss: 74373.4922\n",
      "Training Epoch: 2 [4100/49669]\tLoss: 66725.8359\n",
      "Training Epoch: 2 [4120/49669]\tLoss: 69226.7031\n",
      "Training Epoch: 2 [4140/49669]\tLoss: 71285.9609\n",
      "Training Epoch: 2 [4160/49669]\tLoss: 63089.0859\n",
      "Training Epoch: 2 [4180/49669]\tLoss: 63316.6211\n",
      "Training Epoch: 2 [4200/49669]\tLoss: 51679.5742\n",
      "Training Epoch: 2 [4220/49669]\tLoss: 68828.8984\n",
      "Training Epoch: 2 [4240/49669]\tLoss: 73938.8906\n",
      "Training Epoch: 2 [4260/49669]\tLoss: 70590.6406\n",
      "Training Epoch: 2 [4280/49669]\tLoss: 61752.4570\n",
      "Training Epoch: 2 [4300/49669]\tLoss: 70709.0078\n",
      "Training Epoch: 2 [4320/49669]\tLoss: 65068.0234\n",
      "Training Epoch: 2 [4340/49669]\tLoss: 72398.6953\n",
      "Training Epoch: 2 [4360/49669]\tLoss: 64710.7656\n",
      "Training Epoch: 2 [4380/49669]\tLoss: 66554.8984\n",
      "Training Epoch: 2 [4400/49669]\tLoss: 67365.3203\n",
      "Training Epoch: 2 [4420/49669]\tLoss: 82403.6641\n",
      "Training Epoch: 2 [4440/49669]\tLoss: 65717.3047\n",
      "Training Epoch: 2 [4460/49669]\tLoss: 72112.7031\n",
      "Training Epoch: 2 [4480/49669]\tLoss: 61357.8516\n",
      "Training Epoch: 2 [4500/49669]\tLoss: 68509.4141\n",
      "Training Epoch: 2 [4520/49669]\tLoss: 62023.9297\n",
      "Training Epoch: 2 [4540/49669]\tLoss: 72040.2188\n",
      "Training Epoch: 2 [4560/49669]\tLoss: 63260.1758\n",
      "Training Epoch: 2 [4580/49669]\tLoss: 71969.3594\n",
      "Training Epoch: 2 [4600/49669]\tLoss: 59036.8125\n",
      "Training Epoch: 2 [4620/49669]\tLoss: 60047.4102\n",
      "Training Epoch: 2 [4640/49669]\tLoss: 68501.6172\n",
      "Training Epoch: 2 [4660/49669]\tLoss: 69691.5547\n",
      "Training Epoch: 2 [4680/49669]\tLoss: 72570.4375\n",
      "Training Epoch: 2 [4700/49669]\tLoss: 70229.4453\n",
      "Training Epoch: 2 [4720/49669]\tLoss: 53663.7539\n",
      "Training Epoch: 2 [4740/49669]\tLoss: 58477.7148\n",
      "Training Epoch: 2 [4760/49669]\tLoss: 61341.8438\n",
      "Training Epoch: 2 [4780/49669]\tLoss: 60616.1523\n",
      "Training Epoch: 2 [4800/49669]\tLoss: 75095.3750\n",
      "Training Epoch: 2 [4820/49669]\tLoss: 66733.7266\n",
      "Training Epoch: 2 [4840/49669]\tLoss: 58121.6094\n",
      "Training Epoch: 2 [4860/49669]\tLoss: 69523.9766\n",
      "Training Epoch: 2 [4880/49669]\tLoss: 65745.5547\n",
      "Training Epoch: 2 [4900/49669]\tLoss: 72697.6016\n",
      "Training Epoch: 2 [4920/49669]\tLoss: 67672.2344\n",
      "Training Epoch: 2 [4940/49669]\tLoss: 71252.1641\n",
      "Training Epoch: 2 [4960/49669]\tLoss: 62484.7266\n",
      "Training Epoch: 2 [4980/49669]\tLoss: 58913.9883\n",
      "Training Epoch: 2 [5000/49669]\tLoss: 63175.3281\n",
      "Training Epoch: 2 [5020/49669]\tLoss: 67042.8750\n",
      "Training Epoch: 2 [5040/49669]\tLoss: 70743.0703\n",
      "Training Epoch: 2 [5060/49669]\tLoss: 75585.5234\n",
      "Training Epoch: 2 [5080/49669]\tLoss: 63985.5820\n",
      "Training Epoch: 2 [5100/49669]\tLoss: 62259.6836\n",
      "Training Epoch: 2 [5120/49669]\tLoss: 73558.2500\n",
      "Training Epoch: 2 [5140/49669]\tLoss: 61082.0469\n",
      "Training Epoch: 2 [5160/49669]\tLoss: 64855.8750\n",
      "Training Epoch: 2 [5180/49669]\tLoss: 61637.1875\n",
      "Training Epoch: 2 [5200/49669]\tLoss: 66640.1875\n",
      "Training Epoch: 2 [5220/49669]\tLoss: 61886.9102\n",
      "Training Epoch: 2 [5240/49669]\tLoss: 61457.6562\n",
      "Training Epoch: 2 [5260/49669]\tLoss: 65021.6992\n",
      "Training Epoch: 2 [5280/49669]\tLoss: 56756.9336\n",
      "Training Epoch: 2 [5300/49669]\tLoss: 66641.9531\n",
      "Training Epoch: 2 [5320/49669]\tLoss: 66017.2891\n",
      "Training Epoch: 2 [5340/49669]\tLoss: 53844.3711\n",
      "Training Epoch: 2 [5360/49669]\tLoss: 73104.6719\n",
      "Training Epoch: 2 [5380/49669]\tLoss: 59727.6016\n",
      "Training Epoch: 2 [5400/49669]\tLoss: 62569.0703\n",
      "Training Epoch: 2 [5420/49669]\tLoss: 66983.1953\n",
      "Training Epoch: 2 [5440/49669]\tLoss: 70414.0312\n",
      "Training Epoch: 2 [5460/49669]\tLoss: 62566.9492\n",
      "Training Epoch: 2 [5480/49669]\tLoss: 61057.2539\n",
      "Training Epoch: 2 [5500/49669]\tLoss: 64928.8906\n",
      "Training Epoch: 2 [5520/49669]\tLoss: 60252.2344\n",
      "Training Epoch: 2 [5540/49669]\tLoss: 61596.2812\n",
      "Training Epoch: 2 [5560/49669]\tLoss: 54322.3867\n",
      "Training Epoch: 2 [5580/49669]\tLoss: 60719.3281\n",
      "Training Epoch: 2 [5600/49669]\tLoss: 59414.4023\n",
      "Training Epoch: 2 [5620/49669]\tLoss: 65369.2031\n",
      "Training Epoch: 2 [5640/49669]\tLoss: 65403.7344\n",
      "Training Epoch: 2 [5660/49669]\tLoss: 70261.3984\n",
      "Training Epoch: 2 [5680/49669]\tLoss: 65923.6875\n",
      "Training Epoch: 2 [5700/49669]\tLoss: 65786.3438\n",
      "Training Epoch: 2 [5720/49669]\tLoss: 72885.1484\n",
      "Training Epoch: 2 [5740/49669]\tLoss: 55433.8320\n",
      "Training Epoch: 2 [5760/49669]\tLoss: 68924.6875\n",
      "Training Epoch: 2 [5780/49669]\tLoss: 61068.9102\n",
      "Training Epoch: 2 [5800/49669]\tLoss: 56661.4883\n",
      "Training Epoch: 2 [5820/49669]\tLoss: 58487.3711\n",
      "Training Epoch: 2 [5840/49669]\tLoss: 69131.2344\n",
      "Training Epoch: 2 [5860/49669]\tLoss: 63620.5508\n",
      "Training Epoch: 2 [5880/49669]\tLoss: 63560.2383\n",
      "Training Epoch: 2 [5900/49669]\tLoss: 61972.6797\n",
      "Training Epoch: 2 [5920/49669]\tLoss: 54113.2891\n",
      "Training Epoch: 2 [5940/49669]\tLoss: 66311.2266\n",
      "Training Epoch: 2 [5960/49669]\tLoss: 67560.9922\n",
      "Training Epoch: 2 [5980/49669]\tLoss: 59089.3867\n",
      "Training Epoch: 2 [6000/49669]\tLoss: 56904.6953\n",
      "Training Epoch: 2 [6020/49669]\tLoss: 57765.8984\n",
      "Training Epoch: 2 [6040/49669]\tLoss: 54889.0664\n",
      "Training Epoch: 2 [6060/49669]\tLoss: 67491.7344\n",
      "Training Epoch: 2 [6080/49669]\tLoss: 63389.8672\n",
      "Training Epoch: 2 [6100/49669]\tLoss: 61646.2852\n",
      "Training Epoch: 2 [6120/49669]\tLoss: 67221.0547\n",
      "Training Epoch: 2 [6140/49669]\tLoss: 58982.7227\n",
      "Training Epoch: 2 [6160/49669]\tLoss: 68459.1328\n",
      "Training Epoch: 2 [6180/49669]\tLoss: 55957.4023\n",
      "Training Epoch: 2 [6200/49669]\tLoss: 53115.6289\n",
      "Training Epoch: 2 [6220/49669]\tLoss: 59219.7656\n",
      "Training Epoch: 2 [6240/49669]\tLoss: 60314.6641\n",
      "Training Epoch: 2 [6260/49669]\tLoss: 64062.1172\n",
      "Training Epoch: 2 [6280/49669]\tLoss: 66843.9922\n",
      "Training Epoch: 2 [6300/49669]\tLoss: 51839.0195\n",
      "Training Epoch: 2 [6320/49669]\tLoss: 49413.9883\n",
      "Training Epoch: 2 [6340/49669]\tLoss: 64431.6133\n",
      "Training Epoch: 2 [6360/49669]\tLoss: 68307.9688\n",
      "Training Epoch: 2 [6380/49669]\tLoss: 59449.9414\n",
      "Training Epoch: 2 [6400/49669]\tLoss: 52053.0508\n",
      "Training Epoch: 2 [6420/49669]\tLoss: 61989.4609\n",
      "Training Epoch: 2 [6440/49669]\tLoss: 67409.4062\n",
      "Training Epoch: 2 [6460/49669]\tLoss: 61628.8398\n",
      "Training Epoch: 2 [6480/49669]\tLoss: 68505.4219\n",
      "Training Epoch: 2 [6500/49669]\tLoss: 67291.8594\n",
      "Training Epoch: 2 [6520/49669]\tLoss: 67675.0469\n",
      "Training Epoch: 2 [6540/49669]\tLoss: 70894.4375\n",
      "Training Epoch: 2 [6560/49669]\tLoss: 53282.8750\n",
      "Training Epoch: 2 [6580/49669]\tLoss: 57532.3906\n",
      "Training Epoch: 2 [6600/49669]\tLoss: 55411.0781\n",
      "Training Epoch: 2 [6620/49669]\tLoss: 66862.9297\n",
      "Training Epoch: 2 [6640/49669]\tLoss: 49993.3164\n",
      "Training Epoch: 2 [6660/49669]\tLoss: 63630.1211\n",
      "Training Epoch: 2 [6680/49669]\tLoss: 50703.9102\n",
      "Training Epoch: 2 [6700/49669]\tLoss: 62524.5508\n",
      "Training Epoch: 2 [6720/49669]\tLoss: 57704.9961\n",
      "Training Epoch: 2 [6740/49669]\tLoss: 60448.0664\n",
      "Training Epoch: 2 [6760/49669]\tLoss: 70060.4375\n",
      "Training Epoch: 2 [6780/49669]\tLoss: 63609.8594\n",
      "Training Epoch: 2 [6800/49669]\tLoss: 61977.8398\n",
      "Training Epoch: 2 [6820/49669]\tLoss: 65925.6328\n",
      "Training Epoch: 2 [6840/49669]\tLoss: 60558.2734\n",
      "Training Epoch: 2 [6860/49669]\tLoss: 61235.9102\n",
      "Training Epoch: 2 [6880/49669]\tLoss: 56493.8242\n",
      "Training Epoch: 2 [6900/49669]\tLoss: 66330.5469\n",
      "Training Epoch: 2 [6920/49669]\tLoss: 55704.2734\n",
      "Training Epoch: 2 [6940/49669]\tLoss: 64370.7188\n",
      "Training Epoch: 2 [6960/49669]\tLoss: 54261.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [6980/49669]\tLoss: 63766.1523\n",
      "Training Epoch: 2 [7000/49669]\tLoss: 63529.0703\n",
      "Training Epoch: 2 [7020/49669]\tLoss: 44706.3320\n",
      "Training Epoch: 2 [7040/49669]\tLoss: 56797.1953\n",
      "Training Epoch: 2 [7060/49669]\tLoss: 51264.3750\n",
      "Training Epoch: 2 [7080/49669]\tLoss: 51855.6367\n",
      "Training Epoch: 2 [7100/49669]\tLoss: 61459.9531\n",
      "Training Epoch: 2 [7120/49669]\tLoss: 67142.7734\n",
      "Training Epoch: 2 [7140/49669]\tLoss: 52929.2734\n",
      "Training Epoch: 2 [7160/49669]\tLoss: 66720.9922\n",
      "Training Epoch: 2 [7180/49669]\tLoss: 62415.1016\n",
      "Training Epoch: 2 [7200/49669]\tLoss: 55687.6094\n",
      "Training Epoch: 2 [7220/49669]\tLoss: 63582.1641\n",
      "Training Epoch: 2 [7240/49669]\tLoss: 54454.8867\n",
      "Training Epoch: 2 [7260/49669]\tLoss: 60558.3984\n",
      "Training Epoch: 2 [7280/49669]\tLoss: 53662.2070\n",
      "Training Epoch: 2 [7300/49669]\tLoss: 51460.2227\n",
      "Training Epoch: 2 [7320/49669]\tLoss: 55758.3477\n",
      "Training Epoch: 2 [7340/49669]\tLoss: 53392.7695\n",
      "Training Epoch: 2 [7360/49669]\tLoss: 61227.7383\n",
      "Training Epoch: 2 [7380/49669]\tLoss: 53296.1523\n",
      "Training Epoch: 2 [7400/49669]\tLoss: 56478.0195\n",
      "Training Epoch: 2 [7420/49669]\tLoss: 60119.9883\n",
      "Training Epoch: 2 [7440/49669]\tLoss: 56854.2461\n",
      "Training Epoch: 2 [7460/49669]\tLoss: 58870.8242\n",
      "Training Epoch: 2 [7480/49669]\tLoss: 54645.3516\n",
      "Training Epoch: 2 [7500/49669]\tLoss: 69551.0859\n",
      "Training Epoch: 2 [7520/49669]\tLoss: 55586.9023\n",
      "Training Epoch: 2 [7540/49669]\tLoss: 59668.1797\n",
      "Training Epoch: 2 [7560/49669]\tLoss: 63501.2617\n",
      "Training Epoch: 2 [7580/49669]\tLoss: 59012.7422\n",
      "Training Epoch: 2 [7600/49669]\tLoss: 51052.6641\n",
      "Training Epoch: 2 [7620/49669]\tLoss: 54142.0234\n",
      "Training Epoch: 2 [7640/49669]\tLoss: 60965.8438\n",
      "Training Epoch: 2 [7660/49669]\tLoss: 61911.1211\n",
      "Training Epoch: 2 [7680/49669]\tLoss: 47003.1484\n",
      "Training Epoch: 2 [7700/49669]\tLoss: 56193.5547\n",
      "Training Epoch: 2 [7720/49669]\tLoss: 63681.0234\n",
      "Training Epoch: 2 [7740/49669]\tLoss: 54212.2812\n",
      "Training Epoch: 2 [7760/49669]\tLoss: 55105.9414\n",
      "Training Epoch: 2 [7780/49669]\tLoss: 51883.1484\n",
      "Training Epoch: 2 [7800/49669]\tLoss: 57600.9805\n",
      "Training Epoch: 2 [7820/49669]\tLoss: 64695.5625\n",
      "Training Epoch: 2 [7840/49669]\tLoss: 55873.7539\n",
      "Training Epoch: 2 [7860/49669]\tLoss: 51840.0117\n",
      "Training Epoch: 2 [7880/49669]\tLoss: 58634.7500\n",
      "Training Epoch: 2 [7900/49669]\tLoss: 62587.1914\n",
      "Training Epoch: 2 [7920/49669]\tLoss: 57820.8711\n",
      "Training Epoch: 2 [7940/49669]\tLoss: 58977.0781\n",
      "Training Epoch: 2 [7960/49669]\tLoss: 64242.3477\n",
      "Training Epoch: 2 [7980/49669]\tLoss: 65079.2266\n",
      "Training Epoch: 2 [8000/49669]\tLoss: 56088.1992\n",
      "Training Epoch: 2 [8020/49669]\tLoss: 58866.4688\n",
      "Training Epoch: 2 [8040/49669]\tLoss: 58151.1172\n",
      "Training Epoch: 2 [8060/49669]\tLoss: 52801.6094\n",
      "Training Epoch: 2 [8080/49669]\tLoss: 58455.2305\n",
      "Training Epoch: 2 [8100/49669]\tLoss: 57986.8203\n",
      "Training Epoch: 2 [8120/49669]\tLoss: 53635.8750\n",
      "Training Epoch: 2 [8140/49669]\tLoss: 52260.8125\n",
      "Training Epoch: 2 [8160/49669]\tLoss: 53333.5781\n",
      "Training Epoch: 2 [8180/49669]\tLoss: 60550.5156\n",
      "Training Epoch: 2 [8200/49669]\tLoss: 54308.9766\n",
      "Training Epoch: 2 [8220/49669]\tLoss: 47937.3047\n",
      "Training Epoch: 2 [8240/49669]\tLoss: 61618.4062\n",
      "Training Epoch: 2 [8260/49669]\tLoss: 48630.7656\n",
      "Training Epoch: 2 [8280/49669]\tLoss: 53923.1328\n",
      "Training Epoch: 2 [8300/49669]\tLoss: 50374.6992\n",
      "Training Epoch: 2 [8320/49669]\tLoss: 50138.6797\n",
      "Training Epoch: 2 [8340/49669]\tLoss: 54079.6250\n",
      "Training Epoch: 2 [8360/49669]\tLoss: 43963.7734\n",
      "Training Epoch: 2 [8380/49669]\tLoss: 52963.6445\n",
      "Training Epoch: 2 [8400/49669]\tLoss: 58018.5156\n",
      "Training Epoch: 2 [8420/49669]\tLoss: 45451.1172\n",
      "Training Epoch: 2 [8440/49669]\tLoss: 60175.8438\n",
      "Training Epoch: 2 [8460/49669]\tLoss: 62105.7422\n",
      "Training Epoch: 2 [8480/49669]\tLoss: 66965.4453\n",
      "Training Epoch: 2 [8500/49669]\tLoss: 55934.4609\n",
      "Training Epoch: 2 [8520/49669]\tLoss: 55463.7461\n",
      "Training Epoch: 2 [8540/49669]\tLoss: 55347.0469\n",
      "Training Epoch: 2 [8560/49669]\tLoss: 53500.5273\n",
      "Training Epoch: 2 [8580/49669]\tLoss: 49668.3008\n",
      "Training Epoch: 2 [8600/49669]\tLoss: 50211.6797\n",
      "Training Epoch: 2 [8620/49669]\tLoss: 52523.4570\n",
      "Training Epoch: 2 [8640/49669]\tLoss: 66596.2344\n",
      "Training Epoch: 2 [8660/49669]\tLoss: 56711.6211\n",
      "Training Epoch: 2 [8680/49669]\tLoss: 59652.8555\n",
      "Training Epoch: 2 [8700/49669]\tLoss: 53668.7188\n",
      "Training Epoch: 2 [8720/49669]\tLoss: 52813.7109\n",
      "Training Epoch: 2 [8740/49669]\tLoss: 54381.3906\n",
      "Training Epoch: 2 [8760/49669]\tLoss: 51095.8242\n",
      "Training Epoch: 2 [8780/49669]\tLoss: 48721.6875\n",
      "Training Epoch: 2 [8800/49669]\tLoss: 51036.5469\n",
      "Training Epoch: 2 [8820/49669]\tLoss: 50492.5000\n",
      "Training Epoch: 2 [8840/49669]\tLoss: 47036.3594\n",
      "Training Epoch: 2 [8860/49669]\tLoss: 54641.1523\n",
      "Training Epoch: 2 [8880/49669]\tLoss: 56563.1484\n",
      "Training Epoch: 2 [8900/49669]\tLoss: 52378.5352\n",
      "Training Epoch: 2 [8920/49669]\tLoss: 44841.3477\n",
      "Training Epoch: 2 [8940/49669]\tLoss: 46828.8438\n",
      "Training Epoch: 2 [8960/49669]\tLoss: 50716.0898\n",
      "Training Epoch: 2 [8980/49669]\tLoss: 53941.4141\n",
      "Training Epoch: 2 [9000/49669]\tLoss: 58719.2617\n",
      "Training Epoch: 2 [9020/49669]\tLoss: 59583.6094\n",
      "Training Epoch: 2 [9040/49669]\tLoss: 57854.8477\n",
      "Training Epoch: 2 [9060/49669]\tLoss: 59711.5781\n",
      "Training Epoch: 2 [9080/49669]\tLoss: 58079.7852\n",
      "Training Epoch: 2 [9100/49669]\tLoss: 61130.6953\n",
      "Training Epoch: 2 [9120/49669]\tLoss: 53308.6523\n",
      "Training Epoch: 2 [9140/49669]\tLoss: 57094.4492\n",
      "Training Epoch: 2 [9160/49669]\tLoss: 53938.4258\n",
      "Training Epoch: 2 [9180/49669]\tLoss: 46397.3906\n",
      "Training Epoch: 2 [9200/49669]\tLoss: 49165.3438\n",
      "Training Epoch: 2 [9220/49669]\tLoss: 59229.0938\n",
      "Training Epoch: 2 [9240/49669]\tLoss: 62182.4219\n",
      "Training Epoch: 2 [9260/49669]\tLoss: 48605.7109\n",
      "Training Epoch: 2 [9280/49669]\tLoss: 50107.6406\n",
      "Training Epoch: 2 [9300/49669]\tLoss: 52114.0117\n",
      "Training Epoch: 2 [9320/49669]\tLoss: 61292.0820\n",
      "Training Epoch: 2 [9340/49669]\tLoss: 60931.3086\n",
      "Training Epoch: 2 [9360/49669]\tLoss: 58841.0234\n",
      "Training Epoch: 2 [9380/49669]\tLoss: 57338.3906\n",
      "Training Epoch: 2 [9400/49669]\tLoss: 56338.0547\n",
      "Training Epoch: 2 [9420/49669]\tLoss: 59643.6133\n",
      "Training Epoch: 2 [9440/49669]\tLoss: 51213.5547\n",
      "Training Epoch: 2 [9460/49669]\tLoss: 58927.2148\n",
      "Training Epoch: 2 [9480/49669]\tLoss: 58030.2266\n",
      "Training Epoch: 2 [9500/49669]\tLoss: 56218.9453\n",
      "Training Epoch: 2 [9520/49669]\tLoss: 59488.3711\n",
      "Training Epoch: 2 [9540/49669]\tLoss: 60551.0273\n",
      "Training Epoch: 2 [9560/49669]\tLoss: 51835.2891\n",
      "Training Epoch: 2 [9580/49669]\tLoss: 46425.6758\n",
      "Training Epoch: 2 [9600/49669]\tLoss: 56326.0859\n",
      "Training Epoch: 2 [9620/49669]\tLoss: 55088.0508\n",
      "Training Epoch: 2 [9640/49669]\tLoss: 55074.0039\n",
      "Training Epoch: 2 [9660/49669]\tLoss: 47982.0039\n",
      "Training Epoch: 2 [9680/49669]\tLoss: 59821.4180\n",
      "Training Epoch: 2 [9700/49669]\tLoss: 46997.3672\n",
      "Training Epoch: 2 [9720/49669]\tLoss: 52583.0469\n",
      "Training Epoch: 2 [9740/49669]\tLoss: 49566.8711\n",
      "Training Epoch: 2 [9760/49669]\tLoss: 55554.7070\n",
      "Training Epoch: 2 [9780/49669]\tLoss: 55655.4219\n",
      "Training Epoch: 2 [9800/49669]\tLoss: 53765.9805\n",
      "Training Epoch: 2 [9820/49669]\tLoss: 54373.3672\n",
      "Training Epoch: 2 [9840/49669]\tLoss: 49750.3516\n",
      "Training Epoch: 2 [9860/49669]\tLoss: 49646.0664\n",
      "Training Epoch: 2 [9880/49669]\tLoss: 51941.4570\n",
      "Training Epoch: 2 [9900/49669]\tLoss: 51267.3047\n",
      "Training Epoch: 2 [9920/49669]\tLoss: 50785.3281\n",
      "Training Epoch: 2 [9940/49669]\tLoss: 47445.4609\n",
      "Training Epoch: 2 [9960/49669]\tLoss: 53482.1836\n",
      "Training Epoch: 2 [9980/49669]\tLoss: 52154.7422\n",
      "Training Epoch: 2 [10000/49669]\tLoss: 61066.5820\n",
      "Training Epoch: 2 [10020/49669]\tLoss: 48985.2227\n",
      "Training Epoch: 2 [10040/49669]\tLoss: 44477.3047\n",
      "Training Epoch: 2 [10060/49669]\tLoss: 52114.8906\n",
      "Training Epoch: 2 [10080/49669]\tLoss: 57048.1875\n",
      "Training Epoch: 2 [10100/49669]\tLoss: 57412.0898\n",
      "Training Epoch: 2 [10120/49669]\tLoss: 55806.6680\n",
      "Training Epoch: 2 [10140/49669]\tLoss: 51075.0586\n",
      "Training Epoch: 2 [10160/49669]\tLoss: 48726.5273\n",
      "Training Epoch: 2 [10180/49669]\tLoss: 57519.0039\n",
      "Training Epoch: 2 [10200/49669]\tLoss: 54025.4531\n",
      "Training Epoch: 2 [10220/49669]\tLoss: 48249.8008\n",
      "Training Epoch: 2 [10240/49669]\tLoss: 51351.1406\n",
      "Training Epoch: 2 [10260/49669]\tLoss: 50012.5508\n",
      "Training Epoch: 2 [10280/49669]\tLoss: 39441.6992\n",
      "Training Epoch: 2 [10300/49669]\tLoss: 49616.2070\n",
      "Training Epoch: 2 [10320/49669]\tLoss: 44971.8672\n",
      "Training Epoch: 2 [10340/49669]\tLoss: 48654.2734\n",
      "Training Epoch: 2 [10360/49669]\tLoss: 49282.5469\n",
      "Training Epoch: 2 [10380/49669]\tLoss: 52763.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [10400/49669]\tLoss: 43932.7188\n",
      "Training Epoch: 2 [10420/49669]\tLoss: 50987.3711\n",
      "Training Epoch: 2 [10440/49669]\tLoss: 41888.7031\n",
      "Training Epoch: 2 [10460/49669]\tLoss: 54881.8281\n",
      "Training Epoch: 2 [10480/49669]\tLoss: 54244.5078\n",
      "Training Epoch: 2 [10500/49669]\tLoss: 46971.0859\n",
      "Training Epoch: 2 [10520/49669]\tLoss: 51298.5195\n",
      "Training Epoch: 2 [10540/49669]\tLoss: 60587.6172\n",
      "Training Epoch: 2 [10560/49669]\tLoss: 53569.6289\n",
      "Training Epoch: 2 [10580/49669]\tLoss: 42565.9609\n",
      "Training Epoch: 2 [10600/49669]\tLoss: 52089.7031\n",
      "Training Epoch: 2 [10620/49669]\tLoss: 45746.2266\n",
      "Training Epoch: 2 [10640/49669]\tLoss: 51092.2227\n",
      "Training Epoch: 2 [10660/49669]\tLoss: 46628.5938\n",
      "Training Epoch: 2 [10680/49669]\tLoss: 50280.6367\n",
      "Training Epoch: 2 [10700/49669]\tLoss: 50804.3398\n",
      "Training Epoch: 2 [10720/49669]\tLoss: 44754.1641\n",
      "Training Epoch: 2 [10740/49669]\tLoss: 47764.6523\n",
      "Training Epoch: 2 [10760/49669]\tLoss: 48494.6602\n",
      "Training Epoch: 2 [10780/49669]\tLoss: 53847.1055\n",
      "Training Epoch: 2 [10800/49669]\tLoss: 47520.1641\n",
      "Training Epoch: 2 [10820/49669]\tLoss: 48811.8789\n",
      "Training Epoch: 2 [10840/49669]\tLoss: 45242.1367\n",
      "Training Epoch: 2 [10860/49669]\tLoss: 46872.1953\n",
      "Training Epoch: 2 [10880/49669]\tLoss: 38261.4297\n",
      "Training Epoch: 2 [10900/49669]\tLoss: 46533.2891\n",
      "Training Epoch: 2 [10920/49669]\tLoss: 36998.8828\n",
      "Training Epoch: 2 [10940/49669]\tLoss: 46963.5039\n",
      "Training Epoch: 2 [10960/49669]\tLoss: 46766.2656\n",
      "Training Epoch: 2 [10980/49669]\tLoss: 38909.7305\n",
      "Training Epoch: 2 [11000/49669]\tLoss: 51211.0039\n",
      "Training Epoch: 2 [11020/49669]\tLoss: 46302.6055\n",
      "Training Epoch: 2 [11040/49669]\tLoss: 47445.7266\n",
      "Training Epoch: 2 [11060/49669]\tLoss: 44498.1680\n",
      "Training Epoch: 2 [11080/49669]\tLoss: 48848.5000\n",
      "Training Epoch: 2 [11100/49669]\tLoss: 45076.0781\n",
      "Training Epoch: 2 [11120/49669]\tLoss: 48797.0391\n",
      "Training Epoch: 2 [11140/49669]\tLoss: 48310.3594\n",
      "Training Epoch: 2 [11160/49669]\tLoss: 48812.2305\n",
      "Training Epoch: 2 [11180/49669]\tLoss: 56392.1719\n",
      "Training Epoch: 2 [11200/49669]\tLoss: 55619.3086\n",
      "Training Epoch: 2 [11220/49669]\tLoss: 55718.4219\n",
      "Training Epoch: 2 [11240/49669]\tLoss: 48451.5859\n",
      "Training Epoch: 2 [11260/49669]\tLoss: 54733.9961\n",
      "Training Epoch: 2 [11280/49669]\tLoss: 53115.1016\n",
      "Training Epoch: 2 [11300/49669]\tLoss: 48827.4922\n",
      "Training Epoch: 2 [11320/49669]\tLoss: 54055.5547\n",
      "Training Epoch: 2 [11340/49669]\tLoss: 40239.1250\n",
      "Training Epoch: 2 [11360/49669]\tLoss: 46444.1328\n",
      "Training Epoch: 2 [11380/49669]\tLoss: 50421.3906\n",
      "Training Epoch: 2 [11400/49669]\tLoss: 40246.1602\n",
      "Training Epoch: 2 [11420/49669]\tLoss: 46593.6094\n",
      "Training Epoch: 2 [11440/49669]\tLoss: 49858.6328\n",
      "Training Epoch: 2 [11460/49669]\tLoss: 47753.7969\n",
      "Training Epoch: 2 [11480/49669]\tLoss: 43489.9297\n",
      "Training Epoch: 2 [11500/49669]\tLoss: 43547.6758\n",
      "Training Epoch: 2 [11520/49669]\tLoss: 47520.0859\n",
      "Training Epoch: 2 [11540/49669]\tLoss: 46148.3516\n",
      "Training Epoch: 2 [11560/49669]\tLoss: 46836.1055\n",
      "Training Epoch: 2 [11580/49669]\tLoss: 46061.5156\n",
      "Training Epoch: 2 [11600/49669]\tLoss: 46035.6719\n",
      "Training Epoch: 2 [11620/49669]\tLoss: 47513.0117\n",
      "Training Epoch: 2 [11640/49669]\tLoss: 47393.7656\n",
      "Training Epoch: 2 [11660/49669]\tLoss: 48475.9453\n",
      "Training Epoch: 2 [11680/49669]\tLoss: 46982.3047\n",
      "Training Epoch: 2 [11700/49669]\tLoss: 48881.6680\n",
      "Training Epoch: 2 [11720/49669]\tLoss: 52124.7070\n",
      "Training Epoch: 2 [11740/49669]\tLoss: 45701.5977\n",
      "Training Epoch: 2 [11760/49669]\tLoss: 53722.0156\n",
      "Training Epoch: 2 [11780/49669]\tLoss: 44842.6055\n",
      "Training Epoch: 2 [11800/49669]\tLoss: 56090.6797\n",
      "Training Epoch: 2 [11820/49669]\tLoss: 46678.7578\n",
      "Training Epoch: 2 [11840/49669]\tLoss: 43698.0039\n",
      "Training Epoch: 2 [11860/49669]\tLoss: 52186.9297\n",
      "Training Epoch: 2 [11880/49669]\tLoss: 49044.5234\n",
      "Training Epoch: 2 [11900/49669]\tLoss: 56252.2383\n",
      "Training Epoch: 2 [11920/49669]\tLoss: 54807.0000\n",
      "Training Epoch: 2 [11940/49669]\tLoss: 35965.6328\n",
      "Training Epoch: 2 [11960/49669]\tLoss: 48968.0859\n",
      "Training Epoch: 2 [11980/49669]\tLoss: 49168.7383\n",
      "Training Epoch: 2 [12000/49669]\tLoss: 45810.9375\n",
      "Training Epoch: 2 [12020/49669]\tLoss: 55552.8047\n",
      "Training Epoch: 2 [12040/49669]\tLoss: 46764.2930\n",
      "Training Epoch: 2 [12060/49669]\tLoss: 53681.7109\n",
      "Training Epoch: 2 [12080/49669]\tLoss: 51650.2383\n",
      "Training Epoch: 2 [12100/49669]\tLoss: 47389.6680\n",
      "Training Epoch: 2 [12120/49669]\tLoss: 49733.3555\n",
      "Training Epoch: 2 [12140/49669]\tLoss: 48407.9570\n",
      "Training Epoch: 2 [12160/49669]\tLoss: 49192.2852\n",
      "Training Epoch: 2 [12180/49669]\tLoss: 43006.8750\n",
      "Training Epoch: 2 [12200/49669]\tLoss: 52233.2461\n",
      "Training Epoch: 2 [12220/49669]\tLoss: 48421.4258\n",
      "Training Epoch: 2 [12240/49669]\tLoss: 39583.3477\n",
      "Training Epoch: 2 [12260/49669]\tLoss: 47098.0000\n",
      "Training Epoch: 2 [12280/49669]\tLoss: 50103.2930\n",
      "Training Epoch: 2 [12300/49669]\tLoss: 49917.2461\n",
      "Training Epoch: 2 [12320/49669]\tLoss: 46101.7891\n",
      "Training Epoch: 2 [12340/49669]\tLoss: 42775.8242\n",
      "Training Epoch: 2 [12360/49669]\tLoss: 47031.8242\n",
      "Training Epoch: 2 [12380/49669]\tLoss: 42721.2852\n",
      "Training Epoch: 2 [12400/49669]\tLoss: 50588.9922\n",
      "Training Epoch: 2 [12420/49669]\tLoss: 46916.1133\n",
      "Training Epoch: 2 [12440/49669]\tLoss: 46849.4570\n",
      "Training Epoch: 2 [12460/49669]\tLoss: 51558.2578\n",
      "Training Epoch: 2 [12480/49669]\tLoss: 42374.9688\n",
      "Training Epoch: 2 [12500/49669]\tLoss: 48743.4805\n",
      "Training Epoch: 2 [12520/49669]\tLoss: 46821.5508\n",
      "Training Epoch: 2 [12540/49669]\tLoss: 43085.1719\n",
      "Training Epoch: 2 [12560/49669]\tLoss: 45761.8398\n",
      "Training Epoch: 2 [12580/49669]\tLoss: 49466.7617\n",
      "Training Epoch: 2 [12600/49669]\tLoss: 48779.1992\n",
      "Training Epoch: 2 [12620/49669]\tLoss: 47877.1992\n",
      "Training Epoch: 2 [12640/49669]\tLoss: 45486.2539\n",
      "Training Epoch: 2 [12660/49669]\tLoss: 50999.6758\n",
      "Training Epoch: 2 [12680/49669]\tLoss: 50014.0195\n",
      "Training Epoch: 2 [12700/49669]\tLoss: 47030.1602\n",
      "Training Epoch: 2 [12720/49669]\tLoss: 48578.2344\n",
      "Training Epoch: 2 [12740/49669]\tLoss: 38932.3359\n",
      "Training Epoch: 2 [12760/49669]\tLoss: 40661.7812\n",
      "Training Epoch: 2 [12780/49669]\tLoss: 48382.2266\n",
      "Training Epoch: 2 [12800/49669]\tLoss: 45634.0156\n",
      "Training Epoch: 2 [12820/49669]\tLoss: 52076.3008\n",
      "Training Epoch: 2 [12840/49669]\tLoss: 53776.5586\n",
      "Training Epoch: 2 [12860/49669]\tLoss: 50450.8555\n",
      "Training Epoch: 2 [12880/49669]\tLoss: 44053.1797\n",
      "Training Epoch: 2 [12900/49669]\tLoss: 39871.1758\n",
      "Training Epoch: 2 [12920/49669]\tLoss: 46794.2461\n",
      "Training Epoch: 2 [12940/49669]\tLoss: 50491.2266\n",
      "Training Epoch: 2 [12960/49669]\tLoss: 46391.0430\n",
      "Training Epoch: 2 [12980/49669]\tLoss: 50430.4258\n",
      "Training Epoch: 2 [13000/49669]\tLoss: 51250.8867\n",
      "Training Epoch: 2 [13020/49669]\tLoss: 44850.8164\n",
      "Training Epoch: 2 [13040/49669]\tLoss: 38327.0664\n",
      "Training Epoch: 2 [13060/49669]\tLoss: 47814.9844\n",
      "Training Epoch: 2 [13080/49669]\tLoss: 41557.8281\n",
      "Training Epoch: 2 [13100/49669]\tLoss: 51838.9062\n",
      "Training Epoch: 2 [13120/49669]\tLoss: 51595.3125\n",
      "Training Epoch: 2 [13140/49669]\tLoss: 41984.6914\n",
      "Training Epoch: 2 [13160/49669]\tLoss: 43462.3203\n",
      "Training Epoch: 2 [13180/49669]\tLoss: 47778.5820\n",
      "Training Epoch: 2 [13200/49669]\tLoss: 40767.7930\n",
      "Training Epoch: 2 [13220/49669]\tLoss: 43950.6914\n",
      "Training Epoch: 2 [13240/49669]\tLoss: 44885.3281\n",
      "Training Epoch: 2 [13260/49669]\tLoss: 49517.5430\n",
      "Training Epoch: 2 [13280/49669]\tLoss: 44491.5391\n",
      "Training Epoch: 2 [13300/49669]\tLoss: 49759.2188\n",
      "Training Epoch: 2 [13320/49669]\tLoss: 41859.6289\n",
      "Training Epoch: 2 [13340/49669]\tLoss: 43562.5586\n",
      "Training Epoch: 2 [13360/49669]\tLoss: 41006.9141\n",
      "Training Epoch: 2 [13380/49669]\tLoss: 46366.8711\n",
      "Training Epoch: 2 [13400/49669]\tLoss: 48329.4062\n",
      "Training Epoch: 2 [13420/49669]\tLoss: 50441.5625\n",
      "Training Epoch: 2 [13440/49669]\tLoss: 40231.6680\n",
      "Training Epoch: 2 [13460/49669]\tLoss: 45937.5234\n",
      "Training Epoch: 2 [13480/49669]\tLoss: 48933.7188\n",
      "Training Epoch: 2 [13500/49669]\tLoss: 42002.6289\n",
      "Training Epoch: 2 [13520/49669]\tLoss: 46406.7109\n",
      "Training Epoch: 2 [13540/49669]\tLoss: 44306.4883\n",
      "Training Epoch: 2 [13560/49669]\tLoss: 47595.0117\n",
      "Training Epoch: 2 [13580/49669]\tLoss: 39744.4844\n",
      "Training Epoch: 2 [13600/49669]\tLoss: 41451.6016\n",
      "Training Epoch: 2 [13620/49669]\tLoss: 44508.4180\n",
      "Training Epoch: 2 [13640/49669]\tLoss: 41938.8438\n",
      "Training Epoch: 2 [13660/49669]\tLoss: 48647.9062\n",
      "Training Epoch: 2 [13680/49669]\tLoss: 44388.8047\n",
      "Training Epoch: 2 [13700/49669]\tLoss: 46590.1445\n",
      "Training Epoch: 2 [13720/49669]\tLoss: 36513.2109\n",
      "Training Epoch: 2 [13740/49669]\tLoss: 41741.8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [13760/49669]\tLoss: 41875.8516\n",
      "Training Epoch: 2 [13780/49669]\tLoss: 44230.1367\n",
      "Training Epoch: 2 [13800/49669]\tLoss: 47177.6641\n",
      "Training Epoch: 2 [13820/49669]\tLoss: 46587.7109\n",
      "Training Epoch: 2 [13840/49669]\tLoss: 45344.6016\n",
      "Training Epoch: 2 [13860/49669]\tLoss: 47161.4102\n",
      "Training Epoch: 2 [13880/49669]\tLoss: 41102.0508\n",
      "Training Epoch: 2 [13900/49669]\tLoss: 40544.8164\n",
      "Training Epoch: 2 [13920/49669]\tLoss: 47228.9219\n",
      "Training Epoch: 2 [13940/49669]\tLoss: 49301.9727\n",
      "Training Epoch: 2 [13960/49669]\tLoss: 37261.4219\n",
      "Training Epoch: 2 [13980/49669]\tLoss: 44303.4961\n",
      "Training Epoch: 2 [14000/49669]\tLoss: 45467.4766\n",
      "Training Epoch: 2 [14020/49669]\tLoss: 43183.1602\n",
      "Training Epoch: 2 [14040/49669]\tLoss: 44212.0664\n",
      "Training Epoch: 2 [14060/49669]\tLoss: 38771.2227\n",
      "Training Epoch: 2 [14080/49669]\tLoss: 47412.5117\n",
      "Training Epoch: 2 [14100/49669]\tLoss: 40658.1406\n",
      "Training Epoch: 2 [14120/49669]\tLoss: 43287.3672\n",
      "Training Epoch: 2 [14140/49669]\tLoss: 49369.8594\n",
      "Training Epoch: 2 [14160/49669]\tLoss: 48064.5977\n",
      "Training Epoch: 2 [14180/49669]\tLoss: 45678.6953\n",
      "Training Epoch: 2 [14200/49669]\tLoss: 41606.0039\n",
      "Training Epoch: 2 [14220/49669]\tLoss: 45615.1016\n",
      "Training Epoch: 2 [14240/49669]\tLoss: 43120.2969\n",
      "Training Epoch: 2 [14260/49669]\tLoss: 43838.6016\n",
      "Training Epoch: 2 [14280/49669]\tLoss: 36512.8281\n",
      "Training Epoch: 2 [14300/49669]\tLoss: 38711.8867\n",
      "Training Epoch: 2 [14320/49669]\tLoss: 39943.3398\n",
      "Training Epoch: 2 [14340/49669]\tLoss: 41819.3555\n",
      "Training Epoch: 2 [14360/49669]\tLoss: 39625.4414\n",
      "Training Epoch: 2 [14380/49669]\tLoss: 48494.8984\n",
      "Training Epoch: 2 [14400/49669]\tLoss: 50440.6406\n",
      "Training Epoch: 2 [14420/49669]\tLoss: 41974.9609\n",
      "Training Epoch: 2 [14440/49669]\tLoss: 44620.3125\n",
      "Training Epoch: 2 [14460/49669]\tLoss: 45062.8828\n",
      "Training Epoch: 2 [14480/49669]\tLoss: 40059.2305\n",
      "Training Epoch: 2 [14500/49669]\tLoss: 46818.7539\n",
      "Training Epoch: 2 [14520/49669]\tLoss: 43253.9883\n",
      "Training Epoch: 2 [14540/49669]\tLoss: 41804.6172\n",
      "Training Epoch: 2 [14560/49669]\tLoss: 40145.6172\n",
      "Training Epoch: 2 [14580/49669]\tLoss: 42347.2266\n",
      "Training Epoch: 2 [14600/49669]\tLoss: 42733.2148\n",
      "Training Epoch: 2 [14620/49669]\tLoss: 46701.7773\n",
      "Training Epoch: 2 [14640/49669]\tLoss: 35503.8516\n",
      "Training Epoch: 2 [14660/49669]\tLoss: 43454.4297\n",
      "Training Epoch: 2 [14680/49669]\tLoss: 45749.8359\n",
      "Training Epoch: 2 [14700/49669]\tLoss: 32825.9258\n",
      "Training Epoch: 2 [14720/49669]\tLoss: 46832.2695\n",
      "Training Epoch: 2 [14740/49669]\tLoss: 45157.6758\n",
      "Training Epoch: 2 [14760/49669]\tLoss: 44695.0312\n",
      "Training Epoch: 2 [14780/49669]\tLoss: 40318.0273\n",
      "Training Epoch: 2 [14800/49669]\tLoss: 38993.5000\n",
      "Training Epoch: 2 [14820/49669]\tLoss: 43134.5625\n",
      "Training Epoch: 2 [14840/49669]\tLoss: 46053.6797\n",
      "Training Epoch: 2 [14860/49669]\tLoss: 46109.2461\n",
      "Training Epoch: 2 [14880/49669]\tLoss: 37860.2930\n",
      "Training Epoch: 2 [14900/49669]\tLoss: 39960.6484\n",
      "Training Epoch: 2 [14920/49669]\tLoss: 38445.2656\n",
      "Training Epoch: 2 [14940/49669]\tLoss: 38531.0117\n",
      "Training Epoch: 2 [14960/49669]\tLoss: 38184.0117\n",
      "Training Epoch: 2 [14980/49669]\tLoss: 41798.4102\n",
      "Training Epoch: 2 [15000/49669]\tLoss: 36624.7461\n",
      "Training Epoch: 2 [15020/49669]\tLoss: 41573.8945\n",
      "Training Epoch: 2 [15040/49669]\tLoss: 39953.0859\n",
      "Training Epoch: 2 [15060/49669]\tLoss: 40867.6875\n",
      "Training Epoch: 2 [15080/49669]\tLoss: 39545.1602\n",
      "Training Epoch: 2 [15100/49669]\tLoss: 38174.8867\n",
      "Training Epoch: 2 [15120/49669]\tLoss: 37982.8203\n",
      "Training Epoch: 2 [15140/49669]\tLoss: 47947.5820\n",
      "Training Epoch: 2 [15160/49669]\tLoss: 39112.8945\n",
      "Training Epoch: 2 [15180/49669]\tLoss: 42198.4766\n",
      "Training Epoch: 2 [15200/49669]\tLoss: 44456.9297\n",
      "Training Epoch: 2 [15220/49669]\tLoss: 46992.6328\n",
      "Training Epoch: 2 [15240/49669]\tLoss: 37738.9492\n",
      "Training Epoch: 2 [15260/49669]\tLoss: 41899.8633\n",
      "Training Epoch: 2 [15280/49669]\tLoss: 41717.9648\n",
      "Training Epoch: 2 [15300/49669]\tLoss: 37753.3633\n",
      "Training Epoch: 2 [15320/49669]\tLoss: 43781.7500\n",
      "Training Epoch: 2 [15340/49669]\tLoss: 37429.0195\n",
      "Training Epoch: 2 [15360/49669]\tLoss: 41312.8516\n",
      "Training Epoch: 2 [15380/49669]\tLoss: 43407.3398\n",
      "Training Epoch: 2 [15400/49669]\tLoss: 41409.6602\n",
      "Training Epoch: 2 [15420/49669]\tLoss: 45834.5742\n",
      "Training Epoch: 2 [15440/49669]\tLoss: 47462.4375\n",
      "Training Epoch: 2 [15460/49669]\tLoss: 45158.4102\n",
      "Training Epoch: 2 [15480/49669]\tLoss: 38516.6523\n",
      "Training Epoch: 2 [15500/49669]\tLoss: 41517.0859\n",
      "Training Epoch: 2 [15520/49669]\tLoss: 40405.6836\n",
      "Training Epoch: 2 [15540/49669]\tLoss: 41948.7344\n",
      "Training Epoch: 2 [15560/49669]\tLoss: 38310.0664\n",
      "Training Epoch: 2 [15580/49669]\tLoss: 38514.9805\n",
      "Training Epoch: 2 [15600/49669]\tLoss: 41722.4297\n",
      "Training Epoch: 2 [15620/49669]\tLoss: 48491.1055\n",
      "Training Epoch: 2 [15640/49669]\tLoss: 37707.6758\n",
      "Training Epoch: 2 [15660/49669]\tLoss: 37028.0547\n",
      "Training Epoch: 2 [15680/49669]\tLoss: 35401.4258\n",
      "Training Epoch: 2 [15700/49669]\tLoss: 31597.8770\n",
      "Training Epoch: 2 [15720/49669]\tLoss: 38987.5938\n",
      "Training Epoch: 2 [15740/49669]\tLoss: 47514.9727\n",
      "Training Epoch: 2 [15760/49669]\tLoss: 42944.2461\n",
      "Training Epoch: 2 [15780/49669]\tLoss: 45616.7305\n",
      "Training Epoch: 2 [15800/49669]\tLoss: 43069.4844\n",
      "Training Epoch: 2 [15820/49669]\tLoss: 44272.3438\n",
      "Training Epoch: 2 [15840/49669]\tLoss: 37838.1445\n",
      "Training Epoch: 2 [15860/49669]\tLoss: 41924.3203\n",
      "Training Epoch: 2 [15880/49669]\tLoss: 43566.0391\n",
      "Training Epoch: 2 [15900/49669]\tLoss: 41813.1133\n",
      "Training Epoch: 2 [15920/49669]\tLoss: 36954.8984\n",
      "Training Epoch: 2 [15940/49669]\tLoss: 31513.9707\n",
      "Training Epoch: 2 [15960/49669]\tLoss: 37935.2891\n",
      "Training Epoch: 2 [15980/49669]\tLoss: 42804.0547\n",
      "Training Epoch: 2 [16000/49669]\tLoss: 42404.1211\n",
      "Training Epoch: 2 [16020/49669]\tLoss: 41196.0547\n",
      "Training Epoch: 2 [16040/49669]\tLoss: 38365.0547\n",
      "Training Epoch: 2 [16060/49669]\tLoss: 45530.1680\n",
      "Training Epoch: 2 [16080/49669]\tLoss: 47788.3555\n",
      "Training Epoch: 2 [16100/49669]\tLoss: 43023.1641\n",
      "Training Epoch: 2 [16120/49669]\tLoss: 39944.6523\n",
      "Training Epoch: 2 [16140/49669]\tLoss: 41276.5195\n",
      "Training Epoch: 2 [16160/49669]\tLoss: 41689.6172\n",
      "Training Epoch: 2 [16180/49669]\tLoss: 37886.6250\n",
      "Training Epoch: 2 [16200/49669]\tLoss: 37271.2148\n",
      "Training Epoch: 2 [16220/49669]\tLoss: 35877.3359\n",
      "Training Epoch: 2 [16240/49669]\tLoss: 40357.5625\n",
      "Training Epoch: 2 [16260/49669]\tLoss: 41046.5938\n",
      "Training Epoch: 2 [16280/49669]\tLoss: 37406.5859\n",
      "Training Epoch: 2 [16300/49669]\tLoss: 41159.1562\n",
      "Training Epoch: 2 [16320/49669]\tLoss: 43983.8438\n",
      "Training Epoch: 2 [16340/49669]\tLoss: 37997.5352\n",
      "Training Epoch: 2 [16360/49669]\tLoss: 42126.3828\n",
      "Training Epoch: 2 [16380/49669]\tLoss: 35854.8203\n",
      "Training Epoch: 2 [16400/49669]\tLoss: 37385.1992\n",
      "Training Epoch: 2 [16420/49669]\tLoss: 43979.7266\n",
      "Training Epoch: 2 [16440/49669]\tLoss: 37225.8828\n",
      "Training Epoch: 2 [16460/49669]\tLoss: 38880.7500\n",
      "Training Epoch: 2 [16480/49669]\tLoss: 40969.1992\n",
      "Training Epoch: 2 [16500/49669]\tLoss: 27195.0820\n",
      "Training Epoch: 2 [16520/49669]\tLoss: 35412.6367\n",
      "Training Epoch: 2 [16540/49669]\tLoss: 35211.4883\n",
      "Training Epoch: 2 [16560/49669]\tLoss: 43487.2969\n",
      "Training Epoch: 2 [16580/49669]\tLoss: 47112.3672\n",
      "Training Epoch: 2 [16600/49669]\tLoss: 45965.8047\n",
      "Training Epoch: 2 [16620/49669]\tLoss: 37129.1094\n",
      "Training Epoch: 2 [16640/49669]\tLoss: 41182.6133\n",
      "Training Epoch: 2 [16660/49669]\tLoss: 39496.5898\n",
      "Training Epoch: 2 [16680/49669]\tLoss: 37794.2070\n",
      "Training Epoch: 2 [16700/49669]\tLoss: 35766.1211\n",
      "Training Epoch: 2 [16720/49669]\tLoss: 40122.4922\n",
      "Training Epoch: 2 [16740/49669]\tLoss: 36308.5742\n",
      "Training Epoch: 2 [16760/49669]\tLoss: 43553.5039\n",
      "Training Epoch: 2 [16780/49669]\tLoss: 38272.5078\n",
      "Training Epoch: 2 [16800/49669]\tLoss: 36145.6367\n",
      "Training Epoch: 2 [16820/49669]\tLoss: 36178.1484\n",
      "Training Epoch: 2 [16840/49669]\tLoss: 40511.6523\n",
      "Training Epoch: 2 [16860/49669]\tLoss: 39169.7734\n",
      "Training Epoch: 2 [16880/49669]\tLoss: 35731.1914\n",
      "Training Epoch: 2 [16900/49669]\tLoss: 37915.1992\n",
      "Training Epoch: 2 [16920/49669]\tLoss: 35590.2109\n",
      "Training Epoch: 2 [16940/49669]\tLoss: 35228.9531\n",
      "Training Epoch: 2 [16960/49669]\tLoss: 40034.2031\n",
      "Training Epoch: 2 [16980/49669]\tLoss: 38570.6367\n",
      "Training Epoch: 2 [17000/49669]\tLoss: 34663.3516\n",
      "Training Epoch: 2 [17020/49669]\tLoss: 37840.0391\n",
      "Training Epoch: 2 [17040/49669]\tLoss: 38311.5742\n",
      "Training Epoch: 2 [17060/49669]\tLoss: 38557.2656\n",
      "Training Epoch: 2 [17080/49669]\tLoss: 41993.1133\n",
      "Training Epoch: 2 [17100/49669]\tLoss: 37513.5195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [17120/49669]\tLoss: 29513.2344\n",
      "Training Epoch: 2 [17140/49669]\tLoss: 38023.2539\n",
      "Training Epoch: 2 [17160/49669]\tLoss: 34785.1094\n",
      "Training Epoch: 2 [17180/49669]\tLoss: 43325.1875\n",
      "Training Epoch: 2 [17200/49669]\tLoss: 39056.3711\n",
      "Training Epoch: 2 [17220/49669]\tLoss: 39001.2539\n",
      "Training Epoch: 2 [17240/49669]\tLoss: 42874.0742\n",
      "Training Epoch: 2 [17260/49669]\tLoss: 36996.5312\n",
      "Training Epoch: 2 [17280/49669]\tLoss: 37551.9844\n",
      "Training Epoch: 2 [17300/49669]\tLoss: 34717.0742\n",
      "Training Epoch: 2 [17320/49669]\tLoss: 38438.4609\n",
      "Training Epoch: 2 [17340/49669]\tLoss: 38818.2422\n",
      "Training Epoch: 2 [17360/49669]\tLoss: 41346.5625\n",
      "Training Epoch: 2 [17380/49669]\tLoss: 38358.5781\n",
      "Training Epoch: 2 [17400/49669]\tLoss: 40016.9023\n",
      "Training Epoch: 2 [17420/49669]\tLoss: 43169.4844\n",
      "Training Epoch: 2 [17440/49669]\tLoss: 40623.6289\n",
      "Training Epoch: 2 [17460/49669]\tLoss: 41015.8945\n",
      "Training Epoch: 2 [17480/49669]\tLoss: 39637.9414\n",
      "Training Epoch: 2 [17500/49669]\tLoss: 36215.3398\n",
      "Training Epoch: 2 [17520/49669]\tLoss: 39644.3320\n",
      "Training Epoch: 2 [17540/49669]\tLoss: 36255.0039\n",
      "Training Epoch: 2 [17560/49669]\tLoss: 41900.4180\n",
      "Training Epoch: 2 [17580/49669]\tLoss: 38518.3711\n",
      "Training Epoch: 2 [17600/49669]\tLoss: 37915.1328\n",
      "Training Epoch: 2 [17620/49669]\tLoss: 34652.6445\n",
      "Training Epoch: 2 [17640/49669]\tLoss: 38673.8867\n",
      "Training Epoch: 2 [17660/49669]\tLoss: 32492.3770\n",
      "Training Epoch: 2 [17680/49669]\tLoss: 41766.4180\n",
      "Training Epoch: 2 [17700/49669]\tLoss: 42869.3047\n",
      "Training Epoch: 2 [17720/49669]\tLoss: 36857.0664\n",
      "Training Epoch: 2 [17740/49669]\tLoss: 36900.9023\n",
      "Training Epoch: 2 [17760/49669]\tLoss: 41843.3438\n",
      "Training Epoch: 2 [17780/49669]\tLoss: 35956.5547\n",
      "Training Epoch: 2 [17800/49669]\tLoss: 41970.1016\n",
      "Training Epoch: 2 [17820/49669]\tLoss: 40338.9805\n",
      "Training Epoch: 2 [17840/49669]\tLoss: 38346.6836\n",
      "Training Epoch: 2 [17860/49669]\tLoss: 38015.3945\n",
      "Training Epoch: 2 [17880/49669]\tLoss: 39270.2383\n",
      "Training Epoch: 2 [17900/49669]\tLoss: 40395.0508\n",
      "Training Epoch: 2 [17920/49669]\tLoss: 34513.7227\n",
      "Training Epoch: 2 [17940/49669]\tLoss: 30424.2188\n",
      "Training Epoch: 2 [17960/49669]\tLoss: 36932.3945\n",
      "Training Epoch: 2 [17980/49669]\tLoss: 37032.6602\n",
      "Training Epoch: 2 [18000/49669]\tLoss: 39606.7305\n",
      "Training Epoch: 2 [18020/49669]\tLoss: 38652.7461\n",
      "Training Epoch: 2 [18040/49669]\tLoss: 40687.0664\n",
      "Training Epoch: 2 [18060/49669]\tLoss: 35256.4297\n",
      "Training Epoch: 2 [18080/49669]\tLoss: 38372.1523\n",
      "Training Epoch: 2 [18100/49669]\tLoss: 37947.0781\n",
      "Training Epoch: 2 [18120/49669]\tLoss: 36460.2734\n",
      "Training Epoch: 2 [18140/49669]\tLoss: 34585.2539\n",
      "Training Epoch: 2 [18160/49669]\tLoss: 37540.3555\n",
      "Training Epoch: 2 [18180/49669]\tLoss: 36040.5273\n",
      "Training Epoch: 2 [18200/49669]\tLoss: 38309.9023\n",
      "Training Epoch: 2 [18220/49669]\tLoss: 40055.3945\n",
      "Training Epoch: 2 [18240/49669]\tLoss: 32617.9961\n",
      "Training Epoch: 2 [18260/49669]\tLoss: 36554.9375\n",
      "Training Epoch: 2 [18280/49669]\tLoss: 34224.1680\n",
      "Training Epoch: 2 [18300/49669]\tLoss: 31785.9668\n",
      "Training Epoch: 2 [18320/49669]\tLoss: 39927.3008\n",
      "Training Epoch: 2 [18340/49669]\tLoss: 40859.8086\n",
      "Training Epoch: 2 [18360/49669]\tLoss: 42771.4062\n",
      "Training Epoch: 2 [18380/49669]\tLoss: 39974.3828\n",
      "Training Epoch: 2 [18400/49669]\tLoss: 35530.7383\n",
      "Training Epoch: 2 [18420/49669]\tLoss: 32937.3867\n",
      "Training Epoch: 2 [18440/49669]\tLoss: 37667.1016\n",
      "Training Epoch: 2 [18460/49669]\tLoss: 31036.0527\n",
      "Training Epoch: 2 [18480/49669]\tLoss: 39181.6797\n",
      "Training Epoch: 2 [18500/49669]\tLoss: 35479.1289\n",
      "Training Epoch: 2 [18520/49669]\tLoss: 33508.6953\n",
      "Training Epoch: 2 [18540/49669]\tLoss: 38219.6367\n",
      "Training Epoch: 2 [18560/49669]\tLoss: 38767.9883\n",
      "Training Epoch: 2 [18580/49669]\tLoss: 32473.4922\n",
      "Training Epoch: 2 [18600/49669]\tLoss: 35411.9141\n",
      "Training Epoch: 2 [18620/49669]\tLoss: 34012.7188\n",
      "Training Epoch: 2 [18640/49669]\tLoss: 35078.5977\n",
      "Training Epoch: 2 [18660/49669]\tLoss: 35667.7344\n",
      "Training Epoch: 2 [18680/49669]\tLoss: 38337.3789\n",
      "Training Epoch: 2 [18700/49669]\tLoss: 38860.5859\n",
      "Training Epoch: 2 [18720/49669]\tLoss: 33154.7031\n",
      "Training Epoch: 2 [18740/49669]\tLoss: 40962.0078\n",
      "Training Epoch: 2 [18760/49669]\tLoss: 34802.7617\n",
      "Training Epoch: 2 [18780/49669]\tLoss: 30742.3027\n",
      "Training Epoch: 2 [18800/49669]\tLoss: 38729.6328\n",
      "Training Epoch: 2 [18820/49669]\tLoss: 36860.9844\n",
      "Training Epoch: 2 [18840/49669]\tLoss: 35105.8555\n",
      "Training Epoch: 2 [18860/49669]\tLoss: 41605.0234\n",
      "Training Epoch: 2 [18880/49669]\tLoss: 36227.8047\n",
      "Training Epoch: 2 [18900/49669]\tLoss: 34163.8125\n",
      "Training Epoch: 2 [18920/49669]\tLoss: 42636.1602\n",
      "Training Epoch: 2 [18940/49669]\tLoss: 33139.8672\n",
      "Training Epoch: 2 [18960/49669]\tLoss: 35892.2461\n",
      "Training Epoch: 2 [18980/49669]\tLoss: 36470.8789\n",
      "Training Epoch: 2 [19000/49669]\tLoss: 40021.5156\n",
      "Training Epoch: 2 [19020/49669]\tLoss: 34083.6562\n",
      "Training Epoch: 2 [19040/49669]\tLoss: 32715.6973\n",
      "Training Epoch: 2 [19060/49669]\tLoss: 35577.3750\n",
      "Training Epoch: 2 [19080/49669]\tLoss: 37972.7070\n",
      "Training Epoch: 2 [19100/49669]\tLoss: 31359.8926\n",
      "Training Epoch: 2 [19120/49669]\tLoss: 31564.2246\n",
      "Training Epoch: 2 [19140/49669]\tLoss: 28808.8809\n",
      "Training Epoch: 2 [19160/49669]\tLoss: 34867.2422\n",
      "Training Epoch: 2 [19180/49669]\tLoss: 32900.4570\n",
      "Training Epoch: 2 [19200/49669]\tLoss: 39601.9102\n",
      "Training Epoch: 2 [19220/49669]\tLoss: 39521.8477\n",
      "Training Epoch: 2 [19240/49669]\tLoss: 32619.9062\n",
      "Training Epoch: 2 [19260/49669]\tLoss: 36414.1797\n",
      "Training Epoch: 2 [19280/49669]\tLoss: 37185.6523\n",
      "Training Epoch: 2 [19300/49669]\tLoss: 35696.1133\n",
      "Training Epoch: 2 [19320/49669]\tLoss: 36064.2578\n",
      "Training Epoch: 2 [19340/49669]\tLoss: 40690.9531\n",
      "Training Epoch: 2 [19360/49669]\tLoss: 39087.8203\n",
      "Training Epoch: 2 [19380/49669]\tLoss: 34443.4609\n",
      "Training Epoch: 2 [19400/49669]\tLoss: 35498.8320\n",
      "Training Epoch: 2 [19420/49669]\tLoss: 32944.2656\n",
      "Training Epoch: 2 [19440/49669]\tLoss: 30893.6855\n",
      "Training Epoch: 2 [19460/49669]\tLoss: 35092.2344\n",
      "Training Epoch: 2 [19480/49669]\tLoss: 35241.8477\n",
      "Training Epoch: 2 [19500/49669]\tLoss: 38695.9219\n",
      "Training Epoch: 2 [19520/49669]\tLoss: 39650.8398\n",
      "Training Epoch: 2 [19540/49669]\tLoss: 38462.2461\n",
      "Training Epoch: 2 [19560/49669]\tLoss: 40213.9219\n",
      "Training Epoch: 2 [19580/49669]\tLoss: 33717.7539\n",
      "Training Epoch: 2 [19600/49669]\tLoss: 40965.2891\n",
      "Training Epoch: 2 [19620/49669]\tLoss: 38223.7266\n",
      "Training Epoch: 2 [19640/49669]\tLoss: 33888.2500\n",
      "Training Epoch: 2 [19660/49669]\tLoss: 40209.3125\n",
      "Training Epoch: 2 [19680/49669]\tLoss: 31368.5898\n",
      "Training Epoch: 2 [19700/49669]\tLoss: 40448.4609\n",
      "Training Epoch: 2 [19720/49669]\tLoss: 40049.7266\n",
      "Training Epoch: 2 [19740/49669]\tLoss: 35862.8203\n",
      "Training Epoch: 2 [19760/49669]\tLoss: 33990.5391\n",
      "Training Epoch: 2 [19780/49669]\tLoss: 35543.2930\n",
      "Training Epoch: 2 [19800/49669]\tLoss: 37160.5078\n",
      "Training Epoch: 2 [19820/49669]\tLoss: 35905.6992\n",
      "Training Epoch: 2 [19840/49669]\tLoss: 42180.6836\n",
      "Training Epoch: 2 [19860/49669]\tLoss: 39768.0898\n",
      "Training Epoch: 2 [19880/49669]\tLoss: 37189.0039\n",
      "Training Epoch: 2 [19900/49669]\tLoss: 33695.7305\n",
      "Training Epoch: 2 [19920/49669]\tLoss: 39868.0742\n",
      "Training Epoch: 2 [19940/49669]\tLoss: 32976.1641\n",
      "Training Epoch: 2 [19960/49669]\tLoss: 29826.7148\n",
      "Training Epoch: 2 [19980/49669]\tLoss: 34078.4570\n",
      "Training Epoch: 2 [20000/49669]\tLoss: 39301.2734\n",
      "Training Epoch: 2 [20020/49669]\tLoss: 36366.6172\n",
      "Training Epoch: 2 [20040/49669]\tLoss: 33074.4688\n",
      "Training Epoch: 2 [20060/49669]\tLoss: 36773.3828\n",
      "Training Epoch: 2 [20080/49669]\tLoss: 32823.3281\n",
      "Training Epoch: 2 [20100/49669]\tLoss: 33572.5703\n",
      "Training Epoch: 2 [20120/49669]\tLoss: 40452.3086\n",
      "Training Epoch: 2 [20140/49669]\tLoss: 32472.4648\n",
      "Training Epoch: 2 [20160/49669]\tLoss: 34388.4727\n",
      "Training Epoch: 2 [20180/49669]\tLoss: 34354.7539\n",
      "Training Epoch: 2 [20200/49669]\tLoss: 33558.3203\n",
      "Training Epoch: 2 [20220/49669]\tLoss: 38134.9883\n",
      "Training Epoch: 2 [20240/49669]\tLoss: 38276.0469\n",
      "Training Epoch: 2 [20260/49669]\tLoss: 33888.5938\n",
      "Training Epoch: 2 [20280/49669]\tLoss: 34425.0000\n",
      "Training Epoch: 2 [20300/49669]\tLoss: 34834.3203\n",
      "Training Epoch: 2 [20320/49669]\tLoss: 32970.7891\n",
      "Training Epoch: 2 [20340/49669]\tLoss: 33959.2656\n",
      "Training Epoch: 2 [20360/49669]\tLoss: 32579.2793\n",
      "Training Epoch: 2 [20380/49669]\tLoss: 37618.0547\n",
      "Training Epoch: 2 [20400/49669]\tLoss: 36655.6211\n",
      "Training Epoch: 2 [20420/49669]\tLoss: 31581.7910\n",
      "Training Epoch: 2 [20440/49669]\tLoss: 36583.9414\n",
      "Training Epoch: 2 [20460/49669]\tLoss: 31527.7598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [20480/49669]\tLoss: 33683.1914\n",
      "Training Epoch: 2 [20500/49669]\tLoss: 33582.2305\n",
      "Training Epoch: 2 [20520/49669]\tLoss: 35821.2461\n",
      "Training Epoch: 2 [20540/49669]\tLoss: 32065.9121\n",
      "Training Epoch: 2 [20560/49669]\tLoss: 30393.4062\n",
      "Training Epoch: 2 [20580/49669]\tLoss: 37384.7109\n",
      "Training Epoch: 2 [20600/49669]\tLoss: 33779.9375\n",
      "Training Epoch: 2 [20620/49669]\tLoss: 35934.8125\n",
      "Training Epoch: 2 [20640/49669]\tLoss: 30183.0234\n",
      "Training Epoch: 2 [20660/49669]\tLoss: 33284.1094\n",
      "Training Epoch: 2 [20680/49669]\tLoss: 38853.8242\n",
      "Training Epoch: 2 [20700/49669]\tLoss: 31519.1875\n",
      "Training Epoch: 2 [20720/49669]\tLoss: 34689.3867\n",
      "Training Epoch: 2 [20740/49669]\tLoss: 36040.4922\n",
      "Training Epoch: 2 [20760/49669]\tLoss: 36226.5859\n",
      "Training Epoch: 2 [20780/49669]\tLoss: 35649.3086\n",
      "Training Epoch: 2 [20800/49669]\tLoss: 31432.1973\n",
      "Training Epoch: 2 [20820/49669]\tLoss: 36714.3359\n",
      "Training Epoch: 2 [20840/49669]\tLoss: 38248.2148\n",
      "Training Epoch: 2 [20860/49669]\tLoss: 29543.0430\n",
      "Training Epoch: 2 [20880/49669]\tLoss: 33223.1875\n",
      "Training Epoch: 2 [20900/49669]\tLoss: 30086.0059\n",
      "Training Epoch: 2 [20920/49669]\tLoss: 35105.3438\n",
      "Training Epoch: 2 [20940/49669]\tLoss: 34665.2344\n",
      "Training Epoch: 2 [20960/49669]\tLoss: 37179.9805\n",
      "Training Epoch: 2 [20980/49669]\tLoss: 33862.3789\n",
      "Training Epoch: 2 [21000/49669]\tLoss: 37924.1211\n",
      "Training Epoch: 2 [21020/49669]\tLoss: 34898.2070\n",
      "Training Epoch: 2 [21040/49669]\tLoss: 36499.3594\n",
      "Training Epoch: 2 [21060/49669]\tLoss: 33461.4727\n",
      "Training Epoch: 2 [21080/49669]\tLoss: 30616.3320\n",
      "Training Epoch: 2 [21100/49669]\tLoss: 34378.6680\n",
      "Training Epoch: 2 [21120/49669]\tLoss: 35567.7227\n",
      "Training Epoch: 2 [21140/49669]\tLoss: 34330.7031\n",
      "Training Epoch: 2 [21160/49669]\tLoss: 32913.5391\n",
      "Training Epoch: 2 [21180/49669]\tLoss: 37748.3047\n",
      "Training Epoch: 2 [21200/49669]\tLoss: 33507.1875\n",
      "Training Epoch: 2 [21220/49669]\tLoss: 35082.1953\n",
      "Training Epoch: 2 [21240/49669]\tLoss: 38059.2852\n",
      "Training Epoch: 2 [21260/49669]\tLoss: 34794.7891\n",
      "Training Epoch: 2 [21280/49669]\tLoss: 33411.9023\n",
      "Training Epoch: 2 [21300/49669]\tLoss: 37701.8359\n",
      "Training Epoch: 2 [21320/49669]\tLoss: 32225.2383\n",
      "Training Epoch: 2 [21340/49669]\tLoss: 34569.9414\n",
      "Training Epoch: 2 [21360/49669]\tLoss: 39267.6758\n",
      "Training Epoch: 2 [21380/49669]\tLoss: 32568.5098\n",
      "Training Epoch: 2 [21400/49669]\tLoss: 37351.2930\n",
      "Training Epoch: 2 [21420/49669]\tLoss: 35325.4023\n",
      "Training Epoch: 2 [21440/49669]\tLoss: 34650.2656\n",
      "Training Epoch: 2 [21460/49669]\tLoss: 41485.0625\n",
      "Training Epoch: 2 [21480/49669]\tLoss: 38310.6250\n",
      "Training Epoch: 2 [21500/49669]\tLoss: 35750.5859\n",
      "Training Epoch: 2 [21520/49669]\tLoss: 39448.1172\n",
      "Training Epoch: 2 [21540/49669]\tLoss: 36564.2344\n",
      "Training Epoch: 2 [21560/49669]\tLoss: 32186.1836\n",
      "Training Epoch: 2 [21580/49669]\tLoss: 40362.3086\n",
      "Training Epoch: 2 [21600/49669]\tLoss: 37375.3516\n",
      "Training Epoch: 2 [21620/49669]\tLoss: 40439.1211\n",
      "Training Epoch: 2 [21640/49669]\tLoss: 34351.6953\n",
      "Training Epoch: 2 [21660/49669]\tLoss: 34585.4727\n",
      "Training Epoch: 2 [21680/49669]\tLoss: 34369.5898\n",
      "Training Epoch: 2 [21700/49669]\tLoss: 28432.2461\n",
      "Training Epoch: 2 [21720/49669]\tLoss: 33712.8164\n",
      "Training Epoch: 2 [21740/49669]\tLoss: 29801.6270\n",
      "Training Epoch: 2 [21760/49669]\tLoss: 32380.6406\n",
      "Training Epoch: 2 [21780/49669]\tLoss: 28475.6152\n",
      "Training Epoch: 2 [21800/49669]\tLoss: 30917.1504\n",
      "Training Epoch: 2 [21820/49669]\tLoss: 36675.5547\n",
      "Training Epoch: 2 [21840/49669]\tLoss: 32128.5488\n",
      "Training Epoch: 2 [21860/49669]\tLoss: 29699.9395\n",
      "Training Epoch: 2 [21880/49669]\tLoss: 35379.8008\n",
      "Training Epoch: 2 [21900/49669]\tLoss: 33349.3008\n",
      "Training Epoch: 2 [21920/49669]\tLoss: 27072.0547\n",
      "Training Epoch: 2 [21940/49669]\tLoss: 35043.7578\n",
      "Training Epoch: 2 [21960/49669]\tLoss: 35057.4648\n",
      "Training Epoch: 2 [21980/49669]\tLoss: 32595.1113\n",
      "Training Epoch: 2 [22000/49669]\tLoss: 31024.6387\n",
      "Training Epoch: 2 [22020/49669]\tLoss: 32625.6504\n",
      "Training Epoch: 2 [22040/49669]\tLoss: 35549.8555\n",
      "Training Epoch: 2 [22060/49669]\tLoss: 34782.0156\n",
      "Training Epoch: 2 [22080/49669]\tLoss: 30578.2148\n",
      "Training Epoch: 2 [22100/49669]\tLoss: 35272.2734\n",
      "Training Epoch: 2 [22120/49669]\tLoss: 33337.0312\n",
      "Training Epoch: 2 [22140/49669]\tLoss: 31797.5781\n",
      "Training Epoch: 2 [22160/49669]\tLoss: 32715.2598\n",
      "Training Epoch: 2 [22180/49669]\tLoss: 31207.5547\n",
      "Training Epoch: 2 [22200/49669]\tLoss: 28083.6934\n",
      "Training Epoch: 2 [22220/49669]\tLoss: 34183.6758\n",
      "Training Epoch: 2 [22240/49669]\tLoss: 34028.6211\n",
      "Training Epoch: 2 [22260/49669]\tLoss: 31646.8262\n",
      "Training Epoch: 2 [22280/49669]\tLoss: 30936.7754\n",
      "Training Epoch: 2 [22300/49669]\tLoss: 34084.9492\n",
      "Training Epoch: 2 [22320/49669]\tLoss: 36963.2422\n",
      "Training Epoch: 2 [22340/49669]\tLoss: 33530.5469\n",
      "Training Epoch: 2 [22360/49669]\tLoss: 33149.9492\n",
      "Training Epoch: 2 [22380/49669]\tLoss: 31407.0586\n",
      "Training Epoch: 2 [22400/49669]\tLoss: 32723.0371\n",
      "Training Epoch: 2 [22420/49669]\tLoss: 30106.2793\n",
      "Training Epoch: 2 [22440/49669]\tLoss: 32644.8887\n",
      "Training Epoch: 2 [22460/49669]\tLoss: 33634.7266\n",
      "Training Epoch: 2 [22480/49669]\tLoss: 30006.9238\n",
      "Training Epoch: 2 [22500/49669]\tLoss: 36188.0469\n",
      "Training Epoch: 2 [22520/49669]\tLoss: 29510.2734\n",
      "Training Epoch: 2 [22540/49669]\tLoss: 30008.2559\n",
      "Training Epoch: 2 [22560/49669]\tLoss: 26785.8789\n",
      "Training Epoch: 2 [22580/49669]\tLoss: 30035.6172\n",
      "Training Epoch: 2 [22600/49669]\tLoss: 32999.5273\n",
      "Training Epoch: 2 [22620/49669]\tLoss: 34138.2969\n",
      "Training Epoch: 2 [22640/49669]\tLoss: 29374.2891\n",
      "Training Epoch: 2 [22660/49669]\tLoss: 37425.7773\n",
      "Training Epoch: 2 [22680/49669]\tLoss: 31980.2559\n",
      "Training Epoch: 2 [22700/49669]\tLoss: 28708.7441\n",
      "Training Epoch: 2 [22720/49669]\tLoss: 35095.0898\n",
      "Training Epoch: 2 [22740/49669]\tLoss: 33858.6133\n",
      "Training Epoch: 2 [22760/49669]\tLoss: 30921.7891\n",
      "Training Epoch: 2 [22780/49669]\tLoss: 32120.3496\n",
      "Training Epoch: 2 [22800/49669]\tLoss: 23655.1055\n",
      "Training Epoch: 2 [22820/49669]\tLoss: 35191.7695\n",
      "Training Epoch: 2 [22840/49669]\tLoss: 29309.8984\n",
      "Training Epoch: 2 [22860/49669]\tLoss: 33789.4375\n",
      "Training Epoch: 2 [22880/49669]\tLoss: 30279.0117\n",
      "Training Epoch: 2 [22900/49669]\tLoss: 30722.9590\n",
      "Training Epoch: 2 [22920/49669]\tLoss: 30500.2891\n",
      "Training Epoch: 2 [22940/49669]\tLoss: 34097.7422\n",
      "Training Epoch: 2 [22960/49669]\tLoss: 33648.2500\n",
      "Training Epoch: 2 [22980/49669]\tLoss: 31620.5703\n",
      "Training Epoch: 2 [23000/49669]\tLoss: 33589.3281\n",
      "Training Epoch: 2 [23020/49669]\tLoss: 32156.6895\n",
      "Training Epoch: 2 [23040/49669]\tLoss: 31238.9102\n",
      "Training Epoch: 2 [23060/49669]\tLoss: 31788.6934\n",
      "Training Epoch: 2 [23080/49669]\tLoss: 34400.2969\n",
      "Training Epoch: 2 [23100/49669]\tLoss: 27656.4277\n",
      "Training Epoch: 2 [23120/49669]\tLoss: 33041.6523\n",
      "Training Epoch: 2 [23140/49669]\tLoss: 35779.1836\n",
      "Training Epoch: 2 [23160/49669]\tLoss: 33272.2070\n",
      "Training Epoch: 2 [23180/49669]\tLoss: 32018.7285\n",
      "Training Epoch: 2 [23200/49669]\tLoss: 27897.2305\n",
      "Training Epoch: 2 [23220/49669]\tLoss: 30031.1973\n",
      "Training Epoch: 2 [23240/49669]\tLoss: 35592.1953\n",
      "Training Epoch: 2 [23260/49669]\tLoss: 36889.8945\n",
      "Training Epoch: 2 [23280/49669]\tLoss: 33061.7578\n",
      "Training Epoch: 2 [23300/49669]\tLoss: 31683.3672\n",
      "Training Epoch: 2 [23320/49669]\tLoss: 26427.8320\n",
      "Training Epoch: 2 [23340/49669]\tLoss: 28812.5059\n",
      "Training Epoch: 2 [23360/49669]\tLoss: 25017.6797\n",
      "Training Epoch: 2 [23380/49669]\tLoss: 30900.6543\n",
      "Training Epoch: 2 [23400/49669]\tLoss: 32394.1445\n",
      "Training Epoch: 2 [23420/49669]\tLoss: 31655.1172\n",
      "Training Epoch: 2 [23440/49669]\tLoss: 29969.4141\n",
      "Training Epoch: 2 [23460/49669]\tLoss: 28221.0703\n",
      "Training Epoch: 2 [23480/49669]\tLoss: 30433.1172\n",
      "Training Epoch: 2 [23500/49669]\tLoss: 26372.4102\n",
      "Training Epoch: 2 [23520/49669]\tLoss: 33005.3477\n",
      "Training Epoch: 2 [23540/49669]\tLoss: 30950.9805\n",
      "Training Epoch: 2 [23560/49669]\tLoss: 31737.1738\n",
      "Training Epoch: 2 [23580/49669]\tLoss: 29765.0293\n",
      "Training Epoch: 2 [23600/49669]\tLoss: 33845.5117\n",
      "Training Epoch: 2 [23620/49669]\tLoss: 32978.3711\n",
      "Training Epoch: 2 [23640/49669]\tLoss: 29486.6934\n",
      "Training Epoch: 2 [23660/49669]\tLoss: 33313.4766\n",
      "Training Epoch: 2 [23680/49669]\tLoss: 33579.8359\n",
      "Training Epoch: 2 [23700/49669]\tLoss: 37805.3867\n",
      "Training Epoch: 2 [23720/49669]\tLoss: 37399.7773\n",
      "Training Epoch: 2 [23740/49669]\tLoss: 31589.0449\n",
      "Training Epoch: 2 [23760/49669]\tLoss: 25147.4785\n",
      "Training Epoch: 2 [23780/49669]\tLoss: 27517.6113\n",
      "Training Epoch: 2 [23800/49669]\tLoss: 32078.0039\n",
      "Training Epoch: 2 [23820/49669]\tLoss: 34510.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [23840/49669]\tLoss: 31593.8340\n",
      "Training Epoch: 2 [23860/49669]\tLoss: 31682.5410\n",
      "Training Epoch: 2 [23880/49669]\tLoss: 31035.0918\n",
      "Training Epoch: 2 [23900/49669]\tLoss: 32661.4727\n",
      "Training Epoch: 2 [23920/49669]\tLoss: 30821.3223\n",
      "Training Epoch: 2 [23940/49669]\tLoss: 33035.7266\n",
      "Training Epoch: 2 [23960/49669]\tLoss: 32681.4609\n",
      "Training Epoch: 2 [23980/49669]\tLoss: 32898.6328\n",
      "Training Epoch: 2 [24000/49669]\tLoss: 35691.2617\n",
      "Training Epoch: 2 [24020/49669]\tLoss: 30309.1348\n",
      "Training Epoch: 2 [24040/49669]\tLoss: 33050.1914\n",
      "Training Epoch: 2 [24060/49669]\tLoss: 26552.6914\n",
      "Training Epoch: 2 [24080/49669]\tLoss: 32123.6719\n",
      "Training Epoch: 2 [24100/49669]\tLoss: 33134.9297\n",
      "Training Epoch: 2 [24120/49669]\tLoss: 29683.3672\n",
      "Training Epoch: 2 [24140/49669]\tLoss: 33122.1719\n",
      "Training Epoch: 2 [24160/49669]\tLoss: 29899.6465\n",
      "Training Epoch: 2 [24180/49669]\tLoss: 30107.5977\n",
      "Training Epoch: 2 [24200/49669]\tLoss: 32722.6816\n",
      "Training Epoch: 2 [24220/49669]\tLoss: 28643.9824\n",
      "Training Epoch: 2 [24240/49669]\tLoss: 30738.3340\n",
      "Training Epoch: 2 [24260/49669]\tLoss: 32624.6797\n",
      "Training Epoch: 2 [24280/49669]\tLoss: 31250.6973\n",
      "Training Epoch: 2 [24300/49669]\tLoss: 28135.8164\n",
      "Training Epoch: 2 [24320/49669]\tLoss: 28299.4473\n",
      "Training Epoch: 2 [24340/49669]\tLoss: 28231.2852\n",
      "Training Epoch: 2 [24360/49669]\tLoss: 28663.2891\n",
      "Training Epoch: 2 [24380/49669]\tLoss: 31243.7188\n",
      "Training Epoch: 2 [24400/49669]\tLoss: 29368.4316\n",
      "Training Epoch: 2 [24420/49669]\tLoss: 29691.4355\n",
      "Training Epoch: 2 [24440/49669]\tLoss: 32789.2578\n",
      "Training Epoch: 2 [24460/49669]\tLoss: 29953.7344\n",
      "Training Epoch: 2 [24480/49669]\tLoss: 29817.4844\n",
      "Training Epoch: 2 [24500/49669]\tLoss: 28246.5586\n",
      "Training Epoch: 2 [24520/49669]\tLoss: 26565.1152\n",
      "Training Epoch: 2 [24540/49669]\tLoss: 32745.0410\n",
      "Training Epoch: 2 [24560/49669]\tLoss: 29354.5566\n",
      "Training Epoch: 2 [24580/49669]\tLoss: 30926.5605\n",
      "Training Epoch: 2 [24600/49669]\tLoss: 32122.6699\n",
      "Training Epoch: 2 [24620/49669]\tLoss: 33204.6367\n",
      "Training Epoch: 2 [24640/49669]\tLoss: 29942.9707\n",
      "Training Epoch: 2 [24660/49669]\tLoss: 28588.0098\n",
      "Training Epoch: 2 [24680/49669]\tLoss: 30204.7871\n",
      "Training Epoch: 2 [24700/49669]\tLoss: 30209.8496\n",
      "Training Epoch: 2 [24720/49669]\tLoss: 31027.2246\n",
      "Training Epoch: 2 [24740/49669]\tLoss: 27907.7930\n",
      "Training Epoch: 2 [24760/49669]\tLoss: 29860.9004\n",
      "Training Epoch: 2 [24780/49669]\tLoss: 32908.6055\n",
      "Training Epoch: 2 [24800/49669]\tLoss: 33985.1367\n",
      "Training Epoch: 2 [24820/49669]\tLoss: 28720.3457\n",
      "Training Epoch: 2 [24840/49669]\tLoss: 31524.0332\n",
      "Training Epoch: 2 [24860/49669]\tLoss: 24626.2969\n",
      "Training Epoch: 2 [24880/49669]\tLoss: 31515.8164\n",
      "Training Epoch: 2 [24900/49669]\tLoss: 30775.0137\n",
      "Training Epoch: 2 [24920/49669]\tLoss: 30845.8047\n",
      "Training Epoch: 2 [24940/49669]\tLoss: 35805.2188\n",
      "Training Epoch: 2 [24960/49669]\tLoss: 32600.0195\n",
      "Training Epoch: 2 [24980/49669]\tLoss: 32509.5078\n",
      "Training Epoch: 2 [25000/49669]\tLoss: 34627.9883\n",
      "Training Epoch: 2 [25020/49669]\tLoss: 31970.4785\n",
      "Training Epoch: 2 [25040/49669]\tLoss: 29305.5742\n",
      "Training Epoch: 2 [25060/49669]\tLoss: 28358.5859\n",
      "Training Epoch: 2 [25080/49669]\tLoss: 24690.8535\n",
      "Training Epoch: 2 [25100/49669]\tLoss: 31770.3887\n",
      "Training Epoch: 2 [25120/49669]\tLoss: 22881.8770\n",
      "Training Epoch: 2 [25140/49669]\tLoss: 29811.8711\n",
      "Training Epoch: 2 [25160/49669]\tLoss: 25262.4531\n",
      "Training Epoch: 2 [25180/49669]\tLoss: 31324.7207\n",
      "Training Epoch: 2 [25200/49669]\tLoss: 32003.1719\n",
      "Training Epoch: 2 [25220/49669]\tLoss: 31818.3477\n",
      "Training Epoch: 2 [25240/49669]\tLoss: 31048.9629\n",
      "Training Epoch: 2 [25260/49669]\tLoss: 30227.7109\n",
      "Training Epoch: 2 [25280/49669]\tLoss: 29483.0195\n",
      "Training Epoch: 2 [25300/49669]\tLoss: 33123.7695\n",
      "Training Epoch: 2 [25320/49669]\tLoss: 30481.4062\n",
      "Training Epoch: 2 [25340/49669]\tLoss: 29045.4902\n",
      "Training Epoch: 2 [25360/49669]\tLoss: 30734.6621\n",
      "Training Epoch: 2 [25380/49669]\tLoss: 27852.3867\n",
      "Training Epoch: 2 [25400/49669]\tLoss: 30164.6562\n",
      "Training Epoch: 2 [25420/49669]\tLoss: 28141.5352\n",
      "Training Epoch: 2 [25440/49669]\tLoss: 30357.7305\n",
      "Training Epoch: 2 [25460/49669]\tLoss: 29452.4551\n",
      "Training Epoch: 2 [25480/49669]\tLoss: 34321.2930\n",
      "Training Epoch: 2 [25500/49669]\tLoss: 30876.7090\n",
      "Training Epoch: 2 [25520/49669]\tLoss: 27322.1699\n",
      "Training Epoch: 2 [25540/49669]\tLoss: 27445.0977\n",
      "Training Epoch: 2 [25560/49669]\tLoss: 31793.6816\n",
      "Training Epoch: 2 [25580/49669]\tLoss: 31242.0020\n",
      "Training Epoch: 2 [25600/49669]\tLoss: 29263.8516\n",
      "Training Epoch: 2 [25620/49669]\tLoss: 29461.6934\n",
      "Training Epoch: 2 [25640/49669]\tLoss: 29203.6309\n",
      "Training Epoch: 2 [25660/49669]\tLoss: 29731.5391\n",
      "Training Epoch: 2 [25680/49669]\tLoss: 28508.3145\n",
      "Training Epoch: 2 [25700/49669]\tLoss: 30841.3301\n",
      "Training Epoch: 2 [25720/49669]\tLoss: 29288.9375\n",
      "Training Epoch: 2 [25740/49669]\tLoss: 30917.8496\n",
      "Training Epoch: 2 [25760/49669]\tLoss: 27820.7500\n",
      "Training Epoch: 2 [25780/49669]\tLoss: 27385.3027\n",
      "Training Epoch: 2 [25800/49669]\tLoss: 32954.7461\n",
      "Training Epoch: 2 [25820/49669]\tLoss: 31156.1816\n",
      "Training Epoch: 2 [25840/49669]\tLoss: 26077.0332\n",
      "Training Epoch: 2 [25860/49669]\tLoss: 24738.7500\n",
      "Training Epoch: 2 [25880/49669]\tLoss: 32726.7168\n",
      "Training Epoch: 2 [25900/49669]\tLoss: 29425.5098\n",
      "Training Epoch: 2 [25920/49669]\tLoss: 31018.8008\n",
      "Training Epoch: 2 [25940/49669]\tLoss: 28445.0020\n",
      "Training Epoch: 2 [25960/49669]\tLoss: 31029.7910\n",
      "Training Epoch: 2 [25980/49669]\tLoss: 32823.8555\n",
      "Training Epoch: 2 [26000/49669]\tLoss: 32186.9219\n",
      "Training Epoch: 2 [26020/49669]\tLoss: 28644.2207\n",
      "Training Epoch: 2 [26040/49669]\tLoss: 27208.5352\n",
      "Training Epoch: 2 [26060/49669]\tLoss: 32725.3242\n",
      "Training Epoch: 2 [26080/49669]\tLoss: 26671.3906\n",
      "Training Epoch: 2 [26100/49669]\tLoss: 30933.8613\n",
      "Training Epoch: 2 [26120/49669]\tLoss: 31897.9746\n",
      "Training Epoch: 2 [26140/49669]\tLoss: 27929.4863\n",
      "Training Epoch: 2 [26160/49669]\tLoss: 26553.3398\n",
      "Training Epoch: 2 [26180/49669]\tLoss: 28536.7754\n",
      "Training Epoch: 2 [26200/49669]\tLoss: 31418.5547\n",
      "Training Epoch: 2 [26220/49669]\tLoss: 27957.2324\n",
      "Training Epoch: 2 [26240/49669]\tLoss: 28244.6484\n",
      "Training Epoch: 2 [26260/49669]\tLoss: 29401.3066\n",
      "Training Epoch: 2 [26280/49669]\tLoss: 31073.9277\n",
      "Training Epoch: 2 [26300/49669]\tLoss: 29876.2285\n",
      "Training Epoch: 2 [26320/49669]\tLoss: 32083.0059\n",
      "Training Epoch: 2 [26340/49669]\tLoss: 31570.5410\n",
      "Training Epoch: 2 [26360/49669]\tLoss: 33342.7148\n",
      "Training Epoch: 2 [26380/49669]\tLoss: 30484.7539\n",
      "Training Epoch: 2 [26400/49669]\tLoss: 27392.6426\n",
      "Training Epoch: 2 [26420/49669]\tLoss: 26534.0000\n",
      "Training Epoch: 2 [26440/49669]\tLoss: 23777.3809\n",
      "Training Epoch: 2 [26460/49669]\tLoss: 27223.2773\n",
      "Training Epoch: 2 [26480/49669]\tLoss: 30651.1953\n",
      "Training Epoch: 2 [26500/49669]\tLoss: 28608.1172\n",
      "Training Epoch: 2 [26520/49669]\tLoss: 30768.3711\n",
      "Training Epoch: 2 [26540/49669]\tLoss: 26534.9238\n",
      "Training Epoch: 2 [26560/49669]\tLoss: 29495.0430\n",
      "Training Epoch: 2 [26580/49669]\tLoss: 29135.2129\n",
      "Training Epoch: 2 [26600/49669]\tLoss: 25935.0039\n",
      "Training Epoch: 2 [26620/49669]\tLoss: 31129.3613\n",
      "Training Epoch: 2 [26640/49669]\tLoss: 30513.2891\n",
      "Training Epoch: 2 [26660/49669]\tLoss: 28809.4648\n",
      "Training Epoch: 2 [26680/49669]\tLoss: 26965.8711\n",
      "Training Epoch: 2 [26700/49669]\tLoss: 30142.3691\n",
      "Training Epoch: 2 [26720/49669]\tLoss: 26099.7832\n",
      "Training Epoch: 2 [26740/49669]\tLoss: 31033.1992\n",
      "Training Epoch: 2 [26760/49669]\tLoss: 29337.0488\n",
      "Training Epoch: 2 [26780/49669]\tLoss: 29967.0586\n",
      "Training Epoch: 2 [26800/49669]\tLoss: 29023.4238\n",
      "Training Epoch: 2 [26820/49669]\tLoss: 24867.2363\n",
      "Training Epoch: 2 [26840/49669]\tLoss: 28451.6895\n",
      "Training Epoch: 2 [26860/49669]\tLoss: 27040.6426\n",
      "Training Epoch: 2 [26880/49669]\tLoss: 29323.1875\n",
      "Training Epoch: 2 [26900/49669]\tLoss: 27716.8789\n",
      "Training Epoch: 2 [26920/49669]\tLoss: 26369.8184\n",
      "Training Epoch: 2 [26940/49669]\tLoss: 29218.9492\n",
      "Training Epoch: 2 [26960/49669]\tLoss: 27652.7891\n",
      "Training Epoch: 2 [26980/49669]\tLoss: 25714.2402\n",
      "Training Epoch: 2 [27000/49669]\tLoss: 24891.1074\n",
      "Training Epoch: 2 [27020/49669]\tLoss: 26829.9980\n",
      "Training Epoch: 2 [27040/49669]\tLoss: 29713.0176\n",
      "Training Epoch: 2 [27060/49669]\tLoss: 28543.8496\n",
      "Training Epoch: 2 [27080/49669]\tLoss: 25026.8457\n",
      "Training Epoch: 2 [27100/49669]\tLoss: 31231.7363\n",
      "Training Epoch: 2 [27120/49669]\tLoss: 28378.2227\n",
      "Training Epoch: 2 [27140/49669]\tLoss: 23247.0039\n",
      "Training Epoch: 2 [27160/49669]\tLoss: 19944.9707\n",
      "Training Epoch: 2 [27180/49669]\tLoss: 31594.3574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [27200/49669]\tLoss: 30111.0723\n",
      "Training Epoch: 2 [27220/49669]\tLoss: 29670.4746\n",
      "Training Epoch: 2 [27240/49669]\tLoss: 26325.0215\n",
      "Training Epoch: 2 [27260/49669]\tLoss: 29879.8203\n",
      "Training Epoch: 2 [27280/49669]\tLoss: 31164.5488\n",
      "Training Epoch: 2 [27300/49669]\tLoss: 28407.7891\n",
      "Training Epoch: 2 [27320/49669]\tLoss: 25302.9043\n",
      "Training Epoch: 2 [27340/49669]\tLoss: 29531.3438\n",
      "Training Epoch: 2 [27360/49669]\tLoss: 27326.8496\n",
      "Training Epoch: 2 [27380/49669]\tLoss: 25605.4922\n",
      "Training Epoch: 2 [27400/49669]\tLoss: 27690.6348\n",
      "Training Epoch: 2 [27420/49669]\tLoss: 29337.8242\n",
      "Training Epoch: 2 [27440/49669]\tLoss: 29767.4473\n",
      "Training Epoch: 2 [27460/49669]\tLoss: 30295.0273\n",
      "Training Epoch: 2 [27480/49669]\tLoss: 28248.2246\n",
      "Training Epoch: 2 [27500/49669]\tLoss: 30020.8047\n",
      "Training Epoch: 2 [27520/49669]\tLoss: 30390.0723\n",
      "Training Epoch: 2 [27540/49669]\tLoss: 24798.8594\n",
      "Training Epoch: 2 [27560/49669]\tLoss: 24176.9141\n",
      "Training Epoch: 2 [27580/49669]\tLoss: 29652.0488\n",
      "Training Epoch: 2 [27600/49669]\tLoss: 29282.7598\n",
      "Training Epoch: 2 [27620/49669]\tLoss: 25436.4453\n",
      "Training Epoch: 2 [27640/49669]\tLoss: 29731.6328\n",
      "Training Epoch: 2 [27660/49669]\tLoss: 27777.3789\n",
      "Training Epoch: 2 [27680/49669]\tLoss: 30073.3242\n",
      "Training Epoch: 2 [27700/49669]\tLoss: 27007.3691\n",
      "Training Epoch: 2 [27720/49669]\tLoss: 27379.8574\n",
      "Training Epoch: 2 [27740/49669]\tLoss: 28185.9609\n",
      "Training Epoch: 2 [27760/49669]\tLoss: 26318.3906\n",
      "Training Epoch: 2 [27780/49669]\tLoss: 27340.8438\n",
      "Training Epoch: 2 [27800/49669]\tLoss: 30485.7402\n",
      "Training Epoch: 2 [27820/49669]\tLoss: 21213.8066\n",
      "Training Epoch: 2 [27840/49669]\tLoss: 30981.7500\n",
      "Training Epoch: 2 [27860/49669]\tLoss: 25627.5527\n",
      "Training Epoch: 2 [27880/49669]\tLoss: 25403.3379\n",
      "Training Epoch: 2 [27900/49669]\tLoss: 28157.2090\n",
      "Training Epoch: 2 [27920/49669]\tLoss: 26674.2129\n",
      "Training Epoch: 2 [27940/49669]\tLoss: 30062.1504\n",
      "Training Epoch: 2 [27960/49669]\tLoss: 26293.7754\n",
      "Training Epoch: 2 [27980/49669]\tLoss: 29129.8574\n",
      "Training Epoch: 2 [28000/49669]\tLoss: 29468.4707\n",
      "Training Epoch: 2 [28020/49669]\tLoss: 26842.4492\n",
      "Training Epoch: 2 [28040/49669]\tLoss: 27696.7012\n",
      "Training Epoch: 2 [28060/49669]\tLoss: 23428.5352\n",
      "Training Epoch: 2 [28080/49669]\tLoss: 24901.7676\n",
      "Training Epoch: 2 [28100/49669]\tLoss: 24215.2461\n",
      "Training Epoch: 2 [28120/49669]\tLoss: 26543.4277\n",
      "Training Epoch: 2 [28140/49669]\tLoss: 29607.2852\n",
      "Training Epoch: 2 [28160/49669]\tLoss: 29203.2246\n",
      "Training Epoch: 2 [28180/49669]\tLoss: 29030.4434\n",
      "Training Epoch: 2 [28200/49669]\tLoss: 29235.7363\n",
      "Training Epoch: 2 [28220/49669]\tLoss: 24049.6289\n",
      "Training Epoch: 2 [28240/49669]\tLoss: 24514.6602\n",
      "Training Epoch: 2 [28260/49669]\tLoss: 28782.0137\n",
      "Training Epoch: 2 [28280/49669]\tLoss: 26255.8301\n",
      "Training Epoch: 2 [28300/49669]\tLoss: 29719.1895\n",
      "Training Epoch: 2 [28320/49669]\tLoss: 26420.6953\n",
      "Training Epoch: 2 [28340/49669]\tLoss: 29528.1621\n",
      "Training Epoch: 2 [28360/49669]\tLoss: 29265.9512\n",
      "Training Epoch: 2 [28380/49669]\tLoss: 25521.0059\n",
      "Training Epoch: 2 [28400/49669]\tLoss: 25574.3809\n",
      "Training Epoch: 2 [28420/49669]\tLoss: 26165.5273\n",
      "Training Epoch: 2 [28440/49669]\tLoss: 27608.5039\n",
      "Training Epoch: 2 [28460/49669]\tLoss: 32144.8477\n",
      "Training Epoch: 2 [28480/49669]\tLoss: 22725.5176\n",
      "Training Epoch: 2 [28500/49669]\tLoss: 23884.6680\n",
      "Training Epoch: 2 [28520/49669]\tLoss: 25767.7910\n",
      "Training Epoch: 2 [28540/49669]\tLoss: 30366.8418\n",
      "Training Epoch: 2 [28560/49669]\tLoss: 28018.6035\n",
      "Training Epoch: 2 [28580/49669]\tLoss: 28248.1562\n",
      "Training Epoch: 2 [28600/49669]\tLoss: 32542.1699\n",
      "Training Epoch: 2 [28620/49669]\tLoss: 25070.3457\n",
      "Training Epoch: 2 [28640/49669]\tLoss: 29062.6172\n",
      "Training Epoch: 2 [28660/49669]\tLoss: 25273.4980\n",
      "Training Epoch: 2 [28680/49669]\tLoss: 30139.7305\n",
      "Training Epoch: 2 [28700/49669]\tLoss: 22807.1523\n",
      "Training Epoch: 2 [28720/49669]\tLoss: 28268.3145\n",
      "Training Epoch: 2 [28740/49669]\tLoss: 25857.2891\n",
      "Training Epoch: 2 [28760/49669]\tLoss: 27065.5312\n",
      "Training Epoch: 2 [28780/49669]\tLoss: 24860.7812\n",
      "Training Epoch: 2 [28800/49669]\tLoss: 26313.1836\n",
      "Training Epoch: 2 [28820/49669]\tLoss: 27755.6875\n",
      "Training Epoch: 2 [28840/49669]\tLoss: 29251.0547\n",
      "Training Epoch: 2 [28860/49669]\tLoss: 23787.8164\n",
      "Training Epoch: 2 [28880/49669]\tLoss: 29795.0996\n",
      "Training Epoch: 2 [28900/49669]\tLoss: 28283.1855\n",
      "Training Epoch: 2 [28920/49669]\tLoss: 28608.9141\n",
      "Training Epoch: 2 [28940/49669]\tLoss: 24948.4727\n",
      "Training Epoch: 2 [28960/49669]\tLoss: 25846.0684\n",
      "Training Epoch: 2 [28980/49669]\tLoss: 26097.6289\n",
      "Training Epoch: 2 [29000/49669]\tLoss: 31083.1016\n",
      "Training Epoch: 2 [29020/49669]\tLoss: 28917.0254\n",
      "Training Epoch: 2 [29040/49669]\tLoss: 26865.2832\n",
      "Training Epoch: 2 [29060/49669]\tLoss: 26573.7109\n",
      "Training Epoch: 2 [29080/49669]\tLoss: 24710.4492\n",
      "Training Epoch: 2 [29100/49669]\tLoss: 25292.5625\n",
      "Training Epoch: 2 [29120/49669]\tLoss: 25397.0430\n",
      "Training Epoch: 2 [29140/49669]\tLoss: 27528.8633\n",
      "Training Epoch: 2 [29160/49669]\tLoss: 29169.5312\n",
      "Training Epoch: 2 [29180/49669]\tLoss: 30593.1113\n",
      "Training Epoch: 2 [29200/49669]\tLoss: 29119.1504\n",
      "Training Epoch: 2 [29220/49669]\tLoss: 25256.2168\n",
      "Training Epoch: 2 [29240/49669]\tLoss: 26800.4453\n",
      "Training Epoch: 2 [29260/49669]\tLoss: 26103.1055\n",
      "Training Epoch: 2 [29280/49669]\tLoss: 27240.0605\n",
      "Training Epoch: 2 [29300/49669]\tLoss: 27607.5879\n",
      "Training Epoch: 2 [29320/49669]\tLoss: 26535.9766\n",
      "Training Epoch: 2 [29340/49669]\tLoss: 25835.8027\n",
      "Training Epoch: 2 [29360/49669]\tLoss: 25853.2148\n",
      "Training Epoch: 2 [29380/49669]\tLoss: 27176.4316\n",
      "Training Epoch: 2 [29400/49669]\tLoss: 27748.9941\n",
      "Training Epoch: 2 [29420/49669]\tLoss: 24692.0879\n",
      "Training Epoch: 2 [29440/49669]\tLoss: 28529.0703\n",
      "Training Epoch: 2 [29460/49669]\tLoss: 22808.5254\n",
      "Training Epoch: 2 [29480/49669]\tLoss: 26382.0547\n",
      "Training Epoch: 2 [29500/49669]\tLoss: 27735.6230\n",
      "Training Epoch: 2 [29520/49669]\tLoss: 27230.9844\n",
      "Training Epoch: 2 [29540/49669]\tLoss: 27725.5977\n",
      "Training Epoch: 2 [29560/49669]\tLoss: 27155.9551\n",
      "Training Epoch: 2 [29580/49669]\tLoss: 26206.8730\n",
      "Training Epoch: 2 [29600/49669]\tLoss: 26453.4414\n",
      "Training Epoch: 2 [29620/49669]\tLoss: 23552.2363\n",
      "Training Epoch: 2 [29640/49669]\tLoss: 27873.8105\n",
      "Training Epoch: 2 [29660/49669]\tLoss: 23647.7988\n",
      "Training Epoch: 2 [29680/49669]\tLoss: 27238.4434\n",
      "Training Epoch: 2 [29700/49669]\tLoss: 26764.5938\n",
      "Training Epoch: 2 [29720/49669]\tLoss: 26536.8711\n",
      "Training Epoch: 2 [29740/49669]\tLoss: 25714.5684\n",
      "Training Epoch: 2 [29760/49669]\tLoss: 23905.4570\n",
      "Training Epoch: 2 [29780/49669]\tLoss: 25412.9355\n",
      "Training Epoch: 2 [29800/49669]\tLoss: 25083.7695\n",
      "Training Epoch: 2 [29820/49669]\tLoss: 25941.7852\n",
      "Training Epoch: 2 [29840/49669]\tLoss: 30360.1621\n",
      "Training Epoch: 2 [29860/49669]\tLoss: 28245.0605\n",
      "Training Epoch: 2 [29880/49669]\tLoss: 27098.3633\n",
      "Training Epoch: 2 [29900/49669]\tLoss: 26379.1641\n",
      "Training Epoch: 2 [29920/49669]\tLoss: 27891.9258\n",
      "Training Epoch: 2 [29940/49669]\tLoss: 27175.3926\n",
      "Training Epoch: 2 [29960/49669]\tLoss: 28475.5469\n",
      "Training Epoch: 2 [29980/49669]\tLoss: 24284.9336\n",
      "Training Epoch: 2 [30000/49669]\tLoss: 28852.2734\n",
      "Training Epoch: 2 [30020/49669]\tLoss: 23087.9766\n",
      "Training Epoch: 2 [30040/49669]\tLoss: 21403.3633\n",
      "Training Epoch: 2 [30060/49669]\tLoss: 29069.3789\n",
      "Training Epoch: 2 [30080/49669]\tLoss: 18551.5664\n",
      "Training Epoch: 2 [30100/49669]\tLoss: 27419.3281\n",
      "Training Epoch: 2 [30120/49669]\tLoss: 27396.8379\n",
      "Training Epoch: 2 [30140/49669]\tLoss: 26568.0410\n",
      "Training Epoch: 2 [30160/49669]\tLoss: 27411.3672\n",
      "Training Epoch: 2 [30180/49669]\tLoss: 28975.0586\n",
      "Training Epoch: 2 [30200/49669]\tLoss: 26761.6328\n",
      "Training Epoch: 2 [30220/49669]\tLoss: 26447.3789\n",
      "Training Epoch: 2 [30240/49669]\tLoss: 23726.0039\n",
      "Training Epoch: 2 [30260/49669]\tLoss: 25396.1191\n",
      "Training Epoch: 2 [30280/49669]\tLoss: 25480.6289\n",
      "Training Epoch: 2 [30300/49669]\tLoss: 26202.8691\n",
      "Training Epoch: 2 [30320/49669]\tLoss: 22004.5195\n",
      "Training Epoch: 2 [30340/49669]\tLoss: 26335.8027\n",
      "Training Epoch: 2 [30360/49669]\tLoss: 22627.3301\n",
      "Training Epoch: 2 [30380/49669]\tLoss: 22222.6289\n",
      "Training Epoch: 2 [30400/49669]\tLoss: 26341.2871\n",
      "Training Epoch: 2 [30420/49669]\tLoss: 25156.2734\n",
      "Training Epoch: 2 [30440/49669]\tLoss: 29427.4922\n",
      "Training Epoch: 2 [30460/49669]\tLoss: 25326.8145\n",
      "Training Epoch: 2 [30480/49669]\tLoss: 26552.4004\n",
      "Training Epoch: 2 [30500/49669]\tLoss: 27561.1250\n",
      "Training Epoch: 2 [30520/49669]\tLoss: 25840.0000\n",
      "Training Epoch: 2 [30540/49669]\tLoss: 28727.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [30560/49669]\tLoss: 29109.9785\n",
      "Training Epoch: 2 [30580/49669]\tLoss: 23735.1445\n",
      "Training Epoch: 2 [30600/49669]\tLoss: 29390.5156\n",
      "Training Epoch: 2 [30620/49669]\tLoss: 24122.9453\n",
      "Training Epoch: 2 [30640/49669]\tLoss: 23315.0469\n",
      "Training Epoch: 2 [30660/49669]\tLoss: 27934.9375\n",
      "Training Epoch: 2 [30680/49669]\tLoss: 27353.9453\n",
      "Training Epoch: 2 [30700/49669]\tLoss: 26067.7637\n",
      "Training Epoch: 2 [30720/49669]\tLoss: 24924.4375\n",
      "Training Epoch: 2 [30740/49669]\tLoss: 20827.8477\n",
      "Training Epoch: 2 [30760/49669]\tLoss: 25257.1934\n",
      "Training Epoch: 2 [30780/49669]\tLoss: 27435.7637\n",
      "Training Epoch: 2 [30800/49669]\tLoss: 21799.0645\n",
      "Training Epoch: 2 [30820/49669]\tLoss: 25168.7480\n",
      "Training Epoch: 2 [30840/49669]\tLoss: 26828.4785\n",
      "Training Epoch: 2 [30860/49669]\tLoss: 24428.5098\n",
      "Training Epoch: 2 [30880/49669]\tLoss: 25319.6895\n",
      "Training Epoch: 2 [30900/49669]\tLoss: 29481.6484\n",
      "Training Epoch: 2 [30920/49669]\tLoss: 25071.2695\n",
      "Training Epoch: 2 [30940/49669]\tLoss: 26604.9844\n",
      "Training Epoch: 2 [30960/49669]\tLoss: 28856.0703\n",
      "Training Epoch: 2 [30980/49669]\tLoss: 27684.5879\n",
      "Training Epoch: 2 [31000/49669]\tLoss: 23115.5625\n",
      "Training Epoch: 2 [31020/49669]\tLoss: 24026.3867\n",
      "Training Epoch: 2 [31040/49669]\tLoss: 25008.5918\n",
      "Training Epoch: 2 [31060/49669]\tLoss: 28806.5000\n",
      "Training Epoch: 2 [31080/49669]\tLoss: 23537.3457\n",
      "Training Epoch: 2 [31100/49669]\tLoss: 22991.0137\n",
      "Training Epoch: 2 [31120/49669]\tLoss: 28901.3262\n",
      "Training Epoch: 2 [31140/49669]\tLoss: 25786.7832\n",
      "Training Epoch: 2 [31160/49669]\tLoss: 25557.2969\n",
      "Training Epoch: 2 [31180/49669]\tLoss: 26001.1738\n",
      "Training Epoch: 2 [31200/49669]\tLoss: 26031.1191\n",
      "Training Epoch: 2 [31220/49669]\tLoss: 24432.7852\n",
      "Training Epoch: 2 [31240/49669]\tLoss: 29511.4883\n",
      "Training Epoch: 2 [31260/49669]\tLoss: 26740.7734\n",
      "Training Epoch: 2 [31280/49669]\tLoss: 29573.2246\n",
      "Training Epoch: 2 [31300/49669]\tLoss: 27757.9355\n",
      "Training Epoch: 2 [31320/49669]\tLoss: 26085.9629\n",
      "Training Epoch: 2 [31340/49669]\tLoss: 28713.6836\n",
      "Training Epoch: 2 [31360/49669]\tLoss: 24897.9219\n",
      "Training Epoch: 2 [31380/49669]\tLoss: 26113.5156\n",
      "Training Epoch: 2 [31400/49669]\tLoss: 23527.9141\n",
      "Training Epoch: 2 [31420/49669]\tLoss: 24713.6289\n",
      "Training Epoch: 2 [31440/49669]\tLoss: 26229.6973\n",
      "Training Epoch: 2 [31460/49669]\tLoss: 26502.7480\n",
      "Training Epoch: 2 [31480/49669]\tLoss: 22869.8301\n",
      "Training Epoch: 2 [31500/49669]\tLoss: 24469.6875\n",
      "Training Epoch: 2 [31520/49669]\tLoss: 22298.6855\n",
      "Training Epoch: 2 [31540/49669]\tLoss: 27402.5508\n",
      "Training Epoch: 2 [31560/49669]\tLoss: 23530.2520\n",
      "Training Epoch: 2 [31580/49669]\tLoss: 24564.9473\n",
      "Training Epoch: 2 [31600/49669]\tLoss: 24648.5762\n",
      "Training Epoch: 2 [31620/49669]\tLoss: 25248.0781\n",
      "Training Epoch: 2 [31640/49669]\tLoss: 25177.7246\n",
      "Training Epoch: 2 [31660/49669]\tLoss: 24465.5820\n",
      "Training Epoch: 2 [31680/49669]\tLoss: 21547.9648\n",
      "Training Epoch: 2 [31700/49669]\tLoss: 21286.0918\n",
      "Training Epoch: 2 [31720/49669]\tLoss: 25431.0430\n",
      "Training Epoch: 2 [31740/49669]\tLoss: 25102.5703\n",
      "Training Epoch: 2 [31760/49669]\tLoss: 21814.0176\n",
      "Training Epoch: 2 [31780/49669]\tLoss: 26480.7148\n",
      "Training Epoch: 2 [31800/49669]\tLoss: 24180.9199\n",
      "Training Epoch: 2 [31820/49669]\tLoss: 23716.7422\n",
      "Training Epoch: 2 [31840/49669]\tLoss: 24902.5234\n",
      "Training Epoch: 2 [31860/49669]\tLoss: 25751.4805\n",
      "Training Epoch: 2 [31880/49669]\tLoss: 28033.3633\n",
      "Training Epoch: 2 [31900/49669]\tLoss: 27311.3008\n",
      "Training Epoch: 2 [31920/49669]\tLoss: 26726.4082\n",
      "Training Epoch: 2 [31940/49669]\tLoss: 22939.6855\n",
      "Training Epoch: 2 [31960/49669]\tLoss: 26377.9902\n",
      "Training Epoch: 2 [31980/49669]\tLoss: 25810.8281\n",
      "Training Epoch: 2 [32000/49669]\tLoss: 28048.7012\n",
      "Training Epoch: 2 [32020/49669]\tLoss: 25634.4746\n",
      "Training Epoch: 2 [32040/49669]\tLoss: 24276.5664\n",
      "Training Epoch: 2 [32060/49669]\tLoss: 24012.3926\n",
      "Training Epoch: 2 [32080/49669]\tLoss: 27649.4062\n",
      "Training Epoch: 2 [32100/49669]\tLoss: 27522.2402\n",
      "Training Epoch: 2 [32120/49669]\tLoss: 26534.5684\n",
      "Training Epoch: 2 [32140/49669]\tLoss: 25464.5488\n",
      "Training Epoch: 2 [32160/49669]\tLoss: 24103.7754\n",
      "Training Epoch: 2 [32180/49669]\tLoss: 23940.2520\n",
      "Training Epoch: 2 [32200/49669]\tLoss: 25534.4609\n",
      "Training Epoch: 2 [32220/49669]\tLoss: 22366.6699\n",
      "Training Epoch: 2 [32240/49669]\tLoss: 22467.6855\n",
      "Training Epoch: 2 [32260/49669]\tLoss: 25212.1406\n",
      "Training Epoch: 2 [32280/49669]\tLoss: 24134.4453\n",
      "Training Epoch: 2 [32300/49669]\tLoss: 22336.7637\n",
      "Training Epoch: 2 [32320/49669]\tLoss: 25997.8164\n",
      "Training Epoch: 2 [32340/49669]\tLoss: 27180.0723\n",
      "Training Epoch: 2 [32360/49669]\tLoss: 19111.4492\n",
      "Training Epoch: 2 [32380/49669]\tLoss: 22699.2070\n",
      "Training Epoch: 2 [32400/49669]\tLoss: 25358.4102\n",
      "Training Epoch: 2 [32420/49669]\tLoss: 22502.6738\n",
      "Training Epoch: 2 [32440/49669]\tLoss: 22815.2754\n",
      "Training Epoch: 2 [32460/49669]\tLoss: 22608.3047\n",
      "Training Epoch: 2 [32480/49669]\tLoss: 27356.9277\n",
      "Training Epoch: 2 [32500/49669]\tLoss: 27056.3086\n",
      "Training Epoch: 2 [32520/49669]\tLoss: 26879.8398\n",
      "Training Epoch: 2 [32540/49669]\tLoss: 25045.1816\n",
      "Training Epoch: 2 [32560/49669]\tLoss: 24415.7598\n",
      "Training Epoch: 2 [32580/49669]\tLoss: 25486.1914\n",
      "Training Epoch: 2 [32600/49669]\tLoss: 21611.0781\n",
      "Training Epoch: 2 [32620/49669]\tLoss: 25081.8867\n",
      "Training Epoch: 2 [32640/49669]\tLoss: 24118.2930\n",
      "Training Epoch: 2 [32660/49669]\tLoss: 26928.2832\n",
      "Training Epoch: 2 [32680/49669]\tLoss: 25620.9570\n",
      "Training Epoch: 2 [32700/49669]\tLoss: 23234.3027\n",
      "Training Epoch: 2 [32720/49669]\tLoss: 27053.8457\n",
      "Training Epoch: 2 [32740/49669]\tLoss: 23125.6836\n",
      "Training Epoch: 2 [32760/49669]\tLoss: 25210.6270\n",
      "Training Epoch: 2 [32780/49669]\tLoss: 25137.4102\n",
      "Training Epoch: 2 [32800/49669]\tLoss: 21112.8086\n",
      "Training Epoch: 2 [32820/49669]\tLoss: 25382.7422\n",
      "Training Epoch: 2 [32840/49669]\tLoss: 21084.0156\n",
      "Training Epoch: 2 [32860/49669]\tLoss: 27928.3750\n",
      "Training Epoch: 2 [32880/49669]\tLoss: 25277.3809\n",
      "Training Epoch: 2 [32900/49669]\tLoss: 24963.2500\n",
      "Training Epoch: 2 [32920/49669]\tLoss: 23045.6035\n",
      "Training Epoch: 2 [32940/49669]\tLoss: 23811.5176\n",
      "Training Epoch: 2 [32960/49669]\tLoss: 23159.8320\n",
      "Training Epoch: 2 [32980/49669]\tLoss: 22004.6426\n",
      "Training Epoch: 2 [33000/49669]\tLoss: 25271.9863\n",
      "Training Epoch: 2 [33020/49669]\tLoss: 27675.4473\n",
      "Training Epoch: 2 [33040/49669]\tLoss: 26103.0781\n",
      "Training Epoch: 2 [33060/49669]\tLoss: 23250.1328\n",
      "Training Epoch: 2 [33080/49669]\tLoss: 25617.5547\n",
      "Training Epoch: 2 [33100/49669]\tLoss: 24339.7559\n",
      "Training Epoch: 2 [33120/49669]\tLoss: 19462.2461\n",
      "Training Epoch: 2 [33140/49669]\tLoss: 23761.2520\n",
      "Training Epoch: 2 [33160/49669]\tLoss: 21845.8145\n",
      "Training Epoch: 2 [33180/49669]\tLoss: 22669.2090\n",
      "Training Epoch: 2 [33200/49669]\tLoss: 25300.9980\n",
      "Training Epoch: 2 [33220/49669]\tLoss: 24010.4043\n",
      "Training Epoch: 2 [33240/49669]\tLoss: 23377.2129\n",
      "Training Epoch: 2 [33260/49669]\tLoss: 27823.8828\n",
      "Training Epoch: 2 [33280/49669]\tLoss: 24446.3008\n",
      "Training Epoch: 2 [33300/49669]\tLoss: 27540.7891\n",
      "Training Epoch: 2 [33320/49669]\tLoss: 26454.3027\n",
      "Training Epoch: 2 [33340/49669]\tLoss: 20174.0098\n",
      "Training Epoch: 2 [33360/49669]\tLoss: 24186.7109\n",
      "Training Epoch: 2 [33380/49669]\tLoss: 22220.2441\n",
      "Training Epoch: 2 [33400/49669]\tLoss: 24770.7598\n",
      "Training Epoch: 2 [33420/49669]\tLoss: 25556.8340\n",
      "Training Epoch: 2 [33440/49669]\tLoss: 21208.0586\n",
      "Training Epoch: 2 [33460/49669]\tLoss: 25779.6191\n",
      "Training Epoch: 2 [33480/49669]\tLoss: 25331.1680\n",
      "Training Epoch: 2 [33500/49669]\tLoss: 21886.3750\n",
      "Training Epoch: 2 [33520/49669]\tLoss: 22593.0742\n",
      "Training Epoch: 2 [33540/49669]\tLoss: 24849.7500\n",
      "Training Epoch: 2 [33560/49669]\tLoss: 22278.8516\n",
      "Training Epoch: 2 [33580/49669]\tLoss: 25888.6465\n",
      "Training Epoch: 2 [33600/49669]\tLoss: 20928.2324\n",
      "Training Epoch: 2 [33620/49669]\tLoss: 23153.2441\n",
      "Training Epoch: 2 [33640/49669]\tLoss: 26652.6582\n",
      "Training Epoch: 2 [33660/49669]\tLoss: 20881.0859\n",
      "Training Epoch: 2 [33680/49669]\tLoss: 26620.8203\n",
      "Training Epoch: 2 [33700/49669]\tLoss: 28519.3613\n",
      "Training Epoch: 2 [33720/49669]\tLoss: 23695.3262\n",
      "Training Epoch: 2 [33740/49669]\tLoss: 21919.2266\n",
      "Training Epoch: 2 [33760/49669]\tLoss: 23188.0039\n",
      "Training Epoch: 2 [33780/49669]\tLoss: 25309.3652\n",
      "Training Epoch: 2 [33800/49669]\tLoss: 24859.8066\n",
      "Training Epoch: 2 [33820/49669]\tLoss: 23529.7676\n",
      "Training Epoch: 2 [33840/49669]\tLoss: 24348.0430\n",
      "Training Epoch: 2 [33860/49669]\tLoss: 23872.2363\n",
      "Training Epoch: 2 [33880/49669]\tLoss: 24913.7812\n",
      "Training Epoch: 2 [33900/49669]\tLoss: 23946.5547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [33920/49669]\tLoss: 25602.4023\n",
      "Training Epoch: 2 [33940/49669]\tLoss: 26425.0254\n",
      "Training Epoch: 2 [33960/49669]\tLoss: 25475.9395\n",
      "Training Epoch: 2 [33980/49669]\tLoss: 25147.8984\n",
      "Training Epoch: 2 [34000/49669]\tLoss: 24844.7539\n",
      "Training Epoch: 2 [34020/49669]\tLoss: 26752.0371\n",
      "Training Epoch: 2 [34040/49669]\tLoss: 21829.9941\n",
      "Training Epoch: 2 [34060/49669]\tLoss: 24693.0059\n",
      "Training Epoch: 2 [34080/49669]\tLoss: 26666.6074\n",
      "Training Epoch: 2 [34100/49669]\tLoss: 24884.1973\n",
      "Training Epoch: 2 [34120/49669]\tLoss: 20494.0859\n",
      "Training Epoch: 2 [34140/49669]\tLoss: 22655.8203\n",
      "Training Epoch: 2 [34160/49669]\tLoss: 24572.8145\n",
      "Training Epoch: 2 [34180/49669]\tLoss: 25750.0723\n",
      "Training Epoch: 2 [34200/49669]\tLoss: 25657.4199\n",
      "Training Epoch: 2 [34220/49669]\tLoss: 23502.4570\n",
      "Training Epoch: 2 [34240/49669]\tLoss: 25484.2598\n",
      "Training Epoch: 2 [34260/49669]\tLoss: 24785.3027\n",
      "Training Epoch: 2 [34280/49669]\tLoss: 24930.4434\n",
      "Training Epoch: 2 [34300/49669]\tLoss: 25857.7500\n",
      "Training Epoch: 2 [34320/49669]\tLoss: 23969.1172\n",
      "Training Epoch: 2 [34340/49669]\tLoss: 22188.1992\n",
      "Training Epoch: 2 [34360/49669]\tLoss: 23537.0293\n",
      "Training Epoch: 2 [34380/49669]\tLoss: 24494.3438\n",
      "Training Epoch: 2 [34400/49669]\tLoss: 24721.6758\n",
      "Training Epoch: 2 [34420/49669]\tLoss: 24094.1582\n",
      "Training Epoch: 2 [34440/49669]\tLoss: 22136.5254\n",
      "Training Epoch: 2 [34460/49669]\tLoss: 20683.7812\n",
      "Training Epoch: 2 [34480/49669]\tLoss: 25806.9434\n",
      "Training Epoch: 2 [34500/49669]\tLoss: 22226.6152\n",
      "Training Epoch: 2 [34520/49669]\tLoss: 25142.6543\n",
      "Training Epoch: 2 [34540/49669]\tLoss: 23362.9844\n",
      "Training Epoch: 2 [34560/49669]\tLoss: 24435.2266\n",
      "Training Epoch: 2 [34580/49669]\tLoss: 22679.3555\n",
      "Training Epoch: 2 [34600/49669]\tLoss: 20631.7285\n",
      "Training Epoch: 2 [34620/49669]\tLoss: 22299.0039\n",
      "Training Epoch: 2 [34640/49669]\tLoss: 22190.5156\n",
      "Training Epoch: 2 [34660/49669]\tLoss: 22370.6074\n",
      "Training Epoch: 2 [34680/49669]\tLoss: 22721.9160\n",
      "Training Epoch: 2 [34700/49669]\tLoss: 23224.0918\n",
      "Training Epoch: 2 [34720/49669]\tLoss: 19829.8125\n",
      "Training Epoch: 2 [34740/49669]\tLoss: 23272.8848\n",
      "Training Epoch: 2 [34760/49669]\tLoss: 22799.2969\n",
      "Training Epoch: 2 [34780/49669]\tLoss: 23904.1875\n",
      "Training Epoch: 2 [34800/49669]\tLoss: 23492.4004\n",
      "Training Epoch: 2 [34820/49669]\tLoss: 23805.3418\n",
      "Training Epoch: 2 [34840/49669]\tLoss: 25027.6465\n",
      "Training Epoch: 2 [34860/49669]\tLoss: 24950.7637\n",
      "Training Epoch: 2 [34880/49669]\tLoss: 25222.7910\n",
      "Training Epoch: 2 [34900/49669]\tLoss: 26377.2910\n",
      "Training Epoch: 2 [34920/49669]\tLoss: 25540.2578\n",
      "Training Epoch: 2 [34940/49669]\tLoss: 21990.3125\n",
      "Training Epoch: 2 [34960/49669]\tLoss: 21312.8008\n",
      "Training Epoch: 2 [34980/49669]\tLoss: 23987.0898\n",
      "Training Epoch: 2 [35000/49669]\tLoss: 23279.0527\n",
      "Training Epoch: 2 [35020/49669]\tLoss: 20396.5410\n",
      "Training Epoch: 2 [35040/49669]\tLoss: 21143.7383\n",
      "Training Epoch: 2 [35060/49669]\tLoss: 24842.3516\n",
      "Training Epoch: 2 [35080/49669]\tLoss: 26053.7461\n",
      "Training Epoch: 2 [35100/49669]\tLoss: 26024.3398\n",
      "Training Epoch: 2 [35120/49669]\tLoss: 19981.5234\n",
      "Training Epoch: 2 [35140/49669]\tLoss: 23363.4297\n",
      "Training Epoch: 2 [35160/49669]\tLoss: 23679.1152\n",
      "Training Epoch: 2 [35180/49669]\tLoss: 20489.9160\n",
      "Training Epoch: 2 [35200/49669]\tLoss: 23706.0469\n",
      "Training Epoch: 2 [35220/49669]\tLoss: 24217.9180\n",
      "Training Epoch: 2 [35240/49669]\tLoss: 25013.5137\n",
      "Training Epoch: 2 [35260/49669]\tLoss: 23989.0254\n",
      "Training Epoch: 2 [35280/49669]\tLoss: 20163.1875\n",
      "Training Epoch: 2 [35300/49669]\tLoss: 23717.3242\n",
      "Training Epoch: 2 [35320/49669]\tLoss: 22628.3691\n",
      "Training Epoch: 2 [35340/49669]\tLoss: 23095.2539\n",
      "Training Epoch: 2 [35360/49669]\tLoss: 25577.5938\n",
      "Training Epoch: 2 [35380/49669]\tLoss: 19061.4727\n",
      "Training Epoch: 2 [35400/49669]\tLoss: 26673.4375\n",
      "Training Epoch: 2 [35420/49669]\tLoss: 23371.5352\n",
      "Training Epoch: 2 [35440/49669]\tLoss: 21584.5898\n",
      "Training Epoch: 2 [35460/49669]\tLoss: 21947.4551\n",
      "Training Epoch: 2 [35480/49669]\tLoss: 24779.4336\n",
      "Training Epoch: 2 [35500/49669]\tLoss: 21223.9570\n",
      "Training Epoch: 2 [35520/49669]\tLoss: 22907.2480\n",
      "Training Epoch: 2 [35540/49669]\tLoss: 20609.4629\n",
      "Training Epoch: 2 [35560/49669]\tLoss: 21422.2305\n",
      "Training Epoch: 2 [35580/49669]\tLoss: 25099.8320\n",
      "Training Epoch: 2 [35600/49669]\tLoss: 24150.8809\n",
      "Training Epoch: 2 [35620/49669]\tLoss: 23590.9980\n",
      "Training Epoch: 2 [35640/49669]\tLoss: 22277.1582\n",
      "Training Epoch: 2 [35660/49669]\tLoss: 22900.6445\n",
      "Training Epoch: 2 [35680/49669]\tLoss: 23248.2051\n",
      "Training Epoch: 2 [35700/49669]\tLoss: 22840.6289\n",
      "Training Epoch: 2 [35720/49669]\tLoss: 21405.1621\n",
      "Training Epoch: 2 [35740/49669]\tLoss: 23060.8945\n",
      "Training Epoch: 2 [35760/49669]\tLoss: 25024.4902\n",
      "Training Epoch: 2 [35780/49669]\tLoss: 23322.1895\n",
      "Training Epoch: 2 [35800/49669]\tLoss: 22859.8086\n",
      "Training Epoch: 2 [35820/49669]\tLoss: 20834.2480\n",
      "Training Epoch: 2 [35840/49669]\tLoss: 22689.2871\n",
      "Training Epoch: 2 [35860/49669]\tLoss: 22208.1953\n",
      "Training Epoch: 2 [35880/49669]\tLoss: 22137.1250\n",
      "Training Epoch: 2 [35900/49669]\tLoss: 22944.8867\n",
      "Training Epoch: 2 [35920/49669]\tLoss: 23164.2930\n",
      "Training Epoch: 2 [35940/49669]\tLoss: 21412.5449\n",
      "Training Epoch: 2 [35960/49669]\tLoss: 24493.7422\n",
      "Training Epoch: 2 [35980/49669]\tLoss: 25256.1016\n",
      "Training Epoch: 2 [36000/49669]\tLoss: 21334.8359\n",
      "Training Epoch: 2 [36020/49669]\tLoss: 23307.3848\n",
      "Training Epoch: 2 [36040/49669]\tLoss: 24025.2988\n",
      "Training Epoch: 2 [36060/49669]\tLoss: 23797.7715\n",
      "Training Epoch: 2 [36080/49669]\tLoss: 19076.5703\n",
      "Training Epoch: 2 [36100/49669]\tLoss: 23416.9805\n",
      "Training Epoch: 2 [36120/49669]\tLoss: 23009.7891\n",
      "Training Epoch: 2 [36140/49669]\tLoss: 21767.5508\n",
      "Training Epoch: 2 [36160/49669]\tLoss: 22131.2363\n",
      "Training Epoch: 2 [36180/49669]\tLoss: 24826.8086\n",
      "Training Epoch: 2 [36200/49669]\tLoss: 21847.1074\n",
      "Training Epoch: 2 [36220/49669]\tLoss: 24632.9336\n",
      "Training Epoch: 2 [36240/49669]\tLoss: 21967.7500\n",
      "Training Epoch: 2 [36260/49669]\tLoss: 22760.9023\n",
      "Training Epoch: 2 [36280/49669]\tLoss: 26893.9297\n",
      "Training Epoch: 2 [36300/49669]\tLoss: 25821.1992\n",
      "Training Epoch: 2 [36320/49669]\tLoss: 20549.3750\n",
      "Training Epoch: 2 [36340/49669]\tLoss: 22641.4004\n",
      "Training Epoch: 2 [36360/49669]\tLoss: 23940.1055\n",
      "Training Epoch: 2 [36380/49669]\tLoss: 23052.7500\n",
      "Training Epoch: 2 [36400/49669]\tLoss: 23081.6270\n",
      "Training Epoch: 2 [36420/49669]\tLoss: 19309.5723\n",
      "Training Epoch: 2 [36440/49669]\tLoss: 26199.7891\n",
      "Training Epoch: 2 [36460/49669]\tLoss: 24039.8691\n",
      "Training Epoch: 2 [36480/49669]\tLoss: 18370.1152\n",
      "Training Epoch: 2 [36500/49669]\tLoss: 22481.6543\n",
      "Training Epoch: 2 [36520/49669]\tLoss: 21280.8105\n",
      "Training Epoch: 2 [36540/49669]\tLoss: 25083.7500\n",
      "Training Epoch: 2 [36560/49669]\tLoss: 21825.8926\n",
      "Training Epoch: 2 [36580/49669]\tLoss: 23556.1992\n",
      "Training Epoch: 2 [36600/49669]\tLoss: 21888.9902\n",
      "Training Epoch: 2 [36620/49669]\tLoss: 19986.7852\n",
      "Training Epoch: 2 [36640/49669]\tLoss: 21753.6172\n",
      "Training Epoch: 2 [36660/49669]\tLoss: 19896.7695\n",
      "Training Epoch: 2 [36680/49669]\tLoss: 23023.5996\n",
      "Training Epoch: 2 [36700/49669]\tLoss: 25799.0234\n",
      "Training Epoch: 2 [36720/49669]\tLoss: 23152.2676\n",
      "Training Epoch: 2 [36740/49669]\tLoss: 23470.1973\n",
      "Training Epoch: 2 [36760/49669]\tLoss: 24276.0039\n",
      "Training Epoch: 2 [36780/49669]\tLoss: 21769.3848\n",
      "Training Epoch: 2 [36800/49669]\tLoss: 25304.1992\n",
      "Training Epoch: 2 [36820/49669]\tLoss: 20975.3008\n",
      "Training Epoch: 2 [36840/49669]\tLoss: 20421.2695\n",
      "Training Epoch: 2 [36860/49669]\tLoss: 21750.8926\n",
      "Training Epoch: 2 [36880/49669]\tLoss: 23586.1133\n",
      "Training Epoch: 2 [36900/49669]\tLoss: 23201.5352\n",
      "Training Epoch: 2 [36920/49669]\tLoss: 21381.8789\n",
      "Training Epoch: 2 [36940/49669]\tLoss: 23340.4004\n",
      "Training Epoch: 2 [36960/49669]\tLoss: 20976.0684\n",
      "Training Epoch: 2 [36980/49669]\tLoss: 21918.3164\n",
      "Training Epoch: 2 [37000/49669]\tLoss: 27449.2344\n",
      "Training Epoch: 2 [37020/49669]\tLoss: 25335.0645\n",
      "Training Epoch: 2 [37040/49669]\tLoss: 21895.4805\n",
      "Training Epoch: 2 [37060/49669]\tLoss: 23945.3906\n",
      "Training Epoch: 2 [37080/49669]\tLoss: 24074.7910\n",
      "Training Epoch: 2 [37100/49669]\tLoss: 22413.6055\n",
      "Training Epoch: 2 [37120/49669]\tLoss: 22575.4941\n",
      "Training Epoch: 2 [37140/49669]\tLoss: 20995.9004\n",
      "Training Epoch: 2 [37160/49669]\tLoss: 19377.0879\n",
      "Training Epoch: 2 [37180/49669]\tLoss: 24423.7734\n",
      "Training Epoch: 2 [37200/49669]\tLoss: 21856.4375\n",
      "Training Epoch: 2 [37220/49669]\tLoss: 20126.2578\n",
      "Training Epoch: 2 [37240/49669]\tLoss: 21929.6738\n",
      "Training Epoch: 2 [37260/49669]\tLoss: 23240.1953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [37280/49669]\tLoss: 20855.6270\n",
      "Training Epoch: 2 [37300/49669]\tLoss: 22156.0820\n",
      "Training Epoch: 2 [37320/49669]\tLoss: 22260.1445\n",
      "Training Epoch: 2 [37340/49669]\tLoss: 19432.8496\n",
      "Training Epoch: 2 [37360/49669]\tLoss: 20677.5410\n",
      "Training Epoch: 2 [37380/49669]\tLoss: 23193.6309\n",
      "Training Epoch: 2 [37400/49669]\tLoss: 20998.2656\n",
      "Training Epoch: 2 [37420/49669]\tLoss: 22779.8320\n",
      "Training Epoch: 2 [37440/49669]\tLoss: 23223.0410\n",
      "Training Epoch: 2 [37460/49669]\tLoss: 21240.2012\n",
      "Training Epoch: 2 [37480/49669]\tLoss: 17102.7305\n",
      "Training Epoch: 2 [37500/49669]\tLoss: 22103.6191\n",
      "Training Epoch: 2 [37520/49669]\tLoss: 18754.6367\n",
      "Training Epoch: 2 [37540/49669]\tLoss: 20614.3770\n",
      "Training Epoch: 2 [37560/49669]\tLoss: 21606.2090\n",
      "Training Epoch: 2 [37580/49669]\tLoss: 21401.7832\n",
      "Training Epoch: 2 [37600/49669]\tLoss: 20953.7617\n",
      "Training Epoch: 2 [37620/49669]\tLoss: 23057.5391\n",
      "Training Epoch: 2 [37640/49669]\tLoss: 24585.6777\n",
      "Training Epoch: 2 [37660/49669]\tLoss: 20191.8164\n",
      "Training Epoch: 2 [37680/49669]\tLoss: 24675.7090\n",
      "Training Epoch: 2 [37700/49669]\tLoss: 21642.7598\n",
      "Training Epoch: 2 [37720/49669]\tLoss: 21221.1387\n",
      "Training Epoch: 2 [37740/49669]\tLoss: 19066.9766\n",
      "Training Epoch: 2 [37760/49669]\tLoss: 22872.4355\n",
      "Training Epoch: 2 [37780/49669]\tLoss: 22522.2793\n",
      "Training Epoch: 2 [37800/49669]\tLoss: 21119.1250\n",
      "Training Epoch: 2 [37820/49669]\tLoss: 21544.0352\n",
      "Training Epoch: 2 [37840/49669]\tLoss: 22192.4746\n",
      "Training Epoch: 2 [37860/49669]\tLoss: 20275.7559\n",
      "Training Epoch: 2 [37880/49669]\tLoss: 23758.5039\n",
      "Training Epoch: 2 [37900/49669]\tLoss: 21437.3672\n",
      "Training Epoch: 2 [37920/49669]\tLoss: 22295.1055\n",
      "Training Epoch: 2 [37940/49669]\tLoss: 22982.6309\n",
      "Training Epoch: 2 [37960/49669]\tLoss: 23676.6152\n",
      "Training Epoch: 2 [37980/49669]\tLoss: 24413.9160\n",
      "Training Epoch: 2 [38000/49669]\tLoss: 20688.4883\n",
      "Training Epoch: 2 [38020/49669]\tLoss: 22445.7793\n",
      "Training Epoch: 2 [38040/49669]\tLoss: 22684.5566\n",
      "Training Epoch: 2 [38060/49669]\tLoss: 22576.1250\n",
      "Training Epoch: 2 [38080/49669]\tLoss: 26187.3418\n",
      "Training Epoch: 2 [38100/49669]\tLoss: 23583.4336\n",
      "Training Epoch: 2 [38120/49669]\tLoss: 21228.4414\n",
      "Training Epoch: 2 [38140/49669]\tLoss: 22116.1914\n",
      "Training Epoch: 2 [38160/49669]\tLoss: 19490.2324\n",
      "Training Epoch: 2 [38180/49669]\tLoss: 20135.8223\n",
      "Training Epoch: 2 [38200/49669]\tLoss: 23953.9688\n",
      "Training Epoch: 2 [38220/49669]\tLoss: 23121.9570\n",
      "Training Epoch: 2 [38240/49669]\tLoss: 20343.6387\n",
      "Training Epoch: 2 [38260/49669]\tLoss: 22402.3027\n",
      "Training Epoch: 2 [38280/49669]\tLoss: 22341.8496\n",
      "Training Epoch: 2 [38300/49669]\tLoss: 21518.4707\n",
      "Training Epoch: 2 [38320/49669]\tLoss: 23474.2793\n",
      "Training Epoch: 2 [38340/49669]\tLoss: 21364.4570\n",
      "Training Epoch: 2 [38360/49669]\tLoss: 21619.2871\n",
      "Training Epoch: 2 [38380/49669]\tLoss: 22621.0840\n",
      "Training Epoch: 2 [38400/49669]\tLoss: 19965.9160\n",
      "Training Epoch: 2 [38420/49669]\tLoss: 21207.6973\n",
      "Training Epoch: 2 [38440/49669]\tLoss: 23122.9824\n",
      "Training Epoch: 2 [38460/49669]\tLoss: 21195.0254\n",
      "Training Epoch: 2 [38480/49669]\tLoss: 23346.1367\n",
      "Training Epoch: 2 [38500/49669]\tLoss: 23728.3809\n",
      "Training Epoch: 2 [38520/49669]\tLoss: 23659.0078\n",
      "Training Epoch: 2 [38540/49669]\tLoss: 20149.8555\n",
      "Training Epoch: 2 [38560/49669]\tLoss: 23970.5312\n",
      "Training Epoch: 2 [38580/49669]\tLoss: 23024.5645\n",
      "Training Epoch: 2 [38600/49669]\tLoss: 19147.1250\n",
      "Training Epoch: 2 [38620/49669]\tLoss: 20098.2656\n",
      "Training Epoch: 2 [38640/49669]\tLoss: 22199.4199\n",
      "Training Epoch: 2 [38660/49669]\tLoss: 21351.6914\n",
      "Training Epoch: 2 [38680/49669]\tLoss: 23280.3594\n",
      "Training Epoch: 2 [38700/49669]\tLoss: 21795.9121\n",
      "Training Epoch: 2 [38720/49669]\tLoss: 20645.7266\n",
      "Training Epoch: 2 [38740/49669]\tLoss: 21660.2910\n",
      "Training Epoch: 2 [38760/49669]\tLoss: 23425.1328\n",
      "Training Epoch: 2 [38780/49669]\tLoss: 20686.9395\n",
      "Training Epoch: 2 [38800/49669]\tLoss: 19791.3926\n",
      "Training Epoch: 2 [38820/49669]\tLoss: 20186.4902\n",
      "Training Epoch: 2 [38840/49669]\tLoss: 19741.3125\n",
      "Training Epoch: 2 [38860/49669]\tLoss: 22589.4258\n",
      "Training Epoch: 2 [38880/49669]\tLoss: 20163.3633\n",
      "Training Epoch: 2 [38900/49669]\tLoss: 21690.4082\n",
      "Training Epoch: 2 [38920/49669]\tLoss: 23653.3574\n",
      "Training Epoch: 2 [38940/49669]\tLoss: 21749.5977\n",
      "Training Epoch: 2 [38960/49669]\tLoss: 21863.4043\n",
      "Training Epoch: 2 [38980/49669]\tLoss: 21771.1406\n",
      "Training Epoch: 2 [39000/49669]\tLoss: 21761.7852\n",
      "Training Epoch: 2 [39020/49669]\tLoss: 21517.8086\n",
      "Training Epoch: 2 [39040/49669]\tLoss: 20631.4961\n",
      "Training Epoch: 2 [39060/49669]\tLoss: 22300.7617\n",
      "Training Epoch: 2 [39080/49669]\tLoss: 22573.0020\n",
      "Training Epoch: 2 [39100/49669]\tLoss: 23210.0898\n",
      "Training Epoch: 2 [39120/49669]\tLoss: 24082.0059\n",
      "Training Epoch: 2 [39140/49669]\tLoss: 18397.7090\n",
      "Training Epoch: 2 [39160/49669]\tLoss: 19912.5547\n",
      "Training Epoch: 2 [39180/49669]\tLoss: 20421.8164\n",
      "Training Epoch: 2 [39200/49669]\tLoss: 19091.4648\n",
      "Training Epoch: 2 [39220/49669]\tLoss: 22560.0859\n",
      "Training Epoch: 2 [39240/49669]\tLoss: 21468.5371\n",
      "Training Epoch: 2 [39260/49669]\tLoss: 22001.1855\n",
      "Training Epoch: 2 [39280/49669]\tLoss: 21397.7070\n",
      "Training Epoch: 2 [39300/49669]\tLoss: 21541.1758\n",
      "Training Epoch: 2 [39320/49669]\tLoss: 23100.6309\n",
      "Training Epoch: 2 [39340/49669]\tLoss: 20826.5547\n",
      "Training Epoch: 2 [39360/49669]\tLoss: 21249.7891\n",
      "Training Epoch: 2 [39380/49669]\tLoss: 25158.8145\n",
      "Training Epoch: 2 [39400/49669]\tLoss: 19641.5410\n",
      "Training Epoch: 2 [39420/49669]\tLoss: 19821.4824\n",
      "Training Epoch: 2 [39440/49669]\tLoss: 20854.7988\n",
      "Training Epoch: 2 [39460/49669]\tLoss: 22656.9180\n",
      "Training Epoch: 2 [39480/49669]\tLoss: 23433.5000\n",
      "Training Epoch: 2 [39500/49669]\tLoss: 24441.6309\n",
      "Training Epoch: 2 [39520/49669]\tLoss: 24055.4707\n",
      "Training Epoch: 2 [39540/49669]\tLoss: 18994.8086\n",
      "Training Epoch: 2 [39560/49669]\tLoss: 24053.6309\n",
      "Training Epoch: 2 [39580/49669]\tLoss: 24631.7812\n",
      "Training Epoch: 2 [39600/49669]\tLoss: 23091.2988\n",
      "Training Epoch: 2 [39620/49669]\tLoss: 21084.2305\n",
      "Training Epoch: 2 [39640/49669]\tLoss: 19396.9609\n",
      "Training Epoch: 2 [39660/49669]\tLoss: 19854.5020\n",
      "Training Epoch: 2 [39680/49669]\tLoss: 21488.1211\n",
      "Training Epoch: 2 [39700/49669]\tLoss: 23035.7480\n",
      "Training Epoch: 2 [39720/49669]\tLoss: 22116.1016\n",
      "Training Epoch: 2 [39740/49669]\tLoss: 19086.3535\n",
      "Training Epoch: 2 [39760/49669]\tLoss: 19767.0391\n",
      "Training Epoch: 2 [39780/49669]\tLoss: 18406.6582\n",
      "Training Epoch: 2 [39800/49669]\tLoss: 19500.0371\n",
      "Training Epoch: 2 [39820/49669]\tLoss: 20810.9141\n",
      "Training Epoch: 2 [39840/49669]\tLoss: 16565.6387\n",
      "Training Epoch: 2 [39860/49669]\tLoss: 19998.1816\n",
      "Training Epoch: 2 [39880/49669]\tLoss: 20238.4297\n",
      "Training Epoch: 2 [39900/49669]\tLoss: 21603.5859\n",
      "Training Epoch: 2 [39920/49669]\tLoss: 20439.8027\n",
      "Training Epoch: 2 [39940/49669]\tLoss: 22216.6543\n",
      "Training Epoch: 2 [39960/49669]\tLoss: 17712.8281\n",
      "Training Epoch: 2 [39980/49669]\tLoss: 21795.4336\n",
      "Training Epoch: 2 [40000/49669]\tLoss: 22306.3105\n",
      "Training Epoch: 2 [40020/49669]\tLoss: 21038.5332\n",
      "Training Epoch: 2 [40040/49669]\tLoss: 20627.8438\n",
      "Training Epoch: 2 [40060/49669]\tLoss: 20843.6602\n",
      "Training Epoch: 2 [40080/49669]\tLoss: 19975.6641\n",
      "Training Epoch: 2 [40100/49669]\tLoss: 20063.1035\n",
      "Training Epoch: 2 [40120/49669]\tLoss: 23466.2109\n",
      "Training Epoch: 2 [40140/49669]\tLoss: 18838.1465\n",
      "Training Epoch: 2 [40160/49669]\tLoss: 23659.7461\n",
      "Training Epoch: 2 [40180/49669]\tLoss: 20437.7148\n",
      "Training Epoch: 2 [40200/49669]\tLoss: 21252.3555\n",
      "Training Epoch: 2 [40220/49669]\tLoss: 19114.4961\n",
      "Training Epoch: 2 [40240/49669]\tLoss: 16151.1084\n",
      "Training Epoch: 2 [40260/49669]\tLoss: 21742.9883\n",
      "Training Epoch: 2 [40280/49669]\tLoss: 22675.2031\n",
      "Training Epoch: 2 [40300/49669]\tLoss: 20516.7129\n",
      "Training Epoch: 2 [40320/49669]\tLoss: 23380.2480\n",
      "Training Epoch: 2 [40340/49669]\tLoss: 20851.0469\n",
      "Training Epoch: 2 [40360/49669]\tLoss: 19521.7969\n",
      "Training Epoch: 2 [40380/49669]\tLoss: 22792.0664\n",
      "Training Epoch: 2 [40400/49669]\tLoss: 19720.0332\n",
      "Training Epoch: 2 [40420/49669]\tLoss: 22101.1875\n",
      "Training Epoch: 2 [40440/49669]\tLoss: 23417.6836\n",
      "Training Epoch: 2 [40460/49669]\tLoss: 21027.6270\n",
      "Training Epoch: 2 [40480/49669]\tLoss: 19834.5293\n",
      "Training Epoch: 2 [40500/49669]\tLoss: 18628.5918\n",
      "Training Epoch: 2 [40520/49669]\tLoss: 18177.1035\n",
      "Training Epoch: 2 [40540/49669]\tLoss: 22023.9570\n",
      "Training Epoch: 2 [40560/49669]\tLoss: 19927.9707\n",
      "Training Epoch: 2 [40580/49669]\tLoss: 20040.4805\n",
      "Training Epoch: 2 [40600/49669]\tLoss: 20438.9492\n",
      "Training Epoch: 2 [40620/49669]\tLoss: 24258.2617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [40640/49669]\tLoss: 21280.7266\n",
      "Training Epoch: 2 [40660/49669]\tLoss: 19707.7227\n",
      "Training Epoch: 2 [40680/49669]\tLoss: 20959.0684\n",
      "Training Epoch: 2 [40700/49669]\tLoss: 21540.5117\n",
      "Training Epoch: 2 [40720/49669]\tLoss: 19821.7051\n",
      "Training Epoch: 2 [40740/49669]\tLoss: 24439.4941\n",
      "Training Epoch: 2 [40760/49669]\tLoss: 22146.9082\n",
      "Training Epoch: 2 [40780/49669]\tLoss: 23889.7793\n",
      "Training Epoch: 2 [40800/49669]\tLoss: 23532.3223\n",
      "Training Epoch: 2 [40820/49669]\tLoss: 21990.6211\n",
      "Training Epoch: 2 [40840/49669]\tLoss: 20982.0371\n",
      "Training Epoch: 2 [40860/49669]\tLoss: 18610.2383\n",
      "Training Epoch: 2 [40880/49669]\tLoss: 21613.5742\n",
      "Training Epoch: 2 [40900/49669]\tLoss: 20492.7246\n",
      "Training Epoch: 2 [40920/49669]\tLoss: 23136.2324\n",
      "Training Epoch: 2 [40940/49669]\tLoss: 22491.0078\n",
      "Training Epoch: 2 [40960/49669]\tLoss: 21598.3848\n",
      "Training Epoch: 2 [40980/49669]\tLoss: 21363.8418\n",
      "Training Epoch: 2 [41000/49669]\tLoss: 19686.0156\n",
      "Training Epoch: 2 [41020/49669]\tLoss: 18097.6719\n",
      "Training Epoch: 2 [41040/49669]\tLoss: 20989.3320\n",
      "Training Epoch: 2 [41060/49669]\tLoss: 21320.1133\n",
      "Training Epoch: 2 [41080/49669]\tLoss: 21489.6641\n",
      "Training Epoch: 2 [41100/49669]\tLoss: 21883.4141\n",
      "Training Epoch: 2 [41120/49669]\tLoss: 17698.9863\n",
      "Training Epoch: 2 [41140/49669]\tLoss: 19823.7090\n",
      "Training Epoch: 2 [41160/49669]\tLoss: 19211.7754\n",
      "Training Epoch: 2 [41180/49669]\tLoss: 19617.3047\n",
      "Training Epoch: 2 [41200/49669]\tLoss: 20101.3105\n",
      "Training Epoch: 2 [41220/49669]\tLoss: 18176.8398\n",
      "Training Epoch: 2 [41240/49669]\tLoss: 22626.2129\n",
      "Training Epoch: 2 [41260/49669]\tLoss: 18248.0137\n",
      "Training Epoch: 2 [41280/49669]\tLoss: 23458.2168\n",
      "Training Epoch: 2 [41300/49669]\tLoss: 21187.0840\n",
      "Training Epoch: 2 [41320/49669]\tLoss: 21146.7090\n",
      "Training Epoch: 2 [41340/49669]\tLoss: 20756.9824\n",
      "Training Epoch: 2 [41360/49669]\tLoss: 21099.2266\n",
      "Training Epoch: 2 [41380/49669]\tLoss: 22018.5859\n",
      "Training Epoch: 2 [41400/49669]\tLoss: 20965.3008\n",
      "Training Epoch: 2 [41420/49669]\tLoss: 17090.5781\n",
      "Training Epoch: 2 [41440/49669]\tLoss: 22566.5957\n",
      "Training Epoch: 2 [41460/49669]\tLoss: 20135.9590\n",
      "Training Epoch: 2 [41480/49669]\tLoss: 21571.7754\n",
      "Training Epoch: 2 [41500/49669]\tLoss: 22009.3027\n",
      "Training Epoch: 2 [41520/49669]\tLoss: 18639.9590\n",
      "Training Epoch: 2 [41540/49669]\tLoss: 21440.9316\n",
      "Training Epoch: 2 [41560/49669]\tLoss: 22073.4551\n",
      "Training Epoch: 2 [41580/49669]\tLoss: 20381.7812\n",
      "Training Epoch: 2 [41600/49669]\tLoss: 18025.8457\n",
      "Training Epoch: 2 [41620/49669]\tLoss: 20543.9629\n",
      "Training Epoch: 2 [41640/49669]\tLoss: 20730.1016\n",
      "Training Epoch: 2 [41660/49669]\tLoss: 22288.5801\n",
      "Training Epoch: 2 [41680/49669]\tLoss: 24103.5664\n",
      "Training Epoch: 2 [41700/49669]\tLoss: 21618.6348\n",
      "Training Epoch: 2 [41720/49669]\tLoss: 18652.3691\n",
      "Training Epoch: 2 [41740/49669]\tLoss: 20949.9980\n",
      "Training Epoch: 2 [41760/49669]\tLoss: 21231.5742\n",
      "Training Epoch: 2 [41780/49669]\tLoss: 20568.7148\n",
      "Training Epoch: 2 [41800/49669]\tLoss: 19494.9043\n",
      "Training Epoch: 2 [41820/49669]\tLoss: 14933.0879\n",
      "Training Epoch: 2 [41840/49669]\tLoss: 16850.6191\n",
      "Training Epoch: 2 [41860/49669]\tLoss: 21937.5176\n",
      "Training Epoch: 2 [41880/49669]\tLoss: 21937.3887\n",
      "Training Epoch: 2 [41900/49669]\tLoss: 20577.2031\n",
      "Training Epoch: 2 [41920/49669]\tLoss: 18320.2871\n",
      "Training Epoch: 2 [41940/49669]\tLoss: 20657.6211\n",
      "Training Epoch: 2 [41960/49669]\tLoss: 22635.5234\n",
      "Training Epoch: 2 [41980/49669]\tLoss: 18466.4590\n",
      "Training Epoch: 2 [42000/49669]\tLoss: 20604.9375\n",
      "Training Epoch: 2 [42020/49669]\tLoss: 21345.6133\n",
      "Training Epoch: 2 [42040/49669]\tLoss: 21002.0449\n",
      "Training Epoch: 2 [42060/49669]\tLoss: 17153.9160\n",
      "Training Epoch: 2 [42080/49669]\tLoss: 20086.5020\n",
      "Training Epoch: 2 [42100/49669]\tLoss: 22263.4023\n",
      "Training Epoch: 2 [42120/49669]\tLoss: 21986.5137\n",
      "Training Epoch: 2 [42140/49669]\tLoss: 21979.1797\n",
      "Training Epoch: 2 [42160/49669]\tLoss: 21830.6426\n",
      "Training Epoch: 2 [42180/49669]\tLoss: 21663.6113\n",
      "Training Epoch: 2 [42200/49669]\tLoss: 18307.6523\n",
      "Training Epoch: 2 [42220/49669]\tLoss: 21299.3672\n",
      "Training Epoch: 2 [42240/49669]\tLoss: 20561.7168\n",
      "Training Epoch: 2 [42260/49669]\tLoss: 20214.0664\n",
      "Training Epoch: 2 [42280/49669]\tLoss: 22323.7559\n",
      "Training Epoch: 2 [42300/49669]\tLoss: 22574.3789\n",
      "Training Epoch: 2 [42320/49669]\tLoss: 18232.7637\n",
      "Training Epoch: 2 [42340/49669]\tLoss: 20951.5039\n",
      "Training Epoch: 2 [42360/49669]\tLoss: 18839.6914\n",
      "Training Epoch: 2 [42380/49669]\tLoss: 19782.6875\n",
      "Training Epoch: 2 [42400/49669]\tLoss: 20931.6230\n",
      "Training Epoch: 2 [42420/49669]\tLoss: 20102.0488\n",
      "Training Epoch: 2 [42440/49669]\tLoss: 23088.9355\n",
      "Training Epoch: 2 [42460/49669]\tLoss: 21535.7207\n",
      "Training Epoch: 2 [42480/49669]\tLoss: 20943.2598\n",
      "Training Epoch: 2 [42500/49669]\tLoss: 21065.7559\n",
      "Training Epoch: 2 [42520/49669]\tLoss: 22819.9492\n",
      "Training Epoch: 2 [42540/49669]\tLoss: 16751.3438\n",
      "Training Epoch: 2 [42560/49669]\tLoss: 16679.2832\n",
      "Training Epoch: 2 [42580/49669]\tLoss: 19968.4219\n",
      "Training Epoch: 2 [42600/49669]\tLoss: 19610.0137\n",
      "Training Epoch: 2 [42620/49669]\tLoss: 17186.9746\n",
      "Training Epoch: 2 [42640/49669]\tLoss: 18930.3867\n",
      "Training Epoch: 2 [42660/49669]\tLoss: 21508.6113\n",
      "Training Epoch: 2 [42680/49669]\tLoss: 22284.4590\n",
      "Training Epoch: 2 [42700/49669]\tLoss: 24182.0840\n",
      "Training Epoch: 2 [42720/49669]\tLoss: 21609.8066\n",
      "Training Epoch: 2 [42740/49669]\tLoss: 19324.5879\n",
      "Training Epoch: 2 [42760/49669]\tLoss: 19089.6230\n",
      "Training Epoch: 2 [42780/49669]\tLoss: 21427.3984\n",
      "Training Epoch: 2 [42800/49669]\tLoss: 20347.3633\n",
      "Training Epoch: 2 [42820/49669]\tLoss: 18792.7266\n",
      "Training Epoch: 2 [42840/49669]\tLoss: 21792.8613\n",
      "Training Epoch: 2 [42860/49669]\tLoss: 19990.0684\n",
      "Training Epoch: 2 [42880/49669]\tLoss: 19887.2305\n",
      "Training Epoch: 2 [42900/49669]\tLoss: 22008.1289\n",
      "Training Epoch: 2 [42920/49669]\tLoss: 20620.5996\n",
      "Training Epoch: 2 [42940/49669]\tLoss: 20302.9473\n",
      "Training Epoch: 2 [42960/49669]\tLoss: 18440.7168\n",
      "Training Epoch: 2 [42980/49669]\tLoss: 22582.3457\n",
      "Training Epoch: 2 [43000/49669]\tLoss: 22405.5273\n",
      "Training Epoch: 2 [43020/49669]\tLoss: 17787.2988\n",
      "Training Epoch: 2 [43040/49669]\tLoss: 21330.1465\n",
      "Training Epoch: 2 [43060/49669]\tLoss: 20571.4688\n",
      "Training Epoch: 2 [43080/49669]\tLoss: 24053.3574\n",
      "Training Epoch: 2 [43100/49669]\tLoss: 19683.4355\n",
      "Training Epoch: 2 [43120/49669]\tLoss: 20878.3613\n",
      "Training Epoch: 2 [43140/49669]\tLoss: 19616.2285\n",
      "Training Epoch: 2 [43160/49669]\tLoss: 17795.4004\n",
      "Training Epoch: 2 [43180/49669]\tLoss: 19507.8867\n",
      "Training Epoch: 2 [43200/49669]\tLoss: 18617.8711\n",
      "Training Epoch: 2 [43220/49669]\tLoss: 17878.8926\n",
      "Training Epoch: 2 [43240/49669]\tLoss: 18532.6660\n",
      "Training Epoch: 2 [43260/49669]\tLoss: 23350.6602\n",
      "Training Epoch: 2 [43280/49669]\tLoss: 21266.4004\n",
      "Training Epoch: 2 [43300/49669]\tLoss: 19547.3535\n",
      "Training Epoch: 2 [43320/49669]\tLoss: 19264.0996\n",
      "Training Epoch: 2 [43340/49669]\tLoss: 21696.6973\n",
      "Training Epoch: 2 [43360/49669]\tLoss: 19263.0977\n",
      "Training Epoch: 2 [43380/49669]\tLoss: 22387.0664\n",
      "Training Epoch: 2 [43400/49669]\tLoss: 20154.4883\n",
      "Training Epoch: 2 [43420/49669]\tLoss: 20126.8477\n",
      "Training Epoch: 2 [43440/49669]\tLoss: 18321.5547\n",
      "Training Epoch: 2 [43460/49669]\tLoss: 21096.4102\n",
      "Training Epoch: 2 [43480/49669]\tLoss: 21237.5840\n",
      "Training Epoch: 2 [43500/49669]\tLoss: 20459.6152\n",
      "Training Epoch: 2 [43520/49669]\tLoss: 19653.0996\n",
      "Training Epoch: 2 [43540/49669]\tLoss: 18173.3555\n",
      "Training Epoch: 2 [43560/49669]\tLoss: 20346.3105\n",
      "Training Epoch: 2 [43580/49669]\tLoss: 20958.6660\n",
      "Training Epoch: 2 [43600/49669]\tLoss: 21388.1016\n",
      "Training Epoch: 2 [43620/49669]\tLoss: 23906.0254\n",
      "Training Epoch: 2 [43640/49669]\tLoss: 20278.3848\n",
      "Training Epoch: 2 [43660/49669]\tLoss: 21161.8281\n",
      "Training Epoch: 2 [43680/49669]\tLoss: 18756.3359\n",
      "Training Epoch: 2 [43700/49669]\tLoss: 18054.0605\n",
      "Training Epoch: 2 [43720/49669]\tLoss: 21105.0293\n",
      "Training Epoch: 2 [43740/49669]\tLoss: 21936.7305\n",
      "Training Epoch: 2 [43760/49669]\tLoss: 20036.9512\n",
      "Training Epoch: 2 [43780/49669]\tLoss: 20576.7793\n",
      "Training Epoch: 2 [43800/49669]\tLoss: 19429.7012\n",
      "Training Epoch: 2 [43820/49669]\tLoss: 19348.6797\n",
      "Training Epoch: 2 [43840/49669]\tLoss: 21583.0215\n",
      "Training Epoch: 2 [43860/49669]\tLoss: 19056.5449\n",
      "Training Epoch: 2 [43880/49669]\tLoss: 20309.0957\n",
      "Training Epoch: 2 [43900/49669]\tLoss: 19188.5879\n",
      "Training Epoch: 2 [43920/49669]\tLoss: 17038.4121\n",
      "Training Epoch: 2 [43940/49669]\tLoss: 19768.6289\n",
      "Training Epoch: 2 [43960/49669]\tLoss: 21425.3418\n",
      "Training Epoch: 2 [43980/49669]\tLoss: 16859.1445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [44000/49669]\tLoss: 21259.3945\n",
      "Training Epoch: 2 [44020/49669]\tLoss: 21140.9258\n",
      "Training Epoch: 2 [44040/49669]\tLoss: 20071.1992\n",
      "Training Epoch: 2 [44060/49669]\tLoss: 19203.1406\n",
      "Training Epoch: 2 [44080/49669]\tLoss: 16113.8242\n",
      "Training Epoch: 2 [44100/49669]\tLoss: 20542.9883\n",
      "Training Epoch: 2 [44120/49669]\tLoss: 22238.5137\n",
      "Training Epoch: 2 [44140/49669]\tLoss: 18748.0586\n",
      "Training Epoch: 2 [44160/49669]\tLoss: 22348.5898\n",
      "Training Epoch: 2 [44180/49669]\tLoss: 22450.0938\n",
      "Training Epoch: 2 [44200/49669]\tLoss: 20068.5918\n",
      "Training Epoch: 2 [44220/49669]\tLoss: 20870.2598\n",
      "Training Epoch: 2 [44240/49669]\tLoss: 18791.9727\n",
      "Training Epoch: 2 [44260/49669]\tLoss: 18277.0605\n",
      "Training Epoch: 2 [44280/49669]\tLoss: 20140.8281\n",
      "Training Epoch: 2 [44300/49669]\tLoss: 18874.9727\n",
      "Training Epoch: 2 [44320/49669]\tLoss: 19908.0449\n",
      "Training Epoch: 2 [44340/49669]\tLoss: 21771.5605\n",
      "Training Epoch: 2 [44360/49669]\tLoss: 20414.5723\n",
      "Training Epoch: 2 [44380/49669]\tLoss: 17022.4277\n",
      "Training Epoch: 2 [44400/49669]\tLoss: 23226.6465\n",
      "Training Epoch: 2 [44420/49669]\tLoss: 23152.4512\n",
      "Training Epoch: 2 [44440/49669]\tLoss: 20508.1504\n",
      "Training Epoch: 2 [44460/49669]\tLoss: 19333.4062\n",
      "Training Epoch: 2 [44480/49669]\tLoss: 20681.2676\n",
      "Training Epoch: 2 [44500/49669]\tLoss: 19367.7070\n",
      "Training Epoch: 2 [44520/49669]\tLoss: 18563.9180\n",
      "Training Epoch: 2 [44540/49669]\tLoss: 23187.3984\n",
      "Training Epoch: 2 [44560/49669]\tLoss: 22214.5859\n",
      "Training Epoch: 2 [44580/49669]\tLoss: 19110.1113\n",
      "Training Epoch: 2 [44600/49669]\tLoss: 20081.3633\n",
      "Training Epoch: 2 [44620/49669]\tLoss: 19132.2227\n",
      "Training Epoch: 2 [44640/49669]\tLoss: 18856.3105\n",
      "Training Epoch: 2 [44660/49669]\tLoss: 21229.9512\n",
      "Training Epoch: 2 [44680/49669]\tLoss: 22893.2480\n",
      "Training Epoch: 2 [44700/49669]\tLoss: 21693.8066\n",
      "Training Epoch: 2 [44720/49669]\tLoss: 18879.3906\n",
      "Training Epoch: 2 [44740/49669]\tLoss: 17402.1934\n",
      "Training Epoch: 2 [44760/49669]\tLoss: 20580.3301\n",
      "Training Epoch: 2 [44780/49669]\tLoss: 18956.2656\n",
      "Training Epoch: 2 [44800/49669]\tLoss: 20479.9277\n",
      "Training Epoch: 2 [44820/49669]\tLoss: 20297.5430\n",
      "Training Epoch: 2 [44840/49669]\tLoss: 19441.9023\n",
      "Training Epoch: 2 [44860/49669]\tLoss: 21639.4141\n",
      "Training Epoch: 2 [44880/49669]\tLoss: 20391.8477\n",
      "Training Epoch: 2 [44900/49669]\tLoss: 16805.4746\n",
      "Training Epoch: 2 [44920/49669]\tLoss: 20335.2461\n",
      "Training Epoch: 2 [44940/49669]\tLoss: 21135.7578\n",
      "Training Epoch: 2 [44960/49669]\tLoss: 22774.9922\n",
      "Training Epoch: 2 [44980/49669]\tLoss: 19826.3750\n",
      "Training Epoch: 2 [45000/49669]\tLoss: 19380.2617\n",
      "Training Epoch: 2 [45020/49669]\tLoss: 20209.5527\n",
      "Training Epoch: 2 [45040/49669]\tLoss: 23203.3281\n",
      "Training Epoch: 2 [45060/49669]\tLoss: 19789.7988\n",
      "Training Epoch: 2 [45080/49669]\tLoss: 19308.6934\n",
      "Training Epoch: 2 [45100/49669]\tLoss: 20815.5586\n",
      "Training Epoch: 2 [45120/49669]\tLoss: 16780.1504\n",
      "Training Epoch: 2 [45140/49669]\tLoss: 17545.7793\n",
      "Training Epoch: 2 [45160/49669]\tLoss: 19592.3555\n",
      "Training Epoch: 2 [45180/49669]\tLoss: 19662.7773\n",
      "Training Epoch: 2 [45200/49669]\tLoss: 18948.6406\n",
      "Training Epoch: 2 [45220/49669]\tLoss: 20263.6465\n",
      "Training Epoch: 2 [45240/49669]\tLoss: 20447.4277\n",
      "Training Epoch: 2 [45260/49669]\tLoss: 17985.9785\n",
      "Training Epoch: 2 [45280/49669]\tLoss: 19562.8555\n",
      "Training Epoch: 2 [45300/49669]\tLoss: 19459.5449\n",
      "Training Epoch: 2 [45320/49669]\tLoss: 18866.7695\n",
      "Training Epoch: 2 [45340/49669]\tLoss: 19741.0000\n",
      "Training Epoch: 2 [45360/49669]\tLoss: 18291.0527\n",
      "Training Epoch: 2 [45380/49669]\tLoss: 21783.7715\n",
      "Training Epoch: 2 [45400/49669]\tLoss: 20446.1738\n",
      "Training Epoch: 2 [45420/49669]\tLoss: 18491.2266\n",
      "Training Epoch: 2 [45440/49669]\tLoss: 19576.5078\n",
      "Training Epoch: 2 [45460/49669]\tLoss: 19566.1543\n",
      "Training Epoch: 2 [45480/49669]\tLoss: 18642.7070\n",
      "Training Epoch: 2 [45500/49669]\tLoss: 18881.9160\n",
      "Training Epoch: 2 [45520/49669]\tLoss: 18791.5742\n",
      "Training Epoch: 2 [45540/49669]\tLoss: 18681.5586\n",
      "Training Epoch: 2 [45560/49669]\tLoss: 18438.4277\n",
      "Training Epoch: 2 [45580/49669]\tLoss: 20294.7305\n",
      "Training Epoch: 2 [45600/49669]\tLoss: 17773.7227\n",
      "Training Epoch: 2 [45620/49669]\tLoss: 19947.9277\n",
      "Training Epoch: 2 [45640/49669]\tLoss: 17939.3906\n",
      "Training Epoch: 2 [45660/49669]\tLoss: 21206.1504\n",
      "Training Epoch: 2 [45680/49669]\tLoss: 18300.0898\n",
      "Training Epoch: 2 [45700/49669]\tLoss: 21557.7852\n",
      "Training Epoch: 2 [45720/49669]\tLoss: 20536.7715\n",
      "Training Epoch: 2 [45740/49669]\tLoss: 18407.4492\n",
      "Training Epoch: 2 [45760/49669]\tLoss: 22630.8535\n",
      "Training Epoch: 2 [45780/49669]\tLoss: 16468.0410\n",
      "Training Epoch: 2 [45800/49669]\tLoss: 20324.8008\n",
      "Training Epoch: 2 [45820/49669]\tLoss: 22723.9141\n",
      "Training Epoch: 2 [45840/49669]\tLoss: 20966.2207\n",
      "Training Epoch: 2 [45860/49669]\tLoss: 18046.0430\n",
      "Training Epoch: 2 [45880/49669]\tLoss: 20374.5059\n",
      "Training Epoch: 2 [45900/49669]\tLoss: 17753.9434\n",
      "Training Epoch: 2 [45920/49669]\tLoss: 13816.0010\n",
      "Training Epoch: 2 [45940/49669]\tLoss: 19262.6758\n",
      "Training Epoch: 2 [45960/49669]\tLoss: 18524.8184\n",
      "Training Epoch: 2 [45980/49669]\tLoss: 21291.8691\n",
      "Training Epoch: 2 [46000/49669]\tLoss: 15401.8398\n",
      "Training Epoch: 2 [46020/49669]\tLoss: 20314.0918\n",
      "Training Epoch: 2 [46040/49669]\tLoss: 19448.3262\n",
      "Training Epoch: 2 [46060/49669]\tLoss: 19753.2812\n",
      "Training Epoch: 2 [46080/49669]\tLoss: 20672.5312\n",
      "Training Epoch: 2 [46100/49669]\tLoss: 18060.1934\n",
      "Training Epoch: 2 [46120/49669]\tLoss: 18666.6895\n",
      "Training Epoch: 2 [46140/49669]\tLoss: 19379.1035\n",
      "Training Epoch: 2 [46160/49669]\tLoss: 19622.6367\n",
      "Training Epoch: 2 [46180/49669]\tLoss: 20424.9414\n",
      "Training Epoch: 2 [46200/49669]\tLoss: 19589.6172\n",
      "Training Epoch: 2 [46220/49669]\tLoss: 20378.1270\n",
      "Training Epoch: 2 [46240/49669]\tLoss: 17161.7188\n",
      "Training Epoch: 2 [46260/49669]\tLoss: 16571.9043\n",
      "Training Epoch: 2 [46280/49669]\tLoss: 19688.2148\n",
      "Training Epoch: 2 [46300/49669]\tLoss: 19376.0391\n",
      "Training Epoch: 2 [46320/49669]\tLoss: 20220.5156\n",
      "Training Epoch: 2 [46340/49669]\tLoss: 20334.6250\n",
      "Training Epoch: 2 [46360/49669]\tLoss: 18124.9902\n",
      "Training Epoch: 2 [46380/49669]\tLoss: 16178.1348\n",
      "Training Epoch: 2 [46400/49669]\tLoss: 17231.3867\n",
      "Training Epoch: 2 [46420/49669]\tLoss: 19060.9395\n",
      "Training Epoch: 2 [46440/49669]\tLoss: 21463.8691\n",
      "Training Epoch: 2 [46460/49669]\tLoss: 19990.6016\n",
      "Training Epoch: 2 [46480/49669]\tLoss: 18610.6973\n",
      "Training Epoch: 2 [46500/49669]\tLoss: 20736.2441\n",
      "Training Epoch: 2 [46520/49669]\tLoss: 19207.4082\n",
      "Training Epoch: 2 [46540/49669]\tLoss: 17030.8145\n",
      "Training Epoch: 2 [46560/49669]\tLoss: 17203.0801\n",
      "Training Epoch: 2 [46580/49669]\tLoss: 18699.1895\n",
      "Training Epoch: 2 [46600/49669]\tLoss: 21080.9883\n",
      "Training Epoch: 2 [46620/49669]\tLoss: 19426.1621\n",
      "Training Epoch: 2 [46640/49669]\tLoss: 17618.5605\n",
      "Training Epoch: 2 [46660/49669]\tLoss: 20526.2129\n",
      "Training Epoch: 2 [46680/49669]\tLoss: 18220.5332\n",
      "Training Epoch: 2 [46700/49669]\tLoss: 19968.4609\n",
      "Training Epoch: 2 [46720/49669]\tLoss: 19626.9609\n",
      "Training Epoch: 2 [46740/49669]\tLoss: 19107.1504\n",
      "Training Epoch: 2 [46760/49669]\tLoss: 16960.2461\n",
      "Training Epoch: 2 [46780/49669]\tLoss: 22919.4043\n",
      "Training Epoch: 2 [46800/49669]\tLoss: 21655.6191\n",
      "Training Epoch: 2 [46820/49669]\tLoss: 19240.5645\n",
      "Training Epoch: 2 [46840/49669]\tLoss: 19139.6836\n",
      "Training Epoch: 2 [46860/49669]\tLoss: 19255.2090\n",
      "Training Epoch: 2 [46880/49669]\tLoss: 20857.2031\n",
      "Training Epoch: 2 [46900/49669]\tLoss: 19137.9141\n",
      "Training Epoch: 2 [46920/49669]\tLoss: 16963.8809\n",
      "Training Epoch: 2 [46940/49669]\tLoss: 19047.3086\n",
      "Training Epoch: 2 [46960/49669]\tLoss: 17576.3574\n",
      "Training Epoch: 2 [46980/49669]\tLoss: 18864.5840\n",
      "Training Epoch: 2 [47000/49669]\tLoss: 19469.8359\n",
      "Training Epoch: 2 [47020/49669]\tLoss: 17845.3965\n",
      "Training Epoch: 2 [47040/49669]\tLoss: 19136.3086\n",
      "Training Epoch: 2 [47060/49669]\tLoss: 17786.2793\n",
      "Training Epoch: 2 [47080/49669]\tLoss: 18683.7852\n",
      "Training Epoch: 2 [47100/49669]\tLoss: 18150.2012\n",
      "Training Epoch: 2 [47120/49669]\tLoss: 18859.6973\n",
      "Training Epoch: 2 [47140/49669]\tLoss: 19218.3809\n",
      "Training Epoch: 2 [47160/49669]\tLoss: 19270.9043\n",
      "Training Epoch: 2 [47180/49669]\tLoss: 20569.3652\n",
      "Training Epoch: 2 [47200/49669]\tLoss: 18252.4043\n",
      "Training Epoch: 2 [47220/49669]\tLoss: 21142.8027\n",
      "Training Epoch: 2 [47240/49669]\tLoss: 20788.9941\n",
      "Training Epoch: 2 [47260/49669]\tLoss: 17802.4180\n",
      "Training Epoch: 2 [47280/49669]\tLoss: 18007.2754\n",
      "Training Epoch: 2 [47300/49669]\tLoss: 19870.6445\n",
      "Training Epoch: 2 [47320/49669]\tLoss: 16844.8125\n",
      "Training Epoch: 2 [47340/49669]\tLoss: 17631.3945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [47360/49669]\tLoss: 19735.2988\n",
      "Training Epoch: 2 [47380/49669]\tLoss: 18049.4922\n",
      "Training Epoch: 2 [47400/49669]\tLoss: 20332.7949\n",
      "Training Epoch: 2 [47420/49669]\tLoss: 18754.2793\n",
      "Training Epoch: 2 [47440/49669]\tLoss: 20855.0938\n",
      "Training Epoch: 2 [47460/49669]\tLoss: 21480.1719\n",
      "Training Epoch: 2 [47480/49669]\tLoss: 18296.9980\n",
      "Training Epoch: 2 [47500/49669]\tLoss: 18436.7148\n",
      "Training Epoch: 2 [47520/49669]\tLoss: 18110.3105\n",
      "Training Epoch: 2 [47540/49669]\tLoss: 16256.0889\n",
      "Training Epoch: 2 [47560/49669]\tLoss: 19363.4453\n",
      "Training Epoch: 2 [47580/49669]\tLoss: 17048.5410\n",
      "Training Epoch: 2 [47600/49669]\tLoss: 18893.8516\n",
      "Training Epoch: 2 [47620/49669]\tLoss: 18562.7812\n",
      "Training Epoch: 2 [47640/49669]\tLoss: 17982.8672\n",
      "Training Epoch: 2 [47660/49669]\tLoss: 19793.1582\n",
      "Training Epoch: 2 [47680/49669]\tLoss: 19817.6406\n",
      "Training Epoch: 2 [47700/49669]\tLoss: 17006.7109\n",
      "Training Epoch: 2 [47720/49669]\tLoss: 15092.4482\n",
      "Training Epoch: 2 [47740/49669]\tLoss: 20951.5312\n",
      "Training Epoch: 2 [47760/49669]\tLoss: 19519.6758\n",
      "Training Epoch: 2 [47780/49669]\tLoss: 16868.8535\n",
      "Training Epoch: 2 [47800/49669]\tLoss: 17560.0703\n",
      "Training Epoch: 2 [47820/49669]\tLoss: 16229.3955\n",
      "Training Epoch: 2 [47840/49669]\tLoss: 16503.5742\n",
      "Training Epoch: 2 [47860/49669]\tLoss: 18200.5254\n",
      "Training Epoch: 2 [47880/49669]\tLoss: 16229.5596\n",
      "Training Epoch: 2 [47900/49669]\tLoss: 20294.1641\n",
      "Training Epoch: 2 [47920/49669]\tLoss: 18132.7441\n",
      "Training Epoch: 2 [47940/49669]\tLoss: 21009.0312\n",
      "Training Epoch: 2 [47960/49669]\tLoss: 18999.1309\n",
      "Training Epoch: 2 [47980/49669]\tLoss: 19993.3535\n",
      "Training Epoch: 2 [48000/49669]\tLoss: 17259.4375\n",
      "Training Epoch: 2 [48020/49669]\tLoss: 20184.1191\n",
      "Training Epoch: 2 [48040/49669]\tLoss: 16201.1348\n",
      "Training Epoch: 2 [48060/49669]\tLoss: 16446.7480\n",
      "Training Epoch: 2 [48080/49669]\tLoss: 17631.9902\n",
      "Training Epoch: 2 [48100/49669]\tLoss: 21065.7402\n",
      "Training Epoch: 2 [48120/49669]\tLoss: 17840.9844\n",
      "Training Epoch: 2 [48140/49669]\tLoss: 17798.1816\n",
      "Training Epoch: 2 [48160/49669]\tLoss: 18594.8242\n",
      "Training Epoch: 2 [48180/49669]\tLoss: 18854.8457\n",
      "Training Epoch: 2 [48200/49669]\tLoss: 19938.5508\n",
      "Training Epoch: 2 [48220/49669]\tLoss: 18831.1348\n",
      "Training Epoch: 2 [48240/49669]\tLoss: 16401.2305\n",
      "Training Epoch: 2 [48260/49669]\tLoss: 17518.9121\n",
      "Training Epoch: 2 [48280/49669]\tLoss: 18493.8242\n",
      "Training Epoch: 2 [48300/49669]\tLoss: 18962.5684\n",
      "Training Epoch: 2 [48320/49669]\tLoss: 19040.1484\n",
      "Training Epoch: 2 [48340/49669]\tLoss: 19028.4844\n",
      "Training Epoch: 2 [48360/49669]\tLoss: 16293.5957\n",
      "Training Epoch: 2 [48380/49669]\tLoss: 16422.0195\n",
      "Training Epoch: 2 [48400/49669]\tLoss: 20852.8438\n",
      "Training Epoch: 2 [48420/49669]\tLoss: 19168.2578\n",
      "Training Epoch: 2 [48440/49669]\tLoss: 19289.7422\n",
      "Training Epoch: 2 [48460/49669]\tLoss: 16407.0723\n",
      "Training Epoch: 2 [48480/49669]\tLoss: 16997.1172\n",
      "Training Epoch: 2 [48500/49669]\tLoss: 18751.8457\n",
      "Training Epoch: 2 [48520/49669]\tLoss: 18587.5859\n",
      "Training Epoch: 2 [48540/49669]\tLoss: 18333.6328\n",
      "Training Epoch: 2 [48560/49669]\tLoss: 19691.8672\n",
      "Training Epoch: 2 [48580/49669]\tLoss: 20954.9258\n",
      "Training Epoch: 2 [48600/49669]\tLoss: 18289.9922\n",
      "Training Epoch: 2 [48620/49669]\tLoss: 20256.7754\n",
      "Training Epoch: 2 [48640/49669]\tLoss: 18436.0488\n",
      "Training Epoch: 2 [48660/49669]\tLoss: 17293.2988\n",
      "Training Epoch: 2 [48680/49669]\tLoss: 17984.9375\n",
      "Training Epoch: 2 [48700/49669]\tLoss: 18734.6699\n",
      "Training Epoch: 2 [48720/49669]\tLoss: 17104.3750\n",
      "Training Epoch: 2 [48740/49669]\tLoss: 20046.4883\n",
      "Training Epoch: 2 [48760/49669]\tLoss: 18484.6934\n",
      "Training Epoch: 2 [48780/49669]\tLoss: 19897.4863\n",
      "Training Epoch: 2 [48800/49669]\tLoss: 17790.6348\n",
      "Training Epoch: 2 [48820/49669]\tLoss: 18163.0332\n",
      "Training Epoch: 2 [48840/49669]\tLoss: 21145.5723\n",
      "Training Epoch: 2 [48860/49669]\tLoss: 12235.6758\n",
      "Training Epoch: 2 [48880/49669]\tLoss: 19459.6211\n",
      "Training Epoch: 2 [48900/49669]\tLoss: 18913.4395\n",
      "Training Epoch: 2 [48920/49669]\tLoss: 15358.3936\n",
      "Training Epoch: 2 [48940/49669]\tLoss: 21215.4453\n",
      "Training Epoch: 2 [48960/49669]\tLoss: 18949.2559\n",
      "Training Epoch: 2 [48980/49669]\tLoss: 19529.6875\n",
      "Training Epoch: 2 [49000/49669]\tLoss: 18775.1211\n",
      "Training Epoch: 2 [49020/49669]\tLoss: 17940.4609\n",
      "Training Epoch: 2 [49040/49669]\tLoss: 20964.8730\n",
      "Training Epoch: 2 [49060/49669]\tLoss: 16863.8926\n",
      "Training Epoch: 2 [49080/49669]\tLoss: 19471.1621\n",
      "Training Epoch: 2 [49100/49669]\tLoss: 18162.2559\n",
      "Training Epoch: 2 [49120/49669]\tLoss: 20351.8770\n",
      "Training Epoch: 2 [49140/49669]\tLoss: 20351.2910\n",
      "Training Epoch: 2 [49160/49669]\tLoss: 21224.2500\n",
      "Training Epoch: 2 [49180/49669]\tLoss: 19334.4238\n",
      "Training Epoch: 2 [49200/49669]\tLoss: 18766.4883\n",
      "Training Epoch: 2 [49220/49669]\tLoss: 17398.8457\n",
      "Training Epoch: 2 [49240/49669]\tLoss: 17407.8887\n",
      "Training Epoch: 2 [49260/49669]\tLoss: 20072.7871\n",
      "Training Epoch: 2 [49280/49669]\tLoss: 20626.6328\n",
      "Training Epoch: 2 [49300/49669]\tLoss: 17709.1562\n",
      "Training Epoch: 2 [49320/49669]\tLoss: 18995.6523\n",
      "Training Epoch: 2 [49340/49669]\tLoss: 19850.5234\n",
      "Training Epoch: 2 [49360/49669]\tLoss: 18477.8691\n",
      "Training Epoch: 2 [49380/49669]\tLoss: 18025.9766\n",
      "Training Epoch: 2 [49400/49669]\tLoss: 19200.5684\n",
      "Training Epoch: 2 [49420/49669]\tLoss: 17661.8535\n",
      "Training Epoch: 2 [49440/49669]\tLoss: 20310.6211\n",
      "Training Epoch: 2 [49460/49669]\tLoss: 19621.2031\n",
      "Training Epoch: 2 [49480/49669]\tLoss: 18155.5566\n",
      "Training Epoch: 2 [49500/49669]\tLoss: 15627.9834\n",
      "Training Epoch: 2 [49520/49669]\tLoss: 20239.9316\n",
      "Training Epoch: 2 [49540/49669]\tLoss: 18835.2520\n",
      "Training Epoch: 2 [49560/49669]\tLoss: 18286.8457\n",
      "Training Epoch: 2 [49580/49669]\tLoss: 18581.9961\n",
      "Training Epoch: 2 [49600/49669]\tLoss: 17294.9785\n",
      "Training Epoch: 2 [49620/49669]\tLoss: 21680.3223\n",
      "Training Epoch: 2 [49640/49669]\tLoss: 16568.4980\n",
      "Training Epoch: 2 [49660/49669]\tLoss: 19561.0410\n",
      "Training Epoch: 2 [49669/49669]\tLoss: 19711.0293\n",
      "Training Epoch: 2 [5519/5519]\tLoss: 18352.1507\n",
      "Training Epoch: 3 [20/49669]\tLoss: 19138.0605\n",
      "Training Epoch: 3 [40/49669]\tLoss: 18276.9570\n",
      "Training Epoch: 3 [60/49669]\tLoss: 17212.4883\n",
      "Training Epoch: 3 [80/49669]\tLoss: 19940.3906\n",
      "Training Epoch: 3 [100/49669]\tLoss: 16554.3984\n",
      "Training Epoch: 3 [120/49669]\tLoss: 18259.0879\n",
      "Training Epoch: 3 [140/49669]\tLoss: 17566.9434\n",
      "Training Epoch: 3 [160/49669]\tLoss: 18249.0273\n",
      "Training Epoch: 3 [180/49669]\tLoss: 17278.6035\n",
      "Training Epoch: 3 [200/49669]\tLoss: 19117.8789\n",
      "Training Epoch: 3 [220/49669]\tLoss: 17474.8086\n",
      "Training Epoch: 3 [240/49669]\tLoss: 18266.9023\n",
      "Training Epoch: 3 [260/49669]\tLoss: 20294.3398\n",
      "Training Epoch: 3 [280/49669]\tLoss: 18261.0938\n",
      "Training Epoch: 3 [300/49669]\tLoss: 18101.3359\n",
      "Training Epoch: 3 [320/49669]\tLoss: 18217.9082\n",
      "Training Epoch: 3 [340/49669]\tLoss: 18827.7969\n",
      "Training Epoch: 3 [360/49669]\tLoss: 18288.7148\n",
      "Training Epoch: 3 [380/49669]\tLoss: 17705.2031\n",
      "Training Epoch: 3 [400/49669]\tLoss: 14982.9258\n",
      "Training Epoch: 3 [420/49669]\tLoss: 17510.2324\n",
      "Training Epoch: 3 [440/49669]\tLoss: 15287.2705\n",
      "Training Epoch: 3 [460/49669]\tLoss: 14751.3936\n",
      "Training Epoch: 3 [480/49669]\tLoss: 15800.5254\n",
      "Training Epoch: 3 [500/49669]\tLoss: 21251.5957\n",
      "Training Epoch: 3 [520/49669]\tLoss: 17749.0469\n",
      "Training Epoch: 3 [540/49669]\tLoss: 20684.7598\n",
      "Training Epoch: 3 [560/49669]\tLoss: 17031.0898\n",
      "Training Epoch: 3 [580/49669]\tLoss: 18553.4629\n",
      "Training Epoch: 3 [600/49669]\tLoss: 19866.7539\n",
      "Training Epoch: 3 [620/49669]\tLoss: 16500.8457\n",
      "Training Epoch: 3 [640/49669]\tLoss: 19436.3770\n",
      "Training Epoch: 3 [660/49669]\tLoss: 19284.2383\n",
      "Training Epoch: 3 [680/49669]\tLoss: 16766.1758\n",
      "Training Epoch: 3 [700/49669]\tLoss: 17795.8535\n",
      "Training Epoch: 3 [720/49669]\tLoss: 19479.1582\n",
      "Training Epoch: 3 [740/49669]\tLoss: 16682.0156\n",
      "Training Epoch: 3 [760/49669]\tLoss: 18105.8828\n",
      "Training Epoch: 3 [780/49669]\tLoss: 19395.2266\n",
      "Training Epoch: 3 [800/49669]\tLoss: 18079.4258\n",
      "Training Epoch: 3 [820/49669]\tLoss: 13574.6748\n",
      "Training Epoch: 3 [840/49669]\tLoss: 19764.4629\n",
      "Training Epoch: 3 [860/49669]\tLoss: 18028.6055\n",
      "Training Epoch: 3 [880/49669]\tLoss: 18291.9102\n",
      "Training Epoch: 3 [900/49669]\tLoss: 17451.9590\n",
      "Training Epoch: 3 [920/49669]\tLoss: 16906.6621\n",
      "Training Epoch: 3 [940/49669]\tLoss: 19770.0918\n",
      "Training Epoch: 3 [960/49669]\tLoss: 17851.3594\n",
      "Training Epoch: 3 [980/49669]\tLoss: 16021.5352\n",
      "Training Epoch: 3 [1000/49669]\tLoss: 16223.8574\n",
      "Training Epoch: 3 [1020/49669]\tLoss: 18513.8359\n",
      "Training Epoch: 3 [1040/49669]\tLoss: 18086.1621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [1060/49669]\tLoss: 17101.8652\n",
      "Training Epoch: 3 [1080/49669]\tLoss: 20325.4668\n",
      "Training Epoch: 3 [1100/49669]\tLoss: 18831.2441\n",
      "Training Epoch: 3 [1120/49669]\tLoss: 19243.8340\n",
      "Training Epoch: 3 [1140/49669]\tLoss: 17146.7070\n",
      "Training Epoch: 3 [1160/49669]\tLoss: 19196.7617\n",
      "Training Epoch: 3 [1180/49669]\tLoss: 17737.6582\n",
      "Training Epoch: 3 [1200/49669]\tLoss: 19105.1270\n",
      "Training Epoch: 3 [1220/49669]\tLoss: 18003.9980\n",
      "Training Epoch: 3 [1240/49669]\tLoss: 21508.2168\n",
      "Training Epoch: 3 [1260/49669]\tLoss: 16388.9160\n",
      "Training Epoch: 3 [1280/49669]\tLoss: 17151.4434\n",
      "Training Epoch: 3 [1300/49669]\tLoss: 17889.1465\n",
      "Training Epoch: 3 [1320/49669]\tLoss: 17525.0664\n",
      "Training Epoch: 3 [1340/49669]\tLoss: 17569.7949\n",
      "Training Epoch: 3 [1360/49669]\tLoss: 16844.1973\n",
      "Training Epoch: 3 [1380/49669]\tLoss: 17392.4590\n",
      "Training Epoch: 3 [1400/49669]\tLoss: 18767.2227\n",
      "Training Epoch: 3 [1420/49669]\tLoss: 19819.7637\n",
      "Training Epoch: 3 [1440/49669]\tLoss: 18019.8574\n",
      "Training Epoch: 3 [1460/49669]\tLoss: 19106.1230\n",
      "Training Epoch: 3 [1480/49669]\tLoss: 20122.4727\n",
      "Training Epoch: 3 [1500/49669]\tLoss: 20174.6445\n",
      "Training Epoch: 3 [1520/49669]\tLoss: 17843.9551\n",
      "Training Epoch: 3 [1540/49669]\tLoss: 17940.4668\n",
      "Training Epoch: 3 [1560/49669]\tLoss: 15209.5000\n",
      "Training Epoch: 3 [1580/49669]\tLoss: 18063.7793\n",
      "Training Epoch: 3 [1600/49669]\tLoss: 18293.9551\n",
      "Training Epoch: 3 [1620/49669]\tLoss: 18014.7188\n",
      "Training Epoch: 3 [1640/49669]\tLoss: 19217.2676\n",
      "Training Epoch: 3 [1660/49669]\tLoss: 17315.0801\n",
      "Training Epoch: 3 [1680/49669]\tLoss: 18592.9805\n",
      "Training Epoch: 3 [1700/49669]\tLoss: 15357.8145\n",
      "Training Epoch: 3 [1720/49669]\tLoss: 17967.3711\n",
      "Training Epoch: 3 [1740/49669]\tLoss: 18394.8145\n",
      "Training Epoch: 3 [1760/49669]\tLoss: 16932.8262\n",
      "Training Epoch: 3 [1780/49669]\tLoss: 15756.1631\n",
      "Training Epoch: 3 [1800/49669]\tLoss: 15200.3369\n",
      "Training Epoch: 3 [1820/49669]\tLoss: 18266.5234\n",
      "Training Epoch: 3 [1840/49669]\tLoss: 17753.6191\n",
      "Training Epoch: 3 [1860/49669]\tLoss: 20329.3691\n",
      "Training Epoch: 3 [1880/49669]\tLoss: 18137.0391\n",
      "Training Epoch: 3 [1900/49669]\tLoss: 16301.9346\n",
      "Training Epoch: 3 [1920/49669]\tLoss: 17276.3809\n",
      "Training Epoch: 3 [1940/49669]\tLoss: 19722.6152\n",
      "Training Epoch: 3 [1960/49669]\tLoss: 18976.1270\n",
      "Training Epoch: 3 [1980/49669]\tLoss: 18728.3555\n",
      "Training Epoch: 3 [2000/49669]\tLoss: 18811.0410\n",
      "Training Epoch: 3 [2020/49669]\tLoss: 17446.0879\n",
      "Training Epoch: 3 [2040/49669]\tLoss: 15257.7451\n",
      "Training Epoch: 3 [2060/49669]\tLoss: 17553.0156\n",
      "Training Epoch: 3 [2080/49669]\tLoss: 14803.3809\n",
      "Training Epoch: 3 [2100/49669]\tLoss: 16444.2812\n",
      "Training Epoch: 3 [2120/49669]\tLoss: 18505.8203\n",
      "Training Epoch: 3 [2140/49669]\tLoss: 19139.0410\n",
      "Training Epoch: 3 [2160/49669]\tLoss: 16922.7910\n",
      "Training Epoch: 3 [2180/49669]\tLoss: 18572.2852\n",
      "Training Epoch: 3 [2200/49669]\tLoss: 20823.1797\n",
      "Training Epoch: 3 [2220/49669]\tLoss: 18695.8359\n",
      "Training Epoch: 3 [2240/49669]\tLoss: 17043.8086\n",
      "Training Epoch: 3 [2260/49669]\tLoss: 20714.9863\n",
      "Training Epoch: 3 [2280/49669]\tLoss: 18508.2617\n",
      "Training Epoch: 3 [2300/49669]\tLoss: 18300.5430\n",
      "Training Epoch: 3 [2320/49669]\tLoss: 18605.7734\n",
      "Training Epoch: 3 [2340/49669]\tLoss: 17698.3125\n",
      "Training Epoch: 3 [2360/49669]\tLoss: 17026.4629\n",
      "Training Epoch: 3 [2380/49669]\tLoss: 19598.1328\n",
      "Training Epoch: 3 [2400/49669]\tLoss: 19433.1680\n",
      "Training Epoch: 3 [2420/49669]\tLoss: 18681.5254\n",
      "Training Epoch: 3 [2440/49669]\tLoss: 12874.7930\n",
      "Training Epoch: 3 [2460/49669]\tLoss: 16137.3340\n",
      "Training Epoch: 3 [2480/49669]\tLoss: 17211.4375\n",
      "Training Epoch: 3 [2500/49669]\tLoss: 19314.4844\n",
      "Training Epoch: 3 [2520/49669]\tLoss: 19857.4004\n",
      "Training Epoch: 3 [2540/49669]\tLoss: 16470.1309\n",
      "Training Epoch: 3 [2560/49669]\tLoss: 16984.6641\n",
      "Training Epoch: 3 [2580/49669]\tLoss: 20670.0918\n",
      "Training Epoch: 3 [2600/49669]\tLoss: 17439.8828\n",
      "Training Epoch: 3 [2620/49669]\tLoss: 16756.9902\n",
      "Training Epoch: 3 [2640/49669]\tLoss: 19238.2559\n",
      "Training Epoch: 3 [2660/49669]\tLoss: 18894.6504\n",
      "Training Epoch: 3 [2680/49669]\tLoss: 20347.1328\n",
      "Training Epoch: 3 [2700/49669]\tLoss: 18065.2305\n",
      "Training Epoch: 3 [2720/49669]\tLoss: 20491.7793\n",
      "Training Epoch: 3 [2740/49669]\tLoss: 16734.1504\n",
      "Training Epoch: 3 [2760/49669]\tLoss: 17310.2598\n",
      "Training Epoch: 3 [2780/49669]\tLoss: 17975.9395\n",
      "Training Epoch: 3 [2800/49669]\tLoss: 17404.6523\n",
      "Training Epoch: 3 [2820/49669]\tLoss: 17038.9277\n",
      "Training Epoch: 3 [2840/49669]\tLoss: 19544.9883\n",
      "Training Epoch: 3 [2860/49669]\tLoss: 19795.2402\n",
      "Training Epoch: 3 [2880/49669]\tLoss: 18928.7715\n",
      "Training Epoch: 3 [2900/49669]\tLoss: 17247.2227\n",
      "Training Epoch: 3 [2920/49669]\tLoss: 18484.2168\n",
      "Training Epoch: 3 [2940/49669]\tLoss: 15607.9297\n",
      "Training Epoch: 3 [2960/49669]\tLoss: 17065.4941\n",
      "Training Epoch: 3 [2980/49669]\tLoss: 15299.2803\n",
      "Training Epoch: 3 [3000/49669]\tLoss: 18226.6680\n",
      "Training Epoch: 3 [3020/49669]\tLoss: 14681.3252\n",
      "Training Epoch: 3 [3040/49669]\tLoss: 18115.9824\n",
      "Training Epoch: 3 [3060/49669]\tLoss: 20373.4043\n",
      "Training Epoch: 3 [3080/49669]\tLoss: 17253.8203\n",
      "Training Epoch: 3 [3100/49669]\tLoss: 19166.2676\n",
      "Training Epoch: 3 [3120/49669]\tLoss: 18629.8516\n",
      "Training Epoch: 3 [3140/49669]\tLoss: 17625.9434\n",
      "Training Epoch: 3 [3160/49669]\tLoss: 18093.6152\n",
      "Training Epoch: 3 [3180/49669]\tLoss: 19783.2207\n",
      "Training Epoch: 3 [3200/49669]\tLoss: 18651.8477\n",
      "Training Epoch: 3 [3220/49669]\tLoss: 15266.2461\n",
      "Training Epoch: 3 [3240/49669]\tLoss: 18305.7637\n",
      "Training Epoch: 3 [3260/49669]\tLoss: 17312.1523\n",
      "Training Epoch: 3 [3280/49669]\tLoss: 19389.0703\n",
      "Training Epoch: 3 [3300/49669]\tLoss: 18603.3281\n",
      "Training Epoch: 3 [3320/49669]\tLoss: 19053.8086\n",
      "Training Epoch: 3 [3340/49669]\tLoss: 19451.2070\n",
      "Training Epoch: 3 [3360/49669]\tLoss: 16749.5117\n",
      "Training Epoch: 3 [3380/49669]\tLoss: 17735.5508\n",
      "Training Epoch: 3 [3400/49669]\tLoss: 20614.0820\n",
      "Training Epoch: 3 [3420/49669]\tLoss: 17963.0527\n",
      "Training Epoch: 3 [3440/49669]\tLoss: 18357.1875\n",
      "Training Epoch: 3 [3460/49669]\tLoss: 18375.1914\n",
      "Training Epoch: 3 [3480/49669]\tLoss: 16117.4971\n",
      "Training Epoch: 3 [3500/49669]\tLoss: 17765.2012\n",
      "Training Epoch: 3 [3520/49669]\tLoss: 16883.8770\n",
      "Training Epoch: 3 [3540/49669]\tLoss: 16442.9688\n",
      "Training Epoch: 3 [3560/49669]\tLoss: 18712.5879\n",
      "Training Epoch: 3 [3580/49669]\tLoss: 17042.9160\n",
      "Training Epoch: 3 [3600/49669]\tLoss: 15809.0684\n",
      "Training Epoch: 3 [3620/49669]\tLoss: 17860.6035\n",
      "Training Epoch: 3 [3640/49669]\tLoss: 17664.8359\n",
      "Training Epoch: 3 [3660/49669]\tLoss: 19748.3926\n",
      "Training Epoch: 3 [3680/49669]\tLoss: 18311.2129\n",
      "Training Epoch: 3 [3700/49669]\tLoss: 17995.5723\n",
      "Training Epoch: 3 [3720/49669]\tLoss: 17588.2520\n",
      "Training Epoch: 3 [3740/49669]\tLoss: 18848.3203\n",
      "Training Epoch: 3 [3760/49669]\tLoss: 20462.8281\n",
      "Training Epoch: 3 [3780/49669]\tLoss: 19327.2500\n",
      "Training Epoch: 3 [3800/49669]\tLoss: 18468.1914\n",
      "Training Epoch: 3 [3820/49669]\tLoss: 15883.6230\n",
      "Training Epoch: 3 [3840/49669]\tLoss: 17593.3867\n",
      "Training Epoch: 3 [3860/49669]\tLoss: 17907.3457\n",
      "Training Epoch: 3 [3880/49669]\tLoss: 16704.0723\n",
      "Training Epoch: 3 [3900/49669]\tLoss: 18086.0625\n",
      "Training Epoch: 3 [3920/49669]\tLoss: 16309.0713\n",
      "Training Epoch: 3 [3940/49669]\tLoss: 17460.9766\n",
      "Training Epoch: 3 [3960/49669]\tLoss: 18553.4863\n",
      "Training Epoch: 3 [3980/49669]\tLoss: 20650.2461\n",
      "Training Epoch: 3 [4000/49669]\tLoss: 16483.7656\n",
      "Training Epoch: 3 [4020/49669]\tLoss: 15611.2666\n",
      "Training Epoch: 3 [4040/49669]\tLoss: 16880.8672\n",
      "Training Epoch: 3 [4060/49669]\tLoss: 17520.8887\n",
      "Training Epoch: 3 [4080/49669]\tLoss: 16622.6699\n",
      "Training Epoch: 3 [4100/49669]\tLoss: 16049.6787\n",
      "Training Epoch: 3 [4120/49669]\tLoss: 16135.0146\n",
      "Training Epoch: 3 [4140/49669]\tLoss: 17658.6230\n",
      "Training Epoch: 3 [4160/49669]\tLoss: 18776.3730\n",
      "Training Epoch: 3 [4180/49669]\tLoss: 16224.3223\n",
      "Training Epoch: 3 [4200/49669]\tLoss: 17320.8965\n",
      "Training Epoch: 3 [4220/49669]\tLoss: 17902.4766\n",
      "Training Epoch: 3 [4240/49669]\tLoss: 16250.1084\n",
      "Training Epoch: 3 [4260/49669]\tLoss: 14290.7236\n",
      "Training Epoch: 3 [4280/49669]\tLoss: 17366.1699\n",
      "Training Epoch: 3 [4300/49669]\tLoss: 16575.3750\n",
      "Training Epoch: 3 [4320/49669]\tLoss: 16617.5000\n",
      "Training Epoch: 3 [4340/49669]\tLoss: 16389.5527\n",
      "Training Epoch: 3 [4360/49669]\tLoss: 17735.8887\n",
      "Training Epoch: 3 [4380/49669]\tLoss: 20123.0352\n",
      "Training Epoch: 3 [4400/49669]\tLoss: 16499.2930\n",
      "Training Epoch: 3 [4420/49669]\tLoss: 17377.2617\n",
      "Training Epoch: 3 [4440/49669]\tLoss: 18354.5000\n",
      "Training Epoch: 3 [4460/49669]\tLoss: 17644.3281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [4480/49669]\tLoss: 18184.3965\n",
      "Training Epoch: 3 [4500/49669]\tLoss: 18435.3438\n",
      "Training Epoch: 3 [4520/49669]\tLoss: 17972.2031\n",
      "Training Epoch: 3 [4540/49669]\tLoss: 17900.8086\n",
      "Training Epoch: 3 [4560/49669]\tLoss: 17084.8242\n",
      "Training Epoch: 3 [4580/49669]\tLoss: 15658.4834\n",
      "Training Epoch: 3 [4600/49669]\tLoss: 18796.5664\n",
      "Training Epoch: 3 [4620/49669]\tLoss: 16831.2324\n",
      "Training Epoch: 3 [4640/49669]\tLoss: 15426.4600\n",
      "Training Epoch: 3 [4660/49669]\tLoss: 18030.1660\n",
      "Training Epoch: 3 [4680/49669]\tLoss: 16169.9746\n",
      "Training Epoch: 3 [4700/49669]\tLoss: 16590.3633\n",
      "Training Epoch: 3 [4720/49669]\tLoss: 17968.7559\n",
      "Training Epoch: 3 [4740/49669]\tLoss: 19559.3887\n",
      "Training Epoch: 3 [4760/49669]\tLoss: 18864.4980\n",
      "Training Epoch: 3 [4780/49669]\tLoss: 13495.7510\n",
      "Training Epoch: 3 [4800/49669]\tLoss: 15838.0283\n",
      "Training Epoch: 3 [4820/49669]\tLoss: 19089.5801\n",
      "Training Epoch: 3 [4840/49669]\tLoss: 18214.6484\n",
      "Training Epoch: 3 [4860/49669]\tLoss: 20291.7266\n",
      "Training Epoch: 3 [4880/49669]\tLoss: 16569.9297\n",
      "Training Epoch: 3 [4900/49669]\tLoss: 18760.0723\n",
      "Training Epoch: 3 [4920/49669]\tLoss: 15290.4609\n",
      "Training Epoch: 3 [4940/49669]\tLoss: 19668.6484\n",
      "Training Epoch: 3 [4960/49669]\tLoss: 16910.6953\n",
      "Training Epoch: 3 [4980/49669]\tLoss: 16373.8223\n",
      "Training Epoch: 3 [5000/49669]\tLoss: 19813.5059\n",
      "Training Epoch: 3 [5020/49669]\tLoss: 17589.1699\n",
      "Training Epoch: 3 [5040/49669]\tLoss: 16498.4395\n",
      "Training Epoch: 3 [5060/49669]\tLoss: 17096.9395\n",
      "Training Epoch: 3 [5080/49669]\tLoss: 17107.9727\n",
      "Training Epoch: 3 [5100/49669]\tLoss: 17676.8027\n",
      "Training Epoch: 3 [5120/49669]\tLoss: 19544.4434\n",
      "Training Epoch: 3 [5140/49669]\tLoss: 15715.6631\n",
      "Training Epoch: 3 [5160/49669]\tLoss: 18084.0527\n",
      "Training Epoch: 3 [5180/49669]\tLoss: 17588.2773\n",
      "Training Epoch: 3 [5200/49669]\tLoss: 16254.1729\n",
      "Training Epoch: 3 [5220/49669]\tLoss: 15639.7637\n",
      "Training Epoch: 3 [5240/49669]\tLoss: 15791.0225\n",
      "Training Epoch: 3 [5260/49669]\tLoss: 15550.4619\n",
      "Training Epoch: 3 [5280/49669]\tLoss: 19202.1250\n",
      "Training Epoch: 3 [5300/49669]\tLoss: 17832.9746\n",
      "Training Epoch: 3 [5320/49669]\tLoss: 19007.0430\n",
      "Training Epoch: 3 [5340/49669]\tLoss: 18438.2500\n",
      "Training Epoch: 3 [5360/49669]\tLoss: 16655.9160\n",
      "Training Epoch: 3 [5380/49669]\tLoss: 17314.6133\n",
      "Training Epoch: 3 [5400/49669]\tLoss: 14936.5410\n",
      "Training Epoch: 3 [5420/49669]\tLoss: 16900.8574\n",
      "Training Epoch: 3 [5440/49669]\tLoss: 17206.2930\n",
      "Training Epoch: 3 [5460/49669]\tLoss: 14004.7295\n",
      "Training Epoch: 3 [5480/49669]\tLoss: 17826.0391\n",
      "Training Epoch: 3 [5500/49669]\tLoss: 19340.1270\n",
      "Training Epoch: 3 [5520/49669]\tLoss: 16236.1592\n",
      "Training Epoch: 3 [5540/49669]\tLoss: 17262.3887\n",
      "Training Epoch: 3 [5560/49669]\tLoss: 17375.4531\n",
      "Training Epoch: 3 [5580/49669]\tLoss: 18122.0957\n",
      "Training Epoch: 3 [5600/49669]\tLoss: 16331.1416\n",
      "Training Epoch: 3 [5620/49669]\tLoss: 14290.7246\n",
      "Training Epoch: 3 [5640/49669]\tLoss: 14564.0967\n",
      "Training Epoch: 3 [5660/49669]\tLoss: 16918.5566\n",
      "Training Epoch: 3 [5680/49669]\tLoss: 18515.9746\n",
      "Training Epoch: 3 [5700/49669]\tLoss: 16301.6396\n",
      "Training Epoch: 3 [5720/49669]\tLoss: 13214.0283\n",
      "Training Epoch: 3 [5740/49669]\tLoss: 18771.6367\n",
      "Training Epoch: 3 [5760/49669]\tLoss: 18444.7031\n",
      "Training Epoch: 3 [5780/49669]\tLoss: 17047.5605\n",
      "Training Epoch: 3 [5800/49669]\tLoss: 17697.3828\n",
      "Training Epoch: 3 [5820/49669]\tLoss: 16085.6719\n",
      "Training Epoch: 3 [5840/49669]\tLoss: 17250.5742\n",
      "Training Epoch: 3 [5860/49669]\tLoss: 19344.8301\n",
      "Training Epoch: 3 [5880/49669]\tLoss: 16382.9072\n",
      "Training Epoch: 3 [5900/49669]\tLoss: 18214.0469\n",
      "Training Epoch: 3 [5920/49669]\tLoss: 17064.4980\n",
      "Training Epoch: 3 [5940/49669]\tLoss: 18868.5039\n",
      "Training Epoch: 3 [5960/49669]\tLoss: 16024.2510\n",
      "Training Epoch: 3 [5980/49669]\tLoss: 16282.0645\n",
      "Training Epoch: 3 [6000/49669]\tLoss: 15224.3145\n",
      "Training Epoch: 3 [6020/49669]\tLoss: 16541.8242\n",
      "Training Epoch: 3 [6040/49669]\tLoss: 20510.3926\n",
      "Training Epoch: 3 [6060/49669]\tLoss: 18409.1055\n",
      "Training Epoch: 3 [6080/49669]\tLoss: 17628.7461\n",
      "Training Epoch: 3 [6100/49669]\tLoss: 17695.7461\n",
      "Training Epoch: 3 [6120/49669]\tLoss: 16525.2559\n",
      "Training Epoch: 3 [6140/49669]\tLoss: 17140.8203\n",
      "Training Epoch: 3 [6160/49669]\tLoss: 18829.3359\n",
      "Training Epoch: 3 [6180/49669]\tLoss: 16718.5312\n",
      "Training Epoch: 3 [6200/49669]\tLoss: 18048.9355\n",
      "Training Epoch: 3 [6220/49669]\tLoss: 19902.8652\n",
      "Training Epoch: 3 [6240/49669]\tLoss: 19242.4551\n",
      "Training Epoch: 3 [6260/49669]\tLoss: 19910.3926\n",
      "Training Epoch: 3 [6280/49669]\tLoss: 15926.6582\n",
      "Training Epoch: 3 [6300/49669]\tLoss: 18123.1738\n",
      "Training Epoch: 3 [6320/49669]\tLoss: 18039.1152\n",
      "Training Epoch: 3 [6340/49669]\tLoss: 18565.8164\n",
      "Training Epoch: 3 [6360/49669]\tLoss: 18257.6816\n",
      "Training Epoch: 3 [6380/49669]\tLoss: 18717.2910\n",
      "Training Epoch: 3 [6400/49669]\tLoss: 15289.8213\n",
      "Training Epoch: 3 [6420/49669]\tLoss: 13984.4893\n",
      "Training Epoch: 3 [6440/49669]\tLoss: 15529.2549\n",
      "Training Epoch: 3 [6460/49669]\tLoss: 17814.0371\n",
      "Training Epoch: 3 [6480/49669]\tLoss: 16877.4961\n",
      "Training Epoch: 3 [6500/49669]\tLoss: 18421.6855\n",
      "Training Epoch: 3 [6520/49669]\tLoss: 19548.5254\n",
      "Training Epoch: 3 [6540/49669]\tLoss: 14939.2852\n",
      "Training Epoch: 3 [6560/49669]\tLoss: 19843.4512\n",
      "Training Epoch: 3 [6580/49669]\tLoss: 19147.0820\n",
      "Training Epoch: 3 [6600/49669]\tLoss: 17644.4258\n",
      "Training Epoch: 3 [6620/49669]\tLoss: 19232.6855\n",
      "Training Epoch: 3 [6640/49669]\tLoss: 15185.9619\n",
      "Training Epoch: 3 [6660/49669]\tLoss: 17938.8262\n",
      "Training Epoch: 3 [6680/49669]\tLoss: 16838.1719\n",
      "Training Epoch: 3 [6700/49669]\tLoss: 17791.3086\n",
      "Training Epoch: 3 [6720/49669]\tLoss: 16181.1025\n",
      "Training Epoch: 3 [6740/49669]\tLoss: 19249.6719\n",
      "Training Epoch: 3 [6760/49669]\tLoss: 17089.8086\n",
      "Training Epoch: 3 [6780/49669]\tLoss: 16755.2598\n",
      "Training Epoch: 3 [6800/49669]\tLoss: 17065.2344\n",
      "Training Epoch: 3 [6820/49669]\tLoss: 18240.6230\n",
      "Training Epoch: 3 [6840/49669]\tLoss: 15338.5283\n",
      "Training Epoch: 3 [6860/49669]\tLoss: 18650.5898\n",
      "Training Epoch: 3 [6880/49669]\tLoss: 16149.1416\n",
      "Training Epoch: 3 [6900/49669]\tLoss: 19069.6758\n",
      "Training Epoch: 3 [6920/49669]\tLoss: 19107.1504\n",
      "Training Epoch: 3 [6940/49669]\tLoss: 17238.3750\n",
      "Training Epoch: 3 [6960/49669]\tLoss: 17234.8965\n",
      "Training Epoch: 3 [6980/49669]\tLoss: 15290.6982\n",
      "Training Epoch: 3 [7000/49669]\tLoss: 15069.9453\n",
      "Training Epoch: 3 [7020/49669]\tLoss: 17368.7383\n",
      "Training Epoch: 3 [7040/49669]\tLoss: 17429.5391\n",
      "Training Epoch: 3 [7060/49669]\tLoss: 17687.5508\n",
      "Training Epoch: 3 [7080/49669]\tLoss: 18629.9160\n",
      "Training Epoch: 3 [7100/49669]\tLoss: 16711.2617\n",
      "Training Epoch: 3 [7120/49669]\tLoss: 17660.2070\n",
      "Training Epoch: 3 [7140/49669]\tLoss: 19575.5430\n",
      "Training Epoch: 3 [7160/49669]\tLoss: 17788.8418\n",
      "Training Epoch: 3 [7180/49669]\tLoss: 17077.5156\n",
      "Training Epoch: 3 [7200/49669]\tLoss: 16389.5684\n",
      "Training Epoch: 3 [7220/49669]\tLoss: 19345.7109\n",
      "Training Epoch: 3 [7240/49669]\tLoss: 18658.3184\n",
      "Training Epoch: 3 [7260/49669]\tLoss: 17331.8535\n",
      "Training Epoch: 3 [7280/49669]\tLoss: 17549.9941\n",
      "Training Epoch: 3 [7300/49669]\tLoss: 16109.8125\n",
      "Training Epoch: 3 [7320/49669]\tLoss: 19486.6504\n",
      "Training Epoch: 3 [7340/49669]\tLoss: 17888.3145\n",
      "Training Epoch: 3 [7360/49669]\tLoss: 15357.5176\n",
      "Training Epoch: 3 [7380/49669]\tLoss: 16312.0195\n",
      "Training Epoch: 3 [7400/49669]\tLoss: 18474.2363\n",
      "Training Epoch: 3 [7420/49669]\tLoss: 17506.2031\n",
      "Training Epoch: 3 [7440/49669]\tLoss: 17238.3379\n",
      "Training Epoch: 3 [7460/49669]\tLoss: 16118.8740\n",
      "Training Epoch: 3 [7480/49669]\tLoss: 16248.1670\n",
      "Training Epoch: 3 [7500/49669]\tLoss: 18702.2070\n",
      "Training Epoch: 3 [7520/49669]\tLoss: 14524.0566\n",
      "Training Epoch: 3 [7540/49669]\tLoss: 16139.2949\n",
      "Training Epoch: 3 [7560/49669]\tLoss: 14100.5322\n",
      "Training Epoch: 3 [7580/49669]\tLoss: 13687.1016\n",
      "Training Epoch: 3 [7600/49669]\tLoss: 17743.0254\n",
      "Training Epoch: 3 [7620/49669]\tLoss: 19609.4648\n",
      "Training Epoch: 3 [7640/49669]\tLoss: 15799.3672\n",
      "Training Epoch: 3 [7660/49669]\tLoss: 18838.3555\n",
      "Training Epoch: 3 [7680/49669]\tLoss: 17284.1992\n",
      "Training Epoch: 3 [7700/49669]\tLoss: 15800.9639\n",
      "Training Epoch: 3 [7720/49669]\tLoss: 16549.5957\n",
      "Training Epoch: 3 [7740/49669]\tLoss: 17644.9297\n",
      "Training Epoch: 3 [7760/49669]\tLoss: 17021.7461\n",
      "Training Epoch: 3 [7780/49669]\tLoss: 17660.4180\n",
      "Training Epoch: 3 [7800/49669]\tLoss: 15643.3008\n",
      "Training Epoch: 3 [7820/49669]\tLoss: 17232.1816\n",
      "Training Epoch: 3 [7840/49669]\tLoss: 16514.9746\n",
      "Training Epoch: 3 [7860/49669]\tLoss: 19182.2363\n",
      "Training Epoch: 3 [7880/49669]\tLoss: 16588.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [7900/49669]\tLoss: 18185.2676\n",
      "Training Epoch: 3 [7920/49669]\tLoss: 17641.2363\n",
      "Training Epoch: 3 [7940/49669]\tLoss: 18980.6758\n",
      "Training Epoch: 3 [7960/49669]\tLoss: 19114.7266\n",
      "Training Epoch: 3 [7980/49669]\tLoss: 16302.0391\n",
      "Training Epoch: 3 [8000/49669]\tLoss: 17305.6582\n",
      "Training Epoch: 3 [8020/49669]\tLoss: 19266.2285\n",
      "Training Epoch: 3 [8040/49669]\tLoss: 18378.6250\n",
      "Training Epoch: 3 [8060/49669]\tLoss: 17817.3770\n",
      "Training Epoch: 3 [8080/49669]\tLoss: 15603.9570\n",
      "Training Epoch: 3 [8100/49669]\tLoss: 15089.2051\n",
      "Training Epoch: 3 [8120/49669]\tLoss: 19019.1816\n",
      "Training Epoch: 3 [8140/49669]\tLoss: 16711.1504\n",
      "Training Epoch: 3 [8160/49669]\tLoss: 15209.0381\n",
      "Training Epoch: 3 [8180/49669]\tLoss: 17533.3184\n",
      "Training Epoch: 3 [8200/49669]\tLoss: 15909.0479\n",
      "Training Epoch: 3 [8220/49669]\tLoss: 16122.3047\n",
      "Training Epoch: 3 [8240/49669]\tLoss: 17699.4121\n",
      "Training Epoch: 3 [8260/49669]\tLoss: 17166.3047\n",
      "Training Epoch: 3 [8280/49669]\tLoss: 18259.3379\n",
      "Training Epoch: 3 [8300/49669]\tLoss: 13188.5420\n",
      "Training Epoch: 3 [8320/49669]\tLoss: 17783.4180\n",
      "Training Epoch: 3 [8340/49669]\tLoss: 17142.5508\n",
      "Training Epoch: 3 [8360/49669]\tLoss: 16315.4170\n",
      "Training Epoch: 3 [8380/49669]\tLoss: 17339.7188\n",
      "Training Epoch: 3 [8400/49669]\tLoss: 15383.8389\n",
      "Training Epoch: 3 [8420/49669]\tLoss: 17593.3281\n",
      "Training Epoch: 3 [8440/49669]\tLoss: 15990.8066\n",
      "Training Epoch: 3 [8460/49669]\tLoss: 16508.4121\n",
      "Training Epoch: 3 [8480/49669]\tLoss: 19129.2559\n",
      "Training Epoch: 3 [8500/49669]\tLoss: 14945.5703\n",
      "Training Epoch: 3 [8520/49669]\tLoss: 18999.3398\n",
      "Training Epoch: 3 [8540/49669]\tLoss: 16136.0664\n",
      "Training Epoch: 3 [8560/49669]\tLoss: 15568.6270\n",
      "Training Epoch: 3 [8580/49669]\tLoss: 16996.0430\n",
      "Training Epoch: 3 [8600/49669]\tLoss: 14646.6309\n",
      "Training Epoch: 3 [8620/49669]\tLoss: 15663.0801\n",
      "Training Epoch: 3 [8640/49669]\tLoss: 17099.9863\n",
      "Training Epoch: 3 [8660/49669]\tLoss: 17351.7363\n",
      "Training Epoch: 3 [8680/49669]\tLoss: 16822.7500\n",
      "Training Epoch: 3 [8700/49669]\tLoss: 18773.5000\n",
      "Training Epoch: 3 [8720/49669]\tLoss: 14362.7578\n",
      "Training Epoch: 3 [8740/49669]\tLoss: 17593.2773\n",
      "Training Epoch: 3 [8760/49669]\tLoss: 17527.5742\n",
      "Training Epoch: 3 [8780/49669]\tLoss: 16524.2754\n",
      "Training Epoch: 3 [8800/49669]\tLoss: 15633.7354\n",
      "Training Epoch: 3 [8820/49669]\tLoss: 15605.1553\n",
      "Training Epoch: 3 [8840/49669]\tLoss: 19493.5605\n",
      "Training Epoch: 3 [8860/49669]\tLoss: 17485.8105\n",
      "Training Epoch: 3 [8880/49669]\tLoss: 16318.6758\n",
      "Training Epoch: 3 [8900/49669]\tLoss: 17001.1074\n",
      "Training Epoch: 3 [8920/49669]\tLoss: 19271.2891\n",
      "Training Epoch: 3 [8940/49669]\tLoss: 16136.5088\n",
      "Training Epoch: 3 [8960/49669]\tLoss: 15635.9766\n",
      "Training Epoch: 3 [8980/49669]\tLoss: 17181.6172\n",
      "Training Epoch: 3 [9000/49669]\tLoss: 18845.4121\n",
      "Training Epoch: 3 [9020/49669]\tLoss: 16299.6992\n",
      "Training Epoch: 3 [9040/49669]\tLoss: 18232.0254\n",
      "Training Epoch: 3 [9060/49669]\tLoss: 16923.1406\n",
      "Training Epoch: 3 [9080/49669]\tLoss: 16966.4531\n",
      "Training Epoch: 3 [9100/49669]\tLoss: 18514.2559\n",
      "Training Epoch: 3 [9120/49669]\tLoss: 17090.6133\n",
      "Training Epoch: 3 [9140/49669]\tLoss: 16693.6035\n",
      "Training Epoch: 3 [9160/49669]\tLoss: 18894.9199\n",
      "Training Epoch: 3 [9180/49669]\tLoss: 18154.2324\n",
      "Training Epoch: 3 [9200/49669]\tLoss: 16325.2598\n",
      "Training Epoch: 3 [9220/49669]\tLoss: 16329.0342\n",
      "Training Epoch: 3 [9240/49669]\tLoss: 17694.4551\n",
      "Training Epoch: 3 [9260/49669]\tLoss: 17117.3574\n",
      "Training Epoch: 3 [9280/49669]\tLoss: 19952.5098\n",
      "Training Epoch: 3 [9300/49669]\tLoss: 16364.8027\n",
      "Training Epoch: 3 [9320/49669]\tLoss: 16106.8643\n",
      "Training Epoch: 3 [9340/49669]\tLoss: 15395.7305\n",
      "Training Epoch: 3 [9360/49669]\tLoss: 16264.8232\n",
      "Training Epoch: 3 [9380/49669]\tLoss: 17241.6504\n",
      "Training Epoch: 3 [9400/49669]\tLoss: 14396.0723\n",
      "Training Epoch: 3 [9420/49669]\tLoss: 17354.2070\n",
      "Training Epoch: 3 [9440/49669]\tLoss: 15024.6523\n",
      "Training Epoch: 3 [9460/49669]\tLoss: 18132.1113\n",
      "Training Epoch: 3 [9480/49669]\tLoss: 15472.3584\n",
      "Training Epoch: 3 [9500/49669]\tLoss: 19245.0938\n",
      "Training Epoch: 3 [9520/49669]\tLoss: 16045.7256\n",
      "Training Epoch: 3 [9540/49669]\tLoss: 16952.2031\n",
      "Training Epoch: 3 [9560/49669]\tLoss: 16279.9561\n",
      "Training Epoch: 3 [9580/49669]\tLoss: 17465.3242\n",
      "Training Epoch: 3 [9600/49669]\tLoss: 16829.2227\n",
      "Training Epoch: 3 [9620/49669]\tLoss: 15879.7627\n",
      "Training Epoch: 3 [9640/49669]\tLoss: 17552.8262\n",
      "Training Epoch: 3 [9660/49669]\tLoss: 15608.7314\n",
      "Training Epoch: 3 [9680/49669]\tLoss: 16937.6465\n",
      "Training Epoch: 3 [9700/49669]\tLoss: 17485.4277\n",
      "Training Epoch: 3 [9720/49669]\tLoss: 17939.5469\n",
      "Training Epoch: 3 [9740/49669]\tLoss: 16017.4922\n",
      "Training Epoch: 3 [9760/49669]\tLoss: 16399.2402\n",
      "Training Epoch: 3 [9780/49669]\tLoss: 16694.3027\n",
      "Training Epoch: 3 [9800/49669]\tLoss: 17636.5801\n",
      "Training Epoch: 3 [9820/49669]\tLoss: 17988.0449\n",
      "Training Epoch: 3 [9840/49669]\tLoss: 15713.3887\n",
      "Training Epoch: 3 [9860/49669]\tLoss: 15154.6035\n",
      "Training Epoch: 3 [9880/49669]\tLoss: 19058.0215\n",
      "Training Epoch: 3 [9900/49669]\tLoss: 15879.0244\n",
      "Training Epoch: 3 [9920/49669]\tLoss: 17515.6172\n",
      "Training Epoch: 3 [9940/49669]\tLoss: 16307.7959\n",
      "Training Epoch: 3 [9960/49669]\tLoss: 14154.4316\n",
      "Training Epoch: 3 [9980/49669]\tLoss: 17743.4648\n",
      "Training Epoch: 3 [10000/49669]\tLoss: 16147.1406\n",
      "Training Epoch: 3 [10020/49669]\tLoss: 15929.9961\n",
      "Training Epoch: 3 [10040/49669]\tLoss: 15918.8662\n",
      "Training Epoch: 3 [10060/49669]\tLoss: 16179.6455\n",
      "Training Epoch: 3 [10080/49669]\tLoss: 16168.1074\n",
      "Training Epoch: 3 [10100/49669]\tLoss: 18168.7812\n",
      "Training Epoch: 3 [10120/49669]\tLoss: 17381.9102\n",
      "Training Epoch: 3 [10140/49669]\tLoss: 16555.4141\n",
      "Training Epoch: 3 [10160/49669]\tLoss: 16320.4883\n",
      "Training Epoch: 3 [10180/49669]\tLoss: 19638.5273\n",
      "Training Epoch: 3 [10200/49669]\tLoss: 15690.3271\n",
      "Training Epoch: 3 [10220/49669]\tLoss: 17006.8438\n",
      "Training Epoch: 3 [10240/49669]\tLoss: 16308.0234\n",
      "Training Epoch: 3 [10260/49669]\tLoss: 14963.9297\n",
      "Training Epoch: 3 [10280/49669]\tLoss: 13264.5801\n",
      "Training Epoch: 3 [10300/49669]\tLoss: 17381.4727\n",
      "Training Epoch: 3 [10320/49669]\tLoss: 15912.7109\n",
      "Training Epoch: 3 [10340/49669]\tLoss: 17373.4414\n",
      "Training Epoch: 3 [10360/49669]\tLoss: 17592.1660\n",
      "Training Epoch: 3 [10380/49669]\tLoss: 16183.6982\n",
      "Training Epoch: 3 [10400/49669]\tLoss: 17840.1758\n",
      "Training Epoch: 3 [10420/49669]\tLoss: 17331.1660\n",
      "Training Epoch: 3 [10440/49669]\tLoss: 18375.9414\n",
      "Training Epoch: 3 [10460/49669]\tLoss: 18083.5000\n",
      "Training Epoch: 3 [10480/49669]\tLoss: 16289.5264\n",
      "Training Epoch: 3 [10500/49669]\tLoss: 17644.1328\n",
      "Training Epoch: 3 [10520/49669]\tLoss: 16405.8789\n",
      "Training Epoch: 3 [10540/49669]\tLoss: 16709.1406\n",
      "Training Epoch: 3 [10560/49669]\tLoss: 18604.5312\n",
      "Training Epoch: 3 [10580/49669]\tLoss: 18070.1738\n",
      "Training Epoch: 3 [10600/49669]\tLoss: 17612.9141\n",
      "Training Epoch: 3 [10620/49669]\tLoss: 17435.9590\n",
      "Training Epoch: 3 [10640/49669]\tLoss: 17257.7266\n",
      "Training Epoch: 3 [10660/49669]\tLoss: 16709.1855\n",
      "Training Epoch: 3 [10680/49669]\tLoss: 18777.8164\n",
      "Training Epoch: 3 [10700/49669]\tLoss: 16229.8281\n",
      "Training Epoch: 3 [10720/49669]\tLoss: 17895.9199\n",
      "Training Epoch: 3 [10740/49669]\tLoss: 16649.6914\n",
      "Training Epoch: 3 [10760/49669]\tLoss: 17137.3848\n",
      "Training Epoch: 3 [10780/49669]\tLoss: 16651.4473\n",
      "Training Epoch: 3 [10800/49669]\tLoss: 19489.6035\n",
      "Training Epoch: 3 [10820/49669]\tLoss: 17485.4238\n",
      "Training Epoch: 3 [10840/49669]\tLoss: 15559.5645\n",
      "Training Epoch: 3 [10860/49669]\tLoss: 16550.4375\n",
      "Training Epoch: 3 [10880/49669]\tLoss: 17277.0117\n",
      "Training Epoch: 3 [10900/49669]\tLoss: 15409.6152\n",
      "Training Epoch: 3 [10920/49669]\tLoss: 15635.0244\n",
      "Training Epoch: 3 [10940/49669]\tLoss: 17486.2305\n",
      "Training Epoch: 3 [10960/49669]\tLoss: 16215.1426\n",
      "Training Epoch: 3 [10980/49669]\tLoss: 16622.1895\n",
      "Training Epoch: 3 [11000/49669]\tLoss: 17316.0059\n",
      "Training Epoch: 3 [11020/49669]\tLoss: 16960.2441\n",
      "Training Epoch: 3 [11040/49669]\tLoss: 15659.9033\n",
      "Training Epoch: 3 [11060/49669]\tLoss: 17535.1074\n",
      "Training Epoch: 3 [11080/49669]\tLoss: 13097.5859\n",
      "Training Epoch: 3 [11100/49669]\tLoss: 17818.8242\n",
      "Training Epoch: 3 [11120/49669]\tLoss: 15778.9717\n",
      "Training Epoch: 3 [11140/49669]\tLoss: 16827.8281\n",
      "Training Epoch: 3 [11160/49669]\tLoss: 18697.5898\n",
      "Training Epoch: 3 [11180/49669]\tLoss: 15265.4014\n",
      "Training Epoch: 3 [11200/49669]\tLoss: 19575.5059\n",
      "Training Epoch: 3 [11220/49669]\tLoss: 17108.2324\n",
      "Training Epoch: 3 [11240/49669]\tLoss: 15598.3291\n",
      "Training Epoch: 3 [11260/49669]\tLoss: 16025.6621\n",
      "Training Epoch: 3 [11280/49669]\tLoss: 17659.4980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [11300/49669]\tLoss: 19099.5039\n",
      "Training Epoch: 3 [11320/49669]\tLoss: 14966.3945\n",
      "Training Epoch: 3 [11340/49669]\tLoss: 17906.9961\n",
      "Training Epoch: 3 [11360/49669]\tLoss: 16601.5234\n",
      "Training Epoch: 3 [11380/49669]\tLoss: 15776.1592\n",
      "Training Epoch: 3 [11400/49669]\tLoss: 15018.8301\n",
      "Training Epoch: 3 [11420/49669]\tLoss: 19045.3672\n",
      "Training Epoch: 3 [11440/49669]\tLoss: 16773.1777\n",
      "Training Epoch: 3 [11460/49669]\tLoss: 16601.1035\n",
      "Training Epoch: 3 [11480/49669]\tLoss: 19084.3066\n",
      "Training Epoch: 3 [11500/49669]\tLoss: 16858.9648\n",
      "Training Epoch: 3 [11520/49669]\tLoss: 17010.9746\n",
      "Training Epoch: 3 [11540/49669]\tLoss: 17362.5703\n",
      "Training Epoch: 3 [11560/49669]\tLoss: 16498.4102\n",
      "Training Epoch: 3 [11580/49669]\tLoss: 16812.2148\n",
      "Training Epoch: 3 [11600/49669]\tLoss: 17312.2188\n",
      "Training Epoch: 3 [11620/49669]\tLoss: 15668.7324\n",
      "Training Epoch: 3 [11640/49669]\tLoss: 12396.1523\n",
      "Training Epoch: 3 [11660/49669]\tLoss: 13803.1729\n",
      "Training Epoch: 3 [11680/49669]\tLoss: 17118.2988\n",
      "Training Epoch: 3 [11700/49669]\tLoss: 15429.0879\n",
      "Training Epoch: 3 [11720/49669]\tLoss: 16962.5469\n",
      "Training Epoch: 3 [11740/49669]\tLoss: 18006.0098\n",
      "Training Epoch: 3 [11760/49669]\tLoss: 14013.3916\n",
      "Training Epoch: 3 [11780/49669]\tLoss: 12183.7803\n",
      "Training Epoch: 3 [11800/49669]\tLoss: 16814.6621\n",
      "Training Epoch: 3 [11820/49669]\tLoss: 17993.4375\n",
      "Training Epoch: 3 [11840/49669]\tLoss: 14490.5078\n",
      "Training Epoch: 3 [11860/49669]\tLoss: 18017.1211\n",
      "Training Epoch: 3 [11880/49669]\tLoss: 17170.5059\n",
      "Training Epoch: 3 [11900/49669]\tLoss: 16783.6562\n",
      "Training Epoch: 3 [11920/49669]\tLoss: 16767.2441\n",
      "Training Epoch: 3 [11940/49669]\tLoss: 12978.9180\n",
      "Training Epoch: 3 [11960/49669]\tLoss: 17416.9336\n",
      "Training Epoch: 3 [11980/49669]\tLoss: 17320.4727\n",
      "Training Epoch: 3 [12000/49669]\tLoss: 16959.9434\n",
      "Training Epoch: 3 [12020/49669]\tLoss: 13882.1504\n",
      "Training Epoch: 3 [12040/49669]\tLoss: 16343.9180\n",
      "Training Epoch: 3 [12060/49669]\tLoss: 14592.7383\n",
      "Training Epoch: 3 [12080/49669]\tLoss: 17767.8379\n",
      "Training Epoch: 3 [12100/49669]\tLoss: 14997.7529\n",
      "Training Epoch: 3 [12120/49669]\tLoss: 16876.1133\n",
      "Training Epoch: 3 [12140/49669]\tLoss: 16258.8623\n",
      "Training Epoch: 3 [12160/49669]\tLoss: 17259.6172\n",
      "Training Epoch: 3 [12180/49669]\tLoss: 19235.0156\n",
      "Training Epoch: 3 [12200/49669]\tLoss: 16115.6494\n",
      "Training Epoch: 3 [12220/49669]\tLoss: 16093.2275\n",
      "Training Epoch: 3 [12240/49669]\tLoss: 16749.8066\n",
      "Training Epoch: 3 [12260/49669]\tLoss: 15019.0293\n",
      "Training Epoch: 3 [12280/49669]\tLoss: 17203.9492\n",
      "Training Epoch: 3 [12300/49669]\tLoss: 18194.9629\n",
      "Training Epoch: 3 [12320/49669]\tLoss: 18353.3457\n",
      "Training Epoch: 3 [12340/49669]\tLoss: 18348.2305\n",
      "Training Epoch: 3 [12360/49669]\tLoss: 17777.1973\n",
      "Training Epoch: 3 [12380/49669]\tLoss: 16924.1992\n",
      "Training Epoch: 3 [12400/49669]\tLoss: 18575.9023\n",
      "Training Epoch: 3 [12420/49669]\tLoss: 16413.1855\n",
      "Training Epoch: 3 [12440/49669]\tLoss: 16366.7021\n",
      "Training Epoch: 3 [12460/49669]\tLoss: 17808.7676\n",
      "Training Epoch: 3 [12480/49669]\tLoss: 17422.3164\n",
      "Training Epoch: 3 [12500/49669]\tLoss: 15739.9414\n",
      "Training Epoch: 3 [12520/49669]\tLoss: 13641.2285\n",
      "Training Epoch: 3 [12540/49669]\tLoss: 16943.3086\n",
      "Training Epoch: 3 [12560/49669]\tLoss: 16893.5762\n",
      "Training Epoch: 3 [12580/49669]\tLoss: 16183.2705\n",
      "Training Epoch: 3 [12600/49669]\tLoss: 16452.9688\n",
      "Training Epoch: 3 [12620/49669]\tLoss: 17989.7812\n",
      "Training Epoch: 3 [12640/49669]\tLoss: 16944.3320\n",
      "Training Epoch: 3 [12660/49669]\tLoss: 19324.3086\n",
      "Training Epoch: 3 [12680/49669]\tLoss: 16513.2324\n",
      "Training Epoch: 3 [12700/49669]\tLoss: 17513.5957\n",
      "Training Epoch: 3 [12720/49669]\tLoss: 16850.0430\n",
      "Training Epoch: 3 [12740/49669]\tLoss: 16688.7773\n",
      "Training Epoch: 3 [12760/49669]\tLoss: 17523.1582\n",
      "Training Epoch: 3 [12780/49669]\tLoss: 17901.0273\n",
      "Training Epoch: 3 [12800/49669]\tLoss: 15487.8125\n",
      "Training Epoch: 3 [12820/49669]\tLoss: 17892.4785\n",
      "Training Epoch: 3 [12840/49669]\tLoss: 17887.4023\n",
      "Training Epoch: 3 [12860/49669]\tLoss: 16987.3262\n",
      "Training Epoch: 3 [12880/49669]\tLoss: 16401.8848\n",
      "Training Epoch: 3 [12900/49669]\tLoss: 15358.1826\n",
      "Training Epoch: 3 [12920/49669]\tLoss: 17187.8613\n",
      "Training Epoch: 3 [12940/49669]\tLoss: 17954.1797\n",
      "Training Epoch: 3 [12960/49669]\tLoss: 17126.0625\n",
      "Training Epoch: 3 [12980/49669]\tLoss: 16056.1318\n",
      "Training Epoch: 3 [13000/49669]\tLoss: 17113.0078\n",
      "Training Epoch: 3 [13020/49669]\tLoss: 18487.1934\n",
      "Training Epoch: 3 [13040/49669]\tLoss: 15842.2383\n",
      "Training Epoch: 3 [13060/49669]\tLoss: 15462.0889\n",
      "Training Epoch: 3 [13080/49669]\tLoss: 13554.8887\n",
      "Training Epoch: 3 [13100/49669]\tLoss: 17506.7305\n",
      "Training Epoch: 3 [13120/49669]\tLoss: 18305.2461\n",
      "Training Epoch: 3 [13140/49669]\tLoss: 16340.0283\n",
      "Training Epoch: 3 [13160/49669]\tLoss: 17188.0977\n",
      "Training Epoch: 3 [13180/49669]\tLoss: 13244.2285\n",
      "Training Epoch: 3 [13200/49669]\tLoss: 17907.7109\n",
      "Training Epoch: 3 [13220/49669]\tLoss: 17311.2969\n",
      "Training Epoch: 3 [13240/49669]\tLoss: 14163.0244\n",
      "Training Epoch: 3 [13260/49669]\tLoss: 16202.8418\n",
      "Training Epoch: 3 [13280/49669]\tLoss: 18217.5605\n",
      "Training Epoch: 3 [13300/49669]\tLoss: 15776.1025\n",
      "Training Epoch: 3 [13320/49669]\tLoss: 13930.5986\n",
      "Training Epoch: 3 [13340/49669]\tLoss: 14561.5869\n",
      "Training Epoch: 3 [13360/49669]\tLoss: 18093.2305\n",
      "Training Epoch: 3 [13380/49669]\tLoss: 16018.5537\n",
      "Training Epoch: 3 [13400/49669]\tLoss: 15915.3633\n",
      "Training Epoch: 3 [13420/49669]\tLoss: 15543.7734\n",
      "Training Epoch: 3 [13440/49669]\tLoss: 17201.4609\n",
      "Training Epoch: 3 [13460/49669]\tLoss: 17829.7305\n",
      "Training Epoch: 3 [13480/49669]\tLoss: 15622.0215\n",
      "Training Epoch: 3 [13500/49669]\tLoss: 16289.1074\n",
      "Training Epoch: 3 [13520/49669]\tLoss: 13410.8701\n",
      "Training Epoch: 3 [13540/49669]\tLoss: 15011.5615\n",
      "Training Epoch: 3 [13560/49669]\tLoss: 15534.7754\n",
      "Training Epoch: 3 [13580/49669]\tLoss: 16965.4316\n",
      "Training Epoch: 3 [13600/49669]\tLoss: 15734.0439\n",
      "Training Epoch: 3 [13620/49669]\tLoss: 19673.4336\n",
      "Training Epoch: 3 [13640/49669]\tLoss: 16169.0156\n",
      "Training Epoch: 3 [13660/49669]\tLoss: 17278.1270\n",
      "Training Epoch: 3 [13680/49669]\tLoss: 14176.1680\n",
      "Training Epoch: 3 [13700/49669]\tLoss: 17573.5039\n",
      "Training Epoch: 3 [13720/49669]\tLoss: 18193.9844\n",
      "Training Epoch: 3 [13740/49669]\tLoss: 15246.1768\n",
      "Training Epoch: 3 [13760/49669]\tLoss: 15465.3682\n",
      "Training Epoch: 3 [13780/49669]\tLoss: 17228.9551\n",
      "Training Epoch: 3 [13800/49669]\tLoss: 15506.7090\n",
      "Training Epoch: 3 [13820/49669]\tLoss: 16674.4980\n",
      "Training Epoch: 3 [13840/49669]\tLoss: 17155.5977\n",
      "Training Epoch: 3 [13860/49669]\tLoss: 15353.5684\n",
      "Training Epoch: 3 [13880/49669]\tLoss: 20581.6914\n",
      "Training Epoch: 3 [13900/49669]\tLoss: 16355.4590\n",
      "Training Epoch: 3 [13920/49669]\tLoss: 15407.2002\n",
      "Training Epoch: 3 [13940/49669]\tLoss: 16678.3809\n",
      "Training Epoch: 3 [13960/49669]\tLoss: 17293.1211\n",
      "Training Epoch: 3 [13980/49669]\tLoss: 18433.9629\n",
      "Training Epoch: 3 [14000/49669]\tLoss: 17800.9863\n",
      "Training Epoch: 3 [14020/49669]\tLoss: 16541.8496\n",
      "Training Epoch: 3 [14040/49669]\tLoss: 15547.8135\n",
      "Training Epoch: 3 [14060/49669]\tLoss: 17086.2207\n",
      "Training Epoch: 3 [14080/49669]\tLoss: 17181.5117\n",
      "Training Epoch: 3 [14100/49669]\tLoss: 16171.7236\n",
      "Training Epoch: 3 [14120/49669]\tLoss: 15276.7012\n",
      "Training Epoch: 3 [14140/49669]\tLoss: 14814.7490\n",
      "Training Epoch: 3 [14160/49669]\tLoss: 17423.9238\n",
      "Training Epoch: 3 [14180/49669]\tLoss: 16815.5781\n",
      "Training Epoch: 3 [14200/49669]\tLoss: 14092.0967\n",
      "Training Epoch: 3 [14220/49669]\tLoss: 18489.2188\n",
      "Training Epoch: 3 [14240/49669]\tLoss: 14135.5264\n",
      "Training Epoch: 3 [14260/49669]\tLoss: 17221.5352\n",
      "Training Epoch: 3 [14280/49669]\tLoss: 13876.5986\n",
      "Training Epoch: 3 [14300/49669]\tLoss: 16648.2383\n",
      "Training Epoch: 3 [14320/49669]\tLoss: 18433.6230\n",
      "Training Epoch: 3 [14340/49669]\tLoss: 16620.7363\n",
      "Training Epoch: 3 [14360/49669]\tLoss: 16321.0762\n",
      "Training Epoch: 3 [14380/49669]\tLoss: 16423.0996\n",
      "Training Epoch: 3 [14400/49669]\tLoss: 18337.1602\n",
      "Training Epoch: 3 [14420/49669]\tLoss: 16216.2822\n",
      "Training Epoch: 3 [14440/49669]\tLoss: 14147.6240\n",
      "Training Epoch: 3 [14460/49669]\tLoss: 13502.9229\n",
      "Training Epoch: 3 [14480/49669]\tLoss: 15141.9473\n",
      "Training Epoch: 3 [14500/49669]\tLoss: 13352.9688\n",
      "Training Epoch: 3 [14520/49669]\tLoss: 16122.2959\n",
      "Training Epoch: 3 [14540/49669]\tLoss: 17175.1777\n",
      "Training Epoch: 3 [14560/49669]\tLoss: 15008.1074\n",
      "Training Epoch: 3 [14580/49669]\tLoss: 17277.8320\n",
      "Training Epoch: 3 [14600/49669]\tLoss: 16175.6934\n",
      "Training Epoch: 3 [14620/49669]\tLoss: 19217.2227\n",
      "Training Epoch: 3 [14640/49669]\tLoss: 16316.3564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [14660/49669]\tLoss: 15641.0283\n",
      "Training Epoch: 3 [14680/49669]\tLoss: 18384.7246\n",
      "Training Epoch: 3 [14700/49669]\tLoss: 17269.6074\n",
      "Training Epoch: 3 [14720/49669]\tLoss: 17029.0293\n",
      "Training Epoch: 3 [14740/49669]\tLoss: 17404.9160\n",
      "Training Epoch: 3 [14760/49669]\tLoss: 17837.4512\n",
      "Training Epoch: 3 [14780/49669]\tLoss: 17356.6523\n",
      "Training Epoch: 3 [14800/49669]\tLoss: 17786.2949\n",
      "Training Epoch: 3 [14820/49669]\tLoss: 17289.5879\n",
      "Training Epoch: 3 [14840/49669]\tLoss: 18681.4473\n",
      "Training Epoch: 3 [14860/49669]\tLoss: 13291.5381\n",
      "Training Epoch: 3 [14880/49669]\tLoss: 17235.3809\n",
      "Training Epoch: 3 [14900/49669]\tLoss: 16896.7734\n",
      "Training Epoch: 3 [14920/49669]\tLoss: 17321.2773\n",
      "Training Epoch: 3 [14940/49669]\tLoss: 15567.1611\n",
      "Training Epoch: 3 [14960/49669]\tLoss: 17077.6855\n",
      "Training Epoch: 3 [14980/49669]\tLoss: 15442.9717\n",
      "Training Epoch: 3 [15000/49669]\tLoss: 18396.3496\n",
      "Training Epoch: 3 [15020/49669]\tLoss: 14915.5850\n",
      "Training Epoch: 3 [15040/49669]\tLoss: 15634.2256\n",
      "Training Epoch: 3 [15060/49669]\tLoss: 17813.8887\n",
      "Training Epoch: 3 [15080/49669]\tLoss: 14385.6328\n",
      "Training Epoch: 3 [15100/49669]\tLoss: 17654.5059\n",
      "Training Epoch: 3 [15120/49669]\tLoss: 16987.9551\n",
      "Training Epoch: 3 [15140/49669]\tLoss: 16156.3574\n",
      "Training Epoch: 3 [15160/49669]\tLoss: 15316.8203\n",
      "Training Epoch: 3 [15180/49669]\tLoss: 16129.0234\n",
      "Training Epoch: 3 [15200/49669]\tLoss: 15945.3291\n",
      "Training Epoch: 3 [15220/49669]\tLoss: 18197.4531\n",
      "Training Epoch: 3 [15240/49669]\tLoss: 16353.7881\n",
      "Training Epoch: 3 [15260/49669]\tLoss: 17071.2031\n",
      "Training Epoch: 3 [15280/49669]\tLoss: 17368.5918\n",
      "Training Epoch: 3 [15300/49669]\tLoss: 17618.9395\n",
      "Training Epoch: 3 [15320/49669]\tLoss: 16133.7764\n",
      "Training Epoch: 3 [15340/49669]\tLoss: 16103.2344\n",
      "Training Epoch: 3 [15360/49669]\tLoss: 14180.8477\n",
      "Training Epoch: 3 [15380/49669]\tLoss: 14811.4697\n",
      "Training Epoch: 3 [15400/49669]\tLoss: 16849.2461\n",
      "Training Epoch: 3 [15420/49669]\tLoss: 16471.4141\n",
      "Training Epoch: 3 [15440/49669]\tLoss: 17414.0273\n",
      "Training Epoch: 3 [15460/49669]\tLoss: 18548.7480\n",
      "Training Epoch: 3 [15480/49669]\tLoss: 17134.7539\n",
      "Training Epoch: 3 [15500/49669]\tLoss: 14464.0039\n",
      "Training Epoch: 3 [15520/49669]\tLoss: 17303.9473\n",
      "Training Epoch: 3 [15540/49669]\tLoss: 17199.1074\n",
      "Training Epoch: 3 [15560/49669]\tLoss: 17400.5566\n",
      "Training Epoch: 3 [15580/49669]\tLoss: 16435.4609\n",
      "Training Epoch: 3 [15600/49669]\tLoss: 15806.0127\n",
      "Training Epoch: 3 [15620/49669]\tLoss: 16538.7051\n",
      "Training Epoch: 3 [15640/49669]\tLoss: 15312.7148\n",
      "Training Epoch: 3 [15660/49669]\tLoss: 16282.6357\n",
      "Training Epoch: 3 [15680/49669]\tLoss: 15571.5684\n",
      "Training Epoch: 3 [15700/49669]\tLoss: 15621.6475\n",
      "Training Epoch: 3 [15720/49669]\tLoss: 17884.8281\n",
      "Training Epoch: 3 [15740/49669]\tLoss: 14194.0430\n",
      "Training Epoch: 3 [15760/49669]\tLoss: 14872.3037\n",
      "Training Epoch: 3 [15780/49669]\tLoss: 18985.7832\n",
      "Training Epoch: 3 [15800/49669]\tLoss: 15706.4863\n",
      "Training Epoch: 3 [15820/49669]\tLoss: 14767.2471\n",
      "Training Epoch: 3 [15840/49669]\tLoss: 14946.3164\n",
      "Training Epoch: 3 [15860/49669]\tLoss: 16608.0801\n",
      "Training Epoch: 3 [15880/49669]\tLoss: 15615.5283\n",
      "Training Epoch: 3 [15900/49669]\tLoss: 16615.1953\n",
      "Training Epoch: 3 [15920/49669]\tLoss: 19827.1895\n",
      "Training Epoch: 3 [15940/49669]\tLoss: 16820.0625\n",
      "Training Epoch: 3 [15960/49669]\tLoss: 14166.6074\n",
      "Training Epoch: 3 [15980/49669]\tLoss: 17245.0195\n",
      "Training Epoch: 3 [16000/49669]\tLoss: 16880.9492\n",
      "Training Epoch: 3 [16020/49669]\tLoss: 17395.7402\n",
      "Training Epoch: 3 [16040/49669]\tLoss: 13358.8652\n",
      "Training Epoch: 3 [16060/49669]\tLoss: 16391.5605\n",
      "Training Epoch: 3 [16080/49669]\tLoss: 16299.5947\n",
      "Training Epoch: 3 [16100/49669]\tLoss: 16571.8750\n",
      "Training Epoch: 3 [16120/49669]\tLoss: 16761.3203\n",
      "Training Epoch: 3 [16140/49669]\tLoss: 12846.7910\n",
      "Training Epoch: 3 [16160/49669]\tLoss: 16299.5771\n",
      "Training Epoch: 3 [16180/49669]\tLoss: 13934.4824\n",
      "Training Epoch: 3 [16200/49669]\tLoss: 16193.6357\n",
      "Training Epoch: 3 [16220/49669]\tLoss: 15640.2412\n",
      "Training Epoch: 3 [16240/49669]\tLoss: 18797.1367\n",
      "Training Epoch: 3 [16260/49669]\tLoss: 17056.0234\n",
      "Training Epoch: 3 [16280/49669]\tLoss: 17391.5684\n",
      "Training Epoch: 3 [16300/49669]\tLoss: 14576.0234\n",
      "Training Epoch: 3 [16320/49669]\tLoss: 17382.8945\n",
      "Training Epoch: 3 [16340/49669]\tLoss: 16015.7090\n",
      "Training Epoch: 3 [16360/49669]\tLoss: 16589.5059\n",
      "Training Epoch: 3 [16380/49669]\tLoss: 13968.0684\n",
      "Training Epoch: 3 [16400/49669]\tLoss: 16326.7646\n",
      "Training Epoch: 3 [16420/49669]\tLoss: 14760.7500\n",
      "Training Epoch: 3 [16440/49669]\tLoss: 16678.1680\n",
      "Training Epoch: 3 [16460/49669]\tLoss: 17240.5039\n",
      "Training Epoch: 3 [16480/49669]\tLoss: 17377.4941\n",
      "Training Epoch: 3 [16500/49669]\tLoss: 15437.6533\n",
      "Training Epoch: 3 [16520/49669]\tLoss: 16405.3125\n",
      "Training Epoch: 3 [16540/49669]\tLoss: 14852.7607\n",
      "Training Epoch: 3 [16560/49669]\tLoss: 16400.7520\n",
      "Training Epoch: 3 [16580/49669]\tLoss: 17398.4023\n",
      "Training Epoch: 3 [16600/49669]\tLoss: 15618.1270\n",
      "Training Epoch: 3 [16620/49669]\tLoss: 15744.4023\n",
      "Training Epoch: 3 [16640/49669]\tLoss: 15205.1934\n",
      "Training Epoch: 3 [16660/49669]\tLoss: 18239.9785\n",
      "Training Epoch: 3 [16680/49669]\tLoss: 17292.4277\n",
      "Training Epoch: 3 [16700/49669]\tLoss: 17654.4648\n",
      "Training Epoch: 3 [16720/49669]\tLoss: 17991.9082\n",
      "Training Epoch: 3 [16740/49669]\tLoss: 17063.5605\n",
      "Training Epoch: 3 [16760/49669]\tLoss: 17610.1074\n",
      "Training Epoch: 3 [16780/49669]\tLoss: 19801.9590\n",
      "Training Epoch: 3 [16800/49669]\tLoss: 17278.3789\n",
      "Training Epoch: 3 [16820/49669]\tLoss: 18171.3652\n",
      "Training Epoch: 3 [16840/49669]\tLoss: 14835.0781\n",
      "Training Epoch: 3 [16860/49669]\tLoss: 15452.2939\n",
      "Training Epoch: 3 [16880/49669]\tLoss: 17050.8359\n",
      "Training Epoch: 3 [16900/49669]\tLoss: 16405.8906\n",
      "Training Epoch: 3 [16920/49669]\tLoss: 16350.9521\n",
      "Training Epoch: 3 [16940/49669]\tLoss: 16581.6328\n",
      "Training Epoch: 3 [16960/49669]\tLoss: 14554.4814\n",
      "Training Epoch: 3 [16980/49669]\tLoss: 18333.7500\n",
      "Training Epoch: 3 [17000/49669]\tLoss: 16934.7051\n",
      "Training Epoch: 3 [17020/49669]\tLoss: 17007.3223\n",
      "Training Epoch: 3 [17040/49669]\tLoss: 17087.1230\n",
      "Training Epoch: 3 [17060/49669]\tLoss: 17076.6660\n",
      "Training Epoch: 3 [17080/49669]\tLoss: 16559.2949\n",
      "Training Epoch: 3 [17100/49669]\tLoss: 16925.3203\n",
      "Training Epoch: 3 [17120/49669]\tLoss: 14280.5322\n",
      "Training Epoch: 3 [17140/49669]\tLoss: 15927.5078\n",
      "Training Epoch: 3 [17160/49669]\tLoss: 15294.8721\n",
      "Training Epoch: 3 [17180/49669]\tLoss: 16912.0703\n",
      "Training Epoch: 3 [17200/49669]\tLoss: 16801.2715\n",
      "Training Epoch: 3 [17220/49669]\tLoss: 13633.4697\n",
      "Training Epoch: 3 [17240/49669]\tLoss: 15551.2100\n",
      "Training Epoch: 3 [17260/49669]\tLoss: 18164.9219\n",
      "Training Epoch: 3 [17280/49669]\tLoss: 15829.9404\n",
      "Training Epoch: 3 [17300/49669]\tLoss: 14869.9971\n",
      "Training Epoch: 3 [17320/49669]\tLoss: 15451.9912\n",
      "Training Epoch: 3 [17340/49669]\tLoss: 16161.3018\n",
      "Training Epoch: 3 [17360/49669]\tLoss: 13381.2441\n",
      "Training Epoch: 3 [17380/49669]\tLoss: 18871.3633\n",
      "Training Epoch: 3 [17400/49669]\tLoss: 16546.2324\n",
      "Training Epoch: 3 [17420/49669]\tLoss: 16557.8242\n",
      "Training Epoch: 3 [17440/49669]\tLoss: 14805.2617\n",
      "Training Epoch: 3 [17460/49669]\tLoss: 17534.8945\n",
      "Training Epoch: 3 [17480/49669]\tLoss: 18229.1504\n",
      "Training Epoch: 3 [17500/49669]\tLoss: 14569.0000\n",
      "Training Epoch: 3 [17520/49669]\tLoss: 18926.6680\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
