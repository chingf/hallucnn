{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.ReconstructionTrainingDataset import CleanSoundsDataset\n",
    "from data.ReconstructionTrainingDataset import NoisySoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5d6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs = ['neg6', 'neg3', '0', '3']\n",
    "pnet_names = ['pnet_snr-6', 'pnet_snr-3', 'pnet_snr0', 'pnet_snr3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a437651",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/13500]\tLoss: 95908.8750\n",
      "Training Epoch: 1 [100/13500]\tLoss: 92091.5469\n",
      "Training Epoch: 1 [150/13500]\tLoss: 88681.8516\n",
      "Training Epoch: 1 [200/13500]\tLoss: 84799.6641\n",
      "Training Epoch: 1 [250/13500]\tLoss: 80978.5391\n",
      "Training Epoch: 1 [300/13500]\tLoss: 78071.8359\n",
      "Training Epoch: 1 [350/13500]\tLoss: 75638.1875\n",
      "Training Epoch: 1 [400/13500]\tLoss: 71907.4062\n",
      "Training Epoch: 1 [450/13500]\tLoss: 68443.6328\n",
      "Training Epoch: 1 [500/13500]\tLoss: 66457.0625\n",
      "Training Epoch: 1 [550/13500]\tLoss: 63745.8086\n",
      "Training Epoch: 1 [600/13500]\tLoss: 60915.1719\n",
      "Training Epoch: 1 [650/13500]\tLoss: 58007.0391\n",
      "Training Epoch: 1 [700/13500]\tLoss: 57634.0039\n",
      "Training Epoch: 1 [750/13500]\tLoss: 52949.9336\n",
      "Training Epoch: 1 [800/13500]\tLoss: 51815.1133\n",
      "Training Epoch: 1 [850/13500]\tLoss: 49306.4375\n",
      "Training Epoch: 1 [900/13500]\tLoss: 45973.4844\n",
      "Training Epoch: 1 [950/13500]\tLoss: 44763.1875\n",
      "Training Epoch: 1 [1000/13500]\tLoss: 43941.3516\n",
      "Training Epoch: 1 [1050/13500]\tLoss: 40997.6445\n",
      "Training Epoch: 1 [1100/13500]\tLoss: 37796.5352\n",
      "Training Epoch: 1 [1150/13500]\tLoss: 36868.8008\n",
      "Training Epoch: 1 [1200/13500]\tLoss: 36683.4102\n",
      "Training Epoch: 1 [1250/13500]\tLoss: 35376.2930\n",
      "Training Epoch: 1 [1300/13500]\tLoss: 33949.4414\n",
      "Training Epoch: 1 [1350/13500]\tLoss: 30554.5742\n",
      "Training Epoch: 1 [1400/13500]\tLoss: 30825.4551\n",
      "Training Epoch: 1 [1450/13500]\tLoss: 29898.8672\n",
      "Training Epoch: 1 [1500/13500]\tLoss: 27380.1270\n",
      "Training Epoch: 1 [1550/13500]\tLoss: 26369.2188\n",
      "Training Epoch: 1 [1600/13500]\tLoss: 25391.7363\n",
      "Training Epoch: 1 [1650/13500]\tLoss: 26164.4805\n",
      "Training Epoch: 1 [1700/13500]\tLoss: 23080.8574\n",
      "Training Epoch: 1 [1750/13500]\tLoss: 22060.3086\n",
      "Training Epoch: 1 [1800/13500]\tLoss: 22761.3379\n",
      "Training Epoch: 1 [1850/13500]\tLoss: 22539.6016\n",
      "Training Epoch: 1 [1900/13500]\tLoss: 22004.9199\n",
      "Training Epoch: 1 [1950/13500]\tLoss: 21087.3848\n",
      "Training Epoch: 1 [2000/13500]\tLoss: 19967.8438\n",
      "Training Epoch: 1 [2050/13500]\tLoss: 20063.7988\n",
      "Training Epoch: 1 [2100/13500]\tLoss: 19814.2383\n",
      "Training Epoch: 1 [2150/13500]\tLoss: 18974.3242\n",
      "Training Epoch: 1 [2200/13500]\tLoss: 18396.8711\n",
      "Training Epoch: 1 [2250/13500]\tLoss: 18197.4102\n",
      "Training Epoch: 1 [2300/13500]\tLoss: 18355.1055\n",
      "Training Epoch: 1 [2350/13500]\tLoss: 17649.9512\n",
      "Training Epoch: 1 [2400/13500]\tLoss: 17775.2148\n",
      "Training Epoch: 1 [2450/13500]\tLoss: 17015.1914\n",
      "Training Epoch: 1 [2500/13500]\tLoss: 17294.8516\n",
      "Training Epoch: 1 [2550/13500]\tLoss: 17484.7148\n",
      "Training Epoch: 1 [2600/13500]\tLoss: 17100.7285\n",
      "Training Epoch: 1 [2650/13500]\tLoss: 16373.3555\n",
      "Training Epoch: 1 [2700/13500]\tLoss: 16767.7305\n",
      "Training Epoch: 1 [2750/13500]\tLoss: 16235.7939\n",
      "Training Epoch: 1 [2800/13500]\tLoss: 16443.0703\n",
      "Training Epoch: 1 [2850/13500]\tLoss: 16385.1543\n",
      "Training Epoch: 1 [2900/13500]\tLoss: 15987.1289\n",
      "Training Epoch: 1 [2950/13500]\tLoss: 16145.5889\n",
      "Training Epoch: 1 [3000/13500]\tLoss: 16051.1855\n",
      "Training Epoch: 1 [3050/13500]\tLoss: 15957.0684\n",
      "Training Epoch: 1 [3100/13500]\tLoss: 16135.7979\n",
      "Training Epoch: 1 [3150/13500]\tLoss: 16290.8711\n",
      "Training Epoch: 1 [3200/13500]\tLoss: 16053.8975\n",
      "Training Epoch: 1 [3250/13500]\tLoss: 16256.7393\n",
      "Training Epoch: 1 [3300/13500]\tLoss: 16110.8818\n",
      "Training Epoch: 1 [3350/13500]\tLoss: 16160.8643\n",
      "Training Epoch: 1 [3400/13500]\tLoss: 15338.8281\n",
      "Training Epoch: 1 [3450/13500]\tLoss: 15559.5645\n",
      "Training Epoch: 1 [3500/13500]\tLoss: 16079.5537\n",
      "Training Epoch: 1 [3550/13500]\tLoss: 15995.9844\n",
      "Training Epoch: 1 [3600/13500]\tLoss: 15957.5859\n",
      "Training Epoch: 1 [3650/13500]\tLoss: 15865.8135\n",
      "Training Epoch: 1 [3700/13500]\tLoss: 15506.7773\n",
      "Training Epoch: 1 [3750/13500]\tLoss: 16009.1016\n",
      "Training Epoch: 1 [3800/13500]\tLoss: 16121.1660\n",
      "Training Epoch: 1 [3850/13500]\tLoss: 16183.9180\n",
      "Training Epoch: 1 [3900/13500]\tLoss: 15384.7471\n",
      "Training Epoch: 1 [3950/13500]\tLoss: 15981.9414\n",
      "Training Epoch: 1 [4000/13500]\tLoss: 15338.6758\n",
      "Training Epoch: 1 [4050/13500]\tLoss: 15632.1533\n",
      "Training Epoch: 1 [4100/13500]\tLoss: 15646.2480\n",
      "Training Epoch: 1 [4150/13500]\tLoss: 16082.1396\n",
      "Training Epoch: 1 [4200/13500]\tLoss: 15752.6953\n",
      "Training Epoch: 1 [4250/13500]\tLoss: 15577.4570\n",
      "Training Epoch: 1 [4300/13500]\tLoss: 15271.4902\n",
      "Training Epoch: 1 [4350/13500]\tLoss: 15287.8584\n",
      "Training Epoch: 1 [4400/13500]\tLoss: 15377.0498\n",
      "Training Epoch: 1 [4450/13500]\tLoss: 15468.0127\n",
      "Training Epoch: 1 [4500/13500]\tLoss: 14921.3184\n",
      "Training Epoch: 1 [4550/13500]\tLoss: 14663.1182\n",
      "Training Epoch: 1 [4600/13500]\tLoss: 15103.3447\n",
      "Training Epoch: 1 [4650/13500]\tLoss: 15872.3564\n",
      "Training Epoch: 1 [4700/13500]\tLoss: 15441.8340\n",
      "Training Epoch: 1 [4750/13500]\tLoss: 15332.8447\n",
      "Training Epoch: 1 [4800/13500]\tLoss: 15194.9756\n",
      "Training Epoch: 1 [4850/13500]\tLoss: 15092.3066\n",
      "Training Epoch: 1 [4900/13500]\tLoss: 15143.2334\n",
      "Training Epoch: 1 [4950/13500]\tLoss: 14806.0459\n",
      "Training Epoch: 1 [5000/13500]\tLoss: 14694.0967\n",
      "Training Epoch: 1 [5050/13500]\tLoss: 14877.1309\n",
      "Training Epoch: 1 [5100/13500]\tLoss: 15307.6406\n",
      "Training Epoch: 1 [5150/13500]\tLoss: 14798.0137\n",
      "Training Epoch: 1 [5200/13500]\tLoss: 14864.1924\n",
      "Training Epoch: 1 [5250/13500]\tLoss: 14934.6621\n",
      "Training Epoch: 1 [5300/13500]\tLoss: 14642.1621\n",
      "Training Epoch: 1 [5350/13500]\tLoss: 14866.7686\n",
      "Training Epoch: 1 [5400/13500]\tLoss: 14995.1328\n",
      "Training Epoch: 1 [5450/13500]\tLoss: 14610.3906\n",
      "Training Epoch: 1 [5500/13500]\tLoss: 14932.6162\n",
      "Training Epoch: 1 [5550/13500]\tLoss: 14228.9111\n",
      "Training Epoch: 1 [5600/13500]\tLoss: 14825.9434\n",
      "Training Epoch: 1 [5650/13500]\tLoss: 14948.8086\n",
      "Training Epoch: 1 [5700/13500]\tLoss: 15336.3486\n",
      "Training Epoch: 1 [5750/13500]\tLoss: 14804.9229\n",
      "Training Epoch: 1 [5800/13500]\tLoss: 14351.7305\n",
      "Training Epoch: 1 [5850/13500]\tLoss: 14450.7197\n",
      "Training Epoch: 1 [5900/13500]\tLoss: 14680.6025\n",
      "Training Epoch: 1 [5950/13500]\tLoss: 14718.1816\n",
      "Training Epoch: 1 [6000/13500]\tLoss: 14362.7480\n",
      "Training Epoch: 1 [6050/13500]\tLoss: 14196.3018\n",
      "Training Epoch: 1 [6100/13500]\tLoss: 14279.2871\n",
      "Training Epoch: 1 [6150/13500]\tLoss: 14115.1504\n",
      "Training Epoch: 1 [6200/13500]\tLoss: 14697.8301\n",
      "Training Epoch: 1 [6250/13500]\tLoss: 13991.0547\n",
      "Training Epoch: 1 [6300/13500]\tLoss: 14378.0264\n",
      "Training Epoch: 1 [6350/13500]\tLoss: 14110.5547\n",
      "Training Epoch: 1 [6400/13500]\tLoss: 14176.8418\n",
      "Training Epoch: 1 [6450/13500]\tLoss: 14048.4141\n",
      "Training Epoch: 1 [6500/13500]\tLoss: 14683.8809\n",
      "Training Epoch: 1 [6550/13500]\tLoss: 14266.6426\n",
      "Training Epoch: 1 [6600/13500]\tLoss: 14030.9805\n",
      "Training Epoch: 1 [6650/13500]\tLoss: 14237.8857\n",
      "Training Epoch: 1 [6700/13500]\tLoss: 14029.9014\n",
      "Training Epoch: 1 [6750/13500]\tLoss: 14089.4883\n",
      "Training Epoch: 1 [6800/13500]\tLoss: 13773.1924\n",
      "Training Epoch: 1 [6850/13500]\tLoss: 14282.8770\n",
      "Training Epoch: 1 [6900/13500]\tLoss: 14199.3301\n",
      "Training Epoch: 1 [6950/13500]\tLoss: 13970.6475\n",
      "Training Epoch: 1 [7000/13500]\tLoss: 13690.5801\n",
      "Training Epoch: 1 [7050/13500]\tLoss: 14133.2041\n",
      "Training Epoch: 1 [7100/13500]\tLoss: 14475.0908\n",
      "Training Epoch: 1 [7150/13500]\tLoss: 13869.8574\n",
      "Training Epoch: 1 [7200/13500]\tLoss: 13669.6807\n",
      "Training Epoch: 1 [7250/13500]\tLoss: 13756.5322\n",
      "Training Epoch: 1 [7300/13500]\tLoss: 13759.1094\n",
      "Training Epoch: 1 [7350/13500]\tLoss: 13999.3076\n",
      "Training Epoch: 1 [7400/13500]\tLoss: 13450.3916\n",
      "Training Epoch: 1 [7450/13500]\tLoss: 13818.0645\n",
      "Training Epoch: 1 [7500/13500]\tLoss: 13843.9814\n",
      "Training Epoch: 1 [7550/13500]\tLoss: 13587.6953\n",
      "Training Epoch: 1 [7600/13500]\tLoss: 13574.6807\n",
      "Training Epoch: 1 [7650/13500]\tLoss: 13808.8545\n",
      "Training Epoch: 1 [7700/13500]\tLoss: 13900.4277\n",
      "Training Epoch: 1 [7750/13500]\tLoss: 13427.9492\n",
      "Training Epoch: 1 [7800/13500]\tLoss: 13646.6660\n",
      "Training Epoch: 1 [7850/13500]\tLoss: 13559.7803\n",
      "Training Epoch: 1 [7900/13500]\tLoss: 13124.3037\n",
      "Training Epoch: 1 [7950/13500]\tLoss: 13624.8936\n",
      "Training Epoch: 1 [8000/13500]\tLoss: 13930.6396\n",
      "Training Epoch: 1 [8050/13500]\tLoss: 13333.2861\n",
      "Training Epoch: 1 [8100/13500]\tLoss: 13708.2490\n",
      "Training Epoch: 1 [8150/13500]\tLoss: 13483.7666\n",
      "Training Epoch: 1 [8200/13500]\tLoss: 13515.3486\n",
      "Training Epoch: 1 [8250/13500]\tLoss: 13742.1758\n",
      "Training Epoch: 1 [8300/13500]\tLoss: 13243.0898\n",
      "Training Epoch: 1 [8350/13500]\tLoss: 13618.3770\n",
      "Training Epoch: 1 [8400/13500]\tLoss: 13579.6035\n",
      "Training Epoch: 1 [8450/13500]\tLoss: 13014.1172\n",
      "Training Epoch: 1 [8500/13500]\tLoss: 13210.8809\n",
      "Training Epoch: 1 [8550/13500]\tLoss: 13301.6797\n",
      "Training Epoch: 1 [8600/13500]\tLoss: 13472.1045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8650/13500]\tLoss: 13205.1875\n",
      "Training Epoch: 1 [8700/13500]\tLoss: 13262.8867\n",
      "Training Epoch: 1 [8750/13500]\tLoss: 12931.3613\n",
      "Training Epoch: 1 [8800/13500]\tLoss: 12956.1025\n",
      "Training Epoch: 1 [8850/13500]\tLoss: 12558.4326\n",
      "Training Epoch: 1 [8900/13500]\tLoss: 13189.6826\n",
      "Training Epoch: 1 [8950/13500]\tLoss: 13303.2412\n",
      "Training Epoch: 1 [9000/13500]\tLoss: 12785.3740\n",
      "Training Epoch: 1 [9050/13500]\tLoss: 12888.8555\n",
      "Training Epoch: 1 [9100/13500]\tLoss: 13224.3936\n",
      "Training Epoch: 1 [9150/13500]\tLoss: 13017.5430\n",
      "Training Epoch: 1 [9200/13500]\tLoss: 12529.8350\n",
      "Training Epoch: 1 [9250/13500]\tLoss: 12816.5205\n",
      "Training Epoch: 1 [9300/13500]\tLoss: 13279.3721\n",
      "Training Epoch: 1 [9350/13500]\tLoss: 13538.3291\n",
      "Training Epoch: 1 [9400/13500]\tLoss: 12641.2324\n",
      "Training Epoch: 1 [9450/13500]\tLoss: 12332.1191\n",
      "Training Epoch: 1 [9500/13500]\tLoss: 12523.5596\n",
      "Training Epoch: 1 [9550/13500]\tLoss: 12805.7041\n",
      "Training Epoch: 1 [9600/13500]\tLoss: 13002.4180\n",
      "Training Epoch: 1 [9650/13500]\tLoss: 12462.6133\n",
      "Training Epoch: 1 [9700/13500]\tLoss: 12796.0371\n",
      "Training Epoch: 1 [9750/13500]\tLoss: 12778.8379\n",
      "Training Epoch: 1 [9800/13500]\tLoss: 12363.0674\n",
      "Training Epoch: 1 [9850/13500]\tLoss: 12451.4990\n",
      "Training Epoch: 1 [9900/13500]\tLoss: 12231.6738\n",
      "Training Epoch: 1 [9950/13500]\tLoss: 12829.4219\n",
      "Training Epoch: 1 [10000/13500]\tLoss: 12272.5742\n",
      "Training Epoch: 1 [10050/13500]\tLoss: 12644.0039\n",
      "Training Epoch: 1 [10100/13500]\tLoss: 12530.8467\n",
      "Training Epoch: 1 [10150/13500]\tLoss: 12406.8594\n",
      "Training Epoch: 1 [10200/13500]\tLoss: 12346.2695\n",
      "Training Epoch: 1 [10250/13500]\tLoss: 12190.7236\n",
      "Training Epoch: 1 [10300/13500]\tLoss: 12238.0684\n",
      "Training Epoch: 1 [10350/13500]\tLoss: 12405.6611\n",
      "Training Epoch: 1 [10400/13500]\tLoss: 12770.8223\n",
      "Training Epoch: 1 [10450/13500]\tLoss: 12389.9795\n",
      "Training Epoch: 1 [10500/13500]\tLoss: 12595.3535\n",
      "Training Epoch: 1 [10550/13500]\tLoss: 12626.7646\n",
      "Training Epoch: 1 [10600/13500]\tLoss: 12400.9482\n",
      "Training Epoch: 1 [10650/13500]\tLoss: 12022.1289\n",
      "Training Epoch: 1 [10700/13500]\tLoss: 12313.0312\n",
      "Training Epoch: 1 [10750/13500]\tLoss: 12382.5381\n",
      "Training Epoch: 1 [10800/13500]\tLoss: 12151.0273\n",
      "Training Epoch: 1 [10850/13500]\tLoss: 12058.1045\n",
      "Training Epoch: 1 [10900/13500]\tLoss: 11970.6611\n",
      "Training Epoch: 1 [10950/13500]\tLoss: 12361.3389\n",
      "Training Epoch: 1 [11000/13500]\tLoss: 12064.2949\n",
      "Training Epoch: 1 [11050/13500]\tLoss: 12421.8516\n",
      "Training Epoch: 1 [11100/13500]\tLoss: 12125.7861\n",
      "Training Epoch: 1 [11150/13500]\tLoss: 12051.3984\n",
      "Training Epoch: 1 [11200/13500]\tLoss: 11929.9775\n",
      "Training Epoch: 1 [11250/13500]\tLoss: 11881.6914\n",
      "Training Epoch: 1 [11300/13500]\tLoss: 11785.3027\n",
      "Training Epoch: 1 [11350/13500]\tLoss: 12032.0156\n",
      "Training Epoch: 1 [11400/13500]\tLoss: 12210.3008\n",
      "Training Epoch: 1 [11450/13500]\tLoss: 12110.6689\n",
      "Training Epoch: 1 [11500/13500]\tLoss: 11701.0146\n",
      "Training Epoch: 1 [11550/13500]\tLoss: 11783.4502\n",
      "Training Epoch: 1 [11600/13500]\tLoss: 11638.0547\n",
      "Training Epoch: 1 [11650/13500]\tLoss: 11617.1260\n",
      "Training Epoch: 1 [11700/13500]\tLoss: 11558.0566\n",
      "Training Epoch: 1 [11750/13500]\tLoss: 12140.5850\n",
      "Training Epoch: 1 [11800/13500]\tLoss: 11931.2637\n",
      "Training Epoch: 1 [11850/13500]\tLoss: 11774.1602\n",
      "Training Epoch: 1 [11900/13500]\tLoss: 12180.8809\n",
      "Training Epoch: 1 [11950/13500]\tLoss: 11830.5039\n",
      "Training Epoch: 1 [12000/13500]\tLoss: 11340.3936\n",
      "Training Epoch: 1 [12050/13500]\tLoss: 11511.7100\n",
      "Training Epoch: 1 [12100/13500]\tLoss: 11794.5654\n",
      "Training Epoch: 1 [12150/13500]\tLoss: 11890.2051\n",
      "Training Epoch: 1 [12200/13500]\tLoss: 11698.1680\n",
      "Training Epoch: 1 [12250/13500]\tLoss: 11303.3340\n",
      "Training Epoch: 1 [12300/13500]\tLoss: 11936.4121\n",
      "Training Epoch: 1 [12350/13500]\tLoss: 11546.2529\n",
      "Training Epoch: 1 [12400/13500]\tLoss: 11456.8486\n",
      "Training Epoch: 1 [12450/13500]\tLoss: 11437.4512\n",
      "Training Epoch: 1 [12500/13500]\tLoss: 11698.2148\n",
      "Training Epoch: 1 [12550/13500]\tLoss: 11286.9141\n",
      "Training Epoch: 1 [12600/13500]\tLoss: 11324.6475\n",
      "Training Epoch: 1 [12650/13500]\tLoss: 11681.3418\n",
      "Training Epoch: 1 [12700/13500]\tLoss: 11642.3682\n",
      "Training Epoch: 1 [12750/13500]\tLoss: 11248.2246\n",
      "Training Epoch: 1 [12800/13500]\tLoss: 11291.1328\n",
      "Training Epoch: 1 [12850/13500]\tLoss: 11170.0957\n",
      "Training Epoch: 1 [12900/13500]\tLoss: 11353.2344\n",
      "Training Epoch: 1 [12950/13500]\tLoss: 11198.9277\n",
      "Training Epoch: 1 [13000/13500]\tLoss: 11196.8340\n",
      "Training Epoch: 1 [13050/13500]\tLoss: 11093.3252\n",
      "Training Epoch: 1 [13100/13500]\tLoss: 10856.9043\n",
      "Training Epoch: 1 [13150/13500]\tLoss: 11079.4268\n",
      "Training Epoch: 1 [13200/13500]\tLoss: 11000.4561\n",
      "Training Epoch: 1 [13250/13500]\tLoss: 11232.5342\n",
      "Training Epoch: 1 [13300/13500]\tLoss: 10873.8984\n",
      "Training Epoch: 1 [13350/13500]\tLoss: 11027.6768\n",
      "Training Epoch: 1 [13400/13500]\tLoss: 11057.2129\n",
      "Training Epoch: 1 [13450/13500]\tLoss: 10956.7100\n",
      "Training Epoch: 1 [13500/13500]\tLoss: 11354.6963\n",
      "Training Epoch: 1 [1499/1499]\tLoss: 10954.2832\n",
      "Training Epoch: 2 [50/13500]\tLoss: 11118.6025\n",
      "Training Epoch: 2 [100/13500]\tLoss: 10898.8418\n",
      "Training Epoch: 2 [150/13500]\tLoss: 11049.5898\n",
      "Training Epoch: 2 [200/13500]\tLoss: 10922.9590\n",
      "Training Epoch: 2 [250/13500]\tLoss: 11070.8369\n",
      "Training Epoch: 2 [300/13500]\tLoss: 10919.3633\n",
      "Training Epoch: 2 [350/13500]\tLoss: 11001.5000\n",
      "Training Epoch: 2 [400/13500]\tLoss: 11014.2207\n",
      "Training Epoch: 2 [450/13500]\tLoss: 10590.1768\n",
      "Training Epoch: 2 [500/13500]\tLoss: 10671.8447\n",
      "Training Epoch: 2 [550/13500]\tLoss: 10821.8496\n",
      "Training Epoch: 2 [600/13500]\tLoss: 10815.2822\n",
      "Training Epoch: 2 [650/13500]\tLoss: 10684.4766\n",
      "Training Epoch: 2 [700/13500]\tLoss: 10954.6201\n",
      "Training Epoch: 2 [750/13500]\tLoss: 10689.5068\n",
      "Training Epoch: 2 [800/13500]\tLoss: 10395.8232\n",
      "Training Epoch: 2 [850/13500]\tLoss: 10488.4268\n",
      "Training Epoch: 2 [900/13500]\tLoss: 10124.4141\n",
      "Training Epoch: 2 [950/13500]\tLoss: 10126.4082\n",
      "Training Epoch: 2 [1000/13500]\tLoss: 10662.6924\n",
      "Training Epoch: 2 [1050/13500]\tLoss: 10434.1787\n",
      "Training Epoch: 2 [1100/13500]\tLoss: 10530.3467\n",
      "Training Epoch: 2 [1150/13500]\tLoss: 10746.0840\n",
      "Training Epoch: 2 [1200/13500]\tLoss: 10390.6611\n",
      "Training Epoch: 2 [1250/13500]\tLoss: 10519.3428\n",
      "Training Epoch: 2 [1300/13500]\tLoss: 10393.7539\n",
      "Training Epoch: 2 [1350/13500]\tLoss: 10489.4590\n",
      "Training Epoch: 2 [1400/13500]\tLoss: 10651.7598\n",
      "Training Epoch: 2 [1450/13500]\tLoss: 10293.8818\n",
      "Training Epoch: 2 [1500/13500]\tLoss: 10870.6328\n",
      "Training Epoch: 2 [1550/13500]\tLoss: 10077.0645\n",
      "Training Epoch: 2 [1600/13500]\tLoss: 10438.5381\n",
      "Training Epoch: 2 [1650/13500]\tLoss: 10457.0068\n",
      "Training Epoch: 2 [1700/13500]\tLoss: 10420.7773\n",
      "Training Epoch: 2 [1750/13500]\tLoss: 10397.5205\n",
      "Training Epoch: 2 [1800/13500]\tLoss: 10089.1484\n",
      "Training Epoch: 2 [1850/13500]\tLoss: 10286.4873\n",
      "Training Epoch: 2 [1900/13500]\tLoss: 10542.2725\n",
      "Training Epoch: 2 [1950/13500]\tLoss: 9706.1973\n",
      "Training Epoch: 2 [2000/13500]\tLoss: 10352.4121\n",
      "Training Epoch: 2 [2050/13500]\tLoss: 10068.9229\n",
      "Training Epoch: 2 [2100/13500]\tLoss: 10104.1094\n",
      "Training Epoch: 2 [2150/13500]\tLoss: 10021.8857\n",
      "Training Epoch: 2 [2200/13500]\tLoss: 10380.3633\n",
      "Training Epoch: 2 [2250/13500]\tLoss: 10023.6143\n",
      "Training Epoch: 2 [2300/13500]\tLoss: 10219.7666\n",
      "Training Epoch: 2 [2350/13500]\tLoss: 9832.1699\n",
      "Training Epoch: 2 [2400/13500]\tLoss: 10015.7432\n",
      "Training Epoch: 2 [2450/13500]\tLoss: 10147.1211\n",
      "Training Epoch: 2 [2500/13500]\tLoss: 9979.4092\n",
      "Training Epoch: 2 [2550/13500]\tLoss: 10299.1152\n",
      "Training Epoch: 2 [2600/13500]\tLoss: 10135.4033\n",
      "Training Epoch: 2 [2650/13500]\tLoss: 9755.2744\n",
      "Training Epoch: 2 [2700/13500]\tLoss: 10207.0010\n",
      "Training Epoch: 2 [2750/13500]\tLoss: 9715.4189\n",
      "Training Epoch: 2 [2800/13500]\tLoss: 9924.9238\n",
      "Training Epoch: 2 [2850/13500]\tLoss: 10033.6660\n",
      "Training Epoch: 2 [2900/13500]\tLoss: 9845.7773\n",
      "Training Epoch: 2 [2950/13500]\tLoss: 9850.5732\n",
      "Training Epoch: 2 [3000/13500]\tLoss: 9756.6992\n",
      "Training Epoch: 2 [3050/13500]\tLoss: 9714.1943\n",
      "Training Epoch: 2 [3100/13500]\tLoss: 9861.0518\n",
      "Training Epoch: 2 [3150/13500]\tLoss: 10106.1250\n",
      "Training Epoch: 2 [3200/13500]\tLoss: 9957.5576\n",
      "Training Epoch: 2 [3250/13500]\tLoss: 10101.4229\n",
      "Training Epoch: 2 [3300/13500]\tLoss: 9976.9639\n",
      "Training Epoch: 2 [3350/13500]\tLoss: 9954.2812\n",
      "Training Epoch: 2 [3400/13500]\tLoss: 9416.2012\n",
      "Training Epoch: 2 [3450/13500]\tLoss: 9690.7021\n",
      "Training Epoch: 2 [3500/13500]\tLoss: 9965.3984\n",
      "Training Epoch: 2 [3550/13500]\tLoss: 9932.1904\n",
      "Training Epoch: 2 [3600/13500]\tLoss: 9918.3457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [3650/13500]\tLoss: 9773.3516\n",
      "Training Epoch: 2 [3700/13500]\tLoss: 9583.1143\n",
      "Training Epoch: 2 [3750/13500]\tLoss: 9739.9102\n",
      "Training Epoch: 2 [3800/13500]\tLoss: 9980.4629\n",
      "Training Epoch: 2 [3850/13500]\tLoss: 9890.6826\n",
      "Training Epoch: 2 [3900/13500]\tLoss: 9416.0752\n",
      "Training Epoch: 2 [3950/13500]\tLoss: 9743.9053\n",
      "Training Epoch: 2 [4000/13500]\tLoss: 9402.0596\n",
      "Training Epoch: 2 [4050/13500]\tLoss: 9651.7002\n",
      "Training Epoch: 2 [4100/13500]\tLoss: 9632.4121\n",
      "Training Epoch: 2 [4150/13500]\tLoss: 9938.3184\n",
      "Training Epoch: 2 [4200/13500]\tLoss: 9633.9258\n",
      "Training Epoch: 2 [4250/13500]\tLoss: 9528.5381\n",
      "Training Epoch: 2 [4300/13500]\tLoss: 9419.1104\n",
      "Training Epoch: 2 [4350/13500]\tLoss: 9524.7285\n",
      "Training Epoch: 2 [4400/13500]\tLoss: 9404.9414\n",
      "Training Epoch: 2 [4450/13500]\tLoss: 9581.7822\n",
      "Training Epoch: 2 [4500/13500]\tLoss: 9220.6357\n",
      "Training Epoch: 2 [4550/13500]\tLoss: 9093.5176\n",
      "Training Epoch: 2 [4600/13500]\tLoss: 9221.5020\n",
      "Training Epoch: 2 [4650/13500]\tLoss: 9678.8555\n",
      "Training Epoch: 2 [4700/13500]\tLoss: 9551.8223\n",
      "Training Epoch: 2 [4750/13500]\tLoss: 9404.0820\n",
      "Training Epoch: 2 [4800/13500]\tLoss: 9466.7939\n",
      "Training Epoch: 2 [4850/13500]\tLoss: 9394.8096\n",
      "Training Epoch: 2 [4900/13500]\tLoss: 9300.6318\n",
      "Training Epoch: 2 [4950/13500]\tLoss: 9077.0723\n",
      "Training Epoch: 2 [5000/13500]\tLoss: 9043.5781\n",
      "Training Epoch: 2 [5050/13500]\tLoss: 9248.9629\n",
      "Training Epoch: 2 [5100/13500]\tLoss: 9373.2168\n",
      "Training Epoch: 2 [5150/13500]\tLoss: 9133.8994\n",
      "Training Epoch: 2 [5200/13500]\tLoss: 9183.7295\n",
      "Training Epoch: 2 [5250/13500]\tLoss: 9240.7461\n",
      "Training Epoch: 2 [5300/13500]\tLoss: 9023.5117\n",
      "Training Epoch: 2 [5350/13500]\tLoss: 9195.5566\n",
      "Training Epoch: 2 [5400/13500]\tLoss: 9221.3887\n",
      "Training Epoch: 2 [5450/13500]\tLoss: 8981.6582\n",
      "Training Epoch: 2 [5500/13500]\tLoss: 9189.6133\n",
      "Training Epoch: 2 [5550/13500]\tLoss: 8791.5352\n",
      "Training Epoch: 2 [5600/13500]\tLoss: 9205.0107\n",
      "Training Epoch: 2 [5650/13500]\tLoss: 9175.4668\n",
      "Training Epoch: 2 [5700/13500]\tLoss: 9426.3965\n",
      "Training Epoch: 2 [5750/13500]\tLoss: 9178.2930\n",
      "Training Epoch: 2 [5800/13500]\tLoss: 8830.7031\n",
      "Training Epoch: 2 [5850/13500]\tLoss: 8923.3975\n",
      "Training Epoch: 2 [5900/13500]\tLoss: 9048.9033\n",
      "Training Epoch: 2 [5950/13500]\tLoss: 9070.7637\n",
      "Training Epoch: 2 [6000/13500]\tLoss: 8950.8828\n",
      "Training Epoch: 2 [6050/13500]\tLoss: 8858.3389\n",
      "Training Epoch: 2 [6100/13500]\tLoss: 8815.9580\n",
      "Training Epoch: 2 [6150/13500]\tLoss: 8696.6289\n",
      "Training Epoch: 2 [6200/13500]\tLoss: 9091.4336\n",
      "Training Epoch: 2 [6250/13500]\tLoss: 8659.5508\n",
      "Training Epoch: 2 [6300/13500]\tLoss: 8911.8994\n",
      "Training Epoch: 2 [6350/13500]\tLoss: 8742.3291\n",
      "Training Epoch: 2 [6400/13500]\tLoss: 8804.6533\n",
      "Training Epoch: 2 [6450/13500]\tLoss: 8744.2002\n",
      "Training Epoch: 2 [6500/13500]\tLoss: 9089.7441\n",
      "Training Epoch: 2 [6550/13500]\tLoss: 8880.3232\n",
      "Training Epoch: 2 [6600/13500]\tLoss: 8698.8018\n",
      "Training Epoch: 2 [6650/13500]\tLoss: 8866.1816\n",
      "Training Epoch: 2 [6700/13500]\tLoss: 8779.9678\n",
      "Training Epoch: 2 [6750/13500]\tLoss: 8748.6465\n",
      "Training Epoch: 2 [6800/13500]\tLoss: 8522.4922\n",
      "Training Epoch: 2 [6850/13500]\tLoss: 8912.8223\n",
      "Training Epoch: 2 [6900/13500]\tLoss: 8806.4150\n",
      "Training Epoch: 2 [6950/13500]\tLoss: 8681.3447\n",
      "Training Epoch: 2 [7000/13500]\tLoss: 8503.1611\n",
      "Training Epoch: 2 [7050/13500]\tLoss: 8790.3271\n",
      "Training Epoch: 2 [7100/13500]\tLoss: 8972.3076\n",
      "Training Epoch: 2 [7150/13500]\tLoss: 8639.8770\n",
      "Training Epoch: 2 [7200/13500]\tLoss: 8462.5361\n",
      "Training Epoch: 2 [7250/13500]\tLoss: 8621.0996\n",
      "Training Epoch: 2 [7300/13500]\tLoss: 8579.8154\n",
      "Training Epoch: 2 [7350/13500]\tLoss: 8789.9180\n",
      "Training Epoch: 2 [7400/13500]\tLoss: 8394.6914\n",
      "Training Epoch: 2 [7450/13500]\tLoss: 8557.1494\n",
      "Training Epoch: 2 [7500/13500]\tLoss: 8676.4580\n",
      "Training Epoch: 2 [7550/13500]\tLoss: 8484.3965\n",
      "Training Epoch: 2 [7600/13500]\tLoss: 8462.3857\n",
      "Training Epoch: 2 [7650/13500]\tLoss: 8535.7803\n",
      "Training Epoch: 2 [7700/13500]\tLoss: 8617.4717\n",
      "Training Epoch: 2 [7750/13500]\tLoss: 8386.8135\n",
      "Training Epoch: 2 [7800/13500]\tLoss: 8516.3418\n",
      "Training Epoch: 2 [7850/13500]\tLoss: 8537.5957\n",
      "Training Epoch: 2 [7900/13500]\tLoss: 8192.8906\n",
      "Training Epoch: 2 [7950/13500]\tLoss: 8548.3184\n",
      "Training Epoch: 2 [8000/13500]\tLoss: 8718.2129\n",
      "Training Epoch: 2 [8050/13500]\tLoss: 8377.6396\n",
      "Training Epoch: 2 [8100/13500]\tLoss: 8539.1270\n",
      "Training Epoch: 2 [8150/13500]\tLoss: 8507.0205\n",
      "Training Epoch: 2 [8200/13500]\tLoss: 8437.3965\n",
      "Training Epoch: 2 [8250/13500]\tLoss: 8637.2090\n",
      "Training Epoch: 2 [8300/13500]\tLoss: 8252.5439\n",
      "Training Epoch: 2 [8350/13500]\tLoss: 8529.8369\n",
      "Training Epoch: 2 [8400/13500]\tLoss: 8523.7051\n",
      "Training Epoch: 2 [8450/13500]\tLoss: 8203.2715\n",
      "Training Epoch: 2 [8500/13500]\tLoss: 8287.4990\n",
      "Training Epoch: 2 [8550/13500]\tLoss: 8365.2969\n",
      "Training Epoch: 2 [8600/13500]\tLoss: 8419.5205\n",
      "Training Epoch: 2 [8650/13500]\tLoss: 8320.8994\n",
      "Training Epoch: 2 [8700/13500]\tLoss: 8346.9922\n",
      "Training Epoch: 2 [8750/13500]\tLoss: 8157.0977\n",
      "Training Epoch: 2 [8800/13500]\tLoss: 8135.4004\n",
      "Training Epoch: 2 [8850/13500]\tLoss: 7862.8579\n",
      "Training Epoch: 2 [8900/13500]\tLoss: 8287.3154\n",
      "Training Epoch: 2 [8950/13500]\tLoss: 8410.5283\n",
      "Training Epoch: 2 [9000/13500]\tLoss: 7995.7729\n",
      "Training Epoch: 2 [9050/13500]\tLoss: 8092.1865\n",
      "Training Epoch: 2 [9100/13500]\tLoss: 8344.6074\n",
      "Training Epoch: 2 [9150/13500]\tLoss: 8231.8965\n",
      "Training Epoch: 2 [9200/13500]\tLoss: 7961.1509\n",
      "Training Epoch: 2 [9250/13500]\tLoss: 8110.2612\n",
      "Training Epoch: 2 [9300/13500]\tLoss: 8395.0908\n",
      "Training Epoch: 2 [9350/13500]\tLoss: 8558.3643\n",
      "Training Epoch: 2 [9400/13500]\tLoss: 7986.6270\n",
      "Training Epoch: 2 [9450/13500]\tLoss: 7833.2627\n",
      "Training Epoch: 2 [9500/13500]\tLoss: 8024.1382\n",
      "Training Epoch: 2 [9550/13500]\tLoss: 8119.8442\n",
      "Training Epoch: 2 [9600/13500]\tLoss: 8199.6035\n",
      "Training Epoch: 2 [9650/13500]\tLoss: 7912.4448\n",
      "Training Epoch: 2 [9700/13500]\tLoss: 8128.8335\n",
      "Training Epoch: 2 [9750/13500]\tLoss: 8144.9243\n",
      "Training Epoch: 2 [9800/13500]\tLoss: 7843.4844\n",
      "Training Epoch: 2 [9850/13500]\tLoss: 7927.4312\n",
      "Training Epoch: 2 [9900/13500]\tLoss: 7799.1582\n",
      "Training Epoch: 2 [9950/13500]\tLoss: 8183.8936\n",
      "Training Epoch: 2 [10000/13500]\tLoss: 7844.4980\n",
      "Training Epoch: 2 [10050/13500]\tLoss: 8064.1494\n",
      "Training Epoch: 2 [10100/13500]\tLoss: 7976.8052\n",
      "Training Epoch: 2 [10150/13500]\tLoss: 7930.1973\n",
      "Training Epoch: 2 [10200/13500]\tLoss: 7850.2666\n",
      "Training Epoch: 2 [10250/13500]\tLoss: 7761.1167\n",
      "Training Epoch: 2 [10300/13500]\tLoss: 7813.3252\n",
      "Training Epoch: 2 [10350/13500]\tLoss: 7898.1328\n",
      "Training Epoch: 2 [10400/13500]\tLoss: 8118.2803\n",
      "Training Epoch: 2 [10450/13500]\tLoss: 7887.8667\n",
      "Training Epoch: 2 [10500/13500]\tLoss: 8062.5332\n",
      "Training Epoch: 2 [10550/13500]\tLoss: 8094.8105\n",
      "Training Epoch: 2 [10600/13500]\tLoss: 7971.3037\n",
      "Training Epoch: 2 [10650/13500]\tLoss: 7682.1948\n",
      "Training Epoch: 2 [10700/13500]\tLoss: 7926.4233\n",
      "Training Epoch: 2 [10750/13500]\tLoss: 7879.7979\n",
      "Training Epoch: 2 [10800/13500]\tLoss: 7800.1914\n",
      "Training Epoch: 2 [10850/13500]\tLoss: 7785.4561\n",
      "Training Epoch: 2 [10900/13500]\tLoss: 7723.9282\n",
      "Training Epoch: 2 [10950/13500]\tLoss: 7976.0493\n",
      "Training Epoch: 2 [11000/13500]\tLoss: 7769.1294\n",
      "Training Epoch: 2 [11050/13500]\tLoss: 8006.0610\n",
      "Training Epoch: 2 [11100/13500]\tLoss: 7796.5327\n",
      "Training Epoch: 2 [11150/13500]\tLoss: 7790.4673\n",
      "Training Epoch: 2 [11200/13500]\tLoss: 7717.3975\n",
      "Training Epoch: 2 [11250/13500]\tLoss: 7643.6274\n",
      "Training Epoch: 2 [11300/13500]\tLoss: 7620.0752\n",
      "Training Epoch: 2 [11350/13500]\tLoss: 7776.7349\n",
      "Training Epoch: 2 [11400/13500]\tLoss: 7893.6812\n",
      "Training Epoch: 2 [11450/13500]\tLoss: 7836.7266\n",
      "Training Epoch: 2 [11500/13500]\tLoss: 7509.9829\n",
      "Training Epoch: 2 [11550/13500]\tLoss: 7616.0977\n",
      "Training Epoch: 2 [11600/13500]\tLoss: 7551.7114\n",
      "Training Epoch: 2 [11650/13500]\tLoss: 7507.3442\n",
      "Training Epoch: 2 [11700/13500]\tLoss: 7503.5337\n",
      "Training Epoch: 2 [11750/13500]\tLoss: 7887.0488\n",
      "Training Epoch: 2 [11800/13500]\tLoss: 7745.6499\n",
      "Training Epoch: 2 [11850/13500]\tLoss: 7587.4668\n",
      "Training Epoch: 2 [11900/13500]\tLoss: 7915.4722\n",
      "Training Epoch: 2 [11950/13500]\tLoss: 7692.2266\n",
      "Training Epoch: 2 [12000/13500]\tLoss: 7336.7080\n",
      "Training Epoch: 2 [12050/13500]\tLoss: 7519.2842\n",
      "Training Epoch: 2 [12100/13500]\tLoss: 7703.5781\n",
      "Training Epoch: 2 [12150/13500]\tLoss: 7748.5474\n",
      "Training Epoch: 2 [12200/13500]\tLoss: 7625.9888\n",
      "Training Epoch: 2 [12250/13500]\tLoss: 7322.9585\n",
      "Training Epoch: 2 [12300/13500]\tLoss: 7773.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [12350/13500]\tLoss: 7562.5425\n",
      "Training Epoch: 2 [12400/13500]\tLoss: 7494.8696\n",
      "Training Epoch: 2 [12450/13500]\tLoss: 7511.9810\n",
      "Training Epoch: 2 [12500/13500]\tLoss: 7664.0010\n",
      "Training Epoch: 2 [12550/13500]\tLoss: 7422.5845\n",
      "Training Epoch: 2 [12600/13500]\tLoss: 7419.8604\n",
      "Training Epoch: 2 [12650/13500]\tLoss: 7590.4634\n",
      "Training Epoch: 2 [12700/13500]\tLoss: 7653.4595\n",
      "Training Epoch: 2 [12750/13500]\tLoss: 7352.8735\n",
      "Training Epoch: 2 [12800/13500]\tLoss: 7415.0522\n",
      "Training Epoch: 2 [12850/13500]\tLoss: 7348.2935\n",
      "Training Epoch: 2 [12900/13500]\tLoss: 7497.4463\n",
      "Training Epoch: 2 [12950/13500]\tLoss: 7370.0620\n",
      "Training Epoch: 2 [13000/13500]\tLoss: 7344.0767\n",
      "Training Epoch: 2 [13050/13500]\tLoss: 7326.0557\n",
      "Training Epoch: 2 [13100/13500]\tLoss: 7143.2285\n",
      "Training Epoch: 2 [13150/13500]\tLoss: 7325.4497\n",
      "Training Epoch: 2 [13200/13500]\tLoss: 7294.3979\n",
      "Training Epoch: 2 [13250/13500]\tLoss: 7419.0835\n",
      "Training Epoch: 2 [13300/13500]\tLoss: 7169.7661\n",
      "Training Epoch: 2 [13350/13500]\tLoss: 7283.7329\n",
      "Training Epoch: 2 [13400/13500]\tLoss: 7280.7349\n",
      "Training Epoch: 2 [13450/13500]\tLoss: 7301.4448\n",
      "Training Epoch: 2 [13500/13500]\tLoss: 7541.7188\n",
      "Training Epoch: 2 [1499/1499]\tLoss: 7251.7368\n",
      "Training Epoch: 3 [50/13500]\tLoss: 7353.8462\n",
      "Training Epoch: 3 [100/13500]\tLoss: 7212.7056\n",
      "Training Epoch: 3 [150/13500]\tLoss: 7347.1328\n",
      "Training Epoch: 3 [200/13500]\tLoss: 7221.6001\n",
      "Training Epoch: 3 [250/13500]\tLoss: 7390.8862\n",
      "Training Epoch: 3 [300/13500]\tLoss: 7275.3267\n",
      "Training Epoch: 3 [350/13500]\tLoss: 7357.1138\n",
      "Training Epoch: 3 [400/13500]\tLoss: 7348.7246\n",
      "Training Epoch: 3 [450/13500]\tLoss: 7080.7388\n",
      "Training Epoch: 3 [500/13500]\tLoss: 7092.6909\n",
      "Training Epoch: 3 [550/13500]\tLoss: 7209.1890\n",
      "Training Epoch: 3 [600/13500]\tLoss: 7173.5620\n",
      "Training Epoch: 3 [650/13500]\tLoss: 7129.4155\n",
      "Training Epoch: 3 [700/13500]\tLoss: 7249.1968\n",
      "Training Epoch: 3 [750/13500]\tLoss: 7112.0864\n",
      "Training Epoch: 3 [800/13500]\tLoss: 6991.0894\n",
      "Training Epoch: 3 [850/13500]\tLoss: 7045.7578\n",
      "Training Epoch: 3 [900/13500]\tLoss: 6827.3994\n",
      "Training Epoch: 3 [950/13500]\tLoss: 6797.5254\n",
      "Training Epoch: 3 [1000/13500]\tLoss: 7109.1191\n",
      "Training Epoch: 3 [1050/13500]\tLoss: 7016.8579\n",
      "Training Epoch: 3 [1100/13500]\tLoss: 7063.1836\n",
      "Training Epoch: 3 [1150/13500]\tLoss: 7215.8330\n",
      "Training Epoch: 3 [1200/13500]\tLoss: 6951.8774\n",
      "Training Epoch: 3 [1250/13500]\tLoss: 7054.4902\n",
      "Training Epoch: 3 [1300/13500]\tLoss: 6960.3965\n",
      "Training Epoch: 3 [1350/13500]\tLoss: 7056.2451\n",
      "Training Epoch: 3 [1400/13500]\tLoss: 7129.1929\n",
      "Training Epoch: 3 [1450/13500]\tLoss: 6904.2173\n",
      "Training Epoch: 3 [1500/13500]\tLoss: 7307.9419\n",
      "Training Epoch: 3 [1550/13500]\tLoss: 6834.6836\n",
      "Training Epoch: 3 [1600/13500]\tLoss: 7037.7178\n",
      "Training Epoch: 3 [1650/13500]\tLoss: 7045.4443\n",
      "Training Epoch: 3 [1700/13500]\tLoss: 7030.3721\n",
      "Training Epoch: 3 [1750/13500]\tLoss: 7029.1963\n",
      "Training Epoch: 3 [1800/13500]\tLoss: 6868.7344\n",
      "Training Epoch: 3 [1850/13500]\tLoss: 6910.6846\n",
      "Training Epoch: 3 [1900/13500]\tLoss: 7116.9360\n",
      "Training Epoch: 3 [1950/13500]\tLoss: 6535.2095\n",
      "Training Epoch: 3 [2000/13500]\tLoss: 7046.1382\n",
      "Training Epoch: 3 [2050/13500]\tLoss: 6831.7451\n",
      "Training Epoch: 3 [2100/13500]\tLoss: 6819.1963\n",
      "Training Epoch: 3 [2150/13500]\tLoss: 6805.8765\n",
      "Training Epoch: 3 [2200/13500]\tLoss: 7068.6953\n",
      "Training Epoch: 3 [2250/13500]\tLoss: 6802.7031\n",
      "Training Epoch: 3 [2300/13500]\tLoss: 6940.4512\n",
      "Training Epoch: 3 [2350/13500]\tLoss: 6685.4775\n",
      "Training Epoch: 3 [2400/13500]\tLoss: 6763.3101\n",
      "Training Epoch: 3 [2450/13500]\tLoss: 6922.6465\n",
      "Training Epoch: 3 [2500/13500]\tLoss: 6794.9487\n",
      "Training Epoch: 3 [2550/13500]\tLoss: 7011.7744\n",
      "Training Epoch: 3 [2600/13500]\tLoss: 6893.4429\n",
      "Training Epoch: 3 [2650/13500]\tLoss: 6655.1577\n",
      "Training Epoch: 3 [2700/13500]\tLoss: 6985.1626\n",
      "Training Epoch: 3 [2750/13500]\tLoss: 6626.6011\n",
      "Training Epoch: 3 [2800/13500]\tLoss: 6791.7373\n",
      "Training Epoch: 3 [2850/13500]\tLoss: 6855.6079\n",
      "Training Epoch: 3 [2900/13500]\tLoss: 6762.0952\n",
      "Training Epoch: 3 [2950/13500]\tLoss: 6752.8281\n",
      "Training Epoch: 3 [3000/13500]\tLoss: 6626.9883\n",
      "Training Epoch: 3 [3050/13500]\tLoss: 6608.9541\n",
      "Training Epoch: 3 [3100/13500]\tLoss: 6726.2329\n",
      "Training Epoch: 3 [3150/13500]\tLoss: 6937.9600\n",
      "Training Epoch: 3 [3200/13500]\tLoss: 6866.5522\n",
      "Training Epoch: 3 [3250/13500]\tLoss: 6976.9482\n",
      "Training Epoch: 3 [3300/13500]\tLoss: 6874.1167\n",
      "Training Epoch: 3 [3350/13500]\tLoss: 6808.7593\n",
      "Training Epoch: 3 [3400/13500]\tLoss: 6410.1152\n",
      "Training Epoch: 3 [3450/13500]\tLoss: 6689.9561\n",
      "Training Epoch: 3 [3500/13500]\tLoss: 6891.0762\n",
      "Training Epoch: 3 [3550/13500]\tLoss: 6858.2935\n",
      "Training Epoch: 3 [3600/13500]\tLoss: 6834.5737\n",
      "Training Epoch: 3 [3650/13500]\tLoss: 6758.9526\n",
      "Training Epoch: 3 [3700/13500]\tLoss: 6602.4243\n",
      "Training Epoch: 3 [3750/13500]\tLoss: 6748.4746\n",
      "Training Epoch: 3 [3800/13500]\tLoss: 6921.8682\n",
      "Training Epoch: 3 [3850/13500]\tLoss: 6800.5503\n",
      "Training Epoch: 3 [3900/13500]\tLoss: 6535.9375\n",
      "Training Epoch: 3 [3950/13500]\tLoss: 6709.9580\n",
      "Training Epoch: 3 [4000/13500]\tLoss: 6511.1694\n",
      "Training Epoch: 3 [4050/13500]\tLoss: 6690.9790\n",
      "Training Epoch: 3 [4100/13500]\tLoss: 6691.2441\n",
      "Training Epoch: 3 [4150/13500]\tLoss: 6910.2549\n",
      "Training Epoch: 3 [4200/13500]\tLoss: 6743.6875\n",
      "Training Epoch: 3 [4250/13500]\tLoss: 6596.1353\n",
      "Training Epoch: 3 [4300/13500]\tLoss: 6538.2539\n",
      "Training Epoch: 3 [4350/13500]\tLoss: 6585.3525\n",
      "Training Epoch: 3 [4400/13500]\tLoss: 6551.4849\n",
      "Training Epoch: 3 [4450/13500]\tLoss: 6653.7256\n",
      "Training Epoch: 3 [4500/13500]\tLoss: 6461.8965\n",
      "Training Epoch: 3 [4550/13500]\tLoss: 6377.3369\n",
      "Training Epoch: 3 [4600/13500]\tLoss: 6457.1265\n",
      "Training Epoch: 3 [4650/13500]\tLoss: 6761.6167\n",
      "Training Epoch: 3 [4700/13500]\tLoss: 6659.7739\n",
      "Training Epoch: 3 [4750/13500]\tLoss: 6595.0352\n",
      "Training Epoch: 3 [4800/13500]\tLoss: 6624.5308\n",
      "Training Epoch: 3 [4850/13500]\tLoss: 6571.9277\n",
      "Training Epoch: 3 [4900/13500]\tLoss: 6499.8740\n",
      "Training Epoch: 3 [4950/13500]\tLoss: 6331.9146\n",
      "Training Epoch: 3 [5000/13500]\tLoss: 6300.2354\n",
      "Training Epoch: 3 [5050/13500]\tLoss: 6467.8853\n",
      "Training Epoch: 3 [5100/13500]\tLoss: 6568.8516\n",
      "Training Epoch: 3 [5150/13500]\tLoss: 6383.5020\n",
      "Training Epoch: 3 [5200/13500]\tLoss: 6390.2119\n",
      "Training Epoch: 3 [5250/13500]\tLoss: 6477.1885\n",
      "Training Epoch: 3 [5300/13500]\tLoss: 6308.9536\n",
      "Training Epoch: 3 [5350/13500]\tLoss: 6429.9053\n",
      "Training Epoch: 3 [5400/13500]\tLoss: 6483.3750\n",
      "Training Epoch: 3 [5450/13500]\tLoss: 6338.7607\n",
      "Training Epoch: 3 [5500/13500]\tLoss: 6473.5195\n",
      "Training Epoch: 3 [5550/13500]\tLoss: 6194.6846\n",
      "Training Epoch: 3 [5600/13500]\tLoss: 6466.0205\n",
      "Training Epoch: 3 [5650/13500]\tLoss: 6463.7754\n",
      "Training Epoch: 3 [5700/13500]\tLoss: 6672.1641\n",
      "Training Epoch: 3 [5750/13500]\tLoss: 6506.2100\n",
      "Training Epoch: 3 [5800/13500]\tLoss: 6245.3042\n",
      "Training Epoch: 3 [5850/13500]\tLoss: 6255.7163\n",
      "Training Epoch: 3 [5900/13500]\tLoss: 6362.1436\n",
      "Training Epoch: 3 [5950/13500]\tLoss: 6422.2881\n",
      "Training Epoch: 3 [6000/13500]\tLoss: 6314.0352\n",
      "Training Epoch: 3 [6050/13500]\tLoss: 6295.3325\n",
      "Training Epoch: 3 [6100/13500]\tLoss: 6216.7529\n",
      "Training Epoch: 3 [6150/13500]\tLoss: 6173.6543\n",
      "Training Epoch: 3 [6200/13500]\tLoss: 6409.1270\n",
      "Training Epoch: 3 [6250/13500]\tLoss: 6210.9307\n",
      "Training Epoch: 3 [6300/13500]\tLoss: 6288.5088\n",
      "Training Epoch: 3 [6350/13500]\tLoss: 6208.2808\n",
      "Training Epoch: 3 [6400/13500]\tLoss: 6254.1426\n",
      "Training Epoch: 3 [6450/13500]\tLoss: 6238.9473\n",
      "Training Epoch: 3 [6500/13500]\tLoss: 6414.4917\n",
      "Training Epoch: 3 [6550/13500]\tLoss: 6303.0718\n",
      "Training Epoch: 3 [6600/13500]\tLoss: 6168.0063\n",
      "Training Epoch: 3 [6650/13500]\tLoss: 6294.8535\n",
      "Training Epoch: 3 [6700/13500]\tLoss: 6262.1323\n",
      "Training Epoch: 3 [6750/13500]\tLoss: 6244.7651\n",
      "Training Epoch: 3 [6800/13500]\tLoss: 6062.4287\n",
      "Training Epoch: 3 [6850/13500]\tLoss: 6361.9380\n",
      "Training Epoch: 3 [6900/13500]\tLoss: 6249.9097\n",
      "Training Epoch: 3 [6950/13500]\tLoss: 6212.3027\n",
      "Training Epoch: 3 [7000/13500]\tLoss: 6067.1479\n",
      "Training Epoch: 3 [7050/13500]\tLoss: 6259.8560\n",
      "Training Epoch: 3 [7100/13500]\tLoss: 6378.1299\n",
      "Training Epoch: 3 [7150/13500]\tLoss: 6143.6636\n",
      "Training Epoch: 3 [7200/13500]\tLoss: 6065.7349\n",
      "Training Epoch: 3 [7250/13500]\tLoss: 6208.4922\n",
      "Training Epoch: 3 [7300/13500]\tLoss: 6140.7964\n",
      "Training Epoch: 3 [7350/13500]\tLoss: 6290.0894\n",
      "Training Epoch: 3 [7400/13500]\tLoss: 6025.5439\n",
      "Training Epoch: 3 [7450/13500]\tLoss: 6114.3398\n",
      "Training Epoch: 3 [7500/13500]\tLoss: 6213.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [7550/13500]\tLoss: 6108.2705\n",
      "Training Epoch: 3 [7600/13500]\tLoss: 6103.9727\n",
      "Training Epoch: 3 [7650/13500]\tLoss: 6109.3320\n",
      "Training Epoch: 3 [7700/13500]\tLoss: 6144.4805\n",
      "Training Epoch: 3 [7750/13500]\tLoss: 6039.8877\n",
      "Training Epoch: 3 [7800/13500]\tLoss: 6124.4785\n",
      "Training Epoch: 3 [7850/13500]\tLoss: 6138.7397\n",
      "Training Epoch: 3 [7900/13500]\tLoss: 5920.5708\n",
      "Training Epoch: 3 [7950/13500]\tLoss: 6138.3047\n",
      "Training Epoch: 3 [8000/13500]\tLoss: 6271.2837\n",
      "Training Epoch: 3 [8050/13500]\tLoss: 6060.1504\n",
      "Training Epoch: 3 [8100/13500]\tLoss: 6134.6025\n",
      "Training Epoch: 3 [8150/13500]\tLoss: 6164.3853\n",
      "Training Epoch: 3 [8200/13500]\tLoss: 6063.7627\n",
      "Training Epoch: 3 [8250/13500]\tLoss: 6218.7568\n",
      "Training Epoch: 3 [8300/13500]\tLoss: 5971.3970\n",
      "Training Epoch: 3 [8350/13500]\tLoss: 6144.9233\n",
      "Training Epoch: 3 [8400/13500]\tLoss: 6145.2622\n",
      "Training Epoch: 3 [8450/13500]\tLoss: 6002.3491\n",
      "Training Epoch: 3 [8500/13500]\tLoss: 5994.4629\n",
      "Training Epoch: 3 [8550/13500]\tLoss: 6030.8950\n",
      "Training Epoch: 3 [8600/13500]\tLoss: 6039.4692\n",
      "Training Epoch: 3 [8650/13500]\tLoss: 6048.6636\n",
      "Training Epoch: 3 [8700/13500]\tLoss: 6052.3804\n",
      "Training Epoch: 3 [8750/13500]\tLoss: 5936.7817\n",
      "Training Epoch: 3 [8800/13500]\tLoss: 5915.4697\n",
      "Training Epoch: 3 [8850/13500]\tLoss: 5698.9341\n",
      "Training Epoch: 3 [8900/13500]\tLoss: 6000.5132\n",
      "Training Epoch: 3 [8950/13500]\tLoss: 6112.5361\n",
      "Training Epoch: 3 [9000/13500]\tLoss: 5801.9780\n",
      "Training Epoch: 3 [9050/13500]\tLoss: 5890.8599\n",
      "Training Epoch: 3 [9100/13500]\tLoss: 6073.2720\n",
      "Training Epoch: 3 [9150/13500]\tLoss: 6007.4512\n",
      "Training Epoch: 3 [9200/13500]\tLoss: 5829.6904\n",
      "Training Epoch: 3 [9250/13500]\tLoss: 5929.3853\n",
      "Training Epoch: 3 [9300/13500]\tLoss: 6105.0581\n",
      "Training Epoch: 3 [9350/13500]\tLoss: 6212.6724\n",
      "Training Epoch: 3 [9400/13500]\tLoss: 5835.4482\n",
      "Training Epoch: 3 [9450/13500]\tLoss: 5761.2559\n",
      "Training Epoch: 3 [9500/13500]\tLoss: 5906.5361\n",
      "Training Epoch: 3 [9550/13500]\tLoss: 5938.3032\n",
      "Training Epoch: 3 [9600/13500]\tLoss: 5952.4702\n",
      "Training Epoch: 3 [9650/13500]\tLoss: 5794.4458\n",
      "Training Epoch: 3 [9700/13500]\tLoss: 5988.2041\n",
      "Training Epoch: 3 [9750/13500]\tLoss: 5966.9062\n",
      "Training Epoch: 3 [9800/13500]\tLoss: 5742.6040\n",
      "Training Epoch: 3 [9850/13500]\tLoss: 5833.0806\n",
      "Training Epoch: 3 [9900/13500]\tLoss: 5714.1362\n",
      "Training Epoch: 3 [9950/13500]\tLoss: 6010.9580\n",
      "Training Epoch: 3 [10000/13500]\tLoss: 5792.4683\n",
      "Training Epoch: 3 [10050/13500]\tLoss: 5917.7476\n",
      "Training Epoch: 3 [10100/13500]\tLoss: 5849.7021\n",
      "Training Epoch: 3 [10150/13500]\tLoss: 5875.9692\n",
      "Training Epoch: 3 [10200/13500]\tLoss: 5764.4287\n",
      "Training Epoch: 3 [10250/13500]\tLoss: 5720.7637\n",
      "Training Epoch: 3 [10300/13500]\tLoss: 5746.6279\n",
      "Training Epoch: 3 [10350/13500]\tLoss: 5806.8853\n",
      "Training Epoch: 3 [10400/13500]\tLoss: 5967.0430\n",
      "Training Epoch: 3 [10450/13500]\tLoss: 5825.2964\n",
      "Training Epoch: 3 [10500/13500]\tLoss: 5931.1230\n",
      "Training Epoch: 3 [10550/13500]\tLoss: 5947.0845\n",
      "Training Epoch: 3 [10600/13500]\tLoss: 5888.1533\n",
      "Training Epoch: 3 [10650/13500]\tLoss: 5677.5947\n",
      "Training Epoch: 3 [10700/13500]\tLoss: 5860.4648\n",
      "Training Epoch: 3 [10750/13500]\tLoss: 5785.0576\n",
      "Training Epoch: 3 [10800/13500]\tLoss: 5763.3223\n",
      "Training Epoch: 3 [10850/13500]\tLoss: 5783.0371\n",
      "Training Epoch: 3 [10900/13500]\tLoss: 5703.1855\n",
      "Training Epoch: 3 [10950/13500]\tLoss: 5893.8662\n",
      "Training Epoch: 3 [11000/13500]\tLoss: 5760.6206\n",
      "Training Epoch: 3 [11050/13500]\tLoss: 5929.6709\n",
      "Training Epoch: 3 [11100/13500]\tLoss: 5751.4517\n",
      "Training Epoch: 3 [11150/13500]\tLoss: 5795.8862\n",
      "Training Epoch: 3 [11200/13500]\tLoss: 5729.7373\n",
      "Training Epoch: 3 [11250/13500]\tLoss: 5633.1504\n",
      "Training Epoch: 3 [11300/13500]\tLoss: 5670.2720\n",
      "Training Epoch: 3 [11350/13500]\tLoss: 5761.8325\n",
      "Training Epoch: 3 [11400/13500]\tLoss: 5836.6069\n",
      "Training Epoch: 3 [11450/13500]\tLoss: 5830.8784\n",
      "Training Epoch: 3 [11500/13500]\tLoss: 5565.1558\n",
      "Training Epoch: 3 [11550/13500]\tLoss: 5691.4482\n",
      "Training Epoch: 3 [11600/13500]\tLoss: 5627.5225\n",
      "Training Epoch: 3 [11650/13500]\tLoss: 5573.3589\n",
      "Training Epoch: 3 [11700/13500]\tLoss: 5617.4336\n",
      "Training Epoch: 3 [11750/13500]\tLoss: 5853.4648\n",
      "Training Epoch: 3 [11800/13500]\tLoss: 5758.0811\n",
      "Training Epoch: 3 [11850/13500]\tLoss: 5612.8350\n",
      "Training Epoch: 3 [11900/13500]\tLoss: 5875.8506\n",
      "Training Epoch: 3 [11950/13500]\tLoss: 5738.8340\n",
      "Training Epoch: 3 [12000/13500]\tLoss: 5474.3271\n",
      "Training Epoch: 3 [12050/13500]\tLoss: 5624.9199\n",
      "Training Epoch: 3 [12100/13500]\tLoss: 5762.7295\n",
      "Training Epoch: 3 [12150/13500]\tLoss: 5757.5908\n",
      "Training Epoch: 3 [12200/13500]\tLoss: 5673.3511\n",
      "Training Epoch: 3 [12250/13500]\tLoss: 5461.0767\n",
      "Training Epoch: 3 [12300/13500]\tLoss: 5774.6348\n",
      "Training Epoch: 3 [12350/13500]\tLoss: 5673.4492\n",
      "Training Epoch: 3 [12400/13500]\tLoss: 5609.3032\n",
      "Training Epoch: 3 [12450/13500]\tLoss: 5639.3955\n",
      "Training Epoch: 3 [12500/13500]\tLoss: 5753.7031\n",
      "Training Epoch: 3 [12550/13500]\tLoss: 5589.0684\n",
      "Training Epoch: 3 [12600/13500]\tLoss: 5582.1025\n",
      "Training Epoch: 3 [12650/13500]\tLoss: 5665.1465\n",
      "Training Epoch: 3 [12700/13500]\tLoss: 5766.2744\n",
      "Training Epoch: 3 [12750/13500]\tLoss: 5525.9780\n",
      "Training Epoch: 3 [12800/13500]\tLoss: 5574.9595\n",
      "Training Epoch: 3 [12850/13500]\tLoss: 5533.3438\n",
      "Training Epoch: 3 [12900/13500]\tLoss: 5633.0762\n",
      "Training Epoch: 3 [12950/13500]\tLoss: 5509.0527\n",
      "Training Epoch: 3 [13000/13500]\tLoss: 5522.9766\n",
      "Training Epoch: 3 [13050/13500]\tLoss: 5525.9683\n",
      "Training Epoch: 3 [13100/13500]\tLoss: 5365.2812\n",
      "Training Epoch: 3 [13150/13500]\tLoss: 5524.2358\n",
      "Training Epoch: 3 [13200/13500]\tLoss: 5504.8955\n",
      "Training Epoch: 3 [13250/13500]\tLoss: 5568.3042\n",
      "Training Epoch: 3 [13300/13500]\tLoss: 5405.1782\n",
      "Training Epoch: 3 [13350/13500]\tLoss: 5503.1440\n",
      "Training Epoch: 3 [13400/13500]\tLoss: 5456.4741\n",
      "Training Epoch: 3 [13450/13500]\tLoss: 5537.4717\n",
      "Training Epoch: 3 [13500/13500]\tLoss: 5683.2432\n",
      "Training Epoch: 3 [1499/1499]\tLoss: 5476.7240\n",
      "Training Epoch: 4 [50/13500]\tLoss: 5542.4883\n",
      "Training Epoch: 4 [100/13500]\tLoss: 5442.2959\n",
      "Training Epoch: 4 [150/13500]\tLoss: 5528.3330\n",
      "Training Epoch: 4 [200/13500]\tLoss: 5443.4834\n",
      "Training Epoch: 4 [250/13500]\tLoss: 5593.9146\n",
      "Training Epoch: 4 [300/13500]\tLoss: 5509.1274\n",
      "Training Epoch: 4 [350/13500]\tLoss: 5586.8726\n",
      "Training Epoch: 4 [400/13500]\tLoss: 5559.1230\n",
      "Training Epoch: 4 [450/13500]\tLoss: 5387.1348\n",
      "Training Epoch: 4 [500/13500]\tLoss: 5366.1113\n",
      "Training Epoch: 4 [550/13500]\tLoss: 5442.4624\n",
      "Training Epoch: 4 [600/13500]\tLoss: 5444.6353\n",
      "Training Epoch: 4 [650/13500]\tLoss: 5392.2832\n",
      "Training Epoch: 4 [700/13500]\tLoss: 5440.6484\n",
      "Training Epoch: 4 [750/13500]\tLoss: 5387.1040\n",
      "Training Epoch: 4 [800/13500]\tLoss: 5353.3511\n",
      "Training Epoch: 4 [850/13500]\tLoss: 5372.1680\n",
      "Training Epoch: 4 [900/13500]\tLoss: 5224.4951\n",
      "Training Epoch: 4 [950/13500]\tLoss: 5207.4966\n",
      "Training Epoch: 4 [1000/13500]\tLoss: 5370.7397\n",
      "Training Epoch: 4 [1050/13500]\tLoss: 5339.0967\n",
      "Training Epoch: 4 [1100/13500]\tLoss: 5394.5762\n",
      "Training Epoch: 4 [1150/13500]\tLoss: 5489.1372\n",
      "Training Epoch: 4 [1200/13500]\tLoss: 5288.8159\n",
      "Training Epoch: 4 [1250/13500]\tLoss: 5358.9224\n",
      "Training Epoch: 4 [1300/13500]\tLoss: 5264.7866\n",
      "Training Epoch: 4 [1350/13500]\tLoss: 5379.8901\n",
      "Training Epoch: 4 [1400/13500]\tLoss: 5398.8311\n",
      "Training Epoch: 4 [1450/13500]\tLoss: 5254.9966\n",
      "Training Epoch: 4 [1500/13500]\tLoss: 5559.7026\n",
      "Training Epoch: 4 [1550/13500]\tLoss: 5255.5859\n",
      "Training Epoch: 4 [1600/13500]\tLoss: 5389.4121\n",
      "Training Epoch: 4 [1650/13500]\tLoss: 5370.1938\n",
      "Training Epoch: 4 [1700/13500]\tLoss: 5388.5547\n",
      "Training Epoch: 4 [1750/13500]\tLoss: 5399.6650\n",
      "Training Epoch: 4 [1800/13500]\tLoss: 5291.3701\n",
      "Training Epoch: 4 [1850/13500]\tLoss: 5256.6333\n",
      "Training Epoch: 4 [1900/13500]\tLoss: 5408.3105\n",
      "Training Epoch: 4 [1950/13500]\tLoss: 5003.6382\n",
      "Training Epoch: 4 [2000/13500]\tLoss: 5407.7964\n",
      "Training Epoch: 4 [2050/13500]\tLoss: 5231.5693\n",
      "Training Epoch: 4 [2100/13500]\tLoss: 5201.8066\n",
      "Training Epoch: 4 [2150/13500]\tLoss: 5232.8262\n",
      "Training Epoch: 4 [2200/13500]\tLoss: 5431.6582\n",
      "Training Epoch: 4 [2250/13500]\tLoss: 5224.7871\n",
      "Training Epoch: 4 [2300/13500]\tLoss: 5310.3521\n",
      "Training Epoch: 4 [2350/13500]\tLoss: 5146.0825\n",
      "Training Epoch: 4 [2400/13500]\tLoss: 5167.7671\n",
      "Training Epoch: 4 [2450/13500]\tLoss: 5335.4214\n",
      "Training Epoch: 4 [2500/13500]\tLoss: 5212.4824\n",
      "Training Epoch: 4 [2550/13500]\tLoss: 5360.5562\n",
      "Training Epoch: 4 [2600/13500]\tLoss: 5285.8052\n",
      "Training Epoch: 4 [2650/13500]\tLoss: 5134.2349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [2700/13500]\tLoss: 5387.7349\n",
      "Training Epoch: 4 [2750/13500]\tLoss: 5105.9634\n",
      "Training Epoch: 4 [2800/13500]\tLoss: 5234.2358\n",
      "Training Epoch: 4 [2850/13500]\tLoss: 5263.9546\n",
      "Training Epoch: 4 [2900/13500]\tLoss: 5214.8403\n",
      "Training Epoch: 4 [2950/13500]\tLoss: 5205.5469\n",
      "Training Epoch: 4 [3000/13500]\tLoss: 5104.3994\n",
      "Training Epoch: 4 [3050/13500]\tLoss: 5076.4297\n",
      "Training Epoch: 4 [3100/13500]\tLoss: 5183.3149\n",
      "Training Epoch: 4 [3150/13500]\tLoss: 5336.4360\n",
      "Training Epoch: 4 [3200/13500]\tLoss: 5311.6089\n",
      "Training Epoch: 4 [3250/13500]\tLoss: 5396.2563\n",
      "Training Epoch: 4 [3300/13500]\tLoss: 5312.5166\n",
      "Training Epoch: 4 [3350/13500]\tLoss: 5240.1152\n",
      "Training Epoch: 4 [3400/13500]\tLoss: 4936.7031\n",
      "Training Epoch: 4 [3450/13500]\tLoss: 5172.5713\n",
      "Training Epoch: 4 [3500/13500]\tLoss: 5330.3105\n",
      "Training Epoch: 4 [3550/13500]\tLoss: 5298.0200\n",
      "Training Epoch: 4 [3600/13500]\tLoss: 5266.0586\n",
      "Training Epoch: 4 [3650/13500]\tLoss: 5250.3003\n",
      "Training Epoch: 4 [3700/13500]\tLoss: 5112.2896\n",
      "Training Epoch: 4 [3750/13500]\tLoss: 5253.3506\n",
      "Training Epoch: 4 [3800/13500]\tLoss: 5355.2134\n",
      "Training Epoch: 4 [3850/13500]\tLoss: 5255.5054\n",
      "Training Epoch: 4 [3900/13500]\tLoss: 5097.5078\n",
      "Training Epoch: 4 [3950/13500]\tLoss: 5193.4482\n",
      "Training Epoch: 4 [4000/13500]\tLoss: 5070.1050\n",
      "Training Epoch: 4 [4050/13500]\tLoss: 5187.6245\n",
      "Training Epoch: 4 [4100/13500]\tLoss: 5200.4385\n",
      "Training Epoch: 4 [4150/13500]\tLoss: 5368.2036\n",
      "Training Epoch: 4 [4200/13500]\tLoss: 5281.7354\n",
      "Training Epoch: 4 [4250/13500]\tLoss: 5130.7437\n",
      "Training Epoch: 4 [4300/13500]\tLoss: 5078.3354\n",
      "Training Epoch: 4 [4350/13500]\tLoss: 5080.7231\n",
      "Training Epoch: 4 [4400/13500]\tLoss: 5113.6938\n",
      "Training Epoch: 4 [4450/13500]\tLoss: 5154.5908\n",
      "Training Epoch: 4 [4500/13500]\tLoss: 5057.1538\n",
      "Training Epoch: 4 [4550/13500]\tLoss: 4999.5371\n",
      "Training Epoch: 4 [4600/13500]\tLoss: 5062.0879\n",
      "Training Epoch: 4 [4650/13500]\tLoss: 5279.3643\n",
      "Training Epoch: 4 [4700/13500]\tLoss: 5175.3169\n",
      "Training Epoch: 4 [4750/13500]\tLoss: 5158.8105\n",
      "Training Epoch: 4 [4800/13500]\tLoss: 5153.2529\n",
      "Training Epoch: 4 [4850/13500]\tLoss: 5112.5229\n",
      "Training Epoch: 4 [4900/13500]\tLoss: 5080.8086\n",
      "Training Epoch: 4 [4950/13500]\tLoss: 4943.8022\n",
      "Training Epoch: 4 [5000/13500]\tLoss: 4911.8350\n",
      "Training Epoch: 4 [5050/13500]\tLoss: 5041.9028\n",
      "Training Epoch: 4 [5100/13500]\tLoss: 5139.5542\n",
      "Training Epoch: 4 [5150/13500]\tLoss: 4983.3140\n",
      "Training Epoch: 4 [5200/13500]\tLoss: 4969.7080\n",
      "Training Epoch: 4 [5250/13500]\tLoss: 5053.9053\n",
      "Training Epoch: 4 [5300/13500]\tLoss: 4931.5317\n",
      "Training Epoch: 4 [5350/13500]\tLoss: 5006.9038\n",
      "Training Epoch: 4 [5400/13500]\tLoss: 5081.0938\n",
      "Training Epoch: 4 [5450/13500]\tLoss: 4988.1953\n",
      "Training Epoch: 4 [5500/13500]\tLoss: 5076.2612\n",
      "Training Epoch: 4 [5550/13500]\tLoss: 4862.5420\n",
      "Training Epoch: 4 [5600/13500]\tLoss: 5040.8950\n",
      "Training Epoch: 4 [5650/13500]\tLoss: 5072.9155\n",
      "Training Epoch: 4 [5700/13500]\tLoss: 5252.8857\n",
      "Training Epoch: 4 [5750/13500]\tLoss: 5117.0098\n",
      "Training Epoch: 4 [5800/13500]\tLoss: 4922.5234\n",
      "Training Epoch: 4 [5850/13500]\tLoss: 4886.7466\n",
      "Training Epoch: 4 [5900/13500]\tLoss: 4981.1162\n",
      "Training Epoch: 4 [5950/13500]\tLoss: 5053.5957\n",
      "Training Epoch: 4 [6000/13500]\tLoss: 4929.6406\n",
      "Training Epoch: 4 [6050/13500]\tLoss: 4948.3379\n",
      "Training Epoch: 4 [6100/13500]\tLoss: 4877.0596\n",
      "Training Epoch: 4 [6150/13500]\tLoss: 4871.8188\n",
      "Training Epoch: 4 [6200/13500]\tLoss: 5020.4780\n",
      "Training Epoch: 4 [6250/13500]\tLoss: 4946.3169\n",
      "Training Epoch: 4 [6300/13500]\tLoss: 4921.8174\n",
      "Training Epoch: 4 [6350/13500]\tLoss: 4888.8643\n",
      "Training Epoch: 4 [6400/13500]\tLoss: 4922.6294\n",
      "Training Epoch: 4 [6450/13500]\tLoss: 4923.7012\n",
      "Training Epoch: 4 [6500/13500]\tLoss: 5020.9536\n",
      "Training Epoch: 4 [6550/13500]\tLoss: 4945.5439\n",
      "Training Epoch: 4 [6600/13500]\tLoss: 4846.9360\n",
      "Training Epoch: 4 [6650/13500]\tLoss: 4941.6504\n",
      "Training Epoch: 4 [6700/13500]\tLoss: 4925.9702\n",
      "Training Epoch: 4 [6750/13500]\tLoss: 4942.0605\n",
      "Training Epoch: 4 [6800/13500]\tLoss: 4784.3804\n",
      "Training Epoch: 4 [6850/13500]\tLoss: 5017.1680\n",
      "Training Epoch: 4 [6900/13500]\tLoss: 4914.3154\n",
      "Training Epoch: 4 [6950/13500]\tLoss: 4918.0933\n",
      "Training Epoch: 4 [7000/13500]\tLoss: 4792.6753\n",
      "Training Epoch: 4 [7050/13500]\tLoss: 4930.6211\n",
      "Training Epoch: 4 [7100/13500]\tLoss: 5025.8833\n",
      "Training Epoch: 4 [7150/13500]\tLoss: 4830.9106\n",
      "Training Epoch: 4 [7200/13500]\tLoss: 4819.3433\n",
      "Training Epoch: 4 [7250/13500]\tLoss: 4927.3511\n",
      "Training Epoch: 4 [7300/13500]\tLoss: 4853.0449\n",
      "Training Epoch: 4 [7350/13500]\tLoss: 4950.9336\n",
      "Training Epoch: 4 [7400/13500]\tLoss: 4771.1572\n",
      "Training Epoch: 4 [7450/13500]\tLoss: 4841.7085\n",
      "Training Epoch: 4 [7500/13500]\tLoss: 4897.4121\n",
      "Training Epoch: 4 [7550/13500]\tLoss: 4854.2178\n",
      "Training Epoch: 4 [7600/13500]\tLoss: 4861.8379\n",
      "Training Epoch: 4 [7650/13500]\tLoss: 4848.0249\n",
      "Training Epoch: 4 [7700/13500]\tLoss: 4843.3599\n",
      "Training Epoch: 4 [7750/13500]\tLoss: 4796.6479\n",
      "Training Epoch: 4 [7800/13500]\tLoss: 4856.8135\n",
      "Training Epoch: 4 [7850/13500]\tLoss: 4849.6240\n",
      "Training Epoch: 4 [7900/13500]\tLoss: 4716.0234\n",
      "Training Epoch: 4 [7950/13500]\tLoss: 4852.2808\n",
      "Training Epoch: 4 [8000/13500]\tLoss: 4968.1699\n",
      "Training Epoch: 4 [8050/13500]\tLoss: 4819.0942\n",
      "Training Epoch: 4 [8100/13500]\tLoss: 4866.4492\n",
      "Training Epoch: 4 [8150/13500]\tLoss: 4903.4272\n",
      "Training Epoch: 4 [8200/13500]\tLoss: 4804.7949\n",
      "Training Epoch: 4 [8250/13500]\tLoss: 4919.3042\n",
      "Training Epoch: 4 [8300/13500]\tLoss: 4770.8765\n",
      "Training Epoch: 4 [8350/13500]\tLoss: 4870.7808\n",
      "Training Epoch: 4 [8400/13500]\tLoss: 4872.5117\n",
      "Training Epoch: 4 [8450/13500]\tLoss: 4821.1943\n",
      "Training Epoch: 4 [8500/13500]\tLoss: 4767.7422\n",
      "Training Epoch: 4 [8550/13500]\tLoss: 4775.7305\n",
      "Training Epoch: 4 [8600/13500]\tLoss: 4773.1436\n",
      "Training Epoch: 4 [8650/13500]\tLoss: 4828.1460\n",
      "Training Epoch: 4 [8700/13500]\tLoss: 4820.7578\n",
      "Training Epoch: 4 [8750/13500]\tLoss: 4743.9980\n",
      "Training Epoch: 4 [8800/13500]\tLoss: 4727.5703\n",
      "Training Epoch: 4 [8850/13500]\tLoss: 4546.3892\n",
      "Training Epoch: 4 [8900/13500]\tLoss: 4777.8750\n",
      "Training Epoch: 4 [8950/13500]\tLoss: 4869.6982\n",
      "Training Epoch: 4 [9000/13500]\tLoss: 4638.5308\n",
      "Training Epoch: 4 [9050/13500]\tLoss: 4715.6885\n",
      "Training Epoch: 4 [9100/13500]\tLoss: 4850.7529\n",
      "Training Epoch: 4 [9150/13500]\tLoss: 4805.8018\n",
      "Training Epoch: 4 [9200/13500]\tLoss: 4667.9756\n",
      "Training Epoch: 4 [9250/13500]\tLoss: 4749.3081\n",
      "Training Epoch: 4 [9300/13500]\tLoss: 4867.6509\n",
      "Training Epoch: 4 [9350/13500]\tLoss: 4941.1592\n",
      "Training Epoch: 4 [9400/13500]\tLoss: 4675.4111\n",
      "Training Epoch: 4 [9450/13500]\tLoss: 4638.5405\n",
      "Training Epoch: 4 [9500/13500]\tLoss: 4740.0122\n",
      "Training Epoch: 4 [9550/13500]\tLoss: 4755.1396\n",
      "Training Epoch: 4 [9600/13500]\tLoss: 4742.2632\n",
      "Training Epoch: 4 [9650/13500]\tLoss: 4642.5811\n",
      "Training Epoch: 4 [9700/13500]\tLoss: 4826.3994\n",
      "Training Epoch: 4 [9750/13500]\tLoss: 4776.4390\n",
      "Training Epoch: 4 [9800/13500]\tLoss: 4603.2178\n",
      "Training Epoch: 4 [9850/13500]\tLoss: 4691.8760\n",
      "Training Epoch: 4 [9900/13500]\tLoss: 4571.1226\n",
      "Training Epoch: 4 [9950/13500]\tLoss: 4821.0703\n",
      "Training Epoch: 4 [10000/13500]\tLoss: 4668.4297\n",
      "Training Epoch: 4 [10050/13500]\tLoss: 4743.1660\n",
      "Training Epoch: 4 [10100/13500]\tLoss: 4692.0591\n",
      "Training Epoch: 4 [10150/13500]\tLoss: 4755.1001\n",
      "Training Epoch: 4 [10200/13500]\tLoss: 4629.1826\n",
      "Training Epoch: 4 [10250/13500]\tLoss: 4611.0845\n",
      "Training Epoch: 4 [10300/13500]\tLoss: 4615.0840\n",
      "Training Epoch: 4 [10350/13500]\tLoss: 4670.7466\n",
      "Training Epoch: 4 [10400/13500]\tLoss: 4800.9409\n",
      "Training Epoch: 4 [10450/13500]\tLoss: 4710.1626\n",
      "Training Epoch: 4 [10500/13500]\tLoss: 4761.5205\n",
      "Training Epoch: 4 [10550/13500]\tLoss: 4763.9976\n",
      "Training Epoch: 4 [10600/13500]\tLoss: 4739.0532\n",
      "Training Epoch: 4 [10650/13500]\tLoss: 4582.9844\n",
      "Training Epoch: 4 [10700/13500]\tLoss: 4717.9805\n",
      "Training Epoch: 4 [10750/13500]\tLoss: 4649.0039\n",
      "Training Epoch: 4 [10800/13500]\tLoss: 4642.7510\n",
      "Training Epoch: 4 [10850/13500]\tLoss: 4670.9927\n",
      "Training Epoch: 4 [10900/13500]\tLoss: 4578.1753\n",
      "Training Epoch: 4 [10950/13500]\tLoss: 4737.7754\n",
      "Training Epoch: 4 [11000/13500]\tLoss: 4653.9717\n",
      "Training Epoch: 4 [11050/13500]\tLoss: 4783.7231\n",
      "Training Epoch: 4 [11100/13500]\tLoss: 4621.8701\n",
      "Training Epoch: 4 [11150/13500]\tLoss: 4688.9058\n",
      "Training Epoch: 4 [11200/13500]\tLoss: 4623.4062\n",
      "Training Epoch: 4 [11250/13500]\tLoss: 4518.0264\n",
      "Training Epoch: 4 [11300/13500]\tLoss: 4591.0195\n",
      "Training Epoch: 4 [11350/13500]\tLoss: 4643.4482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [11400/13500]\tLoss: 4692.6748\n",
      "Training Epoch: 4 [11450/13500]\tLoss: 4718.7173\n",
      "Training Epoch: 4 [11500/13500]\tLoss: 4499.0078\n",
      "Training Epoch: 4 [11550/13500]\tLoss: 4629.7236\n",
      "Training Epoch: 4 [11600/13500]\tLoss: 4553.5532\n",
      "Training Epoch: 4 [11650/13500]\tLoss: 4500.2866\n",
      "Training Epoch: 4 [11700/13500]\tLoss: 4565.4834\n",
      "Training Epoch: 4 [11750/13500]\tLoss: 4715.3691\n",
      "Training Epoch: 4 [11800/13500]\tLoss: 4651.2056\n",
      "Training Epoch: 4 [11850/13500]\tLoss: 4525.0186\n",
      "Training Epoch: 4 [11900/13500]\tLoss: 4739.8271\n",
      "Training Epoch: 4 [11950/13500]\tLoss: 4649.5776\n",
      "Training Epoch: 4 [12000/13500]\tLoss: 4442.8921\n",
      "Training Epoch: 4 [12050/13500]\tLoss: 4558.1265\n",
      "Training Epoch: 4 [12100/13500]\tLoss: 4676.4854\n",
      "Training Epoch: 4 [12150/13500]\tLoss: 4636.7065\n",
      "Training Epoch: 4 [12200/13500]\tLoss: 4573.3633\n",
      "Training Epoch: 4 [12250/13500]\tLoss: 4424.9097\n",
      "Training Epoch: 4 [12300/13500]\tLoss: 4658.9287\n",
      "Training Epoch: 4 [12350/13500]\tLoss: 4610.6680\n",
      "Training Epoch: 4 [12400/13500]\tLoss: 4545.7979\n",
      "Training Epoch: 4 [12450/13500]\tLoss: 4579.3594\n",
      "Training Epoch: 4 [12500/13500]\tLoss: 4685.2480\n",
      "Training Epoch: 4 [12550/13500]\tLoss: 4553.2251\n",
      "Training Epoch: 4 [12600/13500]\tLoss: 4550.8276\n",
      "Training Epoch: 4 [12650/13500]\tLoss: 4597.7354\n",
      "Training Epoch: 4 [12700/13500]\tLoss: 4708.4902\n",
      "Training Epoch: 4 [12750/13500]\tLoss: 4509.8364\n",
      "Training Epoch: 4 [12800/13500]\tLoss: 4535.0884\n",
      "Training Epoch: 4 [12850/13500]\tLoss: 4511.3853\n",
      "Training Epoch: 4 [12900/13500]\tLoss: 4570.7051\n",
      "Training Epoch: 4 [12950/13500]\tLoss: 4445.2632\n",
      "Training Epoch: 4 [13000/13500]\tLoss: 4499.0005\n",
      "Training Epoch: 4 [13050/13500]\tLoss: 4501.5684\n",
      "Training Epoch: 4 [13100/13500]\tLoss: 4354.9180\n",
      "Training Epoch: 4 [13150/13500]\tLoss: 4499.4756\n",
      "Training Epoch: 4 [13200/13500]\tLoss: 4479.5200\n",
      "Training Epoch: 4 [13250/13500]\tLoss: 4511.9087\n",
      "Training Epoch: 4 [13300/13500]\tLoss: 4403.6646\n",
      "Training Epoch: 4 [13350/13500]\tLoss: 4496.6719\n",
      "Training Epoch: 4 [13400/13500]\tLoss: 4419.9453\n",
      "Training Epoch: 4 [13450/13500]\tLoss: 4522.3784\n",
      "Training Epoch: 4 [13500/13500]\tLoss: 4620.4707\n",
      "Training Epoch: 4 [1499/1499]\tLoss: 4466.6072\n",
      "Training Epoch: 5 [50/13500]\tLoss: 4514.0581\n",
      "Training Epoch: 5 [100/13500]\tLoss: 4435.6299\n",
      "Training Epoch: 5 [150/13500]\tLoss: 4478.7036\n",
      "Training Epoch: 5 [200/13500]\tLoss: 4430.5605\n",
      "Training Epoch: 5 [250/13500]\tLoss: 4557.7031\n",
      "Training Epoch: 5 [300/13500]\tLoss: 4494.6519\n",
      "Training Epoch: 5 [350/13500]\tLoss: 4562.7783\n",
      "Training Epoch: 5 [400/13500]\tLoss: 4530.7700\n",
      "Training Epoch: 5 [450/13500]\tLoss: 4408.7124\n",
      "Training Epoch: 5 [500/13500]\tLoss: 4377.4482\n",
      "Training Epoch: 5 [550/13500]\tLoss: 4425.9082\n",
      "Training Epoch: 5 [600/13500]\tLoss: 4466.4629\n",
      "Training Epoch: 5 [650/13500]\tLoss: 4391.6143\n",
      "Training Epoch: 5 [700/13500]\tLoss: 4407.7832\n",
      "Training Epoch: 5 [750/13500]\tLoss: 4403.2959\n",
      "Training Epoch: 5 [800/13500]\tLoss: 4401.4199\n",
      "Training Epoch: 5 [850/13500]\tLoss: 4402.2441\n",
      "Training Epoch: 5 [900/13500]\tLoss: 4287.3428\n",
      "Training Epoch: 5 [950/13500]\tLoss: 4285.9121\n",
      "Training Epoch: 5 [1000/13500]\tLoss: 4367.7231\n",
      "Training Epoch: 5 [1050/13500]\tLoss: 4362.9014\n",
      "Training Epoch: 5 [1100/13500]\tLoss: 4439.3999\n",
      "Training Epoch: 5 [1150/13500]\tLoss: 4494.8462\n",
      "Training Epoch: 5 [1200/13500]\tLoss: 4332.1343\n",
      "Training Epoch: 5 [1250/13500]\tLoss: 4377.6323\n",
      "Training Epoch: 5 [1300/13500]\tLoss: 4280.0708\n",
      "Training Epoch: 5 [1350/13500]\tLoss: 4416.1533\n",
      "Training Epoch: 5 [1400/13500]\tLoss: 4400.5112\n",
      "Training Epoch: 5 [1450/13500]\tLoss: 4301.3159\n",
      "Training Epoch: 5 [1500/13500]\tLoss: 4552.2363\n",
      "Training Epoch: 5 [1550/13500]\tLoss: 4334.8789\n",
      "Training Epoch: 5 [1600/13500]\tLoss: 4438.8564\n",
      "Training Epoch: 5 [1650/13500]\tLoss: 4393.3511\n",
      "Training Epoch: 5 [1700/13500]\tLoss: 4446.2070\n",
      "Training Epoch: 5 [1750/13500]\tLoss: 4462.7817\n",
      "Training Epoch: 5 [1800/13500]\tLoss: 4362.6655\n",
      "Training Epoch: 5 [1850/13500]\tLoss: 4298.7417\n",
      "Training Epoch: 5 [1900/13500]\tLoss: 4413.3589\n",
      "Training Epoch: 5 [1950/13500]\tLoss: 4116.4355\n",
      "Training Epoch: 5 [2000/13500]\tLoss: 4448.4277\n",
      "Training Epoch: 5 [2050/13500]\tLoss: 4292.7188\n",
      "Training Epoch: 5 [2100/13500]\tLoss: 4262.7739\n",
      "Training Epoch: 5 [2150/13500]\tLoss: 4316.0718\n",
      "Training Epoch: 5 [2200/13500]\tLoss: 4471.9385\n",
      "Training Epoch: 5 [2250/13500]\tLoss: 4305.9365\n",
      "Training Epoch: 5 [2300/13500]\tLoss: 4353.1606\n",
      "Training Epoch: 5 [2350/13500]\tLoss: 4247.9883\n",
      "Training Epoch: 5 [2400/13500]\tLoss: 4243.1406\n",
      "Training Epoch: 5 [2450/13500]\tLoss: 4409.0640\n",
      "Training Epoch: 5 [2500/13500]\tLoss: 4280.1025\n",
      "Training Epoch: 5 [2550/13500]\tLoss: 4388.8330\n",
      "Training Epoch: 5 [2600/13500]\tLoss: 4342.4561\n",
      "Training Epoch: 5 [2650/13500]\tLoss: 4246.6846\n",
      "Training Epoch: 5 [2700/13500]\tLoss: 4450.1567\n",
      "Training Epoch: 5 [2750/13500]\tLoss: 4213.8945\n",
      "Training Epoch: 5 [2800/13500]\tLoss: 4314.0449\n",
      "Training Epoch: 5 [2850/13500]\tLoss: 4328.1406\n",
      "Training Epoch: 5 [2900/13500]\tLoss: 4296.3179\n",
      "Training Epoch: 5 [2950/13500]\tLoss: 4286.3823\n",
      "Training Epoch: 5 [3000/13500]\tLoss: 4223.1245\n",
      "Training Epoch: 5 [3050/13500]\tLoss: 4180.9424\n",
      "Training Epoch: 5 [3100/13500]\tLoss: 4280.7021\n",
      "Training Epoch: 5 [3150/13500]\tLoss: 4385.2324\n",
      "Training Epoch: 5 [3200/13500]\tLoss: 4388.3228\n",
      "Training Epoch: 5 [3250/13500]\tLoss: 4456.1997\n",
      "Training Epoch: 5 [3300/13500]\tLoss: 4388.4927\n",
      "Training Epoch: 5 [3350/13500]\tLoss: 4319.1587\n",
      "Training Epoch: 5 [3400/13500]\tLoss: 4071.3284\n",
      "Training Epoch: 5 [3450/13500]\tLoss: 4264.8687\n",
      "Training Epoch: 5 [3500/13500]\tLoss: 4402.4771\n",
      "Training Epoch: 5 [3550/13500]\tLoss: 4367.8120\n",
      "Training Epoch: 5 [3600/13500]\tLoss: 4332.9355\n",
      "Training Epoch: 5 [3650/13500]\tLoss: 4354.2593\n",
      "Training Epoch: 5 [3700/13500]\tLoss: 4226.2075\n",
      "Training Epoch: 5 [3750/13500]\tLoss: 4371.1226\n",
      "Training Epoch: 5 [3800/13500]\tLoss: 4419.8120\n",
      "Training Epoch: 5 [3850/13500]\tLoss: 4345.8076\n",
      "Training Epoch: 5 [3900/13500]\tLoss: 4238.6221\n",
      "Training Epoch: 5 [3950/13500]\tLoss: 4300.9004\n",
      "Training Epoch: 5 [4000/13500]\tLoss: 4214.6338\n",
      "Training Epoch: 5 [4050/13500]\tLoss: 4292.2852\n",
      "Training Epoch: 5 [4100/13500]\tLoss: 4307.1572\n",
      "Training Epoch: 5 [4150/13500]\tLoss: 4445.3198\n",
      "Training Epoch: 5 [4200/13500]\tLoss: 4401.8301\n",
      "Training Epoch: 5 [4250/13500]\tLoss: 4264.2505\n",
      "Training Epoch: 5 [4300/13500]\tLoss: 4205.1729\n",
      "Training Epoch: 5 [4350/13500]\tLoss: 4180.8853\n",
      "Training Epoch: 5 [4400/13500]\tLoss: 4255.9424\n",
      "Training Epoch: 5 [4450/13500]\tLoss: 4257.0044\n",
      "Training Epoch: 5 [4500/13500]\tLoss: 4205.0420\n",
      "Training Epoch: 5 [4550/13500]\tLoss: 4165.2178\n",
      "Training Epoch: 5 [4600/13500]\tLoss: 4223.6943\n",
      "Training Epoch: 5 [4650/13500]\tLoss: 4390.7290\n",
      "Training Epoch: 5 [4700/13500]\tLoss: 4283.5068\n",
      "Training Epoch: 5 [4750/13500]\tLoss: 4291.1445\n",
      "Training Epoch: 5 [4800/13500]\tLoss: 4262.1108\n",
      "Training Epoch: 5 [4850/13500]\tLoss: 4230.3477\n",
      "Training Epoch: 5 [4900/13500]\tLoss: 4232.6177\n",
      "Training Epoch: 5 [4950/13500]\tLoss: 4111.5942\n",
      "Training Epoch: 5 [5000/13500]\tLoss: 4082.0205\n",
      "Training Epoch: 5 [5050/13500]\tLoss: 4180.5430\n",
      "Training Epoch: 5 [5100/13500]\tLoss: 4278.7544\n",
      "Training Epoch: 5 [5150/13500]\tLoss: 4140.4668\n",
      "Training Epoch: 5 [5200/13500]\tLoss: 4118.9595\n",
      "Training Epoch: 5 [5250/13500]\tLoss: 4194.4546\n",
      "Training Epoch: 5 [5300/13500]\tLoss: 4105.1401\n",
      "Training Epoch: 5 [5350/13500]\tLoss: 4148.6685\n",
      "Training Epoch: 5 [5400/13500]\tLoss: 4234.9263\n",
      "Training Epoch: 5 [5450/13500]\tLoss: 4169.7476\n",
      "Training Epoch: 5 [5500/13500]\tLoss: 4231.1050\n",
      "Training Epoch: 5 [5550/13500]\tLoss: 4053.5935\n",
      "Training Epoch: 5 [5600/13500]\tLoss: 4174.4023\n",
      "Training Epoch: 5 [5650/13500]\tLoss: 4231.4058\n",
      "Training Epoch: 5 [5700/13500]\tLoss: 4393.4336\n",
      "Training Epoch: 5 [5750/13500]\tLoss: 4268.2363\n",
      "Training Epoch: 5 [5800/13500]\tLoss: 4120.0923\n",
      "Training Epoch: 5 [5850/13500]\tLoss: 4057.7571\n",
      "Training Epoch: 5 [5900/13500]\tLoss: 4147.1304\n",
      "Training Epoch: 5 [5950/13500]\tLoss: 4220.1167\n",
      "Training Epoch: 5 [6000/13500]\tLoss: 4081.8604\n",
      "Training Epoch: 5 [6050/13500]\tLoss: 4119.6729\n",
      "Training Epoch: 5 [6100/13500]\tLoss: 4058.3862\n",
      "Training Epoch: 5 [6150/13500]\tLoss: 4075.6597\n",
      "Training Epoch: 5 [6200/13500]\tLoss: 4178.5215\n",
      "Training Epoch: 5 [6250/13500]\tLoss: 4166.2441\n",
      "Training Epoch: 5 [6300/13500]\tLoss: 4088.3633\n",
      "Training Epoch: 5 [6350/13500]\tLoss: 4078.2927\n",
      "Training Epoch: 5 [6400/13500]\tLoss: 4104.5278\n",
      "Training Epoch: 5 [6450/13500]\tLoss: 4110.7646\n",
      "Training Epoch: 5 [6500/13500]\tLoss: 4173.1631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [6550/13500]\tLoss: 4110.8745\n",
      "Training Epoch: 5 [6600/13500]\tLoss: 4037.9722\n",
      "Training Epoch: 5 [6650/13500]\tLoss: 4110.3022\n",
      "Training Epoch: 5 [6700/13500]\tLoss: 4097.7993\n",
      "Training Epoch: 5 [6750/13500]\tLoss: 4141.9155\n",
      "Training Epoch: 5 [6800/13500]\tLoss: 3999.5547\n",
      "Training Epoch: 5 [6850/13500]\tLoss: 4190.2319\n",
      "Training Epoch: 5 [6900/13500]\tLoss: 4098.1333\n",
      "Training Epoch: 5 [6950/13500]\tLoss: 4118.3008\n",
      "Training Epoch: 5 [7000/13500]\tLoss: 4003.8643\n",
      "Training Epoch: 5 [7050/13500]\tLoss: 4111.7832\n",
      "Training Epoch: 5 [7100/13500]\tLoss: 4197.3760\n",
      "Training Epoch: 5 [7150/13500]\tLoss: 4023.0078\n",
      "Training Epoch: 5 [7200/13500]\tLoss: 4049.1440\n",
      "Training Epoch: 5 [7250/13500]\tLoss: 4127.4409\n",
      "Training Epoch: 5 [7300/13500]\tLoss: 4058.2944\n",
      "Training Epoch: 5 [7350/13500]\tLoss: 4117.2500\n",
      "Training Epoch: 5 [7400/13500]\tLoss: 3988.1929\n",
      "Training Epoch: 5 [7450/13500]\tLoss: 4063.8752\n",
      "Training Epoch: 5 [7500/13500]\tLoss: 4077.6194\n",
      "Training Epoch: 5 [7550/13500]\tLoss: 4073.0208\n",
      "Training Epoch: 5 [7600/13500]\tLoss: 4086.4629\n",
      "Training Epoch: 5 [7650/13500]\tLoss: 4072.3762\n",
      "Training Epoch: 5 [7700/13500]\tLoss: 4043.0693\n",
      "Training Epoch: 5 [7750/13500]\tLoss: 4019.2542\n",
      "Training Epoch: 5 [7800/13500]\tLoss: 4068.4067\n",
      "Training Epoch: 5 [7850/13500]\tLoss: 4044.9404\n",
      "Training Epoch: 5 [7900/13500]\tLoss: 3961.8271\n",
      "Training Epoch: 5 [7950/13500]\tLoss: 4052.0361\n",
      "Training Epoch: 5 [8000/13500]\tLoss: 4158.0083\n",
      "Training Epoch: 5 [8050/13500]\tLoss: 4040.2158\n",
      "Training Epoch: 5 [8100/13500]\tLoss: 4079.8091\n",
      "Training Epoch: 5 [8150/13500]\tLoss: 4108.4014\n",
      "Training Epoch: 5 [8200/13500]\tLoss: 4020.9602\n",
      "Training Epoch: 5 [8250/13500]\tLoss: 4106.2119\n",
      "Training Epoch: 5 [8300/13500]\tLoss: 4023.4631\n",
      "Training Epoch: 5 [8350/13500]\tLoss: 4076.4114\n",
      "Training Epoch: 5 [8400/13500]\tLoss: 4074.4529\n",
      "Training Epoch: 5 [8450/13500]\tLoss: 4071.5476\n",
      "Training Epoch: 5 [8500/13500]\tLoss: 3997.6455\n",
      "Training Epoch: 5 [8550/13500]\tLoss: 3989.8296\n",
      "Training Epoch: 5 [8600/13500]\tLoss: 3988.3103\n",
      "Training Epoch: 5 [8650/13500]\tLoss: 4058.6550\n",
      "Training Epoch: 5 [8700/13500]\tLoss: 4045.5625\n",
      "Training Epoch: 5 [8750/13500]\tLoss: 3991.1211\n",
      "Training Epoch: 5 [8800/13500]\tLoss: 3976.5867\n",
      "Training Epoch: 5 [8850/13500]\tLoss: 3822.6597\n",
      "Training Epoch: 5 [8900/13500]\tLoss: 4008.6597\n",
      "Training Epoch: 5 [8950/13500]\tLoss: 4084.7395\n",
      "Training Epoch: 5 [9000/13500]\tLoss: 3911.3687\n",
      "Training Epoch: 5 [9050/13500]\tLoss: 3976.4309\n",
      "Training Epoch: 5 [9100/13500]\tLoss: 4081.2007\n",
      "Training Epoch: 5 [9150/13500]\tLoss: 4045.4109\n",
      "Training Epoch: 5 [9200/13500]\tLoss: 3925.4041\n",
      "Training Epoch: 5 [9250/13500]\tLoss: 4000.4189\n",
      "Training Epoch: 5 [9300/13500]\tLoss: 4086.7524\n",
      "Training Epoch: 5 [9350/13500]\tLoss: 4139.8096\n",
      "Training Epoch: 5 [9400/13500]\tLoss: 3939.3660\n",
      "Training Epoch: 5 [9450/13500]\tLoss: 3922.0117\n",
      "Training Epoch: 5 [9500/13500]\tLoss: 3989.5493\n",
      "Training Epoch: 5 [9550/13500]\tLoss: 4004.2148\n",
      "Training Epoch: 5 [9600/13500]\tLoss: 3980.9531\n",
      "Training Epoch: 5 [9650/13500]\tLoss: 3909.0291\n",
      "Training Epoch: 5 [9700/13500]\tLoss: 4083.8201\n",
      "Training Epoch: 5 [9750/13500]\tLoss: 4016.3389\n",
      "Training Epoch: 5 [9800/13500]\tLoss: 3877.2354\n",
      "Training Epoch: 5 [9850/13500]\tLoss: 3958.4675\n",
      "Training Epoch: 5 [9900/13500]\tLoss: 3840.0981\n",
      "Training Epoch: 5 [9950/13500]\tLoss: 4060.4580\n",
      "Training Epoch: 5 [10000/13500]\tLoss: 3947.2300\n",
      "Training Epoch: 5 [10050/13500]\tLoss: 3991.2480\n",
      "Training Epoch: 5 [10100/13500]\tLoss: 3952.9697\n",
      "Training Epoch: 5 [10150/13500]\tLoss: 4032.4722\n",
      "Training Epoch: 5 [10200/13500]\tLoss: 3905.1304\n",
      "Training Epoch: 5 [10250/13500]\tLoss: 3899.3293\n",
      "Training Epoch: 5 [10300/13500]\tLoss: 3890.5403\n",
      "Training Epoch: 5 [10350/13500]\tLoss: 3946.0220\n",
      "Training Epoch: 5 [10400/13500]\tLoss: 4056.2876\n",
      "Training Epoch: 5 [10450/13500]\tLoss: 3996.7275\n",
      "Training Epoch: 5 [10500/13500]\tLoss: 4013.0603\n",
      "Training Epoch: 5 [10550/13500]\tLoss: 4007.1589\n",
      "Training Epoch: 5 [10600/13500]\tLoss: 3998.9473\n",
      "Training Epoch: 5 [10650/13500]\tLoss: 3879.2488\n",
      "Training Epoch: 5 [10700/13500]\tLoss: 3982.0437\n",
      "Training Epoch: 5 [10750/13500]\tLoss: 3923.4392\n",
      "Training Epoch: 5 [10800/13500]\tLoss: 3921.0789\n",
      "Training Epoch: 5 [10850/13500]\tLoss: 3947.9609\n",
      "Training Epoch: 5 [10900/13500]\tLoss: 3852.6943\n",
      "Training Epoch: 5 [10950/13500]\tLoss: 3991.8032\n",
      "Training Epoch: 5 [11000/13500]\tLoss: 3940.7351\n",
      "Training Epoch: 5 [11050/13500]\tLoss: 4044.4885\n",
      "Training Epoch: 5 [11100/13500]\tLoss: 3894.0540\n",
      "Training Epoch: 5 [11150/13500]\tLoss: 3969.4629\n",
      "Training Epoch: 5 [11200/13500]\tLoss: 3906.6702\n",
      "Training Epoch: 5 [11250/13500]\tLoss: 3799.2683\n",
      "Training Epoch: 5 [11300/13500]\tLoss: 3891.2415\n",
      "Training Epoch: 5 [11350/13500]\tLoss: 3920.6584\n",
      "Training Epoch: 5 [11400/13500]\tLoss: 3952.9724\n",
      "Training Epoch: 5 [11450/13500]\tLoss: 3995.2568\n",
      "Training Epoch: 5 [11500/13500]\tLoss: 3810.3557\n",
      "Training Epoch: 5 [11550/13500]\tLoss: 3938.2703\n",
      "Training Epoch: 5 [11600/13500]\tLoss: 3854.9297\n",
      "Training Epoch: 5 [11650/13500]\tLoss: 3805.0408\n",
      "Training Epoch: 5 [11700/13500]\tLoss: 3874.8601\n",
      "Training Epoch: 5 [11750/13500]\tLoss: 3977.3315\n",
      "Training Epoch: 5 [11800/13500]\tLoss: 3934.2004\n",
      "Training Epoch: 5 [11850/13500]\tLoss: 3827.3274\n",
      "Training Epoch: 5 [11900/13500]\tLoss: 4005.2021\n",
      "Training Epoch: 5 [11950/13500]\tLoss: 3940.3513\n",
      "Training Epoch: 5 [12000/13500]\tLoss: 3772.1326\n",
      "Training Epoch: 5 [12050/13500]\tLoss: 3856.8384\n",
      "Training Epoch: 5 [12100/13500]\tLoss: 3967.7285\n",
      "Training Epoch: 5 [12150/13500]\tLoss: 3904.9282\n",
      "Training Epoch: 5 [12200/13500]\tLoss: 3855.5730\n",
      "Training Epoch: 5 [12250/13500]\tLoss: 3749.7998\n",
      "Training Epoch: 5 [12300/13500]\tLoss: 3937.6934\n",
      "Training Epoch: 5 [12350/13500]\tLoss: 3911.7319\n",
      "Training Epoch: 5 [12400/13500]\tLoss: 3846.8296\n",
      "Training Epoch: 5 [12450/13500]\tLoss: 3882.0669\n",
      "Training Epoch: 5 [12500/13500]\tLoss: 3984.9404\n",
      "Training Epoch: 5 [12550/13500]\tLoss: 3871.2771\n",
      "Training Epoch: 5 [12600/13500]\tLoss: 3870.1182\n",
      "Training Epoch: 5 [12650/13500]\tLoss: 3903.7180\n",
      "Training Epoch: 5 [12700/13500]\tLoss: 4013.1147\n",
      "Training Epoch: 5 [12750/13500]\tLoss: 3845.1907\n",
      "Training Epoch: 5 [12800/13500]\tLoss: 3851.3555\n",
      "Training Epoch: 5 [12850/13500]\tLoss: 3841.1101\n",
      "Training Epoch: 5 [12900/13500]\tLoss: 3870.7231\n",
      "Training Epoch: 5 [12950/13500]\tLoss: 3746.8088\n",
      "Training Epoch: 5 [13000/13500]\tLoss: 3825.5940\n",
      "Training Epoch: 5 [13050/13500]\tLoss: 3822.6260\n",
      "Training Epoch: 5 [13100/13500]\tLoss: 3690.6956\n",
      "Training Epoch: 5 [13150/13500]\tLoss: 3821.7529\n",
      "Training Epoch: 5 [13200/13500]\tLoss: 3798.9492\n",
      "Training Epoch: 5 [13250/13500]\tLoss: 3815.2876\n",
      "Training Epoch: 5 [13300/13500]\tLoss: 3742.5203\n",
      "Training Epoch: 5 [13350/13500]\tLoss: 3830.4512\n",
      "Training Epoch: 5 [13400/13500]\tLoss: 3738.7576\n",
      "Training Epoch: 5 [13450/13500]\tLoss: 3842.8220\n",
      "Training Epoch: 5 [13500/13500]\tLoss: 3918.3513\n",
      "Training Epoch: 5 [1499/1499]\tLoss: 3796.9742\n",
      "Training Epoch: 6 [50/13500]\tLoss: 3835.4641\n",
      "Training Epoch: 6 [100/13500]\tLoss: 3770.6340\n",
      "Training Epoch: 6 [150/13500]\tLoss: 3782.4814\n",
      "Training Epoch: 6 [200/13500]\tLoss: 3759.3677\n",
      "Training Epoch: 6 [250/13500]\tLoss: 3868.1707\n",
      "Training Epoch: 6 [300/13500]\tLoss: 3818.8191\n",
      "Training Epoch: 6 [350/13500]\tLoss: 3876.0281\n",
      "Training Epoch: 6 [400/13500]\tLoss: 3847.5879\n",
      "Training Epoch: 6 [450/13500]\tLoss: 3750.8850\n",
      "Training Epoch: 6 [500/13500]\tLoss: 3720.4258\n",
      "Training Epoch: 6 [550/13500]\tLoss: 3749.5537\n",
      "Training Epoch: 6 [600/13500]\tLoss: 3816.9041\n",
      "Training Epoch: 6 [650/13500]\tLoss: 3726.2197\n",
      "Training Epoch: 6 [700/13500]\tLoss: 3729.1143\n",
      "Training Epoch: 6 [750/13500]\tLoss: 3750.3806\n",
      "Training Epoch: 6 [800/13500]\tLoss: 3757.0642\n",
      "Training Epoch: 6 [850/13500]\tLoss: 3751.5732\n",
      "Training Epoch: 6 [900/13500]\tLoss: 3653.3523\n",
      "Training Epoch: 6 [950/13500]\tLoss: 3661.2539\n",
      "Training Epoch: 6 [1000/13500]\tLoss: 3700.2073\n",
      "Training Epoch: 6 [1050/13500]\tLoss: 3708.9661\n",
      "Training Epoch: 6 [1100/13500]\tLoss: 3800.4858\n",
      "Training Epoch: 6 [1150/13500]\tLoss: 3830.8730\n",
      "Training Epoch: 6 [1200/13500]\tLoss: 3694.9077\n",
      "Training Epoch: 6 [1250/13500]\tLoss: 3721.6318\n",
      "Training Epoch: 6 [1300/13500]\tLoss: 3624.4519\n",
      "Training Epoch: 6 [1350/13500]\tLoss: 3773.4429\n",
      "Training Epoch: 6 [1400/13500]\tLoss: 3735.4153\n",
      "Training Epoch: 6 [1450/13500]\tLoss: 3662.9639\n",
      "Training Epoch: 6 [1500/13500]\tLoss: 3877.3201\n",
      "Training Epoch: 6 [1550/13500]\tLoss: 3710.5884\n",
      "Training Epoch: 6 [1600/13500]\tLoss: 3798.1724\n",
      "Training Epoch: 6 [1650/13500]\tLoss: 3735.6179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [1700/13500]\tLoss: 3812.3789\n",
      "Training Epoch: 6 [1750/13500]\tLoss: 3829.4805\n",
      "Training Epoch: 6 [1800/13500]\tLoss: 3729.0361\n",
      "Training Epoch: 6 [1850/13500]\tLoss: 3657.5325\n",
      "Training Epoch: 6 [1900/13500]\tLoss: 3748.1575\n",
      "Training Epoch: 6 [1950/13500]\tLoss: 3520.3215\n",
      "Training Epoch: 6 [2000/13500]\tLoss: 3798.5417\n",
      "Training Epoch: 6 [2050/13500]\tLoss: 3658.4280\n",
      "Training Epoch: 6 [2100/13500]\tLoss: 3633.8513\n",
      "Training Epoch: 6 [2150/13500]\tLoss: 3694.9607\n",
      "Training Epoch: 6 [2200/13500]\tLoss: 3819.1248\n",
      "Training Epoch: 6 [2250/13500]\tLoss: 3684.9492\n",
      "Training Epoch: 6 [2300/13500]\tLoss: 3706.0637\n",
      "Training Epoch: 6 [2350/13500]\tLoss: 3640.6455\n",
      "Training Epoch: 6 [2400/13500]\tLoss: 3623.2773\n",
      "Training Epoch: 6 [2450/13500]\tLoss: 3780.4580\n",
      "Training Epoch: 6 [2500/13500]\tLoss: 3647.7466\n",
      "Training Epoch: 6 [2550/13500]\tLoss: 3732.2476\n",
      "Training Epoch: 6 [2600/13500]\tLoss: 3702.9678\n",
      "Training Epoch: 6 [2650/13500]\tLoss: 3644.2903\n",
      "Training Epoch: 6 [2700/13500]\tLoss: 3812.8975\n",
      "Training Epoch: 6 [2750/13500]\tLoss: 3607.5623\n",
      "Training Epoch: 6 [2800/13500]\tLoss: 3688.1042\n",
      "Training Epoch: 6 [2850/13500]\tLoss: 3695.6824\n",
      "Training Epoch: 6 [2900/13500]\tLoss: 3669.4468\n",
      "Training Epoch: 6 [2950/13500]\tLoss: 3659.8953\n",
      "Training Epoch: 6 [3000/13500]\tLoss: 3628.1213\n",
      "Training Epoch: 6 [3050/13500]\tLoss: 3576.9788\n",
      "Training Epoch: 6 [3100/13500]\tLoss: 3669.0139\n",
      "Training Epoch: 6 [3150/13500]\tLoss: 3737.5005\n",
      "Training Epoch: 6 [3200/13500]\tLoss: 3756.3447\n",
      "Training Epoch: 6 [3250/13500]\tLoss: 3814.7126\n",
      "Training Epoch: 6 [3300/13500]\tLoss: 3758.3579\n",
      "Training Epoch: 6 [3350/13500]\tLoss: 3695.8860\n",
      "Training Epoch: 6 [3400/13500]\tLoss: 3483.5056\n",
      "Training Epoch: 6 [3450/13500]\tLoss: 3644.0173\n",
      "Training Epoch: 6 [3500/13500]\tLoss: 3768.7458\n",
      "Training Epoch: 6 [3550/13500]\tLoss: 3732.0303\n",
      "Training Epoch: 6 [3600/13500]\tLoss: 3698.3462\n",
      "Training Epoch: 6 [3650/13500]\tLoss: 3739.4744\n",
      "Training Epoch: 6 [3700/13500]\tLoss: 3620.9971\n",
      "Training Epoch: 6 [3750/13500]\tLoss: 3765.9282\n",
      "Training Epoch: 6 [3800/13500]\tLoss: 3779.5376\n",
      "Training Epoch: 6 [3850/13500]\tLoss: 3726.0581\n",
      "Training Epoch: 6 [3900/13500]\tLoss: 3645.4690\n",
      "Training Epoch: 6 [3950/13500]\tLoss: 3692.2551\n",
      "Training Epoch: 6 [4000/13500]\tLoss: 3627.3843\n",
      "Training Epoch: 6 [4050/13500]\tLoss: 3679.9922\n",
      "Training Epoch: 6 [4100/13500]\tLoss: 3691.2847\n",
      "Training Epoch: 6 [4150/13500]\tLoss: 3811.1233\n",
      "Training Epoch: 6 [4200/13500]\tLoss: 3790.6709\n",
      "Training Epoch: 6 [4250/13500]\tLoss: 3670.6650\n",
      "Training Epoch: 6 [4300/13500]\tLoss: 3607.1487\n",
      "Training Epoch: 6 [4350/13500]\tLoss: 3569.0276\n",
      "Training Epoch: 6 [4400/13500]\tLoss: 3666.5288\n",
      "Training Epoch: 6 [4450/13500]\tLoss: 3642.6392\n",
      "Training Epoch: 6 [4500/13500]\tLoss: 3612.9019\n",
      "Training Epoch: 6 [4550/13500]\tLoss: 3585.9836\n",
      "Training Epoch: 6 [4600/13500]\tLoss: 3642.7207\n",
      "Training Epoch: 6 [4650/13500]\tLoss: 3775.5200\n",
      "Training Epoch: 6 [4700/13500]\tLoss: 3670.6802\n",
      "Training Epoch: 6 [4750/13500]\tLoss: 3689.1948\n",
      "Training Epoch: 6 [4800/13500]\tLoss: 3647.4360\n",
      "Training Epoch: 6 [4850/13500]\tLoss: 3623.4360\n",
      "Training Epoch: 6 [4900/13500]\tLoss: 3647.5825\n",
      "Training Epoch: 6 [4950/13500]\tLoss: 3536.1768\n",
      "Training Epoch: 6 [5000/13500]\tLoss: 3511.8811\n",
      "Training Epoch: 6 [5050/13500]\tLoss: 3585.1980\n",
      "Training Epoch: 6 [5100/13500]\tLoss: 3681.3464\n",
      "Training Epoch: 6 [5150/13500]\tLoss: 3557.3396\n",
      "Training Epoch: 6 [5200/13500]\tLoss: 3534.3896\n",
      "Training Epoch: 6 [5250/13500]\tLoss: 3600.9780\n",
      "Training Epoch: 6 [5300/13500]\tLoss: 3535.8020\n",
      "Training Epoch: 6 [5350/13500]\tLoss: 3558.6375\n",
      "Training Epoch: 6 [5400/13500]\tLoss: 3648.7014\n",
      "Training Epoch: 6 [5450/13500]\tLoss: 3599.9922\n",
      "Training Epoch: 6 [5500/13500]\tLoss: 3643.9873\n",
      "Training Epoch: 6 [5550/13500]\tLoss: 3492.8564\n",
      "Training Epoch: 6 [5600/13500]\tLoss: 3576.2036\n",
      "Training Epoch: 6 [5650/13500]\tLoss: 3648.0063\n",
      "Training Epoch: 6 [5700/13500]\tLoss: 3793.6824\n",
      "Training Epoch: 6 [5750/13500]\tLoss: 3676.9326\n",
      "Training Epoch: 6 [5800/13500]\tLoss: 3561.2139\n",
      "Training Epoch: 6 [5850/13500]\tLoss: 3486.3618\n",
      "Training Epoch: 6 [5900/13500]\tLoss: 3570.7083\n",
      "Training Epoch: 6 [5950/13500]\tLoss: 3639.6111\n",
      "Training Epoch: 6 [6000/13500]\tLoss: 3495.0254\n",
      "Training Epoch: 6 [6050/13500]\tLoss: 3543.0146\n",
      "Training Epoch: 6 [6100/13500]\tLoss: 3489.4426\n",
      "Training Epoch: 6 [6150/13500]\tLoss: 3518.2192\n",
      "Training Epoch: 6 [6200/13500]\tLoss: 3595.5554\n",
      "Training Epoch: 6 [6250/13500]\tLoss: 3614.2915\n",
      "Training Epoch: 6 [6300/13500]\tLoss: 3511.8726\n",
      "Training Epoch: 6 [6350/13500]\tLoss: 3512.8054\n",
      "Training Epoch: 6 [6400/13500]\tLoss: 3534.0010\n",
      "Training Epoch: 6 [6450/13500]\tLoss: 3540.6909\n",
      "Training Epoch: 6 [6500/13500]\tLoss: 3584.8362\n",
      "Training Epoch: 6 [6550/13500]\tLoss: 3531.2812\n",
      "Training Epoch: 6 [6600/13500]\tLoss: 3476.9341\n",
      "Training Epoch: 6 [6650/13500]\tLoss: 3531.6331\n",
      "Training Epoch: 6 [6700/13500]\tLoss: 3519.1982\n",
      "Training Epoch: 6 [6750/13500]\tLoss: 3581.3276\n",
      "Training Epoch: 6 [6800/13500]\tLoss: 3450.9915\n",
      "Training Epoch: 6 [6850/13500]\tLoss: 3612.3977\n",
      "Training Epoch: 6 [6900/13500]\tLoss: 3531.2712\n",
      "Training Epoch: 6 [6950/13500]\tLoss: 3556.8127\n",
      "Training Epoch: 6 [7000/13500]\tLoss: 3450.9365\n",
      "Training Epoch: 6 [7050/13500]\tLoss: 3539.1597\n",
      "Training Epoch: 6 [7100/13500]\tLoss: 3618.5989\n",
      "Training Epoch: 6 [7150/13500]\tLoss: 3460.6677\n",
      "Training Epoch: 6 [7200/13500]\tLoss: 3505.8459\n",
      "Training Epoch: 6 [7250/13500]\tLoss: 3563.5630\n",
      "Training Epoch: 6 [7300/13500]\tLoss: 3502.3811\n",
      "Training Epoch: 6 [7350/13500]\tLoss: 3534.1384\n",
      "Training Epoch: 6 [7400/13500]\tLoss: 3438.4685\n",
      "Training Epoch: 6 [7450/13500]\tLoss: 3520.8135\n",
      "Training Epoch: 6 [7500/13500]\tLoss: 3504.0520\n",
      "Training Epoch: 6 [7550/13500]\tLoss: 3522.4021\n",
      "Training Epoch: 6 [7600/13500]\tLoss: 3538.1479\n",
      "Training Epoch: 6 [7650/13500]\tLoss: 3527.2595\n",
      "Training Epoch: 6 [7700/13500]\tLoss: 3485.4734\n",
      "Training Epoch: 6 [7750/13500]\tLoss: 3470.7385\n",
      "Training Epoch: 6 [7800/13500]\tLoss: 3514.5378\n",
      "Training Epoch: 6 [7850/13500]\tLoss: 3482.2417\n",
      "Training Epoch: 6 [7900/13500]\tLoss: 3429.5945\n",
      "Training Epoch: 6 [7950/13500]\tLoss: 3491.4822\n",
      "Training Epoch: 6 [8000/13500]\tLoss: 3588.4702\n",
      "Training Epoch: 6 [8050/13500]\tLoss: 3489.2664\n",
      "Training Epoch: 6 [8100/13500]\tLoss: 3525.8147\n",
      "Training Epoch: 6 [8150/13500]\tLoss: 3545.5322\n",
      "Training Epoch: 6 [8200/13500]\tLoss: 3469.7117\n",
      "Training Epoch: 6 [8250/13500]\tLoss: 3534.6089\n",
      "Training Epoch: 6 [8300/13500]\tLoss: 3495.0955\n",
      "Training Epoch: 6 [8350/13500]\tLoss: 3517.2842\n",
      "Training Epoch: 6 [8400/13500]\tLoss: 3512.3384\n",
      "Training Epoch: 6 [8450/13500]\tLoss: 3534.9785\n",
      "Training Epoch: 6 [8500/13500]\tLoss: 3454.0857\n",
      "Training Epoch: 6 [8550/13500]\tLoss: 3436.4868\n",
      "Training Epoch: 6 [8600/13500]\tLoss: 3439.9187\n",
      "Training Epoch: 6 [8650/13500]\tLoss: 3512.1909\n",
      "Training Epoch: 6 [8700/13500]\tLoss: 3496.4570\n",
      "Training Epoch: 6 [8750/13500]\tLoss: 3456.5999\n",
      "Training Epoch: 6 [8800/13500]\tLoss: 3442.6790\n",
      "Training Epoch: 6 [8850/13500]\tLoss: 3311.4097\n",
      "Training Epoch: 6 [8900/13500]\tLoss: 3463.5776\n",
      "Training Epoch: 6 [8950/13500]\tLoss: 3528.1206\n",
      "Training Epoch: 6 [9000/13500]\tLoss: 3397.9397\n",
      "Training Epoch: 6 [9050/13500]\tLoss: 3451.5818\n",
      "Training Epoch: 6 [9100/13500]\tLoss: 3535.2205\n",
      "Training Epoch: 6 [9150/13500]\tLoss: 3504.8408\n",
      "Training Epoch: 6 [9200/13500]\tLoss: 3395.6997\n",
      "Training Epoch: 6 [9250/13500]\tLoss: 3466.4873\n",
      "Training Epoch: 6 [9300/13500]\tLoss: 3532.2109\n",
      "Training Epoch: 6 [9350/13500]\tLoss: 3572.6089\n",
      "Training Epoch: 6 [9400/13500]\tLoss: 3415.5725\n",
      "Training Epoch: 6 [9450/13500]\tLoss: 3409.3206\n",
      "Training Epoch: 6 [9500/13500]\tLoss: 3451.8894\n",
      "Training Epoch: 6 [9550/13500]\tLoss: 3469.6921\n",
      "Training Epoch: 6 [9600/13500]\tLoss: 3442.6182\n",
      "Training Epoch: 6 [9650/13500]\tLoss: 3387.0623\n",
      "Training Epoch: 6 [9700/13500]\tLoss: 3551.0193\n",
      "Training Epoch: 6 [9750/13500]\tLoss: 3473.8818\n",
      "Training Epoch: 6 [9800/13500]\tLoss: 3360.3267\n",
      "Training Epoch: 6 [9850/13500]\tLoss: 3431.9783\n",
      "Training Epoch: 6 [9900/13500]\tLoss: 3320.9331\n",
      "Training Epoch: 6 [9950/13500]\tLoss: 3518.0435\n",
      "Training Epoch: 6 [10000/13500]\tLoss: 3431.1003\n",
      "Training Epoch: 6 [10050/13500]\tLoss: 3454.5903\n",
      "Training Epoch: 6 [10100/13500]\tLoss: 3425.8438\n",
      "Training Epoch: 6 [10150/13500]\tLoss: 3509.7993\n",
      "Training Epoch: 6 [10200/13500]\tLoss: 3388.5386\n",
      "Training Epoch: 6 [10250/13500]\tLoss: 3389.2002\n",
      "Training Epoch: 6 [10300/13500]\tLoss: 3373.4111\n",
      "Training Epoch: 6 [10350/13500]\tLoss: 3427.9680\n",
      "Training Epoch: 6 [10400/13500]\tLoss: 3522.2549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [10450/13500]\tLoss: 3483.1567\n",
      "Training Epoch: 6 [10500/13500]\tLoss: 3479.0698\n",
      "Training Epoch: 6 [10550/13500]\tLoss: 3467.6013\n",
      "Training Epoch: 6 [10600/13500]\tLoss: 3468.2119\n",
      "Training Epoch: 6 [10650/13500]\tLoss: 3374.5662\n",
      "Training Epoch: 6 [10700/13500]\tLoss: 3454.2268\n",
      "Training Epoch: 6 [10750/13500]\tLoss: 3403.9863\n",
      "Training Epoch: 6 [10800/13500]\tLoss: 3403.3647\n",
      "Training Epoch: 6 [10850/13500]\tLoss: 3427.2820\n",
      "Training Epoch: 6 [10900/13500]\tLoss: 3335.2119\n",
      "Training Epoch: 6 [10950/13500]\tLoss: 3457.0420\n",
      "Training Epoch: 6 [11000/13500]\tLoss: 3428.3354\n",
      "Training Epoch: 6 [11050/13500]\tLoss: 3513.5417\n",
      "Training Epoch: 6 [11100/13500]\tLoss: 3373.1326\n",
      "Training Epoch: 6 [11150/13500]\tLoss: 3450.6829\n",
      "Training Epoch: 6 [11200/13500]\tLoss: 3392.4470\n",
      "Training Epoch: 6 [11250/13500]\tLoss: 3287.2629\n",
      "Training Epoch: 6 [11300/13500]\tLoss: 3386.7397\n",
      "Training Epoch: 6 [11350/13500]\tLoss: 3402.3979\n",
      "Training Epoch: 6 [11400/13500]\tLoss: 3421.6614\n",
      "Training Epoch: 6 [11450/13500]\tLoss: 3472.5671\n",
      "Training Epoch: 6 [11500/13500]\tLoss: 3314.6316\n",
      "Training Epoch: 6 [11550/13500]\tLoss: 3437.2834\n",
      "Training Epoch: 6 [11600/13500]\tLoss: 3352.9202\n",
      "Training Epoch: 6 [11650/13500]\tLoss: 3305.4785\n",
      "Training Epoch: 6 [11700/13500]\tLoss: 3373.7351\n",
      "Training Epoch: 6 [11750/13500]\tLoss: 3447.8376\n",
      "Training Epoch: 6 [11800/13500]\tLoss: 3418.9636\n",
      "Training Epoch: 6 [11850/13500]\tLoss: 3328.8950\n",
      "Training Epoch: 6 [11900/13500]\tLoss: 3477.5999\n",
      "Training Epoch: 6 [11950/13500]\tLoss: 3428.7004\n",
      "Training Epoch: 6 [12000/13500]\tLoss: 3288.7368\n",
      "Training Epoch: 6 [12050/13500]\tLoss: 3348.8335\n",
      "Training Epoch: 6 [12100/13500]\tLoss: 3455.1409\n",
      "Training Epoch: 6 [12150/13500]\tLoss: 3379.0647\n",
      "Training Epoch: 6 [12200/13500]\tLoss: 3339.7734\n",
      "Training Epoch: 6 [12250/13500]\tLoss: 3263.9280\n",
      "Training Epoch: 6 [12300/13500]\tLoss: 3420.7393\n",
      "Training Epoch: 6 [12350/13500]\tLoss: 3404.5554\n",
      "Training Epoch: 6 [12400/13500]\tLoss: 3341.0503\n",
      "Training Epoch: 6 [12450/13500]\tLoss: 3376.8821\n",
      "Training Epoch: 6 [12500/13500]\tLoss: 3475.8552\n",
      "Training Epoch: 6 [12550/13500]\tLoss: 3376.9373\n",
      "Training Epoch: 6 [12600/13500]\tLoss: 3373.3745\n",
      "Training Epoch: 6 [12650/13500]\tLoss: 3402.7515\n",
      "Training Epoch: 6 [12700/13500]\tLoss: 3507.3303\n",
      "Training Epoch: 6 [12750/13500]\tLoss: 3362.3835\n",
      "Training Epoch: 6 [12800/13500]\tLoss: 3356.7358\n",
      "Training Epoch: 6 [12850/13500]\tLoss: 3355.6934\n",
      "Training Epoch: 6 [12900/13500]\tLoss: 3365.1184\n",
      "Training Epoch: 6 [12950/13500]\tLoss: 3245.5430\n",
      "Training Epoch: 6 [13000/13500]\tLoss: 3337.3503\n",
      "Training Epoch: 6 [13050/13500]\tLoss: 3328.8831\n",
      "Training Epoch: 6 [13100/13500]\tLoss: 3211.5542\n",
      "Training Epoch: 6 [13150/13500]\tLoss: 3330.1768\n",
      "Training Epoch: 6 [13200/13500]\tLoss: 3305.1199\n",
      "Training Epoch: 6 [13250/13500]\tLoss: 3311.5396\n",
      "Training Epoch: 6 [13300/13500]\tLoss: 3263.3313\n",
      "Training Epoch: 6 [13350/13500]\tLoss: 3344.6543\n",
      "Training Epoch: 6 [13400/13500]\tLoss: 3247.8391\n",
      "Training Epoch: 6 [13450/13500]\tLoss: 3345.5964\n",
      "Training Epoch: 6 [13500/13500]\tLoss: 3409.2737\n",
      "Training Epoch: 6 [1499/1499]\tLoss: 3309.9706\n",
      "Training Epoch: 7 [50/13500]\tLoss: 3342.9976\n",
      "Training Epoch: 7 [100/13500]\tLoss: 3287.7410\n",
      "Training Epoch: 7 [150/13500]\tLoss: 3278.9246\n",
      "Training Epoch: 7 [200/13500]\tLoss: 3272.1218\n",
      "Training Epoch: 7 [250/13500]\tLoss: 3367.6160\n",
      "Training Epoch: 7 [300/13500]\tLoss: 3326.1506\n",
      "Training Epoch: 7 [350/13500]\tLoss: 3373.6880\n",
      "Training Epoch: 7 [400/13500]\tLoss: 3351.0911\n",
      "Training Epoch: 7 [450/13500]\tLoss: 3269.4312\n",
      "Training Epoch: 7 [500/13500]\tLoss: 3243.1558\n",
      "Training Epoch: 7 [550/13500]\tLoss: 3257.9690\n",
      "Training Epoch: 7 [600/13500]\tLoss: 3342.0532\n",
      "Training Epoch: 7 [650/13500]\tLoss: 3242.9199\n",
      "Training Epoch: 7 [700/13500]\tLoss: 3241.0200\n",
      "Training Epoch: 7 [750/13500]\tLoss: 3275.3191\n",
      "Training Epoch: 7 [800/13500]\tLoss: 3282.6340\n",
      "Training Epoch: 7 [850/13500]\tLoss: 3276.2361\n",
      "Training Epoch: 7 [900/13500]\tLoss: 3188.6880\n",
      "Training Epoch: 7 [950/13500]\tLoss: 3201.1750\n",
      "Training Epoch: 7 [1000/13500]\tLoss: 3215.8782\n",
      "Training Epoch: 7 [1050/13500]\tLoss: 3232.9941\n",
      "Training Epoch: 7 [1100/13500]\tLoss: 3332.0376\n",
      "Training Epoch: 7 [1150/13500]\tLoss: 3345.8735\n",
      "Training Epoch: 7 [1200/13500]\tLoss: 3231.6082\n",
      "Training Epoch: 7 [1250/13500]\tLoss: 3243.6604\n",
      "Training Epoch: 7 [1300/13500]\tLoss: 3150.4543\n",
      "Training Epoch: 7 [1350/13500]\tLoss: 3304.4395\n",
      "Training Epoch: 7 [1400/13500]\tLoss: 3252.4814\n",
      "Training Epoch: 7 [1450/13500]\tLoss: 3197.7512\n",
      "Training Epoch: 7 [1500/13500]\tLoss: 3383.6057\n",
      "Training Epoch: 7 [1550/13500]\tLoss: 3251.0525\n",
      "Training Epoch: 7 [1600/13500]\tLoss: 3326.2888\n",
      "Training Epoch: 7 [1650/13500]\tLoss: 3255.5354\n",
      "Training Epoch: 7 [1700/13500]\tLoss: 3345.0725\n",
      "Training Epoch: 7 [1750/13500]\tLoss: 3361.0667\n",
      "Training Epoch: 7 [1800/13500]\tLoss: 3261.6782\n",
      "Training Epoch: 7 [1850/13500]\tLoss: 3190.2808\n",
      "Training Epoch: 7 [1900/13500]\tLoss: 3264.4421\n",
      "Training Epoch: 7 [1950/13500]\tLoss: 3084.5674\n",
      "Training Epoch: 7 [2000/13500]\tLoss: 3320.2715\n",
      "Training Epoch: 7 [2050/13500]\tLoss: 3194.1897\n",
      "Training Epoch: 7 [2100/13500]\tLoss: 3175.7185\n",
      "Training Epoch: 7 [2150/13500]\tLoss: 3237.1135\n",
      "Training Epoch: 7 [2200/13500]\tLoss: 3337.3926\n",
      "Training Epoch: 7 [2250/13500]\tLoss: 3229.2964\n",
      "Training Epoch: 7 [2300/13500]\tLoss: 3232.9866\n",
      "Training Epoch: 7 [2350/13500]\tLoss: 3193.9634\n",
      "Training Epoch: 7 [2400/13500]\tLoss: 3170.8181\n",
      "Training Epoch: 7 [2450/13500]\tLoss: 3316.0701\n",
      "Training Epoch: 7 [2500/13500]\tLoss: 3183.9644\n",
      "Training Epoch: 7 [2550/13500]\tLoss: 3251.6401\n",
      "Training Epoch: 7 [2600/13500]\tLoss: 3233.5208\n",
      "Training Epoch: 7 [2650/13500]\tLoss: 3200.0417\n",
      "Training Epoch: 7 [2700/13500]\tLoss: 3342.5793\n",
      "Training Epoch: 7 [2750/13500]\tLoss: 3161.7322\n",
      "Training Epoch: 7 [2800/13500]\tLoss: 3228.0510\n",
      "Training Epoch: 7 [2850/13500]\tLoss: 3232.2825\n",
      "Training Epoch: 7 [2900/13500]\tLoss: 3207.5991\n",
      "Training Epoch: 7 [2950/13500]\tLoss: 3199.4741\n",
      "Training Epoch: 7 [3000/13500]\tLoss: 3190.0942\n",
      "Training Epoch: 7 [3050/13500]\tLoss: 3134.5481\n",
      "Training Epoch: 7 [3100/13500]\tLoss: 3219.4380\n",
      "Training Epoch: 7 [3150/13500]\tLoss: 3261.1965\n",
      "Training Epoch: 7 [3200/13500]\tLoss: 3288.5085\n",
      "Training Epoch: 7 [3250/13500]\tLoss: 3341.2764\n",
      "Training Epoch: 7 [3300/13500]\tLoss: 3293.0032\n",
      "Training Epoch: 7 [3350/13500]\tLoss: 3238.5229\n",
      "Training Epoch: 7 [3400/13500]\tLoss: 3052.3733\n",
      "Training Epoch: 7 [3450/13500]\tLoss: 3187.2363\n",
      "Training Epoch: 7 [3500/13500]\tLoss: 3300.2249\n",
      "Training Epoch: 7 [3550/13500]\tLoss: 3263.6357\n",
      "Training Epoch: 7 [3600/13500]\tLoss: 3231.9714\n",
      "Training Epoch: 7 [3650/13500]\tLoss: 3283.8945\n",
      "Training Epoch: 7 [3700/13500]\tLoss: 3175.1877\n",
      "Training Epoch: 7 [3750/13500]\tLoss: 3315.6660\n",
      "Training Epoch: 7 [3800/13500]\tLoss: 3306.6995\n",
      "Training Epoch: 7 [3850/13500]\tLoss: 3268.2102\n",
      "Training Epoch: 7 [3900/13500]\tLoss: 3204.8376\n",
      "Training Epoch: 7 [3950/13500]\tLoss: 3242.1365\n",
      "Training Epoch: 7 [4000/13500]\tLoss: 3191.2014\n",
      "Training Epoch: 7 [4050/13500]\tLoss: 3227.3093\n",
      "Training Epoch: 7 [4100/13500]\tLoss: 3234.6802\n",
      "Training Epoch: 7 [4150/13500]\tLoss: 3340.9021\n",
      "Training Epoch: 7 [4200/13500]\tLoss: 3334.4211\n",
      "Training Epoch: 7 [4250/13500]\tLoss: 3230.1658\n",
      "Training Epoch: 7 [4300/13500]\tLoss: 3166.8904\n",
      "Training Epoch: 7 [4350/13500]\tLoss: 3121.1360\n",
      "Training Epoch: 7 [4400/13500]\tLoss: 3229.6560\n",
      "Training Epoch: 7 [4450/13500]\tLoss: 3189.9260\n",
      "Training Epoch: 7 [4500/13500]\tLoss: 3172.2883\n",
      "Training Epoch: 7 [4550/13500]\tLoss: 3154.9258\n",
      "Training Epoch: 7 [4600/13500]\tLoss: 3210.3455\n",
      "Training Epoch: 7 [4650/13500]\tLoss: 3316.2454\n",
      "Training Epoch: 7 [4700/13500]\tLoss: 3217.3772\n",
      "Training Epoch: 7 [4750/13500]\tLoss: 3241.2898\n",
      "Training Epoch: 7 [4800/13500]\tLoss: 3193.2432\n",
      "Training Epoch: 7 [4850/13500]\tLoss: 3175.8130\n",
      "Training Epoch: 7 [4900/13500]\tLoss: 3212.0488\n",
      "Training Epoch: 7 [4950/13500]\tLoss: 3108.7480\n",
      "Training Epoch: 7 [5000/13500]\tLoss: 3089.8889\n",
      "Training Epoch: 7 [5050/13500]\tLoss: 3144.0818\n",
      "Training Epoch: 7 [5100/13500]\tLoss: 3236.2195\n",
      "Training Epoch: 7 [5150/13500]\tLoss: 3124.2161\n",
      "Training Epoch: 7 [5200/13500]\tLoss: 3101.9336\n",
      "Training Epoch: 7 [5250/13500]\tLoss: 3161.3149\n",
      "Training Epoch: 7 [5300/13500]\tLoss: 3114.4539\n",
      "Training Epoch: 7 [5350/13500]\tLoss: 3123.3105\n",
      "Training Epoch: 7 [5400/13500]\tLoss: 3211.9265\n",
      "Training Epoch: 7 [5450/13500]\tLoss: 3174.8770\n",
      "Training Epoch: 7 [5500/13500]\tLoss: 3206.0320\n",
      "Training Epoch: 7 [5550/13500]\tLoss: 3077.5164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [5600/13500]\tLoss: 3134.7749\n",
      "Training Epoch: 7 [5650/13500]\tLoss: 3214.6594\n",
      "Training Epoch: 7 [5700/13500]\tLoss: 3344.2153\n",
      "Training Epoch: 7 [5750/13500]\tLoss: 3236.0688\n",
      "Training Epoch: 7 [5800/13500]\tLoss: 3144.4275\n",
      "Training Epoch: 7 [5850/13500]\tLoss: 3064.8567\n",
      "Training Epoch: 7 [5900/13500]\tLoss: 3143.1597\n",
      "Training Epoch: 7 [5950/13500]\tLoss: 3206.9753\n",
      "Training Epoch: 7 [6000/13500]\tLoss: 3062.0396\n",
      "Training Epoch: 7 [6050/13500]\tLoss: 3115.2231\n",
      "Training Epoch: 7 [6100/13500]\tLoss: 3068.0603\n",
      "Training Epoch: 7 [6150/13500]\tLoss: 3101.8411\n",
      "Training Epoch: 7 [6200/13500]\tLoss: 3162.2935\n",
      "Training Epoch: 7 [6250/13500]\tLoss: 3198.2915\n",
      "Training Epoch: 7 [6300/13500]\tLoss: 3085.4949\n",
      "Training Epoch: 7 [6350/13500]\tLoss: 3092.4819\n",
      "Training Epoch: 7 [6400/13500]\tLoss: 3109.8000\n",
      "Training Epoch: 7 [6450/13500]\tLoss: 3115.4666\n",
      "Training Epoch: 7 [6500/13500]\tLoss: 3147.6550\n",
      "Training Epoch: 7 [6550/13500]\tLoss: 3102.5620\n",
      "Training Epoch: 7 [6600/13500]\tLoss: 3061.4917\n",
      "Training Epoch: 7 [6650/13500]\tLoss: 3101.6899\n",
      "Training Epoch: 7 [6700/13500]\tLoss: 3089.8071\n",
      "Training Epoch: 7 [6750/13500]\tLoss: 3161.2661\n",
      "Training Epoch: 7 [6800/13500]\tLoss: 3042.1541\n",
      "Training Epoch: 7 [6850/13500]\tLoss: 3180.8711\n",
      "Training Epoch: 7 [6900/13500]\tLoss: 3109.9634\n",
      "Training Epoch: 7 [6950/13500]\tLoss: 3137.2917\n",
      "Training Epoch: 7 [7000/13500]\tLoss: 3038.7549\n",
      "Training Epoch: 7 [7050/13500]\tLoss: 3112.1475\n",
      "Training Epoch: 7 [7100/13500]\tLoss: 3186.3840\n",
      "Training Epoch: 7 [7150/13500]\tLoss: 3043.4187\n",
      "Training Epoch: 7 [7200/13500]\tLoss: 3097.4937\n",
      "Training Epoch: 7 [7250/13500]\tLoss: 3141.3657\n",
      "Training Epoch: 7 [7300/13500]\tLoss: 3088.3406\n",
      "Training Epoch: 7 [7350/13500]\tLoss: 3100.8926\n",
      "Training Epoch: 7 [7400/13500]\tLoss: 3029.3611\n",
      "Training Epoch: 7 [7450/13500]\tLoss: 3114.8770\n",
      "Training Epoch: 7 [7500/13500]\tLoss: 3078.1626\n",
      "Training Epoch: 7 [7550/13500]\tLoss: 3110.6875\n",
      "Training Epoch: 7 [7600/13500]\tLoss: 3126.5151\n",
      "Training Epoch: 7 [7650/13500]\tLoss: 3118.9316\n",
      "Training Epoch: 7 [7700/13500]\tLoss: 3071.6914\n",
      "Training Epoch: 7 [7750/13500]\tLoss: 3060.9683\n",
      "Training Epoch: 7 [7800/13500]\tLoss: 3101.0679\n",
      "Training Epoch: 7 [7850/13500]\tLoss: 3064.3389\n",
      "Training Epoch: 7 [7900/13500]\tLoss: 3031.6201\n",
      "Training Epoch: 7 [7950/13500]\tLoss: 3073.8845\n",
      "Training Epoch: 7 [8000/13500]\tLoss: 3162.4429\n",
      "Training Epoch: 7 [8050/13500]\tLoss: 3076.0359\n",
      "Training Epoch: 7 [8100/13500]\tLoss: 3110.7273\n",
      "Training Epoch: 7 [8150/13500]\tLoss: 3123.6948\n",
      "Training Epoch: 7 [8200/13500]\tLoss: 3058.0881\n",
      "Training Epoch: 7 [8250/13500]\tLoss: 3108.6860\n",
      "Training Epoch: 7 [8300/13500]\tLoss: 3097.1750\n",
      "Training Epoch: 7 [8350/13500]\tLoss: 3099.2949\n",
      "Training Epoch: 7 [8400/13500]\tLoss: 3092.1733\n",
      "Training Epoch: 7 [8450/13500]\tLoss: 3129.0166\n",
      "Training Epoch: 7 [8500/13500]\tLoss: 3047.8301\n",
      "Training Epoch: 7 [8550/13500]\tLoss: 3023.6272\n",
      "Training Epoch: 7 [8600/13500]\tLoss: 3031.8848\n",
      "Training Epoch: 7 [8650/13500]\tLoss: 3101.1672\n",
      "Training Epoch: 7 [8700/13500]\tLoss: 3084.7517\n",
      "Training Epoch: 7 [8750/13500]\tLoss: 3054.6389\n",
      "Training Epoch: 7 [8800/13500]\tLoss: 3042.0195\n",
      "Training Epoch: 7 [8850/13500]\tLoss: 2928.8630\n",
      "Training Epoch: 7 [8900/13500]\tLoss: 3055.0203\n",
      "Training Epoch: 7 [8950/13500]\tLoss: 3110.3035\n",
      "Training Epoch: 7 [9000/13500]\tLoss: 3013.0981\n",
      "Training Epoch: 7 [9050/13500]\tLoss: 3056.6584\n",
      "Training Epoch: 7 [9100/13500]\tLoss: 3124.4478\n",
      "Training Epoch: 7 [9150/13500]\tLoss: 3099.2434\n",
      "Training Epoch: 7 [9200/13500]\tLoss: 2998.0547\n",
      "Training Epoch: 7 [9250/13500]\tLoss: 3064.0225\n",
      "Training Epoch: 7 [9300/13500]\tLoss: 3115.0632\n",
      "Training Epoch: 7 [9350/13500]\tLoss: 3147.1523\n",
      "Training Epoch: 7 [9400/13500]\tLoss: 3021.8572\n",
      "Training Epoch: 7 [9450/13500]\tLoss: 3022.3206\n",
      "Training Epoch: 7 [9500/13500]\tLoss: 3046.4001\n",
      "Training Epoch: 7 [9550/13500]\tLoss: 3067.6187\n",
      "Training Epoch: 7 [9600/13500]\tLoss: 3039.2856\n",
      "Training Epoch: 7 [9650/13500]\tLoss: 2994.7725\n",
      "Training Epoch: 7 [9700/13500]\tLoss: 3147.5693\n",
      "Training Epoch: 7 [9750/13500]\tLoss: 3065.5271\n",
      "Training Epoch: 7 [9800/13500]\tLoss: 2972.5952\n",
      "Training Epoch: 7 [9850/13500]\tLoss: 3034.7102\n",
      "Training Epoch: 7 [9900/13500]\tLoss: 2933.2805\n",
      "Training Epoch: 7 [9950/13500]\tLoss: 3110.6147\n",
      "Training Epoch: 7 [10000/13500]\tLoss: 3042.0601\n",
      "Training Epoch: 7 [10050/13500]\tLoss: 3051.4128\n",
      "Training Epoch: 7 [10100/13500]\tLoss: 3029.6362\n",
      "Training Epoch: 7 [10150/13500]\tLoss: 3111.6172\n",
      "Training Epoch: 7 [10200/13500]\tLoss: 2999.6594\n",
      "Training Epoch: 7 [10250/13500]\tLoss: 3004.2551\n",
      "Training Epoch: 7 [10300/13500]\tLoss: 2984.8096\n",
      "Training Epoch: 7 [10350/13500]\tLoss: 3037.1289\n",
      "Training Epoch: 7 [10400/13500]\tLoss: 3118.3396\n",
      "Training Epoch: 7 [10450/13500]\tLoss: 3093.1533\n",
      "Training Epoch: 7 [10500/13500]\tLoss: 3077.0549\n",
      "Training Epoch: 7 [10550/13500]\tLoss: 3061.6592\n",
      "Training Epoch: 7 [10600/13500]\tLoss: 3067.7617\n",
      "Training Epoch: 7 [10650/13500]\tLoss: 2993.9114\n",
      "Training Epoch: 7 [10700/13500]\tLoss: 3055.8567\n",
      "Training Epoch: 7 [10750/13500]\tLoss: 3011.9580\n",
      "Training Epoch: 7 [10800/13500]\tLoss: 3012.6545\n",
      "Training Epoch: 7 [10850/13500]\tLoss: 3034.6267\n",
      "Training Epoch: 7 [10900/13500]\tLoss: 2947.2136\n",
      "Training Epoch: 7 [10950/13500]\tLoss: 3053.8779\n",
      "Training Epoch: 7 [11000/13500]\tLoss: 3040.8994\n",
      "Training Epoch: 7 [11050/13500]\tLoss: 3112.1892\n",
      "Training Epoch: 7 [11100/13500]\tLoss: 2981.5388\n",
      "Training Epoch: 7 [11150/13500]\tLoss: 3058.5125\n",
      "Training Epoch: 7 [11200/13500]\tLoss: 3004.9658\n",
      "Training Epoch: 7 [11250/13500]\tLoss: 2904.7808\n",
      "Training Epoch: 7 [11300/13500]\tLoss: 3004.6008\n",
      "Training Epoch: 7 [11350/13500]\tLoss: 3011.6321\n",
      "Training Epoch: 7 [11400/13500]\tLoss: 3020.6748\n",
      "Training Epoch: 7 [11450/13500]\tLoss: 3076.4495\n",
      "Training Epoch: 7 [11500/13500]\tLoss: 2939.8120\n",
      "Training Epoch: 7 [11550/13500]\tLoss: 3056.2927\n",
      "Training Epoch: 7 [11600/13500]\tLoss: 2974.6699\n",
      "Training Epoch: 7 [11650/13500]\tLoss: 2928.6736\n",
      "Training Epoch: 7 [11700/13500]\tLoss: 2993.5869\n",
      "Training Epoch: 7 [11750/13500]\tLoss: 3048.4568\n",
      "Training Epoch: 7 [11800/13500]\tLoss: 3029.5430\n",
      "Training Epoch: 7 [11850/13500]\tLoss: 2953.2524\n",
      "Training Epoch: 7 [11900/13500]\tLoss: 3078.6467\n",
      "Training Epoch: 7 [11950/13500]\tLoss: 3041.4678\n",
      "Training Epoch: 7 [12000/13500]\tLoss: 2922.9880\n",
      "Training Epoch: 7 [12050/13500]\tLoss: 2964.7361\n",
      "Training Epoch: 7 [12100/13500]\tLoss: 3065.1865\n",
      "Training Epoch: 7 [12150/13500]\tLoss: 2983.7581\n",
      "Training Epoch: 7 [12200/13500]\tLoss: 2951.0334\n",
      "Training Epoch: 7 [12250/13500]\tLoss: 2897.6729\n",
      "Training Epoch: 7 [12300/13500]\tLoss: 3030.3064\n",
      "Training Epoch: 7 [12350/13500]\tLoss: 3019.3938\n",
      "Training Epoch: 7 [12400/13500]\tLoss: 2958.5486\n",
      "Training Epoch: 7 [12450/13500]\tLoss: 2993.4954\n",
      "Training Epoch: 7 [12500/13500]\tLoss: 3087.6226\n",
      "Training Epoch: 7 [12550/13500]\tLoss: 3001.7632\n",
      "Training Epoch: 7 [12600/13500]\tLoss: 2994.4133\n",
      "Training Epoch: 7 [12650/13500]\tLoss: 3022.7725\n",
      "Training Epoch: 7 [12700/13500]\tLoss: 3121.2251\n",
      "Training Epoch: 7 [12750/13500]\tLoss: 2994.4043\n",
      "Training Epoch: 7 [12800/13500]\tLoss: 2982.2651\n",
      "Training Epoch: 7 [12850/13500]\tLoss: 2987.1194\n",
      "Training Epoch: 7 [12900/13500]\tLoss: 2983.6807\n",
      "Training Epoch: 7 [12950/13500]\tLoss: 2869.5840\n",
      "Training Epoch: 7 [13000/13500]\tLoss: 2966.8091\n",
      "Training Epoch: 7 [13050/13500]\tLoss: 2954.2061\n",
      "Training Epoch: 7 [13100/13500]\tLoss: 2849.9521\n",
      "Training Epoch: 7 [13150/13500]\tLoss: 2958.0520\n",
      "Training Epoch: 7 [13200/13500]\tLoss: 2931.4436\n",
      "Training Epoch: 7 [13250/13500]\tLoss: 2931.1072\n",
      "Training Epoch: 7 [13300/13500]\tLoss: 2900.4380\n",
      "Training Epoch: 7 [13350/13500]\tLoss: 2974.7400\n",
      "Training Epoch: 7 [13400/13500]\tLoss: 2878.3547\n",
      "Training Epoch: 7 [13450/13500]\tLoss: 2967.1028\n",
      "Training Epoch: 7 [13500/13500]\tLoss: 3023.0825\n",
      "Training Epoch: 7 [1499/1499]\tLoss: 2940.3431\n",
      "Training Epoch: 8 [50/13500]\tLoss: 2969.1587\n",
      "Training Epoch: 8 [100/13500]\tLoss: 2921.0154\n",
      "Training Epoch: 8 [150/13500]\tLoss: 2899.4260\n",
      "Training Epoch: 8 [200/13500]\tLoss: 2903.1899\n",
      "Training Epoch: 8 [250/13500]\tLoss: 2988.6289\n",
      "Training Epoch: 8 [300/13500]\tLoss: 2951.9041\n",
      "Training Epoch: 8 [350/13500]\tLoss: 2991.3059\n",
      "Training Epoch: 8 [400/13500]\tLoss: 2974.4983\n",
      "Training Epoch: 8 [450/13500]\tLoss: 2903.7419\n",
      "Training Epoch: 8 [500/13500]\tLoss: 2881.6123\n",
      "Training Epoch: 8 [550/13500]\tLoss: 2885.9480\n",
      "Training Epoch: 8 [600/13500]\tLoss: 2979.4790\n",
      "Training Epoch: 8 [650/13500]\tLoss: 2876.8386\n",
      "Training Epoch: 8 [700/13500]\tLoss: 2873.7307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [750/13500]\tLoss: 2914.4460\n",
      "Training Epoch: 8 [800/13500]\tLoss: 2919.9141\n",
      "Training Epoch: 8 [850/13500]\tLoss: 2914.8894\n",
      "Training Epoch: 8 [900/13500]\tLoss: 2835.6111\n",
      "Training Epoch: 8 [950/13500]\tLoss: 2850.0459\n",
      "Training Epoch: 8 [1000/13500]\tLoss: 2849.8923\n",
      "Training Epoch: 8 [1050/13500]\tLoss: 2872.5471\n",
      "Training Epoch: 8 [1100/13500]\tLoss: 2973.6951\n",
      "Training Epoch: 8 [1150/13500]\tLoss: 2976.3347\n",
      "Training Epoch: 8 [1200/13500]\tLoss: 2879.9827\n",
      "Training Epoch: 8 [1250/13500]\tLoss: 2881.1873\n",
      "Training Epoch: 8 [1300/13500]\tLoss: 2793.7527\n",
      "Training Epoch: 8 [1350/13500]\tLoss: 2946.9922\n",
      "Training Epoch: 8 [1400/13500]\tLoss: 2887.1960\n",
      "Training Epoch: 8 [1450/13500]\tLoss: 2844.9265\n",
      "Training Epoch: 8 [1500/13500]\tLoss: 3007.7148\n",
      "Training Epoch: 8 [1550/13500]\tLoss: 2900.2876\n",
      "Training Epoch: 8 [1600/13500]\tLoss: 2964.5676\n",
      "Training Epoch: 8 [1650/13500]\tLoss: 2891.9753\n",
      "Training Epoch: 8 [1700/13500]\tLoss: 2985.9829\n",
      "Training Epoch: 8 [1750/13500]\tLoss: 3000.7246\n",
      "Training Epoch: 8 [1800/13500]\tLoss: 2904.8157\n",
      "Training Epoch: 8 [1850/13500]\tLoss: 2835.6855\n",
      "Training Epoch: 8 [1900/13500]\tLoss: 2897.7288\n",
      "Training Epoch: 8 [1950/13500]\tLoss: 2752.8792\n",
      "Training Epoch: 8 [2000/13500]\tLoss: 2954.1033\n",
      "Training Epoch: 8 [2050/13500]\tLoss: 2841.4189\n",
      "Training Epoch: 8 [2100/13500]\tLoss: 2828.5002\n",
      "Training Epoch: 8 [2150/13500]\tLoss: 2886.4141\n",
      "Training Epoch: 8 [2200/13500]\tLoss: 2968.5728\n",
      "Training Epoch: 8 [2250/13500]\tLoss: 2882.0886\n",
      "Training Epoch: 8 [2300/13500]\tLoss: 2874.3252\n",
      "Training Epoch: 8 [2350/13500]\tLoss: 2852.3362\n",
      "Training Epoch: 8 [2400/13500]\tLoss: 2826.5691\n",
      "Training Epoch: 8 [2450/13500]\tLoss: 2959.5640\n",
      "Training Epoch: 8 [2500/13500]\tLoss: 2831.2458\n",
      "Training Epoch: 8 [2550/13500]\tLoss: 2886.6077\n",
      "Training Epoch: 8 [2600/13500]\tLoss: 2876.1575\n",
      "Training Epoch: 8 [2650/13500]\tLoss: 2859.6577\n",
      "Training Epoch: 8 [2700/13500]\tLoss: 2981.8926\n",
      "Training Epoch: 8 [2750/13500]\tLoss: 2822.2695\n",
      "Training Epoch: 8 [2800/13500]\tLoss: 2877.3005\n",
      "Training Epoch: 8 [2850/13500]\tLoss: 2879.2698\n",
      "Training Epoch: 8 [2900/13500]\tLoss: 2855.0669\n",
      "Training Epoch: 8 [2950/13500]\tLoss: 2848.7725\n",
      "Training Epoch: 8 [3000/13500]\tLoss: 2854.4395\n",
      "Training Epoch: 8 [3050/13500]\tLoss: 2797.3406\n",
      "Training Epoch: 8 [3100/13500]\tLoss: 2876.3345\n",
      "Training Epoch: 8 [3150/13500]\tLoss: 2898.1355\n",
      "Training Epoch: 8 [3200/13500]\tLoss: 2929.6421\n",
      "Training Epoch: 8 [3250/13500]\tLoss: 2978.5129\n",
      "Training Epoch: 8 [3300/13500]\tLoss: 2936.2183\n",
      "Training Epoch: 8 [3350/13500]\tLoss: 2889.5005\n",
      "Training Epoch: 8 [3400/13500]\tLoss: 2724.9961\n",
      "Training Epoch: 8 [3450/13500]\tLoss: 2839.2654\n",
      "Training Epoch: 8 [3500/13500]\tLoss: 2941.0049\n",
      "Training Epoch: 8 [3550/13500]\tLoss: 2906.0701\n",
      "Training Epoch: 8 [3600/13500]\tLoss: 2876.3357\n",
      "Training Epoch: 8 [3650/13500]\tLoss: 2933.9216\n",
      "Training Epoch: 8 [3700/13500]\tLoss: 2834.7759\n",
      "Training Epoch: 8 [3750/13500]\tLoss: 2967.8804\n",
      "Training Epoch: 8 [3800/13500]\tLoss: 2944.9810\n",
      "Training Epoch: 8 [3850/13500]\tLoss: 2917.0996\n",
      "Training Epoch: 8 [3900/13500]\tLoss: 2866.6382\n",
      "Training Epoch: 8 [3950/13500]\tLoss: 2896.4807\n",
      "Training Epoch: 8 [4000/13500]\tLoss: 2855.0955\n",
      "Training Epoch: 8 [4050/13500]\tLoss: 2879.7515\n",
      "Training Epoch: 8 [4100/13500]\tLoss: 2885.2554\n",
      "Training Epoch: 8 [4150/13500]\tLoss: 2979.4500\n",
      "Training Epoch: 8 [4200/13500]\tLoss: 2982.1589\n",
      "Training Epoch: 8 [4250/13500]\tLoss: 2891.2039\n",
      "Training Epoch: 8 [4300/13500]\tLoss: 2831.1875\n",
      "Training Epoch: 8 [4350/13500]\tLoss: 2781.0737\n",
      "Training Epoch: 8 [4400/13500]\tLoss: 2893.9292\n",
      "Training Epoch: 8 [4450/13500]\tLoss: 2844.4792\n",
      "Training Epoch: 8 [4500/13500]\tLoss: 2833.9883\n",
      "Training Epoch: 8 [4550/13500]\tLoss: 2823.5552\n",
      "Training Epoch: 8 [4600/13500]\tLoss: 2877.5959\n",
      "Training Epoch: 8 [4650/13500]\tLoss: 2961.1130\n",
      "Training Epoch: 8 [4700/13500]\tLoss: 2870.1978\n",
      "Training Epoch: 8 [4750/13500]\tLoss: 2896.8953\n",
      "Training Epoch: 8 [4800/13500]\tLoss: 2846.4302\n",
      "Training Epoch: 8 [4850/13500]\tLoss: 2834.3552\n",
      "Training Epoch: 8 [4900/13500]\tLoss: 2876.1462\n",
      "Training Epoch: 8 [4950/13500]\tLoss: 2780.8506\n",
      "Training Epoch: 8 [5000/13500]\tLoss: 2766.0188\n",
      "Training Epoch: 8 [5050/13500]\tLoss: 2806.6394\n",
      "Training Epoch: 8 [5100/13500]\tLoss: 2893.4224\n",
      "Training Epoch: 8 [5150/13500]\tLoss: 2792.1169\n",
      "Training Epoch: 8 [5200/13500]\tLoss: 2770.5266\n",
      "Training Epoch: 8 [5250/13500]\tLoss: 2824.1116\n",
      "Training Epoch: 8 [5300/13500]\tLoss: 2791.8650\n",
      "Training Epoch: 8 [5350/13500]\tLoss: 2790.4636\n",
      "Training Epoch: 8 [5400/13500]\tLoss: 2874.9426\n",
      "Training Epoch: 8 [5450/13500]\tLoss: 2846.5613\n",
      "Training Epoch: 8 [5500/13500]\tLoss: 2868.4897\n",
      "Training Epoch: 8 [5550/13500]\tLoss: 2759.5083\n",
      "Training Epoch: 8 [5600/13500]\tLoss: 2798.2073\n",
      "Training Epoch: 8 [5650/13500]\tLoss: 2881.7271\n",
      "Training Epoch: 8 [5700/13500]\tLoss: 2995.7031\n",
      "Training Epoch: 8 [5750/13500]\tLoss: 2896.5837\n",
      "Training Epoch: 8 [5800/13500]\tLoss: 2822.9680\n",
      "Training Epoch: 8 [5850/13500]\tLoss: 2743.5020\n",
      "Training Epoch: 8 [5900/13500]\tLoss: 2814.8779\n",
      "Training Epoch: 8 [5950/13500]\tLoss: 2873.6458\n",
      "Training Epoch: 8 [6000/13500]\tLoss: 2732.1577\n",
      "Training Epoch: 8 [6050/13500]\tLoss: 2787.2717\n",
      "Training Epoch: 8 [6100/13500]\tLoss: 2746.3977\n",
      "Training Epoch: 8 [6150/13500]\tLoss: 2781.0708\n",
      "Training Epoch: 8 [6200/13500]\tLoss: 2829.0950\n",
      "Training Epoch: 8 [6250/13500]\tLoss: 2875.4333\n",
      "Training Epoch: 8 [6300/13500]\tLoss: 2759.5669\n",
      "Training Epoch: 8 [6350/13500]\tLoss: 2770.3391\n",
      "Training Epoch: 8 [6400/13500]\tLoss: 2784.2153\n",
      "Training Epoch: 8 [6450/13500]\tLoss: 2788.7529\n",
      "Training Epoch: 8 [6500/13500]\tLoss: 2812.0288\n",
      "Training Epoch: 8 [6550/13500]\tLoss: 2775.0811\n",
      "Training Epoch: 8 [6600/13500]\tLoss: 2743.6951\n",
      "Training Epoch: 8 [6650/13500]\tLoss: 2772.1519\n",
      "Training Epoch: 8 [6700/13500]\tLoss: 2761.5706\n",
      "Training Epoch: 8 [6750/13500]\tLoss: 2836.1870\n",
      "Training Epoch: 8 [6800/13500]\tLoss: 2727.9673\n",
      "Training Epoch: 8 [6850/13500]\tLoss: 2847.9868\n",
      "Training Epoch: 8 [6900/13500]\tLoss: 2785.8997\n",
      "Training Epoch: 8 [6950/13500]\tLoss: 2814.3899\n",
      "Training Epoch: 8 [7000/13500]\tLoss: 2722.7722\n",
      "Training Epoch: 8 [7050/13500]\tLoss: 2783.7803\n",
      "Training Epoch: 8 [7100/13500]\tLoss: 2853.2681\n",
      "Training Epoch: 8 [7150/13500]\tLoss: 2724.2607\n",
      "Training Epoch: 8 [7200/13500]\tLoss: 2781.7444\n",
      "Training Epoch: 8 [7250/13500]\tLoss: 2815.8291\n",
      "Training Epoch: 8 [7300/13500]\tLoss: 2770.2180\n",
      "Training Epoch: 8 [7350/13500]\tLoss: 2769.2424\n",
      "Training Epoch: 8 [7400/13500]\tLoss: 2716.2581\n",
      "Training Epoch: 8 [7450/13500]\tLoss: 2800.9324\n",
      "Training Epoch: 8 [7500/13500]\tLoss: 2752.1763\n",
      "Training Epoch: 8 [7550/13500]\tLoss: 2793.9202\n",
      "Training Epoch: 8 [7600/13500]\tLoss: 2808.2886\n",
      "Training Epoch: 8 [7650/13500]\tLoss: 2803.7900\n",
      "Training Epoch: 8 [7700/13500]\tLoss: 2754.8330\n",
      "Training Epoch: 8 [7750/13500]\tLoss: 2746.4683\n",
      "Training Epoch: 8 [7800/13500]\tLoss: 2782.9810\n",
      "Training Epoch: 8 [7850/13500]\tLoss: 2744.6011\n",
      "Training Epoch: 8 [7900/13500]\tLoss: 2725.0945\n",
      "Training Epoch: 8 [7950/13500]\tLoss: 2753.4543\n",
      "Training Epoch: 8 [8000/13500]\tLoss: 2834.0896\n",
      "Training Epoch: 8 [8050/13500]\tLoss: 2757.4019\n",
      "Training Epoch: 8 [8100/13500]\tLoss: 2790.6924\n",
      "Training Epoch: 8 [8150/13500]\tLoss: 2798.6431\n",
      "Training Epoch: 8 [8200/13500]\tLoss: 2742.1624\n",
      "Training Epoch: 8 [8250/13500]\tLoss: 2781.8811\n",
      "Training Epoch: 8 [8300/13500]\tLoss: 2788.2507\n",
      "Training Epoch: 8 [8350/13500]\tLoss: 2777.4148\n",
      "Training Epoch: 8 [8400/13500]\tLoss: 2768.9324\n",
      "Training Epoch: 8 [8450/13500]\tLoss: 2813.1907\n",
      "Training Epoch: 8 [8500/13500]\tLoss: 2735.5479\n",
      "Training Epoch: 8 [8550/13500]\tLoss: 2707.0146\n",
      "Training Epoch: 8 [8600/13500]\tLoss: 2718.8091\n",
      "Training Epoch: 8 [8650/13500]\tLoss: 2783.4622\n",
      "Training Epoch: 8 [8700/13500]\tLoss: 2767.3169\n",
      "Training Epoch: 8 [8750/13500]\tLoss: 2743.7703\n",
      "Training Epoch: 8 [8800/13500]\tLoss: 2733.1853\n",
      "Training Epoch: 8 [8850/13500]\tLoss: 2634.4697\n",
      "Training Epoch: 8 [8900/13500]\tLoss: 2740.3953\n",
      "Training Epoch: 8 [8950/13500]\tLoss: 2787.4448\n",
      "Training Epoch: 8 [9000/13500]\tLoss: 2715.6836\n",
      "Training Epoch: 8 [9050/13500]\tLoss: 2750.7549\n",
      "Training Epoch: 8 [9100/13500]\tLoss: 2805.8528\n",
      "Training Epoch: 8 [9150/13500]\tLoss: 2786.4807\n",
      "Training Epoch: 8 [9200/13500]\tLoss: 2691.9470\n",
      "Training Epoch: 8 [9250/13500]\tLoss: 2752.1763\n",
      "Training Epoch: 8 [9300/13500]\tLoss: 2792.3088\n",
      "Training Epoch: 8 [9350/13500]\tLoss: 2818.7173\n",
      "Training Epoch: 8 [9400/13500]\tLoss: 2717.7632\n",
      "Training Epoch: 8 [9450/13500]\tLoss: 2722.3037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [9500/13500]\tLoss: 2732.5061\n",
      "Training Epoch: 8 [9550/13500]\tLoss: 2756.7817\n",
      "Training Epoch: 8 [9600/13500]\tLoss: 2728.4631\n",
      "Training Epoch: 8 [9650/13500]\tLoss: 2691.5305\n",
      "Training Epoch: 8 [9700/13500]\tLoss: 2833.3174\n",
      "Training Epoch: 8 [9750/13500]\tLoss: 2749.9414\n",
      "Training Epoch: 8 [9800/13500]\tLoss: 2674.2620\n",
      "Training Epoch: 8 [9850/13500]\tLoss: 2727.5176\n",
      "Training Epoch: 8 [9900/13500]\tLoss: 2636.2253\n",
      "Training Epoch: 8 [9950/13500]\tLoss: 2796.4951\n",
      "Training Epoch: 8 [10000/13500]\tLoss: 2741.1013\n",
      "Training Epoch: 8 [10050/13500]\tLoss: 2740.6272\n",
      "Training Epoch: 8 [10100/13500]\tLoss: 2724.1431\n",
      "Training Epoch: 8 [10150/13500]\tLoss: 2800.3333\n",
      "Training Epoch: 8 [10200/13500]\tLoss: 2698.8389\n",
      "Training Epoch: 8 [10250/13500]\tLoss: 2705.9980\n",
      "Training Epoch: 8 [10300/13500]\tLoss: 2684.8994\n",
      "Training Epoch: 8 [10350/13500]\tLoss: 2734.2063\n",
      "Training Epoch: 8 [10400/13500]\tLoss: 2804.5647\n",
      "Training Epoch: 8 [10450/13500]\tLoss: 2789.0359\n",
      "Training Epoch: 8 [10500/13500]\tLoss: 2765.9126\n",
      "Training Epoch: 8 [10550/13500]\tLoss: 2747.7180\n",
      "Training Epoch: 8 [10600/13500]\tLoss: 2757.6792\n",
      "Training Epoch: 8 [10650/13500]\tLoss: 2699.0615\n",
      "Training Epoch: 8 [10700/13500]\tLoss: 2747.1445\n",
      "Training Epoch: 8 [10750/13500]\tLoss: 2708.2974\n",
      "Training Epoch: 8 [10800/13500]\tLoss: 2709.9170\n",
      "Training Epoch: 8 [10850/13500]\tLoss: 2731.2495\n",
      "Training Epoch: 8 [10900/13500]\tLoss: 2648.6184\n",
      "Training Epoch: 8 [10950/13500]\tLoss: 2741.9946\n",
      "Training Epoch: 8 [11000/13500]\tLoss: 2740.1360\n",
      "Training Epoch: 8 [11050/13500]\tLoss: 2800.5283\n",
      "Training Epoch: 8 [11100/13500]\tLoss: 2679.8743\n",
      "Training Epoch: 8 [11150/13500]\tLoss: 2754.7517\n",
      "Training Epoch: 8 [11200/13500]\tLoss: 2704.7869\n",
      "Training Epoch: 8 [11250/13500]\tLoss: 2611.9597\n",
      "Training Epoch: 8 [11300/13500]\tLoss: 2707.4646\n",
      "Training Epoch: 8 [11350/13500]\tLoss: 2709.2747\n",
      "Training Epoch: 8 [11400/13500]\tLoss: 2710.8196\n",
      "Training Epoch: 8 [11450/13500]\tLoss: 2768.4915\n",
      "Training Epoch: 8 [11500/13500]\tLoss: 2649.5154\n",
      "Training Epoch: 8 [11550/13500]\tLoss: 2758.6504\n",
      "Training Epoch: 8 [11600/13500]\tLoss: 2682.0288\n",
      "Training Epoch: 8 [11650/13500]\tLoss: 2637.5383\n",
      "Training Epoch: 8 [11700/13500]\tLoss: 2698.1372\n",
      "Training Epoch: 8 [11750/13500]\tLoss: 2739.4099\n",
      "Training Epoch: 8 [11800/13500]\tLoss: 2727.1541\n",
      "Training Epoch: 8 [11850/13500]\tLoss: 2662.2173\n",
      "Training Epoch: 8 [11900/13500]\tLoss: 2768.9412\n",
      "Training Epoch: 8 [11950/13500]\tLoss: 2740.8164\n",
      "Training Epoch: 8 [12000/13500]\tLoss: 2639.0693\n",
      "Training Epoch: 8 [12050/13500]\tLoss: 2667.5168\n",
      "Training Epoch: 8 [12100/13500]\tLoss: 2760.8550\n",
      "Training Epoch: 8 [12150/13500]\tLoss: 2679.3005\n",
      "Training Epoch: 8 [12200/13500]\tLoss: 2651.2102\n",
      "Training Epoch: 8 [12250/13500]\tLoss: 2614.5344\n",
      "Training Epoch: 8 [12300/13500]\tLoss: 2727.3225\n",
      "Training Epoch: 8 [12350/13500]\tLoss: 2720.0574\n",
      "Training Epoch: 8 [12400/13500]\tLoss: 2662.7415\n",
      "Training Epoch: 8 [12450/13500]\tLoss: 2695.3254\n",
      "Training Epoch: 8 [12500/13500]\tLoss: 2784.2476\n",
      "Training Epoch: 8 [12550/13500]\tLoss: 2709.9429\n",
      "Training Epoch: 8 [12600/13500]\tLoss: 2698.6675\n",
      "Training Epoch: 8 [12650/13500]\tLoss: 2727.0347\n",
      "Training Epoch: 8 [12700/13500]\tLoss: 2818.2766\n",
      "Training Epoch: 8 [12750/13500]\tLoss: 2706.6938\n",
      "Training Epoch: 8 [12800/13500]\tLoss: 2691.6658\n",
      "Training Epoch: 8 [12850/13500]\tLoss: 2699.9480\n",
      "Training Epoch: 8 [12900/13500]\tLoss: 2688.5347\n",
      "Training Epoch: 8 [12950/13500]\tLoss: 2580.8901\n",
      "Training Epoch: 8 [13000/13500]\tLoss: 2678.5459\n",
      "Training Epoch: 8 [13050/13500]\tLoss: 2662.9727\n",
      "Training Epoch: 8 [13100/13500]\tLoss: 2570.5320\n",
      "Training Epoch: 8 [13150/13500]\tLoss: 2669.4697\n",
      "Training Epoch: 8 [13200/13500]\tLoss: 2641.7905\n",
      "Training Epoch: 8 [13250/13500]\tLoss: 2637.3477\n",
      "Training Epoch: 8 [13300/13500]\tLoss: 2618.9292\n",
      "Training Epoch: 8 [13350/13500]\tLoss: 2686.5940\n",
      "Training Epoch: 8 [13400/13500]\tLoss: 2593.8425\n",
      "Training Epoch: 8 [13450/13500]\tLoss: 2672.6970\n",
      "Training Epoch: 8 [13500/13500]\tLoss: 2722.9094\n",
      "Training Epoch: 8 [1499/1499]\tLoss: 2653.1000\n",
      "Training Epoch: 9 [50/13500]\tLoss: 2678.3828\n",
      "Training Epoch: 9 [100/13500]\tLoss: 2635.7913\n",
      "Training Epoch: 9 [150/13500]\tLoss: 2606.9924\n",
      "Training Epoch: 9 [200/13500]\tLoss: 2617.3826\n",
      "Training Epoch: 9 [250/13500]\tLoss: 2694.4458\n",
      "Training Epoch: 9 [300/13500]\tLoss: 2661.1562\n",
      "Training Epoch: 9 [350/13500]\tLoss: 2693.8975\n",
      "Training Epoch: 9 [400/13500]\tLoss: 2681.9861\n",
      "Training Epoch: 9 [450/13500]\tLoss: 2620.1182\n",
      "Training Epoch: 9 [500/13500]\tLoss: 2601.3093\n",
      "Training Epoch: 9 [550/13500]\tLoss: 2598.1885\n",
      "Training Epoch: 9 [600/13500]\tLoss: 2695.9866\n",
      "Training Epoch: 9 [650/13500]\tLoss: 2593.1013\n",
      "Training Epoch: 9 [700/13500]\tLoss: 2590.4446\n",
      "Training Epoch: 9 [750/13500]\tLoss: 2633.6299\n",
      "Training Epoch: 9 [800/13500]\tLoss: 2636.6726\n",
      "Training Epoch: 9 [850/13500]\tLoss: 2634.0779\n",
      "Training Epoch: 9 [900/13500]\tLoss: 2561.5835\n",
      "Training Epoch: 9 [950/13500]\tLoss: 2576.6033\n",
      "Training Epoch: 9 [1000/13500]\tLoss: 2567.3843\n",
      "Training Epoch: 9 [1050/13500]\tLoss: 2593.3162\n",
      "Training Epoch: 9 [1100/13500]\tLoss: 2692.7778\n",
      "Training Epoch: 9 [1150/13500]\tLoss: 2688.1311\n",
      "Training Epoch: 9 [1200/13500]\tLoss: 2606.3318\n",
      "Training Epoch: 9 [1250/13500]\tLoss: 2600.2764\n",
      "Training Epoch: 9 [1300/13500]\tLoss: 2519.2961\n",
      "Training Epoch: 9 [1350/13500]\tLoss: 2667.5317\n",
      "Training Epoch: 9 [1400/13500]\tLoss: 2604.4570\n",
      "Training Epoch: 9 [1450/13500]\tLoss: 2571.2175\n",
      "Training Epoch: 9 [1500/13500]\tLoss: 2715.0771\n",
      "Training Epoch: 9 [1550/13500]\tLoss: 2626.6113\n",
      "Training Epoch: 9 [1600/13500]\tLoss: 2680.9368\n",
      "Training Epoch: 9 [1650/13500]\tLoss: 2611.0259\n",
      "Training Epoch: 9 [1700/13500]\tLoss: 2703.5010\n",
      "Training Epoch: 9 [1750/13500]\tLoss: 2717.6755\n",
      "Training Epoch: 9 [1800/13500]\tLoss: 2626.5681\n",
      "Training Epoch: 9 [1850/13500]\tLoss: 2560.5469\n",
      "Training Epoch: 9 [1900/13500]\tLoss: 2613.2927\n",
      "Training Epoch: 9 [1950/13500]\tLoss: 2494.5295\n",
      "Training Epoch: 9 [2000/13500]\tLoss: 2667.5056\n",
      "Training Epoch: 9 [2050/13500]\tLoss: 2567.2122\n",
      "Training Epoch: 9 [2100/13500]\tLoss: 2559.4744\n",
      "Training Epoch: 9 [2150/13500]\tLoss: 2611.9336\n",
      "Training Epoch: 9 [2200/13500]\tLoss: 2680.2681\n",
      "Training Epoch: 9 [2250/13500]\tLoss: 2611.2314\n",
      "Training Epoch: 9 [2300/13500]\tLoss: 2596.3989\n",
      "Training Epoch: 9 [2350/13500]\tLoss: 2584.8867\n",
      "Training Epoch: 9 [2400/13500]\tLoss: 2558.6501\n",
      "Training Epoch: 9 [2450/13500]\tLoss: 2679.4666\n",
      "Training Epoch: 9 [2500/13500]\tLoss: 2557.4170\n",
      "Training Epoch: 9 [2550/13500]\tLoss: 2603.5535\n",
      "Training Epoch: 9 [2600/13500]\tLoss: 2598.3083\n",
      "Training Epoch: 9 [2650/13500]\tLoss: 2593.0542\n",
      "Training Epoch: 9 [2700/13500]\tLoss: 2698.9966\n",
      "Training Epoch: 9 [2750/13500]\tLoss: 2558.1577\n",
      "Training Epoch: 9 [2800/13500]\tLoss: 2603.8918\n",
      "Training Epoch: 9 [2850/13500]\tLoss: 2603.9790\n",
      "Training Epoch: 9 [2900/13500]\tLoss: 2580.1650\n",
      "Training Epoch: 9 [2950/13500]\tLoss: 2575.8093\n",
      "Training Epoch: 9 [3000/13500]\tLoss: 2591.0725\n",
      "Training Epoch: 9 [3050/13500]\tLoss: 2534.3279\n",
      "Training Epoch: 9 [3100/13500]\tLoss: 2608.3835\n",
      "Training Epoch: 9 [3150/13500]\tLoss: 2615.4092\n",
      "Training Epoch: 9 [3200/13500]\tLoss: 2648.7393\n",
      "Training Epoch: 9 [3250/13500]\tLoss: 2694.1731\n",
      "Training Epoch: 9 [3300/13500]\tLoss: 2656.4556\n",
      "Training Epoch: 9 [3350/13500]\tLoss: 2616.7766\n",
      "Training Epoch: 9 [3400/13500]\tLoss: 2471.0872\n",
      "Training Epoch: 9 [3450/13500]\tLoss: 2568.5540\n",
      "Training Epoch: 9 [3500/13500]\tLoss: 2659.6072\n",
      "Training Epoch: 9 [3550/13500]\tLoss: 2627.1882\n",
      "Training Epoch: 9 [3600/13500]\tLoss: 2599.4016\n",
      "Training Epoch: 9 [3650/13500]\tLoss: 2659.1909\n",
      "Training Epoch: 9 [3700/13500]\tLoss: 2569.0903\n",
      "Training Epoch: 9 [3750/13500]\tLoss: 2692.6079\n",
      "Training Epoch: 9 [3800/13500]\tLoss: 2662.3142\n",
      "Training Epoch: 9 [3850/13500]\tLoss: 2641.5942\n",
      "Training Epoch: 9 [3900/13500]\tLoss: 2601.4326\n",
      "Training Epoch: 9 [3950/13500]\tLoss: 2624.9368\n",
      "Training Epoch: 9 [4000/13500]\tLoss: 2590.3335\n",
      "Training Epoch: 9 [4050/13500]\tLoss: 2606.8135\n",
      "Training Epoch: 9 [4100/13500]\tLoss: 2612.2920\n",
      "Training Epoch: 9 [4150/13500]\tLoss: 2695.4785\n",
      "Training Epoch: 9 [4200/13500]\tLoss: 2704.0820\n",
      "Training Epoch: 9 [4250/13500]\tLoss: 2624.5237\n",
      "Training Epoch: 9 [4300/13500]\tLoss: 2569.3042\n",
      "Training Epoch: 9 [4350/13500]\tLoss: 2516.8542\n",
      "Training Epoch: 9 [4400/13500]\tLoss: 2629.8718\n",
      "Training Epoch: 9 [4450/13500]\tLoss: 2574.9937\n",
      "Training Epoch: 9 [4500/13500]\tLoss: 2568.9946\n",
      "Training Epoch: 9 [4550/13500]\tLoss: 2563.5686\n",
      "Training Epoch: 9 [4600/13500]\tLoss: 2615.6997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [4650/13500]\tLoss: 2680.6389\n",
      "Training Epoch: 9 [4700/13500]\tLoss: 2598.5574\n",
      "Training Epoch: 9 [4750/13500]\tLoss: 2626.4741\n",
      "Training Epoch: 9 [4800/13500]\tLoss: 2575.9080\n",
      "Training Epoch: 9 [4850/13500]\tLoss: 2568.3779\n",
      "Training Epoch: 9 [4900/13500]\tLoss: 2611.6873\n",
      "Training Epoch: 9 [4950/13500]\tLoss: 2524.0774\n",
      "Training Epoch: 9 [5000/13500]\tLoss: 2511.9617\n",
      "Training Epoch: 9 [5050/13500]\tLoss: 2543.3062\n",
      "Training Epoch: 9 [5100/13500]\tLoss: 2623.7937\n",
      "Training Epoch: 9 [5150/13500]\tLoss: 2532.1721\n",
      "Training Epoch: 9 [5200/13500]\tLoss: 2511.4619\n",
      "Training Epoch: 9 [5250/13500]\tLoss: 2559.8687\n",
      "Training Epoch: 9 [5300/13500]\tLoss: 2539.1655\n",
      "Training Epoch: 9 [5350/13500]\tLoss: 2530.5051\n",
      "Training Epoch: 9 [5400/13500]\tLoss: 2608.9524\n",
      "Training Epoch: 9 [5450/13500]\tLoss: 2587.4231\n",
      "Training Epoch: 9 [5500/13500]\tLoss: 2602.7664\n",
      "Training Epoch: 9 [5550/13500]\tLoss: 2510.4644\n",
      "Training Epoch: 9 [5600/13500]\tLoss: 2536.1560\n",
      "Training Epoch: 9 [5650/13500]\tLoss: 2619.7312\n",
      "Training Epoch: 9 [5700/13500]\tLoss: 2719.6907\n",
      "Training Epoch: 9 [5750/13500]\tLoss: 2629.4338\n",
      "Training Epoch: 9 [5800/13500]\tLoss: 2569.4631\n",
      "Training Epoch: 9 [5850/13500]\tLoss: 2493.1687\n",
      "Training Epoch: 9 [5900/13500]\tLoss: 2557.1643\n",
      "Training Epoch: 9 [5950/13500]\tLoss: 2611.1931\n",
      "Training Epoch: 9 [6000/13500]\tLoss: 2475.4890\n",
      "Training Epoch: 9 [6050/13500]\tLoss: 2530.5073\n",
      "Training Epoch: 9 [6100/13500]\tLoss: 2495.6675\n",
      "Training Epoch: 9 [6150/13500]\tLoss: 2528.5264\n",
      "Training Epoch: 9 [6200/13500]\tLoss: 2567.3542\n",
      "Training Epoch: 9 [6250/13500]\tLoss: 2619.3677\n",
      "Training Epoch: 9 [6300/13500]\tLoss: 2504.7991\n",
      "Training Epoch: 9 [6350/13500]\tLoss: 2518.2131\n",
      "Training Epoch: 9 [6400/13500]\tLoss: 2528.7878\n",
      "Training Epoch: 9 [6450/13500]\tLoss: 2532.4780\n",
      "Training Epoch: 9 [6500/13500]\tLoss: 2548.8918\n",
      "Training Epoch: 9 [6550/13500]\tLoss: 2519.2075\n",
      "Training Epoch: 9 [6600/13500]\tLoss: 2495.1055\n",
      "Training Epoch: 9 [6650/13500]\tLoss: 2514.4253\n",
      "Training Epoch: 9 [6700/13500]\tLoss: 2505.3367\n",
      "Training Epoch: 9 [6750/13500]\tLoss: 2579.2913\n",
      "Training Epoch: 9 [6800/13500]\tLoss: 2481.4050\n",
      "Training Epoch: 9 [6850/13500]\tLoss: 2585.4504\n",
      "Training Epoch: 9 [6900/13500]\tLoss: 2530.7866\n",
      "Training Epoch: 9 [6950/13500]\tLoss: 2560.2317\n",
      "Training Epoch: 9 [7000/13500]\tLoss: 2475.7297\n",
      "Training Epoch: 9 [7050/13500]\tLoss: 2525.8914\n",
      "Training Epoch: 9 [7100/13500]\tLoss: 2590.7988\n",
      "Training Epoch: 9 [7150/13500]\tLoss: 2475.0938\n",
      "Training Epoch: 9 [7200/13500]\tLoss: 2532.6096\n",
      "Training Epoch: 9 [7250/13500]\tLoss: 2559.5896\n",
      "Training Epoch: 9 [7300/13500]\tLoss: 2520.1558\n",
      "Training Epoch: 9 [7350/13500]\tLoss: 2510.1057\n",
      "Training Epoch: 9 [7400/13500]\tLoss: 2471.5486\n",
      "Training Epoch: 9 [7450/13500]\tLoss: 2552.1624\n",
      "Training Epoch: 9 [7500/13500]\tLoss: 2497.3735\n",
      "Training Epoch: 9 [7550/13500]\tLoss: 2544.9204\n",
      "Training Epoch: 9 [7600/13500]\tLoss: 2556.6721\n",
      "Training Epoch: 9 [7650/13500]\tLoss: 2554.7417\n",
      "Training Epoch: 9 [7700/13500]\tLoss: 2506.7239\n",
      "Training Epoch: 9 [7750/13500]\tLoss: 2499.9270\n",
      "Training Epoch: 9 [7800/13500]\tLoss: 2532.8254\n",
      "Training Epoch: 9 [7850/13500]\tLoss: 2494.3994\n",
      "Training Epoch: 9 [7900/13500]\tLoss: 2483.5525\n",
      "Training Epoch: 9 [7950/13500]\tLoss: 2502.4446\n",
      "Training Epoch: 9 [8000/13500]\tLoss: 2575.4055\n",
      "Training Epoch: 9 [8050/13500]\tLoss: 2506.5408\n",
      "Training Epoch: 9 [8100/13500]\tLoss: 2538.6208\n",
      "Training Epoch: 9 [8150/13500]\tLoss: 2542.8364\n",
      "Training Epoch: 9 [8200/13500]\tLoss: 2494.4858\n",
      "Training Epoch: 9 [8250/13500]\tLoss: 2525.6047\n",
      "Training Epoch: 9 [8300/13500]\tLoss: 2542.7090\n",
      "Training Epoch: 9 [8350/13500]\tLoss: 2524.2827\n",
      "Training Epoch: 9 [8400/13500]\tLoss: 2514.6516\n",
      "Training Epoch: 9 [8450/13500]\tLoss: 2562.2871\n",
      "Training Epoch: 9 [8500/13500]\tLoss: 2490.2549\n",
      "Training Epoch: 9 [8550/13500]\tLoss: 2459.1580\n",
      "Training Epoch: 9 [8600/13500]\tLoss: 2473.2346\n",
      "Training Epoch: 9 [8650/13500]\tLoss: 2532.6785\n",
      "Training Epoch: 9 [8700/13500]\tLoss: 2517.3948\n",
      "Training Epoch: 9 [8750/13500]\tLoss: 2498.3545\n",
      "Training Epoch: 9 [8800/13500]\tLoss: 2489.7334\n",
      "Training Epoch: 9 [8850/13500]\tLoss: 2403.2397\n",
      "Training Epoch: 9 [8900/13500]\tLoss: 2492.9746\n",
      "Training Epoch: 9 [8950/13500]\tLoss: 2532.6379\n",
      "Training Epoch: 9 [9000/13500]\tLoss: 2480.4282\n",
      "Training Epoch: 9 [9050/13500]\tLoss: 2508.5325\n",
      "Training Epoch: 9 [9100/13500]\tLoss: 2553.3074\n",
      "Training Epoch: 9 [9150/13500]\tLoss: 2539.6199\n",
      "Training Epoch: 9 [9200/13500]\tLoss: 2451.6423\n",
      "Training Epoch: 9 [9250/13500]\tLoss: 2505.3513\n",
      "Training Epoch: 9 [9300/13500]\tLoss: 2537.1777\n",
      "Training Epoch: 9 [9350/13500]\tLoss: 2559.7827\n",
      "Training Epoch: 9 [9400/13500]\tLoss: 2477.7542\n",
      "Training Epoch: 9 [9450/13500]\tLoss: 2484.4839\n",
      "Training Epoch: 9 [9500/13500]\tLoss: 2484.5256\n",
      "Training Epoch: 9 [9550/13500]\tLoss: 2511.2676\n",
      "Training Epoch: 9 [9600/13500]\tLoss: 2483.7822\n",
      "Training Epoch: 9 [9650/13500]\tLoss: 2451.9683\n",
      "Training Epoch: 9 [9700/13500]\tLoss: 2583.0745\n",
      "Training Epoch: 9 [9750/13500]\tLoss: 2501.0449\n",
      "Training Epoch: 9 [9800/13500]\tLoss: 2439.9817\n",
      "Training Epoch: 9 [9850/13500]\tLoss: 2484.7566\n",
      "Training Epoch: 9 [9900/13500]\tLoss: 2403.7104\n",
      "Training Epoch: 9 [9950/13500]\tLoss: 2548.7578\n",
      "Training Epoch: 9 [10000/13500]\tLoss: 2502.6914\n",
      "Training Epoch: 9 [10050/13500]\tLoss: 2495.7942\n",
      "Training Epoch: 9 [10100/13500]\tLoss: 2483.3665\n",
      "Training Epoch: 9 [10150/13500]\tLoss: 2551.5500\n",
      "Training Epoch: 9 [10200/13500]\tLoss: 2461.0432\n",
      "Training Epoch: 9 [10250/13500]\tLoss: 2469.6143\n",
      "Training Epoch: 9 [10300/13500]\tLoss: 2448.2087\n",
      "Training Epoch: 9 [10350/13500]\tLoss: 2494.2407\n",
      "Training Epoch: 9 [10400/13500]\tLoss: 2555.1631\n",
      "Training Epoch: 9 [10450/13500]\tLoss: 2546.3862\n",
      "Training Epoch: 9 [10500/13500]\tLoss: 2519.4614\n",
      "Training Epoch: 9 [10550/13500]\tLoss: 2499.5251\n",
      "Training Epoch: 9 [10600/13500]\tLoss: 2512.1599\n",
      "Training Epoch: 9 [10650/13500]\tLoss: 2465.4631\n",
      "Training Epoch: 9 [10700/13500]\tLoss: 2502.4011\n",
      "Training Epoch: 9 [10750/13500]\tLoss: 2467.9917\n",
      "Training Epoch: 9 [10800/13500]\tLoss: 2470.3501\n",
      "Training Epoch: 9 [10850/13500]\tLoss: 2491.3477\n",
      "Training Epoch: 9 [10900/13500]\tLoss: 2413.6909\n",
      "Training Epoch: 9 [10950/13500]\tLoss: 2495.6963\n",
      "Training Epoch: 9 [11000/13500]\tLoss: 2501.1401\n",
      "Training Epoch: 9 [11050/13500]\tLoss: 2553.0083\n",
      "Training Epoch: 9 [11100/13500]\tLoss: 2442.6355\n",
      "Training Epoch: 9 [11150/13500]\tLoss: 2514.2031\n",
      "Training Epoch: 9 [11200/13500]\tLoss: 2466.8232\n",
      "Training Epoch: 9 [11250/13500]\tLoss: 2382.5747\n",
      "Training Epoch: 9 [11300/13500]\tLoss: 2471.5439\n",
      "Training Epoch: 9 [11350/13500]\tLoss: 2470.1819\n",
      "Training Epoch: 9 [11400/13500]\tLoss: 2466.5820\n",
      "Training Epoch: 9 [11450/13500]\tLoss: 2523.9214\n",
      "Training Epoch: 9 [11500/13500]\tLoss: 2419.6958\n",
      "Training Epoch: 9 [11550/13500]\tLoss: 2520.6077\n",
      "Training Epoch: 9 [11600/13500]\tLoss: 2450.1689\n",
      "Training Epoch: 9 [11650/13500]\tLoss: 2407.7632\n",
      "Training Epoch: 9 [11700/13500]\tLoss: 2463.5991\n",
      "Training Epoch: 9 [11750/13500]\tLoss: 2494.7598\n",
      "Training Epoch: 9 [11800/13500]\tLoss: 2487.2009\n",
      "Training Epoch: 9 [11850/13500]\tLoss: 2431.5361\n",
      "Training Epoch: 9 [11900/13500]\tLoss: 2523.3943\n",
      "Training Epoch: 9 [11950/13500]\tLoss: 2502.2085\n",
      "Training Epoch: 9 [12000/13500]\tLoss: 2413.4099\n",
      "Training Epoch: 9 [12050/13500]\tLoss: 2433.1035\n",
      "Training Epoch: 9 [12100/13500]\tLoss: 2517.9836\n",
      "Training Epoch: 9 [12150/13500]\tLoss: 2439.7805\n",
      "Training Epoch: 9 [12200/13500]\tLoss: 2415.1814\n",
      "Training Epoch: 9 [12250/13500]\tLoss: 2390.6140\n",
      "Training Epoch: 9 [12300/13500]\tLoss: 2486.9219\n",
      "Training Epoch: 9 [12350/13500]\tLoss: 2482.1782\n",
      "Training Epoch: 9 [12400/13500]\tLoss: 2429.5320\n",
      "Training Epoch: 9 [12450/13500]\tLoss: 2458.4678\n",
      "Training Epoch: 9 [12500/13500]\tLoss: 2541.9133\n",
      "Training Epoch: 9 [12550/13500]\tLoss: 2477.5684\n",
      "Training Epoch: 9 [12600/13500]\tLoss: 2463.1643\n",
      "Training Epoch: 9 [12650/13500]\tLoss: 2491.6711\n",
      "Training Epoch: 9 [12700/13500]\tLoss: 2574.4226\n",
      "Training Epoch: 9 [12750/13500]\tLoss: 2476.7673\n",
      "Training Epoch: 9 [12800/13500]\tLoss: 2461.2390\n",
      "Training Epoch: 9 [12850/13500]\tLoss: 2470.9871\n",
      "Training Epoch: 9 [12900/13500]\tLoss: 2455.0471\n",
      "Training Epoch: 9 [12950/13500]\tLoss: 2354.6169\n",
      "Training Epoch: 9 [13000/13500]\tLoss: 2449.2627\n",
      "Training Epoch: 9 [13050/13500]\tLoss: 2431.8823\n",
      "Training Epoch: 9 [13100/13500]\tLoss: 2349.9319\n",
      "Training Epoch: 9 [13150/13500]\tLoss: 2440.5266\n",
      "Training Epoch: 9 [13200/13500]\tLoss: 2412.0996\n",
      "Training Epoch: 9 [13250/13500]\tLoss: 2405.9724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [13300/13500]\tLoss: 2395.9692\n",
      "Training Epoch: 9 [13350/13500]\tLoss: 2457.1318\n",
      "Training Epoch: 9 [13400/13500]\tLoss: 2370.2734\n",
      "Training Epoch: 9 [13450/13500]\tLoss: 2438.8840\n",
      "Training Epoch: 9 [13500/13500]\tLoss: 2484.5833\n",
      "Training Epoch: 9 [1499/1499]\tLoss: 2424.9558\n",
      "Training Epoch: 10 [50/13500]\tLoss: 2447.2761\n",
      "Training Epoch: 10 [100/13500]\tLoss: 2409.1567\n",
      "Training Epoch: 10 [150/13500]\tLoss: 2377.2871\n",
      "Training Epoch: 10 [200/13500]\tLoss: 2390.9792\n",
      "Training Epoch: 10 [250/13500]\tLoss: 2460.6719\n",
      "Training Epoch: 10 [300/13500]\tLoss: 2430.4329\n",
      "Training Epoch: 10 [350/13500]\tLoss: 2457.6050\n",
      "Training Epoch: 10 [400/13500]\tLoss: 2449.8616\n",
      "Training Epoch: 10 [450/13500]\tLoss: 2395.2173\n",
      "Training Epoch: 10 [500/13500]\tLoss: 2379.1592\n",
      "Training Epoch: 10 [550/13500]\tLoss: 2370.9565\n",
      "Training Epoch: 10 [600/13500]\tLoss: 2469.1299\n",
      "Training Epoch: 10 [650/13500]\tLoss: 2368.4294\n",
      "Training Epoch: 10 [700/13500]\tLoss: 2366.8901\n",
      "Training Epoch: 10 [750/13500]\tLoss: 2410.0237\n",
      "Training Epoch: 10 [800/13500]\tLoss: 2410.8711\n",
      "Training Epoch: 10 [850/13500]\tLoss: 2410.9458\n",
      "Training Epoch: 10 [900/13500]\tLoss: 2344.2446\n",
      "Training Epoch: 10 [950/13500]\tLoss: 2358.9431\n",
      "Training Epoch: 10 [1000/13500]\tLoss: 2344.5820\n",
      "Training Epoch: 10 [1050/13500]\tLoss: 2372.1038\n",
      "Training Epoch: 10 [1100/13500]\tLoss: 2467.1528\n",
      "Training Epoch: 10 [1150/13500]\tLoss: 2458.1497\n",
      "Training Epoch: 10 [1200/13500]\tLoss: 2388.1860\n",
      "Training Epoch: 10 [1250/13500]\tLoss: 2377.6741\n",
      "Training Epoch: 10 [1300/13500]\tLoss: 2303.3953\n",
      "Training Epoch: 10 [1350/13500]\tLoss: 2443.5332\n",
      "Training Epoch: 10 [1400/13500]\tLoss: 2380.3816\n",
      "Training Epoch: 10 [1450/13500]\tLoss: 2354.1489\n",
      "Training Epoch: 10 [1500/13500]\tLoss: 2481.7119\n",
      "Training Epoch: 10 [1550/13500]\tLoss: 2407.9849\n",
      "Training Epoch: 10 [1600/13500]\tLoss: 2453.3523\n",
      "Training Epoch: 10 [1650/13500]\tLoss: 2389.0562\n",
      "Training Epoch: 10 [1700/13500]\tLoss: 2476.0808\n",
      "Training Epoch: 10 [1750/13500]\tLoss: 2490.1743\n",
      "Training Epoch: 10 [1800/13500]\tLoss: 2404.6702\n",
      "Training Epoch: 10 [1850/13500]\tLoss: 2342.1484\n",
      "Training Epoch: 10 [1900/13500]\tLoss: 2387.7693\n",
      "Training Epoch: 10 [1950/13500]\tLoss: 2288.5759\n",
      "Training Epoch: 10 [2000/13500]\tLoss: 2438.0549\n",
      "Training Epoch: 10 [2050/13500]\tLoss: 2349.1145\n",
      "Training Epoch: 10 [2100/13500]\tLoss: 2346.2263\n",
      "Training Epoch: 10 [2150/13500]\tLoss: 2392.4084\n",
      "Training Epoch: 10 [2200/13500]\tLoss: 2449.7834\n",
      "Training Epoch: 10 [2250/13500]\tLoss: 2394.7102\n",
      "Training Epoch: 10 [2300/13500]\tLoss: 2375.7559\n",
      "Training Epoch: 10 [2350/13500]\tLoss: 2370.6235\n",
      "Training Epoch: 10 [2400/13500]\tLoss: 2345.3328\n",
      "Training Epoch: 10 [2450/13500]\tLoss: 2454.1641\n",
      "Training Epoch: 10 [2500/13500]\tLoss: 2340.1812\n",
      "Training Epoch: 10 [2550/13500]\tLoss: 2379.2490\n",
      "Training Epoch: 10 [2600/13500]\tLoss: 2377.3330\n",
      "Training Epoch: 10 [2650/13500]\tLoss: 2379.1904\n",
      "Training Epoch: 10 [2700/13500]\tLoss: 2471.9771\n",
      "Training Epoch: 10 [2750/13500]\tLoss: 2347.6858\n",
      "Training Epoch: 10 [2800/13500]\tLoss: 2385.7673\n",
      "Training Epoch: 10 [2850/13500]\tLoss: 2384.2144\n",
      "Training Epoch: 10 [2900/13500]\tLoss: 2360.9395\n",
      "Training Epoch: 10 [2950/13500]\tLoss: 2358.5605\n",
      "Training Epoch: 10 [3000/13500]\tLoss: 2379.3743\n",
      "Training Epoch: 10 [3050/13500]\tLoss: 2324.4053\n",
      "Training Epoch: 10 [3100/13500]\tLoss: 2393.9761\n",
      "Training Epoch: 10 [3150/13500]\tLoss: 2390.2366\n",
      "Training Epoch: 10 [3200/13500]\tLoss: 2424.0359\n",
      "Training Epoch: 10 [3250/13500]\tLoss: 2466.0676\n",
      "Training Epoch: 10 [3300/13500]\tLoss: 2431.8953\n",
      "Training Epoch: 10 [3350/13500]\tLoss: 2398.4412\n",
      "Training Epoch: 10 [3400/13500]\tLoss: 2269.4946\n",
      "Training Epoch: 10 [3450/13500]\tLoss: 2353.0728\n",
      "Training Epoch: 10 [3500/13500]\tLoss: 2434.2070\n",
      "Training Epoch: 10 [3550/13500]\tLoss: 2404.5476\n",
      "Training Epoch: 10 [3600/13500]\tLoss: 2378.9580\n",
      "Training Epoch: 10 [3650/13500]\tLoss: 2438.5288\n",
      "Training Epoch: 10 [3700/13500]\tLoss: 2356.8901\n",
      "Training Epoch: 10 [3750/13500]\tLoss: 2469.1440\n",
      "Training Epoch: 10 [3800/13500]\tLoss: 2436.2209\n",
      "Training Epoch: 10 [3850/13500]\tLoss: 2420.1592\n",
      "Training Epoch: 10 [3900/13500]\tLoss: 2388.4216\n",
      "Training Epoch: 10 [3950/13500]\tLoss: 2406.4282\n",
      "Training Epoch: 10 [4000/13500]\tLoss: 2376.9888\n",
      "Training Epoch: 10 [4050/13500]\tLoss: 2387.5088\n",
      "Training Epoch: 10 [4100/13500]\tLoss: 2394.0195\n",
      "Training Epoch: 10 [4150/13500]\tLoss: 2467.3174\n",
      "Training Epoch: 10 [4200/13500]\tLoss: 2479.2854\n",
      "Training Epoch: 10 [4250/13500]\tLoss: 2409.7041\n",
      "Training Epoch: 10 [4300/13500]\tLoss: 2359.8296\n",
      "Training Epoch: 10 [4350/13500]\tLoss: 2306.6135\n",
      "Training Epoch: 10 [4400/13500]\tLoss: 2416.8479\n",
      "Training Epoch: 10 [4450/13500]\tLoss: 2359.7678\n",
      "Training Epoch: 10 [4500/13500]\tLoss: 2356.4907\n",
      "Training Epoch: 10 [4550/13500]\tLoss: 2354.7542\n",
      "Training Epoch: 10 [4600/13500]\tLoss: 2404.2917\n",
      "Training Epoch: 10 [4650/13500]\tLoss: 2453.9270\n",
      "Training Epoch: 10 [4700/13500]\tLoss: 2381.0591\n",
      "Training Epoch: 10 [4750/13500]\tLoss: 2408.8833\n",
      "Training Epoch: 10 [4800/13500]\tLoss: 2359.8420\n",
      "Training Epoch: 10 [4850/13500]\tLoss: 2356.1636\n",
      "Training Epoch: 10 [4900/13500]\tLoss: 2398.6899\n",
      "Training Epoch: 10 [4950/13500]\tLoss: 2318.1270\n",
      "Training Epoch: 10 [5000/13500]\tLoss: 2307.7710\n",
      "Training Epoch: 10 [5050/13500]\tLoss: 2333.0471\n",
      "Training Epoch: 10 [5100/13500]\tLoss: 2406.5183\n",
      "Training Epoch: 10 [5150/13500]\tLoss: 2323.8521\n",
      "Training Epoch: 10 [5200/13500]\tLoss: 2304.3218\n",
      "Training Epoch: 10 [5250/13500]\tLoss: 2347.7717\n",
      "Training Epoch: 10 [5300/13500]\tLoss: 2336.1326\n",
      "Training Epoch: 10 [5350/13500]\tLoss: 2322.5735\n",
      "Training Epoch: 10 [5400/13500]\tLoss: 2393.9558\n",
      "Training Epoch: 10 [5450/13500]\tLoss: 2377.6768\n",
      "Training Epoch: 10 [5500/13500]\tLoss: 2388.5198\n",
      "Training Epoch: 10 [5550/13500]\tLoss: 2310.3184\n",
      "Training Epoch: 10 [5600/13500]\tLoss: 2327.0454\n",
      "Training Epoch: 10 [5650/13500]\tLoss: 2407.9578\n",
      "Training Epoch: 10 [5700/13500]\tLoss: 2495.6409\n",
      "Training Epoch: 10 [5750/13500]\tLoss: 2414.0623\n",
      "Training Epoch: 10 [5800/13500]\tLoss: 2364.3960\n",
      "Training Epoch: 10 [5850/13500]\tLoss: 2293.1450\n",
      "Training Epoch: 10 [5900/13500]\tLoss: 2349.9844\n",
      "Training Epoch: 10 [5950/13500]\tLoss: 2399.2451\n",
      "Training Epoch: 10 [6000/13500]\tLoss: 2270.9485\n",
      "Training Epoch: 10 [6050/13500]\tLoss: 2324.4355\n",
      "Training Epoch: 10 [6100/13500]\tLoss: 2295.1997\n",
      "Training Epoch: 10 [6150/13500]\tLoss: 2324.6814\n",
      "Training Epoch: 10 [6200/13500]\tLoss: 2356.6621\n",
      "Training Epoch: 10 [6250/13500]\tLoss: 2411.2156\n",
      "Training Epoch: 10 [6300/13500]\tLoss: 2300.7383\n",
      "Training Epoch: 10 [6350/13500]\tLoss: 2315.9546\n",
      "Training Epoch: 10 [6400/13500]\tLoss: 2323.3096\n",
      "Training Epoch: 10 [6450/13500]\tLoss: 2326.5386\n",
      "Training Epoch: 10 [6500/13500]\tLoss: 2337.5352\n",
      "Training Epoch: 10 [6550/13500]\tLoss: 2314.1465\n",
      "Training Epoch: 10 [6600/13500]\tLoss: 2295.6814\n",
      "Training Epoch: 10 [6650/13500]\tLoss: 2308.1350\n",
      "Training Epoch: 10 [6700/13500]\tLoss: 2300.3870\n",
      "Training Epoch: 10 [6750/13500]\tLoss: 2371.4165\n",
      "Training Epoch: 10 [6800/13500]\tLoss: 2283.1431\n",
      "Training Epoch: 10 [6850/13500]\tLoss: 2373.2446\n",
      "Training Epoch: 10 [6900/13500]\tLoss: 2324.8577\n",
      "Training Epoch: 10 [6950/13500]\tLoss: 2354.9670\n",
      "Training Epoch: 10 [7000/13500]\tLoss: 2277.9402\n",
      "Training Epoch: 10 [7050/13500]\tLoss: 2318.4109\n",
      "Training Epoch: 10 [7100/13500]\tLoss: 2378.8357\n",
      "Training Epoch: 10 [7150/13500]\tLoss: 2275.7976\n",
      "Training Epoch: 10 [7200/13500]\tLoss: 2331.3066\n",
      "Training Epoch: 10 [7250/13500]\tLoss: 2352.8870\n",
      "Training Epoch: 10 [7300/13500]\tLoss: 2318.3774\n",
      "Training Epoch: 10 [7350/13500]\tLoss: 2302.6890\n",
      "Training Epoch: 10 [7400/13500]\tLoss: 2275.3774\n",
      "Training Epoch: 10 [7450/13500]\tLoss: 2349.7388\n",
      "Training Epoch: 10 [7500/13500]\tLoss: 2293.2063\n",
      "Training Epoch: 10 [7550/13500]\tLoss: 2344.1147\n",
      "Training Epoch: 10 [7600/13500]\tLoss: 2352.4597\n",
      "Training Epoch: 10 [7650/13500]\tLoss: 2352.4329\n",
      "Training Epoch: 10 [7700/13500]\tLoss: 2307.2581\n",
      "Training Epoch: 10 [7750/13500]\tLoss: 2301.6265\n",
      "Training Epoch: 10 [7800/13500]\tLoss: 2330.9866\n",
      "Training Epoch: 10 [7850/13500]\tLoss: 2293.6296\n",
      "Training Epoch: 10 [7900/13500]\tLoss: 2288.1052\n",
      "Training Epoch: 10 [7950/13500]\tLoss: 2301.0088\n",
      "Training Epoch: 10 [8000/13500]\tLoss: 2366.3701\n",
      "Training Epoch: 10 [8050/13500]\tLoss: 2304.0896\n",
      "Training Epoch: 10 [8100/13500]\tLoss: 2334.9905\n",
      "Training Epoch: 10 [8150/13500]\tLoss: 2336.4409\n",
      "Training Epoch: 10 [8200/13500]\tLoss: 2295.3171\n",
      "Training Epoch: 10 [8250/13500]\tLoss: 2319.3782\n",
      "Training Epoch: 10 [8300/13500]\tLoss: 2342.3918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [8350/13500]\tLoss: 2320.0581\n",
      "Training Epoch: 10 [8400/13500]\tLoss: 2309.5728\n",
      "Training Epoch: 10 [8450/13500]\tLoss: 2357.9058\n",
      "Training Epoch: 10 [8500/13500]\tLoss: 2292.5552\n",
      "Training Epoch: 10 [8550/13500]\tLoss: 2260.3796\n",
      "Training Epoch: 10 [8600/13500]\tLoss: 2275.6338\n",
      "Training Epoch: 10 [8650/13500]\tLoss: 2329.7280\n",
      "Training Epoch: 10 [8700/13500]\tLoss: 2315.5801\n",
      "Training Epoch: 10 [8750/13500]\tLoss: 2299.7634\n",
      "Training Epoch: 10 [8800/13500]\tLoss: 2292.6897\n",
      "Training Epoch: 10 [8850/13500]\tLoss: 2217.0566\n",
      "Training Epoch: 10 [8900/13500]\tLoss: 2293.3931\n",
      "Training Epoch: 10 [8950/13500]\tLoss: 2326.4270\n",
      "Training Epoch: 10 [9000/13500]\tLoss: 2289.1804\n",
      "Training Epoch: 10 [9050/13500]\tLoss: 2311.6431\n",
      "Training Epoch: 10 [9100/13500]\tLoss: 2347.8723\n",
      "Training Epoch: 10 [9150/13500]\tLoss: 2339.2385\n",
      "Training Epoch: 10 [9200/13500]\tLoss: 2258.1140\n",
      "Training Epoch: 10 [9250/13500]\tLoss: 2304.9443\n",
      "Training Epoch: 10 [9300/13500]\tLoss: 2330.3857\n",
      "Training Epoch: 10 [9350/13500]\tLoss: 2350.5466\n",
      "Training Epoch: 10 [9400/13500]\tLoss: 2283.2400\n",
      "Training Epoch: 10 [9450/13500]\tLoss: 2290.8438\n",
      "Training Epoch: 10 [9500/13500]\tLoss: 2283.7456\n",
      "Training Epoch: 10 [9550/13500]\tLoss: 2312.2397\n",
      "Training Epoch: 10 [9600/13500]\tLoss: 2286.1958\n",
      "Training Epoch: 10 [9650/13500]\tLoss: 2257.8071\n",
      "Training Epoch: 10 [9700/13500]\tLoss: 2378.4309\n",
      "Training Epoch: 10 [9750/13500]\tLoss: 2299.8770\n",
      "Training Epoch: 10 [9800/13500]\tLoss: 2251.0552\n",
      "Training Epoch: 10 [9850/13500]\tLoss: 2287.8784\n",
      "Training Epoch: 10 [9900/13500]\tLoss: 2216.8955\n",
      "Training Epoch: 10 [9950/13500]\tLoss: 2347.9016\n",
      "Training Epoch: 10 [10000/13500]\tLoss: 2308.7705\n",
      "Training Epoch: 10 [10050/13500]\tLoss: 2297.7847\n",
      "Training Epoch: 10 [10100/13500]\tLoss: 2288.4861\n",
      "Training Epoch: 10 [10150/13500]\tLoss: 2347.7185\n",
      "Training Epoch: 10 [10200/13500]\tLoss: 2268.2231\n",
      "Training Epoch: 10 [10250/13500]\tLoss: 2277.0803\n",
      "Training Epoch: 10 [10300/13500]\tLoss: 2256.5718\n",
      "Training Epoch: 10 [10350/13500]\tLoss: 2299.1855\n",
      "Training Epoch: 10 [10400/13500]\tLoss: 2351.5029\n",
      "Training Epoch: 10 [10450/13500]\tLoss: 2347.7344\n",
      "Training Epoch: 10 [10500/13500]\tLoss: 2319.0449\n",
      "Training Epoch: 10 [10550/13500]\tLoss: 2298.2249\n",
      "Training Epoch: 10 [10600/13500]\tLoss: 2312.6980\n",
      "Training Epoch: 10 [10650/13500]\tLoss: 2275.2334\n",
      "Training Epoch: 10 [10700/13500]\tLoss: 2303.2681\n",
      "Training Epoch: 10 [10750/13500]\tLoss: 2272.8586\n",
      "Training Epoch: 10 [10800/13500]\tLoss: 2275.6755\n",
      "Training Epoch: 10 [10850/13500]\tLoss: 2296.5786\n",
      "Training Epoch: 10 [10900/13500]\tLoss: 2224.0054\n",
      "Training Epoch: 10 [10950/13500]\tLoss: 2296.2791\n",
      "Training Epoch: 10 [11000/13500]\tLoss: 2306.1360\n",
      "Training Epoch: 10 [11050/13500]\tLoss: 2351.0361\n",
      "Training Epoch: 10 [11100/13500]\tLoss: 2251.3020\n",
      "Training Epoch: 10 [11150/13500]\tLoss: 2318.4324\n",
      "Training Epoch: 10 [11200/13500]\tLoss: 2272.8928\n",
      "Training Epoch: 10 [11250/13500]\tLoss: 2197.9270\n",
      "Training Epoch: 10 [11300/13500]\tLoss: 2279.2908\n",
      "Training Epoch: 10 [11350/13500]\tLoss: 2276.2590\n",
      "Training Epoch: 10 [11400/13500]\tLoss: 2269.2219\n",
      "Training Epoch: 10 [11450/13500]\tLoss: 2324.5078\n",
      "Training Epoch: 10 [11500/13500]\tLoss: 2233.0134\n",
      "Training Epoch: 10 [11550/13500]\tLoss: 2324.8613\n",
      "Training Epoch: 10 [11600/13500]\tLoss: 2261.3616\n",
      "Training Epoch: 10 [11650/13500]\tLoss: 2221.6406\n",
      "Training Epoch: 10 [11700/13500]\tLoss: 2272.3713\n",
      "Training Epoch: 10 [11750/13500]\tLoss: 2296.0867\n",
      "Training Epoch: 10 [11800/13500]\tLoss: 2291.5972\n",
      "Training Epoch: 10 [11850/13500]\tLoss: 2243.8088\n",
      "Training Epoch: 10 [11900/13500]\tLoss: 2323.6267\n",
      "Training Epoch: 10 [11950/13500]\tLoss: 2307.5767\n",
      "Training Epoch: 10 [12000/13500]\tLoss: 2228.9919\n",
      "Training Epoch: 10 [12050/13500]\tLoss: 2243.3079\n",
      "Training Epoch: 10 [12100/13500]\tLoss: 2319.0842\n",
      "Training Epoch: 10 [12150/13500]\tLoss: 2246.1646\n",
      "Training Epoch: 10 [12200/13500]\tLoss: 2224.5439\n",
      "Training Epoch: 10 [12250/13500]\tLoss: 2208.4504\n",
      "Training Epoch: 10 [12300/13500]\tLoss: 2291.0605\n",
      "Training Epoch: 10 [12350/13500]\tLoss: 2287.9045\n",
      "Training Epoch: 10 [12400/13500]\tLoss: 2240.9111\n",
      "Training Epoch: 10 [12450/13500]\tLoss: 2265.4148\n",
      "Training Epoch: 10 [12500/13500]\tLoss: 2343.0769\n",
      "Training Epoch: 10 [12550/13500]\tLoss: 2287.3694\n",
      "Training Epoch: 10 [12600/13500]\tLoss: 2270.7412\n",
      "Training Epoch: 10 [12650/13500]\tLoss: 2299.2126\n",
      "Training Epoch: 10 [12700/13500]\tLoss: 2372.4507\n",
      "Training Epoch: 10 [12750/13500]\tLoss: 2287.9441\n",
      "Training Epoch: 10 [12800/13500]\tLoss: 2273.5942\n",
      "Training Epoch: 10 [12850/13500]\tLoss: 2283.2104\n",
      "Training Epoch: 10 [12900/13500]\tLoss: 2265.0312\n",
      "Training Epoch: 10 [12950/13500]\tLoss: 2172.4849\n",
      "Training Epoch: 10 [13000/13500]\tLoss: 2261.7803\n",
      "Training Epoch: 10 [13050/13500]\tLoss: 2243.5125\n",
      "Training Epoch: 10 [13100/13500]\tLoss: 2171.0500\n",
      "Training Epoch: 10 [13150/13500]\tLoss: 2253.6111\n",
      "Training Epoch: 10 [13200/13500]\tLoss: 2224.7864\n",
      "Training Epoch: 10 [13250/13500]\tLoss: 2218.9282\n",
      "Training Epoch: 10 [13300/13500]\tLoss: 2214.5320\n",
      "Training Epoch: 10 [13350/13500]\tLoss: 2269.4082\n",
      "Training Epoch: 10 [13400/13500]\tLoss: 2189.6924\n",
      "Training Epoch: 10 [13450/13500]\tLoss: 2248.2231\n",
      "Training Epoch: 10 [13500/13500]\tLoss: 2290.2441\n",
      "Training Epoch: 10 [1499/1499]\tLoss: 2238.6601\n",
      "Training Epoch: 11 [50/13500]\tLoss: 2258.5178\n",
      "Training Epoch: 11 [100/13500]\tLoss: 2224.2805\n",
      "Training Epoch: 11 [150/13500]\tLoss: 2192.0186\n",
      "Training Epoch: 11 [200/13500]\tLoss: 2206.6375\n",
      "Training Epoch: 11 [250/13500]\tLoss: 2269.5623\n",
      "Training Epoch: 11 [300/13500]\tLoss: 2242.3174\n",
      "Training Epoch: 11 [350/13500]\tLoss: 2264.9436\n",
      "Training Epoch: 11 [400/13500]\tLoss: 2260.6494\n",
      "Training Epoch: 11 [450/13500]\tLoss: 2211.8625\n",
      "Training Epoch: 11 [500/13500]\tLoss: 2198.1721\n",
      "Training Epoch: 11 [550/13500]\tLoss: 2186.5994\n",
      "Training Epoch: 11 [600/13500]\tLoss: 2282.5391\n",
      "Training Epoch: 11 [650/13500]\tLoss: 2185.5808\n",
      "Training Epoch: 11 [700/13500]\tLoss: 2185.5066\n",
      "Training Epoch: 11 [750/13500]\tLoss: 2226.8347\n",
      "Training Epoch: 11 [800/13500]\tLoss: 2226.0862\n",
      "Training Epoch: 11 [850/13500]\tLoss: 2228.7568\n",
      "Training Epoch: 11 [900/13500]\tLoss: 2166.9260\n",
      "Training Epoch: 11 [950/13500]\tLoss: 2180.9099\n",
      "Training Epoch: 11 [1000/13500]\tLoss: 2164.2283\n",
      "Training Epoch: 11 [1050/13500]\tLoss: 2191.8860\n",
      "Training Epoch: 11 [1100/13500]\tLoss: 2280.7754\n",
      "Training Epoch: 11 [1150/13500]\tLoss: 2269.5000\n",
      "Training Epoch: 11 [1200/13500]\tLoss: 2209.2183\n",
      "Training Epoch: 11 [1250/13500]\tLoss: 2196.3926\n",
      "Training Epoch: 11 [1300/13500]\tLoss: 2128.6504\n",
      "Training Epoch: 11 [1350/13500]\tLoss: 2258.7744\n",
      "Training Epoch: 11 [1400/13500]\tLoss: 2197.7249\n",
      "Training Epoch: 11 [1450/13500]\tLoss: 2177.1118\n",
      "Training Epoch: 11 [1500/13500]\tLoss: 2290.2476\n",
      "Training Epoch: 11 [1550/13500]\tLoss: 2228.3638\n",
      "Training Epoch: 11 [1600/13500]\tLoss: 2265.5920\n",
      "Training Epoch: 11 [1650/13500]\tLoss: 2208.8213\n",
      "Training Epoch: 11 [1700/13500]\tLoss: 2287.9148\n",
      "Training Epoch: 11 [1750/13500]\tLoss: 2302.1626\n",
      "Training Epoch: 11 [1800/13500]\tLoss: 2222.7915\n",
      "Training Epoch: 11 [1850/13500]\tLoss: 2163.8494\n",
      "Training Epoch: 11 [1900/13500]\tLoss: 2204.0791\n",
      "Training Epoch: 11 [1950/13500]\tLoss: 2119.7144\n",
      "Training Epoch: 11 [2000/13500]\tLoss: 2249.3035\n",
      "Training Epoch: 11 [2050/13500]\tLoss: 2170.6865\n",
      "Training Epoch: 11 [2100/13500]\tLoss: 2172.2559\n",
      "Training Epoch: 11 [2150/13500]\tLoss: 2212.0774\n",
      "Training Epoch: 11 [2200/13500]\tLoss: 2260.3591\n",
      "Training Epoch: 11 [2250/13500]\tLoss: 2216.4548\n",
      "Training Epoch: 11 [2300/13500]\tLoss: 2195.3135\n",
      "Training Epoch: 11 [2350/13500]\tLoss: 2194.1067\n",
      "Training Epoch: 11 [2400/13500]\tLoss: 2170.6526\n",
      "Training Epoch: 11 [2450/13500]\tLoss: 2267.7896\n",
      "Training Epoch: 11 [2500/13500]\tLoss: 2163.0120\n",
      "Training Epoch: 11 [2550/13500]\tLoss: 2196.4727\n",
      "Training Epoch: 11 [2600/13500]\tLoss: 2196.5271\n",
      "Training Epoch: 11 [2650/13500]\tLoss: 2202.5933\n",
      "Training Epoch: 11 [2700/13500]\tLoss: 2284.6624\n",
      "Training Epoch: 11 [2750/13500]\tLoss: 2174.8938\n",
      "Training Epoch: 11 [2800/13500]\tLoss: 2206.7087\n",
      "Training Epoch: 11 [2850/13500]\tLoss: 2203.7542\n",
      "Training Epoch: 11 [2900/13500]\tLoss: 2181.1348\n",
      "Training Epoch: 11 [2950/13500]\tLoss: 2180.7876\n",
      "Training Epoch: 11 [3000/13500]\tLoss: 2204.2419\n",
      "Training Epoch: 11 [3050/13500]\tLoss: 2152.0576\n",
      "Training Epoch: 11 [3100/13500]\tLoss: 2217.3164\n",
      "Training Epoch: 11 [3150/13500]\tLoss: 2205.7822\n",
      "Training Epoch: 11 [3200/13500]\tLoss: 2239.3130\n",
      "Training Epoch: 11 [3250/13500]\tLoss: 2277.8562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [3300/13500]\tLoss: 2246.4783\n",
      "Training Epoch: 11 [3350/13500]\tLoss: 2218.5554\n",
      "Training Epoch: 11 [3400/13500]\tLoss: 2104.5820\n",
      "Training Epoch: 11 [3450/13500]\tLoss: 2176.5803\n",
      "Training Epoch: 11 [3500/13500]\tLoss: 2248.6343\n",
      "Training Epoch: 11 [3550/13500]\tLoss: 2221.6094\n",
      "Training Epoch: 11 [3600/13500]\tLoss: 2198.5286\n",
      "Training Epoch: 11 [3650/13500]\tLoss: 2256.2241\n",
      "Training Epoch: 11 [3700/13500]\tLoss: 2182.5251\n",
      "Training Epoch: 11 [3750/13500]\tLoss: 2282.4326\n",
      "Training Epoch: 11 [3800/13500]\tLoss: 2250.0652\n",
      "Training Epoch: 11 [3850/13500]\tLoss: 2237.0452\n",
      "Training Epoch: 11 [3900/13500]\tLoss: 2212.2236\n",
      "Training Epoch: 11 [3950/13500]\tLoss: 2225.5266\n",
      "Training Epoch: 11 [4000/13500]\tLoss: 2200.2952\n",
      "Training Epoch: 11 [4050/13500]\tLoss: 2206.3984\n",
      "Training Epoch: 11 [4100/13500]\tLoss: 2214.3113\n",
      "Training Epoch: 11 [4150/13500]\tLoss: 2278.9395\n",
      "Training Epoch: 11 [4200/13500]\tLoss: 2292.3862\n",
      "Training Epoch: 11 [4250/13500]\tLoss: 2231.6160\n",
      "Training Epoch: 11 [4300/13500]\tLoss: 2187.1301\n",
      "Training Epoch: 11 [4350/13500]\tLoss: 2134.3413\n",
      "Training Epoch: 11 [4400/13500]\tLoss: 2239.7581\n",
      "Training Epoch: 11 [4450/13500]\tLoss: 2182.8359\n",
      "Training Epoch: 11 [4500/13500]\tLoss: 2181.0439\n",
      "Training Epoch: 11 [4550/13500]\tLoss: 2182.1086\n",
      "Training Epoch: 11 [4600/13500]\tLoss: 2228.4758\n",
      "Training Epoch: 11 [4650/13500]\tLoss: 2265.5315\n",
      "Training Epoch: 11 [4700/13500]\tLoss: 2201.8711\n",
      "Training Epoch: 11 [4750/13500]\tLoss: 2228.6101\n",
      "Training Epoch: 11 [4800/13500]\tLoss: 2182.1462\n",
      "Training Epoch: 11 [4850/13500]\tLoss: 2181.7881\n",
      "Training Epoch: 11 [4900/13500]\tLoss: 2222.2820\n",
      "Training Epoch: 11 [4950/13500]\tLoss: 2147.9897\n",
      "Training Epoch: 11 [5000/13500]\tLoss: 2138.8362\n",
      "Training Epoch: 11 [5050/13500]\tLoss: 2160.2009\n",
      "Training Epoch: 11 [5100/13500]\tLoss: 2226.3022\n",
      "Training Epoch: 11 [5150/13500]\tLoss: 2151.8940\n",
      "Training Epoch: 11 [5200/13500]\tLoss: 2134.0486\n",
      "Training Epoch: 11 [5250/13500]\tLoss: 2172.6208\n",
      "Training Epoch: 11 [5300/13500]\tLoss: 2167.9390\n",
      "Training Epoch: 11 [5350/13500]\tLoss: 2151.4233\n",
      "Training Epoch: 11 [5400/13500]\tLoss: 2215.2026\n",
      "Training Epoch: 11 [5450/13500]\tLoss: 2202.8926\n",
      "Training Epoch: 11 [5500/13500]\tLoss: 2210.7632\n",
      "Training Epoch: 11 [5550/13500]\tLoss: 2144.4153\n",
      "Training Epoch: 11 [5600/13500]\tLoss: 2155.1536\n",
      "Training Epoch: 11 [5650/13500]\tLoss: 2231.3870\n",
      "Training Epoch: 11 [5700/13500]\tLoss: 2308.4797\n",
      "Training Epoch: 11 [5750/13500]\tLoss: 2235.3975\n",
      "Training Epoch: 11 [5800/13500]\tLoss: 2193.4814\n",
      "Training Epoch: 11 [5850/13500]\tLoss: 2128.3369\n",
      "Training Epoch: 11 [5900/13500]\tLoss: 2178.5464\n",
      "Training Epoch: 11 [5950/13500]\tLoss: 2222.9583\n",
      "Training Epoch: 11 [6000/13500]\tLoss: 2103.0713\n",
      "Training Epoch: 11 [6050/13500]\tLoss: 2154.0332\n",
      "Training Epoch: 11 [6100/13500]\tLoss: 2129.9014\n",
      "Training Epoch: 11 [6150/13500]\tLoss: 2155.2253\n",
      "Training Epoch: 11 [6200/13500]\tLoss: 2182.0288\n",
      "Training Epoch: 11 [6250/13500]\tLoss: 2237.0676\n",
      "Training Epoch: 11 [6300/13500]\tLoss: 2132.3894\n",
      "Training Epoch: 11 [6350/13500]\tLoss: 2148.7195\n",
      "Training Epoch: 11 [6400/13500]\tLoss: 2153.0566\n",
      "Training Epoch: 11 [6450/13500]\tLoss: 2156.0325\n",
      "Training Epoch: 11 [6500/13500]\tLoss: 2162.7732\n",
      "Training Epoch: 11 [6550/13500]\tLoss: 2144.7637\n",
      "Training Epoch: 11 [6600/13500]\tLoss: 2130.7524\n",
      "Training Epoch: 11 [6650/13500]\tLoss: 2138.1465\n",
      "Training Epoch: 11 [6700/13500]\tLoss: 2131.4819\n",
      "Training Epoch: 11 [6750/13500]\tLoss: 2198.3936\n",
      "Training Epoch: 11 [6800/13500]\tLoss: 2118.9036\n",
      "Training Epoch: 11 [6850/13500]\tLoss: 2196.6936\n",
      "Training Epoch: 11 [6900/13500]\tLoss: 2153.6614\n",
      "Training Epoch: 11 [6950/13500]\tLoss: 2184.1484\n",
      "Training Epoch: 11 [7000/13500]\tLoss: 2114.7395\n",
      "Training Epoch: 11 [7050/13500]\tLoss: 2146.5737\n",
      "Training Epoch: 11 [7100/13500]\tLoss: 2202.6204\n",
      "Training Epoch: 11 [7150/13500]\tLoss: 2111.5100\n",
      "Training Epoch: 11 [7200/13500]\tLoss: 2163.9197\n",
      "Training Epoch: 11 [7250/13500]\tLoss: 2181.1494\n",
      "Training Epoch: 11 [7300/13500]\tLoss: 2150.5149\n",
      "Training Epoch: 11 [7350/13500]\tLoss: 2131.7036\n",
      "Training Epoch: 11 [7400/13500]\tLoss: 2113.1912\n",
      "Training Epoch: 11 [7450/13500]\tLoss: 2180.0845\n",
      "Training Epoch: 11 [7500/13500]\tLoss: 2124.6501\n",
      "Training Epoch: 11 [7550/13500]\tLoss: 2177.1719\n",
      "Training Epoch: 11 [7600/13500]\tLoss: 2181.7200\n",
      "Training Epoch: 11 [7650/13500]\tLoss: 2182.8728\n",
      "Training Epoch: 11 [7700/13500]\tLoss: 2141.8767\n",
      "Training Epoch: 11 [7750/13500]\tLoss: 2137.0742\n",
      "Training Epoch: 11 [7800/13500]\tLoss: 2163.1699\n",
      "Training Epoch: 11 [7850/13500]\tLoss: 2127.5730\n",
      "Training Epoch: 11 [7900/13500]\tLoss: 2124.9712\n",
      "Training Epoch: 11 [7950/13500]\tLoss: 2134.5227\n",
      "Training Epoch: 11 [8000/13500]\tLoss: 2192.2349\n",
      "Training Epoch: 11 [8050/13500]\tLoss: 2135.8450\n",
      "Training Epoch: 11 [8100/13500]\tLoss: 2165.4094\n",
      "Training Epoch: 11 [8150/13500]\tLoss: 2164.8135\n",
      "Training Epoch: 11 [8200/13500]\tLoss: 2130.1936\n",
      "Training Epoch: 11 [8250/13500]\tLoss: 2148.2478\n",
      "Training Epoch: 11 [8300/13500]\tLoss: 2174.1868\n",
      "Training Epoch: 11 [8350/13500]\tLoss: 2150.1230\n",
      "Training Epoch: 11 [8400/13500]\tLoss: 2139.3071\n",
      "Training Epoch: 11 [8450/13500]\tLoss: 2186.5654\n",
      "Training Epoch: 11 [8500/13500]\tLoss: 2128.1409\n",
      "Training Epoch: 11 [8550/13500]\tLoss: 2096.2336\n",
      "Training Epoch: 11 [8600/13500]\tLoss: 2111.7097\n",
      "Training Epoch: 11 [8650/13500]\tLoss: 2160.5786\n",
      "Training Epoch: 11 [8700/13500]\tLoss: 2147.6899\n",
      "Training Epoch: 11 [8750/13500]\tLoss: 2134.3262\n",
      "Training Epoch: 11 [8800/13500]\tLoss: 2128.3367\n",
      "Training Epoch: 11 [8850/13500]\tLoss: 2062.5420\n",
      "Training Epoch: 11 [8900/13500]\tLoss: 2127.5234\n",
      "Training Epoch: 11 [8950/13500]\tLoss: 2154.6438\n",
      "Training Epoch: 11 [9000/13500]\tLoss: 2128.7654\n",
      "Training Epoch: 11 [9050/13500]\tLoss: 2146.7820\n",
      "Training Epoch: 11 [9100/13500]\tLoss: 2175.7322\n",
      "Training Epoch: 11 [9150/13500]\tLoss: 2171.3909\n",
      "Training Epoch: 11 [9200/13500]\tLoss: 2097.4167\n",
      "Training Epoch: 11 [9250/13500]\tLoss: 2137.3171\n",
      "Training Epoch: 11 [9300/13500]\tLoss: 2157.8833\n",
      "Training Epoch: 11 [9350/13500]\tLoss: 2176.3918\n"
     ]
    }
   ],
   "source": [
    "for snr, pnet_name in zip(snrs, pnet_names):\n",
    "    _train_datafile = 'hyperparameter_pooled_training_dataset_random_order_noNulls'\n",
    "    SoundsDataset = NoisySoundsDataset\n",
    "    dset_kwargs = {'snr': snr}\n",
    "    from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "    PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "\n",
    "    DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Device: {DEVICE}')\n",
    "    BATCH_SIZE = 50\n",
    "    NUM_WORKERS = 2\n",
    "    PIN_MEMORY = True\n",
    "    NUM_EPOCHS = 70\n",
    "\n",
    "    lr = 1E-5\n",
    "    engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "    checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "    tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "    train_datafile = f'{engram_dir}{_train_datafile}.hdf5'\n",
    "\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "\n",
    "    pnet = PNetClass(net, build_graph=True)\n",
    "    pnet.eval()\n",
    "    pnet.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "        weight_decay=5e-4)\n",
    "\n",
    "    train_dataset = SoundsDataset(\n",
    "        train_datafile, subset=.9, **dset_kwargs)\n",
    "    test_dataset = SoundsDataset(\n",
    "        train_datafile, subset=.9,\n",
    "        train = False, **dset_kwargs\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "        )\n",
    "    eval_loader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "        )\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "    # summarywriter\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "    if not os.path.exists(tensorboard_path):\n",
    "        os.makedirs(tensorboard_path)\n",
    "    sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')\n",
    "\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "        eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "        # save checkpoints every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b57981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
