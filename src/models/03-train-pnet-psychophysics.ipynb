{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_a1 import PBranchedNetwork_A1SeparateHP\n",
    "PNetClass = PBranchedNetwork_A1SeparateHP\n",
    "pnet_name = 'a1v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv1 import PBranchedNetwork_Conv1SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv1SeparateHP\n",
    "pnet_name = 'conv1v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'allv2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50 #64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 14 00:23:58 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1D:00.0 Off |                  N/A |\r\n",
      "| 27%   29C    P8    13W / 250W |      3MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  On   | 00000000:1F:00.0 Off |                  N/A |\r\n",
      "| 27%   24C    P8    18W / 250W |      3MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_A1SeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PsychoPhysics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = h5py.File(f\"{engram_dir}PsychophysicsWord2017W_not_resampled.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metadata = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META_key.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPsychophysics2017WCleanCochleagrams():\n",
    "    \n",
    "    cochleagrams_clean = []\n",
    "   \n",
    "    cochleagrams = []\n",
    "    for batch_ii in range(0,15300,100):\n",
    "        hdf5_path = '/mnt/smb/locker/issa-locker/users/Erica/cgrams_for_noise_robustness_analysis/PsychophysicsWord2017W_clean/batch_'+str(batch_ii)+'_to_'+str(batch_ii+100)+'.hdf5'\n",
    "        with h5py.File(hdf5_path, 'r') as f_in:\n",
    "            cochleagrams += list(f_in['data'])\n",
    "\n",
    "    return cochleagrams\n",
    "clean_in = getPsychophysics2017WCleanCochleagrams()\n",
    "clean_in = np.array(clean_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for word in f_metadata['word']:\n",
    "    idx = np.argwhere(f_key == word)\n",
    "    if len(idx) == 0:\n",
    "        labels.append(-1)\n",
    "    else:\n",
    "        labels.append(idx.item())\n",
    "labels = np.array(labels)\n",
    "labels += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dset = []\n",
    "for _orig_dset in f_metadata['orig_dset']:\n",
    "    _orig_dset = str(_orig_dset, 'utf-8')\n",
    "    _orig_dset = 'WSJ' if 'WSJ' in _orig_dset else 'Timit'\n",
    "    orig_dset.append(_orig_dset)\n",
    "orig_dset = np.array(orig_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = PsychophysicsCleanSoundsDataset(\n",
    "    clean_in, labels, orig_dset, exclude_timit=True\n",
    "    )\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/13219]\tLoss: 48283.1875\n",
      "Training Epoch: 1 [100/13219]\tLoss: 26204.1973\n",
      "Training Epoch: 1 [150/13219]\tLoss: 17961.8164\n",
      "Training Epoch: 1 [200/13219]\tLoss: 20394.4180\n",
      "Training Epoch: 1 [250/13219]\tLoss: 26420.9844\n",
      "Training Epoch: 1 [300/13219]\tLoss: 26866.3906\n",
      "Training Epoch: 1 [350/13219]\tLoss: 24480.7754\n",
      "Training Epoch: 1 [400/13219]\tLoss: 19651.7129\n",
      "Training Epoch: 1 [450/13219]\tLoss: 15681.5283\n",
      "Training Epoch: 1 [500/13219]\tLoss: 14367.2773\n",
      "Training Epoch: 1 [550/13219]\tLoss: 15406.2051\n",
      "Training Epoch: 1 [600/13219]\tLoss: 15659.7402\n",
      "Training Epoch: 1 [650/13219]\tLoss: 16077.7100\n",
      "Training Epoch: 1 [700/13219]\tLoss: 16421.9375\n",
      "Training Epoch: 1 [750/13219]\tLoss: 15502.5732\n",
      "Training Epoch: 1 [800/13219]\tLoss: 13262.4473\n",
      "Training Epoch: 1 [850/13219]\tLoss: 11620.0312\n",
      "Training Epoch: 1 [900/13219]\tLoss: 11201.5186\n",
      "Training Epoch: 1 [950/13219]\tLoss: 10886.3682\n",
      "Training Epoch: 1 [1000/13219]\tLoss: 12153.4697\n",
      "Training Epoch: 1 [1050/13219]\tLoss: 11770.5449\n",
      "Training Epoch: 1 [1100/13219]\tLoss: 11925.7568\n",
      "Training Epoch: 1 [1150/13219]\tLoss: 11318.2930\n",
      "Training Epoch: 1 [1200/13219]\tLoss: 10434.1553\n",
      "Training Epoch: 1 [1250/13219]\tLoss: 9849.9170\n",
      "Training Epoch: 1 [1300/13219]\tLoss: 8708.7129\n",
      "Training Epoch: 1 [1350/13219]\tLoss: 9293.9453\n",
      "Training Epoch: 1 [1400/13219]\tLoss: 9238.6201\n",
      "Training Epoch: 1 [1450/13219]\tLoss: 9440.2490\n",
      "Training Epoch: 1 [1500/13219]\tLoss: 9368.4395\n",
      "Training Epoch: 1 [1550/13219]\tLoss: 8634.0469\n",
      "Training Epoch: 1 [1600/13219]\tLoss: 8487.4336\n",
      "Training Epoch: 1 [1650/13219]\tLoss: 8356.7549\n",
      "Training Epoch: 1 [1700/13219]\tLoss: 8215.6689\n",
      "Training Epoch: 1 [1750/13219]\tLoss: 7975.8008\n",
      "Training Epoch: 1 [1800/13219]\tLoss: 8181.8804\n",
      "Training Epoch: 1 [1850/13219]\tLoss: 7938.5713\n",
      "Training Epoch: 1 [1900/13219]\tLoss: 8217.6348\n",
      "Training Epoch: 1 [1950/13219]\tLoss: 7804.3584\n",
      "Training Epoch: 1 [2000/13219]\tLoss: 7486.2485\n",
      "Training Epoch: 1 [2050/13219]\tLoss: 6826.4932\n",
      "Training Epoch: 1 [2100/13219]\tLoss: 6820.6719\n",
      "Training Epoch: 1 [2150/13219]\tLoss: 7455.5308\n",
      "Training Epoch: 1 [2200/13219]\tLoss: 7083.6587\n",
      "Training Epoch: 1 [2250/13219]\tLoss: 6860.7534\n",
      "Training Epoch: 1 [2300/13219]\tLoss: 6647.5215\n",
      "Training Epoch: 1 [2350/13219]\tLoss: 6825.9600\n",
      "Training Epoch: 1 [2400/13219]\tLoss: 6391.6514\n",
      "Training Epoch: 1 [2450/13219]\tLoss: 6592.7031\n",
      "Training Epoch: 1 [2500/13219]\tLoss: 6630.7456\n",
      "Training Epoch: 1 [2550/13219]\tLoss: 6437.8618\n",
      "Training Epoch: 1 [2600/13219]\tLoss: 6683.8662\n",
      "Training Epoch: 1 [2650/13219]\tLoss: 6360.7847\n",
      "Training Epoch: 1 [2700/13219]\tLoss: 5996.4521\n",
      "Training Epoch: 1 [2750/13219]\tLoss: 6264.4966\n",
      "Training Epoch: 1 [2800/13219]\tLoss: 6039.0054\n",
      "Training Epoch: 1 [2850/13219]\tLoss: 5943.5566\n",
      "Training Epoch: 1 [2900/13219]\tLoss: 6194.5854\n",
      "Training Epoch: 1 [2950/13219]\tLoss: 6172.9492\n",
      "Training Epoch: 1 [3000/13219]\tLoss: 5918.4980\n",
      "Training Epoch: 1 [3050/13219]\tLoss: 5899.1631\n",
      "Training Epoch: 1 [3100/13219]\tLoss: 5829.9697\n",
      "Training Epoch: 1 [3150/13219]\tLoss: 5764.0039\n",
      "Training Epoch: 1 [3200/13219]\tLoss: 5878.8623\n",
      "Training Epoch: 1 [3250/13219]\tLoss: 5787.0527\n",
      "Training Epoch: 1 [3300/13219]\tLoss: 5909.2769\n",
      "Training Epoch: 1 [3350/13219]\tLoss: 5355.4775\n",
      "Training Epoch: 1 [3400/13219]\tLoss: 5553.5161\n",
      "Training Epoch: 1 [3450/13219]\tLoss: 5663.1812\n",
      "Training Epoch: 1 [3500/13219]\tLoss: 5552.2671\n",
      "Training Epoch: 1 [3550/13219]\tLoss: 5700.1582\n",
      "Training Epoch: 1 [3600/13219]\tLoss: 5443.6895\n",
      "Training Epoch: 1 [3650/13219]\tLoss: 5344.6553\n",
      "Training Epoch: 1 [3700/13219]\tLoss: 5117.6665\n",
      "Training Epoch: 1 [3750/13219]\tLoss: 5066.8774\n",
      "Training Epoch: 1 [3800/13219]\tLoss: 5220.7402\n",
      "Training Epoch: 1 [3850/13219]\tLoss: 5000.9497\n",
      "Training Epoch: 1 [3900/13219]\tLoss: 5015.6079\n",
      "Training Epoch: 1 [3950/13219]\tLoss: 4877.2148\n",
      "Training Epoch: 1 [4000/13219]\tLoss: 4863.4482\n",
      "Training Epoch: 1 [4050/13219]\tLoss: 5203.3579\n",
      "Training Epoch: 1 [4100/13219]\tLoss: 5084.4365\n",
      "Training Epoch: 1 [4150/13219]\tLoss: 5173.7729\n",
      "Training Epoch: 1 [4200/13219]\tLoss: 4738.0166\n",
      "Training Epoch: 1 [4250/13219]\tLoss: 4801.3569\n",
      "Training Epoch: 1 [4300/13219]\tLoss: 4920.4658\n",
      "Training Epoch: 1 [4350/13219]\tLoss: 4804.6133\n",
      "Training Epoch: 1 [4400/13219]\tLoss: 4774.7471\n",
      "Training Epoch: 1 [4450/13219]\tLoss: 4742.4980\n",
      "Training Epoch: 1 [4500/13219]\tLoss: 4809.9023\n",
      "Training Epoch: 1 [4550/13219]\tLoss: 4606.0249\n",
      "Training Epoch: 1 [4600/13219]\tLoss: 4429.5991\n",
      "Training Epoch: 1 [4650/13219]\tLoss: 4546.6758\n",
      "Training Epoch: 1 [4700/13219]\tLoss: 4438.9712\n",
      "Training Epoch: 1 [4750/13219]\tLoss: 4568.9150\n",
      "Training Epoch: 1 [4800/13219]\tLoss: 4594.7148\n",
      "Training Epoch: 1 [4850/13219]\tLoss: 4651.0332\n",
      "Training Epoch: 1 [4900/13219]\tLoss: 4454.7866\n",
      "Training Epoch: 1 [4950/13219]\tLoss: 4640.6597\n",
      "Training Epoch: 1 [5000/13219]\tLoss: 4439.6011\n",
      "Training Epoch: 1 [5050/13219]\tLoss: 4477.0044\n",
      "Training Epoch: 1 [5100/13219]\tLoss: 4411.3179\n",
      "Training Epoch: 1 [5150/13219]\tLoss: 4493.8765\n",
      "Training Epoch: 1 [5200/13219]\tLoss: 4095.5127\n",
      "Training Epoch: 1 [5250/13219]\tLoss: 4511.7471\n",
      "Training Epoch: 1 [5300/13219]\tLoss: 4353.3594\n",
      "Training Epoch: 1 [5350/13219]\tLoss: 4356.1738\n",
      "Training Epoch: 1 [5400/13219]\tLoss: 4568.6743\n",
      "Training Epoch: 1 [5450/13219]\tLoss: 4140.8481\n",
      "Training Epoch: 1 [5500/13219]\tLoss: 4162.7783\n",
      "Training Epoch: 1 [5550/13219]\tLoss: 4241.4106\n",
      "Training Epoch: 1 [5600/13219]\tLoss: 4086.0637\n",
      "Training Epoch: 1 [5650/13219]\tLoss: 3973.0315\n",
      "Training Epoch: 1 [5700/13219]\tLoss: 4129.1147\n",
      "Training Epoch: 1 [5750/13219]\tLoss: 3878.4590\n",
      "Training Epoch: 1 [5800/13219]\tLoss: 4006.0798\n",
      "Training Epoch: 1 [5850/13219]\tLoss: 3841.2007\n",
      "Training Epoch: 1 [5900/13219]\tLoss: 4017.6401\n",
      "Training Epoch: 1 [5950/13219]\tLoss: 3980.5466\n",
      "Training Epoch: 1 [6000/13219]\tLoss: 4305.7637\n",
      "Training Epoch: 1 [6050/13219]\tLoss: 4093.8691\n",
      "Training Epoch: 1 [6100/13219]\tLoss: 4070.2039\n",
      "Training Epoch: 1 [6150/13219]\tLoss: 3953.4148\n",
      "Training Epoch: 1 [6200/13219]\tLoss: 3908.0261\n",
      "Training Epoch: 1 [6250/13219]\tLoss: 3714.4402\n",
      "Training Epoch: 1 [6300/13219]\tLoss: 3977.0554\n",
      "Training Epoch: 1 [6350/13219]\tLoss: 4005.2651\n",
      "Training Epoch: 1 [6400/13219]\tLoss: 3820.6443\n",
      "Training Epoch: 1 [6450/13219]\tLoss: 3935.3479\n",
      "Training Epoch: 1 [6500/13219]\tLoss: 3873.1426\n",
      "Training Epoch: 1 [6550/13219]\tLoss: 3826.3621\n",
      "Training Epoch: 1 [6600/13219]\tLoss: 3905.3418\n",
      "Training Epoch: 1 [6650/13219]\tLoss: 3789.5383\n",
      "Training Epoch: 1 [6700/13219]\tLoss: 3847.7751\n",
      "Training Epoch: 1 [6750/13219]\tLoss: 3803.1350\n",
      "Training Epoch: 1 [6800/13219]\tLoss: 3944.0051\n",
      "Training Epoch: 1 [6850/13219]\tLoss: 3667.6279\n",
      "Training Epoch: 1 [6900/13219]\tLoss: 3683.3879\n",
      "Training Epoch: 1 [6950/13219]\tLoss: 3699.5916\n",
      "Training Epoch: 1 [7000/13219]\tLoss: 3726.5557\n",
      "Training Epoch: 1 [7050/13219]\tLoss: 3638.4939\n",
      "Training Epoch: 1 [7100/13219]\tLoss: 3644.1877\n",
      "Training Epoch: 1 [7150/13219]\tLoss: 3512.0398\n",
      "Training Epoch: 1 [7200/13219]\tLoss: 3620.7656\n",
      "Training Epoch: 1 [7250/13219]\tLoss: 3472.6829\n",
      "Training Epoch: 1 [7300/13219]\tLoss: 3568.4011\n",
      "Training Epoch: 1 [7350/13219]\tLoss: 3605.4912\n",
      "Training Epoch: 1 [7400/13219]\tLoss: 3514.7253\n",
      "Training Epoch: 1 [7450/13219]\tLoss: 3501.3496\n",
      "Training Epoch: 1 [7500/13219]\tLoss: 3550.1790\n",
      "Training Epoch: 1 [7550/13219]\tLoss: 3306.4500\n",
      "Training Epoch: 1 [7600/13219]\tLoss: 3692.8713\n",
      "Training Epoch: 1 [7650/13219]\tLoss: 3486.2529\n",
      "Training Epoch: 1 [7700/13219]\tLoss: 3331.1187\n",
      "Training Epoch: 1 [7750/13219]\tLoss: 3266.5159\n",
      "Training Epoch: 1 [7800/13219]\tLoss: 3488.9592\n",
      "Training Epoch: 1 [7850/13219]\tLoss: 3387.0374\n",
      "Training Epoch: 1 [7900/13219]\tLoss: 3345.9436\n",
      "Training Epoch: 1 [7950/13219]\tLoss: 3386.2954\n",
      "Training Epoch: 1 [8000/13219]\tLoss: 3246.4260\n",
      "Training Epoch: 1 [8050/13219]\tLoss: 3236.4963\n",
      "Training Epoch: 1 [8100/13219]\tLoss: 3274.2285\n",
      "Training Epoch: 1 [8150/13219]\tLoss: 3345.8411\n",
      "Training Epoch: 1 [8200/13219]\tLoss: 3317.2598\n",
      "Training Epoch: 1 [8250/13219]\tLoss: 3363.6936\n",
      "Training Epoch: 1 [8300/13219]\tLoss: 3172.0513\n",
      "Training Epoch: 1 [8350/13219]\tLoss: 3388.6912\n",
      "Training Epoch: 1 [8400/13219]\tLoss: 3305.3948\n",
      "Training Epoch: 1 [8450/13219]\tLoss: 3233.8655\n",
      "Training Epoch: 1 [8500/13219]\tLoss: 3420.4446\n",
      "Training Epoch: 1 [8550/13219]\tLoss: 3068.6226\n",
      "Training Epoch: 1 [8600/13219]\tLoss: 3440.8767\n",
      "Training Epoch: 1 [8650/13219]\tLoss: 3173.3284\n",
      "Training Epoch: 1 [8700/13219]\tLoss: 3133.5278\n",
      "Training Epoch: 1 [8750/13219]\tLoss: 3267.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8800/13219]\tLoss: 3054.5881\n",
      "Training Epoch: 1 [8850/13219]\tLoss: 3074.7666\n",
      "Training Epoch: 1 [8900/13219]\tLoss: 3153.3689\n",
      "Training Epoch: 1 [8950/13219]\tLoss: 3068.8169\n",
      "Training Epoch: 1 [9000/13219]\tLoss: 3028.8743\n",
      "Training Epoch: 1 [9050/13219]\tLoss: 3116.4746\n",
      "Training Epoch: 1 [9100/13219]\tLoss: 3111.8782\n",
      "Training Epoch: 1 [9150/13219]\tLoss: 3093.8135\n",
      "Training Epoch: 1 [9200/13219]\tLoss: 3066.3623\n",
      "Training Epoch: 1 [9250/13219]\tLoss: 3158.4348\n",
      "Training Epoch: 1 [9300/13219]\tLoss: 3044.4260\n",
      "Training Epoch: 1 [9350/13219]\tLoss: 3051.1345\n",
      "Training Epoch: 1 [9400/13219]\tLoss: 3076.4529\n",
      "Training Epoch: 1 [9450/13219]\tLoss: 2954.8838\n",
      "Training Epoch: 1 [9500/13219]\tLoss: 2833.8411\n",
      "Training Epoch: 1 [9550/13219]\tLoss: 2857.6765\n",
      "Training Epoch: 1 [9600/13219]\tLoss: 2857.7402\n",
      "Training Epoch: 1 [9650/13219]\tLoss: 2975.7339\n",
      "Training Epoch: 1 [9700/13219]\tLoss: 2908.2051\n",
      "Training Epoch: 1 [9750/13219]\tLoss: 2993.5847\n",
      "Training Epoch: 1 [9800/13219]\tLoss: 2915.6770\n",
      "Training Epoch: 1 [9850/13219]\tLoss: 2983.0120\n",
      "Training Epoch: 1 [9900/13219]\tLoss: 3043.8567\n",
      "Training Epoch: 1 [9950/13219]\tLoss: 3093.2773\n",
      "Training Epoch: 1 [10000/13219]\tLoss: 3025.8652\n",
      "Training Epoch: 1 [10050/13219]\tLoss: 2853.8311\n",
      "Training Epoch: 1 [10100/13219]\tLoss: 2960.8115\n",
      "Training Epoch: 1 [10150/13219]\tLoss: 2844.3799\n",
      "Training Epoch: 1 [10200/13219]\tLoss: 2805.7588\n",
      "Training Epoch: 1 [10250/13219]\tLoss: 2894.8469\n",
      "Training Epoch: 1 [10300/13219]\tLoss: 2896.4141\n",
      "Training Epoch: 1 [10350/13219]\tLoss: 2671.4709\n",
      "Training Epoch: 1 [10400/13219]\tLoss: 2759.3782\n",
      "Training Epoch: 1 [10450/13219]\tLoss: 2867.5291\n",
      "Training Epoch: 1 [10500/13219]\tLoss: 2820.5051\n",
      "Training Epoch: 1 [10550/13219]\tLoss: 2829.6318\n",
      "Training Epoch: 1 [10600/13219]\tLoss: 2777.1431\n",
      "Training Epoch: 1 [10650/13219]\tLoss: 2796.2261\n",
      "Training Epoch: 1 [10700/13219]\tLoss: 2682.1365\n",
      "Training Epoch: 1 [10750/13219]\tLoss: 2858.7336\n",
      "Training Epoch: 1 [10800/13219]\tLoss: 2799.1060\n",
      "Training Epoch: 1 [10850/13219]\tLoss: 2792.7783\n",
      "Training Epoch: 1 [10900/13219]\tLoss: 2771.0603\n",
      "Training Epoch: 1 [10950/13219]\tLoss: 2781.6975\n",
      "Training Epoch: 1 [11000/13219]\tLoss: 2761.5774\n",
      "Training Epoch: 1 [11050/13219]\tLoss: 2677.4749\n",
      "Training Epoch: 1 [11100/13219]\tLoss: 2694.2756\n",
      "Training Epoch: 1 [11150/13219]\tLoss: 2560.4297\n",
      "Training Epoch: 1 [11200/13219]\tLoss: 2569.3982\n",
      "Training Epoch: 1 [11250/13219]\tLoss: 2614.3901\n",
      "Training Epoch: 1 [11300/13219]\tLoss: 2670.7747\n",
      "Training Epoch: 1 [11350/13219]\tLoss: 2779.8560\n",
      "Training Epoch: 1 [11400/13219]\tLoss: 2602.1885\n",
      "Training Epoch: 1 [11450/13219]\tLoss: 2646.0977\n",
      "Training Epoch: 1 [11500/13219]\tLoss: 2761.6006\n",
      "Training Epoch: 1 [11550/13219]\tLoss: 2753.6055\n",
      "Training Epoch: 1 [11600/13219]\tLoss: 2633.9128\n",
      "Training Epoch: 1 [11650/13219]\tLoss: 2575.8767\n",
      "Training Epoch: 1 [11700/13219]\tLoss: 2653.3044\n",
      "Training Epoch: 1 [11750/13219]\tLoss: 2630.7590\n",
      "Training Epoch: 1 [11800/13219]\tLoss: 2479.3958\n",
      "Training Epoch: 1 [11850/13219]\tLoss: 2523.9766\n",
      "Training Epoch: 1 [11900/13219]\tLoss: 2619.0911\n",
      "Training Epoch: 1 [11950/13219]\tLoss: 2670.3918\n",
      "Training Epoch: 1 [12000/13219]\tLoss: 2545.2874\n",
      "Training Epoch: 1 [12050/13219]\tLoss: 2458.1396\n",
      "Training Epoch: 1 [12100/13219]\tLoss: 2723.8665\n",
      "Training Epoch: 1 [12150/13219]\tLoss: 2596.6924\n",
      "Training Epoch: 1 [12200/13219]\tLoss: 2553.9062\n",
      "Training Epoch: 1 [12250/13219]\tLoss: 2609.8022\n",
      "Training Epoch: 1 [12300/13219]\tLoss: 2536.2678\n",
      "Training Epoch: 1 [12350/13219]\tLoss: 2600.8503\n",
      "Training Epoch: 1 [12400/13219]\tLoss: 2548.3579\n",
      "Training Epoch: 1 [12450/13219]\tLoss: 2369.4387\n",
      "Training Epoch: 1 [12500/13219]\tLoss: 2469.3179\n",
      "Training Epoch: 1 [12550/13219]\tLoss: 2511.2688\n",
      "Training Epoch: 1 [12600/13219]\tLoss: 2605.9775\n",
      "Training Epoch: 1 [12650/13219]\tLoss: 2492.8728\n",
      "Training Epoch: 1 [12700/13219]\tLoss: 2393.3115\n",
      "Training Epoch: 1 [12750/13219]\tLoss: 2405.8357\n",
      "Training Epoch: 1 [12800/13219]\tLoss: 2403.6147\n",
      "Training Epoch: 1 [12850/13219]\tLoss: 2470.6943\n",
      "Training Epoch: 1 [12900/13219]\tLoss: 2512.8989\n",
      "Training Epoch: 1 [12950/13219]\tLoss: 2453.3022\n",
      "Training Epoch: 1 [13000/13219]\tLoss: 2505.5957\n",
      "Training Epoch: 1 [13050/13219]\tLoss: 2365.3513\n",
      "Training Epoch: 1 [13100/13219]\tLoss: 2415.5044\n",
      "Training Epoch: 1 [13150/13219]\tLoss: 2387.6831\n",
      "Training Epoch: 1 [13200/13219]\tLoss: 2297.3469\n",
      "Training Epoch: 1 [13219/13219]\tLoss: 2267.1372\n",
      "Training Epoch: 1 [1469/1469]\tLoss: 2370.0394\n",
      "Training Epoch: 2 [50/13219]\tLoss: 2343.6870\n",
      "Training Epoch: 2 [100/13219]\tLoss: 2312.2773\n",
      "Training Epoch: 2 [150/13219]\tLoss: 2438.8098\n",
      "Training Epoch: 2 [200/13219]\tLoss: 2261.2612\n",
      "Training Epoch: 2 [250/13219]\tLoss: 2423.2227\n",
      "Training Epoch: 2 [300/13219]\tLoss: 2336.2129\n",
      "Training Epoch: 2 [350/13219]\tLoss: 2319.3530\n",
      "Training Epoch: 2 [400/13219]\tLoss: 2299.9180\n",
      "Training Epoch: 2 [450/13219]\tLoss: 2232.7271\n",
      "Training Epoch: 2 [500/13219]\tLoss: 2427.0452\n",
      "Training Epoch: 2 [550/13219]\tLoss: 2320.2437\n",
      "Training Epoch: 2 [600/13219]\tLoss: 2252.2908\n",
      "Training Epoch: 2 [650/13219]\tLoss: 2286.7039\n",
      "Training Epoch: 2 [700/13219]\tLoss: 2319.7756\n",
      "Training Epoch: 2 [750/13219]\tLoss: 2306.0618\n",
      "Training Epoch: 2 [800/13219]\tLoss: 2226.3728\n",
      "Training Epoch: 2 [850/13219]\tLoss: 2340.1663\n",
      "Training Epoch: 2 [900/13219]\tLoss: 2240.6450\n",
      "Training Epoch: 2 [950/13219]\tLoss: 2271.1777\n",
      "Training Epoch: 2 [1000/13219]\tLoss: 2285.3125\n",
      "Training Epoch: 2 [1050/13219]\tLoss: 2206.0364\n",
      "Training Epoch: 2 [1100/13219]\tLoss: 2259.3523\n",
      "Training Epoch: 2 [1150/13219]\tLoss: 2249.1226\n",
      "Training Epoch: 2 [1200/13219]\tLoss: 2280.3113\n",
      "Training Epoch: 2 [1250/13219]\tLoss: 2205.1479\n",
      "Training Epoch: 2 [1300/13219]\tLoss: 2184.9355\n",
      "Training Epoch: 2 [1350/13219]\tLoss: 2382.3721\n",
      "Training Epoch: 2 [1400/13219]\tLoss: 2216.4556\n",
      "Training Epoch: 2 [1450/13219]\tLoss: 2222.2908\n",
      "Training Epoch: 2 [1500/13219]\tLoss: 2230.4194\n",
      "Training Epoch: 2 [1550/13219]\tLoss: 2119.6187\n",
      "Training Epoch: 2 [1600/13219]\tLoss: 2144.0210\n",
      "Training Epoch: 2 [1650/13219]\tLoss: 2167.3069\n",
      "Training Epoch: 2 [1700/13219]\tLoss: 2263.8625\n",
      "Training Epoch: 2 [1750/13219]\tLoss: 2154.7515\n",
      "Training Epoch: 2 [1800/13219]\tLoss: 2213.9011\n",
      "Training Epoch: 2 [1850/13219]\tLoss: 2296.4011\n",
      "Training Epoch: 2 [1900/13219]\tLoss: 2188.3928\n",
      "Training Epoch: 2 [1950/13219]\tLoss: 2216.9277\n",
      "Training Epoch: 2 [2000/13219]\tLoss: 2188.0808\n",
      "Training Epoch: 2 [2050/13219]\tLoss: 2220.8650\n",
      "Training Epoch: 2 [2100/13219]\tLoss: 2321.6562\n",
      "Training Epoch: 2 [2150/13219]\tLoss: 2133.5759\n",
      "Training Epoch: 2 [2200/13219]\tLoss: 2155.1724\n",
      "Training Epoch: 2 [2250/13219]\tLoss: 2128.4985\n",
      "Training Epoch: 2 [2300/13219]\tLoss: 2134.5386\n",
      "Training Epoch: 2 [2350/13219]\tLoss: 2215.7632\n",
      "Training Epoch: 2 [2400/13219]\tLoss: 2152.5024\n",
      "Training Epoch: 2 [2450/13219]\tLoss: 2081.9319\n",
      "Training Epoch: 2 [2500/13219]\tLoss: 2092.9355\n",
      "Training Epoch: 2 [2550/13219]\tLoss: 2213.6235\n",
      "Training Epoch: 2 [2600/13219]\tLoss: 2079.4785\n",
      "Training Epoch: 2 [2650/13219]\tLoss: 1936.6152\n",
      "Training Epoch: 2 [2700/13219]\tLoss: 2163.2314\n",
      "Training Epoch: 2 [2750/13219]\tLoss: 2112.9336\n",
      "Training Epoch: 2 [2800/13219]\tLoss: 2157.6260\n",
      "Training Epoch: 2 [2850/13219]\tLoss: 2142.3633\n",
      "Training Epoch: 2 [2900/13219]\tLoss: 2087.4590\n",
      "Training Epoch: 2 [2950/13219]\tLoss: 2069.0024\n",
      "Training Epoch: 2 [3000/13219]\tLoss: 2075.8289\n",
      "Training Epoch: 2 [3050/13219]\tLoss: 2010.7031\n",
      "Training Epoch: 2 [3100/13219]\tLoss: 2120.7119\n",
      "Training Epoch: 2 [3150/13219]\tLoss: 1971.5792\n",
      "Training Epoch: 2 [3200/13219]\tLoss: 2150.1853\n",
      "Training Epoch: 2 [3250/13219]\tLoss: 2034.7314\n",
      "Training Epoch: 2 [3300/13219]\tLoss: 2096.2007\n",
      "Training Epoch: 2 [3350/13219]\tLoss: 2048.2615\n",
      "Training Epoch: 2 [3400/13219]\tLoss: 2115.8201\n",
      "Training Epoch: 2 [3450/13219]\tLoss: 2074.2368\n",
      "Training Epoch: 2 [3500/13219]\tLoss: 2151.4158\n",
      "Training Epoch: 2 [3550/13219]\tLoss: 2103.3264\n",
      "Training Epoch: 2 [3600/13219]\tLoss: 2067.8179\n",
      "Training Epoch: 2 [3650/13219]\tLoss: 2016.4248\n",
      "Training Epoch: 2 [3700/13219]\tLoss: 2006.0852\n",
      "Training Epoch: 2 [3750/13219]\tLoss: 2038.2688\n",
      "Training Epoch: 2 [3800/13219]\tLoss: 1986.0374\n",
      "Training Epoch: 2 [3850/13219]\tLoss: 1881.7725\n",
      "Training Epoch: 2 [3900/13219]\tLoss: 1937.2406\n",
      "Training Epoch: 2 [3950/13219]\tLoss: 1997.4685\n",
      "Training Epoch: 2 [4000/13219]\tLoss: 2092.1709\n",
      "Training Epoch: 2 [4050/13219]\tLoss: 1995.5295\n",
      "Training Epoch: 2 [4100/13219]\tLoss: 2055.9729\n",
      "Training Epoch: 2 [4150/13219]\tLoss: 1937.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [4200/13219]\tLoss: 1929.2828\n",
      "Training Epoch: 2 [4250/13219]\tLoss: 2006.1085\n",
      "Training Epoch: 2 [4300/13219]\tLoss: 2015.4489\n",
      "Training Epoch: 2 [4350/13219]\tLoss: 1900.1544\n",
      "Training Epoch: 2 [4400/13219]\tLoss: 1983.4686\n",
      "Training Epoch: 2 [4450/13219]\tLoss: 1910.0205\n",
      "Training Epoch: 2 [4500/13219]\tLoss: 1821.3707\n",
      "Training Epoch: 2 [4550/13219]\tLoss: 1984.9662\n",
      "Training Epoch: 2 [4600/13219]\tLoss: 1891.2726\n",
      "Training Epoch: 2 [4650/13219]\tLoss: 1963.4421\n",
      "Training Epoch: 2 [4700/13219]\tLoss: 1879.2384\n",
      "Training Epoch: 2 [4750/13219]\tLoss: 1977.6809\n",
      "Training Epoch: 2 [4800/13219]\tLoss: 1924.3112\n",
      "Training Epoch: 2 [4850/13219]\tLoss: 1900.6223\n",
      "Training Epoch: 2 [4900/13219]\tLoss: 1963.2020\n",
      "Training Epoch: 2 [4950/13219]\tLoss: 1912.2756\n",
      "Training Epoch: 2 [5000/13219]\tLoss: 1853.3003\n",
      "Training Epoch: 2 [5050/13219]\tLoss: 1904.2053\n",
      "Training Epoch: 2 [5100/13219]\tLoss: 1902.0833\n",
      "Training Epoch: 2 [5150/13219]\tLoss: 1924.4987\n",
      "Training Epoch: 2 [5200/13219]\tLoss: 1830.1880\n",
      "Training Epoch: 2 [5250/13219]\tLoss: 1875.9248\n",
      "Training Epoch: 2 [5300/13219]\tLoss: 1921.8147\n",
      "Training Epoch: 2 [5350/13219]\tLoss: 1965.3567\n",
      "Training Epoch: 2 [5400/13219]\tLoss: 1861.4197\n",
      "Training Epoch: 2 [5450/13219]\tLoss: 1832.9302\n",
      "Training Epoch: 2 [5500/13219]\tLoss: 1879.5614\n",
      "Training Epoch: 2 [5550/13219]\tLoss: 1918.8953\n",
      "Training Epoch: 2 [5600/13219]\tLoss: 1793.8656\n",
      "Training Epoch: 2 [5650/13219]\tLoss: 1863.2062\n",
      "Training Epoch: 2 [5700/13219]\tLoss: 1869.1805\n",
      "Training Epoch: 2 [5750/13219]\tLoss: 1923.3397\n",
      "Training Epoch: 2 [5800/13219]\tLoss: 1876.8900\n",
      "Training Epoch: 2 [5850/13219]\tLoss: 1931.1813\n",
      "Training Epoch: 2 [5900/13219]\tLoss: 1884.5806\n",
      "Training Epoch: 2 [5950/13219]\tLoss: 1877.5439\n",
      "Training Epoch: 2 [6000/13219]\tLoss: 1908.4220\n",
      "Training Epoch: 2 [6050/13219]\tLoss: 1848.0162\n",
      "Training Epoch: 2 [6100/13219]\tLoss: 1728.3518\n",
      "Training Epoch: 2 [6150/13219]\tLoss: 1783.9718\n",
      "Training Epoch: 2 [6200/13219]\tLoss: 1899.5308\n",
      "Training Epoch: 2 [6250/13219]\tLoss: 1753.0227\n",
      "Training Epoch: 2 [6300/13219]\tLoss: 1838.6289\n",
      "Training Epoch: 2 [6350/13219]\tLoss: 1796.2137\n",
      "Training Epoch: 2 [6400/13219]\tLoss: 1853.4290\n",
      "Training Epoch: 2 [6450/13219]\tLoss: 1848.4324\n",
      "Training Epoch: 2 [6500/13219]\tLoss: 1843.9515\n",
      "Training Epoch: 2 [6550/13219]\tLoss: 1810.7944\n",
      "Training Epoch: 2 [6600/13219]\tLoss: 1780.1085\n",
      "Training Epoch: 2 [6650/13219]\tLoss: 1851.7797\n",
      "Training Epoch: 2 [6700/13219]\tLoss: 1766.3148\n",
      "Training Epoch: 2 [6750/13219]\tLoss: 1775.3617\n",
      "Training Epoch: 2 [6800/13219]\tLoss: 1747.2892\n",
      "Training Epoch: 2 [6850/13219]\tLoss: 1747.0862\n",
      "Training Epoch: 2 [6900/13219]\tLoss: 1742.6277\n",
      "Training Epoch: 2 [6950/13219]\tLoss: 1636.1534\n",
      "Training Epoch: 2 [7000/13219]\tLoss: 1677.4053\n",
      "Training Epoch: 2 [7050/13219]\tLoss: 1697.6355\n",
      "Training Epoch: 2 [7100/13219]\tLoss: 1773.7762\n",
      "Training Epoch: 2 [7150/13219]\tLoss: 1678.9899\n",
      "Training Epoch: 2 [7200/13219]\tLoss: 1809.9021\n",
      "Training Epoch: 2 [7250/13219]\tLoss: 1786.8798\n",
      "Training Epoch: 2 [7300/13219]\tLoss: 1791.0604\n",
      "Training Epoch: 2 [7350/13219]\tLoss: 1774.6001\n",
      "Training Epoch: 2 [7400/13219]\tLoss: 1786.1516\n",
      "Training Epoch: 2 [7450/13219]\tLoss: 1937.2014\n",
      "Training Epoch: 2 [7500/13219]\tLoss: 1779.9132\n",
      "Training Epoch: 2 [7550/13219]\tLoss: 1733.2048\n",
      "Training Epoch: 2 [7600/13219]\tLoss: 1699.1752\n",
      "Training Epoch: 2 [7650/13219]\tLoss: 1732.2759\n",
      "Training Epoch: 2 [7700/13219]\tLoss: 1822.9734\n",
      "Training Epoch: 2 [7750/13219]\tLoss: 1782.1399\n",
      "Training Epoch: 2 [7800/13219]\tLoss: 1699.1564\n",
      "Training Epoch: 2 [7850/13219]\tLoss: 1663.5424\n",
      "Training Epoch: 2 [7900/13219]\tLoss: 1653.1357\n",
      "Training Epoch: 2 [7950/13219]\tLoss: 1666.8540\n",
      "Training Epoch: 2 [8000/13219]\tLoss: 1696.4102\n",
      "Training Epoch: 2 [8050/13219]\tLoss: 1684.7053\n",
      "Training Epoch: 2 [8100/13219]\tLoss: 1723.5511\n",
      "Training Epoch: 2 [8150/13219]\tLoss: 1622.6995\n",
      "Training Epoch: 2 [8200/13219]\tLoss: 1722.5897\n",
      "Training Epoch: 2 [8250/13219]\tLoss: 1683.2670\n",
      "Training Epoch: 2 [8300/13219]\tLoss: 1742.6147\n",
      "Training Epoch: 2 [8350/13219]\tLoss: 1676.3037\n",
      "Training Epoch: 2 [8400/13219]\tLoss: 1669.3495\n",
      "Training Epoch: 2 [8450/13219]\tLoss: 1653.6896\n",
      "Training Epoch: 2 [8500/13219]\tLoss: 1659.8193\n",
      "Training Epoch: 2 [8550/13219]\tLoss: 1672.3445\n",
      "Training Epoch: 2 [8600/13219]\tLoss: 1668.7415\n",
      "Training Epoch: 2 [8650/13219]\tLoss: 1604.7559\n",
      "Training Epoch: 2 [8700/13219]\tLoss: 1745.8511\n",
      "Training Epoch: 2 [8750/13219]\tLoss: 1666.2087\n",
      "Training Epoch: 2 [8800/13219]\tLoss: 1615.8190\n",
      "Training Epoch: 2 [8850/13219]\tLoss: 1646.1160\n",
      "Training Epoch: 2 [8900/13219]\tLoss: 1607.7573\n",
      "Training Epoch: 2 [8950/13219]\tLoss: 1720.9110\n",
      "Training Epoch: 2 [9000/13219]\tLoss: 1679.4780\n",
      "Training Epoch: 2 [9050/13219]\tLoss: 1674.8361\n",
      "Training Epoch: 2 [9100/13219]\tLoss: 1615.6776\n",
      "Training Epoch: 2 [9150/13219]\tLoss: 1600.5059\n",
      "Training Epoch: 2 [9200/13219]\tLoss: 1654.9177\n",
      "Training Epoch: 2 [9250/13219]\tLoss: 1700.2483\n",
      "Training Epoch: 2 [9300/13219]\tLoss: 1676.3311\n",
      "Training Epoch: 2 [9350/13219]\tLoss: 1559.4839\n",
      "Training Epoch: 2 [9400/13219]\tLoss: 1638.8567\n",
      "Training Epoch: 2 [9450/13219]\tLoss: 1681.4490\n",
      "Training Epoch: 2 [9500/13219]\tLoss: 1630.8624\n",
      "Training Epoch: 2 [9550/13219]\tLoss: 1652.8790\n",
      "Training Epoch: 2 [9600/13219]\tLoss: 1664.1702\n",
      "Training Epoch: 2 [9650/13219]\tLoss: 1568.8485\n",
      "Training Epoch: 2 [9700/13219]\tLoss: 1669.7157\n",
      "Training Epoch: 2 [9750/13219]\tLoss: 1659.8054\n",
      "Training Epoch: 2 [9800/13219]\tLoss: 1572.5486\n",
      "Training Epoch: 2 [9850/13219]\tLoss: 1660.4440\n",
      "Training Epoch: 2 [9900/13219]\tLoss: 1660.7883\n",
      "Training Epoch: 2 [9950/13219]\tLoss: 1635.8381\n",
      "Training Epoch: 2 [10000/13219]\tLoss: 1635.7825\n",
      "Training Epoch: 2 [10050/13219]\tLoss: 1680.5359\n",
      "Training Epoch: 2 [10100/13219]\tLoss: 1569.6388\n",
      "Training Epoch: 2 [10150/13219]\tLoss: 1648.5056\n",
      "Training Epoch: 2 [10200/13219]\tLoss: 1671.1697\n",
      "Training Epoch: 2 [10250/13219]\tLoss: 1583.7936\n",
      "Training Epoch: 2 [10300/13219]\tLoss: 1677.2831\n",
      "Training Epoch: 2 [10350/13219]\tLoss: 1606.0649\n",
      "Training Epoch: 2 [10400/13219]\tLoss: 1574.6829\n",
      "Training Epoch: 2 [10450/13219]\tLoss: 1541.6666\n",
      "Training Epoch: 2 [10500/13219]\tLoss: 1660.2113\n",
      "Training Epoch: 2 [10550/13219]\tLoss: 1586.3912\n",
      "Training Epoch: 2 [10600/13219]\tLoss: 1639.2122\n",
      "Training Epoch: 2 [10650/13219]\tLoss: 1606.2832\n",
      "Training Epoch: 2 [10700/13219]\tLoss: 1709.8531\n",
      "Training Epoch: 2 [10750/13219]\tLoss: 1541.1842\n",
      "Training Epoch: 2 [10800/13219]\tLoss: 1542.9735\n",
      "Training Epoch: 2 [10850/13219]\tLoss: 1581.4982\n",
      "Training Epoch: 2 [10900/13219]\tLoss: 1583.2007\n",
      "Training Epoch: 2 [10950/13219]\tLoss: 1464.5320\n",
      "Training Epoch: 2 [11000/13219]\tLoss: 1532.6224\n",
      "Training Epoch: 2 [11050/13219]\tLoss: 1561.6068\n",
      "Training Epoch: 2 [11100/13219]\tLoss: 1588.7764\n",
      "Training Epoch: 2 [11150/13219]\tLoss: 1488.9329\n",
      "Training Epoch: 2 [11200/13219]\tLoss: 1517.7949\n",
      "Training Epoch: 2 [11250/13219]\tLoss: 1545.5128\n",
      "Training Epoch: 2 [11300/13219]\tLoss: 1493.4757\n",
      "Training Epoch: 2 [11350/13219]\tLoss: 1538.3480\n",
      "Training Epoch: 2 [11400/13219]\tLoss: 1476.6602\n",
      "Training Epoch: 2 [11450/13219]\tLoss: 1496.6605\n",
      "Training Epoch: 2 [11500/13219]\tLoss: 1556.8678\n",
      "Training Epoch: 2 [11550/13219]\tLoss: 1534.6919\n",
      "Training Epoch: 2 [11600/13219]\tLoss: 1561.6945\n",
      "Training Epoch: 2 [11650/13219]\tLoss: 1545.9883\n",
      "Training Epoch: 2 [11700/13219]\tLoss: 1545.7319\n",
      "Training Epoch: 2 [11750/13219]\tLoss: 1493.6960\n",
      "Training Epoch: 2 [11800/13219]\tLoss: 1505.4689\n",
      "Training Epoch: 2 [11850/13219]\tLoss: 1485.7356\n",
      "Training Epoch: 2 [11900/13219]\tLoss: 1444.1140\n",
      "Training Epoch: 2 [11950/13219]\tLoss: 1546.8073\n",
      "Training Epoch: 2 [12000/13219]\tLoss: 1460.9518\n",
      "Training Epoch: 2 [12050/13219]\tLoss: 1522.4210\n",
      "Training Epoch: 2 [12100/13219]\tLoss: 1564.7576\n",
      "Training Epoch: 2 [12150/13219]\tLoss: 1543.9211\n",
      "Training Epoch: 2 [12200/13219]\tLoss: 1446.4252\n",
      "Training Epoch: 2 [12250/13219]\tLoss: 1470.5291\n",
      "Training Epoch: 2 [12300/13219]\tLoss: 1533.5947\n",
      "Training Epoch: 2 [12350/13219]\tLoss: 1524.6530\n",
      "Training Epoch: 2 [12400/13219]\tLoss: 1510.5170\n",
      "Training Epoch: 2 [12450/13219]\tLoss: 1519.5278\n",
      "Training Epoch: 2 [12500/13219]\tLoss: 1456.3846\n",
      "Training Epoch: 2 [12550/13219]\tLoss: 1463.5632\n",
      "Training Epoch: 2 [12600/13219]\tLoss: 1471.1824\n",
      "Training Epoch: 2 [12650/13219]\tLoss: 1440.8713\n",
      "Training Epoch: 2 [12700/13219]\tLoss: 1490.8610\n",
      "Training Epoch: 2 [12750/13219]\tLoss: 1509.9005\n",
      "Training Epoch: 2 [12800/13219]\tLoss: 1551.3495\n",
      "Training Epoch: 2 [12850/13219]\tLoss: 1505.0366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [12900/13219]\tLoss: 1371.5718\n",
      "Training Epoch: 2 [12950/13219]\tLoss: 1547.6409\n",
      "Training Epoch: 2 [13000/13219]\tLoss: 1494.9054\n",
      "Training Epoch: 2 [13050/13219]\tLoss: 1475.3027\n",
      "Training Epoch: 2 [13100/13219]\tLoss: 1484.1483\n",
      "Training Epoch: 2 [13150/13219]\tLoss: 1489.2255\n",
      "Training Epoch: 2 [13200/13219]\tLoss: 1427.2339\n",
      "Training Epoch: 2 [13219/13219]\tLoss: 1509.0775\n",
      "Training Epoch: 2 [1469/1469]\tLoss: 1455.6856\n",
      "Training Epoch: 3 [50/13219]\tLoss: 1520.6245\n",
      "Training Epoch: 3 [100/13219]\tLoss: 1507.2808\n",
      "Training Epoch: 3 [150/13219]\tLoss: 1441.5325\n",
      "Training Epoch: 3 [200/13219]\tLoss: 1520.8237\n",
      "Training Epoch: 3 [250/13219]\tLoss: 1442.8201\n",
      "Training Epoch: 3 [300/13219]\tLoss: 1422.6147\n",
      "Training Epoch: 3 [350/13219]\tLoss: 1429.6383\n",
      "Training Epoch: 3 [400/13219]\tLoss: 1396.1559\n",
      "Training Epoch: 3 [450/13219]\tLoss: 1426.1342\n",
      "Training Epoch: 3 [500/13219]\tLoss: 1450.2825\n",
      "Training Epoch: 3 [550/13219]\tLoss: 1424.5626\n",
      "Training Epoch: 3 [600/13219]\tLoss: 1361.8462\n",
      "Training Epoch: 3 [650/13219]\tLoss: 1401.8229\n",
      "Training Epoch: 3 [700/13219]\tLoss: 1471.4341\n",
      "Training Epoch: 3 [750/13219]\tLoss: 1376.6348\n",
      "Training Epoch: 3 [800/13219]\tLoss: 1416.0695\n",
      "Training Epoch: 3 [850/13219]\tLoss: 1468.0433\n",
      "Training Epoch: 3 [900/13219]\tLoss: 1489.0858\n",
      "Training Epoch: 3 [950/13219]\tLoss: 1410.4111\n",
      "Training Epoch: 3 [1000/13219]\tLoss: 1340.6263\n",
      "Training Epoch: 3 [1050/13219]\tLoss: 1487.2097\n",
      "Training Epoch: 3 [1100/13219]\tLoss: 1416.8347\n",
      "Training Epoch: 3 [1150/13219]\tLoss: 1409.2216\n",
      "Training Epoch: 3 [1200/13219]\tLoss: 1432.1442\n",
      "Training Epoch: 3 [1250/13219]\tLoss: 1405.9564\n",
      "Training Epoch: 3 [1300/13219]\tLoss: 1413.9796\n",
      "Training Epoch: 3 [1350/13219]\tLoss: 1407.4163\n",
      "Training Epoch: 3 [1400/13219]\tLoss: 1411.0308\n",
      "Training Epoch: 3 [1450/13219]\tLoss: 1435.3740\n",
      "Training Epoch: 3 [1500/13219]\tLoss: 1477.2937\n",
      "Training Epoch: 3 [1550/13219]\tLoss: 1334.9694\n",
      "Training Epoch: 3 [1600/13219]\tLoss: 1378.2113\n",
      "Training Epoch: 3 [1650/13219]\tLoss: 1489.2112\n",
      "Training Epoch: 3 [1700/13219]\tLoss: 1443.3994\n",
      "Training Epoch: 3 [1750/13219]\tLoss: 1416.8837\n",
      "Training Epoch: 3 [1800/13219]\tLoss: 1418.2791\n",
      "Training Epoch: 3 [1850/13219]\tLoss: 1406.5974\n",
      "Training Epoch: 3 [1900/13219]\tLoss: 1365.1786\n",
      "Training Epoch: 3 [1950/13219]\tLoss: 1455.0465\n",
      "Training Epoch: 3 [2000/13219]\tLoss: 1411.9845\n",
      "Training Epoch: 3 [2050/13219]\tLoss: 1403.7080\n",
      "Training Epoch: 3 [2100/13219]\tLoss: 1382.1989\n",
      "Training Epoch: 3 [2150/13219]\tLoss: 1404.0474\n",
      "Training Epoch: 3 [2200/13219]\tLoss: 1368.8927\n",
      "Training Epoch: 3 [2250/13219]\tLoss: 1347.3483\n",
      "Training Epoch: 3 [2300/13219]\tLoss: 1334.0588\n",
      "Training Epoch: 3 [2350/13219]\tLoss: 1437.2080\n",
      "Training Epoch: 3 [2400/13219]\tLoss: 1444.1605\n",
      "Training Epoch: 3 [2450/13219]\tLoss: 1361.3405\n",
      "Training Epoch: 3 [2500/13219]\tLoss: 1388.5776\n",
      "Training Epoch: 3 [2550/13219]\tLoss: 1274.9718\n",
      "Training Epoch: 3 [2600/13219]\tLoss: 1344.3494\n",
      "Training Epoch: 3 [2650/13219]\tLoss: 1343.4241\n",
      "Training Epoch: 3 [2700/13219]\tLoss: 1361.4703\n",
      "Training Epoch: 3 [2750/13219]\tLoss: 1410.2606\n",
      "Training Epoch: 3 [2800/13219]\tLoss: 1349.7792\n",
      "Training Epoch: 3 [2850/13219]\tLoss: 1368.7273\n",
      "Training Epoch: 3 [2900/13219]\tLoss: 1438.2345\n",
      "Training Epoch: 3 [2950/13219]\tLoss: 1366.5149\n",
      "Training Epoch: 3 [3000/13219]\tLoss: 1413.9158\n",
      "Training Epoch: 3 [3050/13219]\tLoss: 1421.0532\n",
      "Training Epoch: 3 [3100/13219]\tLoss: 1330.1599\n",
      "Training Epoch: 3 [3150/13219]\tLoss: 1329.5026\n",
      "Training Epoch: 3 [3200/13219]\tLoss: 1362.8192\n",
      "Training Epoch: 3 [3250/13219]\tLoss: 1382.8230\n",
      "Training Epoch: 3 [3300/13219]\tLoss: 1380.8162\n",
      "Training Epoch: 3 [3350/13219]\tLoss: 1374.2365\n",
      "Training Epoch: 3 [3400/13219]\tLoss: 1364.5850\n",
      "Training Epoch: 3 [3450/13219]\tLoss: 1401.7332\n",
      "Training Epoch: 3 [3500/13219]\tLoss: 1343.1823\n",
      "Training Epoch: 3 [3550/13219]\tLoss: 1338.0562\n",
      "Training Epoch: 3 [3600/13219]\tLoss: 1407.9568\n",
      "Training Epoch: 3 [3650/13219]\tLoss: 1340.0941\n",
      "Training Epoch: 3 [3700/13219]\tLoss: 1308.9541\n",
      "Training Epoch: 3 [3750/13219]\tLoss: 1357.5289\n",
      "Training Epoch: 3 [3800/13219]\tLoss: 1344.0105\n",
      "Training Epoch: 3 [3850/13219]\tLoss: 1289.7147\n",
      "Training Epoch: 3 [3900/13219]\tLoss: 1358.9121\n",
      "Training Epoch: 3 [3950/13219]\tLoss: 1323.1689\n",
      "Training Epoch: 3 [4000/13219]\tLoss: 1371.7772\n",
      "Training Epoch: 3 [4050/13219]\tLoss: 1339.6394\n",
      "Training Epoch: 3 [4100/13219]\tLoss: 1337.9463\n",
      "Training Epoch: 3 [4150/13219]\tLoss: 1306.8434\n",
      "Training Epoch: 3 [4200/13219]\tLoss: 1327.4740\n",
      "Training Epoch: 3 [4250/13219]\tLoss: 1380.6711\n",
      "Training Epoch: 3 [4300/13219]\tLoss: 1273.7649\n",
      "Training Epoch: 3 [4350/13219]\tLoss: 1344.4838\n",
      "Training Epoch: 3 [4400/13219]\tLoss: 1326.7488\n",
      "Training Epoch: 3 [4450/13219]\tLoss: 1302.5330\n",
      "Training Epoch: 3 [4500/13219]\tLoss: 1339.5974\n",
      "Training Epoch: 3 [4550/13219]\tLoss: 1365.7739\n",
      "Training Epoch: 3 [4600/13219]\tLoss: 1304.0483\n",
      "Training Epoch: 3 [4650/13219]\tLoss: 1291.9541\n",
      "Training Epoch: 3 [4700/13219]\tLoss: 1330.3683\n",
      "Training Epoch: 3 [4750/13219]\tLoss: 1321.2438\n",
      "Training Epoch: 3 [4800/13219]\tLoss: 1365.9611\n",
      "Training Epoch: 3 [4850/13219]\tLoss: 1316.5513\n",
      "Training Epoch: 3 [4900/13219]\tLoss: 1238.2390\n",
      "Training Epoch: 3 [4950/13219]\tLoss: 1319.6606\n",
      "Training Epoch: 3 [5000/13219]\tLoss: 1351.5726\n",
      "Training Epoch: 3 [5050/13219]\tLoss: 1270.5549\n",
      "Training Epoch: 3 [5100/13219]\tLoss: 1280.0350\n",
      "Training Epoch: 3 [5150/13219]\tLoss: 1306.0665\n",
      "Training Epoch: 3 [5200/13219]\tLoss: 1278.6786\n",
      "Training Epoch: 3 [5250/13219]\tLoss: 1215.5372\n",
      "Training Epoch: 3 [5300/13219]\tLoss: 1278.8876\n",
      "Training Epoch: 3 [5350/13219]\tLoss: 1291.7776\n",
      "Training Epoch: 3 [5400/13219]\tLoss: 1224.5736\n",
      "Training Epoch: 3 [5450/13219]\tLoss: 1232.4818\n",
      "Training Epoch: 3 [5500/13219]\tLoss: 1352.9657\n",
      "Training Epoch: 3 [5550/13219]\tLoss: 1329.5345\n",
      "Training Epoch: 3 [5600/13219]\tLoss: 1267.2322\n",
      "Training Epoch: 3 [5650/13219]\tLoss: 1299.0215\n",
      "Training Epoch: 3 [5700/13219]\tLoss: 1286.7142\n",
      "Training Epoch: 3 [5750/13219]\tLoss: 1319.1777\n",
      "Training Epoch: 3 [5800/13219]\tLoss: 1299.5732\n",
      "Training Epoch: 3 [5850/13219]\tLoss: 1273.9399\n",
      "Training Epoch: 3 [5900/13219]\tLoss: 1343.6969\n",
      "Training Epoch: 3 [5950/13219]\tLoss: 1305.2189\n",
      "Training Epoch: 3 [6000/13219]\tLoss: 1341.1235\n",
      "Training Epoch: 3 [6050/13219]\tLoss: 1261.8199\n",
      "Training Epoch: 3 [6100/13219]\tLoss: 1291.7673\n",
      "Training Epoch: 3 [6150/13219]\tLoss: 1296.1493\n",
      "Training Epoch: 3 [6200/13219]\tLoss: 1309.6864\n",
      "Training Epoch: 3 [6250/13219]\tLoss: 1294.1141\n",
      "Training Epoch: 3 [6300/13219]\tLoss: 1246.4987\n",
      "Training Epoch: 3 [6350/13219]\tLoss: 1338.3353\n",
      "Training Epoch: 3 [6400/13219]\tLoss: 1262.8060\n",
      "Training Epoch: 3 [6450/13219]\tLoss: 1313.3538\n",
      "Training Epoch: 3 [6500/13219]\tLoss: 1244.4944\n",
      "Training Epoch: 3 [6550/13219]\tLoss: 1259.1921\n",
      "Training Epoch: 3 [6600/13219]\tLoss: 1269.8892\n",
      "Training Epoch: 3 [6650/13219]\tLoss: 1260.1079\n",
      "Training Epoch: 3 [6700/13219]\tLoss: 1262.2935\n",
      "Training Epoch: 3 [6750/13219]\tLoss: 1245.5536\n",
      "Training Epoch: 3 [6800/13219]\tLoss: 1227.4048\n",
      "Training Epoch: 3 [6850/13219]\tLoss: 1233.2549\n",
      "Training Epoch: 3 [6900/13219]\tLoss: 1265.4467\n",
      "Training Epoch: 3 [6950/13219]\tLoss: 1254.9219\n",
      "Training Epoch: 3 [7000/13219]\tLoss: 1270.5660\n",
      "Training Epoch: 3 [7050/13219]\tLoss: 1267.8285\n",
      "Training Epoch: 3 [7100/13219]\tLoss: 1290.2891\n",
      "Training Epoch: 3 [7150/13219]\tLoss: 1198.4200\n",
      "Training Epoch: 3 [7200/13219]\tLoss: 1343.5959\n",
      "Training Epoch: 3 [7250/13219]\tLoss: 1271.9948\n",
      "Training Epoch: 3 [7300/13219]\tLoss: 1202.7638\n",
      "Training Epoch: 3 [7350/13219]\tLoss: 1177.2762\n",
      "Training Epoch: 3 [7400/13219]\tLoss: 1244.8934\n",
      "Training Epoch: 3 [7450/13219]\tLoss: 1218.7227\n",
      "Training Epoch: 3 [7500/13219]\tLoss: 1255.6040\n",
      "Training Epoch: 3 [7550/13219]\tLoss: 1200.0647\n",
      "Training Epoch: 3 [7600/13219]\tLoss: 1252.8438\n",
      "Training Epoch: 3 [7650/13219]\tLoss: 1266.2000\n",
      "Training Epoch: 3 [7700/13219]\tLoss: 1237.8911\n",
      "Training Epoch: 3 [7750/13219]\tLoss: 1235.7126\n",
      "Training Epoch: 3 [7800/13219]\tLoss: 1211.7722\n",
      "Training Epoch: 3 [7850/13219]\tLoss: 1229.7839\n",
      "Training Epoch: 3 [7900/13219]\tLoss: 1283.9067\n",
      "Training Epoch: 3 [7950/13219]\tLoss: 1163.5472\n",
      "Training Epoch: 3 [8000/13219]\tLoss: 1161.4316\n",
      "Training Epoch: 3 [8050/13219]\tLoss: 1208.0410\n",
      "Training Epoch: 3 [8100/13219]\tLoss: 1232.0833\n",
      "Training Epoch: 3 [8150/13219]\tLoss: 1230.5953\n",
      "Training Epoch: 3 [8200/13219]\tLoss: 1220.4088\n",
      "Training Epoch: 3 [8250/13219]\tLoss: 1172.4976\n",
      "Training Epoch: 3 [8300/13219]\tLoss: 1257.1460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [8350/13219]\tLoss: 1176.7708\n",
      "Training Epoch: 3 [8400/13219]\tLoss: 1247.6809\n",
      "Training Epoch: 3 [8450/13219]\tLoss: 1258.4487\n",
      "Training Epoch: 3 [8500/13219]\tLoss: 1247.1472\n",
      "Training Epoch: 3 [8550/13219]\tLoss: 1190.7521\n",
      "Training Epoch: 3 [8600/13219]\tLoss: 1199.1277\n",
      "Training Epoch: 3 [8650/13219]\tLoss: 1234.5358\n",
      "Training Epoch: 3 [8700/13219]\tLoss: 1140.6998\n",
      "Training Epoch: 3 [8750/13219]\tLoss: 1296.7870\n",
      "Training Epoch: 3 [8800/13219]\tLoss: 1208.1084\n",
      "Training Epoch: 3 [8850/13219]\tLoss: 1196.2471\n",
      "Training Epoch: 3 [8900/13219]\tLoss: 1241.1312\n",
      "Training Epoch: 3 [8950/13219]\tLoss: 1230.0819\n",
      "Training Epoch: 3 [9000/13219]\tLoss: 1203.0620\n",
      "Training Epoch: 3 [9050/13219]\tLoss: 1230.9832\n",
      "Training Epoch: 3 [9100/13219]\tLoss: 1248.4717\n",
      "Training Epoch: 3 [9150/13219]\tLoss: 1219.8292\n",
      "Training Epoch: 3 [9200/13219]\tLoss: 1180.4055\n",
      "Training Epoch: 3 [9250/13219]\tLoss: 1154.4281\n",
      "Training Epoch: 3 [9300/13219]\tLoss: 1184.8253\n",
      "Training Epoch: 3 [9350/13219]\tLoss: 1269.1868\n",
      "Training Epoch: 3 [9400/13219]\tLoss: 1226.2070\n",
      "Training Epoch: 3 [9450/13219]\tLoss: 1207.2289\n",
      "Training Epoch: 3 [9500/13219]\tLoss: 1197.7025\n",
      "Training Epoch: 3 [9550/13219]\tLoss: 1279.1478\n",
      "Training Epoch: 3 [9600/13219]\tLoss: 1198.1968\n",
      "Training Epoch: 3 [9650/13219]\tLoss: 1195.1710\n",
      "Training Epoch: 3 [9700/13219]\tLoss: 1208.5166\n",
      "Training Epoch: 3 [9750/13219]\tLoss: 1206.8967\n",
      "Training Epoch: 3 [9800/13219]\tLoss: 1182.9250\n",
      "Training Epoch: 3 [9850/13219]\tLoss: 1174.9796\n",
      "Training Epoch: 3 [9900/13219]\tLoss: 1182.7589\n",
      "Training Epoch: 3 [9950/13219]\tLoss: 1199.1471\n",
      "Training Epoch: 3 [10000/13219]\tLoss: 1162.8019\n",
      "Training Epoch: 3 [10050/13219]\tLoss: 1188.3800\n",
      "Training Epoch: 3 [10100/13219]\tLoss: 1195.0438\n",
      "Training Epoch: 3 [10150/13219]\tLoss: 1194.5297\n",
      "Training Epoch: 3 [10200/13219]\tLoss: 1237.5778\n",
      "Training Epoch: 3 [10250/13219]\tLoss: 1124.9867\n",
      "Training Epoch: 3 [10300/13219]\tLoss: 1209.1580\n",
      "Training Epoch: 3 [10350/13219]\tLoss: 1177.4984\n",
      "Training Epoch: 3 [10400/13219]\tLoss: 1214.6219\n",
      "Training Epoch: 3 [10450/13219]\tLoss: 1186.0999\n",
      "Training Epoch: 3 [10500/13219]\tLoss: 1162.0066\n",
      "Training Epoch: 3 [10550/13219]\tLoss: 1197.9823\n",
      "Training Epoch: 3 [10600/13219]\tLoss: 1174.2671\n",
      "Training Epoch: 3 [10650/13219]\tLoss: 1264.0087\n",
      "Training Epoch: 3 [10700/13219]\tLoss: 1210.6740\n",
      "Training Epoch: 3 [10750/13219]\tLoss: 1172.0376\n",
      "Training Epoch: 3 [10800/13219]\tLoss: 1146.0588\n",
      "Training Epoch: 3 [10850/13219]\tLoss: 1226.7114\n",
      "Training Epoch: 3 [10900/13219]\tLoss: 1155.6707\n",
      "Training Epoch: 3 [10950/13219]\tLoss: 1176.7994\n",
      "Training Epoch: 3 [11000/13219]\tLoss: 1144.8361\n",
      "Training Epoch: 3 [11050/13219]\tLoss: 1196.3157\n",
      "Training Epoch: 3 [11100/13219]\tLoss: 1174.9604\n",
      "Training Epoch: 3 [11150/13219]\tLoss: 1182.7812\n",
      "Training Epoch: 3 [11200/13219]\tLoss: 1172.9423\n",
      "Training Epoch: 3 [11250/13219]\tLoss: 1170.4922\n",
      "Training Epoch: 3 [11300/13219]\tLoss: 1101.2139\n",
      "Training Epoch: 3 [11350/13219]\tLoss: 1164.2334\n",
      "Training Epoch: 3 [11400/13219]\tLoss: 1176.0425\n",
      "Training Epoch: 3 [11450/13219]\tLoss: 1149.7786\n",
      "Training Epoch: 3 [11500/13219]\tLoss: 1201.1196\n",
      "Training Epoch: 3 [11550/13219]\tLoss: 1181.2173\n",
      "Training Epoch: 3 [11600/13219]\tLoss: 1179.9622\n",
      "Training Epoch: 3 [11650/13219]\tLoss: 1150.0865\n",
      "Training Epoch: 3 [11700/13219]\tLoss: 1156.6376\n",
      "Training Epoch: 3 [11750/13219]\tLoss: 1212.4902\n",
      "Training Epoch: 3 [11800/13219]\tLoss: 1168.8945\n",
      "Training Epoch: 3 [11850/13219]\tLoss: 1082.0869\n",
      "Training Epoch: 3 [11900/13219]\tLoss: 1179.3445\n",
      "Training Epoch: 3 [11950/13219]\tLoss: 1164.1052\n",
      "Training Epoch: 3 [12000/13219]\tLoss: 1142.1621\n",
      "Training Epoch: 3 [12050/13219]\tLoss: 1177.3950\n",
      "Training Epoch: 3 [12100/13219]\tLoss: 1145.7477\n",
      "Training Epoch: 3 [12150/13219]\tLoss: 1145.5189\n",
      "Training Epoch: 3 [12200/13219]\tLoss: 1158.5409\n",
      "Training Epoch: 3 [12250/13219]\tLoss: 1217.2152\n",
      "Training Epoch: 3 [12300/13219]\tLoss: 1234.9600\n",
      "Training Epoch: 3 [12350/13219]\tLoss: 1114.7272\n",
      "Training Epoch: 3 [12400/13219]\tLoss: 1191.8604\n",
      "Training Epoch: 3 [12450/13219]\tLoss: 1163.1058\n",
      "Training Epoch: 3 [12500/13219]\tLoss: 1164.5972\n",
      "Training Epoch: 3 [12550/13219]\tLoss: 1165.4218\n",
      "Training Epoch: 3 [12600/13219]\tLoss: 1146.9276\n",
      "Training Epoch: 3 [12650/13219]\tLoss: 1171.6200\n",
      "Training Epoch: 3 [12700/13219]\tLoss: 1114.3635\n",
      "Training Epoch: 3 [12750/13219]\tLoss: 1125.0033\n",
      "Training Epoch: 3 [12800/13219]\tLoss: 1181.8519\n",
      "Training Epoch: 3 [12850/13219]\tLoss: 1166.3700\n",
      "Training Epoch: 3 [12900/13219]\tLoss: 1185.4823\n",
      "Training Epoch: 3 [12950/13219]\tLoss: 1199.4260\n",
      "Training Epoch: 3 [13000/13219]\tLoss: 1144.8451\n",
      "Training Epoch: 3 [13050/13219]\tLoss: 1096.7408\n",
      "Training Epoch: 3 [13100/13219]\tLoss: 1149.6621\n",
      "Training Epoch: 3 [13150/13219]\tLoss: 1088.6570\n",
      "Training Epoch: 3 [13200/13219]\tLoss: 1166.1599\n",
      "Training Epoch: 3 [13219/13219]\tLoss: 1121.7003\n",
      "Training Epoch: 3 [1469/1469]\tLoss: 1122.4604\n",
      "Training Epoch: 4 [50/13219]\tLoss: 1162.5475\n",
      "Training Epoch: 4 [100/13219]\tLoss: 1143.2866\n",
      "Training Epoch: 4 [150/13219]\tLoss: 1154.4297\n",
      "Training Epoch: 4 [200/13219]\tLoss: 1187.1941\n",
      "Training Epoch: 4 [250/13219]\tLoss: 1118.6136\n",
      "Training Epoch: 4 [300/13219]\tLoss: 1144.5649\n",
      "Training Epoch: 4 [350/13219]\tLoss: 1135.7662\n",
      "Training Epoch: 4 [400/13219]\tLoss: 1072.2902\n",
      "Training Epoch: 4 [450/13219]\tLoss: 1095.6506\n",
      "Training Epoch: 4 [500/13219]\tLoss: 1134.8445\n",
      "Training Epoch: 4 [550/13219]\tLoss: 1111.5728\n",
      "Training Epoch: 4 [600/13219]\tLoss: 1154.9268\n",
      "Training Epoch: 4 [650/13219]\tLoss: 1157.7686\n",
      "Training Epoch: 4 [700/13219]\tLoss: 1111.6515\n",
      "Training Epoch: 4 [750/13219]\tLoss: 1142.4883\n",
      "Training Epoch: 4 [800/13219]\tLoss: 1066.8473\n",
      "Training Epoch: 4 [850/13219]\tLoss: 1098.5276\n",
      "Training Epoch: 4 [900/13219]\tLoss: 1134.3652\n",
      "Training Epoch: 4 [950/13219]\tLoss: 1155.4204\n",
      "Training Epoch: 4 [1000/13219]\tLoss: 1064.7756\n",
      "Training Epoch: 4 [1050/13219]\tLoss: 1104.6705\n",
      "Training Epoch: 4 [1100/13219]\tLoss: 1102.0856\n",
      "Training Epoch: 4 [1150/13219]\tLoss: 1133.8248\n",
      "Training Epoch: 4 [1200/13219]\tLoss: 1085.8942\n",
      "Training Epoch: 4 [1250/13219]\tLoss: 1174.7444\n",
      "Training Epoch: 4 [1300/13219]\tLoss: 1107.6685\n",
      "Training Epoch: 4 [1350/13219]\tLoss: 1165.4738\n",
      "Training Epoch: 4 [1400/13219]\tLoss: 1062.7784\n",
      "Training Epoch: 4 [1450/13219]\tLoss: 1116.4232\n",
      "Training Epoch: 4 [1500/13219]\tLoss: 1141.8680\n",
      "Training Epoch: 4 [1550/13219]\tLoss: 1145.5330\n",
      "Training Epoch: 4 [1600/13219]\tLoss: 1106.9662\n",
      "Training Epoch: 4 [1650/13219]\tLoss: 1129.9723\n",
      "Training Epoch: 4 [1700/13219]\tLoss: 1131.9528\n",
      "Training Epoch: 4 [1750/13219]\tLoss: 1064.0479\n",
      "Training Epoch: 4 [1800/13219]\tLoss: 1134.9489\n",
      "Training Epoch: 4 [1850/13219]\tLoss: 1159.3075\n",
      "Training Epoch: 4 [1900/13219]\tLoss: 1127.7765\n",
      "Training Epoch: 4 [1950/13219]\tLoss: 1094.7441\n",
      "Training Epoch: 4 [2000/13219]\tLoss: 1096.2738\n",
      "Training Epoch: 4 [2050/13219]\tLoss: 1117.7209\n",
      "Training Epoch: 4 [2100/13219]\tLoss: 1103.9243\n",
      "Training Epoch: 4 [2150/13219]\tLoss: 1123.4677\n",
      "Training Epoch: 4 [2200/13219]\tLoss: 1112.0585\n",
      "Training Epoch: 4 [2250/13219]\tLoss: 1084.0179\n",
      "Training Epoch: 4 [2300/13219]\tLoss: 1044.8713\n",
      "Training Epoch: 4 [2350/13219]\tLoss: 1095.4446\n",
      "Training Epoch: 4 [2400/13219]\tLoss: 1076.4474\n",
      "Training Epoch: 4 [2450/13219]\tLoss: 1159.7794\n",
      "Training Epoch: 4 [2500/13219]\tLoss: 1089.1567\n",
      "Training Epoch: 4 [2550/13219]\tLoss: 1082.1716\n",
      "Training Epoch: 4 [2600/13219]\tLoss: 1087.2664\n",
      "Training Epoch: 4 [2650/13219]\tLoss: 1046.8345\n",
      "Training Epoch: 4 [2700/13219]\tLoss: 1128.5583\n",
      "Training Epoch: 4 [2750/13219]\tLoss: 1136.1559\n",
      "Training Epoch: 4 [2800/13219]\tLoss: 1065.5203\n",
      "Training Epoch: 4 [2850/13219]\tLoss: 1112.5497\n",
      "Training Epoch: 4 [2900/13219]\tLoss: 1030.4271\n",
      "Training Epoch: 4 [2950/13219]\tLoss: 1073.1045\n",
      "Training Epoch: 4 [3000/13219]\tLoss: 1106.2710\n",
      "Training Epoch: 4 [3050/13219]\tLoss: 1080.7863\n",
      "Training Epoch: 4 [3100/13219]\tLoss: 1115.4623\n",
      "Training Epoch: 4 [3150/13219]\tLoss: 1106.6686\n",
      "Training Epoch: 4 [3200/13219]\tLoss: 1076.1759\n",
      "Training Epoch: 4 [3250/13219]\tLoss: 1031.1210\n",
      "Training Epoch: 4 [3300/13219]\tLoss: 1101.4771\n",
      "Training Epoch: 4 [3350/13219]\tLoss: 1067.6134\n",
      "Training Epoch: 4 [3400/13219]\tLoss: 1079.5786\n",
      "Training Epoch: 4 [3450/13219]\tLoss: 1071.4579\n",
      "Training Epoch: 4 [3500/13219]\tLoss: 1054.1046\n",
      "Training Epoch: 4 [3550/13219]\tLoss: 1093.1868\n",
      "Training Epoch: 4 [3600/13219]\tLoss: 1122.2469\n",
      "Training Epoch: 4 [3650/13219]\tLoss: 1078.2898\n",
      "Training Epoch: 4 [3700/13219]\tLoss: 1117.9584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [3750/13219]\tLoss: 1082.7719\n",
      "Training Epoch: 4 [3800/13219]\tLoss: 1078.2201\n",
      "Training Epoch: 4 [3850/13219]\tLoss: 1038.9868\n",
      "Training Epoch: 4 [3900/13219]\tLoss: 1047.2157\n",
      "Training Epoch: 4 [3950/13219]\tLoss: 1054.0239\n",
      "Training Epoch: 4 [4000/13219]\tLoss: 1080.6592\n",
      "Training Epoch: 4 [4050/13219]\tLoss: 1055.3109\n",
      "Training Epoch: 4 [4100/13219]\tLoss: 1041.2759\n",
      "Training Epoch: 4 [4150/13219]\tLoss: 1011.2642\n",
      "Training Epoch: 4 [4200/13219]\tLoss: 1061.8191\n",
      "Training Epoch: 4 [4250/13219]\tLoss: 1132.3274\n",
      "Training Epoch: 4 [4300/13219]\tLoss: 1058.8918\n",
      "Training Epoch: 4 [4350/13219]\tLoss: 1105.2333\n",
      "Training Epoch: 4 [4400/13219]\tLoss: 1137.1910\n",
      "Training Epoch: 4 [4450/13219]\tLoss: 1081.0756\n",
      "Training Epoch: 4 [4500/13219]\tLoss: 1084.1183\n",
      "Training Epoch: 4 [4550/13219]\tLoss: 1058.9478\n",
      "Training Epoch: 4 [4600/13219]\tLoss: 1073.0679\n",
      "Training Epoch: 4 [4650/13219]\tLoss: 1078.1088\n",
      "Training Epoch: 4 [4700/13219]\tLoss: 1060.3414\n",
      "Training Epoch: 4 [4750/13219]\tLoss: 1035.4117\n",
      "Training Epoch: 4 [4800/13219]\tLoss: 1122.5031\n",
      "Training Epoch: 4 [4850/13219]\tLoss: 1112.3051\n",
      "Training Epoch: 4 [4900/13219]\tLoss: 1085.4650\n",
      "Training Epoch: 4 [4950/13219]\tLoss: 1109.1246\n",
      "Training Epoch: 4 [5000/13219]\tLoss: 1018.9476\n",
      "Training Epoch: 4 [5050/13219]\tLoss: 1031.0442\n",
      "Training Epoch: 4 [5100/13219]\tLoss: 1124.6772\n",
      "Training Epoch: 4 [5150/13219]\tLoss: 1068.1572\n",
      "Training Epoch: 4 [5200/13219]\tLoss: 1018.7495\n",
      "Training Epoch: 4 [5250/13219]\tLoss: 1044.4996\n",
      "Training Epoch: 4 [5300/13219]\tLoss: 1053.6910\n",
      "Training Epoch: 4 [5350/13219]\tLoss: 1031.0884\n",
      "Training Epoch: 4 [5400/13219]\tLoss: 1083.2468\n",
      "Training Epoch: 4 [5450/13219]\tLoss: 1045.8434\n",
      "Training Epoch: 4 [5500/13219]\tLoss: 1033.0869\n",
      "Training Epoch: 4 [5550/13219]\tLoss: 1058.7155\n",
      "Training Epoch: 4 [5600/13219]\tLoss: 1078.6943\n",
      "Training Epoch: 4 [5650/13219]\tLoss: 1085.5426\n",
      "Training Epoch: 4 [5700/13219]\tLoss: 1066.6229\n",
      "Training Epoch: 4 [5750/13219]\tLoss: 1064.9879\n",
      "Training Epoch: 4 [5800/13219]\tLoss: 990.0316\n",
      "Training Epoch: 4 [5850/13219]\tLoss: 1050.9484\n",
      "Training Epoch: 4 [5900/13219]\tLoss: 1017.0682\n",
      "Training Epoch: 4 [5950/13219]\tLoss: 1069.4363\n",
      "Training Epoch: 4 [6000/13219]\tLoss: 1074.1327\n",
      "Training Epoch: 4 [6050/13219]\tLoss: 1062.9482\n",
      "Training Epoch: 4 [6100/13219]\tLoss: 1059.5314\n",
      "Training Epoch: 4 [6150/13219]\tLoss: 1040.2208\n",
      "Training Epoch: 4 [6200/13219]\tLoss: 1067.8517\n",
      "Training Epoch: 4 [6250/13219]\tLoss: 1033.3927\n",
      "Training Epoch: 4 [6300/13219]\tLoss: 1111.5981\n",
      "Training Epoch: 4 [6350/13219]\tLoss: 1068.4280\n",
      "Training Epoch: 4 [6400/13219]\tLoss: 1018.2567\n",
      "Training Epoch: 4 [6450/13219]\tLoss: 1085.4639\n",
      "Training Epoch: 4 [6500/13219]\tLoss: 1076.3665\n",
      "Training Epoch: 4 [6550/13219]\tLoss: 1035.6730\n",
      "Training Epoch: 4 [6600/13219]\tLoss: 1082.1193\n",
      "Training Epoch: 4 [6650/13219]\tLoss: 1047.7975\n",
      "Training Epoch: 4 [6700/13219]\tLoss: 1012.5363\n",
      "Training Epoch: 4 [6750/13219]\tLoss: 1079.9757\n",
      "Training Epoch: 4 [6800/13219]\tLoss: 1077.1428\n",
      "Training Epoch: 4 [6850/13219]\tLoss: 1045.0206\n",
      "Training Epoch: 4 [6900/13219]\tLoss: 1115.3906\n",
      "Training Epoch: 4 [6950/13219]\tLoss: 1039.1157\n",
      "Training Epoch: 4 [7000/13219]\tLoss: 1037.1023\n",
      "Training Epoch: 4 [7050/13219]\tLoss: 1073.3585\n",
      "Training Epoch: 4 [7100/13219]\tLoss: 1033.7479\n",
      "Training Epoch: 4 [7150/13219]\tLoss: 1045.8778\n",
      "Training Epoch: 4 [7200/13219]\tLoss: 1044.8326\n",
      "Training Epoch: 4 [7250/13219]\tLoss: 1013.5757\n",
      "Training Epoch: 4 [7300/13219]\tLoss: 1055.4276\n",
      "Training Epoch: 4 [7350/13219]\tLoss: 1063.3345\n",
      "Training Epoch: 4 [7400/13219]\tLoss: 1021.2241\n",
      "Training Epoch: 4 [7450/13219]\tLoss: 1014.4184\n",
      "Training Epoch: 4 [7500/13219]\tLoss: 967.8045\n",
      "Training Epoch: 4 [7550/13219]\tLoss: 1029.0394\n",
      "Training Epoch: 4 [7600/13219]\tLoss: 994.6064\n",
      "Training Epoch: 4 [7650/13219]\tLoss: 1084.4667\n",
      "Training Epoch: 4 [7700/13219]\tLoss: 1071.3125\n",
      "Training Epoch: 4 [7750/13219]\tLoss: 1056.8008\n",
      "Training Epoch: 4 [7800/13219]\tLoss: 1038.9972\n",
      "Training Epoch: 4 [7850/13219]\tLoss: 1039.2250\n",
      "Training Epoch: 4 [7900/13219]\tLoss: 1064.2367\n",
      "Training Epoch: 4 [7950/13219]\tLoss: 998.1154\n",
      "Training Epoch: 4 [8000/13219]\tLoss: 1098.5778\n",
      "Training Epoch: 4 [8050/13219]\tLoss: 996.1165\n",
      "Training Epoch: 4 [8100/13219]\tLoss: 1002.9729\n",
      "Training Epoch: 4 [8150/13219]\tLoss: 943.2606\n",
      "Training Epoch: 4 [8200/13219]\tLoss: 1052.6401\n",
      "Training Epoch: 4 [8250/13219]\tLoss: 1005.6287\n",
      "Training Epoch: 4 [8300/13219]\tLoss: 1023.0891\n",
      "Training Epoch: 4 [8350/13219]\tLoss: 1066.0732\n",
      "Training Epoch: 4 [8400/13219]\tLoss: 999.6021\n",
      "Training Epoch: 4 [8450/13219]\tLoss: 1057.5820\n",
      "Training Epoch: 4 [8500/13219]\tLoss: 1002.8195\n",
      "Training Epoch: 4 [8550/13219]\tLoss: 1048.1479\n",
      "Training Epoch: 4 [8600/13219]\tLoss: 1002.2949\n",
      "Training Epoch: 4 [8650/13219]\tLoss: 991.1124\n",
      "Training Epoch: 4 [8700/13219]\tLoss: 1051.3398\n",
      "Training Epoch: 4 [8750/13219]\tLoss: 1025.4757\n",
      "Training Epoch: 4 [8800/13219]\tLoss: 1042.0903\n",
      "Training Epoch: 4 [8850/13219]\tLoss: 967.7620\n",
      "Training Epoch: 4 [8900/13219]\tLoss: 1029.4918\n",
      "Training Epoch: 4 [8950/13219]\tLoss: 1036.9474\n",
      "Training Epoch: 4 [9000/13219]\tLoss: 1009.1596\n",
      "Training Epoch: 4 [9050/13219]\tLoss: 1000.7498\n",
      "Training Epoch: 4 [9100/13219]\tLoss: 1039.4852\n",
      "Training Epoch: 4 [9150/13219]\tLoss: 963.5652\n",
      "Training Epoch: 4 [9200/13219]\tLoss: 976.7429\n",
      "Training Epoch: 4 [9250/13219]\tLoss: 1033.5078\n",
      "Training Epoch: 4 [9300/13219]\tLoss: 1072.8823\n",
      "Training Epoch: 4 [9350/13219]\tLoss: 1024.0850\n",
      "Training Epoch: 4 [9400/13219]\tLoss: 1043.7845\n",
      "Training Epoch: 4 [9450/13219]\tLoss: 1016.8503\n",
      "Training Epoch: 4 [9500/13219]\tLoss: 1011.4563\n",
      "Training Epoch: 4 [9550/13219]\tLoss: 1029.1230\n",
      "Training Epoch: 4 [9600/13219]\tLoss: 980.9364\n",
      "Training Epoch: 4 [9650/13219]\tLoss: 1027.0646\n",
      "Training Epoch: 4 [9700/13219]\tLoss: 999.1871\n",
      "Training Epoch: 4 [9750/13219]\tLoss: 992.4037\n",
      "Training Epoch: 4 [9800/13219]\tLoss: 986.4086\n",
      "Training Epoch: 4 [9850/13219]\tLoss: 980.3286\n",
      "Training Epoch: 4 [9900/13219]\tLoss: 991.9691\n",
      "Training Epoch: 4 [9950/13219]\tLoss: 1005.1180\n",
      "Training Epoch: 4 [10000/13219]\tLoss: 1016.7830\n",
      "Training Epoch: 4 [10050/13219]\tLoss: 1009.4882\n",
      "Training Epoch: 4 [10100/13219]\tLoss: 1038.6664\n",
      "Training Epoch: 4 [10150/13219]\tLoss: 974.5641\n",
      "Training Epoch: 4 [10200/13219]\tLoss: 1021.8624\n",
      "Training Epoch: 4 [10250/13219]\tLoss: 988.9017\n",
      "Training Epoch: 4 [10300/13219]\tLoss: 995.1442\n",
      "Training Epoch: 4 [10350/13219]\tLoss: 1014.3500\n",
      "Training Epoch: 4 [10400/13219]\tLoss: 1035.2437\n",
      "Training Epoch: 4 [10450/13219]\tLoss: 1040.4105\n",
      "Training Epoch: 4 [10500/13219]\tLoss: 1012.8867\n",
      "Training Epoch: 4 [10550/13219]\tLoss: 980.8309\n",
      "Training Epoch: 4 [10600/13219]\tLoss: 979.5114\n",
      "Training Epoch: 4 [10650/13219]\tLoss: 1012.5601\n",
      "Training Epoch: 4 [10700/13219]\tLoss: 957.2307\n",
      "Training Epoch: 4 [10750/13219]\tLoss: 978.5817\n",
      "Training Epoch: 4 [10800/13219]\tLoss: 968.7932\n",
      "Training Epoch: 4 [10850/13219]\tLoss: 986.7135\n",
      "Training Epoch: 4 [10900/13219]\tLoss: 987.9186\n",
      "Training Epoch: 4 [10950/13219]\tLoss: 989.8520\n",
      "Training Epoch: 4 [11000/13219]\tLoss: 1028.3011\n",
      "Training Epoch: 4 [11050/13219]\tLoss: 1057.0809\n",
      "Training Epoch: 4 [11100/13219]\tLoss: 1046.1230\n",
      "Training Epoch: 4 [11150/13219]\tLoss: 1026.9755\n",
      "Training Epoch: 4 [11200/13219]\tLoss: 1028.9911\n",
      "Training Epoch: 4 [11250/13219]\tLoss: 1026.7993\n",
      "Training Epoch: 4 [11300/13219]\tLoss: 995.4648\n",
      "Training Epoch: 4 [11350/13219]\tLoss: 945.3027\n",
      "Training Epoch: 4 [11400/13219]\tLoss: 973.0555\n",
      "Training Epoch: 4 [11450/13219]\tLoss: 1020.1937\n",
      "Training Epoch: 4 [11500/13219]\tLoss: 998.4051\n",
      "Training Epoch: 4 [11550/13219]\tLoss: 977.9705\n",
      "Training Epoch: 4 [11600/13219]\tLoss: 966.2763\n",
      "Training Epoch: 4 [11650/13219]\tLoss: 1039.7744\n",
      "Training Epoch: 4 [11700/13219]\tLoss: 1048.9883\n",
      "Training Epoch: 4 [11750/13219]\tLoss: 980.1572\n",
      "Training Epoch: 4 [11800/13219]\tLoss: 983.7937\n",
      "Training Epoch: 4 [11850/13219]\tLoss: 978.6691\n",
      "Training Epoch: 4 [11900/13219]\tLoss: 1045.6525\n",
      "Training Epoch: 4 [11950/13219]\tLoss: 982.8701\n",
      "Training Epoch: 4 [12000/13219]\tLoss: 973.0538\n",
      "Training Epoch: 4 [12050/13219]\tLoss: 987.1769\n",
      "Training Epoch: 4 [12100/13219]\tLoss: 953.2979\n",
      "Training Epoch: 4 [12150/13219]\tLoss: 1017.9865\n",
      "Training Epoch: 4 [12200/13219]\tLoss: 986.7584\n",
      "Training Epoch: 4 [12250/13219]\tLoss: 962.1066\n",
      "Training Epoch: 4 [12300/13219]\tLoss: 1003.7055\n",
      "Training Epoch: 4 [12350/13219]\tLoss: 986.1094\n",
      "Training Epoch: 4 [12400/13219]\tLoss: 1017.4913\n",
      "Training Epoch: 4 [12450/13219]\tLoss: 961.7344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [12500/13219]\tLoss: 1006.6086\n",
      "Training Epoch: 4 [12550/13219]\tLoss: 1005.8234\n",
      "Training Epoch: 4 [12600/13219]\tLoss: 1010.6536\n",
      "Training Epoch: 4 [12650/13219]\tLoss: 1000.2637\n",
      "Training Epoch: 4 [12700/13219]\tLoss: 1021.3529\n",
      "Training Epoch: 4 [12750/13219]\tLoss: 1027.6575\n",
      "Training Epoch: 4 [12800/13219]\tLoss: 996.2526\n",
      "Training Epoch: 4 [12850/13219]\tLoss: 989.2789\n",
      "Training Epoch: 4 [12900/13219]\tLoss: 1038.0986\n",
      "Training Epoch: 4 [12950/13219]\tLoss: 990.4302\n",
      "Training Epoch: 4 [13000/13219]\tLoss: 998.8106\n",
      "Training Epoch: 4 [13050/13219]\tLoss: 1002.9542\n",
      "Training Epoch: 4 [13100/13219]\tLoss: 967.8397\n",
      "Training Epoch: 4 [13150/13219]\tLoss: 1032.4781\n",
      "Training Epoch: 4 [13200/13219]\tLoss: 963.3773\n",
      "Training Epoch: 4 [13219/13219]\tLoss: 1005.4556\n",
      "Training Epoch: 4 [1469/1469]\tLoss: 976.9172\n",
      "Training Epoch: 5 [50/13219]\tLoss: 953.9113\n",
      "Training Epoch: 5 [100/13219]\tLoss: 970.4792\n",
      "Training Epoch: 5 [150/13219]\tLoss: 959.1561\n",
      "Training Epoch: 5 [200/13219]\tLoss: 980.8093\n",
      "Training Epoch: 5 [250/13219]\tLoss: 991.6753\n",
      "Training Epoch: 5 [300/13219]\tLoss: 959.4662\n",
      "Training Epoch: 5 [350/13219]\tLoss: 962.6246\n",
      "Training Epoch: 5 [400/13219]\tLoss: 1000.0621\n",
      "Training Epoch: 5 [450/13219]\tLoss: 993.2714\n",
      "Training Epoch: 5 [500/13219]\tLoss: 1000.9603\n",
      "Training Epoch: 5 [550/13219]\tLoss: 930.2128\n",
      "Training Epoch: 5 [600/13219]\tLoss: 980.1755\n",
      "Training Epoch: 5 [650/13219]\tLoss: 920.3790\n",
      "Training Epoch: 5 [700/13219]\tLoss: 995.7992\n",
      "Training Epoch: 5 [750/13219]\tLoss: 966.4658\n",
      "Training Epoch: 5 [800/13219]\tLoss: 956.1693\n",
      "Training Epoch: 5 [850/13219]\tLoss: 987.0397\n",
      "Training Epoch: 5 [900/13219]\tLoss: 952.9748\n",
      "Training Epoch: 5 [950/13219]\tLoss: 975.1443\n",
      "Training Epoch: 5 [1000/13219]\tLoss: 989.2989\n",
      "Training Epoch: 5 [1050/13219]\tLoss: 960.0193\n",
      "Training Epoch: 5 [1100/13219]\tLoss: 1013.6948\n",
      "Training Epoch: 5 [1150/13219]\tLoss: 950.7618\n",
      "Training Epoch: 5 [1200/13219]\tLoss: 991.9894\n",
      "Training Epoch: 5 [1250/13219]\tLoss: 972.4570\n",
      "Training Epoch: 5 [1300/13219]\tLoss: 998.3311\n",
      "Training Epoch: 5 [1350/13219]\tLoss: 961.5797\n",
      "Training Epoch: 5 [1400/13219]\tLoss: 1023.8086\n",
      "Training Epoch: 5 [1450/13219]\tLoss: 975.6051\n",
      "Training Epoch: 5 [1500/13219]\tLoss: 987.5892\n",
      "Training Epoch: 5 [1550/13219]\tLoss: 980.0165\n",
      "Training Epoch: 5 [1600/13219]\tLoss: 920.9121\n",
      "Training Epoch: 5 [1650/13219]\tLoss: 975.9741\n",
      "Training Epoch: 5 [1700/13219]\tLoss: 979.4470\n",
      "Training Epoch: 5 [1750/13219]\tLoss: 997.2406\n",
      "Training Epoch: 5 [1800/13219]\tLoss: 994.7233\n",
      "Training Epoch: 5 [1850/13219]\tLoss: 983.2903\n",
      "Training Epoch: 5 [1900/13219]\tLoss: 1009.9560\n",
      "Training Epoch: 5 [1950/13219]\tLoss: 959.4811\n",
      "Training Epoch: 5 [2000/13219]\tLoss: 968.6238\n",
      "Training Epoch: 5 [2050/13219]\tLoss: 947.9119\n",
      "Training Epoch: 5 [2100/13219]\tLoss: 944.8569\n",
      "Training Epoch: 5 [2150/13219]\tLoss: 1002.1445\n",
      "Training Epoch: 5 [2200/13219]\tLoss: 980.2938\n",
      "Training Epoch: 5 [2250/13219]\tLoss: 977.5345\n",
      "Training Epoch: 5 [2300/13219]\tLoss: 989.0938\n",
      "Training Epoch: 5 [2350/13219]\tLoss: 988.3808\n",
      "Training Epoch: 5 [2400/13219]\tLoss: 959.4556\n",
      "Training Epoch: 5 [2450/13219]\tLoss: 974.8953\n",
      "Training Epoch: 5 [2500/13219]\tLoss: 978.9264\n",
      "Training Epoch: 5 [2550/13219]\tLoss: 961.9162\n",
      "Training Epoch: 5 [2600/13219]\tLoss: 979.1615\n",
      "Training Epoch: 5 [2650/13219]\tLoss: 976.6727\n",
      "Training Epoch: 5 [2700/13219]\tLoss: 982.5063\n",
      "Training Epoch: 5 [2750/13219]\tLoss: 928.9303\n",
      "Training Epoch: 5 [2800/13219]\tLoss: 1004.0851\n",
      "Training Epoch: 5 [2850/13219]\tLoss: 965.6575\n",
      "Training Epoch: 5 [2900/13219]\tLoss: 959.2062\n",
      "Training Epoch: 5 [2950/13219]\tLoss: 952.1275\n",
      "Training Epoch: 5 [3000/13219]\tLoss: 968.7676\n",
      "Training Epoch: 5 [3050/13219]\tLoss: 1015.9635\n",
      "Training Epoch: 5 [3100/13219]\tLoss: 1014.0548\n",
      "Training Epoch: 5 [3150/13219]\tLoss: 1003.0742\n",
      "Training Epoch: 5 [3200/13219]\tLoss: 969.0094\n",
      "Training Epoch: 5 [3250/13219]\tLoss: 929.3587\n",
      "Training Epoch: 5 [3300/13219]\tLoss: 964.8430\n",
      "Training Epoch: 5 [3350/13219]\tLoss: 989.1443\n",
      "Training Epoch: 5 [3400/13219]\tLoss: 967.7239\n",
      "Training Epoch: 5 [3450/13219]\tLoss: 955.5193\n",
      "Training Epoch: 5 [3500/13219]\tLoss: 938.7410\n",
      "Training Epoch: 5 [3550/13219]\tLoss: 992.2977\n",
      "Training Epoch: 5 [3600/13219]\tLoss: 984.9243\n",
      "Training Epoch: 5 [3650/13219]\tLoss: 968.5136\n",
      "Training Epoch: 5 [3700/13219]\tLoss: 960.9306\n",
      "Training Epoch: 5 [3750/13219]\tLoss: 975.8530\n",
      "Training Epoch: 5 [3800/13219]\tLoss: 954.4552\n",
      "Training Epoch: 5 [3850/13219]\tLoss: 955.3336\n",
      "Training Epoch: 5 [3900/13219]\tLoss: 926.7921\n",
      "Training Epoch: 5 [3950/13219]\tLoss: 935.3081\n",
      "Training Epoch: 5 [4000/13219]\tLoss: 1004.8712\n",
      "Training Epoch: 5 [4050/13219]\tLoss: 937.0104\n",
      "Training Epoch: 5 [4100/13219]\tLoss: 970.4047\n",
      "Training Epoch: 5 [4150/13219]\tLoss: 979.5484\n",
      "Training Epoch: 5 [4200/13219]\tLoss: 925.3086\n",
      "Training Epoch: 5 [4250/13219]\tLoss: 916.3662\n",
      "Training Epoch: 5 [4300/13219]\tLoss: 960.5423\n",
      "Training Epoch: 5 [4350/13219]\tLoss: 984.2120\n",
      "Training Epoch: 5 [4400/13219]\tLoss: 992.3759\n",
      "Training Epoch: 5 [4450/13219]\tLoss: 958.3090\n",
      "Training Epoch: 5 [4500/13219]\tLoss: 908.4538\n",
      "Training Epoch: 5 [4550/13219]\tLoss: 1000.1537\n",
      "Training Epoch: 5 [4600/13219]\tLoss: 953.3376\n",
      "Training Epoch: 5 [4650/13219]\tLoss: 949.7117\n",
      "Training Epoch: 5 [4700/13219]\tLoss: 906.6486\n",
      "Training Epoch: 5 [4750/13219]\tLoss: 952.2648\n",
      "Training Epoch: 5 [4800/13219]\tLoss: 987.8575\n",
      "Training Epoch: 5 [4850/13219]\tLoss: 962.8186\n",
      "Training Epoch: 5 [4900/13219]\tLoss: 944.3417\n",
      "Training Epoch: 5 [4950/13219]\tLoss: 955.2807\n",
      "Training Epoch: 5 [5000/13219]\tLoss: 960.9914\n",
      "Training Epoch: 5 [5050/13219]\tLoss: 984.8286\n",
      "Training Epoch: 5 [5100/13219]\tLoss: 955.6332\n",
      "Training Epoch: 5 [5150/13219]\tLoss: 963.4882\n",
      "Training Epoch: 5 [5200/13219]\tLoss: 982.5780\n",
      "Training Epoch: 5 [5250/13219]\tLoss: 950.6200\n",
      "Training Epoch: 5 [5300/13219]\tLoss: 921.5522\n",
      "Training Epoch: 5 [5350/13219]\tLoss: 968.1255\n",
      "Training Epoch: 5 [5400/13219]\tLoss: 943.3029\n",
      "Training Epoch: 5 [5450/13219]\tLoss: 953.9297\n",
      "Training Epoch: 5 [5500/13219]\tLoss: 980.9478\n",
      "Training Epoch: 5 [5550/13219]\tLoss: 904.7755\n",
      "Training Epoch: 5 [5600/13219]\tLoss: 958.7574\n",
      "Training Epoch: 5 [5650/13219]\tLoss: 955.8978\n",
      "Training Epoch: 5 [5700/13219]\tLoss: 906.2231\n",
      "Training Epoch: 5 [5750/13219]\tLoss: 974.3008\n",
      "Training Epoch: 5 [5800/13219]\tLoss: 937.0384\n",
      "Training Epoch: 5 [5850/13219]\tLoss: 930.5594\n",
      "Training Epoch: 5 [5900/13219]\tLoss: 954.3604\n",
      "Training Epoch: 5 [5950/13219]\tLoss: 938.8792\n",
      "Training Epoch: 5 [6000/13219]\tLoss: 924.3722\n",
      "Training Epoch: 5 [6050/13219]\tLoss: 956.6973\n",
      "Training Epoch: 5 [6100/13219]\tLoss: 971.3284\n",
      "Training Epoch: 5 [6150/13219]\tLoss: 924.0438\n",
      "Training Epoch: 5 [6200/13219]\tLoss: 967.6362\n",
      "Training Epoch: 5 [6250/13219]\tLoss: 980.1016\n",
      "Training Epoch: 5 [6300/13219]\tLoss: 916.1707\n",
      "Training Epoch: 5 [6350/13219]\tLoss: 884.3347\n",
      "Training Epoch: 5 [6400/13219]\tLoss: 903.7098\n",
      "Training Epoch: 5 [6450/13219]\tLoss: 959.4341\n",
      "Training Epoch: 5 [6500/13219]\tLoss: 951.7596\n",
      "Training Epoch: 5 [6550/13219]\tLoss: 970.7328\n",
      "Training Epoch: 5 [6600/13219]\tLoss: 977.8077\n",
      "Training Epoch: 5 [6650/13219]\tLoss: 985.8250\n",
      "Training Epoch: 5 [6700/13219]\tLoss: 945.0055\n",
      "Training Epoch: 5 [6750/13219]\tLoss: 975.5565\n",
      "Training Epoch: 5 [6800/13219]\tLoss: 969.3524\n",
      "Training Epoch: 5 [6850/13219]\tLoss: 942.3354\n",
      "Training Epoch: 5 [6900/13219]\tLoss: 995.3882\n",
      "Training Epoch: 5 [6950/13219]\tLoss: 932.4373\n",
      "Training Epoch: 5 [7000/13219]\tLoss: 937.7645\n",
      "Training Epoch: 5 [7050/13219]\tLoss: 909.5544\n",
      "Training Epoch: 5 [7100/13219]\tLoss: 936.7927\n",
      "Training Epoch: 5 [7150/13219]\tLoss: 937.3924\n",
      "Training Epoch: 5 [7200/13219]\tLoss: 923.7912\n",
      "Training Epoch: 5 [7250/13219]\tLoss: 997.4854\n",
      "Training Epoch: 5 [7300/13219]\tLoss: 999.3009\n",
      "Training Epoch: 5 [7350/13219]\tLoss: 992.2461\n",
      "Training Epoch: 5 [7400/13219]\tLoss: 936.4977\n",
      "Training Epoch: 5 [7450/13219]\tLoss: 958.3286\n",
      "Training Epoch: 5 [7500/13219]\tLoss: 948.6658\n",
      "Training Epoch: 5 [7550/13219]\tLoss: 946.8044\n",
      "Training Epoch: 5 [7600/13219]\tLoss: 946.9184\n",
      "Training Epoch: 5 [7650/13219]\tLoss: 934.4282\n",
      "Training Epoch: 5 [7700/13219]\tLoss: 989.6976\n",
      "Training Epoch: 5 [7750/13219]\tLoss: 918.0349\n",
      "Training Epoch: 5 [7800/13219]\tLoss: 912.7572\n",
      "Training Epoch: 5 [7850/13219]\tLoss: 925.8017\n",
      "Training Epoch: 5 [7900/13219]\tLoss: 951.5364\n",
      "Training Epoch: 5 [7950/13219]\tLoss: 978.5870\n",
      "Training Epoch: 5 [8000/13219]\tLoss: 924.5614\n",
      "Training Epoch: 5 [8050/13219]\tLoss: 928.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [8100/13219]\tLoss: 934.2198\n",
      "Training Epoch: 5 [8150/13219]\tLoss: 935.9357\n",
      "Training Epoch: 5 [8200/13219]\tLoss: 901.4794\n",
      "Training Epoch: 5 [8250/13219]\tLoss: 933.7513\n",
      "Training Epoch: 5 [8300/13219]\tLoss: 987.2938\n",
      "Training Epoch: 5 [8350/13219]\tLoss: 938.8389\n",
      "Training Epoch: 5 [8400/13219]\tLoss: 941.9653\n",
      "Training Epoch: 5 [8450/13219]\tLoss: 946.5894\n",
      "Training Epoch: 5 [8500/13219]\tLoss: 908.3687\n",
      "Training Epoch: 5 [8550/13219]\tLoss: 918.2588\n",
      "Training Epoch: 5 [8600/13219]\tLoss: 910.3162\n",
      "Training Epoch: 5 [8650/13219]\tLoss: 968.4758\n",
      "Training Epoch: 5 [8700/13219]\tLoss: 942.4081\n",
      "Training Epoch: 5 [8750/13219]\tLoss: 926.0640\n",
      "Training Epoch: 5 [8800/13219]\tLoss: 870.2958\n",
      "Training Epoch: 5 [8850/13219]\tLoss: 941.6176\n",
      "Training Epoch: 5 [8900/13219]\tLoss: 964.3077\n",
      "Training Epoch: 5 [8950/13219]\tLoss: 900.9124\n",
      "Training Epoch: 5 [9000/13219]\tLoss: 925.0699\n",
      "Training Epoch: 5 [9050/13219]\tLoss: 914.8190\n",
      "Training Epoch: 5 [9100/13219]\tLoss: 975.3777\n",
      "Training Epoch: 5 [9150/13219]\tLoss: 919.3097\n",
      "Training Epoch: 5 [9200/13219]\tLoss: 918.9830\n",
      "Training Epoch: 5 [9250/13219]\tLoss: 904.4498\n",
      "Training Epoch: 5 [9300/13219]\tLoss: 963.6277\n",
      "Training Epoch: 5 [9350/13219]\tLoss: 949.9480\n",
      "Training Epoch: 5 [9400/13219]\tLoss: 936.6458\n",
      "Training Epoch: 5 [9450/13219]\tLoss: 900.7767\n",
      "Training Epoch: 5 [9500/13219]\tLoss: 968.8024\n",
      "Training Epoch: 5 [9550/13219]\tLoss: 966.1351\n",
      "Training Epoch: 5 [9600/13219]\tLoss: 934.4707\n",
      "Training Epoch: 5 [9650/13219]\tLoss: 970.0261\n",
      "Training Epoch: 5 [9700/13219]\tLoss: 906.9199\n",
      "Training Epoch: 5 [9750/13219]\tLoss: 955.2347\n",
      "Training Epoch: 5 [9800/13219]\tLoss: 945.0172\n",
      "Training Epoch: 5 [9850/13219]\tLoss: 921.4268\n",
      "Training Epoch: 5 [9900/13219]\tLoss: 951.5714\n",
      "Training Epoch: 5 [9950/13219]\tLoss: 933.0818\n",
      "Training Epoch: 5 [10000/13219]\tLoss: 914.5388\n",
      "Training Epoch: 5 [10050/13219]\tLoss: 932.2322\n",
      "Training Epoch: 5 [10100/13219]\tLoss: 926.6419\n",
      "Training Epoch: 5 [10150/13219]\tLoss: 948.1429\n",
      "Training Epoch: 5 [10200/13219]\tLoss: 927.5629\n",
      "Training Epoch: 5 [10250/13219]\tLoss: 946.5274\n",
      "Training Epoch: 5 [10300/13219]\tLoss: 913.9207\n",
      "Training Epoch: 5 [10350/13219]\tLoss: 941.0281\n",
      "Training Epoch: 5 [10400/13219]\tLoss: 950.1283\n",
      "Training Epoch: 5 [10450/13219]\tLoss: 916.9514\n",
      "Training Epoch: 5 [10500/13219]\tLoss: 927.6933\n",
      "Training Epoch: 5 [10550/13219]\tLoss: 884.6342\n",
      "Training Epoch: 5 [10600/13219]\tLoss: 900.8049\n",
      "Training Epoch: 5 [10650/13219]\tLoss: 913.5379\n",
      "Training Epoch: 5 [10700/13219]\tLoss: 956.8433\n",
      "Training Epoch: 5 [10750/13219]\tLoss: 910.0118\n",
      "Training Epoch: 5 [10800/13219]\tLoss: 941.9781\n",
      "Training Epoch: 5 [10850/13219]\tLoss: 933.9330\n",
      "Training Epoch: 5 [10900/13219]\tLoss: 908.4738\n",
      "Training Epoch: 5 [10950/13219]\tLoss: 911.0712\n",
      "Training Epoch: 5 [11000/13219]\tLoss: 939.2225\n",
      "Training Epoch: 5 [11050/13219]\tLoss: 875.7853\n",
      "Training Epoch: 5 [11100/13219]\tLoss: 947.5246\n",
      "Training Epoch: 5 [11150/13219]\tLoss: 949.0169\n",
      "Training Epoch: 5 [11200/13219]\tLoss: 893.6328\n",
      "Training Epoch: 5 [11250/13219]\tLoss: 944.7568\n",
      "Training Epoch: 5 [11300/13219]\tLoss: 924.0909\n",
      "Training Epoch: 5 [11350/13219]\tLoss: 893.2668\n",
      "Training Epoch: 5 [11400/13219]\tLoss: 933.4800\n",
      "Training Epoch: 5 [11450/13219]\tLoss: 932.5060\n",
      "Training Epoch: 5 [11500/13219]\tLoss: 895.6887\n",
      "Training Epoch: 5 [11550/13219]\tLoss: 897.2944\n",
      "Training Epoch: 5 [11600/13219]\tLoss: 935.5334\n",
      "Training Epoch: 5 [11650/13219]\tLoss: 931.3003\n",
      "Training Epoch: 5 [11700/13219]\tLoss: 938.0042\n",
      "Training Epoch: 5 [11750/13219]\tLoss: 937.4334\n",
      "Training Epoch: 5 [11800/13219]\tLoss: 896.8986\n",
      "Training Epoch: 5 [11850/13219]\tLoss: 982.1390\n",
      "Training Epoch: 5 [11900/13219]\tLoss: 973.0193\n",
      "Training Epoch: 5 [11950/13219]\tLoss: 923.4695\n",
      "Training Epoch: 5 [12000/13219]\tLoss: 896.9113\n",
      "Training Epoch: 5 [12050/13219]\tLoss: 884.7969\n",
      "Training Epoch: 5 [12100/13219]\tLoss: 901.6984\n",
      "Training Epoch: 5 [12150/13219]\tLoss: 949.7432\n",
      "Training Epoch: 5 [12200/13219]\tLoss: 922.0587\n",
      "Training Epoch: 5 [12250/13219]\tLoss: 927.6479\n",
      "Training Epoch: 5 [12300/13219]\tLoss: 899.3097\n",
      "Training Epoch: 5 [12350/13219]\tLoss: 887.8799\n",
      "Training Epoch: 5 [12400/13219]\tLoss: 931.4898\n",
      "Training Epoch: 5 [12450/13219]\tLoss: 937.6444\n",
      "Training Epoch: 5 [12500/13219]\tLoss: 936.3040\n",
      "Training Epoch: 5 [12550/13219]\tLoss: 915.8697\n",
      "Training Epoch: 5 [12600/13219]\tLoss: 929.6335\n",
      "Training Epoch: 5 [12650/13219]\tLoss: 863.9127\n",
      "Training Epoch: 5 [12700/13219]\tLoss: 941.9181\n",
      "Training Epoch: 5 [12750/13219]\tLoss: 889.6710\n",
      "Training Epoch: 5 [12800/13219]\tLoss: 898.7211\n",
      "Training Epoch: 5 [12850/13219]\tLoss: 890.3372\n",
      "Training Epoch: 5 [12900/13219]\tLoss: 915.5856\n",
      "Training Epoch: 5 [12950/13219]\tLoss: 904.6239\n",
      "Training Epoch: 5 [13000/13219]\tLoss: 869.6295\n",
      "Training Epoch: 5 [13050/13219]\tLoss: 909.9659\n",
      "Training Epoch: 5 [13100/13219]\tLoss: 903.9793\n",
      "Training Epoch: 5 [13150/13219]\tLoss: 896.6871\n",
      "Training Epoch: 5 [13200/13219]\tLoss: 912.5959\n",
      "Training Epoch: 5 [13219/13219]\tLoss: 925.3536\n",
      "Training Epoch: 5 [1469/1469]\tLoss: 905.3995\n",
      "Training Epoch: 6 [50/13219]\tLoss: 880.7909\n",
      "Training Epoch: 6 [100/13219]\tLoss: 950.4418\n",
      "Training Epoch: 6 [150/13219]\tLoss: 893.0438\n",
      "Training Epoch: 6 [200/13219]\tLoss: 935.6574\n",
      "Training Epoch: 6 [250/13219]\tLoss: 916.9515\n",
      "Training Epoch: 6 [300/13219]\tLoss: 903.6591\n",
      "Training Epoch: 6 [350/13219]\tLoss: 893.5190\n",
      "Training Epoch: 6 [400/13219]\tLoss: 927.5354\n",
      "Training Epoch: 6 [450/13219]\tLoss: 899.0311\n",
      "Training Epoch: 6 [500/13219]\tLoss: 893.8799\n",
      "Training Epoch: 6 [550/13219]\tLoss: 875.1708\n",
      "Training Epoch: 6 [600/13219]\tLoss: 924.1127\n",
      "Training Epoch: 6 [650/13219]\tLoss: 881.2053\n",
      "Training Epoch: 6 [700/13219]\tLoss: 963.8242\n",
      "Training Epoch: 6 [750/13219]\tLoss: 930.0833\n",
      "Training Epoch: 6 [800/13219]\tLoss: 926.2031\n",
      "Training Epoch: 6 [850/13219]\tLoss: 940.3245\n",
      "Training Epoch: 6 [900/13219]\tLoss: 920.4072\n",
      "Training Epoch: 6 [950/13219]\tLoss: 873.7347\n",
      "Training Epoch: 6 [1000/13219]\tLoss: 922.2775\n",
      "Training Epoch: 6 [1050/13219]\tLoss: 907.4346\n",
      "Training Epoch: 6 [1100/13219]\tLoss: 922.5844\n",
      "Training Epoch: 6 [1150/13219]\tLoss: 910.0007\n",
      "Training Epoch: 6 [1200/13219]\tLoss: 908.3521\n",
      "Training Epoch: 6 [1250/13219]\tLoss: 846.1504\n",
      "Training Epoch: 6 [1300/13219]\tLoss: 913.6854\n",
      "Training Epoch: 6 [1350/13219]\tLoss: 957.2885\n",
      "Training Epoch: 6 [1400/13219]\tLoss: 839.6909\n",
      "Training Epoch: 6 [1450/13219]\tLoss: 890.3663\n",
      "Training Epoch: 6 [1500/13219]\tLoss: 869.9900\n",
      "Training Epoch: 6 [1550/13219]\tLoss: 942.9260\n",
      "Training Epoch: 6 [1600/13219]\tLoss: 874.3774\n",
      "Training Epoch: 6 [1650/13219]\tLoss: 891.0256\n",
      "Training Epoch: 6 [1700/13219]\tLoss: 897.9325\n",
      "Training Epoch: 6 [1750/13219]\tLoss: 909.0034\n",
      "Training Epoch: 6 [1800/13219]\tLoss: 963.2064\n",
      "Training Epoch: 6 [1850/13219]\tLoss: 940.3126\n",
      "Training Epoch: 6 [1900/13219]\tLoss: 920.0875\n",
      "Training Epoch: 6 [1950/13219]\tLoss: 917.5442\n",
      "Training Epoch: 6 [2000/13219]\tLoss: 923.7459\n",
      "Training Epoch: 6 [2050/13219]\tLoss: 916.7117\n",
      "Training Epoch: 6 [2100/13219]\tLoss: 945.3029\n",
      "Training Epoch: 6 [2150/13219]\tLoss: 892.0193\n",
      "Training Epoch: 6 [2200/13219]\tLoss: 920.7065\n",
      "Training Epoch: 6 [2250/13219]\tLoss: 908.3329\n",
      "Training Epoch: 6 [2300/13219]\tLoss: 931.4575\n",
      "Training Epoch: 6 [2350/13219]\tLoss: 926.9282\n",
      "Training Epoch: 6 [2400/13219]\tLoss: 930.3426\n",
      "Training Epoch: 6 [2450/13219]\tLoss: 883.6642\n",
      "Training Epoch: 6 [2500/13219]\tLoss: 900.0782\n",
      "Training Epoch: 6 [2550/13219]\tLoss: 907.5793\n",
      "Training Epoch: 6 [2600/13219]\tLoss: 903.3095\n",
      "Training Epoch: 6 [2650/13219]\tLoss: 876.0890\n",
      "Training Epoch: 6 [2700/13219]\tLoss: 854.0314\n",
      "Training Epoch: 6 [2750/13219]\tLoss: 868.2380\n",
      "Training Epoch: 6 [2800/13219]\tLoss: 915.6966\n",
      "Training Epoch: 6 [2850/13219]\tLoss: 945.0210\n",
      "Training Epoch: 6 [2900/13219]\tLoss: 962.1885\n",
      "Training Epoch: 6 [2950/13219]\tLoss: 888.9226\n",
      "Training Epoch: 6 [3000/13219]\tLoss: 918.6748\n",
      "Training Epoch: 6 [3050/13219]\tLoss: 929.4784\n",
      "Training Epoch: 6 [3100/13219]\tLoss: 905.6260\n",
      "Training Epoch: 6 [3150/13219]\tLoss: 916.1099\n",
      "Training Epoch: 6 [3200/13219]\tLoss: 890.9828\n",
      "Training Epoch: 6 [3250/13219]\tLoss: 880.1031\n",
      "Training Epoch: 6 [3300/13219]\tLoss: 907.4046\n",
      "Training Epoch: 6 [3350/13219]\tLoss: 922.4543\n",
      "Training Epoch: 6 [3400/13219]\tLoss: 911.2741\n",
      "Training Epoch: 6 [3450/13219]\tLoss: 981.8293\n",
      "Training Epoch: 6 [3500/13219]\tLoss: 913.0127\n",
      "Training Epoch: 6 [3550/13219]\tLoss: 927.8855\n",
      "Training Epoch: 6 [3600/13219]\tLoss: 899.6254\n",
      "Training Epoch: 6 [3650/13219]\tLoss: 905.4456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [3700/13219]\tLoss: 909.7663\n",
      "Training Epoch: 6 [3750/13219]\tLoss: 903.2748\n",
      "Training Epoch: 6 [3800/13219]\tLoss: 884.7264\n",
      "Training Epoch: 6 [3850/13219]\tLoss: 932.5129\n",
      "Training Epoch: 6 [3900/13219]\tLoss: 884.4041\n",
      "Training Epoch: 6 [3950/13219]\tLoss: 912.4421\n",
      "Training Epoch: 6 [4000/13219]\tLoss: 922.0784\n",
      "Training Epoch: 6 [4050/13219]\tLoss: 890.2385\n",
      "Training Epoch: 6 [4100/13219]\tLoss: 911.5116\n",
      "Training Epoch: 6 [4150/13219]\tLoss: 918.1044\n",
      "Training Epoch: 6 [4200/13219]\tLoss: 816.5418\n",
      "Training Epoch: 6 [4250/13219]\tLoss: 932.3583\n",
      "Training Epoch: 6 [4300/13219]\tLoss: 875.4907\n",
      "Training Epoch: 6 [4350/13219]\tLoss: 860.4087\n",
      "Training Epoch: 6 [4400/13219]\tLoss: 909.4656\n",
      "Training Epoch: 6 [4450/13219]\tLoss: 914.3690\n",
      "Training Epoch: 6 [4500/13219]\tLoss: 854.6987\n",
      "Training Epoch: 6 [4550/13219]\tLoss: 895.6014\n",
      "Training Epoch: 6 [4600/13219]\tLoss: 917.1588\n",
      "Training Epoch: 6 [4650/13219]\tLoss: 911.4146\n",
      "Training Epoch: 6 [4700/13219]\tLoss: 909.3754\n",
      "Training Epoch: 6 [4750/13219]\tLoss: 912.3582\n",
      "Training Epoch: 6 [4800/13219]\tLoss: 873.6905\n",
      "Training Epoch: 6 [4850/13219]\tLoss: 909.8619\n",
      "Training Epoch: 6 [4900/13219]\tLoss: 872.0059\n",
      "Training Epoch: 6 [4950/13219]\tLoss: 898.2140\n",
      "Training Epoch: 6 [5000/13219]\tLoss: 889.7410\n",
      "Training Epoch: 6 [5050/13219]\tLoss: 875.2767\n",
      "Training Epoch: 6 [5100/13219]\tLoss: 925.0338\n",
      "Training Epoch: 6 [5150/13219]\tLoss: 907.8185\n",
      "Training Epoch: 6 [5200/13219]\tLoss: 920.5276\n",
      "Training Epoch: 6 [5250/13219]\tLoss: 918.9125\n",
      "Training Epoch: 6 [5300/13219]\tLoss: 878.8800\n",
      "Training Epoch: 6 [5350/13219]\tLoss: 888.6078\n",
      "Training Epoch: 6 [5400/13219]\tLoss: 884.3140\n",
      "Training Epoch: 6 [5450/13219]\tLoss: 884.0100\n",
      "Training Epoch: 6 [5500/13219]\tLoss: 935.1143\n",
      "Training Epoch: 6 [5550/13219]\tLoss: 867.6635\n",
      "Training Epoch: 6 [5600/13219]\tLoss: 910.5125\n",
      "Training Epoch: 6 [5650/13219]\tLoss: 920.0212\n",
      "Training Epoch: 6 [5700/13219]\tLoss: 920.4653\n",
      "Training Epoch: 6 [5750/13219]\tLoss: 924.2554\n",
      "Training Epoch: 6 [5800/13219]\tLoss: 889.1615\n",
      "Training Epoch: 6 [5850/13219]\tLoss: 844.2667\n",
      "Training Epoch: 6 [5900/13219]\tLoss: 883.6025\n",
      "Training Epoch: 6 [5950/13219]\tLoss: 834.5232\n",
      "Training Epoch: 6 [6000/13219]\tLoss: 908.8441\n",
      "Training Epoch: 6 [6050/13219]\tLoss: 915.3254\n",
      "Training Epoch: 6 [6100/13219]\tLoss: 895.1295\n",
      "Training Epoch: 6 [6150/13219]\tLoss: 893.0194\n",
      "Training Epoch: 6 [6200/13219]\tLoss: 885.1664\n",
      "Training Epoch: 6 [6250/13219]\tLoss: 914.3288\n",
      "Training Epoch: 6 [6300/13219]\tLoss: 875.9712\n",
      "Training Epoch: 6 [6350/13219]\tLoss: 890.7365\n",
      "Training Epoch: 6 [6400/13219]\tLoss: 891.0150\n",
      "Training Epoch: 6 [6450/13219]\tLoss: 858.5245\n",
      "Training Epoch: 6 [6500/13219]\tLoss: 872.8206\n",
      "Training Epoch: 6 [6550/13219]\tLoss: 936.2759\n",
      "Training Epoch: 6 [6600/13219]\tLoss: 913.6941\n",
      "Training Epoch: 6 [6650/13219]\tLoss: 855.6312\n",
      "Training Epoch: 6 [6700/13219]\tLoss: 864.6224\n",
      "Training Epoch: 6 [6750/13219]\tLoss: 921.5626\n",
      "Training Epoch: 6 [6800/13219]\tLoss: 885.7320\n",
      "Training Epoch: 6 [6850/13219]\tLoss: 891.2707\n",
      "Training Epoch: 6 [6900/13219]\tLoss: 873.1089\n",
      "Training Epoch: 6 [6950/13219]\tLoss: 903.9214\n",
      "Training Epoch: 6 [7000/13219]\tLoss: 891.9985\n",
      "Training Epoch: 6 [7050/13219]\tLoss: 872.3820\n",
      "Training Epoch: 6 [7100/13219]\tLoss: 883.1497\n",
      "Training Epoch: 6 [7150/13219]\tLoss: 896.2703\n",
      "Training Epoch: 6 [7200/13219]\tLoss: 911.9479\n",
      "Training Epoch: 6 [7250/13219]\tLoss: 866.5324\n",
      "Training Epoch: 6 [7300/13219]\tLoss: 904.9435\n",
      "Training Epoch: 6 [7350/13219]\tLoss: 899.6283\n",
      "Training Epoch: 6 [7400/13219]\tLoss: 862.5865\n",
      "Training Epoch: 6 [7450/13219]\tLoss: 884.5732\n",
      "Training Epoch: 6 [7500/13219]\tLoss: 866.7464\n",
      "Training Epoch: 6 [7550/13219]\tLoss: 906.8430\n",
      "Training Epoch: 6 [7600/13219]\tLoss: 882.1663\n",
      "Training Epoch: 6 [7650/13219]\tLoss: 864.8425\n",
      "Training Epoch: 6 [7700/13219]\tLoss: 826.0537\n",
      "Training Epoch: 6 [7750/13219]\tLoss: 887.4327\n",
      "Training Epoch: 6 [7800/13219]\tLoss: 846.6711\n",
      "Training Epoch: 6 [7850/13219]\tLoss: 906.3308\n",
      "Training Epoch: 6 [7900/13219]\tLoss: 847.1271\n",
      "Training Epoch: 6 [7950/13219]\tLoss: 878.1454\n",
      "Training Epoch: 6 [8000/13219]\tLoss: 861.6214\n",
      "Training Epoch: 6 [8050/13219]\tLoss: 911.6487\n",
      "Training Epoch: 6 [8100/13219]\tLoss: 889.6334\n",
      "Training Epoch: 6 [8150/13219]\tLoss: 842.9669\n",
      "Training Epoch: 6 [8200/13219]\tLoss: 912.1664\n",
      "Training Epoch: 6 [8250/13219]\tLoss: 893.5052\n",
      "Training Epoch: 6 [8300/13219]\tLoss: 901.2345\n",
      "Training Epoch: 6 [8350/13219]\tLoss: 899.8058\n",
      "Training Epoch: 6 [8400/13219]\tLoss: 879.8552\n",
      "Training Epoch: 6 [8450/13219]\tLoss: 912.7833\n",
      "Training Epoch: 6 [8500/13219]\tLoss: 923.5003\n",
      "Training Epoch: 6 [8550/13219]\tLoss: 924.1862\n",
      "Training Epoch: 6 [8600/13219]\tLoss: 870.9200\n",
      "Training Epoch: 6 [8650/13219]\tLoss: 882.3168\n",
      "Training Epoch: 6 [8700/13219]\tLoss: 878.2376\n",
      "Training Epoch: 6 [8750/13219]\tLoss: 891.9713\n",
      "Training Epoch: 6 [8800/13219]\tLoss: 889.7859\n",
      "Training Epoch: 6 [8850/13219]\tLoss: 895.2286\n",
      "Training Epoch: 6 [8900/13219]\tLoss: 928.4966\n",
      "Training Epoch: 6 [8950/13219]\tLoss: 949.1716\n",
      "Training Epoch: 6 [9000/13219]\tLoss: 893.9445\n",
      "Training Epoch: 6 [9050/13219]\tLoss: 871.2106\n",
      "Training Epoch: 6 [9100/13219]\tLoss: 900.7416\n",
      "Training Epoch: 6 [9150/13219]\tLoss: 901.3491\n",
      "Training Epoch: 6 [9200/13219]\tLoss: 888.8249\n",
      "Training Epoch: 6 [9250/13219]\tLoss: 858.2062\n",
      "Training Epoch: 6 [9300/13219]\tLoss: 885.4526\n",
      "Training Epoch: 6 [9350/13219]\tLoss: 890.3554\n",
      "Training Epoch: 6 [9400/13219]\tLoss: 846.4446\n",
      "Training Epoch: 6 [9450/13219]\tLoss: 853.0242\n",
      "Training Epoch: 6 [9500/13219]\tLoss: 857.6478\n",
      "Training Epoch: 6 [9550/13219]\tLoss: 867.6522\n",
      "Training Epoch: 6 [9600/13219]\tLoss: 846.7646\n",
      "Training Epoch: 6 [9650/13219]\tLoss: 881.6435\n",
      "Training Epoch: 6 [9700/13219]\tLoss: 892.9069\n",
      "Training Epoch: 6 [9750/13219]\tLoss: 845.0034\n",
      "Training Epoch: 6 [9800/13219]\tLoss: 916.7872\n",
      "Training Epoch: 6 [9850/13219]\tLoss: 907.9983\n",
      "Training Epoch: 6 [9900/13219]\tLoss: 838.2477\n",
      "Training Epoch: 6 [9950/13219]\tLoss: 880.7230\n",
      "Training Epoch: 6 [10000/13219]\tLoss: 909.2729\n",
      "Training Epoch: 6 [10050/13219]\tLoss: 839.5883\n",
      "Training Epoch: 6 [10100/13219]\tLoss: 855.0562\n",
      "Training Epoch: 6 [10150/13219]\tLoss: 870.6575\n",
      "Training Epoch: 6 [10200/13219]\tLoss: 840.8857\n",
      "Training Epoch: 6 [10250/13219]\tLoss: 877.5790\n",
      "Training Epoch: 6 [10300/13219]\tLoss: 872.0159\n",
      "Training Epoch: 6 [10350/13219]\tLoss: 912.6266\n",
      "Training Epoch: 6 [10400/13219]\tLoss: 876.5656\n",
      "Training Epoch: 6 [10450/13219]\tLoss: 937.3979\n",
      "Training Epoch: 6 [10500/13219]\tLoss: 887.0789\n",
      "Training Epoch: 6 [10550/13219]\tLoss: 839.0794\n",
      "Training Epoch: 6 [10600/13219]\tLoss: 854.4380\n",
      "Training Epoch: 6 [10650/13219]\tLoss: 891.6740\n",
      "Training Epoch: 6 [10700/13219]\tLoss: 811.8376\n",
      "Training Epoch: 6 [10750/13219]\tLoss: 865.2023\n",
      "Training Epoch: 6 [10800/13219]\tLoss: 912.9931\n",
      "Training Epoch: 6 [10850/13219]\tLoss: 845.6889\n",
      "Training Epoch: 6 [10900/13219]\tLoss: 865.4855\n",
      "Training Epoch: 6 [10950/13219]\tLoss: 891.6129\n",
      "Training Epoch: 6 [11000/13219]\tLoss: 884.8823\n",
      "Training Epoch: 6 [11050/13219]\tLoss: 890.2384\n",
      "Training Epoch: 6 [11100/13219]\tLoss: 869.4377\n",
      "Training Epoch: 6 [11150/13219]\tLoss: 896.7787\n",
      "Training Epoch: 6 [11200/13219]\tLoss: 837.3160\n",
      "Training Epoch: 6 [11250/13219]\tLoss: 860.6758\n",
      "Training Epoch: 6 [11300/13219]\tLoss: 904.6368\n",
      "Training Epoch: 6 [11350/13219]\tLoss: 901.0885\n",
      "Training Epoch: 6 [11400/13219]\tLoss: 860.6358\n",
      "Training Epoch: 6 [11450/13219]\tLoss: 859.7464\n",
      "Training Epoch: 6 [11500/13219]\tLoss: 919.3678\n",
      "Training Epoch: 6 [11550/13219]\tLoss: 930.5665\n",
      "Training Epoch: 6 [11600/13219]\tLoss: 872.8609\n",
      "Training Epoch: 6 [11650/13219]\tLoss: 882.3346\n",
      "Training Epoch: 6 [11700/13219]\tLoss: 856.6149\n",
      "Training Epoch: 6 [11750/13219]\tLoss: 909.8149\n",
      "Training Epoch: 6 [11800/13219]\tLoss: 848.9653\n",
      "Training Epoch: 6 [11850/13219]\tLoss: 879.9767\n",
      "Training Epoch: 6 [11900/13219]\tLoss: 868.2106\n",
      "Training Epoch: 6 [11950/13219]\tLoss: 902.7881\n",
      "Training Epoch: 6 [12000/13219]\tLoss: 871.6257\n",
      "Training Epoch: 6 [12050/13219]\tLoss: 859.3743\n",
      "Training Epoch: 6 [12100/13219]\tLoss: 854.7523\n",
      "Training Epoch: 6 [12150/13219]\tLoss: 849.0144\n",
      "Training Epoch: 6 [12200/13219]\tLoss: 856.4724\n",
      "Training Epoch: 6 [12250/13219]\tLoss: 894.4306\n",
      "Training Epoch: 6 [12300/13219]\tLoss: 860.4833\n",
      "Training Epoch: 6 [12350/13219]\tLoss: 835.8754\n",
      "Training Epoch: 6 [12400/13219]\tLoss: 867.3365\n",
      "Training Epoch: 6 [12450/13219]\tLoss: 892.6826\n",
      "Training Epoch: 6 [12500/13219]\tLoss: 873.3965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [12550/13219]\tLoss: 878.6816\n",
      "Training Epoch: 6 [12600/13219]\tLoss: 887.7422\n",
      "Training Epoch: 6 [12650/13219]\tLoss: 890.3297\n",
      "Training Epoch: 6 [12700/13219]\tLoss: 859.9435\n",
      "Training Epoch: 6 [12750/13219]\tLoss: 862.0797\n",
      "Training Epoch: 6 [12800/13219]\tLoss: 879.7292\n",
      "Training Epoch: 6 [12850/13219]\tLoss: 905.7179\n",
      "Training Epoch: 6 [12900/13219]\tLoss: 861.3505\n",
      "Training Epoch: 6 [12950/13219]\tLoss: 889.9720\n",
      "Training Epoch: 6 [13000/13219]\tLoss: 863.8606\n",
      "Training Epoch: 6 [13050/13219]\tLoss: 885.1263\n",
      "Training Epoch: 6 [13100/13219]\tLoss: 883.3953\n",
      "Training Epoch: 6 [13150/13219]\tLoss: 836.2928\n",
      "Training Epoch: 6 [13200/13219]\tLoss: 864.9023\n",
      "Training Epoch: 6 [13219/13219]\tLoss: 850.6714\n",
      "Training Epoch: 6 [1469/1469]\tLoss: 862.0853\n",
      "Training Epoch: 7 [50/13219]\tLoss: 825.7457\n",
      "Training Epoch: 7 [100/13219]\tLoss: 873.2762\n",
      "Training Epoch: 7 [150/13219]\tLoss: 844.6437\n",
      "Training Epoch: 7 [200/13219]\tLoss: 855.6839\n",
      "Training Epoch: 7 [250/13219]\tLoss: 845.9752\n",
      "Training Epoch: 7 [300/13219]\tLoss: 895.3461\n",
      "Training Epoch: 7 [350/13219]\tLoss: 874.5350\n",
      "Training Epoch: 7 [400/13219]\tLoss: 861.1327\n",
      "Training Epoch: 7 [450/13219]\tLoss: 915.4578\n",
      "Training Epoch: 7 [500/13219]\tLoss: 897.8386\n",
      "Training Epoch: 7 [550/13219]\tLoss: 922.6072\n",
      "Training Epoch: 7 [600/13219]\tLoss: 853.7206\n",
      "Training Epoch: 7 [650/13219]\tLoss: 921.2986\n",
      "Training Epoch: 7 [700/13219]\tLoss: 893.2188\n",
      "Training Epoch: 7 [750/13219]\tLoss: 825.9769\n",
      "Training Epoch: 7 [800/13219]\tLoss: 882.1395\n",
      "Training Epoch: 7 [850/13219]\tLoss: 893.5560\n",
      "Training Epoch: 7 [900/13219]\tLoss: 874.1447\n",
      "Training Epoch: 7 [950/13219]\tLoss: 855.7117\n",
      "Training Epoch: 7 [1000/13219]\tLoss: 863.6937\n",
      "Training Epoch: 7 [1050/13219]\tLoss: 816.6108\n",
      "Training Epoch: 7 [1100/13219]\tLoss: 856.1043\n",
      "Training Epoch: 7 [1150/13219]\tLoss: 909.0092\n",
      "Training Epoch: 7 [1200/13219]\tLoss: 845.2610\n",
      "Training Epoch: 7 [1250/13219]\tLoss: 855.7358\n",
      "Training Epoch: 7 [1300/13219]\tLoss: 860.2043\n",
      "Training Epoch: 7 [1350/13219]\tLoss: 837.4269\n",
      "Training Epoch: 7 [1400/13219]\tLoss: 861.3016\n",
      "Training Epoch: 7 [1450/13219]\tLoss: 906.0392\n",
      "Training Epoch: 7 [1500/13219]\tLoss: 820.6586\n",
      "Training Epoch: 7 [1550/13219]\tLoss: 885.1263\n",
      "Training Epoch: 7 [1600/13219]\tLoss: 859.0112\n",
      "Training Epoch: 7 [1650/13219]\tLoss: 881.1077\n",
      "Training Epoch: 7 [1700/13219]\tLoss: 838.9539\n",
      "Training Epoch: 7 [1750/13219]\tLoss: 822.8949\n",
      "Training Epoch: 7 [1800/13219]\tLoss: 851.1997\n",
      "Training Epoch: 7 [1850/13219]\tLoss: 870.1330\n",
      "Training Epoch: 7 [1900/13219]\tLoss: 863.0004\n",
      "Training Epoch: 7 [1950/13219]\tLoss: 896.1240\n",
      "Training Epoch: 7 [2000/13219]\tLoss: 897.3347\n",
      "Training Epoch: 7 [2050/13219]\tLoss: 869.1403\n",
      "Training Epoch: 7 [2100/13219]\tLoss: 902.7715\n",
      "Training Epoch: 7 [2150/13219]\tLoss: 865.8514\n",
      "Training Epoch: 7 [2200/13219]\tLoss: 925.8688\n",
      "Training Epoch: 7 [2250/13219]\tLoss: 861.7997\n",
      "Training Epoch: 7 [2300/13219]\tLoss: 871.4874\n",
      "Training Epoch: 7 [2350/13219]\tLoss: 863.5923\n",
      "Training Epoch: 7 [2400/13219]\tLoss: 877.2004\n",
      "Training Epoch: 7 [2450/13219]\tLoss: 859.1490\n",
      "Training Epoch: 7 [2500/13219]\tLoss: 831.1639\n",
      "Training Epoch: 7 [2550/13219]\tLoss: 870.0740\n",
      "Training Epoch: 7 [2600/13219]\tLoss: 826.4653\n",
      "Training Epoch: 7 [2650/13219]\tLoss: 860.2693\n",
      "Training Epoch: 7 [2700/13219]\tLoss: 906.8392\n",
      "Training Epoch: 7 [2750/13219]\tLoss: 837.2397\n",
      "Training Epoch: 7 [2800/13219]\tLoss: 863.6797\n",
      "Training Epoch: 7 [2850/13219]\tLoss: 859.7727\n",
      "Training Epoch: 7 [2900/13219]\tLoss: 847.7955\n",
      "Training Epoch: 7 [2950/13219]\tLoss: 855.6932\n",
      "Training Epoch: 7 [3000/13219]\tLoss: 867.5900\n",
      "Training Epoch: 7 [3050/13219]\tLoss: 857.5400\n",
      "Training Epoch: 7 [3100/13219]\tLoss: 874.4148\n",
      "Training Epoch: 7 [3150/13219]\tLoss: 842.5576\n",
      "Training Epoch: 7 [3200/13219]\tLoss: 860.7661\n",
      "Training Epoch: 7 [3250/13219]\tLoss: 868.9504\n",
      "Training Epoch: 7 [3300/13219]\tLoss: 880.1666\n",
      "Training Epoch: 7 [3350/13219]\tLoss: 861.2871\n",
      "Training Epoch: 7 [3400/13219]\tLoss: 834.6637\n",
      "Training Epoch: 7 [3450/13219]\tLoss: 847.1960\n",
      "Training Epoch: 7 [3500/13219]\tLoss: 859.4766\n",
      "Training Epoch: 7 [3550/13219]\tLoss: 886.2750\n",
      "Training Epoch: 7 [3600/13219]\tLoss: 863.8290\n",
      "Training Epoch: 7 [3650/13219]\tLoss: 894.3674\n",
      "Training Epoch: 7 [3700/13219]\tLoss: 818.5314\n",
      "Training Epoch: 7 [3750/13219]\tLoss: 836.7551\n",
      "Training Epoch: 7 [3800/13219]\tLoss: 831.1625\n",
      "Training Epoch: 7 [3850/13219]\tLoss: 874.7576\n",
      "Training Epoch: 7 [3900/13219]\tLoss: 854.2896\n",
      "Training Epoch: 7 [3950/13219]\tLoss: 880.4816\n",
      "Training Epoch: 7 [4000/13219]\tLoss: 847.9226\n",
      "Training Epoch: 7 [4050/13219]\tLoss: 859.3414\n",
      "Training Epoch: 7 [4100/13219]\tLoss: 859.4752\n",
      "Training Epoch: 7 [4150/13219]\tLoss: 870.2978\n",
      "Training Epoch: 7 [4200/13219]\tLoss: 890.8398\n",
      "Training Epoch: 7 [4250/13219]\tLoss: 830.7419\n",
      "Training Epoch: 7 [4300/13219]\tLoss: 912.2165\n",
      "Training Epoch: 7 [4350/13219]\tLoss: 867.5486\n",
      "Training Epoch: 7 [4400/13219]\tLoss: 834.2308\n",
      "Training Epoch: 7 [4450/13219]\tLoss: 805.4396\n",
      "Training Epoch: 7 [4500/13219]\tLoss: 898.8636\n",
      "Training Epoch: 7 [4550/13219]\tLoss: 843.0112\n",
      "Training Epoch: 7 [4600/13219]\tLoss: 896.2867\n",
      "Training Epoch: 7 [4650/13219]\tLoss: 836.5926\n",
      "Training Epoch: 7 [4700/13219]\tLoss: 880.1500\n",
      "Training Epoch: 7 [4750/13219]\tLoss: 880.1893\n",
      "Training Epoch: 7 [4800/13219]\tLoss: 809.2844\n",
      "Training Epoch: 7 [4850/13219]\tLoss: 860.6036\n",
      "Training Epoch: 7 [4900/13219]\tLoss: 871.6599\n",
      "Training Epoch: 7 [4950/13219]\tLoss: 866.2160\n",
      "Training Epoch: 7 [5000/13219]\tLoss: 903.3272\n",
      "Training Epoch: 7 [5050/13219]\tLoss: 867.9731\n",
      "Training Epoch: 7 [5100/13219]\tLoss: 867.0529\n",
      "Training Epoch: 7 [5150/13219]\tLoss: 822.9559\n",
      "Training Epoch: 7 [5200/13219]\tLoss: 830.3047\n",
      "Training Epoch: 7 [5250/13219]\tLoss: 856.2963\n",
      "Training Epoch: 7 [5300/13219]\tLoss: 860.5935\n",
      "Training Epoch: 7 [5350/13219]\tLoss: 922.0130\n",
      "Training Epoch: 7 [5400/13219]\tLoss: 851.1978\n",
      "Training Epoch: 7 [5450/13219]\tLoss: 836.5350\n",
      "Training Epoch: 7 [5500/13219]\tLoss: 845.3546\n",
      "Training Epoch: 7 [5550/13219]\tLoss: 861.1672\n",
      "Training Epoch: 7 [5600/13219]\tLoss: 878.1562\n",
      "Training Epoch: 7 [5650/13219]\tLoss: 827.2217\n",
      "Training Epoch: 7 [5700/13219]\tLoss: 872.5794\n",
      "Training Epoch: 7 [5750/13219]\tLoss: 822.0918\n",
      "Training Epoch: 7 [5800/13219]\tLoss: 875.8419\n",
      "Training Epoch: 7 [5850/13219]\tLoss: 821.7457\n",
      "Training Epoch: 7 [5900/13219]\tLoss: 856.6059\n",
      "Training Epoch: 7 [5950/13219]\tLoss: 838.9155\n",
      "Training Epoch: 7 [6000/13219]\tLoss: 881.2229\n",
      "Training Epoch: 7 [6050/13219]\tLoss: 822.9227\n",
      "Training Epoch: 7 [6100/13219]\tLoss: 842.6316\n",
      "Training Epoch: 7 [6150/13219]\tLoss: 904.2690\n",
      "Training Epoch: 7 [6200/13219]\tLoss: 896.6401\n",
      "Training Epoch: 7 [6250/13219]\tLoss: 839.0571\n",
      "Training Epoch: 7 [6300/13219]\tLoss: 837.2844\n",
      "Training Epoch: 7 [6350/13219]\tLoss: 874.1710\n",
      "Training Epoch: 7 [6400/13219]\tLoss: 858.7708\n",
      "Training Epoch: 7 [6450/13219]\tLoss: 874.2255\n",
      "Training Epoch: 7 [6500/13219]\tLoss: 861.3628\n",
      "Training Epoch: 7 [6550/13219]\tLoss: 897.9507\n",
      "Training Epoch: 7 [6600/13219]\tLoss: 880.8140\n",
      "Training Epoch: 7 [6650/13219]\tLoss: 820.8682\n",
      "Training Epoch: 7 [6700/13219]\tLoss: 838.5137\n",
      "Training Epoch: 7 [6750/13219]\tLoss: 854.8437\n",
      "Training Epoch: 7 [6800/13219]\tLoss: 841.5668\n",
      "Training Epoch: 7 [6850/13219]\tLoss: 842.7454\n",
      "Training Epoch: 7 [6900/13219]\tLoss: 827.6099\n",
      "Training Epoch: 7 [6950/13219]\tLoss: 835.4274\n",
      "Training Epoch: 7 [7000/13219]\tLoss: 861.0558\n",
      "Training Epoch: 7 [7050/13219]\tLoss: 843.7350\n",
      "Training Epoch: 7 [7100/13219]\tLoss: 845.0447\n",
      "Training Epoch: 7 [7150/13219]\tLoss: 851.5753\n",
      "Training Epoch: 7 [7200/13219]\tLoss: 846.2410\n",
      "Training Epoch: 7 [7250/13219]\tLoss: 836.4315\n",
      "Training Epoch: 7 [7300/13219]\tLoss: 836.8935\n",
      "Training Epoch: 7 [7350/13219]\tLoss: 844.4843\n",
      "Training Epoch: 7 [7400/13219]\tLoss: 834.8578\n",
      "Training Epoch: 7 [7450/13219]\tLoss: 800.2373\n",
      "Training Epoch: 7 [7500/13219]\tLoss: 847.5147\n",
      "Training Epoch: 7 [7550/13219]\tLoss: 879.8127\n",
      "Training Epoch: 7 [7600/13219]\tLoss: 805.8479\n",
      "Training Epoch: 7 [7650/13219]\tLoss: 848.9022\n",
      "Training Epoch: 7 [7700/13219]\tLoss: 852.5854\n",
      "Training Epoch: 7 [7750/13219]\tLoss: 863.3225\n",
      "Training Epoch: 7 [7800/13219]\tLoss: 846.7391\n",
      "Training Epoch: 7 [7850/13219]\tLoss: 859.9780\n",
      "Training Epoch: 7 [7900/13219]\tLoss: 876.5200\n",
      "Training Epoch: 7 [7950/13219]\tLoss: 845.0458\n",
      "Training Epoch: 7 [8000/13219]\tLoss: 917.3508\n",
      "Training Epoch: 7 [8050/13219]\tLoss: 866.9520\n",
      "Training Epoch: 7 [8100/13219]\tLoss: 873.4335\n",
      "Training Epoch: 7 [8150/13219]\tLoss: 861.7036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [8200/13219]\tLoss: 877.0861\n",
      "Training Epoch: 7 [8250/13219]\tLoss: 830.9686\n",
      "Training Epoch: 7 [8300/13219]\tLoss: 843.7876\n",
      "Training Epoch: 7 [8350/13219]\tLoss: 825.0164\n",
      "Training Epoch: 7 [8400/13219]\tLoss: 861.1598\n",
      "Training Epoch: 7 [8450/13219]\tLoss: 847.4582\n",
      "Training Epoch: 7 [8500/13219]\tLoss: 827.4771\n",
      "Training Epoch: 7 [8550/13219]\tLoss: 893.2391\n",
      "Training Epoch: 7 [8600/13219]\tLoss: 873.8938\n",
      "Training Epoch: 7 [8650/13219]\tLoss: 877.3271\n",
      "Training Epoch: 7 [8700/13219]\tLoss: 885.0537\n",
      "Training Epoch: 7 [8750/13219]\tLoss: 841.6786\n",
      "Training Epoch: 7 [8800/13219]\tLoss: 858.4250\n",
      "Training Epoch: 7 [8850/13219]\tLoss: 825.0958\n",
      "Training Epoch: 7 [8900/13219]\tLoss: 812.2264\n",
      "Training Epoch: 7 [8950/13219]\tLoss: 881.5654\n",
      "Training Epoch: 7 [9000/13219]\tLoss: 853.2955\n",
      "Training Epoch: 7 [9050/13219]\tLoss: 827.4947\n",
      "Training Epoch: 7 [9100/13219]\tLoss: 870.2894\n",
      "Training Epoch: 7 [9150/13219]\tLoss: 821.1812\n",
      "Training Epoch: 7 [9200/13219]\tLoss: 815.1049\n",
      "Training Epoch: 7 [9250/13219]\tLoss: 876.7934\n",
      "Training Epoch: 7 [9300/13219]\tLoss: 884.3666\n",
      "Training Epoch: 7 [9350/13219]\tLoss: 843.3096\n",
      "Training Epoch: 7 [9400/13219]\tLoss: 931.0175\n",
      "Training Epoch: 7 [9450/13219]\tLoss: 823.4979\n",
      "Training Epoch: 7 [9500/13219]\tLoss: 852.3728\n",
      "Training Epoch: 7 [9550/13219]\tLoss: 834.5071\n",
      "Training Epoch: 7 [9600/13219]\tLoss: 844.2421\n",
      "Training Epoch: 7 [9650/13219]\tLoss: 852.8327\n",
      "Training Epoch: 7 [9700/13219]\tLoss: 850.5081\n",
      "Training Epoch: 7 [9750/13219]\tLoss: 857.9693\n",
      "Training Epoch: 7 [9800/13219]\tLoss: 831.1101\n",
      "Training Epoch: 7 [9850/13219]\tLoss: 848.9295\n",
      "Training Epoch: 7 [9900/13219]\tLoss: 832.5490\n",
      "Training Epoch: 7 [9950/13219]\tLoss: 862.6660\n",
      "Training Epoch: 7 [10000/13219]\tLoss: 825.8196\n",
      "Training Epoch: 7 [10050/13219]\tLoss: 858.9399\n",
      "Training Epoch: 7 [10100/13219]\tLoss: 826.1582\n",
      "Training Epoch: 7 [10150/13219]\tLoss: 867.2466\n",
      "Training Epoch: 7 [10200/13219]\tLoss: 835.0562\n",
      "Training Epoch: 7 [10250/13219]\tLoss: 823.9407\n",
      "Training Epoch: 7 [10300/13219]\tLoss: 801.6238\n",
      "Training Epoch: 7 [10350/13219]\tLoss: 836.0273\n",
      "Training Epoch: 7 [10400/13219]\tLoss: 828.3896\n",
      "Training Epoch: 7 [10450/13219]\tLoss: 826.6403\n",
      "Training Epoch: 7 [10500/13219]\tLoss: 842.8834\n",
      "Training Epoch: 7 [10550/13219]\tLoss: 893.5306\n",
      "Training Epoch: 7 [10600/13219]\tLoss: 875.1643\n",
      "Training Epoch: 7 [10650/13219]\tLoss: 849.2264\n",
      "Training Epoch: 7 [10700/13219]\tLoss: 899.9735\n",
      "Training Epoch: 7 [10750/13219]\tLoss: 835.9802\n",
      "Training Epoch: 7 [10800/13219]\tLoss: 861.0322\n",
      "Training Epoch: 7 [10850/13219]\tLoss: 838.8120\n",
      "Training Epoch: 7 [10900/13219]\tLoss: 821.5074\n",
      "Training Epoch: 7 [10950/13219]\tLoss: 817.2051\n",
      "Training Epoch: 7 [11000/13219]\tLoss: 836.2296\n",
      "Training Epoch: 7 [11050/13219]\tLoss: 869.7573\n",
      "Training Epoch: 7 [11100/13219]\tLoss: 838.3225\n",
      "Training Epoch: 7 [11150/13219]\tLoss: 833.1495\n",
      "Training Epoch: 7 [11200/13219]\tLoss: 812.6833\n",
      "Training Epoch: 7 [11250/13219]\tLoss: 811.4976\n",
      "Training Epoch: 7 [11300/13219]\tLoss: 811.8685\n",
      "Training Epoch: 7 [11350/13219]\tLoss: 868.5739\n",
      "Training Epoch: 7 [11400/13219]\tLoss: 845.4402\n",
      "Training Epoch: 7 [11450/13219]\tLoss: 816.6659\n",
      "Training Epoch: 7 [11500/13219]\tLoss: 827.0474\n",
      "Training Epoch: 7 [11550/13219]\tLoss: 836.4363\n",
      "Training Epoch: 7 [11600/13219]\tLoss: 849.1489\n",
      "Training Epoch: 7 [11650/13219]\tLoss: 830.7787\n",
      "Training Epoch: 7 [11700/13219]\tLoss: 864.4560\n",
      "Training Epoch: 7 [11750/13219]\tLoss: 840.9160\n",
      "Training Epoch: 7 [11800/13219]\tLoss: 829.8061\n",
      "Training Epoch: 7 [11850/13219]\tLoss: 825.6263\n",
      "Training Epoch: 7 [11900/13219]\tLoss: 813.1257\n",
      "Training Epoch: 7 [11950/13219]\tLoss: 821.3720\n",
      "Training Epoch: 7 [12000/13219]\tLoss: 865.1591\n",
      "Training Epoch: 7 [12050/13219]\tLoss: 915.8409\n",
      "Training Epoch: 7 [12100/13219]\tLoss: 844.6574\n",
      "Training Epoch: 7 [12150/13219]\tLoss: 813.8797\n",
      "Training Epoch: 7 [12200/13219]\tLoss: 827.0715\n",
      "Training Epoch: 7 [12250/13219]\tLoss: 821.0751\n",
      "Training Epoch: 7 [12300/13219]\tLoss: 843.3738\n",
      "Training Epoch: 7 [12350/13219]\tLoss: 842.4258\n",
      "Training Epoch: 7 [12400/13219]\tLoss: 835.6315\n",
      "Training Epoch: 7 [12450/13219]\tLoss: 830.3905\n",
      "Training Epoch: 7 [12500/13219]\tLoss: 871.5525\n",
      "Training Epoch: 7 [12550/13219]\tLoss: 836.3813\n",
      "Training Epoch: 7 [12600/13219]\tLoss: 875.9984\n",
      "Training Epoch: 7 [12650/13219]\tLoss: 877.4251\n",
      "Training Epoch: 7 [12700/13219]\tLoss: 832.5252\n",
      "Training Epoch: 7 [12750/13219]\tLoss: 818.5804\n",
      "Training Epoch: 7 [12800/13219]\tLoss: 804.4503\n",
      "Training Epoch: 7 [12850/13219]\tLoss: 844.9166\n",
      "Training Epoch: 7 [12900/13219]\tLoss: 891.6122\n",
      "Training Epoch: 7 [12950/13219]\tLoss: 820.1145\n",
      "Training Epoch: 7 [13000/13219]\tLoss: 856.1249\n",
      "Training Epoch: 7 [13050/13219]\tLoss: 853.7585\n",
      "Training Epoch: 7 [13100/13219]\tLoss: 870.5259\n",
      "Training Epoch: 7 [13150/13219]\tLoss: 839.8234\n",
      "Training Epoch: 7 [13200/13219]\tLoss: 860.2219\n",
      "Training Epoch: 7 [13219/13219]\tLoss: 789.7245\n",
      "Training Epoch: 7 [1469/1469]\tLoss: 830.9537\n",
      "Training Epoch: 8 [50/13219]\tLoss: 837.3137\n",
      "Training Epoch: 8 [100/13219]\tLoss: 832.6616\n",
      "Training Epoch: 8 [150/13219]\tLoss: 868.8106\n",
      "Training Epoch: 8 [200/13219]\tLoss: 845.6874\n",
      "Training Epoch: 8 [250/13219]\tLoss: 855.9999\n",
      "Training Epoch: 8 [300/13219]\tLoss: 804.7297\n",
      "Training Epoch: 8 [350/13219]\tLoss: 846.7131\n",
      "Training Epoch: 8 [400/13219]\tLoss: 807.9630\n",
      "Training Epoch: 8 [450/13219]\tLoss: 845.5540\n",
      "Training Epoch: 8 [500/13219]\tLoss: 878.1348\n",
      "Training Epoch: 8 [550/13219]\tLoss: 822.7209\n",
      "Training Epoch: 8 [600/13219]\tLoss: 798.3278\n",
      "Training Epoch: 8 [650/13219]\tLoss: 824.5335\n",
      "Training Epoch: 8 [700/13219]\tLoss: 827.9130\n",
      "Training Epoch: 8 [750/13219]\tLoss: 903.4974\n",
      "Training Epoch: 8 [800/13219]\tLoss: 819.4171\n",
      "Training Epoch: 8 [850/13219]\tLoss: 856.7352\n",
      "Training Epoch: 8 [900/13219]\tLoss: 848.2682\n",
      "Training Epoch: 8 [950/13219]\tLoss: 842.5194\n",
      "Training Epoch: 8 [1000/13219]\tLoss: 830.2500\n",
      "Training Epoch: 8 [1050/13219]\tLoss: 840.9518\n",
      "Training Epoch: 8 [1100/13219]\tLoss: 826.2889\n",
      "Training Epoch: 8 [1150/13219]\tLoss: 853.6224\n",
      "Training Epoch: 8 [1200/13219]\tLoss: 792.5671\n",
      "Training Epoch: 8 [1250/13219]\tLoss: 872.0604\n",
      "Training Epoch: 8 [1300/13219]\tLoss: 823.9569\n",
      "Training Epoch: 8 [1350/13219]\tLoss: 828.4086\n",
      "Training Epoch: 8 [1400/13219]\tLoss: 862.6559\n",
      "Training Epoch: 8 [1450/13219]\tLoss: 843.6273\n",
      "Training Epoch: 8 [1500/13219]\tLoss: 842.4623\n",
      "Training Epoch: 8 [1550/13219]\tLoss: 805.8384\n",
      "Training Epoch: 8 [1600/13219]\tLoss: 820.5500\n",
      "Training Epoch: 8 [1650/13219]\tLoss: 819.6954\n",
      "Training Epoch: 8 [1700/13219]\tLoss: 840.2058\n",
      "Training Epoch: 8 [1750/13219]\tLoss: 822.4350\n",
      "Training Epoch: 8 [1800/13219]\tLoss: 829.1274\n",
      "Training Epoch: 8 [1850/13219]\tLoss: 791.2968\n",
      "Training Epoch: 8 [1900/13219]\tLoss: 850.9385\n",
      "Training Epoch: 8 [1950/13219]\tLoss: 800.0250\n",
      "Training Epoch: 8 [2000/13219]\tLoss: 844.8384\n",
      "Training Epoch: 8 [2050/13219]\tLoss: 799.5613\n",
      "Training Epoch: 8 [2100/13219]\tLoss: 864.8670\n",
      "Training Epoch: 8 [2150/13219]\tLoss: 832.4682\n",
      "Training Epoch: 8 [2200/13219]\tLoss: 851.0789\n",
      "Training Epoch: 8 [2250/13219]\tLoss: 861.9379\n",
      "Training Epoch: 8 [2300/13219]\tLoss: 868.9766\n",
      "Training Epoch: 8 [2350/13219]\tLoss: 831.3247\n",
      "Training Epoch: 8 [2400/13219]\tLoss: 844.9070\n",
      "Training Epoch: 8 [2450/13219]\tLoss: 882.8561\n",
      "Training Epoch: 8 [2500/13219]\tLoss: 799.0979\n",
      "Training Epoch: 8 [2550/13219]\tLoss: 843.6725\n",
      "Training Epoch: 8 [2600/13219]\tLoss: 796.6183\n",
      "Training Epoch: 8 [2650/13219]\tLoss: 820.7189\n",
      "Training Epoch: 8 [2700/13219]\tLoss: 787.3961\n",
      "Training Epoch: 8 [2750/13219]\tLoss: 825.6396\n",
      "Training Epoch: 8 [2800/13219]\tLoss: 782.1883\n",
      "Training Epoch: 8 [2850/13219]\tLoss: 841.8455\n",
      "Training Epoch: 8 [2900/13219]\tLoss: 853.0259\n",
      "Training Epoch: 8 [2950/13219]\tLoss: 844.8073\n",
      "Training Epoch: 8 [3000/13219]\tLoss: 826.9863\n",
      "Training Epoch: 8 [3050/13219]\tLoss: 839.0812\n",
      "Training Epoch: 8 [3100/13219]\tLoss: 858.9254\n",
      "Training Epoch: 8 [3150/13219]\tLoss: 824.2798\n",
      "Training Epoch: 8 [3200/13219]\tLoss: 806.9954\n",
      "Training Epoch: 8 [3250/13219]\tLoss: 859.9733\n",
      "Training Epoch: 8 [3300/13219]\tLoss: 816.5721\n",
      "Training Epoch: 8 [3350/13219]\tLoss: 820.6649\n",
      "Training Epoch: 8 [3400/13219]\tLoss: 823.2018\n",
      "Training Epoch: 8 [3450/13219]\tLoss: 830.3447\n",
      "Training Epoch: 8 [3500/13219]\tLoss: 841.5777\n",
      "Training Epoch: 8 [3550/13219]\tLoss: 823.1016\n",
      "Training Epoch: 8 [3600/13219]\tLoss: 823.7689\n",
      "Training Epoch: 8 [3650/13219]\tLoss: 858.4832\n",
      "Training Epoch: 8 [3700/13219]\tLoss: 841.6339\n",
      "Training Epoch: 8 [3750/13219]\tLoss: 882.1558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [3800/13219]\tLoss: 829.0250\n",
      "Training Epoch: 8 [3850/13219]\tLoss: 845.7405\n",
      "Training Epoch: 8 [3900/13219]\tLoss: 875.4832\n",
      "Training Epoch: 8 [3950/13219]\tLoss: 846.1655\n",
      "Training Epoch: 8 [4000/13219]\tLoss: 832.8147\n",
      "Training Epoch: 8 [4050/13219]\tLoss: 768.5366\n",
      "Training Epoch: 8 [4100/13219]\tLoss: 886.2620\n",
      "Training Epoch: 8 [4150/13219]\tLoss: 869.6443\n",
      "Training Epoch: 8 [4200/13219]\tLoss: 837.0613\n",
      "Training Epoch: 8 [4250/13219]\tLoss: 821.9595\n",
      "Training Epoch: 8 [4300/13219]\tLoss: 784.1664\n",
      "Training Epoch: 8 [4350/13219]\tLoss: 825.6724\n",
      "Training Epoch: 8 [4400/13219]\tLoss: 884.0707\n",
      "Training Epoch: 8 [4450/13219]\tLoss: 823.3099\n",
      "Training Epoch: 8 [4500/13219]\tLoss: 830.6812\n",
      "Training Epoch: 8 [4550/13219]\tLoss: 807.4348\n",
      "Training Epoch: 8 [4600/13219]\tLoss: 838.5571\n",
      "Training Epoch: 8 [4650/13219]\tLoss: 837.2103\n",
      "Training Epoch: 8 [4700/13219]\tLoss: 805.1562\n",
      "Training Epoch: 8 [4750/13219]\tLoss: 747.6846\n",
      "Training Epoch: 8 [4800/13219]\tLoss: 850.6390\n",
      "Training Epoch: 8 [4850/13219]\tLoss: 814.0250\n",
      "Training Epoch: 8 [4900/13219]\tLoss: 830.9547\n",
      "Training Epoch: 8 [4950/13219]\tLoss: 788.8921\n",
      "Training Epoch: 8 [5000/13219]\tLoss: 836.7016\n",
      "Training Epoch: 8 [5050/13219]\tLoss: 883.6833\n",
      "Training Epoch: 8 [5100/13219]\tLoss: 837.2468\n",
      "Training Epoch: 8 [5150/13219]\tLoss: 841.7181\n",
      "Training Epoch: 8 [5200/13219]\tLoss: 879.0232\n",
      "Training Epoch: 8 [5250/13219]\tLoss: 833.0901\n",
      "Training Epoch: 8 [5300/13219]\tLoss: 830.6739\n",
      "Training Epoch: 8 [5350/13219]\tLoss: 777.4263\n",
      "Training Epoch: 8 [5400/13219]\tLoss: 822.4583\n",
      "Training Epoch: 8 [5450/13219]\tLoss: 857.9681\n",
      "Training Epoch: 8 [5500/13219]\tLoss: 847.7025\n",
      "Training Epoch: 8 [5550/13219]\tLoss: 805.1426\n",
      "Training Epoch: 8 [5600/13219]\tLoss: 826.0854\n",
      "Training Epoch: 8 [5650/13219]\tLoss: 854.8300\n",
      "Training Epoch: 8 [5700/13219]\tLoss: 865.8345\n",
      "Training Epoch: 8 [5750/13219]\tLoss: 852.8312\n",
      "Training Epoch: 8 [5800/13219]\tLoss: 847.0823\n",
      "Training Epoch: 8 [5850/13219]\tLoss: 770.6918\n",
      "Training Epoch: 8 [5900/13219]\tLoss: 840.7876\n",
      "Training Epoch: 8 [5950/13219]\tLoss: 842.7784\n",
      "Training Epoch: 8 [6000/13219]\tLoss: 852.1735\n",
      "Training Epoch: 8 [6050/13219]\tLoss: 857.3829\n",
      "Training Epoch: 8 [6100/13219]\tLoss: 832.9557\n",
      "Training Epoch: 8 [6150/13219]\tLoss: 802.0107\n",
      "Training Epoch: 8 [6200/13219]\tLoss: 817.5546\n",
      "Training Epoch: 8 [6250/13219]\tLoss: 780.3905\n",
      "Training Epoch: 8 [6300/13219]\tLoss: 804.3562\n",
      "Training Epoch: 8 [6350/13219]\tLoss: 811.4388\n",
      "Training Epoch: 8 [6400/13219]\tLoss: 857.8228\n",
      "Training Epoch: 8 [6450/13219]\tLoss: 794.0408\n",
      "Training Epoch: 8 [6500/13219]\tLoss: 866.0866\n",
      "Training Epoch: 8 [6550/13219]\tLoss: 821.4429\n",
      "Training Epoch: 8 [6600/13219]\tLoss: 792.2911\n",
      "Training Epoch: 8 [6650/13219]\tLoss: 826.5537\n",
      "Training Epoch: 8 [6700/13219]\tLoss: 838.2578\n",
      "Training Epoch: 8 [6750/13219]\tLoss: 844.4446\n",
      "Training Epoch: 8 [6800/13219]\tLoss: 833.5911\n",
      "Training Epoch: 8 [6850/13219]\tLoss: 783.7676\n",
      "Training Epoch: 8 [6900/13219]\tLoss: 838.6213\n",
      "Training Epoch: 8 [6950/13219]\tLoss: 802.7049\n",
      "Training Epoch: 8 [7000/13219]\tLoss: 796.3347\n",
      "Training Epoch: 8 [7050/13219]\tLoss: 836.1337\n",
      "Training Epoch: 8 [7100/13219]\tLoss: 782.6781\n",
      "Training Epoch: 8 [7150/13219]\tLoss: 833.7497\n",
      "Training Epoch: 8 [7200/13219]\tLoss: 864.4681\n",
      "Training Epoch: 8 [7250/13219]\tLoss: 842.2870\n",
      "Training Epoch: 8 [7300/13219]\tLoss: 835.4180\n",
      "Training Epoch: 8 [7350/13219]\tLoss: 815.4667\n",
      "Training Epoch: 8 [7400/13219]\tLoss: 839.7040\n",
      "Training Epoch: 8 [7450/13219]\tLoss: 825.1643\n",
      "Training Epoch: 8 [7500/13219]\tLoss: 808.5641\n",
      "Training Epoch: 8 [7550/13219]\tLoss: 837.4461\n",
      "Training Epoch: 8 [7600/13219]\tLoss: 785.9286\n",
      "Training Epoch: 8 [7650/13219]\tLoss: 820.4599\n",
      "Training Epoch: 8 [7700/13219]\tLoss: 847.0610\n",
      "Training Epoch: 8 [7750/13219]\tLoss: 859.8762\n",
      "Training Epoch: 8 [7800/13219]\tLoss: 829.0834\n",
      "Training Epoch: 8 [7850/13219]\tLoss: 802.6497\n",
      "Training Epoch: 8 [7900/13219]\tLoss: 821.9169\n",
      "Training Epoch: 8 [7950/13219]\tLoss: 820.7833\n",
      "Training Epoch: 8 [8000/13219]\tLoss: 829.6602\n",
      "Training Epoch: 8 [8050/13219]\tLoss: 808.0116\n",
      "Training Epoch: 8 [8100/13219]\tLoss: 850.0786\n",
      "Training Epoch: 8 [8150/13219]\tLoss: 806.4830\n",
      "Training Epoch: 8 [8200/13219]\tLoss: 797.7734\n",
      "Training Epoch: 8 [8250/13219]\tLoss: 837.7001\n",
      "Training Epoch: 8 [8300/13219]\tLoss: 807.3006\n",
      "Training Epoch: 8 [8350/13219]\tLoss: 830.9243\n",
      "Training Epoch: 8 [8400/13219]\tLoss: 831.4842\n",
      "Training Epoch: 8 [8450/13219]\tLoss: 810.5592\n",
      "Training Epoch: 8 [8500/13219]\tLoss: 804.9461\n",
      "Training Epoch: 8 [8550/13219]\tLoss: 797.2457\n",
      "Training Epoch: 8 [8600/13219]\tLoss: 868.8671\n",
      "Training Epoch: 8 [8650/13219]\tLoss: 828.5078\n",
      "Training Epoch: 8 [8700/13219]\tLoss: 861.1083\n",
      "Training Epoch: 8 [8750/13219]\tLoss: 843.3138\n",
      "Training Epoch: 8 [8800/13219]\tLoss: 826.0167\n",
      "Training Epoch: 8 [8850/13219]\tLoss: 789.4077\n",
      "Training Epoch: 8 [8900/13219]\tLoss: 835.7758\n",
      "Training Epoch: 8 [8950/13219]\tLoss: 867.4900\n",
      "Training Epoch: 8 [9000/13219]\tLoss: 805.4652\n",
      "Training Epoch: 8 [9050/13219]\tLoss: 798.0175\n",
      "Training Epoch: 8 [9100/13219]\tLoss: 831.9994\n",
      "Training Epoch: 8 [9150/13219]\tLoss: 843.7211\n",
      "Training Epoch: 8 [9200/13219]\tLoss: 805.6414\n",
      "Training Epoch: 8 [9250/13219]\tLoss: 816.0648\n",
      "Training Epoch: 8 [9300/13219]\tLoss: 864.0428\n",
      "Training Epoch: 8 [9350/13219]\tLoss: 829.4245\n",
      "Training Epoch: 8 [9400/13219]\tLoss: 783.4598\n",
      "Training Epoch: 8 [9450/13219]\tLoss: 824.8580\n",
      "Training Epoch: 8 [9500/13219]\tLoss: 795.7789\n",
      "Training Epoch: 8 [9550/13219]\tLoss: 805.5289\n",
      "Training Epoch: 8 [9600/13219]\tLoss: 824.4689\n",
      "Training Epoch: 8 [9650/13219]\tLoss: 889.1097\n",
      "Training Epoch: 8 [9700/13219]\tLoss: 812.3881\n",
      "Training Epoch: 8 [9750/13219]\tLoss: 795.6582\n",
      "Training Epoch: 8 [9800/13219]\tLoss: 852.2354\n",
      "Training Epoch: 8 [9850/13219]\tLoss: 792.4624\n",
      "Training Epoch: 8 [9900/13219]\tLoss: 830.1987\n",
      "Training Epoch: 8 [9950/13219]\tLoss: 812.1978\n",
      "Training Epoch: 8 [10000/13219]\tLoss: 816.2867\n",
      "Training Epoch: 8 [10050/13219]\tLoss: 844.1459\n",
      "Training Epoch: 8 [10100/13219]\tLoss: 789.7277\n",
      "Training Epoch: 8 [10150/13219]\tLoss: 806.4359\n",
      "Training Epoch: 8 [10200/13219]\tLoss: 765.8552\n",
      "Training Epoch: 8 [10250/13219]\tLoss: 839.4056\n",
      "Training Epoch: 8 [10300/13219]\tLoss: 816.4482\n",
      "Training Epoch: 8 [10350/13219]\tLoss: 820.7938\n",
      "Training Epoch: 8 [10400/13219]\tLoss: 865.7369\n",
      "Training Epoch: 8 [10450/13219]\tLoss: 833.9904\n",
      "Training Epoch: 8 [10500/13219]\tLoss: 797.4678\n",
      "Training Epoch: 8 [10550/13219]\tLoss: 799.0239\n",
      "Training Epoch: 8 [10600/13219]\tLoss: 784.0577\n",
      "Training Epoch: 8 [10650/13219]\tLoss: 838.4464\n",
      "Training Epoch: 8 [10700/13219]\tLoss: 770.5953\n",
      "Training Epoch: 8 [10750/13219]\tLoss: 832.8927\n",
      "Training Epoch: 8 [10800/13219]\tLoss: 825.1887\n",
      "Training Epoch: 8 [10850/13219]\tLoss: 815.5167\n",
      "Training Epoch: 8 [10900/13219]\tLoss: 804.9872\n",
      "Training Epoch: 8 [10950/13219]\tLoss: 807.8704\n",
      "Training Epoch: 8 [11000/13219]\tLoss: 832.3890\n",
      "Training Epoch: 8 [11050/13219]\tLoss: 844.7256\n",
      "Training Epoch: 8 [11100/13219]\tLoss: 807.2709\n",
      "Training Epoch: 8 [11150/13219]\tLoss: 828.5439\n",
      "Training Epoch: 8 [11200/13219]\tLoss: 829.4459\n",
      "Training Epoch: 8 [11250/13219]\tLoss: 852.0867\n",
      "Training Epoch: 8 [11300/13219]\tLoss: 807.3871\n",
      "Training Epoch: 8 [11350/13219]\tLoss: 833.4858\n",
      "Training Epoch: 8 [11400/13219]\tLoss: 800.8716\n",
      "Training Epoch: 8 [11450/13219]\tLoss: 841.4502\n",
      "Training Epoch: 8 [11500/13219]\tLoss: 818.5801\n",
      "Training Epoch: 8 [11550/13219]\tLoss: 787.9582\n",
      "Training Epoch: 8 [11600/13219]\tLoss: 863.6056\n",
      "Training Epoch: 8 [11650/13219]\tLoss: 821.1201\n",
      "Training Epoch: 8 [11700/13219]\tLoss: 835.2465\n",
      "Training Epoch: 8 [11750/13219]\tLoss: 781.0146\n",
      "Training Epoch: 8 [11800/13219]\tLoss: 796.0499\n",
      "Training Epoch: 8 [11850/13219]\tLoss: 796.3475\n",
      "Training Epoch: 8 [11900/13219]\tLoss: 809.3826\n",
      "Training Epoch: 8 [11950/13219]\tLoss: 840.7471\n",
      "Training Epoch: 8 [12000/13219]\tLoss: 797.6403\n",
      "Training Epoch: 8 [12050/13219]\tLoss: 840.0069\n",
      "Training Epoch: 8 [12100/13219]\tLoss: 851.1539\n",
      "Training Epoch: 8 [12150/13219]\tLoss: 842.9878\n",
      "Training Epoch: 8 [12200/13219]\tLoss: 756.7442\n",
      "Training Epoch: 8 [12250/13219]\tLoss: 784.4477\n",
      "Training Epoch: 8 [12300/13219]\tLoss: 823.7834\n",
      "Training Epoch: 8 [12350/13219]\tLoss: 813.5939\n",
      "Training Epoch: 8 [12400/13219]\tLoss: 820.1636\n",
      "Training Epoch: 8 [12450/13219]\tLoss: 778.5355\n",
      "Training Epoch: 8 [12500/13219]\tLoss: 809.7554\n",
      "Training Epoch: 8 [12550/13219]\tLoss: 830.9373\n",
      "Training Epoch: 8 [12600/13219]\tLoss: 860.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [12650/13219]\tLoss: 843.9741\n",
      "Training Epoch: 8 [12700/13219]\tLoss: 826.1496\n",
      "Training Epoch: 8 [12750/13219]\tLoss: 818.8113\n",
      "Training Epoch: 8 [12800/13219]\tLoss: 806.3499\n",
      "Training Epoch: 8 [12850/13219]\tLoss: 799.1468\n",
      "Training Epoch: 8 [12900/13219]\tLoss: 778.9999\n",
      "Training Epoch: 8 [12950/13219]\tLoss: 783.7272\n",
      "Training Epoch: 8 [13000/13219]\tLoss: 814.8130\n",
      "Training Epoch: 8 [13050/13219]\tLoss: 829.4254\n",
      "Training Epoch: 8 [13100/13219]\tLoss: 804.8653\n",
      "Training Epoch: 8 [13150/13219]\tLoss: 813.4750\n",
      "Training Epoch: 8 [13200/13219]\tLoss: 790.4636\n",
      "Training Epoch: 8 [13219/13219]\tLoss: 793.0077\n",
      "Training Epoch: 8 [1469/1469]\tLoss: 805.4560\n",
      "Training Epoch: 9 [50/13219]\tLoss: 799.7013\n",
      "Training Epoch: 9 [100/13219]\tLoss: 793.2948\n",
      "Training Epoch: 9 [150/13219]\tLoss: 815.7064\n",
      "Training Epoch: 9 [200/13219]\tLoss: 853.6061\n",
      "Training Epoch: 9 [250/13219]\tLoss: 849.7086\n",
      "Training Epoch: 9 [300/13219]\tLoss: 812.7291\n",
      "Training Epoch: 9 [350/13219]\tLoss: 814.6081\n",
      "Training Epoch: 9 [400/13219]\tLoss: 799.1904\n",
      "Training Epoch: 9 [450/13219]\tLoss: 810.6302\n",
      "Training Epoch: 9 [500/13219]\tLoss: 814.0383\n",
      "Training Epoch: 9 [550/13219]\tLoss: 773.3033\n",
      "Training Epoch: 9 [600/13219]\tLoss: 791.6152\n",
      "Training Epoch: 9 [650/13219]\tLoss: 769.1653\n",
      "Training Epoch: 9 [700/13219]\tLoss: 800.8859\n",
      "Training Epoch: 9 [750/13219]\tLoss: 857.8583\n",
      "Training Epoch: 9 [800/13219]\tLoss: 823.9438\n",
      "Training Epoch: 9 [850/13219]\tLoss: 796.1313\n",
      "Training Epoch: 9 [900/13219]\tLoss: 781.8333\n",
      "Training Epoch: 9 [950/13219]\tLoss: 825.3737\n",
      "Training Epoch: 9 [1000/13219]\tLoss: 811.3884\n",
      "Training Epoch: 9 [1050/13219]\tLoss: 795.9464\n",
      "Training Epoch: 9 [1100/13219]\tLoss: 816.2204\n",
      "Training Epoch: 9 [1150/13219]\tLoss: 810.0027\n",
      "Training Epoch: 9 [1200/13219]\tLoss: 831.1122\n",
      "Training Epoch: 9 [1250/13219]\tLoss: 813.6053\n",
      "Training Epoch: 9 [1300/13219]\tLoss: 789.9246\n",
      "Training Epoch: 9 [1350/13219]\tLoss: 782.2559\n",
      "Training Epoch: 9 [1400/13219]\tLoss: 780.8798\n",
      "Training Epoch: 9 [1450/13219]\tLoss: 779.4935\n",
      "Training Epoch: 9 [1500/13219]\tLoss: 822.8164\n",
      "Training Epoch: 9 [1550/13219]\tLoss: 811.9919\n",
      "Training Epoch: 9 [1600/13219]\tLoss: 799.9847\n",
      "Training Epoch: 9 [1650/13219]\tLoss: 804.5729\n",
      "Training Epoch: 9 [1700/13219]\tLoss: 816.7069\n",
      "Training Epoch: 9 [1750/13219]\tLoss: 791.1414\n",
      "Training Epoch: 9 [1800/13219]\tLoss: 806.8441\n",
      "Training Epoch: 9 [1850/13219]\tLoss: 814.2106\n",
      "Training Epoch: 9 [1900/13219]\tLoss: 819.5110\n",
      "Training Epoch: 9 [1950/13219]\tLoss: 807.6163\n",
      "Training Epoch: 9 [2000/13219]\tLoss: 791.3242\n",
      "Training Epoch: 9 [2050/13219]\tLoss: 818.4766\n",
      "Training Epoch: 9 [2100/13219]\tLoss: 786.6768\n",
      "Training Epoch: 9 [2150/13219]\tLoss: 782.0106\n",
      "Training Epoch: 9 [2200/13219]\tLoss: 840.1848\n",
      "Training Epoch: 9 [2250/13219]\tLoss: 796.1019\n",
      "Training Epoch: 9 [2300/13219]\tLoss: 771.1016\n",
      "Training Epoch: 9 [2350/13219]\tLoss: 814.0924\n",
      "Training Epoch: 9 [2400/13219]\tLoss: 807.9985\n",
      "Training Epoch: 9 [2450/13219]\tLoss: 812.0743\n",
      "Training Epoch: 9 [2500/13219]\tLoss: 793.3097\n",
      "Training Epoch: 9 [2550/13219]\tLoss: 849.6338\n",
      "Training Epoch: 9 [2600/13219]\tLoss: 786.2925\n",
      "Training Epoch: 9 [2650/13219]\tLoss: 803.4508\n",
      "Training Epoch: 9 [2700/13219]\tLoss: 834.1505\n",
      "Training Epoch: 9 [2750/13219]\tLoss: 826.7731\n",
      "Training Epoch: 9 [2800/13219]\tLoss: 816.4156\n",
      "Training Epoch: 9 [2850/13219]\tLoss: 831.5241\n",
      "Training Epoch: 9 [2900/13219]\tLoss: 788.8658\n",
      "Training Epoch: 9 [2950/13219]\tLoss: 810.4478\n",
      "Training Epoch: 9 [3000/13219]\tLoss: 840.7250\n",
      "Training Epoch: 9 [3050/13219]\tLoss: 797.2537\n",
      "Training Epoch: 9 [3100/13219]\tLoss: 797.6861\n",
      "Training Epoch: 9 [3150/13219]\tLoss: 857.5342\n",
      "Training Epoch: 9 [3200/13219]\tLoss: 844.1855\n",
      "Training Epoch: 9 [3250/13219]\tLoss: 841.6579\n",
      "Training Epoch: 9 [3300/13219]\tLoss: 806.6489\n",
      "Training Epoch: 9 [3350/13219]\tLoss: 815.9850\n",
      "Training Epoch: 9 [3400/13219]\tLoss: 770.1855\n",
      "Training Epoch: 9 [3450/13219]\tLoss: 771.5302\n",
      "Training Epoch: 9 [3500/13219]\tLoss: 806.3990\n",
      "Training Epoch: 9 [3550/13219]\tLoss: 833.3613\n",
      "Training Epoch: 9 [3600/13219]\tLoss: 838.1673\n",
      "Training Epoch: 9 [3650/13219]\tLoss: 793.4936\n",
      "Training Epoch: 9 [3700/13219]\tLoss: 806.2842\n",
      "Training Epoch: 9 [3750/13219]\tLoss: 816.7218\n",
      "Training Epoch: 9 [3800/13219]\tLoss: 830.1285\n",
      "Training Epoch: 9 [3850/13219]\tLoss: 810.2954\n",
      "Training Epoch: 9 [3900/13219]\tLoss: 740.1767\n",
      "Training Epoch: 9 [3950/13219]\tLoss: 808.5245\n",
      "Training Epoch: 9 [4000/13219]\tLoss: 807.2714\n",
      "Training Epoch: 9 [4050/13219]\tLoss: 806.4655\n",
      "Training Epoch: 9 [4100/13219]\tLoss: 797.0562\n",
      "Training Epoch: 9 [4150/13219]\tLoss: 796.6921\n",
      "Training Epoch: 9 [4200/13219]\tLoss: 827.2258\n",
      "Training Epoch: 9 [4250/13219]\tLoss: 824.2007\n",
      "Training Epoch: 9 [4300/13219]\tLoss: 823.7523\n",
      "Training Epoch: 9 [4350/13219]\tLoss: 786.8477\n",
      "Training Epoch: 9 [4400/13219]\tLoss: 744.5752\n",
      "Training Epoch: 9 [4450/13219]\tLoss: 753.8840\n",
      "Training Epoch: 9 [4500/13219]\tLoss: 769.8436\n",
      "Training Epoch: 9 [4550/13219]\tLoss: 804.5537\n",
      "Training Epoch: 9 [4600/13219]\tLoss: 845.9894\n",
      "Training Epoch: 9 [4650/13219]\tLoss: 777.7050\n",
      "Training Epoch: 9 [4700/13219]\tLoss: 784.8688\n",
      "Training Epoch: 9 [4750/13219]\tLoss: 786.1328\n",
      "Training Epoch: 9 [4800/13219]\tLoss: 802.0491\n",
      "Training Epoch: 9 [4850/13219]\tLoss: 794.9451\n",
      "Training Epoch: 9 [4900/13219]\tLoss: 824.5994\n",
      "Training Epoch: 9 [4950/13219]\tLoss: 822.6760\n",
      "Training Epoch: 9 [5000/13219]\tLoss: 824.4077\n",
      "Training Epoch: 9 [5050/13219]\tLoss: 880.8746\n",
      "Training Epoch: 9 [5100/13219]\tLoss: 804.7776\n",
      "Training Epoch: 9 [5150/13219]\tLoss: 801.3947\n",
      "Training Epoch: 9 [5200/13219]\tLoss: 798.0472\n",
      "Training Epoch: 9 [5250/13219]\tLoss: 812.0341\n",
      "Training Epoch: 9 [5300/13219]\tLoss: 790.0787\n",
      "Training Epoch: 9 [5350/13219]\tLoss: 815.8004\n",
      "Training Epoch: 9 [5400/13219]\tLoss: 780.8229\n",
      "Training Epoch: 9 [5450/13219]\tLoss: 772.4424\n",
      "Training Epoch: 9 [5500/13219]\tLoss: 814.3923\n",
      "Training Epoch: 9 [5550/13219]\tLoss: 777.5406\n",
      "Training Epoch: 9 [5600/13219]\tLoss: 850.1207\n",
      "Training Epoch: 9 [5650/13219]\tLoss: 835.6906\n",
      "Training Epoch: 9 [5700/13219]\tLoss: 795.7171\n",
      "Training Epoch: 9 [5750/13219]\tLoss: 765.9973\n",
      "Training Epoch: 9 [5800/13219]\tLoss: 781.6660\n",
      "Training Epoch: 9 [5850/13219]\tLoss: 792.4178\n",
      "Training Epoch: 9 [5900/13219]\tLoss: 805.0385\n",
      "Training Epoch: 9 [5950/13219]\tLoss: 763.4120\n",
      "Training Epoch: 9 [6000/13219]\tLoss: 821.1850\n",
      "Training Epoch: 9 [6050/13219]\tLoss: 846.6078\n",
      "Training Epoch: 9 [6100/13219]\tLoss: 806.6956\n",
      "Training Epoch: 9 [6150/13219]\tLoss: 811.6448\n",
      "Training Epoch: 9 [6200/13219]\tLoss: 763.9828\n",
      "Training Epoch: 9 [6250/13219]\tLoss: 841.7337\n",
      "Training Epoch: 9 [6300/13219]\tLoss: 796.5923\n",
      "Training Epoch: 9 [6350/13219]\tLoss: 797.1773\n",
      "Training Epoch: 9 [6400/13219]\tLoss: 853.2641\n",
      "Training Epoch: 9 [6450/13219]\tLoss: 827.6946\n",
      "Training Epoch: 9 [6500/13219]\tLoss: 809.3219\n",
      "Training Epoch: 9 [6550/13219]\tLoss: 765.8336\n",
      "Training Epoch: 9 [6600/13219]\tLoss: 799.2979\n",
      "Training Epoch: 9 [6650/13219]\tLoss: 829.9577\n",
      "Training Epoch: 9 [6700/13219]\tLoss: 805.2473\n",
      "Training Epoch: 9 [6750/13219]\tLoss: 845.4363\n",
      "Training Epoch: 9 [6800/13219]\tLoss: 804.6644\n",
      "Training Epoch: 9 [6850/13219]\tLoss: 795.6546\n",
      "Training Epoch: 9 [6900/13219]\tLoss: 795.3125\n",
      "Training Epoch: 9 [6950/13219]\tLoss: 790.9855\n",
      "Training Epoch: 9 [7000/13219]\tLoss: 824.8298\n",
      "Training Epoch: 9 [7050/13219]\tLoss: 809.2890\n",
      "Training Epoch: 9 [7100/13219]\tLoss: 785.6218\n",
      "Training Epoch: 9 [7150/13219]\tLoss: 856.5410\n",
      "Training Epoch: 9 [7200/13219]\tLoss: 813.4082\n",
      "Training Epoch: 9 [7250/13219]\tLoss: 834.9008\n",
      "Training Epoch: 9 [7300/13219]\tLoss: 824.0720\n",
      "Training Epoch: 9 [7350/13219]\tLoss: 757.5065\n",
      "Training Epoch: 9 [7400/13219]\tLoss: 840.3347\n",
      "Training Epoch: 9 [7450/13219]\tLoss: 779.7130\n",
      "Training Epoch: 9 [7500/13219]\tLoss: 832.2098\n",
      "Training Epoch: 9 [7550/13219]\tLoss: 760.3741\n",
      "Training Epoch: 9 [7600/13219]\tLoss: 822.8631\n",
      "Training Epoch: 9 [7650/13219]\tLoss: 806.6476\n",
      "Training Epoch: 9 [7700/13219]\tLoss: 781.8546\n",
      "Training Epoch: 9 [7750/13219]\tLoss: 808.2919\n",
      "Training Epoch: 9 [7800/13219]\tLoss: 801.4394\n",
      "Training Epoch: 9 [7850/13219]\tLoss: 810.2363\n",
      "Training Epoch: 9 [7900/13219]\tLoss: 856.0553\n",
      "Training Epoch: 9 [7950/13219]\tLoss: 835.9018\n",
      "Training Epoch: 9 [8000/13219]\tLoss: 792.3434\n",
      "Training Epoch: 9 [8050/13219]\tLoss: 788.1846\n",
      "Training Epoch: 9 [8100/13219]\tLoss: 778.3608\n",
      "Training Epoch: 9 [8150/13219]\tLoss: 790.2388\n",
      "Training Epoch: 9 [8200/13219]\tLoss: 823.2280\n",
      "Training Epoch: 9 [8250/13219]\tLoss: 775.8068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [8300/13219]\tLoss: 773.0236\n",
      "Training Epoch: 9 [8350/13219]\tLoss: 828.9975\n",
      "Training Epoch: 9 [8400/13219]\tLoss: 838.7911\n",
      "Training Epoch: 9 [8450/13219]\tLoss: 758.4443\n",
      "Training Epoch: 9 [8500/13219]\tLoss: 807.3549\n",
      "Training Epoch: 9 [8550/13219]\tLoss: 782.2885\n",
      "Training Epoch: 9 [8600/13219]\tLoss: 790.9294\n",
      "Training Epoch: 9 [8650/13219]\tLoss: 834.2553\n",
      "Training Epoch: 9 [8700/13219]\tLoss: 789.1072\n",
      "Training Epoch: 9 [8750/13219]\tLoss: 807.3245\n",
      "Training Epoch: 9 [8800/13219]\tLoss: 830.9526\n",
      "Training Epoch: 9 [8850/13219]\tLoss: 774.9564\n",
      "Training Epoch: 9 [8900/13219]\tLoss: 741.7930\n",
      "Training Epoch: 9 [8950/13219]\tLoss: 758.5947\n",
      "Training Epoch: 9 [9000/13219]\tLoss: 783.5591\n",
      "Training Epoch: 9 [9050/13219]\tLoss: 805.2612\n",
      "Training Epoch: 9 [9100/13219]\tLoss: 805.8212\n",
      "Training Epoch: 9 [9150/13219]\tLoss: 772.4128\n",
      "Training Epoch: 9 [9200/13219]\tLoss: 808.9575\n",
      "Training Epoch: 9 [9250/13219]\tLoss: 806.2648\n",
      "Training Epoch: 9 [9300/13219]\tLoss: 762.3000\n",
      "Training Epoch: 9 [9350/13219]\tLoss: 829.5547\n",
      "Training Epoch: 9 [9400/13219]\tLoss: 829.6269\n",
      "Training Epoch: 9 [9450/13219]\tLoss: 796.8938\n",
      "Training Epoch: 9 [9500/13219]\tLoss: 820.5843\n",
      "Training Epoch: 9 [9550/13219]\tLoss: 815.6471\n",
      "Training Epoch: 9 [9600/13219]\tLoss: 825.5419\n",
      "Training Epoch: 9 [9650/13219]\tLoss: 756.4908\n",
      "Training Epoch: 9 [9700/13219]\tLoss: 804.6681\n",
      "Training Epoch: 9 [9750/13219]\tLoss: 794.5150\n",
      "Training Epoch: 9 [9800/13219]\tLoss: 838.3583\n",
      "Training Epoch: 9 [9850/13219]\tLoss: 814.9618\n",
      "Training Epoch: 9 [9900/13219]\tLoss: 802.0047\n",
      "Training Epoch: 9 [9950/13219]\tLoss: 756.5012\n",
      "Training Epoch: 9 [10000/13219]\tLoss: 797.3228\n",
      "Training Epoch: 9 [10050/13219]\tLoss: 828.0963\n",
      "Training Epoch: 9 [10100/13219]\tLoss: 802.8966\n",
      "Training Epoch: 9 [10150/13219]\tLoss: 791.9581\n",
      "Training Epoch: 9 [10200/13219]\tLoss: 810.8829\n",
      "Training Epoch: 9 [10250/13219]\tLoss: 814.5283\n",
      "Training Epoch: 9 [10300/13219]\tLoss: 781.9086\n",
      "Training Epoch: 9 [10350/13219]\tLoss: 805.4374\n",
      "Training Epoch: 9 [10400/13219]\tLoss: 773.6293\n",
      "Training Epoch: 9 [10450/13219]\tLoss: 772.7541\n",
      "Training Epoch: 9 [10500/13219]\tLoss: 812.6157\n",
      "Training Epoch: 9 [10550/13219]\tLoss: 751.8613\n",
      "Training Epoch: 9 [10600/13219]\tLoss: 796.1736\n",
      "Training Epoch: 9 [10650/13219]\tLoss: 800.9890\n",
      "Training Epoch: 9 [10700/13219]\tLoss: 799.0843\n",
      "Training Epoch: 9 [10750/13219]\tLoss: 826.5858\n",
      "Training Epoch: 9 [10800/13219]\tLoss: 804.4297\n",
      "Training Epoch: 9 [10850/13219]\tLoss: 813.7552\n",
      "Training Epoch: 9 [10900/13219]\tLoss: 783.4761\n",
      "Training Epoch: 9 [10950/13219]\tLoss: 790.8849\n",
      "Training Epoch: 9 [11000/13219]\tLoss: 817.7219\n",
      "Training Epoch: 9 [11050/13219]\tLoss: 781.1805\n",
      "Training Epoch: 9 [11100/13219]\tLoss: 803.0121\n",
      "Training Epoch: 9 [11150/13219]\tLoss: 833.4706\n",
      "Training Epoch: 9 [11200/13219]\tLoss: 798.5843\n",
      "Training Epoch: 9 [11250/13219]\tLoss: 810.3311\n",
      "Training Epoch: 9 [11300/13219]\tLoss: 783.9063\n",
      "Training Epoch: 9 [11350/13219]\tLoss: 768.0710\n",
      "Training Epoch: 9 [11400/13219]\tLoss: 765.0160\n",
      "Training Epoch: 9 [11450/13219]\tLoss: 817.2726\n",
      "Training Epoch: 9 [11500/13219]\tLoss: 773.7147\n",
      "Training Epoch: 9 [11550/13219]\tLoss: 823.6175\n",
      "Training Epoch: 9 [11600/13219]\tLoss: 801.9894\n",
      "Training Epoch: 9 [11650/13219]\tLoss: 789.6069\n",
      "Training Epoch: 9 [11700/13219]\tLoss: 816.3070\n",
      "Training Epoch: 9 [11750/13219]\tLoss: 838.9611\n",
      "Training Epoch: 9 [11800/13219]\tLoss: 788.2565\n",
      "Training Epoch: 9 [11850/13219]\tLoss: 766.8256\n",
      "Training Epoch: 9 [11900/13219]\tLoss: 830.6753\n",
      "Training Epoch: 9 [11950/13219]\tLoss: 756.3023\n",
      "Training Epoch: 9 [12000/13219]\tLoss: 848.5593\n",
      "Training Epoch: 9 [12050/13219]\tLoss: 817.8890\n",
      "Training Epoch: 9 [12100/13219]\tLoss: 781.4402\n",
      "Training Epoch: 9 [12150/13219]\tLoss: 761.6652\n",
      "Training Epoch: 9 [12200/13219]\tLoss: 750.9016\n",
      "Training Epoch: 9 [12250/13219]\tLoss: 806.8995\n",
      "Training Epoch: 9 [12300/13219]\tLoss: 846.5504\n",
      "Training Epoch: 9 [12350/13219]\tLoss: 768.4371\n",
      "Training Epoch: 9 [12400/13219]\tLoss: 771.0447\n",
      "Training Epoch: 9 [12450/13219]\tLoss: 790.6523\n",
      "Training Epoch: 9 [12500/13219]\tLoss: 788.5237\n",
      "Training Epoch: 9 [12550/13219]\tLoss: 813.3544\n",
      "Training Epoch: 9 [12600/13219]\tLoss: 812.2859\n",
      "Training Epoch: 9 [12650/13219]\tLoss: 790.9545\n",
      "Training Epoch: 9 [12700/13219]\tLoss: 776.7119\n",
      "Training Epoch: 9 [12750/13219]\tLoss: 799.6136\n",
      "Training Epoch: 9 [12800/13219]\tLoss: 796.4160\n",
      "Training Epoch: 9 [12850/13219]\tLoss: 824.0692\n",
      "Training Epoch: 9 [12900/13219]\tLoss: 798.5742\n",
      "Training Epoch: 9 [12950/13219]\tLoss: 765.2690\n",
      "Training Epoch: 9 [13000/13219]\tLoss: 758.1101\n",
      "Training Epoch: 9 [13050/13219]\tLoss: 776.8920\n",
      "Training Epoch: 9 [13100/13219]\tLoss: 825.9616\n",
      "Training Epoch: 9 [13150/13219]\tLoss: 801.7486\n",
      "Training Epoch: 9 [13200/13219]\tLoss: 774.0952\n",
      "Training Epoch: 9 [13219/13219]\tLoss: 785.9182\n",
      "Training Epoch: 9 [1469/1469]\tLoss: 784.8123\n",
      "Training Epoch: 10 [50/13219]\tLoss: 816.9971\n",
      "Training Epoch: 10 [100/13219]\tLoss: 787.2620\n",
      "Training Epoch: 10 [150/13219]\tLoss: 802.8109\n",
      "Training Epoch: 10 [200/13219]\tLoss: 834.4429\n",
      "Training Epoch: 10 [250/13219]\tLoss: 798.0628\n",
      "Training Epoch: 10 [300/13219]\tLoss: 805.7625\n",
      "Training Epoch: 10 [350/13219]\tLoss: 799.2206\n",
      "Training Epoch: 10 [400/13219]\tLoss: 820.3360\n",
      "Training Epoch: 10 [450/13219]\tLoss: 806.5650\n",
      "Training Epoch: 10 [500/13219]\tLoss: 779.2678\n",
      "Training Epoch: 10 [550/13219]\tLoss: 791.0955\n",
      "Training Epoch: 10 [600/13219]\tLoss: 786.9221\n",
      "Training Epoch: 10 [650/13219]\tLoss: 763.2361\n",
      "Training Epoch: 10 [700/13219]\tLoss: 764.4611\n",
      "Training Epoch: 10 [750/13219]\tLoss: 819.1124\n",
      "Training Epoch: 10 [800/13219]\tLoss: 751.1888\n",
      "Training Epoch: 10 [850/13219]\tLoss: 777.1479\n",
      "Training Epoch: 10 [900/13219]\tLoss: 781.0382\n",
      "Training Epoch: 10 [950/13219]\tLoss: 786.8008\n",
      "Training Epoch: 10 [1000/13219]\tLoss: 786.5935\n",
      "Training Epoch: 10 [1050/13219]\tLoss: 796.3058\n",
      "Training Epoch: 10 [1100/13219]\tLoss: 756.4296\n",
      "Training Epoch: 10 [1150/13219]\tLoss: 765.3798\n",
      "Training Epoch: 10 [1200/13219]\tLoss: 810.8596\n",
      "Training Epoch: 10 [1250/13219]\tLoss: 761.8613\n",
      "Training Epoch: 10 [1300/13219]\tLoss: 800.3999\n",
      "Training Epoch: 10 [1350/13219]\tLoss: 804.5897\n",
      "Training Epoch: 10 [1400/13219]\tLoss: 801.6110\n",
      "Training Epoch: 10 [1450/13219]\tLoss: 794.6241\n",
      "Training Epoch: 10 [1500/13219]\tLoss: 820.7106\n",
      "Training Epoch: 10 [1550/13219]\tLoss: 827.5564\n",
      "Training Epoch: 10 [1600/13219]\tLoss: 763.4850\n",
      "Training Epoch: 10 [1650/13219]\tLoss: 795.4349\n",
      "Training Epoch: 10 [1700/13219]\tLoss: 755.5637\n",
      "Training Epoch: 10 [1750/13219]\tLoss: 777.1129\n",
      "Training Epoch: 10 [1800/13219]\tLoss: 822.3148\n",
      "Training Epoch: 10 [1850/13219]\tLoss: 788.3366\n",
      "Training Epoch: 10 [1900/13219]\tLoss: 788.4708\n",
      "Training Epoch: 10 [1950/13219]\tLoss: 782.5663\n",
      "Training Epoch: 10 [2000/13219]\tLoss: 742.1448\n",
      "Training Epoch: 10 [2050/13219]\tLoss: 777.2131\n",
      "Training Epoch: 10 [2100/13219]\tLoss: 779.7672\n",
      "Training Epoch: 10 [2150/13219]\tLoss: 786.3104\n",
      "Training Epoch: 10 [2200/13219]\tLoss: 777.4736\n",
      "Training Epoch: 10 [2250/13219]\tLoss: 783.6813\n",
      "Training Epoch: 10 [2300/13219]\tLoss: 794.0854\n",
      "Training Epoch: 10 [2350/13219]\tLoss: 780.5906\n",
      "Training Epoch: 10 [2400/13219]\tLoss: 829.9549\n",
      "Training Epoch: 10 [2450/13219]\tLoss: 786.0151\n",
      "Training Epoch: 10 [2500/13219]\tLoss: 810.9884\n",
      "Training Epoch: 10 [2550/13219]\tLoss: 785.8256\n",
      "Training Epoch: 10 [2600/13219]\tLoss: 819.6066\n",
      "Training Epoch: 10 [2650/13219]\tLoss: 786.2623\n",
      "Training Epoch: 10 [2700/13219]\tLoss: 792.7551\n",
      "Training Epoch: 10 [2750/13219]\tLoss: 766.3488\n",
      "Training Epoch: 10 [2800/13219]\tLoss: 786.2680\n",
      "Training Epoch: 10 [2850/13219]\tLoss: 806.4321\n",
      "Training Epoch: 10 [2900/13219]\tLoss: 739.5710\n",
      "Training Epoch: 10 [2950/13219]\tLoss: 767.4005\n",
      "Training Epoch: 10 [3000/13219]\tLoss: 793.1512\n",
      "Training Epoch: 10 [3050/13219]\tLoss: 802.6493\n",
      "Training Epoch: 10 [3100/13219]\tLoss: 783.5301\n",
      "Training Epoch: 10 [3150/13219]\tLoss: 789.3710\n",
      "Training Epoch: 10 [3200/13219]\tLoss: 867.4816\n",
      "Training Epoch: 10 [3250/13219]\tLoss: 843.7739\n",
      "Training Epoch: 10 [3300/13219]\tLoss: 784.5903\n",
      "Training Epoch: 10 [3350/13219]\tLoss: 776.8093\n",
      "Training Epoch: 10 [3400/13219]\tLoss: 776.0146\n",
      "Training Epoch: 10 [3450/13219]\tLoss: 758.5410\n",
      "Training Epoch: 10 [3500/13219]\tLoss: 805.5782\n",
      "Training Epoch: 10 [3550/13219]\tLoss: 762.7134\n",
      "Training Epoch: 10 [3600/13219]\tLoss: 783.8151\n",
      "Training Epoch: 10 [3650/13219]\tLoss: 768.5911\n",
      "Training Epoch: 10 [3700/13219]\tLoss: 786.2693\n",
      "Training Epoch: 10 [3750/13219]\tLoss: 811.8639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [3800/13219]\tLoss: 796.2307\n",
      "Training Epoch: 10 [3850/13219]\tLoss: 745.1934\n",
      "Training Epoch: 10 [3900/13219]\tLoss: 778.1451\n",
      "Training Epoch: 10 [3950/13219]\tLoss: 837.1130\n",
      "Training Epoch: 10 [4000/13219]\tLoss: 799.7120\n",
      "Training Epoch: 10 [4050/13219]\tLoss: 806.9354\n",
      "Training Epoch: 10 [4100/13219]\tLoss: 783.7639\n",
      "Training Epoch: 10 [4150/13219]\tLoss: 755.3148\n",
      "Training Epoch: 10 [4200/13219]\tLoss: 802.0400\n",
      "Training Epoch: 10 [4250/13219]\tLoss: 792.2134\n",
      "Training Epoch: 10 [4300/13219]\tLoss: 779.1022\n",
      "Training Epoch: 10 [4350/13219]\tLoss: 790.2639\n",
      "Training Epoch: 10 [4400/13219]\tLoss: 781.1014\n",
      "Training Epoch: 10 [4450/13219]\tLoss: 798.0435\n",
      "Training Epoch: 10 [4500/13219]\tLoss: 778.0965\n",
      "Training Epoch: 10 [4550/13219]\tLoss: 809.8346\n",
      "Training Epoch: 10 [4600/13219]\tLoss: 785.8663\n",
      "Training Epoch: 10 [4650/13219]\tLoss: 792.5225\n",
      "Training Epoch: 10 [4700/13219]\tLoss: 765.7509\n",
      "Training Epoch: 10 [4750/13219]\tLoss: 779.7787\n",
      "Training Epoch: 10 [4800/13219]\tLoss: 777.3023\n",
      "Training Epoch: 10 [4850/13219]\tLoss: 773.7325\n",
      "Training Epoch: 10 [4900/13219]\tLoss: 778.2776\n",
      "Training Epoch: 10 [4950/13219]\tLoss: 793.4543\n",
      "Training Epoch: 10 [5000/13219]\tLoss: 800.0934\n",
      "Training Epoch: 10 [5050/13219]\tLoss: 734.5143\n",
      "Training Epoch: 10 [5100/13219]\tLoss: 803.1534\n",
      "Training Epoch: 10 [5150/13219]\tLoss: 822.8327\n",
      "Training Epoch: 10 [5200/13219]\tLoss: 747.8309\n",
      "Training Epoch: 10 [5250/13219]\tLoss: 826.9616\n",
      "Training Epoch: 10 [5300/13219]\tLoss: 758.8344\n",
      "Training Epoch: 10 [5350/13219]\tLoss: 801.3312\n",
      "Training Epoch: 10 [5400/13219]\tLoss: 810.4860\n",
      "Training Epoch: 10 [5450/13219]\tLoss: 781.3665\n",
      "Training Epoch: 10 [5500/13219]\tLoss: 760.0325\n",
      "Training Epoch: 10 [5550/13219]\tLoss: 775.8011\n",
      "Training Epoch: 10 [5600/13219]\tLoss: 759.4413\n",
      "Training Epoch: 10 [5650/13219]\tLoss: 774.7770\n",
      "Training Epoch: 10 [5700/13219]\tLoss: 796.4969\n",
      "Training Epoch: 10 [5750/13219]\tLoss: 765.1937\n",
      "Training Epoch: 10 [5800/13219]\tLoss: 774.2119\n",
      "Training Epoch: 10 [5850/13219]\tLoss: 767.2167\n",
      "Training Epoch: 10 [5900/13219]\tLoss: 793.9858\n",
      "Training Epoch: 10 [5950/13219]\tLoss: 750.5983\n",
      "Training Epoch: 10 [6000/13219]\tLoss: 779.1307\n",
      "Training Epoch: 10 [6050/13219]\tLoss: 797.1309\n",
      "Training Epoch: 10 [6100/13219]\tLoss: 776.4030\n",
      "Training Epoch: 10 [6150/13219]\tLoss: 742.6083\n",
      "Training Epoch: 10 [6200/13219]\tLoss: 776.5110\n",
      "Training Epoch: 10 [6250/13219]\tLoss: 853.3694\n",
      "Training Epoch: 10 [6300/13219]\tLoss: 805.8682\n",
      "Training Epoch: 10 [6350/13219]\tLoss: 802.3797\n",
      "Training Epoch: 10 [6400/13219]\tLoss: 793.6803\n",
      "Training Epoch: 10 [6450/13219]\tLoss: 771.1312\n",
      "Training Epoch: 10 [6500/13219]\tLoss: 772.9092\n",
      "Training Epoch: 10 [6550/13219]\tLoss: 752.7451\n",
      "Training Epoch: 10 [6600/13219]\tLoss: 796.0455\n",
      "Training Epoch: 10 [6650/13219]\tLoss: 765.7362\n",
      "Training Epoch: 10 [6700/13219]\tLoss: 753.3660\n",
      "Training Epoch: 10 [6750/13219]\tLoss: 776.7057\n",
      "Training Epoch: 10 [6800/13219]\tLoss: 803.4659\n",
      "Training Epoch: 10 [6850/13219]\tLoss: 773.2965\n",
      "Training Epoch: 10 [6900/13219]\tLoss: 776.1263\n",
      "Training Epoch: 10 [6950/13219]\tLoss: 761.1240\n",
      "Training Epoch: 10 [7000/13219]\tLoss: 796.8720\n",
      "Training Epoch: 10 [7050/13219]\tLoss: 779.1672\n",
      "Training Epoch: 10 [7100/13219]\tLoss: 768.2950\n",
      "Training Epoch: 10 [7150/13219]\tLoss: 788.4916\n",
      "Training Epoch: 10 [7200/13219]\tLoss: 814.3391\n",
      "Training Epoch: 10 [7250/13219]\tLoss: 857.3087\n",
      "Training Epoch: 10 [7300/13219]\tLoss: 804.7169\n",
      "Training Epoch: 10 [7350/13219]\tLoss: 818.5277\n",
      "Training Epoch: 10 [7400/13219]\tLoss: 784.7368\n",
      "Training Epoch: 10 [7450/13219]\tLoss: 766.8950\n",
      "Training Epoch: 10 [7500/13219]\tLoss: 786.2033\n",
      "Training Epoch: 10 [7550/13219]\tLoss: 785.1587\n",
      "Training Epoch: 10 [7600/13219]\tLoss: 758.1378\n",
      "Training Epoch: 10 [7650/13219]\tLoss: 773.2595\n",
      "Training Epoch: 10 [7700/13219]\tLoss: 798.0089\n",
      "Training Epoch: 10 [7750/13219]\tLoss: 790.5888\n",
      "Training Epoch: 10 [7800/13219]\tLoss: 744.7012\n",
      "Training Epoch: 10 [7850/13219]\tLoss: 797.1985\n",
      "Training Epoch: 10 [7900/13219]\tLoss: 776.1642\n",
      "Training Epoch: 10 [7950/13219]\tLoss: 781.9365\n",
      "Training Epoch: 10 [8000/13219]\tLoss: 762.4752\n",
      "Training Epoch: 10 [8050/13219]\tLoss: 770.7269\n",
      "Training Epoch: 10 [8100/13219]\tLoss: 777.3489\n",
      "Training Epoch: 10 [8150/13219]\tLoss: 836.5152\n",
      "Training Epoch: 10 [8200/13219]\tLoss: 793.2297\n",
      "Training Epoch: 10 [8250/13219]\tLoss: 858.6047\n",
      "Training Epoch: 10 [8300/13219]\tLoss: 800.5551\n",
      "Training Epoch: 10 [8350/13219]\tLoss: 776.9355\n",
      "Training Epoch: 10 [8400/13219]\tLoss: 770.1922\n",
      "Training Epoch: 10 [8450/13219]\tLoss: 794.1439\n",
      "Training Epoch: 10 [8500/13219]\tLoss: 763.8074\n",
      "Training Epoch: 10 [8550/13219]\tLoss: 791.6500\n",
      "Training Epoch: 10 [8600/13219]\tLoss: 816.8569\n",
      "Training Epoch: 10 [8650/13219]\tLoss: 778.4335\n",
      "Training Epoch: 10 [8700/13219]\tLoss: 792.0343\n",
      "Training Epoch: 10 [8750/13219]\tLoss: 803.9524\n",
      "Training Epoch: 10 [8800/13219]\tLoss: 800.3045\n",
      "Training Epoch: 10 [8850/13219]\tLoss: 786.8812\n",
      "Training Epoch: 10 [8900/13219]\tLoss: 752.9473\n",
      "Training Epoch: 10 [8950/13219]\tLoss: 796.3230\n",
      "Training Epoch: 10 [9000/13219]\tLoss: 800.8053\n",
      "Training Epoch: 10 [9050/13219]\tLoss: 782.1370\n",
      "Training Epoch: 10 [9100/13219]\tLoss: 788.3402\n",
      "Training Epoch: 10 [9150/13219]\tLoss: 755.8104\n",
      "Training Epoch: 10 [9200/13219]\tLoss: 778.8792\n",
      "Training Epoch: 10 [9250/13219]\tLoss: 725.5435\n",
      "Training Epoch: 10 [9300/13219]\tLoss: 776.9518\n",
      "Training Epoch: 10 [9350/13219]\tLoss: 754.8068\n",
      "Training Epoch: 10 [9400/13219]\tLoss: 785.6770\n",
      "Training Epoch: 10 [9450/13219]\tLoss: 772.2476\n",
      "Training Epoch: 10 [9500/13219]\tLoss: 815.4285\n",
      "Training Epoch: 10 [9550/13219]\tLoss: 737.7078\n",
      "Training Epoch: 10 [9600/13219]\tLoss: 803.3834\n",
      "Training Epoch: 10 [9650/13219]\tLoss: 747.9368\n",
      "Training Epoch: 10 [9700/13219]\tLoss: 797.9901\n",
      "Training Epoch: 10 [9750/13219]\tLoss: 769.8572\n",
      "Training Epoch: 10 [9800/13219]\tLoss: 795.6998\n",
      "Training Epoch: 10 [9850/13219]\tLoss: 760.2560\n",
      "Training Epoch: 10 [9900/13219]\tLoss: 748.2606\n",
      "Training Epoch: 10 [9950/13219]\tLoss: 794.4034\n",
      "Training Epoch: 10 [10000/13219]\tLoss: 760.5068\n",
      "Training Epoch: 10 [10050/13219]\tLoss: 766.4464\n",
      "Training Epoch: 10 [10100/13219]\tLoss: 782.2201\n",
      "Training Epoch: 10 [10150/13219]\tLoss: 792.3039\n",
      "Training Epoch: 10 [10200/13219]\tLoss: 743.0399\n",
      "Training Epoch: 10 [10250/13219]\tLoss: 772.1873\n",
      "Training Epoch: 10 [10300/13219]\tLoss: 807.5576\n",
      "Training Epoch: 10 [10350/13219]\tLoss: 780.5743\n",
      "Training Epoch: 10 [10400/13219]\tLoss: 779.0901\n",
      "Training Epoch: 10 [10450/13219]\tLoss: 740.0969\n",
      "Training Epoch: 10 [10500/13219]\tLoss: 748.9899\n",
      "Training Epoch: 10 [10550/13219]\tLoss: 749.3414\n",
      "Training Epoch: 10 [10600/13219]\tLoss: 778.6485\n",
      "Training Epoch: 10 [10650/13219]\tLoss: 765.6611\n",
      "Training Epoch: 10 [10700/13219]\tLoss: 775.9498\n",
      "Training Epoch: 10 [10750/13219]\tLoss: 770.0007\n",
      "Training Epoch: 10 [10800/13219]\tLoss: 773.1880\n",
      "Training Epoch: 10 [10850/13219]\tLoss: 804.8221\n",
      "Training Epoch: 10 [10900/13219]\tLoss: 784.4751\n",
      "Training Epoch: 10 [10950/13219]\tLoss: 764.4423\n",
      "Training Epoch: 10 [11000/13219]\tLoss: 743.6338\n",
      "Training Epoch: 10 [11050/13219]\tLoss: 766.1351\n",
      "Training Epoch: 10 [11100/13219]\tLoss: 791.8376\n",
      "Training Epoch: 10 [11150/13219]\tLoss: 781.4034\n",
      "Training Epoch: 10 [11200/13219]\tLoss: 792.8173\n",
      "Training Epoch: 10 [11250/13219]\tLoss: 816.9633\n",
      "Training Epoch: 10 [11300/13219]\tLoss: 758.4337\n",
      "Training Epoch: 10 [11350/13219]\tLoss: 766.1006\n",
      "Training Epoch: 10 [11400/13219]\tLoss: 768.0103\n",
      "Training Epoch: 10 [11450/13219]\tLoss: 798.2266\n",
      "Training Epoch: 10 [11500/13219]\tLoss: 770.7454\n",
      "Training Epoch: 10 [11550/13219]\tLoss: 803.7418\n",
      "Training Epoch: 10 [11600/13219]\tLoss: 766.3798\n",
      "Training Epoch: 10 [11650/13219]\tLoss: 765.7550\n",
      "Training Epoch: 10 [11700/13219]\tLoss: 784.8557\n",
      "Training Epoch: 10 [11750/13219]\tLoss: 780.1505\n",
      "Training Epoch: 10 [11800/13219]\tLoss: 771.6082\n",
      "Training Epoch: 10 [11850/13219]\tLoss: 750.7721\n",
      "Training Epoch: 10 [11900/13219]\tLoss: 784.3555\n",
      "Training Epoch: 10 [11950/13219]\tLoss: 817.9779\n",
      "Training Epoch: 10 [12000/13219]\tLoss: 776.6628\n",
      "Training Epoch: 10 [12050/13219]\tLoss: 795.2543\n",
      "Training Epoch: 10 [12100/13219]\tLoss: 749.8550\n",
      "Training Epoch: 10 [12150/13219]\tLoss: 760.8812\n",
      "Training Epoch: 10 [12200/13219]\tLoss: 796.7692\n",
      "Training Epoch: 10 [12250/13219]\tLoss: 741.9151\n",
      "Training Epoch: 10 [12300/13219]\tLoss: 784.7199\n",
      "Training Epoch: 10 [12350/13219]\tLoss: 784.1863\n",
      "Training Epoch: 10 [12400/13219]\tLoss: 797.1956\n",
      "Training Epoch: 10 [12450/13219]\tLoss: 770.1163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [12500/13219]\tLoss: 785.1507\n",
      "Training Epoch: 10 [12550/13219]\tLoss: 788.5215\n",
      "Training Epoch: 10 [12600/13219]\tLoss: 757.6089\n",
      "Training Epoch: 10 [12650/13219]\tLoss: 747.1111\n",
      "Training Epoch: 10 [12700/13219]\tLoss: 810.2422\n",
      "Training Epoch: 10 [12750/13219]\tLoss: 780.8810\n",
      "Training Epoch: 10 [12800/13219]\tLoss: 781.0113\n",
      "Training Epoch: 10 [12850/13219]\tLoss: 764.7313\n",
      "Training Epoch: 10 [12900/13219]\tLoss: 749.3702\n",
      "Training Epoch: 10 [12950/13219]\tLoss: 781.1183\n",
      "Training Epoch: 10 [13000/13219]\tLoss: 769.3643\n",
      "Training Epoch: 10 [13050/13219]\tLoss: 734.0872\n",
      "Training Epoch: 10 [13100/13219]\tLoss: 760.7626\n",
      "Training Epoch: 10 [13150/13219]\tLoss: 762.7515\n",
      "Training Epoch: 10 [13200/13219]\tLoss: 778.0087\n",
      "Training Epoch: 10 [13219/13219]\tLoss: 786.2709\n",
      "Training Epoch: 10 [1469/1469]\tLoss: 765.5929\n",
      "Training Epoch: 11 [50/13219]\tLoss: 785.4803\n",
      "Training Epoch: 11 [100/13219]\tLoss: 754.8025\n",
      "Training Epoch: 11 [150/13219]\tLoss: 767.1896\n",
      "Training Epoch: 11 [200/13219]\tLoss: 768.9055\n",
      "Training Epoch: 11 [250/13219]\tLoss: 770.1196\n",
      "Training Epoch: 11 [300/13219]\tLoss: 753.7084\n",
      "Training Epoch: 11 [350/13219]\tLoss: 777.5038\n",
      "Training Epoch: 11 [400/13219]\tLoss: 744.0493\n",
      "Training Epoch: 11 [450/13219]\tLoss: 784.4154\n",
      "Training Epoch: 11 [500/13219]\tLoss: 752.0461\n",
      "Training Epoch: 11 [550/13219]\tLoss: 752.8394\n",
      "Training Epoch: 11 [600/13219]\tLoss: 769.7922\n",
      "Training Epoch: 11 [650/13219]\tLoss: 759.3039\n",
      "Training Epoch: 11 [700/13219]\tLoss: 769.6406\n",
      "Training Epoch: 11 [750/13219]\tLoss: 749.2724\n",
      "Training Epoch: 11 [800/13219]\tLoss: 789.7604\n",
      "Training Epoch: 11 [850/13219]\tLoss: 792.5317\n",
      "Training Epoch: 11 [900/13219]\tLoss: 767.3026\n",
      "Training Epoch: 11 [950/13219]\tLoss: 782.3699\n",
      "Training Epoch: 11 [1000/13219]\tLoss: 779.6553\n",
      "Training Epoch: 11 [1050/13219]\tLoss: 811.3021\n",
      "Training Epoch: 11 [1100/13219]\tLoss: 773.3070\n",
      "Training Epoch: 11 [1150/13219]\tLoss: 797.7959\n",
      "Training Epoch: 11 [1200/13219]\tLoss: 783.7058\n",
      "Training Epoch: 11 [1250/13219]\tLoss: 737.8506\n",
      "Training Epoch: 11 [1300/13219]\tLoss: 737.7128\n",
      "Training Epoch: 11 [1350/13219]\tLoss: 767.1824\n",
      "Training Epoch: 11 [1400/13219]\tLoss: 759.8803\n",
      "Training Epoch: 11 [1450/13219]\tLoss: 755.0526\n",
      "Training Epoch: 11 [1500/13219]\tLoss: 782.4890\n",
      "Training Epoch: 11 [1550/13219]\tLoss: 785.6794\n",
      "Training Epoch: 11 [1600/13219]\tLoss: 785.0267\n",
      "Training Epoch: 11 [1650/13219]\tLoss: 769.1093\n",
      "Training Epoch: 11 [1700/13219]\tLoss: 775.5941\n",
      "Training Epoch: 11 [1750/13219]\tLoss: 792.0817\n",
      "Training Epoch: 11 [1800/13219]\tLoss: 757.3598\n",
      "Training Epoch: 11 [1850/13219]\tLoss: 765.5923\n",
      "Training Epoch: 11 [1900/13219]\tLoss: 762.0736\n",
      "Training Epoch: 11 [1950/13219]\tLoss: 764.9656\n",
      "Training Epoch: 11 [2000/13219]\tLoss: 790.6743\n",
      "Training Epoch: 11 [2050/13219]\tLoss: 764.7121\n",
      "Training Epoch: 11 [2100/13219]\tLoss: 749.2731\n",
      "Training Epoch: 11 [2150/13219]\tLoss: 778.3336\n",
      "Training Epoch: 11 [2200/13219]\tLoss: 807.4182\n",
      "Training Epoch: 11 [2250/13219]\tLoss: 771.0483\n",
      "Training Epoch: 11 [2300/13219]\tLoss: 798.8555\n",
      "Training Epoch: 11 [2350/13219]\tLoss: 773.6721\n",
      "Training Epoch: 11 [2400/13219]\tLoss: 797.2053\n",
      "Training Epoch: 11 [2450/13219]\tLoss: 749.6064\n",
      "Training Epoch: 11 [2500/13219]\tLoss: 757.3073\n",
      "Training Epoch: 11 [2550/13219]\tLoss: 796.0935\n",
      "Training Epoch: 11 [2600/13219]\tLoss: 847.4999\n",
      "Training Epoch: 11 [2650/13219]\tLoss: 761.7833\n",
      "Training Epoch: 11 [2700/13219]\tLoss: 818.7113\n",
      "Training Epoch: 11 [2750/13219]\tLoss: 752.1121\n",
      "Training Epoch: 11 [2800/13219]\tLoss: 758.8813\n",
      "Training Epoch: 11 [2850/13219]\tLoss: 752.8546\n",
      "Training Epoch: 11 [2900/13219]\tLoss: 765.4849\n",
      "Training Epoch: 11 [2950/13219]\tLoss: 747.3737\n",
      "Training Epoch: 11 [3000/13219]\tLoss: 761.3542\n",
      "Training Epoch: 11 [3050/13219]\tLoss: 767.6891\n",
      "Training Epoch: 11 [3100/13219]\tLoss: 775.1268\n",
      "Training Epoch: 11 [3150/13219]\tLoss: 795.1959\n",
      "Training Epoch: 11 [3200/13219]\tLoss: 824.0154\n",
      "Training Epoch: 11 [3250/13219]\tLoss: 805.0054\n",
      "Training Epoch: 11 [3300/13219]\tLoss: 774.8619\n",
      "Training Epoch: 11 [3350/13219]\tLoss: 725.3019\n",
      "Training Epoch: 11 [3400/13219]\tLoss: 759.9598\n",
      "Training Epoch: 11 [3450/13219]\tLoss: 787.4949\n",
      "Training Epoch: 11 [3500/13219]\tLoss: 762.5278\n",
      "Training Epoch: 11 [3550/13219]\tLoss: 763.7898\n",
      "Training Epoch: 11 [3600/13219]\tLoss: 743.2684\n",
      "Training Epoch: 11 [3650/13219]\tLoss: 770.6257\n",
      "Training Epoch: 11 [3700/13219]\tLoss: 739.4379\n",
      "Training Epoch: 11 [3750/13219]\tLoss: 797.0980\n",
      "Training Epoch: 11 [3800/13219]\tLoss: 787.7081\n",
      "Training Epoch: 11 [3850/13219]\tLoss: 776.8088\n",
      "Training Epoch: 11 [3900/13219]\tLoss: 738.5505\n",
      "Training Epoch: 11 [3950/13219]\tLoss: 759.4739\n",
      "Training Epoch: 11 [4000/13219]\tLoss: 815.9998\n",
      "Training Epoch: 11 [4050/13219]\tLoss: 797.9413\n",
      "Training Epoch: 11 [4100/13219]\tLoss: 776.0261\n",
      "Training Epoch: 11 [4150/13219]\tLoss: 751.2726\n",
      "Training Epoch: 11 [4200/13219]\tLoss: 788.3984\n",
      "Training Epoch: 11 [4250/13219]\tLoss: 743.3018\n",
      "Training Epoch: 11 [4300/13219]\tLoss: 757.9374\n",
      "Training Epoch: 11 [4350/13219]\tLoss: 739.1454\n",
      "Training Epoch: 11 [4400/13219]\tLoss: 787.2535\n",
      "Training Epoch: 11 [4450/13219]\tLoss: 765.8906\n",
      "Training Epoch: 11 [4500/13219]\tLoss: 759.2228\n",
      "Training Epoch: 11 [4550/13219]\tLoss: 713.6627\n",
      "Training Epoch: 11 [4600/13219]\tLoss: 775.5120\n",
      "Training Epoch: 11 [4650/13219]\tLoss: 786.9246\n",
      "Training Epoch: 11 [4700/13219]\tLoss: 783.2424\n",
      "Training Epoch: 11 [4750/13219]\tLoss: 760.6690\n",
      "Training Epoch: 11 [4800/13219]\tLoss: 786.3986\n",
      "Training Epoch: 11 [4850/13219]\tLoss: 772.4421\n",
      "Training Epoch: 11 [4900/13219]\tLoss: 776.9421\n",
      "Training Epoch: 11 [4950/13219]\tLoss: 750.0933\n",
      "Training Epoch: 11 [5000/13219]\tLoss: 746.7090\n",
      "Training Epoch: 11 [5050/13219]\tLoss: 780.9936\n",
      "Training Epoch: 11 [5100/13219]\tLoss: 778.2996\n",
      "Training Epoch: 11 [5150/13219]\tLoss: 817.0681\n",
      "Training Epoch: 11 [5200/13219]\tLoss: 757.8064\n",
      "Training Epoch: 11 [5250/13219]\tLoss: 794.4446\n",
      "Training Epoch: 11 [5300/13219]\tLoss: 756.3802\n",
      "Training Epoch: 11 [5350/13219]\tLoss: 756.9456\n",
      "Training Epoch: 11 [5400/13219]\tLoss: 769.5092\n",
      "Training Epoch: 11 [5450/13219]\tLoss: 779.7097\n",
      "Training Epoch: 11 [5500/13219]\tLoss: 787.9327\n",
      "Training Epoch: 11 [5550/13219]\tLoss: 752.5626\n",
      "Training Epoch: 11 [5600/13219]\tLoss: 748.7579\n",
      "Training Epoch: 11 [5650/13219]\tLoss: 811.7304\n",
      "Training Epoch: 11 [5700/13219]\tLoss: 772.8350\n",
      "Training Epoch: 11 [5750/13219]\tLoss: 768.0718\n",
      "Training Epoch: 11 [5800/13219]\tLoss: 765.2409\n",
      "Training Epoch: 11 [5850/13219]\tLoss: 771.6059\n",
      "Training Epoch: 11 [5900/13219]\tLoss: 753.6458\n",
      "Training Epoch: 11 [5950/13219]\tLoss: 748.4045\n",
      "Training Epoch: 11 [6000/13219]\tLoss: 781.2982\n",
      "Training Epoch: 11 [6050/13219]\tLoss: 720.6018\n",
      "Training Epoch: 11 [6100/13219]\tLoss: 784.5162\n",
      "Training Epoch: 11 [6150/13219]\tLoss: 774.7722\n",
      "Training Epoch: 11 [6200/13219]\tLoss: 748.6835\n",
      "Training Epoch: 11 [6250/13219]\tLoss: 767.6522\n",
      "Training Epoch: 11 [6300/13219]\tLoss: 776.6122\n",
      "Training Epoch: 11 [6350/13219]\tLoss: 732.4613\n",
      "Training Epoch: 11 [6400/13219]\tLoss: 757.0073\n",
      "Training Epoch: 11 [6450/13219]\tLoss: 764.8815\n",
      "Training Epoch: 11 [6500/13219]\tLoss: 732.8763\n",
      "Training Epoch: 11 [6550/13219]\tLoss: 778.9918\n",
      "Training Epoch: 11 [6600/13219]\tLoss: 792.7382\n",
      "Training Epoch: 11 [6650/13219]\tLoss: 757.8488\n",
      "Training Epoch: 11 [6700/13219]\tLoss: 756.3555\n",
      "Training Epoch: 11 [6750/13219]\tLoss: 750.7667\n",
      "Training Epoch: 11 [6800/13219]\tLoss: 798.0504\n",
      "Training Epoch: 11 [6850/13219]\tLoss: 777.8568\n",
      "Training Epoch: 11 [6900/13219]\tLoss: 744.3021\n",
      "Training Epoch: 11 [6950/13219]\tLoss: 736.5165\n",
      "Training Epoch: 11 [7000/13219]\tLoss: 761.6136\n",
      "Training Epoch: 11 [7050/13219]\tLoss: 771.5328\n",
      "Training Epoch: 11 [7100/13219]\tLoss: 779.1292\n",
      "Training Epoch: 11 [7150/13219]\tLoss: 756.2946\n",
      "Training Epoch: 11 [7200/13219]\tLoss: 772.4471\n",
      "Training Epoch: 11 [7250/13219]\tLoss: 797.9235\n",
      "Training Epoch: 11 [7300/13219]\tLoss: 758.2045\n",
      "Training Epoch: 11 [7350/13219]\tLoss: 746.9722\n",
      "Training Epoch: 11 [7400/13219]\tLoss: 769.4974\n",
      "Training Epoch: 11 [7450/13219]\tLoss: 734.6820\n",
      "Training Epoch: 11 [7500/13219]\tLoss: 785.8801\n",
      "Training Epoch: 11 [7550/13219]\tLoss: 787.0522\n",
      "Training Epoch: 11 [7600/13219]\tLoss: 774.1514\n",
      "Training Epoch: 11 [7650/13219]\tLoss: 783.8993\n",
      "Training Epoch: 11 [7700/13219]\tLoss: 750.1086\n",
      "Training Epoch: 11 [7750/13219]\tLoss: 722.6815\n",
      "Training Epoch: 11 [7800/13219]\tLoss: 759.4146\n",
      "Training Epoch: 11 [7850/13219]\tLoss: 786.4366\n",
      "Training Epoch: 11 [7900/13219]\tLoss: 756.4521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [7950/13219]\tLoss: 781.0494\n",
      "Training Epoch: 11 [8000/13219]\tLoss: 812.4211\n",
      "Training Epoch: 11 [8050/13219]\tLoss: 796.2480\n",
      "Training Epoch: 11 [8100/13219]\tLoss: 758.9370\n",
      "Training Epoch: 11 [8150/13219]\tLoss: 804.8621\n",
      "Training Epoch: 11 [8200/13219]\tLoss: 782.4818\n",
      "Training Epoch: 11 [8250/13219]\tLoss: 787.1219\n",
      "Training Epoch: 11 [8300/13219]\tLoss: 768.0421\n",
      "Training Epoch: 11 [8350/13219]\tLoss: 775.7699\n",
      "Training Epoch: 11 [8400/13219]\tLoss: 741.6780\n",
      "Training Epoch: 11 [8450/13219]\tLoss: 753.2150\n",
      "Training Epoch: 11 [8500/13219]\tLoss: 713.1623\n",
      "Training Epoch: 11 [8550/13219]\tLoss: 765.9940\n",
      "Training Epoch: 11 [8600/13219]\tLoss: 764.5540\n",
      "Training Epoch: 11 [8650/13219]\tLoss: 758.1782\n",
      "Training Epoch: 11 [8700/13219]\tLoss: 743.1815\n",
      "Training Epoch: 11 [8750/13219]\tLoss: 771.5396\n",
      "Training Epoch: 11 [8800/13219]\tLoss: 752.6624\n",
      "Training Epoch: 11 [8850/13219]\tLoss: 731.4648\n",
      "Training Epoch: 11 [8900/13219]\tLoss: 790.6486\n",
      "Training Epoch: 11 [8950/13219]\tLoss: 751.7080\n",
      "Training Epoch: 11 [9000/13219]\tLoss: 779.7416\n",
      "Training Epoch: 11 [9050/13219]\tLoss: 759.2125\n",
      "Training Epoch: 11 [9100/13219]\tLoss: 794.5811\n",
      "Training Epoch: 11 [9150/13219]\tLoss: 767.5127\n",
      "Training Epoch: 11 [9200/13219]\tLoss: 774.7304\n",
      "Training Epoch: 11 [9250/13219]\tLoss: 734.6422\n",
      "Training Epoch: 11 [9300/13219]\tLoss: 791.1775\n",
      "Training Epoch: 11 [9350/13219]\tLoss: 740.0248\n",
      "Training Epoch: 11 [9400/13219]\tLoss: 764.1423\n",
      "Training Epoch: 11 [9450/13219]\tLoss: 763.2406\n",
      "Training Epoch: 11 [9500/13219]\tLoss: 791.4099\n",
      "Training Epoch: 11 [9550/13219]\tLoss: 801.1823\n",
      "Training Epoch: 11 [9600/13219]\tLoss: 719.2969\n",
      "Training Epoch: 11 [9650/13219]\tLoss: 754.6870\n",
      "Training Epoch: 11 [9700/13219]\tLoss: 787.8203\n",
      "Training Epoch: 11 [9750/13219]\tLoss: 749.0308\n",
      "Training Epoch: 11 [9800/13219]\tLoss: 770.1180\n",
      "Training Epoch: 11 [9850/13219]\tLoss: 711.7108\n",
      "Training Epoch: 11 [9900/13219]\tLoss: 741.4921\n",
      "Training Epoch: 11 [9950/13219]\tLoss: 766.0500\n",
      "Training Epoch: 11 [10000/13219]\tLoss: 760.7527\n",
      "Training Epoch: 11 [10050/13219]\tLoss: 782.2760\n",
      "Training Epoch: 11 [10100/13219]\tLoss: 720.6617\n",
      "Training Epoch: 11 [10150/13219]\tLoss: 728.4436\n",
      "Training Epoch: 11 [10200/13219]\tLoss: 842.0803\n",
      "Training Epoch: 11 [10250/13219]\tLoss: 770.4930\n",
      "Training Epoch: 11 [10300/13219]\tLoss: 776.4281\n",
      "Training Epoch: 11 [10350/13219]\tLoss: 764.1528\n",
      "Training Epoch: 11 [10400/13219]\tLoss: 743.5259\n",
      "Training Epoch: 11 [10450/13219]\tLoss: 799.3498\n",
      "Training Epoch: 11 [10500/13219]\tLoss: 761.9271\n",
      "Training Epoch: 11 [10550/13219]\tLoss: 759.6474\n",
      "Training Epoch: 11 [10600/13219]\tLoss: 800.1693\n",
      "Training Epoch: 11 [10650/13219]\tLoss: 767.5422\n",
      "Training Epoch: 11 [10700/13219]\tLoss: 721.8112\n",
      "Training Epoch: 11 [10750/13219]\tLoss: 733.4467\n",
      "Training Epoch: 11 [10800/13219]\tLoss: 760.5753\n",
      "Training Epoch: 11 [10850/13219]\tLoss: 727.5274\n",
      "Training Epoch: 11 [10900/13219]\tLoss: 733.4432\n",
      "Training Epoch: 11 [10950/13219]\tLoss: 751.9212\n",
      "Training Epoch: 11 [11000/13219]\tLoss: 748.1271\n",
      "Training Epoch: 11 [11050/13219]\tLoss: 768.7609\n",
      "Training Epoch: 11 [11100/13219]\tLoss: 789.3350\n",
      "Training Epoch: 11 [11150/13219]\tLoss: 763.4147\n",
      "Training Epoch: 11 [11200/13219]\tLoss: 764.5189\n",
      "Training Epoch: 11 [11250/13219]\tLoss: 776.5498\n",
      "Training Epoch: 11 [11300/13219]\tLoss: 752.1892\n",
      "Training Epoch: 11 [11350/13219]\tLoss: 748.5952\n",
      "Training Epoch: 11 [11400/13219]\tLoss: 772.3335\n",
      "Training Epoch: 11 [11450/13219]\tLoss: 737.2853\n",
      "Training Epoch: 11 [11500/13219]\tLoss: 799.5914\n",
      "Training Epoch: 11 [11550/13219]\tLoss: 749.1166\n",
      "Training Epoch: 11 [11600/13219]\tLoss: 758.7621\n",
      "Training Epoch: 11 [11650/13219]\tLoss: 732.8654\n",
      "Training Epoch: 11 [11700/13219]\tLoss: 736.8571\n",
      "Training Epoch: 11 [11750/13219]\tLoss: 723.9058\n",
      "Training Epoch: 11 [11800/13219]\tLoss: 744.4229\n",
      "Training Epoch: 11 [11850/13219]\tLoss: 764.1051\n",
      "Training Epoch: 11 [11900/13219]\tLoss: 748.3657\n",
      "Training Epoch: 11 [11950/13219]\tLoss: 803.7772\n",
      "Training Epoch: 11 [12000/13219]\tLoss: 787.8140\n",
      "Training Epoch: 11 [12050/13219]\tLoss: 772.6262\n",
      "Training Epoch: 11 [12100/13219]\tLoss: 758.5972\n",
      "Training Epoch: 11 [12150/13219]\tLoss: 769.7006\n",
      "Training Epoch: 11 [12200/13219]\tLoss: 764.8768\n",
      "Training Epoch: 11 [12250/13219]\tLoss: 796.7051\n",
      "Training Epoch: 11 [12300/13219]\tLoss: 752.4667\n",
      "Training Epoch: 11 [12350/13219]\tLoss: 773.5084\n",
      "Training Epoch: 11 [12400/13219]\tLoss: 789.6089\n",
      "Training Epoch: 11 [12450/13219]\tLoss: 762.5652\n",
      "Training Epoch: 11 [12500/13219]\tLoss: 757.2697\n",
      "Training Epoch: 11 [12550/13219]\tLoss: 699.4894\n",
      "Training Epoch: 11 [12600/13219]\tLoss: 743.4489\n",
      "Training Epoch: 11 [12650/13219]\tLoss: 791.3286\n",
      "Training Epoch: 11 [12700/13219]\tLoss: 711.6008\n",
      "Training Epoch: 11 [12750/13219]\tLoss: 756.3699\n",
      "Training Epoch: 11 [12800/13219]\tLoss: 766.4878\n",
      "Training Epoch: 11 [12850/13219]\tLoss: 747.7151\n",
      "Training Epoch: 11 [12900/13219]\tLoss: 748.9158\n",
      "Training Epoch: 11 [12950/13219]\tLoss: 768.9790\n",
      "Training Epoch: 11 [13000/13219]\tLoss: 702.9572\n",
      "Training Epoch: 11 [13050/13219]\tLoss: 734.4680\n",
      "Training Epoch: 11 [13100/13219]\tLoss: 755.2542\n",
      "Training Epoch: 11 [13150/13219]\tLoss: 734.2482\n",
      "Training Epoch: 11 [13200/13219]\tLoss: 752.8259\n",
      "Training Epoch: 11 [13219/13219]\tLoss: 751.3180\n",
      "Training Epoch: 11 [1469/1469]\tLoss: 749.1782\n",
      "Training Epoch: 12 [50/13219]\tLoss: 801.4380\n",
      "Training Epoch: 12 [100/13219]\tLoss: 713.6412\n",
      "Training Epoch: 12 [150/13219]\tLoss: 737.8062\n",
      "Training Epoch: 12 [200/13219]\tLoss: 762.9059\n",
      "Training Epoch: 12 [250/13219]\tLoss: 747.0310\n",
      "Training Epoch: 12 [300/13219]\tLoss: 745.9301\n",
      "Training Epoch: 12 [350/13219]\tLoss: 756.7660\n",
      "Training Epoch: 12 [400/13219]\tLoss: 760.2801\n",
      "Training Epoch: 12 [450/13219]\tLoss: 749.4849\n",
      "Training Epoch: 12 [500/13219]\tLoss: 752.9916\n",
      "Training Epoch: 12 [550/13219]\tLoss: 753.6169\n",
      "Training Epoch: 12 [600/13219]\tLoss: 790.6081\n",
      "Training Epoch: 12 [650/13219]\tLoss: 748.2315\n",
      "Training Epoch: 12 [700/13219]\tLoss: 781.2299\n",
      "Training Epoch: 12 [750/13219]\tLoss: 752.0966\n",
      "Training Epoch: 12 [800/13219]\tLoss: 764.6151\n",
      "Training Epoch: 12 [850/13219]\tLoss: 746.9890\n",
      "Training Epoch: 12 [900/13219]\tLoss: 755.3599\n",
      "Training Epoch: 12 [950/13219]\tLoss: 766.9590\n",
      "Training Epoch: 12 [1000/13219]\tLoss: 759.9576\n",
      "Training Epoch: 12 [1050/13219]\tLoss: 755.2440\n",
      "Training Epoch: 12 [1100/13219]\tLoss: 752.5990\n",
      "Training Epoch: 12 [1150/13219]\tLoss: 778.3395\n",
      "Training Epoch: 12 [1200/13219]\tLoss: 782.8784\n",
      "Training Epoch: 12 [1250/13219]\tLoss: 768.3203\n",
      "Training Epoch: 12 [1300/13219]\tLoss: 781.6234\n",
      "Training Epoch: 12 [1350/13219]\tLoss: 777.1380\n",
      "Training Epoch: 12 [1400/13219]\tLoss: 759.2595\n",
      "Training Epoch: 12 [1450/13219]\tLoss: 735.6761\n",
      "Training Epoch: 12 [1500/13219]\tLoss: 734.4989\n",
      "Training Epoch: 12 [1550/13219]\tLoss: 772.9723\n",
      "Training Epoch: 12 [1600/13219]\tLoss: 770.4210\n",
      "Training Epoch: 12 [1650/13219]\tLoss: 777.9180\n",
      "Training Epoch: 12 [1700/13219]\tLoss: 742.2310\n",
      "Training Epoch: 12 [1750/13219]\tLoss: 759.4426\n",
      "Training Epoch: 12 [1800/13219]\tLoss: 752.6242\n",
      "Training Epoch: 12 [1850/13219]\tLoss: 762.4013\n",
      "Training Epoch: 12 [1900/13219]\tLoss: 784.7536\n",
      "Training Epoch: 12 [1950/13219]\tLoss: 729.2885\n",
      "Training Epoch: 12 [2000/13219]\tLoss: 726.4798\n",
      "Training Epoch: 12 [2050/13219]\tLoss: 757.2441\n",
      "Training Epoch: 12 [2100/13219]\tLoss: 754.3626\n",
      "Training Epoch: 12 [2150/13219]\tLoss: 753.6970\n",
      "Training Epoch: 12 [2200/13219]\tLoss: 740.7230\n",
      "Training Epoch: 12 [2250/13219]\tLoss: 745.6462\n",
      "Training Epoch: 12 [2300/13219]\tLoss: 739.0199\n",
      "Training Epoch: 12 [2350/13219]\tLoss: 762.7310\n",
      "Training Epoch: 12 [2400/13219]\tLoss: 764.7548\n",
      "Training Epoch: 12 [2450/13219]\tLoss: 719.6797\n",
      "Training Epoch: 12 [2500/13219]\tLoss: 772.8474\n",
      "Training Epoch: 12 [2550/13219]\tLoss: 733.8669\n",
      "Training Epoch: 12 [2600/13219]\tLoss: 781.4712\n",
      "Training Epoch: 12 [2650/13219]\tLoss: 736.1443\n",
      "Training Epoch: 12 [2700/13219]\tLoss: 775.7704\n",
      "Training Epoch: 12 [2750/13219]\tLoss: 748.0065\n",
      "Training Epoch: 12 [2800/13219]\tLoss: 774.4373\n",
      "Training Epoch: 12 [2850/13219]\tLoss: 780.7495\n",
      "Training Epoch: 12 [2900/13219]\tLoss: 790.1979\n",
      "Training Epoch: 12 [2950/13219]\tLoss: 752.3564\n",
      "Training Epoch: 12 [3000/13219]\tLoss: 742.9598\n",
      "Training Epoch: 12 [3050/13219]\tLoss: 736.5872\n",
      "Training Epoch: 12 [3100/13219]\tLoss: 727.8593\n",
      "Training Epoch: 12 [3150/13219]\tLoss: 736.1618\n",
      "Training Epoch: 12 [3200/13219]\tLoss: 753.5297\n",
      "Training Epoch: 12 [3250/13219]\tLoss: 727.6957\n",
      "Training Epoch: 12 [3300/13219]\tLoss: 752.3603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [3350/13219]\tLoss: 765.2080\n",
      "Training Epoch: 12 [3400/13219]\tLoss: 747.4316\n",
      "Training Epoch: 12 [3450/13219]\tLoss: 752.9983\n",
      "Training Epoch: 12 [3500/13219]\tLoss: 744.0005\n",
      "Training Epoch: 12 [3550/13219]\tLoss: 743.2200\n",
      "Training Epoch: 12 [3600/13219]\tLoss: 724.2457\n",
      "Training Epoch: 12 [3650/13219]\tLoss: 779.1958\n",
      "Training Epoch: 12 [3700/13219]\tLoss: 750.3735\n",
      "Training Epoch: 12 [3750/13219]\tLoss: 768.4714\n",
      "Training Epoch: 12 [3800/13219]\tLoss: 756.9950\n",
      "Training Epoch: 12 [3850/13219]\tLoss: 794.5529\n",
      "Training Epoch: 12 [3900/13219]\tLoss: 747.1916\n",
      "Training Epoch: 12 [3950/13219]\tLoss: 765.6245\n",
      "Training Epoch: 12 [4000/13219]\tLoss: 745.7017\n",
      "Training Epoch: 12 [4050/13219]\tLoss: 779.3165\n",
      "Training Epoch: 12 [4100/13219]\tLoss: 709.5043\n",
      "Training Epoch: 12 [4150/13219]\tLoss: 730.7175\n",
      "Training Epoch: 12 [4200/13219]\tLoss: 758.1528\n",
      "Training Epoch: 12 [4250/13219]\tLoss: 766.2653\n",
      "Training Epoch: 12 [4300/13219]\tLoss: 785.7714\n",
      "Training Epoch: 12 [4350/13219]\tLoss: 763.4228\n",
      "Training Epoch: 12 [4400/13219]\tLoss: 744.8657\n",
      "Training Epoch: 12 [4450/13219]\tLoss: 783.2832\n",
      "Training Epoch: 12 [4500/13219]\tLoss: 727.7787\n",
      "Training Epoch: 12 [4550/13219]\tLoss: 748.4558\n",
      "Training Epoch: 12 [4600/13219]\tLoss: 728.8467\n",
      "Training Epoch: 12 [4650/13219]\tLoss: 763.6046\n",
      "Training Epoch: 12 [4700/13219]\tLoss: 737.4054\n",
      "Training Epoch: 12 [4750/13219]\tLoss: 785.3203\n",
      "Training Epoch: 12 [4800/13219]\tLoss: 733.8184\n",
      "Training Epoch: 12 [4850/13219]\tLoss: 737.4868\n",
      "Training Epoch: 12 [4900/13219]\tLoss: 779.8743\n",
      "Training Epoch: 12 [4950/13219]\tLoss: 766.4183\n",
      "Training Epoch: 12 [5000/13219]\tLoss: 713.9824\n",
      "Training Epoch: 12 [5050/13219]\tLoss: 749.6631\n",
      "Training Epoch: 12 [5100/13219]\tLoss: 760.3987\n",
      "Training Epoch: 12 [5150/13219]\tLoss: 781.1655\n",
      "Training Epoch: 12 [5200/13219]\tLoss: 727.5195\n",
      "Training Epoch: 12 [5250/13219]\tLoss: 711.3837\n",
      "Training Epoch: 12 [5300/13219]\tLoss: 764.7534\n",
      "Training Epoch: 12 [5350/13219]\tLoss: 784.1625\n",
      "Training Epoch: 12 [5400/13219]\tLoss: 769.5390\n",
      "Training Epoch: 12 [5450/13219]\tLoss: 762.9721\n",
      "Training Epoch: 12 [5500/13219]\tLoss: 769.3379\n",
      "Training Epoch: 12 [5550/13219]\tLoss: 779.7337\n",
      "Training Epoch: 12 [5600/13219]\tLoss: 771.6901\n",
      "Training Epoch: 12 [5650/13219]\tLoss: 737.2837\n",
      "Training Epoch: 12 [5700/13219]\tLoss: 733.3905\n",
      "Training Epoch: 12 [5750/13219]\tLoss: 716.0344\n",
      "Training Epoch: 12 [5800/13219]\tLoss: 749.2399\n",
      "Training Epoch: 12 [5850/13219]\tLoss: 733.6675\n",
      "Training Epoch: 12 [5900/13219]\tLoss: 784.2073\n",
      "Training Epoch: 12 [5950/13219]\tLoss: 708.6572\n",
      "Training Epoch: 12 [6000/13219]\tLoss: 737.8017\n",
      "Training Epoch: 12 [6050/13219]\tLoss: 739.4948\n",
      "Training Epoch: 12 [6100/13219]\tLoss: 735.8054\n",
      "Training Epoch: 12 [6150/13219]\tLoss: 720.4407\n",
      "Training Epoch: 12 [6200/13219]\tLoss: 747.4895\n",
      "Training Epoch: 12 [6250/13219]\tLoss: 743.6127\n",
      "Training Epoch: 12 [6300/13219]\tLoss: 740.8879\n",
      "Training Epoch: 12 [6350/13219]\tLoss: 754.3360\n",
      "Training Epoch: 12 [6400/13219]\tLoss: 760.7788\n",
      "Training Epoch: 12 [6450/13219]\tLoss: 716.4716\n",
      "Training Epoch: 12 [6500/13219]\tLoss: 753.8835\n",
      "Training Epoch: 12 [6550/13219]\tLoss: 739.9510\n",
      "Training Epoch: 12 [6600/13219]\tLoss: 727.5234\n",
      "Training Epoch: 12 [6650/13219]\tLoss: 750.2612\n",
      "Training Epoch: 12 [6700/13219]\tLoss: 768.6158\n",
      "Training Epoch: 12 [6750/13219]\tLoss: 718.0961\n",
      "Training Epoch: 12 [6800/13219]\tLoss: 754.8383\n",
      "Training Epoch: 12 [6850/13219]\tLoss: 752.5728\n",
      "Training Epoch: 12 [6900/13219]\tLoss: 750.2786\n",
      "Training Epoch: 12 [6950/13219]\tLoss: 771.2012\n",
      "Training Epoch: 12 [7000/13219]\tLoss: 774.3937\n",
      "Training Epoch: 12 [7050/13219]\tLoss: 729.5673\n",
      "Training Epoch: 12 [7100/13219]\tLoss: 734.9277\n",
      "Training Epoch: 12 [7150/13219]\tLoss: 763.2432\n",
      "Training Epoch: 12 [7200/13219]\tLoss: 783.6110\n",
      "Training Epoch: 12 [7250/13219]\tLoss: 729.4236\n",
      "Training Epoch: 12 [7300/13219]\tLoss: 754.4501\n",
      "Training Epoch: 12 [7350/13219]\tLoss: 757.3436\n",
      "Training Epoch: 12 [7400/13219]\tLoss: 749.2048\n",
      "Training Epoch: 12 [7450/13219]\tLoss: 751.3704\n",
      "Training Epoch: 12 [7500/13219]\tLoss: 737.1470\n",
      "Training Epoch: 12 [7550/13219]\tLoss: 732.4777\n",
      "Training Epoch: 12 [7600/13219]\tLoss: 724.8632\n",
      "Training Epoch: 12 [7650/13219]\tLoss: 777.0545\n",
      "Training Epoch: 12 [7700/13219]\tLoss: 772.7951\n",
      "Training Epoch: 12 [7750/13219]\tLoss: 733.7300\n",
      "Training Epoch: 12 [7800/13219]\tLoss: 803.0245\n",
      "Training Epoch: 12 [7850/13219]\tLoss: 746.2642\n",
      "Training Epoch: 12 [7900/13219]\tLoss: 770.5959\n",
      "Training Epoch: 12 [7950/13219]\tLoss: 806.4540\n",
      "Training Epoch: 12 [8000/13219]\tLoss: 785.0279\n",
      "Training Epoch: 12 [8050/13219]\tLoss: 806.7181\n",
      "Training Epoch: 12 [8100/13219]\tLoss: 851.3451\n",
      "Training Epoch: 12 [8150/13219]\tLoss: 884.8688\n",
      "Training Epoch: 12 [8200/13219]\tLoss: 892.1953\n",
      "Training Epoch: 12 [8250/13219]\tLoss: 861.8446\n",
      "Training Epoch: 12 [8300/13219]\tLoss: 871.0968\n",
      "Training Epoch: 12 [8350/13219]\tLoss: 792.1427\n",
      "Training Epoch: 12 [8400/13219]\tLoss: 759.9984\n",
      "Training Epoch: 12 [8450/13219]\tLoss: 785.3018\n",
      "Training Epoch: 12 [8500/13219]\tLoss: 857.4000\n",
      "Training Epoch: 12 [8550/13219]\tLoss: 812.8764\n",
      "Training Epoch: 12 [8600/13219]\tLoss: 774.2960\n",
      "Training Epoch: 12 [8650/13219]\tLoss: 794.8773\n",
      "Training Epoch: 12 [8700/13219]\tLoss: 740.5564\n",
      "Training Epoch: 12 [8750/13219]\tLoss: 791.2701\n",
      "Training Epoch: 12 [8800/13219]\tLoss: 764.7021\n",
      "Training Epoch: 12 [8850/13219]\tLoss: 759.6219\n",
      "Training Epoch: 12 [8900/13219]\tLoss: 729.6909\n",
      "Training Epoch: 12 [8950/13219]\tLoss: 787.5566\n",
      "Training Epoch: 12 [9000/13219]\tLoss: 816.6624\n",
      "Training Epoch: 12 [9050/13219]\tLoss: 816.1381\n",
      "Training Epoch: 12 [9100/13219]\tLoss: 782.3982\n",
      "Training Epoch: 12 [9150/13219]\tLoss: 743.1345\n",
      "Training Epoch: 12 [9200/13219]\tLoss: 779.9561\n",
      "Training Epoch: 12 [9250/13219]\tLoss: 763.2104\n",
      "Training Epoch: 12 [9300/13219]\tLoss: 735.7992\n",
      "Training Epoch: 12 [9350/13219]\tLoss: 744.7417\n",
      "Training Epoch: 12 [9400/13219]\tLoss: 756.7304\n",
      "Training Epoch: 12 [9450/13219]\tLoss: 765.6443\n",
      "Training Epoch: 12 [9500/13219]\tLoss: 712.3902\n",
      "Training Epoch: 12 [9550/13219]\tLoss: 733.4054\n",
      "Training Epoch: 12 [9600/13219]\tLoss: 740.5508\n",
      "Training Epoch: 12 [9650/13219]\tLoss: 731.1396\n",
      "Training Epoch: 12 [9700/13219]\tLoss: 753.1918\n",
      "Training Epoch: 12 [9750/13219]\tLoss: 766.1222\n",
      "Training Epoch: 12 [9800/13219]\tLoss: 748.9787\n",
      "Training Epoch: 12 [9850/13219]\tLoss: 720.2979\n",
      "Training Epoch: 12 [9900/13219]\tLoss: 772.8232\n",
      "Training Epoch: 12 [9950/13219]\tLoss: 722.0865\n",
      "Training Epoch: 12 [10000/13219]\tLoss: 745.6198\n",
      "Training Epoch: 12 [10050/13219]\tLoss: 713.4483\n",
      "Training Epoch: 12 [10100/13219]\tLoss: 753.6689\n",
      "Training Epoch: 12 [10150/13219]\tLoss: 749.2916\n",
      "Training Epoch: 12 [10200/13219]\tLoss: 755.9843\n",
      "Training Epoch: 12 [10250/13219]\tLoss: 763.4874\n",
      "Training Epoch: 12 [10300/13219]\tLoss: 749.7772\n",
      "Training Epoch: 12 [10350/13219]\tLoss: 737.9202\n",
      "Training Epoch: 12 [10400/13219]\tLoss: 740.6148\n",
      "Training Epoch: 12 [10450/13219]\tLoss: 762.1713\n",
      "Training Epoch: 12 [10500/13219]\tLoss: 764.3303\n",
      "Training Epoch: 12 [10550/13219]\tLoss: 761.3350\n",
      "Training Epoch: 12 [10600/13219]\tLoss: 755.1522\n",
      "Training Epoch: 12 [10650/13219]\tLoss: 730.2735\n",
      "Training Epoch: 12 [10700/13219]\tLoss: 745.2534\n",
      "Training Epoch: 12 [10750/13219]\tLoss: 720.6928\n",
      "Training Epoch: 12 [10800/13219]\tLoss: 770.6958\n",
      "Training Epoch: 12 [10850/13219]\tLoss: 715.6619\n",
      "Training Epoch: 12 [10900/13219]\tLoss: 745.5211\n",
      "Training Epoch: 12 [10950/13219]\tLoss: 756.0379\n",
      "Training Epoch: 12 [11000/13219]\tLoss: 759.5552\n",
      "Training Epoch: 12 [11050/13219]\tLoss: 742.2670\n",
      "Training Epoch: 12 [11100/13219]\tLoss: 715.6727\n",
      "Training Epoch: 12 [11150/13219]\tLoss: 775.8644\n",
      "Training Epoch: 12 [11200/13219]\tLoss: 774.2341\n",
      "Training Epoch: 12 [11250/13219]\tLoss: 734.7689\n",
      "Training Epoch: 12 [11300/13219]\tLoss: 756.1616\n",
      "Training Epoch: 12 [11350/13219]\tLoss: 747.0314\n",
      "Training Epoch: 12 [11400/13219]\tLoss: 744.1130\n",
      "Training Epoch: 12 [11450/13219]\tLoss: 707.2883\n",
      "Training Epoch: 12 [11500/13219]\tLoss: 737.5077\n",
      "Training Epoch: 12 [11550/13219]\tLoss: 770.6324\n",
      "Training Epoch: 12 [11600/13219]\tLoss: 770.4101\n",
      "Training Epoch: 12 [11650/13219]\tLoss: 752.0786\n",
      "Training Epoch: 12 [11700/13219]\tLoss: 732.9602\n",
      "Training Epoch: 12 [11750/13219]\tLoss: 761.1357\n",
      "Training Epoch: 12 [11800/13219]\tLoss: 753.9868\n",
      "Training Epoch: 12 [11850/13219]\tLoss: 711.7298\n",
      "Training Epoch: 12 [11900/13219]\tLoss: 743.3098\n",
      "Training Epoch: 12 [11950/13219]\tLoss: 743.7192\n",
      "Training Epoch: 12 [12000/13219]\tLoss: 768.9346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [12050/13219]\tLoss: 752.8764\n",
      "Training Epoch: 12 [12100/13219]\tLoss: 780.7628\n",
      "Training Epoch: 12 [12150/13219]\tLoss: 728.8252\n",
      "Training Epoch: 12 [12200/13219]\tLoss: 749.4528\n",
      "Training Epoch: 12 [12250/13219]\tLoss: 779.3314\n",
      "Training Epoch: 12 [12300/13219]\tLoss: 731.6533\n",
      "Training Epoch: 12 [12350/13219]\tLoss: 767.5294\n",
      "Training Epoch: 12 [12400/13219]\tLoss: 749.3932\n",
      "Training Epoch: 12 [12450/13219]\tLoss: 719.2424\n",
      "Training Epoch: 12 [12500/13219]\tLoss: 746.0179\n",
      "Training Epoch: 12 [12550/13219]\tLoss: 760.7537\n",
      "Training Epoch: 12 [12600/13219]\tLoss: 724.2000\n",
      "Training Epoch: 12 [12650/13219]\tLoss: 776.3216\n",
      "Training Epoch: 12 [12700/13219]\tLoss: 706.2550\n",
      "Training Epoch: 12 [12750/13219]\tLoss: 754.3073\n",
      "Training Epoch: 12 [12800/13219]\tLoss: 740.9314\n",
      "Training Epoch: 12 [12850/13219]\tLoss: 731.7885\n",
      "Training Epoch: 12 [12900/13219]\tLoss: 753.8422\n",
      "Training Epoch: 12 [12950/13219]\tLoss: 714.3953\n",
      "Training Epoch: 12 [13000/13219]\tLoss: 748.5371\n",
      "Training Epoch: 12 [13050/13219]\tLoss: 744.0924\n",
      "Training Epoch: 12 [13100/13219]\tLoss: 720.5784\n",
      "Training Epoch: 12 [13150/13219]\tLoss: 751.3260\n",
      "Training Epoch: 12 [13200/13219]\tLoss: 736.1212\n",
      "Training Epoch: 12 [13219/13219]\tLoss: 693.6935\n",
      "Training Epoch: 12 [1469/1469]\tLoss: 734.8108\n",
      "Training Epoch: 13 [50/13219]\tLoss: 726.1924\n",
      "Training Epoch: 13 [100/13219]\tLoss: 747.9725\n",
      "Training Epoch: 13 [150/13219]\tLoss: 734.9149\n",
      "Training Epoch: 13 [200/13219]\tLoss: 745.2671\n",
      "Training Epoch: 13 [250/13219]\tLoss: 762.9813\n",
      "Training Epoch: 13 [300/13219]\tLoss: 762.3257\n",
      "Training Epoch: 13 [350/13219]\tLoss: 793.8267\n",
      "Training Epoch: 13 [400/13219]\tLoss: 701.8982\n",
      "Training Epoch: 13 [450/13219]\tLoss: 711.9504\n",
      "Training Epoch: 13 [500/13219]\tLoss: 726.9049\n",
      "Training Epoch: 13 [550/13219]\tLoss: 731.2468\n",
      "Training Epoch: 13 [600/13219]\tLoss: 754.6967\n",
      "Training Epoch: 13 [650/13219]\tLoss: 707.2124\n",
      "Training Epoch: 13 [700/13219]\tLoss: 720.0297\n",
      "Training Epoch: 13 [750/13219]\tLoss: 749.3671\n",
      "Training Epoch: 13 [800/13219]\tLoss: 806.7314\n",
      "Training Epoch: 13 [850/13219]\tLoss: 805.6051\n",
      "Training Epoch: 13 [900/13219]\tLoss: 761.1229\n",
      "Training Epoch: 13 [950/13219]\tLoss: 747.8007\n",
      "Training Epoch: 13 [1000/13219]\tLoss: 728.4429\n",
      "Training Epoch: 13 [1050/13219]\tLoss: 721.8105\n",
      "Training Epoch: 13 [1100/13219]\tLoss: 749.6627\n",
      "Training Epoch: 13 [1150/13219]\tLoss: 751.5950\n",
      "Training Epoch: 13 [1200/13219]\tLoss: 790.4998\n",
      "Training Epoch: 13 [1250/13219]\tLoss: 772.5850\n",
      "Training Epoch: 13 [1300/13219]\tLoss: 713.9145\n",
      "Training Epoch: 13 [1350/13219]\tLoss: 732.3202\n",
      "Training Epoch: 13 [1400/13219]\tLoss: 734.5768\n",
      "Training Epoch: 13 [1450/13219]\tLoss: 733.1978\n",
      "Training Epoch: 13 [1500/13219]\tLoss: 725.4033\n",
      "Training Epoch: 13 [1550/13219]\tLoss: 767.0915\n",
      "Training Epoch: 13 [1600/13219]\tLoss: 774.3962\n",
      "Training Epoch: 13 [1650/13219]\tLoss: 776.1283\n",
      "Training Epoch: 13 [1700/13219]\tLoss: 706.8974\n",
      "Training Epoch: 13 [1750/13219]\tLoss: 771.5682\n",
      "Training Epoch: 13 [1800/13219]\tLoss: 777.1063\n",
      "Training Epoch: 13 [1850/13219]\tLoss: 710.6846\n",
      "Training Epoch: 13 [1900/13219]\tLoss: 715.4966\n",
      "Training Epoch: 13 [1950/13219]\tLoss: 737.3363\n",
      "Training Epoch: 13 [2000/13219]\tLoss: 765.4307\n",
      "Training Epoch: 13 [2050/13219]\tLoss: 743.1787\n",
      "Training Epoch: 13 [2100/13219]\tLoss: 736.2488\n",
      "Training Epoch: 13 [2150/13219]\tLoss: 779.9390\n",
      "Training Epoch: 13 [2200/13219]\tLoss: 725.4644\n",
      "Training Epoch: 13 [2250/13219]\tLoss: 758.1472\n",
      "Training Epoch: 13 [2300/13219]\tLoss: 746.6981\n",
      "Training Epoch: 13 [2350/13219]\tLoss: 754.2063\n",
      "Training Epoch: 13 [2400/13219]\tLoss: 750.4236\n",
      "Training Epoch: 13 [2450/13219]\tLoss: 757.3769\n",
      "Training Epoch: 13 [2500/13219]\tLoss: 738.1848\n",
      "Training Epoch: 13 [2550/13219]\tLoss: 763.2501\n",
      "Training Epoch: 13 [2600/13219]\tLoss: 764.5464\n",
      "Training Epoch: 13 [2650/13219]\tLoss: 721.4667\n",
      "Training Epoch: 13 [2700/13219]\tLoss: 732.0824\n",
      "Training Epoch: 13 [2750/13219]\tLoss: 744.8548\n",
      "Training Epoch: 13 [2800/13219]\tLoss: 770.4901\n",
      "Training Epoch: 13 [2850/13219]\tLoss: 749.2208\n",
      "Training Epoch: 13 [2900/13219]\tLoss: 740.6604\n",
      "Training Epoch: 13 [2950/13219]\tLoss: 748.6407\n",
      "Training Epoch: 13 [3000/13219]\tLoss: 755.8840\n",
      "Training Epoch: 13 [3050/13219]\tLoss: 733.4094\n",
      "Training Epoch: 13 [3100/13219]\tLoss: 706.2223\n",
      "Training Epoch: 13 [3150/13219]\tLoss: 723.2530\n",
      "Training Epoch: 13 [3200/13219]\tLoss: 736.0257\n",
      "Training Epoch: 13 [3250/13219]\tLoss: 755.6074\n",
      "Training Epoch: 13 [3300/13219]\tLoss: 724.9164\n",
      "Training Epoch: 13 [3350/13219]\tLoss: 742.3137\n",
      "Training Epoch: 13 [3400/13219]\tLoss: 719.3094\n",
      "Training Epoch: 13 [3450/13219]\tLoss: 734.5431\n",
      "Training Epoch: 13 [3500/13219]\tLoss: 773.7077\n",
      "Training Epoch: 13 [3550/13219]\tLoss: 769.3251\n",
      "Training Epoch: 13 [3600/13219]\tLoss: 779.2063\n",
      "Training Epoch: 13 [3650/13219]\tLoss: 727.3546\n",
      "Training Epoch: 13 [3700/13219]\tLoss: 719.9311\n",
      "Training Epoch: 13 [3750/13219]\tLoss: 735.5480\n",
      "Training Epoch: 13 [3800/13219]\tLoss: 727.8133\n",
      "Training Epoch: 13 [3850/13219]\tLoss: 702.0168\n",
      "Training Epoch: 13 [3900/13219]\tLoss: 741.9400\n",
      "Training Epoch: 13 [3950/13219]\tLoss: 734.9415\n",
      "Training Epoch: 13 [4000/13219]\tLoss: 723.2939\n",
      "Training Epoch: 13 [4050/13219]\tLoss: 735.7912\n",
      "Training Epoch: 13 [4100/13219]\tLoss: 689.9373\n",
      "Training Epoch: 13 [4150/13219]\tLoss: 738.1979\n",
      "Training Epoch: 13 [4200/13219]\tLoss: 754.8289\n",
      "Training Epoch: 13 [4250/13219]\tLoss: 766.4810\n",
      "Training Epoch: 13 [4300/13219]\tLoss: 753.9388\n",
      "Training Epoch: 13 [4350/13219]\tLoss: 768.9486\n",
      "Training Epoch: 13 [4400/13219]\tLoss: 736.8773\n",
      "Training Epoch: 13 [4450/13219]\tLoss: 739.2153\n",
      "Training Epoch: 13 [4500/13219]\tLoss: 773.5967\n",
      "Training Epoch: 13 [4550/13219]\tLoss: 748.8259\n",
      "Training Epoch: 13 [4600/13219]\tLoss: 741.0197\n",
      "Training Epoch: 13 [4650/13219]\tLoss: 721.4819\n",
      "Training Epoch: 13 [4700/13219]\tLoss: 749.3440\n",
      "Training Epoch: 13 [4750/13219]\tLoss: 721.6387\n",
      "Training Epoch: 13 [4800/13219]\tLoss: 681.2744\n",
      "Training Epoch: 13 [4850/13219]\tLoss: 748.5681\n",
      "Training Epoch: 13 [4900/13219]\tLoss: 770.9858\n",
      "Training Epoch: 13 [4950/13219]\tLoss: 766.6740\n",
      "Training Epoch: 13 [5000/13219]\tLoss: 721.8591\n",
      "Training Epoch: 13 [5050/13219]\tLoss: 717.5909\n",
      "Training Epoch: 13 [5100/13219]\tLoss: 754.3726\n",
      "Training Epoch: 13 [5150/13219]\tLoss: 753.7211\n",
      "Training Epoch: 13 [5200/13219]\tLoss: 759.3211\n",
      "Training Epoch: 13 [5250/13219]\tLoss: 718.9562\n",
      "Training Epoch: 13 [5300/13219]\tLoss: 728.4069\n",
      "Training Epoch: 13 [5350/13219]\tLoss: 739.1883\n",
      "Training Epoch: 13 [5400/13219]\tLoss: 709.3185\n",
      "Training Epoch: 13 [5450/13219]\tLoss: 723.6289\n",
      "Training Epoch: 13 [5500/13219]\tLoss: 697.6172\n",
      "Training Epoch: 13 [5550/13219]\tLoss: 738.3032\n",
      "Training Epoch: 13 [5600/13219]\tLoss: 742.0253\n",
      "Training Epoch: 13 [5650/13219]\tLoss: 759.7300\n",
      "Training Epoch: 13 [5700/13219]\tLoss: 736.6067\n",
      "Training Epoch: 13 [5750/13219]\tLoss: 741.8291\n",
      "Training Epoch: 13 [5800/13219]\tLoss: 730.1786\n",
      "Training Epoch: 13 [5850/13219]\tLoss: 780.4200\n",
      "Training Epoch: 13 [5900/13219]\tLoss: 728.4650\n",
      "Training Epoch: 13 [5950/13219]\tLoss: 741.5405\n",
      "Training Epoch: 13 [6000/13219]\tLoss: 734.6868\n",
      "Training Epoch: 13 [6050/13219]\tLoss: 725.8346\n",
      "Training Epoch: 13 [6100/13219]\tLoss: 737.3799\n",
      "Training Epoch: 13 [6150/13219]\tLoss: 732.2361\n",
      "Training Epoch: 13 [6200/13219]\tLoss: 747.1843\n",
      "Training Epoch: 13 [6250/13219]\tLoss: 726.3285\n",
      "Training Epoch: 13 [6300/13219]\tLoss: 697.9437\n",
      "Training Epoch: 13 [6350/13219]\tLoss: 736.7372\n",
      "Training Epoch: 13 [6400/13219]\tLoss: 746.6042\n",
      "Training Epoch: 13 [6450/13219]\tLoss: 711.5175\n",
      "Training Epoch: 13 [6500/13219]\tLoss: 720.4562\n",
      "Training Epoch: 13 [6550/13219]\tLoss: 742.2581\n",
      "Training Epoch: 13 [6600/13219]\tLoss: 713.8267\n",
      "Training Epoch: 13 [6650/13219]\tLoss: 718.2516\n",
      "Training Epoch: 13 [6700/13219]\tLoss: 713.0184\n",
      "Training Epoch: 13 [6750/13219]\tLoss: 715.7711\n",
      "Training Epoch: 13 [6800/13219]\tLoss: 772.6253\n",
      "Training Epoch: 13 [6850/13219]\tLoss: 757.2731\n",
      "Training Epoch: 13 [6900/13219]\tLoss: 755.2242\n",
      "Training Epoch: 13 [6950/13219]\tLoss: 764.1396\n",
      "Training Epoch: 13 [7000/13219]\tLoss: 760.0798\n",
      "Training Epoch: 13 [7050/13219]\tLoss: 755.9382\n",
      "Training Epoch: 13 [7100/13219]\tLoss: 723.5038\n",
      "Training Epoch: 13 [7150/13219]\tLoss: 704.5338\n",
      "Training Epoch: 13 [7200/13219]\tLoss: 692.1734\n",
      "Training Epoch: 13 [7250/13219]\tLoss: 725.5651\n",
      "Training Epoch: 13 [7300/13219]\tLoss: 734.0182\n",
      "Training Epoch: 13 [7350/13219]\tLoss: 719.2628\n",
      "Training Epoch: 13 [7400/13219]\tLoss: 741.2051\n",
      "Training Epoch: 13 [7450/13219]\tLoss: 765.1259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [7500/13219]\tLoss: 743.3723\n",
      "Training Epoch: 13 [7550/13219]\tLoss: 743.9274\n",
      "Training Epoch: 13 [7600/13219]\tLoss: 719.2229\n",
      "Training Epoch: 13 [7650/13219]\tLoss: 725.1526\n",
      "Training Epoch: 13 [7700/13219]\tLoss: 733.1899\n",
      "Training Epoch: 13 [7750/13219]\tLoss: 738.0157\n",
      "Training Epoch: 13 [7800/13219]\tLoss: 751.0411\n",
      "Training Epoch: 13 [7850/13219]\tLoss: 730.1157\n",
      "Training Epoch: 13 [7900/13219]\tLoss: 722.5770\n",
      "Training Epoch: 13 [7950/13219]\tLoss: 751.5290\n",
      "Training Epoch: 13 [8000/13219]\tLoss: 716.4905\n",
      "Training Epoch: 13 [8050/13219]\tLoss: 705.3868\n",
      "Training Epoch: 13 [8100/13219]\tLoss: 744.7054\n",
      "Training Epoch: 13 [8150/13219]\tLoss: 674.5798\n",
      "Training Epoch: 13 [8200/13219]\tLoss: 760.0406\n",
      "Training Epoch: 13 [8250/13219]\tLoss: 729.8603\n",
      "Training Epoch: 13 [8300/13219]\tLoss: 727.8527\n",
      "Training Epoch: 13 [8350/13219]\tLoss: 728.0201\n",
      "Training Epoch: 13 [8400/13219]\tLoss: 731.6627\n",
      "Training Epoch: 13 [8450/13219]\tLoss: 757.0652\n",
      "Training Epoch: 13 [8500/13219]\tLoss: 766.5547\n",
      "Training Epoch: 13 [8550/13219]\tLoss: 720.4986\n",
      "Training Epoch: 13 [8600/13219]\tLoss: 746.5161\n",
      "Training Epoch: 13 [8650/13219]\tLoss: 730.0536\n",
      "Training Epoch: 13 [8700/13219]\tLoss: 726.5804\n",
      "Training Epoch: 13 [8750/13219]\tLoss: 739.9197\n",
      "Training Epoch: 13 [8800/13219]\tLoss: 710.6716\n",
      "Training Epoch: 13 [8850/13219]\tLoss: 747.6718\n",
      "Training Epoch: 13 [8900/13219]\tLoss: 760.4169\n",
      "Training Epoch: 13 [8950/13219]\tLoss: 753.1917\n",
      "Training Epoch: 13 [9000/13219]\tLoss: 703.4564\n",
      "Training Epoch: 13 [9050/13219]\tLoss: 737.8583\n",
      "Training Epoch: 13 [9100/13219]\tLoss: 734.6909\n",
      "Training Epoch: 13 [9150/13219]\tLoss: 719.6605\n",
      "Training Epoch: 13 [9200/13219]\tLoss: 727.0141\n",
      "Training Epoch: 13 [9250/13219]\tLoss: 761.2811\n",
      "Training Epoch: 13 [9300/13219]\tLoss: 752.2460\n",
      "Training Epoch: 13 [9350/13219]\tLoss: 779.1201\n",
      "Training Epoch: 13 [9400/13219]\tLoss: 743.6219\n",
      "Training Epoch: 13 [9450/13219]\tLoss: 755.4713\n",
      "Training Epoch: 13 [9500/13219]\tLoss: 747.8619\n",
      "Training Epoch: 13 [9550/13219]\tLoss: 781.5004\n",
      "Training Epoch: 13 [9600/13219]\tLoss: 739.4104\n",
      "Training Epoch: 13 [9650/13219]\tLoss: 752.5695\n",
      "Training Epoch: 13 [9700/13219]\tLoss: 785.3082\n",
      "Training Epoch: 13 [9750/13219]\tLoss: 742.8967\n",
      "Training Epoch: 13 [9800/13219]\tLoss: 716.2587\n",
      "Training Epoch: 13 [9850/13219]\tLoss: 708.4358\n",
      "Training Epoch: 13 [9900/13219]\tLoss: 761.4009\n",
      "Training Epoch: 13 [9950/13219]\tLoss: 697.1198\n",
      "Training Epoch: 13 [10000/13219]\tLoss: 727.9186\n",
      "Training Epoch: 13 [10050/13219]\tLoss: 725.7841\n",
      "Training Epoch: 13 [10100/13219]\tLoss: 760.2989\n",
      "Training Epoch: 13 [10150/13219]\tLoss: 769.0853\n",
      "Training Epoch: 13 [10200/13219]\tLoss: 745.0220\n",
      "Training Epoch: 13 [10250/13219]\tLoss: 735.1074\n",
      "Training Epoch: 13 [10300/13219]\tLoss: 724.3394\n",
      "Training Epoch: 13 [10350/13219]\tLoss: 762.9408\n",
      "Training Epoch: 13 [10400/13219]\tLoss: 700.8760\n",
      "Training Epoch: 13 [10450/13219]\tLoss: 770.1385\n",
      "Training Epoch: 13 [10500/13219]\tLoss: 694.0758\n",
      "Training Epoch: 13 [10550/13219]\tLoss: 750.3380\n",
      "Training Epoch: 13 [10600/13219]\tLoss: 738.2955\n",
      "Training Epoch: 13 [10650/13219]\tLoss: 717.7318\n",
      "Training Epoch: 13 [10700/13219]\tLoss: 746.1064\n",
      "Training Epoch: 13 [10750/13219]\tLoss: 738.0977\n",
      "Training Epoch: 13 [10800/13219]\tLoss: 752.8077\n",
      "Training Epoch: 13 [10850/13219]\tLoss: 709.7668\n",
      "Training Epoch: 13 [10900/13219]\tLoss: 741.6514\n",
      "Training Epoch: 13 [10950/13219]\tLoss: 732.6473\n",
      "Training Epoch: 13 [11000/13219]\tLoss: 737.1538\n",
      "Training Epoch: 13 [11050/13219]\tLoss: 749.6891\n",
      "Training Epoch: 13 [11100/13219]\tLoss: 768.2953\n",
      "Training Epoch: 13 [11150/13219]\tLoss: 710.6167\n",
      "Training Epoch: 13 [11200/13219]\tLoss: 730.1917\n",
      "Training Epoch: 13 [11250/13219]\tLoss: 717.5671\n",
      "Training Epoch: 13 [11300/13219]\tLoss: 740.7849\n",
      "Training Epoch: 13 [11350/13219]\tLoss: 752.7739\n",
      "Training Epoch: 13 [11400/13219]\tLoss: 738.2749\n",
      "Training Epoch: 13 [11450/13219]\tLoss: 744.8748\n",
      "Training Epoch: 13 [11500/13219]\tLoss: 716.3389\n",
      "Training Epoch: 13 [11550/13219]\tLoss: 762.2493\n",
      "Training Epoch: 13 [11600/13219]\tLoss: 707.2715\n",
      "Training Epoch: 13 [11650/13219]\tLoss: 737.9329\n",
      "Training Epoch: 13 [11700/13219]\tLoss: 723.1561\n",
      "Training Epoch: 13 [11750/13219]\tLoss: 735.1598\n",
      "Training Epoch: 13 [11800/13219]\tLoss: 706.0497\n",
      "Training Epoch: 13 [11850/13219]\tLoss: 715.9091\n",
      "Training Epoch: 13 [11900/13219]\tLoss: 774.6896\n",
      "Training Epoch: 13 [11950/13219]\tLoss: 737.8896\n",
      "Training Epoch: 13 [12000/13219]\tLoss: 722.0638\n",
      "Training Epoch: 13 [12050/13219]\tLoss: 716.6779\n",
      "Training Epoch: 13 [12100/13219]\tLoss: 743.7361\n",
      "Training Epoch: 13 [12150/13219]\tLoss: 721.0363\n",
      "Training Epoch: 13 [12200/13219]\tLoss: 740.9519\n",
      "Training Epoch: 13 [12250/13219]\tLoss: 726.6261\n",
      "Training Epoch: 13 [12300/13219]\tLoss: 707.2159\n",
      "Training Epoch: 13 [12350/13219]\tLoss: 727.7032\n",
      "Training Epoch: 13 [12400/13219]\tLoss: 701.7438\n",
      "Training Epoch: 13 [12450/13219]\tLoss: 709.9670\n",
      "Training Epoch: 13 [12500/13219]\tLoss: 741.9426\n",
      "Training Epoch: 13 [12550/13219]\tLoss: 695.7220\n",
      "Training Epoch: 13 [12600/13219]\tLoss: 711.3898\n",
      "Training Epoch: 13 [12650/13219]\tLoss: 746.3308\n",
      "Training Epoch: 13 [12700/13219]\tLoss: 756.5725\n",
      "Training Epoch: 13 [12750/13219]\tLoss: 729.6277\n",
      "Training Epoch: 13 [12800/13219]\tLoss: 763.7185\n",
      "Training Epoch: 13 [12850/13219]\tLoss: 723.9551\n",
      "Training Epoch: 13 [12900/13219]\tLoss: 757.6210\n",
      "Training Epoch: 13 [12950/13219]\tLoss: 742.9296\n",
      "Training Epoch: 13 [13000/13219]\tLoss: 739.0633\n",
      "Training Epoch: 13 [13050/13219]\tLoss: 742.4608\n",
      "Training Epoch: 13 [13100/13219]\tLoss: 756.8952\n",
      "Training Epoch: 13 [13150/13219]\tLoss: 737.7974\n",
      "Training Epoch: 13 [13200/13219]\tLoss: 740.4546\n",
      "Training Epoch: 13 [13219/13219]\tLoss: 734.0359\n",
      "Training Epoch: 13 [1469/1469]\tLoss: 726.4084\n",
      "Training Epoch: 14 [50/13219]\tLoss: 738.1975\n",
      "Training Epoch: 14 [100/13219]\tLoss: 716.6634\n",
      "Training Epoch: 14 [150/13219]\tLoss: 758.1625\n",
      "Training Epoch: 14 [200/13219]\tLoss: 756.6409\n",
      "Training Epoch: 14 [250/13219]\tLoss: 749.0377\n",
      "Training Epoch: 14 [300/13219]\tLoss: 740.9402\n",
      "Training Epoch: 14 [350/13219]\tLoss: 782.0605\n",
      "Training Epoch: 14 [400/13219]\tLoss: 750.8865\n",
      "Training Epoch: 14 [450/13219]\tLoss: 766.7362\n",
      "Training Epoch: 14 [500/13219]\tLoss: 785.3525\n",
      "Training Epoch: 14 [550/13219]\tLoss: 742.7195\n",
      "Training Epoch: 14 [600/13219]\tLoss: 700.9092\n",
      "Training Epoch: 14 [650/13219]\tLoss: 705.8298\n",
      "Training Epoch: 14 [700/13219]\tLoss: 712.0417\n",
      "Training Epoch: 14 [750/13219]\tLoss: 714.4105\n",
      "Training Epoch: 14 [800/13219]\tLoss: 701.0006\n",
      "Training Epoch: 14 [850/13219]\tLoss: 733.1136\n",
      "Training Epoch: 14 [900/13219]\tLoss: 744.1445\n",
      "Training Epoch: 14 [950/13219]\tLoss: 763.7308\n",
      "Training Epoch: 14 [1000/13219]\tLoss: 745.4135\n",
      "Training Epoch: 14 [1050/13219]\tLoss: 739.6107\n",
      "Training Epoch: 14 [1100/13219]\tLoss: 733.9676\n",
      "Training Epoch: 14 [1150/13219]\tLoss: 745.4337\n",
      "Training Epoch: 14 [1200/13219]\tLoss: 728.0619\n",
      "Training Epoch: 14 [1250/13219]\tLoss: 764.5870\n",
      "Training Epoch: 14 [1300/13219]\tLoss: 705.9100\n",
      "Training Epoch: 14 [1350/13219]\tLoss: 742.0020\n",
      "Training Epoch: 14 [1400/13219]\tLoss: 734.5273\n",
      "Training Epoch: 14 [1450/13219]\tLoss: 683.3619\n",
      "Training Epoch: 14 [1500/13219]\tLoss: 721.2845\n",
      "Training Epoch: 14 [1550/13219]\tLoss: 690.8894\n",
      "Training Epoch: 14 [1600/13219]\tLoss: 713.0208\n",
      "Training Epoch: 14 [1650/13219]\tLoss: 700.6573\n",
      "Training Epoch: 14 [1700/13219]\tLoss: 723.4213\n",
      "Training Epoch: 14 [1750/13219]\tLoss: 703.0958\n",
      "Training Epoch: 14 [1800/13219]\tLoss: 743.5439\n",
      "Training Epoch: 14 [1850/13219]\tLoss: 722.0253\n",
      "Training Epoch: 14 [1900/13219]\tLoss: 729.4253\n",
      "Training Epoch: 14 [1950/13219]\tLoss: 717.6288\n",
      "Training Epoch: 14 [2000/13219]\tLoss: 705.3178\n",
      "Training Epoch: 14 [2050/13219]\tLoss: 714.7164\n",
      "Training Epoch: 14 [2100/13219]\tLoss: 737.0717\n",
      "Training Epoch: 14 [2150/13219]\tLoss: 728.4796\n",
      "Training Epoch: 14 [2200/13219]\tLoss: 748.5941\n",
      "Training Epoch: 14 [2250/13219]\tLoss: 738.4061\n",
      "Training Epoch: 14 [2300/13219]\tLoss: 710.7563\n",
      "Training Epoch: 14 [2350/13219]\tLoss: 743.1990\n",
      "Training Epoch: 14 [2400/13219]\tLoss: 733.2676\n",
      "Training Epoch: 14 [2450/13219]\tLoss: 743.8477\n",
      "Training Epoch: 14 [2500/13219]\tLoss: 739.0828\n",
      "Training Epoch: 14 [2550/13219]\tLoss: 721.9603\n",
      "Training Epoch: 14 [2600/13219]\tLoss: 695.4587\n",
      "Training Epoch: 14 [2650/13219]\tLoss: 714.4584\n",
      "Training Epoch: 14 [2700/13219]\tLoss: 754.7592\n",
      "Training Epoch: 14 [2750/13219]\tLoss: 745.2587\n",
      "Training Epoch: 14 [2800/13219]\tLoss: 752.1818\n",
      "Training Epoch: 14 [2850/13219]\tLoss: 722.3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [2900/13219]\tLoss: 728.0849\n",
      "Training Epoch: 14 [2950/13219]\tLoss: 730.4617\n",
      "Training Epoch: 14 [3000/13219]\tLoss: 710.4531\n",
      "Training Epoch: 14 [3050/13219]\tLoss: 752.3937\n",
      "Training Epoch: 14 [3100/13219]\tLoss: 732.2307\n",
      "Training Epoch: 14 [3150/13219]\tLoss: 724.7183\n",
      "Training Epoch: 14 [3200/13219]\tLoss: 731.2244\n",
      "Training Epoch: 14 [3250/13219]\tLoss: 743.8564\n",
      "Training Epoch: 14 [3300/13219]\tLoss: 730.2906\n",
      "Training Epoch: 14 [3350/13219]\tLoss: 752.2195\n",
      "Training Epoch: 14 [3400/13219]\tLoss: 715.5623\n",
      "Training Epoch: 14 [3450/13219]\tLoss: 712.2864\n",
      "Training Epoch: 14 [3500/13219]\tLoss: 743.0325\n",
      "Training Epoch: 14 [3550/13219]\tLoss: 719.5508\n",
      "Training Epoch: 14 [3600/13219]\tLoss: 711.3622\n",
      "Training Epoch: 14 [3650/13219]\tLoss: 715.3071\n",
      "Training Epoch: 14 [3700/13219]\tLoss: 695.4775\n",
      "Training Epoch: 14 [3750/13219]\tLoss: 731.0399\n",
      "Training Epoch: 14 [3800/13219]\tLoss: 746.0855\n",
      "Training Epoch: 14 [3850/13219]\tLoss: 738.8311\n",
      "Training Epoch: 14 [3900/13219]\tLoss: 744.5433\n",
      "Training Epoch: 14 [3950/13219]\tLoss: 730.3900\n",
      "Training Epoch: 14 [4000/13219]\tLoss: 737.0947\n",
      "Training Epoch: 14 [4050/13219]\tLoss: 751.5488\n",
      "Training Epoch: 14 [4100/13219]\tLoss: 758.8517\n",
      "Training Epoch: 14 [4150/13219]\tLoss: 719.7166\n",
      "Training Epoch: 14 [4200/13219]\tLoss: 737.7386\n",
      "Training Epoch: 14 [4250/13219]\tLoss: 723.3395\n",
      "Training Epoch: 14 [4300/13219]\tLoss: 700.1999\n",
      "Training Epoch: 14 [4350/13219]\tLoss: 726.4100\n",
      "Training Epoch: 14 [4400/13219]\tLoss: 729.6775\n",
      "Training Epoch: 14 [4450/13219]\tLoss: 719.3011\n",
      "Training Epoch: 14 [4500/13219]\tLoss: 722.2410\n",
      "Training Epoch: 14 [4550/13219]\tLoss: 751.3654\n",
      "Training Epoch: 14 [4600/13219]\tLoss: 742.0200\n",
      "Training Epoch: 14 [4650/13219]\tLoss: 740.1620\n",
      "Training Epoch: 14 [4700/13219]\tLoss: 704.3773\n",
      "Training Epoch: 14 [4750/13219]\tLoss: 765.8828\n",
      "Training Epoch: 14 [4800/13219]\tLoss: 721.2136\n",
      "Training Epoch: 14 [4850/13219]\tLoss: 744.9083\n",
      "Training Epoch: 14 [4900/13219]\tLoss: 708.2926\n",
      "Training Epoch: 14 [4950/13219]\tLoss: 734.8740\n",
      "Training Epoch: 14 [5000/13219]\tLoss: 718.1483\n",
      "Training Epoch: 14 [5050/13219]\tLoss: 737.8456\n",
      "Training Epoch: 14 [5100/13219]\tLoss: 755.6729\n",
      "Training Epoch: 14 [5150/13219]\tLoss: 778.1368\n",
      "Training Epoch: 14 [5200/13219]\tLoss: 712.4354\n",
      "Training Epoch: 14 [5250/13219]\tLoss: 683.9754\n",
      "Training Epoch: 14 [5300/13219]\tLoss: 703.5840\n",
      "Training Epoch: 14 [5350/13219]\tLoss: 725.8869\n",
      "Training Epoch: 14 [5400/13219]\tLoss: 741.5356\n",
      "Training Epoch: 14 [5450/13219]\tLoss: 739.7430\n",
      "Training Epoch: 14 [5500/13219]\tLoss: 710.4784\n",
      "Training Epoch: 14 [5550/13219]\tLoss: 744.9004\n",
      "Training Epoch: 14 [5600/13219]\tLoss: 779.1491\n",
      "Training Epoch: 14 [5650/13219]\tLoss: 737.8915\n",
      "Training Epoch: 14 [5700/13219]\tLoss: 710.6559\n",
      "Training Epoch: 14 [5750/13219]\tLoss: 728.1544\n",
      "Training Epoch: 14 [5800/13219]\tLoss: 737.3850\n",
      "Training Epoch: 14 [5850/13219]\tLoss: 708.7563\n",
      "Training Epoch: 14 [5900/13219]\tLoss: 741.9156\n",
      "Training Epoch: 14 [5950/13219]\tLoss: 774.1653\n",
      "Training Epoch: 14 [6000/13219]\tLoss: 717.1201\n",
      "Training Epoch: 14 [6050/13219]\tLoss: 742.6693\n",
      "Training Epoch: 14 [6100/13219]\tLoss: 770.2074\n",
      "Training Epoch: 14 [6150/13219]\tLoss: 706.7958\n",
      "Training Epoch: 14 [6200/13219]\tLoss: 692.1920\n",
      "Training Epoch: 14 [6250/13219]\tLoss: 736.4950\n",
      "Training Epoch: 14 [6300/13219]\tLoss: 703.1150\n",
      "Training Epoch: 14 [6350/13219]\tLoss: 738.0651\n",
      "Training Epoch: 14 [6400/13219]\tLoss: 730.6840\n",
      "Training Epoch: 14 [6450/13219]\tLoss: 724.1160\n",
      "Training Epoch: 14 [6500/13219]\tLoss: 738.0151\n",
      "Training Epoch: 14 [6550/13219]\tLoss: 737.2087\n",
      "Training Epoch: 14 [6600/13219]\tLoss: 715.4017\n",
      "Training Epoch: 14 [6650/13219]\tLoss: 740.8349\n",
      "Training Epoch: 14 [6700/13219]\tLoss: 738.1064\n",
      "Training Epoch: 14 [6750/13219]\tLoss: 737.0110\n",
      "Training Epoch: 14 [6800/13219]\tLoss: 726.0326\n",
      "Training Epoch: 14 [6850/13219]\tLoss: 732.0306\n",
      "Training Epoch: 14 [6900/13219]\tLoss: 740.6530\n",
      "Training Epoch: 14 [6950/13219]\tLoss: 756.2245\n",
      "Training Epoch: 14 [7000/13219]\tLoss: 724.5487\n",
      "Training Epoch: 14 [7050/13219]\tLoss: 757.3026\n",
      "Training Epoch: 14 [7100/13219]\tLoss: 742.8352\n",
      "Training Epoch: 14 [7150/13219]\tLoss: 710.1328\n",
      "Training Epoch: 14 [7200/13219]\tLoss: 736.2374\n",
      "Training Epoch: 14 [7250/13219]\tLoss: 758.2442\n",
      "Training Epoch: 14 [7300/13219]\tLoss: 714.5931\n",
      "Training Epoch: 14 [7350/13219]\tLoss: 681.1838\n",
      "Training Epoch: 14 [7400/13219]\tLoss: 737.7603\n",
      "Training Epoch: 14 [7450/13219]\tLoss: 737.7108\n",
      "Training Epoch: 14 [7500/13219]\tLoss: 736.9714\n",
      "Training Epoch: 14 [7550/13219]\tLoss: 692.2720\n",
      "Training Epoch: 14 [7600/13219]\tLoss: 709.2815\n",
      "Training Epoch: 14 [7650/13219]\tLoss: 695.7017\n",
      "Training Epoch: 14 [7700/13219]\tLoss: 701.3428\n",
      "Training Epoch: 14 [7750/13219]\tLoss: 712.0964\n",
      "Training Epoch: 14 [7800/13219]\tLoss: 699.3595\n",
      "Training Epoch: 14 [7850/13219]\tLoss: 743.6825\n",
      "Training Epoch: 14 [7900/13219]\tLoss: 739.1454\n",
      "Training Epoch: 14 [7950/13219]\tLoss: 699.1190\n",
      "Training Epoch: 14 [8000/13219]\tLoss: 708.8810\n",
      "Training Epoch: 14 [8050/13219]\tLoss: 718.9181\n",
      "Training Epoch: 14 [8100/13219]\tLoss: 737.7560\n",
      "Training Epoch: 14 [8150/13219]\tLoss: 785.4938\n",
      "Training Epoch: 14 [8200/13219]\tLoss: 684.5954\n",
      "Training Epoch: 14 [8250/13219]\tLoss: 726.1155\n",
      "Training Epoch: 14 [8300/13219]\tLoss: 733.9330\n",
      "Training Epoch: 14 [8350/13219]\tLoss: 754.1321\n",
      "Training Epoch: 14 [8400/13219]\tLoss: 704.5427\n",
      "Training Epoch: 14 [8450/13219]\tLoss: 751.7921\n",
      "Training Epoch: 14 [8500/13219]\tLoss: 745.8054\n",
      "Training Epoch: 14 [8550/13219]\tLoss: 703.2335\n",
      "Training Epoch: 14 [8600/13219]\tLoss: 752.6053\n",
      "Training Epoch: 14 [8650/13219]\tLoss: 702.7236\n",
      "Training Epoch: 14 [8700/13219]\tLoss: 738.6569\n",
      "Training Epoch: 14 [8750/13219]\tLoss: 723.9634\n",
      "Training Epoch: 14 [8800/13219]\tLoss: 736.3084\n",
      "Training Epoch: 14 [8850/13219]\tLoss: 709.2123\n",
      "Training Epoch: 14 [8900/13219]\tLoss: 697.6960\n",
      "Training Epoch: 14 [8950/13219]\tLoss: 712.0388\n",
      "Training Epoch: 14 [9000/13219]\tLoss: 738.1649\n",
      "Training Epoch: 14 [9050/13219]\tLoss: 706.3137\n",
      "Training Epoch: 14 [9100/13219]\tLoss: 719.8051\n",
      "Training Epoch: 14 [9150/13219]\tLoss: 730.6918\n",
      "Training Epoch: 14 [9200/13219]\tLoss: 733.3553\n",
      "Training Epoch: 14 [9250/13219]\tLoss: 738.4913\n",
      "Training Epoch: 14 [9300/13219]\tLoss: 698.2598\n",
      "Training Epoch: 14 [9350/13219]\tLoss: 711.9353\n",
      "Training Epoch: 14 [9400/13219]\tLoss: 698.1591\n",
      "Training Epoch: 14 [9450/13219]\tLoss: 728.3259\n",
      "Training Epoch: 14 [9500/13219]\tLoss: 706.6615\n",
      "Training Epoch: 14 [9550/13219]\tLoss: 697.7849\n",
      "Training Epoch: 14 [9600/13219]\tLoss: 730.1504\n",
      "Training Epoch: 14 [9650/13219]\tLoss: 733.3957\n",
      "Training Epoch: 14 [9700/13219]\tLoss: 697.7165\n",
      "Training Epoch: 14 [9750/13219]\tLoss: 734.3553\n",
      "Training Epoch: 14 [9800/13219]\tLoss: 729.0789\n",
      "Training Epoch: 14 [9850/13219]\tLoss: 738.2081\n",
      "Training Epoch: 14 [9900/13219]\tLoss: 728.2003\n",
      "Training Epoch: 14 [9950/13219]\tLoss: 724.4159\n",
      "Training Epoch: 14 [10000/13219]\tLoss: 715.1980\n",
      "Training Epoch: 14 [10050/13219]\tLoss: 733.0143\n",
      "Training Epoch: 14 [10100/13219]\tLoss: 704.5522\n",
      "Training Epoch: 14 [10150/13219]\tLoss: 708.0461\n",
      "Training Epoch: 14 [10200/13219]\tLoss: 699.7820\n",
      "Training Epoch: 14 [10250/13219]\tLoss: 741.7913\n",
      "Training Epoch: 14 [10300/13219]\tLoss: 746.7388\n",
      "Training Epoch: 14 [10350/13219]\tLoss: 731.7103\n",
      "Training Epoch: 14 [10400/13219]\tLoss: 711.2556\n",
      "Training Epoch: 14 [10450/13219]\tLoss: 745.2443\n",
      "Training Epoch: 14 [10500/13219]\tLoss: 726.5687\n",
      "Training Epoch: 14 [10550/13219]\tLoss: 717.7633\n",
      "Training Epoch: 14 [10600/13219]\tLoss: 681.7770\n",
      "Training Epoch: 14 [10650/13219]\tLoss: 718.3623\n",
      "Training Epoch: 14 [10700/13219]\tLoss: 708.9579\n",
      "Training Epoch: 14 [10750/13219]\tLoss: 764.5864\n",
      "Training Epoch: 14 [10800/13219]\tLoss: 715.7180\n",
      "Training Epoch: 14 [10850/13219]\tLoss: 714.7719\n",
      "Training Epoch: 14 [10900/13219]\tLoss: 717.4641\n",
      "Training Epoch: 14 [10950/13219]\tLoss: 688.5792\n",
      "Training Epoch: 14 [11000/13219]\tLoss: 724.4580\n",
      "Training Epoch: 14 [11050/13219]\tLoss: 738.6984\n",
      "Training Epoch: 14 [11100/13219]\tLoss: 680.4932\n",
      "Training Epoch: 14 [11150/13219]\tLoss: 777.1792\n",
      "Training Epoch: 14 [11200/13219]\tLoss: 747.8691\n",
      "Training Epoch: 14 [11250/13219]\tLoss: 727.2552\n",
      "Training Epoch: 14 [11300/13219]\tLoss: 725.3291\n",
      "Training Epoch: 14 [11350/13219]\tLoss: 694.9023\n",
      "Training Epoch: 14 [11400/13219]\tLoss: 694.5617\n",
      "Training Epoch: 14 [11450/13219]\tLoss: 731.6760\n",
      "Training Epoch: 14 [11500/13219]\tLoss: 779.2190\n",
      "Training Epoch: 14 [11550/13219]\tLoss: 706.7516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [11600/13219]\tLoss: 700.8413\n",
      "Training Epoch: 14 [11650/13219]\tLoss: 754.0212\n",
      "Training Epoch: 14 [11700/13219]\tLoss: 716.5876\n",
      "Training Epoch: 14 [11750/13219]\tLoss: 722.1716\n",
      "Training Epoch: 14 [11800/13219]\tLoss: 727.4462\n",
      "Training Epoch: 14 [11850/13219]\tLoss: 738.7188\n",
      "Training Epoch: 14 [11900/13219]\tLoss: 742.1146\n",
      "Training Epoch: 14 [11950/13219]\tLoss: 728.0741\n",
      "Training Epoch: 14 [12000/13219]\tLoss: 716.2756\n",
      "Training Epoch: 14 [12050/13219]\tLoss: 706.9046\n",
      "Training Epoch: 14 [12100/13219]\tLoss: 716.1285\n",
      "Training Epoch: 14 [12150/13219]\tLoss: 749.6113\n",
      "Training Epoch: 14 [12200/13219]\tLoss: 726.3292\n",
      "Training Epoch: 14 [12250/13219]\tLoss: 776.8123\n",
      "Training Epoch: 14 [12300/13219]\tLoss: 767.0355\n",
      "Training Epoch: 14 [12350/13219]\tLoss: 722.6252\n",
      "Training Epoch: 14 [12400/13219]\tLoss: 722.2745\n",
      "Training Epoch: 14 [12450/13219]\tLoss: 765.0991\n",
      "Training Epoch: 14 [12500/13219]\tLoss: 729.5065\n",
      "Training Epoch: 14 [12550/13219]\tLoss: 724.6375\n",
      "Training Epoch: 14 [12600/13219]\tLoss: 710.1946\n",
      "Training Epoch: 14 [12650/13219]\tLoss: 750.8828\n",
      "Training Epoch: 14 [12700/13219]\tLoss: 732.8691\n",
      "Training Epoch: 14 [12750/13219]\tLoss: 721.8320\n",
      "Training Epoch: 14 [12800/13219]\tLoss: 665.0938\n",
      "Training Epoch: 14 [12850/13219]\tLoss: 739.8965\n",
      "Training Epoch: 14 [12900/13219]\tLoss: 720.4322\n",
      "Training Epoch: 14 [12950/13219]\tLoss: 769.8871\n",
      "Training Epoch: 14 [13000/13219]\tLoss: 752.8742\n",
      "Training Epoch: 14 [13050/13219]\tLoss: 697.6296\n",
      "Training Epoch: 14 [13100/13219]\tLoss: 717.8249\n",
      "Training Epoch: 14 [13150/13219]\tLoss: 751.6870\n",
      "Training Epoch: 14 [13200/13219]\tLoss: 741.1551\n",
      "Training Epoch: 14 [13219/13219]\tLoss: 681.1729\n",
      "Training Epoch: 14 [1469/1469]\tLoss: 723.5939\n",
      "Training Epoch: 15 [50/13219]\tLoss: 716.7107\n",
      "Training Epoch: 15 [100/13219]\tLoss: 741.2307\n",
      "Training Epoch: 15 [150/13219]\tLoss: 725.0654\n",
      "Training Epoch: 15 [200/13219]\tLoss: 712.3057\n",
      "Training Epoch: 15 [250/13219]\tLoss: 703.3624\n",
      "Training Epoch: 15 [300/13219]\tLoss: 729.9148\n",
      "Training Epoch: 15 [350/13219]\tLoss: 733.1771\n",
      "Training Epoch: 15 [400/13219]\tLoss: 728.5423\n",
      "Training Epoch: 15 [450/13219]\tLoss: 732.4748\n",
      "Training Epoch: 15 [500/13219]\tLoss: 755.6714\n",
      "Training Epoch: 15 [550/13219]\tLoss: 742.6185\n",
      "Training Epoch: 15 [600/13219]\tLoss: 775.4507\n",
      "Training Epoch: 15 [650/13219]\tLoss: 779.1207\n",
      "Training Epoch: 15 [700/13219]\tLoss: 799.8861\n",
      "Training Epoch: 15 [750/13219]\tLoss: 750.8308\n",
      "Training Epoch: 15 [800/13219]\tLoss: 778.9337\n",
      "Training Epoch: 15 [850/13219]\tLoss: 787.2427\n",
      "Training Epoch: 15 [900/13219]\tLoss: 741.1097\n",
      "Training Epoch: 15 [950/13219]\tLoss: 737.8906\n",
      "Training Epoch: 15 [1000/13219]\tLoss: 701.8715\n",
      "Training Epoch: 15 [1050/13219]\tLoss: 741.1885\n",
      "Training Epoch: 15 [1100/13219]\tLoss: 725.5540\n",
      "Training Epoch: 15 [1150/13219]\tLoss: 692.2640\n",
      "Training Epoch: 15 [1200/13219]\tLoss: 716.4157\n",
      "Training Epoch: 15 [1250/13219]\tLoss: 746.6609\n",
      "Training Epoch: 15 [1300/13219]\tLoss: 708.3028\n",
      "Training Epoch: 15 [1350/13219]\tLoss: 732.8022\n",
      "Training Epoch: 15 [1400/13219]\tLoss: 710.9875\n",
      "Training Epoch: 15 [1450/13219]\tLoss: 712.5277\n",
      "Training Epoch: 15 [1500/13219]\tLoss: 696.5643\n",
      "Training Epoch: 15 [1550/13219]\tLoss: 691.0782\n",
      "Training Epoch: 15 [1600/13219]\tLoss: 726.4070\n",
      "Training Epoch: 15 [1650/13219]\tLoss: 719.2471\n",
      "Training Epoch: 15 [1700/13219]\tLoss: 708.0000\n",
      "Training Epoch: 15 [1750/13219]\tLoss: 683.4352\n",
      "Training Epoch: 15 [1800/13219]\tLoss: 699.7950\n",
      "Training Epoch: 15 [1850/13219]\tLoss: 721.6029\n",
      "Training Epoch: 15 [1900/13219]\tLoss: 682.4527\n",
      "Training Epoch: 15 [1950/13219]\tLoss: 693.5793\n",
      "Training Epoch: 15 [2000/13219]\tLoss: 699.5710\n",
      "Training Epoch: 15 [2050/13219]\tLoss: 745.0613\n",
      "Training Epoch: 15 [2100/13219]\tLoss: 715.2621\n",
      "Training Epoch: 15 [2150/13219]\tLoss: 744.1049\n",
      "Training Epoch: 15 [2200/13219]\tLoss: 709.2264\n",
      "Training Epoch: 15 [2250/13219]\tLoss: 715.6513\n",
      "Training Epoch: 15 [2300/13219]\tLoss: 735.2730\n",
      "Training Epoch: 15 [2350/13219]\tLoss: 720.0010\n",
      "Training Epoch: 15 [2400/13219]\tLoss: 699.8895\n",
      "Training Epoch: 15 [2450/13219]\tLoss: 690.2726\n",
      "Training Epoch: 15 [2500/13219]\tLoss: 760.2112\n",
      "Training Epoch: 15 [2550/13219]\tLoss: 690.7748\n",
      "Training Epoch: 15 [2600/13219]\tLoss: 737.7117\n",
      "Training Epoch: 15 [2650/13219]\tLoss: 727.3993\n",
      "Training Epoch: 15 [2700/13219]\tLoss: 744.2740\n",
      "Training Epoch: 15 [2750/13219]\tLoss: 736.6617\n",
      "Training Epoch: 15 [2800/13219]\tLoss: 729.9883\n",
      "Training Epoch: 15 [2850/13219]\tLoss: 749.4933\n",
      "Training Epoch: 15 [2900/13219]\tLoss: 685.4474\n",
      "Training Epoch: 15 [2950/13219]\tLoss: 689.9614\n",
      "Training Epoch: 15 [3000/13219]\tLoss: 728.9023\n",
      "Training Epoch: 15 [3050/13219]\tLoss: 697.9913\n",
      "Training Epoch: 15 [3100/13219]\tLoss: 742.8473\n",
      "Training Epoch: 15 [3150/13219]\tLoss: 731.3434\n",
      "Training Epoch: 15 [3200/13219]\tLoss: 719.7589\n",
      "Training Epoch: 15 [3250/13219]\tLoss: 702.9899\n",
      "Training Epoch: 15 [3300/13219]\tLoss: 716.4801\n",
      "Training Epoch: 15 [3350/13219]\tLoss: 679.2088\n",
      "Training Epoch: 15 [3400/13219]\tLoss: 705.7237\n",
      "Training Epoch: 15 [3450/13219]\tLoss: 715.4431\n",
      "Training Epoch: 15 [3500/13219]\tLoss: 727.6886\n",
      "Training Epoch: 15 [3550/13219]\tLoss: 727.1568\n",
      "Training Epoch: 15 [3600/13219]\tLoss: 704.0444\n",
      "Training Epoch: 15 [3650/13219]\tLoss: 713.3594\n",
      "Training Epoch: 15 [3700/13219]\tLoss: 706.6260\n",
      "Training Epoch: 15 [3750/13219]\tLoss: 686.1113\n",
      "Training Epoch: 15 [3800/13219]\tLoss: 668.9688\n",
      "Training Epoch: 15 [3850/13219]\tLoss: 702.6246\n",
      "Training Epoch: 15 [3900/13219]\tLoss: 718.4490\n",
      "Training Epoch: 15 [3950/13219]\tLoss: 751.4835\n",
      "Training Epoch: 15 [4000/13219]\tLoss: 712.5671\n",
      "Training Epoch: 15 [4050/13219]\tLoss: 686.9702\n",
      "Training Epoch: 15 [4100/13219]\tLoss: 710.9280\n",
      "Training Epoch: 15 [4150/13219]\tLoss: 706.2064\n",
      "Training Epoch: 15 [4200/13219]\tLoss: 670.9439\n",
      "Training Epoch: 15 [4250/13219]\tLoss: 708.9190\n",
      "Training Epoch: 15 [4300/13219]\tLoss: 751.8895\n",
      "Training Epoch: 15 [4350/13219]\tLoss: 718.5524\n",
      "Training Epoch: 15 [4400/13219]\tLoss: 728.4746\n",
      "Training Epoch: 15 [4450/13219]\tLoss: 715.8647\n",
      "Training Epoch: 15 [4500/13219]\tLoss: 707.8022\n",
      "Training Epoch: 15 [4550/13219]\tLoss: 711.0017\n",
      "Training Epoch: 15 [4600/13219]\tLoss: 715.5178\n",
      "Training Epoch: 15 [4650/13219]\tLoss: 768.0540\n",
      "Training Epoch: 15 [4700/13219]\tLoss: 733.6343\n",
      "Training Epoch: 15 [4750/13219]\tLoss: 697.5479\n",
      "Training Epoch: 15 [4800/13219]\tLoss: 690.3804\n",
      "Training Epoch: 15 [4850/13219]\tLoss: 723.0807\n",
      "Training Epoch: 15 [4900/13219]\tLoss: 712.8229\n",
      "Training Epoch: 15 [4950/13219]\tLoss: 692.3861\n",
      "Training Epoch: 15 [5000/13219]\tLoss: 722.8033\n",
      "Training Epoch: 15 [5050/13219]\tLoss: 737.5630\n",
      "Training Epoch: 15 [5100/13219]\tLoss: 726.8497\n",
      "Training Epoch: 15 [5150/13219]\tLoss: 715.8727\n",
      "Training Epoch: 15 [5200/13219]\tLoss: 698.0382\n",
      "Training Epoch: 15 [5250/13219]\tLoss: 683.2038\n",
      "Training Epoch: 15 [5300/13219]\tLoss: 746.5369\n",
      "Training Epoch: 15 [5350/13219]\tLoss: 696.0459\n",
      "Training Epoch: 15 [5400/13219]\tLoss: 718.6983\n",
      "Training Epoch: 15 [5450/13219]\tLoss: 715.4300\n",
      "Training Epoch: 15 [5500/13219]\tLoss: 733.6465\n",
      "Training Epoch: 15 [5550/13219]\tLoss: 721.5959\n",
      "Training Epoch: 15 [5600/13219]\tLoss: 712.2701\n",
      "Training Epoch: 15 [5650/13219]\tLoss: 720.5983\n",
      "Training Epoch: 15 [5700/13219]\tLoss: 720.7029\n",
      "Training Epoch: 15 [5750/13219]\tLoss: 704.8159\n",
      "Training Epoch: 15 [5800/13219]\tLoss: 710.5320\n",
      "Training Epoch: 15 [5850/13219]\tLoss: 729.5943\n",
      "Training Epoch: 15 [5900/13219]\tLoss: 704.2528\n",
      "Training Epoch: 15 [5950/13219]\tLoss: 689.8696\n",
      "Training Epoch: 15 [6000/13219]\tLoss: 704.2680\n",
      "Training Epoch: 15 [6050/13219]\tLoss: 727.3655\n",
      "Training Epoch: 15 [6100/13219]\tLoss: 675.9477\n",
      "Training Epoch: 15 [6150/13219]\tLoss: 748.8821\n",
      "Training Epoch: 15 [6200/13219]\tLoss: 719.0548\n",
      "Training Epoch: 15 [6250/13219]\tLoss: 723.3158\n",
      "Training Epoch: 15 [6300/13219]\tLoss: 746.4135\n",
      "Training Epoch: 15 [6350/13219]\tLoss: 711.7255\n",
      "Training Epoch: 15 [6400/13219]\tLoss: 709.1938\n",
      "Training Epoch: 15 [6450/13219]\tLoss: 706.0298\n",
      "Training Epoch: 15 [6500/13219]\tLoss: 713.6924\n",
      "Training Epoch: 15 [6550/13219]\tLoss: 735.3473\n",
      "Training Epoch: 15 [6600/13219]\tLoss: 736.4272\n",
      "Training Epoch: 15 [6650/13219]\tLoss: 712.4296\n",
      "Training Epoch: 15 [6700/13219]\tLoss: 708.6379\n",
      "Training Epoch: 15 [6750/13219]\tLoss: 692.4283\n",
      "Training Epoch: 15 [6800/13219]\tLoss: 699.1130\n",
      "Training Epoch: 15 [6850/13219]\tLoss: 685.6477\n",
      "Training Epoch: 15 [6900/13219]\tLoss: 718.0991\n",
      "Training Epoch: 15 [6950/13219]\tLoss: 723.5218\n",
      "Training Epoch: 15 [7000/13219]\tLoss: 737.5067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [7050/13219]\tLoss: 731.7645\n",
      "Training Epoch: 15 [7100/13219]\tLoss: 706.7906\n",
      "Training Epoch: 15 [7150/13219]\tLoss: 727.5759\n",
      "Training Epoch: 15 [7200/13219]\tLoss: 687.4910\n",
      "Training Epoch: 15 [7250/13219]\tLoss: 727.7625\n",
      "Training Epoch: 15 [7300/13219]\tLoss: 718.5217\n",
      "Training Epoch: 15 [7350/13219]\tLoss: 718.4708\n",
      "Training Epoch: 15 [7400/13219]\tLoss: 753.5355\n",
      "Training Epoch: 15 [7450/13219]\tLoss: 714.9842\n",
      "Training Epoch: 15 [7500/13219]\tLoss: 722.2338\n",
      "Training Epoch: 15 [7550/13219]\tLoss: 712.0779\n",
      "Training Epoch: 15 [7600/13219]\tLoss: 699.7518\n",
      "Training Epoch: 15 [7650/13219]\tLoss: 683.3535\n",
      "Training Epoch: 15 [7700/13219]\tLoss: 697.5016\n",
      "Training Epoch: 15 [7750/13219]\tLoss: 721.4477\n",
      "Training Epoch: 15 [7800/13219]\tLoss: 696.9599\n",
      "Training Epoch: 15 [7850/13219]\tLoss: 698.7310\n",
      "Training Epoch: 15 [7900/13219]\tLoss: 712.4798\n",
      "Training Epoch: 15 [7950/13219]\tLoss: 708.3447\n",
      "Training Epoch: 15 [8000/13219]\tLoss: 712.8180\n",
      "Training Epoch: 15 [8050/13219]\tLoss: 761.0558\n",
      "Training Epoch: 15 [8100/13219]\tLoss: 718.9742\n",
      "Training Epoch: 15 [8150/13219]\tLoss: 753.6012\n",
      "Training Epoch: 15 [8200/13219]\tLoss: 703.7349\n",
      "Training Epoch: 15 [8250/13219]\tLoss: 721.5641\n",
      "Training Epoch: 15 [8300/13219]\tLoss: 742.5345\n",
      "Training Epoch: 15 [8350/13219]\tLoss: 744.3653\n",
      "Training Epoch: 15 [8400/13219]\tLoss: 702.2619\n",
      "Training Epoch: 15 [8450/13219]\tLoss: 714.3774\n",
      "Training Epoch: 15 [8500/13219]\tLoss: 703.8221\n",
      "Training Epoch: 15 [8550/13219]\tLoss: 737.2096\n",
      "Training Epoch: 15 [8600/13219]\tLoss: 695.6603\n",
      "Training Epoch: 15 [8650/13219]\tLoss: 720.3529\n",
      "Training Epoch: 15 [8700/13219]\tLoss: 727.3642\n",
      "Training Epoch: 15 [8750/13219]\tLoss: 678.5371\n",
      "Training Epoch: 15 [8800/13219]\tLoss: 696.7657\n",
      "Training Epoch: 15 [8850/13219]\tLoss: 735.4288\n",
      "Training Epoch: 15 [8900/13219]\tLoss: 669.7325\n",
      "Training Epoch: 15 [8950/13219]\tLoss: 710.0095\n",
      "Training Epoch: 15 [9000/13219]\tLoss: 697.5764\n",
      "Training Epoch: 15 [9050/13219]\tLoss: 722.5637\n",
      "Training Epoch: 15 [9100/13219]\tLoss: 697.2333\n",
      "Training Epoch: 15 [9150/13219]\tLoss: 681.7908\n",
      "Training Epoch: 15 [9200/13219]\tLoss: 690.5423\n",
      "Training Epoch: 15 [9250/13219]\tLoss: 736.1777\n",
      "Training Epoch: 15 [9300/13219]\tLoss: 695.7618\n",
      "Training Epoch: 15 [9350/13219]\tLoss: 683.3601\n",
      "Training Epoch: 15 [9400/13219]\tLoss: 686.9958\n",
      "Training Epoch: 15 [9450/13219]\tLoss: 740.6384\n",
      "Training Epoch: 15 [9500/13219]\tLoss: 717.1577\n",
      "Training Epoch: 15 [9550/13219]\tLoss: 715.9468\n",
      "Training Epoch: 15 [9600/13219]\tLoss: 667.7387\n",
      "Training Epoch: 15 [9650/13219]\tLoss: 698.7311\n",
      "Training Epoch: 15 [9700/13219]\tLoss: 674.7864\n",
      "Training Epoch: 15 [9750/13219]\tLoss: 694.7987\n",
      "Training Epoch: 15 [9800/13219]\tLoss: 760.9409\n",
      "Training Epoch: 15 [9850/13219]\tLoss: 700.9573\n",
      "Training Epoch: 15 [9900/13219]\tLoss: 701.5061\n",
      "Training Epoch: 15 [9950/13219]\tLoss: 735.6810\n",
      "Training Epoch: 15 [10000/13219]\tLoss: 713.8465\n",
      "Training Epoch: 15 [10050/13219]\tLoss: 756.9415\n",
      "Training Epoch: 15 [10100/13219]\tLoss: 748.4994\n",
      "Training Epoch: 15 [10150/13219]\tLoss: 790.1462\n",
      "Training Epoch: 15 [10200/13219]\tLoss: 744.3070\n",
      "Training Epoch: 15 [10250/13219]\tLoss: 768.2910\n",
      "Training Epoch: 15 [10300/13219]\tLoss: 746.9773\n",
      "Training Epoch: 15 [10350/13219]\tLoss: 720.6799\n",
      "Training Epoch: 15 [10400/13219]\tLoss: 714.7084\n",
      "Training Epoch: 15 [10450/13219]\tLoss: 686.4872\n",
      "Training Epoch: 15 [10500/13219]\tLoss: 677.8094\n",
      "Training Epoch: 15 [10550/13219]\tLoss: 713.4715\n",
      "Training Epoch: 15 [10600/13219]\tLoss: 700.6177\n",
      "Training Epoch: 15 [10650/13219]\tLoss: 725.8044\n",
      "Training Epoch: 15 [10700/13219]\tLoss: 700.1437\n",
      "Training Epoch: 15 [10750/13219]\tLoss: 740.2306\n",
      "Training Epoch: 15 [10800/13219]\tLoss: 732.4854\n",
      "Training Epoch: 15 [10850/13219]\tLoss: 726.3521\n",
      "Training Epoch: 15 [10900/13219]\tLoss: 748.5308\n",
      "Training Epoch: 15 [10950/13219]\tLoss: 726.6581\n",
      "Training Epoch: 15 [11000/13219]\tLoss: 728.2446\n",
      "Training Epoch: 15 [11050/13219]\tLoss: 710.0168\n",
      "Training Epoch: 15 [11100/13219]\tLoss: 691.6913\n",
      "Training Epoch: 15 [11150/13219]\tLoss: 719.4928\n",
      "Training Epoch: 15 [11200/13219]\tLoss: 724.5355\n",
      "Training Epoch: 15 [11250/13219]\tLoss: 732.1459\n",
      "Training Epoch: 15 [11300/13219]\tLoss: 721.6032\n",
      "Training Epoch: 15 [11350/13219]\tLoss: 683.4019\n",
      "Training Epoch: 15 [11400/13219]\tLoss: 708.9109\n",
      "Training Epoch: 15 [11450/13219]\tLoss: 714.4410\n",
      "Training Epoch: 15 [11500/13219]\tLoss: 710.8188\n",
      "Training Epoch: 15 [11550/13219]\tLoss: 752.2136\n",
      "Training Epoch: 15 [11600/13219]\tLoss: 715.7352\n",
      "Training Epoch: 15 [11650/13219]\tLoss: 707.8343\n",
      "Training Epoch: 15 [11700/13219]\tLoss: 690.4536\n",
      "Training Epoch: 15 [11750/13219]\tLoss: 682.6396\n",
      "Training Epoch: 15 [11800/13219]\tLoss: 713.1583\n",
      "Training Epoch: 15 [11850/13219]\tLoss: 732.6548\n",
      "Training Epoch: 15 [11900/13219]\tLoss: 721.0049\n",
      "Training Epoch: 15 [11950/13219]\tLoss: 724.2678\n",
      "Training Epoch: 15 [12000/13219]\tLoss: 698.7214\n",
      "Training Epoch: 15 [12050/13219]\tLoss: 719.6763\n",
      "Training Epoch: 15 [12100/13219]\tLoss: 715.6423\n",
      "Training Epoch: 15 [12150/13219]\tLoss: 677.9826\n",
      "Training Epoch: 15 [12200/13219]\tLoss: 696.8654\n",
      "Training Epoch: 15 [12250/13219]\tLoss: 757.7999\n",
      "Training Epoch: 15 [12300/13219]\tLoss: 717.7524\n",
      "Training Epoch: 15 [12350/13219]\tLoss: 720.2453\n",
      "Training Epoch: 15 [12400/13219]\tLoss: 687.1143\n",
      "Training Epoch: 15 [12450/13219]\tLoss: 706.2838\n",
      "Training Epoch: 15 [12500/13219]\tLoss: 697.9570\n",
      "Training Epoch: 15 [12550/13219]\tLoss: 681.0253\n",
      "Training Epoch: 15 [12600/13219]\tLoss: 676.8737\n",
      "Training Epoch: 15 [12650/13219]\tLoss: 696.8403\n",
      "Training Epoch: 15 [12700/13219]\tLoss: 710.9136\n",
      "Training Epoch: 15 [12750/13219]\tLoss: 742.6732\n",
      "Training Epoch: 15 [12800/13219]\tLoss: 727.7169\n",
      "Training Epoch: 15 [12850/13219]\tLoss: 717.9116\n",
      "Training Epoch: 15 [12900/13219]\tLoss: 697.7835\n",
      "Training Epoch: 15 [12950/13219]\tLoss: 735.3300\n",
      "Training Epoch: 15 [13000/13219]\tLoss: 737.0714\n",
      "Training Epoch: 15 [13050/13219]\tLoss: 714.6249\n",
      "Training Epoch: 15 [13100/13219]\tLoss: 690.5136\n",
      "Training Epoch: 15 [13150/13219]\tLoss: 687.0057\n",
      "Training Epoch: 15 [13200/13219]\tLoss: 746.2484\n",
      "Training Epoch: 15 [13219/13219]\tLoss: 703.2941\n",
      "Training Epoch: 15 [1469/1469]\tLoss: 702.1751\n",
      "Training Epoch: 16 [50/13219]\tLoss: 692.2485\n",
      "Training Epoch: 16 [100/13219]\tLoss: 710.1462\n",
      "Training Epoch: 16 [150/13219]\tLoss: 722.3734\n",
      "Training Epoch: 16 [200/13219]\tLoss: 710.1617\n",
      "Training Epoch: 16 [250/13219]\tLoss: 697.5200\n",
      "Training Epoch: 16 [300/13219]\tLoss: 743.5899\n",
      "Training Epoch: 16 [350/13219]\tLoss: 697.8026\n",
      "Training Epoch: 16 [400/13219]\tLoss: 733.1408\n",
      "Training Epoch: 16 [450/13219]\tLoss: 708.0269\n",
      "Training Epoch: 16 [500/13219]\tLoss: 703.5759\n",
      "Training Epoch: 16 [550/13219]\tLoss: 706.8353\n",
      "Training Epoch: 16 [600/13219]\tLoss: 743.7188\n",
      "Training Epoch: 16 [650/13219]\tLoss: 691.5928\n",
      "Training Epoch: 16 [700/13219]\tLoss: 675.5924\n",
      "Training Epoch: 16 [750/13219]\tLoss: 716.2382\n",
      "Training Epoch: 16 [800/13219]\tLoss: 710.4344\n",
      "Training Epoch: 16 [850/13219]\tLoss: 689.3385\n",
      "Training Epoch: 16 [900/13219]\tLoss: 698.3328\n",
      "Training Epoch: 16 [950/13219]\tLoss: 739.6171\n",
      "Training Epoch: 16 [1000/13219]\tLoss: 691.7050\n",
      "Training Epoch: 16 [1050/13219]\tLoss: 725.1682\n",
      "Training Epoch: 16 [1100/13219]\tLoss: 678.4839\n",
      "Training Epoch: 16 [1150/13219]\tLoss: 712.9615\n",
      "Training Epoch: 16 [1200/13219]\tLoss: 725.5233\n",
      "Training Epoch: 16 [1250/13219]\tLoss: 671.4257\n",
      "Training Epoch: 16 [1300/13219]\tLoss: 716.6080\n",
      "Training Epoch: 16 [1350/13219]\tLoss: 739.3653\n",
      "Training Epoch: 16 [1400/13219]\tLoss: 710.0617\n",
      "Training Epoch: 16 [1450/13219]\tLoss: 723.3044\n",
      "Training Epoch: 16 [1500/13219]\tLoss: 709.5976\n",
      "Training Epoch: 16 [1550/13219]\tLoss: 735.0817\n",
      "Training Epoch: 16 [1600/13219]\tLoss: 724.6144\n",
      "Training Epoch: 16 [1650/13219]\tLoss: 751.0474\n",
      "Training Epoch: 16 [1700/13219]\tLoss: 701.6057\n",
      "Training Epoch: 16 [1750/13219]\tLoss: 728.3337\n",
      "Training Epoch: 16 [1800/13219]\tLoss: 746.3530\n",
      "Training Epoch: 16 [1850/13219]\tLoss: 706.6146\n",
      "Training Epoch: 16 [1900/13219]\tLoss: 740.0821\n",
      "Training Epoch: 16 [1950/13219]\tLoss: 687.8254\n",
      "Training Epoch: 16 [2000/13219]\tLoss: 754.1534\n",
      "Training Epoch: 16 [2050/13219]\tLoss: 658.7265\n",
      "Training Epoch: 16 [2100/13219]\tLoss: 690.4822\n",
      "Training Epoch: 16 [2150/13219]\tLoss: 703.3888\n",
      "Training Epoch: 16 [2200/13219]\tLoss: 729.6423\n",
      "Training Epoch: 16 [2250/13219]\tLoss: 691.4399\n",
      "Training Epoch: 16 [2300/13219]\tLoss: 691.2269\n",
      "Training Epoch: 16 [2350/13219]\tLoss: 735.7494\n",
      "Training Epoch: 16 [2400/13219]\tLoss: 723.7302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [2450/13219]\tLoss: 727.6563\n",
      "Training Epoch: 16 [2500/13219]\tLoss: 753.9682\n",
      "Training Epoch: 16 [2550/13219]\tLoss: 763.3097\n",
      "Training Epoch: 16 [2600/13219]\tLoss: 725.5445\n",
      "Training Epoch: 16 [2650/13219]\tLoss: 790.8953\n",
      "Training Epoch: 16 [2700/13219]\tLoss: 761.7496\n",
      "Training Epoch: 16 [2750/13219]\tLoss: 715.7839\n",
      "Training Epoch: 16 [2800/13219]\tLoss: 686.4170\n",
      "Training Epoch: 16 [2850/13219]\tLoss: 708.5341\n",
      "Training Epoch: 16 [2900/13219]\tLoss: 695.4252\n",
      "Training Epoch: 16 [2950/13219]\tLoss: 724.5978\n",
      "Training Epoch: 16 [3000/13219]\tLoss: 748.6279\n",
      "Training Epoch: 16 [3050/13219]\tLoss: 740.8932\n",
      "Training Epoch: 16 [3100/13219]\tLoss: 699.7446\n",
      "Training Epoch: 16 [3150/13219]\tLoss: 717.2307\n",
      "Training Epoch: 16 [3200/13219]\tLoss: 718.6259\n",
      "Training Epoch: 16 [3250/13219]\tLoss: 692.0477\n",
      "Training Epoch: 16 [3300/13219]\tLoss: 736.0128\n",
      "Training Epoch: 16 [3350/13219]\tLoss: 715.4318\n",
      "Training Epoch: 16 [3400/13219]\tLoss: 703.3478\n",
      "Training Epoch: 16 [3450/13219]\tLoss: 714.9396\n",
      "Training Epoch: 16 [3500/13219]\tLoss: 675.4908\n",
      "Training Epoch: 16 [3550/13219]\tLoss: 706.1758\n",
      "Training Epoch: 16 [3600/13219]\tLoss: 722.7388\n",
      "Training Epoch: 16 [3650/13219]\tLoss: 714.0948\n",
      "Training Epoch: 16 [3700/13219]\tLoss: 693.4175\n",
      "Training Epoch: 16 [3750/13219]\tLoss: 715.3371\n",
      "Training Epoch: 16 [3800/13219]\tLoss: 684.3005\n",
      "Training Epoch: 16 [3850/13219]\tLoss: 676.2724\n",
      "Training Epoch: 16 [3900/13219]\tLoss: 726.5942\n",
      "Training Epoch: 16 [3950/13219]\tLoss: 687.1542\n",
      "Training Epoch: 16 [4000/13219]\tLoss: 702.5820\n",
      "Training Epoch: 16 [4050/13219]\tLoss: 719.9403\n",
      "Training Epoch: 16 [4100/13219]\tLoss: 738.4052\n",
      "Training Epoch: 16 [4150/13219]\tLoss: 733.5842\n",
      "Training Epoch: 16 [4200/13219]\tLoss: 677.7131\n",
      "Training Epoch: 16 [4250/13219]\tLoss: 701.3160\n",
      "Training Epoch: 16 [4300/13219]\tLoss: 691.6107\n",
      "Training Epoch: 16 [4350/13219]\tLoss: 704.6140\n",
      "Training Epoch: 16 [4400/13219]\tLoss: 679.1220\n",
      "Training Epoch: 16 [4450/13219]\tLoss: 731.2343\n",
      "Training Epoch: 16 [4500/13219]\tLoss: 726.6346\n",
      "Training Epoch: 16 [4550/13219]\tLoss: 686.5347\n",
      "Training Epoch: 16 [4600/13219]\tLoss: 738.9105\n",
      "Training Epoch: 16 [4650/13219]\tLoss: 745.4910\n",
      "Training Epoch: 16 [4700/13219]\tLoss: 714.0208\n",
      "Training Epoch: 16 [4750/13219]\tLoss: 740.2884\n",
      "Training Epoch: 16 [4800/13219]\tLoss: 709.6321\n",
      "Training Epoch: 16 [4850/13219]\tLoss: 706.3976\n",
      "Training Epoch: 16 [4900/13219]\tLoss: 699.6623\n",
      "Training Epoch: 16 [4950/13219]\tLoss: 722.6429\n",
      "Training Epoch: 16 [5000/13219]\tLoss: 708.8834\n",
      "Training Epoch: 16 [5050/13219]\tLoss: 731.9681\n",
      "Training Epoch: 16 [5100/13219]\tLoss: 683.8715\n",
      "Training Epoch: 16 [5150/13219]\tLoss: 681.1870\n",
      "Training Epoch: 16 [5200/13219]\tLoss: 693.5457\n",
      "Training Epoch: 16 [5250/13219]\tLoss: 690.3937\n",
      "Training Epoch: 16 [5300/13219]\tLoss: 655.0906\n",
      "Training Epoch: 16 [5350/13219]\tLoss: 706.4084\n",
      "Training Epoch: 16 [5400/13219]\tLoss: 679.7607\n",
      "Training Epoch: 16 [5450/13219]\tLoss: 680.6557\n",
      "Training Epoch: 16 [5500/13219]\tLoss: 714.1499\n",
      "Training Epoch: 16 [5550/13219]\tLoss: 719.7134\n",
      "Training Epoch: 16 [5600/13219]\tLoss: 686.4005\n",
      "Training Epoch: 16 [5650/13219]\tLoss: 702.4695\n",
      "Training Epoch: 16 [5700/13219]\tLoss: 699.3384\n",
      "Training Epoch: 16 [5750/13219]\tLoss: 721.9775\n",
      "Training Epoch: 16 [5800/13219]\tLoss: 701.8828\n",
      "Training Epoch: 16 [5850/13219]\tLoss: 668.6650\n",
      "Training Epoch: 16 [5900/13219]\tLoss: 673.1401\n",
      "Training Epoch: 16 [5950/13219]\tLoss: 717.1596\n",
      "Training Epoch: 16 [6000/13219]\tLoss: 664.6096\n",
      "Training Epoch: 16 [6050/13219]\tLoss: 711.5166\n",
      "Training Epoch: 16 [6100/13219]\tLoss: 713.1781\n",
      "Training Epoch: 16 [6150/13219]\tLoss: 727.7788\n",
      "Training Epoch: 16 [6200/13219]\tLoss: 713.7216\n",
      "Training Epoch: 16 [6250/13219]\tLoss: 701.7643\n",
      "Training Epoch: 16 [6300/13219]\tLoss: 695.0876\n",
      "Training Epoch: 16 [6350/13219]\tLoss: 736.9114\n",
      "Training Epoch: 16 [6400/13219]\tLoss: 665.2216\n",
      "Training Epoch: 16 [6450/13219]\tLoss: 705.2775\n",
      "Training Epoch: 16 [6500/13219]\tLoss: 699.0050\n",
      "Training Epoch: 16 [6550/13219]\tLoss: 749.3323\n",
      "Training Epoch: 16 [6600/13219]\tLoss: 718.3582\n",
      "Training Epoch: 16 [6650/13219]\tLoss: 720.1843\n",
      "Training Epoch: 16 [6700/13219]\tLoss: 699.6584\n",
      "Training Epoch: 16 [6750/13219]\tLoss: 701.2216\n",
      "Training Epoch: 16 [6800/13219]\tLoss: 698.5135\n",
      "Training Epoch: 16 [6850/13219]\tLoss: 687.1662\n",
      "Training Epoch: 16 [6900/13219]\tLoss: 688.6094\n",
      "Training Epoch: 16 [6950/13219]\tLoss: 699.4270\n",
      "Training Epoch: 16 [7000/13219]\tLoss: 699.1843\n",
      "Training Epoch: 16 [7050/13219]\tLoss: 708.2543\n",
      "Training Epoch: 16 [7100/13219]\tLoss: 667.9371\n",
      "Training Epoch: 16 [7150/13219]\tLoss: 711.7363\n",
      "Training Epoch: 16 [7200/13219]\tLoss: 682.6346\n",
      "Training Epoch: 16 [7250/13219]\tLoss: 694.8437\n",
      "Training Epoch: 16 [7300/13219]\tLoss: 667.1429\n",
      "Training Epoch: 16 [7350/13219]\tLoss: 714.3870\n",
      "Training Epoch: 16 [7400/13219]\tLoss: 721.5100\n",
      "Training Epoch: 16 [7450/13219]\tLoss: 697.8425\n",
      "Training Epoch: 16 [7500/13219]\tLoss: 727.5225\n",
      "Training Epoch: 16 [7550/13219]\tLoss: 706.2530\n",
      "Training Epoch: 16 [7600/13219]\tLoss: 688.0942\n",
      "Training Epoch: 16 [7650/13219]\tLoss: 717.0613\n",
      "Training Epoch: 16 [7700/13219]\tLoss: 702.8787\n",
      "Training Epoch: 16 [7750/13219]\tLoss: 688.3732\n",
      "Training Epoch: 16 [7800/13219]\tLoss: 675.6078\n",
      "Training Epoch: 16 [7850/13219]\tLoss: 703.0360\n",
      "Training Epoch: 16 [7900/13219]\tLoss: 691.6082\n",
      "Training Epoch: 16 [7950/13219]\tLoss: 717.0936\n",
      "Training Epoch: 16 [8000/13219]\tLoss: 658.6435\n",
      "Training Epoch: 16 [8050/13219]\tLoss: 696.8924\n",
      "Training Epoch: 16 [8100/13219]\tLoss: 684.3296\n",
      "Training Epoch: 16 [8150/13219]\tLoss: 727.7459\n",
      "Training Epoch: 16 [8200/13219]\tLoss: 682.0830\n",
      "Training Epoch: 16 [8250/13219]\tLoss: 719.5401\n",
      "Training Epoch: 16 [8300/13219]\tLoss: 693.2702\n",
      "Training Epoch: 16 [8350/13219]\tLoss: 695.3599\n",
      "Training Epoch: 16 [8400/13219]\tLoss: 706.6401\n",
      "Training Epoch: 16 [8450/13219]\tLoss: 680.2784\n",
      "Training Epoch: 16 [8500/13219]\tLoss: 711.3577\n",
      "Training Epoch: 16 [8550/13219]\tLoss: 697.8216\n",
      "Training Epoch: 16 [8600/13219]\tLoss: 697.6526\n",
      "Training Epoch: 16 [8650/13219]\tLoss: 701.1374\n",
      "Training Epoch: 16 [8700/13219]\tLoss: 719.5497\n",
      "Training Epoch: 16 [8750/13219]\tLoss: 694.2465\n",
      "Training Epoch: 16 [8800/13219]\tLoss: 711.1517\n",
      "Training Epoch: 16 [8850/13219]\tLoss: 695.5051\n",
      "Training Epoch: 16 [8900/13219]\tLoss: 713.7629\n",
      "Training Epoch: 16 [8950/13219]\tLoss: 692.6505\n",
      "Training Epoch: 16 [9000/13219]\tLoss: 693.8030\n",
      "Training Epoch: 16 [9050/13219]\tLoss: 695.0053\n",
      "Training Epoch: 16 [9100/13219]\tLoss: 761.9005\n",
      "Training Epoch: 16 [9150/13219]\tLoss: 691.3867\n",
      "Training Epoch: 16 [9200/13219]\tLoss: 722.6475\n",
      "Training Epoch: 16 [9250/13219]\tLoss: 710.8538\n",
      "Training Epoch: 16 [9300/13219]\tLoss: 713.5788\n",
      "Training Epoch: 16 [9350/13219]\tLoss: 715.7727\n",
      "Training Epoch: 16 [9400/13219]\tLoss: 729.2245\n",
      "Training Epoch: 16 [9450/13219]\tLoss: 716.7477\n",
      "Training Epoch: 16 [9500/13219]\tLoss: 720.0912\n",
      "Training Epoch: 16 [9550/13219]\tLoss: 701.0927\n",
      "Training Epoch: 16 [9600/13219]\tLoss: 688.9489\n",
      "Training Epoch: 16 [9650/13219]\tLoss: 734.7195\n",
      "Training Epoch: 16 [9700/13219]\tLoss: 724.4003\n",
      "Training Epoch: 16 [9750/13219]\tLoss: 735.7418\n",
      "Training Epoch: 16 [9800/13219]\tLoss: 778.9028\n",
      "Training Epoch: 16 [9850/13219]\tLoss: 707.2120\n",
      "Training Epoch: 16 [9900/13219]\tLoss: 725.4512\n",
      "Training Epoch: 16 [9950/13219]\tLoss: 702.8224\n",
      "Training Epoch: 16 [10000/13219]\tLoss: 675.9880\n",
      "Training Epoch: 16 [10050/13219]\tLoss: 711.1543\n",
      "Training Epoch: 16 [10100/13219]\tLoss: 716.2981\n",
      "Training Epoch: 16 [10150/13219]\tLoss: 693.8099\n",
      "Training Epoch: 16 [10200/13219]\tLoss: 693.5408\n",
      "Training Epoch: 16 [10250/13219]\tLoss: 692.0507\n",
      "Training Epoch: 16 [10300/13219]\tLoss: 729.0717\n",
      "Training Epoch: 16 [10350/13219]\tLoss: 725.8777\n",
      "Training Epoch: 16 [10400/13219]\tLoss: 696.0352\n",
      "Training Epoch: 16 [10450/13219]\tLoss: 685.7234\n",
      "Training Epoch: 16 [10500/13219]\tLoss: 687.3711\n",
      "Training Epoch: 16 [10550/13219]\tLoss: 669.2184\n",
      "Training Epoch: 16 [10600/13219]\tLoss: 704.2438\n",
      "Training Epoch: 16 [10650/13219]\tLoss: 706.4891\n",
      "Training Epoch: 16 [10700/13219]\tLoss: 730.5303\n",
      "Training Epoch: 16 [10750/13219]\tLoss: 690.4565\n",
      "Training Epoch: 16 [10800/13219]\tLoss: 715.7480\n",
      "Training Epoch: 16 [10850/13219]\tLoss: 706.9427\n",
      "Training Epoch: 16 [10900/13219]\tLoss: 699.6754\n",
      "Training Epoch: 16 [10950/13219]\tLoss: 682.5918\n",
      "Training Epoch: 16 [11000/13219]\tLoss: 676.2090\n",
      "Training Epoch: 16 [11050/13219]\tLoss: 674.8973\n",
      "Training Epoch: 16 [11100/13219]\tLoss: 680.5455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [11150/13219]\tLoss: 712.3098\n",
      "Training Epoch: 16 [11200/13219]\tLoss: 704.4591\n",
      "Training Epoch: 16 [11250/13219]\tLoss: 686.9704\n",
      "Training Epoch: 16 [11300/13219]\tLoss: 685.7860\n",
      "Training Epoch: 16 [11350/13219]\tLoss: 694.5526\n",
      "Training Epoch: 16 [11400/13219]\tLoss: 680.9238\n",
      "Training Epoch: 16 [11450/13219]\tLoss: 686.9296\n",
      "Training Epoch: 16 [11500/13219]\tLoss: 738.8110\n",
      "Training Epoch: 16 [11550/13219]\tLoss: 719.6867\n",
      "Training Epoch: 16 [11600/13219]\tLoss: 707.3965\n",
      "Training Epoch: 16 [11650/13219]\tLoss: 718.5328\n",
      "Training Epoch: 16 [11700/13219]\tLoss: 682.1667\n",
      "Training Epoch: 16 [11750/13219]\tLoss: 720.3328\n",
      "Training Epoch: 16 [11800/13219]\tLoss: 669.2908\n",
      "Training Epoch: 16 [11850/13219]\tLoss: 667.8064\n",
      "Training Epoch: 16 [11900/13219]\tLoss: 711.2332\n",
      "Training Epoch: 16 [11950/13219]\tLoss: 701.3683\n",
      "Training Epoch: 16 [12000/13219]\tLoss: 680.5057\n",
      "Training Epoch: 16 [12050/13219]\tLoss: 711.0894\n",
      "Training Epoch: 16 [12100/13219]\tLoss: 663.7670\n",
      "Training Epoch: 16 [12150/13219]\tLoss: 699.0799\n",
      "Training Epoch: 16 [12200/13219]\tLoss: 698.4773\n",
      "Training Epoch: 16 [12250/13219]\tLoss: 675.3534\n",
      "Training Epoch: 16 [12300/13219]\tLoss: 680.5720\n",
      "Training Epoch: 16 [12350/13219]\tLoss: 729.3230\n",
      "Training Epoch: 16 [12400/13219]\tLoss: 701.5290\n",
      "Training Epoch: 16 [12450/13219]\tLoss: 688.5085\n",
      "Training Epoch: 16 [12500/13219]\tLoss: 699.5756\n",
      "Training Epoch: 16 [12550/13219]\tLoss: 705.7247\n",
      "Training Epoch: 16 [12600/13219]\tLoss: 661.2669\n",
      "Training Epoch: 16 [12650/13219]\tLoss: 697.7351\n",
      "Training Epoch: 16 [12700/13219]\tLoss: 698.4529\n",
      "Training Epoch: 16 [12750/13219]\tLoss: 690.0641\n",
      "Training Epoch: 16 [12800/13219]\tLoss: 687.8474\n",
      "Training Epoch: 16 [12850/13219]\tLoss: 721.3864\n",
      "Training Epoch: 16 [12900/13219]\tLoss: 698.4611\n",
      "Training Epoch: 16 [12950/13219]\tLoss: 685.5304\n",
      "Training Epoch: 16 [13000/13219]\tLoss: 671.7238\n",
      "Training Epoch: 16 [13050/13219]\tLoss: 724.0790\n",
      "Training Epoch: 16 [13100/13219]\tLoss: 713.5831\n",
      "Training Epoch: 16 [13150/13219]\tLoss: 690.6948\n",
      "Training Epoch: 16 [13200/13219]\tLoss: 704.0190\n",
      "Training Epoch: 16 [13219/13219]\tLoss: 701.2303\n",
      "Training Epoch: 16 [1469/1469]\tLoss: 703.1480\n",
      "Training Epoch: 17 [50/13219]\tLoss: 693.1552\n",
      "Training Epoch: 17 [100/13219]\tLoss: 706.5313\n",
      "Training Epoch: 17 [150/13219]\tLoss: 691.8223\n",
      "Training Epoch: 17 [200/13219]\tLoss: 705.4438\n",
      "Training Epoch: 17 [250/13219]\tLoss: 720.4901\n",
      "Training Epoch: 17 [300/13219]\tLoss: 755.2985\n",
      "Training Epoch: 17 [350/13219]\tLoss: 690.6045\n",
      "Training Epoch: 17 [400/13219]\tLoss: 747.8070\n",
      "Training Epoch: 17 [450/13219]\tLoss: 706.5850\n",
      "Training Epoch: 17 [500/13219]\tLoss: 718.5117\n",
      "Training Epoch: 17 [550/13219]\tLoss: 713.1249\n",
      "Training Epoch: 17 [600/13219]\tLoss: 682.9006\n",
      "Training Epoch: 17 [650/13219]\tLoss: 691.1470\n",
      "Training Epoch: 17 [700/13219]\tLoss: 656.6766\n",
      "Training Epoch: 17 [750/13219]\tLoss: 693.9351\n",
      "Training Epoch: 17 [800/13219]\tLoss: 705.1761\n",
      "Training Epoch: 17 [850/13219]\tLoss: 738.6238\n",
      "Training Epoch: 17 [900/13219]\tLoss: 722.0330\n",
      "Training Epoch: 17 [950/13219]\tLoss: 703.3190\n",
      "Training Epoch: 17 [1000/13219]\tLoss: 668.8257\n",
      "Training Epoch: 17 [1050/13219]\tLoss: 684.2899\n",
      "Training Epoch: 17 [1100/13219]\tLoss: 703.0739\n",
      "Training Epoch: 17 [1150/13219]\tLoss: 666.0404\n",
      "Training Epoch: 17 [1200/13219]\tLoss: 711.8828\n",
      "Training Epoch: 17 [1250/13219]\tLoss: 714.0883\n",
      "Training Epoch: 17 [1300/13219]\tLoss: 699.7186\n",
      "Training Epoch: 17 [1350/13219]\tLoss: 724.1053\n",
      "Training Epoch: 17 [1400/13219]\tLoss: 718.8219\n",
      "Training Epoch: 17 [1450/13219]\tLoss: 688.4167\n",
      "Training Epoch: 17 [1500/13219]\tLoss: 747.0985\n",
      "Training Epoch: 17 [1550/13219]\tLoss: 713.6642\n",
      "Training Epoch: 17 [1600/13219]\tLoss: 681.9007\n",
      "Training Epoch: 17 [1650/13219]\tLoss: 701.0596\n",
      "Training Epoch: 17 [1700/13219]\tLoss: 706.3001\n",
      "Training Epoch: 17 [1750/13219]\tLoss: 674.0972\n",
      "Training Epoch: 17 [1800/13219]\tLoss: 717.3073\n",
      "Training Epoch: 17 [1850/13219]\tLoss: 700.2252\n",
      "Training Epoch: 17 [1900/13219]\tLoss: 680.3616\n",
      "Training Epoch: 17 [1950/13219]\tLoss: 720.4135\n",
      "Training Epoch: 17 [2000/13219]\tLoss: 695.5731\n",
      "Training Epoch: 17 [2050/13219]\tLoss: 705.8630\n",
      "Training Epoch: 17 [2100/13219]\tLoss: 666.8941\n",
      "Training Epoch: 17 [2150/13219]\tLoss: 702.0019\n",
      "Training Epoch: 17 [2200/13219]\tLoss: 698.2355\n",
      "Training Epoch: 17 [2250/13219]\tLoss: 696.8694\n",
      "Training Epoch: 17 [2300/13219]\tLoss: 652.1369\n",
      "Training Epoch: 17 [2350/13219]\tLoss: 696.3959\n",
      "Training Epoch: 17 [2400/13219]\tLoss: 629.3289\n",
      "Training Epoch: 17 [2450/13219]\tLoss: 706.6080\n",
      "Training Epoch: 17 [2500/13219]\tLoss: 677.2031\n",
      "Training Epoch: 17 [2550/13219]\tLoss: 670.5623\n",
      "Training Epoch: 17 [2600/13219]\tLoss: 701.5339\n",
      "Training Epoch: 17 [2650/13219]\tLoss: 684.9318\n",
      "Training Epoch: 17 [2700/13219]\tLoss: 728.0773\n",
      "Training Epoch: 17 [2750/13219]\tLoss: 707.2639\n",
      "Training Epoch: 17 [2800/13219]\tLoss: 714.3939\n",
      "Training Epoch: 17 [2850/13219]\tLoss: 647.8016\n",
      "Training Epoch: 17 [2900/13219]\tLoss: 657.1080\n",
      "Training Epoch: 17 [2950/13219]\tLoss: 690.1982\n",
      "Training Epoch: 17 [3000/13219]\tLoss: 652.3189\n",
      "Training Epoch: 17 [3050/13219]\tLoss: 678.9836\n",
      "Training Epoch: 17 [3100/13219]\tLoss: 717.0975\n",
      "Training Epoch: 17 [3150/13219]\tLoss: 702.8683\n",
      "Training Epoch: 17 [3200/13219]\tLoss: 702.6555\n",
      "Training Epoch: 17 [3250/13219]\tLoss: 717.6290\n",
      "Training Epoch: 17 [3300/13219]\tLoss: 664.8553\n",
      "Training Epoch: 17 [3350/13219]\tLoss: 703.9055\n",
      "Training Epoch: 17 [3400/13219]\tLoss: 679.2830\n",
      "Training Epoch: 17 [3450/13219]\tLoss: 693.9053\n",
      "Training Epoch: 17 [3500/13219]\tLoss: 727.3513\n",
      "Training Epoch: 17 [3550/13219]\tLoss: 682.5735\n",
      "Training Epoch: 17 [3600/13219]\tLoss: 686.1232\n",
      "Training Epoch: 17 [3650/13219]\tLoss: 681.1623\n",
      "Training Epoch: 17 [3700/13219]\tLoss: 721.1307\n",
      "Training Epoch: 17 [3750/13219]\tLoss: 671.6980\n",
      "Training Epoch: 17 [3800/13219]\tLoss: 700.2493\n",
      "Training Epoch: 17 [3850/13219]\tLoss: 688.5284\n",
      "Training Epoch: 17 [3900/13219]\tLoss: 690.3691\n",
      "Training Epoch: 17 [3950/13219]\tLoss: 679.4410\n",
      "Training Epoch: 17 [4000/13219]\tLoss: 735.8588\n",
      "Training Epoch: 17 [4050/13219]\tLoss: 683.2079\n",
      "Training Epoch: 17 [4100/13219]\tLoss: 695.5336\n",
      "Training Epoch: 17 [4150/13219]\tLoss: 732.0450\n",
      "Training Epoch: 17 [4200/13219]\tLoss: 713.6730\n",
      "Training Epoch: 17 [4250/13219]\tLoss: 702.8774\n",
      "Training Epoch: 17 [4300/13219]\tLoss: 695.1949\n",
      "Training Epoch: 17 [4350/13219]\tLoss: 718.0164\n",
      "Training Epoch: 17 [4400/13219]\tLoss: 750.3354\n",
      "Training Epoch: 17 [4450/13219]\tLoss: 660.7918\n",
      "Training Epoch: 17 [4500/13219]\tLoss: 735.6702\n",
      "Training Epoch: 17 [4550/13219]\tLoss: 711.1230\n",
      "Training Epoch: 17 [4600/13219]\tLoss: 741.0845\n",
      "Training Epoch: 17 [4650/13219]\tLoss: 692.2578\n",
      "Training Epoch: 17 [4700/13219]\tLoss: 678.2488\n",
      "Training Epoch: 17 [4750/13219]\tLoss: 686.8630\n",
      "Training Epoch: 17 [4800/13219]\tLoss: 721.8691\n",
      "Training Epoch: 17 [4850/13219]\tLoss: 734.3842\n",
      "Training Epoch: 17 [4900/13219]\tLoss: 715.1032\n",
      "Training Epoch: 17 [4950/13219]\tLoss: 723.6700\n",
      "Training Epoch: 17 [5000/13219]\tLoss: 736.2208\n",
      "Training Epoch: 17 [5050/13219]\tLoss: 700.7117\n",
      "Training Epoch: 17 [5100/13219]\tLoss: 698.8289\n",
      "Training Epoch: 17 [5150/13219]\tLoss: 641.8814\n",
      "Training Epoch: 17 [5200/13219]\tLoss: 681.5621\n",
      "Training Epoch: 17 [5250/13219]\tLoss: 694.8803\n",
      "Training Epoch: 17 [5300/13219]\tLoss: 680.9722\n",
      "Training Epoch: 17 [5350/13219]\tLoss: 682.3347\n",
      "Training Epoch: 17 [5400/13219]\tLoss: 683.3577\n",
      "Training Epoch: 17 [5450/13219]\tLoss: 740.6002\n",
      "Training Epoch: 17 [5500/13219]\tLoss: 687.1833\n",
      "Training Epoch: 17 [5550/13219]\tLoss: 679.3359\n",
      "Training Epoch: 17 [5600/13219]\tLoss: 682.0740\n",
      "Training Epoch: 17 [5650/13219]\tLoss: 697.1801\n",
      "Training Epoch: 17 [5700/13219]\tLoss: 693.5662\n",
      "Training Epoch: 17 [5750/13219]\tLoss: 723.3715\n",
      "Training Epoch: 17 [5800/13219]\tLoss: 680.8755\n",
      "Training Epoch: 17 [5850/13219]\tLoss: 718.5453\n",
      "Training Epoch: 17 [5900/13219]\tLoss: 687.8188\n",
      "Training Epoch: 17 [5950/13219]\tLoss: 681.6131\n",
      "Training Epoch: 17 [6000/13219]\tLoss: 735.7383\n",
      "Training Epoch: 17 [6050/13219]\tLoss: 704.5290\n",
      "Training Epoch: 17 [6100/13219]\tLoss: 691.6832\n",
      "Training Epoch: 17 [6150/13219]\tLoss: 678.2275\n",
      "Training Epoch: 17 [6200/13219]\tLoss: 674.7495\n",
      "Training Epoch: 17 [6250/13219]\tLoss: 689.9467\n",
      "Training Epoch: 17 [6300/13219]\tLoss: 696.9279\n",
      "Training Epoch: 17 [6350/13219]\tLoss: 677.2299\n",
      "Training Epoch: 17 [6400/13219]\tLoss: 658.1997\n",
      "Training Epoch: 17 [6450/13219]\tLoss: 667.1897\n",
      "Training Epoch: 17 [6500/13219]\tLoss: 688.8357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [6550/13219]\tLoss: 701.6326\n",
      "Training Epoch: 17 [6600/13219]\tLoss: 673.6462\n",
      "Training Epoch: 17 [6650/13219]\tLoss: 695.2227\n",
      "Training Epoch: 17 [6700/13219]\tLoss: 684.0609\n",
      "Training Epoch: 17 [6750/13219]\tLoss: 715.0433\n",
      "Training Epoch: 17 [6800/13219]\tLoss: 685.8969\n",
      "Training Epoch: 17 [6850/13219]\tLoss: 723.5406\n",
      "Training Epoch: 17 [6900/13219]\tLoss: 695.9122\n",
      "Training Epoch: 17 [6950/13219]\tLoss: 667.4089\n",
      "Training Epoch: 17 [7000/13219]\tLoss: 691.5577\n",
      "Training Epoch: 17 [7050/13219]\tLoss: 705.8397\n",
      "Training Epoch: 17 [7100/13219]\tLoss: 701.1441\n",
      "Training Epoch: 17 [7150/13219]\tLoss: 692.4494\n",
      "Training Epoch: 17 [7200/13219]\tLoss: 727.1346\n",
      "Training Epoch: 17 [7250/13219]\tLoss: 703.5995\n",
      "Training Epoch: 17 [7300/13219]\tLoss: 699.8321\n",
      "Training Epoch: 17 [7350/13219]\tLoss: 710.7709\n",
      "Training Epoch: 17 [7400/13219]\tLoss: 687.4626\n",
      "Training Epoch: 17 [7450/13219]\tLoss: 728.2179\n",
      "Training Epoch: 17 [7500/13219]\tLoss: 754.9102\n",
      "Training Epoch: 17 [7550/13219]\tLoss: 694.9301\n",
      "Training Epoch: 17 [7600/13219]\tLoss: 668.5368\n",
      "Training Epoch: 17 [7650/13219]\tLoss: 717.4809\n",
      "Training Epoch: 17 [7700/13219]\tLoss: 662.7536\n",
      "Training Epoch: 17 [7750/13219]\tLoss: 704.2236\n",
      "Training Epoch: 17 [7800/13219]\tLoss: 695.6191\n",
      "Training Epoch: 17 [7850/13219]\tLoss: 699.7404\n",
      "Training Epoch: 17 [7900/13219]\tLoss: 676.8406\n",
      "Training Epoch: 17 [7950/13219]\tLoss: 690.6312\n",
      "Training Epoch: 17 [8000/13219]\tLoss: 673.4640\n",
      "Training Epoch: 17 [8050/13219]\tLoss: 681.0514\n",
      "Training Epoch: 17 [8100/13219]\tLoss: 676.9219\n",
      "Training Epoch: 17 [8150/13219]\tLoss: 706.3716\n",
      "Training Epoch: 17 [8200/13219]\tLoss: 696.4309\n",
      "Training Epoch: 17 [8250/13219]\tLoss: 699.3403\n",
      "Training Epoch: 17 [8300/13219]\tLoss: 717.4995\n",
      "Training Epoch: 17 [8350/13219]\tLoss: 670.1055\n",
      "Training Epoch: 17 [8400/13219]\tLoss: 699.5348\n",
      "Training Epoch: 17 [8450/13219]\tLoss: 696.4819\n",
      "Training Epoch: 17 [8500/13219]\tLoss: 699.7097\n",
      "Training Epoch: 17 [8550/13219]\tLoss: 715.5425\n",
      "Training Epoch: 17 [8600/13219]\tLoss: 704.5496\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
