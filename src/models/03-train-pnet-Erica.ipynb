{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca42f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a44f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6049b",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee63c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_a1 import PBranchedNetwork_A1SeparateHP\n",
    "PNetClass = PBranchedNetwork_A1SeparateHP\n",
    "pnet_name = 'a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1589a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv1 import PBranchedNetwork_Conv1SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv1SeparateHP\n",
    "pnet_name = 'conv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99c2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61c147",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d0d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = '/mnt/smb/locker/abbott-locker/hcnn/checkpoints/'\n",
    "tensorboard_dir = '/mnt/smb/locker/abbott-locker/hcnn/tensorboard/'\n",
    "train_datafile = f'{engram_dir}training_dataset_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4bf8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/smb/locker/issa-locker/users/Erica/training_dataset_random_order.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(train_datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df50c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  6 16:56:37 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:1D:00.0 Off |                  N/A |\r\n",
      "| 55%   48C    P8    22W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d0d78",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89cb275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a471bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7716f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pnet.load_state_dict(torch.load(\n",
    "#    f\"{checkpoints_dir}all/all-25-regular.pth\",\n",
    "#    map_location='cpu'\n",
    "#    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d577eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb23894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73880e46",
   "metadata": {},
   "source": [
    "# Set up TrainSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52604401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4725b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c203620e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class CleanSoundsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Clean sounds dataset from WSJ, but excludes the psychophysics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hdf_file, subset = None, train = True):\n",
    "        self.hdf_file = hdf_file\n",
    "        self.train = train\n",
    "        self.f = h5py.File(hdf_file, 'r')\n",
    "        self.n_data, __ =  np.shape(self.f['data'])\n",
    "        \n",
    "        if subset is not None:\n",
    "            \n",
    "            if train: \n",
    "                \n",
    "                self.n_data = int(self.n_data*subset)\n",
    "            else:\n",
    "                self.n_data = int(self.n_data * (1-subset))\n",
    "                self.start_ind = int(self.n_data * subset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if not self.train:\n",
    "            idx = idx + self.start_ind # Add the off set of the start of the test set \n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return torch.tensor(np.array(self.f['data'][idx]).reshape((-1, 164, 400))), torch.tensor(self.f['labels'][idx])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573fcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c9b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7265fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CleanSoundsDataset(train_datafile, .9)\n",
    "test_dataset = CleanSoundsDataset(train_datafile, .9, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af8aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a0fde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ea683",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30421964",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d03df8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4cdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/49669]\tLoss: 0.6606\n",
      "Training Epoch: 1 [100/49669]\tLoss: 0.4528\n",
      "Training Epoch: 1 [150/49669]\tLoss: 0.4229\n",
      "Training Epoch: 1 [200/49669]\tLoss: 0.1998\n",
      "Training Epoch: 1 [250/49669]\tLoss: 0.1939\n",
      "Training Epoch: 1 [300/49669]\tLoss: 0.2723\n",
      "Training Epoch: 1 [350/49669]\tLoss: 0.2518\n",
      "Training Epoch: 1 [400/49669]\tLoss: 0.1381\n",
      "Training Epoch: 1 [450/49669]\tLoss: 0.1299\n",
      "Training Epoch: 1 [500/49669]\tLoss: 0.1874\n",
      "Training Epoch: 1 [550/49669]\tLoss: 0.1854\n",
      "Training Epoch: 1 [600/49669]\tLoss: 0.1295\n",
      "Training Epoch: 1 [650/49669]\tLoss: 0.0914\n",
      "Training Epoch: 1 [700/49669]\tLoss: 0.1107\n",
      "Training Epoch: 1 [750/49669]\tLoss: 0.1349\n",
      "Training Epoch: 1 [800/49669]\tLoss: 0.1148\n",
      "Training Epoch: 1 [850/49669]\tLoss: 0.0888\n",
      "Training Epoch: 1 [900/49669]\tLoss: 0.0717\n",
      "Training Epoch: 1 [950/49669]\tLoss: 0.0886\n",
      "Training Epoch: 1 [1000/49669]\tLoss: 0.1003\n",
      "Training Epoch: 1 [1050/49669]\tLoss: 0.0826\n",
      "Training Epoch: 1 [1100/49669]\tLoss: 0.0616\n",
      "Training Epoch: 1 [1150/49669]\tLoss: 0.0612\n",
      "Training Epoch: 1 [1200/49669]\tLoss: 0.0765\n",
      "Training Epoch: 1 [1250/49669]\tLoss: 0.0752\n",
      "Training Epoch: 1 [1300/49669]\tLoss: 0.0603\n",
      "Training Epoch: 1 [1350/49669]\tLoss: 0.0492\n",
      "Training Epoch: 1 [1400/49669]\tLoss: 0.0546\n",
      "Training Epoch: 1 [1450/49669]\tLoss: 0.0616\n",
      "Training Epoch: 1 [1500/49669]\tLoss: 0.0552\n",
      "Training Epoch: 1 [1550/49669]\tLoss: 0.0458\n",
      "Training Epoch: 1 [1600/49669]\tLoss: 0.0453\n",
      "Training Epoch: 1 [1650/49669]\tLoss: 0.0481\n",
      "Training Epoch: 1 [1700/49669]\tLoss: 0.0484\n",
      "Training Epoch: 1 [1750/49669]\tLoss: 0.0417\n",
      "Training Epoch: 1 [1800/49669]\tLoss: 0.0390\n",
      "Training Epoch: 1 [1850/49669]\tLoss: 0.0412\n",
      "Training Epoch: 1 [1900/49669]\tLoss: 0.0421\n",
      "Training Epoch: 1 [1950/49669]\tLoss: 0.0407\n",
      "Training Epoch: 1 [2000/49669]\tLoss: 0.0354\n",
      "Training Epoch: 1 [2050/49669]\tLoss: 0.0363\n",
      "Training Epoch: 1 [2100/49669]\tLoss: 0.0373\n",
      "Training Epoch: 1 [2150/49669]\tLoss: 0.0360\n",
      "Training Epoch: 1 [2200/49669]\tLoss: 0.0329\n",
      "Training Epoch: 1 [2250/49669]\tLoss: 0.0331\n",
      "Training Epoch: 1 [2300/49669]\tLoss: 0.0325\n",
      "Training Epoch: 1 [2350/49669]\tLoss: 0.0328\n",
      "Training Epoch: 1 [2400/49669]\tLoss: 0.0309\n",
      "Training Epoch: 1 [2450/49669]\tLoss: 0.0292\n",
      "Training Epoch: 1 [2500/49669]\tLoss: 0.0304\n",
      "Training Epoch: 1 [2550/49669]\tLoss: 0.0309\n",
      "Training Epoch: 1 [2600/49669]\tLoss: 0.0295\n",
      "Training Epoch: 1 [2650/49669]\tLoss: 0.0286\n",
      "Training Epoch: 1 [2700/49669]\tLoss: 0.0291\n",
      "Training Epoch: 1 [2750/49669]\tLoss: 0.0294\n",
      "Training Epoch: 1 [2800/49669]\tLoss: 0.0277\n",
      "Training Epoch: 1 [2850/49669]\tLoss: 0.0261\n",
      "Training Epoch: 1 [2900/49669]\tLoss: 0.0273\n",
      "Training Epoch: 1 [2950/49669]\tLoss: 0.0262\n",
      "Training Epoch: 1 [3000/49669]\tLoss: 0.0256\n",
      "Training Epoch: 1 [3050/49669]\tLoss: 0.0250\n",
      "Training Epoch: 1 [3100/49669]\tLoss: 0.0265\n",
      "Training Epoch: 1 [3150/49669]\tLoss: 0.0251\n",
      "Training Epoch: 1 [3200/49669]\tLoss: 0.0241\n",
      "Training Epoch: 1 [3250/49669]\tLoss: 0.0242\n",
      "Training Epoch: 1 [3300/49669]\tLoss: 0.0240\n",
      "Training Epoch: 1 [3350/49669]\tLoss: 0.0240\n",
      "Training Epoch: 1 [3400/49669]\tLoss: 0.0228\n",
      "Training Epoch: 1 [3450/49669]\tLoss: 0.0229\n",
      "Training Epoch: 1 [3500/49669]\tLoss: 0.0227\n",
      "Training Epoch: 1 [3550/49669]\tLoss: 0.0229\n",
      "Training Epoch: 1 [3600/49669]\tLoss: 0.0233\n",
      "Training Epoch: 1 [3650/49669]\tLoss: 0.0228\n",
      "Training Epoch: 1 [3700/49669]\tLoss: 0.0224\n",
      "Training Epoch: 1 [3750/49669]\tLoss: 0.0225\n",
      "Training Epoch: 1 [3800/49669]\tLoss: 0.0214\n",
      "Training Epoch: 1 [3850/49669]\tLoss: 0.0215\n",
      "Training Epoch: 1 [3900/49669]\tLoss: 0.0208\n",
      "Training Epoch: 1 [3950/49669]\tLoss: 0.0203\n",
      "Training Epoch: 1 [4000/49669]\tLoss: 0.0219\n",
      "Training Epoch: 1 [4050/49669]\tLoss: 0.0202\n",
      "Training Epoch: 1 [4100/49669]\tLoss: 0.0206\n",
      "Training Epoch: 1 [4150/49669]\tLoss: 0.0202\n",
      "Training Epoch: 1 [4200/49669]\tLoss: 0.0201\n",
      "Training Epoch: 1 [4250/49669]\tLoss: 0.0199\n",
      "Training Epoch: 1 [4300/49669]\tLoss: 0.0189\n",
      "Training Epoch: 1 [4350/49669]\tLoss: 0.0194\n",
      "Training Epoch: 1 [4400/49669]\tLoss: 0.0190\n",
      "Training Epoch: 1 [4450/49669]\tLoss: 0.0193\n",
      "Training Epoch: 1 [4500/49669]\tLoss: 0.0193\n",
      "Training Epoch: 1 [4550/49669]\tLoss: 0.0195\n",
      "Training Epoch: 1 [4600/49669]\tLoss: 0.0183\n",
      "Training Epoch: 1 [4650/49669]\tLoss: 0.0189\n",
      "Training Epoch: 1 [4700/49669]\tLoss: 0.0187\n",
      "Training Epoch: 1 [4750/49669]\tLoss: 0.0184\n",
      "Training Epoch: 1 [4800/49669]\tLoss: 0.0180\n",
      "Training Epoch: 1 [4850/49669]\tLoss: 0.0176\n",
      "Training Epoch: 1 [4900/49669]\tLoss: 0.0180\n",
      "Training Epoch: 1 [4950/49669]\tLoss: 0.0173\n",
      "Training Epoch: 1 [5000/49669]\tLoss: 0.0178\n",
      "Training Epoch: 1 [5050/49669]\tLoss: 0.0176\n",
      "Training Epoch: 1 [5100/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [5150/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [5200/49669]\tLoss: 0.0166\n",
      "Training Epoch: 1 [5250/49669]\tLoss: 0.0170\n",
      "Training Epoch: 1 [5300/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [5350/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [5400/49669]\tLoss: 0.0175\n",
      "Training Epoch: 1 [5450/49669]\tLoss: 0.0167\n",
      "Training Epoch: 1 [5500/49669]\tLoss: 0.0168\n",
      "Training Epoch: 1 [5550/49669]\tLoss: 0.0166\n",
      "Training Epoch: 1 [5600/49669]\tLoss: 0.0165\n",
      "Training Epoch: 1 [5650/49669]\tLoss: 0.0158\n",
      "Training Epoch: 1 [5700/49669]\tLoss: 0.0162\n",
      "Training Epoch: 1 [5750/49669]\tLoss: 0.0159\n",
      "Training Epoch: 1 [5800/49669]\tLoss: 0.0161\n",
      "Training Epoch: 1 [5850/49669]\tLoss: 0.0163\n",
      "Training Epoch: 1 [5900/49669]\tLoss: 0.0157\n",
      "Training Epoch: 1 [5950/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6000/49669]\tLoss: 0.0150\n",
      "Training Epoch: 1 [6050/49669]\tLoss: 0.0152\n",
      "Training Epoch: 1 [6100/49669]\tLoss: 0.0152\n",
      "Training Epoch: 1 [6150/49669]\tLoss: 0.0150\n",
      "Training Epoch: 1 [6200/49669]\tLoss: 0.0148\n",
      "Training Epoch: 1 [6250/49669]\tLoss: 0.0162\n",
      "Training Epoch: 1 [6300/49669]\tLoss: 0.0150\n",
      "Training Epoch: 1 [6350/49669]\tLoss: 0.0149\n",
      "Training Epoch: 1 [6400/49669]\tLoss: 0.0146\n",
      "Training Epoch: 1 [6450/49669]\tLoss: 0.0145\n",
      "Training Epoch: 1 [6500/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [6550/49669]\tLoss: 0.0140\n",
      "Training Epoch: 1 [6600/49669]\tLoss: 0.0147\n",
      "Training Epoch: 1 [6650/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [6700/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [6750/49669]\tLoss: 0.0146\n",
      "Training Epoch: 1 [6800/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [6850/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [6900/49669]\tLoss: 0.0146\n",
      "Training Epoch: 1 [6950/49669]\tLoss: 0.0140\n",
      "Training Epoch: 1 [7000/49669]\tLoss: 0.0144\n",
      "Training Epoch: 1 [7050/49669]\tLoss: 0.0138\n",
      "Training Epoch: 1 [7100/49669]\tLoss: 0.0138\n",
      "Training Epoch: 1 [7150/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [7200/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [7250/49669]\tLoss: 0.0135\n",
      "Training Epoch: 1 [7300/49669]\tLoss: 0.0134\n",
      "Training Epoch: 1 [7350/49669]\tLoss: 0.0135\n",
      "Training Epoch: 1 [7400/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [7450/49669]\tLoss: 0.0133\n",
      "Training Epoch: 1 [7500/49669]\tLoss: 0.0130\n",
      "Training Epoch: 1 [7550/49669]\tLoss: 0.0133\n",
      "Training Epoch: 1 [7600/49669]\tLoss: 0.0128\n",
      "Training Epoch: 1 [7650/49669]\tLoss: 0.0132\n",
      "Training Epoch: 1 [7700/49669]\tLoss: 0.0127\n",
      "Training Epoch: 1 [7750/49669]\tLoss: 0.0132\n",
      "Training Epoch: 1 [7800/49669]\tLoss: 0.0132\n",
      "Training Epoch: 1 [7850/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [7900/49669]\tLoss: 0.0128\n",
      "Training Epoch: 1 [7950/49669]\tLoss: 0.0124\n",
      "Training Epoch: 1 [8000/49669]\tLoss: 0.0128\n",
      "Training Epoch: 1 [8050/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [8100/49669]\tLoss: 0.0130\n",
      "Training Epoch: 1 [8150/49669]\tLoss: 0.0127\n",
      "Training Epoch: 1 [8200/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [8250/49669]\tLoss: 0.0119\n",
      "Training Epoch: 1 [8300/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [8350/49669]\tLoss: 0.0123\n",
      "Training Epoch: 1 [8400/49669]\tLoss: 0.0122\n",
      "Training Epoch: 1 [8450/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [8500/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [8550/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [8600/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [8650/49669]\tLoss: 0.0116\n",
      "Training Epoch: 1 [8700/49669]\tLoss: 0.0122\n",
      "Training Epoch: 1 [8750/49669]\tLoss: 0.0123\n",
      "Training Epoch: 1 [8800/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [8850/49669]\tLoss: 0.0120\n",
      "Training Epoch: 1 [8900/49669]\tLoss: 0.0115\n",
      "Training Epoch: 1 [8950/49669]\tLoss: 0.0115\n",
      "Training Epoch: 1 [9000/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [9050/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [9100/49669]\tLoss: 0.0116\n",
      "Training Epoch: 1 [9150/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [9200/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [9250/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [9300/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [9350/49669]\tLoss: 0.0113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [9400/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [9450/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [9500/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [9550/49669]\tLoss: 0.0106\n",
      "Training Epoch: 1 [9600/49669]\tLoss: 0.0111\n",
      "Training Epoch: 1 [9650/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [9700/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [9750/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [9800/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [9850/49669]\tLoss: 0.0106\n",
      "Training Epoch: 1 [9900/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [9950/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [10000/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [10050/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [10100/49669]\tLoss: 0.0111\n",
      "Training Epoch: 1 [10150/49669]\tLoss: 0.0105\n",
      "Training Epoch: 1 [10200/49669]\tLoss: 0.0106\n",
      "Training Epoch: 1 [10250/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [10300/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [10350/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [10400/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [10450/49669]\tLoss: 0.0106\n",
      "Training Epoch: 1 [10500/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [10550/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [10600/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [10650/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [10700/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [10750/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [10800/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [10850/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [10900/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [10950/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [11000/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [11050/49669]\tLoss: 0.0105\n",
      "Training Epoch: 1 [11100/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [11150/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [11200/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [11250/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [11300/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [11350/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [11400/49669]\tLoss: 0.0095\n",
      "Training Epoch: 1 [11450/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [11500/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [11550/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [11600/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [11650/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [11700/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [11750/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [11800/49669]\tLoss: 0.0095\n",
      "Training Epoch: 1 [11850/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [11900/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [11950/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [12000/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [12050/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [12100/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [12150/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [12200/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [12250/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [12300/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [12350/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [12400/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [12450/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [12500/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [12550/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [12600/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [12650/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [12700/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [12750/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [12800/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [12850/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [12900/49669]\tLoss: 0.0095\n",
      "Training Epoch: 1 [12950/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [13000/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [13050/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [13100/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [13150/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [13200/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [13250/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [13300/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [13350/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [13400/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [13450/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [13500/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [13550/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [13600/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [13650/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [13700/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [13750/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [13800/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [13850/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [13900/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [13950/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [14000/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [14050/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [14100/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [14150/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [14200/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [14250/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [14300/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [14350/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [14400/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [14450/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [14500/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [14550/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [14600/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [14650/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [14700/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [14750/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [14800/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [14850/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [14900/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [14950/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15000/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [15050/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [15100/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15150/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [15200/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15250/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [15300/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [15350/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [15400/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [15450/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15500/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15550/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [15600/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [15650/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [15700/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15750/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [15800/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [15850/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [15900/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [15950/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [16000/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [16050/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [16100/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [16150/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [16200/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [16250/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [16300/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [16350/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [16400/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [16450/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [16500/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [16550/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [16600/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [16650/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [16700/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [16750/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [16800/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [16850/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [16900/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [16950/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [17000/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [17050/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [17100/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [17150/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [17200/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17250/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17300/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [17350/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17400/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17450/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [17500/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [17550/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [17600/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [17650/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [17700/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [17750/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [17800/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17850/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17900/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [17950/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [18000/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [18050/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [18100/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [18150/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [18200/49669]\tLoss: 0.0074\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a3d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757005f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
