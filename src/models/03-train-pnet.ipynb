{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8efd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a437651",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfdc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}training_dataset_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9766a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  6 18:07:34 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:60:00.0 Off |                  N/A |\r\n",
      "|  0%   25C    P8     1W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d748a",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b087e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae18933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6839214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23a4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779893ac",
   "metadata": {},
   "source": [
    "# Set up train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc69b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CleanSoundsDataset(train_datafile, .9)\n",
    "test_dataset = CleanSoundsDataset(train_datafile, .9, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a14c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026ae3a",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b449fc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/49669]\tLoss: 0.4564\n",
      "Training Epoch: 1 [100/49669]\tLoss: 0.6180\n",
      "Training Epoch: 1 [150/49669]\tLoss: 0.2616\n",
      "Training Epoch: 1 [200/49669]\tLoss: 0.2126\n",
      "Training Epoch: 1 [250/49669]\tLoss: 0.3171\n",
      "Training Epoch: 1 [300/49669]\tLoss: 0.2529\n",
      "Training Epoch: 1 [350/49669]\tLoss: 0.1540\n",
      "Training Epoch: 1 [400/49669]\tLoss: 0.1414\n",
      "Training Epoch: 1 [450/49669]\tLoss: 0.1964\n",
      "Training Epoch: 1 [500/49669]\tLoss: 0.2026\n",
      "Training Epoch: 1 [550/49669]\tLoss: 0.1363\n",
      "Training Epoch: 1 [600/49669]\tLoss: 0.0996\n",
      "Training Epoch: 1 [650/49669]\tLoss: 0.1165\n",
      "Training Epoch: 1 [700/49669]\tLoss: 0.1430\n",
      "Training Epoch: 1 [750/49669]\tLoss: 0.1281\n",
      "Training Epoch: 1 [800/49669]\tLoss: 0.0880\n",
      "Training Epoch: 1 [850/49669]\tLoss: 0.0811\n",
      "Training Epoch: 1 [900/49669]\tLoss: 0.0898\n",
      "Training Epoch: 1 [950/49669]\tLoss: 0.1011\n",
      "Training Epoch: 1 [1000/49669]\tLoss: 0.0921\n",
      "Training Epoch: 1 [1050/49669]\tLoss: 0.0689\n",
      "Training Epoch: 1 [1100/49669]\tLoss: 0.0621\n",
      "Training Epoch: 1 [1150/49669]\tLoss: 0.0736\n",
      "Training Epoch: 1 [1200/49669]\tLoss: 0.0808\n",
      "Training Epoch: 1 [1250/49669]\tLoss: 0.0683\n",
      "Training Epoch: 1 [1300/49669]\tLoss: 0.0555\n",
      "Training Epoch: 1 [1350/49669]\tLoss: 0.0542\n",
      "Training Epoch: 1 [1400/49669]\tLoss: 0.0640\n",
      "Training Epoch: 1 [1450/49669]\tLoss: 0.0621\n",
      "Training Epoch: 1 [1500/49669]\tLoss: 0.0534\n",
      "Training Epoch: 1 [1550/49669]\tLoss: 0.0472\n",
      "Training Epoch: 1 [1600/49669]\tLoss: 0.0522\n",
      "Training Epoch: 1 [1650/49669]\tLoss: 0.0530\n",
      "Training Epoch: 1 [1700/49669]\tLoss: 0.0490\n",
      "Training Epoch: 1 [1750/49669]\tLoss: 0.0419\n",
      "Training Epoch: 1 [1800/49669]\tLoss: 0.0427\n",
      "Training Epoch: 1 [1850/49669]\tLoss: 0.0481\n",
      "Training Epoch: 1 [1900/49669]\tLoss: 0.0445\n",
      "Training Epoch: 1 [1950/49669]\tLoss: 0.0416\n",
      "Training Epoch: 1 [2000/49669]\tLoss: 0.0372\n",
      "Training Epoch: 1 [2050/49669]\tLoss: 0.0404\n",
      "Training Epoch: 1 [2100/49669]\tLoss: 0.0404\n",
      "Training Epoch: 1 [2150/49669]\tLoss: 0.0378\n",
      "Training Epoch: 1 [2200/49669]\tLoss: 0.0347\n",
      "Training Epoch: 1 [2250/49669]\tLoss: 0.0374\n",
      "Training Epoch: 1 [2300/49669]\tLoss: 0.0344\n",
      "Training Epoch: 1 [2350/49669]\tLoss: 0.0340\n",
      "Training Epoch: 1 [2400/49669]\tLoss: 0.0330\n",
      "Training Epoch: 1 [2450/49669]\tLoss: 0.0313\n",
      "Training Epoch: 1 [2500/49669]\tLoss: 0.0332\n",
      "Training Epoch: 1 [2550/49669]\tLoss: 0.0338\n",
      "Training Epoch: 1 [2600/49669]\tLoss: 0.0316\n",
      "Training Epoch: 1 [2650/49669]\tLoss: 0.0315\n",
      "Training Epoch: 1 [2700/49669]\tLoss: 0.0320\n",
      "Training Epoch: 1 [2750/49669]\tLoss: 0.0323\n",
      "Training Epoch: 1 [2800/49669]\tLoss: 0.0306\n",
      "Training Epoch: 1 [2850/49669]\tLoss: 0.0287\n",
      "Training Epoch: 1 [2900/49669]\tLoss: 0.0299\n",
      "Training Epoch: 1 [2950/49669]\tLoss: 0.0285\n",
      "Training Epoch: 1 [3000/49669]\tLoss: 0.0282\n",
      "Training Epoch: 1 [3050/49669]\tLoss: 0.0271\n",
      "Training Epoch: 1 [3100/49669]\tLoss: 0.0299\n",
      "Training Epoch: 1 [3150/49669]\tLoss: 0.0267\n",
      "Training Epoch: 1 [3200/49669]\tLoss: 0.0258\n",
      "Training Epoch: 1 [3250/49669]\tLoss: 0.0261\n",
      "Training Epoch: 1 [3300/49669]\tLoss: 0.0259\n",
      "Training Epoch: 1 [3350/49669]\tLoss: 0.0262\n",
      "Training Epoch: 1 [3400/49669]\tLoss: 0.0250\n",
      "Training Epoch: 1 [3450/49669]\tLoss: 0.0253\n",
      "Training Epoch: 1 [3500/49669]\tLoss: 0.0238\n",
      "Training Epoch: 1 [3550/49669]\tLoss: 0.0252\n",
      "Training Epoch: 1 [3600/49669]\tLoss: 0.0254\n",
      "Training Epoch: 1 [3650/49669]\tLoss: 0.0251\n",
      "Training Epoch: 1 [3700/49669]\tLoss: 0.0251\n",
      "Training Epoch: 1 [3750/49669]\tLoss: 0.0253\n",
      "Training Epoch: 1 [3800/49669]\tLoss: 0.0240\n",
      "Training Epoch: 1 [3850/49669]\tLoss: 0.0230\n",
      "Training Epoch: 1 [3900/49669]\tLoss: 0.0229\n",
      "Training Epoch: 1 [3950/49669]\tLoss: 0.0220\n",
      "Training Epoch: 1 [4000/49669]\tLoss: 0.0245\n",
      "Training Epoch: 1 [4050/49669]\tLoss: 0.0219\n",
      "Training Epoch: 1 [4100/49669]\tLoss: 0.0239\n",
      "Training Epoch: 1 [4150/49669]\tLoss: 0.0230\n",
      "Training Epoch: 1 [4200/49669]\tLoss: 0.0210\n",
      "Training Epoch: 1 [4250/49669]\tLoss: 0.0214\n",
      "Training Epoch: 1 [4300/49669]\tLoss: 0.0210\n",
      "Training Epoch: 1 [4350/49669]\tLoss: 0.0211\n",
      "Training Epoch: 1 [4400/49669]\tLoss: 0.0210\n",
      "Training Epoch: 1 [4450/49669]\tLoss: 0.0211\n",
      "Training Epoch: 1 [4500/49669]\tLoss: 0.0210\n",
      "Training Epoch: 1 [4550/49669]\tLoss: 0.0217\n",
      "Training Epoch: 1 [4600/49669]\tLoss: 0.0203\n",
      "Training Epoch: 1 [4650/49669]\tLoss: 0.0214\n",
      "Training Epoch: 1 [4700/49669]\tLoss: 0.0210\n",
      "Training Epoch: 1 [4750/49669]\tLoss: 0.0210\n",
      "Training Epoch: 1 [4800/49669]\tLoss: 0.0199\n",
      "Training Epoch: 1 [4850/49669]\tLoss: 0.0196\n",
      "Training Epoch: 1 [4900/49669]\tLoss: 0.0204\n",
      "Training Epoch: 1 [4950/49669]\tLoss: 0.0194\n",
      "Training Epoch: 1 [5000/49669]\tLoss: 0.0204\n",
      "Training Epoch: 1 [5050/49669]\tLoss: 0.0196\n",
      "Training Epoch: 1 [5100/49669]\tLoss: 0.0196\n",
      "Training Epoch: 1 [5150/49669]\tLoss: 0.0194\n",
      "Training Epoch: 1 [5200/49669]\tLoss: 0.0181\n",
      "Training Epoch: 1 [5250/49669]\tLoss: 0.0196\n",
      "Training Epoch: 1 [5300/49669]\tLoss: 0.0186\n",
      "Training Epoch: 1 [5350/49669]\tLoss: 0.0192\n",
      "Training Epoch: 1 [5400/49669]\tLoss: 0.0198\n",
      "Training Epoch: 1 [5450/49669]\tLoss: 0.0181\n",
      "Training Epoch: 1 [5500/49669]\tLoss: 0.0179\n",
      "Training Epoch: 1 [5550/49669]\tLoss: 0.0186\n",
      "Training Epoch: 1 [5600/49669]\tLoss: 0.0184\n",
      "Training Epoch: 1 [5650/49669]\tLoss: 0.0174\n",
      "Training Epoch: 1 [5700/49669]\tLoss: 0.0185\n",
      "Training Epoch: 1 [5750/49669]\tLoss: 0.0181\n",
      "Training Epoch: 1 [5800/49669]\tLoss: 0.0183\n",
      "Training Epoch: 1 [5850/49669]\tLoss: 0.0182\n",
      "Training Epoch: 1 [5900/49669]\tLoss: 0.0173\n",
      "Training Epoch: 1 [5950/49669]\tLoss: 0.0197\n",
      "Training Epoch: 1 [6000/49669]\tLoss: 0.0167\n",
      "Training Epoch: 1 [6050/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [6100/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6150/49669]\tLoss: 0.0166\n",
      "Training Epoch: 1 [6200/49669]\tLoss: 0.0162\n",
      "Training Epoch: 1 [6250/49669]\tLoss: 0.0190\n",
      "Training Epoch: 1 [6300/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6350/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6400/49669]\tLoss: 0.0167\n",
      "Training Epoch: 1 [6450/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [6500/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6550/49669]\tLoss: 0.0165\n",
      "Training Epoch: 1 [6600/49669]\tLoss: 0.0186\n",
      "Training Epoch: 1 [6650/49669]\tLoss: 0.0174\n",
      "Training Epoch: 1 [6700/49669]\tLoss: 0.0173\n",
      "Training Epoch: 1 [6750/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [6800/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6850/49669]\tLoss: 0.0169\n",
      "Training Epoch: 1 [6900/49669]\tLoss: 0.0177\n",
      "Training Epoch: 1 [6950/49669]\tLoss: 0.0160\n",
      "Training Epoch: 1 [7000/49669]\tLoss: 0.0166\n",
      "Training Epoch: 1 [7050/49669]\tLoss: 0.0163\n",
      "Training Epoch: 1 [7100/49669]\tLoss: 0.0163\n",
      "Training Epoch: 1 [7150/49669]\tLoss: 0.0145\n",
      "Training Epoch: 1 [7200/49669]\tLoss: 0.0171\n",
      "Training Epoch: 1 [7250/49669]\tLoss: 0.0153\n",
      "Training Epoch: 1 [7300/49669]\tLoss: 0.0154\n",
      "Training Epoch: 1 [7350/49669]\tLoss: 0.0157\n",
      "Training Epoch: 1 [7400/49669]\tLoss: 0.0145\n",
      "Training Epoch: 1 [7450/49669]\tLoss: 0.0158\n",
      "Training Epoch: 1 [7500/49669]\tLoss: 0.0148\n",
      "Training Epoch: 1 [7550/49669]\tLoss: 0.0158\n",
      "Training Epoch: 1 [7600/49669]\tLoss: 0.0152\n",
      "Training Epoch: 1 [7650/49669]\tLoss: 0.0154\n",
      "Training Epoch: 1 [7700/49669]\tLoss: 0.0146\n",
      "Training Epoch: 1 [7750/49669]\tLoss: 0.0158\n",
      "Training Epoch: 1 [7800/49669]\tLoss: 0.0152\n",
      "Training Epoch: 1 [7850/49669]\tLoss: 0.0157\n",
      "Training Epoch: 1 [7900/49669]\tLoss: 0.0153\n",
      "Training Epoch: 1 [7950/49669]\tLoss: 0.0142\n",
      "Training Epoch: 1 [8000/49669]\tLoss: 0.0148\n",
      "Training Epoch: 1 [8050/49669]\tLoss: 0.0145\n",
      "Training Epoch: 1 [8100/49669]\tLoss: 0.0158\n",
      "Training Epoch: 1 [8150/49669]\tLoss: 0.0153\n",
      "Training Epoch: 1 [8200/49669]\tLoss: 0.0149\n",
      "Training Epoch: 1 [8250/49669]\tLoss: 0.0138\n",
      "Training Epoch: 1 [8300/49669]\tLoss: 0.0152\n",
      "Training Epoch: 1 [8350/49669]\tLoss: 0.0147\n",
      "Training Epoch: 1 [8400/49669]\tLoss: 0.0148\n",
      "Training Epoch: 1 [8450/49669]\tLoss: 0.0145\n",
      "Training Epoch: 1 [8500/49669]\tLoss: 0.0130\n",
      "Training Epoch: 1 [8550/49669]\tLoss: 0.0147\n",
      "Training Epoch: 1 [8600/49669]\tLoss: 0.0141\n",
      "Training Epoch: 1 [8650/49669]\tLoss: 0.0133\n",
      "Training Epoch: 1 [8700/49669]\tLoss: 0.0151\n",
      "Training Epoch: 1 [8750/49669]\tLoss: 0.0151\n",
      "Training Epoch: 1 [8800/49669]\tLoss: 0.0130\n",
      "Training Epoch: 1 [8850/49669]\tLoss: 0.0145\n",
      "Training Epoch: 1 [8900/49669]\tLoss: 0.0139\n",
      "Training Epoch: 1 [8950/49669]\tLoss: 0.0135\n",
      "Training Epoch: 1 [9000/49669]\tLoss: 0.0126\n",
      "Training Epoch: 1 [9050/49669]\tLoss: 0.0127\n",
      "Training Epoch: 1 [9100/49669]\tLoss: 0.0137\n",
      "Training Epoch: 1 [9150/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [9200/49669]\tLoss: 0.0124\n",
      "Training Epoch: 1 [9250/49669]\tLoss: 0.0141\n",
      "Training Epoch: 1 [9300/49669]\tLoss: 0.0139\n",
      "Training Epoch: 1 [9350/49669]\tLoss: 0.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [9400/49669]\tLoss: 0.0138\n",
      "Training Epoch: 1 [9450/49669]\tLoss: 0.0143\n",
      "Training Epoch: 1 [9500/49669]\tLoss: 0.0129\n",
      "Training Epoch: 1 [9550/49669]\tLoss: 0.0122\n",
      "Training Epoch: 1 [9600/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [9650/49669]\tLoss: 0.0132\n",
      "Training Epoch: 1 [9700/49669]\tLoss: 0.0138\n",
      "Training Epoch: 1 [9750/49669]\tLoss: 0.0133\n",
      "Training Epoch: 1 [9800/49669]\tLoss: 0.0136\n",
      "Training Epoch: 1 [9850/49669]\tLoss: 0.0129\n",
      "Training Epoch: 1 [9900/49669]\tLoss: 0.0123\n",
      "Training Epoch: 1 [9950/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [10000/49669]\tLoss: 0.0133\n",
      "Training Epoch: 1 [10050/49669]\tLoss: 0.0137\n",
      "Training Epoch: 1 [10100/49669]\tLoss: 0.0132\n",
      "Training Epoch: 1 [10150/49669]\tLoss: 0.0124\n",
      "Training Epoch: 1 [10200/49669]\tLoss: 0.0130\n",
      "Training Epoch: 1 [10250/49669]\tLoss: 0.0134\n",
      "Training Epoch: 1 [10300/49669]\tLoss: 0.0132\n",
      "Training Epoch: 1 [10350/49669]\tLoss: 0.0118\n",
      "Training Epoch: 1 [10400/49669]\tLoss: 0.0138\n",
      "Training Epoch: 1 [10450/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [10500/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [10550/49669]\tLoss: 0.0119\n",
      "Training Epoch: 1 [10600/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [10650/49669]\tLoss: 0.0126\n",
      "Training Epoch: 1 [10700/49669]\tLoss: 0.0128\n",
      "Training Epoch: 1 [10750/49669]\tLoss: 0.0128\n",
      "Training Epoch: 1 [10800/49669]\tLoss: 0.0119\n",
      "Training Epoch: 1 [10850/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [10900/49669]\tLoss: 0.0119\n",
      "Training Epoch: 1 [10950/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [11000/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [11050/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [11100/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [11150/49669]\tLoss: 0.0123\n",
      "Training Epoch: 1 [11200/49669]\tLoss: 0.0124\n",
      "Training Epoch: 1 [11250/49669]\tLoss: 0.0127\n",
      "Training Epoch: 1 [11300/49669]\tLoss: 0.0131\n",
      "Training Epoch: 1 [11350/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [11400/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [11450/49669]\tLoss: 0.0127\n",
      "Training Epoch: 1 [11500/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [11550/49669]\tLoss: 0.0118\n",
      "Training Epoch: 1 [11600/49669]\tLoss: 0.0120\n",
      "Training Epoch: 1 [11650/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [11700/49669]\tLoss: 0.0127\n",
      "Training Epoch: 1 [11750/49669]\tLoss: 0.0125\n",
      "Training Epoch: 1 [11800/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [11850/49669]\tLoss: 0.0118\n",
      "Training Epoch: 1 [11900/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [11950/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [12000/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [12050/49669]\tLoss: 0.0119\n",
      "Training Epoch: 1 [12100/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [12150/49669]\tLoss: 0.0115\n",
      "Training Epoch: 1 [12200/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [12250/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [12300/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [12350/49669]\tLoss: 0.0130\n",
      "Training Epoch: 1 [12400/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [12450/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [12500/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [12550/49669]\tLoss: 0.0115\n",
      "Training Epoch: 1 [12600/49669]\tLoss: 0.0114\n",
      "Training Epoch: 1 [12650/49669]\tLoss: 0.0116\n",
      "Training Epoch: 1 [12700/49669]\tLoss: 0.0126\n",
      "Training Epoch: 1 [12750/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [12800/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [12850/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [12900/49669]\tLoss: 0.0120\n",
      "Training Epoch: 1 [12950/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [13000/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [13050/49669]\tLoss: 0.0115\n",
      "Training Epoch: 1 [13100/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [13150/49669]\tLoss: 0.0124\n",
      "Training Epoch: 1 [13200/49669]\tLoss: 0.0116\n",
      "Training Epoch: 1 [13250/49669]\tLoss: 0.0121\n",
      "Training Epoch: 1 [13300/49669]\tLoss: 0.0123\n",
      "Training Epoch: 1 [13350/49669]\tLoss: 0.0111\n",
      "Training Epoch: 1 [13400/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [13450/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [13500/49669]\tLoss: 0.0116\n",
      "Training Epoch: 1 [13550/49669]\tLoss: 0.0117\n",
      "Training Epoch: 1 [13600/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [13650/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [13700/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [13750/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [13800/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [13850/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [13900/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [13950/49669]\tLoss: 0.0105\n",
      "Training Epoch: 1 [14000/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [14050/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [14100/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [14150/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [14200/49669]\tLoss: 0.0116\n",
      "Training Epoch: 1 [14250/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [14300/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [14350/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [14400/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [14450/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [14500/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [14550/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [14600/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [14650/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [14700/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [14750/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [14800/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [14850/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [14900/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [14950/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [15000/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [15050/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [15100/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [15150/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [15200/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [15250/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [15300/49669]\tLoss: 0.0113\n",
      "Training Epoch: 1 [15350/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [15400/49669]\tLoss: 0.0106\n",
      "Training Epoch: 1 [15450/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [15500/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [15550/49669]\tLoss: 0.0112\n",
      "Training Epoch: 1 [15600/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [15650/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [15700/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [15750/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [15800/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [15850/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [15900/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [15950/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [16000/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [16050/49669]\tLoss: 0.0108\n",
      "Training Epoch: 1 [16100/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [16150/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [16200/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [16250/49669]\tLoss: 0.0109\n",
      "Training Epoch: 1 [16300/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [16350/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [16400/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [16450/49669]\tLoss: 0.0107\n",
      "Training Epoch: 1 [16500/49669]\tLoss: 0.0105\n",
      "Training Epoch: 1 [16550/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [16600/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [16650/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [16700/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [16750/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [16800/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [16850/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [16900/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [16950/49669]\tLoss: 0.0110\n",
      "Training Epoch: 1 [17000/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [17050/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [17100/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [17150/49669]\tLoss: 0.0102\n",
      "Training Epoch: 1 [17200/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [17250/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [17300/49669]\tLoss: 0.0106\n",
      "Training Epoch: 1 [17350/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [17400/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [17450/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [17500/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [17550/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [17600/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [17650/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [17700/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [17750/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [17800/49669]\tLoss: 0.0099\n",
      "Training Epoch: 1 [17850/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [17900/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [17950/49669]\tLoss: 0.0104\n",
      "Training Epoch: 1 [18000/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [18050/49669]\tLoss: 0.0098\n",
      "Training Epoch: 1 [18100/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [18150/49669]\tLoss: 0.0103\n",
      "Training Epoch: 1 [18200/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [18250/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [18300/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [18350/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [18400/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [18450/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [18500/49669]\tLoss: 0.0105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [18550/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [18600/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [18650/49669]\tLoss: 0.0095\n",
      "Training Epoch: 1 [18700/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [18750/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [18800/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [18850/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [18900/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [18950/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [19000/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [19050/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [19100/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [19150/49669]\tLoss: 0.0095\n",
      "Training Epoch: 1 [19200/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [19250/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [19300/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [19350/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [19400/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [19450/49669]\tLoss: 0.0100\n",
      "Training Epoch: 1 [19500/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [19550/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [19600/49669]\tLoss: 0.0101\n",
      "Training Epoch: 1 [19650/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [19700/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [19750/49669]\tLoss: 0.0094\n",
      "Training Epoch: 1 [19800/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [19850/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [19900/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [19950/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [20000/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [20050/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [20100/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [20150/49669]\tLoss: 0.0096\n",
      "Training Epoch: 1 [20200/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [20250/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [20300/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [20350/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [20400/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [20450/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [20500/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [20550/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [20600/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [20650/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [20700/49669]\tLoss: 0.0097\n",
      "Training Epoch: 1 [20750/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [20800/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [20850/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [20900/49669]\tLoss: 0.0093\n",
      "Training Epoch: 1 [20950/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [21000/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [21050/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [21100/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [21150/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [21200/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [21250/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [21300/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [21350/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [21400/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [21450/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [21500/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [21550/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [21600/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [21650/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [21700/49669]\tLoss: 0.0091\n",
      "Training Epoch: 1 [21750/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [21800/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [21850/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [21900/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [21950/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [22000/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [22050/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [22100/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [22150/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [22200/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [22250/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [22300/49669]\tLoss: 0.0089\n",
      "Training Epoch: 1 [22350/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [22400/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [22450/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [22500/49669]\tLoss: 0.0092\n",
      "Training Epoch: 1 [22550/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [22600/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [22650/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [22700/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [22750/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [22800/49669]\tLoss: 0.0090\n",
      "Training Epoch: 1 [22850/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [22900/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [22950/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [23000/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [23050/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [23100/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [23150/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [23200/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [23250/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [23300/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [23350/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [23400/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [23450/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [23500/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [23550/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [23600/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [23650/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [23700/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [23750/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [23800/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [23850/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [23900/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [23950/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [24000/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [24050/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [24100/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [24150/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [24200/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [24250/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [24300/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [24350/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [24400/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [24450/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [24500/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [24550/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [24600/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [24650/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [24700/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [24750/49669]\tLoss: 0.0085\n",
      "Training Epoch: 1 [24800/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [24850/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [24900/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [24950/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [25000/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [25050/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [25100/49669]\tLoss: 0.0087\n",
      "Training Epoch: 1 [25150/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [25200/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [25250/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [25300/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [25350/49669]\tLoss: 0.0088\n",
      "Training Epoch: 1 [25400/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [25450/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [25500/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [25550/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [25600/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [25650/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [25700/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [25750/49669]\tLoss: 0.0084\n",
      "Training Epoch: 1 [25800/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [25850/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [25900/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [25950/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [26000/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [26050/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [26100/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [26150/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [26200/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [26250/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [26300/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [26350/49669]\tLoss: 0.0086\n",
      "Training Epoch: 1 [26400/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [26450/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [26500/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [26550/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [26600/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [26650/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [26700/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [26750/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [26800/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [26850/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [26900/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [26950/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [27000/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [27050/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [27100/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [27150/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [27200/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [27250/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [27300/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [27350/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [27400/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [27450/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [27500/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [27550/49669]\tLoss: 0.0083\n",
      "Training Epoch: 1 [27600/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [27650/49669]\tLoss: 0.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [27700/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [27750/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [27800/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [27850/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [27900/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [27950/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [28000/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [28050/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [28100/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [28150/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [28200/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [28250/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [28300/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [28350/49669]\tLoss: 0.0080\n",
      "Training Epoch: 1 [28400/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [28450/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [28500/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [28550/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [28600/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [28650/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [28700/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [28750/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [28800/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [28850/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [28900/49669]\tLoss: 0.0079\n",
      "Training Epoch: 1 [28950/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29000/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29050/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [29100/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [29150/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [29200/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [29250/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [29300/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [29350/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [29400/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [29450/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [29500/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29550/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29600/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [29650/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29700/49669]\tLoss: 0.0082\n",
      "Training Epoch: 1 [29750/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [29800/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [29850/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29900/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [29950/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [30000/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [30050/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [30100/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [30150/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [30200/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [30250/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [30300/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [30350/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [30400/49669]\tLoss: 0.0077\n",
      "Training Epoch: 1 [30450/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [30500/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [30550/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [30600/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [30650/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [30700/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [30750/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [30800/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [30850/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [30900/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [30950/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [31000/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [31050/49669]\tLoss: 0.0074\n",
      "Training Epoch: 1 [31100/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [31150/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [31200/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [31250/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [31300/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [31350/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [31400/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [31450/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [31500/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [31550/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [31600/49669]\tLoss: 0.0081\n",
      "Training Epoch: 1 [31650/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [31700/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [31750/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [31800/49669]\tLoss: 0.0078\n",
      "Training Epoch: 1 [31850/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [31900/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [31950/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [32000/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [32050/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [32100/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [32150/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [32200/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [32250/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [32300/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [32350/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [32400/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [32450/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [32500/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [32550/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [32600/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [32650/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [32700/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [32750/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [32800/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [32850/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [32900/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [32950/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [33000/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [33050/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [33100/49669]\tLoss: 0.0075\n",
      "Training Epoch: 1 [33150/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [33200/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [33250/49669]\tLoss: 0.0076\n",
      "Training Epoch: 1 [33300/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [33350/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [33400/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [33450/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [33500/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [33550/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [33600/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [33650/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [33700/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [33750/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [33800/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [33850/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [33900/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [33950/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [34000/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [34050/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [34100/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [34150/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [34200/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [34250/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [34300/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [34350/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [34400/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [34450/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [34500/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [34550/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [34600/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [34650/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [34700/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [34750/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [34800/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [34850/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [34900/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [34950/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [35000/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [35050/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [35100/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [35150/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [35200/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [35250/49669]\tLoss: 0.0073\n",
      "Training Epoch: 1 [35300/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [35350/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [35400/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [35450/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [35500/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [35550/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [35600/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [35650/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [35700/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [35750/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [35800/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [35850/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [35900/49669]\tLoss: 0.0072\n",
      "Training Epoch: 1 [35950/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [36000/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [36050/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [36100/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [36150/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [36200/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [36250/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [36300/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [36350/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [36400/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [36450/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [36500/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [36550/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [36600/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [36650/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [36700/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [36750/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [36800/49669]\tLoss: 0.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [36850/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [36900/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [36950/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [37000/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [37050/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [37100/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [37150/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [37200/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [37250/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [37300/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [37350/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [37400/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [37450/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [37500/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [37550/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [37600/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [37650/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [37700/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [37750/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [37800/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [37850/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [37900/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [37950/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [38000/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [38050/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [38100/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [38150/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [38200/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [38250/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [38300/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [38350/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [38400/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [38450/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [38500/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [38550/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [38600/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [38650/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [38700/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [38750/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [38800/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [38850/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [38900/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [38950/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [39000/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [39050/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [39100/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [39150/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [39200/49669]\tLoss: 0.0071\n",
      "Training Epoch: 1 [39250/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [39300/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [39350/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [39400/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [39450/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [39500/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [39550/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [39600/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [39650/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [39700/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [39750/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [39800/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [39850/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [39900/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [39950/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [40000/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [40050/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [40100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [40150/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [40200/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [40250/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [40300/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [40350/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [40400/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [40450/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [40500/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [40550/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [40600/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [40650/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [40700/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [40750/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [40800/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [40850/49669]\tLoss: 0.0067\n",
      "Training Epoch: 1 [40900/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [40950/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [41000/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [41050/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [41100/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [41150/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [41200/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [41250/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [41300/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [41350/49669]\tLoss: 0.0070\n",
      "Training Epoch: 1 [41400/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [41450/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [41500/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [41550/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [41600/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [41650/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [41700/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [41750/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [41800/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [41850/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [41900/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [41950/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [42000/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [42050/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [42100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [42150/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [42200/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [42250/49669]\tLoss: 0.0069\n",
      "Training Epoch: 1 [42300/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [42350/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [42400/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [42450/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [42500/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [42550/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [42600/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [42650/49669]\tLoss: 0.0068\n",
      "Training Epoch: 1 [42700/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [42750/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [42800/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [42850/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [42900/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [42950/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [43000/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [43050/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [43100/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [43150/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [43200/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [43250/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [43300/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [43350/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [43400/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [43450/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [43500/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [43550/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [43600/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [43650/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [43700/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [43750/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [43800/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [43850/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [43900/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [43950/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [44000/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [44050/49669]\tLoss: 0.0066\n",
      "Training Epoch: 1 [44100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [44150/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [44200/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [44250/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [44300/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [44350/49669]\tLoss: 0.0065\n",
      "Training Epoch: 1 [44400/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [44450/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [44500/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [44550/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [44600/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [44650/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [44700/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [44750/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [44800/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [44850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [44900/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [44950/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [45000/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [45050/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [45100/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [45150/49669]\tLoss: 0.0052\n",
      "Training Epoch: 1 [45200/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [45250/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [45300/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [45350/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [45400/49669]\tLoss: 0.0053\n",
      "Training Epoch: 1 [45450/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [45500/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [45550/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [45600/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [45650/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [45700/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [45750/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [45800/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [45850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [45900/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [45950/49669]\tLoss: 0.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [46000/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [46050/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [46100/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [46150/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [46200/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [46250/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [46300/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [46350/49669]\tLoss: 0.0064\n",
      "Training Epoch: 1 [46400/49669]\tLoss: 0.0053\n",
      "Training Epoch: 1 [46450/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [46500/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [46550/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [46600/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [46650/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [46700/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [46750/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [46800/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [46850/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [46900/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [46950/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [47000/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [47050/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [47100/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [47150/49669]\tLoss: 0.0063\n",
      "Training Epoch: 1 [47200/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [47250/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [47300/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [47350/49669]\tLoss: 0.0053\n",
      "Training Epoch: 1 [47400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 1 [47450/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [47500/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [47550/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [47600/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [47650/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [47700/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [47750/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [47800/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [47850/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [47900/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [47950/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [48000/49669]\tLoss: 0.0060\n",
      "Training Epoch: 1 [48050/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [48100/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [48150/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [48200/49669]\tLoss: 0.0061\n",
      "Training Epoch: 1 [48250/49669]\tLoss: 0.0052\n",
      "Training Epoch: 1 [48300/49669]\tLoss: 0.0050\n",
      "Training Epoch: 1 [48350/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [48400/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [48450/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [48500/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [48550/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [48600/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [48650/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [48700/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [48750/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [48800/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [48850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 1 [48900/49669]\tLoss: 0.0052\n",
      "Training Epoch: 1 [48950/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [49000/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [49050/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [49100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [49150/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [49200/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [49250/49669]\tLoss: 0.0055\n",
      "Training Epoch: 1 [49300/49669]\tLoss: 0.0059\n",
      "Training Epoch: 1 [49350/49669]\tLoss: 0.0058\n",
      "Training Epoch: 1 [49400/49669]\tLoss: 0.0050\n",
      "Training Epoch: 1 [49450/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [49500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 1 [49550/49669]\tLoss: 0.0056\n",
      "Training Epoch: 1 [49600/49669]\tLoss: 0.0054\n",
      "Training Epoch: 1 [49650/49669]\tLoss: 0.0057\n",
      "Training Epoch: 1 [49669/49669]\tLoss: 0.0062\n",
      "Training Epoch: 1 [5518/5518]\tLoss: 0.0056\n",
      "Training Epoch: 2 [50/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [100/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [150/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [200/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [250/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [350/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [400/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [450/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [500/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [550/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [600/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [650/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [700/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [750/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [800/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [850/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [900/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [950/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1000/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1050/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1100/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [1150/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [1200/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [1250/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1300/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [1350/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [1400/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [1450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [1500/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [1550/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1600/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [1650/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1700/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1750/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [1800/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [1850/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [1900/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [1950/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [2000/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [2050/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [2100/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [2150/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [2200/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [2250/49669]\tLoss: 0.0061\n",
      "Training Epoch: 2 [2300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [2350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [2400/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [2450/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [2500/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [2550/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [2600/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [2650/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [2700/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [2750/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [2800/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [2850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [2900/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [2950/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [3000/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [3050/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3100/49669]\tLoss: 0.0061\n",
      "Training Epoch: 2 [3150/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3200/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3250/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3300/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3350/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [3450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [3500/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [3550/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [3600/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [3650/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [3700/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [3750/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [3800/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [3850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [3900/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [3950/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [4000/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [4050/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [4100/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [4150/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [4200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [4250/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [4300/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [4350/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [4400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [4450/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [4500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [4550/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [4600/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [4650/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [4700/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [4750/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [4800/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [4850/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [4900/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [4950/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [5000/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [5050/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [5100/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [5150/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [5200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [5250/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [5300/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [5350/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [5400/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [5450/49669]\tLoss: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [5500/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [5550/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [5600/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [5650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [5700/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [5750/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [5800/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [5850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [5900/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [5950/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [6000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [6050/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [6100/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [6150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [6200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [6250/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [6300/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [6350/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [6400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [6450/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [6500/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [6550/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [6600/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [6650/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [6700/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [6750/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [6800/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [6850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [6900/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [6950/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [7000/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [7050/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [7100/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [7150/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [7200/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [7250/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [7300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [7350/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [7400/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [7450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [7500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [7550/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [7600/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [7650/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [7700/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [7750/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [7800/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [7850/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [7900/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [7950/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [8000/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [8050/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [8100/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [8150/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [8200/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [8250/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [8300/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [8350/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [8400/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [8450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [8500/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [8550/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [8600/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [8650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [8700/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [8750/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [8800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [8850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [8900/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [8950/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [9000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [9050/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [9100/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [9150/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [9200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [9250/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [9300/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [9350/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [9400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [9450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [9500/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [9550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [9600/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [9650/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [9700/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [9750/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [9800/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [9850/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [9900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [9950/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [10000/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [10050/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [10100/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [10150/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [10200/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [10250/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [10300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [10350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [10400/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [10450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [10500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [10550/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [10600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [10650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [10700/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [10750/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [10800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [10850/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [10900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [10950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [11000/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [11050/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [11100/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [11150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [11200/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [11250/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [11300/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [11350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [11400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [11450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [11500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [11550/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [11600/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [11650/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [11700/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [11750/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [11800/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [11850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [11900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [11950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [12000/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [12050/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [12100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [12150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [12200/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [12250/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [12300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [12350/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [12400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [12450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [12500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [12550/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [12600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [12650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [12700/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [12750/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [12800/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [12850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [12900/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [12950/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [13000/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [13050/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [13100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [13150/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [13200/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [13250/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [13300/49669]\tLoss: 0.0062\n",
      "Training Epoch: 2 [13350/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [13400/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [13450/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [13500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [13550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [13600/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [13650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [13700/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [13750/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [13800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [13850/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [13900/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [13950/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14000/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [14050/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [14100/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [14200/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [14250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [14300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [14400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [14450/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14500/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14550/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [14600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [14650/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14700/49669]\tLoss: 0.0046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [14750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [14800/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [14850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [14900/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [14950/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [15000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [15050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15100/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15150/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [15200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [15250/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [15300/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [15350/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [15400/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [15450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15500/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [15600/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [15650/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [15700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15750/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15850/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [15900/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [15950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [16000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [16050/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [16100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [16150/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [16200/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [16250/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [16300/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [16350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [16400/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [16450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [16500/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [16550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [16600/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [16650/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [16700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [16750/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [16800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [16850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [16900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [16950/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [17000/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [17050/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [17100/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [17150/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [17200/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [17250/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [17300/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [17350/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [17400/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [17450/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [17500/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [17550/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [17600/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [17650/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [17700/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [17750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [17800/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [17850/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [17900/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [17950/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [18000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [18050/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [18100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [18150/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [18200/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [18250/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [18300/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [18350/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [18400/49669]\tLoss: 0.0063\n",
      "Training Epoch: 2 [18450/49669]\tLoss: 0.0061\n",
      "Training Epoch: 2 [18500/49669]\tLoss: 0.0067\n",
      "Training Epoch: 2 [18550/49669]\tLoss: 0.0065\n",
      "Training Epoch: 2 [18600/49669]\tLoss: 0.0070\n",
      "Training Epoch: 2 [18650/49669]\tLoss: 0.0073\n",
      "Training Epoch: 2 [18700/49669]\tLoss: 0.0072\n",
      "Training Epoch: 2 [18750/49669]\tLoss: 0.0067\n",
      "Training Epoch: 2 [18800/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [18850/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [18900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [18950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [19000/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [19050/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [19100/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [19150/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [19200/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [19250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [19300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [19350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [19400/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [19450/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [19500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [19550/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [19600/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [19650/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [19700/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [19750/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [19800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [19850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [19900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [19950/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [20000/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [20050/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [20100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [20150/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [20200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [20250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [20300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [20350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [20400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [20450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [20500/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [20550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [20600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [20650/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [20700/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [20750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [20800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [20850/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [20900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [20950/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [21000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [21050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [21100/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [21150/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [21200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [21250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [21300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [21350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [21400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [21450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [21500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [21550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [21600/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [21650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [21700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [21750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [21800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [21850/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [21900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [21950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [22000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [22050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [22100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [22150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [22200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [22250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [22300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [22350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [22400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [22450/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [22500/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [22550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [22600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [22650/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [22700/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [22750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [22800/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [22850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [22900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [22950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [23000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [23050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [23100/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [23150/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [23200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [23250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [23300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [23350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [23400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [23450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [23500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [23550/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [23600/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [23650/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [23700/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [23750/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [23800/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [23850/49669]\tLoss: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [23900/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [23950/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [24000/49669]\tLoss: 0.0061\n",
      "Training Epoch: 2 [24050/49669]\tLoss: 0.0064\n",
      "Training Epoch: 2 [24100/49669]\tLoss: 0.0063\n",
      "Training Epoch: 2 [24150/49669]\tLoss: 0.0062\n",
      "Training Epoch: 2 [24200/49669]\tLoss: 0.0061\n",
      "Training Epoch: 2 [24250/49669]\tLoss: 0.0063\n",
      "Training Epoch: 2 [24300/49669]\tLoss: 0.0066\n",
      "Training Epoch: 2 [24350/49669]\tLoss: 0.0070\n",
      "Training Epoch: 2 [24400/49669]\tLoss: 0.0072\n",
      "Training Epoch: 2 [24450/49669]\tLoss: 0.0070\n",
      "Training Epoch: 2 [24500/49669]\tLoss: 0.0066\n",
      "Training Epoch: 2 [24550/49669]\tLoss: 0.0064\n",
      "Training Epoch: 2 [24600/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [24650/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [24700/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [24750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [24800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [24850/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [24900/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [24950/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [25000/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [25050/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [25100/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [25150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [25200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [25250/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [25300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [25350/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [25400/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [25450/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [25500/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [25550/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [25600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [25650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [25700/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [25750/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [25800/49669]\tLoss: 0.0065\n",
      "Training Epoch: 2 [25850/49669]\tLoss: 0.0065\n",
      "Training Epoch: 2 [25900/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [25950/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [26000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [26050/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [26100/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [26150/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [26200/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [26250/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [26300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [26350/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [26400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [26450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [26500/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [26550/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [26600/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [26650/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [26700/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [26750/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [26800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [26850/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [26900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [26950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [27000/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [27050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [27100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [27150/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [27200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [27250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [27300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [27350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [27400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [27450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [27500/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [27550/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [27600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [27650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [27700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [27750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [27800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [27850/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [27900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [27950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [28000/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [28050/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [28100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [28150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [28200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [28250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [28300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 2 [28350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [28400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [28450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [28500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [28550/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [28600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [28650/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [28700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [28750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [28800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [28850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [28900/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [28950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [29000/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [29050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [29100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [29150/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [29200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [29250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [29300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [29350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [29400/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [29450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [29500/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [29550/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [29600/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [29650/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [29700/49669]\tLoss: 0.0063\n",
      "Training Epoch: 2 [29750/49669]\tLoss: 0.0063\n",
      "Training Epoch: 2 [29800/49669]\tLoss: 0.0064\n",
      "Training Epoch: 2 [29850/49669]\tLoss: 0.0066\n",
      "Training Epoch: 2 [29900/49669]\tLoss: 0.0065\n",
      "Training Epoch: 2 [29950/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [30000/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [30050/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [30100/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [30150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [30200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [30250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [30300/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [30350/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [30400/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [30450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [30500/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [30550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [30600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [30650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [30700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [30750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [30800/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [30850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [30900/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [30950/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [31000/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [31050/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [31100/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [31150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [31200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [31250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [31300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [31350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [31400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [31450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [31500/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [31550/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [31600/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [31650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [31700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [31750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [31800/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [31850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [31900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [31950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [32000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [32050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [32100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [32150/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [32200/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [32250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [32300/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [32350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [32400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [32450/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [32500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [32550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [32600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [32650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [32700/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [32750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [32800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [32850/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [32900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [32950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [33000/49669]\tLoss: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [33050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [33100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [33150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [33200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [33250/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [33300/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [33350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [33400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [33450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [33500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [33550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [33600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [33650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [33700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 2 [33750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [33800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [33850/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [33900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [33950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [34100/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [34150/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [34250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [34300/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [34400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [34450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34500/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [34550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [34600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [34650/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [34750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34800/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [34850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [34900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [34950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 2 [35000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [35050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [35100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [35150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [35200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [35250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [35300/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [35350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [35400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [35450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [35500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [35550/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [35600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [35650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [35700/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [35750/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [35800/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [35850/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [35900/49669]\tLoss: 0.0067\n",
      "Training Epoch: 2 [35950/49669]\tLoss: 0.0074\n",
      "Training Epoch: 2 [36000/49669]\tLoss: 0.0086\n",
      "Training Epoch: 2 [36050/49669]\tLoss: 0.0097\n",
      "Training Epoch: 2 [36100/49669]\tLoss: 0.0104\n",
      "Training Epoch: 2 [36150/49669]\tLoss: 0.0102\n",
      "Training Epoch: 2 [36200/49669]\tLoss: 0.0091\n",
      "Training Epoch: 2 [36250/49669]\tLoss: 0.0072\n",
      "Training Epoch: 2 [36300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [36350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [36400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [36450/49669]\tLoss: 0.0062\n",
      "Training Epoch: 2 [36500/49669]\tLoss: 0.0072\n",
      "Training Epoch: 2 [36550/49669]\tLoss: 0.0079\n",
      "Training Epoch: 2 [36600/49669]\tLoss: 0.0074\n",
      "Training Epoch: 2 [36650/49669]\tLoss: 0.0062\n",
      "Training Epoch: 2 [36700/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [36750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [36800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [36850/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [36900/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [36950/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [37000/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [37050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [37100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [37150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [37200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [37250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [37300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [37350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [37400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [37450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [37500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [37550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [37600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [37650/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [37700/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [37750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [37800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 2 [37850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [37900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [37950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [38000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [38050/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [38100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [38150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [38200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [38250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [38300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [38350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [38400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 2 [38450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [38500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [38550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [38600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [38650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [38700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [38750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [38800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [38850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [38900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [38950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [39000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [39050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [39100/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [39150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [39200/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [39250/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [39300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [39350/49669]\tLoss: 0.0055\n",
      "Training Epoch: 2 [39400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [39450/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [39500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [39550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [39600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [39650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [39700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [39750/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [39800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [39850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [39900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [39950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [40000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [40050/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [40100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [40150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [40200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [40250/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [40300/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [40350/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [40400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [40450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [40500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [40550/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [40600/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [40650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [40700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [40750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [40800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [40850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [40900/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [40950/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [41000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [41050/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [41100/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [41150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [41200/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [41250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [41300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [41350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [41400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [41450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [41500/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [41550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [41600/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [41650/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [41700/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [41750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [41800/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [41850/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [41900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [41950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [42050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [42100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42150/49669]\tLoss: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [42200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42250/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [42300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [42450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [42550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [42600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [42650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [42700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [42750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [42800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [42850/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [42900/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [42950/49669]\tLoss: 0.0053\n",
      "Training Epoch: 2 [43000/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [43050/49669]\tLoss: 0.0058\n",
      "Training Epoch: 2 [43100/49669]\tLoss: 0.0063\n",
      "Training Epoch: 2 [43150/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [43200/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [43250/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [43300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [43350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [43400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [43450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [43500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [43550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [43600/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [43650/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [43700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [43750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [43800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [43850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [43900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [43950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [44000/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [44050/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [44100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 2 [44150/49669]\tLoss: 0.0070\n",
      "Training Epoch: 2 [44200/49669]\tLoss: 0.0080\n",
      "Training Epoch: 2 [44250/49669]\tLoss: 0.0080\n",
      "Training Epoch: 2 [44300/49669]\tLoss: 0.0073\n",
      "Training Epoch: 2 [44350/49669]\tLoss: 0.0064\n",
      "Training Epoch: 2 [44400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [44450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [44500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [44550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [44600/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [44650/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [44700/49669]\tLoss: 0.0060\n",
      "Training Epoch: 2 [44750/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [44800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [44850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 2 [44900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [44950/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [45000/49669]\tLoss: 0.0057\n",
      "Training Epoch: 2 [45050/49669]\tLoss: 0.0056\n",
      "Training Epoch: 2 [45100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [45150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [45200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [45250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [45300/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [45350/49669]\tLoss: 0.0054\n",
      "Training Epoch: 2 [45400/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [45450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [45500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [45550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [45600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [45650/49669]\tLoss: 0.0051\n",
      "Training Epoch: 2 [45700/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [45750/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [45800/49669]\tLoss: 0.0052\n",
      "Training Epoch: 2 [45850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [45900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [45950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [46000/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [46050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [46100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [46150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [46200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [46250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [46300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 2 [46350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [46400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [46450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [46500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [46550/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [46600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [46650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [46700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [46750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [46800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [46850/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [46900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [46950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [47000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [47050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [47100/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [47150/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [47200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [47250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [47300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [47350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [47400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 2 [47450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [47500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [47550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [47600/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [47650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [47700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [47750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [47800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [47850/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [47900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [47950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [48000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [48050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [48100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [48150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [48200/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [48250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 2 [48300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 2 [48350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [48400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [48450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [48500/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [48550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [48600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [48650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [48700/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [48750/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [48800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [48850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [48900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [48950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [49000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [49050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 2 [49100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [49150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [49200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [49250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 2 [49300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 2 [49350/49669]\tLoss: 0.0048\n",
      "Training Epoch: 2 [49400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [49450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 2 [49500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [49550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [49600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 2 [49650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 2 [49669/49669]\tLoss: 0.0050\n",
      "Training Epoch: 2 [5518/5518]\tLoss: 0.0049\n",
      "Training Epoch: 3 [50/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [150/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [200/49669]\tLoss: 0.0054\n",
      "Training Epoch: 3 [250/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [350/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [850/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [1000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [1050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [1100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [1150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [1200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [1250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [1300/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [1350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [1400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [1450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [1500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [1550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [1600/49669]\tLoss: 0.0046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [1650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [1700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [1750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [1800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [1850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [1900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [1950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [2000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [2050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [2150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [2250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [2300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [2350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [2400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [2450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [2500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [2550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [2600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [2650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [2900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [2950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [3000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [3050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [3100/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [3150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [3200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [3250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [3300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [3350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [3400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [3450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [3500/49669]\tLoss: 0.0052\n",
      "Training Epoch: 3 [3550/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [3600/49669]\tLoss: 0.0063\n",
      "Training Epoch: 3 [3650/49669]\tLoss: 0.0068\n",
      "Training Epoch: 3 [3700/49669]\tLoss: 0.0078\n",
      "Training Epoch: 3 [3750/49669]\tLoss: 0.0098\n",
      "Training Epoch: 3 [3800/49669]\tLoss: 0.0118\n",
      "Training Epoch: 3 [3850/49669]\tLoss: 0.0141\n",
      "Training Epoch: 3 [3900/49669]\tLoss: 0.0134\n",
      "Training Epoch: 3 [3950/49669]\tLoss: 0.0092\n",
      "Training Epoch: 3 [4000/49669]\tLoss: 0.0056\n",
      "Training Epoch: 3 [4050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [4100/49669]\tLoss: 0.0068\n",
      "Training Epoch: 3 [4150/49669]\tLoss: 0.0108\n",
      "Training Epoch: 3 [4200/49669]\tLoss: 0.0139\n",
      "Training Epoch: 3 [4250/49669]\tLoss: 0.0104\n",
      "Training Epoch: 3 [4300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [4350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [4400/49669]\tLoss: 0.0087\n",
      "Training Epoch: 3 [4450/49669]\tLoss: 0.0113\n",
      "Training Epoch: 3 [4500/49669]\tLoss: 0.0097\n",
      "Training Epoch: 3 [4550/49669]\tLoss: 0.0062\n",
      "Training Epoch: 3 [4600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [4650/49669]\tLoss: 0.0055\n",
      "Training Epoch: 3 [4700/49669]\tLoss: 0.0076\n",
      "Training Epoch: 3 [4750/49669]\tLoss: 0.0085\n",
      "Training Epoch: 3 [4800/49669]\tLoss: 0.0073\n",
      "Training Epoch: 3 [4850/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [4900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [4950/49669]\tLoss: 0.0056\n",
      "Training Epoch: 3 [5000/49669]\tLoss: 0.0068\n",
      "Training Epoch: 3 [5050/49669]\tLoss: 0.0062\n",
      "Training Epoch: 3 [5100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [5150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [5200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [5250/49669]\tLoss: 0.0054\n",
      "Training Epoch: 3 [5300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [5350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [5400/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [5450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [5500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [5550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [5600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [5650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [5700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [5750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [5800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [5850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [5900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [5950/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [6000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [6050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [6150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [6200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6250/49669]\tLoss: 0.0052\n",
      "Training Epoch: 3 [6300/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [6450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [6500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [6550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [6600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [6650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [6800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [6850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [6900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [6950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [7050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [7100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [7200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [7250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [7300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [7350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [7400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [7450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [7550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [7650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [7750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7850/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [7900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [7950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [8000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [8050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [8100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [8250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [8300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [8400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [8550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [8650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [8700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [8750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [8800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [8850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [8900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [8950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [9000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [9050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [9100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [9150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [9200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [9250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [9300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [9350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [9400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [9450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [9500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [9550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [9600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [9650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [9700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [9750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [9800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [9850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [9900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [9950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [10050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [10100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [10150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [10300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [10350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [10400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [10450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [10550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [10700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [10750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [10800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [10850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [10900/49669]\tLoss: 0.0042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [10950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [11000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [11050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [11100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [11150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [11200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [11250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [11300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [11350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [11400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [11450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [11500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [11550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [11600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [11650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [11700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [11750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [11800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [11850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [11900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [11950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [12000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [12050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [12100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [12150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [12200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [12250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [12300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [12350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [12400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [12450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [12500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [12550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [12600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [12650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [12700/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [12750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [12800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [12850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [12900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [12950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [13000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [13050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [13100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [13150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [13200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [13250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [13300/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [13350/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [13400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [13450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [13500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [13550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [13600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [13650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [13700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [13750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [13800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [13850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [13900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [13950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [14000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [14050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [14100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [14150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [14200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [14250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [14300/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [14350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [14400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [14450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [14500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [14550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [14600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [14650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [14700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [14750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [14800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [14850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [14900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [14950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [15200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [15250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [15300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [15350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [15400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [15450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [15600/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [15650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [15700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [15900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [15950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [16000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [16050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [16100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [16150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [16200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [16250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [16300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [16350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [16400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [16450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [16500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [16550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [16600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [16650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [16700/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [16750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [16800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [16850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [16900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [16950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [17000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [17050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [17100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [17150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [17200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [17250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [17300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [17350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [17400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [17450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [17500/49669]\tLoss: 0.0052\n",
      "Training Epoch: 3 [17550/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [17600/49669]\tLoss: 0.0060\n",
      "Training Epoch: 3 [17650/49669]\tLoss: 0.0065\n",
      "Training Epoch: 3 [17700/49669]\tLoss: 0.0067\n",
      "Training Epoch: 3 [17750/49669]\tLoss: 0.0072\n",
      "Training Epoch: 3 [17800/49669]\tLoss: 0.0077\n",
      "Training Epoch: 3 [17850/49669]\tLoss: 0.0081\n",
      "Training Epoch: 3 [17900/49669]\tLoss: 0.0082\n",
      "Training Epoch: 3 [17950/49669]\tLoss: 0.0084\n",
      "Training Epoch: 3 [18000/49669]\tLoss: 0.0079\n",
      "Training Epoch: 3 [18050/49669]\tLoss: 0.0073\n",
      "Training Epoch: 3 [18100/49669]\tLoss: 0.0056\n",
      "Training Epoch: 3 [18150/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [18200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [18250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [18300/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [18350/49669]\tLoss: 0.0065\n",
      "Training Epoch: 3 [18400/49669]\tLoss: 0.0070\n",
      "Training Epoch: 3 [18450/49669]\tLoss: 0.0063\n",
      "Training Epoch: 3 [18500/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [18550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [18600/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [18650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [18700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [18750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [18800/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [18850/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [18900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [18950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [19000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [19050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [19100/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [19150/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [19200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [19250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [19300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [19350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [19400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [19450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [19500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [19550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [19600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [19650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [19700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [19750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [19800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [19850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [19900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [19950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [20000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [20050/49669]\tLoss: 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [20100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [20200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [20250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [20300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [20400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [20650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [20700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [20750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [20800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [20850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [20900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [20950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [21000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [21050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [21100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [21150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [21200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [21250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [21300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [21350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [21400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [21450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [21500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [21550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [21600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [21650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [21700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [21750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [21800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [21850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [21900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [21950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [22000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [22050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [22100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [22150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [22200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [22250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [22300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [22350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [22400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [22450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [22500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [22550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [22600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [22650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [22700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [22750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [22800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [22850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [22900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [22950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [23000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [23050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [23100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [23150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [23200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [23250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [23300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [23350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [23400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [23450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [23500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [23550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [23600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [23650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [23700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [23750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [23800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [23850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [23900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [23950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [24000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [24050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [24100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [24150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [24200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [24250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [24300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [24350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [24400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [24450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [24500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [24550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [24600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [24650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [24700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [24750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [24800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [24850/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [24900/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [24950/49669]\tLoss: 0.0055\n",
      "Training Epoch: 3 [25000/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [25050/49669]\tLoss: 0.0062\n",
      "Training Epoch: 3 [25100/49669]\tLoss: 0.0065\n",
      "Training Epoch: 3 [25150/49669]\tLoss: 0.0068\n",
      "Training Epoch: 3 [25200/49669]\tLoss: 0.0070\n",
      "Training Epoch: 3 [25250/49669]\tLoss: 0.0066\n",
      "Training Epoch: 3 [25300/49669]\tLoss: 0.0061\n",
      "Training Epoch: 3 [25350/49669]\tLoss: 0.0063\n",
      "Training Epoch: 3 [25400/49669]\tLoss: 0.0066\n",
      "Training Epoch: 3 [25450/49669]\tLoss: 0.0072\n",
      "Training Epoch: 3 [25500/49669]\tLoss: 0.0068\n",
      "Training Epoch: 3 [25550/49669]\tLoss: 0.0060\n",
      "Training Epoch: 3 [25600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [25650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [25700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [25750/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [25800/49669]\tLoss: 0.0061\n",
      "Training Epoch: 3 [25850/49669]\tLoss: 0.0067\n",
      "Training Epoch: 3 [25900/49669]\tLoss: 0.0066\n",
      "Training Epoch: 3 [25950/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [26000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [26050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [26100/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [26150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [26200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [26250/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [26300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [26350/49669]\tLoss: 0.0054\n",
      "Training Epoch: 3 [26400/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [26450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [26500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [26550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [26600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [26650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [26700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [26750/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [26800/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [26850/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [26900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [26950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [27000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [27050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [27150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [27300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [27350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [27400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [27550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [27600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [27700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [27750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [27800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [27850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [27900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [27950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [28000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [28050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [28100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [28150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [28200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [28250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [28300/49669]\tLoss: 0.0037\n",
      "Training Epoch: 3 [28350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [28400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [28450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [28500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [28550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [28600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [28650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [28700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [28750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [28800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [28850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [28900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [28950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [29000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [29050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [29100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [29150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [29200/49669]\tLoss: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [29250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [29300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [29350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [29400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [29450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [29500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [29550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [29600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [29650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [29700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [29750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [29800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [29850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [29900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [29950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [30000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [30050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [30100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [30150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [30200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [30250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [30300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [30350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [30400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [30450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [30500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [30550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [30600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [30650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [30700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [30750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [30800/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [30850/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [30900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [30950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [31000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [31050/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [31100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 3 [31150/49669]\tLoss: 0.0066\n",
      "Training Epoch: 3 [31200/49669]\tLoss: 0.0075\n",
      "Training Epoch: 3 [31250/49669]\tLoss: 0.0085\n",
      "Training Epoch: 3 [31300/49669]\tLoss: 0.0089\n",
      "Training Epoch: 3 [31350/49669]\tLoss: 0.0091\n",
      "Training Epoch: 3 [31400/49669]\tLoss: 0.0083\n",
      "Training Epoch: 3 [31450/49669]\tLoss: 0.0072\n",
      "Training Epoch: 3 [31500/49669]\tLoss: 0.0059\n",
      "Training Epoch: 3 [31550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 3 [31600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [31650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [31700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [31750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [31800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [31850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [31900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [31950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [32000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [32050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [32100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [32150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [32200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [32250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [32300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [32350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [32400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [32450/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [32500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [32550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [32600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [32650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [32700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [32750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [32800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [32850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [32900/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [32950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [33000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [33050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [33100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [33150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [33200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [33250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [33300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [33350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [33400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [33450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [33500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [33550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [33600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [33650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [33700/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [33750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [33800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [33850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [33900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [33950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [34000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [34050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [34100/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [34150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [34200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [34250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [34300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [34350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [34400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [34450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [34500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [34550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [34600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [34650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [34700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [34750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [34800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [34850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [34900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [34950/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [35000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [35050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [35100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [35150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [35200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [35250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [35300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [35350/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [35400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [35450/49669]\tLoss: 0.0055\n",
      "Training Epoch: 3 [35500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [35550/49669]\tLoss: 0.0055\n",
      "Training Epoch: 3 [35600/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [35650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [35700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [35750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [35800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [35850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [35900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [35950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [36000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [36050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [36150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [36200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [36300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [36350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [36500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [36600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [36650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [36700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [36750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [36800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [36900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [36950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [37000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [37050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [37100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [37150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [37200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [37250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [37300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [37350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [37400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [37450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [37500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [37550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [37600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [37650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [37700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [37750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [37800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [37850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [37900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [37950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [38000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [38050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [38100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [38150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [38200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [38250/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [38300/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [38350/49669]\tLoss: 0.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [38400/49669]\tLoss: 0.0065\n",
      "Training Epoch: 3 [38450/49669]\tLoss: 0.0067\n",
      "Training Epoch: 3 [38500/49669]\tLoss: 0.0063\n",
      "Training Epoch: 3 [38550/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [38600/49669]\tLoss: 0.0055\n",
      "Training Epoch: 3 [38650/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [38700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [38750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [38800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [38850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [38900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [38950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [39000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [39050/49669]\tLoss: 0.0053\n",
      "Training Epoch: 3 [39100/49669]\tLoss: 0.0060\n",
      "Training Epoch: 3 [39150/49669]\tLoss: 0.0068\n",
      "Training Epoch: 3 [39200/49669]\tLoss: 0.0078\n",
      "Training Epoch: 3 [39250/49669]\tLoss: 0.0094\n",
      "Training Epoch: 3 [39300/49669]\tLoss: 0.0119\n",
      "Training Epoch: 3 [39350/49669]\tLoss: 0.0130\n",
      "Training Epoch: 3 [39400/49669]\tLoss: 0.0132\n",
      "Training Epoch: 3 [39450/49669]\tLoss: 0.0121\n",
      "Training Epoch: 3 [39500/49669]\tLoss: 0.0076\n",
      "Training Epoch: 3 [39550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [39600/49669]\tLoss: 0.0058\n",
      "Training Epoch: 3 [39650/49669]\tLoss: 0.0096\n",
      "Training Epoch: 3 [39700/49669]\tLoss: 0.0113\n",
      "Training Epoch: 3 [39750/49669]\tLoss: 0.0090\n",
      "Training Epoch: 3 [39800/49669]\tLoss: 0.0057\n",
      "Training Epoch: 3 [39850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [39900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [39950/49669]\tLoss: 0.0063\n",
      "Training Epoch: 3 [40000/49669]\tLoss: 0.0070\n",
      "Training Epoch: 3 [40050/49669]\tLoss: 0.0056\n",
      "Training Epoch: 3 [40100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [40150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [40200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [40250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [40300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 3 [40350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [40400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [40450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [40500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [40550/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [40600/49669]\tLoss: 0.0048\n",
      "Training Epoch: 3 [40650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 3 [40700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [40750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [40800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [40850/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [40900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [40950/49669]\tLoss: 0.0049\n",
      "Training Epoch: 3 [41000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [41050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [41100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [41250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [41300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [41350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [41400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [41550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [41600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [41650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [41800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [41850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [41900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [41950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [42000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [42050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [42100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [42150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [42200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [42250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [42300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [42350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [42400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [42450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [42500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [42550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [42600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [42650/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [42700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [42750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [42800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [42850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [42900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [42950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [43000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [43050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [43100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [43150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [43200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [43250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [43300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [43350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [43400/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [43450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [43500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [43550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [43600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [43650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [43700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [43750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [43800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [43850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [43900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [43950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [44000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [44050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [44100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [44150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [44200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [44250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [44300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [44350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [44400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [44450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [44500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [44550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [44600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [44650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [44700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [44750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [44800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [44850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [44900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [44950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [45050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [45100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [45150/49669]\tLoss: 0.0037\n",
      "Training Epoch: 3 [45200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [45300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [45400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [45450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [45550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [45600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [45750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [45850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [45900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [45950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [46000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [46050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [46100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [46150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [46200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [46250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [46300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [46350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [46400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [46450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [46500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [46550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [46600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [46650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [46700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [46750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [46800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [46850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [46900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [46950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [47000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [47050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [47100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [47150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [47200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [47250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [47300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [47350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 3 [47400/49669]\tLoss: 0.0037\n",
      "Training Epoch: 3 [47450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [47500/49669]\tLoss: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [47550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [47600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [47650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [47700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [47750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [47800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [47850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [47900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [47950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [48000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [48050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [48100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [48150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [48200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [48250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [48300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 3 [48350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [48400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [48450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [48500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [48550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [48600/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [48650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [48700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [48750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 3 [48800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [48850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 3 [48900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [48950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [49000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [49050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [49100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [49150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 3 [49200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [49250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [49300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [49350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [49400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [49450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 3 [49500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [49550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 3 [49600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 3 [49650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 3 [49669/49669]\tLoss: 0.0046\n",
      "Training Epoch: 3 [5518/5518]\tLoss: 0.0042\n",
      "Training Epoch: 4 [50/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [1000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [1050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [1100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [1150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [1200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [1250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [1300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [1350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [1400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [1450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [1500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [1550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [1600/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [1650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [1700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [1750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [1800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [1850/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [1900/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [1950/49669]\tLoss: 0.0058\n",
      "Training Epoch: 4 [2000/49669]\tLoss: 0.0061\n",
      "Training Epoch: 4 [2050/49669]\tLoss: 0.0069\n",
      "Training Epoch: 4 [2100/49669]\tLoss: 0.0076\n",
      "Training Epoch: 4 [2150/49669]\tLoss: 0.0084\n",
      "Training Epoch: 4 [2200/49669]\tLoss: 0.0085\n",
      "Training Epoch: 4 [2250/49669]\tLoss: 0.0081\n",
      "Training Epoch: 4 [2300/49669]\tLoss: 0.0067\n",
      "Training Epoch: 4 [2350/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [2400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [2450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [2500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [2550/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [2600/49669]\tLoss: 0.0059\n",
      "Training Epoch: 4 [2650/49669]\tLoss: 0.0066\n",
      "Training Epoch: 4 [2700/49669]\tLoss: 0.0072\n",
      "Training Epoch: 4 [2750/49669]\tLoss: 0.0075\n",
      "Training Epoch: 4 [2800/49669]\tLoss: 0.0078\n",
      "Training Epoch: 4 [2850/49669]\tLoss: 0.0074\n",
      "Training Epoch: 4 [2900/49669]\tLoss: 0.0065\n",
      "Training Epoch: 4 [2950/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [3000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [3050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [3100/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [3150/49669]\tLoss: 0.0055\n",
      "Training Epoch: 4 [3200/49669]\tLoss: 0.0060\n",
      "Training Epoch: 4 [3250/49669]\tLoss: 0.0056\n",
      "Training Epoch: 4 [3300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [3350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [3400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [3450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [3500/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [3550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 4 [3600/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [3650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [3700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [3750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [3800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [3850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [3900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [3950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [4000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [4050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [4100/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [4150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [4200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [4250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [4300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [4350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [4400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [4450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [4500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [4550/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [4600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [4650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [4700/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [4750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [4800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [4850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [4900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [4950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [5050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [5200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [5250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [5300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [5450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [5500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [5550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [5650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [5700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [5900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [5950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [6000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [6050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [6100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [6150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [6200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [6250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [6300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [6350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [6400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [6450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [6500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [6550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [6600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [6650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [6700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [6750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [6800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [6850/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [6900/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [6950/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [7000/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [7050/49669]\tLoss: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [7100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [7150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [7200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [7250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [7300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [7350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [7400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [7450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [7500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [7550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [7600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [7650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [7700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [7750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [7800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [7850/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [7900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [7950/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [8000/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [8050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [8100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [8150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [8200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [8250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [8300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [8350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [8400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [8450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [8500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [8550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [8600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [8650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [8700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [8750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [8800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [8850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [8900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [8950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [9000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [9050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [9100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [9150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [9200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [9250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [9300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [9350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [9400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [9450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [9500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [9550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [9600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [9650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [9700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [9750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [9800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [9850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [9900/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [9950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [10050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [10100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [10150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [10300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [10350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [10400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [10450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10500/49669]\tLoss: 0.0037\n",
      "Training Epoch: 4 [10550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [10750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [10800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [10850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [10900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [10950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [11000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [11050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [11150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [11200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11300/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [11350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11450/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [11500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [11550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [11600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [11650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [11750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [11800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [11850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [11900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [11950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [12000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [12050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [12100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [12150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [12200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [12250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [12300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [12350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [12400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [12450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [12500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [12550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [12600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [12650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [12700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [12750/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [12800/49669]\tLoss: 0.0058\n",
      "Training Epoch: 4 [12850/49669]\tLoss: 0.0068\n",
      "Training Epoch: 4 [12900/49669]\tLoss: 0.0093\n",
      "Training Epoch: 4 [12950/49669]\tLoss: 0.0132\n",
      "Training Epoch: 4 [13000/49669]\tLoss: 0.0173\n",
      "Training Epoch: 4 [13050/49669]\tLoss: 0.0170\n",
      "Training Epoch: 4 [13100/49669]\tLoss: 0.0139\n",
      "Training Epoch: 4 [13150/49669]\tLoss: 0.0090\n",
      "Training Epoch: 4 [13200/49669]\tLoss: 0.0054\n",
      "Training Epoch: 4 [13250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [13300/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [13350/49669]\tLoss: 0.0069\n",
      "Training Epoch: 4 [13400/49669]\tLoss: 0.0076\n",
      "Training Epoch: 4 [13450/49669]\tLoss: 0.0067\n",
      "Training Epoch: 4 [13500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [13550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [13600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [13650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [13700/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [13750/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [13800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [13850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [13900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [13950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [14050/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [14100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [14200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [14250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14300/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [14350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [14400/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [14450/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [14500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [14550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [14700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [14750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [14800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [14850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [14950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [15000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [15050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [15100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [15150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [15200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [15250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [15300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [15350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [15400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [15450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [15500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [15550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [15600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [15650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [15700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [15750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [15800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [15850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [15900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [15950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [16000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [16050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [16100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [16150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [16200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [16250/49669]\tLoss: 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [16300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [16350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [16400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [16450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [16500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [16550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [16600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [16650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [16700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [16750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [16800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [16850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [16900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [16950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [17000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [17050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [17100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [17150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [17200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [17250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [17300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [17350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [17400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [17450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [17500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [17550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [17600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [17650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [17700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [17750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [17800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [17850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [17900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [17950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [18000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [18050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [18100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [18200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [18250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [18400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [18450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [18500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [18550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [18650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [18800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [18850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [18900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [18950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [19000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [19050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [19100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [19200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [19300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [19450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [19500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [19550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [19650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [19700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [19750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [19800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [19900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [19950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [20000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [20100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [20150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [20200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [20250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [20300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [20400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [20650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [20700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [20750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [20800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [20850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [20900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [20950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [21000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [21050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [21100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [21150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [21200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [21250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [21300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [21350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [21400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [21450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [21500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [21550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [21600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [21650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [21700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [21750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [21800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [21850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [21900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [21950/49669]\tLoss: 0.0037\n",
      "Training Epoch: 4 [22000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22050/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [22100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [22150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [22250/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [22300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [22350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [22450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [22500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [22550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [22600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22750/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [22800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [22850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [22900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [22950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [23000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [23050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [23100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [23150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [23200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [23250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [23300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [23350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [23400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [23450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [23500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [23550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [23600/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [23650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [23700/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [23750/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [23800/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [23850/49669]\tLoss: 0.0054\n",
      "Training Epoch: 4 [23900/49669]\tLoss: 0.0057\n",
      "Training Epoch: 4 [23950/49669]\tLoss: 0.0065\n",
      "Training Epoch: 4 [24000/49669]\tLoss: 0.0074\n",
      "Training Epoch: 4 [24050/49669]\tLoss: 0.0081\n",
      "Training Epoch: 4 [24100/49669]\tLoss: 0.0085\n",
      "Training Epoch: 4 [24150/49669]\tLoss: 0.0085\n",
      "Training Epoch: 4 [24200/49669]\tLoss: 0.0082\n",
      "Training Epoch: 4 [24250/49669]\tLoss: 0.0084\n",
      "Training Epoch: 4 [24300/49669]\tLoss: 0.0089\n",
      "Training Epoch: 4 [24350/49669]\tLoss: 0.0089\n",
      "Training Epoch: 4 [24400/49669]\tLoss: 0.0075\n",
      "Training Epoch: 4 [24450/49669]\tLoss: 0.0057\n",
      "Training Epoch: 4 [24500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [24550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [24600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [24650/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [24700/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [24750/49669]\tLoss: 0.0056\n",
      "Training Epoch: 4 [24800/49669]\tLoss: 0.0055\n",
      "Training Epoch: 4 [24850/49669]\tLoss: 0.0052\n",
      "Training Epoch: 4 [24900/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [24950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [25000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [25050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [25100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [25150/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [25200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [25250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [25300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [25350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [25400/49669]\tLoss: 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [25450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [25500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [25550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [25600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [25650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [25700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [25750/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [25800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [25850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [25900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [25950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [26000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [26050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [26150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [26200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [26300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [26350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [26400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [26450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [26500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [26550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [26600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [26850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [26900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [26950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [27000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [27050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [27100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [27150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [27200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [27250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [27300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [27350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [27400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [27450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [27500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [27550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [27600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [27650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [27700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [27750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [27800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [27850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [27900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [27950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [28000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [28050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [28100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [28150/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [28200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [28250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [28300/49669]\tLoss: 0.0036\n",
      "Training Epoch: 4 [28350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [28400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [28450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [28500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [28550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [28600/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [28650/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [28700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [28750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [28800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [28850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [28900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [28950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [29000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [29050/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [29100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [29150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [29200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [29250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [29300/49669]\tLoss: 0.0037\n",
      "Training Epoch: 4 [29350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [29400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [29450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [29500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [29550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [29600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [29650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [29700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [29750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [29800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [29850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [29900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [29950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [30000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [30050/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [30100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [30150/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [30200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [30250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [30300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [30350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [30400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [30450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [30500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [30550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [30600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [30650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [30700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [30750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [30800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [30850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [30900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [30950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [31000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [31050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [31100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [31150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [31200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [31250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [31300/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [31350/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [31400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [31450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [31500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [31550/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [31600/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [31650/49669]\tLoss: 0.0056\n",
      "Training Epoch: 4 [31700/49669]\tLoss: 0.0060\n",
      "Training Epoch: 4 [31750/49669]\tLoss: 0.0066\n",
      "Training Epoch: 4 [31800/49669]\tLoss: 0.0075\n",
      "Training Epoch: 4 [31850/49669]\tLoss: 0.0080\n",
      "Training Epoch: 4 [31900/49669]\tLoss: 0.0081\n",
      "Training Epoch: 4 [31950/49669]\tLoss: 0.0074\n",
      "Training Epoch: 4 [32000/49669]\tLoss: 0.0065\n",
      "Training Epoch: 4 [32050/49669]\tLoss: 0.0056\n",
      "Training Epoch: 4 [32100/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [32150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [32200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [32250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [32300/49669]\tLoss: 0.0054\n",
      "Training Epoch: 4 [32350/49669]\tLoss: 0.0058\n",
      "Training Epoch: 4 [32400/49669]\tLoss: 0.0060\n",
      "Training Epoch: 4 [32450/49669]\tLoss: 0.0054\n",
      "Training Epoch: 4 [32500/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [32550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [32600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [32650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [32700/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [32750/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [32800/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [32850/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [32900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [32950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [33000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [33050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [33100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [33150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [33200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [33250/49669]\tLoss: 0.0052\n",
      "Training Epoch: 4 [33300/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [33350/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [33400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [33450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [33500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [33550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [33600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [33650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [33700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [33750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [33800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [33850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [33900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [33950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [34000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [34050/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [34100/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [34150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [34200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [34250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [34300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [34350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [34400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [34450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [34500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [34550/49669]\tLoss: 0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [34600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [34650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [34700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [34750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [34800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [34850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [34900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [34950/49669]\tLoss: 0.0036\n",
      "Training Epoch: 4 [35000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [35050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [35100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [35250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [35300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [35350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [35550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [35650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [35700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [35750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [35800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [35850/49669]\tLoss: 0.0037\n",
      "Training Epoch: 4 [35900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [35950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [36000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [36050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [36150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [36200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [36300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [36350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [36500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [36600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [36650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [36700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [36750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [36800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [36900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [36950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [37000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [37200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [37250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [37350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [37500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [37550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [37600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [37650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [37700/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [37750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [37800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [37900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [37950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [38000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [38050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [38100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [38150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [38200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [38250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [38300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [38350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [38400/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [38450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [38500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [38550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [38600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [38650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [38700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [38750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [38800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [38850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [38900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [38950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [39000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [39050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [39100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [39150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [39200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [39250/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [39300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [39350/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [39400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [39450/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [39500/49669]\tLoss: 0.0052\n",
      "Training Epoch: 4 [39550/49669]\tLoss: 0.0054\n",
      "Training Epoch: 4 [39600/49669]\tLoss: 0.0056\n",
      "Training Epoch: 4 [39650/49669]\tLoss: 0.0060\n",
      "Training Epoch: 4 [39700/49669]\tLoss: 0.0058\n",
      "Training Epoch: 4 [39750/49669]\tLoss: 0.0060\n",
      "Training Epoch: 4 [39800/49669]\tLoss: 0.0055\n",
      "Training Epoch: 4 [39850/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [39900/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [39950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [40000/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [40050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [40100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [40150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [40200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [40250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [40300/49669]\tLoss: 0.0047\n",
      "Training Epoch: 4 [40350/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [40400/49669]\tLoss: 0.0055\n",
      "Training Epoch: 4 [40450/49669]\tLoss: 0.0061\n",
      "Training Epoch: 4 [40500/49669]\tLoss: 0.0064\n",
      "Training Epoch: 4 [40550/49669]\tLoss: 0.0073\n",
      "Training Epoch: 4 [40600/49669]\tLoss: 0.0078\n",
      "Training Epoch: 4 [40650/49669]\tLoss: 0.0088\n",
      "Training Epoch: 4 [40700/49669]\tLoss: 0.0094\n",
      "Training Epoch: 4 [40750/49669]\tLoss: 0.0085\n",
      "Training Epoch: 4 [40800/49669]\tLoss: 0.0071\n",
      "Training Epoch: 4 [40850/49669]\tLoss: 0.0055\n",
      "Training Epoch: 4 [40900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [40950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [41000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [41050/49669]\tLoss: 0.0052\n",
      "Training Epoch: 4 [41100/49669]\tLoss: 0.0057\n",
      "Training Epoch: 4 [41150/49669]\tLoss: 0.0059\n",
      "Training Epoch: 4 [41200/49669]\tLoss: 0.0052\n",
      "Training Epoch: 4 [41250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [41300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [41350/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [41400/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [41450/49669]\tLoss: 0.0059\n",
      "Training Epoch: 4 [41500/49669]\tLoss: 0.0060\n",
      "Training Epoch: 4 [41550/49669]\tLoss: 0.0058\n",
      "Training Epoch: 4 [41600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 4 [41650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [41700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [41750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [41800/49669]\tLoss: 0.0050\n",
      "Training Epoch: 4 [41850/49669]\tLoss: 0.0053\n",
      "Training Epoch: 4 [41900/49669]\tLoss: 0.0051\n",
      "Training Epoch: 4 [41950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [42000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [42150/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [42200/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [42250/49669]\tLoss: 0.0048\n",
      "Training Epoch: 4 [42300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [42350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [42450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [42600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [42650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [42700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [42950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [43000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [43050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [43100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [43150/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [43200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [43250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [43300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [43350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [43400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [43450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [43500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [43550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [43600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [43650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [43700/49669]\tLoss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [43750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [43800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [43850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [43900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [43950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [44000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [44050/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [44100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [44150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [44200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [44250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [44300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [44350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [44400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [44450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [44500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [44550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [44600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [44650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [44700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [44750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [44800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [44850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [44900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [44950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [45000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [45050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [45100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [45150/49669]\tLoss: 0.0037\n",
      "Training Epoch: 4 [45200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [45250/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [45300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [45350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [45400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [45450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [45500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [45550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [45600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [45650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [45700/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [45750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [45800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [45850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [45900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [45950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [46000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [46050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [46100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [46150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [46300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [46350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [46400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [46500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [46550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [46600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [46650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [46700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [46900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [46950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [47000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [47100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [47150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [47200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [47250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [47300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [47400/49669]\tLoss: 0.0036\n",
      "Training Epoch: 4 [47450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [47600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [47650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [47750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [47850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [47900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [47950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [48000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [48050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [48100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [48150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [48200/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [48250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 4 [48300/49669]\tLoss: 0.0037\n",
      "Training Epoch: 4 [48350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [48400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [48450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [48500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [48550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [48600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 4 [48650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [48700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [48750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [48800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 4 [48850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [48900/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [48950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [49000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [49050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [49100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 4 [49150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [49200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [49250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [49300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 4 [49350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [49400/49669]\tLoss: 0.0038\n",
      "Training Epoch: 4 [49450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [49500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [49550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 4 [49600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [49650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 4 [49669/49669]\tLoss: 0.0045\n",
      "Training Epoch: 4 [5518/5518]\tLoss: 0.0041\n",
      "Training Epoch: 5 [50/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [1000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [1050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [1100/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [1150/49669]\tLoss: 0.0050\n",
      "Training Epoch: 5 [1200/49669]\tLoss: 0.0053\n",
      "Training Epoch: 5 [1250/49669]\tLoss: 0.0055\n",
      "Training Epoch: 5 [1300/49669]\tLoss: 0.0062\n",
      "Training Epoch: 5 [1350/49669]\tLoss: 0.0066\n",
      "Training Epoch: 5 [1400/49669]\tLoss: 0.0072\n",
      "Training Epoch: 5 [1450/49669]\tLoss: 0.0074\n",
      "Training Epoch: 5 [1500/49669]\tLoss: 0.0082\n",
      "Training Epoch: 5 [1550/49669]\tLoss: 0.0092\n",
      "Training Epoch: 5 [1600/49669]\tLoss: 0.0098\n",
      "Training Epoch: 5 [1650/49669]\tLoss: 0.0100\n",
      "Training Epoch: 5 [1700/49669]\tLoss: 0.0086\n",
      "Training Epoch: 5 [1750/49669]\tLoss: 0.0061\n",
      "Training Epoch: 5 [1800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [1850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [1900/49669]\tLoss: 0.0058\n",
      "Training Epoch: 5 [1950/49669]\tLoss: 0.0080\n",
      "Training Epoch: 5 [2000/49669]\tLoss: 0.0089\n",
      "Training Epoch: 5 [2050/49669]\tLoss: 0.0079\n",
      "Training Epoch: 5 [2100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 5 [2150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [2200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [2250/49669]\tLoss: 0.0054\n",
      "Training Epoch: 5 [2300/49669]\tLoss: 0.0063\n",
      "Training Epoch: 5 [2350/49669]\tLoss: 0.0063\n",
      "Training Epoch: 5 [2400/49669]\tLoss: 0.0051\n",
      "Training Epoch: 5 [2450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [2500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [2550/49669]\tLoss: 0.0052\n",
      "Training Epoch: 5 [2600/49669]\tLoss: 0.0061\n",
      "Training Epoch: 5 [2650/49669]\tLoss: 0.0063\n",
      "Training Epoch: 5 [2700/49669]\tLoss: 0.0056\n",
      "Training Epoch: 5 [2750/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [2800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [2850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [2900/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [2950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [3000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [3050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [3100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [3150/49669]\tLoss: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [3200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [3250/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [3300/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [3350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [3400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [3450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [3500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [3550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [3600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [3650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [3700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [3750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [3800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [3850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [3900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [3950/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [4000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [4050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [4100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [4150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [4200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [4250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [4300/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [4350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [4400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [4450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [4500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [4550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [4600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [4650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [4700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [4750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [4800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [4850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [4900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [4950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [5000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [5050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [5200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [5250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [5300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [5450/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [5500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [5550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [5600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [5650/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [5700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [5850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [5900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [5950/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [6000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [6050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [6100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [6150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [6200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [6250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [6300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [6350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [6400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [6450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [6500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [6550/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [6600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [6650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [6700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [6750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [6800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [6850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [6900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [6950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [7000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [7050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [7100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [7150/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [7200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [7250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [7300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [7350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [7400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [7450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [7500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [7550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [7600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [7650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [7700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [7750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [7800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [7850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [7900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [7950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [8000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [8100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [8150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [8250/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [8300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [8350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [8400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [8550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [8650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [8700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [8800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [8850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [8900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [8950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [9000/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [9050/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [9100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [9150/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [9200/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [9250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [9300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [9350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [9400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [9450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [9500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [9550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [9600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [9650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [9700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [9750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [9800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [9850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [9900/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [9950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [10000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [10050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [10100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [10150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [10300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [10350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [10400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [10450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10500/49669]\tLoss: 0.0036\n",
      "Training Epoch: 5 [10550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [10700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [10750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [10800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10850/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [10900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [10950/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [11000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [11050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [11100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [11150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [11200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [11250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [11300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [11350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [11400/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [11450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [11500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [11550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [11600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [11650/49669]\tLoss: 0.0036\n",
      "Training Epoch: 5 [11700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [11750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [11800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [11850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [11900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [11950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [12000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [12050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [12100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [12150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [12200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [12250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [12300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [12350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [12400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [12450/49669]\tLoss: 0.0038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [12500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [12550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [12600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [12650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [12700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [12750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [12800/49669]\tLoss: 0.0053\n",
      "Training Epoch: 5 [12850/49669]\tLoss: 0.0060\n",
      "Training Epoch: 5 [12900/49669]\tLoss: 0.0080\n",
      "Training Epoch: 5 [12950/49669]\tLoss: 0.0110\n",
      "Training Epoch: 5 [13000/49669]\tLoss: 0.0144\n",
      "Training Epoch: 5 [13050/49669]\tLoss: 0.0151\n",
      "Training Epoch: 5 [13100/49669]\tLoss: 0.0138\n",
      "Training Epoch: 5 [13150/49669]\tLoss: 0.0106\n",
      "Training Epoch: 5 [13200/49669]\tLoss: 0.0077\n",
      "Training Epoch: 5 [13250/49669]\tLoss: 0.0059\n",
      "Training Epoch: 5 [13300/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [13350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [13400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [13450/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [13500/49669]\tLoss: 0.0052\n",
      "Training Epoch: 5 [13550/49669]\tLoss: 0.0054\n",
      "Training Epoch: 5 [13600/49669]\tLoss: 0.0051\n",
      "Training Epoch: 5 [13650/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [13700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [13750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [13800/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [13850/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [13900/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [13950/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [14000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [14050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [14100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [14150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [14200/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [14250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [14300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [14350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [14400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [14450/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [14500/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [14550/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [14600/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [14650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [14700/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [14750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [14800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [14850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [14900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [14950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [15000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [15050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [15100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15150/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [15200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [15300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [15500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [15550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [15650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [15700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [15750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [15800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [15850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [15900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [15950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [16000/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [16050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [16100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [16150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [16200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [16250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [16300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [16350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [16400/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [16450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [16500/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [16550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [16600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [16650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [16700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [16750/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [16800/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [16850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [16900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [16950/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [17000/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [17050/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [17100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [17150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [17200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17250/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [17300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [17350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [17400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [17550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [17700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [17750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [17800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [17900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [17950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [18000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [18050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [18100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [18150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [18200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [18250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [18300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [18350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [18400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [18450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [18500/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [18550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [18600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [18650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [18700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [18750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [18800/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [18850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [18900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [18950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [19000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [19050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [19100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [19150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [19200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [19250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [19300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [19350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [19400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [19450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [19500/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [19550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [19600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [19650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [19700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [19750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [19800/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [19850/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [19900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [19950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [20000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [20050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [20100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [20150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [20200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [20250/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [20300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [20350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [20400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [20450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [20500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [20550/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [20600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [20650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [20700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [20750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [20800/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [20850/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [20900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [20950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [21000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [21050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [21100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [21150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [21200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [21250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [21300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [21350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [21400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [21450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [21500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [21550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [21600/49669]\tLoss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [21650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [21700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [21750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [21800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [21850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [21900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [21950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [22000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [22050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [22100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [22150/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [22200/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [22250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [22300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [22350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [22400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [22450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [22500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [22550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [22600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [22650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [22700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [22750/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [22800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [22850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [22900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [22950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [23000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [23050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [23100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [23150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [23200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [23250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [23300/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [23350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [23400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [23450/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [23500/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [23550/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [23600/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [23650/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [23700/49669]\tLoss: 0.0055\n",
      "Training Epoch: 5 [23750/49669]\tLoss: 0.0058\n",
      "Training Epoch: 5 [23800/49669]\tLoss: 0.0062\n",
      "Training Epoch: 5 [23850/49669]\tLoss: 0.0067\n",
      "Training Epoch: 5 [23900/49669]\tLoss: 0.0071\n",
      "Training Epoch: 5 [23950/49669]\tLoss: 0.0078\n",
      "Training Epoch: 5 [24000/49669]\tLoss: 0.0079\n",
      "Training Epoch: 5 [24050/49669]\tLoss: 0.0072\n",
      "Training Epoch: 5 [24100/49669]\tLoss: 0.0059\n",
      "Training Epoch: 5 [24150/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [24200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [24250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [24300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [24350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [24400/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [24450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 5 [24500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [24550/49669]\tLoss: 0.0051\n",
      "Training Epoch: 5 [24600/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [24650/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [24700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [24750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [24800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [24850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [24900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [24950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [25000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [25050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [25100/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [25150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [25200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [25250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [25300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [25350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [25400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [25450/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [25500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [25550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [25600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [25650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [25700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [25750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [25800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [25850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [25900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [25950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [26000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [26050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [26100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [26150/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [26200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [26250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [26300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [26350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [26400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [26500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26550/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [26600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [26850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26900/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [26950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [27000/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [27050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [27100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [27150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [27200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [27250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [27300/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [27350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [27400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [27450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [27500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [27550/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [27600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [27650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [27700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [27750/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [27800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [27850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [27900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [27950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [28000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [28050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [28100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [28150/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28250/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [28300/49669]\tLoss: 0.0036\n",
      "Training Epoch: 5 [28350/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [28400/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [28500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [28600/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [28650/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28700/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [28800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [28850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [28900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [28950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [29000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [29050/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [29100/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [29150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [29200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [29250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [29300/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [29350/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [29400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [29450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [29500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [29550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [29600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [29650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [29700/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [29750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [29800/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [29850/49669]\tLoss: 0.0055\n",
      "Training Epoch: 5 [29900/49669]\tLoss: 0.0063\n",
      "Training Epoch: 5 [29950/49669]\tLoss: 0.0074\n",
      "Training Epoch: 5 [30000/49669]\tLoss: 0.0088\n",
      "Training Epoch: 5 [30050/49669]\tLoss: 0.0102\n",
      "Training Epoch: 5 [30100/49669]\tLoss: 0.0112\n",
      "Training Epoch: 5 [30150/49669]\tLoss: 0.0098\n",
      "Training Epoch: 5 [30200/49669]\tLoss: 0.0071\n",
      "Training Epoch: 5 [30250/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [30300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [30350/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [30400/49669]\tLoss: 0.0069\n",
      "Training Epoch: 5 [30450/49669]\tLoss: 0.0083\n",
      "Training Epoch: 5 [30500/49669]\tLoss: 0.0084\n",
      "Training Epoch: 5 [30550/49669]\tLoss: 0.0070\n",
      "Training Epoch: 5 [30600/49669]\tLoss: 0.0050\n",
      "Training Epoch: 5 [30650/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [30700/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [30750/49669]\tLoss: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [30800/49669]\tLoss: 0.0069\n",
      "Training Epoch: 5 [30850/49669]\tLoss: 0.0077\n",
      "Training Epoch: 5 [30900/49669]\tLoss: 0.0083\n",
      "Training Epoch: 5 [30950/49669]\tLoss: 0.0071\n",
      "Training Epoch: 5 [31000/49669]\tLoss: 0.0051\n",
      "Training Epoch: 5 [31050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [31100/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [31150/49669]\tLoss: 0.0057\n",
      "Training Epoch: 5 [31200/49669]\tLoss: 0.0063\n",
      "Training Epoch: 5 [31250/49669]\tLoss: 0.0061\n",
      "Training Epoch: 5 [31300/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [31350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [31400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [31450/49669]\tLoss: 0.0048\n",
      "Training Epoch: 5 [31500/49669]\tLoss: 0.0051\n",
      "Training Epoch: 5 [31550/49669]\tLoss: 0.0053\n",
      "Training Epoch: 5 [31600/49669]\tLoss: 0.0051\n",
      "Training Epoch: 5 [31650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [31700/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [31750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [31800/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [31850/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [31900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [31950/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [32000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32050/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [32100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [32200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [32250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [32300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [32350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [32450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [32550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [32600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [32750/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [32800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [32850/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [32900/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [32950/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [33000/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [33050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [33100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [33150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [33200/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [33250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [33300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [33350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [33400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [33450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [33500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [33550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [33600/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [33650/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [33700/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [33750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [33800/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [33850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [33900/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [33950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [34000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [34150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [34250/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [34300/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34400/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [34450/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [34500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [34550/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [34600/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [34700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [34750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [34800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [34850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [34900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [34950/49669]\tLoss: 0.0035\n",
      "Training Epoch: 5 [35000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [35050/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [35100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [35150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [35200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [35250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [35300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [35350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [35400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [35450/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [35500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [35550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [35600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [35650/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [35700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [35750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [35800/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [35850/49669]\tLoss: 0.0036\n",
      "Training Epoch: 5 [35900/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [35950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [36000/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [36050/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [36100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [36150/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [36200/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [36250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [36300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [36350/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [36400/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [36450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [36500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [36550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [36600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [36650/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [36700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [36750/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [36800/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [36850/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [36900/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [36950/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [37000/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [37100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [37150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [37250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [37300/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [37350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37400/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [37500/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [37550/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [37600/49669]\tLoss: 0.0036\n",
      "Training Epoch: 5 [37650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37700/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [37750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37800/49669]\tLoss: 0.0037\n",
      "Training Epoch: 5 [37850/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [37900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [37950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [38000/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [38050/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [38100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [38150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [38200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [38250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [38300/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [38350/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [38400/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [38450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [38500/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [38550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [38600/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [38650/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [38700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [38750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [38800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [38850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [38900/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [38950/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [39000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [39050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [39100/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [39150/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [39200/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [39250/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [39300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [39350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [39400/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [39450/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [39500/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [39550/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [39600/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [39650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [39700/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [39750/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [39800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [39850/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [39900/49669]\tLoss: 0.0038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [39950/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [40000/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [40050/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [40100/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [40150/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [40200/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [40250/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [40300/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [40350/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [40400/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [40450/49669]\tLoss: 0.0050\n",
      "Training Epoch: 5 [40500/49669]\tLoss: 0.0053\n",
      "Training Epoch: 5 [40550/49669]\tLoss: 0.0063\n",
      "Training Epoch: 5 [40600/49669]\tLoss: 0.0070\n",
      "Training Epoch: 5 [40650/49669]\tLoss: 0.0088\n",
      "Training Epoch: 5 [40700/49669]\tLoss: 0.0108\n",
      "Training Epoch: 5 [40750/49669]\tLoss: 0.0117\n",
      "Training Epoch: 5 [40800/49669]\tLoss: 0.0110\n",
      "Training Epoch: 5 [40850/49669]\tLoss: 0.0091\n",
      "Training Epoch: 5 [40900/49669]\tLoss: 0.0071\n",
      "Training Epoch: 5 [40950/49669]\tLoss: 0.0057\n",
      "Training Epoch: 5 [41000/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [41050/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [41100/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [41150/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [41200/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [41250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [41300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [41350/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [41400/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [41450/49669]\tLoss: 0.0047\n",
      "Training Epoch: 5 [41500/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [41550/49669]\tLoss: 0.0049\n",
      "Training Epoch: 5 [41600/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [41650/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [41700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [41750/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [41800/49669]\tLoss: 0.0043\n",
      "Training Epoch: 5 [41850/49669]\tLoss: 0.0045\n",
      "Training Epoch: 5 [41900/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [41950/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [42000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42050/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [42100/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [42150/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [42200/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [42250/49669]\tLoss: 0.0046\n",
      "Training Epoch: 5 [42300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42350/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42400/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42450/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [42500/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [42550/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [42600/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [42650/49669]\tLoss: 0.0044\n",
      "Training Epoch: 5 [42700/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42800/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42850/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [42900/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [42950/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43000/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43100/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [43150/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [43200/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43250/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [43300/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43350/49669]\tLoss: 0.0040\n",
      "Training Epoch: 5 [43400/49669]\tLoss: 0.0042\n",
      "Training Epoch: 5 [43450/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43500/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43550/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [43600/49669]\tLoss: 0.0038\n",
      "Training Epoch: 5 [43650/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [43700/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [43750/49669]\tLoss: 0.0041\n",
      "Training Epoch: 5 [43800/49669]\tLoss: 0.0039\n",
      "Training Epoch: 5 [43850/49669]\tLoss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ac529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
