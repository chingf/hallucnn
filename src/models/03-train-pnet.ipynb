{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_a1 import PBranchedNetwork_A1SeparateHP\n",
    "PNetClass = PBranchedNetwork_A1SeparateHP\n",
    "pnet_name = 'a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv1 import PBranchedNetwork_Conv1SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv1SeparateHP\n",
    "pnet_name = 'conv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'\n",
    "train_datafile = f'{engram_dir}seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 21 17:32:30 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:B1:00.0 Off |                  N/A |\r\n",
      "|  0%   27C    P8    11W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.load_state_dict(torch.load(\n",
    "    f\"{checkpoints_dir}all/all-25-regular.pth\",\n",
    "    map_location='cpu'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up TrainSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainCleanSoundsDataset(train_datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up PsychophysicsSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = h5py.File(f\"{engram_dir}PsychophysicsWord2017W_not_resampled.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metadata = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META_key.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPsychophysics2017WCleanCochleagrams():\n",
    "    \n",
    "    cochleagrams_clean = []\n",
    "   \n",
    "    cochleagrams = []\n",
    "    for batch_ii in range(0,15300,100):\n",
    "        hdf5_path = '/mnt/smb/locker/issa-locker/users/Erica/cgrams_for_noise_robustness_analysis/PsychophysicsWord2017W_clean/batch_'+str(batch_ii)+'_to_'+str(batch_ii+100)+'.hdf5'\n",
    "        with h5py.File(hdf5_path, 'r') as f_in:\n",
    "            cochleagrams += list(f_in['data'])\n",
    "\n",
    "    return cochleagrams\n",
    "clean_in = getPsychophysics2017WCleanCochleagrams()\n",
    "clean_in = np.array(clean_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for word in f_metadata['word']:\n",
    "    idx = np.argwhere(f_key == word)\n",
    "    if len(idx) == 0:\n",
    "        labels.append(-1)\n",
    "    else:\n",
    "        labels.append(idx.item())\n",
    "labels = np.array(labels)\n",
    "labels += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dset = []\n",
    "for _orig_dset in f_metadata['orig_dset']:\n",
    "    _orig_dset = str(_orig_dset, 'utf-8')\n",
    "    _orig_dset = 'WSJ' if 'WSJ' in _orig_dset else 'Timit'\n",
    "    orig_dset.append(_orig_dset)\n",
    "orig_dset = np.array(orig_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psycho_dataset = PsychophysicsCleanSoundsDataset(\n",
    "    clean_in, labels, orig_dset, exclude_timit=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del orig_dset\n",
    "del clean_in\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55188, 1, 164, 400])\n"
     ]
    }
   ],
   "source": [
    "full_dataset = CleanSoundsDataset(train_dataset, psycho_dataset)\n",
    "del train_dataset\n",
    "del psycho_dataset\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [64/49669]\tLoss: 418.6643\n",
      "Training Epoch: 1 [128/49669]\tLoss: 5880.7471\n",
      "Training Epoch: 1 [192/49669]\tLoss: 763.0746\n",
      "Training Epoch: 1 [256/49669]\tLoss: 1562.0483\n",
      "Training Epoch: 1 [320/49669]\tLoss: 3871.1531\n",
      "Training Epoch: 1 [384/49669]\tLoss: 2819.1885\n",
      "Training Epoch: 1 [448/49669]\tLoss: 1059.6582\n",
      "Training Epoch: 1 [512/49669]\tLoss: 469.8101\n",
      "Training Epoch: 1 [576/49669]\tLoss: 1264.3372\n",
      "Training Epoch: 1 [640/49669]\tLoss: 2022.7345\n",
      "Training Epoch: 1 [704/49669]\tLoss: 1959.6693\n",
      "Training Epoch: 1 [768/49669]\tLoss: 1060.3384\n",
      "Training Epoch: 1 [832/49669]\tLoss: 499.5949\n",
      "Training Epoch: 1 [896/49669]\tLoss: 499.8237\n",
      "Training Epoch: 1 [960/49669]\tLoss: 933.9595\n",
      "Training Epoch: 1 [1024/49669]\tLoss: 1326.8002\n",
      "Training Epoch: 1 [1088/49669]\tLoss: 1120.5374\n",
      "Training Epoch: 1 [1152/49669]\tLoss: 715.4351\n",
      "Training Epoch: 1 [1216/49669]\tLoss: 467.3267\n",
      "Training Epoch: 1 [1280/49669]\tLoss: 478.2563\n",
      "Training Epoch: 1 [1344/49669]\tLoss: 671.1796\n",
      "Training Epoch: 1 [1408/49669]\tLoss: 908.0928\n",
      "Training Epoch: 1 [1472/49669]\tLoss: 787.3925\n",
      "Training Epoch: 1 [1536/49669]\tLoss: 622.7324\n",
      "Training Epoch: 1 [1600/49669]\tLoss: 430.5251\n",
      "Training Epoch: 1 [1664/49669]\tLoss: 453.2581\n",
      "Training Epoch: 1 [1728/49669]\tLoss: 574.3110\n",
      "Training Epoch: 1 [1792/49669]\tLoss: 616.8923\n",
      "Training Epoch: 1 [1856/49669]\tLoss: 609.4609\n",
      "Training Epoch: 1 [1920/49669]\tLoss: 508.2376\n",
      "Training Epoch: 1 [1984/49669]\tLoss: 456.6025\n",
      "Training Epoch: 1 [2048/49669]\tLoss: 434.1150\n",
      "Training Epoch: 1 [2112/49669]\tLoss: 524.8809\n",
      "Training Epoch: 1 [2176/49669]\tLoss: 582.3936\n",
      "Training Epoch: 1 [2240/49669]\tLoss: 521.6693\n",
      "Training Epoch: 1 [2304/49669]\tLoss: 465.8174\n",
      "Training Epoch: 1 [2368/49669]\tLoss: 446.0530\n",
      "Training Epoch: 1 [2432/49669]\tLoss: 447.3633\n",
      "Training Epoch: 1 [2496/49669]\tLoss: 497.5123\n",
      "Training Epoch: 1 [2560/49669]\tLoss: 448.0365\n",
      "Training Epoch: 1 [2624/49669]\tLoss: 454.4978\n",
      "Training Epoch: 1 [2688/49669]\tLoss: 442.2770\n",
      "Training Epoch: 1 [2752/49669]\tLoss: 459.1399\n",
      "Training Epoch: 1 [2816/49669]\tLoss: 472.8970\n",
      "Training Epoch: 1 [2880/49669]\tLoss: 456.8250\n",
      "Training Epoch: 1 [2944/49669]\tLoss: 465.1762\n",
      "Training Epoch: 1 [3008/49669]\tLoss: 416.2169\n",
      "Training Epoch: 1 [3072/49669]\tLoss: 440.2227\n",
      "Training Epoch: 1 [3136/49669]\tLoss: 449.7419\n",
      "Training Epoch: 1 [3200/49669]\tLoss: 486.8011\n",
      "Training Epoch: 1 [3264/49669]\tLoss: 425.3600\n",
      "Training Epoch: 1 [3328/49669]\tLoss: 406.3841\n",
      "Training Epoch: 1 [3392/49669]\tLoss: 439.3392\n",
      "Training Epoch: 1 [3456/49669]\tLoss: 402.1392\n",
      "Training Epoch: 1 [3520/49669]\tLoss: 425.2948\n",
      "Training Epoch: 1 [3584/49669]\tLoss: 440.1729\n",
      "Training Epoch: 1 [3648/49669]\tLoss: 429.2799\n",
      "Training Epoch: 1 [3712/49669]\tLoss: 419.1008\n",
      "Training Epoch: 1 [3776/49669]\tLoss: 451.0100\n",
      "Training Epoch: 1 [3840/49669]\tLoss: 429.5578\n",
      "Training Epoch: 1 [3904/49669]\tLoss: 434.0996\n",
      "Training Epoch: 1 [3968/49669]\tLoss: 440.4840\n",
      "Training Epoch: 1 [4032/49669]\tLoss: 396.7491\n",
      "Training Epoch: 1 [4096/49669]\tLoss: 450.6045\n",
      "Training Epoch: 1 [4160/49669]\tLoss: 429.9083\n",
      "Training Epoch: 1 [4224/49669]\tLoss: 456.4558\n",
      "Training Epoch: 1 [4288/49669]\tLoss: 461.5139\n",
      "Training Epoch: 1 [4352/49669]\tLoss: 443.6490\n",
      "Training Epoch: 1 [4416/49669]\tLoss: 442.3024\n",
      "Training Epoch: 1 [4480/49669]\tLoss: 418.2100\n",
      "Training Epoch: 1 [4544/49669]\tLoss: 434.5271\n",
      "Training Epoch: 1 [4608/49669]\tLoss: 424.2183\n",
      "Training Epoch: 1 [4672/49669]\tLoss: 442.8428\n",
      "Training Epoch: 1 [4736/49669]\tLoss: 427.6743\n",
      "Training Epoch: 1 [4800/49669]\tLoss: 393.8459\n",
      "Training Epoch: 1 [4864/49669]\tLoss: 421.9189\n",
      "Training Epoch: 1 [4928/49669]\tLoss: 438.1988\n",
      "Training Epoch: 1 [4992/49669]\tLoss: 413.9879\n",
      "Training Epoch: 1 [5056/49669]\tLoss: 426.0330\n",
      "Training Epoch: 1 [5120/49669]\tLoss: 438.3943\n",
      "Training Epoch: 1 [5184/49669]\tLoss: 436.1026\n",
      "Training Epoch: 1 [5248/49669]\tLoss: 436.3157\n",
      "Training Epoch: 1 [5312/49669]\tLoss: 422.7489\n",
      "Training Epoch: 1 [5376/49669]\tLoss: 430.1046\n",
      "Training Epoch: 1 [5440/49669]\tLoss: 429.4572\n",
      "Training Epoch: 1 [5504/49669]\tLoss: 429.1761\n",
      "Training Epoch: 1 [5568/49669]\tLoss: 426.6980\n",
      "Training Epoch: 1 [5632/49669]\tLoss: 428.8036\n",
      "Training Epoch: 1 [5696/49669]\tLoss: 447.5806\n",
      "Training Epoch: 1 [5760/49669]\tLoss: 392.7477\n",
      "Training Epoch: 1 [5824/49669]\tLoss: 439.8148\n",
      "Training Epoch: 1 [5888/49669]\tLoss: 416.1318\n",
      "Training Epoch: 1 [5952/49669]\tLoss: 429.0416\n",
      "Training Epoch: 1 [6016/49669]\tLoss: 411.4154\n",
      "Training Epoch: 1 [6080/49669]\tLoss: 427.2933\n",
      "Training Epoch: 1 [6144/49669]\tLoss: 445.3332\n",
      "Training Epoch: 1 [6208/49669]\tLoss: 449.0516\n",
      "Training Epoch: 1 [6272/49669]\tLoss: 434.6336\n",
      "Training Epoch: 1 [6336/49669]\tLoss: 436.6505\n",
      "Training Epoch: 1 [6400/49669]\tLoss: 420.3020\n",
      "Training Epoch: 1 [6464/49669]\tLoss: 415.6302\n",
      "Training Epoch: 1 [6528/49669]\tLoss: 444.2326\n",
      "Training Epoch: 1 [6592/49669]\tLoss: 399.3263\n",
      "Training Epoch: 1 [6656/49669]\tLoss: 437.6781\n",
      "Training Epoch: 1 [6720/49669]\tLoss: 414.5360\n",
      "Training Epoch: 1 [6784/49669]\tLoss: 411.6057\n",
      "Training Epoch: 1 [6848/49669]\tLoss: 393.1289\n",
      "Training Epoch: 1 [6912/49669]\tLoss: 414.3521\n",
      "Training Epoch: 1 [6976/49669]\tLoss: 424.9119\n",
      "Training Epoch: 1 [7040/49669]\tLoss: 402.9620\n",
      "Training Epoch: 1 [7104/49669]\tLoss: 429.9725\n",
      "Training Epoch: 1 [7168/49669]\tLoss: 407.9792\n",
      "Training Epoch: 1 [7232/49669]\tLoss: 439.6713\n",
      "Training Epoch: 1 [7296/49669]\tLoss: 412.4373\n",
      "Training Epoch: 1 [7360/49669]\tLoss: 421.6740\n",
      "Training Epoch: 1 [7424/49669]\tLoss: 424.4361\n",
      "Training Epoch: 1 [7488/49669]\tLoss: 406.8430\n",
      "Training Epoch: 1 [7552/49669]\tLoss: 446.9648\n",
      "Training Epoch: 1 [7616/49669]\tLoss: 405.9750\n",
      "Training Epoch: 1 [7680/49669]\tLoss: 420.9802\n",
      "Training Epoch: 1 [7744/49669]\tLoss: 440.7271\n",
      "Training Epoch: 1 [7808/49669]\tLoss: 451.9591\n",
      "Training Epoch: 1 [7872/49669]\tLoss: 451.9322\n",
      "Training Epoch: 1 [7936/49669]\tLoss: 452.3055\n",
      "Training Epoch: 1 [8000/49669]\tLoss: 444.5550\n",
      "Training Epoch: 1 [8064/49669]\tLoss: 412.0085\n",
      "Training Epoch: 1 [8128/49669]\tLoss: 429.8846\n",
      "Training Epoch: 1 [8192/49669]\tLoss: 403.8131\n",
      "Training Epoch: 1 [8256/49669]\tLoss: 431.3781\n",
      "Training Epoch: 1 [8320/49669]\tLoss: 430.0981\n",
      "Training Epoch: 1 [8384/49669]\tLoss: 439.5430\n",
      "Training Epoch: 1 [8448/49669]\tLoss: 425.3549\n",
      "Training Epoch: 1 [8512/49669]\tLoss: 397.5841\n",
      "Training Epoch: 1 [8576/49669]\tLoss: 406.8152\n",
      "Training Epoch: 1 [8640/49669]\tLoss: 415.9416\n",
      "Training Epoch: 1 [8704/49669]\tLoss: 424.5225\n",
      "Training Epoch: 1 [8768/49669]\tLoss: 413.6769\n",
      "Training Epoch: 1 [8832/49669]\tLoss: 452.0946\n",
      "Training Epoch: 1 [8896/49669]\tLoss: 429.1162\n",
      "Training Epoch: 1 [8960/49669]\tLoss: 462.0321\n",
      "Training Epoch: 1 [9024/49669]\tLoss: 421.2166\n",
      "Training Epoch: 1 [9088/49669]\tLoss: 413.9808\n",
      "Training Epoch: 1 [9152/49669]\tLoss: 426.2791\n",
      "Training Epoch: 1 [9216/49669]\tLoss: 417.0712\n",
      "Training Epoch: 1 [9280/49669]\tLoss: 408.5802\n",
      "Training Epoch: 1 [9344/49669]\tLoss: 414.3944\n",
      "Training Epoch: 1 [9408/49669]\tLoss: 405.3604\n",
      "Training Epoch: 1 [9472/49669]\tLoss: 440.0711\n",
      "Training Epoch: 1 [9536/49669]\tLoss: 423.7516\n",
      "Training Epoch: 1 [9600/49669]\tLoss: 431.6880\n",
      "Training Epoch: 1 [9664/49669]\tLoss: 412.9849\n",
      "Training Epoch: 1 [9728/49669]\tLoss: 420.4005\n",
      "Training Epoch: 1 [9792/49669]\tLoss: 405.9897\n",
      "Training Epoch: 1 [9856/49669]\tLoss: 431.5748\n",
      "Training Epoch: 1 [9920/49669]\tLoss: 454.4397\n",
      "Training Epoch: 1 [9984/49669]\tLoss: 451.0258\n",
      "Training Epoch: 1 [10048/49669]\tLoss: 406.8009\n",
      "Training Epoch: 1 [10112/49669]\tLoss: 395.4317\n",
      "Training Epoch: 1 [10176/49669]\tLoss: 424.0232\n",
      "Training Epoch: 1 [10240/49669]\tLoss: 415.6342\n",
      "Training Epoch: 1 [10304/49669]\tLoss: 413.7583\n",
      "Training Epoch: 1 [10368/49669]\tLoss: 411.0763\n",
      "Training Epoch: 1 [10432/49669]\tLoss: 399.0737\n",
      "Training Epoch: 1 [10496/49669]\tLoss: 452.5910\n",
      "Training Epoch: 1 [10560/49669]\tLoss: 439.5534\n",
      "Training Epoch: 1 [10624/49669]\tLoss: 400.4483\n",
      "Training Epoch: 1 [10688/49669]\tLoss: 417.6938\n",
      "Training Epoch: 1 [10752/49669]\tLoss: 437.8239\n",
      "Training Epoch: 1 [10816/49669]\tLoss: 453.0246\n",
      "Training Epoch: 1 [10880/49669]\tLoss: 412.2032\n",
      "Training Epoch: 1 [10944/49669]\tLoss: 425.7774\n",
      "Training Epoch: 1 [11008/49669]\tLoss: 443.4093\n",
      "Training Epoch: 1 [11072/49669]\tLoss: 408.5661\n",
      "Training Epoch: 1 [11136/49669]\tLoss: 400.4713\n",
      "Training Epoch: 1 [11200/49669]\tLoss: 408.9504\n",
      "Training Epoch: 1 [11264/49669]\tLoss: 409.7918\n",
      "Training Epoch: 1 [11328/49669]\tLoss: 413.6652\n",
      "Training Epoch: 1 [11392/49669]\tLoss: 421.5058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [11456/49669]\tLoss: 450.7382\n",
      "Training Epoch: 1 [11520/49669]\tLoss: 381.8687\n",
      "Training Epoch: 1 [11584/49669]\tLoss: 434.0299\n",
      "Training Epoch: 1 [11648/49669]\tLoss: 449.0807\n",
      "Training Epoch: 1 [11712/49669]\tLoss: 422.3297\n",
      "Training Epoch: 1 [11776/49669]\tLoss: 398.6059\n",
      "Training Epoch: 1 [11840/49669]\tLoss: 425.7851\n",
      "Training Epoch: 1 [11904/49669]\tLoss: 417.0107\n",
      "Training Epoch: 1 [11968/49669]\tLoss: 432.1522\n",
      "Training Epoch: 1 [12032/49669]\tLoss: 408.2046\n",
      "Training Epoch: 1 [12096/49669]\tLoss: 415.4198\n",
      "Training Epoch: 1 [12160/49669]\tLoss: 437.2926\n",
      "Training Epoch: 1 [12224/49669]\tLoss: 380.7126\n",
      "Training Epoch: 1 [12288/49669]\tLoss: 361.0434\n",
      "Training Epoch: 1 [12352/49669]\tLoss: 452.8088\n",
      "Training Epoch: 1 [12416/49669]\tLoss: 422.7009\n",
      "Training Epoch: 1 [12480/49669]\tLoss: 399.8064\n",
      "Training Epoch: 1 [12544/49669]\tLoss: 420.1967\n",
      "Training Epoch: 1 [12608/49669]\tLoss: 429.9666\n",
      "Training Epoch: 1 [12672/49669]\tLoss: 410.1173\n",
      "Training Epoch: 1 [12736/49669]\tLoss: 424.1634\n",
      "Training Epoch: 1 [12800/49669]\tLoss: 448.4467\n",
      "Training Epoch: 1 [12864/49669]\tLoss: 455.7085\n",
      "Training Epoch: 1 [12928/49669]\tLoss: 422.6604\n",
      "Training Epoch: 1 [12992/49669]\tLoss: 413.8552\n",
      "Training Epoch: 1 [13056/49669]\tLoss: 443.0676\n",
      "Training Epoch: 1 [13120/49669]\tLoss: 422.9167\n",
      "Training Epoch: 1 [13184/49669]\tLoss: 444.9046\n",
      "Training Epoch: 1 [13248/49669]\tLoss: 444.4854\n",
      "Training Epoch: 1 [13312/49669]\tLoss: 421.7654\n",
      "Training Epoch: 1 [13376/49669]\tLoss: 435.8993\n",
      "Training Epoch: 1 [13440/49669]\tLoss: 410.1960\n",
      "Training Epoch: 1 [13504/49669]\tLoss: 419.5414\n",
      "Training Epoch: 1 [13568/49669]\tLoss: 440.1865\n",
      "Training Epoch: 1 [13632/49669]\tLoss: 428.9202\n",
      "Training Epoch: 1 [13696/49669]\tLoss: 435.8785\n",
      "Training Epoch: 1 [13760/49669]\tLoss: 413.7042\n",
      "Training Epoch: 1 [13824/49669]\tLoss: 380.2993\n",
      "Training Epoch: 1 [13888/49669]\tLoss: 398.8701\n",
      "Training Epoch: 1 [13952/49669]\tLoss: 429.9606\n",
      "Training Epoch: 1 [14016/49669]\tLoss: 410.0057\n",
      "Training Epoch: 1 [14080/49669]\tLoss: 429.9180\n",
      "Training Epoch: 1 [14144/49669]\tLoss: 418.3807\n",
      "Training Epoch: 1 [14208/49669]\tLoss: 420.3114\n",
      "Training Epoch: 1 [14272/49669]\tLoss: 440.1024\n",
      "Training Epoch: 1 [14336/49669]\tLoss: 407.8481\n",
      "Training Epoch: 1 [14400/49669]\tLoss: 400.4889\n",
      "Training Epoch: 1 [14464/49669]\tLoss: 431.5515\n",
      "Training Epoch: 1 [14528/49669]\tLoss: 450.3634\n",
      "Training Epoch: 1 [14592/49669]\tLoss: 422.0764\n",
      "Training Epoch: 1 [14656/49669]\tLoss: 429.1606\n",
      "Training Epoch: 1 [14720/49669]\tLoss: 435.5384\n",
      "Training Epoch: 1 [14784/49669]\tLoss: 399.5872\n",
      "Training Epoch: 1 [14848/49669]\tLoss: 417.5865\n",
      "Training Epoch: 1 [14912/49669]\tLoss: 420.6084\n",
      "Training Epoch: 1 [14976/49669]\tLoss: 424.8980\n",
      "Training Epoch: 1 [15040/49669]\tLoss: 428.0588\n",
      "Training Epoch: 1 [15104/49669]\tLoss: 433.5209\n",
      "Training Epoch: 1 [15168/49669]\tLoss: 385.8062\n",
      "Training Epoch: 1 [15232/49669]\tLoss: 444.8517\n",
      "Training Epoch: 1 [15296/49669]\tLoss: 454.9041\n",
      "Training Epoch: 1 [15360/49669]\tLoss: 427.6320\n",
      "Training Epoch: 1 [15424/49669]\tLoss: 436.9236\n",
      "Training Epoch: 1 [15488/49669]\tLoss: 416.9218\n",
      "Training Epoch: 1 [15552/49669]\tLoss: 445.7183\n",
      "Training Epoch: 1 [15616/49669]\tLoss: 410.9649\n",
      "Training Epoch: 1 [15680/49669]\tLoss: 426.2598\n",
      "Training Epoch: 1 [15744/49669]\tLoss: 403.4321\n",
      "Training Epoch: 1 [15808/49669]\tLoss: 454.8871\n",
      "Training Epoch: 1 [15872/49669]\tLoss: 451.2687\n",
      "Training Epoch: 1 [15936/49669]\tLoss: 440.4176\n",
      "Training Epoch: 1 [16000/49669]\tLoss: 436.6549\n",
      "Training Epoch: 1 [16064/49669]\tLoss: 422.6339\n",
      "Training Epoch: 1 [16128/49669]\tLoss: 393.3780\n",
      "Training Epoch: 1 [16192/49669]\tLoss: 449.8102\n",
      "Training Epoch: 1 [16256/49669]\tLoss: 430.2524\n",
      "Training Epoch: 1 [16320/49669]\tLoss: 411.7126\n",
      "Training Epoch: 1 [16384/49669]\tLoss: 407.5783\n",
      "Training Epoch: 1 [16448/49669]\tLoss: 407.6511\n",
      "Training Epoch: 1 [16512/49669]\tLoss: 441.3102\n",
      "Training Epoch: 1 [16576/49669]\tLoss: 423.6396\n",
      "Training Epoch: 1 [16640/49669]\tLoss: 428.2672\n",
      "Training Epoch: 1 [16704/49669]\tLoss: 427.2165\n",
      "Training Epoch: 1 [16768/49669]\tLoss: 417.4369\n",
      "Training Epoch: 1 [16832/49669]\tLoss: 422.5081\n",
      "Training Epoch: 1 [16896/49669]\tLoss: 431.9565\n",
      "Training Epoch: 1 [16960/49669]\tLoss: 434.2680\n",
      "Training Epoch: 1 [17024/49669]\tLoss: 439.7798\n",
      "Training Epoch: 1 [17088/49669]\tLoss: 448.3752\n",
      "Training Epoch: 1 [17152/49669]\tLoss: 409.5315\n",
      "Training Epoch: 1 [17216/49669]\tLoss: 416.9093\n",
      "Training Epoch: 1 [17280/49669]\tLoss: 447.3337\n",
      "Training Epoch: 1 [17344/49669]\tLoss: 404.4169\n",
      "Training Epoch: 1 [17408/49669]\tLoss: 412.2167\n",
      "Training Epoch: 1 [17472/49669]\tLoss: 408.9843\n",
      "Training Epoch: 1 [17536/49669]\tLoss: 435.6055\n",
      "Training Epoch: 1 [17600/49669]\tLoss: 441.3136\n",
      "Training Epoch: 1 [17664/49669]\tLoss: 430.6242\n",
      "Training Epoch: 1 [17728/49669]\tLoss: 414.6439\n",
      "Training Epoch: 1 [17792/49669]\tLoss: 402.8501\n",
      "Training Epoch: 1 [17856/49669]\tLoss: 365.4098\n",
      "Training Epoch: 1 [17920/49669]\tLoss: 426.4742\n",
      "Training Epoch: 1 [17984/49669]\tLoss: 425.1505\n",
      "Training Epoch: 1 [18048/49669]\tLoss: 396.3370\n",
      "Training Epoch: 1 [18112/49669]\tLoss: 436.8263\n",
      "Training Epoch: 1 [18176/49669]\tLoss: 402.6649\n",
      "Training Epoch: 1 [18240/49669]\tLoss: 404.1784\n",
      "Training Epoch: 1 [18304/49669]\tLoss: 428.5069\n",
      "Training Epoch: 1 [18368/49669]\tLoss: 393.4765\n",
      "Training Epoch: 1 [18432/49669]\tLoss: 437.4019\n",
      "Training Epoch: 1 [18496/49669]\tLoss: 409.5480\n",
      "Training Epoch: 1 [18560/49669]\tLoss: 417.8352\n",
      "Training Epoch: 1 [18624/49669]\tLoss: 401.9793\n",
      "Training Epoch: 1 [18688/49669]\tLoss: 432.6963\n",
      "Training Epoch: 1 [18752/49669]\tLoss: 418.8899\n",
      "Training Epoch: 1 [18816/49669]\tLoss: 437.8763\n",
      "Training Epoch: 1 [18880/49669]\tLoss: 444.8790\n",
      "Training Epoch: 1 [18944/49669]\tLoss: 451.5468\n",
      "Training Epoch: 1 [19008/49669]\tLoss: 412.6763\n",
      "Training Epoch: 1 [19072/49669]\tLoss: 427.5649\n",
      "Training Epoch: 1 [19136/49669]\tLoss: 418.7980\n",
      "Training Epoch: 1 [19200/49669]\tLoss: 456.4189\n",
      "Training Epoch: 1 [19264/49669]\tLoss: 411.5888\n",
      "Training Epoch: 1 [19328/49669]\tLoss: 427.6923\n",
      "Training Epoch: 1 [19392/49669]\tLoss: 450.0210\n",
      "Training Epoch: 1 [19456/49669]\tLoss: 452.7256\n",
      "Training Epoch: 1 [19520/49669]\tLoss: 417.6340\n",
      "Training Epoch: 1 [19584/49669]\tLoss: 417.9946\n",
      "Training Epoch: 1 [19648/49669]\tLoss: 445.2865\n",
      "Training Epoch: 1 [19712/49669]\tLoss: 393.0337\n",
      "Training Epoch: 1 [19776/49669]\tLoss: 398.9695\n",
      "Training Epoch: 1 [19840/49669]\tLoss: 426.8493\n",
      "Training Epoch: 1 [19904/49669]\tLoss: 411.9405\n",
      "Training Epoch: 1 [19968/49669]\tLoss: 409.2697\n",
      "Training Epoch: 1 [20032/49669]\tLoss: 406.4752\n",
      "Training Epoch: 1 [20096/49669]\tLoss: 450.6991\n",
      "Training Epoch: 1 [20160/49669]\tLoss: 397.6618\n",
      "Training Epoch: 1 [20224/49669]\tLoss: 375.1891\n",
      "Training Epoch: 1 [20288/49669]\tLoss: 466.2839\n",
      "Training Epoch: 1 [20352/49669]\tLoss: 400.9658\n",
      "Training Epoch: 1 [20416/49669]\tLoss: 395.3976\n",
      "Training Epoch: 1 [20480/49669]\tLoss: 437.5658\n",
      "Training Epoch: 1 [20544/49669]\tLoss: 386.5800\n",
      "Training Epoch: 1 [20608/49669]\tLoss: 407.7225\n",
      "Training Epoch: 1 [20672/49669]\tLoss: 429.2570\n",
      "Training Epoch: 1 [20736/49669]\tLoss: 405.8672\n",
      "Training Epoch: 1 [20800/49669]\tLoss: 430.8948\n",
      "Training Epoch: 1 [20864/49669]\tLoss: 442.5576\n",
      "Training Epoch: 1 [20928/49669]\tLoss: 419.1483\n",
      "Training Epoch: 1 [20992/49669]\tLoss: 458.2006\n",
      "Training Epoch: 1 [21056/49669]\tLoss: 415.7368\n",
      "Training Epoch: 1 [21120/49669]\tLoss: 419.5473\n",
      "Training Epoch: 1 [21184/49669]\tLoss: 431.2400\n",
      "Training Epoch: 1 [21248/49669]\tLoss: 438.3885\n",
      "Training Epoch: 1 [21312/49669]\tLoss: 408.4869\n",
      "Training Epoch: 1 [21376/49669]\tLoss: 443.3334\n",
      "Training Epoch: 1 [21440/49669]\tLoss: 415.8174\n",
      "Training Epoch: 1 [21504/49669]\tLoss: 439.6888\n",
      "Training Epoch: 1 [21568/49669]\tLoss: 448.2870\n",
      "Training Epoch: 1 [21632/49669]\tLoss: 442.1777\n",
      "Training Epoch: 1 [21696/49669]\tLoss: 402.4123\n",
      "Training Epoch: 1 [21760/49669]\tLoss: 425.1487\n",
      "Training Epoch: 1 [21824/49669]\tLoss: 445.5127\n",
      "Training Epoch: 1 [21888/49669]\tLoss: 427.7980\n",
      "Training Epoch: 1 [21952/49669]\tLoss: 421.9263\n",
      "Training Epoch: 1 [22016/49669]\tLoss: 409.6516\n",
      "Training Epoch: 1 [22080/49669]\tLoss: 429.5826\n",
      "Training Epoch: 1 [22144/49669]\tLoss: 425.8981\n",
      "Training Epoch: 1 [22208/49669]\tLoss: 421.8506\n",
      "Training Epoch: 1 [22272/49669]\tLoss: 411.3680\n",
      "Training Epoch: 1 [22336/49669]\tLoss: 411.8112\n",
      "Training Epoch: 1 [22400/49669]\tLoss: 452.2418\n",
      "Training Epoch: 1 [22464/49669]\tLoss: 431.4633\n",
      "Training Epoch: 1 [22528/49669]\tLoss: 430.6155\n",
      "Training Epoch: 1 [22592/49669]\tLoss: 431.1589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [22656/49669]\tLoss: 426.0735\n",
      "Training Epoch: 1 [22720/49669]\tLoss: 407.8344\n",
      "Training Epoch: 1 [22784/49669]\tLoss: 425.2044\n",
      "Training Epoch: 1 [22848/49669]\tLoss: 433.7865\n",
      "Training Epoch: 1 [22912/49669]\tLoss: 412.3562\n",
      "Training Epoch: 1 [22976/49669]\tLoss: 410.2115\n",
      "Training Epoch: 1 [23040/49669]\tLoss: 439.9001\n",
      "Training Epoch: 1 [23104/49669]\tLoss: 382.5101\n",
      "Training Epoch: 1 [23168/49669]\tLoss: 440.5617\n",
      "Training Epoch: 1 [23232/49669]\tLoss: 432.9395\n",
      "Training Epoch: 1 [23296/49669]\tLoss: 435.9741\n",
      "Training Epoch: 1 [23360/49669]\tLoss: 394.8442\n",
      "Training Epoch: 1 [23424/49669]\tLoss: 441.6146\n",
      "Training Epoch: 1 [23488/49669]\tLoss: 434.8226\n",
      "Training Epoch: 1 [23552/49669]\tLoss: 444.3911\n",
      "Training Epoch: 1 [23616/49669]\tLoss: 405.8032\n",
      "Training Epoch: 1 [23680/49669]\tLoss: 418.3300\n",
      "Training Epoch: 1 [23744/49669]\tLoss: 411.5098\n",
      "Training Epoch: 1 [23808/49669]\tLoss: 418.2091\n",
      "Training Epoch: 1 [23872/49669]\tLoss: 420.5782\n",
      "Training Epoch: 1 [23936/49669]\tLoss: 435.7285\n",
      "Training Epoch: 1 [24000/49669]\tLoss: 435.6006\n",
      "Training Epoch: 1 [24064/49669]\tLoss: 432.3081\n",
      "Training Epoch: 1 [24128/49669]\tLoss: 442.8826\n",
      "Training Epoch: 1 [24192/49669]\tLoss: 420.6699\n",
      "Training Epoch: 1 [24256/49669]\tLoss: 447.3977\n",
      "Training Epoch: 1 [24320/49669]\tLoss: 439.3475\n",
      "Training Epoch: 1 [24384/49669]\tLoss: 423.2882\n",
      "Training Epoch: 1 [24448/49669]\tLoss: 411.5217\n",
      "Training Epoch: 1 [24512/49669]\tLoss: 402.4043\n",
      "Training Epoch: 1 [24576/49669]\tLoss: 441.6478\n",
      "Training Epoch: 1 [24640/49669]\tLoss: 432.0047\n",
      "Training Epoch: 1 [24704/49669]\tLoss: 410.5750\n",
      "Training Epoch: 1 [24768/49669]\tLoss: 441.0994\n",
      "Training Epoch: 1 [24832/49669]\tLoss: 422.7222\n",
      "Training Epoch: 1 [24896/49669]\tLoss: 429.9371\n",
      "Training Epoch: 1 [24960/49669]\tLoss: 402.2780\n",
      "Training Epoch: 1 [25024/49669]\tLoss: 439.9870\n",
      "Training Epoch: 1 [25088/49669]\tLoss: 403.4453\n",
      "Training Epoch: 1 [25152/49669]\tLoss: 435.5263\n",
      "Training Epoch: 1 [25216/49669]\tLoss: 408.3776\n",
      "Training Epoch: 1 [25280/49669]\tLoss: 441.7705\n",
      "Training Epoch: 1 [25344/49669]\tLoss: 394.9479\n",
      "Training Epoch: 1 [25408/49669]\tLoss: 414.8004\n",
      "Training Epoch: 1 [25472/49669]\tLoss: 410.7981\n",
      "Training Epoch: 1 [25536/49669]\tLoss: 429.8387\n",
      "Training Epoch: 1 [25600/49669]\tLoss: 416.2994\n",
      "Training Epoch: 1 [25664/49669]\tLoss: 429.7867\n",
      "Training Epoch: 1 [25728/49669]\tLoss: 430.3716\n",
      "Training Epoch: 1 [25792/49669]\tLoss: 424.0174\n",
      "Training Epoch: 1 [25856/49669]\tLoss: 409.8482\n",
      "Training Epoch: 1 [25920/49669]\tLoss: 423.8677\n",
      "Training Epoch: 1 [25984/49669]\tLoss: 412.7375\n",
      "Training Epoch: 1 [26048/49669]\tLoss: 418.4816\n",
      "Training Epoch: 1 [26112/49669]\tLoss: 423.4727\n",
      "Training Epoch: 1 [26176/49669]\tLoss: 450.9146\n",
      "Training Epoch: 1 [26240/49669]\tLoss: 426.4704\n",
      "Training Epoch: 1 [26304/49669]\tLoss: 408.0345\n",
      "Training Epoch: 1 [26368/49669]\tLoss: 431.0471\n",
      "Training Epoch: 1 [26432/49669]\tLoss: 424.9873\n",
      "Training Epoch: 1 [26496/49669]\tLoss: 402.7455\n",
      "Training Epoch: 1 [26560/49669]\tLoss: 400.6114\n",
      "Training Epoch: 1 [26624/49669]\tLoss: 430.6271\n",
      "Training Epoch: 1 [26688/49669]\tLoss: 452.0443\n",
      "Training Epoch: 1 [26752/49669]\tLoss: 433.5825\n",
      "Training Epoch: 1 [26816/49669]\tLoss: 441.2156\n",
      "Training Epoch: 1 [26880/49669]\tLoss: 443.4997\n",
      "Training Epoch: 1 [26944/49669]\tLoss: 418.8970\n",
      "Training Epoch: 1 [27008/49669]\tLoss: 454.0601\n",
      "Training Epoch: 1 [27072/49669]\tLoss: 431.8710\n",
      "Training Epoch: 1 [27136/49669]\tLoss: 438.3828\n",
      "Training Epoch: 1 [27200/49669]\tLoss: 405.6704\n",
      "Training Epoch: 1 [27264/49669]\tLoss: 424.9330\n",
      "Training Epoch: 1 [27328/49669]\tLoss: 411.0454\n",
      "Training Epoch: 1 [27392/49669]\tLoss: 438.6581\n",
      "Training Epoch: 1 [27456/49669]\tLoss: 407.2798\n",
      "Training Epoch: 1 [27520/49669]\tLoss: 430.6236\n",
      "Training Epoch: 1 [27584/49669]\tLoss: 447.6884\n",
      "Training Epoch: 1 [27648/49669]\tLoss: 426.1610\n",
      "Training Epoch: 1 [27712/49669]\tLoss: 449.0472\n",
      "Training Epoch: 1 [27776/49669]\tLoss: 396.9625\n",
      "Training Epoch: 1 [27840/49669]\tLoss: 449.8677\n",
      "Training Epoch: 1 [27904/49669]\tLoss: 410.6032\n",
      "Training Epoch: 1 [27968/49669]\tLoss: 392.3148\n",
      "Training Epoch: 1 [28032/49669]\tLoss: 421.0039\n",
      "Training Epoch: 1 [28096/49669]\tLoss: 411.7951\n",
      "Training Epoch: 1 [28160/49669]\tLoss: 386.1874\n",
      "Training Epoch: 1 [28224/49669]\tLoss: 445.5333\n",
      "Training Epoch: 1 [28288/49669]\tLoss: 421.7674\n",
      "Training Epoch: 1 [28352/49669]\tLoss: 446.9342\n",
      "Training Epoch: 1 [28416/49669]\tLoss: 413.9315\n",
      "Training Epoch: 1 [28480/49669]\tLoss: 406.7079\n",
      "Training Epoch: 1 [28544/49669]\tLoss: 433.7307\n",
      "Training Epoch: 1 [28608/49669]\tLoss: 451.4866\n",
      "Training Epoch: 1 [28672/49669]\tLoss: 425.7781\n",
      "Training Epoch: 1 [28736/49669]\tLoss: 436.2156\n",
      "Training Epoch: 1 [28800/49669]\tLoss: 432.4616\n",
      "Training Epoch: 1 [28864/49669]\tLoss: 423.7720\n",
      "Training Epoch: 1 [28928/49669]\tLoss: 434.2962\n",
      "Training Epoch: 1 [28992/49669]\tLoss: 426.9995\n",
      "Training Epoch: 1 [29056/49669]\tLoss: 447.0905\n",
      "Training Epoch: 1 [29120/49669]\tLoss: 409.2182\n",
      "Training Epoch: 1 [29184/49669]\tLoss: 420.6638\n",
      "Training Epoch: 1 [29248/49669]\tLoss: 449.7850\n",
      "Training Epoch: 1 [29312/49669]\tLoss: 413.8294\n",
      "Training Epoch: 1 [29376/49669]\tLoss: 432.0688\n",
      "Training Epoch: 1 [29440/49669]\tLoss: 431.6957\n",
      "Training Epoch: 1 [29504/49669]\tLoss: 423.9056\n",
      "Training Epoch: 1 [29568/49669]\tLoss: 418.2363\n",
      "Training Epoch: 1 [29632/49669]\tLoss: 391.9954\n",
      "Training Epoch: 1 [29696/49669]\tLoss: 403.6888\n",
      "Training Epoch: 1 [29760/49669]\tLoss: 392.1296\n",
      "Training Epoch: 1 [29824/49669]\tLoss: 442.6549\n",
      "Training Epoch: 1 [29888/49669]\tLoss: 433.7486\n",
      "Training Epoch: 1 [29952/49669]\tLoss: 441.8282\n",
      "Training Epoch: 1 [30016/49669]\tLoss: 434.8209\n",
      "Training Epoch: 1 [30080/49669]\tLoss: 423.4974\n",
      "Training Epoch: 1 [30144/49669]\tLoss: 404.8051\n",
      "Training Epoch: 1 [30208/49669]\tLoss: 389.7207\n",
      "Training Epoch: 1 [30272/49669]\tLoss: 394.4659\n",
      "Training Epoch: 1 [30336/49669]\tLoss: 440.3475\n",
      "Training Epoch: 1 [30400/49669]\tLoss: 408.7865\n",
      "Training Epoch: 1 [30464/49669]\tLoss: 415.0898\n",
      "Training Epoch: 1 [30528/49669]\tLoss: 430.1562\n",
      "Training Epoch: 1 [30592/49669]\tLoss: 408.3911\n",
      "Training Epoch: 1 [30656/49669]\tLoss: 451.5777\n",
      "Training Epoch: 1 [30720/49669]\tLoss: 423.2674\n",
      "Training Epoch: 1 [30784/49669]\tLoss: 432.6429\n",
      "Training Epoch: 1 [30848/49669]\tLoss: 409.5217\n",
      "Training Epoch: 1 [30912/49669]\tLoss: 420.1343\n",
      "Training Epoch: 1 [30976/49669]\tLoss: 439.9270\n",
      "Training Epoch: 1 [31040/49669]\tLoss: 417.1114\n",
      "Training Epoch: 1 [31104/49669]\tLoss: 433.4083\n",
      "Training Epoch: 1 [31168/49669]\tLoss: 427.5736\n",
      "Training Epoch: 1 [31232/49669]\tLoss: 430.7606\n",
      "Training Epoch: 1 [31296/49669]\tLoss: 439.2401\n",
      "Training Epoch: 1 [31360/49669]\tLoss: 449.9795\n",
      "Training Epoch: 1 [31424/49669]\tLoss: 424.4085\n",
      "Training Epoch: 1 [31488/49669]\tLoss: 435.2542\n",
      "Training Epoch: 1 [31552/49669]\tLoss: 410.5617\n",
      "Training Epoch: 1 [31616/49669]\tLoss: 425.3973\n",
      "Training Epoch: 1 [31680/49669]\tLoss: 429.5705\n",
      "Training Epoch: 1 [31744/49669]\tLoss: 415.4198\n",
      "Training Epoch: 1 [31808/49669]\tLoss: 437.6969\n",
      "Training Epoch: 1 [31872/49669]\tLoss: 419.0615\n",
      "Training Epoch: 1 [31936/49669]\tLoss: 434.8732\n",
      "Training Epoch: 1 [32000/49669]\tLoss: 394.3276\n",
      "Training Epoch: 1 [32064/49669]\tLoss: 427.3784\n",
      "Training Epoch: 1 [32128/49669]\tLoss: 456.0529\n",
      "Training Epoch: 1 [32192/49669]\tLoss: 421.2941\n",
      "Training Epoch: 1 [32256/49669]\tLoss: 429.8385\n",
      "Training Epoch: 1 [32320/49669]\tLoss: 434.6728\n",
      "Training Epoch: 1 [32384/49669]\tLoss: 432.0659\n",
      "Training Epoch: 1 [32448/49669]\tLoss: 436.3773\n",
      "Training Epoch: 1 [32512/49669]\tLoss: 415.4389\n",
      "Training Epoch: 1 [32576/49669]\tLoss: 436.4414\n",
      "Training Epoch: 1 [32640/49669]\tLoss: 423.2684\n",
      "Training Epoch: 1 [32704/49669]\tLoss: 410.7875\n",
      "Training Epoch: 1 [32768/49669]\tLoss: 421.3090\n",
      "Training Epoch: 1 [32832/49669]\tLoss: 413.9912\n",
      "Training Epoch: 1 [32896/49669]\tLoss: 426.1373\n",
      "Training Epoch: 1 [32960/49669]\tLoss: 459.0173\n",
      "Training Epoch: 1 [33024/49669]\tLoss: 402.7796\n",
      "Training Epoch: 1 [33088/49669]\tLoss: 411.0778\n",
      "Training Epoch: 1 [33152/49669]\tLoss: 418.4274\n",
      "Training Epoch: 1 [33216/49669]\tLoss: 422.3517\n",
      "Training Epoch: 1 [33280/49669]\tLoss: 426.6757\n",
      "Training Epoch: 1 [33344/49669]\tLoss: 409.2057\n",
      "Training Epoch: 1 [33408/49669]\tLoss: 410.1237\n",
      "Training Epoch: 1 [33472/49669]\tLoss: 402.8149\n",
      "Training Epoch: 1 [33536/49669]\tLoss: 439.8535\n",
      "Training Epoch: 1 [33600/49669]\tLoss: 416.2531\n",
      "Training Epoch: 1 [33664/49669]\tLoss: 406.8474\n",
      "Training Epoch: 1 [33728/49669]\tLoss: 438.6243\n",
      "Training Epoch: 1 [33792/49669]\tLoss: 412.3358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [33856/49669]\tLoss: 413.0791\n",
      "Training Epoch: 1 [33920/49669]\tLoss: 416.2403\n",
      "Training Epoch: 1 [33984/49669]\tLoss: 395.6244\n",
      "Training Epoch: 1 [34048/49669]\tLoss: 431.0198\n",
      "Training Epoch: 1 [34112/49669]\tLoss: 416.6628\n",
      "Training Epoch: 1 [34176/49669]\tLoss: 414.9441\n",
      "Training Epoch: 1 [34240/49669]\tLoss: 416.2034\n",
      "Training Epoch: 1 [34304/49669]\tLoss: 438.1298\n",
      "Training Epoch: 1 [34368/49669]\tLoss: 442.4864\n",
      "Training Epoch: 1 [34432/49669]\tLoss: 406.2890\n",
      "Training Epoch: 1 [34496/49669]\tLoss: 429.5029\n",
      "Training Epoch: 1 [34560/49669]\tLoss: 423.8179\n",
      "Training Epoch: 1 [34624/49669]\tLoss: 431.0344\n",
      "Training Epoch: 1 [34688/49669]\tLoss: 406.7956\n",
      "Training Epoch: 1 [34752/49669]\tLoss: 399.3762\n",
      "Training Epoch: 1 [34816/49669]\tLoss: 422.6410\n",
      "Training Epoch: 1 [34880/49669]\tLoss: 432.2469\n",
      "Training Epoch: 1 [34944/49669]\tLoss: 420.7932\n",
      "Training Epoch: 1 [35008/49669]\tLoss: 418.7831\n",
      "Training Epoch: 1 [35072/49669]\tLoss: 457.6133\n",
      "Training Epoch: 1 [35136/49669]\tLoss: 411.4021\n",
      "Training Epoch: 1 [35200/49669]\tLoss: 419.8126\n",
      "Training Epoch: 1 [35264/49669]\tLoss: 464.0393\n",
      "Training Epoch: 1 [35328/49669]\tLoss: 402.4805\n",
      "Training Epoch: 1 [35392/49669]\tLoss: 404.7495\n",
      "Training Epoch: 1 [35456/49669]\tLoss: 443.7905\n",
      "Training Epoch: 1 [35520/49669]\tLoss: 451.5569\n",
      "Training Epoch: 1 [35584/49669]\tLoss: 423.0070\n",
      "Training Epoch: 1 [35648/49669]\tLoss: 442.3546\n",
      "Training Epoch: 1 [35712/49669]\tLoss: 453.9380\n",
      "Training Epoch: 1 [35776/49669]\tLoss: 448.6738\n",
      "Training Epoch: 1 [35840/49669]\tLoss: 405.1480\n",
      "Training Epoch: 1 [35904/49669]\tLoss: 408.9059\n",
      "Training Epoch: 1 [35968/49669]\tLoss: 420.1374\n",
      "Training Epoch: 1 [36032/49669]\tLoss: 416.5214\n",
      "Training Epoch: 1 [36096/49669]\tLoss: 472.4542\n",
      "Training Epoch: 1 [36160/49669]\tLoss: 458.4424\n",
      "Training Epoch: 1 [36224/49669]\tLoss: 436.3797\n",
      "Training Epoch: 1 [36288/49669]\tLoss: 404.4937\n",
      "Training Epoch: 1 [36352/49669]\tLoss: 405.3576\n",
      "Training Epoch: 1 [36416/49669]\tLoss: 426.5840\n",
      "Training Epoch: 1 [36480/49669]\tLoss: 439.1801\n",
      "Training Epoch: 1 [36544/49669]\tLoss: 452.9052\n",
      "Training Epoch: 1 [36608/49669]\tLoss: 406.6008\n",
      "Training Epoch: 1 [36672/49669]\tLoss: 410.4748\n",
      "Training Epoch: 1 [36736/49669]\tLoss: 436.4287\n",
      "Training Epoch: 1 [36800/49669]\tLoss: 408.7450\n",
      "Training Epoch: 1 [36864/49669]\tLoss: 392.1842\n",
      "Training Epoch: 1 [36928/49669]\tLoss: 406.1810\n",
      "Training Epoch: 1 [36992/49669]\tLoss: 424.0072\n",
      "Training Epoch: 1 [37056/49669]\tLoss: 411.8037\n",
      "Training Epoch: 1 [37120/49669]\tLoss: 447.0403\n",
      "Training Epoch: 1 [37184/49669]\tLoss: 407.9111\n",
      "Training Epoch: 1 [37248/49669]\tLoss: 385.6759\n",
      "Training Epoch: 1 [37312/49669]\tLoss: 412.6408\n",
      "Training Epoch: 1 [37376/49669]\tLoss: 414.1302\n",
      "Training Epoch: 1 [37440/49669]\tLoss: 401.2650\n",
      "Training Epoch: 1 [37504/49669]\tLoss: 424.7391\n",
      "Training Epoch: 1 [37568/49669]\tLoss: 410.9075\n",
      "Training Epoch: 1 [37632/49669]\tLoss: 398.1220\n",
      "Training Epoch: 1 [37696/49669]\tLoss: 442.6217\n",
      "Training Epoch: 1 [37760/49669]\tLoss: 441.9784\n",
      "Training Epoch: 1 [37824/49669]\tLoss: 446.2401\n",
      "Training Epoch: 1 [37888/49669]\tLoss: 414.4481\n",
      "Training Epoch: 1 [37952/49669]\tLoss: 440.3645\n",
      "Training Epoch: 1 [38016/49669]\tLoss: 416.4623\n",
      "Training Epoch: 1 [38080/49669]\tLoss: 400.7154\n",
      "Training Epoch: 1 [38144/49669]\tLoss: 410.0390\n",
      "Training Epoch: 1 [38208/49669]\tLoss: 451.6178\n",
      "Training Epoch: 1 [38272/49669]\tLoss: 409.3215\n",
      "Training Epoch: 1 [38336/49669]\tLoss: 419.4589\n",
      "Training Epoch: 1 [38400/49669]\tLoss: 415.4542\n",
      "Training Epoch: 1 [38464/49669]\tLoss: 398.1766\n",
      "Training Epoch: 1 [38528/49669]\tLoss: 422.8477\n",
      "Training Epoch: 1 [38592/49669]\tLoss: 437.0459\n",
      "Training Epoch: 1 [38656/49669]\tLoss: 413.7461\n",
      "Training Epoch: 1 [38720/49669]\tLoss: 434.7370\n",
      "Training Epoch: 1 [38784/49669]\tLoss: 462.0027\n",
      "Training Epoch: 1 [38848/49669]\tLoss: 428.0614\n",
      "Training Epoch: 1 [38912/49669]\tLoss: 432.7232\n",
      "Training Epoch: 1 [38976/49669]\tLoss: 432.4086\n",
      "Training Epoch: 1 [39040/49669]\tLoss: 403.0259\n",
      "Training Epoch: 1 [39104/49669]\tLoss: 429.5867\n",
      "Training Epoch: 1 [39168/49669]\tLoss: 388.0069\n",
      "Training Epoch: 1 [39232/49669]\tLoss: 444.5059\n",
      "Training Epoch: 1 [39296/49669]\tLoss: 409.6577\n",
      "Training Epoch: 1 [39360/49669]\tLoss: 424.5230\n",
      "Training Epoch: 1 [39424/49669]\tLoss: 426.6878\n",
      "Training Epoch: 1 [39488/49669]\tLoss: 419.0092\n",
      "Training Epoch: 1 [39552/49669]\tLoss: 425.7899\n",
      "Training Epoch: 1 [39616/49669]\tLoss: 415.6289\n",
      "Training Epoch: 1 [39680/49669]\tLoss: 430.0881\n",
      "Training Epoch: 1 [39744/49669]\tLoss: 418.1902\n",
      "Training Epoch: 1 [39808/49669]\tLoss: 425.2425\n",
      "Training Epoch: 1 [39872/49669]\tLoss: 450.0101\n",
      "Training Epoch: 1 [39936/49669]\tLoss: 424.9646\n",
      "Training Epoch: 1 [40000/49669]\tLoss: 399.1002\n",
      "Training Epoch: 1 [40064/49669]\tLoss: 412.3263\n",
      "Training Epoch: 1 [40128/49669]\tLoss: 410.8746\n",
      "Training Epoch: 1 [40192/49669]\tLoss: 430.4020\n",
      "Training Epoch: 1 [40256/49669]\tLoss: 436.1507\n",
      "Training Epoch: 1 [40320/49669]\tLoss: 402.0829\n",
      "Training Epoch: 1 [40384/49669]\tLoss: 436.4430\n",
      "Training Epoch: 1 [40448/49669]\tLoss: 422.4997\n",
      "Training Epoch: 1 [40512/49669]\tLoss: 417.4568\n",
      "Training Epoch: 1 [40576/49669]\tLoss: 437.2801\n",
      "Training Epoch: 1 [40640/49669]\tLoss: 424.4687\n",
      "Training Epoch: 1 [40704/49669]\tLoss: 420.1664\n",
      "Training Epoch: 1 [40768/49669]\tLoss: 422.5314\n",
      "Training Epoch: 1 [40832/49669]\tLoss: 427.1395\n",
      "Training Epoch: 1 [40896/49669]\tLoss: 439.2011\n",
      "Training Epoch: 1 [40960/49669]\tLoss: 422.8145\n",
      "Training Epoch: 1 [41024/49669]\tLoss: 414.7065\n",
      "Training Epoch: 1 [41088/49669]\tLoss: 430.3090\n",
      "Training Epoch: 1 [41152/49669]\tLoss: 431.5884\n",
      "Training Epoch: 1 [41216/49669]\tLoss: 429.4281\n",
      "Training Epoch: 1 [41280/49669]\tLoss: 414.9619\n",
      "Training Epoch: 1 [41344/49669]\tLoss: 442.8332\n",
      "Training Epoch: 1 [41408/49669]\tLoss: 417.4140\n",
      "Training Epoch: 1 [41472/49669]\tLoss: 419.4343\n",
      "Training Epoch: 1 [41536/49669]\tLoss: 405.6460\n",
      "Training Epoch: 1 [41600/49669]\tLoss: 423.4673\n",
      "Training Epoch: 1 [41664/49669]\tLoss: 417.2443\n",
      "Training Epoch: 1 [41728/49669]\tLoss: 420.2667\n",
      "Training Epoch: 1 [41792/49669]\tLoss: 420.4090\n",
      "Training Epoch: 1 [41856/49669]\tLoss: 420.5581\n",
      "Training Epoch: 1 [41920/49669]\tLoss: 419.5920\n",
      "Training Epoch: 1 [41984/49669]\tLoss: 417.3346\n",
      "Training Epoch: 1 [42048/49669]\tLoss: 434.8426\n",
      "Training Epoch: 1 [42112/49669]\tLoss: 435.7545\n",
      "Training Epoch: 1 [42176/49669]\tLoss: 422.5314\n",
      "Training Epoch: 1 [42240/49669]\tLoss: 419.3526\n",
      "Training Epoch: 1 [42304/49669]\tLoss: 415.1277\n",
      "Training Epoch: 1 [42368/49669]\tLoss: 394.3950\n",
      "Training Epoch: 1 [42432/49669]\tLoss: 399.4127\n",
      "Training Epoch: 1 [42496/49669]\tLoss: 434.9146\n",
      "Training Epoch: 1 [42560/49669]\tLoss: 431.9084\n",
      "Training Epoch: 1 [42624/49669]\tLoss: 403.2809\n",
      "Training Epoch: 1 [42688/49669]\tLoss: 408.2202\n",
      "Training Epoch: 1 [42752/49669]\tLoss: 398.0488\n",
      "Training Epoch: 1 [42816/49669]\tLoss: 410.6321\n",
      "Training Epoch: 1 [42880/49669]\tLoss: 404.9524\n",
      "Training Epoch: 1 [42944/49669]\tLoss: 408.5691\n",
      "Training Epoch: 1 [43008/49669]\tLoss: 408.7796\n",
      "Training Epoch: 1 [43072/49669]\tLoss: 411.7655\n",
      "Training Epoch: 1 [43136/49669]\tLoss: 427.1973\n",
      "Training Epoch: 1 [43200/49669]\tLoss: 426.3948\n",
      "Training Epoch: 1 [43264/49669]\tLoss: 409.6719\n",
      "Training Epoch: 1 [43328/49669]\tLoss: 439.8768\n",
      "Training Epoch: 1 [43392/49669]\tLoss: 453.1008\n",
      "Training Epoch: 1 [43456/49669]\tLoss: 413.9061\n",
      "Training Epoch: 1 [43520/49669]\tLoss: 408.8362\n",
      "Training Epoch: 1 [43584/49669]\tLoss: 443.6556\n",
      "Training Epoch: 1 [43648/49669]\tLoss: 449.7394\n",
      "Training Epoch: 1 [43712/49669]\tLoss: 420.2320\n",
      "Training Epoch: 1 [43776/49669]\tLoss: 419.8072\n",
      "Training Epoch: 1 [43840/49669]\tLoss: 388.0439\n",
      "Training Epoch: 1 [43904/49669]\tLoss: 415.4896\n",
      "Training Epoch: 1 [43968/49669]\tLoss: 446.9846\n",
      "Training Epoch: 1 [44032/49669]\tLoss: 428.9833\n",
      "Training Epoch: 1 [44096/49669]\tLoss: 427.1519\n",
      "Training Epoch: 1 [44160/49669]\tLoss: 423.1377\n",
      "Training Epoch: 1 [44224/49669]\tLoss: 393.1019\n",
      "Training Epoch: 1 [44288/49669]\tLoss: 423.5915\n",
      "Training Epoch: 1 [44352/49669]\tLoss: 393.6004\n",
      "Training Epoch: 1 [44416/49669]\tLoss: 410.6101\n",
      "Training Epoch: 1 [44480/49669]\tLoss: 412.3920\n",
      "Training Epoch: 1 [44544/49669]\tLoss: 454.1729\n",
      "Training Epoch: 1 [44608/49669]\tLoss: 391.4445\n",
      "Training Epoch: 1 [44672/49669]\tLoss: 413.9539\n",
      "Training Epoch: 1 [44736/49669]\tLoss: 432.4811\n",
      "Training Epoch: 1 [44800/49669]\tLoss: 435.2594\n",
      "Training Epoch: 1 [44864/49669]\tLoss: 439.4320\n",
      "Training Epoch: 1 [44928/49669]\tLoss: 428.7057\n",
      "Training Epoch: 1 [44992/49669]\tLoss: 440.3521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [45056/49669]\tLoss: 406.6609\n",
      "Training Epoch: 1 [45120/49669]\tLoss: 411.7255\n",
      "Training Epoch: 1 [45184/49669]\tLoss: 414.5612\n",
      "Training Epoch: 1 [45248/49669]\tLoss: 432.3354\n",
      "Training Epoch: 1 [45312/49669]\tLoss: 429.8170\n",
      "Training Epoch: 1 [45376/49669]\tLoss: 431.2327\n",
      "Training Epoch: 1 [45440/49669]\tLoss: 437.6705\n",
      "Training Epoch: 1 [45504/49669]\tLoss: 435.6561\n",
      "Training Epoch: 1 [45568/49669]\tLoss: 425.0330\n",
      "Training Epoch: 1 [45632/49669]\tLoss: 439.7468\n",
      "Training Epoch: 1 [45696/49669]\tLoss: 431.0022\n",
      "Training Epoch: 1 [45760/49669]\tLoss: 415.3430\n",
      "Training Epoch: 1 [45824/49669]\tLoss: 457.6284\n",
      "Training Epoch: 1 [45888/49669]\tLoss: 444.6915\n",
      "Training Epoch: 1 [45952/49669]\tLoss: 431.7097\n",
      "Training Epoch: 1 [46016/49669]\tLoss: 414.9279\n",
      "Training Epoch: 1 [46080/49669]\tLoss: 410.1342\n",
      "Training Epoch: 1 [46144/49669]\tLoss: 428.7167\n",
      "Training Epoch: 1 [46208/49669]\tLoss: 400.9224\n",
      "Training Epoch: 1 [46272/49669]\tLoss: 439.0109\n",
      "Training Epoch: 1 [46336/49669]\tLoss: 417.9189\n",
      "Training Epoch: 1 [46400/49669]\tLoss: 434.6273\n",
      "Training Epoch: 1 [46464/49669]\tLoss: 439.9677\n",
      "Training Epoch: 1 [46528/49669]\tLoss: 437.7943\n",
      "Training Epoch: 1 [46592/49669]\tLoss: 389.1011\n",
      "Training Epoch: 1 [46656/49669]\tLoss: 427.1989\n",
      "Training Epoch: 1 [46720/49669]\tLoss: 445.0414\n",
      "Training Epoch: 1 [46784/49669]\tLoss: 398.5471\n",
      "Training Epoch: 1 [46848/49669]\tLoss: 403.9026\n",
      "Training Epoch: 1 [46912/49669]\tLoss: 422.2156\n",
      "Training Epoch: 1 [46976/49669]\tLoss: 409.4930\n",
      "Training Epoch: 1 [47040/49669]\tLoss: 432.6156\n",
      "Training Epoch: 1 [47104/49669]\tLoss: 444.3141\n",
      "Training Epoch: 1 [47168/49669]\tLoss: 454.9380\n",
      "Training Epoch: 1 [47232/49669]\tLoss: 444.5656\n",
      "Training Epoch: 1 [47296/49669]\tLoss: 437.7402\n",
      "Training Epoch: 1 [47360/49669]\tLoss: 436.5959\n",
      "Training Epoch: 1 [47424/49669]\tLoss: 441.4997\n",
      "Training Epoch: 1 [47488/49669]\tLoss: 423.7808\n",
      "Training Epoch: 1 [47552/49669]\tLoss: 465.2523\n",
      "Training Epoch: 1 [47616/49669]\tLoss: 421.3612\n",
      "Training Epoch: 1 [47680/49669]\tLoss: 416.1998\n",
      "Training Epoch: 1 [47744/49669]\tLoss: 410.8401\n",
      "Training Epoch: 1 [47808/49669]\tLoss: 403.6238\n",
      "Training Epoch: 1 [47872/49669]\tLoss: 450.8503\n",
      "Training Epoch: 1 [47936/49669]\tLoss: 403.1269\n",
      "Training Epoch: 1 [48000/49669]\tLoss: 429.6014\n",
      "Training Epoch: 1 [48064/49669]\tLoss: 398.9475\n",
      "Training Epoch: 1 [48128/49669]\tLoss: 402.7293\n",
      "Training Epoch: 1 [48192/49669]\tLoss: 403.9246\n",
      "Training Epoch: 1 [48256/49669]\tLoss: 419.5608\n",
      "Training Epoch: 1 [48320/49669]\tLoss: 424.8835\n",
      "Training Epoch: 1 [48384/49669]\tLoss: 465.0065\n",
      "Training Epoch: 1 [48448/49669]\tLoss: 402.7114\n",
      "Training Epoch: 1 [48512/49669]\tLoss: 412.8763\n",
      "Training Epoch: 1 [48576/49669]\tLoss: 419.5994\n",
      "Training Epoch: 1 [48640/49669]\tLoss: 442.0087\n",
      "Training Epoch: 1 [48704/49669]\tLoss: 421.0263\n",
      "Training Epoch: 1 [48768/49669]\tLoss: 421.0543\n",
      "Training Epoch: 1 [48832/49669]\tLoss: 420.9830\n",
      "Training Epoch: 1 [48896/49669]\tLoss: 426.2395\n",
      "Training Epoch: 1 [48960/49669]\tLoss: 423.0593\n",
      "Training Epoch: 1 [49024/49669]\tLoss: 440.2785\n",
      "Training Epoch: 1 [49088/49669]\tLoss: 409.8409\n",
      "Training Epoch: 1 [49152/49669]\tLoss: 409.2339\n",
      "Training Epoch: 1 [49216/49669]\tLoss: 430.4530\n",
      "Training Epoch: 1 [49280/49669]\tLoss: 448.0412\n",
      "Training Epoch: 1 [49344/49669]\tLoss: 422.9254\n",
      "Training Epoch: 1 [49408/49669]\tLoss: 418.2019\n",
      "Training Epoch: 1 [49472/49669]\tLoss: 442.2941\n",
      "Training Epoch: 1 [49536/49669]\tLoss: 401.7102\n",
      "Training Epoch: 1 [49600/49669]\tLoss: 448.0270\n",
      "Training Epoch: 1 [49664/49669]\tLoss: 428.9881\n",
      "Training Epoch: 1 [49669/49669]\tLoss: 269.3371\n",
      "Training Epoch: 1 [5519/5519]\tLoss: 424.0030\n",
      "Training Epoch: 2 [64/49669]\tLoss: 452.5928\n",
      "Training Epoch: 2 [128/49669]\tLoss: 447.9359\n",
      "Training Epoch: 2 [192/49669]\tLoss: 424.3057\n",
      "Training Epoch: 2 [256/49669]\tLoss: 422.8510\n",
      "Training Epoch: 2 [320/49669]\tLoss: 420.7796\n",
      "Training Epoch: 2 [384/49669]\tLoss: 425.0904\n",
      "Training Epoch: 2 [448/49669]\tLoss: 425.8547\n",
      "Training Epoch: 2 [512/49669]\tLoss: 439.3159\n",
      "Training Epoch: 2 [576/49669]\tLoss: 440.6475\n",
      "Training Epoch: 2 [640/49669]\tLoss: 404.4800\n",
      "Training Epoch: 2 [704/49669]\tLoss: 443.0524\n",
      "Training Epoch: 2 [768/49669]\tLoss: 425.2355\n",
      "Training Epoch: 2 [832/49669]\tLoss: 440.3255\n",
      "Training Epoch: 2 [896/49669]\tLoss: 443.5187\n",
      "Training Epoch: 2 [960/49669]\tLoss: 397.0975\n",
      "Training Epoch: 2 [1024/49669]\tLoss: 427.4788\n",
      "Training Epoch: 2 [1088/49669]\tLoss: 398.7509\n",
      "Training Epoch: 2 [1152/49669]\tLoss: 412.9765\n",
      "Training Epoch: 2 [1216/49669]\tLoss: 436.9097\n",
      "Training Epoch: 2 [1280/49669]\tLoss: 447.3248\n",
      "Training Epoch: 2 [1344/49669]\tLoss: 438.0593\n",
      "Training Epoch: 2 [1408/49669]\tLoss: 438.2845\n",
      "Training Epoch: 2 [1472/49669]\tLoss: 433.6215\n",
      "Training Epoch: 2 [1536/49669]\tLoss: 428.0596\n",
      "Training Epoch: 2 [1600/49669]\tLoss: 426.7633\n",
      "Training Epoch: 2 [1664/49669]\tLoss: 453.3501\n",
      "Training Epoch: 2 [1728/49669]\tLoss: 390.2542\n",
      "Training Epoch: 2 [1792/49669]\tLoss: 415.7720\n",
      "Training Epoch: 2 [1856/49669]\tLoss: 404.8290\n",
      "Training Epoch: 2 [1920/49669]\tLoss: 411.6904\n",
      "Training Epoch: 2 [1984/49669]\tLoss: 429.7753\n",
      "Training Epoch: 2 [2048/49669]\tLoss: 460.0231\n",
      "Training Epoch: 2 [2112/49669]\tLoss: 439.7822\n",
      "Training Epoch: 2 [2176/49669]\tLoss: 434.2205\n",
      "Training Epoch: 2 [2240/49669]\tLoss: 410.9473\n",
      "Training Epoch: 2 [2304/49669]\tLoss: 445.5499\n",
      "Training Epoch: 2 [2368/49669]\tLoss: 445.8824\n",
      "Training Epoch: 2 [2432/49669]\tLoss: 450.9150\n",
      "Training Epoch: 2 [2496/49669]\tLoss: 417.7022\n",
      "Training Epoch: 2 [2560/49669]\tLoss: 442.4491\n",
      "Training Epoch: 2 [2624/49669]\tLoss: 435.5148\n",
      "Training Epoch: 2 [2688/49669]\tLoss: 405.8192\n",
      "Training Epoch: 2 [2752/49669]\tLoss: 430.4178\n",
      "Training Epoch: 2 [2816/49669]\tLoss: 410.5846\n",
      "Training Epoch: 2 [2880/49669]\tLoss: 454.1645\n",
      "Training Epoch: 2 [2944/49669]\tLoss: 416.8981\n",
      "Training Epoch: 2 [3008/49669]\tLoss: 395.0270\n",
      "Training Epoch: 2 [3072/49669]\tLoss: 426.3441\n",
      "Training Epoch: 2 [3136/49669]\tLoss: 435.0562\n",
      "Training Epoch: 2 [3200/49669]\tLoss: 424.8843\n",
      "Training Epoch: 2 [3264/49669]\tLoss: 401.5332\n",
      "Training Epoch: 2 [3328/49669]\tLoss: 405.8807\n",
      "Training Epoch: 2 [3392/49669]\tLoss: 439.5857\n",
      "Training Epoch: 2 [3456/49669]\tLoss: 415.5058\n",
      "Training Epoch: 2 [3520/49669]\tLoss: 409.9156\n",
      "Training Epoch: 2 [3584/49669]\tLoss: 413.7227\n",
      "Training Epoch: 2 [3648/49669]\tLoss: 412.8515\n",
      "Training Epoch: 2 [3712/49669]\tLoss: 395.5019\n",
      "Training Epoch: 2 [3776/49669]\tLoss: 398.1933\n",
      "Training Epoch: 2 [3840/49669]\tLoss: 443.7053\n",
      "Training Epoch: 2 [3904/49669]\tLoss: 418.8810\n",
      "Training Epoch: 2 [3968/49669]\tLoss: 426.9659\n",
      "Training Epoch: 2 [4032/49669]\tLoss: 438.2415\n",
      "Training Epoch: 2 [4096/49669]\tLoss: 454.2920\n",
      "Training Epoch: 2 [4160/49669]\tLoss: 405.5504\n",
      "Training Epoch: 2 [4224/49669]\tLoss: 424.3105\n",
      "Training Epoch: 2 [4288/49669]\tLoss: 440.9102\n",
      "Training Epoch: 2 [4352/49669]\tLoss: 421.7257\n",
      "Training Epoch: 2 [4416/49669]\tLoss: 436.1226\n",
      "Training Epoch: 2 [4480/49669]\tLoss: 376.9391\n",
      "Training Epoch: 2 [4544/49669]\tLoss: 448.4245\n",
      "Training Epoch: 2 [4608/49669]\tLoss: 418.0480\n",
      "Training Epoch: 2 [4672/49669]\tLoss: 427.4444\n",
      "Training Epoch: 2 [4736/49669]\tLoss: 424.0029\n",
      "Training Epoch: 2 [4800/49669]\tLoss: 361.2931\n",
      "Training Epoch: 2 [4864/49669]\tLoss: 413.1194\n",
      "Training Epoch: 2 [4928/49669]\tLoss: 448.6172\n",
      "Training Epoch: 2 [4992/49669]\tLoss: 429.6309\n",
      "Training Epoch: 2 [5056/49669]\tLoss: 415.6055\n",
      "Training Epoch: 2 [5120/49669]\tLoss: 429.3066\n",
      "Training Epoch: 2 [5184/49669]\tLoss: 436.9634\n",
      "Training Epoch: 2 [5248/49669]\tLoss: 440.1990\n",
      "Training Epoch: 2 [5312/49669]\tLoss: 441.5196\n",
      "Training Epoch: 2 [5376/49669]\tLoss: 407.9704\n",
      "Training Epoch: 2 [5440/49669]\tLoss: 386.0954\n",
      "Training Epoch: 2 [5504/49669]\tLoss: 429.1672\n",
      "Training Epoch: 2 [5568/49669]\tLoss: 413.8129\n",
      "Training Epoch: 2 [5632/49669]\tLoss: 427.3040\n",
      "Training Epoch: 2 [5696/49669]\tLoss: 419.4476\n",
      "Training Epoch: 2 [5760/49669]\tLoss: 421.8980\n",
      "Training Epoch: 2 [5824/49669]\tLoss: 434.7074\n",
      "Training Epoch: 2 [5888/49669]\tLoss: 431.3938\n",
      "Training Epoch: 2 [5952/49669]\tLoss: 436.5880\n",
      "Training Epoch: 2 [6016/49669]\tLoss: 402.1344\n",
      "Training Epoch: 2 [6080/49669]\tLoss: 409.2276\n",
      "Training Epoch: 2 [6144/49669]\tLoss: 406.2471\n",
      "Training Epoch: 2 [6208/49669]\tLoss: 417.6572\n",
      "Training Epoch: 2 [6272/49669]\tLoss: 435.3872\n",
      "Training Epoch: 2 [6336/49669]\tLoss: 402.3124\n",
      "Training Epoch: 2 [6400/49669]\tLoss: 397.8937\n",
      "Training Epoch: 2 [6464/49669]\tLoss: 405.1207\n",
      "Training Epoch: 2 [6528/49669]\tLoss: 407.5520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [6592/49669]\tLoss: 406.4768\n",
      "Training Epoch: 2 [6656/49669]\tLoss: 445.9671\n",
      "Training Epoch: 2 [6720/49669]\tLoss: 394.6565\n",
      "Training Epoch: 2 [6784/49669]\tLoss: 443.8698\n",
      "Training Epoch: 2 [6848/49669]\tLoss: 388.9940\n",
      "Training Epoch: 2 [6912/49669]\tLoss: 421.1573\n",
      "Training Epoch: 2 [6976/49669]\tLoss: 404.8917\n",
      "Training Epoch: 2 [7040/49669]\tLoss: 417.9191\n",
      "Training Epoch: 2 [7104/49669]\tLoss: 425.3279\n",
      "Training Epoch: 2 [7168/49669]\tLoss: 439.1316\n",
      "Training Epoch: 2 [7232/49669]\tLoss: 422.7272\n",
      "Training Epoch: 2 [7296/49669]\tLoss: 401.5396\n",
      "Training Epoch: 2 [7360/49669]\tLoss: 460.7169\n",
      "Training Epoch: 2 [7424/49669]\tLoss: 433.0432\n",
      "Training Epoch: 2 [7488/49669]\tLoss: 423.0753\n",
      "Training Epoch: 2 [7552/49669]\tLoss: 471.0616\n",
      "Training Epoch: 2 [7616/49669]\tLoss: 396.3844\n",
      "Training Epoch: 2 [7680/49669]\tLoss: 400.4417\n",
      "Training Epoch: 2 [7744/49669]\tLoss: 454.4989\n",
      "Training Epoch: 2 [7808/49669]\tLoss: 417.7216\n",
      "Training Epoch: 2 [7872/49669]\tLoss: 415.1692\n",
      "Training Epoch: 2 [7936/49669]\tLoss: 456.2128\n",
      "Training Epoch: 2 [8000/49669]\tLoss: 429.5847\n",
      "Training Epoch: 2 [8064/49669]\tLoss: 411.4374\n",
      "Training Epoch: 2 [8128/49669]\tLoss: 408.0177\n",
      "Training Epoch: 2 [8192/49669]\tLoss: 426.0193\n",
      "Training Epoch: 2 [8256/49669]\tLoss: 416.1436\n",
      "Training Epoch: 2 [8320/49669]\tLoss: 436.3548\n",
      "Training Epoch: 2 [8384/49669]\tLoss: 417.9959\n",
      "Training Epoch: 2 [8448/49669]\tLoss: 412.7541\n",
      "Training Epoch: 2 [8512/49669]\tLoss: 439.0373\n",
      "Training Epoch: 2 [8576/49669]\tLoss: 399.9686\n",
      "Training Epoch: 2 [8640/49669]\tLoss: 430.5467\n",
      "Training Epoch: 2 [8704/49669]\tLoss: 431.2374\n",
      "Training Epoch: 2 [8768/49669]\tLoss: 441.1604\n",
      "Training Epoch: 2 [8832/49669]\tLoss: 436.3242\n",
      "Training Epoch: 2 [8896/49669]\tLoss: 393.1372\n",
      "Training Epoch: 2 [8960/49669]\tLoss: 428.0415\n",
      "Training Epoch: 2 [9024/49669]\tLoss: 383.7644\n",
      "Training Epoch: 2 [9088/49669]\tLoss: 420.6447\n",
      "Training Epoch: 2 [9152/49669]\tLoss: 382.8845\n",
      "Training Epoch: 2 [9216/49669]\tLoss: 431.5441\n",
      "Training Epoch: 2 [9280/49669]\tLoss: 438.7391\n",
      "Training Epoch: 2 [9344/49669]\tLoss: 424.5906\n",
      "Training Epoch: 2 [9408/49669]\tLoss: 432.8370\n",
      "Training Epoch: 2 [9472/49669]\tLoss: 434.3503\n",
      "Training Epoch: 2 [9536/49669]\tLoss: 430.8917\n",
      "Training Epoch: 2 [9600/49669]\tLoss: 419.2706\n",
      "Training Epoch: 2 [9664/49669]\tLoss: 419.5289\n",
      "Training Epoch: 2 [9728/49669]\tLoss: 438.7970\n",
      "Training Epoch: 2 [9792/49669]\tLoss: 434.9184\n",
      "Training Epoch: 2 [9856/49669]\tLoss: 427.2010\n",
      "Training Epoch: 2 [9920/49669]\tLoss: 431.4247\n",
      "Training Epoch: 2 [9984/49669]\tLoss: 432.4264\n",
      "Training Epoch: 2 [10048/49669]\tLoss: 431.4064\n",
      "Training Epoch: 2 [10112/49669]\tLoss: 429.5734\n",
      "Training Epoch: 2 [10176/49669]\tLoss: 391.6171\n",
      "Training Epoch: 2 [10240/49669]\tLoss: 388.7882\n",
      "Training Epoch: 2 [10304/49669]\tLoss: 390.4448\n",
      "Training Epoch: 2 [10368/49669]\tLoss: 417.4068\n",
      "Training Epoch: 2 [10432/49669]\tLoss: 409.0704\n",
      "Training Epoch: 2 [10496/49669]\tLoss: 413.3822\n",
      "Training Epoch: 2 [10560/49669]\tLoss: 426.6841\n",
      "Training Epoch: 2 [10624/49669]\tLoss: 422.0871\n",
      "Training Epoch: 2 [10688/49669]\tLoss: 461.7292\n",
      "Training Epoch: 2 [10752/49669]\tLoss: 398.0718\n",
      "Training Epoch: 2 [10816/49669]\tLoss: 421.7968\n",
      "Training Epoch: 2 [10880/49669]\tLoss: 400.2446\n",
      "Training Epoch: 2 [10944/49669]\tLoss: 419.0697\n",
      "Training Epoch: 2 [11008/49669]\tLoss: 437.9348\n",
      "Training Epoch: 2 [11072/49669]\tLoss: 433.3916\n",
      "Training Epoch: 2 [11136/49669]\tLoss: 414.5984\n",
      "Training Epoch: 2 [11200/49669]\tLoss: 410.5465\n",
      "Training Epoch: 2 [11264/49669]\tLoss: 434.7555\n",
      "Training Epoch: 2 [11328/49669]\tLoss: 428.7642\n",
      "Training Epoch: 2 [11392/49669]\tLoss: 395.1452\n",
      "Training Epoch: 2 [11456/49669]\tLoss: 421.7320\n",
      "Training Epoch: 2 [11520/49669]\tLoss: 398.2702\n",
      "Training Epoch: 2 [11584/49669]\tLoss: 423.3749\n",
      "Training Epoch: 2 [11648/49669]\tLoss: 377.6871\n",
      "Training Epoch: 2 [11712/49669]\tLoss: 445.9567\n",
      "Training Epoch: 2 [11776/49669]\tLoss: 400.0682\n",
      "Training Epoch: 2 [11840/49669]\tLoss: 440.7383\n",
      "Training Epoch: 2 [11904/49669]\tLoss: 438.6466\n",
      "Training Epoch: 2 [11968/49669]\tLoss: 407.4393\n",
      "Training Epoch: 2 [12032/49669]\tLoss: 428.2129\n",
      "Training Epoch: 2 [12096/49669]\tLoss: 458.7278\n",
      "Training Epoch: 2 [12160/49669]\tLoss: 431.3557\n",
      "Training Epoch: 2 [12224/49669]\tLoss: 420.6649\n",
      "Training Epoch: 2 [12288/49669]\tLoss: 408.9217\n",
      "Training Epoch: 2 [12352/49669]\tLoss: 433.6513\n",
      "Training Epoch: 2 [12416/49669]\tLoss: 434.7624\n",
      "Training Epoch: 2 [12480/49669]\tLoss: 424.6788\n",
      "Training Epoch: 2 [12544/49669]\tLoss: 426.9785\n",
      "Training Epoch: 2 [12608/49669]\tLoss: 399.1923\n",
      "Training Epoch: 2 [12672/49669]\tLoss: 454.7457\n",
      "Training Epoch: 2 [12736/49669]\tLoss: 430.5679\n",
      "Training Epoch: 2 [12800/49669]\tLoss: 424.3214\n",
      "Training Epoch: 2 [12864/49669]\tLoss: 401.3296\n",
      "Training Epoch: 2 [12928/49669]\tLoss: 404.6040\n",
      "Training Epoch: 2 [12992/49669]\tLoss: 438.6956\n",
      "Training Epoch: 2 [13056/49669]\tLoss: 448.8295\n",
      "Training Epoch: 2 [13120/49669]\tLoss: 415.0402\n",
      "Training Epoch: 2 [13184/49669]\tLoss: 425.1075\n",
      "Training Epoch: 2 [13248/49669]\tLoss: 407.3494\n",
      "Training Epoch: 2 [13312/49669]\tLoss: 432.5539\n",
      "Training Epoch: 2 [13376/49669]\tLoss: 428.3492\n",
      "Training Epoch: 2 [13440/49669]\tLoss: 421.1765\n",
      "Training Epoch: 2 [13504/49669]\tLoss: 447.7922\n",
      "Training Epoch: 2 [13568/49669]\tLoss: 478.4561\n",
      "Training Epoch: 2 [13632/49669]\tLoss: 403.3028\n",
      "Training Epoch: 2 [13696/49669]\tLoss: 413.4816\n",
      "Training Epoch: 2 [13760/49669]\tLoss: 397.7080\n",
      "Training Epoch: 2 [13824/49669]\tLoss: 416.8887\n",
      "Training Epoch: 2 [13888/49669]\tLoss: 430.1395\n",
      "Training Epoch: 2 [13952/49669]\tLoss: 419.3347\n",
      "Training Epoch: 2 [14016/49669]\tLoss: 431.5541\n",
      "Training Epoch: 2 [14080/49669]\tLoss: 410.5923\n",
      "Training Epoch: 2 [14144/49669]\tLoss: 438.4330\n",
      "Training Epoch: 2 [14208/49669]\tLoss: 437.9714\n",
      "Training Epoch: 2 [14272/49669]\tLoss: 390.2089\n",
      "Training Epoch: 2 [14336/49669]\tLoss: 428.8636\n",
      "Training Epoch: 2 [14400/49669]\tLoss: 426.6123\n",
      "Training Epoch: 2 [14464/49669]\tLoss: 400.0117\n",
      "Training Epoch: 2 [14528/49669]\tLoss: 407.8509\n",
      "Training Epoch: 2 [14592/49669]\tLoss: 421.5505\n",
      "Training Epoch: 2 [14656/49669]\tLoss: 448.1954\n",
      "Training Epoch: 2 [14720/49669]\tLoss: 439.6843\n",
      "Training Epoch: 2 [14784/49669]\tLoss: 430.2475\n",
      "Training Epoch: 2 [14848/49669]\tLoss: 450.3656\n",
      "Training Epoch: 2 [14912/49669]\tLoss: 413.3197\n",
      "Training Epoch: 2 [14976/49669]\tLoss: 428.9278\n",
      "Training Epoch: 2 [15040/49669]\tLoss: 442.9861\n",
      "Training Epoch: 2 [15104/49669]\tLoss: 449.4714\n",
      "Training Epoch: 2 [15168/49669]\tLoss: 399.0886\n",
      "Training Epoch: 2 [15232/49669]\tLoss: 408.6836\n",
      "Training Epoch: 2 [15296/49669]\tLoss: 450.0912\n",
      "Training Epoch: 2 [15360/49669]\tLoss: 432.1697\n",
      "Training Epoch: 2 [15424/49669]\tLoss: 433.0509\n",
      "Training Epoch: 2 [15488/49669]\tLoss: 430.3572\n",
      "Training Epoch: 2 [15552/49669]\tLoss: 420.4722\n",
      "Training Epoch: 2 [15616/49669]\tLoss: 400.1650\n",
      "Training Epoch: 2 [15680/49669]\tLoss: 427.2345\n",
      "Training Epoch: 2 [15744/49669]\tLoss: 406.7958\n",
      "Training Epoch: 2 [15808/49669]\tLoss: 428.4429\n",
      "Training Epoch: 2 [15872/49669]\tLoss: 423.6675\n",
      "Training Epoch: 2 [15936/49669]\tLoss: 414.3724\n",
      "Training Epoch: 2 [16000/49669]\tLoss: 454.3216\n",
      "Training Epoch: 2 [16064/49669]\tLoss: 443.3368\n",
      "Training Epoch: 2 [16128/49669]\tLoss: 416.2054\n",
      "Training Epoch: 2 [16192/49669]\tLoss: 416.8976\n",
      "Training Epoch: 2 [16256/49669]\tLoss: 434.9168\n",
      "Training Epoch: 2 [16320/49669]\tLoss: 403.4587\n",
      "Training Epoch: 2 [16384/49669]\tLoss: 432.0862\n",
      "Training Epoch: 2 [16448/49669]\tLoss: 388.6934\n",
      "Training Epoch: 2 [16512/49669]\tLoss: 401.0592\n",
      "Training Epoch: 2 [16576/49669]\tLoss: 424.5985\n",
      "Training Epoch: 2 [16640/49669]\tLoss: 451.5418\n",
      "Training Epoch: 2 [16704/49669]\tLoss: 418.0409\n",
      "Training Epoch: 2 [16768/49669]\tLoss: 432.5540\n",
      "Training Epoch: 2 [16832/49669]\tLoss: 433.9308\n",
      "Training Epoch: 2 [16896/49669]\tLoss: 432.9766\n",
      "Training Epoch: 2 [16960/49669]\tLoss: 437.5109\n",
      "Training Epoch: 2 [17024/49669]\tLoss: 398.9374\n",
      "Training Epoch: 2 [17088/49669]\tLoss: 424.2863\n",
      "Training Epoch: 2 [17152/49669]\tLoss: 422.9668\n",
      "Training Epoch: 2 [17216/49669]\tLoss: 403.3087\n",
      "Training Epoch: 2 [17280/49669]\tLoss: 433.7032\n",
      "Training Epoch: 2 [17344/49669]\tLoss: 409.0882\n",
      "Training Epoch: 2 [17408/49669]\tLoss: 427.7985\n",
      "Training Epoch: 2 [17472/49669]\tLoss: 414.5320\n",
      "Training Epoch: 2 [17536/49669]\tLoss: 433.8217\n",
      "Training Epoch: 2 [17600/49669]\tLoss: 424.0069\n",
      "Training Epoch: 2 [17664/49669]\tLoss: 427.4392\n",
      "Training Epoch: 2 [17728/49669]\tLoss: 414.8054\n",
      "Training Epoch: 2 [17792/49669]\tLoss: 433.3362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [17856/49669]\tLoss: 421.8930\n",
      "Training Epoch: 2 [17920/49669]\tLoss: 410.1604\n",
      "Training Epoch: 2 [17984/49669]\tLoss: 427.3945\n",
      "Training Epoch: 2 [18048/49669]\tLoss: 454.6931\n",
      "Training Epoch: 2 [18112/49669]\tLoss: 427.8575\n",
      "Training Epoch: 2 [18176/49669]\tLoss: 397.9517\n",
      "Training Epoch: 2 [18240/49669]\tLoss: 443.2833\n",
      "Training Epoch: 2 [18304/49669]\tLoss: 405.4933\n",
      "Training Epoch: 2 [18368/49669]\tLoss: 450.6147\n",
      "Training Epoch: 2 [18432/49669]\tLoss: 424.0862\n",
      "Training Epoch: 2 [18496/49669]\tLoss: 440.2739\n",
      "Training Epoch: 2 [18560/49669]\tLoss: 412.0960\n",
      "Training Epoch: 2 [18624/49669]\tLoss: 437.3174\n",
      "Training Epoch: 2 [18688/49669]\tLoss: 405.4484\n",
      "Training Epoch: 2 [18752/49669]\tLoss: 403.9466\n",
      "Training Epoch: 2 [18816/49669]\tLoss: 404.4247\n",
      "Training Epoch: 2 [18880/49669]\tLoss: 408.5775\n",
      "Training Epoch: 2 [18944/49669]\tLoss: 409.3623\n",
      "Training Epoch: 2 [19008/49669]\tLoss: 420.4955\n",
      "Training Epoch: 2 [19072/49669]\tLoss: 433.2208\n",
      "Training Epoch: 2 [19136/49669]\tLoss: 432.4238\n",
      "Training Epoch: 2 [19200/49669]\tLoss: 409.0622\n",
      "Training Epoch: 2 [19264/49669]\tLoss: 446.2374\n",
      "Training Epoch: 2 [19328/49669]\tLoss: 417.9160\n",
      "Training Epoch: 2 [19392/49669]\tLoss: 441.6508\n",
      "Training Epoch: 2 [19456/49669]\tLoss: 401.2139\n",
      "Training Epoch: 2 [19520/49669]\tLoss: 425.9160\n",
      "Training Epoch: 2 [19584/49669]\tLoss: 438.4410\n",
      "Training Epoch: 2 [19648/49669]\tLoss: 438.1325\n",
      "Training Epoch: 2 [19712/49669]\tLoss: 426.4381\n",
      "Training Epoch: 2 [19776/49669]\tLoss: 414.5847\n",
      "Training Epoch: 2 [19840/49669]\tLoss: 456.0565\n",
      "Training Epoch: 2 [19904/49669]\tLoss: 448.6089\n",
      "Training Epoch: 2 [19968/49669]\tLoss: 446.2006\n",
      "Training Epoch: 2 [20032/49669]\tLoss: 414.4700\n",
      "Training Epoch: 2 [20096/49669]\tLoss: 443.8341\n",
      "Training Epoch: 2 [20160/49669]\tLoss: 445.1042\n",
      "Training Epoch: 2 [20224/49669]\tLoss: 401.2812\n",
      "Training Epoch: 2 [20288/49669]\tLoss: 424.7823\n",
      "Training Epoch: 2 [20352/49669]\tLoss: 422.7586\n",
      "Training Epoch: 2 [20416/49669]\tLoss: 429.4123\n",
      "Training Epoch: 2 [20480/49669]\tLoss: 397.2772\n",
      "Training Epoch: 2 [20544/49669]\tLoss: 404.9882\n",
      "Training Epoch: 2 [20608/49669]\tLoss: 435.5367\n",
      "Training Epoch: 2 [20672/49669]\tLoss: 434.8917\n",
      "Training Epoch: 2 [20736/49669]\tLoss: 415.0326\n",
      "Training Epoch: 2 [20800/49669]\tLoss: 431.6562\n",
      "Training Epoch: 2 [20864/49669]\tLoss: 450.5779\n",
      "Training Epoch: 2 [20928/49669]\tLoss: 448.3604\n",
      "Training Epoch: 2 [20992/49669]\tLoss: 436.3592\n",
      "Training Epoch: 2 [21056/49669]\tLoss: 395.2613\n",
      "Training Epoch: 2 [21120/49669]\tLoss: 423.3546\n",
      "Training Epoch: 2 [21184/49669]\tLoss: 426.1252\n",
      "Training Epoch: 2 [21248/49669]\tLoss: 425.9788\n",
      "Training Epoch: 2 [21312/49669]\tLoss: 428.2285\n",
      "Training Epoch: 2 [21376/49669]\tLoss: 412.7532\n",
      "Training Epoch: 2 [21440/49669]\tLoss: 408.7665\n",
      "Training Epoch: 2 [21504/49669]\tLoss: 425.3459\n",
      "Training Epoch: 2 [21568/49669]\tLoss: 409.5164\n",
      "Training Epoch: 2 [21632/49669]\tLoss: 419.7395\n",
      "Training Epoch: 2 [21696/49669]\tLoss: 411.7439\n",
      "Training Epoch: 2 [21760/49669]\tLoss: 402.0575\n",
      "Training Epoch: 2 [21824/49669]\tLoss: 440.8307\n",
      "Training Epoch: 2 [21888/49669]\tLoss: 392.0192\n",
      "Training Epoch: 2 [21952/49669]\tLoss: 420.2286\n",
      "Training Epoch: 2 [22016/49669]\tLoss: 436.0953\n",
      "Training Epoch: 2 [22080/49669]\tLoss: 422.8739\n",
      "Training Epoch: 2 [22144/49669]\tLoss: 396.6614\n",
      "Training Epoch: 2 [22208/49669]\tLoss: 405.8721\n",
      "Training Epoch: 2 [22272/49669]\tLoss: 385.3104\n",
      "Training Epoch: 2 [22336/49669]\tLoss: 419.6569\n",
      "Training Epoch: 2 [22400/49669]\tLoss: 453.1260\n",
      "Training Epoch: 2 [22464/49669]\tLoss: 420.7874\n",
      "Training Epoch: 2 [22528/49669]\tLoss: 400.3996\n",
      "Training Epoch: 2 [22592/49669]\tLoss: 400.3410\n",
      "Training Epoch: 2 [22656/49669]\tLoss: 401.3007\n",
      "Training Epoch: 2 [22720/49669]\tLoss: 437.4558\n",
      "Training Epoch: 2 [22784/49669]\tLoss: 441.3484\n",
      "Training Epoch: 2 [22848/49669]\tLoss: 419.6986\n",
      "Training Epoch: 2 [22912/49669]\tLoss: 424.7376\n",
      "Training Epoch: 2 [22976/49669]\tLoss: 442.0140\n",
      "Training Epoch: 2 [23040/49669]\tLoss: 409.2273\n",
      "Training Epoch: 2 [23104/49669]\tLoss: 421.2504\n",
      "Training Epoch: 2 [23168/49669]\tLoss: 418.4649\n",
      "Training Epoch: 2 [23232/49669]\tLoss: 428.1621\n",
      "Training Epoch: 2 [23296/49669]\tLoss: 414.8350\n",
      "Training Epoch: 2 [23360/49669]\tLoss: 415.8479\n",
      "Training Epoch: 2 [23424/49669]\tLoss: 398.8958\n",
      "Training Epoch: 2 [23488/49669]\tLoss: 438.2090\n",
      "Training Epoch: 2 [23552/49669]\tLoss: 389.4454\n",
      "Training Epoch: 2 [23616/49669]\tLoss: 414.2487\n",
      "Training Epoch: 2 [23680/49669]\tLoss: 448.4045\n",
      "Training Epoch: 2 [23744/49669]\tLoss: 393.9177\n",
      "Training Epoch: 2 [23808/49669]\tLoss: 410.5759\n",
      "Training Epoch: 2 [23872/49669]\tLoss: 431.8871\n",
      "Training Epoch: 2 [23936/49669]\tLoss: 409.3046\n",
      "Training Epoch: 2 [24000/49669]\tLoss: 411.3979\n",
      "Training Epoch: 2 [24064/49669]\tLoss: 421.8484\n",
      "Training Epoch: 2 [24128/49669]\tLoss: 396.2386\n",
      "Training Epoch: 2 [24192/49669]\tLoss: 421.4789\n",
      "Training Epoch: 2 [24256/49669]\tLoss: 430.1939\n",
      "Training Epoch: 2 [24320/49669]\tLoss: 440.7339\n",
      "Training Epoch: 2 [24384/49669]\tLoss: 443.0859\n",
      "Training Epoch: 2 [24448/49669]\tLoss: 437.8576\n",
      "Training Epoch: 2 [24512/49669]\tLoss: 398.7555\n",
      "Training Epoch: 2 [24576/49669]\tLoss: 415.4069\n",
      "Training Epoch: 2 [24640/49669]\tLoss: 441.9549\n",
      "Training Epoch: 2 [24704/49669]\tLoss: 439.5908\n",
      "Training Epoch: 2 [24768/49669]\tLoss: 414.0440\n",
      "Training Epoch: 2 [24832/49669]\tLoss: 414.6430\n",
      "Training Epoch: 2 [24896/49669]\tLoss: 434.8651\n",
      "Training Epoch: 2 [24960/49669]\tLoss: 428.7069\n",
      "Training Epoch: 2 [25024/49669]\tLoss: 434.8129\n",
      "Training Epoch: 2 [25088/49669]\tLoss: 411.7617\n",
      "Training Epoch: 2 [25152/49669]\tLoss: 397.7848\n",
      "Training Epoch: 2 [25216/49669]\tLoss: 421.5907\n",
      "Training Epoch: 2 [25280/49669]\tLoss: 419.5827\n",
      "Training Epoch: 2 [25344/49669]\tLoss: 405.0175\n",
      "Training Epoch: 2 [25408/49669]\tLoss: 411.1146\n",
      "Training Epoch: 2 [25472/49669]\tLoss: 440.9622\n",
      "Training Epoch: 2 [25536/49669]\tLoss: 432.8758\n",
      "Training Epoch: 2 [25600/49669]\tLoss: 444.5828\n",
      "Training Epoch: 2 [25664/49669]\tLoss: 399.7368\n",
      "Training Epoch: 2 [25728/49669]\tLoss: 410.9946\n",
      "Training Epoch: 2 [25792/49669]\tLoss: 396.0581\n",
      "Training Epoch: 2 [25856/49669]\tLoss: 446.9117\n",
      "Training Epoch: 2 [25920/49669]\tLoss: 418.2166\n",
      "Training Epoch: 2 [25984/49669]\tLoss: 420.0305\n",
      "Training Epoch: 2 [26048/49669]\tLoss: 413.0841\n",
      "Training Epoch: 2 [26112/49669]\tLoss: 442.5810\n",
      "Training Epoch: 2 [26176/49669]\tLoss: 431.3038\n",
      "Training Epoch: 2 [26240/49669]\tLoss: 400.5296\n",
      "Training Epoch: 2 [26304/49669]\tLoss: 424.0749\n",
      "Training Epoch: 2 [26368/49669]\tLoss: 443.0248\n",
      "Training Epoch: 2 [26432/49669]\tLoss: 391.5574\n",
      "Training Epoch: 2 [26496/49669]\tLoss: 384.8514\n",
      "Training Epoch: 2 [26560/49669]\tLoss: 408.9398\n",
      "Training Epoch: 2 [26624/49669]\tLoss: 415.8298\n",
      "Training Epoch: 2 [26688/49669]\tLoss: 431.1335\n",
      "Training Epoch: 2 [26752/49669]\tLoss: 418.1607\n",
      "Training Epoch: 2 [26816/49669]\tLoss: 433.1887\n",
      "Training Epoch: 2 [26880/49669]\tLoss: 413.6444\n",
      "Training Epoch: 2 [26944/49669]\tLoss: 446.3805\n",
      "Training Epoch: 2 [27008/49669]\tLoss: 422.8870\n",
      "Training Epoch: 2 [27072/49669]\tLoss: 443.7839\n",
      "Training Epoch: 2 [27136/49669]\tLoss: 429.3039\n",
      "Training Epoch: 2 [27200/49669]\tLoss: 404.9745\n",
      "Training Epoch: 2 [27264/49669]\tLoss: 397.9270\n",
      "Training Epoch: 2 [27328/49669]\tLoss: 444.1086\n",
      "Training Epoch: 2 [27392/49669]\tLoss: 426.6053\n",
      "Training Epoch: 2 [27456/49669]\tLoss: 443.6194\n",
      "Training Epoch: 2 [27520/49669]\tLoss: 446.0109\n",
      "Training Epoch: 2 [27584/49669]\tLoss: 402.7390\n",
      "Training Epoch: 2 [27648/49669]\tLoss: 446.8293\n",
      "Training Epoch: 2 [27712/49669]\tLoss: 430.2237\n",
      "Training Epoch: 2 [27776/49669]\tLoss: 383.7981\n",
      "Training Epoch: 2 [27840/49669]\tLoss: 410.0931\n",
      "Training Epoch: 2 [27904/49669]\tLoss: 415.6979\n",
      "Training Epoch: 2 [27968/49669]\tLoss: 395.6059\n",
      "Training Epoch: 2 [28032/49669]\tLoss: 406.8579\n",
      "Training Epoch: 2 [28096/49669]\tLoss: 422.0355\n",
      "Training Epoch: 2 [28160/49669]\tLoss: 429.4285\n",
      "Training Epoch: 2 [28224/49669]\tLoss: 441.5262\n",
      "Training Epoch: 2 [28288/49669]\tLoss: 429.9908\n",
      "Training Epoch: 2 [28352/49669]\tLoss: 418.0190\n",
      "Training Epoch: 2 [28416/49669]\tLoss: 428.0540\n",
      "Training Epoch: 2 [28480/49669]\tLoss: 441.9166\n",
      "Training Epoch: 2 [28544/49669]\tLoss: 413.5541\n",
      "Training Epoch: 2 [28608/49669]\tLoss: 412.0173\n",
      "Training Epoch: 2 [28672/49669]\tLoss: 442.3836\n",
      "Training Epoch: 2 [28736/49669]\tLoss: 422.3198\n",
      "Training Epoch: 2 [28800/49669]\tLoss: 395.3067\n",
      "Training Epoch: 2 [28864/49669]\tLoss: 446.0489\n",
      "Training Epoch: 2 [28928/49669]\tLoss: 427.2895\n",
      "Training Epoch: 2 [28992/49669]\tLoss: 430.4585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [29056/49669]\tLoss: 402.2053\n",
      "Training Epoch: 2 [29120/49669]\tLoss: 432.0071\n",
      "Training Epoch: 2 [29184/49669]\tLoss: 431.5543\n",
      "Training Epoch: 2 [29248/49669]\tLoss: 420.9364\n",
      "Training Epoch: 2 [29312/49669]\tLoss: 441.2058\n",
      "Training Epoch: 2 [29376/49669]\tLoss: 446.0932\n",
      "Training Epoch: 2 [29440/49669]\tLoss: 414.4692\n",
      "Training Epoch: 2 [29504/49669]\tLoss: 438.3464\n",
      "Training Epoch: 2 [29568/49669]\tLoss: 425.0164\n",
      "Training Epoch: 2 [29632/49669]\tLoss: 436.9050\n",
      "Training Epoch: 2 [29696/49669]\tLoss: 418.7511\n",
      "Training Epoch: 2 [29760/49669]\tLoss: 422.9992\n",
      "Training Epoch: 2 [29824/49669]\tLoss: 453.3222\n",
      "Training Epoch: 2 [29888/49669]\tLoss: 419.5272\n",
      "Training Epoch: 2 [29952/49669]\tLoss: 404.8380\n",
      "Training Epoch: 2 [30016/49669]\tLoss: 430.6826\n",
      "Training Epoch: 2 [30080/49669]\tLoss: 416.9601\n",
      "Training Epoch: 2 [30144/49669]\tLoss: 445.0121\n",
      "Training Epoch: 2 [30208/49669]\tLoss: 445.9742\n",
      "Training Epoch: 2 [30272/49669]\tLoss: 428.5241\n",
      "Training Epoch: 2 [30336/49669]\tLoss: 418.2783\n",
      "Training Epoch: 2 [30400/49669]\tLoss: 415.4190\n",
      "Training Epoch: 2 [30464/49669]\tLoss: 438.8301\n",
      "Training Epoch: 2 [30528/49669]\tLoss: 428.7334\n",
      "Training Epoch: 2 [30592/49669]\tLoss: 426.0148\n",
      "Training Epoch: 2 [30656/49669]\tLoss: 431.2156\n",
      "Training Epoch: 2 [30720/49669]\tLoss: 426.3268\n",
      "Training Epoch: 2 [30784/49669]\tLoss: 460.0461\n",
      "Training Epoch: 2 [30848/49669]\tLoss: 409.4962\n",
      "Training Epoch: 2 [30912/49669]\tLoss: 425.4356\n",
      "Training Epoch: 2 [30976/49669]\tLoss: 413.4944\n",
      "Training Epoch: 2 [31040/49669]\tLoss: 456.1436\n",
      "Training Epoch: 2 [31104/49669]\tLoss: 425.6552\n",
      "Training Epoch: 2 [31168/49669]\tLoss: 398.4895\n",
      "Training Epoch: 2 [31232/49669]\tLoss: 411.7763\n",
      "Training Epoch: 2 [31296/49669]\tLoss: 457.4117\n",
      "Training Epoch: 2 [31360/49669]\tLoss: 390.5070\n",
      "Training Epoch: 2 [31424/49669]\tLoss: 398.8141\n",
      "Training Epoch: 2 [31488/49669]\tLoss: 412.8148\n",
      "Training Epoch: 2 [31552/49669]\tLoss: 444.9824\n",
      "Training Epoch: 2 [31616/49669]\tLoss: 436.0329\n",
      "Training Epoch: 2 [31680/49669]\tLoss: 429.9737\n",
      "Training Epoch: 2 [31744/49669]\tLoss: 417.3390\n",
      "Training Epoch: 2 [31808/49669]\tLoss: 408.5840\n",
      "Training Epoch: 2 [31872/49669]\tLoss: 418.4003\n",
      "Training Epoch: 2 [31936/49669]\tLoss: 401.8515\n",
      "Training Epoch: 2 [32000/49669]\tLoss: 397.8777\n",
      "Training Epoch: 2 [32064/49669]\tLoss: 424.2826\n",
      "Training Epoch: 2 [32128/49669]\tLoss: 423.5560\n",
      "Training Epoch: 2 [32192/49669]\tLoss: 422.6371\n",
      "Training Epoch: 2 [32256/49669]\tLoss: 419.2402\n",
      "Training Epoch: 2 [32320/49669]\tLoss: 430.0851\n",
      "Training Epoch: 2 [32384/49669]\tLoss: 436.8719\n",
      "Training Epoch: 2 [32448/49669]\tLoss: 432.1587\n",
      "Training Epoch: 2 [32512/49669]\tLoss: 449.7165\n",
      "Training Epoch: 2 [32576/49669]\tLoss: 419.3093\n",
      "Training Epoch: 2 [32640/49669]\tLoss: 414.9272\n",
      "Training Epoch: 2 [32704/49669]\tLoss: 440.1559\n",
      "Training Epoch: 2 [32768/49669]\tLoss: 458.0352\n",
      "Training Epoch: 2 [32832/49669]\tLoss: 394.7990\n",
      "Training Epoch: 2 [32896/49669]\tLoss: 405.4348\n",
      "Training Epoch: 2 [32960/49669]\tLoss: 416.5009\n",
      "Training Epoch: 2 [33024/49669]\tLoss: 391.5443\n",
      "Training Epoch: 2 [33088/49669]\tLoss: 435.0541\n",
      "Training Epoch: 2 [33152/49669]\tLoss: 416.0375\n",
      "Training Epoch: 2 [33216/49669]\tLoss: 428.7248\n",
      "Training Epoch: 2 [33280/49669]\tLoss: 433.9709\n",
      "Training Epoch: 2 [33344/49669]\tLoss: 412.7094\n",
      "Training Epoch: 2 [33408/49669]\tLoss: 438.8191\n",
      "Training Epoch: 2 [33472/49669]\tLoss: 410.5226\n",
      "Training Epoch: 2 [33536/49669]\tLoss: 390.2035\n",
      "Training Epoch: 2 [33600/49669]\tLoss: 403.3656\n",
      "Training Epoch: 2 [33664/49669]\tLoss: 423.7670\n",
      "Training Epoch: 2 [33728/49669]\tLoss: 447.1855\n",
      "Training Epoch: 2 [33792/49669]\tLoss: 430.8446\n",
      "Training Epoch: 2 [33856/49669]\tLoss: 454.6066\n",
      "Training Epoch: 2 [33920/49669]\tLoss: 420.9547\n",
      "Training Epoch: 2 [33984/49669]\tLoss: 414.3915\n",
      "Training Epoch: 2 [34048/49669]\tLoss: 416.0723\n",
      "Training Epoch: 2 [34112/49669]\tLoss: 439.5177\n",
      "Training Epoch: 2 [34176/49669]\tLoss: 447.5817\n",
      "Training Epoch: 2 [34240/49669]\tLoss: 444.9548\n",
      "Training Epoch: 2 [34304/49669]\tLoss: 443.3950\n",
      "Training Epoch: 2 [34368/49669]\tLoss: 408.3893\n",
      "Training Epoch: 2 [34432/49669]\tLoss: 431.1751\n",
      "Training Epoch: 2 [34496/49669]\tLoss: 416.7481\n",
      "Training Epoch: 2 [34560/49669]\tLoss: 417.6089\n",
      "Training Epoch: 2 [34624/49669]\tLoss: 444.1234\n",
      "Training Epoch: 2 [34688/49669]\tLoss: 421.1266\n",
      "Training Epoch: 2 [34752/49669]\tLoss: 454.0260\n",
      "Training Epoch: 2 [34816/49669]\tLoss: 442.6616\n",
      "Training Epoch: 2 [34880/49669]\tLoss: 439.3525\n",
      "Training Epoch: 2 [34944/49669]\tLoss: 421.8543\n",
      "Training Epoch: 2 [35008/49669]\tLoss: 435.0604\n",
      "Training Epoch: 2 [35072/49669]\tLoss: 437.1436\n",
      "Training Epoch: 2 [35136/49669]\tLoss: 411.3026\n",
      "Training Epoch: 2 [35200/49669]\tLoss: 437.2538\n",
      "Training Epoch: 2 [35264/49669]\tLoss: 423.8185\n",
      "Training Epoch: 2 [35328/49669]\tLoss: 432.1194\n",
      "Training Epoch: 2 [35392/49669]\tLoss: 436.1373\n",
      "Training Epoch: 2 [35456/49669]\tLoss: 452.0914\n",
      "Training Epoch: 2 [35520/49669]\tLoss: 420.6103\n",
      "Training Epoch: 2 [35584/49669]\tLoss: 423.8100\n",
      "Training Epoch: 2 [35648/49669]\tLoss: 446.9123\n",
      "Training Epoch: 2 [35712/49669]\tLoss: 446.2788\n",
      "Training Epoch: 2 [35776/49669]\tLoss: 417.0784\n",
      "Training Epoch: 2 [35840/49669]\tLoss: 400.5186\n",
      "Training Epoch: 2 [35904/49669]\tLoss: 392.6465\n",
      "Training Epoch: 2 [35968/49669]\tLoss: 411.1736\n",
      "Training Epoch: 2 [36032/49669]\tLoss: 431.9098\n",
      "Training Epoch: 2 [36096/49669]\tLoss: 406.2967\n",
      "Training Epoch: 2 [36160/49669]\tLoss: 424.5669\n",
      "Training Epoch: 2 [36224/49669]\tLoss: 441.2352\n",
      "Training Epoch: 2 [36288/49669]\tLoss: 431.5934\n",
      "Training Epoch: 2 [36352/49669]\tLoss: 432.0773\n",
      "Training Epoch: 2 [36416/49669]\tLoss: 446.7356\n",
      "Training Epoch: 2 [36480/49669]\tLoss: 429.8061\n",
      "Training Epoch: 2 [36544/49669]\tLoss: 393.8115\n",
      "Training Epoch: 2 [36608/49669]\tLoss: 442.2327\n",
      "Training Epoch: 2 [36672/49669]\tLoss: 392.3875\n",
      "Training Epoch: 2 [36736/49669]\tLoss: 410.4974\n",
      "Training Epoch: 2 [36800/49669]\tLoss: 422.7343\n",
      "Training Epoch: 2 [36864/49669]\tLoss: 439.8178\n",
      "Training Epoch: 2 [36928/49669]\tLoss: 406.2792\n",
      "Training Epoch: 2 [36992/49669]\tLoss: 394.1925\n",
      "Training Epoch: 2 [37056/49669]\tLoss: 458.9373\n",
      "Training Epoch: 2 [37120/49669]\tLoss: 389.2833\n",
      "Training Epoch: 2 [37184/49669]\tLoss: 426.6081\n",
      "Training Epoch: 2 [37248/49669]\tLoss: 421.3245\n",
      "Training Epoch: 2 [37312/49669]\tLoss: 424.9927\n",
      "Training Epoch: 2 [37376/49669]\tLoss: 423.2941\n",
      "Training Epoch: 2 [37440/49669]\tLoss: 408.2354\n",
      "Training Epoch: 2 [37504/49669]\tLoss: 423.5400\n",
      "Training Epoch: 2 [37568/49669]\tLoss: 429.4031\n",
      "Training Epoch: 2 [37632/49669]\tLoss: 425.0952\n",
      "Training Epoch: 2 [37696/49669]\tLoss: 441.9325\n",
      "Training Epoch: 2 [37760/49669]\tLoss: 441.0957\n",
      "Training Epoch: 2 [37824/49669]\tLoss: 420.8190\n",
      "Training Epoch: 2 [37888/49669]\tLoss: 412.1550\n",
      "Training Epoch: 2 [37952/49669]\tLoss: 441.5731\n",
      "Training Epoch: 2 [38016/49669]\tLoss: 400.7493\n",
      "Training Epoch: 2 [38080/49669]\tLoss: 398.3127\n",
      "Training Epoch: 2 [38144/49669]\tLoss: 422.3391\n",
      "Training Epoch: 2 [38208/49669]\tLoss: 399.6611\n",
      "Training Epoch: 2 [38272/49669]\tLoss: 415.3370\n",
      "Training Epoch: 2 [38336/49669]\tLoss: 451.8753\n",
      "Training Epoch: 2 [38400/49669]\tLoss: 407.2914\n",
      "Training Epoch: 2 [38464/49669]\tLoss: 443.9539\n",
      "Training Epoch: 2 [38528/49669]\tLoss: 412.9028\n",
      "Training Epoch: 2 [38592/49669]\tLoss: 397.2524\n",
      "Training Epoch: 2 [38656/49669]\tLoss: 438.9178\n",
      "Training Epoch: 2 [38720/49669]\tLoss: 432.0514\n",
      "Training Epoch: 2 [38784/49669]\tLoss: 428.9593\n",
      "Training Epoch: 2 [38848/49669]\tLoss: 421.4671\n",
      "Training Epoch: 2 [38912/49669]\tLoss: 433.0941\n",
      "Training Epoch: 2 [38976/49669]\tLoss: 418.9784\n",
      "Training Epoch: 2 [39040/49669]\tLoss: 427.8532\n",
      "Training Epoch: 2 [39104/49669]\tLoss: 427.5068\n",
      "Training Epoch: 2 [39168/49669]\tLoss: 434.0962\n",
      "Training Epoch: 2 [39232/49669]\tLoss: 434.7351\n",
      "Training Epoch: 2 [39296/49669]\tLoss: 435.6977\n",
      "Training Epoch: 2 [39360/49669]\tLoss: 410.4753\n",
      "Training Epoch: 2 [39424/49669]\tLoss: 424.0892\n",
      "Training Epoch: 2 [39488/49669]\tLoss: 442.1516\n",
      "Training Epoch: 2 [39552/49669]\tLoss: 414.9919\n",
      "Training Epoch: 2 [39616/49669]\tLoss: 469.1233\n",
      "Training Epoch: 2 [39680/49669]\tLoss: 410.6010\n",
      "Training Epoch: 2 [39744/49669]\tLoss: 408.3651\n",
      "Training Epoch: 2 [39808/49669]\tLoss: 405.6578\n",
      "Training Epoch: 2 [39872/49669]\tLoss: 421.4623\n",
      "Training Epoch: 2 [39936/49669]\tLoss: 406.6161\n",
      "Training Epoch: 2 [40000/49669]\tLoss: 445.7615\n",
      "Training Epoch: 2 [40064/49669]\tLoss: 421.1451\n",
      "Training Epoch: 2 [40128/49669]\tLoss: 414.4211\n",
      "Training Epoch: 2 [40192/49669]\tLoss: 409.1865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [40256/49669]\tLoss: 445.5419\n",
      "Training Epoch: 2 [40320/49669]\tLoss: 437.2953\n",
      "Training Epoch: 2 [40384/49669]\tLoss: 439.6790\n",
      "Training Epoch: 2 [40448/49669]\tLoss: 443.0732\n",
      "Training Epoch: 2 [40512/49669]\tLoss: 404.5876\n",
      "Training Epoch: 2 [40576/49669]\tLoss: 412.2504\n",
      "Training Epoch: 2 [40640/49669]\tLoss: 410.7965\n",
      "Training Epoch: 2 [40704/49669]\tLoss: 405.5023\n",
      "Training Epoch: 2 [40768/49669]\tLoss: 420.0228\n",
      "Training Epoch: 2 [40832/49669]\tLoss: 423.2888\n",
      "Training Epoch: 2 [40896/49669]\tLoss: 410.7572\n",
      "Training Epoch: 2 [40960/49669]\tLoss: 421.1574\n",
      "Training Epoch: 2 [41024/49669]\tLoss: 390.9901\n",
      "Training Epoch: 2 [41088/49669]\tLoss: 404.1254\n",
      "Training Epoch: 2 [41152/49669]\tLoss: 413.1286\n",
      "Training Epoch: 2 [41216/49669]\tLoss: 419.9622\n",
      "Training Epoch: 2 [41280/49669]\tLoss: 427.8293\n",
      "Training Epoch: 2 [41344/49669]\tLoss: 451.5304\n",
      "Training Epoch: 2 [41408/49669]\tLoss: 426.2574\n",
      "Training Epoch: 2 [41472/49669]\tLoss: 436.4871\n",
      "Training Epoch: 2 [41536/49669]\tLoss: 413.6287\n",
      "Training Epoch: 2 [41600/49669]\tLoss: 456.0383\n",
      "Training Epoch: 2 [41664/49669]\tLoss: 429.3273\n",
      "Training Epoch: 2 [41728/49669]\tLoss: 414.2842\n",
      "Training Epoch: 2 [41792/49669]\tLoss: 441.0318\n",
      "Training Epoch: 2 [41856/49669]\tLoss: 418.8608\n",
      "Training Epoch: 2 [41920/49669]\tLoss: 434.1883\n",
      "Training Epoch: 2 [41984/49669]\tLoss: 423.3979\n",
      "Training Epoch: 2 [42048/49669]\tLoss: 468.1337\n",
      "Training Epoch: 2 [42112/49669]\tLoss: 429.6733\n",
      "Training Epoch: 2 [42176/49669]\tLoss: 416.8068\n",
      "Training Epoch: 2 [42240/49669]\tLoss: 434.5034\n",
      "Training Epoch: 2 [42304/49669]\tLoss: 430.4176\n",
      "Training Epoch: 2 [42368/49669]\tLoss: 468.7128\n",
      "Training Epoch: 2 [42432/49669]\tLoss: 410.0528\n",
      "Training Epoch: 2 [42496/49669]\tLoss: 427.5571\n",
      "Training Epoch: 2 [42560/49669]\tLoss: 416.9099\n",
      "Training Epoch: 2 [42624/49669]\tLoss: 439.8313\n",
      "Training Epoch: 2 [42688/49669]\tLoss: 416.8482\n",
      "Training Epoch: 2 [42752/49669]\tLoss: 417.7980\n",
      "Training Epoch: 2 [42816/49669]\tLoss: 392.8149\n",
      "Training Epoch: 2 [42880/49669]\tLoss: 434.1311\n",
      "Training Epoch: 2 [42944/49669]\tLoss: 416.3859\n",
      "Training Epoch: 2 [43008/49669]\tLoss: 426.9428\n",
      "Training Epoch: 2 [43072/49669]\tLoss: 436.4742\n",
      "Training Epoch: 2 [43136/49669]\tLoss: 425.4897\n",
      "Training Epoch: 2 [43200/49669]\tLoss: 400.9637\n",
      "Training Epoch: 2 [43264/49669]\tLoss: 381.6989\n",
      "Training Epoch: 2 [43328/49669]\tLoss: 378.7420\n",
      "Training Epoch: 2 [43392/49669]\tLoss: 429.9241\n",
      "Training Epoch: 2 [43456/49669]\tLoss: 397.3597\n",
      "Training Epoch: 2 [43520/49669]\tLoss: 444.9449\n",
      "Training Epoch: 2 [43584/49669]\tLoss: 410.3742\n",
      "Training Epoch: 2 [43648/49669]\tLoss: 432.0877\n",
      "Training Epoch: 2 [43712/49669]\tLoss: 402.0683\n",
      "Training Epoch: 2 [43776/49669]\tLoss: 432.0820\n",
      "Training Epoch: 2 [43840/49669]\tLoss: 406.5283\n",
      "Training Epoch: 2 [43904/49669]\tLoss: 434.8228\n",
      "Training Epoch: 2 [43968/49669]\tLoss: 431.7304\n",
      "Training Epoch: 2 [44032/49669]\tLoss: 407.8243\n",
      "Training Epoch: 2 [44096/49669]\tLoss: 406.1685\n",
      "Training Epoch: 2 [44160/49669]\tLoss: 405.3955\n",
      "Training Epoch: 2 [44224/49669]\tLoss: 412.5448\n",
      "Training Epoch: 2 [44288/49669]\tLoss: 426.4049\n",
      "Training Epoch: 2 [44352/49669]\tLoss: 420.1877\n",
      "Training Epoch: 2 [44416/49669]\tLoss: 430.1444\n",
      "Training Epoch: 2 [44480/49669]\tLoss: 445.5141\n",
      "Training Epoch: 2 [44544/49669]\tLoss: 413.5025\n",
      "Training Epoch: 2 [44608/49669]\tLoss: 420.3539\n",
      "Training Epoch: 2 [44672/49669]\tLoss: 406.9531\n",
      "Training Epoch: 2 [44736/49669]\tLoss: 426.0951\n",
      "Training Epoch: 2 [44800/49669]\tLoss: 437.2383\n",
      "Training Epoch: 2 [44864/49669]\tLoss: 451.0482\n",
      "Training Epoch: 2 [44928/49669]\tLoss: 424.8765\n",
      "Training Epoch: 2 [44992/49669]\tLoss: 426.4543\n",
      "Training Epoch: 2 [45056/49669]\tLoss: 412.0051\n",
      "Training Epoch: 2 [45120/49669]\tLoss: 423.9014\n",
      "Training Epoch: 2 [45184/49669]\tLoss: 431.4856\n",
      "Training Epoch: 2 [45248/49669]\tLoss: 419.9944\n",
      "Training Epoch: 2 [45312/49669]\tLoss: 418.1203\n",
      "Training Epoch: 2 [45376/49669]\tLoss: 386.1141\n",
      "Training Epoch: 2 [45440/49669]\tLoss: 411.2431\n",
      "Training Epoch: 2 [45504/49669]\tLoss: 438.5440\n",
      "Training Epoch: 2 [45568/49669]\tLoss: 425.9492\n",
      "Training Epoch: 2 [45632/49669]\tLoss: 455.4068\n",
      "Training Epoch: 2 [45696/49669]\tLoss: 444.3514\n",
      "Training Epoch: 2 [45760/49669]\tLoss: 395.3489\n",
      "Training Epoch: 2 [45824/49669]\tLoss: 410.7764\n",
      "Training Epoch: 2 [45888/49669]\tLoss: 439.4040\n",
      "Training Epoch: 2 [45952/49669]\tLoss: 415.8171\n",
      "Training Epoch: 2 [46016/49669]\tLoss: 391.5457\n",
      "Training Epoch: 2 [46080/49669]\tLoss: 409.0241\n",
      "Training Epoch: 2 [46144/49669]\tLoss: 402.7810\n",
      "Training Epoch: 2 [46208/49669]\tLoss: 428.2409\n",
      "Training Epoch: 2 [46272/49669]\tLoss: 397.9016\n",
      "Training Epoch: 2 [46336/49669]\tLoss: 425.2188\n",
      "Training Epoch: 2 [46400/49669]\tLoss: 430.5807\n",
      "Training Epoch: 2 [46464/49669]\tLoss: 429.1732\n",
      "Training Epoch: 2 [46528/49669]\tLoss: 429.3276\n",
      "Training Epoch: 2 [46592/49669]\tLoss: 435.4514\n",
      "Training Epoch: 2 [46656/49669]\tLoss: 410.4595\n",
      "Training Epoch: 2 [46720/49669]\tLoss: 423.0874\n",
      "Training Epoch: 2 [46784/49669]\tLoss: 425.5751\n",
      "Training Epoch: 2 [46848/49669]\tLoss: 442.3070\n",
      "Training Epoch: 2 [46912/49669]\tLoss: 401.5292\n",
      "Training Epoch: 2 [46976/49669]\tLoss: 435.1560\n",
      "Training Epoch: 2 [47040/49669]\tLoss: 440.2111\n",
      "Training Epoch: 2 [47104/49669]\tLoss: 433.8512\n",
      "Training Epoch: 2 [47168/49669]\tLoss: 410.2717\n",
      "Training Epoch: 2 [47232/49669]\tLoss: 432.1457\n",
      "Training Epoch: 2 [47296/49669]\tLoss: 413.6297\n",
      "Training Epoch: 2 [47360/49669]\tLoss: 435.7024\n",
      "Training Epoch: 2 [47424/49669]\tLoss: 421.3685\n",
      "Training Epoch: 2 [47488/49669]\tLoss: 446.8512\n",
      "Training Epoch: 2 [47552/49669]\tLoss: 412.5899\n",
      "Training Epoch: 2 [47616/49669]\tLoss: 444.4058\n",
      "Training Epoch: 2 [47680/49669]\tLoss: 405.9453\n",
      "Training Epoch: 2 [47744/49669]\tLoss: 441.8551\n",
      "Training Epoch: 2 [47808/49669]\tLoss: 437.0023\n",
      "Training Epoch: 2 [47872/49669]\tLoss: 408.2753\n",
      "Training Epoch: 2 [47936/49669]\tLoss: 445.2556\n",
      "Training Epoch: 2 [48000/49669]\tLoss: 409.8687\n",
      "Training Epoch: 2 [48064/49669]\tLoss: 443.3623\n",
      "Training Epoch: 2 [48128/49669]\tLoss: 416.5783\n",
      "Training Epoch: 2 [48192/49669]\tLoss: 431.2369\n",
      "Training Epoch: 2 [48256/49669]\tLoss: 455.5337\n",
      "Training Epoch: 2 [48320/49669]\tLoss: 430.4893\n",
      "Training Epoch: 2 [48384/49669]\tLoss: 423.2481\n",
      "Training Epoch: 2 [48448/49669]\tLoss: 433.6971\n",
      "Training Epoch: 2 [48512/49669]\tLoss: 439.9312\n",
      "Training Epoch: 2 [48576/49669]\tLoss: 412.2027\n",
      "Training Epoch: 2 [48640/49669]\tLoss: 445.9139\n",
      "Training Epoch: 2 [48704/49669]\tLoss: 417.2356\n",
      "Training Epoch: 2 [48768/49669]\tLoss: 434.9410\n",
      "Training Epoch: 2 [48832/49669]\tLoss: 412.8294\n",
      "Training Epoch: 2 [48896/49669]\tLoss: 441.4544\n",
      "Training Epoch: 2 [48960/49669]\tLoss: 409.3375\n",
      "Training Epoch: 2 [49024/49669]\tLoss: 415.4354\n",
      "Training Epoch: 2 [49088/49669]\tLoss: 442.3003\n",
      "Training Epoch: 2 [49152/49669]\tLoss: 432.5184\n",
      "Training Epoch: 2 [49216/49669]\tLoss: 398.3214\n",
      "Training Epoch: 2 [49280/49669]\tLoss: 406.0890\n",
      "Training Epoch: 2 [49344/49669]\tLoss: 409.2486\n",
      "Training Epoch: 2 [49408/49669]\tLoss: 426.6703\n",
      "Training Epoch: 2 [49472/49669]\tLoss: 433.0695\n",
      "Training Epoch: 2 [49536/49669]\tLoss: 418.5157\n",
      "Training Epoch: 2 [49600/49669]\tLoss: 441.8791\n",
      "Training Epoch: 2 [49664/49669]\tLoss: 413.1156\n",
      "Training Epoch: 2 [49669/49669]\tLoss: 500.8163\n",
      "Training Epoch: 2 [5519/5519]\tLoss: 426.0896\n",
      "Training Epoch: 3 [64/49669]\tLoss: 411.0879\n",
      "Training Epoch: 3 [128/49669]\tLoss: 427.8982\n",
      "Training Epoch: 3 [192/49669]\tLoss: 449.7835\n",
      "Training Epoch: 3 [256/49669]\tLoss: 402.9671\n",
      "Training Epoch: 3 [320/49669]\tLoss: 442.3071\n",
      "Training Epoch: 3 [384/49669]\tLoss: 446.5260\n",
      "Training Epoch: 3 [448/49669]\tLoss: 428.1651\n",
      "Training Epoch: 3 [512/49669]\tLoss: 427.1426\n",
      "Training Epoch: 3 [576/49669]\tLoss: 409.1063\n",
      "Training Epoch: 3 [640/49669]\tLoss: 415.0685\n",
      "Training Epoch: 3 [704/49669]\tLoss: 409.0353\n",
      "Training Epoch: 3 [768/49669]\tLoss: 424.9651\n",
      "Training Epoch: 3 [832/49669]\tLoss: 441.2006\n",
      "Training Epoch: 3 [896/49669]\tLoss: 430.1364\n",
      "Training Epoch: 3 [960/49669]\tLoss: 440.2968\n",
      "Training Epoch: 3 [1024/49669]\tLoss: 435.0436\n",
      "Training Epoch: 3 [1088/49669]\tLoss: 424.1865\n",
      "Training Epoch: 3 [1152/49669]\tLoss: 401.4648\n",
      "Training Epoch: 3 [1216/49669]\tLoss: 413.0602\n",
      "Training Epoch: 3 [1280/49669]\tLoss: 455.4285\n",
      "Training Epoch: 3 [1344/49669]\tLoss: 431.7482\n",
      "Training Epoch: 3 [1408/49669]\tLoss: 428.3931\n",
      "Training Epoch: 3 [1472/49669]\tLoss: 415.5811\n",
      "Training Epoch: 3 [1536/49669]\tLoss: 405.6711\n",
      "Training Epoch: 3 [1600/49669]\tLoss: 443.8612\n",
      "Training Epoch: 3 [1664/49669]\tLoss: 419.2259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [1728/49669]\tLoss: 416.2283\n",
      "Training Epoch: 3 [1792/49669]\tLoss: 428.6598\n",
      "Training Epoch: 3 [1856/49669]\tLoss: 425.1640\n",
      "Training Epoch: 3 [1920/49669]\tLoss: 429.4701\n",
      "Training Epoch: 3 [1984/49669]\tLoss: 392.7811\n",
      "Training Epoch: 3 [2048/49669]\tLoss: 395.0222\n",
      "Training Epoch: 3 [2112/49669]\tLoss: 452.1660\n",
      "Training Epoch: 3 [2176/49669]\tLoss: 436.5744\n",
      "Training Epoch: 3 [2240/49669]\tLoss: 407.5908\n",
      "Training Epoch: 3 [2304/49669]\tLoss: 397.2968\n",
      "Training Epoch: 3 [2368/49669]\tLoss: 414.8069\n",
      "Training Epoch: 3 [2432/49669]\tLoss: 440.5130\n",
      "Training Epoch: 3 [2496/49669]\tLoss: 423.1903\n",
      "Training Epoch: 3 [2560/49669]\tLoss: 428.3907\n",
      "Training Epoch: 3 [2624/49669]\tLoss: 415.8532\n",
      "Training Epoch: 3 [2688/49669]\tLoss: 405.0765\n",
      "Training Epoch: 3 [2752/49669]\tLoss: 435.9498\n",
      "Training Epoch: 3 [2816/49669]\tLoss: 427.3687\n",
      "Training Epoch: 3 [2880/49669]\tLoss: 396.1945\n",
      "Training Epoch: 3 [2944/49669]\tLoss: 398.6222\n",
      "Training Epoch: 3 [3008/49669]\tLoss: 411.4513\n",
      "Training Epoch: 3 [3072/49669]\tLoss: 390.9300\n",
      "Training Epoch: 3 [3136/49669]\tLoss: 438.2684\n",
      "Training Epoch: 3 [3200/49669]\tLoss: 435.3345\n",
      "Training Epoch: 3 [3264/49669]\tLoss: 435.3701\n",
      "Training Epoch: 3 [3328/49669]\tLoss: 411.2778\n",
      "Training Epoch: 3 [3392/49669]\tLoss: 427.2055\n",
      "Training Epoch: 3 [3456/49669]\tLoss: 420.6684\n",
      "Training Epoch: 3 [3520/49669]\tLoss: 396.3192\n",
      "Training Epoch: 3 [3584/49669]\tLoss: 414.1925\n",
      "Training Epoch: 3 [3648/49669]\tLoss: 410.4971\n",
      "Training Epoch: 3 [3712/49669]\tLoss: 421.1738\n",
      "Training Epoch: 3 [3776/49669]\tLoss: 452.5938\n",
      "Training Epoch: 3 [3840/49669]\tLoss: 407.5786\n",
      "Training Epoch: 3 [3904/49669]\tLoss: 430.1171\n",
      "Training Epoch: 3 [3968/49669]\tLoss: 422.8623\n",
      "Training Epoch: 3 [4032/49669]\tLoss: 426.8372\n",
      "Training Epoch: 3 [4096/49669]\tLoss: 458.1476\n",
      "Training Epoch: 3 [4160/49669]\tLoss: 419.8719\n",
      "Training Epoch: 3 [4224/49669]\tLoss: 398.5949\n",
      "Training Epoch: 3 [4288/49669]\tLoss: 448.8674\n",
      "Training Epoch: 3 [4352/49669]\tLoss: 441.2815\n",
      "Training Epoch: 3 [4416/49669]\tLoss: 415.4536\n",
      "Training Epoch: 3 [4480/49669]\tLoss: 389.4124\n",
      "Training Epoch: 3 [4544/49669]\tLoss: 433.9584\n",
      "Training Epoch: 3 [4608/49669]\tLoss: 420.6022\n",
      "Training Epoch: 3 [4672/49669]\tLoss: 424.7247\n",
      "Training Epoch: 3 [4736/49669]\tLoss: 423.3707\n",
      "Training Epoch: 3 [4800/49669]\tLoss: 439.4271\n",
      "Training Epoch: 3 [4864/49669]\tLoss: 449.0474\n",
      "Training Epoch: 3 [4928/49669]\tLoss: 439.6088\n",
      "Training Epoch: 3 [4992/49669]\tLoss: 448.6592\n",
      "Training Epoch: 3 [5056/49669]\tLoss: 413.4418\n",
      "Training Epoch: 3 [5120/49669]\tLoss: 430.6838\n",
      "Training Epoch: 3 [5184/49669]\tLoss: 436.6587\n",
      "Training Epoch: 3 [5248/49669]\tLoss: 436.7289\n",
      "Training Epoch: 3 [5312/49669]\tLoss: 426.5590\n",
      "Training Epoch: 3 [5376/49669]\tLoss: 422.1608\n",
      "Training Epoch: 3 [5440/49669]\tLoss: 398.0605\n",
      "Training Epoch: 3 [5504/49669]\tLoss: 432.6817\n",
      "Training Epoch: 3 [5568/49669]\tLoss: 410.1061\n",
      "Training Epoch: 3 [5632/49669]\tLoss: 409.3288\n",
      "Training Epoch: 3 [5696/49669]\tLoss: 384.3966\n",
      "Training Epoch: 3 [5760/49669]\tLoss: 440.4962\n",
      "Training Epoch: 3 [5824/49669]\tLoss: 442.6562\n",
      "Training Epoch: 3 [5888/49669]\tLoss: 408.3798\n",
      "Training Epoch: 3 [5952/49669]\tLoss: 446.4737\n",
      "Training Epoch: 3 [6016/49669]\tLoss: 454.9334\n",
      "Training Epoch: 3 [6080/49669]\tLoss: 445.4464\n",
      "Training Epoch: 3 [6144/49669]\tLoss: 451.8527\n",
      "Training Epoch: 3 [6208/49669]\tLoss: 438.5241\n",
      "Training Epoch: 3 [6272/49669]\tLoss: 408.3945\n",
      "Training Epoch: 3 [6336/49669]\tLoss: 438.3241\n",
      "Training Epoch: 3 [6400/49669]\tLoss: 396.6540\n",
      "Training Epoch: 3 [6464/49669]\tLoss: 383.6772\n",
      "Training Epoch: 3 [6528/49669]\tLoss: 415.5912\n",
      "Training Epoch: 3 [6592/49669]\tLoss: 406.6113\n",
      "Training Epoch: 3 [6656/49669]\tLoss: 391.7884\n",
      "Training Epoch: 3 [6720/49669]\tLoss: 434.3680\n",
      "Training Epoch: 3 [6784/49669]\tLoss: 450.6249\n",
      "Training Epoch: 3 [6848/49669]\tLoss: 398.3489\n",
      "Training Epoch: 3 [6912/49669]\tLoss: 417.4926\n",
      "Training Epoch: 3 [6976/49669]\tLoss: 405.8246\n",
      "Training Epoch: 3 [7040/49669]\tLoss: 441.0431\n",
      "Training Epoch: 3 [7104/49669]\tLoss: 435.4050\n",
      "Training Epoch: 3 [7168/49669]\tLoss: 394.9959\n",
      "Training Epoch: 3 [7232/49669]\tLoss: 421.1656\n",
      "Training Epoch: 3 [7296/49669]\tLoss: 428.8437\n",
      "Training Epoch: 3 [7360/49669]\tLoss: 404.3866\n",
      "Training Epoch: 3 [7424/49669]\tLoss: 451.8733\n",
      "Training Epoch: 3 [7488/49669]\tLoss: 428.7624\n",
      "Training Epoch: 3 [7552/49669]\tLoss: 427.0257\n",
      "Training Epoch: 3 [7616/49669]\tLoss: 406.1932\n",
      "Training Epoch: 3 [7680/49669]\tLoss: 421.0083\n",
      "Training Epoch: 3 [7744/49669]\tLoss: 399.8414\n",
      "Training Epoch: 3 [7808/49669]\tLoss: 431.9544\n",
      "Training Epoch: 3 [7872/49669]\tLoss: 420.0175\n",
      "Training Epoch: 3 [7936/49669]\tLoss: 400.5489\n",
      "Training Epoch: 3 [8000/49669]\tLoss: 418.6013\n",
      "Training Epoch: 3 [8064/49669]\tLoss: 431.9324\n",
      "Training Epoch: 3 [8128/49669]\tLoss: 424.3042\n",
      "Training Epoch: 3 [8192/49669]\tLoss: 415.5436\n",
      "Training Epoch: 3 [8256/49669]\tLoss: 425.7036\n",
      "Training Epoch: 3 [8320/49669]\tLoss: 428.8347\n",
      "Training Epoch: 3 [8384/49669]\tLoss: 415.3456\n",
      "Training Epoch: 3 [8448/49669]\tLoss: 403.3920\n",
      "Training Epoch: 3 [8512/49669]\tLoss: 399.5582\n",
      "Training Epoch: 3 [8576/49669]\tLoss: 418.4795\n",
      "Training Epoch: 3 [8640/49669]\tLoss: 412.6919\n",
      "Training Epoch: 3 [8704/49669]\tLoss: 424.8740\n",
      "Training Epoch: 3 [8768/49669]\tLoss: 424.2769\n",
      "Training Epoch: 3 [8832/49669]\tLoss: 426.8520\n",
      "Training Epoch: 3 [8896/49669]\tLoss: 455.7399\n",
      "Training Epoch: 3 [8960/49669]\tLoss: 400.7903\n",
      "Training Epoch: 3 [9024/49669]\tLoss: 424.7290\n",
      "Training Epoch: 3 [9088/49669]\tLoss: 414.1420\n",
      "Training Epoch: 3 [9152/49669]\tLoss: 416.4781\n",
      "Training Epoch: 3 [9216/49669]\tLoss: 394.6102\n",
      "Training Epoch: 3 [9280/49669]\tLoss: 454.4424\n",
      "Training Epoch: 3 [9344/49669]\tLoss: 405.0374\n",
      "Training Epoch: 3 [9408/49669]\tLoss: 428.2380\n",
      "Training Epoch: 3 [9472/49669]\tLoss: 387.7786\n",
      "Training Epoch: 3 [9536/49669]\tLoss: 406.4756\n",
      "Training Epoch: 3 [9600/49669]\tLoss: 419.5528\n",
      "Training Epoch: 3 [9664/49669]\tLoss: 434.7110\n",
      "Training Epoch: 3 [9728/49669]\tLoss: 425.3130\n",
      "Training Epoch: 3 [9792/49669]\tLoss: 425.2668\n",
      "Training Epoch: 3 [9856/49669]\tLoss: 369.2773\n",
      "Training Epoch: 3 [9920/49669]\tLoss: 438.9016\n",
      "Training Epoch: 3 [9984/49669]\tLoss: 416.1572\n",
      "Training Epoch: 3 [10048/49669]\tLoss: 432.7556\n",
      "Training Epoch: 3 [10112/49669]\tLoss: 423.3917\n",
      "Training Epoch: 3 [10176/49669]\tLoss: 403.6504\n",
      "Training Epoch: 3 [10240/49669]\tLoss: 421.2619\n",
      "Training Epoch: 3 [10304/49669]\tLoss: 432.2675\n",
      "Training Epoch: 3 [10368/49669]\tLoss: 404.3485\n",
      "Training Epoch: 3 [10432/49669]\tLoss: 412.5696\n",
      "Training Epoch: 3 [10496/49669]\tLoss: 408.6007\n",
      "Training Epoch: 3 [10560/49669]\tLoss: 412.5341\n",
      "Training Epoch: 3 [10624/49669]\tLoss: 406.2472\n",
      "Training Epoch: 3 [10688/49669]\tLoss: 417.5487\n",
      "Training Epoch: 3 [10752/49669]\tLoss: 444.5778\n",
      "Training Epoch: 3 [10816/49669]\tLoss: 465.5157\n",
      "Training Epoch: 3 [10880/49669]\tLoss: 404.6335\n",
      "Training Epoch: 3 [10944/49669]\tLoss: 419.2651\n",
      "Training Epoch: 3 [11008/49669]\tLoss: 415.3287\n",
      "Training Epoch: 3 [11072/49669]\tLoss: 412.0825\n",
      "Training Epoch: 3 [11136/49669]\tLoss: 412.7415\n",
      "Training Epoch: 3 [11200/49669]\tLoss: 413.7152\n",
      "Training Epoch: 3 [11264/49669]\tLoss: 447.8090\n",
      "Training Epoch: 3 [11328/49669]\tLoss: 399.3971\n",
      "Training Epoch: 3 [11392/49669]\tLoss: 390.2024\n",
      "Training Epoch: 3 [11456/49669]\tLoss: 411.0220\n",
      "Training Epoch: 3 [11520/49669]\tLoss: 394.1743\n",
      "Training Epoch: 3 [11584/49669]\tLoss: 400.1699\n",
      "Training Epoch: 3 [11648/49669]\tLoss: 412.8361\n",
      "Training Epoch: 3 [11712/49669]\tLoss: 455.3863\n",
      "Training Epoch: 3 [11776/49669]\tLoss: 427.4065\n",
      "Training Epoch: 3 [11840/49669]\tLoss: 406.9115\n",
      "Training Epoch: 3 [11904/49669]\tLoss: 423.9034\n",
      "Training Epoch: 3 [11968/49669]\tLoss: 439.6795\n",
      "Training Epoch: 3 [12032/49669]\tLoss: 410.5760\n",
      "Training Epoch: 3 [12096/49669]\tLoss: 437.3933\n",
      "Training Epoch: 3 [12160/49669]\tLoss: 426.6873\n",
      "Training Epoch: 3 [12224/49669]\tLoss: 431.6626\n",
      "Training Epoch: 3 [12288/49669]\tLoss: 432.3581\n",
      "Training Epoch: 3 [12352/49669]\tLoss: 444.1084\n",
      "Training Epoch: 3 [12416/49669]\tLoss: 435.4487\n",
      "Training Epoch: 3 [12480/49669]\tLoss: 434.8622\n",
      "Training Epoch: 3 [12544/49669]\tLoss: 428.7269\n",
      "Training Epoch: 3 [12608/49669]\tLoss: 424.1012\n",
      "Training Epoch: 3 [12672/49669]\tLoss: 397.0330\n",
      "Training Epoch: 3 [12736/49669]\tLoss: 418.2192\n",
      "Training Epoch: 3 [12800/49669]\tLoss: 419.8952\n",
      "Training Epoch: 3 [12864/49669]\tLoss: 423.5535\n",
      "Training Epoch: 3 [12928/49669]\tLoss: 428.0411\n",
      "Training Epoch: 3 [12992/49669]\tLoss: 385.5372\n",
      "Training Epoch: 3 [13056/49669]\tLoss: 437.7176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [13120/49669]\tLoss: 437.8592\n",
      "Training Epoch: 3 [13184/49669]\tLoss: 431.7858\n",
      "Training Epoch: 3 [13248/49669]\tLoss: 441.6584\n",
      "Training Epoch: 3 [13312/49669]\tLoss: 421.4445\n",
      "Training Epoch: 3 [13376/49669]\tLoss: 430.1502\n",
      "Training Epoch: 3 [13440/49669]\tLoss: 411.0287\n",
      "Training Epoch: 3 [13504/49669]\tLoss: 419.6838\n",
      "Training Epoch: 3 [13568/49669]\tLoss: 418.8200\n",
      "Training Epoch: 3 [13632/49669]\tLoss: 424.0332\n",
      "Training Epoch: 3 [13696/49669]\tLoss: 426.4273\n",
      "Training Epoch: 3 [13760/49669]\tLoss: 425.5134\n",
      "Training Epoch: 3 [13824/49669]\tLoss: 396.3084\n",
      "Training Epoch: 3 [13888/49669]\tLoss: 393.7916\n",
      "Training Epoch: 3 [13952/49669]\tLoss: 408.6307\n",
      "Training Epoch: 3 [14016/49669]\tLoss: 396.1949\n",
      "Training Epoch: 3 [14080/49669]\tLoss: 393.4218\n",
      "Training Epoch: 3 [14144/49669]\tLoss: 410.0537\n",
      "Training Epoch: 3 [14208/49669]\tLoss: 412.8867\n",
      "Training Epoch: 3 [14272/49669]\tLoss: 408.9247\n",
      "Training Epoch: 3 [14336/49669]\tLoss: 429.1917\n",
      "Training Epoch: 3 [14400/49669]\tLoss: 415.6924\n",
      "Training Epoch: 3 [14464/49669]\tLoss: 418.7497\n",
      "Training Epoch: 3 [14528/49669]\tLoss: 422.8897\n",
      "Training Epoch: 3 [14592/49669]\tLoss: 462.9962\n",
      "Training Epoch: 3 [14656/49669]\tLoss: 428.9787\n",
      "Training Epoch: 3 [14720/49669]\tLoss: 416.0262\n",
      "Training Epoch: 3 [14784/49669]\tLoss: 410.6802\n",
      "Training Epoch: 3 [14848/49669]\tLoss: 400.9237\n",
      "Training Epoch: 3 [14912/49669]\tLoss: 428.4255\n",
      "Training Epoch: 3 [14976/49669]\tLoss: 410.3338\n",
      "Training Epoch: 3 [15040/49669]\tLoss: 429.0314\n",
      "Training Epoch: 3 [15104/49669]\tLoss: 410.2406\n",
      "Training Epoch: 3 [15168/49669]\tLoss: 424.0314\n",
      "Training Epoch: 3 [15232/49669]\tLoss: 409.7631\n",
      "Training Epoch: 3 [15296/49669]\tLoss: 431.0047\n",
      "Training Epoch: 3 [15360/49669]\tLoss: 412.4262\n",
      "Training Epoch: 3 [15424/49669]\tLoss: 440.9269\n",
      "Training Epoch: 3 [15488/49669]\tLoss: 405.6171\n",
      "Training Epoch: 3 [15552/49669]\tLoss: 435.1165\n",
      "Training Epoch: 3 [15616/49669]\tLoss: 428.3730\n",
      "Training Epoch: 3 [15680/49669]\tLoss: 424.2977\n",
      "Training Epoch: 3 [15744/49669]\tLoss: 412.7665\n",
      "Training Epoch: 3 [15808/49669]\tLoss: 413.3179\n",
      "Training Epoch: 3 [15872/49669]\tLoss: 438.1714\n",
      "Training Epoch: 3 [15936/49669]\tLoss: 411.0554\n",
      "Training Epoch: 3 [16000/49669]\tLoss: 437.5890\n",
      "Training Epoch: 3 [16064/49669]\tLoss: 412.1872\n",
      "Training Epoch: 3 [16128/49669]\tLoss: 448.6792\n",
      "Training Epoch: 3 [16192/49669]\tLoss: 460.7054\n",
      "Training Epoch: 3 [16256/49669]\tLoss: 453.0522\n",
      "Training Epoch: 3 [16320/49669]\tLoss: 461.0645\n",
      "Training Epoch: 3 [16384/49669]\tLoss: 413.6089\n",
      "Training Epoch: 3 [16448/49669]\tLoss: 441.8204\n",
      "Training Epoch: 3 [16512/49669]\tLoss: 429.2166\n",
      "Training Epoch: 3 [16576/49669]\tLoss: 413.8504\n",
      "Training Epoch: 3 [16640/49669]\tLoss: 411.9523\n",
      "Training Epoch: 3 [16704/49669]\tLoss: 418.9816\n",
      "Training Epoch: 3 [16768/49669]\tLoss: 437.9728\n",
      "Training Epoch: 3 [16832/49669]\tLoss: 416.8401\n",
      "Training Epoch: 3 [16896/49669]\tLoss: 418.0463\n",
      "Training Epoch: 3 [16960/49669]\tLoss: 423.7017\n",
      "Training Epoch: 3 [17024/49669]\tLoss: 419.5944\n",
      "Training Epoch: 3 [17088/49669]\tLoss: 404.1248\n",
      "Training Epoch: 3 [17152/49669]\tLoss: 432.6248\n",
      "Training Epoch: 3 [17216/49669]\tLoss: 428.3338\n",
      "Training Epoch: 3 [17280/49669]\tLoss: 459.0455\n",
      "Training Epoch: 3 [17344/49669]\tLoss: 384.1833\n",
      "Training Epoch: 3 [17408/49669]\tLoss: 441.7157\n",
      "Training Epoch: 3 [17472/49669]\tLoss: 421.6227\n",
      "Training Epoch: 3 [17536/49669]\tLoss: 438.3878\n",
      "Training Epoch: 3 [17600/49669]\tLoss: 425.9315\n",
      "Training Epoch: 3 [17664/49669]\tLoss: 448.2011\n",
      "Training Epoch: 3 [17728/49669]\tLoss: 401.6427\n",
      "Training Epoch: 3 [17792/49669]\tLoss: 405.1752\n",
      "Training Epoch: 3 [17856/49669]\tLoss: 438.1202\n",
      "Training Epoch: 3 [17920/49669]\tLoss: 420.1074\n",
      "Training Epoch: 3 [17984/49669]\tLoss: 411.2523\n",
      "Training Epoch: 3 [18048/49669]\tLoss: 414.5665\n",
      "Training Epoch: 3 [18112/49669]\tLoss: 448.7697\n",
      "Training Epoch: 3 [18176/49669]\tLoss: 427.3705\n",
      "Training Epoch: 3 [18240/49669]\tLoss: 422.4661\n",
      "Training Epoch: 3 [18304/49669]\tLoss: 425.9566\n",
      "Training Epoch: 3 [18368/49669]\tLoss: 412.3485\n",
      "Training Epoch: 3 [18432/49669]\tLoss: 433.5840\n",
      "Training Epoch: 3 [18496/49669]\tLoss: 382.5697\n",
      "Training Epoch: 3 [18560/49669]\tLoss: 405.3306\n",
      "Training Epoch: 3 [18624/49669]\tLoss: 416.7655\n",
      "Training Epoch: 3 [18688/49669]\tLoss: 431.1098\n",
      "Training Epoch: 3 [18752/49669]\tLoss: 461.5225\n",
      "Training Epoch: 3 [18816/49669]\tLoss: 438.7310\n",
      "Training Epoch: 3 [18880/49669]\tLoss: 413.4387\n",
      "Training Epoch: 3 [18944/49669]\tLoss: 431.3337\n",
      "Training Epoch: 3 [19008/49669]\tLoss: 417.1940\n",
      "Training Epoch: 3 [19072/49669]\tLoss: 403.3827\n",
      "Training Epoch: 3 [19136/49669]\tLoss: 434.4150\n",
      "Training Epoch: 3 [19200/49669]\tLoss: 435.2605\n",
      "Training Epoch: 3 [19264/49669]\tLoss: 405.7275\n",
      "Training Epoch: 3 [19328/49669]\tLoss: 400.6492\n",
      "Training Epoch: 3 [19392/49669]\tLoss: 439.0561\n",
      "Training Epoch: 3 [19456/49669]\tLoss: 396.9882\n",
      "Training Epoch: 3 [19520/49669]\tLoss: 438.2289\n",
      "Training Epoch: 3 [19584/49669]\tLoss: 420.5594\n",
      "Training Epoch: 3 [19648/49669]\tLoss: 410.0986\n",
      "Training Epoch: 3 [19712/49669]\tLoss: 438.5057\n",
      "Training Epoch: 3 [19776/49669]\tLoss: 408.6437\n",
      "Training Epoch: 3 [19840/49669]\tLoss: 396.8341\n",
      "Training Epoch: 3 [19904/49669]\tLoss: 404.7742\n",
      "Training Epoch: 3 [19968/49669]\tLoss: 433.8390\n",
      "Training Epoch: 3 [20032/49669]\tLoss: 422.8008\n",
      "Training Epoch: 3 [20096/49669]\tLoss: 408.7463\n",
      "Training Epoch: 3 [20160/49669]\tLoss: 438.0016\n",
      "Training Epoch: 3 [20224/49669]\tLoss: 433.7866\n",
      "Training Epoch: 3 [20288/49669]\tLoss: 398.7354\n",
      "Training Epoch: 3 [20352/49669]\tLoss: 401.3879\n",
      "Training Epoch: 3 [20416/49669]\tLoss: 426.2457\n",
      "Training Epoch: 3 [20480/49669]\tLoss: 430.6889\n",
      "Training Epoch: 3 [20544/49669]\tLoss: 432.9598\n",
      "Training Epoch: 3 [20608/49669]\tLoss: 439.5942\n",
      "Training Epoch: 3 [20672/49669]\tLoss: 441.1045\n",
      "Training Epoch: 3 [20736/49669]\tLoss: 442.5383\n",
      "Training Epoch: 3 [20800/49669]\tLoss: 432.9886\n",
      "Training Epoch: 3 [20864/49669]\tLoss: 414.3742\n",
      "Training Epoch: 3 [20928/49669]\tLoss: 376.3061\n",
      "Training Epoch: 3 [20992/49669]\tLoss: 426.1428\n",
      "Training Epoch: 3 [21056/49669]\tLoss: 425.4513\n",
      "Training Epoch: 3 [21120/49669]\tLoss: 403.7245\n",
      "Training Epoch: 3 [21184/49669]\tLoss: 421.7942\n",
      "Training Epoch: 3 [21248/49669]\tLoss: 438.3301\n",
      "Training Epoch: 3 [21312/49669]\tLoss: 430.2646\n",
      "Training Epoch: 3 [21376/49669]\tLoss: 411.2213\n",
      "Training Epoch: 3 [21440/49669]\tLoss: 430.4985\n",
      "Training Epoch: 3 [21504/49669]\tLoss: 410.9031\n",
      "Training Epoch: 3 [21568/49669]\tLoss: 419.3752\n",
      "Training Epoch: 3 [21632/49669]\tLoss: 409.2438\n",
      "Training Epoch: 3 [21696/49669]\tLoss: 415.4169\n",
      "Training Epoch: 3 [21760/49669]\tLoss: 432.6704\n",
      "Training Epoch: 3 [21824/49669]\tLoss: 408.8876\n",
      "Training Epoch: 3 [21888/49669]\tLoss: 434.7357\n",
      "Training Epoch: 3 [21952/49669]\tLoss: 434.2752\n",
      "Training Epoch: 3 [22016/49669]\tLoss: 425.2076\n",
      "Training Epoch: 3 [22080/49669]\tLoss: 420.2779\n",
      "Training Epoch: 3 [22144/49669]\tLoss: 438.6513\n",
      "Training Epoch: 3 [22208/49669]\tLoss: 429.0432\n",
      "Training Epoch: 3 [22272/49669]\tLoss: 421.4706\n",
      "Training Epoch: 3 [22336/49669]\tLoss: 409.2998\n",
      "Training Epoch: 3 [22400/49669]\tLoss: 436.5489\n",
      "Training Epoch: 3 [22464/49669]\tLoss: 433.2055\n",
      "Training Epoch: 3 [22528/49669]\tLoss: 390.1811\n",
      "Training Epoch: 3 [22592/49669]\tLoss: 407.6874\n",
      "Training Epoch: 3 [22656/49669]\tLoss: 443.5664\n",
      "Training Epoch: 3 [22720/49669]\tLoss: 426.9131\n",
      "Training Epoch: 3 [22784/49669]\tLoss: 397.7742\n",
      "Training Epoch: 3 [22848/49669]\tLoss: 397.1517\n",
      "Training Epoch: 3 [22912/49669]\tLoss: 437.8897\n",
      "Training Epoch: 3 [22976/49669]\tLoss: 391.6532\n",
      "Training Epoch: 3 [23040/49669]\tLoss: 410.9864\n",
      "Training Epoch: 3 [23104/49669]\tLoss: 452.3417\n",
      "Training Epoch: 3 [23168/49669]\tLoss: 423.8860\n",
      "Training Epoch: 3 [23232/49669]\tLoss: 463.8475\n",
      "Training Epoch: 3 [23296/49669]\tLoss: 401.1836\n",
      "Training Epoch: 3 [23360/49669]\tLoss: 390.3438\n",
      "Training Epoch: 3 [23424/49669]\tLoss: 432.1060\n",
      "Training Epoch: 3 [23488/49669]\tLoss: 432.0991\n",
      "Training Epoch: 3 [23552/49669]\tLoss: 401.0824\n",
      "Training Epoch: 3 [23616/49669]\tLoss: 391.5766\n",
      "Training Epoch: 3 [23680/49669]\tLoss: 418.1059\n",
      "Training Epoch: 3 [23744/49669]\tLoss: 446.3699\n",
      "Training Epoch: 3 [23808/49669]\tLoss: 446.5175\n",
      "Training Epoch: 3 [23872/49669]\tLoss: 408.6664\n",
      "Training Epoch: 3 [23936/49669]\tLoss: 435.4969\n",
      "Training Epoch: 3 [24000/49669]\tLoss: 434.6172\n",
      "Training Epoch: 3 [24064/49669]\tLoss: 403.6004\n",
      "Training Epoch: 3 [24128/49669]\tLoss: 407.8574\n",
      "Training Epoch: 3 [24192/49669]\tLoss: 389.4136\n",
      "Training Epoch: 3 [24256/49669]\tLoss: 407.1221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [24320/49669]\tLoss: 398.3526\n",
      "Training Epoch: 3 [24384/49669]\tLoss: 424.2775\n",
      "Training Epoch: 3 [24448/49669]\tLoss: 427.8232\n",
      "Training Epoch: 3 [24512/49669]\tLoss: 395.7695\n",
      "Training Epoch: 3 [24576/49669]\tLoss: 419.2215\n",
      "Training Epoch: 3 [24640/49669]\tLoss: 424.3344\n",
      "Training Epoch: 3 [24704/49669]\tLoss: 407.9075\n",
      "Training Epoch: 3 [24768/49669]\tLoss: 415.8056\n",
      "Training Epoch: 3 [24832/49669]\tLoss: 415.1956\n",
      "Training Epoch: 3 [24896/49669]\tLoss: 393.5187\n",
      "Training Epoch: 3 [24960/49669]\tLoss: 439.4731\n",
      "Training Epoch: 3 [25024/49669]\tLoss: 462.8871\n",
      "Training Epoch: 3 [25088/49669]\tLoss: 424.2495\n",
      "Training Epoch: 3 [25152/49669]\tLoss: 412.0529\n",
      "Training Epoch: 3 [25216/49669]\tLoss: 427.9231\n",
      "Training Epoch: 3 [25280/49669]\tLoss: 416.6790\n",
      "Training Epoch: 3 [25344/49669]\tLoss: 396.0172\n",
      "Training Epoch: 3 [25408/49669]\tLoss: 409.0495\n",
      "Training Epoch: 3 [25472/49669]\tLoss: 407.6238\n",
      "Training Epoch: 3 [25536/49669]\tLoss: 431.2281\n",
      "Training Epoch: 3 [25600/49669]\tLoss: 428.5230\n",
      "Training Epoch: 3 [25664/49669]\tLoss: 439.1442\n",
      "Training Epoch: 3 [25728/49669]\tLoss: 427.6492\n",
      "Training Epoch: 3 [25792/49669]\tLoss: 428.1620\n",
      "Training Epoch: 3 [25856/49669]\tLoss: 439.5359\n",
      "Training Epoch: 3 [25920/49669]\tLoss: 412.5102\n",
      "Training Epoch: 3 [25984/49669]\tLoss: 389.4366\n",
      "Training Epoch: 3 [26048/49669]\tLoss: 393.3933\n",
      "Training Epoch: 3 [26112/49669]\tLoss: 426.3993\n",
      "Training Epoch: 3 [26176/49669]\tLoss: 431.3860\n",
      "Training Epoch: 3 [26240/49669]\tLoss: 427.5504\n",
      "Training Epoch: 3 [26304/49669]\tLoss: 428.7397\n",
      "Training Epoch: 3 [26368/49669]\tLoss: 428.3756\n",
      "Training Epoch: 3 [26432/49669]\tLoss: 437.1154\n",
      "Training Epoch: 3 [26496/49669]\tLoss: 391.1786\n",
      "Training Epoch: 3 [26560/49669]\tLoss: 433.2634\n",
      "Training Epoch: 3 [26624/49669]\tLoss: 404.9018\n",
      "Training Epoch: 3 [26688/49669]\tLoss: 435.7274\n",
      "Training Epoch: 3 [26752/49669]\tLoss: 411.0042\n",
      "Training Epoch: 3 [26816/49669]\tLoss: 416.6541\n",
      "Training Epoch: 3 [26880/49669]\tLoss: 419.4102\n",
      "Training Epoch: 3 [26944/49669]\tLoss: 415.4380\n",
      "Training Epoch: 3 [27008/49669]\tLoss: 430.9464\n",
      "Training Epoch: 3 [27072/49669]\tLoss: 424.8610\n",
      "Training Epoch: 3 [27136/49669]\tLoss: 432.0204\n",
      "Training Epoch: 3 [27200/49669]\tLoss: 412.8063\n",
      "Training Epoch: 3 [27264/49669]\tLoss: 424.3892\n",
      "Training Epoch: 3 [27328/49669]\tLoss: 429.8453\n",
      "Training Epoch: 3 [27392/49669]\tLoss: 415.8842\n",
      "Training Epoch: 3 [27456/49669]\tLoss: 408.5977\n",
      "Training Epoch: 3 [27520/49669]\tLoss: 465.7325\n",
      "Training Epoch: 3 [27584/49669]\tLoss: 426.9837\n",
      "Training Epoch: 3 [27648/49669]\tLoss: 433.4613\n",
      "Training Epoch: 3 [27712/49669]\tLoss: 425.3272\n",
      "Training Epoch: 3 [27776/49669]\tLoss: 430.7672\n",
      "Training Epoch: 3 [27840/49669]\tLoss: 429.7889\n",
      "Training Epoch: 3 [27904/49669]\tLoss: 401.1546\n",
      "Training Epoch: 3 [27968/49669]\tLoss: 418.7458\n",
      "Training Epoch: 3 [28032/49669]\tLoss: 440.2441\n",
      "Training Epoch: 3 [28096/49669]\tLoss: 412.3166\n",
      "Training Epoch: 3 [28160/49669]\tLoss: 419.2104\n",
      "Training Epoch: 3 [28224/49669]\tLoss: 450.3770\n",
      "Training Epoch: 3 [28288/49669]\tLoss: 410.4785\n",
      "Training Epoch: 3 [28352/49669]\tLoss: 420.9096\n",
      "Training Epoch: 3 [28416/49669]\tLoss: 394.9452\n",
      "Training Epoch: 3 [28480/49669]\tLoss: 439.6613\n",
      "Training Epoch: 3 [28544/49669]\tLoss: 451.9586\n",
      "Training Epoch: 3 [28608/49669]\tLoss: 443.9900\n",
      "Training Epoch: 3 [28672/49669]\tLoss: 433.6456\n",
      "Training Epoch: 3 [28736/49669]\tLoss: 387.4961\n",
      "Training Epoch: 3 [28800/49669]\tLoss: 414.1323\n",
      "Training Epoch: 3 [28864/49669]\tLoss: 433.7417\n",
      "Training Epoch: 3 [28928/49669]\tLoss: 414.3054\n",
      "Training Epoch: 3 [28992/49669]\tLoss: 449.2919\n",
      "Training Epoch: 3 [29056/49669]\tLoss: 426.9462\n",
      "Training Epoch: 3 [29120/49669]\tLoss: 434.2239\n",
      "Training Epoch: 3 [29184/49669]\tLoss: 437.8896\n",
      "Training Epoch: 3 [29248/49669]\tLoss: 408.8659\n",
      "Training Epoch: 3 [29312/49669]\tLoss: 425.5207\n",
      "Training Epoch: 3 [29376/49669]\tLoss: 418.0341\n",
      "Training Epoch: 3 [29440/49669]\tLoss: 413.7170\n",
      "Training Epoch: 3 [29504/49669]\tLoss: 405.0224\n",
      "Training Epoch: 3 [29568/49669]\tLoss: 442.1755\n",
      "Training Epoch: 3 [29632/49669]\tLoss: 431.0386\n",
      "Training Epoch: 3 [29696/49669]\tLoss: 413.5705\n",
      "Training Epoch: 3 [29760/49669]\tLoss: 429.3355\n",
      "Training Epoch: 3 [29824/49669]\tLoss: 453.8309\n",
      "Training Epoch: 3 [29888/49669]\tLoss: 423.8628\n",
      "Training Epoch: 3 [29952/49669]\tLoss: 419.9496\n",
      "Training Epoch: 3 [30016/49669]\tLoss: 402.5529\n",
      "Training Epoch: 3 [30080/49669]\tLoss: 429.1614\n",
      "Training Epoch: 3 [30144/49669]\tLoss: 428.9389\n",
      "Training Epoch: 3 [30208/49669]\tLoss: 429.1292\n",
      "Training Epoch: 3 [30272/49669]\tLoss: 464.1624\n",
      "Training Epoch: 3 [30336/49669]\tLoss: 448.8792\n",
      "Training Epoch: 3 [30400/49669]\tLoss: 416.7867\n",
      "Training Epoch: 3 [30464/49669]\tLoss: 452.4891\n",
      "Training Epoch: 3 [30528/49669]\tLoss: 453.7093\n",
      "Training Epoch: 3 [30592/49669]\tLoss: 466.6427\n",
      "Training Epoch: 3 [30656/49669]\tLoss: 452.8998\n",
      "Training Epoch: 3 [30720/49669]\tLoss: 439.6022\n",
      "Training Epoch: 3 [30784/49669]\tLoss: 428.5586\n",
      "Training Epoch: 3 [30848/49669]\tLoss: 403.7587\n",
      "Training Epoch: 3 [30912/49669]\tLoss: 450.0814\n",
      "Training Epoch: 3 [30976/49669]\tLoss: 408.3857\n",
      "Training Epoch: 3 [31040/49669]\tLoss: 407.4748\n",
      "Training Epoch: 3 [31104/49669]\tLoss: 434.0810\n",
      "Training Epoch: 3 [31168/49669]\tLoss: 411.5587\n",
      "Training Epoch: 3 [31232/49669]\tLoss: 399.8159\n",
      "Training Epoch: 3 [31296/49669]\tLoss: 439.8471\n",
      "Training Epoch: 3 [31360/49669]\tLoss: 429.2294\n",
      "Training Epoch: 3 [31424/49669]\tLoss: 425.2810\n",
      "Training Epoch: 3 [31488/49669]\tLoss: 440.3927\n",
      "Training Epoch: 3 [31552/49669]\tLoss: 462.5844\n",
      "Training Epoch: 3 [31616/49669]\tLoss: 471.9717\n",
      "Training Epoch: 3 [31680/49669]\tLoss: 473.8360\n",
      "Training Epoch: 3 [31744/49669]\tLoss: 455.2776\n",
      "Training Epoch: 3 [31808/49669]\tLoss: 441.8816\n",
      "Training Epoch: 3 [31872/49669]\tLoss: 411.3699\n",
      "Training Epoch: 3 [31936/49669]\tLoss: 412.5951\n",
      "Training Epoch: 3 [32000/49669]\tLoss: 441.3768\n",
      "Training Epoch: 3 [32064/49669]\tLoss: 428.9662\n",
      "Training Epoch: 3 [32128/49669]\tLoss: 453.1170\n",
      "Training Epoch: 3 [32192/49669]\tLoss: 435.1279\n",
      "Training Epoch: 3 [32256/49669]\tLoss: 400.4800\n",
      "Training Epoch: 3 [32320/49669]\tLoss: 427.6591\n",
      "Training Epoch: 3 [32384/49669]\tLoss: 446.6154\n",
      "Training Epoch: 3 [32448/49669]\tLoss: 456.7739\n",
      "Training Epoch: 3 [32512/49669]\tLoss: 452.5478\n",
      "Training Epoch: 3 [32576/49669]\tLoss: 462.4113\n",
      "Training Epoch: 3 [32640/49669]\tLoss: 435.7109\n",
      "Training Epoch: 3 [32704/49669]\tLoss: 446.3638\n",
      "Training Epoch: 3 [32768/49669]\tLoss: 398.4758\n",
      "Training Epoch: 3 [32832/49669]\tLoss: 422.2787\n",
      "Training Epoch: 3 [32896/49669]\tLoss: 429.6302\n",
      "Training Epoch: 3 [32960/49669]\tLoss: 408.3429\n",
      "Training Epoch: 3 [33024/49669]\tLoss: 428.7798\n",
      "Training Epoch: 3 [33088/49669]\tLoss: 446.5295\n",
      "Training Epoch: 3 [33152/49669]\tLoss: 408.0421\n",
      "Training Epoch: 3 [33216/49669]\tLoss: 453.3864\n",
      "Training Epoch: 3 [33280/49669]\tLoss: 417.7933\n",
      "Training Epoch: 3 [33344/49669]\tLoss: 422.2408\n",
      "Training Epoch: 3 [33408/49669]\tLoss: 422.8219\n",
      "Training Epoch: 3 [33472/49669]\tLoss: 448.1512\n",
      "Training Epoch: 3 [33536/49669]\tLoss: 398.1530\n",
      "Training Epoch: 3 [33600/49669]\tLoss: 423.2845\n",
      "Training Epoch: 3 [33664/49669]\tLoss: 426.6380\n",
      "Training Epoch: 3 [33728/49669]\tLoss: 395.2054\n",
      "Training Epoch: 3 [33792/49669]\tLoss: 410.9916\n",
      "Training Epoch: 3 [33856/49669]\tLoss: 427.2206\n",
      "Training Epoch: 3 [33920/49669]\tLoss: 424.5585\n",
      "Training Epoch: 3 [33984/49669]\tLoss: 428.0347\n",
      "Training Epoch: 3 [34048/49669]\tLoss: 444.9754\n",
      "Training Epoch: 3 [34112/49669]\tLoss: 406.6324\n",
      "Training Epoch: 3 [34176/49669]\tLoss: 442.3666\n",
      "Training Epoch: 3 [34240/49669]\tLoss: 425.3000\n",
      "Training Epoch: 3 [34304/49669]\tLoss: 434.4580\n",
      "Training Epoch: 3 [34368/49669]\tLoss: 418.5555\n",
      "Training Epoch: 3 [34432/49669]\tLoss: 437.5636\n",
      "Training Epoch: 3 [34496/49669]\tLoss: 397.0736\n",
      "Training Epoch: 3 [34560/49669]\tLoss: 418.2391\n",
      "Training Epoch: 3 [34624/49669]\tLoss: 419.5417\n",
      "Training Epoch: 3 [34688/49669]\tLoss: 435.8360\n",
      "Training Epoch: 3 [34752/49669]\tLoss: 388.5761\n",
      "Training Epoch: 3 [34816/49669]\tLoss: 425.3050\n",
      "Training Epoch: 3 [34880/49669]\tLoss: 413.3347\n",
      "Training Epoch: 3 [34944/49669]\tLoss: 425.3354\n",
      "Training Epoch: 3 [35008/49669]\tLoss: 440.1223\n",
      "Training Epoch: 3 [35072/49669]\tLoss: 408.6287\n",
      "Training Epoch: 3 [35136/49669]\tLoss: 430.8511\n",
      "Training Epoch: 3 [35200/49669]\tLoss: 414.1513\n",
      "Training Epoch: 3 [35264/49669]\tLoss: 408.3049\n",
      "Training Epoch: 3 [35328/49669]\tLoss: 412.8239\n",
      "Training Epoch: 3 [35392/49669]\tLoss: 434.1559\n",
      "Training Epoch: 3 [35456/49669]\tLoss: 390.2486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [35520/49669]\tLoss: 411.9873\n",
      "Training Epoch: 3 [35584/49669]\tLoss: 429.8738\n",
      "Training Epoch: 3 [35648/49669]\tLoss: 435.3770\n",
      "Training Epoch: 3 [35712/49669]\tLoss: 426.6332\n",
      "Training Epoch: 3 [35776/49669]\tLoss: 414.3267\n",
      "Training Epoch: 3 [35840/49669]\tLoss: 407.9511\n",
      "Training Epoch: 3 [35904/49669]\tLoss: 449.2812\n",
      "Training Epoch: 3 [35968/49669]\tLoss: 409.7649\n",
      "Training Epoch: 3 [36032/49669]\tLoss: 414.5303\n",
      "Training Epoch: 3 [36096/49669]\tLoss: 425.0096\n",
      "Training Epoch: 3 [36160/49669]\tLoss: 458.0053\n",
      "Training Epoch: 3 [36224/49669]\tLoss: 411.9411\n",
      "Training Epoch: 3 [36288/49669]\tLoss: 429.7010\n",
      "Training Epoch: 3 [36352/49669]\tLoss: 423.1877\n",
      "Training Epoch: 3 [36416/49669]\tLoss: 424.1000\n",
      "Training Epoch: 3 [36480/49669]\tLoss: 428.8139\n",
      "Training Epoch: 3 [36544/49669]\tLoss: 438.9772\n",
      "Training Epoch: 3 [36608/49669]\tLoss: 421.8380\n",
      "Training Epoch: 3 [36672/49669]\tLoss: 421.0289\n",
      "Training Epoch: 3 [36736/49669]\tLoss: 428.3245\n",
      "Training Epoch: 3 [36800/49669]\tLoss: 415.3087\n",
      "Training Epoch: 3 [36864/49669]\tLoss: 409.2157\n",
      "Training Epoch: 3 [36928/49669]\tLoss: 416.7471\n",
      "Training Epoch: 3 [36992/49669]\tLoss: 403.1214\n",
      "Training Epoch: 3 [37056/49669]\tLoss: 465.7090\n",
      "Training Epoch: 3 [37120/49669]\tLoss: 445.1621\n",
      "Training Epoch: 3 [37184/49669]\tLoss: 414.2676\n",
      "Training Epoch: 3 [37248/49669]\tLoss: 428.3017\n",
      "Training Epoch: 3 [37312/49669]\tLoss: 460.4104\n",
      "Training Epoch: 3 [37376/49669]\tLoss: 424.3868\n",
      "Training Epoch: 3 [37440/49669]\tLoss: 443.4334\n",
      "Training Epoch: 3 [37504/49669]\tLoss: 410.0919\n",
      "Training Epoch: 3 [37568/49669]\tLoss: 430.0323\n",
      "Training Epoch: 3 [37632/49669]\tLoss: 458.9888\n",
      "Training Epoch: 3 [37696/49669]\tLoss: 456.7986\n",
      "Training Epoch: 3 [37760/49669]\tLoss: 448.2204\n",
      "Training Epoch: 3 [37824/49669]\tLoss: 427.3672\n",
      "Training Epoch: 3 [37888/49669]\tLoss: 405.7867\n",
      "Training Epoch: 3 [37952/49669]\tLoss: 410.0158\n",
      "Training Epoch: 3 [38016/49669]\tLoss: 437.1560\n",
      "Training Epoch: 3 [38080/49669]\tLoss: 424.5515\n",
      "Training Epoch: 3 [38144/49669]\tLoss: 424.5324\n",
      "Training Epoch: 3 [38208/49669]\tLoss: 408.7424\n",
      "Training Epoch: 3 [38272/49669]\tLoss: 407.1911\n",
      "Training Epoch: 3 [38336/49669]\tLoss: 421.8737\n",
      "Training Epoch: 3 [38400/49669]\tLoss: 442.7673\n",
      "Training Epoch: 3 [38464/49669]\tLoss: 414.1383\n",
      "Training Epoch: 3 [38528/49669]\tLoss: 426.2026\n",
      "Training Epoch: 3 [38592/49669]\tLoss: 419.9238\n",
      "Training Epoch: 3 [38656/49669]\tLoss: 419.2974\n",
      "Training Epoch: 3 [38720/49669]\tLoss: 439.6001\n",
      "Training Epoch: 3 [38784/49669]\tLoss: 346.6611\n",
      "Training Epoch: 3 [38848/49669]\tLoss: 401.4041\n",
      "Training Epoch: 3 [38912/49669]\tLoss: 441.2473\n",
      "Training Epoch: 3 [38976/49669]\tLoss: 391.1525\n",
      "Training Epoch: 3 [39040/49669]\tLoss: 445.3471\n",
      "Training Epoch: 3 [39104/49669]\tLoss: 419.9974\n",
      "Training Epoch: 3 [39168/49669]\tLoss: 428.4734\n",
      "Training Epoch: 3 [39232/49669]\tLoss: 408.3964\n",
      "Training Epoch: 3 [39296/49669]\tLoss: 377.0326\n",
      "Training Epoch: 3 [39360/49669]\tLoss: 401.6915\n",
      "Training Epoch: 3 [39424/49669]\tLoss: 390.6524\n",
      "Training Epoch: 3 [39488/49669]\tLoss: 428.5261\n",
      "Training Epoch: 3 [39552/49669]\tLoss: 444.1615\n",
      "Training Epoch: 3 [39616/49669]\tLoss: 389.6647\n",
      "Training Epoch: 3 [39680/49669]\tLoss: 409.3107\n",
      "Training Epoch: 3 [39744/49669]\tLoss: 423.6162\n",
      "Training Epoch: 3 [39808/49669]\tLoss: 447.3412\n",
      "Training Epoch: 3 [39872/49669]\tLoss: 386.5223\n",
      "Training Epoch: 3 [39936/49669]\tLoss: 430.8382\n",
      "Training Epoch: 3 [40000/49669]\tLoss: 423.5593\n",
      "Training Epoch: 3 [40064/49669]\tLoss: 474.9803\n",
      "Training Epoch: 3 [40128/49669]\tLoss: 435.1022\n",
      "Training Epoch: 3 [40192/49669]\tLoss: 451.9804\n",
      "Training Epoch: 3 [40256/49669]\tLoss: 433.0541\n",
      "Training Epoch: 3 [40320/49669]\tLoss: 451.0702\n",
      "Training Epoch: 3 [40384/49669]\tLoss: 459.9142\n",
      "Training Epoch: 3 [40448/49669]\tLoss: 462.7102\n",
      "Training Epoch: 3 [40512/49669]\tLoss: 458.4847\n",
      "Training Epoch: 3 [40576/49669]\tLoss: 450.9893\n",
      "Training Epoch: 3 [40640/49669]\tLoss: 464.0876\n",
      "Training Epoch: 3 [40704/49669]\tLoss: 411.2177\n",
      "Training Epoch: 3 [40768/49669]\tLoss: 430.8816\n",
      "Training Epoch: 3 [40832/49669]\tLoss: 447.6807\n",
      "Training Epoch: 3 [40896/49669]\tLoss: 433.0342\n",
      "Training Epoch: 3 [40960/49669]\tLoss: 440.3712\n",
      "Training Epoch: 3 [41024/49669]\tLoss: 452.7450\n",
      "Training Epoch: 3 [41088/49669]\tLoss: 419.1361\n",
      "Training Epoch: 3 [41152/49669]\tLoss: 469.5972\n",
      "Training Epoch: 3 [41216/49669]\tLoss: 434.9583\n",
      "Training Epoch: 3 [41280/49669]\tLoss: 425.0472\n",
      "Training Epoch: 3 [41344/49669]\tLoss: 430.2310\n",
      "Training Epoch: 3 [41408/49669]\tLoss: 439.8849\n",
      "Training Epoch: 3 [41472/49669]\tLoss: 402.4607\n",
      "Training Epoch: 3 [41536/49669]\tLoss: 438.0130\n",
      "Training Epoch: 3 [41600/49669]\tLoss: 445.9550\n",
      "Training Epoch: 3 [41664/49669]\tLoss: 424.5356\n",
      "Training Epoch: 3 [41728/49669]\tLoss: 441.3864\n",
      "Training Epoch: 3 [41792/49669]\tLoss: 432.3389\n",
      "Training Epoch: 3 [41856/49669]\tLoss: 418.7899\n",
      "Training Epoch: 3 [41920/49669]\tLoss: 405.6530\n",
      "Training Epoch: 3 [41984/49669]\tLoss: 460.3859\n",
      "Training Epoch: 3 [42048/49669]\tLoss: 431.9637\n",
      "Training Epoch: 3 [42112/49669]\tLoss: 443.8980\n",
      "Training Epoch: 3 [42176/49669]\tLoss: 419.0143\n",
      "Training Epoch: 3 [42240/49669]\tLoss: 421.7221\n",
      "Training Epoch: 3 [42304/49669]\tLoss: 458.5583\n",
      "Training Epoch: 3 [42368/49669]\tLoss: 416.2304\n",
      "Training Epoch: 3 [42432/49669]\tLoss: 448.4653\n",
      "Training Epoch: 3 [42496/49669]\tLoss: 420.0465\n",
      "Training Epoch: 3 [42560/49669]\tLoss: 472.5799\n",
      "Training Epoch: 3 [42624/49669]\tLoss: 429.5639\n",
      "Training Epoch: 3 [42688/49669]\tLoss: 422.0989\n",
      "Training Epoch: 3 [42752/49669]\tLoss: 403.0571\n",
      "Training Epoch: 3 [42816/49669]\tLoss: 427.5880\n",
      "Training Epoch: 3 [42880/49669]\tLoss: 411.3128\n",
      "Training Epoch: 3 [42944/49669]\tLoss: 415.8822\n",
      "Training Epoch: 3 [43008/49669]\tLoss: 421.7122\n",
      "Training Epoch: 3 [43072/49669]\tLoss: 437.5983\n",
      "Training Epoch: 3 [43136/49669]\tLoss: 408.6047\n",
      "Training Epoch: 3 [43200/49669]\tLoss: 427.3098\n",
      "Training Epoch: 3 [43264/49669]\tLoss: 446.6742\n",
      "Training Epoch: 3 [43328/49669]\tLoss: 394.4783\n",
      "Training Epoch: 3 [43392/49669]\tLoss: 410.3167\n",
      "Training Epoch: 3 [43456/49669]\tLoss: 450.4623\n",
      "Training Epoch: 3 [43520/49669]\tLoss: 421.2444\n",
      "Training Epoch: 3 [43584/49669]\tLoss: 429.2025\n",
      "Training Epoch: 3 [43648/49669]\tLoss: 414.7982\n",
      "Training Epoch: 3 [43712/49669]\tLoss: 438.9341\n",
      "Training Epoch: 3 [43776/49669]\tLoss: 433.2517\n",
      "Training Epoch: 3 [43840/49669]\tLoss: 424.7345\n",
      "Training Epoch: 3 [43904/49669]\tLoss: 440.5435\n",
      "Training Epoch: 3 [43968/49669]\tLoss: 441.5604\n",
      "Training Epoch: 3 [44032/49669]\tLoss: 432.1649\n",
      "Training Epoch: 3 [44096/49669]\tLoss: 443.8456\n",
      "Training Epoch: 3 [44160/49669]\tLoss: 426.0023\n",
      "Training Epoch: 3 [44224/49669]\tLoss: 419.3428\n",
      "Training Epoch: 3 [44288/49669]\tLoss: 422.8698\n",
      "Training Epoch: 3 [44352/49669]\tLoss: 433.1476\n",
      "Training Epoch: 3 [44416/49669]\tLoss: 433.7380\n",
      "Training Epoch: 3 [44480/49669]\tLoss: 447.1739\n",
      "Training Epoch: 3 [44544/49669]\tLoss: 442.9410\n",
      "Training Epoch: 3 [44608/49669]\tLoss: 449.2791\n",
      "Training Epoch: 3 [44672/49669]\tLoss: 446.0382\n",
      "Training Epoch: 3 [44736/49669]\tLoss: 437.5972\n",
      "Training Epoch: 3 [44800/49669]\tLoss: 454.4020\n",
      "Training Epoch: 3 [44864/49669]\tLoss: 441.3859\n",
      "Training Epoch: 3 [44928/49669]\tLoss: 400.8481\n",
      "Training Epoch: 3 [44992/49669]\tLoss: 468.1825\n",
      "Training Epoch: 3 [45056/49669]\tLoss: 426.0734\n",
      "Training Epoch: 3 [45120/49669]\tLoss: 388.8823\n",
      "Training Epoch: 3 [45184/49669]\tLoss: 405.4999\n",
      "Training Epoch: 3 [45248/49669]\tLoss: 448.4551\n",
      "Training Epoch: 3 [45312/49669]\tLoss: 417.9225\n",
      "Training Epoch: 3 [45376/49669]\tLoss: 424.5159\n",
      "Training Epoch: 3 [45440/49669]\tLoss: 407.8920\n",
      "Training Epoch: 3 [45504/49669]\tLoss: 417.1924\n",
      "Training Epoch: 3 [45568/49669]\tLoss: 425.2918\n",
      "Training Epoch: 3 [45632/49669]\tLoss: 438.6090\n",
      "Training Epoch: 3 [45696/49669]\tLoss: 423.3064\n",
      "Training Epoch: 3 [45760/49669]\tLoss: 434.9026\n",
      "Training Epoch: 3 [45824/49669]\tLoss: 418.2104\n",
      "Training Epoch: 3 [45888/49669]\tLoss: 430.1715\n",
      "Training Epoch: 3 [45952/49669]\tLoss: 442.9256\n",
      "Training Epoch: 3 [46016/49669]\tLoss: 426.4431\n",
      "Training Epoch: 3 [46080/49669]\tLoss: 407.3105\n",
      "Training Epoch: 3 [46144/49669]\tLoss: 442.2883\n",
      "Training Epoch: 3 [46208/49669]\tLoss: 450.6285\n",
      "Training Epoch: 3 [46272/49669]\tLoss: 374.5133\n",
      "Training Epoch: 3 [46336/49669]\tLoss: 438.5882\n",
      "Training Epoch: 3 [46400/49669]\tLoss: 419.5934\n",
      "Training Epoch: 3 [46464/49669]\tLoss: 383.4267\n",
      "Training Epoch: 3 [46528/49669]\tLoss: 423.0446\n",
      "Training Epoch: 3 [46592/49669]\tLoss: 435.5180\n",
      "Training Epoch: 3 [46656/49669]\tLoss: 445.3470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [46720/49669]\tLoss: 444.0328\n",
      "Training Epoch: 3 [46784/49669]\tLoss: 428.6400\n",
      "Training Epoch: 3 [46848/49669]\tLoss: 446.7880\n",
      "Training Epoch: 3 [46912/49669]\tLoss: 425.1751\n",
      "Training Epoch: 3 [46976/49669]\tLoss: 410.0552\n",
      "Training Epoch: 3 [47040/49669]\tLoss: 400.0959\n",
      "Training Epoch: 3 [47104/49669]\tLoss: 418.9932\n",
      "Training Epoch: 3 [47168/49669]\tLoss: 444.0182\n",
      "Training Epoch: 3 [47232/49669]\tLoss: 417.2075\n",
      "Training Epoch: 3 [47296/49669]\tLoss: 440.2778\n",
      "Training Epoch: 3 [47360/49669]\tLoss: 443.3351\n",
      "Training Epoch: 3 [47424/49669]\tLoss: 432.3072\n",
      "Training Epoch: 3 [47488/49669]\tLoss: 440.7128\n",
      "Training Epoch: 3 [47552/49669]\tLoss: 432.2021\n",
      "Training Epoch: 3 [47616/49669]\tLoss: 451.9369\n",
      "Training Epoch: 3 [47680/49669]\tLoss: 421.2770\n",
      "Training Epoch: 3 [47744/49669]\tLoss: 430.1863\n",
      "Training Epoch: 3 [47808/49669]\tLoss: 433.7281\n",
      "Training Epoch: 3 [47872/49669]\tLoss: 425.0100\n",
      "Training Epoch: 3 [47936/49669]\tLoss: 435.6981\n",
      "Training Epoch: 3 [48000/49669]\tLoss: 399.4909\n",
      "Training Epoch: 3 [48064/49669]\tLoss: 406.3437\n",
      "Training Epoch: 3 [48128/49669]\tLoss: 451.1382\n",
      "Training Epoch: 3 [48192/49669]\tLoss: 427.7385\n",
      "Training Epoch: 3 [48256/49669]\tLoss: 412.7274\n",
      "Training Epoch: 3 [48320/49669]\tLoss: 413.7751\n",
      "Training Epoch: 3 [48384/49669]\tLoss: 425.5042\n",
      "Training Epoch: 3 [48448/49669]\tLoss: 434.1668\n",
      "Training Epoch: 3 [48512/49669]\tLoss: 417.9013\n",
      "Training Epoch: 3 [48576/49669]\tLoss: 424.3280\n",
      "Training Epoch: 3 [48640/49669]\tLoss: 450.5897\n",
      "Training Epoch: 3 [48704/49669]\tLoss: 409.4009\n",
      "Training Epoch: 3 [48768/49669]\tLoss: 434.3585\n",
      "Training Epoch: 3 [48832/49669]\tLoss: 424.3893\n",
      "Training Epoch: 3 [48896/49669]\tLoss: 453.9282\n",
      "Training Epoch: 3 [48960/49669]\tLoss: 409.5816\n",
      "Training Epoch: 3 [49024/49669]\tLoss: 429.7617\n",
      "Training Epoch: 3 [49088/49669]\tLoss: 443.6060\n",
      "Training Epoch: 3 [49152/49669]\tLoss: 428.9207\n",
      "Training Epoch: 3 [49216/49669]\tLoss: 400.2012\n",
      "Training Epoch: 3 [49280/49669]\tLoss: 448.6472\n",
      "Training Epoch: 3 [49344/49669]\tLoss: 390.5742\n",
      "Training Epoch: 3 [49408/49669]\tLoss: 417.8889\n",
      "Training Epoch: 3 [49472/49669]\tLoss: 418.9029\n",
      "Training Epoch: 3 [49536/49669]\tLoss: 424.0495\n",
      "Training Epoch: 3 [49600/49669]\tLoss: 429.0374\n",
      "Training Epoch: 3 [49664/49669]\tLoss: 431.9938\n",
      "Training Epoch: 3 [49669/49669]\tLoss: 429.6865\n",
      "Training Epoch: 3 [5519/5519]\tLoss: 424.5982\n",
      "Training Epoch: 4 [64/49669]\tLoss: 389.0321\n",
      "Training Epoch: 4 [128/49669]\tLoss: 431.0002\n",
      "Training Epoch: 4 [192/49669]\tLoss: 406.6432\n",
      "Training Epoch: 4 [256/49669]\tLoss: 421.9286\n",
      "Training Epoch: 4 [320/49669]\tLoss: 423.3754\n",
      "Training Epoch: 4 [384/49669]\tLoss: 430.7213\n",
      "Training Epoch: 4 [448/49669]\tLoss: 415.1946\n",
      "Training Epoch: 4 [512/49669]\tLoss: 456.3682\n",
      "Training Epoch: 4 [576/49669]\tLoss: 433.1742\n",
      "Training Epoch: 4 [640/49669]\tLoss: 421.2390\n",
      "Training Epoch: 4 [704/49669]\tLoss: 437.7976\n",
      "Training Epoch: 4 [768/49669]\tLoss: 420.1649\n",
      "Training Epoch: 4 [832/49669]\tLoss: 460.5896\n",
      "Training Epoch: 4 [896/49669]\tLoss: 466.0608\n",
      "Training Epoch: 4 [960/49669]\tLoss: 418.7191\n",
      "Training Epoch: 4 [1024/49669]\tLoss: 431.5421\n",
      "Training Epoch: 4 [1088/49669]\tLoss: 420.5992\n",
      "Training Epoch: 4 [1152/49669]\tLoss: 443.1388\n",
      "Training Epoch: 4 [1216/49669]\tLoss: 436.1178\n",
      "Training Epoch: 4 [1280/49669]\tLoss: 443.8793\n",
      "Training Epoch: 4 [1344/49669]\tLoss: 443.9742\n",
      "Training Epoch: 4 [1408/49669]\tLoss: 420.0432\n",
      "Training Epoch: 4 [1472/49669]\tLoss: 410.1540\n",
      "Training Epoch: 4 [1536/49669]\tLoss: 428.6999\n",
      "Training Epoch: 4 [1600/49669]\tLoss: 431.4699\n",
      "Training Epoch: 4 [1664/49669]\tLoss: 440.7957\n",
      "Training Epoch: 4 [1728/49669]\tLoss: 416.7708\n",
      "Training Epoch: 4 [1792/49669]\tLoss: 436.4947\n",
      "Training Epoch: 4 [1856/49669]\tLoss: 439.3517\n",
      "Training Epoch: 4 [1920/49669]\tLoss: 431.1375\n",
      "Training Epoch: 4 [1984/49669]\tLoss: 461.3300\n",
      "Training Epoch: 4 [2048/49669]\tLoss: 442.6434\n",
      "Training Epoch: 4 [2112/49669]\tLoss: 459.9033\n",
      "Training Epoch: 4 [2176/49669]\tLoss: 440.9239\n",
      "Training Epoch: 4 [2240/49669]\tLoss: 440.8786\n",
      "Training Epoch: 4 [2304/49669]\tLoss: 435.4646\n",
      "Training Epoch: 4 [2368/49669]\tLoss: 363.0250\n",
      "Training Epoch: 4 [2432/49669]\tLoss: 391.8325\n",
      "Training Epoch: 4 [2496/49669]\tLoss: 420.7451\n",
      "Training Epoch: 4 [2560/49669]\tLoss: 426.8823\n",
      "Training Epoch: 4 [2624/49669]\tLoss: 465.6854\n",
      "Training Epoch: 4 [2688/49669]\tLoss: 480.5872\n",
      "Training Epoch: 4 [2752/49669]\tLoss: 475.5045\n",
      "Training Epoch: 4 [2816/49669]\tLoss: 452.8178\n",
      "Training Epoch: 4 [2880/49669]\tLoss: 395.5795\n",
      "Training Epoch: 4 [2944/49669]\tLoss: 400.8061\n",
      "Training Epoch: 4 [3008/49669]\tLoss: 420.8526\n",
      "Training Epoch: 4 [3072/49669]\tLoss: 435.3882\n",
      "Training Epoch: 4 [3136/49669]\tLoss: 468.1640\n",
      "Training Epoch: 4 [3200/49669]\tLoss: 446.9146\n",
      "Training Epoch: 4 [3264/49669]\tLoss: 458.9380\n",
      "Training Epoch: 4 [3328/49669]\tLoss: 397.9771\n",
      "Training Epoch: 4 [3392/49669]\tLoss: 399.7783\n",
      "Training Epoch: 4 [3456/49669]\tLoss: 443.5414\n",
      "Training Epoch: 4 [3520/49669]\tLoss: 468.9247\n",
      "Training Epoch: 4 [3584/49669]\tLoss: 449.1899\n",
      "Training Epoch: 4 [3648/49669]\tLoss: 480.7530\n",
      "Training Epoch: 4 [3712/49669]\tLoss: 426.0713\n",
      "Training Epoch: 4 [3776/49669]\tLoss: 433.7365\n",
      "Training Epoch: 4 [3840/49669]\tLoss: 409.5320\n",
      "Training Epoch: 4 [3904/49669]\tLoss: 393.5338\n",
      "Training Epoch: 4 [3968/49669]\tLoss: 462.8375\n",
      "Training Epoch: 4 [4032/49669]\tLoss: 428.3105\n",
      "Training Epoch: 4 [4096/49669]\tLoss: 409.2838\n",
      "Training Epoch: 4 [4160/49669]\tLoss: 414.7051\n",
      "Training Epoch: 4 [4224/49669]\tLoss: 426.1716\n",
      "Training Epoch: 4 [4288/49669]\tLoss: 437.9373\n",
      "Training Epoch: 4 [4352/49669]\tLoss: 413.7201\n",
      "Training Epoch: 4 [4416/49669]\tLoss: 442.8222\n",
      "Training Epoch: 4 [4480/49669]\tLoss: 419.4280\n",
      "Training Epoch: 4 [4544/49669]\tLoss: 395.2301\n",
      "Training Epoch: 4 [4608/49669]\tLoss: 449.7897\n",
      "Training Epoch: 4 [4672/49669]\tLoss: 397.5298\n",
      "Training Epoch: 4 [4736/49669]\tLoss: 420.6380\n",
      "Training Epoch: 4 [4800/49669]\tLoss: 397.2641\n",
      "Training Epoch: 4 [4864/49669]\tLoss: 424.4274\n",
      "Training Epoch: 4 [4928/49669]\tLoss: 425.4978\n",
      "Training Epoch: 4 [4992/49669]\tLoss: 407.5389\n",
      "Training Epoch: 4 [5056/49669]\tLoss: 421.5602\n",
      "Training Epoch: 4 [5120/49669]\tLoss: 426.1283\n",
      "Training Epoch: 4 [5184/49669]\tLoss: 417.4127\n",
      "Training Epoch: 4 [5248/49669]\tLoss: 437.3555\n",
      "Training Epoch: 4 [5312/49669]\tLoss: 426.8295\n",
      "Training Epoch: 4 [5376/49669]\tLoss: 419.2068\n",
      "Training Epoch: 4 [5440/49669]\tLoss: 437.8199\n",
      "Training Epoch: 4 [5504/49669]\tLoss: 416.7521\n",
      "Training Epoch: 4 [5568/49669]\tLoss: 411.9442\n",
      "Training Epoch: 4 [5632/49669]\tLoss: 450.3759\n",
      "Training Epoch: 4 [5696/49669]\tLoss: 453.5830\n",
      "Training Epoch: 4 [5760/49669]\tLoss: 440.3719\n",
      "Training Epoch: 4 [5824/49669]\tLoss: 427.0800\n",
      "Training Epoch: 4 [5888/49669]\tLoss: 411.6123\n",
      "Training Epoch: 4 [5952/49669]\tLoss: 405.8278\n",
      "Training Epoch: 4 [6016/49669]\tLoss: 404.6057\n",
      "Training Epoch: 4 [6080/49669]\tLoss: 444.9024\n",
      "Training Epoch: 4 [6144/49669]\tLoss: 392.8324\n",
      "Training Epoch: 4 [6208/49669]\tLoss: 410.8948\n",
      "Training Epoch: 4 [6272/49669]\tLoss: 413.1082\n",
      "Training Epoch: 4 [6336/49669]\tLoss: 432.8722\n",
      "Training Epoch: 4 [6400/49669]\tLoss: 425.0954\n",
      "Training Epoch: 4 [6464/49669]\tLoss: 414.0446\n",
      "Training Epoch: 4 [6528/49669]\tLoss: 416.5506\n",
      "Training Epoch: 4 [6592/49669]\tLoss: 460.5319\n",
      "Training Epoch: 4 [6656/49669]\tLoss: 425.5506\n",
      "Training Epoch: 4 [6720/49669]\tLoss: 429.4100\n",
      "Training Epoch: 4 [6784/49669]\tLoss: 431.1440\n",
      "Training Epoch: 4 [6848/49669]\tLoss: 408.8329\n",
      "Training Epoch: 4 [6912/49669]\tLoss: 418.6494\n",
      "Training Epoch: 4 [6976/49669]\tLoss: 399.3794\n",
      "Training Epoch: 4 [7040/49669]\tLoss: 406.9639\n",
      "Training Epoch: 4 [7104/49669]\tLoss: 422.3385\n",
      "Training Epoch: 4 [7168/49669]\tLoss: 412.3693\n",
      "Training Epoch: 4 [7232/49669]\tLoss: 393.2072\n",
      "Training Epoch: 4 [7296/49669]\tLoss: 419.3329\n",
      "Training Epoch: 4 [7360/49669]\tLoss: 408.0657\n",
      "Training Epoch: 4 [7424/49669]\tLoss: 435.2470\n",
      "Training Epoch: 4 [7488/49669]\tLoss: 430.2137\n",
      "Training Epoch: 4 [7552/49669]\tLoss: 444.2680\n",
      "Training Epoch: 4 [7616/49669]\tLoss: 397.8215\n",
      "Training Epoch: 4 [7680/49669]\tLoss: 423.5081\n",
      "Training Epoch: 4 [7744/49669]\tLoss: 417.3930\n",
      "Training Epoch: 4 [7808/49669]\tLoss: 399.9655\n",
      "Training Epoch: 4 [7872/49669]\tLoss: 452.9081\n",
      "Training Epoch: 4 [7936/49669]\tLoss: 426.3153\n",
      "Training Epoch: 4 [8000/49669]\tLoss: 398.1557\n",
      "Training Epoch: 4 [8064/49669]\tLoss: 415.4194\n",
      "Training Epoch: 4 [8128/49669]\tLoss: 445.6585\n",
      "Training Epoch: 4 [8192/49669]\tLoss: 405.0315\n",
      "Training Epoch: 4 [8256/49669]\tLoss: 438.5464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [8320/49669]\tLoss: 414.9159\n",
      "Training Epoch: 4 [8384/49669]\tLoss: 429.5704\n",
      "Training Epoch: 4 [8448/49669]\tLoss: 404.4991\n",
      "Training Epoch: 4 [8512/49669]\tLoss: 415.9413\n",
      "Training Epoch: 4 [8576/49669]\tLoss: 433.0065\n",
      "Training Epoch: 4 [8640/49669]\tLoss: 431.0983\n",
      "Training Epoch: 4 [8704/49669]\tLoss: 430.0692\n",
      "Training Epoch: 4 [8768/49669]\tLoss: 422.5381\n",
      "Training Epoch: 4 [8832/49669]\tLoss: 417.3244\n",
      "Training Epoch: 4 [8896/49669]\tLoss: 420.7677\n",
      "Training Epoch: 4 [8960/49669]\tLoss: 415.2049\n",
      "Training Epoch: 4 [9024/49669]\tLoss: 448.6713\n",
      "Training Epoch: 4 [9088/49669]\tLoss: 425.6052\n",
      "Training Epoch: 4 [9152/49669]\tLoss: 406.4594\n",
      "Training Epoch: 4 [9216/49669]\tLoss: 423.7594\n",
      "Training Epoch: 4 [9280/49669]\tLoss: 405.2107\n",
      "Training Epoch: 4 [9344/49669]\tLoss: 426.6456\n",
      "Training Epoch: 4 [9408/49669]\tLoss: 401.7508\n",
      "Training Epoch: 4 [9472/49669]\tLoss: 410.6117\n",
      "Training Epoch: 4 [9536/49669]\tLoss: 437.8779\n",
      "Training Epoch: 4 [9600/49669]\tLoss: 435.3380\n",
      "Training Epoch: 4 [9664/49669]\tLoss: 435.1353\n",
      "Training Epoch: 4 [9728/49669]\tLoss: 427.2617\n",
      "Training Epoch: 4 [9792/49669]\tLoss: 412.1239\n",
      "Training Epoch: 4 [9856/49669]\tLoss: 383.0622\n",
      "Training Epoch: 4 [9920/49669]\tLoss: 418.2369\n",
      "Training Epoch: 4 [9984/49669]\tLoss: 435.2254\n",
      "Training Epoch: 4 [10048/49669]\tLoss: 401.6911\n",
      "Training Epoch: 4 [10112/49669]\tLoss: 422.1430\n",
      "Training Epoch: 4 [10176/49669]\tLoss: 415.5456\n",
      "Training Epoch: 4 [10240/49669]\tLoss: 399.4044\n",
      "Training Epoch: 4 [10304/49669]\tLoss: 421.9378\n",
      "Training Epoch: 4 [10368/49669]\tLoss: 417.0458\n",
      "Training Epoch: 4 [10432/49669]\tLoss: 425.4477\n",
      "Training Epoch: 4 [10496/49669]\tLoss: 429.0979\n",
      "Training Epoch: 4 [10560/49669]\tLoss: 432.7770\n",
      "Training Epoch: 4 [10624/49669]\tLoss: 419.9843\n",
      "Training Epoch: 4 [10688/49669]\tLoss: 427.6579\n",
      "Training Epoch: 4 [10752/49669]\tLoss: 445.0050\n",
      "Training Epoch: 4 [10816/49669]\tLoss: 412.0752\n",
      "Training Epoch: 4 [10880/49669]\tLoss: 397.2484\n",
      "Training Epoch: 4 [10944/49669]\tLoss: 443.5878\n",
      "Training Epoch: 4 [11008/49669]\tLoss: 407.5207\n",
      "Training Epoch: 4 [11072/49669]\tLoss: 444.6742\n",
      "Training Epoch: 4 [11136/49669]\tLoss: 417.9512\n",
      "Training Epoch: 4 [11200/49669]\tLoss: 411.3448\n",
      "Training Epoch: 4 [11264/49669]\tLoss: 423.4976\n",
      "Training Epoch: 4 [11328/49669]\tLoss: 410.8482\n",
      "Training Epoch: 4 [11392/49669]\tLoss: 407.3825\n",
      "Training Epoch: 4 [11456/49669]\tLoss: 400.5172\n",
      "Training Epoch: 4 [11520/49669]\tLoss: 427.7686\n",
      "Training Epoch: 4 [11584/49669]\tLoss: 443.8912\n",
      "Training Epoch: 4 [11648/49669]\tLoss: 387.7979\n",
      "Training Epoch: 4 [11712/49669]\tLoss: 428.5848\n",
      "Training Epoch: 4 [11776/49669]\tLoss: 388.7469\n",
      "Training Epoch: 4 [11840/49669]\tLoss: 440.0242\n",
      "Training Epoch: 4 [11904/49669]\tLoss: 400.6680\n",
      "Training Epoch: 4 [11968/49669]\tLoss: 432.9814\n",
      "Training Epoch: 4 [12032/49669]\tLoss: 455.1807\n",
      "Training Epoch: 4 [12096/49669]\tLoss: 412.5893\n",
      "Training Epoch: 4 [12160/49669]\tLoss: 410.0391\n",
      "Training Epoch: 4 [12224/49669]\tLoss: 446.6705\n",
      "Training Epoch: 4 [12288/49669]\tLoss: 394.7841\n",
      "Training Epoch: 4 [12352/49669]\tLoss: 429.4623\n",
      "Training Epoch: 4 [12416/49669]\tLoss: 411.1157\n",
      "Training Epoch: 4 [12480/49669]\tLoss: 415.8636\n",
      "Training Epoch: 4 [12544/49669]\tLoss: 453.9307\n",
      "Training Epoch: 4 [12608/49669]\tLoss: 411.7435\n",
      "Training Epoch: 4 [12672/49669]\tLoss: 413.0827\n",
      "Training Epoch: 4 [12736/49669]\tLoss: 436.3909\n",
      "Training Epoch: 4 [12800/49669]\tLoss: 429.2903\n",
      "Training Epoch: 4 [12864/49669]\tLoss: 430.5481\n",
      "Training Epoch: 4 [12928/49669]\tLoss: 437.0029\n",
      "Training Epoch: 4 [12992/49669]\tLoss: 437.8118\n",
      "Training Epoch: 4 [13056/49669]\tLoss: 435.5453\n",
      "Training Epoch: 4 [13120/49669]\tLoss: 409.2154\n",
      "Training Epoch: 4 [13184/49669]\tLoss: 438.0026\n",
      "Training Epoch: 4 [13248/49669]\tLoss: 402.9560\n",
      "Training Epoch: 4 [13312/49669]\tLoss: 424.3952\n",
      "Training Epoch: 4 [13376/49669]\tLoss: 404.3622\n",
      "Training Epoch: 4 [13440/49669]\tLoss: 424.2125\n",
      "Training Epoch: 4 [13504/49669]\tLoss: 424.8405\n",
      "Training Epoch: 4 [13568/49669]\tLoss: 375.1867\n",
      "Training Epoch: 4 [13632/49669]\tLoss: 407.6303\n",
      "Training Epoch: 4 [13696/49669]\tLoss: 411.9514\n",
      "Training Epoch: 4 [13760/49669]\tLoss: 417.9273\n",
      "Training Epoch: 4 [13824/49669]\tLoss: 442.9613\n",
      "Training Epoch: 4 [13888/49669]\tLoss: 447.4292\n",
      "Training Epoch: 4 [13952/49669]\tLoss: 468.9785\n",
      "Training Epoch: 4 [14016/49669]\tLoss: 414.4322\n",
      "Training Epoch: 4 [14080/49669]\tLoss: 433.9503\n",
      "Training Epoch: 4 [14144/49669]\tLoss: 415.7714\n",
      "Training Epoch: 4 [14208/49669]\tLoss: 439.8033\n",
      "Training Epoch: 4 [14272/49669]\tLoss: 444.8682\n",
      "Training Epoch: 4 [14336/49669]\tLoss: 415.4640\n",
      "Training Epoch: 4 [14400/49669]\tLoss: 424.2304\n",
      "Training Epoch: 4 [14464/49669]\tLoss: 402.3629\n",
      "Training Epoch: 4 [14528/49669]\tLoss: 422.1585\n",
      "Training Epoch: 4 [14592/49669]\tLoss: 422.3122\n",
      "Training Epoch: 4 [14656/49669]\tLoss: 400.5544\n",
      "Training Epoch: 4 [14720/49669]\tLoss: 440.0150\n",
      "Training Epoch: 4 [14784/49669]\tLoss: 423.1772\n",
      "Training Epoch: 4 [14848/49669]\tLoss: 444.9496\n",
      "Training Epoch: 4 [14912/49669]\tLoss: 399.0971\n",
      "Training Epoch: 4 [14976/49669]\tLoss: 411.9878\n",
      "Training Epoch: 4 [15040/49669]\tLoss: 446.3861\n",
      "Training Epoch: 4 [15104/49669]\tLoss: 445.2411\n",
      "Training Epoch: 4 [15168/49669]\tLoss: 426.8449\n",
      "Training Epoch: 4 [15232/49669]\tLoss: 424.5517\n",
      "Training Epoch: 4 [15296/49669]\tLoss: 438.1328\n",
      "Training Epoch: 4 [15360/49669]\tLoss: 408.4567\n",
      "Training Epoch: 4 [15424/49669]\tLoss: 425.5190\n",
      "Training Epoch: 4 [15488/49669]\tLoss: 423.8892\n",
      "Training Epoch: 4 [15552/49669]\tLoss: 429.6647\n",
      "Training Epoch: 4 [15616/49669]\tLoss: 424.0878\n",
      "Training Epoch: 4 [15680/49669]\tLoss: 441.0311\n",
      "Training Epoch: 4 [15744/49669]\tLoss: 429.0474\n",
      "Training Epoch: 4 [15808/49669]\tLoss: 431.8932\n",
      "Training Epoch: 4 [15872/49669]\tLoss: 467.6061\n",
      "Training Epoch: 4 [15936/49669]\tLoss: 432.6292\n",
      "Training Epoch: 4 [16000/49669]\tLoss: 451.3776\n",
      "Training Epoch: 4 [16064/49669]\tLoss: 448.6720\n",
      "Training Epoch: 4 [16128/49669]\tLoss: 445.0153\n",
      "Training Epoch: 4 [16192/49669]\tLoss: 444.1843\n",
      "Training Epoch: 4 [16256/49669]\tLoss: 397.7267\n",
      "Training Epoch: 4 [16320/49669]\tLoss: 431.9868\n",
      "Training Epoch: 4 [16384/49669]\tLoss: 419.5075\n",
      "Training Epoch: 4 [16448/49669]\tLoss: 444.9554\n",
      "Training Epoch: 4 [16512/49669]\tLoss: 427.7663\n",
      "Training Epoch: 4 [16576/49669]\tLoss: 431.2524\n",
      "Training Epoch: 4 [16640/49669]\tLoss: 418.1689\n",
      "Training Epoch: 4 [16704/49669]\tLoss: 404.9376\n",
      "Training Epoch: 4 [16768/49669]\tLoss: 397.5204\n",
      "Training Epoch: 4 [16832/49669]\tLoss: 420.4198\n",
      "Training Epoch: 4 [16896/49669]\tLoss: 413.6478\n",
      "Training Epoch: 4 [16960/49669]\tLoss: 414.3347\n",
      "Training Epoch: 4 [17024/49669]\tLoss: 413.4591\n",
      "Training Epoch: 4 [17088/49669]\tLoss: 387.1994\n",
      "Training Epoch: 4 [17152/49669]\tLoss: 419.6970\n",
      "Training Epoch: 4 [17216/49669]\tLoss: 395.0068\n",
      "Training Epoch: 4 [17280/49669]\tLoss: 430.2611\n",
      "Training Epoch: 4 [17344/49669]\tLoss: 410.6505\n",
      "Training Epoch: 4 [17408/49669]\tLoss: 411.1337\n",
      "Training Epoch: 4 [17472/49669]\tLoss: 445.4194\n",
      "Training Epoch: 4 [17536/49669]\tLoss: 427.9801\n",
      "Training Epoch: 4 [17600/49669]\tLoss: 414.4430\n",
      "Training Epoch: 4 [17664/49669]\tLoss: 400.2574\n",
      "Training Epoch: 4 [17728/49669]\tLoss: 444.8397\n",
      "Training Epoch: 4 [17792/49669]\tLoss: 431.6746\n",
      "Training Epoch: 4 [17856/49669]\tLoss: 423.4738\n",
      "Training Epoch: 4 [17920/49669]\tLoss: 441.1521\n",
      "Training Epoch: 4 [17984/49669]\tLoss: 432.0413\n",
      "Training Epoch: 4 [18048/49669]\tLoss: 414.9926\n",
      "Training Epoch: 4 [18112/49669]\tLoss: 427.3173\n",
      "Training Epoch: 4 [18176/49669]\tLoss: 409.0327\n",
      "Training Epoch: 4 [18240/49669]\tLoss: 401.8239\n",
      "Training Epoch: 4 [18304/49669]\tLoss: 428.3777\n",
      "Training Epoch: 4 [18368/49669]\tLoss: 419.9445\n",
      "Training Epoch: 4 [18432/49669]\tLoss: 443.1931\n",
      "Training Epoch: 4 [18496/49669]\tLoss: 434.8741\n",
      "Training Epoch: 4 [18560/49669]\tLoss: 440.8106\n",
      "Training Epoch: 4 [18624/49669]\tLoss: 405.9259\n",
      "Training Epoch: 4 [18688/49669]\tLoss: 426.2905\n",
      "Training Epoch: 4 [18752/49669]\tLoss: 401.8930\n",
      "Training Epoch: 4 [18816/49669]\tLoss: 415.8589\n",
      "Training Epoch: 4 [18880/49669]\tLoss: 389.4192\n",
      "Training Epoch: 4 [18944/49669]\tLoss: 382.6207\n",
      "Training Epoch: 4 [19008/49669]\tLoss: 413.1951\n",
      "Training Epoch: 4 [19072/49669]\tLoss: 424.3283\n",
      "Training Epoch: 4 [19136/49669]\tLoss: 438.6147\n",
      "Training Epoch: 4 [19200/49669]\tLoss: 388.9799\n",
      "Training Epoch: 4 [19264/49669]\tLoss: 417.8774\n",
      "Training Epoch: 4 [19328/49669]\tLoss: 388.0508\n",
      "Training Epoch: 4 [19392/49669]\tLoss: 425.5130\n",
      "Training Epoch: 4 [19456/49669]\tLoss: 447.9779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [19520/49669]\tLoss: 449.2930\n",
      "Training Epoch: 4 [19584/49669]\tLoss: 430.5338\n",
      "Training Epoch: 4 [19648/49669]\tLoss: 422.6362\n",
      "Training Epoch: 4 [19712/49669]\tLoss: 435.5710\n",
      "Training Epoch: 4 [19776/49669]\tLoss: 437.9362\n",
      "Training Epoch: 4 [19840/49669]\tLoss: 462.2587\n",
      "Training Epoch: 4 [19904/49669]\tLoss: 456.8811\n",
      "Training Epoch: 4 [19968/49669]\tLoss: 420.9663\n",
      "Training Epoch: 4 [20032/49669]\tLoss: 429.2365\n",
      "Training Epoch: 4 [20096/49669]\tLoss: 426.5208\n",
      "Training Epoch: 4 [20160/49669]\tLoss: 446.4648\n",
      "Training Epoch: 4 [20224/49669]\tLoss: 450.8951\n",
      "Training Epoch: 4 [20288/49669]\tLoss: 462.2835\n",
      "Training Epoch: 4 [20352/49669]\tLoss: 427.1671\n",
      "Training Epoch: 4 [20416/49669]\tLoss: 450.3431\n",
      "Training Epoch: 4 [20480/49669]\tLoss: 414.6674\n",
      "Training Epoch: 4 [20544/49669]\tLoss: 428.5570\n",
      "Training Epoch: 4 [20608/49669]\tLoss: 410.7000\n",
      "Training Epoch: 4 [20672/49669]\tLoss: 425.1736\n",
      "Training Epoch: 4 [20736/49669]\tLoss: 426.4424\n",
      "Training Epoch: 4 [20800/49669]\tLoss: 435.5684\n",
      "Training Epoch: 4 [20864/49669]\tLoss: 446.1241\n",
      "Training Epoch: 4 [20928/49669]\tLoss: 441.2380\n",
      "Training Epoch: 4 [20992/49669]\tLoss: 390.8085\n",
      "Training Epoch: 4 [21056/49669]\tLoss: 416.3649\n",
      "Training Epoch: 4 [21120/49669]\tLoss: 427.9652\n",
      "Training Epoch: 4 [21184/49669]\tLoss: 437.3417\n",
      "Training Epoch: 4 [21248/49669]\tLoss: 389.8148\n",
      "Training Epoch: 4 [21312/49669]\tLoss: 450.8256\n",
      "Training Epoch: 4 [21376/49669]\tLoss: 388.8495\n",
      "Training Epoch: 4 [21440/49669]\tLoss: 428.7719\n",
      "Training Epoch: 4 [21504/49669]\tLoss: 393.5424\n",
      "Training Epoch: 4 [21568/49669]\tLoss: 412.0870\n",
      "Training Epoch: 4 [21632/49669]\tLoss: 436.0356\n",
      "Training Epoch: 4 [21696/49669]\tLoss: 441.4159\n",
      "Training Epoch: 4 [21760/49669]\tLoss: 434.9998\n",
      "Training Epoch: 4 [21824/49669]\tLoss: 464.2794\n",
      "Training Epoch: 4 [21888/49669]\tLoss: 452.2501\n",
      "Training Epoch: 4 [21952/49669]\tLoss: 487.9849\n",
      "Training Epoch: 4 [22016/49669]\tLoss: 506.1165\n",
      "Training Epoch: 4 [22080/49669]\tLoss: 517.0674\n",
      "Training Epoch: 4 [22144/49669]\tLoss: 538.2855\n",
      "Training Epoch: 4 [22208/49669]\tLoss: 518.6661\n",
      "Training Epoch: 4 [22272/49669]\tLoss: 456.4797\n",
      "Training Epoch: 4 [22336/49669]\tLoss: 453.2572\n",
      "Training Epoch: 4 [22400/49669]\tLoss: 443.8517\n",
      "Training Epoch: 4 [22464/49669]\tLoss: 480.8027\n",
      "Training Epoch: 4 [22528/49669]\tLoss: 521.2708\n",
      "Training Epoch: 4 [22592/49669]\tLoss: 458.5763\n",
      "Training Epoch: 4 [22656/49669]\tLoss: 447.0659\n",
      "Training Epoch: 4 [22720/49669]\tLoss: 413.3658\n",
      "Training Epoch: 4 [22784/49669]\tLoss: 430.4923\n",
      "Training Epoch: 4 [22848/49669]\tLoss: 450.9461\n",
      "Training Epoch: 4 [22912/49669]\tLoss: 426.0150\n",
      "Training Epoch: 4 [22976/49669]\tLoss: 430.0347\n",
      "Training Epoch: 4 [23040/49669]\tLoss: 428.2134\n",
      "Training Epoch: 4 [23104/49669]\tLoss: 428.8242\n",
      "Training Epoch: 4 [23168/49669]\tLoss: 445.2758\n",
      "Training Epoch: 4 [23232/49669]\tLoss: 436.8766\n",
      "Training Epoch: 4 [23296/49669]\tLoss: 475.3518\n",
      "Training Epoch: 4 [23360/49669]\tLoss: 427.2454\n",
      "Training Epoch: 4 [23424/49669]\tLoss: 432.3431\n",
      "Training Epoch: 4 [23488/49669]\tLoss: 411.7625\n",
      "Training Epoch: 4 [23552/49669]\tLoss: 463.6217\n",
      "Training Epoch: 4 [23616/49669]\tLoss: 437.5017\n",
      "Training Epoch: 4 [23680/49669]\tLoss: 463.6291\n",
      "Training Epoch: 4 [23744/49669]\tLoss: 405.6065\n",
      "Training Epoch: 4 [23808/49669]\tLoss: 445.9864\n",
      "Training Epoch: 4 [23872/49669]\tLoss: 413.4383\n",
      "Training Epoch: 4 [23936/49669]\tLoss: 408.3212\n",
      "Training Epoch: 4 [24000/49669]\tLoss: 416.5359\n",
      "Training Epoch: 4 [24064/49669]\tLoss: 434.4601\n",
      "Training Epoch: 4 [24128/49669]\tLoss: 446.6488\n",
      "Training Epoch: 4 [24192/49669]\tLoss: 451.4025\n",
      "Training Epoch: 4 [24256/49669]\tLoss: 438.2053\n",
      "Training Epoch: 4 [24320/49669]\tLoss: 433.0949\n",
      "Training Epoch: 4 [24384/49669]\tLoss: 412.4406\n",
      "Training Epoch: 4 [24448/49669]\tLoss: 394.1915\n",
      "Training Epoch: 4 [24512/49669]\tLoss: 385.0209\n",
      "Training Epoch: 4 [24576/49669]\tLoss: 426.2614\n",
      "Training Epoch: 4 [24640/49669]\tLoss: 430.0448\n",
      "Training Epoch: 4 [24704/49669]\tLoss: 421.3672\n",
      "Training Epoch: 4 [24768/49669]\tLoss: 444.4027\n",
      "Training Epoch: 4 [24832/49669]\tLoss: 413.5189\n",
      "Training Epoch: 4 [24896/49669]\tLoss: 429.7344\n",
      "Training Epoch: 4 [24960/49669]\tLoss: 432.9729\n",
      "Training Epoch: 4 [25024/49669]\tLoss: 461.9239\n",
      "Training Epoch: 4 [25088/49669]\tLoss: 424.7134\n",
      "Training Epoch: 4 [25152/49669]\tLoss: 415.6071\n",
      "Training Epoch: 4 [25216/49669]\tLoss: 418.6102\n",
      "Training Epoch: 4 [25280/49669]\tLoss: 420.7740\n",
      "Training Epoch: 4 [25344/49669]\tLoss: 445.8357\n",
      "Training Epoch: 4 [25408/49669]\tLoss: 425.1101\n",
      "Training Epoch: 4 [25472/49669]\tLoss: 453.7877\n",
      "Training Epoch: 4 [25536/49669]\tLoss: 438.2133\n",
      "Training Epoch: 4 [25600/49669]\tLoss: 405.1052\n",
      "Training Epoch: 4 [25664/49669]\tLoss: 449.1319\n",
      "Training Epoch: 4 [25728/49669]\tLoss: 422.6153\n",
      "Training Epoch: 4 [25792/49669]\tLoss: 444.4560\n",
      "Training Epoch: 4 [25856/49669]\tLoss: 418.4037\n",
      "Training Epoch: 4 [25920/49669]\tLoss: 410.5449\n",
      "Training Epoch: 4 [25984/49669]\tLoss: 445.5268\n",
      "Training Epoch: 4 [26048/49669]\tLoss: 436.7999\n",
      "Training Epoch: 4 [26112/49669]\tLoss: 387.7434\n",
      "Training Epoch: 4 [26176/49669]\tLoss: 413.4847\n",
      "Training Epoch: 4 [26240/49669]\tLoss: 415.6252\n",
      "Training Epoch: 4 [26304/49669]\tLoss: 405.2928\n",
      "Training Epoch: 4 [26368/49669]\tLoss: 398.4669\n",
      "Training Epoch: 4 [26432/49669]\tLoss: 418.5861\n",
      "Training Epoch: 4 [26496/49669]\tLoss: 400.6341\n",
      "Training Epoch: 4 [26560/49669]\tLoss: 418.5764\n",
      "Training Epoch: 4 [26624/49669]\tLoss: 423.7425\n",
      "Training Epoch: 4 [26688/49669]\tLoss: 387.6516\n",
      "Training Epoch: 4 [26752/49669]\tLoss: 411.1657\n",
      "Training Epoch: 4 [26816/49669]\tLoss: 412.6680\n",
      "Training Epoch: 4 [26880/49669]\tLoss: 430.7570\n",
      "Training Epoch: 4 [26944/49669]\tLoss: 414.2071\n",
      "Training Epoch: 4 [27008/49669]\tLoss: 431.8985\n",
      "Training Epoch: 4 [27072/49669]\tLoss: 415.5894\n",
      "Training Epoch: 4 [27136/49669]\tLoss: 398.4475\n",
      "Training Epoch: 4 [27200/49669]\tLoss: 408.5026\n",
      "Training Epoch: 4 [27264/49669]\tLoss: 403.6909\n",
      "Training Epoch: 4 [27328/49669]\tLoss: 413.1544\n",
      "Training Epoch: 4 [27392/49669]\tLoss: 397.2580\n",
      "Training Epoch: 4 [27456/49669]\tLoss: 434.7977\n",
      "Training Epoch: 4 [27520/49669]\tLoss: 449.8972\n",
      "Training Epoch: 4 [27584/49669]\tLoss: 416.0664\n",
      "Training Epoch: 4 [27648/49669]\tLoss: 425.5609\n",
      "Training Epoch: 4 [27712/49669]\tLoss: 405.8748\n",
      "Training Epoch: 4 [27776/49669]\tLoss: 358.0107\n",
      "Training Epoch: 4 [27840/49669]\tLoss: 413.6119\n",
      "Training Epoch: 4 [27904/49669]\tLoss: 464.5325\n",
      "Training Epoch: 4 [27968/49669]\tLoss: 393.9825\n",
      "Training Epoch: 4 [28032/49669]\tLoss: 414.1884\n",
      "Training Epoch: 4 [28096/49669]\tLoss: 441.6129\n",
      "Training Epoch: 4 [28160/49669]\tLoss: 429.7773\n",
      "Training Epoch: 4 [28224/49669]\tLoss: 412.8820\n",
      "Training Epoch: 4 [28288/49669]\tLoss: 419.2114\n",
      "Training Epoch: 4 [28352/49669]\tLoss: 428.6465\n",
      "Training Epoch: 4 [28416/49669]\tLoss: 395.7268\n",
      "Training Epoch: 4 [28480/49669]\tLoss: 431.0542\n",
      "Training Epoch: 4 [28544/49669]\tLoss: 404.5806\n",
      "Training Epoch: 4 [28608/49669]\tLoss: 429.4620\n",
      "Training Epoch: 4 [28672/49669]\tLoss: 429.8644\n",
      "Training Epoch: 4 [28736/49669]\tLoss: 419.7743\n",
      "Training Epoch: 4 [28800/49669]\tLoss: 414.3230\n",
      "Training Epoch: 4 [28864/49669]\tLoss: 425.1237\n",
      "Training Epoch: 4 [28928/49669]\tLoss: 433.2847\n",
      "Training Epoch: 4 [28992/49669]\tLoss: 403.7098\n",
      "Training Epoch: 4 [29056/49669]\tLoss: 428.0589\n",
      "Training Epoch: 4 [29120/49669]\tLoss: 412.1537\n",
      "Training Epoch: 4 [29184/49669]\tLoss: 423.7644\n",
      "Training Epoch: 4 [29248/49669]\tLoss: 419.7122\n",
      "Training Epoch: 4 [29312/49669]\tLoss: 400.3256\n",
      "Training Epoch: 4 [29376/49669]\tLoss: 416.0635\n",
      "Training Epoch: 4 [29440/49669]\tLoss: 435.8935\n",
      "Training Epoch: 4 [29504/49669]\tLoss: 435.0908\n",
      "Training Epoch: 4 [29568/49669]\tLoss: 425.1069\n",
      "Training Epoch: 4 [29632/49669]\tLoss: 439.8104\n",
      "Training Epoch: 4 [29696/49669]\tLoss: 402.2320\n",
      "Training Epoch: 4 [29760/49669]\tLoss: 462.2047\n",
      "Training Epoch: 4 [29824/49669]\tLoss: 405.8244\n",
      "Training Epoch: 4 [29888/49669]\tLoss: 436.8168\n",
      "Training Epoch: 4 [29952/49669]\tLoss: 437.8173\n",
      "Training Epoch: 4 [30016/49669]\tLoss: 422.5518\n",
      "Training Epoch: 4 [30080/49669]\tLoss: 434.8335\n",
      "Training Epoch: 4 [30144/49669]\tLoss: 409.5973\n",
      "Training Epoch: 4 [30208/49669]\tLoss: 430.0627\n",
      "Training Epoch: 4 [30272/49669]\tLoss: 426.9586\n",
      "Training Epoch: 4 [30336/49669]\tLoss: 443.9305\n",
      "Training Epoch: 4 [30400/49669]\tLoss: 383.2782\n",
      "Training Epoch: 4 [30464/49669]\tLoss: 406.9339\n",
      "Training Epoch: 4 [30528/49669]\tLoss: 416.7210\n",
      "Training Epoch: 4 [30592/49669]\tLoss: 430.7897\n",
      "Training Epoch: 4 [30656/49669]\tLoss: 405.5554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [30720/49669]\tLoss: 430.8829\n",
      "Training Epoch: 4 [30784/49669]\tLoss: 436.5433\n",
      "Training Epoch: 4 [30848/49669]\tLoss: 444.4247\n",
      "Training Epoch: 4 [30912/49669]\tLoss: 424.9590\n",
      "Training Epoch: 4 [30976/49669]\tLoss: 436.5257\n",
      "Training Epoch: 4 [31040/49669]\tLoss: 452.2036\n",
      "Training Epoch: 4 [31104/49669]\tLoss: 375.4717\n",
      "Training Epoch: 4 [31168/49669]\tLoss: 393.1735\n",
      "Training Epoch: 4 [31232/49669]\tLoss: 394.4246\n",
      "Training Epoch: 4 [31296/49669]\tLoss: 427.2594\n",
      "Training Epoch: 4 [31360/49669]\tLoss: 406.8780\n",
      "Training Epoch: 4 [31424/49669]\tLoss: 407.0771\n",
      "Training Epoch: 4 [31488/49669]\tLoss: 438.2620\n",
      "Training Epoch: 4 [31552/49669]\tLoss: 411.9319\n",
      "Training Epoch: 4 [31616/49669]\tLoss: 443.6159\n",
      "Training Epoch: 4 [31680/49669]\tLoss: 420.6482\n",
      "Training Epoch: 4 [31744/49669]\tLoss: 418.2183\n",
      "Training Epoch: 4 [31808/49669]\tLoss: 469.5272\n",
      "Training Epoch: 4 [31872/49669]\tLoss: 415.3810\n",
      "Training Epoch: 4 [31936/49669]\tLoss: 385.7415\n",
      "Training Epoch: 4 [32000/49669]\tLoss: 444.9640\n",
      "Training Epoch: 4 [32064/49669]\tLoss: 429.8714\n",
      "Training Epoch: 4 [32128/49669]\tLoss: 398.4565\n",
      "Training Epoch: 4 [32192/49669]\tLoss: 407.6605\n",
      "Training Epoch: 4 [32256/49669]\tLoss: 423.1078\n",
      "Training Epoch: 4 [32320/49669]\tLoss: 384.8208\n",
      "Training Epoch: 4 [32384/49669]\tLoss: 430.4203\n",
      "Training Epoch: 4 [32448/49669]\tLoss: 445.4982\n",
      "Training Epoch: 4 [32512/49669]\tLoss: 423.2914\n",
      "Training Epoch: 4 [32576/49669]\tLoss: 460.1547\n",
      "Training Epoch: 4 [32640/49669]\tLoss: 421.2991\n",
      "Training Epoch: 4 [32704/49669]\tLoss: 437.2230\n",
      "Training Epoch: 4 [32768/49669]\tLoss: 444.9095\n",
      "Training Epoch: 4 [32832/49669]\tLoss: 445.3423\n",
      "Training Epoch: 4 [32896/49669]\tLoss: 389.7054\n",
      "Training Epoch: 4 [32960/49669]\tLoss: 436.8129\n",
      "Training Epoch: 4 [33024/49669]\tLoss: 416.8116\n",
      "Training Epoch: 4 [33088/49669]\tLoss: 422.8709\n",
      "Training Epoch: 4 [33152/49669]\tLoss: 441.7529\n",
      "Training Epoch: 4 [33216/49669]\tLoss: 429.4982\n",
      "Training Epoch: 4 [33280/49669]\tLoss: 433.9919\n",
      "Training Epoch: 4 [33344/49669]\tLoss: 429.8767\n",
      "Training Epoch: 4 [33408/49669]\tLoss: 419.3105\n",
      "Training Epoch: 4 [33472/49669]\tLoss: 421.3981\n",
      "Training Epoch: 4 [33536/49669]\tLoss: 434.9699\n",
      "Training Epoch: 4 [33600/49669]\tLoss: 424.8875\n",
      "Training Epoch: 4 [33664/49669]\tLoss: 410.9091\n",
      "Training Epoch: 4 [33728/49669]\tLoss: 450.5938\n",
      "Training Epoch: 4 [33792/49669]\tLoss: 430.3476\n",
      "Training Epoch: 4 [33856/49669]\tLoss: 409.2578\n",
      "Training Epoch: 4 [33920/49669]\tLoss: 418.1569\n",
      "Training Epoch: 4 [33984/49669]\tLoss: 414.5637\n",
      "Training Epoch: 4 [34048/49669]\tLoss: 415.4078\n",
      "Training Epoch: 4 [34112/49669]\tLoss: 397.0708\n",
      "Training Epoch: 4 [34176/49669]\tLoss: 405.4856\n",
      "Training Epoch: 4 [34240/49669]\tLoss: 391.7182\n",
      "Training Epoch: 4 [34304/49669]\tLoss: 403.4944\n",
      "Training Epoch: 4 [34368/49669]\tLoss: 400.5726\n",
      "Training Epoch: 4 [34432/49669]\tLoss: 415.9198\n",
      "Training Epoch: 4 [34496/49669]\tLoss: 385.0957\n",
      "Training Epoch: 4 [34560/49669]\tLoss: 425.0205\n",
      "Training Epoch: 4 [34624/49669]\tLoss: 439.2881\n",
      "Training Epoch: 4 [34688/49669]\tLoss: 446.3660\n",
      "Training Epoch: 4 [34752/49669]\tLoss: 410.1229\n",
      "Training Epoch: 4 [34816/49669]\tLoss: 433.9436\n",
      "Training Epoch: 4 [34880/49669]\tLoss: 420.9518\n",
      "Training Epoch: 4 [34944/49669]\tLoss: 401.8026\n",
      "Training Epoch: 4 [35008/49669]\tLoss: 399.9485\n",
      "Training Epoch: 4 [35072/49669]\tLoss: 401.2085\n",
      "Training Epoch: 4 [35136/49669]\tLoss: 442.9655\n",
      "Training Epoch: 4 [35200/49669]\tLoss: 419.6585\n",
      "Training Epoch: 4 [35264/49669]\tLoss: 429.6501\n",
      "Training Epoch: 4 [35328/49669]\tLoss: 400.7958\n",
      "Training Epoch: 4 [35392/49669]\tLoss: 420.0508\n",
      "Training Epoch: 4 [35456/49669]\tLoss: 405.8017\n",
      "Training Epoch: 4 [35520/49669]\tLoss: 419.2631\n",
      "Training Epoch: 4 [35584/49669]\tLoss: 427.7448\n",
      "Training Epoch: 4 [35648/49669]\tLoss: 435.7321\n",
      "Training Epoch: 4 [35712/49669]\tLoss: 422.4331\n",
      "Training Epoch: 4 [35776/49669]\tLoss: 447.4755\n",
      "Training Epoch: 4 [35840/49669]\tLoss: 440.7798\n",
      "Training Epoch: 4 [35904/49669]\tLoss: 416.7094\n",
      "Training Epoch: 4 [35968/49669]\tLoss: 418.0479\n",
      "Training Epoch: 4 [36032/49669]\tLoss: 425.7081\n",
      "Training Epoch: 4 [36096/49669]\tLoss: 405.9988\n",
      "Training Epoch: 4 [36160/49669]\tLoss: 413.7930\n",
      "Training Epoch: 4 [36224/49669]\tLoss: 413.8934\n",
      "Training Epoch: 4 [36288/49669]\tLoss: 430.3636\n",
      "Training Epoch: 4 [36352/49669]\tLoss: 450.9775\n",
      "Training Epoch: 4 [36416/49669]\tLoss: 438.5461\n",
      "Training Epoch: 4 [36480/49669]\tLoss: 466.2167\n",
      "Training Epoch: 4 [36544/49669]\tLoss: 422.9822\n",
      "Training Epoch: 4 [36608/49669]\tLoss: 428.8433\n",
      "Training Epoch: 4 [36672/49669]\tLoss: 412.0021\n",
      "Training Epoch: 4 [36736/49669]\tLoss: 436.2367\n",
      "Training Epoch: 4 [36800/49669]\tLoss: 435.8187\n",
      "Training Epoch: 4 [36864/49669]\tLoss: 400.5448\n",
      "Training Epoch: 4 [36928/49669]\tLoss: 419.6747\n",
      "Training Epoch: 4 [36992/49669]\tLoss: 390.2547\n",
      "Training Epoch: 4 [37056/49669]\tLoss: 417.2534\n",
      "Training Epoch: 4 [37120/49669]\tLoss: 414.6017\n",
      "Training Epoch: 4 [37184/49669]\tLoss: 430.6771\n",
      "Training Epoch: 4 [37248/49669]\tLoss: 398.9049\n",
      "Training Epoch: 4 [37312/49669]\tLoss: 428.7856\n",
      "Training Epoch: 4 [37376/49669]\tLoss: 434.8921\n",
      "Training Epoch: 4 [37440/49669]\tLoss: 412.1912\n",
      "Training Epoch: 4 [37504/49669]\tLoss: 407.2985\n",
      "Training Epoch: 4 [37568/49669]\tLoss: 416.9960\n",
      "Training Epoch: 4 [37632/49669]\tLoss: 422.5990\n",
      "Training Epoch: 4 [37696/49669]\tLoss: 434.1272\n",
      "Training Epoch: 4 [37760/49669]\tLoss: 409.4835\n",
      "Training Epoch: 4 [37824/49669]\tLoss: 416.4100\n",
      "Training Epoch: 4 [37888/49669]\tLoss: 403.8501\n",
      "Training Epoch: 4 [37952/49669]\tLoss: 438.7467\n",
      "Training Epoch: 4 [38016/49669]\tLoss: 430.0921\n",
      "Training Epoch: 4 [38080/49669]\tLoss: 414.4903\n",
      "Training Epoch: 4 [38144/49669]\tLoss: 429.8629\n",
      "Training Epoch: 4 [38208/49669]\tLoss: 425.4719\n",
      "Training Epoch: 4 [38272/49669]\tLoss: 436.9810\n",
      "Training Epoch: 4 [38336/49669]\tLoss: 425.4989\n",
      "Training Epoch: 4 [38400/49669]\tLoss: 442.1097\n",
      "Training Epoch: 4 [38464/49669]\tLoss: 467.1862\n",
      "Training Epoch: 4 [38528/49669]\tLoss: 451.6030\n",
      "Training Epoch: 4 [38592/49669]\tLoss: 435.6010\n",
      "Training Epoch: 4 [38656/49669]\tLoss: 423.4640\n",
      "Training Epoch: 4 [38720/49669]\tLoss: 403.5452\n",
      "Training Epoch: 4 [38784/49669]\tLoss: 452.2495\n",
      "Training Epoch: 4 [38848/49669]\tLoss: 397.2161\n",
      "Training Epoch: 4 [38912/49669]\tLoss: 420.3669\n",
      "Training Epoch: 4 [38976/49669]\tLoss: 423.6039\n",
      "Training Epoch: 4 [39040/49669]\tLoss: 445.3017\n",
      "Training Epoch: 4 [39104/49669]\tLoss: 423.3266\n",
      "Training Epoch: 4 [39168/49669]\tLoss: 404.2601\n",
      "Training Epoch: 4 [39232/49669]\tLoss: 402.4392\n",
      "Training Epoch: 4 [39296/49669]\tLoss: 429.1943\n",
      "Training Epoch: 4 [39360/49669]\tLoss: 452.1378\n",
      "Training Epoch: 4 [39424/49669]\tLoss: 426.8405\n",
      "Training Epoch: 4 [39488/49669]\tLoss: 426.2144\n",
      "Training Epoch: 4 [39552/49669]\tLoss: 407.1381\n",
      "Training Epoch: 4 [39616/49669]\tLoss: 424.0590\n",
      "Training Epoch: 4 [39680/49669]\tLoss: 422.7014\n",
      "Training Epoch: 4 [39744/49669]\tLoss: 452.3577\n",
      "Training Epoch: 4 [39808/49669]\tLoss: 411.4302\n",
      "Training Epoch: 4 [39872/49669]\tLoss: 416.5660\n",
      "Training Epoch: 4 [39936/49669]\tLoss: 447.7363\n",
      "Training Epoch: 4 [40000/49669]\tLoss: 434.5608\n",
      "Training Epoch: 4 [40064/49669]\tLoss: 464.8349\n",
      "Training Epoch: 4 [40128/49669]\tLoss: 426.3605\n",
      "Training Epoch: 4 [40192/49669]\tLoss: 442.6627\n",
      "Training Epoch: 4 [40256/49669]\tLoss: 449.4352\n",
      "Training Epoch: 4 [40320/49669]\tLoss: 444.5173\n",
      "Training Epoch: 4 [40384/49669]\tLoss: 425.0763\n",
      "Training Epoch: 4 [40448/49669]\tLoss: 405.2501\n",
      "Training Epoch: 4 [40512/49669]\tLoss: 415.3997\n",
      "Training Epoch: 4 [40576/49669]\tLoss: 438.9463\n",
      "Training Epoch: 4 [40640/49669]\tLoss: 439.9451\n",
      "Training Epoch: 4 [40704/49669]\tLoss: 480.2706\n",
      "Training Epoch: 4 [40768/49669]\tLoss: 458.6092\n",
      "Training Epoch: 4 [40832/49669]\tLoss: 468.6548\n",
      "Training Epoch: 4 [40896/49669]\tLoss: 463.9156\n",
      "Training Epoch: 4 [40960/49669]\tLoss: 516.9881\n",
      "Training Epoch: 4 [41024/49669]\tLoss: 512.1224\n",
      "Training Epoch: 4 [41088/49669]\tLoss: 484.5390\n",
      "Training Epoch: 4 [41152/49669]\tLoss: 470.6233\n",
      "Training Epoch: 4 [41216/49669]\tLoss: 398.6311\n",
      "Training Epoch: 4 [41280/49669]\tLoss: 416.8337\n",
      "Training Epoch: 4 [41344/49669]\tLoss: 467.1057\n",
      "Training Epoch: 4 [41408/49669]\tLoss: 463.7616\n",
      "Training Epoch: 4 [41472/49669]\tLoss: 446.3232\n",
      "Training Epoch: 4 [41536/49669]\tLoss: 424.7552\n",
      "Training Epoch: 4 [41600/49669]\tLoss: 412.1724\n",
      "Training Epoch: 4 [41664/49669]\tLoss: 418.8255\n",
      "Training Epoch: 4 [41728/49669]\tLoss: 447.3542\n",
      "Training Epoch: 4 [41792/49669]\tLoss: 451.5312\n",
      "Training Epoch: 4 [41856/49669]\tLoss: 381.2131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [41920/49669]\tLoss: 424.7817\n",
      "Training Epoch: 4 [41984/49669]\tLoss: 444.7597\n",
      "Training Epoch: 4 [42048/49669]\tLoss: 443.8374\n",
      "Training Epoch: 4 [42112/49669]\tLoss: 426.4044\n",
      "Training Epoch: 4 [42176/49669]\tLoss: 424.5222\n",
      "Training Epoch: 4 [42240/49669]\tLoss: 404.9399\n",
      "Training Epoch: 4 [42304/49669]\tLoss: 427.8065\n",
      "Training Epoch: 4 [42368/49669]\tLoss: 418.7840\n",
      "Training Epoch: 4 [42432/49669]\tLoss: 433.2270\n",
      "Training Epoch: 4 [42496/49669]\tLoss: 440.2306\n",
      "Training Epoch: 4 [42560/49669]\tLoss: 419.4420\n",
      "Training Epoch: 4 [42624/49669]\tLoss: 397.4462\n",
      "Training Epoch: 4 [42688/49669]\tLoss: 392.5934\n",
      "Training Epoch: 4 [42752/49669]\tLoss: 420.4294\n",
      "Training Epoch: 4 [42816/49669]\tLoss: 458.0744\n",
      "Training Epoch: 4 [42880/49669]\tLoss: 425.7809\n",
      "Training Epoch: 4 [42944/49669]\tLoss: 419.7239\n",
      "Training Epoch: 4 [43008/49669]\tLoss: 428.8253\n",
      "Training Epoch: 4 [43072/49669]\tLoss: 411.3472\n",
      "Training Epoch: 4 [43136/49669]\tLoss: 429.3220\n",
      "Training Epoch: 4 [43200/49669]\tLoss: 399.4485\n",
      "Training Epoch: 4 [43264/49669]\tLoss: 396.7235\n",
      "Training Epoch: 4 [43328/49669]\tLoss: 427.9553\n",
      "Training Epoch: 4 [43392/49669]\tLoss: 431.0454\n",
      "Training Epoch: 4 [43456/49669]\tLoss: 414.8929\n",
      "Training Epoch: 4 [43520/49669]\tLoss: 414.4102\n",
      "Training Epoch: 4 [43584/49669]\tLoss: 442.3081\n",
      "Training Epoch: 4 [43648/49669]\tLoss: 413.3300\n",
      "Training Epoch: 4 [43712/49669]\tLoss: 389.3937\n",
      "Training Epoch: 4 [43776/49669]\tLoss: 410.4345\n",
      "Training Epoch: 4 [43840/49669]\tLoss: 425.3380\n",
      "Training Epoch: 4 [43904/49669]\tLoss: 442.2808\n",
      "Training Epoch: 4 [43968/49669]\tLoss: 421.4338\n",
      "Training Epoch: 4 [44032/49669]\tLoss: 441.0495\n",
      "Training Epoch: 4 [44096/49669]\tLoss: 410.5010\n",
      "Training Epoch: 4 [44160/49669]\tLoss: 424.1883\n",
      "Training Epoch: 4 [44224/49669]\tLoss: 438.2073\n",
      "Training Epoch: 4 [44288/49669]\tLoss: 416.8313\n",
      "Training Epoch: 4 [44352/49669]\tLoss: 448.4442\n",
      "Training Epoch: 4 [44416/49669]\tLoss: 437.2192\n",
      "Training Epoch: 4 [44480/49669]\tLoss: 435.3072\n",
      "Training Epoch: 4 [44544/49669]\tLoss: 421.9788\n",
      "Training Epoch: 4 [44608/49669]\tLoss: 403.4675\n",
      "Training Epoch: 4 [44672/49669]\tLoss: 405.3441\n",
      "Training Epoch: 4 [44736/49669]\tLoss: 428.3409\n",
      "Training Epoch: 4 [44800/49669]\tLoss: 429.9893\n",
      "Training Epoch: 4 [44864/49669]\tLoss: 444.3542\n",
      "Training Epoch: 4 [44928/49669]\tLoss: 421.9137\n",
      "Training Epoch: 4 [44992/49669]\tLoss: 411.8838\n",
      "Training Epoch: 4 [45056/49669]\tLoss: 428.4532\n",
      "Training Epoch: 4 [45120/49669]\tLoss: 456.3337\n",
      "Training Epoch: 4 [45184/49669]\tLoss: 447.3384\n",
      "Training Epoch: 4 [45248/49669]\tLoss: 425.7320\n",
      "Training Epoch: 4 [45312/49669]\tLoss: 411.9860\n",
      "Training Epoch: 4 [45376/49669]\tLoss: 395.7044\n",
      "Training Epoch: 4 [45440/49669]\tLoss: 452.9246\n",
      "Training Epoch: 4 [45504/49669]\tLoss: 429.3811\n",
      "Training Epoch: 4 [45568/49669]\tLoss: 388.9676\n",
      "Training Epoch: 4 [45632/49669]\tLoss: 383.4738\n",
      "Training Epoch: 4 [45696/49669]\tLoss: 427.7667\n",
      "Training Epoch: 4 [45760/49669]\tLoss: 428.7335\n",
      "Training Epoch: 4 [45824/49669]\tLoss: 407.2869\n",
      "Training Epoch: 4 [45888/49669]\tLoss: 420.5025\n",
      "Training Epoch: 4 [45952/49669]\tLoss: 440.2142\n",
      "Training Epoch: 4 [46016/49669]\tLoss: 420.4420\n",
      "Training Epoch: 4 [46080/49669]\tLoss: 409.7693\n",
      "Training Epoch: 4 [46144/49669]\tLoss: 436.3667\n",
      "Training Epoch: 4 [46208/49669]\tLoss: 411.3411\n",
      "Training Epoch: 4 [46272/49669]\tLoss: 446.4025\n",
      "Training Epoch: 4 [46336/49669]\tLoss: 419.1570\n",
      "Training Epoch: 4 [46400/49669]\tLoss: 417.8347\n",
      "Training Epoch: 4 [46464/49669]\tLoss: 386.0317\n",
      "Training Epoch: 4 [46528/49669]\tLoss: 431.9505\n",
      "Training Epoch: 4 [46592/49669]\tLoss: 433.7132\n",
      "Training Epoch: 4 [46656/49669]\tLoss: 405.3459\n",
      "Training Epoch: 4 [46720/49669]\tLoss: 427.5318\n",
      "Training Epoch: 4 [46784/49669]\tLoss: 414.7688\n",
      "Training Epoch: 4 [46848/49669]\tLoss: 433.9966\n",
      "Training Epoch: 4 [46912/49669]\tLoss: 402.8045\n",
      "Training Epoch: 4 [46976/49669]\tLoss: 412.4073\n",
      "Training Epoch: 4 [47040/49669]\tLoss: 400.9455\n",
      "Training Epoch: 4 [47104/49669]\tLoss: 408.5175\n",
      "Training Epoch: 4 [47168/49669]\tLoss: 407.7553\n",
      "Training Epoch: 4 [47232/49669]\tLoss: 423.4537\n",
      "Training Epoch: 4 [47296/49669]\tLoss: 380.1097\n",
      "Training Epoch: 4 [47360/49669]\tLoss: 406.7238\n",
      "Training Epoch: 4 [47424/49669]\tLoss: 414.2958\n",
      "Training Epoch: 4 [47488/49669]\tLoss: 425.1327\n",
      "Training Epoch: 4 [47552/49669]\tLoss: 436.4240\n",
      "Training Epoch: 4 [47616/49669]\tLoss: 432.0081\n",
      "Training Epoch: 4 [47680/49669]\tLoss: 430.0313\n",
      "Training Epoch: 4 [47744/49669]\tLoss: 434.9501\n",
      "Training Epoch: 4 [47808/49669]\tLoss: 415.6540\n",
      "Training Epoch: 4 [47872/49669]\tLoss: 433.8193\n",
      "Training Epoch: 4 [47936/49669]\tLoss: 433.0194\n",
      "Training Epoch: 4 [48000/49669]\tLoss: 424.7855\n",
      "Training Epoch: 4 [48064/49669]\tLoss: 410.0954\n",
      "Training Epoch: 4 [48128/49669]\tLoss: 418.2887\n",
      "Training Epoch: 4 [48192/49669]\tLoss: 451.1961\n",
      "Training Epoch: 4 [48256/49669]\tLoss: 417.6140\n",
      "Training Epoch: 4 [48320/49669]\tLoss: 410.3187\n",
      "Training Epoch: 4 [48384/49669]\tLoss: 425.8864\n",
      "Training Epoch: 4 [48448/49669]\tLoss: 426.6880\n",
      "Training Epoch: 4 [48512/49669]\tLoss: 404.7738\n",
      "Training Epoch: 4 [48576/49669]\tLoss: 397.9351\n",
      "Training Epoch: 4 [48640/49669]\tLoss: 377.4986\n",
      "Training Epoch: 4 [48704/49669]\tLoss: 388.6547\n",
      "Training Epoch: 4 [48768/49669]\tLoss: 394.5475\n",
      "Training Epoch: 4 [48832/49669]\tLoss: 454.4352\n",
      "Training Epoch: 4 [48896/49669]\tLoss: 436.6634\n",
      "Training Epoch: 4 [48960/49669]\tLoss: 404.9622\n",
      "Training Epoch: 4 [49024/49669]\tLoss: 440.9044\n",
      "Training Epoch: 4 [49088/49669]\tLoss: 420.8403\n",
      "Training Epoch: 4 [49152/49669]\tLoss: 426.9696\n",
      "Training Epoch: 4 [49216/49669]\tLoss: 426.0875\n",
      "Training Epoch: 4 [49280/49669]\tLoss: 421.7613\n",
      "Training Epoch: 4 [49344/49669]\tLoss: 414.9390\n",
      "Training Epoch: 4 [49408/49669]\tLoss: 401.6888\n",
      "Training Epoch: 4 [49472/49669]\tLoss: 435.9022\n",
      "Training Epoch: 4 [49536/49669]\tLoss: 434.2532\n",
      "Training Epoch: 4 [49600/49669]\tLoss: 411.1034\n",
      "Training Epoch: 4 [49664/49669]\tLoss: 412.0878\n",
      "Training Epoch: 4 [49669/49669]\tLoss: 440.6429\n",
      "Training Epoch: 4 [5519/5519]\tLoss: 421.6514\n",
      "Training Epoch: 5 [64/49669]\tLoss: 405.1854\n",
      "Training Epoch: 5 [128/49669]\tLoss: 444.8310\n",
      "Training Epoch: 5 [192/49669]\tLoss: 429.9330\n",
      "Training Epoch: 5 [256/49669]\tLoss: 416.3124\n",
      "Training Epoch: 5 [320/49669]\tLoss: 422.3598\n",
      "Training Epoch: 5 [384/49669]\tLoss: 414.0062\n",
      "Training Epoch: 5 [448/49669]\tLoss: 407.1931\n",
      "Training Epoch: 5 [512/49669]\tLoss: 408.8622\n",
      "Training Epoch: 5 [576/49669]\tLoss: 407.5129\n",
      "Training Epoch: 5 [640/49669]\tLoss: 398.1310\n",
      "Training Epoch: 5 [704/49669]\tLoss: 414.5396\n",
      "Training Epoch: 5 [768/49669]\tLoss: 438.6544\n",
      "Training Epoch: 5 [832/49669]\tLoss: 441.6618\n",
      "Training Epoch: 5 [896/49669]\tLoss: 423.9832\n",
      "Training Epoch: 5 [960/49669]\tLoss: 394.6223\n",
      "Training Epoch: 5 [1024/49669]\tLoss: 417.9186\n",
      "Training Epoch: 5 [1088/49669]\tLoss: 439.4174\n",
      "Training Epoch: 5 [1152/49669]\tLoss: 422.5228\n",
      "Training Epoch: 5 [1216/49669]\tLoss: 451.6028\n",
      "Training Epoch: 5 [1280/49669]\tLoss: 386.8347\n",
      "Training Epoch: 5 [1344/49669]\tLoss: 446.5918\n",
      "Training Epoch: 5 [1408/49669]\tLoss: 419.3411\n",
      "Training Epoch: 5 [1472/49669]\tLoss: 416.9073\n",
      "Training Epoch: 5 [1536/49669]\tLoss: 429.0060\n",
      "Training Epoch: 5 [1600/49669]\tLoss: 447.9478\n",
      "Training Epoch: 5 [1664/49669]\tLoss: 419.5493\n",
      "Training Epoch: 5 [1728/49669]\tLoss: 389.0518\n",
      "Training Epoch: 5 [1792/49669]\tLoss: 409.7964\n",
      "Training Epoch: 5 [1856/49669]\tLoss: 435.6039\n",
      "Training Epoch: 5 [1920/49669]\tLoss: 403.2163\n",
      "Training Epoch: 5 [1984/49669]\tLoss: 412.9153\n",
      "Training Epoch: 5 [2048/49669]\tLoss: 411.7475\n",
      "Training Epoch: 5 [2112/49669]\tLoss: 400.1010\n",
      "Training Epoch: 5 [2176/49669]\tLoss: 396.6332\n",
      "Training Epoch: 5 [2240/49669]\tLoss: 427.4051\n",
      "Training Epoch: 5 [2304/49669]\tLoss: 435.1465\n",
      "Training Epoch: 5 [2368/49669]\tLoss: 420.3285\n",
      "Training Epoch: 5 [2432/49669]\tLoss: 424.8436\n",
      "Training Epoch: 5 [2496/49669]\tLoss: 443.2710\n",
      "Training Epoch: 5 [2560/49669]\tLoss: 429.6852\n",
      "Training Epoch: 5 [2624/49669]\tLoss: 417.0424\n",
      "Training Epoch: 5 [2688/49669]\tLoss: 415.4878\n",
      "Training Epoch: 5 [2752/49669]\tLoss: 392.9345\n",
      "Training Epoch: 5 [2816/49669]\tLoss: 435.4062\n",
      "Training Epoch: 5 [2880/49669]\tLoss: 420.0396\n",
      "Training Epoch: 5 [2944/49669]\tLoss: 423.4818\n",
      "Training Epoch: 5 [3008/49669]\tLoss: 437.6051\n",
      "Training Epoch: 5 [3072/49669]\tLoss: 410.9795\n",
      "Training Epoch: 5 [3136/49669]\tLoss: 397.0396\n",
      "Training Epoch: 5 [3200/49669]\tLoss: 399.2041\n",
      "Training Epoch: 5 [3264/49669]\tLoss: 393.5143\n",
      "Training Epoch: 5 [3328/49669]\tLoss: 442.8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [3392/49669]\tLoss: 403.9254\n",
      "Training Epoch: 5 [3456/49669]\tLoss: 442.8482\n",
      "Training Epoch: 5 [3520/49669]\tLoss: 425.8968\n",
      "Training Epoch: 5 [3584/49669]\tLoss: 428.2894\n",
      "Training Epoch: 5 [3648/49669]\tLoss: 412.4301\n",
      "Training Epoch: 5 [3712/49669]\tLoss: 427.1553\n",
      "Training Epoch: 5 [3776/49669]\tLoss: 414.5643\n",
      "Training Epoch: 5 [3840/49669]\tLoss: 442.6673\n",
      "Training Epoch: 5 [3904/49669]\tLoss: 410.9405\n",
      "Training Epoch: 5 [3968/49669]\tLoss: 428.6741\n",
      "Training Epoch: 5 [4032/49669]\tLoss: 414.8005\n",
      "Training Epoch: 5 [4096/49669]\tLoss: 426.7535\n",
      "Training Epoch: 5 [4160/49669]\tLoss: 414.8014\n",
      "Training Epoch: 5 [4224/49669]\tLoss: 402.0738\n",
      "Training Epoch: 5 [4288/49669]\tLoss: 439.2248\n",
      "Training Epoch: 5 [4352/49669]\tLoss: 436.5688\n",
      "Training Epoch: 5 [4416/49669]\tLoss: 426.0378\n",
      "Training Epoch: 5 [4480/49669]\tLoss: 438.8239\n",
      "Training Epoch: 5 [4544/49669]\tLoss: 436.0836\n",
      "Training Epoch: 5 [4608/49669]\tLoss: 354.5999\n",
      "Training Epoch: 5 [4672/49669]\tLoss: 398.7978\n",
      "Training Epoch: 5 [4736/49669]\tLoss: 420.2785\n",
      "Training Epoch: 5 [4800/49669]\tLoss: 422.7783\n",
      "Training Epoch: 5 [4864/49669]\tLoss: 411.5320\n",
      "Training Epoch: 5 [4928/49669]\tLoss: 403.7124\n",
      "Training Epoch: 5 [4992/49669]\tLoss: 434.7803\n",
      "Training Epoch: 5 [5056/49669]\tLoss: 399.3172\n",
      "Training Epoch: 5 [5120/49669]\tLoss: 392.6502\n",
      "Training Epoch: 5 [5184/49669]\tLoss: 412.5712\n",
      "Training Epoch: 5 [5248/49669]\tLoss: 427.3820\n",
      "Training Epoch: 5 [5312/49669]\tLoss: 432.4340\n",
      "Training Epoch: 5 [5376/49669]\tLoss: 431.3419\n",
      "Training Epoch: 5 [5440/49669]\tLoss: 409.8599\n",
      "Training Epoch: 5 [5504/49669]\tLoss: 421.4664\n",
      "Training Epoch: 5 [5568/49669]\tLoss: 410.0650\n",
      "Training Epoch: 5 [5632/49669]\tLoss: 418.0201\n",
      "Training Epoch: 5 [5696/49669]\tLoss: 452.7651\n",
      "Training Epoch: 5 [5760/49669]\tLoss: 432.5163\n",
      "Training Epoch: 5 [5824/49669]\tLoss: 390.3429\n",
      "Training Epoch: 5 [5888/49669]\tLoss: 409.6753\n",
      "Training Epoch: 5 [5952/49669]\tLoss: 407.4938\n",
      "Training Epoch: 5 [6016/49669]\tLoss: 413.9309\n",
      "Training Epoch: 5 [6080/49669]\tLoss: 420.3228\n",
      "Training Epoch: 5 [6144/49669]\tLoss: 415.3197\n",
      "Training Epoch: 5 [6208/49669]\tLoss: 405.1751\n",
      "Training Epoch: 5 [6272/49669]\tLoss: 413.7512\n",
      "Training Epoch: 5 [6336/49669]\tLoss: 430.2832\n",
      "Training Epoch: 5 [6400/49669]\tLoss: 449.1950\n",
      "Training Epoch: 5 [6464/49669]\tLoss: 414.3802\n",
      "Training Epoch: 5 [6528/49669]\tLoss: 435.8030\n",
      "Training Epoch: 5 [6592/49669]\tLoss: 429.3952\n",
      "Training Epoch: 5 [6656/49669]\tLoss: 393.1647\n",
      "Training Epoch: 5 [6720/49669]\tLoss: 427.9514\n",
      "Training Epoch: 5 [6784/49669]\tLoss: 407.6530\n",
      "Training Epoch: 5 [6848/49669]\tLoss: 441.8820\n",
      "Training Epoch: 5 [6912/49669]\tLoss: 422.9150\n",
      "Training Epoch: 5 [6976/49669]\tLoss: 432.2661\n",
      "Training Epoch: 5 [7040/49669]\tLoss: 406.5330\n",
      "Training Epoch: 5 [7104/49669]\tLoss: 418.4980\n",
      "Training Epoch: 5 [7168/49669]\tLoss: 417.2885\n",
      "Training Epoch: 5 [7232/49669]\tLoss: 436.5503\n",
      "Training Epoch: 5 [7296/49669]\tLoss: 406.8120\n",
      "Training Epoch: 5 [7360/49669]\tLoss: 464.5717\n",
      "Training Epoch: 5 [7424/49669]\tLoss: 427.8831\n",
      "Training Epoch: 5 [7488/49669]\tLoss: 406.9797\n",
      "Training Epoch: 5 [7552/49669]\tLoss: 439.8035\n",
      "Training Epoch: 5 [7616/49669]\tLoss: 442.7025\n",
      "Training Epoch: 5 [7680/49669]\tLoss: 444.6364\n",
      "Training Epoch: 5 [7744/49669]\tLoss: 423.0571\n",
      "Training Epoch: 5 [7808/49669]\tLoss: 421.1613\n",
      "Training Epoch: 5 [7872/49669]\tLoss: 427.6870\n",
      "Training Epoch: 5 [7936/49669]\tLoss: 458.2150\n",
      "Training Epoch: 5 [8000/49669]\tLoss: 399.7300\n",
      "Training Epoch: 5 [8064/49669]\tLoss: 441.3355\n",
      "Training Epoch: 5 [8128/49669]\tLoss: 468.7056\n",
      "Training Epoch: 5 [8192/49669]\tLoss: 433.1073\n",
      "Training Epoch: 5 [8256/49669]\tLoss: 393.7253\n",
      "Training Epoch: 5 [8320/49669]\tLoss: 429.5542\n",
      "Training Epoch: 5 [8384/49669]\tLoss: 411.5566\n",
      "Training Epoch: 5 [8448/49669]\tLoss: 416.0131\n",
      "Training Epoch: 5 [8512/49669]\tLoss: 410.2332\n",
      "Training Epoch: 5 [8576/49669]\tLoss: 375.6119\n",
      "Training Epoch: 5 [8640/49669]\tLoss: 466.1353\n",
      "Training Epoch: 5 [8704/49669]\tLoss: 442.0711\n",
      "Training Epoch: 5 [8768/49669]\tLoss: 422.3742\n",
      "Training Epoch: 5 [8832/49669]\tLoss: 447.6002\n",
      "Training Epoch: 5 [8896/49669]\tLoss: 405.8961\n",
      "Training Epoch: 5 [8960/49669]\tLoss: 397.7539\n",
      "Training Epoch: 5 [9024/49669]\tLoss: 432.5174\n",
      "Training Epoch: 5 [9088/49669]\tLoss: 377.0500\n",
      "Training Epoch: 5 [9152/49669]\tLoss: 424.9486\n",
      "Training Epoch: 5 [9216/49669]\tLoss: 423.2861\n",
      "Training Epoch: 5 [9280/49669]\tLoss: 445.5725\n",
      "Training Epoch: 5 [9344/49669]\tLoss: 443.1533\n",
      "Training Epoch: 5 [9408/49669]\tLoss: 423.2576\n",
      "Training Epoch: 5 [9472/49669]\tLoss: 426.6711\n",
      "Training Epoch: 5 [9536/49669]\tLoss: 405.5074\n",
      "Training Epoch: 5 [9600/49669]\tLoss: 438.7911\n",
      "Training Epoch: 5 [9664/49669]\tLoss: 450.3792\n",
      "Training Epoch: 5 [9728/49669]\tLoss: 455.3169\n",
      "Training Epoch: 5 [9792/49669]\tLoss: 423.3740\n",
      "Training Epoch: 5 [9856/49669]\tLoss: 413.2248\n",
      "Training Epoch: 5 [9920/49669]\tLoss: 459.8718\n",
      "Training Epoch: 5 [9984/49669]\tLoss: 439.8871\n",
      "Training Epoch: 5 [10048/49669]\tLoss: 425.7545\n",
      "Training Epoch: 5 [10112/49669]\tLoss: 413.2023\n",
      "Training Epoch: 5 [10176/49669]\tLoss: 408.0169\n",
      "Training Epoch: 5 [10240/49669]\tLoss: 402.7663\n",
      "Training Epoch: 5 [10304/49669]\tLoss: 392.8440\n",
      "Training Epoch: 5 [10368/49669]\tLoss: 418.0555\n",
      "Training Epoch: 5 [10432/49669]\tLoss: 406.9439\n",
      "Training Epoch: 5 [10496/49669]\tLoss: 445.9432\n",
      "Training Epoch: 5 [10560/49669]\tLoss: 432.2188\n",
      "Training Epoch: 5 [10624/49669]\tLoss: 451.8212\n",
      "Training Epoch: 5 [10688/49669]\tLoss: 447.0937\n",
      "Training Epoch: 5 [10752/49669]\tLoss: 483.6214\n",
      "Training Epoch: 5 [10816/49669]\tLoss: 458.8281\n",
      "Training Epoch: 5 [10880/49669]\tLoss: 440.6812\n",
      "Training Epoch: 5 [10944/49669]\tLoss: 467.4719\n",
      "Training Epoch: 5 [11008/49669]\tLoss: 436.4457\n",
      "Training Epoch: 5 [11072/49669]\tLoss: 446.2477\n",
      "Training Epoch: 5 [11136/49669]\tLoss: 389.4584\n",
      "Training Epoch: 5 [11200/49669]\tLoss: 424.5930\n",
      "Training Epoch: 5 [11264/49669]\tLoss: 421.1796\n",
      "Training Epoch: 5 [11328/49669]\tLoss: 410.4622\n",
      "Training Epoch: 5 [11392/49669]\tLoss: 453.0610\n",
      "Training Epoch: 5 [11456/49669]\tLoss: 412.6492\n",
      "Training Epoch: 5 [11520/49669]\tLoss: 456.5084\n",
      "Training Epoch: 5 [11584/49669]\tLoss: 426.2342\n",
      "Training Epoch: 5 [11648/49669]\tLoss: 409.0330\n",
      "Training Epoch: 5 [11712/49669]\tLoss: 404.4561\n",
      "Training Epoch: 5 [11776/49669]\tLoss: 401.3108\n",
      "Training Epoch: 5 [11840/49669]\tLoss: 414.8939\n",
      "Training Epoch: 5 [11904/49669]\tLoss: 444.4493\n",
      "Training Epoch: 5 [11968/49669]\tLoss: 419.5596\n",
      "Training Epoch: 5 [12032/49669]\tLoss: 412.1555\n",
      "Training Epoch: 5 [12096/49669]\tLoss: 467.6455\n",
      "Training Epoch: 5 [12160/49669]\tLoss: 433.6204\n",
      "Training Epoch: 5 [12224/49669]\tLoss: 411.7406\n",
      "Training Epoch: 5 [12288/49669]\tLoss: 419.3555\n",
      "Training Epoch: 5 [12352/49669]\tLoss: 416.0933\n",
      "Training Epoch: 5 [12416/49669]\tLoss: 422.1325\n",
      "Training Epoch: 5 [12480/49669]\tLoss: 406.0924\n",
      "Training Epoch: 5 [12544/49669]\tLoss: 399.6469\n",
      "Training Epoch: 5 [12608/49669]\tLoss: 398.7174\n",
      "Training Epoch: 5 [12672/49669]\tLoss: 415.1335\n",
      "Training Epoch: 5 [12736/49669]\tLoss: 442.4329\n",
      "Training Epoch: 5 [12800/49669]\tLoss: 456.5830\n",
      "Training Epoch: 5 [12864/49669]\tLoss: 415.3864\n",
      "Training Epoch: 5 [12928/49669]\tLoss: 451.6249\n",
      "Training Epoch: 5 [12992/49669]\tLoss: 418.9547\n",
      "Training Epoch: 5 [13056/49669]\tLoss: 412.3979\n",
      "Training Epoch: 5 [13120/49669]\tLoss: 410.0288\n",
      "Training Epoch: 5 [13184/49669]\tLoss: 423.7758\n",
      "Training Epoch: 5 [13248/49669]\tLoss: 410.7145\n",
      "Training Epoch: 5 [13312/49669]\tLoss: 440.1288\n",
      "Training Epoch: 5 [13376/49669]\tLoss: 425.2354\n",
      "Training Epoch: 5 [13440/49669]\tLoss: 450.9927\n",
      "Training Epoch: 5 [13504/49669]\tLoss: 440.7408\n",
      "Training Epoch: 5 [13568/49669]\tLoss: 423.4282\n",
      "Training Epoch: 5 [13632/49669]\tLoss: 408.0605\n",
      "Training Epoch: 5 [13696/49669]\tLoss: 413.8142\n",
      "Training Epoch: 5 [13760/49669]\tLoss: 442.7809\n",
      "Training Epoch: 5 [13824/49669]\tLoss: 378.1705\n",
      "Training Epoch: 5 [13888/49669]\tLoss: 403.3329\n",
      "Training Epoch: 5 [13952/49669]\tLoss: 426.9431\n",
      "Training Epoch: 5 [14016/49669]\tLoss: 429.1972\n",
      "Training Epoch: 5 [14080/49669]\tLoss: 433.2299\n",
      "Training Epoch: 5 [14144/49669]\tLoss: 430.8292\n",
      "Training Epoch: 5 [14208/49669]\tLoss: 387.5338\n",
      "Training Epoch: 5 [14272/49669]\tLoss: 444.6137\n",
      "Training Epoch: 5 [14336/49669]\tLoss: 409.3312\n",
      "Training Epoch: 5 [14400/49669]\tLoss: 413.5034\n",
      "Training Epoch: 5 [14464/49669]\tLoss: 446.5056\n",
      "Training Epoch: 5 [14528/49669]\tLoss: 431.7434\n",
      "Training Epoch: 5 [14592/49669]\tLoss: 441.8260\n",
      "Training Epoch: 5 [14656/49669]\tLoss: 393.0868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [14720/49669]\tLoss: 421.1997\n",
      "Training Epoch: 5 [14784/49669]\tLoss: 418.6481\n",
      "Training Epoch: 5 [14848/49669]\tLoss: 422.9428\n",
      "Training Epoch: 5 [14912/49669]\tLoss: 422.7192\n",
      "Training Epoch: 5 [14976/49669]\tLoss: 401.5530\n",
      "Training Epoch: 5 [15040/49669]\tLoss: 392.0732\n",
      "Training Epoch: 5 [15104/49669]\tLoss: 415.0746\n",
      "Training Epoch: 5 [15168/49669]\tLoss: 415.7863\n",
      "Training Epoch: 5 [15232/49669]\tLoss: 423.7281\n",
      "Training Epoch: 5 [15296/49669]\tLoss: 450.5852\n",
      "Training Epoch: 5 [15360/49669]\tLoss: 413.3674\n",
      "Training Epoch: 5 [15424/49669]\tLoss: 432.3564\n",
      "Training Epoch: 5 [15488/49669]\tLoss: 439.7164\n",
      "Training Epoch: 5 [15552/49669]\tLoss: 426.5225\n",
      "Training Epoch: 5 [15616/49669]\tLoss: 446.1907\n",
      "Training Epoch: 5 [15680/49669]\tLoss: 406.6478\n",
      "Training Epoch: 5 [15744/49669]\tLoss: 423.7549\n",
      "Training Epoch: 5 [15808/49669]\tLoss: 414.1079\n",
      "Training Epoch: 5 [15872/49669]\tLoss: 424.7166\n",
      "Training Epoch: 5 [15936/49669]\tLoss: 415.9121\n",
      "Training Epoch: 5 [16000/49669]\tLoss: 408.4304\n",
      "Training Epoch: 5 [16064/49669]\tLoss: 430.1346\n",
      "Training Epoch: 5 [16128/49669]\tLoss: 393.7215\n",
      "Training Epoch: 5 [16192/49669]\tLoss: 447.1362\n",
      "Training Epoch: 5 [16256/49669]\tLoss: 412.0581\n",
      "Training Epoch: 5 [16320/49669]\tLoss: 409.7563\n",
      "Training Epoch: 5 [16384/49669]\tLoss: 415.1165\n",
      "Training Epoch: 5 [16448/49669]\tLoss: 421.9723\n",
      "Training Epoch: 5 [16512/49669]\tLoss: 421.2868\n",
      "Training Epoch: 5 [16576/49669]\tLoss: 400.7930\n",
      "Training Epoch: 5 [16640/49669]\tLoss: 439.0472\n",
      "Training Epoch: 5 [16704/49669]\tLoss: 427.4724\n",
      "Training Epoch: 5 [16768/49669]\tLoss: 436.0579\n",
      "Training Epoch: 5 [16832/49669]\tLoss: 416.7879\n",
      "Training Epoch: 5 [16896/49669]\tLoss: 416.0757\n",
      "Training Epoch: 5 [16960/49669]\tLoss: 411.8915\n",
      "Training Epoch: 5 [17024/49669]\tLoss: 433.8989\n",
      "Training Epoch: 5 [17088/49669]\tLoss: 434.3947\n",
      "Training Epoch: 5 [17152/49669]\tLoss: 459.2764\n",
      "Training Epoch: 5 [17216/49669]\tLoss: 415.2596\n",
      "Training Epoch: 5 [17280/49669]\tLoss: 434.8010\n",
      "Training Epoch: 5 [17344/49669]\tLoss: 397.5444\n",
      "Training Epoch: 5 [17408/49669]\tLoss: 448.6504\n",
      "Training Epoch: 5 [17472/49669]\tLoss: 451.6194\n",
      "Training Epoch: 5 [17536/49669]\tLoss: 433.9749\n",
      "Training Epoch: 5 [17600/49669]\tLoss: 447.1881\n",
      "Training Epoch: 5 [17664/49669]\tLoss: 411.0498\n",
      "Training Epoch: 5 [17728/49669]\tLoss: 433.9147\n",
      "Training Epoch: 5 [17792/49669]\tLoss: 416.5803\n",
      "Training Epoch: 5 [17856/49669]\tLoss: 459.6145\n",
      "Training Epoch: 5 [17920/49669]\tLoss: 435.8451\n",
      "Training Epoch: 5 [17984/49669]\tLoss: 438.8048\n",
      "Training Epoch: 5 [18048/49669]\tLoss: 455.1686\n",
      "Training Epoch: 5 [18112/49669]\tLoss: 462.4763\n",
      "Training Epoch: 5 [18176/49669]\tLoss: 427.9472\n",
      "Training Epoch: 5 [18240/49669]\tLoss: 451.9972\n",
      "Training Epoch: 5 [18304/49669]\tLoss: 432.7728\n",
      "Training Epoch: 5 [18368/49669]\tLoss: 404.9427\n",
      "Training Epoch: 5 [18432/49669]\tLoss: 424.8929\n",
      "Training Epoch: 5 [18496/49669]\tLoss: 415.3410\n",
      "Training Epoch: 5 [18560/49669]\tLoss: 431.9531\n",
      "Training Epoch: 5 [18624/49669]\tLoss: 409.6920\n",
      "Training Epoch: 5 [18688/49669]\tLoss: 443.5644\n",
      "Training Epoch: 5 [18752/49669]\tLoss: 402.0747\n",
      "Training Epoch: 5 [18816/49669]\tLoss: 438.9066\n",
      "Training Epoch: 5 [18880/49669]\tLoss: 415.5926\n",
      "Training Epoch: 5 [18944/49669]\tLoss: 452.6411\n",
      "Training Epoch: 5 [19008/49669]\tLoss: 426.2614\n",
      "Training Epoch: 5 [19072/49669]\tLoss: 427.3476\n",
      "Training Epoch: 5 [19136/49669]\tLoss: 441.2928\n",
      "Training Epoch: 5 [19200/49669]\tLoss: 445.5049\n",
      "Training Epoch: 5 [19264/49669]\tLoss: 437.8884\n",
      "Training Epoch: 5 [19328/49669]\tLoss: 404.0025\n",
      "Training Epoch: 5 [19392/49669]\tLoss: 412.3463\n",
      "Training Epoch: 5 [19456/49669]\tLoss: 433.6633\n",
      "Training Epoch: 5 [19520/49669]\tLoss: 445.3073\n",
      "Training Epoch: 5 [19584/49669]\tLoss: 449.4489\n",
      "Training Epoch: 5 [19648/49669]\tLoss: 450.9651\n",
      "Training Epoch: 5 [19712/49669]\tLoss: 440.7611\n",
      "Training Epoch: 5 [19776/49669]\tLoss: 453.2239\n",
      "Training Epoch: 5 [19840/49669]\tLoss: 440.0995\n",
      "Training Epoch: 5 [19904/49669]\tLoss: 440.9266\n",
      "Training Epoch: 5 [19968/49669]\tLoss: 416.0895\n",
      "Training Epoch: 5 [20032/49669]\tLoss: 412.3929\n",
      "Training Epoch: 5 [20096/49669]\tLoss: 424.0794\n",
      "Training Epoch: 5 [20160/49669]\tLoss: 441.5653\n",
      "Training Epoch: 5 [20224/49669]\tLoss: 440.9523\n",
      "Training Epoch: 5 [20288/49669]\tLoss: 416.2360\n",
      "Training Epoch: 5 [20352/49669]\tLoss: 415.8087\n",
      "Training Epoch: 5 [20416/49669]\tLoss: 408.9118\n",
      "Training Epoch: 5 [20480/49669]\tLoss: 445.1759\n",
      "Training Epoch: 5 [20544/49669]\tLoss: 411.0652\n",
      "Training Epoch: 5 [20608/49669]\tLoss: 417.6390\n",
      "Training Epoch: 5 [20672/49669]\tLoss: 415.7637\n",
      "Training Epoch: 5 [20736/49669]\tLoss: 388.2363\n",
      "Training Epoch: 5 [20800/49669]\tLoss: 416.2931\n",
      "Training Epoch: 5 [20864/49669]\tLoss: 426.3283\n",
      "Training Epoch: 5 [20928/49669]\tLoss: 412.9758\n",
      "Training Epoch: 5 [20992/49669]\tLoss: 438.5921\n",
      "Training Epoch: 5 [21056/49669]\tLoss: 457.7052\n",
      "Training Epoch: 5 [21120/49669]\tLoss: 392.9566\n",
      "Training Epoch: 5 [21184/49669]\tLoss: 397.7004\n",
      "Training Epoch: 5 [21248/49669]\tLoss: 421.0948\n",
      "Training Epoch: 5 [21312/49669]\tLoss: 396.2601\n",
      "Training Epoch: 5 [21376/49669]\tLoss: 445.1177\n",
      "Training Epoch: 5 [21440/49669]\tLoss: 443.7245\n",
      "Training Epoch: 5 [21504/49669]\tLoss: 422.6163\n",
      "Training Epoch: 5 [21568/49669]\tLoss: 467.5659\n",
      "Training Epoch: 5 [21632/49669]\tLoss: 431.5525\n",
      "Training Epoch: 5 [21696/49669]\tLoss: 446.6479\n",
      "Training Epoch: 5 [21760/49669]\tLoss: 403.7342\n",
      "Training Epoch: 5 [21824/49669]\tLoss: 419.5011\n",
      "Training Epoch: 5 [21888/49669]\tLoss: 412.5679\n",
      "Training Epoch: 5 [21952/49669]\tLoss: 408.9484\n",
      "Training Epoch: 5 [22016/49669]\tLoss: 424.5511\n",
      "Training Epoch: 5 [22080/49669]\tLoss: 444.3682\n",
      "Training Epoch: 5 [22144/49669]\tLoss: 439.6828\n",
      "Training Epoch: 5 [22208/49669]\tLoss: 423.8958\n",
      "Training Epoch: 5 [22272/49669]\tLoss: 449.7574\n",
      "Training Epoch: 5 [22336/49669]\tLoss: 422.4887\n",
      "Training Epoch: 5 [22400/49669]\tLoss: 412.2155\n",
      "Training Epoch: 5 [22464/49669]\tLoss: 414.4819\n",
      "Training Epoch: 5 [22528/49669]\tLoss: 427.5030\n",
      "Training Epoch: 5 [22592/49669]\tLoss: 437.4036\n",
      "Training Epoch: 5 [22656/49669]\tLoss: 454.4196\n",
      "Training Epoch: 5 [22720/49669]\tLoss: 414.8839\n",
      "Training Epoch: 5 [22784/49669]\tLoss: 431.0197\n",
      "Training Epoch: 5 [22848/49669]\tLoss: 416.9517\n",
      "Training Epoch: 5 [22912/49669]\tLoss: 402.5714\n",
      "Training Epoch: 5 [22976/49669]\tLoss: 409.3105\n",
      "Training Epoch: 5 [23040/49669]\tLoss: 417.9773\n",
      "Training Epoch: 5 [23104/49669]\tLoss: 438.0341\n",
      "Training Epoch: 5 [23168/49669]\tLoss: 395.9398\n",
      "Training Epoch: 5 [23232/49669]\tLoss: 410.1284\n",
      "Training Epoch: 5 [23296/49669]\tLoss: 384.7192\n",
      "Training Epoch: 5 [23360/49669]\tLoss: 460.2039\n",
      "Training Epoch: 5 [23424/49669]\tLoss: 434.7765\n",
      "Training Epoch: 5 [23488/49669]\tLoss: 406.4540\n",
      "Training Epoch: 5 [23552/49669]\tLoss: 426.5029\n",
      "Training Epoch: 5 [23616/49669]\tLoss: 446.2650\n",
      "Training Epoch: 5 [23680/49669]\tLoss: 429.2711\n",
      "Training Epoch: 5 [23744/49669]\tLoss: 420.1045\n",
      "Training Epoch: 5 [23808/49669]\tLoss: 411.6106\n",
      "Training Epoch: 5 [23872/49669]\tLoss: 441.8818\n",
      "Training Epoch: 5 [23936/49669]\tLoss: 415.5298\n",
      "Training Epoch: 5 [24000/49669]\tLoss: 427.1822\n",
      "Training Epoch: 5 [24064/49669]\tLoss: 447.9385\n",
      "Training Epoch: 5 [24128/49669]\tLoss: 405.9204\n",
      "Training Epoch: 5 [24192/49669]\tLoss: 411.9385\n",
      "Training Epoch: 5 [24256/49669]\tLoss: 429.4731\n",
      "Training Epoch: 5 [24320/49669]\tLoss: 384.1400\n",
      "Training Epoch: 5 [24384/49669]\tLoss: 430.0233\n",
      "Training Epoch: 5 [24448/49669]\tLoss: 421.6749\n",
      "Training Epoch: 5 [24512/49669]\tLoss: 430.4951\n",
      "Training Epoch: 5 [24576/49669]\tLoss: 406.4991\n",
      "Training Epoch: 5 [24640/49669]\tLoss: 417.9081\n",
      "Training Epoch: 5 [24704/49669]\tLoss: 454.3604\n",
      "Training Epoch: 5 [24768/49669]\tLoss: 417.9851\n",
      "Training Epoch: 5 [24832/49669]\tLoss: 421.7606\n",
      "Training Epoch: 5 [24896/49669]\tLoss: 424.3525\n",
      "Training Epoch: 5 [24960/49669]\tLoss: 410.2353\n",
      "Training Epoch: 5 [25024/49669]\tLoss: 419.4080\n",
      "Training Epoch: 5 [25088/49669]\tLoss: 390.8120\n",
      "Training Epoch: 5 [25152/49669]\tLoss: 432.8971\n",
      "Training Epoch: 5 [25216/49669]\tLoss: 463.5154\n",
      "Training Epoch: 5 [25280/49669]\tLoss: 404.3133\n",
      "Training Epoch: 5 [25344/49669]\tLoss: 437.0766\n",
      "Training Epoch: 5 [25408/49669]\tLoss: 439.0612\n",
      "Training Epoch: 5 [25472/49669]\tLoss: 409.8146\n",
      "Training Epoch: 5 [25536/49669]\tLoss: 414.8423\n",
      "Training Epoch: 5 [25600/49669]\tLoss: 424.1137\n",
      "Training Epoch: 5 [25664/49669]\tLoss: 399.9077\n",
      "Training Epoch: 5 [25728/49669]\tLoss: 428.0669\n",
      "Training Epoch: 5 [25792/49669]\tLoss: 410.1065\n",
      "Training Epoch: 5 [25856/49669]\tLoss: 388.0516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [25920/49669]\tLoss: 402.7459\n",
      "Training Epoch: 5 [25984/49669]\tLoss: 433.7207\n",
      "Training Epoch: 5 [26048/49669]\tLoss: 435.9365\n",
      "Training Epoch: 5 [26112/49669]\tLoss: 409.7468\n",
      "Training Epoch: 5 [26176/49669]\tLoss: 423.2475\n",
      "Training Epoch: 5 [26240/49669]\tLoss: 396.2229\n",
      "Training Epoch: 5 [26304/49669]\tLoss: 412.7655\n",
      "Training Epoch: 5 [26368/49669]\tLoss: 431.0678\n",
      "Training Epoch: 5 [26432/49669]\tLoss: 425.1873\n",
      "Training Epoch: 5 [26496/49669]\tLoss: 417.3214\n",
      "Training Epoch: 5 [26560/49669]\tLoss: 393.3076\n",
      "Training Epoch: 5 [26624/49669]\tLoss: 399.9069\n",
      "Training Epoch: 5 [26688/49669]\tLoss: 432.0542\n",
      "Training Epoch: 5 [26752/49669]\tLoss: 399.9066\n",
      "Training Epoch: 5 [26816/49669]\tLoss: 406.3245\n",
      "Training Epoch: 5 [26880/49669]\tLoss: 396.6866\n",
      "Training Epoch: 5 [26944/49669]\tLoss: 429.3233\n",
      "Training Epoch: 5 [27008/49669]\tLoss: 420.9497\n",
      "Training Epoch: 5 [27072/49669]\tLoss: 454.3123\n",
      "Training Epoch: 5 [27136/49669]\tLoss: 419.8909\n",
      "Training Epoch: 5 [27200/49669]\tLoss: 388.3342\n",
      "Training Epoch: 5 [27264/49669]\tLoss: 429.3997\n",
      "Training Epoch: 5 [27328/49669]\tLoss: 423.9306\n",
      "Training Epoch: 5 [27392/49669]\tLoss: 437.8655\n",
      "Training Epoch: 5 [27456/49669]\tLoss: 417.5733\n",
      "Training Epoch: 5 [27520/49669]\tLoss: 420.6392\n",
      "Training Epoch: 5 [27584/49669]\tLoss: 413.2149\n",
      "Training Epoch: 5 [27648/49669]\tLoss: 413.4551\n",
      "Training Epoch: 5 [27712/49669]\tLoss: 421.5724\n",
      "Training Epoch: 5 [27776/49669]\tLoss: 415.6017\n",
      "Training Epoch: 5 [27840/49669]\tLoss: 432.0274\n",
      "Training Epoch: 5 [27904/49669]\tLoss: 411.9025\n",
      "Training Epoch: 5 [27968/49669]\tLoss: 429.9753\n",
      "Training Epoch: 5 [28032/49669]\tLoss: 413.6186\n",
      "Training Epoch: 5 [28096/49669]\tLoss: 423.2942\n",
      "Training Epoch: 5 [28160/49669]\tLoss: 411.8547\n",
      "Training Epoch: 5 [28224/49669]\tLoss: 444.8914\n",
      "Training Epoch: 5 [28288/49669]\tLoss: 400.0868\n",
      "Training Epoch: 5 [28352/49669]\tLoss: 404.6314\n",
      "Training Epoch: 5 [28416/49669]\tLoss: 393.0823\n",
      "Training Epoch: 5 [28480/49669]\tLoss: 448.1171\n",
      "Training Epoch: 5 [28544/49669]\tLoss: 423.7517\n",
      "Training Epoch: 5 [28608/49669]\tLoss: 445.8684\n",
      "Training Epoch: 5 [28672/49669]\tLoss: 428.3216\n",
      "Training Epoch: 5 [28736/49669]\tLoss: 438.6039\n",
      "Training Epoch: 5 [28800/49669]\tLoss: 395.9597\n",
      "Training Epoch: 5 [28864/49669]\tLoss: 437.9296\n",
      "Training Epoch: 5 [28928/49669]\tLoss: 419.2522\n",
      "Training Epoch: 5 [28992/49669]\tLoss: 418.8071\n",
      "Training Epoch: 5 [29056/49669]\tLoss: 423.9391\n",
      "Training Epoch: 5 [29120/49669]\tLoss: 407.7329\n",
      "Training Epoch: 5 [29184/49669]\tLoss: 438.5151\n",
      "Training Epoch: 5 [29248/49669]\tLoss: 410.2491\n",
      "Training Epoch: 5 [29312/49669]\tLoss: 401.6763\n",
      "Training Epoch: 5 [29376/49669]\tLoss: 396.2426\n",
      "Training Epoch: 5 [29440/49669]\tLoss: 420.9892\n",
      "Training Epoch: 5 [29504/49669]\tLoss: 466.9713\n",
      "Training Epoch: 5 [29568/49669]\tLoss: 433.3067\n",
      "Training Epoch: 5 [29632/49669]\tLoss: 436.3427\n",
      "Training Epoch: 5 [29696/49669]\tLoss: 423.3665\n",
      "Training Epoch: 5 [29760/49669]\tLoss: 408.8330\n",
      "Training Epoch: 5 [29824/49669]\tLoss: 412.7146\n",
      "Training Epoch: 5 [29888/49669]\tLoss: 432.9359\n",
      "Training Epoch: 5 [29952/49669]\tLoss: 428.0781\n",
      "Training Epoch: 5 [30016/49669]\tLoss: 410.7112\n",
      "Training Epoch: 5 [30080/49669]\tLoss: 431.7210\n",
      "Training Epoch: 5 [30144/49669]\tLoss: 424.4690\n",
      "Training Epoch: 5 [30208/49669]\tLoss: 422.7168\n",
      "Training Epoch: 5 [30272/49669]\tLoss: 416.3636\n",
      "Training Epoch: 5 [30336/49669]\tLoss: 413.1103\n",
      "Training Epoch: 5 [30400/49669]\tLoss: 431.8652\n",
      "Training Epoch: 5 [30464/49669]\tLoss: 413.4341\n",
      "Training Epoch: 5 [30528/49669]\tLoss: 458.8195\n",
      "Training Epoch: 5 [30592/49669]\tLoss: 434.5081\n",
      "Training Epoch: 5 [30656/49669]\tLoss: 415.5491\n",
      "Training Epoch: 5 [30720/49669]\tLoss: 443.1700\n",
      "Training Epoch: 5 [30784/49669]\tLoss: 413.1861\n",
      "Training Epoch: 5 [30848/49669]\tLoss: 430.9581\n",
      "Training Epoch: 5 [30912/49669]\tLoss: 434.2937\n",
      "Training Epoch: 5 [30976/49669]\tLoss: 453.3824\n",
      "Training Epoch: 5 [31040/49669]\tLoss: 466.6042\n",
      "Training Epoch: 5 [31104/49669]\tLoss: 458.9870\n",
      "Training Epoch: 5 [31168/49669]\tLoss: 501.3149\n",
      "Training Epoch: 5 [31232/49669]\tLoss: 535.8344\n",
      "Training Epoch: 5 [31296/49669]\tLoss: 598.5245\n",
      "Training Epoch: 5 [31360/49669]\tLoss: 577.0116\n",
      "Training Epoch: 5 [31424/49669]\tLoss: 519.7393\n",
      "Training Epoch: 5 [31488/49669]\tLoss: 481.7892\n",
      "Training Epoch: 5 [31552/49669]\tLoss: 435.1252\n",
      "Training Epoch: 5 [31616/49669]\tLoss: 449.2032\n",
      "Training Epoch: 5 [31680/49669]\tLoss: 474.2675\n",
      "Training Epoch: 5 [31744/49669]\tLoss: 476.7529\n",
      "Training Epoch: 5 [31808/49669]\tLoss: 431.3414\n",
      "Training Epoch: 5 [31872/49669]\tLoss: 420.4054\n",
      "Training Epoch: 5 [31936/49669]\tLoss: 401.8948\n",
      "Training Epoch: 5 [32000/49669]\tLoss: 446.2281\n",
      "Training Epoch: 5 [32064/49669]\tLoss: 439.6834\n",
      "Training Epoch: 5 [32128/49669]\tLoss: 400.7089\n",
      "Training Epoch: 5 [32192/49669]\tLoss: 425.3228\n",
      "Training Epoch: 5 [32256/49669]\tLoss: 438.0596\n",
      "Training Epoch: 5 [32320/49669]\tLoss: 415.2235\n",
      "Training Epoch: 5 [32384/49669]\tLoss: 407.1249\n",
      "Training Epoch: 5 [32448/49669]\tLoss: 414.2429\n",
      "Training Epoch: 5 [32512/49669]\tLoss: 422.0270\n",
      "Training Epoch: 5 [32576/49669]\tLoss: 413.6948\n",
      "Training Epoch: 5 [32640/49669]\tLoss: 429.7023\n",
      "Training Epoch: 5 [32704/49669]\tLoss: 430.5907\n",
      "Training Epoch: 5 [32768/49669]\tLoss: 413.2842\n",
      "Training Epoch: 5 [32832/49669]\tLoss: 402.0373\n",
      "Training Epoch: 5 [32896/49669]\tLoss: 421.3483\n",
      "Training Epoch: 5 [32960/49669]\tLoss: 402.2487\n",
      "Training Epoch: 5 [33024/49669]\tLoss: 441.3745\n",
      "Training Epoch: 5 [33088/49669]\tLoss: 403.0766\n",
      "Training Epoch: 5 [33152/49669]\tLoss: 411.9236\n",
      "Training Epoch: 5 [33216/49669]\tLoss: 449.2533\n",
      "Training Epoch: 5 [33280/49669]\tLoss: 426.5220\n",
      "Training Epoch: 5 [33344/49669]\tLoss: 413.2973\n",
      "Training Epoch: 5 [33408/49669]\tLoss: 415.9541\n",
      "Training Epoch: 5 [33472/49669]\tLoss: 393.3121\n",
      "Training Epoch: 5 [33536/49669]\tLoss: 432.2720\n",
      "Training Epoch: 5 [33600/49669]\tLoss: 406.5507\n",
      "Training Epoch: 5 [33664/49669]\tLoss: 444.4557\n",
      "Training Epoch: 5 [33728/49669]\tLoss: 449.7856\n",
      "Training Epoch: 5 [33792/49669]\tLoss: 423.9722\n",
      "Training Epoch: 5 [33856/49669]\tLoss: 454.0072\n",
      "Training Epoch: 5 [33920/49669]\tLoss: 409.3834\n",
      "Training Epoch: 5 [33984/49669]\tLoss: 430.8692\n",
      "Training Epoch: 5 [34048/49669]\tLoss: 419.4876\n",
      "Training Epoch: 5 [34112/49669]\tLoss: 422.6450\n",
      "Training Epoch: 5 [34176/49669]\tLoss: 422.8692\n",
      "Training Epoch: 5 [34240/49669]\tLoss: 427.3827\n",
      "Training Epoch: 5 [34304/49669]\tLoss: 433.2186\n",
      "Training Epoch: 5 [34368/49669]\tLoss: 435.7532\n",
      "Training Epoch: 5 [34432/49669]\tLoss: 433.2128\n",
      "Training Epoch: 5 [34496/49669]\tLoss: 412.0584\n",
      "Training Epoch: 5 [34560/49669]\tLoss: 436.3492\n",
      "Training Epoch: 5 [34624/49669]\tLoss: 433.9846\n",
      "Training Epoch: 5 [34688/49669]\tLoss: 421.4140\n",
      "Training Epoch: 5 [34752/49669]\tLoss: 382.5165\n",
      "Training Epoch: 5 [34816/49669]\tLoss: 423.5547\n",
      "Training Epoch: 5 [34880/49669]\tLoss: 408.0531\n",
      "Training Epoch: 5 [34944/49669]\tLoss: 412.3141\n",
      "Training Epoch: 5 [35008/49669]\tLoss: 430.2491\n",
      "Training Epoch: 5 [35072/49669]\tLoss: 431.7320\n",
      "Training Epoch: 5 [35136/49669]\tLoss: 411.2279\n",
      "Training Epoch: 5 [35200/49669]\tLoss: 434.3871\n",
      "Training Epoch: 5 [35264/49669]\tLoss: 411.6102\n",
      "Training Epoch: 5 [35328/49669]\tLoss: 422.4824\n",
      "Training Epoch: 5 [35392/49669]\tLoss: 469.4746\n",
      "Training Epoch: 5 [35456/49669]\tLoss: 438.6845\n",
      "Training Epoch: 5 [35520/49669]\tLoss: 443.5112\n",
      "Training Epoch: 5 [35584/49669]\tLoss: 436.7266\n",
      "Training Epoch: 5 [35648/49669]\tLoss: 419.4978\n",
      "Training Epoch: 5 [35712/49669]\tLoss: 417.8492\n",
      "Training Epoch: 5 [35776/49669]\tLoss: 434.7246\n",
      "Training Epoch: 5 [35840/49669]\tLoss: 438.2628\n",
      "Training Epoch: 5 [35904/49669]\tLoss: 418.5607\n",
      "Training Epoch: 5 [35968/49669]\tLoss: 432.0490\n",
      "Training Epoch: 5 [36032/49669]\tLoss: 441.4659\n",
      "Training Epoch: 5 [36096/49669]\tLoss: 408.0320\n",
      "Training Epoch: 5 [36160/49669]\tLoss: 409.1168\n",
      "Training Epoch: 5 [36224/49669]\tLoss: 404.3007\n",
      "Training Epoch: 5 [36288/49669]\tLoss: 420.6587\n",
      "Training Epoch: 5 [36352/49669]\tLoss: 414.6687\n",
      "Training Epoch: 5 [36416/49669]\tLoss: 430.8533\n",
      "Training Epoch: 5 [36480/49669]\tLoss: 423.9829\n",
      "Training Epoch: 5 [36544/49669]\tLoss: 424.5111\n",
      "Training Epoch: 5 [36608/49669]\tLoss: 421.4365\n",
      "Training Epoch: 5 [36672/49669]\tLoss: 398.9189\n",
      "Training Epoch: 5 [36736/49669]\tLoss: 412.8238\n",
      "Training Epoch: 5 [36800/49669]\tLoss: 425.7584\n",
      "Training Epoch: 5 [36864/49669]\tLoss: 439.4763\n",
      "Training Epoch: 5 [36928/49669]\tLoss: 416.4347\n",
      "Training Epoch: 5 [36992/49669]\tLoss: 429.9179\n",
      "Training Epoch: 5 [37056/49669]\tLoss: 399.1735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [37120/49669]\tLoss: 376.2109\n",
      "Training Epoch: 5 [37184/49669]\tLoss: 425.7998\n",
      "Training Epoch: 5 [37248/49669]\tLoss: 434.0048\n",
      "Training Epoch: 5 [37312/49669]\tLoss: 407.6439\n",
      "Training Epoch: 5 [37376/49669]\tLoss: 427.0085\n",
      "Training Epoch: 5 [37440/49669]\tLoss: 452.0612\n",
      "Training Epoch: 5 [37504/49669]\tLoss: 443.6447\n",
      "Training Epoch: 5 [37568/49669]\tLoss: 429.4409\n",
      "Training Epoch: 5 [37632/49669]\tLoss: 418.5919\n",
      "Training Epoch: 5 [37696/49669]\tLoss: 393.1926\n",
      "Training Epoch: 5 [37760/49669]\tLoss: 408.7827\n",
      "Training Epoch: 5 [37824/49669]\tLoss: 437.8407\n",
      "Training Epoch: 5 [37888/49669]\tLoss: 439.7881\n",
      "Training Epoch: 5 [37952/49669]\tLoss: 422.9518\n",
      "Training Epoch: 5 [38016/49669]\tLoss: 402.8888\n",
      "Training Epoch: 5 [38080/49669]\tLoss: 401.4661\n",
      "Training Epoch: 5 [38144/49669]\tLoss: 412.8542\n",
      "Training Epoch: 5 [38208/49669]\tLoss: 415.4167\n",
      "Training Epoch: 5 [38272/49669]\tLoss: 410.6338\n",
      "Training Epoch: 5 [38336/49669]\tLoss: 415.0484\n",
      "Training Epoch: 5 [38400/49669]\tLoss: 413.1867\n",
      "Training Epoch: 5 [38464/49669]\tLoss: 405.9442\n",
      "Training Epoch: 5 [38528/49669]\tLoss: 419.2262\n",
      "Training Epoch: 5 [38592/49669]\tLoss: 439.4705\n",
      "Training Epoch: 5 [38656/49669]\tLoss: 417.4826\n",
      "Training Epoch: 5 [38720/49669]\tLoss: 415.4682\n",
      "Training Epoch: 5 [38784/49669]\tLoss: 415.7712\n",
      "Training Epoch: 5 [38848/49669]\tLoss: 411.9517\n",
      "Training Epoch: 5 [38912/49669]\tLoss: 403.1593\n",
      "Training Epoch: 5 [38976/49669]\tLoss: 402.1566\n",
      "Training Epoch: 5 [39040/49669]\tLoss: 417.7473\n",
      "Training Epoch: 5 [39104/49669]\tLoss: 406.0726\n",
      "Training Epoch: 5 [39168/49669]\tLoss: 465.0332\n",
      "Training Epoch: 5 [39232/49669]\tLoss: 433.2547\n",
      "Training Epoch: 5 [39296/49669]\tLoss: 403.6811\n",
      "Training Epoch: 5 [39360/49669]\tLoss: 394.2202\n",
      "Training Epoch: 5 [39424/49669]\tLoss: 433.8567\n",
      "Training Epoch: 5 [39488/49669]\tLoss: 410.2181\n",
      "Training Epoch: 5 [39552/49669]\tLoss: 447.5498\n",
      "Training Epoch: 5 [39616/49669]\tLoss: 390.7703\n",
      "Training Epoch: 5 [39680/49669]\tLoss: 417.4323\n",
      "Training Epoch: 5 [39744/49669]\tLoss: 444.8045\n",
      "Training Epoch: 5 [39808/49669]\tLoss: 434.4493\n",
      "Training Epoch: 5 [39872/49669]\tLoss: 417.2730\n",
      "Training Epoch: 5 [39936/49669]\tLoss: 416.8377\n",
      "Training Epoch: 5 [40000/49669]\tLoss: 377.4951\n",
      "Training Epoch: 5 [40064/49669]\tLoss: 428.7728\n",
      "Training Epoch: 5 [40128/49669]\tLoss: 430.5074\n",
      "Training Epoch: 5 [40192/49669]\tLoss: 398.8914\n",
      "Training Epoch: 5 [40256/49669]\tLoss: 433.1607\n",
      "Training Epoch: 5 [40320/49669]\tLoss: 400.9637\n",
      "Training Epoch: 5 [40384/49669]\tLoss: 412.2761\n",
      "Training Epoch: 5 [40448/49669]\tLoss: 427.8745\n",
      "Training Epoch: 5 [40512/49669]\tLoss: 435.2641\n",
      "Training Epoch: 5 [40576/49669]\tLoss: 422.9701\n",
      "Training Epoch: 5 [40640/49669]\tLoss: 441.5887\n",
      "Training Epoch: 5 [40704/49669]\tLoss: 401.0257\n",
      "Training Epoch: 5 [40768/49669]\tLoss: 400.3203\n",
      "Training Epoch: 5 [40832/49669]\tLoss: 400.1568\n",
      "Training Epoch: 5 [40896/49669]\tLoss: 429.5837\n",
      "Training Epoch: 5 [40960/49669]\tLoss: 432.3289\n",
      "Training Epoch: 5 [41024/49669]\tLoss: 420.7293\n",
      "Training Epoch: 5 [41088/49669]\tLoss: 405.4158\n",
      "Training Epoch: 5 [41152/49669]\tLoss: 395.1526\n",
      "Training Epoch: 5 [41216/49669]\tLoss: 414.1295\n",
      "Training Epoch: 5 [41280/49669]\tLoss: 406.0283\n",
      "Training Epoch: 5 [41344/49669]\tLoss: 440.7932\n",
      "Training Epoch: 5 [41408/49669]\tLoss: 400.5566\n",
      "Training Epoch: 5 [41472/49669]\tLoss: 436.4465\n",
      "Training Epoch: 5 [41536/49669]\tLoss: 430.6914\n",
      "Training Epoch: 5 [41600/49669]\tLoss: 461.5142\n",
      "Training Epoch: 5 [41664/49669]\tLoss: 416.4946\n",
      "Training Epoch: 5 [41728/49669]\tLoss: 395.0071\n",
      "Training Epoch: 5 [41792/49669]\tLoss: 401.5832\n",
      "Training Epoch: 5 [41856/49669]\tLoss: 434.7930\n",
      "Training Epoch: 5 [41920/49669]\tLoss: 421.3657\n",
      "Training Epoch: 5 [41984/49669]\tLoss: 423.8958\n",
      "Training Epoch: 5 [42048/49669]\tLoss: 393.1611\n",
      "Training Epoch: 5 [42112/49669]\tLoss: 399.6695\n",
      "Training Epoch: 5 [42176/49669]\tLoss: 405.5240\n",
      "Training Epoch: 5 [42240/49669]\tLoss: 389.7490\n",
      "Training Epoch: 5 [42304/49669]\tLoss: 407.9922\n",
      "Training Epoch: 5 [42368/49669]\tLoss: 395.5182\n",
      "Training Epoch: 5 [42432/49669]\tLoss: 393.1230\n",
      "Training Epoch: 5 [42496/49669]\tLoss: 419.8911\n",
      "Training Epoch: 5 [42560/49669]\tLoss: 402.3713\n",
      "Training Epoch: 5 [42624/49669]\tLoss: 406.8169\n",
      "Training Epoch: 5 [42688/49669]\tLoss: 406.9521\n",
      "Training Epoch: 5 [42752/49669]\tLoss: 422.2377\n",
      "Training Epoch: 5 [42816/49669]\tLoss: 412.3236\n",
      "Training Epoch: 5 [42880/49669]\tLoss: 433.4618\n",
      "Training Epoch: 5 [42944/49669]\tLoss: 419.4216\n",
      "Training Epoch: 5 [43008/49669]\tLoss: 425.8171\n",
      "Training Epoch: 5 [43072/49669]\tLoss: 418.9366\n",
      "Training Epoch: 5 [43136/49669]\tLoss: 407.1507\n",
      "Training Epoch: 5 [43200/49669]\tLoss: 439.5391\n",
      "Training Epoch: 5 [43264/49669]\tLoss: 407.9279\n",
      "Training Epoch: 5 [43328/49669]\tLoss: 447.4655\n",
      "Training Epoch: 5 [43392/49669]\tLoss: 436.1333\n",
      "Training Epoch: 5 [43456/49669]\tLoss: 449.5101\n",
      "Training Epoch: 5 [43520/49669]\tLoss: 451.2230\n",
      "Training Epoch: 5 [43584/49669]\tLoss: 435.4371\n",
      "Training Epoch: 5 [43648/49669]\tLoss: 433.4942\n",
      "Training Epoch: 5 [43712/49669]\tLoss: 430.6263\n",
      "Training Epoch: 5 [43776/49669]\tLoss: 399.9724\n",
      "Training Epoch: 5 [43840/49669]\tLoss: 427.0908\n",
      "Training Epoch: 5 [43904/49669]\tLoss: 437.6233\n",
      "Training Epoch: 5 [43968/49669]\tLoss: 418.6574\n",
      "Training Epoch: 5 [44032/49669]\tLoss: 402.7175\n",
      "Training Epoch: 5 [44096/49669]\tLoss: 404.8043\n",
      "Training Epoch: 5 [44160/49669]\tLoss: 445.5927\n",
      "Training Epoch: 5 [44224/49669]\tLoss: 400.7216\n",
      "Training Epoch: 5 [44288/49669]\tLoss: 427.5917\n",
      "Training Epoch: 5 [44352/49669]\tLoss: 434.2245\n",
      "Training Epoch: 5 [44416/49669]\tLoss: 413.5886\n",
      "Training Epoch: 5 [44480/49669]\tLoss: 397.1016\n",
      "Training Epoch: 5 [44544/49669]\tLoss: 423.9771\n",
      "Training Epoch: 5 [44608/49669]\tLoss: 414.5053\n",
      "Training Epoch: 5 [44672/49669]\tLoss: 458.5133\n",
      "Training Epoch: 5 [44736/49669]\tLoss: 413.3069\n",
      "Training Epoch: 5 [44800/49669]\tLoss: 431.3507\n",
      "Training Epoch: 5 [44864/49669]\tLoss: 394.8642\n",
      "Training Epoch: 5 [44928/49669]\tLoss: 444.1857\n",
      "Training Epoch: 5 [44992/49669]\tLoss: 444.5220\n",
      "Training Epoch: 5 [45056/49669]\tLoss: 432.6293\n",
      "Training Epoch: 5 [45120/49669]\tLoss: 439.1271\n",
      "Training Epoch: 5 [45184/49669]\tLoss: 439.9458\n",
      "Training Epoch: 5 [45248/49669]\tLoss: 395.8057\n",
      "Training Epoch: 5 [45312/49669]\tLoss: 422.0654\n",
      "Training Epoch: 5 [45376/49669]\tLoss: 413.9673\n",
      "Training Epoch: 5 [45440/49669]\tLoss: 431.0647\n",
      "Training Epoch: 5 [45504/49669]\tLoss: 388.0893\n",
      "Training Epoch: 5 [45568/49669]\tLoss: 403.8440\n",
      "Training Epoch: 5 [45632/49669]\tLoss: 415.7533\n",
      "Training Epoch: 5 [45696/49669]\tLoss: 413.1667\n",
      "Training Epoch: 5 [45760/49669]\tLoss: 410.4961\n",
      "Training Epoch: 5 [45824/49669]\tLoss: 443.9463\n",
      "Training Epoch: 5 [45888/49669]\tLoss: 425.1789\n",
      "Training Epoch: 5 [45952/49669]\tLoss: 431.1546\n",
      "Training Epoch: 5 [46016/49669]\tLoss: 405.9671\n",
      "Training Epoch: 5 [46080/49669]\tLoss: 400.3633\n",
      "Training Epoch: 5 [46144/49669]\tLoss: 410.7783\n",
      "Training Epoch: 5 [46208/49669]\tLoss: 427.1301\n",
      "Training Epoch: 5 [46272/49669]\tLoss: 418.6746\n",
      "Training Epoch: 5 [46336/49669]\tLoss: 415.5369\n",
      "Training Epoch: 5 [46400/49669]\tLoss: 415.7453\n",
      "Training Epoch: 5 [46464/49669]\tLoss: 417.6976\n",
      "Training Epoch: 5 [46528/49669]\tLoss: 439.9250\n",
      "Training Epoch: 5 [46592/49669]\tLoss: 430.9828\n",
      "Training Epoch: 5 [46656/49669]\tLoss: 413.5888\n",
      "Training Epoch: 5 [46720/49669]\tLoss: 401.5290\n",
      "Training Epoch: 5 [46784/49669]\tLoss: 427.5963\n",
      "Training Epoch: 5 [46848/49669]\tLoss: 410.0078\n",
      "Training Epoch: 5 [46912/49669]\tLoss: 417.3315\n",
      "Training Epoch: 5 [46976/49669]\tLoss: 424.1883\n",
      "Training Epoch: 5 [47040/49669]\tLoss: 420.9637\n",
      "Training Epoch: 5 [47104/49669]\tLoss: 423.6707\n",
      "Training Epoch: 5 [47168/49669]\tLoss: 420.2481\n",
      "Training Epoch: 5 [47232/49669]\tLoss: 436.6750\n",
      "Training Epoch: 5 [47296/49669]\tLoss: 420.5013\n",
      "Training Epoch: 5 [47360/49669]\tLoss: 405.0970\n",
      "Training Epoch: 5 [47424/49669]\tLoss: 425.9386\n",
      "Training Epoch: 5 [47488/49669]\tLoss: 433.2636\n",
      "Training Epoch: 5 [47552/49669]\tLoss: 428.0811\n",
      "Training Epoch: 5 [47616/49669]\tLoss: 420.8964\n",
      "Training Epoch: 5 [47680/49669]\tLoss: 424.8069\n",
      "Training Epoch: 5 [47744/49669]\tLoss: 403.9235\n",
      "Training Epoch: 5 [47808/49669]\tLoss: 412.8658\n",
      "Training Epoch: 5 [47872/49669]\tLoss: 456.3625\n",
      "Training Epoch: 5 [47936/49669]\tLoss: 427.0300\n",
      "Training Epoch: 5 [48000/49669]\tLoss: 444.2133\n",
      "Training Epoch: 5 [48064/49669]\tLoss: 415.3309\n",
      "Training Epoch: 5 [48128/49669]\tLoss: 398.4223\n",
      "Training Epoch: 5 [48192/49669]\tLoss: 416.2753\n",
      "Training Epoch: 5 [48256/49669]\tLoss: 408.3155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [48320/49669]\tLoss: 404.5184\n",
      "Training Epoch: 5 [48384/49669]\tLoss: 397.0565\n",
      "Training Epoch: 5 [48448/49669]\tLoss: 432.8910\n",
      "Training Epoch: 5 [48512/49669]\tLoss: 430.5858\n",
      "Training Epoch: 5 [48576/49669]\tLoss: 427.5914\n",
      "Training Epoch: 5 [48640/49669]\tLoss: 423.7817\n",
      "Training Epoch: 5 [48704/49669]\tLoss: 418.3702\n",
      "Training Epoch: 5 [48768/49669]\tLoss: 453.7589\n",
      "Training Epoch: 5 [48832/49669]\tLoss: 400.5201\n",
      "Training Epoch: 5 [48896/49669]\tLoss: 403.7011\n",
      "Training Epoch: 5 [48960/49669]\tLoss: 416.9954\n",
      "Training Epoch: 5 [49024/49669]\tLoss: 425.7116\n",
      "Training Epoch: 5 [49088/49669]\tLoss: 394.1009\n",
      "Training Epoch: 5 [49152/49669]\tLoss: 435.7843\n",
      "Training Epoch: 5 [49216/49669]\tLoss: 422.3611\n",
      "Training Epoch: 5 [49280/49669]\tLoss: 422.2716\n",
      "Training Epoch: 5 [49344/49669]\tLoss: 422.0350\n",
      "Training Epoch: 5 [49408/49669]\tLoss: 431.3375\n",
      "Training Epoch: 5 [49472/49669]\tLoss: 427.8623\n",
      "Training Epoch: 5 [49536/49669]\tLoss: 422.2696\n",
      "Training Epoch: 5 [49600/49669]\tLoss: 419.5195\n",
      "Training Epoch: 5 [49664/49669]\tLoss: 405.8713\n",
      "Training Epoch: 5 [49669/49669]\tLoss: 372.7228\n",
      "Training Epoch: 5 [5519/5519]\tLoss: 421.3639\n",
      "Training Epoch: 6 [64/49669]\tLoss: 414.6259\n",
      "Training Epoch: 6 [128/49669]\tLoss: 398.1537\n",
      "Training Epoch: 6 [192/49669]\tLoss: 412.3549\n",
      "Training Epoch: 6 [256/49669]\tLoss: 406.0397\n",
      "Training Epoch: 6 [320/49669]\tLoss: 423.5140\n",
      "Training Epoch: 6 [384/49669]\tLoss: 396.6964\n",
      "Training Epoch: 6 [448/49669]\tLoss: 419.5575\n",
      "Training Epoch: 6 [512/49669]\tLoss: 405.2645\n",
      "Training Epoch: 6 [576/49669]\tLoss: 421.4609\n",
      "Training Epoch: 6 [640/49669]\tLoss: 431.8267\n",
      "Training Epoch: 6 [704/49669]\tLoss: 430.7586\n",
      "Training Epoch: 6 [768/49669]\tLoss: 448.9711\n",
      "Training Epoch: 6 [832/49669]\tLoss: 394.5882\n",
      "Training Epoch: 6 [896/49669]\tLoss: 393.2831\n",
      "Training Epoch: 6 [960/49669]\tLoss: 426.5733\n",
      "Training Epoch: 6 [1024/49669]\tLoss: 443.5506\n",
      "Training Epoch: 6 [1088/49669]\tLoss: 426.6234\n",
      "Training Epoch: 6 [1152/49669]\tLoss: 424.8546\n",
      "Training Epoch: 6 [1216/49669]\tLoss: 440.1963\n",
      "Training Epoch: 6 [1280/49669]\tLoss: 396.0359\n",
      "Training Epoch: 6 [1344/49669]\tLoss: 438.6626\n",
      "Training Epoch: 6 [1408/49669]\tLoss: 416.5617\n",
      "Training Epoch: 6 [1472/49669]\tLoss: 399.7717\n",
      "Training Epoch: 6 [1536/49669]\tLoss: 402.2252\n",
      "Training Epoch: 6 [1600/49669]\tLoss: 426.4208\n",
      "Training Epoch: 6 [1664/49669]\tLoss: 409.5697\n",
      "Training Epoch: 6 [1728/49669]\tLoss: 418.0336\n",
      "Training Epoch: 6 [1792/49669]\tLoss: 416.2720\n",
      "Training Epoch: 6 [1856/49669]\tLoss: 408.4627\n",
      "Training Epoch: 6 [1920/49669]\tLoss: 408.7142\n",
      "Training Epoch: 6 [1984/49669]\tLoss: 393.6642\n",
      "Training Epoch: 6 [2048/49669]\tLoss: 415.3034\n",
      "Training Epoch: 6 [2112/49669]\tLoss: 423.2233\n",
      "Training Epoch: 6 [2176/49669]\tLoss: 407.7465\n",
      "Training Epoch: 6 [2240/49669]\tLoss: 440.9473\n",
      "Training Epoch: 6 [2304/49669]\tLoss: 444.4906\n",
      "Training Epoch: 6 [2368/49669]\tLoss: 405.9403\n",
      "Training Epoch: 6 [2432/49669]\tLoss: 426.5250\n",
      "Training Epoch: 6 [2496/49669]\tLoss: 420.4849\n",
      "Training Epoch: 6 [2560/49669]\tLoss: 437.7859\n",
      "Training Epoch: 6 [2624/49669]\tLoss: 433.5858\n",
      "Training Epoch: 6 [2688/49669]\tLoss: 461.6092\n",
      "Training Epoch: 6 [2752/49669]\tLoss: 485.5465\n",
      "Training Epoch: 6 [2816/49669]\tLoss: 526.9539\n",
      "Training Epoch: 6 [2880/49669]\tLoss: 585.0610\n",
      "Training Epoch: 6 [2944/49669]\tLoss: 585.2010\n",
      "Training Epoch: 6 [3008/49669]\tLoss: 569.2225\n",
      "Training Epoch: 6 [3072/49669]\tLoss: 462.7909\n",
      "Training Epoch: 6 [3136/49669]\tLoss: 441.0157\n",
      "Training Epoch: 6 [3200/49669]\tLoss: 453.5320\n",
      "Training Epoch: 6 [3264/49669]\tLoss: 448.9330\n",
      "Training Epoch: 6 [3328/49669]\tLoss: 472.8010\n",
      "Training Epoch: 6 [3392/49669]\tLoss: 473.8573\n",
      "Training Epoch: 6 [3456/49669]\tLoss: 434.6757\n",
      "Training Epoch: 6 [3520/49669]\tLoss: 410.6870\n",
      "Training Epoch: 6 [3584/49669]\tLoss: 451.6047\n",
      "Training Epoch: 6 [3648/49669]\tLoss: 442.6592\n",
      "Training Epoch: 6 [3712/49669]\tLoss: 417.7452\n",
      "Training Epoch: 6 [3776/49669]\tLoss: 421.4236\n",
      "Training Epoch: 6 [3840/49669]\tLoss: 408.9424\n",
      "Training Epoch: 6 [3904/49669]\tLoss: 424.0406\n",
      "Training Epoch: 6 [3968/49669]\tLoss: 412.0084\n",
      "Training Epoch: 6 [4032/49669]\tLoss: 410.6003\n",
      "Training Epoch: 6 [4096/49669]\tLoss: 400.5793\n",
      "Training Epoch: 6 [4160/49669]\tLoss: 402.6711\n",
      "Training Epoch: 6 [4224/49669]\tLoss: 405.2479\n",
      "Training Epoch: 6 [4288/49669]\tLoss: 422.3958\n",
      "Training Epoch: 6 [4352/49669]\tLoss: 438.5468\n",
      "Training Epoch: 6 [4416/49669]\tLoss: 419.6290\n",
      "Training Epoch: 6 [4480/49669]\tLoss: 431.6582\n",
      "Training Epoch: 6 [4544/49669]\tLoss: 433.3752\n",
      "Training Epoch: 6 [4608/49669]\tLoss: 431.3894\n",
      "Training Epoch: 6 [4672/49669]\tLoss: 425.8475\n",
      "Training Epoch: 6 [4736/49669]\tLoss: 406.9174\n",
      "Training Epoch: 6 [4800/49669]\tLoss: 423.1126\n",
      "Training Epoch: 6 [4864/49669]\tLoss: 396.2441\n",
      "Training Epoch: 6 [4928/49669]\tLoss: 421.5714\n",
      "Training Epoch: 6 [4992/49669]\tLoss: 429.2535\n",
      "Training Epoch: 6 [5056/49669]\tLoss: 389.2632\n",
      "Training Epoch: 6 [5120/49669]\tLoss: 446.9818\n",
      "Training Epoch: 6 [5184/49669]\tLoss: 440.2111\n",
      "Training Epoch: 6 [5248/49669]\tLoss: 402.8176\n",
      "Training Epoch: 6 [5312/49669]\tLoss: 424.5651\n",
      "Training Epoch: 6 [5376/49669]\tLoss: 451.0624\n",
      "Training Epoch: 6 [5440/49669]\tLoss: 430.3364\n",
      "Training Epoch: 6 [5504/49669]\tLoss: 422.0774\n",
      "Training Epoch: 6 [5568/49669]\tLoss: 444.8825\n",
      "Training Epoch: 6 [5632/49669]\tLoss: 397.9392\n",
      "Training Epoch: 6 [5696/49669]\tLoss: 421.0051\n",
      "Training Epoch: 6 [5760/49669]\tLoss: 402.3657\n",
      "Training Epoch: 6 [5824/49669]\tLoss: 419.7569\n",
      "Training Epoch: 6 [5888/49669]\tLoss: 428.6991\n",
      "Training Epoch: 6 [5952/49669]\tLoss: 440.3037\n",
      "Training Epoch: 6 [6016/49669]\tLoss: 394.2134\n",
      "Training Epoch: 6 [6080/49669]\tLoss: 422.7608\n",
      "Training Epoch: 6 [6144/49669]\tLoss: 422.0947\n",
      "Training Epoch: 6 [6208/49669]\tLoss: 415.2650\n",
      "Training Epoch: 6 [6272/49669]\tLoss: 426.6077\n",
      "Training Epoch: 6 [6336/49669]\tLoss: 414.1478\n",
      "Training Epoch: 6 [6400/49669]\tLoss: 406.9474\n",
      "Training Epoch: 6 [6464/49669]\tLoss: 424.5208\n",
      "Training Epoch: 6 [6528/49669]\tLoss: 414.1354\n",
      "Training Epoch: 6 [6592/49669]\tLoss: 412.8149\n",
      "Training Epoch: 6 [6656/49669]\tLoss: 437.0385\n",
      "Training Epoch: 6 [6720/49669]\tLoss: 473.0508\n",
      "Training Epoch: 6 [6784/49669]\tLoss: 424.1100\n",
      "Training Epoch: 6 [6848/49669]\tLoss: 420.7297\n",
      "Training Epoch: 6 [6912/49669]\tLoss: 420.3552\n",
      "Training Epoch: 6 [6976/49669]\tLoss: 440.6210\n",
      "Training Epoch: 6 [7040/49669]\tLoss: 442.7383\n",
      "Training Epoch: 6 [7104/49669]\tLoss: 431.4329\n",
      "Training Epoch: 6 [7168/49669]\tLoss: 387.0926\n",
      "Training Epoch: 6 [7232/49669]\tLoss: 400.0046\n",
      "Training Epoch: 6 [7296/49669]\tLoss: 421.5454\n",
      "Training Epoch: 6 [7360/49669]\tLoss: 430.1613\n",
      "Training Epoch: 6 [7424/49669]\tLoss: 421.9564\n",
      "Training Epoch: 6 [7488/49669]\tLoss: 403.7727\n",
      "Training Epoch: 6 [7552/49669]\tLoss: 482.0730\n",
      "Training Epoch: 6 [7616/49669]\tLoss: 421.3968\n",
      "Training Epoch: 6 [7680/49669]\tLoss: 415.9121\n",
      "Training Epoch: 6 [7744/49669]\tLoss: 415.5121\n",
      "Training Epoch: 6 [7808/49669]\tLoss: 416.3990\n",
      "Training Epoch: 6 [7872/49669]\tLoss: 407.9801\n",
      "Training Epoch: 6 [7936/49669]\tLoss: 418.2585\n",
      "Training Epoch: 6 [8000/49669]\tLoss: 437.9784\n",
      "Training Epoch: 6 [8064/49669]\tLoss: 446.1065\n",
      "Training Epoch: 6 [8128/49669]\tLoss: 416.7440\n",
      "Training Epoch: 6 [8192/49669]\tLoss: 395.8096\n",
      "Training Epoch: 6 [8256/49669]\tLoss: 414.1377\n",
      "Training Epoch: 6 [8320/49669]\tLoss: 414.7488\n",
      "Training Epoch: 6 [8384/49669]\tLoss: 427.6594\n",
      "Training Epoch: 6 [8448/49669]\tLoss: 416.6750\n",
      "Training Epoch: 6 [8512/49669]\tLoss: 409.3085\n",
      "Training Epoch: 6 [8576/49669]\tLoss: 422.8013\n",
      "Training Epoch: 6 [8640/49669]\tLoss: 417.0076\n",
      "Training Epoch: 6 [8704/49669]\tLoss: 426.8708\n",
      "Training Epoch: 6 [8768/49669]\tLoss: 414.7822\n",
      "Training Epoch: 6 [8832/49669]\tLoss: 412.6549\n",
      "Training Epoch: 6 [8896/49669]\tLoss: 434.0384\n",
      "Training Epoch: 6 [8960/49669]\tLoss: 453.1817\n",
      "Training Epoch: 6 [9024/49669]\tLoss: 395.0287\n",
      "Training Epoch: 6 [9088/49669]\tLoss: 416.7147\n",
      "Training Epoch: 6 [9152/49669]\tLoss: 392.5695\n",
      "Training Epoch: 6 [9216/49669]\tLoss: 415.7529\n",
      "Training Epoch: 6 [9280/49669]\tLoss: 427.9297\n",
      "Training Epoch: 6 [9344/49669]\tLoss: 403.6982\n",
      "Training Epoch: 6 [9408/49669]\tLoss: 451.6702\n",
      "Training Epoch: 6 [9472/49669]\tLoss: 419.6479\n",
      "Training Epoch: 6 [9536/49669]\tLoss: 441.3886\n",
      "Training Epoch: 6 [9600/49669]\tLoss: 436.0293\n",
      "Training Epoch: 6 [9664/49669]\tLoss: 458.4058\n",
      "Training Epoch: 6 [9728/49669]\tLoss: 434.6267\n",
      "Training Epoch: 6 [9792/49669]\tLoss: 413.1261\n",
      "Training Epoch: 6 [9856/49669]\tLoss: 427.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [9920/49669]\tLoss: 429.7357\n",
      "Training Epoch: 6 [9984/49669]\tLoss: 418.6866\n",
      "Training Epoch: 6 [10048/49669]\tLoss: 426.1619\n",
      "Training Epoch: 6 [10112/49669]\tLoss: 458.0788\n",
      "Training Epoch: 6 [10176/49669]\tLoss: 429.9564\n",
      "Training Epoch: 6 [10240/49669]\tLoss: 457.0960\n",
      "Training Epoch: 6 [10304/49669]\tLoss: 421.0141\n",
      "Training Epoch: 6 [10368/49669]\tLoss: 447.0079\n",
      "Training Epoch: 6 [10432/49669]\tLoss: 433.6044\n",
      "Training Epoch: 6 [10496/49669]\tLoss: 431.2741\n",
      "Training Epoch: 6 [10560/49669]\tLoss: 388.6031\n",
      "Training Epoch: 6 [10624/49669]\tLoss: 406.7151\n",
      "Training Epoch: 6 [10688/49669]\tLoss: 409.9445\n",
      "Training Epoch: 6 [10752/49669]\tLoss: 432.7964\n",
      "Training Epoch: 6 [10816/49669]\tLoss: 396.2534\n",
      "Training Epoch: 6 [10880/49669]\tLoss: 393.4510\n",
      "Training Epoch: 6 [10944/49669]\tLoss: 390.7175\n",
      "Training Epoch: 6 [11008/49669]\tLoss: 426.4619\n",
      "Training Epoch: 6 [11072/49669]\tLoss: 435.3179\n",
      "Training Epoch: 6 [11136/49669]\tLoss: 420.9659\n",
      "Training Epoch: 6 [11200/49669]\tLoss: 432.5319\n",
      "Training Epoch: 6 [11264/49669]\tLoss: 422.4481\n",
      "Training Epoch: 6 [11328/49669]\tLoss: 426.2385\n",
      "Training Epoch: 6 [11392/49669]\tLoss: 432.5046\n",
      "Training Epoch: 6 [11456/49669]\tLoss: 406.4486\n",
      "Training Epoch: 6 [11520/49669]\tLoss: 407.8435\n",
      "Training Epoch: 6 [11584/49669]\tLoss: 418.2610\n",
      "Training Epoch: 6 [11648/49669]\tLoss: 440.1706\n",
      "Training Epoch: 6 [11712/49669]\tLoss: 413.3004\n",
      "Training Epoch: 6 [11776/49669]\tLoss: 396.7312\n",
      "Training Epoch: 6 [11840/49669]\tLoss: 444.7144\n",
      "Training Epoch: 6 [11904/49669]\tLoss: 453.3293\n",
      "Training Epoch: 6 [11968/49669]\tLoss: 442.3553\n",
      "Training Epoch: 6 [12032/49669]\tLoss: 386.0969\n",
      "Training Epoch: 6 [12096/49669]\tLoss: 425.2097\n",
      "Training Epoch: 6 [12160/49669]\tLoss: 414.5113\n",
      "Training Epoch: 6 [12224/49669]\tLoss: 396.2997\n",
      "Training Epoch: 6 [12288/49669]\tLoss: 442.5671\n",
      "Training Epoch: 6 [12352/49669]\tLoss: 435.2654\n",
      "Training Epoch: 6 [12416/49669]\tLoss: 412.9095\n",
      "Training Epoch: 6 [12480/49669]\tLoss: 404.7997\n",
      "Training Epoch: 6 [12544/49669]\tLoss: 403.5614\n",
      "Training Epoch: 6 [12608/49669]\tLoss: 408.9940\n",
      "Training Epoch: 6 [12672/49669]\tLoss: 421.3050\n",
      "Training Epoch: 6 [12736/49669]\tLoss: 413.2206\n",
      "Training Epoch: 6 [12800/49669]\tLoss: 426.8533\n",
      "Training Epoch: 6 [12864/49669]\tLoss: 425.1513\n",
      "Training Epoch: 6 [12928/49669]\tLoss: 421.8476\n",
      "Training Epoch: 6 [12992/49669]\tLoss: 431.2932\n",
      "Training Epoch: 6 [13056/49669]\tLoss: 408.0314\n",
      "Training Epoch: 6 [13120/49669]\tLoss: 423.1620\n",
      "Training Epoch: 6 [13184/49669]\tLoss: 406.2443\n",
      "Training Epoch: 6 [13248/49669]\tLoss: 407.0420\n",
      "Training Epoch: 6 [13312/49669]\tLoss: 424.2941\n",
      "Training Epoch: 6 [13376/49669]\tLoss: 429.4406\n",
      "Training Epoch: 6 [13440/49669]\tLoss: 408.8445\n",
      "Training Epoch: 6 [13504/49669]\tLoss: 423.7046\n",
      "Training Epoch: 6 [13568/49669]\tLoss: 396.3403\n",
      "Training Epoch: 6 [13632/49669]\tLoss: 427.9762\n",
      "Training Epoch: 6 [13696/49669]\tLoss: 406.6041\n",
      "Training Epoch: 6 [13760/49669]\tLoss: 400.3736\n",
      "Training Epoch: 6 [13824/49669]\tLoss: 409.9242\n",
      "Training Epoch: 6 [13888/49669]\tLoss: 418.5254\n",
      "Training Epoch: 6 [13952/49669]\tLoss: 418.2495\n",
      "Training Epoch: 6 [14016/49669]\tLoss: 406.0178\n",
      "Training Epoch: 6 [14080/49669]\tLoss: 418.0151\n",
      "Training Epoch: 6 [14144/49669]\tLoss: 407.0282\n",
      "Training Epoch: 6 [14208/49669]\tLoss: 446.3000\n",
      "Training Epoch: 6 [14272/49669]\tLoss: 404.8891\n",
      "Training Epoch: 6 [14336/49669]\tLoss: 367.3897\n",
      "Training Epoch: 6 [14400/49669]\tLoss: 415.8577\n",
      "Training Epoch: 6 [14464/49669]\tLoss: 418.4868\n",
      "Training Epoch: 6 [14528/49669]\tLoss: 422.5092\n",
      "Training Epoch: 6 [14592/49669]\tLoss: 425.1320\n",
      "Training Epoch: 6 [14656/49669]\tLoss: 391.9314\n",
      "Training Epoch: 6 [14720/49669]\tLoss: 373.4477\n",
      "Training Epoch: 6 [14784/49669]\tLoss: 431.7528\n",
      "Training Epoch: 6 [14848/49669]\tLoss: 436.3278\n",
      "Training Epoch: 6 [14912/49669]\tLoss: 418.0005\n",
      "Training Epoch: 6 [14976/49669]\tLoss: 448.9144\n",
      "Training Epoch: 6 [15040/49669]\tLoss: 404.2017\n",
      "Training Epoch: 6 [15104/49669]\tLoss: 396.1258\n",
      "Training Epoch: 6 [15168/49669]\tLoss: 374.4485\n",
      "Training Epoch: 6 [15232/49669]\tLoss: 424.9006\n",
      "Training Epoch: 6 [15296/49669]\tLoss: 419.8298\n",
      "Training Epoch: 6 [15360/49669]\tLoss: 412.9724\n",
      "Training Epoch: 6 [15424/49669]\tLoss: 413.1152\n",
      "Training Epoch: 6 [15488/49669]\tLoss: 399.0046\n",
      "Training Epoch: 6 [15552/49669]\tLoss: 437.6842\n",
      "Training Epoch: 6 [15616/49669]\tLoss: 441.7484\n",
      "Training Epoch: 6 [15680/49669]\tLoss: 432.5406\n",
      "Training Epoch: 6 [15744/49669]\tLoss: 375.0008\n",
      "Training Epoch: 6 [15808/49669]\tLoss: 388.1571\n",
      "Training Epoch: 6 [15872/49669]\tLoss: 397.7373\n",
      "Training Epoch: 6 [15936/49669]\tLoss: 442.5690\n",
      "Training Epoch: 6 [16000/49669]\tLoss: 422.8214\n",
      "Training Epoch: 6 [16064/49669]\tLoss: 413.8461\n",
      "Training Epoch: 6 [16128/49669]\tLoss: 410.6469\n",
      "Training Epoch: 6 [16192/49669]\tLoss: 433.7969\n",
      "Training Epoch: 6 [16256/49669]\tLoss: 413.6156\n",
      "Training Epoch: 6 [16320/49669]\tLoss: 428.9154\n",
      "Training Epoch: 6 [16384/49669]\tLoss: 399.5623\n",
      "Training Epoch: 6 [16448/49669]\tLoss: 418.2043\n",
      "Training Epoch: 6 [16512/49669]\tLoss: 410.6768\n",
      "Training Epoch: 6 [16576/49669]\tLoss: 430.7045\n",
      "Training Epoch: 6 [16640/49669]\tLoss: 405.7492\n",
      "Training Epoch: 6 [16704/49669]\tLoss: 413.5555\n",
      "Training Epoch: 6 [16768/49669]\tLoss: 443.2491\n",
      "Training Epoch: 6 [16832/49669]\tLoss: 446.2276\n",
      "Training Epoch: 6 [16896/49669]\tLoss: 427.0595\n",
      "Training Epoch: 6 [16960/49669]\tLoss: 424.9731\n",
      "Training Epoch: 6 [17024/49669]\tLoss: 429.5495\n",
      "Training Epoch: 6 [17088/49669]\tLoss: 403.5271\n",
      "Training Epoch: 6 [17152/49669]\tLoss: 418.9452\n",
      "Training Epoch: 6 [17216/49669]\tLoss: 423.2400\n",
      "Training Epoch: 6 [17280/49669]\tLoss: 414.1695\n",
      "Training Epoch: 6 [17344/49669]\tLoss: 413.1031\n",
      "Training Epoch: 6 [17408/49669]\tLoss: 448.4192\n",
      "Training Epoch: 6 [17472/49669]\tLoss: 425.2997\n",
      "Training Epoch: 6 [17536/49669]\tLoss: 402.7349\n",
      "Training Epoch: 6 [17600/49669]\tLoss: 446.9809\n",
      "Training Epoch: 6 [17664/49669]\tLoss: 400.6014\n",
      "Training Epoch: 6 [17728/49669]\tLoss: 420.2757\n",
      "Training Epoch: 6 [17792/49669]\tLoss: 427.0873\n",
      "Training Epoch: 6 [17856/49669]\tLoss: 422.6025\n",
      "Training Epoch: 6 [17920/49669]\tLoss: 390.7075\n",
      "Training Epoch: 6 [17984/49669]\tLoss: 441.7777\n",
      "Training Epoch: 6 [18048/49669]\tLoss: 428.8326\n",
      "Training Epoch: 6 [18112/49669]\tLoss: 414.7245\n",
      "Training Epoch: 6 [18176/49669]\tLoss: 425.7970\n",
      "Training Epoch: 6 [18240/49669]\tLoss: 398.3462\n",
      "Training Epoch: 6 [18304/49669]\tLoss: 441.0214\n",
      "Training Epoch: 6 [18368/49669]\tLoss: 457.5694\n",
      "Training Epoch: 6 [18432/49669]\tLoss: 415.5240\n",
      "Training Epoch: 6 [18496/49669]\tLoss: 439.8357\n",
      "Training Epoch: 6 [18560/49669]\tLoss: 436.5468\n",
      "Training Epoch: 6 [18624/49669]\tLoss: 404.0234\n",
      "Training Epoch: 6 [18688/49669]\tLoss: 406.5087\n",
      "Training Epoch: 6 [18752/49669]\tLoss: 432.3498\n",
      "Training Epoch: 6 [18816/49669]\tLoss: 400.3835\n",
      "Training Epoch: 6 [18880/49669]\tLoss: 399.1812\n",
      "Training Epoch: 6 [18944/49669]\tLoss: 432.1252\n",
      "Training Epoch: 6 [19008/49669]\tLoss: 409.1002\n",
      "Training Epoch: 6 [19072/49669]\tLoss: 432.0234\n",
      "Training Epoch: 6 [19136/49669]\tLoss: 413.4140\n",
      "Training Epoch: 6 [19200/49669]\tLoss: 431.3870\n",
      "Training Epoch: 6 [19264/49669]\tLoss: 413.6750\n",
      "Training Epoch: 6 [19328/49669]\tLoss: 390.8157\n",
      "Training Epoch: 6 [19392/49669]\tLoss: 437.5236\n",
      "Training Epoch: 6 [19456/49669]\tLoss: 399.2122\n",
      "Training Epoch: 6 [19520/49669]\tLoss: 413.6841\n",
      "Training Epoch: 6 [19584/49669]\tLoss: 424.5760\n",
      "Training Epoch: 6 [19648/49669]\tLoss: 422.9318\n",
      "Training Epoch: 6 [19712/49669]\tLoss: 422.1039\n",
      "Training Epoch: 6 [19776/49669]\tLoss: 443.7943\n",
      "Training Epoch: 6 [19840/49669]\tLoss: 445.7189\n",
      "Training Epoch: 6 [19904/49669]\tLoss: 431.1529\n",
      "Training Epoch: 6 [19968/49669]\tLoss: 388.1552\n",
      "Training Epoch: 6 [20032/49669]\tLoss: 442.5567\n",
      "Training Epoch: 6 [20096/49669]\tLoss: 410.7672\n",
      "Training Epoch: 6 [20160/49669]\tLoss: 411.5475\n",
      "Training Epoch: 6 [20224/49669]\tLoss: 405.4474\n",
      "Training Epoch: 6 [20288/49669]\tLoss: 405.9408\n",
      "Training Epoch: 6 [20352/49669]\tLoss: 433.4945\n",
      "Training Epoch: 6 [20416/49669]\tLoss: 398.1563\n",
      "Training Epoch: 6 [20480/49669]\tLoss: 419.6771\n",
      "Training Epoch: 6 [20544/49669]\tLoss: 405.0565\n",
      "Training Epoch: 6 [20608/49669]\tLoss: 440.2720\n",
      "Training Epoch: 6 [20672/49669]\tLoss: 415.4959\n",
      "Training Epoch: 6 [20736/49669]\tLoss: 432.5309\n",
      "Training Epoch: 6 [20800/49669]\tLoss: 432.2645\n",
      "Training Epoch: 6 [20864/49669]\tLoss: 439.4365\n",
      "Training Epoch: 6 [20928/49669]\tLoss: 391.0805\n",
      "Training Epoch: 6 [20992/49669]\tLoss: 433.2315\n",
      "Training Epoch: 6 [21056/49669]\tLoss: 402.8879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [21120/49669]\tLoss: 429.9407\n",
      "Training Epoch: 6 [21184/49669]\tLoss: 406.5209\n",
      "Training Epoch: 6 [21248/49669]\tLoss: 450.6456\n",
      "Training Epoch: 6 [21312/49669]\tLoss: 407.9587\n",
      "Training Epoch: 6 [21376/49669]\tLoss: 414.9124\n",
      "Training Epoch: 6 [21440/49669]\tLoss: 430.1997\n",
      "Training Epoch: 6 [21504/49669]\tLoss: 436.3399\n",
      "Training Epoch: 6 [21568/49669]\tLoss: 414.3790\n",
      "Training Epoch: 6 [21632/49669]\tLoss: 434.6192\n",
      "Training Epoch: 6 [21696/49669]\tLoss: 412.5348\n",
      "Training Epoch: 6 [21760/49669]\tLoss: 405.2521\n",
      "Training Epoch: 6 [21824/49669]\tLoss: 441.4174\n",
      "Training Epoch: 6 [21888/49669]\tLoss: 411.4944\n",
      "Training Epoch: 6 [21952/49669]\tLoss: 417.5153\n",
      "Training Epoch: 6 [22016/49669]\tLoss: 400.4675\n",
      "Training Epoch: 6 [22080/49669]\tLoss: 411.5308\n",
      "Training Epoch: 6 [22144/49669]\tLoss: 412.8715\n",
      "Training Epoch: 6 [22208/49669]\tLoss: 444.8074\n",
      "Training Epoch: 6 [22272/49669]\tLoss: 406.1911\n",
      "Training Epoch: 6 [22336/49669]\tLoss: 397.7199\n",
      "Training Epoch: 6 [22400/49669]\tLoss: 441.6634\n",
      "Training Epoch: 6 [22464/49669]\tLoss: 414.0483\n",
      "Training Epoch: 6 [22528/49669]\tLoss: 462.3680\n",
      "Training Epoch: 6 [22592/49669]\tLoss: 458.6604\n",
      "Training Epoch: 6 [22656/49669]\tLoss: 471.7515\n",
      "Training Epoch: 6 [22720/49669]\tLoss: 479.9591\n",
      "Training Epoch: 6 [22784/49669]\tLoss: 412.2273\n",
      "Training Epoch: 6 [22848/49669]\tLoss: 441.9362\n",
      "Training Epoch: 6 [22912/49669]\tLoss: 413.2051\n",
      "Training Epoch: 6 [22976/49669]\tLoss: 441.8344\n",
      "Training Epoch: 6 [23040/49669]\tLoss: 406.5995\n",
      "Training Epoch: 6 [23104/49669]\tLoss: 448.5120\n",
      "Training Epoch: 6 [23168/49669]\tLoss: 410.4231\n",
      "Training Epoch: 6 [23232/49669]\tLoss: 395.9517\n",
      "Training Epoch: 6 [23296/49669]\tLoss: 425.8406\n",
      "Training Epoch: 6 [23360/49669]\tLoss: 406.5262\n",
      "Training Epoch: 6 [23424/49669]\tLoss: 421.7151\n",
      "Training Epoch: 6 [23488/49669]\tLoss: 432.0311\n",
      "Training Epoch: 6 [23552/49669]\tLoss: 462.7643\n",
      "Training Epoch: 6 [23616/49669]\tLoss: 432.2640\n",
      "Training Epoch: 6 [23680/49669]\tLoss: 428.5511\n",
      "Training Epoch: 6 [23744/49669]\tLoss: 437.1477\n",
      "Training Epoch: 6 [23808/49669]\tLoss: 472.1369\n",
      "Training Epoch: 6 [23872/49669]\tLoss: 401.1832\n",
      "Training Epoch: 6 [23936/49669]\tLoss: 396.6796\n",
      "Training Epoch: 6 [24000/49669]\tLoss: 429.0810\n",
      "Training Epoch: 6 [24064/49669]\tLoss: 408.0919\n",
      "Training Epoch: 6 [24128/49669]\tLoss: 428.1355\n",
      "Training Epoch: 6 [24192/49669]\tLoss: 432.2100\n",
      "Training Epoch: 6 [24256/49669]\tLoss: 447.4680\n",
      "Training Epoch: 6 [24320/49669]\tLoss: 435.3965\n",
      "Training Epoch: 6 [24384/49669]\tLoss: 425.8607\n",
      "Training Epoch: 6 [24448/49669]\tLoss: 423.6822\n",
      "Training Epoch: 6 [24512/49669]\tLoss: 427.2292\n",
      "Training Epoch: 6 [24576/49669]\tLoss: 436.7593\n",
      "Training Epoch: 6 [24640/49669]\tLoss: 422.7606\n",
      "Training Epoch: 6 [24704/49669]\tLoss: 399.3545\n",
      "Training Epoch: 6 [24768/49669]\tLoss: 452.5143\n",
      "Training Epoch: 6 [24832/49669]\tLoss: 408.3815\n",
      "Training Epoch: 6 [24896/49669]\tLoss: 437.6657\n",
      "Training Epoch: 6 [24960/49669]\tLoss: 398.4373\n",
      "Training Epoch: 6 [25024/49669]\tLoss: 467.6349\n",
      "Training Epoch: 6 [25088/49669]\tLoss: 422.9718\n",
      "Training Epoch: 6 [25152/49669]\tLoss: 418.1997\n",
      "Training Epoch: 6 [25216/49669]\tLoss: 412.0409\n",
      "Training Epoch: 6 [25280/49669]\tLoss: 416.3386\n",
      "Training Epoch: 6 [25344/49669]\tLoss: 437.1420\n",
      "Training Epoch: 6 [25408/49669]\tLoss: 417.3333\n",
      "Training Epoch: 6 [25472/49669]\tLoss: 401.9468\n",
      "Training Epoch: 6 [25536/49669]\tLoss: 429.8178\n",
      "Training Epoch: 6 [25600/49669]\tLoss: 439.4755\n",
      "Training Epoch: 6 [25664/49669]\tLoss: 404.9884\n",
      "Training Epoch: 6 [25728/49669]\tLoss: 416.5739\n",
      "Training Epoch: 6 [25792/49669]\tLoss: 429.2003\n",
      "Training Epoch: 6 [25856/49669]\tLoss: 434.3579\n",
      "Training Epoch: 6 [25920/49669]\tLoss: 431.6019\n",
      "Training Epoch: 6 [25984/49669]\tLoss: 406.8094\n",
      "Training Epoch: 6 [26048/49669]\tLoss: 435.1812\n",
      "Training Epoch: 6 [26112/49669]\tLoss: 430.0118\n",
      "Training Epoch: 6 [26176/49669]\tLoss: 382.5461\n",
      "Training Epoch: 6 [26240/49669]\tLoss: 442.1026\n",
      "Training Epoch: 6 [26304/49669]\tLoss: 405.8809\n",
      "Training Epoch: 6 [26368/49669]\tLoss: 417.0872\n",
      "Training Epoch: 6 [26432/49669]\tLoss: 419.3334\n",
      "Training Epoch: 6 [26496/49669]\tLoss: 452.7395\n",
      "Training Epoch: 6 [26560/49669]\tLoss: 414.3606\n",
      "Training Epoch: 6 [26624/49669]\tLoss: 420.9592\n",
      "Training Epoch: 6 [26688/49669]\tLoss: 403.6093\n",
      "Training Epoch: 6 [26752/49669]\tLoss: 404.3145\n",
      "Training Epoch: 6 [26816/49669]\tLoss: 439.7718\n",
      "Training Epoch: 6 [26880/49669]\tLoss: 429.5523\n",
      "Training Epoch: 6 [26944/49669]\tLoss: 393.4818\n",
      "Training Epoch: 6 [27008/49669]\tLoss: 413.3832\n",
      "Training Epoch: 6 [27072/49669]\tLoss: 398.1794\n",
      "Training Epoch: 6 [27136/49669]\tLoss: 421.7041\n",
      "Training Epoch: 6 [27200/49669]\tLoss: 382.8229\n",
      "Training Epoch: 6 [27264/49669]\tLoss: 392.5559\n",
      "Training Epoch: 6 [27328/49669]\tLoss: 411.1101\n",
      "Training Epoch: 6 [27392/49669]\tLoss: 409.7350\n",
      "Training Epoch: 6 [27456/49669]\tLoss: 445.9124\n",
      "Training Epoch: 6 [27520/49669]\tLoss: 391.0933\n",
      "Training Epoch: 6 [27584/49669]\tLoss: 433.3622\n",
      "Training Epoch: 6 [27648/49669]\tLoss: 444.1942\n",
      "Training Epoch: 6 [27712/49669]\tLoss: 434.2638\n",
      "Training Epoch: 6 [27776/49669]\tLoss: 425.0744\n",
      "Training Epoch: 6 [27840/49669]\tLoss: 357.7311\n",
      "Training Epoch: 6 [27904/49669]\tLoss: 424.0655\n",
      "Training Epoch: 6 [27968/49669]\tLoss: 386.1532\n",
      "Training Epoch: 6 [28032/49669]\tLoss: 418.3952\n",
      "Training Epoch: 6 [28096/49669]\tLoss: 399.9775\n",
      "Training Epoch: 6 [28160/49669]\tLoss: 425.4789\n",
      "Training Epoch: 6 [28224/49669]\tLoss: 404.8138\n",
      "Training Epoch: 6 [28288/49669]\tLoss: 400.4514\n",
      "Training Epoch: 6 [28352/49669]\tLoss: 396.2011\n",
      "Training Epoch: 6 [28416/49669]\tLoss: 422.0265\n",
      "Training Epoch: 6 [28480/49669]\tLoss: 416.5749\n",
      "Training Epoch: 6 [28544/49669]\tLoss: 400.0097\n",
      "Training Epoch: 6 [28608/49669]\tLoss: 426.4854\n",
      "Training Epoch: 6 [28672/49669]\tLoss: 448.5317\n",
      "Training Epoch: 6 [28736/49669]\tLoss: 414.6676\n",
      "Training Epoch: 6 [28800/49669]\tLoss: 392.1656\n",
      "Training Epoch: 6 [28864/49669]\tLoss: 427.6537\n",
      "Training Epoch: 6 [28928/49669]\tLoss: 422.6022\n",
      "Training Epoch: 6 [28992/49669]\tLoss: 388.9168\n",
      "Training Epoch: 6 [29056/49669]\tLoss: 395.5195\n",
      "Training Epoch: 6 [29120/49669]\tLoss: 419.5352\n",
      "Training Epoch: 6 [29184/49669]\tLoss: 401.5232\n",
      "Training Epoch: 6 [29248/49669]\tLoss: 414.1932\n",
      "Training Epoch: 6 [29312/49669]\tLoss: 432.4739\n",
      "Training Epoch: 6 [29376/49669]\tLoss: 466.6718\n",
      "Training Epoch: 6 [29440/49669]\tLoss: 425.1834\n",
      "Training Epoch: 6 [29504/49669]\tLoss: 443.1580\n",
      "Training Epoch: 6 [29568/49669]\tLoss: 401.7821\n",
      "Training Epoch: 6 [29632/49669]\tLoss: 417.7578\n",
      "Training Epoch: 6 [29696/49669]\tLoss: 452.5663\n",
      "Training Epoch: 6 [29760/49669]\tLoss: 406.4126\n",
      "Training Epoch: 6 [29824/49669]\tLoss: 393.9119\n",
      "Training Epoch: 6 [29888/49669]\tLoss: 404.4162\n",
      "Training Epoch: 6 [29952/49669]\tLoss: 429.1366\n",
      "Training Epoch: 6 [30016/49669]\tLoss: 429.6042\n",
      "Training Epoch: 6 [30080/49669]\tLoss: 420.0849\n",
      "Training Epoch: 6 [30144/49669]\tLoss: 450.8497\n",
      "Training Epoch: 6 [30208/49669]\tLoss: 422.1786\n",
      "Training Epoch: 6 [30272/49669]\tLoss: 423.0172\n",
      "Training Epoch: 6 [30336/49669]\tLoss: 443.6382\n",
      "Training Epoch: 6 [30400/49669]\tLoss: 425.5209\n",
      "Training Epoch: 6 [30464/49669]\tLoss: 390.4729\n",
      "Training Epoch: 6 [30528/49669]\tLoss: 406.1235\n",
      "Training Epoch: 6 [30592/49669]\tLoss: 436.7449\n",
      "Training Epoch: 6 [30656/49669]\tLoss: 408.6897\n",
      "Training Epoch: 6 [30720/49669]\tLoss: 417.4576\n",
      "Training Epoch: 6 [30784/49669]\tLoss: 442.6661\n",
      "Training Epoch: 6 [30848/49669]\tLoss: 440.2561\n",
      "Training Epoch: 6 [30912/49669]\tLoss: 417.2438\n",
      "Training Epoch: 6 [30976/49669]\tLoss: 434.9585\n",
      "Training Epoch: 6 [31040/49669]\tLoss: 437.8925\n",
      "Training Epoch: 6 [31104/49669]\tLoss: 411.4532\n",
      "Training Epoch: 6 [31168/49669]\tLoss: 441.8325\n",
      "Training Epoch: 6 [31232/49669]\tLoss: 406.6078\n",
      "Training Epoch: 6 [31296/49669]\tLoss: 486.7521\n",
      "Training Epoch: 6 [31360/49669]\tLoss: 437.1828\n",
      "Training Epoch: 6 [31424/49669]\tLoss: 442.0875\n",
      "Training Epoch: 6 [31488/49669]\tLoss: 433.5482\n",
      "Training Epoch: 6 [31552/49669]\tLoss: 424.6046\n",
      "Training Epoch: 6 [31616/49669]\tLoss: 416.3244\n",
      "Training Epoch: 6 [31680/49669]\tLoss: 434.3578\n",
      "Training Epoch: 6 [31744/49669]\tLoss: 413.4226\n",
      "Training Epoch: 6 [31808/49669]\tLoss: 403.5756\n",
      "Training Epoch: 6 [31872/49669]\tLoss: 385.2869\n",
      "Training Epoch: 6 [31936/49669]\tLoss: 427.5081\n",
      "Training Epoch: 6 [32000/49669]\tLoss: 423.6169\n",
      "Training Epoch: 6 [32064/49669]\tLoss: 437.3429\n",
      "Training Epoch: 6 [32128/49669]\tLoss: 412.0516\n",
      "Training Epoch: 6 [32192/49669]\tLoss: 427.7828\n",
      "Training Epoch: 6 [32256/49669]\tLoss: 404.3939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [32320/49669]\tLoss: 423.7437\n",
      "Training Epoch: 6 [32384/49669]\tLoss: 401.6890\n",
      "Training Epoch: 6 [32448/49669]\tLoss: 421.1482\n",
      "Training Epoch: 6 [32512/49669]\tLoss: 402.7161\n",
      "Training Epoch: 6 [32576/49669]\tLoss: 438.8604\n",
      "Training Epoch: 6 [32640/49669]\tLoss: 402.1949\n",
      "Training Epoch: 6 [32704/49669]\tLoss: 385.1666\n",
      "Training Epoch: 6 [32768/49669]\tLoss: 417.1692\n",
      "Training Epoch: 6 [32832/49669]\tLoss: 411.5882\n",
      "Training Epoch: 6 [32896/49669]\tLoss: 413.2045\n",
      "Training Epoch: 6 [32960/49669]\tLoss: 405.3083\n",
      "Training Epoch: 6 [33024/49669]\tLoss: 420.7704\n",
      "Training Epoch: 6 [33088/49669]\tLoss: 406.3889\n",
      "Training Epoch: 6 [33152/49669]\tLoss: 433.7349\n",
      "Training Epoch: 6 [33216/49669]\tLoss: 441.2629\n",
      "Training Epoch: 6 [33280/49669]\tLoss: 399.2471\n",
      "Training Epoch: 6 [33344/49669]\tLoss: 425.4824\n",
      "Training Epoch: 6 [33408/49669]\tLoss: 464.8709\n",
      "Training Epoch: 6 [33472/49669]\tLoss: 418.4882\n",
      "Training Epoch: 6 [33536/49669]\tLoss: 428.3620\n",
      "Training Epoch: 6 [33600/49669]\tLoss: 434.8394\n",
      "Training Epoch: 6 [33664/49669]\tLoss: 391.2802\n",
      "Training Epoch: 6 [33728/49669]\tLoss: 418.4177\n",
      "Training Epoch: 6 [33792/49669]\tLoss: 416.4726\n",
      "Training Epoch: 6 [33856/49669]\tLoss: 412.4607\n",
      "Training Epoch: 6 [33920/49669]\tLoss: 432.8626\n",
      "Training Epoch: 6 [33984/49669]\tLoss: 404.2879\n",
      "Training Epoch: 6 [34048/49669]\tLoss: 397.2402\n",
      "Training Epoch: 6 [34112/49669]\tLoss: 438.6990\n",
      "Training Epoch: 6 [34176/49669]\tLoss: 412.9864\n",
      "Training Epoch: 6 [34240/49669]\tLoss: 414.2028\n",
      "Training Epoch: 6 [34304/49669]\tLoss: 430.0904\n",
      "Training Epoch: 6 [34368/49669]\tLoss: 425.7505\n",
      "Training Epoch: 6 [34432/49669]\tLoss: 426.4056\n",
      "Training Epoch: 6 [34496/49669]\tLoss: 427.0618\n",
      "Training Epoch: 6 [34560/49669]\tLoss: 397.2000\n",
      "Training Epoch: 6 [34624/49669]\tLoss: 425.8252\n",
      "Training Epoch: 6 [34688/49669]\tLoss: 420.2277\n",
      "Training Epoch: 6 [34752/49669]\tLoss: 422.3421\n",
      "Training Epoch: 6 [34816/49669]\tLoss: 429.6591\n",
      "Training Epoch: 6 [34880/49669]\tLoss: 408.2335\n",
      "Training Epoch: 6 [34944/49669]\tLoss: 405.8522\n",
      "Training Epoch: 6 [35008/49669]\tLoss: 484.8125\n",
      "Training Epoch: 6 [35072/49669]\tLoss: 400.4789\n",
      "Training Epoch: 6 [35136/49669]\tLoss: 384.8230\n",
      "Training Epoch: 6 [35200/49669]\tLoss: 403.7146\n",
      "Training Epoch: 6 [35264/49669]\tLoss: 408.4944\n",
      "Training Epoch: 6 [35328/49669]\tLoss: 405.5104\n",
      "Training Epoch: 6 [35392/49669]\tLoss: 426.8523\n",
      "Training Epoch: 6 [35456/49669]\tLoss: 420.0238\n",
      "Training Epoch: 6 [35520/49669]\tLoss: 428.6884\n",
      "Training Epoch: 6 [35584/49669]\tLoss: 409.5484\n",
      "Training Epoch: 6 [35648/49669]\tLoss: 433.5499\n",
      "Training Epoch: 6 [35712/49669]\tLoss: 440.3439\n",
      "Training Epoch: 6 [35776/49669]\tLoss: 427.8354\n",
      "Training Epoch: 6 [35840/49669]\tLoss: 421.3782\n",
      "Training Epoch: 6 [35904/49669]\tLoss: 406.0824\n",
      "Training Epoch: 6 [35968/49669]\tLoss: 412.3191\n",
      "Training Epoch: 6 [36032/49669]\tLoss: 384.8079\n",
      "Training Epoch: 6 [36096/49669]\tLoss: 402.5116\n",
      "Training Epoch: 6 [36160/49669]\tLoss: 416.2955\n",
      "Training Epoch: 6 [36224/49669]\tLoss: 407.4354\n",
      "Training Epoch: 6 [36288/49669]\tLoss: 413.0855\n",
      "Training Epoch: 6 [36352/49669]\tLoss: 425.2762\n",
      "Training Epoch: 6 [36416/49669]\tLoss: 405.6507\n",
      "Training Epoch: 6 [36480/49669]\tLoss: 406.1986\n",
      "Training Epoch: 6 [36544/49669]\tLoss: 421.2452\n",
      "Training Epoch: 6 [36608/49669]\tLoss: 416.1137\n",
      "Training Epoch: 6 [36672/49669]\tLoss: 395.5489\n",
      "Training Epoch: 6 [36736/49669]\tLoss: 421.8503\n",
      "Training Epoch: 6 [36800/49669]\tLoss: 394.0498\n",
      "Training Epoch: 6 [36864/49669]\tLoss: 443.6344\n",
      "Training Epoch: 6 [36928/49669]\tLoss: 427.0034\n",
      "Training Epoch: 6 [36992/49669]\tLoss: 382.2715\n",
      "Training Epoch: 6 [37056/49669]\tLoss: 439.2268\n",
      "Training Epoch: 6 [37120/49669]\tLoss: 417.5710\n",
      "Training Epoch: 6 [37184/49669]\tLoss: 407.0455\n",
      "Training Epoch: 6 [37248/49669]\tLoss: 403.7755\n",
      "Training Epoch: 6 [37312/49669]\tLoss: 425.5204\n",
      "Training Epoch: 6 [37376/49669]\tLoss: 413.6580\n",
      "Training Epoch: 6 [37440/49669]\tLoss: 421.6817\n",
      "Training Epoch: 6 [37504/49669]\tLoss: 409.2714\n",
      "Training Epoch: 6 [37568/49669]\tLoss: 441.4077\n",
      "Training Epoch: 6 [37632/49669]\tLoss: 454.5928\n",
      "Training Epoch: 6 [37696/49669]\tLoss: 450.4233\n",
      "Training Epoch: 6 [37760/49669]\tLoss: 414.9754\n",
      "Training Epoch: 6 [37824/49669]\tLoss: 444.7985\n",
      "Training Epoch: 6 [37888/49669]\tLoss: 396.8531\n",
      "Training Epoch: 6 [37952/49669]\tLoss: 433.3172\n",
      "Training Epoch: 6 [38016/49669]\tLoss: 420.5131\n",
      "Training Epoch: 6 [38080/49669]\tLoss: 415.5753\n",
      "Training Epoch: 6 [38144/49669]\tLoss: 458.6702\n",
      "Training Epoch: 6 [38208/49669]\tLoss: 433.6609\n",
      "Training Epoch: 6 [38272/49669]\tLoss: 453.9048\n",
      "Training Epoch: 6 [38336/49669]\tLoss: 395.9919\n",
      "Training Epoch: 6 [38400/49669]\tLoss: 435.9976\n",
      "Training Epoch: 6 [38464/49669]\tLoss: 421.5469\n",
      "Training Epoch: 6 [38528/49669]\tLoss: 420.3297\n",
      "Training Epoch: 6 [38592/49669]\tLoss: 444.7079\n",
      "Training Epoch: 6 [38656/49669]\tLoss: 414.8341\n",
      "Training Epoch: 6 [38720/49669]\tLoss: 428.9164\n",
      "Training Epoch: 6 [38784/49669]\tLoss: 440.8201\n",
      "Training Epoch: 6 [38848/49669]\tLoss: 409.7346\n",
      "Training Epoch: 6 [38912/49669]\tLoss: 413.5079\n",
      "Training Epoch: 6 [38976/49669]\tLoss: 351.8671\n",
      "Training Epoch: 6 [39040/49669]\tLoss: 419.6779\n",
      "Training Epoch: 6 [39104/49669]\tLoss: 417.6876\n",
      "Training Epoch: 6 [39168/49669]\tLoss: 419.0149\n",
      "Training Epoch: 6 [39232/49669]\tLoss: 432.2948\n",
      "Training Epoch: 6 [39296/49669]\tLoss: 402.8658\n",
      "Training Epoch: 6 [39360/49669]\tLoss: 413.7925\n",
      "Training Epoch: 6 [39424/49669]\tLoss: 438.4763\n",
      "Training Epoch: 6 [39488/49669]\tLoss: 441.0413\n",
      "Training Epoch: 6 [39552/49669]\tLoss: 404.0383\n",
      "Training Epoch: 6 [39616/49669]\tLoss: 426.2034\n",
      "Training Epoch: 6 [39680/49669]\tLoss: 418.6653\n",
      "Training Epoch: 6 [39744/49669]\tLoss: 465.9745\n",
      "Training Epoch: 6 [39808/49669]\tLoss: 426.3329\n",
      "Training Epoch: 6 [39872/49669]\tLoss: 418.4445\n",
      "Training Epoch: 6 [39936/49669]\tLoss: 428.7343\n",
      "Training Epoch: 6 [40000/49669]\tLoss: 432.7104\n",
      "Training Epoch: 6 [40064/49669]\tLoss: 428.2198\n",
      "Training Epoch: 6 [40128/49669]\tLoss: 446.0602\n",
      "Training Epoch: 6 [40192/49669]\tLoss: 418.9342\n",
      "Training Epoch: 6 [40256/49669]\tLoss: 431.3068\n",
      "Training Epoch: 6 [40320/49669]\tLoss: 411.7479\n",
      "Training Epoch: 6 [40384/49669]\tLoss: 450.2013\n",
      "Training Epoch: 6 [40448/49669]\tLoss: 410.3801\n",
      "Training Epoch: 6 [40512/49669]\tLoss: 425.0207\n",
      "Training Epoch: 6 [40576/49669]\tLoss: 399.5590\n",
      "Training Epoch: 6 [40640/49669]\tLoss: 433.5602\n",
      "Training Epoch: 6 [40704/49669]\tLoss: 421.9634\n",
      "Training Epoch: 6 [40768/49669]\tLoss: 434.3003\n",
      "Training Epoch: 6 [40832/49669]\tLoss: 435.9104\n",
      "Training Epoch: 6 [40896/49669]\tLoss: 417.4980\n",
      "Training Epoch: 6 [40960/49669]\tLoss: 411.6697\n",
      "Training Epoch: 6 [41024/49669]\tLoss: 473.1103\n",
      "Training Epoch: 6 [41088/49669]\tLoss: 419.3294\n",
      "Training Epoch: 6 [41152/49669]\tLoss: 431.7197\n",
      "Training Epoch: 6 [41216/49669]\tLoss: 437.7435\n",
      "Training Epoch: 6 [41280/49669]\tLoss: 420.2894\n",
      "Training Epoch: 6 [41344/49669]\tLoss: 417.4475\n",
      "Training Epoch: 6 [41408/49669]\tLoss: 439.0743\n",
      "Training Epoch: 6 [41472/49669]\tLoss: 466.4072\n",
      "Training Epoch: 6 [41536/49669]\tLoss: 456.7370\n",
      "Training Epoch: 6 [41600/49669]\tLoss: 454.4234\n",
      "Training Epoch: 6 [41664/49669]\tLoss: 435.6528\n",
      "Training Epoch: 6 [41728/49669]\tLoss: 433.1869\n",
      "Training Epoch: 6 [41792/49669]\tLoss: 424.8079\n",
      "Training Epoch: 6 [41856/49669]\tLoss: 438.8767\n",
      "Training Epoch: 6 [41920/49669]\tLoss: 436.5668\n",
      "Training Epoch: 6 [41984/49669]\tLoss: 411.2903\n",
      "Training Epoch: 6 [42048/49669]\tLoss: 405.8581\n",
      "Training Epoch: 6 [42112/49669]\tLoss: 432.1198\n",
      "Training Epoch: 6 [42176/49669]\tLoss: 442.4293\n",
      "Training Epoch: 6 [42240/49669]\tLoss: 450.0080\n",
      "Training Epoch: 6 [42304/49669]\tLoss: 421.1982\n",
      "Training Epoch: 6 [42368/49669]\tLoss: 426.6473\n",
      "Training Epoch: 6 [42432/49669]\tLoss: 441.3042\n",
      "Training Epoch: 6 [42496/49669]\tLoss: 383.7187\n",
      "Training Epoch: 6 [42560/49669]\tLoss: 438.3958\n",
      "Training Epoch: 6 [42624/49669]\tLoss: 425.3225\n",
      "Training Epoch: 6 [42688/49669]\tLoss: 423.8630\n",
      "Training Epoch: 6 [42752/49669]\tLoss: 449.3889\n",
      "Training Epoch: 6 [42816/49669]\tLoss: 428.5426\n",
      "Training Epoch: 6 [42880/49669]\tLoss: 386.1583\n",
      "Training Epoch: 6 [42944/49669]\tLoss: 452.2323\n",
      "Training Epoch: 6 [43008/49669]\tLoss: 415.9074\n",
      "Training Epoch: 6 [43072/49669]\tLoss: 409.3294\n",
      "Training Epoch: 6 [43136/49669]\tLoss: 428.4771\n",
      "Training Epoch: 6 [43200/49669]\tLoss: 436.0984\n",
      "Training Epoch: 6 [43264/49669]\tLoss: 413.9214\n",
      "Training Epoch: 6 [43328/49669]\tLoss: 449.2339\n",
      "Training Epoch: 6 [43392/49669]\tLoss: 436.5492\n",
      "Training Epoch: 6 [43456/49669]\tLoss: 423.0590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [43520/49669]\tLoss: 425.7293\n",
      "Training Epoch: 6 [43584/49669]\tLoss: 407.9943\n",
      "Training Epoch: 6 [43648/49669]\tLoss: 412.6462\n",
      "Training Epoch: 6 [43712/49669]\tLoss: 404.4471\n",
      "Training Epoch: 6 [43776/49669]\tLoss: 425.0269\n",
      "Training Epoch: 6 [43840/49669]\tLoss: 433.5444\n",
      "Training Epoch: 6 [43904/49669]\tLoss: 410.3797\n",
      "Training Epoch: 6 [43968/49669]\tLoss: 435.6581\n",
      "Training Epoch: 6 [44032/49669]\tLoss: 409.8965\n",
      "Training Epoch: 6 [44096/49669]\tLoss: 440.6697\n",
      "Training Epoch: 6 [44160/49669]\tLoss: 462.2852\n",
      "Training Epoch: 6 [44224/49669]\tLoss: 433.9009\n",
      "Training Epoch: 6 [44288/49669]\tLoss: 440.4520\n",
      "Training Epoch: 6 [44352/49669]\tLoss: 436.3964\n",
      "Training Epoch: 6 [44416/49669]\tLoss: 403.3260\n",
      "Training Epoch: 6 [44480/49669]\tLoss: 384.9576\n",
      "Training Epoch: 6 [44544/49669]\tLoss: 433.4174\n",
      "Training Epoch: 6 [44608/49669]\tLoss: 445.9031\n",
      "Training Epoch: 6 [44672/49669]\tLoss: 423.6253\n",
      "Training Epoch: 6 [44736/49669]\tLoss: 426.2391\n",
      "Training Epoch: 6 [44800/49669]\tLoss: 424.8422\n",
      "Training Epoch: 6 [44864/49669]\tLoss: 414.3634\n",
      "Training Epoch: 6 [44928/49669]\tLoss: 427.4782\n",
      "Training Epoch: 6 [44992/49669]\tLoss: 422.7236\n",
      "Training Epoch: 6 [45056/49669]\tLoss: 449.3051\n",
      "Training Epoch: 6 [45120/49669]\tLoss: 446.8967\n",
      "Training Epoch: 6 [45184/49669]\tLoss: 422.1797\n",
      "Training Epoch: 6 [45248/49669]\tLoss: 405.3887\n",
      "Training Epoch: 6 [45312/49669]\tLoss: 411.8944\n",
      "Training Epoch: 6 [45376/49669]\tLoss: 414.6558\n",
      "Training Epoch: 6 [45440/49669]\tLoss: 417.1093\n",
      "Training Epoch: 6 [45504/49669]\tLoss: 421.8409\n",
      "Training Epoch: 6 [45568/49669]\tLoss: 386.5668\n",
      "Training Epoch: 6 [45632/49669]\tLoss: 407.7092\n",
      "Training Epoch: 6 [45696/49669]\tLoss: 407.4939\n",
      "Training Epoch: 6 [45760/49669]\tLoss: 401.4153\n",
      "Training Epoch: 6 [45824/49669]\tLoss: 439.0582\n",
      "Training Epoch: 6 [45888/49669]\tLoss: 404.0656\n",
      "Training Epoch: 6 [45952/49669]\tLoss: 382.1390\n",
      "Training Epoch: 6 [46016/49669]\tLoss: 403.3476\n",
      "Training Epoch: 6 [46080/49669]\tLoss: 439.0878\n",
      "Training Epoch: 6 [46144/49669]\tLoss: 401.3318\n",
      "Training Epoch: 6 [46208/49669]\tLoss: 425.6562\n",
      "Training Epoch: 6 [46272/49669]\tLoss: 418.3605\n",
      "Training Epoch: 6 [46336/49669]\tLoss: 426.4330\n",
      "Training Epoch: 6 [46400/49669]\tLoss: 425.3455\n",
      "Training Epoch: 6 [46464/49669]\tLoss: 450.9679\n",
      "Training Epoch: 6 [46528/49669]\tLoss: 432.5309\n",
      "Training Epoch: 6 [46592/49669]\tLoss: 442.6907\n",
      "Training Epoch: 6 [46656/49669]\tLoss: 442.1373\n",
      "Training Epoch: 6 [46720/49669]\tLoss: 421.5112\n",
      "Training Epoch: 6 [46784/49669]\tLoss: 433.4378\n",
      "Training Epoch: 6 [46848/49669]\tLoss: 418.3475\n",
      "Training Epoch: 6 [46912/49669]\tLoss: 410.7616\n",
      "Training Epoch: 6 [46976/49669]\tLoss: 388.5039\n",
      "Training Epoch: 6 [47040/49669]\tLoss: 396.2130\n",
      "Training Epoch: 6 [47104/49669]\tLoss: 430.1369\n",
      "Training Epoch: 6 [47168/49669]\tLoss: 428.8940\n",
      "Training Epoch: 6 [47232/49669]\tLoss: 415.1770\n",
      "Training Epoch: 6 [47296/49669]\tLoss: 416.0681\n",
      "Training Epoch: 6 [47360/49669]\tLoss: 426.7964\n",
      "Training Epoch: 6 [47424/49669]\tLoss: 413.6509\n",
      "Training Epoch: 6 [47488/49669]\tLoss: 405.4731\n",
      "Training Epoch: 6 [47552/49669]\tLoss: 410.2218\n",
      "Training Epoch: 6 [47616/49669]\tLoss: 435.7179\n",
      "Training Epoch: 6 [47680/49669]\tLoss: 409.2062\n",
      "Training Epoch: 6 [47744/49669]\tLoss: 410.7047\n",
      "Training Epoch: 6 [47808/49669]\tLoss: 387.8112\n",
      "Training Epoch: 6 [47872/49669]\tLoss: 408.2434\n",
      "Training Epoch: 6 [47936/49669]\tLoss: 420.5781\n",
      "Training Epoch: 6 [48000/49669]\tLoss: 412.5612\n",
      "Training Epoch: 6 [48064/49669]\tLoss: 436.2035\n",
      "Training Epoch: 6 [48128/49669]\tLoss: 440.6714\n",
      "Training Epoch: 6 [48192/49669]\tLoss: 441.9911\n",
      "Training Epoch: 6 [48256/49669]\tLoss: 412.1364\n",
      "Training Epoch: 6 [48320/49669]\tLoss: 407.5154\n",
      "Training Epoch: 6 [48384/49669]\tLoss: 427.8039\n",
      "Training Epoch: 6 [48448/49669]\tLoss: 409.8470\n",
      "Training Epoch: 6 [48512/49669]\tLoss: 415.6235\n",
      "Training Epoch: 6 [48576/49669]\tLoss: 416.3733\n",
      "Training Epoch: 6 [48640/49669]\tLoss: 416.9282\n",
      "Training Epoch: 6 [48704/49669]\tLoss: 391.1020\n",
      "Training Epoch: 6 [48768/49669]\tLoss: 401.9419\n",
      "Training Epoch: 6 [48832/49669]\tLoss: 419.2829\n",
      "Training Epoch: 6 [48896/49669]\tLoss: 407.5061\n",
      "Training Epoch: 6 [48960/49669]\tLoss: 438.1351\n",
      "Training Epoch: 6 [49024/49669]\tLoss: 419.8720\n",
      "Training Epoch: 6 [49088/49669]\tLoss: 415.9600\n",
      "Training Epoch: 6 [49152/49669]\tLoss: 437.9737\n",
      "Training Epoch: 6 [49216/49669]\tLoss: 415.0161\n",
      "Training Epoch: 6 [49280/49669]\tLoss: 409.0741\n",
      "Training Epoch: 6 [49344/49669]\tLoss: 404.4218\n",
      "Training Epoch: 6 [49408/49669]\tLoss: 411.5304\n",
      "Training Epoch: 6 [49472/49669]\tLoss: 395.2332\n",
      "Training Epoch: 6 [49536/49669]\tLoss: 414.3682\n",
      "Training Epoch: 6 [49600/49669]\tLoss: 440.1770\n",
      "Training Epoch: 6 [49664/49669]\tLoss: 406.9857\n",
      "Training Epoch: 6 [49669/49669]\tLoss: 353.9226\n",
      "Training Epoch: 6 [5519/5519]\tLoss: 420.6716\n",
      "Training Epoch: 7 [64/49669]\tLoss: 425.0816\n",
      "Training Epoch: 7 [128/49669]\tLoss: 447.2266\n",
      "Training Epoch: 7 [192/49669]\tLoss: 440.3306\n",
      "Training Epoch: 7 [256/49669]\tLoss: 420.8602\n",
      "Training Epoch: 7 [320/49669]\tLoss: 467.1782\n",
      "Training Epoch: 7 [384/49669]\tLoss: 399.9618\n",
      "Training Epoch: 7 [448/49669]\tLoss: 470.3130\n",
      "Training Epoch: 7 [512/49669]\tLoss: 430.0587\n",
      "Training Epoch: 7 [576/49669]\tLoss: 442.0567\n",
      "Training Epoch: 7 [640/49669]\tLoss: 434.7438\n",
      "Training Epoch: 7 [704/49669]\tLoss: 433.4133\n",
      "Training Epoch: 7 [768/49669]\tLoss: 411.8400\n",
      "Training Epoch: 7 [832/49669]\tLoss: 430.5719\n",
      "Training Epoch: 7 [896/49669]\tLoss: 419.1576\n",
      "Training Epoch: 7 [960/49669]\tLoss: 445.2120\n",
      "Training Epoch: 7 [1024/49669]\tLoss: 440.0178\n",
      "Training Epoch: 7 [1088/49669]\tLoss: 459.8242\n",
      "Training Epoch: 7 [1152/49669]\tLoss: 460.5439\n",
      "Training Epoch: 7 [1216/49669]\tLoss: 493.4612\n",
      "Training Epoch: 7 [1280/49669]\tLoss: 487.0061\n",
      "Training Epoch: 7 [1344/49669]\tLoss: 476.7496\n",
      "Training Epoch: 7 [1408/49669]\tLoss: 457.4140\n",
      "Training Epoch: 7 [1472/49669]\tLoss: 444.0565\n",
      "Training Epoch: 7 [1536/49669]\tLoss: 409.2862\n",
      "Training Epoch: 7 [1600/49669]\tLoss: 434.6392\n",
      "Training Epoch: 7 [1664/49669]\tLoss: 454.1302\n",
      "Training Epoch: 7 [1728/49669]\tLoss: 457.6923\n",
      "Training Epoch: 7 [1792/49669]\tLoss: 446.5657\n",
      "Training Epoch: 7 [1856/49669]\tLoss: 432.7239\n",
      "Training Epoch: 7 [1920/49669]\tLoss: 419.7864\n",
      "Training Epoch: 7 [1984/49669]\tLoss: 423.4109\n",
      "Training Epoch: 7 [2048/49669]\tLoss: 420.9246\n",
      "Training Epoch: 7 [2112/49669]\tLoss: 438.7766\n",
      "Training Epoch: 7 [2176/49669]\tLoss: 415.1401\n",
      "Training Epoch: 7 [2240/49669]\tLoss: 431.1111\n",
      "Training Epoch: 7 [2304/49669]\tLoss: 402.1549\n",
      "Training Epoch: 7 [2368/49669]\tLoss: 407.4591\n",
      "Training Epoch: 7 [2432/49669]\tLoss: 436.8292\n",
      "Training Epoch: 7 [2496/49669]\tLoss: 442.6340\n",
      "Training Epoch: 7 [2560/49669]\tLoss: 470.3131\n",
      "Training Epoch: 7 [2624/49669]\tLoss: 441.1681\n",
      "Training Epoch: 7 [2688/49669]\tLoss: 451.1755\n",
      "Training Epoch: 7 [2752/49669]\tLoss: 415.0511\n",
      "Training Epoch: 7 [2816/49669]\tLoss: 426.4608\n",
      "Training Epoch: 7 [2880/49669]\tLoss: 446.3708\n",
      "Training Epoch: 7 [2944/49669]\tLoss: 398.6775\n",
      "Training Epoch: 7 [3008/49669]\tLoss: 450.9117\n",
      "Training Epoch: 7 [3072/49669]\tLoss: 405.9436\n",
      "Training Epoch: 7 [3136/49669]\tLoss: 400.8867\n",
      "Training Epoch: 7 [3200/49669]\tLoss: 439.0284\n",
      "Training Epoch: 7 [3264/49669]\tLoss: 451.9831\n",
      "Training Epoch: 7 [3328/49669]\tLoss: 419.0342\n",
      "Training Epoch: 7 [3392/49669]\tLoss: 446.2225\n",
      "Training Epoch: 7 [3456/49669]\tLoss: 442.8972\n",
      "Training Epoch: 7 [3520/49669]\tLoss: 374.0360\n",
      "Training Epoch: 7 [3584/49669]\tLoss: 461.5270\n",
      "Training Epoch: 7 [3648/49669]\tLoss: 461.2505\n",
      "Training Epoch: 7 [3712/49669]\tLoss: 412.6765\n",
      "Training Epoch: 7 [3776/49669]\tLoss: 411.4757\n",
      "Training Epoch: 7 [3840/49669]\tLoss: 448.0933\n",
      "Training Epoch: 7 [3904/49669]\tLoss: 404.3372\n",
      "Training Epoch: 7 [3968/49669]\tLoss: 421.1968\n",
      "Training Epoch: 7 [4032/49669]\tLoss: 444.9062\n",
      "Training Epoch: 7 [4096/49669]\tLoss: 414.5351\n",
      "Training Epoch: 7 [4160/49669]\tLoss: 414.7451\n",
      "Training Epoch: 7 [4224/49669]\tLoss: 402.8342\n",
      "Training Epoch: 7 [4288/49669]\tLoss: 412.5413\n",
      "Training Epoch: 7 [4352/49669]\tLoss: 372.5872\n",
      "Training Epoch: 7 [4416/49669]\tLoss: 436.6859\n",
      "Training Epoch: 7 [4480/49669]\tLoss: 377.6524\n",
      "Training Epoch: 7 [4544/49669]\tLoss: 424.0830\n",
      "Training Epoch: 7 [4608/49669]\tLoss: 423.2047\n",
      "Training Epoch: 7 [4672/49669]\tLoss: 418.5417\n",
      "Training Epoch: 7 [4736/49669]\tLoss: 419.4694\n",
      "Training Epoch: 7 [4800/49669]\tLoss: 423.6793\n",
      "Training Epoch: 7 [4864/49669]\tLoss: 408.5046\n",
      "Training Epoch: 7 [4928/49669]\tLoss: 410.7856\n",
      "Training Epoch: 7 [4992/49669]\tLoss: 449.9163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [5056/49669]\tLoss: 433.4995\n",
      "Training Epoch: 7 [5120/49669]\tLoss: 415.5791\n",
      "Training Epoch: 7 [5184/49669]\tLoss: 408.6724\n",
      "Training Epoch: 7 [5248/49669]\tLoss: 394.4355\n",
      "Training Epoch: 7 [5312/49669]\tLoss: 420.1036\n",
      "Training Epoch: 7 [5376/49669]\tLoss: 426.4429\n",
      "Training Epoch: 7 [5440/49669]\tLoss: 413.0759\n",
      "Training Epoch: 7 [5504/49669]\tLoss: 411.5092\n",
      "Training Epoch: 7 [5568/49669]\tLoss: 442.0206\n",
      "Training Epoch: 7 [5632/49669]\tLoss: 432.7466\n",
      "Training Epoch: 7 [5696/49669]\tLoss: 456.2838\n",
      "Training Epoch: 7 [5760/49669]\tLoss: 407.6010\n",
      "Training Epoch: 7 [5824/49669]\tLoss: 409.2937\n",
      "Training Epoch: 7 [5888/49669]\tLoss: 429.3297\n",
      "Training Epoch: 7 [5952/49669]\tLoss: 424.2076\n",
      "Training Epoch: 7 [6016/49669]\tLoss: 407.4745\n",
      "Training Epoch: 7 [6080/49669]\tLoss: 410.4074\n",
      "Training Epoch: 7 [6144/49669]\tLoss: 409.6229\n",
      "Training Epoch: 7 [6208/49669]\tLoss: 422.2618\n",
      "Training Epoch: 7 [6272/49669]\tLoss: 406.5633\n",
      "Training Epoch: 7 [6336/49669]\tLoss: 447.0204\n",
      "Training Epoch: 7 [6400/49669]\tLoss: 437.6701\n",
      "Training Epoch: 7 [6464/49669]\tLoss: 405.5894\n",
      "Training Epoch: 7 [6528/49669]\tLoss: 424.0010\n",
      "Training Epoch: 7 [6592/49669]\tLoss: 399.1439\n",
      "Training Epoch: 7 [6656/49669]\tLoss: 426.0863\n",
      "Training Epoch: 7 [6720/49669]\tLoss: 423.7491\n",
      "Training Epoch: 7 [6784/49669]\tLoss: 422.6269\n",
      "Training Epoch: 7 [6848/49669]\tLoss: 420.5446\n",
      "Training Epoch: 7 [6912/49669]\tLoss: 397.6139\n",
      "Training Epoch: 7 [6976/49669]\tLoss: 416.9558\n",
      "Training Epoch: 7 [7040/49669]\tLoss: 402.6807\n",
      "Training Epoch: 7 [7104/49669]\tLoss: 416.4102\n",
      "Training Epoch: 7 [7168/49669]\tLoss: 415.6879\n",
      "Training Epoch: 7 [7232/49669]\tLoss: 396.2081\n",
      "Training Epoch: 7 [7296/49669]\tLoss: 426.8692\n",
      "Training Epoch: 7 [7360/49669]\tLoss: 414.7768\n",
      "Training Epoch: 7 [7424/49669]\tLoss: 398.0706\n",
      "Training Epoch: 7 [7488/49669]\tLoss: 451.3689\n",
      "Training Epoch: 7 [7552/49669]\tLoss: 426.2802\n",
      "Training Epoch: 7 [7616/49669]\tLoss: 429.2108\n",
      "Training Epoch: 7 [7680/49669]\tLoss: 408.9956\n",
      "Training Epoch: 7 [7744/49669]\tLoss: 398.8217\n",
      "Training Epoch: 7 [7808/49669]\tLoss: 435.4580\n",
      "Training Epoch: 7 [7872/49669]\tLoss: 437.2003\n",
      "Training Epoch: 7 [7936/49669]\tLoss: 408.8147\n",
      "Training Epoch: 7 [8000/49669]\tLoss: 431.7508\n",
      "Training Epoch: 7 [8064/49669]\tLoss: 424.1374\n",
      "Training Epoch: 7 [8128/49669]\tLoss: 426.2584\n",
      "Training Epoch: 7 [8192/49669]\tLoss: 383.7094\n",
      "Training Epoch: 7 [8256/49669]\tLoss: 439.6338\n",
      "Training Epoch: 7 [8320/49669]\tLoss: 420.2357\n",
      "Training Epoch: 7 [8384/49669]\tLoss: 417.2439\n",
      "Training Epoch: 7 [8448/49669]\tLoss: 426.7791\n",
      "Training Epoch: 7 [8512/49669]\tLoss: 432.5814\n",
      "Training Epoch: 7 [8576/49669]\tLoss: 420.7767\n",
      "Training Epoch: 7 [8640/49669]\tLoss: 411.6193\n",
      "Training Epoch: 7 [8704/49669]\tLoss: 416.9492\n",
      "Training Epoch: 7 [8768/49669]\tLoss: 415.6368\n",
      "Training Epoch: 7 [8832/49669]\tLoss: 446.4683\n",
      "Training Epoch: 7 [8896/49669]\tLoss: 414.3321\n",
      "Training Epoch: 7 [8960/49669]\tLoss: 429.0308\n",
      "Training Epoch: 7 [9024/49669]\tLoss: 409.8669\n",
      "Training Epoch: 7 [9088/49669]\tLoss: 384.5262\n",
      "Training Epoch: 7 [9152/49669]\tLoss: 391.8830\n",
      "Training Epoch: 7 [9216/49669]\tLoss: 424.8272\n",
      "Training Epoch: 7 [9280/49669]\tLoss: 434.2758\n",
      "Training Epoch: 7 [9344/49669]\tLoss: 421.1084\n",
      "Training Epoch: 7 [9408/49669]\tLoss: 414.4518\n",
      "Training Epoch: 7 [9472/49669]\tLoss: 432.0346\n",
      "Training Epoch: 7 [9536/49669]\tLoss: 404.7072\n",
      "Training Epoch: 7 [9600/49669]\tLoss: 407.6624\n",
      "Training Epoch: 7 [9664/49669]\tLoss: 425.4127\n",
      "Training Epoch: 7 [9728/49669]\tLoss: 371.5155\n",
      "Training Epoch: 7 [9792/49669]\tLoss: 433.0777\n",
      "Training Epoch: 7 [9856/49669]\tLoss: 427.9579\n",
      "Training Epoch: 7 [9920/49669]\tLoss: 422.8374\n",
      "Training Epoch: 7 [9984/49669]\tLoss: 419.2181\n",
      "Training Epoch: 7 [10048/49669]\tLoss: 417.8239\n",
      "Training Epoch: 7 [10112/49669]\tLoss: 397.5924\n",
      "Training Epoch: 7 [10176/49669]\tLoss: 420.7372\n",
      "Training Epoch: 7 [10240/49669]\tLoss: 419.8263\n",
      "Training Epoch: 7 [10304/49669]\tLoss: 426.6416\n",
      "Training Epoch: 7 [10368/49669]\tLoss: 410.3345\n",
      "Training Epoch: 7 [10432/49669]\tLoss: 426.0781\n",
      "Training Epoch: 7 [10496/49669]\tLoss: 427.3160\n",
      "Training Epoch: 7 [10560/49669]\tLoss: 410.7909\n",
      "Training Epoch: 7 [10624/49669]\tLoss: 397.9161\n",
      "Training Epoch: 7 [10688/49669]\tLoss: 416.6274\n",
      "Training Epoch: 7 [10752/49669]\tLoss: 440.5480\n",
      "Training Epoch: 7 [10816/49669]\tLoss: 433.2534\n",
      "Training Epoch: 7 [10880/49669]\tLoss: 436.8459\n",
      "Training Epoch: 7 [10944/49669]\tLoss: 384.8996\n",
      "Training Epoch: 7 [11008/49669]\tLoss: 412.7545\n",
      "Training Epoch: 7 [11072/49669]\tLoss: 392.3543\n",
      "Training Epoch: 7 [11136/49669]\tLoss: 420.2647\n",
      "Training Epoch: 7 [11200/49669]\tLoss: 408.1140\n",
      "Training Epoch: 7 [11264/49669]\tLoss: 395.1810\n",
      "Training Epoch: 7 [11328/49669]\tLoss: 429.3064\n",
      "Training Epoch: 7 [11392/49669]\tLoss: 412.9253\n",
      "Training Epoch: 7 [11456/49669]\tLoss: 434.3515\n",
      "Training Epoch: 7 [11520/49669]\tLoss: 398.2602\n",
      "Training Epoch: 7 [11584/49669]\tLoss: 425.1003\n",
      "Training Epoch: 7 [11648/49669]\tLoss: 403.8640\n",
      "Training Epoch: 7 [11712/49669]\tLoss: 393.9756\n",
      "Training Epoch: 7 [11776/49669]\tLoss: 416.8075\n",
      "Training Epoch: 7 [11840/49669]\tLoss: 403.2518\n",
      "Training Epoch: 7 [11904/49669]\tLoss: 431.0021\n",
      "Training Epoch: 7 [11968/49669]\tLoss: 425.6136\n",
      "Training Epoch: 7 [12032/49669]\tLoss: 397.8560\n",
      "Training Epoch: 7 [12096/49669]\tLoss: 456.5638\n",
      "Training Epoch: 7 [12160/49669]\tLoss: 421.4077\n",
      "Training Epoch: 7 [12224/49669]\tLoss: 421.9797\n",
      "Training Epoch: 7 [12288/49669]\tLoss: 422.6026\n",
      "Training Epoch: 7 [12352/49669]\tLoss: 437.0717\n",
      "Training Epoch: 7 [12416/49669]\tLoss: 441.2566\n",
      "Training Epoch: 7 [12480/49669]\tLoss: 416.6892\n",
      "Training Epoch: 7 [12544/49669]\tLoss: 441.2870\n",
      "Training Epoch: 7 [12608/49669]\tLoss: 431.6283\n",
      "Training Epoch: 7 [12672/49669]\tLoss: 413.8360\n",
      "Training Epoch: 7 [12736/49669]\tLoss: 407.7855\n",
      "Training Epoch: 7 [12800/49669]\tLoss: 366.8940\n",
      "Training Epoch: 7 [12864/49669]\tLoss: 457.1990\n",
      "Training Epoch: 7 [12928/49669]\tLoss: 389.3033\n",
      "Training Epoch: 7 [12992/49669]\tLoss: 376.3922\n",
      "Training Epoch: 7 [13056/49669]\tLoss: 406.5545\n",
      "Training Epoch: 7 [13120/49669]\tLoss: 431.4680\n",
      "Training Epoch: 7 [13184/49669]\tLoss: 405.0394\n",
      "Training Epoch: 7 [13248/49669]\tLoss: 408.5679\n",
      "Training Epoch: 7 [13312/49669]\tLoss: 378.9074\n",
      "Training Epoch: 7 [13376/49669]\tLoss: 392.7271\n",
      "Training Epoch: 7 [13440/49669]\tLoss: 401.3608\n",
      "Training Epoch: 7 [13504/49669]\tLoss: 406.1307\n",
      "Training Epoch: 7 [13568/49669]\tLoss: 422.9121\n",
      "Training Epoch: 7 [13632/49669]\tLoss: 385.7656\n",
      "Training Epoch: 7 [13696/49669]\tLoss: 434.2169\n",
      "Training Epoch: 7 [13760/49669]\tLoss: 424.8419\n",
      "Training Epoch: 7 [13824/49669]\tLoss: 394.2173\n",
      "Training Epoch: 7 [13888/49669]\tLoss: 420.7928\n",
      "Training Epoch: 7 [13952/49669]\tLoss: 426.0324\n",
      "Training Epoch: 7 [14016/49669]\tLoss: 395.3809\n",
      "Training Epoch: 7 [14080/49669]\tLoss: 448.2817\n",
      "Training Epoch: 7 [14144/49669]\tLoss: 396.9742\n",
      "Training Epoch: 7 [14208/49669]\tLoss: 390.4639\n",
      "Training Epoch: 7 [14272/49669]\tLoss: 415.7871\n",
      "Training Epoch: 7 [14336/49669]\tLoss: 378.1399\n",
      "Training Epoch: 7 [14400/49669]\tLoss: 414.0922\n",
      "Training Epoch: 7 [14464/49669]\tLoss: 385.3533\n",
      "Training Epoch: 7 [14528/49669]\tLoss: 422.1192\n",
      "Training Epoch: 7 [14592/49669]\tLoss: 406.3797\n",
      "Training Epoch: 7 [14656/49669]\tLoss: 434.6263\n",
      "Training Epoch: 7 [14720/49669]\tLoss: 400.6198\n",
      "Training Epoch: 7 [14784/49669]\tLoss: 396.8703\n",
      "Training Epoch: 7 [14848/49669]\tLoss: 410.0311\n",
      "Training Epoch: 7 [14912/49669]\tLoss: 402.9386\n",
      "Training Epoch: 7 [14976/49669]\tLoss: 430.0647\n",
      "Training Epoch: 7 [15040/49669]\tLoss: 433.8004\n",
      "Training Epoch: 7 [15104/49669]\tLoss: 417.1505\n",
      "Training Epoch: 7 [15168/49669]\tLoss: 441.3200\n",
      "Training Epoch: 7 [15232/49669]\tLoss: 433.7791\n",
      "Training Epoch: 7 [15296/49669]\tLoss: 413.1959\n",
      "Training Epoch: 7 [15360/49669]\tLoss: 407.5835\n",
      "Training Epoch: 7 [15424/49669]\tLoss: 418.0978\n",
      "Training Epoch: 7 [15488/49669]\tLoss: 417.4040\n",
      "Training Epoch: 7 [15552/49669]\tLoss: 407.8940\n",
      "Training Epoch: 7 [15616/49669]\tLoss: 416.7234\n",
      "Training Epoch: 7 [15680/49669]\tLoss: 388.6070\n",
      "Training Epoch: 7 [15744/49669]\tLoss: 408.1564\n",
      "Training Epoch: 7 [15808/49669]\tLoss: 405.1014\n",
      "Training Epoch: 7 [15872/49669]\tLoss: 421.5909\n",
      "Training Epoch: 7 [15936/49669]\tLoss: 403.3197\n",
      "Training Epoch: 7 [16000/49669]\tLoss: 409.1360\n",
      "Training Epoch: 7 [16064/49669]\tLoss: 436.0927\n",
      "Training Epoch: 7 [16128/49669]\tLoss: 401.2766\n",
      "Training Epoch: 7 [16192/49669]\tLoss: 439.4057\n",
      "Training Epoch: 7 [16256/49669]\tLoss: 406.5759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [16320/49669]\tLoss: 408.1106\n",
      "Training Epoch: 7 [16384/49669]\tLoss: 419.8583\n",
      "Training Epoch: 7 [16448/49669]\tLoss: 444.6046\n",
      "Training Epoch: 7 [16512/49669]\tLoss: 394.1748\n",
      "Training Epoch: 7 [16576/49669]\tLoss: 413.9142\n",
      "Training Epoch: 7 [16640/49669]\tLoss: 431.8683\n",
      "Training Epoch: 7 [16704/49669]\tLoss: 426.9635\n",
      "Training Epoch: 7 [16768/49669]\tLoss: 445.8720\n",
      "Training Epoch: 7 [16832/49669]\tLoss: 436.1843\n",
      "Training Epoch: 7 [16896/49669]\tLoss: 449.1648\n",
      "Training Epoch: 7 [16960/49669]\tLoss: 426.4169\n",
      "Training Epoch: 7 [17024/49669]\tLoss: 418.2907\n",
      "Training Epoch: 7 [17088/49669]\tLoss: 464.2092\n",
      "Training Epoch: 7 [17152/49669]\tLoss: 454.9072\n",
      "Training Epoch: 7 [17216/49669]\tLoss: 453.9264\n",
      "Training Epoch: 7 [17280/49669]\tLoss: 455.9205\n",
      "Training Epoch: 7 [17344/49669]\tLoss: 421.2018\n",
      "Training Epoch: 7 [17408/49669]\tLoss: 420.2542\n",
      "Training Epoch: 7 [17472/49669]\tLoss: 415.4566\n",
      "Training Epoch: 7 [17536/49669]\tLoss: 467.4383\n",
      "Training Epoch: 7 [17600/49669]\tLoss: 448.7787\n",
      "Training Epoch: 7 [17664/49669]\tLoss: 459.6312\n",
      "Training Epoch: 7 [17728/49669]\tLoss: 449.1194\n",
      "Training Epoch: 7 [17792/49669]\tLoss: 415.6571\n",
      "Training Epoch: 7 [17856/49669]\tLoss: 435.5606\n",
      "Training Epoch: 7 [17920/49669]\tLoss: 458.6426\n",
      "Training Epoch: 7 [17984/49669]\tLoss: 451.5124\n",
      "Training Epoch: 7 [18048/49669]\tLoss: 416.5641\n",
      "Training Epoch: 7 [18112/49669]\tLoss: 431.0303\n",
      "Training Epoch: 7 [18176/49669]\tLoss: 416.3912\n",
      "Training Epoch: 7 [18240/49669]\tLoss: 413.1724\n",
      "Training Epoch: 7 [18304/49669]\tLoss: 429.1651\n",
      "Training Epoch: 7 [18368/49669]\tLoss: 466.4457\n",
      "Training Epoch: 7 [18432/49669]\tLoss: 417.2299\n",
      "Training Epoch: 7 [18496/49669]\tLoss: 419.2135\n",
      "Training Epoch: 7 [18560/49669]\tLoss: 431.8189\n",
      "Training Epoch: 7 [18624/49669]\tLoss: 429.1703\n",
      "Training Epoch: 7 [18688/49669]\tLoss: 391.1956\n",
      "Training Epoch: 7 [18752/49669]\tLoss: 427.9177\n",
      "Training Epoch: 7 [18816/49669]\tLoss: 416.7998\n",
      "Training Epoch: 7 [18880/49669]\tLoss: 441.1145\n",
      "Training Epoch: 7 [18944/49669]\tLoss: 416.0045\n",
      "Training Epoch: 7 [19008/49669]\tLoss: 435.1735\n",
      "Training Epoch: 7 [19072/49669]\tLoss: 426.7979\n",
      "Training Epoch: 7 [19136/49669]\tLoss: 412.8873\n",
      "Training Epoch: 7 [19200/49669]\tLoss: 412.3437\n",
      "Training Epoch: 7 [19264/49669]\tLoss: 397.2324\n",
      "Training Epoch: 7 [19328/49669]\tLoss: 422.9710\n",
      "Training Epoch: 7 [19392/49669]\tLoss: 392.8910\n",
      "Training Epoch: 7 [19456/49669]\tLoss: 405.7339\n",
      "Training Epoch: 7 [19520/49669]\tLoss: 452.4249\n",
      "Training Epoch: 7 [19584/49669]\tLoss: 408.3860\n",
      "Training Epoch: 7 [19648/49669]\tLoss: 420.5337\n",
      "Training Epoch: 7 [19712/49669]\tLoss: 423.7851\n",
      "Training Epoch: 7 [19776/49669]\tLoss: 415.4790\n",
      "Training Epoch: 7 [19840/49669]\tLoss: 385.3663\n",
      "Training Epoch: 7 [19904/49669]\tLoss: 414.7517\n",
      "Training Epoch: 7 [19968/49669]\tLoss: 438.5897\n",
      "Training Epoch: 7 [20032/49669]\tLoss: 422.9041\n",
      "Training Epoch: 7 [20096/49669]\tLoss: 400.2897\n",
      "Training Epoch: 7 [20160/49669]\tLoss: 428.6937\n",
      "Training Epoch: 7 [20224/49669]\tLoss: 418.1060\n",
      "Training Epoch: 7 [20288/49669]\tLoss: 406.3030\n",
      "Training Epoch: 7 [20352/49669]\tLoss: 442.7296\n",
      "Training Epoch: 7 [20416/49669]\tLoss: 400.4175\n",
      "Training Epoch: 7 [20480/49669]\tLoss: 423.9327\n",
      "Training Epoch: 7 [20544/49669]\tLoss: 374.5116\n",
      "Training Epoch: 7 [20608/49669]\tLoss: 402.5868\n",
      "Training Epoch: 7 [20672/49669]\tLoss: 438.7870\n",
      "Training Epoch: 7 [20736/49669]\tLoss: 419.4027\n",
      "Training Epoch: 7 [20800/49669]\tLoss: 414.8871\n",
      "Training Epoch: 7 [20864/49669]\tLoss: 429.8376\n",
      "Training Epoch: 7 [20928/49669]\tLoss: 418.7324\n",
      "Training Epoch: 7 [20992/49669]\tLoss: 436.1720\n",
      "Training Epoch: 7 [21056/49669]\tLoss: 441.9324\n",
      "Training Epoch: 7 [21120/49669]\tLoss: 430.1483\n",
      "Training Epoch: 7 [21184/49669]\tLoss: 395.4910\n",
      "Training Epoch: 7 [21248/49669]\tLoss: 407.9343\n",
      "Training Epoch: 7 [21312/49669]\tLoss: 421.7317\n",
      "Training Epoch: 7 [21376/49669]\tLoss: 387.2807\n",
      "Training Epoch: 7 [21440/49669]\tLoss: 401.2345\n",
      "Training Epoch: 7 [21504/49669]\tLoss: 430.9806\n",
      "Training Epoch: 7 [21568/49669]\tLoss: 436.2861\n",
      "Training Epoch: 7 [21632/49669]\tLoss: 443.7708\n",
      "Training Epoch: 7 [21696/49669]\tLoss: 420.7458\n",
      "Training Epoch: 7 [21760/49669]\tLoss: 417.2231\n",
      "Training Epoch: 7 [21824/49669]\tLoss: 434.4623\n",
      "Training Epoch: 7 [21888/49669]\tLoss: 406.9753\n",
      "Training Epoch: 7 [21952/49669]\tLoss: 445.8376\n",
      "Training Epoch: 7 [22016/49669]\tLoss: 436.2078\n",
      "Training Epoch: 7 [22080/49669]\tLoss: 430.8470\n",
      "Training Epoch: 7 [22144/49669]\tLoss: 417.7936\n",
      "Training Epoch: 7 [22208/49669]\tLoss: 399.2760\n",
      "Training Epoch: 7 [22272/49669]\tLoss: 450.6505\n",
      "Training Epoch: 7 [22336/49669]\tLoss: 421.1341\n",
      "Training Epoch: 7 [22400/49669]\tLoss: 430.6675\n",
      "Training Epoch: 7 [22464/49669]\tLoss: 413.0593\n",
      "Training Epoch: 7 [22528/49669]\tLoss: 410.3787\n",
      "Training Epoch: 7 [22592/49669]\tLoss: 420.5479\n",
      "Training Epoch: 7 [22656/49669]\tLoss: 425.1994\n",
      "Training Epoch: 7 [22720/49669]\tLoss: 396.5865\n",
      "Training Epoch: 7 [22784/49669]\tLoss: 430.2018\n",
      "Training Epoch: 7 [22848/49669]\tLoss: 421.6913\n",
      "Training Epoch: 7 [22912/49669]\tLoss: 419.2507\n",
      "Training Epoch: 7 [22976/49669]\tLoss: 435.9929\n",
      "Training Epoch: 7 [23040/49669]\tLoss: 443.2888\n",
      "Training Epoch: 7 [23104/49669]\tLoss: 415.4022\n",
      "Training Epoch: 7 [23168/49669]\tLoss: 404.7020\n",
      "Training Epoch: 7 [23232/49669]\tLoss: 418.6019\n",
      "Training Epoch: 7 [23296/49669]\tLoss: 416.7740\n",
      "Training Epoch: 7 [23360/49669]\tLoss: 444.0853\n",
      "Training Epoch: 7 [23424/49669]\tLoss: 412.3046\n",
      "Training Epoch: 7 [23488/49669]\tLoss: 434.0720\n",
      "Training Epoch: 7 [23552/49669]\tLoss: 433.2349\n",
      "Training Epoch: 7 [23616/49669]\tLoss: 451.6250\n",
      "Training Epoch: 7 [23680/49669]\tLoss: 404.8528\n",
      "Training Epoch: 7 [23744/49669]\tLoss: 408.7432\n",
      "Training Epoch: 7 [23808/49669]\tLoss: 420.8628\n",
      "Training Epoch: 7 [23872/49669]\tLoss: 403.0644\n",
      "Training Epoch: 7 [23936/49669]\tLoss: 399.2270\n",
      "Training Epoch: 7 [24000/49669]\tLoss: 432.7144\n",
      "Training Epoch: 7 [24064/49669]\tLoss: 360.9145\n",
      "Training Epoch: 7 [24128/49669]\tLoss: 397.2371\n",
      "Training Epoch: 7 [24192/49669]\tLoss: 383.5908\n",
      "Training Epoch: 7 [24256/49669]\tLoss: 422.9341\n",
      "Training Epoch: 7 [24320/49669]\tLoss: 412.5997\n",
      "Training Epoch: 7 [24384/49669]\tLoss: 415.2752\n",
      "Training Epoch: 7 [24448/49669]\tLoss: 410.6266\n",
      "Training Epoch: 7 [24512/49669]\tLoss: 406.5541\n",
      "Training Epoch: 7 [24576/49669]\tLoss: 421.8570\n",
      "Training Epoch: 7 [24640/49669]\tLoss: 435.6616\n",
      "Training Epoch: 7 [24704/49669]\tLoss: 417.4123\n",
      "Training Epoch: 7 [24768/49669]\tLoss: 420.0179\n",
      "Training Epoch: 7 [24832/49669]\tLoss: 437.9322\n",
      "Training Epoch: 7 [24896/49669]\tLoss: 397.6977\n",
      "Training Epoch: 7 [24960/49669]\tLoss: 427.6349\n",
      "Training Epoch: 7 [25024/49669]\tLoss: 409.3545\n",
      "Training Epoch: 7 [25088/49669]\tLoss: 442.1363\n",
      "Training Epoch: 7 [25152/49669]\tLoss: 445.4241\n",
      "Training Epoch: 7 [25216/49669]\tLoss: 412.4978\n",
      "Training Epoch: 7 [25280/49669]\tLoss: 398.1749\n",
      "Training Epoch: 7 [25344/49669]\tLoss: 430.0267\n",
      "Training Epoch: 7 [25408/49669]\tLoss: 403.6534\n",
      "Training Epoch: 7 [25472/49669]\tLoss: 403.5225\n",
      "Training Epoch: 7 [25536/49669]\tLoss: 415.5525\n",
      "Training Epoch: 7 [25600/49669]\tLoss: 408.7261\n",
      "Training Epoch: 7 [25664/49669]\tLoss: 451.5196\n",
      "Training Epoch: 7 [25728/49669]\tLoss: 424.5096\n",
      "Training Epoch: 7 [25792/49669]\tLoss: 401.3142\n",
      "Training Epoch: 7 [25856/49669]\tLoss: 433.6964\n",
      "Training Epoch: 7 [25920/49669]\tLoss: 400.6006\n",
      "Training Epoch: 7 [25984/49669]\tLoss: 405.4571\n",
      "Training Epoch: 7 [26048/49669]\tLoss: 433.6999\n",
      "Training Epoch: 7 [26112/49669]\tLoss: 398.8204\n",
      "Training Epoch: 7 [26176/49669]\tLoss: 414.0540\n",
      "Training Epoch: 7 [26240/49669]\tLoss: 423.3109\n",
      "Training Epoch: 7 [26304/49669]\tLoss: 445.1814\n",
      "Training Epoch: 7 [26368/49669]\tLoss: 422.8203\n",
      "Training Epoch: 7 [26432/49669]\tLoss: 421.3675\n",
      "Training Epoch: 7 [26496/49669]\tLoss: 433.9048\n",
      "Training Epoch: 7 [26560/49669]\tLoss: 397.0995\n",
      "Training Epoch: 7 [26624/49669]\tLoss: 403.4408\n",
      "Training Epoch: 7 [26688/49669]\tLoss: 436.5144\n",
      "Training Epoch: 7 [26752/49669]\tLoss: 409.4803\n",
      "Training Epoch: 7 [26816/49669]\tLoss: 429.1183\n",
      "Training Epoch: 7 [26880/49669]\tLoss: 420.1774\n",
      "Training Epoch: 7 [26944/49669]\tLoss: 422.9888\n",
      "Training Epoch: 7 [27008/49669]\tLoss: 429.2941\n",
      "Training Epoch: 7 [27072/49669]\tLoss: 411.7719\n",
      "Training Epoch: 7 [27136/49669]\tLoss: 434.1677\n",
      "Training Epoch: 7 [27200/49669]\tLoss: 429.1587\n",
      "Training Epoch: 7 [27264/49669]\tLoss: 420.7145\n",
      "Training Epoch: 7 [27328/49669]\tLoss: 385.7494\n",
      "Training Epoch: 7 [27392/49669]\tLoss: 413.7520\n",
      "Training Epoch: 7 [27456/49669]\tLoss: 424.0224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [27520/49669]\tLoss: 410.9017\n",
      "Training Epoch: 7 [27584/49669]\tLoss: 438.2022\n",
      "Training Epoch: 7 [27648/49669]\tLoss: 382.8065\n",
      "Training Epoch: 7 [27712/49669]\tLoss: 423.4196\n",
      "Training Epoch: 7 [27776/49669]\tLoss: 406.7787\n",
      "Training Epoch: 7 [27840/49669]\tLoss: 424.1120\n",
      "Training Epoch: 7 [27904/49669]\tLoss: 386.1558\n",
      "Training Epoch: 7 [27968/49669]\tLoss: 435.0863\n",
      "Training Epoch: 7 [28032/49669]\tLoss: 396.1078\n",
      "Training Epoch: 7 [28096/49669]\tLoss: 417.5844\n",
      "Training Epoch: 7 [28160/49669]\tLoss: 398.6729\n",
      "Training Epoch: 7 [28224/49669]\tLoss: 413.0190\n",
      "Training Epoch: 7 [28288/49669]\tLoss: 422.5123\n",
      "Training Epoch: 7 [28352/49669]\tLoss: 382.8698\n",
      "Training Epoch: 7 [28416/49669]\tLoss: 392.4680\n",
      "Training Epoch: 7 [28480/49669]\tLoss: 415.7565\n",
      "Training Epoch: 7 [28544/49669]\tLoss: 397.4221\n",
      "Training Epoch: 7 [28608/49669]\tLoss: 442.8863\n",
      "Training Epoch: 7 [28672/49669]\tLoss: 431.3734\n",
      "Training Epoch: 7 [28736/49669]\tLoss: 394.9845\n",
      "Training Epoch: 7 [28800/49669]\tLoss: 397.3468\n",
      "Training Epoch: 7 [28864/49669]\tLoss: 433.4307\n",
      "Training Epoch: 7 [28928/49669]\tLoss: 408.9112\n",
      "Training Epoch: 7 [28992/49669]\tLoss: 452.3954\n",
      "Training Epoch: 7 [29056/49669]\tLoss: 443.7325\n",
      "Training Epoch: 7 [29120/49669]\tLoss: 441.6478\n",
      "Training Epoch: 7 [29184/49669]\tLoss: 427.3736\n",
      "Training Epoch: 7 [29248/49669]\tLoss: 421.2304\n",
      "Training Epoch: 7 [29312/49669]\tLoss: 420.5580\n",
      "Training Epoch: 7 [29376/49669]\tLoss: 431.1960\n",
      "Training Epoch: 7 [29440/49669]\tLoss: 417.8114\n",
      "Training Epoch: 7 [29504/49669]\tLoss: 417.3563\n",
      "Training Epoch: 7 [29568/49669]\tLoss: 410.0071\n",
      "Training Epoch: 7 [29632/49669]\tLoss: 421.6165\n",
      "Training Epoch: 7 [29696/49669]\tLoss: 435.1119\n",
      "Training Epoch: 7 [29760/49669]\tLoss: 392.9760\n",
      "Training Epoch: 7 [29824/49669]\tLoss: 403.0063\n",
      "Training Epoch: 7 [29888/49669]\tLoss: 404.3381\n",
      "Training Epoch: 7 [29952/49669]\tLoss: 420.9478\n",
      "Training Epoch: 7 [30016/49669]\tLoss: 408.0157\n",
      "Training Epoch: 7 [30080/49669]\tLoss: 423.1679\n",
      "Training Epoch: 7 [30144/49669]\tLoss: 416.0939\n",
      "Training Epoch: 7 [30208/49669]\tLoss: 422.4696\n",
      "Training Epoch: 7 [30272/49669]\tLoss: 390.9308\n",
      "Training Epoch: 7 [30336/49669]\tLoss: 419.5649\n",
      "Training Epoch: 7 [30400/49669]\tLoss: 396.9335\n",
      "Training Epoch: 7 [30464/49669]\tLoss: 421.6274\n",
      "Training Epoch: 7 [30528/49669]\tLoss: 420.1359\n",
      "Training Epoch: 7 [30592/49669]\tLoss: 409.1301\n",
      "Training Epoch: 7 [30656/49669]\tLoss: 436.7072\n",
      "Training Epoch: 7 [30720/49669]\tLoss: 413.6136\n",
      "Training Epoch: 7 [30784/49669]\tLoss: 460.3823\n",
      "Training Epoch: 7 [30848/49669]\tLoss: 427.8488\n",
      "Training Epoch: 7 [30912/49669]\tLoss: 449.4201\n",
      "Training Epoch: 7 [30976/49669]\tLoss: 468.3737\n",
      "Training Epoch: 7 [31040/49669]\tLoss: 479.1535\n",
      "Training Epoch: 7 [31104/49669]\tLoss: 516.1977\n",
      "Training Epoch: 7 [31168/49669]\tLoss: 466.2087\n",
      "Training Epoch: 7 [31232/49669]\tLoss: 451.9969\n",
      "Training Epoch: 7 [31296/49669]\tLoss: 441.4332\n",
      "Training Epoch: 7 [31360/49669]\tLoss: 428.7814\n",
      "Training Epoch: 7 [31424/49669]\tLoss: 424.0977\n",
      "Training Epoch: 7 [31488/49669]\tLoss: 483.6774\n",
      "Training Epoch: 7 [31552/49669]\tLoss: 474.9286\n",
      "Training Epoch: 7 [31616/49669]\tLoss: 495.2366\n",
      "Training Epoch: 7 [31680/49669]\tLoss: 431.7440\n",
      "Training Epoch: 7 [31744/49669]\tLoss: 437.7685\n",
      "Training Epoch: 7 [31808/49669]\tLoss: 428.0724\n",
      "Training Epoch: 7 [31872/49669]\tLoss: 433.2089\n",
      "Training Epoch: 7 [31936/49669]\tLoss: 451.5369\n",
      "Training Epoch: 7 [32000/49669]\tLoss: 478.3859\n",
      "Training Epoch: 7 [32064/49669]\tLoss: 442.0400\n",
      "Training Epoch: 7 [32128/49669]\tLoss: 431.2896\n",
      "Training Epoch: 7 [32192/49669]\tLoss: 419.2051\n",
      "Training Epoch: 7 [32256/49669]\tLoss: 435.0244\n",
      "Training Epoch: 7 [32320/49669]\tLoss: 441.1069\n",
      "Training Epoch: 7 [32384/49669]\tLoss: 413.7096\n",
      "Training Epoch: 7 [32448/49669]\tLoss: 416.5149\n",
      "Training Epoch: 7 [32512/49669]\tLoss: 393.9881\n",
      "Training Epoch: 7 [32576/49669]\tLoss: 435.9995\n",
      "Training Epoch: 7 [32640/49669]\tLoss: 429.2302\n",
      "Training Epoch: 7 [32704/49669]\tLoss: 420.4267\n",
      "Training Epoch: 7 [32768/49669]\tLoss: 400.9262\n",
      "Training Epoch: 7 [32832/49669]\tLoss: 414.6049\n",
      "Training Epoch: 7 [32896/49669]\tLoss: 425.5366\n",
      "Training Epoch: 7 [32960/49669]\tLoss: 431.7653\n",
      "Training Epoch: 7 [33024/49669]\tLoss: 427.7198\n",
      "Training Epoch: 7 [33088/49669]\tLoss: 433.6136\n",
      "Training Epoch: 7 [33152/49669]\tLoss: 430.7223\n",
      "Training Epoch: 7 [33216/49669]\tLoss: 453.6102\n",
      "Training Epoch: 7 [33280/49669]\tLoss: 419.1171\n",
      "Training Epoch: 7 [33344/49669]\tLoss: 421.3619\n",
      "Training Epoch: 7 [33408/49669]\tLoss: 399.9121\n",
      "Training Epoch: 7 [33472/49669]\tLoss: 425.1650\n",
      "Training Epoch: 7 [33536/49669]\tLoss: 428.5079\n",
      "Training Epoch: 7 [33600/49669]\tLoss: 430.6511\n",
      "Training Epoch: 7 [33664/49669]\tLoss: 420.2072\n",
      "Training Epoch: 7 [33728/49669]\tLoss: 441.7227\n",
      "Training Epoch: 7 [33792/49669]\tLoss: 420.4073\n",
      "Training Epoch: 7 [33856/49669]\tLoss: 425.7628\n",
      "Training Epoch: 7 [33920/49669]\tLoss: 413.9218\n",
      "Training Epoch: 7 [33984/49669]\tLoss: 433.5583\n",
      "Training Epoch: 7 [34048/49669]\tLoss: 410.5512\n",
      "Training Epoch: 7 [34112/49669]\tLoss: 438.0195\n",
      "Training Epoch: 7 [34176/49669]\tLoss: 400.8926\n",
      "Training Epoch: 7 [34240/49669]\tLoss: 408.2147\n",
      "Training Epoch: 7 [34304/49669]\tLoss: 456.3950\n",
      "Training Epoch: 7 [34368/49669]\tLoss: 389.6954\n",
      "Training Epoch: 7 [34432/49669]\tLoss: 426.3440\n",
      "Training Epoch: 7 [34496/49669]\tLoss: 432.6523\n",
      "Training Epoch: 7 [34560/49669]\tLoss: 392.0184\n",
      "Training Epoch: 7 [34624/49669]\tLoss: 425.7780\n",
      "Training Epoch: 7 [34688/49669]\tLoss: 400.2355\n",
      "Training Epoch: 7 [34752/49669]\tLoss: 410.8556\n",
      "Training Epoch: 7 [34816/49669]\tLoss: 427.6191\n",
      "Training Epoch: 7 [34880/49669]\tLoss: 434.5308\n",
      "Training Epoch: 7 [34944/49669]\tLoss: 417.2606\n",
      "Training Epoch: 7 [35008/49669]\tLoss: 424.2223\n",
      "Training Epoch: 7 [35072/49669]\tLoss: 424.4293\n",
      "Training Epoch: 7 [35136/49669]\tLoss: 435.6126\n",
      "Training Epoch: 7 [35200/49669]\tLoss: 389.2326\n",
      "Training Epoch: 7 [35264/49669]\tLoss: 443.0157\n",
      "Training Epoch: 7 [35328/49669]\tLoss: 438.8182\n",
      "Training Epoch: 7 [35392/49669]\tLoss: 422.1257\n",
      "Training Epoch: 7 [35456/49669]\tLoss: 419.4970\n",
      "Training Epoch: 7 [35520/49669]\tLoss: 385.9094\n",
      "Training Epoch: 7 [35584/49669]\tLoss: 428.5176\n",
      "Training Epoch: 7 [35648/49669]\tLoss: 406.6060\n",
      "Training Epoch: 7 [35712/49669]\tLoss: 422.1838\n",
      "Training Epoch: 7 [35776/49669]\tLoss: 403.5787\n",
      "Training Epoch: 7 [35840/49669]\tLoss: 385.8327\n",
      "Training Epoch: 7 [35904/49669]\tLoss: 428.0202\n",
      "Training Epoch: 7 [35968/49669]\tLoss: 401.2602\n",
      "Training Epoch: 7 [36032/49669]\tLoss: 427.8623\n",
      "Training Epoch: 7 [36096/49669]\tLoss: 409.2838\n",
      "Training Epoch: 7 [36160/49669]\tLoss: 406.9819\n",
      "Training Epoch: 7 [36224/49669]\tLoss: 411.9858\n",
      "Training Epoch: 7 [36288/49669]\tLoss: 420.9801\n",
      "Training Epoch: 7 [36352/49669]\tLoss: 402.2240\n",
      "Training Epoch: 7 [36416/49669]\tLoss: 401.6439\n",
      "Training Epoch: 7 [36480/49669]\tLoss: 407.8440\n",
      "Training Epoch: 7 [36544/49669]\tLoss: 404.8831\n",
      "Training Epoch: 7 [36608/49669]\tLoss: 434.1875\n",
      "Training Epoch: 7 [36672/49669]\tLoss: 443.4321\n",
      "Training Epoch: 7 [36736/49669]\tLoss: 406.1042\n",
      "Training Epoch: 7 [36800/49669]\tLoss: 428.8435\n",
      "Training Epoch: 7 [36864/49669]\tLoss: 407.5217\n",
      "Training Epoch: 7 [36928/49669]\tLoss: 402.6911\n",
      "Training Epoch: 7 [36992/49669]\tLoss: 420.5087\n",
      "Training Epoch: 7 [37056/49669]\tLoss: 421.7011\n",
      "Training Epoch: 7 [37120/49669]\tLoss: 417.9486\n",
      "Training Epoch: 7 [37184/49669]\tLoss: 441.9587\n",
      "Training Epoch: 7 [37248/49669]\tLoss: 419.5463\n",
      "Training Epoch: 7 [37312/49669]\tLoss: 421.0786\n",
      "Training Epoch: 7 [37376/49669]\tLoss: 429.7789\n",
      "Training Epoch: 7 [37440/49669]\tLoss: 434.8611\n",
      "Training Epoch: 7 [37504/49669]\tLoss: 396.9576\n",
      "Training Epoch: 7 [37568/49669]\tLoss: 426.5649\n",
      "Training Epoch: 7 [37632/49669]\tLoss: 437.7894\n",
      "Training Epoch: 7 [37696/49669]\tLoss: 419.2308\n",
      "Training Epoch: 7 [37760/49669]\tLoss: 407.7033\n",
      "Training Epoch: 7 [37824/49669]\tLoss: 400.8394\n",
      "Training Epoch: 7 [37888/49669]\tLoss: 430.2746\n",
      "Training Epoch: 7 [37952/49669]\tLoss: 432.9738\n",
      "Training Epoch: 7 [38016/49669]\tLoss: 416.0764\n",
      "Training Epoch: 7 [38080/49669]\tLoss: 413.5468\n",
      "Training Epoch: 7 [38144/49669]\tLoss: 405.8709\n",
      "Training Epoch: 7 [38208/49669]\tLoss: 422.9792\n",
      "Training Epoch: 7 [38272/49669]\tLoss: 419.8806\n",
      "Training Epoch: 7 [38336/49669]\tLoss: 426.1838\n",
      "Training Epoch: 7 [38400/49669]\tLoss: 410.2796\n",
      "Training Epoch: 7 [38464/49669]\tLoss: 437.3715\n",
      "Training Epoch: 7 [38528/49669]\tLoss: 433.5089\n",
      "Training Epoch: 7 [38592/49669]\tLoss: 405.1153\n",
      "Training Epoch: 7 [38656/49669]\tLoss: 434.9142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [38720/49669]\tLoss: 399.8557\n",
      "Training Epoch: 7 [38784/49669]\tLoss: 419.3862\n",
      "Training Epoch: 7 [38848/49669]\tLoss: 459.5547\n",
      "Training Epoch: 7 [38912/49669]\tLoss: 445.9420\n",
      "Training Epoch: 7 [38976/49669]\tLoss: 420.7378\n",
      "Training Epoch: 7 [39040/49669]\tLoss: 388.0637\n",
      "Training Epoch: 7 [39104/49669]\tLoss: 406.4242\n",
      "Training Epoch: 7 [39168/49669]\tLoss: 401.3154\n",
      "Training Epoch: 7 [39232/49669]\tLoss: 413.6081\n",
      "Training Epoch: 7 [39296/49669]\tLoss: 427.2444\n",
      "Training Epoch: 7 [39360/49669]\tLoss: 433.1341\n",
      "Training Epoch: 7 [39424/49669]\tLoss: 409.6306\n",
      "Training Epoch: 7 [39488/49669]\tLoss: 411.7040\n",
      "Training Epoch: 7 [39552/49669]\tLoss: 401.1026\n",
      "Training Epoch: 7 [39616/49669]\tLoss: 451.9536\n",
      "Training Epoch: 7 [39680/49669]\tLoss: 420.1522\n",
      "Training Epoch: 7 [39744/49669]\tLoss: 408.8505\n",
      "Training Epoch: 7 [39808/49669]\tLoss: 419.2971\n",
      "Training Epoch: 7 [39872/49669]\tLoss: 419.3889\n",
      "Training Epoch: 7 [39936/49669]\tLoss: 418.1071\n",
      "Training Epoch: 7 [40000/49669]\tLoss: 412.3645\n",
      "Training Epoch: 7 [40064/49669]\tLoss: 423.7601\n",
      "Training Epoch: 7 [40128/49669]\tLoss: 417.2145\n",
      "Training Epoch: 7 [40192/49669]\tLoss: 424.6981\n",
      "Training Epoch: 7 [40256/49669]\tLoss: 419.3696\n",
      "Training Epoch: 7 [40320/49669]\tLoss: 428.7409\n",
      "Training Epoch: 7 [40384/49669]\tLoss: 389.8408\n",
      "Training Epoch: 7 [40448/49669]\tLoss: 420.4335\n",
      "Training Epoch: 7 [40512/49669]\tLoss: 411.9544\n",
      "Training Epoch: 7 [40576/49669]\tLoss: 425.1744\n",
      "Training Epoch: 7 [40640/49669]\tLoss: 416.0633\n",
      "Training Epoch: 7 [40704/49669]\tLoss: 424.0738\n",
      "Training Epoch: 7 [40768/49669]\tLoss: 405.5117\n",
      "Training Epoch: 7 [40832/49669]\tLoss: 425.0206\n",
      "Training Epoch: 7 [40896/49669]\tLoss: 443.3264\n",
      "Training Epoch: 7 [40960/49669]\tLoss: 421.6502\n",
      "Training Epoch: 7 [41024/49669]\tLoss: 412.4262\n",
      "Training Epoch: 7 [41088/49669]\tLoss: 426.5908\n",
      "Training Epoch: 7 [41152/49669]\tLoss: 371.5309\n",
      "Training Epoch: 7 [41216/49669]\tLoss: 419.9786\n",
      "Training Epoch: 7 [41280/49669]\tLoss: 427.6943\n",
      "Training Epoch: 7 [41344/49669]\tLoss: 432.9655\n",
      "Training Epoch: 7 [41408/49669]\tLoss: 419.4019\n",
      "Training Epoch: 7 [41472/49669]\tLoss: 424.6620\n",
      "Training Epoch: 7 [41536/49669]\tLoss: 438.6962\n",
      "Training Epoch: 7 [41600/49669]\tLoss: 420.5780\n",
      "Training Epoch: 7 [41664/49669]\tLoss: 385.3431\n",
      "Training Epoch: 7 [41728/49669]\tLoss: 421.1284\n",
      "Training Epoch: 7 [41792/49669]\tLoss: 390.1591\n",
      "Training Epoch: 7 [41856/49669]\tLoss: 394.3613\n",
      "Training Epoch: 7 [41920/49669]\tLoss: 421.3911\n",
      "Training Epoch: 7 [41984/49669]\tLoss: 390.2204\n",
      "Training Epoch: 7 [42048/49669]\tLoss: 442.5859\n",
      "Training Epoch: 7 [42112/49669]\tLoss: 425.0351\n",
      "Training Epoch: 7 [42176/49669]\tLoss: 427.2295\n",
      "Training Epoch: 7 [42240/49669]\tLoss: 398.0774\n",
      "Training Epoch: 7 [42304/49669]\tLoss: 429.0870\n",
      "Training Epoch: 7 [42368/49669]\tLoss: 422.6862\n",
      "Training Epoch: 7 [42432/49669]\tLoss: 440.3544\n",
      "Training Epoch: 7 [42496/49669]\tLoss: 453.2878\n",
      "Training Epoch: 7 [42560/49669]\tLoss: 414.9844\n",
      "Training Epoch: 7 [42624/49669]\tLoss: 409.5141\n",
      "Training Epoch: 7 [42688/49669]\tLoss: 408.1275\n",
      "Training Epoch: 7 [42752/49669]\tLoss: 415.9567\n",
      "Training Epoch: 7 [42816/49669]\tLoss: 424.3616\n",
      "Training Epoch: 7 [42880/49669]\tLoss: 439.5696\n",
      "Training Epoch: 7 [42944/49669]\tLoss: 406.1182\n",
      "Training Epoch: 7 [43008/49669]\tLoss: 412.3932\n",
      "Training Epoch: 7 [43072/49669]\tLoss: 396.6079\n",
      "Training Epoch: 7 [43136/49669]\tLoss: 437.7704\n",
      "Training Epoch: 7 [43200/49669]\tLoss: 396.7652\n",
      "Training Epoch: 7 [43264/49669]\tLoss: 425.2417\n",
      "Training Epoch: 7 [43328/49669]\tLoss: 400.7761\n",
      "Training Epoch: 7 [43392/49669]\tLoss: 433.7288\n",
      "Training Epoch: 7 [43456/49669]\tLoss: 425.8966\n",
      "Training Epoch: 7 [43520/49669]\tLoss: 375.2294\n",
      "Training Epoch: 7 [43584/49669]\tLoss: 431.8037\n",
      "Training Epoch: 7 [43648/49669]\tLoss: 437.3742\n",
      "Training Epoch: 7 [43712/49669]\tLoss: 419.2299\n",
      "Training Epoch: 7 [43776/49669]\tLoss: 459.7611\n",
      "Training Epoch: 7 [43840/49669]\tLoss: 398.7401\n",
      "Training Epoch: 7 [43904/49669]\tLoss: 437.6306\n",
      "Training Epoch: 7 [43968/49669]\tLoss: 410.3646\n",
      "Training Epoch: 7 [44032/49669]\tLoss: 427.7685\n",
      "Training Epoch: 7 [44096/49669]\tLoss: 434.5041\n",
      "Training Epoch: 7 [44160/49669]\tLoss: 424.0692\n",
      "Training Epoch: 7 [44224/49669]\tLoss: 420.4852\n",
      "Training Epoch: 7 [44288/49669]\tLoss: 404.6170\n",
      "Training Epoch: 7 [44352/49669]\tLoss: 442.9649\n",
      "Training Epoch: 7 [44416/49669]\tLoss: 401.8808\n",
      "Training Epoch: 7 [44480/49669]\tLoss: 408.3156\n",
      "Training Epoch: 7 [44544/49669]\tLoss: 409.6331\n",
      "Training Epoch: 7 [44608/49669]\tLoss: 436.6695\n",
      "Training Epoch: 7 [44672/49669]\tLoss: 438.3760\n",
      "Training Epoch: 7 [44736/49669]\tLoss: 438.3149\n",
      "Training Epoch: 7 [44800/49669]\tLoss: 414.3937\n",
      "Training Epoch: 7 [44864/49669]\tLoss: 417.0656\n",
      "Training Epoch: 7 [44928/49669]\tLoss: 421.8769\n",
      "Training Epoch: 7 [44992/49669]\tLoss: 375.5827\n",
      "Training Epoch: 7 [45056/49669]\tLoss: 426.0089\n",
      "Training Epoch: 7 [45120/49669]\tLoss: 402.6554\n",
      "Training Epoch: 7 [45184/49669]\tLoss: 407.3237\n",
      "Training Epoch: 7 [45248/49669]\tLoss: 431.1287\n",
      "Training Epoch: 7 [45312/49669]\tLoss: 402.6039\n",
      "Training Epoch: 7 [45376/49669]\tLoss: 453.2553\n",
      "Training Epoch: 7 [45440/49669]\tLoss: 430.7126\n",
      "Training Epoch: 7 [45504/49669]\tLoss: 394.9442\n",
      "Training Epoch: 7 [45568/49669]\tLoss: 402.7794\n",
      "Training Epoch: 7 [45632/49669]\tLoss: 401.7333\n",
      "Training Epoch: 7 [45696/49669]\tLoss: 419.7087\n",
      "Training Epoch: 7 [45760/49669]\tLoss: 410.4726\n",
      "Training Epoch: 7 [45824/49669]\tLoss: 403.9087\n",
      "Training Epoch: 7 [45888/49669]\tLoss: 403.8834\n",
      "Training Epoch: 7 [45952/49669]\tLoss: 443.7693\n",
      "Training Epoch: 7 [46016/49669]\tLoss: 414.7935\n",
      "Training Epoch: 7 [46080/49669]\tLoss: 410.9256\n",
      "Training Epoch: 7 [46144/49669]\tLoss: 449.9376\n",
      "Training Epoch: 7 [46208/49669]\tLoss: 389.4323\n",
      "Training Epoch: 7 [46272/49669]\tLoss: 414.0811\n",
      "Training Epoch: 7 [46336/49669]\tLoss: 411.0635\n",
      "Training Epoch: 7 [46400/49669]\tLoss: 402.4679\n",
      "Training Epoch: 7 [46464/49669]\tLoss: 449.7267\n",
      "Training Epoch: 7 [46528/49669]\tLoss: 381.5658\n",
      "Training Epoch: 7 [46592/49669]\tLoss: 433.6493\n",
      "Training Epoch: 7 [46656/49669]\tLoss: 430.6411\n",
      "Training Epoch: 7 [46720/49669]\tLoss: 430.9356\n",
      "Training Epoch: 7 [46784/49669]\tLoss: 444.1351\n",
      "Training Epoch: 7 [46848/49669]\tLoss: 414.6314\n",
      "Training Epoch: 7 [46912/49669]\tLoss: 388.4932\n",
      "Training Epoch: 7 [46976/49669]\tLoss: 417.4523\n",
      "Training Epoch: 7 [47040/49669]\tLoss: 427.1132\n",
      "Training Epoch: 7 [47104/49669]\tLoss: 399.8524\n",
      "Training Epoch: 7 [47168/49669]\tLoss: 405.2288\n",
      "Training Epoch: 7 [47232/49669]\tLoss: 400.8992\n",
      "Training Epoch: 7 [47296/49669]\tLoss: 443.1710\n",
      "Training Epoch: 7 [47360/49669]\tLoss: 413.4307\n",
      "Training Epoch: 7 [47424/49669]\tLoss: 458.4245\n",
      "Training Epoch: 7 [47488/49669]\tLoss: 379.2475\n",
      "Training Epoch: 7 [47552/49669]\tLoss: 378.3024\n",
      "Training Epoch: 7 [47616/49669]\tLoss: 430.1788\n",
      "Training Epoch: 7 [47680/49669]\tLoss: 399.5185\n",
      "Training Epoch: 7 [47744/49669]\tLoss: 446.6456\n",
      "Training Epoch: 7 [47808/49669]\tLoss: 401.9815\n",
      "Training Epoch: 7 [47872/49669]\tLoss: 437.7046\n",
      "Training Epoch: 7 [47936/49669]\tLoss: 411.6999\n",
      "Training Epoch: 7 [48000/49669]\tLoss: 422.8777\n",
      "Training Epoch: 7 [48064/49669]\tLoss: 441.4295\n",
      "Training Epoch: 7 [48128/49669]\tLoss: 431.9299\n",
      "Training Epoch: 7 [48192/49669]\tLoss: 486.9883\n",
      "Training Epoch: 7 [48256/49669]\tLoss: 445.0671\n",
      "Training Epoch: 7 [48320/49669]\tLoss: 525.3445\n",
      "Training Epoch: 7 [48384/49669]\tLoss: 513.1104\n",
      "Training Epoch: 7 [48448/49669]\tLoss: 507.1336\n",
      "Training Epoch: 7 [48512/49669]\tLoss: 487.7132\n",
      "Training Epoch: 7 [48576/49669]\tLoss: 418.3712\n",
      "Training Epoch: 7 [48640/49669]\tLoss: 423.6806\n",
      "Training Epoch: 7 [48704/49669]\tLoss: 449.8462\n",
      "Training Epoch: 7 [48768/49669]\tLoss: 509.6703\n",
      "Training Epoch: 7 [48832/49669]\tLoss: 488.2191\n",
      "Training Epoch: 7 [48896/49669]\tLoss: 456.8312\n",
      "Training Epoch: 7 [48960/49669]\tLoss: 419.9600\n",
      "Training Epoch: 7 [49024/49669]\tLoss: 446.9997\n",
      "Training Epoch: 7 [49088/49669]\tLoss: 444.2168\n",
      "Training Epoch: 7 [49152/49669]\tLoss: 436.5306\n",
      "Training Epoch: 7 [49216/49669]\tLoss: 426.9384\n",
      "Training Epoch: 7 [49280/49669]\tLoss: 417.0390\n",
      "Training Epoch: 7 [49344/49669]\tLoss: 398.3094\n",
      "Training Epoch: 7 [49408/49669]\tLoss: 431.6190\n",
      "Training Epoch: 7 [49472/49669]\tLoss: 420.4277\n",
      "Training Epoch: 7 [49536/49669]\tLoss: 410.7391\n",
      "Training Epoch: 7 [49600/49669]\tLoss: 398.7074\n",
      "Training Epoch: 7 [49664/49669]\tLoss: 449.1658\n",
      "Training Epoch: 7 [49669/49669]\tLoss: 515.3761\n",
      "Training Epoch: 7 [5519/5519]\tLoss: 463.7970\n",
      "Training Epoch: 8 [64/49669]\tLoss: 472.3561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [128/49669]\tLoss: 442.0367\n",
      "Training Epoch: 8 [192/49669]\tLoss: 405.7708\n",
      "Training Epoch: 8 [256/49669]\tLoss: 419.5688\n",
      "Training Epoch: 8 [320/49669]\tLoss: 459.1204\n",
      "Training Epoch: 8 [384/49669]\tLoss: 424.7303\n",
      "Training Epoch: 8 [448/49669]\tLoss: 439.3214\n",
      "Training Epoch: 8 [512/49669]\tLoss: 416.0331\n",
      "Training Epoch: 8 [576/49669]\tLoss: 407.5915\n",
      "Training Epoch: 8 [640/49669]\tLoss: 399.6945\n",
      "Training Epoch: 8 [704/49669]\tLoss: 438.6703\n",
      "Training Epoch: 8 [768/49669]\tLoss: 430.1521\n",
      "Training Epoch: 8 [832/49669]\tLoss: 419.9922\n",
      "Training Epoch: 8 [896/49669]\tLoss: 409.1457\n",
      "Training Epoch: 8 [960/49669]\tLoss: 437.6995\n",
      "Training Epoch: 8 [1024/49669]\tLoss: 369.1020\n",
      "Training Epoch: 8 [1088/49669]\tLoss: 403.8547\n",
      "Training Epoch: 8 [1152/49669]\tLoss: 418.5319\n",
      "Training Epoch: 8 [1216/49669]\tLoss: 430.1023\n",
      "Training Epoch: 8 [1280/49669]\tLoss: 417.5107\n",
      "Training Epoch: 8 [1344/49669]\tLoss: 403.6050\n",
      "Training Epoch: 8 [1408/49669]\tLoss: 415.4725\n",
      "Training Epoch: 8 [1472/49669]\tLoss: 409.2740\n",
      "Training Epoch: 8 [1536/49669]\tLoss: 402.9695\n",
      "Training Epoch: 8 [1600/49669]\tLoss: 418.5007\n",
      "Training Epoch: 8 [1664/49669]\tLoss: 430.7140\n",
      "Training Epoch: 8 [1728/49669]\tLoss: 423.1204\n",
      "Training Epoch: 8 [1792/49669]\tLoss: 414.0378\n",
      "Training Epoch: 8 [1856/49669]\tLoss: 446.4551\n",
      "Training Epoch: 8 [1920/49669]\tLoss: 444.0674\n",
      "Training Epoch: 8 [1984/49669]\tLoss: 378.3098\n",
      "Training Epoch: 8 [2048/49669]\tLoss: 403.7683\n",
      "Training Epoch: 8 [2112/49669]\tLoss: 412.3789\n",
      "Training Epoch: 8 [2176/49669]\tLoss: 408.5749\n",
      "Training Epoch: 8 [2240/49669]\tLoss: 404.2480\n",
      "Training Epoch: 8 [2304/49669]\tLoss: 411.8094\n",
      "Training Epoch: 8 [2368/49669]\tLoss: 440.4052\n",
      "Training Epoch: 8 [2432/49669]\tLoss: 419.3578\n",
      "Training Epoch: 8 [2496/49669]\tLoss: 404.8882\n",
      "Training Epoch: 8 [2560/49669]\tLoss: 422.2732\n",
      "Training Epoch: 8 [2624/49669]\tLoss: 394.3906\n",
      "Training Epoch: 8 [2688/49669]\tLoss: 432.7621\n",
      "Training Epoch: 8 [2752/49669]\tLoss: 453.4693\n",
      "Training Epoch: 8 [2816/49669]\tLoss: 405.9893\n",
      "Training Epoch: 8 [2880/49669]\tLoss: 395.8451\n",
      "Training Epoch: 8 [2944/49669]\tLoss: 442.3578\n",
      "Training Epoch: 8 [3008/49669]\tLoss: 435.5133\n",
      "Training Epoch: 8 [3072/49669]\tLoss: 435.2953\n",
      "Training Epoch: 8 [3136/49669]\tLoss: 428.4804\n",
      "Training Epoch: 8 [3200/49669]\tLoss: 382.7693\n",
      "Training Epoch: 8 [3264/49669]\tLoss: 408.7431\n",
      "Training Epoch: 8 [3328/49669]\tLoss: 449.4473\n",
      "Training Epoch: 8 [3392/49669]\tLoss: 431.9673\n",
      "Training Epoch: 8 [3456/49669]\tLoss: 409.8661\n",
      "Training Epoch: 8 [3520/49669]\tLoss: 415.5474\n",
      "Training Epoch: 8 [3584/49669]\tLoss: 435.0073\n",
      "Training Epoch: 8 [3648/49669]\tLoss: 414.8247\n",
      "Training Epoch: 8 [3712/49669]\tLoss: 409.0091\n",
      "Training Epoch: 8 [3776/49669]\tLoss: 415.2105\n",
      "Training Epoch: 8 [3840/49669]\tLoss: 385.8950\n",
      "Training Epoch: 8 [3904/49669]\tLoss: 445.1784\n",
      "Training Epoch: 8 [3968/49669]\tLoss: 371.2179\n",
      "Training Epoch: 8 [4032/49669]\tLoss: 407.8710\n",
      "Training Epoch: 8 [4096/49669]\tLoss: 413.0040\n",
      "Training Epoch: 8 [4160/49669]\tLoss: 394.4786\n",
      "Training Epoch: 8 [4224/49669]\tLoss: 401.5566\n",
      "Training Epoch: 8 [4288/49669]\tLoss: 420.2849\n",
      "Training Epoch: 8 [4352/49669]\tLoss: 450.6350\n",
      "Training Epoch: 8 [4416/49669]\tLoss: 409.8589\n",
      "Training Epoch: 8 [4480/49669]\tLoss: 387.9321\n",
      "Training Epoch: 8 [4544/49669]\tLoss: 438.6733\n",
      "Training Epoch: 8 [4608/49669]\tLoss: 438.2494\n",
      "Training Epoch: 8 [4672/49669]\tLoss: 407.7729\n",
      "Training Epoch: 8 [4736/49669]\tLoss: 416.6177\n",
      "Training Epoch: 8 [4800/49669]\tLoss: 443.2509\n",
      "Training Epoch: 8 [4864/49669]\tLoss: 430.4408\n",
      "Training Epoch: 8 [4928/49669]\tLoss: 393.9835\n",
      "Training Epoch: 8 [4992/49669]\tLoss: 411.8221\n",
      "Training Epoch: 8 [5056/49669]\tLoss: 424.3874\n",
      "Training Epoch: 8 [5120/49669]\tLoss: 420.5704\n",
      "Training Epoch: 8 [5184/49669]\tLoss: 401.4143\n",
      "Training Epoch: 8 [5248/49669]\tLoss: 386.9096\n",
      "Training Epoch: 8 [5312/49669]\tLoss: 418.1393\n",
      "Training Epoch: 8 [5376/49669]\tLoss: 397.9097\n",
      "Training Epoch: 8 [5440/49669]\tLoss: 451.5474\n",
      "Training Epoch: 8 [5504/49669]\tLoss: 414.1661\n",
      "Training Epoch: 8 [5568/49669]\tLoss: 417.6895\n",
      "Training Epoch: 8 [5632/49669]\tLoss: 420.3615\n",
      "Training Epoch: 8 [5696/49669]\tLoss: 381.4279\n",
      "Training Epoch: 8 [5760/49669]\tLoss: 401.2157\n",
      "Training Epoch: 8 [5824/49669]\tLoss: 421.1293\n",
      "Training Epoch: 8 [5888/49669]\tLoss: 379.9003\n",
      "Training Epoch: 8 [5952/49669]\tLoss: 412.8488\n",
      "Training Epoch: 8 [6016/49669]\tLoss: 414.5061\n",
      "Training Epoch: 8 [6080/49669]\tLoss: 432.4897\n",
      "Training Epoch: 8 [6144/49669]\tLoss: 432.5328\n",
      "Training Epoch: 8 [6208/49669]\tLoss: 402.5748\n",
      "Training Epoch: 8 [6272/49669]\tLoss: 395.9248\n",
      "Training Epoch: 8 [6336/49669]\tLoss: 407.5137\n",
      "Training Epoch: 8 [6400/49669]\tLoss: 413.7922\n",
      "Training Epoch: 8 [6464/49669]\tLoss: 439.9017\n",
      "Training Epoch: 8 [6528/49669]\tLoss: 382.4744\n",
      "Training Epoch: 8 [6592/49669]\tLoss: 391.2242\n",
      "Training Epoch: 8 [6656/49669]\tLoss: 377.9146\n",
      "Training Epoch: 8 [6720/49669]\tLoss: 416.2973\n",
      "Training Epoch: 8 [6784/49669]\tLoss: 409.2320\n",
      "Training Epoch: 8 [6848/49669]\tLoss: 412.8890\n",
      "Training Epoch: 8 [6912/49669]\tLoss: 431.0648\n",
      "Training Epoch: 8 [6976/49669]\tLoss: 404.7495\n",
      "Training Epoch: 8 [7040/49669]\tLoss: 395.8510\n",
      "Training Epoch: 8 [7104/49669]\tLoss: 394.9991\n",
      "Training Epoch: 8 [7168/49669]\tLoss: 415.2016\n",
      "Training Epoch: 8 [7232/49669]\tLoss: 448.2730\n",
      "Training Epoch: 8 [7296/49669]\tLoss: 417.9670\n",
      "Training Epoch: 8 [7360/49669]\tLoss: 412.8033\n",
      "Training Epoch: 8 [7424/49669]\tLoss: 418.4322\n",
      "Training Epoch: 8 [7488/49669]\tLoss: 429.6593\n",
      "Training Epoch: 8 [7552/49669]\tLoss: 407.1587\n",
      "Training Epoch: 8 [7616/49669]\tLoss: 404.1088\n",
      "Training Epoch: 8 [7680/49669]\tLoss: 410.3280\n",
      "Training Epoch: 8 [7744/49669]\tLoss: 426.2083\n",
      "Training Epoch: 8 [7808/49669]\tLoss: 432.0328\n",
      "Training Epoch: 8 [7872/49669]\tLoss: 419.1175\n",
      "Training Epoch: 8 [7936/49669]\tLoss: 441.9940\n",
      "Training Epoch: 8 [8000/49669]\tLoss: 398.3740\n",
      "Training Epoch: 8 [8064/49669]\tLoss: 440.9543\n",
      "Training Epoch: 8 [8128/49669]\tLoss: 406.8693\n",
      "Training Epoch: 8 [8192/49669]\tLoss: 411.0833\n",
      "Training Epoch: 8 [8256/49669]\tLoss: 410.8502\n",
      "Training Epoch: 8 [8320/49669]\tLoss: 467.5364\n",
      "Training Epoch: 8 [8384/49669]\tLoss: 430.7586\n",
      "Training Epoch: 8 [8448/49669]\tLoss: 418.5485\n",
      "Training Epoch: 8 [8512/49669]\tLoss: 444.0781\n",
      "Training Epoch: 8 [8576/49669]\tLoss: 415.6638\n",
      "Training Epoch: 8 [8640/49669]\tLoss: 410.4859\n",
      "Training Epoch: 8 [8704/49669]\tLoss: 406.2335\n",
      "Training Epoch: 8 [8768/49669]\tLoss: 422.8463\n",
      "Training Epoch: 8 [8832/49669]\tLoss: 391.7722\n",
      "Training Epoch: 8 [8896/49669]\tLoss: 401.5640\n",
      "Training Epoch: 8 [8960/49669]\tLoss: 447.2453\n",
      "Training Epoch: 8 [9024/49669]\tLoss: 406.5491\n",
      "Training Epoch: 8 [9088/49669]\tLoss: 420.4079\n",
      "Training Epoch: 8 [9152/49669]\tLoss: 423.2528\n",
      "Training Epoch: 8 [9216/49669]\tLoss: 446.2917\n",
      "Training Epoch: 8 [9280/49669]\tLoss: 411.3034\n",
      "Training Epoch: 8 [9344/49669]\tLoss: 446.5444\n",
      "Training Epoch: 8 [9408/49669]\tLoss: 412.1165\n",
      "Training Epoch: 8 [9472/49669]\tLoss: 407.2891\n",
      "Training Epoch: 8 [9536/49669]\tLoss: 419.9880\n",
      "Training Epoch: 8 [9600/49669]\tLoss: 428.2762\n",
      "Training Epoch: 8 [9664/49669]\tLoss: 405.8926\n",
      "Training Epoch: 8 [9728/49669]\tLoss: 419.1250\n",
      "Training Epoch: 8 [9792/49669]\tLoss: 413.1241\n",
      "Training Epoch: 8 [9856/49669]\tLoss: 415.2664\n",
      "Training Epoch: 8 [9920/49669]\tLoss: 406.9493\n",
      "Training Epoch: 8 [9984/49669]\tLoss: 393.5537\n",
      "Training Epoch: 8 [10048/49669]\tLoss: 416.7656\n",
      "Training Epoch: 8 [10112/49669]\tLoss: 424.2328\n",
      "Training Epoch: 8 [10176/49669]\tLoss: 418.9532\n",
      "Training Epoch: 8 [10240/49669]\tLoss: 411.9004\n",
      "Training Epoch: 8 [10304/49669]\tLoss: 387.9911\n",
      "Training Epoch: 8 [10368/49669]\tLoss: 436.4472\n",
      "Training Epoch: 8 [10432/49669]\tLoss: 398.1723\n",
      "Training Epoch: 8 [10496/49669]\tLoss: 429.3079\n",
      "Training Epoch: 8 [10560/49669]\tLoss: 421.4174\n",
      "Training Epoch: 8 [10624/49669]\tLoss: 428.6490\n",
      "Training Epoch: 8 [10688/49669]\tLoss: 437.7490\n",
      "Training Epoch: 8 [10752/49669]\tLoss: 458.8970\n",
      "Training Epoch: 8 [10816/49669]\tLoss: 412.3440\n",
      "Training Epoch: 8 [10880/49669]\tLoss: 401.4022\n",
      "Training Epoch: 8 [10944/49669]\tLoss: 420.5801\n",
      "Training Epoch: 8 [11008/49669]\tLoss: 375.1753\n",
      "Training Epoch: 8 [11072/49669]\tLoss: 412.7900\n",
      "Training Epoch: 8 [11136/49669]\tLoss: 387.8873\n",
      "Training Epoch: 8 [11200/49669]\tLoss: 415.4484\n",
      "Training Epoch: 8 [11264/49669]\tLoss: 411.1372\n",
      "Training Epoch: 8 [11328/49669]\tLoss: 404.8902\n",
      "Training Epoch: 8 [11392/49669]\tLoss: 415.5311\n",
      "Training Epoch: 8 [11456/49669]\tLoss: 410.8719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [11520/49669]\tLoss: 404.0154\n",
      "Training Epoch: 8 [11584/49669]\tLoss: 417.6203\n",
      "Training Epoch: 8 [11648/49669]\tLoss: 412.6775\n",
      "Training Epoch: 8 [11712/49669]\tLoss: 427.8027\n",
      "Training Epoch: 8 [11776/49669]\tLoss: 424.6575\n",
      "Training Epoch: 8 [11840/49669]\tLoss: 382.5258\n",
      "Training Epoch: 8 [11904/49669]\tLoss: 398.6190\n",
      "Training Epoch: 8 [11968/49669]\tLoss: 429.4963\n",
      "Training Epoch: 8 [12032/49669]\tLoss: 436.0565\n",
      "Training Epoch: 8 [12096/49669]\tLoss: 420.2625\n",
      "Training Epoch: 8 [12160/49669]\tLoss: 394.3867\n",
      "Training Epoch: 8 [12224/49669]\tLoss: 437.8772\n",
      "Training Epoch: 8 [12288/49669]\tLoss: 430.3893\n",
      "Training Epoch: 8 [12352/49669]\tLoss: 403.8731\n",
      "Training Epoch: 8 [12416/49669]\tLoss: 410.6255\n",
      "Training Epoch: 8 [12480/49669]\tLoss: 438.6749\n",
      "Training Epoch: 8 [12544/49669]\tLoss: 420.6833\n",
      "Training Epoch: 8 [12608/49669]\tLoss: 407.3534\n",
      "Training Epoch: 8 [12672/49669]\tLoss: 430.0269\n",
      "Training Epoch: 8 [12736/49669]\tLoss: 420.5157\n",
      "Training Epoch: 8 [12800/49669]\tLoss: 371.6452\n",
      "Training Epoch: 8 [12864/49669]\tLoss: 417.8871\n",
      "Training Epoch: 8 [12928/49669]\tLoss: 428.0466\n",
      "Training Epoch: 8 [12992/49669]\tLoss: 431.2308\n",
      "Training Epoch: 8 [13056/49669]\tLoss: 419.7321\n",
      "Training Epoch: 8 [13120/49669]\tLoss: 413.3195\n",
      "Training Epoch: 8 [13184/49669]\tLoss: 433.4491\n",
      "Training Epoch: 8 [13248/49669]\tLoss: 406.3429\n",
      "Training Epoch: 8 [13312/49669]\tLoss: 441.8566\n",
      "Training Epoch: 8 [13376/49669]\tLoss: 379.3111\n",
      "Training Epoch: 8 [13440/49669]\tLoss: 412.7596\n",
      "Training Epoch: 8 [13504/49669]\tLoss: 402.0116\n",
      "Training Epoch: 8 [13568/49669]\tLoss: 425.7715\n",
      "Training Epoch: 8 [13632/49669]\tLoss: 408.4780\n",
      "Training Epoch: 8 [13696/49669]\tLoss: 397.9665\n",
      "Training Epoch: 8 [13760/49669]\tLoss: 425.5580\n",
      "Training Epoch: 8 [13824/49669]\tLoss: 420.2309\n",
      "Training Epoch: 8 [13888/49669]\tLoss: 388.7508\n",
      "Training Epoch: 8 [13952/49669]\tLoss: 392.4360\n",
      "Training Epoch: 8 [14016/49669]\tLoss: 386.7453\n",
      "Training Epoch: 8 [14080/49669]\tLoss: 410.5941\n",
      "Training Epoch: 8 [14144/49669]\tLoss: 379.0442\n",
      "Training Epoch: 8 [14208/49669]\tLoss: 421.1474\n",
      "Training Epoch: 8 [14272/49669]\tLoss: 408.1916\n",
      "Training Epoch: 8 [14336/49669]\tLoss: 387.8460\n",
      "Training Epoch: 8 [14400/49669]\tLoss: 397.1041\n",
      "Training Epoch: 8 [14464/49669]\tLoss: 435.2256\n",
      "Training Epoch: 8 [14528/49669]\tLoss: 393.1779\n",
      "Training Epoch: 8 [14592/49669]\tLoss: 435.8809\n",
      "Training Epoch: 8 [14656/49669]\tLoss: 419.1148\n",
      "Training Epoch: 8 [14720/49669]\tLoss: 435.4360\n",
      "Training Epoch: 8 [14784/49669]\tLoss: 389.5895\n",
      "Training Epoch: 8 [14848/49669]\tLoss: 409.2177\n",
      "Training Epoch: 8 [14912/49669]\tLoss: 420.1711\n",
      "Training Epoch: 8 [14976/49669]\tLoss: 430.5166\n",
      "Training Epoch: 8 [15040/49669]\tLoss: 416.6880\n",
      "Training Epoch: 8 [15104/49669]\tLoss: 454.5418\n",
      "Training Epoch: 8 [15168/49669]\tLoss: 420.5681\n",
      "Training Epoch: 8 [15232/49669]\tLoss: 417.1754\n",
      "Training Epoch: 8 [15296/49669]\tLoss: 377.7752\n",
      "Training Epoch: 8 [15360/49669]\tLoss: 436.7315\n",
      "Training Epoch: 8 [15424/49669]\tLoss: 396.6647\n",
      "Training Epoch: 8 [15488/49669]\tLoss: 407.8477\n",
      "Training Epoch: 8 [15552/49669]\tLoss: 412.7693\n",
      "Training Epoch: 8 [15616/49669]\tLoss: 431.8562\n",
      "Training Epoch: 8 [15680/49669]\tLoss: 441.8352\n",
      "Training Epoch: 8 [15744/49669]\tLoss: 427.9893\n",
      "Training Epoch: 8 [15808/49669]\tLoss: 392.1819\n",
      "Training Epoch: 8 [15872/49669]\tLoss: 400.0380\n",
      "Training Epoch: 8 [15936/49669]\tLoss: 425.9299\n",
      "Training Epoch: 8 [16000/49669]\tLoss: 449.5381\n",
      "Training Epoch: 8 [16064/49669]\tLoss: 432.3108\n",
      "Training Epoch: 8 [16128/49669]\tLoss: 424.8245\n",
      "Training Epoch: 8 [16192/49669]\tLoss: 405.9630\n",
      "Training Epoch: 8 [16256/49669]\tLoss: 401.9038\n",
      "Training Epoch: 8 [16320/49669]\tLoss: 415.3944\n",
      "Training Epoch: 8 [16384/49669]\tLoss: 446.1184\n",
      "Training Epoch: 8 [16448/49669]\tLoss: 375.3131\n",
      "Training Epoch: 8 [16512/49669]\tLoss: 423.1864\n",
      "Training Epoch: 8 [16576/49669]\tLoss: 425.3684\n",
      "Training Epoch: 8 [16640/49669]\tLoss: 420.9706\n",
      "Training Epoch: 8 [16704/49669]\tLoss: 419.6152\n",
      "Training Epoch: 8 [16768/49669]\tLoss: 410.5194\n",
      "Training Epoch: 8 [16832/49669]\tLoss: 387.8335\n",
      "Training Epoch: 8 [16896/49669]\tLoss: 409.4682\n",
      "Training Epoch: 8 [16960/49669]\tLoss: 446.1179\n",
      "Training Epoch: 8 [17024/49669]\tLoss: 412.5336\n",
      "Training Epoch: 8 [17088/49669]\tLoss: 446.9949\n",
      "Training Epoch: 8 [17152/49669]\tLoss: 427.8084\n",
      "Training Epoch: 8 [17216/49669]\tLoss: 443.5080\n",
      "Training Epoch: 8 [17280/49669]\tLoss: 404.1458\n",
      "Training Epoch: 8 [17344/49669]\tLoss: 409.9836\n",
      "Training Epoch: 8 [17408/49669]\tLoss: 413.9641\n",
      "Training Epoch: 8 [17472/49669]\tLoss: 385.6253\n",
      "Training Epoch: 8 [17536/49669]\tLoss: 402.2955\n",
      "Training Epoch: 8 [17600/49669]\tLoss: 413.4433\n",
      "Training Epoch: 8 [17664/49669]\tLoss: 396.4415\n",
      "Training Epoch: 8 [17728/49669]\tLoss: 435.7694\n",
      "Training Epoch: 8 [17792/49669]\tLoss: 393.3875\n",
      "Training Epoch: 8 [17856/49669]\tLoss: 426.4367\n",
      "Training Epoch: 8 [17920/49669]\tLoss: 429.2571\n",
      "Training Epoch: 8 [17984/49669]\tLoss: 426.9916\n",
      "Training Epoch: 8 [18048/49669]\tLoss: 418.8004\n",
      "Training Epoch: 8 [18112/49669]\tLoss: 436.6197\n",
      "Training Epoch: 8 [18176/49669]\tLoss: 429.6316\n",
      "Training Epoch: 8 [18240/49669]\tLoss: 424.6881\n",
      "Training Epoch: 8 [18304/49669]\tLoss: 432.8684\n",
      "Training Epoch: 8 [18368/49669]\tLoss: 418.2057\n",
      "Training Epoch: 8 [18432/49669]\tLoss: 433.4309\n",
      "Training Epoch: 8 [18496/49669]\tLoss: 416.4097\n",
      "Training Epoch: 8 [18560/49669]\tLoss: 451.9150\n",
      "Training Epoch: 8 [18624/49669]\tLoss: 441.4998\n",
      "Training Epoch: 8 [18688/49669]\tLoss: 422.0681\n",
      "Training Epoch: 8 [18752/49669]\tLoss: 452.9926\n",
      "Training Epoch: 8 [18816/49669]\tLoss: 435.5966\n",
      "Training Epoch: 8 [18880/49669]\tLoss: 406.8024\n",
      "Training Epoch: 8 [18944/49669]\tLoss: 455.8409\n",
      "Training Epoch: 8 [19008/49669]\tLoss: 419.9662\n",
      "Training Epoch: 8 [19072/49669]\tLoss: 420.3076\n",
      "Training Epoch: 8 [19136/49669]\tLoss: 460.4070\n",
      "Training Epoch: 8 [19200/49669]\tLoss: 399.1289\n",
      "Training Epoch: 8 [19264/49669]\tLoss: 417.5582\n",
      "Training Epoch: 8 [19328/49669]\tLoss: 418.4015\n",
      "Training Epoch: 8 [19392/49669]\tLoss: 435.1267\n",
      "Training Epoch: 8 [19456/49669]\tLoss: 395.7276\n",
      "Training Epoch: 8 [19520/49669]\tLoss: 443.7484\n",
      "Training Epoch: 8 [19584/49669]\tLoss: 432.2050\n",
      "Training Epoch: 8 [19648/49669]\tLoss: 405.7603\n",
      "Training Epoch: 8 [19712/49669]\tLoss: 410.4483\n",
      "Training Epoch: 8 [19776/49669]\tLoss: 429.0870\n",
      "Training Epoch: 8 [19840/49669]\tLoss: 442.8258\n",
      "Training Epoch: 8 [19904/49669]\tLoss: 402.7527\n",
      "Training Epoch: 8 [19968/49669]\tLoss: 403.4056\n",
      "Training Epoch: 8 [20032/49669]\tLoss: 409.9084\n",
      "Training Epoch: 8 [20096/49669]\tLoss: 422.4885\n",
      "Training Epoch: 8 [20160/49669]\tLoss: 399.7180\n",
      "Training Epoch: 8 [20224/49669]\tLoss: 433.8961\n",
      "Training Epoch: 8 [20288/49669]\tLoss: 461.3775\n",
      "Training Epoch: 8 [20352/49669]\tLoss: 389.5779\n",
      "Training Epoch: 8 [20416/49669]\tLoss: 386.7994\n",
      "Training Epoch: 8 [20480/49669]\tLoss: 396.0214\n",
      "Training Epoch: 8 [20544/49669]\tLoss: 415.1638\n",
      "Training Epoch: 8 [20608/49669]\tLoss: 414.6619\n",
      "Training Epoch: 8 [20672/49669]\tLoss: 430.6852\n",
      "Training Epoch: 8 [20736/49669]\tLoss: 437.3006\n",
      "Training Epoch: 8 [20800/49669]\tLoss: 423.8639\n",
      "Training Epoch: 8 [20864/49669]\tLoss: 438.1545\n",
      "Training Epoch: 8 [20928/49669]\tLoss: 471.7853\n",
      "Training Epoch: 8 [20992/49669]\tLoss: 467.5823\n",
      "Training Epoch: 8 [21056/49669]\tLoss: 480.2962\n",
      "Training Epoch: 8 [21120/49669]\tLoss: 444.6942\n",
      "Training Epoch: 8 [21184/49669]\tLoss: 430.4149\n",
      "Training Epoch: 8 [21248/49669]\tLoss: 412.5526\n",
      "Training Epoch: 8 [21312/49669]\tLoss: 379.0076\n",
      "Training Epoch: 8 [21376/49669]\tLoss: 454.7465\n",
      "Training Epoch: 8 [21440/49669]\tLoss: 470.2475\n",
      "Training Epoch: 8 [21504/49669]\tLoss: 438.7126\n",
      "Training Epoch: 8 [21568/49669]\tLoss: 491.5513\n",
      "Training Epoch: 8 [21632/49669]\tLoss: 497.8276\n",
      "Training Epoch: 8 [21696/49669]\tLoss: 528.9732\n",
      "Training Epoch: 8 [21760/49669]\tLoss: 519.0402\n",
      "Training Epoch: 8 [21824/49669]\tLoss: 432.7366\n",
      "Training Epoch: 8 [21888/49669]\tLoss: 397.8741\n",
      "Training Epoch: 8 [21952/49669]\tLoss: 441.9669\n",
      "Training Epoch: 8 [22016/49669]\tLoss: 470.6213\n",
      "Training Epoch: 8 [22080/49669]\tLoss: 474.3468\n",
      "Training Epoch: 8 [22144/49669]\tLoss: 467.5896\n",
      "Training Epoch: 8 [22208/49669]\tLoss: 413.5848\n",
      "Training Epoch: 8 [22272/49669]\tLoss: 435.4053\n",
      "Training Epoch: 8 [22336/49669]\tLoss: 471.9090\n",
      "Training Epoch: 8 [22400/49669]\tLoss: 490.0039\n",
      "Training Epoch: 8 [22464/49669]\tLoss: 406.6273\n",
      "Training Epoch: 8 [22528/49669]\tLoss: 451.5996\n",
      "Training Epoch: 8 [22592/49669]\tLoss: 468.8004\n",
      "Training Epoch: 8 [22656/49669]\tLoss: 492.5506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [22720/49669]\tLoss: 467.0517\n",
      "Training Epoch: 8 [22784/49669]\tLoss: 405.6621\n",
      "Training Epoch: 8 [22848/49669]\tLoss: 438.6024\n",
      "Training Epoch: 8 [22912/49669]\tLoss: 452.3278\n",
      "Training Epoch: 8 [22976/49669]\tLoss: 443.3767\n",
      "Training Epoch: 8 [23040/49669]\tLoss: 428.0464\n",
      "Training Epoch: 8 [23104/49669]\tLoss: 415.0926\n",
      "Training Epoch: 8 [23168/49669]\tLoss: 449.0774\n",
      "Training Epoch: 8 [23232/49669]\tLoss: 427.9252\n",
      "Training Epoch: 8 [23296/49669]\tLoss: 410.6149\n",
      "Training Epoch: 8 [23360/49669]\tLoss: 399.6201\n",
      "Training Epoch: 8 [23424/49669]\tLoss: 428.9567\n",
      "Training Epoch: 8 [23488/49669]\tLoss: 454.2827\n",
      "Training Epoch: 8 [23552/49669]\tLoss: 422.0468\n",
      "Training Epoch: 8 [23616/49669]\tLoss: 397.7578\n",
      "Training Epoch: 8 [23680/49669]\tLoss: 410.1651\n",
      "Training Epoch: 8 [23744/49669]\tLoss: 427.4356\n",
      "Training Epoch: 8 [23808/49669]\tLoss: 433.0749\n",
      "Training Epoch: 8 [23872/49669]\tLoss: 432.6898\n",
      "Training Epoch: 8 [23936/49669]\tLoss: 431.5847\n",
      "Training Epoch: 8 [24000/49669]\tLoss: 423.5399\n",
      "Training Epoch: 8 [24064/49669]\tLoss: 434.0038\n",
      "Training Epoch: 8 [24128/49669]\tLoss: 422.7654\n",
      "Training Epoch: 8 [24192/49669]\tLoss: 402.0154\n",
      "Training Epoch: 8 [24256/49669]\tLoss: 422.1028\n",
      "Training Epoch: 8 [24320/49669]\tLoss: 426.2308\n",
      "Training Epoch: 8 [24384/49669]\tLoss: 415.6606\n",
      "Training Epoch: 8 [24448/49669]\tLoss: 444.3600\n",
      "Training Epoch: 8 [24512/49669]\tLoss: 406.9024\n",
      "Training Epoch: 8 [24576/49669]\tLoss: 412.6010\n",
      "Training Epoch: 8 [24640/49669]\tLoss: 419.2105\n",
      "Training Epoch: 8 [24704/49669]\tLoss: 421.6443\n",
      "Training Epoch: 8 [24768/49669]\tLoss: 421.9903\n",
      "Training Epoch: 8 [24832/49669]\tLoss: 429.9570\n",
      "Training Epoch: 8 [24896/49669]\tLoss: 391.4978\n",
      "Training Epoch: 8 [24960/49669]\tLoss: 408.9583\n",
      "Training Epoch: 8 [25024/49669]\tLoss: 411.3552\n",
      "Training Epoch: 8 [25088/49669]\tLoss: 418.4359\n",
      "Training Epoch: 8 [25152/49669]\tLoss: 411.6661\n",
      "Training Epoch: 8 [25216/49669]\tLoss: 397.3277\n",
      "Training Epoch: 8 [25280/49669]\tLoss: 435.7483\n",
      "Training Epoch: 8 [25344/49669]\tLoss: 456.3835\n",
      "Training Epoch: 8 [25408/49669]\tLoss: 404.2413\n",
      "Training Epoch: 8 [25472/49669]\tLoss: 441.4104\n",
      "Training Epoch: 8 [25536/49669]\tLoss: 416.0731\n",
      "Training Epoch: 8 [25600/49669]\tLoss: 424.0197\n",
      "Training Epoch: 8 [25664/49669]\tLoss: 401.9089\n",
      "Training Epoch: 8 [25728/49669]\tLoss: 430.9520\n",
      "Training Epoch: 8 [25792/49669]\tLoss: 424.4225\n",
      "Training Epoch: 8 [25856/49669]\tLoss: 437.4362\n",
      "Training Epoch: 8 [25920/49669]\tLoss: 439.7330\n",
      "Training Epoch: 8 [25984/49669]\tLoss: 407.5063\n",
      "Training Epoch: 8 [26048/49669]\tLoss: 414.4916\n",
      "Training Epoch: 8 [26112/49669]\tLoss: 423.0519\n",
      "Training Epoch: 8 [26176/49669]\tLoss: 425.2848\n",
      "Training Epoch: 8 [26240/49669]\tLoss: 421.1047\n",
      "Training Epoch: 8 [26304/49669]\tLoss: 412.6278\n",
      "Training Epoch: 8 [26368/49669]\tLoss: 398.4764\n",
      "Training Epoch: 8 [26432/49669]\tLoss: 424.9046\n",
      "Training Epoch: 8 [26496/49669]\tLoss: 436.4584\n",
      "Training Epoch: 8 [26560/49669]\tLoss: 416.8736\n",
      "Training Epoch: 8 [26624/49669]\tLoss: 430.2552\n",
      "Training Epoch: 8 [26688/49669]\tLoss: 426.8943\n",
      "Training Epoch: 8 [26752/49669]\tLoss: 440.5386\n",
      "Training Epoch: 8 [26816/49669]\tLoss: 427.5722\n",
      "Training Epoch: 8 [26880/49669]\tLoss: 423.9219\n",
      "Training Epoch: 8 [26944/49669]\tLoss: 419.9507\n",
      "Training Epoch: 8 [27008/49669]\tLoss: 450.8857\n",
      "Training Epoch: 8 [27072/49669]\tLoss: 404.0491\n",
      "Training Epoch: 8 [27136/49669]\tLoss: 381.8403\n",
      "Training Epoch: 8 [27200/49669]\tLoss: 430.2795\n",
      "Training Epoch: 8 [27264/49669]\tLoss: 408.7244\n",
      "Training Epoch: 8 [27328/49669]\tLoss: 444.4188\n",
      "Training Epoch: 8 [27392/49669]\tLoss: 404.5118\n",
      "Training Epoch: 8 [27456/49669]\tLoss: 413.7825\n",
      "Training Epoch: 8 [27520/49669]\tLoss: 390.5081\n",
      "Training Epoch: 8 [27584/49669]\tLoss: 421.8930\n",
      "Training Epoch: 8 [27648/49669]\tLoss: 428.7050\n",
      "Training Epoch: 8 [27712/49669]\tLoss: 416.0614\n",
      "Training Epoch: 8 [27776/49669]\tLoss: 425.6002\n",
      "Training Epoch: 8 [27840/49669]\tLoss: 420.9568\n",
      "Training Epoch: 8 [27904/49669]\tLoss: 406.8481\n",
      "Training Epoch: 8 [27968/49669]\tLoss: 402.2715\n",
      "Training Epoch: 8 [28032/49669]\tLoss: 391.6738\n",
      "Training Epoch: 8 [28096/49669]\tLoss: 428.2397\n",
      "Training Epoch: 8 [28160/49669]\tLoss: 422.1326\n",
      "Training Epoch: 8 [28224/49669]\tLoss: 424.8691\n",
      "Training Epoch: 8 [28288/49669]\tLoss: 419.5736\n",
      "Training Epoch: 8 [28352/49669]\tLoss: 405.4709\n",
      "Training Epoch: 8 [28416/49669]\tLoss: 421.3982\n",
      "Training Epoch: 8 [28480/49669]\tLoss: 431.6139\n",
      "Training Epoch: 8 [28544/49669]\tLoss: 427.0461\n",
      "Training Epoch: 8 [28608/49669]\tLoss: 417.6887\n",
      "Training Epoch: 8 [28672/49669]\tLoss: 425.6715\n",
      "Training Epoch: 8 [28736/49669]\tLoss: 412.2878\n",
      "Training Epoch: 8 [28800/49669]\tLoss: 425.6452\n",
      "Training Epoch: 8 [28864/49669]\tLoss: 404.0922\n",
      "Training Epoch: 8 [28928/49669]\tLoss: 388.3880\n",
      "Training Epoch: 8 [28992/49669]\tLoss: 426.6564\n",
      "Training Epoch: 8 [29056/49669]\tLoss: 437.1565\n",
      "Training Epoch: 8 [29120/49669]\tLoss: 440.2810\n",
      "Training Epoch: 8 [29184/49669]\tLoss: 422.5381\n",
      "Training Epoch: 8 [29248/49669]\tLoss: 414.1634\n",
      "Training Epoch: 8 [29312/49669]\tLoss: 427.5891\n",
      "Training Epoch: 8 [29376/49669]\tLoss: 391.5549\n",
      "Training Epoch: 8 [29440/49669]\tLoss: 407.4438\n",
      "Training Epoch: 8 [29504/49669]\tLoss: 433.8984\n",
      "Training Epoch: 8 [29568/49669]\tLoss: 412.0811\n",
      "Training Epoch: 8 [29632/49669]\tLoss: 428.2001\n",
      "Training Epoch: 8 [29696/49669]\tLoss: 432.0713\n",
      "Training Epoch: 8 [29760/49669]\tLoss: 416.0414\n",
      "Training Epoch: 8 [29824/49669]\tLoss: 430.3543\n",
      "Training Epoch: 8 [29888/49669]\tLoss: 415.9791\n",
      "Training Epoch: 8 [29952/49669]\tLoss: 432.4630\n",
      "Training Epoch: 8 [30016/49669]\tLoss: 428.1504\n",
      "Training Epoch: 8 [30080/49669]\tLoss: 417.7012\n",
      "Training Epoch: 8 [30144/49669]\tLoss: 418.1083\n",
      "Training Epoch: 8 [30208/49669]\tLoss: 403.4950\n",
      "Training Epoch: 8 [30272/49669]\tLoss: 424.7127\n",
      "Training Epoch: 8 [30336/49669]\tLoss: 374.1756\n",
      "Training Epoch: 8 [30400/49669]\tLoss: 406.4297\n",
      "Training Epoch: 8 [30464/49669]\tLoss: 417.8694\n",
      "Training Epoch: 8 [30528/49669]\tLoss: 421.6017\n",
      "Training Epoch: 8 [30592/49669]\tLoss: 438.5800\n",
      "Training Epoch: 8 [30656/49669]\tLoss: 407.3380\n",
      "Training Epoch: 8 [30720/49669]\tLoss: 410.8413\n",
      "Training Epoch: 8 [30784/49669]\tLoss: 428.0286\n",
      "Training Epoch: 8 [30848/49669]\tLoss: 422.6408\n",
      "Training Epoch: 8 [30912/49669]\tLoss: 407.5684\n",
      "Training Epoch: 8 [30976/49669]\tLoss: 381.9717\n",
      "Training Epoch: 8 [31040/49669]\tLoss: 385.5472\n",
      "Training Epoch: 8 [31104/49669]\tLoss: 393.5452\n",
      "Training Epoch: 8 [31168/49669]\tLoss: 415.3201\n",
      "Training Epoch: 8 [31232/49669]\tLoss: 404.8356\n",
      "Training Epoch: 8 [31296/49669]\tLoss: 424.2013\n",
      "Training Epoch: 8 [31360/49669]\tLoss: 415.7730\n",
      "Training Epoch: 8 [31424/49669]\tLoss: 385.1557\n",
      "Training Epoch: 8 [31488/49669]\tLoss: 457.4192\n",
      "Training Epoch: 8 [31552/49669]\tLoss: 437.4993\n",
      "Training Epoch: 8 [31616/49669]\tLoss: 426.2063\n",
      "Training Epoch: 8 [31680/49669]\tLoss: 419.6293\n",
      "Training Epoch: 8 [31744/49669]\tLoss: 406.0339\n",
      "Training Epoch: 8 [31808/49669]\tLoss: 397.7957\n",
      "Training Epoch: 8 [31872/49669]\tLoss: 407.6264\n",
      "Training Epoch: 8 [31936/49669]\tLoss: 435.1800\n",
      "Training Epoch: 8 [32000/49669]\tLoss: 400.5695\n",
      "Training Epoch: 8 [32064/49669]\tLoss: 411.8591\n",
      "Training Epoch: 8 [32128/49669]\tLoss: 424.2325\n",
      "Training Epoch: 8 [32192/49669]\tLoss: 409.0617\n",
      "Training Epoch: 8 [32256/49669]\tLoss: 410.0558\n",
      "Training Epoch: 8 [32320/49669]\tLoss: 425.3474\n",
      "Training Epoch: 8 [32384/49669]\tLoss: 422.5471\n",
      "Training Epoch: 8 [32448/49669]\tLoss: 422.5023\n",
      "Training Epoch: 8 [32512/49669]\tLoss: 403.3982\n",
      "Training Epoch: 8 [32576/49669]\tLoss: 406.2278\n",
      "Training Epoch: 8 [32640/49669]\tLoss: 400.9884\n",
      "Training Epoch: 8 [32704/49669]\tLoss: 421.0032\n",
      "Training Epoch: 8 [32768/49669]\tLoss: 437.1459\n",
      "Training Epoch: 8 [32832/49669]\tLoss: 430.6775\n",
      "Training Epoch: 8 [32896/49669]\tLoss: 435.7661\n",
      "Training Epoch: 8 [32960/49669]\tLoss: 429.9857\n",
      "Training Epoch: 8 [33024/49669]\tLoss: 414.0920\n",
      "Training Epoch: 8 [33088/49669]\tLoss: 429.4592\n",
      "Training Epoch: 8 [33152/49669]\tLoss: 412.8911\n",
      "Training Epoch: 8 [33216/49669]\tLoss: 398.3892\n",
      "Training Epoch: 8 [33280/49669]\tLoss: 413.6445\n",
      "Training Epoch: 8 [33344/49669]\tLoss: 400.6280\n",
      "Training Epoch: 8 [33408/49669]\tLoss: 413.9989\n",
      "Training Epoch: 8 [33472/49669]\tLoss: 420.8835\n",
      "Training Epoch: 8 [33536/49669]\tLoss: 411.7082\n",
      "Training Epoch: 8 [33600/49669]\tLoss: 406.6871\n",
      "Training Epoch: 8 [33664/49669]\tLoss: 402.8617\n",
      "Training Epoch: 8 [33728/49669]\tLoss: 458.0860\n",
      "Training Epoch: 8 [33792/49669]\tLoss: 413.5756\n",
      "Training Epoch: 8 [33856/49669]\tLoss: 400.3578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [33920/49669]\tLoss: 419.1997\n",
      "Training Epoch: 8 [33984/49669]\tLoss: 422.1327\n",
      "Training Epoch: 8 [34048/49669]\tLoss: 390.8705\n",
      "Training Epoch: 8 [34112/49669]\tLoss: 425.1499\n",
      "Training Epoch: 8 [34176/49669]\tLoss: 428.2462\n",
      "Training Epoch: 8 [34240/49669]\tLoss: 404.6274\n",
      "Training Epoch: 8 [34304/49669]\tLoss: 416.9857\n",
      "Training Epoch: 8 [34368/49669]\tLoss: 429.4157\n",
      "Training Epoch: 8 [34432/49669]\tLoss: 398.7981\n",
      "Training Epoch: 8 [34496/49669]\tLoss: 399.5500\n",
      "Training Epoch: 8 [34560/49669]\tLoss: 414.4854\n",
      "Training Epoch: 8 [34624/49669]\tLoss: 407.3691\n",
      "Training Epoch: 8 [34688/49669]\tLoss: 413.1103\n",
      "Training Epoch: 8 [34752/49669]\tLoss: 421.8542\n",
      "Training Epoch: 8 [34816/49669]\tLoss: 437.0094\n",
      "Training Epoch: 8 [34880/49669]\tLoss: 424.5097\n",
      "Training Epoch: 8 [34944/49669]\tLoss: 419.8683\n",
      "Training Epoch: 8 [35008/49669]\tLoss: 410.8249\n",
      "Training Epoch: 8 [35072/49669]\tLoss: 387.2928\n",
      "Training Epoch: 8 [35136/49669]\tLoss: 391.0015\n",
      "Training Epoch: 8 [35200/49669]\tLoss: 392.8133\n",
      "Training Epoch: 8 [35264/49669]\tLoss: 394.4334\n",
      "Training Epoch: 8 [35328/49669]\tLoss: 438.3954\n",
      "Training Epoch: 8 [35392/49669]\tLoss: 405.6100\n",
      "Training Epoch: 8 [35456/49669]\tLoss: 388.0702\n",
      "Training Epoch: 8 [35520/49669]\tLoss: 427.5938\n",
      "Training Epoch: 8 [35584/49669]\tLoss: 407.1333\n",
      "Training Epoch: 8 [35648/49669]\tLoss: 431.8216\n",
      "Training Epoch: 8 [35712/49669]\tLoss: 422.0454\n",
      "Training Epoch: 8 [35776/49669]\tLoss: 416.9543\n",
      "Training Epoch: 8 [35840/49669]\tLoss: 392.8959\n",
      "Training Epoch: 8 [35904/49669]\tLoss: 435.1266\n",
      "Training Epoch: 8 [35968/49669]\tLoss: 394.7892\n",
      "Training Epoch: 8 [36032/49669]\tLoss: 386.4823\n",
      "Training Epoch: 8 [36096/49669]\tLoss: 419.9719\n",
      "Training Epoch: 8 [36160/49669]\tLoss: 373.1272\n",
      "Training Epoch: 8 [36224/49669]\tLoss: 430.3837\n",
      "Training Epoch: 8 [36288/49669]\tLoss: 427.3644\n",
      "Training Epoch: 8 [36352/49669]\tLoss: 409.5588\n",
      "Training Epoch: 8 [36416/49669]\tLoss: 416.4852\n",
      "Training Epoch: 8 [36480/49669]\tLoss: 429.2722\n",
      "Training Epoch: 8 [36544/49669]\tLoss: 439.0642\n",
      "Training Epoch: 8 [36608/49669]\tLoss: 411.0349\n",
      "Training Epoch: 8 [36672/49669]\tLoss: 419.2203\n",
      "Training Epoch: 8 [36736/49669]\tLoss: 425.2698\n",
      "Training Epoch: 8 [36800/49669]\tLoss: 451.1515\n",
      "Training Epoch: 8 [36864/49669]\tLoss: 412.3733\n",
      "Training Epoch: 8 [36928/49669]\tLoss: 421.0263\n",
      "Training Epoch: 8 [36992/49669]\tLoss: 414.2426\n",
      "Training Epoch: 8 [37056/49669]\tLoss: 424.1262\n",
      "Training Epoch: 8 [37120/49669]\tLoss: 432.0519\n",
      "Training Epoch: 8 [37184/49669]\tLoss: 387.4268\n",
      "Training Epoch: 8 [37248/49669]\tLoss: 417.6039\n",
      "Training Epoch: 8 [37312/49669]\tLoss: 432.3459\n",
      "Training Epoch: 8 [37376/49669]\tLoss: 393.1226\n",
      "Training Epoch: 8 [37440/49669]\tLoss: 393.1367\n",
      "Training Epoch: 8 [37504/49669]\tLoss: 397.3161\n",
      "Training Epoch: 8 [37568/49669]\tLoss: 416.3842\n",
      "Training Epoch: 8 [37632/49669]\tLoss: 417.7687\n",
      "Training Epoch: 8 [37696/49669]\tLoss: 420.1974\n",
      "Training Epoch: 8 [37760/49669]\tLoss: 427.1920\n",
      "Training Epoch: 8 [37824/49669]\tLoss: 416.5967\n",
      "Training Epoch: 8 [37888/49669]\tLoss: 414.8236\n",
      "Training Epoch: 8 [37952/49669]\tLoss: 404.2452\n",
      "Training Epoch: 8 [38016/49669]\tLoss: 397.9405\n",
      "Training Epoch: 8 [38080/49669]\tLoss: 411.4091\n",
      "Training Epoch: 8 [38144/49669]\tLoss: 409.4388\n",
      "Training Epoch: 8 [38208/49669]\tLoss: 429.8910\n",
      "Training Epoch: 8 [38272/49669]\tLoss: 436.9321\n",
      "Training Epoch: 8 [38336/49669]\tLoss: 415.5636\n",
      "Training Epoch: 8 [38400/49669]\tLoss: 415.6965\n",
      "Training Epoch: 8 [38464/49669]\tLoss: 400.4718\n",
      "Training Epoch: 8 [38528/49669]\tLoss: 464.5849\n",
      "Training Epoch: 8 [38592/49669]\tLoss: 400.4744\n",
      "Training Epoch: 8 [38656/49669]\tLoss: 439.5070\n",
      "Training Epoch: 8 [38720/49669]\tLoss: 412.6547\n",
      "Training Epoch: 8 [38784/49669]\tLoss: 424.1418\n",
      "Training Epoch: 8 [38848/49669]\tLoss: 451.1352\n",
      "Training Epoch: 8 [38912/49669]\tLoss: 410.9339\n",
      "Training Epoch: 8 [38976/49669]\tLoss: 455.2964\n",
      "Training Epoch: 8 [39040/49669]\tLoss: 411.8723\n",
      "Training Epoch: 8 [39104/49669]\tLoss: 437.0712\n",
      "Training Epoch: 8 [39168/49669]\tLoss: 408.8188\n",
      "Training Epoch: 8 [39232/49669]\tLoss: 413.2444\n",
      "Training Epoch: 8 [39296/49669]\tLoss: 409.9820\n",
      "Training Epoch: 8 [39360/49669]\tLoss: 429.7992\n",
      "Training Epoch: 8 [39424/49669]\tLoss: 396.0780\n",
      "Training Epoch: 8 [39488/49669]\tLoss: 420.7315\n",
      "Training Epoch: 8 [39552/49669]\tLoss: 406.3796\n",
      "Training Epoch: 8 [39616/49669]\tLoss: 411.7045\n",
      "Training Epoch: 8 [39680/49669]\tLoss: 404.2867\n",
      "Training Epoch: 8 [39744/49669]\tLoss: 441.6201\n",
      "Training Epoch: 8 [39808/49669]\tLoss: 420.0659\n",
      "Training Epoch: 8 [39872/49669]\tLoss: 431.9688\n",
      "Training Epoch: 8 [39936/49669]\tLoss: 430.9801\n",
      "Training Epoch: 8 [40000/49669]\tLoss: 409.3678\n",
      "Training Epoch: 8 [40064/49669]\tLoss: 395.7085\n",
      "Training Epoch: 8 [40128/49669]\tLoss: 439.3889\n",
      "Training Epoch: 8 [40192/49669]\tLoss: 411.5245\n",
      "Training Epoch: 8 [40256/49669]\tLoss: 430.9704\n",
      "Training Epoch: 8 [40320/49669]\tLoss: 425.4184\n",
      "Training Epoch: 8 [40384/49669]\tLoss: 410.9094\n",
      "Training Epoch: 8 [40448/49669]\tLoss: 413.9215\n",
      "Training Epoch: 8 [40512/49669]\tLoss: 405.3675\n",
      "Training Epoch: 8 [40576/49669]\tLoss: 407.5828\n",
      "Training Epoch: 8 [40640/49669]\tLoss: 408.2686\n",
      "Training Epoch: 8 [40704/49669]\tLoss: 422.3568\n",
      "Training Epoch: 8 [40768/49669]\tLoss: 405.3346\n",
      "Training Epoch: 8 [40832/49669]\tLoss: 416.0329\n",
      "Training Epoch: 8 [40896/49669]\tLoss: 407.2886\n",
      "Training Epoch: 8 [40960/49669]\tLoss: 399.2772\n",
      "Training Epoch: 8 [41024/49669]\tLoss: 408.0214\n",
      "Training Epoch: 8 [41088/49669]\tLoss: 400.7376\n",
      "Training Epoch: 8 [41152/49669]\tLoss: 404.5790\n",
      "Training Epoch: 8 [41216/49669]\tLoss: 442.1620\n",
      "Training Epoch: 8 [41280/49669]\tLoss: 408.8061\n",
      "Training Epoch: 8 [41344/49669]\tLoss: 406.1926\n",
      "Training Epoch: 8 [41408/49669]\tLoss: 430.4758\n",
      "Training Epoch: 8 [41472/49669]\tLoss: 403.8649\n",
      "Training Epoch: 8 [41536/49669]\tLoss: 409.5369\n",
      "Training Epoch: 8 [41600/49669]\tLoss: 443.6098\n",
      "Training Epoch: 8 [41664/49669]\tLoss: 396.1429\n",
      "Training Epoch: 8 [41728/49669]\tLoss: 393.3895\n",
      "Training Epoch: 8 [41792/49669]\tLoss: 404.5141\n",
      "Training Epoch: 8 [41856/49669]\tLoss: 435.0151\n",
      "Training Epoch: 8 [41920/49669]\tLoss: 408.4904\n",
      "Training Epoch: 8 [41984/49669]\tLoss: 396.2164\n",
      "Training Epoch: 8 [42048/49669]\tLoss: 425.3125\n",
      "Training Epoch: 8 [42112/49669]\tLoss: 399.1894\n",
      "Training Epoch: 8 [42176/49669]\tLoss: 395.4602\n",
      "Training Epoch: 8 [42240/49669]\tLoss: 426.5224\n",
      "Training Epoch: 8 [42304/49669]\tLoss: 418.0629\n",
      "Training Epoch: 8 [42368/49669]\tLoss: 426.1238\n",
      "Training Epoch: 8 [42432/49669]\tLoss: 418.7072\n",
      "Training Epoch: 8 [42496/49669]\tLoss: 388.8907\n",
      "Training Epoch: 8 [42560/49669]\tLoss: 395.7057\n",
      "Training Epoch: 8 [42624/49669]\tLoss: 394.1255\n",
      "Training Epoch: 8 [42688/49669]\tLoss: 402.8488\n",
      "Training Epoch: 8 [42752/49669]\tLoss: 413.1106\n",
      "Training Epoch: 8 [42816/49669]\tLoss: 424.3348\n",
      "Training Epoch: 8 [42880/49669]\tLoss: 401.8063\n",
      "Training Epoch: 8 [42944/49669]\tLoss: 410.4001\n",
      "Training Epoch: 8 [43008/49669]\tLoss: 472.0953\n",
      "Training Epoch: 8 [43072/49669]\tLoss: 438.5211\n",
      "Training Epoch: 8 [43136/49669]\tLoss: 406.4026\n",
      "Training Epoch: 8 [43200/49669]\tLoss: 439.6845\n",
      "Training Epoch: 8 [43264/49669]\tLoss: 410.7584\n",
      "Training Epoch: 8 [43328/49669]\tLoss: 412.1372\n",
      "Training Epoch: 8 [43392/49669]\tLoss: 417.5310\n",
      "Training Epoch: 8 [43456/49669]\tLoss: 417.5361\n",
      "Training Epoch: 8 [43520/49669]\tLoss: 433.1370\n",
      "Training Epoch: 8 [43584/49669]\tLoss: 396.6318\n",
      "Training Epoch: 8 [43648/49669]\tLoss: 416.0370\n",
      "Training Epoch: 8 [43712/49669]\tLoss: 431.1501\n",
      "Training Epoch: 8 [43776/49669]\tLoss: 393.4028\n",
      "Training Epoch: 8 [43840/49669]\tLoss: 390.3662\n",
      "Training Epoch: 8 [43904/49669]\tLoss: 395.7988\n",
      "Training Epoch: 8 [43968/49669]\tLoss: 393.7534\n",
      "Training Epoch: 8 [44032/49669]\tLoss: 431.3477\n",
      "Training Epoch: 8 [44096/49669]\tLoss: 436.8626\n",
      "Training Epoch: 8 [44160/49669]\tLoss: 422.2152\n",
      "Training Epoch: 8 [44224/49669]\tLoss: 420.1021\n",
      "Training Epoch: 8 [44288/49669]\tLoss: 421.7629\n",
      "Training Epoch: 8 [44352/49669]\tLoss: 424.8929\n",
      "Training Epoch: 8 [44416/49669]\tLoss: 410.4626\n",
      "Training Epoch: 8 [44480/49669]\tLoss: 411.3874\n",
      "Training Epoch: 8 [44544/49669]\tLoss: 402.4608\n",
      "Training Epoch: 8 [44608/49669]\tLoss: 428.4655\n",
      "Training Epoch: 8 [44672/49669]\tLoss: 397.6331\n",
      "Training Epoch: 8 [44736/49669]\tLoss: 429.5008\n",
      "Training Epoch: 8 [44800/49669]\tLoss: 428.5229\n",
      "Training Epoch: 8 [44864/49669]\tLoss: 408.9067\n",
      "Training Epoch: 8 [44928/49669]\tLoss: 413.3536\n",
      "Training Epoch: 8 [44992/49669]\tLoss: 447.2548\n",
      "Training Epoch: 8 [45056/49669]\tLoss: 424.4936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [45120/49669]\tLoss: 406.6629\n",
      "Training Epoch: 8 [45184/49669]\tLoss: 430.1143\n",
      "Training Epoch: 8 [45248/49669]\tLoss: 416.7499\n",
      "Training Epoch: 8 [45312/49669]\tLoss: 408.5396\n",
      "Training Epoch: 8 [45376/49669]\tLoss: 406.3349\n",
      "Training Epoch: 8 [45440/49669]\tLoss: 436.6657\n",
      "Training Epoch: 8 [45504/49669]\tLoss: 393.7387\n",
      "Training Epoch: 8 [45568/49669]\tLoss: 419.7370\n",
      "Training Epoch: 8 [45632/49669]\tLoss: 406.9078\n",
      "Training Epoch: 8 [45696/49669]\tLoss: 410.8169\n",
      "Training Epoch: 8 [45760/49669]\tLoss: 424.8711\n",
      "Training Epoch: 8 [45824/49669]\tLoss: 421.8104\n",
      "Training Epoch: 8 [45888/49669]\tLoss: 422.8550\n",
      "Training Epoch: 8 [45952/49669]\tLoss: 423.0647\n",
      "Training Epoch: 8 [46016/49669]\tLoss: 404.9679\n",
      "Training Epoch: 8 [46080/49669]\tLoss: 435.4805\n",
      "Training Epoch: 8 [46144/49669]\tLoss: 366.2614\n",
      "Training Epoch: 8 [46208/49669]\tLoss: 404.8835\n",
      "Training Epoch: 8 [46272/49669]\tLoss: 396.2339\n",
      "Training Epoch: 8 [46336/49669]\tLoss: 399.0282\n",
      "Training Epoch: 8 [46400/49669]\tLoss: 450.7435\n",
      "Training Epoch: 8 [46464/49669]\tLoss: 399.4924\n",
      "Training Epoch: 8 [46528/49669]\tLoss: 421.7672\n",
      "Training Epoch: 8 [46592/49669]\tLoss: 431.0280\n",
      "Training Epoch: 8 [46656/49669]\tLoss: 439.5408\n",
      "Training Epoch: 8 [46720/49669]\tLoss: 413.8105\n",
      "Training Epoch: 8 [46784/49669]\tLoss: 416.8739\n",
      "Training Epoch: 8 [46848/49669]\tLoss: 439.7467\n",
      "Training Epoch: 8 [46912/49669]\tLoss: 403.7107\n",
      "Training Epoch: 8 [46976/49669]\tLoss: 411.5771\n",
      "Training Epoch: 8 [47040/49669]\tLoss: 430.9084\n",
      "Training Epoch: 8 [47104/49669]\tLoss: 462.0720\n",
      "Training Epoch: 8 [47168/49669]\tLoss: 427.3768\n",
      "Training Epoch: 8 [47232/49669]\tLoss: 429.3544\n",
      "Training Epoch: 8 [47296/49669]\tLoss: 444.5056\n",
      "Training Epoch: 8 [47360/49669]\tLoss: 448.2845\n",
      "Training Epoch: 8 [47424/49669]\tLoss: 457.0084\n",
      "Training Epoch: 8 [47488/49669]\tLoss: 424.3645\n",
      "Training Epoch: 8 [47552/49669]\tLoss: 434.6864\n",
      "Training Epoch: 8 [47616/49669]\tLoss: 434.2285\n",
      "Training Epoch: 8 [47680/49669]\tLoss: 433.8469\n",
      "Training Epoch: 8 [47744/49669]\tLoss: 420.4242\n",
      "Training Epoch: 8 [47808/49669]\tLoss: 440.9835\n",
      "Training Epoch: 8 [47872/49669]\tLoss: 425.0746\n",
      "Training Epoch: 8 [47936/49669]\tLoss: 409.7250\n",
      "Training Epoch: 8 [48000/49669]\tLoss: 444.4018\n",
      "Training Epoch: 8 [48064/49669]\tLoss: 432.0764\n",
      "Training Epoch: 8 [48128/49669]\tLoss: 423.8408\n",
      "Training Epoch: 8 [48192/49669]\tLoss: 419.5404\n",
      "Training Epoch: 8 [48256/49669]\tLoss: 418.3309\n",
      "Training Epoch: 8 [48320/49669]\tLoss: 445.6811\n",
      "Training Epoch: 8 [48384/49669]\tLoss: 406.1350\n",
      "Training Epoch: 8 [48448/49669]\tLoss: 410.5175\n",
      "Training Epoch: 8 [48512/49669]\tLoss: 437.1454\n",
      "Training Epoch: 8 [48576/49669]\tLoss: 432.9199\n",
      "Training Epoch: 8 [48640/49669]\tLoss: 421.9739\n",
      "Training Epoch: 8 [48704/49669]\tLoss: 428.3583\n",
      "Training Epoch: 8 [48768/49669]\tLoss: 406.6403\n",
      "Training Epoch: 8 [48832/49669]\tLoss: 416.7841\n",
      "Training Epoch: 8 [48896/49669]\tLoss: 414.1188\n",
      "Training Epoch: 8 [48960/49669]\tLoss: 389.0194\n",
      "Training Epoch: 8 [49024/49669]\tLoss: 414.5752\n",
      "Training Epoch: 8 [49088/49669]\tLoss: 386.5128\n",
      "Training Epoch: 8 [49152/49669]\tLoss: 425.6453\n",
      "Training Epoch: 8 [49216/49669]\tLoss: 406.1435\n",
      "Training Epoch: 8 [49280/49669]\tLoss: 434.5725\n",
      "Training Epoch: 8 [49344/49669]\tLoss: 392.9147\n",
      "Training Epoch: 8 [49408/49669]\tLoss: 416.7556\n",
      "Training Epoch: 8 [49472/49669]\tLoss: 427.1285\n",
      "Training Epoch: 8 [49536/49669]\tLoss: 432.4758\n",
      "Training Epoch: 8 [49600/49669]\tLoss: 414.6206\n",
      "Training Epoch: 8 [49664/49669]\tLoss: 397.7999\n",
      "Training Epoch: 8 [49669/49669]\tLoss: 481.9254\n",
      "Training Epoch: 8 [5519/5519]\tLoss: 418.4561\n",
      "Training Epoch: 9 [64/49669]\tLoss: 425.9150\n",
      "Training Epoch: 9 [128/49669]\tLoss: 418.8336\n",
      "Training Epoch: 9 [192/49669]\tLoss: 415.7136\n",
      "Training Epoch: 9 [256/49669]\tLoss: 430.2217\n",
      "Training Epoch: 9 [320/49669]\tLoss: 457.7583\n",
      "Training Epoch: 9 [384/49669]\tLoss: 438.0378\n",
      "Training Epoch: 9 [448/49669]\tLoss: 422.7127\n",
      "Training Epoch: 9 [512/49669]\tLoss: 419.8250\n",
      "Training Epoch: 9 [576/49669]\tLoss: 427.5791\n",
      "Training Epoch: 9 [640/49669]\tLoss: 408.6986\n",
      "Training Epoch: 9 [704/49669]\tLoss: 426.1832\n",
      "Training Epoch: 9 [768/49669]\tLoss: 439.5585\n",
      "Training Epoch: 9 [832/49669]\tLoss: 419.8430\n",
      "Training Epoch: 9 [896/49669]\tLoss: 412.3772\n",
      "Training Epoch: 9 [960/49669]\tLoss: 419.7769\n",
      "Training Epoch: 9 [1024/49669]\tLoss: 466.1867\n",
      "Training Epoch: 9 [1088/49669]\tLoss: 407.8800\n",
      "Training Epoch: 9 [1152/49669]\tLoss: 397.7283\n",
      "Training Epoch: 9 [1216/49669]\tLoss: 408.1057\n",
      "Training Epoch: 9 [1280/49669]\tLoss: 403.8486\n",
      "Training Epoch: 9 [1344/49669]\tLoss: 428.1216\n",
      "Training Epoch: 9 [1408/49669]\tLoss: 407.4957\n",
      "Training Epoch: 9 [1472/49669]\tLoss: 424.5620\n",
      "Training Epoch: 9 [1536/49669]\tLoss: 457.2598\n",
      "Training Epoch: 9 [1600/49669]\tLoss: 412.2176\n",
      "Training Epoch: 9 [1664/49669]\tLoss: 435.4694\n",
      "Training Epoch: 9 [1728/49669]\tLoss: 405.3330\n",
      "Training Epoch: 9 [1792/49669]\tLoss: 413.1921\n",
      "Training Epoch: 9 [1856/49669]\tLoss: 425.6966\n",
      "Training Epoch: 9 [1920/49669]\tLoss: 416.1988\n",
      "Training Epoch: 9 [1984/49669]\tLoss: 414.9871\n",
      "Training Epoch: 9 [2048/49669]\tLoss: 449.9351\n",
      "Training Epoch: 9 [2112/49669]\tLoss: 406.9265\n",
      "Training Epoch: 9 [2176/49669]\tLoss: 389.1681\n",
      "Training Epoch: 9 [2240/49669]\tLoss: 424.3147\n",
      "Training Epoch: 9 [2304/49669]\tLoss: 411.7265\n",
      "Training Epoch: 9 [2368/49669]\tLoss: 430.2171\n",
      "Training Epoch: 9 [2432/49669]\tLoss: 435.0858\n",
      "Training Epoch: 9 [2496/49669]\tLoss: 370.7303\n",
      "Training Epoch: 9 [2560/49669]\tLoss: 404.7379\n",
      "Training Epoch: 9 [2624/49669]\tLoss: 422.0791\n",
      "Training Epoch: 9 [2688/49669]\tLoss: 435.7472\n",
      "Training Epoch: 9 [2752/49669]\tLoss: 412.7381\n",
      "Training Epoch: 9 [2816/49669]\tLoss: 413.6782\n",
      "Training Epoch: 9 [2880/49669]\tLoss: 422.9663\n",
      "Training Epoch: 9 [2944/49669]\tLoss: 410.0636\n",
      "Training Epoch: 9 [3008/49669]\tLoss: 401.8449\n",
      "Training Epoch: 9 [3072/49669]\tLoss: 450.0542\n",
      "Training Epoch: 9 [3136/49669]\tLoss: 419.1488\n",
      "Training Epoch: 9 [3200/49669]\tLoss: 393.4798\n",
      "Training Epoch: 9 [3264/49669]\tLoss: 420.1567\n",
      "Training Epoch: 9 [3328/49669]\tLoss: 401.8282\n",
      "Training Epoch: 9 [3392/49669]\tLoss: 402.9429\n",
      "Training Epoch: 9 [3456/49669]\tLoss: 389.0369\n",
      "Training Epoch: 9 [3520/49669]\tLoss: 446.8080\n",
      "Training Epoch: 9 [3584/49669]\tLoss: 421.9049\n",
      "Training Epoch: 9 [3648/49669]\tLoss: 434.2463\n",
      "Training Epoch: 9 [3712/49669]\tLoss: 395.5926\n",
      "Training Epoch: 9 [3776/49669]\tLoss: 417.4920\n",
      "Training Epoch: 9 [3840/49669]\tLoss: 419.7235\n",
      "Training Epoch: 9 [3904/49669]\tLoss: 442.3004\n",
      "Training Epoch: 9 [3968/49669]\tLoss: 416.0558\n",
      "Training Epoch: 9 [4032/49669]\tLoss: 434.7410\n",
      "Training Epoch: 9 [4096/49669]\tLoss: 430.6070\n",
      "Training Epoch: 9 [4160/49669]\tLoss: 408.5130\n",
      "Training Epoch: 9 [4224/49669]\tLoss: 393.1401\n",
      "Training Epoch: 9 [4288/49669]\tLoss: 401.1696\n",
      "Training Epoch: 9 [4352/49669]\tLoss: 411.9303\n",
      "Training Epoch: 9 [4416/49669]\tLoss: 410.4806\n",
      "Training Epoch: 9 [4480/49669]\tLoss: 436.6447\n",
      "Training Epoch: 9 [4544/49669]\tLoss: 418.3584\n",
      "Training Epoch: 9 [4608/49669]\tLoss: 401.0408\n",
      "Training Epoch: 9 [4672/49669]\tLoss: 404.9829\n",
      "Training Epoch: 9 [4736/49669]\tLoss: 407.4940\n",
      "Training Epoch: 9 [4800/49669]\tLoss: 433.3690\n",
      "Training Epoch: 9 [4864/49669]\tLoss: 435.2454\n",
      "Training Epoch: 9 [4928/49669]\tLoss: 439.4332\n",
      "Training Epoch: 9 [4992/49669]\tLoss: 368.7314\n",
      "Training Epoch: 9 [5056/49669]\tLoss: 436.4843\n",
      "Training Epoch: 9 [5120/49669]\tLoss: 412.9941\n",
      "Training Epoch: 9 [5184/49669]\tLoss: 419.7549\n",
      "Training Epoch: 9 [5248/49669]\tLoss: 392.4697\n",
      "Training Epoch: 9 [5312/49669]\tLoss: 428.6113\n",
      "Training Epoch: 9 [5376/49669]\tLoss: 425.2021\n",
      "Training Epoch: 9 [5440/49669]\tLoss: 432.0082\n",
      "Training Epoch: 9 [5504/49669]\tLoss: 440.3532\n",
      "Training Epoch: 9 [5568/49669]\tLoss: 396.7954\n",
      "Training Epoch: 9 [5632/49669]\tLoss: 423.0810\n",
      "Training Epoch: 9 [5696/49669]\tLoss: 426.0718\n",
      "Training Epoch: 9 [5760/49669]\tLoss: 427.1444\n",
      "Training Epoch: 9 [5824/49669]\tLoss: 448.0506\n",
      "Training Epoch: 9 [5888/49669]\tLoss: 469.6656\n",
      "Training Epoch: 9 [5952/49669]\tLoss: 392.9078\n",
      "Training Epoch: 9 [6016/49669]\tLoss: 436.0987\n",
      "Training Epoch: 9 [6080/49669]\tLoss: 393.8713\n",
      "Training Epoch: 9 [6144/49669]\tLoss: 425.0926\n",
      "Training Epoch: 9 [6208/49669]\tLoss: 423.8325\n",
      "Training Epoch: 9 [6272/49669]\tLoss: 403.6578\n",
      "Training Epoch: 9 [6336/49669]\tLoss: 407.1443\n",
      "Training Epoch: 9 [6400/49669]\tLoss: 415.4209\n",
      "Training Epoch: 9 [6464/49669]\tLoss: 458.4087\n",
      "Training Epoch: 9 [6528/49669]\tLoss: 390.9955\n",
      "Training Epoch: 9 [6592/49669]\tLoss: 405.1441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [6656/49669]\tLoss: 421.7896\n",
      "Training Epoch: 9 [6720/49669]\tLoss: 427.6041\n",
      "Training Epoch: 9 [6784/49669]\tLoss: 398.4203\n",
      "Training Epoch: 9 [6848/49669]\tLoss: 431.0042\n",
      "Training Epoch: 9 [6912/49669]\tLoss: 418.8108\n",
      "Training Epoch: 9 [6976/49669]\tLoss: 465.5044\n",
      "Training Epoch: 9 [7040/49669]\tLoss: 381.0317\n",
      "Training Epoch: 9 [7104/49669]\tLoss: 416.1216\n",
      "Training Epoch: 9 [7168/49669]\tLoss: 405.5202\n",
      "Training Epoch: 9 [7232/49669]\tLoss: 418.4261\n",
      "Training Epoch: 9 [7296/49669]\tLoss: 422.4670\n",
      "Training Epoch: 9 [7360/49669]\tLoss: 416.5725\n",
      "Training Epoch: 9 [7424/49669]\tLoss: 404.9088\n",
      "Training Epoch: 9 [7488/49669]\tLoss: 424.2829\n",
      "Training Epoch: 9 [7552/49669]\tLoss: 424.6535\n",
      "Training Epoch: 9 [7616/49669]\tLoss: 420.5879\n",
      "Training Epoch: 9 [7680/49669]\tLoss: 433.0567\n",
      "Training Epoch: 9 [7744/49669]\tLoss: 389.7137\n",
      "Training Epoch: 9 [7808/49669]\tLoss: 402.3872\n",
      "Training Epoch: 9 [7872/49669]\tLoss: 417.3535\n",
      "Training Epoch: 9 [7936/49669]\tLoss: 430.5058\n",
      "Training Epoch: 9 [8000/49669]\tLoss: 422.8425\n",
      "Training Epoch: 9 [8064/49669]\tLoss: 435.9253\n",
      "Training Epoch: 9 [8128/49669]\tLoss: 427.4642\n",
      "Training Epoch: 9 [8192/49669]\tLoss: 403.3736\n",
      "Training Epoch: 9 [8256/49669]\tLoss: 403.5292\n",
      "Training Epoch: 9 [8320/49669]\tLoss: 438.5244\n",
      "Training Epoch: 9 [8384/49669]\tLoss: 416.5750\n",
      "Training Epoch: 9 [8448/49669]\tLoss: 422.7756\n",
      "Training Epoch: 9 [8512/49669]\tLoss: 399.2987\n",
      "Training Epoch: 9 [8576/49669]\tLoss: 437.7149\n",
      "Training Epoch: 9 [8640/49669]\tLoss: 388.7825\n",
      "Training Epoch: 9 [8704/49669]\tLoss: 417.4444\n",
      "Training Epoch: 9 [8768/49669]\tLoss: 411.7423\n",
      "Training Epoch: 9 [8832/49669]\tLoss: 428.4943\n",
      "Training Epoch: 9 [8896/49669]\tLoss: 442.0430\n",
      "Training Epoch: 9 [8960/49669]\tLoss: 418.5035\n",
      "Training Epoch: 9 [9024/49669]\tLoss: 425.4381\n",
      "Training Epoch: 9 [9088/49669]\tLoss: 410.5972\n",
      "Training Epoch: 9 [9152/49669]\tLoss: 412.2209\n",
      "Training Epoch: 9 [9216/49669]\tLoss: 395.4406\n",
      "Training Epoch: 9 [9280/49669]\tLoss: 416.7854\n",
      "Training Epoch: 9 [9344/49669]\tLoss: 406.3158\n",
      "Training Epoch: 9 [9408/49669]\tLoss: 424.1659\n",
      "Training Epoch: 9 [9472/49669]\tLoss: 403.7723\n",
      "Training Epoch: 9 [9536/49669]\tLoss: 416.9425\n",
      "Training Epoch: 9 [9600/49669]\tLoss: 422.4748\n",
      "Training Epoch: 9 [9664/49669]\tLoss: 414.1386\n",
      "Training Epoch: 9 [9728/49669]\tLoss: 446.9726\n",
      "Training Epoch: 9 [9792/49669]\tLoss: 407.2460\n",
      "Training Epoch: 9 [9856/49669]\tLoss: 433.8745\n",
      "Training Epoch: 9 [9920/49669]\tLoss: 391.8415\n",
      "Training Epoch: 9 [9984/49669]\tLoss: 422.4378\n",
      "Training Epoch: 9 [10048/49669]\tLoss: 427.6991\n",
      "Training Epoch: 9 [10112/49669]\tLoss: 407.0056\n",
      "Training Epoch: 9 [10176/49669]\tLoss: 420.7613\n",
      "Training Epoch: 9 [10240/49669]\tLoss: 425.5164\n",
      "Training Epoch: 9 [10304/49669]\tLoss: 430.7365\n",
      "Training Epoch: 9 [10368/49669]\tLoss: 422.0671\n",
      "Training Epoch: 9 [10432/49669]\tLoss: 424.0704\n",
      "Training Epoch: 9 [10496/49669]\tLoss: 407.4343\n",
      "Training Epoch: 9 [10560/49669]\tLoss: 422.4666\n",
      "Training Epoch: 9 [10624/49669]\tLoss: 404.6643\n",
      "Training Epoch: 9 [10688/49669]\tLoss: 436.5953\n",
      "Training Epoch: 9 [10752/49669]\tLoss: 402.4924\n",
      "Training Epoch: 9 [10816/49669]\tLoss: 427.6656\n",
      "Training Epoch: 9 [10880/49669]\tLoss: 414.3535\n",
      "Training Epoch: 9 [10944/49669]\tLoss: 428.2348\n",
      "Training Epoch: 9 [11008/49669]\tLoss: 423.7217\n",
      "Training Epoch: 9 [11072/49669]\tLoss: 427.3145\n",
      "Training Epoch: 9 [11136/49669]\tLoss: 418.4023\n",
      "Training Epoch: 9 [11200/49669]\tLoss: 410.3165\n",
      "Training Epoch: 9 [11264/49669]\tLoss: 435.1644\n",
      "Training Epoch: 9 [11328/49669]\tLoss: 466.4673\n",
      "Training Epoch: 9 [11392/49669]\tLoss: 447.4694\n",
      "Training Epoch: 9 [11456/49669]\tLoss: 445.8564\n",
      "Training Epoch: 9 [11520/49669]\tLoss: 484.6219\n",
      "Training Epoch: 9 [11584/49669]\tLoss: 445.7040\n",
      "Training Epoch: 9 [11648/49669]\tLoss: 431.2674\n",
      "Training Epoch: 9 [11712/49669]\tLoss: 417.8442\n",
      "Training Epoch: 9 [11776/49669]\tLoss: 426.8148\n",
      "Training Epoch: 9 [11840/49669]\tLoss: 432.4435\n",
      "Training Epoch: 9 [11904/49669]\tLoss: 445.9897\n",
      "Training Epoch: 9 [11968/49669]\tLoss: 484.5441\n",
      "Training Epoch: 9 [12032/49669]\tLoss: 438.7650\n",
      "Training Epoch: 9 [12096/49669]\tLoss: 418.3650\n",
      "Training Epoch: 9 [12160/49669]\tLoss: 420.4306\n",
      "Training Epoch: 9 [12224/49669]\tLoss: 423.4323\n",
      "Training Epoch: 9 [12288/49669]\tLoss: 454.1216\n",
      "Training Epoch: 9 [12352/49669]\tLoss: 450.1678\n",
      "Training Epoch: 9 [12416/49669]\tLoss: 452.4090\n",
      "Training Epoch: 9 [12480/49669]\tLoss: 417.3225\n",
      "Training Epoch: 9 [12544/49669]\tLoss: 412.8563\n",
      "Training Epoch: 9 [12608/49669]\tLoss: 410.6307\n",
      "Training Epoch: 9 [12672/49669]\tLoss: 455.6783\n",
      "Training Epoch: 9 [12736/49669]\tLoss: 408.4463\n",
      "Training Epoch: 9 [12800/49669]\tLoss: 401.1447\n",
      "Training Epoch: 9 [12864/49669]\tLoss: 430.4737\n",
      "Training Epoch: 9 [12928/49669]\tLoss: 423.8409\n",
      "Training Epoch: 9 [12992/49669]\tLoss: 439.7623\n",
      "Training Epoch: 9 [13056/49669]\tLoss: 410.1776\n",
      "Training Epoch: 9 [13120/49669]\tLoss: 440.5089\n",
      "Training Epoch: 9 [13184/49669]\tLoss: 413.8304\n",
      "Training Epoch: 9 [13248/49669]\tLoss: 434.8726\n",
      "Training Epoch: 9 [13312/49669]\tLoss: 421.1080\n",
      "Training Epoch: 9 [13376/49669]\tLoss: 442.8542\n",
      "Training Epoch: 9 [13440/49669]\tLoss: 415.1500\n",
      "Training Epoch: 9 [13504/49669]\tLoss: 440.6439\n",
      "Training Epoch: 9 [13568/49669]\tLoss: 447.4513\n",
      "Training Epoch: 9 [13632/49669]\tLoss: 406.5793\n",
      "Training Epoch: 9 [13696/49669]\tLoss: 427.8999\n",
      "Training Epoch: 9 [13760/49669]\tLoss: 394.1260\n",
      "Training Epoch: 9 [13824/49669]\tLoss: 417.7409\n",
      "Training Epoch: 9 [13888/49669]\tLoss: 405.4452\n",
      "Training Epoch: 9 [13952/49669]\tLoss: 438.9163\n",
      "Training Epoch: 9 [14016/49669]\tLoss: 422.9982\n",
      "Training Epoch: 9 [14080/49669]\tLoss: 401.6974\n",
      "Training Epoch: 9 [14144/49669]\tLoss: 421.1564\n",
      "Training Epoch: 9 [14208/49669]\tLoss: 410.2812\n",
      "Training Epoch: 9 [14272/49669]\tLoss: 436.6471\n",
      "Training Epoch: 9 [14336/49669]\tLoss: 431.6365\n",
      "Training Epoch: 9 [14400/49669]\tLoss: 419.9253\n",
      "Training Epoch: 9 [14464/49669]\tLoss: 428.8840\n",
      "Training Epoch: 9 [14528/49669]\tLoss: 426.9326\n",
      "Training Epoch: 9 [14592/49669]\tLoss: 406.5373\n",
      "Training Epoch: 9 [14656/49669]\tLoss: 409.2415\n",
      "Training Epoch: 9 [14720/49669]\tLoss: 416.1436\n",
      "Training Epoch: 9 [14784/49669]\tLoss: 429.3036\n",
      "Training Epoch: 9 [14848/49669]\tLoss: 412.1629\n",
      "Training Epoch: 9 [14912/49669]\tLoss: 432.9222\n",
      "Training Epoch: 9 [14976/49669]\tLoss: 431.7086\n",
      "Training Epoch: 9 [15040/49669]\tLoss: 388.1530\n",
      "Training Epoch: 9 [15104/49669]\tLoss: 447.8942\n",
      "Training Epoch: 9 [15168/49669]\tLoss: 442.0299\n",
      "Training Epoch: 9 [15232/49669]\tLoss: 441.9504\n",
      "Training Epoch: 9 [15296/49669]\tLoss: 398.7360\n",
      "Training Epoch: 9 [15360/49669]\tLoss: 368.5609\n",
      "Training Epoch: 9 [15424/49669]\tLoss: 442.5079\n",
      "Training Epoch: 9 [15488/49669]\tLoss: 400.1960\n",
      "Training Epoch: 9 [15552/49669]\tLoss: 395.8393\n",
      "Training Epoch: 9 [15616/49669]\tLoss: 434.2294\n",
      "Training Epoch: 9 [15680/49669]\tLoss: 436.2528\n",
      "Training Epoch: 9 [15744/49669]\tLoss: 430.6127\n",
      "Training Epoch: 9 [15808/49669]\tLoss: 414.2205\n",
      "Training Epoch: 9 [15872/49669]\tLoss: 390.8404\n",
      "Training Epoch: 9 [15936/49669]\tLoss: 410.6664\n",
      "Training Epoch: 9 [16000/49669]\tLoss: 432.8009\n",
      "Training Epoch: 9 [16064/49669]\tLoss: 386.6761\n",
      "Training Epoch: 9 [16128/49669]\tLoss: 399.4977\n",
      "Training Epoch: 9 [16192/49669]\tLoss: 401.7264\n",
      "Training Epoch: 9 [16256/49669]\tLoss: 405.1913\n",
      "Training Epoch: 9 [16320/49669]\tLoss: 398.3333\n",
      "Training Epoch: 9 [16384/49669]\tLoss: 421.9082\n",
      "Training Epoch: 9 [16448/49669]\tLoss: 434.4401\n",
      "Training Epoch: 9 [16512/49669]\tLoss: 444.4739\n",
      "Training Epoch: 9 [16576/49669]\tLoss: 416.7511\n",
      "Training Epoch: 9 [16640/49669]\tLoss: 434.7141\n",
      "Training Epoch: 9 [16704/49669]\tLoss: 409.8065\n",
      "Training Epoch: 9 [16768/49669]\tLoss: 421.6500\n",
      "Training Epoch: 9 [16832/49669]\tLoss: 411.0351\n",
      "Training Epoch: 9 [16896/49669]\tLoss: 448.7263\n",
      "Training Epoch: 9 [16960/49669]\tLoss: 417.5817\n",
      "Training Epoch: 9 [17024/49669]\tLoss: 397.2189\n",
      "Training Epoch: 9 [17088/49669]\tLoss: 458.4985\n",
      "Training Epoch: 9 [17152/49669]\tLoss: 415.5111\n",
      "Training Epoch: 9 [17216/49669]\tLoss: 420.0865\n",
      "Training Epoch: 9 [17280/49669]\tLoss: 424.2329\n",
      "Training Epoch: 9 [17344/49669]\tLoss: 405.5426\n",
      "Training Epoch: 9 [17408/49669]\tLoss: 416.2482\n",
      "Training Epoch: 9 [17472/49669]\tLoss: 414.7479\n",
      "Training Epoch: 9 [17536/49669]\tLoss: 418.3169\n",
      "Training Epoch: 9 [17600/49669]\tLoss: 418.6181\n",
      "Training Epoch: 9 [17664/49669]\tLoss: 391.5684\n",
      "Training Epoch: 9 [17728/49669]\tLoss: 419.0783\n",
      "Training Epoch: 9 [17792/49669]\tLoss: 420.9353\n",
      "Training Epoch: 9 [17856/49669]\tLoss: 427.4661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [17920/49669]\tLoss: 404.3440\n",
      "Training Epoch: 9 [17984/49669]\tLoss: 418.3977\n",
      "Training Epoch: 9 [18048/49669]\tLoss: 442.2637\n",
      "Training Epoch: 9 [18112/49669]\tLoss: 422.2051\n",
      "Training Epoch: 9 [18176/49669]\tLoss: 409.1602\n",
      "Training Epoch: 9 [18240/49669]\tLoss: 418.8980\n",
      "Training Epoch: 9 [18304/49669]\tLoss: 418.8571\n",
      "Training Epoch: 9 [18368/49669]\tLoss: 396.4408\n",
      "Training Epoch: 9 [18432/49669]\tLoss: 389.3119\n",
      "Training Epoch: 9 [18496/49669]\tLoss: 413.2067\n",
      "Training Epoch: 9 [18560/49669]\tLoss: 417.1204\n",
      "Training Epoch: 9 [18624/49669]\tLoss: 412.7543\n",
      "Training Epoch: 9 [18688/49669]\tLoss: 412.3698\n",
      "Training Epoch: 9 [18752/49669]\tLoss: 418.8874\n",
      "Training Epoch: 9 [18816/49669]\tLoss: 419.0162\n",
      "Training Epoch: 9 [18880/49669]\tLoss: 417.6061\n",
      "Training Epoch: 9 [18944/49669]\tLoss: 426.5680\n",
      "Training Epoch: 9 [19008/49669]\tLoss: 446.2508\n",
      "Training Epoch: 9 [19072/49669]\tLoss: 394.6104\n",
      "Training Epoch: 9 [19136/49669]\tLoss: 425.0830\n",
      "Training Epoch: 9 [19200/49669]\tLoss: 412.8626\n",
      "Training Epoch: 9 [19264/49669]\tLoss: 415.3994\n",
      "Training Epoch: 9 [19328/49669]\tLoss: 420.2527\n",
      "Training Epoch: 9 [19392/49669]\tLoss: 428.8740\n",
      "Training Epoch: 9 [19456/49669]\tLoss: 446.3893\n",
      "Training Epoch: 9 [19520/49669]\tLoss: 416.4418\n",
      "Training Epoch: 9 [19584/49669]\tLoss: 422.1218\n",
      "Training Epoch: 9 [19648/49669]\tLoss: 396.7318\n",
      "Training Epoch: 9 [19712/49669]\tLoss: 396.5385\n",
      "Training Epoch: 9 [19776/49669]\tLoss: 393.2292\n",
      "Training Epoch: 9 [19840/49669]\tLoss: 393.2437\n",
      "Training Epoch: 9 [19904/49669]\tLoss: 417.4102\n",
      "Training Epoch: 9 [19968/49669]\tLoss: 446.7591\n",
      "Training Epoch: 9 [20032/49669]\tLoss: 421.6295\n",
      "Training Epoch: 9 [20096/49669]\tLoss: 431.9779\n",
      "Training Epoch: 9 [20160/49669]\tLoss: 416.5201\n",
      "Training Epoch: 9 [20224/49669]\tLoss: 425.1805\n",
      "Training Epoch: 9 [20288/49669]\tLoss: 403.1787\n",
      "Training Epoch: 9 [20352/49669]\tLoss: 417.1713\n",
      "Training Epoch: 9 [20416/49669]\tLoss: 433.2601\n",
      "Training Epoch: 9 [20480/49669]\tLoss: 419.0931\n",
      "Training Epoch: 9 [20544/49669]\tLoss: 422.1945\n",
      "Training Epoch: 9 [20608/49669]\tLoss: 401.2371\n",
      "Training Epoch: 9 [20672/49669]\tLoss: 403.1783\n",
      "Training Epoch: 9 [20736/49669]\tLoss: 403.0256\n",
      "Training Epoch: 9 [20800/49669]\tLoss: 418.2446\n",
      "Training Epoch: 9 [20864/49669]\tLoss: 408.4188\n",
      "Training Epoch: 9 [20928/49669]\tLoss: 402.9229\n",
      "Training Epoch: 9 [20992/49669]\tLoss: 424.3651\n",
      "Training Epoch: 9 [21056/49669]\tLoss: 415.6689\n",
      "Training Epoch: 9 [21120/49669]\tLoss: 422.0116\n",
      "Training Epoch: 9 [21184/49669]\tLoss: 366.6454\n",
      "Training Epoch: 9 [21248/49669]\tLoss: 414.4895\n",
      "Training Epoch: 9 [21312/49669]\tLoss: 404.5603\n",
      "Training Epoch: 9 [21376/49669]\tLoss: 426.9069\n",
      "Training Epoch: 9 [21440/49669]\tLoss: 410.3000\n",
      "Training Epoch: 9 [21504/49669]\tLoss: 417.1296\n",
      "Training Epoch: 9 [21568/49669]\tLoss: 433.8839\n",
      "Training Epoch: 9 [21632/49669]\tLoss: 422.2237\n",
      "Training Epoch: 9 [21696/49669]\tLoss: 418.1356\n",
      "Training Epoch: 9 [21760/49669]\tLoss: 406.4657\n",
      "Training Epoch: 9 [21824/49669]\tLoss: 413.8423\n",
      "Training Epoch: 9 [21888/49669]\tLoss: 393.9509\n",
      "Training Epoch: 9 [21952/49669]\tLoss: 401.9089\n",
      "Training Epoch: 9 [22016/49669]\tLoss: 394.9983\n",
      "Training Epoch: 9 [22080/49669]\tLoss: 401.0094\n",
      "Training Epoch: 9 [22144/49669]\tLoss: 420.5341\n",
      "Training Epoch: 9 [22208/49669]\tLoss: 433.6756\n",
      "Training Epoch: 9 [22272/49669]\tLoss: 429.9487\n",
      "Training Epoch: 9 [22336/49669]\tLoss: 413.3083\n",
      "Training Epoch: 9 [22400/49669]\tLoss: 409.2207\n",
      "Training Epoch: 9 [22464/49669]\tLoss: 424.8134\n",
      "Training Epoch: 9 [22528/49669]\tLoss: 409.1329\n",
      "Training Epoch: 9 [22592/49669]\tLoss: 437.4111\n",
      "Training Epoch: 9 [22656/49669]\tLoss: 415.7687\n",
      "Training Epoch: 9 [22720/49669]\tLoss: 439.3743\n",
      "Training Epoch: 9 [22784/49669]\tLoss: 441.9571\n",
      "Training Epoch: 9 [22848/49669]\tLoss: 396.2625\n",
      "Training Epoch: 9 [22912/49669]\tLoss: 392.7089\n",
      "Training Epoch: 9 [22976/49669]\tLoss: 442.4943\n",
      "Training Epoch: 9 [23040/49669]\tLoss: 387.0612\n",
      "Training Epoch: 9 [23104/49669]\tLoss: 412.5745\n",
      "Training Epoch: 9 [23168/49669]\tLoss: 427.1278\n",
      "Training Epoch: 9 [23232/49669]\tLoss: 407.3945\n",
      "Training Epoch: 9 [23296/49669]\tLoss: 421.3246\n",
      "Training Epoch: 9 [23360/49669]\tLoss: 445.6277\n",
      "Training Epoch: 9 [23424/49669]\tLoss: 382.1657\n",
      "Training Epoch: 9 [23488/49669]\tLoss: 426.6714\n",
      "Training Epoch: 9 [23552/49669]\tLoss: 431.1169\n",
      "Training Epoch: 9 [23616/49669]\tLoss: 419.1998\n",
      "Training Epoch: 9 [23680/49669]\tLoss: 424.9578\n",
      "Training Epoch: 9 [23744/49669]\tLoss: 412.5356\n",
      "Training Epoch: 9 [23808/49669]\tLoss: 404.8437\n",
      "Training Epoch: 9 [23872/49669]\tLoss: 445.4381\n",
      "Training Epoch: 9 [23936/49669]\tLoss: 425.8069\n",
      "Training Epoch: 9 [24000/49669]\tLoss: 409.4197\n",
      "Training Epoch: 9 [24064/49669]\tLoss: 420.7378\n",
      "Training Epoch: 9 [24128/49669]\tLoss: 412.1609\n",
      "Training Epoch: 9 [24192/49669]\tLoss: 411.9682\n",
      "Training Epoch: 9 [24256/49669]\tLoss: 413.8397\n",
      "Training Epoch: 9 [24320/49669]\tLoss: 408.9684\n",
      "Training Epoch: 9 [24384/49669]\tLoss: 442.1972\n",
      "Training Epoch: 9 [24448/49669]\tLoss: 400.8853\n",
      "Training Epoch: 9 [24512/49669]\tLoss: 438.8986\n",
      "Training Epoch: 9 [24576/49669]\tLoss: 397.0857\n",
      "Training Epoch: 9 [24640/49669]\tLoss: 410.3924\n",
      "Training Epoch: 9 [24704/49669]\tLoss: 428.7252\n",
      "Training Epoch: 9 [24768/49669]\tLoss: 447.8839\n",
      "Training Epoch: 9 [24832/49669]\tLoss: 410.5534\n",
      "Training Epoch: 9 [24896/49669]\tLoss: 403.6349\n",
      "Training Epoch: 9 [24960/49669]\tLoss: 415.8520\n",
      "Training Epoch: 9 [25024/49669]\tLoss: 417.2827\n",
      "Training Epoch: 9 [25088/49669]\tLoss: 443.5908\n",
      "Training Epoch: 9 [25152/49669]\tLoss: 393.4340\n",
      "Training Epoch: 9 [25216/49669]\tLoss: 425.8743\n",
      "Training Epoch: 9 [25280/49669]\tLoss: 404.7579\n",
      "Training Epoch: 9 [25344/49669]\tLoss: 420.9627\n",
      "Training Epoch: 9 [25408/49669]\tLoss: 430.3491\n",
      "Training Epoch: 9 [25472/49669]\tLoss: 388.7061\n",
      "Training Epoch: 9 [25536/49669]\tLoss: 424.5782\n",
      "Training Epoch: 9 [25600/49669]\tLoss: 417.2114\n",
      "Training Epoch: 9 [25664/49669]\tLoss: 420.3971\n",
      "Training Epoch: 9 [25728/49669]\tLoss: 410.6668\n",
      "Training Epoch: 9 [25792/49669]\tLoss: 439.3830\n",
      "Training Epoch: 9 [25856/49669]\tLoss: 435.4642\n",
      "Training Epoch: 9 [25920/49669]\tLoss: 432.7505\n",
      "Training Epoch: 9 [25984/49669]\tLoss: 434.2511\n",
      "Training Epoch: 9 [26048/49669]\tLoss: 398.0020\n",
      "Training Epoch: 9 [26112/49669]\tLoss: 398.1813\n",
      "Training Epoch: 9 [26176/49669]\tLoss: 415.4657\n",
      "Training Epoch: 9 [26240/49669]\tLoss: 419.5630\n",
      "Training Epoch: 9 [26304/49669]\tLoss: 392.2235\n",
      "Training Epoch: 9 [26368/49669]\tLoss: 359.9511\n",
      "Training Epoch: 9 [26432/49669]\tLoss: 390.3830\n",
      "Training Epoch: 9 [26496/49669]\tLoss: 428.0892\n",
      "Training Epoch: 9 [26560/49669]\tLoss: 446.9042\n",
      "Training Epoch: 9 [26624/49669]\tLoss: 407.9179\n",
      "Training Epoch: 9 [26688/49669]\tLoss: 403.7710\n",
      "Training Epoch: 9 [26752/49669]\tLoss: 411.2326\n",
      "Training Epoch: 9 [26816/49669]\tLoss: 405.2446\n",
      "Training Epoch: 9 [26880/49669]\tLoss: 422.2689\n",
      "Training Epoch: 9 [26944/49669]\tLoss: 396.5070\n",
      "Training Epoch: 9 [27008/49669]\tLoss: 435.7396\n",
      "Training Epoch: 9 [27072/49669]\tLoss: 402.8403\n",
      "Training Epoch: 9 [27136/49669]\tLoss: 404.4862\n",
      "Training Epoch: 9 [27200/49669]\tLoss: 397.4611\n",
      "Training Epoch: 9 [27264/49669]\tLoss: 410.5145\n",
      "Training Epoch: 9 [27328/49669]\tLoss: 389.5006\n",
      "Training Epoch: 9 [27392/49669]\tLoss: 387.2426\n",
      "Training Epoch: 9 [27456/49669]\tLoss: 396.8065\n",
      "Training Epoch: 9 [27520/49669]\tLoss: 428.3084\n",
      "Training Epoch: 9 [27584/49669]\tLoss: 390.8625\n",
      "Training Epoch: 9 [27648/49669]\tLoss: 399.2483\n",
      "Training Epoch: 9 [27712/49669]\tLoss: 408.4469\n",
      "Training Epoch: 9 [27776/49669]\tLoss: 400.7089\n",
      "Training Epoch: 9 [27840/49669]\tLoss: 423.2513\n",
      "Training Epoch: 9 [27904/49669]\tLoss: 419.0914\n",
      "Training Epoch: 9 [27968/49669]\tLoss: 409.4485\n",
      "Training Epoch: 9 [28032/49669]\tLoss: 378.2820\n",
      "Training Epoch: 9 [28096/49669]\tLoss: 410.8510\n",
      "Training Epoch: 9 [28160/49669]\tLoss: 400.0858\n",
      "Training Epoch: 9 [28224/49669]\tLoss: 420.7900\n",
      "Training Epoch: 9 [28288/49669]\tLoss: 412.6746\n",
      "Training Epoch: 9 [28352/49669]\tLoss: 424.9219\n",
      "Training Epoch: 9 [28416/49669]\tLoss: 438.1294\n",
      "Training Epoch: 9 [28480/49669]\tLoss: 423.7608\n",
      "Training Epoch: 9 [28544/49669]\tLoss: 410.4063\n",
      "Training Epoch: 9 [28608/49669]\tLoss: 423.6683\n",
      "Training Epoch: 9 [28672/49669]\tLoss: 426.3859\n",
      "Training Epoch: 9 [28736/49669]\tLoss: 391.1237\n",
      "Training Epoch: 9 [28800/49669]\tLoss: 449.8381\n",
      "Training Epoch: 9 [28864/49669]\tLoss: 400.9677\n",
      "Training Epoch: 9 [28928/49669]\tLoss: 431.8344\n",
      "Training Epoch: 9 [28992/49669]\tLoss: 416.7579\n",
      "Training Epoch: 9 [29056/49669]\tLoss: 419.1838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [29120/49669]\tLoss: 420.4718\n",
      "Training Epoch: 9 [29184/49669]\tLoss: 428.5099\n",
      "Training Epoch: 9 [29248/49669]\tLoss: 423.6927\n",
      "Training Epoch: 9 [29312/49669]\tLoss: 407.1404\n",
      "Training Epoch: 9 [29376/49669]\tLoss: 427.5875\n",
      "Training Epoch: 9 [29440/49669]\tLoss: 428.9067\n",
      "Training Epoch: 9 [29504/49669]\tLoss: 425.9844\n",
      "Training Epoch: 9 [29568/49669]\tLoss: 433.5539\n",
      "Training Epoch: 9 [29632/49669]\tLoss: 414.5603\n",
      "Training Epoch: 9 [29696/49669]\tLoss: 433.2619\n",
      "Training Epoch: 9 [29760/49669]\tLoss: 429.7997\n",
      "Training Epoch: 9 [29824/49669]\tLoss: 422.4029\n",
      "Training Epoch: 9 [29888/49669]\tLoss: 431.6084\n",
      "Training Epoch: 9 [29952/49669]\tLoss: 395.1457\n",
      "Training Epoch: 9 [30016/49669]\tLoss: 423.6514\n",
      "Training Epoch: 9 [30080/49669]\tLoss: 454.4257\n",
      "Training Epoch: 9 [30144/49669]\tLoss: 419.5584\n",
      "Training Epoch: 9 [30208/49669]\tLoss: 448.2585\n",
      "Training Epoch: 9 [30272/49669]\tLoss: 469.1782\n",
      "Training Epoch: 9 [30336/49669]\tLoss: 455.5466\n",
      "Training Epoch: 9 [30400/49669]\tLoss: 523.0544\n",
      "Training Epoch: 9 [30464/49669]\tLoss: 504.0469\n",
      "Training Epoch: 9 [30528/49669]\tLoss: 529.9980\n",
      "Training Epoch: 9 [30592/49669]\tLoss: 506.8770\n",
      "Training Epoch: 9 [30656/49669]\tLoss: 470.5361\n",
      "Training Epoch: 9 [30720/49669]\tLoss: 393.5049\n",
      "Training Epoch: 9 [30784/49669]\tLoss: 426.3414\n",
      "Training Epoch: 9 [30848/49669]\tLoss: 487.3510\n",
      "Training Epoch: 9 [30912/49669]\tLoss: 486.9427\n",
      "Training Epoch: 9 [30976/49669]\tLoss: 470.6580\n",
      "Training Epoch: 9 [31040/49669]\tLoss: 431.7413\n",
      "Training Epoch: 9 [31104/49669]\tLoss: 409.6026\n",
      "Training Epoch: 9 [31168/49669]\tLoss: 444.5905\n",
      "Training Epoch: 9 [31232/49669]\tLoss: 472.5721\n",
      "Training Epoch: 9 [31296/49669]\tLoss: 452.7499\n",
      "Training Epoch: 9 [31360/49669]\tLoss: 445.6679\n",
      "Training Epoch: 9 [31424/49669]\tLoss: 477.2051\n",
      "Training Epoch: 9 [31488/49669]\tLoss: 426.2379\n",
      "Training Epoch: 9 [31552/49669]\tLoss: 443.5458\n",
      "Training Epoch: 9 [31616/49669]\tLoss: 410.4506\n",
      "Training Epoch: 9 [31680/49669]\tLoss: 408.5614\n",
      "Training Epoch: 9 [31744/49669]\tLoss: 437.2210\n",
      "Training Epoch: 9 [31808/49669]\tLoss: 437.6342\n",
      "Training Epoch: 9 [31872/49669]\tLoss: 476.2955\n",
      "Training Epoch: 9 [31936/49669]\tLoss: 449.3618\n",
      "Training Epoch: 9 [32000/49669]\tLoss: 458.1870\n",
      "Training Epoch: 9 [32064/49669]\tLoss: 394.3407\n",
      "Training Epoch: 9 [32128/49669]\tLoss: 421.8034\n",
      "Training Epoch: 9 [32192/49669]\tLoss: 425.3174\n",
      "Training Epoch: 9 [32256/49669]\tLoss: 399.2602\n",
      "Training Epoch: 9 [32320/49669]\tLoss: 387.0833\n",
      "Training Epoch: 9 [32384/49669]\tLoss: 431.1289\n",
      "Training Epoch: 9 [32448/49669]\tLoss: 411.0222\n",
      "Training Epoch: 9 [32512/49669]\tLoss: 424.4971\n",
      "Training Epoch: 9 [32576/49669]\tLoss: 417.4690\n",
      "Training Epoch: 9 [32640/49669]\tLoss: 425.6927\n",
      "Training Epoch: 9 [32704/49669]\tLoss: 434.8301\n",
      "Training Epoch: 9 [32768/49669]\tLoss: 406.5348\n",
      "Training Epoch: 9 [32832/49669]\tLoss: 386.0079\n",
      "Training Epoch: 9 [32896/49669]\tLoss: 405.4431\n",
      "Training Epoch: 9 [32960/49669]\tLoss: 400.3785\n",
      "Training Epoch: 9 [33024/49669]\tLoss: 406.8045\n",
      "Training Epoch: 9 [33088/49669]\tLoss: 406.4124\n",
      "Training Epoch: 9 [33152/49669]\tLoss: 406.9817\n",
      "Training Epoch: 9 [33216/49669]\tLoss: 392.5000\n",
      "Training Epoch: 9 [33280/49669]\tLoss: 406.0647\n",
      "Training Epoch: 9 [33344/49669]\tLoss: 420.1260\n",
      "Training Epoch: 9 [33408/49669]\tLoss: 401.6338\n",
      "Training Epoch: 9 [33472/49669]\tLoss: 414.4077\n",
      "Training Epoch: 9 [33536/49669]\tLoss: 407.4738\n",
      "Training Epoch: 9 [33600/49669]\tLoss: 444.5555\n",
      "Training Epoch: 9 [33664/49669]\tLoss: 427.7411\n",
      "Training Epoch: 9 [33728/49669]\tLoss: 414.9901\n",
      "Training Epoch: 9 [33792/49669]\tLoss: 438.2499\n",
      "Training Epoch: 9 [33856/49669]\tLoss: 401.2146\n",
      "Training Epoch: 9 [33920/49669]\tLoss: 387.0538\n",
      "Training Epoch: 9 [33984/49669]\tLoss: 415.3521\n",
      "Training Epoch: 9 [34048/49669]\tLoss: 417.7360\n",
      "Training Epoch: 9 [34112/49669]\tLoss: 373.2682\n",
      "Training Epoch: 9 [34176/49669]\tLoss: 425.7545\n",
      "Training Epoch: 9 [34240/49669]\tLoss: 432.8606\n",
      "Training Epoch: 9 [34304/49669]\tLoss: 452.3694\n",
      "Training Epoch: 9 [34368/49669]\tLoss: 410.4276\n",
      "Training Epoch: 9 [34432/49669]\tLoss: 402.7956\n",
      "Training Epoch: 9 [34496/49669]\tLoss: 421.7327\n",
      "Training Epoch: 9 [34560/49669]\tLoss: 430.4060\n",
      "Training Epoch: 9 [34624/49669]\tLoss: 394.9594\n",
      "Training Epoch: 9 [34688/49669]\tLoss: 415.6705\n",
      "Training Epoch: 9 [34752/49669]\tLoss: 436.0214\n",
      "Training Epoch: 9 [34816/49669]\tLoss: 398.4882\n",
      "Training Epoch: 9 [34880/49669]\tLoss: 429.4335\n",
      "Training Epoch: 9 [34944/49669]\tLoss: 397.5994\n",
      "Training Epoch: 9 [35008/49669]\tLoss: 424.2247\n",
      "Training Epoch: 9 [35072/49669]\tLoss: 400.8440\n",
      "Training Epoch: 9 [35136/49669]\tLoss: 425.4163\n",
      "Training Epoch: 9 [35200/49669]\tLoss: 413.4211\n",
      "Training Epoch: 9 [35264/49669]\tLoss: 421.5089\n",
      "Training Epoch: 9 [35328/49669]\tLoss: 389.9066\n",
      "Training Epoch: 9 [35392/49669]\tLoss: 399.5859\n",
      "Training Epoch: 9 [35456/49669]\tLoss: 397.8806\n",
      "Training Epoch: 9 [35520/49669]\tLoss: 410.2274\n",
      "Training Epoch: 9 [35584/49669]\tLoss: 427.3464\n",
      "Training Epoch: 9 [35648/49669]\tLoss: 420.1555\n",
      "Training Epoch: 9 [35712/49669]\tLoss: 394.9015\n",
      "Training Epoch: 9 [35776/49669]\tLoss: 422.9227\n",
      "Training Epoch: 9 [35840/49669]\tLoss: 409.9025\n",
      "Training Epoch: 9 [35904/49669]\tLoss: 422.6620\n",
      "Training Epoch: 9 [35968/49669]\tLoss: 413.8255\n",
      "Training Epoch: 9 [36032/49669]\tLoss: 381.6519\n",
      "Training Epoch: 9 [36096/49669]\tLoss: 403.8958\n",
      "Training Epoch: 9 [36160/49669]\tLoss: 402.9683\n",
      "Training Epoch: 9 [36224/49669]\tLoss: 425.0059\n",
      "Training Epoch: 9 [36288/49669]\tLoss: 443.7928\n",
      "Training Epoch: 9 [36352/49669]\tLoss: 423.6353\n",
      "Training Epoch: 9 [36416/49669]\tLoss: 415.3340\n",
      "Training Epoch: 9 [36480/49669]\tLoss: 401.6070\n",
      "Training Epoch: 9 [36544/49669]\tLoss: 436.4715\n",
      "Training Epoch: 9 [36608/49669]\tLoss: 401.0020\n",
      "Training Epoch: 9 [36672/49669]\tLoss: 425.6042\n",
      "Training Epoch: 9 [36736/49669]\tLoss: 404.5608\n",
      "Training Epoch: 9 [36800/49669]\tLoss: 412.0922\n",
      "Training Epoch: 9 [36864/49669]\tLoss: 375.2830\n",
      "Training Epoch: 9 [36928/49669]\tLoss: 446.2056\n",
      "Training Epoch: 9 [36992/49669]\tLoss: 432.3418\n",
      "Training Epoch: 9 [37056/49669]\tLoss: 404.8542\n",
      "Training Epoch: 9 [37120/49669]\tLoss: 430.4539\n",
      "Training Epoch: 9 [37184/49669]\tLoss: 419.6725\n",
      "Training Epoch: 9 [37248/49669]\tLoss: 443.8419\n",
      "Training Epoch: 9 [37312/49669]\tLoss: 417.1039\n",
      "Training Epoch: 9 [37376/49669]\tLoss: 401.4107\n",
      "Training Epoch: 9 [37440/49669]\tLoss: 428.2253\n",
      "Training Epoch: 9 [37504/49669]\tLoss: 387.1082\n",
      "Training Epoch: 9 [37568/49669]\tLoss: 408.3871\n",
      "Training Epoch: 9 [37632/49669]\tLoss: 415.6831\n",
      "Training Epoch: 9 [37696/49669]\tLoss: 417.5095\n",
      "Training Epoch: 9 [37760/49669]\tLoss: 393.2843\n",
      "Training Epoch: 9 [37824/49669]\tLoss: 413.8632\n",
      "Training Epoch: 9 [37888/49669]\tLoss: 414.0281\n",
      "Training Epoch: 9 [37952/49669]\tLoss: 375.5812\n",
      "Training Epoch: 9 [38016/49669]\tLoss: 426.5271\n",
      "Training Epoch: 9 [38080/49669]\tLoss: 395.3788\n",
      "Training Epoch: 9 [38144/49669]\tLoss: 444.6679\n",
      "Training Epoch: 9 [38208/49669]\tLoss: 423.0120\n",
      "Training Epoch: 9 [38272/49669]\tLoss: 407.3043\n",
      "Training Epoch: 9 [38336/49669]\tLoss: 414.9460\n",
      "Training Epoch: 9 [38400/49669]\tLoss: 418.9183\n",
      "Training Epoch: 9 [38464/49669]\tLoss: 447.4079\n",
      "Training Epoch: 9 [38528/49669]\tLoss: 367.2089\n",
      "Training Epoch: 9 [38592/49669]\tLoss: 410.9982\n",
      "Training Epoch: 9 [38656/49669]\tLoss: 421.2763\n",
      "Training Epoch: 9 [38720/49669]\tLoss: 421.0424\n",
      "Training Epoch: 9 [38784/49669]\tLoss: 407.9959\n",
      "Training Epoch: 9 [38848/49669]\tLoss: 381.6823\n",
      "Training Epoch: 9 [38912/49669]\tLoss: 420.7453\n",
      "Training Epoch: 9 [38976/49669]\tLoss: 403.4393\n",
      "Training Epoch: 9 [39040/49669]\tLoss: 421.3851\n",
      "Training Epoch: 9 [39104/49669]\tLoss: 412.3481\n",
      "Training Epoch: 9 [39168/49669]\tLoss: 414.6775\n",
      "Training Epoch: 9 [39232/49669]\tLoss: 410.1290\n",
      "Training Epoch: 9 [39296/49669]\tLoss: 432.0060\n",
      "Training Epoch: 9 [39360/49669]\tLoss: 405.2350\n",
      "Training Epoch: 9 [39424/49669]\tLoss: 395.1614\n",
      "Training Epoch: 9 [39488/49669]\tLoss: 427.3581\n",
      "Training Epoch: 9 [39552/49669]\tLoss: 427.4590\n",
      "Training Epoch: 9 [39616/49669]\tLoss: 405.0637\n",
      "Training Epoch: 9 [39680/49669]\tLoss: 392.0962\n",
      "Training Epoch: 9 [39744/49669]\tLoss: 386.4960\n",
      "Training Epoch: 9 [39808/49669]\tLoss: 393.0921\n",
      "Training Epoch: 9 [39872/49669]\tLoss: 392.7018\n",
      "Training Epoch: 9 [39936/49669]\tLoss: 417.9374\n",
      "Training Epoch: 9 [40000/49669]\tLoss: 430.2628\n",
      "Training Epoch: 9 [40064/49669]\tLoss: 424.6181\n",
      "Training Epoch: 9 [40128/49669]\tLoss: 404.6862\n",
      "Training Epoch: 9 [40192/49669]\tLoss: 410.4921\n",
      "Training Epoch: 9 [40256/49669]\tLoss: 396.1713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [40320/49669]\tLoss: 419.9441\n",
      "Training Epoch: 9 [40384/49669]\tLoss: 455.9032\n",
      "Training Epoch: 9 [40448/49669]\tLoss: 421.5812\n",
      "Training Epoch: 9 [40512/49669]\tLoss: 398.5784\n",
      "Training Epoch: 9 [40576/49669]\tLoss: 419.2208\n",
      "Training Epoch: 9 [40640/49669]\tLoss: 405.9065\n",
      "Training Epoch: 9 [40704/49669]\tLoss: 405.7008\n",
      "Training Epoch: 9 [40768/49669]\tLoss: 417.0780\n",
      "Training Epoch: 9 [40832/49669]\tLoss: 439.6915\n",
      "Training Epoch: 9 [40896/49669]\tLoss: 384.7039\n",
      "Training Epoch: 9 [40960/49669]\tLoss: 380.6682\n",
      "Training Epoch: 9 [41024/49669]\tLoss: 415.8844\n",
      "Training Epoch: 9 [41088/49669]\tLoss: 382.5202\n",
      "Training Epoch: 9 [41152/49669]\tLoss: 392.7781\n",
      "Training Epoch: 9 [41216/49669]\tLoss: 451.5096\n",
      "Training Epoch: 9 [41280/49669]\tLoss: 436.4867\n",
      "Training Epoch: 9 [41344/49669]\tLoss: 432.2261\n",
      "Training Epoch: 9 [41408/49669]\tLoss: 439.1373\n",
      "Training Epoch: 9 [41472/49669]\tLoss: 466.8913\n",
      "Training Epoch: 9 [41536/49669]\tLoss: 380.6230\n",
      "Training Epoch: 9 [41600/49669]\tLoss: 404.9501\n",
      "Training Epoch: 9 [41664/49669]\tLoss: 428.6533\n",
      "Training Epoch: 9 [41728/49669]\tLoss: 431.1292\n",
      "Training Epoch: 9 [41792/49669]\tLoss: 451.0965\n",
      "Training Epoch: 9 [41856/49669]\tLoss: 414.5337\n",
      "Training Epoch: 9 [41920/49669]\tLoss: 382.5415\n",
      "Training Epoch: 9 [41984/49669]\tLoss: 440.3337\n",
      "Training Epoch: 9 [42048/49669]\tLoss: 394.6150\n",
      "Training Epoch: 9 [42112/49669]\tLoss: 433.6039\n",
      "Training Epoch: 9 [42176/49669]\tLoss: 427.1554\n",
      "Training Epoch: 9 [42240/49669]\tLoss: 436.2768\n",
      "Training Epoch: 9 [42304/49669]\tLoss: 408.1031\n",
      "Training Epoch: 9 [42368/49669]\tLoss: 395.4308\n",
      "Training Epoch: 9 [42432/49669]\tLoss: 379.5119\n",
      "Training Epoch: 9 [42496/49669]\tLoss: 433.0865\n",
      "Training Epoch: 9 [42560/49669]\tLoss: 435.9390\n",
      "Training Epoch: 9 [42624/49669]\tLoss: 390.6739\n",
      "Training Epoch: 9 [42688/49669]\tLoss: 388.6937\n",
      "Training Epoch: 9 [42752/49669]\tLoss: 431.5383\n",
      "Training Epoch: 9 [42816/49669]\tLoss: 414.7629\n",
      "Training Epoch: 9 [42880/49669]\tLoss: 412.5698\n",
      "Training Epoch: 9 [42944/49669]\tLoss: 391.2112\n",
      "Training Epoch: 9 [43008/49669]\tLoss: 411.4867\n",
      "Training Epoch: 9 [43072/49669]\tLoss: 400.3238\n",
      "Training Epoch: 9 [43136/49669]\tLoss: 391.3689\n",
      "Training Epoch: 9 [43200/49669]\tLoss: 415.4556\n",
      "Training Epoch: 9 [43264/49669]\tLoss: 404.8168\n",
      "Training Epoch: 9 [43328/49669]\tLoss: 446.8562\n",
      "Training Epoch: 9 [43392/49669]\tLoss: 420.7822\n",
      "Training Epoch: 9 [43456/49669]\tLoss: 409.8355\n",
      "Training Epoch: 9 [43520/49669]\tLoss: 423.2605\n",
      "Training Epoch: 9 [43584/49669]\tLoss: 392.4742\n",
      "Training Epoch: 9 [43648/49669]\tLoss: 395.3293\n",
      "Training Epoch: 9 [43712/49669]\tLoss: 401.3010\n",
      "Training Epoch: 9 [43776/49669]\tLoss: 410.7520\n",
      "Training Epoch: 9 [43840/49669]\tLoss: 432.9805\n",
      "Training Epoch: 9 [43904/49669]\tLoss: 431.7489\n",
      "Training Epoch: 9 [43968/49669]\tLoss: 398.0474\n",
      "Training Epoch: 9 [44032/49669]\tLoss: 421.0723\n",
      "Training Epoch: 9 [44096/49669]\tLoss: 387.5698\n",
      "Training Epoch: 9 [44160/49669]\tLoss: 415.5397\n",
      "Training Epoch: 9 [44224/49669]\tLoss: 405.9247\n",
      "Training Epoch: 9 [44288/49669]\tLoss: 425.6776\n",
      "Training Epoch: 9 [44352/49669]\tLoss: 414.3727\n",
      "Training Epoch: 9 [44416/49669]\tLoss: 404.2678\n",
      "Training Epoch: 9 [44480/49669]\tLoss: 408.2853\n",
      "Training Epoch: 9 [44544/49669]\tLoss: 405.1754\n",
      "Training Epoch: 9 [44608/49669]\tLoss: 439.7726\n",
      "Training Epoch: 9 [44672/49669]\tLoss: 395.9014\n",
      "Training Epoch: 9 [44736/49669]\tLoss: 443.0943\n",
      "Training Epoch: 9 [44800/49669]\tLoss: 404.7519\n",
      "Training Epoch: 9 [44864/49669]\tLoss: 405.1699\n",
      "Training Epoch: 9 [44928/49669]\tLoss: 426.2872\n",
      "Training Epoch: 9 [44992/49669]\tLoss: 433.5941\n",
      "Training Epoch: 9 [45056/49669]\tLoss: 416.0948\n",
      "Training Epoch: 9 [45120/49669]\tLoss: 405.5445\n",
      "Training Epoch: 9 [45184/49669]\tLoss: 410.9742\n",
      "Training Epoch: 9 [45248/49669]\tLoss: 391.5338\n",
      "Training Epoch: 9 [45312/49669]\tLoss: 425.9706\n",
      "Training Epoch: 9 [45376/49669]\tLoss: 408.6533\n",
      "Training Epoch: 9 [45440/49669]\tLoss: 429.8847\n",
      "Training Epoch: 9 [45504/49669]\tLoss: 398.7183\n",
      "Training Epoch: 9 [45568/49669]\tLoss: 433.8138\n",
      "Training Epoch: 9 [45632/49669]\tLoss: 420.6583\n",
      "Training Epoch: 9 [45696/49669]\tLoss: 370.3162\n",
      "Training Epoch: 9 [45760/49669]\tLoss: 383.2628\n",
      "Training Epoch: 9 [45824/49669]\tLoss: 417.8610\n",
      "Training Epoch: 9 [45888/49669]\tLoss: 402.1168\n",
      "Training Epoch: 9 [45952/49669]\tLoss: 427.7159\n",
      "Training Epoch: 9 [46016/49669]\tLoss: 398.6228\n",
      "Training Epoch: 9 [46080/49669]\tLoss: 418.2749\n",
      "Training Epoch: 9 [46144/49669]\tLoss: 410.6597\n",
      "Training Epoch: 9 [46208/49669]\tLoss: 409.5206\n",
      "Training Epoch: 9 [46272/49669]\tLoss: 408.1696\n",
      "Training Epoch: 9 [46336/49669]\tLoss: 415.7220\n",
      "Training Epoch: 9 [46400/49669]\tLoss: 438.1841\n",
      "Training Epoch: 9 [46464/49669]\tLoss: 419.1836\n",
      "Training Epoch: 9 [46528/49669]\tLoss: 393.3587\n",
      "Training Epoch: 9 [46592/49669]\tLoss: 429.7552\n",
      "Training Epoch: 9 [46656/49669]\tLoss: 407.7641\n",
      "Training Epoch: 9 [46720/49669]\tLoss: 394.7993\n",
      "Training Epoch: 9 [46784/49669]\tLoss: 436.9138\n",
      "Training Epoch: 9 [46848/49669]\tLoss: 395.0508\n",
      "Training Epoch: 9 [46912/49669]\tLoss: 420.2963\n",
      "Training Epoch: 9 [46976/49669]\tLoss: 389.8810\n",
      "Training Epoch: 9 [47040/49669]\tLoss: 440.2993\n",
      "Training Epoch: 9 [47104/49669]\tLoss: 418.1289\n",
      "Training Epoch: 9 [47168/49669]\tLoss: 408.5859\n",
      "Training Epoch: 9 [47232/49669]\tLoss: 441.1146\n",
      "Training Epoch: 9 [47296/49669]\tLoss: 437.0086\n",
      "Training Epoch: 9 [47360/49669]\tLoss: 422.7507\n",
      "Training Epoch: 9 [47424/49669]\tLoss: 410.5407\n",
      "Training Epoch: 9 [47488/49669]\tLoss: 394.0635\n",
      "Training Epoch: 9 [47552/49669]\tLoss: 427.5364\n",
      "Training Epoch: 9 [47616/49669]\tLoss: 419.4131\n",
      "Training Epoch: 9 [47680/49669]\tLoss: 404.3969\n",
      "Training Epoch: 9 [47744/49669]\tLoss: 413.0793\n",
      "Training Epoch: 9 [47808/49669]\tLoss: 434.7896\n",
      "Training Epoch: 9 [47872/49669]\tLoss: 393.8669\n",
      "Training Epoch: 9 [47936/49669]\tLoss: 417.3033\n",
      "Training Epoch: 9 [48000/49669]\tLoss: 424.0450\n",
      "Training Epoch: 9 [48064/49669]\tLoss: 428.4660\n",
      "Training Epoch: 9 [48128/49669]\tLoss: 366.6067\n",
      "Training Epoch: 9 [48192/49669]\tLoss: 407.7337\n",
      "Training Epoch: 9 [48256/49669]\tLoss: 412.1049\n",
      "Training Epoch: 9 [48320/49669]\tLoss: 412.5472\n",
      "Training Epoch: 9 [48384/49669]\tLoss: 401.9772\n",
      "Training Epoch: 9 [48448/49669]\tLoss: 421.8217\n",
      "Training Epoch: 9 [48512/49669]\tLoss: 407.7469\n",
      "Training Epoch: 9 [48576/49669]\tLoss: 409.5273\n",
      "Training Epoch: 9 [48640/49669]\tLoss: 421.7216\n",
      "Training Epoch: 9 [48704/49669]\tLoss: 418.7953\n",
      "Training Epoch: 9 [48768/49669]\tLoss: 420.1025\n",
      "Training Epoch: 9 [48832/49669]\tLoss: 441.3596\n",
      "Training Epoch: 9 [48896/49669]\tLoss: 380.0663\n",
      "Training Epoch: 9 [48960/49669]\tLoss: 412.7089\n",
      "Training Epoch: 9 [49024/49669]\tLoss: 417.4328\n",
      "Training Epoch: 9 [49088/49669]\tLoss: 427.7422\n",
      "Training Epoch: 9 [49152/49669]\tLoss: 412.2368\n",
      "Training Epoch: 9 [49216/49669]\tLoss: 437.9223\n",
      "Training Epoch: 9 [49280/49669]\tLoss: 433.5224\n",
      "Training Epoch: 9 [49344/49669]\tLoss: 417.2360\n",
      "Training Epoch: 9 [49408/49669]\tLoss: 412.9753\n",
      "Training Epoch: 9 [49472/49669]\tLoss: 436.4229\n",
      "Training Epoch: 9 [49536/49669]\tLoss: 443.7767\n",
      "Training Epoch: 9 [49600/49669]\tLoss: 415.4749\n",
      "Training Epoch: 9 [49664/49669]\tLoss: 420.7409\n",
      "Training Epoch: 9 [49669/49669]\tLoss: 374.6863\n",
      "Training Epoch: 9 [5519/5519]\tLoss: 425.8778\n",
      "Training Epoch: 10 [64/49669]\tLoss: 420.3655\n",
      "Training Epoch: 10 [128/49669]\tLoss: 409.8389\n",
      "Training Epoch: 10 [192/49669]\tLoss: 402.0158\n",
      "Training Epoch: 10 [256/49669]\tLoss: 428.8033\n",
      "Training Epoch: 10 [320/49669]\tLoss: 402.6678\n",
      "Training Epoch: 10 [384/49669]\tLoss: 443.1389\n",
      "Training Epoch: 10 [448/49669]\tLoss: 425.1586\n",
      "Training Epoch: 10 [512/49669]\tLoss: 407.4087\n",
      "Training Epoch: 10 [576/49669]\tLoss: 412.3336\n",
      "Training Epoch: 10 [640/49669]\tLoss: 401.3864\n",
      "Training Epoch: 10 [704/49669]\tLoss: 413.6001\n",
      "Training Epoch: 10 [768/49669]\tLoss: 396.5211\n",
      "Training Epoch: 10 [832/49669]\tLoss: 447.2347\n",
      "Training Epoch: 10 [896/49669]\tLoss: 419.7058\n",
      "Training Epoch: 10 [960/49669]\tLoss: 402.8714\n",
      "Training Epoch: 10 [1024/49669]\tLoss: 404.6412\n",
      "Training Epoch: 10 [1088/49669]\tLoss: 408.9547\n",
      "Training Epoch: 10 [1152/49669]\tLoss: 414.0683\n",
      "Training Epoch: 10 [1216/49669]\tLoss: 432.3732\n",
      "Training Epoch: 10 [1280/49669]\tLoss: 406.9325\n",
      "Training Epoch: 10 [1344/49669]\tLoss: 404.6956\n",
      "Training Epoch: 10 [1408/49669]\tLoss: 413.9400\n",
      "Training Epoch: 10 [1472/49669]\tLoss: 385.6161\n",
      "Training Epoch: 10 [1536/49669]\tLoss: 418.5274\n",
      "Training Epoch: 10 [1600/49669]\tLoss: 420.2379\n",
      "Training Epoch: 10 [1664/49669]\tLoss: 399.1954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [1728/49669]\tLoss: 398.9247\n",
      "Training Epoch: 10 [1792/49669]\tLoss: 404.9774\n",
      "Training Epoch: 10 [1856/49669]\tLoss: 415.6125\n",
      "Training Epoch: 10 [1920/49669]\tLoss: 429.9124\n",
      "Training Epoch: 10 [1984/49669]\tLoss: 405.1960\n",
      "Training Epoch: 10 [2048/49669]\tLoss: 409.3351\n",
      "Training Epoch: 10 [2112/49669]\tLoss: 432.2119\n",
      "Training Epoch: 10 [2176/49669]\tLoss: 431.1236\n",
      "Training Epoch: 10 [2240/49669]\tLoss: 458.3142\n",
      "Training Epoch: 10 [2304/49669]\tLoss: 442.2852\n",
      "Training Epoch: 10 [2368/49669]\tLoss: 480.0486\n",
      "Training Epoch: 10 [2432/49669]\tLoss: 517.3576\n",
      "Training Epoch: 10 [2496/49669]\tLoss: 561.0898\n",
      "Training Epoch: 10 [2560/49669]\tLoss: 640.2592\n",
      "Training Epoch: 10 [2624/49669]\tLoss: 631.2377\n",
      "Training Epoch: 10 [2688/49669]\tLoss: 501.5995\n",
      "Training Epoch: 10 [2752/49669]\tLoss: 416.8208\n",
      "Training Epoch: 10 [2816/49669]\tLoss: 464.9050\n",
      "Training Epoch: 10 [2880/49669]\tLoss: 516.0703\n",
      "Training Epoch: 10 [2944/49669]\tLoss: 459.4838\n",
      "Training Epoch: 10 [3008/49669]\tLoss: 401.9547\n",
      "Training Epoch: 10 [3072/49669]\tLoss: 468.1147\n",
      "Training Epoch: 10 [3136/49669]\tLoss: 517.5729\n",
      "Training Epoch: 10 [3200/49669]\tLoss: 478.7580\n",
      "Training Epoch: 10 [3264/49669]\tLoss: 387.3850\n",
      "Training Epoch: 10 [3328/49669]\tLoss: 425.9778\n",
      "Training Epoch: 10 [3392/49669]\tLoss: 476.6005\n",
      "Training Epoch: 10 [3456/49669]\tLoss: 446.9944\n",
      "Training Epoch: 10 [3520/49669]\tLoss: 405.7149\n",
      "Training Epoch: 10 [3584/49669]\tLoss: 463.7191\n",
      "Training Epoch: 10 [3648/49669]\tLoss: 471.8852\n",
      "Training Epoch: 10 [3712/49669]\tLoss: 399.0695\n",
      "Training Epoch: 10 [3776/49669]\tLoss: 422.5925\n",
      "Training Epoch: 10 [3840/49669]\tLoss: 448.8162\n",
      "Training Epoch: 10 [3904/49669]\tLoss: 421.9518\n",
      "Training Epoch: 10 [3968/49669]\tLoss: 408.4009\n",
      "Training Epoch: 10 [4032/49669]\tLoss: 431.6563\n",
      "Training Epoch: 10 [4096/49669]\tLoss: 412.0763\n",
      "Training Epoch: 10 [4160/49669]\tLoss: 428.4249\n",
      "Training Epoch: 10 [4224/49669]\tLoss: 417.2519\n",
      "Training Epoch: 10 [4288/49669]\tLoss: 455.7357\n",
      "Training Epoch: 10 [4352/49669]\tLoss: 414.2970\n",
      "Training Epoch: 10 [4416/49669]\tLoss: 429.7692\n",
      "Training Epoch: 10 [4480/49669]\tLoss: 426.6281\n",
      "Training Epoch: 10 [4544/49669]\tLoss: 420.0173\n",
      "Training Epoch: 10 [4608/49669]\tLoss: 421.5232\n",
      "Training Epoch: 10 [4672/49669]\tLoss: 412.1798\n",
      "Training Epoch: 10 [4736/49669]\tLoss: 396.2232\n",
      "Training Epoch: 10 [4800/49669]\tLoss: 425.4291\n",
      "Training Epoch: 10 [4864/49669]\tLoss: 409.2453\n",
      "Training Epoch: 10 [4928/49669]\tLoss: 421.1871\n",
      "Training Epoch: 10 [4992/49669]\tLoss: 437.0141\n",
      "Training Epoch: 10 [5056/49669]\tLoss: 424.1741\n",
      "Training Epoch: 10 [5120/49669]\tLoss: 386.5701\n",
      "Training Epoch: 10 [5184/49669]\tLoss: 412.1128\n",
      "Training Epoch: 10 [5248/49669]\tLoss: 440.4425\n",
      "Training Epoch: 10 [5312/49669]\tLoss: 437.7712\n",
      "Training Epoch: 10 [5376/49669]\tLoss: 407.1405\n",
      "Training Epoch: 10 [5440/49669]\tLoss: 411.2983\n",
      "Training Epoch: 10 [5504/49669]\tLoss: 455.7867\n",
      "Training Epoch: 10 [5568/49669]\tLoss: 439.9789\n",
      "Training Epoch: 10 [5632/49669]\tLoss: 450.6411\n",
      "Training Epoch: 10 [5696/49669]\tLoss: 403.1387\n",
      "Training Epoch: 10 [5760/49669]\tLoss: 416.0939\n",
      "Training Epoch: 10 [5824/49669]\tLoss: 398.4622\n",
      "Training Epoch: 10 [5888/49669]\tLoss: 403.6592\n",
      "Training Epoch: 10 [5952/49669]\tLoss: 410.8860\n",
      "Training Epoch: 10 [6016/49669]\tLoss: 418.5991\n",
      "Training Epoch: 10 [6080/49669]\tLoss: 392.2127\n",
      "Training Epoch: 10 [6144/49669]\tLoss: 411.1160\n",
      "Training Epoch: 10 [6208/49669]\tLoss: 431.8163\n",
      "Training Epoch: 10 [6272/49669]\tLoss: 403.7893\n",
      "Training Epoch: 10 [6336/49669]\tLoss: 407.7879\n",
      "Training Epoch: 10 [6400/49669]\tLoss: 388.3610\n",
      "Training Epoch: 10 [6464/49669]\tLoss: 418.1586\n",
      "Training Epoch: 10 [6528/49669]\tLoss: 423.0463\n",
      "Training Epoch: 10 [6592/49669]\tLoss: 407.6562\n",
      "Training Epoch: 10 [6656/49669]\tLoss: 408.1793\n",
      "Training Epoch: 10 [6720/49669]\tLoss: 392.2968\n",
      "Training Epoch: 10 [6784/49669]\tLoss: 443.4684\n",
      "Training Epoch: 10 [6848/49669]\tLoss: 403.7938\n",
      "Training Epoch: 10 [6912/49669]\tLoss: 450.2485\n",
      "Training Epoch: 10 [6976/49669]\tLoss: 366.4479\n",
      "Training Epoch: 10 [7040/49669]\tLoss: 427.2709\n",
      "Training Epoch: 10 [7104/49669]\tLoss: 413.9658\n",
      "Training Epoch: 10 [7168/49669]\tLoss: 435.7542\n",
      "Training Epoch: 10 [7232/49669]\tLoss: 433.9384\n",
      "Training Epoch: 10 [7296/49669]\tLoss: 409.0569\n",
      "Training Epoch: 10 [7360/49669]\tLoss: 440.1730\n",
      "Training Epoch: 10 [7424/49669]\tLoss: 404.6746\n",
      "Training Epoch: 10 [7488/49669]\tLoss: 409.3490\n",
      "Training Epoch: 10 [7552/49669]\tLoss: 365.5473\n",
      "Training Epoch: 10 [7616/49669]\tLoss: 405.3462\n",
      "Training Epoch: 10 [7680/49669]\tLoss: 449.7219\n",
      "Training Epoch: 10 [7744/49669]\tLoss: 388.7277\n",
      "Training Epoch: 10 [7808/49669]\tLoss: 423.1120\n",
      "Training Epoch: 10 [7872/49669]\tLoss: 399.6137\n",
      "Training Epoch: 10 [7936/49669]\tLoss: 434.3316\n",
      "Training Epoch: 10 [8000/49669]\tLoss: 397.1061\n",
      "Training Epoch: 10 [8064/49669]\tLoss: 420.5239\n",
      "Training Epoch: 10 [8128/49669]\tLoss: 429.8409\n",
      "Training Epoch: 10 [8192/49669]\tLoss: 408.6986\n",
      "Training Epoch: 10 [8256/49669]\tLoss: 433.9412\n",
      "Training Epoch: 10 [8320/49669]\tLoss: 447.2112\n",
      "Training Epoch: 10 [8384/49669]\tLoss: 417.2874\n",
      "Training Epoch: 10 [8448/49669]\tLoss: 424.5535\n",
      "Training Epoch: 10 [8512/49669]\tLoss: 411.0730\n",
      "Training Epoch: 10 [8576/49669]\tLoss: 401.3915\n",
      "Training Epoch: 10 [8640/49669]\tLoss: 426.1607\n",
      "Training Epoch: 10 [8704/49669]\tLoss: 413.9529\n",
      "Training Epoch: 10 [8768/49669]\tLoss: 412.4442\n",
      "Training Epoch: 10 [8832/49669]\tLoss: 407.5150\n",
      "Training Epoch: 10 [8896/49669]\tLoss: 397.4017\n",
      "Training Epoch: 10 [8960/49669]\tLoss: 433.4354\n",
      "Training Epoch: 10 [9024/49669]\tLoss: 433.8979\n",
      "Training Epoch: 10 [9088/49669]\tLoss: 402.3770\n",
      "Training Epoch: 10 [9152/49669]\tLoss: 420.7743\n",
      "Training Epoch: 10 [9216/49669]\tLoss: 432.6959\n",
      "Training Epoch: 10 [9280/49669]\tLoss: 403.8771\n",
      "Training Epoch: 10 [9344/49669]\tLoss: 401.9053\n",
      "Training Epoch: 10 [9408/49669]\tLoss: 417.6140\n",
      "Training Epoch: 10 [9472/49669]\tLoss: 424.3593\n",
      "Training Epoch: 10 [9536/49669]\tLoss: 391.7609\n",
      "Training Epoch: 10 [9600/49669]\tLoss: 397.0273\n",
      "Training Epoch: 10 [9664/49669]\tLoss: 421.9437\n",
      "Training Epoch: 10 [9728/49669]\tLoss: 407.7570\n",
      "Training Epoch: 10 [9792/49669]\tLoss: 432.0076\n",
      "Training Epoch: 10 [9856/49669]\tLoss: 410.4917\n",
      "Training Epoch: 10 [9920/49669]\tLoss: 443.3440\n",
      "Training Epoch: 10 [9984/49669]\tLoss: 398.0402\n",
      "Training Epoch: 10 [10048/49669]\tLoss: 432.2728\n",
      "Training Epoch: 10 [10112/49669]\tLoss: 417.1015\n",
      "Training Epoch: 10 [10176/49669]\tLoss: 402.9644\n",
      "Training Epoch: 10 [10240/49669]\tLoss: 422.9769\n",
      "Training Epoch: 10 [10304/49669]\tLoss: 409.3690\n",
      "Training Epoch: 10 [10368/49669]\tLoss: 414.9861\n",
      "Training Epoch: 10 [10432/49669]\tLoss: 399.2390\n",
      "Training Epoch: 10 [10496/49669]\tLoss: 395.3839\n",
      "Training Epoch: 10 [10560/49669]\tLoss: 415.5294\n",
      "Training Epoch: 10 [10624/49669]\tLoss: 399.2909\n",
      "Training Epoch: 10 [10688/49669]\tLoss: 383.8983\n",
      "Training Epoch: 10 [10752/49669]\tLoss: 412.3840\n",
      "Training Epoch: 10 [10816/49669]\tLoss: 406.3170\n",
      "Training Epoch: 10 [10880/49669]\tLoss: 412.3100\n",
      "Training Epoch: 10 [10944/49669]\tLoss: 432.7733\n",
      "Training Epoch: 10 [11008/49669]\tLoss: 433.9305\n",
      "Training Epoch: 10 [11072/49669]\tLoss: 414.1430\n",
      "Training Epoch: 10 [11136/49669]\tLoss: 416.1409\n",
      "Training Epoch: 10 [11200/49669]\tLoss: 431.4708\n",
      "Training Epoch: 10 [11264/49669]\tLoss: 419.9435\n",
      "Training Epoch: 10 [11328/49669]\tLoss: 407.5393\n",
      "Training Epoch: 10 [11392/49669]\tLoss: 417.9442\n",
      "Training Epoch: 10 [11456/49669]\tLoss: 391.4057\n",
      "Training Epoch: 10 [11520/49669]\tLoss: 418.0950\n",
      "Training Epoch: 10 [11584/49669]\tLoss: 395.0154\n",
      "Training Epoch: 10 [11648/49669]\tLoss: 424.0216\n",
      "Training Epoch: 10 [11712/49669]\tLoss: 382.9084\n",
      "Training Epoch: 10 [11776/49669]\tLoss: 425.0419\n",
      "Training Epoch: 10 [11840/49669]\tLoss: 438.3250\n",
      "Training Epoch: 10 [11904/49669]\tLoss: 422.4139\n",
      "Training Epoch: 10 [11968/49669]\tLoss: 437.4042\n",
      "Training Epoch: 10 [12032/49669]\tLoss: 433.6095\n",
      "Training Epoch: 10 [12096/49669]\tLoss: 416.6160\n",
      "Training Epoch: 10 [12160/49669]\tLoss: 430.4523\n",
      "Training Epoch: 10 [12224/49669]\tLoss: 418.7570\n",
      "Training Epoch: 10 [12288/49669]\tLoss: 454.4395\n",
      "Training Epoch: 10 [12352/49669]\tLoss: 409.1361\n",
      "Training Epoch: 10 [12416/49669]\tLoss: 424.1722\n",
      "Training Epoch: 10 [12480/49669]\tLoss: 411.6506\n",
      "Training Epoch: 10 [12544/49669]\tLoss: 421.5505\n",
      "Training Epoch: 10 [12608/49669]\tLoss: 433.9688\n",
      "Training Epoch: 10 [12672/49669]\tLoss: 420.4702\n",
      "Training Epoch: 10 [12736/49669]\tLoss: 442.4686\n",
      "Training Epoch: 10 [12800/49669]\tLoss: 424.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [12864/49669]\tLoss: 451.6404\n",
      "Training Epoch: 10 [12928/49669]\tLoss: 413.3669\n",
      "Training Epoch: 10 [12992/49669]\tLoss: 387.3236\n",
      "Training Epoch: 10 [13056/49669]\tLoss: 431.2969\n",
      "Training Epoch: 10 [13120/49669]\tLoss: 414.2066\n",
      "Training Epoch: 10 [13184/49669]\tLoss: 443.5385\n",
      "Training Epoch: 10 [13248/49669]\tLoss: 385.3948\n",
      "Training Epoch: 10 [13312/49669]\tLoss: 412.2881\n",
      "Training Epoch: 10 [13376/49669]\tLoss: 431.7422\n",
      "Training Epoch: 10 [13440/49669]\tLoss: 386.0771\n",
      "Training Epoch: 10 [13504/49669]\tLoss: 404.3915\n",
      "Training Epoch: 10 [13568/49669]\tLoss: 398.4844\n",
      "Training Epoch: 10 [13632/49669]\tLoss: 388.0246\n",
      "Training Epoch: 10 [13696/49669]\tLoss: 400.8583\n",
      "Training Epoch: 10 [13760/49669]\tLoss: 418.3354\n",
      "Training Epoch: 10 [13824/49669]\tLoss: 419.2352\n",
      "Training Epoch: 10 [13888/49669]\tLoss: 437.0861\n",
      "Training Epoch: 10 [13952/49669]\tLoss: 418.7377\n",
      "Training Epoch: 10 [14016/49669]\tLoss: 397.0667\n",
      "Training Epoch: 10 [14080/49669]\tLoss: 422.7090\n",
      "Training Epoch: 10 [14144/49669]\tLoss: 416.4442\n",
      "Training Epoch: 10 [14208/49669]\tLoss: 407.8970\n",
      "Training Epoch: 10 [14272/49669]\tLoss: 379.5720\n",
      "Training Epoch: 10 [14336/49669]\tLoss: 393.5889\n",
      "Training Epoch: 10 [14400/49669]\tLoss: 401.9866\n",
      "Training Epoch: 10 [14464/49669]\tLoss: 423.7771\n",
      "Training Epoch: 10 [14528/49669]\tLoss: 396.5484\n",
      "Training Epoch: 10 [14592/49669]\tLoss: 422.6232\n",
      "Training Epoch: 10 [14656/49669]\tLoss: 399.8564\n",
      "Training Epoch: 10 [14720/49669]\tLoss: 425.2214\n",
      "Training Epoch: 10 [14784/49669]\tLoss: 403.5801\n",
      "Training Epoch: 10 [14848/49669]\tLoss: 416.2291\n",
      "Training Epoch: 10 [14912/49669]\tLoss: 405.3340\n",
      "Training Epoch: 10 [14976/49669]\tLoss: 403.3647\n",
      "Training Epoch: 10 [15040/49669]\tLoss: 416.9812\n",
      "Training Epoch: 10 [15104/49669]\tLoss: 414.1929\n",
      "Training Epoch: 10 [15168/49669]\tLoss: 402.3319\n",
      "Training Epoch: 10 [15232/49669]\tLoss: 428.8116\n",
      "Training Epoch: 10 [15296/49669]\tLoss: 427.7169\n",
      "Training Epoch: 10 [15360/49669]\tLoss: 391.9527\n",
      "Training Epoch: 10 [15424/49669]\tLoss: 392.5074\n",
      "Training Epoch: 10 [15488/49669]\tLoss: 427.4112\n",
      "Training Epoch: 10 [15552/49669]\tLoss: 412.6257\n",
      "Training Epoch: 10 [15616/49669]\tLoss: 418.2016\n",
      "Training Epoch: 10 [15680/49669]\tLoss: 413.3167\n",
      "Training Epoch: 10 [15744/49669]\tLoss: 403.7023\n",
      "Training Epoch: 10 [15808/49669]\tLoss: 409.1296\n",
      "Training Epoch: 10 [15872/49669]\tLoss: 402.2125\n",
      "Training Epoch: 10 [15936/49669]\tLoss: 441.5896\n",
      "Training Epoch: 10 [16000/49669]\tLoss: 406.4014\n",
      "Training Epoch: 10 [16064/49669]\tLoss: 426.7316\n",
      "Training Epoch: 10 [16128/49669]\tLoss: 416.6454\n",
      "Training Epoch: 10 [16192/49669]\tLoss: 437.6572\n",
      "Training Epoch: 10 [16256/49669]\tLoss: 426.0314\n",
      "Training Epoch: 10 [16320/49669]\tLoss: 389.1117\n",
      "Training Epoch: 10 [16384/49669]\tLoss: 406.0903\n",
      "Training Epoch: 10 [16448/49669]\tLoss: 411.3201\n",
      "Training Epoch: 10 [16512/49669]\tLoss: 433.1489\n",
      "Training Epoch: 10 [16576/49669]\tLoss: 416.0349\n",
      "Training Epoch: 10 [16640/49669]\tLoss: 427.3589\n",
      "Training Epoch: 10 [16704/49669]\tLoss: 424.9060\n",
      "Training Epoch: 10 [16768/49669]\tLoss: 446.0873\n",
      "Training Epoch: 10 [16832/49669]\tLoss: 394.0623\n",
      "Training Epoch: 10 [16896/49669]\tLoss: 414.2817\n",
      "Training Epoch: 10 [16960/49669]\tLoss: 418.0885\n",
      "Training Epoch: 10 [17024/49669]\tLoss: 427.1356\n",
      "Training Epoch: 10 [17088/49669]\tLoss: 398.0021\n",
      "Training Epoch: 10 [17152/49669]\tLoss: 377.5835\n",
      "Training Epoch: 10 [17216/49669]\tLoss: 421.2435\n",
      "Training Epoch: 10 [17280/49669]\tLoss: 429.3182\n",
      "Training Epoch: 10 [17344/49669]\tLoss: 425.8770\n",
      "Training Epoch: 10 [17408/49669]\tLoss: 411.5925\n",
      "Training Epoch: 10 [17472/49669]\tLoss: 406.6427\n",
      "Training Epoch: 10 [17536/49669]\tLoss: 398.2961\n",
      "Training Epoch: 10 [17600/49669]\tLoss: 409.5488\n",
      "Training Epoch: 10 [17664/49669]\tLoss: 414.3519\n",
      "Training Epoch: 10 [17728/49669]\tLoss: 398.9912\n",
      "Training Epoch: 10 [17792/49669]\tLoss: 449.2310\n",
      "Training Epoch: 10 [17856/49669]\tLoss: 428.8950\n",
      "Training Epoch: 10 [17920/49669]\tLoss: 428.0636\n",
      "Training Epoch: 10 [17984/49669]\tLoss: 433.5684\n",
      "Training Epoch: 10 [18048/49669]\tLoss: 409.6401\n",
      "Training Epoch: 10 [18112/49669]\tLoss: 395.7868\n",
      "Training Epoch: 10 [18176/49669]\tLoss: 401.3780\n",
      "Training Epoch: 10 [18240/49669]\tLoss: 417.0676\n",
      "Training Epoch: 10 [18304/49669]\tLoss: 410.3085\n",
      "Training Epoch: 10 [18368/49669]\tLoss: 418.2480\n",
      "Training Epoch: 10 [18432/49669]\tLoss: 425.2022\n",
      "Training Epoch: 10 [18496/49669]\tLoss: 416.8476\n",
      "Training Epoch: 10 [18560/49669]\tLoss: 436.6426\n",
      "Training Epoch: 10 [18624/49669]\tLoss: 407.3809\n",
      "Training Epoch: 10 [18688/49669]\tLoss: 416.5147\n",
      "Training Epoch: 10 [18752/49669]\tLoss: 361.3136\n",
      "Training Epoch: 10 [18816/49669]\tLoss: 410.4260\n",
      "Training Epoch: 10 [18880/49669]\tLoss: 425.0693\n",
      "Training Epoch: 10 [18944/49669]\tLoss: 431.9266\n",
      "Training Epoch: 10 [19008/49669]\tLoss: 411.2047\n",
      "Training Epoch: 10 [19072/49669]\tLoss: 420.6972\n",
      "Training Epoch: 10 [19136/49669]\tLoss: 419.7068\n",
      "Training Epoch: 10 [19200/49669]\tLoss: 409.5091\n",
      "Training Epoch: 10 [19264/49669]\tLoss: 398.1407\n",
      "Training Epoch: 10 [19328/49669]\tLoss: 419.8932\n",
      "Training Epoch: 10 [19392/49669]\tLoss: 430.5788\n",
      "Training Epoch: 10 [19456/49669]\tLoss: 383.3948\n",
      "Training Epoch: 10 [19520/49669]\tLoss: 396.1246\n",
      "Training Epoch: 10 [19584/49669]\tLoss: 435.2538\n",
      "Training Epoch: 10 [19648/49669]\tLoss: 383.2023\n",
      "Training Epoch: 10 [19712/49669]\tLoss: 406.9982\n",
      "Training Epoch: 10 [19776/49669]\tLoss: 402.3573\n",
      "Training Epoch: 10 [19840/49669]\tLoss: 403.9360\n",
      "Training Epoch: 10 [19904/49669]\tLoss: 427.4258\n",
      "Training Epoch: 10 [19968/49669]\tLoss: 398.3608\n",
      "Training Epoch: 10 [20032/49669]\tLoss: 388.8208\n",
      "Training Epoch: 10 [20096/49669]\tLoss: 400.8464\n",
      "Training Epoch: 10 [20160/49669]\tLoss: 410.9934\n",
      "Training Epoch: 10 [20224/49669]\tLoss: 414.6983\n",
      "Training Epoch: 10 [20288/49669]\tLoss: 395.3230\n",
      "Training Epoch: 10 [20352/49669]\tLoss: 440.8969\n",
      "Training Epoch: 10 [20416/49669]\tLoss: 417.2925\n",
      "Training Epoch: 10 [20480/49669]\tLoss: 397.9821\n",
      "Training Epoch: 10 [20544/49669]\tLoss: 411.4200\n",
      "Training Epoch: 10 [20608/49669]\tLoss: 410.7046\n",
      "Training Epoch: 10 [20672/49669]\tLoss: 420.2432\n",
      "Training Epoch: 10 [20736/49669]\tLoss: 408.6255\n",
      "Training Epoch: 10 [20800/49669]\tLoss: 388.9115\n",
      "Training Epoch: 10 [20864/49669]\tLoss: 440.3733\n",
      "Training Epoch: 10 [20928/49669]\tLoss: 439.0224\n",
      "Training Epoch: 10 [20992/49669]\tLoss: 433.3459\n",
      "Training Epoch: 10 [21056/49669]\tLoss: 368.8735\n",
      "Training Epoch: 10 [21120/49669]\tLoss: 388.2224\n",
      "Training Epoch: 10 [21184/49669]\tLoss: 396.6385\n",
      "Training Epoch: 10 [21248/49669]\tLoss: 393.2527\n",
      "Training Epoch: 10 [21312/49669]\tLoss: 403.0581\n",
      "Training Epoch: 10 [21376/49669]\tLoss: 408.9187\n",
      "Training Epoch: 10 [21440/49669]\tLoss: 425.9932\n",
      "Training Epoch: 10 [21504/49669]\tLoss: 390.8553\n",
      "Training Epoch: 10 [21568/49669]\tLoss: 404.3309\n",
      "Training Epoch: 10 [21632/49669]\tLoss: 403.0106\n",
      "Training Epoch: 10 [21696/49669]\tLoss: 421.7555\n",
      "Training Epoch: 10 [21760/49669]\tLoss: 385.7123\n",
      "Training Epoch: 10 [21824/49669]\tLoss: 403.7952\n",
      "Training Epoch: 10 [21888/49669]\tLoss: 430.0625\n",
      "Training Epoch: 10 [21952/49669]\tLoss: 431.0376\n",
      "Training Epoch: 10 [22016/49669]\tLoss: 431.9465\n",
      "Training Epoch: 10 [22080/49669]\tLoss: 433.0561\n",
      "Training Epoch: 10 [22144/49669]\tLoss: 405.4384\n",
      "Training Epoch: 10 [22208/49669]\tLoss: 389.8191\n",
      "Training Epoch: 10 [22272/49669]\tLoss: 392.7132\n",
      "Training Epoch: 10 [22336/49669]\tLoss: 401.0619\n",
      "Training Epoch: 10 [22400/49669]\tLoss: 389.0809\n",
      "Training Epoch: 10 [22464/49669]\tLoss: 378.3523\n",
      "Training Epoch: 10 [22528/49669]\tLoss: 434.4504\n",
      "Training Epoch: 10 [22592/49669]\tLoss: 430.6382\n",
      "Training Epoch: 10 [22656/49669]\tLoss: 420.2264\n",
      "Training Epoch: 10 [22720/49669]\tLoss: 396.1204\n",
      "Training Epoch: 10 [22784/49669]\tLoss: 412.2705\n",
      "Training Epoch: 10 [22848/49669]\tLoss: 410.2870\n",
      "Training Epoch: 10 [22912/49669]\tLoss: 390.7081\n",
      "Training Epoch: 10 [22976/49669]\tLoss: 431.5378\n",
      "Training Epoch: 10 [23040/49669]\tLoss: 419.4302\n",
      "Training Epoch: 10 [23104/49669]\tLoss: 411.8038\n",
      "Training Epoch: 10 [23168/49669]\tLoss: 410.7935\n",
      "Training Epoch: 10 [23232/49669]\tLoss: 396.6657\n",
      "Training Epoch: 10 [23296/49669]\tLoss: 413.8747\n",
      "Training Epoch: 10 [23360/49669]\tLoss: 395.8268\n",
      "Training Epoch: 10 [23424/49669]\tLoss: 419.3090\n",
      "Training Epoch: 10 [23488/49669]\tLoss: 407.3662\n",
      "Training Epoch: 10 [23552/49669]\tLoss: 405.3900\n",
      "Training Epoch: 10 [23616/49669]\tLoss: 383.8324\n",
      "Training Epoch: 10 [23680/49669]\tLoss: 433.5781\n",
      "Training Epoch: 10 [23744/49669]\tLoss: 419.6847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [23808/49669]\tLoss: 409.7161\n",
      "Training Epoch: 10 [23872/49669]\tLoss: 409.9421\n",
      "Training Epoch: 10 [23936/49669]\tLoss: 395.7144\n",
      "Training Epoch: 10 [24000/49669]\tLoss: 439.3110\n",
      "Training Epoch: 10 [24064/49669]\tLoss: 408.5070\n",
      "Training Epoch: 10 [24128/49669]\tLoss: 422.7032\n",
      "Training Epoch: 10 [24192/49669]\tLoss: 408.2470\n",
      "Training Epoch: 10 [24256/49669]\tLoss: 424.7709\n",
      "Training Epoch: 10 [24320/49669]\tLoss: 426.5014\n",
      "Training Epoch: 10 [24384/49669]\tLoss: 461.5648\n",
      "Training Epoch: 10 [24448/49669]\tLoss: 432.3814\n",
      "Training Epoch: 10 [24512/49669]\tLoss: 430.0753\n",
      "Training Epoch: 10 [24576/49669]\tLoss: 420.3363\n",
      "Training Epoch: 10 [24640/49669]\tLoss: 420.2180\n",
      "Training Epoch: 10 [24704/49669]\tLoss: 431.9104\n",
      "Training Epoch: 10 [24768/49669]\tLoss: 432.5871\n",
      "Training Epoch: 10 [24832/49669]\tLoss: 425.6600\n",
      "Training Epoch: 10 [24896/49669]\tLoss: 389.3523\n",
      "Training Epoch: 10 [24960/49669]\tLoss: 433.7482\n",
      "Training Epoch: 10 [25024/49669]\tLoss: 405.0320\n",
      "Training Epoch: 10 [25088/49669]\tLoss: 437.1998\n",
      "Training Epoch: 10 [25152/49669]\tLoss: 408.3931\n",
      "Training Epoch: 10 [25216/49669]\tLoss: 435.3282\n",
      "Training Epoch: 10 [25280/49669]\tLoss: 395.9847\n",
      "Training Epoch: 10 [25344/49669]\tLoss: 414.0587\n",
      "Training Epoch: 10 [25408/49669]\tLoss: 425.3857\n",
      "Training Epoch: 10 [25472/49669]\tLoss: 433.7910\n",
      "Training Epoch: 10 [25536/49669]\tLoss: 397.5280\n",
      "Training Epoch: 10 [25600/49669]\tLoss: 427.3669\n",
      "Training Epoch: 10 [25664/49669]\tLoss: 395.2212\n",
      "Training Epoch: 10 [25728/49669]\tLoss: 420.0134\n",
      "Training Epoch: 10 [25792/49669]\tLoss: 432.5630\n",
      "Training Epoch: 10 [25856/49669]\tLoss: 416.0045\n",
      "Training Epoch: 10 [25920/49669]\tLoss: 412.8685\n",
      "Training Epoch: 10 [25984/49669]\tLoss: 388.7204\n",
      "Training Epoch: 10 [26048/49669]\tLoss: 398.3577\n",
      "Training Epoch: 10 [26112/49669]\tLoss: 435.9262\n",
      "Training Epoch: 10 [26176/49669]\tLoss: 393.9276\n",
      "Training Epoch: 10 [26240/49669]\tLoss: 407.5046\n",
      "Training Epoch: 10 [26304/49669]\tLoss: 397.3644\n",
      "Training Epoch: 10 [26368/49669]\tLoss: 409.0477\n",
      "Training Epoch: 10 [26432/49669]\tLoss: 396.9853\n",
      "Training Epoch: 10 [26496/49669]\tLoss: 416.0805\n",
      "Training Epoch: 10 [26560/49669]\tLoss: 409.8050\n",
      "Training Epoch: 10 [26624/49669]\tLoss: 404.8008\n",
      "Training Epoch: 10 [26688/49669]\tLoss: 421.1927\n",
      "Training Epoch: 10 [26752/49669]\tLoss: 446.9597\n",
      "Training Epoch: 10 [26816/49669]\tLoss: 417.2711\n",
      "Training Epoch: 10 [26880/49669]\tLoss: 421.5928\n",
      "Training Epoch: 10 [26944/49669]\tLoss: 414.5076\n",
      "Training Epoch: 10 [27008/49669]\tLoss: 419.4351\n",
      "Training Epoch: 10 [27072/49669]\tLoss: 429.9783\n",
      "Training Epoch: 10 [27136/49669]\tLoss: 397.4087\n",
      "Training Epoch: 10 [27200/49669]\tLoss: 407.5857\n",
      "Training Epoch: 10 [27264/49669]\tLoss: 435.2572\n",
      "Training Epoch: 10 [27328/49669]\tLoss: 381.7834\n",
      "Training Epoch: 10 [27392/49669]\tLoss: 445.9024\n",
      "Training Epoch: 10 [27456/49669]\tLoss: 404.2208\n",
      "Training Epoch: 10 [27520/49669]\tLoss: 423.1903\n",
      "Training Epoch: 10 [27584/49669]\tLoss: 406.6798\n",
      "Training Epoch: 10 [27648/49669]\tLoss: 409.5463\n",
      "Training Epoch: 10 [27712/49669]\tLoss: 400.9698\n",
      "Training Epoch: 10 [27776/49669]\tLoss: 425.2507\n",
      "Training Epoch: 10 [27840/49669]\tLoss: 418.4486\n",
      "Training Epoch: 10 [27904/49669]\tLoss: 418.9596\n",
      "Training Epoch: 10 [27968/49669]\tLoss: 420.7242\n",
      "Training Epoch: 10 [28032/49669]\tLoss: 411.7933\n",
      "Training Epoch: 10 [28096/49669]\tLoss: 436.6918\n",
      "Training Epoch: 10 [28160/49669]\tLoss: 419.8510\n",
      "Training Epoch: 10 [28224/49669]\tLoss: 394.1010\n",
      "Training Epoch: 10 [28288/49669]\tLoss: 413.5775\n",
      "Training Epoch: 10 [28352/49669]\tLoss: 422.3997\n",
      "Training Epoch: 10 [28416/49669]\tLoss: 393.2151\n",
      "Training Epoch: 10 [28480/49669]\tLoss: 389.1212\n",
      "Training Epoch: 10 [28544/49669]\tLoss: 406.7979\n",
      "Training Epoch: 10 [28608/49669]\tLoss: 426.3631\n",
      "Training Epoch: 10 [28672/49669]\tLoss: 408.8154\n",
      "Training Epoch: 10 [28736/49669]\tLoss: 403.0821\n",
      "Training Epoch: 10 [28800/49669]\tLoss: 418.9790\n",
      "Training Epoch: 10 [28864/49669]\tLoss: 424.2369\n",
      "Training Epoch: 10 [28928/49669]\tLoss: 410.0894\n",
      "Training Epoch: 10 [28992/49669]\tLoss: 417.8328\n",
      "Training Epoch: 10 [29056/49669]\tLoss: 430.1868\n",
      "Training Epoch: 10 [29120/49669]\tLoss: 432.4310\n",
      "Training Epoch: 10 [29184/49669]\tLoss: 393.8966\n",
      "Training Epoch: 10 [29248/49669]\tLoss: 422.6471\n",
      "Training Epoch: 10 [29312/49669]\tLoss: 391.9814\n",
      "Training Epoch: 10 [29376/49669]\tLoss: 405.8934\n",
      "Training Epoch: 10 [29440/49669]\tLoss: 409.2715\n",
      "Training Epoch: 10 [29504/49669]\tLoss: 413.6343\n",
      "Training Epoch: 10 [29568/49669]\tLoss: 414.2028\n",
      "Training Epoch: 10 [29632/49669]\tLoss: 401.0395\n",
      "Training Epoch: 10 [29696/49669]\tLoss: 419.9369\n",
      "Training Epoch: 10 [29760/49669]\tLoss: 426.7354\n",
      "Training Epoch: 10 [29824/49669]\tLoss: 405.1931\n",
      "Training Epoch: 10 [29888/49669]\tLoss: 410.8229\n",
      "Training Epoch: 10 [29952/49669]\tLoss: 404.8453\n",
      "Training Epoch: 10 [30016/49669]\tLoss: 408.0667\n",
      "Training Epoch: 10 [30080/49669]\tLoss: 394.1265\n",
      "Training Epoch: 10 [30144/49669]\tLoss: 408.9695\n",
      "Training Epoch: 10 [30208/49669]\tLoss: 384.5742\n",
      "Training Epoch: 10 [30272/49669]\tLoss: 417.5242\n",
      "Training Epoch: 10 [30336/49669]\tLoss: 421.9422\n",
      "Training Epoch: 10 [30400/49669]\tLoss: 390.8801\n",
      "Training Epoch: 10 [30464/49669]\tLoss: 389.1638\n",
      "Training Epoch: 10 [30528/49669]\tLoss: 407.5772\n",
      "Training Epoch: 10 [30592/49669]\tLoss: 411.2069\n",
      "Training Epoch: 10 [30656/49669]\tLoss: 408.9697\n",
      "Training Epoch: 10 [30720/49669]\tLoss: 390.5135\n",
      "Training Epoch: 10 [30784/49669]\tLoss: 396.9423\n",
      "Training Epoch: 10 [30848/49669]\tLoss: 424.7741\n",
      "Training Epoch: 10 [30912/49669]\tLoss: 405.3253\n",
      "Training Epoch: 10 [30976/49669]\tLoss: 402.9628\n",
      "Training Epoch: 10 [31040/49669]\tLoss: 406.7538\n",
      "Training Epoch: 10 [31104/49669]\tLoss: 415.0993\n",
      "Training Epoch: 10 [31168/49669]\tLoss: 388.5332\n",
      "Training Epoch: 10 [31232/49669]\tLoss: 414.4633\n",
      "Training Epoch: 10 [31296/49669]\tLoss: 431.4248\n",
      "Training Epoch: 10 [31360/49669]\tLoss: 371.0557\n",
      "Training Epoch: 10 [31424/49669]\tLoss: 426.0779\n",
      "Training Epoch: 10 [31488/49669]\tLoss: 380.9109\n",
      "Training Epoch: 10 [31552/49669]\tLoss: 421.5004\n",
      "Training Epoch: 10 [31616/49669]\tLoss: 425.5756\n",
      "Training Epoch: 10 [31680/49669]\tLoss: 409.3282\n",
      "Training Epoch: 10 [31744/49669]\tLoss: 410.3727\n",
      "Training Epoch: 10 [31808/49669]\tLoss: 430.8741\n",
      "Training Epoch: 10 [31872/49669]\tLoss: 415.6341\n",
      "Training Epoch: 10 [31936/49669]\tLoss: 421.0523\n",
      "Training Epoch: 10 [32000/49669]\tLoss: 410.7582\n",
      "Training Epoch: 10 [32064/49669]\tLoss: 399.5927\n",
      "Training Epoch: 10 [32128/49669]\tLoss: 412.2749\n",
      "Training Epoch: 10 [32192/49669]\tLoss: 459.4022\n",
      "Training Epoch: 10 [32256/49669]\tLoss: 446.9070\n",
      "Training Epoch: 10 [32320/49669]\tLoss: 442.4521\n",
      "Training Epoch: 10 [32384/49669]\tLoss: 448.5929\n",
      "Training Epoch: 10 [32448/49669]\tLoss: 479.5675\n",
      "Training Epoch: 10 [32512/49669]\tLoss: 455.5228\n",
      "Training Epoch: 10 [32576/49669]\tLoss: 509.8965\n",
      "Training Epoch: 10 [32640/49669]\tLoss: 539.8202\n",
      "Training Epoch: 10 [32704/49669]\tLoss: 593.8442\n",
      "Training Epoch: 10 [32768/49669]\tLoss: 592.6322\n",
      "Training Epoch: 10 [32832/49669]\tLoss: 515.6147\n",
      "Training Epoch: 10 [32896/49669]\tLoss: 466.3697\n",
      "Training Epoch: 10 [32960/49669]\tLoss: 405.8158\n",
      "Training Epoch: 10 [33024/49669]\tLoss: 485.4003\n",
      "Training Epoch: 10 [33088/49669]\tLoss: 560.1609\n",
      "Training Epoch: 10 [33152/49669]\tLoss: 521.8270\n",
      "Training Epoch: 10 [33216/49669]\tLoss: 454.9186\n",
      "Training Epoch: 10 [33280/49669]\tLoss: 414.5194\n",
      "Training Epoch: 10 [33344/49669]\tLoss: 496.6869\n",
      "Training Epoch: 10 [33408/49669]\tLoss: 512.6198\n",
      "Training Epoch: 10 [33472/49669]\tLoss: 451.4449\n",
      "Training Epoch: 10 [33536/49669]\tLoss: 400.3242\n",
      "Training Epoch: 10 [33600/49669]\tLoss: 434.5168\n",
      "Training Epoch: 10 [33664/49669]\tLoss: 481.1349\n",
      "Training Epoch: 10 [33728/49669]\tLoss: 458.0065\n",
      "Training Epoch: 10 [33792/49669]\tLoss: 435.9162\n",
      "Training Epoch: 10 [33856/49669]\tLoss: 400.1833\n",
      "Training Epoch: 10 [33920/49669]\tLoss: 432.1270\n",
      "Training Epoch: 10 [33984/49669]\tLoss: 415.0775\n",
      "Training Epoch: 10 [34048/49669]\tLoss: 385.9491\n",
      "Training Epoch: 10 [34112/49669]\tLoss: 434.0811\n",
      "Training Epoch: 10 [34176/49669]\tLoss: 430.3180\n",
      "Training Epoch: 10 [34240/49669]\tLoss: 425.2494\n",
      "Training Epoch: 10 [34304/49669]\tLoss: 429.5357\n",
      "Training Epoch: 10 [34368/49669]\tLoss: 425.4083\n",
      "Training Epoch: 10 [34432/49669]\tLoss: 413.9733\n",
      "Training Epoch: 10 [34496/49669]\tLoss: 410.5213\n",
      "Training Epoch: 10 [34560/49669]\tLoss: 426.7408\n",
      "Training Epoch: 10 [34624/49669]\tLoss: 450.2152\n",
      "Training Epoch: 10 [34688/49669]\tLoss: 377.9159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [34752/49669]\tLoss: 421.4215\n",
      "Training Epoch: 10 [34816/49669]\tLoss: 407.2849\n",
      "Training Epoch: 10 [34880/49669]\tLoss: 425.2390\n",
      "Training Epoch: 10 [34944/49669]\tLoss: 433.6429\n",
      "Training Epoch: 10 [35008/49669]\tLoss: 379.0604\n",
      "Training Epoch: 10 [35072/49669]\tLoss: 423.3388\n",
      "Training Epoch: 10 [35136/49669]\tLoss: 409.6739\n",
      "Training Epoch: 10 [35200/49669]\tLoss: 413.5844\n",
      "Training Epoch: 10 [35264/49669]\tLoss: 402.0481\n",
      "Training Epoch: 10 [35328/49669]\tLoss: 412.7093\n",
      "Training Epoch: 10 [35392/49669]\tLoss: 434.9862\n",
      "Training Epoch: 10 [35456/49669]\tLoss: 410.5423\n",
      "Training Epoch: 10 [35520/49669]\tLoss: 397.5034\n",
      "Training Epoch: 10 [35584/49669]\tLoss: 399.2651\n",
      "Training Epoch: 10 [35648/49669]\tLoss: 404.8072\n",
      "Training Epoch: 10 [35712/49669]\tLoss: 443.3093\n",
      "Training Epoch: 10 [35776/49669]\tLoss: 394.8717\n",
      "Training Epoch: 10 [35840/49669]\tLoss: 416.9518\n",
      "Training Epoch: 10 [35904/49669]\tLoss: 407.9468\n",
      "Training Epoch: 10 [35968/49669]\tLoss: 435.5150\n",
      "Training Epoch: 10 [36032/49669]\tLoss: 406.5779\n",
      "Training Epoch: 10 [36096/49669]\tLoss: 434.2457\n",
      "Training Epoch: 10 [36160/49669]\tLoss: 407.6206\n",
      "Training Epoch: 10 [36224/49669]\tLoss: 427.0927\n",
      "Training Epoch: 10 [36288/49669]\tLoss: 409.4718\n",
      "Training Epoch: 10 [36352/49669]\tLoss: 400.7362\n",
      "Training Epoch: 10 [36416/49669]\tLoss: 398.8662\n",
      "Training Epoch: 10 [36480/49669]\tLoss: 418.9360\n",
      "Training Epoch: 10 [36544/49669]\tLoss: 434.8714\n",
      "Training Epoch: 10 [36608/49669]\tLoss: 385.0784\n",
      "Training Epoch: 10 [36672/49669]\tLoss: 406.5977\n",
      "Training Epoch: 10 [36736/49669]\tLoss: 395.7386\n",
      "Training Epoch: 10 [36800/49669]\tLoss: 411.4520\n",
      "Training Epoch: 10 [36864/49669]\tLoss: 420.4536\n",
      "Training Epoch: 10 [36928/49669]\tLoss: 400.0381\n",
      "Training Epoch: 10 [36992/49669]\tLoss: 407.6684\n",
      "Training Epoch: 10 [37056/49669]\tLoss: 402.1984\n",
      "Training Epoch: 10 [37120/49669]\tLoss: 409.4890\n",
      "Training Epoch: 10 [37184/49669]\tLoss: 422.7038\n",
      "Training Epoch: 10 [37248/49669]\tLoss: 400.6544\n",
      "Training Epoch: 10 [37312/49669]\tLoss: 407.0815\n",
      "Training Epoch: 10 [37376/49669]\tLoss: 404.2503\n",
      "Training Epoch: 10 [37440/49669]\tLoss: 424.0638\n",
      "Training Epoch: 10 [37504/49669]\tLoss: 434.8335\n",
      "Training Epoch: 10 [37568/49669]\tLoss: 413.6541\n",
      "Training Epoch: 10 [37632/49669]\tLoss: 431.8963\n",
      "Training Epoch: 10 [37696/49669]\tLoss: 429.2404\n",
      "Training Epoch: 10 [37760/49669]\tLoss: 426.0076\n",
      "Training Epoch: 10 [37824/49669]\tLoss: 423.8482\n",
      "Training Epoch: 10 [37888/49669]\tLoss: 425.9383\n",
      "Training Epoch: 10 [37952/49669]\tLoss: 424.8248\n",
      "Training Epoch: 10 [38016/49669]\tLoss: 462.5958\n",
      "Training Epoch: 10 [38080/49669]\tLoss: 419.1111\n",
      "Training Epoch: 10 [38144/49669]\tLoss: 424.8372\n",
      "Training Epoch: 10 [38208/49669]\tLoss: 440.4820\n",
      "Training Epoch: 10 [38272/49669]\tLoss: 420.5416\n",
      "Training Epoch: 10 [38336/49669]\tLoss: 422.3828\n",
      "Training Epoch: 10 [38400/49669]\tLoss: 419.2026\n",
      "Training Epoch: 10 [38464/49669]\tLoss: 434.8447\n",
      "Training Epoch: 10 [38528/49669]\tLoss: 417.0711\n",
      "Training Epoch: 10 [38592/49669]\tLoss: 402.3074\n",
      "Training Epoch: 10 [38656/49669]\tLoss: 407.3997\n",
      "Training Epoch: 10 [38720/49669]\tLoss: 419.9686\n",
      "Training Epoch: 10 [38784/49669]\tLoss: 424.7477\n",
      "Training Epoch: 10 [38848/49669]\tLoss: 427.9741\n",
      "Training Epoch: 10 [38912/49669]\tLoss: 423.8201\n",
      "Training Epoch: 10 [38976/49669]\tLoss: 381.7172\n",
      "Training Epoch: 10 [39040/49669]\tLoss: 396.9044\n",
      "Training Epoch: 10 [39104/49669]\tLoss: 430.0482\n",
      "Training Epoch: 10 [39168/49669]\tLoss: 398.9735\n",
      "Training Epoch: 10 [39232/49669]\tLoss: 402.7427\n",
      "Training Epoch: 10 [39296/49669]\tLoss: 397.2346\n",
      "Training Epoch: 10 [39360/49669]\tLoss: 410.4846\n",
      "Training Epoch: 10 [39424/49669]\tLoss: 414.7650\n",
      "Training Epoch: 10 [39488/49669]\tLoss: 420.5128\n",
      "Training Epoch: 10 [39552/49669]\tLoss: 439.4272\n",
      "Training Epoch: 10 [39616/49669]\tLoss: 426.4651\n",
      "Training Epoch: 10 [39680/49669]\tLoss: 392.0427\n",
      "Training Epoch: 10 [39744/49669]\tLoss: 398.9605\n",
      "Training Epoch: 10 [39808/49669]\tLoss: 437.1307\n",
      "Training Epoch: 10 [39872/49669]\tLoss: 420.8264\n",
      "Training Epoch: 10 [39936/49669]\tLoss: 419.7392\n",
      "Training Epoch: 10 [40000/49669]\tLoss: 404.8746\n",
      "Training Epoch: 10 [40064/49669]\tLoss: 398.4124\n",
      "Training Epoch: 10 [40128/49669]\tLoss: 415.0622\n",
      "Training Epoch: 10 [40192/49669]\tLoss: 408.2408\n",
      "Training Epoch: 10 [40256/49669]\tLoss: 402.1625\n",
      "Training Epoch: 10 [40320/49669]\tLoss: 425.5365\n",
      "Training Epoch: 10 [40384/49669]\tLoss: 415.4426\n",
      "Training Epoch: 10 [40448/49669]\tLoss: 412.8278\n",
      "Training Epoch: 10 [40512/49669]\tLoss: 410.8704\n",
      "Training Epoch: 10 [40576/49669]\tLoss: 399.3304\n",
      "Training Epoch: 10 [40640/49669]\tLoss: 411.0629\n",
      "Training Epoch: 10 [40704/49669]\tLoss: 432.0014\n",
      "Training Epoch: 10 [40768/49669]\tLoss: 431.6136\n",
      "Training Epoch: 10 [40832/49669]\tLoss: 404.6257\n",
      "Training Epoch: 10 [40896/49669]\tLoss: 391.4674\n",
      "Training Epoch: 10 [40960/49669]\tLoss: 421.3067\n",
      "Training Epoch: 10 [41024/49669]\tLoss: 403.7013\n",
      "Training Epoch: 10 [41088/49669]\tLoss: 417.3714\n",
      "Training Epoch: 10 [41152/49669]\tLoss: 439.2257\n",
      "Training Epoch: 10 [41216/49669]\tLoss: 440.6035\n",
      "Training Epoch: 10 [41280/49669]\tLoss: 407.3175\n",
      "Training Epoch: 10 [41344/49669]\tLoss: 449.0339\n",
      "Training Epoch: 10 [41408/49669]\tLoss: 437.6825\n",
      "Training Epoch: 10 [41472/49669]\tLoss: 413.8264\n",
      "Training Epoch: 10 [41536/49669]\tLoss: 410.7314\n",
      "Training Epoch: 10 [41600/49669]\tLoss: 437.6658\n",
      "Training Epoch: 10 [41664/49669]\tLoss: 420.6085\n",
      "Training Epoch: 10 [41728/49669]\tLoss: 451.4240\n",
      "Training Epoch: 10 [41792/49669]\tLoss: 446.6350\n",
      "Training Epoch: 10 [41856/49669]\tLoss: 401.7295\n",
      "Training Epoch: 10 [41920/49669]\tLoss: 425.5556\n",
      "Training Epoch: 10 [41984/49669]\tLoss: 388.7298\n",
      "Training Epoch: 10 [42048/49669]\tLoss: 397.0904\n",
      "Training Epoch: 10 [42112/49669]\tLoss: 425.3643\n",
      "Training Epoch: 10 [42176/49669]\tLoss: 398.1628\n",
      "Training Epoch: 10 [42240/49669]\tLoss: 400.3922\n",
      "Training Epoch: 10 [42304/49669]\tLoss: 430.8969\n",
      "Training Epoch: 10 [42368/49669]\tLoss: 432.4677\n",
      "Training Epoch: 10 [42432/49669]\tLoss: 430.4341\n",
      "Training Epoch: 10 [42496/49669]\tLoss: 434.3933\n",
      "Training Epoch: 10 [42560/49669]\tLoss: 424.1706\n",
      "Training Epoch: 10 [42624/49669]\tLoss: 431.4874\n",
      "Training Epoch: 10 [42688/49669]\tLoss: 409.3541\n",
      "Training Epoch: 10 [42752/49669]\tLoss: 410.9465\n",
      "Training Epoch: 10 [42816/49669]\tLoss: 419.6630\n",
      "Training Epoch: 10 [42880/49669]\tLoss: 394.6692\n",
      "Training Epoch: 10 [42944/49669]\tLoss: 410.1066\n",
      "Training Epoch: 10 [43008/49669]\tLoss: 425.2514\n",
      "Training Epoch: 10 [43072/49669]\tLoss: 416.9286\n",
      "Training Epoch: 10 [43136/49669]\tLoss: 410.5454\n",
      "Training Epoch: 10 [43200/49669]\tLoss: 449.9001\n",
      "Training Epoch: 10 [43264/49669]\tLoss: 418.5752\n",
      "Training Epoch: 10 [43328/49669]\tLoss: 414.4430\n",
      "Training Epoch: 10 [43392/49669]\tLoss: 390.7530\n",
      "Training Epoch: 10 [43456/49669]\tLoss: 425.0899\n",
      "Training Epoch: 10 [43520/49669]\tLoss: 416.9143\n",
      "Training Epoch: 10 [43584/49669]\tLoss: 398.3984\n",
      "Training Epoch: 10 [43648/49669]\tLoss: 402.2876\n",
      "Training Epoch: 10 [43712/49669]\tLoss: 400.3566\n",
      "Training Epoch: 10 [43776/49669]\tLoss: 409.6001\n",
      "Training Epoch: 10 [43840/49669]\tLoss: 426.6830\n",
      "Training Epoch: 10 [43904/49669]\tLoss: 408.0840\n",
      "Training Epoch: 10 [43968/49669]\tLoss: 420.1457\n",
      "Training Epoch: 10 [44032/49669]\tLoss: 435.0774\n",
      "Training Epoch: 10 [44096/49669]\tLoss: 397.9415\n",
      "Training Epoch: 10 [44160/49669]\tLoss: 417.0505\n",
      "Training Epoch: 10 [44224/49669]\tLoss: 440.4228\n",
      "Training Epoch: 10 [44288/49669]\tLoss: 412.6192\n",
      "Training Epoch: 10 [44352/49669]\tLoss: 447.7339\n",
      "Training Epoch: 10 [44416/49669]\tLoss: 397.7720\n",
      "Training Epoch: 10 [44480/49669]\tLoss: 439.4694\n",
      "Training Epoch: 10 [44544/49669]\tLoss: 392.7529\n",
      "Training Epoch: 10 [44608/49669]\tLoss: 385.5338\n",
      "Training Epoch: 10 [44672/49669]\tLoss: 402.5350\n",
      "Training Epoch: 10 [44736/49669]\tLoss: 399.8958\n",
      "Training Epoch: 10 [44800/49669]\tLoss: 432.2250\n",
      "Training Epoch: 10 [44864/49669]\tLoss: 409.8511\n",
      "Training Epoch: 10 [44928/49669]\tLoss: 458.1801\n",
      "Training Epoch: 10 [44992/49669]\tLoss: 412.8804\n",
      "Training Epoch: 10 [45056/49669]\tLoss: 434.5103\n",
      "Training Epoch: 10 [45120/49669]\tLoss: 405.1811\n",
      "Training Epoch: 10 [45184/49669]\tLoss: 396.6418\n",
      "Training Epoch: 10 [45248/49669]\tLoss: 424.7272\n",
      "Training Epoch: 10 [45312/49669]\tLoss: 403.1656\n",
      "Training Epoch: 10 [45376/49669]\tLoss: 405.0033\n",
      "Training Epoch: 10 [45440/49669]\tLoss: 416.4677\n",
      "Training Epoch: 10 [45504/49669]\tLoss: 423.7489\n",
      "Training Epoch: 10 [45568/49669]\tLoss: 400.6523\n",
      "Training Epoch: 10 [45632/49669]\tLoss: 428.3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [45696/49669]\tLoss: 418.2969\n",
      "Training Epoch: 10 [45760/49669]\tLoss: 419.1860\n",
      "Training Epoch: 10 [45824/49669]\tLoss: 407.3501\n",
      "Training Epoch: 10 [45888/49669]\tLoss: 405.0077\n",
      "Training Epoch: 10 [45952/49669]\tLoss: 396.5085\n",
      "Training Epoch: 10 [46016/49669]\tLoss: 431.1366\n",
      "Training Epoch: 10 [46080/49669]\tLoss: 380.3894\n",
      "Training Epoch: 10 [46144/49669]\tLoss: 416.1570\n",
      "Training Epoch: 10 [46208/49669]\tLoss: 397.6228\n",
      "Training Epoch: 10 [46272/49669]\tLoss: 407.3368\n",
      "Training Epoch: 10 [46336/49669]\tLoss: 412.9467\n",
      "Training Epoch: 10 [46400/49669]\tLoss: 397.9366\n",
      "Training Epoch: 10 [46464/49669]\tLoss: 415.9736\n",
      "Training Epoch: 10 [46528/49669]\tLoss: 402.2188\n",
      "Training Epoch: 10 [46592/49669]\tLoss: 425.9604\n",
      "Training Epoch: 10 [46656/49669]\tLoss: 402.7036\n",
      "Training Epoch: 10 [46720/49669]\tLoss: 423.9947\n",
      "Training Epoch: 10 [46784/49669]\tLoss: 430.3544\n",
      "Training Epoch: 10 [46848/49669]\tLoss: 426.8062\n",
      "Training Epoch: 10 [46912/49669]\tLoss: 380.4321\n",
      "Training Epoch: 10 [46976/49669]\tLoss: 413.3263\n",
      "Training Epoch: 10 [47040/49669]\tLoss: 397.6265\n",
      "Training Epoch: 10 [47104/49669]\tLoss: 413.6417\n",
      "Training Epoch: 10 [47168/49669]\tLoss: 421.7572\n",
      "Training Epoch: 10 [47232/49669]\tLoss: 412.6711\n",
      "Training Epoch: 10 [47296/49669]\tLoss: 424.0094\n",
      "Training Epoch: 10 [47360/49669]\tLoss: 414.4049\n",
      "Training Epoch: 10 [47424/49669]\tLoss: 423.2574\n",
      "Training Epoch: 10 [47488/49669]\tLoss: 422.6350\n",
      "Training Epoch: 10 [47552/49669]\tLoss: 425.5482\n",
      "Training Epoch: 10 [47616/49669]\tLoss: 426.7613\n",
      "Training Epoch: 10 [47680/49669]\tLoss: 428.2355\n",
      "Training Epoch: 10 [47744/49669]\tLoss: 381.2534\n",
      "Training Epoch: 10 [47808/49669]\tLoss: 407.9932\n",
      "Training Epoch: 10 [47872/49669]\tLoss: 392.9900\n",
      "Training Epoch: 10 [47936/49669]\tLoss: 447.2688\n",
      "Training Epoch: 10 [48000/49669]\tLoss: 425.0988\n",
      "Training Epoch: 10 [48064/49669]\tLoss: 412.9156\n",
      "Training Epoch: 10 [48128/49669]\tLoss: 370.9148\n",
      "Training Epoch: 10 [48192/49669]\tLoss: 395.6334\n",
      "Training Epoch: 10 [48256/49669]\tLoss: 398.1963\n",
      "Training Epoch: 10 [48320/49669]\tLoss: 391.4465\n",
      "Training Epoch: 10 [48384/49669]\tLoss: 411.1020\n",
      "Training Epoch: 10 [48448/49669]\tLoss: 406.2731\n",
      "Training Epoch: 10 [48512/49669]\tLoss: 377.3160\n",
      "Training Epoch: 10 [48576/49669]\tLoss: 437.9957\n",
      "Training Epoch: 10 [48640/49669]\tLoss: 426.6562\n",
      "Training Epoch: 10 [48704/49669]\tLoss: 434.4276\n",
      "Training Epoch: 10 [48768/49669]\tLoss: 415.9779\n",
      "Training Epoch: 10 [48832/49669]\tLoss: 424.1474\n",
      "Training Epoch: 10 [48896/49669]\tLoss: 402.2549\n",
      "Training Epoch: 10 [48960/49669]\tLoss: 413.9470\n",
      "Training Epoch: 10 [49024/49669]\tLoss: 404.9749\n",
      "Training Epoch: 10 [49088/49669]\tLoss: 393.8020\n",
      "Training Epoch: 10 [49152/49669]\tLoss: 420.7119\n",
      "Training Epoch: 10 [49216/49669]\tLoss: 433.7887\n",
      "Training Epoch: 10 [49280/49669]\tLoss: 407.9652\n",
      "Training Epoch: 10 [49344/49669]\tLoss: 416.3390\n",
      "Training Epoch: 10 [49408/49669]\tLoss: 397.3580\n",
      "Training Epoch: 10 [49472/49669]\tLoss: 427.3161\n",
      "Training Epoch: 10 [49536/49669]\tLoss: 390.9679\n",
      "Training Epoch: 10 [49600/49669]\tLoss: 430.3937\n",
      "Training Epoch: 10 [49664/49669]\tLoss: 413.4571\n",
      "Training Epoch: 10 [49669/49669]\tLoss: 489.2968\n",
      "Training Epoch: 10 [5519/5519]\tLoss: 414.5770\n",
      "Training Epoch: 11 [64/49669]\tLoss: 408.9099\n",
      "Training Epoch: 11 [128/49669]\tLoss: 432.6038\n",
      "Training Epoch: 11 [192/49669]\tLoss: 389.7796\n",
      "Training Epoch: 11 [256/49669]\tLoss: 410.9961\n",
      "Training Epoch: 11 [320/49669]\tLoss: 398.7928\n",
      "Training Epoch: 11 [384/49669]\tLoss: 429.4637\n",
      "Training Epoch: 11 [448/49669]\tLoss: 411.1758\n",
      "Training Epoch: 11 [512/49669]\tLoss: 418.4857\n",
      "Training Epoch: 11 [576/49669]\tLoss: 411.3422\n",
      "Training Epoch: 11 [640/49669]\tLoss: 426.3754\n",
      "Training Epoch: 11 [704/49669]\tLoss: 429.5561\n",
      "Training Epoch: 11 [768/49669]\tLoss: 416.6256\n",
      "Training Epoch: 11 [832/49669]\tLoss: 386.1873\n",
      "Training Epoch: 11 [896/49669]\tLoss: 432.5158\n",
      "Training Epoch: 11 [960/49669]\tLoss: 410.2938\n",
      "Training Epoch: 11 [1024/49669]\tLoss: 404.5757\n",
      "Training Epoch: 11 [1088/49669]\tLoss: 443.7864\n",
      "Training Epoch: 11 [1152/49669]\tLoss: 429.3208\n",
      "Training Epoch: 11 [1216/49669]\tLoss: 400.3773\n",
      "Training Epoch: 11 [1280/49669]\tLoss: 413.9815\n",
      "Training Epoch: 11 [1344/49669]\tLoss: 416.5083\n",
      "Training Epoch: 11 [1408/49669]\tLoss: 432.8480\n",
      "Training Epoch: 11 [1472/49669]\tLoss: 429.3071\n",
      "Training Epoch: 11 [1536/49669]\tLoss: 416.8832\n",
      "Training Epoch: 11 [1600/49669]\tLoss: 422.2083\n",
      "Training Epoch: 11 [1664/49669]\tLoss: 384.4141\n",
      "Training Epoch: 11 [1728/49669]\tLoss: 409.2798\n",
      "Training Epoch: 11 [1792/49669]\tLoss: 374.6058\n",
      "Training Epoch: 11 [1856/49669]\tLoss: 418.8971\n",
      "Training Epoch: 11 [1920/49669]\tLoss: 403.0888\n",
      "Training Epoch: 11 [1984/49669]\tLoss: 408.4815\n",
      "Training Epoch: 11 [2048/49669]\tLoss: 443.3900\n",
      "Training Epoch: 11 [2112/49669]\tLoss: 419.0156\n",
      "Training Epoch: 11 [2176/49669]\tLoss: 434.3242\n",
      "Training Epoch: 11 [2240/49669]\tLoss: 417.7105\n",
      "Training Epoch: 11 [2304/49669]\tLoss: 415.6832\n",
      "Training Epoch: 11 [2368/49669]\tLoss: 423.2729\n",
      "Training Epoch: 11 [2432/49669]\tLoss: 421.1021\n",
      "Training Epoch: 11 [2496/49669]\tLoss: 407.8397\n",
      "Training Epoch: 11 [2560/49669]\tLoss: 396.1901\n",
      "Training Epoch: 11 [2624/49669]\tLoss: 403.2674\n",
      "Training Epoch: 11 [2688/49669]\tLoss: 421.8942\n",
      "Training Epoch: 11 [2752/49669]\tLoss: 408.4028\n",
      "Training Epoch: 11 [2816/49669]\tLoss: 423.9252\n",
      "Training Epoch: 11 [2880/49669]\tLoss: 405.3888\n",
      "Training Epoch: 11 [2944/49669]\tLoss: 457.8104\n",
      "Training Epoch: 11 [3008/49669]\tLoss: 417.2772\n",
      "Training Epoch: 11 [3072/49669]\tLoss: 411.4268\n",
      "Training Epoch: 11 [3136/49669]\tLoss: 423.8461\n",
      "Training Epoch: 11 [3200/49669]\tLoss: 418.0195\n",
      "Training Epoch: 11 [3264/49669]\tLoss: 430.0894\n",
      "Training Epoch: 11 [3328/49669]\tLoss: 405.9713\n",
      "Training Epoch: 11 [3392/49669]\tLoss: 422.4651\n",
      "Training Epoch: 11 [3456/49669]\tLoss: 425.5367\n",
      "Training Epoch: 11 [3520/49669]\tLoss: 437.7761\n",
      "Training Epoch: 11 [3584/49669]\tLoss: 432.2293\n",
      "Training Epoch: 11 [3648/49669]\tLoss: 422.3189\n",
      "Training Epoch: 11 [3712/49669]\tLoss: 409.4546\n",
      "Training Epoch: 11 [3776/49669]\tLoss: 408.9027\n",
      "Training Epoch: 11 [3840/49669]\tLoss: 396.2357\n",
      "Training Epoch: 11 [3904/49669]\tLoss: 407.6605\n",
      "Training Epoch: 11 [3968/49669]\tLoss: 392.1982\n",
      "Training Epoch: 11 [4032/49669]\tLoss: 425.9507\n",
      "Training Epoch: 11 [4096/49669]\tLoss: 418.0788\n",
      "Training Epoch: 11 [4160/49669]\tLoss: 392.6365\n",
      "Training Epoch: 11 [4224/49669]\tLoss: 434.9057\n",
      "Training Epoch: 11 [4288/49669]\tLoss: 426.8958\n",
      "Training Epoch: 11 [4352/49669]\tLoss: 409.4159\n",
      "Training Epoch: 11 [4416/49669]\tLoss: 385.0638\n",
      "Training Epoch: 11 [4480/49669]\tLoss: 400.2511\n",
      "Training Epoch: 11 [4544/49669]\tLoss: 439.3240\n",
      "Training Epoch: 11 [4608/49669]\tLoss: 415.4546\n",
      "Training Epoch: 11 [4672/49669]\tLoss: 408.9971\n",
      "Training Epoch: 11 [4736/49669]\tLoss: 391.8298\n",
      "Training Epoch: 11 [4800/49669]\tLoss: 419.5161\n",
      "Training Epoch: 11 [4864/49669]\tLoss: 429.1429\n",
      "Training Epoch: 11 [4928/49669]\tLoss: 398.1752\n",
      "Training Epoch: 11 [4992/49669]\tLoss: 428.8752\n",
      "Training Epoch: 11 [5056/49669]\tLoss: 420.3399\n",
      "Training Epoch: 11 [5120/49669]\tLoss: 406.7794\n",
      "Training Epoch: 11 [5184/49669]\tLoss: 414.5318\n",
      "Training Epoch: 11 [5248/49669]\tLoss: 404.4687\n",
      "Training Epoch: 11 [5312/49669]\tLoss: 395.2583\n",
      "Training Epoch: 11 [5376/49669]\tLoss: 419.7654\n",
      "Training Epoch: 11 [5440/49669]\tLoss: 393.2507\n",
      "Training Epoch: 11 [5504/49669]\tLoss: 402.6686\n",
      "Training Epoch: 11 [5568/49669]\tLoss: 419.0444\n",
      "Training Epoch: 11 [5632/49669]\tLoss: 420.9864\n",
      "Training Epoch: 11 [5696/49669]\tLoss: 419.6693\n",
      "Training Epoch: 11 [5760/49669]\tLoss: 388.0849\n",
      "Training Epoch: 11 [5824/49669]\tLoss: 427.2781\n",
      "Training Epoch: 11 [5888/49669]\tLoss: 383.1213\n",
      "Training Epoch: 11 [5952/49669]\tLoss: 431.6031\n",
      "Training Epoch: 11 [6016/49669]\tLoss: 432.8393\n",
      "Training Epoch: 11 [6080/49669]\tLoss: 408.4826\n",
      "Training Epoch: 11 [6144/49669]\tLoss: 430.3133\n",
      "Training Epoch: 11 [6208/49669]\tLoss: 396.6950\n",
      "Training Epoch: 11 [6272/49669]\tLoss: 407.9886\n",
      "Training Epoch: 11 [6336/49669]\tLoss: 376.1960\n",
      "Training Epoch: 11 [6400/49669]\tLoss: 407.6034\n",
      "Training Epoch: 11 [6464/49669]\tLoss: 448.2257\n",
      "Training Epoch: 11 [6528/49669]\tLoss: 398.6575\n",
      "Training Epoch: 11 [6592/49669]\tLoss: 439.9801\n",
      "Training Epoch: 11 [6656/49669]\tLoss: 386.8380\n",
      "Training Epoch: 11 [6720/49669]\tLoss: 391.4484\n",
      "Training Epoch: 11 [6784/49669]\tLoss: 423.5747\n",
      "Training Epoch: 11 [6848/49669]\tLoss: 419.5680\n",
      "Training Epoch: 11 [6912/49669]\tLoss: 443.5010\n",
      "Training Epoch: 11 [6976/49669]\tLoss: 422.3476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [7040/49669]\tLoss: 391.5504\n",
      "Training Epoch: 11 [7104/49669]\tLoss: 440.8793\n",
      "Training Epoch: 11 [7168/49669]\tLoss: 452.7343\n",
      "Training Epoch: 11 [7232/49669]\tLoss: 396.8301\n",
      "Training Epoch: 11 [7296/49669]\tLoss: 419.5417\n",
      "Training Epoch: 11 [7360/49669]\tLoss: 420.8360\n",
      "Training Epoch: 11 [7424/49669]\tLoss: 417.0146\n",
      "Training Epoch: 11 [7488/49669]\tLoss: 407.5324\n",
      "Training Epoch: 11 [7552/49669]\tLoss: 415.9167\n",
      "Training Epoch: 11 [7616/49669]\tLoss: 400.4977\n",
      "Training Epoch: 11 [7680/49669]\tLoss: 433.1719\n",
      "Training Epoch: 11 [7744/49669]\tLoss: 400.1882\n",
      "Training Epoch: 11 [7808/49669]\tLoss: 415.8123\n",
      "Training Epoch: 11 [7872/49669]\tLoss: 401.7790\n",
      "Training Epoch: 11 [7936/49669]\tLoss: 402.8132\n",
      "Training Epoch: 11 [8000/49669]\tLoss: 411.2425\n",
      "Training Epoch: 11 [8064/49669]\tLoss: 408.6396\n",
      "Training Epoch: 11 [8128/49669]\tLoss: 431.9528\n",
      "Training Epoch: 11 [8192/49669]\tLoss: 418.3062\n",
      "Training Epoch: 11 [8256/49669]\tLoss: 403.8992\n",
      "Training Epoch: 11 [8320/49669]\tLoss: 438.6443\n",
      "Training Epoch: 11 [8384/49669]\tLoss: 428.9214\n",
      "Training Epoch: 11 [8448/49669]\tLoss: 408.9682\n",
      "Training Epoch: 11 [8512/49669]\tLoss: 421.3313\n",
      "Training Epoch: 11 [8576/49669]\tLoss: 372.7357\n",
      "Training Epoch: 11 [8640/49669]\tLoss: 376.0085\n",
      "Training Epoch: 11 [8704/49669]\tLoss: 404.6810\n",
      "Training Epoch: 11 [8768/49669]\tLoss: 426.1412\n",
      "Training Epoch: 11 [8832/49669]\tLoss: 403.1298\n",
      "Training Epoch: 11 [8896/49669]\tLoss: 448.7420\n",
      "Training Epoch: 11 [8960/49669]\tLoss: 392.4954\n",
      "Training Epoch: 11 [9024/49669]\tLoss: 403.6806\n",
      "Training Epoch: 11 [9088/49669]\tLoss: 399.0387\n",
      "Training Epoch: 11 [9152/49669]\tLoss: 421.8896\n",
      "Training Epoch: 11 [9216/49669]\tLoss: 403.3945\n",
      "Training Epoch: 11 [9280/49669]\tLoss: 432.4551\n",
      "Training Epoch: 11 [9344/49669]\tLoss: 414.7596\n",
      "Training Epoch: 11 [9408/49669]\tLoss: 417.9773\n",
      "Training Epoch: 11 [9472/49669]\tLoss: 447.6408\n",
      "Training Epoch: 11 [9536/49669]\tLoss: 421.7765\n",
      "Training Epoch: 11 [9600/49669]\tLoss: 403.7480\n",
      "Training Epoch: 11 [9664/49669]\tLoss: 439.5591\n",
      "Training Epoch: 11 [9728/49669]\tLoss: 419.1836\n",
      "Training Epoch: 11 [9792/49669]\tLoss: 421.7198\n",
      "Training Epoch: 11 [9856/49669]\tLoss: 420.9722\n",
      "Training Epoch: 11 [9920/49669]\tLoss: 430.1243\n",
      "Training Epoch: 11 [9984/49669]\tLoss: 404.5617\n",
      "Training Epoch: 11 [10048/49669]\tLoss: 411.0158\n",
      "Training Epoch: 11 [10112/49669]\tLoss: 430.4712\n",
      "Training Epoch: 11 [10176/49669]\tLoss: 409.9901\n",
      "Training Epoch: 11 [10240/49669]\tLoss: 405.9655\n",
      "Training Epoch: 11 [10304/49669]\tLoss: 397.2850\n",
      "Training Epoch: 11 [10368/49669]\tLoss: 420.7373\n",
      "Training Epoch: 11 [10432/49669]\tLoss: 403.0626\n",
      "Training Epoch: 11 [10496/49669]\tLoss: 417.2361\n",
      "Training Epoch: 11 [10560/49669]\tLoss: 429.5013\n",
      "Training Epoch: 11 [10624/49669]\tLoss: 393.1469\n",
      "Training Epoch: 11 [10688/49669]\tLoss: 395.6414\n",
      "Training Epoch: 11 [10752/49669]\tLoss: 392.4685\n",
      "Training Epoch: 11 [10816/49669]\tLoss: 398.9604\n",
      "Training Epoch: 11 [10880/49669]\tLoss: 396.1740\n",
      "Training Epoch: 11 [10944/49669]\tLoss: 406.7120\n",
      "Training Epoch: 11 [11008/49669]\tLoss: 415.2530\n",
      "Training Epoch: 11 [11072/49669]\tLoss: 418.1848\n",
      "Training Epoch: 11 [11136/49669]\tLoss: 442.7254\n",
      "Training Epoch: 11 [11200/49669]\tLoss: 436.0124\n",
      "Training Epoch: 11 [11264/49669]\tLoss: 431.4774\n",
      "Training Epoch: 11 [11328/49669]\tLoss: 432.1871\n",
      "Training Epoch: 11 [11392/49669]\tLoss: 426.2092\n",
      "Training Epoch: 11 [11456/49669]\tLoss: 435.7461\n",
      "Training Epoch: 11 [11520/49669]\tLoss: 462.3623\n",
      "Training Epoch: 11 [11584/49669]\tLoss: 450.1948\n",
      "Training Epoch: 11 [11648/49669]\tLoss: 422.2489\n",
      "Training Epoch: 11 [11712/49669]\tLoss: 434.8610\n",
      "Training Epoch: 11 [11776/49669]\tLoss: 424.6566\n",
      "Training Epoch: 11 [11840/49669]\tLoss: 437.4025\n",
      "Training Epoch: 11 [11904/49669]\tLoss: 416.1076\n",
      "Training Epoch: 11 [11968/49669]\tLoss: 435.7925\n",
      "Training Epoch: 11 [12032/49669]\tLoss: 412.2441\n",
      "Training Epoch: 11 [12096/49669]\tLoss: 396.7606\n",
      "Training Epoch: 11 [12160/49669]\tLoss: 433.4392\n",
      "Training Epoch: 11 [12224/49669]\tLoss: 409.9089\n",
      "Training Epoch: 11 [12288/49669]\tLoss: 401.1531\n",
      "Training Epoch: 11 [12352/49669]\tLoss: 420.7994\n",
      "Training Epoch: 11 [12416/49669]\tLoss: 409.5838\n",
      "Training Epoch: 11 [12480/49669]\tLoss: 387.8842\n",
      "Training Epoch: 11 [12544/49669]\tLoss: 421.2149\n",
      "Training Epoch: 11 [12608/49669]\tLoss: 413.3624\n",
      "Training Epoch: 11 [12672/49669]\tLoss: 416.4681\n",
      "Training Epoch: 11 [12736/49669]\tLoss: 395.0645\n",
      "Training Epoch: 11 [12800/49669]\tLoss: 400.0084\n",
      "Training Epoch: 11 [12864/49669]\tLoss: 410.7364\n",
      "Training Epoch: 11 [12928/49669]\tLoss: 391.2267\n",
      "Training Epoch: 11 [12992/49669]\tLoss: 446.6924\n",
      "Training Epoch: 11 [13056/49669]\tLoss: 451.6944\n",
      "Training Epoch: 11 [13120/49669]\tLoss: 420.5818\n",
      "Training Epoch: 11 [13184/49669]\tLoss: 398.1522\n",
      "Training Epoch: 11 [13248/49669]\tLoss: 396.9131\n",
      "Training Epoch: 11 [13312/49669]\tLoss: 438.6959\n",
      "Training Epoch: 11 [13376/49669]\tLoss: 424.2822\n",
      "Training Epoch: 11 [13440/49669]\tLoss: 399.5073\n",
      "Training Epoch: 11 [13504/49669]\tLoss: 375.2202\n",
      "Training Epoch: 11 [13568/49669]\tLoss: 383.2404\n",
      "Training Epoch: 11 [13632/49669]\tLoss: 429.3349\n",
      "Training Epoch: 11 [13696/49669]\tLoss: 454.5915\n",
      "Training Epoch: 11 [13760/49669]\tLoss: 406.9687\n",
      "Training Epoch: 11 [13824/49669]\tLoss: 399.5318\n",
      "Training Epoch: 11 [13888/49669]\tLoss: 368.3541\n",
      "Training Epoch: 11 [13952/49669]\tLoss: 426.9384\n",
      "Training Epoch: 11 [14016/49669]\tLoss: 399.2279\n",
      "Training Epoch: 11 [14080/49669]\tLoss: 388.3719\n",
      "Training Epoch: 11 [14144/49669]\tLoss: 418.7074\n",
      "Training Epoch: 11 [14208/49669]\tLoss: 415.0107\n",
      "Training Epoch: 11 [14272/49669]\tLoss: 400.1459\n",
      "Training Epoch: 11 [14336/49669]\tLoss: 409.0621\n",
      "Training Epoch: 11 [14400/49669]\tLoss: 410.0368\n",
      "Training Epoch: 11 [14464/49669]\tLoss: 377.9338\n",
      "Training Epoch: 11 [14528/49669]\tLoss: 434.9372\n",
      "Training Epoch: 11 [14592/49669]\tLoss: 413.0564\n",
      "Training Epoch: 11 [14656/49669]\tLoss: 431.9663\n",
      "Training Epoch: 11 [14720/49669]\tLoss: 454.1636\n",
      "Training Epoch: 11 [14784/49669]\tLoss: 433.2332\n",
      "Training Epoch: 11 [14848/49669]\tLoss: 428.3752\n",
      "Training Epoch: 11 [14912/49669]\tLoss: 417.0433\n",
      "Training Epoch: 11 [14976/49669]\tLoss: 417.3118\n",
      "Training Epoch: 11 [15040/49669]\tLoss: 432.9548\n",
      "Training Epoch: 11 [15104/49669]\tLoss: 414.6292\n",
      "Training Epoch: 11 [15168/49669]\tLoss: 454.9785\n",
      "Training Epoch: 11 [15232/49669]\tLoss: 437.6257\n",
      "Training Epoch: 11 [15296/49669]\tLoss: 404.6535\n",
      "Training Epoch: 11 [15360/49669]\tLoss: 396.6256\n",
      "Training Epoch: 11 [15424/49669]\tLoss: 433.8993\n",
      "Training Epoch: 11 [15488/49669]\tLoss: 417.3221\n",
      "Training Epoch: 11 [15552/49669]\tLoss: 417.3957\n",
      "Training Epoch: 11 [15616/49669]\tLoss: 424.1953\n",
      "Training Epoch: 11 [15680/49669]\tLoss: 424.2939\n",
      "Training Epoch: 11 [15744/49669]\tLoss: 428.4615\n",
      "Training Epoch: 11 [15808/49669]\tLoss: 428.9146\n",
      "Training Epoch: 11 [15872/49669]\tLoss: 440.0495\n",
      "Training Epoch: 11 [15936/49669]\tLoss: 419.6377\n",
      "Training Epoch: 11 [16000/49669]\tLoss: 415.3791\n",
      "Training Epoch: 11 [16064/49669]\tLoss: 427.4629\n",
      "Training Epoch: 11 [16128/49669]\tLoss: 415.9968\n",
      "Training Epoch: 11 [16192/49669]\tLoss: 426.5108\n",
      "Training Epoch: 11 [16256/49669]\tLoss: 431.0273\n",
      "Training Epoch: 11 [16320/49669]\tLoss: 440.6986\n",
      "Training Epoch: 11 [16384/49669]\tLoss: 432.3027\n",
      "Training Epoch: 11 [16448/49669]\tLoss: 388.4487\n",
      "Training Epoch: 11 [16512/49669]\tLoss: 437.5411\n",
      "Training Epoch: 11 [16576/49669]\tLoss: 384.9376\n",
      "Training Epoch: 11 [16640/49669]\tLoss: 417.8741\n",
      "Training Epoch: 11 [16704/49669]\tLoss: 427.9334\n",
      "Training Epoch: 11 [16768/49669]\tLoss: 411.7851\n",
      "Training Epoch: 11 [16832/49669]\tLoss: 430.5221\n",
      "Training Epoch: 11 [16896/49669]\tLoss: 424.7798\n",
      "Training Epoch: 11 [16960/49669]\tLoss: 434.6954\n",
      "Training Epoch: 11 [17024/49669]\tLoss: 408.0610\n",
      "Training Epoch: 11 [17088/49669]\tLoss: 432.0276\n",
      "Training Epoch: 11 [17152/49669]\tLoss: 421.3031\n",
      "Training Epoch: 11 [17216/49669]\tLoss: 409.6688\n",
      "Training Epoch: 11 [17280/49669]\tLoss: 398.5416\n",
      "Training Epoch: 11 [17344/49669]\tLoss: 413.8737\n",
      "Training Epoch: 11 [17408/49669]\tLoss: 401.9724\n",
      "Training Epoch: 11 [17472/49669]\tLoss: 419.7418\n",
      "Training Epoch: 11 [17536/49669]\tLoss: 444.5244\n",
      "Training Epoch: 11 [17600/49669]\tLoss: 394.8079\n",
      "Training Epoch: 11 [17664/49669]\tLoss: 443.0869\n",
      "Training Epoch: 11 [17728/49669]\tLoss: 417.1836\n",
      "Training Epoch: 11 [17792/49669]\tLoss: 402.4626\n",
      "Training Epoch: 11 [17856/49669]\tLoss: 417.2406\n",
      "Training Epoch: 11 [17920/49669]\tLoss: 420.2634\n",
      "Training Epoch: 11 [17984/49669]\tLoss: 380.1769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [18048/49669]\tLoss: 410.7986\n",
      "Training Epoch: 11 [18112/49669]\tLoss: 417.5493\n",
      "Training Epoch: 11 [18176/49669]\tLoss: 395.5345\n",
      "Training Epoch: 11 [18240/49669]\tLoss: 438.8190\n",
      "Training Epoch: 11 [18304/49669]\tLoss: 406.0986\n",
      "Training Epoch: 11 [18368/49669]\tLoss: 393.5881\n",
      "Training Epoch: 11 [18432/49669]\tLoss: 429.1529\n",
      "Training Epoch: 11 [18496/49669]\tLoss: 438.7985\n",
      "Training Epoch: 11 [18560/49669]\tLoss: 408.0851\n",
      "Training Epoch: 11 [18624/49669]\tLoss: 424.7555\n",
      "Training Epoch: 11 [18688/49669]\tLoss: 430.8906\n",
      "Training Epoch: 11 [18752/49669]\tLoss: 394.3506\n",
      "Training Epoch: 11 [18816/49669]\tLoss: 391.4048\n",
      "Training Epoch: 11 [18880/49669]\tLoss: 386.4727\n",
      "Training Epoch: 11 [18944/49669]\tLoss: 389.9418\n",
      "Training Epoch: 11 [19008/49669]\tLoss: 418.5875\n",
      "Training Epoch: 11 [19072/49669]\tLoss: 443.0193\n",
      "Training Epoch: 11 [19136/49669]\tLoss: 400.3215\n",
      "Training Epoch: 11 [19200/49669]\tLoss: 437.7668\n",
      "Training Epoch: 11 [19264/49669]\tLoss: 420.0365\n",
      "Training Epoch: 11 [19328/49669]\tLoss: 389.2593\n",
      "Training Epoch: 11 [19392/49669]\tLoss: 401.4011\n",
      "Training Epoch: 11 [19456/49669]\tLoss: 421.0577\n",
      "Training Epoch: 11 [19520/49669]\tLoss: 455.9775\n",
      "Training Epoch: 11 [19584/49669]\tLoss: 421.1848\n",
      "Training Epoch: 11 [19648/49669]\tLoss: 408.8394\n",
      "Training Epoch: 11 [19712/49669]\tLoss: 390.6642\n",
      "Training Epoch: 11 [19776/49669]\tLoss: 425.9124\n",
      "Training Epoch: 11 [19840/49669]\tLoss: 398.8856\n",
      "Training Epoch: 11 [19904/49669]\tLoss: 387.2119\n",
      "Training Epoch: 11 [19968/49669]\tLoss: 431.7465\n",
      "Training Epoch: 11 [20032/49669]\tLoss: 420.1692\n",
      "Training Epoch: 11 [20096/49669]\tLoss: 426.6309\n",
      "Training Epoch: 11 [20160/49669]\tLoss: 432.3877\n",
      "Training Epoch: 11 [20224/49669]\tLoss: 398.8261\n",
      "Training Epoch: 11 [20288/49669]\tLoss: 426.7216\n",
      "Training Epoch: 11 [20352/49669]\tLoss: 417.8685\n",
      "Training Epoch: 11 [20416/49669]\tLoss: 406.4478\n",
      "Training Epoch: 11 [20480/49669]\tLoss: 433.8684\n",
      "Training Epoch: 11 [20544/49669]\tLoss: 372.7475\n",
      "Training Epoch: 11 [20608/49669]\tLoss: 420.7931\n",
      "Training Epoch: 11 [20672/49669]\tLoss: 416.2285\n",
      "Training Epoch: 11 [20736/49669]\tLoss: 392.5113\n",
      "Training Epoch: 11 [20800/49669]\tLoss: 402.2232\n",
      "Training Epoch: 11 [20864/49669]\tLoss: 423.0780\n",
      "Training Epoch: 11 [20928/49669]\tLoss: 422.3914\n",
      "Training Epoch: 11 [20992/49669]\tLoss: 411.2723\n",
      "Training Epoch: 11 [21056/49669]\tLoss: 392.9067\n",
      "Training Epoch: 11 [21120/49669]\tLoss: 404.7849\n",
      "Training Epoch: 11 [21184/49669]\tLoss: 402.6437\n",
      "Training Epoch: 11 [21248/49669]\tLoss: 411.0706\n",
      "Training Epoch: 11 [21312/49669]\tLoss: 435.0365\n",
      "Training Epoch: 11 [21376/49669]\tLoss: 402.2710\n",
      "Training Epoch: 11 [21440/49669]\tLoss: 412.7747\n",
      "Training Epoch: 11 [21504/49669]\tLoss: 447.1416\n",
      "Training Epoch: 11 [21568/49669]\tLoss: 453.5807\n",
      "Training Epoch: 11 [21632/49669]\tLoss: 440.4153\n",
      "Training Epoch: 11 [21696/49669]\tLoss: 461.8674\n",
      "Training Epoch: 11 [21760/49669]\tLoss: 480.1060\n",
      "Training Epoch: 11 [21824/49669]\tLoss: 504.3858\n",
      "Training Epoch: 11 [21888/49669]\tLoss: 463.7851\n",
      "Training Epoch: 11 [21952/49669]\tLoss: 426.9090\n",
      "Training Epoch: 11 [22016/49669]\tLoss: 408.7564\n",
      "Training Epoch: 11 [22080/49669]\tLoss: 424.9258\n",
      "Training Epoch: 11 [22144/49669]\tLoss: 457.0032\n",
      "Training Epoch: 11 [22208/49669]\tLoss: 480.4914\n",
      "Training Epoch: 11 [22272/49669]\tLoss: 507.1202\n",
      "Training Epoch: 11 [22336/49669]\tLoss: 494.3635\n",
      "Training Epoch: 11 [22400/49669]\tLoss: 465.2493\n",
      "Training Epoch: 11 [22464/49669]\tLoss: 461.9563\n",
      "Training Epoch: 11 [22528/49669]\tLoss: 423.7803\n",
      "Training Epoch: 11 [22592/49669]\tLoss: 409.4365\n",
      "Training Epoch: 11 [22656/49669]\tLoss: 458.5013\n",
      "Training Epoch: 11 [22720/49669]\tLoss: 444.2772\n",
      "Training Epoch: 11 [22784/49669]\tLoss: 406.4365\n",
      "Training Epoch: 11 [22848/49669]\tLoss: 424.4446\n",
      "Training Epoch: 11 [22912/49669]\tLoss: 406.4867\n",
      "Training Epoch: 11 [22976/49669]\tLoss: 456.4448\n",
      "Training Epoch: 11 [23040/49669]\tLoss: 382.3762\n",
      "Training Epoch: 11 [23104/49669]\tLoss: 449.3928\n",
      "Training Epoch: 11 [23168/49669]\tLoss: 395.8484\n",
      "Training Epoch: 11 [23232/49669]\tLoss: 436.6127\n",
      "Training Epoch: 11 [23296/49669]\tLoss: 419.5523\n",
      "Training Epoch: 11 [23360/49669]\tLoss: 414.0931\n",
      "Training Epoch: 11 [23424/49669]\tLoss: 434.6765\n",
      "Training Epoch: 11 [23488/49669]\tLoss: 399.2829\n",
      "Training Epoch: 11 [23552/49669]\tLoss: 412.2637\n",
      "Training Epoch: 11 [23616/49669]\tLoss: 410.1116\n",
      "Training Epoch: 11 [23680/49669]\tLoss: 402.5416\n",
      "Training Epoch: 11 [23744/49669]\tLoss: 387.3865\n",
      "Training Epoch: 11 [23808/49669]\tLoss: 410.9767\n",
      "Training Epoch: 11 [23872/49669]\tLoss: 435.6600\n",
      "Training Epoch: 11 [23936/49669]\tLoss: 427.2201\n",
      "Training Epoch: 11 [24000/49669]\tLoss: 394.7791\n",
      "Training Epoch: 11 [24064/49669]\tLoss: 440.0587\n",
      "Training Epoch: 11 [24128/49669]\tLoss: 436.5183\n",
      "Training Epoch: 11 [24192/49669]\tLoss: 406.8806\n",
      "Training Epoch: 11 [24256/49669]\tLoss: 393.2974\n",
      "Training Epoch: 11 [24320/49669]\tLoss: 430.6384\n",
      "Training Epoch: 11 [24384/49669]\tLoss: 437.3883\n",
      "Training Epoch: 11 [24448/49669]\tLoss: 400.9154\n",
      "Training Epoch: 11 [24512/49669]\tLoss: 418.6111\n",
      "Training Epoch: 11 [24576/49669]\tLoss: 388.2867\n",
      "Training Epoch: 11 [24640/49669]\tLoss: 426.2172\n",
      "Training Epoch: 11 [24704/49669]\tLoss: 395.8153\n",
      "Training Epoch: 11 [24768/49669]\tLoss: 402.2206\n",
      "Training Epoch: 11 [24832/49669]\tLoss: 424.0610\n",
      "Training Epoch: 11 [24896/49669]\tLoss: 387.3198\n",
      "Training Epoch: 11 [24960/49669]\tLoss: 422.9080\n",
      "Training Epoch: 11 [25024/49669]\tLoss: 413.6277\n",
      "Training Epoch: 11 [25088/49669]\tLoss: 392.9855\n",
      "Training Epoch: 11 [25152/49669]\tLoss: 390.5492\n",
      "Training Epoch: 11 [25216/49669]\tLoss: 429.9537\n",
      "Training Epoch: 11 [25280/49669]\tLoss: 396.3463\n",
      "Training Epoch: 11 [25344/49669]\tLoss: 440.3385\n",
      "Training Epoch: 11 [25408/49669]\tLoss: 384.8235\n",
      "Training Epoch: 11 [25472/49669]\tLoss: 446.8290\n",
      "Training Epoch: 11 [25536/49669]\tLoss: 432.6440\n",
      "Training Epoch: 11 [25600/49669]\tLoss: 426.4768\n",
      "Training Epoch: 11 [25664/49669]\tLoss: 401.4583\n",
      "Training Epoch: 11 [25728/49669]\tLoss: 397.9412\n",
      "Training Epoch: 11 [25792/49669]\tLoss: 411.2200\n",
      "Training Epoch: 11 [25856/49669]\tLoss: 436.5740\n",
      "Training Epoch: 11 [25920/49669]\tLoss: 382.2620\n",
      "Training Epoch: 11 [25984/49669]\tLoss: 438.0945\n",
      "Training Epoch: 11 [26048/49669]\tLoss: 384.8459\n",
      "Training Epoch: 11 [26112/49669]\tLoss: 423.5688\n",
      "Training Epoch: 11 [26176/49669]\tLoss: 420.1038\n",
      "Training Epoch: 11 [26240/49669]\tLoss: 426.0241\n",
      "Training Epoch: 11 [26304/49669]\tLoss: 407.1249\n",
      "Training Epoch: 11 [26368/49669]\tLoss: 385.1311\n",
      "Training Epoch: 11 [26432/49669]\tLoss: 413.1514\n",
      "Training Epoch: 11 [26496/49669]\tLoss: 394.4124\n",
      "Training Epoch: 11 [26560/49669]\tLoss: 408.1595\n",
      "Training Epoch: 11 [26624/49669]\tLoss: 419.9490\n",
      "Training Epoch: 11 [26688/49669]\tLoss: 410.8113\n",
      "Training Epoch: 11 [26752/49669]\tLoss: 384.8844\n",
      "Training Epoch: 11 [26816/49669]\tLoss: 434.4134\n",
      "Training Epoch: 11 [26880/49669]\tLoss: 404.2675\n",
      "Training Epoch: 11 [26944/49669]\tLoss: 412.7813\n",
      "Training Epoch: 11 [27008/49669]\tLoss: 421.0876\n",
      "Training Epoch: 11 [27072/49669]\tLoss: 402.1652\n",
      "Training Epoch: 11 [27136/49669]\tLoss: 417.1218\n",
      "Training Epoch: 11 [27200/49669]\tLoss: 420.1737\n",
      "Training Epoch: 11 [27264/49669]\tLoss: 415.9056\n",
      "Training Epoch: 11 [27328/49669]\tLoss: 392.5804\n",
      "Training Epoch: 11 [27392/49669]\tLoss: 398.2460\n",
      "Training Epoch: 11 [27456/49669]\tLoss: 447.5250\n",
      "Training Epoch: 11 [27520/49669]\tLoss: 411.5528\n",
      "Training Epoch: 11 [27584/49669]\tLoss: 432.8797\n",
      "Training Epoch: 11 [27648/49669]\tLoss: 407.5272\n",
      "Training Epoch: 11 [27712/49669]\tLoss: 409.9075\n",
      "Training Epoch: 11 [27776/49669]\tLoss: 415.2800\n",
      "Training Epoch: 11 [27840/49669]\tLoss: 436.5659\n",
      "Training Epoch: 11 [27904/49669]\tLoss: 445.1018\n",
      "Training Epoch: 11 [27968/49669]\tLoss: 379.6471\n",
      "Training Epoch: 11 [28032/49669]\tLoss: 440.5316\n",
      "Training Epoch: 11 [28096/49669]\tLoss: 392.7257\n",
      "Training Epoch: 11 [28160/49669]\tLoss: 412.4449\n",
      "Training Epoch: 11 [28224/49669]\tLoss: 424.0226\n",
      "Training Epoch: 11 [28288/49669]\tLoss: 428.5350\n",
      "Training Epoch: 11 [28352/49669]\tLoss: 395.6202\n",
      "Training Epoch: 11 [28416/49669]\tLoss: 409.5833\n",
      "Training Epoch: 11 [28480/49669]\tLoss: 417.8515\n",
      "Training Epoch: 11 [28544/49669]\tLoss: 368.1951\n",
      "Training Epoch: 11 [28608/49669]\tLoss: 426.0096\n",
      "Training Epoch: 11 [28672/49669]\tLoss: 405.0359\n",
      "Training Epoch: 11 [28736/49669]\tLoss: 429.7157\n",
      "Training Epoch: 11 [28800/49669]\tLoss: 413.5940\n",
      "Training Epoch: 11 [28864/49669]\tLoss: 437.7715\n",
      "Training Epoch: 11 [28928/49669]\tLoss: 424.9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [28992/49669]\tLoss: 401.9270\n",
      "Training Epoch: 11 [29056/49669]\tLoss: 428.0180\n",
      "Training Epoch: 11 [29120/49669]\tLoss: 445.5690\n",
      "Training Epoch: 11 [29184/49669]\tLoss: 391.3022\n",
      "Training Epoch: 11 [29248/49669]\tLoss: 405.0833\n",
      "Training Epoch: 11 [29312/49669]\tLoss: 413.7969\n",
      "Training Epoch: 11 [29376/49669]\tLoss: 438.7039\n",
      "Training Epoch: 11 [29440/49669]\tLoss: 386.6053\n",
      "Training Epoch: 11 [29504/49669]\tLoss: 421.8864\n",
      "Training Epoch: 11 [29568/49669]\tLoss: 397.8861\n",
      "Training Epoch: 11 [29632/49669]\tLoss: 416.3734\n",
      "Training Epoch: 11 [29696/49669]\tLoss: 408.3801\n",
      "Training Epoch: 11 [29760/49669]\tLoss: 409.6906\n",
      "Training Epoch: 11 [29824/49669]\tLoss: 383.8138\n",
      "Training Epoch: 11 [29888/49669]\tLoss: 421.1576\n",
      "Training Epoch: 11 [29952/49669]\tLoss: 389.3379\n",
      "Training Epoch: 11 [30016/49669]\tLoss: 439.1830\n",
      "Training Epoch: 11 [30080/49669]\tLoss: 367.0228\n",
      "Training Epoch: 11 [30144/49669]\tLoss: 425.9719\n",
      "Training Epoch: 11 [30208/49669]\tLoss: 420.5211\n",
      "Training Epoch: 11 [30272/49669]\tLoss: 398.6739\n",
      "Training Epoch: 11 [30336/49669]\tLoss: 422.9889\n",
      "Training Epoch: 11 [30400/49669]\tLoss: 436.3666\n",
      "Training Epoch: 11 [30464/49669]\tLoss: 389.4258\n",
      "Training Epoch: 11 [30528/49669]\tLoss: 415.4507\n",
      "Training Epoch: 11 [30592/49669]\tLoss: 469.2173\n",
      "Training Epoch: 11 [30656/49669]\tLoss: 424.9738\n",
      "Training Epoch: 11 [30720/49669]\tLoss: 385.4297\n",
      "Training Epoch: 11 [30784/49669]\tLoss: 398.6296\n",
      "Training Epoch: 11 [30848/49669]\tLoss: 405.7518\n",
      "Training Epoch: 11 [30912/49669]\tLoss: 425.5555\n",
      "Training Epoch: 11 [30976/49669]\tLoss: 386.8314\n",
      "Training Epoch: 11 [31040/49669]\tLoss: 417.6170\n",
      "Training Epoch: 11 [31104/49669]\tLoss: 432.3442\n",
      "Training Epoch: 11 [31168/49669]\tLoss: 416.6087\n",
      "Training Epoch: 11 [31232/49669]\tLoss: 384.9458\n",
      "Training Epoch: 11 [31296/49669]\tLoss: 405.8973\n",
      "Training Epoch: 11 [31360/49669]\tLoss: 416.9562\n",
      "Training Epoch: 11 [31424/49669]\tLoss: 377.6078\n",
      "Training Epoch: 11 [31488/49669]\tLoss: 405.8611\n",
      "Training Epoch: 11 [31552/49669]\tLoss: 396.5158\n",
      "Training Epoch: 11 [31616/49669]\tLoss: 389.1961\n",
      "Training Epoch: 11 [31680/49669]\tLoss: 416.3401\n",
      "Training Epoch: 11 [31744/49669]\tLoss: 434.0004\n",
      "Training Epoch: 11 [31808/49669]\tLoss: 415.4077\n",
      "Training Epoch: 11 [31872/49669]\tLoss: 421.9163\n",
      "Training Epoch: 11 [31936/49669]\tLoss: 408.4615\n",
      "Training Epoch: 11 [32000/49669]\tLoss: 404.2908\n",
      "Training Epoch: 11 [32064/49669]\tLoss: 430.4395\n",
      "Training Epoch: 11 [32128/49669]\tLoss: 419.6042\n",
      "Training Epoch: 11 [32192/49669]\tLoss: 389.5096\n",
      "Training Epoch: 11 [32256/49669]\tLoss: 390.1281\n",
      "Training Epoch: 11 [32320/49669]\tLoss: 422.1676\n",
      "Training Epoch: 11 [32384/49669]\tLoss: 430.6711\n",
      "Training Epoch: 11 [32448/49669]\tLoss: 429.5819\n",
      "Training Epoch: 11 [32512/49669]\tLoss: 395.8212\n",
      "Training Epoch: 11 [32576/49669]\tLoss: 417.0186\n",
      "Training Epoch: 11 [32640/49669]\tLoss: 382.5091\n",
      "Training Epoch: 11 [32704/49669]\tLoss: 410.9877\n",
      "Training Epoch: 11 [32768/49669]\tLoss: 381.4737\n",
      "Training Epoch: 11 [32832/49669]\tLoss: 394.9961\n",
      "Training Epoch: 11 [32896/49669]\tLoss: 423.3558\n",
      "Training Epoch: 11 [32960/49669]\tLoss: 429.5382\n",
      "Training Epoch: 11 [33024/49669]\tLoss: 404.9776\n",
      "Training Epoch: 11 [33088/49669]\tLoss: 406.5272\n",
      "Training Epoch: 11 [33152/49669]\tLoss: 407.4750\n",
      "Training Epoch: 11 [33216/49669]\tLoss: 386.5782\n",
      "Training Epoch: 11 [33280/49669]\tLoss: 406.6666\n",
      "Training Epoch: 11 [33344/49669]\tLoss: 417.7697\n",
      "Training Epoch: 11 [33408/49669]\tLoss: 412.6793\n",
      "Training Epoch: 11 [33472/49669]\tLoss: 399.7518\n",
      "Training Epoch: 11 [33536/49669]\tLoss: 415.3329\n",
      "Training Epoch: 11 [33600/49669]\tLoss: 395.3869\n",
      "Training Epoch: 11 [33664/49669]\tLoss: 404.5582\n",
      "Training Epoch: 11 [33728/49669]\tLoss: 375.4669\n",
      "Training Epoch: 11 [33792/49669]\tLoss: 409.9609\n",
      "Training Epoch: 11 [33856/49669]\tLoss: 423.1320\n",
      "Training Epoch: 11 [33920/49669]\tLoss: 415.6147\n",
      "Training Epoch: 11 [33984/49669]\tLoss: 426.4342\n",
      "Training Epoch: 11 [34048/49669]\tLoss: 407.3757\n",
      "Training Epoch: 11 [34112/49669]\tLoss: 417.1520\n",
      "Training Epoch: 11 [34176/49669]\tLoss: 421.3150\n",
      "Training Epoch: 11 [34240/49669]\tLoss: 429.3957\n",
      "Training Epoch: 11 [34304/49669]\tLoss: 384.9316\n",
      "Training Epoch: 11 [34368/49669]\tLoss: 428.0909\n",
      "Training Epoch: 11 [34432/49669]\tLoss: 418.2601\n",
      "Training Epoch: 11 [34496/49669]\tLoss: 398.1091\n",
      "Training Epoch: 11 [34560/49669]\tLoss: 397.0068\n",
      "Training Epoch: 11 [34624/49669]\tLoss: 419.4617\n",
      "Training Epoch: 11 [34688/49669]\tLoss: 393.8037\n",
      "Training Epoch: 11 [34752/49669]\tLoss: 415.4904\n",
      "Training Epoch: 11 [34816/49669]\tLoss: 398.4153\n",
      "Training Epoch: 11 [34880/49669]\tLoss: 422.7285\n",
      "Training Epoch: 11 [34944/49669]\tLoss: 409.9734\n",
      "Training Epoch: 11 [35008/49669]\tLoss: 439.4794\n",
      "Training Epoch: 11 [35072/49669]\tLoss: 415.7973\n",
      "Training Epoch: 11 [35136/49669]\tLoss: 443.0708\n",
      "Training Epoch: 11 [35200/49669]\tLoss: 421.3705\n",
      "Training Epoch: 11 [35264/49669]\tLoss: 418.1691\n",
      "Training Epoch: 11 [35328/49669]\tLoss: 378.2248\n",
      "Training Epoch: 11 [35392/49669]\tLoss: 407.0708\n",
      "Training Epoch: 11 [35456/49669]\tLoss: 432.6289\n",
      "Training Epoch: 11 [35520/49669]\tLoss: 402.0002\n",
      "Training Epoch: 11 [35584/49669]\tLoss: 416.6653\n",
      "Training Epoch: 11 [35648/49669]\tLoss: 440.2608\n",
      "Training Epoch: 11 [35712/49669]\tLoss: 385.4853\n",
      "Training Epoch: 11 [35776/49669]\tLoss: 418.5933\n",
      "Training Epoch: 11 [35840/49669]\tLoss: 412.9345\n",
      "Training Epoch: 11 [35904/49669]\tLoss: 399.4115\n",
      "Training Epoch: 11 [35968/49669]\tLoss: 415.6931\n",
      "Training Epoch: 11 [36032/49669]\tLoss: 408.6086\n",
      "Training Epoch: 11 [36096/49669]\tLoss: 429.0764\n",
      "Training Epoch: 11 [36160/49669]\tLoss: 430.1816\n",
      "Training Epoch: 11 [36224/49669]\tLoss: 417.7916\n",
      "Training Epoch: 11 [36288/49669]\tLoss: 384.0044\n",
      "Training Epoch: 11 [36352/49669]\tLoss: 405.2547\n",
      "Training Epoch: 11 [36416/49669]\tLoss: 399.4339\n",
      "Training Epoch: 11 [36480/49669]\tLoss: 412.6700\n",
      "Training Epoch: 11 [36544/49669]\tLoss: 409.5524\n",
      "Training Epoch: 11 [36608/49669]\tLoss: 408.3987\n",
      "Training Epoch: 11 [36672/49669]\tLoss: 396.2110\n",
      "Training Epoch: 11 [36736/49669]\tLoss: 408.2146\n",
      "Training Epoch: 11 [36800/49669]\tLoss: 412.3434\n",
      "Training Epoch: 11 [36864/49669]\tLoss: 417.7433\n",
      "Training Epoch: 11 [36928/49669]\tLoss: 425.2021\n",
      "Training Epoch: 11 [36992/49669]\tLoss: 434.2174\n",
      "Training Epoch: 11 [37056/49669]\tLoss: 424.0885\n",
      "Training Epoch: 11 [37120/49669]\tLoss: 393.7386\n",
      "Training Epoch: 11 [37184/49669]\tLoss: 424.6964\n",
      "Training Epoch: 11 [37248/49669]\tLoss: 449.3958\n",
      "Training Epoch: 11 [37312/49669]\tLoss: 405.1054\n",
      "Training Epoch: 11 [37376/49669]\tLoss: 438.4144\n",
      "Training Epoch: 11 [37440/49669]\tLoss: 401.0249\n",
      "Training Epoch: 11 [37504/49669]\tLoss: 427.3760\n",
      "Training Epoch: 11 [37568/49669]\tLoss: 377.1626\n",
      "Training Epoch: 11 [37632/49669]\tLoss: 438.8496\n",
      "Training Epoch: 11 [37696/49669]\tLoss: 409.2892\n",
      "Training Epoch: 11 [37760/49669]\tLoss: 398.3286\n",
      "Training Epoch: 11 [37824/49669]\tLoss: 422.8968\n",
      "Training Epoch: 11 [37888/49669]\tLoss: 430.7026\n",
      "Training Epoch: 11 [37952/49669]\tLoss: 420.9581\n",
      "Training Epoch: 11 [38016/49669]\tLoss: 411.0884\n",
      "Training Epoch: 11 [38080/49669]\tLoss: 402.1036\n",
      "Training Epoch: 11 [38144/49669]\tLoss: 427.3231\n",
      "Training Epoch: 11 [38208/49669]\tLoss: 411.8322\n",
      "Training Epoch: 11 [38272/49669]\tLoss: 442.6090\n",
      "Training Epoch: 11 [38336/49669]\tLoss: 455.1745\n",
      "Training Epoch: 11 [38400/49669]\tLoss: 459.4235\n",
      "Training Epoch: 11 [38464/49669]\tLoss: 427.5143\n",
      "Training Epoch: 11 [38528/49669]\tLoss: 465.0222\n",
      "Training Epoch: 11 [38592/49669]\tLoss: 463.8437\n",
      "Training Epoch: 11 [38656/49669]\tLoss: 436.0221\n",
      "Training Epoch: 11 [38720/49669]\tLoss: 450.8319\n",
      "Training Epoch: 11 [38784/49669]\tLoss: 451.9998\n",
      "Training Epoch: 11 [38848/49669]\tLoss: 425.6382\n",
      "Training Epoch: 11 [38912/49669]\tLoss: 440.1715\n",
      "Training Epoch: 11 [38976/49669]\tLoss: 420.6855\n",
      "Training Epoch: 11 [39040/49669]\tLoss: 433.9628\n",
      "Training Epoch: 11 [39104/49669]\tLoss: 424.5544\n",
      "Training Epoch: 11 [39168/49669]\tLoss: 425.6532\n",
      "Training Epoch: 11 [39232/49669]\tLoss: 408.4318\n",
      "Training Epoch: 11 [39296/49669]\tLoss: 459.3442\n",
      "Training Epoch: 11 [39360/49669]\tLoss: 396.6496\n",
      "Training Epoch: 11 [39424/49669]\tLoss: 398.1614\n",
      "Training Epoch: 11 [39488/49669]\tLoss: 461.4632\n",
      "Training Epoch: 11 [39552/49669]\tLoss: 413.9772\n",
      "Training Epoch: 11 [39616/49669]\tLoss: 401.6174\n",
      "Training Epoch: 11 [39680/49669]\tLoss: 427.9895\n",
      "Training Epoch: 11 [39744/49669]\tLoss: 400.4040\n",
      "Training Epoch: 11 [39808/49669]\tLoss: 423.8361\n",
      "Training Epoch: 11 [39872/49669]\tLoss: 429.6009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [39936/49669]\tLoss: 420.9585\n",
      "Training Epoch: 11 [40000/49669]\tLoss: 434.7823\n",
      "Training Epoch: 11 [40064/49669]\tLoss: 405.4813\n",
      "Training Epoch: 11 [40128/49669]\tLoss: 408.4224\n",
      "Training Epoch: 11 [40192/49669]\tLoss: 386.4763\n",
      "Training Epoch: 11 [40256/49669]\tLoss: 440.1038\n",
      "Training Epoch: 11 [40320/49669]\tLoss: 388.1332\n",
      "Training Epoch: 11 [40384/49669]\tLoss: 385.5811\n",
      "Training Epoch: 11 [40448/49669]\tLoss: 422.5161\n",
      "Training Epoch: 11 [40512/49669]\tLoss: 416.7894\n",
      "Training Epoch: 11 [40576/49669]\tLoss: 419.5410\n",
      "Training Epoch: 11 [40640/49669]\tLoss: 409.4404\n",
      "Training Epoch: 11 [40704/49669]\tLoss: 435.4359\n",
      "Training Epoch: 11 [40768/49669]\tLoss: 396.8964\n",
      "Training Epoch: 11 [40832/49669]\tLoss: 423.3901\n",
      "Training Epoch: 11 [40896/49669]\tLoss: 428.2610\n",
      "Training Epoch: 11 [40960/49669]\tLoss: 432.8110\n",
      "Training Epoch: 11 [41024/49669]\tLoss: 409.0716\n",
      "Training Epoch: 11 [41088/49669]\tLoss: 418.6428\n",
      "Training Epoch: 11 [41152/49669]\tLoss: 388.7090\n",
      "Training Epoch: 11 [41216/49669]\tLoss: 398.2242\n",
      "Training Epoch: 11 [41280/49669]\tLoss: 432.7771\n",
      "Training Epoch: 11 [41344/49669]\tLoss: 405.1993\n",
      "Training Epoch: 11 [41408/49669]\tLoss: 410.4256\n",
      "Training Epoch: 11 [41472/49669]\tLoss: 446.4706\n",
      "Training Epoch: 11 [41536/49669]\tLoss: 421.8993\n",
      "Training Epoch: 11 [41600/49669]\tLoss: 402.8276\n",
      "Training Epoch: 11 [41664/49669]\tLoss: 419.3082\n",
      "Training Epoch: 11 [41728/49669]\tLoss: 442.8640\n",
      "Training Epoch: 11 [41792/49669]\tLoss: 395.5372\n",
      "Training Epoch: 11 [41856/49669]\tLoss: 395.6451\n",
      "Training Epoch: 11 [41920/49669]\tLoss: 436.1983\n",
      "Training Epoch: 11 [41984/49669]\tLoss: 407.3802\n",
      "Training Epoch: 11 [42048/49669]\tLoss: 404.0375\n",
      "Training Epoch: 11 [42112/49669]\tLoss: 414.1131\n",
      "Training Epoch: 11 [42176/49669]\tLoss: 410.1794\n",
      "Training Epoch: 11 [42240/49669]\tLoss: 399.1469\n",
      "Training Epoch: 11 [42304/49669]\tLoss: 408.9814\n",
      "Training Epoch: 11 [42368/49669]\tLoss: 415.3432\n",
      "Training Epoch: 11 [42432/49669]\tLoss: 424.6469\n",
      "Training Epoch: 11 [42496/49669]\tLoss: 436.5610\n",
      "Training Epoch: 11 [42560/49669]\tLoss: 419.3388\n",
      "Training Epoch: 11 [42624/49669]\tLoss: 411.9330\n",
      "Training Epoch: 11 [42688/49669]\tLoss: 427.1374\n",
      "Training Epoch: 11 [42752/49669]\tLoss: 393.0854\n",
      "Training Epoch: 11 [42816/49669]\tLoss: 390.4039\n",
      "Training Epoch: 11 [42880/49669]\tLoss: 388.2693\n",
      "Training Epoch: 11 [42944/49669]\tLoss: 355.9734\n",
      "Training Epoch: 11 [43008/49669]\tLoss: 436.2812\n",
      "Training Epoch: 11 [43072/49669]\tLoss: 384.7058\n",
      "Training Epoch: 11 [43136/49669]\tLoss: 439.7320\n",
      "Training Epoch: 11 [43200/49669]\tLoss: 400.7827\n",
      "Training Epoch: 11 [43264/49669]\tLoss: 411.7718\n",
      "Training Epoch: 11 [43328/49669]\tLoss: 413.9923\n",
      "Training Epoch: 11 [43392/49669]\tLoss: 416.6402\n",
      "Training Epoch: 11 [43456/49669]\tLoss: 392.3215\n",
      "Training Epoch: 11 [43520/49669]\tLoss: 407.8096\n",
      "Training Epoch: 11 [43584/49669]\tLoss: 443.5342\n",
      "Training Epoch: 11 [43648/49669]\tLoss: 388.8147\n",
      "Training Epoch: 11 [43712/49669]\tLoss: 398.1127\n",
      "Training Epoch: 11 [43776/49669]\tLoss: 407.9714\n",
      "Training Epoch: 11 [43840/49669]\tLoss: 418.2824\n",
      "Training Epoch: 11 [43904/49669]\tLoss: 406.7002\n",
      "Training Epoch: 11 [43968/49669]\tLoss: 394.1192\n",
      "Training Epoch: 11 [44032/49669]\tLoss: 403.0705\n",
      "Training Epoch: 11 [44096/49669]\tLoss: 430.0657\n",
      "Training Epoch: 11 [44160/49669]\tLoss: 399.4992\n",
      "Training Epoch: 11 [44224/49669]\tLoss: 421.1695\n",
      "Training Epoch: 11 [44288/49669]\tLoss: 414.7057\n",
      "Training Epoch: 11 [44352/49669]\tLoss: 403.7025\n",
      "Training Epoch: 11 [44416/49669]\tLoss: 420.9659\n",
      "Training Epoch: 11 [44480/49669]\tLoss: 420.8834\n",
      "Training Epoch: 11 [44544/49669]\tLoss: 401.5212\n",
      "Training Epoch: 11 [44608/49669]\tLoss: 409.2006\n",
      "Training Epoch: 11 [44672/49669]\tLoss: 390.1512\n",
      "Training Epoch: 11 [44736/49669]\tLoss: 453.9679\n",
      "Training Epoch: 11 [44800/49669]\tLoss: 429.2942\n",
      "Training Epoch: 11 [44864/49669]\tLoss: 415.0275\n",
      "Training Epoch: 11 [44928/49669]\tLoss: 416.7853\n",
      "Training Epoch: 11 [44992/49669]\tLoss: 453.0949\n",
      "Training Epoch: 11 [45056/49669]\tLoss: 429.0038\n",
      "Training Epoch: 11 [45120/49669]\tLoss: 398.4495\n",
      "Training Epoch: 11 [45184/49669]\tLoss: 413.8468\n",
      "Training Epoch: 11 [45248/49669]\tLoss: 450.7020\n",
      "Training Epoch: 11 [45312/49669]\tLoss: 407.9393\n",
      "Training Epoch: 11 [45376/49669]\tLoss: 394.9420\n",
      "Training Epoch: 11 [45440/49669]\tLoss: 446.1052\n",
      "Training Epoch: 11 [45504/49669]\tLoss: 423.8441\n",
      "Training Epoch: 11 [45568/49669]\tLoss: 422.5196\n",
      "Training Epoch: 11 [45632/49669]\tLoss: 427.1767\n",
      "Training Epoch: 11 [45696/49669]\tLoss: 422.1512\n",
      "Training Epoch: 11 [45760/49669]\tLoss: 428.0453\n",
      "Training Epoch: 11 [45824/49669]\tLoss: 428.1166\n",
      "Training Epoch: 11 [45888/49669]\tLoss: 446.2491\n",
      "Training Epoch: 11 [45952/49669]\tLoss: 409.1643\n",
      "Training Epoch: 11 [46016/49669]\tLoss: 372.4359\n",
      "Training Epoch: 11 [46080/49669]\tLoss: 430.3278\n",
      "Training Epoch: 11 [46144/49669]\tLoss: 390.3797\n",
      "Training Epoch: 11 [46208/49669]\tLoss: 438.4549\n",
      "Training Epoch: 11 [46272/49669]\tLoss: 434.1470\n",
      "Training Epoch: 11 [46336/49669]\tLoss: 383.6006\n",
      "Training Epoch: 11 [46400/49669]\tLoss: 420.1390\n",
      "Training Epoch: 11 [46464/49669]\tLoss: 416.7581\n",
      "Training Epoch: 11 [46528/49669]\tLoss: 411.4135\n",
      "Training Epoch: 11 [46592/49669]\tLoss: 427.7975\n",
      "Training Epoch: 11 [46656/49669]\tLoss: 441.4986\n",
      "Training Epoch: 11 [46720/49669]\tLoss: 407.7232\n",
      "Training Epoch: 11 [46784/49669]\tLoss: 418.2924\n",
      "Training Epoch: 11 [46848/49669]\tLoss: 420.1743\n",
      "Training Epoch: 11 [46912/49669]\tLoss: 387.7287\n",
      "Training Epoch: 11 [46976/49669]\tLoss: 416.2634\n",
      "Training Epoch: 11 [47040/49669]\tLoss: 448.1816\n",
      "Training Epoch: 11 [47104/49669]\tLoss: 435.2549\n",
      "Training Epoch: 11 [47168/49669]\tLoss: 458.6700\n",
      "Training Epoch: 11 [47232/49669]\tLoss: 449.5950\n",
      "Training Epoch: 11 [47296/49669]\tLoss: 445.0021\n",
      "Training Epoch: 11 [47360/49669]\tLoss: 433.1514\n",
      "Training Epoch: 11 [47424/49669]\tLoss: 426.0186\n",
      "Training Epoch: 11 [47488/49669]\tLoss: 376.2415\n",
      "Training Epoch: 11 [47552/49669]\tLoss: 403.3458\n",
      "Training Epoch: 11 [47616/49669]\tLoss: 426.6659\n",
      "Training Epoch: 11 [47680/49669]\tLoss: 420.1269\n",
      "Training Epoch: 11 [47744/49669]\tLoss: 417.5578\n",
      "Training Epoch: 11 [47808/49669]\tLoss: 420.2778\n",
      "Training Epoch: 11 [47872/49669]\tLoss: 433.6525\n",
      "Training Epoch: 11 [47936/49669]\tLoss: 415.7606\n",
      "Training Epoch: 11 [48000/49669]\tLoss: 456.5219\n",
      "Training Epoch: 11 [48064/49669]\tLoss: 433.8546\n",
      "Training Epoch: 11 [48128/49669]\tLoss: 428.1866\n",
      "Training Epoch: 11 [48192/49669]\tLoss: 398.0327\n",
      "Training Epoch: 11 [48256/49669]\tLoss: 388.2866\n",
      "Training Epoch: 11 [48320/49669]\tLoss: 406.2109\n",
      "Training Epoch: 11 [48384/49669]\tLoss: 423.5491\n",
      "Training Epoch: 11 [48448/49669]\tLoss: 422.1533\n",
      "Training Epoch: 11 [48512/49669]\tLoss: 390.0364\n",
      "Training Epoch: 11 [48576/49669]\tLoss: 389.7375\n",
      "Training Epoch: 11 [48640/49669]\tLoss: 422.2670\n",
      "Training Epoch: 11 [48704/49669]\tLoss: 412.1343\n",
      "Training Epoch: 11 [48768/49669]\tLoss: 397.8593\n",
      "Training Epoch: 11 [48832/49669]\tLoss: 423.7852\n",
      "Training Epoch: 11 [48896/49669]\tLoss: 402.3065\n",
      "Training Epoch: 11 [48960/49669]\tLoss: 430.1549\n",
      "Training Epoch: 11 [49024/49669]\tLoss: 400.2112\n",
      "Training Epoch: 11 [49088/49669]\tLoss: 428.7837\n",
      "Training Epoch: 11 [49152/49669]\tLoss: 378.9318\n",
      "Training Epoch: 11 [49216/49669]\tLoss: 405.2444\n",
      "Training Epoch: 11 [49280/49669]\tLoss: 423.6492\n",
      "Training Epoch: 11 [49344/49669]\tLoss: 387.0161\n",
      "Training Epoch: 11 [49408/49669]\tLoss: 414.9904\n",
      "Training Epoch: 11 [49472/49669]\tLoss: 381.7038\n",
      "Training Epoch: 11 [49536/49669]\tLoss: 399.6522\n",
      "Training Epoch: 11 [49600/49669]\tLoss: 414.3070\n",
      "Training Epoch: 11 [49664/49669]\tLoss: 413.3369\n",
      "Training Epoch: 11 [49669/49669]\tLoss: 457.2145\n",
      "Training Epoch: 11 [5519/5519]\tLoss: 415.9665\n",
      "Training Epoch: 12 [64/49669]\tLoss: 396.7943\n",
      "Training Epoch: 12 [128/49669]\tLoss: 418.7414\n",
      "Training Epoch: 12 [192/49669]\tLoss: 426.1569\n",
      "Training Epoch: 12 [256/49669]\tLoss: 425.7991\n",
      "Training Epoch: 12 [320/49669]\tLoss: 392.0133\n",
      "Training Epoch: 12 [384/49669]\tLoss: 417.7445\n",
      "Training Epoch: 12 [448/49669]\tLoss: 416.0443\n",
      "Training Epoch: 12 [512/49669]\tLoss: 415.1346\n",
      "Training Epoch: 12 [576/49669]\tLoss: 413.4930\n",
      "Training Epoch: 12 [640/49669]\tLoss: 404.5394\n",
      "Training Epoch: 12 [704/49669]\tLoss: 422.8893\n",
      "Training Epoch: 12 [768/49669]\tLoss: 425.7360\n",
      "Training Epoch: 12 [832/49669]\tLoss: 396.2192\n",
      "Training Epoch: 12 [896/49669]\tLoss: 416.0392\n",
      "Training Epoch: 12 [960/49669]\tLoss: 370.6659\n",
      "Training Epoch: 12 [1024/49669]\tLoss: 406.8269\n",
      "Training Epoch: 12 [1088/49669]\tLoss: 425.9466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [1152/49669]\tLoss: 406.0077\n",
      "Training Epoch: 12 [1216/49669]\tLoss: 394.6521\n",
      "Training Epoch: 12 [1280/49669]\tLoss: 440.7430\n",
      "Training Epoch: 12 [1344/49669]\tLoss: 435.4342\n",
      "Training Epoch: 12 [1408/49669]\tLoss: 426.6569\n",
      "Training Epoch: 12 [1472/49669]\tLoss: 426.1687\n",
      "Training Epoch: 12 [1536/49669]\tLoss: 420.3441\n",
      "Training Epoch: 12 [1600/49669]\tLoss: 419.5069\n",
      "Training Epoch: 12 [1664/49669]\tLoss: 411.8489\n",
      "Training Epoch: 12 [1728/49669]\tLoss: 409.9764\n",
      "Training Epoch: 12 [1792/49669]\tLoss: 446.8019\n",
      "Training Epoch: 12 [1856/49669]\tLoss: 420.7087\n",
      "Training Epoch: 12 [1920/49669]\tLoss: 414.9121\n",
      "Training Epoch: 12 [1984/49669]\tLoss: 432.4101\n",
      "Training Epoch: 12 [2048/49669]\tLoss: 421.6768\n",
      "Training Epoch: 12 [2112/49669]\tLoss: 427.4511\n",
      "Training Epoch: 12 [2176/49669]\tLoss: 406.5121\n",
      "Training Epoch: 12 [2240/49669]\tLoss: 453.4566\n",
      "Training Epoch: 12 [2304/49669]\tLoss: 383.4823\n",
      "Training Epoch: 12 [2368/49669]\tLoss: 419.5825\n",
      "Training Epoch: 12 [2432/49669]\tLoss: 443.8589\n",
      "Training Epoch: 12 [2496/49669]\tLoss: 421.2892\n",
      "Training Epoch: 12 [2560/49669]\tLoss: 439.7714\n",
      "Training Epoch: 12 [2624/49669]\tLoss: 418.1587\n",
      "Training Epoch: 12 [2688/49669]\tLoss: 409.0480\n",
      "Training Epoch: 12 [2752/49669]\tLoss: 396.7424\n",
      "Training Epoch: 12 [2816/49669]\tLoss: 410.9279\n",
      "Training Epoch: 12 [2880/49669]\tLoss: 402.3598\n",
      "Training Epoch: 12 [2944/49669]\tLoss: 386.6404\n",
      "Training Epoch: 12 [3008/49669]\tLoss: 418.1031\n",
      "Training Epoch: 12 [3072/49669]\tLoss: 391.9619\n",
      "Training Epoch: 12 [3136/49669]\tLoss: 424.4765\n",
      "Training Epoch: 12 [3200/49669]\tLoss: 383.9723\n",
      "Training Epoch: 12 [3264/49669]\tLoss: 425.8222\n",
      "Training Epoch: 12 [3328/49669]\tLoss: 411.0198\n",
      "Training Epoch: 12 [3392/49669]\tLoss: 437.5948\n",
      "Training Epoch: 12 [3456/49669]\tLoss: 443.6708\n",
      "Training Epoch: 12 [3520/49669]\tLoss: 417.5135\n",
      "Training Epoch: 12 [3584/49669]\tLoss: 423.1495\n",
      "Training Epoch: 12 [3648/49669]\tLoss: 390.9406\n",
      "Training Epoch: 12 [3712/49669]\tLoss: 365.4370\n",
      "Training Epoch: 12 [3776/49669]\tLoss: 407.1238\n",
      "Training Epoch: 12 [3840/49669]\tLoss: 414.0413\n",
      "Training Epoch: 12 [3904/49669]\tLoss: 432.5308\n",
      "Training Epoch: 12 [3968/49669]\tLoss: 414.3543\n",
      "Training Epoch: 12 [4032/49669]\tLoss: 421.1313\n",
      "Training Epoch: 12 [4096/49669]\tLoss: 403.9153\n",
      "Training Epoch: 12 [4160/49669]\tLoss: 431.3035\n",
      "Training Epoch: 12 [4224/49669]\tLoss: 401.8607\n",
      "Training Epoch: 12 [4288/49669]\tLoss: 431.0859\n",
      "Training Epoch: 12 [4352/49669]\tLoss: 434.0462\n",
      "Training Epoch: 12 [4416/49669]\tLoss: 412.8096\n",
      "Training Epoch: 12 [4480/49669]\tLoss: 411.1745\n",
      "Training Epoch: 12 [4544/49669]\tLoss: 399.5064\n",
      "Training Epoch: 12 [4608/49669]\tLoss: 396.6139\n",
      "Training Epoch: 12 [4672/49669]\tLoss: 415.6992\n",
      "Training Epoch: 12 [4736/49669]\tLoss: 410.5952\n",
      "Training Epoch: 12 [4800/49669]\tLoss: 411.2132\n",
      "Training Epoch: 12 [4864/49669]\tLoss: 407.4962\n",
      "Training Epoch: 12 [4928/49669]\tLoss: 426.6236\n",
      "Training Epoch: 12 [4992/49669]\tLoss: 419.8927\n",
      "Training Epoch: 12 [5056/49669]\tLoss: 436.7852\n",
      "Training Epoch: 12 [5120/49669]\tLoss: 414.4586\n",
      "Training Epoch: 12 [5184/49669]\tLoss: 398.4310\n",
      "Training Epoch: 12 [5248/49669]\tLoss: 398.3630\n",
      "Training Epoch: 12 [5312/49669]\tLoss: 417.6246\n",
      "Training Epoch: 12 [5376/49669]\tLoss: 394.9277\n",
      "Training Epoch: 12 [5440/49669]\tLoss: 399.0507\n",
      "Training Epoch: 12 [5504/49669]\tLoss: 423.7135\n",
      "Training Epoch: 12 [5568/49669]\tLoss: 416.7926\n",
      "Training Epoch: 12 [5632/49669]\tLoss: 393.8103\n",
      "Training Epoch: 12 [5696/49669]\tLoss: 398.0884\n",
      "Training Epoch: 12 [5760/49669]\tLoss: 387.8245\n",
      "Training Epoch: 12 [5824/49669]\tLoss: 392.9254\n",
      "Training Epoch: 12 [5888/49669]\tLoss: 388.8165\n",
      "Training Epoch: 12 [5952/49669]\tLoss: 409.9143\n",
      "Training Epoch: 12 [6016/49669]\tLoss: 435.5157\n",
      "Training Epoch: 12 [6080/49669]\tLoss: 420.4456\n",
      "Training Epoch: 12 [6144/49669]\tLoss: 413.2616\n",
      "Training Epoch: 12 [6208/49669]\tLoss: 395.3439\n",
      "Training Epoch: 12 [6272/49669]\tLoss: 396.1919\n",
      "Training Epoch: 12 [6336/49669]\tLoss: 419.9590\n",
      "Training Epoch: 12 [6400/49669]\tLoss: 422.4914\n",
      "Training Epoch: 12 [6464/49669]\tLoss: 423.6709\n",
      "Training Epoch: 12 [6528/49669]\tLoss: 407.7182\n",
      "Training Epoch: 12 [6592/49669]\tLoss: 391.0069\n",
      "Training Epoch: 12 [6656/49669]\tLoss: 422.3670\n",
      "Training Epoch: 12 [6720/49669]\tLoss: 407.2988\n",
      "Training Epoch: 12 [6784/49669]\tLoss: 425.0068\n",
      "Training Epoch: 12 [6848/49669]\tLoss: 405.3744\n",
      "Training Epoch: 12 [6912/49669]\tLoss: 427.8540\n",
      "Training Epoch: 12 [6976/49669]\tLoss: 400.6860\n",
      "Training Epoch: 12 [7040/49669]\tLoss: 437.7416\n",
      "Training Epoch: 12 [7104/49669]\tLoss: 430.0094\n",
      "Training Epoch: 12 [7168/49669]\tLoss: 401.5995\n",
      "Training Epoch: 12 [7232/49669]\tLoss: 445.7498\n",
      "Training Epoch: 12 [7296/49669]\tLoss: 403.9102\n",
      "Training Epoch: 12 [7360/49669]\tLoss: 392.8316\n",
      "Training Epoch: 12 [7424/49669]\tLoss: 420.1045\n",
      "Training Epoch: 12 [7488/49669]\tLoss: 405.1881\n",
      "Training Epoch: 12 [7552/49669]\tLoss: 417.7965\n",
      "Training Epoch: 12 [7616/49669]\tLoss: 404.5061\n",
      "Training Epoch: 12 [7680/49669]\tLoss: 403.0403\n",
      "Training Epoch: 12 [7744/49669]\tLoss: 399.2032\n",
      "Training Epoch: 12 [7808/49669]\tLoss: 425.2239\n",
      "Training Epoch: 12 [7872/49669]\tLoss: 405.7551\n",
      "Training Epoch: 12 [7936/49669]\tLoss: 420.9852\n",
      "Training Epoch: 12 [8000/49669]\tLoss: 400.8843\n",
      "Training Epoch: 12 [8064/49669]\tLoss: 413.2421\n",
      "Training Epoch: 12 [8128/49669]\tLoss: 406.5706\n",
      "Training Epoch: 12 [8192/49669]\tLoss: 403.8440\n",
      "Training Epoch: 12 [8256/49669]\tLoss: 401.8835\n",
      "Training Epoch: 12 [8320/49669]\tLoss: 423.1830\n",
      "Training Epoch: 12 [8384/49669]\tLoss: 429.1607\n",
      "Training Epoch: 12 [8448/49669]\tLoss: 406.1617\n",
      "Training Epoch: 12 [8512/49669]\tLoss: 421.1262\n",
      "Training Epoch: 12 [8576/49669]\tLoss: 411.2659\n",
      "Training Epoch: 12 [8640/49669]\tLoss: 420.5862\n",
      "Training Epoch: 12 [8704/49669]\tLoss: 401.9164\n",
      "Training Epoch: 12 [8768/49669]\tLoss: 423.2720\n",
      "Training Epoch: 12 [8832/49669]\tLoss: 447.3780\n",
      "Training Epoch: 12 [8896/49669]\tLoss: 440.7714\n",
      "Training Epoch: 12 [8960/49669]\tLoss: 456.4476\n",
      "Training Epoch: 12 [9024/49669]\tLoss: 453.3380\n",
      "Training Epoch: 12 [9088/49669]\tLoss: 463.3983\n",
      "Training Epoch: 12 [9152/49669]\tLoss: 494.4221\n",
      "Training Epoch: 12 [9216/49669]\tLoss: 516.6937\n",
      "Training Epoch: 12 [9280/49669]\tLoss: 521.9586\n",
      "Training Epoch: 12 [9344/49669]\tLoss: 468.0773\n",
      "Training Epoch: 12 [9408/49669]\tLoss: 453.7327\n",
      "Training Epoch: 12 [9472/49669]\tLoss: 433.0621\n",
      "Training Epoch: 12 [9536/49669]\tLoss: 416.1089\n",
      "Training Epoch: 12 [9600/49669]\tLoss: 421.8877\n",
      "Training Epoch: 12 [9664/49669]\tLoss: 433.9053\n",
      "Training Epoch: 12 [9728/49669]\tLoss: 459.2776\n",
      "Training Epoch: 12 [9792/49669]\tLoss: 445.8362\n",
      "Training Epoch: 12 [9856/49669]\tLoss: 414.9554\n",
      "Training Epoch: 12 [9920/49669]\tLoss: 439.6848\n",
      "Training Epoch: 12 [9984/49669]\tLoss: 435.6132\n",
      "Training Epoch: 12 [10048/49669]\tLoss: 417.9612\n",
      "Training Epoch: 12 [10112/49669]\tLoss: 408.8901\n",
      "Training Epoch: 12 [10176/49669]\tLoss: 442.7858\n",
      "Training Epoch: 12 [10240/49669]\tLoss: 382.3116\n",
      "Training Epoch: 12 [10304/49669]\tLoss: 423.1639\n",
      "Training Epoch: 12 [10368/49669]\tLoss: 423.4205\n",
      "Training Epoch: 12 [10432/49669]\tLoss: 439.2275\n",
      "Training Epoch: 12 [10496/49669]\tLoss: 428.3363\n",
      "Training Epoch: 12 [10560/49669]\tLoss: 417.2576\n",
      "Training Epoch: 12 [10624/49669]\tLoss: 421.0653\n",
      "Training Epoch: 12 [10688/49669]\tLoss: 402.7108\n",
      "Training Epoch: 12 [10752/49669]\tLoss: 453.3888\n",
      "Training Epoch: 12 [10816/49669]\tLoss: 403.0828\n",
      "Training Epoch: 12 [10880/49669]\tLoss: 427.9661\n",
      "Training Epoch: 12 [10944/49669]\tLoss: 409.9876\n",
      "Training Epoch: 12 [11008/49669]\tLoss: 421.4049\n",
      "Training Epoch: 12 [11072/49669]\tLoss: 449.8360\n",
      "Training Epoch: 12 [11136/49669]\tLoss: 410.1351\n",
      "Training Epoch: 12 [11200/49669]\tLoss: 435.1100\n",
      "Training Epoch: 12 [11264/49669]\tLoss: 427.7226\n",
      "Training Epoch: 12 [11328/49669]\tLoss: 433.3703\n",
      "Training Epoch: 12 [11392/49669]\tLoss: 398.8158\n",
      "Training Epoch: 12 [11456/49669]\tLoss: 384.1221\n",
      "Training Epoch: 12 [11520/49669]\tLoss: 399.5541\n",
      "Training Epoch: 12 [11584/49669]\tLoss: 390.0020\n",
      "Training Epoch: 12 [11648/49669]\tLoss: 404.2969\n",
      "Training Epoch: 12 [11712/49669]\tLoss: 413.5661\n",
      "Training Epoch: 12 [11776/49669]\tLoss: 461.0111\n",
      "Training Epoch: 12 [11840/49669]\tLoss: 414.4191\n",
      "Training Epoch: 12 [11904/49669]\tLoss: 417.1729\n",
      "Training Epoch: 12 [11968/49669]\tLoss: 417.5303\n",
      "Training Epoch: 12 [12032/49669]\tLoss: 423.0053\n",
      "Training Epoch: 12 [12096/49669]\tLoss: 414.7044\n",
      "Training Epoch: 12 [12160/49669]\tLoss: 411.8210\n",
      "Training Epoch: 12 [12224/49669]\tLoss: 413.9765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [12288/49669]\tLoss: 396.0051\n",
      "Training Epoch: 12 [12352/49669]\tLoss: 421.1273\n",
      "Training Epoch: 12 [12416/49669]\tLoss: 425.5087\n",
      "Training Epoch: 12 [12480/49669]\tLoss: 408.0837\n",
      "Training Epoch: 12 [12544/49669]\tLoss: 423.8859\n",
      "Training Epoch: 12 [12608/49669]\tLoss: 406.9805\n",
      "Training Epoch: 12 [12672/49669]\tLoss: 414.1634\n",
      "Training Epoch: 12 [12736/49669]\tLoss: 416.7799\n",
      "Training Epoch: 12 [12800/49669]\tLoss: 412.0279\n",
      "Training Epoch: 12 [12864/49669]\tLoss: 400.6213\n",
      "Training Epoch: 12 [12928/49669]\tLoss: 418.4517\n",
      "Training Epoch: 12 [12992/49669]\tLoss: 403.1984\n",
      "Training Epoch: 12 [13056/49669]\tLoss: 442.8051\n",
      "Training Epoch: 12 [13120/49669]\tLoss: 403.4766\n",
      "Training Epoch: 12 [13184/49669]\tLoss: 412.7959\n",
      "Training Epoch: 12 [13248/49669]\tLoss: 412.4316\n",
      "Training Epoch: 12 [13312/49669]\tLoss: 412.7441\n",
      "Training Epoch: 12 [13376/49669]\tLoss: 416.2157\n",
      "Training Epoch: 12 [13440/49669]\tLoss: 410.7118\n",
      "Training Epoch: 12 [13504/49669]\tLoss: 411.8514\n",
      "Training Epoch: 12 [13568/49669]\tLoss: 410.3602\n",
      "Training Epoch: 12 [13632/49669]\tLoss: 421.5187\n",
      "Training Epoch: 12 [13696/49669]\tLoss: 413.7352\n",
      "Training Epoch: 12 [13760/49669]\tLoss: 419.9467\n",
      "Training Epoch: 12 [13824/49669]\tLoss: 399.3927\n",
      "Training Epoch: 12 [13888/49669]\tLoss: 402.4034\n",
      "Training Epoch: 12 [13952/49669]\tLoss: 436.3116\n",
      "Training Epoch: 12 [14016/49669]\tLoss: 406.8992\n",
      "Training Epoch: 12 [14080/49669]\tLoss: 429.2365\n",
      "Training Epoch: 12 [14144/49669]\tLoss: 410.4803\n",
      "Training Epoch: 12 [14208/49669]\tLoss: 405.3217\n",
      "Training Epoch: 12 [14272/49669]\tLoss: 391.0019\n",
      "Training Epoch: 12 [14336/49669]\tLoss: 407.8350\n",
      "Training Epoch: 12 [14400/49669]\tLoss: 432.2019\n",
      "Training Epoch: 12 [14464/49669]\tLoss: 412.4586\n",
      "Training Epoch: 12 [14528/49669]\tLoss: 428.9261\n",
      "Training Epoch: 12 [14592/49669]\tLoss: 377.4843\n",
      "Training Epoch: 12 [14656/49669]\tLoss: 445.6300\n",
      "Training Epoch: 12 [14720/49669]\tLoss: 434.2286\n",
      "Training Epoch: 12 [14784/49669]\tLoss: 402.7462\n",
      "Training Epoch: 12 [14848/49669]\tLoss: 434.7371\n",
      "Training Epoch: 12 [14912/49669]\tLoss: 427.5507\n",
      "Training Epoch: 12 [14976/49669]\tLoss: 418.4826\n",
      "Training Epoch: 12 [15040/49669]\tLoss: 393.1371\n",
      "Training Epoch: 12 [15104/49669]\tLoss: 433.5274\n",
      "Training Epoch: 12 [15168/49669]\tLoss: 428.1343\n",
      "Training Epoch: 12 [15232/49669]\tLoss: 416.7769\n",
      "Training Epoch: 12 [15296/49669]\tLoss: 422.2426\n",
      "Training Epoch: 12 [15360/49669]\tLoss: 425.4744\n",
      "Training Epoch: 12 [15424/49669]\tLoss: 432.9770\n",
      "Training Epoch: 12 [15488/49669]\tLoss: 395.9695\n",
      "Training Epoch: 12 [15552/49669]\tLoss: 413.8029\n",
      "Training Epoch: 12 [15616/49669]\tLoss: 411.7980\n",
      "Training Epoch: 12 [15680/49669]\tLoss: 409.1397\n",
      "Training Epoch: 12 [15744/49669]\tLoss: 397.3242\n",
      "Training Epoch: 12 [15808/49669]\tLoss: 417.3918\n",
      "Training Epoch: 12 [15872/49669]\tLoss: 416.4975\n",
      "Training Epoch: 12 [15936/49669]\tLoss: 376.1872\n",
      "Training Epoch: 12 [16000/49669]\tLoss: 412.2672\n",
      "Training Epoch: 12 [16064/49669]\tLoss: 423.0241\n",
      "Training Epoch: 12 [16128/49669]\tLoss: 425.6384\n",
      "Training Epoch: 12 [16192/49669]\tLoss: 424.1775\n",
      "Training Epoch: 12 [16256/49669]\tLoss: 417.7424\n",
      "Training Epoch: 12 [16320/49669]\tLoss: 406.3178\n",
      "Training Epoch: 12 [16384/49669]\tLoss: 437.9968\n",
      "Training Epoch: 12 [16448/49669]\tLoss: 405.4665\n",
      "Training Epoch: 12 [16512/49669]\tLoss: 398.6029\n",
      "Training Epoch: 12 [16576/49669]\tLoss: 423.5614\n",
      "Training Epoch: 12 [16640/49669]\tLoss: 453.5559\n",
      "Training Epoch: 12 [16704/49669]\tLoss: 418.7191\n",
      "Training Epoch: 12 [16768/49669]\tLoss: 410.6581\n",
      "Training Epoch: 12 [16832/49669]\tLoss: 430.6381\n",
      "Training Epoch: 12 [16896/49669]\tLoss: 398.5507\n",
      "Training Epoch: 12 [16960/49669]\tLoss: 425.3693\n",
      "Training Epoch: 12 [17024/49669]\tLoss: 418.7741\n",
      "Training Epoch: 12 [17088/49669]\tLoss: 415.4492\n",
      "Training Epoch: 12 [17152/49669]\tLoss: 382.5212\n",
      "Training Epoch: 12 [17216/49669]\tLoss: 403.9102\n",
      "Training Epoch: 12 [17280/49669]\tLoss: 398.2225\n",
      "Training Epoch: 12 [17344/49669]\tLoss: 416.9705\n",
      "Training Epoch: 12 [17408/49669]\tLoss: 416.4465\n",
      "Training Epoch: 12 [17472/49669]\tLoss: 426.9215\n",
      "Training Epoch: 12 [17536/49669]\tLoss: 395.2547\n",
      "Training Epoch: 12 [17600/49669]\tLoss: 386.3697\n",
      "Training Epoch: 12 [17664/49669]\tLoss: 393.3328\n",
      "Training Epoch: 12 [17728/49669]\tLoss: 394.6838\n",
      "Training Epoch: 12 [17792/49669]\tLoss: 395.9821\n",
      "Training Epoch: 12 [17856/49669]\tLoss: 452.6311\n",
      "Training Epoch: 12 [17920/49669]\tLoss: 400.4852\n",
      "Training Epoch: 12 [17984/49669]\tLoss: 415.8554\n",
      "Training Epoch: 12 [18048/49669]\tLoss: 415.2833\n",
      "Training Epoch: 12 [18112/49669]\tLoss: 434.1874\n",
      "Training Epoch: 12 [18176/49669]\tLoss: 384.6446\n",
      "Training Epoch: 12 [18240/49669]\tLoss: 426.5004\n",
      "Training Epoch: 12 [18304/49669]\tLoss: 406.7630\n",
      "Training Epoch: 12 [18368/49669]\tLoss: 390.8261\n",
      "Training Epoch: 12 [18432/49669]\tLoss: 394.8593\n",
      "Training Epoch: 12 [18496/49669]\tLoss: 423.3716\n",
      "Training Epoch: 12 [18560/49669]\tLoss: 441.8993\n",
      "Training Epoch: 12 [18624/49669]\tLoss: 399.4294\n",
      "Training Epoch: 12 [18688/49669]\tLoss: 402.5168\n",
      "Training Epoch: 12 [18752/49669]\tLoss: 379.5238\n",
      "Training Epoch: 12 [18816/49669]\tLoss: 434.1194\n",
      "Training Epoch: 12 [18880/49669]\tLoss: 368.8877\n",
      "Training Epoch: 12 [18944/49669]\tLoss: 379.5390\n",
      "Training Epoch: 12 [19008/49669]\tLoss: 435.8377\n",
      "Training Epoch: 12 [19072/49669]\tLoss: 424.1553\n",
      "Training Epoch: 12 [19136/49669]\tLoss: 381.7690\n",
      "Training Epoch: 12 [19200/49669]\tLoss: 428.5038\n",
      "Training Epoch: 12 [19264/49669]\tLoss: 438.3944\n",
      "Training Epoch: 12 [19328/49669]\tLoss: 431.8648\n",
      "Training Epoch: 12 [19392/49669]\tLoss: 401.8041\n",
      "Training Epoch: 12 [19456/49669]\tLoss: 454.4200\n",
      "Training Epoch: 12 [19520/49669]\tLoss: 441.2380\n",
      "Training Epoch: 12 [19584/49669]\tLoss: 423.9002\n",
      "Training Epoch: 12 [19648/49669]\tLoss: 413.5819\n",
      "Training Epoch: 12 [19712/49669]\tLoss: 399.5225\n",
      "Training Epoch: 12 [19776/49669]\tLoss: 423.1791\n",
      "Training Epoch: 12 [19840/49669]\tLoss: 419.6273\n",
      "Training Epoch: 12 [19904/49669]\tLoss: 403.8238\n",
      "Training Epoch: 12 [19968/49669]\tLoss: 405.8047\n",
      "Training Epoch: 12 [20032/49669]\tLoss: 420.4417\n",
      "Training Epoch: 12 [20096/49669]\tLoss: 429.9578\n",
      "Training Epoch: 12 [20160/49669]\tLoss: 407.9342\n",
      "Training Epoch: 12 [20224/49669]\tLoss: 407.9744\n",
      "Training Epoch: 12 [20288/49669]\tLoss: 409.3155\n",
      "Training Epoch: 12 [20352/49669]\tLoss: 414.5233\n",
      "Training Epoch: 12 [20416/49669]\tLoss: 397.3823\n",
      "Training Epoch: 12 [20480/49669]\tLoss: 417.0447\n",
      "Training Epoch: 12 [20544/49669]\tLoss: 421.5183\n",
      "Training Epoch: 12 [20608/49669]\tLoss: 408.2160\n",
      "Training Epoch: 12 [20672/49669]\tLoss: 408.7849\n",
      "Training Epoch: 12 [20736/49669]\tLoss: 406.7439\n",
      "Training Epoch: 12 [20800/49669]\tLoss: 426.8833\n",
      "Training Epoch: 12 [20864/49669]\tLoss: 415.2466\n",
      "Training Epoch: 12 [20928/49669]\tLoss: 403.9708\n",
      "Training Epoch: 12 [20992/49669]\tLoss: 429.1526\n",
      "Training Epoch: 12 [21056/49669]\tLoss: 439.1613\n",
      "Training Epoch: 12 [21120/49669]\tLoss: 396.0934\n",
      "Training Epoch: 12 [21184/49669]\tLoss: 443.3776\n",
      "Training Epoch: 12 [21248/49669]\tLoss: 403.5515\n",
      "Training Epoch: 12 [21312/49669]\tLoss: 439.4536\n",
      "Training Epoch: 12 [21376/49669]\tLoss: 405.4380\n",
      "Training Epoch: 12 [21440/49669]\tLoss: 423.4279\n",
      "Training Epoch: 12 [21504/49669]\tLoss: 408.8641\n",
      "Training Epoch: 12 [21568/49669]\tLoss: 413.4467\n",
      "Training Epoch: 12 [21632/49669]\tLoss: 410.4869\n",
      "Training Epoch: 12 [21696/49669]\tLoss: 409.1791\n",
      "Training Epoch: 12 [21760/49669]\tLoss: 405.1585\n",
      "Training Epoch: 12 [21824/49669]\tLoss: 405.9971\n",
      "Training Epoch: 12 [21888/49669]\tLoss: 441.6362\n",
      "Training Epoch: 12 [21952/49669]\tLoss: 417.6554\n",
      "Training Epoch: 12 [22016/49669]\tLoss: 399.5051\n",
      "Training Epoch: 12 [22080/49669]\tLoss: 393.2906\n",
      "Training Epoch: 12 [22144/49669]\tLoss: 397.3737\n",
      "Training Epoch: 12 [22208/49669]\tLoss: 388.7314\n",
      "Training Epoch: 12 [22272/49669]\tLoss: 404.8135\n",
      "Training Epoch: 12 [22336/49669]\tLoss: 407.3191\n",
      "Training Epoch: 12 [22400/49669]\tLoss: 425.5490\n",
      "Training Epoch: 12 [22464/49669]\tLoss: 363.9030\n",
      "Training Epoch: 12 [22528/49669]\tLoss: 426.3065\n",
      "Training Epoch: 12 [22592/49669]\tLoss: 426.4189\n",
      "Training Epoch: 12 [22656/49669]\tLoss: 393.6273\n",
      "Training Epoch: 12 [22720/49669]\tLoss: 411.7986\n",
      "Training Epoch: 12 [22784/49669]\tLoss: 439.2394\n",
      "Training Epoch: 12 [22848/49669]\tLoss: 391.8727\n",
      "Training Epoch: 12 [22912/49669]\tLoss: 407.6112\n",
      "Training Epoch: 12 [22976/49669]\tLoss: 412.5554\n",
      "Training Epoch: 12 [23040/49669]\tLoss: 402.7351\n",
      "Training Epoch: 12 [23104/49669]\tLoss: 418.9637\n",
      "Training Epoch: 12 [23168/49669]\tLoss: 429.8109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [23232/49669]\tLoss: 408.7173\n",
      "Training Epoch: 12 [23296/49669]\tLoss: 405.9153\n",
      "Training Epoch: 12 [23360/49669]\tLoss: 429.8590\n",
      "Training Epoch: 12 [23424/49669]\tLoss: 406.6118\n",
      "Training Epoch: 12 [23488/49669]\tLoss: 386.5896\n",
      "Training Epoch: 12 [23552/49669]\tLoss: 410.5623\n",
      "Training Epoch: 12 [23616/49669]\tLoss: 396.5124\n",
      "Training Epoch: 12 [23680/49669]\tLoss: 411.8153\n",
      "Training Epoch: 12 [23744/49669]\tLoss: 407.1657\n",
      "Training Epoch: 12 [23808/49669]\tLoss: 430.8504\n",
      "Training Epoch: 12 [23872/49669]\tLoss: 416.4788\n",
      "Training Epoch: 12 [23936/49669]\tLoss: 418.6759\n",
      "Training Epoch: 12 [24000/49669]\tLoss: 422.4415\n",
      "Training Epoch: 12 [24064/49669]\tLoss: 431.6599\n",
      "Training Epoch: 12 [24128/49669]\tLoss: 414.3882\n",
      "Training Epoch: 12 [24192/49669]\tLoss: 408.0258\n",
      "Training Epoch: 12 [24256/49669]\tLoss: 411.0230\n",
      "Training Epoch: 12 [24320/49669]\tLoss: 418.5985\n",
      "Training Epoch: 12 [24384/49669]\tLoss: 409.0263\n",
      "Training Epoch: 12 [24448/49669]\tLoss: 440.3599\n",
      "Training Epoch: 12 [24512/49669]\tLoss: 414.7762\n",
      "Training Epoch: 12 [24576/49669]\tLoss: 406.1119\n",
      "Training Epoch: 12 [24640/49669]\tLoss: 424.9767\n",
      "Training Epoch: 12 [24704/49669]\tLoss: 409.1563\n",
      "Training Epoch: 12 [24768/49669]\tLoss: 386.9156\n",
      "Training Epoch: 12 [24832/49669]\tLoss: 419.2164\n",
      "Training Epoch: 12 [24896/49669]\tLoss: 428.3105\n",
      "Training Epoch: 12 [24960/49669]\tLoss: 384.1052\n",
      "Training Epoch: 12 [25024/49669]\tLoss: 426.3806\n",
      "Training Epoch: 12 [25088/49669]\tLoss: 451.6241\n",
      "Training Epoch: 12 [25152/49669]\tLoss: 398.6173\n",
      "Training Epoch: 12 [25216/49669]\tLoss: 400.3140\n",
      "Training Epoch: 12 [25280/49669]\tLoss: 425.7929\n",
      "Training Epoch: 12 [25344/49669]\tLoss: 414.1785\n",
      "Training Epoch: 12 [25408/49669]\tLoss: 395.4124\n",
      "Training Epoch: 12 [25472/49669]\tLoss: 390.3039\n",
      "Training Epoch: 12 [25536/49669]\tLoss: 391.2207\n",
      "Training Epoch: 12 [25600/49669]\tLoss: 425.0942\n",
      "Training Epoch: 12 [25664/49669]\tLoss: 413.8305\n",
      "Training Epoch: 12 [25728/49669]\tLoss: 402.8436\n",
      "Training Epoch: 12 [25792/49669]\tLoss: 427.7390\n",
      "Training Epoch: 12 [25856/49669]\tLoss: 428.4231\n",
      "Training Epoch: 12 [25920/49669]\tLoss: 376.4585\n",
      "Training Epoch: 12 [25984/49669]\tLoss: 424.5829\n",
      "Training Epoch: 12 [26048/49669]\tLoss: 409.4266\n",
      "Training Epoch: 12 [26112/49669]\tLoss: 445.9018\n",
      "Training Epoch: 12 [26176/49669]\tLoss: 411.7993\n",
      "Training Epoch: 12 [26240/49669]\tLoss: 405.5971\n",
      "Training Epoch: 12 [26304/49669]\tLoss: 432.3246\n",
      "Training Epoch: 12 [26368/49669]\tLoss: 459.5833\n",
      "Training Epoch: 12 [26432/49669]\tLoss: 419.5394\n",
      "Training Epoch: 12 [26496/49669]\tLoss: 397.1830\n",
      "Training Epoch: 12 [26560/49669]\tLoss: 422.5284\n",
      "Training Epoch: 12 [26624/49669]\tLoss: 425.8206\n",
      "Training Epoch: 12 [26688/49669]\tLoss: 432.4706\n",
      "Training Epoch: 12 [26752/49669]\tLoss: 441.2230\n",
      "Training Epoch: 12 [26816/49669]\tLoss: 431.2151\n",
      "Training Epoch: 12 [26880/49669]\tLoss: 439.8693\n",
      "Training Epoch: 12 [26944/49669]\tLoss: 489.9298\n",
      "Training Epoch: 12 [27008/49669]\tLoss: 467.8396\n",
      "Training Epoch: 12 [27072/49669]\tLoss: 472.1810\n",
      "Training Epoch: 12 [27136/49669]\tLoss: 481.3335\n",
      "Training Epoch: 12 [27200/49669]\tLoss: 429.9941\n",
      "Training Epoch: 12 [27264/49669]\tLoss: 414.3183\n",
      "Training Epoch: 12 [27328/49669]\tLoss: 414.6252\n",
      "Training Epoch: 12 [27392/49669]\tLoss: 403.3491\n",
      "Training Epoch: 12 [27456/49669]\tLoss: 417.3329\n",
      "Training Epoch: 12 [27520/49669]\tLoss: 449.9113\n",
      "Training Epoch: 12 [27584/49669]\tLoss: 444.2739\n",
      "Training Epoch: 12 [27648/49669]\tLoss: 465.5473\n",
      "Training Epoch: 12 [27712/49669]\tLoss: 413.3031\n",
      "Training Epoch: 12 [27776/49669]\tLoss: 418.8846\n",
      "Training Epoch: 12 [27840/49669]\tLoss: 367.8440\n",
      "Training Epoch: 12 [27904/49669]\tLoss: 406.0867\n",
      "Training Epoch: 12 [27968/49669]\tLoss: 429.7032\n",
      "Training Epoch: 12 [28032/49669]\tLoss: 440.8867\n",
      "Training Epoch: 12 [28096/49669]\tLoss: 447.1381\n",
      "Training Epoch: 12 [28160/49669]\tLoss: 395.6253\n",
      "Training Epoch: 12 [28224/49669]\tLoss: 409.0552\n",
      "Training Epoch: 12 [28288/49669]\tLoss: 429.9880\n",
      "Training Epoch: 12 [28352/49669]\tLoss: 435.7945\n",
      "Training Epoch: 12 [28416/49669]\tLoss: 416.5604\n",
      "Training Epoch: 12 [28480/49669]\tLoss: 389.5201\n",
      "Training Epoch: 12 [28544/49669]\tLoss: 415.6167\n",
      "Training Epoch: 12 [28608/49669]\tLoss: 390.1798\n",
      "Training Epoch: 12 [28672/49669]\tLoss: 402.8440\n",
      "Training Epoch: 12 [28736/49669]\tLoss: 403.4745\n",
      "Training Epoch: 12 [28800/49669]\tLoss: 450.3301\n",
      "Training Epoch: 12 [28864/49669]\tLoss: 436.9387\n",
      "Training Epoch: 12 [28928/49669]\tLoss: 402.6274\n",
      "Training Epoch: 12 [28992/49669]\tLoss: 414.5788\n",
      "Training Epoch: 12 [29056/49669]\tLoss: 429.0738\n",
      "Training Epoch: 12 [29120/49669]\tLoss: 410.1415\n",
      "Training Epoch: 12 [29184/49669]\tLoss: 406.1282\n",
      "Training Epoch: 12 [29248/49669]\tLoss: 429.3438\n",
      "Training Epoch: 12 [29312/49669]\tLoss: 417.9019\n",
      "Training Epoch: 12 [29376/49669]\tLoss: 405.4699\n",
      "Training Epoch: 12 [29440/49669]\tLoss: 411.5146\n",
      "Training Epoch: 12 [29504/49669]\tLoss: 441.4309\n",
      "Training Epoch: 12 [29568/49669]\tLoss: 432.3309\n",
      "Training Epoch: 12 [29632/49669]\tLoss: 415.6444\n",
      "Training Epoch: 12 [29696/49669]\tLoss: 406.1620\n",
      "Training Epoch: 12 [29760/49669]\tLoss: 407.4175\n",
      "Training Epoch: 12 [29824/49669]\tLoss: 419.1899\n",
      "Training Epoch: 12 [29888/49669]\tLoss: 452.4822\n",
      "Training Epoch: 12 [29952/49669]\tLoss: 426.6075\n",
      "Training Epoch: 12 [30016/49669]\tLoss: 425.6908\n",
      "Training Epoch: 12 [30080/49669]\tLoss: 379.1063\n",
      "Training Epoch: 12 [30144/49669]\tLoss: 420.5126\n",
      "Training Epoch: 12 [30208/49669]\tLoss: 392.3433\n",
      "Training Epoch: 12 [30272/49669]\tLoss: 392.0572\n",
      "Training Epoch: 12 [30336/49669]\tLoss: 423.1887\n",
      "Training Epoch: 12 [30400/49669]\tLoss: 402.2534\n",
      "Training Epoch: 12 [30464/49669]\tLoss: 413.6410\n",
      "Training Epoch: 12 [30528/49669]\tLoss: 398.2768\n",
      "Training Epoch: 12 [30592/49669]\tLoss: 423.3800\n",
      "Training Epoch: 12 [30656/49669]\tLoss: 417.1543\n",
      "Training Epoch: 12 [30720/49669]\tLoss: 422.6447\n",
      "Training Epoch: 12 [30784/49669]\tLoss: 431.2821\n",
      "Training Epoch: 12 [30848/49669]\tLoss: 408.3541\n",
      "Training Epoch: 12 [30912/49669]\tLoss: 416.8506\n",
      "Training Epoch: 12 [30976/49669]\tLoss: 406.4902\n",
      "Training Epoch: 12 [31040/49669]\tLoss: 378.3292\n",
      "Training Epoch: 12 [31104/49669]\tLoss: 414.8343\n",
      "Training Epoch: 12 [31168/49669]\tLoss: 378.2369\n",
      "Training Epoch: 12 [31232/49669]\tLoss: 422.2900\n",
      "Training Epoch: 12 [31296/49669]\tLoss: 406.5709\n",
      "Training Epoch: 12 [31360/49669]\tLoss: 402.3213\n",
      "Training Epoch: 12 [31424/49669]\tLoss: 448.0736\n",
      "Training Epoch: 12 [31488/49669]\tLoss: 419.8717\n",
      "Training Epoch: 12 [31552/49669]\tLoss: 429.5762\n",
      "Training Epoch: 12 [31616/49669]\tLoss: 408.1498\n",
      "Training Epoch: 12 [31680/49669]\tLoss: 391.5908\n",
      "Training Epoch: 12 [31744/49669]\tLoss: 392.9083\n",
      "Training Epoch: 12 [31808/49669]\tLoss: 413.2267\n",
      "Training Epoch: 12 [31872/49669]\tLoss: 429.2477\n",
      "Training Epoch: 12 [31936/49669]\tLoss: 427.6959\n",
      "Training Epoch: 12 [32000/49669]\tLoss: 426.2047\n",
      "Training Epoch: 12 [32064/49669]\tLoss: 426.9209\n",
      "Training Epoch: 12 [32128/49669]\tLoss: 412.5679\n",
      "Training Epoch: 12 [32192/49669]\tLoss: 433.3455\n",
      "Training Epoch: 12 [32256/49669]\tLoss: 394.7147\n",
      "Training Epoch: 12 [32320/49669]\tLoss: 418.7906\n",
      "Training Epoch: 12 [32384/49669]\tLoss: 380.8007\n",
      "Training Epoch: 12 [32448/49669]\tLoss: 445.8152\n",
      "Training Epoch: 12 [32512/49669]\tLoss: 448.3660\n",
      "Training Epoch: 12 [32576/49669]\tLoss: 427.9344\n",
      "Training Epoch: 12 [32640/49669]\tLoss: 399.8625\n",
      "Training Epoch: 12 [32704/49669]\tLoss: 412.8038\n",
      "Training Epoch: 12 [32768/49669]\tLoss: 437.1245\n",
      "Training Epoch: 12 [32832/49669]\tLoss: 395.3282\n",
      "Training Epoch: 12 [32896/49669]\tLoss: 422.7122\n",
      "Training Epoch: 12 [32960/49669]\tLoss: 401.7274\n",
      "Training Epoch: 12 [33024/49669]\tLoss: 418.6951\n",
      "Training Epoch: 12 [33088/49669]\tLoss: 413.6078\n",
      "Training Epoch: 12 [33152/49669]\tLoss: 416.6755\n",
      "Training Epoch: 12 [33216/49669]\tLoss: 428.2789\n",
      "Training Epoch: 12 [33280/49669]\tLoss: 412.2728\n",
      "Training Epoch: 12 [33344/49669]\tLoss: 411.2861\n",
      "Training Epoch: 12 [33408/49669]\tLoss: 395.8481\n",
      "Training Epoch: 12 [33472/49669]\tLoss: 433.8865\n",
      "Training Epoch: 12 [33536/49669]\tLoss: 418.9855\n",
      "Training Epoch: 12 [33600/49669]\tLoss: 398.7632\n",
      "Training Epoch: 12 [33664/49669]\tLoss: 422.0751\n",
      "Training Epoch: 12 [33728/49669]\tLoss: 390.5116\n",
      "Training Epoch: 12 [33792/49669]\tLoss: 412.4801\n",
      "Training Epoch: 12 [33856/49669]\tLoss: 365.1706\n",
      "Training Epoch: 12 [33920/49669]\tLoss: 415.7365\n",
      "Training Epoch: 12 [33984/49669]\tLoss: 395.1687\n",
      "Training Epoch: 12 [34048/49669]\tLoss: 403.9580\n",
      "Training Epoch: 12 [34112/49669]\tLoss: 403.4012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [34176/49669]\tLoss: 408.8374\n",
      "Training Epoch: 12 [34240/49669]\tLoss: 449.5520\n",
      "Training Epoch: 12 [34304/49669]\tLoss: 389.3480\n",
      "Training Epoch: 12 [34368/49669]\tLoss: 399.6547\n",
      "Training Epoch: 12 [34432/49669]\tLoss: 396.9135\n",
      "Training Epoch: 12 [34496/49669]\tLoss: 417.1537\n",
      "Training Epoch: 12 [34560/49669]\tLoss: 413.2708\n",
      "Training Epoch: 12 [34624/49669]\tLoss: 418.0885\n",
      "Training Epoch: 12 [34688/49669]\tLoss: 411.9372\n",
      "Training Epoch: 12 [34752/49669]\tLoss: 416.7516\n",
      "Training Epoch: 12 [34816/49669]\tLoss: 420.1038\n",
      "Training Epoch: 12 [34880/49669]\tLoss: 397.1029\n",
      "Training Epoch: 12 [34944/49669]\tLoss: 395.0652\n",
      "Training Epoch: 12 [35008/49669]\tLoss: 398.1211\n",
      "Training Epoch: 12 [35072/49669]\tLoss: 375.4466\n",
      "Training Epoch: 12 [35136/49669]\tLoss: 431.7122\n",
      "Training Epoch: 12 [35200/49669]\tLoss: 405.5051\n",
      "Training Epoch: 12 [35264/49669]\tLoss: 418.6114\n",
      "Training Epoch: 12 [35328/49669]\tLoss: 403.9313\n",
      "Training Epoch: 12 [35392/49669]\tLoss: 394.9334\n",
      "Training Epoch: 12 [35456/49669]\tLoss: 419.0382\n",
      "Training Epoch: 12 [35520/49669]\tLoss: 414.6888\n",
      "Training Epoch: 12 [35584/49669]\tLoss: 390.3167\n",
      "Training Epoch: 12 [35648/49669]\tLoss: 410.5256\n",
      "Training Epoch: 12 [35712/49669]\tLoss: 429.5925\n",
      "Training Epoch: 12 [35776/49669]\tLoss: 419.4839\n",
      "Training Epoch: 12 [35840/49669]\tLoss: 383.8938\n",
      "Training Epoch: 12 [35904/49669]\tLoss: 410.2741\n",
      "Training Epoch: 12 [35968/49669]\tLoss: 420.6353\n",
      "Training Epoch: 12 [36032/49669]\tLoss: 430.2299\n",
      "Training Epoch: 12 [36096/49669]\tLoss: 421.3644\n",
      "Training Epoch: 12 [36160/49669]\tLoss: 418.0825\n",
      "Training Epoch: 12 [36224/49669]\tLoss: 416.3736\n",
      "Training Epoch: 12 [36288/49669]\tLoss: 400.6471\n",
      "Training Epoch: 12 [36352/49669]\tLoss: 394.3708\n",
      "Training Epoch: 12 [36416/49669]\tLoss: 412.8521\n",
      "Training Epoch: 12 [36480/49669]\tLoss: 386.0373\n",
      "Training Epoch: 12 [36544/49669]\tLoss: 412.6940\n",
      "Training Epoch: 12 [36608/49669]\tLoss: 391.6226\n",
      "Training Epoch: 12 [36672/49669]\tLoss: 425.3884\n",
      "Training Epoch: 12 [36736/49669]\tLoss: 430.9844\n",
      "Training Epoch: 12 [36800/49669]\tLoss: 419.4059\n",
      "Training Epoch: 12 [36864/49669]\tLoss: 417.7404\n",
      "Training Epoch: 12 [36928/49669]\tLoss: 407.6895\n",
      "Training Epoch: 12 [36992/49669]\tLoss: 370.4116\n",
      "Training Epoch: 12 [37056/49669]\tLoss: 399.6822\n",
      "Training Epoch: 12 [37120/49669]\tLoss: 403.1118\n",
      "Training Epoch: 12 [37184/49669]\tLoss: 426.8926\n",
      "Training Epoch: 12 [37248/49669]\tLoss: 389.9572\n",
      "Training Epoch: 12 [37312/49669]\tLoss: 408.9037\n",
      "Training Epoch: 12 [37376/49669]\tLoss: 428.7083\n",
      "Training Epoch: 12 [37440/49669]\tLoss: 396.0687\n",
      "Training Epoch: 12 [37504/49669]\tLoss: 403.4161\n",
      "Training Epoch: 12 [37568/49669]\tLoss: 395.9231\n",
      "Training Epoch: 12 [37632/49669]\tLoss: 419.2296\n",
      "Training Epoch: 12 [37696/49669]\tLoss: 429.3452\n",
      "Training Epoch: 12 [37760/49669]\tLoss: 379.1655\n",
      "Training Epoch: 12 [37824/49669]\tLoss: 390.5716\n",
      "Training Epoch: 12 [37888/49669]\tLoss: 380.9074\n",
      "Training Epoch: 12 [37952/49669]\tLoss: 432.3033\n",
      "Training Epoch: 12 [38016/49669]\tLoss: 406.3140\n",
      "Training Epoch: 12 [38080/49669]\tLoss: 428.6204\n",
      "Training Epoch: 12 [38144/49669]\tLoss: 384.1238\n",
      "Training Epoch: 12 [38208/49669]\tLoss: 401.6817\n",
      "Training Epoch: 12 [38272/49669]\tLoss: 410.8301\n",
      "Training Epoch: 12 [38336/49669]\tLoss: 399.6091\n",
      "Training Epoch: 12 [38400/49669]\tLoss: 425.7739\n",
      "Training Epoch: 12 [38464/49669]\tLoss: 418.0593\n",
      "Training Epoch: 12 [38528/49669]\tLoss: 404.0002\n",
      "Training Epoch: 12 [38592/49669]\tLoss: 389.0509\n",
      "Training Epoch: 12 [38656/49669]\tLoss: 406.3834\n",
      "Training Epoch: 12 [38720/49669]\tLoss: 415.7000\n",
      "Training Epoch: 12 [38784/49669]\tLoss: 423.6086\n",
      "Training Epoch: 12 [38848/49669]\tLoss: 404.5905\n",
      "Training Epoch: 12 [38912/49669]\tLoss: 401.4438\n",
      "Training Epoch: 12 [38976/49669]\tLoss: 412.7891\n",
      "Training Epoch: 12 [39040/49669]\tLoss: 388.3324\n",
      "Training Epoch: 12 [39104/49669]\tLoss: 407.5244\n",
      "Training Epoch: 12 [39168/49669]\tLoss: 425.3213\n",
      "Training Epoch: 12 [39232/49669]\tLoss: 393.0808\n",
      "Training Epoch: 12 [39296/49669]\tLoss: 415.9477\n",
      "Training Epoch: 12 [39360/49669]\tLoss: 384.7067\n",
      "Training Epoch: 12 [39424/49669]\tLoss: 407.3156\n",
      "Training Epoch: 12 [39488/49669]\tLoss: 413.2534\n",
      "Training Epoch: 12 [39552/49669]\tLoss: 390.9350\n",
      "Training Epoch: 12 [39616/49669]\tLoss: 446.5718\n",
      "Training Epoch: 12 [39680/49669]\tLoss: 407.4254\n",
      "Training Epoch: 12 [39744/49669]\tLoss: 432.1836\n",
      "Training Epoch: 12 [39808/49669]\tLoss: 434.6758\n",
      "Training Epoch: 12 [39872/49669]\tLoss: 417.8306\n",
      "Training Epoch: 12 [39936/49669]\tLoss: 429.2437\n",
      "Training Epoch: 12 [40000/49669]\tLoss: 460.3142\n",
      "Training Epoch: 12 [40064/49669]\tLoss: 433.9687\n",
      "Training Epoch: 12 [40128/49669]\tLoss: 473.3378\n",
      "Training Epoch: 12 [40192/49669]\tLoss: 469.3243\n",
      "Training Epoch: 12 [40256/49669]\tLoss: 475.6341\n",
      "Training Epoch: 12 [40320/49669]\tLoss: 462.8679\n",
      "Training Epoch: 12 [40384/49669]\tLoss: 443.9357\n",
      "Training Epoch: 12 [40448/49669]\tLoss: 432.2038\n",
      "Training Epoch: 12 [40512/49669]\tLoss: 409.0010\n",
      "Training Epoch: 12 [40576/49669]\tLoss: 428.0892\n",
      "Training Epoch: 12 [40640/49669]\tLoss: 410.3593\n",
      "Training Epoch: 12 [40704/49669]\tLoss: 420.8826\n",
      "Training Epoch: 12 [40768/49669]\tLoss: 418.0745\n",
      "Training Epoch: 12 [40832/49669]\tLoss: 391.1274\n",
      "Training Epoch: 12 [40896/49669]\tLoss: 424.5099\n",
      "Training Epoch: 12 [40960/49669]\tLoss: 420.9774\n",
      "Training Epoch: 12 [41024/49669]\tLoss: 373.3387\n",
      "Training Epoch: 12 [41088/49669]\tLoss: 399.0552\n",
      "Training Epoch: 12 [41152/49669]\tLoss: 416.3051\n",
      "Training Epoch: 12 [41216/49669]\tLoss: 382.0876\n",
      "Training Epoch: 12 [41280/49669]\tLoss: 420.4612\n",
      "Training Epoch: 12 [41344/49669]\tLoss: 409.4630\n",
      "Training Epoch: 12 [41408/49669]\tLoss: 402.8313\n",
      "Training Epoch: 12 [41472/49669]\tLoss: 420.7188\n",
      "Training Epoch: 12 [41536/49669]\tLoss: 415.0230\n",
      "Training Epoch: 12 [41600/49669]\tLoss: 394.4875\n",
      "Training Epoch: 12 [41664/49669]\tLoss: 388.1042\n",
      "Training Epoch: 12 [41728/49669]\tLoss: 416.7398\n",
      "Training Epoch: 12 [41792/49669]\tLoss: 386.0082\n",
      "Training Epoch: 12 [41856/49669]\tLoss: 384.6600\n",
      "Training Epoch: 12 [41920/49669]\tLoss: 412.1822\n",
      "Training Epoch: 12 [41984/49669]\tLoss: 429.0660\n",
      "Training Epoch: 12 [42048/49669]\tLoss: 416.0817\n",
      "Training Epoch: 12 [42112/49669]\tLoss: 390.0810\n",
      "Training Epoch: 12 [42176/49669]\tLoss: 400.9709\n",
      "Training Epoch: 12 [42240/49669]\tLoss: 396.2747\n",
      "Training Epoch: 12 [42304/49669]\tLoss: 396.9861\n",
      "Training Epoch: 12 [42368/49669]\tLoss: 407.1205\n",
      "Training Epoch: 12 [42432/49669]\tLoss: 419.5608\n",
      "Training Epoch: 12 [42496/49669]\tLoss: 394.7215\n",
      "Training Epoch: 12 [42560/49669]\tLoss: 403.7270\n",
      "Training Epoch: 12 [42624/49669]\tLoss: 384.5375\n",
      "Training Epoch: 12 [42688/49669]\tLoss: 412.1210\n",
      "Training Epoch: 12 [42752/49669]\tLoss: 422.6608\n",
      "Training Epoch: 12 [42816/49669]\tLoss: 415.1723\n",
      "Training Epoch: 12 [42880/49669]\tLoss: 413.9131\n",
      "Training Epoch: 12 [42944/49669]\tLoss: 411.6177\n",
      "Training Epoch: 12 [43008/49669]\tLoss: 412.3565\n",
      "Training Epoch: 12 [43072/49669]\tLoss: 391.1002\n",
      "Training Epoch: 12 [43136/49669]\tLoss: 372.6965\n",
      "Training Epoch: 12 [43200/49669]\tLoss: 432.0243\n",
      "Training Epoch: 12 [43264/49669]\tLoss: 425.3735\n",
      "Training Epoch: 12 [43328/49669]\tLoss: 351.3409\n",
      "Training Epoch: 12 [43392/49669]\tLoss: 431.1118\n",
      "Training Epoch: 12 [43456/49669]\tLoss: 404.1408\n",
      "Training Epoch: 12 [43520/49669]\tLoss: 397.4829\n",
      "Training Epoch: 12 [43584/49669]\tLoss: 426.6787\n",
      "Training Epoch: 12 [43648/49669]\tLoss: 394.2480\n",
      "Training Epoch: 12 [43712/49669]\tLoss: 417.9088\n",
      "Training Epoch: 12 [43776/49669]\tLoss: 421.5013\n",
      "Training Epoch: 12 [43840/49669]\tLoss: 394.2928\n",
      "Training Epoch: 12 [43904/49669]\tLoss: 412.8029\n",
      "Training Epoch: 12 [43968/49669]\tLoss: 398.6716\n",
      "Training Epoch: 12 [44032/49669]\tLoss: 445.4151\n",
      "Training Epoch: 12 [44096/49669]\tLoss: 427.2200\n",
      "Training Epoch: 12 [44160/49669]\tLoss: 432.3769\n",
      "Training Epoch: 12 [44224/49669]\tLoss: 427.2507\n",
      "Training Epoch: 12 [44288/49669]\tLoss: 427.0789\n",
      "Training Epoch: 12 [44352/49669]\tLoss: 433.5577\n",
      "Training Epoch: 12 [44416/49669]\tLoss: 398.0608\n",
      "Training Epoch: 12 [44480/49669]\tLoss: 423.5648\n",
      "Training Epoch: 12 [44544/49669]\tLoss: 410.4758\n",
      "Training Epoch: 12 [44608/49669]\tLoss: 405.5305\n",
      "Training Epoch: 12 [44672/49669]\tLoss: 413.3727\n",
      "Training Epoch: 12 [44736/49669]\tLoss: 424.8013\n",
      "Training Epoch: 12 [44800/49669]\tLoss: 432.6899\n",
      "Training Epoch: 12 [44864/49669]\tLoss: 454.1643\n",
      "Training Epoch: 12 [44928/49669]\tLoss: 454.2394\n",
      "Training Epoch: 12 [44992/49669]\tLoss: 420.8585\n",
      "Training Epoch: 12 [45056/49669]\tLoss: 399.4601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [45120/49669]\tLoss: 411.7374\n",
      "Training Epoch: 12 [45184/49669]\tLoss: 420.5133\n",
      "Training Epoch: 12 [45248/49669]\tLoss: 406.4385\n",
      "Training Epoch: 12 [45312/49669]\tLoss: 407.2967\n",
      "Training Epoch: 12 [45376/49669]\tLoss: 398.1985\n",
      "Training Epoch: 12 [45440/49669]\tLoss: 435.1757\n",
      "Training Epoch: 12 [45504/49669]\tLoss: 459.7968\n",
      "Training Epoch: 12 [45568/49669]\tLoss: 410.9857\n",
      "Training Epoch: 12 [45632/49669]\tLoss: 423.0753\n",
      "Training Epoch: 12 [45696/49669]\tLoss: 394.3798\n",
      "Training Epoch: 12 [45760/49669]\tLoss: 416.5543\n",
      "Training Epoch: 12 [45824/49669]\tLoss: 412.5453\n",
      "Training Epoch: 12 [45888/49669]\tLoss: 378.0270\n",
      "Training Epoch: 12 [45952/49669]\tLoss: 410.6799\n",
      "Training Epoch: 12 [46016/49669]\tLoss: 410.8779\n",
      "Training Epoch: 12 [46080/49669]\tLoss: 413.0672\n",
      "Training Epoch: 12 [46144/49669]\tLoss: 414.6069\n",
      "Training Epoch: 12 [46208/49669]\tLoss: 423.6914\n",
      "Training Epoch: 12 [46272/49669]\tLoss: 387.4329\n",
      "Training Epoch: 12 [46336/49669]\tLoss: 400.0741\n",
      "Training Epoch: 12 [46400/49669]\tLoss: 403.3622\n",
      "Training Epoch: 12 [46464/49669]\tLoss: 427.3683\n",
      "Training Epoch: 12 [46528/49669]\tLoss: 424.1821\n",
      "Training Epoch: 12 [46592/49669]\tLoss: 402.0175\n",
      "Training Epoch: 12 [46656/49669]\tLoss: 390.4713\n",
      "Training Epoch: 12 [46720/49669]\tLoss: 398.9955\n",
      "Training Epoch: 12 [46784/49669]\tLoss: 381.3723\n",
      "Training Epoch: 12 [46848/49669]\tLoss: 426.0168\n",
      "Training Epoch: 12 [46912/49669]\tLoss: 415.9963\n",
      "Training Epoch: 12 [46976/49669]\tLoss: 419.8309\n",
      "Training Epoch: 12 [47040/49669]\tLoss: 456.2556\n",
      "Training Epoch: 12 [47104/49669]\tLoss: 424.3979\n",
      "Training Epoch: 12 [47168/49669]\tLoss: 443.2033\n",
      "Training Epoch: 12 [47232/49669]\tLoss: 386.5714\n",
      "Training Epoch: 12 [47296/49669]\tLoss: 404.5963\n",
      "Training Epoch: 12 [47360/49669]\tLoss: 401.5432\n",
      "Training Epoch: 12 [47424/49669]\tLoss: 407.6595\n",
      "Training Epoch: 12 [47488/49669]\tLoss: 416.4430\n",
      "Training Epoch: 12 [47552/49669]\tLoss: 392.2537\n",
      "Training Epoch: 12 [47616/49669]\tLoss: 419.6959\n",
      "Training Epoch: 12 [47680/49669]\tLoss: 407.1501\n",
      "Training Epoch: 12 [47744/49669]\tLoss: 431.1281\n",
      "Training Epoch: 12 [47808/49669]\tLoss: 394.8149\n",
      "Training Epoch: 12 [47872/49669]\tLoss: 416.5717\n",
      "Training Epoch: 12 [47936/49669]\tLoss: 409.6117\n",
      "Training Epoch: 12 [48000/49669]\tLoss: 402.4578\n",
      "Training Epoch: 12 [48064/49669]\tLoss: 395.3500\n",
      "Training Epoch: 12 [48128/49669]\tLoss: 400.2931\n",
      "Training Epoch: 12 [48192/49669]\tLoss: 427.1848\n",
      "Training Epoch: 12 [48256/49669]\tLoss: 417.2456\n",
      "Training Epoch: 12 [48320/49669]\tLoss: 431.3576\n",
      "Training Epoch: 12 [48384/49669]\tLoss: 402.8660\n",
      "Training Epoch: 12 [48448/49669]\tLoss: 413.5532\n",
      "Training Epoch: 12 [48512/49669]\tLoss: 396.1360\n",
      "Training Epoch: 12 [48576/49669]\tLoss: 403.2547\n",
      "Training Epoch: 12 [48640/49669]\tLoss: 414.6887\n",
      "Training Epoch: 12 [48704/49669]\tLoss: 402.1795\n",
      "Training Epoch: 12 [48768/49669]\tLoss: 431.3096\n",
      "Training Epoch: 12 [48832/49669]\tLoss: 407.7988\n",
      "Training Epoch: 12 [48896/49669]\tLoss: 398.2051\n",
      "Training Epoch: 12 [48960/49669]\tLoss: 402.4094\n",
      "Training Epoch: 12 [49024/49669]\tLoss: 413.8398\n",
      "Training Epoch: 12 [49088/49669]\tLoss: 418.7128\n",
      "Training Epoch: 12 [49152/49669]\tLoss: 423.1193\n",
      "Training Epoch: 12 [49216/49669]\tLoss: 395.4960\n",
      "Training Epoch: 12 [49280/49669]\tLoss: 398.2524\n",
      "Training Epoch: 12 [49344/49669]\tLoss: 415.1014\n",
      "Training Epoch: 12 [49408/49669]\tLoss: 414.4632\n",
      "Training Epoch: 12 [49472/49669]\tLoss: 410.5504\n",
      "Training Epoch: 12 [49536/49669]\tLoss: 396.2436\n",
      "Training Epoch: 12 [49600/49669]\tLoss: 405.2647\n",
      "Training Epoch: 12 [49664/49669]\tLoss: 420.1062\n",
      "Training Epoch: 12 [49669/49669]\tLoss: 436.0470\n",
      "Training Epoch: 12 [5519/5519]\tLoss: 413.8535\n",
      "Training Epoch: 13 [64/49669]\tLoss: 413.5420\n",
      "Training Epoch: 13 [128/49669]\tLoss: 429.9167\n",
      "Training Epoch: 13 [192/49669]\tLoss: 412.3006\n",
      "Training Epoch: 13 [256/49669]\tLoss: 390.2830\n",
      "Training Epoch: 13 [320/49669]\tLoss: 403.5869\n",
      "Training Epoch: 13 [384/49669]\tLoss: 429.2819\n",
      "Training Epoch: 13 [448/49669]\tLoss: 381.1956\n",
      "Training Epoch: 13 [512/49669]\tLoss: 397.6134\n",
      "Training Epoch: 13 [576/49669]\tLoss: 440.9668\n",
      "Training Epoch: 13 [640/49669]\tLoss: 400.2352\n",
      "Training Epoch: 13 [704/49669]\tLoss: 408.7101\n",
      "Training Epoch: 13 [768/49669]\tLoss: 386.4663\n",
      "Training Epoch: 13 [832/49669]\tLoss: 399.6001\n",
      "Training Epoch: 13 [896/49669]\tLoss: 441.5557\n",
      "Training Epoch: 13 [960/49669]\tLoss: 425.0319\n",
      "Training Epoch: 13 [1024/49669]\tLoss: 420.3558\n",
      "Training Epoch: 13 [1088/49669]\tLoss: 407.4543\n",
      "Training Epoch: 13 [1152/49669]\tLoss: 439.0766\n",
      "Training Epoch: 13 [1216/49669]\tLoss: 451.8990\n",
      "Training Epoch: 13 [1280/49669]\tLoss: 422.1495\n",
      "Training Epoch: 13 [1344/49669]\tLoss: 436.0815\n",
      "Training Epoch: 13 [1408/49669]\tLoss: 439.2250\n",
      "Training Epoch: 13 [1472/49669]\tLoss: 420.9255\n",
      "Training Epoch: 13 [1536/49669]\tLoss: 462.5729\n",
      "Training Epoch: 13 [1600/49669]\tLoss: 455.5591\n",
      "Training Epoch: 13 [1664/49669]\tLoss: 448.8664\n",
      "Training Epoch: 13 [1728/49669]\tLoss: 426.4229\n",
      "Training Epoch: 13 [1792/49669]\tLoss: 453.5927\n",
      "Training Epoch: 13 [1856/49669]\tLoss: 444.7423\n",
      "Training Epoch: 13 [1920/49669]\tLoss: 411.9980\n",
      "Training Epoch: 13 [1984/49669]\tLoss: 422.0868\n",
      "Training Epoch: 13 [2048/49669]\tLoss: 416.0919\n",
      "Training Epoch: 13 [2112/49669]\tLoss: 449.7375\n",
      "Training Epoch: 13 [2176/49669]\tLoss: 451.8088\n",
      "Training Epoch: 13 [2240/49669]\tLoss: 470.0506\n",
      "Training Epoch: 13 [2304/49669]\tLoss: 457.0117\n",
      "Training Epoch: 13 [2368/49669]\tLoss: 468.7780\n",
      "Training Epoch: 13 [2432/49669]\tLoss: 442.3779\n",
      "Training Epoch: 13 [2496/49669]\tLoss: 395.7556\n",
      "Training Epoch: 13 [2560/49669]\tLoss: 436.4325\n",
      "Training Epoch: 13 [2624/49669]\tLoss: 421.5074\n",
      "Training Epoch: 13 [2688/49669]\tLoss: 430.1093\n",
      "Training Epoch: 13 [2752/49669]\tLoss: 399.0975\n",
      "Training Epoch: 13 [2816/49669]\tLoss: 391.0190\n",
      "Training Epoch: 13 [2880/49669]\tLoss: 423.4729\n",
      "Training Epoch: 13 [2944/49669]\tLoss: 409.8086\n",
      "Training Epoch: 13 [3008/49669]\tLoss: 427.3918\n",
      "Training Epoch: 13 [3072/49669]\tLoss: 427.0854\n",
      "Training Epoch: 13 [3136/49669]\tLoss: 438.6790\n",
      "Training Epoch: 13 [3200/49669]\tLoss: 440.9698\n",
      "Training Epoch: 13 [3264/49669]\tLoss: 434.3357\n",
      "Training Epoch: 13 [3328/49669]\tLoss: 439.8550\n",
      "Training Epoch: 13 [3392/49669]\tLoss: 423.1390\n",
      "Training Epoch: 13 [3456/49669]\tLoss: 417.7803\n",
      "Training Epoch: 13 [3520/49669]\tLoss: 423.1146\n",
      "Training Epoch: 13 [3584/49669]\tLoss: 399.4518\n",
      "Training Epoch: 13 [3648/49669]\tLoss: 401.2608\n",
      "Training Epoch: 13 [3712/49669]\tLoss: 409.2557\n",
      "Training Epoch: 13 [3776/49669]\tLoss: 411.4943\n",
      "Training Epoch: 13 [3840/49669]\tLoss: 406.6070\n",
      "Training Epoch: 13 [3904/49669]\tLoss: 445.6760\n",
      "Training Epoch: 13 [3968/49669]\tLoss: 434.6926\n",
      "Training Epoch: 13 [4032/49669]\tLoss: 415.7877\n",
      "Training Epoch: 13 [4096/49669]\tLoss: 391.2968\n",
      "Training Epoch: 13 [4160/49669]\tLoss: 413.6688\n",
      "Training Epoch: 13 [4224/49669]\tLoss: 436.4420\n",
      "Training Epoch: 13 [4288/49669]\tLoss: 424.6960\n",
      "Training Epoch: 13 [4352/49669]\tLoss: 410.7333\n",
      "Training Epoch: 13 [4416/49669]\tLoss: 443.5615\n",
      "Training Epoch: 13 [4480/49669]\tLoss: 399.5904\n",
      "Training Epoch: 13 [4544/49669]\tLoss: 417.7858\n",
      "Training Epoch: 13 [4608/49669]\tLoss: 403.2417\n",
      "Training Epoch: 13 [4672/49669]\tLoss: 428.1653\n",
      "Training Epoch: 13 [4736/49669]\tLoss: 408.9518\n",
      "Training Epoch: 13 [4800/49669]\tLoss: 412.4803\n",
      "Training Epoch: 13 [4864/49669]\tLoss: 411.5826\n",
      "Training Epoch: 13 [4928/49669]\tLoss: 437.9681\n",
      "Training Epoch: 13 [4992/49669]\tLoss: 412.3086\n",
      "Training Epoch: 13 [5056/49669]\tLoss: 404.1535\n",
      "Training Epoch: 13 [5120/49669]\tLoss: 396.0117\n",
      "Training Epoch: 13 [5184/49669]\tLoss: 426.6851\n",
      "Training Epoch: 13 [5248/49669]\tLoss: 407.0946\n",
      "Training Epoch: 13 [5312/49669]\tLoss: 391.7755\n",
      "Training Epoch: 13 [5376/49669]\tLoss: 433.7546\n",
      "Training Epoch: 13 [5440/49669]\tLoss: 402.9797\n",
      "Training Epoch: 13 [5504/49669]\tLoss: 403.2311\n",
      "Training Epoch: 13 [5568/49669]\tLoss: 409.7085\n",
      "Training Epoch: 13 [5632/49669]\tLoss: 401.6823\n",
      "Training Epoch: 13 [5696/49669]\tLoss: 415.4428\n",
      "Training Epoch: 13 [5760/49669]\tLoss: 387.5704\n",
      "Training Epoch: 13 [5824/49669]\tLoss: 428.0908\n",
      "Training Epoch: 13 [5888/49669]\tLoss: 430.7343\n",
      "Training Epoch: 13 [5952/49669]\tLoss: 415.2523\n",
      "Training Epoch: 13 [6016/49669]\tLoss: 405.8613\n",
      "Training Epoch: 13 [6080/49669]\tLoss: 414.4441\n",
      "Training Epoch: 13 [6144/49669]\tLoss: 433.3626\n",
      "Training Epoch: 13 [6208/49669]\tLoss: 455.6153\n",
      "Training Epoch: 13 [6272/49669]\tLoss: 410.9913\n",
      "Training Epoch: 13 [6336/49669]\tLoss: 410.8496\n",
      "Training Epoch: 13 [6400/49669]\tLoss: 411.1607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [6464/49669]\tLoss: 418.7151\n",
      "Training Epoch: 13 [6528/49669]\tLoss: 444.0168\n",
      "Training Epoch: 13 [6592/49669]\tLoss: 405.1142\n",
      "Training Epoch: 13 [6656/49669]\tLoss: 374.4440\n",
      "Training Epoch: 13 [6720/49669]\tLoss: 389.9003\n",
      "Training Epoch: 13 [6784/49669]\tLoss: 415.1721\n",
      "Training Epoch: 13 [6848/49669]\tLoss: 395.7892\n",
      "Training Epoch: 13 [6912/49669]\tLoss: 421.5124\n",
      "Training Epoch: 13 [6976/49669]\tLoss: 371.4792\n",
      "Training Epoch: 13 [7040/49669]\tLoss: 436.3219\n",
      "Training Epoch: 13 [7104/49669]\tLoss: 377.3314\n",
      "Training Epoch: 13 [7168/49669]\tLoss: 381.7321\n",
      "Training Epoch: 13 [7232/49669]\tLoss: 385.6711\n",
      "Training Epoch: 13 [7296/49669]\tLoss: 432.6302\n",
      "Training Epoch: 13 [7360/49669]\tLoss: 394.7440\n",
      "Training Epoch: 13 [7424/49669]\tLoss: 393.0781\n",
      "Training Epoch: 13 [7488/49669]\tLoss: 422.6276\n",
      "Training Epoch: 13 [7552/49669]\tLoss: 421.7079\n",
      "Training Epoch: 13 [7616/49669]\tLoss: 393.3310\n",
      "Training Epoch: 13 [7680/49669]\tLoss: 426.2089\n",
      "Training Epoch: 13 [7744/49669]\tLoss: 405.0423\n",
      "Training Epoch: 13 [7808/49669]\tLoss: 396.9074\n",
      "Training Epoch: 13 [7872/49669]\tLoss: 424.3628\n",
      "Training Epoch: 13 [7936/49669]\tLoss: 423.7932\n",
      "Training Epoch: 13 [8000/49669]\tLoss: 421.9698\n",
      "Training Epoch: 13 [8064/49669]\tLoss: 392.1313\n",
      "Training Epoch: 13 [8128/49669]\tLoss: 389.6986\n",
      "Training Epoch: 13 [8192/49669]\tLoss: 406.6932\n",
      "Training Epoch: 13 [8256/49669]\tLoss: 418.0626\n",
      "Training Epoch: 13 [8320/49669]\tLoss: 393.4596\n",
      "Training Epoch: 13 [8384/49669]\tLoss: 432.1994\n",
      "Training Epoch: 13 [8448/49669]\tLoss: 397.9454\n",
      "Training Epoch: 13 [8512/49669]\tLoss: 412.1856\n",
      "Training Epoch: 13 [8576/49669]\tLoss: 394.9680\n",
      "Training Epoch: 13 [8640/49669]\tLoss: 420.4335\n",
      "Training Epoch: 13 [8704/49669]\tLoss: 422.4533\n",
      "Training Epoch: 13 [8768/49669]\tLoss: 379.8009\n",
      "Training Epoch: 13 [8832/49669]\tLoss: 377.4344\n",
      "Training Epoch: 13 [8896/49669]\tLoss: 429.5685\n",
      "Training Epoch: 13 [8960/49669]\tLoss: 412.7982\n",
      "Training Epoch: 13 [9024/49669]\tLoss: 422.2739\n",
      "Training Epoch: 13 [9088/49669]\tLoss: 419.4778\n",
      "Training Epoch: 13 [9152/49669]\tLoss: 432.3257\n",
      "Training Epoch: 13 [9216/49669]\tLoss: 442.1954\n",
      "Training Epoch: 13 [9280/49669]\tLoss: 390.4481\n",
      "Training Epoch: 13 [9344/49669]\tLoss: 395.2506\n",
      "Training Epoch: 13 [9408/49669]\tLoss: 402.8489\n",
      "Training Epoch: 13 [9472/49669]\tLoss: 413.8145\n",
      "Training Epoch: 13 [9536/49669]\tLoss: 402.8416\n",
      "Training Epoch: 13 [9600/49669]\tLoss: 413.2503\n",
      "Training Epoch: 13 [9664/49669]\tLoss: 384.3277\n",
      "Training Epoch: 13 [9728/49669]\tLoss: 423.4084\n",
      "Training Epoch: 13 [9792/49669]\tLoss: 432.5571\n",
      "Training Epoch: 13 [9856/49669]\tLoss: 410.0125\n",
      "Training Epoch: 13 [9920/49669]\tLoss: 438.4972\n",
      "Training Epoch: 13 [9984/49669]\tLoss: 396.2341\n",
      "Training Epoch: 13 [10048/49669]\tLoss: 397.5150\n",
      "Training Epoch: 13 [10112/49669]\tLoss: 410.5540\n",
      "Training Epoch: 13 [10176/49669]\tLoss: 402.6759\n",
      "Training Epoch: 13 [10240/49669]\tLoss: 414.5891\n",
      "Training Epoch: 13 [10304/49669]\tLoss: 408.1497\n",
      "Training Epoch: 13 [10368/49669]\tLoss: 432.9240\n",
      "Training Epoch: 13 [10432/49669]\tLoss: 398.0573\n",
      "Training Epoch: 13 [10496/49669]\tLoss: 418.2635\n",
      "Training Epoch: 13 [10560/49669]\tLoss: 419.6962\n",
      "Training Epoch: 13 [10624/49669]\tLoss: 397.2607\n",
      "Training Epoch: 13 [10688/49669]\tLoss: 407.7506\n",
      "Training Epoch: 13 [10752/49669]\tLoss: 423.5189\n",
      "Training Epoch: 13 [10816/49669]\tLoss: 389.5278\n",
      "Training Epoch: 13 [10880/49669]\tLoss: 399.4052\n",
      "Training Epoch: 13 [10944/49669]\tLoss: 417.0948\n",
      "Training Epoch: 13 [11008/49669]\tLoss: 411.7275\n",
      "Training Epoch: 13 [11072/49669]\tLoss: 425.8498\n",
      "Training Epoch: 13 [11136/49669]\tLoss: 401.8007\n",
      "Training Epoch: 13 [11200/49669]\tLoss: 403.4626\n",
      "Training Epoch: 13 [11264/49669]\tLoss: 422.0057\n",
      "Training Epoch: 13 [11328/49669]\tLoss: 395.6259\n",
      "Training Epoch: 13 [11392/49669]\tLoss: 416.6014\n",
      "Training Epoch: 13 [11456/49669]\tLoss: 427.4786\n",
      "Training Epoch: 13 [11520/49669]\tLoss: 412.7262\n",
      "Training Epoch: 13 [11584/49669]\tLoss: 404.0280\n",
      "Training Epoch: 13 [11648/49669]\tLoss: 414.6882\n",
      "Training Epoch: 13 [11712/49669]\tLoss: 406.7285\n",
      "Training Epoch: 13 [11776/49669]\tLoss: 416.2404\n",
      "Training Epoch: 13 [11840/49669]\tLoss: 414.8696\n",
      "Training Epoch: 13 [11904/49669]\tLoss: 438.6559\n",
      "Training Epoch: 13 [11968/49669]\tLoss: 418.7475\n",
      "Training Epoch: 13 [12032/49669]\tLoss: 410.7165\n",
      "Training Epoch: 13 [12096/49669]\tLoss: 405.8241\n",
      "Training Epoch: 13 [12160/49669]\tLoss: 400.1440\n",
      "Training Epoch: 13 [12224/49669]\tLoss: 386.0229\n",
      "Training Epoch: 13 [12288/49669]\tLoss: 422.1052\n",
      "Training Epoch: 13 [12352/49669]\tLoss: 417.8478\n",
      "Training Epoch: 13 [12416/49669]\tLoss: 427.2965\n",
      "Training Epoch: 13 [12480/49669]\tLoss: 395.7029\n",
      "Training Epoch: 13 [12544/49669]\tLoss: 398.4548\n",
      "Training Epoch: 13 [12608/49669]\tLoss: 402.5323\n",
      "Training Epoch: 13 [12672/49669]\tLoss: 399.6627\n",
      "Training Epoch: 13 [12736/49669]\tLoss: 401.1348\n",
      "Training Epoch: 13 [12800/49669]\tLoss: 419.5917\n",
      "Training Epoch: 13 [12864/49669]\tLoss: 397.7013\n",
      "Training Epoch: 13 [12928/49669]\tLoss: 417.5506\n",
      "Training Epoch: 13 [12992/49669]\tLoss: 431.6388\n",
      "Training Epoch: 13 [13056/49669]\tLoss: 405.1250\n",
      "Training Epoch: 13 [13120/49669]\tLoss: 431.3786\n",
      "Training Epoch: 13 [13184/49669]\tLoss: 424.9630\n",
      "Training Epoch: 13 [13248/49669]\tLoss: 414.3371\n",
      "Training Epoch: 13 [13312/49669]\tLoss: 417.0005\n",
      "Training Epoch: 13 [13376/49669]\tLoss: 420.5987\n",
      "Training Epoch: 13 [13440/49669]\tLoss: 421.3185\n",
      "Training Epoch: 13 [13504/49669]\tLoss: 393.3090\n",
      "Training Epoch: 13 [13568/49669]\tLoss: 436.5521\n",
      "Training Epoch: 13 [13632/49669]\tLoss: 419.5941\n",
      "Training Epoch: 13 [13696/49669]\tLoss: 427.8569\n",
      "Training Epoch: 13 [13760/49669]\tLoss: 410.9812\n",
      "Training Epoch: 13 [13824/49669]\tLoss: 419.8714\n",
      "Training Epoch: 13 [13888/49669]\tLoss: 423.1782\n",
      "Training Epoch: 13 [13952/49669]\tLoss: 409.1424\n",
      "Training Epoch: 13 [14016/49669]\tLoss: 413.0510\n",
      "Training Epoch: 13 [14080/49669]\tLoss: 428.7795\n",
      "Training Epoch: 13 [14144/49669]\tLoss: 434.9385\n",
      "Training Epoch: 13 [14208/49669]\tLoss: 441.1630\n",
      "Training Epoch: 13 [14272/49669]\tLoss: 431.4104\n",
      "Training Epoch: 13 [14336/49669]\tLoss: 388.5630\n",
      "Training Epoch: 13 [14400/49669]\tLoss: 468.0012\n",
      "Training Epoch: 13 [14464/49669]\tLoss: 431.5424\n",
      "Training Epoch: 13 [14528/49669]\tLoss: 407.7055\n",
      "Training Epoch: 13 [14592/49669]\tLoss: 416.0119\n",
      "Training Epoch: 13 [14656/49669]\tLoss: 422.7746\n",
      "Training Epoch: 13 [14720/49669]\tLoss: 424.4330\n",
      "Training Epoch: 13 [14784/49669]\tLoss: 435.4565\n",
      "Training Epoch: 13 [14848/49669]\tLoss: 417.1380\n",
      "Training Epoch: 13 [14912/49669]\tLoss: 399.5169\n",
      "Training Epoch: 13 [14976/49669]\tLoss: 426.5399\n",
      "Training Epoch: 13 [15040/49669]\tLoss: 375.8997\n",
      "Training Epoch: 13 [15104/49669]\tLoss: 420.9321\n",
      "Training Epoch: 13 [15168/49669]\tLoss: 404.0245\n",
      "Training Epoch: 13 [15232/49669]\tLoss: 393.2466\n",
      "Training Epoch: 13 [15296/49669]\tLoss: 382.7254\n",
      "Training Epoch: 13 [15360/49669]\tLoss: 420.1482\n",
      "Training Epoch: 13 [15424/49669]\tLoss: 416.8735\n",
      "Training Epoch: 13 [15488/49669]\tLoss: 401.5371\n",
      "Training Epoch: 13 [15552/49669]\tLoss: 421.1611\n",
      "Training Epoch: 13 [15616/49669]\tLoss: 410.5467\n",
      "Training Epoch: 13 [15680/49669]\tLoss: 426.2466\n",
      "Training Epoch: 13 [15744/49669]\tLoss: 404.7765\n",
      "Training Epoch: 13 [15808/49669]\tLoss: 385.3951\n",
      "Training Epoch: 13 [15872/49669]\tLoss: 384.2686\n",
      "Training Epoch: 13 [15936/49669]\tLoss: 384.4762\n",
      "Training Epoch: 13 [16000/49669]\tLoss: 430.8407\n",
      "Training Epoch: 13 [16064/49669]\tLoss: 437.6999\n",
      "Training Epoch: 13 [16128/49669]\tLoss: 398.5482\n",
      "Training Epoch: 13 [16192/49669]\tLoss: 424.1276\n",
      "Training Epoch: 13 [16256/49669]\tLoss: 384.9679\n",
      "Training Epoch: 13 [16320/49669]\tLoss: 414.2464\n",
      "Training Epoch: 13 [16384/49669]\tLoss: 428.0056\n",
      "Training Epoch: 13 [16448/49669]\tLoss: 382.2294\n",
      "Training Epoch: 13 [16512/49669]\tLoss: 406.2461\n",
      "Training Epoch: 13 [16576/49669]\tLoss: 418.3507\n",
      "Training Epoch: 13 [16640/49669]\tLoss: 406.5340\n",
      "Training Epoch: 13 [16704/49669]\tLoss: 410.2516\n",
      "Training Epoch: 13 [16768/49669]\tLoss: 402.2341\n",
      "Training Epoch: 13 [16832/49669]\tLoss: 413.2845\n",
      "Training Epoch: 13 [16896/49669]\tLoss: 389.2726\n",
      "Training Epoch: 13 [16960/49669]\tLoss: 422.4602\n",
      "Training Epoch: 13 [17024/49669]\tLoss: 395.0930\n",
      "Training Epoch: 13 [17088/49669]\tLoss: 416.3366\n",
      "Training Epoch: 13 [17152/49669]\tLoss: 388.3179\n",
      "Training Epoch: 13 [17216/49669]\tLoss: 397.9021\n",
      "Training Epoch: 13 [17280/49669]\tLoss: 400.6236\n",
      "Training Epoch: 13 [17344/49669]\tLoss: 399.4815\n",
      "Training Epoch: 13 [17408/49669]\tLoss: 411.8869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [17472/49669]\tLoss: 405.5252\n",
      "Training Epoch: 13 [17536/49669]\tLoss: 415.0159\n",
      "Training Epoch: 13 [17600/49669]\tLoss: 447.8502\n",
      "Training Epoch: 13 [17664/49669]\tLoss: 417.9915\n",
      "Training Epoch: 13 [17728/49669]\tLoss: 453.7317\n",
      "Training Epoch: 13 [17792/49669]\tLoss: 406.9585\n",
      "Training Epoch: 13 [17856/49669]\tLoss: 417.5735\n",
      "Training Epoch: 13 [17920/49669]\tLoss: 420.7786\n",
      "Training Epoch: 13 [17984/49669]\tLoss: 433.1464\n",
      "Training Epoch: 13 [18048/49669]\tLoss: 426.7698\n",
      "Training Epoch: 13 [18112/49669]\tLoss: 409.5085\n",
      "Training Epoch: 13 [18176/49669]\tLoss: 385.7288\n",
      "Training Epoch: 13 [18240/49669]\tLoss: 399.2649\n",
      "Training Epoch: 13 [18304/49669]\tLoss: 401.3560\n",
      "Training Epoch: 13 [18368/49669]\tLoss: 424.0007\n",
      "Training Epoch: 13 [18432/49669]\tLoss: 377.8098\n",
      "Training Epoch: 13 [18496/49669]\tLoss: 432.2885\n",
      "Training Epoch: 13 [18560/49669]\tLoss: 407.1615\n",
      "Training Epoch: 13 [18624/49669]\tLoss: 402.0436\n",
      "Training Epoch: 13 [18688/49669]\tLoss: 416.4182\n",
      "Training Epoch: 13 [18752/49669]\tLoss: 418.5661\n",
      "Training Epoch: 13 [18816/49669]\tLoss: 428.8210\n",
      "Training Epoch: 13 [18880/49669]\tLoss: 418.7192\n",
      "Training Epoch: 13 [18944/49669]\tLoss: 439.2564\n",
      "Training Epoch: 13 [19008/49669]\tLoss: 430.8863\n",
      "Training Epoch: 13 [19072/49669]\tLoss: 446.2007\n",
      "Training Epoch: 13 [19136/49669]\tLoss: 426.0539\n",
      "Training Epoch: 13 [19200/49669]\tLoss: 440.7233\n",
      "Training Epoch: 13 [19264/49669]\tLoss: 431.0173\n",
      "Training Epoch: 13 [19328/49669]\tLoss: 432.1195\n",
      "Training Epoch: 13 [19392/49669]\tLoss: 442.5618\n",
      "Training Epoch: 13 [19456/49669]\tLoss: 438.7343\n",
      "Training Epoch: 13 [19520/49669]\tLoss: 412.4973\n",
      "Training Epoch: 13 [19584/49669]\tLoss: 388.0601\n",
      "Training Epoch: 13 [19648/49669]\tLoss: 452.5780\n",
      "Training Epoch: 13 [19712/49669]\tLoss: 407.9850\n",
      "Training Epoch: 13 [19776/49669]\tLoss: 421.4048\n",
      "Training Epoch: 13 [19840/49669]\tLoss: 423.4352\n",
      "Training Epoch: 13 [19904/49669]\tLoss: 420.2047\n",
      "Training Epoch: 13 [19968/49669]\tLoss: 425.2137\n",
      "Training Epoch: 13 [20032/49669]\tLoss: 398.0877\n",
      "Training Epoch: 13 [20096/49669]\tLoss: 406.9918\n",
      "Training Epoch: 13 [20160/49669]\tLoss: 415.5671\n",
      "Training Epoch: 13 [20224/49669]\tLoss: 438.5085\n",
      "Training Epoch: 13 [20288/49669]\tLoss: 390.1902\n",
      "Training Epoch: 13 [20352/49669]\tLoss: 470.1717\n",
      "Training Epoch: 13 [20416/49669]\tLoss: 412.3310\n",
      "Training Epoch: 13 [20480/49669]\tLoss: 437.4287\n",
      "Training Epoch: 13 [20544/49669]\tLoss: 418.1204\n",
      "Training Epoch: 13 [20608/49669]\tLoss: 403.2864\n",
      "Training Epoch: 13 [20672/49669]\tLoss: 429.1291\n",
      "Training Epoch: 13 [20736/49669]\tLoss: 432.6197\n",
      "Training Epoch: 13 [20800/49669]\tLoss: 442.5926\n",
      "Training Epoch: 13 [20864/49669]\tLoss: 445.3846\n",
      "Training Epoch: 13 [20928/49669]\tLoss: 406.5487\n",
      "Training Epoch: 13 [20992/49669]\tLoss: 423.6394\n",
      "Training Epoch: 13 [21056/49669]\tLoss: 399.3939\n",
      "Training Epoch: 13 [21120/49669]\tLoss: 406.4476\n",
      "Training Epoch: 13 [21184/49669]\tLoss: 401.5742\n",
      "Training Epoch: 13 [21248/49669]\tLoss: 428.1351\n",
      "Training Epoch: 13 [21312/49669]\tLoss: 410.2698\n",
      "Training Epoch: 13 [21376/49669]\tLoss: 391.5694\n",
      "Training Epoch: 13 [21440/49669]\tLoss: 429.2799\n",
      "Training Epoch: 13 [21504/49669]\tLoss: 400.5917\n",
      "Training Epoch: 13 [21568/49669]\tLoss: 438.9732\n",
      "Training Epoch: 13 [21632/49669]\tLoss: 407.6773\n",
      "Training Epoch: 13 [21696/49669]\tLoss: 415.2754\n",
      "Training Epoch: 13 [21760/49669]\tLoss: 414.2083\n",
      "Training Epoch: 13 [21824/49669]\tLoss: 378.0880\n",
      "Training Epoch: 13 [21888/49669]\tLoss: 410.8795\n",
      "Training Epoch: 13 [21952/49669]\tLoss: 405.0804\n",
      "Training Epoch: 13 [22016/49669]\tLoss: 403.7542\n",
      "Training Epoch: 13 [22080/49669]\tLoss: 423.4245\n",
      "Training Epoch: 13 [22144/49669]\tLoss: 416.9344\n",
      "Training Epoch: 13 [22208/49669]\tLoss: 412.2611\n",
      "Training Epoch: 13 [22272/49669]\tLoss: 422.6480\n",
      "Training Epoch: 13 [22336/49669]\tLoss: 411.8107\n",
      "Training Epoch: 13 [22400/49669]\tLoss: 412.9763\n",
      "Training Epoch: 13 [22464/49669]\tLoss: 395.1935\n",
      "Training Epoch: 13 [22528/49669]\tLoss: 436.4335\n",
      "Training Epoch: 13 [22592/49669]\tLoss: 393.0605\n",
      "Training Epoch: 13 [22656/49669]\tLoss: 412.9742\n",
      "Training Epoch: 13 [22720/49669]\tLoss: 384.1663\n",
      "Training Epoch: 13 [22784/49669]\tLoss: 420.2876\n",
      "Training Epoch: 13 [22848/49669]\tLoss: 407.1099\n",
      "Training Epoch: 13 [22912/49669]\tLoss: 421.2417\n",
      "Training Epoch: 13 [22976/49669]\tLoss: 399.0772\n",
      "Training Epoch: 13 [23040/49669]\tLoss: 375.6402\n",
      "Training Epoch: 13 [23104/49669]\tLoss: 429.6454\n",
      "Training Epoch: 13 [23168/49669]\tLoss: 434.5542\n",
      "Training Epoch: 13 [23232/49669]\tLoss: 415.8931\n",
      "Training Epoch: 13 [23296/49669]\tLoss: 411.7758\n",
      "Training Epoch: 13 [23360/49669]\tLoss: 426.5608\n",
      "Training Epoch: 13 [23424/49669]\tLoss: 398.8882\n",
      "Training Epoch: 13 [23488/49669]\tLoss: 405.0769\n",
      "Training Epoch: 13 [23552/49669]\tLoss: 402.3083\n",
      "Training Epoch: 13 [23616/49669]\tLoss: 398.8475\n",
      "Training Epoch: 13 [23680/49669]\tLoss: 401.8792\n",
      "Training Epoch: 13 [23744/49669]\tLoss: 441.0672\n",
      "Training Epoch: 13 [23808/49669]\tLoss: 419.2506\n",
      "Training Epoch: 13 [23872/49669]\tLoss: 393.9810\n",
      "Training Epoch: 13 [23936/49669]\tLoss: 427.5938\n",
      "Training Epoch: 13 [24000/49669]\tLoss: 411.2811\n",
      "Training Epoch: 13 [24064/49669]\tLoss: 392.0986\n",
      "Training Epoch: 13 [24128/49669]\tLoss: 371.4534\n",
      "Training Epoch: 13 [24192/49669]\tLoss: 409.6065\n",
      "Training Epoch: 13 [24256/49669]\tLoss: 397.7330\n",
      "Training Epoch: 13 [24320/49669]\tLoss: 419.7615\n",
      "Training Epoch: 13 [24384/49669]\tLoss: 404.2924\n",
      "Training Epoch: 13 [24448/49669]\tLoss: 402.5539\n",
      "Training Epoch: 13 [24512/49669]\tLoss: 402.3091\n",
      "Training Epoch: 13 [24576/49669]\tLoss: 398.4654\n",
      "Training Epoch: 13 [24640/49669]\tLoss: 401.4300\n",
      "Training Epoch: 13 [24704/49669]\tLoss: 413.8616\n",
      "Training Epoch: 13 [24768/49669]\tLoss: 390.2990\n",
      "Training Epoch: 13 [24832/49669]\tLoss: 434.6477\n",
      "Training Epoch: 13 [24896/49669]\tLoss: 410.6360\n",
      "Training Epoch: 13 [24960/49669]\tLoss: 396.2135\n",
      "Training Epoch: 13 [25024/49669]\tLoss: 414.6611\n",
      "Training Epoch: 13 [25088/49669]\tLoss: 380.1943\n",
      "Training Epoch: 13 [25152/49669]\tLoss: 409.4515\n",
      "Training Epoch: 13 [25216/49669]\tLoss: 411.2442\n",
      "Training Epoch: 13 [25280/49669]\tLoss: 425.5323\n",
      "Training Epoch: 13 [25344/49669]\tLoss: 412.4983\n",
      "Training Epoch: 13 [25408/49669]\tLoss: 420.4318\n",
      "Training Epoch: 13 [25472/49669]\tLoss: 432.5451\n",
      "Training Epoch: 13 [25536/49669]\tLoss: 424.1226\n",
      "Training Epoch: 13 [25600/49669]\tLoss: 409.5910\n",
      "Training Epoch: 13 [25664/49669]\tLoss: 414.3393\n",
      "Training Epoch: 13 [25728/49669]\tLoss: 426.0429\n",
      "Training Epoch: 13 [25792/49669]\tLoss: 407.2901\n",
      "Training Epoch: 13 [25856/49669]\tLoss: 397.7581\n",
      "Training Epoch: 13 [25920/49669]\tLoss: 375.5466\n",
      "Training Epoch: 13 [25984/49669]\tLoss: 403.6954\n",
      "Training Epoch: 13 [26048/49669]\tLoss: 424.5663\n",
      "Training Epoch: 13 [26112/49669]\tLoss: 420.5675\n",
      "Training Epoch: 13 [26176/49669]\tLoss: 440.2988\n",
      "Training Epoch: 13 [26240/49669]\tLoss: 387.5077\n",
      "Training Epoch: 13 [26304/49669]\tLoss: 389.3037\n",
      "Training Epoch: 13 [26368/49669]\tLoss: 405.3485\n",
      "Training Epoch: 13 [26432/49669]\tLoss: 399.4610\n",
      "Training Epoch: 13 [26496/49669]\tLoss: 409.4416\n",
      "Training Epoch: 13 [26560/49669]\tLoss: 403.5892\n",
      "Training Epoch: 13 [26624/49669]\tLoss: 421.7298\n",
      "Training Epoch: 13 [26688/49669]\tLoss: 429.2633\n",
      "Training Epoch: 13 [26752/49669]\tLoss: 431.7773\n",
      "Training Epoch: 13 [26816/49669]\tLoss: 435.4186\n",
      "Training Epoch: 13 [26880/49669]\tLoss: 418.4464\n",
      "Training Epoch: 13 [26944/49669]\tLoss: 416.9904\n",
      "Training Epoch: 13 [27008/49669]\tLoss: 413.3248\n",
      "Training Epoch: 13 [27072/49669]\tLoss: 417.6354\n",
      "Training Epoch: 13 [27136/49669]\tLoss: 438.6418\n",
      "Training Epoch: 13 [27200/49669]\tLoss: 415.5731\n",
      "Training Epoch: 13 [27264/49669]\tLoss: 403.7395\n",
      "Training Epoch: 13 [27328/49669]\tLoss: 453.6308\n",
      "Training Epoch: 13 [27392/49669]\tLoss: 409.7981\n",
      "Training Epoch: 13 [27456/49669]\tLoss: 404.8636\n",
      "Training Epoch: 13 [27520/49669]\tLoss: 394.8723\n",
      "Training Epoch: 13 [27584/49669]\tLoss: 437.3456\n",
      "Training Epoch: 13 [27648/49669]\tLoss: 416.4069\n",
      "Training Epoch: 13 [27712/49669]\tLoss: 398.1472\n",
      "Training Epoch: 13 [27776/49669]\tLoss: 411.1559\n",
      "Training Epoch: 13 [27840/49669]\tLoss: 415.1537\n",
      "Training Epoch: 13 [27904/49669]\tLoss: 432.3876\n",
      "Training Epoch: 13 [27968/49669]\tLoss: 424.2764\n",
      "Training Epoch: 13 [28032/49669]\tLoss: 456.8835\n",
      "Training Epoch: 13 [28096/49669]\tLoss: 474.2663\n",
      "Training Epoch: 13 [28160/49669]\tLoss: 457.3735\n",
      "Training Epoch: 13 [28224/49669]\tLoss: 461.6281\n",
      "Training Epoch: 13 [28288/49669]\tLoss: 465.9013\n",
      "Training Epoch: 13 [28352/49669]\tLoss: 462.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [28416/49669]\tLoss: 452.3789\n",
      "Training Epoch: 13 [28480/49669]\tLoss: 444.7634\n",
      "Training Epoch: 13 [28544/49669]\tLoss: 429.4484\n",
      "Training Epoch: 13 [28608/49669]\tLoss: 442.4597\n",
      "Training Epoch: 13 [28672/49669]\tLoss: 405.4420\n",
      "Training Epoch: 13 [28736/49669]\tLoss: 426.7417\n",
      "Training Epoch: 13 [28800/49669]\tLoss: 392.1118\n",
      "Training Epoch: 13 [28864/49669]\tLoss: 424.5311\n",
      "Training Epoch: 13 [28928/49669]\tLoss: 416.5348\n",
      "Training Epoch: 13 [28992/49669]\tLoss: 437.4866\n",
      "Training Epoch: 13 [29056/49669]\tLoss: 435.3207\n",
      "Training Epoch: 13 [29120/49669]\tLoss: 395.4445\n",
      "Training Epoch: 13 [29184/49669]\tLoss: 416.5784\n",
      "Training Epoch: 13 [29248/49669]\tLoss: 379.5407\n",
      "Training Epoch: 13 [29312/49669]\tLoss: 427.9367\n",
      "Training Epoch: 13 [29376/49669]\tLoss: 383.4102\n",
      "Training Epoch: 13 [29440/49669]\tLoss: 402.8312\n",
      "Training Epoch: 13 [29504/49669]\tLoss: 387.1730\n",
      "Training Epoch: 13 [29568/49669]\tLoss: 402.0770\n",
      "Training Epoch: 13 [29632/49669]\tLoss: 423.4588\n",
      "Training Epoch: 13 [29696/49669]\tLoss: 419.6519\n",
      "Training Epoch: 13 [29760/49669]\tLoss: 412.8641\n",
      "Training Epoch: 13 [29824/49669]\tLoss: 456.3186\n",
      "Training Epoch: 13 [29888/49669]\tLoss: 415.4518\n",
      "Training Epoch: 13 [29952/49669]\tLoss: 416.3947\n",
      "Training Epoch: 13 [30016/49669]\tLoss: 410.7323\n",
      "Training Epoch: 13 [30080/49669]\tLoss: 418.1457\n",
      "Training Epoch: 13 [30144/49669]\tLoss: 387.0350\n",
      "Training Epoch: 13 [30208/49669]\tLoss: 403.9240\n",
      "Training Epoch: 13 [30272/49669]\tLoss: 404.6100\n",
      "Training Epoch: 13 [30336/49669]\tLoss: 407.2542\n",
      "Training Epoch: 13 [30400/49669]\tLoss: 388.9463\n",
      "Training Epoch: 13 [30464/49669]\tLoss: 413.6328\n",
      "Training Epoch: 13 [30528/49669]\tLoss: 415.8870\n",
      "Training Epoch: 13 [30592/49669]\tLoss: 406.9571\n",
      "Training Epoch: 13 [30656/49669]\tLoss: 414.3965\n",
      "Training Epoch: 13 [30720/49669]\tLoss: 436.1138\n",
      "Training Epoch: 13 [30784/49669]\tLoss: 418.9088\n",
      "Training Epoch: 13 [30848/49669]\tLoss: 420.6122\n",
      "Training Epoch: 13 [30912/49669]\tLoss: 409.6497\n",
      "Training Epoch: 13 [30976/49669]\tLoss: 434.9913\n",
      "Training Epoch: 13 [31040/49669]\tLoss: 418.9641\n",
      "Training Epoch: 13 [31104/49669]\tLoss: 392.1395\n",
      "Training Epoch: 13 [31168/49669]\tLoss: 424.0872\n",
      "Training Epoch: 13 [31232/49669]\tLoss: 428.7709\n",
      "Training Epoch: 13 [31296/49669]\tLoss: 444.8344\n",
      "Training Epoch: 13 [31360/49669]\tLoss: 415.8136\n",
      "Training Epoch: 13 [31424/49669]\tLoss: 413.9454\n",
      "Training Epoch: 13 [31488/49669]\tLoss: 411.0932\n",
      "Training Epoch: 13 [31552/49669]\tLoss: 402.8202\n",
      "Training Epoch: 13 [31616/49669]\tLoss: 386.9924\n",
      "Training Epoch: 13 [31680/49669]\tLoss: 421.7099\n",
      "Training Epoch: 13 [31744/49669]\tLoss: 421.5754\n",
      "Training Epoch: 13 [31808/49669]\tLoss: 436.1191\n",
      "Training Epoch: 13 [31872/49669]\tLoss: 422.0135\n",
      "Training Epoch: 13 [31936/49669]\tLoss: 392.6102\n",
      "Training Epoch: 13 [32000/49669]\tLoss: 418.9257\n",
      "Training Epoch: 13 [32064/49669]\tLoss: 431.0259\n",
      "Training Epoch: 13 [32128/49669]\tLoss: 411.6328\n",
      "Training Epoch: 13 [32192/49669]\tLoss: 438.1403\n",
      "Training Epoch: 13 [32256/49669]\tLoss: 415.0135\n",
      "Training Epoch: 13 [32320/49669]\tLoss: 427.5912\n",
      "Training Epoch: 13 [32384/49669]\tLoss: 413.0176\n",
      "Training Epoch: 13 [32448/49669]\tLoss: 452.7873\n",
      "Training Epoch: 13 [32512/49669]\tLoss: 432.8427\n",
      "Training Epoch: 13 [32576/49669]\tLoss: 436.9408\n",
      "Training Epoch: 13 [32640/49669]\tLoss: 419.0732\n",
      "Training Epoch: 13 [32704/49669]\tLoss: 410.5956\n",
      "Training Epoch: 13 [32768/49669]\tLoss: 435.3053\n",
      "Training Epoch: 13 [32832/49669]\tLoss: 446.2244\n",
      "Training Epoch: 13 [32896/49669]\tLoss: 423.9659\n",
      "Training Epoch: 13 [32960/49669]\tLoss: 429.1612\n",
      "Training Epoch: 13 [33024/49669]\tLoss: 389.7597\n",
      "Training Epoch: 13 [33088/49669]\tLoss: 439.6237\n",
      "Training Epoch: 13 [33152/49669]\tLoss: 413.9619\n",
      "Training Epoch: 13 [33216/49669]\tLoss: 426.4903\n",
      "Training Epoch: 13 [33280/49669]\tLoss: 411.6328\n",
      "Training Epoch: 13 [33344/49669]\tLoss: 399.9146\n",
      "Training Epoch: 13 [33408/49669]\tLoss: 419.6159\n",
      "Training Epoch: 13 [33472/49669]\tLoss: 395.7472\n",
      "Training Epoch: 13 [33536/49669]\tLoss: 399.6567\n",
      "Training Epoch: 13 [33600/49669]\tLoss: 437.6535\n",
      "Training Epoch: 13 [33664/49669]\tLoss: 392.9094\n",
      "Training Epoch: 13 [33728/49669]\tLoss: 394.4440\n",
      "Training Epoch: 13 [33792/49669]\tLoss: 430.5521\n",
      "Training Epoch: 13 [33856/49669]\tLoss: 438.2421\n",
      "Training Epoch: 13 [33920/49669]\tLoss: 395.6422\n",
      "Training Epoch: 13 [33984/49669]\tLoss: 416.0744\n",
      "Training Epoch: 13 [34048/49669]\tLoss: 407.1589\n",
      "Training Epoch: 13 [34112/49669]\tLoss: 414.1089\n",
      "Training Epoch: 13 [34176/49669]\tLoss: 443.7055\n",
      "Training Epoch: 13 [34240/49669]\tLoss: 434.3235\n",
      "Training Epoch: 13 [34304/49669]\tLoss: 405.7507\n",
      "Training Epoch: 13 [34368/49669]\tLoss: 438.0469\n",
      "Training Epoch: 13 [34432/49669]\tLoss: 404.8728\n",
      "Training Epoch: 13 [34496/49669]\tLoss: 429.0084\n",
      "Training Epoch: 13 [34560/49669]\tLoss: 391.7763\n",
      "Training Epoch: 13 [34624/49669]\tLoss: 401.1675\n",
      "Training Epoch: 13 [34688/49669]\tLoss: 396.1083\n",
      "Training Epoch: 13 [34752/49669]\tLoss: 418.8094\n",
      "Training Epoch: 13 [34816/49669]\tLoss: 416.9221\n",
      "Training Epoch: 13 [34880/49669]\tLoss: 430.9083\n",
      "Training Epoch: 13 [34944/49669]\tLoss: 416.2983\n",
      "Training Epoch: 13 [35008/49669]\tLoss: 402.6161\n",
      "Training Epoch: 13 [35072/49669]\tLoss: 432.5106\n",
      "Training Epoch: 13 [35136/49669]\tLoss: 424.6619\n",
      "Training Epoch: 13 [35200/49669]\tLoss: 411.4899\n",
      "Training Epoch: 13 [35264/49669]\tLoss: 394.2810\n",
      "Training Epoch: 13 [35328/49669]\tLoss: 410.4247\n",
      "Training Epoch: 13 [35392/49669]\tLoss: 413.9897\n",
      "Training Epoch: 13 [35456/49669]\tLoss: 427.5422\n",
      "Training Epoch: 13 [35520/49669]\tLoss: 379.7645\n",
      "Training Epoch: 13 [35584/49669]\tLoss: 434.6252\n",
      "Training Epoch: 13 [35648/49669]\tLoss: 404.0021\n",
      "Training Epoch: 13 [35712/49669]\tLoss: 403.1301\n",
      "Training Epoch: 13 [35776/49669]\tLoss: 436.5485\n",
      "Training Epoch: 13 [35840/49669]\tLoss: 397.3615\n",
      "Training Epoch: 13 [35904/49669]\tLoss: 423.2290\n",
      "Training Epoch: 13 [35968/49669]\tLoss: 381.0341\n",
      "Training Epoch: 13 [36032/49669]\tLoss: 402.3638\n",
      "Training Epoch: 13 [36096/49669]\tLoss: 406.3699\n",
      "Training Epoch: 13 [36160/49669]\tLoss: 419.5228\n",
      "Training Epoch: 13 [36224/49669]\tLoss: 394.6024\n",
      "Training Epoch: 13 [36288/49669]\tLoss: 417.2014\n",
      "Training Epoch: 13 [36352/49669]\tLoss: 409.0607\n",
      "Training Epoch: 13 [36416/49669]\tLoss: 382.7008\n",
      "Training Epoch: 13 [36480/49669]\tLoss: 426.7801\n",
      "Training Epoch: 13 [36544/49669]\tLoss: 379.6351\n",
      "Training Epoch: 13 [36608/49669]\tLoss: 416.1869\n",
      "Training Epoch: 13 [36672/49669]\tLoss: 411.5471\n",
      "Training Epoch: 13 [36736/49669]\tLoss: 397.1144\n",
      "Training Epoch: 13 [36800/49669]\tLoss: 392.1250\n",
      "Training Epoch: 13 [36864/49669]\tLoss: 408.6850\n",
      "Training Epoch: 13 [36928/49669]\tLoss: 403.9044\n",
      "Training Epoch: 13 [36992/49669]\tLoss: 392.1621\n",
      "Training Epoch: 13 [37056/49669]\tLoss: 418.4668\n",
      "Training Epoch: 13 [37120/49669]\tLoss: 425.5362\n",
      "Training Epoch: 13 [37184/49669]\tLoss: 421.0814\n",
      "Training Epoch: 13 [37248/49669]\tLoss: 415.0891\n",
      "Training Epoch: 13 [37312/49669]\tLoss: 403.1827\n",
      "Training Epoch: 13 [37376/49669]\tLoss: 411.0698\n",
      "Training Epoch: 13 [37440/49669]\tLoss: 417.2314\n",
      "Training Epoch: 13 [37504/49669]\tLoss: 395.5674\n",
      "Training Epoch: 13 [37568/49669]\tLoss: 383.6937\n",
      "Training Epoch: 13 [37632/49669]\tLoss: 400.1745\n",
      "Training Epoch: 13 [37696/49669]\tLoss: 408.5529\n",
      "Training Epoch: 13 [37760/49669]\tLoss: 413.3912\n",
      "Training Epoch: 13 [37824/49669]\tLoss: 372.6412\n",
      "Training Epoch: 13 [37888/49669]\tLoss: 401.9572\n",
      "Training Epoch: 13 [37952/49669]\tLoss: 380.5688\n",
      "Training Epoch: 13 [38016/49669]\tLoss: 433.2881\n",
      "Training Epoch: 13 [38080/49669]\tLoss: 383.4347\n",
      "Training Epoch: 13 [38144/49669]\tLoss: 416.7433\n",
      "Training Epoch: 13 [38208/49669]\tLoss: 414.1732\n",
      "Training Epoch: 13 [38272/49669]\tLoss: 394.3315\n",
      "Training Epoch: 13 [38336/49669]\tLoss: 385.7558\n",
      "Training Epoch: 13 [38400/49669]\tLoss: 414.3685\n",
      "Training Epoch: 13 [38464/49669]\tLoss: 391.7997\n",
      "Training Epoch: 13 [38528/49669]\tLoss: 422.0649\n",
      "Training Epoch: 13 [38592/49669]\tLoss: 427.4584\n",
      "Training Epoch: 13 [38656/49669]\tLoss: 422.0857\n",
      "Training Epoch: 13 [38720/49669]\tLoss: 413.2374\n",
      "Training Epoch: 13 [38784/49669]\tLoss: 410.9044\n",
      "Training Epoch: 13 [38848/49669]\tLoss: 427.4995\n",
      "Training Epoch: 13 [38912/49669]\tLoss: 481.6638\n",
      "Training Epoch: 13 [38976/49669]\tLoss: 523.1796\n",
      "Training Epoch: 13 [39040/49669]\tLoss: 598.6340\n",
      "Training Epoch: 13 [39104/49669]\tLoss: 754.9018\n",
      "Training Epoch: 13 [39168/49669]\tLoss: 831.5271\n",
      "Training Epoch: 13 [39232/49669]\tLoss: 679.2246\n",
      "Training Epoch: 13 [39296/49669]\tLoss: 440.2430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [39360/49669]\tLoss: 483.4337\n",
      "Training Epoch: 13 [39424/49669]\tLoss: 584.7892\n",
      "Training Epoch: 13 [39488/49669]\tLoss: 545.4081\n",
      "Training Epoch: 13 [39552/49669]\tLoss: 405.7619\n",
      "Training Epoch: 13 [39616/49669]\tLoss: 484.5102\n",
      "Training Epoch: 13 [39680/49669]\tLoss: 545.4733\n",
      "Training Epoch: 13 [39744/49669]\tLoss: 436.3781\n",
      "Training Epoch: 13 [39808/49669]\tLoss: 406.9269\n",
      "Training Epoch: 13 [39872/49669]\tLoss: 473.0773\n",
      "Training Epoch: 13 [39936/49669]\tLoss: 418.8334\n",
      "Training Epoch: 13 [40000/49669]\tLoss: 449.3823\n",
      "Training Epoch: 13 [40064/49669]\tLoss: 480.6729\n",
      "Training Epoch: 13 [40128/49669]\tLoss: 425.9774\n",
      "Training Epoch: 13 [40192/49669]\tLoss: 421.4828\n",
      "Training Epoch: 13 [40256/49669]\tLoss: 446.7635\n",
      "Training Epoch: 13 [40320/49669]\tLoss: 416.7406\n",
      "Training Epoch: 13 [40384/49669]\tLoss: 397.4633\n",
      "Training Epoch: 13 [40448/49669]\tLoss: 456.4357\n",
      "Training Epoch: 13 [40512/49669]\tLoss: 409.0782\n",
      "Training Epoch: 13 [40576/49669]\tLoss: 406.1496\n",
      "Training Epoch: 13 [40640/49669]\tLoss: 409.6626\n",
      "Training Epoch: 13 [40704/49669]\tLoss: 414.1251\n",
      "Training Epoch: 13 [40768/49669]\tLoss: 398.1814\n",
      "Training Epoch: 13 [40832/49669]\tLoss: 481.1649\n",
      "Training Epoch: 13 [40896/49669]\tLoss: 398.9680\n",
      "Training Epoch: 13 [40960/49669]\tLoss: 405.9875\n",
      "Training Epoch: 13 [41024/49669]\tLoss: 433.5204\n",
      "Training Epoch: 13 [41088/49669]\tLoss: 431.6864\n",
      "Training Epoch: 13 [41152/49669]\tLoss: 430.6078\n",
      "Training Epoch: 13 [41216/49669]\tLoss: 385.3012\n",
      "Training Epoch: 13 [41280/49669]\tLoss: 395.9594\n",
      "Training Epoch: 13 [41344/49669]\tLoss: 389.2674\n",
      "Training Epoch: 13 [41408/49669]\tLoss: 423.2957\n",
      "Training Epoch: 13 [41472/49669]\tLoss: 393.5242\n",
      "Training Epoch: 13 [41536/49669]\tLoss: 417.2479\n",
      "Training Epoch: 13 [41600/49669]\tLoss: 420.5421\n",
      "Training Epoch: 13 [41664/49669]\tLoss: 400.6099\n",
      "Training Epoch: 13 [41728/49669]\tLoss: 397.6115\n",
      "Training Epoch: 13 [41792/49669]\tLoss: 428.9417\n",
      "Training Epoch: 13 [41856/49669]\tLoss: 423.0893\n",
      "Training Epoch: 13 [41920/49669]\tLoss: 395.4470\n",
      "Training Epoch: 13 [41984/49669]\tLoss: 429.9320\n",
      "Training Epoch: 13 [42048/49669]\tLoss: 443.1317\n",
      "Training Epoch: 13 [42112/49669]\tLoss: 413.1736\n",
      "Training Epoch: 13 [42176/49669]\tLoss: 393.8809\n",
      "Training Epoch: 13 [42240/49669]\tLoss: 394.0392\n",
      "Training Epoch: 13 [42304/49669]\tLoss: 422.5659\n",
      "Training Epoch: 13 [42368/49669]\tLoss: 383.7533\n",
      "Training Epoch: 13 [42432/49669]\tLoss: 428.2430\n",
      "Training Epoch: 13 [42496/49669]\tLoss: 406.4442\n",
      "Training Epoch: 13 [42560/49669]\tLoss: 399.7930\n",
      "Training Epoch: 13 [42624/49669]\tLoss: 410.8820\n",
      "Training Epoch: 13 [42688/49669]\tLoss: 406.4924\n",
      "Training Epoch: 13 [42752/49669]\tLoss: 409.2099\n",
      "Training Epoch: 13 [42816/49669]\tLoss: 428.4590\n",
      "Training Epoch: 13 [42880/49669]\tLoss: 419.0801\n",
      "Training Epoch: 13 [42944/49669]\tLoss: 416.6051\n",
      "Training Epoch: 13 [43008/49669]\tLoss: 438.9337\n",
      "Training Epoch: 13 [43072/49669]\tLoss: 430.1080\n",
      "Training Epoch: 13 [43136/49669]\tLoss: 416.3156\n",
      "Training Epoch: 13 [43200/49669]\tLoss: 382.8091\n",
      "Training Epoch: 13 [43264/49669]\tLoss: 426.0357\n",
      "Training Epoch: 13 [43328/49669]\tLoss: 381.1823\n",
      "Training Epoch: 13 [43392/49669]\tLoss: 397.5238\n",
      "Training Epoch: 13 [43456/49669]\tLoss: 408.5322\n",
      "Training Epoch: 13 [43520/49669]\tLoss: 389.1358\n",
      "Training Epoch: 13 [43584/49669]\tLoss: 370.0891\n",
      "Training Epoch: 13 [43648/49669]\tLoss: 414.9956\n",
      "Training Epoch: 13 [43712/49669]\tLoss: 392.6847\n",
      "Training Epoch: 13 [43776/49669]\tLoss: 404.4352\n",
      "Training Epoch: 13 [43840/49669]\tLoss: 395.9418\n",
      "Training Epoch: 13 [43904/49669]\tLoss: 424.1913\n",
      "Training Epoch: 13 [43968/49669]\tLoss: 414.6841\n",
      "Training Epoch: 13 [44032/49669]\tLoss: 437.0281\n",
      "Training Epoch: 13 [44096/49669]\tLoss: 391.4074\n",
      "Training Epoch: 13 [44160/49669]\tLoss: 385.0476\n",
      "Training Epoch: 13 [44224/49669]\tLoss: 395.6519\n",
      "Training Epoch: 13 [44288/49669]\tLoss: 420.7849\n",
      "Training Epoch: 13 [44352/49669]\tLoss: 408.4656\n",
      "Training Epoch: 13 [44416/49669]\tLoss: 393.7989\n",
      "Training Epoch: 13 [44480/49669]\tLoss: 402.3401\n",
      "Training Epoch: 13 [44544/49669]\tLoss: 409.3264\n",
      "Training Epoch: 13 [44608/49669]\tLoss: 399.3149\n",
      "Training Epoch: 13 [44672/49669]\tLoss: 447.8113\n",
      "Training Epoch: 13 [44736/49669]\tLoss: 423.1353\n",
      "Training Epoch: 13 [44800/49669]\tLoss: 406.6637\n",
      "Training Epoch: 13 [44864/49669]\tLoss: 395.8570\n",
      "Training Epoch: 13 [44928/49669]\tLoss: 410.2945\n",
      "Training Epoch: 13 [44992/49669]\tLoss: 408.4986\n",
      "Training Epoch: 13 [45056/49669]\tLoss: 389.3507\n",
      "Training Epoch: 13 [45120/49669]\tLoss: 404.5965\n",
      "Training Epoch: 13 [45184/49669]\tLoss: 417.3297\n",
      "Training Epoch: 13 [45248/49669]\tLoss: 399.0942\n",
      "Training Epoch: 13 [45312/49669]\tLoss: 394.6984\n",
      "Training Epoch: 13 [45376/49669]\tLoss: 422.2391\n",
      "Training Epoch: 13 [45440/49669]\tLoss: 384.6903\n",
      "Training Epoch: 13 [45504/49669]\tLoss: 403.9263\n",
      "Training Epoch: 13 [45568/49669]\tLoss: 414.0384\n",
      "Training Epoch: 13 [45632/49669]\tLoss: 432.9403\n",
      "Training Epoch: 13 [45696/49669]\tLoss: 419.0256\n",
      "Training Epoch: 13 [45760/49669]\tLoss: 402.6078\n",
      "Training Epoch: 13 [45824/49669]\tLoss: 385.5037\n",
      "Training Epoch: 13 [45888/49669]\tLoss: 406.7713\n",
      "Training Epoch: 13 [45952/49669]\tLoss: 410.8330\n",
      "Training Epoch: 13 [46016/49669]\tLoss: 383.3206\n",
      "Training Epoch: 13 [46080/49669]\tLoss: 387.9527\n",
      "Training Epoch: 13 [46144/49669]\tLoss: 434.7243\n",
      "Training Epoch: 13 [46208/49669]\tLoss: 445.5034\n",
      "Training Epoch: 13 [46272/49669]\tLoss: 426.1805\n",
      "Training Epoch: 13 [46336/49669]\tLoss: 410.5813\n",
      "Training Epoch: 13 [46400/49669]\tLoss: 414.7012\n",
      "Training Epoch: 13 [46464/49669]\tLoss: 426.3370\n",
      "Training Epoch: 13 [46528/49669]\tLoss: 440.8727\n",
      "Training Epoch: 13 [46592/49669]\tLoss: 416.5736\n",
      "Training Epoch: 13 [46656/49669]\tLoss: 381.4477\n",
      "Training Epoch: 13 [46720/49669]\tLoss: 389.5591\n",
      "Training Epoch: 13 [46784/49669]\tLoss: 420.9665\n",
      "Training Epoch: 13 [46848/49669]\tLoss: 409.3466\n",
      "Training Epoch: 13 [46912/49669]\tLoss: 393.0508\n",
      "Training Epoch: 13 [46976/49669]\tLoss: 414.5648\n",
      "Training Epoch: 13 [47040/49669]\tLoss: 377.7765\n",
      "Training Epoch: 13 [47104/49669]\tLoss: 415.2722\n",
      "Training Epoch: 13 [47168/49669]\tLoss: 402.8654\n",
      "Training Epoch: 13 [47232/49669]\tLoss: 407.0244\n",
      "Training Epoch: 13 [47296/49669]\tLoss: 412.9078\n",
      "Training Epoch: 13 [47360/49669]\tLoss: 411.3814\n",
      "Training Epoch: 13 [47424/49669]\tLoss: 385.4166\n",
      "Training Epoch: 13 [47488/49669]\tLoss: 391.7238\n",
      "Training Epoch: 13 [47552/49669]\tLoss: 370.9272\n",
      "Training Epoch: 13 [47616/49669]\tLoss: 400.5291\n",
      "Training Epoch: 13 [47680/49669]\tLoss: 408.3341\n",
      "Training Epoch: 13 [47744/49669]\tLoss: 419.8176\n",
      "Training Epoch: 13 [47808/49669]\tLoss: 382.3843\n",
      "Training Epoch: 13 [47872/49669]\tLoss: 402.0935\n",
      "Training Epoch: 13 [47936/49669]\tLoss: 391.1869\n",
      "Training Epoch: 13 [48000/49669]\tLoss: 395.2748\n",
      "Training Epoch: 13 [48064/49669]\tLoss: 432.6444\n",
      "Training Epoch: 13 [48128/49669]\tLoss: 405.6998\n",
      "Training Epoch: 13 [48192/49669]\tLoss: 439.0134\n",
      "Training Epoch: 13 [48256/49669]\tLoss: 413.3832\n",
      "Training Epoch: 13 [48320/49669]\tLoss: 405.0419\n",
      "Training Epoch: 13 [48384/49669]\tLoss: 411.7959\n",
      "Training Epoch: 13 [48448/49669]\tLoss: 396.4649\n",
      "Training Epoch: 13 [48512/49669]\tLoss: 430.2136\n",
      "Training Epoch: 13 [48576/49669]\tLoss: 394.4767\n",
      "Training Epoch: 13 [48640/49669]\tLoss: 414.6859\n",
      "Training Epoch: 13 [48704/49669]\tLoss: 424.1410\n",
      "Training Epoch: 13 [48768/49669]\tLoss: 395.2350\n",
      "Training Epoch: 13 [48832/49669]\tLoss: 419.1362\n",
      "Training Epoch: 13 [48896/49669]\tLoss: 422.4405\n",
      "Training Epoch: 13 [48960/49669]\tLoss: 396.6314\n",
      "Training Epoch: 13 [49024/49669]\tLoss: 399.7745\n",
      "Training Epoch: 13 [49088/49669]\tLoss: 419.8492\n",
      "Training Epoch: 13 [49152/49669]\tLoss: 396.0020\n",
      "Training Epoch: 13 [49216/49669]\tLoss: 416.4190\n",
      "Training Epoch: 13 [49280/49669]\tLoss: 371.6423\n",
      "Training Epoch: 13 [49344/49669]\tLoss: 404.3100\n",
      "Training Epoch: 13 [49408/49669]\tLoss: 415.6115\n",
      "Training Epoch: 13 [49472/49669]\tLoss: 388.1761\n",
      "Training Epoch: 13 [49536/49669]\tLoss: 408.8073\n",
      "Training Epoch: 13 [49600/49669]\tLoss: 396.6816\n",
      "Training Epoch: 13 [49664/49669]\tLoss: 378.4198\n",
      "Training Epoch: 13 [49669/49669]\tLoss: 439.5871\n",
      "Training Epoch: 13 [5519/5519]\tLoss: 411.9943\n",
      "Training Epoch: 14 [64/49669]\tLoss: 371.3310\n",
      "Training Epoch: 14 [128/49669]\tLoss: 432.0424\n",
      "Training Epoch: 14 [192/49669]\tLoss: 409.2845\n",
      "Training Epoch: 14 [256/49669]\tLoss: 416.2289\n",
      "Training Epoch: 14 [320/49669]\tLoss: 393.4976\n",
      "Training Epoch: 14 [384/49669]\tLoss: 441.3828\n",
      "Training Epoch: 14 [448/49669]\tLoss: 430.2027\n",
      "Training Epoch: 14 [512/49669]\tLoss: 413.8141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [576/49669]\tLoss: 396.4221\n",
      "Training Epoch: 14 [640/49669]\tLoss: 434.4493\n",
      "Training Epoch: 14 [704/49669]\tLoss: 417.7975\n",
      "Training Epoch: 14 [768/49669]\tLoss: 423.1087\n",
      "Training Epoch: 14 [832/49669]\tLoss: 411.1702\n",
      "Training Epoch: 14 [896/49669]\tLoss: 431.8996\n",
      "Training Epoch: 14 [960/49669]\tLoss: 404.8059\n",
      "Training Epoch: 14 [1024/49669]\tLoss: 423.2762\n",
      "Training Epoch: 14 [1088/49669]\tLoss: 457.0872\n",
      "Training Epoch: 14 [1152/49669]\tLoss: 417.6373\n",
      "Training Epoch: 14 [1216/49669]\tLoss: 413.6458\n",
      "Training Epoch: 14 [1280/49669]\tLoss: 400.4610\n",
      "Training Epoch: 14 [1344/49669]\tLoss: 410.5598\n",
      "Training Epoch: 14 [1408/49669]\tLoss: 401.4695\n",
      "Training Epoch: 14 [1472/49669]\tLoss: 400.0972\n",
      "Training Epoch: 14 [1536/49669]\tLoss: 434.4934\n",
      "Training Epoch: 14 [1600/49669]\tLoss: 428.5716\n",
      "Training Epoch: 14 [1664/49669]\tLoss: 375.6637\n",
      "Training Epoch: 14 [1728/49669]\tLoss: 403.6135\n",
      "Training Epoch: 14 [1792/49669]\tLoss: 404.9598\n",
      "Training Epoch: 14 [1856/49669]\tLoss: 418.2448\n",
      "Training Epoch: 14 [1920/49669]\tLoss: 401.6502\n",
      "Training Epoch: 14 [1984/49669]\tLoss: 424.0493\n",
      "Training Epoch: 14 [2048/49669]\tLoss: 411.0752\n",
      "Training Epoch: 14 [2112/49669]\tLoss: 415.1837\n",
      "Training Epoch: 14 [2176/49669]\tLoss: 398.4944\n",
      "Training Epoch: 14 [2240/49669]\tLoss: 405.3960\n",
      "Training Epoch: 14 [2304/49669]\tLoss: 410.4003\n",
      "Training Epoch: 14 [2368/49669]\tLoss: 410.0974\n",
      "Training Epoch: 14 [2432/49669]\tLoss: 411.1504\n",
      "Training Epoch: 14 [2496/49669]\tLoss: 402.7166\n",
      "Training Epoch: 14 [2560/49669]\tLoss: 395.2246\n",
      "Training Epoch: 14 [2624/49669]\tLoss: 410.6044\n",
      "Training Epoch: 14 [2688/49669]\tLoss: 354.2213\n",
      "Training Epoch: 14 [2752/49669]\tLoss: 391.7008\n",
      "Training Epoch: 14 [2816/49669]\tLoss: 417.8646\n",
      "Training Epoch: 14 [2880/49669]\tLoss: 393.0994\n",
      "Training Epoch: 14 [2944/49669]\tLoss: 444.9382\n",
      "Training Epoch: 14 [3008/49669]\tLoss: 404.5962\n",
      "Training Epoch: 14 [3072/49669]\tLoss: 419.1424\n",
      "Training Epoch: 14 [3136/49669]\tLoss: 429.8434\n",
      "Training Epoch: 14 [3200/49669]\tLoss: 419.6095\n",
      "Training Epoch: 14 [3264/49669]\tLoss: 409.5064\n",
      "Training Epoch: 14 [3328/49669]\tLoss: 407.2189\n",
      "Training Epoch: 14 [3392/49669]\tLoss: 429.6761\n",
      "Training Epoch: 14 [3456/49669]\tLoss: 384.8266\n",
      "Training Epoch: 14 [3520/49669]\tLoss: 387.5593\n",
      "Training Epoch: 14 [3584/49669]\tLoss: 411.6322\n",
      "Training Epoch: 14 [3648/49669]\tLoss: 409.7747\n",
      "Training Epoch: 14 [3712/49669]\tLoss: 426.1297\n",
      "Training Epoch: 14 [3776/49669]\tLoss: 408.4074\n",
      "Training Epoch: 14 [3840/49669]\tLoss: 383.3470\n",
      "Training Epoch: 14 [3904/49669]\tLoss: 423.2254\n",
      "Training Epoch: 14 [3968/49669]\tLoss: 432.2605\n",
      "Training Epoch: 14 [4032/49669]\tLoss: 456.8422\n",
      "Training Epoch: 14 [4096/49669]\tLoss: 413.9121\n",
      "Training Epoch: 14 [4160/49669]\tLoss: 413.7007\n",
      "Training Epoch: 14 [4224/49669]\tLoss: 382.9398\n",
      "Training Epoch: 14 [4288/49669]\tLoss: 435.4540\n",
      "Training Epoch: 14 [4352/49669]\tLoss: 412.9315\n",
      "Training Epoch: 14 [4416/49669]\tLoss: 414.5159\n",
      "Training Epoch: 14 [4480/49669]\tLoss: 406.7570\n",
      "Training Epoch: 14 [4544/49669]\tLoss: 394.9346\n",
      "Training Epoch: 14 [4608/49669]\tLoss: 412.6710\n",
      "Training Epoch: 14 [4672/49669]\tLoss: 418.0149\n",
      "Training Epoch: 14 [4736/49669]\tLoss: 404.1714\n",
      "Training Epoch: 14 [4800/49669]\tLoss: 397.3329\n",
      "Training Epoch: 14 [4864/49669]\tLoss: 407.0573\n",
      "Training Epoch: 14 [4928/49669]\tLoss: 421.2557\n",
      "Training Epoch: 14 [4992/49669]\tLoss: 405.5379\n",
      "Training Epoch: 14 [5056/49669]\tLoss: 395.8002\n",
      "Training Epoch: 14 [5120/49669]\tLoss: 382.1449\n",
      "Training Epoch: 14 [5184/49669]\tLoss: 392.5040\n",
      "Training Epoch: 14 [5248/49669]\tLoss: 402.0381\n",
      "Training Epoch: 14 [5312/49669]\tLoss: 433.7729\n",
      "Training Epoch: 14 [5376/49669]\tLoss: 396.5161\n",
      "Training Epoch: 14 [5440/49669]\tLoss: 421.0798\n",
      "Training Epoch: 14 [5504/49669]\tLoss: 393.0232\n",
      "Training Epoch: 14 [5568/49669]\tLoss: 384.6355\n",
      "Training Epoch: 14 [5632/49669]\tLoss: 408.2065\n",
      "Training Epoch: 14 [5696/49669]\tLoss: 443.3779\n",
      "Training Epoch: 14 [5760/49669]\tLoss: 382.4460\n",
      "Training Epoch: 14 [5824/49669]\tLoss: 427.0571\n",
      "Training Epoch: 14 [5888/49669]\tLoss: 386.6861\n",
      "Training Epoch: 14 [5952/49669]\tLoss: 420.9235\n",
      "Training Epoch: 14 [6016/49669]\tLoss: 423.5206\n",
      "Training Epoch: 14 [6080/49669]\tLoss: 421.4797\n",
      "Training Epoch: 14 [6144/49669]\tLoss: 414.2411\n",
      "Training Epoch: 14 [6208/49669]\tLoss: 424.3394\n",
      "Training Epoch: 14 [6272/49669]\tLoss: 412.4436\n",
      "Training Epoch: 14 [6336/49669]\tLoss: 420.9335\n",
      "Training Epoch: 14 [6400/49669]\tLoss: 406.0515\n",
      "Training Epoch: 14 [6464/49669]\tLoss: 401.1395\n",
      "Training Epoch: 14 [6528/49669]\tLoss: 420.8552\n",
      "Training Epoch: 14 [6592/49669]\tLoss: 416.1004\n",
      "Training Epoch: 14 [6656/49669]\tLoss: 421.3532\n",
      "Training Epoch: 14 [6720/49669]\tLoss: 422.9186\n",
      "Training Epoch: 14 [6784/49669]\tLoss: 422.7490\n",
      "Training Epoch: 14 [6848/49669]\tLoss: 398.1694\n",
      "Training Epoch: 14 [6912/49669]\tLoss: 417.9468\n",
      "Training Epoch: 14 [6976/49669]\tLoss: 391.3203\n",
      "Training Epoch: 14 [7040/49669]\tLoss: 397.6408\n",
      "Training Epoch: 14 [7104/49669]\tLoss: 392.7271\n",
      "Training Epoch: 14 [7168/49669]\tLoss: 409.9611\n",
      "Training Epoch: 14 [7232/49669]\tLoss: 408.0315\n",
      "Training Epoch: 14 [7296/49669]\tLoss: 417.1530\n",
      "Training Epoch: 14 [7360/49669]\tLoss: 410.0045\n",
      "Training Epoch: 14 [7424/49669]\tLoss: 427.8380\n",
      "Training Epoch: 14 [7488/49669]\tLoss: 408.1881\n",
      "Training Epoch: 14 [7552/49669]\tLoss: 390.1458\n",
      "Training Epoch: 14 [7616/49669]\tLoss: 427.9616\n",
      "Training Epoch: 14 [7680/49669]\tLoss: 400.0223\n",
      "Training Epoch: 14 [7744/49669]\tLoss: 398.7855\n",
      "Training Epoch: 14 [7808/49669]\tLoss: 444.4743\n",
      "Training Epoch: 14 [7872/49669]\tLoss: 401.8694\n",
      "Training Epoch: 14 [7936/49669]\tLoss: 431.0587\n",
      "Training Epoch: 14 [8000/49669]\tLoss: 410.0516\n",
      "Training Epoch: 14 [8064/49669]\tLoss: 390.6706\n",
      "Training Epoch: 14 [8128/49669]\tLoss: 408.1626\n",
      "Training Epoch: 14 [8192/49669]\tLoss: 412.7343\n",
      "Training Epoch: 14 [8256/49669]\tLoss: 423.1282\n",
      "Training Epoch: 14 [8320/49669]\tLoss: 377.1635\n",
      "Training Epoch: 14 [8384/49669]\tLoss: 398.1457\n",
      "Training Epoch: 14 [8448/49669]\tLoss: 423.4667\n",
      "Training Epoch: 14 [8512/49669]\tLoss: 391.2868\n",
      "Training Epoch: 14 [8576/49669]\tLoss: 404.8649\n",
      "Training Epoch: 14 [8640/49669]\tLoss: 407.8206\n",
      "Training Epoch: 14 [8704/49669]\tLoss: 391.2140\n",
      "Training Epoch: 14 [8768/49669]\tLoss: 392.0174\n",
      "Training Epoch: 14 [8832/49669]\tLoss: 425.8098\n",
      "Training Epoch: 14 [8896/49669]\tLoss: 430.5448\n",
      "Training Epoch: 14 [8960/49669]\tLoss: 437.6953\n",
      "Training Epoch: 14 [9024/49669]\tLoss: 416.6354\n",
      "Training Epoch: 14 [9088/49669]\tLoss: 422.1400\n",
      "Training Epoch: 14 [9152/49669]\tLoss: 404.9446\n",
      "Training Epoch: 14 [9216/49669]\tLoss: 394.2172\n",
      "Training Epoch: 14 [9280/49669]\tLoss: 390.6466\n",
      "Training Epoch: 14 [9344/49669]\tLoss: 410.8800\n",
      "Training Epoch: 14 [9408/49669]\tLoss: 409.9473\n",
      "Training Epoch: 14 [9472/49669]\tLoss: 446.5822\n",
      "Training Epoch: 14 [9536/49669]\tLoss: 407.9871\n",
      "Training Epoch: 14 [9600/49669]\tLoss: 385.4617\n",
      "Training Epoch: 14 [9664/49669]\tLoss: 410.6533\n",
      "Training Epoch: 14 [9728/49669]\tLoss: 420.5409\n",
      "Training Epoch: 14 [9792/49669]\tLoss: 395.0083\n",
      "Training Epoch: 14 [9856/49669]\tLoss: 400.6353\n",
      "Training Epoch: 14 [9920/49669]\tLoss: 413.2124\n",
      "Training Epoch: 14 [9984/49669]\tLoss: 426.5124\n",
      "Training Epoch: 14 [10048/49669]\tLoss: 407.8705\n",
      "Training Epoch: 14 [10112/49669]\tLoss: 416.8964\n",
      "Training Epoch: 14 [10176/49669]\tLoss: 417.5692\n",
      "Training Epoch: 14 [10240/49669]\tLoss: 437.7164\n",
      "Training Epoch: 14 [10304/49669]\tLoss: 375.2479\n",
      "Training Epoch: 14 [10368/49669]\tLoss: 423.8540\n",
      "Training Epoch: 14 [10432/49669]\tLoss: 425.9982\n",
      "Training Epoch: 14 [10496/49669]\tLoss: 423.4605\n",
      "Training Epoch: 14 [10560/49669]\tLoss: 419.3571\n",
      "Training Epoch: 14 [10624/49669]\tLoss: 419.4913\n",
      "Training Epoch: 14 [10688/49669]\tLoss: 461.9628\n",
      "Training Epoch: 14 [10752/49669]\tLoss: 427.3089\n",
      "Training Epoch: 14 [10816/49669]\tLoss: 416.4005\n",
      "Training Epoch: 14 [10880/49669]\tLoss: 428.1953\n",
      "Training Epoch: 14 [10944/49669]\tLoss: 403.7422\n",
      "Training Epoch: 14 [11008/49669]\tLoss: 412.7383\n",
      "Training Epoch: 14 [11072/49669]\tLoss: 412.0255\n",
      "Training Epoch: 14 [11136/49669]\tLoss: 395.0279\n",
      "Training Epoch: 14 [11200/49669]\tLoss: 383.1259\n",
      "Training Epoch: 14 [11264/49669]\tLoss: 409.7163\n",
      "Training Epoch: 14 [11328/49669]\tLoss: 409.1941\n",
      "Training Epoch: 14 [11392/49669]\tLoss: 405.6052\n",
      "Training Epoch: 14 [11456/49669]\tLoss: 377.0110\n",
      "Training Epoch: 14 [11520/49669]\tLoss: 431.7907\n",
      "Training Epoch: 14 [11584/49669]\tLoss: 443.5527\n",
      "Training Epoch: 14 [11648/49669]\tLoss: 420.9809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [11712/49669]\tLoss: 416.1715\n",
      "Training Epoch: 14 [11776/49669]\tLoss: 420.2441\n",
      "Training Epoch: 14 [11840/49669]\tLoss: 409.5972\n",
      "Training Epoch: 14 [11904/49669]\tLoss: 405.2515\n",
      "Training Epoch: 14 [11968/49669]\tLoss: 415.5361\n",
      "Training Epoch: 14 [12032/49669]\tLoss: 401.0352\n",
      "Training Epoch: 14 [12096/49669]\tLoss: 407.5507\n",
      "Training Epoch: 14 [12160/49669]\tLoss: 428.5192\n",
      "Training Epoch: 14 [12224/49669]\tLoss: 418.5721\n",
      "Training Epoch: 14 [12288/49669]\tLoss: 402.8037\n",
      "Training Epoch: 14 [12352/49669]\tLoss: 399.6938\n",
      "Training Epoch: 14 [12416/49669]\tLoss: 424.5997\n",
      "Training Epoch: 14 [12480/49669]\tLoss: 423.9773\n",
      "Training Epoch: 14 [12544/49669]\tLoss: 396.3513\n",
      "Training Epoch: 14 [12608/49669]\tLoss: 385.5281\n",
      "Training Epoch: 14 [12672/49669]\tLoss: 412.2252\n",
      "Training Epoch: 14 [12736/49669]\tLoss: 412.1810\n",
      "Training Epoch: 14 [12800/49669]\tLoss: 402.6150\n",
      "Training Epoch: 14 [12864/49669]\tLoss: 404.1797\n",
      "Training Epoch: 14 [12928/49669]\tLoss: 411.3573\n",
      "Training Epoch: 14 [12992/49669]\tLoss: 408.8691\n",
      "Training Epoch: 14 [13056/49669]\tLoss: 407.9700\n",
      "Training Epoch: 14 [13120/49669]\tLoss: 398.4636\n",
      "Training Epoch: 14 [13184/49669]\tLoss: 397.8086\n",
      "Training Epoch: 14 [13248/49669]\tLoss: 411.6597\n",
      "Training Epoch: 14 [13312/49669]\tLoss: 386.2344\n",
      "Training Epoch: 14 [13376/49669]\tLoss: 426.9112\n",
      "Training Epoch: 14 [13440/49669]\tLoss: 406.0227\n",
      "Training Epoch: 14 [13504/49669]\tLoss: 383.9926\n",
      "Training Epoch: 14 [13568/49669]\tLoss: 390.3742\n",
      "Training Epoch: 14 [13632/49669]\tLoss: 403.0499\n",
      "Training Epoch: 14 [13696/49669]\tLoss: 417.9282\n",
      "Training Epoch: 14 [13760/49669]\tLoss: 392.4486\n",
      "Training Epoch: 14 [13824/49669]\tLoss: 406.9575\n",
      "Training Epoch: 14 [13888/49669]\tLoss: 400.2647\n",
      "Training Epoch: 14 [13952/49669]\tLoss: 412.0391\n",
      "Training Epoch: 14 [14016/49669]\tLoss: 364.6816\n",
      "Training Epoch: 14 [14080/49669]\tLoss: 413.8623\n",
      "Training Epoch: 14 [14144/49669]\tLoss: 394.5259\n",
      "Training Epoch: 14 [14208/49669]\tLoss: 408.1460\n",
      "Training Epoch: 14 [14272/49669]\tLoss: 429.8450\n",
      "Training Epoch: 14 [14336/49669]\tLoss: 386.8611\n",
      "Training Epoch: 14 [14400/49669]\tLoss: 405.4703\n",
      "Training Epoch: 14 [14464/49669]\tLoss: 416.5757\n",
      "Training Epoch: 14 [14528/49669]\tLoss: 408.5638\n",
      "Training Epoch: 14 [14592/49669]\tLoss: 406.0290\n",
      "Training Epoch: 14 [14656/49669]\tLoss: 390.8709\n",
      "Training Epoch: 14 [14720/49669]\tLoss: 423.8282\n",
      "Training Epoch: 14 [14784/49669]\tLoss: 368.2403\n",
      "Training Epoch: 14 [14848/49669]\tLoss: 410.9349\n",
      "Training Epoch: 14 [14912/49669]\tLoss: 399.4165\n",
      "Training Epoch: 14 [14976/49669]\tLoss: 386.1339\n",
      "Training Epoch: 14 [15040/49669]\tLoss: 441.8817\n",
      "Training Epoch: 14 [15104/49669]\tLoss: 403.5918\n",
      "Training Epoch: 14 [15168/49669]\tLoss: 428.8991\n",
      "Training Epoch: 14 [15232/49669]\tLoss: 422.0263\n",
      "Training Epoch: 14 [15296/49669]\tLoss: 403.7335\n",
      "Training Epoch: 14 [15360/49669]\tLoss: 411.1328\n",
      "Training Epoch: 14 [15424/49669]\tLoss: 407.2243\n",
      "Training Epoch: 14 [15488/49669]\tLoss: 398.0273\n",
      "Training Epoch: 14 [15552/49669]\tLoss: 442.6112\n",
      "Training Epoch: 14 [15616/49669]\tLoss: 424.8337\n",
      "Training Epoch: 14 [15680/49669]\tLoss: 432.1555\n",
      "Training Epoch: 14 [15744/49669]\tLoss: 397.0519\n",
      "Training Epoch: 14 [15808/49669]\tLoss: 410.2154\n",
      "Training Epoch: 14 [15872/49669]\tLoss: 436.8635\n",
      "Training Epoch: 14 [15936/49669]\tLoss: 401.4199\n",
      "Training Epoch: 14 [16000/49669]\tLoss: 410.9198\n",
      "Training Epoch: 14 [16064/49669]\tLoss: 426.8216\n",
      "Training Epoch: 14 [16128/49669]\tLoss: 405.3739\n",
      "Training Epoch: 14 [16192/49669]\tLoss: 393.7121\n",
      "Training Epoch: 14 [16256/49669]\tLoss: 432.6656\n",
      "Training Epoch: 14 [16320/49669]\tLoss: 393.0378\n",
      "Training Epoch: 14 [16384/49669]\tLoss: 399.1661\n",
      "Training Epoch: 14 [16448/49669]\tLoss: 428.6000\n",
      "Training Epoch: 14 [16512/49669]\tLoss: 397.5893\n",
      "Training Epoch: 14 [16576/49669]\tLoss: 422.6073\n",
      "Training Epoch: 14 [16640/49669]\tLoss: 421.2016\n",
      "Training Epoch: 14 [16704/49669]\tLoss: 387.6876\n",
      "Training Epoch: 14 [16768/49669]\tLoss: 417.3728\n",
      "Training Epoch: 14 [16832/49669]\tLoss: 434.6515\n",
      "Training Epoch: 14 [16896/49669]\tLoss: 413.7304\n",
      "Training Epoch: 14 [16960/49669]\tLoss: 366.0930\n",
      "Training Epoch: 14 [17024/49669]\tLoss: 418.0984\n",
      "Training Epoch: 14 [17088/49669]\tLoss: 415.5903\n",
      "Training Epoch: 14 [17152/49669]\tLoss: 411.8140\n",
      "Training Epoch: 14 [17216/49669]\tLoss: 402.0656\n",
      "Training Epoch: 14 [17280/49669]\tLoss: 440.0847\n",
      "Training Epoch: 14 [17344/49669]\tLoss: 425.1223\n",
      "Training Epoch: 14 [17408/49669]\tLoss: 373.1601\n",
      "Training Epoch: 14 [17472/49669]\tLoss: 386.3748\n",
      "Training Epoch: 14 [17536/49669]\tLoss: 424.9967\n",
      "Training Epoch: 14 [17600/49669]\tLoss: 420.6371\n",
      "Training Epoch: 14 [17664/49669]\tLoss: 438.7018\n",
      "Training Epoch: 14 [17728/49669]\tLoss: 398.4706\n",
      "Training Epoch: 14 [17792/49669]\tLoss: 443.9257\n",
      "Training Epoch: 14 [17856/49669]\tLoss: 410.3395\n",
      "Training Epoch: 14 [17920/49669]\tLoss: 405.4757\n",
      "Training Epoch: 14 [17984/49669]\tLoss: 424.4931\n",
      "Training Epoch: 14 [18048/49669]\tLoss: 421.9684\n",
      "Training Epoch: 14 [18112/49669]\tLoss: 395.3490\n",
      "Training Epoch: 14 [18176/49669]\tLoss: 425.1823\n",
      "Training Epoch: 14 [18240/49669]\tLoss: 414.5042\n",
      "Training Epoch: 14 [18304/49669]\tLoss: 423.2182\n",
      "Training Epoch: 14 [18368/49669]\tLoss: 423.4233\n",
      "Training Epoch: 14 [18432/49669]\tLoss: 428.0646\n",
      "Training Epoch: 14 [18496/49669]\tLoss: 417.0124\n",
      "Training Epoch: 14 [18560/49669]\tLoss: 384.8354\n",
      "Training Epoch: 14 [18624/49669]\tLoss: 392.9874\n",
      "Training Epoch: 14 [18688/49669]\tLoss: 421.1167\n",
      "Training Epoch: 14 [18752/49669]\tLoss: 407.1664\n",
      "Training Epoch: 14 [18816/49669]\tLoss: 384.5837\n",
      "Training Epoch: 14 [18880/49669]\tLoss: 431.9622\n",
      "Training Epoch: 14 [18944/49669]\tLoss: 415.1264\n",
      "Training Epoch: 14 [19008/49669]\tLoss: 415.2282\n",
      "Training Epoch: 14 [19072/49669]\tLoss: 424.0651\n",
      "Training Epoch: 14 [19136/49669]\tLoss: 397.7426\n",
      "Training Epoch: 14 [19200/49669]\tLoss: 395.1628\n",
      "Training Epoch: 14 [19264/49669]\tLoss: 428.7462\n",
      "Training Epoch: 14 [19328/49669]\tLoss: 392.9176\n",
      "Training Epoch: 14 [19392/49669]\tLoss: 400.1741\n",
      "Training Epoch: 14 [19456/49669]\tLoss: 411.6540\n",
      "Training Epoch: 14 [19520/49669]\tLoss: 402.7615\n",
      "Training Epoch: 14 [19584/49669]\tLoss: 425.4120\n",
      "Training Epoch: 14 [19648/49669]\tLoss: 425.6827\n",
      "Training Epoch: 14 [19712/49669]\tLoss: 390.7603\n",
      "Training Epoch: 14 [19776/49669]\tLoss: 436.2262\n",
      "Training Epoch: 14 [19840/49669]\tLoss: 399.4978\n",
      "Training Epoch: 14 [19904/49669]\tLoss: 401.8488\n",
      "Training Epoch: 14 [19968/49669]\tLoss: 394.2071\n",
      "Training Epoch: 14 [20032/49669]\tLoss: 428.8200\n",
      "Training Epoch: 14 [20096/49669]\tLoss: 412.1718\n",
      "Training Epoch: 14 [20160/49669]\tLoss: 411.1595\n",
      "Training Epoch: 14 [20224/49669]\tLoss: 424.7262\n",
      "Training Epoch: 14 [20288/49669]\tLoss: 425.8641\n",
      "Training Epoch: 14 [20352/49669]\tLoss: 399.5683\n",
      "Training Epoch: 14 [20416/49669]\tLoss: 367.5686\n",
      "Training Epoch: 14 [20480/49669]\tLoss: 403.0245\n",
      "Training Epoch: 14 [20544/49669]\tLoss: 420.9883\n",
      "Training Epoch: 14 [20608/49669]\tLoss: 430.5372\n",
      "Training Epoch: 14 [20672/49669]\tLoss: 411.0622\n",
      "Training Epoch: 14 [20736/49669]\tLoss: 390.9738\n",
      "Training Epoch: 14 [20800/49669]\tLoss: 402.2477\n",
      "Training Epoch: 14 [20864/49669]\tLoss: 421.6191\n",
      "Training Epoch: 14 [20928/49669]\tLoss: 419.3963\n",
      "Training Epoch: 14 [20992/49669]\tLoss: 428.4905\n",
      "Training Epoch: 14 [21056/49669]\tLoss: 427.4565\n",
      "Training Epoch: 14 [21120/49669]\tLoss: 420.0450\n",
      "Training Epoch: 14 [21184/49669]\tLoss: 404.8341\n",
      "Training Epoch: 14 [21248/49669]\tLoss: 384.1764\n",
      "Training Epoch: 14 [21312/49669]\tLoss: 440.4104\n",
      "Training Epoch: 14 [21376/49669]\tLoss: 400.1071\n",
      "Training Epoch: 14 [21440/49669]\tLoss: 395.0244\n",
      "Training Epoch: 14 [21504/49669]\tLoss: 394.4390\n",
      "Training Epoch: 14 [21568/49669]\tLoss: 416.2831\n",
      "Training Epoch: 14 [21632/49669]\tLoss: 421.7017\n",
      "Training Epoch: 14 [21696/49669]\tLoss: 399.2314\n",
      "Training Epoch: 14 [21760/49669]\tLoss: 418.1542\n",
      "Training Epoch: 14 [21824/49669]\tLoss: 408.3731\n",
      "Training Epoch: 14 [21888/49669]\tLoss: 400.0119\n",
      "Training Epoch: 14 [21952/49669]\tLoss: 412.2110\n",
      "Training Epoch: 14 [22016/49669]\tLoss: 414.3771\n",
      "Training Epoch: 14 [22080/49669]\tLoss: 407.1880\n",
      "Training Epoch: 14 [22144/49669]\tLoss: 397.5354\n",
      "Training Epoch: 14 [22208/49669]\tLoss: 406.7815\n",
      "Training Epoch: 14 [22272/49669]\tLoss: 405.0252\n",
      "Training Epoch: 14 [22336/49669]\tLoss: 403.9615\n",
      "Training Epoch: 14 [22400/49669]\tLoss: 455.9198\n",
      "Training Epoch: 14 [22464/49669]\tLoss: 398.1360\n",
      "Training Epoch: 14 [22528/49669]\tLoss: 395.3687\n",
      "Training Epoch: 14 [22592/49669]\tLoss: 382.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [22656/49669]\tLoss: 421.2504\n",
      "Training Epoch: 14 [22720/49669]\tLoss: 431.8703\n",
      "Training Epoch: 14 [22784/49669]\tLoss: 410.0789\n",
      "Training Epoch: 14 [22848/49669]\tLoss: 448.7343\n",
      "Training Epoch: 14 [22912/49669]\tLoss: 435.4790\n",
      "Training Epoch: 14 [22976/49669]\tLoss: 403.8611\n",
      "Training Epoch: 14 [23040/49669]\tLoss: 413.7413\n",
      "Training Epoch: 14 [23104/49669]\tLoss: 423.7726\n",
      "Training Epoch: 14 [23168/49669]\tLoss: 390.5805\n",
      "Training Epoch: 14 [23232/49669]\tLoss: 397.8025\n",
      "Training Epoch: 14 [23296/49669]\tLoss: 416.5404\n",
      "Training Epoch: 14 [23360/49669]\tLoss: 398.3660\n",
      "Training Epoch: 14 [23424/49669]\tLoss: 385.1449\n",
      "Training Epoch: 14 [23488/49669]\tLoss: 408.4840\n",
      "Training Epoch: 14 [23552/49669]\tLoss: 421.8777\n",
      "Training Epoch: 14 [23616/49669]\tLoss: 439.0561\n",
      "Training Epoch: 14 [23680/49669]\tLoss: 443.7780\n",
      "Training Epoch: 14 [23744/49669]\tLoss: 433.5931\n",
      "Training Epoch: 14 [23808/49669]\tLoss: 416.8144\n",
      "Training Epoch: 14 [23872/49669]\tLoss: 438.1306\n",
      "Training Epoch: 14 [23936/49669]\tLoss: 385.3161\n",
      "Training Epoch: 14 [24000/49669]\tLoss: 424.0079\n",
      "Training Epoch: 14 [24064/49669]\tLoss: 398.9494\n",
      "Training Epoch: 14 [24128/49669]\tLoss: 412.2073\n",
      "Training Epoch: 14 [24192/49669]\tLoss: 393.4011\n",
      "Training Epoch: 14 [24256/49669]\tLoss: 411.9116\n",
      "Training Epoch: 14 [24320/49669]\tLoss: 430.4092\n",
      "Training Epoch: 14 [24384/49669]\tLoss: 400.6012\n",
      "Training Epoch: 14 [24448/49669]\tLoss: 425.7632\n",
      "Training Epoch: 14 [24512/49669]\tLoss: 411.0291\n",
      "Training Epoch: 14 [24576/49669]\tLoss: 410.1288\n",
      "Training Epoch: 14 [24640/49669]\tLoss: 423.3449\n",
      "Training Epoch: 14 [24704/49669]\tLoss: 405.1069\n",
      "Training Epoch: 14 [24768/49669]\tLoss: 419.0923\n",
      "Training Epoch: 14 [24832/49669]\tLoss: 410.4274\n",
      "Training Epoch: 14 [24896/49669]\tLoss: 407.6246\n",
      "Training Epoch: 14 [24960/49669]\tLoss: 431.6463\n",
      "Training Epoch: 14 [25024/49669]\tLoss: 417.8040\n",
      "Training Epoch: 14 [25088/49669]\tLoss: 410.5218\n",
      "Training Epoch: 14 [25152/49669]\tLoss: 435.9043\n",
      "Training Epoch: 14 [25216/49669]\tLoss: 411.3665\n",
      "Training Epoch: 14 [25280/49669]\tLoss: 408.7216\n",
      "Training Epoch: 14 [25344/49669]\tLoss: 418.2104\n",
      "Training Epoch: 14 [25408/49669]\tLoss: 397.1645\n",
      "Training Epoch: 14 [25472/49669]\tLoss: 424.7427\n",
      "Training Epoch: 14 [25536/49669]\tLoss: 401.8540\n",
      "Training Epoch: 14 [25600/49669]\tLoss: 413.4974\n",
      "Training Epoch: 14 [25664/49669]\tLoss: 429.4221\n",
      "Training Epoch: 14 [25728/49669]\tLoss: 419.9343\n",
      "Training Epoch: 14 [25792/49669]\tLoss: 406.9344\n",
      "Training Epoch: 14 [25856/49669]\tLoss: 410.9776\n",
      "Training Epoch: 14 [25920/49669]\tLoss: 408.2596\n",
      "Training Epoch: 14 [25984/49669]\tLoss: 408.9155\n",
      "Training Epoch: 14 [26048/49669]\tLoss: 407.2801\n",
      "Training Epoch: 14 [26112/49669]\tLoss: 435.5617\n",
      "Training Epoch: 14 [26176/49669]\tLoss: 428.5215\n",
      "Training Epoch: 14 [26240/49669]\tLoss: 441.8125\n",
      "Training Epoch: 14 [26304/49669]\tLoss: 404.2283\n",
      "Training Epoch: 14 [26368/49669]\tLoss: 441.9311\n",
      "Training Epoch: 14 [26432/49669]\tLoss: 419.0841\n",
      "Training Epoch: 14 [26496/49669]\tLoss: 410.0237\n",
      "Training Epoch: 14 [26560/49669]\tLoss: 414.9377\n",
      "Training Epoch: 14 [26624/49669]\tLoss: 391.2183\n",
      "Training Epoch: 14 [26688/49669]\tLoss: 414.1913\n",
      "Training Epoch: 14 [26752/49669]\tLoss: 398.3214\n",
      "Training Epoch: 14 [26816/49669]\tLoss: 419.8201\n",
      "Training Epoch: 14 [26880/49669]\tLoss: 433.6080\n",
      "Training Epoch: 14 [26944/49669]\tLoss: 378.6442\n",
      "Training Epoch: 14 [27008/49669]\tLoss: 405.9206\n",
      "Training Epoch: 14 [27072/49669]\tLoss: 402.0809\n",
      "Training Epoch: 14 [27136/49669]\tLoss: 416.5940\n",
      "Training Epoch: 14 [27200/49669]\tLoss: 388.4447\n",
      "Training Epoch: 14 [27264/49669]\tLoss: 408.1477\n",
      "Training Epoch: 14 [27328/49669]\tLoss: 442.5137\n",
      "Training Epoch: 14 [27392/49669]\tLoss: 386.6430\n",
      "Training Epoch: 14 [27456/49669]\tLoss: 399.0500\n",
      "Training Epoch: 14 [27520/49669]\tLoss: 430.4694\n",
      "Training Epoch: 14 [27584/49669]\tLoss: 425.4039\n",
      "Training Epoch: 14 [27648/49669]\tLoss: 405.6473\n",
      "Training Epoch: 14 [27712/49669]\tLoss: 446.1473\n",
      "Training Epoch: 14 [27776/49669]\tLoss: 466.0443\n",
      "Training Epoch: 14 [27840/49669]\tLoss: 489.0810\n",
      "Training Epoch: 14 [27904/49669]\tLoss: 494.3753\n",
      "Training Epoch: 14 [27968/49669]\tLoss: 501.4437\n",
      "Training Epoch: 14 [28032/49669]\tLoss: 432.2082\n",
      "Training Epoch: 14 [28096/49669]\tLoss: 421.9544\n",
      "Training Epoch: 14 [28160/49669]\tLoss: 400.7911\n",
      "Training Epoch: 14 [28224/49669]\tLoss: 437.1597\n",
      "Training Epoch: 14 [28288/49669]\tLoss: 428.2572\n",
      "Training Epoch: 14 [28352/49669]\tLoss: 426.2695\n",
      "Training Epoch: 14 [28416/49669]\tLoss: 387.1239\n",
      "Training Epoch: 14 [28480/49669]\tLoss: 407.1518\n",
      "Training Epoch: 14 [28544/49669]\tLoss: 403.6031\n",
      "Training Epoch: 14 [28608/49669]\tLoss: 437.4415\n",
      "Training Epoch: 14 [28672/49669]\tLoss: 400.3690\n",
      "Training Epoch: 14 [28736/49669]\tLoss: 417.7892\n",
      "Training Epoch: 14 [28800/49669]\tLoss: 403.4284\n",
      "Training Epoch: 14 [28864/49669]\tLoss: 421.7789\n",
      "Training Epoch: 14 [28928/49669]\tLoss: 424.9796\n",
      "Training Epoch: 14 [28992/49669]\tLoss: 418.1649\n",
      "Training Epoch: 14 [29056/49669]\tLoss: 418.1154\n",
      "Training Epoch: 14 [29120/49669]\tLoss: 402.9626\n",
      "Training Epoch: 14 [29184/49669]\tLoss: 414.3862\n",
      "Training Epoch: 14 [29248/49669]\tLoss: 423.3424\n",
      "Training Epoch: 14 [29312/49669]\tLoss: 408.2808\n",
      "Training Epoch: 14 [29376/49669]\tLoss: 433.8745\n",
      "Training Epoch: 14 [29440/49669]\tLoss: 383.1426\n",
      "Training Epoch: 14 [29504/49669]\tLoss: 432.5344\n",
      "Training Epoch: 14 [29568/49669]\tLoss: 403.0193\n",
      "Training Epoch: 14 [29632/49669]\tLoss: 409.7520\n",
      "Training Epoch: 14 [29696/49669]\tLoss: 427.4770\n",
      "Training Epoch: 14 [29760/49669]\tLoss: 419.2460\n",
      "Training Epoch: 14 [29824/49669]\tLoss: 428.5389\n",
      "Training Epoch: 14 [29888/49669]\tLoss: 388.1920\n",
      "Training Epoch: 14 [29952/49669]\tLoss: 404.9250\n",
      "Training Epoch: 14 [30016/49669]\tLoss: 418.0755\n",
      "Training Epoch: 14 [30080/49669]\tLoss: 418.3524\n",
      "Training Epoch: 14 [30144/49669]\tLoss: 372.0186\n",
      "Training Epoch: 14 [30208/49669]\tLoss: 377.3785\n",
      "Training Epoch: 14 [30272/49669]\tLoss: 433.4243\n",
      "Training Epoch: 14 [30336/49669]\tLoss: 424.4493\n",
      "Training Epoch: 14 [30400/49669]\tLoss: 412.0445\n",
      "Training Epoch: 14 [30464/49669]\tLoss: 408.0154\n",
      "Training Epoch: 14 [30528/49669]\tLoss: 404.9921\n",
      "Training Epoch: 14 [30592/49669]\tLoss: 400.4666\n",
      "Training Epoch: 14 [30656/49669]\tLoss: 417.0011\n",
      "Training Epoch: 14 [30720/49669]\tLoss: 402.5177\n",
      "Training Epoch: 14 [30784/49669]\tLoss: 400.1663\n",
      "Training Epoch: 14 [30848/49669]\tLoss: 430.1467\n",
      "Training Epoch: 14 [30912/49669]\tLoss: 406.1842\n",
      "Training Epoch: 14 [30976/49669]\tLoss: 381.2184\n",
      "Training Epoch: 14 [31040/49669]\tLoss: 422.7079\n",
      "Training Epoch: 14 [31104/49669]\tLoss: 406.4021\n",
      "Training Epoch: 14 [31168/49669]\tLoss: 378.2984\n",
      "Training Epoch: 14 [31232/49669]\tLoss: 414.2772\n",
      "Training Epoch: 14 [31296/49669]\tLoss: 411.2802\n",
      "Training Epoch: 14 [31360/49669]\tLoss: 433.2073\n",
      "Training Epoch: 14 [31424/49669]\tLoss: 386.9179\n",
      "Training Epoch: 14 [31488/49669]\tLoss: 399.9899\n",
      "Training Epoch: 14 [31552/49669]\tLoss: 374.3481\n",
      "Training Epoch: 14 [31616/49669]\tLoss: 358.8462\n",
      "Training Epoch: 14 [31680/49669]\tLoss: 392.3177\n",
      "Training Epoch: 14 [31744/49669]\tLoss: 427.9323\n",
      "Training Epoch: 14 [31808/49669]\tLoss: 383.8137\n",
      "Training Epoch: 14 [31872/49669]\tLoss: 421.3598\n",
      "Training Epoch: 14 [31936/49669]\tLoss: 396.3358\n",
      "Training Epoch: 14 [32000/49669]\tLoss: 385.8316\n",
      "Training Epoch: 14 [32064/49669]\tLoss: 398.9444\n",
      "Training Epoch: 14 [32128/49669]\tLoss: 419.2252\n",
      "Training Epoch: 14 [32192/49669]\tLoss: 407.1640\n",
      "Training Epoch: 14 [32256/49669]\tLoss: 409.5392\n",
      "Training Epoch: 14 [32320/49669]\tLoss: 394.3135\n",
      "Training Epoch: 14 [32384/49669]\tLoss: 400.8896\n",
      "Training Epoch: 14 [32448/49669]\tLoss: 430.5862\n",
      "Training Epoch: 14 [32512/49669]\tLoss: 407.6705\n",
      "Training Epoch: 14 [32576/49669]\tLoss: 383.8251\n",
      "Training Epoch: 14 [32640/49669]\tLoss: 406.9555\n",
      "Training Epoch: 14 [32704/49669]\tLoss: 427.0464\n",
      "Training Epoch: 14 [32768/49669]\tLoss: 418.5443\n",
      "Training Epoch: 14 [32832/49669]\tLoss: 419.8153\n",
      "Training Epoch: 14 [32896/49669]\tLoss: 445.8137\n",
      "Training Epoch: 14 [32960/49669]\tLoss: 381.5768\n",
      "Training Epoch: 14 [33024/49669]\tLoss: 401.3693\n",
      "Training Epoch: 14 [33088/49669]\tLoss: 390.8880\n",
      "Training Epoch: 14 [33152/49669]\tLoss: 425.5849\n",
      "Training Epoch: 14 [33216/49669]\tLoss: 399.4572\n",
      "Training Epoch: 14 [33280/49669]\tLoss: 401.0702\n",
      "Training Epoch: 14 [33344/49669]\tLoss: 429.1590\n",
      "Training Epoch: 14 [33408/49669]\tLoss: 406.6132\n",
      "Training Epoch: 14 [33472/49669]\tLoss: 415.9031\n",
      "Training Epoch: 14 [33536/49669]\tLoss: 379.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [33600/49669]\tLoss: 407.6815\n",
      "Training Epoch: 14 [33664/49669]\tLoss: 419.5790\n",
      "Training Epoch: 14 [33728/49669]\tLoss: 420.6326\n",
      "Training Epoch: 14 [33792/49669]\tLoss: 419.0909\n",
      "Training Epoch: 14 [33856/49669]\tLoss: 384.7893\n",
      "Training Epoch: 14 [33920/49669]\tLoss: 397.7521\n",
      "Training Epoch: 14 [33984/49669]\tLoss: 438.0953\n",
      "Training Epoch: 14 [34048/49669]\tLoss: 419.4233\n",
      "Training Epoch: 14 [34112/49669]\tLoss: 415.4583\n",
      "Training Epoch: 14 [34176/49669]\tLoss: 390.9493\n",
      "Training Epoch: 14 [34240/49669]\tLoss: 406.0285\n",
      "Training Epoch: 14 [34304/49669]\tLoss: 426.4561\n",
      "Training Epoch: 14 [34368/49669]\tLoss: 383.4004\n",
      "Training Epoch: 14 [34432/49669]\tLoss: 411.7914\n",
      "Training Epoch: 14 [34496/49669]\tLoss: 427.5594\n",
      "Training Epoch: 14 [34560/49669]\tLoss: 415.6725\n",
      "Training Epoch: 14 [34624/49669]\tLoss: 422.4698\n",
      "Training Epoch: 14 [34688/49669]\tLoss: 389.2638\n",
      "Training Epoch: 14 [34752/49669]\tLoss: 414.8539\n",
      "Training Epoch: 14 [34816/49669]\tLoss: 413.1525\n",
      "Training Epoch: 14 [34880/49669]\tLoss: 376.5078\n",
      "Training Epoch: 14 [34944/49669]\tLoss: 407.3181\n",
      "Training Epoch: 14 [35008/49669]\tLoss: 448.1075\n",
      "Training Epoch: 14 [35072/49669]\tLoss: 402.0619\n",
      "Training Epoch: 14 [35136/49669]\tLoss: 424.3382\n",
      "Training Epoch: 14 [35200/49669]\tLoss: 419.4362\n",
      "Training Epoch: 14 [35264/49669]\tLoss: 412.4050\n",
      "Training Epoch: 14 [35328/49669]\tLoss: 406.7645\n",
      "Training Epoch: 14 [35392/49669]\tLoss: 421.8945\n",
      "Training Epoch: 14 [35456/49669]\tLoss: 371.9231\n",
      "Training Epoch: 14 [35520/49669]\tLoss: 415.9950\n",
      "Training Epoch: 14 [35584/49669]\tLoss: 399.9010\n",
      "Training Epoch: 14 [35648/49669]\tLoss: 406.3269\n",
      "Training Epoch: 14 [35712/49669]\tLoss: 402.3410\n",
      "Training Epoch: 14 [35776/49669]\tLoss: 423.9888\n",
      "Training Epoch: 14 [35840/49669]\tLoss: 394.1302\n",
      "Training Epoch: 14 [35904/49669]\tLoss: 400.4363\n",
      "Training Epoch: 14 [35968/49669]\tLoss: 423.9340\n",
      "Training Epoch: 14 [36032/49669]\tLoss: 416.2463\n",
      "Training Epoch: 14 [36096/49669]\tLoss: 403.0269\n",
      "Training Epoch: 14 [36160/49669]\tLoss: 410.7747\n",
      "Training Epoch: 14 [36224/49669]\tLoss: 401.5585\n",
      "Training Epoch: 14 [36288/49669]\tLoss: 426.5271\n",
      "Training Epoch: 14 [36352/49669]\tLoss: 407.4810\n",
      "Training Epoch: 14 [36416/49669]\tLoss: 418.4990\n",
      "Training Epoch: 14 [36480/49669]\tLoss: 405.0201\n",
      "Training Epoch: 14 [36544/49669]\tLoss: 407.0965\n",
      "Training Epoch: 14 [36608/49669]\tLoss: 406.4492\n",
      "Training Epoch: 14 [36672/49669]\tLoss: 411.5322\n",
      "Training Epoch: 14 [36736/49669]\tLoss: 421.4811\n",
      "Training Epoch: 14 [36800/49669]\tLoss: 424.4441\n",
      "Training Epoch: 14 [36864/49669]\tLoss: 399.2381\n",
      "Training Epoch: 14 [36928/49669]\tLoss: 443.9184\n",
      "Training Epoch: 14 [36992/49669]\tLoss: 404.4026\n",
      "Training Epoch: 14 [37056/49669]\tLoss: 424.3032\n",
      "Training Epoch: 14 [37120/49669]\tLoss: 403.3355\n",
      "Training Epoch: 14 [37184/49669]\tLoss: 414.9504\n",
      "Training Epoch: 14 [37248/49669]\tLoss: 420.4107\n",
      "Training Epoch: 14 [37312/49669]\tLoss: 399.3576\n",
      "Training Epoch: 14 [37376/49669]\tLoss: 422.4774\n",
      "Training Epoch: 14 [37440/49669]\tLoss: 423.3557\n",
      "Training Epoch: 14 [37504/49669]\tLoss: 409.3314\n",
      "Training Epoch: 14 [37568/49669]\tLoss: 444.6546\n",
      "Training Epoch: 14 [37632/49669]\tLoss: 419.0762\n",
      "Training Epoch: 14 [37696/49669]\tLoss: 409.5572\n",
      "Training Epoch: 14 [37760/49669]\tLoss: 429.5590\n",
      "Training Epoch: 14 [37824/49669]\tLoss: 432.6649\n",
      "Training Epoch: 14 [37888/49669]\tLoss: 390.1072\n",
      "Training Epoch: 14 [37952/49669]\tLoss: 417.0513\n",
      "Training Epoch: 14 [38016/49669]\tLoss: 443.5971\n",
      "Training Epoch: 14 [38080/49669]\tLoss: 394.4059\n",
      "Training Epoch: 14 [38144/49669]\tLoss: 412.4373\n",
      "Training Epoch: 14 [38208/49669]\tLoss: 404.4815\n",
      "Training Epoch: 14 [38272/49669]\tLoss: 398.2107\n",
      "Training Epoch: 14 [38336/49669]\tLoss: 384.7664\n",
      "Training Epoch: 14 [38400/49669]\tLoss: 402.1005\n",
      "Training Epoch: 14 [38464/49669]\tLoss: 382.5380\n",
      "Training Epoch: 14 [38528/49669]\tLoss: 410.4214\n",
      "Training Epoch: 14 [38592/49669]\tLoss: 371.4440\n",
      "Training Epoch: 14 [38656/49669]\tLoss: 409.9393\n",
      "Training Epoch: 14 [38720/49669]\tLoss: 398.5652\n",
      "Training Epoch: 14 [38784/49669]\tLoss: 416.9433\n",
      "Training Epoch: 14 [38848/49669]\tLoss: 417.3533\n",
      "Training Epoch: 14 [38912/49669]\tLoss: 409.7679\n",
      "Training Epoch: 14 [38976/49669]\tLoss: 416.7786\n",
      "Training Epoch: 14 [39040/49669]\tLoss: 418.3713\n",
      "Training Epoch: 14 [39104/49669]\tLoss: 396.4990\n",
      "Training Epoch: 14 [39168/49669]\tLoss: 445.2395\n",
      "Training Epoch: 14 [39232/49669]\tLoss: 420.7804\n",
      "Training Epoch: 14 [39296/49669]\tLoss: 424.3655\n",
      "Training Epoch: 14 [39360/49669]\tLoss: 398.3192\n",
      "Training Epoch: 14 [39424/49669]\tLoss: 399.2350\n",
      "Training Epoch: 14 [39488/49669]\tLoss: 416.6383\n",
      "Training Epoch: 14 [39552/49669]\tLoss: 427.4617\n",
      "Training Epoch: 14 [39616/49669]\tLoss: 408.7039\n",
      "Training Epoch: 14 [39680/49669]\tLoss: 405.4924\n",
      "Training Epoch: 14 [39744/49669]\tLoss: 397.2455\n",
      "Training Epoch: 14 [39808/49669]\tLoss: 400.5437\n",
      "Training Epoch: 14 [39872/49669]\tLoss: 423.9739\n",
      "Training Epoch: 14 [39936/49669]\tLoss: 426.1544\n",
      "Training Epoch: 14 [40000/49669]\tLoss: 423.6303\n",
      "Training Epoch: 14 [40064/49669]\tLoss: 439.7029\n",
      "Training Epoch: 14 [40128/49669]\tLoss: 400.8693\n",
      "Training Epoch: 14 [40192/49669]\tLoss: 412.2434\n",
      "Training Epoch: 14 [40256/49669]\tLoss: 422.9531\n",
      "Training Epoch: 14 [40320/49669]\tLoss: 422.9267\n",
      "Training Epoch: 14 [40384/49669]\tLoss: 402.1537\n",
      "Training Epoch: 14 [40448/49669]\tLoss: 412.8959\n",
      "Training Epoch: 14 [40512/49669]\tLoss: 405.7055\n",
      "Training Epoch: 14 [40576/49669]\tLoss: 403.2179\n",
      "Training Epoch: 14 [40640/49669]\tLoss: 414.0609\n",
      "Training Epoch: 14 [40704/49669]\tLoss: 410.5009\n",
      "Training Epoch: 14 [40768/49669]\tLoss: 386.4730\n",
      "Training Epoch: 14 [40832/49669]\tLoss: 404.5227\n",
      "Training Epoch: 14 [40896/49669]\tLoss: 402.1246\n",
      "Training Epoch: 14 [40960/49669]\tLoss: 400.3083\n",
      "Training Epoch: 14 [41024/49669]\tLoss: 418.9526\n",
      "Training Epoch: 14 [41088/49669]\tLoss: 423.7455\n",
      "Training Epoch: 14 [41152/49669]\tLoss: 381.6972\n",
      "Training Epoch: 14 [41216/49669]\tLoss: 399.4323\n",
      "Training Epoch: 14 [41280/49669]\tLoss: 397.6155\n",
      "Training Epoch: 14 [41344/49669]\tLoss: 437.7391\n",
      "Training Epoch: 14 [41408/49669]\tLoss: 418.0367\n",
      "Training Epoch: 14 [41472/49669]\tLoss: 394.0370\n",
      "Training Epoch: 14 [41536/49669]\tLoss: 408.0278\n",
      "Training Epoch: 14 [41600/49669]\tLoss: 403.2162\n",
      "Training Epoch: 14 [41664/49669]\tLoss: 429.4428\n",
      "Training Epoch: 14 [41728/49669]\tLoss: 387.8510\n",
      "Training Epoch: 14 [41792/49669]\tLoss: 420.9135\n",
      "Training Epoch: 14 [41856/49669]\tLoss: 410.1330\n",
      "Training Epoch: 14 [41920/49669]\tLoss: 403.4828\n",
      "Training Epoch: 14 [41984/49669]\tLoss: 409.6989\n",
      "Training Epoch: 14 [42048/49669]\tLoss: 403.6404\n",
      "Training Epoch: 14 [42112/49669]\tLoss: 406.0594\n",
      "Training Epoch: 14 [42176/49669]\tLoss: 413.8314\n",
      "Training Epoch: 14 [42240/49669]\tLoss: 437.5673\n",
      "Training Epoch: 14 [42304/49669]\tLoss: 418.3820\n",
      "Training Epoch: 14 [42368/49669]\tLoss: 408.2724\n",
      "Training Epoch: 14 [42432/49669]\tLoss: 395.8610\n",
      "Training Epoch: 14 [42496/49669]\tLoss: 402.0426\n",
      "Training Epoch: 14 [42560/49669]\tLoss: 413.2270\n",
      "Training Epoch: 14 [42624/49669]\tLoss: 387.6432\n",
      "Training Epoch: 14 [42688/49669]\tLoss: 434.1650\n",
      "Training Epoch: 14 [42752/49669]\tLoss: 393.0209\n",
      "Training Epoch: 14 [42816/49669]\tLoss: 389.0560\n",
      "Training Epoch: 14 [42880/49669]\tLoss: 410.5294\n",
      "Training Epoch: 14 [42944/49669]\tLoss: 433.8026\n",
      "Training Epoch: 14 [43008/49669]\tLoss: 407.1523\n",
      "Training Epoch: 14 [43072/49669]\tLoss: 409.0606\n",
      "Training Epoch: 14 [43136/49669]\tLoss: 412.8657\n",
      "Training Epoch: 14 [43200/49669]\tLoss: 423.5652\n",
      "Training Epoch: 14 [43264/49669]\tLoss: 416.4572\n",
      "Training Epoch: 14 [43328/49669]\tLoss: 432.8633\n",
      "Training Epoch: 14 [43392/49669]\tLoss: 424.8712\n",
      "Training Epoch: 14 [43456/49669]\tLoss: 440.0134\n",
      "Training Epoch: 14 [43520/49669]\tLoss: 431.9435\n",
      "Training Epoch: 14 [43584/49669]\tLoss: 461.7364\n",
      "Training Epoch: 14 [43648/49669]\tLoss: 418.6391\n",
      "Training Epoch: 14 [43712/49669]\tLoss: 408.7087\n",
      "Training Epoch: 14 [43776/49669]\tLoss: 402.1088\n",
      "Training Epoch: 14 [43840/49669]\tLoss: 411.1975\n",
      "Training Epoch: 14 [43904/49669]\tLoss: 408.5203\n",
      "Training Epoch: 14 [43968/49669]\tLoss: 400.9297\n",
      "Training Epoch: 14 [44032/49669]\tLoss: 418.3614\n",
      "Training Epoch: 14 [44096/49669]\tLoss: 412.5650\n",
      "Training Epoch: 14 [44160/49669]\tLoss: 424.4695\n",
      "Training Epoch: 14 [44224/49669]\tLoss: 427.6457\n",
      "Training Epoch: 14 [44288/49669]\tLoss: 414.1672\n",
      "Training Epoch: 14 [44352/49669]\tLoss: 384.0043\n",
      "Training Epoch: 14 [44416/49669]\tLoss: 414.1395\n",
      "Training Epoch: 14 [44480/49669]\tLoss: 421.4757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [44544/49669]\tLoss: 425.8247\n",
      "Training Epoch: 14 [44608/49669]\tLoss: 394.4930\n",
      "Training Epoch: 14 [44672/49669]\tLoss: 424.1413\n",
      "Training Epoch: 14 [44736/49669]\tLoss: 430.9325\n",
      "Training Epoch: 14 [44800/49669]\tLoss: 422.6671\n",
      "Training Epoch: 14 [44864/49669]\tLoss: 408.2862\n",
      "Training Epoch: 14 [44928/49669]\tLoss: 431.3380\n",
      "Training Epoch: 14 [44992/49669]\tLoss: 422.3587\n",
      "Training Epoch: 14 [45056/49669]\tLoss: 377.1996\n",
      "Training Epoch: 14 [45120/49669]\tLoss: 432.3114\n",
      "Training Epoch: 14 [45184/49669]\tLoss: 418.5482\n",
      "Training Epoch: 14 [45248/49669]\tLoss: 437.3418\n",
      "Training Epoch: 14 [45312/49669]\tLoss: 379.5984\n",
      "Training Epoch: 14 [45376/49669]\tLoss: 355.2196\n",
      "Training Epoch: 14 [45440/49669]\tLoss: 428.2499\n",
      "Training Epoch: 14 [45504/49669]\tLoss: 433.8771\n",
      "Training Epoch: 14 [45568/49669]\tLoss: 413.4492\n",
      "Training Epoch: 14 [45632/49669]\tLoss: 385.9919\n",
      "Training Epoch: 14 [45696/49669]\tLoss: 400.4460\n",
      "Training Epoch: 14 [45760/49669]\tLoss: 402.1315\n",
      "Training Epoch: 14 [45824/49669]\tLoss: 412.2983\n",
      "Training Epoch: 14 [45888/49669]\tLoss: 405.8303\n",
      "Training Epoch: 14 [45952/49669]\tLoss: 404.1125\n",
      "Training Epoch: 14 [46016/49669]\tLoss: 409.6399\n",
      "Training Epoch: 14 [46080/49669]\tLoss: 380.3339\n",
      "Training Epoch: 14 [46144/49669]\tLoss: 364.8333\n",
      "Training Epoch: 14 [46208/49669]\tLoss: 402.3348\n",
      "Training Epoch: 14 [46272/49669]\tLoss: 409.9282\n",
      "Training Epoch: 14 [46336/49669]\tLoss: 414.6616\n",
      "Training Epoch: 14 [46400/49669]\tLoss: 446.8571\n",
      "Training Epoch: 14 [46464/49669]\tLoss: 417.6661\n",
      "Training Epoch: 14 [46528/49669]\tLoss: 423.0165\n",
      "Training Epoch: 14 [46592/49669]\tLoss: 405.7222\n",
      "Training Epoch: 14 [46656/49669]\tLoss: 428.5710\n",
      "Training Epoch: 14 [46720/49669]\tLoss: 421.5984\n",
      "Training Epoch: 14 [46784/49669]\tLoss: 425.5322\n",
      "Training Epoch: 14 [46848/49669]\tLoss: 415.2946\n",
      "Training Epoch: 14 [46912/49669]\tLoss: 429.4130\n",
      "Training Epoch: 14 [46976/49669]\tLoss: 422.2451\n",
      "Training Epoch: 14 [47040/49669]\tLoss: 391.5230\n",
      "Training Epoch: 14 [47104/49669]\tLoss: 422.1251\n",
      "Training Epoch: 14 [47168/49669]\tLoss: 395.9939\n",
      "Training Epoch: 14 [47232/49669]\tLoss: 430.8894\n",
      "Training Epoch: 14 [47296/49669]\tLoss: 393.0697\n",
      "Training Epoch: 14 [47360/49669]\tLoss: 399.9173\n",
      "Training Epoch: 14 [47424/49669]\tLoss: 412.4352\n",
      "Training Epoch: 14 [47488/49669]\tLoss: 409.0050\n",
      "Training Epoch: 14 [47552/49669]\tLoss: 396.8517\n",
      "Training Epoch: 14 [47616/49669]\tLoss: 396.8939\n",
      "Training Epoch: 14 [47680/49669]\tLoss: 387.5565\n",
      "Training Epoch: 14 [47744/49669]\tLoss: 400.3034\n",
      "Training Epoch: 14 [47808/49669]\tLoss: 425.8405\n",
      "Training Epoch: 14 [47872/49669]\tLoss: 421.4564\n",
      "Training Epoch: 14 [47936/49669]\tLoss: 399.2111\n",
      "Training Epoch: 14 [48000/49669]\tLoss: 420.7688\n",
      "Training Epoch: 14 [48064/49669]\tLoss: 427.0391\n",
      "Training Epoch: 14 [48128/49669]\tLoss: 428.1994\n",
      "Training Epoch: 14 [48192/49669]\tLoss: 433.4257\n",
      "Training Epoch: 14 [48256/49669]\tLoss: 434.1090\n",
      "Training Epoch: 14 [48320/49669]\tLoss: 417.6662\n",
      "Training Epoch: 14 [48384/49669]\tLoss: 425.6363\n",
      "Training Epoch: 14 [48448/49669]\tLoss: 385.6621\n",
      "Training Epoch: 14 [48512/49669]\tLoss: 411.9474\n",
      "Training Epoch: 14 [48576/49669]\tLoss: 408.0114\n",
      "Training Epoch: 14 [48640/49669]\tLoss: 426.6783\n",
      "Training Epoch: 14 [48704/49669]\tLoss: 382.9618\n",
      "Training Epoch: 14 [48768/49669]\tLoss: 398.7144\n",
      "Training Epoch: 14 [48832/49669]\tLoss: 387.0084\n",
      "Training Epoch: 14 [48896/49669]\tLoss: 416.6769\n",
      "Training Epoch: 14 [48960/49669]\tLoss: 414.1325\n",
      "Training Epoch: 14 [49024/49669]\tLoss: 414.9663\n",
      "Training Epoch: 14 [49088/49669]\tLoss: 407.4067\n",
      "Training Epoch: 14 [49152/49669]\tLoss: 412.6402\n",
      "Training Epoch: 14 [49216/49669]\tLoss: 407.4448\n",
      "Training Epoch: 14 [49280/49669]\tLoss: 405.5736\n",
      "Training Epoch: 14 [49344/49669]\tLoss: 430.8238\n",
      "Training Epoch: 14 [49408/49669]\tLoss: 401.1003\n",
      "Training Epoch: 14 [49472/49669]\tLoss: 420.8395\n",
      "Training Epoch: 14 [49536/49669]\tLoss: 415.5657\n",
      "Training Epoch: 14 [49600/49669]\tLoss: 391.3362\n",
      "Training Epoch: 14 [49664/49669]\tLoss: 390.5222\n",
      "Training Epoch: 14 [49669/49669]\tLoss: 385.4982\n",
      "Training Epoch: 14 [5519/5519]\tLoss: 412.0452\n",
      "Training Epoch: 15 [64/49669]\tLoss: 426.2834\n",
      "Training Epoch: 15 [128/49669]\tLoss: 421.1335\n",
      "Training Epoch: 15 [192/49669]\tLoss: 430.3694\n",
      "Training Epoch: 15 [256/49669]\tLoss: 436.2739\n",
      "Training Epoch: 15 [320/49669]\tLoss: 399.6011\n",
      "Training Epoch: 15 [384/49669]\tLoss: 427.7339\n",
      "Training Epoch: 15 [448/49669]\tLoss: 414.2376\n",
      "Training Epoch: 15 [512/49669]\tLoss: 421.8831\n",
      "Training Epoch: 15 [576/49669]\tLoss: 433.2190\n",
      "Training Epoch: 15 [640/49669]\tLoss: 383.7668\n",
      "Training Epoch: 15 [704/49669]\tLoss: 403.7142\n",
      "Training Epoch: 15 [768/49669]\tLoss: 411.8697\n",
      "Training Epoch: 15 [832/49669]\tLoss: 419.4038\n",
      "Training Epoch: 15 [896/49669]\tLoss: 352.6990\n",
      "Training Epoch: 15 [960/49669]\tLoss: 384.7037\n",
      "Training Epoch: 15 [1024/49669]\tLoss: 406.4756\n",
      "Training Epoch: 15 [1088/49669]\tLoss: 420.8491\n",
      "Training Epoch: 15 [1152/49669]\tLoss: 423.5921\n",
      "Training Epoch: 15 [1216/49669]\tLoss: 383.7008\n",
      "Training Epoch: 15 [1280/49669]\tLoss: 401.4334\n",
      "Training Epoch: 15 [1344/49669]\tLoss: 430.7132\n",
      "Training Epoch: 15 [1408/49669]\tLoss: 431.1337\n",
      "Training Epoch: 15 [1472/49669]\tLoss: 393.3093\n",
      "Training Epoch: 15 [1536/49669]\tLoss: 407.3469\n",
      "Training Epoch: 15 [1600/49669]\tLoss: 410.4428\n",
      "Training Epoch: 15 [1664/49669]\tLoss: 433.3057\n",
      "Training Epoch: 15 [1728/49669]\tLoss: 430.8594\n",
      "Training Epoch: 15 [1792/49669]\tLoss: 391.7935\n",
      "Training Epoch: 15 [1856/49669]\tLoss: 402.5026\n",
      "Training Epoch: 15 [1920/49669]\tLoss: 428.5547\n",
      "Training Epoch: 15 [1984/49669]\tLoss: 421.2509\n",
      "Training Epoch: 15 [2048/49669]\tLoss: 432.3517\n",
      "Training Epoch: 15 [2112/49669]\tLoss: 445.7333\n",
      "Training Epoch: 15 [2176/49669]\tLoss: 450.1701\n",
      "Training Epoch: 15 [2240/49669]\tLoss: 443.3552\n",
      "Training Epoch: 15 [2304/49669]\tLoss: 465.8386\n",
      "Training Epoch: 15 [2368/49669]\tLoss: 501.5186\n",
      "Training Epoch: 15 [2432/49669]\tLoss: 507.4282\n",
      "Training Epoch: 15 [2496/49669]\tLoss: 461.7275\n",
      "Training Epoch: 15 [2560/49669]\tLoss: 441.5450\n",
      "Training Epoch: 15 [2624/49669]\tLoss: 427.2849\n",
      "Training Epoch: 15 [2688/49669]\tLoss: 432.1151\n",
      "Training Epoch: 15 [2752/49669]\tLoss: 453.6324\n",
      "Training Epoch: 15 [2816/49669]\tLoss: 469.4024\n",
      "Training Epoch: 15 [2880/49669]\tLoss: 466.5583\n",
      "Training Epoch: 15 [2944/49669]\tLoss: 447.0583\n",
      "Training Epoch: 15 [3008/49669]\tLoss: 387.2314\n",
      "Training Epoch: 15 [3072/49669]\tLoss: 444.3107\n",
      "Training Epoch: 15 [3136/49669]\tLoss: 453.3611\n",
      "Training Epoch: 15 [3200/49669]\tLoss: 432.2817\n",
      "Training Epoch: 15 [3264/49669]\tLoss: 435.3597\n",
      "Training Epoch: 15 [3328/49669]\tLoss: 423.2745\n",
      "Training Epoch: 15 [3392/49669]\tLoss: 430.1497\n",
      "Training Epoch: 15 [3456/49669]\tLoss: 438.0573\n",
      "Training Epoch: 15 [3520/49669]\tLoss: 455.2596\n",
      "Training Epoch: 15 [3584/49669]\tLoss: 430.8836\n",
      "Training Epoch: 15 [3648/49669]\tLoss: 385.2843\n",
      "Training Epoch: 15 [3712/49669]\tLoss: 401.1046\n",
      "Training Epoch: 15 [3776/49669]\tLoss: 425.5881\n",
      "Training Epoch: 15 [3840/49669]\tLoss: 432.8115\n",
      "Training Epoch: 15 [3904/49669]\tLoss: 458.8554\n",
      "Training Epoch: 15 [3968/49669]\tLoss: 420.7842\n",
      "Training Epoch: 15 [4032/49669]\tLoss: 438.8193\n",
      "Training Epoch: 15 [4096/49669]\tLoss: 400.7138\n",
      "Training Epoch: 15 [4160/49669]\tLoss: 427.6035\n",
      "Training Epoch: 15 [4224/49669]\tLoss: 426.5134\n",
      "Training Epoch: 15 [4288/49669]\tLoss: 411.6707\n",
      "Training Epoch: 15 [4352/49669]\tLoss: 449.5911\n",
      "Training Epoch: 15 [4416/49669]\tLoss: 390.5510\n",
      "Training Epoch: 15 [4480/49669]\tLoss: 423.9391\n",
      "Training Epoch: 15 [4544/49669]\tLoss: 433.3768\n",
      "Training Epoch: 15 [4608/49669]\tLoss: 390.2194\n",
      "Training Epoch: 15 [4672/49669]\tLoss: 399.8040\n",
      "Training Epoch: 15 [4736/49669]\tLoss: 411.2233\n",
      "Training Epoch: 15 [4800/49669]\tLoss: 418.2557\n",
      "Training Epoch: 15 [4864/49669]\tLoss: 417.3027\n",
      "Training Epoch: 15 [4928/49669]\tLoss: 412.7888\n",
      "Training Epoch: 15 [4992/49669]\tLoss: 404.9362\n",
      "Training Epoch: 15 [5056/49669]\tLoss: 428.4377\n",
      "Training Epoch: 15 [5120/49669]\tLoss: 386.5789\n",
      "Training Epoch: 15 [5184/49669]\tLoss: 432.9093\n",
      "Training Epoch: 15 [5248/49669]\tLoss: 409.0997\n",
      "Training Epoch: 15 [5312/49669]\tLoss: 417.8543\n",
      "Training Epoch: 15 [5376/49669]\tLoss: 421.6498\n",
      "Training Epoch: 15 [5440/49669]\tLoss: 420.9310\n",
      "Training Epoch: 15 [5504/49669]\tLoss: 414.1358\n",
      "Training Epoch: 15 [5568/49669]\tLoss: 393.6482\n",
      "Training Epoch: 15 [5632/49669]\tLoss: 420.9771\n",
      "Training Epoch: 15 [5696/49669]\tLoss: 392.4625\n",
      "Training Epoch: 15 [5760/49669]\tLoss: 379.4014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [5824/49669]\tLoss: 398.1000\n",
      "Training Epoch: 15 [5888/49669]\tLoss: 434.8607\n",
      "Training Epoch: 15 [5952/49669]\tLoss: 431.8081\n",
      "Training Epoch: 15 [6016/49669]\tLoss: 415.2029\n",
      "Training Epoch: 15 [6080/49669]\tLoss: 405.7842\n",
      "Training Epoch: 15 [6144/49669]\tLoss: 421.9818\n",
      "Training Epoch: 15 [6208/49669]\tLoss: 428.3898\n",
      "Training Epoch: 15 [6272/49669]\tLoss: 408.8758\n",
      "Training Epoch: 15 [6336/49669]\tLoss: 404.4695\n",
      "Training Epoch: 15 [6400/49669]\tLoss: 412.5929\n",
      "Training Epoch: 15 [6464/49669]\tLoss: 408.2879\n",
      "Training Epoch: 15 [6528/49669]\tLoss: 371.7828\n",
      "Training Epoch: 15 [6592/49669]\tLoss: 399.8846\n",
      "Training Epoch: 15 [6656/49669]\tLoss: 400.7421\n",
      "Training Epoch: 15 [6720/49669]\tLoss: 376.6979\n",
      "Training Epoch: 15 [6784/49669]\tLoss: 427.1512\n",
      "Training Epoch: 15 [6848/49669]\tLoss: 389.6038\n",
      "Training Epoch: 15 [6912/49669]\tLoss: 411.5134\n",
      "Training Epoch: 15 [6976/49669]\tLoss: 425.5452\n",
      "Training Epoch: 15 [7040/49669]\tLoss: 411.7157\n",
      "Training Epoch: 15 [7104/49669]\tLoss: 401.8870\n",
      "Training Epoch: 15 [7168/49669]\tLoss: 419.7739\n",
      "Training Epoch: 15 [7232/49669]\tLoss: 399.1835\n",
      "Training Epoch: 15 [7296/49669]\tLoss: 421.5998\n",
      "Training Epoch: 15 [7360/49669]\tLoss: 409.1273\n",
      "Training Epoch: 15 [7424/49669]\tLoss: 433.6818\n",
      "Training Epoch: 15 [7488/49669]\tLoss: 390.2866\n",
      "Training Epoch: 15 [7552/49669]\tLoss: 426.3501\n",
      "Training Epoch: 15 [7616/49669]\tLoss: 404.8960\n",
      "Training Epoch: 15 [7680/49669]\tLoss: 397.5443\n",
      "Training Epoch: 15 [7744/49669]\tLoss: 408.4150\n",
      "Training Epoch: 15 [7808/49669]\tLoss: 418.4384\n",
      "Training Epoch: 15 [7872/49669]\tLoss: 400.0244\n",
      "Training Epoch: 15 [7936/49669]\tLoss: 413.7711\n",
      "Training Epoch: 15 [8000/49669]\tLoss: 408.2358\n",
      "Training Epoch: 15 [8064/49669]\tLoss: 397.8102\n",
      "Training Epoch: 15 [8128/49669]\tLoss: 419.6064\n",
      "Training Epoch: 15 [8192/49669]\tLoss: 345.6331\n",
      "Training Epoch: 15 [8256/49669]\tLoss: 438.2147\n",
      "Training Epoch: 15 [8320/49669]\tLoss: 423.9715\n",
      "Training Epoch: 15 [8384/49669]\tLoss: 396.1163\n",
      "Training Epoch: 15 [8448/49669]\tLoss: 415.2131\n",
      "Training Epoch: 15 [8512/49669]\tLoss: 429.9145\n",
      "Training Epoch: 15 [8576/49669]\tLoss: 427.5102\n",
      "Training Epoch: 15 [8640/49669]\tLoss: 403.2511\n",
      "Training Epoch: 15 [8704/49669]\tLoss: 420.8644\n",
      "Training Epoch: 15 [8768/49669]\tLoss: 422.6279\n",
      "Training Epoch: 15 [8832/49669]\tLoss: 382.3853\n",
      "Training Epoch: 15 [8896/49669]\tLoss: 400.1104\n",
      "Training Epoch: 15 [8960/49669]\tLoss: 383.3539\n",
      "Training Epoch: 15 [9024/49669]\tLoss: 433.0504\n",
      "Training Epoch: 15 [9088/49669]\tLoss: 400.5192\n",
      "Training Epoch: 15 [9152/49669]\tLoss: 402.1825\n",
      "Training Epoch: 15 [9216/49669]\tLoss: 426.5447\n",
      "Training Epoch: 15 [9280/49669]\tLoss: 396.2692\n",
      "Training Epoch: 15 [9344/49669]\tLoss: 412.5817\n",
      "Training Epoch: 15 [9408/49669]\tLoss: 378.6918\n",
      "Training Epoch: 15 [9472/49669]\tLoss: 395.6218\n",
      "Training Epoch: 15 [9536/49669]\tLoss: 425.5972\n",
      "Training Epoch: 15 [9600/49669]\tLoss: 431.1315\n",
      "Training Epoch: 15 [9664/49669]\tLoss: 374.7859\n",
      "Training Epoch: 15 [9728/49669]\tLoss: 430.1679\n",
      "Training Epoch: 15 [9792/49669]\tLoss: 398.2280\n",
      "Training Epoch: 15 [9856/49669]\tLoss: 428.9630\n",
      "Training Epoch: 15 [9920/49669]\tLoss: 395.6750\n",
      "Training Epoch: 15 [9984/49669]\tLoss: 410.9502\n",
      "Training Epoch: 15 [10048/49669]\tLoss: 440.8940\n",
      "Training Epoch: 15 [10112/49669]\tLoss: 388.1995\n",
      "Training Epoch: 15 [10176/49669]\tLoss: 427.2609\n",
      "Training Epoch: 15 [10240/49669]\tLoss: 390.2400\n",
      "Training Epoch: 15 [10304/49669]\tLoss: 391.3766\n",
      "Training Epoch: 15 [10368/49669]\tLoss: 438.9310\n",
      "Training Epoch: 15 [10432/49669]\tLoss: 404.4343\n",
      "Training Epoch: 15 [10496/49669]\tLoss: 392.6352\n",
      "Training Epoch: 15 [10560/49669]\tLoss: 373.8075\n",
      "Training Epoch: 15 [10624/49669]\tLoss: 391.5580\n",
      "Training Epoch: 15 [10688/49669]\tLoss: 410.1581\n",
      "Training Epoch: 15 [10752/49669]\tLoss: 398.9495\n",
      "Training Epoch: 15 [10816/49669]\tLoss: 409.7456\n",
      "Training Epoch: 15 [10880/49669]\tLoss: 398.1971\n",
      "Training Epoch: 15 [10944/49669]\tLoss: 403.4919\n",
      "Training Epoch: 15 [11008/49669]\tLoss: 419.9266\n",
      "Training Epoch: 15 [11072/49669]\tLoss: 400.6483\n",
      "Training Epoch: 15 [11136/49669]\tLoss: 395.5467\n",
      "Training Epoch: 15 [11200/49669]\tLoss: 444.3669\n",
      "Training Epoch: 15 [11264/49669]\tLoss: 398.8985\n",
      "Training Epoch: 15 [11328/49669]\tLoss: 405.9397\n",
      "Training Epoch: 15 [11392/49669]\tLoss: 415.4642\n",
      "Training Epoch: 15 [11456/49669]\tLoss: 400.1485\n",
      "Training Epoch: 15 [11520/49669]\tLoss: 411.7830\n",
      "Training Epoch: 15 [11584/49669]\tLoss: 413.4744\n",
      "Training Epoch: 15 [11648/49669]\tLoss: 422.6636\n",
      "Training Epoch: 15 [11712/49669]\tLoss: 407.5276\n",
      "Training Epoch: 15 [11776/49669]\tLoss: 433.8806\n",
      "Training Epoch: 15 [11840/49669]\tLoss: 385.3419\n",
      "Training Epoch: 15 [11904/49669]\tLoss: 405.1613\n",
      "Training Epoch: 15 [11968/49669]\tLoss: 418.7339\n",
      "Training Epoch: 15 [12032/49669]\tLoss: 382.0591\n",
      "Training Epoch: 15 [12096/49669]\tLoss: 438.8995\n",
      "Training Epoch: 15 [12160/49669]\tLoss: 429.7563\n",
      "Training Epoch: 15 [12224/49669]\tLoss: 422.5918\n",
      "Training Epoch: 15 [12288/49669]\tLoss: 402.9542\n",
      "Training Epoch: 15 [12352/49669]\tLoss: 404.5577\n",
      "Training Epoch: 15 [12416/49669]\tLoss: 443.1054\n",
      "Training Epoch: 15 [12480/49669]\tLoss: 420.2456\n",
      "Training Epoch: 15 [12544/49669]\tLoss: 404.9024\n",
      "Training Epoch: 15 [12608/49669]\tLoss: 409.8430\n",
      "Training Epoch: 15 [12672/49669]\tLoss: 418.7114\n",
      "Training Epoch: 15 [12736/49669]\tLoss: 402.7365\n",
      "Training Epoch: 15 [12800/49669]\tLoss: 379.4824\n",
      "Training Epoch: 15 [12864/49669]\tLoss: 388.1381\n",
      "Training Epoch: 15 [12928/49669]\tLoss: 387.4486\n",
      "Training Epoch: 15 [12992/49669]\tLoss: 429.3592\n",
      "Training Epoch: 15 [13056/49669]\tLoss: 393.2610\n",
      "Training Epoch: 15 [13120/49669]\tLoss: 392.8350\n",
      "Training Epoch: 15 [13184/49669]\tLoss: 405.7723\n",
      "Training Epoch: 15 [13248/49669]\tLoss: 406.4439\n",
      "Training Epoch: 15 [13312/49669]\tLoss: 428.6189\n",
      "Training Epoch: 15 [13376/49669]\tLoss: 414.1422\n",
      "Training Epoch: 15 [13440/49669]\tLoss: 406.4816\n",
      "Training Epoch: 15 [13504/49669]\tLoss: 424.1789\n",
      "Training Epoch: 15 [13568/49669]\tLoss: 417.8256\n",
      "Training Epoch: 15 [13632/49669]\tLoss: 452.8528\n",
      "Training Epoch: 15 [13696/49669]\tLoss: 457.7582\n",
      "Training Epoch: 15 [13760/49669]\tLoss: 411.5641\n",
      "Training Epoch: 15 [13824/49669]\tLoss: 428.5006\n",
      "Training Epoch: 15 [13888/49669]\tLoss: 429.8937\n",
      "Training Epoch: 15 [13952/49669]\tLoss: 435.1258\n",
      "Training Epoch: 15 [14016/49669]\tLoss: 445.6289\n",
      "Training Epoch: 15 [14080/49669]\tLoss: 406.8312\n",
      "Training Epoch: 15 [14144/49669]\tLoss: 445.2982\n",
      "Training Epoch: 15 [14208/49669]\tLoss: 423.0363\n",
      "Training Epoch: 15 [14272/49669]\tLoss: 437.0219\n",
      "Training Epoch: 15 [14336/49669]\tLoss: 405.5077\n",
      "Training Epoch: 15 [14400/49669]\tLoss: 403.5999\n",
      "Training Epoch: 15 [14464/49669]\tLoss: 432.8959\n",
      "Training Epoch: 15 [14528/49669]\tLoss: 418.6660\n",
      "Training Epoch: 15 [14592/49669]\tLoss: 416.9964\n",
      "Training Epoch: 15 [14656/49669]\tLoss: 391.8097\n",
      "Training Epoch: 15 [14720/49669]\tLoss: 403.9901\n",
      "Training Epoch: 15 [14784/49669]\tLoss: 441.2405\n",
      "Training Epoch: 15 [14848/49669]\tLoss: 432.7147\n",
      "Training Epoch: 15 [14912/49669]\tLoss: 439.3531\n",
      "Training Epoch: 15 [14976/49669]\tLoss: 402.9963\n",
      "Training Epoch: 15 [15040/49669]\tLoss: 392.1879\n",
      "Training Epoch: 15 [15104/49669]\tLoss: 403.4466\n",
      "Training Epoch: 15 [15168/49669]\tLoss: 416.3607\n",
      "Training Epoch: 15 [15232/49669]\tLoss: 407.9445\n",
      "Training Epoch: 15 [15296/49669]\tLoss: 409.9940\n",
      "Training Epoch: 15 [15360/49669]\tLoss: 419.8072\n",
      "Training Epoch: 15 [15424/49669]\tLoss: 425.2046\n",
      "Training Epoch: 15 [15488/49669]\tLoss: 422.9454\n",
      "Training Epoch: 15 [15552/49669]\tLoss: 394.9901\n",
      "Training Epoch: 15 [15616/49669]\tLoss: 393.6714\n",
      "Training Epoch: 15 [15680/49669]\tLoss: 400.2757\n",
      "Training Epoch: 15 [15744/49669]\tLoss: 392.7276\n",
      "Training Epoch: 15 [15808/49669]\tLoss: 374.2230\n",
      "Training Epoch: 15 [15872/49669]\tLoss: 404.3692\n",
      "Training Epoch: 15 [15936/49669]\tLoss: 404.3536\n",
      "Training Epoch: 15 [16000/49669]\tLoss: 409.2049\n",
      "Training Epoch: 15 [16064/49669]\tLoss: 423.0594\n",
      "Training Epoch: 15 [16128/49669]\tLoss: 422.1918\n",
      "Training Epoch: 15 [16192/49669]\tLoss: 399.3699\n",
      "Training Epoch: 15 [16256/49669]\tLoss: 397.1011\n",
      "Training Epoch: 15 [16320/49669]\tLoss: 384.7029\n",
      "Training Epoch: 15 [16384/49669]\tLoss: 412.2114\n",
      "Training Epoch: 15 [16448/49669]\tLoss: 414.7700\n",
      "Training Epoch: 15 [16512/49669]\tLoss: 414.6740\n",
      "Training Epoch: 15 [16576/49669]\tLoss: 393.8885\n",
      "Training Epoch: 15 [16640/49669]\tLoss: 418.0268\n",
      "Training Epoch: 15 [16704/49669]\tLoss: 381.5035\n",
      "Training Epoch: 15 [16768/49669]\tLoss: 442.6819\n",
      "Training Epoch: 15 [16832/49669]\tLoss: 417.3653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [16896/49669]\tLoss: 409.6349\n",
      "Training Epoch: 15 [16960/49669]\tLoss: 426.6530\n",
      "Training Epoch: 15 [17024/49669]\tLoss: 375.6133\n",
      "Training Epoch: 15 [17088/49669]\tLoss: 378.9334\n",
      "Training Epoch: 15 [17152/49669]\tLoss: 422.1438\n",
      "Training Epoch: 15 [17216/49669]\tLoss: 423.4706\n",
      "Training Epoch: 15 [17280/49669]\tLoss: 400.7697\n",
      "Training Epoch: 15 [17344/49669]\tLoss: 429.6740\n",
      "Training Epoch: 15 [17408/49669]\tLoss: 410.9828\n",
      "Training Epoch: 15 [17472/49669]\tLoss: 405.8148\n",
      "Training Epoch: 15 [17536/49669]\tLoss: 393.1515\n",
      "Training Epoch: 15 [17600/49669]\tLoss: 385.0294\n",
      "Training Epoch: 15 [17664/49669]\tLoss: 403.3379\n",
      "Training Epoch: 15 [17728/49669]\tLoss: 401.2543\n",
      "Training Epoch: 15 [17792/49669]\tLoss: 389.0574\n",
      "Training Epoch: 15 [17856/49669]\tLoss: 402.5922\n",
      "Training Epoch: 15 [17920/49669]\tLoss: 376.7388\n",
      "Training Epoch: 15 [17984/49669]\tLoss: 399.1103\n",
      "Training Epoch: 15 [18048/49669]\tLoss: 406.8349\n",
      "Training Epoch: 15 [18112/49669]\tLoss: 429.0271\n",
      "Training Epoch: 15 [18176/49669]\tLoss: 386.6147\n",
      "Training Epoch: 15 [18240/49669]\tLoss: 435.1562\n",
      "Training Epoch: 15 [18304/49669]\tLoss: 415.4333\n",
      "Training Epoch: 15 [18368/49669]\tLoss: 407.5370\n",
      "Training Epoch: 15 [18432/49669]\tLoss: 429.1651\n",
      "Training Epoch: 15 [18496/49669]\tLoss: 380.8279\n",
      "Training Epoch: 15 [18560/49669]\tLoss: 391.5317\n",
      "Training Epoch: 15 [18624/49669]\tLoss: 413.2188\n",
      "Training Epoch: 15 [18688/49669]\tLoss: 404.4314\n",
      "Training Epoch: 15 [18752/49669]\tLoss: 415.5666\n",
      "Training Epoch: 15 [18816/49669]\tLoss: 396.9498\n",
      "Training Epoch: 15 [18880/49669]\tLoss: 432.4436\n",
      "Training Epoch: 15 [18944/49669]\tLoss: 406.2934\n",
      "Training Epoch: 15 [19008/49669]\tLoss: 394.5712\n",
      "Training Epoch: 15 [19072/49669]\tLoss: 414.6206\n",
      "Training Epoch: 15 [19136/49669]\tLoss: 405.6228\n",
      "Training Epoch: 15 [19200/49669]\tLoss: 410.8390\n",
      "Training Epoch: 15 [19264/49669]\tLoss: 418.1790\n",
      "Training Epoch: 15 [19328/49669]\tLoss: 406.2539\n",
      "Training Epoch: 15 [19392/49669]\tLoss: 436.7315\n",
      "Training Epoch: 15 [19456/49669]\tLoss: 369.7787\n",
      "Training Epoch: 15 [19520/49669]\tLoss: 421.8165\n",
      "Training Epoch: 15 [19584/49669]\tLoss: 391.8595\n",
      "Training Epoch: 15 [19648/49669]\tLoss: 437.5639\n",
      "Training Epoch: 15 [19712/49669]\tLoss: 381.4885\n",
      "Training Epoch: 15 [19776/49669]\tLoss: 407.5612\n",
      "Training Epoch: 15 [19840/49669]\tLoss: 387.8315\n",
      "Training Epoch: 15 [19904/49669]\tLoss: 412.6264\n",
      "Training Epoch: 15 [19968/49669]\tLoss: 430.6443\n",
      "Training Epoch: 15 [20032/49669]\tLoss: 422.6055\n",
      "Training Epoch: 15 [20096/49669]\tLoss: 445.9231\n",
      "Training Epoch: 15 [20160/49669]\tLoss: 403.9167\n",
      "Training Epoch: 15 [20224/49669]\tLoss: 411.6319\n",
      "Training Epoch: 15 [20288/49669]\tLoss: 422.5758\n",
      "Training Epoch: 15 [20352/49669]\tLoss: 384.0843\n",
      "Training Epoch: 15 [20416/49669]\tLoss: 443.7002\n",
      "Training Epoch: 15 [20480/49669]\tLoss: 429.7055\n",
      "Training Epoch: 15 [20544/49669]\tLoss: 407.9790\n",
      "Training Epoch: 15 [20608/49669]\tLoss: 419.1962\n",
      "Training Epoch: 15 [20672/49669]\tLoss: 402.2476\n",
      "Training Epoch: 15 [20736/49669]\tLoss: 435.6395\n",
      "Training Epoch: 15 [20800/49669]\tLoss: 411.6716\n",
      "Training Epoch: 15 [20864/49669]\tLoss: 412.3365\n",
      "Training Epoch: 15 [20928/49669]\tLoss: 405.3158\n",
      "Training Epoch: 15 [20992/49669]\tLoss: 405.8824\n",
      "Training Epoch: 15 [21056/49669]\tLoss: 422.0503\n",
      "Training Epoch: 15 [21120/49669]\tLoss: 397.5176\n",
      "Training Epoch: 15 [21184/49669]\tLoss: 422.3165\n",
      "Training Epoch: 15 [21248/49669]\tLoss: 413.6659\n",
      "Training Epoch: 15 [21312/49669]\tLoss: 423.0153\n",
      "Training Epoch: 15 [21376/49669]\tLoss: 428.7600\n",
      "Training Epoch: 15 [21440/49669]\tLoss: 411.8277\n",
      "Training Epoch: 15 [21504/49669]\tLoss: 406.0153\n",
      "Training Epoch: 15 [21568/49669]\tLoss: 381.1073\n",
      "Training Epoch: 15 [21632/49669]\tLoss: 403.1402\n",
      "Training Epoch: 15 [21696/49669]\tLoss: 450.7228\n",
      "Training Epoch: 15 [21760/49669]\tLoss: 412.0552\n",
      "Training Epoch: 15 [21824/49669]\tLoss: 436.2584\n",
      "Training Epoch: 15 [21888/49669]\tLoss: 409.3592\n",
      "Training Epoch: 15 [21952/49669]\tLoss: 435.9890\n",
      "Training Epoch: 15 [22016/49669]\tLoss: 399.0788\n",
      "Training Epoch: 15 [22080/49669]\tLoss: 433.7540\n",
      "Training Epoch: 15 [22144/49669]\tLoss: 396.1356\n",
      "Training Epoch: 15 [22208/49669]\tLoss: 392.2781\n",
      "Training Epoch: 15 [22272/49669]\tLoss: 417.4067\n",
      "Training Epoch: 15 [22336/49669]\tLoss: 448.5992\n",
      "Training Epoch: 15 [22400/49669]\tLoss: 408.1075\n",
      "Training Epoch: 15 [22464/49669]\tLoss: 408.7769\n",
      "Training Epoch: 15 [22528/49669]\tLoss: 425.6389\n",
      "Training Epoch: 15 [22592/49669]\tLoss: 421.6928\n",
      "Training Epoch: 15 [22656/49669]\tLoss: 407.1591\n",
      "Training Epoch: 15 [22720/49669]\tLoss: 414.9541\n",
      "Training Epoch: 15 [22784/49669]\tLoss: 436.3001\n",
      "Training Epoch: 15 [22848/49669]\tLoss: 401.0461\n",
      "Training Epoch: 15 [22912/49669]\tLoss: 399.5882\n",
      "Training Epoch: 15 [22976/49669]\tLoss: 419.5079\n",
      "Training Epoch: 15 [23040/49669]\tLoss: 415.4687\n",
      "Training Epoch: 15 [23104/49669]\tLoss: 394.5286\n",
      "Training Epoch: 15 [23168/49669]\tLoss: 407.2879\n",
      "Training Epoch: 15 [23232/49669]\tLoss: 405.6830\n",
      "Training Epoch: 15 [23296/49669]\tLoss: 424.2323\n",
      "Training Epoch: 15 [23360/49669]\tLoss: 375.8614\n",
      "Training Epoch: 15 [23424/49669]\tLoss: 411.0148\n",
      "Training Epoch: 15 [23488/49669]\tLoss: 424.6512\n",
      "Training Epoch: 15 [23552/49669]\tLoss: 394.8199\n",
      "Training Epoch: 15 [23616/49669]\tLoss: 393.6875\n",
      "Training Epoch: 15 [23680/49669]\tLoss: 387.8336\n",
      "Training Epoch: 15 [23744/49669]\tLoss: 422.2046\n",
      "Training Epoch: 15 [23808/49669]\tLoss: 400.4877\n",
      "Training Epoch: 15 [23872/49669]\tLoss: 407.7761\n",
      "Training Epoch: 15 [23936/49669]\tLoss: 412.8064\n",
      "Training Epoch: 15 [24000/49669]\tLoss: 385.9077\n",
      "Training Epoch: 15 [24064/49669]\tLoss: 415.2055\n",
      "Training Epoch: 15 [24128/49669]\tLoss: 442.5924\n",
      "Training Epoch: 15 [24192/49669]\tLoss: 441.5279\n",
      "Training Epoch: 15 [24256/49669]\tLoss: 439.1584\n",
      "Training Epoch: 15 [24320/49669]\tLoss: 448.0152\n",
      "Training Epoch: 15 [24384/49669]\tLoss: 448.6624\n",
      "Training Epoch: 15 [24448/49669]\tLoss: 459.2193\n",
      "Training Epoch: 15 [24512/49669]\tLoss: 489.3759\n",
      "Training Epoch: 15 [24576/49669]\tLoss: 483.4622\n",
      "Training Epoch: 15 [24640/49669]\tLoss: 489.7518\n",
      "Training Epoch: 15 [24704/49669]\tLoss: 419.6931\n",
      "Training Epoch: 15 [24768/49669]\tLoss: 414.9634\n",
      "Training Epoch: 15 [24832/49669]\tLoss: 436.5379\n",
      "Training Epoch: 15 [24896/49669]\tLoss: 444.9708\n",
      "Training Epoch: 15 [24960/49669]\tLoss: 505.4951\n",
      "Training Epoch: 15 [25024/49669]\tLoss: 452.4421\n",
      "Training Epoch: 15 [25088/49669]\tLoss: 412.9760\n",
      "Training Epoch: 15 [25152/49669]\tLoss: 401.5245\n",
      "Training Epoch: 15 [25216/49669]\tLoss: 450.2310\n",
      "Training Epoch: 15 [25280/49669]\tLoss: 477.1454\n",
      "Training Epoch: 15 [25344/49669]\tLoss: 469.4067\n",
      "Training Epoch: 15 [25408/49669]\tLoss: 379.9545\n",
      "Training Epoch: 15 [25472/49669]\tLoss: 403.5452\n",
      "Training Epoch: 15 [25536/49669]\tLoss: 453.8859\n",
      "Training Epoch: 15 [25600/49669]\tLoss: 455.6471\n",
      "Training Epoch: 15 [25664/49669]\tLoss: 458.1331\n",
      "Training Epoch: 15 [25728/49669]\tLoss: 407.0544\n",
      "Training Epoch: 15 [25792/49669]\tLoss: 410.8936\n",
      "Training Epoch: 15 [25856/49669]\tLoss: 419.0601\n",
      "Training Epoch: 15 [25920/49669]\tLoss: 405.5449\n",
      "Training Epoch: 15 [25984/49669]\tLoss: 418.3207\n",
      "Training Epoch: 15 [26048/49669]\tLoss: 386.6814\n",
      "Training Epoch: 15 [26112/49669]\tLoss: 417.5587\n",
      "Training Epoch: 15 [26176/49669]\tLoss: 418.2330\n",
      "Training Epoch: 15 [26240/49669]\tLoss: 430.5745\n",
      "Training Epoch: 15 [26304/49669]\tLoss: 385.5943\n",
      "Training Epoch: 15 [26368/49669]\tLoss: 408.8956\n",
      "Training Epoch: 15 [26432/49669]\tLoss: 403.8652\n",
      "Training Epoch: 15 [26496/49669]\tLoss: 389.4138\n",
      "Training Epoch: 15 [26560/49669]\tLoss: 402.3914\n",
      "Training Epoch: 15 [26624/49669]\tLoss: 448.3495\n",
      "Training Epoch: 15 [26688/49669]\tLoss: 406.7590\n",
      "Training Epoch: 15 [26752/49669]\tLoss: 434.0132\n",
      "Training Epoch: 15 [26816/49669]\tLoss: 427.9744\n",
      "Training Epoch: 15 [26880/49669]\tLoss: 409.6638\n",
      "Training Epoch: 15 [26944/49669]\tLoss: 413.0187\n",
      "Training Epoch: 15 [27008/49669]\tLoss: 418.2124\n",
      "Training Epoch: 15 [27072/49669]\tLoss: 423.9355\n",
      "Training Epoch: 15 [27136/49669]\tLoss: 403.7588\n",
      "Training Epoch: 15 [27200/49669]\tLoss: 424.6095\n",
      "Training Epoch: 15 [27264/49669]\tLoss: 390.0044\n",
      "Training Epoch: 15 [27328/49669]\tLoss: 407.9286\n",
      "Training Epoch: 15 [27392/49669]\tLoss: 447.8688\n",
      "Training Epoch: 15 [27456/49669]\tLoss: 385.8453\n",
      "Training Epoch: 15 [27520/49669]\tLoss: 428.9928\n",
      "Training Epoch: 15 [27584/49669]\tLoss: 393.9154\n",
      "Training Epoch: 15 [27648/49669]\tLoss: 396.9025\n",
      "Training Epoch: 15 [27712/49669]\tLoss: 410.0297\n",
      "Training Epoch: 15 [27776/49669]\tLoss: 399.6486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [27840/49669]\tLoss: 400.5592\n",
      "Training Epoch: 15 [27904/49669]\tLoss: 407.6175\n",
      "Training Epoch: 15 [27968/49669]\tLoss: 388.4735\n",
      "Training Epoch: 15 [28032/49669]\tLoss: 407.2709\n",
      "Training Epoch: 15 [28096/49669]\tLoss: 388.7056\n",
      "Training Epoch: 15 [28160/49669]\tLoss: 401.6725\n",
      "Training Epoch: 15 [28224/49669]\tLoss: 404.7516\n",
      "Training Epoch: 15 [28288/49669]\tLoss: 406.7231\n",
      "Training Epoch: 15 [28352/49669]\tLoss: 379.4208\n",
      "Training Epoch: 15 [28416/49669]\tLoss: 416.7124\n",
      "Training Epoch: 15 [28480/49669]\tLoss: 391.1116\n",
      "Training Epoch: 15 [28544/49669]\tLoss: 417.9558\n",
      "Training Epoch: 15 [28608/49669]\tLoss: 389.6325\n",
      "Training Epoch: 15 [28672/49669]\tLoss: 430.4492\n",
      "Training Epoch: 15 [28736/49669]\tLoss: 438.1957\n",
      "Training Epoch: 15 [28800/49669]\tLoss: 405.6260\n",
      "Training Epoch: 15 [28864/49669]\tLoss: 409.9222\n",
      "Training Epoch: 15 [28928/49669]\tLoss: 430.9295\n",
      "Training Epoch: 15 [28992/49669]\tLoss: 404.5120\n",
      "Training Epoch: 15 [29056/49669]\tLoss: 427.5782\n",
      "Training Epoch: 15 [29120/49669]\tLoss: 441.3375\n",
      "Training Epoch: 15 [29184/49669]\tLoss: 409.1808\n",
      "Training Epoch: 15 [29248/49669]\tLoss: 398.0558\n",
      "Training Epoch: 15 [29312/49669]\tLoss: 405.6861\n",
      "Training Epoch: 15 [29376/49669]\tLoss: 420.8016\n",
      "Training Epoch: 15 [29440/49669]\tLoss: 416.5286\n",
      "Training Epoch: 15 [29504/49669]\tLoss: 418.2471\n",
      "Training Epoch: 15 [29568/49669]\tLoss: 417.4178\n",
      "Training Epoch: 15 [29632/49669]\tLoss: 388.9685\n",
      "Training Epoch: 15 [29696/49669]\tLoss: 417.1765\n",
      "Training Epoch: 15 [29760/49669]\tLoss: 392.7917\n",
      "Training Epoch: 15 [29824/49669]\tLoss: 417.8591\n",
      "Training Epoch: 15 [29888/49669]\tLoss: 411.3860\n",
      "Training Epoch: 15 [29952/49669]\tLoss: 406.4854\n",
      "Training Epoch: 15 [30016/49669]\tLoss: 402.0933\n",
      "Training Epoch: 15 [30080/49669]\tLoss: 409.5908\n",
      "Training Epoch: 15 [30144/49669]\tLoss: 398.7561\n",
      "Training Epoch: 15 [30208/49669]\tLoss: 417.7126\n",
      "Training Epoch: 15 [30272/49669]\tLoss: 390.5173\n",
      "Training Epoch: 15 [30336/49669]\tLoss: 413.9980\n",
      "Training Epoch: 15 [30400/49669]\tLoss: 400.6787\n",
      "Training Epoch: 15 [30464/49669]\tLoss: 410.7845\n",
      "Training Epoch: 15 [30528/49669]\tLoss: 386.7422\n",
      "Training Epoch: 15 [30592/49669]\tLoss: 421.5868\n",
      "Training Epoch: 15 [30656/49669]\tLoss: 403.5960\n",
      "Training Epoch: 15 [30720/49669]\tLoss: 405.7075\n",
      "Training Epoch: 15 [30784/49669]\tLoss: 456.8795\n",
      "Training Epoch: 15 [30848/49669]\tLoss: 377.0562\n",
      "Training Epoch: 15 [30912/49669]\tLoss: 411.8203\n",
      "Training Epoch: 15 [30976/49669]\tLoss: 371.4247\n",
      "Training Epoch: 15 [31040/49669]\tLoss: 415.1177\n",
      "Training Epoch: 15 [31104/49669]\tLoss: 421.6133\n",
      "Training Epoch: 15 [31168/49669]\tLoss: 423.9100\n",
      "Training Epoch: 15 [31232/49669]\tLoss: 429.7352\n",
      "Training Epoch: 15 [31296/49669]\tLoss: 422.6707\n",
      "Training Epoch: 15 [31360/49669]\tLoss: 421.1688\n",
      "Training Epoch: 15 [31424/49669]\tLoss: 413.6202\n",
      "Training Epoch: 15 [31488/49669]\tLoss: 404.6637\n",
      "Training Epoch: 15 [31552/49669]\tLoss: 428.7690\n",
      "Training Epoch: 15 [31616/49669]\tLoss: 380.0136\n",
      "Training Epoch: 15 [31680/49669]\tLoss: 411.1167\n",
      "Training Epoch: 15 [31744/49669]\tLoss: 427.8513\n",
      "Training Epoch: 15 [31808/49669]\tLoss: 407.5365\n",
      "Training Epoch: 15 [31872/49669]\tLoss: 435.2867\n",
      "Training Epoch: 15 [31936/49669]\tLoss: 415.4305\n",
      "Training Epoch: 15 [32000/49669]\tLoss: 423.4521\n",
      "Training Epoch: 15 [32064/49669]\tLoss: 439.3181\n",
      "Training Epoch: 15 [32128/49669]\tLoss: 375.0999\n",
      "Training Epoch: 15 [32192/49669]\tLoss: 421.8749\n",
      "Training Epoch: 15 [32256/49669]\tLoss: 421.4654\n",
      "Training Epoch: 15 [32320/49669]\tLoss: 409.1467\n",
      "Training Epoch: 15 [32384/49669]\tLoss: 420.5007\n",
      "Training Epoch: 15 [32448/49669]\tLoss: 420.1910\n",
      "Training Epoch: 15 [32512/49669]\tLoss: 414.7855\n",
      "Training Epoch: 15 [32576/49669]\tLoss: 414.8847\n",
      "Training Epoch: 15 [32640/49669]\tLoss: 411.6661\n",
      "Training Epoch: 15 [32704/49669]\tLoss: 383.8442\n",
      "Training Epoch: 15 [32768/49669]\tLoss: 415.7025\n",
      "Training Epoch: 15 [32832/49669]\tLoss: 394.7335\n",
      "Training Epoch: 15 [32896/49669]\tLoss: 402.9909\n",
      "Training Epoch: 15 [32960/49669]\tLoss: 396.7284\n",
      "Training Epoch: 15 [33024/49669]\tLoss: 436.5463\n",
      "Training Epoch: 15 [33088/49669]\tLoss: 398.6601\n",
      "Training Epoch: 15 [33152/49669]\tLoss: 393.8834\n",
      "Training Epoch: 15 [33216/49669]\tLoss: 418.9424\n",
      "Training Epoch: 15 [33280/49669]\tLoss: 392.2528\n",
      "Training Epoch: 15 [33344/49669]\tLoss: 397.5945\n",
      "Training Epoch: 15 [33408/49669]\tLoss: 415.7297\n",
      "Training Epoch: 15 [33472/49669]\tLoss: 399.5928\n",
      "Training Epoch: 15 [33536/49669]\tLoss: 389.6579\n",
      "Training Epoch: 15 [33600/49669]\tLoss: 413.4686\n",
      "Training Epoch: 15 [33664/49669]\tLoss: 407.5533\n",
      "Training Epoch: 15 [33728/49669]\tLoss: 459.4416\n",
      "Training Epoch: 15 [33792/49669]\tLoss: 414.9982\n",
      "Training Epoch: 15 [33856/49669]\tLoss: 414.8356\n",
      "Training Epoch: 15 [33920/49669]\tLoss: 407.4824\n",
      "Training Epoch: 15 [33984/49669]\tLoss: 417.9980\n",
      "Training Epoch: 15 [34048/49669]\tLoss: 413.0525\n",
      "Training Epoch: 15 [34112/49669]\tLoss: 434.4992\n",
      "Training Epoch: 15 [34176/49669]\tLoss: 414.8567\n",
      "Training Epoch: 15 [34240/49669]\tLoss: 403.7800\n",
      "Training Epoch: 15 [34304/49669]\tLoss: 439.1591\n",
      "Training Epoch: 15 [34368/49669]\tLoss: 389.0196\n",
      "Training Epoch: 15 [34432/49669]\tLoss: 393.2477\n",
      "Training Epoch: 15 [34496/49669]\tLoss: 419.0457\n",
      "Training Epoch: 15 [34560/49669]\tLoss: 375.2581\n",
      "Training Epoch: 15 [34624/49669]\tLoss: 407.6215\n",
      "Training Epoch: 15 [34688/49669]\tLoss: 412.4771\n",
      "Training Epoch: 15 [34752/49669]\tLoss: 386.0303\n",
      "Training Epoch: 15 [34816/49669]\tLoss: 406.1790\n",
      "Training Epoch: 15 [34880/49669]\tLoss: 395.4726\n",
      "Training Epoch: 15 [34944/49669]\tLoss: 390.5445\n",
      "Training Epoch: 15 [35008/49669]\tLoss: 415.8127\n",
      "Training Epoch: 15 [35072/49669]\tLoss: 386.2336\n",
      "Training Epoch: 15 [35136/49669]\tLoss: 403.7695\n",
      "Training Epoch: 15 [35200/49669]\tLoss: 428.6452\n",
      "Training Epoch: 15 [35264/49669]\tLoss: 413.2626\n",
      "Training Epoch: 15 [35328/49669]\tLoss: 403.1045\n",
      "Training Epoch: 15 [35392/49669]\tLoss: 406.8573\n",
      "Training Epoch: 15 [35456/49669]\tLoss: 402.1059\n",
      "Training Epoch: 15 [35520/49669]\tLoss: 388.8329\n",
      "Training Epoch: 15 [35584/49669]\tLoss: 378.9281\n",
      "Training Epoch: 15 [35648/49669]\tLoss: 410.7883\n",
      "Training Epoch: 15 [35712/49669]\tLoss: 427.3519\n",
      "Training Epoch: 15 [35776/49669]\tLoss: 412.6945\n",
      "Training Epoch: 15 [35840/49669]\tLoss: 410.0360\n",
      "Training Epoch: 15 [35904/49669]\tLoss: 404.3371\n",
      "Training Epoch: 15 [35968/49669]\tLoss: 381.7390\n",
      "Training Epoch: 15 [36032/49669]\tLoss: 418.9523\n",
      "Training Epoch: 15 [36096/49669]\tLoss: 430.8842\n",
      "Training Epoch: 15 [36160/49669]\tLoss: 402.0817\n",
      "Training Epoch: 15 [36224/49669]\tLoss: 413.3924\n",
      "Training Epoch: 15 [36288/49669]\tLoss: 400.5399\n",
      "Training Epoch: 15 [36352/49669]\tLoss: 423.1566\n",
      "Training Epoch: 15 [36416/49669]\tLoss: 430.9225\n",
      "Training Epoch: 15 [36480/49669]\tLoss: 404.4344\n",
      "Training Epoch: 15 [36544/49669]\tLoss: 433.9160\n",
      "Training Epoch: 15 [36608/49669]\tLoss: 378.9344\n",
      "Training Epoch: 15 [36672/49669]\tLoss: 396.8711\n",
      "Training Epoch: 15 [36736/49669]\tLoss: 426.1014\n",
      "Training Epoch: 15 [36800/49669]\tLoss: 437.3030\n",
      "Training Epoch: 15 [36864/49669]\tLoss: 410.2040\n",
      "Training Epoch: 15 [36928/49669]\tLoss: 414.0687\n",
      "Training Epoch: 15 [36992/49669]\tLoss: 408.6936\n",
      "Training Epoch: 15 [37056/49669]\tLoss: 400.7726\n",
      "Training Epoch: 15 [37120/49669]\tLoss: 401.7064\n",
      "Training Epoch: 15 [37184/49669]\tLoss: 401.5502\n",
      "Training Epoch: 15 [37248/49669]\tLoss: 406.1754\n",
      "Training Epoch: 15 [37312/49669]\tLoss: 398.8187\n",
      "Training Epoch: 15 [37376/49669]\tLoss: 403.1545\n",
      "Training Epoch: 15 [37440/49669]\tLoss: 390.3437\n",
      "Training Epoch: 15 [37504/49669]\tLoss: 401.9015\n",
      "Training Epoch: 15 [37568/49669]\tLoss: 392.3104\n",
      "Training Epoch: 15 [37632/49669]\tLoss: 442.5017\n",
      "Training Epoch: 15 [37696/49669]\tLoss: 407.0343\n",
      "Training Epoch: 15 [37760/49669]\tLoss: 387.3589\n",
      "Training Epoch: 15 [37824/49669]\tLoss: 397.6831\n",
      "Training Epoch: 15 [37888/49669]\tLoss: 377.1003\n",
      "Training Epoch: 15 [37952/49669]\tLoss: 399.2401\n",
      "Training Epoch: 15 [38016/49669]\tLoss: 430.6123\n",
      "Training Epoch: 15 [38080/49669]\tLoss: 382.8853\n",
      "Training Epoch: 15 [38144/49669]\tLoss: 404.0259\n",
      "Training Epoch: 15 [38208/49669]\tLoss: 382.9897\n",
      "Training Epoch: 15 [38272/49669]\tLoss: 390.2193\n",
      "Training Epoch: 15 [38336/49669]\tLoss: 409.3728\n",
      "Training Epoch: 15 [38400/49669]\tLoss: 409.1645\n",
      "Training Epoch: 15 [38464/49669]\tLoss: 409.5742\n",
      "Training Epoch: 15 [38528/49669]\tLoss: 405.7387\n",
      "Training Epoch: 15 [38592/49669]\tLoss: 393.4817\n",
      "Training Epoch: 15 [38656/49669]\tLoss: 416.3595\n",
      "Training Epoch: 15 [38720/49669]\tLoss: 398.9085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [38784/49669]\tLoss: 391.9317\n",
      "Training Epoch: 15 [38848/49669]\tLoss: 406.6346\n",
      "Training Epoch: 15 [38912/49669]\tLoss: 435.0182\n",
      "Training Epoch: 15 [38976/49669]\tLoss: 426.3036\n",
      "Training Epoch: 15 [39040/49669]\tLoss: 419.6279\n",
      "Training Epoch: 15 [39104/49669]\tLoss: 426.5808\n",
      "Training Epoch: 15 [39168/49669]\tLoss: 440.4309\n",
      "Training Epoch: 15 [39232/49669]\tLoss: 400.0721\n",
      "Training Epoch: 15 [39296/49669]\tLoss: 413.9311\n",
      "Training Epoch: 15 [39360/49669]\tLoss: 410.6848\n",
      "Training Epoch: 15 [39424/49669]\tLoss: 397.4199\n",
      "Training Epoch: 15 [39488/49669]\tLoss: 419.5336\n",
      "Training Epoch: 15 [39552/49669]\tLoss: 401.5965\n",
      "Training Epoch: 15 [39616/49669]\tLoss: 385.1702\n",
      "Training Epoch: 15 [39680/49669]\tLoss: 399.6463\n",
      "Training Epoch: 15 [39744/49669]\tLoss: 417.3145\n",
      "Training Epoch: 15 [39808/49669]\tLoss: 421.4831\n",
      "Training Epoch: 15 [39872/49669]\tLoss: 401.4091\n",
      "Training Epoch: 15 [39936/49669]\tLoss: 383.9939\n",
      "Training Epoch: 15 [40000/49669]\tLoss: 413.9776\n",
      "Training Epoch: 15 [40064/49669]\tLoss: 396.0364\n",
      "Training Epoch: 15 [40128/49669]\tLoss: 423.0270\n",
      "Training Epoch: 15 [40192/49669]\tLoss: 412.1492\n",
      "Training Epoch: 15 [40256/49669]\tLoss: 415.9899\n",
      "Training Epoch: 15 [40320/49669]\tLoss: 392.2294\n",
      "Training Epoch: 15 [40384/49669]\tLoss: 410.6680\n",
      "Training Epoch: 15 [40448/49669]\tLoss: 421.1226\n",
      "Training Epoch: 15 [40512/49669]\tLoss: 403.7559\n",
      "Training Epoch: 15 [40576/49669]\tLoss: 388.2823\n",
      "Training Epoch: 15 [40640/49669]\tLoss: 410.6594\n",
      "Training Epoch: 15 [40704/49669]\tLoss: 399.7063\n",
      "Training Epoch: 15 [40768/49669]\tLoss: 407.0302\n",
      "Training Epoch: 15 [40832/49669]\tLoss: 433.1693\n",
      "Training Epoch: 15 [40896/49669]\tLoss: 418.7212\n",
      "Training Epoch: 15 [40960/49669]\tLoss: 402.2157\n",
      "Training Epoch: 15 [41024/49669]\tLoss: 406.7610\n",
      "Training Epoch: 15 [41088/49669]\tLoss: 445.5202\n",
      "Training Epoch: 15 [41152/49669]\tLoss: 412.0689\n",
      "Training Epoch: 15 [41216/49669]\tLoss: 406.3837\n",
      "Training Epoch: 15 [41280/49669]\tLoss: 397.5768\n",
      "Training Epoch: 15 [41344/49669]\tLoss: 392.9089\n",
      "Training Epoch: 15 [41408/49669]\tLoss: 373.5053\n",
      "Training Epoch: 15 [41472/49669]\tLoss: 446.5566\n",
      "Training Epoch: 15 [41536/49669]\tLoss: 401.3677\n",
      "Training Epoch: 15 [41600/49669]\tLoss: 408.6125\n",
      "Training Epoch: 15 [41664/49669]\tLoss: 401.7324\n",
      "Training Epoch: 15 [41728/49669]\tLoss: 411.3699\n",
      "Training Epoch: 15 [41792/49669]\tLoss: 404.1638\n",
      "Training Epoch: 15 [41856/49669]\tLoss: 395.6246\n",
      "Training Epoch: 15 [41920/49669]\tLoss: 389.8944\n",
      "Training Epoch: 15 [41984/49669]\tLoss: 425.3384\n",
      "Training Epoch: 15 [42048/49669]\tLoss: 390.4647\n",
      "Training Epoch: 15 [42112/49669]\tLoss: 412.2497\n",
      "Training Epoch: 15 [42176/49669]\tLoss: 380.9171\n",
      "Training Epoch: 15 [42240/49669]\tLoss: 423.3196\n",
      "Training Epoch: 15 [42304/49669]\tLoss: 403.2862\n",
      "Training Epoch: 15 [42368/49669]\tLoss: 382.4880\n",
      "Training Epoch: 15 [42432/49669]\tLoss: 390.8363\n",
      "Training Epoch: 15 [42496/49669]\tLoss: 391.4905\n",
      "Training Epoch: 15 [42560/49669]\tLoss: 404.3201\n",
      "Training Epoch: 15 [42624/49669]\tLoss: 375.8053\n",
      "Training Epoch: 15 [42688/49669]\tLoss: 415.1791\n",
      "Training Epoch: 15 [42752/49669]\tLoss: 397.3640\n",
      "Training Epoch: 15 [42816/49669]\tLoss: 418.7937\n",
      "Training Epoch: 15 [42880/49669]\tLoss: 387.7084\n",
      "Training Epoch: 15 [42944/49669]\tLoss: 430.1399\n",
      "Training Epoch: 15 [43008/49669]\tLoss: 385.0363\n",
      "Training Epoch: 15 [43072/49669]\tLoss: 389.5179\n",
      "Training Epoch: 15 [43136/49669]\tLoss: 425.8591\n",
      "Training Epoch: 15 [43200/49669]\tLoss: 404.1775\n",
      "Training Epoch: 15 [43264/49669]\tLoss: 449.8836\n",
      "Training Epoch: 15 [43328/49669]\tLoss: 394.6796\n",
      "Training Epoch: 15 [43392/49669]\tLoss: 431.6845\n",
      "Training Epoch: 15 [43456/49669]\tLoss: 401.1156\n",
      "Training Epoch: 15 [43520/49669]\tLoss: 402.2817\n",
      "Training Epoch: 15 [43584/49669]\tLoss: 405.6992\n",
      "Training Epoch: 15 [43648/49669]\tLoss: 416.0411\n",
      "Training Epoch: 15 [43712/49669]\tLoss: 388.3137\n",
      "Training Epoch: 15 [43776/49669]\tLoss: 406.3398\n",
      "Training Epoch: 15 [43840/49669]\tLoss: 385.5288\n",
      "Training Epoch: 15 [43904/49669]\tLoss: 413.4697\n",
      "Training Epoch: 15 [43968/49669]\tLoss: 385.5845\n",
      "Training Epoch: 15 [44032/49669]\tLoss: 390.1833\n",
      "Training Epoch: 15 [44096/49669]\tLoss: 383.9434\n",
      "Training Epoch: 15 [44160/49669]\tLoss: 423.8737\n",
      "Training Epoch: 15 [44224/49669]\tLoss: 447.6794\n",
      "Training Epoch: 15 [44288/49669]\tLoss: 438.8057\n",
      "Training Epoch: 15 [44352/49669]\tLoss: 394.6673\n",
      "Training Epoch: 15 [44416/49669]\tLoss: 385.6260\n",
      "Training Epoch: 15 [44480/49669]\tLoss: 403.4043\n",
      "Training Epoch: 15 [44544/49669]\tLoss: 444.6907\n",
      "Training Epoch: 15 [44608/49669]\tLoss: 407.8160\n",
      "Training Epoch: 15 [44672/49669]\tLoss: 430.7074\n",
      "Training Epoch: 15 [44736/49669]\tLoss: 442.1773\n",
      "Training Epoch: 15 [44800/49669]\tLoss: 433.0691\n",
      "Training Epoch: 15 [44864/49669]\tLoss: 410.7219\n",
      "Training Epoch: 15 [44928/49669]\tLoss: 434.3187\n",
      "Training Epoch: 15 [44992/49669]\tLoss: 427.5460\n",
      "Training Epoch: 15 [45056/49669]\tLoss: 442.9467\n",
      "Training Epoch: 15 [45120/49669]\tLoss: 423.3931\n",
      "Training Epoch: 15 [45184/49669]\tLoss: 408.2933\n",
      "Training Epoch: 15 [45248/49669]\tLoss: 408.9848\n",
      "Training Epoch: 15 [45312/49669]\tLoss: 445.5381\n",
      "Training Epoch: 15 [45376/49669]\tLoss: 390.7611\n",
      "Training Epoch: 15 [45440/49669]\tLoss: 450.8533\n",
      "Training Epoch: 15 [45504/49669]\tLoss: 408.5879\n",
      "Training Epoch: 15 [45568/49669]\tLoss: 440.2748\n",
      "Training Epoch: 15 [45632/49669]\tLoss: 393.1015\n",
      "Training Epoch: 15 [45696/49669]\tLoss: 436.1390\n",
      "Training Epoch: 15 [45760/49669]\tLoss: 403.3355\n",
      "Training Epoch: 15 [45824/49669]\tLoss: 420.5944\n",
      "Training Epoch: 15 [45888/49669]\tLoss: 404.5532\n",
      "Training Epoch: 15 [45952/49669]\tLoss: 444.0694\n",
      "Training Epoch: 15 [46016/49669]\tLoss: 410.3008\n",
      "Training Epoch: 15 [46080/49669]\tLoss: 425.4395\n",
      "Training Epoch: 15 [46144/49669]\tLoss: 451.3772\n",
      "Training Epoch: 15 [46208/49669]\tLoss: 427.9051\n",
      "Training Epoch: 15 [46272/49669]\tLoss: 412.1618\n",
      "Training Epoch: 15 [46336/49669]\tLoss: 406.5526\n",
      "Training Epoch: 15 [46400/49669]\tLoss: 441.0734\n",
      "Training Epoch: 15 [46464/49669]\tLoss: 420.3630\n",
      "Training Epoch: 15 [46528/49669]\tLoss: 395.5220\n",
      "Training Epoch: 15 [46592/49669]\tLoss: 405.0873\n",
      "Training Epoch: 15 [46656/49669]\tLoss: 419.2731\n",
      "Training Epoch: 15 [46720/49669]\tLoss: 422.4961\n",
      "Training Epoch: 15 [46784/49669]\tLoss: 455.1520\n",
      "Training Epoch: 15 [46848/49669]\tLoss: 401.5235\n",
      "Training Epoch: 15 [46912/49669]\tLoss: 377.8347\n",
      "Training Epoch: 15 [46976/49669]\tLoss: 390.0615\n",
      "Training Epoch: 15 [47040/49669]\tLoss: 387.3797\n",
      "Training Epoch: 15 [47104/49669]\tLoss: 410.1829\n",
      "Training Epoch: 15 [47168/49669]\tLoss: 419.0920\n",
      "Training Epoch: 15 [47232/49669]\tLoss: 392.6389\n",
      "Training Epoch: 15 [47296/49669]\tLoss: 404.6834\n",
      "Training Epoch: 15 [47360/49669]\tLoss: 418.9261\n",
      "Training Epoch: 15 [47424/49669]\tLoss: 388.0680\n",
      "Training Epoch: 15 [47488/49669]\tLoss: 402.9583\n",
      "Training Epoch: 15 [47552/49669]\tLoss: 411.0382\n",
      "Training Epoch: 15 [47616/49669]\tLoss: 414.0740\n",
      "Training Epoch: 15 [47680/49669]\tLoss: 407.3633\n",
      "Training Epoch: 15 [47744/49669]\tLoss: 400.4630\n",
      "Training Epoch: 15 [47808/49669]\tLoss: 411.1266\n",
      "Training Epoch: 15 [47872/49669]\tLoss: 418.9576\n",
      "Training Epoch: 15 [47936/49669]\tLoss: 398.7543\n",
      "Training Epoch: 15 [48000/49669]\tLoss: 400.5193\n",
      "Training Epoch: 15 [48064/49669]\tLoss: 407.2883\n",
      "Training Epoch: 15 [48128/49669]\tLoss: 405.4575\n",
      "Training Epoch: 15 [48192/49669]\tLoss: 413.7015\n",
      "Training Epoch: 15 [48256/49669]\tLoss: 401.7512\n",
      "Training Epoch: 15 [48320/49669]\tLoss: 417.1900\n",
      "Training Epoch: 15 [48384/49669]\tLoss: 410.7666\n",
      "Training Epoch: 15 [48448/49669]\tLoss: 407.2122\n",
      "Training Epoch: 15 [48512/49669]\tLoss: 407.7150\n",
      "Training Epoch: 15 [48576/49669]\tLoss: 408.1190\n",
      "Training Epoch: 15 [48640/49669]\tLoss: 391.1716\n",
      "Training Epoch: 15 [48704/49669]\tLoss: 403.7697\n",
      "Training Epoch: 15 [48768/49669]\tLoss: 435.3150\n",
      "Training Epoch: 15 [48832/49669]\tLoss: 410.7168\n",
      "Training Epoch: 15 [48896/49669]\tLoss: 371.0783\n",
      "Training Epoch: 15 [48960/49669]\tLoss: 410.5208\n",
      "Training Epoch: 15 [49024/49669]\tLoss: 389.5909\n",
      "Training Epoch: 15 [49088/49669]\tLoss: 425.2083\n",
      "Training Epoch: 15 [49152/49669]\tLoss: 425.0274\n",
      "Training Epoch: 15 [49216/49669]\tLoss: 422.9587\n",
      "Training Epoch: 15 [49280/49669]\tLoss: 423.9007\n",
      "Training Epoch: 15 [49344/49669]\tLoss: 397.1363\n",
      "Training Epoch: 15 [49408/49669]\tLoss: 434.2896\n",
      "Training Epoch: 15 [49472/49669]\tLoss: 405.8808\n",
      "Training Epoch: 15 [49536/49669]\tLoss: 427.2705\n",
      "Training Epoch: 15 [49600/49669]\tLoss: 403.1619\n",
      "Training Epoch: 15 [49664/49669]\tLoss: 420.9483\n",
      "Training Epoch: 15 [49669/49669]\tLoss: 464.9377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [5519/5519]\tLoss: 412.1602\n",
      "Training Epoch: 16 [64/49669]\tLoss: 419.6480\n",
      "Training Epoch: 16 [128/49669]\tLoss: 387.1002\n",
      "Training Epoch: 16 [192/49669]\tLoss: 427.5350\n",
      "Training Epoch: 16 [256/49669]\tLoss: 399.1904\n",
      "Training Epoch: 16 [320/49669]\tLoss: 378.6918\n",
      "Training Epoch: 16 [384/49669]\tLoss: 391.8823\n",
      "Training Epoch: 16 [448/49669]\tLoss: 406.0368\n",
      "Training Epoch: 16 [512/49669]\tLoss: 432.4359\n",
      "Training Epoch: 16 [576/49669]\tLoss: 391.9428\n",
      "Training Epoch: 16 [640/49669]\tLoss: 411.0911\n",
      "Training Epoch: 16 [704/49669]\tLoss: 406.5185\n",
      "Training Epoch: 16 [768/49669]\tLoss: 430.6057\n",
      "Training Epoch: 16 [832/49669]\tLoss: 408.2564\n",
      "Training Epoch: 16 [896/49669]\tLoss: 412.7878\n",
      "Training Epoch: 16 [960/49669]\tLoss: 410.9952\n",
      "Training Epoch: 16 [1024/49669]\tLoss: 435.6733\n",
      "Training Epoch: 16 [1088/49669]\tLoss: 425.2464\n",
      "Training Epoch: 16 [1152/49669]\tLoss: 409.0002\n",
      "Training Epoch: 16 [1216/49669]\tLoss: 394.6918\n",
      "Training Epoch: 16 [1280/49669]\tLoss: 415.5970\n",
      "Training Epoch: 16 [1344/49669]\tLoss: 385.0258\n",
      "Training Epoch: 16 [1408/49669]\tLoss: 416.9600\n",
      "Training Epoch: 16 [1472/49669]\tLoss: 416.7202\n",
      "Training Epoch: 16 [1536/49669]\tLoss: 382.9538\n",
      "Training Epoch: 16 [1600/49669]\tLoss: 407.9024\n",
      "Training Epoch: 16 [1664/49669]\tLoss: 429.7099\n",
      "Training Epoch: 16 [1728/49669]\tLoss: 400.5852\n",
      "Training Epoch: 16 [1792/49669]\tLoss: 427.5483\n",
      "Training Epoch: 16 [1856/49669]\tLoss: 423.2294\n",
      "Training Epoch: 16 [1920/49669]\tLoss: 431.7789\n",
      "Training Epoch: 16 [1984/49669]\tLoss: 421.1594\n",
      "Training Epoch: 16 [2048/49669]\tLoss: 394.2325\n",
      "Training Epoch: 16 [2112/49669]\tLoss: 376.1291\n",
      "Training Epoch: 16 [2176/49669]\tLoss: 432.4178\n",
      "Training Epoch: 16 [2240/49669]\tLoss: 422.9253\n",
      "Training Epoch: 16 [2304/49669]\tLoss: 435.5269\n",
      "Training Epoch: 16 [2368/49669]\tLoss: 419.0435\n",
      "Training Epoch: 16 [2432/49669]\tLoss: 452.5961\n",
      "Training Epoch: 16 [2496/49669]\tLoss: 419.9978\n",
      "Training Epoch: 16 [2560/49669]\tLoss: 394.4885\n",
      "Training Epoch: 16 [2624/49669]\tLoss: 427.1836\n",
      "Training Epoch: 16 [2688/49669]\tLoss: 410.3516\n",
      "Training Epoch: 16 [2752/49669]\tLoss: 400.8736\n",
      "Training Epoch: 16 [2816/49669]\tLoss: 392.1669\n",
      "Training Epoch: 16 [2880/49669]\tLoss: 397.9777\n",
      "Training Epoch: 16 [2944/49669]\tLoss: 418.5605\n",
      "Training Epoch: 16 [3008/49669]\tLoss: 423.8165\n",
      "Training Epoch: 16 [3072/49669]\tLoss: 420.6011\n",
      "Training Epoch: 16 [3136/49669]\tLoss: 393.3095\n",
      "Training Epoch: 16 [3200/49669]\tLoss: 400.1738\n",
      "Training Epoch: 16 [3264/49669]\tLoss: 382.6971\n",
      "Training Epoch: 16 [3328/49669]\tLoss: 366.6295\n",
      "Training Epoch: 16 [3392/49669]\tLoss: 426.0540\n",
      "Training Epoch: 16 [3456/49669]\tLoss: 404.1004\n",
      "Training Epoch: 16 [3520/49669]\tLoss: 414.7888\n",
      "Training Epoch: 16 [3584/49669]\tLoss: 399.5034\n",
      "Training Epoch: 16 [3648/49669]\tLoss: 420.9195\n",
      "Training Epoch: 16 [3712/49669]\tLoss: 399.3092\n",
      "Training Epoch: 16 [3776/49669]\tLoss: 412.5135\n",
      "Training Epoch: 16 [3840/49669]\tLoss: 373.8503\n",
      "Training Epoch: 16 [3904/49669]\tLoss: 405.3870\n",
      "Training Epoch: 16 [3968/49669]\tLoss: 405.0559\n",
      "Training Epoch: 16 [4032/49669]\tLoss: 421.2903\n",
      "Training Epoch: 16 [4096/49669]\tLoss: 408.2299\n",
      "Training Epoch: 16 [4160/49669]\tLoss: 428.2701\n",
      "Training Epoch: 16 [4224/49669]\tLoss: 419.7549\n",
      "Training Epoch: 16 [4288/49669]\tLoss: 383.6669\n",
      "Training Epoch: 16 [4352/49669]\tLoss: 422.0748\n",
      "Training Epoch: 16 [4416/49669]\tLoss: 420.1725\n",
      "Training Epoch: 16 [4480/49669]\tLoss: 414.7224\n",
      "Training Epoch: 16 [4544/49669]\tLoss: 402.7744\n",
      "Training Epoch: 16 [4608/49669]\tLoss: 417.3150\n",
      "Training Epoch: 16 [4672/49669]\tLoss: 428.4977\n",
      "Training Epoch: 16 [4736/49669]\tLoss: 416.5226\n",
      "Training Epoch: 16 [4800/49669]\tLoss: 412.9019\n",
      "Training Epoch: 16 [4864/49669]\tLoss: 416.7682\n",
      "Training Epoch: 16 [4928/49669]\tLoss: 379.7882\n",
      "Training Epoch: 16 [4992/49669]\tLoss: 404.2017\n",
      "Training Epoch: 16 [5056/49669]\tLoss: 419.5617\n",
      "Training Epoch: 16 [5120/49669]\tLoss: 424.3344\n",
      "Training Epoch: 16 [5184/49669]\tLoss: 410.7623\n",
      "Training Epoch: 16 [5248/49669]\tLoss: 388.1256\n",
      "Training Epoch: 16 [5312/49669]\tLoss: 410.6808\n",
      "Training Epoch: 16 [5376/49669]\tLoss: 405.7329\n",
      "Training Epoch: 16 [5440/49669]\tLoss: 394.9786\n",
      "Training Epoch: 16 [5504/49669]\tLoss: 418.1305\n",
      "Training Epoch: 16 [5568/49669]\tLoss: 393.8910\n",
      "Training Epoch: 16 [5632/49669]\tLoss: 391.7828\n",
      "Training Epoch: 16 [5696/49669]\tLoss: 382.9116\n",
      "Training Epoch: 16 [5760/49669]\tLoss: 392.4866\n",
      "Training Epoch: 16 [5824/49669]\tLoss: 427.9631\n",
      "Training Epoch: 16 [5888/49669]\tLoss: 383.1169\n",
      "Training Epoch: 16 [5952/49669]\tLoss: 398.3678\n",
      "Training Epoch: 16 [6016/49669]\tLoss: 420.8631\n",
      "Training Epoch: 16 [6080/49669]\tLoss: 409.7892\n",
      "Training Epoch: 16 [6144/49669]\tLoss: 412.5985\n",
      "Training Epoch: 16 [6208/49669]\tLoss: 418.6291\n",
      "Training Epoch: 16 [6272/49669]\tLoss: 403.0395\n",
      "Training Epoch: 16 [6336/49669]\tLoss: 420.0941\n",
      "Training Epoch: 16 [6400/49669]\tLoss: 399.2636\n",
      "Training Epoch: 16 [6464/49669]\tLoss: 415.5654\n",
      "Training Epoch: 16 [6528/49669]\tLoss: 433.5264\n",
      "Training Epoch: 16 [6592/49669]\tLoss: 397.8539\n",
      "Training Epoch: 16 [6656/49669]\tLoss: 441.0215\n",
      "Training Epoch: 16 [6720/49669]\tLoss: 421.8385\n",
      "Training Epoch: 16 [6784/49669]\tLoss: 424.8077\n",
      "Training Epoch: 16 [6848/49669]\tLoss: 422.4135\n",
      "Training Epoch: 16 [6912/49669]\tLoss: 449.0072\n",
      "Training Epoch: 16 [6976/49669]\tLoss: 431.3569\n",
      "Training Epoch: 16 [7040/49669]\tLoss: 443.1651\n",
      "Training Epoch: 16 [7104/49669]\tLoss: 469.5747\n",
      "Training Epoch: 16 [7168/49669]\tLoss: 471.9672\n",
      "Training Epoch: 16 [7232/49669]\tLoss: 483.2905\n",
      "Training Epoch: 16 [7296/49669]\tLoss: 486.5403\n",
      "Training Epoch: 16 [7360/49669]\tLoss: 459.7891\n",
      "Training Epoch: 16 [7424/49669]\tLoss: 433.7416\n",
      "Training Epoch: 16 [7488/49669]\tLoss: 435.9907\n",
      "Training Epoch: 16 [7552/49669]\tLoss: 405.3466\n",
      "Training Epoch: 16 [7616/49669]\tLoss: 419.0344\n",
      "Training Epoch: 16 [7680/49669]\tLoss: 435.4379\n",
      "Training Epoch: 16 [7744/49669]\tLoss: 430.9395\n",
      "Training Epoch: 16 [7808/49669]\tLoss: 431.1530\n",
      "Training Epoch: 16 [7872/49669]\tLoss: 416.8608\n",
      "Training Epoch: 16 [7936/49669]\tLoss: 414.7682\n",
      "Training Epoch: 16 [8000/49669]\tLoss: 378.9058\n",
      "Training Epoch: 16 [8064/49669]\tLoss: 419.1403\n",
      "Training Epoch: 16 [8128/49669]\tLoss: 359.1220\n",
      "Training Epoch: 16 [8192/49669]\tLoss: 399.7170\n",
      "Training Epoch: 16 [8256/49669]\tLoss: 405.0061\n",
      "Training Epoch: 16 [8320/49669]\tLoss: 424.3382\n",
      "Training Epoch: 16 [8384/49669]\tLoss: 409.5150\n",
      "Training Epoch: 16 [8448/49669]\tLoss: 399.9630\n",
      "Training Epoch: 16 [8512/49669]\tLoss: 402.1270\n",
      "Training Epoch: 16 [8576/49669]\tLoss: 408.6369\n",
      "Training Epoch: 16 [8640/49669]\tLoss: 405.6385\n",
      "Training Epoch: 16 [8704/49669]\tLoss: 384.6243\n",
      "Training Epoch: 16 [8768/49669]\tLoss: 416.1559\n",
      "Training Epoch: 16 [8832/49669]\tLoss: 385.6804\n",
      "Training Epoch: 16 [8896/49669]\tLoss: 415.4112\n",
      "Training Epoch: 16 [8960/49669]\tLoss: 413.7438\n",
      "Training Epoch: 16 [9024/49669]\tLoss: 395.4743\n",
      "Training Epoch: 16 [9088/49669]\tLoss: 390.5237\n",
      "Training Epoch: 16 [9152/49669]\tLoss: 386.0464\n",
      "Training Epoch: 16 [9216/49669]\tLoss: 415.0804\n",
      "Training Epoch: 16 [9280/49669]\tLoss: 390.3300\n",
      "Training Epoch: 16 [9344/49669]\tLoss: 420.5987\n",
      "Training Epoch: 16 [9408/49669]\tLoss: 395.3708\n",
      "Training Epoch: 16 [9472/49669]\tLoss: 398.8403\n",
      "Training Epoch: 16 [9536/49669]\tLoss: 427.5685\n",
      "Training Epoch: 16 [9600/49669]\tLoss: 399.8678\n",
      "Training Epoch: 16 [9664/49669]\tLoss: 405.9549\n",
      "Training Epoch: 16 [9728/49669]\tLoss: 429.8100\n",
      "Training Epoch: 16 [9792/49669]\tLoss: 407.1732\n",
      "Training Epoch: 16 [9856/49669]\tLoss: 402.1021\n",
      "Training Epoch: 16 [9920/49669]\tLoss: 385.6772\n",
      "Training Epoch: 16 [9984/49669]\tLoss: 396.3286\n",
      "Training Epoch: 16 [10048/49669]\tLoss: 376.4521\n",
      "Training Epoch: 16 [10112/49669]\tLoss: 407.1246\n",
      "Training Epoch: 16 [10176/49669]\tLoss: 399.4369\n",
      "Training Epoch: 16 [10240/49669]\tLoss: 406.0700\n",
      "Training Epoch: 16 [10304/49669]\tLoss: 428.4720\n",
      "Training Epoch: 16 [10368/49669]\tLoss: 436.1027\n",
      "Training Epoch: 16 [10432/49669]\tLoss: 408.8338\n",
      "Training Epoch: 16 [10496/49669]\tLoss: 409.7064\n",
      "Training Epoch: 16 [10560/49669]\tLoss: 410.7818\n",
      "Training Epoch: 16 [10624/49669]\tLoss: 407.6237\n",
      "Training Epoch: 16 [10688/49669]\tLoss: 446.9574\n",
      "Training Epoch: 16 [10752/49669]\tLoss: 405.1496\n",
      "Training Epoch: 16 [10816/49669]\tLoss: 403.4700\n",
      "Training Epoch: 16 [10880/49669]\tLoss: 411.1667\n",
      "Training Epoch: 16 [10944/49669]\tLoss: 422.6780\n",
      "Training Epoch: 16 [11008/49669]\tLoss: 416.3522\n",
      "Training Epoch: 16 [11072/49669]\tLoss: 415.5756\n",
      "Training Epoch: 16 [11136/49669]\tLoss: 403.0365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [11200/49669]\tLoss: 429.4694\n",
      "Training Epoch: 16 [11264/49669]\tLoss: 383.3905\n",
      "Training Epoch: 16 [11328/49669]\tLoss: 393.6865\n",
      "Training Epoch: 16 [11392/49669]\tLoss: 405.3631\n",
      "Training Epoch: 16 [11456/49669]\tLoss: 399.7801\n",
      "Training Epoch: 16 [11520/49669]\tLoss: 400.3646\n",
      "Training Epoch: 16 [11584/49669]\tLoss: 403.1425\n",
      "Training Epoch: 16 [11648/49669]\tLoss: 398.9718\n",
      "Training Epoch: 16 [11712/49669]\tLoss: 400.2523\n",
      "Training Epoch: 16 [11776/49669]\tLoss: 409.3629\n",
      "Training Epoch: 16 [11840/49669]\tLoss: 405.7240\n",
      "Training Epoch: 16 [11904/49669]\tLoss: 404.8686\n",
      "Training Epoch: 16 [11968/49669]\tLoss: 404.6099\n",
      "Training Epoch: 16 [12032/49669]\tLoss: 397.8456\n",
      "Training Epoch: 16 [12096/49669]\tLoss: 400.4142\n",
      "Training Epoch: 16 [12160/49669]\tLoss: 385.9662\n",
      "Training Epoch: 16 [12224/49669]\tLoss: 419.5334\n",
      "Training Epoch: 16 [12288/49669]\tLoss: 395.7253\n",
      "Training Epoch: 16 [12352/49669]\tLoss: 403.8384\n",
      "Training Epoch: 16 [12416/49669]\tLoss: 433.1803\n",
      "Training Epoch: 16 [12480/49669]\tLoss: 407.9764\n",
      "Training Epoch: 16 [12544/49669]\tLoss: 417.7440\n",
      "Training Epoch: 16 [12608/49669]\tLoss: 421.4010\n",
      "Training Epoch: 16 [12672/49669]\tLoss: 417.2138\n",
      "Training Epoch: 16 [12736/49669]\tLoss: 434.4764\n",
      "Training Epoch: 16 [12800/49669]\tLoss: 385.8753\n",
      "Training Epoch: 16 [12864/49669]\tLoss: 414.0612\n",
      "Training Epoch: 16 [12928/49669]\tLoss: 415.8139\n",
      "Training Epoch: 16 [12992/49669]\tLoss: 415.7685\n",
      "Training Epoch: 16 [13056/49669]\tLoss: 397.5066\n",
      "Training Epoch: 16 [13120/49669]\tLoss: 431.9284\n",
      "Training Epoch: 16 [13184/49669]\tLoss: 411.2048\n",
      "Training Epoch: 16 [13248/49669]\tLoss: 414.3347\n",
      "Training Epoch: 16 [13312/49669]\tLoss: 417.8145\n",
      "Training Epoch: 16 [13376/49669]\tLoss: 415.0423\n",
      "Training Epoch: 16 [13440/49669]\tLoss: 432.5370\n",
      "Training Epoch: 16 [13504/49669]\tLoss: 405.8528\n",
      "Training Epoch: 16 [13568/49669]\tLoss: 394.0482\n",
      "Training Epoch: 16 [13632/49669]\tLoss: 384.5877\n",
      "Training Epoch: 16 [13696/49669]\tLoss: 400.8205\n",
      "Training Epoch: 16 [13760/49669]\tLoss: 417.0771\n",
      "Training Epoch: 16 [13824/49669]\tLoss: 417.3334\n",
      "Training Epoch: 16 [13888/49669]\tLoss: 411.5305\n",
      "Training Epoch: 16 [13952/49669]\tLoss: 417.6534\n",
      "Training Epoch: 16 [14016/49669]\tLoss: 422.5344\n",
      "Training Epoch: 16 [14080/49669]\tLoss: 409.2870\n",
      "Training Epoch: 16 [14144/49669]\tLoss: 409.7897\n",
      "Training Epoch: 16 [14208/49669]\tLoss: 430.2584\n",
      "Training Epoch: 16 [14272/49669]\tLoss: 440.2265\n",
      "Training Epoch: 16 [14336/49669]\tLoss: 399.0086\n",
      "Training Epoch: 16 [14400/49669]\tLoss: 408.3465\n",
      "Training Epoch: 16 [14464/49669]\tLoss: 400.1557\n",
      "Training Epoch: 16 [14528/49669]\tLoss: 416.0927\n",
      "Training Epoch: 16 [14592/49669]\tLoss: 420.1118\n",
      "Training Epoch: 16 [14656/49669]\tLoss: 423.8395\n",
      "Training Epoch: 16 [14720/49669]\tLoss: 380.8514\n",
      "Training Epoch: 16 [14784/49669]\tLoss: 425.5765\n",
      "Training Epoch: 16 [14848/49669]\tLoss: 435.6833\n",
      "Training Epoch: 16 [14912/49669]\tLoss: 400.9741\n",
      "Training Epoch: 16 [14976/49669]\tLoss: 413.3030\n",
      "Training Epoch: 16 [15040/49669]\tLoss: 416.3555\n",
      "Training Epoch: 16 [15104/49669]\tLoss: 406.2884\n",
      "Training Epoch: 16 [15168/49669]\tLoss: 395.8174\n",
      "Training Epoch: 16 [15232/49669]\tLoss: 410.4682\n",
      "Training Epoch: 16 [15296/49669]\tLoss: 434.2743\n",
      "Training Epoch: 16 [15360/49669]\tLoss: 374.1639\n",
      "Training Epoch: 16 [15424/49669]\tLoss: 430.7053\n",
      "Training Epoch: 16 [15488/49669]\tLoss: 444.4394\n",
      "Training Epoch: 16 [15552/49669]\tLoss: 413.2531\n",
      "Training Epoch: 16 [15616/49669]\tLoss: 405.4451\n",
      "Training Epoch: 16 [15680/49669]\tLoss: 411.6342\n",
      "Training Epoch: 16 [15744/49669]\tLoss: 411.2380\n",
      "Training Epoch: 16 [15808/49669]\tLoss: 412.1019\n",
      "Training Epoch: 16 [15872/49669]\tLoss: 387.5153\n",
      "Training Epoch: 16 [15936/49669]\tLoss: 413.8905\n",
      "Training Epoch: 16 [16000/49669]\tLoss: 401.1423\n",
      "Training Epoch: 16 [16064/49669]\tLoss: 413.8983\n",
      "Training Epoch: 16 [16128/49669]\tLoss: 393.5269\n",
      "Training Epoch: 16 [16192/49669]\tLoss: 432.0990\n",
      "Training Epoch: 16 [16256/49669]\tLoss: 399.5148\n",
      "Training Epoch: 16 [16320/49669]\tLoss: 422.1420\n",
      "Training Epoch: 16 [16384/49669]\tLoss: 387.8884\n",
      "Training Epoch: 16 [16448/49669]\tLoss: 417.5462\n",
      "Training Epoch: 16 [16512/49669]\tLoss: 404.1659\n",
      "Training Epoch: 16 [16576/49669]\tLoss: 387.1295\n",
      "Training Epoch: 16 [16640/49669]\tLoss: 448.1890\n",
      "Training Epoch: 16 [16704/49669]\tLoss: 387.2804\n",
      "Training Epoch: 16 [16768/49669]\tLoss: 423.4916\n",
      "Training Epoch: 16 [16832/49669]\tLoss: 411.1962\n",
      "Training Epoch: 16 [16896/49669]\tLoss: 391.2911\n",
      "Training Epoch: 16 [16960/49669]\tLoss: 379.0704\n",
      "Training Epoch: 16 [17024/49669]\tLoss: 404.9368\n",
      "Training Epoch: 16 [17088/49669]\tLoss: 404.6288\n",
      "Training Epoch: 16 [17152/49669]\tLoss: 389.3027\n",
      "Training Epoch: 16 [17216/49669]\tLoss: 415.7885\n",
      "Training Epoch: 16 [17280/49669]\tLoss: 421.6712\n",
      "Training Epoch: 16 [17344/49669]\tLoss: 391.3743\n",
      "Training Epoch: 16 [17408/49669]\tLoss: 388.2224\n",
      "Training Epoch: 16 [17472/49669]\tLoss: 388.3352\n",
      "Training Epoch: 16 [17536/49669]\tLoss: 394.9201\n",
      "Training Epoch: 16 [17600/49669]\tLoss: 414.2230\n",
      "Training Epoch: 16 [17664/49669]\tLoss: 433.8036\n",
      "Training Epoch: 16 [17728/49669]\tLoss: 412.5349\n",
      "Training Epoch: 16 [17792/49669]\tLoss: 400.6366\n",
      "Training Epoch: 16 [17856/49669]\tLoss: 397.8963\n",
      "Training Epoch: 16 [17920/49669]\tLoss: 433.3420\n",
      "Training Epoch: 16 [17984/49669]\tLoss: 371.9213\n",
      "Training Epoch: 16 [18048/49669]\tLoss: 376.8843\n",
      "Training Epoch: 16 [18112/49669]\tLoss: 442.7419\n",
      "Training Epoch: 16 [18176/49669]\tLoss: 407.0999\n",
      "Training Epoch: 16 [18240/49669]\tLoss: 408.4314\n",
      "Training Epoch: 16 [18304/49669]\tLoss: 403.6313\n",
      "Training Epoch: 16 [18368/49669]\tLoss: 413.9597\n",
      "Training Epoch: 16 [18432/49669]\tLoss: 385.9827\n",
      "Training Epoch: 16 [18496/49669]\tLoss: 423.5535\n",
      "Training Epoch: 16 [18560/49669]\tLoss: 407.6824\n",
      "Training Epoch: 16 [18624/49669]\tLoss: 436.5836\n",
      "Training Epoch: 16 [18688/49669]\tLoss: 401.3965\n",
      "Training Epoch: 16 [18752/49669]\tLoss: 424.9031\n",
      "Training Epoch: 16 [18816/49669]\tLoss: 389.4435\n",
      "Training Epoch: 16 [18880/49669]\tLoss: 421.0720\n",
      "Training Epoch: 16 [18944/49669]\tLoss: 386.9996\n",
      "Training Epoch: 16 [19008/49669]\tLoss: 411.1515\n",
      "Training Epoch: 16 [19072/49669]\tLoss: 380.3063\n",
      "Training Epoch: 16 [19136/49669]\tLoss: 420.2791\n",
      "Training Epoch: 16 [19200/49669]\tLoss: 428.9697\n",
      "Training Epoch: 16 [19264/49669]\tLoss: 387.2282\n",
      "Training Epoch: 16 [19328/49669]\tLoss: 397.5327\n",
      "Training Epoch: 16 [19392/49669]\tLoss: 422.5488\n",
      "Training Epoch: 16 [19456/49669]\tLoss: 364.1879\n",
      "Training Epoch: 16 [19520/49669]\tLoss: 405.7462\n",
      "Training Epoch: 16 [19584/49669]\tLoss: 382.2175\n",
      "Training Epoch: 16 [19648/49669]\tLoss: 384.7341\n",
      "Training Epoch: 16 [19712/49669]\tLoss: 410.3328\n",
      "Training Epoch: 16 [19776/49669]\tLoss: 412.6036\n",
      "Training Epoch: 16 [19840/49669]\tLoss: 382.4143\n",
      "Training Epoch: 16 [19904/49669]\tLoss: 418.1478\n",
      "Training Epoch: 16 [19968/49669]\tLoss: 424.6644\n",
      "Training Epoch: 16 [20032/49669]\tLoss: 410.3640\n",
      "Training Epoch: 16 [20096/49669]\tLoss: 401.9363\n",
      "Training Epoch: 16 [20160/49669]\tLoss: 388.7257\n",
      "Training Epoch: 16 [20224/49669]\tLoss: 419.2780\n",
      "Training Epoch: 16 [20288/49669]\tLoss: 376.0857\n",
      "Training Epoch: 16 [20352/49669]\tLoss: 422.8345\n",
      "Training Epoch: 16 [20416/49669]\tLoss: 410.7587\n",
      "Training Epoch: 16 [20480/49669]\tLoss: 431.6483\n",
      "Training Epoch: 16 [20544/49669]\tLoss: 390.4982\n",
      "Training Epoch: 16 [20608/49669]\tLoss: 422.4015\n",
      "Training Epoch: 16 [20672/49669]\tLoss: 407.5105\n",
      "Training Epoch: 16 [20736/49669]\tLoss: 418.6870\n",
      "Training Epoch: 16 [20800/49669]\tLoss: 405.9946\n",
      "Training Epoch: 16 [20864/49669]\tLoss: 411.8779\n",
      "Training Epoch: 16 [20928/49669]\tLoss: 421.2330\n",
      "Training Epoch: 16 [20992/49669]\tLoss: 417.6478\n",
      "Training Epoch: 16 [21056/49669]\tLoss: 414.4579\n",
      "Training Epoch: 16 [21120/49669]\tLoss: 444.5804\n",
      "Training Epoch: 16 [21184/49669]\tLoss: 427.0356\n",
      "Training Epoch: 16 [21248/49669]\tLoss: 412.8640\n",
      "Training Epoch: 16 [21312/49669]\tLoss: 478.6916\n",
      "Training Epoch: 16 [21376/49669]\tLoss: 477.2855\n",
      "Training Epoch: 16 [21440/49669]\tLoss: 453.1806\n",
      "Training Epoch: 16 [21504/49669]\tLoss: 457.8353\n",
      "Training Epoch: 16 [21568/49669]\tLoss: 441.2527\n",
      "Training Epoch: 16 [21632/49669]\tLoss: 420.5891\n",
      "Training Epoch: 16 [21696/49669]\tLoss: 431.9709\n",
      "Training Epoch: 16 [21760/49669]\tLoss: 407.2664\n",
      "Training Epoch: 16 [21824/49669]\tLoss: 430.5712\n",
      "Training Epoch: 16 [21888/49669]\tLoss: 454.2404\n",
      "Training Epoch: 16 [21952/49669]\tLoss: 481.0982\n",
      "Training Epoch: 16 [22016/49669]\tLoss: 528.6841\n",
      "Training Epoch: 16 [22080/49669]\tLoss: 473.5381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [22144/49669]\tLoss: 470.8871\n",
      "Training Epoch: 16 [22208/49669]\tLoss: 397.8259\n",
      "Training Epoch: 16 [22272/49669]\tLoss: 427.9146\n",
      "Training Epoch: 16 [22336/49669]\tLoss: 467.7193\n",
      "Training Epoch: 16 [22400/49669]\tLoss: 444.7997\n",
      "Training Epoch: 16 [22464/49669]\tLoss: 423.9575\n",
      "Training Epoch: 16 [22528/49669]\tLoss: 392.0495\n",
      "Training Epoch: 16 [22592/49669]\tLoss: 443.3354\n",
      "Training Epoch: 16 [22656/49669]\tLoss: 454.2250\n",
      "Training Epoch: 16 [22720/49669]\tLoss: 445.2211\n",
      "Training Epoch: 16 [22784/49669]\tLoss: 437.1283\n",
      "Training Epoch: 16 [22848/49669]\tLoss: 415.1725\n",
      "Training Epoch: 16 [22912/49669]\tLoss: 417.1476\n",
      "Training Epoch: 16 [22976/49669]\tLoss: 448.4996\n",
      "Training Epoch: 16 [23040/49669]\tLoss: 423.7769\n",
      "Training Epoch: 16 [23104/49669]\tLoss: 400.2132\n",
      "Training Epoch: 16 [23168/49669]\tLoss: 411.2766\n",
      "Training Epoch: 16 [23232/49669]\tLoss: 435.2951\n",
      "Training Epoch: 16 [23296/49669]\tLoss: 434.9714\n",
      "Training Epoch: 16 [23360/49669]\tLoss: 439.5379\n",
      "Training Epoch: 16 [23424/49669]\tLoss: 434.7550\n",
      "Training Epoch: 16 [23488/49669]\tLoss: 422.0738\n",
      "Training Epoch: 16 [23552/49669]\tLoss: 381.5407\n",
      "Training Epoch: 16 [23616/49669]\tLoss: 401.6959\n",
      "Training Epoch: 16 [23680/49669]\tLoss: 421.4834\n",
      "Training Epoch: 16 [23744/49669]\tLoss: 391.5528\n",
      "Training Epoch: 16 [23808/49669]\tLoss: 399.8908\n",
      "Training Epoch: 16 [23872/49669]\tLoss: 389.8416\n",
      "Training Epoch: 16 [23936/49669]\tLoss: 380.8726\n",
      "Training Epoch: 16 [24000/49669]\tLoss: 419.9885\n",
      "Training Epoch: 16 [24064/49669]\tLoss: 429.8132\n",
      "Training Epoch: 16 [24128/49669]\tLoss: 390.0093\n",
      "Training Epoch: 16 [24192/49669]\tLoss: 411.0188\n",
      "Training Epoch: 16 [24256/49669]\tLoss: 419.5205\n",
      "Training Epoch: 16 [24320/49669]\tLoss: 398.6750\n",
      "Training Epoch: 16 [24384/49669]\tLoss: 436.9073\n",
      "Training Epoch: 16 [24448/49669]\tLoss: 424.4451\n",
      "Training Epoch: 16 [24512/49669]\tLoss: 412.9776\n",
      "Training Epoch: 16 [24576/49669]\tLoss: 401.5317\n",
      "Training Epoch: 16 [24640/49669]\tLoss: 399.8765\n",
      "Training Epoch: 16 [24704/49669]\tLoss: 404.3257\n",
      "Training Epoch: 16 [24768/49669]\tLoss: 422.3595\n",
      "Training Epoch: 16 [24832/49669]\tLoss: 404.0172\n",
      "Training Epoch: 16 [24896/49669]\tLoss: 417.3525\n",
      "Training Epoch: 16 [24960/49669]\tLoss: 410.3789\n",
      "Training Epoch: 16 [25024/49669]\tLoss: 398.8740\n",
      "Training Epoch: 16 [25088/49669]\tLoss: 396.5527\n",
      "Training Epoch: 16 [25152/49669]\tLoss: 399.1797\n",
      "Training Epoch: 16 [25216/49669]\tLoss: 410.6497\n",
      "Training Epoch: 16 [25280/49669]\tLoss: 420.1956\n",
      "Training Epoch: 16 [25344/49669]\tLoss: 400.3842\n",
      "Training Epoch: 16 [25408/49669]\tLoss: 433.0341\n",
      "Training Epoch: 16 [25472/49669]\tLoss: 436.1531\n",
      "Training Epoch: 16 [25536/49669]\tLoss: 385.4223\n",
      "Training Epoch: 16 [25600/49669]\tLoss: 391.2935\n",
      "Training Epoch: 16 [25664/49669]\tLoss: 401.7623\n",
      "Training Epoch: 16 [25728/49669]\tLoss: 416.2137\n",
      "Training Epoch: 16 [25792/49669]\tLoss: 391.3631\n",
      "Training Epoch: 16 [25856/49669]\tLoss: 434.8422\n",
      "Training Epoch: 16 [25920/49669]\tLoss: 423.3525\n",
      "Training Epoch: 16 [25984/49669]\tLoss: 397.9649\n",
      "Training Epoch: 16 [26048/49669]\tLoss: 443.5414\n",
      "Training Epoch: 16 [26112/49669]\tLoss: 396.9803\n",
      "Training Epoch: 16 [26176/49669]\tLoss: 393.9562\n",
      "Training Epoch: 16 [26240/49669]\tLoss: 404.4949\n",
      "Training Epoch: 16 [26304/49669]\tLoss: 402.7746\n",
      "Training Epoch: 16 [26368/49669]\tLoss: 404.5136\n",
      "Training Epoch: 16 [26432/49669]\tLoss: 392.1302\n",
      "Training Epoch: 16 [26496/49669]\tLoss: 427.5758\n",
      "Training Epoch: 16 [26560/49669]\tLoss: 409.7908\n",
      "Training Epoch: 16 [26624/49669]\tLoss: 413.9829\n",
      "Training Epoch: 16 [26688/49669]\tLoss: 422.3594\n",
      "Training Epoch: 16 [26752/49669]\tLoss: 405.5428\n",
      "Training Epoch: 16 [26816/49669]\tLoss: 412.9204\n",
      "Training Epoch: 16 [26880/49669]\tLoss: 426.2402\n",
      "Training Epoch: 16 [26944/49669]\tLoss: 428.3842\n",
      "Training Epoch: 16 [27008/49669]\tLoss: 428.4574\n",
      "Training Epoch: 16 [27072/49669]\tLoss: 419.3500\n",
      "Training Epoch: 16 [27136/49669]\tLoss: 406.6446\n",
      "Training Epoch: 16 [27200/49669]\tLoss: 419.1854\n",
      "Training Epoch: 16 [27264/49669]\tLoss: 380.2166\n",
      "Training Epoch: 16 [27328/49669]\tLoss: 404.9547\n",
      "Training Epoch: 16 [27392/49669]\tLoss: 425.4762\n",
      "Training Epoch: 16 [27456/49669]\tLoss: 409.3139\n",
      "Training Epoch: 16 [27520/49669]\tLoss: 394.3990\n",
      "Training Epoch: 16 [27584/49669]\tLoss: 409.4303\n",
      "Training Epoch: 16 [27648/49669]\tLoss: 408.5075\n",
      "Training Epoch: 16 [27712/49669]\tLoss: 383.9406\n",
      "Training Epoch: 16 [27776/49669]\tLoss: 399.7075\n",
      "Training Epoch: 16 [27840/49669]\tLoss: 449.4088\n",
      "Training Epoch: 16 [27904/49669]\tLoss: 401.0251\n",
      "Training Epoch: 16 [27968/49669]\tLoss: 426.6199\n",
      "Training Epoch: 16 [28032/49669]\tLoss: 428.8596\n",
      "Training Epoch: 16 [28096/49669]\tLoss: 411.9498\n",
      "Training Epoch: 16 [28160/49669]\tLoss: 404.9120\n",
      "Training Epoch: 16 [28224/49669]\tLoss: 418.5092\n",
      "Training Epoch: 16 [28288/49669]\tLoss: 406.3793\n",
      "Training Epoch: 16 [28352/49669]\tLoss: 414.5201\n",
      "Training Epoch: 16 [28416/49669]\tLoss: 410.8147\n",
      "Training Epoch: 16 [28480/49669]\tLoss: 409.2930\n",
      "Training Epoch: 16 [28544/49669]\tLoss: 401.3800\n",
      "Training Epoch: 16 [28608/49669]\tLoss: 407.3731\n",
      "Training Epoch: 16 [28672/49669]\tLoss: 416.3510\n",
      "Training Epoch: 16 [28736/49669]\tLoss: 387.4341\n",
      "Training Epoch: 16 [28800/49669]\tLoss: 407.3273\n",
      "Training Epoch: 16 [28864/49669]\tLoss: 397.6601\n",
      "Training Epoch: 16 [28928/49669]\tLoss: 410.0261\n",
      "Training Epoch: 16 [28992/49669]\tLoss: 406.9137\n",
      "Training Epoch: 16 [29056/49669]\tLoss: 401.3596\n",
      "Training Epoch: 16 [29120/49669]\tLoss: 404.6294\n",
      "Training Epoch: 16 [29184/49669]\tLoss: 429.3297\n",
      "Training Epoch: 16 [29248/49669]\tLoss: 406.6741\n",
      "Training Epoch: 16 [29312/49669]\tLoss: 392.4214\n",
      "Training Epoch: 16 [29376/49669]\tLoss: 371.2199\n",
      "Training Epoch: 16 [29440/49669]\tLoss: 413.9128\n",
      "Training Epoch: 16 [29504/49669]\tLoss: 395.7408\n",
      "Training Epoch: 16 [29568/49669]\tLoss: 424.8673\n",
      "Training Epoch: 16 [29632/49669]\tLoss: 384.8378\n",
      "Training Epoch: 16 [29696/49669]\tLoss: 420.5701\n",
      "Training Epoch: 16 [29760/49669]\tLoss: 402.0153\n",
      "Training Epoch: 16 [29824/49669]\tLoss: 399.8223\n",
      "Training Epoch: 16 [29888/49669]\tLoss: 417.9126\n",
      "Training Epoch: 16 [29952/49669]\tLoss: 444.3857\n",
      "Training Epoch: 16 [30016/49669]\tLoss: 405.3176\n",
      "Training Epoch: 16 [30080/49669]\tLoss: 371.3867\n",
      "Training Epoch: 16 [30144/49669]\tLoss: 401.6304\n",
      "Training Epoch: 16 [30208/49669]\tLoss: 420.8907\n",
      "Training Epoch: 16 [30272/49669]\tLoss: 379.7630\n",
      "Training Epoch: 16 [30336/49669]\tLoss: 426.8484\n",
      "Training Epoch: 16 [30400/49669]\tLoss: 403.3163\n",
      "Training Epoch: 16 [30464/49669]\tLoss: 402.9890\n",
      "Training Epoch: 16 [30528/49669]\tLoss: 426.7393\n",
      "Training Epoch: 16 [30592/49669]\tLoss: 403.5109\n",
      "Training Epoch: 16 [30656/49669]\tLoss: 428.3376\n",
      "Training Epoch: 16 [30720/49669]\tLoss: 391.8071\n",
      "Training Epoch: 16 [30784/49669]\tLoss: 384.3193\n",
      "Training Epoch: 16 [30848/49669]\tLoss: 420.1213\n",
      "Training Epoch: 16 [30912/49669]\tLoss: 424.4641\n",
      "Training Epoch: 16 [30976/49669]\tLoss: 417.3711\n",
      "Training Epoch: 16 [31040/49669]\tLoss: 417.4930\n",
      "Training Epoch: 16 [31104/49669]\tLoss: 416.2558\n",
      "Training Epoch: 16 [31168/49669]\tLoss: 431.4280\n",
      "Training Epoch: 16 [31232/49669]\tLoss: 398.6319\n",
      "Training Epoch: 16 [31296/49669]\tLoss: 385.9206\n",
      "Training Epoch: 16 [31360/49669]\tLoss: 431.3098\n",
      "Training Epoch: 16 [31424/49669]\tLoss: 426.4379\n",
      "Training Epoch: 16 [31488/49669]\tLoss: 433.8079\n",
      "Training Epoch: 16 [31552/49669]\tLoss: 415.3445\n",
      "Training Epoch: 16 [31616/49669]\tLoss: 371.0292\n",
      "Training Epoch: 16 [31680/49669]\tLoss: 377.7791\n",
      "Training Epoch: 16 [31744/49669]\tLoss: 419.2544\n",
      "Training Epoch: 16 [31808/49669]\tLoss: 438.5146\n",
      "Training Epoch: 16 [31872/49669]\tLoss: 402.4093\n",
      "Training Epoch: 16 [31936/49669]\tLoss: 412.6529\n",
      "Training Epoch: 16 [32000/49669]\tLoss: 418.6052\n",
      "Training Epoch: 16 [32064/49669]\tLoss: 408.9174\n",
      "Training Epoch: 16 [32128/49669]\tLoss: 428.8028\n",
      "Training Epoch: 16 [32192/49669]\tLoss: 409.0687\n",
      "Training Epoch: 16 [32256/49669]\tLoss: 382.8324\n",
      "Training Epoch: 16 [32320/49669]\tLoss: 405.6562\n",
      "Training Epoch: 16 [32384/49669]\tLoss: 412.7511\n",
      "Training Epoch: 16 [32448/49669]\tLoss: 402.9254\n",
      "Training Epoch: 16 [32512/49669]\tLoss: 396.0538\n",
      "Training Epoch: 16 [32576/49669]\tLoss: 398.2880\n",
      "Training Epoch: 16 [32640/49669]\tLoss: 415.9525\n",
      "Training Epoch: 16 [32704/49669]\tLoss: 396.1991\n",
      "Training Epoch: 16 [32768/49669]\tLoss: 400.4242\n",
      "Training Epoch: 16 [32832/49669]\tLoss: 423.4641\n",
      "Training Epoch: 16 [32896/49669]\tLoss: 417.0865\n",
      "Training Epoch: 16 [32960/49669]\tLoss: 386.3963\n",
      "Training Epoch: 16 [33024/49669]\tLoss: 423.2077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [33088/49669]\tLoss: 396.7963\n",
      "Training Epoch: 16 [33152/49669]\tLoss: 408.4636\n",
      "Training Epoch: 16 [33216/49669]\tLoss: 422.9475\n",
      "Training Epoch: 16 [33280/49669]\tLoss: 426.7257\n",
      "Training Epoch: 16 [33344/49669]\tLoss: 384.9277\n",
      "Training Epoch: 16 [33408/49669]\tLoss: 408.3376\n",
      "Training Epoch: 16 [33472/49669]\tLoss: 397.6560\n",
      "Training Epoch: 16 [33536/49669]\tLoss: 410.6951\n",
      "Training Epoch: 16 [33600/49669]\tLoss: 394.7477\n",
      "Training Epoch: 16 [33664/49669]\tLoss: 436.7916\n",
      "Training Epoch: 16 [33728/49669]\tLoss: 410.6882\n",
      "Training Epoch: 16 [33792/49669]\tLoss: 427.3170\n",
      "Training Epoch: 16 [33856/49669]\tLoss: 403.5244\n",
      "Training Epoch: 16 [33920/49669]\tLoss: 401.0875\n",
      "Training Epoch: 16 [33984/49669]\tLoss: 418.9424\n",
      "Training Epoch: 16 [34048/49669]\tLoss: 393.3702\n",
      "Training Epoch: 16 [34112/49669]\tLoss: 383.2338\n",
      "Training Epoch: 16 [34176/49669]\tLoss: 424.4689\n",
      "Training Epoch: 16 [34240/49669]\tLoss: 399.5179\n",
      "Training Epoch: 16 [34304/49669]\tLoss: 403.6796\n",
      "Training Epoch: 16 [34368/49669]\tLoss: 411.4032\n",
      "Training Epoch: 16 [34432/49669]\tLoss: 399.7216\n",
      "Training Epoch: 16 [34496/49669]\tLoss: 441.4093\n",
      "Training Epoch: 16 [34560/49669]\tLoss: 393.5740\n",
      "Training Epoch: 16 [34624/49669]\tLoss: 399.4016\n",
      "Training Epoch: 16 [34688/49669]\tLoss: 423.3051\n",
      "Training Epoch: 16 [34752/49669]\tLoss: 415.2075\n",
      "Training Epoch: 16 [34816/49669]\tLoss: 432.3922\n",
      "Training Epoch: 16 [34880/49669]\tLoss: 395.2087\n",
      "Training Epoch: 16 [34944/49669]\tLoss: 392.9983\n",
      "Training Epoch: 16 [35008/49669]\tLoss: 416.2888\n",
      "Training Epoch: 16 [35072/49669]\tLoss: 409.4854\n",
      "Training Epoch: 16 [35136/49669]\tLoss: 401.4905\n",
      "Training Epoch: 16 [35200/49669]\tLoss: 399.6312\n",
      "Training Epoch: 16 [35264/49669]\tLoss: 371.1453\n",
      "Training Epoch: 16 [35328/49669]\tLoss: 430.6437\n",
      "Training Epoch: 16 [35392/49669]\tLoss: 407.2892\n",
      "Training Epoch: 16 [35456/49669]\tLoss: 404.0046\n",
      "Training Epoch: 16 [35520/49669]\tLoss: 410.1996\n",
      "Training Epoch: 16 [35584/49669]\tLoss: 413.2777\n",
      "Training Epoch: 16 [35648/49669]\tLoss: 378.8951\n",
      "Training Epoch: 16 [35712/49669]\tLoss: 414.6018\n",
      "Training Epoch: 16 [35776/49669]\tLoss: 425.6164\n",
      "Training Epoch: 16 [35840/49669]\tLoss: 390.3164\n",
      "Training Epoch: 16 [35904/49669]\tLoss: 434.4054\n",
      "Training Epoch: 16 [35968/49669]\tLoss: 425.0185\n",
      "Training Epoch: 16 [36032/49669]\tLoss: 430.9509\n",
      "Training Epoch: 16 [36096/49669]\tLoss: 419.7828\n",
      "Training Epoch: 16 [36160/49669]\tLoss: 425.9062\n",
      "Training Epoch: 16 [36224/49669]\tLoss: 396.5033\n",
      "Training Epoch: 16 [36288/49669]\tLoss: 395.8810\n",
      "Training Epoch: 16 [36352/49669]\tLoss: 383.4706\n",
      "Training Epoch: 16 [36416/49669]\tLoss: 402.2480\n",
      "Training Epoch: 16 [36480/49669]\tLoss: 399.0240\n",
      "Training Epoch: 16 [36544/49669]\tLoss: 406.5406\n",
      "Training Epoch: 16 [36608/49669]\tLoss: 381.7057\n",
      "Training Epoch: 16 [36672/49669]\tLoss: 409.8807\n",
      "Training Epoch: 16 [36736/49669]\tLoss: 390.6236\n",
      "Training Epoch: 16 [36800/49669]\tLoss: 431.2017\n",
      "Training Epoch: 16 [36864/49669]\tLoss: 420.7362\n",
      "Training Epoch: 16 [36928/49669]\tLoss: 362.7444\n",
      "Training Epoch: 16 [36992/49669]\tLoss: 406.5872\n",
      "Training Epoch: 16 [37056/49669]\tLoss: 401.2137\n",
      "Training Epoch: 16 [37120/49669]\tLoss: 417.0282\n",
      "Training Epoch: 16 [37184/49669]\tLoss: 411.7103\n",
      "Training Epoch: 16 [37248/49669]\tLoss: 414.2438\n",
      "Training Epoch: 16 [37312/49669]\tLoss: 398.6243\n",
      "Training Epoch: 16 [37376/49669]\tLoss: 396.2526\n",
      "Training Epoch: 16 [37440/49669]\tLoss: 433.6638\n",
      "Training Epoch: 16 [37504/49669]\tLoss: 402.4758\n",
      "Training Epoch: 16 [37568/49669]\tLoss: 390.1645\n",
      "Training Epoch: 16 [37632/49669]\tLoss: 409.4941\n",
      "Training Epoch: 16 [37696/49669]\tLoss: 416.6892\n",
      "Training Epoch: 16 [37760/49669]\tLoss: 425.5311\n",
      "Training Epoch: 16 [37824/49669]\tLoss: 411.5176\n",
      "Training Epoch: 16 [37888/49669]\tLoss: 432.4844\n",
      "Training Epoch: 16 [37952/49669]\tLoss: 431.1063\n",
      "Training Epoch: 16 [38016/49669]\tLoss: 416.0934\n",
      "Training Epoch: 16 [38080/49669]\tLoss: 392.6754\n",
      "Training Epoch: 16 [38144/49669]\tLoss: 431.8767\n",
      "Training Epoch: 16 [38208/49669]\tLoss: 383.9484\n",
      "Training Epoch: 16 [38272/49669]\tLoss: 437.8759\n",
      "Training Epoch: 16 [38336/49669]\tLoss: 400.4671\n",
      "Training Epoch: 16 [38400/49669]\tLoss: 434.1963\n",
      "Training Epoch: 16 [38464/49669]\tLoss: 408.4003\n",
      "Training Epoch: 16 [38528/49669]\tLoss: 389.2102\n",
      "Training Epoch: 16 [38592/49669]\tLoss: 420.0383\n",
      "Training Epoch: 16 [38656/49669]\tLoss: 430.3490\n",
      "Training Epoch: 16 [38720/49669]\tLoss: 395.2403\n",
      "Training Epoch: 16 [38784/49669]\tLoss: 378.2195\n",
      "Training Epoch: 16 [38848/49669]\tLoss: 400.0914\n",
      "Training Epoch: 16 [38912/49669]\tLoss: 420.7296\n",
      "Training Epoch: 16 [38976/49669]\tLoss: 406.2842\n",
      "Training Epoch: 16 [39040/49669]\tLoss: 402.1673\n",
      "Training Epoch: 16 [39104/49669]\tLoss: 411.6622\n",
      "Training Epoch: 16 [39168/49669]\tLoss: 429.7593\n",
      "Training Epoch: 16 [39232/49669]\tLoss: 409.3075\n",
      "Training Epoch: 16 [39296/49669]\tLoss: 394.4857\n",
      "Training Epoch: 16 [39360/49669]\tLoss: 394.2984\n",
      "Training Epoch: 16 [39424/49669]\tLoss: 437.7605\n",
      "Training Epoch: 16 [39488/49669]\tLoss: 409.5549\n",
      "Training Epoch: 16 [39552/49669]\tLoss: 413.6620\n",
      "Training Epoch: 16 [39616/49669]\tLoss: 428.1184\n",
      "Training Epoch: 16 [39680/49669]\tLoss: 385.6537\n",
      "Training Epoch: 16 [39744/49669]\tLoss: 376.1137\n",
      "Training Epoch: 16 [39808/49669]\tLoss: 403.6108\n",
      "Training Epoch: 16 [39872/49669]\tLoss: 410.5735\n",
      "Training Epoch: 16 [39936/49669]\tLoss: 394.7274\n",
      "Training Epoch: 16 [40000/49669]\tLoss: 415.8825\n",
      "Training Epoch: 16 [40064/49669]\tLoss: 409.2015\n",
      "Training Epoch: 16 [40128/49669]\tLoss: 425.2842\n",
      "Training Epoch: 16 [40192/49669]\tLoss: 382.5189\n",
      "Training Epoch: 16 [40256/49669]\tLoss: 428.8595\n",
      "Training Epoch: 16 [40320/49669]\tLoss: 390.8939\n",
      "Training Epoch: 16 [40384/49669]\tLoss: 393.1132\n",
      "Training Epoch: 16 [40448/49669]\tLoss: 427.7933\n",
      "Training Epoch: 16 [40512/49669]\tLoss: 364.4573\n",
      "Training Epoch: 16 [40576/49669]\tLoss: 407.3155\n",
      "Training Epoch: 16 [40640/49669]\tLoss: 406.6562\n",
      "Training Epoch: 16 [40704/49669]\tLoss: 391.2942\n",
      "Training Epoch: 16 [40768/49669]\tLoss: 401.1831\n",
      "Training Epoch: 16 [40832/49669]\tLoss: 390.9677\n",
      "Training Epoch: 16 [40896/49669]\tLoss: 436.1180\n",
      "Training Epoch: 16 [40960/49669]\tLoss: 407.8041\n",
      "Training Epoch: 16 [41024/49669]\tLoss: 392.4086\n",
      "Training Epoch: 16 [41088/49669]\tLoss: 433.3470\n",
      "Training Epoch: 16 [41152/49669]\tLoss: 375.5636\n",
      "Training Epoch: 16 [41216/49669]\tLoss: 399.2039\n",
      "Training Epoch: 16 [41280/49669]\tLoss: 424.1107\n",
      "Training Epoch: 16 [41344/49669]\tLoss: 404.8146\n",
      "Training Epoch: 16 [41408/49669]\tLoss: 404.4599\n",
      "Training Epoch: 16 [41472/49669]\tLoss: 439.8945\n",
      "Training Epoch: 16 [41536/49669]\tLoss: 388.0201\n",
      "Training Epoch: 16 [41600/49669]\tLoss: 397.4828\n",
      "Training Epoch: 16 [41664/49669]\tLoss: 432.7900\n",
      "Training Epoch: 16 [41728/49669]\tLoss: 432.3619\n",
      "Training Epoch: 16 [41792/49669]\tLoss: 405.1951\n",
      "Training Epoch: 16 [41856/49669]\tLoss: 393.3340\n",
      "Training Epoch: 16 [41920/49669]\tLoss: 397.4279\n",
      "Training Epoch: 16 [41984/49669]\tLoss: 411.1013\n",
      "Training Epoch: 16 [42048/49669]\tLoss: 400.9452\n",
      "Training Epoch: 16 [42112/49669]\tLoss: 430.9236\n",
      "Training Epoch: 16 [42176/49669]\tLoss: 366.1979\n",
      "Training Epoch: 16 [42240/49669]\tLoss: 411.4421\n",
      "Training Epoch: 16 [42304/49669]\tLoss: 394.8973\n",
      "Training Epoch: 16 [42368/49669]\tLoss: 423.5417\n",
      "Training Epoch: 16 [42432/49669]\tLoss: 424.5863\n",
      "Training Epoch: 16 [42496/49669]\tLoss: 394.6046\n",
      "Training Epoch: 16 [42560/49669]\tLoss: 416.7978\n",
      "Training Epoch: 16 [42624/49669]\tLoss: 416.8210\n",
      "Training Epoch: 16 [42688/49669]\tLoss: 383.6981\n",
      "Training Epoch: 16 [42752/49669]\tLoss: 424.9121\n",
      "Training Epoch: 16 [42816/49669]\tLoss: 420.3333\n",
      "Training Epoch: 16 [42880/49669]\tLoss: 412.7014\n",
      "Training Epoch: 16 [42944/49669]\tLoss: 428.5425\n",
      "Training Epoch: 16 [43008/49669]\tLoss: 453.1494\n",
      "Training Epoch: 16 [43072/49669]\tLoss: 464.4696\n",
      "Training Epoch: 16 [43136/49669]\tLoss: 468.2221\n",
      "Training Epoch: 16 [43200/49669]\tLoss: 517.8500\n",
      "Training Epoch: 16 [43264/49669]\tLoss: 554.7060\n",
      "Training Epoch: 16 [43328/49669]\tLoss: 535.5760\n",
      "Training Epoch: 16 [43392/49669]\tLoss: 475.6423\n",
      "Training Epoch: 16 [43456/49669]\tLoss: 431.4683\n",
      "Training Epoch: 16 [43520/49669]\tLoss: 403.1682\n",
      "Training Epoch: 16 [43584/49669]\tLoss: 448.2959\n",
      "Training Epoch: 16 [43648/49669]\tLoss: 504.9605\n",
      "Training Epoch: 16 [43712/49669]\tLoss: 512.8281\n",
      "Training Epoch: 16 [43776/49669]\tLoss: 511.6201\n",
      "Training Epoch: 16 [43840/49669]\tLoss: 433.8198\n",
      "Training Epoch: 16 [43904/49669]\tLoss: 398.6114\n",
      "Training Epoch: 16 [43968/49669]\tLoss: 454.9127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [44032/49669]\tLoss: 472.0193\n",
      "Training Epoch: 16 [44096/49669]\tLoss: 432.2078\n",
      "Training Epoch: 16 [44160/49669]\tLoss: 401.2124\n",
      "Training Epoch: 16 [44224/49669]\tLoss: 377.6172\n",
      "Training Epoch: 16 [44288/49669]\tLoss: 456.1868\n",
      "Training Epoch: 16 [44352/49669]\tLoss: 419.6215\n",
      "Training Epoch: 16 [44416/49669]\tLoss: 414.2393\n",
      "Training Epoch: 16 [44480/49669]\tLoss: 409.2639\n",
      "Training Epoch: 16 [44544/49669]\tLoss: 419.8499\n",
      "Training Epoch: 16 [44608/49669]\tLoss: 423.1013\n",
      "Training Epoch: 16 [44672/49669]\tLoss: 427.8220\n",
      "Training Epoch: 16 [44736/49669]\tLoss: 405.5545\n",
      "Training Epoch: 16 [44800/49669]\tLoss: 412.0467\n",
      "Training Epoch: 16 [44864/49669]\tLoss: 405.8987\n",
      "Training Epoch: 16 [44928/49669]\tLoss: 403.5420\n",
      "Training Epoch: 16 [44992/49669]\tLoss: 417.0059\n",
      "Training Epoch: 16 [45056/49669]\tLoss: 427.1758\n",
      "Training Epoch: 16 [45120/49669]\tLoss: 410.6316\n",
      "Training Epoch: 16 [45184/49669]\tLoss: 422.1570\n",
      "Training Epoch: 16 [45248/49669]\tLoss: 404.8968\n",
      "Training Epoch: 16 [45312/49669]\tLoss: 380.5125\n",
      "Training Epoch: 16 [45376/49669]\tLoss: 423.7065\n",
      "Training Epoch: 16 [45440/49669]\tLoss: 411.8491\n",
      "Training Epoch: 16 [45504/49669]\tLoss: 411.5631\n",
      "Training Epoch: 16 [45568/49669]\tLoss: 425.0953\n",
      "Training Epoch: 16 [45632/49669]\tLoss: 380.8122\n",
      "Training Epoch: 16 [45696/49669]\tLoss: 431.3889\n",
      "Training Epoch: 16 [45760/49669]\tLoss: 434.1981\n",
      "Training Epoch: 16 [45824/49669]\tLoss: 439.9718\n",
      "Training Epoch: 16 [45888/49669]\tLoss: 383.9914\n",
      "Training Epoch: 16 [45952/49669]\tLoss: 399.6205\n",
      "Training Epoch: 16 [46016/49669]\tLoss: 418.9189\n",
      "Training Epoch: 16 [46080/49669]\tLoss: 381.7567\n",
      "Training Epoch: 16 [46144/49669]\tLoss: 448.5590\n",
      "Training Epoch: 16 [46208/49669]\tLoss: 398.2664\n",
      "Training Epoch: 16 [46272/49669]\tLoss: 405.7669\n",
      "Training Epoch: 16 [46336/49669]\tLoss: 397.8214\n",
      "Training Epoch: 16 [46400/49669]\tLoss: 395.4960\n",
      "Training Epoch: 16 [46464/49669]\tLoss: 419.0496\n",
      "Training Epoch: 16 [46528/49669]\tLoss: 405.3973\n",
      "Training Epoch: 16 [46592/49669]\tLoss: 397.2683\n",
      "Training Epoch: 16 [46656/49669]\tLoss: 414.7361\n",
      "Training Epoch: 16 [46720/49669]\tLoss: 398.4081\n",
      "Training Epoch: 16 [46784/49669]\tLoss: 441.4440\n",
      "Training Epoch: 16 [46848/49669]\tLoss: 414.7164\n",
      "Training Epoch: 16 [46912/49669]\tLoss: 419.2494\n",
      "Training Epoch: 16 [46976/49669]\tLoss: 417.6112\n",
      "Training Epoch: 16 [47040/49669]\tLoss: 414.6005\n",
      "Training Epoch: 16 [47104/49669]\tLoss: 407.1923\n",
      "Training Epoch: 16 [47168/49669]\tLoss: 408.4068\n",
      "Training Epoch: 16 [47232/49669]\tLoss: 413.4536\n",
      "Training Epoch: 16 [47296/49669]\tLoss: 429.3470\n",
      "Training Epoch: 16 [47360/49669]\tLoss: 437.0467\n",
      "Training Epoch: 16 [47424/49669]\tLoss: 365.9401\n",
      "Training Epoch: 16 [47488/49669]\tLoss: 398.3572\n",
      "Training Epoch: 16 [47552/49669]\tLoss: 374.9094\n",
      "Training Epoch: 16 [47616/49669]\tLoss: 409.8656\n",
      "Training Epoch: 16 [47680/49669]\tLoss: 413.0586\n",
      "Training Epoch: 16 [47744/49669]\tLoss: 395.5175\n",
      "Training Epoch: 16 [47808/49669]\tLoss: 392.7501\n",
      "Training Epoch: 16 [47872/49669]\tLoss: 427.2164\n",
      "Training Epoch: 16 [47936/49669]\tLoss: 388.5493\n",
      "Training Epoch: 16 [48000/49669]\tLoss: 395.3792\n",
      "Training Epoch: 16 [48064/49669]\tLoss: 411.1727\n",
      "Training Epoch: 16 [48128/49669]\tLoss: 397.8710\n",
      "Training Epoch: 16 [48192/49669]\tLoss: 377.4525\n",
      "Training Epoch: 16 [48256/49669]\tLoss: 390.9477\n",
      "Training Epoch: 16 [48320/49669]\tLoss: 421.1014\n",
      "Training Epoch: 16 [48384/49669]\tLoss: 431.1874\n",
      "Training Epoch: 16 [48448/49669]\tLoss: 396.6887\n",
      "Training Epoch: 16 [48512/49669]\tLoss: 408.7172\n",
      "Training Epoch: 16 [48576/49669]\tLoss: 416.2266\n",
      "Training Epoch: 16 [48640/49669]\tLoss: 385.4537\n",
      "Training Epoch: 16 [48704/49669]\tLoss: 424.9868\n",
      "Training Epoch: 16 [48768/49669]\tLoss: 403.9185\n",
      "Training Epoch: 16 [48832/49669]\tLoss: 445.4938\n",
      "Training Epoch: 16 [48896/49669]\tLoss: 406.8281\n",
      "Training Epoch: 16 [48960/49669]\tLoss: 428.8493\n",
      "Training Epoch: 16 [49024/49669]\tLoss: 444.9712\n",
      "Training Epoch: 16 [49088/49669]\tLoss: 395.3272\n",
      "Training Epoch: 16 [49152/49669]\tLoss: 403.2986\n",
      "Training Epoch: 16 [49216/49669]\tLoss: 414.3922\n",
      "Training Epoch: 16 [49280/49669]\tLoss: 436.1471\n",
      "Training Epoch: 16 [49344/49669]\tLoss: 397.5268\n",
      "Training Epoch: 16 [49408/49669]\tLoss: 415.1862\n",
      "Training Epoch: 16 [49472/49669]\tLoss: 433.6186\n",
      "Training Epoch: 16 [49536/49669]\tLoss: 402.3076\n",
      "Training Epoch: 16 [49600/49669]\tLoss: 449.0129\n",
      "Training Epoch: 16 [49664/49669]\tLoss: 392.6684\n",
      "Training Epoch: 16 [49669/49669]\tLoss: 449.3109\n",
      "Training Epoch: 16 [5519/5519]\tLoss: 411.7657\n",
      "Training Epoch: 17 [64/49669]\tLoss: 411.3853\n",
      "Training Epoch: 17 [128/49669]\tLoss: 393.5022\n",
      "Training Epoch: 17 [192/49669]\tLoss: 415.8303\n",
      "Training Epoch: 17 [256/49669]\tLoss: 411.8665\n",
      "Training Epoch: 17 [320/49669]\tLoss: 391.3056\n",
      "Training Epoch: 17 [384/49669]\tLoss: 414.8672\n",
      "Training Epoch: 17 [448/49669]\tLoss: 402.1296\n",
      "Training Epoch: 17 [512/49669]\tLoss: 392.0462\n",
      "Training Epoch: 17 [576/49669]\tLoss: 383.1489\n",
      "Training Epoch: 17 [640/49669]\tLoss: 375.3782\n",
      "Training Epoch: 17 [704/49669]\tLoss: 422.3773\n",
      "Training Epoch: 17 [768/49669]\tLoss: 419.7095\n",
      "Training Epoch: 17 [832/49669]\tLoss: 410.7163\n",
      "Training Epoch: 17 [896/49669]\tLoss: 418.3968\n",
      "Training Epoch: 17 [960/49669]\tLoss: 407.4767\n",
      "Training Epoch: 17 [1024/49669]\tLoss: 431.3900\n",
      "Training Epoch: 17 [1088/49669]\tLoss: 409.2455\n",
      "Training Epoch: 17 [1152/49669]\tLoss: 403.7938\n",
      "Training Epoch: 17 [1216/49669]\tLoss: 401.5097\n",
      "Training Epoch: 17 [1280/49669]\tLoss: 389.7722\n",
      "Training Epoch: 17 [1344/49669]\tLoss: 414.3969\n",
      "Training Epoch: 17 [1408/49669]\tLoss: 401.6446\n",
      "Training Epoch: 17 [1472/49669]\tLoss: 409.0903\n",
      "Training Epoch: 17 [1536/49669]\tLoss: 425.7423\n",
      "Training Epoch: 17 [1600/49669]\tLoss: 450.0501\n",
      "Training Epoch: 17 [1664/49669]\tLoss: 405.8592\n",
      "Training Epoch: 17 [1728/49669]\tLoss: 405.6880\n",
      "Training Epoch: 17 [1792/49669]\tLoss: 413.3537\n",
      "Training Epoch: 17 [1856/49669]\tLoss: 375.0958\n",
      "Training Epoch: 17 [1920/49669]\tLoss: 389.1234\n",
      "Training Epoch: 17 [1984/49669]\tLoss: 406.4313\n",
      "Training Epoch: 17 [2048/49669]\tLoss: 393.7858\n",
      "Training Epoch: 17 [2112/49669]\tLoss: 425.7013\n",
      "Training Epoch: 17 [2176/49669]\tLoss: 417.5372\n",
      "Training Epoch: 17 [2240/49669]\tLoss: 416.8000\n",
      "Training Epoch: 17 [2304/49669]\tLoss: 415.4984\n",
      "Training Epoch: 17 [2368/49669]\tLoss: 406.3540\n",
      "Training Epoch: 17 [2432/49669]\tLoss: 409.0902\n",
      "Training Epoch: 17 [2496/49669]\tLoss: 450.7660\n",
      "Training Epoch: 17 [2560/49669]\tLoss: 400.6573\n",
      "Training Epoch: 17 [2624/49669]\tLoss: 419.0981\n",
      "Training Epoch: 17 [2688/49669]\tLoss: 423.4746\n",
      "Training Epoch: 17 [2752/49669]\tLoss: 414.7094\n",
      "Training Epoch: 17 [2816/49669]\tLoss: 456.6878\n",
      "Training Epoch: 17 [2880/49669]\tLoss: 434.8023\n",
      "Training Epoch: 17 [2944/49669]\tLoss: 422.0890\n",
      "Training Epoch: 17 [3008/49669]\tLoss: 402.3464\n",
      "Training Epoch: 17 [3072/49669]\tLoss: 413.0730\n",
      "Training Epoch: 17 [3136/49669]\tLoss: 402.9176\n",
      "Training Epoch: 17 [3200/49669]\tLoss: 457.3766\n",
      "Training Epoch: 17 [3264/49669]\tLoss: 392.7137\n",
      "Training Epoch: 17 [3328/49669]\tLoss: 414.2392\n",
      "Training Epoch: 17 [3392/49669]\tLoss: 416.4074\n",
      "Training Epoch: 17 [3456/49669]\tLoss: 403.2924\n",
      "Training Epoch: 17 [3520/49669]\tLoss: 387.4397\n",
      "Training Epoch: 17 [3584/49669]\tLoss: 421.4944\n",
      "Training Epoch: 17 [3648/49669]\tLoss: 402.0184\n",
      "Training Epoch: 17 [3712/49669]\tLoss: 407.7621\n",
      "Training Epoch: 17 [3776/49669]\tLoss: 410.0273\n",
      "Training Epoch: 17 [3840/49669]\tLoss: 409.5274\n",
      "Training Epoch: 17 [3904/49669]\tLoss: 394.5808\n",
      "Training Epoch: 17 [3968/49669]\tLoss: 395.4433\n",
      "Training Epoch: 17 [4032/49669]\tLoss: 437.1469\n",
      "Training Epoch: 17 [4096/49669]\tLoss: 411.3825\n",
      "Training Epoch: 17 [4160/49669]\tLoss: 411.0662\n",
      "Training Epoch: 17 [4224/49669]\tLoss: 416.0212\n",
      "Training Epoch: 17 [4288/49669]\tLoss: 419.0188\n",
      "Training Epoch: 17 [4352/49669]\tLoss: 406.5473\n",
      "Training Epoch: 17 [4416/49669]\tLoss: 406.6751\n",
      "Training Epoch: 17 [4480/49669]\tLoss: 382.8173\n",
      "Training Epoch: 17 [4544/49669]\tLoss: 412.2081\n",
      "Training Epoch: 17 [4608/49669]\tLoss: 441.3716\n",
      "Training Epoch: 17 [4672/49669]\tLoss: 409.4434\n",
      "Training Epoch: 17 [4736/49669]\tLoss: 432.4218\n",
      "Training Epoch: 17 [4800/49669]\tLoss: 427.5510\n",
      "Training Epoch: 17 [4864/49669]\tLoss: 439.1557\n",
      "Training Epoch: 17 [4928/49669]\tLoss: 399.8897\n",
      "Training Epoch: 17 [4992/49669]\tLoss: 396.6710\n",
      "Training Epoch: 17 [5056/49669]\tLoss: 382.8690\n",
      "Training Epoch: 17 [5120/49669]\tLoss: 419.9764\n",
      "Training Epoch: 17 [5184/49669]\tLoss: 414.3705\n",
      "Training Epoch: 17 [5248/49669]\tLoss: 402.0878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [5312/49669]\tLoss: 383.6973\n",
      "Training Epoch: 17 [5376/49669]\tLoss: 428.0603\n",
      "Training Epoch: 17 [5440/49669]\tLoss: 397.1192\n",
      "Training Epoch: 17 [5504/49669]\tLoss: 386.3350\n",
      "Training Epoch: 17 [5568/49669]\tLoss: 368.4990\n",
      "Training Epoch: 17 [5632/49669]\tLoss: 384.8203\n",
      "Training Epoch: 17 [5696/49669]\tLoss: 420.5149\n",
      "Training Epoch: 17 [5760/49669]\tLoss: 409.3741\n",
      "Training Epoch: 17 [5824/49669]\tLoss: 413.6315\n",
      "Training Epoch: 17 [5888/49669]\tLoss: 388.5248\n",
      "Training Epoch: 17 [5952/49669]\tLoss: 418.2742\n",
      "Training Epoch: 17 [6016/49669]\tLoss: 421.0836\n",
      "Training Epoch: 17 [6080/49669]\tLoss: 398.6370\n",
      "Training Epoch: 17 [6144/49669]\tLoss: 389.6919\n",
      "Training Epoch: 17 [6208/49669]\tLoss: 387.4778\n",
      "Training Epoch: 17 [6272/49669]\tLoss: 415.0930\n",
      "Training Epoch: 17 [6336/49669]\tLoss: 418.1061\n",
      "Training Epoch: 17 [6400/49669]\tLoss: 399.3698\n",
      "Training Epoch: 17 [6464/49669]\tLoss: 411.5078\n",
      "Training Epoch: 17 [6528/49669]\tLoss: 405.3420\n",
      "Training Epoch: 17 [6592/49669]\tLoss: 404.7039\n",
      "Training Epoch: 17 [6656/49669]\tLoss: 406.0569\n",
      "Training Epoch: 17 [6720/49669]\tLoss: 412.3048\n",
      "Training Epoch: 17 [6784/49669]\tLoss: 395.5402\n",
      "Training Epoch: 17 [6848/49669]\tLoss: 390.3427\n",
      "Training Epoch: 17 [6912/49669]\tLoss: 426.1370\n",
      "Training Epoch: 17 [6976/49669]\tLoss: 400.0808\n",
      "Training Epoch: 17 [7040/49669]\tLoss: 385.7718\n",
      "Training Epoch: 17 [7104/49669]\tLoss: 425.0856\n",
      "Training Epoch: 17 [7168/49669]\tLoss: 385.3227\n",
      "Training Epoch: 17 [7232/49669]\tLoss: 422.3339\n",
      "Training Epoch: 17 [7296/49669]\tLoss: 356.6801\n",
      "Training Epoch: 17 [7360/49669]\tLoss: 403.5226\n",
      "Training Epoch: 17 [7424/49669]\tLoss: 404.7887\n",
      "Training Epoch: 17 [7488/49669]\tLoss: 409.6823\n",
      "Training Epoch: 17 [7552/49669]\tLoss: 413.7851\n",
      "Training Epoch: 17 [7616/49669]\tLoss: 409.6276\n",
      "Training Epoch: 17 [7680/49669]\tLoss: 406.3412\n",
      "Training Epoch: 17 [7744/49669]\tLoss: 421.4709\n",
      "Training Epoch: 17 [7808/49669]\tLoss: 426.0345\n",
      "Training Epoch: 17 [7872/49669]\tLoss: 448.3054\n",
      "Training Epoch: 17 [7936/49669]\tLoss: 384.4363\n",
      "Training Epoch: 17 [8000/49669]\tLoss: 417.6514\n",
      "Training Epoch: 17 [8064/49669]\tLoss: 418.5853\n",
      "Training Epoch: 17 [8128/49669]\tLoss: 427.5640\n",
      "Training Epoch: 17 [8192/49669]\tLoss: 416.3924\n",
      "Training Epoch: 17 [8256/49669]\tLoss: 414.2234\n",
      "Training Epoch: 17 [8320/49669]\tLoss: 404.1337\n",
      "Training Epoch: 17 [8384/49669]\tLoss: 424.0213\n",
      "Training Epoch: 17 [8448/49669]\tLoss: 379.8506\n",
      "Training Epoch: 17 [8512/49669]\tLoss: 391.6190\n",
      "Training Epoch: 17 [8576/49669]\tLoss: 422.0850\n",
      "Training Epoch: 17 [8640/49669]\tLoss: 415.1578\n",
      "Training Epoch: 17 [8704/49669]\tLoss: 388.4826\n",
      "Training Epoch: 17 [8768/49669]\tLoss: 413.5373\n",
      "Training Epoch: 17 [8832/49669]\tLoss: 407.7455\n",
      "Training Epoch: 17 [8896/49669]\tLoss: 404.4332\n",
      "Training Epoch: 17 [8960/49669]\tLoss: 426.6945\n",
      "Training Epoch: 17 [9024/49669]\tLoss: 433.4442\n",
      "Training Epoch: 17 [9088/49669]\tLoss: 432.3109\n",
      "Training Epoch: 17 [9152/49669]\tLoss: 403.4618\n",
      "Training Epoch: 17 [9216/49669]\tLoss: 392.9017\n",
      "Training Epoch: 17 [9280/49669]\tLoss: 409.2821\n",
      "Training Epoch: 17 [9344/49669]\tLoss: 432.5862\n",
      "Training Epoch: 17 [9408/49669]\tLoss: 409.3381\n",
      "Training Epoch: 17 [9472/49669]\tLoss: 397.7358\n",
      "Training Epoch: 17 [9536/49669]\tLoss: 404.6344\n",
      "Training Epoch: 17 [9600/49669]\tLoss: 379.0507\n",
      "Training Epoch: 17 [9664/49669]\tLoss: 399.6329\n",
      "Training Epoch: 17 [9728/49669]\tLoss: 404.4925\n",
      "Training Epoch: 17 [9792/49669]\tLoss: 437.9055\n",
      "Training Epoch: 17 [9856/49669]\tLoss: 366.8898\n",
      "Training Epoch: 17 [9920/49669]\tLoss: 428.7144\n",
      "Training Epoch: 17 [9984/49669]\tLoss: 434.4449\n",
      "Training Epoch: 17 [10048/49669]\tLoss: 395.6689\n",
      "Training Epoch: 17 [10112/49669]\tLoss: 404.9863\n",
      "Training Epoch: 17 [10176/49669]\tLoss: 382.3322\n",
      "Training Epoch: 17 [10240/49669]\tLoss: 430.9048\n",
      "Training Epoch: 17 [10304/49669]\tLoss: 411.1271\n",
      "Training Epoch: 17 [10368/49669]\tLoss: 419.3622\n",
      "Training Epoch: 17 [10432/49669]\tLoss: 422.7628\n",
      "Training Epoch: 17 [10496/49669]\tLoss: 399.5788\n",
      "Training Epoch: 17 [10560/49669]\tLoss: 402.7317\n",
      "Training Epoch: 17 [10624/49669]\tLoss: 433.0213\n",
      "Training Epoch: 17 [10688/49669]\tLoss: 400.4342\n",
      "Training Epoch: 17 [10752/49669]\tLoss: 385.4777\n",
      "Training Epoch: 17 [10816/49669]\tLoss: 443.1959\n",
      "Training Epoch: 17 [10880/49669]\tLoss: 409.2023\n",
      "Training Epoch: 17 [10944/49669]\tLoss: 390.9845\n",
      "Training Epoch: 17 [11008/49669]\tLoss: 397.2036\n",
      "Training Epoch: 17 [11072/49669]\tLoss: 401.1534\n",
      "Training Epoch: 17 [11136/49669]\tLoss: 434.1359\n",
      "Training Epoch: 17 [11200/49669]\tLoss: 404.8900\n",
      "Training Epoch: 17 [11264/49669]\tLoss: 427.3656\n",
      "Training Epoch: 17 [11328/49669]\tLoss: 406.1241\n",
      "Training Epoch: 17 [11392/49669]\tLoss: 384.8804\n",
      "Training Epoch: 17 [11456/49669]\tLoss: 407.5569\n",
      "Training Epoch: 17 [11520/49669]\tLoss: 408.8039\n",
      "Training Epoch: 17 [11584/49669]\tLoss: 406.8300\n",
      "Training Epoch: 17 [11648/49669]\tLoss: 433.1169\n",
      "Training Epoch: 17 [11712/49669]\tLoss: 408.9919\n",
      "Training Epoch: 17 [11776/49669]\tLoss: 417.7217\n",
      "Training Epoch: 17 [11840/49669]\tLoss: 428.1996\n",
      "Training Epoch: 17 [11904/49669]\tLoss: 373.9984\n",
      "Training Epoch: 17 [11968/49669]\tLoss: 406.2575\n",
      "Training Epoch: 17 [12032/49669]\tLoss: 409.1445\n",
      "Training Epoch: 17 [12096/49669]\tLoss: 399.1826\n",
      "Training Epoch: 17 [12160/49669]\tLoss: 413.4637\n",
      "Training Epoch: 17 [12224/49669]\tLoss: 403.3009\n",
      "Training Epoch: 17 [12288/49669]\tLoss: 394.2886\n",
      "Training Epoch: 17 [12352/49669]\tLoss: 430.8751\n",
      "Training Epoch: 17 [12416/49669]\tLoss: 428.8242\n",
      "Training Epoch: 17 [12480/49669]\tLoss: 395.4140\n",
      "Training Epoch: 17 [12544/49669]\tLoss: 423.7257\n",
      "Training Epoch: 17 [12608/49669]\tLoss: 384.6270\n",
      "Training Epoch: 17 [12672/49669]\tLoss: 409.6941\n",
      "Training Epoch: 17 [12736/49669]\tLoss: 427.3442\n",
      "Training Epoch: 17 [12800/49669]\tLoss: 425.8314\n",
      "Training Epoch: 17 [12864/49669]\tLoss: 396.0405\n",
      "Training Epoch: 17 [12928/49669]\tLoss: 382.0092\n",
      "Training Epoch: 17 [12992/49669]\tLoss: 393.2120\n",
      "Training Epoch: 17 [13056/49669]\tLoss: 404.2085\n",
      "Training Epoch: 17 [13120/49669]\tLoss: 426.8774\n",
      "Training Epoch: 17 [13184/49669]\tLoss: 394.4820\n",
      "Training Epoch: 17 [13248/49669]\tLoss: 399.7833\n",
      "Training Epoch: 17 [13312/49669]\tLoss: 391.4034\n",
      "Training Epoch: 17 [13376/49669]\tLoss: 408.2526\n",
      "Training Epoch: 17 [13440/49669]\tLoss: 397.7652\n",
      "Training Epoch: 17 [13504/49669]\tLoss: 387.6437\n",
      "Training Epoch: 17 [13568/49669]\tLoss: 395.8198\n",
      "Training Epoch: 17 [13632/49669]\tLoss: 400.2562\n",
      "Training Epoch: 17 [13696/49669]\tLoss: 405.4319\n",
      "Training Epoch: 17 [13760/49669]\tLoss: 436.2009\n",
      "Training Epoch: 17 [13824/49669]\tLoss: 444.8956\n",
      "Training Epoch: 17 [13888/49669]\tLoss: 398.0092\n",
      "Training Epoch: 17 [13952/49669]\tLoss: 418.0736\n",
      "Training Epoch: 17 [14016/49669]\tLoss: 392.9683\n",
      "Training Epoch: 17 [14080/49669]\tLoss: 405.8353\n",
      "Training Epoch: 17 [14144/49669]\tLoss: 399.4833\n",
      "Training Epoch: 17 [14208/49669]\tLoss: 385.2028\n",
      "Training Epoch: 17 [14272/49669]\tLoss: 444.7234\n",
      "Training Epoch: 17 [14336/49669]\tLoss: 394.1701\n",
      "Training Epoch: 17 [14400/49669]\tLoss: 396.9616\n",
      "Training Epoch: 17 [14464/49669]\tLoss: 410.7364\n",
      "Training Epoch: 17 [14528/49669]\tLoss: 384.2357\n",
      "Training Epoch: 17 [14592/49669]\tLoss: 410.6157\n",
      "Training Epoch: 17 [14656/49669]\tLoss: 399.6911\n",
      "Training Epoch: 17 [14720/49669]\tLoss: 427.5161\n",
      "Training Epoch: 17 [14784/49669]\tLoss: 435.0409\n",
      "Training Epoch: 17 [14848/49669]\tLoss: 427.5042\n",
      "Training Epoch: 17 [14912/49669]\tLoss: 392.7295\n",
      "Training Epoch: 17 [14976/49669]\tLoss: 409.7526\n",
      "Training Epoch: 17 [15040/49669]\tLoss: 414.5091\n",
      "Training Epoch: 17 [15104/49669]\tLoss: 386.4414\n",
      "Training Epoch: 17 [15168/49669]\tLoss: 408.9517\n",
      "Training Epoch: 17 [15232/49669]\tLoss: 386.9236\n",
      "Training Epoch: 17 [15296/49669]\tLoss: 433.3888\n",
      "Training Epoch: 17 [15360/49669]\tLoss: 428.9559\n",
      "Training Epoch: 17 [15424/49669]\tLoss: 404.1650\n",
      "Training Epoch: 17 [15488/49669]\tLoss: 413.0221\n",
      "Training Epoch: 17 [15552/49669]\tLoss: 406.1544\n",
      "Training Epoch: 17 [15616/49669]\tLoss: 398.1949\n",
      "Training Epoch: 17 [15680/49669]\tLoss: 411.4772\n",
      "Training Epoch: 17 [15744/49669]\tLoss: 405.6081\n",
      "Training Epoch: 17 [15808/49669]\tLoss: 416.9479\n",
      "Training Epoch: 17 [15872/49669]\tLoss: 388.5870\n",
      "Training Epoch: 17 [15936/49669]\tLoss: 414.9165\n",
      "Training Epoch: 17 [16000/49669]\tLoss: 435.3398\n",
      "Training Epoch: 17 [16064/49669]\tLoss: 380.3633\n",
      "Training Epoch: 17 [16128/49669]\tLoss: 438.1935\n",
      "Training Epoch: 17 [16192/49669]\tLoss: 410.5687\n",
      "Training Epoch: 17 [16256/49669]\tLoss: 402.0238\n",
      "Training Epoch: 17 [16320/49669]\tLoss: 390.7358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [16384/49669]\tLoss: 419.5349\n",
      "Training Epoch: 17 [16448/49669]\tLoss: 383.7484\n",
      "Training Epoch: 17 [16512/49669]\tLoss: 386.9093\n",
      "Training Epoch: 17 [16576/49669]\tLoss: 377.7515\n",
      "Training Epoch: 17 [16640/49669]\tLoss: 426.0921\n",
      "Training Epoch: 17 [16704/49669]\tLoss: 388.8465\n",
      "Training Epoch: 17 [16768/49669]\tLoss: 423.1766\n",
      "Training Epoch: 17 [16832/49669]\tLoss: 418.8353\n",
      "Training Epoch: 17 [16896/49669]\tLoss: 443.9805\n",
      "Training Epoch: 17 [16960/49669]\tLoss: 469.6850\n",
      "Training Epoch: 17 [17024/49669]\tLoss: 432.7827\n",
      "Training Epoch: 17 [17088/49669]\tLoss: 454.7352\n",
      "Training Epoch: 17 [17152/49669]\tLoss: 453.6086\n",
      "Training Epoch: 17 [17216/49669]\tLoss: 418.6106\n",
      "Training Epoch: 17 [17280/49669]\tLoss: 427.5506\n",
      "Training Epoch: 17 [17344/49669]\tLoss: 406.4879\n",
      "Training Epoch: 17 [17408/49669]\tLoss: 443.2478\n",
      "Training Epoch: 17 [17472/49669]\tLoss: 413.8710\n",
      "Training Epoch: 17 [17536/49669]\tLoss: 435.1476\n",
      "Training Epoch: 17 [17600/49669]\tLoss: 409.2720\n",
      "Training Epoch: 17 [17664/49669]\tLoss: 429.6864\n",
      "Training Epoch: 17 [17728/49669]\tLoss: 427.4892\n",
      "Training Epoch: 17 [17792/49669]\tLoss: 423.5457\n",
      "Training Epoch: 17 [17856/49669]\tLoss: 415.1777\n",
      "Training Epoch: 17 [17920/49669]\tLoss: 417.0868\n",
      "Training Epoch: 17 [17984/49669]\tLoss: 395.2968\n",
      "Training Epoch: 17 [18048/49669]\tLoss: 407.4493\n",
      "Training Epoch: 17 [18112/49669]\tLoss: 426.9402\n",
      "Training Epoch: 17 [18176/49669]\tLoss: 420.6309\n",
      "Training Epoch: 17 [18240/49669]\tLoss: 420.4026\n",
      "Training Epoch: 17 [18304/49669]\tLoss: 395.9135\n",
      "Training Epoch: 17 [18368/49669]\tLoss: 405.1851\n",
      "Training Epoch: 17 [18432/49669]\tLoss: 405.7303\n",
      "Training Epoch: 17 [18496/49669]\tLoss: 431.9856\n",
      "Training Epoch: 17 [18560/49669]\tLoss: 439.8914\n",
      "Training Epoch: 17 [18624/49669]\tLoss: 447.1526\n",
      "Training Epoch: 17 [18688/49669]\tLoss: 381.1924\n",
      "Training Epoch: 17 [18752/49669]\tLoss: 401.2445\n",
      "Training Epoch: 17 [18816/49669]\tLoss: 417.8860\n",
      "Training Epoch: 17 [18880/49669]\tLoss: 426.2901\n",
      "Training Epoch: 17 [18944/49669]\tLoss: 428.4432\n",
      "Training Epoch: 17 [19008/49669]\tLoss: 442.5529\n",
      "Training Epoch: 17 [19072/49669]\tLoss: 411.3037\n",
      "Training Epoch: 17 [19136/49669]\tLoss: 408.4073\n",
      "Training Epoch: 17 [19200/49669]\tLoss: 388.0963\n",
      "Training Epoch: 17 [19264/49669]\tLoss: 400.5491\n",
      "Training Epoch: 17 [19328/49669]\tLoss: 388.1876\n",
      "Training Epoch: 17 [19392/49669]\tLoss: 420.0944\n",
      "Training Epoch: 17 [19456/49669]\tLoss: 438.3233\n",
      "Training Epoch: 17 [19520/49669]\tLoss: 434.3009\n",
      "Training Epoch: 17 [19584/49669]\tLoss: 397.6191\n",
      "Training Epoch: 17 [19648/49669]\tLoss: 399.6642\n",
      "Training Epoch: 17 [19712/49669]\tLoss: 414.9729\n",
      "Training Epoch: 17 [19776/49669]\tLoss: 418.7447\n",
      "Training Epoch: 17 [19840/49669]\tLoss: 420.4454\n",
      "Training Epoch: 17 [19904/49669]\tLoss: 422.3068\n",
      "Training Epoch: 17 [19968/49669]\tLoss: 385.8472\n",
      "Training Epoch: 17 [20032/49669]\tLoss: 390.5158\n",
      "Training Epoch: 17 [20096/49669]\tLoss: 396.3892\n",
      "Training Epoch: 17 [20160/49669]\tLoss: 427.0134\n",
      "Training Epoch: 17 [20224/49669]\tLoss: 400.3457\n",
      "Training Epoch: 17 [20288/49669]\tLoss: 374.2449\n",
      "Training Epoch: 17 [20352/49669]\tLoss: 417.5637\n",
      "Training Epoch: 17 [20416/49669]\tLoss: 403.7463\n",
      "Training Epoch: 17 [20480/49669]\tLoss: 402.8703\n",
      "Training Epoch: 17 [20544/49669]\tLoss: 407.0749\n",
      "Training Epoch: 17 [20608/49669]\tLoss: 431.2087\n",
      "Training Epoch: 17 [20672/49669]\tLoss: 412.9162\n",
      "Training Epoch: 17 [20736/49669]\tLoss: 444.8261\n",
      "Training Epoch: 17 [20800/49669]\tLoss: 397.2940\n",
      "Training Epoch: 17 [20864/49669]\tLoss: 354.6965\n",
      "Training Epoch: 17 [20928/49669]\tLoss: 416.3834\n",
      "Training Epoch: 17 [20992/49669]\tLoss: 414.0386\n",
      "Training Epoch: 17 [21056/49669]\tLoss: 411.0499\n",
      "Training Epoch: 17 [21120/49669]\tLoss: 393.0242\n",
      "Training Epoch: 17 [21184/49669]\tLoss: 403.1262\n",
      "Training Epoch: 17 [21248/49669]\tLoss: 404.1978\n",
      "Training Epoch: 17 [21312/49669]\tLoss: 432.2077\n",
      "Training Epoch: 17 [21376/49669]\tLoss: 417.8074\n",
      "Training Epoch: 17 [21440/49669]\tLoss: 423.4438\n",
      "Training Epoch: 17 [21504/49669]\tLoss: 416.8392\n",
      "Training Epoch: 17 [21568/49669]\tLoss: 401.2281\n",
      "Training Epoch: 17 [21632/49669]\tLoss: 401.6177\n",
      "Training Epoch: 17 [21696/49669]\tLoss: 423.4957\n",
      "Training Epoch: 17 [21760/49669]\tLoss: 449.8766\n",
      "Training Epoch: 17 [21824/49669]\tLoss: 421.3094\n",
      "Training Epoch: 17 [21888/49669]\tLoss: 416.1411\n",
      "Training Epoch: 17 [21952/49669]\tLoss: 410.6609\n",
      "Training Epoch: 17 [22016/49669]\tLoss: 419.4774\n",
      "Training Epoch: 17 [22080/49669]\tLoss: 398.4383\n",
      "Training Epoch: 17 [22144/49669]\tLoss: 389.4604\n",
      "Training Epoch: 17 [22208/49669]\tLoss: 394.3282\n",
      "Training Epoch: 17 [22272/49669]\tLoss: 414.7364\n",
      "Training Epoch: 17 [22336/49669]\tLoss: 369.8141\n",
      "Training Epoch: 17 [22400/49669]\tLoss: 377.5774\n",
      "Training Epoch: 17 [22464/49669]\tLoss: 406.3826\n",
      "Training Epoch: 17 [22528/49669]\tLoss: 423.1988\n",
      "Training Epoch: 17 [22592/49669]\tLoss: 420.8721\n",
      "Training Epoch: 17 [22656/49669]\tLoss: 388.9645\n",
      "Training Epoch: 17 [22720/49669]\tLoss: 403.2592\n",
      "Training Epoch: 17 [22784/49669]\tLoss: 414.7205\n",
      "Training Epoch: 17 [22848/49669]\tLoss: 402.6459\n",
      "Training Epoch: 17 [22912/49669]\tLoss: 430.8876\n",
      "Training Epoch: 17 [22976/49669]\tLoss: 394.4921\n",
      "Training Epoch: 17 [23040/49669]\tLoss: 396.7896\n",
      "Training Epoch: 17 [23104/49669]\tLoss: 405.5013\n",
      "Training Epoch: 17 [23168/49669]\tLoss: 444.4088\n",
      "Training Epoch: 17 [23232/49669]\tLoss: 369.3949\n",
      "Training Epoch: 17 [23296/49669]\tLoss: 405.8165\n",
      "Training Epoch: 17 [23360/49669]\tLoss: 392.4570\n",
      "Training Epoch: 17 [23424/49669]\tLoss: 394.3835\n",
      "Training Epoch: 17 [23488/49669]\tLoss: 382.9066\n",
      "Training Epoch: 17 [23552/49669]\tLoss: 385.5358\n",
      "Training Epoch: 17 [23616/49669]\tLoss: 426.5524\n",
      "Training Epoch: 17 [23680/49669]\tLoss: 412.5164\n",
      "Training Epoch: 17 [23744/49669]\tLoss: 391.0981\n",
      "Training Epoch: 17 [23808/49669]\tLoss: 408.6388\n",
      "Training Epoch: 17 [23872/49669]\tLoss: 387.8704\n",
      "Training Epoch: 17 [23936/49669]\tLoss: 401.2278\n",
      "Training Epoch: 17 [24000/49669]\tLoss: 389.7136\n",
      "Training Epoch: 17 [24064/49669]\tLoss: 367.4556\n",
      "Training Epoch: 17 [24128/49669]\tLoss: 382.5670\n",
      "Training Epoch: 17 [24192/49669]\tLoss: 418.0913\n",
      "Training Epoch: 17 [24256/49669]\tLoss: 400.5933\n",
      "Training Epoch: 17 [24320/49669]\tLoss: 405.4111\n",
      "Training Epoch: 17 [24384/49669]\tLoss: 421.7039\n",
      "Training Epoch: 17 [24448/49669]\tLoss: 398.2789\n",
      "Training Epoch: 17 [24512/49669]\tLoss: 387.2281\n",
      "Training Epoch: 17 [24576/49669]\tLoss: 402.9169\n",
      "Training Epoch: 17 [24640/49669]\tLoss: 419.3362\n",
      "Training Epoch: 17 [24704/49669]\tLoss: 385.1952\n",
      "Training Epoch: 17 [24768/49669]\tLoss: 390.3074\n",
      "Training Epoch: 17 [24832/49669]\tLoss: 407.7244\n",
      "Training Epoch: 17 [24896/49669]\tLoss: 404.1492\n",
      "Training Epoch: 17 [24960/49669]\tLoss: 382.0536\n",
      "Training Epoch: 17 [25024/49669]\tLoss: 406.9360\n",
      "Training Epoch: 17 [25088/49669]\tLoss: 417.3500\n",
      "Training Epoch: 17 [25152/49669]\tLoss: 406.6353\n",
      "Training Epoch: 17 [25216/49669]\tLoss: 415.6738\n",
      "Training Epoch: 17 [25280/49669]\tLoss: 384.1726\n",
      "Training Epoch: 17 [25344/49669]\tLoss: 438.1387\n",
      "Training Epoch: 17 [25408/49669]\tLoss: 410.5403\n",
      "Training Epoch: 17 [25472/49669]\tLoss: 392.9670\n",
      "Training Epoch: 17 [25536/49669]\tLoss: 424.0277\n",
      "Training Epoch: 17 [25600/49669]\tLoss: 434.6270\n",
      "Training Epoch: 17 [25664/49669]\tLoss: 400.9922\n",
      "Training Epoch: 17 [25728/49669]\tLoss: 414.1823\n",
      "Training Epoch: 17 [25792/49669]\tLoss: 415.9534\n",
      "Training Epoch: 17 [25856/49669]\tLoss: 429.7314\n",
      "Training Epoch: 17 [25920/49669]\tLoss: 420.5188\n",
      "Training Epoch: 17 [25984/49669]\tLoss: 407.3829\n",
      "Training Epoch: 17 [26048/49669]\tLoss: 435.4045\n",
      "Training Epoch: 17 [26112/49669]\tLoss: 387.6224\n",
      "Training Epoch: 17 [26176/49669]\tLoss: 423.8738\n",
      "Training Epoch: 17 [26240/49669]\tLoss: 433.0286\n",
      "Training Epoch: 17 [26304/49669]\tLoss: 395.5814\n",
      "Training Epoch: 17 [26368/49669]\tLoss: 400.3656\n",
      "Training Epoch: 17 [26432/49669]\tLoss: 437.1474\n",
      "Training Epoch: 17 [26496/49669]\tLoss: 405.7132\n",
      "Training Epoch: 17 [26560/49669]\tLoss: 425.1027\n",
      "Training Epoch: 17 [26624/49669]\tLoss: 412.5296\n",
      "Training Epoch: 17 [26688/49669]\tLoss: 396.9999\n",
      "Training Epoch: 17 [26752/49669]\tLoss: 405.3290\n",
      "Training Epoch: 17 [26816/49669]\tLoss: 405.4197\n",
      "Training Epoch: 17 [26880/49669]\tLoss: 386.7599\n",
      "Training Epoch: 17 [26944/49669]\tLoss: 423.3156\n",
      "Training Epoch: 17 [27008/49669]\tLoss: 408.1451\n",
      "Training Epoch: 17 [27072/49669]\tLoss: 409.4761\n",
      "Training Epoch: 17 [27136/49669]\tLoss: 409.0515\n",
      "Training Epoch: 17 [27200/49669]\tLoss: 390.1571\n",
      "Training Epoch: 17 [27264/49669]\tLoss: 399.0712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [27328/49669]\tLoss: 380.2866\n",
      "Training Epoch: 17 [27392/49669]\tLoss: 357.7882\n",
      "Training Epoch: 17 [27456/49669]\tLoss: 413.3699\n",
      "Training Epoch: 17 [27520/49669]\tLoss: 399.4715\n",
      "Training Epoch: 17 [27584/49669]\tLoss: 398.1970\n",
      "Training Epoch: 17 [27648/49669]\tLoss: 422.6021\n",
      "Training Epoch: 17 [27712/49669]\tLoss: 412.8354\n",
      "Training Epoch: 17 [27776/49669]\tLoss: 425.5048\n",
      "Training Epoch: 17 [27840/49669]\tLoss: 396.7998\n",
      "Training Epoch: 17 [27904/49669]\tLoss: 392.1370\n",
      "Training Epoch: 17 [27968/49669]\tLoss: 383.4207\n",
      "Training Epoch: 17 [28032/49669]\tLoss: 409.3690\n",
      "Training Epoch: 17 [28096/49669]\tLoss: 418.1810\n",
      "Training Epoch: 17 [28160/49669]\tLoss: 389.2175\n",
      "Training Epoch: 17 [28224/49669]\tLoss: 419.1219\n",
      "Training Epoch: 17 [28288/49669]\tLoss: 447.7860\n",
      "Training Epoch: 17 [28352/49669]\tLoss: 444.2947\n",
      "Training Epoch: 17 [28416/49669]\tLoss: 438.1393\n",
      "Training Epoch: 17 [28480/49669]\tLoss: 413.4097\n",
      "Training Epoch: 17 [28544/49669]\tLoss: 414.6129\n",
      "Training Epoch: 17 [28608/49669]\tLoss: 427.8544\n",
      "Training Epoch: 17 [28672/49669]\tLoss: 438.1595\n",
      "Training Epoch: 17 [28736/49669]\tLoss: 413.0530\n",
      "Training Epoch: 17 [28800/49669]\tLoss: 433.0909\n",
      "Training Epoch: 17 [28864/49669]\tLoss: 432.1990\n",
      "Training Epoch: 17 [28928/49669]\tLoss: 423.4897\n",
      "Training Epoch: 17 [28992/49669]\tLoss: 426.7939\n",
      "Training Epoch: 17 [29056/49669]\tLoss: 395.6186\n",
      "Training Epoch: 17 [29120/49669]\tLoss: 435.5833\n",
      "Training Epoch: 17 [29184/49669]\tLoss: 439.3809\n",
      "Training Epoch: 17 [29248/49669]\tLoss: 415.3392\n",
      "Training Epoch: 17 [29312/49669]\tLoss: 440.6969\n",
      "Training Epoch: 17 [29376/49669]\tLoss: 429.5335\n",
      "Training Epoch: 17 [29440/49669]\tLoss: 443.0910\n",
      "Training Epoch: 17 [29504/49669]\tLoss: 417.2000\n",
      "Training Epoch: 17 [29568/49669]\tLoss: 423.1303\n",
      "Training Epoch: 17 [29632/49669]\tLoss: 408.1687\n",
      "Training Epoch: 17 [29696/49669]\tLoss: 394.4625\n",
      "Training Epoch: 17 [29760/49669]\tLoss: 408.6576\n",
      "Training Epoch: 17 [29824/49669]\tLoss: 403.7125\n",
      "Training Epoch: 17 [29888/49669]\tLoss: 423.6884\n",
      "Training Epoch: 17 [29952/49669]\tLoss: 376.4538\n",
      "Training Epoch: 17 [30016/49669]\tLoss: 424.8366\n",
      "Training Epoch: 17 [30080/49669]\tLoss: 425.5138\n",
      "Training Epoch: 17 [30144/49669]\tLoss: 405.8492\n",
      "Training Epoch: 17 [30208/49669]\tLoss: 398.8345\n",
      "Training Epoch: 17 [30272/49669]\tLoss: 395.0213\n",
      "Training Epoch: 17 [30336/49669]\tLoss: 415.2708\n",
      "Training Epoch: 17 [30400/49669]\tLoss: 361.9944\n",
      "Training Epoch: 17 [30464/49669]\tLoss: 426.2029\n",
      "Training Epoch: 17 [30528/49669]\tLoss: 409.1854\n",
      "Training Epoch: 17 [30592/49669]\tLoss: 407.2549\n",
      "Training Epoch: 17 [30656/49669]\tLoss: 408.1517\n",
      "Training Epoch: 17 [30720/49669]\tLoss: 397.2410\n",
      "Training Epoch: 17 [30784/49669]\tLoss: 429.4265\n",
      "Training Epoch: 17 [30848/49669]\tLoss: 377.6173\n",
      "Training Epoch: 17 [30912/49669]\tLoss: 399.8020\n",
      "Training Epoch: 17 [30976/49669]\tLoss: 397.3001\n",
      "Training Epoch: 17 [31040/49669]\tLoss: 416.6707\n",
      "Training Epoch: 17 [31104/49669]\tLoss: 404.9313\n",
      "Training Epoch: 17 [31168/49669]\tLoss: 390.2800\n",
      "Training Epoch: 17 [31232/49669]\tLoss: 407.6255\n",
      "Training Epoch: 17 [31296/49669]\tLoss: 425.5024\n",
      "Training Epoch: 17 [31360/49669]\tLoss: 385.8199\n",
      "Training Epoch: 17 [31424/49669]\tLoss: 433.0165\n",
      "Training Epoch: 17 [31488/49669]\tLoss: 418.8451\n",
      "Training Epoch: 17 [31552/49669]\tLoss: 409.0287\n",
      "Training Epoch: 17 [31616/49669]\tLoss: 393.2211\n",
      "Training Epoch: 17 [31680/49669]\tLoss: 409.9990\n",
      "Training Epoch: 17 [31744/49669]\tLoss: 423.8094\n",
      "Training Epoch: 17 [31808/49669]\tLoss: 419.8165\n",
      "Training Epoch: 17 [31872/49669]\tLoss: 390.8629\n",
      "Training Epoch: 17 [31936/49669]\tLoss: 415.7775\n",
      "Training Epoch: 17 [32000/49669]\tLoss: 398.4663\n",
      "Training Epoch: 17 [32064/49669]\tLoss: 399.8995\n",
      "Training Epoch: 17 [32128/49669]\tLoss: 392.4053\n",
      "Training Epoch: 17 [32192/49669]\tLoss: 404.3934\n",
      "Training Epoch: 17 [32256/49669]\tLoss: 403.0636\n",
      "Training Epoch: 17 [32320/49669]\tLoss: 430.1193\n",
      "Training Epoch: 17 [32384/49669]\tLoss: 398.4818\n",
      "Training Epoch: 17 [32448/49669]\tLoss: 382.3315\n",
      "Training Epoch: 17 [32512/49669]\tLoss: 394.6940\n",
      "Training Epoch: 17 [32576/49669]\tLoss: 429.4639\n",
      "Training Epoch: 17 [32640/49669]\tLoss: 408.1881\n",
      "Training Epoch: 17 [32704/49669]\tLoss: 402.3690\n",
      "Training Epoch: 17 [32768/49669]\tLoss: 436.9368\n",
      "Training Epoch: 17 [32832/49669]\tLoss: 403.4263\n",
      "Training Epoch: 17 [32896/49669]\tLoss: 438.8191\n",
      "Training Epoch: 17 [32960/49669]\tLoss: 419.2470\n",
      "Training Epoch: 17 [33024/49669]\tLoss: 417.1982\n",
      "Training Epoch: 17 [33088/49669]\tLoss: 388.5084\n",
      "Training Epoch: 17 [33152/49669]\tLoss: 421.3613\n",
      "Training Epoch: 17 [33216/49669]\tLoss: 436.8498\n",
      "Training Epoch: 17 [33280/49669]\tLoss: 401.8926\n",
      "Training Epoch: 17 [33344/49669]\tLoss: 416.9366\n",
      "Training Epoch: 17 [33408/49669]\tLoss: 413.3720\n",
      "Training Epoch: 17 [33472/49669]\tLoss: 368.8629\n",
      "Training Epoch: 17 [33536/49669]\tLoss: 396.9124\n",
      "Training Epoch: 17 [33600/49669]\tLoss: 415.5122\n",
      "Training Epoch: 17 [33664/49669]\tLoss: 413.1302\n",
      "Training Epoch: 17 [33728/49669]\tLoss: 402.0258\n",
      "Training Epoch: 17 [33792/49669]\tLoss: 354.5640\n",
      "Training Epoch: 17 [33856/49669]\tLoss: 432.1097\n",
      "Training Epoch: 17 [33920/49669]\tLoss: 427.4493\n",
      "Training Epoch: 17 [33984/49669]\tLoss: 376.5613\n",
      "Training Epoch: 17 [34048/49669]\tLoss: 404.9674\n",
      "Training Epoch: 17 [34112/49669]\tLoss: 416.6766\n",
      "Training Epoch: 17 [34176/49669]\tLoss: 410.9919\n",
      "Training Epoch: 17 [34240/49669]\tLoss: 395.3171\n",
      "Training Epoch: 17 [34304/49669]\tLoss: 417.0361\n",
      "Training Epoch: 17 [34368/49669]\tLoss: 389.0326\n",
      "Training Epoch: 17 [34432/49669]\tLoss: 400.8193\n",
      "Training Epoch: 17 [34496/49669]\tLoss: 425.6604\n",
      "Training Epoch: 17 [34560/49669]\tLoss: 403.5125\n",
      "Training Epoch: 17 [34624/49669]\tLoss: 411.4219\n",
      "Training Epoch: 17 [34688/49669]\tLoss: 393.8321\n",
      "Training Epoch: 17 [34752/49669]\tLoss: 407.2254\n",
      "Training Epoch: 17 [34816/49669]\tLoss: 380.1838\n",
      "Training Epoch: 17 [34880/49669]\tLoss: 376.8662\n",
      "Training Epoch: 17 [34944/49669]\tLoss: 415.3982\n",
      "Training Epoch: 17 [35008/49669]\tLoss: 394.8570\n",
      "Training Epoch: 17 [35072/49669]\tLoss: 424.1859\n",
      "Training Epoch: 17 [35136/49669]\tLoss: 384.9190\n",
      "Training Epoch: 17 [35200/49669]\tLoss: 407.6139\n",
      "Training Epoch: 17 [35264/49669]\tLoss: 391.9529\n",
      "Training Epoch: 17 [35328/49669]\tLoss: 403.2668\n",
      "Training Epoch: 17 [35392/49669]\tLoss: 422.4448\n",
      "Training Epoch: 17 [35456/49669]\tLoss: 407.6422\n",
      "Training Epoch: 17 [35520/49669]\tLoss: 398.1536\n",
      "Training Epoch: 17 [35584/49669]\tLoss: 398.2299\n",
      "Training Epoch: 17 [35648/49669]\tLoss: 434.3932\n",
      "Training Epoch: 17 [35712/49669]\tLoss: 382.5705\n",
      "Training Epoch: 17 [35776/49669]\tLoss: 430.0476\n",
      "Training Epoch: 17 [35840/49669]\tLoss: 440.1064\n",
      "Training Epoch: 17 [35904/49669]\tLoss: 410.4958\n",
      "Training Epoch: 17 [35968/49669]\tLoss: 412.6772\n",
      "Training Epoch: 17 [36032/49669]\tLoss: 392.5449\n",
      "Training Epoch: 17 [36096/49669]\tLoss: 392.3624\n",
      "Training Epoch: 17 [36160/49669]\tLoss: 408.3105\n",
      "Training Epoch: 17 [36224/49669]\tLoss: 403.3518\n",
      "Training Epoch: 17 [36288/49669]\tLoss: 414.1832\n",
      "Training Epoch: 17 [36352/49669]\tLoss: 407.1173\n",
      "Training Epoch: 17 [36416/49669]\tLoss: 416.5550\n",
      "Training Epoch: 17 [36480/49669]\tLoss: 407.5495\n",
      "Training Epoch: 17 [36544/49669]\tLoss: 391.0835\n",
      "Training Epoch: 17 [36608/49669]\tLoss: 410.0071\n",
      "Training Epoch: 17 [36672/49669]\tLoss: 401.2585\n",
      "Training Epoch: 17 [36736/49669]\tLoss: 420.7814\n",
      "Training Epoch: 17 [36800/49669]\tLoss: 433.3588\n",
      "Training Epoch: 17 [36864/49669]\tLoss: 393.8200\n",
      "Training Epoch: 17 [36928/49669]\tLoss: 393.9407\n",
      "Training Epoch: 17 [36992/49669]\tLoss: 393.0658\n",
      "Training Epoch: 17 [37056/49669]\tLoss: 396.3933\n",
      "Training Epoch: 17 [37120/49669]\tLoss: 410.3835\n",
      "Training Epoch: 17 [37184/49669]\tLoss: 415.6760\n",
      "Training Epoch: 17 [37248/49669]\tLoss: 435.5076\n",
      "Training Epoch: 17 [37312/49669]\tLoss: 378.7366\n",
      "Training Epoch: 17 [37376/49669]\tLoss: 433.7757\n",
      "Training Epoch: 17 [37440/49669]\tLoss: 412.2488\n",
      "Training Epoch: 17 [37504/49669]\tLoss: 398.7942\n",
      "Training Epoch: 17 [37568/49669]\tLoss: 402.0356\n",
      "Training Epoch: 17 [37632/49669]\tLoss: 419.4957\n",
      "Training Epoch: 17 [37696/49669]\tLoss: 425.0047\n",
      "Training Epoch: 17 [37760/49669]\tLoss: 406.1650\n",
      "Training Epoch: 17 [37824/49669]\tLoss: 398.9492\n",
      "Training Epoch: 17 [37888/49669]\tLoss: 431.3829\n",
      "Training Epoch: 17 [37952/49669]\tLoss: 424.0908\n",
      "Training Epoch: 17 [38016/49669]\tLoss: 418.9759\n",
      "Training Epoch: 17 [38080/49669]\tLoss: 401.0400\n",
      "Training Epoch: 17 [38144/49669]\tLoss: 427.3692\n",
      "Training Epoch: 17 [38208/49669]\tLoss: 404.0587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [38272/49669]\tLoss: 419.7924\n",
      "Training Epoch: 17 [38336/49669]\tLoss: 412.5309\n",
      "Training Epoch: 17 [38400/49669]\tLoss: 409.9308\n",
      "Training Epoch: 17 [38464/49669]\tLoss: 427.8291\n",
      "Training Epoch: 17 [38528/49669]\tLoss: 424.4832\n",
      "Training Epoch: 17 [38592/49669]\tLoss: 433.7889\n",
      "Training Epoch: 17 [38656/49669]\tLoss: 422.3336\n",
      "Training Epoch: 17 [38720/49669]\tLoss: 373.0226\n",
      "Training Epoch: 17 [38784/49669]\tLoss: 421.7800\n",
      "Training Epoch: 17 [38848/49669]\tLoss: 435.9857\n",
      "Training Epoch: 17 [38912/49669]\tLoss: 416.6458\n",
      "Training Epoch: 17 [38976/49669]\tLoss: 429.1681\n",
      "Training Epoch: 17 [39040/49669]\tLoss: 409.0947\n",
      "Training Epoch: 17 [39104/49669]\tLoss: 427.8147\n",
      "Training Epoch: 17 [39168/49669]\tLoss: 407.7544\n",
      "Training Epoch: 17 [39232/49669]\tLoss: 450.8953\n",
      "Training Epoch: 17 [39296/49669]\tLoss: 360.9962\n",
      "Training Epoch: 17 [39360/49669]\tLoss: 425.5471\n",
      "Training Epoch: 17 [39424/49669]\tLoss: 418.6037\n",
      "Training Epoch: 17 [39488/49669]\tLoss: 410.2794\n",
      "Training Epoch: 17 [39552/49669]\tLoss: 420.8203\n",
      "Training Epoch: 17 [39616/49669]\tLoss: 401.4973\n",
      "Training Epoch: 17 [39680/49669]\tLoss: 430.8720\n",
      "Training Epoch: 17 [39744/49669]\tLoss: 381.9524\n",
      "Training Epoch: 17 [39808/49669]\tLoss: 413.9868\n",
      "Training Epoch: 17 [39872/49669]\tLoss: 432.1729\n",
      "Training Epoch: 17 [39936/49669]\tLoss: 408.9727\n",
      "Training Epoch: 17 [40000/49669]\tLoss: 373.3573\n",
      "Training Epoch: 17 [40064/49669]\tLoss: 438.6871\n",
      "Training Epoch: 17 [40128/49669]\tLoss: 428.5957\n",
      "Training Epoch: 17 [40192/49669]\tLoss: 424.1078\n",
      "Training Epoch: 17 [40256/49669]\tLoss: 403.5305\n",
      "Training Epoch: 17 [40320/49669]\tLoss: 416.3203\n",
      "Training Epoch: 17 [40384/49669]\tLoss: 396.9889\n",
      "Training Epoch: 17 [40448/49669]\tLoss: 417.3845\n",
      "Training Epoch: 17 [40512/49669]\tLoss: 419.4153\n",
      "Training Epoch: 17 [40576/49669]\tLoss: 411.7520\n",
      "Training Epoch: 17 [40640/49669]\tLoss: 400.7351\n",
      "Training Epoch: 17 [40704/49669]\tLoss: 400.0235\n",
      "Training Epoch: 17 [40768/49669]\tLoss: 407.2443\n",
      "Training Epoch: 17 [40832/49669]\tLoss: 412.7239\n",
      "Training Epoch: 17 [40896/49669]\tLoss: 395.1098\n",
      "Training Epoch: 17 [40960/49669]\tLoss: 410.7177\n",
      "Training Epoch: 17 [41024/49669]\tLoss: 394.1936\n",
      "Training Epoch: 17 [41088/49669]\tLoss: 386.3369\n",
      "Training Epoch: 17 [41152/49669]\tLoss: 422.4024\n",
      "Training Epoch: 17 [41216/49669]\tLoss: 420.4471\n",
      "Training Epoch: 17 [41280/49669]\tLoss: 396.5554\n",
      "Training Epoch: 17 [41344/49669]\tLoss: 412.9174\n",
      "Training Epoch: 17 [41408/49669]\tLoss: 409.3112\n",
      "Training Epoch: 17 [41472/49669]\tLoss: 405.7656\n",
      "Training Epoch: 17 [41536/49669]\tLoss: 411.7648\n",
      "Training Epoch: 17 [41600/49669]\tLoss: 415.4087\n",
      "Training Epoch: 17 [41664/49669]\tLoss: 424.3489\n",
      "Training Epoch: 17 [41728/49669]\tLoss: 411.8429\n",
      "Training Epoch: 17 [41792/49669]\tLoss: 400.7944\n",
      "Training Epoch: 17 [41856/49669]\tLoss: 421.5846\n",
      "Training Epoch: 17 [41920/49669]\tLoss: 439.5836\n",
      "Training Epoch: 17 [41984/49669]\tLoss: 426.4029\n",
      "Training Epoch: 17 [42048/49669]\tLoss: 424.5284\n",
      "Training Epoch: 17 [42112/49669]\tLoss: 382.8236\n",
      "Training Epoch: 17 [42176/49669]\tLoss: 433.1482\n",
      "Training Epoch: 17 [42240/49669]\tLoss: 393.3019\n",
      "Training Epoch: 17 [42304/49669]\tLoss: 403.6339\n",
      "Training Epoch: 17 [42368/49669]\tLoss: 406.4247\n",
      "Training Epoch: 17 [42432/49669]\tLoss: 389.1938\n",
      "Training Epoch: 17 [42496/49669]\tLoss: 423.8673\n",
      "Training Epoch: 17 [42560/49669]\tLoss: 423.6444\n",
      "Training Epoch: 17 [42624/49669]\tLoss: 404.7001\n",
      "Training Epoch: 17 [42688/49669]\tLoss: 417.1695\n",
      "Training Epoch: 17 [42752/49669]\tLoss: 417.1343\n",
      "Training Epoch: 17 [42816/49669]\tLoss: 428.3100\n",
      "Training Epoch: 17 [42880/49669]\tLoss: 424.9044\n",
      "Training Epoch: 17 [42944/49669]\tLoss: 418.4555\n",
      "Training Epoch: 17 [43008/49669]\tLoss: 376.8130\n",
      "Training Epoch: 17 [43072/49669]\tLoss: 397.2348\n",
      "Training Epoch: 17 [43136/49669]\tLoss: 393.5324\n",
      "Training Epoch: 17 [43200/49669]\tLoss: 399.0562\n",
      "Training Epoch: 17 [43264/49669]\tLoss: 412.3680\n",
      "Training Epoch: 17 [43328/49669]\tLoss: 388.2336\n",
      "Training Epoch: 17 [43392/49669]\tLoss: 395.8104\n",
      "Training Epoch: 17 [43456/49669]\tLoss: 425.5814\n",
      "Training Epoch: 17 [43520/49669]\tLoss: 396.3014\n",
      "Training Epoch: 17 [43584/49669]\tLoss: 425.8805\n",
      "Training Epoch: 17 [43648/49669]\tLoss: 384.9775\n",
      "Training Epoch: 17 [43712/49669]\tLoss: 412.2979\n",
      "Training Epoch: 17 [43776/49669]\tLoss: 425.0573\n",
      "Training Epoch: 17 [43840/49669]\tLoss: 380.9536\n",
      "Training Epoch: 17 [43904/49669]\tLoss: 420.2378\n",
      "Training Epoch: 17 [43968/49669]\tLoss: 413.9397\n",
      "Training Epoch: 17 [44032/49669]\tLoss: 421.3555\n",
      "Training Epoch: 17 [44096/49669]\tLoss: 411.3094\n",
      "Training Epoch: 17 [44160/49669]\tLoss: 435.7404\n",
      "Training Epoch: 17 [44224/49669]\tLoss: 419.1141\n",
      "Training Epoch: 17 [44288/49669]\tLoss: 433.8036\n",
      "Training Epoch: 17 [44352/49669]\tLoss: 401.0969\n",
      "Training Epoch: 17 [44416/49669]\tLoss: 433.9399\n",
      "Training Epoch: 17 [44480/49669]\tLoss: 430.2128\n",
      "Training Epoch: 17 [44544/49669]\tLoss: 418.6349\n",
      "Training Epoch: 17 [44608/49669]\tLoss: 389.2820\n",
      "Training Epoch: 17 [44672/49669]\tLoss: 430.5103\n",
      "Training Epoch: 17 [44736/49669]\tLoss: 385.6963\n",
      "Training Epoch: 17 [44800/49669]\tLoss: 383.0596\n",
      "Training Epoch: 17 [44864/49669]\tLoss: 428.8279\n",
      "Training Epoch: 17 [44928/49669]\tLoss: 435.5896\n",
      "Training Epoch: 17 [44992/49669]\tLoss: 414.2988\n",
      "Training Epoch: 17 [45056/49669]\tLoss: 401.1586\n",
      "Training Epoch: 17 [45120/49669]\tLoss: 375.5183\n",
      "Training Epoch: 17 [45184/49669]\tLoss: 415.6435\n",
      "Training Epoch: 17 [45248/49669]\tLoss: 413.6386\n",
      "Training Epoch: 17 [45312/49669]\tLoss: 411.9385\n",
      "Training Epoch: 17 [45376/49669]\tLoss: 399.4023\n",
      "Training Epoch: 17 [45440/49669]\tLoss: 407.2554\n",
      "Training Epoch: 17 [45504/49669]\tLoss: 416.3332\n",
      "Training Epoch: 17 [45568/49669]\tLoss: 443.9340\n",
      "Training Epoch: 17 [45632/49669]\tLoss: 432.3665\n",
      "Training Epoch: 17 [45696/49669]\tLoss: 427.5034\n",
      "Training Epoch: 17 [45760/49669]\tLoss: 449.4055\n",
      "Training Epoch: 17 [45824/49669]\tLoss: 398.8338\n",
      "Training Epoch: 17 [45888/49669]\tLoss: 414.0370\n",
      "Training Epoch: 17 [45952/49669]\tLoss: 430.9316\n",
      "Training Epoch: 17 [46016/49669]\tLoss: 399.2112\n",
      "Training Epoch: 17 [46080/49669]\tLoss: 393.4195\n",
      "Training Epoch: 17 [46144/49669]\tLoss: 388.9154\n",
      "Training Epoch: 17 [46208/49669]\tLoss: 436.5952\n",
      "Training Epoch: 17 [46272/49669]\tLoss: 431.1512\n",
      "Training Epoch: 17 [46336/49669]\tLoss: 384.9142\n",
      "Training Epoch: 17 [46400/49669]\tLoss: 400.6441\n",
      "Training Epoch: 17 [46464/49669]\tLoss: 420.0631\n",
      "Training Epoch: 17 [46528/49669]\tLoss: 434.0143\n",
      "Training Epoch: 17 [46592/49669]\tLoss: 404.3633\n",
      "Training Epoch: 17 [46656/49669]\tLoss: 381.4068\n",
      "Training Epoch: 17 [46720/49669]\tLoss: 408.9655\n",
      "Training Epoch: 17 [46784/49669]\tLoss: 400.5331\n",
      "Training Epoch: 17 [46848/49669]\tLoss: 383.6446\n",
      "Training Epoch: 17 [46912/49669]\tLoss: 377.4292\n",
      "Training Epoch: 17 [46976/49669]\tLoss: 403.6083\n",
      "Training Epoch: 17 [47040/49669]\tLoss: 425.4902\n",
      "Training Epoch: 17 [47104/49669]\tLoss: 401.8060\n",
      "Training Epoch: 17 [47168/49669]\tLoss: 427.3705\n",
      "Training Epoch: 17 [47232/49669]\tLoss: 407.5439\n",
      "Training Epoch: 17 [47296/49669]\tLoss: 403.1716\n",
      "Training Epoch: 17 [47360/49669]\tLoss: 406.0137\n",
      "Training Epoch: 17 [47424/49669]\tLoss: 405.3076\n",
      "Training Epoch: 17 [47488/49669]\tLoss: 425.5627\n",
      "Training Epoch: 17 [47552/49669]\tLoss: 420.5185\n",
      "Training Epoch: 17 [47616/49669]\tLoss: 403.0092\n",
      "Training Epoch: 17 [47680/49669]\tLoss: 404.6059\n",
      "Training Epoch: 17 [47744/49669]\tLoss: 405.7473\n",
      "Training Epoch: 17 [47808/49669]\tLoss: 371.1955\n",
      "Training Epoch: 17 [47872/49669]\tLoss: 403.8965\n",
      "Training Epoch: 17 [47936/49669]\tLoss: 384.0882\n",
      "Training Epoch: 17 [48000/49669]\tLoss: 412.5229\n",
      "Training Epoch: 17 [48064/49669]\tLoss: 391.2979\n",
      "Training Epoch: 17 [48128/49669]\tLoss: 419.6046\n",
      "Training Epoch: 17 [48192/49669]\tLoss: 425.4318\n",
      "Training Epoch: 17 [48256/49669]\tLoss: 450.8043\n",
      "Training Epoch: 17 [48320/49669]\tLoss: 418.0378\n",
      "Training Epoch: 17 [48384/49669]\tLoss: 412.0544\n",
      "Training Epoch: 17 [48448/49669]\tLoss: 406.5963\n",
      "Training Epoch: 17 [48512/49669]\tLoss: 401.3932\n",
      "Training Epoch: 17 [48576/49669]\tLoss: 419.2206\n",
      "Training Epoch: 17 [48640/49669]\tLoss: 424.0334\n",
      "Training Epoch: 17 [48704/49669]\tLoss: 408.8546\n",
      "Training Epoch: 17 [48768/49669]\tLoss: 420.9093\n",
      "Training Epoch: 17 [48832/49669]\tLoss: 412.8384\n",
      "Training Epoch: 17 [48896/49669]\tLoss: 394.0845\n",
      "Training Epoch: 17 [48960/49669]\tLoss: 407.1352\n",
      "Training Epoch: 17 [49024/49669]\tLoss: 388.9839\n",
      "Training Epoch: 17 [49088/49669]\tLoss: 414.7657\n",
      "Training Epoch: 17 [49152/49669]\tLoss: 415.0579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [49216/49669]\tLoss: 416.1527\n",
      "Training Epoch: 17 [49280/49669]\tLoss: 402.2229\n",
      "Training Epoch: 17 [49344/49669]\tLoss: 419.4390\n",
      "Training Epoch: 17 [49408/49669]\tLoss: 421.0156\n",
      "Training Epoch: 17 [49472/49669]\tLoss: 404.5939\n",
      "Training Epoch: 17 [49536/49669]\tLoss: 423.8600\n",
      "Training Epoch: 17 [49600/49669]\tLoss: 427.3626\n",
      "Training Epoch: 17 [49664/49669]\tLoss: 368.5282\n",
      "Training Epoch: 17 [49669/49669]\tLoss: 430.3513\n",
      "Training Epoch: 17 [5519/5519]\tLoss: 409.3862\n",
      "Training Epoch: 18 [64/49669]\tLoss: 423.3544\n",
      "Training Epoch: 18 [128/49669]\tLoss: 420.9583\n",
      "Training Epoch: 18 [192/49669]\tLoss: 401.2562\n",
      "Training Epoch: 18 [256/49669]\tLoss: 415.2713\n",
      "Training Epoch: 18 [320/49669]\tLoss: 395.0764\n",
      "Training Epoch: 18 [384/49669]\tLoss: 389.9983\n",
      "Training Epoch: 18 [448/49669]\tLoss: 405.0222\n",
      "Training Epoch: 18 [512/49669]\tLoss: 431.9248\n",
      "Training Epoch: 18 [576/49669]\tLoss: 393.2868\n",
      "Training Epoch: 18 [640/49669]\tLoss: 422.7526\n",
      "Training Epoch: 18 [704/49669]\tLoss: 391.4319\n",
      "Training Epoch: 18 [768/49669]\tLoss: 414.0213\n",
      "Training Epoch: 18 [832/49669]\tLoss: 389.7921\n",
      "Training Epoch: 18 [896/49669]\tLoss: 419.3653\n",
      "Training Epoch: 18 [960/49669]\tLoss: 379.3351\n",
      "Training Epoch: 18 [1024/49669]\tLoss: 440.2241\n",
      "Training Epoch: 18 [1088/49669]\tLoss: 417.6756\n",
      "Training Epoch: 18 [1152/49669]\tLoss: 428.5871\n",
      "Training Epoch: 18 [1216/49669]\tLoss: 389.3694\n",
      "Training Epoch: 18 [1280/49669]\tLoss: 411.3175\n",
      "Training Epoch: 18 [1344/49669]\tLoss: 386.5435\n",
      "Training Epoch: 18 [1408/49669]\tLoss: 377.9716\n",
      "Training Epoch: 18 [1472/49669]\tLoss: 417.2295\n",
      "Training Epoch: 18 [1536/49669]\tLoss: 425.4711\n",
      "Training Epoch: 18 [1600/49669]\tLoss: 385.3698\n",
      "Training Epoch: 18 [1664/49669]\tLoss: 398.0308\n",
      "Training Epoch: 18 [1728/49669]\tLoss: 423.0974\n",
      "Training Epoch: 18 [1792/49669]\tLoss: 408.5539\n",
      "Training Epoch: 18 [1856/49669]\tLoss: 387.5194\n",
      "Training Epoch: 18 [1920/49669]\tLoss: 392.5437\n",
      "Training Epoch: 18 [1984/49669]\tLoss: 413.5075\n",
      "Training Epoch: 18 [2048/49669]\tLoss: 401.7948\n",
      "Training Epoch: 18 [2112/49669]\tLoss: 391.8555\n",
      "Training Epoch: 18 [2176/49669]\tLoss: 409.9409\n",
      "Training Epoch: 18 [2240/49669]\tLoss: 384.4661\n",
      "Training Epoch: 18 [2304/49669]\tLoss: 425.5696\n",
      "Training Epoch: 18 [2368/49669]\tLoss: 418.8087\n",
      "Training Epoch: 18 [2432/49669]\tLoss: 435.1737\n",
      "Training Epoch: 18 [2496/49669]\tLoss: 413.3292\n",
      "Training Epoch: 18 [2560/49669]\tLoss: 422.9389\n",
      "Training Epoch: 18 [2624/49669]\tLoss: 376.7850\n",
      "Training Epoch: 18 [2688/49669]\tLoss: 431.2115\n",
      "Training Epoch: 18 [2752/49669]\tLoss: 428.9242\n",
      "Training Epoch: 18 [2816/49669]\tLoss: 429.2805\n",
      "Training Epoch: 18 [2880/49669]\tLoss: 402.7853\n",
      "Training Epoch: 18 [2944/49669]\tLoss: 398.8205\n",
      "Training Epoch: 18 [3008/49669]\tLoss: 390.0734\n",
      "Training Epoch: 18 [3072/49669]\tLoss: 412.1593\n",
      "Training Epoch: 18 [3136/49669]\tLoss: 447.7178\n",
      "Training Epoch: 18 [3200/49669]\tLoss: 415.5544\n",
      "Training Epoch: 18 [3264/49669]\tLoss: 435.5182\n",
      "Training Epoch: 18 [3328/49669]\tLoss: 426.8750\n",
      "Training Epoch: 18 [3392/49669]\tLoss: 415.3539\n",
      "Training Epoch: 18 [3456/49669]\tLoss: 413.8754\n",
      "Training Epoch: 18 [3520/49669]\tLoss: 437.8910\n",
      "Training Epoch: 18 [3584/49669]\tLoss: 436.6380\n",
      "Training Epoch: 18 [3648/49669]\tLoss: 440.6466\n",
      "Training Epoch: 18 [3712/49669]\tLoss: 415.5135\n",
      "Training Epoch: 18 [3776/49669]\tLoss: 416.4951\n",
      "Training Epoch: 18 [3840/49669]\tLoss: 410.0052\n",
      "Training Epoch: 18 [3904/49669]\tLoss: 393.3855\n",
      "Training Epoch: 18 [3968/49669]\tLoss: 378.4176\n",
      "Training Epoch: 18 [4032/49669]\tLoss: 418.5244\n",
      "Training Epoch: 18 [4096/49669]\tLoss: 409.9672\n",
      "Training Epoch: 18 [4160/49669]\tLoss: 418.8141\n",
      "Training Epoch: 18 [4224/49669]\tLoss: 401.9187\n",
      "Training Epoch: 18 [4288/49669]\tLoss: 387.5223\n",
      "Training Epoch: 18 [4352/49669]\tLoss: 417.1648\n",
      "Training Epoch: 18 [4416/49669]\tLoss: 408.7603\n",
      "Training Epoch: 18 [4480/49669]\tLoss: 413.0204\n",
      "Training Epoch: 18 [4544/49669]\tLoss: 425.1065\n",
      "Training Epoch: 18 [4608/49669]\tLoss: 419.1298\n",
      "Training Epoch: 18 [4672/49669]\tLoss: 389.5111\n",
      "Training Epoch: 18 [4736/49669]\tLoss: 428.7805\n",
      "Training Epoch: 18 [4800/49669]\tLoss: 397.1111\n",
      "Training Epoch: 18 [4864/49669]\tLoss: 421.4274\n",
      "Training Epoch: 18 [4928/49669]\tLoss: 379.5127\n",
      "Training Epoch: 18 [4992/49669]\tLoss: 414.5695\n",
      "Training Epoch: 18 [5056/49669]\tLoss: 407.0212\n",
      "Training Epoch: 18 [5120/49669]\tLoss: 410.8309\n",
      "Training Epoch: 18 [5184/49669]\tLoss: 430.2398\n",
      "Training Epoch: 18 [5248/49669]\tLoss: 402.5927\n",
      "Training Epoch: 18 [5312/49669]\tLoss: 401.8733\n",
      "Training Epoch: 18 [5376/49669]\tLoss: 410.5369\n",
      "Training Epoch: 18 [5440/49669]\tLoss: 417.1100\n",
      "Training Epoch: 18 [5504/49669]\tLoss: 411.8002\n",
      "Training Epoch: 18 [5568/49669]\tLoss: 425.5099\n",
      "Training Epoch: 18 [5632/49669]\tLoss: 414.7453\n",
      "Training Epoch: 18 [5696/49669]\tLoss: 413.5134\n",
      "Training Epoch: 18 [5760/49669]\tLoss: 386.4765\n",
      "Training Epoch: 18 [5824/49669]\tLoss: 375.0281\n",
      "Training Epoch: 18 [5888/49669]\tLoss: 404.0405\n",
      "Training Epoch: 18 [5952/49669]\tLoss: 412.9315\n",
      "Training Epoch: 18 [6016/49669]\tLoss: 407.5627\n",
      "Training Epoch: 18 [6080/49669]\tLoss: 383.2847\n",
      "Training Epoch: 18 [6144/49669]\tLoss: 413.4675\n",
      "Training Epoch: 18 [6208/49669]\tLoss: 423.2110\n",
      "Training Epoch: 18 [6272/49669]\tLoss: 405.7743\n",
      "Training Epoch: 18 [6336/49669]\tLoss: 410.3586\n",
      "Training Epoch: 18 [6400/49669]\tLoss: 426.8713\n",
      "Training Epoch: 18 [6464/49669]\tLoss: 422.6715\n",
      "Training Epoch: 18 [6528/49669]\tLoss: 394.0885\n",
      "Training Epoch: 18 [6592/49669]\tLoss: 406.4084\n",
      "Training Epoch: 18 [6656/49669]\tLoss: 439.6025\n",
      "Training Epoch: 18 [6720/49669]\tLoss: 413.3209\n",
      "Training Epoch: 18 [6784/49669]\tLoss: 382.9823\n",
      "Training Epoch: 18 [6848/49669]\tLoss: 425.0933\n",
      "Training Epoch: 18 [6912/49669]\tLoss: 417.9499\n",
      "Training Epoch: 18 [6976/49669]\tLoss: 407.1037\n",
      "Training Epoch: 18 [7040/49669]\tLoss: 416.1502\n",
      "Training Epoch: 18 [7104/49669]\tLoss: 424.8847\n",
      "Training Epoch: 18 [7168/49669]\tLoss: 406.4426\n",
      "Training Epoch: 18 [7232/49669]\tLoss: 422.9064\n",
      "Training Epoch: 18 [7296/49669]\tLoss: 401.5682\n",
      "Training Epoch: 18 [7360/49669]\tLoss: 401.2932\n",
      "Training Epoch: 18 [7424/49669]\tLoss: 390.4513\n",
      "Training Epoch: 18 [7488/49669]\tLoss: 399.3457\n",
      "Training Epoch: 18 [7552/49669]\tLoss: 389.6803\n",
      "Training Epoch: 18 [7616/49669]\tLoss: 390.4706\n",
      "Training Epoch: 18 [7680/49669]\tLoss: 412.1130\n",
      "Training Epoch: 18 [7744/49669]\tLoss: 431.5121\n",
      "Training Epoch: 18 [7808/49669]\tLoss: 395.9146\n",
      "Training Epoch: 18 [7872/49669]\tLoss: 407.3443\n",
      "Training Epoch: 18 [7936/49669]\tLoss: 391.9492\n",
      "Training Epoch: 18 [8000/49669]\tLoss: 386.7033\n",
      "Training Epoch: 18 [8064/49669]\tLoss: 413.8824\n",
      "Training Epoch: 18 [8128/49669]\tLoss: 397.4314\n",
      "Training Epoch: 18 [8192/49669]\tLoss: 423.3171\n",
      "Training Epoch: 18 [8256/49669]\tLoss: 406.6699\n",
      "Training Epoch: 18 [8320/49669]\tLoss: 406.3962\n",
      "Training Epoch: 18 [8384/49669]\tLoss: 422.0703\n",
      "Training Epoch: 18 [8448/49669]\tLoss: 420.3498\n",
      "Training Epoch: 18 [8512/49669]\tLoss: 432.5404\n",
      "Training Epoch: 18 [8576/49669]\tLoss: 425.6416\n",
      "Training Epoch: 18 [8640/49669]\tLoss: 424.1379\n",
      "Training Epoch: 18 [8704/49669]\tLoss: 407.7361\n",
      "Training Epoch: 18 [8768/49669]\tLoss: 426.3731\n",
      "Training Epoch: 18 [8832/49669]\tLoss: 409.3391\n",
      "Training Epoch: 18 [8896/49669]\tLoss: 417.2732\n",
      "Training Epoch: 18 [8960/49669]\tLoss: 408.3590\n",
      "Training Epoch: 18 [9024/49669]\tLoss: 378.2295\n",
      "Training Epoch: 18 [9088/49669]\tLoss: 395.3190\n",
      "Training Epoch: 18 [9152/49669]\tLoss: 436.3664\n",
      "Training Epoch: 18 [9216/49669]\tLoss: 384.3793\n",
      "Training Epoch: 18 [9280/49669]\tLoss: 371.3891\n",
      "Training Epoch: 18 [9344/49669]\tLoss: 416.0587\n",
      "Training Epoch: 18 [9408/49669]\tLoss: 394.7635\n",
      "Training Epoch: 18 [9472/49669]\tLoss: 369.3930\n",
      "Training Epoch: 18 [9536/49669]\tLoss: 418.6845\n",
      "Training Epoch: 18 [9600/49669]\tLoss: 429.6089\n",
      "Training Epoch: 18 [9664/49669]\tLoss: 419.2463\n",
      "Training Epoch: 18 [9728/49669]\tLoss: 407.5403\n",
      "Training Epoch: 18 [9792/49669]\tLoss: 415.0314\n",
      "Training Epoch: 18 [9856/49669]\tLoss: 402.1086\n",
      "Training Epoch: 18 [9920/49669]\tLoss: 422.4236\n",
      "Training Epoch: 18 [9984/49669]\tLoss: 389.5841\n",
      "Training Epoch: 18 [10048/49669]\tLoss: 417.6726\n",
      "Training Epoch: 18 [10112/49669]\tLoss: 400.5864\n",
      "Training Epoch: 18 [10176/49669]\tLoss: 359.0277\n",
      "Training Epoch: 18 [10240/49669]\tLoss: 386.7343\n",
      "Training Epoch: 18 [10304/49669]\tLoss: 419.1150\n",
      "Training Epoch: 18 [10368/49669]\tLoss: 386.1308\n",
      "Training Epoch: 18 [10432/49669]\tLoss: 384.5920\n",
      "Training Epoch: 18 [10496/49669]\tLoss: 419.9645\n",
      "Training Epoch: 18 [10560/49669]\tLoss: 423.6616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [10624/49669]\tLoss: 379.1293\n",
      "Training Epoch: 18 [10688/49669]\tLoss: 412.3427\n",
      "Training Epoch: 18 [10752/49669]\tLoss: 381.8291\n",
      "Training Epoch: 18 [10816/49669]\tLoss: 440.3646\n",
      "Training Epoch: 18 [10880/49669]\tLoss: 395.8050\n",
      "Training Epoch: 18 [10944/49669]\tLoss: 389.2345\n",
      "Training Epoch: 18 [11008/49669]\tLoss: 400.8783\n",
      "Training Epoch: 18 [11072/49669]\tLoss: 383.1504\n",
      "Training Epoch: 18 [11136/49669]\tLoss: 394.8079\n",
      "Training Epoch: 18 [11200/49669]\tLoss: 425.0247\n",
      "Training Epoch: 18 [11264/49669]\tLoss: 426.7114\n",
      "Training Epoch: 18 [11328/49669]\tLoss: 432.6698\n",
      "Training Epoch: 18 [11392/49669]\tLoss: 401.7928\n",
      "Training Epoch: 18 [11456/49669]\tLoss: 405.2723\n",
      "Training Epoch: 18 [11520/49669]\tLoss: 412.0025\n",
      "Training Epoch: 18 [11584/49669]\tLoss: 412.3267\n",
      "Training Epoch: 18 [11648/49669]\tLoss: 396.9604\n",
      "Training Epoch: 18 [11712/49669]\tLoss: 406.3047\n",
      "Training Epoch: 18 [11776/49669]\tLoss: 406.4934\n",
      "Training Epoch: 18 [11840/49669]\tLoss: 417.3908\n",
      "Training Epoch: 18 [11904/49669]\tLoss: 401.8944\n",
      "Training Epoch: 18 [11968/49669]\tLoss: 417.7622\n",
      "Training Epoch: 18 [12032/49669]\tLoss: 412.0826\n",
      "Training Epoch: 18 [12096/49669]\tLoss: 397.0863\n",
      "Training Epoch: 18 [12160/49669]\tLoss: 396.7001\n",
      "Training Epoch: 18 [12224/49669]\tLoss: 400.1777\n",
      "Training Epoch: 18 [12288/49669]\tLoss: 432.0900\n",
      "Training Epoch: 18 [12352/49669]\tLoss: 384.4120\n",
      "Training Epoch: 18 [12416/49669]\tLoss: 391.1940\n",
      "Training Epoch: 18 [12480/49669]\tLoss: 414.8174\n",
      "Training Epoch: 18 [12544/49669]\tLoss: 408.6918\n",
      "Training Epoch: 18 [12608/49669]\tLoss: 392.3817\n",
      "Training Epoch: 18 [12672/49669]\tLoss: 435.6441\n",
      "Training Epoch: 18 [12736/49669]\tLoss: 411.3704\n",
      "Training Epoch: 18 [12800/49669]\tLoss: 425.2180\n",
      "Training Epoch: 18 [12864/49669]\tLoss: 421.6403\n",
      "Training Epoch: 18 [12928/49669]\tLoss: 390.2241\n",
      "Training Epoch: 18 [12992/49669]\tLoss: 423.1291\n",
      "Training Epoch: 18 [13056/49669]\tLoss: 445.4751\n",
      "Training Epoch: 18 [13120/49669]\tLoss: 433.1583\n",
      "Training Epoch: 18 [13184/49669]\tLoss: 476.6217\n",
      "Training Epoch: 18 [13248/49669]\tLoss: 447.3439\n",
      "Training Epoch: 18 [13312/49669]\tLoss: 468.3567\n",
      "Training Epoch: 18 [13376/49669]\tLoss: 442.0527\n",
      "Training Epoch: 18 [13440/49669]\tLoss: 437.9240\n",
      "Training Epoch: 18 [13504/49669]\tLoss: 402.9669\n",
      "Training Epoch: 18 [13568/49669]\tLoss: 391.4460\n",
      "Training Epoch: 18 [13632/49669]\tLoss: 418.3322\n",
      "Training Epoch: 18 [13696/49669]\tLoss: 447.2455\n",
      "Training Epoch: 18 [13760/49669]\tLoss: 445.1264\n",
      "Training Epoch: 18 [13824/49669]\tLoss: 433.8406\n",
      "Training Epoch: 18 [13888/49669]\tLoss: 415.2565\n",
      "Training Epoch: 18 [13952/49669]\tLoss: 437.6850\n",
      "Training Epoch: 18 [14016/49669]\tLoss: 451.7102\n",
      "Training Epoch: 18 [14080/49669]\tLoss: 405.8080\n",
      "Training Epoch: 18 [14144/49669]\tLoss: 418.5309\n",
      "Training Epoch: 18 [14208/49669]\tLoss: 426.8292\n",
      "Training Epoch: 18 [14272/49669]\tLoss: 419.7840\n",
      "Training Epoch: 18 [14336/49669]\tLoss: 419.2787\n",
      "Training Epoch: 18 [14400/49669]\tLoss: 419.1845\n",
      "Training Epoch: 18 [14464/49669]\tLoss: 355.5750\n",
      "Training Epoch: 18 [14528/49669]\tLoss: 436.2051\n",
      "Training Epoch: 18 [14592/49669]\tLoss: 410.2592\n",
      "Training Epoch: 18 [14656/49669]\tLoss: 431.5502\n",
      "Training Epoch: 18 [14720/49669]\tLoss: 389.4376\n",
      "Training Epoch: 18 [14784/49669]\tLoss: 399.6973\n",
      "Training Epoch: 18 [14848/49669]\tLoss: 395.4281\n",
      "Training Epoch: 18 [14912/49669]\tLoss: 394.7731\n",
      "Training Epoch: 18 [14976/49669]\tLoss: 432.9379\n",
      "Training Epoch: 18 [15040/49669]\tLoss: 435.3518\n",
      "Training Epoch: 18 [15104/49669]\tLoss: 436.5303\n",
      "Training Epoch: 18 [15168/49669]\tLoss: 405.4277\n",
      "Training Epoch: 18 [15232/49669]\tLoss: 414.7278\n",
      "Training Epoch: 18 [15296/49669]\tLoss: 362.3988\n",
      "Training Epoch: 18 [15360/49669]\tLoss: 397.2021\n",
      "Training Epoch: 18 [15424/49669]\tLoss: 395.9316\n",
      "Training Epoch: 18 [15488/49669]\tLoss: 417.1920\n",
      "Training Epoch: 18 [15552/49669]\tLoss: 424.5389\n",
      "Training Epoch: 18 [15616/49669]\tLoss: 417.3332\n",
      "Training Epoch: 18 [15680/49669]\tLoss: 412.3058\n",
      "Training Epoch: 18 [15744/49669]\tLoss: 413.4872\n",
      "Training Epoch: 18 [15808/49669]\tLoss: 433.5020\n",
      "Training Epoch: 18 [15872/49669]\tLoss: 426.1599\n",
      "Training Epoch: 18 [15936/49669]\tLoss: 439.6590\n",
      "Training Epoch: 18 [16000/49669]\tLoss: 422.6089\n",
      "Training Epoch: 18 [16064/49669]\tLoss: 408.3223\n",
      "Training Epoch: 18 [16128/49669]\tLoss: 420.5691\n",
      "Training Epoch: 18 [16192/49669]\tLoss: 400.9704\n",
      "Training Epoch: 18 [16256/49669]\tLoss: 400.2937\n",
      "Training Epoch: 18 [16320/49669]\tLoss: 399.1651\n",
      "Training Epoch: 18 [16384/49669]\tLoss: 386.7722\n",
      "Training Epoch: 18 [16448/49669]\tLoss: 426.0382\n",
      "Training Epoch: 18 [16512/49669]\tLoss: 403.5987\n",
      "Training Epoch: 18 [16576/49669]\tLoss: 411.3423\n",
      "Training Epoch: 18 [16640/49669]\tLoss: 397.4261\n",
      "Training Epoch: 18 [16704/49669]\tLoss: 400.0970\n",
      "Training Epoch: 18 [16768/49669]\tLoss: 402.2637\n",
      "Training Epoch: 18 [16832/49669]\tLoss: 406.5428\n",
      "Training Epoch: 18 [16896/49669]\tLoss: 377.8030\n",
      "Training Epoch: 18 [16960/49669]\tLoss: 394.3811\n",
      "Training Epoch: 18 [17024/49669]\tLoss: 409.4448\n",
      "Training Epoch: 18 [17088/49669]\tLoss: 415.3878\n",
      "Training Epoch: 18 [17152/49669]\tLoss: 376.7682\n",
      "Training Epoch: 18 [17216/49669]\tLoss: 394.5581\n",
      "Training Epoch: 18 [17280/49669]\tLoss: 408.2880\n",
      "Training Epoch: 18 [17344/49669]\tLoss: 400.6039\n",
      "Training Epoch: 18 [17408/49669]\tLoss: 428.2562\n",
      "Training Epoch: 18 [17472/49669]\tLoss: 429.9503\n",
      "Training Epoch: 18 [17536/49669]\tLoss: 431.3027\n",
      "Training Epoch: 18 [17600/49669]\tLoss: 411.7773\n",
      "Training Epoch: 18 [17664/49669]\tLoss: 420.1259\n",
      "Training Epoch: 18 [17728/49669]\tLoss: 414.9966\n",
      "Training Epoch: 18 [17792/49669]\tLoss: 393.9088\n",
      "Training Epoch: 18 [17856/49669]\tLoss: 440.1054\n",
      "Training Epoch: 18 [17920/49669]\tLoss: 406.2861\n",
      "Training Epoch: 18 [17984/49669]\tLoss: 404.0462\n",
      "Training Epoch: 18 [18048/49669]\tLoss: 419.0701\n",
      "Training Epoch: 18 [18112/49669]\tLoss: 416.8540\n",
      "Training Epoch: 18 [18176/49669]\tLoss: 390.5338\n",
      "Training Epoch: 18 [18240/49669]\tLoss: 428.3133\n",
      "Training Epoch: 18 [18304/49669]\tLoss: 423.8416\n",
      "Training Epoch: 18 [18368/49669]\tLoss: 387.3165\n",
      "Training Epoch: 18 [18432/49669]\tLoss: 397.7803\n",
      "Training Epoch: 18 [18496/49669]\tLoss: 405.8343\n",
      "Training Epoch: 18 [18560/49669]\tLoss: 389.0309\n",
      "Training Epoch: 18 [18624/49669]\tLoss: 445.1068\n",
      "Training Epoch: 18 [18688/49669]\tLoss: 412.9029\n",
      "Training Epoch: 18 [18752/49669]\tLoss: 413.5212\n",
      "Training Epoch: 18 [18816/49669]\tLoss: 419.7510\n",
      "Training Epoch: 18 [18880/49669]\tLoss: 397.1175\n",
      "Training Epoch: 18 [18944/49669]\tLoss: 441.1787\n",
      "Training Epoch: 18 [19008/49669]\tLoss: 399.6978\n",
      "Training Epoch: 18 [19072/49669]\tLoss: 411.2621\n",
      "Training Epoch: 18 [19136/49669]\tLoss: 406.5035\n",
      "Training Epoch: 18 [19200/49669]\tLoss: 408.7365\n",
      "Training Epoch: 18 [19264/49669]\tLoss: 382.2291\n",
      "Training Epoch: 18 [19328/49669]\tLoss: 412.4709\n",
      "Training Epoch: 18 [19392/49669]\tLoss: 398.2714\n",
      "Training Epoch: 18 [19456/49669]\tLoss: 420.7021\n",
      "Training Epoch: 18 [19520/49669]\tLoss: 423.7068\n",
      "Training Epoch: 18 [19584/49669]\tLoss: 394.5277\n",
      "Training Epoch: 18 [19648/49669]\tLoss: 425.3628\n",
      "Training Epoch: 18 [19712/49669]\tLoss: 385.7853\n",
      "Training Epoch: 18 [19776/49669]\tLoss: 411.6457\n",
      "Training Epoch: 18 [19840/49669]\tLoss: 425.3063\n",
      "Training Epoch: 18 [19904/49669]\tLoss: 375.6733\n",
      "Training Epoch: 18 [19968/49669]\tLoss: 392.0736\n",
      "Training Epoch: 18 [20032/49669]\tLoss: 402.1512\n",
      "Training Epoch: 18 [20096/49669]\tLoss: 399.5047\n",
      "Training Epoch: 18 [20160/49669]\tLoss: 408.4546\n",
      "Training Epoch: 18 [20224/49669]\tLoss: 374.8060\n",
      "Training Epoch: 18 [20288/49669]\tLoss: 397.7338\n",
      "Training Epoch: 18 [20352/49669]\tLoss: 403.2520\n",
      "Training Epoch: 18 [20416/49669]\tLoss: 416.9492\n",
      "Training Epoch: 18 [20480/49669]\tLoss: 390.8802\n",
      "Training Epoch: 18 [20544/49669]\tLoss: 416.4285\n",
      "Training Epoch: 18 [20608/49669]\tLoss: 403.0174\n",
      "Training Epoch: 18 [20672/49669]\tLoss: 368.4065\n",
      "Training Epoch: 18 [20736/49669]\tLoss: 423.0532\n",
      "Training Epoch: 18 [20800/49669]\tLoss: 422.4321\n",
      "Training Epoch: 18 [20864/49669]\tLoss: 399.4683\n",
      "Training Epoch: 18 [20928/49669]\tLoss: 381.9643\n",
      "Training Epoch: 18 [20992/49669]\tLoss: 407.3244\n",
      "Training Epoch: 18 [21056/49669]\tLoss: 428.3470\n",
      "Training Epoch: 18 [21120/49669]\tLoss: 439.5970\n",
      "Training Epoch: 18 [21184/49669]\tLoss: 421.1378\n",
      "Training Epoch: 18 [21248/49669]\tLoss: 414.8535\n",
      "Training Epoch: 18 [21312/49669]\tLoss: 393.9211\n",
      "Training Epoch: 18 [21376/49669]\tLoss: 409.3961\n",
      "Training Epoch: 18 [21440/49669]\tLoss: 420.0279\n",
      "Training Epoch: 18 [21504/49669]\tLoss: 374.6057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [21568/49669]\tLoss: 411.8836\n",
      "Training Epoch: 18 [21632/49669]\tLoss: 409.9103\n",
      "Training Epoch: 18 [21696/49669]\tLoss: 420.1364\n",
      "Training Epoch: 18 [21760/49669]\tLoss: 381.8500\n",
      "Training Epoch: 18 [21824/49669]\tLoss: 410.3489\n",
      "Training Epoch: 18 [21888/49669]\tLoss: 392.8285\n",
      "Training Epoch: 18 [21952/49669]\tLoss: 387.1885\n",
      "Training Epoch: 18 [22016/49669]\tLoss: 401.7089\n",
      "Training Epoch: 18 [22080/49669]\tLoss: 410.5330\n",
      "Training Epoch: 18 [22144/49669]\tLoss: 411.6825\n",
      "Training Epoch: 18 [22208/49669]\tLoss: 391.5040\n",
      "Training Epoch: 18 [22272/49669]\tLoss: 405.4354\n",
      "Training Epoch: 18 [22336/49669]\tLoss: 391.2713\n",
      "Training Epoch: 18 [22400/49669]\tLoss: 411.9879\n",
      "Training Epoch: 18 [22464/49669]\tLoss: 426.6813\n",
      "Training Epoch: 18 [22528/49669]\tLoss: 422.4279\n",
      "Training Epoch: 18 [22592/49669]\tLoss: 400.4451\n",
      "Training Epoch: 18 [22656/49669]\tLoss: 386.5268\n",
      "Training Epoch: 18 [22720/49669]\tLoss: 393.9763\n",
      "Training Epoch: 18 [22784/49669]\tLoss: 415.1700\n",
      "Training Epoch: 18 [22848/49669]\tLoss: 411.2938\n",
      "Training Epoch: 18 [22912/49669]\tLoss: 406.7120\n",
      "Training Epoch: 18 [22976/49669]\tLoss: 403.7200\n",
      "Training Epoch: 18 [23040/49669]\tLoss: 365.3867\n",
      "Training Epoch: 18 [23104/49669]\tLoss: 452.8311\n",
      "Training Epoch: 18 [23168/49669]\tLoss: 388.1843\n",
      "Training Epoch: 18 [23232/49669]\tLoss: 398.9932\n",
      "Training Epoch: 18 [23296/49669]\tLoss: 386.5597\n",
      "Training Epoch: 18 [23360/49669]\tLoss: 400.6736\n",
      "Training Epoch: 18 [23424/49669]\tLoss: 412.3521\n",
      "Training Epoch: 18 [23488/49669]\tLoss: 406.3684\n",
      "Training Epoch: 18 [23552/49669]\tLoss: 413.8683\n",
      "Training Epoch: 18 [23616/49669]\tLoss: 392.3893\n",
      "Training Epoch: 18 [23680/49669]\tLoss: 411.3245\n",
      "Training Epoch: 18 [23744/49669]\tLoss: 370.9475\n",
      "Training Epoch: 18 [23808/49669]\tLoss: 395.9235\n",
      "Training Epoch: 18 [23872/49669]\tLoss: 427.6813\n",
      "Training Epoch: 18 [23936/49669]\tLoss: 419.9397\n",
      "Training Epoch: 18 [24000/49669]\tLoss: 363.7209\n",
      "Training Epoch: 18 [24064/49669]\tLoss: 428.3974\n",
      "Training Epoch: 18 [24128/49669]\tLoss: 404.0266\n",
      "Training Epoch: 18 [24192/49669]\tLoss: 430.1143\n",
      "Training Epoch: 18 [24256/49669]\tLoss: 406.9986\n",
      "Training Epoch: 18 [24320/49669]\tLoss: 389.0001\n",
      "Training Epoch: 18 [24384/49669]\tLoss: 386.3332\n",
      "Training Epoch: 18 [24448/49669]\tLoss: 422.7213\n",
      "Training Epoch: 18 [24512/49669]\tLoss: 424.0372\n",
      "Training Epoch: 18 [24576/49669]\tLoss: 416.0384\n",
      "Training Epoch: 18 [24640/49669]\tLoss: 444.0752\n",
      "Training Epoch: 18 [24704/49669]\tLoss: 412.7895\n",
      "Training Epoch: 18 [24768/49669]\tLoss: 425.1476\n",
      "Training Epoch: 18 [24832/49669]\tLoss: 382.6841\n",
      "Training Epoch: 18 [24896/49669]\tLoss: 430.4198\n",
      "Training Epoch: 18 [24960/49669]\tLoss: 397.3297\n",
      "Training Epoch: 18 [25024/49669]\tLoss: 398.4261\n",
      "Training Epoch: 18 [25088/49669]\tLoss: 420.1642\n",
      "Training Epoch: 18 [25152/49669]\tLoss: 386.6147\n",
      "Training Epoch: 18 [25216/49669]\tLoss: 392.4262\n",
      "Training Epoch: 18 [25280/49669]\tLoss: 400.8961\n",
      "Training Epoch: 18 [25344/49669]\tLoss: 409.4790\n",
      "Training Epoch: 18 [25408/49669]\tLoss: 407.9968\n",
      "Training Epoch: 18 [25472/49669]\tLoss: 411.7286\n",
      "Training Epoch: 18 [25536/49669]\tLoss: 386.7390\n",
      "Training Epoch: 18 [25600/49669]\tLoss: 420.8824\n",
      "Training Epoch: 18 [25664/49669]\tLoss: 403.8210\n",
      "Training Epoch: 18 [25728/49669]\tLoss: 408.7030\n",
      "Training Epoch: 18 [25792/49669]\tLoss: 416.8643\n",
      "Training Epoch: 18 [25856/49669]\tLoss: 390.2014\n",
      "Training Epoch: 18 [25920/49669]\tLoss: 441.0359\n",
      "Training Epoch: 18 [25984/49669]\tLoss: 376.4373\n",
      "Training Epoch: 18 [26048/49669]\tLoss: 416.1069\n",
      "Training Epoch: 18 [26112/49669]\tLoss: 411.8021\n",
      "Training Epoch: 18 [26176/49669]\tLoss: 403.4745\n",
      "Training Epoch: 18 [26240/49669]\tLoss: 415.1648\n",
      "Training Epoch: 18 [26304/49669]\tLoss: 434.5251\n",
      "Training Epoch: 18 [26368/49669]\tLoss: 377.3843\n",
      "Training Epoch: 18 [26432/49669]\tLoss: 444.4532\n",
      "Training Epoch: 18 [26496/49669]\tLoss: 415.5396\n",
      "Training Epoch: 18 [26560/49669]\tLoss: 423.7616\n",
      "Training Epoch: 18 [26624/49669]\tLoss: 406.3504\n",
      "Training Epoch: 18 [26688/49669]\tLoss: 393.7210\n",
      "Training Epoch: 18 [26752/49669]\tLoss: 437.0732\n",
      "Training Epoch: 18 [26816/49669]\tLoss: 393.5675\n",
      "Training Epoch: 18 [26880/49669]\tLoss: 383.0719\n",
      "Training Epoch: 18 [26944/49669]\tLoss: 392.4148\n",
      "Training Epoch: 18 [27008/49669]\tLoss: 400.7026\n",
      "Training Epoch: 18 [27072/49669]\tLoss: 422.6836\n",
      "Training Epoch: 18 [27136/49669]\tLoss: 404.6329\n",
      "Training Epoch: 18 [27200/49669]\tLoss: 402.2753\n",
      "Training Epoch: 18 [27264/49669]\tLoss: 427.8975\n",
      "Training Epoch: 18 [27328/49669]\tLoss: 422.4697\n",
      "Training Epoch: 18 [27392/49669]\tLoss: 415.3222\n",
      "Training Epoch: 18 [27456/49669]\tLoss: 412.6632\n",
      "Training Epoch: 18 [27520/49669]\tLoss: 393.7870\n",
      "Training Epoch: 18 [27584/49669]\tLoss: 449.5118\n",
      "Training Epoch: 18 [27648/49669]\tLoss: 445.7373\n",
      "Training Epoch: 18 [27712/49669]\tLoss: 404.2330\n",
      "Training Epoch: 18 [27776/49669]\tLoss: 448.8463\n",
      "Training Epoch: 18 [27840/49669]\tLoss: 424.5323\n",
      "Training Epoch: 18 [27904/49669]\tLoss: 447.1713\n",
      "Training Epoch: 18 [27968/49669]\tLoss: 450.9729\n",
      "Training Epoch: 18 [28032/49669]\tLoss: 428.0747\n",
      "Training Epoch: 18 [28096/49669]\tLoss: 437.9001\n",
      "Training Epoch: 18 [28160/49669]\tLoss: 400.7450\n",
      "Training Epoch: 18 [28224/49669]\tLoss: 416.7841\n",
      "Training Epoch: 18 [28288/49669]\tLoss: 414.1850\n",
      "Training Epoch: 18 [28352/49669]\tLoss: 396.9346\n",
      "Training Epoch: 18 [28416/49669]\tLoss: 414.2260\n",
      "Training Epoch: 18 [28480/49669]\tLoss: 419.6526\n",
      "Training Epoch: 18 [28544/49669]\tLoss: 450.4924\n",
      "Training Epoch: 18 [28608/49669]\tLoss: 425.1667\n",
      "Training Epoch: 18 [28672/49669]\tLoss: 410.5164\n",
      "Training Epoch: 18 [28736/49669]\tLoss: 399.9414\n",
      "Training Epoch: 18 [28800/49669]\tLoss: 429.8162\n",
      "Training Epoch: 18 [28864/49669]\tLoss: 428.4958\n",
      "Training Epoch: 18 [28928/49669]\tLoss: 408.7695\n",
      "Training Epoch: 18 [28992/49669]\tLoss: 437.3581\n",
      "Training Epoch: 18 [29056/49669]\tLoss: 404.2046\n",
      "Training Epoch: 18 [29120/49669]\tLoss: 414.5537\n",
      "Training Epoch: 18 [29184/49669]\tLoss: 447.8585\n",
      "Training Epoch: 18 [29248/49669]\tLoss: 389.2914\n",
      "Training Epoch: 18 [29312/49669]\tLoss: 396.5099\n",
      "Training Epoch: 18 [29376/49669]\tLoss: 413.0455\n",
      "Training Epoch: 18 [29440/49669]\tLoss: 375.9456\n",
      "Training Epoch: 18 [29504/49669]\tLoss: 418.2711\n",
      "Training Epoch: 18 [29568/49669]\tLoss: 391.3692\n",
      "Training Epoch: 18 [29632/49669]\tLoss: 397.2911\n",
      "Training Epoch: 18 [29696/49669]\tLoss: 407.9671\n",
      "Training Epoch: 18 [29760/49669]\tLoss: 398.7651\n",
      "Training Epoch: 18 [29824/49669]\tLoss: 421.0865\n",
      "Training Epoch: 18 [29888/49669]\tLoss: 424.6595\n",
      "Training Epoch: 18 [29952/49669]\tLoss: 389.4331\n",
      "Training Epoch: 18 [30016/49669]\tLoss: 388.2314\n",
      "Training Epoch: 18 [30080/49669]\tLoss: 411.9589\n",
      "Training Epoch: 18 [30144/49669]\tLoss: 418.6186\n",
      "Training Epoch: 18 [30208/49669]\tLoss: 414.1935\n",
      "Training Epoch: 18 [30272/49669]\tLoss: 435.9652\n",
      "Training Epoch: 18 [30336/49669]\tLoss: 437.9339\n",
      "Training Epoch: 18 [30400/49669]\tLoss: 386.4695\n",
      "Training Epoch: 18 [30464/49669]\tLoss: 407.2774\n",
      "Training Epoch: 18 [30528/49669]\tLoss: 420.0175\n",
      "Training Epoch: 18 [30592/49669]\tLoss: 388.7687\n",
      "Training Epoch: 18 [30656/49669]\tLoss: 437.8705\n",
      "Training Epoch: 18 [30720/49669]\tLoss: 400.2787\n",
      "Training Epoch: 18 [30784/49669]\tLoss: 387.0253\n",
      "Training Epoch: 18 [30848/49669]\tLoss: 394.6183\n",
      "Training Epoch: 18 [30912/49669]\tLoss: 390.9035\n",
      "Training Epoch: 18 [30976/49669]\tLoss: 427.1672\n",
      "Training Epoch: 18 [31040/49669]\tLoss: 409.9764\n",
      "Training Epoch: 18 [31104/49669]\tLoss: 422.2407\n",
      "Training Epoch: 18 [31168/49669]\tLoss: 395.1287\n",
      "Training Epoch: 18 [31232/49669]\tLoss: 425.2632\n",
      "Training Epoch: 18 [31296/49669]\tLoss: 393.4364\n",
      "Training Epoch: 18 [31360/49669]\tLoss: 411.4590\n",
      "Training Epoch: 18 [31424/49669]\tLoss: 398.9255\n",
      "Training Epoch: 18 [31488/49669]\tLoss: 408.7785\n",
      "Training Epoch: 18 [31552/49669]\tLoss: 396.0872\n",
      "Training Epoch: 18 [31616/49669]\tLoss: 429.4319\n",
      "Training Epoch: 18 [31680/49669]\tLoss: 416.3834\n",
      "Training Epoch: 18 [31744/49669]\tLoss: 444.7389\n",
      "Training Epoch: 18 [31808/49669]\tLoss: 397.9585\n",
      "Training Epoch: 18 [31872/49669]\tLoss: 377.4588\n",
      "Training Epoch: 18 [31936/49669]\tLoss: 418.1580\n",
      "Training Epoch: 18 [32000/49669]\tLoss: 406.5300\n",
      "Training Epoch: 18 [32064/49669]\tLoss: 428.4946\n",
      "Training Epoch: 18 [32128/49669]\tLoss: 406.3936\n",
      "Training Epoch: 18 [32192/49669]\tLoss: 388.7248\n",
      "Training Epoch: 18 [32256/49669]\tLoss: 404.9706\n",
      "Training Epoch: 18 [32320/49669]\tLoss: 409.9290\n",
      "Training Epoch: 18 [32384/49669]\tLoss: 424.8301\n",
      "Training Epoch: 18 [32448/49669]\tLoss: 384.9073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [32512/49669]\tLoss: 379.6035\n",
      "Training Epoch: 18 [32576/49669]\tLoss: 404.6833\n",
      "Training Epoch: 18 [32640/49669]\tLoss: 405.6820\n",
      "Training Epoch: 18 [32704/49669]\tLoss: 407.3270\n",
      "Training Epoch: 18 [32768/49669]\tLoss: 363.8099\n",
      "Training Epoch: 18 [32832/49669]\tLoss: 379.3427\n",
      "Training Epoch: 18 [32896/49669]\tLoss: 414.0713\n",
      "Training Epoch: 18 [32960/49669]\tLoss: 413.9153\n",
      "Training Epoch: 18 [33024/49669]\tLoss: 397.4875\n",
      "Training Epoch: 18 [33088/49669]\tLoss: 427.8060\n",
      "Training Epoch: 18 [33152/49669]\tLoss: 411.1041\n",
      "Training Epoch: 18 [33216/49669]\tLoss: 402.2279\n",
      "Training Epoch: 18 [33280/49669]\tLoss: 429.1225\n",
      "Training Epoch: 18 [33344/49669]\tLoss: 406.3039\n",
      "Training Epoch: 18 [33408/49669]\tLoss: 405.6077\n",
      "Training Epoch: 18 [33472/49669]\tLoss: 410.0626\n",
      "Training Epoch: 18 [33536/49669]\tLoss: 393.9221\n",
      "Training Epoch: 18 [33600/49669]\tLoss: 388.9115\n",
      "Training Epoch: 18 [33664/49669]\tLoss: 421.9802\n",
      "Training Epoch: 18 [33728/49669]\tLoss: 412.3792\n",
      "Training Epoch: 18 [33792/49669]\tLoss: 408.3943\n",
      "Training Epoch: 18 [33856/49669]\tLoss: 383.8890\n",
      "Training Epoch: 18 [33920/49669]\tLoss: 410.6977\n",
      "Training Epoch: 18 [33984/49669]\tLoss: 434.9597\n",
      "Training Epoch: 18 [34048/49669]\tLoss: 390.3185\n",
      "Training Epoch: 18 [34112/49669]\tLoss: 367.7494\n",
      "Training Epoch: 18 [34176/49669]\tLoss: 403.7664\n",
      "Training Epoch: 18 [34240/49669]\tLoss: 407.3169\n",
      "Training Epoch: 18 [34304/49669]\tLoss: 388.8783\n",
      "Training Epoch: 18 [34368/49669]\tLoss: 394.7495\n",
      "Training Epoch: 18 [34432/49669]\tLoss: 402.0739\n",
      "Training Epoch: 18 [34496/49669]\tLoss: 445.4040\n",
      "Training Epoch: 18 [34560/49669]\tLoss: 413.2561\n",
      "Training Epoch: 18 [34624/49669]\tLoss: 401.8608\n",
      "Training Epoch: 18 [34688/49669]\tLoss: 407.4572\n",
      "Training Epoch: 18 [34752/49669]\tLoss: 429.3498\n",
      "Training Epoch: 18 [34816/49669]\tLoss: 417.8571\n",
      "Training Epoch: 18 [34880/49669]\tLoss: 382.3165\n",
      "Training Epoch: 18 [34944/49669]\tLoss: 415.1124\n",
      "Training Epoch: 18 [35008/49669]\tLoss: 401.7081\n",
      "Training Epoch: 18 [35072/49669]\tLoss: 413.9509\n",
      "Training Epoch: 18 [35136/49669]\tLoss: 431.8484\n",
      "Training Epoch: 18 [35200/49669]\tLoss: 396.1497\n",
      "Training Epoch: 18 [35264/49669]\tLoss: 422.3951\n",
      "Training Epoch: 18 [35328/49669]\tLoss: 421.8955\n",
      "Training Epoch: 18 [35392/49669]\tLoss: 414.3715\n",
      "Training Epoch: 18 [35456/49669]\tLoss: 412.1032\n",
      "Training Epoch: 18 [35520/49669]\tLoss: 420.3176\n",
      "Training Epoch: 18 [35584/49669]\tLoss: 415.2795\n",
      "Training Epoch: 18 [35648/49669]\tLoss: 405.8798\n",
      "Training Epoch: 18 [35712/49669]\tLoss: 467.9087\n",
      "Training Epoch: 18 [35776/49669]\tLoss: 480.5406\n",
      "Training Epoch: 18 [35840/49669]\tLoss: 513.1012\n",
      "Training Epoch: 18 [35904/49669]\tLoss: 546.8777\n",
      "Training Epoch: 18 [35968/49669]\tLoss: 488.9149\n",
      "Training Epoch: 18 [36032/49669]\tLoss: 445.5526\n",
      "Training Epoch: 18 [36096/49669]\tLoss: 405.6216\n",
      "Training Epoch: 18 [36160/49669]\tLoss: 420.5501\n",
      "Training Epoch: 18 [36224/49669]\tLoss: 476.2171\n",
      "Training Epoch: 18 [36288/49669]\tLoss: 450.3919\n",
      "Training Epoch: 18 [36352/49669]\tLoss: 425.0253\n",
      "Training Epoch: 18 [36416/49669]\tLoss: 404.0192\n",
      "Training Epoch: 18 [36480/49669]\tLoss: 382.7019\n",
      "Training Epoch: 18 [36544/49669]\tLoss: 445.0431\n",
      "Training Epoch: 18 [36608/49669]\tLoss: 484.1026\n",
      "Training Epoch: 18 [36672/49669]\tLoss: 455.1129\n",
      "Training Epoch: 18 [36736/49669]\tLoss: 450.1229\n",
      "Training Epoch: 18 [36800/49669]\tLoss: 406.8278\n",
      "Training Epoch: 18 [36864/49669]\tLoss: 425.6131\n",
      "Training Epoch: 18 [36928/49669]\tLoss: 390.6769\n",
      "Training Epoch: 18 [36992/49669]\tLoss: 421.8260\n",
      "Training Epoch: 18 [37056/49669]\tLoss: 433.2226\n",
      "Training Epoch: 18 [37120/49669]\tLoss: 421.8528\n",
      "Training Epoch: 18 [37184/49669]\tLoss: 416.3083\n",
      "Training Epoch: 18 [37248/49669]\tLoss: 436.7684\n",
      "Training Epoch: 18 [37312/49669]\tLoss: 416.9510\n",
      "Training Epoch: 18 [37376/49669]\tLoss: 419.7953\n",
      "Training Epoch: 18 [37440/49669]\tLoss: 392.5284\n",
      "Training Epoch: 18 [37504/49669]\tLoss: 403.9881\n",
      "Training Epoch: 18 [37568/49669]\tLoss: 390.0957\n",
      "Training Epoch: 18 [37632/49669]\tLoss: 399.8048\n",
      "Training Epoch: 18 [37696/49669]\tLoss: 406.2797\n",
      "Training Epoch: 18 [37760/49669]\tLoss: 419.7129\n",
      "Training Epoch: 18 [37824/49669]\tLoss: 417.3243\n",
      "Training Epoch: 18 [37888/49669]\tLoss: 395.2144\n",
      "Training Epoch: 18 [37952/49669]\tLoss: 405.6481\n",
      "Training Epoch: 18 [38016/49669]\tLoss: 401.6664\n",
      "Training Epoch: 18 [38080/49669]\tLoss: 384.0287\n",
      "Training Epoch: 18 [38144/49669]\tLoss: 400.9691\n",
      "Training Epoch: 18 [38208/49669]\tLoss: 443.6064\n",
      "Training Epoch: 18 [38272/49669]\tLoss: 403.5580\n",
      "Training Epoch: 18 [38336/49669]\tLoss: 424.9721\n",
      "Training Epoch: 18 [38400/49669]\tLoss: 397.6385\n",
      "Training Epoch: 18 [38464/49669]\tLoss: 417.9695\n",
      "Training Epoch: 18 [38528/49669]\tLoss: 415.2733\n",
      "Training Epoch: 18 [38592/49669]\tLoss: 415.7041\n",
      "Training Epoch: 18 [38656/49669]\tLoss: 387.3593\n",
      "Training Epoch: 18 [38720/49669]\tLoss: 424.4851\n",
      "Training Epoch: 18 [38784/49669]\tLoss: 401.5195\n",
      "Training Epoch: 18 [38848/49669]\tLoss: 402.0541\n",
      "Training Epoch: 18 [38912/49669]\tLoss: 412.7629\n",
      "Training Epoch: 18 [38976/49669]\tLoss: 422.9462\n",
      "Training Epoch: 18 [39040/49669]\tLoss: 391.2617\n",
      "Training Epoch: 18 [39104/49669]\tLoss: 422.0040\n",
      "Training Epoch: 18 [39168/49669]\tLoss: 407.5712\n",
      "Training Epoch: 18 [39232/49669]\tLoss: 411.3823\n",
      "Training Epoch: 18 [39296/49669]\tLoss: 421.0280\n",
      "Training Epoch: 18 [39360/49669]\tLoss: 394.1947\n",
      "Training Epoch: 18 [39424/49669]\tLoss: 422.3138\n",
      "Training Epoch: 18 [39488/49669]\tLoss: 414.7059\n",
      "Training Epoch: 18 [39552/49669]\tLoss: 392.8178\n",
      "Training Epoch: 18 [39616/49669]\tLoss: 402.1859\n",
      "Training Epoch: 18 [39680/49669]\tLoss: 425.2281\n",
      "Training Epoch: 18 [39744/49669]\tLoss: 409.2040\n",
      "Training Epoch: 18 [39808/49669]\tLoss: 436.4033\n",
      "Training Epoch: 18 [39872/49669]\tLoss: 371.1136\n",
      "Training Epoch: 18 [39936/49669]\tLoss: 396.3218\n",
      "Training Epoch: 18 [40000/49669]\tLoss: 435.6964\n",
      "Training Epoch: 18 [40064/49669]\tLoss: 422.7580\n",
      "Training Epoch: 18 [40128/49669]\tLoss: 399.1545\n",
      "Training Epoch: 18 [40192/49669]\tLoss: 413.1873\n",
      "Training Epoch: 18 [40256/49669]\tLoss: 406.8729\n",
      "Training Epoch: 18 [40320/49669]\tLoss: 404.5600\n",
      "Training Epoch: 18 [40384/49669]\tLoss: 395.5822\n",
      "Training Epoch: 18 [40448/49669]\tLoss: 408.4956\n",
      "Training Epoch: 18 [40512/49669]\tLoss: 391.2734\n",
      "Training Epoch: 18 [40576/49669]\tLoss: 387.5904\n",
      "Training Epoch: 18 [40640/49669]\tLoss: 431.6323\n",
      "Training Epoch: 18 [40704/49669]\tLoss: 389.0986\n",
      "Training Epoch: 18 [40768/49669]\tLoss: 403.6566\n",
      "Training Epoch: 18 [40832/49669]\tLoss: 420.7181\n",
      "Training Epoch: 18 [40896/49669]\tLoss: 387.5144\n",
      "Training Epoch: 18 [40960/49669]\tLoss: 398.6794\n",
      "Training Epoch: 18 [41024/49669]\tLoss: 393.0626\n",
      "Training Epoch: 18 [41088/49669]\tLoss: 414.8252\n",
      "Training Epoch: 18 [41152/49669]\tLoss: 388.3465\n",
      "Training Epoch: 18 [41216/49669]\tLoss: 406.4404\n",
      "Training Epoch: 18 [41280/49669]\tLoss: 396.3003\n",
      "Training Epoch: 18 [41344/49669]\tLoss: 382.2242\n",
      "Training Epoch: 18 [41408/49669]\tLoss: 398.9679\n",
      "Training Epoch: 18 [41472/49669]\tLoss: 388.5336\n",
      "Training Epoch: 18 [41536/49669]\tLoss: 412.0550\n",
      "Training Epoch: 18 [41600/49669]\tLoss: 415.5599\n",
      "Training Epoch: 18 [41664/49669]\tLoss: 395.8275\n",
      "Training Epoch: 18 [41728/49669]\tLoss: 401.7165\n",
      "Training Epoch: 18 [41792/49669]\tLoss: 395.8746\n",
      "Training Epoch: 18 [41856/49669]\tLoss: 397.4439\n",
      "Training Epoch: 18 [41920/49669]\tLoss: 422.0647\n",
      "Training Epoch: 18 [41984/49669]\tLoss: 402.3033\n",
      "Training Epoch: 18 [42048/49669]\tLoss: 425.6611\n",
      "Training Epoch: 18 [42112/49669]\tLoss: 412.5290\n",
      "Training Epoch: 18 [42176/49669]\tLoss: 427.2988\n",
      "Training Epoch: 18 [42240/49669]\tLoss: 392.1908\n",
      "Training Epoch: 18 [42304/49669]\tLoss: 415.3218\n",
      "Training Epoch: 18 [42368/49669]\tLoss: 419.3529\n",
      "Training Epoch: 18 [42432/49669]\tLoss: 419.1703\n",
      "Training Epoch: 18 [42496/49669]\tLoss: 431.9436\n",
      "Training Epoch: 18 [42560/49669]\tLoss: 399.6996\n",
      "Training Epoch: 18 [42624/49669]\tLoss: 424.5615\n",
      "Training Epoch: 18 [42688/49669]\tLoss: 401.4282\n",
      "Training Epoch: 18 [42752/49669]\tLoss: 402.9745\n",
      "Training Epoch: 18 [42816/49669]\tLoss: 430.3192\n",
      "Training Epoch: 18 [42880/49669]\tLoss: 416.2001\n",
      "Training Epoch: 18 [42944/49669]\tLoss: 376.4215\n",
      "Training Epoch: 18 [43008/49669]\tLoss: 402.1301\n",
      "Training Epoch: 18 [43072/49669]\tLoss: 400.0611\n",
      "Training Epoch: 18 [43136/49669]\tLoss: 374.4074\n",
      "Training Epoch: 18 [43200/49669]\tLoss: 409.4931\n",
      "Training Epoch: 18 [43264/49669]\tLoss: 417.0042\n",
      "Training Epoch: 18 [43328/49669]\tLoss: 438.3154\n",
      "Training Epoch: 18 [43392/49669]\tLoss: 402.7099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [43456/49669]\tLoss: 402.8355\n",
      "Training Epoch: 18 [43520/49669]\tLoss: 404.1927\n",
      "Training Epoch: 18 [43584/49669]\tLoss: 390.4229\n",
      "Training Epoch: 18 [43648/49669]\tLoss: 435.2325\n",
      "Training Epoch: 18 [43712/49669]\tLoss: 373.5464\n",
      "Training Epoch: 18 [43776/49669]\tLoss: 425.1078\n",
      "Training Epoch: 18 [43840/49669]\tLoss: 381.4108\n",
      "Training Epoch: 18 [43904/49669]\tLoss: 407.6566\n",
      "Training Epoch: 18 [43968/49669]\tLoss: 413.3787\n",
      "Training Epoch: 18 [44032/49669]\tLoss: 420.4178\n",
      "Training Epoch: 18 [44096/49669]\tLoss: 429.7578\n",
      "Training Epoch: 18 [44160/49669]\tLoss: 390.2461\n",
      "Training Epoch: 18 [44224/49669]\tLoss: 410.9263\n",
      "Training Epoch: 18 [44288/49669]\tLoss: 429.0578\n",
      "Training Epoch: 18 [44352/49669]\tLoss: 447.2774\n",
      "Training Epoch: 18 [44416/49669]\tLoss: 397.2763\n",
      "Training Epoch: 18 [44480/49669]\tLoss: 417.3046\n",
      "Training Epoch: 18 [44544/49669]\tLoss: 389.9735\n",
      "Training Epoch: 18 [44608/49669]\tLoss: 410.9727\n",
      "Training Epoch: 18 [44672/49669]\tLoss: 411.4331\n",
      "Training Epoch: 18 [44736/49669]\tLoss: 425.9943\n",
      "Training Epoch: 18 [44800/49669]\tLoss: 408.7859\n",
      "Training Epoch: 18 [44864/49669]\tLoss: 387.5116\n",
      "Training Epoch: 18 [44928/49669]\tLoss: 408.0864\n",
      "Training Epoch: 18 [44992/49669]\tLoss: 364.2988\n",
      "Training Epoch: 18 [45056/49669]\tLoss: 400.4411\n",
      "Training Epoch: 18 [45120/49669]\tLoss: 401.2768\n",
      "Training Epoch: 18 [45184/49669]\tLoss: 414.2296\n",
      "Training Epoch: 18 [45248/49669]\tLoss: 407.2085\n",
      "Training Epoch: 18 [45312/49669]\tLoss: 404.1279\n",
      "Training Epoch: 18 [45376/49669]\tLoss: 365.6564\n",
      "Training Epoch: 18 [45440/49669]\tLoss: 415.6293\n",
      "Training Epoch: 18 [45504/49669]\tLoss: 384.5398\n",
      "Training Epoch: 18 [45568/49669]\tLoss: 399.8811\n",
      "Training Epoch: 18 [45632/49669]\tLoss: 413.8565\n",
      "Training Epoch: 18 [45696/49669]\tLoss: 416.4790\n",
      "Training Epoch: 18 [45760/49669]\tLoss: 418.6936\n",
      "Training Epoch: 18 [45824/49669]\tLoss: 381.8455\n",
      "Training Epoch: 18 [45888/49669]\tLoss: 389.1073\n",
      "Training Epoch: 18 [45952/49669]\tLoss: 409.2606\n",
      "Training Epoch: 18 [46016/49669]\tLoss: 411.6448\n",
      "Training Epoch: 18 [46080/49669]\tLoss: 414.9003\n",
      "Training Epoch: 18 [46144/49669]\tLoss: 406.7690\n",
      "Training Epoch: 18 [46208/49669]\tLoss: 434.6465\n",
      "Training Epoch: 18 [46272/49669]\tLoss: 410.3178\n",
      "Training Epoch: 18 [46336/49669]\tLoss: 434.9201\n",
      "Training Epoch: 18 [46400/49669]\tLoss: 432.0631\n",
      "Training Epoch: 18 [46464/49669]\tLoss: 385.1643\n",
      "Training Epoch: 18 [46528/49669]\tLoss: 371.1593\n",
      "Training Epoch: 18 [46592/49669]\tLoss: 390.4826\n",
      "Training Epoch: 18 [46656/49669]\tLoss: 376.7169\n",
      "Training Epoch: 18 [46720/49669]\tLoss: 404.7885\n",
      "Training Epoch: 18 [46784/49669]\tLoss: 393.5674\n",
      "Training Epoch: 18 [46848/49669]\tLoss: 391.3384\n",
      "Training Epoch: 18 [46912/49669]\tLoss: 394.8027\n",
      "Training Epoch: 18 [46976/49669]\tLoss: 408.4746\n",
      "Training Epoch: 18 [47040/49669]\tLoss: 440.4657\n",
      "Training Epoch: 18 [47104/49669]\tLoss: 423.7155\n",
      "Training Epoch: 18 [47168/49669]\tLoss: 403.6172\n",
      "Training Epoch: 18 [47232/49669]\tLoss: 421.0050\n",
      "Training Epoch: 18 [47296/49669]\tLoss: 402.9020\n",
      "Training Epoch: 18 [47360/49669]\tLoss: 409.4568\n",
      "Training Epoch: 18 [47424/49669]\tLoss: 389.0094\n",
      "Training Epoch: 18 [47488/49669]\tLoss: 409.7435\n",
      "Training Epoch: 18 [47552/49669]\tLoss: 423.5118\n",
      "Training Epoch: 18 [47616/49669]\tLoss: 435.3080\n",
      "Training Epoch: 18 [47680/49669]\tLoss: 404.5191\n",
      "Training Epoch: 18 [47744/49669]\tLoss: 422.5897\n",
      "Training Epoch: 18 [47808/49669]\tLoss: 425.4832\n",
      "Training Epoch: 18 [47872/49669]\tLoss: 433.5796\n",
      "Training Epoch: 18 [47936/49669]\tLoss: 410.7940\n",
      "Training Epoch: 18 [48000/49669]\tLoss: 411.7849\n",
      "Training Epoch: 18 [48064/49669]\tLoss: 416.6101\n",
      "Training Epoch: 18 [48128/49669]\tLoss: 393.9230\n",
      "Training Epoch: 18 [48192/49669]\tLoss: 382.3669\n",
      "Training Epoch: 18 [48256/49669]\tLoss: 403.3530\n",
      "Training Epoch: 18 [48320/49669]\tLoss: 376.9500\n",
      "Training Epoch: 18 [48384/49669]\tLoss: 411.6355\n",
      "Training Epoch: 18 [48448/49669]\tLoss: 400.0904\n",
      "Training Epoch: 18 [48512/49669]\tLoss: 409.0430\n",
      "Training Epoch: 18 [48576/49669]\tLoss: 425.6597\n",
      "Training Epoch: 18 [48640/49669]\tLoss: 406.0514\n",
      "Training Epoch: 18 [48704/49669]\tLoss: 377.7908\n",
      "Training Epoch: 18 [48768/49669]\tLoss: 406.0746\n",
      "Training Epoch: 18 [48832/49669]\tLoss: 428.7777\n",
      "Training Epoch: 18 [48896/49669]\tLoss: 407.3281\n",
      "Training Epoch: 18 [48960/49669]\tLoss: 423.6082\n",
      "Training Epoch: 18 [49024/49669]\tLoss: 448.5776\n",
      "Training Epoch: 18 [49088/49669]\tLoss: 383.9147\n",
      "Training Epoch: 18 [49152/49669]\tLoss: 403.1155\n",
      "Training Epoch: 18 [49216/49669]\tLoss: 421.7832\n",
      "Training Epoch: 18 [49280/49669]\tLoss: 412.8793\n",
      "Training Epoch: 18 [49344/49669]\tLoss: 394.8614\n",
      "Training Epoch: 18 [49408/49669]\tLoss: 384.0124\n",
      "Training Epoch: 18 [49472/49669]\tLoss: 394.1956\n",
      "Training Epoch: 18 [49536/49669]\tLoss: 408.2547\n",
      "Training Epoch: 18 [49600/49669]\tLoss: 414.6400\n",
      "Training Epoch: 18 [49664/49669]\tLoss: 395.4394\n",
      "Training Epoch: 18 [49669/49669]\tLoss: 383.8954\n",
      "Training Epoch: 18 [5519/5519]\tLoss: 408.0315\n",
      "Training Epoch: 19 [64/49669]\tLoss: 440.0372\n",
      "Training Epoch: 19 [128/49669]\tLoss: 392.4545\n",
      "Training Epoch: 19 [192/49669]\tLoss: 415.6505\n",
      "Training Epoch: 19 [256/49669]\tLoss: 425.3908\n",
      "Training Epoch: 19 [320/49669]\tLoss: 403.6979\n",
      "Training Epoch: 19 [384/49669]\tLoss: 403.7643\n",
      "Training Epoch: 19 [448/49669]\tLoss: 408.3103\n",
      "Training Epoch: 19 [512/49669]\tLoss: 408.5362\n",
      "Training Epoch: 19 [576/49669]\tLoss: 404.1379\n",
      "Training Epoch: 19 [640/49669]\tLoss: 391.9486\n",
      "Training Epoch: 19 [704/49669]\tLoss: 408.7149\n",
      "Training Epoch: 19 [768/49669]\tLoss: 407.2937\n",
      "Training Epoch: 19 [832/49669]\tLoss: 385.7329\n",
      "Training Epoch: 19 [896/49669]\tLoss: 363.9809\n",
      "Training Epoch: 19 [960/49669]\tLoss: 397.7478\n",
      "Training Epoch: 19 [1024/49669]\tLoss: 398.7422\n",
      "Training Epoch: 19 [1088/49669]\tLoss: 383.1898\n",
      "Training Epoch: 19 [1152/49669]\tLoss: 401.6213\n",
      "Training Epoch: 19 [1216/49669]\tLoss: 419.5379\n",
      "Training Epoch: 19 [1280/49669]\tLoss: 419.4201\n",
      "Training Epoch: 19 [1344/49669]\tLoss: 399.2131\n",
      "Training Epoch: 19 [1408/49669]\tLoss: 411.5642\n",
      "Training Epoch: 19 [1472/49669]\tLoss: 401.7190\n",
      "Training Epoch: 19 [1536/49669]\tLoss: 413.7442\n",
      "Training Epoch: 19 [1600/49669]\tLoss: 409.2824\n",
      "Training Epoch: 19 [1664/49669]\tLoss: 407.1046\n",
      "Training Epoch: 19 [1728/49669]\tLoss: 398.6157\n",
      "Training Epoch: 19 [1792/49669]\tLoss: 397.8704\n",
      "Training Epoch: 19 [1856/49669]\tLoss: 409.4051\n",
      "Training Epoch: 19 [1920/49669]\tLoss: 410.2791\n",
      "Training Epoch: 19 [1984/49669]\tLoss: 404.6750\n",
      "Training Epoch: 19 [2048/49669]\tLoss: 396.9731\n",
      "Training Epoch: 19 [2112/49669]\tLoss: 410.1576\n",
      "Training Epoch: 19 [2176/49669]\tLoss: 414.2635\n",
      "Training Epoch: 19 [2240/49669]\tLoss: 406.2555\n",
      "Training Epoch: 19 [2304/49669]\tLoss: 405.9121\n",
      "Training Epoch: 19 [2368/49669]\tLoss: 386.0619\n",
      "Training Epoch: 19 [2432/49669]\tLoss: 385.1624\n",
      "Training Epoch: 19 [2496/49669]\tLoss: 420.7037\n",
      "Training Epoch: 19 [2560/49669]\tLoss: 418.0104\n",
      "Training Epoch: 19 [2624/49669]\tLoss: 442.9845\n",
      "Training Epoch: 19 [2688/49669]\tLoss: 386.2448\n",
      "Training Epoch: 19 [2752/49669]\tLoss: 385.2000\n",
      "Training Epoch: 19 [2816/49669]\tLoss: 411.6913\n",
      "Training Epoch: 19 [2880/49669]\tLoss: 384.8275\n",
      "Training Epoch: 19 [2944/49669]\tLoss: 417.8723\n",
      "Training Epoch: 19 [3008/49669]\tLoss: 405.5850\n",
      "Training Epoch: 19 [3072/49669]\tLoss: 441.3300\n",
      "Training Epoch: 19 [3136/49669]\tLoss: 397.7301\n",
      "Training Epoch: 19 [3200/49669]\tLoss: 426.4327\n",
      "Training Epoch: 19 [3264/49669]\tLoss: 381.9083\n",
      "Training Epoch: 19 [3328/49669]\tLoss: 391.2126\n",
      "Training Epoch: 19 [3392/49669]\tLoss: 413.1763\n",
      "Training Epoch: 19 [3456/49669]\tLoss: 399.4309\n",
      "Training Epoch: 19 [3520/49669]\tLoss: 391.2820\n",
      "Training Epoch: 19 [3584/49669]\tLoss: 400.9429\n",
      "Training Epoch: 19 [3648/49669]\tLoss: 437.4392\n",
      "Training Epoch: 19 [3712/49669]\tLoss: 381.7565\n",
      "Training Epoch: 19 [3776/49669]\tLoss: 402.1380\n",
      "Training Epoch: 19 [3840/49669]\tLoss: 406.3792\n",
      "Training Epoch: 19 [3904/49669]\tLoss: 411.2063\n",
      "Training Epoch: 19 [3968/49669]\tLoss: 416.5158\n",
      "Training Epoch: 19 [4032/49669]\tLoss: 370.2676\n",
      "Training Epoch: 19 [4096/49669]\tLoss: 435.1032\n",
      "Training Epoch: 19 [4160/49669]\tLoss: 388.5038\n",
      "Training Epoch: 19 [4224/49669]\tLoss: 396.9265\n",
      "Training Epoch: 19 [4288/49669]\tLoss: 436.4685\n",
      "Training Epoch: 19 [4352/49669]\tLoss: 440.9668\n",
      "Training Epoch: 19 [4416/49669]\tLoss: 437.4612\n",
      "Training Epoch: 19 [4480/49669]\tLoss: 440.1048\n",
      "Training Epoch: 19 [4544/49669]\tLoss: 480.6594\n",
      "Training Epoch: 19 [4608/49669]\tLoss: 525.6165\n",
      "Training Epoch: 19 [4672/49669]\tLoss: 532.3740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [4736/49669]\tLoss: 551.8773\n",
      "Training Epoch: 19 [4800/49669]\tLoss: 489.5633\n",
      "Training Epoch: 19 [4864/49669]\tLoss: 405.3444\n",
      "Training Epoch: 19 [4928/49669]\tLoss: 399.2336\n",
      "Training Epoch: 19 [4992/49669]\tLoss: 436.8883\n",
      "Training Epoch: 19 [5056/49669]\tLoss: 481.9245\n",
      "Training Epoch: 19 [5120/49669]\tLoss: 497.8755\n",
      "Training Epoch: 19 [5184/49669]\tLoss: 449.4112\n",
      "Training Epoch: 19 [5248/49669]\tLoss: 409.1555\n",
      "Training Epoch: 19 [5312/49669]\tLoss: 420.9247\n",
      "Training Epoch: 19 [5376/49669]\tLoss: 461.9646\n",
      "Training Epoch: 19 [5440/49669]\tLoss: 476.1046\n",
      "Training Epoch: 19 [5504/49669]\tLoss: 452.8791\n",
      "Training Epoch: 19 [5568/49669]\tLoss: 379.4901\n",
      "Training Epoch: 19 [5632/49669]\tLoss: 415.6472\n",
      "Training Epoch: 19 [5696/49669]\tLoss: 447.3801\n",
      "Training Epoch: 19 [5760/49669]\tLoss: 444.2785\n",
      "Training Epoch: 19 [5824/49669]\tLoss: 443.4846\n",
      "Training Epoch: 19 [5888/49669]\tLoss: 404.8016\n",
      "Training Epoch: 19 [5952/49669]\tLoss: 427.5107\n",
      "Training Epoch: 19 [6016/49669]\tLoss: 450.3139\n",
      "Training Epoch: 19 [6080/49669]\tLoss: 459.1624\n",
      "Training Epoch: 19 [6144/49669]\tLoss: 383.3834\n",
      "Training Epoch: 19 [6208/49669]\tLoss: 388.2497\n",
      "Training Epoch: 19 [6272/49669]\tLoss: 424.9198\n",
      "Training Epoch: 19 [6336/49669]\tLoss: 435.2707\n",
      "Training Epoch: 19 [6400/49669]\tLoss: 411.7796\n",
      "Training Epoch: 19 [6464/49669]\tLoss: 358.2599\n",
      "Training Epoch: 19 [6528/49669]\tLoss: 369.0468\n",
      "Training Epoch: 19 [6592/49669]\tLoss: 416.8983\n",
      "Training Epoch: 19 [6656/49669]\tLoss: 410.5898\n",
      "Training Epoch: 19 [6720/49669]\tLoss: 389.2938\n",
      "Training Epoch: 19 [6784/49669]\tLoss: 398.0350\n",
      "Training Epoch: 19 [6848/49669]\tLoss: 416.6016\n",
      "Training Epoch: 19 [6912/49669]\tLoss: 422.6461\n",
      "Training Epoch: 19 [6976/49669]\tLoss: 411.7645\n",
      "Training Epoch: 19 [7040/49669]\tLoss: 383.1073\n",
      "Training Epoch: 19 [7104/49669]\tLoss: 404.4125\n",
      "Training Epoch: 19 [7168/49669]\tLoss: 419.3225\n",
      "Training Epoch: 19 [7232/49669]\tLoss: 402.6606\n",
      "Training Epoch: 19 [7296/49669]\tLoss: 438.8705\n",
      "Training Epoch: 19 [7360/49669]\tLoss: 409.6722\n",
      "Training Epoch: 19 [7424/49669]\tLoss: 395.4998\n",
      "Training Epoch: 19 [7488/49669]\tLoss: 393.6212\n",
      "Training Epoch: 19 [7552/49669]\tLoss: 419.4494\n",
      "Training Epoch: 19 [7616/49669]\tLoss: 431.8353\n",
      "Training Epoch: 19 [7680/49669]\tLoss: 423.8705\n",
      "Training Epoch: 19 [7744/49669]\tLoss: 413.9383\n",
      "Training Epoch: 19 [7808/49669]\tLoss: 422.6849\n",
      "Training Epoch: 19 [7872/49669]\tLoss: 393.6642\n",
      "Training Epoch: 19 [7936/49669]\tLoss: 416.7030\n",
      "Training Epoch: 19 [8000/49669]\tLoss: 434.3028\n",
      "Training Epoch: 19 [8064/49669]\tLoss: 396.2408\n",
      "Training Epoch: 19 [8128/49669]\tLoss: 383.7124\n",
      "Training Epoch: 19 [8192/49669]\tLoss: 402.4524\n",
      "Training Epoch: 19 [8256/49669]\tLoss: 433.7161\n",
      "Training Epoch: 19 [8320/49669]\tLoss: 421.5519\n",
      "Training Epoch: 19 [8384/49669]\tLoss: 407.7538\n",
      "Training Epoch: 19 [8448/49669]\tLoss: 401.9350\n",
      "Training Epoch: 19 [8512/49669]\tLoss: 394.6841\n",
      "Training Epoch: 19 [8576/49669]\tLoss: 366.2759\n",
      "Training Epoch: 19 [8640/49669]\tLoss: 411.7617\n",
      "Training Epoch: 19 [8704/49669]\tLoss: 403.9046\n",
      "Training Epoch: 19 [8768/49669]\tLoss: 420.4517\n",
      "Training Epoch: 19 [8832/49669]\tLoss: 419.9509\n",
      "Training Epoch: 19 [8896/49669]\tLoss: 420.7183\n",
      "Training Epoch: 19 [8960/49669]\tLoss: 421.2065\n",
      "Training Epoch: 19 [9024/49669]\tLoss: 393.3485\n",
      "Training Epoch: 19 [9088/49669]\tLoss: 404.6625\n",
      "Training Epoch: 19 [9152/49669]\tLoss: 394.0717\n",
      "Training Epoch: 19 [9216/49669]\tLoss: 424.3801\n",
      "Training Epoch: 19 [9280/49669]\tLoss: 395.8991\n",
      "Training Epoch: 19 [9344/49669]\tLoss: 412.7686\n",
      "Training Epoch: 19 [9408/49669]\tLoss: 392.1226\n",
      "Training Epoch: 19 [9472/49669]\tLoss: 422.9951\n",
      "Training Epoch: 19 [9536/49669]\tLoss: 391.7618\n",
      "Training Epoch: 19 [9600/49669]\tLoss: 390.1395\n",
      "Training Epoch: 19 [9664/49669]\tLoss: 393.6733\n",
      "Training Epoch: 19 [9728/49669]\tLoss: 397.1250\n",
      "Training Epoch: 19 [9792/49669]\tLoss: 425.5558\n",
      "Training Epoch: 19 [9856/49669]\tLoss: 384.9970\n",
      "Training Epoch: 19 [9920/49669]\tLoss: 373.5296\n",
      "Training Epoch: 19 [9984/49669]\tLoss: 404.8150\n",
      "Training Epoch: 19 [10048/49669]\tLoss: 422.7016\n",
      "Training Epoch: 19 [10112/49669]\tLoss: 402.0857\n",
      "Training Epoch: 19 [10176/49669]\tLoss: 408.5325\n",
      "Training Epoch: 19 [10240/49669]\tLoss: 398.8734\n",
      "Training Epoch: 19 [10304/49669]\tLoss: 430.5381\n",
      "Training Epoch: 19 [10368/49669]\tLoss: 425.0268\n",
      "Training Epoch: 19 [10432/49669]\tLoss: 395.9851\n",
      "Training Epoch: 19 [10496/49669]\tLoss: 411.0066\n",
      "Training Epoch: 19 [10560/49669]\tLoss: 364.9254\n",
      "Training Epoch: 19 [10624/49669]\tLoss: 426.6718\n",
      "Training Epoch: 19 [10688/49669]\tLoss: 370.9490\n",
      "Training Epoch: 19 [10752/49669]\tLoss: 413.5194\n",
      "Training Epoch: 19 [10816/49669]\tLoss: 385.5970\n",
      "Training Epoch: 19 [10880/49669]\tLoss: 373.3065\n",
      "Training Epoch: 19 [10944/49669]\tLoss: 428.5357\n",
      "Training Epoch: 19 [11008/49669]\tLoss: 405.2263\n",
      "Training Epoch: 19 [11072/49669]\tLoss: 405.2569\n",
      "Training Epoch: 19 [11136/49669]\tLoss: 391.3525\n",
      "Training Epoch: 19 [11200/49669]\tLoss: 398.9820\n",
      "Training Epoch: 19 [11264/49669]\tLoss: 401.8711\n",
      "Training Epoch: 19 [11328/49669]\tLoss: 385.4420\n",
      "Training Epoch: 19 [11392/49669]\tLoss: 381.6877\n",
      "Training Epoch: 19 [11456/49669]\tLoss: 422.9761\n",
      "Training Epoch: 19 [11520/49669]\tLoss: 412.1237\n",
      "Training Epoch: 19 [11584/49669]\tLoss: 398.6077\n",
      "Training Epoch: 19 [11648/49669]\tLoss: 391.8426\n",
      "Training Epoch: 19 [11712/49669]\tLoss: 425.7529\n",
      "Training Epoch: 19 [11776/49669]\tLoss: 383.1103\n",
      "Training Epoch: 19 [11840/49669]\tLoss: 390.2658\n",
      "Training Epoch: 19 [11904/49669]\tLoss: 424.7237\n",
      "Training Epoch: 19 [11968/49669]\tLoss: 423.6646\n",
      "Training Epoch: 19 [12032/49669]\tLoss: 391.7759\n",
      "Training Epoch: 19 [12096/49669]\tLoss: 407.4402\n",
      "Training Epoch: 19 [12160/49669]\tLoss: 401.7482\n",
      "Training Epoch: 19 [12224/49669]\tLoss: 415.2265\n",
      "Training Epoch: 19 [12288/49669]\tLoss: 401.4690\n",
      "Training Epoch: 19 [12352/49669]\tLoss: 418.2667\n",
      "Training Epoch: 19 [12416/49669]\tLoss: 408.9229\n",
      "Training Epoch: 19 [12480/49669]\tLoss: 393.9861\n",
      "Training Epoch: 19 [12544/49669]\tLoss: 393.5695\n",
      "Training Epoch: 19 [12608/49669]\tLoss: 437.8463\n",
      "Training Epoch: 19 [12672/49669]\tLoss: 385.2793\n",
      "Training Epoch: 19 [12736/49669]\tLoss: 378.3806\n",
      "Training Epoch: 19 [12800/49669]\tLoss: 380.3891\n",
      "Training Epoch: 19 [12864/49669]\tLoss: 414.6487\n",
      "Training Epoch: 19 [12928/49669]\tLoss: 409.4424\n",
      "Training Epoch: 19 [12992/49669]\tLoss: 392.1351\n",
      "Training Epoch: 19 [13056/49669]\tLoss: 408.8335\n",
      "Training Epoch: 19 [13120/49669]\tLoss: 413.6135\n",
      "Training Epoch: 19 [13184/49669]\tLoss: 386.3282\n",
      "Training Epoch: 19 [13248/49669]\tLoss: 381.1868\n",
      "Training Epoch: 19 [13312/49669]\tLoss: 399.0953\n",
      "Training Epoch: 19 [13376/49669]\tLoss: 414.3817\n",
      "Training Epoch: 19 [13440/49669]\tLoss: 424.5670\n",
      "Training Epoch: 19 [13504/49669]\tLoss: 418.3183\n",
      "Training Epoch: 19 [13568/49669]\tLoss: 406.4340\n",
      "Training Epoch: 19 [13632/49669]\tLoss: 398.3254\n",
      "Training Epoch: 19 [13696/49669]\tLoss: 414.1567\n",
      "Training Epoch: 19 [13760/49669]\tLoss: 402.4514\n",
      "Training Epoch: 19 [13824/49669]\tLoss: 405.4659\n",
      "Training Epoch: 19 [13888/49669]\tLoss: 423.7239\n",
      "Training Epoch: 19 [13952/49669]\tLoss: 401.8618\n",
      "Training Epoch: 19 [14016/49669]\tLoss: 398.3848\n",
      "Training Epoch: 19 [14080/49669]\tLoss: 421.9362\n",
      "Training Epoch: 19 [14144/49669]\tLoss: 376.8867\n",
      "Training Epoch: 19 [14208/49669]\tLoss: 384.5364\n",
      "Training Epoch: 19 [14272/49669]\tLoss: 419.9936\n",
      "Training Epoch: 19 [14336/49669]\tLoss: 400.3624\n",
      "Training Epoch: 19 [14400/49669]\tLoss: 421.0450\n",
      "Training Epoch: 19 [14464/49669]\tLoss: 401.1808\n",
      "Training Epoch: 19 [14528/49669]\tLoss: 398.9100\n",
      "Training Epoch: 19 [14592/49669]\tLoss: 401.1328\n",
      "Training Epoch: 19 [14656/49669]\tLoss: 406.3015\n",
      "Training Epoch: 19 [14720/49669]\tLoss: 405.4556\n",
      "Training Epoch: 19 [14784/49669]\tLoss: 387.6702\n",
      "Training Epoch: 19 [14848/49669]\tLoss: 387.8064\n",
      "Training Epoch: 19 [14912/49669]\tLoss: 421.1186\n",
      "Training Epoch: 19 [14976/49669]\tLoss: 418.0140\n",
      "Training Epoch: 19 [15040/49669]\tLoss: 418.2330\n",
      "Training Epoch: 19 [15104/49669]\tLoss: 417.2546\n",
      "Training Epoch: 19 [15168/49669]\tLoss: 415.8376\n",
      "Training Epoch: 19 [15232/49669]\tLoss: 418.0415\n",
      "Training Epoch: 19 [15296/49669]\tLoss: 417.1484\n",
      "Training Epoch: 19 [15360/49669]\tLoss: 374.0496\n",
      "Training Epoch: 19 [15424/49669]\tLoss: 410.3359\n",
      "Training Epoch: 19 [15488/49669]\tLoss: 386.7705\n",
      "Training Epoch: 19 [15552/49669]\tLoss: 407.9889\n",
      "Training Epoch: 19 [15616/49669]\tLoss: 414.1714\n",
      "Training Epoch: 19 [15680/49669]\tLoss: 406.6504\n",
      "Training Epoch: 19 [15744/49669]\tLoss: 394.2612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [15808/49669]\tLoss: 400.4474\n",
      "Training Epoch: 19 [15872/49669]\tLoss: 389.9407\n",
      "Training Epoch: 19 [15936/49669]\tLoss: 417.2989\n",
      "Training Epoch: 19 [16000/49669]\tLoss: 405.3791\n",
      "Training Epoch: 19 [16064/49669]\tLoss: 390.5120\n",
      "Training Epoch: 19 [16128/49669]\tLoss: 399.7082\n",
      "Training Epoch: 19 [16192/49669]\tLoss: 391.0514\n",
      "Training Epoch: 19 [16256/49669]\tLoss: 398.1089\n",
      "Training Epoch: 19 [16320/49669]\tLoss: 431.0931\n",
      "Training Epoch: 19 [16384/49669]\tLoss: 397.2975\n",
      "Training Epoch: 19 [16448/49669]\tLoss: 401.7033\n",
      "Training Epoch: 19 [16512/49669]\tLoss: 414.6943\n",
      "Training Epoch: 19 [16576/49669]\tLoss: 401.2187\n",
      "Training Epoch: 19 [16640/49669]\tLoss: 425.3162\n",
      "Training Epoch: 19 [16704/49669]\tLoss: 416.6119\n",
      "Training Epoch: 19 [16768/49669]\tLoss: 404.2241\n",
      "Training Epoch: 19 [16832/49669]\tLoss: 399.3039\n",
      "Training Epoch: 19 [16896/49669]\tLoss: 425.2159\n",
      "Training Epoch: 19 [16960/49669]\tLoss: 422.7598\n",
      "Training Epoch: 19 [17024/49669]\tLoss: 364.1271\n",
      "Training Epoch: 19 [17088/49669]\tLoss: 407.1383\n",
      "Training Epoch: 19 [17152/49669]\tLoss: 423.2124\n",
      "Training Epoch: 19 [17216/49669]\tLoss: 392.2675\n",
      "Training Epoch: 19 [17280/49669]\tLoss: 427.7558\n",
      "Training Epoch: 19 [17344/49669]\tLoss: 394.7226\n",
      "Training Epoch: 19 [17408/49669]\tLoss: 414.5444\n",
      "Training Epoch: 19 [17472/49669]\tLoss: 408.8849\n",
      "Training Epoch: 19 [17536/49669]\tLoss: 423.2395\n",
      "Training Epoch: 19 [17600/49669]\tLoss: 391.1653\n",
      "Training Epoch: 19 [17664/49669]\tLoss: 394.6925\n",
      "Training Epoch: 19 [17728/49669]\tLoss: 410.2215\n",
      "Training Epoch: 19 [17792/49669]\tLoss: 388.4312\n",
      "Training Epoch: 19 [17856/49669]\tLoss: 419.0215\n",
      "Training Epoch: 19 [17920/49669]\tLoss: 391.6474\n",
      "Training Epoch: 19 [17984/49669]\tLoss: 374.9070\n",
      "Training Epoch: 19 [18048/49669]\tLoss: 414.0324\n",
      "Training Epoch: 19 [18112/49669]\tLoss: 411.7866\n",
      "Training Epoch: 19 [18176/49669]\tLoss: 399.7028\n",
      "Training Epoch: 19 [18240/49669]\tLoss: 394.8836\n",
      "Training Epoch: 19 [18304/49669]\tLoss: 397.8870\n",
      "Training Epoch: 19 [18368/49669]\tLoss: 369.8361\n",
      "Training Epoch: 19 [18432/49669]\tLoss: 398.1191\n",
      "Training Epoch: 19 [18496/49669]\tLoss: 425.4277\n",
      "Training Epoch: 19 [18560/49669]\tLoss: 397.0294\n",
      "Training Epoch: 19 [18624/49669]\tLoss: 421.3376\n",
      "Training Epoch: 19 [18688/49669]\tLoss: 404.9349\n",
      "Training Epoch: 19 [18752/49669]\tLoss: 415.6684\n",
      "Training Epoch: 19 [18816/49669]\tLoss: 417.1644\n",
      "Training Epoch: 19 [18880/49669]\tLoss: 402.5697\n",
      "Training Epoch: 19 [18944/49669]\tLoss: 392.5488\n",
      "Training Epoch: 19 [19008/49669]\tLoss: 432.6141\n",
      "Training Epoch: 19 [19072/49669]\tLoss: 406.7418\n",
      "Training Epoch: 19 [19136/49669]\tLoss: 411.5053\n",
      "Training Epoch: 19 [19200/49669]\tLoss: 412.1342\n",
      "Training Epoch: 19 [19264/49669]\tLoss: 391.2737\n",
      "Training Epoch: 19 [19328/49669]\tLoss: 436.5644\n",
      "Training Epoch: 19 [19392/49669]\tLoss: 424.2277\n",
      "Training Epoch: 19 [19456/49669]\tLoss: 420.6889\n",
      "Training Epoch: 19 [19520/49669]\tLoss: 406.9852\n",
      "Training Epoch: 19 [19584/49669]\tLoss: 437.3386\n",
      "Training Epoch: 19 [19648/49669]\tLoss: 430.2948\n",
      "Training Epoch: 19 [19712/49669]\tLoss: 439.0001\n",
      "Training Epoch: 19 [19776/49669]\tLoss: 445.6243\n",
      "Training Epoch: 19 [19840/49669]\tLoss: 414.4214\n",
      "Training Epoch: 19 [19904/49669]\tLoss: 412.8794\n",
      "Training Epoch: 19 [19968/49669]\tLoss: 409.9712\n",
      "Training Epoch: 19 [20032/49669]\tLoss: 435.0956\n",
      "Training Epoch: 19 [20096/49669]\tLoss: 427.5905\n",
      "Training Epoch: 19 [20160/49669]\tLoss: 388.3888\n",
      "Training Epoch: 19 [20224/49669]\tLoss: 395.5316\n",
      "Training Epoch: 19 [20288/49669]\tLoss: 428.8364\n",
      "Training Epoch: 19 [20352/49669]\tLoss: 420.7575\n",
      "Training Epoch: 19 [20416/49669]\tLoss: 386.2582\n",
      "Training Epoch: 19 [20480/49669]\tLoss: 441.0101\n",
      "Training Epoch: 19 [20544/49669]\tLoss: 404.9757\n",
      "Training Epoch: 19 [20608/49669]\tLoss: 451.6558\n",
      "Training Epoch: 19 [20672/49669]\tLoss: 420.1152\n",
      "Training Epoch: 19 [20736/49669]\tLoss: 428.5171\n",
      "Training Epoch: 19 [20800/49669]\tLoss: 421.2697\n",
      "Training Epoch: 19 [20864/49669]\tLoss: 420.5535\n",
      "Training Epoch: 19 [20928/49669]\tLoss: 402.2251\n",
      "Training Epoch: 19 [20992/49669]\tLoss: 396.8774\n",
      "Training Epoch: 19 [21056/49669]\tLoss: 387.3428\n",
      "Training Epoch: 19 [21120/49669]\tLoss: 424.0202\n",
      "Training Epoch: 19 [21184/49669]\tLoss: 425.2342\n",
      "Training Epoch: 19 [21248/49669]\tLoss: 411.9246\n",
      "Training Epoch: 19 [21312/49669]\tLoss: 445.7616\n",
      "Training Epoch: 19 [21376/49669]\tLoss: 424.8463\n",
      "Training Epoch: 19 [21440/49669]\tLoss: 395.3090\n",
      "Training Epoch: 19 [21504/49669]\tLoss: 396.0264\n",
      "Training Epoch: 19 [21568/49669]\tLoss: 396.1991\n",
      "Training Epoch: 19 [21632/49669]\tLoss: 392.0431\n",
      "Training Epoch: 19 [21696/49669]\tLoss: 377.2169\n",
      "Training Epoch: 19 [21760/49669]\tLoss: 423.6552\n",
      "Training Epoch: 19 [21824/49669]\tLoss: 426.3771\n",
      "Training Epoch: 19 [21888/49669]\tLoss: 417.6131\n",
      "Training Epoch: 19 [21952/49669]\tLoss: 384.4613\n",
      "Training Epoch: 19 [22016/49669]\tLoss: 434.8425\n",
      "Training Epoch: 19 [22080/49669]\tLoss: 395.5589\n",
      "Training Epoch: 19 [22144/49669]\tLoss: 397.6837\n",
      "Training Epoch: 19 [22208/49669]\tLoss: 379.2229\n",
      "Training Epoch: 19 [22272/49669]\tLoss: 385.3457\n",
      "Training Epoch: 19 [22336/49669]\tLoss: 400.8413\n",
      "Training Epoch: 19 [22400/49669]\tLoss: 416.0824\n",
      "Training Epoch: 19 [22464/49669]\tLoss: 389.0255\n",
      "Training Epoch: 19 [22528/49669]\tLoss: 423.3723\n",
      "Training Epoch: 19 [22592/49669]\tLoss: 417.4563\n",
      "Training Epoch: 19 [22656/49669]\tLoss: 424.9175\n",
      "Training Epoch: 19 [22720/49669]\tLoss: 452.1807\n",
      "Training Epoch: 19 [22784/49669]\tLoss: 412.1861\n",
      "Training Epoch: 19 [22848/49669]\tLoss: 402.8866\n",
      "Training Epoch: 19 [22912/49669]\tLoss: 405.0709\n",
      "Training Epoch: 19 [22976/49669]\tLoss: 393.0647\n",
      "Training Epoch: 19 [23040/49669]\tLoss: 422.9094\n",
      "Training Epoch: 19 [23104/49669]\tLoss: 423.5482\n",
      "Training Epoch: 19 [23168/49669]\tLoss: 381.5365\n",
      "Training Epoch: 19 [23232/49669]\tLoss: 407.1744\n",
      "Training Epoch: 19 [23296/49669]\tLoss: 414.7668\n",
      "Training Epoch: 19 [23360/49669]\tLoss: 408.5398\n",
      "Training Epoch: 19 [23424/49669]\tLoss: 380.8395\n",
      "Training Epoch: 19 [23488/49669]\tLoss: 391.7099\n",
      "Training Epoch: 19 [23552/49669]\tLoss: 382.1686\n",
      "Training Epoch: 19 [23616/49669]\tLoss: 386.9798\n",
      "Training Epoch: 19 [23680/49669]\tLoss: 411.3545\n",
      "Training Epoch: 19 [23744/49669]\tLoss: 419.7991\n",
      "Training Epoch: 19 [23808/49669]\tLoss: 432.0647\n",
      "Training Epoch: 19 [23872/49669]\tLoss: 412.1550\n",
      "Training Epoch: 19 [23936/49669]\tLoss: 415.5755\n",
      "Training Epoch: 19 [24000/49669]\tLoss: 403.3055\n",
      "Training Epoch: 19 [24064/49669]\tLoss: 383.9459\n",
      "Training Epoch: 19 [24128/49669]\tLoss: 378.6924\n",
      "Training Epoch: 19 [24192/49669]\tLoss: 375.3606\n",
      "Training Epoch: 19 [24256/49669]\tLoss: 406.5116\n",
      "Training Epoch: 19 [24320/49669]\tLoss: 415.9787\n",
      "Training Epoch: 19 [24384/49669]\tLoss: 389.8286\n",
      "Training Epoch: 19 [24448/49669]\tLoss: 425.8129\n",
      "Training Epoch: 19 [24512/49669]\tLoss: 411.6009\n",
      "Training Epoch: 19 [24576/49669]\tLoss: 378.1507\n",
      "Training Epoch: 19 [24640/49669]\tLoss: 420.5788\n",
      "Training Epoch: 19 [24704/49669]\tLoss: 399.9371\n",
      "Training Epoch: 19 [24768/49669]\tLoss: 415.2477\n",
      "Training Epoch: 19 [24832/49669]\tLoss: 416.4422\n",
      "Training Epoch: 19 [24896/49669]\tLoss: 389.3013\n",
      "Training Epoch: 19 [24960/49669]\tLoss: 406.6443\n",
      "Training Epoch: 19 [25024/49669]\tLoss: 414.8462\n",
      "Training Epoch: 19 [25088/49669]\tLoss: 366.3908\n",
      "Training Epoch: 19 [25152/49669]\tLoss: 407.5939\n",
      "Training Epoch: 19 [25216/49669]\tLoss: 407.0208\n",
      "Training Epoch: 19 [25280/49669]\tLoss: 407.4222\n",
      "Training Epoch: 19 [25344/49669]\tLoss: 411.9641\n",
      "Training Epoch: 19 [25408/49669]\tLoss: 415.8803\n",
      "Training Epoch: 19 [25472/49669]\tLoss: 409.6073\n",
      "Training Epoch: 19 [25536/49669]\tLoss: 406.3367\n",
      "Training Epoch: 19 [25600/49669]\tLoss: 419.1817\n",
      "Training Epoch: 19 [25664/49669]\tLoss: 388.8680\n",
      "Training Epoch: 19 [25728/49669]\tLoss: 387.9429\n",
      "Training Epoch: 19 [25792/49669]\tLoss: 392.4824\n",
      "Training Epoch: 19 [25856/49669]\tLoss: 404.6481\n",
      "Training Epoch: 19 [25920/49669]\tLoss: 388.4127\n",
      "Training Epoch: 19 [25984/49669]\tLoss: 388.1060\n",
      "Training Epoch: 19 [26048/49669]\tLoss: 380.4675\n",
      "Training Epoch: 19 [26112/49669]\tLoss: 418.3754\n",
      "Training Epoch: 19 [26176/49669]\tLoss: 398.9622\n",
      "Training Epoch: 19 [26240/49669]\tLoss: 422.7107\n",
      "Training Epoch: 19 [26304/49669]\tLoss: 396.6491\n",
      "Training Epoch: 19 [26368/49669]\tLoss: 426.1549\n",
      "Training Epoch: 19 [26432/49669]\tLoss: 396.8753\n",
      "Training Epoch: 19 [26496/49669]\tLoss: 430.0582\n",
      "Training Epoch: 19 [26560/49669]\tLoss: 402.0481\n",
      "Training Epoch: 19 [26624/49669]\tLoss: 417.2241\n",
      "Training Epoch: 19 [26688/49669]\tLoss: 405.8649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [26752/49669]\tLoss: 400.9327\n",
      "Training Epoch: 19 [26816/49669]\tLoss: 373.6433\n",
      "Training Epoch: 19 [26880/49669]\tLoss: 395.9339\n",
      "Training Epoch: 19 [26944/49669]\tLoss: 420.5157\n",
      "Training Epoch: 19 [27008/49669]\tLoss: 413.4760\n",
      "Training Epoch: 19 [27072/49669]\tLoss: 416.8201\n",
      "Training Epoch: 19 [27136/49669]\tLoss: 430.4200\n",
      "Training Epoch: 19 [27200/49669]\tLoss: 412.1502\n",
      "Training Epoch: 19 [27264/49669]\tLoss: 396.0242\n",
      "Training Epoch: 19 [27328/49669]\tLoss: 405.4999\n",
      "Training Epoch: 19 [27392/49669]\tLoss: 430.1810\n",
      "Training Epoch: 19 [27456/49669]\tLoss: 428.3535\n",
      "Training Epoch: 19 [27520/49669]\tLoss: 407.2537\n",
      "Training Epoch: 19 [27584/49669]\tLoss: 387.8202\n",
      "Training Epoch: 19 [27648/49669]\tLoss: 429.6504\n",
      "Training Epoch: 19 [27712/49669]\tLoss: 417.7773\n",
      "Training Epoch: 19 [27776/49669]\tLoss: 396.3973\n",
      "Training Epoch: 19 [27840/49669]\tLoss: 394.1438\n",
      "Training Epoch: 19 [27904/49669]\tLoss: 374.0602\n",
      "Training Epoch: 19 [27968/49669]\tLoss: 417.9296\n",
      "Training Epoch: 19 [28032/49669]\tLoss: 411.1436\n",
      "Training Epoch: 19 [28096/49669]\tLoss: 400.4111\n",
      "Training Epoch: 19 [28160/49669]\tLoss: 387.1539\n",
      "Training Epoch: 19 [28224/49669]\tLoss: 393.4102\n",
      "Training Epoch: 19 [28288/49669]\tLoss: 392.2852\n",
      "Training Epoch: 19 [28352/49669]\tLoss: 415.5477\n",
      "Training Epoch: 19 [28416/49669]\tLoss: 369.2074\n",
      "Training Epoch: 19 [28480/49669]\tLoss: 419.8941\n",
      "Training Epoch: 19 [28544/49669]\tLoss: 409.8754\n",
      "Training Epoch: 19 [28608/49669]\tLoss: 389.2396\n",
      "Training Epoch: 19 [28672/49669]\tLoss: 414.9423\n",
      "Training Epoch: 19 [28736/49669]\tLoss: 389.9496\n",
      "Training Epoch: 19 [28800/49669]\tLoss: 406.0256\n",
      "Training Epoch: 19 [28864/49669]\tLoss: 374.2216\n",
      "Training Epoch: 19 [28928/49669]\tLoss: 428.9640\n",
      "Training Epoch: 19 [28992/49669]\tLoss: 398.6296\n",
      "Training Epoch: 19 [29056/49669]\tLoss: 429.6408\n",
      "Training Epoch: 19 [29120/49669]\tLoss: 376.2017\n",
      "Training Epoch: 19 [29184/49669]\tLoss: 361.7906\n",
      "Training Epoch: 19 [29248/49669]\tLoss: 411.1571\n",
      "Training Epoch: 19 [29312/49669]\tLoss: 408.5621\n",
      "Training Epoch: 19 [29376/49669]\tLoss: 410.4584\n",
      "Training Epoch: 19 [29440/49669]\tLoss: 401.7927\n",
      "Training Epoch: 19 [29504/49669]\tLoss: 419.7914\n",
      "Training Epoch: 19 [29568/49669]\tLoss: 423.2348\n",
      "Training Epoch: 19 [29632/49669]\tLoss: 421.1020\n",
      "Training Epoch: 19 [29696/49669]\tLoss: 400.1958\n",
      "Training Epoch: 19 [29760/49669]\tLoss: 433.5191\n",
      "Training Epoch: 19 [29824/49669]\tLoss: 430.0626\n",
      "Training Epoch: 19 [29888/49669]\tLoss: 427.7821\n",
      "Training Epoch: 19 [29952/49669]\tLoss: 406.3509\n",
      "Training Epoch: 19 [30016/49669]\tLoss: 443.1478\n",
      "Training Epoch: 19 [30080/49669]\tLoss: 440.3705\n",
      "Training Epoch: 19 [30144/49669]\tLoss: 425.9280\n",
      "Training Epoch: 19 [30208/49669]\tLoss: 442.3240\n",
      "Training Epoch: 19 [30272/49669]\tLoss: 419.4101\n",
      "Training Epoch: 19 [30336/49669]\tLoss: 392.5263\n",
      "Training Epoch: 19 [30400/49669]\tLoss: 401.9061\n",
      "Training Epoch: 19 [30464/49669]\tLoss: 379.8016\n",
      "Training Epoch: 19 [30528/49669]\tLoss: 417.4313\n",
      "Training Epoch: 19 [30592/49669]\tLoss: 406.9653\n",
      "Training Epoch: 19 [30656/49669]\tLoss: 441.0046\n",
      "Training Epoch: 19 [30720/49669]\tLoss: 440.3683\n",
      "Training Epoch: 19 [30784/49669]\tLoss: 435.4515\n",
      "Training Epoch: 19 [30848/49669]\tLoss: 456.5465\n",
      "Training Epoch: 19 [30912/49669]\tLoss: 411.6881\n",
      "Training Epoch: 19 [30976/49669]\tLoss: 457.7009\n",
      "Training Epoch: 19 [31040/49669]\tLoss: 397.9323\n",
      "Training Epoch: 19 [31104/49669]\tLoss: 418.4486\n",
      "Training Epoch: 19 [31168/49669]\tLoss: 420.2363\n",
      "Training Epoch: 19 [31232/49669]\tLoss: 414.8532\n",
      "Training Epoch: 19 [31296/49669]\tLoss: 433.8034\n",
      "Training Epoch: 19 [31360/49669]\tLoss: 440.8283\n",
      "Training Epoch: 19 [31424/49669]\tLoss: 390.1073\n",
      "Training Epoch: 19 [31488/49669]\tLoss: 420.0340\n",
      "Training Epoch: 19 [31552/49669]\tLoss: 394.6569\n",
      "Training Epoch: 19 [31616/49669]\tLoss: 411.8426\n",
      "Training Epoch: 19 [31680/49669]\tLoss: 410.5403\n",
      "Training Epoch: 19 [31744/49669]\tLoss: 419.1006\n",
      "Training Epoch: 19 [31808/49669]\tLoss: 417.7060\n",
      "Training Epoch: 19 [31872/49669]\tLoss: 445.6717\n",
      "Training Epoch: 19 [31936/49669]\tLoss: 401.2138\n",
      "Training Epoch: 19 [32000/49669]\tLoss: 438.2811\n",
      "Training Epoch: 19 [32064/49669]\tLoss: 404.3133\n",
      "Training Epoch: 19 [32128/49669]\tLoss: 425.2409\n",
      "Training Epoch: 19 [32192/49669]\tLoss: 431.4688\n",
      "Training Epoch: 19 [32256/49669]\tLoss: 423.9868\n",
      "Training Epoch: 19 [32320/49669]\tLoss: 426.9889\n",
      "Training Epoch: 19 [32384/49669]\tLoss: 395.4357\n",
      "Training Epoch: 19 [32448/49669]\tLoss: 404.3731\n",
      "Training Epoch: 19 [32512/49669]\tLoss: 400.8929\n",
      "Training Epoch: 19 [32576/49669]\tLoss: 410.0854\n",
      "Training Epoch: 19 [32640/49669]\tLoss: 422.4729\n",
      "Training Epoch: 19 [32704/49669]\tLoss: 367.7561\n",
      "Training Epoch: 19 [32768/49669]\tLoss: 379.9256\n",
      "Training Epoch: 19 [32832/49669]\tLoss: 421.0327\n",
      "Training Epoch: 19 [32896/49669]\tLoss: 419.5004\n",
      "Training Epoch: 19 [32960/49669]\tLoss: 381.6024\n",
      "Training Epoch: 19 [33024/49669]\tLoss: 403.1396\n",
      "Training Epoch: 19 [33088/49669]\tLoss: 402.4284\n",
      "Training Epoch: 19 [33152/49669]\tLoss: 397.8378\n",
      "Training Epoch: 19 [33216/49669]\tLoss: 428.0676\n",
      "Training Epoch: 19 [33280/49669]\tLoss: 428.8610\n",
      "Training Epoch: 19 [33344/49669]\tLoss: 402.4271\n",
      "Training Epoch: 19 [33408/49669]\tLoss: 413.2259\n",
      "Training Epoch: 19 [33472/49669]\tLoss: 390.3617\n",
      "Training Epoch: 19 [33536/49669]\tLoss: 390.4491\n",
      "Training Epoch: 19 [33600/49669]\tLoss: 398.1567\n",
      "Training Epoch: 19 [33664/49669]\tLoss: 383.9190\n",
      "Training Epoch: 19 [33728/49669]\tLoss: 432.4008\n",
      "Training Epoch: 19 [33792/49669]\tLoss: 434.2528\n",
      "Training Epoch: 19 [33856/49669]\tLoss: 401.0593\n",
      "Training Epoch: 19 [33920/49669]\tLoss: 422.3513\n",
      "Training Epoch: 19 [33984/49669]\tLoss: 394.4659\n",
      "Training Epoch: 19 [34048/49669]\tLoss: 378.5476\n",
      "Training Epoch: 19 [34112/49669]\tLoss: 407.2733\n",
      "Training Epoch: 19 [34176/49669]\tLoss: 408.8361\n",
      "Training Epoch: 19 [34240/49669]\tLoss: 381.1491\n",
      "Training Epoch: 19 [34304/49669]\tLoss: 393.8157\n",
      "Training Epoch: 19 [34368/49669]\tLoss: 411.2607\n",
      "Training Epoch: 19 [34432/49669]\tLoss: 421.6479\n",
      "Training Epoch: 19 [34496/49669]\tLoss: 437.5667\n",
      "Training Epoch: 19 [34560/49669]\tLoss: 409.6540\n",
      "Training Epoch: 19 [34624/49669]\tLoss: 411.2767\n",
      "Training Epoch: 19 [34688/49669]\tLoss: 396.8986\n",
      "Training Epoch: 19 [34752/49669]\tLoss: 397.8207\n",
      "Training Epoch: 19 [34816/49669]\tLoss: 396.4825\n",
      "Training Epoch: 19 [34880/49669]\tLoss: 399.9211\n",
      "Training Epoch: 19 [34944/49669]\tLoss: 420.8601\n",
      "Training Epoch: 19 [35008/49669]\tLoss: 404.0832\n",
      "Training Epoch: 19 [35072/49669]\tLoss: 398.8136\n",
      "Training Epoch: 19 [35136/49669]\tLoss: 372.4579\n",
      "Training Epoch: 19 [35200/49669]\tLoss: 391.9124\n",
      "Training Epoch: 19 [35264/49669]\tLoss: 437.7804\n",
      "Training Epoch: 19 [35328/49669]\tLoss: 396.1695\n",
      "Training Epoch: 19 [35392/49669]\tLoss: 376.4929\n",
      "Training Epoch: 19 [35456/49669]\tLoss: 348.4572\n",
      "Training Epoch: 19 [35520/49669]\tLoss: 419.9878\n",
      "Training Epoch: 19 [35584/49669]\tLoss: 399.0701\n",
      "Training Epoch: 19 [35648/49669]\tLoss: 407.4171\n",
      "Training Epoch: 19 [35712/49669]\tLoss: 395.3687\n",
      "Training Epoch: 19 [35776/49669]\tLoss: 413.0438\n",
      "Training Epoch: 19 [35840/49669]\tLoss: 377.4003\n",
      "Training Epoch: 19 [35904/49669]\tLoss: 397.9238\n",
      "Training Epoch: 19 [35968/49669]\tLoss: 391.4376\n",
      "Training Epoch: 19 [36032/49669]\tLoss: 414.6432\n",
      "Training Epoch: 19 [36096/49669]\tLoss: 394.9662\n",
      "Training Epoch: 19 [36160/49669]\tLoss: 428.3450\n",
      "Training Epoch: 19 [36224/49669]\tLoss: 392.6180\n",
      "Training Epoch: 19 [36288/49669]\tLoss: 403.8722\n",
      "Training Epoch: 19 [36352/49669]\tLoss: 377.9390\n",
      "Training Epoch: 19 [36416/49669]\tLoss: 387.8710\n",
      "Training Epoch: 19 [36480/49669]\tLoss: 410.5477\n",
      "Training Epoch: 19 [36544/49669]\tLoss: 427.0072\n",
      "Training Epoch: 19 [36608/49669]\tLoss: 379.9769\n",
      "Training Epoch: 19 [36672/49669]\tLoss: 428.7758\n",
      "Training Epoch: 19 [36736/49669]\tLoss: 407.0526\n",
      "Training Epoch: 19 [36800/49669]\tLoss: 422.9188\n",
      "Training Epoch: 19 [36864/49669]\tLoss: 411.7683\n",
      "Training Epoch: 19 [36928/49669]\tLoss: 383.3250\n",
      "Training Epoch: 19 [36992/49669]\tLoss: 421.0580\n",
      "Training Epoch: 19 [37056/49669]\tLoss: 423.2599\n",
      "Training Epoch: 19 [37120/49669]\tLoss: 446.1015\n",
      "Training Epoch: 19 [37184/49669]\tLoss: 410.3789\n",
      "Training Epoch: 19 [37248/49669]\tLoss: 405.0574\n",
      "Training Epoch: 19 [37312/49669]\tLoss: 416.1294\n",
      "Training Epoch: 19 [37376/49669]\tLoss: 400.1403\n",
      "Training Epoch: 19 [37440/49669]\tLoss: 390.9332\n",
      "Training Epoch: 19 [37504/49669]\tLoss: 403.7744\n",
      "Training Epoch: 19 [37568/49669]\tLoss: 426.1888\n",
      "Training Epoch: 19 [37632/49669]\tLoss: 430.2588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [37696/49669]\tLoss: 401.8072\n",
      "Training Epoch: 19 [37760/49669]\tLoss: 393.7903\n",
      "Training Epoch: 19 [37824/49669]\tLoss: 413.4675\n",
      "Training Epoch: 19 [37888/49669]\tLoss: 413.6452\n",
      "Training Epoch: 19 [37952/49669]\tLoss: 416.7943\n",
      "Training Epoch: 19 [38016/49669]\tLoss: 405.7139\n",
      "Training Epoch: 19 [38080/49669]\tLoss: 429.1188\n",
      "Training Epoch: 19 [38144/49669]\tLoss: 411.8955\n",
      "Training Epoch: 19 [38208/49669]\tLoss: 417.6931\n",
      "Training Epoch: 19 [38272/49669]\tLoss: 402.0798\n",
      "Training Epoch: 19 [38336/49669]\tLoss: 406.1339\n",
      "Training Epoch: 19 [38400/49669]\tLoss: 390.8734\n",
      "Training Epoch: 19 [38464/49669]\tLoss: 446.0634\n",
      "Training Epoch: 19 [38528/49669]\tLoss: 387.9413\n",
      "Training Epoch: 19 [38592/49669]\tLoss: 419.0724\n",
      "Training Epoch: 19 [38656/49669]\tLoss: 407.3415\n",
      "Training Epoch: 19 [38720/49669]\tLoss: 423.6276\n",
      "Training Epoch: 19 [38784/49669]\tLoss: 432.9003\n",
      "Training Epoch: 19 [38848/49669]\tLoss: 421.7834\n",
      "Training Epoch: 19 [38912/49669]\tLoss: 403.0771\n",
      "Training Epoch: 19 [38976/49669]\tLoss: 413.9250\n",
      "Training Epoch: 19 [39040/49669]\tLoss: 391.9217\n",
      "Training Epoch: 19 [39104/49669]\tLoss: 418.4058\n",
      "Training Epoch: 19 [39168/49669]\tLoss: 424.6620\n",
      "Training Epoch: 19 [39232/49669]\tLoss: 392.4094\n",
      "Training Epoch: 19 [39296/49669]\tLoss: 403.0846\n",
      "Training Epoch: 19 [39360/49669]\tLoss: 407.6607\n",
      "Training Epoch: 19 [39424/49669]\tLoss: 399.5971\n",
      "Training Epoch: 19 [39488/49669]\tLoss: 424.2998\n",
      "Training Epoch: 19 [39552/49669]\tLoss: 423.3061\n",
      "Training Epoch: 19 [39616/49669]\tLoss: 435.7025\n",
      "Training Epoch: 19 [39680/49669]\tLoss: 381.4054\n",
      "Training Epoch: 19 [39744/49669]\tLoss: 417.2273\n",
      "Training Epoch: 19 [39808/49669]\tLoss: 405.1251\n",
      "Training Epoch: 19 [39872/49669]\tLoss: 410.2573\n",
      "Training Epoch: 19 [39936/49669]\tLoss: 394.2863\n",
      "Training Epoch: 19 [40000/49669]\tLoss: 418.5960\n",
      "Training Epoch: 19 [40064/49669]\tLoss: 417.4666\n",
      "Training Epoch: 19 [40128/49669]\tLoss: 397.1496\n",
      "Training Epoch: 19 [40192/49669]\tLoss: 401.4831\n",
      "Training Epoch: 19 [40256/49669]\tLoss: 407.0675\n",
      "Training Epoch: 19 [40320/49669]\tLoss: 408.1498\n",
      "Training Epoch: 19 [40384/49669]\tLoss: 431.8943\n",
      "Training Epoch: 19 [40448/49669]\tLoss: 432.7384\n",
      "Training Epoch: 19 [40512/49669]\tLoss: 425.6772\n",
      "Training Epoch: 19 [40576/49669]\tLoss: 450.2497\n",
      "Training Epoch: 19 [40640/49669]\tLoss: 412.4298\n",
      "Training Epoch: 19 [40704/49669]\tLoss: 431.3276\n",
      "Training Epoch: 19 [40768/49669]\tLoss: 440.5212\n",
      "Training Epoch: 19 [40832/49669]\tLoss: 437.0470\n",
      "Training Epoch: 19 [40896/49669]\tLoss: 420.3263\n",
      "Training Epoch: 19 [40960/49669]\tLoss: 432.5617\n",
      "Training Epoch: 19 [41024/49669]\tLoss: 418.1566\n",
      "Training Epoch: 19 [41088/49669]\tLoss: 395.7130\n",
      "Training Epoch: 19 [41152/49669]\tLoss: 386.4631\n",
      "Training Epoch: 19 [41216/49669]\tLoss: 413.1227\n",
      "Training Epoch: 19 [41280/49669]\tLoss: 398.5224\n",
      "Training Epoch: 19 [41344/49669]\tLoss: 411.8447\n",
      "Training Epoch: 19 [41408/49669]\tLoss: 469.8629\n",
      "Training Epoch: 19 [41472/49669]\tLoss: 426.9037\n",
      "Training Epoch: 19 [41536/49669]\tLoss: 404.4833\n",
      "Training Epoch: 19 [41600/49669]\tLoss: 402.9779\n",
      "Training Epoch: 19 [41664/49669]\tLoss: 396.1602\n",
      "Training Epoch: 19 [41728/49669]\tLoss: 401.4322\n",
      "Training Epoch: 19 [41792/49669]\tLoss: 422.1990\n",
      "Training Epoch: 19 [41856/49669]\tLoss: 414.7303\n",
      "Training Epoch: 19 [41920/49669]\tLoss: 408.2983\n",
      "Training Epoch: 19 [41984/49669]\tLoss: 397.4796\n",
      "Training Epoch: 19 [42048/49669]\tLoss: 407.8503\n",
      "Training Epoch: 19 [42112/49669]\tLoss: 417.8287\n",
      "Training Epoch: 19 [42176/49669]\tLoss: 364.1547\n",
      "Training Epoch: 19 [42240/49669]\tLoss: 423.0217\n",
      "Training Epoch: 19 [42304/49669]\tLoss: 414.6826\n",
      "Training Epoch: 19 [42368/49669]\tLoss: 415.0750\n",
      "Training Epoch: 19 [42432/49669]\tLoss: 425.7400\n",
      "Training Epoch: 19 [42496/49669]\tLoss: 422.1541\n",
      "Training Epoch: 19 [42560/49669]\tLoss: 398.7710\n",
      "Training Epoch: 19 [42624/49669]\tLoss: 399.9619\n",
      "Training Epoch: 19 [42688/49669]\tLoss: 393.9637\n",
      "Training Epoch: 19 [42752/49669]\tLoss: 408.7024\n",
      "Training Epoch: 19 [42816/49669]\tLoss: 369.9799\n",
      "Training Epoch: 19 [42880/49669]\tLoss: 394.6838\n",
      "Training Epoch: 19 [42944/49669]\tLoss: 399.1070\n",
      "Training Epoch: 19 [43008/49669]\tLoss: 390.2939\n",
      "Training Epoch: 19 [43072/49669]\tLoss: 394.3585\n",
      "Training Epoch: 19 [43136/49669]\tLoss: 412.9026\n",
      "Training Epoch: 19 [43200/49669]\tLoss: 426.5452\n",
      "Training Epoch: 19 [43264/49669]\tLoss: 402.5427\n",
      "Training Epoch: 19 [43328/49669]\tLoss: 393.5863\n",
      "Training Epoch: 19 [43392/49669]\tLoss: 409.3237\n",
      "Training Epoch: 19 [43456/49669]\tLoss: 389.7868\n",
      "Training Epoch: 19 [43520/49669]\tLoss: 397.4499\n",
      "Training Epoch: 19 [43584/49669]\tLoss: 394.8671\n",
      "Training Epoch: 19 [43648/49669]\tLoss: 418.8322\n",
      "Training Epoch: 19 [43712/49669]\tLoss: 422.7842\n",
      "Training Epoch: 19 [43776/49669]\tLoss: 414.1057\n",
      "Training Epoch: 19 [43840/49669]\tLoss: 409.7753\n",
      "Training Epoch: 19 [43904/49669]\tLoss: 409.1968\n",
      "Training Epoch: 19 [43968/49669]\tLoss: 428.2892\n",
      "Training Epoch: 19 [44032/49669]\tLoss: 385.0776\n",
      "Training Epoch: 19 [44096/49669]\tLoss: 397.4608\n",
      "Training Epoch: 19 [44160/49669]\tLoss: 399.1512\n",
      "Training Epoch: 19 [44224/49669]\tLoss: 415.9954\n",
      "Training Epoch: 19 [44288/49669]\tLoss: 444.3987\n",
      "Training Epoch: 19 [44352/49669]\tLoss: 386.5683\n",
      "Training Epoch: 19 [44416/49669]\tLoss: 391.3033\n",
      "Training Epoch: 19 [44480/49669]\tLoss: 417.7853\n",
      "Training Epoch: 19 [44544/49669]\tLoss: 394.0829\n",
      "Training Epoch: 19 [44608/49669]\tLoss: 404.2440\n",
      "Training Epoch: 19 [44672/49669]\tLoss: 421.3125\n",
      "Training Epoch: 19 [44736/49669]\tLoss: 426.8329\n",
      "Training Epoch: 19 [44800/49669]\tLoss: 433.7383\n",
      "Training Epoch: 19 [44864/49669]\tLoss: 386.7591\n",
      "Training Epoch: 19 [44928/49669]\tLoss: 442.2562\n",
      "Training Epoch: 19 [44992/49669]\tLoss: 384.2079\n",
      "Training Epoch: 19 [45056/49669]\tLoss: 413.1937\n",
      "Training Epoch: 19 [45120/49669]\tLoss: 383.5188\n",
      "Training Epoch: 19 [45184/49669]\tLoss: 446.2329\n",
      "Training Epoch: 19 [45248/49669]\tLoss: 400.2567\n",
      "Training Epoch: 19 [45312/49669]\tLoss: 431.7385\n",
      "Training Epoch: 19 [45376/49669]\tLoss: 409.4826\n",
      "Training Epoch: 19 [45440/49669]\tLoss: 422.1702\n",
      "Training Epoch: 19 [45504/49669]\tLoss: 445.9599\n",
      "Training Epoch: 19 [45568/49669]\tLoss: 441.3785\n",
      "Training Epoch: 19 [45632/49669]\tLoss: 383.8233\n",
      "Training Epoch: 19 [45696/49669]\tLoss: 392.1882\n",
      "Training Epoch: 19 [45760/49669]\tLoss: 399.8545\n",
      "Training Epoch: 19 [45824/49669]\tLoss: 403.5048\n",
      "Training Epoch: 19 [45888/49669]\tLoss: 381.2883\n",
      "Training Epoch: 19 [45952/49669]\tLoss: 403.5696\n",
      "Training Epoch: 19 [46016/49669]\tLoss: 392.0104\n",
      "Training Epoch: 19 [46080/49669]\tLoss: 398.3986\n",
      "Training Epoch: 19 [46144/49669]\tLoss: 440.1442\n",
      "Training Epoch: 19 [46208/49669]\tLoss: 425.6335\n",
      "Training Epoch: 19 [46272/49669]\tLoss: 427.3227\n",
      "Training Epoch: 19 [46336/49669]\tLoss: 396.9778\n",
      "Training Epoch: 19 [46400/49669]\tLoss: 408.8792\n",
      "Training Epoch: 19 [46464/49669]\tLoss: 453.1340\n",
      "Training Epoch: 19 [46528/49669]\tLoss: 415.1587\n",
      "Training Epoch: 19 [46592/49669]\tLoss: 446.2944\n",
      "Training Epoch: 19 [46656/49669]\tLoss: 436.5809\n",
      "Training Epoch: 19 [46720/49669]\tLoss: 447.8369\n",
      "Training Epoch: 19 [46784/49669]\tLoss: 464.2544\n",
      "Training Epoch: 19 [46848/49669]\tLoss: 474.7519\n",
      "Training Epoch: 19 [46912/49669]\tLoss: 485.0613\n",
      "Training Epoch: 19 [46976/49669]\tLoss: 448.9689\n",
      "Training Epoch: 19 [47040/49669]\tLoss: 407.0204\n",
      "Training Epoch: 19 [47104/49669]\tLoss: 429.1205\n",
      "Training Epoch: 19 [47168/49669]\tLoss: 453.5698\n",
      "Training Epoch: 19 [47232/49669]\tLoss: 451.3169\n",
      "Training Epoch: 19 [47296/49669]\tLoss: 501.1370\n",
      "Training Epoch: 19 [47360/49669]\tLoss: 486.1722\n",
      "Training Epoch: 19 [47424/49669]\tLoss: 426.4441\n",
      "Training Epoch: 19 [47488/49669]\tLoss: 381.5359\n",
      "Training Epoch: 19 [47552/49669]\tLoss: 435.9802\n",
      "Training Epoch: 19 [47616/49669]\tLoss: 439.9498\n",
      "Training Epoch: 19 [47680/49669]\tLoss: 471.8396\n",
      "Training Epoch: 19 [47744/49669]\tLoss: 416.7272\n",
      "Training Epoch: 19 [47808/49669]\tLoss: 404.8577\n",
      "Training Epoch: 19 [47872/49669]\tLoss: 448.1219\n",
      "Training Epoch: 19 [47936/49669]\tLoss: 440.8469\n",
      "Training Epoch: 19 [48000/49669]\tLoss: 450.6829\n",
      "Training Epoch: 19 [48064/49669]\tLoss: 413.2787\n",
      "Training Epoch: 19 [48128/49669]\tLoss: 396.3348\n",
      "Training Epoch: 19 [48192/49669]\tLoss: 419.6758\n",
      "Training Epoch: 19 [48256/49669]\tLoss: 426.4717\n",
      "Training Epoch: 19 [48320/49669]\tLoss: 406.4870\n",
      "Training Epoch: 19 [48384/49669]\tLoss: 427.6519\n",
      "Training Epoch: 19 [48448/49669]\tLoss: 395.4662\n",
      "Training Epoch: 19 [48512/49669]\tLoss: 416.2215\n",
      "Training Epoch: 19 [48576/49669]\tLoss: 429.4267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [48640/49669]\tLoss: 430.8986\n",
      "Training Epoch: 19 [48704/49669]\tLoss: 383.9333\n",
      "Training Epoch: 19 [48768/49669]\tLoss: 411.3967\n",
      "Training Epoch: 19 [48832/49669]\tLoss: 421.5717\n",
      "Training Epoch: 19 [48896/49669]\tLoss: 421.8141\n",
      "Training Epoch: 19 [48960/49669]\tLoss: 404.5756\n",
      "Training Epoch: 19 [49024/49669]\tLoss: 378.4594\n",
      "Training Epoch: 19 [49088/49669]\tLoss: 409.2263\n",
      "Training Epoch: 19 [49152/49669]\tLoss: 421.9084\n",
      "Training Epoch: 19 [49216/49669]\tLoss: 420.9535\n",
      "Training Epoch: 19 [49280/49669]\tLoss: 401.5268\n",
      "Training Epoch: 19 [49344/49669]\tLoss: 379.3596\n",
      "Training Epoch: 19 [49408/49669]\tLoss: 400.3905\n",
      "Training Epoch: 19 [49472/49669]\tLoss: 362.6990\n",
      "Training Epoch: 19 [49536/49669]\tLoss: 406.2036\n",
      "Training Epoch: 19 [49600/49669]\tLoss: 388.6393\n",
      "Training Epoch: 19 [49664/49669]\tLoss: 388.3760\n",
      "Training Epoch: 19 [49669/49669]\tLoss: 413.4752\n",
      "Training Epoch: 19 [5519/5519]\tLoss: 408.0740\n",
      "Training Epoch: 20 [64/49669]\tLoss: 424.9959\n",
      "Training Epoch: 20 [128/49669]\tLoss: 415.7310\n",
      "Training Epoch: 20 [192/49669]\tLoss: 423.8301\n",
      "Training Epoch: 20 [256/49669]\tLoss: 380.7723\n",
      "Training Epoch: 20 [320/49669]\tLoss: 404.4186\n",
      "Training Epoch: 20 [384/49669]\tLoss: 411.1809\n",
      "Training Epoch: 20 [448/49669]\tLoss: 398.3843\n",
      "Training Epoch: 20 [512/49669]\tLoss: 421.5830\n",
      "Training Epoch: 20 [576/49669]\tLoss: 427.7610\n",
      "Training Epoch: 20 [640/49669]\tLoss: 390.6148\n",
      "Training Epoch: 20 [704/49669]\tLoss: 426.4871\n",
      "Training Epoch: 20 [768/49669]\tLoss: 407.4536\n",
      "Training Epoch: 20 [832/49669]\tLoss: 410.6649\n",
      "Training Epoch: 20 [896/49669]\tLoss: 401.4475\n",
      "Training Epoch: 20 [960/49669]\tLoss: 404.6921\n",
      "Training Epoch: 20 [1024/49669]\tLoss: 427.0567\n",
      "Training Epoch: 20 [1088/49669]\tLoss: 430.6491\n",
      "Training Epoch: 20 [1152/49669]\tLoss: 400.5739\n",
      "Training Epoch: 20 [1216/49669]\tLoss: 392.4899\n",
      "Training Epoch: 20 [1280/49669]\tLoss: 383.9772\n",
      "Training Epoch: 20 [1344/49669]\tLoss: 393.7037\n",
      "Training Epoch: 20 [1408/49669]\tLoss: 408.4589\n",
      "Training Epoch: 20 [1472/49669]\tLoss: 433.2367\n",
      "Training Epoch: 20 [1536/49669]\tLoss: 412.1094\n",
      "Training Epoch: 20 [1600/49669]\tLoss: 410.7818\n",
      "Training Epoch: 20 [1664/49669]\tLoss: 404.3676\n",
      "Training Epoch: 20 [1728/49669]\tLoss: 358.5474\n",
      "Training Epoch: 20 [1792/49669]\tLoss: 412.0546\n",
      "Training Epoch: 20 [1856/49669]\tLoss: 398.9268\n",
      "Training Epoch: 20 [1920/49669]\tLoss: 412.0594\n",
      "Training Epoch: 20 [1984/49669]\tLoss: 387.4335\n",
      "Training Epoch: 20 [2048/49669]\tLoss: 422.4134\n",
      "Training Epoch: 20 [2112/49669]\tLoss: 405.3635\n",
      "Training Epoch: 20 [2176/49669]\tLoss: 426.7333\n",
      "Training Epoch: 20 [2240/49669]\tLoss: 402.6837\n",
      "Training Epoch: 20 [2304/49669]\tLoss: 375.4595\n",
      "Training Epoch: 20 [2368/49669]\tLoss: 422.1752\n",
      "Training Epoch: 20 [2432/49669]\tLoss: 396.8473\n",
      "Training Epoch: 20 [2496/49669]\tLoss: 393.1983\n",
      "Training Epoch: 20 [2560/49669]\tLoss: 420.8966\n",
      "Training Epoch: 20 [2624/49669]\tLoss: 418.0131\n",
      "Training Epoch: 20 [2688/49669]\tLoss: 412.7866\n",
      "Training Epoch: 20 [2752/49669]\tLoss: 392.6147\n",
      "Training Epoch: 20 [2816/49669]\tLoss: 409.2708\n",
      "Training Epoch: 20 [2880/49669]\tLoss: 404.7708\n",
      "Training Epoch: 20 [2944/49669]\tLoss: 400.1874\n",
      "Training Epoch: 20 [3008/49669]\tLoss: 425.4398\n",
      "Training Epoch: 20 [3072/49669]\tLoss: 414.8036\n",
      "Training Epoch: 20 [3136/49669]\tLoss: 399.3030\n",
      "Training Epoch: 20 [3200/49669]\tLoss: 392.3002\n",
      "Training Epoch: 20 [3264/49669]\tLoss: 403.4266\n",
      "Training Epoch: 20 [3328/49669]\tLoss: 407.2110\n",
      "Training Epoch: 20 [3392/49669]\tLoss: 402.7540\n",
      "Training Epoch: 20 [3456/49669]\tLoss: 399.0137\n",
      "Training Epoch: 20 [3520/49669]\tLoss: 393.2025\n",
      "Training Epoch: 20 [3584/49669]\tLoss: 403.6614\n",
      "Training Epoch: 20 [3648/49669]\tLoss: 433.7253\n",
      "Training Epoch: 20 [3712/49669]\tLoss: 394.8084\n",
      "Training Epoch: 20 [3776/49669]\tLoss: 430.5766\n",
      "Training Epoch: 20 [3840/49669]\tLoss: 420.7593\n",
      "Training Epoch: 20 [3904/49669]\tLoss: 393.9493\n",
      "Training Epoch: 20 [3968/49669]\tLoss: 383.7909\n",
      "Training Epoch: 20 [4032/49669]\tLoss: 404.3570\n",
      "Training Epoch: 20 [4096/49669]\tLoss: 407.3958\n",
      "Training Epoch: 20 [4160/49669]\tLoss: 404.0847\n",
      "Training Epoch: 20 [4224/49669]\tLoss: 415.4067\n",
      "Training Epoch: 20 [4288/49669]\tLoss: 429.4906\n",
      "Training Epoch: 20 [4352/49669]\tLoss: 405.7066\n",
      "Training Epoch: 20 [4416/49669]\tLoss: 431.6895\n",
      "Training Epoch: 20 [4480/49669]\tLoss: 444.7588\n",
      "Training Epoch: 20 [4544/49669]\tLoss: 388.3307\n",
      "Training Epoch: 20 [4608/49669]\tLoss: 399.3020\n",
      "Training Epoch: 20 [4672/49669]\tLoss: 414.5849\n",
      "Training Epoch: 20 [4736/49669]\tLoss: 407.5924\n",
      "Training Epoch: 20 [4800/49669]\tLoss: 375.7200\n",
      "Training Epoch: 20 [4864/49669]\tLoss: 408.3386\n",
      "Training Epoch: 20 [4928/49669]\tLoss: 414.7829\n",
      "Training Epoch: 20 [4992/49669]\tLoss: 387.5630\n",
      "Training Epoch: 20 [5056/49669]\tLoss: 400.8452\n",
      "Training Epoch: 20 [5120/49669]\tLoss: 387.8208\n",
      "Training Epoch: 20 [5184/49669]\tLoss: 399.6994\n",
      "Training Epoch: 20 [5248/49669]\tLoss: 415.9172\n",
      "Training Epoch: 20 [5312/49669]\tLoss: 398.9130\n",
      "Training Epoch: 20 [5376/49669]\tLoss: 427.3336\n",
      "Training Epoch: 20 [5440/49669]\tLoss: 395.2588\n",
      "Training Epoch: 20 [5504/49669]\tLoss: 403.5459\n",
      "Training Epoch: 20 [5568/49669]\tLoss: 409.8336\n",
      "Training Epoch: 20 [5632/49669]\tLoss: 409.2455\n",
      "Training Epoch: 20 [5696/49669]\tLoss: 420.3944\n",
      "Training Epoch: 20 [5760/49669]\tLoss: 378.8799\n",
      "Training Epoch: 20 [5824/49669]\tLoss: 407.8413\n",
      "Training Epoch: 20 [5888/49669]\tLoss: 429.8971\n",
      "Training Epoch: 20 [5952/49669]\tLoss: 406.4877\n",
      "Training Epoch: 20 [6016/49669]\tLoss: 393.9791\n",
      "Training Epoch: 20 [6080/49669]\tLoss: 397.2771\n",
      "Training Epoch: 20 [6144/49669]\tLoss: 408.5572\n",
      "Training Epoch: 20 [6208/49669]\tLoss: 404.5895\n",
      "Training Epoch: 20 [6272/49669]\tLoss: 430.3020\n",
      "Training Epoch: 20 [6336/49669]\tLoss: 412.2355\n",
      "Training Epoch: 20 [6400/49669]\tLoss: 394.0699\n",
      "Training Epoch: 20 [6464/49669]\tLoss: 399.8534\n",
      "Training Epoch: 20 [6528/49669]\tLoss: 397.6100\n",
      "Training Epoch: 20 [6592/49669]\tLoss: 420.3246\n",
      "Training Epoch: 20 [6656/49669]\tLoss: 416.3515\n",
      "Training Epoch: 20 [6720/49669]\tLoss: 412.4619\n",
      "Training Epoch: 20 [6784/49669]\tLoss: 412.0511\n",
      "Training Epoch: 20 [6848/49669]\tLoss: 415.4466\n",
      "Training Epoch: 20 [6912/49669]\tLoss: 409.7131\n",
      "Training Epoch: 20 [6976/49669]\tLoss: 400.8154\n",
      "Training Epoch: 20 [7040/49669]\tLoss: 404.1390\n",
      "Training Epoch: 20 [7104/49669]\tLoss: 373.2127\n",
      "Training Epoch: 20 [7168/49669]\tLoss: 403.1876\n",
      "Training Epoch: 20 [7232/49669]\tLoss: 385.5707\n",
      "Training Epoch: 20 [7296/49669]\tLoss: 385.2531\n",
      "Training Epoch: 20 [7360/49669]\tLoss: 394.0740\n",
      "Training Epoch: 20 [7424/49669]\tLoss: 419.9730\n",
      "Training Epoch: 20 [7488/49669]\tLoss: 407.8391\n",
      "Training Epoch: 20 [7552/49669]\tLoss: 402.9377\n",
      "Training Epoch: 20 [7616/49669]\tLoss: 393.5529\n",
      "Training Epoch: 20 [7680/49669]\tLoss: 379.3595\n",
      "Training Epoch: 20 [7744/49669]\tLoss: 403.6718\n",
      "Training Epoch: 20 [7808/49669]\tLoss: 421.1172\n",
      "Training Epoch: 20 [7872/49669]\tLoss: 413.8327\n",
      "Training Epoch: 20 [7936/49669]\tLoss: 390.9211\n",
      "Training Epoch: 20 [8000/49669]\tLoss: 423.0974\n",
      "Training Epoch: 20 [8064/49669]\tLoss: 413.9695\n",
      "Training Epoch: 20 [8128/49669]\tLoss: 434.8032\n",
      "Training Epoch: 20 [8192/49669]\tLoss: 450.1087\n",
      "Training Epoch: 20 [8256/49669]\tLoss: 379.6261\n",
      "Training Epoch: 20 [8320/49669]\tLoss: 442.7861\n",
      "Training Epoch: 20 [8384/49669]\tLoss: 398.7697\n",
      "Training Epoch: 20 [8448/49669]\tLoss: 376.0929\n",
      "Training Epoch: 20 [8512/49669]\tLoss: 391.6760\n",
      "Training Epoch: 20 [8576/49669]\tLoss: 397.6522\n",
      "Training Epoch: 20 [8640/49669]\tLoss: 412.7016\n",
      "Training Epoch: 20 [8704/49669]\tLoss: 394.3641\n",
      "Training Epoch: 20 [8768/49669]\tLoss: 389.3640\n",
      "Training Epoch: 20 [8832/49669]\tLoss: 382.7607\n",
      "Training Epoch: 20 [8896/49669]\tLoss: 405.9558\n",
      "Training Epoch: 20 [8960/49669]\tLoss: 411.0329\n",
      "Training Epoch: 20 [9024/49669]\tLoss: 394.2993\n",
      "Training Epoch: 20 [9088/49669]\tLoss: 423.0159\n",
      "Training Epoch: 20 [9152/49669]\tLoss: 394.7428\n",
      "Training Epoch: 20 [9216/49669]\tLoss: 425.3585\n",
      "Training Epoch: 20 [9280/49669]\tLoss: 411.5184\n",
      "Training Epoch: 20 [9344/49669]\tLoss: 419.1095\n",
      "Training Epoch: 20 [9408/49669]\tLoss: 403.4650\n",
      "Training Epoch: 20 [9472/49669]\tLoss: 399.5085\n",
      "Training Epoch: 20 [9536/49669]\tLoss: 410.7688\n",
      "Training Epoch: 20 [9600/49669]\tLoss: 392.3646\n",
      "Training Epoch: 20 [9664/49669]\tLoss: 410.7384\n",
      "Training Epoch: 20 [9728/49669]\tLoss: 411.9524\n",
      "Training Epoch: 20 [9792/49669]\tLoss: 442.6625\n",
      "Training Epoch: 20 [9856/49669]\tLoss: 386.1243\n",
      "Training Epoch: 20 [9920/49669]\tLoss: 355.6215\n",
      "Training Epoch: 20 [9984/49669]\tLoss: 397.1656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [10048/49669]\tLoss: 417.2223\n",
      "Training Epoch: 20 [10112/49669]\tLoss: 404.5651\n",
      "Training Epoch: 20 [10176/49669]\tLoss: 399.1155\n",
      "Training Epoch: 20 [10240/49669]\tLoss: 437.8307\n",
      "Training Epoch: 20 [10304/49669]\tLoss: 442.5335\n",
      "Training Epoch: 20 [10368/49669]\tLoss: 402.9807\n",
      "Training Epoch: 20 [10432/49669]\tLoss: 413.4272\n",
      "Training Epoch: 20 [10496/49669]\tLoss: 417.8672\n",
      "Training Epoch: 20 [10560/49669]\tLoss: 399.1621\n",
      "Training Epoch: 20 [10624/49669]\tLoss: 433.3734\n",
      "Training Epoch: 20 [10688/49669]\tLoss: 420.3176\n",
      "Training Epoch: 20 [10752/49669]\tLoss: 434.5084\n",
      "Training Epoch: 20 [10816/49669]\tLoss: 397.5885\n",
      "Training Epoch: 20 [10880/49669]\tLoss: 391.7994\n",
      "Training Epoch: 20 [10944/49669]\tLoss: 384.3969\n",
      "Training Epoch: 20 [11008/49669]\tLoss: 397.9539\n",
      "Training Epoch: 20 [11072/49669]\tLoss: 428.5740\n",
      "Training Epoch: 20 [11136/49669]\tLoss: 422.9062\n",
      "Training Epoch: 20 [11200/49669]\tLoss: 407.5491\n",
      "Training Epoch: 20 [11264/49669]\tLoss: 425.8033\n",
      "Training Epoch: 20 [11328/49669]\tLoss: 413.2241\n",
      "Training Epoch: 20 [11392/49669]\tLoss: 379.7098\n",
      "Training Epoch: 20 [11456/49669]\tLoss: 396.7982\n",
      "Training Epoch: 20 [11520/49669]\tLoss: 416.5158\n",
      "Training Epoch: 20 [11584/49669]\tLoss: 410.0180\n",
      "Training Epoch: 20 [11648/49669]\tLoss: 430.9111\n",
      "Training Epoch: 20 [11712/49669]\tLoss: 406.7798\n",
      "Training Epoch: 20 [11776/49669]\tLoss: 398.2815\n",
      "Training Epoch: 20 [11840/49669]\tLoss: 426.5556\n",
      "Training Epoch: 20 [11904/49669]\tLoss: 399.9034\n",
      "Training Epoch: 20 [11968/49669]\tLoss: 385.1981\n",
      "Training Epoch: 20 [12032/49669]\tLoss: 422.8329\n",
      "Training Epoch: 20 [12096/49669]\tLoss: 393.4955\n",
      "Training Epoch: 20 [12160/49669]\tLoss: 409.1765\n",
      "Training Epoch: 20 [12224/49669]\tLoss: 413.9469\n",
      "Training Epoch: 20 [12288/49669]\tLoss: 399.3521\n",
      "Training Epoch: 20 [12352/49669]\tLoss: 403.2553\n",
      "Training Epoch: 20 [12416/49669]\tLoss: 391.6868\n",
      "Training Epoch: 20 [12480/49669]\tLoss: 412.0985\n",
      "Training Epoch: 20 [12544/49669]\tLoss: 402.1035\n",
      "Training Epoch: 20 [12608/49669]\tLoss: 399.2505\n",
      "Training Epoch: 20 [12672/49669]\tLoss: 395.6353\n",
      "Training Epoch: 20 [12736/49669]\tLoss: 395.3371\n",
      "Training Epoch: 20 [12800/49669]\tLoss: 421.5414\n",
      "Training Epoch: 20 [12864/49669]\tLoss: 418.8462\n",
      "Training Epoch: 20 [12928/49669]\tLoss: 383.9271\n",
      "Training Epoch: 20 [12992/49669]\tLoss: 375.0642\n",
      "Training Epoch: 20 [13056/49669]\tLoss: 387.7191\n",
      "Training Epoch: 20 [13120/49669]\tLoss: 378.1499\n",
      "Training Epoch: 20 [13184/49669]\tLoss: 401.9089\n",
      "Training Epoch: 20 [13248/49669]\tLoss: 399.7926\n",
      "Training Epoch: 20 [13312/49669]\tLoss: 403.7343\n",
      "Training Epoch: 20 [13376/49669]\tLoss: 397.6757\n",
      "Training Epoch: 20 [13440/49669]\tLoss: 387.0431\n",
      "Training Epoch: 20 [13504/49669]\tLoss: 397.1475\n",
      "Training Epoch: 20 [13568/49669]\tLoss: 440.5805\n",
      "Training Epoch: 20 [13632/49669]\tLoss: 403.8044\n",
      "Training Epoch: 20 [13696/49669]\tLoss: 414.3637\n",
      "Training Epoch: 20 [13760/49669]\tLoss: 408.6418\n",
      "Training Epoch: 20 [13824/49669]\tLoss: 394.4784\n",
      "Training Epoch: 20 [13888/49669]\tLoss: 403.0486\n",
      "Training Epoch: 20 [13952/49669]\tLoss: 389.4834\n",
      "Training Epoch: 20 [14016/49669]\tLoss: 427.0386\n",
      "Training Epoch: 20 [14080/49669]\tLoss: 400.9444\n",
      "Training Epoch: 20 [14144/49669]\tLoss: 415.7983\n",
      "Training Epoch: 20 [14208/49669]\tLoss: 392.1012\n",
      "Training Epoch: 20 [14272/49669]\tLoss: 426.9771\n",
      "Training Epoch: 20 [14336/49669]\tLoss: 382.6415\n",
      "Training Epoch: 20 [14400/49669]\tLoss: 407.5307\n",
      "Training Epoch: 20 [14464/49669]\tLoss: 403.0386\n",
      "Training Epoch: 20 [14528/49669]\tLoss: 426.8836\n",
      "Training Epoch: 20 [14592/49669]\tLoss: 405.6529\n",
      "Training Epoch: 20 [14656/49669]\tLoss: 421.3386\n",
      "Training Epoch: 20 [14720/49669]\tLoss: 414.9448\n",
      "Training Epoch: 20 [14784/49669]\tLoss: 386.4999\n",
      "Training Epoch: 20 [14848/49669]\tLoss: 415.4113\n",
      "Training Epoch: 20 [14912/49669]\tLoss: 421.3208\n",
      "Training Epoch: 20 [14976/49669]\tLoss: 427.2235\n",
      "Training Epoch: 20 [15040/49669]\tLoss: 408.4318\n",
      "Training Epoch: 20 [15104/49669]\tLoss: 389.7463\n",
      "Training Epoch: 20 [15168/49669]\tLoss: 403.9417\n",
      "Training Epoch: 20 [15232/49669]\tLoss: 416.6658\n",
      "Training Epoch: 20 [15296/49669]\tLoss: 386.7191\n",
      "Training Epoch: 20 [15360/49669]\tLoss: 397.5891\n",
      "Training Epoch: 20 [15424/49669]\tLoss: 376.4324\n",
      "Training Epoch: 20 [15488/49669]\tLoss: 406.6598\n",
      "Training Epoch: 20 [15552/49669]\tLoss: 415.6419\n",
      "Training Epoch: 20 [15616/49669]\tLoss: 371.9604\n",
      "Training Epoch: 20 [15680/49669]\tLoss: 403.0251\n",
      "Training Epoch: 20 [15744/49669]\tLoss: 420.0627\n",
      "Training Epoch: 20 [15808/49669]\tLoss: 426.4663\n",
      "Training Epoch: 20 [15872/49669]\tLoss: 414.4893\n",
      "Training Epoch: 20 [15936/49669]\tLoss: 406.7421\n",
      "Training Epoch: 20 [16000/49669]\tLoss: 405.4810\n",
      "Training Epoch: 20 [16064/49669]\tLoss: 402.2002\n",
      "Training Epoch: 20 [16128/49669]\tLoss: 412.7068\n",
      "Training Epoch: 20 [16192/49669]\tLoss: 386.5524\n",
      "Training Epoch: 20 [16256/49669]\tLoss: 400.3281\n",
      "Training Epoch: 20 [16320/49669]\tLoss: 426.9185\n",
      "Training Epoch: 20 [16384/49669]\tLoss: 400.7483\n",
      "Training Epoch: 20 [16448/49669]\tLoss: 411.3724\n",
      "Training Epoch: 20 [16512/49669]\tLoss: 439.6004\n",
      "Training Epoch: 20 [16576/49669]\tLoss: 401.8238\n",
      "Training Epoch: 20 [16640/49669]\tLoss: 392.2020\n",
      "Training Epoch: 20 [16704/49669]\tLoss: 389.0423\n",
      "Training Epoch: 20 [16768/49669]\tLoss: 402.6811\n",
      "Training Epoch: 20 [16832/49669]\tLoss: 399.4775\n",
      "Training Epoch: 20 [16896/49669]\tLoss: 407.6130\n",
      "Training Epoch: 20 [16960/49669]\tLoss: 368.1164\n",
      "Training Epoch: 20 [17024/49669]\tLoss: 401.5350\n",
      "Training Epoch: 20 [17088/49669]\tLoss: 409.4253\n",
      "Training Epoch: 20 [17152/49669]\tLoss: 429.8858\n",
      "Training Epoch: 20 [17216/49669]\tLoss: 414.1385\n",
      "Training Epoch: 20 [17280/49669]\tLoss: 369.4847\n",
      "Training Epoch: 20 [17344/49669]\tLoss: 400.6091\n",
      "Training Epoch: 20 [17408/49669]\tLoss: 417.7230\n",
      "Training Epoch: 20 [17472/49669]\tLoss: 417.4737\n",
      "Training Epoch: 20 [17536/49669]\tLoss: 440.6495\n",
      "Training Epoch: 20 [17600/49669]\tLoss: 408.0894\n",
      "Training Epoch: 20 [17664/49669]\tLoss: 406.1887\n",
      "Training Epoch: 20 [17728/49669]\tLoss: 401.6484\n",
      "Training Epoch: 20 [17792/49669]\tLoss: 394.4891\n",
      "Training Epoch: 20 [17856/49669]\tLoss: 419.7414\n",
      "Training Epoch: 20 [17920/49669]\tLoss: 398.3632\n",
      "Training Epoch: 20 [17984/49669]\tLoss: 386.4307\n",
      "Training Epoch: 20 [18048/49669]\tLoss: 440.7380\n",
      "Training Epoch: 20 [18112/49669]\tLoss: 405.4596\n",
      "Training Epoch: 20 [18176/49669]\tLoss: 394.1717\n",
      "Training Epoch: 20 [18240/49669]\tLoss: 429.8838\n",
      "Training Epoch: 20 [18304/49669]\tLoss: 433.4843\n",
      "Training Epoch: 20 [18368/49669]\tLoss: 411.8036\n",
      "Training Epoch: 20 [18432/49669]\tLoss: 410.7952\n",
      "Training Epoch: 20 [18496/49669]\tLoss: 410.7242\n",
      "Training Epoch: 20 [18560/49669]\tLoss: 424.0293\n",
      "Training Epoch: 20 [18624/49669]\tLoss: 438.5949\n",
      "Training Epoch: 20 [18688/49669]\tLoss: 416.9136\n",
      "Training Epoch: 20 [18752/49669]\tLoss: 436.5126\n",
      "Training Epoch: 20 [18816/49669]\tLoss: 433.7789\n",
      "Training Epoch: 20 [18880/49669]\tLoss: 436.3782\n",
      "Training Epoch: 20 [18944/49669]\tLoss: 457.3090\n",
      "Training Epoch: 20 [19008/49669]\tLoss: 446.3704\n",
      "Training Epoch: 20 [19072/49669]\tLoss: 479.0674\n",
      "Training Epoch: 20 [19136/49669]\tLoss: 436.3210\n",
      "Training Epoch: 20 [19200/49669]\tLoss: 412.9371\n",
      "Training Epoch: 20 [19264/49669]\tLoss: 392.9528\n",
      "Training Epoch: 20 [19328/49669]\tLoss: 405.4514\n",
      "Training Epoch: 20 [19392/49669]\tLoss: 419.5446\n",
      "Training Epoch: 20 [19456/49669]\tLoss: 453.9148\n",
      "Training Epoch: 20 [19520/49669]\tLoss: 450.3329\n",
      "Training Epoch: 20 [19584/49669]\tLoss: 460.4728\n",
      "Training Epoch: 20 [19648/49669]\tLoss: 418.8658\n",
      "Training Epoch: 20 [19712/49669]\tLoss: 406.3374\n",
      "Training Epoch: 20 [19776/49669]\tLoss: 404.3939\n",
      "Training Epoch: 20 [19840/49669]\tLoss: 446.6754\n",
      "Training Epoch: 20 [19904/49669]\tLoss: 399.6469\n",
      "Training Epoch: 20 [19968/49669]\tLoss: 449.4364\n",
      "Training Epoch: 20 [20032/49669]\tLoss: 424.6680\n",
      "Training Epoch: 20 [20096/49669]\tLoss: 425.9563\n",
      "Training Epoch: 20 [20160/49669]\tLoss: 401.4058\n",
      "Training Epoch: 20 [20224/49669]\tLoss: 405.0894\n",
      "Training Epoch: 20 [20288/49669]\tLoss: 408.7563\n",
      "Training Epoch: 20 [20352/49669]\tLoss: 407.0514\n",
      "Training Epoch: 20 [20416/49669]\tLoss: 415.8231\n",
      "Training Epoch: 20 [20480/49669]\tLoss: 378.4660\n",
      "Training Epoch: 20 [20544/49669]\tLoss: 392.2922\n",
      "Training Epoch: 20 [20608/49669]\tLoss: 409.2968\n",
      "Training Epoch: 20 [20672/49669]\tLoss: 401.8214\n",
      "Training Epoch: 20 [20736/49669]\tLoss: 405.0965\n",
      "Training Epoch: 20 [20800/49669]\tLoss: 380.3087\n",
      "Training Epoch: 20 [20864/49669]\tLoss: 376.9026\n",
      "Training Epoch: 20 [20928/49669]\tLoss: 389.6610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [20992/49669]\tLoss: 404.2135\n",
      "Training Epoch: 20 [21056/49669]\tLoss: 394.4934\n",
      "Training Epoch: 20 [21120/49669]\tLoss: 417.0705\n",
      "Training Epoch: 20 [21184/49669]\tLoss: 414.5228\n",
      "Training Epoch: 20 [21248/49669]\tLoss: 401.3658\n",
      "Training Epoch: 20 [21312/49669]\tLoss: 384.0018\n",
      "Training Epoch: 20 [21376/49669]\tLoss: 450.8618\n",
      "Training Epoch: 20 [21440/49669]\tLoss: 403.9823\n",
      "Training Epoch: 20 [21504/49669]\tLoss: 415.6996\n",
      "Training Epoch: 20 [21568/49669]\tLoss: 415.3894\n",
      "Training Epoch: 20 [21632/49669]\tLoss: 410.0260\n",
      "Training Epoch: 20 [21696/49669]\tLoss: 403.6895\n",
      "Training Epoch: 20 [21760/49669]\tLoss: 417.7308\n",
      "Training Epoch: 20 [21824/49669]\tLoss: 414.9339\n",
      "Training Epoch: 20 [21888/49669]\tLoss: 412.5696\n",
      "Training Epoch: 20 [21952/49669]\tLoss: 420.9770\n",
      "Training Epoch: 20 [22016/49669]\tLoss: 439.3954\n",
      "Training Epoch: 20 [22080/49669]\tLoss: 418.2524\n",
      "Training Epoch: 20 [22144/49669]\tLoss: 436.6819\n",
      "Training Epoch: 20 [22208/49669]\tLoss: 409.8058\n",
      "Training Epoch: 20 [22272/49669]\tLoss: 438.9088\n",
      "Training Epoch: 20 [22336/49669]\tLoss: 404.8530\n",
      "Training Epoch: 20 [22400/49669]\tLoss: 395.3038\n",
      "Training Epoch: 20 [22464/49669]\tLoss: 394.1656\n",
      "Training Epoch: 20 [22528/49669]\tLoss: 371.7246\n",
      "Training Epoch: 20 [22592/49669]\tLoss: 430.7321\n",
      "Training Epoch: 20 [22656/49669]\tLoss: 403.7781\n",
      "Training Epoch: 20 [22720/49669]\tLoss: 426.2226\n",
      "Training Epoch: 20 [22784/49669]\tLoss: 392.7411\n",
      "Training Epoch: 20 [22848/49669]\tLoss: 410.7418\n",
      "Training Epoch: 20 [22912/49669]\tLoss: 410.2726\n",
      "Training Epoch: 20 [22976/49669]\tLoss: 383.4044\n",
      "Training Epoch: 20 [23040/49669]\tLoss: 411.0606\n",
      "Training Epoch: 20 [23104/49669]\tLoss: 430.6905\n",
      "Training Epoch: 20 [23168/49669]\tLoss: 362.0737\n",
      "Training Epoch: 20 [23232/49669]\tLoss: 434.6542\n",
      "Training Epoch: 20 [23296/49669]\tLoss: 394.0215\n",
      "Training Epoch: 20 [23360/49669]\tLoss: 408.5528\n",
      "Training Epoch: 20 [23424/49669]\tLoss: 426.6599\n",
      "Training Epoch: 20 [23488/49669]\tLoss: 384.2824\n",
      "Training Epoch: 20 [23552/49669]\tLoss: 424.4879\n",
      "Training Epoch: 20 [23616/49669]\tLoss: 434.7124\n",
      "Training Epoch: 20 [23680/49669]\tLoss: 399.9039\n",
      "Training Epoch: 20 [23744/49669]\tLoss: 436.4122\n",
      "Training Epoch: 20 [23808/49669]\tLoss: 404.1656\n",
      "Training Epoch: 20 [23872/49669]\tLoss: 425.5776\n",
      "Training Epoch: 20 [23936/49669]\tLoss: 404.1873\n",
      "Training Epoch: 20 [24000/49669]\tLoss: 420.7702\n",
      "Training Epoch: 20 [24064/49669]\tLoss: 446.4331\n",
      "Training Epoch: 20 [24128/49669]\tLoss: 439.3328\n",
      "Training Epoch: 20 [24192/49669]\tLoss: 382.3002\n",
      "Training Epoch: 20 [24256/49669]\tLoss: 392.8555\n",
      "Training Epoch: 20 [24320/49669]\tLoss: 427.5573\n",
      "Training Epoch: 20 [24384/49669]\tLoss: 402.9234\n",
      "Training Epoch: 20 [24448/49669]\tLoss: 404.0510\n",
      "Training Epoch: 20 [24512/49669]\tLoss: 433.7087\n",
      "Training Epoch: 20 [24576/49669]\tLoss: 405.1596\n",
      "Training Epoch: 20 [24640/49669]\tLoss: 404.8240\n",
      "Training Epoch: 20 [24704/49669]\tLoss: 389.7408\n",
      "Training Epoch: 20 [24768/49669]\tLoss: 385.4239\n",
      "Training Epoch: 20 [24832/49669]\tLoss: 406.2000\n",
      "Training Epoch: 20 [24896/49669]\tLoss: 391.7764\n",
      "Training Epoch: 20 [24960/49669]\tLoss: 391.0535\n",
      "Training Epoch: 20 [25024/49669]\tLoss: 407.3657\n",
      "Training Epoch: 20 [25088/49669]\tLoss: 418.8641\n",
      "Training Epoch: 20 [25152/49669]\tLoss: 400.9333\n",
      "Training Epoch: 20 [25216/49669]\tLoss: 420.5775\n",
      "Training Epoch: 20 [25280/49669]\tLoss: 391.8541\n",
      "Training Epoch: 20 [25344/49669]\tLoss: 412.1272\n",
      "Training Epoch: 20 [25408/49669]\tLoss: 404.9105\n",
      "Training Epoch: 20 [25472/49669]\tLoss: 415.3215\n",
      "Training Epoch: 20 [25536/49669]\tLoss: 399.7711\n",
      "Training Epoch: 20 [25600/49669]\tLoss: 404.7304\n",
      "Training Epoch: 20 [25664/49669]\tLoss: 400.7481\n",
      "Training Epoch: 20 [25728/49669]\tLoss: 401.1538\n",
      "Training Epoch: 20 [25792/49669]\tLoss: 357.5878\n",
      "Training Epoch: 20 [25856/49669]\tLoss: 392.5641\n",
      "Training Epoch: 20 [25920/49669]\tLoss: 443.2888\n",
      "Training Epoch: 20 [25984/49669]\tLoss: 406.8754\n",
      "Training Epoch: 20 [26048/49669]\tLoss: 396.7032\n",
      "Training Epoch: 20 [26112/49669]\tLoss: 405.8704\n",
      "Training Epoch: 20 [26176/49669]\tLoss: 419.9850\n",
      "Training Epoch: 20 [26240/49669]\tLoss: 412.0150\n",
      "Training Epoch: 20 [26304/49669]\tLoss: 399.3522\n",
      "Training Epoch: 20 [26368/49669]\tLoss: 417.4250\n",
      "Training Epoch: 20 [26432/49669]\tLoss: 378.6197\n",
      "Training Epoch: 20 [26496/49669]\tLoss: 407.0688\n",
      "Training Epoch: 20 [26560/49669]\tLoss: 420.8150\n",
      "Training Epoch: 20 [26624/49669]\tLoss: 392.1043\n",
      "Training Epoch: 20 [26688/49669]\tLoss: 400.4836\n",
      "Training Epoch: 20 [26752/49669]\tLoss: 437.2202\n",
      "Training Epoch: 20 [26816/49669]\tLoss: 389.2907\n",
      "Training Epoch: 20 [26880/49669]\tLoss: 424.1098\n",
      "Training Epoch: 20 [26944/49669]\tLoss: 390.0133\n",
      "Training Epoch: 20 [27008/49669]\tLoss: 398.7764\n",
      "Training Epoch: 20 [27072/49669]\tLoss: 400.9330\n",
      "Training Epoch: 20 [27136/49669]\tLoss: 403.5070\n",
      "Training Epoch: 20 [27200/49669]\tLoss: 375.8097\n",
      "Training Epoch: 20 [27264/49669]\tLoss: 417.5965\n",
      "Training Epoch: 20 [27328/49669]\tLoss: 408.5118\n",
      "Training Epoch: 20 [27392/49669]\tLoss: 389.8413\n",
      "Training Epoch: 20 [27456/49669]\tLoss: 401.4702\n",
      "Training Epoch: 20 [27520/49669]\tLoss: 411.2238\n",
      "Training Epoch: 20 [27584/49669]\tLoss: 412.9714\n",
      "Training Epoch: 20 [27648/49669]\tLoss: 391.2195\n",
      "Training Epoch: 20 [27712/49669]\tLoss: 395.4857\n",
      "Training Epoch: 20 [27776/49669]\tLoss: 378.2885\n",
      "Training Epoch: 20 [27840/49669]\tLoss: 417.2895\n",
      "Training Epoch: 20 [27904/49669]\tLoss: 412.7399\n",
      "Training Epoch: 20 [27968/49669]\tLoss: 401.8466\n",
      "Training Epoch: 20 [28032/49669]\tLoss: 404.4017\n",
      "Training Epoch: 20 [28096/49669]\tLoss: 388.7444\n",
      "Training Epoch: 20 [28160/49669]\tLoss: 391.9263\n",
      "Training Epoch: 20 [28224/49669]\tLoss: 396.2657\n",
      "Training Epoch: 20 [28288/49669]\tLoss: 411.2493\n",
      "Training Epoch: 20 [28352/49669]\tLoss: 409.9309\n",
      "Training Epoch: 20 [28416/49669]\tLoss: 393.2573\n",
      "Training Epoch: 20 [28480/49669]\tLoss: 403.8700\n",
      "Training Epoch: 20 [28544/49669]\tLoss: 429.7513\n",
      "Training Epoch: 20 [28608/49669]\tLoss: 409.3757\n",
      "Training Epoch: 20 [28672/49669]\tLoss: 404.5940\n",
      "Training Epoch: 20 [28736/49669]\tLoss: 420.9302\n",
      "Training Epoch: 20 [28800/49669]\tLoss: 394.6765\n",
      "Training Epoch: 20 [28864/49669]\tLoss: 369.7271\n",
      "Training Epoch: 20 [28928/49669]\tLoss: 362.6173\n",
      "Training Epoch: 20 [28992/49669]\tLoss: 421.6561\n",
      "Training Epoch: 20 [29056/49669]\tLoss: 426.9295\n",
      "Training Epoch: 20 [29120/49669]\tLoss: 420.9943\n",
      "Training Epoch: 20 [29184/49669]\tLoss: 408.7689\n",
      "Training Epoch: 20 [29248/49669]\tLoss: 382.6489\n",
      "Training Epoch: 20 [29312/49669]\tLoss: 378.8820\n",
      "Training Epoch: 20 [29376/49669]\tLoss: 378.3983\n",
      "Training Epoch: 20 [29440/49669]\tLoss: 416.1100\n",
      "Training Epoch: 20 [29504/49669]\tLoss: 437.7882\n",
      "Training Epoch: 20 [29568/49669]\tLoss: 392.0932\n",
      "Training Epoch: 20 [29632/49669]\tLoss: 421.7587\n",
      "Training Epoch: 20 [29696/49669]\tLoss: 407.2135\n",
      "Training Epoch: 20 [29760/49669]\tLoss: 412.3981\n",
      "Training Epoch: 20 [29824/49669]\tLoss: 410.8737\n",
      "Training Epoch: 20 [29888/49669]\tLoss: 416.8279\n",
      "Training Epoch: 20 [29952/49669]\tLoss: 359.6999\n",
      "Training Epoch: 20 [30016/49669]\tLoss: 365.6972\n",
      "Training Epoch: 20 [30080/49669]\tLoss: 404.7250\n",
      "Training Epoch: 20 [30144/49669]\tLoss: 427.9252\n",
      "Training Epoch: 20 [30208/49669]\tLoss: 413.6433\n",
      "Training Epoch: 20 [30272/49669]\tLoss: 402.3814\n",
      "Training Epoch: 20 [30336/49669]\tLoss: 390.8997\n",
      "Training Epoch: 20 [30400/49669]\tLoss: 399.5834\n",
      "Training Epoch: 20 [30464/49669]\tLoss: 415.2852\n",
      "Training Epoch: 20 [30528/49669]\tLoss: 417.0841\n",
      "Training Epoch: 20 [30592/49669]\tLoss: 426.7314\n",
      "Training Epoch: 20 [30656/49669]\tLoss: 407.9012\n",
      "Training Epoch: 20 [30720/49669]\tLoss: 430.9847\n",
      "Training Epoch: 20 [30784/49669]\tLoss: 445.8293\n",
      "Training Epoch: 20 [30848/49669]\tLoss: 394.1423\n",
      "Training Epoch: 20 [30912/49669]\tLoss: 403.5986\n",
      "Training Epoch: 20 [30976/49669]\tLoss: 390.2373\n",
      "Training Epoch: 20 [31040/49669]\tLoss: 418.7041\n",
      "Training Epoch: 20 [31104/49669]\tLoss: 423.3762\n",
      "Training Epoch: 20 [31168/49669]\tLoss: 452.1207\n",
      "Training Epoch: 20 [31232/49669]\tLoss: 418.5679\n",
      "Training Epoch: 20 [31296/49669]\tLoss: 373.2710\n",
      "Training Epoch: 20 [31360/49669]\tLoss: 450.9771\n",
      "Training Epoch: 20 [31424/49669]\tLoss: 402.6067\n",
      "Training Epoch: 20 [31488/49669]\tLoss: 368.6826\n",
      "Training Epoch: 20 [31552/49669]\tLoss: 410.9407\n",
      "Training Epoch: 20 [31616/49669]\tLoss: 399.9619\n",
      "Training Epoch: 20 [31680/49669]\tLoss: 381.0757\n",
      "Training Epoch: 20 [31744/49669]\tLoss: 395.4604\n",
      "Training Epoch: 20 [31808/49669]\tLoss: 398.0965\n",
      "Training Epoch: 20 [31872/49669]\tLoss: 428.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [31936/49669]\tLoss: 407.1974\n",
      "Training Epoch: 20 [32000/49669]\tLoss: 398.3169\n",
      "Training Epoch: 20 [32064/49669]\tLoss: 395.4168\n",
      "Training Epoch: 20 [32128/49669]\tLoss: 403.4706\n",
      "Training Epoch: 20 [32192/49669]\tLoss: 387.1937\n",
      "Training Epoch: 20 [32256/49669]\tLoss: 423.7703\n",
      "Training Epoch: 20 [32320/49669]\tLoss: 406.6884\n",
      "Training Epoch: 20 [32384/49669]\tLoss: 391.2006\n",
      "Training Epoch: 20 [32448/49669]\tLoss: 386.4939\n",
      "Training Epoch: 20 [32512/49669]\tLoss: 395.8063\n",
      "Training Epoch: 20 [32576/49669]\tLoss: 403.7655\n",
      "Training Epoch: 20 [32640/49669]\tLoss: 393.2521\n",
      "Training Epoch: 20 [32704/49669]\tLoss: 402.9721\n",
      "Training Epoch: 20 [32768/49669]\tLoss: 367.7534\n",
      "Training Epoch: 20 [32832/49669]\tLoss: 389.4235\n",
      "Training Epoch: 20 [32896/49669]\tLoss: 403.3523\n",
      "Training Epoch: 20 [32960/49669]\tLoss: 406.3468\n",
      "Training Epoch: 20 [33024/49669]\tLoss: 413.0974\n",
      "Training Epoch: 20 [33088/49669]\tLoss: 428.1288\n",
      "Training Epoch: 20 [33152/49669]\tLoss: 408.1771\n",
      "Training Epoch: 20 [33216/49669]\tLoss: 438.1697\n",
      "Training Epoch: 20 [33280/49669]\tLoss: 387.6330\n",
      "Training Epoch: 20 [33344/49669]\tLoss: 408.0619\n",
      "Training Epoch: 20 [33408/49669]\tLoss: 439.5049\n",
      "Training Epoch: 20 [33472/49669]\tLoss: 410.2406\n",
      "Training Epoch: 20 [33536/49669]\tLoss: 429.0572\n",
      "Training Epoch: 20 [33600/49669]\tLoss: 439.9569\n",
      "Training Epoch: 20 [33664/49669]\tLoss: 448.7964\n",
      "Training Epoch: 20 [33728/49669]\tLoss: 458.7538\n",
      "Training Epoch: 20 [33792/49669]\tLoss: 456.2477\n",
      "Training Epoch: 20 [33856/49669]\tLoss: 514.1129\n",
      "Training Epoch: 20 [33920/49669]\tLoss: 480.3307\n",
      "Training Epoch: 20 [33984/49669]\tLoss: 458.2344\n",
      "Training Epoch: 20 [34048/49669]\tLoss: 425.9739\n",
      "Training Epoch: 20 [34112/49669]\tLoss: 375.6655\n",
      "Training Epoch: 20 [34176/49669]\tLoss: 443.1963\n",
      "Training Epoch: 20 [34240/49669]\tLoss: 411.8594\n",
      "Training Epoch: 20 [34304/49669]\tLoss: 427.5183\n",
      "Training Epoch: 20 [34368/49669]\tLoss: 426.6534\n",
      "Training Epoch: 20 [34432/49669]\tLoss: 400.9894\n",
      "Training Epoch: 20 [34496/49669]\tLoss: 445.1425\n",
      "Training Epoch: 20 [34560/49669]\tLoss: 390.2674\n",
      "Training Epoch: 20 [34624/49669]\tLoss: 426.7449\n",
      "Training Epoch: 20 [34688/49669]\tLoss: 385.5313\n",
      "Training Epoch: 20 [34752/49669]\tLoss: 430.6650\n",
      "Training Epoch: 20 [34816/49669]\tLoss: 410.3019\n",
      "Training Epoch: 20 [34880/49669]\tLoss: 388.1851\n",
      "Training Epoch: 20 [34944/49669]\tLoss: 397.6025\n",
      "Training Epoch: 20 [35008/49669]\tLoss: 421.1674\n",
      "Training Epoch: 20 [35072/49669]\tLoss: 415.1596\n",
      "Training Epoch: 20 [35136/49669]\tLoss: 383.8019\n",
      "Training Epoch: 20 [35200/49669]\tLoss: 402.6558\n",
      "Training Epoch: 20 [35264/49669]\tLoss: 400.6291\n",
      "Training Epoch: 20 [35328/49669]\tLoss: 398.8449\n",
      "Training Epoch: 20 [35392/49669]\tLoss: 393.2040\n",
      "Training Epoch: 20 [35456/49669]\tLoss: 419.3232\n",
      "Training Epoch: 20 [35520/49669]\tLoss: 425.8333\n",
      "Training Epoch: 20 [35584/49669]\tLoss: 374.7489\n",
      "Training Epoch: 20 [35648/49669]\tLoss: 428.0146\n",
      "Training Epoch: 20 [35712/49669]\tLoss: 455.5242\n",
      "Training Epoch: 20 [35776/49669]\tLoss: 407.8254\n",
      "Training Epoch: 20 [35840/49669]\tLoss: 436.4019\n",
      "Training Epoch: 20 [35904/49669]\tLoss: 401.1128\n",
      "Training Epoch: 20 [35968/49669]\tLoss: 391.6013\n",
      "Training Epoch: 20 [36032/49669]\tLoss: 419.0283\n",
      "Training Epoch: 20 [36096/49669]\tLoss: 386.7097\n",
      "Training Epoch: 20 [36160/49669]\tLoss: 392.4790\n",
      "Training Epoch: 20 [36224/49669]\tLoss: 425.5190\n",
      "Training Epoch: 20 [36288/49669]\tLoss: 397.8518\n",
      "Training Epoch: 20 [36352/49669]\tLoss: 419.5494\n",
      "Training Epoch: 20 [36416/49669]\tLoss: 399.1160\n",
      "Training Epoch: 20 [36480/49669]\tLoss: 419.9702\n",
      "Training Epoch: 20 [36544/49669]\tLoss: 421.7528\n",
      "Training Epoch: 20 [36608/49669]\tLoss: 385.4507\n",
      "Training Epoch: 20 [36672/49669]\tLoss: 401.8778\n",
      "Training Epoch: 20 [36736/49669]\tLoss: 400.6894\n",
      "Training Epoch: 20 [36800/49669]\tLoss: 402.7860\n",
      "Training Epoch: 20 [36864/49669]\tLoss: 426.5980\n",
      "Training Epoch: 20 [36928/49669]\tLoss: 417.3604\n",
      "Training Epoch: 20 [36992/49669]\tLoss: 401.0792\n",
      "Training Epoch: 20 [37056/49669]\tLoss: 393.9806\n",
      "Training Epoch: 20 [37120/49669]\tLoss: 409.6936\n",
      "Training Epoch: 20 [37184/49669]\tLoss: 397.3184\n",
      "Training Epoch: 20 [37248/49669]\tLoss: 415.2654\n",
      "Training Epoch: 20 [37312/49669]\tLoss: 421.0016\n",
      "Training Epoch: 20 [37376/49669]\tLoss: 386.0120\n",
      "Training Epoch: 20 [37440/49669]\tLoss: 399.8218\n",
      "Training Epoch: 20 [37504/49669]\tLoss: 424.4383\n",
      "Training Epoch: 20 [37568/49669]\tLoss: 412.0313\n",
      "Training Epoch: 20 [37632/49669]\tLoss: 407.3805\n",
      "Training Epoch: 20 [37696/49669]\tLoss: 429.6197\n",
      "Training Epoch: 20 [37760/49669]\tLoss: 396.3680\n",
      "Training Epoch: 20 [37824/49669]\tLoss: 393.1935\n",
      "Training Epoch: 20 [37888/49669]\tLoss: 391.1973\n",
      "Training Epoch: 20 [37952/49669]\tLoss: 395.8732\n",
      "Training Epoch: 20 [38016/49669]\tLoss: 382.7169\n",
      "Training Epoch: 20 [38080/49669]\tLoss: 397.4924\n",
      "Training Epoch: 20 [38144/49669]\tLoss: 413.9427\n",
      "Training Epoch: 20 [38208/49669]\tLoss: 378.6175\n",
      "Training Epoch: 20 [38272/49669]\tLoss: 396.5879\n",
      "Training Epoch: 20 [38336/49669]\tLoss: 418.8915\n",
      "Training Epoch: 20 [38400/49669]\tLoss: 357.7006\n",
      "Training Epoch: 20 [38464/49669]\tLoss: 410.0163\n",
      "Training Epoch: 20 [38528/49669]\tLoss: 382.9977\n",
      "Training Epoch: 20 [38592/49669]\tLoss: 384.1904\n",
      "Training Epoch: 20 [38656/49669]\tLoss: 400.5873\n",
      "Training Epoch: 20 [38720/49669]\tLoss: 385.8268\n",
      "Training Epoch: 20 [38784/49669]\tLoss: 405.5746\n",
      "Training Epoch: 20 [38848/49669]\tLoss: 412.0142\n",
      "Training Epoch: 20 [38912/49669]\tLoss: 364.6075\n",
      "Training Epoch: 20 [38976/49669]\tLoss: 406.0438\n",
      "Training Epoch: 20 [39040/49669]\tLoss: 401.0671\n",
      "Training Epoch: 20 [39104/49669]\tLoss: 398.6411\n",
      "Training Epoch: 20 [39168/49669]\tLoss: 416.0670\n",
      "Training Epoch: 20 [39232/49669]\tLoss: 406.4428\n",
      "Training Epoch: 20 [39296/49669]\tLoss: 411.0960\n",
      "Training Epoch: 20 [39360/49669]\tLoss: 410.0331\n",
      "Training Epoch: 20 [39424/49669]\tLoss: 417.9032\n",
      "Training Epoch: 20 [39488/49669]\tLoss: 386.9103\n",
      "Training Epoch: 20 [39552/49669]\tLoss: 407.9829\n",
      "Training Epoch: 20 [39616/49669]\tLoss: 403.9235\n",
      "Training Epoch: 20 [39680/49669]\tLoss: 391.2190\n",
      "Training Epoch: 20 [39744/49669]\tLoss: 423.5868\n",
      "Training Epoch: 20 [39808/49669]\tLoss: 420.7096\n",
      "Training Epoch: 20 [39872/49669]\tLoss: 405.2831\n",
      "Training Epoch: 20 [39936/49669]\tLoss: 431.2693\n",
      "Training Epoch: 20 [40000/49669]\tLoss: 381.5392\n",
      "Training Epoch: 20 [40064/49669]\tLoss: 375.6562\n",
      "Training Epoch: 20 [40128/49669]\tLoss: 412.5032\n",
      "Training Epoch: 20 [40192/49669]\tLoss: 390.1808\n",
      "Training Epoch: 20 [40256/49669]\tLoss: 418.5286\n",
      "Training Epoch: 20 [40320/49669]\tLoss: 427.7344\n",
      "Training Epoch: 20 [40384/49669]\tLoss: 402.4009\n",
      "Training Epoch: 20 [40448/49669]\tLoss: 398.1521\n",
      "Training Epoch: 20 [40512/49669]\tLoss: 381.3713\n",
      "Training Epoch: 20 [40576/49669]\tLoss: 391.4102\n",
      "Training Epoch: 20 [40640/49669]\tLoss: 398.0306\n",
      "Training Epoch: 20 [40704/49669]\tLoss: 392.5577\n",
      "Training Epoch: 20 [40768/49669]\tLoss: 394.7498\n",
      "Training Epoch: 20 [40832/49669]\tLoss: 419.5381\n",
      "Training Epoch: 20 [40896/49669]\tLoss: 406.0710\n",
      "Training Epoch: 20 [40960/49669]\tLoss: 403.0704\n",
      "Training Epoch: 20 [41024/49669]\tLoss: 399.8468\n",
      "Training Epoch: 20 [41088/49669]\tLoss: 431.2119\n",
      "Training Epoch: 20 [41152/49669]\tLoss: 415.3755\n",
      "Training Epoch: 20 [41216/49669]\tLoss: 411.4911\n",
      "Training Epoch: 20 [41280/49669]\tLoss: 398.5514\n",
      "Training Epoch: 20 [41344/49669]\tLoss: 409.9224\n",
      "Training Epoch: 20 [41408/49669]\tLoss: 394.4346\n",
      "Training Epoch: 20 [41472/49669]\tLoss: 410.7699\n",
      "Training Epoch: 20 [41536/49669]\tLoss: 392.9860\n",
      "Training Epoch: 20 [41600/49669]\tLoss: 421.9619\n",
      "Training Epoch: 20 [41664/49669]\tLoss: 421.0732\n",
      "Training Epoch: 20 [41728/49669]\tLoss: 406.6830\n",
      "Training Epoch: 20 [41792/49669]\tLoss: 437.4452\n",
      "Training Epoch: 20 [41856/49669]\tLoss: 423.5680\n",
      "Training Epoch: 20 [41920/49669]\tLoss: 424.1664\n",
      "Training Epoch: 20 [41984/49669]\tLoss: 401.2570\n",
      "Training Epoch: 20 [42048/49669]\tLoss: 385.3703\n",
      "Training Epoch: 20 [42112/49669]\tLoss: 372.2234\n",
      "Training Epoch: 20 [42176/49669]\tLoss: 403.4326\n",
      "Training Epoch: 20 [42240/49669]\tLoss: 364.6213\n",
      "Training Epoch: 20 [42304/49669]\tLoss: 416.6778\n",
      "Training Epoch: 20 [42368/49669]\tLoss: 394.0887\n",
      "Training Epoch: 20 [42432/49669]\tLoss: 385.8707\n",
      "Training Epoch: 20 [42496/49669]\tLoss: 376.9707\n",
      "Training Epoch: 20 [42560/49669]\tLoss: 386.3859\n",
      "Training Epoch: 20 [42624/49669]\tLoss: 417.9203\n",
      "Training Epoch: 20 [42688/49669]\tLoss: 417.1125\n",
      "Training Epoch: 20 [42752/49669]\tLoss: 392.8431\n",
      "Training Epoch: 20 [42816/49669]\tLoss: 398.4617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [42880/49669]\tLoss: 385.8612\n",
      "Training Epoch: 20 [42944/49669]\tLoss: 412.8257\n",
      "Training Epoch: 20 [43008/49669]\tLoss: 407.3040\n",
      "Training Epoch: 20 [43072/49669]\tLoss: 400.3036\n",
      "Training Epoch: 20 [43136/49669]\tLoss: 415.8303\n",
      "Training Epoch: 20 [43200/49669]\tLoss: 428.5822\n",
      "Training Epoch: 20 [43264/49669]\tLoss: 426.5968\n",
      "Training Epoch: 20 [43328/49669]\tLoss: 391.9979\n",
      "Training Epoch: 20 [43392/49669]\tLoss: 394.0017\n",
      "Training Epoch: 20 [43456/49669]\tLoss: 399.1792\n",
      "Training Epoch: 20 [43520/49669]\tLoss: 405.4604\n",
      "Training Epoch: 20 [43584/49669]\tLoss: 422.0388\n",
      "Training Epoch: 20 [43648/49669]\tLoss: 408.2243\n",
      "Training Epoch: 20 [43712/49669]\tLoss: 409.1195\n",
      "Training Epoch: 20 [43776/49669]\tLoss: 400.6311\n",
      "Training Epoch: 20 [43840/49669]\tLoss: 388.9483\n",
      "Training Epoch: 20 [43904/49669]\tLoss: 418.8949\n",
      "Training Epoch: 20 [43968/49669]\tLoss: 413.9706\n",
      "Training Epoch: 20 [44032/49669]\tLoss: 408.5552\n",
      "Training Epoch: 20 [44096/49669]\tLoss: 396.9686\n",
      "Training Epoch: 20 [44160/49669]\tLoss: 378.1841\n",
      "Training Epoch: 20 [44224/49669]\tLoss: 422.1739\n",
      "Training Epoch: 20 [44288/49669]\tLoss: 415.1759\n",
      "Training Epoch: 20 [44352/49669]\tLoss: 409.7791\n",
      "Training Epoch: 20 [44416/49669]\tLoss: 404.2819\n",
      "Training Epoch: 20 [44480/49669]\tLoss: 404.9827\n",
      "Training Epoch: 20 [44544/49669]\tLoss: 435.8857\n",
      "Training Epoch: 20 [44608/49669]\tLoss: 388.9475\n",
      "Training Epoch: 20 [44672/49669]\tLoss: 410.7864\n",
      "Training Epoch: 20 [44736/49669]\tLoss: 409.7645\n",
      "Training Epoch: 20 [44800/49669]\tLoss: 431.2394\n",
      "Training Epoch: 20 [44864/49669]\tLoss: 398.4885\n",
      "Training Epoch: 20 [44928/49669]\tLoss: 418.4030\n",
      "Training Epoch: 20 [44992/49669]\tLoss: 409.1846\n",
      "Training Epoch: 20 [45056/49669]\tLoss: 417.2379\n",
      "Training Epoch: 20 [45120/49669]\tLoss: 399.0429\n",
      "Training Epoch: 20 [45184/49669]\tLoss: 403.6207\n",
      "Training Epoch: 20 [45248/49669]\tLoss: 401.6313\n",
      "Training Epoch: 20 [45312/49669]\tLoss: 428.5787\n",
      "Training Epoch: 20 [45376/49669]\tLoss: 412.8313\n",
      "Training Epoch: 20 [45440/49669]\tLoss: 407.8696\n",
      "Training Epoch: 20 [45504/49669]\tLoss: 405.8464\n",
      "Training Epoch: 20 [45568/49669]\tLoss: 431.1137\n",
      "Training Epoch: 20 [45632/49669]\tLoss: 423.2492\n",
      "Training Epoch: 20 [45696/49669]\tLoss: 447.0238\n",
      "Training Epoch: 20 [45760/49669]\tLoss: 425.1913\n",
      "Training Epoch: 20 [45824/49669]\tLoss: 491.8939\n",
      "Training Epoch: 20 [45888/49669]\tLoss: 494.2745\n",
      "Training Epoch: 20 [45952/49669]\tLoss: 519.4961\n",
      "Training Epoch: 20 [46016/49669]\tLoss: 539.9901\n",
      "Training Epoch: 20 [46080/49669]\tLoss: 522.9739\n",
      "Training Epoch: 20 [46144/49669]\tLoss: 491.9556\n",
      "Training Epoch: 20 [46208/49669]\tLoss: 443.0373\n",
      "Training Epoch: 20 [46272/49669]\tLoss: 434.6215\n",
      "Training Epoch: 20 [46336/49669]\tLoss: 420.6242\n",
      "Training Epoch: 20 [46400/49669]\tLoss: 437.3314\n",
      "Training Epoch: 20 [46464/49669]\tLoss: 429.9897\n",
      "Training Epoch: 20 [46528/49669]\tLoss: 382.1667\n",
      "Training Epoch: 20 [46592/49669]\tLoss: 380.1098\n",
      "Training Epoch: 20 [46656/49669]\tLoss: 449.5392\n",
      "Training Epoch: 20 [46720/49669]\tLoss: 387.5558\n",
      "Training Epoch: 20 [46784/49669]\tLoss: 386.6444\n",
      "Training Epoch: 20 [46848/49669]\tLoss: 394.1129\n",
      "Training Epoch: 20 [46912/49669]\tLoss: 408.6799\n",
      "Training Epoch: 20 [46976/49669]\tLoss: 370.2769\n",
      "Training Epoch: 20 [47040/49669]\tLoss: 401.1024\n",
      "Training Epoch: 20 [47104/49669]\tLoss: 417.2752\n",
      "Training Epoch: 20 [47168/49669]\tLoss: 439.4264\n",
      "Training Epoch: 20 [47232/49669]\tLoss: 443.0898\n",
      "Training Epoch: 20 [47296/49669]\tLoss: 425.8896\n",
      "Training Epoch: 20 [47360/49669]\tLoss: 394.6485\n",
      "Training Epoch: 20 [47424/49669]\tLoss: 417.5352\n",
      "Training Epoch: 20 [47488/49669]\tLoss: 420.6076\n",
      "Training Epoch: 20 [47552/49669]\tLoss: 409.3903\n",
      "Training Epoch: 20 [47616/49669]\tLoss: 403.9993\n",
      "Training Epoch: 20 [47680/49669]\tLoss: 412.7261\n",
      "Training Epoch: 20 [47744/49669]\tLoss: 410.5242\n",
      "Training Epoch: 20 [47808/49669]\tLoss: 418.1406\n",
      "Training Epoch: 20 [47872/49669]\tLoss: 407.7231\n",
      "Training Epoch: 20 [47936/49669]\tLoss: 376.9228\n",
      "Training Epoch: 20 [48000/49669]\tLoss: 399.6294\n",
      "Training Epoch: 20 [48064/49669]\tLoss: 436.4562\n",
      "Training Epoch: 20 [48128/49669]\tLoss: 415.9237\n",
      "Training Epoch: 20 [48192/49669]\tLoss: 389.2949\n",
      "Training Epoch: 20 [48256/49669]\tLoss: 438.5692\n",
      "Training Epoch: 20 [48320/49669]\tLoss: 372.2096\n",
      "Training Epoch: 20 [48384/49669]\tLoss: 426.3565\n",
      "Training Epoch: 20 [48448/49669]\tLoss: 373.1307\n",
      "Training Epoch: 20 [48512/49669]\tLoss: 390.4849\n",
      "Training Epoch: 20 [48576/49669]\tLoss: 395.0043\n",
      "Training Epoch: 20 [48640/49669]\tLoss: 386.9201\n",
      "Training Epoch: 20 [48704/49669]\tLoss: 407.8636\n",
      "Training Epoch: 20 [48768/49669]\tLoss: 403.3982\n",
      "Training Epoch: 20 [48832/49669]\tLoss: 410.7927\n",
      "Training Epoch: 20 [48896/49669]\tLoss: 444.5779\n",
      "Training Epoch: 20 [48960/49669]\tLoss: 433.0990\n",
      "Training Epoch: 20 [49024/49669]\tLoss: 380.5223\n",
      "Training Epoch: 20 [49088/49669]\tLoss: 386.9954\n",
      "Training Epoch: 20 [49152/49669]\tLoss: 414.4412\n",
      "Training Epoch: 20 [49216/49669]\tLoss: 389.1271\n",
      "Training Epoch: 20 [49280/49669]\tLoss: 410.6077\n",
      "Training Epoch: 20 [49344/49669]\tLoss: 402.8047\n",
      "Training Epoch: 20 [49408/49669]\tLoss: 417.6177\n",
      "Training Epoch: 20 [49472/49669]\tLoss: 407.9016\n",
      "Training Epoch: 20 [49536/49669]\tLoss: 398.9315\n",
      "Training Epoch: 20 [49600/49669]\tLoss: 391.7321\n",
      "Training Epoch: 20 [49664/49669]\tLoss: 390.2971\n",
      "Training Epoch: 20 [49669/49669]\tLoss: 372.9158\n",
      "Training Epoch: 20 [5519/5519]\tLoss: 406.2858\n",
      "Training Epoch: 21 [64/49669]\tLoss: 393.7710\n",
      "Training Epoch: 21 [128/49669]\tLoss: 402.4972\n",
      "Training Epoch: 21 [192/49669]\tLoss: 393.4530\n",
      "Training Epoch: 21 [256/49669]\tLoss: 404.1285\n",
      "Training Epoch: 21 [320/49669]\tLoss: 401.0706\n",
      "Training Epoch: 21 [384/49669]\tLoss: 399.7730\n",
      "Training Epoch: 21 [448/49669]\tLoss: 375.1934\n",
      "Training Epoch: 21 [512/49669]\tLoss: 395.6526\n",
      "Training Epoch: 21 [576/49669]\tLoss: 427.9915\n",
      "Training Epoch: 21 [640/49669]\tLoss: 409.2575\n",
      "Training Epoch: 21 [704/49669]\tLoss: 439.8298\n",
      "Training Epoch: 21 [768/49669]\tLoss: 412.6484\n",
      "Training Epoch: 21 [832/49669]\tLoss: 425.9508\n",
      "Training Epoch: 21 [896/49669]\tLoss: 424.2713\n",
      "Training Epoch: 21 [960/49669]\tLoss: 405.9055\n",
      "Training Epoch: 21 [1024/49669]\tLoss: 405.5610\n",
      "Training Epoch: 21 [1088/49669]\tLoss: 393.7264\n",
      "Training Epoch: 21 [1152/49669]\tLoss: 398.4554\n",
      "Training Epoch: 21 [1216/49669]\tLoss: 400.9613\n",
      "Training Epoch: 21 [1280/49669]\tLoss: 446.6938\n",
      "Training Epoch: 21 [1344/49669]\tLoss: 397.6844\n",
      "Training Epoch: 21 [1408/49669]\tLoss: 421.4734\n",
      "Training Epoch: 21 [1472/49669]\tLoss: 399.4593\n",
      "Training Epoch: 21 [1536/49669]\tLoss: 423.0306\n",
      "Training Epoch: 21 [1600/49669]\tLoss: 409.7531\n",
      "Training Epoch: 21 [1664/49669]\tLoss: 408.5479\n",
      "Training Epoch: 21 [1728/49669]\tLoss: 403.7096\n",
      "Training Epoch: 21 [1792/49669]\tLoss: 400.7270\n",
      "Training Epoch: 21 [1856/49669]\tLoss: 406.2054\n",
      "Training Epoch: 21 [1920/49669]\tLoss: 386.3178\n",
      "Training Epoch: 21 [1984/49669]\tLoss: 394.9989\n",
      "Training Epoch: 21 [2048/49669]\tLoss: 376.5361\n",
      "Training Epoch: 21 [2112/49669]\tLoss: 417.8495\n",
      "Training Epoch: 21 [2176/49669]\tLoss: 431.6535\n",
      "Training Epoch: 21 [2240/49669]\tLoss: 390.4263\n",
      "Training Epoch: 21 [2304/49669]\tLoss: 367.9172\n",
      "Training Epoch: 21 [2368/49669]\tLoss: 399.7877\n",
      "Training Epoch: 21 [2432/49669]\tLoss: 395.6841\n",
      "Training Epoch: 21 [2496/49669]\tLoss: 391.4363\n",
      "Training Epoch: 21 [2560/49669]\tLoss: 403.9189\n",
      "Training Epoch: 21 [2624/49669]\tLoss: 421.5321\n",
      "Training Epoch: 21 [2688/49669]\tLoss: 392.9467\n",
      "Training Epoch: 21 [2752/49669]\tLoss: 408.7717\n",
      "Training Epoch: 21 [2816/49669]\tLoss: 400.7868\n",
      "Training Epoch: 21 [2880/49669]\tLoss: 407.3071\n",
      "Training Epoch: 21 [2944/49669]\tLoss: 377.2166\n",
      "Training Epoch: 21 [3008/49669]\tLoss: 412.4226\n",
      "Training Epoch: 21 [3072/49669]\tLoss: 392.7618\n",
      "Training Epoch: 21 [3136/49669]\tLoss: 385.4244\n",
      "Training Epoch: 21 [3200/49669]\tLoss: 406.5342\n",
      "Training Epoch: 21 [3264/49669]\tLoss: 377.1291\n",
      "Training Epoch: 21 [3328/49669]\tLoss: 396.2487\n",
      "Training Epoch: 21 [3392/49669]\tLoss: 386.9424\n",
      "Training Epoch: 21 [3456/49669]\tLoss: 402.6827\n",
      "Training Epoch: 21 [3520/49669]\tLoss: 398.3147\n",
      "Training Epoch: 21 [3584/49669]\tLoss: 421.1613\n",
      "Training Epoch: 21 [3648/49669]\tLoss: 403.3544\n",
      "Training Epoch: 21 [3712/49669]\tLoss: 400.5083\n",
      "Training Epoch: 21 [3776/49669]\tLoss: 426.3660\n",
      "Training Epoch: 21 [3840/49669]\tLoss: 441.6348\n",
      "Training Epoch: 21 [3904/49669]\tLoss: 403.2890\n",
      "Training Epoch: 21 [3968/49669]\tLoss: 365.7157\n",
      "Training Epoch: 21 [4032/49669]\tLoss: 411.2973\n",
      "Training Epoch: 21 [4096/49669]\tLoss: 405.0810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [4160/49669]\tLoss: 440.0373\n",
      "Training Epoch: 21 [4224/49669]\tLoss: 395.4372\n",
      "Training Epoch: 21 [4288/49669]\tLoss: 423.7017\n",
      "Training Epoch: 21 [4352/49669]\tLoss: 411.4456\n",
      "Training Epoch: 21 [4416/49669]\tLoss: 413.7653\n",
      "Training Epoch: 21 [4480/49669]\tLoss: 426.3425\n",
      "Training Epoch: 21 [4544/49669]\tLoss: 408.7315\n",
      "Training Epoch: 21 [4608/49669]\tLoss: 410.0804\n",
      "Training Epoch: 21 [4672/49669]\tLoss: 411.0554\n",
      "Training Epoch: 21 [4736/49669]\tLoss: 428.0809\n",
      "Training Epoch: 21 [4800/49669]\tLoss: 401.5271\n",
      "Training Epoch: 21 [4864/49669]\tLoss: 417.6923\n",
      "Training Epoch: 21 [4928/49669]\tLoss: 381.3272\n",
      "Training Epoch: 21 [4992/49669]\tLoss: 401.6769\n",
      "Training Epoch: 21 [5056/49669]\tLoss: 388.6332\n",
      "Training Epoch: 21 [5120/49669]\tLoss: 402.3624\n",
      "Training Epoch: 21 [5184/49669]\tLoss: 417.4129\n",
      "Training Epoch: 21 [5248/49669]\tLoss: 392.8228\n",
      "Training Epoch: 21 [5312/49669]\tLoss: 424.8802\n",
      "Training Epoch: 21 [5376/49669]\tLoss: 374.7900\n",
      "Training Epoch: 21 [5440/49669]\tLoss: 390.0011\n",
      "Training Epoch: 21 [5504/49669]\tLoss: 430.9350\n",
      "Training Epoch: 21 [5568/49669]\tLoss: 413.5118\n",
      "Training Epoch: 21 [5632/49669]\tLoss: 408.7563\n",
      "Training Epoch: 21 [5696/49669]\tLoss: 411.6391\n",
      "Training Epoch: 21 [5760/49669]\tLoss: 430.6284\n",
      "Training Epoch: 21 [5824/49669]\tLoss: 428.1315\n",
      "Training Epoch: 21 [5888/49669]\tLoss: 439.2126\n",
      "Training Epoch: 21 [5952/49669]\tLoss: 396.4485\n",
      "Training Epoch: 21 [6016/49669]\tLoss: 428.4939\n",
      "Training Epoch: 21 [6080/49669]\tLoss: 389.7434\n",
      "Training Epoch: 21 [6144/49669]\tLoss: 404.1373\n",
      "Training Epoch: 21 [6208/49669]\tLoss: 377.7457\n",
      "Training Epoch: 21 [6272/49669]\tLoss: 391.7090\n",
      "Training Epoch: 21 [6336/49669]\tLoss: 412.8110\n",
      "Training Epoch: 21 [6400/49669]\tLoss: 390.2771\n",
      "Training Epoch: 21 [6464/49669]\tLoss: 404.0618\n",
      "Training Epoch: 21 [6528/49669]\tLoss: 397.6589\n",
      "Training Epoch: 21 [6592/49669]\tLoss: 417.5007\n",
      "Training Epoch: 21 [6656/49669]\tLoss: 382.2135\n",
      "Training Epoch: 21 [6720/49669]\tLoss: 386.2027\n",
      "Training Epoch: 21 [6784/49669]\tLoss: 384.5827\n",
      "Training Epoch: 21 [6848/49669]\tLoss: 443.9759\n",
      "Training Epoch: 21 [6912/49669]\tLoss: 379.2957\n",
      "Training Epoch: 21 [6976/49669]\tLoss: 412.1170\n",
      "Training Epoch: 21 [7040/49669]\tLoss: 425.3295\n",
      "Training Epoch: 21 [7104/49669]\tLoss: 409.4062\n",
      "Training Epoch: 21 [7168/49669]\tLoss: 396.7097\n",
      "Training Epoch: 21 [7232/49669]\tLoss: 424.9648\n",
      "Training Epoch: 21 [7296/49669]\tLoss: 392.3224\n",
      "Training Epoch: 21 [7360/49669]\tLoss: 404.7697\n",
      "Training Epoch: 21 [7424/49669]\tLoss: 396.7413\n",
      "Training Epoch: 21 [7488/49669]\tLoss: 350.3873\n",
      "Training Epoch: 21 [7552/49669]\tLoss: 367.8516\n",
      "Training Epoch: 21 [7616/49669]\tLoss: 400.6254\n",
      "Training Epoch: 21 [7680/49669]\tLoss: 428.1402\n",
      "Training Epoch: 21 [7744/49669]\tLoss: 376.4492\n",
      "Training Epoch: 21 [7808/49669]\tLoss: 409.0493\n",
      "Training Epoch: 21 [7872/49669]\tLoss: 417.0312\n",
      "Training Epoch: 21 [7936/49669]\tLoss: 372.5495\n",
      "Training Epoch: 21 [8000/49669]\tLoss: 408.2364\n",
      "Training Epoch: 21 [8064/49669]\tLoss: 412.9377\n",
      "Training Epoch: 21 [8128/49669]\tLoss: 431.9931\n",
      "Training Epoch: 21 [8192/49669]\tLoss: 378.6053\n",
      "Training Epoch: 21 [8256/49669]\tLoss: 387.2670\n",
      "Training Epoch: 21 [8320/49669]\tLoss: 410.4648\n",
      "Training Epoch: 21 [8384/49669]\tLoss: 409.2917\n",
      "Training Epoch: 21 [8448/49669]\tLoss: 408.5215\n",
      "Training Epoch: 21 [8512/49669]\tLoss: 401.3114\n",
      "Training Epoch: 21 [8576/49669]\tLoss: 406.7089\n",
      "Training Epoch: 21 [8640/49669]\tLoss: 395.7673\n",
      "Training Epoch: 21 [8704/49669]\tLoss: 405.4199\n",
      "Training Epoch: 21 [8768/49669]\tLoss: 401.5591\n",
      "Training Epoch: 21 [8832/49669]\tLoss: 394.2718\n",
      "Training Epoch: 21 [8896/49669]\tLoss: 387.3792\n",
      "Training Epoch: 21 [8960/49669]\tLoss: 424.7053\n",
      "Training Epoch: 21 [9024/49669]\tLoss: 399.8979\n",
      "Training Epoch: 21 [9088/49669]\tLoss: 409.0007\n",
      "Training Epoch: 21 [9152/49669]\tLoss: 386.7246\n",
      "Training Epoch: 21 [9216/49669]\tLoss: 396.1441\n",
      "Training Epoch: 21 [9280/49669]\tLoss: 410.0095\n",
      "Training Epoch: 21 [9344/49669]\tLoss: 398.1654\n",
      "Training Epoch: 21 [9408/49669]\tLoss: 394.9497\n",
      "Training Epoch: 21 [9472/49669]\tLoss: 415.8840\n",
      "Training Epoch: 21 [9536/49669]\tLoss: 404.6458\n",
      "Training Epoch: 21 [9600/49669]\tLoss: 409.1330\n",
      "Training Epoch: 21 [9664/49669]\tLoss: 370.5994\n",
      "Training Epoch: 21 [9728/49669]\tLoss: 409.3542\n",
      "Training Epoch: 21 [9792/49669]\tLoss: 394.5854\n",
      "Training Epoch: 21 [9856/49669]\tLoss: 412.2334\n",
      "Training Epoch: 21 [9920/49669]\tLoss: 399.8671\n",
      "Training Epoch: 21 [9984/49669]\tLoss: 402.9511\n",
      "Training Epoch: 21 [10048/49669]\tLoss: 394.1021\n",
      "Training Epoch: 21 [10112/49669]\tLoss: 393.7498\n",
      "Training Epoch: 21 [10176/49669]\tLoss: 409.5517\n",
      "Training Epoch: 21 [10240/49669]\tLoss: 437.9602\n",
      "Training Epoch: 21 [10304/49669]\tLoss: 413.3351\n",
      "Training Epoch: 21 [10368/49669]\tLoss: 400.7188\n",
      "Training Epoch: 21 [10432/49669]\tLoss: 426.6263\n",
      "Training Epoch: 21 [10496/49669]\tLoss: 392.0469\n",
      "Training Epoch: 21 [10560/49669]\tLoss: 407.8192\n",
      "Training Epoch: 21 [10624/49669]\tLoss: 428.3932\n",
      "Training Epoch: 21 [10688/49669]\tLoss: 415.2047\n",
      "Training Epoch: 21 [10752/49669]\tLoss: 448.0686\n",
      "Training Epoch: 21 [10816/49669]\tLoss: 388.7353\n",
      "Training Epoch: 21 [10880/49669]\tLoss: 421.9204\n",
      "Training Epoch: 21 [10944/49669]\tLoss: 417.1251\n",
      "Training Epoch: 21 [11008/49669]\tLoss: 403.3079\n",
      "Training Epoch: 21 [11072/49669]\tLoss: 386.3055\n",
      "Training Epoch: 21 [11136/49669]\tLoss: 412.3346\n",
      "Training Epoch: 21 [11200/49669]\tLoss: 399.3094\n",
      "Training Epoch: 21 [11264/49669]\tLoss: 376.2301\n",
      "Training Epoch: 21 [11328/49669]\tLoss: 399.2567\n",
      "Training Epoch: 21 [11392/49669]\tLoss: 413.0530\n",
      "Training Epoch: 21 [11456/49669]\tLoss: 383.8852\n",
      "Training Epoch: 21 [11520/49669]\tLoss: 415.2529\n",
      "Training Epoch: 21 [11584/49669]\tLoss: 405.0598\n",
      "Training Epoch: 21 [11648/49669]\tLoss: 408.1354\n",
      "Training Epoch: 21 [11712/49669]\tLoss: 412.2035\n",
      "Training Epoch: 21 [11776/49669]\tLoss: 419.3196\n",
      "Training Epoch: 21 [11840/49669]\tLoss: 428.6403\n",
      "Training Epoch: 21 [11904/49669]\tLoss: 402.0075\n",
      "Training Epoch: 21 [11968/49669]\tLoss: 414.5287\n",
      "Training Epoch: 21 [12032/49669]\tLoss: 420.0687\n",
      "Training Epoch: 21 [12096/49669]\tLoss: 386.4677\n",
      "Training Epoch: 21 [12160/49669]\tLoss: 390.8374\n",
      "Training Epoch: 21 [12224/49669]\tLoss: 399.5644\n",
      "Training Epoch: 21 [12288/49669]\tLoss: 376.4657\n",
      "Training Epoch: 21 [12352/49669]\tLoss: 384.8130\n",
      "Training Epoch: 21 [12416/49669]\tLoss: 385.7644\n",
      "Training Epoch: 21 [12480/49669]\tLoss: 423.8714\n",
      "Training Epoch: 21 [12544/49669]\tLoss: 409.7621\n",
      "Training Epoch: 21 [12608/49669]\tLoss: 417.1028\n",
      "Training Epoch: 21 [12672/49669]\tLoss: 386.7048\n",
      "Training Epoch: 21 [12736/49669]\tLoss: 399.1138\n",
      "Training Epoch: 21 [12800/49669]\tLoss: 409.2511\n",
      "Training Epoch: 21 [12864/49669]\tLoss: 376.6637\n",
      "Training Epoch: 21 [12928/49669]\tLoss: 410.1258\n",
      "Training Epoch: 21 [12992/49669]\tLoss: 429.3997\n",
      "Training Epoch: 21 [13056/49669]\tLoss: 401.2782\n",
      "Training Epoch: 21 [13120/49669]\tLoss: 411.7335\n",
      "Training Epoch: 21 [13184/49669]\tLoss: 405.5297\n",
      "Training Epoch: 21 [13248/49669]\tLoss: 419.7920\n",
      "Training Epoch: 21 [13312/49669]\tLoss: 384.1380\n",
      "Training Epoch: 21 [13376/49669]\tLoss: 390.4035\n",
      "Training Epoch: 21 [13440/49669]\tLoss: 389.3091\n",
      "Training Epoch: 21 [13504/49669]\tLoss: 399.6917\n",
      "Training Epoch: 21 [13568/49669]\tLoss: 411.0601\n",
      "Training Epoch: 21 [13632/49669]\tLoss: 433.7528\n",
      "Training Epoch: 21 [13696/49669]\tLoss: 409.3185\n",
      "Training Epoch: 21 [13760/49669]\tLoss: 404.3536\n",
      "Training Epoch: 21 [13824/49669]\tLoss: 388.0778\n",
      "Training Epoch: 21 [13888/49669]\tLoss: 406.0302\n",
      "Training Epoch: 21 [13952/49669]\tLoss: 404.6072\n",
      "Training Epoch: 21 [14016/49669]\tLoss: 411.7359\n",
      "Training Epoch: 21 [14080/49669]\tLoss: 373.3988\n",
      "Training Epoch: 21 [14144/49669]\tLoss: 400.3901\n",
      "Training Epoch: 21 [14208/49669]\tLoss: 402.1443\n",
      "Training Epoch: 21 [14272/49669]\tLoss: 416.5154\n",
      "Training Epoch: 21 [14336/49669]\tLoss: 396.8831\n",
      "Training Epoch: 21 [14400/49669]\tLoss: 419.8172\n",
      "Training Epoch: 21 [14464/49669]\tLoss: 370.2435\n",
      "Training Epoch: 21 [14528/49669]\tLoss: 401.2940\n",
      "Training Epoch: 21 [14592/49669]\tLoss: 424.3770\n",
      "Training Epoch: 21 [14656/49669]\tLoss: 412.2370\n",
      "Training Epoch: 21 [14720/49669]\tLoss: 394.7953\n",
      "Training Epoch: 21 [14784/49669]\tLoss: 396.0211\n",
      "Training Epoch: 21 [14848/49669]\tLoss: 367.8633\n",
      "Training Epoch: 21 [14912/49669]\tLoss: 408.3696\n",
      "Training Epoch: 21 [14976/49669]\tLoss: 404.6740\n",
      "Training Epoch: 21 [15040/49669]\tLoss: 406.2390\n",
      "Training Epoch: 21 [15104/49669]\tLoss: 383.5903\n",
      "Training Epoch: 21 [15168/49669]\tLoss: 434.0181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [15232/49669]\tLoss: 393.8304\n",
      "Training Epoch: 21 [15296/49669]\tLoss: 407.6225\n",
      "Training Epoch: 21 [15360/49669]\tLoss: 399.8745\n",
      "Training Epoch: 21 [15424/49669]\tLoss: 410.0788\n",
      "Training Epoch: 21 [15488/49669]\tLoss: 384.5935\n",
      "Training Epoch: 21 [15552/49669]\tLoss: 414.1847\n",
      "Training Epoch: 21 [15616/49669]\tLoss: 395.7925\n",
      "Training Epoch: 21 [15680/49669]\tLoss: 388.7242\n",
      "Training Epoch: 21 [15744/49669]\tLoss: 422.6257\n",
      "Training Epoch: 21 [15808/49669]\tLoss: 419.1129\n",
      "Training Epoch: 21 [15872/49669]\tLoss: 405.7142\n",
      "Training Epoch: 21 [15936/49669]\tLoss: 405.9585\n",
      "Training Epoch: 21 [16000/49669]\tLoss: 382.3109\n",
      "Training Epoch: 21 [16064/49669]\tLoss: 405.1745\n",
      "Training Epoch: 21 [16128/49669]\tLoss: 378.6537\n",
      "Training Epoch: 21 [16192/49669]\tLoss: 397.7355\n",
      "Training Epoch: 21 [16256/49669]\tLoss: 414.4178\n",
      "Training Epoch: 21 [16320/49669]\tLoss: 413.7616\n",
      "Training Epoch: 21 [16384/49669]\tLoss: 385.2423\n",
      "Training Epoch: 21 [16448/49669]\tLoss: 381.9042\n",
      "Training Epoch: 21 [16512/49669]\tLoss: 428.8368\n",
      "Training Epoch: 21 [16576/49669]\tLoss: 418.3553\n",
      "Training Epoch: 21 [16640/49669]\tLoss: 388.9785\n",
      "Training Epoch: 21 [16704/49669]\tLoss: 402.6460\n",
      "Training Epoch: 21 [16768/49669]\tLoss: 389.7408\n",
      "Training Epoch: 21 [16832/49669]\tLoss: 400.5854\n",
      "Training Epoch: 21 [16896/49669]\tLoss: 402.5000\n",
      "Training Epoch: 21 [16960/49669]\tLoss: 420.3677\n",
      "Training Epoch: 21 [17024/49669]\tLoss: 408.4323\n",
      "Training Epoch: 21 [17088/49669]\tLoss: 381.6817\n",
      "Training Epoch: 21 [17152/49669]\tLoss: 405.6765\n",
      "Training Epoch: 21 [17216/49669]\tLoss: 403.8878\n",
      "Training Epoch: 21 [17280/49669]\tLoss: 384.7372\n",
      "Training Epoch: 21 [17344/49669]\tLoss: 419.0027\n",
      "Training Epoch: 21 [17408/49669]\tLoss: 405.1923\n",
      "Training Epoch: 21 [17472/49669]\tLoss: 398.8871\n",
      "Training Epoch: 21 [17536/49669]\tLoss: 374.0428\n",
      "Training Epoch: 21 [17600/49669]\tLoss: 414.9944\n",
      "Training Epoch: 21 [17664/49669]\tLoss: 399.3942\n",
      "Training Epoch: 21 [17728/49669]\tLoss: 422.8687\n",
      "Training Epoch: 21 [17792/49669]\tLoss: 409.9086\n",
      "Training Epoch: 21 [17856/49669]\tLoss: 426.0702\n",
      "Training Epoch: 21 [17920/49669]\tLoss: 430.2897\n",
      "Training Epoch: 21 [17984/49669]\tLoss: 424.8396\n",
      "Training Epoch: 21 [18048/49669]\tLoss: 428.5521\n",
      "Training Epoch: 21 [18112/49669]\tLoss: 427.7538\n",
      "Training Epoch: 21 [18176/49669]\tLoss: 442.8970\n",
      "Training Epoch: 21 [18240/49669]\tLoss: 458.9724\n",
      "Training Epoch: 21 [18304/49669]\tLoss: 435.7934\n",
      "Training Epoch: 21 [18368/49669]\tLoss: 436.9801\n",
      "Training Epoch: 21 [18432/49669]\tLoss: 428.7946\n",
      "Training Epoch: 21 [18496/49669]\tLoss: 420.9571\n",
      "Training Epoch: 21 [18560/49669]\tLoss: 406.7903\n",
      "Training Epoch: 21 [18624/49669]\tLoss: 434.1410\n",
      "Training Epoch: 21 [18688/49669]\tLoss: 399.2115\n",
      "Training Epoch: 21 [18752/49669]\tLoss: 415.0915\n",
      "Training Epoch: 21 [18816/49669]\tLoss: 419.4324\n",
      "Training Epoch: 21 [18880/49669]\tLoss: 437.5966\n",
      "Training Epoch: 21 [18944/49669]\tLoss: 426.8500\n",
      "Training Epoch: 21 [19008/49669]\tLoss: 415.8061\n",
      "Training Epoch: 21 [19072/49669]\tLoss: 387.8240\n",
      "Training Epoch: 21 [19136/49669]\tLoss: 422.6982\n",
      "Training Epoch: 21 [19200/49669]\tLoss: 408.5938\n",
      "Training Epoch: 21 [19264/49669]\tLoss: 418.3046\n",
      "Training Epoch: 21 [19328/49669]\tLoss: 424.6144\n",
      "Training Epoch: 21 [19392/49669]\tLoss: 430.7629\n",
      "Training Epoch: 21 [19456/49669]\tLoss: 427.8257\n",
      "Training Epoch: 21 [19520/49669]\tLoss: 412.7487\n",
      "Training Epoch: 21 [19584/49669]\tLoss: 387.5106\n",
      "Training Epoch: 21 [19648/49669]\tLoss: 405.4372\n",
      "Training Epoch: 21 [19712/49669]\tLoss: 376.9649\n",
      "Training Epoch: 21 [19776/49669]\tLoss: 418.8591\n",
      "Training Epoch: 21 [19840/49669]\tLoss: 410.2332\n",
      "Training Epoch: 21 [19904/49669]\tLoss: 397.3480\n",
      "Training Epoch: 21 [19968/49669]\tLoss: 424.5526\n",
      "Training Epoch: 21 [20032/49669]\tLoss: 413.9085\n",
      "Training Epoch: 21 [20096/49669]\tLoss: 403.4965\n",
      "Training Epoch: 21 [20160/49669]\tLoss: 408.5265\n",
      "Training Epoch: 21 [20224/49669]\tLoss: 441.8034\n",
      "Training Epoch: 21 [20288/49669]\tLoss: 395.0770\n",
      "Training Epoch: 21 [20352/49669]\tLoss: 398.1589\n",
      "Training Epoch: 21 [20416/49669]\tLoss: 417.1628\n",
      "Training Epoch: 21 [20480/49669]\tLoss: 420.4544\n",
      "Training Epoch: 21 [20544/49669]\tLoss: 413.3935\n",
      "Training Epoch: 21 [20608/49669]\tLoss: 405.5498\n",
      "Training Epoch: 21 [20672/49669]\tLoss: 416.7927\n",
      "Training Epoch: 21 [20736/49669]\tLoss: 428.8947\n",
      "Training Epoch: 21 [20800/49669]\tLoss: 415.2641\n",
      "Training Epoch: 21 [20864/49669]\tLoss: 410.7061\n",
      "Training Epoch: 21 [20928/49669]\tLoss: 401.2436\n",
      "Training Epoch: 21 [20992/49669]\tLoss: 416.8589\n",
      "Training Epoch: 21 [21056/49669]\tLoss: 402.5762\n",
      "Training Epoch: 21 [21120/49669]\tLoss: 367.4455\n",
      "Training Epoch: 21 [21184/49669]\tLoss: 413.9459\n",
      "Training Epoch: 21 [21248/49669]\tLoss: 399.5116\n",
      "Training Epoch: 21 [21312/49669]\tLoss: 412.3287\n",
      "Training Epoch: 21 [21376/49669]\tLoss: 405.1299\n",
      "Training Epoch: 21 [21440/49669]\tLoss: 389.5187\n",
      "Training Epoch: 21 [21504/49669]\tLoss: 414.8536\n",
      "Training Epoch: 21 [21568/49669]\tLoss: 435.7142\n",
      "Training Epoch: 21 [21632/49669]\tLoss: 430.2777\n",
      "Training Epoch: 21 [21696/49669]\tLoss: 405.3069\n",
      "Training Epoch: 21 [21760/49669]\tLoss: 436.3005\n",
      "Training Epoch: 21 [21824/49669]\tLoss: 419.5370\n",
      "Training Epoch: 21 [21888/49669]\tLoss: 395.7072\n",
      "Training Epoch: 21 [21952/49669]\tLoss: 418.3332\n",
      "Training Epoch: 21 [22016/49669]\tLoss: 438.2572\n",
      "Training Epoch: 21 [22080/49669]\tLoss: 449.9969\n",
      "Training Epoch: 21 [22144/49669]\tLoss: 390.3703\n",
      "Training Epoch: 21 [22208/49669]\tLoss: 388.2034\n",
      "Training Epoch: 21 [22272/49669]\tLoss: 414.8315\n",
      "Training Epoch: 21 [22336/49669]\tLoss: 391.7798\n",
      "Training Epoch: 21 [22400/49669]\tLoss: 424.9141\n",
      "Training Epoch: 21 [22464/49669]\tLoss: 421.4841\n",
      "Training Epoch: 21 [22528/49669]\tLoss: 383.9373\n",
      "Training Epoch: 21 [22592/49669]\tLoss: 395.9267\n",
      "Training Epoch: 21 [22656/49669]\tLoss: 406.1232\n",
      "Training Epoch: 21 [22720/49669]\tLoss: 397.2913\n",
      "Training Epoch: 21 [22784/49669]\tLoss: 403.6057\n",
      "Training Epoch: 21 [22848/49669]\tLoss: 401.0964\n",
      "Training Epoch: 21 [22912/49669]\tLoss: 370.6237\n",
      "Training Epoch: 21 [22976/49669]\tLoss: 401.4468\n",
      "Training Epoch: 21 [23040/49669]\tLoss: 410.7477\n",
      "Training Epoch: 21 [23104/49669]\tLoss: 379.7615\n",
      "Training Epoch: 21 [23168/49669]\tLoss: 423.9914\n",
      "Training Epoch: 21 [23232/49669]\tLoss: 399.4166\n",
      "Training Epoch: 21 [23296/49669]\tLoss: 394.2402\n",
      "Training Epoch: 21 [23360/49669]\tLoss: 412.9846\n",
      "Training Epoch: 21 [23424/49669]\tLoss: 404.6132\n",
      "Training Epoch: 21 [23488/49669]\tLoss: 393.7599\n",
      "Training Epoch: 21 [23552/49669]\tLoss: 431.4513\n",
      "Training Epoch: 21 [23616/49669]\tLoss: 391.9312\n",
      "Training Epoch: 21 [23680/49669]\tLoss: 392.2225\n",
      "Training Epoch: 21 [23744/49669]\tLoss: 383.8775\n",
      "Training Epoch: 21 [23808/49669]\tLoss: 383.1695\n",
      "Training Epoch: 21 [23872/49669]\tLoss: 404.1334\n",
      "Training Epoch: 21 [23936/49669]\tLoss: 407.2643\n",
      "Training Epoch: 21 [24000/49669]\tLoss: 412.9207\n",
      "Training Epoch: 21 [24064/49669]\tLoss: 457.1372\n",
      "Training Epoch: 21 [24128/49669]\tLoss: 430.8587\n",
      "Training Epoch: 21 [24192/49669]\tLoss: 403.1375\n",
      "Training Epoch: 21 [24256/49669]\tLoss: 364.4494\n",
      "Training Epoch: 21 [24320/49669]\tLoss: 395.7251\n",
      "Training Epoch: 21 [24384/49669]\tLoss: 389.8425\n",
      "Training Epoch: 21 [24448/49669]\tLoss: 412.9961\n",
      "Training Epoch: 21 [24512/49669]\tLoss: 389.1249\n",
      "Training Epoch: 21 [24576/49669]\tLoss: 397.1639\n",
      "Training Epoch: 21 [24640/49669]\tLoss: 401.9402\n",
      "Training Epoch: 21 [24704/49669]\tLoss: 377.9035\n",
      "Training Epoch: 21 [24768/49669]\tLoss: 398.4531\n",
      "Training Epoch: 21 [24832/49669]\tLoss: 418.9382\n",
      "Training Epoch: 21 [24896/49669]\tLoss: 404.5984\n",
      "Training Epoch: 21 [24960/49669]\tLoss: 381.7901\n",
      "Training Epoch: 21 [25024/49669]\tLoss: 420.7070\n",
      "Training Epoch: 21 [25088/49669]\tLoss: 404.1374\n",
      "Training Epoch: 21 [25152/49669]\tLoss: 377.2054\n",
      "Training Epoch: 21 [25216/49669]\tLoss: 355.2039\n",
      "Training Epoch: 21 [25280/49669]\tLoss: 395.2886\n",
      "Training Epoch: 21 [25344/49669]\tLoss: 400.1901\n",
      "Training Epoch: 21 [25408/49669]\tLoss: 386.2713\n",
      "Training Epoch: 21 [25472/49669]\tLoss: 411.4952\n",
      "Training Epoch: 21 [25536/49669]\tLoss: 410.6969\n",
      "Training Epoch: 21 [25600/49669]\tLoss: 419.7412\n",
      "Training Epoch: 21 [25664/49669]\tLoss: 408.7404\n",
      "Training Epoch: 21 [25728/49669]\tLoss: 396.0044\n",
      "Training Epoch: 21 [25792/49669]\tLoss: 417.8178\n",
      "Training Epoch: 21 [25856/49669]\tLoss: 409.1530\n",
      "Training Epoch: 21 [25920/49669]\tLoss: 431.9375\n",
      "Training Epoch: 21 [25984/49669]\tLoss: 397.0614\n",
      "Training Epoch: 21 [26048/49669]\tLoss: 418.3700\n",
      "Training Epoch: 21 [26112/49669]\tLoss: 388.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [26176/49669]\tLoss: 396.2858\n",
      "Training Epoch: 21 [26240/49669]\tLoss: 417.5031\n",
      "Training Epoch: 21 [26304/49669]\tLoss: 415.7660\n",
      "Training Epoch: 21 [26368/49669]\tLoss: 410.1734\n",
      "Training Epoch: 21 [26432/49669]\tLoss: 425.2083\n",
      "Training Epoch: 21 [26496/49669]\tLoss: 413.3478\n",
      "Training Epoch: 21 [26560/49669]\tLoss: 426.9778\n",
      "Training Epoch: 21 [26624/49669]\tLoss: 455.4121\n",
      "Training Epoch: 21 [26688/49669]\tLoss: 441.8238\n",
      "Training Epoch: 21 [26752/49669]\tLoss: 433.9237\n",
      "Training Epoch: 21 [26816/49669]\tLoss: 489.3013\n",
      "Training Epoch: 21 [26880/49669]\tLoss: 481.4796\n",
      "Training Epoch: 21 [26944/49669]\tLoss: 460.7403\n",
      "Training Epoch: 21 [27008/49669]\tLoss: 473.3499\n",
      "Training Epoch: 21 [27072/49669]\tLoss: 441.8159\n",
      "Training Epoch: 21 [27136/49669]\tLoss: 407.3024\n",
      "Training Epoch: 21 [27200/49669]\tLoss: 434.5534\n",
      "Training Epoch: 21 [27264/49669]\tLoss: 412.7356\n",
      "Training Epoch: 21 [27328/49669]\tLoss: 438.8162\n",
      "Training Epoch: 21 [27392/49669]\tLoss: 381.7833\n",
      "Training Epoch: 21 [27456/49669]\tLoss: 425.9136\n",
      "Training Epoch: 21 [27520/49669]\tLoss: 384.3570\n",
      "Training Epoch: 21 [27584/49669]\tLoss: 409.5985\n",
      "Training Epoch: 21 [27648/49669]\tLoss: 421.7668\n",
      "Training Epoch: 21 [27712/49669]\tLoss: 410.9748\n",
      "Training Epoch: 21 [27776/49669]\tLoss: 399.9023\n",
      "Training Epoch: 21 [27840/49669]\tLoss: 410.8097\n",
      "Training Epoch: 21 [27904/49669]\tLoss: 403.0935\n",
      "Training Epoch: 21 [27968/49669]\tLoss: 447.5413\n",
      "Training Epoch: 21 [28032/49669]\tLoss: 412.8163\n",
      "Training Epoch: 21 [28096/49669]\tLoss: 407.0568\n",
      "Training Epoch: 21 [28160/49669]\tLoss: 424.1762\n",
      "Training Epoch: 21 [28224/49669]\tLoss: 394.0968\n",
      "Training Epoch: 21 [28288/49669]\tLoss: 413.6840\n",
      "Training Epoch: 21 [28352/49669]\tLoss: 396.1833\n",
      "Training Epoch: 21 [28416/49669]\tLoss: 418.9160\n",
      "Training Epoch: 21 [28480/49669]\tLoss: 382.9812\n",
      "Training Epoch: 21 [28544/49669]\tLoss: 417.7497\n",
      "Training Epoch: 21 [28608/49669]\tLoss: 415.1012\n",
      "Training Epoch: 21 [28672/49669]\tLoss: 381.9632\n",
      "Training Epoch: 21 [28736/49669]\tLoss: 395.0610\n",
      "Training Epoch: 21 [28800/49669]\tLoss: 424.1285\n",
      "Training Epoch: 21 [28864/49669]\tLoss: 382.8622\n",
      "Training Epoch: 21 [28928/49669]\tLoss: 401.3248\n",
      "Training Epoch: 21 [28992/49669]\tLoss: 413.4046\n",
      "Training Epoch: 21 [29056/49669]\tLoss: 404.2367\n",
      "Training Epoch: 21 [29120/49669]\tLoss: 358.3333\n",
      "Training Epoch: 21 [29184/49669]\tLoss: 428.6904\n",
      "Training Epoch: 21 [29248/49669]\tLoss: 391.6018\n",
      "Training Epoch: 21 [29312/49669]\tLoss: 395.0030\n",
      "Training Epoch: 21 [29376/49669]\tLoss: 391.1075\n",
      "Training Epoch: 21 [29440/49669]\tLoss: 418.3466\n",
      "Training Epoch: 21 [29504/49669]\tLoss: 394.8385\n",
      "Training Epoch: 21 [29568/49669]\tLoss: 395.7621\n",
      "Training Epoch: 21 [29632/49669]\tLoss: 401.5871\n",
      "Training Epoch: 21 [29696/49669]\tLoss: 411.4501\n",
      "Training Epoch: 21 [29760/49669]\tLoss: 422.1096\n",
      "Training Epoch: 21 [29824/49669]\tLoss: 388.1665\n",
      "Training Epoch: 21 [29888/49669]\tLoss: 383.4399\n",
      "Training Epoch: 21 [29952/49669]\tLoss: 406.6256\n",
      "Training Epoch: 21 [30016/49669]\tLoss: 414.7114\n",
      "Training Epoch: 21 [30080/49669]\tLoss: 399.9664\n",
      "Training Epoch: 21 [30144/49669]\tLoss: 412.9220\n",
      "Training Epoch: 21 [30208/49669]\tLoss: 435.4369\n",
      "Training Epoch: 21 [30272/49669]\tLoss: 390.8516\n",
      "Training Epoch: 21 [30336/49669]\tLoss: 416.7825\n",
      "Training Epoch: 21 [30400/49669]\tLoss: 381.5473\n",
      "Training Epoch: 21 [30464/49669]\tLoss: 418.1672\n",
      "Training Epoch: 21 [30528/49669]\tLoss: 407.9835\n",
      "Training Epoch: 21 [30592/49669]\tLoss: 408.9014\n",
      "Training Epoch: 21 [30656/49669]\tLoss: 401.1872\n",
      "Training Epoch: 21 [30720/49669]\tLoss: 430.7304\n",
      "Training Epoch: 21 [30784/49669]\tLoss: 414.4696\n",
      "Training Epoch: 21 [30848/49669]\tLoss: 427.3007\n",
      "Training Epoch: 21 [30912/49669]\tLoss: 429.6595\n",
      "Training Epoch: 21 [30976/49669]\tLoss: 434.1797\n",
      "Training Epoch: 21 [31040/49669]\tLoss: 418.7254\n",
      "Training Epoch: 21 [31104/49669]\tLoss: 420.6193\n",
      "Training Epoch: 21 [31168/49669]\tLoss: 377.9268\n",
      "Training Epoch: 21 [31232/49669]\tLoss: 408.9678\n",
      "Training Epoch: 21 [31296/49669]\tLoss: 430.1224\n",
      "Training Epoch: 21 [31360/49669]\tLoss: 404.5786\n",
      "Training Epoch: 21 [31424/49669]\tLoss: 393.6655\n",
      "Training Epoch: 21 [31488/49669]\tLoss: 403.3857\n",
      "Training Epoch: 21 [31552/49669]\tLoss: 399.9046\n",
      "Training Epoch: 21 [31616/49669]\tLoss: 378.0138\n",
      "Training Epoch: 21 [31680/49669]\tLoss: 410.4843\n",
      "Training Epoch: 21 [31744/49669]\tLoss: 398.7978\n",
      "Training Epoch: 21 [31808/49669]\tLoss: 422.4580\n",
      "Training Epoch: 21 [31872/49669]\tLoss: 430.8942\n",
      "Training Epoch: 21 [31936/49669]\tLoss: 436.5578\n",
      "Training Epoch: 21 [32000/49669]\tLoss: 392.5773\n",
      "Training Epoch: 21 [32064/49669]\tLoss: 417.7512\n",
      "Training Epoch: 21 [32128/49669]\tLoss: 420.5447\n",
      "Training Epoch: 21 [32192/49669]\tLoss: 400.3371\n",
      "Training Epoch: 21 [32256/49669]\tLoss: 394.9410\n",
      "Training Epoch: 21 [32320/49669]\tLoss: 402.9987\n",
      "Training Epoch: 21 [32384/49669]\tLoss: 416.6285\n",
      "Training Epoch: 21 [32448/49669]\tLoss: 396.6534\n",
      "Training Epoch: 21 [32512/49669]\tLoss: 429.6349\n",
      "Training Epoch: 21 [32576/49669]\tLoss: 400.4118\n",
      "Training Epoch: 21 [32640/49669]\tLoss: 422.8969\n",
      "Training Epoch: 21 [32704/49669]\tLoss: 382.0568\n",
      "Training Epoch: 21 [32768/49669]\tLoss: 345.9704\n",
      "Training Epoch: 21 [32832/49669]\tLoss: 419.6745\n",
      "Training Epoch: 21 [32896/49669]\tLoss: 394.4602\n",
      "Training Epoch: 21 [32960/49669]\tLoss: 427.9205\n",
      "Training Epoch: 21 [33024/49669]\tLoss: 407.0959\n",
      "Training Epoch: 21 [33088/49669]\tLoss: 403.7090\n",
      "Training Epoch: 21 [33152/49669]\tLoss: 384.0678\n",
      "Training Epoch: 21 [33216/49669]\tLoss: 406.4258\n",
      "Training Epoch: 21 [33280/49669]\tLoss: 364.4911\n",
      "Training Epoch: 21 [33344/49669]\tLoss: 391.0527\n",
      "Training Epoch: 21 [33408/49669]\tLoss: 421.3196\n",
      "Training Epoch: 21 [33472/49669]\tLoss: 395.8151\n",
      "Training Epoch: 21 [33536/49669]\tLoss: 427.6229\n",
      "Training Epoch: 21 [33600/49669]\tLoss: 416.3062\n",
      "Training Epoch: 21 [33664/49669]\tLoss: 400.1715\n",
      "Training Epoch: 21 [33728/49669]\tLoss: 414.3469\n",
      "Training Epoch: 21 [33792/49669]\tLoss: 415.3153\n",
      "Training Epoch: 21 [33856/49669]\tLoss: 381.6481\n",
      "Training Epoch: 21 [33920/49669]\tLoss: 414.5680\n",
      "Training Epoch: 21 [33984/49669]\tLoss: 416.4617\n",
      "Training Epoch: 21 [34048/49669]\tLoss: 398.3018\n",
      "Training Epoch: 21 [34112/49669]\tLoss: 391.3946\n",
      "Training Epoch: 21 [34176/49669]\tLoss: 400.3234\n",
      "Training Epoch: 21 [34240/49669]\tLoss: 431.8745\n",
      "Training Epoch: 21 [34304/49669]\tLoss: 408.6297\n",
      "Training Epoch: 21 [34368/49669]\tLoss: 414.6059\n",
      "Training Epoch: 21 [34432/49669]\tLoss: 404.7904\n",
      "Training Epoch: 21 [34496/49669]\tLoss: 383.5504\n",
      "Training Epoch: 21 [34560/49669]\tLoss: 420.0709\n",
      "Training Epoch: 21 [34624/49669]\tLoss: 389.2050\n",
      "Training Epoch: 21 [34688/49669]\tLoss: 393.2957\n",
      "Training Epoch: 21 [34752/49669]\tLoss: 407.9122\n",
      "Training Epoch: 21 [34816/49669]\tLoss: 397.9533\n",
      "Training Epoch: 21 [34880/49669]\tLoss: 404.4227\n",
      "Training Epoch: 21 [34944/49669]\tLoss: 419.8572\n",
      "Training Epoch: 21 [35008/49669]\tLoss: 411.0955\n",
      "Training Epoch: 21 [35072/49669]\tLoss: 417.4527\n",
      "Training Epoch: 21 [35136/49669]\tLoss: 401.0717\n",
      "Training Epoch: 21 [35200/49669]\tLoss: 407.7610\n",
      "Training Epoch: 21 [35264/49669]\tLoss: 354.9717\n",
      "Training Epoch: 21 [35328/49669]\tLoss: 438.2338\n",
      "Training Epoch: 21 [35392/49669]\tLoss: 395.2234\n",
      "Training Epoch: 21 [35456/49669]\tLoss: 439.6477\n",
      "Training Epoch: 21 [35520/49669]\tLoss: 415.6597\n",
      "Training Epoch: 21 [35584/49669]\tLoss: 398.8213\n",
      "Training Epoch: 21 [35648/49669]\tLoss: 404.6315\n",
      "Training Epoch: 21 [35712/49669]\tLoss: 389.3179\n",
      "Training Epoch: 21 [35776/49669]\tLoss: 412.4462\n",
      "Training Epoch: 21 [35840/49669]\tLoss: 400.9953\n",
      "Training Epoch: 21 [35904/49669]\tLoss: 434.2541\n",
      "Training Epoch: 21 [35968/49669]\tLoss: 386.6885\n",
      "Training Epoch: 21 [36032/49669]\tLoss: 397.2397\n",
      "Training Epoch: 21 [36096/49669]\tLoss: 402.5999\n",
      "Training Epoch: 21 [36160/49669]\tLoss: 426.8232\n",
      "Training Epoch: 21 [36224/49669]\tLoss: 415.3940\n",
      "Training Epoch: 21 [36288/49669]\tLoss: 428.4240\n",
      "Training Epoch: 21 [36352/49669]\tLoss: 404.0164\n",
      "Training Epoch: 21 [36416/49669]\tLoss: 394.0926\n",
      "Training Epoch: 21 [36480/49669]\tLoss: 387.2147\n",
      "Training Epoch: 21 [36544/49669]\tLoss: 387.6931\n",
      "Training Epoch: 21 [36608/49669]\tLoss: 414.5171\n",
      "Training Epoch: 21 [36672/49669]\tLoss: 393.6501\n",
      "Training Epoch: 21 [36736/49669]\tLoss: 403.7710\n",
      "Training Epoch: 21 [36800/49669]\tLoss: 414.4280\n",
      "Training Epoch: 21 [36864/49669]\tLoss: 384.7739\n",
      "Training Epoch: 21 [36928/49669]\tLoss: 407.9341\n",
      "Training Epoch: 21 [36992/49669]\tLoss: 407.8521\n",
      "Training Epoch: 21 [37056/49669]\tLoss: 411.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [37120/49669]\tLoss: 411.0656\n",
      "Training Epoch: 21 [37184/49669]\tLoss: 404.7209\n",
      "Training Epoch: 21 [37248/49669]\tLoss: 425.4879\n",
      "Training Epoch: 21 [37312/49669]\tLoss: 402.3511\n",
      "Training Epoch: 21 [37376/49669]\tLoss: 402.2097\n",
      "Training Epoch: 21 [37440/49669]\tLoss: 407.0976\n",
      "Training Epoch: 21 [37504/49669]\tLoss: 436.6809\n",
      "Training Epoch: 21 [37568/49669]\tLoss: 377.9390\n",
      "Training Epoch: 21 [37632/49669]\tLoss: 430.0685\n",
      "Training Epoch: 21 [37696/49669]\tLoss: 394.5527\n",
      "Training Epoch: 21 [37760/49669]\tLoss: 401.8435\n",
      "Training Epoch: 21 [37824/49669]\tLoss: 401.7001\n",
      "Training Epoch: 21 [37888/49669]\tLoss: 406.3790\n",
      "Training Epoch: 21 [37952/49669]\tLoss: 384.6623\n",
      "Training Epoch: 21 [38016/49669]\tLoss: 430.7617\n",
      "Training Epoch: 21 [38080/49669]\tLoss: 414.3351\n",
      "Training Epoch: 21 [38144/49669]\tLoss: 386.6850\n",
      "Training Epoch: 21 [38208/49669]\tLoss: 402.7473\n",
      "Training Epoch: 21 [38272/49669]\tLoss: 381.7413\n",
      "Training Epoch: 21 [38336/49669]\tLoss: 435.5505\n",
      "Training Epoch: 21 [38400/49669]\tLoss: 410.0078\n",
      "Training Epoch: 21 [38464/49669]\tLoss: 422.9361\n",
      "Training Epoch: 21 [38528/49669]\tLoss: 390.9955\n",
      "Training Epoch: 21 [38592/49669]\tLoss: 440.4788\n",
      "Training Epoch: 21 [38656/49669]\tLoss: 413.8957\n",
      "Training Epoch: 21 [38720/49669]\tLoss: 426.7577\n",
      "Training Epoch: 21 [38784/49669]\tLoss: 405.7927\n",
      "Training Epoch: 21 [38848/49669]\tLoss: 405.4909\n",
      "Training Epoch: 21 [38912/49669]\tLoss: 409.2435\n",
      "Training Epoch: 21 [38976/49669]\tLoss: 363.3578\n",
      "Training Epoch: 21 [39040/49669]\tLoss: 394.4636\n",
      "Training Epoch: 21 [39104/49669]\tLoss: 412.5814\n",
      "Training Epoch: 21 [39168/49669]\tLoss: 423.5873\n",
      "Training Epoch: 21 [39232/49669]\tLoss: 396.3002\n",
      "Training Epoch: 21 [39296/49669]\tLoss: 414.6594\n",
      "Training Epoch: 21 [39360/49669]\tLoss: 408.3753\n",
      "Training Epoch: 21 [39424/49669]\tLoss: 400.0077\n",
      "Training Epoch: 21 [39488/49669]\tLoss: 410.1177\n",
      "Training Epoch: 21 [39552/49669]\tLoss: 408.1060\n",
      "Training Epoch: 21 [39616/49669]\tLoss: 383.7919\n",
      "Training Epoch: 21 [39680/49669]\tLoss: 408.8289\n",
      "Training Epoch: 21 [39744/49669]\tLoss: 405.2693\n",
      "Training Epoch: 21 [39808/49669]\tLoss: 415.6506\n",
      "Training Epoch: 21 [39872/49669]\tLoss: 383.5475\n",
      "Training Epoch: 21 [39936/49669]\tLoss: 443.8335\n",
      "Training Epoch: 21 [40000/49669]\tLoss: 394.3237\n",
      "Training Epoch: 21 [40064/49669]\tLoss: 390.1295\n",
      "Training Epoch: 21 [40128/49669]\tLoss: 409.1943\n",
      "Training Epoch: 21 [40192/49669]\tLoss: 386.3923\n",
      "Training Epoch: 21 [40256/49669]\tLoss: 428.6987\n",
      "Training Epoch: 21 [40320/49669]\tLoss: 407.0570\n",
      "Training Epoch: 21 [40384/49669]\tLoss: 371.6146\n",
      "Training Epoch: 21 [40448/49669]\tLoss: 416.0510\n",
      "Training Epoch: 21 [40512/49669]\tLoss: 362.7728\n",
      "Training Epoch: 21 [40576/49669]\tLoss: 455.1609\n",
      "Training Epoch: 21 [40640/49669]\tLoss: 417.5565\n",
      "Training Epoch: 21 [40704/49669]\tLoss: 410.6146\n",
      "Training Epoch: 21 [40768/49669]\tLoss: 416.1232\n",
      "Training Epoch: 21 [40832/49669]\tLoss: 397.1875\n",
      "Training Epoch: 21 [40896/49669]\tLoss: 385.9558\n",
      "Training Epoch: 21 [40960/49669]\tLoss: 425.3127\n",
      "Training Epoch: 21 [41024/49669]\tLoss: 407.0544\n",
      "Training Epoch: 21 [41088/49669]\tLoss: 407.5068\n",
      "Training Epoch: 21 [41152/49669]\tLoss: 412.5028\n",
      "Training Epoch: 21 [41216/49669]\tLoss: 419.1283\n",
      "Training Epoch: 21 [41280/49669]\tLoss: 391.0465\n",
      "Training Epoch: 21 [41344/49669]\tLoss: 441.0983\n",
      "Training Epoch: 21 [41408/49669]\tLoss: 408.0840\n",
      "Training Epoch: 21 [41472/49669]\tLoss: 398.7588\n",
      "Training Epoch: 21 [41536/49669]\tLoss: 421.2640\n",
      "Training Epoch: 21 [41600/49669]\tLoss: 408.4943\n",
      "Training Epoch: 21 [41664/49669]\tLoss: 411.3594\n",
      "Training Epoch: 21 [41728/49669]\tLoss: 414.4814\n",
      "Training Epoch: 21 [41792/49669]\tLoss: 394.4919\n",
      "Training Epoch: 21 [41856/49669]\tLoss: 402.4871\n",
      "Training Epoch: 21 [41920/49669]\tLoss: 397.8188\n",
      "Training Epoch: 21 [41984/49669]\tLoss: 441.1269\n",
      "Training Epoch: 21 [42048/49669]\tLoss: 414.1761\n",
      "Training Epoch: 21 [42112/49669]\tLoss: 404.6444\n",
      "Training Epoch: 21 [42176/49669]\tLoss: 403.3692\n",
      "Training Epoch: 21 [42240/49669]\tLoss: 403.9559\n",
      "Training Epoch: 21 [42304/49669]\tLoss: 413.3159\n",
      "Training Epoch: 21 [42368/49669]\tLoss: 425.1041\n",
      "Training Epoch: 21 [42432/49669]\tLoss: 415.9633\n",
      "Training Epoch: 21 [42496/49669]\tLoss: 434.4106\n",
      "Training Epoch: 21 [42560/49669]\tLoss: 413.7251\n",
      "Training Epoch: 21 [42624/49669]\tLoss: 422.8912\n",
      "Training Epoch: 21 [42688/49669]\tLoss: 435.0452\n",
      "Training Epoch: 21 [42752/49669]\tLoss: 415.8282\n",
      "Training Epoch: 21 [42816/49669]\tLoss: 399.1903\n",
      "Training Epoch: 21 [42880/49669]\tLoss: 427.0245\n",
      "Training Epoch: 21 [42944/49669]\tLoss: 415.1824\n",
      "Training Epoch: 21 [43008/49669]\tLoss: 413.0707\n",
      "Training Epoch: 21 [43072/49669]\tLoss: 429.0970\n",
      "Training Epoch: 21 [43136/49669]\tLoss: 436.3700\n",
      "Training Epoch: 21 [43200/49669]\tLoss: 451.2777\n",
      "Training Epoch: 21 [43264/49669]\tLoss: 452.3983\n",
      "Training Epoch: 21 [43328/49669]\tLoss: 425.4492\n",
      "Training Epoch: 21 [43392/49669]\tLoss: 430.1428\n",
      "Training Epoch: 21 [43456/49669]\tLoss: 429.3073\n",
      "Training Epoch: 21 [43520/49669]\tLoss: 426.0501\n",
      "Training Epoch: 21 [43584/49669]\tLoss: 369.7414\n",
      "Training Epoch: 21 [43648/49669]\tLoss: 412.6437\n",
      "Training Epoch: 21 [43712/49669]\tLoss: 421.7502\n",
      "Training Epoch: 21 [43776/49669]\tLoss: 452.5912\n",
      "Training Epoch: 21 [43840/49669]\tLoss: 407.2688\n",
      "Training Epoch: 21 [43904/49669]\tLoss: 426.3841\n",
      "Training Epoch: 21 [43968/49669]\tLoss: 392.2831\n",
      "Training Epoch: 21 [44032/49669]\tLoss: 404.5316\n",
      "Training Epoch: 21 [44096/49669]\tLoss: 386.9826\n",
      "Training Epoch: 21 [44160/49669]\tLoss: 410.2924\n",
      "Training Epoch: 21 [44224/49669]\tLoss: 417.9820\n",
      "Training Epoch: 21 [44288/49669]\tLoss: 397.5707\n",
      "Training Epoch: 21 [44352/49669]\tLoss: 402.7636\n",
      "Training Epoch: 21 [44416/49669]\tLoss: 373.9274\n",
      "Training Epoch: 21 [44480/49669]\tLoss: 382.6305\n",
      "Training Epoch: 21 [44544/49669]\tLoss: 439.6938\n",
      "Training Epoch: 21 [44608/49669]\tLoss: 431.9384\n",
      "Training Epoch: 21 [44672/49669]\tLoss: 439.3990\n",
      "Training Epoch: 21 [44736/49669]\tLoss: 424.5505\n",
      "Training Epoch: 21 [44800/49669]\tLoss: 415.7463\n",
      "Training Epoch: 21 [44864/49669]\tLoss: 401.5810\n",
      "Training Epoch: 21 [44928/49669]\tLoss: 379.9801\n",
      "Training Epoch: 21 [44992/49669]\tLoss: 409.5705\n",
      "Training Epoch: 21 [45056/49669]\tLoss: 416.5694\n",
      "Training Epoch: 21 [45120/49669]\tLoss: 389.5574\n",
      "Training Epoch: 21 [45184/49669]\tLoss: 395.1598\n",
      "Training Epoch: 21 [45248/49669]\tLoss: 422.5513\n",
      "Training Epoch: 21 [45312/49669]\tLoss: 396.7416\n",
      "Training Epoch: 21 [45376/49669]\tLoss: 435.0004\n",
      "Training Epoch: 21 [45440/49669]\tLoss: 408.7502\n",
      "Training Epoch: 21 [45504/49669]\tLoss: 431.2210\n",
      "Training Epoch: 21 [45568/49669]\tLoss: 414.9876\n",
      "Training Epoch: 21 [45632/49669]\tLoss: 433.1518\n",
      "Training Epoch: 21 [45696/49669]\tLoss: 419.6802\n",
      "Training Epoch: 21 [45760/49669]\tLoss: 416.6968\n",
      "Training Epoch: 21 [45824/49669]\tLoss: 414.5602\n",
      "Training Epoch: 21 [45888/49669]\tLoss: 423.8581\n",
      "Training Epoch: 21 [45952/49669]\tLoss: 388.2028\n",
      "Training Epoch: 21 [46016/49669]\tLoss: 421.3287\n",
      "Training Epoch: 21 [46080/49669]\tLoss: 422.0776\n",
      "Training Epoch: 21 [46144/49669]\tLoss: 418.1917\n",
      "Training Epoch: 21 [46208/49669]\tLoss: 409.4529\n",
      "Training Epoch: 21 [46272/49669]\tLoss: 401.0174\n",
      "Training Epoch: 21 [46336/49669]\tLoss: 429.3248\n",
      "Training Epoch: 21 [46400/49669]\tLoss: 403.3955\n",
      "Training Epoch: 21 [46464/49669]\tLoss: 413.2703\n",
      "Training Epoch: 21 [46528/49669]\tLoss: 396.2784\n",
      "Training Epoch: 21 [46592/49669]\tLoss: 418.6964\n",
      "Training Epoch: 21 [46656/49669]\tLoss: 431.0371\n",
      "Training Epoch: 21 [46720/49669]\tLoss: 370.7931\n",
      "Training Epoch: 21 [46784/49669]\tLoss: 392.1738\n",
      "Training Epoch: 21 [46848/49669]\tLoss: 380.3272\n",
      "Training Epoch: 21 [46912/49669]\tLoss: 412.4602\n",
      "Training Epoch: 21 [46976/49669]\tLoss: 368.3119\n",
      "Training Epoch: 21 [47040/49669]\tLoss: 382.7121\n",
      "Training Epoch: 21 [47104/49669]\tLoss: 435.8300\n",
      "Training Epoch: 21 [47168/49669]\tLoss: 410.3002\n",
      "Training Epoch: 21 [47232/49669]\tLoss: 404.5791\n",
      "Training Epoch: 21 [47296/49669]\tLoss: 416.7319\n",
      "Training Epoch: 21 [47360/49669]\tLoss: 389.3706\n",
      "Training Epoch: 21 [47424/49669]\tLoss: 389.9599\n",
      "Training Epoch: 21 [47488/49669]\tLoss: 416.5368\n",
      "Training Epoch: 21 [47552/49669]\tLoss: 405.8343\n",
      "Training Epoch: 21 [47616/49669]\tLoss: 411.7826\n",
      "Training Epoch: 21 [47680/49669]\tLoss: 419.5154\n",
      "Training Epoch: 21 [47744/49669]\tLoss: 418.1462\n",
      "Training Epoch: 21 [47808/49669]\tLoss: 430.8887\n",
      "Training Epoch: 21 [47872/49669]\tLoss: 415.6728\n",
      "Training Epoch: 21 [47936/49669]\tLoss: 413.3600\n",
      "Training Epoch: 21 [48000/49669]\tLoss: 415.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [48064/49669]\tLoss: 385.8213\n",
      "Training Epoch: 21 [48128/49669]\tLoss: 410.5935\n",
      "Training Epoch: 21 [48192/49669]\tLoss: 409.2817\n",
      "Training Epoch: 21 [48256/49669]\tLoss: 418.4890\n",
      "Training Epoch: 21 [48320/49669]\tLoss: 386.6452\n",
      "Training Epoch: 21 [48384/49669]\tLoss: 392.9964\n",
      "Training Epoch: 21 [48448/49669]\tLoss: 393.3225\n",
      "Training Epoch: 21 [48512/49669]\tLoss: 412.7868\n",
      "Training Epoch: 21 [48576/49669]\tLoss: 410.1856\n",
      "Training Epoch: 21 [48640/49669]\tLoss: 410.1814\n",
      "Training Epoch: 21 [48704/49669]\tLoss: 424.7236\n",
      "Training Epoch: 21 [48768/49669]\tLoss: 420.2755\n",
      "Training Epoch: 21 [48832/49669]\tLoss: 419.9459\n",
      "Training Epoch: 21 [48896/49669]\tLoss: 418.8119\n",
      "Training Epoch: 21 [48960/49669]\tLoss: 395.5974\n",
      "Training Epoch: 21 [49024/49669]\tLoss: 425.1977\n",
      "Training Epoch: 21 [49088/49669]\tLoss: 405.1652\n",
      "Training Epoch: 21 [49152/49669]\tLoss: 397.2660\n",
      "Training Epoch: 21 [49216/49669]\tLoss: 399.4704\n",
      "Training Epoch: 21 [49280/49669]\tLoss: 422.4782\n",
      "Training Epoch: 21 [49344/49669]\tLoss: 398.7014\n",
      "Training Epoch: 21 [49408/49669]\tLoss: 415.0155\n",
      "Training Epoch: 21 [49472/49669]\tLoss: 393.6989\n",
      "Training Epoch: 21 [49536/49669]\tLoss: 420.9709\n",
      "Training Epoch: 21 [49600/49669]\tLoss: 408.3804\n",
      "Training Epoch: 21 [49664/49669]\tLoss: 407.8602\n",
      "Training Epoch: 21 [49669/49669]\tLoss: 414.6802\n",
      "Training Epoch: 21 [5519/5519]\tLoss: 405.7114\n",
      "Training Epoch: 22 [64/49669]\tLoss: 392.6661\n",
      "Training Epoch: 22 [128/49669]\tLoss: 392.6983\n",
      "Training Epoch: 22 [192/49669]\tLoss: 406.2932\n",
      "Training Epoch: 22 [256/49669]\tLoss: 438.8920\n",
      "Training Epoch: 22 [320/49669]\tLoss: 421.2159\n",
      "Training Epoch: 22 [384/49669]\tLoss: 419.8308\n",
      "Training Epoch: 22 [448/49669]\tLoss: 378.1097\n",
      "Training Epoch: 22 [512/49669]\tLoss: 381.8880\n",
      "Training Epoch: 22 [576/49669]\tLoss: 399.8658\n",
      "Training Epoch: 22 [640/49669]\tLoss: 385.9543\n",
      "Training Epoch: 22 [704/49669]\tLoss: 416.4340\n",
      "Training Epoch: 22 [768/49669]\tLoss: 396.7208\n",
      "Training Epoch: 22 [832/49669]\tLoss: 380.7603\n",
      "Training Epoch: 22 [896/49669]\tLoss: 412.1267\n",
      "Training Epoch: 22 [960/49669]\tLoss: 432.2321\n",
      "Training Epoch: 22 [1024/49669]\tLoss: 414.0567\n",
      "Training Epoch: 22 [1088/49669]\tLoss: 413.5470\n",
      "Training Epoch: 22 [1152/49669]\tLoss: 405.9141\n",
      "Training Epoch: 22 [1216/49669]\tLoss: 415.7986\n",
      "Training Epoch: 22 [1280/49669]\tLoss: 399.5425\n",
      "Training Epoch: 22 [1344/49669]\tLoss: 432.1062\n",
      "Training Epoch: 22 [1408/49669]\tLoss: 398.2229\n",
      "Training Epoch: 22 [1472/49669]\tLoss: 384.7494\n",
      "Training Epoch: 22 [1536/49669]\tLoss: 399.5989\n",
      "Training Epoch: 22 [1600/49669]\tLoss: 396.4059\n",
      "Training Epoch: 22 [1664/49669]\tLoss: 391.4020\n",
      "Training Epoch: 22 [1728/49669]\tLoss: 396.2100\n",
      "Training Epoch: 22 [1792/49669]\tLoss: 377.3957\n",
      "Training Epoch: 22 [1856/49669]\tLoss: 384.9120\n",
      "Training Epoch: 22 [1920/49669]\tLoss: 388.2952\n",
      "Training Epoch: 22 [1984/49669]\tLoss: 413.5314\n",
      "Training Epoch: 22 [2048/49669]\tLoss: 376.4542\n",
      "Training Epoch: 22 [2112/49669]\tLoss: 412.2378\n",
      "Training Epoch: 22 [2176/49669]\tLoss: 414.3307\n",
      "Training Epoch: 22 [2240/49669]\tLoss: 400.4426\n",
      "Training Epoch: 22 [2304/49669]\tLoss: 401.1460\n",
      "Training Epoch: 22 [2368/49669]\tLoss: 404.4826\n",
      "Training Epoch: 22 [2432/49669]\tLoss: 398.4860\n",
      "Training Epoch: 22 [2496/49669]\tLoss: 424.9889\n",
      "Training Epoch: 22 [2560/49669]\tLoss: 386.5945\n",
      "Training Epoch: 22 [2624/49669]\tLoss: 398.6064\n",
      "Training Epoch: 22 [2688/49669]\tLoss: 411.7370\n",
      "Training Epoch: 22 [2752/49669]\tLoss: 379.3089\n",
      "Training Epoch: 22 [2816/49669]\tLoss: 397.1915\n",
      "Training Epoch: 22 [2880/49669]\tLoss: 419.6087\n",
      "Training Epoch: 22 [2944/49669]\tLoss: 374.1072\n",
      "Training Epoch: 22 [3008/49669]\tLoss: 376.0718\n",
      "Training Epoch: 22 [3072/49669]\tLoss: 405.3622\n",
      "Training Epoch: 22 [3136/49669]\tLoss: 419.1992\n",
      "Training Epoch: 22 [3200/49669]\tLoss: 388.4059\n",
      "Training Epoch: 22 [3264/49669]\tLoss: 402.6118\n",
      "Training Epoch: 22 [3328/49669]\tLoss: 401.5727\n",
      "Training Epoch: 22 [3392/49669]\tLoss: 404.7285\n",
      "Training Epoch: 22 [3456/49669]\tLoss: 399.7081\n",
      "Training Epoch: 22 [3520/49669]\tLoss: 428.4065\n",
      "Training Epoch: 22 [3584/49669]\tLoss: 424.9065\n",
      "Training Epoch: 22 [3648/49669]\tLoss: 396.4374\n",
      "Training Epoch: 22 [3712/49669]\tLoss: 425.7320\n",
      "Training Epoch: 22 [3776/49669]\tLoss: 413.3954\n",
      "Training Epoch: 22 [3840/49669]\tLoss: 385.6516\n",
      "Training Epoch: 22 [3904/49669]\tLoss: 413.3761\n",
      "Training Epoch: 22 [3968/49669]\tLoss: 416.6682\n",
      "Training Epoch: 22 [4032/49669]\tLoss: 407.7697\n",
      "Training Epoch: 22 [4096/49669]\tLoss: 385.3582\n",
      "Training Epoch: 22 [4160/49669]\tLoss: 375.4837\n",
      "Training Epoch: 22 [4224/49669]\tLoss: 402.8474\n",
      "Training Epoch: 22 [4288/49669]\tLoss: 413.6587\n",
      "Training Epoch: 22 [4352/49669]\tLoss: 404.5786\n",
      "Training Epoch: 22 [4416/49669]\tLoss: 405.5508\n",
      "Training Epoch: 22 [4480/49669]\tLoss: 392.5117\n",
      "Training Epoch: 22 [4544/49669]\tLoss: 418.5160\n",
      "Training Epoch: 22 [4608/49669]\tLoss: 418.8356\n",
      "Training Epoch: 22 [4672/49669]\tLoss: 388.8842\n",
      "Training Epoch: 22 [4736/49669]\tLoss: 413.4271\n",
      "Training Epoch: 22 [4800/49669]\tLoss: 404.6576\n",
      "Training Epoch: 22 [4864/49669]\tLoss: 418.5967\n",
      "Training Epoch: 22 [4928/49669]\tLoss: 410.6359\n",
      "Training Epoch: 22 [4992/49669]\tLoss: 394.6187\n",
      "Training Epoch: 22 [5056/49669]\tLoss: 419.8087\n",
      "Training Epoch: 22 [5120/49669]\tLoss: 398.2212\n",
      "Training Epoch: 22 [5184/49669]\tLoss: 396.3672\n",
      "Training Epoch: 22 [5248/49669]\tLoss: 425.6983\n",
      "Training Epoch: 22 [5312/49669]\tLoss: 433.7737\n",
      "Training Epoch: 22 [5376/49669]\tLoss: 382.9833\n",
      "Training Epoch: 22 [5440/49669]\tLoss: 420.6562\n",
      "Training Epoch: 22 [5504/49669]\tLoss: 414.0947\n",
      "Training Epoch: 22 [5568/49669]\tLoss: 414.2543\n",
      "Training Epoch: 22 [5632/49669]\tLoss: 405.5959\n",
      "Training Epoch: 22 [5696/49669]\tLoss: 420.0184\n",
      "Training Epoch: 22 [5760/49669]\tLoss: 394.2288\n",
      "Training Epoch: 22 [5824/49669]\tLoss: 400.3396\n",
      "Training Epoch: 22 [5888/49669]\tLoss: 414.4066\n",
      "Training Epoch: 22 [5952/49669]\tLoss: 425.9403\n",
      "Training Epoch: 22 [6016/49669]\tLoss: 407.6627\n",
      "Training Epoch: 22 [6080/49669]\tLoss: 346.7689\n",
      "Training Epoch: 22 [6144/49669]\tLoss: 445.1069\n",
      "Training Epoch: 22 [6208/49669]\tLoss: 393.4744\n",
      "Training Epoch: 22 [6272/49669]\tLoss: 368.8367\n",
      "Training Epoch: 22 [6336/49669]\tLoss: 353.8352\n",
      "Training Epoch: 22 [6400/49669]\tLoss: 400.7793\n",
      "Training Epoch: 22 [6464/49669]\tLoss: 399.1311\n",
      "Training Epoch: 22 [6528/49669]\tLoss: 375.4727\n",
      "Training Epoch: 22 [6592/49669]\tLoss: 388.4626\n",
      "Training Epoch: 22 [6656/49669]\tLoss: 422.6633\n",
      "Training Epoch: 22 [6720/49669]\tLoss: 405.4908\n",
      "Training Epoch: 22 [6784/49669]\tLoss: 436.5845\n",
      "Training Epoch: 22 [6848/49669]\tLoss: 406.7247\n",
      "Training Epoch: 22 [6912/49669]\tLoss: 378.5589\n",
      "Training Epoch: 22 [6976/49669]\tLoss: 410.7997\n",
      "Training Epoch: 22 [7040/49669]\tLoss: 423.8408\n",
      "Training Epoch: 22 [7104/49669]\tLoss: 387.1748\n",
      "Training Epoch: 22 [7168/49669]\tLoss: 371.8360\n",
      "Training Epoch: 22 [7232/49669]\tLoss: 415.6574\n",
      "Training Epoch: 22 [7296/49669]\tLoss: 416.3716\n",
      "Training Epoch: 22 [7360/49669]\tLoss: 408.8914\n",
      "Training Epoch: 22 [7424/49669]\tLoss: 404.5332\n",
      "Training Epoch: 22 [7488/49669]\tLoss: 380.1713\n",
      "Training Epoch: 22 [7552/49669]\tLoss: 407.4442\n",
      "Training Epoch: 22 [7616/49669]\tLoss: 399.8082\n",
      "Training Epoch: 22 [7680/49669]\tLoss: 380.4530\n",
      "Training Epoch: 22 [7744/49669]\tLoss: 390.7982\n",
      "Training Epoch: 22 [7808/49669]\tLoss: 389.9955\n",
      "Training Epoch: 22 [7872/49669]\tLoss: 400.3616\n",
      "Training Epoch: 22 [7936/49669]\tLoss: 396.9839\n",
      "Training Epoch: 22 [8000/49669]\tLoss: 411.8878\n",
      "Training Epoch: 22 [8064/49669]\tLoss: 413.6732\n",
      "Training Epoch: 22 [8128/49669]\tLoss: 388.0383\n",
      "Training Epoch: 22 [8192/49669]\tLoss: 429.9717\n",
      "Training Epoch: 22 [8256/49669]\tLoss: 411.2724\n",
      "Training Epoch: 22 [8320/49669]\tLoss: 408.4812\n",
      "Training Epoch: 22 [8384/49669]\tLoss: 403.2111\n",
      "Training Epoch: 22 [8448/49669]\tLoss: 414.9478\n",
      "Training Epoch: 22 [8512/49669]\tLoss: 386.9075\n",
      "Training Epoch: 22 [8576/49669]\tLoss: 398.6347\n",
      "Training Epoch: 22 [8640/49669]\tLoss: 408.1055\n",
      "Training Epoch: 22 [8704/49669]\tLoss: 390.6762\n",
      "Training Epoch: 22 [8768/49669]\tLoss: 394.7181\n",
      "Training Epoch: 22 [8832/49669]\tLoss: 410.9785\n",
      "Training Epoch: 22 [8896/49669]\tLoss: 417.1506\n",
      "Training Epoch: 22 [8960/49669]\tLoss: 400.0165\n",
      "Training Epoch: 22 [9024/49669]\tLoss: 411.6434\n",
      "Training Epoch: 22 [9088/49669]\tLoss: 403.8804\n",
      "Training Epoch: 22 [9152/49669]\tLoss: 412.8911\n",
      "Training Epoch: 22 [9216/49669]\tLoss: 423.9089\n",
      "Training Epoch: 22 [9280/49669]\tLoss: 415.8456\n",
      "Training Epoch: 22 [9344/49669]\tLoss: 408.1771\n",
      "Training Epoch: 22 [9408/49669]\tLoss: 432.9461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [9472/49669]\tLoss: 388.5497\n",
      "Training Epoch: 22 [9536/49669]\tLoss: 418.7178\n",
      "Training Epoch: 22 [9600/49669]\tLoss: 413.5173\n",
      "Training Epoch: 22 [9664/49669]\tLoss: 393.9678\n",
      "Training Epoch: 22 [9728/49669]\tLoss: 406.0610\n",
      "Training Epoch: 22 [9792/49669]\tLoss: 425.2108\n",
      "Training Epoch: 22 [9856/49669]\tLoss: 405.2002\n",
      "Training Epoch: 22 [9920/49669]\tLoss: 417.6071\n",
      "Training Epoch: 22 [9984/49669]\tLoss: 405.9211\n",
      "Training Epoch: 22 [10048/49669]\tLoss: 426.3381\n",
      "Training Epoch: 22 [10112/49669]\tLoss: 391.4573\n",
      "Training Epoch: 22 [10176/49669]\tLoss: 425.3156\n",
      "Training Epoch: 22 [10240/49669]\tLoss: 440.6111\n",
      "Training Epoch: 22 [10304/49669]\tLoss: 420.7263\n",
      "Training Epoch: 22 [10368/49669]\tLoss: 410.3195\n",
      "Training Epoch: 22 [10432/49669]\tLoss: 428.8699\n",
      "Training Epoch: 22 [10496/49669]\tLoss: 427.2302\n",
      "Training Epoch: 22 [10560/49669]\tLoss: 411.3022\n",
      "Training Epoch: 22 [10624/49669]\tLoss: 409.7452\n",
      "Training Epoch: 22 [10688/49669]\tLoss: 385.1868\n",
      "Training Epoch: 22 [10752/49669]\tLoss: 423.8776\n",
      "Training Epoch: 22 [10816/49669]\tLoss: 379.4459\n",
      "Training Epoch: 22 [10880/49669]\tLoss: 412.3549\n",
      "Training Epoch: 22 [10944/49669]\tLoss: 422.6022\n",
      "Training Epoch: 22 [11008/49669]\tLoss: 395.0649\n",
      "Training Epoch: 22 [11072/49669]\tLoss: 399.0789\n",
      "Training Epoch: 22 [11136/49669]\tLoss: 415.0073\n",
      "Training Epoch: 22 [11200/49669]\tLoss: 395.9703\n",
      "Training Epoch: 22 [11264/49669]\tLoss: 409.7900\n",
      "Training Epoch: 22 [11328/49669]\tLoss: 394.4224\n",
      "Training Epoch: 22 [11392/49669]\tLoss: 416.0920\n",
      "Training Epoch: 22 [11456/49669]\tLoss: 400.8241\n",
      "Training Epoch: 22 [11520/49669]\tLoss: 425.5640\n",
      "Training Epoch: 22 [11584/49669]\tLoss: 417.2434\n",
      "Training Epoch: 22 [11648/49669]\tLoss: 370.4133\n",
      "Training Epoch: 22 [11712/49669]\tLoss: 405.1676\n",
      "Training Epoch: 22 [11776/49669]\tLoss: 418.2909\n",
      "Training Epoch: 22 [11840/49669]\tLoss: 370.7296\n",
      "Training Epoch: 22 [11904/49669]\tLoss: 405.8070\n",
      "Training Epoch: 22 [11968/49669]\tLoss: 396.8889\n",
      "Training Epoch: 22 [12032/49669]\tLoss: 412.3151\n",
      "Training Epoch: 22 [12096/49669]\tLoss: 374.4962\n",
      "Training Epoch: 22 [12160/49669]\tLoss: 422.1827\n",
      "Training Epoch: 22 [12224/49669]\tLoss: 407.4082\n",
      "Training Epoch: 22 [12288/49669]\tLoss: 387.0712\n",
      "Training Epoch: 22 [12352/49669]\tLoss: 415.1416\n",
      "Training Epoch: 22 [12416/49669]\tLoss: 415.5135\n",
      "Training Epoch: 22 [12480/49669]\tLoss: 402.3600\n",
      "Training Epoch: 22 [12544/49669]\tLoss: 395.7142\n",
      "Training Epoch: 22 [12608/49669]\tLoss: 398.5233\n",
      "Training Epoch: 22 [12672/49669]\tLoss: 409.4604\n",
      "Training Epoch: 22 [12736/49669]\tLoss: 398.4153\n",
      "Training Epoch: 22 [12800/49669]\tLoss: 415.7091\n",
      "Training Epoch: 22 [12864/49669]\tLoss: 427.1709\n",
      "Training Epoch: 22 [12928/49669]\tLoss: 389.0971\n",
      "Training Epoch: 22 [12992/49669]\tLoss: 418.2253\n",
      "Training Epoch: 22 [13056/49669]\tLoss: 410.0329\n",
      "Training Epoch: 22 [13120/49669]\tLoss: 398.8118\n",
      "Training Epoch: 22 [13184/49669]\tLoss: 435.2984\n",
      "Training Epoch: 22 [13248/49669]\tLoss: 423.6083\n",
      "Training Epoch: 22 [13312/49669]\tLoss: 397.4826\n",
      "Training Epoch: 22 [13376/49669]\tLoss: 429.8783\n",
      "Training Epoch: 22 [13440/49669]\tLoss: 420.7780\n",
      "Training Epoch: 22 [13504/49669]\tLoss: 402.4048\n",
      "Training Epoch: 22 [13568/49669]\tLoss: 406.0183\n",
      "Training Epoch: 22 [13632/49669]\tLoss: 404.0462\n",
      "Training Epoch: 22 [13696/49669]\tLoss: 424.5269\n",
      "Training Epoch: 22 [13760/49669]\tLoss: 434.7793\n",
      "Training Epoch: 22 [13824/49669]\tLoss: 392.4144\n",
      "Training Epoch: 22 [13888/49669]\tLoss: 402.5989\n",
      "Training Epoch: 22 [13952/49669]\tLoss: 407.5818\n",
      "Training Epoch: 22 [14016/49669]\tLoss: 411.5227\n",
      "Training Epoch: 22 [14080/49669]\tLoss: 406.6926\n",
      "Training Epoch: 22 [14144/49669]\tLoss: 378.8283\n",
      "Training Epoch: 22 [14208/49669]\tLoss: 403.6277\n",
      "Training Epoch: 22 [14272/49669]\tLoss: 394.1638\n",
      "Training Epoch: 22 [14336/49669]\tLoss: 415.5887\n",
      "Training Epoch: 22 [14400/49669]\tLoss: 427.8252\n",
      "Training Epoch: 22 [14464/49669]\tLoss: 382.6606\n",
      "Training Epoch: 22 [14528/49669]\tLoss: 380.3792\n",
      "Training Epoch: 22 [14592/49669]\tLoss: 397.6592\n",
      "Training Epoch: 22 [14656/49669]\tLoss: 415.3912\n",
      "Training Epoch: 22 [14720/49669]\tLoss: 398.7148\n",
      "Training Epoch: 22 [14784/49669]\tLoss: 412.1980\n",
      "Training Epoch: 22 [14848/49669]\tLoss: 397.5335\n",
      "Training Epoch: 22 [14912/49669]\tLoss: 394.7460\n",
      "Training Epoch: 22 [14976/49669]\tLoss: 417.2470\n",
      "Training Epoch: 22 [15040/49669]\tLoss: 421.7184\n",
      "Training Epoch: 22 [15104/49669]\tLoss: 372.3098\n",
      "Training Epoch: 22 [15168/49669]\tLoss: 420.2330\n",
      "Training Epoch: 22 [15232/49669]\tLoss: 437.0315\n",
      "Training Epoch: 22 [15296/49669]\tLoss: 395.3155\n",
      "Training Epoch: 22 [15360/49669]\tLoss: 395.4019\n",
      "Training Epoch: 22 [15424/49669]\tLoss: 378.4305\n",
      "Training Epoch: 22 [15488/49669]\tLoss: 433.7778\n",
      "Training Epoch: 22 [15552/49669]\tLoss: 403.3080\n",
      "Training Epoch: 22 [15616/49669]\tLoss: 406.9160\n",
      "Training Epoch: 22 [15680/49669]\tLoss: 390.9946\n",
      "Training Epoch: 22 [15744/49669]\tLoss: 404.8591\n",
      "Training Epoch: 22 [15808/49669]\tLoss: 403.4874\n",
      "Training Epoch: 22 [15872/49669]\tLoss: 395.0321\n",
      "Training Epoch: 22 [15936/49669]\tLoss: 405.7539\n",
      "Training Epoch: 22 [16000/49669]\tLoss: 405.9016\n",
      "Training Epoch: 22 [16064/49669]\tLoss: 399.1708\n",
      "Training Epoch: 22 [16128/49669]\tLoss: 409.2830\n",
      "Training Epoch: 22 [16192/49669]\tLoss: 429.0466\n",
      "Training Epoch: 22 [16256/49669]\tLoss: 432.6718\n",
      "Training Epoch: 22 [16320/49669]\tLoss: 425.8588\n",
      "Training Epoch: 22 [16384/49669]\tLoss: 431.0984\n",
      "Training Epoch: 22 [16448/49669]\tLoss: 419.8185\n",
      "Training Epoch: 22 [16512/49669]\tLoss: 428.9140\n",
      "Training Epoch: 22 [16576/49669]\tLoss: 419.9163\n",
      "Training Epoch: 22 [16640/49669]\tLoss: 414.0197\n",
      "Training Epoch: 22 [16704/49669]\tLoss: 429.9509\n",
      "Training Epoch: 22 [16768/49669]\tLoss: 426.8042\n",
      "Training Epoch: 22 [16832/49669]\tLoss: 387.0999\n",
      "Training Epoch: 22 [16896/49669]\tLoss: 413.7845\n",
      "Training Epoch: 22 [16960/49669]\tLoss: 396.9877\n",
      "Training Epoch: 22 [17024/49669]\tLoss: 390.6995\n",
      "Training Epoch: 22 [17088/49669]\tLoss: 409.2209\n",
      "Training Epoch: 22 [17152/49669]\tLoss: 415.6392\n",
      "Training Epoch: 22 [17216/49669]\tLoss: 444.2131\n",
      "Training Epoch: 22 [17280/49669]\tLoss: 416.5572\n",
      "Training Epoch: 22 [17344/49669]\tLoss: 437.1594\n",
      "Training Epoch: 22 [17408/49669]\tLoss: 447.4330\n",
      "Training Epoch: 22 [17472/49669]\tLoss: 469.9180\n",
      "Training Epoch: 22 [17536/49669]\tLoss: 491.7623\n",
      "Training Epoch: 22 [17600/49669]\tLoss: 515.9288\n",
      "Training Epoch: 22 [17664/49669]\tLoss: 534.1729\n",
      "Training Epoch: 22 [17728/49669]\tLoss: 529.5470\n",
      "Training Epoch: 22 [17792/49669]\tLoss: 462.4684\n",
      "Training Epoch: 22 [17856/49669]\tLoss: 421.7785\n",
      "Training Epoch: 22 [17920/49669]\tLoss: 398.6170\n",
      "Training Epoch: 22 [17984/49669]\tLoss: 443.0532\n",
      "Training Epoch: 22 [18048/49669]\tLoss: 501.6847\n",
      "Training Epoch: 22 [18112/49669]\tLoss: 431.2581\n",
      "Training Epoch: 22 [18176/49669]\tLoss: 410.1605\n",
      "Training Epoch: 22 [18240/49669]\tLoss: 398.4500\n",
      "Training Epoch: 22 [18304/49669]\tLoss: 449.4118\n",
      "Training Epoch: 22 [18368/49669]\tLoss: 463.7175\n",
      "Training Epoch: 22 [18432/49669]\tLoss: 437.8379\n",
      "Training Epoch: 22 [18496/49669]\tLoss: 377.3907\n",
      "Training Epoch: 22 [18560/49669]\tLoss: 422.0000\n",
      "Training Epoch: 22 [18624/49669]\tLoss: 455.4181\n",
      "Training Epoch: 22 [18688/49669]\tLoss: 510.5596\n",
      "Training Epoch: 22 [18752/49669]\tLoss: 449.6537\n",
      "Training Epoch: 22 [18816/49669]\tLoss: 427.0325\n",
      "Training Epoch: 22 [18880/49669]\tLoss: 396.7181\n",
      "Training Epoch: 22 [18944/49669]\tLoss: 450.1467\n",
      "Training Epoch: 22 [19008/49669]\tLoss: 455.4246\n",
      "Training Epoch: 22 [19072/49669]\tLoss: 416.2010\n",
      "Training Epoch: 22 [19136/49669]\tLoss: 401.6151\n",
      "Training Epoch: 22 [19200/49669]\tLoss: 412.7541\n",
      "Training Epoch: 22 [19264/49669]\tLoss: 406.3284\n",
      "Training Epoch: 22 [19328/49669]\tLoss: 431.1615\n",
      "Training Epoch: 22 [19392/49669]\tLoss: 408.0657\n",
      "Training Epoch: 22 [19456/49669]\tLoss: 429.8871\n",
      "Training Epoch: 22 [19520/49669]\tLoss: 413.6032\n",
      "Training Epoch: 22 [19584/49669]\tLoss: 427.6230\n",
      "Training Epoch: 22 [19648/49669]\tLoss: 386.3187\n",
      "Training Epoch: 22 [19712/49669]\tLoss: 424.0724\n",
      "Training Epoch: 22 [19776/49669]\tLoss: 373.8172\n",
      "Training Epoch: 22 [19840/49669]\tLoss: 389.2622\n",
      "Training Epoch: 22 [19904/49669]\tLoss: 410.6850\n",
      "Training Epoch: 22 [19968/49669]\tLoss: 416.9675\n",
      "Training Epoch: 22 [20032/49669]\tLoss: 429.3509\n",
      "Training Epoch: 22 [20096/49669]\tLoss: 427.2647\n",
      "Training Epoch: 22 [20160/49669]\tLoss: 399.1866\n",
      "Training Epoch: 22 [20224/49669]\tLoss: 399.6862\n",
      "Training Epoch: 22 [20288/49669]\tLoss: 387.7450\n",
      "Training Epoch: 22 [20352/49669]\tLoss: 411.2650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [20416/49669]\tLoss: 406.8046\n",
      "Training Epoch: 22 [20480/49669]\tLoss: 393.3553\n",
      "Training Epoch: 22 [20544/49669]\tLoss: 410.0693\n",
      "Training Epoch: 22 [20608/49669]\tLoss: 421.4915\n",
      "Training Epoch: 22 [20672/49669]\tLoss: 407.0386\n",
      "Training Epoch: 22 [20736/49669]\tLoss: 383.7748\n",
      "Training Epoch: 22 [20800/49669]\tLoss: 401.5459\n",
      "Training Epoch: 22 [20864/49669]\tLoss: 408.3691\n",
      "Training Epoch: 22 [20928/49669]\tLoss: 394.1764\n",
      "Training Epoch: 22 [20992/49669]\tLoss: 430.1632\n",
      "Training Epoch: 22 [21056/49669]\tLoss: 417.0990\n",
      "Training Epoch: 22 [21120/49669]\tLoss: 377.6806\n",
      "Training Epoch: 22 [21184/49669]\tLoss: 408.5391\n",
      "Training Epoch: 22 [21248/49669]\tLoss: 433.4922\n",
      "Training Epoch: 22 [21312/49669]\tLoss: 394.8161\n",
      "Training Epoch: 22 [21376/49669]\tLoss: 405.8364\n",
      "Training Epoch: 22 [21440/49669]\tLoss: 423.5197\n",
      "Training Epoch: 22 [21504/49669]\tLoss: 414.5148\n",
      "Training Epoch: 22 [21568/49669]\tLoss: 406.5211\n",
      "Training Epoch: 22 [21632/49669]\tLoss: 383.1831\n",
      "Training Epoch: 22 [21696/49669]\tLoss: 413.1361\n",
      "Training Epoch: 22 [21760/49669]\tLoss: 399.8394\n",
      "Training Epoch: 22 [21824/49669]\tLoss: 419.4579\n",
      "Training Epoch: 22 [21888/49669]\tLoss: 386.3039\n",
      "Training Epoch: 22 [21952/49669]\tLoss: 398.6936\n",
      "Training Epoch: 22 [22016/49669]\tLoss: 392.7074\n",
      "Training Epoch: 22 [22080/49669]\tLoss: 419.7280\n",
      "Training Epoch: 22 [22144/49669]\tLoss: 414.6419\n",
      "Training Epoch: 22 [22208/49669]\tLoss: 394.7081\n",
      "Training Epoch: 22 [22272/49669]\tLoss: 407.5861\n",
      "Training Epoch: 22 [22336/49669]\tLoss: 399.2728\n",
      "Training Epoch: 22 [22400/49669]\tLoss: 384.8941\n",
      "Training Epoch: 22 [22464/49669]\tLoss: 405.7182\n",
      "Training Epoch: 22 [22528/49669]\tLoss: 389.0706\n",
      "Training Epoch: 22 [22592/49669]\tLoss: 407.8700\n",
      "Training Epoch: 22 [22656/49669]\tLoss: 410.1281\n",
      "Training Epoch: 22 [22720/49669]\tLoss: 424.8414\n",
      "Training Epoch: 22 [22784/49669]\tLoss: 424.0328\n",
      "Training Epoch: 22 [22848/49669]\tLoss: 428.1027\n",
      "Training Epoch: 22 [22912/49669]\tLoss: 444.4454\n",
      "Training Epoch: 22 [22976/49669]\tLoss: 403.3325\n",
      "Training Epoch: 22 [23040/49669]\tLoss: 392.4029\n",
      "Training Epoch: 22 [23104/49669]\tLoss: 388.3095\n",
      "Training Epoch: 22 [23168/49669]\tLoss: 402.3334\n",
      "Training Epoch: 22 [23232/49669]\tLoss: 396.5760\n",
      "Training Epoch: 22 [23296/49669]\tLoss: 414.7704\n",
      "Training Epoch: 22 [23360/49669]\tLoss: 402.8764\n",
      "Training Epoch: 22 [23424/49669]\tLoss: 408.4614\n",
      "Training Epoch: 22 [23488/49669]\tLoss: 386.8356\n",
      "Training Epoch: 22 [23552/49669]\tLoss: 403.7415\n",
      "Training Epoch: 22 [23616/49669]\tLoss: 397.6554\n",
      "Training Epoch: 22 [23680/49669]\tLoss: 367.8606\n",
      "Training Epoch: 22 [23744/49669]\tLoss: 394.8229\n",
      "Training Epoch: 22 [23808/49669]\tLoss: 390.6377\n",
      "Training Epoch: 22 [23872/49669]\tLoss: 397.6780\n",
      "Training Epoch: 22 [23936/49669]\tLoss: 416.8458\n",
      "Training Epoch: 22 [24000/49669]\tLoss: 413.2885\n",
      "Training Epoch: 22 [24064/49669]\tLoss: 394.3576\n",
      "Training Epoch: 22 [24128/49669]\tLoss: 416.0524\n",
      "Training Epoch: 22 [24192/49669]\tLoss: 385.0019\n",
      "Training Epoch: 22 [24256/49669]\tLoss: 400.0612\n",
      "Training Epoch: 22 [24320/49669]\tLoss: 401.0316\n",
      "Training Epoch: 22 [24384/49669]\tLoss: 378.6548\n",
      "Training Epoch: 22 [24448/49669]\tLoss: 400.7915\n",
      "Training Epoch: 22 [24512/49669]\tLoss: 406.3343\n",
      "Training Epoch: 22 [24576/49669]\tLoss: 418.1508\n",
      "Training Epoch: 22 [24640/49669]\tLoss: 427.7872\n",
      "Training Epoch: 22 [24704/49669]\tLoss: 435.2010\n",
      "Training Epoch: 22 [24768/49669]\tLoss: 391.3896\n",
      "Training Epoch: 22 [24832/49669]\tLoss: 384.6513\n",
      "Training Epoch: 22 [24896/49669]\tLoss: 406.6003\n",
      "Training Epoch: 22 [24960/49669]\tLoss: 407.7385\n",
      "Training Epoch: 22 [25024/49669]\tLoss: 389.6810\n",
      "Training Epoch: 22 [25088/49669]\tLoss: 408.5196\n",
      "Training Epoch: 22 [25152/49669]\tLoss: 431.8526\n",
      "Training Epoch: 22 [25216/49669]\tLoss: 400.8082\n",
      "Training Epoch: 22 [25280/49669]\tLoss: 409.3313\n",
      "Training Epoch: 22 [25344/49669]\tLoss: 408.2538\n",
      "Training Epoch: 22 [25408/49669]\tLoss: 401.5801\n",
      "Training Epoch: 22 [25472/49669]\tLoss: 419.7800\n",
      "Training Epoch: 22 [25536/49669]\tLoss: 414.0819\n",
      "Training Epoch: 22 [25600/49669]\tLoss: 408.0291\n",
      "Training Epoch: 22 [25664/49669]\tLoss: 384.0059\n",
      "Training Epoch: 22 [25728/49669]\tLoss: 395.1982\n",
      "Training Epoch: 22 [25792/49669]\tLoss: 363.4829\n",
      "Training Epoch: 22 [25856/49669]\tLoss: 406.9260\n",
      "Training Epoch: 22 [25920/49669]\tLoss: 436.4020\n",
      "Training Epoch: 22 [25984/49669]\tLoss: 415.4352\n",
      "Training Epoch: 22 [26048/49669]\tLoss: 417.9324\n",
      "Training Epoch: 22 [26112/49669]\tLoss: 384.4371\n",
      "Training Epoch: 22 [26176/49669]\tLoss: 394.1344\n",
      "Training Epoch: 22 [26240/49669]\tLoss: 396.3557\n",
      "Training Epoch: 22 [26304/49669]\tLoss: 427.0472\n",
      "Training Epoch: 22 [26368/49669]\tLoss: 419.9123\n",
      "Training Epoch: 22 [26432/49669]\tLoss: 390.5405\n",
      "Training Epoch: 22 [26496/49669]\tLoss: 425.4253\n",
      "Training Epoch: 22 [26560/49669]\tLoss: 402.8389\n",
      "Training Epoch: 22 [26624/49669]\tLoss: 403.0076\n",
      "Training Epoch: 22 [26688/49669]\tLoss: 397.9872\n",
      "Training Epoch: 22 [26752/49669]\tLoss: 398.7773\n",
      "Training Epoch: 22 [26816/49669]\tLoss: 428.7589\n",
      "Training Epoch: 22 [26880/49669]\tLoss: 421.3130\n",
      "Training Epoch: 22 [26944/49669]\tLoss: 388.8394\n",
      "Training Epoch: 22 [27008/49669]\tLoss: 380.2854\n",
      "Training Epoch: 22 [27072/49669]\tLoss: 390.8607\n",
      "Training Epoch: 22 [27136/49669]\tLoss: 419.8027\n",
      "Training Epoch: 22 [27200/49669]\tLoss: 433.1218\n",
      "Training Epoch: 22 [27264/49669]\tLoss: 388.3593\n",
      "Training Epoch: 22 [27328/49669]\tLoss: 430.1554\n",
      "Training Epoch: 22 [27392/49669]\tLoss: 398.1776\n",
      "Training Epoch: 22 [27456/49669]\tLoss: 388.3738\n",
      "Training Epoch: 22 [27520/49669]\tLoss: 418.8079\n",
      "Training Epoch: 22 [27584/49669]\tLoss: 368.5616\n",
      "Training Epoch: 22 [27648/49669]\tLoss: 387.2866\n",
      "Training Epoch: 22 [27712/49669]\tLoss: 385.3634\n",
      "Training Epoch: 22 [27776/49669]\tLoss: 384.1856\n",
      "Training Epoch: 22 [27840/49669]\tLoss: 386.7224\n",
      "Training Epoch: 22 [27904/49669]\tLoss: 403.5891\n",
      "Training Epoch: 22 [27968/49669]\tLoss: 419.4649\n",
      "Training Epoch: 22 [28032/49669]\tLoss: 398.4605\n",
      "Training Epoch: 22 [28096/49669]\tLoss: 408.0108\n",
      "Training Epoch: 22 [28160/49669]\tLoss: 446.7526\n",
      "Training Epoch: 22 [28224/49669]\tLoss: 379.7292\n",
      "Training Epoch: 22 [28288/49669]\tLoss: 411.5984\n",
      "Training Epoch: 22 [28352/49669]\tLoss: 378.7564\n",
      "Training Epoch: 22 [28416/49669]\tLoss: 387.5014\n",
      "Training Epoch: 22 [28480/49669]\tLoss: 431.0457\n",
      "Training Epoch: 22 [28544/49669]\tLoss: 403.0895\n",
      "Training Epoch: 22 [28608/49669]\tLoss: 368.7239\n",
      "Training Epoch: 22 [28672/49669]\tLoss: 405.3332\n",
      "Training Epoch: 22 [28736/49669]\tLoss: 422.1451\n",
      "Training Epoch: 22 [28800/49669]\tLoss: 382.8662\n",
      "Training Epoch: 22 [28864/49669]\tLoss: 406.9623\n",
      "Training Epoch: 22 [28928/49669]\tLoss: 392.0782\n",
      "Training Epoch: 22 [28992/49669]\tLoss: 378.7944\n",
      "Training Epoch: 22 [29056/49669]\tLoss: 429.9733\n",
      "Training Epoch: 22 [29120/49669]\tLoss: 396.8616\n",
      "Training Epoch: 22 [29184/49669]\tLoss: 398.5965\n",
      "Training Epoch: 22 [29248/49669]\tLoss: 411.4708\n",
      "Training Epoch: 22 [29312/49669]\tLoss: 378.1914\n",
      "Training Epoch: 22 [29376/49669]\tLoss: 388.6105\n",
      "Training Epoch: 22 [29440/49669]\tLoss: 378.4141\n",
      "Training Epoch: 22 [29504/49669]\tLoss: 414.1927\n",
      "Training Epoch: 22 [29568/49669]\tLoss: 421.6420\n",
      "Training Epoch: 22 [29632/49669]\tLoss: 406.8048\n",
      "Training Epoch: 22 [29696/49669]\tLoss: 418.7247\n",
      "Training Epoch: 22 [29760/49669]\tLoss: 408.5629\n",
      "Training Epoch: 22 [29824/49669]\tLoss: 417.6366\n",
      "Training Epoch: 22 [29888/49669]\tLoss: 426.9924\n",
      "Training Epoch: 22 [29952/49669]\tLoss: 412.1696\n",
      "Training Epoch: 22 [30016/49669]\tLoss: 413.6490\n",
      "Training Epoch: 22 [30080/49669]\tLoss: 425.4693\n",
      "Training Epoch: 22 [30144/49669]\tLoss: 408.0606\n",
      "Training Epoch: 22 [30208/49669]\tLoss: 413.3581\n",
      "Training Epoch: 22 [30272/49669]\tLoss: 385.5108\n",
      "Training Epoch: 22 [30336/49669]\tLoss: 391.5211\n",
      "Training Epoch: 22 [30400/49669]\tLoss: 395.4489\n",
      "Training Epoch: 22 [30464/49669]\tLoss: 404.1859\n",
      "Training Epoch: 22 [30528/49669]\tLoss: 419.6709\n",
      "Training Epoch: 22 [30592/49669]\tLoss: 409.3708\n",
      "Training Epoch: 22 [30656/49669]\tLoss: 374.1970\n",
      "Training Epoch: 22 [30720/49669]\tLoss: 383.0989\n",
      "Training Epoch: 22 [30784/49669]\tLoss: 416.2085\n",
      "Training Epoch: 22 [30848/49669]\tLoss: 403.4993\n",
      "Training Epoch: 22 [30912/49669]\tLoss: 387.9883\n",
      "Training Epoch: 22 [30976/49669]\tLoss: 382.5396\n",
      "Training Epoch: 22 [31040/49669]\tLoss: 389.8224\n",
      "Training Epoch: 22 [31104/49669]\tLoss: 373.6024\n",
      "Training Epoch: 22 [31168/49669]\tLoss: 397.6065\n",
      "Training Epoch: 22 [31232/49669]\tLoss: 415.1123\n",
      "Training Epoch: 22 [31296/49669]\tLoss: 408.8695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [31360/49669]\tLoss: 388.0580\n",
      "Training Epoch: 22 [31424/49669]\tLoss: 403.7107\n",
      "Training Epoch: 22 [31488/49669]\tLoss: 405.1096\n",
      "Training Epoch: 22 [31552/49669]\tLoss: 411.3654\n",
      "Training Epoch: 22 [31616/49669]\tLoss: 403.7931\n",
      "Training Epoch: 22 [31680/49669]\tLoss: 410.9865\n",
      "Training Epoch: 22 [31744/49669]\tLoss: 403.6000\n",
      "Training Epoch: 22 [31808/49669]\tLoss: 394.5229\n",
      "Training Epoch: 22 [31872/49669]\tLoss: 403.5193\n",
      "Training Epoch: 22 [31936/49669]\tLoss: 405.0414\n",
      "Training Epoch: 22 [32000/49669]\tLoss: 396.0485\n",
      "Training Epoch: 22 [32064/49669]\tLoss: 407.4711\n",
      "Training Epoch: 22 [32128/49669]\tLoss: 391.8015\n",
      "Training Epoch: 22 [32192/49669]\tLoss: 382.8642\n",
      "Training Epoch: 22 [32256/49669]\tLoss: 459.2969\n",
      "Training Epoch: 22 [32320/49669]\tLoss: 393.8310\n",
      "Training Epoch: 22 [32384/49669]\tLoss: 425.1154\n",
      "Training Epoch: 22 [32448/49669]\tLoss: 406.3719\n",
      "Training Epoch: 22 [32512/49669]\tLoss: 397.9478\n",
      "Training Epoch: 22 [32576/49669]\tLoss: 400.2587\n",
      "Training Epoch: 22 [32640/49669]\tLoss: 427.7255\n",
      "Training Epoch: 22 [32704/49669]\tLoss: 421.0722\n",
      "Training Epoch: 22 [32768/49669]\tLoss: 424.6253\n",
      "Training Epoch: 22 [32832/49669]\tLoss: 390.8506\n",
      "Training Epoch: 22 [32896/49669]\tLoss: 412.7767\n",
      "Training Epoch: 22 [32960/49669]\tLoss: 400.5362\n",
      "Training Epoch: 22 [33024/49669]\tLoss: 417.2463\n",
      "Training Epoch: 22 [33088/49669]\tLoss: 406.7297\n",
      "Training Epoch: 22 [33152/49669]\tLoss: 398.9110\n",
      "Training Epoch: 22 [33216/49669]\tLoss: 410.9594\n",
      "Training Epoch: 22 [33280/49669]\tLoss: 404.1563\n",
      "Training Epoch: 22 [33344/49669]\tLoss: 406.2683\n",
      "Training Epoch: 22 [33408/49669]\tLoss: 409.7815\n",
      "Training Epoch: 22 [33472/49669]\tLoss: 408.4153\n",
      "Training Epoch: 22 [33536/49669]\tLoss: 427.8148\n",
      "Training Epoch: 22 [33600/49669]\tLoss: 443.7538\n",
      "Training Epoch: 22 [33664/49669]\tLoss: 398.5870\n",
      "Training Epoch: 22 [33728/49669]\tLoss: 397.1431\n",
      "Training Epoch: 22 [33792/49669]\tLoss: 417.8465\n",
      "Training Epoch: 22 [33856/49669]\tLoss: 415.7190\n",
      "Training Epoch: 22 [33920/49669]\tLoss: 408.2225\n",
      "Training Epoch: 22 [33984/49669]\tLoss: 395.6904\n",
      "Training Epoch: 22 [34048/49669]\tLoss: 373.4521\n",
      "Training Epoch: 22 [34112/49669]\tLoss: 403.2849\n",
      "Training Epoch: 22 [34176/49669]\tLoss: 372.9241\n",
      "Training Epoch: 22 [34240/49669]\tLoss: 397.1379\n",
      "Training Epoch: 22 [34304/49669]\tLoss: 421.4311\n",
      "Training Epoch: 22 [34368/49669]\tLoss: 397.4316\n",
      "Training Epoch: 22 [34432/49669]\tLoss: 376.7255\n",
      "Training Epoch: 22 [34496/49669]\tLoss: 427.2630\n",
      "Training Epoch: 22 [34560/49669]\tLoss: 441.4126\n",
      "Training Epoch: 22 [34624/49669]\tLoss: 393.5547\n",
      "Training Epoch: 22 [34688/49669]\tLoss: 390.2743\n",
      "Training Epoch: 22 [34752/49669]\tLoss: 410.9566\n",
      "Training Epoch: 22 [34816/49669]\tLoss: 391.4630\n",
      "Training Epoch: 22 [34880/49669]\tLoss: 401.1555\n",
      "Training Epoch: 22 [34944/49669]\tLoss: 422.0500\n",
      "Training Epoch: 22 [35008/49669]\tLoss: 402.3698\n",
      "Training Epoch: 22 [35072/49669]\tLoss: 406.9913\n",
      "Training Epoch: 22 [35136/49669]\tLoss: 407.6503\n",
      "Training Epoch: 22 [35200/49669]\tLoss: 410.7261\n",
      "Training Epoch: 22 [35264/49669]\tLoss: 405.2169\n",
      "Training Epoch: 22 [35328/49669]\tLoss: 365.9316\n",
      "Training Epoch: 22 [35392/49669]\tLoss: 392.8533\n",
      "Training Epoch: 22 [35456/49669]\tLoss: 407.1261\n",
      "Training Epoch: 22 [35520/49669]\tLoss: 406.7353\n",
      "Training Epoch: 22 [35584/49669]\tLoss: 415.0071\n",
      "Training Epoch: 22 [35648/49669]\tLoss: 410.1112\n",
      "Training Epoch: 22 [35712/49669]\tLoss: 408.2521\n",
      "Training Epoch: 22 [35776/49669]\tLoss: 424.8034\n",
      "Training Epoch: 22 [35840/49669]\tLoss: 412.9476\n",
      "Training Epoch: 22 [35904/49669]\tLoss: 395.6344\n",
      "Training Epoch: 22 [35968/49669]\tLoss: 416.1072\n",
      "Training Epoch: 22 [36032/49669]\tLoss: 416.8856\n",
      "Training Epoch: 22 [36096/49669]\tLoss: 400.3002\n",
      "Training Epoch: 22 [36160/49669]\tLoss: 399.5367\n",
      "Training Epoch: 22 [36224/49669]\tLoss: 421.1837\n",
      "Training Epoch: 22 [36288/49669]\tLoss: 438.5327\n",
      "Training Epoch: 22 [36352/49669]\tLoss: 410.4268\n",
      "Training Epoch: 22 [36416/49669]\tLoss: 420.2429\n",
      "Training Epoch: 22 [36480/49669]\tLoss: 387.5127\n",
      "Training Epoch: 22 [36544/49669]\tLoss: 397.7101\n",
      "Training Epoch: 22 [36608/49669]\tLoss: 403.8004\n",
      "Training Epoch: 22 [36672/49669]\tLoss: 441.1707\n",
      "Training Epoch: 22 [36736/49669]\tLoss: 394.9467\n",
      "Training Epoch: 22 [36800/49669]\tLoss: 382.8246\n",
      "Training Epoch: 22 [36864/49669]\tLoss: 424.1440\n",
      "Training Epoch: 22 [36928/49669]\tLoss: 396.5426\n",
      "Training Epoch: 22 [36992/49669]\tLoss: 426.9284\n",
      "Training Epoch: 22 [37056/49669]\tLoss: 405.3865\n",
      "Training Epoch: 22 [37120/49669]\tLoss: 395.3237\n",
      "Training Epoch: 22 [37184/49669]\tLoss: 443.2229\n",
      "Training Epoch: 22 [37248/49669]\tLoss: 406.4150\n",
      "Training Epoch: 22 [37312/49669]\tLoss: 385.3537\n",
      "Training Epoch: 22 [37376/49669]\tLoss: 421.0685\n",
      "Training Epoch: 22 [37440/49669]\tLoss: 431.1618\n",
      "Training Epoch: 22 [37504/49669]\tLoss: 424.3903\n",
      "Training Epoch: 22 [37568/49669]\tLoss: 413.9217\n",
      "Training Epoch: 22 [37632/49669]\tLoss: 419.4262\n",
      "Training Epoch: 22 [37696/49669]\tLoss: 403.3502\n",
      "Training Epoch: 22 [37760/49669]\tLoss: 388.5849\n",
      "Training Epoch: 22 [37824/49669]\tLoss: 399.2165\n",
      "Training Epoch: 22 [37888/49669]\tLoss: 388.7549\n",
      "Training Epoch: 22 [37952/49669]\tLoss: 394.9877\n",
      "Training Epoch: 22 [38016/49669]\tLoss: 409.0112\n",
      "Training Epoch: 22 [38080/49669]\tLoss: 427.1863\n",
      "Training Epoch: 22 [38144/49669]\tLoss: 357.4988\n",
      "Training Epoch: 22 [38208/49669]\tLoss: 361.8043\n",
      "Training Epoch: 22 [38272/49669]\tLoss: 404.6630\n",
      "Training Epoch: 22 [38336/49669]\tLoss: 406.6337\n",
      "Training Epoch: 22 [38400/49669]\tLoss: 402.4890\n",
      "Training Epoch: 22 [38464/49669]\tLoss: 415.5929\n",
      "Training Epoch: 22 [38528/49669]\tLoss: 380.5627\n",
      "Training Epoch: 22 [38592/49669]\tLoss: 414.6197\n",
      "Training Epoch: 22 [38656/49669]\tLoss: 421.5578\n",
      "Training Epoch: 22 [38720/49669]\tLoss: 401.1342\n",
      "Training Epoch: 22 [38784/49669]\tLoss: 399.1854\n",
      "Training Epoch: 22 [38848/49669]\tLoss: 390.8214\n",
      "Training Epoch: 22 [38912/49669]\tLoss: 402.4366\n",
      "Training Epoch: 22 [38976/49669]\tLoss: 399.7531\n",
      "Training Epoch: 22 [39040/49669]\tLoss: 389.7807\n",
      "Training Epoch: 22 [39104/49669]\tLoss: 420.9708\n",
      "Training Epoch: 22 [39168/49669]\tLoss: 392.1356\n",
      "Training Epoch: 22 [39232/49669]\tLoss: 400.3655\n",
      "Training Epoch: 22 [39296/49669]\tLoss: 379.5699\n",
      "Training Epoch: 22 [39360/49669]\tLoss: 378.0631\n",
      "Training Epoch: 22 [39424/49669]\tLoss: 402.6269\n",
      "Training Epoch: 22 [39488/49669]\tLoss: 398.3941\n",
      "Training Epoch: 22 [39552/49669]\tLoss: 443.6353\n",
      "Training Epoch: 22 [39616/49669]\tLoss: 410.2974\n",
      "Training Epoch: 22 [39680/49669]\tLoss: 429.1596\n",
      "Training Epoch: 22 [39744/49669]\tLoss: 406.2222\n",
      "Training Epoch: 22 [39808/49669]\tLoss: 401.9371\n",
      "Training Epoch: 22 [39872/49669]\tLoss: 379.9004\n",
      "Training Epoch: 22 [39936/49669]\tLoss: 391.3395\n",
      "Training Epoch: 22 [40000/49669]\tLoss: 407.6129\n",
      "Training Epoch: 22 [40064/49669]\tLoss: 419.2656\n",
      "Training Epoch: 22 [40128/49669]\tLoss: 408.7390\n",
      "Training Epoch: 22 [40192/49669]\tLoss: 383.0254\n",
      "Training Epoch: 22 [40256/49669]\tLoss: 359.3044\n",
      "Training Epoch: 22 [40320/49669]\tLoss: 421.2294\n",
      "Training Epoch: 22 [40384/49669]\tLoss: 420.2359\n",
      "Training Epoch: 22 [40448/49669]\tLoss: 416.1468\n",
      "Training Epoch: 22 [40512/49669]\tLoss: 396.3298\n",
      "Training Epoch: 22 [40576/49669]\tLoss: 412.3949\n",
      "Training Epoch: 22 [40640/49669]\tLoss: 393.5577\n",
      "Training Epoch: 22 [40704/49669]\tLoss: 400.0408\n",
      "Training Epoch: 22 [40768/49669]\tLoss: 394.2397\n",
      "Training Epoch: 22 [40832/49669]\tLoss: 435.6092\n",
      "Training Epoch: 22 [40896/49669]\tLoss: 425.4041\n",
      "Training Epoch: 22 [40960/49669]\tLoss: 426.8954\n",
      "Training Epoch: 22 [41024/49669]\tLoss: 401.9872\n",
      "Training Epoch: 22 [41088/49669]\tLoss: 388.1151\n",
      "Training Epoch: 22 [41152/49669]\tLoss: 410.9821\n",
      "Training Epoch: 22 [41216/49669]\tLoss: 416.2204\n",
      "Training Epoch: 22 [41280/49669]\tLoss: 386.5265\n",
      "Training Epoch: 22 [41344/49669]\tLoss: 385.9243\n",
      "Training Epoch: 22 [41408/49669]\tLoss: 410.7814\n",
      "Training Epoch: 22 [41472/49669]\tLoss: 399.9244\n",
      "Training Epoch: 22 [41536/49669]\tLoss: 416.0910\n",
      "Training Epoch: 22 [41600/49669]\tLoss: 422.4149\n",
      "Training Epoch: 22 [41664/49669]\tLoss: 409.8456\n",
      "Training Epoch: 22 [41728/49669]\tLoss: 424.3554\n",
      "Training Epoch: 22 [41792/49669]\tLoss: 420.1488\n",
      "Training Epoch: 22 [41856/49669]\tLoss: 402.9655\n",
      "Training Epoch: 22 [41920/49669]\tLoss: 405.0034\n",
      "Training Epoch: 22 [41984/49669]\tLoss: 386.1494\n",
      "Training Epoch: 22 [42048/49669]\tLoss: 398.6367\n",
      "Training Epoch: 22 [42112/49669]\tLoss: 404.9716\n",
      "Training Epoch: 22 [42176/49669]\tLoss: 417.4486\n",
      "Training Epoch: 22 [42240/49669]\tLoss: 420.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [42304/49669]\tLoss: 416.6610\n",
      "Training Epoch: 22 [42368/49669]\tLoss: 397.5385\n",
      "Training Epoch: 22 [42432/49669]\tLoss: 407.4126\n",
      "Training Epoch: 22 [42496/49669]\tLoss: 399.3085\n",
      "Training Epoch: 22 [42560/49669]\tLoss: 423.9127\n",
      "Training Epoch: 22 [42624/49669]\tLoss: 414.7166\n",
      "Training Epoch: 22 [42688/49669]\tLoss: 433.8734\n",
      "Training Epoch: 22 [42752/49669]\tLoss: 428.6664\n",
      "Training Epoch: 22 [42816/49669]\tLoss: 438.1060\n",
      "Training Epoch: 22 [42880/49669]\tLoss: 436.9975\n",
      "Training Epoch: 22 [42944/49669]\tLoss: 397.2955\n",
      "Training Epoch: 22 [43008/49669]\tLoss: 412.9021\n",
      "Training Epoch: 22 [43072/49669]\tLoss: 419.8015\n",
      "Training Epoch: 22 [43136/49669]\tLoss: 383.9294\n",
      "Training Epoch: 22 [43200/49669]\tLoss: 405.8236\n",
      "Training Epoch: 22 [43264/49669]\tLoss: 406.7245\n",
      "Training Epoch: 22 [43328/49669]\tLoss: 432.2352\n",
      "Training Epoch: 22 [43392/49669]\tLoss: 419.5106\n",
      "Training Epoch: 22 [43456/49669]\tLoss: 410.8060\n",
      "Training Epoch: 22 [43520/49669]\tLoss: 421.1058\n",
      "Training Epoch: 22 [43584/49669]\tLoss: 416.0925\n",
      "Training Epoch: 22 [43648/49669]\tLoss: 430.8209\n",
      "Training Epoch: 22 [43712/49669]\tLoss: 441.2104\n",
      "Training Epoch: 22 [43776/49669]\tLoss: 387.4829\n",
      "Training Epoch: 22 [43840/49669]\tLoss: 407.7300\n",
      "Training Epoch: 22 [43904/49669]\tLoss: 401.2999\n",
      "Training Epoch: 22 [43968/49669]\tLoss: 411.6503\n",
      "Training Epoch: 22 [44032/49669]\tLoss: 420.8585\n",
      "Training Epoch: 22 [44096/49669]\tLoss: 388.6400\n",
      "Training Epoch: 22 [44160/49669]\tLoss: 410.0482\n",
      "Training Epoch: 22 [44224/49669]\tLoss: 419.7861\n",
      "Training Epoch: 22 [44288/49669]\tLoss: 411.0662\n",
      "Training Epoch: 22 [44352/49669]\tLoss: 398.2205\n",
      "Training Epoch: 22 [44416/49669]\tLoss: 406.7319\n",
      "Training Epoch: 22 [44480/49669]\tLoss: 450.0830\n",
      "Training Epoch: 22 [44544/49669]\tLoss: 420.2832\n",
      "Training Epoch: 22 [44608/49669]\tLoss: 402.2678\n",
      "Training Epoch: 22 [44672/49669]\tLoss: 447.5489\n",
      "Training Epoch: 22 [44736/49669]\tLoss: 423.7564\n",
      "Training Epoch: 22 [44800/49669]\tLoss: 421.9536\n",
      "Training Epoch: 22 [44864/49669]\tLoss: 405.6488\n",
      "Training Epoch: 22 [44928/49669]\tLoss: 458.2809\n",
      "Training Epoch: 22 [44992/49669]\tLoss: 382.1045\n",
      "Training Epoch: 22 [45056/49669]\tLoss: 421.2400\n",
      "Training Epoch: 22 [45120/49669]\tLoss: 419.0036\n",
      "Training Epoch: 22 [45184/49669]\tLoss: 411.6303\n",
      "Training Epoch: 22 [45248/49669]\tLoss: 372.0902\n",
      "Training Epoch: 22 [45312/49669]\tLoss: 414.6029\n",
      "Training Epoch: 22 [45376/49669]\tLoss: 408.2003\n",
      "Training Epoch: 22 [45440/49669]\tLoss: 418.3127\n",
      "Training Epoch: 22 [45504/49669]\tLoss: 387.8845\n",
      "Training Epoch: 22 [45568/49669]\tLoss: 410.5541\n",
      "Training Epoch: 22 [45632/49669]\tLoss: 409.0383\n",
      "Training Epoch: 22 [45696/49669]\tLoss: 422.1676\n",
      "Training Epoch: 22 [45760/49669]\tLoss: 419.7255\n",
      "Training Epoch: 22 [45824/49669]\tLoss: 378.7560\n",
      "Training Epoch: 22 [45888/49669]\tLoss: 416.9231\n",
      "Training Epoch: 22 [45952/49669]\tLoss: 417.8789\n",
      "Training Epoch: 22 [46016/49669]\tLoss: 409.2757\n",
      "Training Epoch: 22 [46080/49669]\tLoss: 401.5979\n",
      "Training Epoch: 22 [46144/49669]\tLoss: 384.0199\n",
      "Training Epoch: 22 [46208/49669]\tLoss: 407.6070\n",
      "Training Epoch: 22 [46272/49669]\tLoss: 381.2961\n",
      "Training Epoch: 22 [46336/49669]\tLoss: 382.4784\n",
      "Training Epoch: 22 [46400/49669]\tLoss: 419.6291\n",
      "Training Epoch: 22 [46464/49669]\tLoss: 382.4961\n",
      "Training Epoch: 22 [46528/49669]\tLoss: 433.2763\n",
      "Training Epoch: 22 [46592/49669]\tLoss: 418.3864\n",
      "Training Epoch: 22 [46656/49669]\tLoss: 396.5420\n",
      "Training Epoch: 22 [46720/49669]\tLoss: 388.5058\n",
      "Training Epoch: 22 [46784/49669]\tLoss: 396.2733\n",
      "Training Epoch: 22 [46848/49669]\tLoss: 388.1009\n",
      "Training Epoch: 22 [46912/49669]\tLoss: 402.9388\n",
      "Training Epoch: 22 [46976/49669]\tLoss: 416.0438\n",
      "Training Epoch: 22 [47040/49669]\tLoss: 433.5957\n",
      "Training Epoch: 22 [47104/49669]\tLoss: 398.2367\n",
      "Training Epoch: 22 [47168/49669]\tLoss: 391.8993\n",
      "Training Epoch: 22 [47232/49669]\tLoss: 404.7336\n",
      "Training Epoch: 22 [47296/49669]\tLoss: 413.4095\n",
      "Training Epoch: 22 [47360/49669]\tLoss: 389.9919\n",
      "Training Epoch: 22 [47424/49669]\tLoss: 376.2647\n",
      "Training Epoch: 22 [47488/49669]\tLoss: 394.2365\n",
      "Training Epoch: 22 [47552/49669]\tLoss: 426.7134\n",
      "Training Epoch: 22 [47616/49669]\tLoss: 386.8151\n",
      "Training Epoch: 22 [47680/49669]\tLoss: 403.7664\n",
      "Training Epoch: 22 [47744/49669]\tLoss: 414.4690\n",
      "Training Epoch: 22 [47808/49669]\tLoss: 398.1953\n",
      "Training Epoch: 22 [47872/49669]\tLoss: 408.5932\n",
      "Training Epoch: 22 [47936/49669]\tLoss: 398.9021\n",
      "Training Epoch: 22 [48000/49669]\tLoss: 420.5427\n",
      "Training Epoch: 22 [48064/49669]\tLoss: 403.2161\n",
      "Training Epoch: 22 [48128/49669]\tLoss: 403.3272\n",
      "Training Epoch: 22 [48192/49669]\tLoss: 432.9523\n",
      "Training Epoch: 22 [48256/49669]\tLoss: 373.9929\n",
      "Training Epoch: 22 [48320/49669]\tLoss: 396.6556\n",
      "Training Epoch: 22 [48384/49669]\tLoss: 359.9229\n",
      "Training Epoch: 22 [48448/49669]\tLoss: 373.8315\n",
      "Training Epoch: 22 [48512/49669]\tLoss: 395.7017\n",
      "Training Epoch: 22 [48576/49669]\tLoss: 416.1662\n",
      "Training Epoch: 22 [48640/49669]\tLoss: 381.9868\n",
      "Training Epoch: 22 [48704/49669]\tLoss: 435.4686\n",
      "Training Epoch: 22 [48768/49669]\tLoss: 398.3726\n",
      "Training Epoch: 22 [48832/49669]\tLoss: 411.3206\n",
      "Training Epoch: 22 [48896/49669]\tLoss: 408.0188\n",
      "Training Epoch: 22 [48960/49669]\tLoss: 392.5225\n",
      "Training Epoch: 22 [49024/49669]\tLoss: 397.4349\n",
      "Training Epoch: 22 [49088/49669]\tLoss: 400.7065\n",
      "Training Epoch: 22 [49152/49669]\tLoss: 397.9559\n",
      "Training Epoch: 22 [49216/49669]\tLoss: 411.8319\n",
      "Training Epoch: 22 [49280/49669]\tLoss: 397.4609\n",
      "Training Epoch: 22 [49344/49669]\tLoss: 417.0187\n",
      "Training Epoch: 22 [49408/49669]\tLoss: 413.5372\n",
      "Training Epoch: 22 [49472/49669]\tLoss: 408.5816\n",
      "Training Epoch: 22 [49536/49669]\tLoss: 403.6963\n",
      "Training Epoch: 22 [49600/49669]\tLoss: 388.7381\n",
      "Training Epoch: 22 [49664/49669]\tLoss: 413.0565\n",
      "Training Epoch: 22 [49669/49669]\tLoss: 397.6224\n",
      "Training Epoch: 22 [5519/5519]\tLoss: 404.8798\n",
      "Training Epoch: 23 [64/49669]\tLoss: 417.0616\n",
      "Training Epoch: 23 [128/49669]\tLoss: 405.7004\n",
      "Training Epoch: 23 [192/49669]\tLoss: 397.7620\n",
      "Training Epoch: 23 [256/49669]\tLoss: 409.3134\n",
      "Training Epoch: 23 [320/49669]\tLoss: 403.2065\n",
      "Training Epoch: 23 [384/49669]\tLoss: 412.1411\n",
      "Training Epoch: 23 [448/49669]\tLoss: 417.3859\n",
      "Training Epoch: 23 [512/49669]\tLoss: 427.9304\n",
      "Training Epoch: 23 [576/49669]\tLoss: 396.3345\n",
      "Training Epoch: 23 [640/49669]\tLoss: 421.4696\n",
      "Training Epoch: 23 [704/49669]\tLoss: 412.9202\n",
      "Training Epoch: 23 [768/49669]\tLoss: 369.7472\n",
      "Training Epoch: 23 [832/49669]\tLoss: 390.3421\n",
      "Training Epoch: 23 [896/49669]\tLoss: 417.2850\n",
      "Training Epoch: 23 [960/49669]\tLoss: 397.9417\n",
      "Training Epoch: 23 [1024/49669]\tLoss: 404.8533\n",
      "Training Epoch: 23 [1088/49669]\tLoss: 392.3080\n",
      "Training Epoch: 23 [1152/49669]\tLoss: 422.9099\n",
      "Training Epoch: 23 [1216/49669]\tLoss: 413.2055\n",
      "Training Epoch: 23 [1280/49669]\tLoss: 413.8947\n",
      "Training Epoch: 23 [1344/49669]\tLoss: 390.2010\n",
      "Training Epoch: 23 [1408/49669]\tLoss: 411.5188\n",
      "Training Epoch: 23 [1472/49669]\tLoss: 462.1268\n",
      "Training Epoch: 23 [1536/49669]\tLoss: 420.4447\n",
      "Training Epoch: 23 [1600/49669]\tLoss: 419.6516\n",
      "Training Epoch: 23 [1664/49669]\tLoss: 448.0825\n",
      "Training Epoch: 23 [1728/49669]\tLoss: 448.0656\n",
      "Training Epoch: 23 [1792/49669]\tLoss: 425.5216\n",
      "Training Epoch: 23 [1856/49669]\tLoss: 457.7238\n",
      "Training Epoch: 23 [1920/49669]\tLoss: 461.6962\n",
      "Training Epoch: 23 [1984/49669]\tLoss: 427.5914\n",
      "Training Epoch: 23 [2048/49669]\tLoss: 417.6373\n",
      "Training Epoch: 23 [2112/49669]\tLoss: 418.4108\n",
      "Training Epoch: 23 [2176/49669]\tLoss: 396.8191\n",
      "Training Epoch: 23 [2240/49669]\tLoss: 409.0669\n",
      "Training Epoch: 23 [2304/49669]\tLoss: 438.8972\n",
      "Training Epoch: 23 [2368/49669]\tLoss: 440.4470\n",
      "Training Epoch: 23 [2432/49669]\tLoss: 431.3861\n",
      "Training Epoch: 23 [2496/49669]\tLoss: 462.2802\n",
      "Training Epoch: 23 [2560/49669]\tLoss: 455.0006\n",
      "Training Epoch: 23 [2624/49669]\tLoss: 419.9604\n",
      "Training Epoch: 23 [2688/49669]\tLoss: 399.3777\n",
      "Training Epoch: 23 [2752/49669]\tLoss: 380.9943\n",
      "Training Epoch: 23 [2816/49669]\tLoss: 391.3908\n",
      "Training Epoch: 23 [2880/49669]\tLoss: 455.9015\n",
      "Training Epoch: 23 [2944/49669]\tLoss: 421.9341\n",
      "Training Epoch: 23 [3008/49669]\tLoss: 439.1355\n",
      "Training Epoch: 23 [3072/49669]\tLoss: 399.4612\n",
      "Training Epoch: 23 [3136/49669]\tLoss: 389.5172\n",
      "Training Epoch: 23 [3200/49669]\tLoss: 406.3142\n",
      "Training Epoch: 23 [3264/49669]\tLoss: 433.1229\n",
      "Training Epoch: 23 [3328/49669]\tLoss: 386.7806\n",
      "Training Epoch: 23 [3392/49669]\tLoss: 442.9167\n",
      "Training Epoch: 23 [3456/49669]\tLoss: 423.1680\n",
      "Training Epoch: 23 [3520/49669]\tLoss: 420.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [3584/49669]\tLoss: 459.7555\n",
      "Training Epoch: 23 [3648/49669]\tLoss: 430.3105\n",
      "Training Epoch: 23 [3712/49669]\tLoss: 391.2082\n",
      "Training Epoch: 23 [3776/49669]\tLoss: 441.8608\n",
      "Training Epoch: 23 [3840/49669]\tLoss: 422.1563\n",
      "Training Epoch: 23 [3904/49669]\tLoss: 387.0961\n",
      "Training Epoch: 23 [3968/49669]\tLoss: 404.3086\n",
      "Training Epoch: 23 [4032/49669]\tLoss: 402.2051\n",
      "Training Epoch: 23 [4096/49669]\tLoss: 410.2274\n",
      "Training Epoch: 23 [4160/49669]\tLoss: 385.3820\n",
      "Training Epoch: 23 [4224/49669]\tLoss: 420.5982\n",
      "Training Epoch: 23 [4288/49669]\tLoss: 420.4013\n",
      "Training Epoch: 23 [4352/49669]\tLoss: 407.0356\n",
      "Training Epoch: 23 [4416/49669]\tLoss: 405.7538\n",
      "Training Epoch: 23 [4480/49669]\tLoss: 411.2690\n",
      "Training Epoch: 23 [4544/49669]\tLoss: 397.0230\n",
      "Training Epoch: 23 [4608/49669]\tLoss: 394.5471\n",
      "Training Epoch: 23 [4672/49669]\tLoss: 431.2121\n",
      "Training Epoch: 23 [4736/49669]\tLoss: 400.3324\n",
      "Training Epoch: 23 [4800/49669]\tLoss: 404.7370\n",
      "Training Epoch: 23 [4864/49669]\tLoss: 407.7785\n",
      "Training Epoch: 23 [4928/49669]\tLoss: 405.4354\n",
      "Training Epoch: 23 [4992/49669]\tLoss: 403.8255\n",
      "Training Epoch: 23 [5056/49669]\tLoss: 422.7976\n",
      "Training Epoch: 23 [5120/49669]\tLoss: 428.2935\n",
      "Training Epoch: 23 [5184/49669]\tLoss: 413.1211\n",
      "Training Epoch: 23 [5248/49669]\tLoss: 417.1812\n",
      "Training Epoch: 23 [5312/49669]\tLoss: 395.5727\n",
      "Training Epoch: 23 [5376/49669]\tLoss: 414.3596\n",
      "Training Epoch: 23 [5440/49669]\tLoss: 402.1627\n",
      "Training Epoch: 23 [5504/49669]\tLoss: 385.7411\n",
      "Training Epoch: 23 [5568/49669]\tLoss: 369.9390\n",
      "Training Epoch: 23 [5632/49669]\tLoss: 413.1601\n",
      "Training Epoch: 23 [5696/49669]\tLoss: 409.4591\n",
      "Training Epoch: 23 [5760/49669]\tLoss: 390.6891\n",
      "Training Epoch: 23 [5824/49669]\tLoss: 386.5749\n",
      "Training Epoch: 23 [5888/49669]\tLoss: 405.7870\n",
      "Training Epoch: 23 [5952/49669]\tLoss: 406.4034\n",
      "Training Epoch: 23 [6016/49669]\tLoss: 428.6089\n",
      "Training Epoch: 23 [6080/49669]\tLoss: 431.1498\n",
      "Training Epoch: 23 [6144/49669]\tLoss: 401.0004\n",
      "Training Epoch: 23 [6208/49669]\tLoss: 409.5651\n",
      "Training Epoch: 23 [6272/49669]\tLoss: 412.0076\n",
      "Training Epoch: 23 [6336/49669]\tLoss: 432.2891\n",
      "Training Epoch: 23 [6400/49669]\tLoss: 398.1409\n",
      "Training Epoch: 23 [6464/49669]\tLoss: 402.1519\n",
      "Training Epoch: 23 [6528/49669]\tLoss: 381.8031\n",
      "Training Epoch: 23 [6592/49669]\tLoss: 417.2636\n",
      "Training Epoch: 23 [6656/49669]\tLoss: 404.0818\n",
      "Training Epoch: 23 [6720/49669]\tLoss: 378.0369\n",
      "Training Epoch: 23 [6784/49669]\tLoss: 409.5475\n",
      "Training Epoch: 23 [6848/49669]\tLoss: 386.0334\n",
      "Training Epoch: 23 [6912/49669]\tLoss: 388.3087\n",
      "Training Epoch: 23 [6976/49669]\tLoss: 387.5583\n",
      "Training Epoch: 23 [7040/49669]\tLoss: 404.5039\n",
      "Training Epoch: 23 [7104/49669]\tLoss: 384.3199\n",
      "Training Epoch: 23 [7168/49669]\tLoss: 389.6063\n",
      "Training Epoch: 23 [7232/49669]\tLoss: 418.4980\n",
      "Training Epoch: 23 [7296/49669]\tLoss: 398.2481\n",
      "Training Epoch: 23 [7360/49669]\tLoss: 385.7075\n",
      "Training Epoch: 23 [7424/49669]\tLoss: 404.6851\n",
      "Training Epoch: 23 [7488/49669]\tLoss: 391.8934\n",
      "Training Epoch: 23 [7552/49669]\tLoss: 378.1720\n",
      "Training Epoch: 23 [7616/49669]\tLoss: 395.9660\n",
      "Training Epoch: 23 [7680/49669]\tLoss: 417.3986\n",
      "Training Epoch: 23 [7744/49669]\tLoss: 379.8929\n",
      "Training Epoch: 23 [7808/49669]\tLoss: 400.8048\n",
      "Training Epoch: 23 [7872/49669]\tLoss: 386.6656\n",
      "Training Epoch: 23 [7936/49669]\tLoss: 423.4644\n",
      "Training Epoch: 23 [8000/49669]\tLoss: 405.9450\n",
      "Training Epoch: 23 [8064/49669]\tLoss: 405.0083\n",
      "Training Epoch: 23 [8128/49669]\tLoss: 387.6795\n",
      "Training Epoch: 23 [8192/49669]\tLoss: 409.1564\n",
      "Training Epoch: 23 [8256/49669]\tLoss: 381.9332\n",
      "Training Epoch: 23 [8320/49669]\tLoss: 409.7259\n",
      "Training Epoch: 23 [8384/49669]\tLoss: 389.2658\n",
      "Training Epoch: 23 [8448/49669]\tLoss: 420.9502\n",
      "Training Epoch: 23 [8512/49669]\tLoss: 405.3658\n",
      "Training Epoch: 23 [8576/49669]\tLoss: 401.8912\n",
      "Training Epoch: 23 [8640/49669]\tLoss: 404.9288\n",
      "Training Epoch: 23 [8704/49669]\tLoss: 418.2179\n",
      "Training Epoch: 23 [8768/49669]\tLoss: 433.1323\n",
      "Training Epoch: 23 [8832/49669]\tLoss: 388.7369\n",
      "Training Epoch: 23 [8896/49669]\tLoss: 407.6779\n",
      "Training Epoch: 23 [8960/49669]\tLoss: 400.0527\n",
      "Training Epoch: 23 [9024/49669]\tLoss: 402.3618\n",
      "Training Epoch: 23 [9088/49669]\tLoss: 420.3826\n",
      "Training Epoch: 23 [9152/49669]\tLoss: 440.4355\n",
      "Training Epoch: 23 [9216/49669]\tLoss: 412.8315\n",
      "Training Epoch: 23 [9280/49669]\tLoss: 410.7571\n",
      "Training Epoch: 23 [9344/49669]\tLoss: 380.8515\n",
      "Training Epoch: 23 [9408/49669]\tLoss: 413.7982\n",
      "Training Epoch: 23 [9472/49669]\tLoss: 407.5043\n",
      "Training Epoch: 23 [9536/49669]\tLoss: 412.6749\n",
      "Training Epoch: 23 [9600/49669]\tLoss: 393.5670\n",
      "Training Epoch: 23 [9664/49669]\tLoss: 399.8854\n",
      "Training Epoch: 23 [9728/49669]\tLoss: 378.2308\n",
      "Training Epoch: 23 [9792/49669]\tLoss: 404.1789\n",
      "Training Epoch: 23 [9856/49669]\tLoss: 392.3458\n",
      "Training Epoch: 23 [9920/49669]\tLoss: 434.6776\n",
      "Training Epoch: 23 [9984/49669]\tLoss: 417.3286\n",
      "Training Epoch: 23 [10048/49669]\tLoss: 406.3386\n",
      "Training Epoch: 23 [10112/49669]\tLoss: 432.5168\n",
      "Training Epoch: 23 [10176/49669]\tLoss: 401.7371\n",
      "Training Epoch: 23 [10240/49669]\tLoss: 407.8087\n",
      "Training Epoch: 23 [10304/49669]\tLoss: 405.3140\n",
      "Training Epoch: 23 [10368/49669]\tLoss: 433.5726\n",
      "Training Epoch: 23 [10432/49669]\tLoss: 425.6832\n",
      "Training Epoch: 23 [10496/49669]\tLoss: 410.2048\n",
      "Training Epoch: 23 [10560/49669]\tLoss: 421.5255\n",
      "Training Epoch: 23 [10624/49669]\tLoss: 406.4288\n",
      "Training Epoch: 23 [10688/49669]\tLoss: 400.3052\n",
      "Training Epoch: 23 [10752/49669]\tLoss: 404.9030\n",
      "Training Epoch: 23 [10816/49669]\tLoss: 412.2916\n",
      "Training Epoch: 23 [10880/49669]\tLoss: 387.0030\n",
      "Training Epoch: 23 [10944/49669]\tLoss: 390.7535\n",
      "Training Epoch: 23 [11008/49669]\tLoss: 410.2668\n",
      "Training Epoch: 23 [11072/49669]\tLoss: 409.5978\n",
      "Training Epoch: 23 [11136/49669]\tLoss: 397.7994\n",
      "Training Epoch: 23 [11200/49669]\tLoss: 411.3345\n",
      "Training Epoch: 23 [11264/49669]\tLoss: 402.0789\n",
      "Training Epoch: 23 [11328/49669]\tLoss: 410.9003\n",
      "Training Epoch: 23 [11392/49669]\tLoss: 413.1522\n",
      "Training Epoch: 23 [11456/49669]\tLoss: 421.2775\n",
      "Training Epoch: 23 [11520/49669]\tLoss: 390.4904\n",
      "Training Epoch: 23 [11584/49669]\tLoss: 402.4027\n",
      "Training Epoch: 23 [11648/49669]\tLoss: 393.9506\n",
      "Training Epoch: 23 [11712/49669]\tLoss: 403.6082\n",
      "Training Epoch: 23 [11776/49669]\tLoss: 427.2399\n",
      "Training Epoch: 23 [11840/49669]\tLoss: 409.5436\n",
      "Training Epoch: 23 [11904/49669]\tLoss: 434.4466\n",
      "Training Epoch: 23 [11968/49669]\tLoss: 424.3256\n",
      "Training Epoch: 23 [12032/49669]\tLoss: 427.2388\n",
      "Training Epoch: 23 [12096/49669]\tLoss: 405.4125\n",
      "Training Epoch: 23 [12160/49669]\tLoss: 426.5475\n",
      "Training Epoch: 23 [12224/49669]\tLoss: 406.1172\n",
      "Training Epoch: 23 [12288/49669]\tLoss: 410.7769\n",
      "Training Epoch: 23 [12352/49669]\tLoss: 421.6554\n",
      "Training Epoch: 23 [12416/49669]\tLoss: 445.6820\n",
      "Training Epoch: 23 [12480/49669]\tLoss: 408.1611\n",
      "Training Epoch: 23 [12544/49669]\tLoss: 418.3734\n",
      "Training Epoch: 23 [12608/49669]\tLoss: 422.3058\n",
      "Training Epoch: 23 [12672/49669]\tLoss: 415.8784\n",
      "Training Epoch: 23 [12736/49669]\tLoss: 433.6461\n",
      "Training Epoch: 23 [12800/49669]\tLoss: 427.1992\n",
      "Training Epoch: 23 [12864/49669]\tLoss: 418.5131\n",
      "Training Epoch: 23 [12928/49669]\tLoss: 457.6739\n",
      "Training Epoch: 23 [12992/49669]\tLoss: 403.5969\n",
      "Training Epoch: 23 [13056/49669]\tLoss: 415.2854\n",
      "Training Epoch: 23 [13120/49669]\tLoss: 433.2764\n",
      "Training Epoch: 23 [13184/49669]\tLoss: 399.7769\n",
      "Training Epoch: 23 [13248/49669]\tLoss: 374.8523\n",
      "Training Epoch: 23 [13312/49669]\tLoss: 424.3705\n",
      "Training Epoch: 23 [13376/49669]\tLoss: 410.1111\n",
      "Training Epoch: 23 [13440/49669]\tLoss: 409.2356\n",
      "Training Epoch: 23 [13504/49669]\tLoss: 411.1263\n",
      "Training Epoch: 23 [13568/49669]\tLoss: 407.1895\n",
      "Training Epoch: 23 [13632/49669]\tLoss: 403.3029\n",
      "Training Epoch: 23 [13696/49669]\tLoss: 408.9735\n",
      "Training Epoch: 23 [13760/49669]\tLoss: 409.4821\n",
      "Training Epoch: 23 [13824/49669]\tLoss: 376.7172\n",
      "Training Epoch: 23 [13888/49669]\tLoss: 385.3482\n",
      "Training Epoch: 23 [13952/49669]\tLoss: 407.0761\n",
      "Training Epoch: 23 [14016/49669]\tLoss: 431.9013\n",
      "Training Epoch: 23 [14080/49669]\tLoss: 367.5422\n",
      "Training Epoch: 23 [14144/49669]\tLoss: 407.4261\n",
      "Training Epoch: 23 [14208/49669]\tLoss: 415.5880\n",
      "Training Epoch: 23 [14272/49669]\tLoss: 376.8737\n",
      "Training Epoch: 23 [14336/49669]\tLoss: 399.0735\n",
      "Training Epoch: 23 [14400/49669]\tLoss: 403.0216\n",
      "Training Epoch: 23 [14464/49669]\tLoss: 415.2725\n",
      "Training Epoch: 23 [14528/49669]\tLoss: 396.8237\n",
      "Training Epoch: 23 [14592/49669]\tLoss: 431.3033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [14656/49669]\tLoss: 408.4613\n",
      "Training Epoch: 23 [14720/49669]\tLoss: 419.7917\n",
      "Training Epoch: 23 [14784/49669]\tLoss: 438.1348\n",
      "Training Epoch: 23 [14848/49669]\tLoss: 376.9584\n",
      "Training Epoch: 23 [14912/49669]\tLoss: 391.9993\n",
      "Training Epoch: 23 [14976/49669]\tLoss: 388.9319\n",
      "Training Epoch: 23 [15040/49669]\tLoss: 413.4095\n",
      "Training Epoch: 23 [15104/49669]\tLoss: 420.2302\n",
      "Training Epoch: 23 [15168/49669]\tLoss: 421.1604\n",
      "Training Epoch: 23 [15232/49669]\tLoss: 390.7954\n",
      "Training Epoch: 23 [15296/49669]\tLoss: 408.5376\n",
      "Training Epoch: 23 [15360/49669]\tLoss: 378.8026\n",
      "Training Epoch: 23 [15424/49669]\tLoss: 395.0978\n",
      "Training Epoch: 23 [15488/49669]\tLoss: 408.0385\n",
      "Training Epoch: 23 [15552/49669]\tLoss: 398.7084\n",
      "Training Epoch: 23 [15616/49669]\tLoss: 398.7912\n",
      "Training Epoch: 23 [15680/49669]\tLoss: 424.4868\n",
      "Training Epoch: 23 [15744/49669]\tLoss: 397.2876\n",
      "Training Epoch: 23 [15808/49669]\tLoss: 414.6095\n",
      "Training Epoch: 23 [15872/49669]\tLoss: 406.6987\n",
      "Training Epoch: 23 [15936/49669]\tLoss: 392.7340\n",
      "Training Epoch: 23 [16000/49669]\tLoss: 414.9331\n",
      "Training Epoch: 23 [16064/49669]\tLoss: 379.5292\n",
      "Training Epoch: 23 [16128/49669]\tLoss: 392.6066\n",
      "Training Epoch: 23 [16192/49669]\tLoss: 387.4468\n",
      "Training Epoch: 23 [16256/49669]\tLoss: 384.4142\n",
      "Training Epoch: 23 [16320/49669]\tLoss: 371.3295\n",
      "Training Epoch: 23 [16384/49669]\tLoss: 425.9600\n",
      "Training Epoch: 23 [16448/49669]\tLoss: 416.1654\n",
      "Training Epoch: 23 [16512/49669]\tLoss: 400.7674\n",
      "Training Epoch: 23 [16576/49669]\tLoss: 369.0652\n",
      "Training Epoch: 23 [16640/49669]\tLoss: 407.9967\n",
      "Training Epoch: 23 [16704/49669]\tLoss: 382.0543\n",
      "Training Epoch: 23 [16768/49669]\tLoss: 382.5528\n",
      "Training Epoch: 23 [16832/49669]\tLoss: 418.7307\n",
      "Training Epoch: 23 [16896/49669]\tLoss: 406.2262\n",
      "Training Epoch: 23 [16960/49669]\tLoss: 405.7019\n",
      "Training Epoch: 23 [17024/49669]\tLoss: 383.1181\n",
      "Training Epoch: 23 [17088/49669]\tLoss: 412.4568\n",
      "Training Epoch: 23 [17152/49669]\tLoss: 407.8784\n",
      "Training Epoch: 23 [17216/49669]\tLoss: 383.3459\n",
      "Training Epoch: 23 [17280/49669]\tLoss: 413.1285\n",
      "Training Epoch: 23 [17344/49669]\tLoss: 380.5259\n",
      "Training Epoch: 23 [17408/49669]\tLoss: 386.7492\n",
      "Training Epoch: 23 [17472/49669]\tLoss: 436.4958\n",
      "Training Epoch: 23 [17536/49669]\tLoss: 424.2397\n",
      "Training Epoch: 23 [17600/49669]\tLoss: 382.7650\n",
      "Training Epoch: 23 [17664/49669]\tLoss: 381.7350\n",
      "Training Epoch: 23 [17728/49669]\tLoss: 386.0619\n",
      "Training Epoch: 23 [17792/49669]\tLoss: 393.7865\n",
      "Training Epoch: 23 [17856/49669]\tLoss: 417.7198\n",
      "Training Epoch: 23 [17920/49669]\tLoss: 451.7517\n",
      "Training Epoch: 23 [17984/49669]\tLoss: 402.0538\n",
      "Training Epoch: 23 [18048/49669]\tLoss: 399.6521\n",
      "Training Epoch: 23 [18112/49669]\tLoss: 367.8434\n",
      "Training Epoch: 23 [18176/49669]\tLoss: 393.4334\n",
      "Training Epoch: 23 [18240/49669]\tLoss: 414.0186\n",
      "Training Epoch: 23 [18304/49669]\tLoss: 417.6143\n",
      "Training Epoch: 23 [18368/49669]\tLoss: 429.2384\n",
      "Training Epoch: 23 [18432/49669]\tLoss: 389.9131\n",
      "Training Epoch: 23 [18496/49669]\tLoss: 416.2552\n",
      "Training Epoch: 23 [18560/49669]\tLoss: 393.6871\n",
      "Training Epoch: 23 [18624/49669]\tLoss: 437.5258\n",
      "Training Epoch: 23 [18688/49669]\tLoss: 396.8307\n",
      "Training Epoch: 23 [18752/49669]\tLoss: 405.8296\n",
      "Training Epoch: 23 [18816/49669]\tLoss: 399.0110\n",
      "Training Epoch: 23 [18880/49669]\tLoss: 400.6549\n",
      "Training Epoch: 23 [18944/49669]\tLoss: 442.4286\n",
      "Training Epoch: 23 [19008/49669]\tLoss: 421.8206\n",
      "Training Epoch: 23 [19072/49669]\tLoss: 423.6917\n",
      "Training Epoch: 23 [19136/49669]\tLoss: 434.8096\n",
      "Training Epoch: 23 [19200/49669]\tLoss: 427.0670\n",
      "Training Epoch: 23 [19264/49669]\tLoss: 462.7282\n",
      "Training Epoch: 23 [19328/49669]\tLoss: 488.9330\n",
      "Training Epoch: 23 [19392/49669]\tLoss: 515.9323\n",
      "Training Epoch: 23 [19456/49669]\tLoss: 483.2668\n",
      "Training Epoch: 23 [19520/49669]\tLoss: 490.2527\n",
      "Training Epoch: 23 [19584/49669]\tLoss: 430.7539\n",
      "Training Epoch: 23 [19648/49669]\tLoss: 378.0576\n",
      "Training Epoch: 23 [19712/49669]\tLoss: 430.3113\n",
      "Training Epoch: 23 [19776/49669]\tLoss: 442.2312\n",
      "Training Epoch: 23 [19840/49669]\tLoss: 460.0294\n",
      "Training Epoch: 23 [19904/49669]\tLoss: 476.0327\n",
      "Training Epoch: 23 [19968/49669]\tLoss: 473.8388\n",
      "Training Epoch: 23 [20032/49669]\tLoss: 420.5934\n",
      "Training Epoch: 23 [20096/49669]\tLoss: 390.4007\n",
      "Training Epoch: 23 [20160/49669]\tLoss: 433.9172\n",
      "Training Epoch: 23 [20224/49669]\tLoss: 398.0316\n",
      "Training Epoch: 23 [20288/49669]\tLoss: 389.0637\n",
      "Training Epoch: 23 [20352/49669]\tLoss: 422.6958\n",
      "Training Epoch: 23 [20416/49669]\tLoss: 402.9114\n",
      "Training Epoch: 23 [20480/49669]\tLoss: 437.7255\n",
      "Training Epoch: 23 [20544/49669]\tLoss: 440.0202\n",
      "Training Epoch: 23 [20608/49669]\tLoss: 425.7372\n",
      "Training Epoch: 23 [20672/49669]\tLoss: 402.9730\n",
      "Training Epoch: 23 [20736/49669]\tLoss: 364.1877\n",
      "Training Epoch: 23 [20800/49669]\tLoss: 409.9961\n",
      "Training Epoch: 23 [20864/49669]\tLoss: 398.5822\n",
      "Training Epoch: 23 [20928/49669]\tLoss: 417.7924\n",
      "Training Epoch: 23 [20992/49669]\tLoss: 418.9990\n",
      "Training Epoch: 23 [21056/49669]\tLoss: 392.0375\n",
      "Training Epoch: 23 [21120/49669]\tLoss: 407.8932\n",
      "Training Epoch: 23 [21184/49669]\tLoss: 426.4136\n",
      "Training Epoch: 23 [21248/49669]\tLoss: 398.4164\n",
      "Training Epoch: 23 [21312/49669]\tLoss: 380.6357\n",
      "Training Epoch: 23 [21376/49669]\tLoss: 409.4161\n",
      "Training Epoch: 23 [21440/49669]\tLoss: 422.1211\n",
      "Training Epoch: 23 [21504/49669]\tLoss: 410.3660\n",
      "Training Epoch: 23 [21568/49669]\tLoss: 437.8045\n",
      "Training Epoch: 23 [21632/49669]\tLoss: 383.5953\n",
      "Training Epoch: 23 [21696/49669]\tLoss: 395.2423\n",
      "Training Epoch: 23 [21760/49669]\tLoss: 407.8495\n",
      "Training Epoch: 23 [21824/49669]\tLoss: 451.0997\n",
      "Training Epoch: 23 [21888/49669]\tLoss: 373.6007\n",
      "Training Epoch: 23 [21952/49669]\tLoss: 395.0760\n",
      "Training Epoch: 23 [22016/49669]\tLoss: 409.0655\n",
      "Training Epoch: 23 [22080/49669]\tLoss: 411.4527\n",
      "Training Epoch: 23 [22144/49669]\tLoss: 432.7742\n",
      "Training Epoch: 23 [22208/49669]\tLoss: 415.2122\n",
      "Training Epoch: 23 [22272/49669]\tLoss: 430.2866\n",
      "Training Epoch: 23 [22336/49669]\tLoss: 412.8416\n",
      "Training Epoch: 23 [22400/49669]\tLoss: 437.7109\n",
      "Training Epoch: 23 [22464/49669]\tLoss: 427.9541\n",
      "Training Epoch: 23 [22528/49669]\tLoss: 422.2045\n",
      "Training Epoch: 23 [22592/49669]\tLoss: 426.9442\n",
      "Training Epoch: 23 [22656/49669]\tLoss: 428.2074\n",
      "Training Epoch: 23 [22720/49669]\tLoss: 441.0751\n",
      "Training Epoch: 23 [22784/49669]\tLoss: 420.9839\n",
      "Training Epoch: 23 [22848/49669]\tLoss: 395.3914\n",
      "Training Epoch: 23 [22912/49669]\tLoss: 436.0524\n",
      "Training Epoch: 23 [22976/49669]\tLoss: 414.8510\n",
      "Training Epoch: 23 [23040/49669]\tLoss: 399.9540\n",
      "Training Epoch: 23 [23104/49669]\tLoss: 406.1746\n",
      "Training Epoch: 23 [23168/49669]\tLoss: 410.6169\n",
      "Training Epoch: 23 [23232/49669]\tLoss: 392.2974\n",
      "Training Epoch: 23 [23296/49669]\tLoss: 400.4939\n",
      "Training Epoch: 23 [23360/49669]\tLoss: 415.9865\n",
      "Training Epoch: 23 [23424/49669]\tLoss: 368.4048\n",
      "Training Epoch: 23 [23488/49669]\tLoss: 411.0344\n",
      "Training Epoch: 23 [23552/49669]\tLoss: 418.5181\n",
      "Training Epoch: 23 [23616/49669]\tLoss: 411.7706\n",
      "Training Epoch: 23 [23680/49669]\tLoss: 390.8791\n",
      "Training Epoch: 23 [23744/49669]\tLoss: 400.6446\n",
      "Training Epoch: 23 [23808/49669]\tLoss: 387.2339\n",
      "Training Epoch: 23 [23872/49669]\tLoss: 422.8498\n",
      "Training Epoch: 23 [23936/49669]\tLoss: 388.7468\n",
      "Training Epoch: 23 [24000/49669]\tLoss: 433.7399\n",
      "Training Epoch: 23 [24064/49669]\tLoss: 429.0343\n",
      "Training Epoch: 23 [24128/49669]\tLoss: 389.3329\n",
      "Training Epoch: 23 [24192/49669]\tLoss: 406.9291\n",
      "Training Epoch: 23 [24256/49669]\tLoss: 390.6277\n",
      "Training Epoch: 23 [24320/49669]\tLoss: 429.1719\n",
      "Training Epoch: 23 [24384/49669]\tLoss: 386.4608\n",
      "Training Epoch: 23 [24448/49669]\tLoss: 413.7555\n",
      "Training Epoch: 23 [24512/49669]\tLoss: 411.8260\n",
      "Training Epoch: 23 [24576/49669]\tLoss: 387.0387\n",
      "Training Epoch: 23 [24640/49669]\tLoss: 404.0549\n",
      "Training Epoch: 23 [24704/49669]\tLoss: 426.9360\n",
      "Training Epoch: 23 [24768/49669]\tLoss: 370.8221\n",
      "Training Epoch: 23 [24832/49669]\tLoss: 380.4394\n",
      "Training Epoch: 23 [24896/49669]\tLoss: 396.7091\n",
      "Training Epoch: 23 [24960/49669]\tLoss: 411.4274\n",
      "Training Epoch: 23 [25024/49669]\tLoss: 396.9576\n",
      "Training Epoch: 23 [25088/49669]\tLoss: 432.4182\n",
      "Training Epoch: 23 [25152/49669]\tLoss: 387.4949\n",
      "Training Epoch: 23 [25216/49669]\tLoss: 408.8955\n",
      "Training Epoch: 23 [25280/49669]\tLoss: 409.3545\n",
      "Training Epoch: 23 [25344/49669]\tLoss: 374.1054\n",
      "Training Epoch: 23 [25408/49669]\tLoss: 409.4147\n",
      "Training Epoch: 23 [25472/49669]\tLoss: 399.1681\n",
      "Training Epoch: 23 [25536/49669]\tLoss: 418.1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [25600/49669]\tLoss: 414.8225\n",
      "Training Epoch: 23 [25664/49669]\tLoss: 376.3900\n",
      "Training Epoch: 23 [25728/49669]\tLoss: 395.3961\n",
      "Training Epoch: 23 [25792/49669]\tLoss: 384.6262\n",
      "Training Epoch: 23 [25856/49669]\tLoss: 425.5306\n",
      "Training Epoch: 23 [25920/49669]\tLoss: 384.0384\n",
      "Training Epoch: 23 [25984/49669]\tLoss: 379.8508\n",
      "Training Epoch: 23 [26048/49669]\tLoss: 367.6135\n",
      "Training Epoch: 23 [26112/49669]\tLoss: 378.3516\n",
      "Training Epoch: 23 [26176/49669]\tLoss: 421.4711\n",
      "Training Epoch: 23 [26240/49669]\tLoss: 401.5453\n",
      "Training Epoch: 23 [26304/49669]\tLoss: 405.9258\n",
      "Training Epoch: 23 [26368/49669]\tLoss: 393.1161\n",
      "Training Epoch: 23 [26432/49669]\tLoss: 420.6623\n",
      "Training Epoch: 23 [26496/49669]\tLoss: 407.6362\n",
      "Training Epoch: 23 [26560/49669]\tLoss: 406.2411\n",
      "Training Epoch: 23 [26624/49669]\tLoss: 409.9786\n",
      "Training Epoch: 23 [26688/49669]\tLoss: 387.1064\n",
      "Training Epoch: 23 [26752/49669]\tLoss: 386.2631\n",
      "Training Epoch: 23 [26816/49669]\tLoss: 387.8586\n",
      "Training Epoch: 23 [26880/49669]\tLoss: 421.5041\n",
      "Training Epoch: 23 [26944/49669]\tLoss: 417.6021\n",
      "Training Epoch: 23 [27008/49669]\tLoss: 388.1116\n",
      "Training Epoch: 23 [27072/49669]\tLoss: 395.6228\n",
      "Training Epoch: 23 [27136/49669]\tLoss: 424.4825\n",
      "Training Epoch: 23 [27200/49669]\tLoss: 418.4825\n",
      "Training Epoch: 23 [27264/49669]\tLoss: 415.7963\n",
      "Training Epoch: 23 [27328/49669]\tLoss: 429.8233\n",
      "Training Epoch: 23 [27392/49669]\tLoss: 389.0256\n",
      "Training Epoch: 23 [27456/49669]\tLoss: 403.4901\n",
      "Training Epoch: 23 [27520/49669]\tLoss: 407.3722\n",
      "Training Epoch: 23 [27584/49669]\tLoss: 406.5309\n",
      "Training Epoch: 23 [27648/49669]\tLoss: 388.9724\n",
      "Training Epoch: 23 [27712/49669]\tLoss: 423.0792\n",
      "Training Epoch: 23 [27776/49669]\tLoss: 430.1375\n",
      "Training Epoch: 23 [27840/49669]\tLoss: 400.3568\n",
      "Training Epoch: 23 [27904/49669]\tLoss: 386.8046\n",
      "Training Epoch: 23 [27968/49669]\tLoss: 407.9433\n",
      "Training Epoch: 23 [28032/49669]\tLoss: 425.9172\n",
      "Training Epoch: 23 [28096/49669]\tLoss: 409.3079\n",
      "Training Epoch: 23 [28160/49669]\tLoss: 374.2622\n",
      "Training Epoch: 23 [28224/49669]\tLoss: 406.9247\n",
      "Training Epoch: 23 [28288/49669]\tLoss: 389.8372\n",
      "Training Epoch: 23 [28352/49669]\tLoss: 413.3355\n",
      "Training Epoch: 23 [28416/49669]\tLoss: 376.1514\n",
      "Training Epoch: 23 [28480/49669]\tLoss: 427.5916\n",
      "Training Epoch: 23 [28544/49669]\tLoss: 408.2884\n",
      "Training Epoch: 23 [28608/49669]\tLoss: 413.8873\n",
      "Training Epoch: 23 [28672/49669]\tLoss: 403.8561\n",
      "Training Epoch: 23 [28736/49669]\tLoss: 390.6341\n",
      "Training Epoch: 23 [28800/49669]\tLoss: 410.2973\n",
      "Training Epoch: 23 [28864/49669]\tLoss: 385.0715\n",
      "Training Epoch: 23 [28928/49669]\tLoss: 411.8318\n",
      "Training Epoch: 23 [28992/49669]\tLoss: 423.4712\n",
      "Training Epoch: 23 [29056/49669]\tLoss: 404.6149\n",
      "Training Epoch: 23 [29120/49669]\tLoss: 400.3600\n",
      "Training Epoch: 23 [29184/49669]\tLoss: 375.7325\n",
      "Training Epoch: 23 [29248/49669]\tLoss: 406.4128\n",
      "Training Epoch: 23 [29312/49669]\tLoss: 423.7596\n",
      "Training Epoch: 23 [29376/49669]\tLoss: 386.8631\n",
      "Training Epoch: 23 [29440/49669]\tLoss: 404.0389\n",
      "Training Epoch: 23 [29504/49669]\tLoss: 404.2953\n",
      "Training Epoch: 23 [29568/49669]\tLoss: 378.9899\n",
      "Training Epoch: 23 [29632/49669]\tLoss: 390.2797\n",
      "Training Epoch: 23 [29696/49669]\tLoss: 402.4908\n",
      "Training Epoch: 23 [29760/49669]\tLoss: 406.0412\n",
      "Training Epoch: 23 [29824/49669]\tLoss: 443.3029\n",
      "Training Epoch: 23 [29888/49669]\tLoss: 383.5269\n",
      "Training Epoch: 23 [29952/49669]\tLoss: 401.0469\n",
      "Training Epoch: 23 [30016/49669]\tLoss: 393.3022\n",
      "Training Epoch: 23 [30080/49669]\tLoss: 363.9486\n",
      "Training Epoch: 23 [30144/49669]\tLoss: 367.0603\n",
      "Training Epoch: 23 [30208/49669]\tLoss: 426.0857\n",
      "Training Epoch: 23 [30272/49669]\tLoss: 425.3511\n",
      "Training Epoch: 23 [30336/49669]\tLoss: 390.7025\n",
      "Training Epoch: 23 [30400/49669]\tLoss: 386.2623\n",
      "Training Epoch: 23 [30464/49669]\tLoss: 399.1618\n",
      "Training Epoch: 23 [30528/49669]\tLoss: 423.7759\n",
      "Training Epoch: 23 [30592/49669]\tLoss: 405.3461\n",
      "Training Epoch: 23 [30656/49669]\tLoss: 425.5367\n",
      "Training Epoch: 23 [30720/49669]\tLoss: 395.4907\n",
      "Training Epoch: 23 [30784/49669]\tLoss: 377.7322\n",
      "Training Epoch: 23 [30848/49669]\tLoss: 407.3856\n",
      "Training Epoch: 23 [30912/49669]\tLoss: 388.1448\n",
      "Training Epoch: 23 [30976/49669]\tLoss: 381.4179\n",
      "Training Epoch: 23 [31040/49669]\tLoss: 373.6024\n",
      "Training Epoch: 23 [31104/49669]\tLoss: 413.0958\n",
      "Training Epoch: 23 [31168/49669]\tLoss: 425.3594\n",
      "Training Epoch: 23 [31232/49669]\tLoss: 407.5505\n",
      "Training Epoch: 23 [31296/49669]\tLoss: 420.9371\n",
      "Training Epoch: 23 [31360/49669]\tLoss: 419.8949\n",
      "Training Epoch: 23 [31424/49669]\tLoss: 388.3369\n",
      "Training Epoch: 23 [31488/49669]\tLoss: 391.7627\n",
      "Training Epoch: 23 [31552/49669]\tLoss: 382.8083\n",
      "Training Epoch: 23 [31616/49669]\tLoss: 388.9556\n",
      "Training Epoch: 23 [31680/49669]\tLoss: 427.2708\n",
      "Training Epoch: 23 [31744/49669]\tLoss: 433.9216\n",
      "Training Epoch: 23 [31808/49669]\tLoss: 390.9345\n",
      "Training Epoch: 23 [31872/49669]\tLoss: 408.7242\n",
      "Training Epoch: 23 [31936/49669]\tLoss: 408.8634\n",
      "Training Epoch: 23 [32000/49669]\tLoss: 420.1869\n",
      "Training Epoch: 23 [32064/49669]\tLoss: 414.6027\n",
      "Training Epoch: 23 [32128/49669]\tLoss: 409.2143\n",
      "Training Epoch: 23 [32192/49669]\tLoss: 423.8933\n",
      "Training Epoch: 23 [32256/49669]\tLoss: 408.3850\n",
      "Training Epoch: 23 [32320/49669]\tLoss: 401.3648\n",
      "Training Epoch: 23 [32384/49669]\tLoss: 406.5448\n",
      "Training Epoch: 23 [32448/49669]\tLoss: 383.8653\n",
      "Training Epoch: 23 [32512/49669]\tLoss: 414.0064\n",
      "Training Epoch: 23 [32576/49669]\tLoss: 412.1997\n",
      "Training Epoch: 23 [32640/49669]\tLoss: 372.9381\n",
      "Training Epoch: 23 [32704/49669]\tLoss: 398.7598\n",
      "Training Epoch: 23 [32768/49669]\tLoss: 396.5135\n",
      "Training Epoch: 23 [32832/49669]\tLoss: 412.6766\n",
      "Training Epoch: 23 [32896/49669]\tLoss: 388.4396\n",
      "Training Epoch: 23 [32960/49669]\tLoss: 400.4880\n",
      "Training Epoch: 23 [33024/49669]\tLoss: 413.0744\n",
      "Training Epoch: 23 [33088/49669]\tLoss: 426.3521\n",
      "Training Epoch: 23 [33152/49669]\tLoss: 405.1649\n",
      "Training Epoch: 23 [33216/49669]\tLoss: 410.1051\n",
      "Training Epoch: 23 [33280/49669]\tLoss: 424.7240\n",
      "Training Epoch: 23 [33344/49669]\tLoss: 408.1981\n",
      "Training Epoch: 23 [33408/49669]\tLoss: 382.2390\n",
      "Training Epoch: 23 [33472/49669]\tLoss: 391.7398\n",
      "Training Epoch: 23 [33536/49669]\tLoss: 404.2215\n",
      "Training Epoch: 23 [33600/49669]\tLoss: 392.1723\n",
      "Training Epoch: 23 [33664/49669]\tLoss: 391.4757\n",
      "Training Epoch: 23 [33728/49669]\tLoss: 387.5332\n",
      "Training Epoch: 23 [33792/49669]\tLoss: 437.1415\n",
      "Training Epoch: 23 [33856/49669]\tLoss: 382.3904\n",
      "Training Epoch: 23 [33920/49669]\tLoss: 390.0663\n",
      "Training Epoch: 23 [33984/49669]\tLoss: 410.3580\n",
      "Training Epoch: 23 [34048/49669]\tLoss: 403.4504\n",
      "Training Epoch: 23 [34112/49669]\tLoss: 375.4995\n",
      "Training Epoch: 23 [34176/49669]\tLoss: 404.2858\n",
      "Training Epoch: 23 [34240/49669]\tLoss: 387.7081\n",
      "Training Epoch: 23 [34304/49669]\tLoss: 426.9230\n",
      "Training Epoch: 23 [34368/49669]\tLoss: 390.1268\n",
      "Training Epoch: 23 [34432/49669]\tLoss: 401.1296\n",
      "Training Epoch: 23 [34496/49669]\tLoss: 416.5067\n",
      "Training Epoch: 23 [34560/49669]\tLoss: 391.2558\n",
      "Training Epoch: 23 [34624/49669]\tLoss: 389.1065\n",
      "Training Epoch: 23 [34688/49669]\tLoss: 382.4182\n",
      "Training Epoch: 23 [34752/49669]\tLoss: 403.3053\n",
      "Training Epoch: 23 [34816/49669]\tLoss: 370.3428\n",
      "Training Epoch: 23 [34880/49669]\tLoss: 398.3191\n",
      "Training Epoch: 23 [34944/49669]\tLoss: 382.8781\n",
      "Training Epoch: 23 [35008/49669]\tLoss: 396.1664\n",
      "Training Epoch: 23 [35072/49669]\tLoss: 390.4118\n",
      "Training Epoch: 23 [35136/49669]\tLoss: 428.0577\n",
      "Training Epoch: 23 [35200/49669]\tLoss: 408.3669\n",
      "Training Epoch: 23 [35264/49669]\tLoss: 389.4073\n",
      "Training Epoch: 23 [35328/49669]\tLoss: 403.6200\n",
      "Training Epoch: 23 [35392/49669]\tLoss: 411.3976\n",
      "Training Epoch: 23 [35456/49669]\tLoss: 390.0813\n",
      "Training Epoch: 23 [35520/49669]\tLoss: 400.8334\n",
      "Training Epoch: 23 [35584/49669]\tLoss: 425.4252\n",
      "Training Epoch: 23 [35648/49669]\tLoss: 396.4778\n",
      "Training Epoch: 23 [35712/49669]\tLoss: 400.6299\n",
      "Training Epoch: 23 [35776/49669]\tLoss: 389.8014\n",
      "Training Epoch: 23 [35840/49669]\tLoss: 417.0831\n",
      "Training Epoch: 23 [35904/49669]\tLoss: 378.8722\n",
      "Training Epoch: 23 [35968/49669]\tLoss: 407.7671\n",
      "Training Epoch: 23 [36032/49669]\tLoss: 398.7858\n",
      "Training Epoch: 23 [36096/49669]\tLoss: 368.8746\n",
      "Training Epoch: 23 [36160/49669]\tLoss: 401.6408\n",
      "Training Epoch: 23 [36224/49669]\tLoss: 402.7766\n",
      "Training Epoch: 23 [36288/49669]\tLoss: 427.4904\n",
      "Training Epoch: 23 [36352/49669]\tLoss: 401.6862\n",
      "Training Epoch: 23 [36416/49669]\tLoss: 414.0731\n",
      "Training Epoch: 23 [36480/49669]\tLoss: 409.3943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [36544/49669]\tLoss: 404.9538\n",
      "Training Epoch: 23 [36608/49669]\tLoss: 412.0556\n",
      "Training Epoch: 23 [36672/49669]\tLoss: 399.3841\n",
      "Training Epoch: 23 [36736/49669]\tLoss: 392.8757\n",
      "Training Epoch: 23 [36800/49669]\tLoss: 405.1981\n",
      "Training Epoch: 23 [36864/49669]\tLoss: 408.7009\n",
      "Training Epoch: 23 [36928/49669]\tLoss: 418.9909\n",
      "Training Epoch: 23 [36992/49669]\tLoss: 382.5835\n",
      "Training Epoch: 23 [37056/49669]\tLoss: 406.3357\n",
      "Training Epoch: 23 [37120/49669]\tLoss: 408.4320\n",
      "Training Epoch: 23 [37184/49669]\tLoss: 407.4476\n",
      "Training Epoch: 23 [37248/49669]\tLoss: 421.0497\n",
      "Training Epoch: 23 [37312/49669]\tLoss: 385.8502\n",
      "Training Epoch: 23 [37376/49669]\tLoss: 428.0545\n",
      "Training Epoch: 23 [37440/49669]\tLoss: 404.0233\n",
      "Training Epoch: 23 [37504/49669]\tLoss: 382.0139\n",
      "Training Epoch: 23 [37568/49669]\tLoss: 394.7149\n",
      "Training Epoch: 23 [37632/49669]\tLoss: 409.8567\n",
      "Training Epoch: 23 [37696/49669]\tLoss: 426.4174\n",
      "Training Epoch: 23 [37760/49669]\tLoss: 386.7541\n",
      "Training Epoch: 23 [37824/49669]\tLoss: 428.0649\n",
      "Training Epoch: 23 [37888/49669]\tLoss: 405.6060\n",
      "Training Epoch: 23 [37952/49669]\tLoss: 402.5585\n",
      "Training Epoch: 23 [38016/49669]\tLoss: 365.2756\n",
      "Training Epoch: 23 [38080/49669]\tLoss: 425.8926\n",
      "Training Epoch: 23 [38144/49669]\tLoss: 401.6899\n",
      "Training Epoch: 23 [38208/49669]\tLoss: 405.3489\n",
      "Training Epoch: 23 [38272/49669]\tLoss: 431.4341\n",
      "Training Epoch: 23 [38336/49669]\tLoss: 426.8059\n",
      "Training Epoch: 23 [38400/49669]\tLoss: 428.2843\n",
      "Training Epoch: 23 [38464/49669]\tLoss: 422.4413\n",
      "Training Epoch: 23 [38528/49669]\tLoss: 441.7524\n",
      "Training Epoch: 23 [38592/49669]\tLoss: 448.9407\n",
      "Training Epoch: 23 [38656/49669]\tLoss: 430.7077\n",
      "Training Epoch: 23 [38720/49669]\tLoss: 445.9146\n",
      "Training Epoch: 23 [38784/49669]\tLoss: 454.5609\n",
      "Training Epoch: 23 [38848/49669]\tLoss: 435.2157\n",
      "Training Epoch: 23 [38912/49669]\tLoss: 406.9943\n",
      "Training Epoch: 23 [38976/49669]\tLoss: 401.3737\n",
      "Training Epoch: 23 [39040/49669]\tLoss: 435.2316\n",
      "Training Epoch: 23 [39104/49669]\tLoss: 471.2307\n",
      "Training Epoch: 23 [39168/49669]\tLoss: 452.1155\n",
      "Training Epoch: 23 [39232/49669]\tLoss: 429.7562\n",
      "Training Epoch: 23 [39296/49669]\tLoss: 422.0057\n",
      "Training Epoch: 23 [39360/49669]\tLoss: 388.7762\n",
      "Training Epoch: 23 [39424/49669]\tLoss: 417.5692\n",
      "Training Epoch: 23 [39488/49669]\tLoss: 446.1984\n",
      "Training Epoch: 23 [39552/49669]\tLoss: 483.3526\n",
      "Training Epoch: 23 [39616/49669]\tLoss: 472.0383\n",
      "Training Epoch: 23 [39680/49669]\tLoss: 473.8552\n",
      "Training Epoch: 23 [39744/49669]\tLoss: 394.4020\n",
      "Training Epoch: 23 [39808/49669]\tLoss: 399.7954\n",
      "Training Epoch: 23 [39872/49669]\tLoss: 407.1117\n",
      "Training Epoch: 23 [39936/49669]\tLoss: 458.9408\n",
      "Training Epoch: 23 [40000/49669]\tLoss: 439.0363\n",
      "Training Epoch: 23 [40064/49669]\tLoss: 404.5900\n",
      "Training Epoch: 23 [40128/49669]\tLoss: 396.4090\n",
      "Training Epoch: 23 [40192/49669]\tLoss: 408.8308\n",
      "Training Epoch: 23 [40256/49669]\tLoss: 433.8464\n",
      "Training Epoch: 23 [40320/49669]\tLoss: 439.6459\n",
      "Training Epoch: 23 [40384/49669]\tLoss: 427.1550\n",
      "Training Epoch: 23 [40448/49669]\tLoss: 408.3920\n",
      "Training Epoch: 23 [40512/49669]\tLoss: 424.5152\n",
      "Training Epoch: 23 [40576/49669]\tLoss: 426.4050\n",
      "Training Epoch: 23 [40640/49669]\tLoss: 408.9661\n",
      "Training Epoch: 23 [40704/49669]\tLoss: 424.4049\n",
      "Training Epoch: 23 [40768/49669]\tLoss: 390.7421\n",
      "Training Epoch: 23 [40832/49669]\tLoss: 429.7795\n",
      "Training Epoch: 23 [40896/49669]\tLoss: 413.2776\n",
      "Training Epoch: 23 [40960/49669]\tLoss: 448.5349\n",
      "Training Epoch: 23 [41024/49669]\tLoss: 432.6341\n",
      "Training Epoch: 23 [41088/49669]\tLoss: 422.0302\n",
      "Training Epoch: 23 [41152/49669]\tLoss: 417.5835\n",
      "Training Epoch: 23 [41216/49669]\tLoss: 417.5367\n",
      "Training Epoch: 23 [41280/49669]\tLoss: 408.2408\n",
      "Training Epoch: 23 [41344/49669]\tLoss: 414.3820\n",
      "Training Epoch: 23 [41408/49669]\tLoss: 377.7270\n",
      "Training Epoch: 23 [41472/49669]\tLoss: 421.6554\n",
      "Training Epoch: 23 [41536/49669]\tLoss: 408.0504\n",
      "Training Epoch: 23 [41600/49669]\tLoss: 366.1168\n",
      "Training Epoch: 23 [41664/49669]\tLoss: 439.2579\n",
      "Training Epoch: 23 [41728/49669]\tLoss: 424.5767\n",
      "Training Epoch: 23 [41792/49669]\tLoss: 401.9379\n",
      "Training Epoch: 23 [41856/49669]\tLoss: 398.5328\n",
      "Training Epoch: 23 [41920/49669]\tLoss: 415.5252\n",
      "Training Epoch: 23 [41984/49669]\tLoss: 413.9549\n",
      "Training Epoch: 23 [42048/49669]\tLoss: 395.0497\n",
      "Training Epoch: 23 [42112/49669]\tLoss: 370.6986\n",
      "Training Epoch: 23 [42176/49669]\tLoss: 401.6281\n",
      "Training Epoch: 23 [42240/49669]\tLoss: 375.5145\n",
      "Training Epoch: 23 [42304/49669]\tLoss: 390.0488\n",
      "Training Epoch: 23 [42368/49669]\tLoss: 395.7457\n",
      "Training Epoch: 23 [42432/49669]\tLoss: 421.5709\n",
      "Training Epoch: 23 [42496/49669]\tLoss: 371.7113\n",
      "Training Epoch: 23 [42560/49669]\tLoss: 372.4565\n",
      "Training Epoch: 23 [42624/49669]\tLoss: 382.8307\n",
      "Training Epoch: 23 [42688/49669]\tLoss: 417.6499\n",
      "Training Epoch: 23 [42752/49669]\tLoss: 412.6204\n",
      "Training Epoch: 23 [42816/49669]\tLoss: 413.0397\n",
      "Training Epoch: 23 [42880/49669]\tLoss: 404.0108\n",
      "Training Epoch: 23 [42944/49669]\tLoss: 408.9451\n",
      "Training Epoch: 23 [43008/49669]\tLoss: 394.4169\n",
      "Training Epoch: 23 [43072/49669]\tLoss: 385.6367\n",
      "Training Epoch: 23 [43136/49669]\tLoss: 409.7639\n",
      "Training Epoch: 23 [43200/49669]\tLoss: 437.8759\n",
      "Training Epoch: 23 [43264/49669]\tLoss: 368.7648\n",
      "Training Epoch: 23 [43328/49669]\tLoss: 397.4256\n",
      "Training Epoch: 23 [43392/49669]\tLoss: 413.4479\n",
      "Training Epoch: 23 [43456/49669]\tLoss: 388.0803\n",
      "Training Epoch: 23 [43520/49669]\tLoss: 419.4327\n",
      "Training Epoch: 23 [43584/49669]\tLoss: 405.0839\n",
      "Training Epoch: 23 [43648/49669]\tLoss: 385.5437\n",
      "Training Epoch: 23 [43712/49669]\tLoss: 404.2341\n",
      "Training Epoch: 23 [43776/49669]\tLoss: 429.1188\n",
      "Training Epoch: 23 [43840/49669]\tLoss: 431.9150\n",
      "Training Epoch: 23 [43904/49669]\tLoss: 389.2938\n",
      "Training Epoch: 23 [43968/49669]\tLoss: 394.3661\n",
      "Training Epoch: 23 [44032/49669]\tLoss: 394.6567\n",
      "Training Epoch: 23 [44096/49669]\tLoss: 408.7074\n",
      "Training Epoch: 23 [44160/49669]\tLoss: 416.2010\n",
      "Training Epoch: 23 [44224/49669]\tLoss: 409.0708\n",
      "Training Epoch: 23 [44288/49669]\tLoss: 412.5352\n",
      "Training Epoch: 23 [44352/49669]\tLoss: 395.5315\n",
      "Training Epoch: 23 [44416/49669]\tLoss: 401.5157\n",
      "Training Epoch: 23 [44480/49669]\tLoss: 403.7918\n",
      "Training Epoch: 23 [44544/49669]\tLoss: 367.0565\n",
      "Training Epoch: 23 [44608/49669]\tLoss: 387.4983\n",
      "Training Epoch: 23 [44672/49669]\tLoss: 387.8622\n",
      "Training Epoch: 23 [44736/49669]\tLoss: 377.8338\n",
      "Training Epoch: 23 [44800/49669]\tLoss: 419.3314\n",
      "Training Epoch: 23 [44864/49669]\tLoss: 404.7979\n",
      "Training Epoch: 23 [44928/49669]\tLoss: 396.0056\n",
      "Training Epoch: 23 [44992/49669]\tLoss: 391.1444\n",
      "Training Epoch: 23 [45056/49669]\tLoss: 380.7328\n",
      "Training Epoch: 23 [45120/49669]\tLoss: 394.0341\n",
      "Training Epoch: 23 [45184/49669]\tLoss: 361.2412\n",
      "Training Epoch: 23 [45248/49669]\tLoss: 388.4435\n",
      "Training Epoch: 23 [45312/49669]\tLoss: 404.8355\n",
      "Training Epoch: 23 [45376/49669]\tLoss: 405.8313\n",
      "Training Epoch: 23 [45440/49669]\tLoss: 367.9590\n",
      "Training Epoch: 23 [45504/49669]\tLoss: 363.7668\n",
      "Training Epoch: 23 [45568/49669]\tLoss: 392.5083\n",
      "Training Epoch: 23 [45632/49669]\tLoss: 371.7299\n",
      "Training Epoch: 23 [45696/49669]\tLoss: 420.5095\n",
      "Training Epoch: 23 [45760/49669]\tLoss: 405.5037\n",
      "Training Epoch: 23 [45824/49669]\tLoss: 416.0128\n",
      "Training Epoch: 23 [45888/49669]\tLoss: 375.1662\n",
      "Training Epoch: 23 [45952/49669]\tLoss: 408.8771\n",
      "Training Epoch: 23 [46016/49669]\tLoss: 396.7897\n",
      "Training Epoch: 23 [46080/49669]\tLoss: 410.9399\n",
      "Training Epoch: 23 [46144/49669]\tLoss: 354.7210\n",
      "Training Epoch: 23 [46208/49669]\tLoss: 389.2570\n",
      "Training Epoch: 23 [46272/49669]\tLoss: 384.9020\n",
      "Training Epoch: 23 [46336/49669]\tLoss: 417.0996\n",
      "Training Epoch: 23 [46400/49669]\tLoss: 427.0881\n",
      "Training Epoch: 23 [46464/49669]\tLoss: 390.7175\n",
      "Training Epoch: 23 [46528/49669]\tLoss: 402.0749\n",
      "Training Epoch: 23 [46592/49669]\tLoss: 368.0932\n",
      "Training Epoch: 23 [46656/49669]\tLoss: 393.6779\n",
      "Training Epoch: 23 [46720/49669]\tLoss: 415.9980\n",
      "Training Epoch: 23 [46784/49669]\tLoss: 401.7789\n",
      "Training Epoch: 23 [46848/49669]\tLoss: 385.5331\n",
      "Training Epoch: 23 [46912/49669]\tLoss: 387.0285\n",
      "Training Epoch: 23 [46976/49669]\tLoss: 409.9375\n",
      "Training Epoch: 23 [47040/49669]\tLoss: 416.2057\n",
      "Training Epoch: 23 [47104/49669]\tLoss: 407.4639\n",
      "Training Epoch: 23 [47168/49669]\tLoss: 395.9328\n",
      "Training Epoch: 23 [47232/49669]\tLoss: 410.9978\n",
      "Training Epoch: 23 [47296/49669]\tLoss: 388.1816\n",
      "Training Epoch: 23 [47360/49669]\tLoss: 399.0189\n",
      "Training Epoch: 23 [47424/49669]\tLoss: 422.5965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [47488/49669]\tLoss: 411.2242\n",
      "Training Epoch: 23 [47552/49669]\tLoss: 381.8778\n",
      "Training Epoch: 23 [47616/49669]\tLoss: 393.5871\n",
      "Training Epoch: 23 [47680/49669]\tLoss: 398.0908\n",
      "Training Epoch: 23 [47744/49669]\tLoss: 409.9665\n",
      "Training Epoch: 23 [47808/49669]\tLoss: 394.1202\n",
      "Training Epoch: 23 [47872/49669]\tLoss: 408.7920\n",
      "Training Epoch: 23 [47936/49669]\tLoss: 408.9019\n",
      "Training Epoch: 23 [48000/49669]\tLoss: 409.0385\n",
      "Training Epoch: 23 [48064/49669]\tLoss: 400.9005\n",
      "Training Epoch: 23 [48128/49669]\tLoss: 394.0045\n",
      "Training Epoch: 23 [48192/49669]\tLoss: 411.3717\n",
      "Training Epoch: 23 [48256/49669]\tLoss: 425.3026\n",
      "Training Epoch: 23 [48320/49669]\tLoss: 418.7516\n",
      "Training Epoch: 23 [48384/49669]\tLoss: 387.5173\n",
      "Training Epoch: 23 [48448/49669]\tLoss: 436.7353\n",
      "Training Epoch: 23 [48512/49669]\tLoss: 384.5312\n",
      "Training Epoch: 23 [48576/49669]\tLoss: 381.7845\n",
      "Training Epoch: 23 [48640/49669]\tLoss: 415.7932\n",
      "Training Epoch: 23 [48704/49669]\tLoss: 398.0587\n",
      "Training Epoch: 23 [48768/49669]\tLoss: 384.9935\n",
      "Training Epoch: 23 [48832/49669]\tLoss: 418.6129\n",
      "Training Epoch: 23 [48896/49669]\tLoss: 401.3719\n",
      "Training Epoch: 23 [48960/49669]\tLoss: 390.7495\n",
      "Training Epoch: 23 [49024/49669]\tLoss: 389.1915\n",
      "Training Epoch: 23 [49088/49669]\tLoss: 371.2482\n",
      "Training Epoch: 23 [49152/49669]\tLoss: 397.0426\n",
      "Training Epoch: 23 [49216/49669]\tLoss: 433.8983\n",
      "Training Epoch: 23 [49280/49669]\tLoss: 416.1738\n",
      "Training Epoch: 23 [49344/49669]\tLoss: 434.1271\n",
      "Training Epoch: 23 [49408/49669]\tLoss: 407.0377\n",
      "Training Epoch: 23 [49472/49669]\tLoss: 403.1120\n",
      "Training Epoch: 23 [49536/49669]\tLoss: 390.2694\n",
      "Training Epoch: 23 [49600/49669]\tLoss: 376.3960\n",
      "Training Epoch: 23 [49664/49669]\tLoss: 387.6805\n",
      "Training Epoch: 23 [49669/49669]\tLoss: 403.9127\n",
      "Training Epoch: 23 [5519/5519]\tLoss: 403.9435\n",
      "Training Epoch: 24 [64/49669]\tLoss: 377.0991\n",
      "Training Epoch: 24 [128/49669]\tLoss: 394.6250\n",
      "Training Epoch: 24 [192/49669]\tLoss: 424.7780\n",
      "Training Epoch: 24 [256/49669]\tLoss: 419.2899\n",
      "Training Epoch: 24 [320/49669]\tLoss: 416.0713\n",
      "Training Epoch: 24 [384/49669]\tLoss: 394.0757\n",
      "Training Epoch: 24 [448/49669]\tLoss: 406.2949\n",
      "Training Epoch: 24 [512/49669]\tLoss: 408.0883\n",
      "Training Epoch: 24 [576/49669]\tLoss: 400.6673\n",
      "Training Epoch: 24 [640/49669]\tLoss: 400.8579\n",
      "Training Epoch: 24 [704/49669]\tLoss: 394.8781\n",
      "Training Epoch: 24 [768/49669]\tLoss: 394.1224\n",
      "Training Epoch: 24 [832/49669]\tLoss: 435.7476\n",
      "Training Epoch: 24 [896/49669]\tLoss: 409.1518\n",
      "Training Epoch: 24 [960/49669]\tLoss: 390.2684\n",
      "Training Epoch: 24 [1024/49669]\tLoss: 395.7035\n",
      "Training Epoch: 24 [1088/49669]\tLoss: 348.2764\n",
      "Training Epoch: 24 [1152/49669]\tLoss: 401.5880\n",
      "Training Epoch: 24 [1216/49669]\tLoss: 380.6126\n",
      "Training Epoch: 24 [1280/49669]\tLoss: 398.3984\n",
      "Training Epoch: 24 [1344/49669]\tLoss: 409.9822\n",
      "Training Epoch: 24 [1408/49669]\tLoss: 403.6248\n",
      "Training Epoch: 24 [1472/49669]\tLoss: 428.8203\n",
      "Training Epoch: 24 [1536/49669]\tLoss: 421.2326\n",
      "Training Epoch: 24 [1600/49669]\tLoss: 398.0153\n",
      "Training Epoch: 24 [1664/49669]\tLoss: 399.4688\n",
      "Training Epoch: 24 [1728/49669]\tLoss: 401.9948\n",
      "Training Epoch: 24 [1792/49669]\tLoss: 373.6862\n",
      "Training Epoch: 24 [1856/49669]\tLoss: 412.1302\n",
      "Training Epoch: 24 [1920/49669]\tLoss: 439.9630\n",
      "Training Epoch: 24 [1984/49669]\tLoss: 407.0590\n",
      "Training Epoch: 24 [2048/49669]\tLoss: 433.0715\n",
      "Training Epoch: 24 [2112/49669]\tLoss: 425.0587\n",
      "Training Epoch: 24 [2176/49669]\tLoss: 440.2344\n",
      "Training Epoch: 24 [2240/49669]\tLoss: 406.0941\n",
      "Training Epoch: 24 [2304/49669]\tLoss: 369.0516\n",
      "Training Epoch: 24 [2368/49669]\tLoss: 403.7921\n",
      "Training Epoch: 24 [2432/49669]\tLoss: 388.9210\n",
      "Training Epoch: 24 [2496/49669]\tLoss: 412.5157\n",
      "Training Epoch: 24 [2560/49669]\tLoss: 409.9840\n",
      "Training Epoch: 24 [2624/49669]\tLoss: 371.7119\n",
      "Training Epoch: 24 [2688/49669]\tLoss: 426.6109\n",
      "Training Epoch: 24 [2752/49669]\tLoss: 397.4876\n",
      "Training Epoch: 24 [2816/49669]\tLoss: 409.0280\n",
      "Training Epoch: 24 [2880/49669]\tLoss: 411.7075\n",
      "Training Epoch: 24 [2944/49669]\tLoss: 415.4927\n",
      "Training Epoch: 24 [3008/49669]\tLoss: 403.9935\n",
      "Training Epoch: 24 [3072/49669]\tLoss: 406.5052\n",
      "Training Epoch: 24 [3136/49669]\tLoss: 393.2330\n",
      "Training Epoch: 24 [3200/49669]\tLoss: 402.7762\n",
      "Training Epoch: 24 [3264/49669]\tLoss: 411.7585\n",
      "Training Epoch: 24 [3328/49669]\tLoss: 403.4738\n",
      "Training Epoch: 24 [3392/49669]\tLoss: 405.5850\n",
      "Training Epoch: 24 [3456/49669]\tLoss: 368.1691\n",
      "Training Epoch: 24 [3520/49669]\tLoss: 406.9106\n",
      "Training Epoch: 24 [3584/49669]\tLoss: 421.9803\n",
      "Training Epoch: 24 [3648/49669]\tLoss: 374.5472\n",
      "Training Epoch: 24 [3712/49669]\tLoss: 396.9366\n",
      "Training Epoch: 24 [3776/49669]\tLoss: 385.8161\n",
      "Training Epoch: 24 [3840/49669]\tLoss: 375.8024\n",
      "Training Epoch: 24 [3904/49669]\tLoss: 428.0532\n",
      "Training Epoch: 24 [3968/49669]\tLoss: 394.8892\n",
      "Training Epoch: 24 [4032/49669]\tLoss: 432.1588\n",
      "Training Epoch: 24 [4096/49669]\tLoss: 397.4117\n",
      "Training Epoch: 24 [4160/49669]\tLoss: 411.2418\n",
      "Training Epoch: 24 [4224/49669]\tLoss: 422.0616\n",
      "Training Epoch: 24 [4288/49669]\tLoss: 412.6824\n",
      "Training Epoch: 24 [4352/49669]\tLoss: 383.4990\n",
      "Training Epoch: 24 [4416/49669]\tLoss: 393.6317\n",
      "Training Epoch: 24 [4480/49669]\tLoss: 435.5357\n",
      "Training Epoch: 24 [4544/49669]\tLoss: 395.0530\n",
      "Training Epoch: 24 [4608/49669]\tLoss: 399.3650\n",
      "Training Epoch: 24 [4672/49669]\tLoss: 418.6455\n",
      "Training Epoch: 24 [4736/49669]\tLoss: 405.6217\n",
      "Training Epoch: 24 [4800/49669]\tLoss: 396.2074\n",
      "Training Epoch: 24 [4864/49669]\tLoss: 377.8707\n",
      "Training Epoch: 24 [4928/49669]\tLoss: 425.8008\n",
      "Training Epoch: 24 [4992/49669]\tLoss: 398.5993\n",
      "Training Epoch: 24 [5056/49669]\tLoss: 382.0649\n",
      "Training Epoch: 24 [5120/49669]\tLoss: 426.6852\n",
      "Training Epoch: 24 [5184/49669]\tLoss: 415.4271\n",
      "Training Epoch: 24 [5248/49669]\tLoss: 409.1455\n",
      "Training Epoch: 24 [5312/49669]\tLoss: 392.8690\n",
      "Training Epoch: 24 [5376/49669]\tLoss: 405.9438\n",
      "Training Epoch: 24 [5440/49669]\tLoss: 379.0267\n",
      "Training Epoch: 24 [5504/49669]\tLoss: 406.5766\n",
      "Training Epoch: 24 [5568/49669]\tLoss: 389.0220\n",
      "Training Epoch: 24 [5632/49669]\tLoss: 374.5677\n",
      "Training Epoch: 24 [5696/49669]\tLoss: 412.1021\n",
      "Training Epoch: 24 [5760/49669]\tLoss: 422.0932\n",
      "Training Epoch: 24 [5824/49669]\tLoss: 393.1991\n",
      "Training Epoch: 24 [5888/49669]\tLoss: 401.9967\n",
      "Training Epoch: 24 [5952/49669]\tLoss: 382.5587\n",
      "Training Epoch: 24 [6016/49669]\tLoss: 425.8402\n",
      "Training Epoch: 24 [6080/49669]\tLoss: 405.0321\n",
      "Training Epoch: 24 [6144/49669]\tLoss: 415.5871\n",
      "Training Epoch: 24 [6208/49669]\tLoss: 402.5485\n",
      "Training Epoch: 24 [6272/49669]\tLoss: 392.4627\n",
      "Training Epoch: 24 [6336/49669]\tLoss: 380.6490\n",
      "Training Epoch: 24 [6400/49669]\tLoss: 391.6320\n",
      "Training Epoch: 24 [6464/49669]\tLoss: 408.1548\n",
      "Training Epoch: 24 [6528/49669]\tLoss: 409.9513\n",
      "Training Epoch: 24 [6592/49669]\tLoss: 406.2446\n",
      "Training Epoch: 24 [6656/49669]\tLoss: 410.6836\n",
      "Training Epoch: 24 [6720/49669]\tLoss: 396.9279\n",
      "Training Epoch: 24 [6784/49669]\tLoss: 397.9257\n",
      "Training Epoch: 24 [6848/49669]\tLoss: 394.2377\n",
      "Training Epoch: 24 [6912/49669]\tLoss: 402.1756\n",
      "Training Epoch: 24 [6976/49669]\tLoss: 396.9962\n",
      "Training Epoch: 24 [7040/49669]\tLoss: 413.4807\n",
      "Training Epoch: 24 [7104/49669]\tLoss: 401.8942\n",
      "Training Epoch: 24 [7168/49669]\tLoss: 390.3076\n",
      "Training Epoch: 24 [7232/49669]\tLoss: 415.0049\n",
      "Training Epoch: 24 [7296/49669]\tLoss: 406.8759\n",
      "Training Epoch: 24 [7360/49669]\tLoss: 412.8338\n",
      "Training Epoch: 24 [7424/49669]\tLoss: 412.1327\n",
      "Training Epoch: 24 [7488/49669]\tLoss: 407.0043\n",
      "Training Epoch: 24 [7552/49669]\tLoss: 397.6248\n",
      "Training Epoch: 24 [7616/49669]\tLoss: 375.4605\n",
      "Training Epoch: 24 [7680/49669]\tLoss: 397.1380\n",
      "Training Epoch: 24 [7744/49669]\tLoss: 402.4609\n",
      "Training Epoch: 24 [7808/49669]\tLoss: 416.5186\n",
      "Training Epoch: 24 [7872/49669]\tLoss: 405.6099\n",
      "Training Epoch: 24 [7936/49669]\tLoss: 419.9886\n",
      "Training Epoch: 24 [8000/49669]\tLoss: 388.4768\n",
      "Training Epoch: 24 [8064/49669]\tLoss: 382.6773\n",
      "Training Epoch: 24 [8128/49669]\tLoss: 395.4765\n",
      "Training Epoch: 24 [8192/49669]\tLoss: 372.0750\n",
      "Training Epoch: 24 [8256/49669]\tLoss: 383.0439\n",
      "Training Epoch: 24 [8320/49669]\tLoss: 397.2109\n",
      "Training Epoch: 24 [8384/49669]\tLoss: 403.6324\n",
      "Training Epoch: 24 [8448/49669]\tLoss: 414.4955\n",
      "Training Epoch: 24 [8512/49669]\tLoss: 419.4169\n",
      "Training Epoch: 24 [8576/49669]\tLoss: 392.8172\n",
      "Training Epoch: 24 [8640/49669]\tLoss: 402.9681\n",
      "Training Epoch: 24 [8704/49669]\tLoss: 389.3502\n",
      "Training Epoch: 24 [8768/49669]\tLoss: 410.6307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [8832/49669]\tLoss: 390.7788\n",
      "Training Epoch: 24 [8896/49669]\tLoss: 421.8170\n",
      "Training Epoch: 24 [8960/49669]\tLoss: 383.0626\n",
      "Training Epoch: 24 [9024/49669]\tLoss: 394.4865\n",
      "Training Epoch: 24 [9088/49669]\tLoss: 397.8147\n",
      "Training Epoch: 24 [9152/49669]\tLoss: 415.7311\n",
      "Training Epoch: 24 [9216/49669]\tLoss: 417.3168\n",
      "Training Epoch: 24 [9280/49669]\tLoss: 404.2358\n",
      "Training Epoch: 24 [9344/49669]\tLoss: 416.8894\n",
      "Training Epoch: 24 [9408/49669]\tLoss: 472.8110\n",
      "Training Epoch: 24 [9472/49669]\tLoss: 501.0491\n",
      "Training Epoch: 24 [9536/49669]\tLoss: 537.8757\n",
      "Training Epoch: 24 [9600/49669]\tLoss: 538.8417\n",
      "Training Epoch: 24 [9664/49669]\tLoss: 493.2600\n",
      "Training Epoch: 24 [9728/49669]\tLoss: 425.2687\n",
      "Training Epoch: 24 [9792/49669]\tLoss: 410.9515\n",
      "Training Epoch: 24 [9856/49669]\tLoss: 428.5863\n",
      "Training Epoch: 24 [9920/49669]\tLoss: 475.1828\n",
      "Training Epoch: 24 [9984/49669]\tLoss: 446.8755\n",
      "Training Epoch: 24 [10048/49669]\tLoss: 404.3378\n",
      "Training Epoch: 24 [10112/49669]\tLoss: 389.5614\n",
      "Training Epoch: 24 [10176/49669]\tLoss: 426.7932\n",
      "Training Epoch: 24 [10240/49669]\tLoss: 427.8925\n",
      "Training Epoch: 24 [10304/49669]\tLoss: 432.8984\n",
      "Training Epoch: 24 [10368/49669]\tLoss: 428.6374\n",
      "Training Epoch: 24 [10432/49669]\tLoss: 432.8393\n",
      "Training Epoch: 24 [10496/49669]\tLoss: 391.6890\n",
      "Training Epoch: 24 [10560/49669]\tLoss: 386.4516\n",
      "Training Epoch: 24 [10624/49669]\tLoss: 418.0832\n",
      "Training Epoch: 24 [10688/49669]\tLoss: 413.3251\n",
      "Training Epoch: 24 [10752/49669]\tLoss: 410.1104\n",
      "Training Epoch: 24 [10816/49669]\tLoss: 427.8926\n",
      "Training Epoch: 24 [10880/49669]\tLoss: 442.0027\n",
      "Training Epoch: 24 [10944/49669]\tLoss: 447.3892\n",
      "Training Epoch: 24 [11008/49669]\tLoss: 407.7949\n",
      "Training Epoch: 24 [11072/49669]\tLoss: 417.0043\n",
      "Training Epoch: 24 [11136/49669]\tLoss: 431.4638\n",
      "Training Epoch: 24 [11200/49669]\tLoss: 436.5403\n",
      "Training Epoch: 24 [11264/49669]\tLoss: 427.6926\n",
      "Training Epoch: 24 [11328/49669]\tLoss: 428.3205\n",
      "Training Epoch: 24 [11392/49669]\tLoss: 415.9577\n",
      "Training Epoch: 24 [11456/49669]\tLoss: 413.2938\n",
      "Training Epoch: 24 [11520/49669]\tLoss: 414.9075\n",
      "Training Epoch: 24 [11584/49669]\tLoss: 426.2429\n",
      "Training Epoch: 24 [11648/49669]\tLoss: 372.3390\n",
      "Training Epoch: 24 [11712/49669]\tLoss: 415.0291\n",
      "Training Epoch: 24 [11776/49669]\tLoss: 408.6097\n",
      "Training Epoch: 24 [11840/49669]\tLoss: 429.6220\n",
      "Training Epoch: 24 [11904/49669]\tLoss: 375.8378\n",
      "Training Epoch: 24 [11968/49669]\tLoss: 384.3110\n",
      "Training Epoch: 24 [12032/49669]\tLoss: 416.9357\n",
      "Training Epoch: 24 [12096/49669]\tLoss: 418.4581\n",
      "Training Epoch: 24 [12160/49669]\tLoss: 452.7817\n",
      "Training Epoch: 24 [12224/49669]\tLoss: 403.9600\n",
      "Training Epoch: 24 [12288/49669]\tLoss: 400.2813\n",
      "Training Epoch: 24 [12352/49669]\tLoss: 430.9369\n",
      "Training Epoch: 24 [12416/49669]\tLoss: 402.9334\n",
      "Training Epoch: 24 [12480/49669]\tLoss: 426.5418\n",
      "Training Epoch: 24 [12544/49669]\tLoss: 397.9762\n",
      "Training Epoch: 24 [12608/49669]\tLoss: 397.8788\n",
      "Training Epoch: 24 [12672/49669]\tLoss: 396.9754\n",
      "Training Epoch: 24 [12736/49669]\tLoss: 423.9857\n",
      "Training Epoch: 24 [12800/49669]\tLoss: 392.8755\n",
      "Training Epoch: 24 [12864/49669]\tLoss: 409.5022\n",
      "Training Epoch: 24 [12928/49669]\tLoss: 394.0170\n",
      "Training Epoch: 24 [12992/49669]\tLoss: 393.6838\n",
      "Training Epoch: 24 [13056/49669]\tLoss: 385.4868\n",
      "Training Epoch: 24 [13120/49669]\tLoss: 431.6545\n",
      "Training Epoch: 24 [13184/49669]\tLoss: 428.5701\n",
      "Training Epoch: 24 [13248/49669]\tLoss: 396.3510\n",
      "Training Epoch: 24 [13312/49669]\tLoss: 418.3099\n",
      "Training Epoch: 24 [13376/49669]\tLoss: 424.4690\n",
      "Training Epoch: 24 [13440/49669]\tLoss: 383.0567\n",
      "Training Epoch: 24 [13504/49669]\tLoss: 392.4790\n",
      "Training Epoch: 24 [13568/49669]\tLoss: 381.1729\n",
      "Training Epoch: 24 [13632/49669]\tLoss: 428.2516\n",
      "Training Epoch: 24 [13696/49669]\tLoss: 392.3434\n",
      "Training Epoch: 24 [13760/49669]\tLoss: 384.3342\n",
      "Training Epoch: 24 [13824/49669]\tLoss: 412.9460\n",
      "Training Epoch: 24 [13888/49669]\tLoss: 382.5289\n",
      "Training Epoch: 24 [13952/49669]\tLoss: 419.4236\n",
      "Training Epoch: 24 [14016/49669]\tLoss: 407.9160\n",
      "Training Epoch: 24 [14080/49669]\tLoss: 400.5349\n",
      "Training Epoch: 24 [14144/49669]\tLoss: 391.1878\n",
      "Training Epoch: 24 [14208/49669]\tLoss: 392.2337\n",
      "Training Epoch: 24 [14272/49669]\tLoss: 407.3943\n",
      "Training Epoch: 24 [14336/49669]\tLoss: 396.2554\n",
      "Training Epoch: 24 [14400/49669]\tLoss: 394.6866\n",
      "Training Epoch: 24 [14464/49669]\tLoss: 394.3364\n",
      "Training Epoch: 24 [14528/49669]\tLoss: 412.3035\n",
      "Training Epoch: 24 [14592/49669]\tLoss: 379.7517\n",
      "Training Epoch: 24 [14656/49669]\tLoss: 378.0656\n",
      "Training Epoch: 24 [14720/49669]\tLoss: 437.3580\n",
      "Training Epoch: 24 [14784/49669]\tLoss: 440.2204\n",
      "Training Epoch: 24 [14848/49669]\tLoss: 411.2509\n",
      "Training Epoch: 24 [14912/49669]\tLoss: 412.1759\n",
      "Training Epoch: 24 [14976/49669]\tLoss: 390.6537\n",
      "Training Epoch: 24 [15040/49669]\tLoss: 393.3046\n",
      "Training Epoch: 24 [15104/49669]\tLoss: 427.9116\n",
      "Training Epoch: 24 [15168/49669]\tLoss: 387.5628\n",
      "Training Epoch: 24 [15232/49669]\tLoss: 398.9136\n",
      "Training Epoch: 24 [15296/49669]\tLoss: 417.2020\n",
      "Training Epoch: 24 [15360/49669]\tLoss: 406.0600\n",
      "Training Epoch: 24 [15424/49669]\tLoss: 397.1458\n",
      "Training Epoch: 24 [15488/49669]\tLoss: 426.9815\n",
      "Training Epoch: 24 [15552/49669]\tLoss: 411.9706\n",
      "Training Epoch: 24 [15616/49669]\tLoss: 415.5681\n",
      "Training Epoch: 24 [15680/49669]\tLoss: 411.1606\n",
      "Training Epoch: 24 [15744/49669]\tLoss: 429.7473\n",
      "Training Epoch: 24 [15808/49669]\tLoss: 396.9387\n",
      "Training Epoch: 24 [15872/49669]\tLoss: 384.9859\n",
      "Training Epoch: 24 [15936/49669]\tLoss: 412.9330\n",
      "Training Epoch: 24 [16000/49669]\tLoss: 391.4809\n",
      "Training Epoch: 24 [16064/49669]\tLoss: 379.7930\n",
      "Training Epoch: 24 [16128/49669]\tLoss: 369.5351\n",
      "Training Epoch: 24 [16192/49669]\tLoss: 405.8778\n",
      "Training Epoch: 24 [16256/49669]\tLoss: 395.8138\n",
      "Training Epoch: 24 [16320/49669]\tLoss: 389.2538\n",
      "Training Epoch: 24 [16384/49669]\tLoss: 426.3885\n",
      "Training Epoch: 24 [16448/49669]\tLoss: 444.0840\n",
      "Training Epoch: 24 [16512/49669]\tLoss: 391.1991\n",
      "Training Epoch: 24 [16576/49669]\tLoss: 415.7663\n",
      "Training Epoch: 24 [16640/49669]\tLoss: 398.3566\n",
      "Training Epoch: 24 [16704/49669]\tLoss: 401.5815\n",
      "Training Epoch: 24 [16768/49669]\tLoss: 394.1920\n",
      "Training Epoch: 24 [16832/49669]\tLoss: 380.4368\n",
      "Training Epoch: 24 [16896/49669]\tLoss: 379.0469\n",
      "Training Epoch: 24 [16960/49669]\tLoss: 395.5382\n",
      "Training Epoch: 24 [17024/49669]\tLoss: 415.6321\n",
      "Training Epoch: 24 [17088/49669]\tLoss: 421.7723\n",
      "Training Epoch: 24 [17152/49669]\tLoss: 421.2768\n",
      "Training Epoch: 24 [17216/49669]\tLoss: 393.4258\n",
      "Training Epoch: 24 [17280/49669]\tLoss: 408.5251\n",
      "Training Epoch: 24 [17344/49669]\tLoss: 423.0659\n",
      "Training Epoch: 24 [17408/49669]\tLoss: 409.6967\n",
      "Training Epoch: 24 [17472/49669]\tLoss: 386.0862\n",
      "Training Epoch: 24 [17536/49669]\tLoss: 417.4770\n",
      "Training Epoch: 24 [17600/49669]\tLoss: 400.0708\n",
      "Training Epoch: 24 [17664/49669]\tLoss: 419.3862\n",
      "Training Epoch: 24 [17728/49669]\tLoss: 412.4450\n",
      "Training Epoch: 24 [17792/49669]\tLoss: 403.7138\n",
      "Training Epoch: 24 [17856/49669]\tLoss: 393.3837\n",
      "Training Epoch: 24 [17920/49669]\tLoss: 396.7958\n",
      "Training Epoch: 24 [17984/49669]\tLoss: 384.3293\n",
      "Training Epoch: 24 [18048/49669]\tLoss: 414.8949\n",
      "Training Epoch: 24 [18112/49669]\tLoss: 378.0598\n",
      "Training Epoch: 24 [18176/49669]\tLoss: 411.5639\n",
      "Training Epoch: 24 [18240/49669]\tLoss: 408.8517\n",
      "Training Epoch: 24 [18304/49669]\tLoss: 422.8530\n",
      "Training Epoch: 24 [18368/49669]\tLoss: 420.8965\n",
      "Training Epoch: 24 [18432/49669]\tLoss: 399.7359\n",
      "Training Epoch: 24 [18496/49669]\tLoss: 403.5923\n",
      "Training Epoch: 24 [18560/49669]\tLoss: 431.7632\n",
      "Training Epoch: 24 [18624/49669]\tLoss: 391.5932\n",
      "Training Epoch: 24 [18688/49669]\tLoss: 392.0365\n",
      "Training Epoch: 24 [18752/49669]\tLoss: 404.5431\n",
      "Training Epoch: 24 [18816/49669]\tLoss: 374.4674\n",
      "Training Epoch: 24 [18880/49669]\tLoss: 398.9219\n",
      "Training Epoch: 24 [18944/49669]\tLoss: 406.6501\n",
      "Training Epoch: 24 [19008/49669]\tLoss: 432.4477\n",
      "Training Epoch: 24 [19072/49669]\tLoss: 410.2960\n",
      "Training Epoch: 24 [19136/49669]\tLoss: 384.7769\n",
      "Training Epoch: 24 [19200/49669]\tLoss: 420.4198\n",
      "Training Epoch: 24 [19264/49669]\tLoss: 415.6438\n",
      "Training Epoch: 24 [19328/49669]\tLoss: 371.8838\n",
      "Training Epoch: 24 [19392/49669]\tLoss: 417.9558\n",
      "Training Epoch: 24 [19456/49669]\tLoss: 399.2513\n",
      "Training Epoch: 24 [19520/49669]\tLoss: 397.7663\n",
      "Training Epoch: 24 [19584/49669]\tLoss: 425.4479\n",
      "Training Epoch: 24 [19648/49669]\tLoss: 374.5175\n",
      "Training Epoch: 24 [19712/49669]\tLoss: 393.8190\n",
      "Training Epoch: 24 [19776/49669]\tLoss: 415.9018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [19840/49669]\tLoss: 410.2425\n",
      "Training Epoch: 24 [19904/49669]\tLoss: 419.1215\n",
      "Training Epoch: 24 [19968/49669]\tLoss: 383.1776\n",
      "Training Epoch: 24 [20032/49669]\tLoss: 411.3432\n",
      "Training Epoch: 24 [20096/49669]\tLoss: 405.7875\n",
      "Training Epoch: 24 [20160/49669]\tLoss: 435.4291\n",
      "Training Epoch: 24 [20224/49669]\tLoss: 413.4874\n",
      "Training Epoch: 24 [20288/49669]\tLoss: 411.7410\n",
      "Training Epoch: 24 [20352/49669]\tLoss: 397.3595\n",
      "Training Epoch: 24 [20416/49669]\tLoss: 386.9377\n",
      "Training Epoch: 24 [20480/49669]\tLoss: 387.6653\n",
      "Training Epoch: 24 [20544/49669]\tLoss: 398.4797\n",
      "Training Epoch: 24 [20608/49669]\tLoss: 405.1081\n",
      "Training Epoch: 24 [20672/49669]\tLoss: 415.0839\n",
      "Training Epoch: 24 [20736/49669]\tLoss: 375.2245\n",
      "Training Epoch: 24 [20800/49669]\tLoss: 413.3413\n",
      "Training Epoch: 24 [20864/49669]\tLoss: 405.7112\n",
      "Training Epoch: 24 [20928/49669]\tLoss: 402.2204\n",
      "Training Epoch: 24 [20992/49669]\tLoss: 415.2588\n",
      "Training Epoch: 24 [21056/49669]\tLoss: 384.2519\n",
      "Training Epoch: 24 [21120/49669]\tLoss: 382.5107\n",
      "Training Epoch: 24 [21184/49669]\tLoss: 410.1478\n",
      "Training Epoch: 24 [21248/49669]\tLoss: 412.8816\n",
      "Training Epoch: 24 [21312/49669]\tLoss: 408.6328\n",
      "Training Epoch: 24 [21376/49669]\tLoss: 395.9416\n",
      "Training Epoch: 24 [21440/49669]\tLoss: 399.4075\n",
      "Training Epoch: 24 [21504/49669]\tLoss: 413.2130\n",
      "Training Epoch: 24 [21568/49669]\tLoss: 406.1304\n",
      "Training Epoch: 24 [21632/49669]\tLoss: 407.7362\n",
      "Training Epoch: 24 [21696/49669]\tLoss: 390.2284\n",
      "Training Epoch: 24 [21760/49669]\tLoss: 399.0672\n",
      "Training Epoch: 24 [21824/49669]\tLoss: 421.3311\n",
      "Training Epoch: 24 [21888/49669]\tLoss: 400.9388\n",
      "Training Epoch: 24 [21952/49669]\tLoss: 393.2782\n",
      "Training Epoch: 24 [22016/49669]\tLoss: 405.5136\n",
      "Training Epoch: 24 [22080/49669]\tLoss: 399.8853\n",
      "Training Epoch: 24 [22144/49669]\tLoss: 394.7020\n",
      "Training Epoch: 24 [22208/49669]\tLoss: 400.6557\n",
      "Training Epoch: 24 [22272/49669]\tLoss: 408.0597\n",
      "Training Epoch: 24 [22336/49669]\tLoss: 379.4141\n",
      "Training Epoch: 24 [22400/49669]\tLoss: 442.0281\n",
      "Training Epoch: 24 [22464/49669]\tLoss: 434.4274\n",
      "Training Epoch: 24 [22528/49669]\tLoss: 407.3276\n",
      "Training Epoch: 24 [22592/49669]\tLoss: 370.7162\n",
      "Training Epoch: 24 [22656/49669]\tLoss: 405.4793\n",
      "Training Epoch: 24 [22720/49669]\tLoss: 383.3788\n",
      "Training Epoch: 24 [22784/49669]\tLoss: 360.0805\n",
      "Training Epoch: 24 [22848/49669]\tLoss: 426.2665\n",
      "Training Epoch: 24 [22912/49669]\tLoss: 414.1873\n",
      "Training Epoch: 24 [22976/49669]\tLoss: 407.0852\n",
      "Training Epoch: 24 [23040/49669]\tLoss: 408.7320\n",
      "Training Epoch: 24 [23104/49669]\tLoss: 400.7607\n",
      "Training Epoch: 24 [23168/49669]\tLoss: 409.2158\n",
      "Training Epoch: 24 [23232/49669]\tLoss: 412.0305\n",
      "Training Epoch: 24 [23296/49669]\tLoss: 382.5523\n",
      "Training Epoch: 24 [23360/49669]\tLoss: 365.6975\n",
      "Training Epoch: 24 [23424/49669]\tLoss: 433.8631\n",
      "Training Epoch: 24 [23488/49669]\tLoss: 372.7469\n",
      "Training Epoch: 24 [23552/49669]\tLoss: 387.4002\n",
      "Training Epoch: 24 [23616/49669]\tLoss: 397.5328\n",
      "Training Epoch: 24 [23680/49669]\tLoss: 396.6810\n",
      "Training Epoch: 24 [23744/49669]\tLoss: 399.7170\n",
      "Training Epoch: 24 [23808/49669]\tLoss: 398.3898\n",
      "Training Epoch: 24 [23872/49669]\tLoss: 398.8186\n",
      "Training Epoch: 24 [23936/49669]\tLoss: 386.7064\n",
      "Training Epoch: 24 [24000/49669]\tLoss: 389.8775\n",
      "Training Epoch: 24 [24064/49669]\tLoss: 445.9011\n",
      "Training Epoch: 24 [24128/49669]\tLoss: 393.4108\n",
      "Training Epoch: 24 [24192/49669]\tLoss: 410.2285\n",
      "Training Epoch: 24 [24256/49669]\tLoss: 388.5423\n",
      "Training Epoch: 24 [24320/49669]\tLoss: 389.9673\n",
      "Training Epoch: 24 [24384/49669]\tLoss: 442.9100\n",
      "Training Epoch: 24 [24448/49669]\tLoss: 387.1535\n",
      "Training Epoch: 24 [24512/49669]\tLoss: 430.9238\n",
      "Training Epoch: 24 [24576/49669]\tLoss: 380.0045\n",
      "Training Epoch: 24 [24640/49669]\tLoss: 402.5496\n",
      "Training Epoch: 24 [24704/49669]\tLoss: 383.3449\n",
      "Training Epoch: 24 [24768/49669]\tLoss: 401.9728\n",
      "Training Epoch: 24 [24832/49669]\tLoss: 387.3269\n",
      "Training Epoch: 24 [24896/49669]\tLoss: 397.2413\n",
      "Training Epoch: 24 [24960/49669]\tLoss: 419.7676\n",
      "Training Epoch: 24 [25024/49669]\tLoss: 412.0237\n",
      "Training Epoch: 24 [25088/49669]\tLoss: 376.0970\n",
      "Training Epoch: 24 [25152/49669]\tLoss: 388.5652\n",
      "Training Epoch: 24 [25216/49669]\tLoss: 427.8945\n",
      "Training Epoch: 24 [25280/49669]\tLoss: 385.6266\n",
      "Training Epoch: 24 [25344/49669]\tLoss: 362.3389\n",
      "Training Epoch: 24 [25408/49669]\tLoss: 386.9771\n",
      "Training Epoch: 24 [25472/49669]\tLoss: 389.5137\n",
      "Training Epoch: 24 [25536/49669]\tLoss: 390.7771\n",
      "Training Epoch: 24 [25600/49669]\tLoss: 420.3379\n",
      "Training Epoch: 24 [25664/49669]\tLoss: 398.8804\n",
      "Training Epoch: 24 [25728/49669]\tLoss: 399.5132\n",
      "Training Epoch: 24 [25792/49669]\tLoss: 393.4830\n",
      "Training Epoch: 24 [25856/49669]\tLoss: 426.5940\n",
      "Training Epoch: 24 [25920/49669]\tLoss: 409.0190\n",
      "Training Epoch: 24 [25984/49669]\tLoss: 376.5074\n",
      "Training Epoch: 24 [26048/49669]\tLoss: 400.3121\n",
      "Training Epoch: 24 [26112/49669]\tLoss: 385.0260\n",
      "Training Epoch: 24 [26176/49669]\tLoss: 400.2888\n",
      "Training Epoch: 24 [26240/49669]\tLoss: 429.6661\n",
      "Training Epoch: 24 [26304/49669]\tLoss: 395.7919\n",
      "Training Epoch: 24 [26368/49669]\tLoss: 397.2599\n",
      "Training Epoch: 24 [26432/49669]\tLoss: 424.4623\n",
      "Training Epoch: 24 [26496/49669]\tLoss: 399.6840\n",
      "Training Epoch: 24 [26560/49669]\tLoss: 398.3482\n",
      "Training Epoch: 24 [26624/49669]\tLoss: 395.2472\n",
      "Training Epoch: 24 [26688/49669]\tLoss: 418.7499\n",
      "Training Epoch: 24 [26752/49669]\tLoss: 427.5437\n",
      "Training Epoch: 24 [26816/49669]\tLoss: 374.5088\n",
      "Training Epoch: 24 [26880/49669]\tLoss: 400.3523\n",
      "Training Epoch: 24 [26944/49669]\tLoss: 425.8860\n",
      "Training Epoch: 24 [27008/49669]\tLoss: 379.0322\n",
      "Training Epoch: 24 [27072/49669]\tLoss: 428.9387\n",
      "Training Epoch: 24 [27136/49669]\tLoss: 400.3718\n",
      "Training Epoch: 24 [27200/49669]\tLoss: 412.3073\n",
      "Training Epoch: 24 [27264/49669]\tLoss: 417.2514\n",
      "Training Epoch: 24 [27328/49669]\tLoss: 385.9114\n",
      "Training Epoch: 24 [27392/49669]\tLoss: 422.8186\n",
      "Training Epoch: 24 [27456/49669]\tLoss: 367.5163\n",
      "Training Epoch: 24 [27520/49669]\tLoss: 401.1699\n",
      "Training Epoch: 24 [27584/49669]\tLoss: 387.1369\n",
      "Training Epoch: 24 [27648/49669]\tLoss: 421.7233\n",
      "Training Epoch: 24 [27712/49669]\tLoss: 402.3217\n",
      "Training Epoch: 24 [27776/49669]\tLoss: 422.9420\n",
      "Training Epoch: 24 [27840/49669]\tLoss: 402.3275\n",
      "Training Epoch: 24 [27904/49669]\tLoss: 422.9362\n",
      "Training Epoch: 24 [27968/49669]\tLoss: 395.2649\n",
      "Training Epoch: 24 [28032/49669]\tLoss: 384.1593\n",
      "Training Epoch: 24 [28096/49669]\tLoss: 388.9378\n",
      "Training Epoch: 24 [28160/49669]\tLoss: 398.3899\n",
      "Training Epoch: 24 [28224/49669]\tLoss: 395.7415\n",
      "Training Epoch: 24 [28288/49669]\tLoss: 402.5056\n",
      "Training Epoch: 24 [28352/49669]\tLoss: 384.8699\n",
      "Training Epoch: 24 [28416/49669]\tLoss: 411.6872\n",
      "Training Epoch: 24 [28480/49669]\tLoss: 402.6908\n",
      "Training Epoch: 24 [28544/49669]\tLoss: 422.2788\n",
      "Training Epoch: 24 [28608/49669]\tLoss: 416.5538\n",
      "Training Epoch: 24 [28672/49669]\tLoss: 425.8811\n",
      "Training Epoch: 24 [28736/49669]\tLoss: 463.4722\n",
      "Training Epoch: 24 [28800/49669]\tLoss: 492.7554\n",
      "Training Epoch: 24 [28864/49669]\tLoss: 529.2016\n",
      "Training Epoch: 24 [28928/49669]\tLoss: 537.6011\n",
      "Training Epoch: 24 [28992/49669]\tLoss: 509.6620\n",
      "Training Epoch: 24 [29056/49669]\tLoss: 456.9981\n",
      "Training Epoch: 24 [29120/49669]\tLoss: 420.4943\n",
      "Training Epoch: 24 [29184/49669]\tLoss: 424.5952\n",
      "Training Epoch: 24 [29248/49669]\tLoss: 441.1594\n",
      "Training Epoch: 24 [29312/49669]\tLoss: 461.9857\n",
      "Training Epoch: 24 [29376/49669]\tLoss: 447.6050\n",
      "Training Epoch: 24 [29440/49669]\tLoss: 404.5891\n",
      "Training Epoch: 24 [29504/49669]\tLoss: 380.3271\n",
      "Training Epoch: 24 [29568/49669]\tLoss: 422.5571\n",
      "Training Epoch: 24 [29632/49669]\tLoss: 403.9619\n",
      "Training Epoch: 24 [29696/49669]\tLoss: 417.0954\n",
      "Training Epoch: 24 [29760/49669]\tLoss: 405.4691\n",
      "Training Epoch: 24 [29824/49669]\tLoss: 404.8096\n",
      "Training Epoch: 24 [29888/49669]\tLoss: 425.5338\n",
      "Training Epoch: 24 [29952/49669]\tLoss: 410.8176\n",
      "Training Epoch: 24 [30016/49669]\tLoss: 409.9931\n",
      "Training Epoch: 24 [30080/49669]\tLoss: 417.6797\n",
      "Training Epoch: 24 [30144/49669]\tLoss: 439.0262\n",
      "Training Epoch: 24 [30208/49669]\tLoss: 429.5977\n",
      "Training Epoch: 24 [30272/49669]\tLoss: 394.1949\n",
      "Training Epoch: 24 [30336/49669]\tLoss: 400.0490\n",
      "Training Epoch: 24 [30400/49669]\tLoss: 395.5578\n",
      "Training Epoch: 24 [30464/49669]\tLoss: 429.0342\n",
      "Training Epoch: 24 [30528/49669]\tLoss: 449.0290\n",
      "Training Epoch: 24 [30592/49669]\tLoss: 447.9951\n",
      "Training Epoch: 24 [30656/49669]\tLoss: 375.0099\n",
      "Training Epoch: 24 [30720/49669]\tLoss: 390.1738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [30784/49669]\tLoss: 415.2272\n",
      "Training Epoch: 24 [30848/49669]\tLoss: 412.8821\n",
      "Training Epoch: 24 [30912/49669]\tLoss: 400.4946\n",
      "Training Epoch: 24 [30976/49669]\tLoss: 380.3593\n",
      "Training Epoch: 24 [31040/49669]\tLoss: 395.6608\n",
      "Training Epoch: 24 [31104/49669]\tLoss: 382.1868\n",
      "Training Epoch: 24 [31168/49669]\tLoss: 395.1383\n",
      "Training Epoch: 24 [31232/49669]\tLoss: 388.8465\n",
      "Training Epoch: 24 [31296/49669]\tLoss: 420.3130\n",
      "Training Epoch: 24 [31360/49669]\tLoss: 405.7491\n",
      "Training Epoch: 24 [31424/49669]\tLoss: 406.0172\n",
      "Training Epoch: 24 [31488/49669]\tLoss: 416.7971\n",
      "Training Epoch: 24 [31552/49669]\tLoss: 405.9323\n",
      "Training Epoch: 24 [31616/49669]\tLoss: 377.1385\n",
      "Training Epoch: 24 [31680/49669]\tLoss: 366.8868\n",
      "Training Epoch: 24 [31744/49669]\tLoss: 416.5356\n",
      "Training Epoch: 24 [31808/49669]\tLoss: 400.2117\n",
      "Training Epoch: 24 [31872/49669]\tLoss: 426.6025\n",
      "Training Epoch: 24 [31936/49669]\tLoss: 393.6717\n",
      "Training Epoch: 24 [32000/49669]\tLoss: 413.9922\n",
      "Training Epoch: 24 [32064/49669]\tLoss: 387.6663\n",
      "Training Epoch: 24 [32128/49669]\tLoss: 384.5593\n",
      "Training Epoch: 24 [32192/49669]\tLoss: 373.9910\n",
      "Training Epoch: 24 [32256/49669]\tLoss: 375.4785\n",
      "Training Epoch: 24 [32320/49669]\tLoss: 420.0805\n",
      "Training Epoch: 24 [32384/49669]\tLoss: 419.2716\n",
      "Training Epoch: 24 [32448/49669]\tLoss: 402.6180\n",
      "Training Epoch: 24 [32512/49669]\tLoss: 397.3409\n",
      "Training Epoch: 24 [32576/49669]\tLoss: 427.4098\n",
      "Training Epoch: 24 [32640/49669]\tLoss: 420.1516\n",
      "Training Epoch: 24 [32704/49669]\tLoss: 390.6447\n",
      "Training Epoch: 24 [32768/49669]\tLoss: 393.0537\n",
      "Training Epoch: 24 [32832/49669]\tLoss: 410.9689\n",
      "Training Epoch: 24 [32896/49669]\tLoss: 429.1049\n",
      "Training Epoch: 24 [32960/49669]\tLoss: 385.9625\n",
      "Training Epoch: 24 [33024/49669]\tLoss: 402.0397\n",
      "Training Epoch: 24 [33088/49669]\tLoss: 437.4747\n",
      "Training Epoch: 24 [33152/49669]\tLoss: 415.1685\n",
      "Training Epoch: 24 [33216/49669]\tLoss: 380.2454\n",
      "Training Epoch: 24 [33280/49669]\tLoss: 403.4632\n",
      "Training Epoch: 24 [33344/49669]\tLoss: 411.1106\n",
      "Training Epoch: 24 [33408/49669]\tLoss: 401.7628\n",
      "Training Epoch: 24 [33472/49669]\tLoss: 414.6793\n",
      "Training Epoch: 24 [33536/49669]\tLoss: 406.3702\n",
      "Training Epoch: 24 [33600/49669]\tLoss: 374.4680\n",
      "Training Epoch: 24 [33664/49669]\tLoss: 396.2983\n",
      "Training Epoch: 24 [33728/49669]\tLoss: 426.1172\n",
      "Training Epoch: 24 [33792/49669]\tLoss: 417.7581\n",
      "Training Epoch: 24 [33856/49669]\tLoss: 374.7216\n",
      "Training Epoch: 24 [33920/49669]\tLoss: 403.7873\n",
      "Training Epoch: 24 [33984/49669]\tLoss: 412.6262\n",
      "Training Epoch: 24 [34048/49669]\tLoss: 379.7767\n",
      "Training Epoch: 24 [34112/49669]\tLoss: 413.4398\n",
      "Training Epoch: 24 [34176/49669]\tLoss: 406.4297\n",
      "Training Epoch: 24 [34240/49669]\tLoss: 404.4019\n",
      "Training Epoch: 24 [34304/49669]\tLoss: 394.6203\n",
      "Training Epoch: 24 [34368/49669]\tLoss: 384.9384\n",
      "Training Epoch: 24 [34432/49669]\tLoss: 385.4531\n",
      "Training Epoch: 24 [34496/49669]\tLoss: 393.6383\n",
      "Training Epoch: 24 [34560/49669]\tLoss: 395.7374\n",
      "Training Epoch: 24 [34624/49669]\tLoss: 413.0208\n",
      "Training Epoch: 24 [34688/49669]\tLoss: 418.8780\n",
      "Training Epoch: 24 [34752/49669]\tLoss: 416.7412\n",
      "Training Epoch: 24 [34816/49669]\tLoss: 416.0846\n",
      "Training Epoch: 24 [34880/49669]\tLoss: 410.0153\n",
      "Training Epoch: 24 [34944/49669]\tLoss: 400.4383\n",
      "Training Epoch: 24 [35008/49669]\tLoss: 425.0209\n",
      "Training Epoch: 24 [35072/49669]\tLoss: 386.0955\n",
      "Training Epoch: 24 [35136/49669]\tLoss: 403.6145\n",
      "Training Epoch: 24 [35200/49669]\tLoss: 439.9072\n",
      "Training Epoch: 24 [35264/49669]\tLoss: 400.5370\n",
      "Training Epoch: 24 [35328/49669]\tLoss: 392.5214\n",
      "Training Epoch: 24 [35392/49669]\tLoss: 409.0182\n",
      "Training Epoch: 24 [35456/49669]\tLoss: 427.5359\n",
      "Training Epoch: 24 [35520/49669]\tLoss: 378.1227\n",
      "Training Epoch: 24 [35584/49669]\tLoss: 427.5623\n",
      "Training Epoch: 24 [35648/49669]\tLoss: 412.6721\n",
      "Training Epoch: 24 [35712/49669]\tLoss: 402.5244\n",
      "Training Epoch: 24 [35776/49669]\tLoss: 378.2581\n",
      "Training Epoch: 24 [35840/49669]\tLoss: 385.6575\n",
      "Training Epoch: 24 [35904/49669]\tLoss: 423.5988\n",
      "Training Epoch: 24 [35968/49669]\tLoss: 392.3909\n",
      "Training Epoch: 24 [36032/49669]\tLoss: 377.3658\n",
      "Training Epoch: 24 [36096/49669]\tLoss: 380.9558\n",
      "Training Epoch: 24 [36160/49669]\tLoss: 399.4615\n",
      "Training Epoch: 24 [36224/49669]\tLoss: 408.4728\n",
      "Training Epoch: 24 [36288/49669]\tLoss: 409.7934\n",
      "Training Epoch: 24 [36352/49669]\tLoss: 388.7438\n",
      "Training Epoch: 24 [36416/49669]\tLoss: 445.4201\n",
      "Training Epoch: 24 [36480/49669]\tLoss: 408.0664\n",
      "Training Epoch: 24 [36544/49669]\tLoss: 403.2853\n",
      "Training Epoch: 24 [36608/49669]\tLoss: 430.5615\n",
      "Training Epoch: 24 [36672/49669]\tLoss: 389.0611\n",
      "Training Epoch: 24 [36736/49669]\tLoss: 391.1458\n",
      "Training Epoch: 24 [36800/49669]\tLoss: 407.0138\n",
      "Training Epoch: 24 [36864/49669]\tLoss: 401.7841\n",
      "Training Epoch: 24 [36928/49669]\tLoss: 390.1257\n",
      "Training Epoch: 24 [36992/49669]\tLoss: 406.4589\n",
      "Training Epoch: 24 [37056/49669]\tLoss: 427.3276\n",
      "Training Epoch: 24 [37120/49669]\tLoss: 415.2706\n",
      "Training Epoch: 24 [37184/49669]\tLoss: 423.8378\n",
      "Training Epoch: 24 [37248/49669]\tLoss: 397.5443\n",
      "Training Epoch: 24 [37312/49669]\tLoss: 383.7416\n",
      "Training Epoch: 24 [37376/49669]\tLoss: 412.5013\n",
      "Training Epoch: 24 [37440/49669]\tLoss: 412.2510\n",
      "Training Epoch: 24 [37504/49669]\tLoss: 394.2890\n",
      "Training Epoch: 24 [37568/49669]\tLoss: 412.5926\n",
      "Training Epoch: 24 [37632/49669]\tLoss: 384.8211\n",
      "Training Epoch: 24 [37696/49669]\tLoss: 380.9328\n",
      "Training Epoch: 24 [37760/49669]\tLoss: 415.6214\n",
      "Training Epoch: 24 [37824/49669]\tLoss: 394.1447\n",
      "Training Epoch: 24 [37888/49669]\tLoss: 410.1310\n",
      "Training Epoch: 24 [37952/49669]\tLoss: 400.9504\n",
      "Training Epoch: 24 [38016/49669]\tLoss: 417.4048\n",
      "Training Epoch: 24 [38080/49669]\tLoss: 371.0778\n",
      "Training Epoch: 24 [38144/49669]\tLoss: 381.3176\n",
      "Training Epoch: 24 [38208/49669]\tLoss: 394.1229\n",
      "Training Epoch: 24 [38272/49669]\tLoss: 410.7599\n",
      "Training Epoch: 24 [38336/49669]\tLoss: 422.3247\n",
      "Training Epoch: 24 [38400/49669]\tLoss: 401.4670\n",
      "Training Epoch: 24 [38464/49669]\tLoss: 392.0254\n",
      "Training Epoch: 24 [38528/49669]\tLoss: 417.6890\n",
      "Training Epoch: 24 [38592/49669]\tLoss: 409.3060\n",
      "Training Epoch: 24 [38656/49669]\tLoss: 420.3279\n",
      "Training Epoch: 24 [38720/49669]\tLoss: 375.5491\n",
      "Training Epoch: 24 [38784/49669]\tLoss: 415.5340\n",
      "Training Epoch: 24 [38848/49669]\tLoss: 398.0498\n",
      "Training Epoch: 24 [38912/49669]\tLoss: 404.8875\n",
      "Training Epoch: 24 [38976/49669]\tLoss: 396.9165\n",
      "Training Epoch: 24 [39040/49669]\tLoss: 430.8174\n",
      "Training Epoch: 24 [39104/49669]\tLoss: 420.1583\n",
      "Training Epoch: 24 [39168/49669]\tLoss: 394.8484\n",
      "Training Epoch: 24 [39232/49669]\tLoss: 407.0843\n",
      "Training Epoch: 24 [39296/49669]\tLoss: 420.1807\n",
      "Training Epoch: 24 [39360/49669]\tLoss: 396.5134\n",
      "Training Epoch: 24 [39424/49669]\tLoss: 437.8459\n",
      "Training Epoch: 24 [39488/49669]\tLoss: 390.7143\n",
      "Training Epoch: 24 [39552/49669]\tLoss: 401.3524\n",
      "Training Epoch: 24 [39616/49669]\tLoss: 438.2302\n",
      "Training Epoch: 24 [39680/49669]\tLoss: 415.8524\n",
      "Training Epoch: 24 [39744/49669]\tLoss: 392.0260\n",
      "Training Epoch: 24 [39808/49669]\tLoss: 412.3853\n",
      "Training Epoch: 24 [39872/49669]\tLoss: 399.8804\n",
      "Training Epoch: 24 [39936/49669]\tLoss: 387.1729\n",
      "Training Epoch: 24 [40000/49669]\tLoss: 397.4267\n",
      "Training Epoch: 24 [40064/49669]\tLoss: 407.5861\n",
      "Training Epoch: 24 [40128/49669]\tLoss: 410.3394\n",
      "Training Epoch: 24 [40192/49669]\tLoss: 425.9009\n",
      "Training Epoch: 24 [40256/49669]\tLoss: 389.6470\n",
      "Training Epoch: 24 [40320/49669]\tLoss: 400.9884\n",
      "Training Epoch: 24 [40384/49669]\tLoss: 394.1012\n",
      "Training Epoch: 24 [40448/49669]\tLoss: 401.4770\n",
      "Training Epoch: 24 [40512/49669]\tLoss: 427.6347\n",
      "Training Epoch: 24 [40576/49669]\tLoss: 366.5188\n",
      "Training Epoch: 24 [40640/49669]\tLoss: 396.6825\n",
      "Training Epoch: 24 [40704/49669]\tLoss: 401.6443\n",
      "Training Epoch: 24 [40768/49669]\tLoss: 413.7188\n",
      "Training Epoch: 24 [40832/49669]\tLoss: 389.1868\n",
      "Training Epoch: 24 [40896/49669]\tLoss: 383.8448\n",
      "Training Epoch: 24 [40960/49669]\tLoss: 407.4696\n",
      "Training Epoch: 24 [41024/49669]\tLoss: 405.4737\n",
      "Training Epoch: 24 [41088/49669]\tLoss: 444.1299\n",
      "Training Epoch: 24 [41152/49669]\tLoss: 378.6355\n",
      "Training Epoch: 24 [41216/49669]\tLoss: 416.6430\n",
      "Training Epoch: 24 [41280/49669]\tLoss: 384.7532\n",
      "Training Epoch: 24 [41344/49669]\tLoss: 408.7531\n",
      "Training Epoch: 24 [41408/49669]\tLoss: 395.4363\n",
      "Training Epoch: 24 [41472/49669]\tLoss: 424.4912\n",
      "Training Epoch: 24 [41536/49669]\tLoss: 412.7168\n",
      "Training Epoch: 24 [41600/49669]\tLoss: 374.0045\n",
      "Training Epoch: 24 [41664/49669]\tLoss: 431.9657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [41728/49669]\tLoss: 394.5131\n",
      "Training Epoch: 24 [41792/49669]\tLoss: 387.6484\n",
      "Training Epoch: 24 [41856/49669]\tLoss: 399.9649\n",
      "Training Epoch: 24 [41920/49669]\tLoss: 455.2955\n",
      "Training Epoch: 24 [41984/49669]\tLoss: 405.9703\n",
      "Training Epoch: 24 [42048/49669]\tLoss: 414.8978\n",
      "Training Epoch: 24 [42112/49669]\tLoss: 402.8037\n",
      "Training Epoch: 24 [42176/49669]\tLoss: 410.1940\n",
      "Training Epoch: 24 [42240/49669]\tLoss: 406.3939\n",
      "Training Epoch: 24 [42304/49669]\tLoss: 397.8400\n",
      "Training Epoch: 24 [42368/49669]\tLoss: 420.0903\n",
      "Training Epoch: 24 [42432/49669]\tLoss: 429.9396\n",
      "Training Epoch: 24 [42496/49669]\tLoss: 398.8871\n",
      "Training Epoch: 24 [42560/49669]\tLoss: 399.0952\n",
      "Training Epoch: 24 [42624/49669]\tLoss: 411.9337\n",
      "Training Epoch: 24 [42688/49669]\tLoss: 427.6010\n",
      "Training Epoch: 24 [42752/49669]\tLoss: 424.3972\n",
      "Training Epoch: 24 [42816/49669]\tLoss: 392.6609\n",
      "Training Epoch: 24 [42880/49669]\tLoss: 407.4099\n",
      "Training Epoch: 24 [42944/49669]\tLoss: 427.3581\n",
      "Training Epoch: 24 [43008/49669]\tLoss: 414.0566\n",
      "Training Epoch: 24 [43072/49669]\tLoss: 406.7611\n",
      "Training Epoch: 24 [43136/49669]\tLoss: 389.7629\n",
      "Training Epoch: 24 [43200/49669]\tLoss: 391.8073\n",
      "Training Epoch: 24 [43264/49669]\tLoss: 377.3156\n",
      "Training Epoch: 24 [43328/49669]\tLoss: 396.8572\n",
      "Training Epoch: 24 [43392/49669]\tLoss: 416.7058\n",
      "Training Epoch: 24 [43456/49669]\tLoss: 411.8730\n",
      "Training Epoch: 24 [43520/49669]\tLoss: 397.7668\n",
      "Training Epoch: 24 [43584/49669]\tLoss: 378.1477\n",
      "Training Epoch: 24 [43648/49669]\tLoss: 383.0550\n",
      "Training Epoch: 24 [43712/49669]\tLoss: 405.0389\n",
      "Training Epoch: 24 [43776/49669]\tLoss: 397.7045\n",
      "Training Epoch: 24 [43840/49669]\tLoss: 390.1906\n",
      "Training Epoch: 24 [43904/49669]\tLoss: 425.1532\n",
      "Training Epoch: 24 [43968/49669]\tLoss: 404.5218\n",
      "Training Epoch: 24 [44032/49669]\tLoss: 398.3448\n",
      "Training Epoch: 24 [44096/49669]\tLoss: 393.3049\n",
      "Training Epoch: 24 [44160/49669]\tLoss: 433.2457\n",
      "Training Epoch: 24 [44224/49669]\tLoss: 375.3321\n",
      "Training Epoch: 24 [44288/49669]\tLoss: 428.0983\n",
      "Training Epoch: 24 [44352/49669]\tLoss: 415.9941\n",
      "Training Epoch: 24 [44416/49669]\tLoss: 382.3863\n",
      "Training Epoch: 24 [44480/49669]\tLoss: 426.5469\n",
      "Training Epoch: 24 [44544/49669]\tLoss: 402.9190\n",
      "Training Epoch: 24 [44608/49669]\tLoss: 414.0590\n",
      "Training Epoch: 24 [44672/49669]\tLoss: 381.1688\n",
      "Training Epoch: 24 [44736/49669]\tLoss: 424.3603\n",
      "Training Epoch: 24 [44800/49669]\tLoss: 396.5130\n",
      "Training Epoch: 24 [44864/49669]\tLoss: 403.8927\n",
      "Training Epoch: 24 [44928/49669]\tLoss: 399.2882\n",
      "Training Epoch: 24 [44992/49669]\tLoss: 405.8807\n",
      "Training Epoch: 24 [45056/49669]\tLoss: 435.3455\n",
      "Training Epoch: 24 [45120/49669]\tLoss: 379.6174\n",
      "Training Epoch: 24 [45184/49669]\tLoss: 408.1183\n",
      "Training Epoch: 24 [45248/49669]\tLoss: 398.7446\n",
      "Training Epoch: 24 [45312/49669]\tLoss: 409.1233\n",
      "Training Epoch: 24 [45376/49669]\tLoss: 394.0156\n",
      "Training Epoch: 24 [45440/49669]\tLoss: 421.2323\n",
      "Training Epoch: 24 [45504/49669]\tLoss: 420.7627\n",
      "Training Epoch: 24 [45568/49669]\tLoss: 401.1809\n",
      "Training Epoch: 24 [45632/49669]\tLoss: 396.0755\n",
      "Training Epoch: 24 [45696/49669]\tLoss: 397.0194\n",
      "Training Epoch: 24 [45760/49669]\tLoss: 390.6047\n",
      "Training Epoch: 24 [45824/49669]\tLoss: 408.2966\n",
      "Training Epoch: 24 [45888/49669]\tLoss: 416.3769\n",
      "Training Epoch: 24 [45952/49669]\tLoss: 398.3084\n",
      "Training Epoch: 24 [46016/49669]\tLoss: 405.0759\n",
      "Training Epoch: 24 [46080/49669]\tLoss: 425.6309\n",
      "Training Epoch: 24 [46144/49669]\tLoss: 405.9604\n",
      "Training Epoch: 24 [46208/49669]\tLoss: 387.1534\n",
      "Training Epoch: 24 [46272/49669]\tLoss: 408.7894\n",
      "Training Epoch: 24 [46336/49669]\tLoss: 397.8364\n",
      "Training Epoch: 24 [46400/49669]\tLoss: 404.8135\n",
      "Training Epoch: 24 [46464/49669]\tLoss: 388.7472\n",
      "Training Epoch: 24 [46528/49669]\tLoss: 382.3546\n",
      "Training Epoch: 24 [46592/49669]\tLoss: 395.2392\n",
      "Training Epoch: 24 [46656/49669]\tLoss: 409.2276\n",
      "Training Epoch: 24 [46720/49669]\tLoss: 407.8381\n",
      "Training Epoch: 24 [46784/49669]\tLoss: 394.1876\n",
      "Training Epoch: 24 [46848/49669]\tLoss: 393.1165\n",
      "Training Epoch: 24 [46912/49669]\tLoss: 392.0557\n",
      "Training Epoch: 24 [46976/49669]\tLoss: 388.8042\n",
      "Training Epoch: 24 [47040/49669]\tLoss: 379.4248\n",
      "Training Epoch: 24 [47104/49669]\tLoss: 418.4353\n",
      "Training Epoch: 24 [47168/49669]\tLoss: 434.5504\n",
      "Training Epoch: 24 [47232/49669]\tLoss: 423.8615\n",
      "Training Epoch: 24 [47296/49669]\tLoss: 404.6047\n",
      "Training Epoch: 24 [47360/49669]\tLoss: 398.7729\n",
      "Training Epoch: 24 [47424/49669]\tLoss: 420.3067\n",
      "Training Epoch: 24 [47488/49669]\tLoss: 408.7075\n",
      "Training Epoch: 24 [47552/49669]\tLoss: 391.8007\n",
      "Training Epoch: 24 [47616/49669]\tLoss: 387.9016\n",
      "Training Epoch: 24 [47680/49669]\tLoss: 419.5245\n",
      "Training Epoch: 24 [47744/49669]\tLoss: 413.1834\n",
      "Training Epoch: 24 [47808/49669]\tLoss: 401.1581\n",
      "Training Epoch: 24 [47872/49669]\tLoss: 398.2314\n",
      "Training Epoch: 24 [47936/49669]\tLoss: 396.1540\n",
      "Training Epoch: 24 [48000/49669]\tLoss: 413.3662\n",
      "Training Epoch: 24 [48064/49669]\tLoss: 404.9145\n",
      "Training Epoch: 24 [48128/49669]\tLoss: 426.3690\n",
      "Training Epoch: 24 [48192/49669]\tLoss: 383.7422\n",
      "Training Epoch: 24 [48256/49669]\tLoss: 403.5652\n",
      "Training Epoch: 24 [48320/49669]\tLoss: 403.9777\n",
      "Training Epoch: 24 [48384/49669]\tLoss: 385.9256\n",
      "Training Epoch: 24 [48448/49669]\tLoss: 398.3220\n",
      "Training Epoch: 24 [48512/49669]\tLoss: 377.5302\n",
      "Training Epoch: 24 [48576/49669]\tLoss: 414.5313\n",
      "Training Epoch: 24 [48640/49669]\tLoss: 419.4571\n",
      "Training Epoch: 24 [48704/49669]\tLoss: 405.0930\n",
      "Training Epoch: 24 [48768/49669]\tLoss: 420.4725\n",
      "Training Epoch: 24 [48832/49669]\tLoss: 436.7941\n",
      "Training Epoch: 24 [48896/49669]\tLoss: 425.5034\n",
      "Training Epoch: 24 [48960/49669]\tLoss: 437.1775\n",
      "Training Epoch: 24 [49024/49669]\tLoss: 446.3279\n",
      "Training Epoch: 24 [49088/49669]\tLoss: 427.7870\n",
      "Training Epoch: 24 [49152/49669]\tLoss: 453.0480\n",
      "Training Epoch: 24 [49216/49669]\tLoss: 443.0290\n",
      "Training Epoch: 24 [49280/49669]\tLoss: 484.9680\n",
      "Training Epoch: 24 [49344/49669]\tLoss: 501.8260\n",
      "Training Epoch: 24 [49408/49669]\tLoss: 485.0316\n",
      "Training Epoch: 24 [49472/49669]\tLoss: 457.4915\n",
      "Training Epoch: 24 [49536/49669]\tLoss: 402.6936\n",
      "Training Epoch: 24 [49600/49669]\tLoss: 401.8936\n",
      "Training Epoch: 24 [49664/49669]\tLoss: 425.9584\n",
      "Training Epoch: 24 [49669/49669]\tLoss: 429.5736\n",
      "Training Epoch: 24 [5519/5519]\tLoss: 472.9442\n",
      "Training Epoch: 25 [64/49669]\tLoss: 454.6472\n",
      "Training Epoch: 25 [128/49669]\tLoss: 447.3746\n",
      "Training Epoch: 25 [192/49669]\tLoss: 408.0748\n",
      "Training Epoch: 25 [256/49669]\tLoss: 404.6487\n",
      "Training Epoch: 25 [320/49669]\tLoss: 424.5283\n",
      "Training Epoch: 25 [384/49669]\tLoss: 467.3216\n",
      "Training Epoch: 25 [448/49669]\tLoss: 423.5676\n",
      "Training Epoch: 25 [512/49669]\tLoss: 412.6568\n",
      "Training Epoch: 25 [576/49669]\tLoss: 406.5034\n",
      "Training Epoch: 25 [640/49669]\tLoss: 424.7212\n",
      "Training Epoch: 25 [704/49669]\tLoss: 420.3736\n",
      "Training Epoch: 25 [768/49669]\tLoss: 421.3801\n",
      "Training Epoch: 25 [832/49669]\tLoss: 394.3474\n",
      "Training Epoch: 25 [896/49669]\tLoss: 402.1117\n",
      "Training Epoch: 25 [960/49669]\tLoss: 413.0921\n",
      "Training Epoch: 25 [1024/49669]\tLoss: 440.7426\n",
      "Training Epoch: 25 [1088/49669]\tLoss: 374.6350\n",
      "Training Epoch: 25 [1152/49669]\tLoss: 401.2802\n",
      "Training Epoch: 25 [1216/49669]\tLoss: 399.3290\n",
      "Training Epoch: 25 [1280/49669]\tLoss: 382.3670\n",
      "Training Epoch: 25 [1344/49669]\tLoss: 400.4396\n",
      "Training Epoch: 25 [1408/49669]\tLoss: 416.6452\n",
      "Training Epoch: 25 [1472/49669]\tLoss: 368.6801\n",
      "Training Epoch: 25 [1536/49669]\tLoss: 422.7915\n",
      "Training Epoch: 25 [1600/49669]\tLoss: 436.7042\n",
      "Training Epoch: 25 [1664/49669]\tLoss: 387.9489\n",
      "Training Epoch: 25 [1728/49669]\tLoss: 387.7582\n",
      "Training Epoch: 25 [1792/49669]\tLoss: 405.6208\n",
      "Training Epoch: 25 [1856/49669]\tLoss: 409.3052\n",
      "Training Epoch: 25 [1920/49669]\tLoss: 418.1743\n",
      "Training Epoch: 25 [1984/49669]\tLoss: 413.5586\n",
      "Training Epoch: 25 [2048/49669]\tLoss: 415.9427\n",
      "Training Epoch: 25 [2112/49669]\tLoss: 407.8257\n",
      "Training Epoch: 25 [2176/49669]\tLoss: 381.5873\n",
      "Training Epoch: 25 [2240/49669]\tLoss: 408.7716\n",
      "Training Epoch: 25 [2304/49669]\tLoss: 418.5579\n",
      "Training Epoch: 25 [2368/49669]\tLoss: 412.1006\n",
      "Training Epoch: 25 [2432/49669]\tLoss: 398.5624\n",
      "Training Epoch: 25 [2496/49669]\tLoss: 398.9097\n",
      "Training Epoch: 25 [2560/49669]\tLoss: 398.2997\n",
      "Training Epoch: 25 [2624/49669]\tLoss: 430.3638\n",
      "Training Epoch: 25 [2688/49669]\tLoss: 376.2525\n",
      "Training Epoch: 25 [2752/49669]\tLoss: 399.7280\n",
      "Training Epoch: 25 [2816/49669]\tLoss: 389.7380\n",
      "Training Epoch: 25 [2880/49669]\tLoss: 401.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [2944/49669]\tLoss: 408.4120\n",
      "Training Epoch: 25 [3008/49669]\tLoss: 412.4193\n",
      "Training Epoch: 25 [3072/49669]\tLoss: 408.3710\n",
      "Training Epoch: 25 [3136/49669]\tLoss: 403.8450\n",
      "Training Epoch: 25 [3200/49669]\tLoss: 410.9007\n",
      "Training Epoch: 25 [3264/49669]\tLoss: 394.7764\n",
      "Training Epoch: 25 [3328/49669]\tLoss: 401.0379\n",
      "Training Epoch: 25 [3392/49669]\tLoss: 389.1027\n",
      "Training Epoch: 25 [3456/49669]\tLoss: 449.8111\n",
      "Training Epoch: 25 [3520/49669]\tLoss: 406.0571\n",
      "Training Epoch: 25 [3584/49669]\tLoss: 397.2728\n",
      "Training Epoch: 25 [3648/49669]\tLoss: 416.5198\n",
      "Training Epoch: 25 [3712/49669]\tLoss: 400.5620\n",
      "Training Epoch: 25 [3776/49669]\tLoss: 378.4697\n",
      "Training Epoch: 25 [3840/49669]\tLoss: 399.9008\n",
      "Training Epoch: 25 [3904/49669]\tLoss: 372.3414\n",
      "Training Epoch: 25 [3968/49669]\tLoss: 392.5368\n",
      "Training Epoch: 25 [4032/49669]\tLoss: 437.2819\n",
      "Training Epoch: 25 [4096/49669]\tLoss: 382.4401\n",
      "Training Epoch: 25 [4160/49669]\tLoss: 382.7393\n",
      "Training Epoch: 25 [4224/49669]\tLoss: 406.2212\n",
      "Training Epoch: 25 [4288/49669]\tLoss: 379.5692\n",
      "Training Epoch: 25 [4352/49669]\tLoss: 423.2656\n",
      "Training Epoch: 25 [4416/49669]\tLoss: 409.1649\n",
      "Training Epoch: 25 [4480/49669]\tLoss: 419.3871\n",
      "Training Epoch: 25 [4544/49669]\tLoss: 403.0768\n",
      "Training Epoch: 25 [4608/49669]\tLoss: 396.3582\n",
      "Training Epoch: 25 [4672/49669]\tLoss: 402.9235\n",
      "Training Epoch: 25 [4736/49669]\tLoss: 382.3746\n",
      "Training Epoch: 25 [4800/49669]\tLoss: 431.5843\n",
      "Training Epoch: 25 [4864/49669]\tLoss: 396.3371\n",
      "Training Epoch: 25 [4928/49669]\tLoss: 400.6404\n",
      "Training Epoch: 25 [4992/49669]\tLoss: 371.5751\n",
      "Training Epoch: 25 [5056/49669]\tLoss: 407.4185\n",
      "Training Epoch: 25 [5120/49669]\tLoss: 381.9384\n",
      "Training Epoch: 25 [5184/49669]\tLoss: 391.4054\n",
      "Training Epoch: 25 [5248/49669]\tLoss: 403.2901\n",
      "Training Epoch: 25 [5312/49669]\tLoss: 389.1559\n",
      "Training Epoch: 25 [5376/49669]\tLoss: 389.1412\n",
      "Training Epoch: 25 [5440/49669]\tLoss: 417.8210\n",
      "Training Epoch: 25 [5504/49669]\tLoss: 427.2244\n",
      "Training Epoch: 25 [5568/49669]\tLoss: 374.6349\n",
      "Training Epoch: 25 [5632/49669]\tLoss: 398.2563\n",
      "Training Epoch: 25 [5696/49669]\tLoss: 396.8478\n",
      "Training Epoch: 25 [5760/49669]\tLoss: 393.7275\n",
      "Training Epoch: 25 [5824/49669]\tLoss: 410.5072\n",
      "Training Epoch: 25 [5888/49669]\tLoss: 410.2417\n",
      "Training Epoch: 25 [5952/49669]\tLoss: 378.6672\n",
      "Training Epoch: 25 [6016/49669]\tLoss: 395.9258\n",
      "Training Epoch: 25 [6080/49669]\tLoss: 411.0547\n",
      "Training Epoch: 25 [6144/49669]\tLoss: 386.0320\n",
      "Training Epoch: 25 [6208/49669]\tLoss: 406.2678\n",
      "Training Epoch: 25 [6272/49669]\tLoss: 408.7488\n",
      "Training Epoch: 25 [6336/49669]\tLoss: 387.4494\n",
      "Training Epoch: 25 [6400/49669]\tLoss: 397.9450\n",
      "Training Epoch: 25 [6464/49669]\tLoss: 415.1488\n",
      "Training Epoch: 25 [6528/49669]\tLoss: 388.8361\n",
      "Training Epoch: 25 [6592/49669]\tLoss: 381.1049\n",
      "Training Epoch: 25 [6656/49669]\tLoss: 403.4329\n",
      "Training Epoch: 25 [6720/49669]\tLoss: 393.3747\n",
      "Training Epoch: 25 [6784/49669]\tLoss: 405.5381\n",
      "Training Epoch: 25 [6848/49669]\tLoss: 384.9119\n",
      "Training Epoch: 25 [6912/49669]\tLoss: 383.9071\n",
      "Training Epoch: 25 [6976/49669]\tLoss: 425.5885\n",
      "Training Epoch: 25 [7040/49669]\tLoss: 384.6122\n",
      "Training Epoch: 25 [7104/49669]\tLoss: 407.5396\n",
      "Training Epoch: 25 [7168/49669]\tLoss: 401.8055\n",
      "Training Epoch: 25 [7232/49669]\tLoss: 412.1954\n",
      "Training Epoch: 25 [7296/49669]\tLoss: 378.2291\n",
      "Training Epoch: 25 [7360/49669]\tLoss: 402.3885\n",
      "Training Epoch: 25 [7424/49669]\tLoss: 386.4984\n",
      "Training Epoch: 25 [7488/49669]\tLoss: 404.2932\n",
      "Training Epoch: 25 [7552/49669]\tLoss: 413.8639\n",
      "Training Epoch: 25 [7616/49669]\tLoss: 415.7094\n",
      "Training Epoch: 25 [7680/49669]\tLoss: 421.3651\n",
      "Training Epoch: 25 [7744/49669]\tLoss: 373.1670\n",
      "Training Epoch: 25 [7808/49669]\tLoss: 407.6517\n",
      "Training Epoch: 25 [7872/49669]\tLoss: 428.3914\n",
      "Training Epoch: 25 [7936/49669]\tLoss: 408.5649\n",
      "Training Epoch: 25 [8000/49669]\tLoss: 413.2137\n",
      "Training Epoch: 25 [8064/49669]\tLoss: 391.3343\n",
      "Training Epoch: 25 [8128/49669]\tLoss: 395.4334\n",
      "Training Epoch: 25 [8192/49669]\tLoss: 410.5946\n",
      "Training Epoch: 25 [8256/49669]\tLoss: 402.4739\n",
      "Training Epoch: 25 [8320/49669]\tLoss: 399.6951\n",
      "Training Epoch: 25 [8384/49669]\tLoss: 420.7014\n",
      "Training Epoch: 25 [8448/49669]\tLoss: 422.2178\n",
      "Training Epoch: 25 [8512/49669]\tLoss: 423.9745\n",
      "Training Epoch: 25 [8576/49669]\tLoss: 394.7173\n",
      "Training Epoch: 25 [8640/49669]\tLoss: 435.7140\n",
      "Training Epoch: 25 [8704/49669]\tLoss: 384.4330\n",
      "Training Epoch: 25 [8768/49669]\tLoss: 391.1308\n",
      "Training Epoch: 25 [8832/49669]\tLoss: 385.6529\n",
      "Training Epoch: 25 [8896/49669]\tLoss: 416.6295\n",
      "Training Epoch: 25 [8960/49669]\tLoss: 412.3123\n",
      "Training Epoch: 25 [9024/49669]\tLoss: 410.5864\n",
      "Training Epoch: 25 [9088/49669]\tLoss: 419.0723\n",
      "Training Epoch: 25 [9152/49669]\tLoss: 416.0211\n",
      "Training Epoch: 25 [9216/49669]\tLoss: 394.3334\n",
      "Training Epoch: 25 [9280/49669]\tLoss: 394.0032\n",
      "Training Epoch: 25 [9344/49669]\tLoss: 422.4270\n",
      "Training Epoch: 25 [9408/49669]\tLoss: 397.6143\n",
      "Training Epoch: 25 [9472/49669]\tLoss: 390.9370\n",
      "Training Epoch: 25 [9536/49669]\tLoss: 401.2043\n",
      "Training Epoch: 25 [9600/49669]\tLoss: 391.9988\n",
      "Training Epoch: 25 [9664/49669]\tLoss: 376.2771\n",
      "Training Epoch: 25 [9728/49669]\tLoss: 386.1382\n",
      "Training Epoch: 25 [9792/49669]\tLoss: 418.6273\n",
      "Training Epoch: 25 [9856/49669]\tLoss: 385.7910\n",
      "Training Epoch: 25 [9920/49669]\tLoss: 395.8596\n",
      "Training Epoch: 25 [9984/49669]\tLoss: 390.0036\n",
      "Training Epoch: 25 [10048/49669]\tLoss: 392.3208\n",
      "Training Epoch: 25 [10112/49669]\tLoss: 405.5303\n",
      "Training Epoch: 25 [10176/49669]\tLoss: 395.4132\n",
      "Training Epoch: 25 [10240/49669]\tLoss: 388.2759\n",
      "Training Epoch: 25 [10304/49669]\tLoss: 401.5491\n",
      "Training Epoch: 25 [10368/49669]\tLoss: 407.7827\n",
      "Training Epoch: 25 [10432/49669]\tLoss: 385.4191\n",
      "Training Epoch: 25 [10496/49669]\tLoss: 376.4650\n",
      "Training Epoch: 25 [10560/49669]\tLoss: 368.9634\n",
      "Training Epoch: 25 [10624/49669]\tLoss: 421.8383\n",
      "Training Epoch: 25 [10688/49669]\tLoss: 416.9482\n",
      "Training Epoch: 25 [10752/49669]\tLoss: 383.9397\n",
      "Training Epoch: 25 [10816/49669]\tLoss: 404.3443\n",
      "Training Epoch: 25 [10880/49669]\tLoss: 388.2617\n",
      "Training Epoch: 25 [10944/49669]\tLoss: 416.9265\n",
      "Training Epoch: 25 [11008/49669]\tLoss: 398.7257\n",
      "Training Epoch: 25 [11072/49669]\tLoss: 396.9633\n",
      "Training Epoch: 25 [11136/49669]\tLoss: 413.3234\n",
      "Training Epoch: 25 [11200/49669]\tLoss: 389.4861\n",
      "Training Epoch: 25 [11264/49669]\tLoss: 417.3510\n",
      "Training Epoch: 25 [11328/49669]\tLoss: 389.7200\n",
      "Training Epoch: 25 [11392/49669]\tLoss: 394.9096\n",
      "Training Epoch: 25 [11456/49669]\tLoss: 404.3304\n",
      "Training Epoch: 25 [11520/49669]\tLoss: 400.3057\n",
      "Training Epoch: 25 [11584/49669]\tLoss: 391.2310\n",
      "Training Epoch: 25 [11648/49669]\tLoss: 408.5064\n",
      "Training Epoch: 25 [11712/49669]\tLoss: 428.2764\n",
      "Training Epoch: 25 [11776/49669]\tLoss: 385.0659\n",
      "Training Epoch: 25 [11840/49669]\tLoss: 414.2726\n",
      "Training Epoch: 25 [11904/49669]\tLoss: 396.8907\n",
      "Training Epoch: 25 [11968/49669]\tLoss: 422.1684\n",
      "Training Epoch: 25 [12032/49669]\tLoss: 415.2636\n",
      "Training Epoch: 25 [12096/49669]\tLoss: 383.0719\n",
      "Training Epoch: 25 [12160/49669]\tLoss: 402.1336\n",
      "Training Epoch: 25 [12224/49669]\tLoss: 415.9376\n",
      "Training Epoch: 25 [12288/49669]\tLoss: 403.5571\n",
      "Training Epoch: 25 [12352/49669]\tLoss: 437.2776\n",
      "Training Epoch: 25 [12416/49669]\tLoss: 387.9099\n",
      "Training Epoch: 25 [12480/49669]\tLoss: 369.3257\n",
      "Training Epoch: 25 [12544/49669]\tLoss: 428.7202\n",
      "Training Epoch: 25 [12608/49669]\tLoss: 400.2608\n",
      "Training Epoch: 25 [12672/49669]\tLoss: 406.7028\n",
      "Training Epoch: 25 [12736/49669]\tLoss: 358.8562\n",
      "Training Epoch: 25 [12800/49669]\tLoss: 393.8138\n",
      "Training Epoch: 25 [12864/49669]\tLoss: 391.7378\n",
      "Training Epoch: 25 [12928/49669]\tLoss: 379.1190\n",
      "Training Epoch: 25 [12992/49669]\tLoss: 393.3798\n",
      "Training Epoch: 25 [13056/49669]\tLoss: 393.7134\n",
      "Training Epoch: 25 [13120/49669]\tLoss: 401.1675\n",
      "Training Epoch: 25 [13184/49669]\tLoss: 395.2291\n",
      "Training Epoch: 25 [13248/49669]\tLoss: 391.1344\n",
      "Training Epoch: 25 [13312/49669]\tLoss: 397.7729\n",
      "Training Epoch: 25 [13376/49669]\tLoss: 420.5111\n",
      "Training Epoch: 25 [13440/49669]\tLoss: 396.2821\n",
      "Training Epoch: 25 [13504/49669]\tLoss: 382.0828\n",
      "Training Epoch: 25 [13568/49669]\tLoss: 392.8437\n",
      "Training Epoch: 25 [13632/49669]\tLoss: 412.5786\n",
      "Training Epoch: 25 [13696/49669]\tLoss: 410.7754\n",
      "Training Epoch: 25 [13760/49669]\tLoss: 389.0996\n",
      "Training Epoch: 25 [13824/49669]\tLoss: 421.0820\n",
      "Training Epoch: 25 [13888/49669]\tLoss: 385.9760\n",
      "Training Epoch: 25 [13952/49669]\tLoss: 396.9271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [14016/49669]\tLoss: 402.4999\n",
      "Training Epoch: 25 [14080/49669]\tLoss: 368.3528\n",
      "Training Epoch: 25 [14144/49669]\tLoss: 431.4693\n",
      "Training Epoch: 25 [14208/49669]\tLoss: 410.8520\n",
      "Training Epoch: 25 [14272/49669]\tLoss: 395.6760\n",
      "Training Epoch: 25 [14336/49669]\tLoss: 396.0593\n",
      "Training Epoch: 25 [14400/49669]\tLoss: 406.9565\n",
      "Training Epoch: 25 [14464/49669]\tLoss: 397.1046\n",
      "Training Epoch: 25 [14528/49669]\tLoss: 417.1661\n",
      "Training Epoch: 25 [14592/49669]\tLoss: 418.0932\n",
      "Training Epoch: 25 [14656/49669]\tLoss: 400.4425\n",
      "Training Epoch: 25 [14720/49669]\tLoss: 413.2882\n",
      "Training Epoch: 25 [14784/49669]\tLoss: 434.2831\n",
      "Training Epoch: 25 [14848/49669]\tLoss: 420.4188\n",
      "Training Epoch: 25 [14912/49669]\tLoss: 404.2495\n",
      "Training Epoch: 25 [14976/49669]\tLoss: 416.7420\n",
      "Training Epoch: 25 [15040/49669]\tLoss: 411.5908\n",
      "Training Epoch: 25 [15104/49669]\tLoss: 426.8720\n",
      "Training Epoch: 25 [15168/49669]\tLoss: 431.8862\n",
      "Training Epoch: 25 [15232/49669]\tLoss: 428.1800\n",
      "Training Epoch: 25 [15296/49669]\tLoss: 433.7931\n",
      "Training Epoch: 25 [15360/49669]\tLoss: 421.0313\n",
      "Training Epoch: 25 [15424/49669]\tLoss: 387.1647\n",
      "Training Epoch: 25 [15488/49669]\tLoss: 416.6668\n",
      "Training Epoch: 25 [15552/49669]\tLoss: 424.2615\n",
      "Training Epoch: 25 [15616/49669]\tLoss: 372.0231\n",
      "Training Epoch: 25 [15680/49669]\tLoss: 387.9485\n",
      "Training Epoch: 25 [15744/49669]\tLoss: 411.4707\n",
      "Training Epoch: 25 [15808/49669]\tLoss: 391.4516\n",
      "Training Epoch: 25 [15872/49669]\tLoss: 417.6641\n",
      "Training Epoch: 25 [15936/49669]\tLoss: 420.1692\n",
      "Training Epoch: 25 [16000/49669]\tLoss: 415.5284\n",
      "Training Epoch: 25 [16064/49669]\tLoss: 407.8011\n",
      "Training Epoch: 25 [16128/49669]\tLoss: 402.2694\n",
      "Training Epoch: 25 [16192/49669]\tLoss: 425.8460\n",
      "Training Epoch: 25 [16256/49669]\tLoss: 428.4621\n",
      "Training Epoch: 25 [16320/49669]\tLoss: 378.9918\n",
      "Training Epoch: 25 [16384/49669]\tLoss: 391.0932\n",
      "Training Epoch: 25 [16448/49669]\tLoss: 425.3301\n",
      "Training Epoch: 25 [16512/49669]\tLoss: 410.3586\n",
      "Training Epoch: 25 [16576/49669]\tLoss: 423.6620\n",
      "Training Epoch: 25 [16640/49669]\tLoss: 435.8275\n",
      "Training Epoch: 25 [16704/49669]\tLoss: 418.2930\n",
      "Training Epoch: 25 [16768/49669]\tLoss: 393.2773\n",
      "Training Epoch: 25 [16832/49669]\tLoss: 379.4247\n",
      "Training Epoch: 25 [16896/49669]\tLoss: 410.7453\n",
      "Training Epoch: 25 [16960/49669]\tLoss: 408.0071\n",
      "Training Epoch: 25 [17024/49669]\tLoss: 399.8107\n",
      "Training Epoch: 25 [17088/49669]\tLoss: 408.0354\n",
      "Training Epoch: 25 [17152/49669]\tLoss: 377.5080\n",
      "Training Epoch: 25 [17216/49669]\tLoss: 389.0697\n",
      "Training Epoch: 25 [17280/49669]\tLoss: 375.7805\n",
      "Training Epoch: 25 [17344/49669]\tLoss: 385.7140\n",
      "Training Epoch: 25 [17408/49669]\tLoss: 412.3757\n",
      "Training Epoch: 25 [17472/49669]\tLoss: 396.3340\n",
      "Training Epoch: 25 [17536/49669]\tLoss: 375.8224\n",
      "Training Epoch: 25 [17600/49669]\tLoss: 398.0552\n",
      "Training Epoch: 25 [17664/49669]\tLoss: 397.1304\n",
      "Training Epoch: 25 [17728/49669]\tLoss: 412.3045\n",
      "Training Epoch: 25 [17792/49669]\tLoss: 408.4858\n",
      "Training Epoch: 25 [17856/49669]\tLoss: 399.0652\n",
      "Training Epoch: 25 [17920/49669]\tLoss: 421.0326\n",
      "Training Epoch: 25 [17984/49669]\tLoss: 420.5239\n",
      "Training Epoch: 25 [18048/49669]\tLoss: 415.8716\n",
      "Training Epoch: 25 [18112/49669]\tLoss: 441.9147\n",
      "Training Epoch: 25 [18176/49669]\tLoss: 425.7933\n",
      "Training Epoch: 25 [18240/49669]\tLoss: 413.2707\n",
      "Training Epoch: 25 [18304/49669]\tLoss: 388.9810\n",
      "Training Epoch: 25 [18368/49669]\tLoss: 402.4644\n",
      "Training Epoch: 25 [18432/49669]\tLoss: 417.4628\n",
      "Training Epoch: 25 [18496/49669]\tLoss: 393.2424\n",
      "Training Epoch: 25 [18560/49669]\tLoss: 402.6062\n",
      "Training Epoch: 25 [18624/49669]\tLoss: 410.0813\n",
      "Training Epoch: 25 [18688/49669]\tLoss: 380.4524\n",
      "Training Epoch: 25 [18752/49669]\tLoss: 389.9100\n",
      "Training Epoch: 25 [18816/49669]\tLoss: 381.3182\n",
      "Training Epoch: 25 [18880/49669]\tLoss: 415.0762\n",
      "Training Epoch: 25 [18944/49669]\tLoss: 400.3380\n",
      "Training Epoch: 25 [19008/49669]\tLoss: 411.0084\n",
      "Training Epoch: 25 [19072/49669]\tLoss: 446.3740\n",
      "Training Epoch: 25 [19136/49669]\tLoss: 444.3513\n",
      "Training Epoch: 25 [19200/49669]\tLoss: 380.5543\n",
      "Training Epoch: 25 [19264/49669]\tLoss: 422.7789\n",
      "Training Epoch: 25 [19328/49669]\tLoss: 434.6004\n",
      "Training Epoch: 25 [19392/49669]\tLoss: 420.4808\n",
      "Training Epoch: 25 [19456/49669]\tLoss: 392.1972\n",
      "Training Epoch: 25 [19520/49669]\tLoss: 367.2907\n",
      "Training Epoch: 25 [19584/49669]\tLoss: 428.2032\n",
      "Training Epoch: 25 [19648/49669]\tLoss: 405.2591\n",
      "Training Epoch: 25 [19712/49669]\tLoss: 414.5780\n",
      "Training Epoch: 25 [19776/49669]\tLoss: 448.2502\n",
      "Training Epoch: 25 [19840/49669]\tLoss: 412.9275\n",
      "Training Epoch: 25 [19904/49669]\tLoss: 438.2234\n",
      "Training Epoch: 25 [19968/49669]\tLoss: 453.5886\n",
      "Training Epoch: 25 [20032/49669]\tLoss: 452.0947\n",
      "Training Epoch: 25 [20096/49669]\tLoss: 454.3596\n",
      "Training Epoch: 25 [20160/49669]\tLoss: 415.2369\n",
      "Training Epoch: 25 [20224/49669]\tLoss: 401.5637\n",
      "Training Epoch: 25 [20288/49669]\tLoss: 378.8170\n",
      "Training Epoch: 25 [20352/49669]\tLoss: 399.4613\n",
      "Training Epoch: 25 [20416/49669]\tLoss: 424.4977\n",
      "Training Epoch: 25 [20480/49669]\tLoss: 464.2101\n",
      "Training Epoch: 25 [20544/49669]\tLoss: 433.5351\n",
      "Training Epoch: 25 [20608/49669]\tLoss: 444.4756\n",
      "Training Epoch: 25 [20672/49669]\tLoss: 461.5909\n",
      "Training Epoch: 25 [20736/49669]\tLoss: 427.0696\n",
      "Training Epoch: 25 [20800/49669]\tLoss: 409.2249\n",
      "Training Epoch: 25 [20864/49669]\tLoss: 423.5519\n",
      "Training Epoch: 25 [20928/49669]\tLoss: 421.9461\n",
      "Training Epoch: 25 [20992/49669]\tLoss: 395.9445\n",
      "Training Epoch: 25 [21056/49669]\tLoss: 380.2333\n",
      "Training Epoch: 25 [21120/49669]\tLoss: 385.9328\n",
      "Training Epoch: 25 [21184/49669]\tLoss: 425.1172\n",
      "Training Epoch: 25 [21248/49669]\tLoss: 414.1172\n",
      "Training Epoch: 25 [21312/49669]\tLoss: 404.1332\n",
      "Training Epoch: 25 [21376/49669]\tLoss: 404.1835\n",
      "Training Epoch: 25 [21440/49669]\tLoss: 407.9711\n",
      "Training Epoch: 25 [21504/49669]\tLoss: 451.4510\n",
      "Training Epoch: 25 [21568/49669]\tLoss: 397.1525\n",
      "Training Epoch: 25 [21632/49669]\tLoss: 391.4320\n",
      "Training Epoch: 25 [21696/49669]\tLoss: 394.4384\n",
      "Training Epoch: 25 [21760/49669]\tLoss: 418.9569\n",
      "Training Epoch: 25 [21824/49669]\tLoss: 426.5470\n",
      "Training Epoch: 25 [21888/49669]\tLoss: 424.7123\n",
      "Training Epoch: 25 [21952/49669]\tLoss: 398.2497\n",
      "Training Epoch: 25 [22016/49669]\tLoss: 386.2505\n",
      "Training Epoch: 25 [22080/49669]\tLoss: 388.1688\n",
      "Training Epoch: 25 [22144/49669]\tLoss: 397.2059\n",
      "Training Epoch: 25 [22208/49669]\tLoss: 396.6101\n",
      "Training Epoch: 25 [22272/49669]\tLoss: 422.5579\n",
      "Training Epoch: 25 [22336/49669]\tLoss: 391.9431\n",
      "Training Epoch: 25 [22400/49669]\tLoss: 405.2719\n",
      "Training Epoch: 25 [22464/49669]\tLoss: 398.9085\n",
      "Training Epoch: 25 [22528/49669]\tLoss: 391.8946\n",
      "Training Epoch: 25 [22592/49669]\tLoss: 406.6302\n",
      "Training Epoch: 25 [22656/49669]\tLoss: 399.8452\n",
      "Training Epoch: 25 [22720/49669]\tLoss: 374.2561\n",
      "Training Epoch: 25 [22784/49669]\tLoss: 407.7134\n",
      "Training Epoch: 25 [22848/49669]\tLoss: 404.7194\n",
      "Training Epoch: 25 [22912/49669]\tLoss: 413.3147\n",
      "Training Epoch: 25 [22976/49669]\tLoss: 410.1853\n",
      "Training Epoch: 25 [23040/49669]\tLoss: 404.4059\n",
      "Training Epoch: 25 [23104/49669]\tLoss: 433.3732\n",
      "Training Epoch: 25 [23168/49669]\tLoss: 420.3842\n",
      "Training Epoch: 25 [23232/49669]\tLoss: 394.9993\n",
      "Training Epoch: 25 [23296/49669]\tLoss: 433.0372\n",
      "Training Epoch: 25 [23360/49669]\tLoss: 397.4814\n",
      "Training Epoch: 25 [23424/49669]\tLoss: 378.4687\n",
      "Training Epoch: 25 [23488/49669]\tLoss: 400.6830\n",
      "Training Epoch: 25 [23552/49669]\tLoss: 418.8806\n",
      "Training Epoch: 25 [23616/49669]\tLoss: 391.1418\n",
      "Training Epoch: 25 [23680/49669]\tLoss: 414.2041\n",
      "Training Epoch: 25 [23744/49669]\tLoss: 398.0846\n",
      "Training Epoch: 25 [23808/49669]\tLoss: 414.6260\n",
      "Training Epoch: 25 [23872/49669]\tLoss: 428.2539\n",
      "Training Epoch: 25 [23936/49669]\tLoss: 376.7612\n",
      "Training Epoch: 25 [24000/49669]\tLoss: 375.7820\n",
      "Training Epoch: 25 [24064/49669]\tLoss: 435.2437\n",
      "Training Epoch: 25 [24128/49669]\tLoss: 407.5661\n",
      "Training Epoch: 25 [24192/49669]\tLoss: 404.2102\n",
      "Training Epoch: 25 [24256/49669]\tLoss: 411.0184\n",
      "Training Epoch: 25 [24320/49669]\tLoss: 407.6119\n",
      "Training Epoch: 25 [24384/49669]\tLoss: 416.5379\n",
      "Training Epoch: 25 [24448/49669]\tLoss: 405.0605\n",
      "Training Epoch: 25 [24512/49669]\tLoss: 397.6004\n",
      "Training Epoch: 25 [24576/49669]\tLoss: 405.2272\n",
      "Training Epoch: 25 [24640/49669]\tLoss: 398.9030\n",
      "Training Epoch: 25 [24704/49669]\tLoss: 412.0511\n",
      "Training Epoch: 25 [24768/49669]\tLoss: 384.8010\n",
      "Training Epoch: 25 [24832/49669]\tLoss: 405.7078\n",
      "Training Epoch: 25 [24896/49669]\tLoss: 381.1526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [24960/49669]\tLoss: 419.5810\n",
      "Training Epoch: 25 [25024/49669]\tLoss: 393.1563\n",
      "Training Epoch: 25 [25088/49669]\tLoss: 363.4088\n",
      "Training Epoch: 25 [25152/49669]\tLoss: 407.9048\n",
      "Training Epoch: 25 [25216/49669]\tLoss: 418.5191\n",
      "Training Epoch: 25 [25280/49669]\tLoss: 374.7005\n",
      "Training Epoch: 25 [25344/49669]\tLoss: 387.7010\n",
      "Training Epoch: 25 [25408/49669]\tLoss: 362.3132\n",
      "Training Epoch: 25 [25472/49669]\tLoss: 421.8323\n",
      "Training Epoch: 25 [25536/49669]\tLoss: 432.0610\n",
      "Training Epoch: 25 [25600/49669]\tLoss: 427.7781\n",
      "Training Epoch: 25 [25664/49669]\tLoss: 406.7841\n",
      "Training Epoch: 25 [25728/49669]\tLoss: 430.4296\n",
      "Training Epoch: 25 [25792/49669]\tLoss: 384.4032\n",
      "Training Epoch: 25 [25856/49669]\tLoss: 398.4578\n",
      "Training Epoch: 25 [25920/49669]\tLoss: 431.4867\n",
      "Training Epoch: 25 [25984/49669]\tLoss: 406.2296\n",
      "Training Epoch: 25 [26048/49669]\tLoss: 417.5350\n",
      "Training Epoch: 25 [26112/49669]\tLoss: 421.7696\n",
      "Training Epoch: 25 [26176/49669]\tLoss: 413.9372\n",
      "Training Epoch: 25 [26240/49669]\tLoss: 399.7599\n",
      "Training Epoch: 25 [26304/49669]\tLoss: 394.8926\n",
      "Training Epoch: 25 [26368/49669]\tLoss: 411.1726\n",
      "Training Epoch: 25 [26432/49669]\tLoss: 405.5698\n",
      "Training Epoch: 25 [26496/49669]\tLoss: 403.9919\n",
      "Training Epoch: 25 [26560/49669]\tLoss: 418.9819\n",
      "Training Epoch: 25 [26624/49669]\tLoss: 432.1504\n",
      "Training Epoch: 25 [26688/49669]\tLoss: 404.3221\n",
      "Training Epoch: 25 [26752/49669]\tLoss: 408.7453\n",
      "Training Epoch: 25 [26816/49669]\tLoss: 396.6226\n",
      "Training Epoch: 25 [26880/49669]\tLoss: 442.5865\n",
      "Training Epoch: 25 [26944/49669]\tLoss: 401.9760\n",
      "Training Epoch: 25 [27008/49669]\tLoss: 405.1727\n",
      "Training Epoch: 25 [27072/49669]\tLoss: 409.8411\n",
      "Training Epoch: 25 [27136/49669]\tLoss: 361.8358\n",
      "Training Epoch: 25 [27200/49669]\tLoss: 413.6614\n",
      "Training Epoch: 25 [27264/49669]\tLoss: 364.8531\n",
      "Training Epoch: 25 [27328/49669]\tLoss: 417.9892\n",
      "Training Epoch: 25 [27392/49669]\tLoss: 409.9475\n",
      "Training Epoch: 25 [27456/49669]\tLoss: 420.0898\n",
      "Training Epoch: 25 [27520/49669]\tLoss: 415.2039\n",
      "Training Epoch: 25 [27584/49669]\tLoss: 385.0041\n",
      "Training Epoch: 25 [27648/49669]\tLoss: 433.7178\n",
      "Training Epoch: 25 [27712/49669]\tLoss: 384.1698\n",
      "Training Epoch: 25 [27776/49669]\tLoss: 410.5666\n",
      "Training Epoch: 25 [27840/49669]\tLoss: 398.9587\n",
      "Training Epoch: 25 [27904/49669]\tLoss: 395.7704\n",
      "Training Epoch: 25 [27968/49669]\tLoss: 411.7150\n",
      "Training Epoch: 25 [28032/49669]\tLoss: 422.9693\n",
      "Training Epoch: 25 [28096/49669]\tLoss: 416.1212\n",
      "Training Epoch: 25 [28160/49669]\tLoss: 415.0783\n",
      "Training Epoch: 25 [28224/49669]\tLoss: 455.8944\n",
      "Training Epoch: 25 [28288/49669]\tLoss: 427.4193\n",
      "Training Epoch: 25 [28352/49669]\tLoss: 420.4629\n",
      "Training Epoch: 25 [28416/49669]\tLoss: 417.4977\n",
      "Training Epoch: 25 [28480/49669]\tLoss: 433.7199\n",
      "Training Epoch: 25 [28544/49669]\tLoss: 410.0084\n",
      "Training Epoch: 25 [28608/49669]\tLoss: 413.4304\n",
      "Training Epoch: 25 [28672/49669]\tLoss: 437.8137\n",
      "Training Epoch: 25 [28736/49669]\tLoss: 415.8008\n",
      "Training Epoch: 25 [28800/49669]\tLoss: 428.9901\n",
      "Training Epoch: 25 [28864/49669]\tLoss: 418.1433\n",
      "Training Epoch: 25 [28928/49669]\tLoss: 421.9140\n",
      "Training Epoch: 25 [28992/49669]\tLoss: 436.3562\n",
      "Training Epoch: 25 [29056/49669]\tLoss: 436.5206\n",
      "Training Epoch: 25 [29120/49669]\tLoss: 405.4163\n",
      "Training Epoch: 25 [29184/49669]\tLoss: 421.4118\n",
      "Training Epoch: 25 [29248/49669]\tLoss: 430.2879\n",
      "Training Epoch: 25 [29312/49669]\tLoss: 408.7757\n",
      "Training Epoch: 25 [29376/49669]\tLoss: 420.1395\n",
      "Training Epoch: 25 [29440/49669]\tLoss: 412.7367\n",
      "Training Epoch: 25 [29504/49669]\tLoss: 389.6911\n",
      "Training Epoch: 25 [29568/49669]\tLoss: 408.1818\n",
      "Training Epoch: 25 [29632/49669]\tLoss: 411.9005\n",
      "Training Epoch: 25 [29696/49669]\tLoss: 438.8467\n",
      "Training Epoch: 25 [29760/49669]\tLoss: 402.1492\n",
      "Training Epoch: 25 [29824/49669]\tLoss: 424.1136\n",
      "Training Epoch: 25 [29888/49669]\tLoss: 440.8864\n",
      "Training Epoch: 25 [29952/49669]\tLoss: 405.5788\n",
      "Training Epoch: 25 [30016/49669]\tLoss: 404.0014\n",
      "Training Epoch: 25 [30080/49669]\tLoss: 397.4533\n",
      "Training Epoch: 25 [30144/49669]\tLoss: 392.0176\n",
      "Training Epoch: 25 [30208/49669]\tLoss: 390.5749\n",
      "Training Epoch: 25 [30272/49669]\tLoss: 395.8020\n",
      "Training Epoch: 25 [30336/49669]\tLoss: 391.4888\n",
      "Training Epoch: 25 [30400/49669]\tLoss: 396.6273\n",
      "Training Epoch: 25 [30464/49669]\tLoss: 402.7836\n",
      "Training Epoch: 25 [30528/49669]\tLoss: 409.1816\n",
      "Training Epoch: 25 [30592/49669]\tLoss: 396.4375\n",
      "Training Epoch: 25 [30656/49669]\tLoss: 405.1338\n",
      "Training Epoch: 25 [30720/49669]\tLoss: 412.3115\n",
      "Training Epoch: 25 [30784/49669]\tLoss: 386.1864\n",
      "Training Epoch: 25 [30848/49669]\tLoss: 412.1765\n",
      "Training Epoch: 25 [30912/49669]\tLoss: 444.2394\n",
      "Training Epoch: 25 [30976/49669]\tLoss: 416.9640\n",
      "Training Epoch: 25 [31040/49669]\tLoss: 417.8726\n",
      "Training Epoch: 25 [31104/49669]\tLoss: 427.9476\n",
      "Training Epoch: 25 [31168/49669]\tLoss: 411.2625\n",
      "Training Epoch: 25 [31232/49669]\tLoss: 396.2088\n",
      "Training Epoch: 25 [31296/49669]\tLoss: 401.6345\n",
      "Training Epoch: 25 [31360/49669]\tLoss: 409.7910\n",
      "Training Epoch: 25 [31424/49669]\tLoss: 392.0100\n",
      "Training Epoch: 25 [31488/49669]\tLoss: 419.9672\n",
      "Training Epoch: 25 [31552/49669]\tLoss: 410.9826\n",
      "Training Epoch: 25 [31616/49669]\tLoss: 397.8481\n",
      "Training Epoch: 25 [31680/49669]\tLoss: 374.8792\n",
      "Training Epoch: 25 [31744/49669]\tLoss: 395.7298\n",
      "Training Epoch: 25 [31808/49669]\tLoss: 428.8018\n",
      "Training Epoch: 25 [31872/49669]\tLoss: 405.3333\n",
      "Training Epoch: 25 [31936/49669]\tLoss: 420.5692\n",
      "Training Epoch: 25 [32000/49669]\tLoss: 390.1117\n",
      "Training Epoch: 25 [32064/49669]\tLoss: 423.2224\n",
      "Training Epoch: 25 [32128/49669]\tLoss: 395.8352\n",
      "Training Epoch: 25 [32192/49669]\tLoss: 399.5257\n",
      "Training Epoch: 25 [32256/49669]\tLoss: 408.5271\n",
      "Training Epoch: 25 [32320/49669]\tLoss: 398.5465\n",
      "Training Epoch: 25 [32384/49669]\tLoss: 413.0052\n",
      "Training Epoch: 25 [32448/49669]\tLoss: 439.5725\n",
      "Training Epoch: 25 [32512/49669]\tLoss: 418.7897\n",
      "Training Epoch: 25 [32576/49669]\tLoss: 371.2698\n",
      "Training Epoch: 25 [32640/49669]\tLoss: 391.6052\n",
      "Training Epoch: 25 [32704/49669]\tLoss: 408.2130\n",
      "Training Epoch: 25 [32768/49669]\tLoss: 409.9483\n",
      "Training Epoch: 25 [32832/49669]\tLoss: 385.1080\n",
      "Training Epoch: 25 [32896/49669]\tLoss: 414.4135\n",
      "Training Epoch: 25 [32960/49669]\tLoss: 410.7207\n",
      "Training Epoch: 25 [33024/49669]\tLoss: 417.6777\n",
      "Training Epoch: 25 [33088/49669]\tLoss: 381.7943\n",
      "Training Epoch: 25 [33152/49669]\tLoss: 432.6047\n",
      "Training Epoch: 25 [33216/49669]\tLoss: 412.4376\n",
      "Training Epoch: 25 [33280/49669]\tLoss: 431.1765\n",
      "Training Epoch: 25 [33344/49669]\tLoss: 397.9321\n",
      "Training Epoch: 25 [33408/49669]\tLoss: 402.1954\n",
      "Training Epoch: 25 [33472/49669]\tLoss: 391.6499\n",
      "Training Epoch: 25 [33536/49669]\tLoss: 414.7818\n",
      "Training Epoch: 25 [33600/49669]\tLoss: 395.1591\n",
      "Training Epoch: 25 [33664/49669]\tLoss: 424.1906\n",
      "Training Epoch: 25 [33728/49669]\tLoss: 426.8277\n",
      "Training Epoch: 25 [33792/49669]\tLoss: 394.6162\n",
      "Training Epoch: 25 [33856/49669]\tLoss: 368.6902\n",
      "Training Epoch: 25 [33920/49669]\tLoss: 424.6567\n",
      "Training Epoch: 25 [33984/49669]\tLoss: 364.9216\n",
      "Training Epoch: 25 [34048/49669]\tLoss: 372.2002\n",
      "Training Epoch: 25 [34112/49669]\tLoss: 430.9374\n",
      "Training Epoch: 25 [34176/49669]\tLoss: 399.0003\n",
      "Training Epoch: 25 [34240/49669]\tLoss: 416.3816\n",
      "Training Epoch: 25 [34304/49669]\tLoss: 385.6158\n",
      "Training Epoch: 25 [34368/49669]\tLoss: 413.6061\n",
      "Training Epoch: 25 [34432/49669]\tLoss: 395.5171\n",
      "Training Epoch: 25 [34496/49669]\tLoss: 410.8518\n",
      "Training Epoch: 25 [34560/49669]\tLoss: 391.1055\n",
      "Training Epoch: 25 [34624/49669]\tLoss: 374.9125\n",
      "Training Epoch: 25 [34688/49669]\tLoss: 397.2675\n",
      "Training Epoch: 25 [34752/49669]\tLoss: 396.4548\n",
      "Training Epoch: 25 [34816/49669]\tLoss: 395.6703\n",
      "Training Epoch: 25 [34880/49669]\tLoss: 373.0546\n",
      "Training Epoch: 25 [34944/49669]\tLoss: 399.2244\n",
      "Training Epoch: 25 [35008/49669]\tLoss: 401.0574\n",
      "Training Epoch: 25 [35072/49669]\tLoss: 388.2887\n",
      "Training Epoch: 25 [35136/49669]\tLoss: 412.9451\n",
      "Training Epoch: 25 [35200/49669]\tLoss: 388.1005\n",
      "Training Epoch: 25 [35264/49669]\tLoss: 401.6711\n",
      "Training Epoch: 25 [35328/49669]\tLoss: 391.9414\n",
      "Training Epoch: 25 [35392/49669]\tLoss: 383.2407\n",
      "Training Epoch: 25 [35456/49669]\tLoss: 373.2003\n",
      "Training Epoch: 25 [35520/49669]\tLoss: 427.0791\n",
      "Training Epoch: 25 [35584/49669]\tLoss: 401.6602\n",
      "Training Epoch: 25 [35648/49669]\tLoss: 401.7474\n",
      "Training Epoch: 25 [35712/49669]\tLoss: 428.8400\n",
      "Training Epoch: 25 [35776/49669]\tLoss: 409.5145\n",
      "Training Epoch: 25 [35840/49669]\tLoss: 381.7230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [35904/49669]\tLoss: 411.3078\n",
      "Training Epoch: 25 [35968/49669]\tLoss: 393.5923\n",
      "Training Epoch: 25 [36032/49669]\tLoss: 381.4935\n",
      "Training Epoch: 25 [36096/49669]\tLoss: 404.2573\n",
      "Training Epoch: 25 [36160/49669]\tLoss: 410.4663\n",
      "Training Epoch: 25 [36224/49669]\tLoss: 396.0224\n",
      "Training Epoch: 25 [36288/49669]\tLoss: 390.6343\n",
      "Training Epoch: 25 [36352/49669]\tLoss: 397.8160\n",
      "Training Epoch: 25 [36416/49669]\tLoss: 390.2706\n",
      "Training Epoch: 25 [36480/49669]\tLoss: 416.1415\n",
      "Training Epoch: 25 [36544/49669]\tLoss: 410.2665\n",
      "Training Epoch: 25 [36608/49669]\tLoss: 427.2034\n",
      "Training Epoch: 25 [36672/49669]\tLoss: 400.0058\n",
      "Training Epoch: 25 [36736/49669]\tLoss: 394.8288\n",
      "Training Epoch: 25 [36800/49669]\tLoss: 404.9187\n",
      "Training Epoch: 25 [36864/49669]\tLoss: 408.6851\n",
      "Training Epoch: 25 [36928/49669]\tLoss: 414.9474\n",
      "Training Epoch: 25 [36992/49669]\tLoss: 416.1478\n",
      "Training Epoch: 25 [37056/49669]\tLoss: 392.1352\n",
      "Training Epoch: 25 [37120/49669]\tLoss: 401.4899\n",
      "Training Epoch: 25 [37184/49669]\tLoss: 387.7636\n",
      "Training Epoch: 25 [37248/49669]\tLoss: 398.5342\n",
      "Training Epoch: 25 [37312/49669]\tLoss: 417.1754\n",
      "Training Epoch: 25 [37376/49669]\tLoss: 394.4199\n",
      "Training Epoch: 25 [37440/49669]\tLoss: 381.7766\n",
      "Training Epoch: 25 [37504/49669]\tLoss: 388.2553\n",
      "Training Epoch: 25 [37568/49669]\tLoss: 403.2517\n",
      "Training Epoch: 25 [37632/49669]\tLoss: 410.8375\n",
      "Training Epoch: 25 [37696/49669]\tLoss: 387.6377\n",
      "Training Epoch: 25 [37760/49669]\tLoss: 380.0077\n",
      "Training Epoch: 25 [37824/49669]\tLoss: 421.9147\n",
      "Training Epoch: 25 [37888/49669]\tLoss: 456.3566\n",
      "Training Epoch: 25 [37952/49669]\tLoss: 380.2875\n",
      "Training Epoch: 25 [38016/49669]\tLoss: 397.9146\n",
      "Training Epoch: 25 [38080/49669]\tLoss: 429.5016\n",
      "Training Epoch: 25 [38144/49669]\tLoss: 391.8072\n",
      "Training Epoch: 25 [38208/49669]\tLoss: 403.4145\n",
      "Training Epoch: 25 [38272/49669]\tLoss: 402.4209\n",
      "Training Epoch: 25 [38336/49669]\tLoss: 392.7823\n",
      "Training Epoch: 25 [38400/49669]\tLoss: 398.5897\n",
      "Training Epoch: 25 [38464/49669]\tLoss: 402.1609\n",
      "Training Epoch: 25 [38528/49669]\tLoss: 412.9131\n",
      "Training Epoch: 25 [38592/49669]\tLoss: 378.3738\n",
      "Training Epoch: 25 [38656/49669]\tLoss: 410.0655\n",
      "Training Epoch: 25 [38720/49669]\tLoss: 421.5807\n",
      "Training Epoch: 25 [38784/49669]\tLoss: 419.6883\n",
      "Training Epoch: 25 [38848/49669]\tLoss: 409.1806\n",
      "Training Epoch: 25 [38912/49669]\tLoss: 425.7797\n",
      "Training Epoch: 25 [38976/49669]\tLoss: 395.7799\n",
      "Training Epoch: 25 [39040/49669]\tLoss: 400.5275\n",
      "Training Epoch: 25 [39104/49669]\tLoss: 413.6658\n",
      "Training Epoch: 25 [39168/49669]\tLoss: 384.6010\n",
      "Training Epoch: 25 [39232/49669]\tLoss: 415.1581\n",
      "Training Epoch: 25 [39296/49669]\tLoss: 404.6236\n",
      "Training Epoch: 25 [39360/49669]\tLoss: 390.0286\n",
      "Training Epoch: 25 [39424/49669]\tLoss: 381.6289\n",
      "Training Epoch: 25 [39488/49669]\tLoss: 389.5573\n",
      "Training Epoch: 25 [39552/49669]\tLoss: 415.0737\n",
      "Training Epoch: 25 [39616/49669]\tLoss: 405.5450\n",
      "Training Epoch: 25 [39680/49669]\tLoss: 381.8654\n",
      "Training Epoch: 25 [39744/49669]\tLoss: 401.0288\n",
      "Training Epoch: 25 [39808/49669]\tLoss: 405.1580\n",
      "Training Epoch: 25 [39872/49669]\tLoss: 362.7181\n",
      "Training Epoch: 25 [39936/49669]\tLoss: 348.2705\n",
      "Training Epoch: 25 [40000/49669]\tLoss: 417.3432\n",
      "Training Epoch: 25 [40064/49669]\tLoss: 358.1466\n",
      "Training Epoch: 25 [40128/49669]\tLoss: 397.6047\n",
      "Training Epoch: 25 [40192/49669]\tLoss: 404.3067\n",
      "Training Epoch: 25 [40256/49669]\tLoss: 397.0676\n",
      "Training Epoch: 25 [40320/49669]\tLoss: 410.5470\n",
      "Training Epoch: 25 [40384/49669]\tLoss: 399.2241\n",
      "Training Epoch: 25 [40448/49669]\tLoss: 379.1437\n",
      "Training Epoch: 25 [40512/49669]\tLoss: 409.7180\n",
      "Training Epoch: 25 [40576/49669]\tLoss: 400.0329\n",
      "Training Epoch: 25 [40640/49669]\tLoss: 376.4372\n",
      "Training Epoch: 25 [40704/49669]\tLoss: 395.8205\n",
      "Training Epoch: 25 [40768/49669]\tLoss: 403.8533\n",
      "Training Epoch: 25 [40832/49669]\tLoss: 421.8716\n",
      "Training Epoch: 25 [40896/49669]\tLoss: 431.7885\n",
      "Training Epoch: 25 [40960/49669]\tLoss: 409.8891\n",
      "Training Epoch: 25 [41024/49669]\tLoss: 383.6411\n",
      "Training Epoch: 25 [41088/49669]\tLoss: 419.5660\n",
      "Training Epoch: 25 [41152/49669]\tLoss: 400.4785\n",
      "Training Epoch: 25 [41216/49669]\tLoss: 383.2742\n",
      "Training Epoch: 25 [41280/49669]\tLoss: 369.4352\n",
      "Training Epoch: 25 [41344/49669]\tLoss: 423.4435\n",
      "Training Epoch: 25 [41408/49669]\tLoss: 384.0604\n",
      "Training Epoch: 25 [41472/49669]\tLoss: 409.4441\n",
      "Training Epoch: 25 [41536/49669]\tLoss: 400.7775\n",
      "Training Epoch: 25 [41600/49669]\tLoss: 424.7770\n",
      "Training Epoch: 25 [41664/49669]\tLoss: 413.7276\n",
      "Training Epoch: 25 [41728/49669]\tLoss: 393.5516\n",
      "Training Epoch: 25 [41792/49669]\tLoss: 389.0193\n",
      "Training Epoch: 25 [41856/49669]\tLoss: 394.2755\n",
      "Training Epoch: 25 [41920/49669]\tLoss: 438.7127\n",
      "Training Epoch: 25 [41984/49669]\tLoss: 400.2073\n",
      "Training Epoch: 25 [42048/49669]\tLoss: 405.9268\n",
      "Training Epoch: 25 [42112/49669]\tLoss: 404.2701\n",
      "Training Epoch: 25 [42176/49669]\tLoss: 423.2330\n",
      "Training Epoch: 25 [42240/49669]\tLoss: 400.6416\n",
      "Training Epoch: 25 [42304/49669]\tLoss: 429.1145\n",
      "Training Epoch: 25 [42368/49669]\tLoss: 372.6295\n",
      "Training Epoch: 25 [42432/49669]\tLoss: 427.8464\n",
      "Training Epoch: 25 [42496/49669]\tLoss: 412.2015\n",
      "Training Epoch: 25 [42560/49669]\tLoss: 410.5989\n",
      "Training Epoch: 25 [42624/49669]\tLoss: 429.6123\n",
      "Training Epoch: 25 [42688/49669]\tLoss: 423.9236\n",
      "Training Epoch: 25 [42752/49669]\tLoss: 415.9852\n",
      "Training Epoch: 25 [42816/49669]\tLoss: 430.8930\n",
      "Training Epoch: 25 [42880/49669]\tLoss: 434.9248\n",
      "Training Epoch: 25 [42944/49669]\tLoss: 412.5489\n",
      "Training Epoch: 25 [43008/49669]\tLoss: 444.1045\n",
      "Training Epoch: 25 [43072/49669]\tLoss: 447.8666\n",
      "Training Epoch: 25 [43136/49669]\tLoss: 411.1307\n",
      "Training Epoch: 25 [43200/49669]\tLoss: 403.7205\n",
      "Training Epoch: 25 [43264/49669]\tLoss: 415.1363\n",
      "Training Epoch: 25 [43328/49669]\tLoss: 397.5926\n",
      "Training Epoch: 25 [43392/49669]\tLoss: 406.0825\n",
      "Training Epoch: 25 [43456/49669]\tLoss: 410.6086\n",
      "Training Epoch: 25 [43520/49669]\tLoss: 414.1302\n",
      "Training Epoch: 25 [43584/49669]\tLoss: 419.1144\n",
      "Training Epoch: 25 [43648/49669]\tLoss: 429.4888\n",
      "Training Epoch: 25 [43712/49669]\tLoss: 402.9373\n",
      "Training Epoch: 25 [43776/49669]\tLoss: 386.2788\n",
      "Training Epoch: 25 [43840/49669]\tLoss: 406.3166\n",
      "Training Epoch: 25 [43904/49669]\tLoss: 408.0276\n",
      "Training Epoch: 25 [43968/49669]\tLoss: 435.7652\n",
      "Training Epoch: 25 [44032/49669]\tLoss: 377.0631\n",
      "Training Epoch: 25 [44096/49669]\tLoss: 390.7111\n",
      "Training Epoch: 25 [44160/49669]\tLoss: 417.9866\n",
      "Training Epoch: 25 [44224/49669]\tLoss: 396.2284\n",
      "Training Epoch: 25 [44288/49669]\tLoss: 397.6309\n",
      "Training Epoch: 25 [44352/49669]\tLoss: 404.9229\n",
      "Training Epoch: 25 [44416/49669]\tLoss: 406.6660\n",
      "Training Epoch: 25 [44480/49669]\tLoss: 438.3087\n",
      "Training Epoch: 25 [44544/49669]\tLoss: 404.3904\n",
      "Training Epoch: 25 [44608/49669]\tLoss: 388.0234\n",
      "Training Epoch: 25 [44672/49669]\tLoss: 389.4601\n",
      "Training Epoch: 25 [44736/49669]\tLoss: 395.9887\n",
      "Training Epoch: 25 [44800/49669]\tLoss: 394.4144\n",
      "Training Epoch: 25 [44864/49669]\tLoss: 416.4946\n",
      "Training Epoch: 25 [44928/49669]\tLoss: 414.7143\n",
      "Training Epoch: 25 [44992/49669]\tLoss: 412.2123\n",
      "Training Epoch: 25 [45056/49669]\tLoss: 396.2880\n",
      "Training Epoch: 25 [45120/49669]\tLoss: 401.5751\n",
      "Training Epoch: 25 [45184/49669]\tLoss: 414.4628\n",
      "Training Epoch: 25 [45248/49669]\tLoss: 413.9634\n",
      "Training Epoch: 25 [45312/49669]\tLoss: 390.7912\n",
      "Training Epoch: 25 [45376/49669]\tLoss: 408.2358\n",
      "Training Epoch: 25 [45440/49669]\tLoss: 407.3146\n",
      "Training Epoch: 25 [45504/49669]\tLoss: 415.7827\n",
      "Training Epoch: 25 [45568/49669]\tLoss: 418.8877\n",
      "Training Epoch: 25 [45632/49669]\tLoss: 396.4940\n",
      "Training Epoch: 25 [45696/49669]\tLoss: 394.9290\n",
      "Training Epoch: 25 [45760/49669]\tLoss: 411.0674\n",
      "Training Epoch: 25 [45824/49669]\tLoss: 389.9495\n",
      "Training Epoch: 25 [45888/49669]\tLoss: 410.8324\n",
      "Training Epoch: 25 [45952/49669]\tLoss: 421.3588\n",
      "Training Epoch: 25 [46016/49669]\tLoss: 407.1461\n",
      "Training Epoch: 25 [46080/49669]\tLoss: 383.5066\n",
      "Training Epoch: 25 [46144/49669]\tLoss: 399.0477\n",
      "Training Epoch: 25 [46208/49669]\tLoss: 412.2199\n",
      "Training Epoch: 25 [46272/49669]\tLoss: 404.2989\n",
      "Training Epoch: 25 [46336/49669]\tLoss: 419.6878\n",
      "Training Epoch: 25 [46400/49669]\tLoss: 424.2309\n",
      "Training Epoch: 25 [46464/49669]\tLoss: 434.8948\n",
      "Training Epoch: 25 [46528/49669]\tLoss: 397.1486\n",
      "Training Epoch: 25 [46592/49669]\tLoss: 412.9884\n",
      "Training Epoch: 25 [46656/49669]\tLoss: 395.4918\n",
      "Training Epoch: 25 [46720/49669]\tLoss: 392.5803\n",
      "Training Epoch: 25 [46784/49669]\tLoss: 404.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [46848/49669]\tLoss: 401.3480\n",
      "Training Epoch: 25 [46912/49669]\tLoss: 398.8952\n",
      "Training Epoch: 25 [46976/49669]\tLoss: 405.3425\n",
      "Training Epoch: 25 [47040/49669]\tLoss: 378.3658\n",
      "Training Epoch: 25 [47104/49669]\tLoss: 413.2639\n",
      "Training Epoch: 25 [47168/49669]\tLoss: 400.4926\n",
      "Training Epoch: 25 [47232/49669]\tLoss: 407.9313\n",
      "Training Epoch: 25 [47296/49669]\tLoss: 395.5142\n",
      "Training Epoch: 25 [47360/49669]\tLoss: 419.9992\n",
      "Training Epoch: 25 [47424/49669]\tLoss: 372.1383\n",
      "Training Epoch: 25 [47488/49669]\tLoss: 384.2719\n",
      "Training Epoch: 25 [47552/49669]\tLoss: 381.2139\n",
      "Training Epoch: 25 [47616/49669]\tLoss: 408.2503\n",
      "Training Epoch: 25 [47680/49669]\tLoss: 398.0858\n",
      "Training Epoch: 25 [47744/49669]\tLoss: 426.5901\n",
      "Training Epoch: 25 [47808/49669]\tLoss: 369.6903\n",
      "Training Epoch: 25 [47872/49669]\tLoss: 432.6960\n",
      "Training Epoch: 25 [47936/49669]\tLoss: 418.0448\n",
      "Training Epoch: 25 [48000/49669]\tLoss: 386.4862\n",
      "Training Epoch: 25 [48064/49669]\tLoss: 386.2552\n",
      "Training Epoch: 25 [48128/49669]\tLoss: 396.7979\n",
      "Training Epoch: 25 [48192/49669]\tLoss: 396.5332\n",
      "Training Epoch: 25 [48256/49669]\tLoss: 418.8834\n",
      "Training Epoch: 25 [48320/49669]\tLoss: 425.0876\n",
      "Training Epoch: 25 [48384/49669]\tLoss: 424.7353\n",
      "Training Epoch: 25 [48448/49669]\tLoss: 429.2838\n",
      "Training Epoch: 25 [48512/49669]\tLoss: 411.9114\n",
      "Training Epoch: 25 [48576/49669]\tLoss: 417.2471\n",
      "Training Epoch: 25 [48640/49669]\tLoss: 379.8990\n",
      "Training Epoch: 25 [48704/49669]\tLoss: 425.0348\n",
      "Training Epoch: 25 [48768/49669]\tLoss: 392.5324\n",
      "Training Epoch: 25 [48832/49669]\tLoss: 437.4685\n",
      "Training Epoch: 25 [48896/49669]\tLoss: 403.6726\n",
      "Training Epoch: 25 [48960/49669]\tLoss: 410.2012\n",
      "Training Epoch: 25 [49024/49669]\tLoss: 400.5386\n",
      "Training Epoch: 25 [49088/49669]\tLoss: 409.0975\n",
      "Training Epoch: 25 [49152/49669]\tLoss: 399.6158\n",
      "Training Epoch: 25 [49216/49669]\tLoss: 402.5089\n",
      "Training Epoch: 25 [49280/49669]\tLoss: 388.9506\n",
      "Training Epoch: 25 [49344/49669]\tLoss: 424.6096\n",
      "Training Epoch: 25 [49408/49669]\tLoss: 457.2530\n",
      "Training Epoch: 25 [49472/49669]\tLoss: 420.5799\n",
      "Training Epoch: 25 [49536/49669]\tLoss: 407.5204\n",
      "Training Epoch: 25 [49600/49669]\tLoss: 402.2025\n",
      "Training Epoch: 25 [49664/49669]\tLoss: 402.4739\n",
      "Training Epoch: 25 [49669/49669]\tLoss: 442.5227\n",
      "Training Epoch: 25 [5519/5519]\tLoss: 456.7248\n",
      "Training Epoch: 26 [64/49669]\tLoss: 457.0264\n",
      "Training Epoch: 26 [128/49669]\tLoss: 522.3747\n",
      "Training Epoch: 26 [192/49669]\tLoss: 538.8102\n",
      "Training Epoch: 26 [256/49669]\tLoss: 584.2375\n",
      "Training Epoch: 26 [320/49669]\tLoss: 558.2186\n",
      "Training Epoch: 26 [384/49669]\tLoss: 447.8636\n",
      "Training Epoch: 26 [448/49669]\tLoss: 405.9008\n",
      "Training Epoch: 26 [512/49669]\tLoss: 403.5400\n",
      "Training Epoch: 26 [576/49669]\tLoss: 497.6588\n",
      "Training Epoch: 26 [640/49669]\tLoss: 446.6790\n",
      "Training Epoch: 26 [704/49669]\tLoss: 393.5916\n",
      "Training Epoch: 26 [768/49669]\tLoss: 461.6471\n",
      "Training Epoch: 26 [832/49669]\tLoss: 485.8857\n",
      "Training Epoch: 26 [896/49669]\tLoss: 511.3146\n",
      "Training Epoch: 26 [960/49669]\tLoss: 473.5571\n",
      "Training Epoch: 26 [1024/49669]\tLoss: 399.8084\n",
      "Training Epoch: 26 [1088/49669]\tLoss: 433.5023\n",
      "Training Epoch: 26 [1152/49669]\tLoss: 434.3687\n",
      "Training Epoch: 26 [1216/49669]\tLoss: 399.0395\n",
      "Training Epoch: 26 [1280/49669]\tLoss: 396.3019\n",
      "Training Epoch: 26 [1344/49669]\tLoss: 408.3593\n",
      "Training Epoch: 26 [1408/49669]\tLoss: 457.4084\n",
      "Training Epoch: 26 [1472/49669]\tLoss: 423.1868\n",
      "Training Epoch: 26 [1536/49669]\tLoss: 399.6359\n",
      "Training Epoch: 26 [1600/49669]\tLoss: 397.6483\n",
      "Training Epoch: 26 [1664/49669]\tLoss: 420.7539\n",
      "Training Epoch: 26 [1728/49669]\tLoss: 415.3595\n",
      "Training Epoch: 26 [1792/49669]\tLoss: 424.3639\n",
      "Training Epoch: 26 [1856/49669]\tLoss: 390.9959\n",
      "Training Epoch: 26 [1920/49669]\tLoss: 446.7869\n",
      "Training Epoch: 26 [1984/49669]\tLoss: 399.1605\n",
      "Training Epoch: 26 [2048/49669]\tLoss: 393.7508\n",
      "Training Epoch: 26 [2112/49669]\tLoss: 400.4716\n",
      "Training Epoch: 26 [2176/49669]\tLoss: 364.7452\n",
      "Training Epoch: 26 [2240/49669]\tLoss: 409.0985\n",
      "Training Epoch: 26 [2304/49669]\tLoss: 397.4677\n",
      "Training Epoch: 26 [2368/49669]\tLoss: 419.9679\n",
      "Training Epoch: 26 [2432/49669]\tLoss: 383.5188\n",
      "Training Epoch: 26 [2496/49669]\tLoss: 419.2253\n",
      "Training Epoch: 26 [2560/49669]\tLoss: 404.3410\n",
      "Training Epoch: 26 [2624/49669]\tLoss: 443.7796\n",
      "Training Epoch: 26 [2688/49669]\tLoss: 397.2380\n",
      "Training Epoch: 26 [2752/49669]\tLoss: 402.1088\n",
      "Training Epoch: 26 [2816/49669]\tLoss: 416.7529\n",
      "Training Epoch: 26 [2880/49669]\tLoss: 411.4741\n",
      "Training Epoch: 26 [2944/49669]\tLoss: 392.5550\n",
      "Training Epoch: 26 [3008/49669]\tLoss: 394.2694\n",
      "Training Epoch: 26 [3072/49669]\tLoss: 406.9548\n",
      "Training Epoch: 26 [3136/49669]\tLoss: 400.3914\n",
      "Training Epoch: 26 [3200/49669]\tLoss: 398.2659\n",
      "Training Epoch: 26 [3264/49669]\tLoss: 430.2306\n",
      "Training Epoch: 26 [3328/49669]\tLoss: 411.7637\n",
      "Training Epoch: 26 [3392/49669]\tLoss: 380.6054\n",
      "Training Epoch: 26 [3456/49669]\tLoss: 417.6909\n",
      "Training Epoch: 26 [3520/49669]\tLoss: 416.1811\n",
      "Training Epoch: 26 [3584/49669]\tLoss: 412.1650\n",
      "Training Epoch: 26 [3648/49669]\tLoss: 397.2274\n",
      "Training Epoch: 26 [3712/49669]\tLoss: 427.6870\n",
      "Training Epoch: 26 [3776/49669]\tLoss: 406.5758\n",
      "Training Epoch: 26 [3840/49669]\tLoss: 408.1240\n",
      "Training Epoch: 26 [3904/49669]\tLoss: 381.0508\n",
      "Training Epoch: 26 [3968/49669]\tLoss: 413.8437\n",
      "Training Epoch: 26 [4032/49669]\tLoss: 366.5943\n",
      "Training Epoch: 26 [4096/49669]\tLoss: 393.6536\n",
      "Training Epoch: 26 [4160/49669]\tLoss: 409.3066\n",
      "Training Epoch: 26 [4224/49669]\tLoss: 397.4237\n",
      "Training Epoch: 26 [4288/49669]\tLoss: 419.4769\n",
      "Training Epoch: 26 [4352/49669]\tLoss: 394.8661\n",
      "Training Epoch: 26 [4416/49669]\tLoss: 399.7332\n",
      "Training Epoch: 26 [4480/49669]\tLoss: 382.5577\n",
      "Training Epoch: 26 [4544/49669]\tLoss: 390.5479\n",
      "Training Epoch: 26 [4608/49669]\tLoss: 400.8168\n",
      "Training Epoch: 26 [4672/49669]\tLoss: 388.3714\n",
      "Training Epoch: 26 [4736/49669]\tLoss: 396.4051\n",
      "Training Epoch: 26 [4800/49669]\tLoss: 409.7742\n",
      "Training Epoch: 26 [4864/49669]\tLoss: 406.9636\n",
      "Training Epoch: 26 [4928/49669]\tLoss: 420.3478\n",
      "Training Epoch: 26 [4992/49669]\tLoss: 385.7606\n",
      "Training Epoch: 26 [5056/49669]\tLoss: 380.2724\n",
      "Training Epoch: 26 [5120/49669]\tLoss: 371.7342\n",
      "Training Epoch: 26 [5184/49669]\tLoss: 390.5381\n",
      "Training Epoch: 26 [5248/49669]\tLoss: 414.6665\n",
      "Training Epoch: 26 [5312/49669]\tLoss: 421.8497\n",
      "Training Epoch: 26 [5376/49669]\tLoss: 402.2950\n",
      "Training Epoch: 26 [5440/49669]\tLoss: 431.8780\n",
      "Training Epoch: 26 [5504/49669]\tLoss: 407.4414\n",
      "Training Epoch: 26 [5568/49669]\tLoss: 366.5439\n",
      "Training Epoch: 26 [5632/49669]\tLoss: 411.4697\n",
      "Training Epoch: 26 [5696/49669]\tLoss: 413.7443\n",
      "Training Epoch: 26 [5760/49669]\tLoss: 405.3791\n",
      "Training Epoch: 26 [5824/49669]\tLoss: 411.0749\n",
      "Training Epoch: 26 [5888/49669]\tLoss: 410.8401\n",
      "Training Epoch: 26 [5952/49669]\tLoss: 389.1581\n",
      "Training Epoch: 26 [6016/49669]\tLoss: 403.8234\n",
      "Training Epoch: 26 [6080/49669]\tLoss: 366.7126\n",
      "Training Epoch: 26 [6144/49669]\tLoss: 397.4029\n",
      "Training Epoch: 26 [6208/49669]\tLoss: 387.8839\n",
      "Training Epoch: 26 [6272/49669]\tLoss: 418.7347\n",
      "Training Epoch: 26 [6336/49669]\tLoss: 389.6169\n",
      "Training Epoch: 26 [6400/49669]\tLoss: 362.9667\n",
      "Training Epoch: 26 [6464/49669]\tLoss: 389.3060\n",
      "Training Epoch: 26 [6528/49669]\tLoss: 391.9152\n",
      "Training Epoch: 26 [6592/49669]\tLoss: 419.3880\n",
      "Training Epoch: 26 [6656/49669]\tLoss: 407.3973\n",
      "Training Epoch: 26 [6720/49669]\tLoss: 387.0855\n",
      "Training Epoch: 26 [6784/49669]\tLoss: 410.0655\n",
      "Training Epoch: 26 [6848/49669]\tLoss: 394.0463\n",
      "Training Epoch: 26 [6912/49669]\tLoss: 397.7794\n",
      "Training Epoch: 26 [6976/49669]\tLoss: 404.8276\n",
      "Training Epoch: 26 [7040/49669]\tLoss: 401.6157\n",
      "Training Epoch: 26 [7104/49669]\tLoss: 390.8748\n",
      "Training Epoch: 26 [7168/49669]\tLoss: 417.9461\n",
      "Training Epoch: 26 [7232/49669]\tLoss: 391.6401\n",
      "Training Epoch: 26 [7296/49669]\tLoss: 413.0844\n",
      "Training Epoch: 26 [7360/49669]\tLoss: 420.2654\n",
      "Training Epoch: 26 [7424/49669]\tLoss: 398.8940\n",
      "Training Epoch: 26 [7488/49669]\tLoss: 377.6609\n",
      "Training Epoch: 26 [7552/49669]\tLoss: 402.1742\n",
      "Training Epoch: 26 [7616/49669]\tLoss: 417.6919\n",
      "Training Epoch: 26 [7680/49669]\tLoss: 416.1811\n",
      "Training Epoch: 26 [7744/49669]\tLoss: 440.3596\n",
      "Training Epoch: 26 [7808/49669]\tLoss: 370.2794\n",
      "Training Epoch: 26 [7872/49669]\tLoss: 404.4699\n",
      "Training Epoch: 26 [7936/49669]\tLoss: 373.5110\n",
      "Training Epoch: 26 [8000/49669]\tLoss: 403.6188\n",
      "Training Epoch: 26 [8064/49669]\tLoss: 403.5883\n",
      "Training Epoch: 26 [8128/49669]\tLoss: 340.1691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [8192/49669]\tLoss: 403.5692\n",
      "Training Epoch: 26 [8256/49669]\tLoss: 420.0362\n",
      "Training Epoch: 26 [8320/49669]\tLoss: 411.9062\n",
      "Training Epoch: 26 [8384/49669]\tLoss: 438.2788\n",
      "Training Epoch: 26 [8448/49669]\tLoss: 392.6800\n",
      "Training Epoch: 26 [8512/49669]\tLoss: 392.0917\n",
      "Training Epoch: 26 [8576/49669]\tLoss: 398.1846\n",
      "Training Epoch: 26 [8640/49669]\tLoss: 396.5370\n",
      "Training Epoch: 26 [8704/49669]\tLoss: 427.4084\n",
      "Training Epoch: 26 [8768/49669]\tLoss: 379.1494\n",
      "Training Epoch: 26 [8832/49669]\tLoss: 393.2917\n",
      "Training Epoch: 26 [8896/49669]\tLoss: 386.0851\n",
      "Training Epoch: 26 [8960/49669]\tLoss: 393.5712\n",
      "Training Epoch: 26 [9024/49669]\tLoss: 393.8239\n",
      "Training Epoch: 26 [9088/49669]\tLoss: 417.8440\n",
      "Training Epoch: 26 [9152/49669]\tLoss: 363.0670\n",
      "Training Epoch: 26 [9216/49669]\tLoss: 425.2934\n",
      "Training Epoch: 26 [9280/49669]\tLoss: 386.9256\n",
      "Training Epoch: 26 [9344/49669]\tLoss: 395.2127\n",
      "Training Epoch: 26 [9408/49669]\tLoss: 375.6985\n",
      "Training Epoch: 26 [9472/49669]\tLoss: 415.1520\n",
      "Training Epoch: 26 [9536/49669]\tLoss: 371.5453\n",
      "Training Epoch: 26 [9600/49669]\tLoss: 402.7626\n",
      "Training Epoch: 26 [9664/49669]\tLoss: 382.7808\n",
      "Training Epoch: 26 [9728/49669]\tLoss: 407.6221\n",
      "Training Epoch: 26 [9792/49669]\tLoss: 421.8799\n",
      "Training Epoch: 26 [9856/49669]\tLoss: 406.0820\n",
      "Training Epoch: 26 [9920/49669]\tLoss: 400.1915\n",
      "Training Epoch: 26 [9984/49669]\tLoss: 400.9229\n",
      "Training Epoch: 26 [10048/49669]\tLoss: 397.1736\n",
      "Training Epoch: 26 [10112/49669]\tLoss: 401.7415\n",
      "Training Epoch: 26 [10176/49669]\tLoss: 423.6487\n",
      "Training Epoch: 26 [10240/49669]\tLoss: 391.0397\n",
      "Training Epoch: 26 [10304/49669]\tLoss: 390.0650\n",
      "Training Epoch: 26 [10368/49669]\tLoss: 415.7787\n",
      "Training Epoch: 26 [10432/49669]\tLoss: 389.6431\n",
      "Training Epoch: 26 [10496/49669]\tLoss: 395.2150\n",
      "Training Epoch: 26 [10560/49669]\tLoss: 403.6543\n",
      "Training Epoch: 26 [10624/49669]\tLoss: 414.7792\n",
      "Training Epoch: 26 [10688/49669]\tLoss: 399.7639\n",
      "Training Epoch: 26 [10752/49669]\tLoss: 465.0532\n",
      "Training Epoch: 26 [10816/49669]\tLoss: 407.9007\n",
      "Training Epoch: 26 [10880/49669]\tLoss: 430.3930\n",
      "Training Epoch: 26 [10944/49669]\tLoss: 376.8858\n",
      "Training Epoch: 26 [11008/49669]\tLoss: 416.1474\n",
      "Training Epoch: 26 [11072/49669]\tLoss: 418.5051\n",
      "Training Epoch: 26 [11136/49669]\tLoss: 420.6640\n",
      "Training Epoch: 26 [11200/49669]\tLoss: 404.5480\n",
      "Training Epoch: 26 [11264/49669]\tLoss: 397.2716\n",
      "Training Epoch: 26 [11328/49669]\tLoss: 403.0311\n",
      "Training Epoch: 26 [11392/49669]\tLoss: 425.1441\n",
      "Training Epoch: 26 [11456/49669]\tLoss: 370.0381\n",
      "Training Epoch: 26 [11520/49669]\tLoss: 404.3934\n",
      "Training Epoch: 26 [11584/49669]\tLoss: 398.8327\n",
      "Training Epoch: 26 [11648/49669]\tLoss: 375.2846\n",
      "Training Epoch: 26 [11712/49669]\tLoss: 406.5016\n",
      "Training Epoch: 26 [11776/49669]\tLoss: 396.0187\n",
      "Training Epoch: 26 [11840/49669]\tLoss: 435.5003\n",
      "Training Epoch: 26 [11904/49669]\tLoss: 433.9913\n",
      "Training Epoch: 26 [11968/49669]\tLoss: 409.8255\n",
      "Training Epoch: 26 [12032/49669]\tLoss: 396.9548\n",
      "Training Epoch: 26 [12096/49669]\tLoss: 418.6127\n",
      "Training Epoch: 26 [12160/49669]\tLoss: 422.7105\n",
      "Training Epoch: 26 [12224/49669]\tLoss: 425.0062\n",
      "Training Epoch: 26 [12288/49669]\tLoss: 386.0805\n",
      "Training Epoch: 26 [12352/49669]\tLoss: 420.8304\n",
      "Training Epoch: 26 [12416/49669]\tLoss: 402.1288\n",
      "Training Epoch: 26 [12480/49669]\tLoss: 417.7872\n",
      "Training Epoch: 26 [12544/49669]\tLoss: 391.2848\n",
      "Training Epoch: 26 [12608/49669]\tLoss: 440.6740\n",
      "Training Epoch: 26 [12672/49669]\tLoss: 400.8972\n",
      "Training Epoch: 26 [12736/49669]\tLoss: 394.7206\n",
      "Training Epoch: 26 [12800/49669]\tLoss: 368.2232\n",
      "Training Epoch: 26 [12864/49669]\tLoss: 426.9495\n",
      "Training Epoch: 26 [12928/49669]\tLoss: 392.2983\n",
      "Training Epoch: 26 [12992/49669]\tLoss: 377.0624\n",
      "Training Epoch: 26 [13056/49669]\tLoss: 421.3668\n",
      "Training Epoch: 26 [13120/49669]\tLoss: 379.8031\n",
      "Training Epoch: 26 [13184/49669]\tLoss: 389.1622\n",
      "Training Epoch: 26 [13248/49669]\tLoss: 419.2009\n",
      "Training Epoch: 26 [13312/49669]\tLoss: 378.8504\n",
      "Training Epoch: 26 [13376/49669]\tLoss: 388.3236\n",
      "Training Epoch: 26 [13440/49669]\tLoss: 415.8237\n",
      "Training Epoch: 26 [13504/49669]\tLoss: 404.6712\n",
      "Training Epoch: 26 [13568/49669]\tLoss: 394.2665\n",
      "Training Epoch: 26 [13632/49669]\tLoss: 381.3267\n",
      "Training Epoch: 26 [13696/49669]\tLoss: 409.1355\n",
      "Training Epoch: 26 [13760/49669]\tLoss: 393.7454\n",
      "Training Epoch: 26 [13824/49669]\tLoss: 389.1592\n",
      "Training Epoch: 26 [13888/49669]\tLoss: 386.4352\n",
      "Training Epoch: 26 [13952/49669]\tLoss: 409.2852\n",
      "Training Epoch: 26 [14016/49669]\tLoss: 426.3505\n",
      "Training Epoch: 26 [14080/49669]\tLoss: 392.9794\n",
      "Training Epoch: 26 [14144/49669]\tLoss: 381.4681\n",
      "Training Epoch: 26 [14208/49669]\tLoss: 446.2406\n",
      "Training Epoch: 26 [14272/49669]\tLoss: 400.8868\n",
      "Training Epoch: 26 [14336/49669]\tLoss: 388.5822\n",
      "Training Epoch: 26 [14400/49669]\tLoss: 367.9611\n",
      "Training Epoch: 26 [14464/49669]\tLoss: 403.0744\n",
      "Training Epoch: 26 [14528/49669]\tLoss: 403.8149\n",
      "Training Epoch: 26 [14592/49669]\tLoss: 380.9744\n",
      "Training Epoch: 26 [14656/49669]\tLoss: 429.2152\n",
      "Training Epoch: 26 [14720/49669]\tLoss: 406.7462\n",
      "Training Epoch: 26 [14784/49669]\tLoss: 386.3880\n",
      "Training Epoch: 26 [14848/49669]\tLoss: 408.2583\n",
      "Training Epoch: 26 [14912/49669]\tLoss: 413.2981\n",
      "Training Epoch: 26 [14976/49669]\tLoss: 389.9568\n",
      "Training Epoch: 26 [15040/49669]\tLoss: 407.6924\n",
      "Training Epoch: 26 [15104/49669]\tLoss: 442.2983\n",
      "Training Epoch: 26 [15168/49669]\tLoss: 372.7522\n",
      "Training Epoch: 26 [15232/49669]\tLoss: 395.5097\n",
      "Training Epoch: 26 [15296/49669]\tLoss: 420.2201\n",
      "Training Epoch: 26 [15360/49669]\tLoss: 394.0308\n",
      "Training Epoch: 26 [15424/49669]\tLoss: 421.4701\n",
      "Training Epoch: 26 [15488/49669]\tLoss: 385.9586\n",
      "Training Epoch: 26 [15552/49669]\tLoss: 425.0229\n",
      "Training Epoch: 26 [15616/49669]\tLoss: 383.2990\n",
      "Training Epoch: 26 [15680/49669]\tLoss: 409.5604\n",
      "Training Epoch: 26 [15744/49669]\tLoss: 397.6258\n",
      "Training Epoch: 26 [15808/49669]\tLoss: 412.7738\n",
      "Training Epoch: 26 [15872/49669]\tLoss: 400.4842\n",
      "Training Epoch: 26 [15936/49669]\tLoss: 397.6003\n",
      "Training Epoch: 26 [16000/49669]\tLoss: 403.1703\n",
      "Training Epoch: 26 [16064/49669]\tLoss: 390.7449\n",
      "Training Epoch: 26 [16128/49669]\tLoss: 364.1813\n",
      "Training Epoch: 26 [16192/49669]\tLoss: 423.0312\n",
      "Training Epoch: 26 [16256/49669]\tLoss: 400.0709\n",
      "Training Epoch: 26 [16320/49669]\tLoss: 390.8120\n",
      "Training Epoch: 26 [16384/49669]\tLoss: 410.7356\n",
      "Training Epoch: 26 [16448/49669]\tLoss: 400.7057\n",
      "Training Epoch: 26 [16512/49669]\tLoss: 395.8951\n",
      "Training Epoch: 26 [16576/49669]\tLoss: 396.1512\n",
      "Training Epoch: 26 [16640/49669]\tLoss: 414.7509\n",
      "Training Epoch: 26 [16704/49669]\tLoss: 395.6345\n",
      "Training Epoch: 26 [16768/49669]\tLoss: 402.1436\n",
      "Training Epoch: 26 [16832/49669]\tLoss: 424.9199\n",
      "Training Epoch: 26 [16896/49669]\tLoss: 409.0477\n",
      "Training Epoch: 26 [16960/49669]\tLoss: 404.6911\n",
      "Training Epoch: 26 [17024/49669]\tLoss: 430.6631\n",
      "Training Epoch: 26 [17088/49669]\tLoss: 381.7300\n",
      "Training Epoch: 26 [17152/49669]\tLoss: 425.4043\n",
      "Training Epoch: 26 [17216/49669]\tLoss: 403.8142\n",
      "Training Epoch: 26 [17280/49669]\tLoss: 399.2480\n",
      "Training Epoch: 26 [17344/49669]\tLoss: 423.4102\n",
      "Training Epoch: 26 [17408/49669]\tLoss: 401.5206\n",
      "Training Epoch: 26 [17472/49669]\tLoss: 406.7154\n",
      "Training Epoch: 26 [17536/49669]\tLoss: 390.3211\n",
      "Training Epoch: 26 [17600/49669]\tLoss: 429.8913\n",
      "Training Epoch: 26 [17664/49669]\tLoss: 384.9148\n",
      "Training Epoch: 26 [17728/49669]\tLoss: 375.7895\n",
      "Training Epoch: 26 [17792/49669]\tLoss: 386.9341\n",
      "Training Epoch: 26 [17856/49669]\tLoss: 383.0893\n",
      "Training Epoch: 26 [17920/49669]\tLoss: 393.0938\n",
      "Training Epoch: 26 [17984/49669]\tLoss: 395.2623\n",
      "Training Epoch: 26 [18048/49669]\tLoss: 439.1422\n",
      "Training Epoch: 26 [18112/49669]\tLoss: 397.1086\n",
      "Training Epoch: 26 [18176/49669]\tLoss: 426.8320\n",
      "Training Epoch: 26 [18240/49669]\tLoss: 411.1703\n",
      "Training Epoch: 26 [18304/49669]\tLoss: 404.0410\n",
      "Training Epoch: 26 [18368/49669]\tLoss: 405.5265\n",
      "Training Epoch: 26 [18432/49669]\tLoss: 371.4696\n",
      "Training Epoch: 26 [18496/49669]\tLoss: 391.2417\n",
      "Training Epoch: 26 [18560/49669]\tLoss: 413.6334\n",
      "Training Epoch: 26 [18624/49669]\tLoss: 401.6652\n",
      "Training Epoch: 26 [18688/49669]\tLoss: 401.0746\n",
      "Training Epoch: 26 [18752/49669]\tLoss: 423.5103\n",
      "Training Epoch: 26 [18816/49669]\tLoss: 409.8911\n",
      "Training Epoch: 26 [18880/49669]\tLoss: 380.9304\n",
      "Training Epoch: 26 [18944/49669]\tLoss: 384.8320\n",
      "Training Epoch: 26 [19008/49669]\tLoss: 412.1784\n",
      "Training Epoch: 26 [19072/49669]\tLoss: 417.7824\n",
      "Training Epoch: 26 [19136/49669]\tLoss: 396.5413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [19200/49669]\tLoss: 396.3096\n",
      "Training Epoch: 26 [19264/49669]\tLoss: 421.4970\n",
      "Training Epoch: 26 [19328/49669]\tLoss: 416.0515\n",
      "Training Epoch: 26 [19392/49669]\tLoss: 409.9872\n",
      "Training Epoch: 26 [19456/49669]\tLoss: 411.4631\n",
      "Training Epoch: 26 [19520/49669]\tLoss: 399.7210\n",
      "Training Epoch: 26 [19584/49669]\tLoss: 406.4860\n",
      "Training Epoch: 26 [19648/49669]\tLoss: 383.4542\n",
      "Training Epoch: 26 [19712/49669]\tLoss: 418.4103\n",
      "Training Epoch: 26 [19776/49669]\tLoss: 421.9182\n",
      "Training Epoch: 26 [19840/49669]\tLoss: 405.6413\n",
      "Training Epoch: 26 [19904/49669]\tLoss: 381.1359\n",
      "Training Epoch: 26 [19968/49669]\tLoss: 382.9170\n",
      "Training Epoch: 26 [20032/49669]\tLoss: 400.3184\n",
      "Training Epoch: 26 [20096/49669]\tLoss: 397.1067\n",
      "Training Epoch: 26 [20160/49669]\tLoss: 417.9089\n",
      "Training Epoch: 26 [20224/49669]\tLoss: 394.2613\n",
      "Training Epoch: 26 [20288/49669]\tLoss: 366.8062\n",
      "Training Epoch: 26 [20352/49669]\tLoss: 413.9185\n",
      "Training Epoch: 26 [20416/49669]\tLoss: 412.1227\n",
      "Training Epoch: 26 [20480/49669]\tLoss: 371.9170\n",
      "Training Epoch: 26 [20544/49669]\tLoss: 384.9016\n",
      "Training Epoch: 26 [20608/49669]\tLoss: 405.4366\n",
      "Training Epoch: 26 [20672/49669]\tLoss: 446.4158\n",
      "Training Epoch: 26 [20736/49669]\tLoss: 414.5826\n",
      "Training Epoch: 26 [20800/49669]\tLoss: 412.8403\n",
      "Training Epoch: 26 [20864/49669]\tLoss: 390.2378\n",
      "Training Epoch: 26 [20928/49669]\tLoss: 387.5948\n",
      "Training Epoch: 26 [20992/49669]\tLoss: 401.8156\n",
      "Training Epoch: 26 [21056/49669]\tLoss: 400.6872\n",
      "Training Epoch: 26 [21120/49669]\tLoss: 381.7493\n",
      "Training Epoch: 26 [21184/49669]\tLoss: 399.0862\n",
      "Training Epoch: 26 [21248/49669]\tLoss: 369.4218\n",
      "Training Epoch: 26 [21312/49669]\tLoss: 396.1559\n",
      "Training Epoch: 26 [21376/49669]\tLoss: 420.2451\n",
      "Training Epoch: 26 [21440/49669]\tLoss: 403.6825\n",
      "Training Epoch: 26 [21504/49669]\tLoss: 419.9823\n",
      "Training Epoch: 26 [21568/49669]\tLoss: 401.1850\n",
      "Training Epoch: 26 [21632/49669]\tLoss: 398.5717\n",
      "Training Epoch: 26 [21696/49669]\tLoss: 428.6586\n",
      "Training Epoch: 26 [21760/49669]\tLoss: 416.8750\n",
      "Training Epoch: 26 [21824/49669]\tLoss: 387.6236\n",
      "Training Epoch: 26 [21888/49669]\tLoss: 418.4568\n",
      "Training Epoch: 26 [21952/49669]\tLoss: 416.2219\n",
      "Training Epoch: 26 [22016/49669]\tLoss: 385.9318\n",
      "Training Epoch: 26 [22080/49669]\tLoss: 421.7691\n",
      "Training Epoch: 26 [22144/49669]\tLoss: 412.4337\n",
      "Training Epoch: 26 [22208/49669]\tLoss: 409.3450\n",
      "Training Epoch: 26 [22272/49669]\tLoss: 420.2425\n",
      "Training Epoch: 26 [22336/49669]\tLoss: 410.4795\n",
      "Training Epoch: 26 [22400/49669]\tLoss: 424.8764\n",
      "Training Epoch: 26 [22464/49669]\tLoss: 398.6301\n",
      "Training Epoch: 26 [22528/49669]\tLoss: 396.9864\n",
      "Training Epoch: 26 [22592/49669]\tLoss: 406.3833\n",
      "Training Epoch: 26 [22656/49669]\tLoss: 423.1515\n",
      "Training Epoch: 26 [22720/49669]\tLoss: 396.6107\n",
      "Training Epoch: 26 [22784/49669]\tLoss: 394.5988\n",
      "Training Epoch: 26 [22848/49669]\tLoss: 389.6531\n",
      "Training Epoch: 26 [22912/49669]\tLoss: 388.4391\n",
      "Training Epoch: 26 [22976/49669]\tLoss: 420.4350\n",
      "Training Epoch: 26 [23040/49669]\tLoss: 426.3532\n",
      "Training Epoch: 26 [23104/49669]\tLoss: 412.2338\n",
      "Training Epoch: 26 [23168/49669]\tLoss: 419.4256\n",
      "Training Epoch: 26 [23232/49669]\tLoss: 411.3884\n",
      "Training Epoch: 26 [23296/49669]\tLoss: 404.1778\n",
      "Training Epoch: 26 [23360/49669]\tLoss: 365.0883\n",
      "Training Epoch: 26 [23424/49669]\tLoss: 398.3144\n",
      "Training Epoch: 26 [23488/49669]\tLoss: 404.4375\n",
      "Training Epoch: 26 [23552/49669]\tLoss: 398.3096\n",
      "Training Epoch: 26 [23616/49669]\tLoss: 399.4901\n",
      "Training Epoch: 26 [23680/49669]\tLoss: 411.5581\n",
      "Training Epoch: 26 [23744/49669]\tLoss: 389.1922\n",
      "Training Epoch: 26 [23808/49669]\tLoss: 386.5339\n",
      "Training Epoch: 26 [23872/49669]\tLoss: 410.1053\n",
      "Training Epoch: 26 [23936/49669]\tLoss: 406.1079\n",
      "Training Epoch: 26 [24000/49669]\tLoss: 392.7621\n",
      "Training Epoch: 26 [24064/49669]\tLoss: 405.4207\n",
      "Training Epoch: 26 [24128/49669]\tLoss: 385.8478\n",
      "Training Epoch: 26 [24192/49669]\tLoss: 386.1077\n",
      "Training Epoch: 26 [24256/49669]\tLoss: 394.3429\n",
      "Training Epoch: 26 [24320/49669]\tLoss: 399.4612\n",
      "Training Epoch: 26 [24384/49669]\tLoss: 397.8678\n",
      "Training Epoch: 26 [24448/49669]\tLoss: 413.3979\n",
      "Training Epoch: 26 [24512/49669]\tLoss: 401.0266\n",
      "Training Epoch: 26 [24576/49669]\tLoss: 409.2324\n",
      "Training Epoch: 26 [24640/49669]\tLoss: 400.8510\n",
      "Training Epoch: 26 [24704/49669]\tLoss: 417.6529\n",
      "Training Epoch: 26 [24768/49669]\tLoss: 427.6310\n",
      "Training Epoch: 26 [24832/49669]\tLoss: 413.5793\n",
      "Training Epoch: 26 [24896/49669]\tLoss: 430.4447\n",
      "Training Epoch: 26 [24960/49669]\tLoss: 421.9252\n",
      "Training Epoch: 26 [25024/49669]\tLoss: 422.5368\n",
      "Training Epoch: 26 [25088/49669]\tLoss: 393.0646\n",
      "Training Epoch: 26 [25152/49669]\tLoss: 411.8412\n",
      "Training Epoch: 26 [25216/49669]\tLoss: 442.4776\n",
      "Training Epoch: 26 [25280/49669]\tLoss: 434.1691\n",
      "Training Epoch: 26 [25344/49669]\tLoss: 405.8393\n",
      "Training Epoch: 26 [25408/49669]\tLoss: 398.7124\n",
      "Training Epoch: 26 [25472/49669]\tLoss: 433.9758\n",
      "Training Epoch: 26 [25536/49669]\tLoss: 400.7150\n",
      "Training Epoch: 26 [25600/49669]\tLoss: 358.1954\n",
      "Training Epoch: 26 [25664/49669]\tLoss: 394.9492\n",
      "Training Epoch: 26 [25728/49669]\tLoss: 398.7006\n",
      "Training Epoch: 26 [25792/49669]\tLoss: 399.9818\n",
      "Training Epoch: 26 [25856/49669]\tLoss: 392.8618\n",
      "Training Epoch: 26 [25920/49669]\tLoss: 402.4041\n",
      "Training Epoch: 26 [25984/49669]\tLoss: 419.0231\n",
      "Training Epoch: 26 [26048/49669]\tLoss: 410.3251\n",
      "Training Epoch: 26 [26112/49669]\tLoss: 382.7179\n",
      "Training Epoch: 26 [26176/49669]\tLoss: 398.3417\n",
      "Training Epoch: 26 [26240/49669]\tLoss: 399.8036\n",
      "Training Epoch: 26 [26304/49669]\tLoss: 374.4724\n",
      "Training Epoch: 26 [26368/49669]\tLoss: 410.4644\n",
      "Training Epoch: 26 [26432/49669]\tLoss: 428.1963\n",
      "Training Epoch: 26 [26496/49669]\tLoss: 426.4799\n",
      "Training Epoch: 26 [26560/49669]\tLoss: 411.7499\n",
      "Training Epoch: 26 [26624/49669]\tLoss: 406.0197\n",
      "Training Epoch: 26 [26688/49669]\tLoss: 419.3495\n",
      "Training Epoch: 26 [26752/49669]\tLoss: 409.3942\n",
      "Training Epoch: 26 [26816/49669]\tLoss: 407.8948\n",
      "Training Epoch: 26 [26880/49669]\tLoss: 418.0847\n",
      "Training Epoch: 26 [26944/49669]\tLoss: 404.7653\n",
      "Training Epoch: 26 [27008/49669]\tLoss: 386.9103\n",
      "Training Epoch: 26 [27072/49669]\tLoss: 397.1692\n",
      "Training Epoch: 26 [27136/49669]\tLoss: 395.0179\n",
      "Training Epoch: 26 [27200/49669]\tLoss: 373.5903\n",
      "Training Epoch: 26 [27264/49669]\tLoss: 389.8177\n",
      "Training Epoch: 26 [27328/49669]\tLoss: 408.0729\n",
      "Training Epoch: 26 [27392/49669]\tLoss: 421.4763\n",
      "Training Epoch: 26 [27456/49669]\tLoss: 405.1547\n",
      "Training Epoch: 26 [27520/49669]\tLoss: 412.0212\n",
      "Training Epoch: 26 [27584/49669]\tLoss: 401.5265\n",
      "Training Epoch: 26 [27648/49669]\tLoss: 426.5797\n",
      "Training Epoch: 26 [27712/49669]\tLoss: 401.5042\n",
      "Training Epoch: 26 [27776/49669]\tLoss: 417.7521\n",
      "Training Epoch: 26 [27840/49669]\tLoss: 417.2115\n",
      "Training Epoch: 26 [27904/49669]\tLoss: 396.6675\n",
      "Training Epoch: 26 [27968/49669]\tLoss: 413.0345\n",
      "Training Epoch: 26 [28032/49669]\tLoss: 387.9357\n",
      "Training Epoch: 26 [28096/49669]\tLoss: 419.3074\n",
      "Training Epoch: 26 [28160/49669]\tLoss: 399.3454\n",
      "Training Epoch: 26 [28224/49669]\tLoss: 403.5730\n",
      "Training Epoch: 26 [28288/49669]\tLoss: 403.8197\n",
      "Training Epoch: 26 [28352/49669]\tLoss: 407.1276\n",
      "Training Epoch: 26 [28416/49669]\tLoss: 398.9484\n",
      "Training Epoch: 26 [28480/49669]\tLoss: 386.6546\n",
      "Training Epoch: 26 [28544/49669]\tLoss: 390.3744\n",
      "Training Epoch: 26 [28608/49669]\tLoss: 400.0889\n",
      "Training Epoch: 26 [28672/49669]\tLoss: 444.4425\n",
      "Training Epoch: 26 [28736/49669]\tLoss: 415.1527\n",
      "Training Epoch: 26 [28800/49669]\tLoss: 401.8481\n",
      "Training Epoch: 26 [28864/49669]\tLoss: 409.5889\n",
      "Training Epoch: 26 [28928/49669]\tLoss: 400.6061\n",
      "Training Epoch: 26 [28992/49669]\tLoss: 399.3857\n",
      "Training Epoch: 26 [29056/49669]\tLoss: 403.1947\n",
      "Training Epoch: 26 [29120/49669]\tLoss: 391.9772\n",
      "Training Epoch: 26 [29184/49669]\tLoss: 406.0075\n",
      "Training Epoch: 26 [29248/49669]\tLoss: 393.8666\n",
      "Training Epoch: 26 [29312/49669]\tLoss: 383.0759\n",
      "Training Epoch: 26 [29376/49669]\tLoss: 434.6566\n",
      "Training Epoch: 26 [29440/49669]\tLoss: 422.2729\n",
      "Training Epoch: 26 [29504/49669]\tLoss: 402.4920\n",
      "Training Epoch: 26 [29568/49669]\tLoss: 394.7137\n",
      "Training Epoch: 26 [29632/49669]\tLoss: 417.9883\n",
      "Training Epoch: 26 [29696/49669]\tLoss: 407.5956\n",
      "Training Epoch: 26 [29760/49669]\tLoss: 397.1526\n",
      "Training Epoch: 26 [29824/49669]\tLoss: 410.6174\n",
      "Training Epoch: 26 [29888/49669]\tLoss: 403.0796\n",
      "Training Epoch: 26 [29952/49669]\tLoss: 401.3640\n",
      "Training Epoch: 26 [30016/49669]\tLoss: 399.9057\n",
      "Training Epoch: 26 [30080/49669]\tLoss: 385.2085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [30144/49669]\tLoss: 375.6632\n",
      "Training Epoch: 26 [30208/49669]\tLoss: 391.1133\n",
      "Training Epoch: 26 [30272/49669]\tLoss: 392.0906\n",
      "Training Epoch: 26 [30336/49669]\tLoss: 397.8862\n",
      "Training Epoch: 26 [30400/49669]\tLoss: 410.2973\n",
      "Training Epoch: 26 [30464/49669]\tLoss: 427.6750\n",
      "Training Epoch: 26 [30528/49669]\tLoss: 396.4009\n",
      "Training Epoch: 26 [30592/49669]\tLoss: 386.6401\n",
      "Training Epoch: 26 [30656/49669]\tLoss: 425.5139\n",
      "Training Epoch: 26 [30720/49669]\tLoss: 402.1587\n",
      "Training Epoch: 26 [30784/49669]\tLoss: 432.9281\n",
      "Training Epoch: 26 [30848/49669]\tLoss: 434.7452\n",
      "Training Epoch: 26 [30912/49669]\tLoss: 430.6795\n",
      "Training Epoch: 26 [30976/49669]\tLoss: 458.2134\n",
      "Training Epoch: 26 [31040/49669]\tLoss: 454.8648\n",
      "Training Epoch: 26 [31104/49669]\tLoss: 427.4465\n",
      "Training Epoch: 26 [31168/49669]\tLoss: 417.7398\n",
      "Training Epoch: 26 [31232/49669]\tLoss: 456.5778\n",
      "Training Epoch: 26 [31296/49669]\tLoss: 403.7846\n",
      "Training Epoch: 26 [31360/49669]\tLoss: 433.6093\n",
      "Training Epoch: 26 [31424/49669]\tLoss: 417.1642\n",
      "Training Epoch: 26 [31488/49669]\tLoss: 405.4597\n",
      "Training Epoch: 26 [31552/49669]\tLoss: 400.2611\n",
      "Training Epoch: 26 [31616/49669]\tLoss: 411.4964\n",
      "Training Epoch: 26 [31680/49669]\tLoss: 398.2332\n",
      "Training Epoch: 26 [31744/49669]\tLoss: 394.7145\n",
      "Training Epoch: 26 [31808/49669]\tLoss: 408.8479\n",
      "Training Epoch: 26 [31872/49669]\tLoss: 385.7652\n",
      "Training Epoch: 26 [31936/49669]\tLoss: 401.8152\n",
      "Training Epoch: 26 [32000/49669]\tLoss: 427.5559\n",
      "Training Epoch: 26 [32064/49669]\tLoss: 422.5815\n",
      "Training Epoch: 26 [32128/49669]\tLoss: 375.8011\n",
      "Training Epoch: 26 [32192/49669]\tLoss: 426.3882\n",
      "Training Epoch: 26 [32256/49669]\tLoss: 403.4774\n",
      "Training Epoch: 26 [32320/49669]\tLoss: 383.7531\n",
      "Training Epoch: 26 [32384/49669]\tLoss: 409.8069\n",
      "Training Epoch: 26 [32448/49669]\tLoss: 405.4000\n",
      "Training Epoch: 26 [32512/49669]\tLoss: 392.5461\n",
      "Training Epoch: 26 [32576/49669]\tLoss: 374.5337\n",
      "Training Epoch: 26 [32640/49669]\tLoss: 411.8665\n",
      "Training Epoch: 26 [32704/49669]\tLoss: 376.8084\n",
      "Training Epoch: 26 [32768/49669]\tLoss: 386.8641\n",
      "Training Epoch: 26 [32832/49669]\tLoss: 389.7018\n",
      "Training Epoch: 26 [32896/49669]\tLoss: 383.2661\n",
      "Training Epoch: 26 [32960/49669]\tLoss: 403.7485\n",
      "Training Epoch: 26 [33024/49669]\tLoss: 385.3155\n",
      "Training Epoch: 26 [33088/49669]\tLoss: 393.1574\n",
      "Training Epoch: 26 [33152/49669]\tLoss: 439.8577\n",
      "Training Epoch: 26 [33216/49669]\tLoss: 426.1696\n",
      "Training Epoch: 26 [33280/49669]\tLoss: 378.0991\n",
      "Training Epoch: 26 [33344/49669]\tLoss: 416.0280\n",
      "Training Epoch: 26 [33408/49669]\tLoss: 409.4550\n",
      "Training Epoch: 26 [33472/49669]\tLoss: 414.4885\n",
      "Training Epoch: 26 [33536/49669]\tLoss: 398.5916\n",
      "Training Epoch: 26 [33600/49669]\tLoss: 386.6433\n",
      "Training Epoch: 26 [33664/49669]\tLoss: 406.6357\n",
      "Training Epoch: 26 [33728/49669]\tLoss: 408.4152\n",
      "Training Epoch: 26 [33792/49669]\tLoss: 405.0903\n",
      "Training Epoch: 26 [33856/49669]\tLoss: 410.7078\n",
      "Training Epoch: 26 [33920/49669]\tLoss: 423.1527\n",
      "Training Epoch: 26 [33984/49669]\tLoss: 401.8044\n",
      "Training Epoch: 26 [34048/49669]\tLoss: 408.1150\n",
      "Training Epoch: 26 [34112/49669]\tLoss: 400.9112\n",
      "Training Epoch: 26 [34176/49669]\tLoss: 389.5464\n",
      "Training Epoch: 26 [34240/49669]\tLoss: 398.8897\n",
      "Training Epoch: 26 [34304/49669]\tLoss: 415.5471\n",
      "Training Epoch: 26 [34368/49669]\tLoss: 401.1996\n",
      "Training Epoch: 26 [34432/49669]\tLoss: 404.4181\n",
      "Training Epoch: 26 [34496/49669]\tLoss: 415.1346\n",
      "Training Epoch: 26 [34560/49669]\tLoss: 375.9101\n",
      "Training Epoch: 26 [34624/49669]\tLoss: 388.6385\n",
      "Training Epoch: 26 [34688/49669]\tLoss: 424.4758\n",
      "Training Epoch: 26 [34752/49669]\tLoss: 410.8029\n",
      "Training Epoch: 26 [34816/49669]\tLoss: 398.7794\n",
      "Training Epoch: 26 [34880/49669]\tLoss: 363.0013\n",
      "Training Epoch: 26 [34944/49669]\tLoss: 396.9452\n",
      "Training Epoch: 26 [35008/49669]\tLoss: 413.8595\n",
      "Training Epoch: 26 [35072/49669]\tLoss: 391.4771\n",
      "Training Epoch: 26 [35136/49669]\tLoss: 396.0138\n",
      "Training Epoch: 26 [35200/49669]\tLoss: 420.3487\n",
      "Training Epoch: 26 [35264/49669]\tLoss: 400.1529\n",
      "Training Epoch: 26 [35328/49669]\tLoss: 376.9414\n",
      "Training Epoch: 26 [35392/49669]\tLoss: 405.5286\n",
      "Training Epoch: 26 [35456/49669]\tLoss: 429.1629\n",
      "Training Epoch: 26 [35520/49669]\tLoss: 391.7399\n",
      "Training Epoch: 26 [35584/49669]\tLoss: 411.0541\n",
      "Training Epoch: 26 [35648/49669]\tLoss: 396.3240\n",
      "Training Epoch: 26 [35712/49669]\tLoss: 403.8418\n",
      "Training Epoch: 26 [35776/49669]\tLoss: 409.5752\n",
      "Training Epoch: 26 [35840/49669]\tLoss: 418.2302\n",
      "Training Epoch: 26 [35904/49669]\tLoss: 414.6615\n",
      "Training Epoch: 26 [35968/49669]\tLoss: 412.6581\n",
      "Training Epoch: 26 [36032/49669]\tLoss: 415.5641\n",
      "Training Epoch: 26 [36096/49669]\tLoss: 401.0014\n",
      "Training Epoch: 26 [36160/49669]\tLoss: 414.0832\n",
      "Training Epoch: 26 [36224/49669]\tLoss: 397.0721\n",
      "Training Epoch: 26 [36288/49669]\tLoss: 415.6316\n",
      "Training Epoch: 26 [36352/49669]\tLoss: 367.1191\n",
      "Training Epoch: 26 [36416/49669]\tLoss: 398.8626\n",
      "Training Epoch: 26 [36480/49669]\tLoss: 408.0979\n",
      "Training Epoch: 26 [36544/49669]\tLoss: 393.6964\n",
      "Training Epoch: 26 [36608/49669]\tLoss: 406.9618\n",
      "Training Epoch: 26 [36672/49669]\tLoss: 398.8880\n",
      "Training Epoch: 26 [36736/49669]\tLoss: 403.8862\n",
      "Training Epoch: 26 [36800/49669]\tLoss: 380.9016\n",
      "Training Epoch: 26 [36864/49669]\tLoss: 398.8358\n",
      "Training Epoch: 26 [36928/49669]\tLoss: 418.7336\n",
      "Training Epoch: 26 [36992/49669]\tLoss: 394.1682\n",
      "Training Epoch: 26 [37056/49669]\tLoss: 414.2235\n",
      "Training Epoch: 26 [37120/49669]\tLoss: 409.3741\n",
      "Training Epoch: 26 [37184/49669]\tLoss: 403.0864\n",
      "Training Epoch: 26 [37248/49669]\tLoss: 402.0864\n",
      "Training Epoch: 26 [37312/49669]\tLoss: 402.0682\n",
      "Training Epoch: 26 [37376/49669]\tLoss: 441.9810\n",
      "Training Epoch: 26 [37440/49669]\tLoss: 401.1718\n",
      "Training Epoch: 26 [37504/49669]\tLoss: 433.5046\n",
      "Training Epoch: 26 [37568/49669]\tLoss: 397.7291\n",
      "Training Epoch: 26 [37632/49669]\tLoss: 426.9246\n",
      "Training Epoch: 26 [37696/49669]\tLoss: 420.2483\n",
      "Training Epoch: 26 [37760/49669]\tLoss: 425.4420\n",
      "Training Epoch: 26 [37824/49669]\tLoss: 406.6889\n",
      "Training Epoch: 26 [37888/49669]\tLoss: 424.8304\n",
      "Training Epoch: 26 [37952/49669]\tLoss: 444.7487\n",
      "Training Epoch: 26 [38016/49669]\tLoss: 426.4070\n",
      "Training Epoch: 26 [38080/49669]\tLoss: 430.0407\n",
      "Training Epoch: 26 [38144/49669]\tLoss: 424.4037\n",
      "Training Epoch: 26 [38208/49669]\tLoss: 396.5341\n",
      "Training Epoch: 26 [38272/49669]\tLoss: 389.3227\n",
      "Training Epoch: 26 [38336/49669]\tLoss: 406.0099\n",
      "Training Epoch: 26 [38400/49669]\tLoss: 400.7613\n",
      "Training Epoch: 26 [38464/49669]\tLoss: 455.7563\n",
      "Training Epoch: 26 [38528/49669]\tLoss: 453.8932\n",
      "Training Epoch: 26 [38592/49669]\tLoss: 504.7698\n",
      "Training Epoch: 26 [38656/49669]\tLoss: 469.3505\n",
      "Training Epoch: 26 [38720/49669]\tLoss: 421.0367\n",
      "Training Epoch: 26 [38784/49669]\tLoss: 397.9148\n",
      "Training Epoch: 26 [38848/49669]\tLoss: 404.2702\n",
      "Training Epoch: 26 [38912/49669]\tLoss: 403.2054\n",
      "Training Epoch: 26 [38976/49669]\tLoss: 404.5659\n",
      "Training Epoch: 26 [39040/49669]\tLoss: 400.6583\n",
      "Training Epoch: 26 [39104/49669]\tLoss: 389.0025\n",
      "Training Epoch: 26 [39168/49669]\tLoss: 422.0400\n",
      "Training Epoch: 26 [39232/49669]\tLoss: 411.9481\n",
      "Training Epoch: 26 [39296/49669]\tLoss: 401.3558\n",
      "Training Epoch: 26 [39360/49669]\tLoss: 437.9308\n",
      "Training Epoch: 26 [39424/49669]\tLoss: 393.0154\n",
      "Training Epoch: 26 [39488/49669]\tLoss: 417.2870\n",
      "Training Epoch: 26 [39552/49669]\tLoss: 407.8958\n",
      "Training Epoch: 26 [39616/49669]\tLoss: 412.3073\n",
      "Training Epoch: 26 [39680/49669]\tLoss: 416.7519\n",
      "Training Epoch: 26 [39744/49669]\tLoss: 413.2592\n",
      "Training Epoch: 26 [39808/49669]\tLoss: 376.8798\n",
      "Training Epoch: 26 [39872/49669]\tLoss: 371.0852\n",
      "Training Epoch: 26 [39936/49669]\tLoss: 411.4897\n",
      "Training Epoch: 26 [40000/49669]\tLoss: 415.1321\n",
      "Training Epoch: 26 [40064/49669]\tLoss: 399.5840\n",
      "Training Epoch: 26 [40128/49669]\tLoss: 405.9891\n",
      "Training Epoch: 26 [40192/49669]\tLoss: 362.5056\n",
      "Training Epoch: 26 [40256/49669]\tLoss: 388.5511\n",
      "Training Epoch: 26 [40320/49669]\tLoss: 407.8353\n",
      "Training Epoch: 26 [40384/49669]\tLoss: 415.6082\n",
      "Training Epoch: 26 [40448/49669]\tLoss: 414.7573\n",
      "Training Epoch: 26 [40512/49669]\tLoss: 452.2540\n",
      "Training Epoch: 26 [40576/49669]\tLoss: 408.3870\n",
      "Training Epoch: 26 [40640/49669]\tLoss: 394.0239\n",
      "Training Epoch: 26 [40704/49669]\tLoss: 394.6521\n",
      "Training Epoch: 26 [40768/49669]\tLoss: 390.3473\n",
      "Training Epoch: 26 [40832/49669]\tLoss: 410.9681\n",
      "Training Epoch: 26 [40896/49669]\tLoss: 386.3756\n",
      "Training Epoch: 26 [40960/49669]\tLoss: 378.1394\n",
      "Training Epoch: 26 [41024/49669]\tLoss: 439.0530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [41088/49669]\tLoss: 396.0349\n",
      "Training Epoch: 26 [41152/49669]\tLoss: 378.8180\n",
      "Training Epoch: 26 [41216/49669]\tLoss: 391.2385\n",
      "Training Epoch: 26 [41280/49669]\tLoss: 421.2629\n",
      "Training Epoch: 26 [41344/49669]\tLoss: 413.5082\n",
      "Training Epoch: 26 [41408/49669]\tLoss: 410.9133\n",
      "Training Epoch: 26 [41472/49669]\tLoss: 403.5182\n",
      "Training Epoch: 26 [41536/49669]\tLoss: 387.5939\n",
      "Training Epoch: 26 [41600/49669]\tLoss: 379.6458\n",
      "Training Epoch: 26 [41664/49669]\tLoss: 399.4780\n",
      "Training Epoch: 26 [41728/49669]\tLoss: 392.1933\n",
      "Training Epoch: 26 [41792/49669]\tLoss: 411.8184\n",
      "Training Epoch: 26 [41856/49669]\tLoss: 400.1744\n",
      "Training Epoch: 26 [41920/49669]\tLoss: 411.4521\n",
      "Training Epoch: 26 [41984/49669]\tLoss: 393.4761\n",
      "Training Epoch: 26 [42048/49669]\tLoss: 427.6860\n",
      "Training Epoch: 26 [42112/49669]\tLoss: 376.7860\n",
      "Training Epoch: 26 [42176/49669]\tLoss: 386.3460\n",
      "Training Epoch: 26 [42240/49669]\tLoss: 416.3235\n",
      "Training Epoch: 26 [42304/49669]\tLoss: 386.7466\n",
      "Training Epoch: 26 [42368/49669]\tLoss: 385.1386\n",
      "Training Epoch: 26 [42432/49669]\tLoss: 387.5103\n",
      "Training Epoch: 26 [42496/49669]\tLoss: 386.0964\n",
      "Training Epoch: 26 [42560/49669]\tLoss: 419.7031\n",
      "Training Epoch: 26 [42624/49669]\tLoss: 426.9106\n",
      "Training Epoch: 26 [42688/49669]\tLoss: 410.0640\n",
      "Training Epoch: 26 [42752/49669]\tLoss: 396.5093\n",
      "Training Epoch: 26 [42816/49669]\tLoss: 424.3611\n",
      "Training Epoch: 26 [42880/49669]\tLoss: 418.4467\n",
      "Training Epoch: 26 [42944/49669]\tLoss: 387.1258\n",
      "Training Epoch: 26 [43008/49669]\tLoss: 412.6004\n",
      "Training Epoch: 26 [43072/49669]\tLoss: 386.3726\n",
      "Training Epoch: 26 [43136/49669]\tLoss: 427.8362\n",
      "Training Epoch: 26 [43200/49669]\tLoss: 403.1628\n",
      "Training Epoch: 26 [43264/49669]\tLoss: 402.6527\n",
      "Training Epoch: 26 [43328/49669]\tLoss: 378.6518\n",
      "Training Epoch: 26 [43392/49669]\tLoss: 400.7553\n",
      "Training Epoch: 26 [43456/49669]\tLoss: 412.7156\n",
      "Training Epoch: 26 [43520/49669]\tLoss: 390.1794\n",
      "Training Epoch: 26 [43584/49669]\tLoss: 416.5379\n",
      "Training Epoch: 26 [43648/49669]\tLoss: 399.7113\n",
      "Training Epoch: 26 [43712/49669]\tLoss: 407.2985\n",
      "Training Epoch: 26 [43776/49669]\tLoss: 396.0542\n",
      "Training Epoch: 26 [43840/49669]\tLoss: 417.5616\n",
      "Training Epoch: 26 [43904/49669]\tLoss: 401.8553\n",
      "Training Epoch: 26 [43968/49669]\tLoss: 397.9755\n",
      "Training Epoch: 26 [44032/49669]\tLoss: 386.7373\n",
      "Training Epoch: 26 [44096/49669]\tLoss: 369.0642\n",
      "Training Epoch: 26 [44160/49669]\tLoss: 402.1295\n",
      "Training Epoch: 26 [44224/49669]\tLoss: 421.1717\n",
      "Training Epoch: 26 [44288/49669]\tLoss: 417.7511\n",
      "Training Epoch: 26 [44352/49669]\tLoss: 416.9141\n",
      "Training Epoch: 26 [44416/49669]\tLoss: 414.0787\n",
      "Training Epoch: 26 [44480/49669]\tLoss: 426.8760\n",
      "Training Epoch: 26 [44544/49669]\tLoss: 434.5264\n",
      "Training Epoch: 26 [44608/49669]\tLoss: 441.9001\n",
      "Training Epoch: 26 [44672/49669]\tLoss: 403.6955\n",
      "Training Epoch: 26 [44736/49669]\tLoss: 381.7126\n",
      "Training Epoch: 26 [44800/49669]\tLoss: 424.5152\n",
      "Training Epoch: 26 [44864/49669]\tLoss: 392.4006\n",
      "Training Epoch: 26 [44928/49669]\tLoss: 401.4087\n",
      "Training Epoch: 26 [44992/49669]\tLoss: 383.1675\n",
      "Training Epoch: 26 [45056/49669]\tLoss: 425.4480\n",
      "Training Epoch: 26 [45120/49669]\tLoss: 407.3873\n",
      "Training Epoch: 26 [45184/49669]\tLoss: 429.4994\n",
      "Training Epoch: 26 [45248/49669]\tLoss: 400.2386\n",
      "Training Epoch: 26 [45312/49669]\tLoss: 419.0704\n",
      "Training Epoch: 26 [45376/49669]\tLoss: 399.0919\n",
      "Training Epoch: 26 [45440/49669]\tLoss: 395.8020\n",
      "Training Epoch: 26 [45504/49669]\tLoss: 407.6217\n",
      "Training Epoch: 26 [45568/49669]\tLoss: 428.7052\n",
      "Training Epoch: 26 [45632/49669]\tLoss: 419.5751\n",
      "Training Epoch: 26 [45696/49669]\tLoss: 419.7182\n",
      "Training Epoch: 26 [45760/49669]\tLoss: 399.6658\n",
      "Training Epoch: 26 [45824/49669]\tLoss: 427.2043\n",
      "Training Epoch: 26 [45888/49669]\tLoss: 389.6383\n",
      "Training Epoch: 26 [45952/49669]\tLoss: 406.8614\n",
      "Training Epoch: 26 [46016/49669]\tLoss: 414.1435\n",
      "Training Epoch: 26 [46080/49669]\tLoss: 385.2997\n",
      "Training Epoch: 26 [46144/49669]\tLoss: 434.2805\n",
      "Training Epoch: 26 [46208/49669]\tLoss: 398.1507\n",
      "Training Epoch: 26 [46272/49669]\tLoss: 410.2256\n",
      "Training Epoch: 26 [46336/49669]\tLoss: 396.7201\n",
      "Training Epoch: 26 [46400/49669]\tLoss: 379.6782\n",
      "Training Epoch: 26 [46464/49669]\tLoss: 381.2754\n",
      "Training Epoch: 26 [46528/49669]\tLoss: 417.9452\n",
      "Training Epoch: 26 [46592/49669]\tLoss: 374.1244\n",
      "Training Epoch: 26 [46656/49669]\tLoss: 370.9360\n",
      "Training Epoch: 26 [46720/49669]\tLoss: 399.7847\n",
      "Training Epoch: 26 [46784/49669]\tLoss: 404.3894\n",
      "Training Epoch: 26 [46848/49669]\tLoss: 386.3038\n",
      "Training Epoch: 26 [46912/49669]\tLoss: 397.5352\n",
      "Training Epoch: 26 [46976/49669]\tLoss: 395.8080\n",
      "Training Epoch: 26 [47040/49669]\tLoss: 412.2222\n",
      "Training Epoch: 26 [47104/49669]\tLoss: 392.4430\n",
      "Training Epoch: 26 [47168/49669]\tLoss: 426.0435\n",
      "Training Epoch: 26 [47232/49669]\tLoss: 424.5221\n",
      "Training Epoch: 26 [47296/49669]\tLoss: 414.1280\n",
      "Training Epoch: 26 [47360/49669]\tLoss: 425.7096\n",
      "Training Epoch: 26 [47424/49669]\tLoss: 436.5232\n",
      "Training Epoch: 26 [47488/49669]\tLoss: 377.2497\n",
      "Training Epoch: 26 [47552/49669]\tLoss: 419.3127\n",
      "Training Epoch: 26 [47616/49669]\tLoss: 394.5260\n",
      "Training Epoch: 26 [47680/49669]\tLoss: 399.5955\n",
      "Training Epoch: 26 [47744/49669]\tLoss: 389.3914\n",
      "Training Epoch: 26 [47808/49669]\tLoss: 378.4821\n",
      "Training Epoch: 26 [47872/49669]\tLoss: 398.4842\n",
      "Training Epoch: 26 [47936/49669]\tLoss: 374.7531\n",
      "Training Epoch: 26 [48000/49669]\tLoss: 367.0654\n",
      "Training Epoch: 26 [48064/49669]\tLoss: 401.7246\n",
      "Training Epoch: 26 [48128/49669]\tLoss: 426.3061\n",
      "Training Epoch: 26 [48192/49669]\tLoss: 402.4539\n",
      "Training Epoch: 26 [48256/49669]\tLoss: 408.2946\n",
      "Training Epoch: 26 [48320/49669]\tLoss: 412.1477\n",
      "Training Epoch: 26 [48384/49669]\tLoss: 393.6508\n",
      "Training Epoch: 26 [48448/49669]\tLoss: 383.7159\n",
      "Training Epoch: 26 [48512/49669]\tLoss: 425.1700\n",
      "Training Epoch: 26 [48576/49669]\tLoss: 413.9624\n",
      "Training Epoch: 26 [48640/49669]\tLoss: 391.8612\n",
      "Training Epoch: 26 [48704/49669]\tLoss: 392.6337\n",
      "Training Epoch: 26 [48768/49669]\tLoss: 378.3775\n",
      "Training Epoch: 26 [48832/49669]\tLoss: 420.0988\n",
      "Training Epoch: 26 [48896/49669]\tLoss: 395.3727\n",
      "Training Epoch: 26 [48960/49669]\tLoss: 426.6160\n",
      "Training Epoch: 26 [49024/49669]\tLoss: 399.2128\n",
      "Training Epoch: 26 [49088/49669]\tLoss: 399.9006\n",
      "Training Epoch: 26 [49152/49669]\tLoss: 437.1768\n",
      "Training Epoch: 26 [49216/49669]\tLoss: 386.8699\n",
      "Training Epoch: 26 [49280/49669]\tLoss: 386.9745\n",
      "Training Epoch: 26 [49344/49669]\tLoss: 416.6934\n",
      "Training Epoch: 26 [49408/49669]\tLoss: 405.6469\n",
      "Training Epoch: 26 [49472/49669]\tLoss: 395.6858\n",
      "Training Epoch: 26 [49536/49669]\tLoss: 419.2058\n",
      "Training Epoch: 26 [49600/49669]\tLoss: 389.0053\n",
      "Training Epoch: 26 [49664/49669]\tLoss: 385.1131\n",
      "Training Epoch: 26 [49669/49669]\tLoss: 325.5868\n",
      "Training Epoch: 26 [5519/5519]\tLoss: 403.7519\n",
      "Training Epoch: 27 [64/49669]\tLoss: 398.5919\n",
      "Training Epoch: 27 [128/49669]\tLoss: 409.9604\n",
      "Training Epoch: 27 [192/49669]\tLoss: 421.0379\n",
      "Training Epoch: 27 [256/49669]\tLoss: 435.4496\n",
      "Training Epoch: 27 [320/49669]\tLoss: 427.4736\n",
      "Training Epoch: 27 [384/49669]\tLoss: 438.8922\n",
      "Training Epoch: 27 [448/49669]\tLoss: 435.9741\n",
      "Training Epoch: 27 [512/49669]\tLoss: 451.5620\n",
      "Training Epoch: 27 [576/49669]\tLoss: 434.2602\n",
      "Training Epoch: 27 [640/49669]\tLoss: 416.5333\n",
      "Training Epoch: 27 [704/49669]\tLoss: 435.8590\n",
      "Training Epoch: 27 [768/49669]\tLoss: 418.0923\n",
      "Training Epoch: 27 [832/49669]\tLoss: 425.2615\n",
      "Training Epoch: 27 [896/49669]\tLoss: 437.0110\n",
      "Training Epoch: 27 [960/49669]\tLoss: 427.0812\n",
      "Training Epoch: 27 [1024/49669]\tLoss: 429.2711\n",
      "Training Epoch: 27 [1088/49669]\tLoss: 421.7906\n",
      "Training Epoch: 27 [1152/49669]\tLoss: 413.3592\n",
      "Training Epoch: 27 [1216/49669]\tLoss: 366.8180\n",
      "Training Epoch: 27 [1280/49669]\tLoss: 415.6160\n",
      "Training Epoch: 27 [1344/49669]\tLoss: 435.2974\n",
      "Training Epoch: 27 [1408/49669]\tLoss: 401.2299\n",
      "Training Epoch: 27 [1472/49669]\tLoss: 420.8058\n",
      "Training Epoch: 27 [1536/49669]\tLoss: 418.6280\n",
      "Training Epoch: 27 [1600/49669]\tLoss: 408.5793\n",
      "Training Epoch: 27 [1664/49669]\tLoss: 420.3743\n",
      "Training Epoch: 27 [1728/49669]\tLoss: 411.9546\n",
      "Training Epoch: 27 [1792/49669]\tLoss: 405.9467\n",
      "Training Epoch: 27 [1856/49669]\tLoss: 402.7191\n",
      "Training Epoch: 27 [1920/49669]\tLoss: 395.8187\n",
      "Training Epoch: 27 [1984/49669]\tLoss: 423.9449\n",
      "Training Epoch: 27 [2048/49669]\tLoss: 416.0968\n",
      "Training Epoch: 27 [2112/49669]\tLoss: 417.9651\n",
      "Training Epoch: 27 [2176/49669]\tLoss: 428.1463\n",
      "Training Epoch: 27 [2240/49669]\tLoss: 398.2612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [2304/49669]\tLoss: 398.2783\n",
      "Training Epoch: 27 [2368/49669]\tLoss: 386.3647\n",
      "Training Epoch: 27 [2432/49669]\tLoss: 405.7424\n",
      "Training Epoch: 27 [2496/49669]\tLoss: 405.9032\n",
      "Training Epoch: 27 [2560/49669]\tLoss: 408.0788\n",
      "Training Epoch: 27 [2624/49669]\tLoss: 396.3592\n",
      "Training Epoch: 27 [2688/49669]\tLoss: 401.3203\n",
      "Training Epoch: 27 [2752/49669]\tLoss: 414.7662\n",
      "Training Epoch: 27 [2816/49669]\tLoss: 389.7934\n",
      "Training Epoch: 27 [2880/49669]\tLoss: 406.9838\n",
      "Training Epoch: 27 [2944/49669]\tLoss: 408.2165\n",
      "Training Epoch: 27 [3008/49669]\tLoss: 413.5610\n",
      "Training Epoch: 27 [3072/49669]\tLoss: 387.8465\n",
      "Training Epoch: 27 [3136/49669]\tLoss: 392.0984\n",
      "Training Epoch: 27 [3200/49669]\tLoss: 376.7887\n",
      "Training Epoch: 27 [3264/49669]\tLoss: 399.6633\n",
      "Training Epoch: 27 [3328/49669]\tLoss: 402.6246\n",
      "Training Epoch: 27 [3392/49669]\tLoss: 418.6074\n",
      "Training Epoch: 27 [3456/49669]\tLoss: 408.0758\n",
      "Training Epoch: 27 [3520/49669]\tLoss: 396.8625\n",
      "Training Epoch: 27 [3584/49669]\tLoss: 397.9271\n",
      "Training Epoch: 27 [3648/49669]\tLoss: 406.2706\n",
      "Training Epoch: 27 [3712/49669]\tLoss: 383.6886\n",
      "Training Epoch: 27 [3776/49669]\tLoss: 384.3595\n",
      "Training Epoch: 27 [3840/49669]\tLoss: 397.5822\n",
      "Training Epoch: 27 [3904/49669]\tLoss: 393.5348\n",
      "Training Epoch: 27 [3968/49669]\tLoss: 404.4418\n",
      "Training Epoch: 27 [4032/49669]\tLoss: 401.0858\n",
      "Training Epoch: 27 [4096/49669]\tLoss: 373.9011\n",
      "Training Epoch: 27 [4160/49669]\tLoss: 411.9134\n",
      "Training Epoch: 27 [4224/49669]\tLoss: 384.9052\n",
      "Training Epoch: 27 [4288/49669]\tLoss: 374.7612\n",
      "Training Epoch: 27 [4352/49669]\tLoss: 416.0470\n",
      "Training Epoch: 27 [4416/49669]\tLoss: 403.6427\n",
      "Training Epoch: 27 [4480/49669]\tLoss: 401.1632\n",
      "Training Epoch: 27 [4544/49669]\tLoss: 399.7785\n",
      "Training Epoch: 27 [4608/49669]\tLoss: 412.5438\n",
      "Training Epoch: 27 [4672/49669]\tLoss: 412.0578\n",
      "Training Epoch: 27 [4736/49669]\tLoss: 412.2749\n",
      "Training Epoch: 27 [4800/49669]\tLoss: 377.7477\n",
      "Training Epoch: 27 [4864/49669]\tLoss: 401.0750\n",
      "Training Epoch: 27 [4928/49669]\tLoss: 381.5881\n",
      "Training Epoch: 27 [4992/49669]\tLoss: 410.7434\n",
      "Training Epoch: 27 [5056/49669]\tLoss: 396.0672\n",
      "Training Epoch: 27 [5120/49669]\tLoss: 408.2968\n",
      "Training Epoch: 27 [5184/49669]\tLoss: 416.3489\n",
      "Training Epoch: 27 [5248/49669]\tLoss: 426.8953\n",
      "Training Epoch: 27 [5312/49669]\tLoss: 392.3822\n",
      "Training Epoch: 27 [5376/49669]\tLoss: 381.1227\n",
      "Training Epoch: 27 [5440/49669]\tLoss: 392.1785\n",
      "Training Epoch: 27 [5504/49669]\tLoss: 395.2544\n",
      "Training Epoch: 27 [5568/49669]\tLoss: 400.0898\n",
      "Training Epoch: 27 [5632/49669]\tLoss: 384.0324\n",
      "Training Epoch: 27 [5696/49669]\tLoss: 418.8629\n",
      "Training Epoch: 27 [5760/49669]\tLoss: 424.4906\n",
      "Training Epoch: 27 [5824/49669]\tLoss: 390.4564\n",
      "Training Epoch: 27 [5888/49669]\tLoss: 419.8759\n",
      "Training Epoch: 27 [5952/49669]\tLoss: 433.8803\n",
      "Training Epoch: 27 [6016/49669]\tLoss: 388.2854\n",
      "Training Epoch: 27 [6080/49669]\tLoss: 409.4540\n",
      "Training Epoch: 27 [6144/49669]\tLoss: 375.4922\n",
      "Training Epoch: 27 [6208/49669]\tLoss: 390.8170\n",
      "Training Epoch: 27 [6272/49669]\tLoss: 393.8361\n",
      "Training Epoch: 27 [6336/49669]\tLoss: 393.7120\n",
      "Training Epoch: 27 [6400/49669]\tLoss: 395.6993\n",
      "Training Epoch: 27 [6464/49669]\tLoss: 384.2766\n",
      "Training Epoch: 27 [6528/49669]\tLoss: 399.8151\n",
      "Training Epoch: 27 [6592/49669]\tLoss: 388.1739\n",
      "Training Epoch: 27 [6656/49669]\tLoss: 413.5821\n",
      "Training Epoch: 27 [6720/49669]\tLoss: 431.0696\n",
      "Training Epoch: 27 [6784/49669]\tLoss: 424.9329\n",
      "Training Epoch: 27 [6848/49669]\tLoss: 374.2341\n",
      "Training Epoch: 27 [6912/49669]\tLoss: 364.6570\n",
      "Training Epoch: 27 [6976/49669]\tLoss: 430.3807\n",
      "Training Epoch: 27 [7040/49669]\tLoss: 412.0810\n",
      "Training Epoch: 27 [7104/49669]\tLoss: 394.0494\n",
      "Training Epoch: 27 [7168/49669]\tLoss: 420.5667\n",
      "Training Epoch: 27 [7232/49669]\tLoss: 433.6582\n",
      "Training Epoch: 27 [7296/49669]\tLoss: 410.6854\n",
      "Training Epoch: 27 [7360/49669]\tLoss: 410.0520\n",
      "Training Epoch: 27 [7424/49669]\tLoss: 436.6634\n",
      "Training Epoch: 27 [7488/49669]\tLoss: 436.7405\n",
      "Training Epoch: 27 [7552/49669]\tLoss: 481.9897\n",
      "Training Epoch: 27 [7616/49669]\tLoss: 499.4773\n",
      "Training Epoch: 27 [7680/49669]\tLoss: 476.9558\n",
      "Training Epoch: 27 [7744/49669]\tLoss: 462.2775\n",
      "Training Epoch: 27 [7808/49669]\tLoss: 382.5835\n",
      "Training Epoch: 27 [7872/49669]\tLoss: 415.0830\n",
      "Training Epoch: 27 [7936/49669]\tLoss: 423.9334\n",
      "Training Epoch: 27 [8000/49669]\tLoss: 457.5263\n",
      "Training Epoch: 27 [8064/49669]\tLoss: 466.0773\n",
      "Training Epoch: 27 [8128/49669]\tLoss: 449.1837\n",
      "Training Epoch: 27 [8192/49669]\tLoss: 396.9954\n",
      "Training Epoch: 27 [8256/49669]\tLoss: 418.5045\n",
      "Training Epoch: 27 [8320/49669]\tLoss: 421.2482\n",
      "Training Epoch: 27 [8384/49669]\tLoss: 462.8504\n",
      "Training Epoch: 27 [8448/49669]\tLoss: 435.2911\n",
      "Training Epoch: 27 [8512/49669]\tLoss: 449.5071\n",
      "Training Epoch: 27 [8576/49669]\tLoss: 414.9797\n",
      "Training Epoch: 27 [8640/49669]\tLoss: 402.5963\n",
      "Training Epoch: 27 [8704/49669]\tLoss: 439.4819\n",
      "Training Epoch: 27 [8768/49669]\tLoss: 397.4181\n",
      "Training Epoch: 27 [8832/49669]\tLoss: 413.0215\n",
      "Training Epoch: 27 [8896/49669]\tLoss: 408.5996\n",
      "Training Epoch: 27 [8960/49669]\tLoss: 399.3629\n",
      "Training Epoch: 27 [9024/49669]\tLoss: 393.3172\n",
      "Training Epoch: 27 [9088/49669]\tLoss: 386.5870\n",
      "Training Epoch: 27 [9152/49669]\tLoss: 418.5440\n",
      "Training Epoch: 27 [9216/49669]\tLoss: 416.7759\n",
      "Training Epoch: 27 [9280/49669]\tLoss: 399.3135\n",
      "Training Epoch: 27 [9344/49669]\tLoss: 379.1756\n",
      "Training Epoch: 27 [9408/49669]\tLoss: 441.9300\n",
      "Training Epoch: 27 [9472/49669]\tLoss: 385.8416\n",
      "Training Epoch: 27 [9536/49669]\tLoss: 416.8583\n",
      "Training Epoch: 27 [9600/49669]\tLoss: 403.0664\n",
      "Training Epoch: 27 [9664/49669]\tLoss: 408.2634\n",
      "Training Epoch: 27 [9728/49669]\tLoss: 414.2650\n",
      "Training Epoch: 27 [9792/49669]\tLoss: 419.3488\n",
      "Training Epoch: 27 [9856/49669]\tLoss: 405.2910\n",
      "Training Epoch: 27 [9920/49669]\tLoss: 432.3058\n",
      "Training Epoch: 27 [9984/49669]\tLoss: 385.7523\n",
      "Training Epoch: 27 [10048/49669]\tLoss: 431.1872\n",
      "Training Epoch: 27 [10112/49669]\tLoss: 430.5293\n",
      "Training Epoch: 27 [10176/49669]\tLoss: 396.3349\n",
      "Training Epoch: 27 [10240/49669]\tLoss: 395.1594\n",
      "Training Epoch: 27 [10304/49669]\tLoss: 401.0035\n",
      "Training Epoch: 27 [10368/49669]\tLoss: 402.8567\n",
      "Training Epoch: 27 [10432/49669]\tLoss: 424.7789\n",
      "Training Epoch: 27 [10496/49669]\tLoss: 415.6169\n",
      "Training Epoch: 27 [10560/49669]\tLoss: 411.6997\n",
      "Training Epoch: 27 [10624/49669]\tLoss: 401.3929\n",
      "Training Epoch: 27 [10688/49669]\tLoss: 397.3638\n",
      "Training Epoch: 27 [10752/49669]\tLoss: 379.9077\n",
      "Training Epoch: 27 [10816/49669]\tLoss: 400.8398\n",
      "Training Epoch: 27 [10880/49669]\tLoss: 399.3178\n",
      "Training Epoch: 27 [10944/49669]\tLoss: 382.9305\n",
      "Training Epoch: 27 [11008/49669]\tLoss: 383.5829\n",
      "Training Epoch: 27 [11072/49669]\tLoss: 385.0122\n",
      "Training Epoch: 27 [11136/49669]\tLoss: 405.4616\n",
      "Training Epoch: 27 [11200/49669]\tLoss: 409.6484\n",
      "Training Epoch: 27 [11264/49669]\tLoss: 408.6724\n",
      "Training Epoch: 27 [11328/49669]\tLoss: 404.2095\n",
      "Training Epoch: 27 [11392/49669]\tLoss: 401.6805\n",
      "Training Epoch: 27 [11456/49669]\tLoss: 373.3490\n",
      "Training Epoch: 27 [11520/49669]\tLoss: 386.7822\n",
      "Training Epoch: 27 [11584/49669]\tLoss: 412.4351\n",
      "Training Epoch: 27 [11648/49669]\tLoss: 362.5445\n",
      "Training Epoch: 27 [11712/49669]\tLoss: 402.8535\n",
      "Training Epoch: 27 [11776/49669]\tLoss: 388.1573\n",
      "Training Epoch: 27 [11840/49669]\tLoss: 415.9576\n",
      "Training Epoch: 27 [11904/49669]\tLoss: 417.4467\n",
      "Training Epoch: 27 [11968/49669]\tLoss: 409.3703\n",
      "Training Epoch: 27 [12032/49669]\tLoss: 408.2591\n",
      "Training Epoch: 27 [12096/49669]\tLoss: 382.5697\n",
      "Training Epoch: 27 [12160/49669]\tLoss: 404.8674\n",
      "Training Epoch: 27 [12224/49669]\tLoss: 382.7289\n",
      "Training Epoch: 27 [12288/49669]\tLoss: 420.4352\n",
      "Training Epoch: 27 [12352/49669]\tLoss: 420.0553\n",
      "Training Epoch: 27 [12416/49669]\tLoss: 414.1805\n",
      "Training Epoch: 27 [12480/49669]\tLoss: 437.7373\n",
      "Training Epoch: 27 [12544/49669]\tLoss: 391.5928\n",
      "Training Epoch: 27 [12608/49669]\tLoss: 392.4386\n",
      "Training Epoch: 27 [12672/49669]\tLoss: 361.5830\n",
      "Training Epoch: 27 [12736/49669]\tLoss: 388.7309\n",
      "Training Epoch: 27 [12800/49669]\tLoss: 391.6128\n",
      "Training Epoch: 27 [12864/49669]\tLoss: 403.2362\n",
      "Training Epoch: 27 [12928/49669]\tLoss: 421.9033\n",
      "Training Epoch: 27 [12992/49669]\tLoss: 396.9545\n",
      "Training Epoch: 27 [13056/49669]\tLoss: 410.3982\n",
      "Training Epoch: 27 [13120/49669]\tLoss: 398.5144\n",
      "Training Epoch: 27 [13184/49669]\tLoss: 447.2056\n",
      "Training Epoch: 27 [13248/49669]\tLoss: 395.3615\n",
      "Training Epoch: 27 [13312/49669]\tLoss: 380.3927\n",
      "Training Epoch: 27 [13376/49669]\tLoss: 389.8334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [13440/49669]\tLoss: 379.7763\n",
      "Training Epoch: 27 [13504/49669]\tLoss: 405.1828\n",
      "Training Epoch: 27 [13568/49669]\tLoss: 422.0396\n",
      "Training Epoch: 27 [13632/49669]\tLoss: 406.2553\n",
      "Training Epoch: 27 [13696/49669]\tLoss: 391.8012\n",
      "Training Epoch: 27 [13760/49669]\tLoss: 390.8534\n",
      "Training Epoch: 27 [13824/49669]\tLoss: 391.0119\n",
      "Training Epoch: 27 [13888/49669]\tLoss: 400.7001\n",
      "Training Epoch: 27 [13952/49669]\tLoss: 417.9770\n",
      "Training Epoch: 27 [14016/49669]\tLoss: 393.8734\n",
      "Training Epoch: 27 [14080/49669]\tLoss: 408.2666\n",
      "Training Epoch: 27 [14144/49669]\tLoss: 380.9260\n",
      "Training Epoch: 27 [14208/49669]\tLoss: 400.1320\n",
      "Training Epoch: 27 [14272/49669]\tLoss: 390.7023\n",
      "Training Epoch: 27 [14336/49669]\tLoss: 427.2431\n",
      "Training Epoch: 27 [14400/49669]\tLoss: 394.4367\n",
      "Training Epoch: 27 [14464/49669]\tLoss: 421.3456\n",
      "Training Epoch: 27 [14528/49669]\tLoss: 418.5798\n",
      "Training Epoch: 27 [14592/49669]\tLoss: 372.1967\n",
      "Training Epoch: 27 [14656/49669]\tLoss: 402.5438\n",
      "Training Epoch: 27 [14720/49669]\tLoss: 415.5793\n",
      "Training Epoch: 27 [14784/49669]\tLoss: 395.3332\n",
      "Training Epoch: 27 [14848/49669]\tLoss: 360.3355\n",
      "Training Epoch: 27 [14912/49669]\tLoss: 412.3288\n",
      "Training Epoch: 27 [14976/49669]\tLoss: 394.8203\n",
      "Training Epoch: 27 [15040/49669]\tLoss: 387.1591\n",
      "Training Epoch: 27 [15104/49669]\tLoss: 395.2358\n",
      "Training Epoch: 27 [15168/49669]\tLoss: 416.4167\n",
      "Training Epoch: 27 [15232/49669]\tLoss: 403.1067\n",
      "Training Epoch: 27 [15296/49669]\tLoss: 362.6475\n",
      "Training Epoch: 27 [15360/49669]\tLoss: 422.7248\n",
      "Training Epoch: 27 [15424/49669]\tLoss: 369.4755\n",
      "Training Epoch: 27 [15488/49669]\tLoss: 378.9085\n",
      "Training Epoch: 27 [15552/49669]\tLoss: 413.8477\n",
      "Training Epoch: 27 [15616/49669]\tLoss: 387.7935\n",
      "Training Epoch: 27 [15680/49669]\tLoss: 379.0854\n",
      "Training Epoch: 27 [15744/49669]\tLoss: 415.0632\n",
      "Training Epoch: 27 [15808/49669]\tLoss: 377.2497\n",
      "Training Epoch: 27 [15872/49669]\tLoss: 412.3408\n",
      "Training Epoch: 27 [15936/49669]\tLoss: 385.3408\n",
      "Training Epoch: 27 [16000/49669]\tLoss: 396.7460\n",
      "Training Epoch: 27 [16064/49669]\tLoss: 397.7498\n",
      "Training Epoch: 27 [16128/49669]\tLoss: 375.7635\n",
      "Training Epoch: 27 [16192/49669]\tLoss: 395.6129\n",
      "Training Epoch: 27 [16256/49669]\tLoss: 418.8062\n",
      "Training Epoch: 27 [16320/49669]\tLoss: 400.7838\n",
      "Training Epoch: 27 [16384/49669]\tLoss: 386.1008\n",
      "Training Epoch: 27 [16448/49669]\tLoss: 397.0842\n",
      "Training Epoch: 27 [16512/49669]\tLoss: 412.0943\n",
      "Training Epoch: 27 [16576/49669]\tLoss: 412.4978\n",
      "Training Epoch: 27 [16640/49669]\tLoss: 388.1768\n",
      "Training Epoch: 27 [16704/49669]\tLoss: 391.1584\n",
      "Training Epoch: 27 [16768/49669]\tLoss: 406.1071\n",
      "Training Epoch: 27 [16832/49669]\tLoss: 415.1446\n",
      "Training Epoch: 27 [16896/49669]\tLoss: 414.6435\n",
      "Training Epoch: 27 [16960/49669]\tLoss: 410.9271\n",
      "Training Epoch: 27 [17024/49669]\tLoss: 388.1420\n",
      "Training Epoch: 27 [17088/49669]\tLoss: 400.1533\n",
      "Training Epoch: 27 [17152/49669]\tLoss: 395.3809\n",
      "Training Epoch: 27 [17216/49669]\tLoss: 395.3875\n",
      "Training Epoch: 27 [17280/49669]\tLoss: 397.2092\n",
      "Training Epoch: 27 [17344/49669]\tLoss: 381.6499\n",
      "Training Epoch: 27 [17408/49669]\tLoss: 396.6432\n",
      "Training Epoch: 27 [17472/49669]\tLoss: 404.4248\n",
      "Training Epoch: 27 [17536/49669]\tLoss: 415.3117\n",
      "Training Epoch: 27 [17600/49669]\tLoss: 393.1600\n",
      "Training Epoch: 27 [17664/49669]\tLoss: 415.6411\n",
      "Training Epoch: 27 [17728/49669]\tLoss: 383.0054\n",
      "Training Epoch: 27 [17792/49669]\tLoss: 370.8583\n",
      "Training Epoch: 27 [17856/49669]\tLoss: 398.3759\n",
      "Training Epoch: 27 [17920/49669]\tLoss: 375.9474\n",
      "Training Epoch: 27 [17984/49669]\tLoss: 396.0258\n",
      "Training Epoch: 27 [18048/49669]\tLoss: 395.8424\n",
      "Training Epoch: 27 [18112/49669]\tLoss: 390.4730\n",
      "Training Epoch: 27 [18176/49669]\tLoss: 409.9908\n",
      "Training Epoch: 27 [18240/49669]\tLoss: 391.7612\n",
      "Training Epoch: 27 [18304/49669]\tLoss: 413.0821\n",
      "Training Epoch: 27 [18368/49669]\tLoss: 375.2459\n",
      "Training Epoch: 27 [18432/49669]\tLoss: 413.8349\n",
      "Training Epoch: 27 [18496/49669]\tLoss: 424.7652\n",
      "Training Epoch: 27 [18560/49669]\tLoss: 405.8899\n",
      "Training Epoch: 27 [18624/49669]\tLoss: 383.9464\n",
      "Training Epoch: 27 [18688/49669]\tLoss: 403.2064\n",
      "Training Epoch: 27 [18752/49669]\tLoss: 385.3671\n",
      "Training Epoch: 27 [18816/49669]\tLoss: 416.6384\n",
      "Training Epoch: 27 [18880/49669]\tLoss: 383.9669\n",
      "Training Epoch: 27 [18944/49669]\tLoss: 398.4855\n",
      "Training Epoch: 27 [19008/49669]\tLoss: 393.4302\n",
      "Training Epoch: 27 [19072/49669]\tLoss: 425.7519\n",
      "Training Epoch: 27 [19136/49669]\tLoss: 393.2338\n",
      "Training Epoch: 27 [19200/49669]\tLoss: 398.0211\n",
      "Training Epoch: 27 [19264/49669]\tLoss: 400.8982\n",
      "Training Epoch: 27 [19328/49669]\tLoss: 408.5330\n",
      "Training Epoch: 27 [19392/49669]\tLoss: 403.9108\n",
      "Training Epoch: 27 [19456/49669]\tLoss: 411.6923\n",
      "Training Epoch: 27 [19520/49669]\tLoss: 428.3251\n",
      "Training Epoch: 27 [19584/49669]\tLoss: 398.9286\n",
      "Training Epoch: 27 [19648/49669]\tLoss: 405.2057\n",
      "Training Epoch: 27 [19712/49669]\tLoss: 407.6300\n",
      "Training Epoch: 27 [19776/49669]\tLoss: 387.8478\n",
      "Training Epoch: 27 [19840/49669]\tLoss: 394.9572\n",
      "Training Epoch: 27 [19904/49669]\tLoss: 381.6197\n",
      "Training Epoch: 27 [19968/49669]\tLoss: 402.6292\n",
      "Training Epoch: 27 [20032/49669]\tLoss: 392.8212\n",
      "Training Epoch: 27 [20096/49669]\tLoss: 431.4004\n",
      "Training Epoch: 27 [20160/49669]\tLoss: 391.0519\n",
      "Training Epoch: 27 [20224/49669]\tLoss: 414.8236\n",
      "Training Epoch: 27 [20288/49669]\tLoss: 387.6279\n",
      "Training Epoch: 27 [20352/49669]\tLoss: 412.0010\n",
      "Training Epoch: 27 [20416/49669]\tLoss: 390.3449\n",
      "Training Epoch: 27 [20480/49669]\tLoss: 422.2119\n",
      "Training Epoch: 27 [20544/49669]\tLoss: 427.6437\n",
      "Training Epoch: 27 [20608/49669]\tLoss: 408.1143\n",
      "Training Epoch: 27 [20672/49669]\tLoss: 410.3234\n",
      "Training Epoch: 27 [20736/49669]\tLoss: 391.9312\n",
      "Training Epoch: 27 [20800/49669]\tLoss: 413.6969\n",
      "Training Epoch: 27 [20864/49669]\tLoss: 373.5701\n",
      "Training Epoch: 27 [20928/49669]\tLoss: 415.2329\n",
      "Training Epoch: 27 [20992/49669]\tLoss: 390.3991\n",
      "Training Epoch: 27 [21056/49669]\tLoss: 440.8909\n",
      "Training Epoch: 27 [21120/49669]\tLoss: 382.4138\n",
      "Training Epoch: 27 [21184/49669]\tLoss: 443.4519\n",
      "Training Epoch: 27 [21248/49669]\tLoss: 390.9834\n",
      "Training Epoch: 27 [21312/49669]\tLoss: 399.7593\n",
      "Training Epoch: 27 [21376/49669]\tLoss: 423.3560\n",
      "Training Epoch: 27 [21440/49669]\tLoss: 403.5167\n",
      "Training Epoch: 27 [21504/49669]\tLoss: 399.8719\n",
      "Training Epoch: 27 [21568/49669]\tLoss: 395.6702\n",
      "Training Epoch: 27 [21632/49669]\tLoss: 404.9039\n",
      "Training Epoch: 27 [21696/49669]\tLoss: 389.9652\n",
      "Training Epoch: 27 [21760/49669]\tLoss: 397.1168\n",
      "Training Epoch: 27 [21824/49669]\tLoss: 386.2392\n",
      "Training Epoch: 27 [21888/49669]\tLoss: 407.6387\n",
      "Training Epoch: 27 [21952/49669]\tLoss: 389.8567\n",
      "Training Epoch: 27 [22016/49669]\tLoss: 412.4078\n",
      "Training Epoch: 27 [22080/49669]\tLoss: 407.6316\n",
      "Training Epoch: 27 [22144/49669]\tLoss: 424.7478\n",
      "Training Epoch: 27 [22208/49669]\tLoss: 377.7174\n",
      "Training Epoch: 27 [22272/49669]\tLoss: 414.2229\n",
      "Training Epoch: 27 [22336/49669]\tLoss: 415.9078\n",
      "Training Epoch: 27 [22400/49669]\tLoss: 423.1263\n",
      "Training Epoch: 27 [22464/49669]\tLoss: 402.2825\n",
      "Training Epoch: 27 [22528/49669]\tLoss: 412.1732\n",
      "Training Epoch: 27 [22592/49669]\tLoss: 426.9147\n",
      "Training Epoch: 27 [22656/49669]\tLoss: 427.9678\n",
      "Training Epoch: 27 [22720/49669]\tLoss: 444.7045\n",
      "Training Epoch: 27 [22784/49669]\tLoss: 381.3105\n",
      "Training Epoch: 27 [22848/49669]\tLoss: 390.2647\n",
      "Training Epoch: 27 [22912/49669]\tLoss: 391.5271\n",
      "Training Epoch: 27 [22976/49669]\tLoss: 432.9341\n",
      "Training Epoch: 27 [23040/49669]\tLoss: 412.4833\n",
      "Training Epoch: 27 [23104/49669]\tLoss: 425.0045\n",
      "Training Epoch: 27 [23168/49669]\tLoss: 430.9555\n",
      "Training Epoch: 27 [23232/49669]\tLoss: 439.2870\n",
      "Training Epoch: 27 [23296/49669]\tLoss: 419.0906\n",
      "Training Epoch: 27 [23360/49669]\tLoss: 437.0735\n",
      "Training Epoch: 27 [23424/49669]\tLoss: 427.4136\n",
      "Training Epoch: 27 [23488/49669]\tLoss: 429.6556\n",
      "Training Epoch: 27 [23552/49669]\tLoss: 449.0172\n",
      "Training Epoch: 27 [23616/49669]\tLoss: 437.2003\n",
      "Training Epoch: 27 [23680/49669]\tLoss: 425.4242\n",
      "Training Epoch: 27 [23744/49669]\tLoss: 441.9207\n",
      "Training Epoch: 27 [23808/49669]\tLoss: 410.2909\n",
      "Training Epoch: 27 [23872/49669]\tLoss: 420.2759\n",
      "Training Epoch: 27 [23936/49669]\tLoss: 407.4879\n",
      "Training Epoch: 27 [24000/49669]\tLoss: 406.9555\n",
      "Training Epoch: 27 [24064/49669]\tLoss: 387.0756\n",
      "Training Epoch: 27 [24128/49669]\tLoss: 358.3701\n",
      "Training Epoch: 27 [24192/49669]\tLoss: 404.3510\n",
      "Training Epoch: 27 [24256/49669]\tLoss: 403.6182\n",
      "Training Epoch: 27 [24320/49669]\tLoss: 402.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [24384/49669]\tLoss: 390.8466\n",
      "Training Epoch: 27 [24448/49669]\tLoss: 406.2743\n",
      "Training Epoch: 27 [24512/49669]\tLoss: 390.1787\n",
      "Training Epoch: 27 [24576/49669]\tLoss: 416.6388\n",
      "Training Epoch: 27 [24640/49669]\tLoss: 399.8027\n",
      "Training Epoch: 27 [24704/49669]\tLoss: 406.6814\n",
      "Training Epoch: 27 [24768/49669]\tLoss: 406.1705\n",
      "Training Epoch: 27 [24832/49669]\tLoss: 420.0191\n",
      "Training Epoch: 27 [24896/49669]\tLoss: 398.0817\n",
      "Training Epoch: 27 [24960/49669]\tLoss: 409.1525\n",
      "Training Epoch: 27 [25024/49669]\tLoss: 427.4172\n",
      "Training Epoch: 27 [25088/49669]\tLoss: 414.1250\n",
      "Training Epoch: 27 [25152/49669]\tLoss: 399.5100\n",
      "Training Epoch: 27 [25216/49669]\tLoss: 417.0887\n",
      "Training Epoch: 27 [25280/49669]\tLoss: 415.3813\n",
      "Training Epoch: 27 [25344/49669]\tLoss: 408.9326\n",
      "Training Epoch: 27 [25408/49669]\tLoss: 404.2614\n",
      "Training Epoch: 27 [25472/49669]\tLoss: 417.3096\n",
      "Training Epoch: 27 [25536/49669]\tLoss: 396.4156\n",
      "Training Epoch: 27 [25600/49669]\tLoss: 413.2459\n",
      "Training Epoch: 27 [25664/49669]\tLoss: 388.1693\n",
      "Training Epoch: 27 [25728/49669]\tLoss: 433.7671\n",
      "Training Epoch: 27 [25792/49669]\tLoss: 376.9515\n",
      "Training Epoch: 27 [25856/49669]\tLoss: 370.7643\n",
      "Training Epoch: 27 [25920/49669]\tLoss: 410.9758\n",
      "Training Epoch: 27 [25984/49669]\tLoss: 399.0365\n",
      "Training Epoch: 27 [26048/49669]\tLoss: 396.0174\n",
      "Training Epoch: 27 [26112/49669]\tLoss: 406.5715\n",
      "Training Epoch: 27 [26176/49669]\tLoss: 385.5655\n",
      "Training Epoch: 27 [26240/49669]\tLoss: 377.3606\n",
      "Training Epoch: 27 [26304/49669]\tLoss: 407.0654\n",
      "Training Epoch: 27 [26368/49669]\tLoss: 405.5333\n",
      "Training Epoch: 27 [26432/49669]\tLoss: 412.6600\n",
      "Training Epoch: 27 [26496/49669]\tLoss: 419.6159\n",
      "Training Epoch: 27 [26560/49669]\tLoss: 414.0616\n",
      "Training Epoch: 27 [26624/49669]\tLoss: 389.8206\n",
      "Training Epoch: 27 [26688/49669]\tLoss: 398.0323\n",
      "Training Epoch: 27 [26752/49669]\tLoss: 381.3406\n",
      "Training Epoch: 27 [26816/49669]\tLoss: 426.8782\n",
      "Training Epoch: 27 [26880/49669]\tLoss: 402.8453\n",
      "Training Epoch: 27 [26944/49669]\tLoss: 393.2849\n",
      "Training Epoch: 27 [27008/49669]\tLoss: 439.7106\n",
      "Training Epoch: 27 [27072/49669]\tLoss: 381.8803\n",
      "Training Epoch: 27 [27136/49669]\tLoss: 406.4944\n",
      "Training Epoch: 27 [27200/49669]\tLoss: 392.2527\n",
      "Training Epoch: 27 [27264/49669]\tLoss: 435.7415\n",
      "Training Epoch: 27 [27328/49669]\tLoss: 384.4873\n",
      "Training Epoch: 27 [27392/49669]\tLoss: 408.5587\n",
      "Training Epoch: 27 [27456/49669]\tLoss: 409.9530\n",
      "Training Epoch: 27 [27520/49669]\tLoss: 414.0909\n",
      "Training Epoch: 27 [27584/49669]\tLoss: 403.9051\n",
      "Training Epoch: 27 [27648/49669]\tLoss: 387.1679\n",
      "Training Epoch: 27 [27712/49669]\tLoss: 416.3965\n",
      "Training Epoch: 27 [27776/49669]\tLoss: 408.1509\n",
      "Training Epoch: 27 [27840/49669]\tLoss: 385.1638\n",
      "Training Epoch: 27 [27904/49669]\tLoss: 443.5184\n",
      "Training Epoch: 27 [27968/49669]\tLoss: 423.9707\n",
      "Training Epoch: 27 [28032/49669]\tLoss: 417.8785\n",
      "Training Epoch: 27 [28096/49669]\tLoss: 406.6793\n",
      "Training Epoch: 27 [28160/49669]\tLoss: 410.1548\n",
      "Training Epoch: 27 [28224/49669]\tLoss: 395.9108\n",
      "Training Epoch: 27 [28288/49669]\tLoss: 393.2178\n",
      "Training Epoch: 27 [28352/49669]\tLoss: 410.2829\n",
      "Training Epoch: 27 [28416/49669]\tLoss: 389.8708\n",
      "Training Epoch: 27 [28480/49669]\tLoss: 360.8278\n",
      "Training Epoch: 27 [28544/49669]\tLoss: 386.1744\n",
      "Training Epoch: 27 [28608/49669]\tLoss: 398.3217\n",
      "Training Epoch: 27 [28672/49669]\tLoss: 391.7435\n",
      "Training Epoch: 27 [28736/49669]\tLoss: 393.9966\n",
      "Training Epoch: 27 [28800/49669]\tLoss: 408.0792\n",
      "Training Epoch: 27 [28864/49669]\tLoss: 405.1961\n",
      "Training Epoch: 27 [28928/49669]\tLoss: 428.2359\n",
      "Training Epoch: 27 [28992/49669]\tLoss: 378.6327\n",
      "Training Epoch: 27 [29056/49669]\tLoss: 415.3526\n",
      "Training Epoch: 27 [29120/49669]\tLoss: 401.2457\n",
      "Training Epoch: 27 [29184/49669]\tLoss: 396.5739\n",
      "Training Epoch: 27 [29248/49669]\tLoss: 391.0089\n",
      "Training Epoch: 27 [29312/49669]\tLoss: 398.8539\n",
      "Training Epoch: 27 [29376/49669]\tLoss: 405.9539\n",
      "Training Epoch: 27 [29440/49669]\tLoss: 409.6443\n",
      "Training Epoch: 27 [29504/49669]\tLoss: 412.0498\n",
      "Training Epoch: 27 [29568/49669]\tLoss: 409.1866\n",
      "Training Epoch: 27 [29632/49669]\tLoss: 414.2701\n",
      "Training Epoch: 27 [29696/49669]\tLoss: 426.3548\n",
      "Training Epoch: 27 [29760/49669]\tLoss: 399.1549\n",
      "Training Epoch: 27 [29824/49669]\tLoss: 397.0445\n",
      "Training Epoch: 27 [29888/49669]\tLoss: 426.5225\n",
      "Training Epoch: 27 [29952/49669]\tLoss: 420.8355\n",
      "Training Epoch: 27 [30016/49669]\tLoss: 417.5110\n",
      "Training Epoch: 27 [30080/49669]\tLoss: 435.4008\n",
      "Training Epoch: 27 [30144/49669]\tLoss: 426.8612\n",
      "Training Epoch: 27 [30208/49669]\tLoss: 449.9495\n",
      "Training Epoch: 27 [30272/49669]\tLoss: 426.5502\n",
      "Training Epoch: 27 [30336/49669]\tLoss: 434.6285\n",
      "Training Epoch: 27 [30400/49669]\tLoss: 467.2595\n",
      "Training Epoch: 27 [30464/49669]\tLoss: 425.6563\n",
      "Training Epoch: 27 [30528/49669]\tLoss: 436.9365\n",
      "Training Epoch: 27 [30592/49669]\tLoss: 421.6072\n",
      "Training Epoch: 27 [30656/49669]\tLoss: 442.8125\n",
      "Training Epoch: 27 [30720/49669]\tLoss: 364.8433\n",
      "Training Epoch: 27 [30784/49669]\tLoss: 395.1050\n",
      "Training Epoch: 27 [30848/49669]\tLoss: 419.8579\n",
      "Training Epoch: 27 [30912/49669]\tLoss: 401.4227\n",
      "Training Epoch: 27 [30976/49669]\tLoss: 384.0322\n",
      "Training Epoch: 27 [31040/49669]\tLoss: 417.7372\n",
      "Training Epoch: 27 [31104/49669]\tLoss: 412.4260\n",
      "Training Epoch: 27 [31168/49669]\tLoss: 415.9020\n",
      "Training Epoch: 27 [31232/49669]\tLoss: 398.7356\n",
      "Training Epoch: 27 [31296/49669]\tLoss: 387.8791\n",
      "Training Epoch: 27 [31360/49669]\tLoss: 391.2106\n",
      "Training Epoch: 27 [31424/49669]\tLoss: 418.4520\n",
      "Training Epoch: 27 [31488/49669]\tLoss: 413.9810\n",
      "Training Epoch: 27 [31552/49669]\tLoss: 414.8145\n",
      "Training Epoch: 27 [31616/49669]\tLoss: 377.1583\n",
      "Training Epoch: 27 [31680/49669]\tLoss: 408.7256\n",
      "Training Epoch: 27 [31744/49669]\tLoss: 425.6490\n",
      "Training Epoch: 27 [31808/49669]\tLoss: 418.2318\n",
      "Training Epoch: 27 [31872/49669]\tLoss: 407.2787\n",
      "Training Epoch: 27 [31936/49669]\tLoss: 396.8874\n",
      "Training Epoch: 27 [32000/49669]\tLoss: 403.4666\n",
      "Training Epoch: 27 [32064/49669]\tLoss: 414.3812\n",
      "Training Epoch: 27 [32128/49669]\tLoss: 393.7217\n",
      "Training Epoch: 27 [32192/49669]\tLoss: 401.4503\n",
      "Training Epoch: 27 [32256/49669]\tLoss: 391.5619\n",
      "Training Epoch: 27 [32320/49669]\tLoss: 395.1050\n",
      "Training Epoch: 27 [32384/49669]\tLoss: 448.0009\n",
      "Training Epoch: 27 [32448/49669]\tLoss: 412.8625\n",
      "Training Epoch: 27 [32512/49669]\tLoss: 395.9420\n",
      "Training Epoch: 27 [32576/49669]\tLoss: 405.5435\n",
      "Training Epoch: 27 [32640/49669]\tLoss: 390.2000\n",
      "Training Epoch: 27 [32704/49669]\tLoss: 395.1563\n",
      "Training Epoch: 27 [32768/49669]\tLoss: 414.2140\n",
      "Training Epoch: 27 [32832/49669]\tLoss: 389.3311\n",
      "Training Epoch: 27 [32896/49669]\tLoss: 448.2585\n",
      "Training Epoch: 27 [32960/49669]\tLoss: 410.3832\n",
      "Training Epoch: 27 [33024/49669]\tLoss: 394.6050\n",
      "Training Epoch: 27 [33088/49669]\tLoss: 393.4945\n",
      "Training Epoch: 27 [33152/49669]\tLoss: 379.6386\n",
      "Training Epoch: 27 [33216/49669]\tLoss: 380.4144\n",
      "Training Epoch: 27 [33280/49669]\tLoss: 406.1297\n",
      "Training Epoch: 27 [33344/49669]\tLoss: 398.9683\n",
      "Training Epoch: 27 [33408/49669]\tLoss: 393.5932\n",
      "Training Epoch: 27 [33472/49669]\tLoss: 408.9385\n",
      "Training Epoch: 27 [33536/49669]\tLoss: 398.8913\n",
      "Training Epoch: 27 [33600/49669]\tLoss: 387.6448\n",
      "Training Epoch: 27 [33664/49669]\tLoss: 385.5904\n",
      "Training Epoch: 27 [33728/49669]\tLoss: 386.3061\n",
      "Training Epoch: 27 [33792/49669]\tLoss: 394.2928\n",
      "Training Epoch: 27 [33856/49669]\tLoss: 412.7118\n",
      "Training Epoch: 27 [33920/49669]\tLoss: 390.4736\n",
      "Training Epoch: 27 [33984/49669]\tLoss: 379.0677\n",
      "Training Epoch: 27 [34048/49669]\tLoss: 400.6958\n",
      "Training Epoch: 27 [34112/49669]\tLoss: 360.4948\n",
      "Training Epoch: 27 [34176/49669]\tLoss: 413.3913\n",
      "Training Epoch: 27 [34240/49669]\tLoss: 413.1835\n",
      "Training Epoch: 27 [34304/49669]\tLoss: 391.6545\n",
      "Training Epoch: 27 [34368/49669]\tLoss: 409.7801\n",
      "Training Epoch: 27 [34432/49669]\tLoss: 418.0943\n",
      "Training Epoch: 27 [34496/49669]\tLoss: 366.8572\n",
      "Training Epoch: 27 [34560/49669]\tLoss: 375.0442\n",
      "Training Epoch: 27 [34624/49669]\tLoss: 401.6447\n",
      "Training Epoch: 27 [34688/49669]\tLoss: 385.3112\n",
      "Training Epoch: 27 [34752/49669]\tLoss: 365.5526\n",
      "Training Epoch: 27 [34816/49669]\tLoss: 398.9438\n",
      "Training Epoch: 27 [34880/49669]\tLoss: 401.0745\n",
      "Training Epoch: 27 [34944/49669]\tLoss: 418.5536\n",
      "Training Epoch: 27 [35008/49669]\tLoss: 412.7655\n",
      "Training Epoch: 27 [35072/49669]\tLoss: 393.4633\n",
      "Training Epoch: 27 [35136/49669]\tLoss: 390.9998\n",
      "Training Epoch: 27 [35200/49669]\tLoss: 408.5211\n",
      "Training Epoch: 27 [35264/49669]\tLoss: 408.5688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [35328/49669]\tLoss: 401.7394\n",
      "Training Epoch: 27 [35392/49669]\tLoss: 400.7675\n",
      "Training Epoch: 27 [35456/49669]\tLoss: 418.8795\n",
      "Training Epoch: 27 [35520/49669]\tLoss: 365.1646\n",
      "Training Epoch: 27 [35584/49669]\tLoss: 410.6468\n",
      "Training Epoch: 27 [35648/49669]\tLoss: 385.4223\n",
      "Training Epoch: 27 [35712/49669]\tLoss: 420.2221\n",
      "Training Epoch: 27 [35776/49669]\tLoss: 437.8239\n",
      "Training Epoch: 27 [35840/49669]\tLoss: 433.9928\n",
      "Training Epoch: 27 [35904/49669]\tLoss: 397.3222\n",
      "Training Epoch: 27 [35968/49669]\tLoss: 379.7874\n",
      "Training Epoch: 27 [36032/49669]\tLoss: 369.4187\n",
      "Training Epoch: 27 [36096/49669]\tLoss: 399.9725\n",
      "Training Epoch: 27 [36160/49669]\tLoss: 404.8395\n",
      "Training Epoch: 27 [36224/49669]\tLoss: 416.9663\n",
      "Training Epoch: 27 [36288/49669]\tLoss: 431.3881\n",
      "Training Epoch: 27 [36352/49669]\tLoss: 405.4867\n",
      "Training Epoch: 27 [36416/49669]\tLoss: 408.3632\n",
      "Training Epoch: 27 [36480/49669]\tLoss: 394.5984\n",
      "Training Epoch: 27 [36544/49669]\tLoss: 394.5132\n",
      "Training Epoch: 27 [36608/49669]\tLoss: 390.2209\n",
      "Training Epoch: 27 [36672/49669]\tLoss: 384.4384\n",
      "Training Epoch: 27 [36736/49669]\tLoss: 402.9433\n",
      "Training Epoch: 27 [36800/49669]\tLoss: 398.6596\n",
      "Training Epoch: 27 [36864/49669]\tLoss: 404.1353\n",
      "Training Epoch: 27 [36928/49669]\tLoss: 424.9593\n",
      "Training Epoch: 27 [36992/49669]\tLoss: 431.7013\n",
      "Training Epoch: 27 [37056/49669]\tLoss: 431.4681\n",
      "Training Epoch: 27 [37120/49669]\tLoss: 387.9608\n",
      "Training Epoch: 27 [37184/49669]\tLoss: 394.2478\n",
      "Training Epoch: 27 [37248/49669]\tLoss: 400.7640\n",
      "Training Epoch: 27 [37312/49669]\tLoss: 372.4849\n",
      "Training Epoch: 27 [37376/49669]\tLoss: 405.8579\n",
      "Training Epoch: 27 [37440/49669]\tLoss: 409.4594\n",
      "Training Epoch: 27 [37504/49669]\tLoss: 397.4255\n",
      "Training Epoch: 27 [37568/49669]\tLoss: 387.8763\n",
      "Training Epoch: 27 [37632/49669]\tLoss: 396.3463\n",
      "Training Epoch: 27 [37696/49669]\tLoss: 408.7238\n",
      "Training Epoch: 27 [37760/49669]\tLoss: 413.1273\n",
      "Training Epoch: 27 [37824/49669]\tLoss: 396.9843\n",
      "Training Epoch: 27 [37888/49669]\tLoss: 373.8992\n",
      "Training Epoch: 27 [37952/49669]\tLoss: 385.3976\n",
      "Training Epoch: 27 [38016/49669]\tLoss: 395.3693\n",
      "Training Epoch: 27 [38080/49669]\tLoss: 382.6109\n",
      "Training Epoch: 27 [38144/49669]\tLoss: 413.5135\n",
      "Training Epoch: 27 [38208/49669]\tLoss: 401.2495\n",
      "Training Epoch: 27 [38272/49669]\tLoss: 386.3575\n",
      "Training Epoch: 27 [38336/49669]\tLoss: 407.0829\n",
      "Training Epoch: 27 [38400/49669]\tLoss: 402.7638\n",
      "Training Epoch: 27 [38464/49669]\tLoss: 399.5623\n",
      "Training Epoch: 27 [38528/49669]\tLoss: 408.0420\n",
      "Training Epoch: 27 [38592/49669]\tLoss: 396.8199\n",
      "Training Epoch: 27 [38656/49669]\tLoss: 402.0536\n",
      "Training Epoch: 27 [38720/49669]\tLoss: 413.8214\n",
      "Training Epoch: 27 [38784/49669]\tLoss: 362.2202\n",
      "Training Epoch: 27 [38848/49669]\tLoss: 419.2958\n",
      "Training Epoch: 27 [38912/49669]\tLoss: 395.2880\n",
      "Training Epoch: 27 [38976/49669]\tLoss: 414.0138\n",
      "Training Epoch: 27 [39040/49669]\tLoss: 415.2825\n",
      "Training Epoch: 27 [39104/49669]\tLoss: 388.5442\n",
      "Training Epoch: 27 [39168/49669]\tLoss: 409.4235\n",
      "Training Epoch: 27 [39232/49669]\tLoss: 412.4853\n",
      "Training Epoch: 27 [39296/49669]\tLoss: 389.3980\n",
      "Training Epoch: 27 [39360/49669]\tLoss: 370.1589\n",
      "Training Epoch: 27 [39424/49669]\tLoss: 413.3268\n",
      "Training Epoch: 27 [39488/49669]\tLoss: 382.1777\n",
      "Training Epoch: 27 [39552/49669]\tLoss: 388.9161\n",
      "Training Epoch: 27 [39616/49669]\tLoss: 407.1118\n",
      "Training Epoch: 27 [39680/49669]\tLoss: 415.1923\n",
      "Training Epoch: 27 [39744/49669]\tLoss: 424.4954\n",
      "Training Epoch: 27 [39808/49669]\tLoss: 410.4092\n",
      "Training Epoch: 27 [39872/49669]\tLoss: 422.9323\n",
      "Training Epoch: 27 [39936/49669]\tLoss: 396.2820\n",
      "Training Epoch: 27 [40000/49669]\tLoss: 412.4298\n",
      "Training Epoch: 27 [40064/49669]\tLoss: 385.6198\n",
      "Training Epoch: 27 [40128/49669]\tLoss: 398.5687\n",
      "Training Epoch: 27 [40192/49669]\tLoss: 399.1816\n",
      "Training Epoch: 27 [40256/49669]\tLoss: 423.8602\n",
      "Training Epoch: 27 [40320/49669]\tLoss: 399.0816\n",
      "Training Epoch: 27 [40384/49669]\tLoss: 411.2262\n",
      "Training Epoch: 27 [40448/49669]\tLoss: 407.3907\n",
      "Training Epoch: 27 [40512/49669]\tLoss: 394.4221\n",
      "Training Epoch: 27 [40576/49669]\tLoss: 400.5395\n",
      "Training Epoch: 27 [40640/49669]\tLoss: 399.5958\n",
      "Training Epoch: 27 [40704/49669]\tLoss: 376.7624\n",
      "Training Epoch: 27 [40768/49669]\tLoss: 424.3285\n",
      "Training Epoch: 27 [40832/49669]\tLoss: 384.6859\n",
      "Training Epoch: 27 [40896/49669]\tLoss: 411.4933\n",
      "Training Epoch: 27 [40960/49669]\tLoss: 413.1757\n",
      "Training Epoch: 27 [41024/49669]\tLoss: 370.6235\n",
      "Training Epoch: 27 [41088/49669]\tLoss: 391.5311\n",
      "Training Epoch: 27 [41152/49669]\tLoss: 405.4687\n",
      "Training Epoch: 27 [41216/49669]\tLoss: 405.6020\n",
      "Training Epoch: 27 [41280/49669]\tLoss: 381.6840\n",
      "Training Epoch: 27 [41344/49669]\tLoss: 400.6216\n",
      "Training Epoch: 27 [41408/49669]\tLoss: 415.4920\n",
      "Training Epoch: 27 [41472/49669]\tLoss: 379.4428\n",
      "Training Epoch: 27 [41536/49669]\tLoss: 382.7268\n",
      "Training Epoch: 27 [41600/49669]\tLoss: 388.5717\n",
      "Training Epoch: 27 [41664/49669]\tLoss: 420.9890\n",
      "Training Epoch: 27 [41728/49669]\tLoss: 383.9648\n",
      "Training Epoch: 27 [41792/49669]\tLoss: 389.7721\n",
      "Training Epoch: 27 [41856/49669]\tLoss: 370.9290\n",
      "Training Epoch: 27 [41920/49669]\tLoss: 417.9097\n",
      "Training Epoch: 27 [41984/49669]\tLoss: 383.7802\n",
      "Training Epoch: 27 [42048/49669]\tLoss: 403.1896\n",
      "Training Epoch: 27 [42112/49669]\tLoss: 406.6237\n",
      "Training Epoch: 27 [42176/49669]\tLoss: 402.1127\n",
      "Training Epoch: 27 [42240/49669]\tLoss: 407.2727\n",
      "Training Epoch: 27 [42304/49669]\tLoss: 416.7337\n",
      "Training Epoch: 27 [42368/49669]\tLoss: 417.1667\n",
      "Training Epoch: 27 [42432/49669]\tLoss: 428.3556\n",
      "Training Epoch: 27 [42496/49669]\tLoss: 399.1417\n",
      "Training Epoch: 27 [42560/49669]\tLoss: 382.0043\n",
      "Training Epoch: 27 [42624/49669]\tLoss: 406.5068\n",
      "Training Epoch: 27 [42688/49669]\tLoss: 413.4731\n",
      "Training Epoch: 27 [42752/49669]\tLoss: 445.8669\n",
      "Training Epoch: 27 [42816/49669]\tLoss: 401.4133\n",
      "Training Epoch: 27 [42880/49669]\tLoss: 391.0620\n",
      "Training Epoch: 27 [42944/49669]\tLoss: 418.6119\n",
      "Training Epoch: 27 [43008/49669]\tLoss: 399.7941\n",
      "Training Epoch: 27 [43072/49669]\tLoss: 388.9688\n",
      "Training Epoch: 27 [43136/49669]\tLoss: 429.2287\n",
      "Training Epoch: 27 [43200/49669]\tLoss: 375.3123\n",
      "Training Epoch: 27 [43264/49669]\tLoss: 393.5015\n",
      "Training Epoch: 27 [43328/49669]\tLoss: 426.1528\n",
      "Training Epoch: 27 [43392/49669]\tLoss: 409.2413\n",
      "Training Epoch: 27 [43456/49669]\tLoss: 403.9168\n",
      "Training Epoch: 27 [43520/49669]\tLoss: 411.7323\n",
      "Training Epoch: 27 [43584/49669]\tLoss: 415.0590\n",
      "Training Epoch: 27 [43648/49669]\tLoss: 402.0144\n",
      "Training Epoch: 27 [43712/49669]\tLoss: 444.6650\n",
      "Training Epoch: 27 [43776/49669]\tLoss: 420.1512\n",
      "Training Epoch: 27 [43840/49669]\tLoss: 384.0318\n",
      "Training Epoch: 27 [43904/49669]\tLoss: 418.3948\n",
      "Training Epoch: 27 [43968/49669]\tLoss: 409.2373\n",
      "Training Epoch: 27 [44032/49669]\tLoss: 446.1509\n",
      "Training Epoch: 27 [44096/49669]\tLoss: 427.5826\n",
      "Training Epoch: 27 [44160/49669]\tLoss: 450.4748\n",
      "Training Epoch: 27 [44224/49669]\tLoss: 405.4722\n",
      "Training Epoch: 27 [44288/49669]\tLoss: 438.1700\n",
      "Training Epoch: 27 [44352/49669]\tLoss: 404.9295\n",
      "Training Epoch: 27 [44416/49669]\tLoss: 443.1573\n",
      "Training Epoch: 27 [44480/49669]\tLoss: 390.0993\n",
      "Training Epoch: 27 [44544/49669]\tLoss: 404.0816\n",
      "Training Epoch: 27 [44608/49669]\tLoss: 407.0900\n",
      "Training Epoch: 27 [44672/49669]\tLoss: 394.4977\n",
      "Training Epoch: 27 [44736/49669]\tLoss: 383.8604\n",
      "Training Epoch: 27 [44800/49669]\tLoss: 354.4635\n",
      "Training Epoch: 27 [44864/49669]\tLoss: 383.5702\n",
      "Training Epoch: 27 [44928/49669]\tLoss: 387.1388\n",
      "Training Epoch: 27 [44992/49669]\tLoss: 396.4369\n",
      "Training Epoch: 27 [45056/49669]\tLoss: 397.7997\n",
      "Training Epoch: 27 [45120/49669]\tLoss: 401.1478\n",
      "Training Epoch: 27 [45184/49669]\tLoss: 392.1687\n",
      "Training Epoch: 27 [45248/49669]\tLoss: 416.2285\n",
      "Training Epoch: 27 [45312/49669]\tLoss: 383.5872\n",
      "Training Epoch: 27 [45376/49669]\tLoss: 451.5092\n",
      "Training Epoch: 27 [45440/49669]\tLoss: 370.8750\n",
      "Training Epoch: 27 [45504/49669]\tLoss: 413.2216\n",
      "Training Epoch: 27 [45568/49669]\tLoss: 386.8765\n",
      "Training Epoch: 27 [45632/49669]\tLoss: 417.9388\n",
      "Training Epoch: 27 [45696/49669]\tLoss: 406.7371\n",
      "Training Epoch: 27 [45760/49669]\tLoss: 404.4038\n",
      "Training Epoch: 27 [45824/49669]\tLoss: 441.1504\n",
      "Training Epoch: 27 [45888/49669]\tLoss: 377.1464\n",
      "Training Epoch: 27 [45952/49669]\tLoss: 411.1481\n",
      "Training Epoch: 27 [46016/49669]\tLoss: 395.3654\n",
      "Training Epoch: 27 [46080/49669]\tLoss: 392.2510\n",
      "Training Epoch: 27 [46144/49669]\tLoss: 411.8812\n",
      "Training Epoch: 27 [46208/49669]\tLoss: 418.3525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [46272/49669]\tLoss: 403.1955\n",
      "Training Epoch: 27 [46336/49669]\tLoss: 403.0764\n",
      "Training Epoch: 27 [46400/49669]\tLoss: 408.6662\n",
      "Training Epoch: 27 [46464/49669]\tLoss: 426.4362\n",
      "Training Epoch: 27 [46528/49669]\tLoss: 379.4236\n",
      "Training Epoch: 27 [46592/49669]\tLoss: 392.4240\n",
      "Training Epoch: 27 [46656/49669]\tLoss: 404.1714\n",
      "Training Epoch: 27 [46720/49669]\tLoss: 452.8797\n",
      "Training Epoch: 27 [46784/49669]\tLoss: 404.3690\n",
      "Training Epoch: 27 [46848/49669]\tLoss: 396.5363\n",
      "Training Epoch: 27 [46912/49669]\tLoss: 409.9021\n",
      "Training Epoch: 27 [46976/49669]\tLoss: 401.3122\n",
      "Training Epoch: 27 [47040/49669]\tLoss: 396.2042\n",
      "Training Epoch: 27 [47104/49669]\tLoss: 402.0591\n",
      "Training Epoch: 27 [47168/49669]\tLoss: 370.4762\n",
      "Training Epoch: 27 [47232/49669]\tLoss: 398.6324\n",
      "Training Epoch: 27 [47296/49669]\tLoss: 411.8579\n",
      "Training Epoch: 27 [47360/49669]\tLoss: 401.8789\n",
      "Training Epoch: 27 [47424/49669]\tLoss: 397.9593\n",
      "Training Epoch: 27 [47488/49669]\tLoss: 401.4521\n",
      "Training Epoch: 27 [47552/49669]\tLoss: 396.8520\n",
      "Training Epoch: 27 [47616/49669]\tLoss: 419.1425\n",
      "Training Epoch: 27 [47680/49669]\tLoss: 395.6195\n",
      "Training Epoch: 27 [47744/49669]\tLoss: 420.2269\n",
      "Training Epoch: 27 [47808/49669]\tLoss: 379.2420\n",
      "Training Epoch: 27 [47872/49669]\tLoss: 421.8972\n",
      "Training Epoch: 27 [47936/49669]\tLoss: 383.7623\n",
      "Training Epoch: 27 [48000/49669]\tLoss: 415.0235\n",
      "Training Epoch: 27 [48064/49669]\tLoss: 412.8713\n",
      "Training Epoch: 27 [48128/49669]\tLoss: 413.7072\n",
      "Training Epoch: 27 [48192/49669]\tLoss: 416.0074\n",
      "Training Epoch: 27 [48256/49669]\tLoss: 398.9638\n",
      "Training Epoch: 27 [48320/49669]\tLoss: 381.6447\n",
      "Training Epoch: 27 [48384/49669]\tLoss: 421.5078\n",
      "Training Epoch: 27 [48448/49669]\tLoss: 435.5641\n",
      "Training Epoch: 27 [48512/49669]\tLoss: 402.3452\n",
      "Training Epoch: 27 [48576/49669]\tLoss: 391.0323\n",
      "Training Epoch: 27 [48640/49669]\tLoss: 419.7825\n",
      "Training Epoch: 27 [48704/49669]\tLoss: 391.6596\n",
      "Training Epoch: 27 [48768/49669]\tLoss: 422.4155\n",
      "Training Epoch: 27 [48832/49669]\tLoss: 400.9360\n",
      "Training Epoch: 27 [48896/49669]\tLoss: 419.6306\n",
      "Training Epoch: 27 [48960/49669]\tLoss: 425.3391\n",
      "Training Epoch: 27 [49024/49669]\tLoss: 409.3389\n",
      "Training Epoch: 27 [49088/49669]\tLoss: 419.7567\n",
      "Training Epoch: 27 [49152/49669]\tLoss: 433.9653\n",
      "Training Epoch: 27 [49216/49669]\tLoss: 409.7983\n",
      "Training Epoch: 27 [49280/49669]\tLoss: 418.1582\n",
      "Training Epoch: 27 [49344/49669]\tLoss: 406.7466\n",
      "Training Epoch: 27 [49408/49669]\tLoss: 396.9167\n",
      "Training Epoch: 27 [49472/49669]\tLoss: 392.9557\n",
      "Training Epoch: 27 [49536/49669]\tLoss: 402.9167\n",
      "Training Epoch: 27 [49600/49669]\tLoss: 435.8369\n",
      "Training Epoch: 27 [49664/49669]\tLoss: 409.8966\n",
      "Training Epoch: 27 [49669/49669]\tLoss: 431.4108\n",
      "Training Epoch: 27 [5519/5519]\tLoss: 435.6463\n",
      "Training Epoch: 28 [64/49669]\tLoss: 432.0725\n",
      "Training Epoch: 28 [128/49669]\tLoss: 411.1982\n",
      "Training Epoch: 28 [192/49669]\tLoss: 439.2417\n",
      "Training Epoch: 28 [256/49669]\tLoss: 414.0269\n",
      "Training Epoch: 28 [320/49669]\tLoss: 388.7913\n",
      "Training Epoch: 28 [384/49669]\tLoss: 394.0627\n",
      "Training Epoch: 28 [448/49669]\tLoss: 392.9294\n",
      "Training Epoch: 28 [512/49669]\tLoss: 406.9132\n",
      "Training Epoch: 28 [576/49669]\tLoss: 436.7029\n",
      "Training Epoch: 28 [640/49669]\tLoss: 432.9838\n",
      "Training Epoch: 28 [704/49669]\tLoss: 406.6220\n",
      "Training Epoch: 28 [768/49669]\tLoss: 397.1144\n",
      "Training Epoch: 28 [832/49669]\tLoss: 402.4707\n",
      "Training Epoch: 28 [896/49669]\tLoss: 436.1455\n",
      "Training Epoch: 28 [960/49669]\tLoss: 411.4025\n",
      "Training Epoch: 28 [1024/49669]\tLoss: 393.2191\n",
      "Training Epoch: 28 [1088/49669]\tLoss: 400.2370\n",
      "Training Epoch: 28 [1152/49669]\tLoss: 408.5150\n",
      "Training Epoch: 28 [1216/49669]\tLoss: 432.0944\n",
      "Training Epoch: 28 [1280/49669]\tLoss: 403.9568\n",
      "Training Epoch: 28 [1344/49669]\tLoss: 390.7704\n",
      "Training Epoch: 28 [1408/49669]\tLoss: 435.1604\n",
      "Training Epoch: 28 [1472/49669]\tLoss: 403.0367\n",
      "Training Epoch: 28 [1536/49669]\tLoss: 387.6140\n",
      "Training Epoch: 28 [1600/49669]\tLoss: 402.0450\n",
      "Training Epoch: 28 [1664/49669]\tLoss: 376.2487\n",
      "Training Epoch: 28 [1728/49669]\tLoss: 389.3095\n",
      "Training Epoch: 28 [1792/49669]\tLoss: 399.2109\n",
      "Training Epoch: 28 [1856/49669]\tLoss: 404.9873\n",
      "Training Epoch: 28 [1920/49669]\tLoss: 390.0419\n",
      "Training Epoch: 28 [1984/49669]\tLoss: 433.4182\n",
      "Training Epoch: 28 [2048/49669]\tLoss: 368.8650\n",
      "Training Epoch: 28 [2112/49669]\tLoss: 402.8524\n",
      "Training Epoch: 28 [2176/49669]\tLoss: 397.0492\n",
      "Training Epoch: 28 [2240/49669]\tLoss: 417.8401\n",
      "Training Epoch: 28 [2304/49669]\tLoss: 414.4215\n",
      "Training Epoch: 28 [2368/49669]\tLoss: 413.5956\n",
      "Training Epoch: 28 [2432/49669]\tLoss: 409.9006\n",
      "Training Epoch: 28 [2496/49669]\tLoss: 403.8711\n",
      "Training Epoch: 28 [2560/49669]\tLoss: 400.8416\n",
      "Training Epoch: 28 [2624/49669]\tLoss: 366.7262\n",
      "Training Epoch: 28 [2688/49669]\tLoss: 388.7466\n",
      "Training Epoch: 28 [2752/49669]\tLoss: 380.5198\n",
      "Training Epoch: 28 [2816/49669]\tLoss: 404.2226\n",
      "Training Epoch: 28 [2880/49669]\tLoss: 430.8359\n",
      "Training Epoch: 28 [2944/49669]\tLoss: 404.1254\n",
      "Training Epoch: 28 [3008/49669]\tLoss: 402.2068\n",
      "Training Epoch: 28 [3072/49669]\tLoss: 410.5208\n",
      "Training Epoch: 28 [3136/49669]\tLoss: 404.8524\n",
      "Training Epoch: 28 [3200/49669]\tLoss: 396.3930\n",
      "Training Epoch: 28 [3264/49669]\tLoss: 378.4033\n",
      "Training Epoch: 28 [3328/49669]\tLoss: 400.8421\n",
      "Training Epoch: 28 [3392/49669]\tLoss: 393.4923\n",
      "Training Epoch: 28 [3456/49669]\tLoss: 381.6266\n",
      "Training Epoch: 28 [3520/49669]\tLoss: 402.9689\n",
      "Training Epoch: 28 [3584/49669]\tLoss: 411.9881\n",
      "Training Epoch: 28 [3648/49669]\tLoss: 423.6871\n",
      "Training Epoch: 28 [3712/49669]\tLoss: 356.0213\n",
      "Training Epoch: 28 [3776/49669]\tLoss: 371.0228\n",
      "Training Epoch: 28 [3840/49669]\tLoss: 399.5195\n",
      "Training Epoch: 28 [3904/49669]\tLoss: 414.6835\n",
      "Training Epoch: 28 [3968/49669]\tLoss: 412.1583\n",
      "Training Epoch: 28 [4032/49669]\tLoss: 413.6856\n",
      "Training Epoch: 28 [4096/49669]\tLoss: 395.4306\n",
      "Training Epoch: 28 [4160/49669]\tLoss: 422.1913\n",
      "Training Epoch: 28 [4224/49669]\tLoss: 404.7646\n",
      "Training Epoch: 28 [4288/49669]\tLoss: 367.3615\n",
      "Training Epoch: 28 [4352/49669]\tLoss: 398.4243\n",
      "Training Epoch: 28 [4416/49669]\tLoss: 398.2621\n",
      "Training Epoch: 28 [4480/49669]\tLoss: 423.8220\n",
      "Training Epoch: 28 [4544/49669]\tLoss: 387.8656\n",
      "Training Epoch: 28 [4608/49669]\tLoss: 402.5326\n",
      "Training Epoch: 28 [4672/49669]\tLoss: 420.3955\n",
      "Training Epoch: 28 [4736/49669]\tLoss: 379.1083\n",
      "Training Epoch: 28 [4800/49669]\tLoss: 417.9916\n",
      "Training Epoch: 28 [4864/49669]\tLoss: 412.6756\n",
      "Training Epoch: 28 [4928/49669]\tLoss: 391.2862\n",
      "Training Epoch: 28 [4992/49669]\tLoss: 393.9195\n",
      "Training Epoch: 28 [5056/49669]\tLoss: 372.9186\n",
      "Training Epoch: 28 [5120/49669]\tLoss: 392.4858\n",
      "Training Epoch: 28 [5184/49669]\tLoss: 404.5706\n",
      "Training Epoch: 28 [5248/49669]\tLoss: 399.2971\n",
      "Training Epoch: 28 [5312/49669]\tLoss: 408.2881\n",
      "Training Epoch: 28 [5376/49669]\tLoss: 413.5694\n",
      "Training Epoch: 28 [5440/49669]\tLoss: 411.1952\n",
      "Training Epoch: 28 [5504/49669]\tLoss: 437.6890\n",
      "Training Epoch: 28 [5568/49669]\tLoss: 429.9011\n",
      "Training Epoch: 28 [5632/49669]\tLoss: 404.0554\n",
      "Training Epoch: 28 [5696/49669]\tLoss: 415.9382\n",
      "Training Epoch: 28 [5760/49669]\tLoss: 387.3040\n",
      "Training Epoch: 28 [5824/49669]\tLoss: 420.8196\n",
      "Training Epoch: 28 [5888/49669]\tLoss: 418.3975\n",
      "Training Epoch: 28 [5952/49669]\tLoss: 436.4163\n",
      "Training Epoch: 28 [6016/49669]\tLoss: 373.6428\n",
      "Training Epoch: 28 [6080/49669]\tLoss: 420.3001\n",
      "Training Epoch: 28 [6144/49669]\tLoss: 416.5254\n",
      "Training Epoch: 28 [6208/49669]\tLoss: 419.6497\n",
      "Training Epoch: 28 [6272/49669]\tLoss: 396.0246\n",
      "Training Epoch: 28 [6336/49669]\tLoss: 410.1248\n",
      "Training Epoch: 28 [6400/49669]\tLoss: 428.4998\n",
      "Training Epoch: 28 [6464/49669]\tLoss: 414.9254\n",
      "Training Epoch: 28 [6528/49669]\tLoss: 423.1264\n",
      "Training Epoch: 28 [6592/49669]\tLoss: 409.4668\n",
      "Training Epoch: 28 [6656/49669]\tLoss: 430.9555\n",
      "Training Epoch: 28 [6720/49669]\tLoss: 406.6180\n",
      "Training Epoch: 28 [6784/49669]\tLoss: 440.8734\n",
      "Training Epoch: 28 [6848/49669]\tLoss: 419.8938\n",
      "Training Epoch: 28 [6912/49669]\tLoss: 437.9133\n",
      "Training Epoch: 28 [6976/49669]\tLoss: 408.1996\n",
      "Training Epoch: 28 [7040/49669]\tLoss: 421.2598\n",
      "Training Epoch: 28 [7104/49669]\tLoss: 384.0020\n",
      "Training Epoch: 28 [7168/49669]\tLoss: 395.9690\n",
      "Training Epoch: 28 [7232/49669]\tLoss: 396.0061\n",
      "Training Epoch: 28 [7296/49669]\tLoss: 406.2127\n",
      "Training Epoch: 28 [7360/49669]\tLoss: 416.3473\n",
      "Training Epoch: 28 [7424/49669]\tLoss: 411.5590\n",
      "Training Epoch: 28 [7488/49669]\tLoss: 399.1537\n",
      "Training Epoch: 28 [7552/49669]\tLoss: 427.7165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [7616/49669]\tLoss: 422.2072\n",
      "Training Epoch: 28 [7680/49669]\tLoss: 409.7107\n",
      "Training Epoch: 28 [7744/49669]\tLoss: 419.5075\n",
      "Training Epoch: 28 [7808/49669]\tLoss: 401.4602\n",
      "Training Epoch: 28 [7872/49669]\tLoss: 407.4666\n",
      "Training Epoch: 28 [7936/49669]\tLoss: 370.5431\n",
      "Training Epoch: 28 [8000/49669]\tLoss: 394.5644\n",
      "Training Epoch: 28 [8064/49669]\tLoss: 441.6491\n",
      "Training Epoch: 28 [8128/49669]\tLoss: 424.8891\n",
      "Training Epoch: 28 [8192/49669]\tLoss: 402.5815\n",
      "Training Epoch: 28 [8256/49669]\tLoss: 431.3364\n",
      "Training Epoch: 28 [8320/49669]\tLoss: 442.7186\n",
      "Training Epoch: 28 [8384/49669]\tLoss: 404.3519\n",
      "Training Epoch: 28 [8448/49669]\tLoss: 402.5383\n",
      "Training Epoch: 28 [8512/49669]\tLoss: 426.1873\n",
      "Training Epoch: 28 [8576/49669]\tLoss: 402.5497\n",
      "Training Epoch: 28 [8640/49669]\tLoss: 408.4918\n",
      "Training Epoch: 28 [8704/49669]\tLoss: 416.9507\n",
      "Training Epoch: 28 [8768/49669]\tLoss: 413.0502\n",
      "Training Epoch: 28 [8832/49669]\tLoss: 414.9594\n",
      "Training Epoch: 28 [8896/49669]\tLoss: 421.0814\n",
      "Training Epoch: 28 [8960/49669]\tLoss: 424.9897\n",
      "Training Epoch: 28 [9024/49669]\tLoss: 384.8563\n",
      "Training Epoch: 28 [9088/49669]\tLoss: 412.6700\n",
      "Training Epoch: 28 [9152/49669]\tLoss: 399.5121\n",
      "Training Epoch: 28 [9216/49669]\tLoss: 407.3824\n",
      "Training Epoch: 28 [9280/49669]\tLoss: 412.1769\n",
      "Training Epoch: 28 [9344/49669]\tLoss: 424.2237\n",
      "Training Epoch: 28 [9408/49669]\tLoss: 424.2067\n",
      "Training Epoch: 28 [9472/49669]\tLoss: 436.8870\n",
      "Training Epoch: 28 [9536/49669]\tLoss: 399.1718\n",
      "Training Epoch: 28 [9600/49669]\tLoss: 429.0080\n",
      "Training Epoch: 28 [9664/49669]\tLoss: 415.9862\n",
      "Training Epoch: 28 [9728/49669]\tLoss: 381.1200\n",
      "Training Epoch: 28 [9792/49669]\tLoss: 385.0760\n",
      "Training Epoch: 28 [9856/49669]\tLoss: 392.4144\n",
      "Training Epoch: 28 [9920/49669]\tLoss: 381.6450\n",
      "Training Epoch: 28 [9984/49669]\tLoss: 384.7937\n",
      "Training Epoch: 28 [10048/49669]\tLoss: 383.0314\n",
      "Training Epoch: 28 [10112/49669]\tLoss: 419.7398\n",
      "Training Epoch: 28 [10176/49669]\tLoss: 393.9525\n",
      "Training Epoch: 28 [10240/49669]\tLoss: 401.6592\n",
      "Training Epoch: 28 [10304/49669]\tLoss: 417.4524\n",
      "Training Epoch: 28 [10368/49669]\tLoss: 422.0892\n",
      "Training Epoch: 28 [10432/49669]\tLoss: 388.7691\n",
      "Training Epoch: 28 [10496/49669]\tLoss: 415.5096\n",
      "Training Epoch: 28 [10560/49669]\tLoss: 378.0877\n",
      "Training Epoch: 28 [10624/49669]\tLoss: 393.5424\n",
      "Training Epoch: 28 [10688/49669]\tLoss: 398.9201\n",
      "Training Epoch: 28 [10752/49669]\tLoss: 428.2443\n",
      "Training Epoch: 28 [10816/49669]\tLoss: 409.5276\n",
      "Training Epoch: 28 [10880/49669]\tLoss: 389.3505\n",
      "Training Epoch: 28 [10944/49669]\tLoss: 408.9396\n",
      "Training Epoch: 28 [11008/49669]\tLoss: 407.5219\n",
      "Training Epoch: 28 [11072/49669]\tLoss: 392.7189\n",
      "Training Epoch: 28 [11136/49669]\tLoss: 400.9514\n",
      "Training Epoch: 28 [11200/49669]\tLoss: 374.6888\n",
      "Training Epoch: 28 [11264/49669]\tLoss: 396.1830\n",
      "Training Epoch: 28 [11328/49669]\tLoss: 424.2151\n",
      "Training Epoch: 28 [11392/49669]\tLoss: 415.9251\n",
      "Training Epoch: 28 [11456/49669]\tLoss: 398.7205\n",
      "Training Epoch: 28 [11520/49669]\tLoss: 358.1508\n",
      "Training Epoch: 28 [11584/49669]\tLoss: 398.8073\n",
      "Training Epoch: 28 [11648/49669]\tLoss: 408.1424\n",
      "Training Epoch: 28 [11712/49669]\tLoss: 386.3674\n",
      "Training Epoch: 28 [11776/49669]\tLoss: 427.6165\n",
      "Training Epoch: 28 [11840/49669]\tLoss: 392.6436\n",
      "Training Epoch: 28 [11904/49669]\tLoss: 406.7407\n",
      "Training Epoch: 28 [11968/49669]\tLoss: 413.5420\n",
      "Training Epoch: 28 [12032/49669]\tLoss: 396.1964\n",
      "Training Epoch: 28 [12096/49669]\tLoss: 385.2620\n",
      "Training Epoch: 28 [12160/49669]\tLoss: 368.9511\n",
      "Training Epoch: 28 [12224/49669]\tLoss: 389.7050\n",
      "Training Epoch: 28 [12288/49669]\tLoss: 411.1101\n",
      "Training Epoch: 28 [12352/49669]\tLoss: 401.5534\n",
      "Training Epoch: 28 [12416/49669]\tLoss: 431.8381\n",
      "Training Epoch: 28 [12480/49669]\tLoss: 408.8697\n",
      "Training Epoch: 28 [12544/49669]\tLoss: 410.0769\n",
      "Training Epoch: 28 [12608/49669]\tLoss: 412.0198\n",
      "Training Epoch: 28 [12672/49669]\tLoss: 396.0508\n",
      "Training Epoch: 28 [12736/49669]\tLoss: 433.0844\n",
      "Training Epoch: 28 [12800/49669]\tLoss: 406.5059\n",
      "Training Epoch: 28 [12864/49669]\tLoss: 393.3978\n",
      "Training Epoch: 28 [12928/49669]\tLoss: 399.5710\n",
      "Training Epoch: 28 [12992/49669]\tLoss: 392.9346\n",
      "Training Epoch: 28 [13056/49669]\tLoss: 387.3126\n",
      "Training Epoch: 28 [13120/49669]\tLoss: 419.0232\n",
      "Training Epoch: 28 [13184/49669]\tLoss: 421.7458\n",
      "Training Epoch: 28 [13248/49669]\tLoss: 417.2086\n",
      "Training Epoch: 28 [13312/49669]\tLoss: 388.1886\n",
      "Training Epoch: 28 [13376/49669]\tLoss: 383.1722\n",
      "Training Epoch: 28 [13440/49669]\tLoss: 376.1163\n",
      "Training Epoch: 28 [13504/49669]\tLoss: 418.1544\n",
      "Training Epoch: 28 [13568/49669]\tLoss: 408.7085\n",
      "Training Epoch: 28 [13632/49669]\tLoss: 408.6779\n",
      "Training Epoch: 28 [13696/49669]\tLoss: 380.6284\n",
      "Training Epoch: 28 [13760/49669]\tLoss: 400.3289\n",
      "Training Epoch: 28 [13824/49669]\tLoss: 395.0343\n",
      "Training Epoch: 28 [13888/49669]\tLoss: 406.0227\n",
      "Training Epoch: 28 [13952/49669]\tLoss: 402.4102\n",
      "Training Epoch: 28 [14016/49669]\tLoss: 381.1937\n",
      "Training Epoch: 28 [14080/49669]\tLoss: 393.3965\n",
      "Training Epoch: 28 [14144/49669]\tLoss: 404.2601\n",
      "Training Epoch: 28 [14208/49669]\tLoss: 403.2360\n",
      "Training Epoch: 28 [14272/49669]\tLoss: 380.7729\n",
      "Training Epoch: 28 [14336/49669]\tLoss: 395.5021\n",
      "Training Epoch: 28 [14400/49669]\tLoss: 403.3997\n",
      "Training Epoch: 28 [14464/49669]\tLoss: 399.1152\n",
      "Training Epoch: 28 [14528/49669]\tLoss: 421.7864\n",
      "Training Epoch: 28 [14592/49669]\tLoss: 395.7512\n",
      "Training Epoch: 28 [14656/49669]\tLoss: 417.5269\n",
      "Training Epoch: 28 [14720/49669]\tLoss: 420.2877\n",
      "Training Epoch: 28 [14784/49669]\tLoss: 413.0867\n",
      "Training Epoch: 28 [14848/49669]\tLoss: 428.7777\n",
      "Training Epoch: 28 [14912/49669]\tLoss: 421.3838\n",
      "Training Epoch: 28 [14976/49669]\tLoss: 427.7346\n",
      "Training Epoch: 28 [15040/49669]\tLoss: 407.0902\n",
      "Training Epoch: 28 [15104/49669]\tLoss: 395.5640\n",
      "Training Epoch: 28 [15168/49669]\tLoss: 396.4009\n",
      "Training Epoch: 28 [15232/49669]\tLoss: 412.1855\n",
      "Training Epoch: 28 [15296/49669]\tLoss: 387.4960\n",
      "Training Epoch: 28 [15360/49669]\tLoss: 376.7188\n",
      "Training Epoch: 28 [15424/49669]\tLoss: 382.1863\n",
      "Training Epoch: 28 [15488/49669]\tLoss: 408.1076\n",
      "Training Epoch: 28 [15552/49669]\tLoss: 396.3784\n",
      "Training Epoch: 28 [15616/49669]\tLoss: 397.7321\n",
      "Training Epoch: 28 [15680/49669]\tLoss: 438.9908\n",
      "Training Epoch: 28 [15744/49669]\tLoss: 399.3685\n",
      "Training Epoch: 28 [15808/49669]\tLoss: 417.8671\n",
      "Training Epoch: 28 [15872/49669]\tLoss: 387.9764\n",
      "Training Epoch: 28 [15936/49669]\tLoss: 391.7041\n",
      "Training Epoch: 28 [16000/49669]\tLoss: 393.1437\n",
      "Training Epoch: 28 [16064/49669]\tLoss: 405.8926\n",
      "Training Epoch: 28 [16128/49669]\tLoss: 392.2004\n",
      "Training Epoch: 28 [16192/49669]\tLoss: 402.2102\n",
      "Training Epoch: 28 [16256/49669]\tLoss: 405.8701\n",
      "Training Epoch: 28 [16320/49669]\tLoss: 395.6737\n",
      "Training Epoch: 28 [16384/49669]\tLoss: 426.4681\n",
      "Training Epoch: 28 [16448/49669]\tLoss: 428.0917\n",
      "Training Epoch: 28 [16512/49669]\tLoss: 458.8907\n",
      "Training Epoch: 28 [16576/49669]\tLoss: 464.5644\n",
      "Training Epoch: 28 [16640/49669]\tLoss: 476.6293\n",
      "Training Epoch: 28 [16704/49669]\tLoss: 449.2709\n",
      "Training Epoch: 28 [16768/49669]\tLoss: 449.6964\n",
      "Training Epoch: 28 [16832/49669]\tLoss: 420.2573\n",
      "Training Epoch: 28 [16896/49669]\tLoss: 393.7932\n",
      "Training Epoch: 28 [16960/49669]\tLoss: 431.8149\n",
      "Training Epoch: 28 [17024/49669]\tLoss: 419.9572\n",
      "Training Epoch: 28 [17088/49669]\tLoss: 400.3407\n",
      "Training Epoch: 28 [17152/49669]\tLoss: 389.4153\n",
      "Training Epoch: 28 [17216/49669]\tLoss: 416.5048\n",
      "Training Epoch: 28 [17280/49669]\tLoss: 448.5287\n",
      "Training Epoch: 28 [17344/49669]\tLoss: 442.2942\n",
      "Training Epoch: 28 [17408/49669]\tLoss: 455.5120\n",
      "Training Epoch: 28 [17472/49669]\tLoss: 460.8681\n",
      "Training Epoch: 28 [17536/49669]\tLoss: 428.6657\n",
      "Training Epoch: 28 [17600/49669]\tLoss: 414.1054\n",
      "Training Epoch: 28 [17664/49669]\tLoss: 403.6485\n",
      "Training Epoch: 28 [17728/49669]\tLoss: 406.7505\n",
      "Training Epoch: 28 [17792/49669]\tLoss: 419.4181\n",
      "Training Epoch: 28 [17856/49669]\tLoss: 407.1753\n",
      "Training Epoch: 28 [17920/49669]\tLoss: 437.2882\n",
      "Training Epoch: 28 [17984/49669]\tLoss: 391.4458\n",
      "Training Epoch: 28 [18048/49669]\tLoss: 405.5246\n",
      "Training Epoch: 28 [18112/49669]\tLoss: 447.8216\n",
      "Training Epoch: 28 [18176/49669]\tLoss: 400.7848\n",
      "Training Epoch: 28 [18240/49669]\tLoss: 407.3716\n",
      "Training Epoch: 28 [18304/49669]\tLoss: 418.7387\n",
      "Training Epoch: 28 [18368/49669]\tLoss: 421.1004\n",
      "Training Epoch: 28 [18432/49669]\tLoss: 399.4787\n",
      "Training Epoch: 28 [18496/49669]\tLoss: 384.2155\n",
      "Training Epoch: 28 [18560/49669]\tLoss: 417.2872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [18624/49669]\tLoss: 395.5304\n",
      "Training Epoch: 28 [18688/49669]\tLoss: 410.3055\n",
      "Training Epoch: 28 [18752/49669]\tLoss: 418.4026\n",
      "Training Epoch: 28 [18816/49669]\tLoss: 425.7836\n",
      "Training Epoch: 28 [18880/49669]\tLoss: 392.1302\n",
      "Training Epoch: 28 [18944/49669]\tLoss: 410.5497\n",
      "Training Epoch: 28 [19008/49669]\tLoss: 405.8972\n",
      "Training Epoch: 28 [19072/49669]\tLoss: 409.1027\n",
      "Training Epoch: 28 [19136/49669]\tLoss: 386.6304\n",
      "Training Epoch: 28 [19200/49669]\tLoss: 388.2404\n",
      "Training Epoch: 28 [19264/49669]\tLoss: 377.6294\n",
      "Training Epoch: 28 [19328/49669]\tLoss: 410.6660\n",
      "Training Epoch: 28 [19392/49669]\tLoss: 376.6329\n",
      "Training Epoch: 28 [19456/49669]\tLoss: 405.3841\n",
      "Training Epoch: 28 [19520/49669]\tLoss: 417.7780\n",
      "Training Epoch: 28 [19584/49669]\tLoss: 417.7682\n",
      "Training Epoch: 28 [19648/49669]\tLoss: 384.8342\n",
      "Training Epoch: 28 [19712/49669]\tLoss: 411.4398\n",
      "Training Epoch: 28 [19776/49669]\tLoss: 429.0655\n",
      "Training Epoch: 28 [19840/49669]\tLoss: 383.0393\n",
      "Training Epoch: 28 [19904/49669]\tLoss: 379.3756\n",
      "Training Epoch: 28 [19968/49669]\tLoss: 403.5186\n",
      "Training Epoch: 28 [20032/49669]\tLoss: 398.3587\n",
      "Training Epoch: 28 [20096/49669]\tLoss: 400.3771\n",
      "Training Epoch: 28 [20160/49669]\tLoss: 390.8594\n",
      "Training Epoch: 28 [20224/49669]\tLoss: 389.4099\n",
      "Training Epoch: 28 [20288/49669]\tLoss: 386.0877\n",
      "Training Epoch: 28 [20352/49669]\tLoss: 386.6881\n",
      "Training Epoch: 28 [20416/49669]\tLoss: 403.1773\n",
      "Training Epoch: 28 [20480/49669]\tLoss: 398.7857\n",
      "Training Epoch: 28 [20544/49669]\tLoss: 415.7205\n",
      "Training Epoch: 28 [20608/49669]\tLoss: 426.7315\n",
      "Training Epoch: 28 [20672/49669]\tLoss: 379.9766\n",
      "Training Epoch: 28 [20736/49669]\tLoss: 371.0900\n",
      "Training Epoch: 28 [20800/49669]\tLoss: 406.7533\n",
      "Training Epoch: 28 [20864/49669]\tLoss: 413.1722\n",
      "Training Epoch: 28 [20928/49669]\tLoss: 415.2757\n",
      "Training Epoch: 28 [20992/49669]\tLoss: 358.0717\n",
      "Training Epoch: 28 [21056/49669]\tLoss: 420.1415\n",
      "Training Epoch: 28 [21120/49669]\tLoss: 397.3860\n",
      "Training Epoch: 28 [21184/49669]\tLoss: 424.7263\n",
      "Training Epoch: 28 [21248/49669]\tLoss: 422.2169\n",
      "Training Epoch: 28 [21312/49669]\tLoss: 403.5632\n",
      "Training Epoch: 28 [21376/49669]\tLoss: 389.8308\n",
      "Training Epoch: 28 [21440/49669]\tLoss: 405.4279\n",
      "Training Epoch: 28 [21504/49669]\tLoss: 385.0063\n",
      "Training Epoch: 28 [21568/49669]\tLoss: 412.1062\n",
      "Training Epoch: 28 [21632/49669]\tLoss: 408.3399\n",
      "Training Epoch: 28 [21696/49669]\tLoss: 395.0857\n",
      "Training Epoch: 28 [21760/49669]\tLoss: 417.1470\n",
      "Training Epoch: 28 [21824/49669]\tLoss: 405.0721\n",
      "Training Epoch: 28 [21888/49669]\tLoss: 391.4336\n",
      "Training Epoch: 28 [21952/49669]\tLoss: 371.0456\n",
      "Training Epoch: 28 [22016/49669]\tLoss: 397.8659\n",
      "Training Epoch: 28 [22080/49669]\tLoss: 415.3925\n",
      "Training Epoch: 28 [22144/49669]\tLoss: 402.2977\n",
      "Training Epoch: 28 [22208/49669]\tLoss: 407.7869\n",
      "Training Epoch: 28 [22272/49669]\tLoss: 419.8235\n",
      "Training Epoch: 28 [22336/49669]\tLoss: 401.5317\n",
      "Training Epoch: 28 [22400/49669]\tLoss: 404.1024\n",
      "Training Epoch: 28 [22464/49669]\tLoss: 414.0417\n",
      "Training Epoch: 28 [22528/49669]\tLoss: 414.3240\n",
      "Training Epoch: 28 [22592/49669]\tLoss: 400.2506\n",
      "Training Epoch: 28 [22656/49669]\tLoss: 358.5952\n",
      "Training Epoch: 28 [22720/49669]\tLoss: 405.2562\n",
      "Training Epoch: 28 [22784/49669]\tLoss: 412.2122\n",
      "Training Epoch: 28 [22848/49669]\tLoss: 381.1873\n",
      "Training Epoch: 28 [22912/49669]\tLoss: 394.1346\n",
      "Training Epoch: 28 [22976/49669]\tLoss: 403.8358\n",
      "Training Epoch: 28 [23040/49669]\tLoss: 387.3756\n",
      "Training Epoch: 28 [23104/49669]\tLoss: 381.0583\n",
      "Training Epoch: 28 [23168/49669]\tLoss: 386.8452\n",
      "Training Epoch: 28 [23232/49669]\tLoss: 388.0271\n",
      "Training Epoch: 28 [23296/49669]\tLoss: 390.6859\n",
      "Training Epoch: 28 [23360/49669]\tLoss: 384.2093\n",
      "Training Epoch: 28 [23424/49669]\tLoss: 405.6274\n",
      "Training Epoch: 28 [23488/49669]\tLoss: 411.2823\n",
      "Training Epoch: 28 [23552/49669]\tLoss: 407.1118\n",
      "Training Epoch: 28 [23616/49669]\tLoss: 412.3976\n",
      "Training Epoch: 28 [23680/49669]\tLoss: 413.1720\n",
      "Training Epoch: 28 [23744/49669]\tLoss: 398.5781\n",
      "Training Epoch: 28 [23808/49669]\tLoss: 391.2597\n",
      "Training Epoch: 28 [23872/49669]\tLoss: 397.6429\n",
      "Training Epoch: 28 [23936/49669]\tLoss: 403.0369\n",
      "Training Epoch: 28 [24000/49669]\tLoss: 422.0076\n",
      "Training Epoch: 28 [24064/49669]\tLoss: 403.8791\n",
      "Training Epoch: 28 [24128/49669]\tLoss: 381.8207\n",
      "Training Epoch: 28 [24192/49669]\tLoss: 405.4088\n",
      "Training Epoch: 28 [24256/49669]\tLoss: 400.4399\n",
      "Training Epoch: 28 [24320/49669]\tLoss: 393.4177\n",
      "Training Epoch: 28 [24384/49669]\tLoss: 416.4910\n",
      "Training Epoch: 28 [24448/49669]\tLoss: 370.6218\n",
      "Training Epoch: 28 [24512/49669]\tLoss: 391.7001\n",
      "Training Epoch: 28 [24576/49669]\tLoss: 371.5386\n",
      "Training Epoch: 28 [24640/49669]\tLoss: 391.4169\n",
      "Training Epoch: 28 [24704/49669]\tLoss: 411.0867\n",
      "Training Epoch: 28 [24768/49669]\tLoss: 409.1744\n",
      "Training Epoch: 28 [24832/49669]\tLoss: 414.3961\n",
      "Training Epoch: 28 [24896/49669]\tLoss: 411.7426\n",
      "Training Epoch: 28 [24960/49669]\tLoss: 394.1613\n",
      "Training Epoch: 28 [25024/49669]\tLoss: 421.6485\n",
      "Training Epoch: 28 [25088/49669]\tLoss: 399.2650\n",
      "Training Epoch: 28 [25152/49669]\tLoss: 399.6364\n",
      "Training Epoch: 28 [25216/49669]\tLoss: 394.7267\n",
      "Training Epoch: 28 [25280/49669]\tLoss: 390.7141\n",
      "Training Epoch: 28 [25344/49669]\tLoss: 392.5630\n",
      "Training Epoch: 28 [25408/49669]\tLoss: 404.0643\n",
      "Training Epoch: 28 [25472/49669]\tLoss: 417.8727\n",
      "Training Epoch: 28 [25536/49669]\tLoss: 414.4628\n",
      "Training Epoch: 28 [25600/49669]\tLoss: 393.6249\n",
      "Training Epoch: 28 [25664/49669]\tLoss: 378.2700\n",
      "Training Epoch: 28 [25728/49669]\tLoss: 408.8430\n",
      "Training Epoch: 28 [25792/49669]\tLoss: 425.8361\n",
      "Training Epoch: 28 [25856/49669]\tLoss: 399.6766\n",
      "Training Epoch: 28 [25920/49669]\tLoss: 400.8037\n",
      "Training Epoch: 28 [25984/49669]\tLoss: 443.1951\n",
      "Training Epoch: 28 [26048/49669]\tLoss: 396.9085\n",
      "Training Epoch: 28 [26112/49669]\tLoss: 398.2159\n",
      "Training Epoch: 28 [26176/49669]\tLoss: 410.6960\n",
      "Training Epoch: 28 [26240/49669]\tLoss: 367.3466\n",
      "Training Epoch: 28 [26304/49669]\tLoss: 408.4429\n",
      "Training Epoch: 28 [26368/49669]\tLoss: 388.0598\n",
      "Training Epoch: 28 [26432/49669]\tLoss: 407.6311\n",
      "Training Epoch: 28 [26496/49669]\tLoss: 412.5800\n",
      "Training Epoch: 28 [26560/49669]\tLoss: 393.5619\n",
      "Training Epoch: 28 [26624/49669]\tLoss: 406.1763\n",
      "Training Epoch: 28 [26688/49669]\tLoss: 402.2504\n",
      "Training Epoch: 28 [26752/49669]\tLoss: 403.3559\n",
      "Training Epoch: 28 [26816/49669]\tLoss: 416.1147\n",
      "Training Epoch: 28 [26880/49669]\tLoss: 429.4017\n",
      "Training Epoch: 28 [26944/49669]\tLoss: 415.3536\n",
      "Training Epoch: 28 [27008/49669]\tLoss: 411.7291\n",
      "Training Epoch: 28 [27072/49669]\tLoss: 410.9948\n",
      "Training Epoch: 28 [27136/49669]\tLoss: 390.8175\n",
      "Training Epoch: 28 [27200/49669]\tLoss: 413.1493\n",
      "Training Epoch: 28 [27264/49669]\tLoss: 381.1597\n",
      "Training Epoch: 28 [27328/49669]\tLoss: 385.2824\n",
      "Training Epoch: 28 [27392/49669]\tLoss: 400.5763\n",
      "Training Epoch: 28 [27456/49669]\tLoss: 407.1327\n",
      "Training Epoch: 28 [27520/49669]\tLoss: 387.4357\n",
      "Training Epoch: 28 [27584/49669]\tLoss: 398.5237\n",
      "Training Epoch: 28 [27648/49669]\tLoss: 409.4207\n",
      "Training Epoch: 28 [27712/49669]\tLoss: 398.5450\n",
      "Training Epoch: 28 [27776/49669]\tLoss: 390.5099\n",
      "Training Epoch: 28 [27840/49669]\tLoss: 412.8105\n",
      "Training Epoch: 28 [27904/49669]\tLoss: 382.3669\n",
      "Training Epoch: 28 [27968/49669]\tLoss: 427.7503\n",
      "Training Epoch: 28 [28032/49669]\tLoss: 402.8874\n",
      "Training Epoch: 28 [28096/49669]\tLoss: 395.7197\n",
      "Training Epoch: 28 [28160/49669]\tLoss: 406.9466\n",
      "Training Epoch: 28 [28224/49669]\tLoss: 392.7064\n",
      "Training Epoch: 28 [28288/49669]\tLoss: 426.1869\n",
      "Training Epoch: 28 [28352/49669]\tLoss: 404.0436\n",
      "Training Epoch: 28 [28416/49669]\tLoss: 382.0555\n",
      "Training Epoch: 28 [28480/49669]\tLoss: 378.1168\n",
      "Training Epoch: 28 [28544/49669]\tLoss: 376.9000\n",
      "Training Epoch: 28 [28608/49669]\tLoss: 427.7755\n",
      "Training Epoch: 28 [28672/49669]\tLoss: 401.6806\n",
      "Training Epoch: 28 [28736/49669]\tLoss: 437.5555\n",
      "Training Epoch: 28 [28800/49669]\tLoss: 419.6106\n",
      "Training Epoch: 28 [28864/49669]\tLoss: 406.9159\n",
      "Training Epoch: 28 [28928/49669]\tLoss: 405.2402\n",
      "Training Epoch: 28 [28992/49669]\tLoss: 380.1111\n",
      "Training Epoch: 28 [29056/49669]\tLoss: 428.2223\n",
      "Training Epoch: 28 [29120/49669]\tLoss: 405.5925\n",
      "Training Epoch: 28 [29184/49669]\tLoss: 397.8215\n",
      "Training Epoch: 28 [29248/49669]\tLoss: 415.8890\n",
      "Training Epoch: 28 [29312/49669]\tLoss: 416.2408\n",
      "Training Epoch: 28 [29376/49669]\tLoss: 412.3163\n",
      "Training Epoch: 28 [29440/49669]\tLoss: 420.1472\n",
      "Training Epoch: 28 [29504/49669]\tLoss: 409.6001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [29568/49669]\tLoss: 433.6353\n",
      "Training Epoch: 28 [29632/49669]\tLoss: 422.7314\n",
      "Training Epoch: 28 [29696/49669]\tLoss: 413.6786\n",
      "Training Epoch: 28 [29760/49669]\tLoss: 408.5330\n",
      "Training Epoch: 28 [29824/49669]\tLoss: 414.3767\n",
      "Training Epoch: 28 [29888/49669]\tLoss: 398.5984\n",
      "Training Epoch: 28 [29952/49669]\tLoss: 408.7229\n",
      "Training Epoch: 28 [30016/49669]\tLoss: 409.3174\n",
      "Training Epoch: 28 [30080/49669]\tLoss: 383.8630\n",
      "Training Epoch: 28 [30144/49669]\tLoss: 401.0450\n",
      "Training Epoch: 28 [30208/49669]\tLoss: 409.5286\n",
      "Training Epoch: 28 [30272/49669]\tLoss: 399.8263\n",
      "Training Epoch: 28 [30336/49669]\tLoss: 392.6605\n",
      "Training Epoch: 28 [30400/49669]\tLoss: 414.0013\n",
      "Training Epoch: 28 [30464/49669]\tLoss: 415.0612\n",
      "Training Epoch: 28 [30528/49669]\tLoss: 401.0700\n",
      "Training Epoch: 28 [30592/49669]\tLoss: 413.0822\n",
      "Training Epoch: 28 [30656/49669]\tLoss: 435.3688\n",
      "Training Epoch: 28 [30720/49669]\tLoss: 391.6050\n",
      "Training Epoch: 28 [30784/49669]\tLoss: 401.7816\n",
      "Training Epoch: 28 [30848/49669]\tLoss: 397.1192\n",
      "Training Epoch: 28 [30912/49669]\tLoss: 411.3427\n",
      "Training Epoch: 28 [30976/49669]\tLoss: 408.0353\n",
      "Training Epoch: 28 [31040/49669]\tLoss: 386.6034\n",
      "Training Epoch: 28 [31104/49669]\tLoss: 358.7831\n",
      "Training Epoch: 28 [31168/49669]\tLoss: 383.7522\n",
      "Training Epoch: 28 [31232/49669]\tLoss: 389.5580\n",
      "Training Epoch: 28 [31296/49669]\tLoss: 399.9381\n",
      "Training Epoch: 28 [31360/49669]\tLoss: 436.4383\n",
      "Training Epoch: 28 [31424/49669]\tLoss: 423.2540\n",
      "Training Epoch: 28 [31488/49669]\tLoss: 365.9788\n",
      "Training Epoch: 28 [31552/49669]\tLoss: 418.9696\n",
      "Training Epoch: 28 [31616/49669]\tLoss: 410.8327\n",
      "Training Epoch: 28 [31680/49669]\tLoss: 445.6725\n",
      "Training Epoch: 28 [31744/49669]\tLoss: 374.9350\n",
      "Training Epoch: 28 [31808/49669]\tLoss: 393.1611\n",
      "Training Epoch: 28 [31872/49669]\tLoss: 413.0137\n",
      "Training Epoch: 28 [31936/49669]\tLoss: 385.7704\n",
      "Training Epoch: 28 [32000/49669]\tLoss: 438.9957\n",
      "Training Epoch: 28 [32064/49669]\tLoss: 410.0372\n",
      "Training Epoch: 28 [32128/49669]\tLoss: 400.0592\n",
      "Training Epoch: 28 [32192/49669]\tLoss: 406.1449\n",
      "Training Epoch: 28 [32256/49669]\tLoss: 414.0228\n",
      "Training Epoch: 28 [32320/49669]\tLoss: 393.0928\n",
      "Training Epoch: 28 [32384/49669]\tLoss: 410.7892\n",
      "Training Epoch: 28 [32448/49669]\tLoss: 379.6425\n",
      "Training Epoch: 28 [32512/49669]\tLoss: 347.8137\n",
      "Training Epoch: 28 [32576/49669]\tLoss: 416.5016\n",
      "Training Epoch: 28 [32640/49669]\tLoss: 379.4081\n",
      "Training Epoch: 28 [32704/49669]\tLoss: 374.7820\n",
      "Training Epoch: 28 [32768/49669]\tLoss: 404.3494\n",
      "Training Epoch: 28 [32832/49669]\tLoss: 400.9504\n",
      "Training Epoch: 28 [32896/49669]\tLoss: 388.2106\n",
      "Training Epoch: 28 [32960/49669]\tLoss: 385.4432\n",
      "Training Epoch: 28 [33024/49669]\tLoss: 407.7939\n",
      "Training Epoch: 28 [33088/49669]\tLoss: 430.6694\n",
      "Training Epoch: 28 [33152/49669]\tLoss: 396.7616\n",
      "Training Epoch: 28 [33216/49669]\tLoss: 401.6178\n",
      "Training Epoch: 28 [33280/49669]\tLoss: 393.1516\n",
      "Training Epoch: 28 [33344/49669]\tLoss: 418.3725\n",
      "Training Epoch: 28 [33408/49669]\tLoss: 397.9401\n",
      "Training Epoch: 28 [33472/49669]\tLoss: 395.6703\n",
      "Training Epoch: 28 [33536/49669]\tLoss: 401.8519\n",
      "Training Epoch: 28 [33600/49669]\tLoss: 400.0155\n",
      "Training Epoch: 28 [33664/49669]\tLoss: 378.8277\n",
      "Training Epoch: 28 [33728/49669]\tLoss: 392.2003\n",
      "Training Epoch: 28 [33792/49669]\tLoss: 357.0932\n",
      "Training Epoch: 28 [33856/49669]\tLoss: 433.4644\n",
      "Training Epoch: 28 [33920/49669]\tLoss: 422.2025\n",
      "Training Epoch: 28 [33984/49669]\tLoss: 411.2537\n",
      "Training Epoch: 28 [34048/49669]\tLoss: 376.5576\n",
      "Training Epoch: 28 [34112/49669]\tLoss: 403.6019\n",
      "Training Epoch: 28 [34176/49669]\tLoss: 394.3652\n",
      "Training Epoch: 28 [34240/49669]\tLoss: 395.0206\n",
      "Training Epoch: 28 [34304/49669]\tLoss: 406.5354\n",
      "Training Epoch: 28 [34368/49669]\tLoss: 396.1640\n",
      "Training Epoch: 28 [34432/49669]\tLoss: 411.5835\n",
      "Training Epoch: 28 [34496/49669]\tLoss: 408.4088\n",
      "Training Epoch: 28 [34560/49669]\tLoss: 395.5984\n",
      "Training Epoch: 28 [34624/49669]\tLoss: 388.2159\n",
      "Training Epoch: 28 [34688/49669]\tLoss: 378.7339\n",
      "Training Epoch: 28 [34752/49669]\tLoss: 405.2269\n",
      "Training Epoch: 28 [34816/49669]\tLoss: 405.6218\n",
      "Training Epoch: 28 [34880/49669]\tLoss: 421.0605\n",
      "Training Epoch: 28 [34944/49669]\tLoss: 405.2256\n",
      "Training Epoch: 28 [35008/49669]\tLoss: 408.2076\n",
      "Training Epoch: 28 [35072/49669]\tLoss: 405.6210\n",
      "Training Epoch: 28 [35136/49669]\tLoss: 410.3408\n",
      "Training Epoch: 28 [35200/49669]\tLoss: 420.8879\n",
      "Training Epoch: 28 [35264/49669]\tLoss: 447.5331\n",
      "Training Epoch: 28 [35328/49669]\tLoss: 408.1086\n",
      "Training Epoch: 28 [35392/49669]\tLoss: 419.1306\n",
      "Training Epoch: 28 [35456/49669]\tLoss: 417.3480\n",
      "Training Epoch: 28 [35520/49669]\tLoss: 415.0680\n",
      "Training Epoch: 28 [35584/49669]\tLoss: 421.4790\n",
      "Training Epoch: 28 [35648/49669]\tLoss: 415.2477\n",
      "Training Epoch: 28 [35712/49669]\tLoss: 395.0422\n",
      "Training Epoch: 28 [35776/49669]\tLoss: 393.4517\n",
      "Training Epoch: 28 [35840/49669]\tLoss: 403.4029\n",
      "Training Epoch: 28 [35904/49669]\tLoss: 398.3590\n",
      "Training Epoch: 28 [35968/49669]\tLoss: 401.5550\n",
      "Training Epoch: 28 [36032/49669]\tLoss: 411.1428\n",
      "Training Epoch: 28 [36096/49669]\tLoss: 405.8676\n",
      "Training Epoch: 28 [36160/49669]\tLoss: 384.6900\n",
      "Training Epoch: 28 [36224/49669]\tLoss: 392.3821\n",
      "Training Epoch: 28 [36288/49669]\tLoss: 414.5281\n",
      "Training Epoch: 28 [36352/49669]\tLoss: 404.1250\n",
      "Training Epoch: 28 [36416/49669]\tLoss: 408.9213\n",
      "Training Epoch: 28 [36480/49669]\tLoss: 445.5470\n",
      "Training Epoch: 28 [36544/49669]\tLoss: 446.9702\n",
      "Training Epoch: 28 [36608/49669]\tLoss: 437.5067\n",
      "Training Epoch: 28 [36672/49669]\tLoss: 459.2144\n",
      "Training Epoch: 28 [36736/49669]\tLoss: 443.2076\n",
      "Training Epoch: 28 [36800/49669]\tLoss: 419.7406\n",
      "Training Epoch: 28 [36864/49669]\tLoss: 399.9686\n",
      "Training Epoch: 28 [36928/49669]\tLoss: 423.6790\n",
      "Training Epoch: 28 [36992/49669]\tLoss: 413.2299\n",
      "Training Epoch: 28 [37056/49669]\tLoss: 431.1689\n",
      "Training Epoch: 28 [37120/49669]\tLoss: 431.8853\n",
      "Training Epoch: 28 [37184/49669]\tLoss: 427.1866\n",
      "Training Epoch: 28 [37248/49669]\tLoss: 414.1201\n",
      "Training Epoch: 28 [37312/49669]\tLoss: 408.6708\n",
      "Training Epoch: 28 [37376/49669]\tLoss: 399.3973\n",
      "Training Epoch: 28 [37440/49669]\tLoss: 403.7823\n",
      "Training Epoch: 28 [37504/49669]\tLoss: 418.3917\n",
      "Training Epoch: 28 [37568/49669]\tLoss: 398.9073\n",
      "Training Epoch: 28 [37632/49669]\tLoss: 409.6028\n",
      "Training Epoch: 28 [37696/49669]\tLoss: 405.4320\n",
      "Training Epoch: 28 [37760/49669]\tLoss: 379.9395\n",
      "Training Epoch: 28 [37824/49669]\tLoss: 398.0200\n",
      "Training Epoch: 28 [37888/49669]\tLoss: 425.0220\n",
      "Training Epoch: 28 [37952/49669]\tLoss: 374.8333\n",
      "Training Epoch: 28 [38016/49669]\tLoss: 418.9607\n",
      "Training Epoch: 28 [38080/49669]\tLoss: 410.9396\n",
      "Training Epoch: 28 [38144/49669]\tLoss: 395.1937\n",
      "Training Epoch: 28 [38208/49669]\tLoss: 384.6402\n",
      "Training Epoch: 28 [38272/49669]\tLoss: 426.0296\n",
      "Training Epoch: 28 [38336/49669]\tLoss: 412.8199\n",
      "Training Epoch: 28 [38400/49669]\tLoss: 430.5899\n",
      "Training Epoch: 28 [38464/49669]\tLoss: 395.7790\n",
      "Training Epoch: 28 [38528/49669]\tLoss: 409.4141\n",
      "Training Epoch: 28 [38592/49669]\tLoss: 395.0995\n",
      "Training Epoch: 28 [38656/49669]\tLoss: 394.9425\n",
      "Training Epoch: 28 [38720/49669]\tLoss: 382.2232\n",
      "Training Epoch: 28 [38784/49669]\tLoss: 388.1503\n",
      "Training Epoch: 28 [38848/49669]\tLoss: 385.7162\n",
      "Training Epoch: 28 [38912/49669]\tLoss: 361.7812\n",
      "Training Epoch: 28 [38976/49669]\tLoss: 392.3355\n",
      "Training Epoch: 28 [39040/49669]\tLoss: 399.2518\n",
      "Training Epoch: 28 [39104/49669]\tLoss: 408.0388\n",
      "Training Epoch: 28 [39168/49669]\tLoss: 387.0730\n",
      "Training Epoch: 28 [39232/49669]\tLoss: 403.6944\n",
      "Training Epoch: 28 [39296/49669]\tLoss: 422.6415\n",
      "Training Epoch: 28 [39360/49669]\tLoss: 392.8105\n",
      "Training Epoch: 28 [39424/49669]\tLoss: 399.8176\n",
      "Training Epoch: 28 [39488/49669]\tLoss: 381.7603\n",
      "Training Epoch: 28 [39552/49669]\tLoss: 411.3970\n",
      "Training Epoch: 28 [39616/49669]\tLoss: 401.5330\n",
      "Training Epoch: 28 [39680/49669]\tLoss: 417.8324\n",
      "Training Epoch: 28 [39744/49669]\tLoss: 394.2244\n",
      "Training Epoch: 28 [39808/49669]\tLoss: 417.5256\n",
      "Training Epoch: 28 [39872/49669]\tLoss: 395.8608\n",
      "Training Epoch: 28 [39936/49669]\tLoss: 408.7253\n",
      "Training Epoch: 28 [40000/49669]\tLoss: 390.1512\n",
      "Training Epoch: 28 [40064/49669]\tLoss: 384.0070\n",
      "Training Epoch: 28 [40128/49669]\tLoss: 396.5223\n",
      "Training Epoch: 28 [40192/49669]\tLoss: 414.0530\n",
      "Training Epoch: 28 [40256/49669]\tLoss: 394.2851\n",
      "Training Epoch: 28 [40320/49669]\tLoss: 421.2426\n",
      "Training Epoch: 28 [40384/49669]\tLoss: 401.2579\n",
      "Training Epoch: 28 [40448/49669]\tLoss: 424.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [40512/49669]\tLoss: 396.9156\n",
      "Training Epoch: 28 [40576/49669]\tLoss: 423.8334\n",
      "Training Epoch: 28 [40640/49669]\tLoss: 418.9860\n",
      "Training Epoch: 28 [40704/49669]\tLoss: 390.8165\n",
      "Training Epoch: 28 [40768/49669]\tLoss: 421.3763\n",
      "Training Epoch: 28 [40832/49669]\tLoss: 394.5236\n",
      "Training Epoch: 28 [40896/49669]\tLoss: 406.8017\n",
      "Training Epoch: 28 [40960/49669]\tLoss: 405.0974\n",
      "Training Epoch: 28 [41024/49669]\tLoss: 386.1700\n",
      "Training Epoch: 28 [41088/49669]\tLoss: 379.1340\n",
      "Training Epoch: 28 [41152/49669]\tLoss: 398.7783\n",
      "Training Epoch: 28 [41216/49669]\tLoss: 409.5279\n",
      "Training Epoch: 28 [41280/49669]\tLoss: 421.5625\n",
      "Training Epoch: 28 [41344/49669]\tLoss: 413.8186\n",
      "Training Epoch: 28 [41408/49669]\tLoss: 411.5336\n",
      "Training Epoch: 28 [41472/49669]\tLoss: 393.3508\n",
      "Training Epoch: 28 [41536/49669]\tLoss: 390.2818\n",
      "Training Epoch: 28 [41600/49669]\tLoss: 379.3264\n",
      "Training Epoch: 28 [41664/49669]\tLoss: 432.1357\n",
      "Training Epoch: 28 [41728/49669]\tLoss: 380.5859\n",
      "Training Epoch: 28 [41792/49669]\tLoss: 424.9953\n",
      "Training Epoch: 28 [41856/49669]\tLoss: 404.7610\n",
      "Training Epoch: 28 [41920/49669]\tLoss: 404.5547\n",
      "Training Epoch: 28 [41984/49669]\tLoss: 414.7229\n",
      "Training Epoch: 28 [42048/49669]\tLoss: 409.3950\n",
      "Training Epoch: 28 [42112/49669]\tLoss: 398.5166\n",
      "Training Epoch: 28 [42176/49669]\tLoss: 412.9818\n",
      "Training Epoch: 28 [42240/49669]\tLoss: 401.8020\n",
      "Training Epoch: 28 [42304/49669]\tLoss: 407.5709\n",
      "Training Epoch: 28 [42368/49669]\tLoss: 430.7241\n",
      "Training Epoch: 28 [42432/49669]\tLoss: 379.9713\n",
      "Training Epoch: 28 [42496/49669]\tLoss: 384.4291\n",
      "Training Epoch: 28 [42560/49669]\tLoss: 400.7629\n",
      "Training Epoch: 28 [42624/49669]\tLoss: 366.7149\n",
      "Training Epoch: 28 [42688/49669]\tLoss: 417.9468\n",
      "Training Epoch: 28 [42752/49669]\tLoss: 410.0013\n",
      "Training Epoch: 28 [42816/49669]\tLoss: 362.7973\n",
      "Training Epoch: 28 [42880/49669]\tLoss: 370.5287\n",
      "Training Epoch: 28 [42944/49669]\tLoss: 387.6712\n",
      "Training Epoch: 28 [43008/49669]\tLoss: 381.1682\n",
      "Training Epoch: 28 [43072/49669]\tLoss: 378.2829\n",
      "Training Epoch: 28 [43136/49669]\tLoss: 398.7243\n",
      "Training Epoch: 28 [43200/49669]\tLoss: 398.5096\n",
      "Training Epoch: 28 [43264/49669]\tLoss: 401.1000\n",
      "Training Epoch: 28 [43328/49669]\tLoss: 381.0114\n",
      "Training Epoch: 28 [43392/49669]\tLoss: 367.7442\n",
      "Training Epoch: 28 [43456/49669]\tLoss: 400.0347\n",
      "Training Epoch: 28 [43520/49669]\tLoss: 407.4381\n",
      "Training Epoch: 28 [43584/49669]\tLoss: 406.3457\n",
      "Training Epoch: 28 [43648/49669]\tLoss: 403.8863\n",
      "Training Epoch: 28 [43712/49669]\tLoss: 406.7435\n",
      "Training Epoch: 28 [43776/49669]\tLoss: 366.4667\n",
      "Training Epoch: 28 [43840/49669]\tLoss: 381.7019\n",
      "Training Epoch: 28 [43904/49669]\tLoss: 384.2124\n",
      "Training Epoch: 28 [43968/49669]\tLoss: 390.6142\n",
      "Training Epoch: 28 [44032/49669]\tLoss: 383.6049\n",
      "Training Epoch: 28 [44096/49669]\tLoss: 406.8773\n",
      "Training Epoch: 28 [44160/49669]\tLoss: 411.1778\n",
      "Training Epoch: 28 [44224/49669]\tLoss: 386.2729\n",
      "Training Epoch: 28 [44288/49669]\tLoss: 386.7220\n",
      "Training Epoch: 28 [44352/49669]\tLoss: 439.9594\n",
      "Training Epoch: 28 [44416/49669]\tLoss: 411.2917\n",
      "Training Epoch: 28 [44480/49669]\tLoss: 381.9370\n",
      "Training Epoch: 28 [44544/49669]\tLoss: 401.5774\n",
      "Training Epoch: 28 [44608/49669]\tLoss: 381.0249\n",
      "Training Epoch: 28 [44672/49669]\tLoss: 394.1457\n",
      "Training Epoch: 28 [44736/49669]\tLoss: 427.5854\n",
      "Training Epoch: 28 [44800/49669]\tLoss: 420.1053\n",
      "Training Epoch: 28 [44864/49669]\tLoss: 353.3303\n",
      "Training Epoch: 28 [44928/49669]\tLoss: 377.6987\n",
      "Training Epoch: 28 [44992/49669]\tLoss: 427.6758\n",
      "Training Epoch: 28 [45056/49669]\tLoss: 404.7057\n",
      "Training Epoch: 28 [45120/49669]\tLoss: 430.4395\n",
      "Training Epoch: 28 [45184/49669]\tLoss: 410.3777\n",
      "Training Epoch: 28 [45248/49669]\tLoss: 394.4557\n",
      "Training Epoch: 28 [45312/49669]\tLoss: 376.8018\n",
      "Training Epoch: 28 [45376/49669]\tLoss: 422.1157\n",
      "Training Epoch: 28 [45440/49669]\tLoss: 400.6973\n",
      "Training Epoch: 28 [45504/49669]\tLoss: 403.4588\n",
      "Training Epoch: 28 [45568/49669]\tLoss: 404.8587\n",
      "Training Epoch: 28 [45632/49669]\tLoss: 407.5821\n",
      "Training Epoch: 28 [45696/49669]\tLoss: 419.3650\n",
      "Training Epoch: 28 [45760/49669]\tLoss: 421.7330\n",
      "Training Epoch: 28 [45824/49669]\tLoss: 407.0162\n",
      "Training Epoch: 28 [45888/49669]\tLoss: 417.3183\n",
      "Training Epoch: 28 [45952/49669]\tLoss: 395.1918\n",
      "Training Epoch: 28 [46016/49669]\tLoss: 390.1312\n",
      "Training Epoch: 28 [46080/49669]\tLoss: 400.0059\n",
      "Training Epoch: 28 [46144/49669]\tLoss: 381.6866\n",
      "Training Epoch: 28 [46208/49669]\tLoss: 386.1542\n",
      "Training Epoch: 28 [46272/49669]\tLoss: 408.1216\n",
      "Training Epoch: 28 [46336/49669]\tLoss: 424.0265\n",
      "Training Epoch: 28 [46400/49669]\tLoss: 414.1165\n",
      "Training Epoch: 28 [46464/49669]\tLoss: 425.0786\n",
      "Training Epoch: 28 [46528/49669]\tLoss: 406.9430\n",
      "Training Epoch: 28 [46592/49669]\tLoss: 391.9035\n",
      "Training Epoch: 28 [46656/49669]\tLoss: 421.7803\n",
      "Training Epoch: 28 [46720/49669]\tLoss: 395.1866\n",
      "Training Epoch: 28 [46784/49669]\tLoss: 355.2359\n",
      "Training Epoch: 28 [46848/49669]\tLoss: 405.8406\n",
      "Training Epoch: 28 [46912/49669]\tLoss: 424.8755\n",
      "Training Epoch: 28 [46976/49669]\tLoss: 409.3807\n",
      "Training Epoch: 28 [47040/49669]\tLoss: 413.0451\n",
      "Training Epoch: 28 [47104/49669]\tLoss: 407.0113\n",
      "Training Epoch: 28 [47168/49669]\tLoss: 381.3311\n",
      "Training Epoch: 28 [47232/49669]\tLoss: 420.3104\n",
      "Training Epoch: 28 [47296/49669]\tLoss: 419.1616\n",
      "Training Epoch: 28 [47360/49669]\tLoss: 400.1086\n",
      "Training Epoch: 28 [47424/49669]\tLoss: 401.6399\n",
      "Training Epoch: 28 [47488/49669]\tLoss: 407.3436\n",
      "Training Epoch: 28 [47552/49669]\tLoss: 410.7574\n",
      "Training Epoch: 28 [47616/49669]\tLoss: 402.9293\n",
      "Training Epoch: 28 [47680/49669]\tLoss: 407.0704\n",
      "Training Epoch: 28 [47744/49669]\tLoss: 391.3243\n",
      "Training Epoch: 28 [47808/49669]\tLoss: 427.6805\n",
      "Training Epoch: 28 [47872/49669]\tLoss: 404.6707\n",
      "Training Epoch: 28 [47936/49669]\tLoss: 407.8872\n",
      "Training Epoch: 28 [48000/49669]\tLoss: 410.5634\n",
      "Training Epoch: 28 [48064/49669]\tLoss: 401.8781\n",
      "Training Epoch: 28 [48128/49669]\tLoss: 423.5318\n",
      "Training Epoch: 28 [48192/49669]\tLoss: 435.4643\n",
      "Training Epoch: 28 [48256/49669]\tLoss: 471.1452\n",
      "Training Epoch: 28 [48320/49669]\tLoss: 450.0082\n",
      "Training Epoch: 28 [48384/49669]\tLoss: 481.1600\n",
      "Training Epoch: 28 [48448/49669]\tLoss: 406.9241\n",
      "Training Epoch: 28 [48512/49669]\tLoss: 421.7469\n",
      "Training Epoch: 28 [48576/49669]\tLoss: 394.6399\n",
      "Training Epoch: 28 [48640/49669]\tLoss: 409.5861\n",
      "Training Epoch: 28 [48704/49669]\tLoss: 416.4632\n",
      "Training Epoch: 28 [48768/49669]\tLoss: 495.1667\n",
      "Training Epoch: 28 [48832/49669]\tLoss: 474.6033\n",
      "Training Epoch: 28 [48896/49669]\tLoss: 470.7301\n",
      "Training Epoch: 28 [48960/49669]\tLoss: 424.9700\n",
      "Training Epoch: 28 [49024/49669]\tLoss: 396.4637\n",
      "Training Epoch: 28 [49088/49669]\tLoss: 390.3303\n",
      "Training Epoch: 28 [49152/49669]\tLoss: 440.6177\n",
      "Training Epoch: 28 [49216/49669]\tLoss: 416.0516\n",
      "Training Epoch: 28 [49280/49669]\tLoss: 382.6938\n",
      "Training Epoch: 28 [49344/49669]\tLoss: 398.9238\n",
      "Training Epoch: 28 [49408/49669]\tLoss: 408.3422\n",
      "Training Epoch: 28 [49472/49669]\tLoss: 435.7691\n",
      "Training Epoch: 28 [49536/49669]\tLoss: 420.4144\n",
      "Training Epoch: 28 [49600/49669]\tLoss: 405.0305\n",
      "Training Epoch: 28 [49664/49669]\tLoss: 396.6766\n",
      "Training Epoch: 28 [49669/49669]\tLoss: 427.0406\n",
      "Training Epoch: 28 [5519/5519]\tLoss: 414.4088\n",
      "Training Epoch: 29 [64/49669]\tLoss: 433.2116\n",
      "Training Epoch: 29 [128/49669]\tLoss: 388.7117\n",
      "Training Epoch: 29 [192/49669]\tLoss: 398.9637\n",
      "Training Epoch: 29 [256/49669]\tLoss: 424.7881\n",
      "Training Epoch: 29 [320/49669]\tLoss: 439.0635\n",
      "Training Epoch: 29 [384/49669]\tLoss: 413.0255\n",
      "Training Epoch: 29 [448/49669]\tLoss: 403.5252\n",
      "Training Epoch: 29 [512/49669]\tLoss: 400.7672\n",
      "Training Epoch: 29 [576/49669]\tLoss: 399.3637\n",
      "Training Epoch: 29 [640/49669]\tLoss: 408.6092\n",
      "Training Epoch: 29 [704/49669]\tLoss: 427.1592\n",
      "Training Epoch: 29 [768/49669]\tLoss: 440.3501\n",
      "Training Epoch: 29 [832/49669]\tLoss: 425.1472\n",
      "Training Epoch: 29 [896/49669]\tLoss: 375.1074\n",
      "Training Epoch: 29 [960/49669]\tLoss: 394.0576\n",
      "Training Epoch: 29 [1024/49669]\tLoss: 426.7063\n",
      "Training Epoch: 29 [1088/49669]\tLoss: 402.9652\n",
      "Training Epoch: 29 [1152/49669]\tLoss: 395.0632\n",
      "Training Epoch: 29 [1216/49669]\tLoss: 379.7276\n",
      "Training Epoch: 29 [1280/49669]\tLoss: 413.9348\n",
      "Training Epoch: 29 [1344/49669]\tLoss: 399.9760\n",
      "Training Epoch: 29 [1408/49669]\tLoss: 398.6828\n",
      "Training Epoch: 29 [1472/49669]\tLoss: 425.4399\n",
      "Training Epoch: 29 [1536/49669]\tLoss: 425.8399\n",
      "Training Epoch: 29 [1600/49669]\tLoss: 401.9513\n",
      "Training Epoch: 29 [1664/49669]\tLoss: 397.1393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [1728/49669]\tLoss: 410.8109\n",
      "Training Epoch: 29 [1792/49669]\tLoss: 402.1844\n",
      "Training Epoch: 29 [1856/49669]\tLoss: 392.8007\n",
      "Training Epoch: 29 [1920/49669]\tLoss: 408.4084\n",
      "Training Epoch: 29 [1984/49669]\tLoss: 377.5083\n",
      "Training Epoch: 29 [2048/49669]\tLoss: 393.9807\n",
      "Training Epoch: 29 [2112/49669]\tLoss: 402.2947\n",
      "Training Epoch: 29 [2176/49669]\tLoss: 389.6414\n",
      "Training Epoch: 29 [2240/49669]\tLoss: 392.0541\n",
      "Training Epoch: 29 [2304/49669]\tLoss: 411.2081\n",
      "Training Epoch: 29 [2368/49669]\tLoss: 387.3698\n",
      "Training Epoch: 29 [2432/49669]\tLoss: 422.1077\n",
      "Training Epoch: 29 [2496/49669]\tLoss: 391.2401\n",
      "Training Epoch: 29 [2560/49669]\tLoss: 403.1415\n",
      "Training Epoch: 29 [2624/49669]\tLoss: 414.3227\n",
      "Training Epoch: 29 [2688/49669]\tLoss: 367.9657\n",
      "Training Epoch: 29 [2752/49669]\tLoss: 390.7318\n",
      "Training Epoch: 29 [2816/49669]\tLoss: 438.2411\n",
      "Training Epoch: 29 [2880/49669]\tLoss: 386.4484\n",
      "Training Epoch: 29 [2944/49669]\tLoss: 415.9295\n",
      "Training Epoch: 29 [3008/49669]\tLoss: 400.4836\n",
      "Training Epoch: 29 [3072/49669]\tLoss: 419.0824\n",
      "Training Epoch: 29 [3136/49669]\tLoss: 371.5657\n",
      "Training Epoch: 29 [3200/49669]\tLoss: 382.7382\n",
      "Training Epoch: 29 [3264/49669]\tLoss: 392.5822\n",
      "Training Epoch: 29 [3328/49669]\tLoss: 379.7054\n",
      "Training Epoch: 29 [3392/49669]\tLoss: 395.4496\n",
      "Training Epoch: 29 [3456/49669]\tLoss: 386.1222\n",
      "Training Epoch: 29 [3520/49669]\tLoss: 396.0951\n",
      "Training Epoch: 29 [3584/49669]\tLoss: 422.9985\n",
      "Training Epoch: 29 [3648/49669]\tLoss: 413.9084\n",
      "Training Epoch: 29 [3712/49669]\tLoss: 400.4956\n",
      "Training Epoch: 29 [3776/49669]\tLoss: 378.9657\n",
      "Training Epoch: 29 [3840/49669]\tLoss: 390.7379\n",
      "Training Epoch: 29 [3904/49669]\tLoss: 392.2644\n",
      "Training Epoch: 29 [3968/49669]\tLoss: 427.6184\n",
      "Training Epoch: 29 [4032/49669]\tLoss: 368.6156\n",
      "Training Epoch: 29 [4096/49669]\tLoss: 385.8438\n",
      "Training Epoch: 29 [4160/49669]\tLoss: 405.8370\n",
      "Training Epoch: 29 [4224/49669]\tLoss: 417.3198\n",
      "Training Epoch: 29 [4288/49669]\tLoss: 437.1430\n",
      "Training Epoch: 29 [4352/49669]\tLoss: 404.2789\n",
      "Training Epoch: 29 [4416/49669]\tLoss: 415.2182\n",
      "Training Epoch: 29 [4480/49669]\tLoss: 403.4964\n",
      "Training Epoch: 29 [4544/49669]\tLoss: 393.4607\n",
      "Training Epoch: 29 [4608/49669]\tLoss: 405.5523\n",
      "Training Epoch: 29 [4672/49669]\tLoss: 412.1826\n",
      "Training Epoch: 29 [4736/49669]\tLoss: 404.5703\n",
      "Training Epoch: 29 [4800/49669]\tLoss: 412.4874\n",
      "Training Epoch: 29 [4864/49669]\tLoss: 401.7138\n",
      "Training Epoch: 29 [4928/49669]\tLoss: 425.4296\n",
      "Training Epoch: 29 [4992/49669]\tLoss: 381.1256\n",
      "Training Epoch: 29 [5056/49669]\tLoss: 402.2214\n",
      "Training Epoch: 29 [5120/49669]\tLoss: 417.9681\n",
      "Training Epoch: 29 [5184/49669]\tLoss: 389.2661\n",
      "Training Epoch: 29 [5248/49669]\tLoss: 417.3668\n",
      "Training Epoch: 29 [5312/49669]\tLoss: 406.5885\n",
      "Training Epoch: 29 [5376/49669]\tLoss: 400.5000\n",
      "Training Epoch: 29 [5440/49669]\tLoss: 401.9541\n",
      "Training Epoch: 29 [5504/49669]\tLoss: 412.8677\n",
      "Training Epoch: 29 [5568/49669]\tLoss: 384.8799\n",
      "Training Epoch: 29 [5632/49669]\tLoss: 378.6486\n",
      "Training Epoch: 29 [5696/49669]\tLoss: 393.5790\n",
      "Training Epoch: 29 [5760/49669]\tLoss: 420.2000\n",
      "Training Epoch: 29 [5824/49669]\tLoss: 403.6065\n",
      "Training Epoch: 29 [5888/49669]\tLoss: 392.7022\n",
      "Training Epoch: 29 [5952/49669]\tLoss: 374.0955\n",
      "Training Epoch: 29 [6016/49669]\tLoss: 391.2232\n",
      "Training Epoch: 29 [6080/49669]\tLoss: 424.1311\n",
      "Training Epoch: 29 [6144/49669]\tLoss: 419.0888\n",
      "Training Epoch: 29 [6208/49669]\tLoss: 405.3174\n",
      "Training Epoch: 29 [6272/49669]\tLoss: 384.3671\n",
      "Training Epoch: 29 [6336/49669]\tLoss: 398.3905\n",
      "Training Epoch: 29 [6400/49669]\tLoss: 416.8209\n",
      "Training Epoch: 29 [6464/49669]\tLoss: 391.1978\n",
      "Training Epoch: 29 [6528/49669]\tLoss: 397.2843\n",
      "Training Epoch: 29 [6592/49669]\tLoss: 400.6646\n",
      "Training Epoch: 29 [6656/49669]\tLoss: 436.6852\n",
      "Training Epoch: 29 [6720/49669]\tLoss: 395.2623\n",
      "Training Epoch: 29 [6784/49669]\tLoss: 389.6035\n",
      "Training Epoch: 29 [6848/49669]\tLoss: 403.0634\n",
      "Training Epoch: 29 [6912/49669]\tLoss: 409.2459\n",
      "Training Epoch: 29 [6976/49669]\tLoss: 401.0628\n",
      "Training Epoch: 29 [7040/49669]\tLoss: 407.6420\n",
      "Training Epoch: 29 [7104/49669]\tLoss: 376.2532\n",
      "Training Epoch: 29 [7168/49669]\tLoss: 381.5641\n",
      "Training Epoch: 29 [7232/49669]\tLoss: 382.0158\n",
      "Training Epoch: 29 [7296/49669]\tLoss: 392.9076\n",
      "Training Epoch: 29 [7360/49669]\tLoss: 385.5038\n",
      "Training Epoch: 29 [7424/49669]\tLoss: 401.0597\n",
      "Training Epoch: 29 [7488/49669]\tLoss: 426.5635\n",
      "Training Epoch: 29 [7552/49669]\tLoss: 388.5158\n",
      "Training Epoch: 29 [7616/49669]\tLoss: 401.6637\n",
      "Training Epoch: 29 [7680/49669]\tLoss: 370.6093\n",
      "Training Epoch: 29 [7744/49669]\tLoss: 411.4734\n",
      "Training Epoch: 29 [7808/49669]\tLoss: 390.9540\n",
      "Training Epoch: 29 [7872/49669]\tLoss: 393.3348\n",
      "Training Epoch: 29 [7936/49669]\tLoss: 374.8085\n",
      "Training Epoch: 29 [8000/49669]\tLoss: 412.2876\n",
      "Training Epoch: 29 [8064/49669]\tLoss: 404.1246\n",
      "Training Epoch: 29 [8128/49669]\tLoss: 421.6873\n",
      "Training Epoch: 29 [8192/49669]\tLoss: 421.4620\n",
      "Training Epoch: 29 [8256/49669]\tLoss: 423.5915\n",
      "Training Epoch: 29 [8320/49669]\tLoss: 419.6321\n",
      "Training Epoch: 29 [8384/49669]\tLoss: 403.9076\n",
      "Training Epoch: 29 [8448/49669]\tLoss: 406.0047\n",
      "Training Epoch: 29 [8512/49669]\tLoss: 397.4666\n",
      "Training Epoch: 29 [8576/49669]\tLoss: 377.1533\n",
      "Training Epoch: 29 [8640/49669]\tLoss: 436.4238\n",
      "Training Epoch: 29 [8704/49669]\tLoss: 407.7560\n",
      "Training Epoch: 29 [8768/49669]\tLoss: 412.0556\n",
      "Training Epoch: 29 [8832/49669]\tLoss: 372.3098\n",
      "Training Epoch: 29 [8896/49669]\tLoss: 387.9571\n",
      "Training Epoch: 29 [8960/49669]\tLoss: 397.1754\n",
      "Training Epoch: 29 [9024/49669]\tLoss: 370.7027\n",
      "Training Epoch: 29 [9088/49669]\tLoss: 403.0289\n",
      "Training Epoch: 29 [9152/49669]\tLoss: 393.1195\n",
      "Training Epoch: 29 [9216/49669]\tLoss: 396.1791\n",
      "Training Epoch: 29 [9280/49669]\tLoss: 434.7727\n",
      "Training Epoch: 29 [9344/49669]\tLoss: 387.3063\n",
      "Training Epoch: 29 [9408/49669]\tLoss: 415.0774\n",
      "Training Epoch: 29 [9472/49669]\tLoss: 390.7413\n",
      "Training Epoch: 29 [9536/49669]\tLoss: 418.7682\n",
      "Training Epoch: 29 [9600/49669]\tLoss: 397.0905\n",
      "Training Epoch: 29 [9664/49669]\tLoss: 423.3486\n",
      "Training Epoch: 29 [9728/49669]\tLoss: 391.1451\n",
      "Training Epoch: 29 [9792/49669]\tLoss: 384.2985\n",
      "Training Epoch: 29 [9856/49669]\tLoss: 379.2214\n",
      "Training Epoch: 29 [9920/49669]\tLoss: 411.3396\n",
      "Training Epoch: 29 [9984/49669]\tLoss: 420.3509\n",
      "Training Epoch: 29 [10048/49669]\tLoss: 408.6307\n",
      "Training Epoch: 29 [10112/49669]\tLoss: 402.2121\n",
      "Training Epoch: 29 [10176/49669]\tLoss: 401.9734\n",
      "Training Epoch: 29 [10240/49669]\tLoss: 382.5159\n",
      "Training Epoch: 29 [10304/49669]\tLoss: 397.9833\n",
      "Training Epoch: 29 [10368/49669]\tLoss: 391.9707\n",
      "Training Epoch: 29 [10432/49669]\tLoss: 385.6928\n",
      "Training Epoch: 29 [10496/49669]\tLoss: 383.2594\n",
      "Training Epoch: 29 [10560/49669]\tLoss: 414.1841\n",
      "Training Epoch: 29 [10624/49669]\tLoss: 428.1500\n",
      "Training Epoch: 29 [10688/49669]\tLoss: 439.3246\n",
      "Training Epoch: 29 [10752/49669]\tLoss: 407.8328\n",
      "Training Epoch: 29 [10816/49669]\tLoss: 444.3694\n",
      "Training Epoch: 29 [10880/49669]\tLoss: 395.6332\n",
      "Training Epoch: 29 [10944/49669]\tLoss: 405.1286\n",
      "Training Epoch: 29 [11008/49669]\tLoss: 398.0666\n",
      "Training Epoch: 29 [11072/49669]\tLoss: 406.5305\n",
      "Training Epoch: 29 [11136/49669]\tLoss: 368.5491\n",
      "Training Epoch: 29 [11200/49669]\tLoss: 390.4817\n",
      "Training Epoch: 29 [11264/49669]\tLoss: 401.4879\n",
      "Training Epoch: 29 [11328/49669]\tLoss: 408.5653\n",
      "Training Epoch: 29 [11392/49669]\tLoss: 393.2729\n",
      "Training Epoch: 29 [11456/49669]\tLoss: 411.4279\n",
      "Training Epoch: 29 [11520/49669]\tLoss: 396.2998\n",
      "Training Epoch: 29 [11584/49669]\tLoss: 432.1057\n",
      "Training Epoch: 29 [11648/49669]\tLoss: 404.3948\n",
      "Training Epoch: 29 [11712/49669]\tLoss: 403.9127\n",
      "Training Epoch: 29 [11776/49669]\tLoss: 396.1837\n",
      "Training Epoch: 29 [11840/49669]\tLoss: 376.5090\n",
      "Training Epoch: 29 [11904/49669]\tLoss: 376.4926\n",
      "Training Epoch: 29 [11968/49669]\tLoss: 428.2463\n",
      "Training Epoch: 29 [12032/49669]\tLoss: 414.7476\n",
      "Training Epoch: 29 [12096/49669]\tLoss: 395.9859\n",
      "Training Epoch: 29 [12160/49669]\tLoss: 410.9186\n",
      "Training Epoch: 29 [12224/49669]\tLoss: 429.2356\n",
      "Training Epoch: 29 [12288/49669]\tLoss: 376.9814\n",
      "Training Epoch: 29 [12352/49669]\tLoss: 427.2167\n",
      "Training Epoch: 29 [12416/49669]\tLoss: 438.3365\n",
      "Training Epoch: 29 [12480/49669]\tLoss: 430.9286\n",
      "Training Epoch: 29 [12544/49669]\tLoss: 362.4144\n",
      "Training Epoch: 29 [12608/49669]\tLoss: 389.9086\n",
      "Training Epoch: 29 [12672/49669]\tLoss: 381.3571\n",
      "Training Epoch: 29 [12736/49669]\tLoss: 412.3116\n",
      "Training Epoch: 29 [12800/49669]\tLoss: 408.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [12864/49669]\tLoss: 424.1054\n",
      "Training Epoch: 29 [12928/49669]\tLoss: 401.2653\n",
      "Training Epoch: 29 [12992/49669]\tLoss: 382.0482\n",
      "Training Epoch: 29 [13056/49669]\tLoss: 393.6234\n",
      "Training Epoch: 29 [13120/49669]\tLoss: 390.2188\n",
      "Training Epoch: 29 [13184/49669]\tLoss: 385.5728\n",
      "Training Epoch: 29 [13248/49669]\tLoss: 369.5249\n",
      "Training Epoch: 29 [13312/49669]\tLoss: 437.6946\n",
      "Training Epoch: 29 [13376/49669]\tLoss: 387.5948\n",
      "Training Epoch: 29 [13440/49669]\tLoss: 422.3585\n",
      "Training Epoch: 29 [13504/49669]\tLoss: 373.2578\n",
      "Training Epoch: 29 [13568/49669]\tLoss: 423.4810\n",
      "Training Epoch: 29 [13632/49669]\tLoss: 393.1844\n",
      "Training Epoch: 29 [13696/49669]\tLoss: 401.5604\n",
      "Training Epoch: 29 [13760/49669]\tLoss: 392.3686\n",
      "Training Epoch: 29 [13824/49669]\tLoss: 378.1959\n",
      "Training Epoch: 29 [13888/49669]\tLoss: 409.3130\n",
      "Training Epoch: 29 [13952/49669]\tLoss: 422.0121\n",
      "Training Epoch: 29 [14016/49669]\tLoss: 375.6317\n",
      "Training Epoch: 29 [14080/49669]\tLoss: 388.7864\n",
      "Training Epoch: 29 [14144/49669]\tLoss: 398.1500\n",
      "Training Epoch: 29 [14208/49669]\tLoss: 376.5488\n",
      "Training Epoch: 29 [14272/49669]\tLoss: 391.2790\n",
      "Training Epoch: 29 [14336/49669]\tLoss: 438.8197\n",
      "Training Epoch: 29 [14400/49669]\tLoss: 408.4807\n",
      "Training Epoch: 29 [14464/49669]\tLoss: 380.9077\n",
      "Training Epoch: 29 [14528/49669]\tLoss: 389.6877\n",
      "Training Epoch: 29 [14592/49669]\tLoss: 391.9272\n",
      "Training Epoch: 29 [14656/49669]\tLoss: 393.0677\n",
      "Training Epoch: 29 [14720/49669]\tLoss: 368.6174\n",
      "Training Epoch: 29 [14784/49669]\tLoss: 405.4392\n",
      "Training Epoch: 29 [14848/49669]\tLoss: 388.2262\n",
      "Training Epoch: 29 [14912/49669]\tLoss: 405.7958\n",
      "Training Epoch: 29 [14976/49669]\tLoss: 384.7811\n",
      "Training Epoch: 29 [15040/49669]\tLoss: 423.4738\n",
      "Training Epoch: 29 [15104/49669]\tLoss: 398.8532\n",
      "Training Epoch: 29 [15168/49669]\tLoss: 394.3379\n",
      "Training Epoch: 29 [15232/49669]\tLoss: 400.9315\n",
      "Training Epoch: 29 [15296/49669]\tLoss: 381.1959\n",
      "Training Epoch: 29 [15360/49669]\tLoss: 406.1929\n",
      "Training Epoch: 29 [15424/49669]\tLoss: 418.4664\n",
      "Training Epoch: 29 [15488/49669]\tLoss: 380.8549\n",
      "Training Epoch: 29 [15552/49669]\tLoss: 372.9702\n",
      "Training Epoch: 29 [15616/49669]\tLoss: 378.7844\n",
      "Training Epoch: 29 [15680/49669]\tLoss: 421.6600\n",
      "Training Epoch: 29 [15744/49669]\tLoss: 415.7022\n",
      "Training Epoch: 29 [15808/49669]\tLoss: 401.4026\n",
      "Training Epoch: 29 [15872/49669]\tLoss: 441.1454\n",
      "Training Epoch: 29 [15936/49669]\tLoss: 399.7436\n",
      "Training Epoch: 29 [16000/49669]\tLoss: 424.4835\n",
      "Training Epoch: 29 [16064/49669]\tLoss: 404.8825\n",
      "Training Epoch: 29 [16128/49669]\tLoss: 431.8337\n",
      "Training Epoch: 29 [16192/49669]\tLoss: 425.7951\n",
      "Training Epoch: 29 [16256/49669]\tLoss: 433.1467\n",
      "Training Epoch: 29 [16320/49669]\tLoss: 435.2810\n",
      "Training Epoch: 29 [16384/49669]\tLoss: 446.3960\n",
      "Training Epoch: 29 [16448/49669]\tLoss: 449.4827\n",
      "Training Epoch: 29 [16512/49669]\tLoss: 420.1589\n",
      "Training Epoch: 29 [16576/49669]\tLoss: 425.9331\n",
      "Training Epoch: 29 [16640/49669]\tLoss: 412.2256\n",
      "Training Epoch: 29 [16704/49669]\tLoss: 406.8050\n",
      "Training Epoch: 29 [16768/49669]\tLoss: 419.9727\n",
      "Training Epoch: 29 [16832/49669]\tLoss: 404.1024\n",
      "Training Epoch: 29 [16896/49669]\tLoss: 426.3331\n",
      "Training Epoch: 29 [16960/49669]\tLoss: 398.0351\n",
      "Training Epoch: 29 [17024/49669]\tLoss: 438.7710\n",
      "Training Epoch: 29 [17088/49669]\tLoss: 385.6954\n",
      "Training Epoch: 29 [17152/49669]\tLoss: 432.9712\n",
      "Training Epoch: 29 [17216/49669]\tLoss: 405.9355\n",
      "Training Epoch: 29 [17280/49669]\tLoss: 429.7843\n",
      "Training Epoch: 29 [17344/49669]\tLoss: 425.2878\n",
      "Training Epoch: 29 [17408/49669]\tLoss: 443.3915\n",
      "Training Epoch: 29 [17472/49669]\tLoss: 390.7667\n",
      "Training Epoch: 29 [17536/49669]\tLoss: 398.3522\n",
      "Training Epoch: 29 [17600/49669]\tLoss: 409.3267\n",
      "Training Epoch: 29 [17664/49669]\tLoss: 414.8927\n",
      "Training Epoch: 29 [17728/49669]\tLoss: 404.6761\n",
      "Training Epoch: 29 [17792/49669]\tLoss: 413.5996\n",
      "Training Epoch: 29 [17856/49669]\tLoss: 392.5114\n",
      "Training Epoch: 29 [17920/49669]\tLoss: 376.4247\n",
      "Training Epoch: 29 [17984/49669]\tLoss: 428.5134\n",
      "Training Epoch: 29 [18048/49669]\tLoss: 433.4517\n",
      "Training Epoch: 29 [18112/49669]\tLoss: 411.3504\n",
      "Training Epoch: 29 [18176/49669]\tLoss: 409.8598\n",
      "Training Epoch: 29 [18240/49669]\tLoss: 383.9079\n",
      "Training Epoch: 29 [18304/49669]\tLoss: 379.3948\n",
      "Training Epoch: 29 [18368/49669]\tLoss: 395.2250\n",
      "Training Epoch: 29 [18432/49669]\tLoss: 420.4937\n",
      "Training Epoch: 29 [18496/49669]\tLoss: 401.1250\n",
      "Training Epoch: 29 [18560/49669]\tLoss: 414.4563\n",
      "Training Epoch: 29 [18624/49669]\tLoss: 412.2522\n",
      "Training Epoch: 29 [18688/49669]\tLoss: 383.3521\n",
      "Training Epoch: 29 [18752/49669]\tLoss: 393.9575\n",
      "Training Epoch: 29 [18816/49669]\tLoss: 360.5435\n",
      "Training Epoch: 29 [18880/49669]\tLoss: 371.3752\n",
      "Training Epoch: 29 [18944/49669]\tLoss: 387.4983\n",
      "Training Epoch: 29 [19008/49669]\tLoss: 399.0600\n",
      "Training Epoch: 29 [19072/49669]\tLoss: 419.5736\n",
      "Training Epoch: 29 [19136/49669]\tLoss: 376.0561\n",
      "Training Epoch: 29 [19200/49669]\tLoss: 393.7308\n",
      "Training Epoch: 29 [19264/49669]\tLoss: 370.2145\n",
      "Training Epoch: 29 [19328/49669]\tLoss: 391.6358\n",
      "Training Epoch: 29 [19392/49669]\tLoss: 406.6667\n",
      "Training Epoch: 29 [19456/49669]\tLoss: 403.1510\n",
      "Training Epoch: 29 [19520/49669]\tLoss: 395.9632\n",
      "Training Epoch: 29 [19584/49669]\tLoss: 398.8249\n",
      "Training Epoch: 29 [19648/49669]\tLoss: 413.5912\n",
      "Training Epoch: 29 [19712/49669]\tLoss: 424.2701\n",
      "Training Epoch: 29 [19776/49669]\tLoss: 415.4071\n",
      "Training Epoch: 29 [19840/49669]\tLoss: 437.7544\n",
      "Training Epoch: 29 [19904/49669]\tLoss: 431.2706\n",
      "Training Epoch: 29 [19968/49669]\tLoss: 382.8816\n",
      "Training Epoch: 29 [20032/49669]\tLoss: 398.5145\n",
      "Training Epoch: 29 [20096/49669]\tLoss: 372.9783\n",
      "Training Epoch: 29 [20160/49669]\tLoss: 414.0108\n",
      "Training Epoch: 29 [20224/49669]\tLoss: 418.0004\n",
      "Training Epoch: 29 [20288/49669]\tLoss: 390.5068\n",
      "Training Epoch: 29 [20352/49669]\tLoss: 402.5923\n",
      "Training Epoch: 29 [20416/49669]\tLoss: 404.1731\n",
      "Training Epoch: 29 [20480/49669]\tLoss: 397.4623\n",
      "Training Epoch: 29 [20544/49669]\tLoss: 403.6183\n",
      "Training Epoch: 29 [20608/49669]\tLoss: 403.4135\n",
      "Training Epoch: 29 [20672/49669]\tLoss: 381.9910\n",
      "Training Epoch: 29 [20736/49669]\tLoss: 400.8331\n",
      "Training Epoch: 29 [20800/49669]\tLoss: 413.5842\n",
      "Training Epoch: 29 [20864/49669]\tLoss: 409.2236\n",
      "Training Epoch: 29 [20928/49669]\tLoss: 374.7174\n",
      "Training Epoch: 29 [20992/49669]\tLoss: 393.6281\n",
      "Training Epoch: 29 [21056/49669]\tLoss: 374.5222\n",
      "Training Epoch: 29 [21120/49669]\tLoss: 391.7352\n",
      "Training Epoch: 29 [21184/49669]\tLoss: 403.9268\n",
      "Training Epoch: 29 [21248/49669]\tLoss: 381.0752\n",
      "Training Epoch: 29 [21312/49669]\tLoss: 380.9471\n",
      "Training Epoch: 29 [21376/49669]\tLoss: 404.4243\n",
      "Training Epoch: 29 [21440/49669]\tLoss: 415.4619\n",
      "Training Epoch: 29 [21504/49669]\tLoss: 411.7335\n",
      "Training Epoch: 29 [21568/49669]\tLoss: 414.3207\n",
      "Training Epoch: 29 [21632/49669]\tLoss: 385.2654\n",
      "Training Epoch: 29 [21696/49669]\tLoss: 400.4874\n",
      "Training Epoch: 29 [21760/49669]\tLoss: 421.1073\n",
      "Training Epoch: 29 [21824/49669]\tLoss: 421.5771\n",
      "Training Epoch: 29 [21888/49669]\tLoss: 382.8824\n",
      "Training Epoch: 29 [21952/49669]\tLoss: 402.4236\n",
      "Training Epoch: 29 [22016/49669]\tLoss: 399.3680\n",
      "Training Epoch: 29 [22080/49669]\tLoss: 399.8554\n",
      "Training Epoch: 29 [22144/49669]\tLoss: 408.5716\n",
      "Training Epoch: 29 [22208/49669]\tLoss: 431.1548\n",
      "Training Epoch: 29 [22272/49669]\tLoss: 404.7608\n",
      "Training Epoch: 29 [22336/49669]\tLoss: 393.4613\n",
      "Training Epoch: 29 [22400/49669]\tLoss: 396.9766\n",
      "Training Epoch: 29 [22464/49669]\tLoss: 398.6355\n",
      "Training Epoch: 29 [22528/49669]\tLoss: 407.7149\n",
      "Training Epoch: 29 [22592/49669]\tLoss: 448.0843\n",
      "Training Epoch: 29 [22656/49669]\tLoss: 418.2451\n",
      "Training Epoch: 29 [22720/49669]\tLoss: 387.8653\n",
      "Training Epoch: 29 [22784/49669]\tLoss: 399.8717\n",
      "Training Epoch: 29 [22848/49669]\tLoss: 420.4339\n",
      "Training Epoch: 29 [22912/49669]\tLoss: 399.3319\n",
      "Training Epoch: 29 [22976/49669]\tLoss: 425.8361\n",
      "Training Epoch: 29 [23040/49669]\tLoss: 414.6136\n",
      "Training Epoch: 29 [23104/49669]\tLoss: 387.4709\n",
      "Training Epoch: 29 [23168/49669]\tLoss: 393.6302\n",
      "Training Epoch: 29 [23232/49669]\tLoss: 409.0745\n",
      "Training Epoch: 29 [23296/49669]\tLoss: 396.6131\n",
      "Training Epoch: 29 [23360/49669]\tLoss: 405.7548\n",
      "Training Epoch: 29 [23424/49669]\tLoss: 414.7933\n",
      "Training Epoch: 29 [23488/49669]\tLoss: 427.2977\n",
      "Training Epoch: 29 [23552/49669]\tLoss: 416.8719\n",
      "Training Epoch: 29 [23616/49669]\tLoss: 394.9551\n",
      "Training Epoch: 29 [23680/49669]\tLoss: 359.7138\n",
      "Training Epoch: 29 [23744/49669]\tLoss: 426.7911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [23808/49669]\tLoss: 418.8740\n",
      "Training Epoch: 29 [23872/49669]\tLoss: 433.9265\n",
      "Training Epoch: 29 [23936/49669]\tLoss: 405.3350\n",
      "Training Epoch: 29 [24000/49669]\tLoss: 427.1480\n",
      "Training Epoch: 29 [24064/49669]\tLoss: 398.1562\n",
      "Training Epoch: 29 [24128/49669]\tLoss: 394.6758\n",
      "Training Epoch: 29 [24192/49669]\tLoss: 420.1566\n",
      "Training Epoch: 29 [24256/49669]\tLoss: 438.5907\n",
      "Training Epoch: 29 [24320/49669]\tLoss: 400.0602\n",
      "Training Epoch: 29 [24384/49669]\tLoss: 387.3707\n",
      "Training Epoch: 29 [24448/49669]\tLoss: 428.5514\n",
      "Training Epoch: 29 [24512/49669]\tLoss: 395.1480\n",
      "Training Epoch: 29 [24576/49669]\tLoss: 410.8250\n",
      "Training Epoch: 29 [24640/49669]\tLoss: 393.7882\n",
      "Training Epoch: 29 [24704/49669]\tLoss: 409.1642\n",
      "Training Epoch: 29 [24768/49669]\tLoss: 414.1287\n",
      "Training Epoch: 29 [24832/49669]\tLoss: 403.2664\n",
      "Training Epoch: 29 [24896/49669]\tLoss: 409.7204\n",
      "Training Epoch: 29 [24960/49669]\tLoss: 405.5413\n",
      "Training Epoch: 29 [25024/49669]\tLoss: 399.6560\n",
      "Training Epoch: 29 [25088/49669]\tLoss: 351.8609\n",
      "Training Epoch: 29 [25152/49669]\tLoss: 386.3817\n",
      "Training Epoch: 29 [25216/49669]\tLoss: 392.5099\n",
      "Training Epoch: 29 [25280/49669]\tLoss: 423.0744\n",
      "Training Epoch: 29 [25344/49669]\tLoss: 412.3027\n",
      "Training Epoch: 29 [25408/49669]\tLoss: 399.5340\n",
      "Training Epoch: 29 [25472/49669]\tLoss: 430.5233\n",
      "Training Epoch: 29 [25536/49669]\tLoss: 388.1404\n",
      "Training Epoch: 29 [25600/49669]\tLoss: 403.2068\n",
      "Training Epoch: 29 [25664/49669]\tLoss: 423.0087\n",
      "Training Epoch: 29 [25728/49669]\tLoss: 403.0988\n",
      "Training Epoch: 29 [25792/49669]\tLoss: 422.6849\n",
      "Training Epoch: 29 [25856/49669]\tLoss: 403.8828\n",
      "Training Epoch: 29 [25920/49669]\tLoss: 405.1981\n",
      "Training Epoch: 29 [25984/49669]\tLoss: 393.9184\n",
      "Training Epoch: 29 [26048/49669]\tLoss: 393.7865\n",
      "Training Epoch: 29 [26112/49669]\tLoss: 414.4122\n",
      "Training Epoch: 29 [26176/49669]\tLoss: 412.5039\n",
      "Training Epoch: 29 [26240/49669]\tLoss: 389.6187\n",
      "Training Epoch: 29 [26304/49669]\tLoss: 405.8653\n",
      "Training Epoch: 29 [26368/49669]\tLoss: 412.0376\n",
      "Training Epoch: 29 [26432/49669]\tLoss: 393.5788\n",
      "Training Epoch: 29 [26496/49669]\tLoss: 419.5855\n",
      "Training Epoch: 29 [26560/49669]\tLoss: 401.9129\n",
      "Training Epoch: 29 [26624/49669]\tLoss: 400.6835\n",
      "Training Epoch: 29 [26688/49669]\tLoss: 415.7385\n",
      "Training Epoch: 29 [26752/49669]\tLoss: 392.5117\n",
      "Training Epoch: 29 [26816/49669]\tLoss: 387.7660\n",
      "Training Epoch: 29 [26880/49669]\tLoss: 400.7400\n",
      "Training Epoch: 29 [26944/49669]\tLoss: 404.7562\n",
      "Training Epoch: 29 [27008/49669]\tLoss: 396.2839\n",
      "Training Epoch: 29 [27072/49669]\tLoss: 389.7698\n",
      "Training Epoch: 29 [27136/49669]\tLoss: 384.0431\n",
      "Training Epoch: 29 [27200/49669]\tLoss: 355.4192\n",
      "Training Epoch: 29 [27264/49669]\tLoss: 399.9721\n",
      "Training Epoch: 29 [27328/49669]\tLoss: 400.2783\n",
      "Training Epoch: 29 [27392/49669]\tLoss: 398.0653\n",
      "Training Epoch: 29 [27456/49669]\tLoss: 397.5141\n",
      "Training Epoch: 29 [27520/49669]\tLoss: 387.0946\n",
      "Training Epoch: 29 [27584/49669]\tLoss: 416.7837\n",
      "Training Epoch: 29 [27648/49669]\tLoss: 401.5774\n",
      "Training Epoch: 29 [27712/49669]\tLoss: 378.9260\n",
      "Training Epoch: 29 [27776/49669]\tLoss: 398.8750\n",
      "Training Epoch: 29 [27840/49669]\tLoss: 407.5376\n",
      "Training Epoch: 29 [27904/49669]\tLoss: 415.4677\n",
      "Training Epoch: 29 [27968/49669]\tLoss: 420.7540\n",
      "Training Epoch: 29 [28032/49669]\tLoss: 391.7321\n",
      "Training Epoch: 29 [28096/49669]\tLoss: 410.9838\n",
      "Training Epoch: 29 [28160/49669]\tLoss: 367.3125\n",
      "Training Epoch: 29 [28224/49669]\tLoss: 393.5555\n",
      "Training Epoch: 29 [28288/49669]\tLoss: 411.0518\n",
      "Training Epoch: 29 [28352/49669]\tLoss: 431.5640\n",
      "Training Epoch: 29 [28416/49669]\tLoss: 398.8168\n",
      "Training Epoch: 29 [28480/49669]\tLoss: 408.0815\n",
      "Training Epoch: 29 [28544/49669]\tLoss: 388.1530\n",
      "Training Epoch: 29 [28608/49669]\tLoss: 398.4883\n",
      "Training Epoch: 29 [28672/49669]\tLoss: 391.2509\n",
      "Training Epoch: 29 [28736/49669]\tLoss: 412.5531\n",
      "Training Epoch: 29 [28800/49669]\tLoss: 404.8959\n",
      "Training Epoch: 29 [28864/49669]\tLoss: 406.6393\n",
      "Training Epoch: 29 [28928/49669]\tLoss: 413.1082\n",
      "Training Epoch: 29 [28992/49669]\tLoss: 397.1884\n",
      "Training Epoch: 29 [29056/49669]\tLoss: 388.7141\n",
      "Training Epoch: 29 [29120/49669]\tLoss: 413.6953\n",
      "Training Epoch: 29 [29184/49669]\tLoss: 405.0330\n",
      "Training Epoch: 29 [29248/49669]\tLoss: 413.9444\n",
      "Training Epoch: 29 [29312/49669]\tLoss: 398.4310\n",
      "Training Epoch: 29 [29376/49669]\tLoss: 413.7563\n",
      "Training Epoch: 29 [29440/49669]\tLoss: 359.0750\n",
      "Training Epoch: 29 [29504/49669]\tLoss: 385.9024\n",
      "Training Epoch: 29 [29568/49669]\tLoss: 375.9020\n",
      "Training Epoch: 29 [29632/49669]\tLoss: 389.6586\n",
      "Training Epoch: 29 [29696/49669]\tLoss: 393.9235\n",
      "Training Epoch: 29 [29760/49669]\tLoss: 398.9870\n",
      "Training Epoch: 29 [29824/49669]\tLoss: 415.0659\n",
      "Training Epoch: 29 [29888/49669]\tLoss: 392.3892\n",
      "Training Epoch: 29 [29952/49669]\tLoss: 415.4450\n",
      "Training Epoch: 29 [30016/49669]\tLoss: 418.1880\n",
      "Training Epoch: 29 [30080/49669]\tLoss: 415.4255\n",
      "Training Epoch: 29 [30144/49669]\tLoss: 423.7697\n",
      "Training Epoch: 29 [30208/49669]\tLoss: 384.6137\n",
      "Training Epoch: 29 [30272/49669]\tLoss: 405.8617\n",
      "Training Epoch: 29 [30336/49669]\tLoss: 425.9723\n",
      "Training Epoch: 29 [30400/49669]\tLoss: 405.3831\n",
      "Training Epoch: 29 [30464/49669]\tLoss: 404.1049\n",
      "Training Epoch: 29 [30528/49669]\tLoss: 424.2161\n",
      "Training Epoch: 29 [30592/49669]\tLoss: 415.4030\n",
      "Training Epoch: 29 [30656/49669]\tLoss: 401.4507\n",
      "Training Epoch: 29 [30720/49669]\tLoss: 429.4446\n",
      "Training Epoch: 29 [30784/49669]\tLoss: 392.7601\n",
      "Training Epoch: 29 [30848/49669]\tLoss: 380.9673\n",
      "Training Epoch: 29 [30912/49669]\tLoss: 388.6104\n",
      "Training Epoch: 29 [30976/49669]\tLoss: 434.2926\n",
      "Training Epoch: 29 [31040/49669]\tLoss: 414.6212\n",
      "Training Epoch: 29 [31104/49669]\tLoss: 394.0902\n",
      "Training Epoch: 29 [31168/49669]\tLoss: 433.0652\n",
      "Training Epoch: 29 [31232/49669]\tLoss: 354.8245\n",
      "Training Epoch: 29 [31296/49669]\tLoss: 404.5263\n",
      "Training Epoch: 29 [31360/49669]\tLoss: 397.9266\n",
      "Training Epoch: 29 [31424/49669]\tLoss: 397.7079\n",
      "Training Epoch: 29 [31488/49669]\tLoss: 375.9306\n",
      "Training Epoch: 29 [31552/49669]\tLoss: 410.9926\n",
      "Training Epoch: 29 [31616/49669]\tLoss: 370.6859\n",
      "Training Epoch: 29 [31680/49669]\tLoss: 412.2453\n",
      "Training Epoch: 29 [31744/49669]\tLoss: 369.5413\n",
      "Training Epoch: 29 [31808/49669]\tLoss: 390.4194\n",
      "Training Epoch: 29 [31872/49669]\tLoss: 401.1435\n",
      "Training Epoch: 29 [31936/49669]\tLoss: 385.0268\n",
      "Training Epoch: 29 [32000/49669]\tLoss: 399.3828\n",
      "Training Epoch: 29 [32064/49669]\tLoss: 413.7322\n",
      "Training Epoch: 29 [32128/49669]\tLoss: 448.2451\n",
      "Training Epoch: 29 [32192/49669]\tLoss: 415.7951\n",
      "Training Epoch: 29 [32256/49669]\tLoss: 415.4866\n",
      "Training Epoch: 29 [32320/49669]\tLoss: 410.7495\n",
      "Training Epoch: 29 [32384/49669]\tLoss: 416.5695\n",
      "Training Epoch: 29 [32448/49669]\tLoss: 434.4124\n",
      "Training Epoch: 29 [32512/49669]\tLoss: 407.4213\n",
      "Training Epoch: 29 [32576/49669]\tLoss: 410.5503\n",
      "Training Epoch: 29 [32640/49669]\tLoss: 389.4144\n",
      "Training Epoch: 29 [32704/49669]\tLoss: 403.6365\n",
      "Training Epoch: 29 [32768/49669]\tLoss: 419.5674\n",
      "Training Epoch: 29 [32832/49669]\tLoss: 387.8797\n",
      "Training Epoch: 29 [32896/49669]\tLoss: 414.1775\n",
      "Training Epoch: 29 [32960/49669]\tLoss: 434.8445\n",
      "Training Epoch: 29 [33024/49669]\tLoss: 413.2020\n",
      "Training Epoch: 29 [33088/49669]\tLoss: 419.0117\n",
      "Training Epoch: 29 [33152/49669]\tLoss: 423.3219\n",
      "Training Epoch: 29 [33216/49669]\tLoss: 400.3027\n",
      "Training Epoch: 29 [33280/49669]\tLoss: 413.6092\n",
      "Training Epoch: 29 [33344/49669]\tLoss: 386.3766\n",
      "Training Epoch: 29 [33408/49669]\tLoss: 394.3460\n",
      "Training Epoch: 29 [33472/49669]\tLoss: 389.9078\n",
      "Training Epoch: 29 [33536/49669]\tLoss: 426.9630\n",
      "Training Epoch: 29 [33600/49669]\tLoss: 406.9858\n",
      "Training Epoch: 29 [33664/49669]\tLoss: 397.0364\n",
      "Training Epoch: 29 [33728/49669]\tLoss: 422.8149\n",
      "Training Epoch: 29 [33792/49669]\tLoss: 410.6414\n",
      "Training Epoch: 29 [33856/49669]\tLoss: 402.2203\n",
      "Training Epoch: 29 [33920/49669]\tLoss: 410.0677\n",
      "Training Epoch: 29 [33984/49669]\tLoss: 392.0537\n",
      "Training Epoch: 29 [34048/49669]\tLoss: 420.0453\n",
      "Training Epoch: 29 [34112/49669]\tLoss: 407.9673\n",
      "Training Epoch: 29 [34176/49669]\tLoss: 392.3606\n",
      "Training Epoch: 29 [34240/49669]\tLoss: 388.6580\n",
      "Training Epoch: 29 [34304/49669]\tLoss: 413.4871\n",
      "Training Epoch: 29 [34368/49669]\tLoss: 454.7728\n",
      "Training Epoch: 29 [34432/49669]\tLoss: 387.6726\n",
      "Training Epoch: 29 [34496/49669]\tLoss: 410.9945\n",
      "Training Epoch: 29 [34560/49669]\tLoss: 423.5477\n",
      "Training Epoch: 29 [34624/49669]\tLoss: 380.7596\n",
      "Training Epoch: 29 [34688/49669]\tLoss: 373.6664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [34752/49669]\tLoss: 367.8736\n",
      "Training Epoch: 29 [34816/49669]\tLoss: 405.8194\n",
      "Training Epoch: 29 [34880/49669]\tLoss: 388.0798\n",
      "Training Epoch: 29 [34944/49669]\tLoss: 395.4813\n",
      "Training Epoch: 29 [35008/49669]\tLoss: 402.3485\n",
      "Training Epoch: 29 [35072/49669]\tLoss: 386.9647\n",
      "Training Epoch: 29 [35136/49669]\tLoss: 403.0438\n",
      "Training Epoch: 29 [35200/49669]\tLoss: 433.6698\n",
      "Training Epoch: 29 [35264/49669]\tLoss: 371.7657\n",
      "Training Epoch: 29 [35328/49669]\tLoss: 386.4140\n",
      "Training Epoch: 29 [35392/49669]\tLoss: 424.3576\n",
      "Training Epoch: 29 [35456/49669]\tLoss: 385.5638\n",
      "Training Epoch: 29 [35520/49669]\tLoss: 402.0300\n",
      "Training Epoch: 29 [35584/49669]\tLoss: 396.8468\n",
      "Training Epoch: 29 [35648/49669]\tLoss: 423.9716\n",
      "Training Epoch: 29 [35712/49669]\tLoss: 403.9807\n",
      "Training Epoch: 29 [35776/49669]\tLoss: 385.9080\n",
      "Training Epoch: 29 [35840/49669]\tLoss: 403.0236\n",
      "Training Epoch: 29 [35904/49669]\tLoss: 404.4012\n",
      "Training Epoch: 29 [35968/49669]\tLoss: 408.4607\n",
      "Training Epoch: 29 [36032/49669]\tLoss: 390.0434\n",
      "Training Epoch: 29 [36096/49669]\tLoss: 416.6900\n",
      "Training Epoch: 29 [36160/49669]\tLoss: 422.0053\n",
      "Training Epoch: 29 [36224/49669]\tLoss: 411.3123\n",
      "Training Epoch: 29 [36288/49669]\tLoss: 438.7474\n",
      "Training Epoch: 29 [36352/49669]\tLoss: 374.4315\n",
      "Training Epoch: 29 [36416/49669]\tLoss: 410.8564\n",
      "Training Epoch: 29 [36480/49669]\tLoss: 391.7942\n",
      "Training Epoch: 29 [36544/49669]\tLoss: 418.2363\n",
      "Training Epoch: 29 [36608/49669]\tLoss: 420.4453\n",
      "Training Epoch: 29 [36672/49669]\tLoss: 351.2262\n",
      "Training Epoch: 29 [36736/49669]\tLoss: 389.3158\n",
      "Training Epoch: 29 [36800/49669]\tLoss: 380.9130\n",
      "Training Epoch: 29 [36864/49669]\tLoss: 415.5237\n",
      "Training Epoch: 29 [36928/49669]\tLoss: 386.7266\n",
      "Training Epoch: 29 [36992/49669]\tLoss: 394.5593\n",
      "Training Epoch: 29 [37056/49669]\tLoss: 416.1065\n",
      "Training Epoch: 29 [37120/49669]\tLoss: 398.4151\n",
      "Training Epoch: 29 [37184/49669]\tLoss: 387.7962\n",
      "Training Epoch: 29 [37248/49669]\tLoss: 427.9537\n",
      "Training Epoch: 29 [37312/49669]\tLoss: 414.4287\n",
      "Training Epoch: 29 [37376/49669]\tLoss: 444.0245\n",
      "Training Epoch: 29 [37440/49669]\tLoss: 407.5682\n",
      "Training Epoch: 29 [37504/49669]\tLoss: 404.9976\n",
      "Training Epoch: 29 [37568/49669]\tLoss: 403.9591\n",
      "Training Epoch: 29 [37632/49669]\tLoss: 415.2926\n",
      "Training Epoch: 29 [37696/49669]\tLoss: 388.1249\n",
      "Training Epoch: 29 [37760/49669]\tLoss: 416.6675\n",
      "Training Epoch: 29 [37824/49669]\tLoss: 362.2310\n",
      "Training Epoch: 29 [37888/49669]\tLoss: 405.7484\n",
      "Training Epoch: 29 [37952/49669]\tLoss: 372.8616\n",
      "Training Epoch: 29 [38016/49669]\tLoss: 420.6154\n",
      "Training Epoch: 29 [38080/49669]\tLoss: 425.2614\n",
      "Training Epoch: 29 [38144/49669]\tLoss: 408.8732\n",
      "Training Epoch: 29 [38208/49669]\tLoss: 401.5965\n",
      "Training Epoch: 29 [38272/49669]\tLoss: 365.0795\n",
      "Training Epoch: 29 [38336/49669]\tLoss: 402.3829\n",
      "Training Epoch: 29 [38400/49669]\tLoss: 406.0367\n",
      "Training Epoch: 29 [38464/49669]\tLoss: 406.3105\n",
      "Training Epoch: 29 [38528/49669]\tLoss: 422.1492\n",
      "Training Epoch: 29 [38592/49669]\tLoss: 412.4925\n",
      "Training Epoch: 29 [38656/49669]\tLoss: 397.7241\n",
      "Training Epoch: 29 [38720/49669]\tLoss: 410.0810\n",
      "Training Epoch: 29 [38784/49669]\tLoss: 427.6078\n",
      "Training Epoch: 29 [38848/49669]\tLoss: 432.2666\n",
      "Training Epoch: 29 [38912/49669]\tLoss: 422.7288\n",
      "Training Epoch: 29 [38976/49669]\tLoss: 409.5005\n",
      "Training Epoch: 29 [39040/49669]\tLoss: 420.5907\n",
      "Training Epoch: 29 [39104/49669]\tLoss: 426.8131\n",
      "Training Epoch: 29 [39168/49669]\tLoss: 435.9094\n",
      "Training Epoch: 29 [39232/49669]\tLoss: 426.0428\n",
      "Training Epoch: 29 [39296/49669]\tLoss: 393.2752\n",
      "Training Epoch: 29 [39360/49669]\tLoss: 404.8904\n",
      "Training Epoch: 29 [39424/49669]\tLoss: 388.0609\n",
      "Training Epoch: 29 [39488/49669]\tLoss: 384.3349\n",
      "Training Epoch: 29 [39552/49669]\tLoss: 383.6607\n",
      "Training Epoch: 29 [39616/49669]\tLoss: 437.3612\n",
      "Training Epoch: 29 [39680/49669]\tLoss: 414.7272\n",
      "Training Epoch: 29 [39744/49669]\tLoss: 443.8656\n",
      "Training Epoch: 29 [39808/49669]\tLoss: 419.0302\n",
      "Training Epoch: 29 [39872/49669]\tLoss: 453.6271\n",
      "Training Epoch: 29 [39936/49669]\tLoss: 423.4679\n",
      "Training Epoch: 29 [40000/49669]\tLoss: 406.6157\n",
      "Training Epoch: 29 [40064/49669]\tLoss: 388.1938\n",
      "Training Epoch: 29 [40128/49669]\tLoss: 397.9765\n",
      "Training Epoch: 29 [40192/49669]\tLoss: 410.8019\n",
      "Training Epoch: 29 [40256/49669]\tLoss: 403.9381\n",
      "Training Epoch: 29 [40320/49669]\tLoss: 438.0212\n",
      "Training Epoch: 29 [40384/49669]\tLoss: 414.9144\n",
      "Training Epoch: 29 [40448/49669]\tLoss: 397.4814\n",
      "Training Epoch: 29 [40512/49669]\tLoss: 416.6048\n",
      "Training Epoch: 29 [40576/49669]\tLoss: 411.8499\n",
      "Training Epoch: 29 [40640/49669]\tLoss: 427.4083\n",
      "Training Epoch: 29 [40704/49669]\tLoss: 435.4427\n",
      "Training Epoch: 29 [40768/49669]\tLoss: 416.8631\n",
      "Training Epoch: 29 [40832/49669]\tLoss: 403.5568\n",
      "Training Epoch: 29 [40896/49669]\tLoss: 384.6107\n",
      "Training Epoch: 29 [40960/49669]\tLoss: 404.0236\n",
      "Training Epoch: 29 [41024/49669]\tLoss: 409.7239\n",
      "Training Epoch: 29 [41088/49669]\tLoss: 428.4001\n",
      "Training Epoch: 29 [41152/49669]\tLoss: 404.1481\n",
      "Training Epoch: 29 [41216/49669]\tLoss: 386.9250\n",
      "Training Epoch: 29 [41280/49669]\tLoss: 377.7936\n",
      "Training Epoch: 29 [41344/49669]\tLoss: 388.1201\n",
      "Training Epoch: 29 [41408/49669]\tLoss: 404.4862\n",
      "Training Epoch: 29 [41472/49669]\tLoss: 426.4302\n",
      "Training Epoch: 29 [41536/49669]\tLoss: 402.4652\n",
      "Training Epoch: 29 [41600/49669]\tLoss: 397.3884\n",
      "Training Epoch: 29 [41664/49669]\tLoss: 415.0382\n",
      "Training Epoch: 29 [41728/49669]\tLoss: 419.0082\n",
      "Training Epoch: 29 [41792/49669]\tLoss: 414.1339\n",
      "Training Epoch: 29 [41856/49669]\tLoss: 409.7070\n",
      "Training Epoch: 29 [41920/49669]\tLoss: 396.6926\n",
      "Training Epoch: 29 [41984/49669]\tLoss: 400.8270\n",
      "Training Epoch: 29 [42048/49669]\tLoss: 408.1166\n",
      "Training Epoch: 29 [42112/49669]\tLoss: 373.7347\n",
      "Training Epoch: 29 [42176/49669]\tLoss: 395.6095\n",
      "Training Epoch: 29 [42240/49669]\tLoss: 413.8876\n",
      "Training Epoch: 29 [42304/49669]\tLoss: 398.7288\n",
      "Training Epoch: 29 [42368/49669]\tLoss: 407.3025\n",
      "Training Epoch: 29 [42432/49669]\tLoss: 419.5062\n",
      "Training Epoch: 29 [42496/49669]\tLoss: 376.9534\n",
      "Training Epoch: 29 [42560/49669]\tLoss: 429.6237\n",
      "Training Epoch: 29 [42624/49669]\tLoss: 415.7184\n",
      "Training Epoch: 29 [42688/49669]\tLoss: 401.7275\n",
      "Training Epoch: 29 [42752/49669]\tLoss: 383.9532\n",
      "Training Epoch: 29 [42816/49669]\tLoss: 370.0848\n",
      "Training Epoch: 29 [42880/49669]\tLoss: 405.1733\n",
      "Training Epoch: 29 [42944/49669]\tLoss: 429.8212\n",
      "Training Epoch: 29 [43008/49669]\tLoss: 407.0741\n",
      "Training Epoch: 29 [43072/49669]\tLoss: 432.6119\n",
      "Training Epoch: 29 [43136/49669]\tLoss: 400.7652\n",
      "Training Epoch: 29 [43200/49669]\tLoss: 404.7406\n",
      "Training Epoch: 29 [43264/49669]\tLoss: 412.1645\n",
      "Training Epoch: 29 [43328/49669]\tLoss: 392.2254\n",
      "Training Epoch: 29 [43392/49669]\tLoss: 375.7014\n",
      "Training Epoch: 29 [43456/49669]\tLoss: 385.5753\n",
      "Training Epoch: 29 [43520/49669]\tLoss: 444.6793\n",
      "Training Epoch: 29 [43584/49669]\tLoss: 400.3789\n",
      "Training Epoch: 29 [43648/49669]\tLoss: 422.5582\n",
      "Training Epoch: 29 [43712/49669]\tLoss: 393.0326\n",
      "Training Epoch: 29 [43776/49669]\tLoss: 405.7285\n",
      "Training Epoch: 29 [43840/49669]\tLoss: 387.6451\n",
      "Training Epoch: 29 [43904/49669]\tLoss: 417.0094\n",
      "Training Epoch: 29 [43968/49669]\tLoss: 390.2275\n",
      "Training Epoch: 29 [44032/49669]\tLoss: 423.1472\n",
      "Training Epoch: 29 [44096/49669]\tLoss: 430.5353\n",
      "Training Epoch: 29 [44160/49669]\tLoss: 401.1062\n",
      "Training Epoch: 29 [44224/49669]\tLoss: 400.1708\n",
      "Training Epoch: 29 [44288/49669]\tLoss: 407.2302\n",
      "Training Epoch: 29 [44352/49669]\tLoss: 399.7628\n",
      "Training Epoch: 29 [44416/49669]\tLoss: 381.8431\n",
      "Training Epoch: 29 [44480/49669]\tLoss: 388.1021\n",
      "Training Epoch: 29 [44544/49669]\tLoss: 410.8842\n",
      "Training Epoch: 29 [44608/49669]\tLoss: 420.2722\n",
      "Training Epoch: 29 [44672/49669]\tLoss: 424.7768\n",
      "Training Epoch: 29 [44736/49669]\tLoss: 391.0444\n",
      "Training Epoch: 29 [44800/49669]\tLoss: 408.4296\n",
      "Training Epoch: 29 [44864/49669]\tLoss: 439.3310\n",
      "Training Epoch: 29 [44928/49669]\tLoss: 373.3859\n",
      "Training Epoch: 29 [44992/49669]\tLoss: 413.9740\n",
      "Training Epoch: 29 [45056/49669]\tLoss: 412.9347\n",
      "Training Epoch: 29 [45120/49669]\tLoss: 411.5340\n",
      "Training Epoch: 29 [45184/49669]\tLoss: 368.6317\n",
      "Training Epoch: 29 [45248/49669]\tLoss: 426.8110\n",
      "Training Epoch: 29 [45312/49669]\tLoss: 377.7459\n",
      "Training Epoch: 29 [45376/49669]\tLoss: 406.8829\n",
      "Training Epoch: 29 [45440/49669]\tLoss: 406.0764\n",
      "Training Epoch: 29 [45504/49669]\tLoss: 403.0970\n",
      "Training Epoch: 29 [45568/49669]\tLoss: 369.6815\n",
      "Training Epoch: 29 [45632/49669]\tLoss: 379.9854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [45696/49669]\tLoss: 371.9380\n",
      "Training Epoch: 29 [45760/49669]\tLoss: 370.3229\n",
      "Training Epoch: 29 [45824/49669]\tLoss: 389.8614\n",
      "Training Epoch: 29 [45888/49669]\tLoss: 432.6526\n",
      "Training Epoch: 29 [45952/49669]\tLoss: 412.4586\n",
      "Training Epoch: 29 [46016/49669]\tLoss: 409.6139\n",
      "Training Epoch: 29 [46080/49669]\tLoss: 398.4478\n",
      "Training Epoch: 29 [46144/49669]\tLoss: 432.5108\n",
      "Training Epoch: 29 [46208/49669]\tLoss: 428.6329\n",
      "Training Epoch: 29 [46272/49669]\tLoss: 404.4388\n",
      "Training Epoch: 29 [46336/49669]\tLoss: 405.8222\n",
      "Training Epoch: 29 [46400/49669]\tLoss: 374.7645\n",
      "Training Epoch: 29 [46464/49669]\tLoss: 406.7453\n",
      "Training Epoch: 29 [46528/49669]\tLoss: 415.5286\n",
      "Training Epoch: 29 [46592/49669]\tLoss: 391.2115\n",
      "Training Epoch: 29 [46656/49669]\tLoss: 395.5297\n",
      "Training Epoch: 29 [46720/49669]\tLoss: 401.9819\n",
      "Training Epoch: 29 [46784/49669]\tLoss: 398.1496\n",
      "Training Epoch: 29 [46848/49669]\tLoss: 394.5837\n",
      "Training Epoch: 29 [46912/49669]\tLoss: 366.5949\n",
      "Training Epoch: 29 [46976/49669]\tLoss: 398.5322\n",
      "Training Epoch: 29 [47040/49669]\tLoss: 388.6285\n",
      "Training Epoch: 29 [47104/49669]\tLoss: 430.3418\n",
      "Training Epoch: 29 [47168/49669]\tLoss: 386.9385\n",
      "Training Epoch: 29 [47232/49669]\tLoss: 374.8878\n",
      "Training Epoch: 29 [47296/49669]\tLoss: 419.8440\n",
      "Training Epoch: 29 [47360/49669]\tLoss: 370.3605\n",
      "Training Epoch: 29 [47424/49669]\tLoss: 396.1201\n",
      "Training Epoch: 29 [47488/49669]\tLoss: 398.6196\n",
      "Training Epoch: 29 [47552/49669]\tLoss: 421.9185\n",
      "Training Epoch: 29 [47616/49669]\tLoss: 406.7627\n",
      "Training Epoch: 29 [47680/49669]\tLoss: 407.2814\n",
      "Training Epoch: 29 [47744/49669]\tLoss: 403.7645\n",
      "Training Epoch: 29 [47808/49669]\tLoss: 417.2320\n",
      "Training Epoch: 29 [47872/49669]\tLoss: 397.6045\n",
      "Training Epoch: 29 [47936/49669]\tLoss: 395.5798\n",
      "Training Epoch: 29 [48000/49669]\tLoss: 394.5990\n",
      "Training Epoch: 29 [48064/49669]\tLoss: 420.2125\n",
      "Training Epoch: 29 [48128/49669]\tLoss: 431.5935\n",
      "Training Epoch: 29 [48192/49669]\tLoss: 380.9174\n",
      "Training Epoch: 29 [48256/49669]\tLoss: 415.0159\n",
      "Training Epoch: 29 [48320/49669]\tLoss: 374.5974\n",
      "Training Epoch: 29 [48384/49669]\tLoss: 407.8724\n",
      "Training Epoch: 29 [48448/49669]\tLoss: 403.4545\n",
      "Training Epoch: 29 [48512/49669]\tLoss: 383.1375\n",
      "Training Epoch: 29 [48576/49669]\tLoss: 392.3814\n",
      "Training Epoch: 29 [48640/49669]\tLoss: 396.3600\n",
      "Training Epoch: 29 [48704/49669]\tLoss: 450.3847\n",
      "Training Epoch: 29 [48768/49669]\tLoss: 364.9893\n",
      "Training Epoch: 29 [48832/49669]\tLoss: 405.9644\n",
      "Training Epoch: 29 [48896/49669]\tLoss: 433.2910\n",
      "Training Epoch: 29 [48960/49669]\tLoss: 402.4086\n",
      "Training Epoch: 29 [49024/49669]\tLoss: 397.1806\n",
      "Training Epoch: 29 [49088/49669]\tLoss: 393.8884\n",
      "Training Epoch: 29 [49152/49669]\tLoss: 368.5328\n",
      "Training Epoch: 29 [49216/49669]\tLoss: 399.6042\n",
      "Training Epoch: 29 [49280/49669]\tLoss: 395.1308\n",
      "Training Epoch: 29 [49344/49669]\tLoss: 407.4745\n",
      "Training Epoch: 29 [49408/49669]\tLoss: 364.0175\n",
      "Training Epoch: 29 [49472/49669]\tLoss: 417.2797\n",
      "Training Epoch: 29 [49536/49669]\tLoss: 395.1501\n",
      "Training Epoch: 29 [49600/49669]\tLoss: 402.8058\n",
      "Training Epoch: 29 [49664/49669]\tLoss: 407.4178\n",
      "Training Epoch: 29 [49669/49669]\tLoss: 444.7498\n",
      "Training Epoch: 29 [5519/5519]\tLoss: 403.1350\n",
      "Training Epoch: 30 [64/49669]\tLoss: 411.9485\n",
      "Training Epoch: 30 [128/49669]\tLoss: 386.3779\n",
      "Training Epoch: 30 [192/49669]\tLoss: 400.6037\n",
      "Training Epoch: 30 [256/49669]\tLoss: 381.3399\n",
      "Training Epoch: 30 [320/49669]\tLoss: 403.1215\n",
      "Training Epoch: 30 [384/49669]\tLoss: 387.1415\n",
      "Training Epoch: 30 [448/49669]\tLoss: 390.4724\n",
      "Training Epoch: 30 [512/49669]\tLoss: 391.8692\n",
      "Training Epoch: 30 [576/49669]\tLoss: 404.1046\n",
      "Training Epoch: 30 [640/49669]\tLoss: 393.1645\n",
      "Training Epoch: 30 [704/49669]\tLoss: 408.8570\n",
      "Training Epoch: 30 [768/49669]\tLoss: 390.3894\n",
      "Training Epoch: 30 [832/49669]\tLoss: 341.3193\n",
      "Training Epoch: 30 [896/49669]\tLoss: 377.0520\n",
      "Training Epoch: 30 [960/49669]\tLoss: 375.6351\n",
      "Training Epoch: 30 [1024/49669]\tLoss: 411.8074\n",
      "Training Epoch: 30 [1088/49669]\tLoss: 402.4941\n",
      "Training Epoch: 30 [1152/49669]\tLoss: 436.3751\n",
      "Training Epoch: 30 [1216/49669]\tLoss: 390.6768\n",
      "Training Epoch: 30 [1280/49669]\tLoss: 426.0327\n",
      "Training Epoch: 30 [1344/49669]\tLoss: 399.4500\n",
      "Training Epoch: 30 [1408/49669]\tLoss: 388.9540\n",
      "Training Epoch: 30 [1472/49669]\tLoss: 417.2577\n",
      "Training Epoch: 30 [1536/49669]\tLoss: 433.9321\n",
      "Training Epoch: 30 [1600/49669]\tLoss: 401.1321\n",
      "Training Epoch: 30 [1664/49669]\tLoss: 382.3848\n",
      "Training Epoch: 30 [1728/49669]\tLoss: 422.1334\n",
      "Training Epoch: 30 [1792/49669]\tLoss: 412.5461\n",
      "Training Epoch: 30 [1856/49669]\tLoss: 387.1283\n",
      "Training Epoch: 30 [1920/49669]\tLoss: 397.3309\n",
      "Training Epoch: 30 [1984/49669]\tLoss: 393.1171\n",
      "Training Epoch: 30 [2048/49669]\tLoss: 399.8809\n",
      "Training Epoch: 30 [2112/49669]\tLoss: 387.2704\n",
      "Training Epoch: 30 [2176/49669]\tLoss: 398.5017\n",
      "Training Epoch: 30 [2240/49669]\tLoss: 393.7007\n",
      "Training Epoch: 30 [2304/49669]\tLoss: 406.6919\n",
      "Training Epoch: 30 [2368/49669]\tLoss: 363.4046\n",
      "Training Epoch: 30 [2432/49669]\tLoss: 372.0921\n",
      "Training Epoch: 30 [2496/49669]\tLoss: 408.0136\n",
      "Training Epoch: 30 [2560/49669]\tLoss: 405.3261\n",
      "Training Epoch: 30 [2624/49669]\tLoss: 398.5625\n",
      "Training Epoch: 30 [2688/49669]\tLoss: 410.7914\n",
      "Training Epoch: 30 [2752/49669]\tLoss: 392.1313\n",
      "Training Epoch: 30 [2816/49669]\tLoss: 424.4265\n",
      "Training Epoch: 30 [2880/49669]\tLoss: 395.1112\n",
      "Training Epoch: 30 [2944/49669]\tLoss: 402.4778\n",
      "Training Epoch: 30 [3008/49669]\tLoss: 374.5375\n",
      "Training Epoch: 30 [3072/49669]\tLoss: 397.3615\n",
      "Training Epoch: 30 [3136/49669]\tLoss: 418.2734\n",
      "Training Epoch: 30 [3200/49669]\tLoss: 397.0582\n",
      "Training Epoch: 30 [3264/49669]\tLoss: 450.7705\n",
      "Training Epoch: 30 [3328/49669]\tLoss: 426.8740\n",
      "Training Epoch: 30 [3392/49669]\tLoss: 467.3893\n",
      "Training Epoch: 30 [3456/49669]\tLoss: 443.0750\n",
      "Training Epoch: 30 [3520/49669]\tLoss: 456.5009\n",
      "Training Epoch: 30 [3584/49669]\tLoss: 414.6660\n",
      "Training Epoch: 30 [3648/49669]\tLoss: 409.9070\n",
      "Training Epoch: 30 [3712/49669]\tLoss: 412.0732\n",
      "Training Epoch: 30 [3776/49669]\tLoss: 416.0818\n",
      "Training Epoch: 30 [3840/49669]\tLoss: 423.6179\n",
      "Training Epoch: 30 [3904/49669]\tLoss: 411.5195\n",
      "Training Epoch: 30 [3968/49669]\tLoss: 471.2113\n",
      "Training Epoch: 30 [4032/49669]\tLoss: 431.1332\n",
      "Training Epoch: 30 [4096/49669]\tLoss: 424.6441\n",
      "Training Epoch: 30 [4160/49669]\tLoss: 424.6216\n",
      "Training Epoch: 30 [4224/49669]\tLoss: 385.8922\n",
      "Training Epoch: 30 [4288/49669]\tLoss: 360.1725\n",
      "Training Epoch: 30 [4352/49669]\tLoss: 392.6133\n",
      "Training Epoch: 30 [4416/49669]\tLoss: 415.9977\n",
      "Training Epoch: 30 [4480/49669]\tLoss: 402.8578\n",
      "Training Epoch: 30 [4544/49669]\tLoss: 410.6807\n",
      "Training Epoch: 30 [4608/49669]\tLoss: 418.5797\n",
      "Training Epoch: 30 [4672/49669]\tLoss: 391.0773\n",
      "Training Epoch: 30 [4736/49669]\tLoss: 404.2521\n",
      "Training Epoch: 30 [4800/49669]\tLoss: 375.9471\n",
      "Training Epoch: 30 [4864/49669]\tLoss: 400.7480\n",
      "Training Epoch: 30 [4928/49669]\tLoss: 437.8248\n",
      "Training Epoch: 30 [4992/49669]\tLoss: 423.5099\n",
      "Training Epoch: 30 [5056/49669]\tLoss: 394.8709\n",
      "Training Epoch: 30 [5120/49669]\tLoss: 418.9591\n",
      "Training Epoch: 30 [5184/49669]\tLoss: 403.7934\n",
      "Training Epoch: 30 [5248/49669]\tLoss: 406.3001\n",
      "Training Epoch: 30 [5312/49669]\tLoss: 387.5014\n",
      "Training Epoch: 30 [5376/49669]\tLoss: 397.1615\n",
      "Training Epoch: 30 [5440/49669]\tLoss: 399.1537\n",
      "Training Epoch: 30 [5504/49669]\tLoss: 415.0539\n",
      "Training Epoch: 30 [5568/49669]\tLoss: 395.3967\n",
      "Training Epoch: 30 [5632/49669]\tLoss: 377.6940\n",
      "Training Epoch: 30 [5696/49669]\tLoss: 396.3483\n",
      "Training Epoch: 30 [5760/49669]\tLoss: 393.9857\n",
      "Training Epoch: 30 [5824/49669]\tLoss: 415.6345\n",
      "Training Epoch: 30 [5888/49669]\tLoss: 401.6773\n",
      "Training Epoch: 30 [5952/49669]\tLoss: 424.4546\n",
      "Training Epoch: 30 [6016/49669]\tLoss: 387.6835\n",
      "Training Epoch: 30 [6080/49669]\tLoss: 419.7354\n",
      "Training Epoch: 30 [6144/49669]\tLoss: 397.5236\n",
      "Training Epoch: 30 [6208/49669]\tLoss: 384.8446\n",
      "Training Epoch: 30 [6272/49669]\tLoss: 408.3595\n",
      "Training Epoch: 30 [6336/49669]\tLoss: 396.1465\n",
      "Training Epoch: 30 [6400/49669]\tLoss: 393.3166\n",
      "Training Epoch: 30 [6464/49669]\tLoss: 418.4610\n",
      "Training Epoch: 30 [6528/49669]\tLoss: 389.0311\n",
      "Training Epoch: 30 [6592/49669]\tLoss: 446.3570\n",
      "Training Epoch: 30 [6656/49669]\tLoss: 401.3955\n",
      "Training Epoch: 30 [6720/49669]\tLoss: 389.1415\n",
      "Training Epoch: 30 [6784/49669]\tLoss: 384.9659\n",
      "Training Epoch: 30 [6848/49669]\tLoss: 391.9985\n",
      "Training Epoch: 30 [6912/49669]\tLoss: 394.2816\n",
      "Training Epoch: 30 [6976/49669]\tLoss: 381.2404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [7040/49669]\tLoss: 398.6771\n",
      "Training Epoch: 30 [7104/49669]\tLoss: 383.9459\n",
      "Training Epoch: 30 [7168/49669]\tLoss: 406.2878\n",
      "Training Epoch: 30 [7232/49669]\tLoss: 386.9082\n",
      "Training Epoch: 30 [7296/49669]\tLoss: 395.3141\n",
      "Training Epoch: 30 [7360/49669]\tLoss: 383.7905\n",
      "Training Epoch: 30 [7424/49669]\tLoss: 409.9842\n",
      "Training Epoch: 30 [7488/49669]\tLoss: 411.0700\n",
      "Training Epoch: 30 [7552/49669]\tLoss: 398.1660\n",
      "Training Epoch: 30 [7616/49669]\tLoss: 385.6425\n",
      "Training Epoch: 30 [7680/49669]\tLoss: 436.2613\n",
      "Training Epoch: 30 [7744/49669]\tLoss: 440.4814\n",
      "Training Epoch: 30 [7808/49669]\tLoss: 388.0141\n",
      "Training Epoch: 30 [7872/49669]\tLoss: 402.6691\n",
      "Training Epoch: 30 [7936/49669]\tLoss: 373.1789\n",
      "Training Epoch: 30 [8000/49669]\tLoss: 382.3628\n",
      "Training Epoch: 30 [8064/49669]\tLoss: 399.0727\n",
      "Training Epoch: 30 [8128/49669]\tLoss: 412.3539\n",
      "Training Epoch: 30 [8192/49669]\tLoss: 394.3021\n",
      "Training Epoch: 30 [8256/49669]\tLoss: 388.0067\n",
      "Training Epoch: 30 [8320/49669]\tLoss: 410.6070\n",
      "Training Epoch: 30 [8384/49669]\tLoss: 427.2076\n",
      "Training Epoch: 30 [8448/49669]\tLoss: 401.9058\n",
      "Training Epoch: 30 [8512/49669]\tLoss: 392.7625\n",
      "Training Epoch: 30 [8576/49669]\tLoss: 405.6597\n",
      "Training Epoch: 30 [8640/49669]\tLoss: 418.1357\n",
      "Training Epoch: 30 [8704/49669]\tLoss: 399.5175\n",
      "Training Epoch: 30 [8768/49669]\tLoss: 447.3909\n",
      "Training Epoch: 30 [8832/49669]\tLoss: 398.4664\n",
      "Training Epoch: 30 [8896/49669]\tLoss: 399.0203\n",
      "Training Epoch: 30 [8960/49669]\tLoss: 391.2944\n",
      "Training Epoch: 30 [9024/49669]\tLoss: 416.1071\n",
      "Training Epoch: 30 [9088/49669]\tLoss: 387.4192\n",
      "Training Epoch: 30 [9152/49669]\tLoss: 394.7289\n",
      "Training Epoch: 30 [9216/49669]\tLoss: 416.0930\n",
      "Training Epoch: 30 [9280/49669]\tLoss: 383.3443\n",
      "Training Epoch: 30 [9344/49669]\tLoss: 401.1193\n",
      "Training Epoch: 30 [9408/49669]\tLoss: 423.3779\n",
      "Training Epoch: 30 [9472/49669]\tLoss: 416.2818\n",
      "Training Epoch: 30 [9536/49669]\tLoss: 415.1319\n",
      "Training Epoch: 30 [9600/49669]\tLoss: 406.7301\n",
      "Training Epoch: 30 [9664/49669]\tLoss: 410.9319\n",
      "Training Epoch: 30 [9728/49669]\tLoss: 385.5839\n",
      "Training Epoch: 30 [9792/49669]\tLoss: 379.5217\n",
      "Training Epoch: 30 [9856/49669]\tLoss: 358.4982\n",
      "Training Epoch: 30 [9920/49669]\tLoss: 376.8167\n",
      "Training Epoch: 30 [9984/49669]\tLoss: 396.1014\n",
      "Training Epoch: 30 [10048/49669]\tLoss: 386.3469\n",
      "Training Epoch: 30 [10112/49669]\tLoss: 424.1625\n",
      "Training Epoch: 30 [10176/49669]\tLoss: 405.8234\n",
      "Training Epoch: 30 [10240/49669]\tLoss: 402.4275\n",
      "Training Epoch: 30 [10304/49669]\tLoss: 402.1167\n",
      "Training Epoch: 30 [10368/49669]\tLoss: 400.8398\n",
      "Training Epoch: 30 [10432/49669]\tLoss: 391.2751\n",
      "Training Epoch: 30 [10496/49669]\tLoss: 420.2709\n",
      "Training Epoch: 30 [10560/49669]\tLoss: 395.5493\n",
      "Training Epoch: 30 [10624/49669]\tLoss: 416.3511\n",
      "Training Epoch: 30 [10688/49669]\tLoss: 439.4471\n",
      "Training Epoch: 30 [10752/49669]\tLoss: 413.4889\n",
      "Training Epoch: 30 [10816/49669]\tLoss: 414.3323\n",
      "Training Epoch: 30 [10880/49669]\tLoss: 413.4229\n",
      "Training Epoch: 30 [10944/49669]\tLoss: 402.6346\n",
      "Training Epoch: 30 [11008/49669]\tLoss: 387.0644\n",
      "Training Epoch: 30 [11072/49669]\tLoss: 399.2424\n",
      "Training Epoch: 30 [11136/49669]\tLoss: 375.1807\n",
      "Training Epoch: 30 [11200/49669]\tLoss: 424.9187\n",
      "Training Epoch: 30 [11264/49669]\tLoss: 386.5815\n",
      "Training Epoch: 30 [11328/49669]\tLoss: 369.2576\n",
      "Training Epoch: 30 [11392/49669]\tLoss: 406.1826\n",
      "Training Epoch: 30 [11456/49669]\tLoss: 380.6111\n",
      "Training Epoch: 30 [11520/49669]\tLoss: 440.1557\n",
      "Training Epoch: 30 [11584/49669]\tLoss: 366.0775\n",
      "Training Epoch: 30 [11648/49669]\tLoss: 417.6686\n",
      "Training Epoch: 30 [11712/49669]\tLoss: 420.7968\n",
      "Training Epoch: 30 [11776/49669]\tLoss: 409.7141\n",
      "Training Epoch: 30 [11840/49669]\tLoss: 392.0718\n",
      "Training Epoch: 30 [11904/49669]\tLoss: 396.2161\n",
      "Training Epoch: 30 [11968/49669]\tLoss: 434.2691\n",
      "Training Epoch: 30 [12032/49669]\tLoss: 394.6722\n",
      "Training Epoch: 30 [12096/49669]\tLoss: 410.3042\n",
      "Training Epoch: 30 [12160/49669]\tLoss: 411.9908\n",
      "Training Epoch: 30 [12224/49669]\tLoss: 395.1350\n",
      "Training Epoch: 30 [12288/49669]\tLoss: 396.1497\n",
      "Training Epoch: 30 [12352/49669]\tLoss: 410.4964\n",
      "Training Epoch: 30 [12416/49669]\tLoss: 429.2063\n",
      "Training Epoch: 30 [12480/49669]\tLoss: 409.3000\n",
      "Training Epoch: 30 [12544/49669]\tLoss: 407.0495\n",
      "Training Epoch: 30 [12608/49669]\tLoss: 427.8126\n",
      "Training Epoch: 30 [12672/49669]\tLoss: 404.0763\n",
      "Training Epoch: 30 [12736/49669]\tLoss: 400.4080\n",
      "Training Epoch: 30 [12800/49669]\tLoss: 430.1824\n",
      "Training Epoch: 30 [12864/49669]\tLoss: 389.2395\n",
      "Training Epoch: 30 [12928/49669]\tLoss: 412.3816\n",
      "Training Epoch: 30 [12992/49669]\tLoss: 376.3324\n",
      "Training Epoch: 30 [13056/49669]\tLoss: 392.1559\n",
      "Training Epoch: 30 [13120/49669]\tLoss: 408.2775\n",
      "Training Epoch: 30 [13184/49669]\tLoss: 406.9524\n",
      "Training Epoch: 30 [13248/49669]\tLoss: 378.1731\n",
      "Training Epoch: 30 [13312/49669]\tLoss: 414.5670\n",
      "Training Epoch: 30 [13376/49669]\tLoss: 407.7894\n",
      "Training Epoch: 30 [13440/49669]\tLoss: 381.0130\n",
      "Training Epoch: 30 [13504/49669]\tLoss: 406.1194\n",
      "Training Epoch: 30 [13568/49669]\tLoss: 422.2820\n",
      "Training Epoch: 30 [13632/49669]\tLoss: 407.6591\n",
      "Training Epoch: 30 [13696/49669]\tLoss: 400.2025\n",
      "Training Epoch: 30 [13760/49669]\tLoss: 409.7365\n",
      "Training Epoch: 30 [13824/49669]\tLoss: 400.7886\n",
      "Training Epoch: 30 [13888/49669]\tLoss: 423.4456\n",
      "Training Epoch: 30 [13952/49669]\tLoss: 416.3275\n",
      "Training Epoch: 30 [14016/49669]\tLoss: 397.9991\n",
      "Training Epoch: 30 [14080/49669]\tLoss: 387.7357\n",
      "Training Epoch: 30 [14144/49669]\tLoss: 415.3271\n",
      "Training Epoch: 30 [14208/49669]\tLoss: 369.0443\n",
      "Training Epoch: 30 [14272/49669]\tLoss: 405.6610\n",
      "Training Epoch: 30 [14336/49669]\tLoss: 411.0924\n",
      "Training Epoch: 30 [14400/49669]\tLoss: 409.7672\n",
      "Training Epoch: 30 [14464/49669]\tLoss: 365.4037\n",
      "Training Epoch: 30 [14528/49669]\tLoss: 399.4101\n",
      "Training Epoch: 30 [14592/49669]\tLoss: 397.9637\n",
      "Training Epoch: 30 [14656/49669]\tLoss: 410.6202\n",
      "Training Epoch: 30 [14720/49669]\tLoss: 407.9779\n",
      "Training Epoch: 30 [14784/49669]\tLoss: 388.2135\n",
      "Training Epoch: 30 [14848/49669]\tLoss: 404.9352\n",
      "Training Epoch: 30 [14912/49669]\tLoss: 374.8207\n",
      "Training Epoch: 30 [14976/49669]\tLoss: 421.1027\n",
      "Training Epoch: 30 [15040/49669]\tLoss: 406.8311\n",
      "Training Epoch: 30 [15104/49669]\tLoss: 395.2947\n",
      "Training Epoch: 30 [15168/49669]\tLoss: 383.4473\n",
      "Training Epoch: 30 [15232/49669]\tLoss: 415.2079\n",
      "Training Epoch: 30 [15296/49669]\tLoss: 410.3875\n",
      "Training Epoch: 30 [15360/49669]\tLoss: 396.1294\n",
      "Training Epoch: 30 [15424/49669]\tLoss: 425.1580\n",
      "Training Epoch: 30 [15488/49669]\tLoss: 410.1298\n",
      "Training Epoch: 30 [15552/49669]\tLoss: 400.3445\n",
      "Training Epoch: 30 [15616/49669]\tLoss: 383.8528\n",
      "Training Epoch: 30 [15680/49669]\tLoss: 415.3820\n",
      "Training Epoch: 30 [15744/49669]\tLoss: 367.9727\n",
      "Training Epoch: 30 [15808/49669]\tLoss: 418.5443\n",
      "Training Epoch: 30 [15872/49669]\tLoss: 392.9495\n",
      "Training Epoch: 30 [15936/49669]\tLoss: 397.0539\n",
      "Training Epoch: 30 [16000/49669]\tLoss: 416.8099\n",
      "Training Epoch: 30 [16064/49669]\tLoss: 403.1485\n",
      "Training Epoch: 30 [16128/49669]\tLoss: 421.4229\n",
      "Training Epoch: 30 [16192/49669]\tLoss: 425.0807\n",
      "Training Epoch: 30 [16256/49669]\tLoss: 411.9482\n",
      "Training Epoch: 30 [16320/49669]\tLoss: 432.7924\n",
      "Training Epoch: 30 [16384/49669]\tLoss: 405.7221\n",
      "Training Epoch: 30 [16448/49669]\tLoss: 422.8525\n",
      "Training Epoch: 30 [16512/49669]\tLoss: 458.6358\n",
      "Training Epoch: 30 [16576/49669]\tLoss: 424.9519\n",
      "Training Epoch: 30 [16640/49669]\tLoss: 445.1022\n",
      "Training Epoch: 30 [16704/49669]\tLoss: 463.7388\n",
      "Training Epoch: 30 [16768/49669]\tLoss: 459.7139\n",
      "Training Epoch: 30 [16832/49669]\tLoss: 440.4184\n",
      "Training Epoch: 30 [16896/49669]\tLoss: 400.2470\n",
      "Training Epoch: 30 [16960/49669]\tLoss: 413.7707\n",
      "Training Epoch: 30 [17024/49669]\tLoss: 410.8028\n",
      "Training Epoch: 30 [17088/49669]\tLoss: 425.7343\n",
      "Training Epoch: 30 [17152/49669]\tLoss: 449.8918\n",
      "Training Epoch: 30 [17216/49669]\tLoss: 421.2696\n",
      "Training Epoch: 30 [17280/49669]\tLoss: 430.2223\n",
      "Training Epoch: 30 [17344/49669]\tLoss: 419.3565\n",
      "Training Epoch: 30 [17408/49669]\tLoss: 381.6485\n",
      "Training Epoch: 30 [17472/49669]\tLoss: 392.9250\n",
      "Training Epoch: 30 [17536/49669]\tLoss: 396.4303\n",
      "Training Epoch: 30 [17600/49669]\tLoss: 391.9549\n",
      "Training Epoch: 30 [17664/49669]\tLoss: 385.3842\n",
      "Training Epoch: 30 [17728/49669]\tLoss: 412.1016\n",
      "Training Epoch: 30 [17792/49669]\tLoss: 387.7720\n",
      "Training Epoch: 30 [17856/49669]\tLoss: 445.4759\n",
      "Training Epoch: 30 [17920/49669]\tLoss: 415.1041\n",
      "Training Epoch: 30 [17984/49669]\tLoss: 425.4661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [18048/49669]\tLoss: 405.4565\n",
      "Training Epoch: 30 [18112/49669]\tLoss: 449.8830\n",
      "Training Epoch: 30 [18176/49669]\tLoss: 416.0983\n",
      "Training Epoch: 30 [18240/49669]\tLoss: 400.2983\n",
      "Training Epoch: 30 [18304/49669]\tLoss: 433.2831\n",
      "Training Epoch: 30 [18368/49669]\tLoss: 422.2506\n",
      "Training Epoch: 30 [18432/49669]\tLoss: 406.2612\n",
      "Training Epoch: 30 [18496/49669]\tLoss: 416.3469\n",
      "Training Epoch: 30 [18560/49669]\tLoss: 381.7301\n",
      "Training Epoch: 30 [18624/49669]\tLoss: 374.2618\n",
      "Training Epoch: 30 [18688/49669]\tLoss: 354.2135\n",
      "Training Epoch: 30 [18752/49669]\tLoss: 384.6910\n",
      "Training Epoch: 30 [18816/49669]\tLoss: 405.5122\n",
      "Training Epoch: 30 [18880/49669]\tLoss: 419.2879\n",
      "Training Epoch: 30 [18944/49669]\tLoss: 375.9311\n",
      "Training Epoch: 30 [19008/49669]\tLoss: 424.5264\n",
      "Training Epoch: 30 [19072/49669]\tLoss: 432.1579\n",
      "Training Epoch: 30 [19136/49669]\tLoss: 409.2379\n",
      "Training Epoch: 30 [19200/49669]\tLoss: 368.6528\n",
      "Training Epoch: 30 [19264/49669]\tLoss: 401.3951\n",
      "Training Epoch: 30 [19328/49669]\tLoss: 399.3732\n",
      "Training Epoch: 30 [19392/49669]\tLoss: 406.5352\n",
      "Training Epoch: 30 [19456/49669]\tLoss: 396.8267\n",
      "Training Epoch: 30 [19520/49669]\tLoss: 386.2275\n",
      "Training Epoch: 30 [19584/49669]\tLoss: 414.6468\n",
      "Training Epoch: 30 [19648/49669]\tLoss: 415.9458\n",
      "Training Epoch: 30 [19712/49669]\tLoss: 426.6952\n",
      "Training Epoch: 30 [19776/49669]\tLoss: 415.0458\n",
      "Training Epoch: 30 [19840/49669]\tLoss: 394.6034\n",
      "Training Epoch: 30 [19904/49669]\tLoss: 411.9554\n",
      "Training Epoch: 30 [19968/49669]\tLoss: 398.6443\n",
      "Training Epoch: 30 [20032/49669]\tLoss: 391.1453\n",
      "Training Epoch: 30 [20096/49669]\tLoss: 369.7878\n",
      "Training Epoch: 30 [20160/49669]\tLoss: 393.1432\n",
      "Training Epoch: 30 [20224/49669]\tLoss: 407.6057\n",
      "Training Epoch: 30 [20288/49669]\tLoss: 435.1819\n",
      "Training Epoch: 30 [20352/49669]\tLoss: 401.4292\n",
      "Training Epoch: 30 [20416/49669]\tLoss: 422.6942\n",
      "Training Epoch: 30 [20480/49669]\tLoss: 405.9653\n",
      "Training Epoch: 30 [20544/49669]\tLoss: 414.8397\n",
      "Training Epoch: 30 [20608/49669]\tLoss: 399.4512\n",
      "Training Epoch: 30 [20672/49669]\tLoss: 419.0476\n",
      "Training Epoch: 30 [20736/49669]\tLoss: 379.7360\n",
      "Training Epoch: 30 [20800/49669]\tLoss: 413.4577\n",
      "Training Epoch: 30 [20864/49669]\tLoss: 426.2702\n",
      "Training Epoch: 30 [20928/49669]\tLoss: 400.6560\n",
      "Training Epoch: 30 [20992/49669]\tLoss: 413.9856\n",
      "Training Epoch: 30 [21056/49669]\tLoss: 361.3358\n",
      "Training Epoch: 30 [21120/49669]\tLoss: 374.5273\n",
      "Training Epoch: 30 [21184/49669]\tLoss: 426.7947\n",
      "Training Epoch: 30 [21248/49669]\tLoss: 378.2623\n",
      "Training Epoch: 30 [21312/49669]\tLoss: 377.1248\n",
      "Training Epoch: 30 [21376/49669]\tLoss: 408.6535\n",
      "Training Epoch: 30 [21440/49669]\tLoss: 413.3045\n",
      "Training Epoch: 30 [21504/49669]\tLoss: 418.1833\n",
      "Training Epoch: 30 [21568/49669]\tLoss: 392.4701\n",
      "Training Epoch: 30 [21632/49669]\tLoss: 407.2405\n",
      "Training Epoch: 30 [21696/49669]\tLoss: 402.3092\n",
      "Training Epoch: 30 [21760/49669]\tLoss: 390.5685\n",
      "Training Epoch: 30 [21824/49669]\tLoss: 410.5502\n",
      "Training Epoch: 30 [21888/49669]\tLoss: 404.2174\n",
      "Training Epoch: 30 [21952/49669]\tLoss: 418.7899\n",
      "Training Epoch: 30 [22016/49669]\tLoss: 412.2469\n",
      "Training Epoch: 30 [22080/49669]\tLoss: 399.5012\n",
      "Training Epoch: 30 [22144/49669]\tLoss: 399.8952\n",
      "Training Epoch: 30 [22208/49669]\tLoss: 389.3963\n",
      "Training Epoch: 30 [22272/49669]\tLoss: 396.7717\n",
      "Training Epoch: 30 [22336/49669]\tLoss: 402.4469\n",
      "Training Epoch: 30 [22400/49669]\tLoss: 415.0791\n",
      "Training Epoch: 30 [22464/49669]\tLoss: 377.7518\n",
      "Training Epoch: 30 [22528/49669]\tLoss: 407.9890\n",
      "Training Epoch: 30 [22592/49669]\tLoss: 399.8299\n",
      "Training Epoch: 30 [22656/49669]\tLoss: 393.4024\n",
      "Training Epoch: 30 [22720/49669]\tLoss: 393.4956\n",
      "Training Epoch: 30 [22784/49669]\tLoss: 436.5715\n",
      "Training Epoch: 30 [22848/49669]\tLoss: 392.6695\n",
      "Training Epoch: 30 [22912/49669]\tLoss: 411.4408\n",
      "Training Epoch: 30 [22976/49669]\tLoss: 374.6479\n",
      "Training Epoch: 30 [23040/49669]\tLoss: 391.4192\n",
      "Training Epoch: 30 [23104/49669]\tLoss: 406.1158\n",
      "Training Epoch: 30 [23168/49669]\tLoss: 393.4594\n",
      "Training Epoch: 30 [23232/49669]\tLoss: 407.1150\n",
      "Training Epoch: 30 [23296/49669]\tLoss: 417.4519\n",
      "Training Epoch: 30 [23360/49669]\tLoss: 435.5423\n",
      "Training Epoch: 30 [23424/49669]\tLoss: 365.4885\n",
      "Training Epoch: 30 [23488/49669]\tLoss: 390.3526\n",
      "Training Epoch: 30 [23552/49669]\tLoss: 412.2945\n",
      "Training Epoch: 30 [23616/49669]\tLoss: 412.9062\n",
      "Training Epoch: 30 [23680/49669]\tLoss: 401.5043\n",
      "Training Epoch: 30 [23744/49669]\tLoss: 391.7213\n",
      "Training Epoch: 30 [23808/49669]\tLoss: 402.9077\n",
      "Training Epoch: 30 [23872/49669]\tLoss: 377.1643\n",
      "Training Epoch: 30 [23936/49669]\tLoss: 381.5442\n",
      "Training Epoch: 30 [24000/49669]\tLoss: 408.3043\n",
      "Training Epoch: 30 [24064/49669]\tLoss: 416.2935\n",
      "Training Epoch: 30 [24128/49669]\tLoss: 372.8430\n",
      "Training Epoch: 30 [24192/49669]\tLoss: 386.5193\n",
      "Training Epoch: 30 [24256/49669]\tLoss: 400.9395\n",
      "Training Epoch: 30 [24320/49669]\tLoss: 372.6382\n",
      "Training Epoch: 30 [24384/49669]\tLoss: 392.0984\n",
      "Training Epoch: 30 [24448/49669]\tLoss: 390.3721\n",
      "Training Epoch: 30 [24512/49669]\tLoss: 397.2354\n",
      "Training Epoch: 30 [24576/49669]\tLoss: 378.6860\n",
      "Training Epoch: 30 [24640/49669]\tLoss: 364.6469\n",
      "Training Epoch: 30 [24704/49669]\tLoss: 433.8138\n",
      "Training Epoch: 30 [24768/49669]\tLoss: 420.0410\n",
      "Training Epoch: 30 [24832/49669]\tLoss: 418.8836\n",
      "Training Epoch: 30 [24896/49669]\tLoss: 407.5916\n",
      "Training Epoch: 30 [24960/49669]\tLoss: 381.8359\n",
      "Training Epoch: 30 [25024/49669]\tLoss: 426.1922\n",
      "Training Epoch: 30 [25088/49669]\tLoss: 413.8403\n",
      "Training Epoch: 30 [25152/49669]\tLoss: 387.9327\n",
      "Training Epoch: 30 [25216/49669]\tLoss: 385.8817\n",
      "Training Epoch: 30 [25280/49669]\tLoss: 416.0294\n",
      "Training Epoch: 30 [25344/49669]\tLoss: 377.8968\n",
      "Training Epoch: 30 [25408/49669]\tLoss: 409.0441\n",
      "Training Epoch: 30 [25472/49669]\tLoss: 381.4124\n",
      "Training Epoch: 30 [25536/49669]\tLoss: 388.0171\n",
      "Training Epoch: 30 [25600/49669]\tLoss: 389.1625\n",
      "Training Epoch: 30 [25664/49669]\tLoss: 405.6037\n",
      "Training Epoch: 30 [25728/49669]\tLoss: 390.6156\n",
      "Training Epoch: 30 [25792/49669]\tLoss: 417.5907\n",
      "Training Epoch: 30 [25856/49669]\tLoss: 413.5811\n",
      "Training Epoch: 30 [25920/49669]\tLoss: 382.9427\n",
      "Training Epoch: 30 [25984/49669]\tLoss: 417.6729\n",
      "Training Epoch: 30 [26048/49669]\tLoss: 419.4339\n",
      "Training Epoch: 30 [26112/49669]\tLoss: 415.3043\n",
      "Training Epoch: 30 [26176/49669]\tLoss: 414.5876\n",
      "Training Epoch: 30 [26240/49669]\tLoss: 390.7554\n",
      "Training Epoch: 30 [26304/49669]\tLoss: 386.5392\n",
      "Training Epoch: 30 [26368/49669]\tLoss: 404.7246\n",
      "Training Epoch: 30 [26432/49669]\tLoss: 405.6606\n",
      "Training Epoch: 30 [26496/49669]\tLoss: 387.0009\n",
      "Training Epoch: 30 [26560/49669]\tLoss: 396.3997\n",
      "Training Epoch: 30 [26624/49669]\tLoss: 401.5557\n",
      "Training Epoch: 30 [26688/49669]\tLoss: 395.0123\n",
      "Training Epoch: 30 [26752/49669]\tLoss: 406.7783\n",
      "Training Epoch: 30 [26816/49669]\tLoss: 411.2073\n",
      "Training Epoch: 30 [26880/49669]\tLoss: 375.7478\n",
      "Training Epoch: 30 [26944/49669]\tLoss: 404.1689\n",
      "Training Epoch: 30 [27008/49669]\tLoss: 386.4139\n",
      "Training Epoch: 30 [27072/49669]\tLoss: 402.3893\n",
      "Training Epoch: 30 [27136/49669]\tLoss: 425.3231\n",
      "Training Epoch: 30 [27200/49669]\tLoss: 409.4618\n",
      "Training Epoch: 30 [27264/49669]\tLoss: 411.7909\n",
      "Training Epoch: 30 [27328/49669]\tLoss: 427.1732\n",
      "Training Epoch: 30 [27392/49669]\tLoss: 408.7293\n",
      "Training Epoch: 30 [27456/49669]\tLoss: 401.9824\n",
      "Training Epoch: 30 [27520/49669]\tLoss: 408.2497\n",
      "Training Epoch: 30 [27584/49669]\tLoss: 404.3353\n",
      "Training Epoch: 30 [27648/49669]\tLoss: 395.4634\n",
      "Training Epoch: 30 [27712/49669]\tLoss: 403.4879\n",
      "Training Epoch: 30 [27776/49669]\tLoss: 389.3863\n",
      "Training Epoch: 30 [27840/49669]\tLoss: 401.2553\n",
      "Training Epoch: 30 [27904/49669]\tLoss: 381.2522\n",
      "Training Epoch: 30 [27968/49669]\tLoss: 398.9000\n",
      "Training Epoch: 30 [28032/49669]\tLoss: 416.5897\n",
      "Training Epoch: 30 [28096/49669]\tLoss: 391.4502\n",
      "Training Epoch: 30 [28160/49669]\tLoss: 383.6643\n",
      "Training Epoch: 30 [28224/49669]\tLoss: 390.8758\n",
      "Training Epoch: 30 [28288/49669]\tLoss: 399.1411\n",
      "Training Epoch: 30 [28352/49669]\tLoss: 410.3493\n",
      "Training Epoch: 30 [28416/49669]\tLoss: 417.0919\n",
      "Training Epoch: 30 [28480/49669]\tLoss: 394.2050\n",
      "Training Epoch: 30 [28544/49669]\tLoss: 399.6668\n",
      "Training Epoch: 30 [28608/49669]\tLoss: 406.0716\n",
      "Training Epoch: 30 [28672/49669]\tLoss: 385.7946\n",
      "Training Epoch: 30 [28736/49669]\tLoss: 413.5402\n",
      "Training Epoch: 30 [28800/49669]\tLoss: 386.1436\n",
      "Training Epoch: 30 [28864/49669]\tLoss: 373.9765\n",
      "Training Epoch: 30 [28928/49669]\tLoss: 384.5700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [28992/49669]\tLoss: 403.7639\n",
      "Training Epoch: 30 [29056/49669]\tLoss: 368.4398\n",
      "Training Epoch: 30 [29120/49669]\tLoss: 407.1647\n",
      "Training Epoch: 30 [29184/49669]\tLoss: 396.9297\n",
      "Training Epoch: 30 [29248/49669]\tLoss: 363.5916\n",
      "Training Epoch: 30 [29312/49669]\tLoss: 345.5349\n",
      "Training Epoch: 30 [29376/49669]\tLoss: 401.1095\n",
      "Training Epoch: 30 [29440/49669]\tLoss: 423.1356\n",
      "Training Epoch: 30 [29504/49669]\tLoss: 406.6464\n",
      "Training Epoch: 30 [29568/49669]\tLoss: 382.6423\n",
      "Training Epoch: 30 [29632/49669]\tLoss: 397.7026\n",
      "Training Epoch: 30 [29696/49669]\tLoss: 404.7704\n",
      "Training Epoch: 30 [29760/49669]\tLoss: 411.0496\n",
      "Training Epoch: 30 [29824/49669]\tLoss: 434.8773\n",
      "Training Epoch: 30 [29888/49669]\tLoss: 444.2714\n",
      "Training Epoch: 30 [29952/49669]\tLoss: 386.3034\n",
      "Training Epoch: 30 [30016/49669]\tLoss: 392.7641\n",
      "Training Epoch: 30 [30080/49669]\tLoss: 387.7584\n",
      "Training Epoch: 30 [30144/49669]\tLoss: 397.8548\n",
      "Training Epoch: 30 [30208/49669]\tLoss: 384.3885\n",
      "Training Epoch: 30 [30272/49669]\tLoss: 416.1389\n",
      "Training Epoch: 30 [30336/49669]\tLoss: 425.5793\n",
      "Training Epoch: 30 [30400/49669]\tLoss: 386.5169\n",
      "Training Epoch: 30 [30464/49669]\tLoss: 392.7296\n",
      "Training Epoch: 30 [30528/49669]\tLoss: 421.8294\n",
      "Training Epoch: 30 [30592/49669]\tLoss: 397.6400\n",
      "Training Epoch: 30 [30656/49669]\tLoss: 403.1557\n",
      "Training Epoch: 30 [30720/49669]\tLoss: 428.1835\n",
      "Training Epoch: 30 [30784/49669]\tLoss: 407.0219\n",
      "Training Epoch: 30 [30848/49669]\tLoss: 407.7096\n",
      "Training Epoch: 30 [30912/49669]\tLoss: 391.0232\n",
      "Training Epoch: 30 [30976/49669]\tLoss: 390.9411\n",
      "Training Epoch: 30 [31040/49669]\tLoss: 395.6235\n",
      "Training Epoch: 30 [31104/49669]\tLoss: 367.5294\n",
      "Training Epoch: 30 [31168/49669]\tLoss: 414.3246\n",
      "Training Epoch: 30 [31232/49669]\tLoss: 409.1596\n",
      "Training Epoch: 30 [31296/49669]\tLoss: 364.1465\n",
      "Training Epoch: 30 [31360/49669]\tLoss: 368.1614\n",
      "Training Epoch: 30 [31424/49669]\tLoss: 387.8967\n",
      "Training Epoch: 30 [31488/49669]\tLoss: 395.8051\n",
      "Training Epoch: 30 [31552/49669]\tLoss: 453.1315\n",
      "Training Epoch: 30 [31616/49669]\tLoss: 443.5232\n",
      "Training Epoch: 30 [31680/49669]\tLoss: 378.1771\n",
      "Training Epoch: 30 [31744/49669]\tLoss: 413.4878\n",
      "Training Epoch: 30 [31808/49669]\tLoss: 394.2307\n",
      "Training Epoch: 30 [31872/49669]\tLoss: 407.4245\n",
      "Training Epoch: 30 [31936/49669]\tLoss: 372.6145\n",
      "Training Epoch: 30 [32000/49669]\tLoss: 382.6127\n",
      "Training Epoch: 30 [32064/49669]\tLoss: 390.1534\n",
      "Training Epoch: 30 [32128/49669]\tLoss: 389.3549\n",
      "Training Epoch: 30 [32192/49669]\tLoss: 388.9394\n",
      "Training Epoch: 30 [32256/49669]\tLoss: 381.7219\n",
      "Training Epoch: 30 [32320/49669]\tLoss: 418.7685\n",
      "Training Epoch: 30 [32384/49669]\tLoss: 401.2209\n",
      "Training Epoch: 30 [32448/49669]\tLoss: 395.6401\n",
      "Training Epoch: 30 [32512/49669]\tLoss: 373.7712\n",
      "Training Epoch: 30 [32576/49669]\tLoss: 411.6978\n",
      "Training Epoch: 30 [32640/49669]\tLoss: 378.9995\n",
      "Training Epoch: 30 [32704/49669]\tLoss: 400.9078\n",
      "Training Epoch: 30 [32768/49669]\tLoss: 396.4267\n",
      "Training Epoch: 30 [32832/49669]\tLoss: 409.2144\n",
      "Training Epoch: 30 [32896/49669]\tLoss: 427.4170\n",
      "Training Epoch: 30 [32960/49669]\tLoss: 430.3755\n",
      "Training Epoch: 30 [33024/49669]\tLoss: 404.6717\n",
      "Training Epoch: 30 [33088/49669]\tLoss: 369.2539\n",
      "Training Epoch: 30 [33152/49669]\tLoss: 416.3742\n",
      "Training Epoch: 30 [33216/49669]\tLoss: 427.0786\n",
      "Training Epoch: 30 [33280/49669]\tLoss: 395.2538\n",
      "Training Epoch: 30 [33344/49669]\tLoss: 425.3364\n",
      "Training Epoch: 30 [33408/49669]\tLoss: 411.3624\n",
      "Training Epoch: 30 [33472/49669]\tLoss: 418.9696\n",
      "Training Epoch: 30 [33536/49669]\tLoss: 423.8333\n",
      "Training Epoch: 30 [33600/49669]\tLoss: 414.4194\n",
      "Training Epoch: 30 [33664/49669]\tLoss: 411.2249\n",
      "Training Epoch: 30 [33728/49669]\tLoss: 417.1623\n",
      "Training Epoch: 30 [33792/49669]\tLoss: 402.0450\n",
      "Training Epoch: 30 [33856/49669]\tLoss: 426.0463\n",
      "Training Epoch: 30 [33920/49669]\tLoss: 435.3947\n",
      "Training Epoch: 30 [33984/49669]\tLoss: 441.4006\n",
      "Training Epoch: 30 [34048/49669]\tLoss: 443.4382\n",
      "Training Epoch: 30 [34112/49669]\tLoss: 387.2738\n",
      "Training Epoch: 30 [34176/49669]\tLoss: 395.2960\n",
      "Training Epoch: 30 [34240/49669]\tLoss: 395.2811\n",
      "Training Epoch: 30 [34304/49669]\tLoss: 441.5620\n",
      "Training Epoch: 30 [34368/49669]\tLoss: 410.5847\n",
      "Training Epoch: 30 [34432/49669]\tLoss: 392.3463\n",
      "Training Epoch: 30 [34496/49669]\tLoss: 399.1551\n",
      "Training Epoch: 30 [34560/49669]\tLoss: 451.6215\n",
      "Training Epoch: 30 [34624/49669]\tLoss: 407.3428\n",
      "Training Epoch: 30 [34688/49669]\tLoss: 390.9493\n",
      "Training Epoch: 30 [34752/49669]\tLoss: 409.5579\n",
      "Training Epoch: 30 [34816/49669]\tLoss: 375.8477\n",
      "Training Epoch: 30 [34880/49669]\tLoss: 439.9371\n",
      "Training Epoch: 30 [34944/49669]\tLoss: 401.8537\n",
      "Training Epoch: 30 [35008/49669]\tLoss: 438.8249\n",
      "Training Epoch: 30 [35072/49669]\tLoss: 423.8371\n",
      "Training Epoch: 30 [35136/49669]\tLoss: 402.8832\n",
      "Training Epoch: 30 [35200/49669]\tLoss: 382.0651\n",
      "Training Epoch: 30 [35264/49669]\tLoss: 393.8148\n",
      "Training Epoch: 30 [35328/49669]\tLoss: 391.6492\n",
      "Training Epoch: 30 [35392/49669]\tLoss: 408.9445\n",
      "Training Epoch: 30 [35456/49669]\tLoss: 418.2054\n",
      "Training Epoch: 30 [35520/49669]\tLoss: 439.5411\n",
      "Training Epoch: 30 [35584/49669]\tLoss: 375.6075\n",
      "Training Epoch: 30 [35648/49669]\tLoss: 431.5343\n",
      "Training Epoch: 30 [35712/49669]\tLoss: 405.1358\n",
      "Training Epoch: 30 [35776/49669]\tLoss: 391.3192\n",
      "Training Epoch: 30 [35840/49669]\tLoss: 398.2028\n",
      "Training Epoch: 30 [35904/49669]\tLoss: 389.9409\n",
      "Training Epoch: 30 [35968/49669]\tLoss: 409.6138\n",
      "Training Epoch: 30 [36032/49669]\tLoss: 430.2558\n",
      "Training Epoch: 30 [36096/49669]\tLoss: 397.1690\n",
      "Training Epoch: 30 [36160/49669]\tLoss: 431.0561\n",
      "Training Epoch: 30 [36224/49669]\tLoss: 403.8286\n",
      "Training Epoch: 30 [36288/49669]\tLoss: 409.6252\n",
      "Training Epoch: 30 [36352/49669]\tLoss: 391.5703\n",
      "Training Epoch: 30 [36416/49669]\tLoss: 378.1431\n",
      "Training Epoch: 30 [36480/49669]\tLoss: 401.8018\n",
      "Training Epoch: 30 [36544/49669]\tLoss: 415.7145\n",
      "Training Epoch: 30 [36608/49669]\tLoss: 396.4059\n",
      "Training Epoch: 30 [36672/49669]\tLoss: 405.5216\n",
      "Training Epoch: 30 [36736/49669]\tLoss: 410.9077\n",
      "Training Epoch: 30 [36800/49669]\tLoss: 392.0779\n",
      "Training Epoch: 30 [36864/49669]\tLoss: 418.7304\n",
      "Training Epoch: 30 [36928/49669]\tLoss: 403.3891\n",
      "Training Epoch: 30 [36992/49669]\tLoss: 414.9528\n",
      "Training Epoch: 30 [37056/49669]\tLoss: 420.1622\n",
      "Training Epoch: 30 [37120/49669]\tLoss: 377.1700\n",
      "Training Epoch: 30 [37184/49669]\tLoss: 417.8980\n",
      "Training Epoch: 30 [37248/49669]\tLoss: 416.2861\n",
      "Training Epoch: 30 [37312/49669]\tLoss: 414.0751\n",
      "Training Epoch: 30 [37376/49669]\tLoss: 405.3666\n",
      "Training Epoch: 30 [37440/49669]\tLoss: 392.8219\n",
      "Training Epoch: 30 [37504/49669]\tLoss: 401.4274\n",
      "Training Epoch: 30 [37568/49669]\tLoss: 412.8553\n",
      "Training Epoch: 30 [37632/49669]\tLoss: 401.6842\n",
      "Training Epoch: 30 [37696/49669]\tLoss: 395.7530\n",
      "Training Epoch: 30 [37760/49669]\tLoss: 384.4696\n",
      "Training Epoch: 30 [37824/49669]\tLoss: 401.5371\n",
      "Training Epoch: 30 [37888/49669]\tLoss: 387.6735\n",
      "Training Epoch: 30 [37952/49669]\tLoss: 411.5684\n",
      "Training Epoch: 30 [38016/49669]\tLoss: 396.3469\n",
      "Training Epoch: 30 [38080/49669]\tLoss: 411.1662\n",
      "Training Epoch: 30 [38144/49669]\tLoss: 416.0112\n",
      "Training Epoch: 30 [38208/49669]\tLoss: 441.4951\n",
      "Training Epoch: 30 [38272/49669]\tLoss: 398.2471\n",
      "Training Epoch: 30 [38336/49669]\tLoss: 415.0726\n",
      "Training Epoch: 30 [38400/49669]\tLoss: 431.6508\n",
      "Training Epoch: 30 [38464/49669]\tLoss: 435.4607\n",
      "Training Epoch: 30 [38528/49669]\tLoss: 433.7953\n",
      "Training Epoch: 30 [38592/49669]\tLoss: 426.9170\n",
      "Training Epoch: 30 [38656/49669]\tLoss: 436.9588\n",
      "Training Epoch: 30 [38720/49669]\tLoss: 410.2280\n",
      "Training Epoch: 30 [38784/49669]\tLoss: 390.8462\n",
      "Training Epoch: 30 [38848/49669]\tLoss: 401.5319\n",
      "Training Epoch: 30 [38912/49669]\tLoss: 391.0283\n",
      "Training Epoch: 30 [38976/49669]\tLoss: 415.2763\n",
      "Training Epoch: 30 [39040/49669]\tLoss: 426.8319\n",
      "Training Epoch: 30 [39104/49669]\tLoss: 390.2260\n",
      "Training Epoch: 30 [39168/49669]\tLoss: 418.2424\n",
      "Training Epoch: 30 [39232/49669]\tLoss: 408.9589\n",
      "Training Epoch: 30 [39296/49669]\tLoss: 401.9600\n",
      "Training Epoch: 30 [39360/49669]\tLoss: 415.9640\n",
      "Training Epoch: 30 [39424/49669]\tLoss: 396.7708\n",
      "Training Epoch: 30 [39488/49669]\tLoss: 431.0186\n",
      "Training Epoch: 30 [39552/49669]\tLoss: 409.2715\n",
      "Training Epoch: 30 [39616/49669]\tLoss: 397.9637\n",
      "Training Epoch: 30 [39680/49669]\tLoss: 413.1816\n",
      "Training Epoch: 30 [39744/49669]\tLoss: 429.1125\n",
      "Training Epoch: 30 [39808/49669]\tLoss: 402.9368\n",
      "Training Epoch: 30 [39872/49669]\tLoss: 412.7645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [39936/49669]\tLoss: 399.7203\n",
      "Training Epoch: 30 [40000/49669]\tLoss: 432.9738\n",
      "Training Epoch: 30 [40064/49669]\tLoss: 416.7090\n",
      "Training Epoch: 30 [40128/49669]\tLoss: 412.2441\n",
      "Training Epoch: 30 [40192/49669]\tLoss: 422.2906\n",
      "Training Epoch: 30 [40256/49669]\tLoss: 414.4973\n",
      "Training Epoch: 30 [40320/49669]\tLoss: 403.1668\n",
      "Training Epoch: 30 [40384/49669]\tLoss: 397.2641\n",
      "Training Epoch: 30 [40448/49669]\tLoss: 389.8711\n",
      "Training Epoch: 30 [40512/49669]\tLoss: 398.9217\n",
      "Training Epoch: 30 [40576/49669]\tLoss: 412.5717\n",
      "Training Epoch: 30 [40640/49669]\tLoss: 417.2870\n",
      "Training Epoch: 30 [40704/49669]\tLoss: 387.6442\n",
      "Training Epoch: 30 [40768/49669]\tLoss: 443.9510\n",
      "Training Epoch: 30 [40832/49669]\tLoss: 419.0638\n",
      "Training Epoch: 30 [40896/49669]\tLoss: 406.0201\n",
      "Training Epoch: 30 [40960/49669]\tLoss: 405.9333\n",
      "Training Epoch: 30 [41024/49669]\tLoss: 408.9439\n",
      "Training Epoch: 30 [41088/49669]\tLoss: 413.4149\n",
      "Training Epoch: 30 [41152/49669]\tLoss: 415.9998\n",
      "Training Epoch: 30 [41216/49669]\tLoss: 416.7328\n",
      "Training Epoch: 30 [41280/49669]\tLoss: 396.4025\n",
      "Training Epoch: 30 [41344/49669]\tLoss: 460.3448\n",
      "Training Epoch: 30 [41408/49669]\tLoss: 394.6148\n",
      "Training Epoch: 30 [41472/49669]\tLoss: 410.1382\n",
      "Training Epoch: 30 [41536/49669]\tLoss: 382.6085\n",
      "Training Epoch: 30 [41600/49669]\tLoss: 399.5566\n",
      "Training Epoch: 30 [41664/49669]\tLoss: 393.7876\n",
      "Training Epoch: 30 [41728/49669]\tLoss: 369.9409\n",
      "Training Epoch: 30 [41792/49669]\tLoss: 404.6101\n",
      "Training Epoch: 30 [41856/49669]\tLoss: 400.9984\n",
      "Training Epoch: 30 [41920/49669]\tLoss: 405.3868\n",
      "Training Epoch: 30 [41984/49669]\tLoss: 389.6210\n",
      "Training Epoch: 30 [42048/49669]\tLoss: 411.9698\n",
      "Training Epoch: 30 [42112/49669]\tLoss: 395.0931\n",
      "Training Epoch: 30 [42176/49669]\tLoss: 425.8213\n",
      "Training Epoch: 30 [42240/49669]\tLoss: 395.8553\n",
      "Training Epoch: 30 [42304/49669]\tLoss: 399.2785\n",
      "Training Epoch: 30 [42368/49669]\tLoss: 382.9044\n",
      "Training Epoch: 30 [42432/49669]\tLoss: 398.4034\n",
      "Training Epoch: 30 [42496/49669]\tLoss: 387.8799\n",
      "Training Epoch: 30 [42560/49669]\tLoss: 422.6799\n",
      "Training Epoch: 30 [42624/49669]\tLoss: 393.4652\n",
      "Training Epoch: 30 [42688/49669]\tLoss: 352.4266\n",
      "Training Epoch: 30 [42752/49669]\tLoss: 401.7912\n",
      "Training Epoch: 30 [42816/49669]\tLoss: 394.8752\n",
      "Training Epoch: 30 [42880/49669]\tLoss: 381.0996\n",
      "Training Epoch: 30 [42944/49669]\tLoss: 407.7597\n",
      "Training Epoch: 30 [43008/49669]\tLoss: 421.6831\n",
      "Training Epoch: 30 [43072/49669]\tLoss: 399.7036\n",
      "Training Epoch: 30 [43136/49669]\tLoss: 391.8056\n",
      "Training Epoch: 30 [43200/49669]\tLoss: 419.7138\n",
      "Training Epoch: 30 [43264/49669]\tLoss: 392.9246\n",
      "Training Epoch: 30 [43328/49669]\tLoss: 420.6017\n",
      "Training Epoch: 30 [43392/49669]\tLoss: 411.6776\n",
      "Training Epoch: 30 [43456/49669]\tLoss: 433.5186\n",
      "Training Epoch: 30 [43520/49669]\tLoss: 373.4798\n",
      "Training Epoch: 30 [43584/49669]\tLoss: 419.4729\n",
      "Training Epoch: 30 [43648/49669]\tLoss: 376.8137\n",
      "Training Epoch: 30 [43712/49669]\tLoss: 407.7003\n",
      "Training Epoch: 30 [43776/49669]\tLoss: 423.4008\n",
      "Training Epoch: 30 [43840/49669]\tLoss: 398.0344\n",
      "Training Epoch: 30 [43904/49669]\tLoss: 409.9970\n",
      "Training Epoch: 30 [43968/49669]\tLoss: 419.2710\n",
      "Training Epoch: 30 [44032/49669]\tLoss: 407.8232\n",
      "Training Epoch: 30 [44096/49669]\tLoss: 416.6015\n",
      "Training Epoch: 30 [44160/49669]\tLoss: 386.9849\n",
      "Training Epoch: 30 [44224/49669]\tLoss: 379.1685\n",
      "Training Epoch: 30 [44288/49669]\tLoss: 411.1627\n",
      "Training Epoch: 30 [44352/49669]\tLoss: 404.9381\n",
      "Training Epoch: 30 [44416/49669]\tLoss: 400.2967\n",
      "Training Epoch: 30 [44480/49669]\tLoss: 401.7370\n",
      "Training Epoch: 30 [44544/49669]\tLoss: 393.0992\n",
      "Training Epoch: 30 [44608/49669]\tLoss: 375.0936\n",
      "Training Epoch: 30 [44672/49669]\tLoss: 399.5408\n",
      "Training Epoch: 30 [44736/49669]\tLoss: 389.7744\n",
      "Training Epoch: 30 [44800/49669]\tLoss: 409.4177\n",
      "Training Epoch: 30 [44864/49669]\tLoss: 421.6350\n",
      "Training Epoch: 30 [44928/49669]\tLoss: 370.6661\n",
      "Training Epoch: 30 [44992/49669]\tLoss: 426.3235\n",
      "Training Epoch: 30 [45056/49669]\tLoss: 415.4969\n",
      "Training Epoch: 30 [45120/49669]\tLoss: 385.4501\n",
      "Training Epoch: 30 [45184/49669]\tLoss: 391.6164\n",
      "Training Epoch: 30 [45248/49669]\tLoss: 398.3634\n",
      "Training Epoch: 30 [45312/49669]\tLoss: 369.9523\n",
      "Training Epoch: 30 [45376/49669]\tLoss: 427.9298\n",
      "Training Epoch: 30 [45440/49669]\tLoss: 407.7423\n",
      "Training Epoch: 30 [45504/49669]\tLoss: 382.0829\n",
      "Training Epoch: 30 [45568/49669]\tLoss: 446.4206\n",
      "Training Epoch: 30 [45632/49669]\tLoss: 417.5123\n",
      "Training Epoch: 30 [45696/49669]\tLoss: 391.0254\n",
      "Training Epoch: 30 [45760/49669]\tLoss: 394.7059\n",
      "Training Epoch: 30 [45824/49669]\tLoss: 371.6143\n",
      "Training Epoch: 30 [45888/49669]\tLoss: 424.0397\n",
      "Training Epoch: 30 [45952/49669]\tLoss: 387.1757\n",
      "Training Epoch: 30 [46016/49669]\tLoss: 387.4665\n",
      "Training Epoch: 30 [46080/49669]\tLoss: 374.6339\n",
      "Training Epoch: 30 [46144/49669]\tLoss: 399.2724\n",
      "Training Epoch: 30 [46208/49669]\tLoss: 427.5712\n",
      "Training Epoch: 30 [46272/49669]\tLoss: 392.2507\n",
      "Training Epoch: 30 [46336/49669]\tLoss: 385.5987\n",
      "Training Epoch: 30 [46400/49669]\tLoss: 410.7108\n",
      "Training Epoch: 30 [46464/49669]\tLoss: 359.8445\n",
      "Training Epoch: 30 [46528/49669]\tLoss: 407.9772\n",
      "Training Epoch: 30 [46592/49669]\tLoss: 387.4047\n",
      "Training Epoch: 30 [46656/49669]\tLoss: 400.3774\n",
      "Training Epoch: 30 [46720/49669]\tLoss: 396.1417\n",
      "Training Epoch: 30 [46784/49669]\tLoss: 384.5282\n",
      "Training Epoch: 30 [46848/49669]\tLoss: 382.9711\n",
      "Training Epoch: 30 [46912/49669]\tLoss: 412.5624\n",
      "Training Epoch: 30 [46976/49669]\tLoss: 391.8853\n",
      "Training Epoch: 30 [47040/49669]\tLoss: 401.7780\n",
      "Training Epoch: 30 [47104/49669]\tLoss: 383.7130\n",
      "Training Epoch: 30 [47168/49669]\tLoss: 394.5352\n",
      "Training Epoch: 30 [47232/49669]\tLoss: 403.6042\n",
      "Training Epoch: 30 [47296/49669]\tLoss: 393.8632\n",
      "Training Epoch: 30 [47360/49669]\tLoss: 385.8715\n",
      "Training Epoch: 30 [47424/49669]\tLoss: 397.6799\n",
      "Training Epoch: 30 [47488/49669]\tLoss: 431.1164\n",
      "Training Epoch: 30 [47552/49669]\tLoss: 408.4254\n",
      "Training Epoch: 30 [47616/49669]\tLoss: 392.3824\n",
      "Training Epoch: 30 [47680/49669]\tLoss: 420.3791\n",
      "Training Epoch: 30 [47744/49669]\tLoss: 402.0193\n",
      "Training Epoch: 30 [47808/49669]\tLoss: 388.3355\n",
      "Training Epoch: 30 [47872/49669]\tLoss: 403.2515\n",
      "Training Epoch: 30 [47936/49669]\tLoss: 401.4803\n",
      "Training Epoch: 30 [48000/49669]\tLoss: 408.5349\n",
      "Training Epoch: 30 [48064/49669]\tLoss: 372.0104\n",
      "Training Epoch: 30 [48128/49669]\tLoss: 401.9039\n",
      "Training Epoch: 30 [48192/49669]\tLoss: 406.0928\n",
      "Training Epoch: 30 [48256/49669]\tLoss: 392.4576\n",
      "Training Epoch: 30 [48320/49669]\tLoss: 415.7819\n",
      "Training Epoch: 30 [48384/49669]\tLoss: 422.6047\n",
      "Training Epoch: 30 [48448/49669]\tLoss: 400.4670\n",
      "Training Epoch: 30 [48512/49669]\tLoss: 386.9723\n",
      "Training Epoch: 30 [48576/49669]\tLoss: 383.1861\n",
      "Training Epoch: 30 [48640/49669]\tLoss: 366.9578\n",
      "Training Epoch: 30 [48704/49669]\tLoss: 415.6873\n",
      "Training Epoch: 30 [48768/49669]\tLoss: 384.1274\n",
      "Training Epoch: 30 [48832/49669]\tLoss: 424.7245\n",
      "Training Epoch: 30 [48896/49669]\tLoss: 397.5996\n",
      "Training Epoch: 30 [48960/49669]\tLoss: 398.4976\n",
      "Training Epoch: 30 [49024/49669]\tLoss: 394.3974\n",
      "Training Epoch: 30 [49088/49669]\tLoss: 410.5194\n",
      "Training Epoch: 30 [49152/49669]\tLoss: 427.7034\n",
      "Training Epoch: 30 [49216/49669]\tLoss: 405.5230\n",
      "Training Epoch: 30 [49280/49669]\tLoss: 409.5493\n",
      "Training Epoch: 30 [49344/49669]\tLoss: 385.3815\n",
      "Training Epoch: 30 [49408/49669]\tLoss: 407.3993\n",
      "Training Epoch: 30 [49472/49669]\tLoss: 411.0179\n",
      "Training Epoch: 30 [49536/49669]\tLoss: 417.3577\n",
      "Training Epoch: 30 [49600/49669]\tLoss: 412.0015\n",
      "Training Epoch: 30 [49664/49669]\tLoss: 436.2903\n",
      "Training Epoch: 30 [49669/49669]\tLoss: 545.1071\n",
      "Training Epoch: 30 [5519/5519]\tLoss: 537.3672\n",
      "Training Epoch: 31 [64/49669]\tLoss: 555.3923\n",
      "Training Epoch: 31 [128/49669]\tLoss: 689.3817\n",
      "Training Epoch: 31 [192/49669]\tLoss: 729.5605\n",
      "Training Epoch: 31 [256/49669]\tLoss: 475.4684\n",
      "Training Epoch: 31 [320/49669]\tLoss: 445.6361\n",
      "Training Epoch: 31 [384/49669]\tLoss: 669.0375\n",
      "Training Epoch: 31 [448/49669]\tLoss: 723.9206\n",
      "Training Epoch: 31 [512/49669]\tLoss: 495.6525\n",
      "Training Epoch: 31 [576/49669]\tLoss: 450.8774\n",
      "Training Epoch: 31 [640/49669]\tLoss: 578.1397\n",
      "Training Epoch: 31 [704/49669]\tLoss: 485.3683\n",
      "Training Epoch: 31 [768/49669]\tLoss: 420.2093\n",
      "Training Epoch: 31 [832/49669]\tLoss: 544.7322\n",
      "Training Epoch: 31 [896/49669]\tLoss: 490.8995\n",
      "Training Epoch: 31 [960/49669]\tLoss: 430.6089\n",
      "Training Epoch: 31 [1024/49669]\tLoss: 509.7986\n",
      "Training Epoch: 31 [1088/49669]\tLoss: 460.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31 [1152/49669]\tLoss: 425.2187\n",
      "Training Epoch: 31 [1216/49669]\tLoss: 505.1439\n",
      "Training Epoch: 31 [1280/49669]\tLoss: 443.9224\n",
      "Training Epoch: 31 [1344/49669]\tLoss: 410.7302\n",
      "Training Epoch: 31 [1408/49669]\tLoss: 468.8516\n",
      "Training Epoch: 31 [1472/49669]\tLoss: 441.6345\n",
      "Training Epoch: 31 [1536/49669]\tLoss: 416.0933\n",
      "Training Epoch: 31 [1600/49669]\tLoss: 428.9854\n",
      "Training Epoch: 31 [1664/49669]\tLoss: 418.7392\n",
      "Training Epoch: 31 [1728/49669]\tLoss: 415.9656\n",
      "Training Epoch: 31 [1792/49669]\tLoss: 463.5035\n",
      "Training Epoch: 31 [1856/49669]\tLoss: 430.1030\n",
      "Training Epoch: 31 [1920/49669]\tLoss: 370.4975\n",
      "Training Epoch: 31 [1984/49669]\tLoss: 408.7682\n",
      "Training Epoch: 31 [2048/49669]\tLoss: 392.2877\n",
      "Training Epoch: 31 [2112/49669]\tLoss: 416.8257\n",
      "Training Epoch: 31 [2176/49669]\tLoss: 402.4201\n",
      "Training Epoch: 31 [2240/49669]\tLoss: 410.8046\n",
      "Training Epoch: 31 [2304/49669]\tLoss: 433.4731\n",
      "Training Epoch: 31 [2368/49669]\tLoss: 428.3170\n",
      "Training Epoch: 31 [2432/49669]\tLoss: 399.6384\n",
      "Training Epoch: 31 [2496/49669]\tLoss: 406.1873\n",
      "Training Epoch: 31 [2560/49669]\tLoss: 372.9870\n",
      "Training Epoch: 31 [2624/49669]\tLoss: 407.0222\n",
      "Training Epoch: 31 [2688/49669]\tLoss: 396.4049\n",
      "Training Epoch: 31 [2752/49669]\tLoss: 416.9523\n",
      "Training Epoch: 31 [2816/49669]\tLoss: 404.9913\n",
      "Training Epoch: 31 [2880/49669]\tLoss: 400.3588\n",
      "Training Epoch: 31 [2944/49669]\tLoss: 414.9579\n",
      "Training Epoch: 31 [3008/49669]\tLoss: 377.8830\n",
      "Training Epoch: 31 [3072/49669]\tLoss: 420.4348\n",
      "Training Epoch: 31 [3136/49669]\tLoss: 402.9770\n",
      "Training Epoch: 31 [3200/49669]\tLoss: 402.3456\n",
      "Training Epoch: 31 [3264/49669]\tLoss: 412.8106\n",
      "Training Epoch: 31 [3328/49669]\tLoss: 395.6115\n",
      "Training Epoch: 31 [3392/49669]\tLoss: 415.9370\n",
      "Training Epoch: 31 [3456/49669]\tLoss: 401.8534\n",
      "Training Epoch: 31 [3520/49669]\tLoss: 385.5047\n",
      "Training Epoch: 31 [3584/49669]\tLoss: 385.6254\n",
      "Training Epoch: 31 [3648/49669]\tLoss: 388.0631\n",
      "Training Epoch: 31 [3712/49669]\tLoss: 384.6123\n",
      "Training Epoch: 31 [3776/49669]\tLoss: 443.8990\n",
      "Training Epoch: 31 [3840/49669]\tLoss: 408.7745\n",
      "Training Epoch: 31 [3904/49669]\tLoss: 403.3571\n",
      "Training Epoch: 31 [3968/49669]\tLoss: 370.7410\n",
      "Training Epoch: 31 [4032/49669]\tLoss: 412.4938\n",
      "Training Epoch: 31 [4096/49669]\tLoss: 388.2770\n",
      "Training Epoch: 31 [4160/49669]\tLoss: 415.9409\n",
      "Training Epoch: 31 [4224/49669]\tLoss: 414.0002\n",
      "Training Epoch: 31 [4288/49669]\tLoss: 368.2907\n",
      "Training Epoch: 31 [4352/49669]\tLoss: 353.8510\n",
      "Training Epoch: 31 [4416/49669]\tLoss: 393.9482\n",
      "Training Epoch: 31 [4480/49669]\tLoss: 390.7010\n",
      "Training Epoch: 31 [4544/49669]\tLoss: 422.7798\n",
      "Training Epoch: 31 [4608/49669]\tLoss: 396.2342\n",
      "Training Epoch: 31 [4672/49669]\tLoss: 393.3381\n",
      "Training Epoch: 31 [4736/49669]\tLoss: 377.3391\n",
      "Training Epoch: 31 [4800/49669]\tLoss: 420.7430\n",
      "Training Epoch: 31 [4864/49669]\tLoss: 398.4035\n",
      "Training Epoch: 31 [4928/49669]\tLoss: 400.9277\n",
      "Training Epoch: 31 [4992/49669]\tLoss: 444.1817\n",
      "Training Epoch: 31 [5056/49669]\tLoss: 406.9546\n",
      "Training Epoch: 31 [5120/49669]\tLoss: 385.0877\n",
      "Training Epoch: 31 [5184/49669]\tLoss: 400.2371\n",
      "Training Epoch: 31 [5248/49669]\tLoss: 377.7499\n",
      "Training Epoch: 31 [5312/49669]\tLoss: 400.9637\n",
      "Training Epoch: 31 [5376/49669]\tLoss: 407.4384\n",
      "Training Epoch: 31 [5440/49669]\tLoss: 411.2847\n",
      "Training Epoch: 31 [5504/49669]\tLoss: 390.6481\n",
      "Training Epoch: 31 [5568/49669]\tLoss: 394.8878\n",
      "Training Epoch: 31 [5632/49669]\tLoss: 411.7422\n",
      "Training Epoch: 31 [5696/49669]\tLoss: 388.4005\n",
      "Training Epoch: 31 [5760/49669]\tLoss: 401.5167\n",
      "Training Epoch: 31 [5824/49669]\tLoss: 379.5213\n",
      "Training Epoch: 31 [5888/49669]\tLoss: 402.6645\n",
      "Training Epoch: 31 [5952/49669]\tLoss: 387.3581\n",
      "Training Epoch: 31 [6016/49669]\tLoss: 369.4469\n",
      "Training Epoch: 31 [6080/49669]\tLoss: 401.1845\n",
      "Training Epoch: 31 [6144/49669]\tLoss: 385.3563\n",
      "Training Epoch: 31 [6208/49669]\tLoss: 408.5386\n",
      "Training Epoch: 31 [6272/49669]\tLoss: 384.9122\n",
      "Training Epoch: 31 [6336/49669]\tLoss: 409.7078\n",
      "Training Epoch: 31 [6400/49669]\tLoss: 417.7568\n",
      "Training Epoch: 31 [6464/49669]\tLoss: 413.9698\n",
      "Training Epoch: 31 [6528/49669]\tLoss: 439.6804\n",
      "Training Epoch: 31 [6592/49669]\tLoss: 410.0889\n",
      "Training Epoch: 31 [6656/49669]\tLoss: 421.7783\n",
      "Training Epoch: 31 [6720/49669]\tLoss: 406.0898\n",
      "Training Epoch: 31 [6784/49669]\tLoss: 388.4582\n",
      "Training Epoch: 31 [6848/49669]\tLoss: 397.9011\n",
      "Training Epoch: 31 [6912/49669]\tLoss: 401.1649\n",
      "Training Epoch: 31 [6976/49669]\tLoss: 390.6724\n",
      "Training Epoch: 31 [7040/49669]\tLoss: 411.1870\n",
      "Training Epoch: 31 [7104/49669]\tLoss: 404.4195\n",
      "Training Epoch: 31 [7168/49669]\tLoss: 380.9736\n",
      "Training Epoch: 31 [7232/49669]\tLoss: 400.9638\n",
      "Training Epoch: 31 [7296/49669]\tLoss: 393.2355\n",
      "Training Epoch: 31 [7360/49669]\tLoss: 415.3416\n",
      "Training Epoch: 31 [7424/49669]\tLoss: 414.9957\n",
      "Training Epoch: 31 [7488/49669]\tLoss: 406.7315\n",
      "Training Epoch: 31 [7552/49669]\tLoss: 362.3921\n",
      "Training Epoch: 31 [7616/49669]\tLoss: 411.0849\n",
      "Training Epoch: 31 [7680/49669]\tLoss: 394.9210\n",
      "Training Epoch: 31 [7744/49669]\tLoss: 407.4320\n",
      "Training Epoch: 31 [7808/49669]\tLoss: 399.3824\n",
      "Training Epoch: 31 [7872/49669]\tLoss: 398.4789\n",
      "Training Epoch: 31 [7936/49669]\tLoss: 406.6126\n",
      "Training Epoch: 31 [8000/49669]\tLoss: 402.9479\n",
      "Training Epoch: 31 [8064/49669]\tLoss: 371.6773\n",
      "Training Epoch: 31 [8128/49669]\tLoss: 406.7094\n",
      "Training Epoch: 31 [8192/49669]\tLoss: 411.3113\n",
      "Training Epoch: 31 [8256/49669]\tLoss: 371.0110\n",
      "Training Epoch: 31 [8320/49669]\tLoss: 394.5699\n",
      "Training Epoch: 31 [8384/49669]\tLoss: 426.6805\n",
      "Training Epoch: 31 [8448/49669]\tLoss: 376.8660\n",
      "Training Epoch: 31 [8512/49669]\tLoss: 413.7193\n",
      "Training Epoch: 31 [8576/49669]\tLoss: 379.0559\n",
      "Training Epoch: 31 [8640/49669]\tLoss: 415.9233\n",
      "Training Epoch: 31 [8704/49669]\tLoss: 390.3700\n",
      "Training Epoch: 31 [8768/49669]\tLoss: 385.1858\n",
      "Training Epoch: 31 [8832/49669]\tLoss: 386.9868\n",
      "Training Epoch: 31 [8896/49669]\tLoss: 386.8024\n",
      "Training Epoch: 31 [8960/49669]\tLoss: 402.3761\n",
      "Training Epoch: 31 [9024/49669]\tLoss: 410.2326\n",
      "Training Epoch: 31 [9088/49669]\tLoss: 401.2836\n",
      "Training Epoch: 31 [9152/49669]\tLoss: 385.0750\n",
      "Training Epoch: 31 [9216/49669]\tLoss: 418.3805\n",
      "Training Epoch: 31 [9280/49669]\tLoss: 428.7531\n",
      "Training Epoch: 31 [9344/49669]\tLoss: 387.6148\n",
      "Training Epoch: 31 [9408/49669]\tLoss: 382.9009\n",
      "Training Epoch: 31 [9472/49669]\tLoss: 397.7046\n",
      "Training Epoch: 31 [9536/49669]\tLoss: 424.8736\n",
      "Training Epoch: 31 [9600/49669]\tLoss: 366.4411\n",
      "Training Epoch: 31 [9664/49669]\tLoss: 382.8298\n",
      "Training Epoch: 31 [9728/49669]\tLoss: 404.2119\n",
      "Training Epoch: 31 [9792/49669]\tLoss: 418.7284\n",
      "Training Epoch: 31 [9856/49669]\tLoss: 398.0479\n",
      "Training Epoch: 31 [9920/49669]\tLoss: 435.3438\n",
      "Training Epoch: 31 [9984/49669]\tLoss: 411.2255\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
