{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.ReconstructionTrainingDataset import CleanSoundsDataset\n",
    "from data.ReconstructionTrainingDataset import NoisySoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_name = 'pnet'\n",
    "_train_datafile = 'clean_reconstruction_training_set'\n",
    "SoundsDataset = CleanSoundsDataset\n",
    "dset_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4755a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = 'neg9'\n",
    "pnet_name = 'pnet_snr-9'\n",
    "_train_datafile = 'hyperparameter_pooled_training_dataset_random_order_noNulls'\n",
    "SoundsDataset = NoisySoundsDataset\n",
    "dset_kwargs = {'snr': snr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96589b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a437651",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfdc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "lr = 1E-5\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}{_train_datafile}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9766a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  5 13:57:01 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:21:00.0 Off |                  N/A |\r\n",
      "| 27%   26C    P8     2W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d748a",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b087e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae18933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6839214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23a4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779893ac",
   "metadata": {},
   "source": [
    "# Set up train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc69b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SoundsDataset(\n",
    "    train_datafile, subset=.9, **dset_kwargs)\n",
    "test_dataset = SoundsDataset(\n",
    "    train_datafile, subset=.9,\n",
    "    train = False, **dset_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a14c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026ae3a",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b449fc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/13500]\tLoss: 24215.2930\n",
      "Training Epoch: 1 [100/13500]\tLoss: 24014.3848\n",
      "Training Epoch: 1 [150/13500]\tLoss: 23833.8438\n",
      "Training Epoch: 1 [200/13500]\tLoss: 23284.7617\n",
      "Training Epoch: 1 [250/13500]\tLoss: 23395.9492\n",
      "Training Epoch: 1 [300/13500]\tLoss: 22806.3750\n",
      "Training Epoch: 1 [350/13500]\tLoss: 23726.7734\n",
      "Training Epoch: 1 [400/13500]\tLoss: 23776.9043\n",
      "Training Epoch: 1 [450/13500]\tLoss: 23178.8516\n",
      "Training Epoch: 1 [500/13500]\tLoss: 22711.7578\n",
      "Training Epoch: 1 [550/13500]\tLoss: 22917.3652\n",
      "Training Epoch: 1 [600/13500]\tLoss: 21668.8301\n",
      "Training Epoch: 1 [650/13500]\tLoss: 22736.4512\n",
      "Training Epoch: 1 [700/13500]\tLoss: 22905.9160\n",
      "Training Epoch: 1 [750/13500]\tLoss: 21949.5234\n",
      "Training Epoch: 1 [800/13500]\tLoss: 22526.7598\n",
      "Training Epoch: 1 [850/13500]\tLoss: 22799.1035\n",
      "Training Epoch: 1 [900/13500]\tLoss: 21909.0469\n",
      "Training Epoch: 1 [950/13500]\tLoss: 22144.9492\n",
      "Training Epoch: 1 [1000/13500]\tLoss: 22784.3203\n",
      "Training Epoch: 1 [1050/13500]\tLoss: 21287.2109\n",
      "Training Epoch: 1 [1100/13500]\tLoss: 22763.5234\n",
      "Training Epoch: 1 [1150/13500]\tLoss: 21981.6465\n",
      "Training Epoch: 1 [1200/13500]\tLoss: 21890.6055\n",
      "Training Epoch: 1 [1250/13500]\tLoss: 21449.0000\n",
      "Training Epoch: 1 [1300/13500]\tLoss: 21864.9180\n",
      "Training Epoch: 1 [1350/13500]\tLoss: 21155.0977\n",
      "Training Epoch: 1 [1400/13500]\tLoss: 21018.1523\n",
      "Training Epoch: 1 [1450/13500]\tLoss: 21399.1680\n",
      "Training Epoch: 1 [1500/13500]\tLoss: 21238.8008\n",
      "Training Epoch: 1 [1550/13500]\tLoss: 21337.3164\n",
      "Training Epoch: 1 [1600/13500]\tLoss: 21715.1250\n",
      "Training Epoch: 1 [1650/13500]\tLoss: 21484.6191\n",
      "Training Epoch: 1 [1700/13500]\tLoss: 20719.0898\n",
      "Training Epoch: 1 [1750/13500]\tLoss: 21419.3906\n",
      "Training Epoch: 1 [1800/13500]\tLoss: 20461.8613\n",
      "Training Epoch: 1 [1850/13500]\tLoss: 21604.4863\n",
      "Training Epoch: 1 [1900/13500]\tLoss: 20637.1738\n",
      "Training Epoch: 1 [1950/13500]\tLoss: 20805.0879\n",
      "Training Epoch: 1 [2000/13500]\tLoss: 20086.4121\n",
      "Training Epoch: 1 [2050/13500]\tLoss: 19942.2637\n",
      "Training Epoch: 1 [2100/13500]\tLoss: 21131.8750\n",
      "Training Epoch: 1 [2150/13500]\tLoss: 20507.8047\n",
      "Training Epoch: 1 [2200/13500]\tLoss: 19977.6777\n",
      "Training Epoch: 1 [2250/13500]\tLoss: 19946.7305\n",
      "Training Epoch: 1 [2300/13500]\tLoss: 19838.1934\n",
      "Training Epoch: 1 [2350/13500]\tLoss: 20057.9980\n",
      "Training Epoch: 1 [2400/13500]\tLoss: 19798.5762\n",
      "Training Epoch: 1 [2450/13500]\tLoss: 20541.6934\n",
      "Training Epoch: 1 [2500/13500]\tLoss: 19926.6816\n",
      "Training Epoch: 1 [2550/13500]\tLoss: 19468.2715\n",
      "Training Epoch: 1 [2600/13500]\tLoss: 20014.1914\n",
      "Training Epoch: 1 [2650/13500]\tLoss: 19821.6230\n",
      "Training Epoch: 1 [2700/13500]\tLoss: 18902.6094\n",
      "Training Epoch: 1 [2750/13500]\tLoss: 19201.1074\n",
      "Training Epoch: 1 [2800/13500]\tLoss: 19360.7598\n",
      "Training Epoch: 1 [2850/13500]\tLoss: 19467.7910\n",
      "Training Epoch: 1 [2900/13500]\tLoss: 18589.2109\n",
      "Training Epoch: 1 [2950/13500]\tLoss: 19130.6816\n",
      "Training Epoch: 1 [3000/13500]\tLoss: 18969.1230\n",
      "Training Epoch: 1 [3050/13500]\tLoss: 19191.8340\n",
      "Training Epoch: 1 [3100/13500]\tLoss: 18839.3359\n",
      "Training Epoch: 1 [3150/13500]\tLoss: 18068.5801\n",
      "Training Epoch: 1 [3200/13500]\tLoss: 19031.1855\n",
      "Training Epoch: 1 [3250/13500]\tLoss: 18185.2500\n",
      "Training Epoch: 1 [3300/13500]\tLoss: 18490.9648\n",
      "Training Epoch: 1 [3350/13500]\tLoss: 18315.3535\n",
      "Training Epoch: 1 [3400/13500]\tLoss: 18661.8965\n",
      "Training Epoch: 1 [3450/13500]\tLoss: 18234.8809\n",
      "Training Epoch: 1 [3500/13500]\tLoss: 18175.3555\n",
      "Training Epoch: 1 [3550/13500]\tLoss: 18025.5742\n",
      "Training Epoch: 1 [3600/13500]\tLoss: 18324.7793\n",
      "Training Epoch: 1 [3650/13500]\tLoss: 18224.7344\n",
      "Training Epoch: 1 [3700/13500]\tLoss: 18311.5039\n",
      "Training Epoch: 1 [3750/13500]\tLoss: 17225.0137\n",
      "Training Epoch: 1 [3800/13500]\tLoss: 17916.5098\n",
      "Training Epoch: 1 [3850/13500]\tLoss: 17350.7676\n",
      "Training Epoch: 1 [3900/13500]\tLoss: 17390.0156\n",
      "Training Epoch: 1 [3950/13500]\tLoss: 17710.2090\n",
      "Training Epoch: 1 [4000/13500]\tLoss: 17987.3281\n",
      "Training Epoch: 1 [4050/13500]\tLoss: 17185.6328\n",
      "Training Epoch: 1 [4100/13500]\tLoss: 17163.3086\n",
      "Training Epoch: 1 [4150/13500]\tLoss: 17856.8379\n",
      "Training Epoch: 1 [4200/13500]\tLoss: 17135.8301\n",
      "Training Epoch: 1 [4250/13500]\tLoss: 17879.4980\n",
      "Training Epoch: 1 [4300/13500]\tLoss: 17867.4121\n",
      "Training Epoch: 1 [4350/13500]\tLoss: 17186.7148\n",
      "Training Epoch: 1 [4400/13500]\tLoss: 16917.9629\n",
      "Training Epoch: 1 [4450/13500]\tLoss: 17522.7715\n",
      "Training Epoch: 1 [4500/13500]\tLoss: 16339.7080\n",
      "Training Epoch: 1 [4550/13500]\tLoss: 16867.0879\n",
      "Training Epoch: 1 [4600/13500]\tLoss: 16192.3818\n",
      "Training Epoch: 1 [4650/13500]\tLoss: 17112.3125\n",
      "Training Epoch: 1 [4700/13500]\tLoss: 16876.2578\n",
      "Training Epoch: 1 [4750/13500]\tLoss: 16635.4785\n",
      "Training Epoch: 1 [4800/13500]\tLoss: 16561.9551\n",
      "Training Epoch: 1 [4850/13500]\tLoss: 16117.6572\n",
      "Training Epoch: 1 [4900/13500]\tLoss: 16391.3770\n",
      "Training Epoch: 1 [4950/13500]\tLoss: 16179.9131\n",
      "Training Epoch: 1 [5000/13500]\tLoss: 16416.8340\n",
      "Training Epoch: 1 [5050/13500]\tLoss: 16192.9160\n",
      "Training Epoch: 1 [5100/13500]\tLoss: 15835.2979\n",
      "Training Epoch: 1 [5150/13500]\tLoss: 15604.4717\n",
      "Training Epoch: 1 [5200/13500]\tLoss: 15411.6650\n",
      "Training Epoch: 1 [5250/13500]\tLoss: 16046.3545\n",
      "Training Epoch: 1 [5300/13500]\tLoss: 15280.0508\n",
      "Training Epoch: 1 [5350/13500]\tLoss: 16252.6631\n",
      "Training Epoch: 1 [5400/13500]\tLoss: 15525.2773\n",
      "Training Epoch: 1 [5450/13500]\tLoss: 15825.4561\n",
      "Training Epoch: 1 [5500/13500]\tLoss: 15815.7539\n",
      "Training Epoch: 1 [5550/13500]\tLoss: 15419.0996\n",
      "Training Epoch: 1 [5600/13500]\tLoss: 15847.3809\n",
      "Training Epoch: 1 [5650/13500]\tLoss: 15548.5342\n",
      "Training Epoch: 1 [5700/13500]\tLoss: 16115.6094\n",
      "Training Epoch: 1 [5750/13500]\tLoss: 15425.0615\n",
      "Training Epoch: 1 [5800/13500]\tLoss: 15073.6250\n",
      "Training Epoch: 1 [5850/13500]\tLoss: 15744.1387\n",
      "Training Epoch: 1 [5900/13500]\tLoss: 15212.4434\n",
      "Training Epoch: 1 [5950/13500]\tLoss: 15662.1357\n",
      "Training Epoch: 1 [6000/13500]\tLoss: 15127.2666\n",
      "Training Epoch: 1 [6050/13500]\tLoss: 14483.7549\n",
      "Training Epoch: 1 [6100/13500]\tLoss: 15262.0557\n",
      "Training Epoch: 1 [6150/13500]\tLoss: 14765.8262\n",
      "Training Epoch: 1 [6200/13500]\tLoss: 15337.6543\n",
      "Training Epoch: 1 [6250/13500]\tLoss: 14688.4922\n",
      "Training Epoch: 1 [6300/13500]\tLoss: 14774.1221\n",
      "Training Epoch: 1 [6350/13500]\tLoss: 15011.0566\n",
      "Training Epoch: 1 [6400/13500]\tLoss: 14482.8018\n",
      "Training Epoch: 1 [6450/13500]\tLoss: 14476.7852\n",
      "Training Epoch: 1 [6500/13500]\tLoss: 14868.0742\n",
      "Training Epoch: 1 [6550/13500]\tLoss: 14567.9541\n",
      "Training Epoch: 1 [6600/13500]\tLoss: 15063.0117\n",
      "Training Epoch: 1 [6650/13500]\tLoss: 14556.5059\n",
      "Training Epoch: 1 [6700/13500]\tLoss: 14285.3994\n",
      "Training Epoch: 1 [6750/13500]\tLoss: 14157.8174\n",
      "Training Epoch: 1 [6800/13500]\tLoss: 14611.6143\n",
      "Training Epoch: 1 [6850/13500]\tLoss: 13964.6846\n",
      "Training Epoch: 1 [6900/13500]\tLoss: 13989.4463\n",
      "Training Epoch: 1 [6950/13500]\tLoss: 14221.8018\n",
      "Training Epoch: 1 [7000/13500]\tLoss: 14908.2627\n",
      "Training Epoch: 1 [7050/13500]\tLoss: 14721.2510\n",
      "Training Epoch: 1 [7100/13500]\tLoss: 14236.5674\n",
      "Training Epoch: 1 [7150/13500]\tLoss: 14277.0010\n",
      "Training Epoch: 1 [7200/13500]\tLoss: 13458.0986\n",
      "Training Epoch: 1 [7250/13500]\tLoss: 13830.5723\n",
      "Training Epoch: 1 [7300/13500]\tLoss: 13852.2773\n",
      "Training Epoch: 1 [7350/13500]\tLoss: 14457.6035\n",
      "Training Epoch: 1 [7400/13500]\tLoss: 13626.6729\n",
      "Training Epoch: 1 [7450/13500]\tLoss: 13481.5244\n",
      "Training Epoch: 1 [7500/13500]\tLoss: 13805.2607\n",
      "Training Epoch: 1 [7550/13500]\tLoss: 13743.5010\n",
      "Training Epoch: 1 [7600/13500]\tLoss: 13900.2881\n",
      "Training Epoch: 1 [7650/13500]\tLoss: 13723.7148\n",
      "Training Epoch: 1 [7700/13500]\tLoss: 13518.2373\n",
      "Training Epoch: 1 [7750/13500]\tLoss: 13564.0068\n",
      "Training Epoch: 1 [7800/13500]\tLoss: 13663.0283\n",
      "Training Epoch: 1 [7850/13500]\tLoss: 13776.3760\n",
      "Training Epoch: 1 [7900/13500]\tLoss: 13792.6738\n",
      "Training Epoch: 1 [7950/13500]\tLoss: 13629.7402\n",
      "Training Epoch: 1 [8000/13500]\tLoss: 13601.5312\n",
      "Training Epoch: 1 [8050/13500]\tLoss: 12956.6436\n",
      "Training Epoch: 1 [8100/13500]\tLoss: 13504.2656\n",
      "Training Epoch: 1 [8150/13500]\tLoss: 12894.0342\n",
      "Training Epoch: 1 [8200/13500]\tLoss: 13521.1836\n",
      "Training Epoch: 1 [8250/13500]\tLoss: 13405.5732\n",
      "Training Epoch: 1 [8300/13500]\tLoss: 12908.7803\n",
      "Training Epoch: 1 [8350/13500]\tLoss: 13227.6240\n",
      "Training Epoch: 1 [8400/13500]\tLoss: 13463.4678\n",
      "Training Epoch: 1 [8450/13500]\tLoss: 13020.4326\n",
      "Training Epoch: 1 [8500/13500]\tLoss: 13135.7354\n",
      "Training Epoch: 1 [8550/13500]\tLoss: 12739.8066\n",
      "Training Epoch: 1 [8600/13500]\tLoss: 12720.6084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8650/13500]\tLoss: 12791.5938\n",
      "Training Epoch: 1 [8700/13500]\tLoss: 12535.9443\n",
      "Training Epoch: 1 [8750/13500]\tLoss: 12518.3164\n",
      "Training Epoch: 1 [8800/13500]\tLoss: 12345.1836\n",
      "Training Epoch: 1 [8850/13500]\tLoss: 13377.4355\n",
      "Training Epoch: 1 [8900/13500]\tLoss: 12303.7373\n",
      "Training Epoch: 1 [8950/13500]\tLoss: 12195.7910\n",
      "Training Epoch: 1 [9000/13500]\tLoss: 11824.3232\n",
      "Training Epoch: 1 [9050/13500]\tLoss: 12457.0791\n",
      "Training Epoch: 1 [9100/13500]\tLoss: 12547.8242\n",
      "Training Epoch: 1 [9150/13500]\tLoss: 12556.1084\n",
      "Training Epoch: 1 [9200/13500]\tLoss: 11887.7227\n",
      "Training Epoch: 1 [9250/13500]\tLoss: 12426.8555\n",
      "Training Epoch: 1 [9300/13500]\tLoss: 12023.7363\n",
      "Training Epoch: 1 [9350/13500]\tLoss: 12296.0537\n",
      "Training Epoch: 1 [9400/13500]\tLoss: 12364.1318\n",
      "Training Epoch: 1 [9450/13500]\tLoss: 11804.3135\n",
      "Training Epoch: 1 [9500/13500]\tLoss: 12186.6250\n",
      "Training Epoch: 1 [9550/13500]\tLoss: 11947.7441\n",
      "Training Epoch: 1 [9600/13500]\tLoss: 12229.6523\n",
      "Training Epoch: 1 [9650/13500]\tLoss: 12192.5811\n",
      "Training Epoch: 1 [9700/13500]\tLoss: 12299.6006\n",
      "Training Epoch: 1 [9750/13500]\tLoss: 11892.3594\n",
      "Training Epoch: 1 [9800/13500]\tLoss: 11766.6182\n",
      "Training Epoch: 1 [9850/13500]\tLoss: 11796.5557\n",
      "Training Epoch: 1 [9900/13500]\tLoss: 11783.3037\n",
      "Training Epoch: 1 [9950/13500]\tLoss: 12467.7109\n",
      "Training Epoch: 1 [10000/13500]\tLoss: 12061.0176\n",
      "Training Epoch: 1 [10050/13500]\tLoss: 11711.5879\n",
      "Training Epoch: 1 [10100/13500]\tLoss: 11840.6572\n",
      "Training Epoch: 1 [10150/13500]\tLoss: 11659.2988\n",
      "Training Epoch: 1 [10200/13500]\tLoss: 11546.2578\n",
      "Training Epoch: 1 [10250/13500]\tLoss: 11831.3164\n",
      "Training Epoch: 1 [10300/13500]\tLoss: 11849.5400\n",
      "Training Epoch: 1 [10350/13500]\tLoss: 11957.6973\n",
      "Training Epoch: 1 [10400/13500]\tLoss: 11608.9736\n",
      "Training Epoch: 1 [10450/13500]\tLoss: 11920.4863\n",
      "Training Epoch: 1 [10500/13500]\tLoss: 11057.4043\n",
      "Training Epoch: 1 [10550/13500]\tLoss: 11794.9844\n",
      "Training Epoch: 1 [10600/13500]\tLoss: 11365.6152\n",
      "Training Epoch: 1 [10650/13500]\tLoss: 11539.6318\n",
      "Training Epoch: 1 [10700/13500]\tLoss: 11344.6436\n",
      "Training Epoch: 1 [10750/13500]\tLoss: 11587.3887\n",
      "Training Epoch: 1 [10800/13500]\tLoss: 11643.8711\n",
      "Training Epoch: 1 [10850/13500]\tLoss: 11404.7402\n",
      "Training Epoch: 1 [10900/13500]\tLoss: 11043.5137\n",
      "Training Epoch: 1 [10950/13500]\tLoss: 11135.2471\n",
      "Training Epoch: 1 [11000/13500]\tLoss: 11470.5176\n",
      "Training Epoch: 1 [11050/13500]\tLoss: 11013.6133\n",
      "Training Epoch: 1 [11100/13500]\tLoss: 10793.3613\n",
      "Training Epoch: 1 [11150/13500]\tLoss: 10901.7637\n",
      "Training Epoch: 1 [11200/13500]\tLoss: 11209.3604\n",
      "Training Epoch: 1 [11250/13500]\tLoss: 10806.0332\n",
      "Training Epoch: 1 [11300/13500]\tLoss: 11168.9248\n",
      "Training Epoch: 1 [11350/13500]\tLoss: 10818.0049\n",
      "Training Epoch: 1 [11400/13500]\tLoss: 10849.3447\n",
      "Training Epoch: 1 [11450/13500]\tLoss: 10916.1270\n",
      "Training Epoch: 1 [11500/13500]\tLoss: 11045.7314\n",
      "Training Epoch: 1 [11550/13500]\tLoss: 10798.8740\n",
      "Training Epoch: 1 [11600/13500]\tLoss: 10844.9531\n",
      "Training Epoch: 1 [11650/13500]\tLoss: 10815.7256\n",
      "Training Epoch: 1 [11700/13500]\tLoss: 10643.6123\n",
      "Training Epoch: 1 [11750/13500]\tLoss: 10618.6816\n",
      "Training Epoch: 1 [11800/13500]\tLoss: 10508.8066\n",
      "Training Epoch: 1 [11850/13500]\tLoss: 10530.0127\n",
      "Training Epoch: 1 [11900/13500]\tLoss: 10548.3945\n",
      "Training Epoch: 1 [11950/13500]\tLoss: 10678.0176\n",
      "Training Epoch: 1 [12000/13500]\tLoss: 11017.5840\n",
      "Training Epoch: 1 [12050/13500]\tLoss: 10472.9883\n",
      "Training Epoch: 1 [12100/13500]\tLoss: 10584.7207\n",
      "Training Epoch: 1 [12150/13500]\tLoss: 10136.3047\n",
      "Training Epoch: 1 [12200/13500]\tLoss: 10235.8223\n",
      "Training Epoch: 1 [12250/13500]\tLoss: 10286.8672\n",
      "Training Epoch: 1 [12300/13500]\tLoss: 10203.0840\n",
      "Training Epoch: 1 [12350/13500]\tLoss: 10423.2754\n",
      "Training Epoch: 1 [12400/13500]\tLoss: 10369.2705\n",
      "Training Epoch: 1 [12450/13500]\tLoss: 10690.4385\n",
      "Training Epoch: 1 [12500/13500]\tLoss: 10253.3789\n",
      "Training Epoch: 1 [12550/13500]\tLoss: 10281.8623\n",
      "Training Epoch: 1 [12600/13500]\tLoss: 10675.5479\n",
      "Training Epoch: 1 [12650/13500]\tLoss: 10385.9502\n",
      "Training Epoch: 1 [12700/13500]\tLoss: 10592.8057\n",
      "Training Epoch: 1 [12750/13500]\tLoss: 10145.1338\n",
      "Training Epoch: 1 [12800/13500]\tLoss: 10323.7480\n",
      "Training Epoch: 1 [12850/13500]\tLoss: 10750.7803\n",
      "Training Epoch: 1 [12900/13500]\tLoss: 10071.5371\n",
      "Training Epoch: 1 [12950/13500]\tLoss: 10198.3203\n",
      "Training Epoch: 1 [13000/13500]\tLoss: 10362.2471\n",
      "Training Epoch: 1 [13050/13500]\tLoss: 9990.1992\n",
      "Training Epoch: 1 [13100/13500]\tLoss: 10334.4170\n",
      "Training Epoch: 1 [13150/13500]\tLoss: 9762.6182\n",
      "Training Epoch: 1 [13200/13500]\tLoss: 9999.1934\n",
      "Training Epoch: 1 [13250/13500]\tLoss: 10552.3320\n",
      "Training Epoch: 1 [13300/13500]\tLoss: 10153.2520\n",
      "Training Epoch: 1 [13350/13500]\tLoss: 10006.8896\n",
      "Training Epoch: 1 [13400/13500]\tLoss: 9951.9668\n",
      "Training Epoch: 1 [13450/13500]\tLoss: 9747.6309\n",
      "Training Epoch: 1 [13500/13500]\tLoss: 10451.8818\n",
      "Training Epoch: 1 [1499/1499]\tLoss: 10122.3402\n",
      "Training Epoch: 2 [50/13500]\tLoss: 9869.3623\n",
      "Training Epoch: 2 [100/13500]\tLoss: 9896.8887\n",
      "Training Epoch: 2 [150/13500]\tLoss: 9762.9600\n",
      "Training Epoch: 2 [200/13500]\tLoss: 9797.5059\n",
      "Training Epoch: 2 [250/13500]\tLoss: 9829.9941\n",
      "Training Epoch: 2 [300/13500]\tLoss: 9633.9150\n",
      "Training Epoch: 2 [350/13500]\tLoss: 10291.3877\n",
      "Training Epoch: 2 [400/13500]\tLoss: 10333.2334\n",
      "Training Epoch: 2 [450/13500]\tLoss: 9965.5459\n",
      "Training Epoch: 2 [500/13500]\tLoss: 9657.1416\n",
      "Training Epoch: 2 [550/13500]\tLoss: 9946.6602\n",
      "Training Epoch: 2 [600/13500]\tLoss: 9235.0820\n",
      "Training Epoch: 2 [650/13500]\tLoss: 9778.9541\n",
      "Training Epoch: 2 [700/13500]\tLoss: 9965.0068\n",
      "Training Epoch: 2 [750/13500]\tLoss: 9500.2676\n",
      "Training Epoch: 2 [800/13500]\tLoss: 9743.9668\n",
      "Training Epoch: 2 [850/13500]\tLoss: 10014.9434\n",
      "Training Epoch: 2 [900/13500]\tLoss: 9513.7129\n",
      "Training Epoch: 2 [950/13500]\tLoss: 9643.7227\n",
      "Training Epoch: 2 [1000/13500]\tLoss: 10201.6992\n",
      "Training Epoch: 2 [1050/13500]\tLoss: 9241.3281\n",
      "Training Epoch: 2 [1100/13500]\tLoss: 10231.1465\n",
      "Training Epoch: 2 [1150/13500]\tLoss: 9739.7432\n",
      "Training Epoch: 2 [1200/13500]\tLoss: 9691.9111\n",
      "Training Epoch: 2 [1250/13500]\tLoss: 9557.2354\n",
      "Training Epoch: 2 [1300/13500]\tLoss: 9770.8662\n",
      "Training Epoch: 2 [1350/13500]\tLoss: 9330.1211\n",
      "Training Epoch: 2 [1400/13500]\tLoss: 9325.9658\n",
      "Training Epoch: 2 [1450/13500]\tLoss: 9708.8145\n",
      "Training Epoch: 2 [1500/13500]\tLoss: 9620.2705\n",
      "Training Epoch: 2 [1550/13500]\tLoss: 9674.4473\n",
      "Training Epoch: 2 [1600/13500]\tLoss: 9947.0957\n",
      "Training Epoch: 2 [1650/13500]\tLoss: 9817.3604\n",
      "Training Epoch: 2 [1700/13500]\tLoss: 9333.0967\n",
      "Training Epoch: 2 [1750/13500]\tLoss: 10032.3779\n",
      "Training Epoch: 2 [1800/13500]\tLoss: 9232.2373\n",
      "Training Epoch: 2 [1850/13500]\tLoss: 10000.0332\n",
      "Training Epoch: 2 [1900/13500]\tLoss: 9414.2715\n",
      "Training Epoch: 2 [1950/13500]\tLoss: 9670.4336\n",
      "Training Epoch: 2 [2000/13500]\tLoss: 9115.6523\n",
      "Training Epoch: 2 [2050/13500]\tLoss: 9068.4688\n",
      "Training Epoch: 2 [2100/13500]\tLoss: 9901.0283\n",
      "Training Epoch: 2 [2150/13500]\tLoss: 9580.1240\n",
      "Training Epoch: 2 [2200/13500]\tLoss: 9149.4805\n",
      "Training Epoch: 2 [2250/13500]\tLoss: 9153.9951\n",
      "Training Epoch: 2 [2300/13500]\tLoss: 9176.0781\n",
      "Training Epoch: 2 [2350/13500]\tLoss: 9350.4248\n",
      "Training Epoch: 2 [2400/13500]\tLoss: 9212.4814\n",
      "Training Epoch: 2 [2450/13500]\tLoss: 9796.4277\n",
      "Training Epoch: 2 [2500/13500]\tLoss: 9290.1494\n",
      "Training Epoch: 2 [2550/13500]\tLoss: 9026.1191\n",
      "Training Epoch: 2 [2600/13500]\tLoss: 9508.1191\n",
      "Training Epoch: 2 [2650/13500]\tLoss: 9497.7285\n",
      "Training Epoch: 2 [2700/13500]\tLoss: 8840.6367\n",
      "Training Epoch: 2 [2750/13500]\tLoss: 9110.3262\n",
      "Training Epoch: 2 [2800/13500]\tLoss: 9264.4932\n",
      "Training Epoch: 2 [2850/13500]\tLoss: 9357.7979\n",
      "Training Epoch: 2 [2900/13500]\tLoss: 8803.8398\n",
      "Training Epoch: 2 [2950/13500]\tLoss: 9159.2734\n",
      "Training Epoch: 2 [3000/13500]\tLoss: 9010.5879\n",
      "Training Epoch: 2 [3050/13500]\tLoss: 9325.6377\n",
      "Training Epoch: 2 [3100/13500]\tLoss: 9073.1260\n",
      "Training Epoch: 2 [3150/13500]\tLoss: 8504.7734\n",
      "Training Epoch: 2 [3200/13500]\tLoss: 9266.8506\n",
      "Training Epoch: 2 [3250/13500]\tLoss: 8651.4795\n",
      "Training Epoch: 2 [3300/13500]\tLoss: 9013.1953\n",
      "Training Epoch: 2 [3350/13500]\tLoss: 8893.2715\n",
      "Training Epoch: 2 [3400/13500]\tLoss: 9127.3750\n",
      "Training Epoch: 2 [3450/13500]\tLoss: 8893.8975\n",
      "Training Epoch: 2 [3500/13500]\tLoss: 8948.0527\n",
      "Training Epoch: 2 [3550/13500]\tLoss: 8778.3545\n",
      "Training Epoch: 2 [3600/13500]\tLoss: 8994.2061\n",
      "Training Epoch: 2 [3650/13500]\tLoss: 9118.1318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [3700/13500]\tLoss: 9228.2910\n",
      "Training Epoch: 2 [3750/13500]\tLoss: 8462.7822\n",
      "Training Epoch: 2 [3800/13500]\tLoss: 8899.3096\n",
      "Training Epoch: 2 [3850/13500]\tLoss: 8733.8193\n",
      "Training Epoch: 2 [3900/13500]\tLoss: 8685.4316\n",
      "Training Epoch: 2 [3950/13500]\tLoss: 8898.7393\n",
      "Training Epoch: 2 [4000/13500]\tLoss: 9191.3320\n",
      "Training Epoch: 2 [4050/13500]\tLoss: 8601.7354\n",
      "Training Epoch: 2 [4100/13500]\tLoss: 8559.7217\n",
      "Training Epoch: 2 [4150/13500]\tLoss: 9207.0840\n",
      "Training Epoch: 2 [4200/13500]\tLoss: 8676.0732\n",
      "Training Epoch: 2 [4250/13500]\tLoss: 9226.0469\n",
      "Training Epoch: 2 [4300/13500]\tLoss: 9368.9697\n",
      "Training Epoch: 2 [4350/13500]\tLoss: 8835.9854\n",
      "Training Epoch: 2 [4400/13500]\tLoss: 8692.5488\n",
      "Training Epoch: 2 [4450/13500]\tLoss: 9082.9258\n",
      "Training Epoch: 2 [4500/13500]\tLoss: 8356.1055\n",
      "Training Epoch: 2 [4550/13500]\tLoss: 8697.3613\n",
      "Training Epoch: 2 [4600/13500]\tLoss: 8309.4307\n",
      "Training Epoch: 2 [4650/13500]\tLoss: 8964.4814\n",
      "Training Epoch: 2 [4700/13500]\tLoss: 8840.0762\n",
      "Training Epoch: 2 [4750/13500]\tLoss: 8668.8281\n",
      "Training Epoch: 2 [4800/13500]\tLoss: 8622.1553\n",
      "Training Epoch: 2 [4850/13500]\tLoss: 8345.8086\n",
      "Training Epoch: 2 [4900/13500]\tLoss: 8631.0703\n",
      "Training Epoch: 2 [4950/13500]\tLoss: 8427.2217\n",
      "Training Epoch: 2 [5000/13500]\tLoss: 8639.1582\n",
      "Training Epoch: 2 [5050/13500]\tLoss: 8645.7773\n",
      "Training Epoch: 2 [5100/13500]\tLoss: 8239.5264\n",
      "Training Epoch: 2 [5150/13500]\tLoss: 8138.1274\n",
      "Training Epoch: 2 [5200/13500]\tLoss: 8126.8135\n",
      "Training Epoch: 2 [5250/13500]\tLoss: 8643.1309\n",
      "Training Epoch: 2 [5300/13500]\tLoss: 7902.3994\n",
      "Training Epoch: 2 [5350/13500]\tLoss: 8874.3613\n",
      "Training Epoch: 2 [5400/13500]\tLoss: 8378.4756\n",
      "Training Epoch: 2 [5450/13500]\tLoss: 8513.2734\n",
      "Training Epoch: 2 [5500/13500]\tLoss: 8577.3115\n",
      "Training Epoch: 2 [5550/13500]\tLoss: 8457.3896\n",
      "Training Epoch: 2 [5600/13500]\tLoss: 8621.9111\n",
      "Training Epoch: 2 [5650/13500]\tLoss: 8449.7744\n",
      "Training Epoch: 2 [5700/13500]\tLoss: 8916.1240\n",
      "Training Epoch: 2 [5750/13500]\tLoss: 8524.3779\n",
      "Training Epoch: 2 [5800/13500]\tLoss: 8147.0869\n",
      "Training Epoch: 2 [5850/13500]\tLoss: 8720.5781\n",
      "Training Epoch: 2 [5900/13500]\tLoss: 8364.0742\n",
      "Training Epoch: 2 [5950/13500]\tLoss: 8842.1230\n",
      "Training Epoch: 2 [6000/13500]\tLoss: 8334.5234\n",
      "Training Epoch: 2 [6050/13500]\tLoss: 7811.2368\n",
      "Training Epoch: 2 [6100/13500]\tLoss: 8539.3477\n",
      "Training Epoch: 2 [6150/13500]\tLoss: 8235.5342\n",
      "Training Epoch: 2 [6200/13500]\tLoss: 8719.4189\n",
      "Training Epoch: 2 [6250/13500]\tLoss: 8164.3413\n",
      "Training Epoch: 2 [6300/13500]\tLoss: 8301.0801\n",
      "Training Epoch: 2 [6350/13500]\tLoss: 8472.5732\n",
      "Training Epoch: 2 [6400/13500]\tLoss: 8120.1084\n",
      "Training Epoch: 2 [6450/13500]\tLoss: 7965.4053\n",
      "Training Epoch: 2 [6500/13500]\tLoss: 8398.9287\n",
      "Training Epoch: 2 [6550/13500]\tLoss: 8202.3535\n",
      "Training Epoch: 2 [6600/13500]\tLoss: 8714.7949\n",
      "Training Epoch: 2 [6650/13500]\tLoss: 8250.6074\n",
      "Training Epoch: 2 [6700/13500]\tLoss: 8004.4922\n",
      "Training Epoch: 2 [6750/13500]\tLoss: 8015.0259\n",
      "Training Epoch: 2 [6800/13500]\tLoss: 8531.2695\n",
      "Training Epoch: 2 [6850/13500]\tLoss: 7958.4575\n",
      "Training Epoch: 2 [6900/13500]\tLoss: 7948.5576\n",
      "Training Epoch: 2 [6950/13500]\tLoss: 8130.1128\n",
      "Training Epoch: 2 [7000/13500]\tLoss: 8809.0566\n",
      "Training Epoch: 2 [7050/13500]\tLoss: 8714.2412\n",
      "Training Epoch: 2 [7100/13500]\tLoss: 8289.6719\n",
      "Training Epoch: 2 [7150/13500]\tLoss: 8409.9307\n",
      "Training Epoch: 2 [7200/13500]\tLoss: 7699.9346\n",
      "Training Epoch: 2 [7250/13500]\tLoss: 8115.4204\n",
      "Training Epoch: 2 [7300/13500]\tLoss: 8091.4893\n",
      "Training Epoch: 2 [7350/13500]\tLoss: 8764.7842\n",
      "Training Epoch: 2 [7400/13500]\tLoss: 7944.5596\n",
      "Training Epoch: 2 [7450/13500]\tLoss: 7937.4888\n",
      "Training Epoch: 2 [7500/13500]\tLoss: 8203.2246\n",
      "Training Epoch: 2 [7550/13500]\tLoss: 8240.9629\n",
      "Training Epoch: 2 [7600/13500]\tLoss: 8399.2285\n",
      "Training Epoch: 2 [7650/13500]\tLoss: 8347.8145\n",
      "Training Epoch: 2 [7700/13500]\tLoss: 8015.6963\n",
      "Training Epoch: 2 [7750/13500]\tLoss: 8240.9658\n",
      "Training Epoch: 2 [7800/13500]\tLoss: 8303.4570\n",
      "Training Epoch: 2 [7850/13500]\tLoss: 8309.1299\n",
      "Training Epoch: 2 [7900/13500]\tLoss: 8574.5752\n",
      "Training Epoch: 2 [7950/13500]\tLoss: 8411.3203\n",
      "Training Epoch: 2 [8000/13500]\tLoss: 8296.8652\n",
      "Training Epoch: 2 [8050/13500]\tLoss: 7698.1040\n",
      "Training Epoch: 2 [8100/13500]\tLoss: 8314.0322\n",
      "Training Epoch: 2 [8150/13500]\tLoss: 7730.8066\n",
      "Training Epoch: 2 [8200/13500]\tLoss: 8446.4346\n",
      "Training Epoch: 2 [8250/13500]\tLoss: 8370.7520\n",
      "Training Epoch: 2 [8300/13500]\tLoss: 7866.4404\n",
      "Training Epoch: 2 [8350/13500]\tLoss: 8170.7056\n",
      "Training Epoch: 2 [8400/13500]\tLoss: 8487.9502\n",
      "Training Epoch: 2 [8450/13500]\tLoss: 8023.1201\n",
      "Training Epoch: 2 [8500/13500]\tLoss: 8213.9082\n",
      "Training Epoch: 2 [8550/13500]\tLoss: 7890.9844\n",
      "Training Epoch: 2 [8600/13500]\tLoss: 7909.3672\n",
      "Training Epoch: 2 [8650/13500]\tLoss: 8079.7158\n",
      "Training Epoch: 2 [8700/13500]\tLoss: 7727.2153\n",
      "Training Epoch: 2 [8750/13500]\tLoss: 7755.0684\n",
      "Training Epoch: 2 [8800/13500]\tLoss: 7627.7446\n",
      "Training Epoch: 2 [8850/13500]\tLoss: 8660.9941\n",
      "Training Epoch: 2 [8900/13500]\tLoss: 7563.4629\n",
      "Training Epoch: 2 [8950/13500]\tLoss: 7544.7690\n",
      "Training Epoch: 2 [9000/13500]\tLoss: 7201.7632\n",
      "Training Epoch: 2 [9050/13500]\tLoss: 7875.9731\n",
      "Training Epoch: 2 [9100/13500]\tLoss: 7960.8525\n",
      "Training Epoch: 2 [9150/13500]\tLoss: 7990.7246\n",
      "Training Epoch: 2 [9200/13500]\tLoss: 7440.0664\n",
      "Training Epoch: 2 [9250/13500]\tLoss: 7916.9648\n",
      "Training Epoch: 2 [9300/13500]\tLoss: 7563.6997\n",
      "Training Epoch: 2 [9350/13500]\tLoss: 7837.4893\n",
      "Training Epoch: 2 [9400/13500]\tLoss: 7860.6738\n",
      "Training Epoch: 2 [9450/13500]\tLoss: 7379.7427\n",
      "Training Epoch: 2 [9500/13500]\tLoss: 7848.0059\n",
      "Training Epoch: 2 [9550/13500]\tLoss: 7554.4429\n",
      "Training Epoch: 2 [9600/13500]\tLoss: 7899.3042\n",
      "Training Epoch: 2 [9650/13500]\tLoss: 7915.1152\n",
      "Training Epoch: 2 [9700/13500]\tLoss: 7953.6641\n",
      "Training Epoch: 2 [9750/13500]\tLoss: 7596.4321\n",
      "Training Epoch: 2 [9800/13500]\tLoss: 7549.8501\n",
      "Training Epoch: 2 [9850/13500]\tLoss: 7586.9219\n",
      "Training Epoch: 2 [9900/13500]\tLoss: 7658.5596\n",
      "Training Epoch: 2 [9950/13500]\tLoss: 8302.5615\n",
      "Training Epoch: 2 [10000/13500]\tLoss: 7945.7339\n",
      "Training Epoch: 2 [10050/13500]\tLoss: 7657.3369\n",
      "Training Epoch: 2 [10100/13500]\tLoss: 7858.5474\n",
      "Training Epoch: 2 [10150/13500]\tLoss: 7569.5776\n",
      "Training Epoch: 2 [10200/13500]\tLoss: 7520.1626\n",
      "Training Epoch: 2 [10250/13500]\tLoss: 7790.6567\n",
      "Training Epoch: 2 [10300/13500]\tLoss: 7834.0161\n",
      "Training Epoch: 2 [10350/13500]\tLoss: 8075.9702\n",
      "Training Epoch: 2 [10400/13500]\tLoss: 7607.0488\n",
      "Training Epoch: 2 [10450/13500]\tLoss: 8055.0298\n",
      "Training Epoch: 2 [10500/13500]\tLoss: 7187.4424\n",
      "Training Epoch: 2 [10550/13500]\tLoss: 7896.1543\n",
      "Training Epoch: 2 [10600/13500]\tLoss: 7505.0293\n",
      "Training Epoch: 2 [10650/13500]\tLoss: 7680.4351\n",
      "Training Epoch: 2 [10700/13500]\tLoss: 7590.0947\n",
      "Training Epoch: 2 [10750/13500]\tLoss: 7778.5625\n",
      "Training Epoch: 2 [10800/13500]\tLoss: 7847.9097\n",
      "Training Epoch: 2 [10850/13500]\tLoss: 7566.0400\n",
      "Training Epoch: 2 [10900/13500]\tLoss: 7270.0630\n",
      "Training Epoch: 2 [10950/13500]\tLoss: 7470.2075\n",
      "Training Epoch: 2 [11000/13500]\tLoss: 7681.4556\n",
      "Training Epoch: 2 [11050/13500]\tLoss: 7387.4189\n",
      "Training Epoch: 2 [11100/13500]\tLoss: 7140.5752\n",
      "Training Epoch: 2 [11150/13500]\tLoss: 7204.9424\n",
      "Training Epoch: 2 [11200/13500]\tLoss: 7632.3589\n",
      "Training Epoch: 2 [11250/13500]\tLoss: 7298.8125\n",
      "Training Epoch: 2 [11300/13500]\tLoss: 7624.3169\n",
      "Training Epoch: 2 [11350/13500]\tLoss: 7272.5127\n",
      "Training Epoch: 2 [11400/13500]\tLoss: 7307.4722\n",
      "Training Epoch: 2 [11450/13500]\tLoss: 7359.3052\n",
      "Training Epoch: 2 [11500/13500]\tLoss: 7529.1904\n",
      "Training Epoch: 2 [11550/13500]\tLoss: 7266.8594\n",
      "Training Epoch: 2 [11600/13500]\tLoss: 7301.9268\n",
      "Training Epoch: 2 [11650/13500]\tLoss: 7411.5269\n",
      "Training Epoch: 2 [11700/13500]\tLoss: 7166.5532\n",
      "Training Epoch: 2 [11750/13500]\tLoss: 7155.3838\n",
      "Training Epoch: 2 [11800/13500]\tLoss: 7159.9961\n",
      "Training Epoch: 2 [11850/13500]\tLoss: 7090.6489\n",
      "Training Epoch: 2 [11900/13500]\tLoss: 7144.7817\n",
      "Training Epoch: 2 [11950/13500]\tLoss: 7329.2529\n",
      "Training Epoch: 2 [12000/13500]\tLoss: 7641.4277\n",
      "Training Epoch: 2 [12050/13500]\tLoss: 7188.4062\n",
      "Training Epoch: 2 [12100/13500]\tLoss: 7283.8770\n",
      "Training Epoch: 2 [12150/13500]\tLoss: 6925.2383\n",
      "Training Epoch: 2 [12200/13500]\tLoss: 6971.2734\n",
      "Training Epoch: 2 [12250/13500]\tLoss: 6947.2212\n",
      "Training Epoch: 2 [12300/13500]\tLoss: 6990.5781\n",
      "Training Epoch: 2 [12350/13500]\tLoss: 7252.7207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [12400/13500]\tLoss: 7248.1934\n",
      "Training Epoch: 2 [12450/13500]\tLoss: 7415.1006\n",
      "Training Epoch: 2 [12500/13500]\tLoss: 7079.3555\n",
      "Training Epoch: 2 [12550/13500]\tLoss: 7090.4468\n",
      "Training Epoch: 2 [12600/13500]\tLoss: 7480.1450\n",
      "Training Epoch: 2 [12650/13500]\tLoss: 7260.3823\n",
      "Training Epoch: 2 [12700/13500]\tLoss: 7438.0161\n",
      "Training Epoch: 2 [12750/13500]\tLoss: 7073.9272\n",
      "Training Epoch: 2 [12800/13500]\tLoss: 7211.3330\n",
      "Training Epoch: 2 [12850/13500]\tLoss: 7627.7729\n",
      "Training Epoch: 2 [12900/13500]\tLoss: 7007.8027\n",
      "Training Epoch: 2 [12950/13500]\tLoss: 7089.3359\n",
      "Training Epoch: 2 [13000/13500]\tLoss: 7186.5088\n",
      "Training Epoch: 2 [13050/13500]\tLoss: 6886.9897\n",
      "Training Epoch: 2 [13100/13500]\tLoss: 7242.1094\n",
      "Training Epoch: 2 [13150/13500]\tLoss: 6813.9712\n",
      "Training Epoch: 2 [13200/13500]\tLoss: 7160.1191\n",
      "Training Epoch: 2 [13250/13500]\tLoss: 7544.5923\n",
      "Training Epoch: 2 [13300/13500]\tLoss: 7153.8359\n",
      "Training Epoch: 2 [13350/13500]\tLoss: 7065.4556\n",
      "Training Epoch: 2 [13400/13500]\tLoss: 7065.3154\n",
      "Training Epoch: 2 [13450/13500]\tLoss: 6795.3623\n",
      "Training Epoch: 2 [13500/13500]\tLoss: 7499.7935\n",
      "Training Epoch: 2 [1499/1499]\tLoss: 7203.5472\n",
      "Training Epoch: 3 [50/13500]\tLoss: 7033.0254\n",
      "Training Epoch: 3 [100/13500]\tLoss: 7012.7817\n",
      "Training Epoch: 3 [150/13500]\tLoss: 6785.9629\n",
      "Training Epoch: 3 [200/13500]\tLoss: 6944.5034\n",
      "Training Epoch: 3 [250/13500]\tLoss: 6995.5073\n",
      "Training Epoch: 3 [300/13500]\tLoss: 6788.1128\n",
      "Training Epoch: 3 [350/13500]\tLoss: 7378.9854\n",
      "Training Epoch: 3 [400/13500]\tLoss: 7445.9927\n",
      "Training Epoch: 3 [450/13500]\tLoss: 7073.6680\n",
      "Training Epoch: 3 [500/13500]\tLoss: 6814.8901\n",
      "Training Epoch: 3 [550/13500]\tLoss: 7198.2969\n",
      "Training Epoch: 3 [600/13500]\tLoss: 6532.5283\n",
      "Training Epoch: 3 [650/13500]\tLoss: 7009.4062\n",
      "Training Epoch: 3 [700/13500]\tLoss: 7186.1479\n",
      "Training Epoch: 3 [750/13500]\tLoss: 6782.7900\n",
      "Training Epoch: 3 [800/13500]\tLoss: 6912.7739\n",
      "Training Epoch: 3 [850/13500]\tLoss: 7237.7900\n",
      "Training Epoch: 3 [900/13500]\tLoss: 6753.0215\n",
      "Training Epoch: 3 [950/13500]\tLoss: 6851.0078\n",
      "Training Epoch: 3 [1000/13500]\tLoss: 7437.2466\n",
      "Training Epoch: 3 [1050/13500]\tLoss: 6565.7070\n",
      "Training Epoch: 3 [1100/13500]\tLoss: 7468.2505\n",
      "Training Epoch: 3 [1150/13500]\tLoss: 7045.0474\n",
      "Training Epoch: 3 [1200/13500]\tLoss: 6997.7324\n",
      "Training Epoch: 3 [1250/13500]\tLoss: 6912.4604\n",
      "Training Epoch: 3 [1300/13500]\tLoss: 7052.8105\n",
      "Training Epoch: 3 [1350/13500]\tLoss: 6658.8955\n",
      "Training Epoch: 3 [1400/13500]\tLoss: 6745.2559\n",
      "Training Epoch: 3 [1450/13500]\tLoss: 7098.5698\n",
      "Training Epoch: 3 [1500/13500]\tLoss: 7006.2720\n",
      "Training Epoch: 3 [1550/13500]\tLoss: 7104.3374\n",
      "Training Epoch: 3 [1600/13500]\tLoss: 7316.0840\n",
      "Training Epoch: 3 [1650/13500]\tLoss: 7194.0010\n",
      "Training Epoch: 3 [1700/13500]\tLoss: 6764.8301\n",
      "Training Epoch: 3 [1750/13500]\tLoss: 7517.5376\n",
      "Training Epoch: 3 [1800/13500]\tLoss: 6656.6201\n",
      "Training Epoch: 3 [1850/13500]\tLoss: 7345.3735\n",
      "Training Epoch: 3 [1900/13500]\tLoss: 6867.0068\n",
      "Training Epoch: 3 [1950/13500]\tLoss: 7096.9937\n",
      "Training Epoch: 3 [2000/13500]\tLoss: 6634.0029\n",
      "Training Epoch: 3 [2050/13500]\tLoss: 6575.9429\n",
      "Training Epoch: 3 [2100/13500]\tLoss: 7357.6118\n",
      "Training Epoch: 3 [2150/13500]\tLoss: 7043.5503\n",
      "Training Epoch: 3 [2200/13500]\tLoss: 6685.2622\n",
      "Training Epoch: 3 [2250/13500]\tLoss: 6643.0122\n",
      "Training Epoch: 3 [2300/13500]\tLoss: 6730.0566\n",
      "Training Epoch: 3 [2350/13500]\tLoss: 6893.1606\n",
      "Training Epoch: 3 [2400/13500]\tLoss: 6711.3677\n",
      "Training Epoch: 3 [2450/13500]\tLoss: 7291.3994\n",
      "Training Epoch: 3 [2500/13500]\tLoss: 6747.3359\n",
      "Training Epoch: 3 [2550/13500]\tLoss: 6540.1206\n",
      "Training Epoch: 3 [2600/13500]\tLoss: 7030.3369\n",
      "Training Epoch: 3 [2650/13500]\tLoss: 7046.0415\n",
      "Training Epoch: 3 [2700/13500]\tLoss: 6374.5508\n",
      "Training Epoch: 3 [2750/13500]\tLoss: 6623.9141\n",
      "Training Epoch: 3 [2800/13500]\tLoss: 6895.0645\n",
      "Training Epoch: 3 [2850/13500]\tLoss: 6946.7930\n",
      "Training Epoch: 3 [2900/13500]\tLoss: 6468.0854\n",
      "Training Epoch: 3 [2950/13500]\tLoss: 6739.8838\n",
      "Training Epoch: 3 [3000/13500]\tLoss: 6592.3325\n",
      "Training Epoch: 3 [3050/13500]\tLoss: 6901.1963\n",
      "Training Epoch: 3 [3100/13500]\tLoss: 6683.8384\n",
      "Training Epoch: 3 [3150/13500]\tLoss: 6156.2051\n",
      "Training Epoch: 3 [3200/13500]\tLoss: 6903.4341\n",
      "Training Epoch: 3 [3250/13500]\tLoss: 6306.5693\n",
      "Training Epoch: 3 [3300/13500]\tLoss: 6714.8101\n",
      "Training Epoch: 3 [3350/13500]\tLoss: 6582.5083\n",
      "Training Epoch: 3 [3400/13500]\tLoss: 6710.8462\n",
      "Training Epoch: 3 [3450/13500]\tLoss: 6553.6353\n",
      "Training Epoch: 3 [3500/13500]\tLoss: 6634.2993\n",
      "Training Epoch: 3 [3550/13500]\tLoss: 6451.6045\n",
      "Training Epoch: 3 [3600/13500]\tLoss: 6626.3862\n",
      "Training Epoch: 3 [3650/13500]\tLoss: 6791.8862\n",
      "Training Epoch: 3 [3700/13500]\tLoss: 6918.2856\n",
      "Training Epoch: 3 [3750/13500]\tLoss: 6263.5688\n",
      "Training Epoch: 3 [3800/13500]\tLoss: 6622.4546\n",
      "Training Epoch: 3 [3850/13500]\tLoss: 6509.8770\n",
      "Training Epoch: 3 [3900/13500]\tLoss: 6466.8779\n",
      "Training Epoch: 3 [3950/13500]\tLoss: 6610.2256\n",
      "Training Epoch: 3 [4000/13500]\tLoss: 6902.4219\n",
      "Training Epoch: 3 [4050/13500]\tLoss: 6373.7651\n",
      "Training Epoch: 3 [4100/13500]\tLoss: 6299.1162\n",
      "Training Epoch: 3 [4150/13500]\tLoss: 6912.6953\n",
      "Training Epoch: 3 [4200/13500]\tLoss: 6473.8745\n",
      "Training Epoch: 3 [4250/13500]\tLoss: 6926.9316\n",
      "Training Epoch: 3 [4300/13500]\tLoss: 7089.1172\n",
      "Training Epoch: 3 [4350/13500]\tLoss: 6586.2305\n",
      "Training Epoch: 3 [4400/13500]\tLoss: 6513.8677\n",
      "Training Epoch: 3 [4450/13500]\tLoss: 6817.1138\n",
      "Training Epoch: 3 [4500/13500]\tLoss: 6228.8330\n",
      "Training Epoch: 3 [4550/13500]\tLoss: 6504.5308\n",
      "Training Epoch: 3 [4600/13500]\tLoss: 6228.1890\n",
      "Training Epoch: 3 [4650/13500]\tLoss: 6758.3677\n",
      "Training Epoch: 3 [4700/13500]\tLoss: 6620.4810\n",
      "Training Epoch: 3 [4750/13500]\tLoss: 6474.1763\n",
      "Training Epoch: 3 [4800/13500]\tLoss: 6423.5430\n",
      "Training Epoch: 3 [4850/13500]\tLoss: 6218.3774\n",
      "Training Epoch: 3 [4900/13500]\tLoss: 6443.1704\n",
      "Training Epoch: 3 [4950/13500]\tLoss: 6275.3438\n",
      "Training Epoch: 3 [5000/13500]\tLoss: 6453.2544\n",
      "Training Epoch: 3 [5050/13500]\tLoss: 6528.9746\n",
      "Training Epoch: 3 [5100/13500]\tLoss: 6132.9150\n",
      "Training Epoch: 3 [5150/13500]\tLoss: 6045.0532\n",
      "Training Epoch: 3 [5200/13500]\tLoss: 6114.0488\n",
      "Training Epoch: 3 [5250/13500]\tLoss: 6508.7686\n",
      "Training Epoch: 3 [5300/13500]\tLoss: 5821.2026\n",
      "Training Epoch: 3 [5350/13500]\tLoss: 6751.5601\n",
      "Training Epoch: 3 [5400/13500]\tLoss: 6325.9224\n",
      "Training Epoch: 3 [5450/13500]\tLoss: 6403.7754\n",
      "Training Epoch: 3 [5500/13500]\tLoss: 6453.7642\n",
      "Training Epoch: 3 [5550/13500]\tLoss: 6436.4609\n",
      "Training Epoch: 3 [5600/13500]\tLoss: 6508.3320\n",
      "Training Epoch: 3 [5650/13500]\tLoss: 6363.9478\n",
      "Training Epoch: 3 [5700/13500]\tLoss: 6712.9619\n",
      "Training Epoch: 3 [5750/13500]\tLoss: 6463.2729\n",
      "Training Epoch: 3 [5800/13500]\tLoss: 6107.8076\n",
      "Training Epoch: 3 [5850/13500]\tLoss: 6603.0083\n",
      "Training Epoch: 3 [5900/13500]\tLoss: 6317.3765\n",
      "Training Epoch: 3 [5950/13500]\tLoss: 6737.5889\n",
      "Training Epoch: 3 [6000/13500]\tLoss: 6296.7285\n",
      "Training Epoch: 3 [6050/13500]\tLoss: 5828.1606\n",
      "Training Epoch: 3 [6100/13500]\tLoss: 6483.9214\n",
      "Training Epoch: 3 [6150/13500]\tLoss: 6265.6299\n",
      "Training Epoch: 3 [6200/13500]\tLoss: 6645.9502\n",
      "Training Epoch: 3 [6250/13500]\tLoss: 6157.8667\n",
      "Training Epoch: 3 [6300/13500]\tLoss: 6321.0620\n",
      "Training Epoch: 3 [6350/13500]\tLoss: 6412.5337\n",
      "Training Epoch: 3 [6400/13500]\tLoss: 6138.1221\n",
      "Training Epoch: 3 [6450/13500]\tLoss: 5931.9614\n",
      "Training Epoch: 3 [6500/13500]\tLoss: 6351.8076\n",
      "Training Epoch: 3 [6550/13500]\tLoss: 6186.0923\n",
      "Training Epoch: 3 [6600/13500]\tLoss: 6626.3296\n",
      "Training Epoch: 3 [6650/13500]\tLoss: 6216.0317\n",
      "Training Epoch: 3 [6700/13500]\tLoss: 6006.1108\n",
      "Training Epoch: 3 [6750/13500]\tLoss: 6086.7446\n",
      "Training Epoch: 3 [6800/13500]\tLoss: 6523.2119\n",
      "Training Epoch: 3 [6850/13500]\tLoss: 6033.4487\n",
      "Training Epoch: 3 [6900/13500]\tLoss: 5996.9307\n",
      "Training Epoch: 3 [6950/13500]\tLoss: 6141.9106\n",
      "Training Epoch: 3 [7000/13500]\tLoss: 6758.5088\n",
      "Training Epoch: 3 [7050/13500]\tLoss: 6693.6094\n",
      "Training Epoch: 3 [7100/13500]\tLoss: 6326.2407\n",
      "Training Epoch: 3 [7150/13500]\tLoss: 6435.0908\n",
      "Training Epoch: 3 [7200/13500]\tLoss: 5809.3813\n",
      "Training Epoch: 3 [7250/13500]\tLoss: 6200.1465\n",
      "Training Epoch: 3 [7300/13500]\tLoss: 6159.8374\n",
      "Training Epoch: 3 [7350/13500]\tLoss: 6759.2446\n",
      "Training Epoch: 3 [7400/13500]\tLoss: 6024.4868\n",
      "Training Epoch: 3 [7450/13500]\tLoss: 6054.5479\n",
      "Training Epoch: 3 [7500/13500]\tLoss: 6238.5796\n",
      "Training Epoch: 3 [7550/13500]\tLoss: 6340.0068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [7600/13500]\tLoss: 6452.8091\n",
      "Training Epoch: 3 [7650/13500]\tLoss: 6458.7749\n",
      "Training Epoch: 3 [7700/13500]\tLoss: 6124.5952\n",
      "Training Epoch: 3 [7750/13500]\tLoss: 6345.9282\n",
      "Training Epoch: 3 [7800/13500]\tLoss: 6373.7842\n",
      "Training Epoch: 3 [7850/13500]\tLoss: 6344.8838\n",
      "Training Epoch: 3 [7900/13500]\tLoss: 6662.3774\n",
      "Training Epoch: 3 [7950/13500]\tLoss: 6492.2866\n",
      "Training Epoch: 3 [8000/13500]\tLoss: 6369.1851\n",
      "Training Epoch: 3 [8050/13500]\tLoss: 5837.1553\n",
      "Training Epoch: 3 [8100/13500]\tLoss: 6410.7432\n",
      "Training Epoch: 3 [8150/13500]\tLoss: 5883.7544\n",
      "Training Epoch: 3 [8200/13500]\tLoss: 6539.9243\n",
      "Training Epoch: 3 [8250/13500]\tLoss: 6475.6602\n",
      "Training Epoch: 3 [8300/13500]\tLoss: 6040.4600\n",
      "Training Epoch: 3 [8350/13500]\tLoss: 6280.7051\n",
      "Training Epoch: 3 [8400/13500]\tLoss: 6569.5269\n",
      "Training Epoch: 3 [8450/13500]\tLoss: 6147.2168\n",
      "Training Epoch: 3 [8500/13500]\tLoss: 6328.6470\n",
      "Training Epoch: 3 [8550/13500]\tLoss: 6059.5234\n",
      "Training Epoch: 3 [8600/13500]\tLoss: 6056.2812\n",
      "Training Epoch: 3 [8650/13500]\tLoss: 6224.8052\n",
      "Training Epoch: 3 [8700/13500]\tLoss: 5900.7910\n",
      "Training Epoch: 3 [8750/13500]\tLoss: 5948.5259\n",
      "Training Epoch: 3 [8800/13500]\tLoss: 5844.0034\n",
      "Training Epoch: 3 [8850/13500]\tLoss: 6761.4072\n",
      "Training Epoch: 3 [8900/13500]\tLoss: 5751.6250\n",
      "Training Epoch: 3 [8950/13500]\tLoss: 5763.3481\n",
      "Training Epoch: 3 [9000/13500]\tLoss: 5457.5498\n",
      "Training Epoch: 3 [9050/13500]\tLoss: 6055.3916\n",
      "Training Epoch: 3 [9100/13500]\tLoss: 6136.2852\n",
      "Training Epoch: 3 [9150/13500]\tLoss: 6159.4385\n",
      "Training Epoch: 3 [9200/13500]\tLoss: 5717.2129\n",
      "Training Epoch: 3 [9250/13500]\tLoss: 6102.7822\n",
      "Training Epoch: 3 [9300/13500]\tLoss: 5775.2773\n",
      "Training Epoch: 3 [9350/13500]\tLoss: 6030.9688\n",
      "Training Epoch: 3 [9400/13500]\tLoss: 6029.4790\n",
      "Training Epoch: 3 [9450/13500]\tLoss: 5632.2056\n",
      "Training Epoch: 3 [9500/13500]\tLoss: 6070.2695\n",
      "Training Epoch: 3 [9550/13500]\tLoss: 5773.3267\n",
      "Training Epoch: 3 [9600/13500]\tLoss: 6092.0557\n",
      "Training Epoch: 3 [9650/13500]\tLoss: 6115.5771\n",
      "Training Epoch: 3 [9700/13500]\tLoss: 6140.3882\n",
      "Training Epoch: 3 [9750/13500]\tLoss: 5821.6709\n",
      "Training Epoch: 3 [9800/13500]\tLoss: 5789.9121\n",
      "Training Epoch: 3 [9850/13500]\tLoss: 5834.2944\n",
      "Training Epoch: 3 [9900/13500]\tLoss: 5928.6655\n",
      "Training Epoch: 3 [9950/13500]\tLoss: 6469.4746\n",
      "Training Epoch: 3 [10000/13500]\tLoss: 6166.6592\n",
      "Training Epoch: 3 [10050/13500]\tLoss: 5926.4478\n",
      "Training Epoch: 3 [10100/13500]\tLoss: 6109.7085\n",
      "Training Epoch: 3 [10150/13500]\tLoss: 5823.3535\n",
      "Training Epoch: 3 [10200/13500]\tLoss: 5798.2334\n",
      "Training Epoch: 3 [10250/13500]\tLoss: 6022.5737\n",
      "Training Epoch: 3 [10300/13500]\tLoss: 6054.8130\n",
      "Training Epoch: 3 [10350/13500]\tLoss: 6298.8022\n",
      "Training Epoch: 3 [10400/13500]\tLoss: 5866.8823\n",
      "Training Epoch: 3 [10450/13500]\tLoss: 6292.1953\n",
      "Training Epoch: 3 [10500/13500]\tLoss: 5517.0649\n",
      "Training Epoch: 3 [10550/13500]\tLoss: 6114.7847\n",
      "Training Epoch: 3 [10600/13500]\tLoss: 5808.8467\n",
      "Training Epoch: 3 [10650/13500]\tLoss: 5933.1143\n",
      "Training Epoch: 3 [10700/13500]\tLoss: 5884.7593\n",
      "Training Epoch: 3 [10750/13500]\tLoss: 6038.9224\n",
      "Training Epoch: 3 [10800/13500]\tLoss: 6098.9546\n",
      "Training Epoch: 3 [10850/13500]\tLoss: 5838.3223\n",
      "Training Epoch: 3 [10900/13500]\tLoss: 5584.8647\n",
      "Training Epoch: 3 [10950/13500]\tLoss: 5792.6982\n",
      "Training Epoch: 3 [11000/13500]\tLoss: 5940.7646\n",
      "Training Epoch: 3 [11050/13500]\tLoss: 5728.6992\n",
      "Training Epoch: 3 [11100/13500]\tLoss: 5502.9482\n",
      "Training Epoch: 3 [11150/13500]\tLoss: 5525.1001\n",
      "Training Epoch: 3 [11200/13500]\tLoss: 5934.9893\n",
      "Training Epoch: 3 [11250/13500]\tLoss: 5662.1182\n",
      "Training Epoch: 3 [11300/13500]\tLoss: 5935.0894\n",
      "Training Epoch: 3 [11350/13500]\tLoss: 5630.3608\n",
      "Training Epoch: 3 [11400/13500]\tLoss: 5662.7031\n",
      "Training Epoch: 3 [11450/13500]\tLoss: 5698.4868\n",
      "Training Epoch: 3 [11500/13500]\tLoss: 5854.7720\n",
      "Training Epoch: 3 [11550/13500]\tLoss: 5623.9546\n",
      "Training Epoch: 3 [11600/13500]\tLoss: 5631.9150\n",
      "Training Epoch: 3 [11650/13500]\tLoss: 5768.7178\n",
      "Training Epoch: 3 [11700/13500]\tLoss: 5539.8149\n",
      "Training Epoch: 3 [11750/13500]\tLoss: 5535.9629\n",
      "Training Epoch: 3 [11800/13500]\tLoss: 5567.5879\n",
      "Training Epoch: 3 [11850/13500]\tLoss: 5470.0430\n",
      "Training Epoch: 3 [11900/13500]\tLoss: 5537.8345\n",
      "Training Epoch: 3 [11950/13500]\tLoss: 5699.6167\n",
      "Training Epoch: 3 [12000/13500]\tLoss: 5945.8638\n",
      "Training Epoch: 3 [12050/13500]\tLoss: 5587.3198\n",
      "Training Epoch: 3 [12100/13500]\tLoss: 5672.4624\n",
      "Training Epoch: 3 [12150/13500]\tLoss: 5371.1934\n",
      "Training Epoch: 3 [12200/13500]\tLoss: 5405.4551\n",
      "Training Epoch: 3 [12250/13500]\tLoss: 5351.5889\n",
      "Training Epoch: 3 [12300/13500]\tLoss: 5420.1064\n",
      "Training Epoch: 3 [12350/13500]\tLoss: 5658.5078\n",
      "Training Epoch: 3 [12400/13500]\tLoss: 5669.3032\n",
      "Training Epoch: 3 [12450/13500]\tLoss: 5771.2354\n",
      "Training Epoch: 3 [12500/13500]\tLoss: 5491.1987\n",
      "Training Epoch: 3 [12550/13500]\tLoss: 5509.3237\n",
      "Training Epoch: 3 [12600/13500]\tLoss: 5843.6924\n",
      "Training Epoch: 3 [12650/13500]\tLoss: 5670.9238\n",
      "Training Epoch: 3 [12700/13500]\tLoss: 5804.5522\n",
      "Training Epoch: 3 [12750/13500]\tLoss: 5508.9316\n",
      "Training Epoch: 3 [12800/13500]\tLoss: 5626.0771\n",
      "Training Epoch: 3 [12850/13500]\tLoss: 5972.8540\n",
      "Training Epoch: 3 [12900/13500]\tLoss: 5450.6118\n",
      "Training Epoch: 3 [12950/13500]\tLoss: 5505.3726\n",
      "Training Epoch: 3 [13000/13500]\tLoss: 5576.6489\n",
      "Training Epoch: 3 [13050/13500]\tLoss: 5335.7144\n",
      "Training Epoch: 3 [13100/13500]\tLoss: 5635.9321\n",
      "Training Epoch: 3 [13150/13500]\tLoss: 5306.6914\n",
      "Training Epoch: 3 [13200/13500]\tLoss: 5631.5093\n",
      "Training Epoch: 3 [13250/13500]\tLoss: 5920.9346\n",
      "Training Epoch: 3 [13300/13500]\tLoss: 5598.0229\n",
      "Training Epoch: 3 [13350/13500]\tLoss: 5519.0386\n",
      "Training Epoch: 3 [13400/13500]\tLoss: 5534.5469\n",
      "Training Epoch: 3 [13450/13500]\tLoss: 5276.5059\n",
      "Training Epoch: 3 [13500/13500]\tLoss: 5887.2778\n",
      "Training Epoch: 3 [1499/1499]\tLoss: 5641.5884\n",
      "Training Epoch: 4 [50/13500]\tLoss: 5525.8384\n",
      "Training Epoch: 4 [100/13500]\tLoss: 5472.3486\n",
      "Training Epoch: 4 [150/13500]\tLoss: 5263.2783\n",
      "Training Epoch: 4 [200/13500]\tLoss: 5431.5356\n",
      "Training Epoch: 4 [250/13500]\tLoss: 5484.2378\n",
      "Training Epoch: 4 [300/13500]\tLoss: 5292.3799\n",
      "Training Epoch: 4 [350/13500]\tLoss: 5778.0923\n",
      "Training Epoch: 4 [400/13500]\tLoss: 5854.2593\n",
      "Training Epoch: 4 [450/13500]\tLoss: 5522.8477\n",
      "Training Epoch: 4 [500/13500]\tLoss: 5315.4111\n",
      "Training Epoch: 4 [550/13500]\tLoss: 5658.9829\n",
      "Training Epoch: 4 [600/13500]\tLoss: 5102.9829\n",
      "Training Epoch: 4 [650/13500]\tLoss: 5491.3799\n",
      "Training Epoch: 4 [700/13500]\tLoss: 5643.5908\n",
      "Training Epoch: 4 [750/13500]\tLoss: 5310.2720\n",
      "Training Epoch: 4 [800/13500]\tLoss: 5390.1313\n",
      "Training Epoch: 4 [850/13500]\tLoss: 5683.3584\n",
      "Training Epoch: 4 [900/13500]\tLoss: 5261.0991\n",
      "Training Epoch: 4 [950/13500]\tLoss: 5350.2891\n",
      "Training Epoch: 4 [1000/13500]\tLoss: 5863.4849\n",
      "Training Epoch: 4 [1050/13500]\tLoss: 5125.0806\n",
      "Training Epoch: 4 [1100/13500]\tLoss: 5892.4458\n",
      "Training Epoch: 4 [1150/13500]\tLoss: 5534.8501\n",
      "Training Epoch: 4 [1200/13500]\tLoss: 5501.0620\n",
      "Training Epoch: 4 [1250/13500]\tLoss: 5426.4189\n",
      "Training Epoch: 4 [1300/13500]\tLoss: 5535.9688\n",
      "Training Epoch: 4 [1350/13500]\tLoss: 5198.6392\n",
      "Training Epoch: 4 [1400/13500]\tLoss: 5300.9189\n",
      "Training Epoch: 4 [1450/13500]\tLoss: 5589.8110\n",
      "Training Epoch: 4 [1500/13500]\tLoss: 5510.7329\n",
      "Training Epoch: 4 [1550/13500]\tLoss: 5623.3848\n",
      "Training Epoch: 4 [1600/13500]\tLoss: 5773.5737\n",
      "Training Epoch: 4 [1650/13500]\tLoss: 5672.5898\n",
      "Training Epoch: 4 [1700/13500]\tLoss: 5304.6489\n",
      "Training Epoch: 4 [1750/13500]\tLoss: 5990.7983\n",
      "Training Epoch: 4 [1800/13500]\tLoss: 5216.5854\n",
      "Training Epoch: 4 [1850/13500]\tLoss: 5796.6929\n",
      "Training Epoch: 4 [1900/13500]\tLoss: 5412.7524\n",
      "Training Epoch: 4 [1950/13500]\tLoss: 5598.8408\n",
      "Training Epoch: 4 [2000/13500]\tLoss: 5227.8945\n",
      "Training Epoch: 4 [2050/13500]\tLoss: 5168.9834\n",
      "Training Epoch: 4 [2100/13500]\tLoss: 5841.5298\n",
      "Training Epoch: 4 [2150/13500]\tLoss: 5558.7554\n",
      "Training Epoch: 4 [2200/13500]\tLoss: 5269.5557\n",
      "Training Epoch: 4 [2250/13500]\tLoss: 5210.7163\n",
      "Training Epoch: 4 [2300/13500]\tLoss: 5317.5293\n",
      "Training Epoch: 4 [2350/13500]\tLoss: 5453.4185\n",
      "Training Epoch: 4 [2400/13500]\tLoss: 5267.1309\n",
      "Training Epoch: 4 [2450/13500]\tLoss: 5777.2075\n",
      "Training Epoch: 4 [2500/13500]\tLoss: 5287.7773\n",
      "Training Epoch: 4 [2550/13500]\tLoss: 5126.5122\n",
      "Training Epoch: 4 [2600/13500]\tLoss: 5561.9175\n",
      "Training Epoch: 4 [2650/13500]\tLoss: 5584.1460\n",
      "Training Epoch: 4 [2700/13500]\tLoss: 4974.5093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [2750/13500]\tLoss: 5184.5259\n",
      "Training Epoch: 4 [2800/13500]\tLoss: 5474.1685\n",
      "Training Epoch: 4 [2850/13500]\tLoss: 5507.6318\n",
      "Training Epoch: 4 [2900/13500]\tLoss: 5108.5264\n",
      "Training Epoch: 4 [2950/13500]\tLoss: 5314.9829\n",
      "Training Epoch: 4 [3000/13500]\tLoss: 5182.0874\n",
      "Training Epoch: 4 [3050/13500]\tLoss: 5455.4277\n",
      "Training Epoch: 4 [3100/13500]\tLoss: 5269.0532\n",
      "Training Epoch: 4 [3150/13500]\tLoss: 4821.1753\n",
      "Training Epoch: 4 [3200/13500]\tLoss: 5478.1431\n",
      "Training Epoch: 4 [3250/13500]\tLoss: 4951.9595\n",
      "Training Epoch: 4 [3300/13500]\tLoss: 5331.9512\n",
      "Training Epoch: 4 [3350/13500]\tLoss: 5209.9878\n",
      "Training Epoch: 4 [3400/13500]\tLoss: 5280.8047\n",
      "Training Epoch: 4 [3450/13500]\tLoss: 5162.7637\n",
      "Training Epoch: 4 [3500/13500]\tLoss: 5246.4751\n",
      "Training Epoch: 4 [3550/13500]\tLoss: 5079.8394\n",
      "Training Epoch: 4 [3600/13500]\tLoss: 5229.5591\n",
      "Training Epoch: 4 [3650/13500]\tLoss: 5375.0479\n",
      "Training Epoch: 4 [3700/13500]\tLoss: 5492.4766\n",
      "Training Epoch: 4 [3750/13500]\tLoss: 4948.1387\n",
      "Training Epoch: 4 [3800/13500]\tLoss: 5244.1870\n",
      "Training Epoch: 4 [3850/13500]\tLoss: 5152.1050\n",
      "Training Epoch: 4 [3900/13500]\tLoss: 5127.1709\n",
      "Training Epoch: 4 [3950/13500]\tLoss: 5229.2036\n",
      "Training Epoch: 4 [4000/13500]\tLoss: 5486.7607\n",
      "Training Epoch: 4 [4050/13500]\tLoss: 5034.7271\n",
      "Training Epoch: 4 [4100/13500]\tLoss: 4959.6938\n",
      "Training Epoch: 4 [4150/13500]\tLoss: 5494.6182\n",
      "Training Epoch: 4 [4200/13500]\tLoss: 5137.5972\n",
      "Training Epoch: 4 [4250/13500]\tLoss: 5499.2651\n",
      "Training Epoch: 4 [4300/13500]\tLoss: 5652.7065\n",
      "Training Epoch: 4 [4350/13500]\tLoss: 5221.4702\n",
      "Training Epoch: 4 [4400/13500]\tLoss: 5177.8726\n",
      "Training Epoch: 4 [4450/13500]\tLoss: 5409.0708\n",
      "Training Epoch: 4 [4500/13500]\tLoss: 4932.2842\n",
      "Training Epoch: 4 [4550/13500]\tLoss: 5165.6377\n",
      "Training Epoch: 4 [4600/13500]\tLoss: 4952.3394\n",
      "Training Epoch: 4 [4650/13500]\tLoss: 5373.6226\n",
      "Training Epoch: 4 [4700/13500]\tLoss: 5252.1709\n",
      "Training Epoch: 4 [4750/13500]\tLoss: 5123.5342\n",
      "Training Epoch: 4 [4800/13500]\tLoss: 5081.4116\n",
      "Training Epoch: 4 [4850/13500]\tLoss: 4922.2847\n",
      "Training Epoch: 4 [4900/13500]\tLoss: 5110.5488\n",
      "Training Epoch: 4 [4950/13500]\tLoss: 4961.7939\n",
      "Training Epoch: 4 [5000/13500]\tLoss: 5107.8525\n",
      "Training Epoch: 4 [5050/13500]\tLoss: 5204.9370\n",
      "Training Epoch: 4 [5100/13500]\tLoss: 4856.2949\n",
      "Training Epoch: 4 [5150/13500]\tLoss: 4775.8354\n",
      "Training Epoch: 4 [5200/13500]\tLoss: 4869.0825\n",
      "Training Epoch: 4 [5250/13500]\tLoss: 5172.0054\n",
      "Training Epoch: 4 [5300/13500]\tLoss: 4583.1362\n",
      "Training Epoch: 4 [5350/13500]\tLoss: 5402.0347\n",
      "Training Epoch: 4 [5400/13500]\tLoss: 5046.1675\n",
      "Training Epoch: 4 [5450/13500]\tLoss: 5092.2388\n",
      "Training Epoch: 4 [5500/13500]\tLoss: 5130.4927\n",
      "Training Epoch: 4 [5550/13500]\tLoss: 5149.2959\n",
      "Training Epoch: 4 [5600/13500]\tLoss: 5184.9204\n",
      "Training Epoch: 4 [5650/13500]\tLoss: 5066.4351\n",
      "Training Epoch: 4 [5700/13500]\tLoss: 5322.7812\n",
      "Training Epoch: 4 [5750/13500]\tLoss: 5152.5439\n",
      "Training Epoch: 4 [5800/13500]\tLoss: 4849.2251\n",
      "Training Epoch: 4 [5850/13500]\tLoss: 5259.7930\n",
      "Training Epoch: 4 [5900/13500]\tLoss: 5030.2993\n",
      "Training Epoch: 4 [5950/13500]\tLoss: 5386.9375\n",
      "Training Epoch: 4 [6000/13500]\tLoss: 5017.6636\n",
      "Training Epoch: 4 [6050/13500]\tLoss: 4619.2705\n",
      "Training Epoch: 4 [6100/13500]\tLoss: 5173.3159\n",
      "Training Epoch: 4 [6150/13500]\tLoss: 5010.7480\n",
      "Training Epoch: 4 [6200/13500]\tLoss: 5317.8926\n",
      "Training Epoch: 4 [6250/13500]\tLoss: 4906.4717\n",
      "Training Epoch: 4 [6300/13500]\tLoss: 5056.4253\n",
      "Training Epoch: 4 [6350/13500]\tLoss: 5113.1880\n",
      "Training Epoch: 4 [6400/13500]\tLoss: 4894.6289\n",
      "Training Epoch: 4 [6450/13500]\tLoss: 4694.4976\n",
      "Training Epoch: 4 [6500/13500]\tLoss: 5056.7476\n",
      "Training Epoch: 4 [6550/13500]\tLoss: 4923.8076\n",
      "Training Epoch: 4 [6600/13500]\tLoss: 5280.9897\n",
      "Training Epoch: 4 [6650/13500]\tLoss: 4945.8315\n",
      "Training Epoch: 4 [6700/13500]\tLoss: 4763.3921\n",
      "Training Epoch: 4 [6750/13500]\tLoss: 4877.1060\n",
      "Training Epoch: 4 [6800/13500]\tLoss: 5219.3125\n",
      "Training Epoch: 4 [6850/13500]\tLoss: 4812.7812\n",
      "Training Epoch: 4 [6900/13500]\tLoss: 4775.7773\n",
      "Training Epoch: 4 [6950/13500]\tLoss: 4892.4077\n",
      "Training Epoch: 4 [7000/13500]\tLoss: 5422.1074\n",
      "Training Epoch: 4 [7050/13500]\tLoss: 5378.0908\n",
      "Training Epoch: 4 [7100/13500]\tLoss: 5068.0273\n",
      "Training Epoch: 4 [7150/13500]\tLoss: 5162.0859\n",
      "Training Epoch: 4 [7200/13500]\tLoss: 4628.4775\n",
      "Training Epoch: 4 [7250/13500]\tLoss: 4978.8638\n",
      "Training Epoch: 4 [7300/13500]\tLoss: 4929.9277\n",
      "Training Epoch: 4 [7350/13500]\tLoss: 5433.4575\n",
      "Training Epoch: 4 [7400/13500]\tLoss: 4806.0151\n",
      "Training Epoch: 4 [7450/13500]\tLoss: 4851.3247\n",
      "Training Epoch: 4 [7500/13500]\tLoss: 4979.6924\n",
      "Training Epoch: 4 [7550/13500]\tLoss: 5100.2681\n",
      "Training Epoch: 4 [7600/13500]\tLoss: 5174.1694\n",
      "Training Epoch: 4 [7650/13500]\tLoss: 5208.6133\n",
      "Training Epoch: 4 [7700/13500]\tLoss: 4910.8003\n",
      "Training Epoch: 4 [7750/13500]\tLoss: 5107.3018\n",
      "Training Epoch: 4 [7800/13500]\tLoss: 5113.4990\n",
      "Training Epoch: 4 [7850/13500]\tLoss: 5078.9448\n",
      "Training Epoch: 4 [7900/13500]\tLoss: 5380.6782\n",
      "Training Epoch: 4 [7950/13500]\tLoss: 5222.6836\n",
      "Training Epoch: 4 [8000/13500]\tLoss: 5113.8315\n",
      "Training Epoch: 4 [8050/13500]\tLoss: 4656.5889\n",
      "Training Epoch: 4 [8100/13500]\tLoss: 5165.1162\n",
      "Training Epoch: 4 [8150/13500]\tLoss: 4703.3154\n",
      "Training Epoch: 4 [8200/13500]\tLoss: 5282.1304\n",
      "Training Epoch: 4 [8250/13500]\tLoss: 5216.1113\n",
      "Training Epoch: 4 [8300/13500]\tLoss: 4857.6704\n",
      "Training Epoch: 4 [8350/13500]\tLoss: 5049.3032\n",
      "Training Epoch: 4 [8400/13500]\tLoss: 5291.6094\n",
      "Training Epoch: 4 [8450/13500]\tLoss: 4931.2388\n",
      "Training Epoch: 4 [8500/13500]\tLoss: 5083.8223\n",
      "Training Epoch: 4 [8550/13500]\tLoss: 4864.9824\n",
      "Training Epoch: 4 [8600/13500]\tLoss: 4851.9595\n",
      "Training Epoch: 4 [8650/13500]\tLoss: 5000.5913\n",
      "Training Epoch: 4 [8700/13500]\tLoss: 4729.8276\n",
      "Training Epoch: 4 [8750/13500]\tLoss: 4780.3667\n",
      "Training Epoch: 4 [8800/13500]\tLoss: 4687.9170\n",
      "Training Epoch: 4 [8850/13500]\tLoss: 5480.2798\n",
      "Training Epoch: 4 [8900/13500]\tLoss: 4597.6147\n",
      "Training Epoch: 4 [8950/13500]\tLoss: 4615.9800\n",
      "Training Epoch: 4 [9000/13500]\tLoss: 4357.2139\n",
      "Training Epoch: 4 [9050/13500]\tLoss: 4859.2402\n",
      "Training Epoch: 4 [9100/13500]\tLoss: 4938.3643\n",
      "Training Epoch: 4 [9150/13500]\tLoss: 4952.4678\n",
      "Training Epoch: 4 [9200/13500]\tLoss: 4602.7773\n",
      "Training Epoch: 4 [9250/13500]\tLoss: 4910.4263\n",
      "Training Epoch: 4 [9300/13500]\tLoss: 4622.3228\n",
      "Training Epoch: 4 [9350/13500]\tLoss: 4853.0723\n",
      "Training Epoch: 4 [9400/13500]\tLoss: 4843.0581\n",
      "Training Epoch: 4 [9450/13500]\tLoss: 4517.4082\n",
      "Training Epoch: 4 [9500/13500]\tLoss: 4895.6099\n",
      "Training Epoch: 4 [9550/13500]\tLoss: 4619.9336\n",
      "Training Epoch: 4 [9600/13500]\tLoss: 4905.2451\n",
      "Training Epoch: 4 [9650/13500]\tLoss: 4927.0835\n",
      "Training Epoch: 4 [9700/13500]\tLoss: 4950.1953\n",
      "Training Epoch: 4 [9750/13500]\tLoss: 4667.7808\n",
      "Training Epoch: 4 [9800/13500]\tLoss: 4649.1270\n",
      "Training Epoch: 4 [9850/13500]\tLoss: 4688.7305\n",
      "Training Epoch: 4 [9900/13500]\tLoss: 4790.9185\n",
      "Training Epoch: 4 [9950/13500]\tLoss: 5233.7319\n",
      "Training Epoch: 4 [10000/13500]\tLoss: 4984.3018\n",
      "Training Epoch: 4 [10050/13500]\tLoss: 4784.9585\n",
      "Training Epoch: 4 [10100/13500]\tLoss: 4939.0249\n",
      "Training Epoch: 4 [10150/13500]\tLoss: 4685.0044\n",
      "Training Epoch: 4 [10200/13500]\tLoss: 4672.8569\n",
      "Training Epoch: 4 [10250/13500]\tLoss: 4855.5625\n",
      "Training Epoch: 4 [10300/13500]\tLoss: 4879.0400\n",
      "Training Epoch: 4 [10350/13500]\tLoss: 5095.4712\n",
      "Training Epoch: 4 [10400/13500]\tLoss: 4724.4922\n",
      "Training Epoch: 4 [10450/13500]\tLoss: 5105.5371\n",
      "Training Epoch: 4 [10500/13500]\tLoss: 4436.6079\n",
      "Training Epoch: 4 [10550/13500]\tLoss: 4927.1973\n",
      "Training Epoch: 4 [10600/13500]\tLoss: 4691.1987\n",
      "Training Epoch: 4 [10650/13500]\tLoss: 4784.3286\n",
      "Training Epoch: 4 [10700/13500]\tLoss: 4751.7993\n",
      "Training Epoch: 4 [10750/13500]\tLoss: 4880.6089\n",
      "Training Epoch: 4 [10800/13500]\tLoss: 4929.4243\n",
      "Training Epoch: 4 [10850/13500]\tLoss: 4702.8037\n",
      "Training Epoch: 4 [10900/13500]\tLoss: 4491.1904\n",
      "Training Epoch: 4 [10950/13500]\tLoss: 4684.8271\n",
      "Training Epoch: 4 [11000/13500]\tLoss: 4786.5532\n",
      "Training Epoch: 4 [11050/13500]\tLoss: 4629.8364\n",
      "Training Epoch: 4 [11100/13500]\tLoss: 4432.2627\n",
      "Training Epoch: 4 [11150/13500]\tLoss: 4438.8594\n",
      "Training Epoch: 4 [11200/13500]\tLoss: 4801.1748\n",
      "Training Epoch: 4 [11250/13500]\tLoss: 4574.8052\n",
      "Training Epoch: 4 [11300/13500]\tLoss: 4806.9849\n",
      "Training Epoch: 4 [11350/13500]\tLoss: 4548.8125\n",
      "Training Epoch: 4 [11400/13500]\tLoss: 4578.6099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [11450/13500]\tLoss: 4603.0166\n",
      "Training Epoch: 4 [11500/13500]\tLoss: 4741.3486\n",
      "Training Epoch: 4 [11550/13500]\tLoss: 4545.5801\n",
      "Training Epoch: 4 [11600/13500]\tLoss: 4540.4263\n",
      "Training Epoch: 4 [11650/13500]\tLoss: 4669.4888\n",
      "Training Epoch: 4 [11700/13500]\tLoss: 4471.3804\n",
      "Training Epoch: 4 [11750/13500]\tLoss: 4475.7568\n",
      "Training Epoch: 4 [11800/13500]\tLoss: 4524.7676\n",
      "Training Epoch: 4 [11850/13500]\tLoss: 4412.1265\n",
      "Training Epoch: 4 [11900/13500]\tLoss: 4482.4546\n",
      "Training Epoch: 4 [11950/13500]\tLoss: 4619.3091\n",
      "Training Epoch: 4 [12000/13500]\tLoss: 4812.8687\n",
      "Training Epoch: 4 [12050/13500]\tLoss: 4526.7949\n",
      "Training Epoch: 4 [12100/13500]\tLoss: 4601.4702\n",
      "Training Epoch: 4 [12150/13500]\tLoss: 4352.0391\n",
      "Training Epoch: 4 [12200/13500]\tLoss: 4378.6489\n",
      "Training Epoch: 4 [12250/13500]\tLoss: 4320.3989\n",
      "Training Epoch: 4 [12300/13500]\tLoss: 4391.2422\n",
      "Training Epoch: 4 [12350/13500]\tLoss: 4592.0747\n",
      "Training Epoch: 4 [12400/13500]\tLoss: 4608.5806\n",
      "Training Epoch: 4 [12450/13500]\tLoss: 4675.3525\n",
      "Training Epoch: 4 [12500/13500]\tLoss: 4444.5737\n",
      "Training Epoch: 4 [12550/13500]\tLoss: 4469.0190\n",
      "Training Epoch: 4 [12600/13500]\tLoss: 4750.0225\n",
      "Training Epoch: 4 [12650/13500]\tLoss: 4613.1514\n",
      "Training Epoch: 4 [12700/13500]\tLoss: 4707.8237\n",
      "Training Epoch: 4 [12750/13500]\tLoss: 4471.0312\n",
      "Training Epoch: 4 [12800/13500]\tLoss: 4568.1670\n",
      "Training Epoch: 4 [12850/13500]\tLoss: 4855.2290\n",
      "Training Epoch: 4 [12900/13500]\tLoss: 4423.6494\n",
      "Training Epoch: 4 [12950/13500]\tLoss: 4461.2417\n",
      "Training Epoch: 4 [13000/13500]\tLoss: 4515.0742\n",
      "Training Epoch: 4 [13050/13500]\tLoss: 4324.1699\n",
      "Training Epoch: 4 [13100/13500]\tLoss: 4569.8589\n",
      "Training Epoch: 4 [13150/13500]\tLoss: 4306.4399\n",
      "Training Epoch: 4 [13200/13500]\tLoss: 4594.5605\n",
      "Training Epoch: 4 [13250/13500]\tLoss: 4820.2173\n",
      "Training Epoch: 4 [13300/13500]\tLoss: 4562.3203\n",
      "Training Epoch: 4 [13350/13500]\tLoss: 4486.4219\n",
      "Training Epoch: 4 [13400/13500]\tLoss: 4507.9507\n",
      "Training Epoch: 4 [13450/13500]\tLoss: 4279.4546\n",
      "Training Epoch: 4 [13500/13500]\tLoss: 4792.4966\n",
      "Training Epoch: 4 [1499/1499]\tLoss: 4593.2568\n",
      "Training Epoch: 5 [50/13500]\tLoss: 4519.0591\n",
      "Training Epoch: 5 [100/13500]\tLoss: 4443.4766\n",
      "Training Epoch: 5 [150/13500]\tLoss: 4268.4946\n",
      "Training Epoch: 5 [200/13500]\tLoss: 4423.6201\n",
      "Training Epoch: 5 [250/13500]\tLoss: 4469.7822\n",
      "Training Epoch: 5 [300/13500]\tLoss: 4298.2373\n",
      "Training Epoch: 5 [350/13500]\tLoss: 4705.5332\n",
      "Training Epoch: 5 [400/13500]\tLoss: 4779.4438\n",
      "Training Epoch: 5 [450/13500]\tLoss: 4490.0073\n",
      "Training Epoch: 5 [500/13500]\tLoss: 4319.1187\n",
      "Training Epoch: 5 [550/13500]\tLoss: 4621.4116\n",
      "Training Epoch: 5 [600/13500]\tLoss: 4160.0376\n",
      "Training Epoch: 5 [650/13500]\tLoss: 4475.2368\n",
      "Training Epoch: 5 [700/13500]\tLoss: 4599.7642\n",
      "Training Epoch: 5 [750/13500]\tLoss: 4327.6299\n",
      "Training Epoch: 5 [800/13500]\tLoss: 4379.3237\n",
      "Training Epoch: 5 [850/13500]\tLoss: 4633.5586\n",
      "Training Epoch: 5 [900/13500]\tLoss: 4270.4663\n",
      "Training Epoch: 5 [950/13500]\tLoss: 4355.1362\n",
      "Training Epoch: 5 [1000/13500]\tLoss: 4796.5557\n",
      "Training Epoch: 5 [1050/13500]\tLoss: 4175.5684\n",
      "Training Epoch: 5 [1100/13500]\tLoss: 4819.4238\n",
      "Training Epoch: 5 [1150/13500]\tLoss: 4515.4043\n",
      "Training Epoch: 5 [1200/13500]\tLoss: 4493.9434\n",
      "Training Epoch: 5 [1250/13500]\tLoss: 4426.1313\n",
      "Training Epoch: 5 [1300/13500]\tLoss: 4517.5791\n",
      "Training Epoch: 5 [1350/13500]\tLoss: 4232.3359\n",
      "Training Epoch: 5 [1400/13500]\tLoss: 4331.5464\n",
      "Training Epoch: 5 [1450/13500]\tLoss: 4566.0674\n",
      "Training Epoch: 5 [1500/13500]\tLoss: 4503.5366\n",
      "Training Epoch: 5 [1550/13500]\tLoss: 4616.1421\n",
      "Training Epoch: 5 [1600/13500]\tLoss: 4719.0815\n",
      "Training Epoch: 5 [1650/13500]\tLoss: 4644.2178\n",
      "Training Epoch: 5 [1700/13500]\tLoss: 4324.6416\n",
      "Training Epoch: 5 [1750/13500]\tLoss: 4926.2524\n",
      "Training Epoch: 5 [1800/13500]\tLoss: 4260.3462\n",
      "Training Epoch: 5 [1850/13500]\tLoss: 4741.2827\n",
      "Training Epoch: 5 [1900/13500]\tLoss: 4432.9951\n",
      "Training Epoch: 5 [1950/13500]\tLoss: 4581.7529\n",
      "Training Epoch: 5 [2000/13500]\tLoss: 4283.6992\n",
      "Training Epoch: 5 [2050/13500]\tLoss: 4228.4863\n",
      "Training Epoch: 5 [2100/13500]\tLoss: 4797.3857\n",
      "Training Epoch: 5 [2150/13500]\tLoss: 4550.4116\n",
      "Training Epoch: 5 [2200/13500]\tLoss: 4316.4595\n",
      "Training Epoch: 5 [2250/13500]\tLoss: 4255.1011\n",
      "Training Epoch: 5 [2300/13500]\tLoss: 4363.4409\n",
      "Training Epoch: 5 [2350/13500]\tLoss: 4474.5825\n",
      "Training Epoch: 5 [2400/13500]\tLoss: 4297.8135\n",
      "Training Epoch: 5 [2450/13500]\tLoss: 4737.4341\n",
      "Training Epoch: 5 [2500/13500]\tLoss: 4312.3296\n",
      "Training Epoch: 5 [2550/13500]\tLoss: 4183.4741\n",
      "Training Epoch: 5 [2600/13500]\tLoss: 4562.8179\n",
      "Training Epoch: 5 [2650/13500]\tLoss: 4585.2617\n",
      "Training Epoch: 5 [2700/13500]\tLoss: 4047.5903\n",
      "Training Epoch: 5 [2750/13500]\tLoss: 4226.8442\n",
      "Training Epoch: 5 [2800/13500]\tLoss: 4505.5073\n",
      "Training Epoch: 5 [2850/13500]\tLoss: 4527.5815\n",
      "Training Epoch: 5 [2900/13500]\tLoss: 4193.6567\n",
      "Training Epoch: 5 [2950/13500]\tLoss: 4352.3062\n",
      "Training Epoch: 5 [3000/13500]\tLoss: 4238.9146\n",
      "Training Epoch: 5 [3050/13500]\tLoss: 4476.3652\n",
      "Training Epoch: 5 [3100/13500]\tLoss: 4313.4956\n",
      "Training Epoch: 5 [3150/13500]\tLoss: 3943.3982\n",
      "Training Epoch: 5 [3200/13500]\tLoss: 4505.1689\n",
      "Training Epoch: 5 [3250/13500]\tLoss: 4050.4673\n",
      "Training Epoch: 5 [3300/13500]\tLoss: 4390.8433\n",
      "Training Epoch: 5 [3350/13500]\tLoss: 4280.7129\n",
      "Training Epoch: 5 [3400/13500]\tLoss: 4321.4141\n",
      "Training Epoch: 5 [3450/13500]\tLoss: 4227.4946\n",
      "Training Epoch: 5 [3500/13500]\tLoss: 4304.9229\n",
      "Training Epoch: 5 [3550/13500]\tLoss: 4157.3882\n",
      "Training Epoch: 5 [3600/13500]\tLoss: 4294.8623\n",
      "Training Epoch: 5 [3650/13500]\tLoss: 4411.5698\n",
      "Training Epoch: 5 [3700/13500]\tLoss: 4515.5859\n",
      "Training Epoch: 5 [3750/13500]\tLoss: 4061.5342\n",
      "Training Epoch: 5 [3800/13500]\tLoss: 4309.9277\n",
      "Training Epoch: 5 [3850/13500]\tLoss: 4230.9453\n",
      "Training Epoch: 5 [3900/13500]\tLoss: 4217.7109\n",
      "Training Epoch: 5 [3950/13500]\tLoss: 4297.7134\n",
      "Training Epoch: 5 [4000/13500]\tLoss: 4512.3584\n",
      "Training Epoch: 5 [4050/13500]\tLoss: 4131.2295\n",
      "Training Epoch: 5 [4100/13500]\tLoss: 4065.1677\n",
      "Training Epoch: 5 [4150/13500]\tLoss: 4524.9521\n",
      "Training Epoch: 5 [4200/13500]\tLoss: 4230.4492\n",
      "Training Epoch: 5 [4250/13500]\tLoss: 4521.1675\n",
      "Training Epoch: 5 [4300/13500]\tLoss: 4657.5225\n",
      "Training Epoch: 5 [4350/13500]\tLoss: 4302.8096\n",
      "Training Epoch: 5 [4400/13500]\tLoss: 4268.8984\n",
      "Training Epoch: 5 [4450/13500]\tLoss: 4445.2305\n",
      "Training Epoch: 5 [4500/13500]\tLoss: 4057.6521\n",
      "Training Epoch: 5 [4550/13500]\tLoss: 4258.1890\n",
      "Training Epoch: 5 [4600/13500]\tLoss: 4089.2104\n",
      "Training Epoch: 5 [4650/13500]\tLoss: 4422.6436\n",
      "Training Epoch: 5 [4700/13500]\tLoss: 4326.7026\n",
      "Training Epoch: 5 [4750/13500]\tLoss: 4209.8955\n",
      "Training Epoch: 5 [4800/13500]\tLoss: 4174.8857\n",
      "Training Epoch: 5 [4850/13500]\tLoss: 4049.6707\n",
      "Training Epoch: 5 [4900/13500]\tLoss: 4215.3076\n",
      "Training Epoch: 5 [4950/13500]\tLoss: 4078.7673\n",
      "Training Epoch: 5 [5000/13500]\tLoss: 4197.8091\n",
      "Training Epoch: 5 [5050/13500]\tLoss: 4299.0654\n",
      "Training Epoch: 5 [5100/13500]\tLoss: 3998.3274\n",
      "Training Epoch: 5 [5150/13500]\tLoss: 3928.4539\n",
      "Training Epoch: 5 [5200/13500]\tLoss: 4023.4749\n",
      "Training Epoch: 5 [5250/13500]\tLoss: 4261.5405\n",
      "Training Epoch: 5 [5300/13500]\tLoss: 3764.3506\n",
      "Training Epoch: 5 [5350/13500]\tLoss: 4470.0347\n",
      "Training Epoch: 5 [5400/13500]\tLoss: 4174.9307\n",
      "Training Epoch: 5 [5450/13500]\tLoss: 4199.3750\n",
      "Training Epoch: 5 [5500/13500]\tLoss: 4232.1279\n",
      "Training Epoch: 5 [5550/13500]\tLoss: 4263.6279\n",
      "Training Epoch: 5 [5600/13500]\tLoss: 4280.4199\n",
      "Training Epoch: 5 [5650/13500]\tLoss: 4183.5952\n",
      "Training Epoch: 5 [5700/13500]\tLoss: 4376.3896\n",
      "Training Epoch: 5 [5750/13500]\tLoss: 4253.4541\n",
      "Training Epoch: 5 [5800/13500]\tLoss: 3999.6902\n",
      "Training Epoch: 5 [5850/13500]\tLoss: 4339.0894\n",
      "Training Epoch: 5 [5900/13500]\tLoss: 4154.3501\n",
      "Training Epoch: 5 [5950/13500]\tLoss: 4454.3975\n",
      "Training Epoch: 5 [6000/13500]\tLoss: 4142.7563\n",
      "Training Epoch: 5 [6050/13500]\tLoss: 3809.8884\n",
      "Training Epoch: 5 [6100/13500]\tLoss: 4273.5410\n",
      "Training Epoch: 5 [6150/13500]\tLoss: 4146.5503\n",
      "Training Epoch: 5 [6200/13500]\tLoss: 4401.7886\n",
      "Training Epoch: 5 [6250/13500]\tLoss: 4057.3464\n",
      "Training Epoch: 5 [6300/13500]\tLoss: 4184.9824\n",
      "Training Epoch: 5 [6350/13500]\tLoss: 4224.9893\n",
      "Training Epoch: 5 [6400/13500]\tLoss: 4048.8899\n",
      "Training Epoch: 5 [6450/13500]\tLoss: 3870.8760\n",
      "Training Epoch: 5 [6500/13500]\tLoss: 4173.8564\n",
      "Training Epoch: 5 [6550/13500]\tLoss: 4066.6699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [6600/13500]\tLoss: 4358.6963\n",
      "Training Epoch: 5 [6650/13500]\tLoss: 4085.3503\n",
      "Training Epoch: 5 [6700/13500]\tLoss: 3925.0420\n",
      "Training Epoch: 5 [6750/13500]\tLoss: 4050.9346\n",
      "Training Epoch: 5 [6800/13500]\tLoss: 4318.8081\n",
      "Training Epoch: 5 [6850/13500]\tLoss: 3980.1550\n",
      "Training Epoch: 5 [6900/13500]\tLoss: 3951.0481\n",
      "Training Epoch: 5 [6950/13500]\tLoss: 4044.1665\n",
      "Training Epoch: 5 [7000/13500]\tLoss: 4494.6162\n",
      "Training Epoch: 5 [7050/13500]\tLoss: 4464.5078\n",
      "Training Epoch: 5 [7100/13500]\tLoss: 4202.1792\n",
      "Training Epoch: 5 [7150/13500]\tLoss: 4285.4565\n",
      "Training Epoch: 5 [7200/13500]\tLoss: 3831.3730\n",
      "Training Epoch: 5 [7250/13500]\tLoss: 4140.8774\n",
      "Training Epoch: 5 [7300/13500]\tLoss: 4089.3281\n",
      "Training Epoch: 5 [7350/13500]\tLoss: 4509.9619\n",
      "Training Epoch: 5 [7400/13500]\tLoss: 3977.2085\n",
      "Training Epoch: 5 [7450/13500]\tLoss: 4026.8904\n",
      "Training Epoch: 5 [7500/13500]\tLoss: 4118.1484\n",
      "Training Epoch: 5 [7550/13500]\tLoss: 4240.8320\n",
      "Training Epoch: 5 [7600/13500]\tLoss: 4286.9907\n",
      "Training Epoch: 5 [7650/13500]\tLoss: 4334.9575\n",
      "Training Epoch: 5 [7700/13500]\tLoss: 4077.4006\n",
      "Training Epoch: 5 [7750/13500]\tLoss: 4250.0210\n",
      "Training Epoch: 5 [7800/13500]\tLoss: 4240.9316\n",
      "Training Epoch: 5 [7850/13500]\tLoss: 4209.4775\n",
      "Training Epoch: 5 [7900/13500]\tLoss: 4477.7593\n",
      "Training Epoch: 5 [7950/13500]\tLoss: 4336.8647\n",
      "Training Epoch: 5 [8000/13500]\tLoss: 4245.6724\n",
      "Training Epoch: 5 [8050/13500]\tLoss: 3855.5105\n",
      "Training Epoch: 5 [8100/13500]\tLoss: 4300.7197\n",
      "Training Epoch: 5 [8150/13500]\tLoss: 3899.5752\n",
      "Training Epoch: 5 [8200/13500]\tLoss: 4407.0361\n",
      "Training Epoch: 5 [8250/13500]\tLoss: 4336.8643\n",
      "Training Epoch: 5 [8300/13500]\tLoss: 4043.2446\n",
      "Training Epoch: 5 [8350/13500]\tLoss: 4197.9087\n",
      "Training Epoch: 5 [8400/13500]\tLoss: 4399.4233\n",
      "Training Epoch: 5 [8450/13500]\tLoss: 4094.7273\n",
      "Training Epoch: 5 [8500/13500]\tLoss: 4219.1729\n",
      "Training Epoch: 5 [8550/13500]\tLoss: 4042.2114\n",
      "Training Epoch: 5 [8600/13500]\tLoss: 4026.3101\n",
      "Training Epoch: 5 [8650/13500]\tLoss: 4152.6118\n",
      "Training Epoch: 5 [8700/13500]\tLoss: 3930.9338\n",
      "Training Epoch: 5 [8750/13500]\tLoss: 3978.3889\n",
      "Training Epoch: 5 [8800/13500]\tLoss: 3896.0344\n",
      "Training Epoch: 5 [8850/13500]\tLoss: 4574.7578\n",
      "Training Epoch: 5 [8900/13500]\tLoss: 3816.3298\n",
      "Training Epoch: 5 [8950/13500]\tLoss: 3835.6309\n",
      "Training Epoch: 5 [9000/13500]\tLoss: 3616.7849\n",
      "Training Epoch: 5 [9050/13500]\tLoss: 4032.9333\n",
      "Training Epoch: 5 [9100/13500]\tLoss: 4108.4658\n",
      "Training Epoch: 5 [9150/13500]\tLoss: 4116.7104\n",
      "Training Epoch: 5 [9200/13500]\tLoss: 3837.7878\n",
      "Training Epoch: 5 [9250/13500]\tLoss: 4083.7253\n",
      "Training Epoch: 5 [9300/13500]\tLoss: 3836.8127\n",
      "Training Epoch: 5 [9350/13500]\tLoss: 4042.8833\n",
      "Training Epoch: 5 [9400/13500]\tLoss: 4028.9902\n",
      "Training Epoch: 5 [9450/13500]\tLoss: 3759.8943\n",
      "Training Epoch: 5 [9500/13500]\tLoss: 4079.0940\n",
      "Training Epoch: 5 [9550/13500]\tLoss: 3832.0977\n",
      "Training Epoch: 5 [9600/13500]\tLoss: 4084.3909\n",
      "Training Epoch: 5 [9650/13500]\tLoss: 4102.4072\n",
      "Training Epoch: 5 [9700/13500]\tLoss: 4125.8105\n",
      "Training Epoch: 5 [9750/13500]\tLoss: 3878.2537\n",
      "Training Epoch: 5 [9800/13500]\tLoss: 3867.9365\n",
      "Training Epoch: 5 [9850/13500]\tLoss: 3898.3047\n",
      "Training Epoch: 5 [9900/13500]\tLoss: 4001.4111\n",
      "Training Epoch: 5 [9950/13500]\tLoss: 4364.7700\n",
      "Training Epoch: 5 [10000/13500]\tLoss: 4160.2021\n",
      "Training Epoch: 5 [10050/13500]\tLoss: 3994.2688\n",
      "Training Epoch: 5 [10100/13500]\tLoss: 4122.1914\n",
      "Training Epoch: 5 [10150/13500]\tLoss: 3901.8633\n",
      "Training Epoch: 5 [10200/13500]\tLoss: 3897.6187\n",
      "Training Epoch: 5 [10250/13500]\tLoss: 4046.7415\n",
      "Training Epoch: 5 [10300/13500]\tLoss: 4063.0142\n",
      "Training Epoch: 5 [10350/13500]\tLoss: 4251.0454\n",
      "Training Epoch: 5 [10400/13500]\tLoss: 3934.5603\n",
      "Training Epoch: 5 [10450/13500]\tLoss: 4271.2690\n",
      "Training Epoch: 5 [10500/13500]\tLoss: 3698.6179\n",
      "Training Epoch: 5 [10550/13500]\tLoss: 4101.8271\n",
      "Training Epoch: 5 [10600/13500]\tLoss: 3917.1370\n",
      "Training Epoch: 5 [10650/13500]\tLoss: 3990.3684\n",
      "Training Epoch: 5 [10700/13500]\tLoss: 3965.3867\n",
      "Training Epoch: 5 [10750/13500]\tLoss: 4073.4790\n",
      "Training Epoch: 5 [10800/13500]\tLoss: 4112.7080\n",
      "Training Epoch: 5 [10850/13500]\tLoss: 3917.4932\n",
      "Training Epoch: 5 [10900/13500]\tLoss: 3743.9421\n",
      "Training Epoch: 5 [10950/13500]\tLoss: 3916.9031\n",
      "Training Epoch: 5 [11000/13500]\tLoss: 3984.5703\n",
      "Training Epoch: 5 [11050/13500]\tLoss: 3866.0437\n",
      "Training Epoch: 5 [11100/13500]\tLoss: 3697.5454\n",
      "Training Epoch: 5 [11150/13500]\tLoss: 3697.8262\n",
      "Training Epoch: 5 [11200/13500]\tLoss: 4011.3706\n",
      "Training Epoch: 5 [11250/13500]\tLoss: 3820.9246\n",
      "Training Epoch: 5 [11300/13500]\tLoss: 4020.6514\n",
      "Training Epoch: 5 [11350/13500]\tLoss: 3800.2603\n",
      "Training Epoch: 5 [11400/13500]\tLoss: 3828.4302\n",
      "Training Epoch: 5 [11450/13500]\tLoss: 3844.6907\n",
      "Training Epoch: 5 [11500/13500]\tLoss: 3965.5066\n",
      "Training Epoch: 5 [11550/13500]\tLoss: 3800.1914\n",
      "Training Epoch: 5 [11600/13500]\tLoss: 3789.8916\n",
      "Training Epoch: 5 [11650/13500]\tLoss: 3903.6042\n",
      "Training Epoch: 5 [11700/13500]\tLoss: 3735.7134\n",
      "Training Epoch: 5 [11750/13500]\tLoss: 3745.4771\n",
      "Training Epoch: 5 [11800/13500]\tLoss: 3802.5183\n",
      "Training Epoch: 5 [11850/13500]\tLoss: 3686.7312\n",
      "Training Epoch: 5 [11900/13500]\tLoss: 3753.2124\n",
      "Training Epoch: 5 [11950/13500]\tLoss: 3869.2280\n",
      "Training Epoch: 5 [12000/13500]\tLoss: 4021.5657\n",
      "Training Epoch: 5 [12050/13500]\tLoss: 3790.8713\n",
      "Training Epoch: 5 [12100/13500]\tLoss: 3856.6992\n",
      "Training Epoch: 5 [12150/13500]\tLoss: 3650.0366\n",
      "Training Epoch: 5 [12200/13500]\tLoss: 3670.9241\n",
      "Training Epoch: 5 [12250/13500]\tLoss: 3615.4111\n",
      "Training Epoch: 5 [12300/13500]\tLoss: 3682.2039\n",
      "Training Epoch: 5 [12350/13500]\tLoss: 3847.6963\n",
      "Training Epoch: 5 [12400/13500]\tLoss: 3865.2090\n",
      "Training Epoch: 5 [12450/13500]\tLoss: 3910.9209\n",
      "Training Epoch: 5 [12500/13500]\tLoss: 3720.7397\n",
      "Training Epoch: 5 [12550/13500]\tLoss: 3747.3774\n",
      "Training Epoch: 5 [12600/13500]\tLoss: 3982.1272\n",
      "Training Epoch: 5 [12650/13500]\tLoss: 3874.7917\n",
      "Training Epoch: 5 [12700/13500]\tLoss: 3939.6350\n",
      "Training Epoch: 5 [12750/13500]\tLoss: 3749.4602\n",
      "Training Epoch: 5 [12800/13500]\tLoss: 3829.3816\n",
      "Training Epoch: 5 [12850/13500]\tLoss: 4069.9248\n",
      "Training Epoch: 5 [12900/13500]\tLoss: 3710.4131\n",
      "Training Epoch: 5 [12950/13500]\tLoss: 3737.6562\n",
      "Training Epoch: 5 [13000/13500]\tLoss: 3780.1616\n",
      "Training Epoch: 5 [13050/13500]\tLoss: 3628.7949\n",
      "Training Epoch: 5 [13100/13500]\tLoss: 3828.2859\n",
      "Training Epoch: 5 [13150/13500]\tLoss: 3611.7693\n",
      "Training Epoch: 5 [13200/13500]\tLoss: 3863.5786\n",
      "Training Epoch: 5 [13250/13500]\tLoss: 4043.3455\n",
      "Training Epoch: 5 [13300/13500]\tLoss: 3836.9121\n",
      "Training Epoch: 5 [13350/13500]\tLoss: 3765.1304\n",
      "Training Epoch: 5 [13400/13500]\tLoss: 3788.8140\n",
      "Training Epoch: 5 [13450/13500]\tLoss: 3589.9421\n",
      "Training Epoch: 5 [13500/13500]\tLoss: 4020.7336\n",
      "Training Epoch: 5 [1499/1499]\tLoss: 3857.5234\n",
      "Training Epoch: 6 [50/13500]\tLoss: 3810.3181\n",
      "Training Epoch: 6 [100/13500]\tLoss: 3727.1711\n",
      "Training Epoch: 6 [150/13500]\tLoss: 3583.1191\n",
      "Training Epoch: 6 [200/13500]\tLoss: 3717.9678\n",
      "Training Epoch: 6 [250/13500]\tLoss: 3757.9104\n",
      "Training Epoch: 6 [300/13500]\tLoss: 3605.9246\n",
      "Training Epoch: 6 [350/13500]\tLoss: 3953.7043\n",
      "Training Epoch: 6 [400/13500]\tLoss: 4021.9260\n",
      "Training Epoch: 6 [450/13500]\tLoss: 3767.5535\n",
      "Training Epoch: 6 [500/13500]\tLoss: 3624.9119\n",
      "Training Epoch: 6 [550/13500]\tLoss: 3890.8274\n",
      "Training Epoch: 6 [600/13500]\tLoss: 3504.0339\n",
      "Training Epoch: 6 [650/13500]\tLoss: 3763.9983\n",
      "Training Epoch: 6 [700/13500]\tLoss: 3863.3464\n",
      "Training Epoch: 6 [750/13500]\tLoss: 3640.2273\n",
      "Training Epoch: 6 [800/13500]\tLoss: 3675.0242\n",
      "Training Epoch: 6 [850/13500]\tLoss: 3892.6619\n",
      "Training Epoch: 6 [900/13500]\tLoss: 3582.5120\n",
      "Training Epoch: 6 [950/13500]\tLoss: 3660.7239\n",
      "Training Epoch: 6 [1000/13500]\tLoss: 4039.5850\n",
      "Training Epoch: 6 [1050/13500]\tLoss: 3515.6548\n",
      "Training Epoch: 6 [1100/13500]\tLoss: 4054.9800\n",
      "Training Epoch: 6 [1150/13500]\tLoss: 3796.5896\n",
      "Training Epoch: 6 [1200/13500]\tLoss: 3784.7061\n",
      "Training Epoch: 6 [1250/13500]\tLoss: 3722.7637\n",
      "Training Epoch: 6 [1300/13500]\tLoss: 3799.8684\n",
      "Training Epoch: 6 [1350/13500]\tLoss: 3560.3630\n",
      "Training Epoch: 6 [1400/13500]\tLoss: 3650.5562\n",
      "Training Epoch: 6 [1450/13500]\tLoss: 3841.6504\n",
      "Training Epoch: 6 [1500/13500]\tLoss: 3793.4673\n",
      "Training Epoch: 6 [1550/13500]\tLoss: 3898.3716\n",
      "Training Epoch: 6 [1600/13500]\tLoss: 3970.1221\n",
      "Training Epoch: 6 [1650/13500]\tLoss: 3914.4128\n",
      "Training Epoch: 6 [1700/13500]\tLoss: 3638.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [1750/13500]\tLoss: 4156.3643\n",
      "Training Epoch: 6 [1800/13500]\tLoss: 3592.9973\n",
      "Training Epoch: 6 [1850/13500]\tLoss: 3991.7158\n",
      "Training Epoch: 6 [1900/13500]\tLoss: 3739.1663\n",
      "Training Epoch: 6 [1950/13500]\tLoss: 3859.0896\n",
      "Training Epoch: 6 [2000/13500]\tLoss: 3618.4109\n",
      "Training Epoch: 6 [2050/13500]\tLoss: 3567.6296\n",
      "Training Epoch: 6 [2100/13500]\tLoss: 4047.4480\n",
      "Training Epoch: 6 [2150/13500]\tLoss: 3834.0024\n",
      "Training Epoch: 6 [2200/13500]\tLoss: 3642.9663\n",
      "Training Epoch: 6 [2250/13500]\tLoss: 3586.6438\n",
      "Training Epoch: 6 [2300/13500]\tLoss: 3685.8123\n",
      "Training Epoch: 6 [2350/13500]\tLoss: 3777.0630\n",
      "Training Epoch: 6 [2400/13500]\tLoss: 3617.3059\n",
      "Training Epoch: 6 [2450/13500]\tLoss: 3992.9556\n",
      "Training Epoch: 6 [2500/13500]\tLoss: 3629.2273\n",
      "Training Epoch: 6 [2550/13500]\tLoss: 3522.4897\n",
      "Training Epoch: 6 [2600/13500]\tLoss: 3849.9731\n",
      "Training Epoch: 6 [2650/13500]\tLoss: 3870.3694\n",
      "Training Epoch: 6 [2700/13500]\tLoss: 3402.7981\n",
      "Training Epoch: 6 [2750/13500]\tLoss: 3557.4456\n",
      "Training Epoch: 6 [2800/13500]\tLoss: 3814.0552\n",
      "Training Epoch: 6 [2850/13500]\tLoss: 3826.5007\n",
      "Training Epoch: 6 [2900/13500]\tLoss: 3547.0674\n",
      "Training Epoch: 6 [2950/13500]\tLoss: 3670.3989\n",
      "Training Epoch: 6 [3000/13500]\tLoss: 3575.2351\n",
      "Training Epoch: 6 [3050/13500]\tLoss: 3780.6416\n",
      "Training Epoch: 6 [3100/13500]\tLoss: 3637.6504\n",
      "Training Epoch: 6 [3150/13500]\tLoss: 3332.4172\n",
      "Training Epoch: 6 [3200/13500]\tLoss: 3809.0457\n",
      "Training Epoch: 6 [3250/13500]\tLoss: 3418.3286\n",
      "Training Epoch: 6 [3300/13500]\tLoss: 3718.3516\n",
      "Training Epoch: 6 [3350/13500]\tLoss: 3619.9871\n",
      "Training Epoch: 6 [3400/13500]\tLoss: 3645.4941\n",
      "Training Epoch: 6 [3450/13500]\tLoss: 3567.6331\n",
      "Training Epoch: 6 [3500/13500]\tLoss: 3635.0789\n",
      "Training Epoch: 6 [3550/13500]\tLoss: 3506.7476\n",
      "Training Epoch: 6 [3600/13500]\tLoss: 3633.0754\n",
      "Training Epoch: 6 [3650/13500]\tLoss: 3725.3428\n",
      "Training Epoch: 6 [3700/13500]\tLoss: 3815.4302\n",
      "Training Epoch: 6 [3750/13500]\tLoss: 3434.6096\n",
      "Training Epoch: 6 [3800/13500]\tLoss: 3645.0435\n",
      "Training Epoch: 6 [3850/13500]\tLoss: 3577.3521\n",
      "Training Epoch: 6 [3900/13500]\tLoss: 3569.1562\n",
      "Training Epoch: 6 [3950/13500]\tLoss: 3635.5034\n",
      "Training Epoch: 6 [4000/13500]\tLoss: 3810.8569\n",
      "Training Epoch: 6 [4050/13500]\tLoss: 3491.7595\n",
      "Training Epoch: 6 [4100/13500]\tLoss: 3435.2698\n",
      "Training Epoch: 6 [4150/13500]\tLoss: 3828.8032\n",
      "Training Epoch: 6 [4200/13500]\tLoss: 3582.8826\n",
      "Training Epoch: 6 [4250/13500]\tLoss: 3819.3315\n",
      "Training Epoch: 6 [4300/13500]\tLoss: 3936.2109\n",
      "Training Epoch: 6 [4350/13500]\tLoss: 3647.8289\n",
      "Training Epoch: 6 [4400/13500]\tLoss: 3617.8518\n",
      "Training Epoch: 6 [4450/13500]\tLoss: 3755.5735\n",
      "Training Epoch: 6 [4500/13500]\tLoss: 3437.2263\n",
      "Training Epoch: 6 [4550/13500]\tLoss: 3609.0056\n",
      "Training Epoch: 6 [4600/13500]\tLoss: 3473.5322\n",
      "Training Epoch: 6 [4650/13500]\tLoss: 3739.0630\n",
      "Training Epoch: 6 [4700/13500]\tLoss: 3665.2041\n",
      "Training Epoch: 6 [4750/13500]\tLoss: 3560.2998\n",
      "Training Epoch: 6 [4800/13500]\tLoss: 3529.9685\n",
      "Training Epoch: 6 [4850/13500]\tLoss: 3431.1567\n",
      "Training Epoch: 6 [4900/13500]\tLoss: 3577.2021\n",
      "Training Epoch: 6 [4950/13500]\tLoss: 3453.4800\n",
      "Training Epoch: 6 [5000/13500]\tLoss: 3550.6389\n",
      "Training Epoch: 6 [5050/13500]\tLoss: 3647.9595\n",
      "Training Epoch: 6 [5100/13500]\tLoss: 3388.7993\n",
      "Training Epoch: 6 [5150/13500]\tLoss: 3330.2239\n",
      "Training Epoch: 6 [5200/13500]\tLoss: 3417.2908\n",
      "Training Epoch: 6 [5250/13500]\tLoss: 3609.3760\n",
      "Training Epoch: 6 [5300/13500]\tLoss: 3191.1311\n",
      "Training Epoch: 6 [5350/13500]\tLoss: 3793.4407\n",
      "Training Epoch: 6 [5400/13500]\tLoss: 3548.3960\n",
      "Training Epoch: 6 [5450/13500]\tLoss: 3558.7043\n",
      "Training Epoch: 6 [5500/13500]\tLoss: 3588.5771\n",
      "Training Epoch: 6 [5550/13500]\tLoss: 3622.4209\n",
      "Training Epoch: 6 [5600/13500]\tLoss: 3630.1643\n",
      "Training Epoch: 6 [5650/13500]\tLoss: 3547.9365\n",
      "Training Epoch: 6 [5700/13500]\tLoss: 3698.8472\n",
      "Training Epoch: 6 [5750/13500]\tLoss: 3605.9358\n",
      "Training Epoch: 6 [5800/13500]\tLoss: 3394.2964\n",
      "Training Epoch: 6 [5850/13500]\tLoss: 3676.2705\n",
      "Training Epoch: 6 [5900/13500]\tLoss: 3526.4072\n",
      "Training Epoch: 6 [5950/13500]\tLoss: 3777.5151\n",
      "Training Epoch: 6 [6000/13500]\tLoss: 3512.9451\n",
      "Training Epoch: 6 [6050/13500]\tLoss: 3235.4277\n",
      "Training Epoch: 6 [6100/13500]\tLoss: 3624.6465\n",
      "Training Epoch: 6 [6150/13500]\tLoss: 3521.1965\n",
      "Training Epoch: 6 [6200/13500]\tLoss: 3734.9749\n",
      "Training Epoch: 6 [6250/13500]\tLoss: 3447.6477\n",
      "Training Epoch: 6 [6300/13500]\tLoss: 3553.2085\n",
      "Training Epoch: 6 [6350/13500]\tLoss: 3584.0464\n",
      "Training Epoch: 6 [6400/13500]\tLoss: 3440.8130\n",
      "Training Epoch: 6 [6450/13500]\tLoss: 3288.0659\n",
      "Training Epoch: 6 [6500/13500]\tLoss: 3539.3479\n",
      "Training Epoch: 6 [6550/13500]\tLoss: 3451.1709\n",
      "Training Epoch: 6 [6600/13500]\tLoss: 3693.6440\n",
      "Training Epoch: 6 [6650/13500]\tLoss: 3467.4663\n",
      "Training Epoch: 6 [6700/13500]\tLoss: 3327.4697\n",
      "Training Epoch: 6 [6750/13500]\tLoss: 3452.0920\n",
      "Training Epoch: 6 [6800/13500]\tLoss: 3664.1899\n",
      "Training Epoch: 6 [6850/13500]\tLoss: 3380.4465\n",
      "Training Epoch: 6 [6900/13500]\tLoss: 3360.4956\n",
      "Training Epoch: 6 [6950/13500]\tLoss: 3434.8142\n",
      "Training Epoch: 6 [7000/13500]\tLoss: 3816.7617\n",
      "Training Epoch: 6 [7050/13500]\tLoss: 3794.6760\n",
      "Training Epoch: 6 [7100/13500]\tLoss: 3572.9541\n",
      "Training Epoch: 6 [7150/13500]\tLoss: 3646.6609\n",
      "Training Epoch: 6 [7200/13500]\tLoss: 3260.3237\n",
      "Training Epoch: 6 [7250/13500]\tLoss: 3530.1458\n",
      "Training Epoch: 6 [7300/13500]\tLoss: 3479.9529\n",
      "Training Epoch: 6 [7350/13500]\tLoss: 3832.3044\n",
      "Training Epoch: 6 [7400/13500]\tLoss: 3381.1191\n",
      "Training Epoch: 6 [7450/13500]\tLoss: 3428.6060\n",
      "Training Epoch: 6 [7500/13500]\tLoss: 3494.7698\n",
      "Training Epoch: 6 [7550/13500]\tLoss: 3611.8962\n",
      "Training Epoch: 6 [7600/13500]\tLoss: 3639.6057\n",
      "Training Epoch: 6 [7650/13500]\tLoss: 3691.1321\n",
      "Training Epoch: 6 [7700/13500]\tLoss: 3471.7617\n",
      "Training Epoch: 6 [7750/13500]\tLoss: 3622.7976\n",
      "Training Epoch: 6 [7800/13500]\tLoss: 3603.7966\n",
      "Training Epoch: 6 [7850/13500]\tLoss: 3577.5461\n",
      "Training Epoch: 6 [7900/13500]\tLoss: 3809.2744\n",
      "Training Epoch: 6 [7950/13500]\tLoss: 3686.2263\n",
      "Training Epoch: 6 [8000/13500]\tLoss: 3611.5657\n",
      "Training Epoch: 6 [8050/13500]\tLoss: 3280.0281\n",
      "Training Epoch: 6 [8100/13500]\tLoss: 3664.6846\n",
      "Training Epoch: 6 [8150/13500]\tLoss: 3319.5903\n",
      "Training Epoch: 6 [8200/13500]\tLoss: 3761.0439\n",
      "Training Epoch: 6 [8250/13500]\tLoss: 3690.6924\n",
      "Training Epoch: 6 [8300/13500]\tLoss: 3449.0603\n",
      "Training Epoch: 6 [8350/13500]\tLoss: 3574.5684\n",
      "Training Epoch: 6 [8400/13500]\tLoss: 3742.5229\n",
      "Training Epoch: 6 [8450/13500]\tLoss: 3485.5190\n",
      "Training Epoch: 6 [8500/13500]\tLoss: 3585.9192\n",
      "Training Epoch: 6 [8550/13500]\tLoss: 3442.8660\n",
      "Training Epoch: 6 [8600/13500]\tLoss: 3426.9973\n",
      "Training Epoch: 6 [8650/13500]\tLoss: 3531.9094\n",
      "Training Epoch: 6 [8700/13500]\tLoss: 3351.8210\n",
      "Training Epoch: 6 [8750/13500]\tLoss: 3394.2266\n",
      "Training Epoch: 6 [8800/13500]\tLoss: 3321.7246\n",
      "Training Epoch: 6 [8850/13500]\tLoss: 3899.5322\n",
      "Training Epoch: 6 [8900/13500]\tLoss: 3253.9861\n",
      "Training Epoch: 6 [8950/13500]\tLoss: 3271.8618\n",
      "Training Epoch: 6 [9000/13500]\tLoss: 3085.1208\n",
      "Training Epoch: 6 [9050/13500]\tLoss: 3429.8760\n",
      "Training Epoch: 6 [9100/13500]\tLoss: 3499.7625\n",
      "Training Epoch: 6 [9150/13500]\tLoss: 3505.4653\n",
      "Training Epoch: 6 [9200/13500]\tLoss: 3279.8066\n",
      "Training Epoch: 6 [9250/13500]\tLoss: 3477.3726\n",
      "Training Epoch: 6 [9300/13500]\tLoss: 3267.6716\n",
      "Training Epoch: 6 [9350/13500]\tLoss: 3450.2080\n",
      "Training Epoch: 6 [9400/13500]\tLoss: 3435.2034\n",
      "Training Epoch: 6 [9450/13500]\tLoss: 3210.5952\n",
      "Training Epoch: 6 [9500/13500]\tLoss: 3477.9675\n",
      "Training Epoch: 6 [9550/13500]\tLoss: 3261.8303\n",
      "Training Epoch: 6 [9600/13500]\tLoss: 3482.0261\n",
      "Training Epoch: 6 [9650/13500]\tLoss: 3496.0596\n",
      "Training Epoch: 6 [9700/13500]\tLoss: 3519.3496\n",
      "Training Epoch: 6 [9750/13500]\tLoss: 3305.0310\n",
      "Training Epoch: 6 [9800/13500]\tLoss: 3298.8552\n",
      "Training Epoch: 6 [9850/13500]\tLoss: 3320.6250\n",
      "Training Epoch: 6 [9900/13500]\tLoss: 3419.4275\n",
      "Training Epoch: 6 [9950/13500]\tLoss: 3719.0925\n",
      "Training Epoch: 6 [10000/13500]\tLoss: 3550.5452\n",
      "Training Epoch: 6 [10050/13500]\tLoss: 3413.1226\n",
      "Training Epoch: 6 [10100/13500]\tLoss: 3518.5684\n",
      "Training Epoch: 6 [10150/13500]\tLoss: 3328.6721\n",
      "Training Epoch: 6 [10200/13500]\tLoss: 3329.4507\n",
      "Training Epoch: 6 [10250/13500]\tLoss: 3451.5837\n",
      "Training Epoch: 6 [10300/13500]\tLoss: 3462.4907\n",
      "Training Epoch: 6 [10350/13500]\tLoss: 3624.1394\n",
      "Training Epoch: 6 [10400/13500]\tLoss: 3355.1257\n",
      "Training Epoch: 6 [10450/13500]\tLoss: 3649.1140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [10500/13500]\tLoss: 3161.3862\n",
      "Training Epoch: 6 [10550/13500]\tLoss: 3494.2959\n",
      "Training Epoch: 6 [10600/13500]\tLoss: 3348.1606\n",
      "Training Epoch: 6 [10650/13500]\tLoss: 3405.9973\n",
      "Training Epoch: 6 [10700/13500]\tLoss: 3386.2188\n",
      "Training Epoch: 6 [10750/13500]\tLoss: 3476.0464\n",
      "Training Epoch: 6 [10800/13500]\tLoss: 3508.8706\n",
      "Training Epoch: 6 [10850/13500]\tLoss: 3340.4448\n",
      "Training Epoch: 6 [10900/13500]\tLoss: 3199.6804\n",
      "Training Epoch: 6 [10950/13500]\tLoss: 3350.1333\n",
      "Training Epoch: 6 [11000/13500]\tLoss: 3394.1936\n",
      "Training Epoch: 6 [11050/13500]\tLoss: 3302.0305\n",
      "Training Epoch: 6 [11100/13500]\tLoss: 3161.4282\n",
      "Training Epoch: 6 [11150/13500]\tLoss: 3158.6733\n",
      "Training Epoch: 6 [11200/13500]\tLoss: 3426.5068\n",
      "Training Epoch: 6 [11250/13500]\tLoss: 3266.2229\n",
      "Training Epoch: 6 [11300/13500]\tLoss: 3438.1301\n",
      "Training Epoch: 6 [11350/13500]\tLoss: 3248.3716\n",
      "Training Epoch: 6 [11400/13500]\tLoss: 3275.2129\n",
      "Training Epoch: 6 [11450/13500]\tLoss: 3286.3586\n",
      "Training Epoch: 6 [11500/13500]\tLoss: 3390.7874\n",
      "Training Epoch: 6 [11550/13500]\tLoss: 3251.2542\n",
      "Training Epoch: 6 [11600/13500]\tLoss: 3239.0857\n",
      "Training Epoch: 6 [11650/13500]\tLoss: 3337.2161\n",
      "Training Epoch: 6 [11700/13500]\tLoss: 3196.2822\n",
      "Training Epoch: 6 [11750/13500]\tLoss: 3208.8064\n",
      "Training Epoch: 6 [11800/13500]\tLoss: 3266.3447\n",
      "Training Epoch: 6 [11850/13500]\tLoss: 3156.5142\n",
      "Training Epoch: 6 [11900/13500]\tLoss: 3215.4038\n",
      "Training Epoch: 6 [11950/13500]\tLoss: 3313.5205\n",
      "Training Epoch: 6 [12000/13500]\tLoss: 3434.1328\n",
      "Training Epoch: 6 [12050/13500]\tLoss: 3245.8704\n",
      "Training Epoch: 6 [12100/13500]\tLoss: 3304.5745\n",
      "Training Epoch: 6 [12150/13500]\tLoss: 3132.8333\n",
      "Training Epoch: 6 [12200/13500]\tLoss: 3149.6091\n",
      "Training Epoch: 6 [12250/13500]\tLoss: 3098.7224\n",
      "Training Epoch: 6 [12300/13500]\tLoss: 3159.4314\n",
      "Training Epoch: 6 [12350/13500]\tLoss: 3294.8606\n",
      "Training Epoch: 6 [12400/13500]\tLoss: 3310.9944\n",
      "Training Epoch: 6 [12450/13500]\tLoss: 3343.4036\n",
      "Training Epoch: 6 [12500/13500]\tLoss: 3186.6475\n",
      "Training Epoch: 6 [12550/13500]\tLoss: 3211.9863\n",
      "Training Epoch: 6 [12600/13500]\tLoss: 3407.1360\n",
      "Training Epoch: 6 [12650/13500]\tLoss: 3324.3518\n",
      "Training Epoch: 6 [12700/13500]\tLoss: 3367.3020\n",
      "Training Epoch: 6 [12750/13500]\tLoss: 3213.8391\n",
      "Training Epoch: 6 [12800/13500]\tLoss: 3279.9854\n",
      "Training Epoch: 6 [12850/13500]\tLoss: 3482.3296\n",
      "Training Epoch: 6 [12900/13500]\tLoss: 3180.3206\n",
      "Training Epoch: 6 [12950/13500]\tLoss: 3202.3535\n",
      "Training Epoch: 6 [13000/13500]\tLoss: 3236.7891\n",
      "Training Epoch: 6 [13050/13500]\tLoss: 3115.7793\n",
      "Training Epoch: 6 [13100/13500]\tLoss: 3277.4734\n",
      "Training Epoch: 6 [13150/13500]\tLoss: 3097.3713\n",
      "Training Epoch: 6 [13200/13500]\tLoss: 3314.8320\n",
      "Training Epoch: 6 [13250/13500]\tLoss: 3460.1370\n",
      "Training Epoch: 6 [13300/13500]\tLoss: 3293.6318\n",
      "Training Epoch: 6 [13350/13500]\tLoss: 3227.8430\n",
      "Training Epoch: 6 [13400/13500]\tLoss: 3251.8728\n",
      "Training Epoch: 6 [13450/13500]\tLoss: 3079.6174\n",
      "Training Epoch: 6 [13500/13500]\tLoss: 3441.4233\n",
      "Training Epoch: 6 [1499/1499]\tLoss: 3306.8928\n",
      "Training Epoch: 7 [50/13500]\tLoss: 3276.1511\n",
      "Training Epoch: 7 [100/13500]\tLoss: 3195.0093\n",
      "Training Epoch: 7 [150/13500]\tLoss: 3076.7437\n",
      "Training Epoch: 7 [200/13500]\tLoss: 3190.0947\n",
      "Training Epoch: 7 [250/13500]\tLoss: 3224.9604\n",
      "Training Epoch: 7 [300/13500]\tLoss: 3092.0896\n",
      "Training Epoch: 7 [350/13500]\tLoss: 3390.7563\n",
      "Training Epoch: 7 [400/13500]\tLoss: 3451.7756\n",
      "Training Epoch: 7 [450/13500]\tLoss: 3228.2344\n",
      "Training Epoch: 7 [500/13500]\tLoss: 3109.0269\n",
      "Training Epoch: 7 [550/13500]\tLoss: 3341.1409\n",
      "Training Epoch: 7 [600/13500]\tLoss: 3014.3501\n",
      "Training Epoch: 7 [650/13500]\tLoss: 3231.4570\n",
      "Training Epoch: 7 [700/13500]\tLoss: 3309.8853\n",
      "Training Epoch: 7 [750/13500]\tLoss: 3126.7883\n",
      "Training Epoch: 7 [800/13500]\tLoss: 3150.4465\n",
      "Training Epoch: 7 [850/13500]\tLoss: 3335.3015\n",
      "Training Epoch: 7 [900/13500]\tLoss: 3072.7590\n",
      "Training Epoch: 7 [950/13500]\tLoss: 3142.3562\n",
      "Training Epoch: 7 [1000/13500]\tLoss: 3465.5417\n",
      "Training Epoch: 7 [1050/13500]\tLoss: 3023.5024\n",
      "Training Epoch: 7 [1100/13500]\tLoss: 3474.6436\n",
      "Training Epoch: 7 [1150/13500]\tLoss: 3256.5146\n",
      "Training Epoch: 7 [1200/13500]\tLoss: 3251.4832\n",
      "Training Epoch: 7 [1250/13500]\tLoss: 3195.5024\n",
      "Training Epoch: 7 [1300/13500]\tLoss: 3260.1919\n",
      "Training Epoch: 7 [1350/13500]\tLoss: 3059.7485\n",
      "Training Epoch: 7 [1400/13500]\tLoss: 3138.9673\n",
      "Training Epoch: 7 [1450/13500]\tLoss: 3295.2268\n",
      "Training Epoch: 7 [1500/13500]\tLoss: 3258.5161\n",
      "Training Epoch: 7 [1550/13500]\tLoss: 3351.8298\n",
      "Training Epoch: 7 [1600/13500]\tLoss: 3403.5823\n",
      "Training Epoch: 7 [1650/13500]\tLoss: 3360.9849\n",
      "Training Epoch: 7 [1700/13500]\tLoss: 3124.1853\n",
      "Training Epoch: 7 [1750/13500]\tLoss: 3565.6775\n",
      "Training Epoch: 7 [1800/13500]\tLoss: 3093.7317\n",
      "Training Epoch: 7 [1850/13500]\tLoss: 3424.2134\n",
      "Training Epoch: 7 [1900/13500]\tLoss: 3214.3735\n",
      "Training Epoch: 7 [1950/13500]\tLoss: 3311.4382\n",
      "Training Epoch: 7 [2000/13500]\tLoss: 3116.6440\n",
      "Training Epoch: 7 [2050/13500]\tLoss: 3070.2175\n",
      "Training Epoch: 7 [2100/13500]\tLoss: 3474.3184\n",
      "Training Epoch: 7 [2150/13500]\tLoss: 3291.6104\n",
      "Training Epoch: 7 [2200/13500]\tLoss: 3134.6660\n",
      "Training Epoch: 7 [2250/13500]\tLoss: 3085.8848\n",
      "Training Epoch: 7 [2300/13500]\tLoss: 3171.6321\n",
      "Training Epoch: 7 [2350/13500]\tLoss: 3247.0391\n",
      "Training Epoch: 7 [2400/13500]\tLoss: 3106.3418\n",
      "Training Epoch: 7 [2450/13500]\tLoss: 3425.8586\n",
      "Training Epoch: 7 [2500/13500]\tLoss: 3117.8696\n",
      "Training Epoch: 7 [2550/13500]\tLoss: 3027.2710\n",
      "Training Epoch: 7 [2600/13500]\tLoss: 3307.2166\n",
      "Training Epoch: 7 [2650/13500]\tLoss: 3325.1648\n",
      "Training Epoch: 7 [2700/13500]\tLoss: 2922.7620\n",
      "Training Epoch: 7 [2750/13500]\tLoss: 3056.0649\n",
      "Training Epoch: 7 [2800/13500]\tLoss: 3286.5115\n",
      "Training Epoch: 7 [2850/13500]\tLoss: 3290.8875\n",
      "Training Epoch: 7 [2900/13500]\tLoss: 3058.5322\n",
      "Training Epoch: 7 [2950/13500]\tLoss: 3154.8660\n",
      "Training Epoch: 7 [3000/13500]\tLoss: 3074.9028\n",
      "Training Epoch: 7 [3050/13500]\tLoss: 3251.9031\n",
      "Training Epoch: 7 [3100/13500]\tLoss: 3126.9731\n",
      "Training Epoch: 7 [3150/13500]\tLoss: 2875.0601\n",
      "Training Epoch: 7 [3200/13500]\tLoss: 3277.7205\n",
      "Training Epoch: 7 [3250/13500]\tLoss: 2943.8357\n",
      "Training Epoch: 7 [3300/13500]\tLoss: 3204.5303\n",
      "Training Epoch: 7 [3350/13500]\tLoss: 3117.8247\n",
      "Training Epoch: 7 [3400/13500]\tLoss: 3135.3362\n",
      "Training Epoch: 7 [3450/13500]\tLoss: 3069.9490\n",
      "Training Epoch: 7 [3500/13500]\tLoss: 3126.8254\n",
      "Training Epoch: 7 [3550/13500]\tLoss: 3016.3250\n",
      "Training Epoch: 7 [3600/13500]\tLoss: 3130.2354\n",
      "Training Epoch: 7 [3650/13500]\tLoss: 3203.4482\n",
      "Training Epoch: 7 [3700/13500]\tLoss: 3280.5835\n",
      "Training Epoch: 7 [3750/13500]\tLoss: 2960.1868\n",
      "Training Epoch: 7 [3800/13500]\tLoss: 3138.6975\n",
      "Training Epoch: 7 [3850/13500]\tLoss: 3081.6345\n",
      "Training Epoch: 7 [3900/13500]\tLoss: 3074.9463\n",
      "Training Epoch: 7 [3950/13500]\tLoss: 3131.6414\n",
      "Training Epoch: 7 [4000/13500]\tLoss: 3273.4353\n",
      "Training Epoch: 7 [4050/13500]\tLoss: 3007.9292\n",
      "Training Epoch: 7 [4100/13500]\tLoss: 2960.3618\n",
      "Training Epoch: 7 [4150/13500]\tLoss: 3295.3682\n",
      "Training Epoch: 7 [4200/13500]\tLoss: 3088.5137\n",
      "Training Epoch: 7 [4250/13500]\tLoss: 3282.0571\n",
      "Training Epoch: 7 [4300/13500]\tLoss: 3380.4988\n",
      "Training Epoch: 7 [4350/13500]\tLoss: 3146.6687\n",
      "Training Epoch: 7 [4400/13500]\tLoss: 3119.1411\n",
      "Training Epoch: 7 [4450/13500]\tLoss: 3229.3403\n",
      "Training Epoch: 7 [4500/13500]\tLoss: 2966.4177\n",
      "Training Epoch: 7 [4550/13500]\tLoss: 3112.2214\n",
      "Training Epoch: 7 [4600/13500]\tLoss: 3003.4526\n",
      "Training Epoch: 7 [4650/13500]\tLoss: 3216.0403\n",
      "Training Epoch: 7 [4700/13500]\tLoss: 3158.6377\n",
      "Training Epoch: 7 [4750/13500]\tLoss: 3066.7732\n",
      "Training Epoch: 7 [4800/13500]\tLoss: 3039.2969\n",
      "Training Epoch: 7 [4850/13500]\tLoss: 2961.6094\n",
      "Training Epoch: 7 [4900/13500]\tLoss: 3089.0415\n",
      "Training Epoch: 7 [4950/13500]\tLoss: 2979.6948\n",
      "Training Epoch: 7 [5000/13500]\tLoss: 3058.4478\n",
      "Training Epoch: 7 [5050/13500]\tLoss: 3148.0754\n",
      "Training Epoch: 7 [5100/13500]\tLoss: 2925.2546\n",
      "Training Epoch: 7 [5150/13500]\tLoss: 2877.1257\n",
      "Training Epoch: 7 [5200/13500]\tLoss: 2952.4116\n",
      "Training Epoch: 7 [5250/13500]\tLoss: 3110.1199\n",
      "Training Epoch: 7 [5300/13500]\tLoss: 2759.8787\n",
      "Training Epoch: 7 [5350/13500]\tLoss: 3269.8508\n",
      "Training Epoch: 7 [5400/13500]\tLoss: 3066.2573\n",
      "Training Epoch: 7 [5450/13500]\tLoss: 3067.2729\n",
      "Training Epoch: 7 [5500/13500]\tLoss: 3095.2649\n",
      "Training Epoch: 7 [5550/13500]\tLoss: 3127.1038\n",
      "Training Epoch: 7 [5600/13500]\tLoss: 3130.7346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [5650/13500]\tLoss: 3058.5635\n",
      "Training Epoch: 7 [5700/13500]\tLoss: 3180.8472\n",
      "Training Epoch: 7 [5750/13500]\tLoss: 3108.5073\n",
      "Training Epoch: 7 [5800/13500]\tLoss: 2932.0520\n",
      "Training Epoch: 7 [5850/13500]\tLoss: 3167.2942\n",
      "Training Epoch: 7 [5900/13500]\tLoss: 3044.6606\n",
      "Training Epoch: 7 [5950/13500]\tLoss: 3254.0818\n",
      "Training Epoch: 7 [6000/13500]\tLoss: 3029.4631\n",
      "Training Epoch: 7 [6050/13500]\tLoss: 2798.3708\n",
      "Training Epoch: 7 [6100/13500]\tLoss: 3125.6123\n",
      "Training Epoch: 7 [6150/13500]\tLoss: 3039.3086\n",
      "Training Epoch: 7 [6200/13500]\tLoss: 3217.6418\n",
      "Training Epoch: 7 [6250/13500]\tLoss: 2978.8521\n",
      "Training Epoch: 7 [6300/13500]\tLoss: 3065.5369\n",
      "Training Epoch: 7 [6350/13500]\tLoss: 3090.8845\n",
      "Training Epoch: 7 [6400/13500]\tLoss: 2973.0557\n",
      "Training Epoch: 7 [6450/13500]\tLoss: 2845.1360\n",
      "Training Epoch: 7 [6500/13500]\tLoss: 3052.0674\n",
      "Training Epoch: 7 [6550/13500]\tLoss: 2978.5596\n",
      "Training Epoch: 7 [6600/13500]\tLoss: 3181.9607\n",
      "Training Epoch: 7 [6650/13500]\tLoss: 2992.4983\n",
      "Training Epoch: 7 [6700/13500]\tLoss: 2871.5552\n",
      "Training Epoch: 7 [6750/13500]\tLoss: 2987.7971\n",
      "Training Epoch: 7 [6800/13500]\tLoss: 3157.5227\n",
      "Training Epoch: 7 [6850/13500]\tLoss: 2919.2480\n",
      "Training Epoch: 7 [6900/13500]\tLoss: 2907.2351\n",
      "Training Epoch: 7 [6950/13500]\tLoss: 2966.8655\n",
      "Training Epoch: 7 [7000/13500]\tLoss: 3289.7036\n",
      "Training Epoch: 7 [7050/13500]\tLoss: 3272.2805\n",
      "Training Epoch: 7 [7100/13500]\tLoss: 3085.3853\n",
      "Training Epoch: 7 [7150/13500]\tLoss: 3149.9014\n",
      "Training Epoch: 7 [7200/13500]\tLoss: 2822.5891\n",
      "Training Epoch: 7 [7250/13500]\tLoss: 3054.5876\n",
      "Training Epoch: 7 [7300/13500]\tLoss: 3007.9868\n",
      "Training Epoch: 7 [7350/13500]\tLoss: 3303.2581\n",
      "Training Epoch: 7 [7400/13500]\tLoss: 2922.4102\n",
      "Training Epoch: 7 [7450/13500]\tLoss: 2965.0420\n",
      "Training Epoch: 7 [7500/13500]\tLoss: 3013.8943\n",
      "Training Epoch: 7 [7550/13500]\tLoss: 3121.3904\n",
      "Training Epoch: 7 [7600/13500]\tLoss: 3137.1519\n",
      "Training Epoch: 7 [7650/13500]\tLoss: 3186.7854\n",
      "Training Epoch: 7 [7700/13500]\tLoss: 3002.5125\n",
      "Training Epoch: 7 [7750/13500]\tLoss: 3133.8870\n",
      "Training Epoch: 7 [7800/13500]\tLoss: 3108.3687\n",
      "Training Epoch: 7 [7850/13500]\tLoss: 3087.2314\n",
      "Training Epoch: 7 [7900/13500]\tLoss: 3285.0642\n",
      "Training Epoch: 7 [7950/13500]\tLoss: 3178.7610\n",
      "Training Epoch: 7 [8000/13500]\tLoss: 3118.2661\n",
      "Training Epoch: 7 [8050/13500]\tLoss: 2838.1169\n",
      "Training Epoch: 7 [8100/13500]\tLoss: 3165.8303\n",
      "Training Epoch: 7 [8150/13500]\tLoss: 2872.2468\n",
      "Training Epoch: 7 [8200/13500]\tLoss: 3252.8442\n",
      "Training Epoch: 7 [8250/13500]\tLoss: 3186.2573\n",
      "Training Epoch: 7 [8300/13500]\tLoss: 2986.8823\n",
      "Training Epoch: 7 [8350/13500]\tLoss: 3088.1101\n",
      "Training Epoch: 7 [8400/13500]\tLoss: 3228.3179\n",
      "Training Epoch: 7 [8450/13500]\tLoss: 3012.5876\n",
      "Training Epoch: 7 [8500/13500]\tLoss: 3092.9392\n",
      "Training Epoch: 7 [8550/13500]\tLoss: 2977.6882\n",
      "Training Epoch: 7 [8600/13500]\tLoss: 2963.2866\n",
      "Training Epoch: 7 [8650/13500]\tLoss: 3048.7324\n",
      "Training Epoch: 7 [8700/13500]\tLoss: 2903.1899\n",
      "Training Epoch: 7 [8750/13500]\tLoss: 2940.2249\n",
      "Training Epoch: 7 [8800/13500]\tLoss: 2877.0691\n",
      "Training Epoch: 7 [8850/13500]\tLoss: 3366.0417\n",
      "Training Epoch: 7 [8900/13500]\tLoss: 2820.1616\n",
      "Training Epoch: 7 [8950/13500]\tLoss: 2836.0444\n",
      "Training Epoch: 7 [9000/13500]\tLoss: 2676.0737\n",
      "Training Epoch: 7 [9050/13500]\tLoss: 2961.6689\n",
      "Training Epoch: 7 [9100/13500]\tLoss: 3024.7173\n",
      "Training Epoch: 7 [9150/13500]\tLoss: 3029.5164\n",
      "Training Epoch: 7 [9200/13500]\tLoss: 2845.9700\n",
      "Training Epoch: 7 [9250/13500]\tLoss: 3004.6697\n",
      "Training Epoch: 7 [9300/13500]\tLoss: 2827.7061\n",
      "Training Epoch: 7 [9350/13500]\tLoss: 2987.3513\n",
      "Training Epoch: 7 [9400/13500]\tLoss: 2973.0327\n",
      "Training Epoch: 7 [9450/13500]\tLoss: 2784.1628\n",
      "Training Epoch: 7 [9500/13500]\tLoss: 3007.5974\n",
      "Training Epoch: 7 [9550/13500]\tLoss: 2821.5413\n",
      "Training Epoch: 7 [9600/13500]\tLoss: 3011.8213\n",
      "Training Epoch: 7 [9650/13500]\tLoss: 3021.5872\n",
      "Training Epoch: 7 [9700/13500]\tLoss: 3044.1353\n",
      "Training Epoch: 7 [9750/13500]\tLoss: 2861.0759\n",
      "Training Epoch: 7 [9800/13500]\tLoss: 2856.7776\n",
      "Training Epoch: 7 [9850/13500]\tLoss: 2871.2815\n",
      "Training Epoch: 7 [9900/13500]\tLoss: 2962.7495\n",
      "Training Epoch: 7 [9950/13500]\tLoss: 3210.9692\n",
      "Training Epoch: 7 [10000/13500]\tLoss: 3070.7861\n",
      "Training Epoch: 7 [10050/13500]\tLoss: 2958.4365\n",
      "Training Epoch: 7 [10100/13500]\tLoss: 3044.5928\n",
      "Training Epoch: 7 [10150/13500]\tLoss: 2881.8362\n",
      "Training Epoch: 7 [10200/13500]\tLoss: 2885.7366\n",
      "Training Epoch: 7 [10250/13500]\tLoss: 2985.9714\n",
      "Training Epoch: 7 [10300/13500]\tLoss: 2993.0579\n",
      "Training Epoch: 7 [10350/13500]\tLoss: 3130.4531\n",
      "Training Epoch: 7 [10400/13500]\tLoss: 2903.1028\n",
      "Training Epoch: 7 [10450/13500]\tLoss: 3157.1738\n",
      "Training Epoch: 7 [10500/13500]\tLoss: 2743.5381\n",
      "Training Epoch: 7 [10550/13500]\tLoss: 3019.0527\n",
      "Training Epoch: 7 [10600/13500]\tLoss: 2902.9915\n",
      "Training Epoch: 7 [10650/13500]\tLoss: 2948.0979\n",
      "Training Epoch: 7 [10700/13500]\tLoss: 2932.6990\n",
      "Training Epoch: 7 [10750/13500]\tLoss: 3006.2788\n",
      "Training Epoch: 7 [10800/13500]\tLoss: 3034.9739\n",
      "Training Epoch: 7 [10850/13500]\tLoss: 2889.7788\n",
      "Training Epoch: 7 [10900/13500]\tLoss: 2776.8982\n",
      "Training Epoch: 7 [10950/13500]\tLoss: 2904.9287\n",
      "Training Epoch: 7 [11000/13500]\tLoss: 2933.2036\n",
      "Training Epoch: 7 [11050/13500]\tLoss: 2859.4827\n",
      "Training Epoch: 7 [11100/13500]\tLoss: 2744.8167\n",
      "Training Epoch: 7 [11150/13500]\tLoss: 2740.4189\n",
      "Training Epoch: 7 [11200/13500]\tLoss: 2966.1743\n",
      "Training Epoch: 7 [11250/13500]\tLoss: 2832.3853\n",
      "Training Epoch: 7 [11300/13500]\tLoss: 2979.9604\n",
      "Training Epoch: 7 [11350/13500]\tLoss: 2815.6809\n",
      "Training Epoch: 7 [11400/13500]\tLoss: 2841.1418\n",
      "Training Epoch: 7 [11450/13500]\tLoss: 2849.2664\n",
      "Training Epoch: 7 [11500/13500]\tLoss: 2938.4790\n",
      "Training Epoch: 7 [11550/13500]\tLoss: 2821.2800\n",
      "Training Epoch: 7 [11600/13500]\tLoss: 2808.3398\n",
      "Training Epoch: 7 [11650/13500]\tLoss: 2892.5474\n",
      "Training Epoch: 7 [11700/13500]\tLoss: 2775.2539\n",
      "Training Epoch: 7 [11750/13500]\tLoss: 2788.6763\n",
      "Training Epoch: 7 [11800/13500]\tLoss: 2842.4368\n",
      "Training Epoch: 7 [11850/13500]\tLoss: 2743.5066\n",
      "Training Epoch: 7 [11900/13500]\tLoss: 2793.2456\n",
      "Training Epoch: 7 [11950/13500]\tLoss: 2875.6677\n",
      "Training Epoch: 7 [12000/13500]\tLoss: 2971.9897\n",
      "Training Epoch: 7 [12050/13500]\tLoss: 2816.6394\n",
      "Training Epoch: 7 [12100/13500]\tLoss: 2869.0891\n",
      "Training Epoch: 7 [12150/13500]\tLoss: 2726.5679\n",
      "Training Epoch: 7 [12200/13500]\tLoss: 2740.5852\n",
      "Training Epoch: 7 [12250/13500]\tLoss: 2695.0125\n",
      "Training Epoch: 7 [12300/13500]\tLoss: 2748.6357\n",
      "Training Epoch: 7 [12350/13500]\tLoss: 2858.9272\n",
      "Training Epoch: 7 [12400/13500]\tLoss: 2872.7964\n",
      "Training Epoch: 7 [12450/13500]\tLoss: 2896.1277\n",
      "Training Epoch: 7 [12500/13500]\tLoss: 2767.6833\n",
      "Training Epoch: 7 [12550/13500]\tLoss: 2789.4175\n",
      "Training Epoch: 7 [12600/13500]\tLoss: 2951.0264\n",
      "Training Epoch: 7 [12650/13500]\tLoss: 2888.2957\n",
      "Training Epoch: 7 [12700/13500]\tLoss: 2915.4082\n",
      "Training Epoch: 7 [12750/13500]\tLoss: 2791.5542\n",
      "Training Epoch: 7 [12800/13500]\tLoss: 2846.6960\n",
      "Training Epoch: 7 [12850/13500]\tLoss: 3016.6433\n",
      "Training Epoch: 7 [12900/13500]\tLoss: 2761.8103\n",
      "Training Epoch: 7 [12950/13500]\tLoss: 2781.5935\n",
      "Training Epoch: 7 [13000/13500]\tLoss: 2809.8706\n",
      "Training Epoch: 7 [13050/13500]\tLoss: 2712.2480\n",
      "Training Epoch: 7 [13100/13500]\tLoss: 2843.1919\n",
      "Training Epoch: 7 [13150/13500]\tLoss: 2693.1960\n",
      "Training Epoch: 7 [13200/13500]\tLoss: 2878.1707\n",
      "Training Epoch: 7 [13250/13500]\tLoss: 2996.6035\n",
      "Training Epoch: 7 [13300/13500]\tLoss: 2862.3181\n",
      "Training Epoch: 7 [13350/13500]\tLoss: 2803.3120\n",
      "Training Epoch: 7 [13400/13500]\tLoss: 2826.6819\n",
      "Training Epoch: 7 [13450/13500]\tLoss: 2678.0796\n",
      "Training Epoch: 7 [13500/13500]\tLoss: 2981.0854\n",
      "Training Epoch: 7 [1499/1499]\tLoss: 2870.2036\n",
      "Training Epoch: 8 [50/13500]\tLoss: 2849.3389\n",
      "Training Epoch: 8 [100/13500]\tLoss: 2775.0681\n",
      "Training Epoch: 8 [150/13500]\tLoss: 2678.5847\n",
      "Training Epoch: 8 [200/13500]\tLoss: 2771.7905\n",
      "Training Epoch: 8 [250/13500]\tLoss: 2802.4456\n",
      "Training Epoch: 8 [300/13500]\tLoss: 2687.8591\n",
      "Training Epoch: 8 [350/13500]\tLoss: 2943.9434\n",
      "Training Epoch: 8 [400/13500]\tLoss: 2997.0950\n",
      "Training Epoch: 8 [450/13500]\tLoss: 2802.0579\n",
      "Training Epoch: 8 [500/13500]\tLoss: 2702.6750\n",
      "Training Epoch: 8 [550/13500]\tLoss: 2903.0422\n",
      "Training Epoch: 8 [600/13500]\tLoss: 2626.0989\n",
      "Training Epoch: 8 [650/13500]\tLoss: 2808.5850\n",
      "Training Epoch: 8 [700/13500]\tLoss: 2870.0178\n",
      "Training Epoch: 8 [750/13500]\tLoss: 2720.5623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [800/13500]\tLoss: 2736.2917\n",
      "Training Epoch: 8 [850/13500]\tLoss: 2891.9819\n",
      "Training Epoch: 8 [900/13500]\tLoss: 2672.2407\n",
      "Training Epoch: 8 [950/13500]\tLoss: 2732.0371\n",
      "Training Epoch: 8 [1000/13500]\tLoss: 3005.2812\n",
      "Training Epoch: 8 [1050/13500]\tLoss: 2633.5623\n",
      "Training Epoch: 8 [1100/13500]\tLoss: 3009.8625\n",
      "Training Epoch: 8 [1150/13500]\tLoss: 2827.6770\n",
      "Training Epoch: 8 [1200/13500]\tLoss: 2827.1833\n",
      "Training Epoch: 8 [1250/13500]\tLoss: 2777.1135\n",
      "Training Epoch: 8 [1300/13500]\tLoss: 2831.3696\n",
      "Training Epoch: 8 [1350/13500]\tLoss: 2664.0974\n",
      "Training Epoch: 8 [1400/13500]\tLoss: 2731.8206\n",
      "Training Epoch: 8 [1450/13500]\tLoss: 2859.4060\n",
      "Training Epoch: 8 [1500/13500]\tLoss: 2831.8950\n",
      "Training Epoch: 8 [1550/13500]\tLoss: 2912.4888\n",
      "Training Epoch: 8 [1600/13500]\tLoss: 2951.1379\n",
      "Training Epoch: 8 [1650/13500]\tLoss: 2917.7339\n",
      "Training Epoch: 8 [1700/13500]\tLoss: 2716.8528\n",
      "Training Epoch: 8 [1750/13500]\tLoss: 3089.1702\n",
      "Training Epoch: 8 [1800/13500]\tLoss: 2697.4268\n",
      "Training Epoch: 8 [1850/13500]\tLoss: 2970.3745\n",
      "Training Epoch: 8 [1900/13500]\tLoss: 2795.3274\n",
      "Training Epoch: 8 [1950/13500]\tLoss: 2873.1384\n",
      "Training Epoch: 8 [2000/13500]\tLoss: 2715.8638\n",
      "Training Epoch: 8 [2050/13500]\tLoss: 2673.7603\n",
      "Training Epoch: 8 [2100/13500]\tLoss: 3013.4016\n",
      "Training Epoch: 8 [2150/13500]\tLoss: 2858.6133\n",
      "Training Epoch: 8 [2200/13500]\tLoss: 2729.2058\n",
      "Training Epoch: 8 [2250/13500]\tLoss: 2687.9810\n",
      "Training Epoch: 8 [2300/13500]\tLoss: 2760.0376\n",
      "Training Epoch: 8 [2350/13500]\tLoss: 2822.5764\n",
      "Training Epoch: 8 [2400/13500]\tLoss: 2700.7771\n",
      "Training Epoch: 8 [2450/13500]\tLoss: 2970.8479\n",
      "Training Epoch: 8 [2500/13500]\tLoss: 2712.6526\n",
      "Training Epoch: 8 [2550/13500]\tLoss: 2634.9819\n",
      "Training Epoch: 8 [2600/13500]\tLoss: 2871.7671\n",
      "Training Epoch: 8 [2650/13500]\tLoss: 2887.5381\n",
      "Training Epoch: 8 [2700/13500]\tLoss: 2544.3281\n",
      "Training Epoch: 8 [2750/13500]\tLoss: 2657.9995\n",
      "Training Epoch: 8 [2800/13500]\tLoss: 2861.7747\n",
      "Training Epoch: 8 [2850/13500]\tLoss: 2859.8533\n",
      "Training Epoch: 8 [2900/13500]\tLoss: 2668.6604\n",
      "Training Epoch: 8 [2950/13500]\tLoss: 2743.6377\n",
      "Training Epoch: 8 [3000/13500]\tLoss: 2676.1692\n",
      "Training Epoch: 8 [3050/13500]\tLoss: 2827.6213\n",
      "Training Epoch: 8 [3100/13500]\tLoss: 2719.5181\n",
      "Training Epoch: 8 [3150/13500]\tLoss: 2512.1226\n",
      "Training Epoch: 8 [3200/13500]\tLoss: 2850.6616\n",
      "Training Epoch: 8 [3250/13500]\tLoss: 2567.5229\n",
      "Training Epoch: 8 [3300/13500]\tLoss: 2790.7532\n",
      "Training Epoch: 8 [3350/13500]\tLoss: 2715.1558\n",
      "Training Epoch: 8 [3400/13500]\tLoss: 2728.2100\n",
      "Training Epoch: 8 [3450/13500]\tLoss: 2673.5098\n",
      "Training Epoch: 8 [3500/13500]\tLoss: 2720.5933\n",
      "Training Epoch: 8 [3550/13500]\tLoss: 2626.0962\n",
      "Training Epoch: 8 [3600/13500]\tLoss: 2726.8464\n",
      "Training Epoch: 8 [3650/13500]\tLoss: 2784.9800\n",
      "Training Epoch: 8 [3700/13500]\tLoss: 2850.9192\n",
      "Training Epoch: 8 [3750/13500]\tLoss: 2580.8735\n",
      "Training Epoch: 8 [3800/13500]\tLoss: 2731.7488\n",
      "Training Epoch: 8 [3850/13500]\tLoss: 2684.6033\n",
      "Training Epoch: 8 [3900/13500]\tLoss: 2677.9070\n",
      "Training Epoch: 8 [3950/13500]\tLoss: 2727.1440\n",
      "Training Epoch: 8 [4000/13500]\tLoss: 2840.9346\n",
      "Training Epoch: 8 [4050/13500]\tLoss: 2621.3301\n",
      "Training Epoch: 8 [4100/13500]\tLoss: 2582.2039\n",
      "Training Epoch: 8 [4150/13500]\tLoss: 2865.0427\n",
      "Training Epoch: 8 [4200/13500]\tLoss: 2690.8733\n",
      "Training Epoch: 8 [4250/13500]\tLoss: 2849.6426\n",
      "Training Epoch: 8 [4300/13500]\tLoss: 2931.7781\n",
      "Training Epoch: 8 [4350/13500]\tLoss: 2742.0947\n",
      "Training Epoch: 8 [4400/13500]\tLoss: 2717.1086\n",
      "Training Epoch: 8 [4450/13500]\tLoss: 2806.9678\n",
      "Training Epoch: 8 [4500/13500]\tLoss: 2589.4844\n",
      "Training Epoch: 8 [4550/13500]\tLoss: 2712.1377\n",
      "Training Epoch: 8 [4600/13500]\tLoss: 2625.0996\n",
      "Training Epoch: 8 [4650/13500]\tLoss: 2795.3975\n",
      "Training Epoch: 8 [4700/13500]\tLoss: 2749.9302\n",
      "Training Epoch: 8 [4750/13500]\tLoss: 2671.6079\n",
      "Training Epoch: 8 [4800/13500]\tLoss: 2646.0828\n",
      "Training Epoch: 8 [4850/13500]\tLoss: 2585.3423\n",
      "Training Epoch: 8 [4900/13500]\tLoss: 2695.1221\n",
      "Training Epoch: 8 [4950/13500]\tLoss: 2600.8772\n",
      "Training Epoch: 8 [5000/13500]\tLoss: 2664.2988\n",
      "Training Epoch: 8 [5050/13500]\tLoss: 2744.2241\n",
      "Training Epoch: 8 [5100/13500]\tLoss: 2554.1572\n",
      "Training Epoch: 8 [5150/13500]\tLoss: 2514.9148\n",
      "Training Epoch: 8 [5200/13500]\tLoss: 2577.3071\n",
      "Training Epoch: 8 [5250/13500]\tLoss: 2708.0620\n",
      "Training Epoch: 8 [5300/13500]\tLoss: 2416.6096\n",
      "Training Epoch: 8 [5350/13500]\tLoss: 2844.9014\n",
      "Training Epoch: 8 [5400/13500]\tLoss: 2675.8506\n",
      "Training Epoch: 8 [5450/13500]\tLoss: 2670.8887\n",
      "Training Epoch: 8 [5500/13500]\tLoss: 2697.2410\n",
      "Training Epoch: 8 [5550/13500]\tLoss: 2725.3896\n",
      "Training Epoch: 8 [5600/13500]\tLoss: 2727.6956\n",
      "Training Epoch: 8 [5650/13500]\tLoss: 2663.2466\n",
      "Training Epoch: 8 [5700/13500]\tLoss: 2764.6968\n",
      "Training Epoch: 8 [5750/13500]\tLoss: 2707.1777\n",
      "Training Epoch: 8 [5800/13500]\tLoss: 2560.2041\n",
      "Training Epoch: 8 [5850/13500]\tLoss: 2756.9573\n",
      "Training Epoch: 8 [5900/13500]\tLoss: 2655.7241\n",
      "Training Epoch: 8 [5950/13500]\tLoss: 2829.9019\n",
      "Training Epoch: 8 [6000/13500]\tLoss: 2639.8149\n",
      "Training Epoch: 8 [6050/13500]\tLoss: 2447.9990\n",
      "Training Epoch: 8 [6100/13500]\tLoss: 2722.6089\n",
      "Training Epoch: 8 [6150/13500]\tLoss: 2650.3088\n",
      "Training Epoch: 8 [6200/13500]\tLoss: 2797.1655\n",
      "Training Epoch: 8 [6250/13500]\tLoss: 2599.8811\n",
      "Training Epoch: 8 [6300/13500]\tLoss: 2671.0334\n",
      "Training Epoch: 8 [6350/13500]\tLoss: 2692.7961\n",
      "Training Epoch: 8 [6400/13500]\tLoss: 2594.9265\n",
      "Training Epoch: 8 [6450/13500]\tLoss: 2490.1414\n",
      "Training Epoch: 8 [6500/13500]\tLoss: 2659.1311\n",
      "Training Epoch: 8 [6550/13500]\tLoss: 2597.1577\n",
      "Training Epoch: 8 [6600/13500]\tLoss: 2768.5742\n",
      "Training Epoch: 8 [6650/13500]\tLoss: 2608.6848\n",
      "Training Epoch: 8 [6700/13500]\tLoss: 2505.9067\n",
      "Training Epoch: 8 [6750/13500]\tLoss: 2610.2205\n",
      "Training Epoch: 8 [6800/13500]\tLoss: 2747.0930\n",
      "Training Epoch: 8 [6850/13500]\tLoss: 2547.0811\n",
      "Training Epoch: 8 [6900/13500]\tLoss: 2541.4380\n",
      "Training Epoch: 8 [6950/13500]\tLoss: 2589.2520\n",
      "Training Epoch: 8 [7000/13500]\tLoss: 2861.1931\n",
      "Training Epoch: 8 [7050/13500]\tLoss: 2846.7112\n",
      "Training Epoch: 8 [7100/13500]\tLoss: 2689.6848\n",
      "Training Epoch: 8 [7150/13500]\tLoss: 2745.3105\n",
      "Training Epoch: 8 [7200/13500]\tLoss: 2469.9949\n",
      "Training Epoch: 8 [7250/13500]\tLoss: 2666.6553\n",
      "Training Epoch: 8 [7300/13500]\tLoss: 2625.0813\n",
      "Training Epoch: 8 [7350/13500]\tLoss: 2871.9092\n",
      "Training Epoch: 8 [7400/13500]\tLoss: 2551.8054\n",
      "Training Epoch: 8 [7450/13500]\tLoss: 2588.6282\n",
      "Training Epoch: 8 [7500/13500]\tLoss: 2625.3599\n",
      "Training Epoch: 8 [7550/13500]\tLoss: 2721.3496\n",
      "Training Epoch: 8 [7600/13500]\tLoss: 2729.4268\n",
      "Training Epoch: 8 [7650/13500]\tLoss: 2774.6499\n",
      "Training Epoch: 8 [7700/13500]\tLoss: 2621.8186\n",
      "Training Epoch: 8 [7750/13500]\tLoss: 2735.1545\n",
      "Training Epoch: 8 [7800/13500]\tLoss: 2705.7302\n",
      "Training Epoch: 8 [7850/13500]\tLoss: 2689.0012\n",
      "Training Epoch: 8 [7900/13500]\tLoss: 2856.7014\n",
      "Training Epoch: 8 [7950/13500]\tLoss: 2765.5342\n",
      "Training Epoch: 8 [8000/13500]\tLoss: 2716.9712\n",
      "Training Epoch: 8 [8050/13500]\tLoss: 2482.1431\n",
      "Training Epoch: 8 [8100/13500]\tLoss: 2757.5955\n",
      "Training Epoch: 8 [8150/13500]\tLoss: 2510.5503\n",
      "Training Epoch: 8 [8200/13500]\tLoss: 2835.1843\n",
      "Training Epoch: 8 [8250/13500]\tLoss: 2774.9651\n",
      "Training Epoch: 8 [8300/13500]\tLoss: 2610.6687\n",
      "Training Epoch: 8 [8350/13500]\tLoss: 2691.6072\n",
      "Training Epoch: 8 [8400/13500]\tLoss: 2808.1743\n",
      "Training Epoch: 8 [8450/13500]\tLoss: 2628.3452\n",
      "Training Epoch: 8 [8500/13500]\tLoss: 2691.9854\n",
      "Training Epoch: 8 [8550/13500]\tLoss: 2599.9592\n",
      "Training Epoch: 8 [8600/13500]\tLoss: 2587.8350\n",
      "Training Epoch: 8 [8650/13500]\tLoss: 2656.0103\n",
      "Training Epoch: 8 [8700/13500]\tLoss: 2539.0710\n",
      "Training Epoch: 8 [8750/13500]\tLoss: 2570.8911\n",
      "Training Epoch: 8 [8800/13500]\tLoss: 2516.4419\n",
      "Training Epoch: 8 [8850/13500]\tLoss: 2927.3423\n",
      "Training Epoch: 8 [8900/13500]\tLoss: 2469.0662\n",
      "Training Epoch: 8 [8950/13500]\tLoss: 2483.0037\n",
      "Training Epoch: 8 [9000/13500]\tLoss: 2345.9329\n",
      "Training Epoch: 8 [9050/13500]\tLoss: 2581.7131\n",
      "Training Epoch: 8 [9100/13500]\tLoss: 2637.6851\n",
      "Training Epoch: 8 [9150/13500]\tLoss: 2642.3313\n",
      "Training Epoch: 8 [9200/13500]\tLoss: 2493.1355\n",
      "Training Epoch: 8 [9250/13500]\tLoss: 2620.3904\n",
      "Training Epoch: 8 [9300/13500]\tLoss: 2471.9236\n",
      "Training Epoch: 8 [9350/13500]\tLoss: 2609.2314\n",
      "Training Epoch: 8 [9400/13500]\tLoss: 2597.1326\n",
      "Training Epoch: 8 [9450/13500]\tLoss: 2437.5422\n",
      "Training Epoch: 8 [9500/13500]\tLoss: 2623.9570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [9550/13500]\tLoss: 2465.7603\n",
      "Training Epoch: 8 [9600/13500]\tLoss: 2628.9473\n",
      "Training Epoch: 8 [9650/13500]\tLoss: 2634.2578\n",
      "Training Epoch: 8 [9700/13500]\tLoss: 2655.6606\n",
      "Training Epoch: 8 [9750/13500]\tLoss: 2501.1599\n",
      "Training Epoch: 8 [9800/13500]\tLoss: 2497.4153\n",
      "Training Epoch: 8 [9850/13500]\tLoss: 2506.2043\n",
      "Training Epoch: 8 [9900/13500]\tLoss: 2589.0186\n",
      "Training Epoch: 8 [9950/13500]\tLoss: 2794.9573\n",
      "Training Epoch: 8 [10000/13500]\tLoss: 2677.1113\n",
      "Training Epoch: 8 [10050/13500]\tLoss: 2586.7932\n",
      "Training Epoch: 8 [10100/13500]\tLoss: 2656.8501\n",
      "Training Epoch: 8 [10150/13500]\tLoss: 2518.1331\n",
      "Training Epoch: 8 [10200/13500]\tLoss: 2523.5430\n",
      "Training Epoch: 8 [10250/13500]\tLoss: 2606.1902\n",
      "Training Epoch: 8 [10300/13500]\tLoss: 2610.7122\n",
      "Training Epoch: 8 [10350/13500]\tLoss: 2725.8762\n",
      "Training Epoch: 8 [10400/13500]\tLoss: 2535.3657\n",
      "Training Epoch: 8 [10450/13500]\tLoss: 2752.3438\n",
      "Training Epoch: 8 [10500/13500]\tLoss: 2403.6050\n",
      "Training Epoch: 8 [10550/13500]\tLoss: 2631.5254\n",
      "Training Epoch: 8 [10600/13500]\tLoss: 2539.5056\n",
      "Training Epoch: 8 [10650/13500]\tLoss: 2573.8582\n",
      "Training Epoch: 8 [10700/13500]\tLoss: 2562.3958\n",
      "Training Epoch: 8 [10750/13500]\tLoss: 2621.7935\n",
      "Training Epoch: 8 [10800/13500]\tLoss: 2647.5913\n",
      "Training Epoch: 8 [10850/13500]\tLoss: 2522.9675\n",
      "Training Epoch: 8 [10900/13500]\tLoss: 2433.5437\n",
      "Training Epoch: 8 [10950/13500]\tLoss: 2540.4604\n",
      "Training Epoch: 8 [11000/13500]\tLoss: 2558.4282\n",
      "Training Epoch: 8 [11050/13500]\tLoss: 2497.8535\n",
      "Training Epoch: 8 [11100/13500]\tLoss: 2406.7507\n",
      "Training Epoch: 8 [11150/13500]\tLoss: 2401.2385\n",
      "Training Epoch: 8 [11200/13500]\tLoss: 2588.9243\n",
      "Training Epoch: 8 [11250/13500]\tLoss: 2478.7471\n",
      "Training Epoch: 8 [11300/13500]\tLoss: 2604.7673\n",
      "Training Epoch: 8 [11350/13500]\tLoss: 2462.2356\n",
      "Training Epoch: 8 [11400/13500]\tLoss: 2486.3821\n",
      "Training Epoch: 8 [11450/13500]\tLoss: 2492.6794\n",
      "Training Epoch: 8 [11500/13500]\tLoss: 2567.7256\n",
      "Training Epoch: 8 [11550/13500]\tLoss: 2470.2485\n",
      "Training Epoch: 8 [11600/13500]\tLoss: 2456.8650\n",
      "Training Epoch: 8 [11650/13500]\tLoss: 2529.1008\n",
      "Training Epoch: 8 [11700/13500]\tLoss: 2432.5464\n",
      "Training Epoch: 8 [11750/13500]\tLoss: 2445.6665\n",
      "Training Epoch: 8 [11800/13500]\tLoss: 2493.3103\n",
      "Training Epoch: 8 [11850/13500]\tLoss: 2407.4053\n",
      "Training Epoch: 8 [11900/13500]\tLoss: 2447.9509\n",
      "Training Epoch: 8 [11950/13500]\tLoss: 2516.3833\n",
      "Training Epoch: 8 [12000/13500]\tLoss: 2593.9302\n",
      "Training Epoch: 8 [12050/13500]\tLoss: 2464.8567\n",
      "Training Epoch: 8 [12100/13500]\tLoss: 2511.3176\n",
      "Training Epoch: 8 [12150/13500]\tLoss: 2393.5200\n",
      "Training Epoch: 8 [12200/13500]\tLoss: 2406.0095\n",
      "Training Epoch: 8 [12250/13500]\tLoss: 2365.9202\n",
      "Training Epoch: 8 [12300/13500]\tLoss: 2411.9705\n",
      "Training Epoch: 8 [12350/13500]\tLoss: 2501.1418\n",
      "Training Epoch: 8 [12400/13500]\tLoss: 2512.6833\n",
      "Training Epoch: 8 [12450/13500]\tLoss: 2529.5347\n",
      "Training Epoch: 8 [12500/13500]\tLoss: 2425.2515\n",
      "Training Epoch: 8 [12550/13500]\tLoss: 2442.2749\n",
      "Training Epoch: 8 [12600/13500]\tLoss: 2575.4158\n",
      "Training Epoch: 8 [12650/13500]\tLoss: 2528.8972\n",
      "Training Epoch: 8 [12700/13500]\tLoss: 2544.7454\n",
      "Training Epoch: 8 [12750/13500]\tLoss: 2445.0945\n",
      "Training Epoch: 8 [12800/13500]\tLoss: 2491.5532\n",
      "Training Epoch: 8 [12850/13500]\tLoss: 2633.5784\n",
      "Training Epoch: 8 [12900/13500]\tLoss: 2418.3489\n",
      "Training Epoch: 8 [12950/13500]\tLoss: 2437.2747\n",
      "Training Epoch: 8 [13000/13500]\tLoss: 2460.9250\n",
      "Training Epoch: 8 [13050/13500]\tLoss: 2381.5220\n",
      "Training Epoch: 8 [13100/13500]\tLoss: 2487.0664\n",
      "Training Epoch: 8 [13150/13500]\tLoss: 2362.9160\n",
      "Training Epoch: 8 [13200/13500]\tLoss: 2517.6223\n",
      "Training Epoch: 8 [13250/13500]\tLoss: 2614.4709\n",
      "Training Epoch: 8 [13300/13500]\tLoss: 2506.8074\n",
      "Training Epoch: 8 [13350/13500]\tLoss: 2454.7517\n",
      "Training Epoch: 8 [13400/13500]\tLoss: 2476.7986\n",
      "Training Epoch: 8 [13450/13500]\tLoss: 2349.3855\n",
      "Training Epoch: 8 [13500/13500]\tLoss: 2601.8286\n",
      "Training Epoch: 8 [1499/1499]\tLoss: 2510.6950\n",
      "Training Epoch: 9 [50/13500]\tLoss: 2495.4854\n",
      "Training Epoch: 9 [100/13500]\tLoss: 2430.4924\n",
      "Training Epoch: 9 [150/13500]\tLoss: 2352.5669\n",
      "Training Epoch: 9 [200/13500]\tLoss: 2427.8962\n",
      "Training Epoch: 9 [250/13500]\tLoss: 2455.0991\n",
      "Training Epoch: 9 [300/13500]\tLoss: 2357.2058\n",
      "Training Epoch: 9 [350/13500]\tLoss: 2575.7119\n",
      "Training Epoch: 9 [400/13500]\tLoss: 2620.9724\n",
      "Training Epoch: 9 [450/13500]\tLoss: 2452.5586\n",
      "Training Epoch: 9 [500/13500]\tLoss: 2370.0105\n",
      "Training Epoch: 9 [550/13500]\tLoss: 2540.5862\n",
      "Training Epoch: 9 [600/13500]\tLoss: 2306.4883\n",
      "Training Epoch: 9 [650/13500]\tLoss: 2460.1973\n",
      "Training Epoch: 9 [700/13500]\tLoss: 2507.5527\n",
      "Training Epoch: 9 [750/13500]\tLoss: 2386.9207\n",
      "Training Epoch: 9 [800/13500]\tLoss: 2396.9158\n",
      "Training Epoch: 9 [850/13500]\tLoss: 2526.5759\n",
      "Training Epoch: 9 [900/13500]\tLoss: 2345.2317\n",
      "Training Epoch: 9 [950/13500]\tLoss: 2394.9326\n",
      "Training Epoch: 9 [1000/13500]\tLoss: 2623.4087\n",
      "Training Epoch: 9 [1050/13500]\tLoss: 2312.6379\n",
      "Training Epoch: 9 [1100/13500]\tLoss: 2624.8611\n",
      "Training Epoch: 9 [1150/13500]\tLoss: 2474.8022\n",
      "Training Epoch: 9 [1200/13500]\tLoss: 2477.1338\n",
      "Training Epoch: 9 [1250/13500]\tLoss: 2432.5881\n",
      "Training Epoch: 9 [1300/13500]\tLoss: 2478.2434\n",
      "Training Epoch: 9 [1350/13500]\tLoss: 2339.3220\n",
      "Training Epoch: 9 [1400/13500]\tLoss: 2395.7417\n",
      "Training Epoch: 9 [1450/13500]\tLoss: 2499.2559\n",
      "Training Epoch: 9 [1500/13500]\tLoss: 2479.3132\n",
      "Training Epoch: 9 [1550/13500]\tLoss: 2547.4138\n",
      "Training Epoch: 9 [1600/13500]\tLoss: 2577.1633\n",
      "Training Epoch: 9 [1650/13500]\tLoss: 2550.3936\n",
      "Training Epoch: 9 [1700/13500]\tLoss: 2381.7395\n",
      "Training Epoch: 9 [1750/13500]\tLoss: 2692.6287\n",
      "Training Epoch: 9 [1800/13500]\tLoss: 2370.9172\n",
      "Training Epoch: 9 [1850/13500]\tLoss: 2594.6526\n",
      "Training Epoch: 9 [1900/13500]\tLoss: 2449.0396\n",
      "Training Epoch: 9 [1950/13500]\tLoss: 2510.2327\n",
      "Training Epoch: 9 [2000/13500]\tLoss: 2384.2000\n",
      "Training Epoch: 9 [2050/13500]\tLoss: 2346.3823\n",
      "Training Epoch: 9 [2100/13500]\tLoss: 2630.8274\n",
      "Training Epoch: 9 [2150/13500]\tLoss: 2501.0986\n",
      "Training Epoch: 9 [2200/13500]\tLoss: 2394.2319\n",
      "Training Epoch: 9 [2250/13500]\tLoss: 2359.9563\n",
      "Training Epoch: 9 [2300/13500]\tLoss: 2419.5928\n",
      "Training Epoch: 9 [2350/13500]\tLoss: 2471.2432\n",
      "Training Epoch: 9 [2400/13500]\tLoss: 2367.3657\n",
      "Training Epoch: 9 [2450/13500]\tLoss: 2593.7722\n",
      "Training Epoch: 9 [2500/13500]\tLoss: 2379.6987\n",
      "Training Epoch: 9 [2550/13500]\tLoss: 2313.0808\n",
      "Training Epoch: 9 [2600/13500]\tLoss: 2510.9612\n",
      "Training Epoch: 9 [2650/13500]\tLoss: 2524.7971\n",
      "Training Epoch: 9 [2700/13500]\tLoss: 2234.8850\n",
      "Training Epoch: 9 [2750/13500]\tLoss: 2330.2275\n",
      "Training Epoch: 9 [2800/13500]\tLoss: 2508.1670\n",
      "Training Epoch: 9 [2850/13500]\tLoss: 2501.8667\n",
      "Training Epoch: 9 [2900/13500]\tLoss: 2346.5422\n",
      "Training Epoch: 9 [2950/13500]\tLoss: 2404.3279\n",
      "Training Epoch: 9 [3000/13500]\tLoss: 2347.3145\n",
      "Training Epoch: 9 [3050/13500]\tLoss: 2475.6470\n",
      "Training Epoch: 9 [3100/13500]\tLoss: 2383.2178\n",
      "Training Epoch: 9 [3150/13500]\tLoss: 2213.3591\n",
      "Training Epoch: 9 [3200/13500]\tLoss: 2496.1860\n",
      "Training Epoch: 9 [3250/13500]\tLoss: 2258.5125\n",
      "Training Epoch: 9 [3300/13500]\tLoss: 2446.7434\n",
      "Training Epoch: 9 [3350/13500]\tLoss: 2381.6116\n",
      "Training Epoch: 9 [3400/13500]\tLoss: 2391.9934\n",
      "Training Epoch: 9 [3450/13500]\tLoss: 2346.6626\n",
      "Training Epoch: 9 [3500/13500]\tLoss: 2385.2139\n",
      "Training Epoch: 9 [3550/13500]\tLoss: 2304.7791\n",
      "Training Epoch: 9 [3600/13500]\tLoss: 2392.6123\n",
      "Training Epoch: 9 [3650/13500]\tLoss: 2438.4319\n",
      "Training Epoch: 9 [3700/13500]\tLoss: 2494.7991\n",
      "Training Epoch: 9 [3750/13500]\tLoss: 2267.3750\n",
      "Training Epoch: 9 [3800/13500]\tLoss: 2393.8562\n",
      "Training Epoch: 9 [3850/13500]\tLoss: 2355.9412\n",
      "Training Epoch: 9 [3900/13500]\tLoss: 2348.5950\n",
      "Training Epoch: 9 [3950/13500]\tLoss: 2391.4583\n",
      "Training Epoch: 9 [4000/13500]\tLoss: 2482.3984\n",
      "Training Epoch: 9 [4050/13500]\tLoss: 2301.8677\n",
      "Training Epoch: 9 [4100/13500]\tLoss: 2270.5205\n",
      "Training Epoch: 9 [4150/13500]\tLoss: 2507.0344\n",
      "Training Epoch: 9 [4200/13500]\tLoss: 2360.8162\n",
      "Training Epoch: 9 [4250/13500]\tLoss: 2490.9070\n",
      "Training Epoch: 9 [4300/13500]\tLoss: 2558.9370\n",
      "Training Epoch: 9 [4350/13500]\tLoss: 2404.8398\n",
      "Training Epoch: 9 [4400/13500]\tLoss: 2383.0142\n",
      "Training Epoch: 9 [4450/13500]\tLoss: 2457.2395\n",
      "Training Epoch: 9 [4500/13500]\tLoss: 2277.4119\n",
      "Training Epoch: 9 [4550/13500]\tLoss: 2379.8064\n",
      "Training Epoch: 9 [4600/13500]\tLoss: 2310.5835\n",
      "Training Epoch: 9 [4650/13500]\tLoss: 2446.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [4700/13500]\tLoss: 2409.8364\n",
      "Training Epoch: 9 [4750/13500]\tLoss: 2344.6880\n",
      "Training Epoch: 9 [4800/13500]\tLoss: 2320.7849\n",
      "Training Epoch: 9 [4850/13500]\tLoss: 2273.7354\n",
      "Training Epoch: 9 [4900/13500]\tLoss: 2366.8396\n",
      "Training Epoch: 9 [4950/13500]\tLoss: 2287.6111\n",
      "Training Epoch: 9 [5000/13500]\tLoss: 2338.5254\n",
      "Training Epoch: 9 [5050/13500]\tLoss: 2407.6787\n",
      "Training Epoch: 9 [5100/13500]\tLoss: 2247.3853\n",
      "Training Epoch: 9 [5150/13500]\tLoss: 2215.4495\n",
      "Training Epoch: 9 [5200/13500]\tLoss: 2265.3997\n",
      "Training Epoch: 9 [5250/13500]\tLoss: 2374.1809\n",
      "Training Epoch: 9 [5300/13500]\tLoss: 2133.6296\n",
      "Training Epoch: 9 [5350/13500]\tLoss: 2490.0471\n",
      "Training Epoch: 9 [5400/13500]\tLoss: 2349.8950\n",
      "Training Epoch: 9 [5450/13500]\tLoss: 2341.5078\n",
      "Training Epoch: 9 [5500/13500]\tLoss: 2366.1089\n",
      "Training Epoch: 9 [5550/13500]\tLoss: 2390.0024\n",
      "Training Epoch: 9 [5600/13500]\tLoss: 2392.5544\n",
      "Training Epoch: 9 [5650/13500]\tLoss: 2334.7312\n",
      "Training Epoch: 9 [5700/13500]\tLoss: 2420.1228\n",
      "Training Epoch: 9 [5750/13500]\tLoss: 2373.6243\n",
      "Training Epoch: 9 [5800/13500]\tLoss: 2251.5491\n",
      "Training Epoch: 9 [5850/13500]\tLoss: 2416.1760\n",
      "Training Epoch: 9 [5900/13500]\tLoss: 2331.8977\n",
      "Training Epoch: 9 [5950/13500]\tLoss: 2476.3284\n",
      "Training Epoch: 9 [6000/13500]\tLoss: 2316.4119\n",
      "Training Epoch: 9 [6050/13500]\tLoss: 2158.0698\n",
      "Training Epoch: 9 [6100/13500]\tLoss: 2387.4204\n",
      "Training Epoch: 9 [6150/13500]\tLoss: 2327.2363\n",
      "Training Epoch: 9 [6200/13500]\tLoss: 2445.9658\n",
      "Training Epoch: 9 [6250/13500]\tLoss: 2284.3044\n",
      "Training Epoch: 9 [6300/13500]\tLoss: 2342.7300\n",
      "Training Epoch: 9 [6350/13500]\tLoss: 2361.8755\n",
      "Training Epoch: 9 [6400/13500]\tLoss: 2280.1038\n",
      "Training Epoch: 9 [6450/13500]\tLoss: 2196.3042\n",
      "Training Epoch: 9 [6500/13500]\tLoss: 2332.9094\n",
      "Training Epoch: 9 [6550/13500]\tLoss: 2280.0232\n",
      "Training Epoch: 9 [6600/13500]\tLoss: 2424.6492\n",
      "Training Epoch: 9 [6650/13500]\tLoss: 2289.2354\n",
      "Training Epoch: 9 [6700/13500]\tLoss: 2203.6660\n",
      "Training Epoch: 9 [6750/13500]\tLoss: 2294.6121\n",
      "Training Epoch: 9 [6800/13500]\tLoss: 2405.5173\n",
      "Training Epoch: 9 [6850/13500]\tLoss: 2237.9563\n",
      "Training Epoch: 9 [6900/13500]\tLoss: 2237.2734\n",
      "Training Epoch: 9 [6950/13500]\tLoss: 2275.2454\n",
      "Training Epoch: 9 [7000/13500]\tLoss: 2503.4307\n",
      "Training Epoch: 9 [7050/13500]\tLoss: 2491.0032\n",
      "Training Epoch: 9 [7100/13500]\tLoss: 2359.6401\n",
      "Training Epoch: 9 [7150/13500]\tLoss: 2406.7979\n",
      "Training Epoch: 9 [7200/13500]\tLoss: 2177.2810\n",
      "Training Epoch: 9 [7250/13500]\tLoss: 2341.3689\n",
      "Training Epoch: 9 [7300/13500]\tLoss: 2305.7473\n",
      "Training Epoch: 9 [7350/13500]\tLoss: 2511.2581\n",
      "Training Epoch: 9 [7400/13500]\tLoss: 2243.7092\n",
      "Training Epoch: 9 [7450/13500]\tLoss: 2274.3628\n",
      "Training Epoch: 9 [7500/13500]\tLoss: 2302.5444\n",
      "Training Epoch: 9 [7550/13500]\tLoss: 2386.3906\n",
      "Training Epoch: 9 [7600/13500]\tLoss: 2389.5872\n",
      "Training Epoch: 9 [7650/13500]\tLoss: 2429.5000\n",
      "Training Epoch: 9 [7700/13500]\tLoss: 2304.3459\n",
      "Training Epoch: 9 [7750/13500]\tLoss: 2401.1279\n",
      "Training Epoch: 9 [7800/13500]\tLoss: 2369.8635\n",
      "Training Epoch: 9 [7850/13500]\tLoss: 2356.6262\n",
      "Training Epoch: 9 [7900/13500]\tLoss: 2497.9629\n",
      "Training Epoch: 9 [7950/13500]\tLoss: 2420.3767\n",
      "Training Epoch: 9 [8000/13500]\tLoss: 2381.7888\n",
      "Training Epoch: 9 [8050/13500]\tLoss: 2186.9758\n",
      "Training Epoch: 9 [8100/13500]\tLoss: 2415.2678\n",
      "Training Epoch: 9 [8150/13500]\tLoss: 2209.6494\n",
      "Training Epoch: 9 [8200/13500]\tLoss: 2483.3567\n",
      "Training Epoch: 9 [8250/13500]\tLoss: 2430.8738\n",
      "Training Epoch: 9 [8300/13500]\tLoss: 2296.1831\n",
      "Training Epoch: 9 [8350/13500]\tLoss: 2360.2129\n",
      "Training Epoch: 9 [8400/13500]\tLoss: 2456.0137\n",
      "Training Epoch: 9 [8450/13500]\tLoss: 2307.5474\n",
      "Training Epoch: 9 [8500/13500]\tLoss: 2357.5100\n",
      "Training Epoch: 9 [8550/13500]\tLoss: 2284.9043\n",
      "Training Epoch: 9 [8600/13500]\tLoss: 2275.3037\n",
      "Training Epoch: 9 [8650/13500]\tLoss: 2328.4624\n",
      "Training Epoch: 9 [8700/13500]\tLoss: 2235.1919\n",
      "Training Epoch: 9 [8750/13500]\tLoss: 2262.2141\n",
      "Training Epoch: 9 [8800/13500]\tLoss: 2215.9524\n",
      "Training Epoch: 9 [8850/13500]\tLoss: 2558.2844\n",
      "Training Epoch: 9 [8900/13500]\tLoss: 2176.5728\n",
      "Training Epoch: 9 [8950/13500]\tLoss: 2188.8210\n",
      "Training Epoch: 9 [9000/13500]\tLoss: 2071.7529\n",
      "Training Epoch: 9 [9050/13500]\tLoss: 2265.3508\n",
      "Training Epoch: 9 [9100/13500]\tLoss: 2314.4409\n",
      "Training Epoch: 9 [9150/13500]\tLoss: 2319.1580\n",
      "Training Epoch: 9 [9200/13500]\tLoss: 2198.3706\n",
      "Training Epoch: 9 [9250/13500]\tLoss: 2300.0115\n",
      "Training Epoch: 9 [9300/13500]\tLoss: 2176.4934\n",
      "Training Epoch: 9 [9350/13500]\tLoss: 2292.3760\n",
      "Training Epoch: 9 [9400/13500]\tLoss: 2283.3511\n",
      "Training Epoch: 9 [9450/13500]\tLoss: 2148.1272\n",
      "Training Epoch: 9 [9500/13500]\tLoss: 2303.4468\n",
      "Training Epoch: 9 [9550/13500]\tLoss: 2170.4504\n",
      "Training Epoch: 9 [9600/13500]\tLoss: 2309.3730\n",
      "Training Epoch: 9 [9650/13500]\tLoss: 2310.2834\n",
      "Training Epoch: 9 [9700/13500]\tLoss: 2330.2402\n",
      "Training Epoch: 9 [9750/13500]\tLoss: 2201.4556\n",
      "Training Epoch: 9 [9800/13500]\tLoss: 2197.7434\n",
      "Training Epoch: 9 [9850/13500]\tLoss: 2202.2217\n",
      "Training Epoch: 9 [9900/13500]\tLoss: 2275.7378\n",
      "Training Epoch: 9 [9950/13500]\tLoss: 2446.4033\n",
      "Training Epoch: 9 [10000/13500]\tLoss: 2346.6321\n",
      "Training Epoch: 9 [10050/13500]\tLoss: 2275.5293\n",
      "Training Epoch: 9 [10100/13500]\tLoss: 2332.1506\n",
      "Training Epoch: 9 [10150/13500]\tLoss: 2214.4480\n",
      "Training Epoch: 9 [10200/13500]\tLoss: 2220.3208\n",
      "Training Epoch: 9 [10250/13500]\tLoss: 2288.8457\n",
      "Training Epoch: 9 [10300/13500]\tLoss: 2291.5000\n",
      "Training Epoch: 9 [10350/13500]\tLoss: 2386.4905\n",
      "Training Epoch: 9 [10400/13500]\tLoss: 2228.8152\n",
      "Training Epoch: 9 [10450/13500]\tLoss: 2411.6584\n",
      "Training Epoch: 9 [10500/13500]\tLoss: 2119.6125\n",
      "Training Epoch: 9 [10550/13500]\tLoss: 2307.7749\n",
      "Training Epoch: 9 [10600/13500]\tLoss: 2235.1123\n",
      "Training Epoch: 9 [10650/13500]\tLoss: 2260.5247\n",
      "Training Epoch: 9 [10700/13500]\tLoss: 2252.5991\n",
      "Training Epoch: 9 [10750/13500]\tLoss: 2299.8716\n",
      "Training Epoch: 9 [10800/13500]\tLoss: 2323.1575\n",
      "Training Epoch: 9 [10850/13500]\tLoss: 2217.0603\n",
      "Training Epoch: 9 [10900/13500]\tLoss: 2147.2822\n",
      "Training Epoch: 9 [10950/13500]\tLoss: 2234.9775\n",
      "Training Epoch: 9 [11000/13500]\tLoss: 2246.3438\n",
      "Training Epoch: 9 [11050/13500]\tLoss: 2195.3569\n",
      "Training Epoch: 9 [11100/13500]\tLoss: 2125.3413\n",
      "Training Epoch: 9 [11150/13500]\tLoss: 2118.7891\n",
      "Training Epoch: 9 [11200/13500]\tLoss: 2272.4656\n",
      "Training Epoch: 9 [11250/13500]\tLoss: 2183.3027\n",
      "Training Epoch: 9 [11300/13500]\tLoss: 2290.2126\n",
      "Training Epoch: 9 [11350/13500]\tLoss: 2166.8047\n",
      "Training Epoch: 9 [11400/13500]\tLoss: 2189.6187\n",
      "Training Epoch: 9 [11450/13500]\tLoss: 2194.6680\n",
      "Training Epoch: 9 [11500/13500]\tLoss: 2256.8433\n",
      "Training Epoch: 9 [11550/13500]\tLoss: 2176.6436\n",
      "Training Epoch: 9 [11600/13500]\tLoss: 2162.9333\n",
      "Training Epoch: 9 [11650/13500]\tLoss: 2225.0264\n",
      "Training Epoch: 9 [11700/13500]\tLoss: 2146.6311\n",
      "Training Epoch: 9 [11750/13500]\tLoss: 2158.7495\n",
      "Training Epoch: 9 [11800/13500]\tLoss: 2199.0879\n",
      "Training Epoch: 9 [11850/13500]\tLoss: 2126.6716\n",
      "Training Epoch: 9 [11900/13500]\tLoss: 2158.8250\n",
      "Training Epoch: 9 [11950/13500]\tLoss: 2214.7324\n",
      "Training Epoch: 9 [12000/13500]\tLoss: 2277.5320\n",
      "Training Epoch: 9 [12050/13500]\tLoss: 2169.9268\n",
      "Training Epoch: 9 [12100/13500]\tLoss: 2210.6755\n",
      "Training Epoch: 9 [12150/13500]\tLoss: 2113.8687\n",
      "Training Epoch: 9 [12200/13500]\tLoss: 2125.7207\n",
      "Training Epoch: 9 [12250/13500]\tLoss: 2090.9028\n",
      "Training Epoch: 9 [12300/13500]\tLoss: 2129.4241\n",
      "Training Epoch: 9 [12350/13500]\tLoss: 2200.7544\n",
      "Training Epoch: 9 [12400/13500]\tLoss: 2210.1594\n",
      "Training Epoch: 9 [12450/13500]\tLoss: 2222.3579\n",
      "Training Epoch: 9 [12500/13500]\tLoss: 2138.6462\n",
      "Training Epoch: 9 [12550/13500]\tLoss: 2150.6340\n",
      "Training Epoch: 9 [12600/13500]\tLoss: 2259.6003\n",
      "Training Epoch: 9 [12650/13500]\tLoss: 2226.2166\n",
      "Training Epoch: 9 [12700/13500]\tLoss: 2234.0588\n",
      "Training Epoch: 9 [12750/13500]\tLoss: 2154.1641\n",
      "Training Epoch: 9 [12800/13500]\tLoss: 2193.9514\n",
      "Training Epoch: 9 [12850/13500]\tLoss: 2311.6829\n",
      "Training Epoch: 9 [12900/13500]\tLoss: 2130.2581\n",
      "Training Epoch: 9 [12950/13500]\tLoss: 2148.8555\n",
      "Training Epoch: 9 [13000/13500]\tLoss: 2169.0874\n",
      "Training Epoch: 9 [13050/13500]\tLoss: 2104.0435\n",
      "Training Epoch: 9 [13100/13500]\tLoss: 2188.4648\n",
      "Training Epoch: 9 [13150/13500]\tLoss: 2086.7749\n",
      "Training Epoch: 9 [13200/13500]\tLoss: 2213.7820\n",
      "Training Epoch: 9 [13250/13500]\tLoss: 2292.9309\n",
      "Training Epoch: 9 [13300/13500]\tLoss: 2207.6787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [13350/13500]\tLoss: 2162.3535\n",
      "Training Epoch: 9 [13400/13500]\tLoss: 2182.6692\n",
      "Training Epoch: 9 [13450/13500]\tLoss: 2074.1396\n",
      "Training Epoch: 9 [13500/13500]\tLoss: 2283.0957\n",
      "Training Epoch: 9 [1499/1499]\tLoss: 2208.4896\n",
      "Training Epoch: 10 [50/13500]\tLoss: 2196.3103\n",
      "Training Epoch: 10 [100/13500]\tLoss: 2141.3936\n",
      "Training Epoch: 10 [150/13500]\tLoss: 2079.2891\n",
      "Training Epoch: 10 [200/13500]\tLoss: 2139.1951\n",
      "Training Epoch: 10 [250/13500]\tLoss: 2163.6016\n",
      "Training Epoch: 10 [300/13500]\tLoss: 2080.6599\n",
      "Training Epoch: 10 [350/13500]\tLoss: 2265.6917\n",
      "Training Epoch: 10 [400/13500]\tLoss: 2303.3562\n",
      "Training Epoch: 10 [450/13500]\tLoss: 2159.7222\n",
      "Training Epoch: 10 [500/13500]\tLoss: 2091.5217\n",
      "Training Epoch: 10 [550/13500]\tLoss: 2234.4626\n",
      "Training Epoch: 10 [600/13500]\tLoss: 2037.8315\n",
      "Training Epoch: 10 [650/13500]\tLoss: 2167.1069\n",
      "Training Epoch: 10 [700/13500]\tLoss: 2202.7495\n",
      "Training Epoch: 10 [750/13500]\tLoss: 2106.9614\n",
      "Training Epoch: 10 [800/13500]\tLoss: 2112.7947\n",
      "Training Epoch: 10 [850/13500]\tLoss: 2219.2717\n",
      "Training Epoch: 10 [900/13500]\tLoss: 2072.1331\n",
      "Training Epoch: 10 [950/13500]\tLoss: 2112.1709\n",
      "Training Epoch: 10 [1000/13500]\tLoss: 2300.8582\n",
      "Training Epoch: 10 [1050/13500]\tLoss: 2042.7822\n",
      "Training Epoch: 10 [1100/13500]\tLoss: 2300.0400\n",
      "Training Epoch: 10 [1150/13500]\tLoss: 2178.4976\n",
      "Training Epoch: 10 [1200/13500]\tLoss: 2182.4265\n",
      "Training Epoch: 10 [1250/13500]\tLoss: 2142.8914\n",
      "Training Epoch: 10 [1300/13500]\tLoss: 2181.5935\n",
      "Training Epoch: 10 [1350/13500]\tLoss: 2066.9934\n",
      "Training Epoch: 10 [1400/13500]\tLoss: 2112.7434\n",
      "Training Epoch: 10 [1450/13500]\tLoss: 2195.8987\n",
      "Training Epoch: 10 [1500/13500]\tLoss: 2182.2305\n",
      "Training Epoch: 10 [1550/13500]\tLoss: 2238.7119\n",
      "Training Epoch: 10 [1600/13500]\tLoss: 2262.2039\n",
      "Training Epoch: 10 [1650/13500]\tLoss: 2240.3052\n",
      "Training Epoch: 10 [1700/13500]\tLoss: 2100.4099\n",
      "Training Epoch: 10 [1750/13500]\tLoss: 2357.2053\n",
      "Training Epoch: 10 [1800/13500]\tLoss: 2096.1792\n",
      "Training Epoch: 10 [1850/13500]\tLoss: 2277.5779\n",
      "Training Epoch: 10 [1900/13500]\tLoss: 2157.4565\n",
      "Training Epoch: 10 [1950/13500]\tLoss: 2204.4038\n",
      "Training Epoch: 10 [2000/13500]\tLoss: 2104.4211\n",
      "Training Epoch: 10 [2050/13500]\tLoss: 2070.6868\n",
      "Training Epoch: 10 [2100/13500]\tLoss: 2307.7830\n",
      "Training Epoch: 10 [2150/13500]\tLoss: 2200.3262\n",
      "Training Epoch: 10 [2200/13500]\tLoss: 2112.1304\n",
      "Training Epoch: 10 [2250/13500]\tLoss: 2083.9482\n",
      "Training Epoch: 10 [2300/13500]\tLoss: 2132.8247\n",
      "Training Epoch: 10 [2350/13500]\tLoss: 2175.0903\n",
      "Training Epoch: 10 [2400/13500]\tLoss: 2087.8257\n",
      "Training Epoch: 10 [2450/13500]\tLoss: 2275.8787\n",
      "Training Epoch: 10 [2500/13500]\tLoss: 2100.3374\n",
      "Training Epoch: 10 [2550/13500]\tLoss: 2043.5255\n",
      "Training Epoch: 10 [2600/13500]\tLoss: 2206.6501\n",
      "Training Epoch: 10 [2650/13500]\tLoss: 2218.8835\n",
      "Training Epoch: 10 [2700/13500]\tLoss: 1976.4980\n",
      "Training Epoch: 10 [2750/13500]\tLoss: 2054.6868\n",
      "Training Epoch: 10 [2800/13500]\tLoss: 2208.4666\n",
      "Training Epoch: 10 [2850/13500]\tLoss: 2199.4585\n",
      "Training Epoch: 10 [2900/13500]\tLoss: 2075.1248\n",
      "Training Epoch: 10 [2950/13500]\tLoss: 2118.9851\n",
      "Training Epoch: 10 [3000/13500]\tLoss: 2070.8545\n",
      "Training Epoch: 10 [3050/13500]\tLoss: 2178.4849\n",
      "Training Epoch: 10 [3100/13500]\tLoss: 2100.4780\n",
      "Training Epoch: 10 [3150/13500]\tLoss: 1962.2952\n",
      "Training Epoch: 10 [3200/13500]\tLoss: 2196.7329\n",
      "Training Epoch: 10 [3250/13500]\tLoss: 1999.6396\n",
      "Training Epoch: 10 [3300/13500]\tLoss: 2155.9817\n",
      "Training Epoch: 10 [3350/13500]\tLoss: 2100.4248\n",
      "Training Epoch: 10 [3400/13500]\tLoss: 2108.8950\n",
      "Training Epoch: 10 [3450/13500]\tLoss: 2071.9370\n",
      "Training Epoch: 10 [3500/13500]\tLoss: 2103.2222\n",
      "Training Epoch: 10 [3550/13500]\tLoss: 2035.0675\n",
      "Training Epoch: 10 [3600/13500]\tLoss: 2110.7561\n",
      "Training Epoch: 10 [3650/13500]\tLoss: 2146.1904\n",
      "Training Epoch: 10 [3700/13500]\tLoss: 2194.5374\n",
      "Training Epoch: 10 [3750/13500]\tLoss: 2003.5123\n",
      "Training Epoch: 10 [3800/13500]\tLoss: 2108.5198\n",
      "Training Epoch: 10 [3850/13500]\tLoss: 2078.6045\n",
      "Training Epoch: 10 [3900/13500]\tLoss: 2070.7000\n",
      "Training Epoch: 10 [3950/13500]\tLoss: 2107.8838\n",
      "Training Epoch: 10 [4000/13500]\tLoss: 2180.1802\n",
      "Training Epoch: 10 [4050/13500]\tLoss: 2032.9338\n",
      "Training Epoch: 10 [4100/13500]\tLoss: 2008.5630\n",
      "Training Epoch: 10 [4150/13500]\tLoss: 2204.2371\n",
      "Training Epoch: 10 [4200/13500]\tLoss: 2082.1809\n",
      "Training Epoch: 10 [4250/13500]\tLoss: 2188.3979\n",
      "Training Epoch: 10 [4300/13500]\tLoss: 2244.4729\n",
      "Training Epoch: 10 [4350/13500]\tLoss: 2118.9727\n",
      "Training Epoch: 10 [4400/13500]\tLoss: 2100.7820\n",
      "Training Epoch: 10 [4450/13500]\tLoss: 2162.6814\n",
      "Training Epoch: 10 [4500/13500]\tLoss: 2014.2305\n",
      "Training Epoch: 10 [4550/13500]\tLoss: 2099.1665\n",
      "Training Epoch: 10 [4600/13500]\tLoss: 2044.5308\n",
      "Training Epoch: 10 [4650/13500]\tLoss: 2152.7954\n",
      "Training Epoch: 10 [4700/13500]\tLoss: 2122.2827\n",
      "Training Epoch: 10 [4750/13500]\tLoss: 2069.3015\n",
      "Training Epoch: 10 [4800/13500]\tLoss: 2046.9551\n",
      "Training Epoch: 10 [4850/13500]\tLoss: 2011.0088\n",
      "Training Epoch: 10 [4900/13500]\tLoss: 2088.5674\n",
      "Training Epoch: 10 [4950/13500]\tLoss: 2023.6716\n",
      "Training Epoch: 10 [5000/13500]\tLoss: 2064.5947\n",
      "Training Epoch: 10 [5050/13500]\tLoss: 2122.6365\n",
      "Training Epoch: 10 [5100/13500]\tLoss: 1989.1693\n",
      "Training Epoch: 10 [5150/13500]\tLoss: 1963.1067\n",
      "Training Epoch: 10 [5200/13500]\tLoss: 2001.8977\n",
      "Training Epoch: 10 [5250/13500]\tLoss: 2092.2651\n",
      "Training Epoch: 10 [5300/13500]\tLoss: 1895.7457\n",
      "Training Epoch: 10 [5350/13500]\tLoss: 2189.2769\n",
      "Training Epoch: 10 [5400/13500]\tLoss: 2073.4260\n",
      "Training Epoch: 10 [5450/13500]\tLoss: 2063.5027\n",
      "Training Epoch: 10 [5500/13500]\tLoss: 2086.1482\n",
      "Training Epoch: 10 [5550/13500]\tLoss: 2105.7336\n",
      "Training Epoch: 10 [5600/13500]\tLoss: 2109.3833\n",
      "Training Epoch: 10 [5650/13500]\tLoss: 2057.6772\n",
      "Training Epoch: 10 [5700/13500]\tLoss: 2129.9500\n",
      "Training Epoch: 10 [5750/13500]\tLoss: 2091.8948\n",
      "Training Epoch: 10 [5800/13500]\tLoss: 1990.9966\n",
      "Training Epoch: 10 [5850/13500]\tLoss: 2128.5950\n",
      "Training Epoch: 10 [5900/13500]\tLoss: 2057.9346\n",
      "Training Epoch: 10 [5950/13500]\tLoss: 2177.2561\n",
      "Training Epoch: 10 [6000/13500]\tLoss: 2043.6840\n",
      "Training Epoch: 10 [6050/13500]\tLoss: 1913.8987\n",
      "Training Epoch: 10 [6100/13500]\tLoss: 2104.2009\n",
      "Training Epoch: 10 [6150/13500]\tLoss: 2054.7671\n",
      "Training Epoch: 10 [6200/13500]\tLoss: 2148.5918\n",
      "Training Epoch: 10 [6250/13500]\tLoss: 2017.4988\n",
      "Training Epoch: 10 [6300/13500]\tLoss: 2065.3396\n",
      "Training Epoch: 10 [6350/13500]\tLoss: 2082.3081\n",
      "Training Epoch: 10 [6400/13500]\tLoss: 2013.9496\n",
      "Training Epoch: 10 [6450/13500]\tLoss: 1948.7366\n",
      "Training Epoch: 10 [6500/13500]\tLoss: 2057.8921\n",
      "Training Epoch: 10 [6550/13500]\tLoss: 2012.1804\n",
      "Training Epoch: 10 [6600/13500]\tLoss: 2133.9819\n",
      "Training Epoch: 10 [6650/13500]\tLoss: 2019.3218\n",
      "Training Epoch: 10 [6700/13500]\tLoss: 1949.6663\n",
      "Training Epoch: 10 [6750/13500]\tLoss: 2027.1653\n",
      "Training Epoch: 10 [6800/13500]\tLoss: 2117.2466\n",
      "Training Epoch: 10 [6850/13500]\tLoss: 1977.1808\n",
      "Training Epoch: 10 [6900/13500]\tLoss: 1980.3561\n",
      "Training Epoch: 10 [6950/13500]\tLoss: 2009.9357\n",
      "Training Epoch: 10 [7000/13500]\tLoss: 2200.5542\n",
      "Training Epoch: 10 [7050/13500]\tLoss: 2189.8064\n",
      "Training Epoch: 10 [7100/13500]\tLoss: 2080.4507\n",
      "Training Epoch: 10 [7150/13500]\tLoss: 2119.7153\n",
      "Training Epoch: 10 [7200/13500]\tLoss: 1930.1862\n",
      "Training Epoch: 10 [7250/13500]\tLoss: 2064.8599\n",
      "Training Epoch: 10 [7300/13500]\tLoss: 2035.6564\n",
      "Training Epoch: 10 [7350/13500]\tLoss: 2205.8076\n",
      "Training Epoch: 10 [7400/13500]\tLoss: 1983.7808\n",
      "Training Epoch: 10 [7450/13500]\tLoss: 2008.2753\n",
      "Training Epoch: 10 [7500/13500]\tLoss: 2030.2949\n",
      "Training Epoch: 10 [7550/13500]\tLoss: 2102.2478\n",
      "Training Epoch: 10 [7600/13500]\tLoss: 2102.4138\n",
      "Training Epoch: 10 [7650/13500]\tLoss: 2136.9434\n",
      "Training Epoch: 10 [7700/13500]\tLoss: 2035.8008\n",
      "Training Epoch: 10 [7750/13500]\tLoss: 2117.4778\n",
      "Training Epoch: 10 [7800/13500]\tLoss: 2086.0276\n",
      "Training Epoch: 10 [7850/13500]\tLoss: 2075.4446\n",
      "Training Epoch: 10 [7900/13500]\tLoss: 2193.8167\n",
      "Training Epoch: 10 [7950/13500]\tLoss: 2128.4031\n",
      "Training Epoch: 10 [8000/13500]\tLoss: 2098.1172\n",
      "Training Epoch: 10 [8050/13500]\tLoss: 1938.3492\n",
      "Training Epoch: 10 [8100/13500]\tLoss: 2124.7334\n",
      "Training Epoch: 10 [8150/13500]\tLoss: 1955.6139\n",
      "Training Epoch: 10 [8200/13500]\tLoss: 2183.5981\n",
      "Training Epoch: 10 [8250/13500]\tLoss: 2139.1829\n",
      "Training Epoch: 10 [8300/13500]\tLoss: 2029.8151\n",
      "Training Epoch: 10 [8350/13500]\tLoss: 2079.6077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [8400/13500]\tLoss: 2157.1609\n",
      "Training Epoch: 10 [8450/13500]\tLoss: 2036.1162\n",
      "Training Epoch: 10 [8500/13500]\tLoss: 2074.8569\n",
      "Training Epoch: 10 [8550/13500]\tLoss: 2018.3368\n",
      "Training Epoch: 10 [8600/13500]\tLoss: 2011.2611\n",
      "Training Epoch: 10 [8650/13500]\tLoss: 2051.5083\n",
      "Training Epoch: 10 [8700/13500]\tLoss: 1978.0128\n",
      "Training Epoch: 10 [8750/13500]\tLoss: 2000.6646\n",
      "Training Epoch: 10 [8800/13500]\tLoss: 1961.9133\n",
      "Training Epoch: 10 [8850/13500]\tLoss: 2244.6072\n",
      "Training Epoch: 10 [8900/13500]\tLoss: 1929.0596\n",
      "Training Epoch: 10 [8950/13500]\tLoss: 1940.0547\n",
      "Training Epoch: 10 [9000/13500]\tLoss: 1840.6223\n",
      "Training Epoch: 10 [9050/13500]\tLoss: 1998.2638\n",
      "Training Epoch: 10 [9100/13500]\tLoss: 2041.0179\n",
      "Training Epoch: 10 [9150/13500]\tLoss: 2045.8025\n",
      "Training Epoch: 10 [9200/13500]\tLoss: 1948.7565\n",
      "Training Epoch: 10 [9250/13500]\tLoss: 2029.3911\n",
      "Training Epoch: 10 [9300/13500]\tLoss: 1927.6165\n",
      "Training Epoch: 10 [9350/13500]\tLoss: 2023.6263\n",
      "Training Epoch: 10 [9400/13500]\tLoss: 2017.7825\n",
      "Training Epoch: 10 [9450/13500]\tLoss: 1903.2383\n",
      "Training Epoch: 10 [9500/13500]\tLoss: 2032.3846\n",
      "Training Epoch: 10 [9550/13500]\tLoss: 1921.6559\n",
      "Training Epoch: 10 [9600/13500]\tLoss: 2039.0841\n",
      "Training Epoch: 10 [9650/13500]\tLoss: 2035.8396\n",
      "Training Epoch: 10 [9700/13500]\tLoss: 2054.3137\n",
      "Training Epoch: 10 [9750/13500]\tLoss: 1948.3175\n",
      "Training Epoch: 10 [9800/13500]\tLoss: 1944.3544\n",
      "Training Epoch: 10 [9850/13500]\tLoss: 1945.6570\n",
      "Training Epoch: 10 [9900/13500]\tLoss: 2010.0282\n",
      "Training Epoch: 10 [9950/13500]\tLoss: 2151.0933\n",
      "Training Epoch: 10 [10000/13500]\tLoss: 2066.1946\n",
      "Training Epoch: 10 [10050/13500]\tLoss: 2011.4785\n",
      "Training Epoch: 10 [10100/13500]\tLoss: 2057.0269\n",
      "Training Epoch: 10 [10150/13500]\tLoss: 1957.6014\n",
      "Training Epoch: 10 [10200/13500]\tLoss: 1963.3044\n",
      "Training Epoch: 10 [10250/13500]\tLoss: 2020.4482\n",
      "Training Epoch: 10 [10300/13500]\tLoss: 2021.5940\n",
      "Training Epoch: 10 [10350/13500]\tLoss: 2098.5046\n",
      "Training Epoch: 10 [10400/13500]\tLoss: 1970.0068\n",
      "Training Epoch: 10 [10450/13500]\tLoss: 2122.0205\n",
      "Training Epoch: 10 [10500/13500]\tLoss: 1879.1466\n",
      "Training Epoch: 10 [10550/13500]\tLoss: 2033.9215\n",
      "Training Epoch: 10 [10600/13500]\tLoss: 1977.1018\n",
      "Training Epoch: 10 [10650/13500]\tLoss: 1995.0964\n",
      "Training Epoch: 10 [10700/13500]\tLoss: 1990.2848\n",
      "Training Epoch: 10 [10750/13500]\tLoss: 2027.2578\n",
      "Training Epoch: 10 [10800/13500]\tLoss: 2048.1282\n",
      "Training Epoch: 10 [10850/13500]\tLoss: 1958.7836\n",
      "Training Epoch: 10 [10900/13500]\tLoss: 1905.2849\n",
      "Training Epoch: 10 [10950/13500]\tLoss: 1975.9231\n",
      "Training Epoch: 10 [11000/13500]\tLoss: 1983.1678\n",
      "Training Epoch: 10 [11050/13500]\tLoss: 1939.4807\n",
      "Training Epoch: 10 [11100/13500]\tLoss: 1887.8545\n",
      "Training Epoch: 10 [11150/13500]\tLoss: 1880.1989\n",
      "Training Epoch: 10 [11200/13500]\tLoss: 2003.9373\n",
      "Training Epoch: 10 [11250/13500]\tLoss: 1933.3512\n",
      "Training Epoch: 10 [11300/13500]\tLoss: 2023.4067\n",
      "Training Epoch: 10 [11350/13500]\tLoss: 1916.7726\n",
      "Training Epoch: 10 [11400/13500]\tLoss: 1938.4032\n",
      "Training Epoch: 10 [11450/13500]\tLoss: 1942.6671\n",
      "Training Epoch: 10 [11500/13500]\tLoss: 1993.2865\n",
      "Training Epoch: 10 [11550/13500]\tLoss: 1927.8325\n",
      "Training Epoch: 10 [11600/13500]\tLoss: 1914.0999\n",
      "Training Epoch: 10 [11650/13500]\tLoss: 1967.7697\n",
      "Training Epoch: 10 [11700/13500]\tLoss: 1905.0372\n",
      "Training Epoch: 10 [11750/13500]\tLoss: 1915.6350\n",
      "Training Epoch: 10 [11800/13500]\tLoss: 1948.4274\n",
      "Training Epoch: 10 [11850/13500]\tLoss: 1889.0614\n",
      "Training Epoch: 10 [11900/13500]\tLoss: 1913.9825\n",
      "Training Epoch: 10 [11950/13500]\tLoss: 1958.5647\n",
      "Training Epoch: 10 [12000/13500]\tLoss: 2009.5747\n",
      "Training Epoch: 10 [12050/13500]\tLoss: 1920.0607\n",
      "Training Epoch: 10 [12100/13500]\tLoss: 1955.4443\n",
      "Training Epoch: 10 [12150/13500]\tLoss: 1876.2559\n",
      "Training Epoch: 10 [12200/13500]\tLoss: 1888.1578\n",
      "Training Epoch: 10 [12250/13500]\tLoss: 1858.2931\n",
      "Training Epoch: 10 [12300/13500]\tLoss: 1889.6936\n",
      "Training Epoch: 10 [12350/13500]\tLoss: 1945.6674\n",
      "Training Epoch: 10 [12400/13500]\tLoss: 1953.3243\n",
      "Training Epoch: 10 [12450/13500]\tLoss: 1962.3420\n",
      "Training Epoch: 10 [12500/13500]\tLoss: 1895.9036\n",
      "Training Epoch: 10 [12550/13500]\tLoss: 1903.0157\n",
      "Training Epoch: 10 [12600/13500]\tLoss: 1991.6744\n",
      "Training Epoch: 10 [12650/13500]\tLoss: 1968.8721\n",
      "Training Epoch: 10 [12700/13500]\tLoss: 1970.9464\n",
      "Training Epoch: 10 [12750/13500]\tLoss: 1907.1941\n",
      "Training Epoch: 10 [12800/13500]\tLoss: 1941.9132\n",
      "Training Epoch: 10 [12850/13500]\tLoss: 2038.4454\n",
      "Training Epoch: 10 [12900/13500]\tLoss: 1886.0797\n",
      "Training Epoch: 10 [12950/13500]\tLoss: 1904.4395\n",
      "Training Epoch: 10 [13000/13500]\tLoss: 1922.0911\n",
      "Training Epoch: 10 [13050/13500]\tLoss: 1868.7173\n",
      "Training Epoch: 10 [13100/13500]\tLoss: 1935.4227\n",
      "Training Epoch: 10 [13150/13500]\tLoss: 1853.1206\n",
      "Training Epoch: 10 [13200/13500]\tLoss: 1955.3198\n",
      "Training Epoch: 10 [13250/13500]\tLoss: 2019.8644\n",
      "Training Epoch: 10 [13300/13500]\tLoss: 1953.5859\n",
      "Training Epoch: 10 [13350/13500]\tLoss: 1914.5146\n",
      "Training Epoch: 10 [13400/13500]\tLoss: 1932.8457\n",
      "Training Epoch: 10 [13450/13500]\tLoss: 1841.1154\n",
      "Training Epoch: 10 [13500/13500]\tLoss: 2012.8737\n",
      "Training Epoch: 10 [1499/1499]\tLoss: 1951.9671\n",
      "Training Epoch: 11 [50/13500]\tLoss: 1941.3284\n",
      "Training Epoch: 11 [100/13500]\tLoss: 1896.1459\n",
      "Training Epoch: 11 [150/13500]\tLoss: 1847.7778\n",
      "Training Epoch: 11 [200/13500]\tLoss: 1894.3868\n",
      "Training Epoch: 11 [250/13500]\tLoss: 1916.6278\n",
      "Training Epoch: 11 [300/13500]\tLoss: 1846.9009\n",
      "Training Epoch: 11 [350/13500]\tLoss: 2002.1066\n",
      "Training Epoch: 11 [400/13500]\tLoss: 2032.8058\n",
      "Training Epoch: 11 [450/13500]\tLoss: 1911.8918\n",
      "Training Epoch: 11 [500/13500]\tLoss: 1855.8632\n",
      "Training Epoch: 11 [550/13500]\tLoss: 1973.5654\n",
      "Training Epoch: 11 [600/13500]\tLoss: 1809.8457\n",
      "Training Epoch: 11 [650/13500]\tLoss: 1918.2638\n",
      "Training Epoch: 11 [700/13500]\tLoss: 1944.2076\n",
      "Training Epoch: 11 [750/13500]\tLoss: 1869.5936\n",
      "Training Epoch: 11 [800/13500]\tLoss: 1872.4301\n",
      "Training Epoch: 11 [850/13500]\tLoss: 1958.5452\n",
      "Training Epoch: 11 [900/13500]\tLoss: 1841.5114\n",
      "Training Epoch: 11 [950/13500]\tLoss: 1872.7511\n",
      "Training Epoch: 11 [1000/13500]\tLoss: 2026.2734\n",
      "Training Epoch: 11 [1050/13500]\tLoss: 1813.6230\n",
      "Training Epoch: 11 [1100/13500]\tLoss: 2024.0137\n",
      "Training Epoch: 11 [1150/13500]\tLoss: 1927.3469\n",
      "Training Epoch: 11 [1200/13500]\tLoss: 1931.9518\n",
      "Training Epoch: 11 [1250/13500]\tLoss: 1896.9388\n",
      "Training Epoch: 11 [1300/13500]\tLoss: 1930.1438\n",
      "Training Epoch: 11 [1350/13500]\tLoss: 1836.2632\n",
      "Training Epoch: 11 [1400/13500]\tLoss: 1872.3644\n",
      "Training Epoch: 11 [1450/13500]\tLoss: 1938.2084\n",
      "Training Epoch: 11 [1500/13500]\tLoss: 1929.7843\n",
      "Training Epoch: 11 [1550/13500]\tLoss: 1975.7988\n",
      "Training Epoch: 11 [1600/13500]\tLoss: 1994.4492\n",
      "Training Epoch: 11 [1650/13500]\tLoss: 1976.5825\n",
      "Training Epoch: 11 [1700/13500]\tLoss: 1861.9652\n",
      "Training Epoch: 11 [1750/13500]\tLoss: 2071.5393\n",
      "Training Epoch: 11 [1800/13500]\tLoss: 1862.7355\n",
      "Training Epoch: 11 [1850/13500]\tLoss: 2007.9413\n",
      "Training Epoch: 11 [1900/13500]\tLoss: 1909.8573\n",
      "Training Epoch: 11 [1950/13500]\tLoss: 1944.6650\n",
      "Training Epoch: 11 [2000/13500]\tLoss: 1866.5305\n",
      "Training Epoch: 11 [2050/13500]\tLoss: 1836.6796\n",
      "Training Epoch: 11 [2100/13500]\tLoss: 2033.1150\n",
      "Training Epoch: 11 [2150/13500]\tLoss: 1945.0283\n",
      "Training Epoch: 11 [2200/13500]\tLoss: 1872.5294\n",
      "Training Epoch: 11 [2250/13500]\tLoss: 1849.5649\n",
      "Training Epoch: 11 [2300/13500]\tLoss: 1889.4010\n",
      "Training Epoch: 11 [2350/13500]\tLoss: 1923.4817\n",
      "Training Epoch: 11 [2400/13500]\tLoss: 1851.2363\n",
      "Training Epoch: 11 [2450/13500]\tLoss: 2005.8386\n",
      "Training Epoch: 11 [2500/13500]\tLoss: 1863.6912\n",
      "Training Epoch: 11 [2550/13500]\tLoss: 1815.6838\n",
      "Training Epoch: 11 [2600/13500]\tLoss: 1948.1826\n",
      "Training Epoch: 11 [2650/13500]\tLoss: 1959.0378\n",
      "Training Epoch: 11 [2700/13500]\tLoss: 1758.3993\n",
      "Training Epoch: 11 [2750/13500]\tLoss: 1820.8942\n",
      "Training Epoch: 11 [2800/13500]\tLoss: 1952.7231\n",
      "Training Epoch: 11 [2850/13500]\tLoss: 1942.3267\n",
      "Training Epoch: 11 [2900/13500]\tLoss: 1844.3859\n",
      "Training Epoch: 11 [2950/13500]\tLoss: 1877.0093\n",
      "Training Epoch: 11 [3000/13500]\tLoss: 1836.4886\n",
      "Training Epoch: 11 [3050/13500]\tLoss: 1925.9001\n",
      "Training Epoch: 11 [3100/13500]\tLoss: 1860.8724\n",
      "Training Epoch: 11 [3150/13500]\tLoss: 1749.4213\n",
      "Training Epoch: 11 [3200/13500]\tLoss: 1942.1290\n",
      "Training Epoch: 11 [3250/13500]\tLoss: 1780.7094\n",
      "Training Epoch: 11 [3300/13500]\tLoss: 1908.6942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [3350/13500]\tLoss: 1861.6595\n",
      "Training Epoch: 11 [3400/13500]\tLoss: 1868.6370\n",
      "Training Epoch: 11 [3450/13500]\tLoss: 1839.1016\n",
      "Training Epoch: 11 [3500/13500]\tLoss: 1864.1066\n",
      "Training Epoch: 11 [3550/13500]\tLoss: 1806.6506\n",
      "Training Epoch: 11 [3600/13500]\tLoss: 1871.4432\n",
      "Training Epoch: 11 [3650/13500]\tLoss: 1897.9745\n",
      "Training Epoch: 11 [3700/13500]\tLoss: 1939.6031\n",
      "Training Epoch: 11 [3750/13500]\tLoss: 1779.6038\n",
      "Training Epoch: 11 [3800/13500]\tLoss: 1866.0944\n",
      "Training Epoch: 11 [3850/13500]\tLoss: 1842.8286\n",
      "Training Epoch: 11 [3900/13500]\tLoss: 1834.4620\n",
      "Training Epoch: 11 [3950/13500]\tLoss: 1866.6251\n",
      "Training Epoch: 11 [4000/13500]\tLoss: 1923.7565\n",
      "Training Epoch: 11 [4050/13500]\tLoss: 1804.8154\n",
      "Training Epoch: 11 [4100/13500]\tLoss: 1786.3018\n",
      "Training Epoch: 11 [4150/13500]\tLoss: 1946.4731\n",
      "Training Epoch: 11 [4200/13500]\tLoss: 1845.3743\n",
      "Training Epoch: 11 [4250/13500]\tLoss: 1931.7200\n",
      "Training Epoch: 11 [4300/13500]\tLoss: 1977.6515\n",
      "Training Epoch: 11 [4350/13500]\tLoss: 1875.1405\n",
      "Training Epoch: 11 [4400/13500]\tLoss: 1860.8300\n",
      "Training Epoch: 11 [4450/13500]\tLoss: 1912.7994\n",
      "Training Epoch: 11 [4500/13500]\tLoss: 1790.4679\n",
      "Training Epoch: 11 [4550/13500]\tLoss: 1860.7073\n",
      "Training Epoch: 11 [4600/13500]\tLoss: 1817.9233\n",
      "Training Epoch: 11 [4650/13500]\tLoss: 1903.3356\n",
      "Training Epoch: 11 [4700/13500]\tLoss: 1877.7958\n",
      "Training Epoch: 11 [4750/13500]\tLoss: 1835.6156\n",
      "Training Epoch: 11 [4800/13500]\tLoss: 1814.8274\n",
      "Training Epoch: 11 [4850/13500]\tLoss: 1787.8091\n",
      "Training Epoch: 11 [4900/13500]\tLoss: 1851.2596\n",
      "Training Epoch: 11 [4950/13500]\tLoss: 1799.5244\n",
      "Training Epoch: 11 [5000/13500]\tLoss: 1832.6232\n",
      "Training Epoch: 11 [5050/13500]\tLoss: 1879.8214\n",
      "Training Epoch: 11 [5100/13500]\tLoss: 1770.2336\n",
      "Training Epoch: 11 [5150/13500]\tLoss: 1748.7423\n",
      "Training Epoch: 11 [5200/13500]\tLoss: 1777.9709\n",
      "Training Epoch: 11 [5250/13500]\tLoss: 1852.7042\n",
      "Training Epoch: 11 [5300/13500]\tLoss: 1694.0746\n",
      "Training Epoch: 11 [5350/13500]\tLoss: 1933.0533\n",
      "Training Epoch: 11 [5400/13500]\tLoss: 1837.6267\n",
      "Training Epoch: 11 [5450/13500]\tLoss: 1827.5918\n",
      "Training Epoch: 11 [5500/13500]\tLoss: 1848.0259\n",
      "Training Epoch: 11 [5550/13500]\tLoss: 1863.6079\n",
      "Training Epoch: 11 [5600/13500]\tLoss: 1868.6536\n",
      "Training Epoch: 11 [5650/13500]\tLoss: 1822.7249\n",
      "Training Epoch: 11 [5700/13500]\tLoss: 1883.9204\n",
      "Training Epoch: 11 [5750/13500]\tLoss: 1852.4231\n",
      "Training Epoch: 11 [5800/13500]\tLoss: 1769.5425\n",
      "Training Epoch: 11 [5850/13500]\tLoss: 1884.4619\n",
      "Training Epoch: 11 [5900/13500]\tLoss: 1824.8184\n",
      "Training Epoch: 11 [5950/13500]\tLoss: 1922.9332\n",
      "Training Epoch: 11 [6000/13500]\tLoss: 1812.3031\n",
      "Training Epoch: 11 [6050/13500]\tLoss: 1706.7679\n",
      "Training Epoch: 11 [6100/13500]\tLoss: 1863.5308\n",
      "Training Epoch: 11 [6150/13500]\tLoss: 1823.5514\n",
      "Training Epoch: 11 [6200/13500]\tLoss: 1895.8442\n",
      "Training Epoch: 11 [6250/13500]\tLoss: 1790.7310\n",
      "Training Epoch: 11 [6300/13500]\tLoss: 1829.6425\n",
      "Training Epoch: 11 [6350/13500]\tLoss: 1844.7054\n",
      "Training Epoch: 11 [6400/13500]\tLoss: 1787.7408\n",
      "Training Epoch: 11 [6450/13500]\tLoss: 1738.6759\n",
      "Training Epoch: 11 [6500/13500]\tLoss: 1824.7452\n",
      "Training Epoch: 11 [6550/13500]\tLoss: 1784.7511\n",
      "Training Epoch: 11 [6600/13500]\tLoss: 1886.9758\n",
      "Training Epoch: 11 [6650/13500]\tLoss: 1790.1381\n",
      "Training Epoch: 11 [6700/13500]\tLoss: 1734.7224\n",
      "Training Epoch: 11 [6750/13500]\tLoss: 1799.6342\n",
      "Training Epoch: 11 [6800/13500]\tLoss: 1872.7506\n",
      "Training Epoch: 11 [6850/13500]\tLoss: 1755.9393\n",
      "Training Epoch: 11 [6900/13500]\tLoss: 1762.1339\n",
      "Training Epoch: 11 [6950/13500]\tLoss: 1784.4154\n",
      "Training Epoch: 11 [7000/13500]\tLoss: 1943.0184\n",
      "Training Epoch: 11 [7050/13500]\tLoss: 1933.8141\n",
      "Training Epoch: 11 [7100/13500]\tLoss: 1843.1632\n",
      "Training Epoch: 11 [7150/13500]\tLoss: 1875.2502\n",
      "Training Epoch: 11 [7200/13500]\tLoss: 1720.3184\n",
      "Training Epoch: 11 [7250/13500]\tLoss: 1829.0521\n",
      "Training Epoch: 11 [7300/13500]\tLoss: 1806.0895\n",
      "Training Epoch: 11 [7350/13500]\tLoss: 1946.0320\n",
      "Training Epoch: 11 [7400/13500]\tLoss: 1763.3225\n",
      "Training Epoch: 11 [7450/13500]\tLoss: 1782.0051\n",
      "Training Epoch: 11 [7500/13500]\tLoss: 1799.3911\n",
      "Training Epoch: 11 [7550/13500]\tLoss: 1860.2617\n",
      "Training Epoch: 11 [7600/13500]\tLoss: 1858.6934\n",
      "Training Epoch: 11 [7650/13500]\tLoss: 1888.1405\n",
      "Training Epoch: 11 [7700/13500]\tLoss: 1807.5453\n",
      "Training Epoch: 11 [7750/13500]\tLoss: 1875.6525\n",
      "Training Epoch: 11 [7800/13500]\tLoss: 1845.1974\n",
      "Training Epoch: 11 [7850/13500]\tLoss: 1836.5447\n",
      "Training Epoch: 11 [7900/13500]\tLoss: 1935.1898\n",
      "Training Epoch: 11 [7950/13500]\tLoss: 1880.5033\n",
      "Training Epoch: 11 [8000/13500]\tLoss: 1857.0675\n",
      "Training Epoch: 11 [8050/13500]\tLoss: 1727.6049\n",
      "Training Epoch: 11 [8100/13500]\tLoss: 1877.3972\n",
      "Training Epoch: 11 [8150/13500]\tLoss: 1740.1375\n",
      "Training Epoch: 11 [8200/13500]\tLoss: 1927.6201\n",
      "Training Epoch: 11 [8250/13500]\tLoss: 1890.9438\n",
      "Training Epoch: 11 [8300/13500]\tLoss: 1803.3468\n",
      "Training Epoch: 11 [8350/13500]\tLoss: 1841.0388\n",
      "Training Epoch: 11 [8400/13500]\tLoss: 1902.8318\n",
      "Training Epoch: 11 [8450/13500]\tLoss: 1805.4653\n",
      "Training Epoch: 11 [8500/13500]\tLoss: 1835.0001\n",
      "Training Epoch: 11 [8550/13500]\tLoss: 1791.8129\n",
      "Training Epoch: 11 [8600/13500]\tLoss: 1786.9797\n",
      "Training Epoch: 11 [8650/13500]\tLoss: 1816.2751\n",
      "Training Epoch: 11 [8700/13500]\tLoss: 1759.4524\n",
      "Training Epoch: 11 [8750/13500]\tLoss: 1778.0674\n",
      "Training Epoch: 11 [8800/13500]\tLoss: 1746.1414\n",
      "Training Epoch: 11 [8850/13500]\tLoss: 1977.5500\n",
      "Training Epoch: 11 [8900/13500]\tLoss: 1718.4595\n",
      "Training Epoch: 11 [8950/13500]\tLoss: 1728.7153\n",
      "Training Epoch: 11 [9000/13500]\tLoss: 1644.7625\n",
      "Training Epoch: 11 [9050/13500]\tLoss: 1771.7301\n",
      "Training Epoch: 11 [9100/13500]\tLoss: 1808.8972\n",
      "Training Epoch: 11 [9150/13500]\tLoss: 1813.6526\n",
      "Training Epoch: 11 [9200/13500]\tLoss: 1736.4684\n",
      "Training Epoch: 11 [9250/13500]\tLoss: 1799.8293\n",
      "Training Epoch: 11 [9300/13500]\tLoss: 1716.8350\n",
      "Training Epoch: 11 [9350/13500]\tLoss: 1795.0023\n",
      "Training Epoch: 11 [9400/13500]\tLoss: 1792.0839\n",
      "Training Epoch: 11 [9450/13500]\tLoss: 1695.2676\n",
      "Training Epoch: 11 [9500/13500]\tLoss: 1802.3060\n",
      "Training Epoch: 11 [9550/13500]\tLoss: 1710.9648\n",
      "Training Epoch: 11 [9600/13500]\tLoss: 1809.5642\n",
      "Training Epoch: 11 [9650/13500]\tLoss: 1802.5747\n",
      "Training Epoch: 11 [9700/13500]\tLoss: 1819.7537\n",
      "Training Epoch: 11 [9750/13500]\tLoss: 1733.4075\n",
      "Training Epoch: 11 [9800/13500]\tLoss: 1729.1755\n",
      "Training Epoch: 11 [9850/13500]\tLoss: 1728.2296\n",
      "Training Epoch: 11 [9900/13500]\tLoss: 1784.0316\n",
      "Training Epoch: 11 [9950/13500]\tLoss: 1900.1417\n",
      "Training Epoch: 11 [10000/13500]\tLoss: 1827.6327\n",
      "Training Epoch: 11 [10050/13500]\tLoss: 1786.7605\n",
      "Training Epoch: 11 [10100/13500]\tLoss: 1823.1005\n",
      "Training Epoch: 11 [10150/13500]\tLoss: 1739.4312\n",
      "Training Epoch: 11 [10200/13500]\tLoss: 1744.8307\n",
      "Training Epoch: 11 [10250/13500]\tLoss: 1792.6764\n",
      "Training Epoch: 11 [10300/13500]\tLoss: 1792.3589\n",
      "Training Epoch: 11 [10350/13500]\tLoss: 1853.4606\n",
      "Training Epoch: 11 [10400/13500]\tLoss: 1750.7076\n",
      "Training Epoch: 11 [10450/13500]\tLoss: 1875.4178\n",
      "Training Epoch: 11 [10500/13500]\tLoss: 1674.6208\n",
      "Training Epoch: 11 [10550/13500]\tLoss: 1801.4648\n",
      "Training Epoch: 11 [10600/13500]\tLoss: 1757.9315\n",
      "Training Epoch: 11 [10650/13500]\tLoss: 1769.5067\n",
      "Training Epoch: 11 [10700/13500]\tLoss: 1767.3575\n",
      "Training Epoch: 11 [10750/13500]\tLoss: 1795.8019\n",
      "Training Epoch: 11 [10800/13500]\tLoss: 1814.3115\n",
      "Training Epoch: 11 [10850/13500]\tLoss: 1739.8496\n",
      "Training Epoch: 11 [10900/13500]\tLoss: 1699.6246\n",
      "Training Epoch: 11 [10950/13500]\tLoss: 1755.7466\n",
      "Training Epoch: 11 [11000/13500]\tLoss: 1760.4529\n",
      "Training Epoch: 11 [11050/13500]\tLoss: 1722.3934\n",
      "Training Epoch: 11 [11100/13500]\tLoss: 1686.3712\n",
      "Training Epoch: 11 [11150/13500]\tLoss: 1677.8442\n",
      "Training Epoch: 11 [11200/13500]\tLoss: 1775.7462\n",
      "Training Epoch: 11 [11250/13500]\tLoss: 1720.9135\n",
      "Training Epoch: 11 [11300/13500]\tLoss: 1796.2773\n",
      "Training Epoch: 11 [11350/13500]\tLoss: 1704.6356\n",
      "Training Epoch: 11 [11400/13500]\tLoss: 1725.2776\n",
      "Training Epoch: 11 [11450/13500]\tLoss: 1728.8285\n",
      "Training Epoch: 11 [11500/13500]\tLoss: 1769.0502\n",
      "Training Epoch: 11 [11550/13500]\tLoss: 1716.2704\n",
      "Training Epoch: 11 [11600/13500]\tLoss: 1702.9987\n",
      "Training Epoch: 11 [11650/13500]\tLoss: 1749.3704\n",
      "Training Epoch: 11 [11700/13500]\tLoss: 1699.9802\n",
      "Training Epoch: 11 [11750/13500]\tLoss: 1709.0422\n",
      "Training Epoch: 11 [11800/13500]\tLoss: 1734.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [11850/13500]\tLoss: 1687.1229\n",
      "Training Epoch: 11 [11900/13500]\tLoss: 1705.8483\n",
      "Training Epoch: 11 [11950/13500]\tLoss: 1740.6226\n",
      "Training Epoch: 11 [12000/13500]\tLoss: 1782.2686\n",
      "Training Epoch: 11 [12050/13500]\tLoss: 1707.6802\n",
      "Training Epoch: 11 [12100/13500]\tLoss: 1738.1461\n",
      "Training Epoch: 11 [12150/13500]\tLoss: 1673.9547\n",
      "Training Epoch: 11 [12200/13500]\tLoss: 1686.3401\n",
      "Training Epoch: 11 [12250/13500]\tLoss: 1660.8983\n",
      "Training Epoch: 11 [12300/13500]\tLoss: 1685.7106\n",
      "Training Epoch: 11 [12350/13500]\tLoss: 1728.6477\n",
      "Training Epoch: 11 [12400/13500]\tLoss: 1734.9696\n",
      "Training Epoch: 11 [12450/13500]\tLoss: 1741.5621\n",
      "Training Epoch: 11 [12500/13500]\tLoss: 1689.4727\n",
      "Training Epoch: 11 [12550/13500]\tLoss: 1692.6260\n",
      "Training Epoch: 11 [12600/13500]\tLoss: 1764.0852\n",
      "Training Epoch: 11 [12650/13500]\tLoss: 1749.5338\n",
      "Training Epoch: 11 [12700/13500]\tLoss: 1747.6957\n",
      "Training Epoch: 11 [12750/13500]\tLoss: 1697.1925\n",
      "Training Epoch: 11 [12800/13500]\tLoss: 1727.8772\n",
      "Training Epoch: 11 [12850/13500]\tLoss: 1806.1261\n",
      "Training Epoch: 11 [12900/13500]\tLoss: 1678.6812\n",
      "Training Epoch: 11 [12950/13500]\tLoss: 1696.9126\n",
      "Training Epoch: 11 [13000/13500]\tLoss: 1712.5946\n",
      "Training Epoch: 11 [13050/13500]\tLoss: 1668.5746\n",
      "Training Epoch: 11 [13100/13500]\tLoss: 1720.5298\n",
      "Training Epoch: 11 [13150/13500]\tLoss: 1654.8325\n",
      "Training Epoch: 11 [13200/13500]\tLoss: 1735.4235\n",
      "Training Epoch: 11 [13250/13500]\tLoss: 1787.7545\n",
      "Training Epoch: 11 [13300/13500]\tLoss: 1737.4659\n",
      "Training Epoch: 11 [13350/13500]\tLoss: 1704.1718\n",
      "Training Epoch: 11 [13400/13500]\tLoss: 1720.3741\n",
      "Training Epoch: 11 [13450/13500]\tLoss: 1643.2573\n",
      "Training Epoch: 11 [13500/13500]\tLoss: 1783.4612\n",
      "Training Epoch: 11 [1499/1499]\tLoss: 1734.0032\n",
      "Training Epoch: 12 [50/13500]\tLoss: 1724.2341\n",
      "Training Epoch: 12 [100/13500]\tLoss: 1687.7126\n",
      "Training Epoch: 12 [150/13500]\tLoss: 1650.9832\n",
      "Training Epoch: 12 [200/13500]\tLoss: 1686.5013\n",
      "Training Epoch: 12 [250/13500]\tLoss: 1707.1641\n",
      "Training Epoch: 12 [300/13500]\tLoss: 1648.8568\n",
      "Training Epoch: 12 [350/13500]\tLoss: 1777.6554\n",
      "Training Epoch: 12 [400/13500]\tLoss: 1802.2942\n",
      "Training Epoch: 12 [450/13500]\tLoss: 1701.8214\n",
      "Training Epoch: 12 [500/13500]\tLoss: 1655.8580\n",
      "Training Epoch: 12 [550/13500]\tLoss: 1751.0699\n",
      "Training Epoch: 12 [600/13500]\tLoss: 1616.2363\n",
      "Training Epoch: 12 [650/13500]\tLoss: 1706.9156\n",
      "Training Epoch: 12 [700/13500]\tLoss: 1724.6693\n",
      "Training Epoch: 12 [750/13500]\tLoss: 1667.9113\n",
      "Training Epoch: 12 [800/13500]\tLoss: 1668.7190\n",
      "Training Epoch: 12 [850/13500]\tLoss: 1737.3395\n",
      "Training Epoch: 12 [900/13500]\tLoss: 1646.1270\n",
      "Training Epoch: 12 [950/13500]\tLoss: 1669.6465\n",
      "Training Epoch: 12 [1000/13500]\tLoss: 1792.6562\n",
      "Training Epoch: 12 [1050/13500]\tLoss: 1618.9277\n",
      "Training Epoch: 12 [1100/13500]\tLoss: 1789.4058\n",
      "Training Epoch: 12 [1150/13500]\tLoss: 1714.0753\n",
      "Training Epoch: 12 [1200/13500]\tLoss: 1718.9331\n",
      "Training Epoch: 12 [1250/13500]\tLoss: 1687.9319\n",
      "Training Epoch: 12 [1300/13500]\tLoss: 1716.6890\n",
      "Training Epoch: 12 [1350/13500]\tLoss: 1640.3440\n",
      "Training Epoch: 12 [1400/13500]\tLoss: 1668.1094\n",
      "Training Epoch: 12 [1450/13500]\tLoss: 1719.2550\n",
      "Training Epoch: 12 [1500/13500]\tLoss: 1715.0669\n",
      "Training Epoch: 12 [1550/13500]\tLoss: 1752.0365\n",
      "Training Epoch: 12 [1600/13500]\tLoss: 1766.8221\n",
      "Training Epoch: 12 [1650/13500]\tLoss: 1752.3058\n",
      "Training Epoch: 12 [1700/13500]\tLoss: 1659.2025\n",
      "Training Epoch: 12 [1750/13500]\tLoss: 1828.5660\n",
      "Training Epoch: 12 [1800/13500]\tLoss: 1664.2316\n",
      "Training Epoch: 12 [1850/13500]\tLoss: 1778.4249\n",
      "Training Epoch: 12 [1900/13500]\tLoss: 1699.3965\n",
      "Training Epoch: 12 [1950/13500]\tLoss: 1724.2271\n",
      "Training Epoch: 12 [2000/13500]\tLoss: 1664.2587\n",
      "Training Epoch: 12 [2050/13500]\tLoss: 1637.7539\n",
      "Training Epoch: 12 [2100/13500]\tLoss: 1799.7367\n",
      "Training Epoch: 12 [2150/13500]\tLoss: 1728.2363\n",
      "Training Epoch: 12 [2200/13500]\tLoss: 1668.9187\n",
      "Training Epoch: 12 [2250/13500]\tLoss: 1650.3290\n",
      "Training Epoch: 12 [2300/13500]\tLoss: 1682.9109\n",
      "Training Epoch: 12 [2350/13500]\tLoss: 1709.7480\n",
      "Training Epoch: 12 [2400/13500]\tLoss: 1650.7059\n",
      "Training Epoch: 12 [2450/13500]\tLoss: 1776.5820\n",
      "Training Epoch: 12 [2500/13500]\tLoss: 1662.9734\n",
      "Training Epoch: 12 [2550/13500]\tLoss: 1622.8905\n",
      "Training Epoch: 12 [2600/13500]\tLoss: 1728.8123\n",
      "Training Epoch: 12 [2650/13500]\tLoss: 1738.4116\n",
      "Training Epoch: 12 [2700/13500]\tLoss: 1573.8594\n",
      "Training Epoch: 12 [2750/13500]\tLoss: 1622.4017\n",
      "Training Epoch: 12 [2800/13500]\tLoss: 1734.7695\n",
      "Training Epoch: 12 [2850/13500]\tLoss: 1723.8694\n",
      "Training Epoch: 12 [2900/13500]\tLoss: 1648.1647\n",
      "Training Epoch: 12 [2950/13500]\tLoss: 1671.7317\n",
      "Training Epoch: 12 [3000/13500]\tLoss: 1637.6025\n",
      "Training Epoch: 12 [3050/13500]\tLoss: 1711.4448\n",
      "Training Epoch: 12 [3100/13500]\tLoss: 1657.8279\n",
      "Training Epoch: 12 [3150/13500]\tLoss: 1568.8121\n",
      "Training Epoch: 12 [3200/13500]\tLoss: 1725.9207\n",
      "Training Epoch: 12 [3250/13500]\tLoss: 1595.2805\n",
      "Training Epoch: 12 [3300/13500]\tLoss: 1698.6576\n",
      "Training Epoch: 12 [3350/13500]\tLoss: 1659.0098\n",
      "Training Epoch: 12 [3400/13500]\tLoss: 1664.8099\n",
      "Training Epoch: 12 [3450/13500]\tLoss: 1641.6868\n",
      "Training Epoch: 12 [3500/13500]\tLoss: 1661.2607\n",
      "Training Epoch: 12 [3550/13500]\tLoss: 1613.0923\n",
      "Training Epoch: 12 [3600/13500]\tLoss: 1668.4287\n",
      "Training Epoch: 12 [3650/13500]\tLoss: 1687.1798\n",
      "Training Epoch: 12 [3700/13500]\tLoss: 1723.2247\n",
      "Training Epoch: 12 [3750/13500]\tLoss: 1589.8428\n",
      "Training Epoch: 12 [3800/13500]\tLoss: 1660.4628\n",
      "Training Epoch: 12 [3850/13500]\tLoss: 1642.3992\n",
      "Training Epoch: 12 [3900/13500]\tLoss: 1633.8058\n",
      "Training Epoch: 12 [3950/13500]\tLoss: 1661.6602\n",
      "Training Epoch: 12 [4000/13500]\tLoss: 1706.5382\n",
      "Training Epoch: 12 [4050/13500]\tLoss: 1611.2953\n",
      "Training Epoch: 12 [4100/13500]\tLoss: 1597.5887\n",
      "Training Epoch: 12 [4150/13500]\tLoss: 1727.3405\n",
      "Training Epoch: 12 [4200/13500]\tLoss: 1644.4131\n",
      "Training Epoch: 12 [4250/13500]\tLoss: 1714.1454\n",
      "Training Epoch: 12 [4300/13500]\tLoss: 1751.5762\n",
      "Training Epoch: 12 [4350/13500]\tLoss: 1667.6382\n",
      "Training Epoch: 12 [4400/13500]\tLoss: 1657.0396\n",
      "Training Epoch: 12 [4450/13500]\tLoss: 1700.8654\n",
      "Training Epoch: 12 [4500/13500]\tLoss: 1600.3453\n",
      "Training Epoch: 12 [4550/13500]\tLoss: 1658.3875\n",
      "Training Epoch: 12 [4600/13500]\tLoss: 1625.1165\n",
      "Training Epoch: 12 [4650/13500]\tLoss: 1691.6315\n",
      "Training Epoch: 12 [4700/13500]\tLoss: 1670.3718\n",
      "Training Epoch: 12 [4750/13500]\tLoss: 1637.3273\n",
      "Training Epoch: 12 [4800/13500]\tLoss: 1618.0530\n",
      "Training Epoch: 12 [4850/13500]\tLoss: 1598.2072\n",
      "Training Epoch: 12 [4900/13500]\tLoss: 1649.2610\n",
      "Training Epoch: 12 [4950/13500]\tLoss: 1609.2350\n",
      "Training Epoch: 12 [5000/13500]\tLoss: 1636.1116\n",
      "Training Epoch: 12 [5050/13500]\tLoss: 1673.3258\n",
      "Training Epoch: 12 [5100/13500]\tLoss: 1584.6208\n",
      "Training Epoch: 12 [5150/13500]\tLoss: 1566.6128\n",
      "Training Epoch: 12 [5200/13500]\tLoss: 1587.9646\n",
      "Training Epoch: 12 [5250/13500]\tLoss: 1649.4174\n",
      "Training Epoch: 12 [5300/13500]\tLoss: 1523.0742\n",
      "Training Epoch: 12 [5350/13500]\tLoss: 1715.1456\n",
      "Training Epoch: 12 [5400/13500]\tLoss: 1637.0419\n",
      "Training Epoch: 12 [5450/13500]\tLoss: 1627.7789\n",
      "Training Epoch: 12 [5500/13500]\tLoss: 1645.8137\n",
      "Training Epoch: 12 [5550/13500]\tLoss: 1657.8965\n",
      "Training Epoch: 12 [5600/13500]\tLoss: 1664.2711\n",
      "Training Epoch: 12 [5650/13500]\tLoss: 1623.7953\n",
      "Training Epoch: 12 [5700/13500]\tLoss: 1675.3878\n",
      "Training Epoch: 12 [5750/13500]\tLoss: 1649.1078\n",
      "Training Epoch: 12 [5800/13500]\tLoss: 1581.5388\n",
      "Training Epoch: 12 [5850/13500]\tLoss: 1677.4960\n",
      "Training Epoch: 12 [5900/13500]\tLoss: 1626.8403\n",
      "Training Epoch: 12 [5950/13500]\tLoss: 1707.1029\n",
      "Training Epoch: 12 [6000/13500]\tLoss: 1616.1874\n",
      "Training Epoch: 12 [6050/13500]\tLoss: 1531.1611\n",
      "Training Epoch: 12 [6100/13500]\tLoss: 1659.2959\n",
      "Training Epoch: 12 [6150/13500]\tLoss: 1627.5947\n",
      "Training Epoch: 12 [6200/13500]\tLoss: 1681.6472\n",
      "Training Epoch: 12 [6250/13500]\tLoss: 1598.3746\n",
      "Training Epoch: 12 [6300/13500]\tLoss: 1629.7511\n",
      "Training Epoch: 12 [6350/13500]\tLoss: 1643.0701\n",
      "Training Epoch: 12 [6400/13500]\tLoss: 1595.8282\n",
      "Training Epoch: 12 [6450/13500]\tLoss: 1560.5192\n",
      "Training Epoch: 12 [6500/13500]\tLoss: 1627.3828\n",
      "Training Epoch: 12 [6550/13500]\tLoss: 1592.0076\n",
      "Training Epoch: 12 [6600/13500]\tLoss: 1677.4221\n",
      "Training Epoch: 12 [6650/13500]\tLoss: 1595.9526\n",
      "Training Epoch: 12 [6700/13500]\tLoss: 1552.8645\n",
      "Training Epoch: 12 [6750/13500]\tLoss: 1606.6205\n",
      "Training Epoch: 12 [6800/13500]\tLoss: 1665.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [6850/13500]\tLoss: 1568.5223\n",
      "Training Epoch: 12 [6900/13500]\tLoss: 1577.0581\n",
      "Training Epoch: 12 [6950/13500]\tLoss: 1592.9443\n",
      "Training Epoch: 12 [7000/13500]\tLoss: 1724.5928\n",
      "Training Epoch: 12 [7050/13500]\tLoss: 1716.8699\n",
      "Training Epoch: 12 [7100/13500]\tLoss: 1641.9529\n",
      "Training Epoch: 12 [7150/13500]\tLoss: 1667.6716\n",
      "Training Epoch: 12 [7200/13500]\tLoss: 1542.3323\n",
      "Training Epoch: 12 [7250/13500]\tLoss: 1628.7114\n",
      "Training Epoch: 12 [7300/13500]\tLoss: 1611.3744\n",
      "Training Epoch: 12 [7350/13500]\tLoss: 1725.7098\n",
      "Training Epoch: 12 [7400/13500]\tLoss: 1576.6295\n",
      "Training Epoch: 12 [7450/13500]\tLoss: 1590.0934\n",
      "Training Epoch: 12 [7500/13500]\tLoss: 1603.7709\n",
      "Training Epoch: 12 [7550/13500]\tLoss: 1654.8380\n",
      "Training Epoch: 12 [7600/13500]\tLoss: 1652.3837\n",
      "Training Epoch: 12 [7650/13500]\tLoss: 1677.1257\n",
      "Training Epoch: 12 [7700/13500]\tLoss: 1613.9709\n",
      "Training Epoch: 12 [7750/13500]\tLoss: 1670.1625\n",
      "Training Epoch: 12 [7800/13500]\tLoss: 1641.3079\n",
      "Training Epoch: 12 [7850/13500]\tLoss: 1634.0636\n",
      "Training Epoch: 12 [7900/13500]\tLoss: 1716.0520\n",
      "Training Epoch: 12 [7950/13500]\tLoss: 1670.5660\n",
      "Training Epoch: 12 [8000/13500]\tLoss: 1652.7950\n",
      "Training Epoch: 12 [8050/13500]\tLoss: 1549.1077\n",
      "Training Epoch: 12 [8100/13500]\tLoss: 1667.6992\n",
      "Training Epoch: 12 [8150/13500]\tLoss: 1557.6570\n",
      "Training Epoch: 12 [8200/13500]\tLoss: 1709.9696\n",
      "Training Epoch: 12 [8250/13500]\tLoss: 1680.3976\n",
      "Training Epoch: 12 [8300/13500]\tLoss: 1611.4269\n",
      "Training Epoch: 12 [8350/13500]\tLoss: 1638.7571\n",
      "Training Epoch: 12 [8400/13500]\tLoss: 1687.1931\n",
      "Training Epoch: 12 [8450/13500]\tLoss: 1609.9728\n",
      "Training Epoch: 12 [8500/13500]\tLoss: 1631.9990\n",
      "Training Epoch: 12 [8550/13500]\tLoss: 1599.7273\n",
      "Training Epoch: 12 [8600/13500]\tLoss: 1596.8135\n",
      "Training Epoch: 12 [8650/13500]\tLoss: 1616.9634\n",
      "Training Epoch: 12 [8700/13500]\tLoss: 1574.1215\n",
      "Training Epoch: 12 [8750/13500]\tLoss: 1589.1030\n",
      "Training Epoch: 12 [8800/13500]\tLoss: 1563.2606\n",
      "Training Epoch: 12 [8850/13500]\tLoss: 1751.2064\n",
      "Training Epoch: 12 [8900/13500]\tLoss: 1539.5712\n",
      "Training Epoch: 12 [8950/13500]\tLoss: 1549.5054\n",
      "Training Epoch: 12 [9000/13500]\tLoss: 1479.1074\n",
      "Training Epoch: 12 [9050/13500]\tLoss: 1579.9581\n",
      "Training Epoch: 12 [9100/13500]\tLoss: 1612.4021\n",
      "Training Epoch: 12 [9150/13500]\tLoss: 1617.0486\n",
      "Training Epoch: 12 [9200/13500]\tLoss: 1556.3680\n",
      "Training Epoch: 12 [9250/13500]\tLoss: 1605.6097\n",
      "Training Epoch: 12 [9300/13500]\tLoss: 1538.5625\n",
      "Training Epoch: 12 [9350/13500]\tLoss: 1601.2538\n",
      "Training Epoch: 12 [9400/13500]\tLoss: 1600.8163\n",
      "Training Epoch: 12 [9450/13500]\tLoss: 1519.1381\n",
      "Training Epoch: 12 [9500/13500]\tLoss: 1607.7018\n",
      "Training Epoch: 12 [9550/13500]\tLoss: 1532.8766\n",
      "Training Epoch: 12 [9600/13500]\tLoss: 1615.1710\n",
      "Training Epoch: 12 [9650/13500]\tLoss: 1605.0231\n",
      "Training Epoch: 12 [9700/13500]\tLoss: 1621.1084\n",
      "Training Epoch: 12 [9750/13500]\tLoss: 1551.3394\n",
      "Training Epoch: 12 [9800/13500]\tLoss: 1547.0436\n",
      "Training Epoch: 12 [9850/13500]\tLoss: 1544.4258\n",
      "Training Epoch: 12 [9900/13500]\tLoss: 1592.5631\n",
      "Training Epoch: 12 [9950/13500]\tLoss: 1687.8301\n",
      "Training Epoch: 12 [10000/13500]\tLoss: 1625.5830\n",
      "Training Epoch: 12 [10050/13500]\tLoss: 1596.1270\n",
      "Training Epoch: 12 [10100/13500]\tLoss: 1624.8678\n",
      "Training Epoch: 12 [10150/13500]\tLoss: 1554.8032\n",
      "Training Epoch: 12 [10200/13500]\tLoss: 1559.7887\n",
      "Training Epoch: 12 [10250/13500]\tLoss: 1599.9556\n",
      "Training Epoch: 12 [10300/13500]\tLoss: 1598.2687\n",
      "Training Epoch: 12 [10350/13500]\tLoss: 1645.8486\n",
      "Training Epoch: 12 [10400/13500]\tLoss: 1565.3154\n",
      "Training Epoch: 12 [10450/13500]\tLoss: 1666.2865\n",
      "Training Epoch: 12 [10500/13500]\tLoss: 1501.2183\n",
      "Training Epoch: 12 [10550/13500]\tLoss: 1604.8071\n",
      "Training Epoch: 12 [10600/13500]\tLoss: 1572.4718\n",
      "Training Epoch: 12 [10650/13500]\tLoss: 1578.3104\n",
      "Training Epoch: 12 [10700/13500]\tLoss: 1578.6284\n",
      "Training Epoch: 12 [10750/13500]\tLoss: 1600.0835\n",
      "Training Epoch: 12 [10800/13500]\tLoss: 1616.0532\n",
      "Training Epoch: 12 [10850/13500]\tLoss: 1554.7070\n",
      "Training Epoch: 12 [10900/13500]\tLoss: 1525.3944\n",
      "Training Epoch: 12 [10950/13500]\tLoss: 1569.4617\n",
      "Training Epoch: 12 [11000/13500]\tLoss: 1572.2610\n",
      "Training Epoch: 12 [11050/13500]\tLoss: 1538.8220\n",
      "Training Epoch: 12 [11100/13500]\tLoss: 1516.0125\n",
      "Training Epoch: 12 [11150/13500]\tLoss: 1506.7056\n",
      "Training Epoch: 12 [11200/13500]\tLoss: 1582.5571\n",
      "Training Epoch: 12 [11250/13500]\tLoss: 1540.7976\n",
      "Training Epoch: 12 [11300/13500]\tLoss: 1603.8153\n",
      "Training Epoch: 12 [11350/13500]\tLoss: 1525.4911\n",
      "Training Epoch: 12 [11400/13500]\tLoss: 1544.9410\n",
      "Training Epoch: 12 [11450/13500]\tLoss: 1547.9525\n",
      "Training Epoch: 12 [11500/13500]\tLoss: 1579.3287\n",
      "Training Epoch: 12 [11550/13500]\tLoss: 1537.0697\n",
      "Training Epoch: 12 [11600/13500]\tLoss: 1524.3148\n",
      "Training Epoch: 12 [11650/13500]\tLoss: 1564.5807\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec27ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
