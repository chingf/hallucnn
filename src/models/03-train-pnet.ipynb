{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_a1 import PBranchedNetwork_A1SeparateHP\n",
    "PNetClass = PBranchedNetwork_A1SeparateHP\n",
    "pnet_name = 'a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv1 import PBranchedNetwork_Conv1SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv1SeparateHP\n",
    "pnet_name = 'conv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'\n",
    "datafile = f'{engram_dir}seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 21 10:15:53 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1D:00.0 Off |                  N/A |\r\n",
      "| 27%   25C    P8    13W / 250W |      3MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_A1SeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CleanSoundsDataset(datafile)\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [64/36450]\tLoss: 7492.5127\n",
      "Training Epoch: 1 [128/36450]\tLoss: 8777.5723\n",
      "Training Epoch: 1 [192/36450]\tLoss: 6675.6455\n",
      "Training Epoch: 1 [256/36450]\tLoss: 7542.8076\n",
      "Training Epoch: 1 [320/36450]\tLoss: 7483.8052\n",
      "Training Epoch: 1 [384/36450]\tLoss: 6024.2134\n",
      "Training Epoch: 1 [448/36450]\tLoss: 5951.9707\n",
      "Training Epoch: 1 [512/36450]\tLoss: 6418.0464\n",
      "Training Epoch: 1 [576/36450]\tLoss: 6349.8696\n",
      "Training Epoch: 1 [640/36450]\tLoss: 5611.2168\n",
      "Training Epoch: 1 [704/36450]\tLoss: 5181.7988\n",
      "Training Epoch: 1 [768/36450]\tLoss: 5816.4316\n",
      "Training Epoch: 1 [832/36450]\tLoss: 5455.4824\n",
      "Training Epoch: 1 [896/36450]\tLoss: 5483.5215\n",
      "Training Epoch: 1 [960/36450]\tLoss: 5065.6382\n",
      "Training Epoch: 1 [1024/36450]\tLoss: 4763.9219\n",
      "Training Epoch: 1 [1088/36450]\tLoss: 5128.7939\n",
      "Training Epoch: 1 [1152/36450]\tLoss: 5030.6118\n",
      "Training Epoch: 1 [1216/36450]\tLoss: 4788.2222\n",
      "Training Epoch: 1 [1280/36450]\tLoss: 4350.8174\n",
      "Training Epoch: 1 [1344/36450]\tLoss: 4531.1528\n",
      "Training Epoch: 1 [1408/36450]\tLoss: 4606.2500\n",
      "Training Epoch: 1 [1472/36450]\tLoss: 4519.6919\n",
      "Training Epoch: 1 [1536/36450]\tLoss: 4236.9653\n",
      "Training Epoch: 1 [1600/36450]\tLoss: 4169.6328\n",
      "Training Epoch: 1 [1664/36450]\tLoss: 4373.4375\n",
      "Training Epoch: 1 [1728/36450]\tLoss: 4225.7178\n",
      "Training Epoch: 1 [1792/36450]\tLoss: 4187.9292\n",
      "Training Epoch: 1 [1856/36450]\tLoss: 4057.3281\n",
      "Training Epoch: 1 [1920/36450]\tLoss: 4033.6121\n",
      "Training Epoch: 1 [1984/36450]\tLoss: 4096.0776\n",
      "Training Epoch: 1 [2048/36450]\tLoss: 4016.1519\n",
      "Training Epoch: 1 [2112/36450]\tLoss: 4046.1863\n",
      "Training Epoch: 1 [2176/36450]\tLoss: 3723.6631\n",
      "Training Epoch: 1 [2240/36450]\tLoss: 3840.3174\n",
      "Training Epoch: 1 [2304/36450]\tLoss: 3692.8477\n",
      "Training Epoch: 1 [2368/36450]\tLoss: 3861.7881\n",
      "Training Epoch: 1 [2432/36450]\tLoss: 3696.5515\n",
      "Training Epoch: 1 [2496/36450]\tLoss: 3461.0232\n",
      "Training Epoch: 1 [2560/36450]\tLoss: 3498.7856\n",
      "Training Epoch: 1 [2624/36450]\tLoss: 3693.6313\n",
      "Training Epoch: 1 [2688/36450]\tLoss: 3527.2732\n",
      "Training Epoch: 1 [2752/36450]\tLoss: 3344.8547\n",
      "Training Epoch: 1 [2816/36450]\tLoss: 3439.5894\n",
      "Training Epoch: 1 [2880/36450]\tLoss: 3458.0134\n",
      "Training Epoch: 1 [2944/36450]\tLoss: 3505.0581\n",
      "Training Epoch: 1 [3008/36450]\tLoss: 3348.0747\n",
      "Training Epoch: 1 [3072/36450]\tLoss: 3274.9504\n",
      "Training Epoch: 1 [3136/36450]\tLoss: 3255.8633\n",
      "Training Epoch: 1 [3200/36450]\tLoss: 3293.8359\n",
      "Training Epoch: 1 [3264/36450]\tLoss: 3155.7026\n",
      "Training Epoch: 1 [3328/36450]\tLoss: 3231.0103\n",
      "Training Epoch: 1 [3392/36450]\tLoss: 3399.1990\n",
      "Training Epoch: 1 [3456/36450]\tLoss: 3176.6665\n",
      "Training Epoch: 1 [3520/36450]\tLoss: 3001.4326\n",
      "Training Epoch: 1 [3584/36450]\tLoss: 3202.6538\n",
      "Training Epoch: 1 [3648/36450]\tLoss: 3008.2112\n",
      "Training Epoch: 1 [3712/36450]\tLoss: 2736.9780\n",
      "Training Epoch: 1 [3776/36450]\tLoss: 2912.3984\n",
      "Training Epoch: 1 [3840/36450]\tLoss: 2998.7783\n",
      "Training Epoch: 1 [3904/36450]\tLoss: 3016.2075\n",
      "Training Epoch: 1 [3968/36450]\tLoss: 2902.0852\n",
      "Training Epoch: 1 [4032/36450]\tLoss: 2822.6938\n",
      "Training Epoch: 1 [4096/36450]\tLoss: 2770.4695\n",
      "Training Epoch: 1 [4160/36450]\tLoss: 2928.1284\n",
      "Training Epoch: 1 [4224/36450]\tLoss: 2861.9429\n",
      "Training Epoch: 1 [4288/36450]\tLoss: 2719.3850\n",
      "Training Epoch: 1 [4352/36450]\tLoss: 2752.0032\n",
      "Training Epoch: 1 [4416/36450]\tLoss: 2561.1533\n",
      "Training Epoch: 1 [4480/36450]\tLoss: 2776.3733\n",
      "Training Epoch: 1 [4544/36450]\tLoss: 2680.2122\n",
      "Training Epoch: 1 [4608/36450]\tLoss: 2519.7629\n",
      "Training Epoch: 1 [4672/36450]\tLoss: 2693.0588\n",
      "Training Epoch: 1 [4736/36450]\tLoss: 2619.1702\n",
      "Training Epoch: 1 [4800/36450]\tLoss: 2483.9131\n",
      "Training Epoch: 1 [4864/36450]\tLoss: 2579.5735\n",
      "Training Epoch: 1 [4928/36450]\tLoss: 2583.7578\n",
      "Training Epoch: 1 [4992/36450]\tLoss: 2487.9768\n",
      "Training Epoch: 1 [5056/36450]\tLoss: 2451.1836\n",
      "Training Epoch: 1 [5120/36450]\tLoss: 2406.9136\n",
      "Training Epoch: 1 [5184/36450]\tLoss: 2450.8193\n",
      "Training Epoch: 1 [5248/36450]\tLoss: 2528.5635\n",
      "Training Epoch: 1 [5312/36450]\tLoss: 2567.2095\n",
      "Training Epoch: 1 [5376/36450]\tLoss: 2540.1543\n",
      "Training Epoch: 1 [5440/36450]\tLoss: 2405.1929\n",
      "Training Epoch: 1 [5504/36450]\tLoss: 2337.5320\n",
      "Training Epoch: 1 [5568/36450]\tLoss: 2371.9424\n",
      "Training Epoch: 1 [5632/36450]\tLoss: 2302.2026\n",
      "Training Epoch: 1 [5696/36450]\tLoss: 2373.3635\n",
      "Training Epoch: 1 [5760/36450]\tLoss: 2304.1824\n",
      "Training Epoch: 1 [5824/36450]\tLoss: 2301.0276\n",
      "Training Epoch: 1 [5888/36450]\tLoss: 2320.6052\n",
      "Training Epoch: 1 [5952/36450]\tLoss: 2270.4915\n",
      "Training Epoch: 1 [6016/36450]\tLoss: 2318.5793\n",
      "Training Epoch: 1 [6080/36450]\tLoss: 2279.5505\n",
      "Training Epoch: 1 [6144/36450]\tLoss: 2345.5962\n",
      "Training Epoch: 1 [6208/36450]\tLoss: 2390.7268\n",
      "Training Epoch: 1 [6272/36450]\tLoss: 2246.7959\n",
      "Training Epoch: 1 [6336/36450]\tLoss: 2407.4011\n",
      "Training Epoch: 1 [6400/36450]\tLoss: 2303.5461\n",
      "Training Epoch: 1 [6464/36450]\tLoss: 2257.7095\n",
      "Training Epoch: 1 [6528/36450]\tLoss: 2299.5745\n",
      "Training Epoch: 1 [6592/36450]\tLoss: 2227.0557\n",
      "Training Epoch: 1 [6656/36450]\tLoss: 2213.6802\n",
      "Training Epoch: 1 [6720/36450]\tLoss: 2190.3657\n",
      "Training Epoch: 1 [6784/36450]\tLoss: 2150.9895\n",
      "Training Epoch: 1 [6848/36450]\tLoss: 2162.4167\n",
      "Training Epoch: 1 [6912/36450]\tLoss: 2213.5562\n",
      "Training Epoch: 1 [6976/36450]\tLoss: 2093.8921\n",
      "Training Epoch: 1 [7040/36450]\tLoss: 2078.6812\n",
      "Training Epoch: 1 [7104/36450]\tLoss: 2093.3174\n",
      "Training Epoch: 1 [7168/36450]\tLoss: 2088.5164\n",
      "Training Epoch: 1 [7232/36450]\tLoss: 2018.9086\n",
      "Training Epoch: 1 [7296/36450]\tLoss: 2048.2935\n",
      "Training Epoch: 1 [7360/36450]\tLoss: 1973.0067\n",
      "Training Epoch: 1 [7424/36450]\tLoss: 2082.6677\n",
      "Training Epoch: 1 [7488/36450]\tLoss: 2111.0706\n",
      "Training Epoch: 1 [7552/36450]\tLoss: 2208.8342\n",
      "Training Epoch: 1 [7616/36450]\tLoss: 2034.3771\n",
      "Training Epoch: 1 [7680/36450]\tLoss: 1991.4440\n",
      "Training Epoch: 1 [7744/36450]\tLoss: 2066.4871\n",
      "Training Epoch: 1 [7808/36450]\tLoss: 1923.4414\n",
      "Training Epoch: 1 [7872/36450]\tLoss: 2014.9546\n",
      "Training Epoch: 1 [7936/36450]\tLoss: 1952.2681\n",
      "Training Epoch: 1 [8000/36450]\tLoss: 2001.8628\n",
      "Training Epoch: 1 [8064/36450]\tLoss: 1974.2880\n",
      "Training Epoch: 1 [8128/36450]\tLoss: 2061.1465\n",
      "Training Epoch: 1 [8192/36450]\tLoss: 1915.3134\n",
      "Training Epoch: 1 [8256/36450]\tLoss: 1853.7312\n",
      "Training Epoch: 1 [8320/36450]\tLoss: 1905.4662\n",
      "Training Epoch: 1 [8384/36450]\tLoss: 1889.3271\n",
      "Training Epoch: 1 [8448/36450]\tLoss: 1848.8514\n",
      "Training Epoch: 1 [8512/36450]\tLoss: 1954.4475\n",
      "Training Epoch: 1 [8576/36450]\tLoss: 1788.3154\n",
      "Training Epoch: 1 [8640/36450]\tLoss: 1827.8346\n",
      "Training Epoch: 1 [8704/36450]\tLoss: 1871.2830\n",
      "Training Epoch: 1 [8768/36450]\tLoss: 1808.5498\n",
      "Training Epoch: 1 [8832/36450]\tLoss: 1728.8987\n",
      "Training Epoch: 1 [8896/36450]\tLoss: 1823.2559\n",
      "Training Epoch: 1 [8960/36450]\tLoss: 1818.0096\n",
      "Training Epoch: 1 [9024/36450]\tLoss: 1812.8842\n",
      "Training Epoch: 1 [9088/36450]\tLoss: 1838.1700\n",
      "Training Epoch: 1 [9152/36450]\tLoss: 1822.2563\n",
      "Training Epoch: 1 [9216/36450]\tLoss: 1813.2972\n",
      "Training Epoch: 1 [9280/36450]\tLoss: 1815.7446\n",
      "Training Epoch: 1 [9344/36450]\tLoss: 1879.1384\n",
      "Training Epoch: 1 [9408/36450]\tLoss: 1775.6516\n",
      "Training Epoch: 1 [9472/36450]\tLoss: 1885.2195\n",
      "Training Epoch: 1 [9536/36450]\tLoss: 1755.5447\n",
      "Training Epoch: 1 [9600/36450]\tLoss: 1818.6962\n",
      "Training Epoch: 1 [9664/36450]\tLoss: 1834.2622\n",
      "Training Epoch: 1 [9728/36450]\tLoss: 1742.6953\n",
      "Training Epoch: 1 [9792/36450]\tLoss: 1619.1791\n",
      "Training Epoch: 1 [9856/36450]\tLoss: 1706.2261\n",
      "Training Epoch: 1 [9920/36450]\tLoss: 1677.6870\n",
      "Training Epoch: 1 [9984/36450]\tLoss: 1771.5590\n",
      "Training Epoch: 1 [10048/36450]\tLoss: 1695.9269\n",
      "Training Epoch: 1 [10112/36450]\tLoss: 1655.0100\n",
      "Training Epoch: 1 [10176/36450]\tLoss: 1620.2307\n",
      "Training Epoch: 1 [10240/36450]\tLoss: 1683.1411\n",
      "Training Epoch: 1 [10304/36450]\tLoss: 1759.4130\n",
      "Training Epoch: 1 [10368/36450]\tLoss: 1794.8148\n",
      "Training Epoch: 1 [10432/36450]\tLoss: 1613.9692\n",
      "Training Epoch: 1 [10496/36450]\tLoss: 1638.3542\n",
      "Training Epoch: 1 [10560/36450]\tLoss: 1655.9866\n",
      "Training Epoch: 1 [10624/36450]\tLoss: 1688.7725\n",
      "Training Epoch: 1 [10688/36450]\tLoss: 1610.4359\n",
      "Training Epoch: 1 [10752/36450]\tLoss: 1562.0992\n",
      "Training Epoch: 1 [10816/36450]\tLoss: 1688.5525\n",
      "Training Epoch: 1 [10880/36450]\tLoss: 1713.0696\n",
      "Training Epoch: 1 [10944/36450]\tLoss: 1660.5549\n",
      "Training Epoch: 1 [11008/36450]\tLoss: 1704.6562\n",
      "Training Epoch: 1 [11072/36450]\tLoss: 1693.2249\n",
      "Training Epoch: 1 [11136/36450]\tLoss: 1709.5393\n",
      "Training Epoch: 1 [11200/36450]\tLoss: 1654.0206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [11264/36450]\tLoss: 1576.4164\n",
      "Training Epoch: 1 [11328/36450]\tLoss: 1596.0035\n",
      "Training Epoch: 1 [11392/36450]\tLoss: 1584.1417\n",
      "Training Epoch: 1 [11456/36450]\tLoss: 1662.4871\n",
      "Training Epoch: 1 [11520/36450]\tLoss: 1491.0220\n",
      "Training Epoch: 1 [11584/36450]\tLoss: 1514.0420\n",
      "Training Epoch: 1 [11648/36450]\tLoss: 1767.6681\n",
      "Training Epoch: 1 [11712/36450]\tLoss: 1498.9977\n",
      "Training Epoch: 1 [11776/36450]\tLoss: 1649.4738\n",
      "Training Epoch: 1 [11840/36450]\tLoss: 1563.4266\n",
      "Training Epoch: 1 [11904/36450]\tLoss: 1546.0212\n",
      "Training Epoch: 1 [11968/36450]\tLoss: 1616.8198\n",
      "Training Epoch: 1 [12032/36450]\tLoss: 1614.4237\n",
      "Training Epoch: 1 [12096/36450]\tLoss: 1626.1426\n",
      "Training Epoch: 1 [12160/36450]\tLoss: 1617.1969\n",
      "Training Epoch: 1 [12224/36450]\tLoss: 1474.4928\n",
      "Training Epoch: 1 [12288/36450]\tLoss: 1601.7637\n",
      "Training Epoch: 1 [12352/36450]\tLoss: 1617.1763\n",
      "Training Epoch: 1 [12416/36450]\tLoss: 1511.8093\n",
      "Training Epoch: 1 [12480/36450]\tLoss: 1557.8224\n",
      "Training Epoch: 1 [12544/36450]\tLoss: 1609.3958\n",
      "Training Epoch: 1 [12608/36450]\tLoss: 1536.9875\n",
      "Training Epoch: 1 [12672/36450]\tLoss: 1588.1022\n",
      "Training Epoch: 1 [12736/36450]\tLoss: 1601.7155\n",
      "Training Epoch: 1 [12800/36450]\tLoss: 1507.5074\n",
      "Training Epoch: 1 [12864/36450]\tLoss: 1459.7557\n",
      "Training Epoch: 1 [12928/36450]\tLoss: 1551.3047\n",
      "Training Epoch: 1 [12992/36450]\tLoss: 1510.6459\n",
      "Training Epoch: 1 [13056/36450]\tLoss: 1550.4084\n",
      "Training Epoch: 1 [13120/36450]\tLoss: 1529.2531\n",
      "Training Epoch: 1 [13184/36450]\tLoss: 1474.9753\n",
      "Training Epoch: 1 [13248/36450]\tLoss: 1449.2296\n",
      "Training Epoch: 1 [13312/36450]\tLoss: 1486.7860\n",
      "Training Epoch: 1 [13376/36450]\tLoss: 1582.4294\n",
      "Training Epoch: 1 [13440/36450]\tLoss: 1450.1869\n",
      "Training Epoch: 1 [13504/36450]\tLoss: 1520.4956\n",
      "Training Epoch: 1 [13568/36450]\tLoss: 1497.6853\n",
      "Training Epoch: 1 [13632/36450]\tLoss: 1511.9567\n",
      "Training Epoch: 1 [13696/36450]\tLoss: 1466.6005\n",
      "Training Epoch: 1 [13760/36450]\tLoss: 1423.9297\n",
      "Training Epoch: 1 [13824/36450]\tLoss: 1552.7366\n",
      "Training Epoch: 1 [13888/36450]\tLoss: 1526.5885\n",
      "Training Epoch: 1 [13952/36450]\tLoss: 1460.2983\n",
      "Training Epoch: 1 [14016/36450]\tLoss: 1539.6570\n",
      "Training Epoch: 1 [14080/36450]\tLoss: 1432.2405\n",
      "Training Epoch: 1 [14144/36450]\tLoss: 1462.9608\n",
      "Training Epoch: 1 [14208/36450]\tLoss: 1486.3615\n",
      "Training Epoch: 1 [14272/36450]\tLoss: 1469.4288\n",
      "Training Epoch: 1 [14336/36450]\tLoss: 1410.8738\n",
      "Training Epoch: 1 [14400/36450]\tLoss: 1460.0958\n",
      "Training Epoch: 1 [14464/36450]\tLoss: 1410.2111\n",
      "Training Epoch: 1 [14528/36450]\tLoss: 1432.1453\n",
      "Training Epoch: 1 [14592/36450]\tLoss: 1383.1226\n",
      "Training Epoch: 1 [14656/36450]\tLoss: 1444.8112\n",
      "Training Epoch: 1 [14720/36450]\tLoss: 1401.8052\n",
      "Training Epoch: 1 [14784/36450]\tLoss: 1361.2943\n",
      "Training Epoch: 1 [14848/36450]\tLoss: 1443.6758\n",
      "Training Epoch: 1 [14912/36450]\tLoss: 1456.0289\n",
      "Training Epoch: 1 [14976/36450]\tLoss: 1309.0551\n",
      "Training Epoch: 1 [15040/36450]\tLoss: 1473.1206\n",
      "Training Epoch: 1 [15104/36450]\tLoss: 1436.7711\n",
      "Training Epoch: 1 [15168/36450]\tLoss: 1468.6779\n",
      "Training Epoch: 1 [15232/36450]\tLoss: 1434.8877\n",
      "Training Epoch: 1 [15296/36450]\tLoss: 1452.8444\n",
      "Training Epoch: 1 [15360/36450]\tLoss: 1334.6002\n",
      "Training Epoch: 1 [15424/36450]\tLoss: 1405.9906\n",
      "Training Epoch: 1 [15488/36450]\tLoss: 1431.6224\n",
      "Training Epoch: 1 [15552/36450]\tLoss: 1369.8345\n",
      "Training Epoch: 1 [15616/36450]\tLoss: 1425.2292\n",
      "Training Epoch: 1 [15680/36450]\tLoss: 1322.9264\n",
      "Training Epoch: 1 [15744/36450]\tLoss: 1359.7585\n",
      "Training Epoch: 1 [15808/36450]\tLoss: 1376.4128\n",
      "Training Epoch: 1 [15872/36450]\tLoss: 1351.9840\n",
      "Training Epoch: 1 [15936/36450]\tLoss: 1392.2122\n",
      "Training Epoch: 1 [16000/36450]\tLoss: 1422.1487\n",
      "Training Epoch: 1 [16064/36450]\tLoss: 1354.8182\n",
      "Training Epoch: 1 [16128/36450]\tLoss: 1406.1703\n",
      "Training Epoch: 1 [16192/36450]\tLoss: 1341.7491\n",
      "Training Epoch: 1 [16256/36450]\tLoss: 1383.6062\n",
      "Training Epoch: 1 [16320/36450]\tLoss: 1369.2920\n",
      "Training Epoch: 1 [16384/36450]\tLoss: 1395.2936\n",
      "Training Epoch: 1 [16448/36450]\tLoss: 1362.3848\n",
      "Training Epoch: 1 [16512/36450]\tLoss: 1409.5760\n",
      "Training Epoch: 1 [16576/36450]\tLoss: 1339.9893\n",
      "Training Epoch: 1 [16640/36450]\tLoss: 1315.8667\n",
      "Training Epoch: 1 [16704/36450]\tLoss: 1337.8340\n",
      "Training Epoch: 1 [16768/36450]\tLoss: 1412.7257\n",
      "Training Epoch: 1 [16832/36450]\tLoss: 1383.5293\n",
      "Training Epoch: 1 [16896/36450]\tLoss: 1318.1128\n",
      "Training Epoch: 1 [16960/36450]\tLoss: 1352.9413\n",
      "Training Epoch: 1 [17024/36450]\tLoss: 1362.4780\n",
      "Training Epoch: 1 [17088/36450]\tLoss: 1368.9150\n",
      "Training Epoch: 1 [17152/36450]\tLoss: 1403.2260\n",
      "Training Epoch: 1 [17216/36450]\tLoss: 1302.8068\n",
      "Training Epoch: 1 [17280/36450]\tLoss: 1317.0465\n",
      "Training Epoch: 1 [17344/36450]\tLoss: 1411.7433\n",
      "Training Epoch: 1 [17408/36450]\tLoss: 1312.8389\n",
      "Training Epoch: 1 [17472/36450]\tLoss: 1242.6016\n",
      "Training Epoch: 1 [17536/36450]\tLoss: 1352.1459\n",
      "Training Epoch: 1 [17600/36450]\tLoss: 1294.9301\n",
      "Training Epoch: 1 [17664/36450]\tLoss: 1359.9166\n",
      "Training Epoch: 1 [17728/36450]\tLoss: 1308.1422\n",
      "Training Epoch: 1 [17792/36450]\tLoss: 1310.1603\n",
      "Training Epoch: 1 [17856/36450]\tLoss: 1332.1140\n",
      "Training Epoch: 1 [17920/36450]\tLoss: 1328.8051\n",
      "Training Epoch: 1 [17984/36450]\tLoss: 1355.2587\n",
      "Training Epoch: 1 [18048/36450]\tLoss: 1316.6453\n",
      "Training Epoch: 1 [18112/36450]\tLoss: 1315.1064\n",
      "Training Epoch: 1 [18176/36450]\tLoss: 1334.8361\n",
      "Training Epoch: 1 [18240/36450]\tLoss: 1294.5736\n",
      "Training Epoch: 1 [18304/36450]\tLoss: 1244.0406\n",
      "Training Epoch: 1 [18368/36450]\tLoss: 1324.3710\n",
      "Training Epoch: 1 [18432/36450]\tLoss: 1229.4454\n",
      "Training Epoch: 1 [18496/36450]\tLoss: 1243.1342\n",
      "Training Epoch: 1 [18560/36450]\tLoss: 1357.0708\n",
      "Training Epoch: 1 [18624/36450]\tLoss: 1252.9810\n",
      "Training Epoch: 1 [18688/36450]\tLoss: 1277.9683\n",
      "Training Epoch: 1 [18752/36450]\tLoss: 1234.9410\n",
      "Training Epoch: 1 [18816/36450]\tLoss: 1310.1836\n",
      "Training Epoch: 1 [18880/36450]\tLoss: 1239.2974\n",
      "Training Epoch: 1 [18944/36450]\tLoss: 1243.7068\n",
      "Training Epoch: 1 [19008/36450]\tLoss: 1254.7391\n",
      "Training Epoch: 1 [19072/36450]\tLoss: 1269.5533\n",
      "Training Epoch: 1 [19136/36450]\tLoss: 1293.6019\n",
      "Training Epoch: 1 [19200/36450]\tLoss: 1316.3723\n",
      "Training Epoch: 1 [19264/36450]\tLoss: 1277.0668\n",
      "Training Epoch: 1 [19328/36450]\tLoss: 1214.7716\n",
      "Training Epoch: 1 [19392/36450]\tLoss: 1216.3438\n",
      "Training Epoch: 1 [19456/36450]\tLoss: 1263.8790\n",
      "Training Epoch: 1 [19520/36450]\tLoss: 1287.4777\n",
      "Training Epoch: 1 [19584/36450]\tLoss: 1249.5663\n",
      "Training Epoch: 1 [19648/36450]\tLoss: 1223.1865\n",
      "Training Epoch: 1 [19712/36450]\tLoss: 1288.1665\n",
      "Training Epoch: 1 [19776/36450]\tLoss: 1272.4221\n",
      "Training Epoch: 1 [19840/36450]\tLoss: 1213.9363\n",
      "Training Epoch: 1 [19904/36450]\tLoss: 1218.4794\n",
      "Training Epoch: 1 [19968/36450]\tLoss: 1255.2144\n",
      "Training Epoch: 1 [20032/36450]\tLoss: 1276.3555\n",
      "Training Epoch: 1 [20096/36450]\tLoss: 1264.7307\n",
      "Training Epoch: 1 [20160/36450]\tLoss: 1152.3069\n",
      "Training Epoch: 1 [20224/36450]\tLoss: 1260.2104\n",
      "Training Epoch: 1 [20288/36450]\tLoss: 1257.3737\n",
      "Training Epoch: 1 [20352/36450]\tLoss: 1185.9009\n",
      "Training Epoch: 1 [20416/36450]\tLoss: 1261.1061\n",
      "Training Epoch: 1 [20480/36450]\tLoss: 1267.9844\n",
      "Training Epoch: 1 [20544/36450]\tLoss: 1202.5289\n",
      "Training Epoch: 1 [20608/36450]\tLoss: 1230.1888\n",
      "Training Epoch: 1 [20672/36450]\tLoss: 1165.4225\n",
      "Training Epoch: 1 [20736/36450]\tLoss: 1221.8308\n",
      "Training Epoch: 1 [20800/36450]\tLoss: 1186.3616\n",
      "Training Epoch: 1 [20864/36450]\tLoss: 1224.2275\n",
      "Training Epoch: 1 [20928/36450]\tLoss: 1225.5302\n",
      "Training Epoch: 1 [20992/36450]\tLoss: 1185.3794\n",
      "Training Epoch: 1 [21056/36450]\tLoss: 1239.7197\n",
      "Training Epoch: 1 [21120/36450]\tLoss: 1247.0513\n",
      "Training Epoch: 1 [21184/36450]\tLoss: 1278.5646\n",
      "Training Epoch: 1 [21248/36450]\tLoss: 1236.3428\n",
      "Training Epoch: 1 [21312/36450]\tLoss: 1232.9971\n",
      "Training Epoch: 1 [21376/36450]\tLoss: 1197.5690\n",
      "Training Epoch: 1 [21440/36450]\tLoss: 1238.4750\n",
      "Training Epoch: 1 [21504/36450]\tLoss: 1216.7751\n",
      "Training Epoch: 1 [21568/36450]\tLoss: 1225.1414\n",
      "Training Epoch: 1 [21632/36450]\tLoss: 1127.0758\n",
      "Training Epoch: 1 [21696/36450]\tLoss: 1253.1936\n",
      "Training Epoch: 1 [21760/36450]\tLoss: 1225.1117\n",
      "Training Epoch: 1 [21824/36450]\tLoss: 1201.1429\n",
      "Training Epoch: 1 [21888/36450]\tLoss: 1188.5767\n",
      "Training Epoch: 1 [21952/36450]\tLoss: 1199.6346\n",
      "Training Epoch: 1 [22016/36450]\tLoss: 1251.6121\n",
      "Training Epoch: 1 [22080/36450]\tLoss: 1198.8461\n",
      "Training Epoch: 1 [22144/36450]\tLoss: 1211.8523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [22208/36450]\tLoss: 1181.3229\n",
      "Training Epoch: 1 [22272/36450]\tLoss: 1159.5680\n",
      "Training Epoch: 1 [22336/36450]\tLoss: 1178.3459\n",
      "Training Epoch: 1 [22400/36450]\tLoss: 1098.3834\n",
      "Training Epoch: 1 [22464/36450]\tLoss: 1148.3333\n",
      "Training Epoch: 1 [22528/36450]\tLoss: 1193.0094\n",
      "Training Epoch: 1 [22592/36450]\tLoss: 1165.5717\n",
      "Training Epoch: 1 [22656/36450]\tLoss: 1181.2286\n",
      "Training Epoch: 1 [22720/36450]\tLoss: 1144.0870\n",
      "Training Epoch: 1 [22784/36450]\tLoss: 1193.6759\n",
      "Training Epoch: 1 [22848/36450]\tLoss: 1156.1155\n",
      "Training Epoch: 1 [22912/36450]\tLoss: 1249.4781\n",
      "Training Epoch: 1 [22976/36450]\tLoss: 1160.3521\n",
      "Training Epoch: 1 [23040/36450]\tLoss: 1138.6875\n",
      "Training Epoch: 1 [23104/36450]\tLoss: 1252.1041\n",
      "Training Epoch: 1 [23168/36450]\tLoss: 1142.5894\n",
      "Training Epoch: 1 [23232/36450]\tLoss: 1191.4714\n",
      "Training Epoch: 1 [23296/36450]\tLoss: 1169.9191\n",
      "Training Epoch: 1 [23360/36450]\tLoss: 1159.5917\n",
      "Training Epoch: 1 [23424/36450]\tLoss: 1126.0704\n",
      "Training Epoch: 1 [23488/36450]\tLoss: 1157.4531\n",
      "Training Epoch: 1 [23552/36450]\tLoss: 1110.6982\n",
      "Training Epoch: 1 [23616/36450]\tLoss: 1158.2207\n",
      "Training Epoch: 1 [23680/36450]\tLoss: 1113.6154\n",
      "Training Epoch: 1 [23744/36450]\tLoss: 1190.7036\n",
      "Training Epoch: 1 [23808/36450]\tLoss: 1138.5314\n",
      "Training Epoch: 1 [23872/36450]\tLoss: 1096.2766\n",
      "Training Epoch: 1 [23936/36450]\tLoss: 1187.7504\n",
      "Training Epoch: 1 [24000/36450]\tLoss: 1078.9417\n",
      "Training Epoch: 1 [24064/36450]\tLoss: 1123.7668\n",
      "Training Epoch: 1 [24128/36450]\tLoss: 1185.9608\n",
      "Training Epoch: 1 [24192/36450]\tLoss: 1194.0624\n",
      "Training Epoch: 1 [24256/36450]\tLoss: 1118.3896\n",
      "Training Epoch: 1 [24320/36450]\tLoss: 1173.5231\n",
      "Training Epoch: 1 [24384/36450]\tLoss: 1181.0026\n",
      "Training Epoch: 1 [24448/36450]\tLoss: 1142.9268\n",
      "Training Epoch: 1 [24512/36450]\tLoss: 1171.2711\n",
      "Training Epoch: 1 [24576/36450]\tLoss: 1157.9885\n",
      "Training Epoch: 1 [24640/36450]\tLoss: 1093.7903\n",
      "Training Epoch: 1 [24704/36450]\tLoss: 1127.0394\n",
      "Training Epoch: 1 [24768/36450]\tLoss: 1128.4602\n",
      "Training Epoch: 1 [24832/36450]\tLoss: 1176.6787\n",
      "Training Epoch: 1 [24896/36450]\tLoss: 1116.9529\n",
      "Training Epoch: 1 [24960/36450]\tLoss: 1116.3473\n",
      "Training Epoch: 1 [25024/36450]\tLoss: 1146.2861\n",
      "Training Epoch: 1 [25088/36450]\tLoss: 1107.7714\n",
      "Training Epoch: 1 [25152/36450]\tLoss: 1073.9254\n",
      "Training Epoch: 1 [25216/36450]\tLoss: 1124.8802\n",
      "Training Epoch: 1 [25280/36450]\tLoss: 1131.6975\n",
      "Training Epoch: 1 [25344/36450]\tLoss: 1113.5365\n",
      "Training Epoch: 1 [25408/36450]\tLoss: 1113.8673\n",
      "Training Epoch: 1 [25472/36450]\tLoss: 1108.1641\n",
      "Training Epoch: 1 [25536/36450]\tLoss: 1145.3923\n",
      "Training Epoch: 1 [25600/36450]\tLoss: 1098.0251\n",
      "Training Epoch: 1 [25664/36450]\tLoss: 1082.5702\n",
      "Training Epoch: 1 [25728/36450]\tLoss: 1140.2863\n",
      "Training Epoch: 1 [25792/36450]\tLoss: 1149.6769\n",
      "Training Epoch: 1 [25856/36450]\tLoss: 1144.1543\n",
      "Training Epoch: 1 [25920/36450]\tLoss: 1134.4883\n",
      "Training Epoch: 1 [25984/36450]\tLoss: 1039.3090\n",
      "Training Epoch: 1 [26048/36450]\tLoss: 1099.7761\n",
      "Training Epoch: 1 [26112/36450]\tLoss: 1050.6575\n",
      "Training Epoch: 1 [26176/36450]\tLoss: 1175.7688\n",
      "Training Epoch: 1 [26240/36450]\tLoss: 1090.1049\n",
      "Training Epoch: 1 [26304/36450]\tLoss: 1096.7202\n",
      "Training Epoch: 1 [26368/36450]\tLoss: 1145.7894\n",
      "Training Epoch: 1 [26432/36450]\tLoss: 1144.0092\n",
      "Training Epoch: 1 [26496/36450]\tLoss: 1121.0409\n",
      "Training Epoch: 1 [26560/36450]\tLoss: 1076.7939\n",
      "Training Epoch: 1 [26624/36450]\tLoss: 1067.6316\n",
      "Training Epoch: 1 [26688/36450]\tLoss: 1115.8020\n",
      "Training Epoch: 1 [26752/36450]\tLoss: 1095.8909\n",
      "Training Epoch: 1 [26816/36450]\tLoss: 1111.1376\n",
      "Training Epoch: 1 [26880/36450]\tLoss: 1145.7365\n",
      "Training Epoch: 1 [26944/36450]\tLoss: 1095.6951\n",
      "Training Epoch: 1 [27008/36450]\tLoss: 1132.7395\n",
      "Training Epoch: 1 [27072/36450]\tLoss: 1163.9229\n",
      "Training Epoch: 1 [27136/36450]\tLoss: 1113.5107\n",
      "Training Epoch: 1 [27200/36450]\tLoss: 1066.2748\n",
      "Training Epoch: 1 [27264/36450]\tLoss: 1086.9927\n",
      "Training Epoch: 1 [27328/36450]\tLoss: 1028.1277\n",
      "Training Epoch: 1 [27392/36450]\tLoss: 1081.5818\n",
      "Training Epoch: 1 [27456/36450]\tLoss: 1103.4226\n",
      "Training Epoch: 1 [27520/36450]\tLoss: 1157.7559\n",
      "Training Epoch: 1 [27584/36450]\tLoss: 1043.6385\n",
      "Training Epoch: 1 [27648/36450]\tLoss: 1076.1877\n",
      "Training Epoch: 1 [27712/36450]\tLoss: 1051.9122\n",
      "Training Epoch: 1 [27776/36450]\tLoss: 1080.6835\n",
      "Training Epoch: 1 [27840/36450]\tLoss: 1050.1495\n",
      "Training Epoch: 1 [27904/36450]\tLoss: 1091.3486\n",
      "Training Epoch: 1 [27968/36450]\tLoss: 1096.6923\n",
      "Training Epoch: 1 [28032/36450]\tLoss: 1032.1517\n",
      "Training Epoch: 1 [28096/36450]\tLoss: 1059.0641\n",
      "Training Epoch: 1 [28160/36450]\tLoss: 1059.3115\n",
      "Training Epoch: 1 [28224/36450]\tLoss: 1034.6740\n",
      "Training Epoch: 1 [28288/36450]\tLoss: 1086.0437\n",
      "Training Epoch: 1 [28352/36450]\tLoss: 1050.9385\n",
      "Training Epoch: 1 [28416/36450]\tLoss: 1079.4755\n",
      "Training Epoch: 1 [28480/36450]\tLoss: 1113.1536\n",
      "Training Epoch: 1 [28544/36450]\tLoss: 1138.5186\n",
      "Training Epoch: 1 [28608/36450]\tLoss: 1111.3669\n",
      "Training Epoch: 1 [28672/36450]\tLoss: 1069.8931\n",
      "Training Epoch: 1 [28736/36450]\tLoss: 1141.4655\n",
      "Training Epoch: 1 [28800/36450]\tLoss: 1102.9580\n",
      "Training Epoch: 1 [28864/36450]\tLoss: 1037.6766\n",
      "Training Epoch: 1 [28928/36450]\tLoss: 1064.5938\n",
      "Training Epoch: 1 [28992/36450]\tLoss: 982.2791\n",
      "Training Epoch: 1 [29056/36450]\tLoss: 1042.8109\n",
      "Training Epoch: 1 [29120/36450]\tLoss: 1061.2369\n",
      "Training Epoch: 1 [29184/36450]\tLoss: 1062.2869\n",
      "Training Epoch: 1 [29248/36450]\tLoss: 1009.8041\n",
      "Training Epoch: 1 [29312/36450]\tLoss: 1092.3024\n",
      "Training Epoch: 1 [29376/36450]\tLoss: 1048.0104\n",
      "Training Epoch: 1 [29440/36450]\tLoss: 1029.1085\n",
      "Training Epoch: 1 [29504/36450]\tLoss: 1058.5425\n",
      "Training Epoch: 1 [29568/36450]\tLoss: 1049.3674\n",
      "Training Epoch: 1 [29632/36450]\tLoss: 1042.0874\n",
      "Training Epoch: 1 [29696/36450]\tLoss: 1049.8574\n",
      "Training Epoch: 1 [29760/36450]\tLoss: 1048.1622\n",
      "Training Epoch: 1 [29824/36450]\tLoss: 1032.7365\n",
      "Training Epoch: 1 [29888/36450]\tLoss: 1035.9282\n",
      "Training Epoch: 1 [29952/36450]\tLoss: 1063.3694\n",
      "Training Epoch: 1 [30016/36450]\tLoss: 1087.1864\n",
      "Training Epoch: 1 [30080/36450]\tLoss: 1027.6212\n",
      "Training Epoch: 1 [30144/36450]\tLoss: 1055.1808\n",
      "Training Epoch: 1 [30208/36450]\tLoss: 1019.3876\n",
      "Training Epoch: 1 [30272/36450]\tLoss: 1048.1115\n",
      "Training Epoch: 1 [30336/36450]\tLoss: 1021.4823\n",
      "Training Epoch: 1 [30400/36450]\tLoss: 1051.7521\n",
      "Training Epoch: 1 [30464/36450]\tLoss: 1045.4517\n",
      "Training Epoch: 1 [30528/36450]\tLoss: 1113.5256\n",
      "Training Epoch: 1 [30592/36450]\tLoss: 1097.8735\n",
      "Training Epoch: 1 [30656/36450]\tLoss: 1066.8199\n",
      "Training Epoch: 1 [30720/36450]\tLoss: 1096.4972\n",
      "Training Epoch: 1 [30784/36450]\tLoss: 1070.0619\n",
      "Training Epoch: 1 [30848/36450]\tLoss: 1015.1801\n",
      "Training Epoch: 1 [30912/36450]\tLoss: 1104.4005\n",
      "Training Epoch: 1 [30976/36450]\tLoss: 1115.9580\n",
      "Training Epoch: 1 [31040/36450]\tLoss: 1036.0076\n",
      "Training Epoch: 1 [31104/36450]\tLoss: 1068.1095\n",
      "Training Epoch: 1 [31168/36450]\tLoss: 1050.4772\n",
      "Training Epoch: 1 [31232/36450]\tLoss: 998.5146\n",
      "Training Epoch: 1 [31296/36450]\tLoss: 1066.9615\n",
      "Training Epoch: 1 [31360/36450]\tLoss: 1012.3928\n",
      "Training Epoch: 1 [31424/36450]\tLoss: 996.3366\n",
      "Training Epoch: 1 [31488/36450]\tLoss: 997.0902\n",
      "Training Epoch: 1 [31552/36450]\tLoss: 1031.7950\n",
      "Training Epoch: 1 [31616/36450]\tLoss: 1000.7936\n",
      "Training Epoch: 1 [31680/36450]\tLoss: 1042.8762\n",
      "Training Epoch: 1 [31744/36450]\tLoss: 1067.0314\n",
      "Training Epoch: 1 [31808/36450]\tLoss: 1040.6182\n",
      "Training Epoch: 1 [31872/36450]\tLoss: 1027.7124\n",
      "Training Epoch: 1 [31936/36450]\tLoss: 1003.5351\n",
      "Training Epoch: 1 [32000/36450]\tLoss: 976.1346\n",
      "Training Epoch: 1 [32064/36450]\tLoss: 1044.7100\n",
      "Training Epoch: 1 [32128/36450]\tLoss: 1022.0852\n",
      "Training Epoch: 1 [32192/36450]\tLoss: 1014.4409\n",
      "Training Epoch: 1 [32256/36450]\tLoss: 1070.5801\n",
      "Training Epoch: 1 [32320/36450]\tLoss: 1052.8611\n",
      "Training Epoch: 1 [32384/36450]\tLoss: 1030.1853\n",
      "Training Epoch: 1 [32448/36450]\tLoss: 1020.8712\n",
      "Training Epoch: 1 [32512/36450]\tLoss: 994.9945\n",
      "Training Epoch: 1 [32576/36450]\tLoss: 1034.3820\n",
      "Training Epoch: 1 [32640/36450]\tLoss: 966.6110\n",
      "Training Epoch: 1 [32704/36450]\tLoss: 955.2276\n",
      "Training Epoch: 1 [32768/36450]\tLoss: 1006.5676\n",
      "Training Epoch: 1 [32832/36450]\tLoss: 1027.5249\n",
      "Training Epoch: 1 [32896/36450]\tLoss: 1027.8176\n",
      "Training Epoch: 1 [32960/36450]\tLoss: 1030.4962\n",
      "Training Epoch: 1 [33024/36450]\tLoss: 1047.0822\n",
      "Training Epoch: 1 [33088/36450]\tLoss: 968.6476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [33152/36450]\tLoss: 1022.9203\n",
      "Training Epoch: 1 [33216/36450]\tLoss: 1000.8578\n",
      "Training Epoch: 1 [33280/36450]\tLoss: 966.3489\n",
      "Training Epoch: 1 [33344/36450]\tLoss: 968.8422\n",
      "Training Epoch: 1 [33408/36450]\tLoss: 1046.9369\n",
      "Training Epoch: 1 [33472/36450]\tLoss: 1016.9899\n",
      "Training Epoch: 1 [33536/36450]\tLoss: 1065.3562\n",
      "Training Epoch: 1 [33600/36450]\tLoss: 996.1122\n",
      "Training Epoch: 1 [33664/36450]\tLoss: 1016.9005\n",
      "Training Epoch: 1 [33728/36450]\tLoss: 1000.7234\n",
      "Training Epoch: 1 [33792/36450]\tLoss: 1003.9410\n",
      "Training Epoch: 1 [33856/36450]\tLoss: 999.4014\n",
      "Training Epoch: 1 [33920/36450]\tLoss: 1003.3289\n",
      "Training Epoch: 1 [33984/36450]\tLoss: 1007.3377\n",
      "Training Epoch: 1 [34048/36450]\tLoss: 975.3292\n",
      "Training Epoch: 1 [34112/36450]\tLoss: 1053.8929\n",
      "Training Epoch: 1 [34176/36450]\tLoss: 995.7677\n",
      "Training Epoch: 1 [34240/36450]\tLoss: 1007.6777\n",
      "Training Epoch: 1 [34304/36450]\tLoss: 979.8196\n",
      "Training Epoch: 1 [34368/36450]\tLoss: 944.8318\n",
      "Training Epoch: 1 [34432/36450]\tLoss: 1063.9951\n",
      "Training Epoch: 1 [34496/36450]\tLoss: 1020.9169\n",
      "Training Epoch: 1 [34560/36450]\tLoss: 989.5997\n",
      "Training Epoch: 1 [34624/36450]\tLoss: 1053.2651\n",
      "Training Epoch: 1 [34688/36450]\tLoss: 1002.4614\n",
      "Training Epoch: 1 [34752/36450]\tLoss: 999.4603\n",
      "Training Epoch: 1 [34816/36450]\tLoss: 1022.6045\n",
      "Training Epoch: 1 [34880/36450]\tLoss: 1023.5798\n",
      "Training Epoch: 1 [34944/36450]\tLoss: 948.7239\n",
      "Training Epoch: 1 [35008/36450]\tLoss: 982.1368\n",
      "Training Epoch: 1 [35072/36450]\tLoss: 981.6945\n",
      "Training Epoch: 1 [35136/36450]\tLoss: 979.5193\n",
      "Training Epoch: 1 [35200/36450]\tLoss: 1026.3649\n",
      "Training Epoch: 1 [35264/36450]\tLoss: 985.9978\n",
      "Training Epoch: 1 [35328/36450]\tLoss: 953.3445\n",
      "Training Epoch: 1 [35392/36450]\tLoss: 928.1482\n",
      "Training Epoch: 1 [35456/36450]\tLoss: 999.4700\n",
      "Training Epoch: 1 [35520/36450]\tLoss: 951.0403\n",
      "Training Epoch: 1 [35584/36450]\tLoss: 1015.6534\n",
      "Training Epoch: 1 [35648/36450]\tLoss: 995.2425\n",
      "Training Epoch: 1 [35712/36450]\tLoss: 981.9849\n",
      "Training Epoch: 1 [35776/36450]\tLoss: 984.9827\n",
      "Training Epoch: 1 [35840/36450]\tLoss: 992.2596\n",
      "Training Epoch: 1 [35904/36450]\tLoss: 989.8817\n",
      "Training Epoch: 1 [35968/36450]\tLoss: 1007.1625\n",
      "Training Epoch: 1 [36032/36450]\tLoss: 1018.2204\n",
      "Training Epoch: 1 [36096/36450]\tLoss: 1001.8230\n",
      "Training Epoch: 1 [36160/36450]\tLoss: 966.4022\n",
      "Training Epoch: 1 [36224/36450]\tLoss: 978.7330\n",
      "Training Epoch: 1 [36288/36450]\tLoss: 1009.9499\n",
      "Training Epoch: 1 [36352/36450]\tLoss: 995.8931\n",
      "Training Epoch: 1 [36416/36450]\tLoss: 938.7042\n",
      "Training Epoch: 1 [36450/36450]\tLoss: 1065.0966\n",
      "Training Epoch: 1 [4050/4050]\tLoss: 489.2327\n",
      "Training Epoch: 2 [64/36450]\tLoss: 1035.0015\n",
      "Training Epoch: 2 [128/36450]\tLoss: 1002.3080\n",
      "Training Epoch: 2 [192/36450]\tLoss: 1029.3610\n",
      "Training Epoch: 2 [256/36450]\tLoss: 997.8000\n",
      "Training Epoch: 2 [320/36450]\tLoss: 997.6597\n",
      "Training Epoch: 2 [384/36450]\tLoss: 945.6118\n",
      "Training Epoch: 2 [448/36450]\tLoss: 1017.5146\n",
      "Training Epoch: 2 [512/36450]\tLoss: 989.5471\n",
      "Training Epoch: 2 [576/36450]\tLoss: 934.3189\n",
      "Training Epoch: 2 [640/36450]\tLoss: 971.5831\n",
      "Training Epoch: 2 [704/36450]\tLoss: 956.4768\n",
      "Training Epoch: 2 [768/36450]\tLoss: 972.4997\n",
      "Training Epoch: 2 [832/36450]\tLoss: 1001.7848\n",
      "Training Epoch: 2 [896/36450]\tLoss: 958.8310\n",
      "Training Epoch: 2 [960/36450]\tLoss: 1042.1111\n",
      "Training Epoch: 2 [1024/36450]\tLoss: 1094.4407\n",
      "Training Epoch: 2 [1088/36450]\tLoss: 1009.0427\n",
      "Training Epoch: 2 [1152/36450]\tLoss: 999.7563\n",
      "Training Epoch: 2 [1216/36450]\tLoss: 988.7598\n",
      "Training Epoch: 2 [1280/36450]\tLoss: 978.1742\n",
      "Training Epoch: 2 [1344/36450]\tLoss: 939.7225\n",
      "Training Epoch: 2 [1408/36450]\tLoss: 958.8822\n",
      "Training Epoch: 2 [1472/36450]\tLoss: 993.2986\n",
      "Training Epoch: 2 [1536/36450]\tLoss: 945.8667\n",
      "Training Epoch: 2 [1600/36450]\tLoss: 961.7948\n",
      "Training Epoch: 2 [1664/36450]\tLoss: 990.7967\n",
      "Training Epoch: 2 [1728/36450]\tLoss: 956.3931\n",
      "Training Epoch: 2 [1792/36450]\tLoss: 924.3228\n",
      "Training Epoch: 2 [1856/36450]\tLoss: 1005.3699\n",
      "Training Epoch: 2 [1920/36450]\tLoss: 959.1796\n",
      "Training Epoch: 2 [1984/36450]\tLoss: 980.0582\n",
      "Training Epoch: 2 [2048/36450]\tLoss: 945.8702\n",
      "Training Epoch: 2 [2112/36450]\tLoss: 930.5824\n",
      "Training Epoch: 2 [2176/36450]\tLoss: 930.3204\n",
      "Training Epoch: 2 [2240/36450]\tLoss: 980.0031\n",
      "Training Epoch: 2 [2304/36450]\tLoss: 963.7510\n",
      "Training Epoch: 2 [2368/36450]\tLoss: 963.3605\n",
      "Training Epoch: 2 [2432/36450]\tLoss: 976.9972\n",
      "Training Epoch: 2 [2496/36450]\tLoss: 936.6149\n",
      "Training Epoch: 2 [2560/36450]\tLoss: 950.2911\n",
      "Training Epoch: 2 [2624/36450]\tLoss: 1019.1820\n",
      "Training Epoch: 2 [2688/36450]\tLoss: 957.0435\n",
      "Training Epoch: 2 [2752/36450]\tLoss: 962.8316\n",
      "Training Epoch: 2 [2816/36450]\tLoss: 934.9225\n",
      "Training Epoch: 2 [2880/36450]\tLoss: 986.1294\n",
      "Training Epoch: 2 [2944/36450]\tLoss: 950.2592\n",
      "Training Epoch: 2 [3008/36450]\tLoss: 952.0262\n",
      "Training Epoch: 2 [3072/36450]\tLoss: 994.9928\n",
      "Training Epoch: 2 [3136/36450]\tLoss: 995.1245\n",
      "Training Epoch: 2 [3200/36450]\tLoss: 941.6777\n",
      "Training Epoch: 2 [3264/36450]\tLoss: 947.5220\n",
      "Training Epoch: 2 [3328/36450]\tLoss: 979.3373\n",
      "Training Epoch: 2 [3392/36450]\tLoss: 942.0968\n",
      "Training Epoch: 2 [3456/36450]\tLoss: 941.8040\n",
      "Training Epoch: 2 [3520/36450]\tLoss: 959.6271\n",
      "Training Epoch: 2 [3584/36450]\tLoss: 915.0294\n",
      "Training Epoch: 2 [3648/36450]\tLoss: 941.8955\n",
      "Training Epoch: 2 [3712/36450]\tLoss: 977.5839\n",
      "Training Epoch: 2 [3776/36450]\tLoss: 914.2546\n",
      "Training Epoch: 2 [3840/36450]\tLoss: 989.8934\n",
      "Training Epoch: 2 [3904/36450]\tLoss: 875.5123\n",
      "Training Epoch: 2 [3968/36450]\tLoss: 980.1289\n",
      "Training Epoch: 2 [4032/36450]\tLoss: 934.9043\n",
      "Training Epoch: 2 [4096/36450]\tLoss: 941.8375\n",
      "Training Epoch: 2 [4160/36450]\tLoss: 943.7593\n",
      "Training Epoch: 2 [4224/36450]\tLoss: 938.1641\n",
      "Training Epoch: 2 [4288/36450]\tLoss: 967.2287\n",
      "Training Epoch: 2 [4352/36450]\tLoss: 988.4483\n",
      "Training Epoch: 2 [4416/36450]\tLoss: 958.7656\n",
      "Training Epoch: 2 [4480/36450]\tLoss: 925.9221\n",
      "Training Epoch: 2 [4544/36450]\tLoss: 934.2682\n",
      "Training Epoch: 2 [4608/36450]\tLoss: 926.4177\n",
      "Training Epoch: 2 [4672/36450]\tLoss: 986.3468\n",
      "Training Epoch: 2 [4736/36450]\tLoss: 956.6414\n",
      "Training Epoch: 2 [4800/36450]\tLoss: 962.8214\n",
      "Training Epoch: 2 [4864/36450]\tLoss: 873.5101\n",
      "Training Epoch: 2 [4928/36450]\tLoss: 934.1764\n",
      "Training Epoch: 2 [4992/36450]\tLoss: 939.7849\n",
      "Training Epoch: 2 [5056/36450]\tLoss: 929.5892\n",
      "Training Epoch: 2 [5120/36450]\tLoss: 955.5242\n",
      "Training Epoch: 2 [5184/36450]\tLoss: 879.5069\n",
      "Training Epoch: 2 [5248/36450]\tLoss: 956.8755\n",
      "Training Epoch: 2 [5312/36450]\tLoss: 910.3023\n",
      "Training Epoch: 2 [5376/36450]\tLoss: 934.4029\n",
      "Training Epoch: 2 [5440/36450]\tLoss: 886.7640\n",
      "Training Epoch: 2 [5504/36450]\tLoss: 910.5917\n",
      "Training Epoch: 2 [5568/36450]\tLoss: 943.2690\n",
      "Training Epoch: 2 [5632/36450]\tLoss: 954.0415\n",
      "Training Epoch: 2 [5696/36450]\tLoss: 910.7361\n",
      "Training Epoch: 2 [5760/36450]\tLoss: 878.8875\n",
      "Training Epoch: 2 [5824/36450]\tLoss: 921.3441\n",
      "Training Epoch: 2 [5888/36450]\tLoss: 912.4811\n",
      "Training Epoch: 2 [5952/36450]\tLoss: 977.2298\n",
      "Training Epoch: 2 [6016/36450]\tLoss: 937.4417\n",
      "Training Epoch: 2 [6080/36450]\tLoss: 934.3072\n",
      "Training Epoch: 2 [6144/36450]\tLoss: 957.2055\n",
      "Training Epoch: 2 [6208/36450]\tLoss: 939.2735\n",
      "Training Epoch: 2 [6272/36450]\tLoss: 913.9118\n",
      "Training Epoch: 2 [6336/36450]\tLoss: 925.4448\n",
      "Training Epoch: 2 [6400/36450]\tLoss: 901.2595\n",
      "Training Epoch: 2 [6464/36450]\tLoss: 935.3447\n",
      "Training Epoch: 2 [6528/36450]\tLoss: 937.1242\n",
      "Training Epoch: 2 [6592/36450]\tLoss: 905.6019\n",
      "Training Epoch: 2 [6656/36450]\tLoss: 926.3862\n",
      "Training Epoch: 2 [6720/36450]\tLoss: 966.7887\n",
      "Training Epoch: 2 [6784/36450]\tLoss: 914.5592\n",
      "Training Epoch: 2 [6848/36450]\tLoss: 946.9660\n",
      "Training Epoch: 2 [6912/36450]\tLoss: 921.2766\n",
      "Training Epoch: 2 [6976/36450]\tLoss: 921.2509\n",
      "Training Epoch: 2 [7040/36450]\tLoss: 956.1171\n",
      "Training Epoch: 2 [7104/36450]\tLoss: 938.8652\n",
      "Training Epoch: 2 [7168/36450]\tLoss: 944.1370\n",
      "Training Epoch: 2 [7232/36450]\tLoss: 907.1846\n",
      "Training Epoch: 2 [7296/36450]\tLoss: 886.0845\n",
      "Training Epoch: 2 [7360/36450]\tLoss: 945.0609\n",
      "Training Epoch: 2 [7424/36450]\tLoss: 885.7883\n",
      "Training Epoch: 2 [7488/36450]\tLoss: 919.0221\n",
      "Training Epoch: 2 [7552/36450]\tLoss: 904.3198\n",
      "Training Epoch: 2 [7616/36450]\tLoss: 923.7176\n",
      "Training Epoch: 2 [7680/36450]\tLoss: 911.0007\n",
      "Training Epoch: 2 [7744/36450]\tLoss: 926.4715\n",
      "Training Epoch: 2 [7808/36450]\tLoss: 977.1160\n",
      "Training Epoch: 2 [7872/36450]\tLoss: 897.1013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [7936/36450]\tLoss: 897.4360\n",
      "Training Epoch: 2 [8000/36450]\tLoss: 882.0624\n",
      "Training Epoch: 2 [8064/36450]\tLoss: 908.5048\n",
      "Training Epoch: 2 [8128/36450]\tLoss: 929.8914\n",
      "Training Epoch: 2 [8192/36450]\tLoss: 886.5322\n",
      "Training Epoch: 2 [8256/36450]\tLoss: 870.8521\n",
      "Training Epoch: 2 [8320/36450]\tLoss: 905.7209\n",
      "Training Epoch: 2 [8384/36450]\tLoss: 875.9422\n",
      "Training Epoch: 2 [8448/36450]\tLoss: 919.6765\n",
      "Training Epoch: 2 [8512/36450]\tLoss: 888.0445\n",
      "Training Epoch: 2 [8576/36450]\tLoss: 925.6239\n",
      "Training Epoch: 2 [8640/36450]\tLoss: 895.8156\n",
      "Training Epoch: 2 [8704/36450]\tLoss: 901.1276\n",
      "Training Epoch: 2 [8768/36450]\tLoss: 984.0720\n",
      "Training Epoch: 2 [8832/36450]\tLoss: 917.5503\n",
      "Training Epoch: 2 [8896/36450]\tLoss: 910.7729\n",
      "Training Epoch: 2 [8960/36450]\tLoss: 888.4880\n",
      "Training Epoch: 2 [9024/36450]\tLoss: 918.9103\n",
      "Training Epoch: 2 [9088/36450]\tLoss: 876.0355\n",
      "Training Epoch: 2 [9152/36450]\tLoss: 939.1909\n",
      "Training Epoch: 2 [9216/36450]\tLoss: 891.1570\n",
      "Training Epoch: 2 [9280/36450]\tLoss: 940.8405\n",
      "Training Epoch: 2 [9344/36450]\tLoss: 945.9670\n",
      "Training Epoch: 2 [9408/36450]\tLoss: 872.9145\n",
      "Training Epoch: 2 [9472/36450]\tLoss: 947.9951\n",
      "Training Epoch: 2 [9536/36450]\tLoss: 955.0901\n",
      "Training Epoch: 2 [9600/36450]\tLoss: 866.4532\n",
      "Training Epoch: 2 [9664/36450]\tLoss: 899.7886\n",
      "Training Epoch: 2 [9728/36450]\tLoss: 894.6030\n",
      "Training Epoch: 2 [9792/36450]\tLoss: 878.2082\n",
      "Training Epoch: 2 [9856/36450]\tLoss: 878.9077\n",
      "Training Epoch: 2 [9920/36450]\tLoss: 921.8184\n",
      "Training Epoch: 2 [9984/36450]\tLoss: 856.1561\n",
      "Training Epoch: 2 [10048/36450]\tLoss: 897.9674\n",
      "Training Epoch: 2 [10112/36450]\tLoss: 892.3773\n",
      "Training Epoch: 2 [10176/36450]\tLoss: 878.1218\n",
      "Training Epoch: 2 [10240/36450]\tLoss: 922.2676\n",
      "Training Epoch: 2 [10304/36450]\tLoss: 886.9414\n",
      "Training Epoch: 2 [10368/36450]\tLoss: 889.6668\n",
      "Training Epoch: 2 [10432/36450]\tLoss: 911.2311\n",
      "Training Epoch: 2 [10496/36450]\tLoss: 869.6879\n",
      "Training Epoch: 2 [10560/36450]\tLoss: 923.1906\n",
      "Training Epoch: 2 [10624/36450]\tLoss: 907.6805\n",
      "Training Epoch: 2 [10688/36450]\tLoss: 880.2504\n",
      "Training Epoch: 2 [10752/36450]\tLoss: 928.7305\n",
      "Training Epoch: 2 [10816/36450]\tLoss: 898.6519\n",
      "Training Epoch: 2 [10880/36450]\tLoss: 896.2328\n",
      "Training Epoch: 2 [10944/36450]\tLoss: 851.9617\n",
      "Training Epoch: 2 [11008/36450]\tLoss: 887.4554\n",
      "Training Epoch: 2 [11072/36450]\tLoss: 879.8862\n",
      "Training Epoch: 2 [11136/36450]\tLoss: 904.2938\n",
      "Training Epoch: 2 [11200/36450]\tLoss: 909.6373\n",
      "Training Epoch: 2 [11264/36450]\tLoss: 913.6903\n",
      "Training Epoch: 2 [11328/36450]\tLoss: 921.8501\n",
      "Training Epoch: 2 [11392/36450]\tLoss: 900.0199\n",
      "Training Epoch: 2 [11456/36450]\tLoss: 899.7844\n",
      "Training Epoch: 2 [11520/36450]\tLoss: 876.6340\n",
      "Training Epoch: 2 [11584/36450]\tLoss: 901.5238\n",
      "Training Epoch: 2 [11648/36450]\tLoss: 920.6647\n",
      "Training Epoch: 2 [11712/36450]\tLoss: 938.6663\n",
      "Training Epoch: 2 [11776/36450]\tLoss: 947.8253\n",
      "Training Epoch: 2 [11840/36450]\tLoss: 906.6171\n",
      "Training Epoch: 2 [11904/36450]\tLoss: 882.6916\n",
      "Training Epoch: 2 [11968/36450]\tLoss: 898.7007\n",
      "Training Epoch: 2 [12032/36450]\tLoss: 932.5141\n",
      "Training Epoch: 2 [12096/36450]\tLoss: 891.6531\n",
      "Training Epoch: 2 [12160/36450]\tLoss: 856.1327\n",
      "Training Epoch: 2 [12224/36450]\tLoss: 881.5031\n",
      "Training Epoch: 2 [12288/36450]\tLoss: 925.7811\n",
      "Training Epoch: 2 [12352/36450]\tLoss: 872.6099\n",
      "Training Epoch: 2 [12416/36450]\tLoss: 916.5809\n",
      "Training Epoch: 2 [12480/36450]\tLoss: 951.4974\n",
      "Training Epoch: 2 [12544/36450]\tLoss: 903.7634\n",
      "Training Epoch: 2 [12608/36450]\tLoss: 860.9912\n",
      "Training Epoch: 2 [12672/36450]\tLoss: 908.2557\n",
      "Training Epoch: 2 [12736/36450]\tLoss: 852.3232\n",
      "Training Epoch: 2 [12800/36450]\tLoss: 892.6127\n",
      "Training Epoch: 2 [12864/36450]\tLoss: 889.8625\n",
      "Training Epoch: 2 [12928/36450]\tLoss: 880.8629\n",
      "Training Epoch: 2 [12992/36450]\tLoss: 896.3331\n",
      "Training Epoch: 2 [13056/36450]\tLoss: 873.0898\n",
      "Training Epoch: 2 [13120/36450]\tLoss: 857.9530\n",
      "Training Epoch: 2 [13184/36450]\tLoss: 909.5206\n",
      "Training Epoch: 2 [13248/36450]\tLoss: 883.5096\n",
      "Training Epoch: 2 [13312/36450]\tLoss: 886.5455\n",
      "Training Epoch: 2 [13376/36450]\tLoss: 864.2011\n",
      "Training Epoch: 2 [13440/36450]\tLoss: 891.5693\n",
      "Training Epoch: 2 [13504/36450]\tLoss: 892.8843\n",
      "Training Epoch: 2 [13568/36450]\tLoss: 829.9987\n",
      "Training Epoch: 2 [13632/36450]\tLoss: 893.4032\n",
      "Training Epoch: 2 [13696/36450]\tLoss: 868.9199\n",
      "Training Epoch: 2 [13760/36450]\tLoss: 869.2520\n",
      "Training Epoch: 2 [13824/36450]\tLoss: 892.9271\n",
      "Training Epoch: 2 [13888/36450]\tLoss: 910.8118\n",
      "Training Epoch: 2 [13952/36450]\tLoss: 850.5737\n",
      "Training Epoch: 2 [14016/36450]\tLoss: 884.0844\n",
      "Training Epoch: 2 [14080/36450]\tLoss: 893.3048\n",
      "Training Epoch: 2 [14144/36450]\tLoss: 840.6310\n",
      "Training Epoch: 2 [14208/36450]\tLoss: 864.7496\n",
      "Training Epoch: 2 [14272/36450]\tLoss: 828.0351\n",
      "Training Epoch: 2 [14336/36450]\tLoss: 877.0488\n",
      "Training Epoch: 2 [14400/36450]\tLoss: 889.3063\n",
      "Training Epoch: 2 [14464/36450]\tLoss: 916.0631\n",
      "Training Epoch: 2 [14528/36450]\tLoss: 877.6636\n",
      "Training Epoch: 2 [14592/36450]\tLoss: 845.3369\n",
      "Training Epoch: 2 [14656/36450]\tLoss: 823.1898\n",
      "Training Epoch: 2 [14720/36450]\tLoss: 897.7230\n",
      "Training Epoch: 2 [14784/36450]\tLoss: 872.9478\n",
      "Training Epoch: 2 [14848/36450]\tLoss: 871.5862\n",
      "Training Epoch: 2 [14912/36450]\tLoss: 861.2621\n",
      "Training Epoch: 2 [14976/36450]\tLoss: 955.5211\n",
      "Training Epoch: 2 [15040/36450]\tLoss: 891.3860\n",
      "Training Epoch: 2 [15104/36450]\tLoss: 895.3757\n",
      "Training Epoch: 2 [15168/36450]\tLoss: 834.4397\n",
      "Training Epoch: 2 [15232/36450]\tLoss: 820.0916\n",
      "Training Epoch: 2 [15296/36450]\tLoss: 835.2830\n",
      "Training Epoch: 2 [15360/36450]\tLoss: 869.4866\n",
      "Training Epoch: 2 [15424/36450]\tLoss: 838.1580\n",
      "Training Epoch: 2 [15488/36450]\tLoss: 856.5051\n",
      "Training Epoch: 2 [15552/36450]\tLoss: 904.3543\n",
      "Training Epoch: 2 [15616/36450]\tLoss: 889.1575\n",
      "Training Epoch: 2 [15680/36450]\tLoss: 873.6661\n",
      "Training Epoch: 2 [15744/36450]\tLoss: 847.7744\n",
      "Training Epoch: 2 [15808/36450]\tLoss: 839.2075\n",
      "Training Epoch: 2 [15872/36450]\tLoss: 876.6390\n",
      "Training Epoch: 2 [15936/36450]\tLoss: 829.7515\n",
      "Training Epoch: 2 [16000/36450]\tLoss: 875.3769\n",
      "Training Epoch: 2 [16064/36450]\tLoss: 853.7064\n",
      "Training Epoch: 2 [16128/36450]\tLoss: 864.4895\n",
      "Training Epoch: 2 [16192/36450]\tLoss: 825.3010\n",
      "Training Epoch: 2 [16256/36450]\tLoss: 867.1303\n",
      "Training Epoch: 2 [16320/36450]\tLoss: 853.0319\n",
      "Training Epoch: 2 [16384/36450]\tLoss: 839.7352\n",
      "Training Epoch: 2 [16448/36450]\tLoss: 837.1589\n",
      "Training Epoch: 2 [16512/36450]\tLoss: 896.2487\n",
      "Training Epoch: 2 [16576/36450]\tLoss: 851.6298\n",
      "Training Epoch: 2 [16640/36450]\tLoss: 886.3436\n",
      "Training Epoch: 2 [16704/36450]\tLoss: 881.7679\n",
      "Training Epoch: 2 [16768/36450]\tLoss: 855.1305\n",
      "Training Epoch: 2 [16832/36450]\tLoss: 867.3176\n",
      "Training Epoch: 2 [16896/36450]\tLoss: 857.2791\n",
      "Training Epoch: 2 [16960/36450]\tLoss: 820.5278\n",
      "Training Epoch: 2 [17024/36450]\tLoss: 856.7107\n",
      "Training Epoch: 2 [17088/36450]\tLoss: 870.9930\n",
      "Training Epoch: 2 [17152/36450]\tLoss: 822.0745\n",
      "Training Epoch: 2 [17216/36450]\tLoss: 809.7109\n",
      "Training Epoch: 2 [17280/36450]\tLoss: 874.9303\n",
      "Training Epoch: 2 [17344/36450]\tLoss: 834.2910\n",
      "Training Epoch: 2 [17408/36450]\tLoss: 855.5400\n",
      "Training Epoch: 2 [17472/36450]\tLoss: 891.9159\n",
      "Training Epoch: 2 [17536/36450]\tLoss: 839.1120\n",
      "Training Epoch: 2 [17600/36450]\tLoss: 875.7875\n",
      "Training Epoch: 2 [17664/36450]\tLoss: 878.7593\n",
      "Training Epoch: 2 [17728/36450]\tLoss: 845.8087\n",
      "Training Epoch: 2 [17792/36450]\tLoss: 890.2652\n",
      "Training Epoch: 2 [17856/36450]\tLoss: 874.5780\n",
      "Training Epoch: 2 [17920/36450]\tLoss: 866.4989\n",
      "Training Epoch: 2 [17984/36450]\tLoss: 833.4553\n",
      "Training Epoch: 2 [18048/36450]\tLoss: 887.0634\n",
      "Training Epoch: 2 [18112/36450]\tLoss: 807.0195\n",
      "Training Epoch: 2 [18176/36450]\tLoss: 905.4502\n",
      "Training Epoch: 2 [18240/36450]\tLoss: 818.4085\n",
      "Training Epoch: 2 [18304/36450]\tLoss: 866.0594\n",
      "Training Epoch: 2 [18368/36450]\tLoss: 903.4722\n",
      "Training Epoch: 2 [18432/36450]\tLoss: 890.8491\n",
      "Training Epoch: 2 [18496/36450]\tLoss: 872.2544\n",
      "Training Epoch: 2 [18560/36450]\tLoss: 823.3386\n",
      "Training Epoch: 2 [18624/36450]\tLoss: 878.4973\n",
      "Training Epoch: 2 [18688/36450]\tLoss: 821.8536\n",
      "Training Epoch: 2 [18752/36450]\tLoss: 849.5527\n",
      "Training Epoch: 2 [18816/36450]\tLoss: 858.6685\n",
      "Training Epoch: 2 [18880/36450]\tLoss: 885.4467\n",
      "Training Epoch: 2 [18944/36450]\tLoss: 856.0697\n",
      "Training Epoch: 2 [19008/36450]\tLoss: 830.4146\n",
      "Training Epoch: 2 [19072/36450]\tLoss: 824.2538\n",
      "Training Epoch: 2 [19136/36450]\tLoss: 829.1586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [19200/36450]\tLoss: 837.6207\n",
      "Training Epoch: 2 [19264/36450]\tLoss: 842.0538\n",
      "Training Epoch: 2 [19328/36450]\tLoss: 881.5728\n",
      "Training Epoch: 2 [19392/36450]\tLoss: 872.3380\n",
      "Training Epoch: 2 [19456/36450]\tLoss: 845.4488\n",
      "Training Epoch: 2 [19520/36450]\tLoss: 893.4167\n",
      "Training Epoch: 2 [19584/36450]\tLoss: 815.8079\n",
      "Training Epoch: 2 [19648/36450]\tLoss: 846.7144\n",
      "Training Epoch: 2 [19712/36450]\tLoss: 857.1034\n",
      "Training Epoch: 2 [19776/36450]\tLoss: 841.3237\n",
      "Training Epoch: 2 [19840/36450]\tLoss: 837.0436\n",
      "Training Epoch: 2 [19904/36450]\tLoss: 808.8551\n",
      "Training Epoch: 2 [19968/36450]\tLoss: 879.5034\n",
      "Training Epoch: 2 [20032/36450]\tLoss: 858.0303\n",
      "Training Epoch: 2 [20096/36450]\tLoss: 820.0941\n",
      "Training Epoch: 2 [20160/36450]\tLoss: 839.4174\n",
      "Training Epoch: 2 [20224/36450]\tLoss: 793.5107\n",
      "Training Epoch: 2 [20288/36450]\tLoss: 877.9504\n",
      "Training Epoch: 2 [20352/36450]\tLoss: 901.0383\n",
      "Training Epoch: 2 [20416/36450]\tLoss: 823.9585\n",
      "Training Epoch: 2 [20480/36450]\tLoss: 872.2980\n",
      "Training Epoch: 2 [20544/36450]\tLoss: 902.4699\n",
      "Training Epoch: 2 [20608/36450]\tLoss: 869.6191\n",
      "Training Epoch: 2 [20672/36450]\tLoss: 826.4015\n",
      "Training Epoch: 2 [20736/36450]\tLoss: 830.0368\n",
      "Training Epoch: 2 [20800/36450]\tLoss: 861.7975\n",
      "Training Epoch: 2 [20864/36450]\tLoss: 808.6036\n",
      "Training Epoch: 2 [20928/36450]\tLoss: 893.8035\n",
      "Training Epoch: 2 [20992/36450]\tLoss: 808.8297\n",
      "Training Epoch: 2 [21056/36450]\tLoss: 801.2139\n",
      "Training Epoch: 2 [21120/36450]\tLoss: 787.7344\n",
      "Training Epoch: 2 [21184/36450]\tLoss: 834.3484\n",
      "Training Epoch: 2 [21248/36450]\tLoss: 845.5145\n",
      "Training Epoch: 2 [21312/36450]\tLoss: 869.6731\n",
      "Training Epoch: 2 [21376/36450]\tLoss: 832.4244\n",
      "Training Epoch: 2 [21440/36450]\tLoss: 842.2426\n",
      "Training Epoch: 2 [21504/36450]\tLoss: 867.0021\n",
      "Training Epoch: 2 [21568/36450]\tLoss: 804.0891\n",
      "Training Epoch: 2 [21632/36450]\tLoss: 821.7335\n",
      "Training Epoch: 2 [21696/36450]\tLoss: 838.9742\n",
      "Training Epoch: 2 [21760/36450]\tLoss: 858.3087\n",
      "Training Epoch: 2 [21824/36450]\tLoss: 788.9952\n",
      "Training Epoch: 2 [21888/36450]\tLoss: 850.0564\n",
      "Training Epoch: 2 [21952/36450]\tLoss: 814.2849\n",
      "Training Epoch: 2 [22016/36450]\tLoss: 833.4890\n",
      "Training Epoch: 2 [22080/36450]\tLoss: 845.8265\n",
      "Training Epoch: 2 [22144/36450]\tLoss: 845.7311\n",
      "Training Epoch: 2 [22208/36450]\tLoss: 889.9757\n",
      "Training Epoch: 2 [22272/36450]\tLoss: 853.5615\n",
      "Training Epoch: 2 [22336/36450]\tLoss: 763.4017\n",
      "Training Epoch: 2 [22400/36450]\tLoss: 829.1169\n",
      "Training Epoch: 2 [22464/36450]\tLoss: 872.4590\n",
      "Training Epoch: 2 [22528/36450]\tLoss: 812.6896\n",
      "Training Epoch: 2 [22592/36450]\tLoss: 788.0740\n",
      "Training Epoch: 2 [22656/36450]\tLoss: 825.6826\n",
      "Training Epoch: 2 [22720/36450]\tLoss: 819.7932\n",
      "Training Epoch: 2 [22784/36450]\tLoss: 832.8633\n",
      "Training Epoch: 2 [22848/36450]\tLoss: 821.6925\n",
      "Training Epoch: 2 [22912/36450]\tLoss: 854.8259\n",
      "Training Epoch: 2 [22976/36450]\tLoss: 811.1067\n",
      "Training Epoch: 2 [23040/36450]\tLoss: 849.7676\n",
      "Training Epoch: 2 [23104/36450]\tLoss: 872.9417\n",
      "Training Epoch: 2 [23168/36450]\tLoss: 804.5681\n",
      "Training Epoch: 2 [23232/36450]\tLoss: 827.7989\n",
      "Training Epoch: 2 [23296/36450]\tLoss: 817.0930\n",
      "Training Epoch: 2 [23360/36450]\tLoss: 840.9079\n",
      "Training Epoch: 2 [23424/36450]\tLoss: 806.2632\n",
      "Training Epoch: 2 [23488/36450]\tLoss: 795.9835\n",
      "Training Epoch: 2 [23552/36450]\tLoss: 873.8099\n",
      "Training Epoch: 2 [23616/36450]\tLoss: 847.2028\n",
      "Training Epoch: 2 [23680/36450]\tLoss: 805.4745\n",
      "Training Epoch: 2 [23744/36450]\tLoss: 801.9445\n",
      "Training Epoch: 2 [23808/36450]\tLoss: 783.9005\n",
      "Training Epoch: 2 [23872/36450]\tLoss: 802.7775\n",
      "Training Epoch: 2 [23936/36450]\tLoss: 810.2946\n",
      "Training Epoch: 2 [24000/36450]\tLoss: 800.5818\n",
      "Training Epoch: 2 [24064/36450]\tLoss: 811.2688\n",
      "Training Epoch: 2 [24128/36450]\tLoss: 780.1673\n",
      "Training Epoch: 2 [24192/36450]\tLoss: 834.9548\n",
      "Training Epoch: 2 [24256/36450]\tLoss: 900.2568\n",
      "Training Epoch: 2 [24320/36450]\tLoss: 820.4902\n",
      "Training Epoch: 2 [24384/36450]\tLoss: 800.9491\n",
      "Training Epoch: 2 [24448/36450]\tLoss: 817.0949\n",
      "Training Epoch: 2 [24512/36450]\tLoss: 792.2175\n",
      "Training Epoch: 2 [24576/36450]\tLoss: 882.0899\n",
      "Training Epoch: 2 [24640/36450]\tLoss: 830.5787\n",
      "Training Epoch: 2 [24704/36450]\tLoss: 815.2844\n",
      "Training Epoch: 2 [24768/36450]\tLoss: 854.3663\n",
      "Training Epoch: 2 [24832/36450]\tLoss: 809.3834\n",
      "Training Epoch: 2 [24896/36450]\tLoss: 819.6987\n",
      "Training Epoch: 2 [24960/36450]\tLoss: 852.7026\n",
      "Training Epoch: 2 [25024/36450]\tLoss: 790.8630\n",
      "Training Epoch: 2 [25088/36450]\tLoss: 838.0833\n",
      "Training Epoch: 2 [25152/36450]\tLoss: 802.9284\n",
      "Training Epoch: 2 [25216/36450]\tLoss: 800.1124\n",
      "Training Epoch: 2 [25280/36450]\tLoss: 829.1042\n",
      "Training Epoch: 2 [25344/36450]\tLoss: 829.1758\n",
      "Training Epoch: 2 [25408/36450]\tLoss: 864.5400\n",
      "Training Epoch: 2 [25472/36450]\tLoss: 835.5453\n",
      "Training Epoch: 2 [25536/36450]\tLoss: 814.5353\n",
      "Training Epoch: 2 [25600/36450]\tLoss: 836.3201\n",
      "Training Epoch: 2 [25664/36450]\tLoss: 807.1881\n",
      "Training Epoch: 2 [25728/36450]\tLoss: 858.8409\n",
      "Training Epoch: 2 [25792/36450]\tLoss: 808.1746\n",
      "Training Epoch: 2 [25856/36450]\tLoss: 871.0482\n",
      "Training Epoch: 2 [25920/36450]\tLoss: 876.7912\n",
      "Training Epoch: 2 [25984/36450]\tLoss: 826.3572\n",
      "Training Epoch: 2 [26048/36450]\tLoss: 809.3942\n",
      "Training Epoch: 2 [26112/36450]\tLoss: 873.3207\n",
      "Training Epoch: 2 [26176/36450]\tLoss: 864.4225\n",
      "Training Epoch: 2 [26240/36450]\tLoss: 820.4603\n",
      "Training Epoch: 2 [26304/36450]\tLoss: 801.2999\n",
      "Training Epoch: 2 [26368/36450]\tLoss: 820.3116\n",
      "Training Epoch: 2 [26432/36450]\tLoss: 831.0398\n",
      "Training Epoch: 2 [26496/36450]\tLoss: 753.9891\n",
      "Training Epoch: 2 [26560/36450]\tLoss: 826.2684\n",
      "Training Epoch: 2 [26624/36450]\tLoss: 786.7340\n",
      "Training Epoch: 2 [26688/36450]\tLoss: 838.4896\n",
      "Training Epoch: 2 [26752/36450]\tLoss: 862.3600\n",
      "Training Epoch: 2 [26816/36450]\tLoss: 821.8135\n",
      "Training Epoch: 2 [26880/36450]\tLoss: 843.1583\n",
      "Training Epoch: 2 [26944/36450]\tLoss: 805.4014\n",
      "Training Epoch: 2 [27008/36450]\tLoss: 771.1419\n",
      "Training Epoch: 2 [27072/36450]\tLoss: 854.5869\n",
      "Training Epoch: 2 [27136/36450]\tLoss: 811.4781\n",
      "Training Epoch: 2 [27200/36450]\tLoss: 769.8237\n",
      "Training Epoch: 2 [27264/36450]\tLoss: 814.2333\n",
      "Training Epoch: 2 [27328/36450]\tLoss: 794.1436\n",
      "Training Epoch: 2 [27392/36450]\tLoss: 806.2684\n",
      "Training Epoch: 2 [27456/36450]\tLoss: 819.4958\n",
      "Training Epoch: 2 [27520/36450]\tLoss: 788.6311\n",
      "Training Epoch: 2 [27584/36450]\tLoss: 850.6225\n",
      "Training Epoch: 2 [27648/36450]\tLoss: 804.0521\n",
      "Training Epoch: 2 [27712/36450]\tLoss: 862.3267\n",
      "Training Epoch: 2 [27776/36450]\tLoss: 828.9415\n",
      "Training Epoch: 2 [27840/36450]\tLoss: 803.7751\n",
      "Training Epoch: 2 [27904/36450]\tLoss: 838.5136\n",
      "Training Epoch: 2 [27968/36450]\tLoss: 786.6791\n",
      "Training Epoch: 2 [28032/36450]\tLoss: 832.6229\n",
      "Training Epoch: 2 [28096/36450]\tLoss: 829.1825\n",
      "Training Epoch: 2 [28160/36450]\tLoss: 798.3759\n",
      "Training Epoch: 2 [28224/36450]\tLoss: 811.3828\n",
      "Training Epoch: 2 [28288/36450]\tLoss: 788.5135\n",
      "Training Epoch: 2 [28352/36450]\tLoss: 851.4698\n",
      "Training Epoch: 2 [28416/36450]\tLoss: 790.0668\n",
      "Training Epoch: 2 [28480/36450]\tLoss: 770.6454\n",
      "Training Epoch: 2 [28544/36450]\tLoss: 811.4246\n",
      "Training Epoch: 2 [28608/36450]\tLoss: 844.7994\n",
      "Training Epoch: 2 [28672/36450]\tLoss: 800.9761\n",
      "Training Epoch: 2 [28736/36450]\tLoss: 824.6648\n",
      "Training Epoch: 2 [28800/36450]\tLoss: 849.0742\n",
      "Training Epoch: 2 [28864/36450]\tLoss: 781.0359\n",
      "Training Epoch: 2 [28928/36450]\tLoss: 839.9700\n",
      "Training Epoch: 2 [28992/36450]\tLoss: 819.8230\n",
      "Training Epoch: 2 [29056/36450]\tLoss: 813.3392\n",
      "Training Epoch: 2 [29120/36450]\tLoss: 777.8968\n",
      "Training Epoch: 2 [29184/36450]\tLoss: 801.3123\n",
      "Training Epoch: 2 [29248/36450]\tLoss: 768.9698\n",
      "Training Epoch: 2 [29312/36450]\tLoss: 824.2638\n",
      "Training Epoch: 2 [29376/36450]\tLoss: 799.9146\n",
      "Training Epoch: 2 [29440/36450]\tLoss: 773.9951\n",
      "Training Epoch: 2 [29504/36450]\tLoss: 820.8795\n",
      "Training Epoch: 2 [29568/36450]\tLoss: 814.4171\n",
      "Training Epoch: 2 [29632/36450]\tLoss: 817.1057\n",
      "Training Epoch: 2 [29696/36450]\tLoss: 800.6130\n",
      "Training Epoch: 2 [29760/36450]\tLoss: 822.1812\n",
      "Training Epoch: 2 [29824/36450]\tLoss: 774.0066\n",
      "Training Epoch: 2 [29888/36450]\tLoss: 848.3447\n",
      "Training Epoch: 2 [29952/36450]\tLoss: 786.9512\n",
      "Training Epoch: 2 [30016/36450]\tLoss: 852.0784\n",
      "Training Epoch: 2 [30080/36450]\tLoss: 766.0304\n",
      "Training Epoch: 2 [30144/36450]\tLoss: 789.5940\n",
      "Training Epoch: 2 [30208/36450]\tLoss: 802.5408\n",
      "Training Epoch: 2 [30272/36450]\tLoss: 831.6318\n",
      "Training Epoch: 2 [30336/36450]\tLoss: 796.1506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [30400/36450]\tLoss: 800.8425\n",
      "Training Epoch: 2 [30464/36450]\tLoss: 768.6740\n",
      "Training Epoch: 2 [30528/36450]\tLoss: 890.6620\n",
      "Training Epoch: 2 [30592/36450]\tLoss: 790.8669\n",
      "Training Epoch: 2 [30656/36450]\tLoss: 845.2683\n",
      "Training Epoch: 2 [30720/36450]\tLoss: 812.0455\n",
      "Training Epoch: 2 [30784/36450]\tLoss: 828.6096\n",
      "Training Epoch: 2 [30848/36450]\tLoss: 804.3537\n",
      "Training Epoch: 2 [30912/36450]\tLoss: 814.4523\n",
      "Training Epoch: 2 [30976/36450]\tLoss: 821.5213\n",
      "Training Epoch: 2 [31040/36450]\tLoss: 802.9980\n",
      "Training Epoch: 2 [31104/36450]\tLoss: 752.3594\n",
      "Training Epoch: 2 [31168/36450]\tLoss: 827.3989\n",
      "Training Epoch: 2 [31232/36450]\tLoss: 817.8359\n",
      "Training Epoch: 2 [31296/36450]\tLoss: 809.5248\n",
      "Training Epoch: 2 [31360/36450]\tLoss: 834.2573\n",
      "Training Epoch: 2 [31424/36450]\tLoss: 777.8204\n",
      "Training Epoch: 2 [31488/36450]\tLoss: 766.0552\n",
      "Training Epoch: 2 [31552/36450]\tLoss: 820.4359\n",
      "Training Epoch: 2 [31616/36450]\tLoss: 797.6363\n",
      "Training Epoch: 2 [31680/36450]\tLoss: 785.0869\n",
      "Training Epoch: 2 [31744/36450]\tLoss: 796.5770\n",
      "Training Epoch: 2 [31808/36450]\tLoss: 751.4691\n",
      "Training Epoch: 2 [31872/36450]\tLoss: 840.5009\n",
      "Training Epoch: 2 [31936/36450]\tLoss: 775.7918\n",
      "Training Epoch: 2 [32000/36450]\tLoss: 805.0908\n",
      "Training Epoch: 2 [32064/36450]\tLoss: 816.9545\n",
      "Training Epoch: 2 [32128/36450]\tLoss: 815.9332\n",
      "Training Epoch: 2 [32192/36450]\tLoss: 814.1476\n",
      "Training Epoch: 2 [32256/36450]\tLoss: 825.4067\n",
      "Training Epoch: 2 [32320/36450]\tLoss: 831.4548\n",
      "Training Epoch: 2 [32384/36450]\tLoss: 866.9941\n",
      "Training Epoch: 2 [32448/36450]\tLoss: 758.5359\n",
      "Training Epoch: 2 [32512/36450]\tLoss: 789.9523\n",
      "Training Epoch: 2 [32576/36450]\tLoss: 768.3650\n",
      "Training Epoch: 2 [32640/36450]\tLoss: 885.6617\n",
      "Training Epoch: 2 [32704/36450]\tLoss: 819.3124\n",
      "Training Epoch: 2 [32768/36450]\tLoss: 840.2192\n",
      "Training Epoch: 2 [32832/36450]\tLoss: 832.1333\n",
      "Training Epoch: 2 [32896/36450]\tLoss: 807.3009\n",
      "Training Epoch: 2 [32960/36450]\tLoss: 810.8598\n",
      "Training Epoch: 2 [33024/36450]\tLoss: 741.9968\n",
      "Training Epoch: 2 [33088/36450]\tLoss: 784.5405\n",
      "Training Epoch: 2 [33152/36450]\tLoss: 824.0034\n",
      "Training Epoch: 2 [33216/36450]\tLoss: 787.1511\n",
      "Training Epoch: 2 [33280/36450]\tLoss: 780.5798\n",
      "Training Epoch: 2 [33344/36450]\tLoss: 784.6660\n",
      "Training Epoch: 2 [33408/36450]\tLoss: 797.6860\n",
      "Training Epoch: 2 [33472/36450]\tLoss: 788.9627\n",
      "Training Epoch: 2 [33536/36450]\tLoss: 777.9293\n",
      "Training Epoch: 2 [33600/36450]\tLoss: 801.1752\n",
      "Training Epoch: 2 [33664/36450]\tLoss: 765.3334\n",
      "Training Epoch: 2 [33728/36450]\tLoss: 817.5073\n",
      "Training Epoch: 2 [33792/36450]\tLoss: 806.5651\n",
      "Training Epoch: 2 [33856/36450]\tLoss: 767.9444\n",
      "Training Epoch: 2 [33920/36450]\tLoss: 764.7081\n",
      "Training Epoch: 2 [33984/36450]\tLoss: 779.2084\n",
      "Training Epoch: 2 [34048/36450]\tLoss: 797.3242\n",
      "Training Epoch: 2 [34112/36450]\tLoss: 831.8330\n",
      "Training Epoch: 2 [34176/36450]\tLoss: 807.8063\n",
      "Training Epoch: 2 [34240/36450]\tLoss: 785.6622\n",
      "Training Epoch: 2 [34304/36450]\tLoss: 775.2928\n",
      "Training Epoch: 2 [34368/36450]\tLoss: 789.6451\n",
      "Training Epoch: 2 [34432/36450]\tLoss: 775.9944\n",
      "Training Epoch: 2 [34496/36450]\tLoss: 784.8821\n",
      "Training Epoch: 2 [34560/36450]\tLoss: 819.1258\n",
      "Training Epoch: 2 [34624/36450]\tLoss: 795.2333\n",
      "Training Epoch: 2 [34688/36450]\tLoss: 752.7483\n",
      "Training Epoch: 2 [34752/36450]\tLoss: 729.8467\n",
      "Training Epoch: 2 [34816/36450]\tLoss: 812.7233\n",
      "Training Epoch: 2 [34880/36450]\tLoss: 792.0983\n",
      "Training Epoch: 2 [34944/36450]\tLoss: 779.9786\n",
      "Training Epoch: 2 [35008/36450]\tLoss: 758.5878\n",
      "Training Epoch: 2 [35072/36450]\tLoss: 777.2053\n",
      "Training Epoch: 2 [35136/36450]\tLoss: 788.1302\n",
      "Training Epoch: 2 [35200/36450]\tLoss: 797.0717\n",
      "Training Epoch: 2 [35264/36450]\tLoss: 813.4351\n",
      "Training Epoch: 2 [35328/36450]\tLoss: 771.5023\n",
      "Training Epoch: 2 [35392/36450]\tLoss: 791.6707\n",
      "Training Epoch: 2 [35456/36450]\tLoss: 794.0704\n",
      "Training Epoch: 2 [35520/36450]\tLoss: 823.9592\n",
      "Training Epoch: 2 [35584/36450]\tLoss: 790.8339\n",
      "Training Epoch: 2 [35648/36450]\tLoss: 795.1826\n",
      "Training Epoch: 2 [35712/36450]\tLoss: 802.5623\n",
      "Training Epoch: 2 [35776/36450]\tLoss: 747.6711\n",
      "Training Epoch: 2 [35840/36450]\tLoss: 766.7032\n",
      "Training Epoch: 2 [35904/36450]\tLoss: 763.6817\n",
      "Training Epoch: 2 [35968/36450]\tLoss: 787.4901\n",
      "Training Epoch: 2 [36032/36450]\tLoss: 799.6416\n",
      "Training Epoch: 2 [36096/36450]\tLoss: 820.0124\n",
      "Training Epoch: 2 [36160/36450]\tLoss: 726.8000\n",
      "Training Epoch: 2 [36224/36450]\tLoss: 765.7719\n",
      "Training Epoch: 2 [36288/36450]\tLoss: 769.3609\n",
      "Training Epoch: 2 [36352/36450]\tLoss: 758.9194\n",
      "Training Epoch: 2 [36416/36450]\tLoss: 776.2096\n",
      "Training Epoch: 2 [36450/36450]\tLoss: 798.0581\n",
      "Training Epoch: 2 [4050/4050]\tLoss: 389.3737\n",
      "Training Epoch: 3 [64/36450]\tLoss: 810.2071\n",
      "Training Epoch: 3 [128/36450]\tLoss: 736.1353\n",
      "Training Epoch: 3 [192/36450]\tLoss: 753.7883\n",
      "Training Epoch: 3 [256/36450]\tLoss: 768.2724\n",
      "Training Epoch: 3 [320/36450]\tLoss: 739.7092\n",
      "Training Epoch: 3 [384/36450]\tLoss: 782.1809\n",
      "Training Epoch: 3 [448/36450]\tLoss: 775.9963\n",
      "Training Epoch: 3 [512/36450]\tLoss: 826.2885\n",
      "Training Epoch: 3 [576/36450]\tLoss: 770.0449\n",
      "Training Epoch: 3 [640/36450]\tLoss: 752.2999\n",
      "Training Epoch: 3 [704/36450]\tLoss: 798.5540\n",
      "Training Epoch: 3 [768/36450]\tLoss: 758.5529\n",
      "Training Epoch: 3 [832/36450]\tLoss: 779.6036\n",
      "Training Epoch: 3 [896/36450]\tLoss: 753.3616\n",
      "Training Epoch: 3 [960/36450]\tLoss: 740.2600\n",
      "Training Epoch: 3 [1024/36450]\tLoss: 765.4453\n",
      "Training Epoch: 3 [1088/36450]\tLoss: 738.9437\n",
      "Training Epoch: 3 [1152/36450]\tLoss: 819.2583\n",
      "Training Epoch: 3 [1216/36450]\tLoss: 747.3752\n",
      "Training Epoch: 3 [1280/36450]\tLoss: 780.9146\n",
      "Training Epoch: 3 [1344/36450]\tLoss: 791.1910\n",
      "Training Epoch: 3 [1408/36450]\tLoss: 767.1728\n",
      "Training Epoch: 3 [1472/36450]\tLoss: 778.7850\n",
      "Training Epoch: 3 [1536/36450]\tLoss: 761.5533\n",
      "Training Epoch: 3 [1600/36450]\tLoss: 767.5327\n",
      "Training Epoch: 3 [1664/36450]\tLoss: 788.5760\n",
      "Training Epoch: 3 [1728/36450]\tLoss: 791.2448\n",
      "Training Epoch: 3 [1792/36450]\tLoss: 792.9909\n",
      "Training Epoch: 3 [1856/36450]\tLoss: 770.7740\n",
      "Training Epoch: 3 [1920/36450]\tLoss: 791.1399\n",
      "Training Epoch: 3 [1984/36450]\tLoss: 777.1212\n",
      "Training Epoch: 3 [2048/36450]\tLoss: 765.1367\n",
      "Training Epoch: 3 [2112/36450]\tLoss: 760.2690\n",
      "Training Epoch: 3 [2176/36450]\tLoss: 743.5153\n",
      "Training Epoch: 3 [2240/36450]\tLoss: 776.4486\n",
      "Training Epoch: 3 [2304/36450]\tLoss: 764.7707\n",
      "Training Epoch: 3 [2368/36450]\tLoss: 762.2706\n",
      "Training Epoch: 3 [2432/36450]\tLoss: 781.2124\n",
      "Training Epoch: 3 [2496/36450]\tLoss: 767.3943\n",
      "Training Epoch: 3 [2560/36450]\tLoss: 735.6428\n",
      "Training Epoch: 3 [2624/36450]\tLoss: 749.6252\n",
      "Training Epoch: 3 [2688/36450]\tLoss: 759.3638\n",
      "Training Epoch: 3 [2752/36450]\tLoss: 807.8746\n",
      "Training Epoch: 3 [2816/36450]\tLoss: 744.2429\n",
      "Training Epoch: 3 [2880/36450]\tLoss: 737.3535\n",
      "Training Epoch: 3 [2944/36450]\tLoss: 749.7120\n",
      "Training Epoch: 3 [3008/36450]\tLoss: 804.5067\n",
      "Training Epoch: 3 [3072/36450]\tLoss: 790.0301\n",
      "Training Epoch: 3 [3136/36450]\tLoss: 786.5831\n",
      "Training Epoch: 3 [3200/36450]\tLoss: 757.8204\n",
      "Training Epoch: 3 [3264/36450]\tLoss: 752.9721\n",
      "Training Epoch: 3 [3328/36450]\tLoss: 791.0273\n",
      "Training Epoch: 3 [3392/36450]\tLoss: 767.0736\n",
      "Training Epoch: 3 [3456/36450]\tLoss: 772.1632\n",
      "Training Epoch: 3 [3520/36450]\tLoss: 743.3893\n",
      "Training Epoch: 3 [3584/36450]\tLoss: 759.6172\n",
      "Training Epoch: 3 [3648/36450]\tLoss: 804.1058\n",
      "Training Epoch: 3 [3712/36450]\tLoss: 753.6364\n",
      "Training Epoch: 3 [3776/36450]\tLoss: 829.1401\n",
      "Training Epoch: 3 [3840/36450]\tLoss: 776.9129\n",
      "Training Epoch: 3 [3904/36450]\tLoss: 765.8499\n",
      "Training Epoch: 3 [3968/36450]\tLoss: 826.8454\n",
      "Training Epoch: 3 [4032/36450]\tLoss: 750.0357\n",
      "Training Epoch: 3 [4096/36450]\tLoss: 770.4006\n",
      "Training Epoch: 3 [4160/36450]\tLoss: 816.6490\n",
      "Training Epoch: 3 [4224/36450]\tLoss: 802.6071\n",
      "Training Epoch: 3 [4288/36450]\tLoss: 803.7801\n",
      "Training Epoch: 3 [4352/36450]\tLoss: 817.3449\n",
      "Training Epoch: 3 [4416/36450]\tLoss: 796.3677\n",
      "Training Epoch: 3 [4480/36450]\tLoss: 736.6165\n",
      "Training Epoch: 3 [4544/36450]\tLoss: 792.3212\n",
      "Training Epoch: 3 [4608/36450]\tLoss: 754.7197\n",
      "Training Epoch: 3 [4672/36450]\tLoss: 734.3008\n",
      "Training Epoch: 3 [4736/36450]\tLoss: 759.9476\n",
      "Training Epoch: 3 [4800/36450]\tLoss: 787.3277\n",
      "Training Epoch: 3 [4864/36450]\tLoss: 791.1093\n",
      "Training Epoch: 3 [4928/36450]\tLoss: 754.8481\n",
      "Training Epoch: 3 [4992/36450]\tLoss: 819.7130\n",
      "Training Epoch: 3 [5056/36450]\tLoss: 783.5449\n",
      "Training Epoch: 3 [5120/36450]\tLoss: 775.1489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [5184/36450]\tLoss: 737.7529\n",
      "Training Epoch: 3 [5248/36450]\tLoss: 811.6615\n",
      "Training Epoch: 3 [5312/36450]\tLoss: 727.6898\n",
      "Training Epoch: 3 [5376/36450]\tLoss: 749.0057\n",
      "Training Epoch: 3 [5440/36450]\tLoss: 748.3891\n",
      "Training Epoch: 3 [5504/36450]\tLoss: 715.7880\n",
      "Training Epoch: 3 [5568/36450]\tLoss: 727.8539\n",
      "Training Epoch: 3 [5632/36450]\tLoss: 769.6050\n",
      "Training Epoch: 3 [5696/36450]\tLoss: 753.8107\n",
      "Training Epoch: 3 [5760/36450]\tLoss: 762.2493\n",
      "Training Epoch: 3 [5824/36450]\tLoss: 710.7614\n",
      "Training Epoch: 3 [5888/36450]\tLoss: 763.4280\n",
      "Training Epoch: 3 [5952/36450]\tLoss: 770.0898\n",
      "Training Epoch: 3 [6016/36450]\tLoss: 744.0392\n",
      "Training Epoch: 3 [6080/36450]\tLoss: 773.8629\n",
      "Training Epoch: 3 [6144/36450]\tLoss: 748.6064\n",
      "Training Epoch: 3 [6208/36450]\tLoss: 750.5208\n",
      "Training Epoch: 3 [6272/36450]\tLoss: 765.4392\n",
      "Training Epoch: 3 [6336/36450]\tLoss: 758.5537\n",
      "Training Epoch: 3 [6400/36450]\tLoss: 763.4662\n",
      "Training Epoch: 3 [6464/36450]\tLoss: 781.9786\n",
      "Training Epoch: 3 [6528/36450]\tLoss: 730.8490\n",
      "Training Epoch: 3 [6592/36450]\tLoss: 760.1606\n",
      "Training Epoch: 3 [6656/36450]\tLoss: 717.4818\n",
      "Training Epoch: 3 [6720/36450]\tLoss: 774.2115\n",
      "Training Epoch: 3 [6784/36450]\tLoss: 749.2704\n",
      "Training Epoch: 3 [6848/36450]\tLoss: 769.1334\n",
      "Training Epoch: 3 [6912/36450]\tLoss: 717.2797\n",
      "Training Epoch: 3 [6976/36450]\tLoss: 717.9340\n",
      "Training Epoch: 3 [7040/36450]\tLoss: 757.0222\n",
      "Training Epoch: 3 [7104/36450]\tLoss: 779.2965\n",
      "Training Epoch: 3 [7168/36450]\tLoss: 791.8069\n",
      "Training Epoch: 3 [7232/36450]\tLoss: 807.1942\n",
      "Training Epoch: 3 [7296/36450]\tLoss: 782.4421\n",
      "Training Epoch: 3 [7360/36450]\tLoss: 778.1756\n",
      "Training Epoch: 3 [7424/36450]\tLoss: 786.5682\n",
      "Training Epoch: 3 [7488/36450]\tLoss: 755.8453\n",
      "Training Epoch: 3 [7552/36450]\tLoss: 784.6463\n",
      "Training Epoch: 3 [7616/36450]\tLoss: 784.9451\n",
      "Training Epoch: 3 [7680/36450]\tLoss: 801.5811\n",
      "Training Epoch: 3 [7744/36450]\tLoss: 766.9521\n",
      "Training Epoch: 3 [7808/36450]\tLoss: 790.5186\n",
      "Training Epoch: 3 [7872/36450]\tLoss: 766.0240\n",
      "Training Epoch: 3 [7936/36450]\tLoss: 750.9142\n",
      "Training Epoch: 3 [8000/36450]\tLoss: 718.8877\n",
      "Training Epoch: 3 [8064/36450]\tLoss: 771.0876\n",
      "Training Epoch: 3 [8128/36450]\tLoss: 750.4773\n",
      "Training Epoch: 3 [8192/36450]\tLoss: 747.8470\n",
      "Training Epoch: 3 [8256/36450]\tLoss: 818.0815\n",
      "Training Epoch: 3 [8320/36450]\tLoss: 761.8903\n",
      "Training Epoch: 3 [8384/36450]\tLoss: 790.8889\n",
      "Training Epoch: 3 [8448/36450]\tLoss: 754.3292\n",
      "Training Epoch: 3 [8512/36450]\tLoss: 751.2087\n",
      "Training Epoch: 3 [8576/36450]\tLoss: 777.1234\n",
      "Training Epoch: 3 [8640/36450]\tLoss: 708.9438\n",
      "Training Epoch: 3 [8704/36450]\tLoss: 760.8586\n",
      "Training Epoch: 3 [8768/36450]\tLoss: 716.1679\n",
      "Training Epoch: 3 [8832/36450]\tLoss: 758.3548\n",
      "Training Epoch: 3 [8896/36450]\tLoss: 777.9915\n",
      "Training Epoch: 3 [8960/36450]\tLoss: 779.1077\n",
      "Training Epoch: 3 [9024/36450]\tLoss: 750.0139\n",
      "Training Epoch: 3 [9088/36450]\tLoss: 777.4954\n",
      "Training Epoch: 3 [9152/36450]\tLoss: 769.7850\n",
      "Training Epoch: 3 [9216/36450]\tLoss: 793.9225\n",
      "Training Epoch: 3 [9280/36450]\tLoss: 738.8446\n",
      "Training Epoch: 3 [9344/36450]\tLoss: 704.7783\n",
      "Training Epoch: 3 [9408/36450]\tLoss: 744.5191\n",
      "Training Epoch: 3 [9472/36450]\tLoss: 776.7974\n",
      "Training Epoch: 3 [9536/36450]\tLoss: 780.1667\n",
      "Training Epoch: 3 [9600/36450]\tLoss: 757.5369\n",
      "Training Epoch: 3 [9664/36450]\tLoss: 743.0005\n",
      "Training Epoch: 3 [9728/36450]\tLoss: 725.9333\n",
      "Training Epoch: 3 [9792/36450]\tLoss: 759.3627\n",
      "Training Epoch: 3 [9856/36450]\tLoss: 775.4520\n",
      "Training Epoch: 3 [9920/36450]\tLoss: 701.9709\n",
      "Training Epoch: 3 [9984/36450]\tLoss: 744.8017\n",
      "Training Epoch: 3 [10048/36450]\tLoss: 732.8712\n",
      "Training Epoch: 3 [10112/36450]\tLoss: 758.7871\n",
      "Training Epoch: 3 [10176/36450]\tLoss: 761.1431\n",
      "Training Epoch: 3 [10240/36450]\tLoss: 771.1633\n",
      "Training Epoch: 3 [10304/36450]\tLoss: 698.6948\n",
      "Training Epoch: 3 [10368/36450]\tLoss: 767.3270\n",
      "Training Epoch: 3 [10432/36450]\tLoss: 742.1206\n",
      "Training Epoch: 3 [10496/36450]\tLoss: 738.0953\n",
      "Training Epoch: 3 [10560/36450]\tLoss: 683.7766\n",
      "Training Epoch: 3 [10624/36450]\tLoss: 756.6224\n",
      "Training Epoch: 3 [10688/36450]\tLoss: 749.5681\n",
      "Training Epoch: 3 [10752/36450]\tLoss: 764.7549\n",
      "Training Epoch: 3 [10816/36450]\tLoss: 747.8975\n",
      "Training Epoch: 3 [10880/36450]\tLoss: 789.4534\n",
      "Training Epoch: 3 [10944/36450]\tLoss: 748.1576\n",
      "Training Epoch: 3 [11008/36450]\tLoss: 754.8423\n",
      "Training Epoch: 3 [11072/36450]\tLoss: 763.5957\n",
      "Training Epoch: 3 [11136/36450]\tLoss: 772.2493\n",
      "Training Epoch: 3 [11200/36450]\tLoss: 779.9642\n",
      "Training Epoch: 3 [11264/36450]\tLoss: 788.7994\n",
      "Training Epoch: 3 [11328/36450]\tLoss: 735.7677\n",
      "Training Epoch: 3 [11392/36450]\tLoss: 750.2382\n",
      "Training Epoch: 3 [11456/36450]\tLoss: 713.7355\n",
      "Training Epoch: 3 [11520/36450]\tLoss: 760.2681\n",
      "Training Epoch: 3 [11584/36450]\tLoss: 724.4739\n",
      "Training Epoch: 3 [11648/36450]\tLoss: 786.0107\n",
      "Training Epoch: 3 [11712/36450]\tLoss: 793.1270\n",
      "Training Epoch: 3 [11776/36450]\tLoss: 756.0836\n",
      "Training Epoch: 3 [11840/36450]\tLoss: 733.0789\n",
      "Training Epoch: 3 [11904/36450]\tLoss: 733.0547\n",
      "Training Epoch: 3 [11968/36450]\tLoss: 747.8475\n",
      "Training Epoch: 3 [12032/36450]\tLoss: 737.7188\n",
      "Training Epoch: 3 [12096/36450]\tLoss: 745.0766\n",
      "Training Epoch: 3 [12160/36450]\tLoss: 757.5612\n",
      "Training Epoch: 3 [12224/36450]\tLoss: 722.7005\n",
      "Training Epoch: 3 [12288/36450]\tLoss: 752.1763\n",
      "Training Epoch: 3 [12352/36450]\tLoss: 769.1672\n",
      "Training Epoch: 3 [12416/36450]\tLoss: 760.3589\n",
      "Training Epoch: 3 [12480/36450]\tLoss: 709.3231\n",
      "Training Epoch: 3 [12544/36450]\tLoss: 707.8824\n",
      "Training Epoch: 3 [12608/36450]\tLoss: 731.2618\n",
      "Training Epoch: 3 [12672/36450]\tLoss: 700.1389\n",
      "Training Epoch: 3 [12736/36450]\tLoss: 763.0156\n",
      "Training Epoch: 3 [12800/36450]\tLoss: 714.2587\n",
      "Training Epoch: 3 [12864/36450]\tLoss: 757.2347\n",
      "Training Epoch: 3 [12928/36450]\tLoss: 732.8322\n",
      "Training Epoch: 3 [12992/36450]\tLoss: 710.6648\n",
      "Training Epoch: 3 [13056/36450]\tLoss: 728.4492\n",
      "Training Epoch: 3 [13120/36450]\tLoss: 750.7219\n",
      "Training Epoch: 3 [13184/36450]\tLoss: 744.7687\n",
      "Training Epoch: 3 [13248/36450]\tLoss: 716.8879\n",
      "Training Epoch: 3 [13312/36450]\tLoss: 723.6852\n",
      "Training Epoch: 3 [13376/36450]\tLoss: 753.3169\n",
      "Training Epoch: 3 [13440/36450]\tLoss: 788.5485\n",
      "Training Epoch: 3 [13504/36450]\tLoss: 721.3552\n",
      "Training Epoch: 3 [13568/36450]\tLoss: 714.5925\n",
      "Training Epoch: 3 [13632/36450]\tLoss: 763.0839\n",
      "Training Epoch: 3 [13696/36450]\tLoss: 741.4927\n",
      "Training Epoch: 3 [13760/36450]\tLoss: 739.8298\n",
      "Training Epoch: 3 [13824/36450]\tLoss: 727.8182\n",
      "Training Epoch: 3 [13888/36450]\tLoss: 790.7560\n",
      "Training Epoch: 3 [13952/36450]\tLoss: 714.6528\n",
      "Training Epoch: 3 [14016/36450]\tLoss: 761.8663\n",
      "Training Epoch: 3 [14080/36450]\tLoss: 724.2622\n",
      "Training Epoch: 3 [14144/36450]\tLoss: 723.7343\n",
      "Training Epoch: 3 [14208/36450]\tLoss: 738.1652\n",
      "Training Epoch: 3 [14272/36450]\tLoss: 727.4617\n",
      "Training Epoch: 3 [14336/36450]\tLoss: 772.8077\n",
      "Training Epoch: 3 [14400/36450]\tLoss: 787.1272\n",
      "Training Epoch: 3 [14464/36450]\tLoss: 753.3774\n",
      "Training Epoch: 3 [14528/36450]\tLoss: 735.5863\n",
      "Training Epoch: 3 [14592/36450]\tLoss: 715.3655\n",
      "Training Epoch: 3 [14656/36450]\tLoss: 709.3360\n",
      "Training Epoch: 3 [14720/36450]\tLoss: 737.9021\n",
      "Training Epoch: 3 [14784/36450]\tLoss: 715.0695\n",
      "Training Epoch: 3 [14848/36450]\tLoss: 755.0688\n",
      "Training Epoch: 3 [14912/36450]\tLoss: 728.0263\n",
      "Training Epoch: 3 [14976/36450]\tLoss: 728.6725\n",
      "Training Epoch: 3 [15040/36450]\tLoss: 741.5151\n",
      "Training Epoch: 3 [15104/36450]\tLoss: 739.0280\n",
      "Training Epoch: 3 [15168/36450]\tLoss: 745.4172\n",
      "Training Epoch: 3 [15232/36450]\tLoss: 730.8846\n",
      "Training Epoch: 3 [15296/36450]\tLoss: 736.6993\n",
      "Training Epoch: 3 [15360/36450]\tLoss: 755.1328\n",
      "Training Epoch: 3 [15424/36450]\tLoss: 717.1410\n",
      "Training Epoch: 3 [15488/36450]\tLoss: 762.7852\n",
      "Training Epoch: 3 [15552/36450]\tLoss: 719.5103\n",
      "Training Epoch: 3 [15616/36450]\tLoss: 743.0709\n",
      "Training Epoch: 3 [15680/36450]\tLoss: 724.7377\n",
      "Training Epoch: 3 [15744/36450]\tLoss: 705.1557\n",
      "Training Epoch: 3 [15808/36450]\tLoss: 732.6655\n",
      "Training Epoch: 3 [15872/36450]\tLoss: 717.6396\n",
      "Training Epoch: 3 [15936/36450]\tLoss: 746.4388\n",
      "Training Epoch: 3 [16000/36450]\tLoss: 723.2230\n",
      "Training Epoch: 3 [16064/36450]\tLoss: 743.5266\n",
      "Training Epoch: 3 [16128/36450]\tLoss: 746.4493\n",
      "Training Epoch: 3 [16192/36450]\tLoss: 765.5214\n",
      "Training Epoch: 3 [16256/36450]\tLoss: 743.6855\n",
      "Training Epoch: 3 [16320/36450]\tLoss: 717.2596\n",
      "Training Epoch: 3 [16384/36450]\tLoss: 716.9305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [16448/36450]\tLoss: 731.3688\n",
      "Training Epoch: 3 [16512/36450]\tLoss: 743.7539\n",
      "Training Epoch: 3 [16576/36450]\tLoss: 765.0790\n",
      "Training Epoch: 3 [16640/36450]\tLoss: 749.0872\n",
      "Training Epoch: 3 [16704/36450]\tLoss: 756.1398\n",
      "Training Epoch: 3 [16768/36450]\tLoss: 785.7608\n",
      "Training Epoch: 3 [16832/36450]\tLoss: 723.3689\n",
      "Training Epoch: 3 [16896/36450]\tLoss: 711.8272\n",
      "Training Epoch: 3 [16960/36450]\tLoss: 774.4983\n",
      "Training Epoch: 3 [17024/36450]\tLoss: 746.3306\n",
      "Training Epoch: 3 [17088/36450]\tLoss: 743.7463\n",
      "Training Epoch: 3 [17152/36450]\tLoss: 712.4303\n",
      "Training Epoch: 3 [17216/36450]\tLoss: 710.6882\n",
      "Training Epoch: 3 [17280/36450]\tLoss: 761.2079\n",
      "Training Epoch: 3 [17344/36450]\tLoss: 756.4841\n",
      "Training Epoch: 3 [17408/36450]\tLoss: 723.8581\n",
      "Training Epoch: 3 [17472/36450]\tLoss: 702.2224\n",
      "Training Epoch: 3 [17536/36450]\tLoss: 710.0462\n",
      "Training Epoch: 3 [17600/36450]\tLoss: 729.4041\n",
      "Training Epoch: 3 [17664/36450]\tLoss: 738.5963\n",
      "Training Epoch: 3 [17728/36450]\tLoss: 710.8511\n",
      "Training Epoch: 3 [17792/36450]\tLoss: 752.6708\n",
      "Training Epoch: 3 [17856/36450]\tLoss: 747.8594\n",
      "Training Epoch: 3 [17920/36450]\tLoss: 742.8961\n",
      "Training Epoch: 3 [17984/36450]\tLoss: 684.0183\n",
      "Training Epoch: 3 [18048/36450]\tLoss: 752.9433\n",
      "Training Epoch: 3 [18112/36450]\tLoss: 721.1509\n",
      "Training Epoch: 3 [18176/36450]\tLoss: 711.3920\n",
      "Training Epoch: 3 [18240/36450]\tLoss: 733.9055\n",
      "Training Epoch: 3 [18304/36450]\tLoss: 784.2069\n",
      "Training Epoch: 3 [18368/36450]\tLoss: 704.9017\n",
      "Training Epoch: 3 [18432/36450]\tLoss: 736.9020\n",
      "Training Epoch: 3 [18496/36450]\tLoss: 735.5196\n",
      "Training Epoch: 3 [18560/36450]\tLoss: 725.0386\n",
      "Training Epoch: 3 [18624/36450]\tLoss: 702.6335\n",
      "Training Epoch: 3 [18688/36450]\tLoss: 726.1754\n",
      "Training Epoch: 3 [18752/36450]\tLoss: 750.5775\n",
      "Training Epoch: 3 [18816/36450]\tLoss: 730.6170\n",
      "Training Epoch: 3 [18880/36450]\tLoss: 737.9460\n",
      "Training Epoch: 3 [18944/36450]\tLoss: 744.9427\n",
      "Training Epoch: 3 [19008/36450]\tLoss: 732.2922\n",
      "Training Epoch: 3 [19072/36450]\tLoss: 742.9448\n",
      "Training Epoch: 3 [19136/36450]\tLoss: 761.5206\n",
      "Training Epoch: 3 [19200/36450]\tLoss: 747.7039\n",
      "Training Epoch: 3 [19264/36450]\tLoss: 750.1865\n",
      "Training Epoch: 3 [19328/36450]\tLoss: 754.4359\n",
      "Training Epoch: 3 [19392/36450]\tLoss: 747.1851\n",
      "Training Epoch: 3 [19456/36450]\tLoss: 792.5380\n",
      "Training Epoch: 3 [19520/36450]\tLoss: 731.9247\n",
      "Training Epoch: 3 [19584/36450]\tLoss: 745.5134\n",
      "Training Epoch: 3 [19648/36450]\tLoss: 774.5906\n",
      "Training Epoch: 3 [19712/36450]\tLoss: 700.7712\n",
      "Training Epoch: 3 [19776/36450]\tLoss: 752.9905\n",
      "Training Epoch: 3 [19840/36450]\tLoss: 742.8153\n",
      "Training Epoch: 3 [19904/36450]\tLoss: 767.6081\n",
      "Training Epoch: 3 [19968/36450]\tLoss: 727.0573\n",
      "Training Epoch: 3 [20032/36450]\tLoss: 700.9193\n",
      "Training Epoch: 3 [20096/36450]\tLoss: 756.8682\n",
      "Training Epoch: 3 [20160/36450]\tLoss: 724.2626\n",
      "Training Epoch: 3 [20224/36450]\tLoss: 755.7323\n",
      "Training Epoch: 3 [20288/36450]\tLoss: 719.6078\n",
      "Training Epoch: 3 [20352/36450]\tLoss: 699.5404\n",
      "Training Epoch: 3 [20416/36450]\tLoss: 783.1529\n",
      "Training Epoch: 3 [20480/36450]\tLoss: 758.2918\n",
      "Training Epoch: 3 [20544/36450]\tLoss: 740.7955\n",
      "Training Epoch: 3 [20608/36450]\tLoss: 716.2373\n",
      "Training Epoch: 3 [20672/36450]\tLoss: 730.3245\n",
      "Training Epoch: 3 [20736/36450]\tLoss: 730.7160\n",
      "Training Epoch: 3 [20800/36450]\tLoss: 717.3039\n",
      "Training Epoch: 3 [20864/36450]\tLoss: 723.3130\n",
      "Training Epoch: 3 [20928/36450]\tLoss: 700.7125\n",
      "Training Epoch: 3 [20992/36450]\tLoss: 690.4203\n",
      "Training Epoch: 3 [21056/36450]\tLoss: 735.7461\n",
      "Training Epoch: 3 [21120/36450]\tLoss: 728.1129\n",
      "Training Epoch: 3 [21184/36450]\tLoss: 702.3225\n",
      "Training Epoch: 3 [21248/36450]\tLoss: 731.8205\n",
      "Training Epoch: 3 [21312/36450]\tLoss: 721.5618\n",
      "Training Epoch: 3 [21376/36450]\tLoss: 703.2496\n",
      "Training Epoch: 3 [21440/36450]\tLoss: 712.7812\n",
      "Training Epoch: 3 [21504/36450]\tLoss: 726.6932\n",
      "Training Epoch: 3 [21568/36450]\tLoss: 669.8176\n",
      "Training Epoch: 3 [21632/36450]\tLoss: 720.3359\n",
      "Training Epoch: 3 [21696/36450]\tLoss: 732.0394\n",
      "Training Epoch: 3 [21760/36450]\tLoss: 732.1827\n",
      "Training Epoch: 3 [21824/36450]\tLoss: 726.9161\n",
      "Training Epoch: 3 [21888/36450]\tLoss: 732.4423\n",
      "Training Epoch: 3 [21952/36450]\tLoss: 707.7412\n",
      "Training Epoch: 3 [22016/36450]\tLoss: 757.5441\n",
      "Training Epoch: 3 [22080/36450]\tLoss: 735.6976\n",
      "Training Epoch: 3 [22144/36450]\tLoss: 689.4863\n",
      "Training Epoch: 3 [22208/36450]\tLoss: 736.1912\n",
      "Training Epoch: 3 [22272/36450]\tLoss: 692.2138\n",
      "Training Epoch: 3 [22336/36450]\tLoss: 735.1556\n",
      "Training Epoch: 3 [22400/36450]\tLoss: 741.6411\n",
      "Training Epoch: 3 [22464/36450]\tLoss: 726.0416\n",
      "Training Epoch: 3 [22528/36450]\tLoss: 736.0419\n",
      "Training Epoch: 3 [22592/36450]\tLoss: 735.4880\n",
      "Training Epoch: 3 [22656/36450]\tLoss: 702.5751\n",
      "Training Epoch: 3 [22720/36450]\tLoss: 725.0387\n",
      "Training Epoch: 3 [22784/36450]\tLoss: 708.7186\n",
      "Training Epoch: 3 [22848/36450]\tLoss: 736.5167\n",
      "Training Epoch: 3 [22912/36450]\tLoss: 745.9875\n",
      "Training Epoch: 3 [22976/36450]\tLoss: 719.4889\n",
      "Training Epoch: 3 [23040/36450]\tLoss: 735.6599\n",
      "Training Epoch: 3 [23104/36450]\tLoss: 745.7725\n",
      "Training Epoch: 3 [23168/36450]\tLoss: 740.3716\n",
      "Training Epoch: 3 [23232/36450]\tLoss: 713.6085\n",
      "Training Epoch: 3 [23296/36450]\tLoss: 739.1426\n",
      "Training Epoch: 3 [23360/36450]\tLoss: 714.4891\n",
      "Training Epoch: 3 [23424/36450]\tLoss: 693.8060\n",
      "Training Epoch: 3 [23488/36450]\tLoss: 710.0136\n",
      "Training Epoch: 3 [23552/36450]\tLoss: 728.7629\n",
      "Training Epoch: 3 [23616/36450]\tLoss: 731.4258\n",
      "Training Epoch: 3 [23680/36450]\tLoss: 713.1502\n",
      "Training Epoch: 3 [23744/36450]\tLoss: 674.5451\n",
      "Training Epoch: 3 [23808/36450]\tLoss: 755.2490\n",
      "Training Epoch: 3 [23872/36450]\tLoss: 752.6779\n",
      "Training Epoch: 3 [23936/36450]\tLoss: 737.4905\n",
      "Training Epoch: 3 [24000/36450]\tLoss: 679.6697\n",
      "Training Epoch: 3 [24064/36450]\tLoss: 691.9688\n",
      "Training Epoch: 3 [24128/36450]\tLoss: 700.6859\n",
      "Training Epoch: 3 [24192/36450]\tLoss: 711.7937\n",
      "Training Epoch: 3 [24256/36450]\tLoss: 719.6712\n",
      "Training Epoch: 3 [24320/36450]\tLoss: 721.3770\n",
      "Training Epoch: 3 [24384/36450]\tLoss: 732.7275\n",
      "Training Epoch: 3 [24448/36450]\tLoss: 721.3705\n",
      "Training Epoch: 3 [24512/36450]\tLoss: 695.3011\n",
      "Training Epoch: 3 [24576/36450]\tLoss: 715.1382\n",
      "Training Epoch: 3 [24640/36450]\tLoss: 723.4388\n",
      "Training Epoch: 3 [24704/36450]\tLoss: 757.0534\n",
      "Training Epoch: 3 [24768/36450]\tLoss: 742.6671\n",
      "Training Epoch: 3 [24832/36450]\tLoss: 717.9788\n",
      "Training Epoch: 3 [24896/36450]\tLoss: 743.0000\n",
      "Training Epoch: 3 [24960/36450]\tLoss: 752.8566\n",
      "Training Epoch: 3 [25024/36450]\tLoss: 781.0785\n",
      "Training Epoch: 3 [25088/36450]\tLoss: 735.7570\n",
      "Training Epoch: 3 [25152/36450]\tLoss: 704.5368\n",
      "Training Epoch: 3 [25216/36450]\tLoss: 689.9831\n",
      "Training Epoch: 3 [25280/36450]\tLoss: 743.0582\n",
      "Training Epoch: 3 [25344/36450]\tLoss: 729.3840\n",
      "Training Epoch: 3 [25408/36450]\tLoss: 760.3949\n",
      "Training Epoch: 3 [25472/36450]\tLoss: 743.1709\n",
      "Training Epoch: 3 [25536/36450]\tLoss: 713.1281\n",
      "Training Epoch: 3 [25600/36450]\tLoss: 726.5908\n",
      "Training Epoch: 3 [25664/36450]\tLoss: 698.5529\n",
      "Training Epoch: 3 [25728/36450]\tLoss: 704.6614\n",
      "Training Epoch: 3 [25792/36450]\tLoss: 717.4599\n",
      "Training Epoch: 3 [25856/36450]\tLoss: 707.0441\n",
      "Training Epoch: 3 [25920/36450]\tLoss: 697.1240\n",
      "Training Epoch: 3 [25984/36450]\tLoss: 765.0792\n",
      "Training Epoch: 3 [26048/36450]\tLoss: 716.1766\n",
      "Training Epoch: 3 [26112/36450]\tLoss: 724.4210\n",
      "Training Epoch: 3 [26176/36450]\tLoss: 730.5803\n",
      "Training Epoch: 3 [26240/36450]\tLoss: 717.8223\n",
      "Training Epoch: 3 [26304/36450]\tLoss: 679.7787\n",
      "Training Epoch: 3 [26368/36450]\tLoss: 774.3431\n",
      "Training Epoch: 3 [26432/36450]\tLoss: 689.7156\n",
      "Training Epoch: 3 [26496/36450]\tLoss: 687.7432\n",
      "Training Epoch: 3 [26560/36450]\tLoss: 672.4725\n",
      "Training Epoch: 3 [26624/36450]\tLoss: 696.3305\n",
      "Training Epoch: 3 [26688/36450]\tLoss: 695.9125\n",
      "Training Epoch: 3 [26752/36450]\tLoss: 693.7428\n",
      "Training Epoch: 3 [26816/36450]\tLoss: 722.7497\n",
      "Training Epoch: 3 [26880/36450]\tLoss: 694.2028\n",
      "Training Epoch: 3 [26944/36450]\tLoss: 687.2874\n",
      "Training Epoch: 3 [27008/36450]\tLoss: 724.0992\n",
      "Training Epoch: 3 [27072/36450]\tLoss: 692.8743\n",
      "Training Epoch: 3 [27136/36450]\tLoss: 678.5474\n",
      "Training Epoch: 3 [27200/36450]\tLoss: 732.4824\n",
      "Training Epoch: 3 [27264/36450]\tLoss: 690.0555\n",
      "Training Epoch: 3 [27328/36450]\tLoss: 706.9519\n",
      "Training Epoch: 3 [27392/36450]\tLoss: 712.4571\n",
      "Training Epoch: 3 [27456/36450]\tLoss: 709.9829\n",
      "Training Epoch: 3 [27520/36450]\tLoss: 719.8580\n",
      "Training Epoch: 3 [27584/36450]\tLoss: 747.3884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [27648/36450]\tLoss: 730.1448\n",
      "Training Epoch: 3 [27712/36450]\tLoss: 724.0997\n",
      "Training Epoch: 3 [27776/36450]\tLoss: 705.5866\n",
      "Training Epoch: 3 [27840/36450]\tLoss: 733.8697\n",
      "Training Epoch: 3 [27904/36450]\tLoss: 713.6450\n",
      "Training Epoch: 3 [27968/36450]\tLoss: 727.3945\n",
      "Training Epoch: 3 [28032/36450]\tLoss: 707.5793\n",
      "Training Epoch: 3 [28096/36450]\tLoss: 678.9048\n",
      "Training Epoch: 3 [28160/36450]\tLoss: 669.3588\n",
      "Training Epoch: 3 [28224/36450]\tLoss: 730.5844\n",
      "Training Epoch: 3 [28288/36450]\tLoss: 653.7769\n",
      "Training Epoch: 3 [28352/36450]\tLoss: 723.9547\n",
      "Training Epoch: 3 [28416/36450]\tLoss: 667.6268\n",
      "Training Epoch: 3 [28480/36450]\tLoss: 705.1775\n",
      "Training Epoch: 3 [28544/36450]\tLoss: 714.4156\n",
      "Training Epoch: 3 [28608/36450]\tLoss: 754.7431\n",
      "Training Epoch: 3 [28672/36450]\tLoss: 699.1748\n",
      "Training Epoch: 3 [28736/36450]\tLoss: 694.8568\n",
      "Training Epoch: 3 [28800/36450]\tLoss: 710.3130\n",
      "Training Epoch: 3 [28864/36450]\tLoss: 717.8348\n",
      "Training Epoch: 3 [28928/36450]\tLoss: 717.8619\n",
      "Training Epoch: 3 [28992/36450]\tLoss: 713.7311\n",
      "Training Epoch: 3 [29056/36450]\tLoss: 746.8528\n",
      "Training Epoch: 3 [29120/36450]\tLoss: 714.3271\n",
      "Training Epoch: 3 [29184/36450]\tLoss: 740.1358\n",
      "Training Epoch: 3 [29248/36450]\tLoss: 738.7714\n",
      "Training Epoch: 3 [29312/36450]\tLoss: 747.3406\n",
      "Training Epoch: 3 [29376/36450]\tLoss: 721.6087\n",
      "Training Epoch: 3 [29440/36450]\tLoss: 754.8757\n",
      "Training Epoch: 3 [29504/36450]\tLoss: 692.1157\n",
      "Training Epoch: 3 [29568/36450]\tLoss: 639.3989\n",
      "Training Epoch: 3 [29632/36450]\tLoss: 642.2706\n",
      "Training Epoch: 3 [29696/36450]\tLoss: 660.3712\n",
      "Training Epoch: 3 [29760/36450]\tLoss: 675.3824\n",
      "Training Epoch: 3 [29824/36450]\tLoss: 696.2770\n",
      "Training Epoch: 3 [29888/36450]\tLoss: 704.5621\n",
      "Training Epoch: 3 [29952/36450]\tLoss: 693.9545\n",
      "Training Epoch: 3 [30016/36450]\tLoss: 735.9384\n",
      "Training Epoch: 3 [30080/36450]\tLoss: 750.0227\n",
      "Training Epoch: 3 [30144/36450]\tLoss: 712.1025\n",
      "Training Epoch: 3 [30208/36450]\tLoss: 697.2826\n",
      "Training Epoch: 3 [30272/36450]\tLoss: 724.2958\n",
      "Training Epoch: 3 [30336/36450]\tLoss: 650.5161\n",
      "Training Epoch: 3 [30400/36450]\tLoss: 727.3212\n",
      "Training Epoch: 3 [30464/36450]\tLoss: 714.4692\n",
      "Training Epoch: 3 [30528/36450]\tLoss: 699.4586\n",
      "Training Epoch: 3 [30592/36450]\tLoss: 741.7546\n",
      "Training Epoch: 3 [30656/36450]\tLoss: 723.9526\n",
      "Training Epoch: 3 [30720/36450]\tLoss: 700.9357\n",
      "Training Epoch: 3 [30784/36450]\tLoss: 720.6933\n",
      "Training Epoch: 3 [30848/36450]\tLoss: 703.4333\n",
      "Training Epoch: 3 [30912/36450]\tLoss: 687.0834\n",
      "Training Epoch: 3 [30976/36450]\tLoss: 719.5479\n",
      "Training Epoch: 3 [31040/36450]\tLoss: 762.0829\n",
      "Training Epoch: 3 [31104/36450]\tLoss: 698.8362\n",
      "Training Epoch: 3 [31168/36450]\tLoss: 685.9553\n",
      "Training Epoch: 3 [31232/36450]\tLoss: 688.3547\n",
      "Training Epoch: 3 [31296/36450]\tLoss: 651.3675\n",
      "Training Epoch: 3 [31360/36450]\tLoss: 740.6221\n",
      "Training Epoch: 3 [31424/36450]\tLoss: 734.7498\n",
      "Training Epoch: 3 [31488/36450]\tLoss: 728.1058\n",
      "Training Epoch: 3 [31552/36450]\tLoss: 735.5620\n",
      "Training Epoch: 3 [31616/36450]\tLoss: 719.2817\n",
      "Training Epoch: 3 [31680/36450]\tLoss: 705.7867\n",
      "Training Epoch: 3 [31744/36450]\tLoss: 707.3270\n",
      "Training Epoch: 3 [31808/36450]\tLoss: 721.0099\n",
      "Training Epoch: 3 [31872/36450]\tLoss: 731.4051\n",
      "Training Epoch: 3 [31936/36450]\tLoss: 692.8036\n",
      "Training Epoch: 3 [32000/36450]\tLoss: 757.8826\n",
      "Training Epoch: 3 [32064/36450]\tLoss: 709.3367\n",
      "Training Epoch: 3 [32128/36450]\tLoss: 694.2782\n",
      "Training Epoch: 3 [32192/36450]\tLoss: 689.0073\n",
      "Training Epoch: 3 [32256/36450]\tLoss: 721.3464\n",
      "Training Epoch: 3 [32320/36450]\tLoss: 744.3351\n",
      "Training Epoch: 3 [32384/36450]\tLoss: 696.4902\n",
      "Training Epoch: 3 [32448/36450]\tLoss: 731.3808\n",
      "Training Epoch: 3 [32512/36450]\tLoss: 657.9318\n",
      "Training Epoch: 3 [32576/36450]\tLoss: 697.6557\n",
      "Training Epoch: 3 [32640/36450]\tLoss: 700.8628\n",
      "Training Epoch: 3 [32704/36450]\tLoss: 699.9379\n",
      "Training Epoch: 3 [32768/36450]\tLoss: 736.1852\n",
      "Training Epoch: 3 [32832/36450]\tLoss: 724.3314\n",
      "Training Epoch: 3 [32896/36450]\tLoss: 710.1179\n",
      "Training Epoch: 3 [32960/36450]\tLoss: 666.9406\n",
      "Training Epoch: 3 [33024/36450]\tLoss: 682.8043\n",
      "Training Epoch: 3 [33088/36450]\tLoss: 673.6106\n",
      "Training Epoch: 3 [33152/36450]\tLoss: 747.8542\n",
      "Training Epoch: 3 [33216/36450]\tLoss: 703.7793\n",
      "Training Epoch: 3 [33280/36450]\tLoss: 718.9424\n",
      "Training Epoch: 3 [33344/36450]\tLoss: 655.0201\n",
      "Training Epoch: 3 [33408/36450]\tLoss: 696.9576\n",
      "Training Epoch: 3 [33472/36450]\tLoss: 683.0280\n",
      "Training Epoch: 3 [33536/36450]\tLoss: 701.7197\n",
      "Training Epoch: 3 [33600/36450]\tLoss: 697.4708\n",
      "Training Epoch: 3 [33664/36450]\tLoss: 689.1697\n",
      "Training Epoch: 3 [33728/36450]\tLoss: 731.7416\n",
      "Training Epoch: 3 [33792/36450]\tLoss: 688.6990\n",
      "Training Epoch: 3 [33856/36450]\tLoss: 669.2576\n",
      "Training Epoch: 3 [33920/36450]\tLoss: 689.9175\n",
      "Training Epoch: 3 [33984/36450]\tLoss: 697.0553\n",
      "Training Epoch: 3 [34048/36450]\tLoss: 665.1078\n",
      "Training Epoch: 3 [34112/36450]\tLoss: 692.1234\n",
      "Training Epoch: 3 [34176/36450]\tLoss: 714.3495\n",
      "Training Epoch: 3 [34240/36450]\tLoss: 654.6481\n",
      "Training Epoch: 3 [34304/36450]\tLoss: 732.3331\n",
      "Training Epoch: 3 [34368/36450]\tLoss: 709.5438\n",
      "Training Epoch: 3 [34432/36450]\tLoss: 702.6107\n",
      "Training Epoch: 3 [34496/36450]\tLoss: 712.1314\n",
      "Training Epoch: 3 [34560/36450]\tLoss: 665.6848\n",
      "Training Epoch: 3 [34624/36450]\tLoss: 679.1386\n",
      "Training Epoch: 3 [34688/36450]\tLoss: 710.3688\n",
      "Training Epoch: 3 [34752/36450]\tLoss: 717.5284\n",
      "Training Epoch: 3 [34816/36450]\tLoss: 659.5132\n",
      "Training Epoch: 3 [34880/36450]\tLoss: 673.0236\n",
      "Training Epoch: 3 [34944/36450]\tLoss: 693.0527\n",
      "Training Epoch: 3 [35008/36450]\tLoss: 677.4478\n",
      "Training Epoch: 3 [35072/36450]\tLoss: 662.7020\n",
      "Training Epoch: 3 [35136/36450]\tLoss: 703.9697\n",
      "Training Epoch: 3 [35200/36450]\tLoss: 688.3137\n",
      "Training Epoch: 3 [35264/36450]\tLoss: 661.6934\n",
      "Training Epoch: 3 [35328/36450]\tLoss: 697.5500\n",
      "Training Epoch: 3 [35392/36450]\tLoss: 675.8009\n",
      "Training Epoch: 3 [35456/36450]\tLoss: 705.0356\n",
      "Training Epoch: 3 [35520/36450]\tLoss: 664.2310\n",
      "Training Epoch: 3 [35584/36450]\tLoss: 670.2714\n",
      "Training Epoch: 3 [35648/36450]\tLoss: 665.4238\n",
      "Training Epoch: 3 [35712/36450]\tLoss: 720.5751\n",
      "Training Epoch: 3 [35776/36450]\tLoss: 654.3511\n",
      "Training Epoch: 3 [35840/36450]\tLoss: 727.6599\n",
      "Training Epoch: 3 [35904/36450]\tLoss: 688.4648\n",
      "Training Epoch: 3 [35968/36450]\tLoss: 697.1392\n",
      "Training Epoch: 3 [36032/36450]\tLoss: 703.2315\n",
      "Training Epoch: 3 [36096/36450]\tLoss: 692.4661\n",
      "Training Epoch: 3 [36160/36450]\tLoss: 715.3409\n",
      "Training Epoch: 3 [36224/36450]\tLoss: 689.5743\n",
      "Training Epoch: 3 [36288/36450]\tLoss: 711.4304\n",
      "Training Epoch: 3 [36352/36450]\tLoss: 692.3101\n",
      "Training Epoch: 3 [36416/36450]\tLoss: 694.7337\n",
      "Training Epoch: 3 [36450/36450]\tLoss: 673.9882\n",
      "Training Epoch: 3 [4050/4050]\tLoss: 344.5201\n",
      "Training Epoch: 4 [64/36450]\tLoss: 677.7380\n",
      "Training Epoch: 4 [128/36450]\tLoss: 701.3721\n",
      "Training Epoch: 4 [192/36450]\tLoss: 725.0082\n",
      "Training Epoch: 4 [256/36450]\tLoss: 669.2859\n",
      "Training Epoch: 4 [320/36450]\tLoss: 668.1984\n",
      "Training Epoch: 4 [384/36450]\tLoss: 693.5308\n",
      "Training Epoch: 4 [448/36450]\tLoss: 692.8297\n",
      "Training Epoch: 4 [512/36450]\tLoss: 710.3112\n",
      "Training Epoch: 4 [576/36450]\tLoss: 701.8230\n",
      "Training Epoch: 4 [640/36450]\tLoss: 667.5463\n",
      "Training Epoch: 4 [704/36450]\tLoss: 695.3400\n",
      "Training Epoch: 4 [768/36450]\tLoss: 689.2094\n",
      "Training Epoch: 4 [832/36450]\tLoss: 737.3105\n",
      "Training Epoch: 4 [896/36450]\tLoss: 661.8699\n",
      "Training Epoch: 4 [960/36450]\tLoss: 682.6569\n",
      "Training Epoch: 4 [1024/36450]\tLoss: 686.5487\n",
      "Training Epoch: 4 [1088/36450]\tLoss: 691.2067\n",
      "Training Epoch: 4 [1152/36450]\tLoss: 705.9372\n",
      "Training Epoch: 4 [1216/36450]\tLoss: 703.9543\n",
      "Training Epoch: 4 [1280/36450]\tLoss: 681.4822\n",
      "Training Epoch: 4 [1344/36450]\tLoss: 686.3019\n",
      "Training Epoch: 4 [1408/36450]\tLoss: 664.4518\n",
      "Training Epoch: 4 [1472/36450]\tLoss: 738.9017\n",
      "Training Epoch: 4 [1536/36450]\tLoss: 714.8149\n",
      "Training Epoch: 4 [1600/36450]\tLoss: 688.5795\n",
      "Training Epoch: 4 [1664/36450]\tLoss: 690.6100\n",
      "Training Epoch: 4 [1728/36450]\tLoss: 642.0691\n",
      "Training Epoch: 4 [1792/36450]\tLoss: 693.3171\n",
      "Training Epoch: 4 [1856/36450]\tLoss: 689.7070\n",
      "Training Epoch: 4 [1920/36450]\tLoss: 663.5732\n",
      "Training Epoch: 4 [1984/36450]\tLoss: 683.1371\n",
      "Training Epoch: 4 [2048/36450]\tLoss: 673.3137\n",
      "Training Epoch: 4 [2112/36450]\tLoss: 641.2699\n",
      "Training Epoch: 4 [2176/36450]\tLoss: 676.3120\n",
      "Training Epoch: 4 [2240/36450]\tLoss: 686.5419\n",
      "Training Epoch: 4 [2304/36450]\tLoss: 698.3742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [2368/36450]\tLoss: 683.9199\n",
      "Training Epoch: 4 [2432/36450]\tLoss: 680.9683\n",
      "Training Epoch: 4 [2496/36450]\tLoss: 718.1974\n",
      "Training Epoch: 4 [2560/36450]\tLoss: 669.8713\n",
      "Training Epoch: 4 [2624/36450]\tLoss: 651.4068\n",
      "Training Epoch: 4 [2688/36450]\tLoss: 691.5490\n",
      "Training Epoch: 4 [2752/36450]\tLoss: 710.1553\n",
      "Training Epoch: 4 [2816/36450]\tLoss: 733.4742\n",
      "Training Epoch: 4 [2880/36450]\tLoss: 720.7542\n",
      "Training Epoch: 4 [2944/36450]\tLoss: 666.6956\n",
      "Training Epoch: 4 [3008/36450]\tLoss: 662.5540\n",
      "Training Epoch: 4 [3072/36450]\tLoss: 685.2915\n",
      "Training Epoch: 4 [3136/36450]\tLoss: 664.9395\n",
      "Training Epoch: 4 [3200/36450]\tLoss: 701.0382\n",
      "Training Epoch: 4 [3264/36450]\tLoss: 658.0340\n",
      "Training Epoch: 4 [3328/36450]\tLoss: 709.7391\n",
      "Training Epoch: 4 [3392/36450]\tLoss: 697.4117\n",
      "Training Epoch: 4 [3456/36450]\tLoss: 686.7603\n",
      "Training Epoch: 4 [3520/36450]\tLoss: 707.9379\n",
      "Training Epoch: 4 [3584/36450]\tLoss: 720.5594\n",
      "Training Epoch: 4 [3648/36450]\tLoss: 708.5878\n",
      "Training Epoch: 4 [3712/36450]\tLoss: 609.5339\n",
      "Training Epoch: 4 [3776/36450]\tLoss: 670.6354\n",
      "Training Epoch: 4 [3840/36450]\tLoss: 694.8247\n",
      "Training Epoch: 4 [3904/36450]\tLoss: 676.8928\n",
      "Training Epoch: 4 [3968/36450]\tLoss: 707.1993\n",
      "Training Epoch: 4 [4032/36450]\tLoss: 683.6768\n",
      "Training Epoch: 4 [4096/36450]\tLoss: 705.7514\n",
      "Training Epoch: 4 [4160/36450]\tLoss: 687.3391\n",
      "Training Epoch: 4 [4224/36450]\tLoss: 698.0235\n",
      "Training Epoch: 4 [4288/36450]\tLoss: 694.3370\n",
      "Training Epoch: 4 [4352/36450]\tLoss: 682.6594\n",
      "Training Epoch: 4 [4416/36450]\tLoss: 708.4994\n",
      "Training Epoch: 4 [4480/36450]\tLoss: 672.0398\n",
      "Training Epoch: 4 [4544/36450]\tLoss: 708.5905\n",
      "Training Epoch: 4 [4608/36450]\tLoss: 717.0368\n",
      "Training Epoch: 4 [4672/36450]\tLoss: 721.5343\n",
      "Training Epoch: 4 [4736/36450]\tLoss: 653.2134\n",
      "Training Epoch: 4 [4800/36450]\tLoss: 676.2547\n",
      "Training Epoch: 4 [4864/36450]\tLoss: 703.0040\n",
      "Training Epoch: 4 [4928/36450]\tLoss: 679.0027\n",
      "Training Epoch: 4 [4992/36450]\tLoss: 672.0514\n",
      "Training Epoch: 4 [5056/36450]\tLoss: 690.4493\n",
      "Training Epoch: 4 [5120/36450]\tLoss: 648.1241\n",
      "Training Epoch: 4 [5184/36450]\tLoss: 712.4965\n",
      "Training Epoch: 4 [5248/36450]\tLoss: 658.0939\n",
      "Training Epoch: 4 [5312/36450]\tLoss: 708.6852\n",
      "Training Epoch: 4 [5376/36450]\tLoss: 669.0357\n",
      "Training Epoch: 4 [5440/36450]\tLoss: 680.6641\n",
      "Training Epoch: 4 [5504/36450]\tLoss: 673.6484\n",
      "Training Epoch: 4 [5568/36450]\tLoss: 677.0696\n",
      "Training Epoch: 4 [5632/36450]\tLoss: 664.4889\n",
      "Training Epoch: 4 [5696/36450]\tLoss: 672.4337\n",
      "Training Epoch: 4 [5760/36450]\tLoss: 677.3826\n",
      "Training Epoch: 4 [5824/36450]\tLoss: 673.8197\n",
      "Training Epoch: 4 [5888/36450]\tLoss: 656.4767\n",
      "Training Epoch: 4 [5952/36450]\tLoss: 683.8156\n",
      "Training Epoch: 4 [6016/36450]\tLoss: 670.2458\n",
      "Training Epoch: 4 [6080/36450]\tLoss: 691.0901\n",
      "Training Epoch: 4 [6144/36450]\tLoss: 690.5765\n",
      "Training Epoch: 4 [6208/36450]\tLoss: 672.6326\n",
      "Training Epoch: 4 [6272/36450]\tLoss: 651.6461\n",
      "Training Epoch: 4 [6336/36450]\tLoss: 651.8754\n",
      "Training Epoch: 4 [6400/36450]\tLoss: 674.0883\n",
      "Training Epoch: 4 [6464/36450]\tLoss: 669.7188\n",
      "Training Epoch: 4 [6528/36450]\tLoss: 667.8002\n",
      "Training Epoch: 4 [6592/36450]\tLoss: 683.9017\n",
      "Training Epoch: 4 [6656/36450]\tLoss: 668.8271\n",
      "Training Epoch: 4 [6720/36450]\tLoss: 692.4023\n",
      "Training Epoch: 4 [6784/36450]\tLoss: 685.4616\n",
      "Training Epoch: 4 [6848/36450]\tLoss: 641.4815\n",
      "Training Epoch: 4 [6912/36450]\tLoss: 701.1656\n",
      "Training Epoch: 4 [6976/36450]\tLoss: 695.5630\n",
      "Training Epoch: 4 [7040/36450]\tLoss: 655.8007\n",
      "Training Epoch: 4 [7104/36450]\tLoss: 721.5494\n",
      "Training Epoch: 4 [7168/36450]\tLoss: 661.5597\n",
      "Training Epoch: 4 [7232/36450]\tLoss: 693.1563\n",
      "Training Epoch: 4 [7296/36450]\tLoss: 691.6022\n",
      "Training Epoch: 4 [7360/36450]\tLoss: 683.9509\n",
      "Training Epoch: 4 [7424/36450]\tLoss: 691.5359\n",
      "Training Epoch: 4 [7488/36450]\tLoss: 673.5037\n",
      "Training Epoch: 4 [7552/36450]\tLoss: 662.3016\n",
      "Training Epoch: 4 [7616/36450]\tLoss: 655.2394\n",
      "Training Epoch: 4 [7680/36450]\tLoss: 694.3839\n",
      "Training Epoch: 4 [7744/36450]\tLoss: 706.4001\n",
      "Training Epoch: 4 [7808/36450]\tLoss: 669.7631\n",
      "Training Epoch: 4 [7872/36450]\tLoss: 685.9125\n",
      "Training Epoch: 4 [7936/36450]\tLoss: 694.0792\n",
      "Training Epoch: 4 [8000/36450]\tLoss: 694.5046\n",
      "Training Epoch: 4 [8064/36450]\tLoss: 688.5991\n",
      "Training Epoch: 4 [8128/36450]\tLoss: 686.8665\n",
      "Training Epoch: 4 [8192/36450]\tLoss: 668.4760\n",
      "Training Epoch: 4 [8256/36450]\tLoss: 643.5422\n",
      "Training Epoch: 4 [8320/36450]\tLoss: 670.2036\n",
      "Training Epoch: 4 [8384/36450]\tLoss: 690.3237\n",
      "Training Epoch: 4 [8448/36450]\tLoss: 642.3568\n",
      "Training Epoch: 4 [8512/36450]\tLoss: 676.6681\n",
      "Training Epoch: 4 [8576/36450]\tLoss: 682.4116\n",
      "Training Epoch: 4 [8640/36450]\tLoss: 648.8781\n",
      "Training Epoch: 4 [8704/36450]\tLoss: 682.1899\n",
      "Training Epoch: 4 [8768/36450]\tLoss: 660.5998\n",
      "Training Epoch: 4 [8832/36450]\tLoss: 693.2631\n",
      "Training Epoch: 4 [8896/36450]\tLoss: 675.0836\n",
      "Training Epoch: 4 [8960/36450]\tLoss: 680.5909\n",
      "Training Epoch: 4 [9024/36450]\tLoss: 679.2411\n",
      "Training Epoch: 4 [9088/36450]\tLoss: 678.0491\n",
      "Training Epoch: 4 [9152/36450]\tLoss: 685.7889\n",
      "Training Epoch: 4 [9216/36450]\tLoss: 682.2828\n",
      "Training Epoch: 4 [9280/36450]\tLoss: 699.0063\n",
      "Training Epoch: 4 [9344/36450]\tLoss: 669.5848\n",
      "Training Epoch: 4 [9408/36450]\tLoss: 684.4966\n",
      "Training Epoch: 4 [9472/36450]\tLoss: 672.1194\n",
      "Training Epoch: 4 [9536/36450]\tLoss: 678.0693\n",
      "Training Epoch: 4 [9600/36450]\tLoss: 699.3458\n",
      "Training Epoch: 4 [9664/36450]\tLoss: 707.4344\n",
      "Training Epoch: 4 [9728/36450]\tLoss: 676.3159\n",
      "Training Epoch: 4 [9792/36450]\tLoss: 697.5872\n",
      "Training Epoch: 4 [9856/36450]\tLoss: 695.8845\n",
      "Training Epoch: 4 [9920/36450]\tLoss: 685.5982\n",
      "Training Epoch: 4 [9984/36450]\tLoss: 657.7491\n",
      "Training Epoch: 4 [10048/36450]\tLoss: 699.4872\n",
      "Training Epoch: 4 [10112/36450]\tLoss: 678.3953\n",
      "Training Epoch: 4 [10176/36450]\tLoss: 647.8539\n",
      "Training Epoch: 4 [10240/36450]\tLoss: 708.6982\n",
      "Training Epoch: 4 [10304/36450]\tLoss: 673.0874\n",
      "Training Epoch: 4 [10368/36450]\tLoss: 671.5257\n",
      "Training Epoch: 4 [10432/36450]\tLoss: 736.9556\n",
      "Training Epoch: 4 [10496/36450]\tLoss: 664.1362\n",
      "Training Epoch: 4 [10560/36450]\tLoss: 693.2227\n",
      "Training Epoch: 4 [10624/36450]\tLoss: 655.1971\n",
      "Training Epoch: 4 [10688/36450]\tLoss: 684.1757\n",
      "Training Epoch: 4 [10752/36450]\tLoss: 725.9451\n",
      "Training Epoch: 4 [10816/36450]\tLoss: 723.1135\n",
      "Training Epoch: 4 [10880/36450]\tLoss: 750.6765\n",
      "Training Epoch: 4 [10944/36450]\tLoss: 729.2931\n",
      "Training Epoch: 4 [11008/36450]\tLoss: 705.0754\n",
      "Training Epoch: 4 [11072/36450]\tLoss: 732.9544\n",
      "Training Epoch: 4 [11136/36450]\tLoss: 677.4880\n",
      "Training Epoch: 4 [11200/36450]\tLoss: 671.2834\n",
      "Training Epoch: 4 [11264/36450]\tLoss: 658.3519\n",
      "Training Epoch: 4 [11328/36450]\tLoss: 686.5522\n",
      "Training Epoch: 4 [11392/36450]\tLoss: 661.6785\n",
      "Training Epoch: 4 [11456/36450]\tLoss: 681.3195\n",
      "Training Epoch: 4 [11520/36450]\tLoss: 678.9490\n",
      "Training Epoch: 4 [11584/36450]\tLoss: 709.3558\n",
      "Training Epoch: 4 [11648/36450]\tLoss: 679.0604\n",
      "Training Epoch: 4 [11712/36450]\tLoss: 652.4979\n",
      "Training Epoch: 4 [11776/36450]\tLoss: 665.6182\n",
      "Training Epoch: 4 [11840/36450]\tLoss: 693.1674\n",
      "Training Epoch: 4 [11904/36450]\tLoss: 679.4968\n",
      "Training Epoch: 4 [11968/36450]\tLoss: 669.3925\n",
      "Training Epoch: 4 [12032/36450]\tLoss: 701.1301\n",
      "Training Epoch: 4 [12096/36450]\tLoss: 699.7031\n",
      "Training Epoch: 4 [12160/36450]\tLoss: 729.5347\n",
      "Training Epoch: 4 [12224/36450]\tLoss: 647.1957\n",
      "Training Epoch: 4 [12288/36450]\tLoss: 637.1077\n",
      "Training Epoch: 4 [12352/36450]\tLoss: 672.5906\n",
      "Training Epoch: 4 [12416/36450]\tLoss: 691.3491\n",
      "Training Epoch: 4 [12480/36450]\tLoss: 676.5742\n",
      "Training Epoch: 4 [12544/36450]\tLoss: 644.8330\n",
      "Training Epoch: 4 [12608/36450]\tLoss: 693.1652\n",
      "Training Epoch: 4 [12672/36450]\tLoss: 669.9389\n",
      "Training Epoch: 4 [12736/36450]\tLoss: 672.5746\n",
      "Training Epoch: 4 [12800/36450]\tLoss: 676.3688\n",
      "Training Epoch: 4 [12864/36450]\tLoss: 666.5670\n",
      "Training Epoch: 4 [12928/36450]\tLoss: 676.7516\n",
      "Training Epoch: 4 [12992/36450]\tLoss: 652.8842\n",
      "Training Epoch: 4 [13056/36450]\tLoss: 694.5995\n",
      "Training Epoch: 4 [13120/36450]\tLoss: 655.8622\n",
      "Training Epoch: 4 [13184/36450]\tLoss: 681.2365\n",
      "Training Epoch: 4 [13248/36450]\tLoss: 681.8029\n",
      "Training Epoch: 4 [13312/36450]\tLoss: 663.9084\n",
      "Training Epoch: 4 [13376/36450]\tLoss: 681.9296\n",
      "Training Epoch: 4 [13440/36450]\tLoss: 682.6594\n",
      "Training Epoch: 4 [13504/36450]\tLoss: 690.8911\n",
      "Training Epoch: 4 [13568/36450]\tLoss: 667.2621\n",
      "Training Epoch: 4 [13632/36450]\tLoss: 638.8841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [13696/36450]\tLoss: 660.8521\n",
      "Training Epoch: 4 [13760/36450]\tLoss: 667.5442\n",
      "Training Epoch: 4 [13824/36450]\tLoss: 658.3145\n",
      "Training Epoch: 4 [13888/36450]\tLoss: 691.7318\n",
      "Training Epoch: 4 [13952/36450]\tLoss: 684.2037\n",
      "Training Epoch: 4 [14016/36450]\tLoss: 657.0869\n",
      "Training Epoch: 4 [14080/36450]\tLoss: 676.8007\n",
      "Training Epoch: 4 [14144/36450]\tLoss: 701.9767\n",
      "Training Epoch: 4 [14208/36450]\tLoss: 676.0151\n",
      "Training Epoch: 4 [14272/36450]\tLoss: 678.3704\n",
      "Training Epoch: 4 [14336/36450]\tLoss: 671.3135\n",
      "Training Epoch: 4 [14400/36450]\tLoss: 642.5093\n",
      "Training Epoch: 4 [14464/36450]\tLoss: 674.0146\n",
      "Training Epoch: 4 [14528/36450]\tLoss: 689.8331\n",
      "Training Epoch: 4 [14592/36450]\tLoss: 672.3350\n",
      "Training Epoch: 4 [14656/36450]\tLoss: 660.9573\n",
      "Training Epoch: 4 [14720/36450]\tLoss: 652.6358\n",
      "Training Epoch: 4 [14784/36450]\tLoss: 659.7518\n",
      "Training Epoch: 4 [14848/36450]\tLoss: 644.2605\n",
      "Training Epoch: 4 [14912/36450]\tLoss: 683.5433\n",
      "Training Epoch: 4 [14976/36450]\tLoss: 655.8306\n",
      "Training Epoch: 4 [15040/36450]\tLoss: 680.8541\n",
      "Training Epoch: 4 [15104/36450]\tLoss: 688.5190\n",
      "Training Epoch: 4 [15168/36450]\tLoss: 679.7452\n",
      "Training Epoch: 4 [15232/36450]\tLoss: 667.5928\n",
      "Training Epoch: 4 [15296/36450]\tLoss: 656.2007\n",
      "Training Epoch: 4 [15360/36450]\tLoss: 664.2138\n",
      "Training Epoch: 4 [15424/36450]\tLoss: 671.5015\n",
      "Training Epoch: 4 [15488/36450]\tLoss: 652.7379\n",
      "Training Epoch: 4 [15552/36450]\tLoss: 657.4716\n",
      "Training Epoch: 4 [15616/36450]\tLoss: 651.9799\n",
      "Training Epoch: 4 [15680/36450]\tLoss: 684.5458\n",
      "Training Epoch: 4 [15744/36450]\tLoss: 681.7017\n",
      "Training Epoch: 4 [15808/36450]\tLoss: 649.5407\n",
      "Training Epoch: 4 [15872/36450]\tLoss: 681.5539\n",
      "Training Epoch: 4 [15936/36450]\tLoss: 650.8636\n",
      "Training Epoch: 4 [16000/36450]\tLoss: 680.3895\n",
      "Training Epoch: 4 [16064/36450]\tLoss: 671.7505\n",
      "Training Epoch: 4 [16128/36450]\tLoss: 668.9567\n",
      "Training Epoch: 4 [16192/36450]\tLoss: 680.4944\n",
      "Training Epoch: 4 [16256/36450]\tLoss: 688.0598\n",
      "Training Epoch: 4 [16320/36450]\tLoss: 676.3140\n",
      "Training Epoch: 4 [16384/36450]\tLoss: 654.7184\n",
      "Training Epoch: 4 [16448/36450]\tLoss: 643.8159\n",
      "Training Epoch: 4 [16512/36450]\tLoss: 689.8734\n",
      "Training Epoch: 4 [16576/36450]\tLoss: 648.6047\n",
      "Training Epoch: 4 [16640/36450]\tLoss: 648.0506\n",
      "Training Epoch: 4 [16704/36450]\tLoss: 673.7853\n",
      "Training Epoch: 4 [16768/36450]\tLoss: 607.1271\n",
      "Training Epoch: 4 [16832/36450]\tLoss: 653.7128\n",
      "Training Epoch: 4 [16896/36450]\tLoss: 682.5233\n",
      "Training Epoch: 4 [16960/36450]\tLoss: 662.0311\n",
      "Training Epoch: 4 [17024/36450]\tLoss: 683.6378\n",
      "Training Epoch: 4 [17088/36450]\tLoss: 697.8892\n",
      "Training Epoch: 4 [17152/36450]\tLoss: 654.6649\n",
      "Training Epoch: 4 [17216/36450]\tLoss: 638.4831\n",
      "Training Epoch: 4 [17280/36450]\tLoss: 664.2509\n",
      "Training Epoch: 4 [17344/36450]\tLoss: 673.9068\n",
      "Training Epoch: 4 [17408/36450]\tLoss: 665.9752\n",
      "Training Epoch: 4 [17472/36450]\tLoss: 659.5306\n",
      "Training Epoch: 4 [17536/36450]\tLoss: 661.6391\n",
      "Training Epoch: 4 [17600/36450]\tLoss: 675.7865\n",
      "Training Epoch: 4 [17664/36450]\tLoss: 663.5899\n",
      "Training Epoch: 4 [17728/36450]\tLoss: 645.5577\n",
      "Training Epoch: 4 [17792/36450]\tLoss: 669.4361\n",
      "Training Epoch: 4 [17856/36450]\tLoss: 661.4250\n",
      "Training Epoch: 4 [17920/36450]\tLoss: 674.8605\n",
      "Training Epoch: 4 [17984/36450]\tLoss: 695.4451\n",
      "Training Epoch: 4 [18048/36450]\tLoss: 698.3029\n",
      "Training Epoch: 4 [18112/36450]\tLoss: 661.6949\n",
      "Training Epoch: 4 [18176/36450]\tLoss: 641.0588\n",
      "Training Epoch: 4 [18240/36450]\tLoss: 696.7483\n",
      "Training Epoch: 4 [18304/36450]\tLoss: 641.0213\n",
      "Training Epoch: 4 [18368/36450]\tLoss: 682.9570\n",
      "Training Epoch: 4 [18432/36450]\tLoss: 636.7232\n",
      "Training Epoch: 4 [18496/36450]\tLoss: 688.8625\n",
      "Training Epoch: 4 [18560/36450]\tLoss: 668.4357\n",
      "Training Epoch: 4 [18624/36450]\tLoss: 641.1007\n",
      "Training Epoch: 4 [18688/36450]\tLoss: 663.4496\n",
      "Training Epoch: 4 [18752/36450]\tLoss: 666.6884\n",
      "Training Epoch: 4 [18816/36450]\tLoss: 644.4466\n",
      "Training Epoch: 4 [18880/36450]\tLoss: 654.8561\n",
      "Training Epoch: 4 [18944/36450]\tLoss: 667.4199\n",
      "Training Epoch: 4 [19008/36450]\tLoss: 661.6716\n",
      "Training Epoch: 4 [19072/36450]\tLoss: 670.2159\n",
      "Training Epoch: 4 [19136/36450]\tLoss: 700.1611\n",
      "Training Epoch: 4 [19200/36450]\tLoss: 654.1032\n",
      "Training Epoch: 4 [19264/36450]\tLoss: 674.9341\n",
      "Training Epoch: 4 [19328/36450]\tLoss: 650.4641\n",
      "Training Epoch: 4 [19392/36450]\tLoss: 664.4740\n",
      "Training Epoch: 4 [19456/36450]\tLoss: 714.7780\n",
      "Training Epoch: 4 [19520/36450]\tLoss: 676.9972\n",
      "Training Epoch: 4 [19584/36450]\tLoss: 614.5840\n",
      "Training Epoch: 4 [19648/36450]\tLoss: 661.8916\n",
      "Training Epoch: 4 [19712/36450]\tLoss: 662.4100\n",
      "Training Epoch: 4 [19776/36450]\tLoss: 674.4711\n",
      "Training Epoch: 4 [19840/36450]\tLoss: 649.0392\n",
      "Training Epoch: 4 [19904/36450]\tLoss: 666.3536\n",
      "Training Epoch: 4 [19968/36450]\tLoss: 674.1729\n",
      "Training Epoch: 4 [20032/36450]\tLoss: 705.8918\n",
      "Training Epoch: 4 [20096/36450]\tLoss: 669.3890\n",
      "Training Epoch: 4 [20160/36450]\tLoss: 671.4008\n",
      "Training Epoch: 4 [20224/36450]\tLoss: 681.3953\n",
      "Training Epoch: 4 [20288/36450]\tLoss: 679.1072\n",
      "Training Epoch: 4 [20352/36450]\tLoss: 654.5477\n",
      "Training Epoch: 4 [20416/36450]\tLoss: 663.1810\n",
      "Training Epoch: 4 [20480/36450]\tLoss: 647.8660\n",
      "Training Epoch: 4 [20544/36450]\tLoss: 675.4736\n",
      "Training Epoch: 4 [20608/36450]\tLoss: 628.8453\n",
      "Training Epoch: 4 [20672/36450]\tLoss: 641.2429\n",
      "Training Epoch: 4 [20736/36450]\tLoss: 676.1359\n",
      "Training Epoch: 4 [20800/36450]\tLoss: 675.5692\n",
      "Training Epoch: 4 [20864/36450]\tLoss: 681.9130\n",
      "Training Epoch: 4 [20928/36450]\tLoss: 664.6155\n",
      "Training Epoch: 4 [20992/36450]\tLoss: 685.7540\n",
      "Training Epoch: 4 [21056/36450]\tLoss: 660.3281\n",
      "Training Epoch: 4 [21120/36450]\tLoss: 657.5170\n",
      "Training Epoch: 4 [21184/36450]\tLoss: 666.6459\n",
      "Training Epoch: 4 [21248/36450]\tLoss: 636.9098\n",
      "Training Epoch: 4 [21312/36450]\tLoss: 667.8309\n",
      "Training Epoch: 4 [21376/36450]\tLoss: 675.6234\n",
      "Training Epoch: 4 [21440/36450]\tLoss: 679.4935\n",
      "Training Epoch: 4 [21504/36450]\tLoss: 645.6539\n",
      "Training Epoch: 4 [21568/36450]\tLoss: 665.9213\n",
      "Training Epoch: 4 [21632/36450]\tLoss: 696.2167\n",
      "Training Epoch: 4 [21696/36450]\tLoss: 661.9167\n",
      "Training Epoch: 4 [21760/36450]\tLoss: 685.3320\n",
      "Training Epoch: 4 [21824/36450]\tLoss: 643.9576\n",
      "Training Epoch: 4 [21888/36450]\tLoss: 612.0555\n",
      "Training Epoch: 4 [21952/36450]\tLoss: 659.2217\n",
      "Training Epoch: 4 [22016/36450]\tLoss: 632.7574\n",
      "Training Epoch: 4 [22080/36450]\tLoss: 686.6627\n",
      "Training Epoch: 4 [22144/36450]\tLoss: 685.2486\n",
      "Training Epoch: 4 [22208/36450]\tLoss: 640.5596\n",
      "Training Epoch: 4 [22272/36450]\tLoss: 645.2101\n",
      "Training Epoch: 4 [22336/36450]\tLoss: 653.8641\n",
      "Training Epoch: 4 [22400/36450]\tLoss: 622.6030\n",
      "Training Epoch: 4 [22464/36450]\tLoss: 639.5894\n",
      "Training Epoch: 4 [22528/36450]\tLoss: 681.6942\n",
      "Training Epoch: 4 [22592/36450]\tLoss: 695.6282\n",
      "Training Epoch: 4 [22656/36450]\tLoss: 681.8731\n",
      "Training Epoch: 4 [22720/36450]\tLoss: 661.6508\n",
      "Training Epoch: 4 [22784/36450]\tLoss: 669.5588\n",
      "Training Epoch: 4 [22848/36450]\tLoss: 714.6177\n",
      "Training Epoch: 4 [22912/36450]\tLoss: 662.6909\n",
      "Training Epoch: 4 [22976/36450]\tLoss: 662.1020\n",
      "Training Epoch: 4 [23040/36450]\tLoss: 641.8292\n",
      "Training Epoch: 4 [23104/36450]\tLoss: 693.0353\n",
      "Training Epoch: 4 [23168/36450]\tLoss: 622.3608\n",
      "Training Epoch: 4 [23232/36450]\tLoss: 655.8380\n",
      "Training Epoch: 4 [23296/36450]\tLoss: 690.3695\n",
      "Training Epoch: 4 [23360/36450]\tLoss: 708.5728\n",
      "Training Epoch: 4 [23424/36450]\tLoss: 663.5356\n",
      "Training Epoch: 4 [23488/36450]\tLoss: 662.8792\n",
      "Training Epoch: 4 [23552/36450]\tLoss: 678.5050\n",
      "Training Epoch: 4 [23616/36450]\tLoss: 655.6917\n",
      "Training Epoch: 4 [23680/36450]\tLoss: 623.0621\n",
      "Training Epoch: 4 [23744/36450]\tLoss: 626.3441\n",
      "Training Epoch: 4 [23808/36450]\tLoss: 645.2339\n",
      "Training Epoch: 4 [23872/36450]\tLoss: 691.5132\n",
      "Training Epoch: 4 [23936/36450]\tLoss: 688.3630\n",
      "Training Epoch: 4 [24000/36450]\tLoss: 680.2178\n",
      "Training Epoch: 4 [24064/36450]\tLoss: 647.8568\n",
      "Training Epoch: 4 [24128/36450]\tLoss: 692.6661\n",
      "Training Epoch: 4 [24192/36450]\tLoss: 651.4351\n",
      "Training Epoch: 4 [24256/36450]\tLoss: 666.5316\n",
      "Training Epoch: 4 [24320/36450]\tLoss: 635.4717\n",
      "Training Epoch: 4 [24384/36450]\tLoss: 659.3838\n",
      "Training Epoch: 4 [24448/36450]\tLoss: 648.3071\n",
      "Training Epoch: 4 [24512/36450]\tLoss: 664.8437\n",
      "Training Epoch: 4 [24576/36450]\tLoss: 642.1163\n",
      "Training Epoch: 4 [24640/36450]\tLoss: 688.0939\n",
      "Training Epoch: 4 [24704/36450]\tLoss: 647.0510\n",
      "Training Epoch: 4 [24768/36450]\tLoss: 665.3962\n",
      "Training Epoch: 4 [24832/36450]\tLoss: 625.3447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [24896/36450]\tLoss: 630.4687\n",
      "Training Epoch: 4 [24960/36450]\tLoss: 622.6753\n",
      "Training Epoch: 4 [25024/36450]\tLoss: 669.6115\n",
      "Training Epoch: 4 [25088/36450]\tLoss: 681.0686\n",
      "Training Epoch: 4 [25152/36450]\tLoss: 674.7183\n",
      "Training Epoch: 4 [25216/36450]\tLoss: 623.5303\n",
      "Training Epoch: 4 [25280/36450]\tLoss: 676.4691\n",
      "Training Epoch: 4 [25344/36450]\tLoss: 631.6143\n",
      "Training Epoch: 4 [25408/36450]\tLoss: 668.5410\n",
      "Training Epoch: 4 [25472/36450]\tLoss: 634.2688\n",
      "Training Epoch: 4 [25536/36450]\tLoss: 620.5645\n",
      "Training Epoch: 4 [25600/36450]\tLoss: 687.4420\n",
      "Training Epoch: 4 [25664/36450]\tLoss: 634.8795\n",
      "Training Epoch: 4 [25728/36450]\tLoss: 678.2557\n",
      "Training Epoch: 4 [25792/36450]\tLoss: 655.8481\n",
      "Training Epoch: 4 [25856/36450]\tLoss: 672.1353\n",
      "Training Epoch: 4 [25920/36450]\tLoss: 671.6425\n",
      "Training Epoch: 4 [25984/36450]\tLoss: 643.4932\n",
      "Training Epoch: 4 [26048/36450]\tLoss: 666.7843\n",
      "Training Epoch: 4 [26112/36450]\tLoss: 636.5116\n",
      "Training Epoch: 4 [26176/36450]\tLoss: 671.0103\n",
      "Training Epoch: 4 [26240/36450]\tLoss: 668.0546\n",
      "Training Epoch: 4 [26304/36450]\tLoss: 616.1325\n",
      "Training Epoch: 4 [26368/36450]\tLoss: 661.8361\n",
      "Training Epoch: 4 [26432/36450]\tLoss: 655.1154\n",
      "Training Epoch: 4 [26496/36450]\tLoss: 633.6674\n",
      "Training Epoch: 4 [26560/36450]\tLoss: 671.2729\n",
      "Training Epoch: 4 [26624/36450]\tLoss: 673.6702\n",
      "Training Epoch: 4 [26688/36450]\tLoss: 616.6364\n",
      "Training Epoch: 4 [26752/36450]\tLoss: 659.9591\n",
      "Training Epoch: 4 [26816/36450]\tLoss: 624.7177\n",
      "Training Epoch: 4 [26880/36450]\tLoss: 655.9217\n",
      "Training Epoch: 4 [26944/36450]\tLoss: 651.4489\n",
      "Training Epoch: 4 [27008/36450]\tLoss: 665.5734\n",
      "Training Epoch: 4 [27072/36450]\tLoss: 669.7321\n",
      "Training Epoch: 4 [27136/36450]\tLoss: 660.2100\n",
      "Training Epoch: 4 [27200/36450]\tLoss: 677.8658\n",
      "Training Epoch: 4 [27264/36450]\tLoss: 645.7407\n",
      "Training Epoch: 4 [27328/36450]\tLoss: 650.6943\n",
      "Training Epoch: 4 [27392/36450]\tLoss: 624.0386\n",
      "Training Epoch: 4 [27456/36450]\tLoss: 625.6315\n",
      "Training Epoch: 4 [27520/36450]\tLoss: 674.8417\n",
      "Training Epoch: 4 [27584/36450]\tLoss: 634.1713\n",
      "Training Epoch: 4 [27648/36450]\tLoss: 629.1113\n",
      "Training Epoch: 4 [27712/36450]\tLoss: 672.0094\n",
      "Training Epoch: 4 [27776/36450]\tLoss: 670.2828\n",
      "Training Epoch: 4 [27840/36450]\tLoss: 656.9210\n",
      "Training Epoch: 4 [27904/36450]\tLoss: 685.3327\n",
      "Training Epoch: 4 [27968/36450]\tLoss: 671.2043\n",
      "Training Epoch: 4 [28032/36450]\tLoss: 640.0194\n",
      "Training Epoch: 4 [28096/36450]\tLoss: 655.1022\n",
      "Training Epoch: 4 [28160/36450]\tLoss: 656.8250\n",
      "Training Epoch: 4 [28224/36450]\tLoss: 639.5894\n",
      "Training Epoch: 4 [28288/36450]\tLoss: 631.0662\n",
      "Training Epoch: 4 [28352/36450]\tLoss: 656.7702\n",
      "Training Epoch: 4 [28416/36450]\tLoss: 627.0354\n",
      "Training Epoch: 4 [28480/36450]\tLoss: 632.6221\n",
      "Training Epoch: 4 [28544/36450]\tLoss: 660.4207\n",
      "Training Epoch: 4 [28608/36450]\tLoss: 658.4315\n",
      "Training Epoch: 4 [28672/36450]\tLoss: 678.5278\n",
      "Training Epoch: 4 [28736/36450]\tLoss: 626.8340\n",
      "Training Epoch: 4 [28800/36450]\tLoss: 674.6276\n",
      "Training Epoch: 4 [28864/36450]\tLoss: 647.5106\n",
      "Training Epoch: 4 [28928/36450]\tLoss: 604.0861\n",
      "Training Epoch: 4 [28992/36450]\tLoss: 634.8557\n",
      "Training Epoch: 4 [29056/36450]\tLoss: 653.8461\n",
      "Training Epoch: 4 [29120/36450]\tLoss: 666.2020\n",
      "Training Epoch: 4 [29184/36450]\tLoss: 688.0772\n",
      "Training Epoch: 4 [29248/36450]\tLoss: 641.7803\n",
      "Training Epoch: 4 [29312/36450]\tLoss: 646.5654\n",
      "Training Epoch: 4 [29376/36450]\tLoss: 631.5372\n",
      "Training Epoch: 4 [29440/36450]\tLoss: 628.2467\n",
      "Training Epoch: 4 [29504/36450]\tLoss: 640.9673\n",
      "Training Epoch: 4 [29568/36450]\tLoss: 666.3346\n",
      "Training Epoch: 4 [29632/36450]\tLoss: 665.8655\n",
      "Training Epoch: 4 [29696/36450]\tLoss: 655.4073\n",
      "Training Epoch: 4 [29760/36450]\tLoss: 648.0323\n",
      "Training Epoch: 4 [29824/36450]\tLoss: 686.0844\n",
      "Training Epoch: 4 [29888/36450]\tLoss: 655.7035\n",
      "Training Epoch: 4 [29952/36450]\tLoss: 639.1429\n",
      "Training Epoch: 4 [30016/36450]\tLoss: 665.6127\n",
      "Training Epoch: 4 [30080/36450]\tLoss: 687.4568\n",
      "Training Epoch: 4 [30144/36450]\tLoss: 660.4526\n",
      "Training Epoch: 4 [30208/36450]\tLoss: 656.3936\n",
      "Training Epoch: 4 [30272/36450]\tLoss: 630.3813\n",
      "Training Epoch: 4 [30336/36450]\tLoss: 668.7433\n",
      "Training Epoch: 4 [30400/36450]\tLoss: 644.8660\n",
      "Training Epoch: 4 [30464/36450]\tLoss: 637.6919\n",
      "Training Epoch: 4 [30528/36450]\tLoss: 658.0071\n",
      "Training Epoch: 4 [30592/36450]\tLoss: 629.4902\n",
      "Training Epoch: 4 [30656/36450]\tLoss: 618.0956\n",
      "Training Epoch: 4 [30720/36450]\tLoss: 631.3917\n",
      "Training Epoch: 4 [30784/36450]\tLoss: 668.8884\n",
      "Training Epoch: 4 [30848/36450]\tLoss: 644.9185\n",
      "Training Epoch: 4 [30912/36450]\tLoss: 638.0647\n",
      "Training Epoch: 4 [30976/36450]\tLoss: 650.5067\n",
      "Training Epoch: 4 [31040/36450]\tLoss: 655.7952\n",
      "Training Epoch: 4 [31104/36450]\tLoss: 621.7924\n",
      "Training Epoch: 4 [31168/36450]\tLoss: 604.7278\n",
      "Training Epoch: 4 [31232/36450]\tLoss: 668.3868\n",
      "Training Epoch: 4 [31296/36450]\tLoss: 639.3245\n",
      "Training Epoch: 4 [31360/36450]\tLoss: 633.0326\n",
      "Training Epoch: 4 [31424/36450]\tLoss: 623.9589\n",
      "Training Epoch: 4 [31488/36450]\tLoss: 624.2817\n",
      "Training Epoch: 4 [31552/36450]\tLoss: 679.3295\n",
      "Training Epoch: 4 [31616/36450]\tLoss: 687.8553\n",
      "Training Epoch: 4 [31680/36450]\tLoss: 611.9585\n",
      "Training Epoch: 4 [31744/36450]\tLoss: 649.7260\n",
      "Training Epoch: 4 [31808/36450]\tLoss: 619.6199\n",
      "Training Epoch: 4 [31872/36450]\tLoss: 658.7568\n",
      "Training Epoch: 4 [31936/36450]\tLoss: 631.9596\n",
      "Training Epoch: 4 [32000/36450]\tLoss: 632.6847\n",
      "Training Epoch: 4 [32064/36450]\tLoss: 643.3190\n",
      "Training Epoch: 4 [32128/36450]\tLoss: 639.9208\n",
      "Training Epoch: 4 [32192/36450]\tLoss: 656.8165\n",
      "Training Epoch: 4 [32256/36450]\tLoss: 709.2336\n",
      "Training Epoch: 4 [32320/36450]\tLoss: 605.9736\n",
      "Training Epoch: 4 [32384/36450]\tLoss: 648.6072\n",
      "Training Epoch: 4 [32448/36450]\tLoss: 648.0945\n",
      "Training Epoch: 4 [32512/36450]\tLoss: 674.5044\n",
      "Training Epoch: 4 [32576/36450]\tLoss: 666.3270\n",
      "Training Epoch: 4 [32640/36450]\tLoss: 617.7242\n",
      "Training Epoch: 4 [32704/36450]\tLoss: 636.6611\n",
      "Training Epoch: 4 [32768/36450]\tLoss: 657.0435\n",
      "Training Epoch: 4 [32832/36450]\tLoss: 628.8557\n",
      "Training Epoch: 4 [32896/36450]\tLoss: 701.0801\n",
      "Training Epoch: 4 [32960/36450]\tLoss: 650.8822\n",
      "Training Epoch: 4 [33024/36450]\tLoss: 612.5834\n",
      "Training Epoch: 4 [33088/36450]\tLoss: 660.7129\n",
      "Training Epoch: 4 [33152/36450]\tLoss: 640.9753\n",
      "Training Epoch: 4 [33216/36450]\tLoss: 635.7448\n",
      "Training Epoch: 4 [33280/36450]\tLoss: 662.5314\n",
      "Training Epoch: 4 [33344/36450]\tLoss: 632.8868\n",
      "Training Epoch: 4 [33408/36450]\tLoss: 639.5502\n",
      "Training Epoch: 4 [33472/36450]\tLoss: 632.0929\n",
      "Training Epoch: 4 [33536/36450]\tLoss: 634.8748\n",
      "Training Epoch: 4 [33600/36450]\tLoss: 634.4501\n",
      "Training Epoch: 4 [33664/36450]\tLoss: 638.6309\n",
      "Training Epoch: 4 [33728/36450]\tLoss: 633.9211\n",
      "Training Epoch: 4 [33792/36450]\tLoss: 634.3531\n",
      "Training Epoch: 4 [33856/36450]\tLoss: 641.3226\n",
      "Training Epoch: 4 [33920/36450]\tLoss: 658.2691\n",
      "Training Epoch: 4 [33984/36450]\tLoss: 617.5923\n",
      "Training Epoch: 4 [34048/36450]\tLoss: 675.6107\n",
      "Training Epoch: 4 [34112/36450]\tLoss: 615.6391\n",
      "Training Epoch: 4 [34176/36450]\tLoss: 656.4944\n",
      "Training Epoch: 4 [34240/36450]\tLoss: 651.4155\n",
      "Training Epoch: 4 [34304/36450]\tLoss: 673.6500\n",
      "Training Epoch: 4 [34368/36450]\tLoss: 687.0302\n",
      "Training Epoch: 4 [34432/36450]\tLoss: 673.1534\n",
      "Training Epoch: 4 [34496/36450]\tLoss: 676.5768\n",
      "Training Epoch: 4 [34560/36450]\tLoss: 617.8289\n",
      "Training Epoch: 4 [34624/36450]\tLoss: 667.2522\n",
      "Training Epoch: 4 [34688/36450]\tLoss: 638.1557\n",
      "Training Epoch: 4 [34752/36450]\tLoss: 665.1660\n",
      "Training Epoch: 4 [34816/36450]\tLoss: 663.0770\n",
      "Training Epoch: 4 [34880/36450]\tLoss: 600.3267\n",
      "Training Epoch: 4 [34944/36450]\tLoss: 630.6042\n",
      "Training Epoch: 4 [35008/36450]\tLoss: 600.1440\n",
      "Training Epoch: 4 [35072/36450]\tLoss: 704.5430\n",
      "Training Epoch: 4 [35136/36450]\tLoss: 635.4634\n",
      "Training Epoch: 4 [35200/36450]\tLoss: 624.8363\n",
      "Training Epoch: 4 [35264/36450]\tLoss: 655.0347\n",
      "Training Epoch: 4 [35328/36450]\tLoss: 647.0448\n",
      "Training Epoch: 4 [35392/36450]\tLoss: 607.2612\n",
      "Training Epoch: 4 [35456/36450]\tLoss: 660.4408\n",
      "Training Epoch: 4 [35520/36450]\tLoss: 632.3845\n",
      "Training Epoch: 4 [35584/36450]\tLoss: 642.5186\n",
      "Training Epoch: 4 [35648/36450]\tLoss: 636.0370\n",
      "Training Epoch: 4 [35712/36450]\tLoss: 655.7139\n",
      "Training Epoch: 4 [35776/36450]\tLoss: 670.4001\n",
      "Training Epoch: 4 [35840/36450]\tLoss: 612.3588\n",
      "Training Epoch: 4 [35904/36450]\tLoss: 656.4202\n",
      "Training Epoch: 4 [35968/36450]\tLoss: 649.2667\n",
      "Training Epoch: 4 [36032/36450]\tLoss: 666.9236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [36096/36450]\tLoss: 634.7034\n",
      "Training Epoch: 4 [36160/36450]\tLoss: 636.2598\n",
      "Training Epoch: 4 [36224/36450]\tLoss: 645.8422\n",
      "Training Epoch: 4 [36288/36450]\tLoss: 655.7424\n",
      "Training Epoch: 4 [36352/36450]\tLoss: 657.0878\n",
      "Training Epoch: 4 [36416/36450]\tLoss: 644.0007\n",
      "Training Epoch: 4 [36450/36450]\tLoss: 664.3168\n",
      "Training Epoch: 4 [4050/4050]\tLoss: 319.7179\n",
      "Training Epoch: 5 [64/36450]\tLoss: 612.1985\n",
      "Training Epoch: 5 [128/36450]\tLoss: 659.1519\n",
      "Training Epoch: 5 [192/36450]\tLoss: 645.1472\n",
      "Training Epoch: 5 [256/36450]\tLoss: 633.8497\n",
      "Training Epoch: 5 [320/36450]\tLoss: 658.5702\n",
      "Training Epoch: 5 [384/36450]\tLoss: 649.1477\n",
      "Training Epoch: 5 [448/36450]\tLoss: 660.7386\n",
      "Training Epoch: 5 [512/36450]\tLoss: 631.4783\n",
      "Training Epoch: 5 [576/36450]\tLoss: 668.6807\n",
      "Training Epoch: 5 [640/36450]\tLoss: 648.6240\n",
      "Training Epoch: 5 [704/36450]\tLoss: 680.9828\n",
      "Training Epoch: 5 [768/36450]\tLoss: 645.4388\n",
      "Training Epoch: 5 [832/36450]\tLoss: 669.6180\n",
      "Training Epoch: 5 [896/36450]\tLoss: 650.5514\n",
      "Training Epoch: 5 [960/36450]\tLoss: 674.9913\n",
      "Training Epoch: 5 [1024/36450]\tLoss: 620.6371\n",
      "Training Epoch: 5 [1088/36450]\tLoss: 634.2961\n",
      "Training Epoch: 5 [1152/36450]\tLoss: 674.5040\n",
      "Training Epoch: 5 [1216/36450]\tLoss: 624.3017\n",
      "Training Epoch: 5 [1280/36450]\tLoss: 633.0989\n",
      "Training Epoch: 5 [1344/36450]\tLoss: 611.5961\n",
      "Training Epoch: 5 [1408/36450]\tLoss: 651.8071\n",
      "Training Epoch: 5 [1472/36450]\tLoss: 655.9659\n",
      "Training Epoch: 5 [1536/36450]\tLoss: 612.7569\n",
      "Training Epoch: 5 [1600/36450]\tLoss: 690.3231\n",
      "Training Epoch: 5 [1664/36450]\tLoss: 685.2602\n",
      "Training Epoch: 5 [1728/36450]\tLoss: 675.3666\n",
      "Training Epoch: 5 [1792/36450]\tLoss: 655.4382\n",
      "Training Epoch: 5 [1856/36450]\tLoss: 644.2466\n",
      "Training Epoch: 5 [1920/36450]\tLoss: 646.6833\n",
      "Training Epoch: 5 [1984/36450]\tLoss: 646.8026\n",
      "Training Epoch: 5 [2048/36450]\tLoss: 660.1743\n",
      "Training Epoch: 5 [2112/36450]\tLoss: 630.4406\n",
      "Training Epoch: 5 [2176/36450]\tLoss: 653.4930\n",
      "Training Epoch: 5 [2240/36450]\tLoss: 680.6427\n",
      "Training Epoch: 5 [2304/36450]\tLoss: 688.9883\n",
      "Training Epoch: 5 [2368/36450]\tLoss: 628.9739\n",
      "Training Epoch: 5 [2432/36450]\tLoss: 663.8531\n",
      "Training Epoch: 5 [2496/36450]\tLoss: 625.0536\n",
      "Training Epoch: 5 [2560/36450]\tLoss: 670.5323\n",
      "Training Epoch: 5 [2624/36450]\tLoss: 654.0926\n",
      "Training Epoch: 5 [2688/36450]\tLoss: 657.8708\n",
      "Training Epoch: 5 [2752/36450]\tLoss: 648.8510\n",
      "Training Epoch: 5 [2816/36450]\tLoss: 672.9296\n",
      "Training Epoch: 5 [2880/36450]\tLoss: 647.9459\n",
      "Training Epoch: 5 [2944/36450]\tLoss: 636.7706\n",
      "Training Epoch: 5 [3008/36450]\tLoss: 605.1843\n",
      "Training Epoch: 5 [3072/36450]\tLoss: 638.9901\n",
      "Training Epoch: 5 [3136/36450]\tLoss: 656.6907\n",
      "Training Epoch: 5 [3200/36450]\tLoss: 602.5152\n",
      "Training Epoch: 5 [3264/36450]\tLoss: 643.8631\n",
      "Training Epoch: 5 [3328/36450]\tLoss: 664.8425\n",
      "Training Epoch: 5 [3392/36450]\tLoss: 671.0626\n",
      "Training Epoch: 5 [3456/36450]\tLoss: 637.1453\n",
      "Training Epoch: 5 [3520/36450]\tLoss: 619.7206\n",
      "Training Epoch: 5 [3584/36450]\tLoss: 607.9905\n",
      "Training Epoch: 5 [3648/36450]\tLoss: 623.0372\n",
      "Training Epoch: 5 [3712/36450]\tLoss: 614.1405\n",
      "Training Epoch: 5 [3776/36450]\tLoss: 667.5739\n",
      "Training Epoch: 5 [3840/36450]\tLoss: 663.1707\n",
      "Training Epoch: 5 [3904/36450]\tLoss: 675.8019\n",
      "Training Epoch: 5 [3968/36450]\tLoss: 633.1008\n",
      "Training Epoch: 5 [4032/36450]\tLoss: 622.0504\n",
      "Training Epoch: 5 [4096/36450]\tLoss: 658.3036\n",
      "Training Epoch: 5 [4160/36450]\tLoss: 643.9265\n",
      "Training Epoch: 5 [4224/36450]\tLoss: 601.1234\n",
      "Training Epoch: 5 [4288/36450]\tLoss: 623.5367\n",
      "Training Epoch: 5 [4352/36450]\tLoss: 663.7524\n",
      "Training Epoch: 5 [4416/36450]\tLoss: 604.4121\n",
      "Training Epoch: 5 [4480/36450]\tLoss: 625.1959\n",
      "Training Epoch: 5 [4544/36450]\tLoss: 629.0120\n",
      "Training Epoch: 5 [4608/36450]\tLoss: 604.3892\n",
      "Training Epoch: 5 [4672/36450]\tLoss: 718.0245\n",
      "Training Epoch: 5 [4736/36450]\tLoss: 603.6136\n",
      "Training Epoch: 5 [4800/36450]\tLoss: 630.2409\n",
      "Training Epoch: 5 [4864/36450]\tLoss: 646.9062\n",
      "Training Epoch: 5 [4928/36450]\tLoss: 613.5862\n",
      "Training Epoch: 5 [4992/36450]\tLoss: 655.3299\n",
      "Training Epoch: 5 [5056/36450]\tLoss: 660.5080\n",
      "Training Epoch: 5 [5120/36450]\tLoss: 665.9283\n",
      "Training Epoch: 5 [5184/36450]\tLoss: 631.7390\n",
      "Training Epoch: 5 [5248/36450]\tLoss: 629.0350\n",
      "Training Epoch: 5 [5312/36450]\tLoss: 598.8953\n",
      "Training Epoch: 5 [5376/36450]\tLoss: 655.0219\n",
      "Training Epoch: 5 [5440/36450]\tLoss: 639.2845\n",
      "Training Epoch: 5 [5504/36450]\tLoss: 629.7025\n",
      "Training Epoch: 5 [5568/36450]\tLoss: 645.5135\n",
      "Training Epoch: 5 [5632/36450]\tLoss: 629.4702\n",
      "Training Epoch: 5 [5696/36450]\tLoss: 650.3454\n",
      "Training Epoch: 5 [5760/36450]\tLoss: 651.6803\n",
      "Training Epoch: 5 [5824/36450]\tLoss: 608.3076\n",
      "Training Epoch: 5 [5888/36450]\tLoss: 665.1440\n",
      "Training Epoch: 5 [5952/36450]\tLoss: 642.5577\n",
      "Training Epoch: 5 [6016/36450]\tLoss: 642.2084\n",
      "Training Epoch: 5 [6080/36450]\tLoss: 616.2730\n",
      "Training Epoch: 5 [6144/36450]\tLoss: 611.4384\n",
      "Training Epoch: 5 [6208/36450]\tLoss: 636.6058\n",
      "Training Epoch: 5 [6272/36450]\tLoss: 608.0264\n",
      "Training Epoch: 5 [6336/36450]\tLoss: 666.1315\n",
      "Training Epoch: 5 [6400/36450]\tLoss: 657.7864\n",
      "Training Epoch: 5 [6464/36450]\tLoss: 615.7576\n",
      "Training Epoch: 5 [6528/36450]\tLoss: 638.3329\n",
      "Training Epoch: 5 [6592/36450]\tLoss: 653.9528\n",
      "Training Epoch: 5 [6656/36450]\tLoss: 631.2766\n",
      "Training Epoch: 5 [6720/36450]\tLoss: 635.6312\n",
      "Training Epoch: 5 [6784/36450]\tLoss: 670.3834\n",
      "Training Epoch: 5 [6848/36450]\tLoss: 654.2726\n",
      "Training Epoch: 5 [6912/36450]\tLoss: 647.3820\n",
      "Training Epoch: 5 [6976/36450]\tLoss: 620.5400\n",
      "Training Epoch: 5 [7040/36450]\tLoss: 659.7697\n",
      "Training Epoch: 5 [7104/36450]\tLoss: 643.2849\n",
      "Training Epoch: 5 [7168/36450]\tLoss: 633.4119\n",
      "Training Epoch: 5 [7232/36450]\tLoss: 626.6119\n",
      "Training Epoch: 5 [7296/36450]\tLoss: 609.9346\n",
      "Training Epoch: 5 [7360/36450]\tLoss: 613.1436\n",
      "Training Epoch: 5 [7424/36450]\tLoss: 611.3040\n",
      "Training Epoch: 5 [7488/36450]\tLoss: 636.2360\n",
      "Training Epoch: 5 [7552/36450]\tLoss: 631.6047\n",
      "Training Epoch: 5 [7616/36450]\tLoss: 630.7766\n",
      "Training Epoch: 5 [7680/36450]\tLoss: 649.0737\n",
      "Training Epoch: 5 [7744/36450]\tLoss: 603.2152\n",
      "Training Epoch: 5 [7808/36450]\tLoss: 633.3541\n",
      "Training Epoch: 5 [7872/36450]\tLoss: 663.9813\n",
      "Training Epoch: 5 [7936/36450]\tLoss: 662.2444\n",
      "Training Epoch: 5 [8000/36450]\tLoss: 613.8125\n",
      "Training Epoch: 5 [8064/36450]\tLoss: 601.3372\n",
      "Training Epoch: 5 [8128/36450]\tLoss: 620.6135\n",
      "Training Epoch: 5 [8192/36450]\tLoss: 627.3078\n",
      "Training Epoch: 5 [8256/36450]\tLoss: 605.5344\n",
      "Training Epoch: 5 [8320/36450]\tLoss: 638.5755\n",
      "Training Epoch: 5 [8384/36450]\tLoss: 629.4882\n",
      "Training Epoch: 5 [8448/36450]\tLoss: 690.0855\n",
      "Training Epoch: 5 [8512/36450]\tLoss: 611.4849\n",
      "Training Epoch: 5 [8576/36450]\tLoss: 633.9684\n",
      "Training Epoch: 5 [8640/36450]\tLoss: 626.1625\n",
      "Training Epoch: 5 [8704/36450]\tLoss: 665.4897\n",
      "Training Epoch: 5 [8768/36450]\tLoss: 625.2166\n",
      "Training Epoch: 5 [8832/36450]\tLoss: 642.2790\n",
      "Training Epoch: 5 [8896/36450]\tLoss: 629.5863\n",
      "Training Epoch: 5 [8960/36450]\tLoss: 641.4825\n",
      "Training Epoch: 5 [9024/36450]\tLoss: 598.4160\n",
      "Training Epoch: 5 [9088/36450]\tLoss: 626.2675\n",
      "Training Epoch: 5 [9152/36450]\tLoss: 625.3699\n",
      "Training Epoch: 5 [9216/36450]\tLoss: 596.3033\n",
      "Training Epoch: 5 [9280/36450]\tLoss: 653.0028\n",
      "Training Epoch: 5 [9344/36450]\tLoss: 649.5449\n",
      "Training Epoch: 5 [9408/36450]\tLoss: 628.6159\n",
      "Training Epoch: 5 [9472/36450]\tLoss: 644.0516\n",
      "Training Epoch: 5 [9536/36450]\tLoss: 618.5483\n",
      "Training Epoch: 5 [9600/36450]\tLoss: 637.9479\n",
      "Training Epoch: 5 [9664/36450]\tLoss: 611.5688\n",
      "Training Epoch: 5 [9728/36450]\tLoss: 618.1324\n",
      "Training Epoch: 5 [9792/36450]\tLoss: 592.3455\n",
      "Training Epoch: 5 [9856/36450]\tLoss: 648.8441\n",
      "Training Epoch: 5 [9920/36450]\tLoss: 620.9053\n",
      "Training Epoch: 5 [9984/36450]\tLoss: 649.0312\n",
      "Training Epoch: 5 [10048/36450]\tLoss: 628.5419\n",
      "Training Epoch: 5 [10112/36450]\tLoss: 659.9155\n",
      "Training Epoch: 5 [10176/36450]\tLoss: 617.1473\n",
      "Training Epoch: 5 [10240/36450]\tLoss: 637.7089\n",
      "Training Epoch: 5 [10304/36450]\tLoss: 601.2675\n",
      "Training Epoch: 5 [10368/36450]\tLoss: 661.7294\n",
      "Training Epoch: 5 [10432/36450]\tLoss: 610.2775\n",
      "Training Epoch: 5 [10496/36450]\tLoss: 652.4634\n",
      "Training Epoch: 5 [10560/36450]\tLoss: 647.6747\n",
      "Training Epoch: 5 [10624/36450]\tLoss: 629.9169\n",
      "Training Epoch: 5 [10688/36450]\tLoss: 658.0948\n",
      "Training Epoch: 5 [10752/36450]\tLoss: 573.3956\n",
      "Training Epoch: 5 [10816/36450]\tLoss: 637.8866\n",
      "Training Epoch: 5 [10880/36450]\tLoss: 614.6968\n",
      "Training Epoch: 5 [10944/36450]\tLoss: 589.1767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [11008/36450]\tLoss: 653.6506\n",
      "Training Epoch: 5 [11072/36450]\tLoss: 603.9258\n",
      "Training Epoch: 5 [11136/36450]\tLoss: 629.0059\n",
      "Training Epoch: 5 [11200/36450]\tLoss: 604.9105\n",
      "Training Epoch: 5 [11264/36450]\tLoss: 656.0858\n",
      "Training Epoch: 5 [11328/36450]\tLoss: 636.1685\n",
      "Training Epoch: 5 [11392/36450]\tLoss: 692.7831\n",
      "Training Epoch: 5 [11456/36450]\tLoss: 643.5282\n",
      "Training Epoch: 5 [11520/36450]\tLoss: 659.7994\n",
      "Training Epoch: 5 [11584/36450]\tLoss: 653.7727\n",
      "Training Epoch: 5 [11648/36450]\tLoss: 646.2255\n",
      "Training Epoch: 5 [11712/36450]\tLoss: 636.7363\n",
      "Training Epoch: 5 [11776/36450]\tLoss: 674.2407\n",
      "Training Epoch: 5 [11840/36450]\tLoss: 656.4611\n",
      "Training Epoch: 5 [11904/36450]\tLoss: 634.2855\n",
      "Training Epoch: 5 [11968/36450]\tLoss: 630.2585\n",
      "Training Epoch: 5 [12032/36450]\tLoss: 608.2915\n",
      "Training Epoch: 5 [12096/36450]\tLoss: 658.4377\n",
      "Training Epoch: 5 [12160/36450]\tLoss: 610.4109\n",
      "Training Epoch: 5 [12224/36450]\tLoss: 653.4090\n",
      "Training Epoch: 5 [12288/36450]\tLoss: 635.8119\n",
      "Training Epoch: 5 [12352/36450]\tLoss: 634.9623\n",
      "Training Epoch: 5 [12416/36450]\tLoss: 634.3456\n",
      "Training Epoch: 5 [12480/36450]\tLoss: 658.5140\n",
      "Training Epoch: 5 [12544/36450]\tLoss: 651.6046\n",
      "Training Epoch: 5 [12608/36450]\tLoss: 646.3568\n",
      "Training Epoch: 5 [12672/36450]\tLoss: 621.4323\n",
      "Training Epoch: 5 [12736/36450]\tLoss: 642.3789\n",
      "Training Epoch: 5 [12800/36450]\tLoss: 651.8104\n",
      "Training Epoch: 5 [12864/36450]\tLoss: 592.1416\n",
      "Training Epoch: 5 [12928/36450]\tLoss: 628.0772\n",
      "Training Epoch: 5 [12992/36450]\tLoss: 603.5474\n",
      "Training Epoch: 5 [13056/36450]\tLoss: 654.0504\n",
      "Training Epoch: 5 [13120/36450]\tLoss: 634.3758\n",
      "Training Epoch: 5 [13184/36450]\tLoss: 612.0437\n",
      "Training Epoch: 5 [13248/36450]\tLoss: 646.8824\n",
      "Training Epoch: 5 [13312/36450]\tLoss: 625.1057\n",
      "Training Epoch: 5 [13376/36450]\tLoss: 639.2996\n",
      "Training Epoch: 5 [13440/36450]\tLoss: 620.5928\n",
      "Training Epoch: 5 [13504/36450]\tLoss: 635.7898\n",
      "Training Epoch: 5 [13568/36450]\tLoss: 614.0813\n",
      "Training Epoch: 5 [13632/36450]\tLoss: 612.9371\n",
      "Training Epoch: 5 [13696/36450]\tLoss: 630.4294\n",
      "Training Epoch: 5 [13760/36450]\tLoss: 636.7825\n",
      "Training Epoch: 5 [13824/36450]\tLoss: 603.6006\n",
      "Training Epoch: 5 [13888/36450]\tLoss: 635.1140\n",
      "Training Epoch: 5 [13952/36450]\tLoss: 620.8651\n",
      "Training Epoch: 5 [14016/36450]\tLoss: 650.7106\n",
      "Training Epoch: 5 [14080/36450]\tLoss: 651.2192\n",
      "Training Epoch: 5 [14144/36450]\tLoss: 608.8310\n",
      "Training Epoch: 5 [14208/36450]\tLoss: 608.6008\n",
      "Training Epoch: 5 [14272/36450]\tLoss: 612.4523\n",
      "Training Epoch: 5 [14336/36450]\tLoss: 582.7711\n",
      "Training Epoch: 5 [14400/36450]\tLoss: 610.0928\n",
      "Training Epoch: 5 [14464/36450]\tLoss: 629.5221\n",
      "Training Epoch: 5 [14528/36450]\tLoss: 632.4241\n",
      "Training Epoch: 5 [14592/36450]\tLoss: 601.0300\n",
      "Training Epoch: 5 [14656/36450]\tLoss: 633.3317\n",
      "Training Epoch: 5 [14720/36450]\tLoss: 635.1160\n",
      "Training Epoch: 5 [14784/36450]\tLoss: 634.3531\n",
      "Training Epoch: 5 [14848/36450]\tLoss: 634.5471\n",
      "Training Epoch: 5 [14912/36450]\tLoss: 621.2666\n",
      "Training Epoch: 5 [14976/36450]\tLoss: 606.4090\n",
      "Training Epoch: 5 [15040/36450]\tLoss: 604.0547\n",
      "Training Epoch: 5 [15104/36450]\tLoss: 612.0596\n",
      "Training Epoch: 5 [15168/36450]\tLoss: 618.3275\n",
      "Training Epoch: 5 [15232/36450]\tLoss: 632.2673\n",
      "Training Epoch: 5 [15296/36450]\tLoss: 595.2896\n",
      "Training Epoch: 5 [15360/36450]\tLoss: 630.8883\n",
      "Training Epoch: 5 [15424/36450]\tLoss: 646.1396\n",
      "Training Epoch: 5 [15488/36450]\tLoss: 596.9318\n",
      "Training Epoch: 5 [15552/36450]\tLoss: 595.8215\n",
      "Training Epoch: 5 [15616/36450]\tLoss: 642.8670\n",
      "Training Epoch: 5 [15680/36450]\tLoss: 623.6916\n",
      "Training Epoch: 5 [15744/36450]\tLoss: 640.9229\n",
      "Training Epoch: 5 [15808/36450]\tLoss: 621.2598\n",
      "Training Epoch: 5 [15872/36450]\tLoss: 617.2339\n",
      "Training Epoch: 5 [15936/36450]\tLoss: 618.8273\n",
      "Training Epoch: 5 [16000/36450]\tLoss: 614.6985\n",
      "Training Epoch: 5 [16064/36450]\tLoss: 668.1794\n",
      "Training Epoch: 5 [16128/36450]\tLoss: 635.5586\n",
      "Training Epoch: 5 [16192/36450]\tLoss: 608.6363\n",
      "Training Epoch: 5 [16256/36450]\tLoss: 609.6268\n",
      "Training Epoch: 5 [16320/36450]\tLoss: 607.5917\n",
      "Training Epoch: 5 [16384/36450]\tLoss: 600.4417\n",
      "Training Epoch: 5 [16448/36450]\tLoss: 600.0497\n",
      "Training Epoch: 5 [16512/36450]\tLoss: 601.6623\n",
      "Training Epoch: 5 [16576/36450]\tLoss: 623.0450\n",
      "Training Epoch: 5 [16640/36450]\tLoss: 607.5179\n",
      "Training Epoch: 5 [16704/36450]\tLoss: 640.4743\n",
      "Training Epoch: 5 [16768/36450]\tLoss: 616.9165\n",
      "Training Epoch: 5 [16832/36450]\tLoss: 632.9860\n",
      "Training Epoch: 5 [16896/36450]\tLoss: 661.2601\n",
      "Training Epoch: 5 [16960/36450]\tLoss: 641.6678\n",
      "Training Epoch: 5 [17024/36450]\tLoss: 647.6299\n",
      "Training Epoch: 5 [17088/36450]\tLoss: 624.8494\n",
      "Training Epoch: 5 [17152/36450]\tLoss: 613.1831\n",
      "Training Epoch: 5 [17216/36450]\tLoss: 656.4954\n",
      "Training Epoch: 5 [17280/36450]\tLoss: 628.4791\n",
      "Training Epoch: 5 [17344/36450]\tLoss: 576.5452\n",
      "Training Epoch: 5 [17408/36450]\tLoss: 638.4730\n",
      "Training Epoch: 5 [17472/36450]\tLoss: 640.2731\n",
      "Training Epoch: 5 [17536/36450]\tLoss: 624.8107\n",
      "Training Epoch: 5 [17600/36450]\tLoss: 624.4796\n",
      "Training Epoch: 5 [17664/36450]\tLoss: 608.5320\n",
      "Training Epoch: 5 [17728/36450]\tLoss: 664.6367\n",
      "Training Epoch: 5 [17792/36450]\tLoss: 623.8091\n",
      "Training Epoch: 5 [17856/36450]\tLoss: 659.7660\n",
      "Training Epoch: 5 [17920/36450]\tLoss: 663.3306\n",
      "Training Epoch: 5 [17984/36450]\tLoss: 631.3770\n",
      "Training Epoch: 5 [18048/36450]\tLoss: 602.5472\n",
      "Training Epoch: 5 [18112/36450]\tLoss: 631.6927\n",
      "Training Epoch: 5 [18176/36450]\tLoss: 672.0721\n",
      "Training Epoch: 5 [18240/36450]\tLoss: 636.5752\n",
      "Training Epoch: 5 [18304/36450]\tLoss: 596.9189\n",
      "Training Epoch: 5 [18368/36450]\tLoss: 619.8253\n",
      "Training Epoch: 5 [18432/36450]\tLoss: 661.5264\n",
      "Training Epoch: 5 [18496/36450]\tLoss: 638.4017\n",
      "Training Epoch: 5 [18560/36450]\tLoss: 652.4026\n",
      "Training Epoch: 5 [18624/36450]\tLoss: 570.0472\n",
      "Training Epoch: 5 [18688/36450]\tLoss: 627.8294\n",
      "Training Epoch: 5 [18752/36450]\tLoss: 601.6559\n",
      "Training Epoch: 5 [18816/36450]\tLoss: 591.8769\n",
      "Training Epoch: 5 [18880/36450]\tLoss: 662.5439\n",
      "Training Epoch: 5 [18944/36450]\tLoss: 605.8156\n",
      "Training Epoch: 5 [19008/36450]\tLoss: 618.1899\n",
      "Training Epoch: 5 [19072/36450]\tLoss: 615.4996\n",
      "Training Epoch: 5 [19136/36450]\tLoss: 656.4990\n",
      "Training Epoch: 5 [19200/36450]\tLoss: 609.7789\n",
      "Training Epoch: 5 [19264/36450]\tLoss: 590.0344\n",
      "Training Epoch: 5 [19328/36450]\tLoss: 605.0582\n",
      "Training Epoch: 5 [19392/36450]\tLoss: 601.1896\n",
      "Training Epoch: 5 [19456/36450]\tLoss: 638.5697\n",
      "Training Epoch: 5 [19520/36450]\tLoss: 619.8856\n",
      "Training Epoch: 5 [19584/36450]\tLoss: 605.0563\n",
      "Training Epoch: 5 [19648/36450]\tLoss: 668.3024\n",
      "Training Epoch: 5 [19712/36450]\tLoss: 628.2136\n",
      "Training Epoch: 5 [19776/36450]\tLoss: 638.5798\n",
      "Training Epoch: 5 [19840/36450]\tLoss: 665.9321\n",
      "Training Epoch: 5 [19904/36450]\tLoss: 614.5660\n",
      "Training Epoch: 5 [19968/36450]\tLoss: 598.6461\n",
      "Training Epoch: 5 [20032/36450]\tLoss: 590.2946\n",
      "Training Epoch: 5 [20096/36450]\tLoss: 631.7148\n",
      "Training Epoch: 5 [20160/36450]\tLoss: 640.0018\n",
      "Training Epoch: 5 [20224/36450]\tLoss: 628.7697\n",
      "Training Epoch: 5 [20288/36450]\tLoss: 625.5320\n",
      "Training Epoch: 5 [20352/36450]\tLoss: 663.5573\n",
      "Training Epoch: 5 [20416/36450]\tLoss: 628.9021\n",
      "Training Epoch: 5 [20480/36450]\tLoss: 604.9307\n",
      "Training Epoch: 5 [20544/36450]\tLoss: 629.1965\n",
      "Training Epoch: 5 [20608/36450]\tLoss: 609.9277\n",
      "Training Epoch: 5 [20672/36450]\tLoss: 630.4696\n",
      "Training Epoch: 5 [20736/36450]\tLoss: 625.3493\n",
      "Training Epoch: 5 [20800/36450]\tLoss: 622.7917\n",
      "Training Epoch: 5 [20864/36450]\tLoss: 582.8994\n",
      "Training Epoch: 5 [20928/36450]\tLoss: 629.6400\n",
      "Training Epoch: 5 [20992/36450]\tLoss: 640.5876\n",
      "Training Epoch: 5 [21056/36450]\tLoss: 634.0770\n",
      "Training Epoch: 5 [21120/36450]\tLoss: 658.7997\n",
      "Training Epoch: 5 [21184/36450]\tLoss: 582.2101\n",
      "Training Epoch: 5 [21248/36450]\tLoss: 671.6406\n",
      "Training Epoch: 5 [21312/36450]\tLoss: 677.3828\n",
      "Training Epoch: 5 [21376/36450]\tLoss: 629.8423\n",
      "Training Epoch: 5 [21440/36450]\tLoss: 634.5939\n",
      "Training Epoch: 5 [21504/36450]\tLoss: 665.3583\n",
      "Training Epoch: 5 [21568/36450]\tLoss: 648.5738\n",
      "Training Epoch: 5 [21632/36450]\tLoss: 661.4894\n",
      "Training Epoch: 5 [21696/36450]\tLoss: 675.3788\n",
      "Training Epoch: 5 [21760/36450]\tLoss: 640.5338\n",
      "Training Epoch: 5 [21824/36450]\tLoss: 638.2741\n",
      "Training Epoch: 5 [21888/36450]\tLoss: 636.2187\n",
      "Training Epoch: 5 [21952/36450]\tLoss: 628.1874\n",
      "Training Epoch: 5 [22016/36450]\tLoss: 608.0743\n",
      "Training Epoch: 5 [22080/36450]\tLoss: 659.8786\n",
      "Training Epoch: 5 [22144/36450]\tLoss: 585.0207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [22208/36450]\tLoss: 606.0726\n",
      "Training Epoch: 5 [22272/36450]\tLoss: 632.8557\n",
      "Training Epoch: 5 [22336/36450]\tLoss: 602.3581\n",
      "Training Epoch: 5 [22400/36450]\tLoss: 609.2767\n",
      "Training Epoch: 5 [22464/36450]\tLoss: 634.0066\n",
      "Training Epoch: 5 [22528/36450]\tLoss: 644.5980\n",
      "Training Epoch: 5 [22592/36450]\tLoss: 653.7001\n",
      "Training Epoch: 5 [22656/36450]\tLoss: 634.1274\n",
      "Training Epoch: 5 [22720/36450]\tLoss: 609.3484\n",
      "Training Epoch: 5 [22784/36450]\tLoss: 633.3679\n",
      "Training Epoch: 5 [22848/36450]\tLoss: 604.3356\n",
      "Training Epoch: 5 [22912/36450]\tLoss: 624.7974\n",
      "Training Epoch: 5 [22976/36450]\tLoss: 624.5375\n",
      "Training Epoch: 5 [23040/36450]\tLoss: 602.6058\n",
      "Training Epoch: 5 [23104/36450]\tLoss: 631.6661\n",
      "Training Epoch: 5 [23168/36450]\tLoss: 649.4167\n",
      "Training Epoch: 5 [23232/36450]\tLoss: 596.2765\n",
      "Training Epoch: 5 [23296/36450]\tLoss: 606.5121\n",
      "Training Epoch: 5 [23360/36450]\tLoss: 633.4078\n",
      "Training Epoch: 5 [23424/36450]\tLoss: 632.4828\n",
      "Training Epoch: 5 [23488/36450]\tLoss: 635.3318\n",
      "Training Epoch: 5 [23552/36450]\tLoss: 622.2776\n",
      "Training Epoch: 5 [23616/36450]\tLoss: 674.1774\n",
      "Training Epoch: 5 [23680/36450]\tLoss: 653.8510\n",
      "Training Epoch: 5 [23744/36450]\tLoss: 662.0253\n",
      "Training Epoch: 5 [23808/36450]\tLoss: 589.8874\n",
      "Training Epoch: 5 [23872/36450]\tLoss: 631.2767\n",
      "Training Epoch: 5 [23936/36450]\tLoss: 621.0871\n",
      "Training Epoch: 5 [24000/36450]\tLoss: 601.6364\n",
      "Training Epoch: 5 [24064/36450]\tLoss: 586.6550\n",
      "Training Epoch: 5 [24128/36450]\tLoss: 647.0168\n",
      "Training Epoch: 5 [24192/36450]\tLoss: 650.2083\n",
      "Training Epoch: 5 [24256/36450]\tLoss: 612.6742\n",
      "Training Epoch: 5 [24320/36450]\tLoss: 628.5897\n",
      "Training Epoch: 5 [24384/36450]\tLoss: 629.5220\n",
      "Training Epoch: 5 [24448/36450]\tLoss: 595.5845\n",
      "Training Epoch: 5 [24512/36450]\tLoss: 599.4097\n",
      "Training Epoch: 5 [24576/36450]\tLoss: 605.0322\n",
      "Training Epoch: 5 [24640/36450]\tLoss: 603.2007\n",
      "Training Epoch: 5 [24704/36450]\tLoss: 632.8245\n",
      "Training Epoch: 5 [24768/36450]\tLoss: 628.4194\n",
      "Training Epoch: 5 [24832/36450]\tLoss: 630.5933\n",
      "Training Epoch: 5 [24896/36450]\tLoss: 596.1263\n",
      "Training Epoch: 5 [24960/36450]\tLoss: 633.1670\n",
      "Training Epoch: 5 [25024/36450]\tLoss: 605.7809\n",
      "Training Epoch: 5 [25088/36450]\tLoss: 654.6168\n",
      "Training Epoch: 5 [25152/36450]\tLoss: 602.3492\n",
      "Training Epoch: 5 [25216/36450]\tLoss: 601.0422\n",
      "Training Epoch: 5 [25280/36450]\tLoss: 626.8292\n",
      "Training Epoch: 5 [25344/36450]\tLoss: 623.7804\n",
      "Training Epoch: 5 [25408/36450]\tLoss: 600.0524\n",
      "Training Epoch: 5 [25472/36450]\tLoss: 590.6481\n",
      "Training Epoch: 5 [25536/36450]\tLoss: 647.8923\n",
      "Training Epoch: 5 [25600/36450]\tLoss: 645.2798\n",
      "Training Epoch: 5 [25664/36450]\tLoss: 612.2234\n",
      "Training Epoch: 5 [25728/36450]\tLoss: 569.2464\n",
      "Training Epoch: 5 [25792/36450]\tLoss: 623.9556\n",
      "Training Epoch: 5 [25856/36450]\tLoss: 620.8940\n",
      "Training Epoch: 5 [25920/36450]\tLoss: 595.3879\n",
      "Training Epoch: 5 [25984/36450]\tLoss: 620.2921\n",
      "Training Epoch: 5 [26048/36450]\tLoss: 628.7258\n",
      "Training Epoch: 5 [26112/36450]\tLoss: 589.5201\n",
      "Training Epoch: 5 [26176/36450]\tLoss: 610.3694\n",
      "Training Epoch: 5 [26240/36450]\tLoss: 586.2065\n",
      "Training Epoch: 5 [26304/36450]\tLoss: 607.4384\n",
      "Training Epoch: 5 [26368/36450]\tLoss: 631.9352\n",
      "Training Epoch: 5 [26432/36450]\tLoss: 642.3453\n",
      "Training Epoch: 5 [26496/36450]\tLoss: 611.9603\n",
      "Training Epoch: 5 [26560/36450]\tLoss: 611.4935\n",
      "Training Epoch: 5 [26624/36450]\tLoss: 651.5432\n",
      "Training Epoch: 5 [26688/36450]\tLoss: 598.2568\n",
      "Training Epoch: 5 [26752/36450]\tLoss: 620.3568\n",
      "Training Epoch: 5 [26816/36450]\tLoss: 612.9534\n",
      "Training Epoch: 5 [26880/36450]\tLoss: 643.1321\n",
      "Training Epoch: 5 [26944/36450]\tLoss: 600.6428\n",
      "Training Epoch: 5 [27008/36450]\tLoss: 630.8384\n",
      "Training Epoch: 5 [27072/36450]\tLoss: 642.1288\n",
      "Training Epoch: 5 [27136/36450]\tLoss: 588.3317\n",
      "Training Epoch: 5 [27200/36450]\tLoss: 606.2926\n",
      "Training Epoch: 5 [27264/36450]\tLoss: 615.3420\n",
      "Training Epoch: 5 [27328/36450]\tLoss: 614.5622\n",
      "Training Epoch: 5 [27392/36450]\tLoss: 658.2503\n",
      "Training Epoch: 5 [27456/36450]\tLoss: 623.9685\n",
      "Training Epoch: 5 [27520/36450]\tLoss: 610.2064\n",
      "Training Epoch: 5 [27584/36450]\tLoss: 633.8982\n",
      "Training Epoch: 5 [27648/36450]\tLoss: 602.0411\n",
      "Training Epoch: 5 [27712/36450]\tLoss: 627.9631\n",
      "Training Epoch: 5 [27776/36450]\tLoss: 564.2584\n",
      "Training Epoch: 5 [27840/36450]\tLoss: 639.8846\n",
      "Training Epoch: 5 [27904/36450]\tLoss: 629.5562\n",
      "Training Epoch: 5 [27968/36450]\tLoss: 637.3841\n",
      "Training Epoch: 5 [28032/36450]\tLoss: 594.5402\n",
      "Training Epoch: 5 [28096/36450]\tLoss: 609.0349\n",
      "Training Epoch: 5 [28160/36450]\tLoss: 615.3115\n",
      "Training Epoch: 5 [28224/36450]\tLoss: 620.7816\n",
      "Training Epoch: 5 [28288/36450]\tLoss: 631.5702\n",
      "Training Epoch: 5 [28352/36450]\tLoss: 596.4453\n",
      "Training Epoch: 5 [28416/36450]\tLoss: 580.7090\n",
      "Training Epoch: 5 [28480/36450]\tLoss: 610.3441\n",
      "Training Epoch: 5 [28544/36450]\tLoss: 607.4644\n",
      "Training Epoch: 5 [28608/36450]\tLoss: 588.3746\n",
      "Training Epoch: 5 [28672/36450]\tLoss: 660.3804\n",
      "Training Epoch: 5 [28736/36450]\tLoss: 598.8854\n",
      "Training Epoch: 5 [28800/36450]\tLoss: 613.9536\n",
      "Training Epoch: 5 [28864/36450]\tLoss: 634.3527\n",
      "Training Epoch: 5 [28928/36450]\tLoss: 595.2825\n",
      "Training Epoch: 5 [28992/36450]\tLoss: 601.3472\n",
      "Training Epoch: 5 [29056/36450]\tLoss: 601.7357\n",
      "Training Epoch: 5 [29120/36450]\tLoss: 636.7112\n",
      "Training Epoch: 5 [29184/36450]\tLoss: 633.8134\n",
      "Training Epoch: 5 [29248/36450]\tLoss: 605.9636\n",
      "Training Epoch: 5 [29312/36450]\tLoss: 590.1021\n",
      "Training Epoch: 5 [29376/36450]\tLoss: 605.3782\n",
      "Training Epoch: 5 [29440/36450]\tLoss: 627.4933\n",
      "Training Epoch: 5 [29504/36450]\tLoss: 580.3280\n",
      "Training Epoch: 5 [29568/36450]\tLoss: 591.1115\n",
      "Training Epoch: 5 [29632/36450]\tLoss: 643.0708\n",
      "Training Epoch: 5 [29696/36450]\tLoss: 600.6895\n",
      "Training Epoch: 5 [29760/36450]\tLoss: 610.9410\n",
      "Training Epoch: 5 [29824/36450]\tLoss: 605.4349\n",
      "Training Epoch: 5 [29888/36450]\tLoss: 610.2507\n",
      "Training Epoch: 5 [29952/36450]\tLoss: 602.9914\n",
      "Training Epoch: 5 [30016/36450]\tLoss: 572.0043\n",
      "Training Epoch: 5 [30080/36450]\tLoss: 601.6973\n",
      "Training Epoch: 5 [30144/36450]\tLoss: 596.6343\n",
      "Training Epoch: 5 [30208/36450]\tLoss: 630.6543\n",
      "Training Epoch: 5 [30272/36450]\tLoss: 612.4716\n",
      "Training Epoch: 5 [30336/36450]\tLoss: 629.2325\n",
      "Training Epoch: 5 [30400/36450]\tLoss: 652.0557\n",
      "Training Epoch: 5 [30464/36450]\tLoss: 616.6552\n",
      "Training Epoch: 5 [30528/36450]\tLoss: 617.8029\n",
      "Training Epoch: 5 [30592/36450]\tLoss: 587.6071\n",
      "Training Epoch: 5 [30656/36450]\tLoss: 619.5178\n",
      "Training Epoch: 5 [30720/36450]\tLoss: 611.0784\n",
      "Training Epoch: 5 [30784/36450]\tLoss: 587.9657\n",
      "Training Epoch: 5 [30848/36450]\tLoss: 611.8339\n",
      "Training Epoch: 5 [30912/36450]\tLoss: 639.6286\n",
      "Training Epoch: 5 [30976/36450]\tLoss: 609.0833\n",
      "Training Epoch: 5 [31040/36450]\tLoss: 601.9628\n",
      "Training Epoch: 5 [31104/36450]\tLoss: 577.8174\n",
      "Training Epoch: 5 [31168/36450]\tLoss: 597.0953\n",
      "Training Epoch: 5 [31232/36450]\tLoss: 616.1224\n",
      "Training Epoch: 5 [31296/36450]\tLoss: 594.8297\n",
      "Training Epoch: 5 [31360/36450]\tLoss: 616.5835\n",
      "Training Epoch: 5 [31424/36450]\tLoss: 631.1569\n",
      "Training Epoch: 5 [31488/36450]\tLoss: 627.6147\n",
      "Training Epoch: 5 [31552/36450]\tLoss: 585.1266\n",
      "Training Epoch: 5 [31616/36450]\tLoss: 635.0172\n",
      "Training Epoch: 5 [31680/36450]\tLoss: 595.8840\n",
      "Training Epoch: 5 [31744/36450]\tLoss: 571.4418\n",
      "Training Epoch: 5 [31808/36450]\tLoss: 588.7505\n",
      "Training Epoch: 5 [31872/36450]\tLoss: 666.3353\n",
      "Training Epoch: 5 [31936/36450]\tLoss: 597.0210\n",
      "Training Epoch: 5 [32000/36450]\tLoss: 603.2549\n",
      "Training Epoch: 5 [32064/36450]\tLoss: 595.5615\n",
      "Training Epoch: 5 [32128/36450]\tLoss: 631.2765\n",
      "Training Epoch: 5 [32192/36450]\tLoss: 601.2733\n",
      "Training Epoch: 5 [32256/36450]\tLoss: 601.9602\n",
      "Training Epoch: 5 [32320/36450]\tLoss: 567.9539\n",
      "Training Epoch: 5 [32384/36450]\tLoss: 613.4449\n",
      "Training Epoch: 5 [32448/36450]\tLoss: 621.7031\n",
      "Training Epoch: 5 [32512/36450]\tLoss: 602.5897\n",
      "Training Epoch: 5 [32576/36450]\tLoss: 610.7656\n",
      "Training Epoch: 5 [32640/36450]\tLoss: 610.2745\n",
      "Training Epoch: 5 [32704/36450]\tLoss: 582.9337\n",
      "Training Epoch: 5 [32768/36450]\tLoss: 634.8814\n",
      "Training Epoch: 5 [32832/36450]\tLoss: 595.5227\n",
      "Training Epoch: 5 [32896/36450]\tLoss: 640.8157\n",
      "Training Epoch: 5 [32960/36450]\tLoss: 607.5300\n",
      "Training Epoch: 5 [33024/36450]\tLoss: 634.5023\n",
      "Training Epoch: 5 [33088/36450]\tLoss: 605.2600\n",
      "Training Epoch: 5 [33152/36450]\tLoss: 645.8960\n",
      "Training Epoch: 5 [33216/36450]\tLoss: 631.2453\n",
      "Training Epoch: 5 [33280/36450]\tLoss: 663.3071\n",
      "Training Epoch: 5 [33344/36450]\tLoss: 684.1658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [33408/36450]\tLoss: 685.7448\n",
      "Training Epoch: 5 [33472/36450]\tLoss: 722.4065\n",
      "Training Epoch: 5 [33536/36450]\tLoss: 763.3439\n",
      "Training Epoch: 5 [33600/36450]\tLoss: 725.3385\n",
      "Training Epoch: 5 [33664/36450]\tLoss: 693.2778\n",
      "Training Epoch: 5 [33728/36450]\tLoss: 634.7335\n",
      "Training Epoch: 5 [33792/36450]\tLoss: 619.1255\n",
      "Training Epoch: 5 [33856/36450]\tLoss: 638.5238\n",
      "Training Epoch: 5 [33920/36450]\tLoss: 656.8921\n",
      "Training Epoch: 5 [33984/36450]\tLoss: 671.5124\n",
      "Training Epoch: 5 [34048/36450]\tLoss: 647.0126\n",
      "Training Epoch: 5 [34112/36450]\tLoss: 666.1212\n",
      "Training Epoch: 5 [34176/36450]\tLoss: 598.3889\n",
      "Training Epoch: 5 [34240/36450]\tLoss: 645.3861\n",
      "Training Epoch: 5 [34304/36450]\tLoss: 652.6320\n",
      "Training Epoch: 5 [34368/36450]\tLoss: 617.1646\n",
      "Training Epoch: 5 [34432/36450]\tLoss: 642.5281\n",
      "Training Epoch: 5 [34496/36450]\tLoss: 619.0714\n",
      "Training Epoch: 5 [34560/36450]\tLoss: 647.2249\n",
      "Training Epoch: 5 [34624/36450]\tLoss: 658.0709\n",
      "Training Epoch: 5 [34688/36450]\tLoss: 625.1934\n",
      "Training Epoch: 5 [34752/36450]\tLoss: 644.2378\n",
      "Training Epoch: 5 [34816/36450]\tLoss: 576.7122\n",
      "Training Epoch: 5 [34880/36450]\tLoss: 610.1335\n",
      "Training Epoch: 5 [34944/36450]\tLoss: 642.2943\n",
      "Training Epoch: 5 [35008/36450]\tLoss: 617.6185\n",
      "Training Epoch: 5 [35072/36450]\tLoss: 616.8057\n",
      "Training Epoch: 5 [35136/36450]\tLoss: 605.4814\n",
      "Training Epoch: 5 [35200/36450]\tLoss: 581.6320\n",
      "Training Epoch: 5 [35264/36450]\tLoss: 624.7786\n",
      "Training Epoch: 5 [35328/36450]\tLoss: 637.2438\n",
      "Training Epoch: 5 [35392/36450]\tLoss: 619.2725\n",
      "Training Epoch: 5 [35456/36450]\tLoss: 628.9977\n",
      "Training Epoch: 5 [35520/36450]\tLoss: 604.3831\n",
      "Training Epoch: 5 [35584/36450]\tLoss: 595.9614\n",
      "Training Epoch: 5 [35648/36450]\tLoss: 597.7472\n",
      "Training Epoch: 5 [35712/36450]\tLoss: 621.5745\n",
      "Training Epoch: 5 [35776/36450]\tLoss: 590.2856\n",
      "Training Epoch: 5 [35840/36450]\tLoss: 620.8717\n",
      "Training Epoch: 5 [35904/36450]\tLoss: 608.3436\n",
      "Training Epoch: 5 [35968/36450]\tLoss: 621.3624\n",
      "Training Epoch: 5 [36032/36450]\tLoss: 574.9877\n",
      "Training Epoch: 5 [36096/36450]\tLoss: 619.9361\n",
      "Training Epoch: 5 [36160/36450]\tLoss: 607.1554\n",
      "Training Epoch: 5 [36224/36450]\tLoss: 577.7280\n",
      "Training Epoch: 5 [36288/36450]\tLoss: 630.8045\n",
      "Training Epoch: 5 [36352/36450]\tLoss: 657.3658\n",
      "Training Epoch: 5 [36416/36450]\tLoss: 620.7244\n",
      "Training Epoch: 5 [36450/36450]\tLoss: 594.7958\n",
      "Training Epoch: 5 [4050/4050]\tLoss: 303.1094\n",
      "Training Epoch: 6 [64/36450]\tLoss: 606.0807\n",
      "Training Epoch: 6 [128/36450]\tLoss: 609.2816\n",
      "Training Epoch: 6 [192/36450]\tLoss: 599.7346\n",
      "Training Epoch: 6 [256/36450]\tLoss: 618.7043\n",
      "Training Epoch: 6 [320/36450]\tLoss: 654.2842\n",
      "Training Epoch: 6 [384/36450]\tLoss: 637.1127\n",
      "Training Epoch: 6 [448/36450]\tLoss: 642.9088\n",
      "Training Epoch: 6 [512/36450]\tLoss: 602.3253\n",
      "Training Epoch: 6 [576/36450]\tLoss: 585.9626\n",
      "Training Epoch: 6 [640/36450]\tLoss: 598.3621\n",
      "Training Epoch: 6 [704/36450]\tLoss: 597.5327\n",
      "Training Epoch: 6 [768/36450]\tLoss: 600.7712\n",
      "Training Epoch: 6 [832/36450]\tLoss: 629.2310\n",
      "Training Epoch: 6 [896/36450]\tLoss: 580.4564\n",
      "Training Epoch: 6 [960/36450]\tLoss: 568.4590\n",
      "Training Epoch: 6 [1024/36450]\tLoss: 630.3676\n",
      "Training Epoch: 6 [1088/36450]\tLoss: 590.9644\n",
      "Training Epoch: 6 [1152/36450]\tLoss: 578.8817\n",
      "Training Epoch: 6 [1216/36450]\tLoss: 623.9100\n",
      "Training Epoch: 6 [1280/36450]\tLoss: 577.3694\n",
      "Training Epoch: 6 [1344/36450]\tLoss: 574.1706\n",
      "Training Epoch: 6 [1408/36450]\tLoss: 621.2991\n",
      "Training Epoch: 6 [1472/36450]\tLoss: 625.3143\n",
      "Training Epoch: 6 [1536/36450]\tLoss: 614.7591\n",
      "Training Epoch: 6 [1600/36450]\tLoss: 584.8828\n",
      "Training Epoch: 6 [1664/36450]\tLoss: 602.3369\n",
      "Training Epoch: 6 [1728/36450]\tLoss: 631.1588\n",
      "Training Epoch: 6 [1792/36450]\tLoss: 614.1928\n",
      "Training Epoch: 6 [1856/36450]\tLoss: 590.9675\n",
      "Training Epoch: 6 [1920/36450]\tLoss: 612.0320\n",
      "Training Epoch: 6 [1984/36450]\tLoss: 633.7666\n",
      "Training Epoch: 6 [2048/36450]\tLoss: 575.7592\n",
      "Training Epoch: 6 [2112/36450]\tLoss: 594.2950\n",
      "Training Epoch: 6 [2176/36450]\tLoss: 612.7577\n",
      "Training Epoch: 6 [2240/36450]\tLoss: 607.5980\n",
      "Training Epoch: 6 [2304/36450]\tLoss: 572.5511\n",
      "Training Epoch: 6 [2368/36450]\tLoss: 592.3359\n",
      "Training Epoch: 6 [2432/36450]\tLoss: 605.0194\n",
      "Training Epoch: 6 [2496/36450]\tLoss: 606.1328\n",
      "Training Epoch: 6 [2560/36450]\tLoss: 629.7995\n",
      "Training Epoch: 6 [2624/36450]\tLoss: 593.7059\n",
      "Training Epoch: 6 [2688/36450]\tLoss: 565.0530\n",
      "Training Epoch: 6 [2752/36450]\tLoss: 591.7122\n",
      "Training Epoch: 6 [2816/36450]\tLoss: 571.8676\n",
      "Training Epoch: 6 [2880/36450]\tLoss: 585.9597\n",
      "Training Epoch: 6 [2944/36450]\tLoss: 611.3462\n",
      "Training Epoch: 6 [3008/36450]\tLoss: 626.2620\n",
      "Training Epoch: 6 [3072/36450]\tLoss: 562.1900\n",
      "Training Epoch: 6 [3136/36450]\tLoss: 580.4902\n",
      "Training Epoch: 6 [3200/36450]\tLoss: 629.2751\n",
      "Training Epoch: 6 [3264/36450]\tLoss: 598.2635\n",
      "Training Epoch: 6 [3328/36450]\tLoss: 616.9415\n",
      "Training Epoch: 6 [3392/36450]\tLoss: 602.0148\n",
      "Training Epoch: 6 [3456/36450]\tLoss: 618.1738\n",
      "Training Epoch: 6 [3520/36450]\tLoss: 603.9909\n",
      "Training Epoch: 6 [3584/36450]\tLoss: 599.5875\n",
      "Training Epoch: 6 [3648/36450]\tLoss: 569.8149\n",
      "Training Epoch: 6 [3712/36450]\tLoss: 619.5588\n",
      "Training Epoch: 6 [3776/36450]\tLoss: 614.9599\n",
      "Training Epoch: 6 [3840/36450]\tLoss: 592.1532\n",
      "Training Epoch: 6 [3904/36450]\tLoss: 586.9048\n",
      "Training Epoch: 6 [3968/36450]\tLoss: 627.5004\n",
      "Training Epoch: 6 [4032/36450]\tLoss: 584.8186\n",
      "Training Epoch: 6 [4096/36450]\tLoss: 584.5957\n",
      "Training Epoch: 6 [4160/36450]\tLoss: 591.5618\n",
      "Training Epoch: 6 [4224/36450]\tLoss: 627.2748\n",
      "Training Epoch: 6 [4288/36450]\tLoss: 608.5102\n",
      "Training Epoch: 6 [4352/36450]\tLoss: 607.4187\n",
      "Training Epoch: 6 [4416/36450]\tLoss: 616.0177\n",
      "Training Epoch: 6 [4480/36450]\tLoss: 610.2098\n",
      "Training Epoch: 6 [4544/36450]\tLoss: 663.0764\n",
      "Training Epoch: 6 [4608/36450]\tLoss: 586.5582\n",
      "Training Epoch: 6 [4672/36450]\tLoss: 620.2180\n",
      "Training Epoch: 6 [4736/36450]\tLoss: 611.8755\n",
      "Training Epoch: 6 [4800/36450]\tLoss: 608.2859\n",
      "Training Epoch: 6 [4864/36450]\tLoss: 629.4514\n",
      "Training Epoch: 6 [4928/36450]\tLoss: 616.5762\n",
      "Training Epoch: 6 [4992/36450]\tLoss: 615.0402\n",
      "Training Epoch: 6 [5056/36450]\tLoss: 618.2407\n",
      "Training Epoch: 6 [5120/36450]\tLoss: 613.0276\n",
      "Training Epoch: 6 [5184/36450]\tLoss: 589.7422\n",
      "Training Epoch: 6 [5248/36450]\tLoss: 616.8493\n",
      "Training Epoch: 6 [5312/36450]\tLoss: 569.7154\n",
      "Training Epoch: 6 [5376/36450]\tLoss: 607.8326\n",
      "Training Epoch: 6 [5440/36450]\tLoss: 585.3790\n",
      "Training Epoch: 6 [5504/36450]\tLoss: 579.8578\n",
      "Training Epoch: 6 [5568/36450]\tLoss: 581.3776\n",
      "Training Epoch: 6 [5632/36450]\tLoss: 601.4991\n",
      "Training Epoch: 6 [5696/36450]\tLoss: 615.4872\n",
      "Training Epoch: 6 [5760/36450]\tLoss: 605.0971\n",
      "Training Epoch: 6 [5824/36450]\tLoss: 627.1220\n",
      "Training Epoch: 6 [5888/36450]\tLoss: 589.7035\n",
      "Training Epoch: 6 [5952/36450]\tLoss: 588.3083\n",
      "Training Epoch: 6 [6016/36450]\tLoss: 593.9977\n",
      "Training Epoch: 6 [6080/36450]\tLoss: 604.1036\n",
      "Training Epoch: 6 [6144/36450]\tLoss: 589.0831\n",
      "Training Epoch: 6 [6208/36450]\tLoss: 602.1792\n",
      "Training Epoch: 6 [6272/36450]\tLoss: 608.5941\n",
      "Training Epoch: 6 [6336/36450]\tLoss: 604.0096\n",
      "Training Epoch: 6 [6400/36450]\tLoss: 657.4705\n",
      "Training Epoch: 6 [6464/36450]\tLoss: 604.8994\n",
      "Training Epoch: 6 [6528/36450]\tLoss: 609.8117\n",
      "Training Epoch: 6 [6592/36450]\tLoss: 591.8105\n",
      "Training Epoch: 6 [6656/36450]\tLoss: 607.3819\n",
      "Training Epoch: 6 [6720/36450]\tLoss: 598.6778\n",
      "Training Epoch: 6 [6784/36450]\tLoss: 616.9655\n",
      "Training Epoch: 6 [6848/36450]\tLoss: 572.1730\n",
      "Training Epoch: 6 [6912/36450]\tLoss: 606.8248\n",
      "Training Epoch: 6 [6976/36450]\tLoss: 607.8518\n",
      "Training Epoch: 6 [7040/36450]\tLoss: 635.9777\n",
      "Training Epoch: 6 [7104/36450]\tLoss: 585.8682\n",
      "Training Epoch: 6 [7168/36450]\tLoss: 616.6462\n",
      "Training Epoch: 6 [7232/36450]\tLoss: 597.2200\n",
      "Training Epoch: 6 [7296/36450]\tLoss: 597.4976\n",
      "Training Epoch: 6 [7360/36450]\tLoss: 623.2605\n",
      "Training Epoch: 6 [7424/36450]\tLoss: 597.4402\n",
      "Training Epoch: 6 [7488/36450]\tLoss: 581.0862\n",
      "Training Epoch: 6 [7552/36450]\tLoss: 609.7028\n",
      "Training Epoch: 6 [7616/36450]\tLoss: 618.7084\n",
      "Training Epoch: 6 [7680/36450]\tLoss: 608.7852\n",
      "Training Epoch: 6 [7744/36450]\tLoss: 630.6649\n",
      "Training Epoch: 6 [7808/36450]\tLoss: 590.4196\n",
      "Training Epoch: 6 [7872/36450]\tLoss: 598.3575\n",
      "Training Epoch: 6 [7936/36450]\tLoss: 612.3036\n",
      "Training Epoch: 6 [8000/36450]\tLoss: 585.3907\n",
      "Training Epoch: 6 [8064/36450]\tLoss: 603.1965\n",
      "Training Epoch: 6 [8128/36450]\tLoss: 606.5946\n",
      "Training Epoch: 6 [8192/36450]\tLoss: 605.2264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [8256/36450]\tLoss: 597.3404\n",
      "Training Epoch: 6 [8320/36450]\tLoss: 596.5334\n",
      "Training Epoch: 6 [8384/36450]\tLoss: 642.2415\n",
      "Training Epoch: 6 [8448/36450]\tLoss: 576.2487\n",
      "Training Epoch: 6 [8512/36450]\tLoss: 587.0026\n",
      "Training Epoch: 6 [8576/36450]\tLoss: 639.7768\n",
      "Training Epoch: 6 [8640/36450]\tLoss: 572.7370\n",
      "Training Epoch: 6 [8704/36450]\tLoss: 629.5775\n",
      "Training Epoch: 6 [8768/36450]\tLoss: 613.0011\n",
      "Training Epoch: 6 [8832/36450]\tLoss: 626.6089\n",
      "Training Epoch: 6 [8896/36450]\tLoss: 588.9465\n",
      "Training Epoch: 6 [8960/36450]\tLoss: 607.2381\n",
      "Training Epoch: 6 [9024/36450]\tLoss: 584.3124\n",
      "Training Epoch: 6 [9088/36450]\tLoss: 626.8297\n",
      "Training Epoch: 6 [9152/36450]\tLoss: 619.5812\n",
      "Training Epoch: 6 [9216/36450]\tLoss: 611.1519\n",
      "Training Epoch: 6 [9280/36450]\tLoss: 621.0703\n",
      "Training Epoch: 6 [9344/36450]\tLoss: 604.2134\n",
      "Training Epoch: 6 [9408/36450]\tLoss: 610.2829\n",
      "Training Epoch: 6 [9472/36450]\tLoss: 649.2366\n",
      "Training Epoch: 6 [9536/36450]\tLoss: 563.1765\n",
      "Training Epoch: 6 [9600/36450]\tLoss: 603.1644\n",
      "Training Epoch: 6 [9664/36450]\tLoss: 594.8732\n",
      "Training Epoch: 6 [9728/36450]\tLoss: 608.5160\n",
      "Training Epoch: 6 [9792/36450]\tLoss: 599.0592\n",
      "Training Epoch: 6 [9856/36450]\tLoss: 598.0992\n",
      "Training Epoch: 6 [9920/36450]\tLoss: 579.3607\n",
      "Training Epoch: 6 [9984/36450]\tLoss: 620.5370\n",
      "Training Epoch: 6 [10048/36450]\tLoss: 590.9229\n",
      "Training Epoch: 6 [10112/36450]\tLoss: 614.4994\n",
      "Training Epoch: 6 [10176/36450]\tLoss: 650.3569\n",
      "Training Epoch: 6 [10240/36450]\tLoss: 630.7841\n",
      "Training Epoch: 6 [10304/36450]\tLoss: 571.6846\n",
      "Training Epoch: 6 [10368/36450]\tLoss: 589.9514\n",
      "Training Epoch: 6 [10432/36450]\tLoss: 608.2442\n",
      "Training Epoch: 6 [10496/36450]\tLoss: 591.7465\n",
      "Training Epoch: 6 [10560/36450]\tLoss: 593.3708\n",
      "Training Epoch: 6 [10624/36450]\tLoss: 602.0966\n",
      "Training Epoch: 6 [10688/36450]\tLoss: 587.2719\n",
      "Training Epoch: 6 [10752/36450]\tLoss: 620.2483\n",
      "Training Epoch: 6 [10816/36450]\tLoss: 611.9027\n",
      "Training Epoch: 6 [10880/36450]\tLoss: 588.7205\n",
      "Training Epoch: 6 [10944/36450]\tLoss: 601.6456\n",
      "Training Epoch: 6 [11008/36450]\tLoss: 599.6856\n",
      "Training Epoch: 6 [11072/36450]\tLoss: 600.3803\n",
      "Training Epoch: 6 [11136/36450]\tLoss: 615.7578\n",
      "Training Epoch: 6 [11200/36450]\tLoss: 547.6268\n",
      "Training Epoch: 6 [11264/36450]\tLoss: 606.8082\n",
      "Training Epoch: 6 [11328/36450]\tLoss: 586.5931\n",
      "Training Epoch: 6 [11392/36450]\tLoss: 610.3637\n",
      "Training Epoch: 6 [11456/36450]\tLoss: 582.9764\n",
      "Training Epoch: 6 [11520/36450]\tLoss: 599.1624\n",
      "Training Epoch: 6 [11584/36450]\tLoss: 612.0282\n",
      "Training Epoch: 6 [11648/36450]\tLoss: 593.7156\n",
      "Training Epoch: 6 [11712/36450]\tLoss: 579.6270\n",
      "Training Epoch: 6 [11776/36450]\tLoss: 579.6486\n",
      "Training Epoch: 6 [11840/36450]\tLoss: 628.9277\n",
      "Training Epoch: 6 [11904/36450]\tLoss: 618.8318\n",
      "Training Epoch: 6 [11968/36450]\tLoss: 583.8497\n",
      "Training Epoch: 6 [12032/36450]\tLoss: 625.6047\n",
      "Training Epoch: 6 [12096/36450]\tLoss: 594.1909\n",
      "Training Epoch: 6 [12160/36450]\tLoss: 578.8344\n",
      "Training Epoch: 6 [12224/36450]\tLoss: 606.8337\n",
      "Training Epoch: 6 [12288/36450]\tLoss: 569.6683\n",
      "Training Epoch: 6 [12352/36450]\tLoss: 610.2666\n",
      "Training Epoch: 6 [12416/36450]\tLoss: 578.8194\n",
      "Training Epoch: 6 [12480/36450]\tLoss: 582.2872\n",
      "Training Epoch: 6 [12544/36450]\tLoss: 581.7133\n",
      "Training Epoch: 6 [12608/36450]\tLoss: 587.6134\n",
      "Training Epoch: 6 [12672/36450]\tLoss: 593.2671\n",
      "Training Epoch: 6 [12736/36450]\tLoss: 567.0300\n",
      "Training Epoch: 6 [12800/36450]\tLoss: 606.1531\n",
      "Training Epoch: 6 [12864/36450]\tLoss: 590.6927\n",
      "Training Epoch: 6 [12928/36450]\tLoss: 602.8936\n",
      "Training Epoch: 6 [12992/36450]\tLoss: 610.8997\n",
      "Training Epoch: 6 [13056/36450]\tLoss: 603.4126\n",
      "Training Epoch: 6 [13120/36450]\tLoss: 615.5888\n",
      "Training Epoch: 6 [13184/36450]\tLoss: 597.3948\n",
      "Training Epoch: 6 [13248/36450]\tLoss: 611.9274\n",
      "Training Epoch: 6 [13312/36450]\tLoss: 639.0935\n",
      "Training Epoch: 6 [13376/36450]\tLoss: 596.1461\n",
      "Training Epoch: 6 [13440/36450]\tLoss: 595.7178\n",
      "Training Epoch: 6 [13504/36450]\tLoss: 620.6106\n",
      "Training Epoch: 6 [13568/36450]\tLoss: 625.2662\n",
      "Training Epoch: 6 [13632/36450]\tLoss: 612.3353\n",
      "Training Epoch: 6 [13696/36450]\tLoss: 579.9993\n",
      "Training Epoch: 6 [13760/36450]\tLoss: 575.8484\n",
      "Training Epoch: 6 [13824/36450]\tLoss: 568.1714\n",
      "Training Epoch: 6 [13888/36450]\tLoss: 573.2516\n",
      "Training Epoch: 6 [13952/36450]\tLoss: 614.8490\n",
      "Training Epoch: 6 [14016/36450]\tLoss: 596.1193\n",
      "Training Epoch: 6 [14080/36450]\tLoss: 612.8951\n",
      "Training Epoch: 6 [14144/36450]\tLoss: 619.5651\n",
      "Training Epoch: 6 [14208/36450]\tLoss: 584.7796\n",
      "Training Epoch: 6 [14272/36450]\tLoss: 635.5191\n",
      "Training Epoch: 6 [14336/36450]\tLoss: 591.4127\n",
      "Training Epoch: 6 [14400/36450]\tLoss: 613.8574\n",
      "Training Epoch: 6 [14464/36450]\tLoss: 591.0583\n",
      "Training Epoch: 6 [14528/36450]\tLoss: 602.5358\n",
      "Training Epoch: 6 [14592/36450]\tLoss: 581.8124\n",
      "Training Epoch: 6 [14656/36450]\tLoss: 583.9503\n",
      "Training Epoch: 6 [14720/36450]\tLoss: 580.5836\n",
      "Training Epoch: 6 [14784/36450]\tLoss: 560.9939\n",
      "Training Epoch: 6 [14848/36450]\tLoss: 672.4377\n",
      "Training Epoch: 6 [14912/36450]\tLoss: 596.6985\n",
      "Training Epoch: 6 [14976/36450]\tLoss: 623.7443\n",
      "Training Epoch: 6 [15040/36450]\tLoss: 604.6835\n",
      "Training Epoch: 6 [15104/36450]\tLoss: 603.9835\n",
      "Training Epoch: 6 [15168/36450]\tLoss: 584.6533\n",
      "Training Epoch: 6 [15232/36450]\tLoss: 590.0336\n",
      "Training Epoch: 6 [15296/36450]\tLoss: 617.5233\n",
      "Training Epoch: 6 [15360/36450]\tLoss: 621.0781\n",
      "Training Epoch: 6 [15424/36450]\tLoss: 610.7899\n",
      "Training Epoch: 6 [15488/36450]\tLoss: 553.5104\n",
      "Training Epoch: 6 [15552/36450]\tLoss: 570.5637\n",
      "Training Epoch: 6 [15616/36450]\tLoss: 635.3609\n",
      "Training Epoch: 6 [15680/36450]\tLoss: 583.2096\n",
      "Training Epoch: 6 [15744/36450]\tLoss: 598.8846\n",
      "Training Epoch: 6 [15808/36450]\tLoss: 596.9204\n",
      "Training Epoch: 6 [15872/36450]\tLoss: 599.3351\n",
      "Training Epoch: 6 [15936/36450]\tLoss: 578.6674\n",
      "Training Epoch: 6 [16000/36450]\tLoss: 562.8652\n",
      "Training Epoch: 6 [16064/36450]\tLoss: 602.7386\n",
      "Training Epoch: 6 [16128/36450]\tLoss: 594.5926\n",
      "Training Epoch: 6 [16192/36450]\tLoss: 604.9062\n",
      "Training Epoch: 6 [16256/36450]\tLoss: 616.4263\n",
      "Training Epoch: 6 [16320/36450]\tLoss: 602.0814\n",
      "Training Epoch: 6 [16384/36450]\tLoss: 595.9128\n",
      "Training Epoch: 6 [16448/36450]\tLoss: 593.4808\n",
      "Training Epoch: 6 [16512/36450]\tLoss: 581.9976\n",
      "Training Epoch: 6 [16576/36450]\tLoss: 616.4702\n",
      "Training Epoch: 6 [16640/36450]\tLoss: 551.4309\n",
      "Training Epoch: 6 [16704/36450]\tLoss: 577.3618\n",
      "Training Epoch: 6 [16768/36450]\tLoss: 557.3862\n",
      "Training Epoch: 6 [16832/36450]\tLoss: 604.6719\n",
      "Training Epoch: 6 [16896/36450]\tLoss: 573.5917\n",
      "Training Epoch: 6 [16960/36450]\tLoss: 605.7437\n",
      "Training Epoch: 6 [17024/36450]\tLoss: 616.2896\n",
      "Training Epoch: 6 [17088/36450]\tLoss: 616.9531\n",
      "Training Epoch: 6 [17152/36450]\tLoss: 615.1289\n",
      "Training Epoch: 6 [17216/36450]\tLoss: 641.2435\n",
      "Training Epoch: 6 [17280/36450]\tLoss: 607.8182\n",
      "Training Epoch: 6 [17344/36450]\tLoss: 588.1260\n",
      "Training Epoch: 6 [17408/36450]\tLoss: 607.0809\n",
      "Training Epoch: 6 [17472/36450]\tLoss: 574.3815\n",
      "Training Epoch: 6 [17536/36450]\tLoss: 595.5043\n",
      "Training Epoch: 6 [17600/36450]\tLoss: 594.1735\n",
      "Training Epoch: 6 [17664/36450]\tLoss: 590.3755\n",
      "Training Epoch: 6 [17728/36450]\tLoss: 568.3422\n",
      "Training Epoch: 6 [17792/36450]\tLoss: 580.6470\n",
      "Training Epoch: 6 [17856/36450]\tLoss: 582.7159\n",
      "Training Epoch: 6 [17920/36450]\tLoss: 618.6694\n",
      "Training Epoch: 6 [17984/36450]\tLoss: 611.5565\n",
      "Training Epoch: 6 [18048/36450]\tLoss: 606.8889\n",
      "Training Epoch: 6 [18112/36450]\tLoss: 589.3307\n",
      "Training Epoch: 6 [18176/36450]\tLoss: 608.0546\n",
      "Training Epoch: 6 [18240/36450]\tLoss: 599.6768\n",
      "Training Epoch: 6 [18304/36450]\tLoss: 627.8153\n",
      "Training Epoch: 6 [18368/36450]\tLoss: 589.8149\n",
      "Training Epoch: 6 [18432/36450]\tLoss: 649.3443\n",
      "Training Epoch: 6 [18496/36450]\tLoss: 588.6460\n",
      "Training Epoch: 6 [18560/36450]\tLoss: 617.8873\n",
      "Training Epoch: 6 [18624/36450]\tLoss: 636.8530\n",
      "Training Epoch: 6 [18688/36450]\tLoss: 603.5535\n",
      "Training Epoch: 6 [18752/36450]\tLoss: 599.9165\n",
      "Training Epoch: 6 [18816/36450]\tLoss: 609.0961\n",
      "Training Epoch: 6 [18880/36450]\tLoss: 591.6153\n",
      "Training Epoch: 6 [18944/36450]\tLoss: 603.0969\n",
      "Training Epoch: 6 [19008/36450]\tLoss: 611.0913\n",
      "Training Epoch: 6 [19072/36450]\tLoss: 635.4368\n",
      "Training Epoch: 6 [19136/36450]\tLoss: 595.0356\n",
      "Training Epoch: 6 [19200/36450]\tLoss: 577.6596\n",
      "Training Epoch: 6 [19264/36450]\tLoss: 599.6498\n",
      "Training Epoch: 6 [19328/36450]\tLoss: 593.1032\n",
      "Training Epoch: 6 [19392/36450]\tLoss: 593.3926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [19456/36450]\tLoss: 594.7100\n",
      "Training Epoch: 6 [19520/36450]\tLoss: 597.4705\n",
      "Training Epoch: 6 [19584/36450]\tLoss: 635.6187\n",
      "Training Epoch: 6 [19648/36450]\tLoss: 657.2141\n",
      "Training Epoch: 6 [19712/36450]\tLoss: 605.9973\n",
      "Training Epoch: 6 [19776/36450]\tLoss: 622.6707\n",
      "Training Epoch: 6 [19840/36450]\tLoss: 571.4515\n",
      "Training Epoch: 6 [19904/36450]\tLoss: 585.9583\n",
      "Training Epoch: 6 [19968/36450]\tLoss: 582.9675\n",
      "Training Epoch: 6 [20032/36450]\tLoss: 578.9684\n",
      "Training Epoch: 6 [20096/36450]\tLoss: 612.3326\n",
      "Training Epoch: 6 [20160/36450]\tLoss: 643.2150\n",
      "Training Epoch: 6 [20224/36450]\tLoss: 601.6290\n",
      "Training Epoch: 6 [20288/36450]\tLoss: 560.0443\n",
      "Training Epoch: 6 [20352/36450]\tLoss: 632.5447\n",
      "Training Epoch: 6 [20416/36450]\tLoss: 604.4877\n",
      "Training Epoch: 6 [20480/36450]\tLoss: 577.3096\n",
      "Training Epoch: 6 [20544/36450]\tLoss: 615.6232\n",
      "Training Epoch: 6 [20608/36450]\tLoss: 643.6087\n",
      "Training Epoch: 6 [20672/36450]\tLoss: 580.4824\n",
      "Training Epoch: 6 [20736/36450]\tLoss: 559.2844\n",
      "Training Epoch: 6 [20800/36450]\tLoss: 605.4052\n",
      "Training Epoch: 6 [20864/36450]\tLoss: 578.2797\n",
      "Training Epoch: 6 [20928/36450]\tLoss: 634.1725\n",
      "Training Epoch: 6 [20992/36450]\tLoss: 594.5197\n",
      "Training Epoch: 6 [21056/36450]\tLoss: 595.5117\n",
      "Training Epoch: 6 [21120/36450]\tLoss: 614.0632\n",
      "Training Epoch: 6 [21184/36450]\tLoss: 637.4998\n",
      "Training Epoch: 6 [21248/36450]\tLoss: 596.1797\n",
      "Training Epoch: 6 [21312/36450]\tLoss: 583.1854\n",
      "Training Epoch: 6 [21376/36450]\tLoss: 577.8801\n",
      "Training Epoch: 6 [21440/36450]\tLoss: 592.3047\n",
      "Training Epoch: 6 [21504/36450]\tLoss: 597.0079\n",
      "Training Epoch: 6 [21568/36450]\tLoss: 602.4691\n",
      "Training Epoch: 6 [21632/36450]\tLoss: 597.2522\n",
      "Training Epoch: 6 [21696/36450]\tLoss: 592.1429\n",
      "Training Epoch: 6 [21760/36450]\tLoss: 559.3647\n",
      "Training Epoch: 6 [21824/36450]\tLoss: 575.4302\n",
      "Training Epoch: 6 [21888/36450]\tLoss: 605.7776\n",
      "Training Epoch: 6 [21952/36450]\tLoss: 587.3195\n",
      "Training Epoch: 6 [22016/36450]\tLoss: 573.3950\n",
      "Training Epoch: 6 [22080/36450]\tLoss: 563.0931\n",
      "Training Epoch: 6 [22144/36450]\tLoss: 610.4741\n",
      "Training Epoch: 6 [22208/36450]\tLoss: 609.5126\n",
      "Training Epoch: 6 [22272/36450]\tLoss: 571.3798\n",
      "Training Epoch: 6 [22336/36450]\tLoss: 584.1699\n",
      "Training Epoch: 6 [22400/36450]\tLoss: 583.0848\n",
      "Training Epoch: 6 [22464/36450]\tLoss: 584.0279\n",
      "Training Epoch: 6 [22528/36450]\tLoss: 557.0404\n",
      "Training Epoch: 6 [22592/36450]\tLoss: 558.8035\n",
      "Training Epoch: 6 [22656/36450]\tLoss: 590.5742\n",
      "Training Epoch: 6 [22720/36450]\tLoss: 608.1473\n",
      "Training Epoch: 6 [22784/36450]\tLoss: 619.8843\n",
      "Training Epoch: 6 [22848/36450]\tLoss: 603.0327\n",
      "Training Epoch: 6 [22912/36450]\tLoss: 597.0431\n",
      "Training Epoch: 6 [22976/36450]\tLoss: 625.4281\n",
      "Training Epoch: 6 [23040/36450]\tLoss: 614.5301\n",
      "Training Epoch: 6 [23104/36450]\tLoss: 627.0395\n",
      "Training Epoch: 6 [23168/36450]\tLoss: 590.9877\n",
      "Training Epoch: 6 [23232/36450]\tLoss: 595.4359\n",
      "Training Epoch: 6 [23296/36450]\tLoss: 564.8574\n",
      "Training Epoch: 6 [23360/36450]\tLoss: 560.0935\n",
      "Training Epoch: 6 [23424/36450]\tLoss: 625.9884\n",
      "Training Epoch: 6 [23488/36450]\tLoss: 604.6232\n",
      "Training Epoch: 6 [23552/36450]\tLoss: 606.1481\n",
      "Training Epoch: 6 [23616/36450]\tLoss: 602.6108\n",
      "Training Epoch: 6 [23680/36450]\tLoss: 587.7484\n",
      "Training Epoch: 6 [23744/36450]\tLoss: 613.1821\n",
      "Training Epoch: 6 [23808/36450]\tLoss: 627.8389\n",
      "Training Epoch: 6 [23872/36450]\tLoss: 608.1510\n",
      "Training Epoch: 6 [23936/36450]\tLoss: 560.4101\n",
      "Training Epoch: 6 [24000/36450]\tLoss: 619.5996\n",
      "Training Epoch: 6 [24064/36450]\tLoss: 594.2697\n",
      "Training Epoch: 6 [24128/36450]\tLoss: 555.7944\n",
      "Training Epoch: 6 [24192/36450]\tLoss: 598.5773\n",
      "Training Epoch: 6 [24256/36450]\tLoss: 606.7338\n",
      "Training Epoch: 6 [24320/36450]\tLoss: 609.6801\n",
      "Training Epoch: 6 [24384/36450]\tLoss: 577.0786\n",
      "Training Epoch: 6 [24448/36450]\tLoss: 569.1088\n",
      "Training Epoch: 6 [24512/36450]\tLoss: 606.1870\n",
      "Training Epoch: 6 [24576/36450]\tLoss: 562.5351\n",
      "Training Epoch: 6 [24640/36450]\tLoss: 598.2860\n",
      "Training Epoch: 6 [24704/36450]\tLoss: 594.7880\n",
      "Training Epoch: 6 [24768/36450]\tLoss: 582.2146\n",
      "Training Epoch: 6 [24832/36450]\tLoss: 589.0797\n",
      "Training Epoch: 6 [24896/36450]\tLoss: 576.0746\n",
      "Training Epoch: 6 [24960/36450]\tLoss: 613.5731\n",
      "Training Epoch: 6 [25024/36450]\tLoss: 624.0274\n",
      "Training Epoch: 6 [25088/36450]\tLoss: 584.2383\n",
      "Training Epoch: 6 [25152/36450]\tLoss: 575.3576\n",
      "Training Epoch: 6 [25216/36450]\tLoss: 598.3154\n",
      "Training Epoch: 6 [25280/36450]\tLoss: 586.8813\n",
      "Training Epoch: 6 [25344/36450]\tLoss: 600.2156\n",
      "Training Epoch: 6 [25408/36450]\tLoss: 609.4572\n",
      "Training Epoch: 6 [25472/36450]\tLoss: 593.4290\n",
      "Training Epoch: 6 [25536/36450]\tLoss: 626.3683\n",
      "Training Epoch: 6 [25600/36450]\tLoss: 590.6512\n",
      "Training Epoch: 6 [25664/36450]\tLoss: 611.4891\n",
      "Training Epoch: 6 [25728/36450]\tLoss: 595.9044\n",
      "Training Epoch: 6 [25792/36450]\tLoss: 558.2950\n",
      "Training Epoch: 6 [25856/36450]\tLoss: 610.0665\n",
      "Training Epoch: 6 [25920/36450]\tLoss: 630.4310\n",
      "Training Epoch: 6 [25984/36450]\tLoss: 585.4549\n",
      "Training Epoch: 6 [26048/36450]\tLoss: 590.7145\n",
      "Training Epoch: 6 [26112/36450]\tLoss: 553.1105\n",
      "Training Epoch: 6 [26176/36450]\tLoss: 559.6545\n",
      "Training Epoch: 6 [26240/36450]\tLoss: 609.4385\n",
      "Training Epoch: 6 [26304/36450]\tLoss: 604.3513\n",
      "Training Epoch: 6 [26368/36450]\tLoss: 613.3548\n",
      "Training Epoch: 6 [26432/36450]\tLoss: 603.2064\n",
      "Training Epoch: 6 [26496/36450]\tLoss: 615.8138\n",
      "Training Epoch: 6 [26560/36450]\tLoss: 558.2493\n",
      "Training Epoch: 6 [26624/36450]\tLoss: 605.1063\n",
      "Training Epoch: 6 [26688/36450]\tLoss: 602.3593\n",
      "Training Epoch: 6 [26752/36450]\tLoss: 562.2031\n",
      "Training Epoch: 6 [26816/36450]\tLoss: 621.7188\n",
      "Training Epoch: 6 [26880/36450]\tLoss: 596.0004\n",
      "Training Epoch: 6 [26944/36450]\tLoss: 595.5129\n",
      "Training Epoch: 6 [27008/36450]\tLoss: 588.3583\n",
      "Training Epoch: 6 [27072/36450]\tLoss: 612.9745\n",
      "Training Epoch: 6 [27136/36450]\tLoss: 607.8569\n",
      "Training Epoch: 6 [27200/36450]\tLoss: 629.4756\n",
      "Training Epoch: 6 [27264/36450]\tLoss: 627.6954\n",
      "Training Epoch: 6 [27328/36450]\tLoss: 658.8539\n",
      "Training Epoch: 6 [27392/36450]\tLoss: 672.4218\n",
      "Training Epoch: 6 [27456/36450]\tLoss: 664.4231\n",
      "Training Epoch: 6 [27520/36450]\tLoss: 641.8620\n",
      "Training Epoch: 6 [27584/36450]\tLoss: 669.0082\n",
      "Training Epoch: 6 [27648/36450]\tLoss: 684.9077\n",
      "Training Epoch: 6 [27712/36450]\tLoss: 615.4346\n",
      "Training Epoch: 6 [27776/36450]\tLoss: 596.0163\n",
      "Training Epoch: 6 [27840/36450]\tLoss: 584.1155\n",
      "Training Epoch: 6 [27904/36450]\tLoss: 626.7829\n",
      "Training Epoch: 6 [27968/36450]\tLoss: 584.5294\n",
      "Training Epoch: 6 [28032/36450]\tLoss: 598.6241\n",
      "Training Epoch: 6 [28096/36450]\tLoss: 616.6450\n",
      "Training Epoch: 6 [28160/36450]\tLoss: 594.3015\n",
      "Training Epoch: 6 [28224/36450]\tLoss: 586.4896\n",
      "Training Epoch: 6 [28288/36450]\tLoss: 615.4158\n",
      "Training Epoch: 6 [28352/36450]\tLoss: 616.5833\n",
      "Training Epoch: 6 [28416/36450]\tLoss: 604.1729\n",
      "Training Epoch: 6 [28480/36450]\tLoss: 649.0906\n",
      "Training Epoch: 6 [28544/36450]\tLoss: 606.7251\n",
      "Training Epoch: 6 [28608/36450]\tLoss: 578.0490\n",
      "Training Epoch: 6 [28672/36450]\tLoss: 592.5781\n",
      "Training Epoch: 6 [28736/36450]\tLoss: 594.6068\n",
      "Training Epoch: 6 [28800/36450]\tLoss: 552.9403\n",
      "Training Epoch: 6 [28864/36450]\tLoss: 613.8056\n",
      "Training Epoch: 6 [28928/36450]\tLoss: 571.3170\n",
      "Training Epoch: 6 [28992/36450]\tLoss: 600.1637\n",
      "Training Epoch: 6 [29056/36450]\tLoss: 575.0269\n",
      "Training Epoch: 6 [29120/36450]\tLoss: 597.8046\n",
      "Training Epoch: 6 [29184/36450]\tLoss: 600.9250\n",
      "Training Epoch: 6 [29248/36450]\tLoss: 605.3562\n",
      "Training Epoch: 6 [29312/36450]\tLoss: 634.1495\n",
      "Training Epoch: 6 [29376/36450]\tLoss: 567.8318\n",
      "Training Epoch: 6 [29440/36450]\tLoss: 622.3727\n",
      "Training Epoch: 6 [29504/36450]\tLoss: 594.3862\n",
      "Training Epoch: 6 [29568/36450]\tLoss: 587.6865\n",
      "Training Epoch: 6 [29632/36450]\tLoss: 598.6299\n",
      "Training Epoch: 6 [29696/36450]\tLoss: 590.5900\n",
      "Training Epoch: 6 [29760/36450]\tLoss: 578.4453\n",
      "Training Epoch: 6 [29824/36450]\tLoss: 587.8180\n",
      "Training Epoch: 6 [29888/36450]\tLoss: 601.7119\n",
      "Training Epoch: 6 [29952/36450]\tLoss: 586.7276\n",
      "Training Epoch: 6 [30016/36450]\tLoss: 594.4465\n",
      "Training Epoch: 6 [30080/36450]\tLoss: 598.0701\n",
      "Training Epoch: 6 [30144/36450]\tLoss: 572.5579\n",
      "Training Epoch: 6 [30208/36450]\tLoss: 593.0145\n",
      "Training Epoch: 6 [30272/36450]\tLoss: 590.1143\n",
      "Training Epoch: 6 [30336/36450]\tLoss: 572.5412\n",
      "Training Epoch: 6 [30400/36450]\tLoss: 558.0448\n",
      "Training Epoch: 6 [30464/36450]\tLoss: 613.7118\n",
      "Training Epoch: 6 [30528/36450]\tLoss: 614.6664\n",
      "Training Epoch: 6 [30592/36450]\tLoss: 616.3044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [30656/36450]\tLoss: 590.6094\n",
      "Training Epoch: 6 [30720/36450]\tLoss: 572.8789\n",
      "Training Epoch: 6 [30784/36450]\tLoss: 614.5510\n",
      "Training Epoch: 6 [30848/36450]\tLoss: 588.8807\n",
      "Training Epoch: 6 [30912/36450]\tLoss: 597.2484\n",
      "Training Epoch: 6 [30976/36450]\tLoss: 577.9965\n",
      "Training Epoch: 6 [31040/36450]\tLoss: 592.3272\n",
      "Training Epoch: 6 [31104/36450]\tLoss: 583.9850\n",
      "Training Epoch: 6 [31168/36450]\tLoss: 569.4730\n",
      "Training Epoch: 6 [31232/36450]\tLoss: 588.0370\n",
      "Training Epoch: 6 [31296/36450]\tLoss: 584.2211\n",
      "Training Epoch: 6 [31360/36450]\tLoss: 600.2134\n",
      "Training Epoch: 6 [31424/36450]\tLoss: 617.7196\n",
      "Training Epoch: 6 [31488/36450]\tLoss: 572.4178\n",
      "Training Epoch: 6 [31552/36450]\tLoss: 635.5385\n",
      "Training Epoch: 6 [31616/36450]\tLoss: 585.2365\n",
      "Training Epoch: 6 [31680/36450]\tLoss: 584.9792\n",
      "Training Epoch: 6 [31744/36450]\tLoss: 553.6059\n",
      "Training Epoch: 6 [31808/36450]\tLoss: 577.6595\n",
      "Training Epoch: 6 [31872/36450]\tLoss: 563.5114\n",
      "Training Epoch: 6 [31936/36450]\tLoss: 570.4504\n",
      "Training Epoch: 6 [32000/36450]\tLoss: 610.3638\n",
      "Training Epoch: 6 [32064/36450]\tLoss: 548.7473\n",
      "Training Epoch: 6 [32128/36450]\tLoss: 554.2841\n",
      "Training Epoch: 6 [32192/36450]\tLoss: 621.9441\n",
      "Training Epoch: 6 [32256/36450]\tLoss: 611.0347\n",
      "Training Epoch: 6 [32320/36450]\tLoss: 578.6097\n",
      "Training Epoch: 6 [32384/36450]\tLoss: 578.3104\n",
      "Training Epoch: 6 [32448/36450]\tLoss: 602.5774\n",
      "Training Epoch: 6 [32512/36450]\tLoss: 610.5629\n",
      "Training Epoch: 6 [32576/36450]\tLoss: 608.1386\n",
      "Training Epoch: 6 [32640/36450]\tLoss: 581.2072\n",
      "Training Epoch: 6 [32704/36450]\tLoss: 583.3801\n",
      "Training Epoch: 6 [32768/36450]\tLoss: 560.2792\n",
      "Training Epoch: 6 [32832/36450]\tLoss: 584.7168\n",
      "Training Epoch: 6 [32896/36450]\tLoss: 613.2741\n",
      "Training Epoch: 6 [32960/36450]\tLoss: 589.5259\n",
      "Training Epoch: 6 [33024/36450]\tLoss: 621.0239\n",
      "Training Epoch: 6 [33088/36450]\tLoss: 577.6359\n",
      "Training Epoch: 6 [33152/36450]\tLoss: 567.6782\n",
      "Training Epoch: 6 [33216/36450]\tLoss: 586.3198\n",
      "Training Epoch: 6 [33280/36450]\tLoss: 583.4086\n",
      "Training Epoch: 6 [33344/36450]\tLoss: 575.2347\n",
      "Training Epoch: 6 [33408/36450]\tLoss: 596.2797\n",
      "Training Epoch: 6 [33472/36450]\tLoss: 594.9919\n",
      "Training Epoch: 6 [33536/36450]\tLoss: 602.5557\n",
      "Training Epoch: 6 [33600/36450]\tLoss: 595.3522\n",
      "Training Epoch: 6 [33664/36450]\tLoss: 575.7498\n",
      "Training Epoch: 6 [33728/36450]\tLoss: 566.7715\n",
      "Training Epoch: 6 [33792/36450]\tLoss: 568.9610\n",
      "Training Epoch: 6 [33856/36450]\tLoss: 577.5621\n",
      "Training Epoch: 6 [33920/36450]\tLoss: 553.9664\n",
      "Training Epoch: 6 [33984/36450]\tLoss: 609.3029\n",
      "Training Epoch: 6 [34048/36450]\tLoss: 597.9610\n",
      "Training Epoch: 6 [34112/36450]\tLoss: 587.9407\n",
      "Training Epoch: 6 [34176/36450]\tLoss: 589.0205\n",
      "Training Epoch: 6 [34240/36450]\tLoss: 569.3845\n",
      "Training Epoch: 6 [34304/36450]\tLoss: 580.7271\n",
      "Training Epoch: 6 [34368/36450]\tLoss: 603.9413\n",
      "Training Epoch: 6 [34432/36450]\tLoss: 551.4995\n",
      "Training Epoch: 6 [34496/36450]\tLoss: 604.8737\n",
      "Training Epoch: 6 [34560/36450]\tLoss: 595.6121\n",
      "Training Epoch: 6 [34624/36450]\tLoss: 573.1725\n",
      "Training Epoch: 6 [34688/36450]\tLoss: 565.6305\n",
      "Training Epoch: 6 [34752/36450]\tLoss: 561.5243\n",
      "Training Epoch: 6 [34816/36450]\tLoss: 565.8283\n",
      "Training Epoch: 6 [34880/36450]\tLoss: 590.5424\n",
      "Training Epoch: 6 [34944/36450]\tLoss: 624.1235\n",
      "Training Epoch: 6 [35008/36450]\tLoss: 556.5958\n",
      "Training Epoch: 6 [35072/36450]\tLoss: 569.9850\n",
      "Training Epoch: 6 [35136/36450]\tLoss: 579.6055\n",
      "Training Epoch: 6 [35200/36450]\tLoss: 592.6699\n",
      "Training Epoch: 6 [35264/36450]\tLoss: 575.0633\n",
      "Training Epoch: 6 [35328/36450]\tLoss: 575.7468\n",
      "Training Epoch: 6 [35392/36450]\tLoss: 603.9142\n",
      "Training Epoch: 6 [35456/36450]\tLoss: 636.6580\n",
      "Training Epoch: 6 [35520/36450]\tLoss: 612.8184\n",
      "Training Epoch: 6 [35584/36450]\tLoss: 617.8385\n",
      "Training Epoch: 6 [35648/36450]\tLoss: 594.0352\n",
      "Training Epoch: 6 [35712/36450]\tLoss: 566.8289\n",
      "Training Epoch: 6 [35776/36450]\tLoss: 573.9495\n",
      "Training Epoch: 6 [35840/36450]\tLoss: 597.6777\n",
      "Training Epoch: 6 [35904/36450]\tLoss: 608.6031\n",
      "Training Epoch: 6 [35968/36450]\tLoss: 559.1233\n",
      "Training Epoch: 6 [36032/36450]\tLoss: 593.9720\n",
      "Training Epoch: 6 [36096/36450]\tLoss: 584.5244\n",
      "Training Epoch: 6 [36160/36450]\tLoss: 585.1231\n",
      "Training Epoch: 6 [36224/36450]\tLoss: 575.9532\n",
      "Training Epoch: 6 [36288/36450]\tLoss: 540.5338\n",
      "Training Epoch: 6 [36352/36450]\tLoss: 593.2501\n",
      "Training Epoch: 6 [36416/36450]\tLoss: 556.4212\n",
      "Training Epoch: 6 [36450/36450]\tLoss: 582.0001\n",
      "Training Epoch: 6 [4050/4050]\tLoss: 291.2588\n",
      "Training Epoch: 7 [64/36450]\tLoss: 618.6592\n",
      "Training Epoch: 7 [128/36450]\tLoss: 586.5303\n",
      "Training Epoch: 7 [192/36450]\tLoss: 595.1329\n",
      "Training Epoch: 7 [256/36450]\tLoss: 571.1473\n",
      "Training Epoch: 7 [320/36450]\tLoss: 618.1753\n",
      "Training Epoch: 7 [384/36450]\tLoss: 582.7057\n",
      "Training Epoch: 7 [448/36450]\tLoss: 594.3533\n",
      "Training Epoch: 7 [512/36450]\tLoss: 592.3051\n",
      "Training Epoch: 7 [576/36450]\tLoss: 595.8457\n",
      "Training Epoch: 7 [640/36450]\tLoss: 608.8420\n",
      "Training Epoch: 7 [704/36450]\tLoss: 658.1423\n",
      "Training Epoch: 7 [768/36450]\tLoss: 619.1658\n",
      "Training Epoch: 7 [832/36450]\tLoss: 643.1882\n",
      "Training Epoch: 7 [896/36450]\tLoss: 575.8148\n",
      "Training Epoch: 7 [960/36450]\tLoss: 621.8175\n",
      "Training Epoch: 7 [1024/36450]\tLoss: 614.5566\n",
      "Training Epoch: 7 [1088/36450]\tLoss: 570.5720\n",
      "Training Epoch: 7 [1152/36450]\tLoss: 598.6399\n",
      "Training Epoch: 7 [1216/36450]\tLoss: 610.5995\n",
      "Training Epoch: 7 [1280/36450]\tLoss: 594.8328\n",
      "Training Epoch: 7 [1344/36450]\tLoss: 619.1957\n",
      "Training Epoch: 7 [1408/36450]\tLoss: 550.7352\n",
      "Training Epoch: 7 [1472/36450]\tLoss: 614.1209\n",
      "Training Epoch: 7 [1536/36450]\tLoss: 574.5834\n",
      "Training Epoch: 7 [1600/36450]\tLoss: 584.5162\n",
      "Training Epoch: 7 [1664/36450]\tLoss: 594.0117\n",
      "Training Epoch: 7 [1728/36450]\tLoss: 566.5452\n",
      "Training Epoch: 7 [1792/36450]\tLoss: 595.8331\n",
      "Training Epoch: 7 [1856/36450]\tLoss: 566.6822\n",
      "Training Epoch: 7 [1920/36450]\tLoss: 603.0337\n",
      "Training Epoch: 7 [1984/36450]\tLoss: 596.8689\n",
      "Training Epoch: 7 [2048/36450]\tLoss: 618.7051\n",
      "Training Epoch: 7 [2112/36450]\tLoss: 559.2926\n",
      "Training Epoch: 7 [2176/36450]\tLoss: 588.4395\n",
      "Training Epoch: 7 [2240/36450]\tLoss: 576.7855\n",
      "Training Epoch: 7 [2304/36450]\tLoss: 594.4309\n",
      "Training Epoch: 7 [2368/36450]\tLoss: 563.8278\n",
      "Training Epoch: 7 [2432/36450]\tLoss: 577.1248\n",
      "Training Epoch: 7 [2496/36450]\tLoss: 579.6000\n",
      "Training Epoch: 7 [2560/36450]\tLoss: 581.6774\n",
      "Training Epoch: 7 [2624/36450]\tLoss: 571.8661\n",
      "Training Epoch: 7 [2688/36450]\tLoss: 551.5109\n",
      "Training Epoch: 7 [2752/36450]\tLoss: 581.7200\n",
      "Training Epoch: 7 [2816/36450]\tLoss: 579.5151\n",
      "Training Epoch: 7 [2880/36450]\tLoss: 592.8072\n",
      "Training Epoch: 7 [2944/36450]\tLoss: 586.6495\n",
      "Training Epoch: 7 [3008/36450]\tLoss: 569.9645\n",
      "Training Epoch: 7 [3072/36450]\tLoss: 587.4988\n",
      "Training Epoch: 7 [3136/36450]\tLoss: 579.5089\n",
      "Training Epoch: 7 [3200/36450]\tLoss: 592.0200\n",
      "Training Epoch: 7 [3264/36450]\tLoss: 587.3247\n",
      "Training Epoch: 7 [3328/36450]\tLoss: 613.6766\n",
      "Training Epoch: 7 [3392/36450]\tLoss: 581.1817\n",
      "Training Epoch: 7 [3456/36450]\tLoss: 577.0848\n",
      "Training Epoch: 7 [3520/36450]\tLoss: 606.0985\n",
      "Training Epoch: 7 [3584/36450]\tLoss: 552.0819\n",
      "Training Epoch: 7 [3648/36450]\tLoss: 602.5506\n",
      "Training Epoch: 7 [3712/36450]\tLoss: 590.6337\n",
      "Training Epoch: 7 [3776/36450]\tLoss: 591.1300\n",
      "Training Epoch: 7 [3840/36450]\tLoss: 587.9670\n",
      "Training Epoch: 7 [3904/36450]\tLoss: 615.0171\n",
      "Training Epoch: 7 [3968/36450]\tLoss: 597.7706\n",
      "Training Epoch: 7 [4032/36450]\tLoss: 597.5234\n",
      "Training Epoch: 7 [4096/36450]\tLoss: 591.2251\n",
      "Training Epoch: 7 [4160/36450]\tLoss: 586.7158\n",
      "Training Epoch: 7 [4224/36450]\tLoss: 600.7485\n",
      "Training Epoch: 7 [4288/36450]\tLoss: 595.7326\n",
      "Training Epoch: 7 [4352/36450]\tLoss: 583.1105\n",
      "Training Epoch: 7 [4416/36450]\tLoss: 570.4352\n",
      "Training Epoch: 7 [4480/36450]\tLoss: 587.6110\n",
      "Training Epoch: 7 [4544/36450]\tLoss: 599.4720\n",
      "Training Epoch: 7 [4608/36450]\tLoss: 567.2526\n",
      "Training Epoch: 7 [4672/36450]\tLoss: 605.1476\n",
      "Training Epoch: 7 [4736/36450]\tLoss: 611.1420\n",
      "Training Epoch: 7 [4800/36450]\tLoss: 589.1979\n",
      "Training Epoch: 7 [4864/36450]\tLoss: 573.6740\n",
      "Training Epoch: 7 [4928/36450]\tLoss: 540.5523\n",
      "Training Epoch: 7 [4992/36450]\tLoss: 559.4569\n",
      "Training Epoch: 7 [5056/36450]\tLoss: 588.0691\n",
      "Training Epoch: 7 [5120/36450]\tLoss: 590.5560\n",
      "Training Epoch: 7 [5184/36450]\tLoss: 557.4049\n",
      "Training Epoch: 7 [5248/36450]\tLoss: 602.4120\n",
      "Training Epoch: 7 [5312/36450]\tLoss: 542.2606\n",
      "Training Epoch: 7 [5376/36450]\tLoss: 581.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [5440/36450]\tLoss: 582.2655\n",
      "Training Epoch: 7 [5504/36450]\tLoss: 581.2382\n",
      "Training Epoch: 7 [5568/36450]\tLoss: 575.9846\n",
      "Training Epoch: 7 [5632/36450]\tLoss: 566.8413\n",
      "Training Epoch: 7 [5696/36450]\tLoss: 545.3776\n",
      "Training Epoch: 7 [5760/36450]\tLoss: 606.7496\n",
      "Training Epoch: 7 [5824/36450]\tLoss: 588.7746\n",
      "Training Epoch: 7 [5888/36450]\tLoss: 560.1507\n",
      "Training Epoch: 7 [5952/36450]\tLoss: 554.4670\n",
      "Training Epoch: 7 [6016/36450]\tLoss: 574.6393\n",
      "Training Epoch: 7 [6080/36450]\tLoss: 608.6028\n",
      "Training Epoch: 7 [6144/36450]\tLoss: 606.3635\n",
      "Training Epoch: 7 [6208/36450]\tLoss: 561.6061\n",
      "Training Epoch: 7 [6272/36450]\tLoss: 622.4280\n",
      "Training Epoch: 7 [6336/36450]\tLoss: 590.8098\n",
      "Training Epoch: 7 [6400/36450]\tLoss: 593.6830\n",
      "Training Epoch: 7 [6464/36450]\tLoss: 561.9243\n",
      "Training Epoch: 7 [6528/36450]\tLoss: 611.2227\n",
      "Training Epoch: 7 [6592/36450]\tLoss: 587.7387\n",
      "Training Epoch: 7 [6656/36450]\tLoss: 606.0200\n",
      "Training Epoch: 7 [6720/36450]\tLoss: 601.1350\n",
      "Training Epoch: 7 [6784/36450]\tLoss: 601.1591\n",
      "Training Epoch: 7 [6848/36450]\tLoss: 574.0164\n",
      "Training Epoch: 7 [6912/36450]\tLoss: 566.0087\n",
      "Training Epoch: 7 [6976/36450]\tLoss: 590.5330\n",
      "Training Epoch: 7 [7040/36450]\tLoss: 597.7498\n",
      "Training Epoch: 7 [7104/36450]\tLoss: 603.1110\n",
      "Training Epoch: 7 [7168/36450]\tLoss: 575.1035\n",
      "Training Epoch: 7 [7232/36450]\tLoss: 627.6053\n",
      "Training Epoch: 7 [7296/36450]\tLoss: 580.8048\n",
      "Training Epoch: 7 [7360/36450]\tLoss: 580.8298\n",
      "Training Epoch: 7 [7424/36450]\tLoss: 569.7719\n",
      "Training Epoch: 7 [7488/36450]\tLoss: 545.2167\n",
      "Training Epoch: 7 [7552/36450]\tLoss: 556.2404\n",
      "Training Epoch: 7 [7616/36450]\tLoss: 605.7992\n",
      "Training Epoch: 7 [7680/36450]\tLoss: 592.6276\n",
      "Training Epoch: 7 [7744/36450]\tLoss: 599.1326\n",
      "Training Epoch: 7 [7808/36450]\tLoss: 570.8812\n",
      "Training Epoch: 7 [7872/36450]\tLoss: 578.7839\n",
      "Training Epoch: 7 [7936/36450]\tLoss: 612.1531\n",
      "Training Epoch: 7 [8000/36450]\tLoss: 566.8586\n",
      "Training Epoch: 7 [8064/36450]\tLoss: 537.0659\n",
      "Training Epoch: 7 [8128/36450]\tLoss: 577.8117\n",
      "Training Epoch: 7 [8192/36450]\tLoss: 597.2630\n",
      "Training Epoch: 7 [8256/36450]\tLoss: 564.7964\n",
      "Training Epoch: 7 [8320/36450]\tLoss: 595.2034\n",
      "Training Epoch: 7 [8384/36450]\tLoss: 597.8572\n",
      "Training Epoch: 7 [8448/36450]\tLoss: 590.7643\n",
      "Training Epoch: 7 [8512/36450]\tLoss: 571.6718\n",
      "Training Epoch: 7 [8576/36450]\tLoss: 589.2327\n",
      "Training Epoch: 7 [8640/36450]\tLoss: 592.0834\n",
      "Training Epoch: 7 [8704/36450]\tLoss: 577.0245\n",
      "Training Epoch: 7 [8768/36450]\tLoss: 574.7457\n",
      "Training Epoch: 7 [8832/36450]\tLoss: 552.5610\n",
      "Training Epoch: 7 [8896/36450]\tLoss: 556.9680\n",
      "Training Epoch: 7 [8960/36450]\tLoss: 597.6681\n",
      "Training Epoch: 7 [9024/36450]\tLoss: 563.3386\n",
      "Training Epoch: 7 [9088/36450]\tLoss: 605.5508\n",
      "Training Epoch: 7 [9152/36450]\tLoss: 575.7512\n",
      "Training Epoch: 7 [9216/36450]\tLoss: 605.1237\n",
      "Training Epoch: 7 [9280/36450]\tLoss: 610.6643\n",
      "Training Epoch: 7 [9344/36450]\tLoss: 585.6464\n",
      "Training Epoch: 7 [9408/36450]\tLoss: 580.6903\n",
      "Training Epoch: 7 [9472/36450]\tLoss: 596.7346\n",
      "Training Epoch: 7 [9536/36450]\tLoss: 602.3730\n",
      "Training Epoch: 7 [9600/36450]\tLoss: 552.7404\n",
      "Training Epoch: 7 [9664/36450]\tLoss: 620.5335\n",
      "Training Epoch: 7 [9728/36450]\tLoss: 607.4623\n",
      "Training Epoch: 7 [9792/36450]\tLoss: 593.5544\n",
      "Training Epoch: 7 [9856/36450]\tLoss: 638.3121\n",
      "Training Epoch: 7 [9920/36450]\tLoss: 585.6048\n",
      "Training Epoch: 7 [9984/36450]\tLoss: 603.1962\n",
      "Training Epoch: 7 [10048/36450]\tLoss: 588.2389\n",
      "Training Epoch: 7 [10112/36450]\tLoss: 576.7665\n",
      "Training Epoch: 7 [10176/36450]\tLoss: 566.5612\n",
      "Training Epoch: 7 [10240/36450]\tLoss: 565.0716\n",
      "Training Epoch: 7 [10304/36450]\tLoss: 582.3687\n",
      "Training Epoch: 7 [10368/36450]\tLoss: 565.0289\n",
      "Training Epoch: 7 [10432/36450]\tLoss: 586.0931\n",
      "Training Epoch: 7 [10496/36450]\tLoss: 607.6591\n",
      "Training Epoch: 7 [10560/36450]\tLoss: 565.4644\n",
      "Training Epoch: 7 [10624/36450]\tLoss: 593.5270\n",
      "Training Epoch: 7 [10688/36450]\tLoss: 604.2423\n",
      "Training Epoch: 7 [10752/36450]\tLoss: 592.7019\n",
      "Training Epoch: 7 [10816/36450]\tLoss: 554.2527\n",
      "Training Epoch: 7 [10880/36450]\tLoss: 571.0828\n",
      "Training Epoch: 7 [10944/36450]\tLoss: 562.7303\n",
      "Training Epoch: 7 [11008/36450]\tLoss: 570.1142\n",
      "Training Epoch: 7 [11072/36450]\tLoss: 567.5759\n",
      "Training Epoch: 7 [11136/36450]\tLoss: 593.0425\n",
      "Training Epoch: 7 [11200/36450]\tLoss: 564.6734\n",
      "Training Epoch: 7 [11264/36450]\tLoss: 548.0721\n",
      "Training Epoch: 7 [11328/36450]\tLoss: 596.0049\n",
      "Training Epoch: 7 [11392/36450]\tLoss: 581.1342\n",
      "Training Epoch: 7 [11456/36450]\tLoss: 569.3994\n",
      "Training Epoch: 7 [11520/36450]\tLoss: 588.9572\n",
      "Training Epoch: 7 [11584/36450]\tLoss: 596.1470\n",
      "Training Epoch: 7 [11648/36450]\tLoss: 579.6306\n",
      "Training Epoch: 7 [11712/36450]\tLoss: 585.4683\n",
      "Training Epoch: 7 [11776/36450]\tLoss: 599.7327\n",
      "Training Epoch: 7 [11840/36450]\tLoss: 550.4347\n",
      "Training Epoch: 7 [11904/36450]\tLoss: 587.7397\n",
      "Training Epoch: 7 [11968/36450]\tLoss: 585.9501\n",
      "Training Epoch: 7 [12032/36450]\tLoss: 577.6127\n",
      "Training Epoch: 7 [12096/36450]\tLoss: 634.2855\n",
      "Training Epoch: 7 [12160/36450]\tLoss: 581.6841\n",
      "Training Epoch: 7 [12224/36450]\tLoss: 627.9169\n",
      "Training Epoch: 7 [12288/36450]\tLoss: 577.9762\n",
      "Training Epoch: 7 [12352/36450]\tLoss: 620.1094\n",
      "Training Epoch: 7 [12416/36450]\tLoss: 587.6249\n",
      "Training Epoch: 7 [12480/36450]\tLoss: 615.7906\n",
      "Training Epoch: 7 [12544/36450]\tLoss: 612.9407\n",
      "Training Epoch: 7 [12608/36450]\tLoss: 593.1143\n",
      "Training Epoch: 7 [12672/36450]\tLoss: 549.6493\n",
      "Training Epoch: 7 [12736/36450]\tLoss: 602.8168\n",
      "Training Epoch: 7 [12800/36450]\tLoss: 554.2698\n",
      "Training Epoch: 7 [12864/36450]\tLoss: 573.8296\n",
      "Training Epoch: 7 [12928/36450]\tLoss: 591.4885\n",
      "Training Epoch: 7 [12992/36450]\tLoss: 573.4465\n",
      "Training Epoch: 7 [13056/36450]\tLoss: 564.2193\n",
      "Training Epoch: 7 [13120/36450]\tLoss: 552.5967\n",
      "Training Epoch: 7 [13184/36450]\tLoss: 560.2041\n",
      "Training Epoch: 7 [13248/36450]\tLoss: 618.6089\n",
      "Training Epoch: 7 [13312/36450]\tLoss: 594.9120\n",
      "Training Epoch: 7 [13376/36450]\tLoss: 578.6665\n",
      "Training Epoch: 7 [13440/36450]\tLoss: 574.5925\n",
      "Training Epoch: 7 [13504/36450]\tLoss: 577.0313\n",
      "Training Epoch: 7 [13568/36450]\tLoss: 598.9536\n",
      "Training Epoch: 7 [13632/36450]\tLoss: 576.0150\n",
      "Training Epoch: 7 [13696/36450]\tLoss: 597.8459\n",
      "Training Epoch: 7 [13760/36450]\tLoss: 593.4791\n",
      "Training Epoch: 7 [13824/36450]\tLoss: 579.5912\n",
      "Training Epoch: 7 [13888/36450]\tLoss: 605.3877\n",
      "Training Epoch: 7 [13952/36450]\tLoss: 556.7562\n",
      "Training Epoch: 7 [14016/36450]\tLoss: 605.2537\n",
      "Training Epoch: 7 [14080/36450]\tLoss: 608.4807\n",
      "Training Epoch: 7 [14144/36450]\tLoss: 553.8954\n",
      "Training Epoch: 7 [14208/36450]\tLoss: 567.2952\n",
      "Training Epoch: 7 [14272/36450]\tLoss: 592.8929\n",
      "Training Epoch: 7 [14336/36450]\tLoss: 619.3712\n",
      "Training Epoch: 7 [14400/36450]\tLoss: 611.6658\n",
      "Training Epoch: 7 [14464/36450]\tLoss: 556.9648\n",
      "Training Epoch: 7 [14528/36450]\tLoss: 584.5677\n",
      "Training Epoch: 7 [14592/36450]\tLoss: 574.2843\n",
      "Training Epoch: 7 [14656/36450]\tLoss: 579.0577\n",
      "Training Epoch: 7 [14720/36450]\tLoss: 592.8798\n",
      "Training Epoch: 7 [14784/36450]\tLoss: 612.7502\n",
      "Training Epoch: 7 [14848/36450]\tLoss: 574.4713\n",
      "Training Epoch: 7 [14912/36450]\tLoss: 626.2877\n",
      "Training Epoch: 7 [14976/36450]\tLoss: 611.9578\n",
      "Training Epoch: 7 [15040/36450]\tLoss: 592.8094\n",
      "Training Epoch: 7 [15104/36450]\tLoss: 605.7050\n",
      "Training Epoch: 7 [15168/36450]\tLoss: 602.3198\n",
      "Training Epoch: 7 [15232/36450]\tLoss: 586.6187\n",
      "Training Epoch: 7 [15296/36450]\tLoss: 608.5892\n",
      "Training Epoch: 7 [15360/36450]\tLoss: 583.9703\n",
      "Training Epoch: 7 [15424/36450]\tLoss: 574.4332\n",
      "Training Epoch: 7 [15488/36450]\tLoss: 566.7608\n",
      "Training Epoch: 7 [15552/36450]\tLoss: 574.6049\n",
      "Training Epoch: 7 [15616/36450]\tLoss: 556.4503\n",
      "Training Epoch: 7 [15680/36450]\tLoss: 555.6143\n",
      "Training Epoch: 7 [15744/36450]\tLoss: 587.8939\n",
      "Training Epoch: 7 [15808/36450]\tLoss: 567.9938\n",
      "Training Epoch: 7 [15872/36450]\tLoss: 567.1641\n",
      "Training Epoch: 7 [15936/36450]\tLoss: 553.5645\n",
      "Training Epoch: 7 [16000/36450]\tLoss: 552.8120\n",
      "Training Epoch: 7 [16064/36450]\tLoss: 573.3430\n",
      "Training Epoch: 7 [16128/36450]\tLoss: 566.8188\n",
      "Training Epoch: 7 [16192/36450]\tLoss: 567.4771\n",
      "Training Epoch: 7 [16256/36450]\tLoss: 552.3367\n",
      "Training Epoch: 7 [16320/36450]\tLoss: 562.7379\n",
      "Training Epoch: 7 [16384/36450]\tLoss: 558.7609\n",
      "Training Epoch: 7 [16448/36450]\tLoss: 595.2301\n",
      "Training Epoch: 7 [16512/36450]\tLoss: 558.8425\n",
      "Training Epoch: 7 [16576/36450]\tLoss: 584.9370\n",
      "Training Epoch: 7 [16640/36450]\tLoss: 587.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [16704/36450]\tLoss: 631.1016\n",
      "Training Epoch: 7 [16768/36450]\tLoss: 584.7549\n",
      "Training Epoch: 7 [16832/36450]\tLoss: 597.8738\n",
      "Training Epoch: 7 [16896/36450]\tLoss: 602.5190\n",
      "Training Epoch: 7 [16960/36450]\tLoss: 580.0754\n",
      "Training Epoch: 7 [17024/36450]\tLoss: 576.5121\n",
      "Training Epoch: 7 [17088/36450]\tLoss: 584.1752\n",
      "Training Epoch: 7 [17152/36450]\tLoss: 575.3484\n",
      "Training Epoch: 7 [17216/36450]\tLoss: 613.5712\n",
      "Training Epoch: 7 [17280/36450]\tLoss: 568.5804\n",
      "Training Epoch: 7 [17344/36450]\tLoss: 588.2443\n",
      "Training Epoch: 7 [17408/36450]\tLoss: 548.3130\n",
      "Training Epoch: 7 [17472/36450]\tLoss: 580.7521\n",
      "Training Epoch: 7 [17536/36450]\tLoss: 588.0184\n",
      "Training Epoch: 7 [17600/36450]\tLoss: 551.3905\n",
      "Training Epoch: 7 [17664/36450]\tLoss: 607.6715\n",
      "Training Epoch: 7 [17728/36450]\tLoss: 605.3260\n",
      "Training Epoch: 7 [17792/36450]\tLoss: 575.6944\n",
      "Training Epoch: 7 [17856/36450]\tLoss: 587.1356\n",
      "Training Epoch: 7 [17920/36450]\tLoss: 550.6937\n",
      "Training Epoch: 7 [17984/36450]\tLoss: 570.0654\n",
      "Training Epoch: 7 [18048/36450]\tLoss: 566.7287\n",
      "Training Epoch: 7 [18112/36450]\tLoss: 534.0583\n",
      "Training Epoch: 7 [18176/36450]\tLoss: 570.0466\n",
      "Training Epoch: 7 [18240/36450]\tLoss: 581.6571\n",
      "Training Epoch: 7 [18304/36450]\tLoss: 587.8259\n",
      "Training Epoch: 7 [18368/36450]\tLoss: 564.9957\n",
      "Training Epoch: 7 [18432/36450]\tLoss: 571.3436\n",
      "Training Epoch: 7 [18496/36450]\tLoss: 581.3843\n",
      "Training Epoch: 7 [18560/36450]\tLoss: 548.5097\n",
      "Training Epoch: 7 [18624/36450]\tLoss: 589.3722\n",
      "Training Epoch: 7 [18688/36450]\tLoss: 552.8389\n",
      "Training Epoch: 7 [18752/36450]\tLoss: 581.9277\n",
      "Training Epoch: 7 [18816/36450]\tLoss: 538.3237\n",
      "Training Epoch: 7 [18880/36450]\tLoss: 597.5713\n",
      "Training Epoch: 7 [18944/36450]\tLoss: 568.0943\n",
      "Training Epoch: 7 [19008/36450]\tLoss: 592.3909\n",
      "Training Epoch: 7 [19072/36450]\tLoss: 566.4420\n",
      "Training Epoch: 7 [19136/36450]\tLoss: 555.9464\n",
      "Training Epoch: 7 [19200/36450]\tLoss: 563.7131\n",
      "Training Epoch: 7 [19264/36450]\tLoss: 550.9429\n",
      "Training Epoch: 7 [19328/36450]\tLoss: 548.6859\n",
      "Training Epoch: 7 [19392/36450]\tLoss: 570.9636\n",
      "Training Epoch: 7 [19456/36450]\tLoss: 597.6731\n",
      "Training Epoch: 7 [19520/36450]\tLoss: 603.2946\n",
      "Training Epoch: 7 [19584/36450]\tLoss: 562.4609\n",
      "Training Epoch: 7 [19648/36450]\tLoss: 588.9894\n",
      "Training Epoch: 7 [19712/36450]\tLoss: 626.1456\n",
      "Training Epoch: 7 [19776/36450]\tLoss: 574.4218\n",
      "Training Epoch: 7 [19840/36450]\tLoss: 572.0865\n",
      "Training Epoch: 7 [19904/36450]\tLoss: 570.9335\n",
      "Training Epoch: 7 [19968/36450]\tLoss: 588.4382\n",
      "Training Epoch: 7 [20032/36450]\tLoss: 572.5969\n",
      "Training Epoch: 7 [20096/36450]\tLoss: 613.9620\n",
      "Training Epoch: 7 [20160/36450]\tLoss: 580.5089\n",
      "Training Epoch: 7 [20224/36450]\tLoss: 591.5336\n",
      "Training Epoch: 7 [20288/36450]\tLoss: 586.5702\n",
      "Training Epoch: 7 [20352/36450]\tLoss: 556.1174\n",
      "Training Epoch: 7 [20416/36450]\tLoss: 597.0686\n",
      "Training Epoch: 7 [20480/36450]\tLoss: 569.9650\n",
      "Training Epoch: 7 [20544/36450]\tLoss: 555.1345\n",
      "Training Epoch: 7 [20608/36450]\tLoss: 573.0898\n",
      "Training Epoch: 7 [20672/36450]\tLoss: 581.1587\n",
      "Training Epoch: 7 [20736/36450]\tLoss: 607.7953\n",
      "Training Epoch: 7 [20800/36450]\tLoss: 623.4332\n",
      "Training Epoch: 7 [20864/36450]\tLoss: 641.4255\n",
      "Training Epoch: 7 [20928/36450]\tLoss: 672.9352\n",
      "Training Epoch: 7 [20992/36450]\tLoss: 677.8326\n",
      "Training Epoch: 7 [21056/36450]\tLoss: 639.7554\n",
      "Training Epoch: 7 [21120/36450]\tLoss: 649.6348\n",
      "Training Epoch: 7 [21184/36450]\tLoss: 627.8192\n",
      "Training Epoch: 7 [21248/36450]\tLoss: 622.2024\n",
      "Training Epoch: 7 [21312/36450]\tLoss: 540.1736\n",
      "Training Epoch: 7 [21376/36450]\tLoss: 567.7944\n",
      "Training Epoch: 7 [21440/36450]\tLoss: 602.8499\n",
      "Training Epoch: 7 [21504/36450]\tLoss: 620.2926\n",
      "Training Epoch: 7 [21568/36450]\tLoss: 585.0033\n",
      "Training Epoch: 7 [21632/36450]\tLoss: 559.9334\n",
      "Training Epoch: 7 [21696/36450]\tLoss: 542.1222\n",
      "Training Epoch: 7 [21760/36450]\tLoss: 599.4198\n",
      "Training Epoch: 7 [21824/36450]\tLoss: 578.2014\n",
      "Training Epoch: 7 [21888/36450]\tLoss: 583.2299\n",
      "Training Epoch: 7 [21952/36450]\tLoss: 574.3365\n",
      "Training Epoch: 7 [22016/36450]\tLoss: 544.0616\n",
      "Training Epoch: 7 [22080/36450]\tLoss: 559.4367\n",
      "Training Epoch: 7 [22144/36450]\tLoss: 549.7299\n",
      "Training Epoch: 7 [22208/36450]\tLoss: 588.2311\n",
      "Training Epoch: 7 [22272/36450]\tLoss: 608.1583\n",
      "Training Epoch: 7 [22336/36450]\tLoss: 565.8683\n",
      "Training Epoch: 7 [22400/36450]\tLoss: 583.1735\n",
      "Training Epoch: 7 [22464/36450]\tLoss: 593.8761\n",
      "Training Epoch: 7 [22528/36450]\tLoss: 572.2981\n",
      "Training Epoch: 7 [22592/36450]\tLoss: 607.5188\n",
      "Training Epoch: 7 [22656/36450]\tLoss: 575.9457\n",
      "Training Epoch: 7 [22720/36450]\tLoss: 569.1898\n",
      "Training Epoch: 7 [22784/36450]\tLoss: 592.2894\n",
      "Training Epoch: 7 [22848/36450]\tLoss: 586.7555\n",
      "Training Epoch: 7 [22912/36450]\tLoss: 581.8510\n",
      "Training Epoch: 7 [22976/36450]\tLoss: 585.3901\n",
      "Training Epoch: 7 [23040/36450]\tLoss: 577.4185\n",
      "Training Epoch: 7 [23104/36450]\tLoss: 587.6796\n",
      "Training Epoch: 7 [23168/36450]\tLoss: 561.7494\n",
      "Training Epoch: 7 [23232/36450]\tLoss: 592.7755\n",
      "Training Epoch: 7 [23296/36450]\tLoss: 561.2501\n",
      "Training Epoch: 7 [23360/36450]\tLoss: 568.9957\n",
      "Training Epoch: 7 [23424/36450]\tLoss: 598.5145\n",
      "Training Epoch: 7 [23488/36450]\tLoss: 567.5708\n",
      "Training Epoch: 7 [23552/36450]\tLoss: 591.4626\n",
      "Training Epoch: 7 [23616/36450]\tLoss: 531.2296\n",
      "Training Epoch: 7 [23680/36450]\tLoss: 588.1755\n",
      "Training Epoch: 7 [23744/36450]\tLoss: 566.8828\n",
      "Training Epoch: 7 [23808/36450]\tLoss: 565.8576\n",
      "Training Epoch: 7 [23872/36450]\tLoss: 574.2449\n",
      "Training Epoch: 7 [23936/36450]\tLoss: 589.6769\n",
      "Training Epoch: 7 [24000/36450]\tLoss: 576.4068\n",
      "Training Epoch: 7 [24064/36450]\tLoss: 571.3973\n",
      "Training Epoch: 7 [24128/36450]\tLoss: 582.2434\n",
      "Training Epoch: 7 [24192/36450]\tLoss: 611.1599\n",
      "Training Epoch: 7 [24256/36450]\tLoss: 599.3173\n",
      "Training Epoch: 7 [24320/36450]\tLoss: 572.8937\n",
      "Training Epoch: 7 [24384/36450]\tLoss: 588.1870\n",
      "Training Epoch: 7 [24448/36450]\tLoss: 553.5557\n",
      "Training Epoch: 7 [24512/36450]\tLoss: 562.5081\n",
      "Training Epoch: 7 [24576/36450]\tLoss: 626.3775\n",
      "Training Epoch: 7 [24640/36450]\tLoss: 555.9052\n",
      "Training Epoch: 7 [24704/36450]\tLoss: 554.6219\n",
      "Training Epoch: 7 [24768/36450]\tLoss: 564.5371\n",
      "Training Epoch: 7 [24832/36450]\tLoss: 592.5046\n",
      "Training Epoch: 7 [24896/36450]\tLoss: 578.9606\n",
      "Training Epoch: 7 [24960/36450]\tLoss: 560.6626\n",
      "Training Epoch: 7 [25024/36450]\tLoss: 598.4679\n",
      "Training Epoch: 7 [25088/36450]\tLoss: 587.8293\n",
      "Training Epoch: 7 [25152/36450]\tLoss: 569.7950\n",
      "Training Epoch: 7 [25216/36450]\tLoss: 571.0729\n",
      "Training Epoch: 7 [25280/36450]\tLoss: 608.8370\n",
      "Training Epoch: 7 [25344/36450]\tLoss: 583.3395\n",
      "Training Epoch: 7 [25408/36450]\tLoss: 564.2401\n",
      "Training Epoch: 7 [25472/36450]\tLoss: 578.0004\n",
      "Training Epoch: 7 [25536/36450]\tLoss: 563.6865\n",
      "Training Epoch: 7 [25600/36450]\tLoss: 557.8782\n",
      "Training Epoch: 7 [25664/36450]\tLoss: 586.2670\n",
      "Training Epoch: 7 [25728/36450]\tLoss: 548.1445\n",
      "Training Epoch: 7 [25792/36450]\tLoss: 543.1629\n",
      "Training Epoch: 7 [25856/36450]\tLoss: 537.5532\n",
      "Training Epoch: 7 [25920/36450]\tLoss: 557.9153\n",
      "Training Epoch: 7 [25984/36450]\tLoss: 603.7655\n",
      "Training Epoch: 7 [26048/36450]\tLoss: 564.3681\n",
      "Training Epoch: 7 [26112/36450]\tLoss: 549.5477\n",
      "Training Epoch: 7 [26176/36450]\tLoss: 593.9999\n",
      "Training Epoch: 7 [26240/36450]\tLoss: 544.6164\n",
      "Training Epoch: 7 [26304/36450]\tLoss: 563.5837\n",
      "Training Epoch: 7 [26368/36450]\tLoss: 584.2458\n",
      "Training Epoch: 7 [26432/36450]\tLoss: 589.9473\n",
      "Training Epoch: 7 [26496/36450]\tLoss: 602.3135\n",
      "Training Epoch: 7 [26560/36450]\tLoss: 584.2410\n",
      "Training Epoch: 7 [26624/36450]\tLoss: 573.2805\n",
      "Training Epoch: 7 [26688/36450]\tLoss: 556.7501\n",
      "Training Epoch: 7 [26752/36450]\tLoss: 586.0340\n",
      "Training Epoch: 7 [26816/36450]\tLoss: 580.4340\n",
      "Training Epoch: 7 [26880/36450]\tLoss: 559.4562\n",
      "Training Epoch: 7 [26944/36450]\tLoss: 569.8594\n",
      "Training Epoch: 7 [27008/36450]\tLoss: 538.9202\n",
      "Training Epoch: 7 [27072/36450]\tLoss: 591.0018\n",
      "Training Epoch: 7 [27136/36450]\tLoss: 562.0132\n",
      "Training Epoch: 7 [27200/36450]\tLoss: 576.8528\n",
      "Training Epoch: 7 [27264/36450]\tLoss: 542.6932\n",
      "Training Epoch: 7 [27328/36450]\tLoss: 559.9720\n",
      "Training Epoch: 7 [27392/36450]\tLoss: 544.1316\n",
      "Training Epoch: 7 [27456/36450]\tLoss: 569.2318\n",
      "Training Epoch: 7 [27520/36450]\tLoss: 553.4034\n",
      "Training Epoch: 7 [27584/36450]\tLoss: 582.1859\n",
      "Training Epoch: 7 [27648/36450]\tLoss: 557.7053\n",
      "Training Epoch: 7 [27712/36450]\tLoss: 559.8754\n",
      "Training Epoch: 7 [27776/36450]\tLoss: 554.0493\n",
      "Training Epoch: 7 [27840/36450]\tLoss: 575.2346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [27904/36450]\tLoss: 562.8594\n",
      "Training Epoch: 7 [27968/36450]\tLoss: 587.6359\n",
      "Training Epoch: 7 [28032/36450]\tLoss: 587.0886\n",
      "Training Epoch: 7 [28096/36450]\tLoss: 577.6056\n",
      "Training Epoch: 7 [28160/36450]\tLoss: 582.7617\n",
      "Training Epoch: 7 [28224/36450]\tLoss: 572.6187\n",
      "Training Epoch: 7 [28288/36450]\tLoss: 597.9944\n",
      "Training Epoch: 7 [28352/36450]\tLoss: 582.3973\n",
      "Training Epoch: 7 [28416/36450]\tLoss: 580.2775\n",
      "Training Epoch: 7 [28480/36450]\tLoss: 601.4733\n",
      "Training Epoch: 7 [28544/36450]\tLoss: 550.2252\n",
      "Training Epoch: 7 [28608/36450]\tLoss: 587.2990\n",
      "Training Epoch: 7 [28672/36450]\tLoss: 560.8412\n",
      "Training Epoch: 7 [28736/36450]\tLoss: 605.2053\n",
      "Training Epoch: 7 [28800/36450]\tLoss: 576.5207\n",
      "Training Epoch: 7 [28864/36450]\tLoss: 549.9603\n",
      "Training Epoch: 7 [28928/36450]\tLoss: 563.6298\n",
      "Training Epoch: 7 [28992/36450]\tLoss: 573.9044\n",
      "Training Epoch: 7 [29056/36450]\tLoss: 552.2852\n",
      "Training Epoch: 7 [29120/36450]\tLoss: 583.1603\n",
      "Training Epoch: 7 [29184/36450]\tLoss: 529.3906\n",
      "Training Epoch: 7 [29248/36450]\tLoss: 550.4593\n",
      "Training Epoch: 7 [29312/36450]\tLoss: 535.2281\n",
      "Training Epoch: 7 [29376/36450]\tLoss: 594.2869\n",
      "Training Epoch: 7 [29440/36450]\tLoss: 554.3038\n",
      "Training Epoch: 7 [29504/36450]\tLoss: 566.2115\n",
      "Training Epoch: 7 [29568/36450]\tLoss: 579.0142\n",
      "Training Epoch: 7 [29632/36450]\tLoss: 557.2898\n",
      "Training Epoch: 7 [29696/36450]\tLoss: 550.1523\n",
      "Training Epoch: 7 [29760/36450]\tLoss: 562.1954\n",
      "Training Epoch: 7 [29824/36450]\tLoss: 552.3238\n",
      "Training Epoch: 7 [29888/36450]\tLoss: 563.4874\n",
      "Training Epoch: 7 [29952/36450]\tLoss: 570.0910\n",
      "Training Epoch: 7 [30016/36450]\tLoss: 573.6669\n",
      "Training Epoch: 7 [30080/36450]\tLoss: 580.7076\n",
      "Training Epoch: 7 [30144/36450]\tLoss: 558.3546\n",
      "Training Epoch: 7 [30208/36450]\tLoss: 576.3540\n",
      "Training Epoch: 7 [30272/36450]\tLoss: 561.0914\n",
      "Training Epoch: 7 [30336/36450]\tLoss: 568.5322\n",
      "Training Epoch: 7 [30400/36450]\tLoss: 548.6112\n",
      "Training Epoch: 7 [30464/36450]\tLoss: 610.7687\n",
      "Training Epoch: 7 [30528/36450]\tLoss: 581.8082\n",
      "Training Epoch: 7 [30592/36450]\tLoss: 584.3302\n",
      "Training Epoch: 7 [30656/36450]\tLoss: 599.6877\n",
      "Training Epoch: 7 [30720/36450]\tLoss: 559.0734\n",
      "Training Epoch: 7 [30784/36450]\tLoss: 528.7699\n",
      "Training Epoch: 7 [30848/36450]\tLoss: 571.4875\n",
      "Training Epoch: 7 [30912/36450]\tLoss: 577.6378\n",
      "Training Epoch: 7 [30976/36450]\tLoss: 559.6165\n",
      "Training Epoch: 7 [31040/36450]\tLoss: 552.5610\n",
      "Training Epoch: 7 [31104/36450]\tLoss: 577.0009\n",
      "Training Epoch: 7 [31168/36450]\tLoss: 585.8556\n",
      "Training Epoch: 7 [31232/36450]\tLoss: 564.2141\n",
      "Training Epoch: 7 [31296/36450]\tLoss: 599.3057\n",
      "Training Epoch: 7 [31360/36450]\tLoss: 551.4287\n",
      "Training Epoch: 7 [31424/36450]\tLoss: 601.0060\n",
      "Training Epoch: 7 [31488/36450]\tLoss: 583.0618\n",
      "Training Epoch: 7 [31552/36450]\tLoss: 541.2638\n",
      "Training Epoch: 7 [31616/36450]\tLoss: 560.0828\n",
      "Training Epoch: 7 [31680/36450]\tLoss: 556.5240\n",
      "Training Epoch: 7 [31744/36450]\tLoss: 582.8922\n",
      "Training Epoch: 7 [31808/36450]\tLoss: 548.2614\n",
      "Training Epoch: 7 [31872/36450]\tLoss: 528.9875\n",
      "Training Epoch: 7 [31936/36450]\tLoss: 560.1703\n",
      "Training Epoch: 7 [32000/36450]\tLoss: 570.1188\n",
      "Training Epoch: 7 [32064/36450]\tLoss: 566.1417\n",
      "Training Epoch: 7 [32128/36450]\tLoss: 570.8572\n",
      "Training Epoch: 7 [32192/36450]\tLoss: 583.2049\n",
      "Training Epoch: 7 [32256/36450]\tLoss: 553.2100\n",
      "Training Epoch: 7 [32320/36450]\tLoss: 578.7475\n",
      "Training Epoch: 7 [32384/36450]\tLoss: 557.7693\n",
      "Training Epoch: 7 [32448/36450]\tLoss: 562.4703\n",
      "Training Epoch: 7 [32512/36450]\tLoss: 629.2881\n",
      "Training Epoch: 7 [32576/36450]\tLoss: 533.1990\n",
      "Training Epoch: 7 [32640/36450]\tLoss: 566.6024\n",
      "Training Epoch: 7 [32704/36450]\tLoss: 552.3949\n",
      "Training Epoch: 7 [32768/36450]\tLoss: 598.9532\n",
      "Training Epoch: 7 [32832/36450]\tLoss: 540.7335\n",
      "Training Epoch: 7 [32896/36450]\tLoss: 584.4402\n",
      "Training Epoch: 7 [32960/36450]\tLoss: 598.9348\n",
      "Training Epoch: 7 [33024/36450]\tLoss: 541.9390\n",
      "Training Epoch: 7 [33088/36450]\tLoss: 587.3708\n",
      "Training Epoch: 7 [33152/36450]\tLoss: 588.8915\n",
      "Training Epoch: 7 [33216/36450]\tLoss: 530.2909\n",
      "Training Epoch: 7 [33280/36450]\tLoss: 588.3052\n",
      "Training Epoch: 7 [33344/36450]\tLoss: 564.0107\n",
      "Training Epoch: 7 [33408/36450]\tLoss: 568.0221\n",
      "Training Epoch: 7 [33472/36450]\tLoss: 566.2460\n",
      "Training Epoch: 7 [33536/36450]\tLoss: 575.0359\n",
      "Training Epoch: 7 [33600/36450]\tLoss: 587.4008\n",
      "Training Epoch: 7 [33664/36450]\tLoss: 561.3845\n",
      "Training Epoch: 7 [33728/36450]\tLoss: 566.4354\n",
      "Training Epoch: 7 [33792/36450]\tLoss: 561.4361\n",
      "Training Epoch: 7 [33856/36450]\tLoss: 597.2593\n",
      "Training Epoch: 7 [33920/36450]\tLoss: 554.9631\n",
      "Training Epoch: 7 [33984/36450]\tLoss: 536.8036\n",
      "Training Epoch: 7 [34048/36450]\tLoss: 575.3796\n",
      "Training Epoch: 7 [34112/36450]\tLoss: 568.0272\n",
      "Training Epoch: 7 [34176/36450]\tLoss: 561.0470\n",
      "Training Epoch: 7 [34240/36450]\tLoss: 560.9117\n",
      "Training Epoch: 7 [34304/36450]\tLoss: 597.2289\n",
      "Training Epoch: 7 [34368/36450]\tLoss: 597.5083\n",
      "Training Epoch: 7 [34432/36450]\tLoss: 561.1959\n",
      "Training Epoch: 7 [34496/36450]\tLoss: 554.8497\n",
      "Training Epoch: 7 [34560/36450]\tLoss: 511.1793\n",
      "Training Epoch: 7 [34624/36450]\tLoss: 538.0486\n",
      "Training Epoch: 7 [34688/36450]\tLoss: 583.0065\n",
      "Training Epoch: 7 [34752/36450]\tLoss: 584.8318\n",
      "Training Epoch: 7 [34816/36450]\tLoss: 544.8823\n",
      "Training Epoch: 7 [34880/36450]\tLoss: 576.1987\n",
      "Training Epoch: 7 [34944/36450]\tLoss: 566.4289\n",
      "Training Epoch: 7 [35008/36450]\tLoss: 538.9011\n",
      "Training Epoch: 7 [35072/36450]\tLoss: 557.8696\n",
      "Training Epoch: 7 [35136/36450]\tLoss: 550.7260\n",
      "Training Epoch: 7 [35200/36450]\tLoss: 564.2651\n",
      "Training Epoch: 7 [35264/36450]\tLoss: 604.3528\n",
      "Training Epoch: 7 [35328/36450]\tLoss: 585.1670\n",
      "Training Epoch: 7 [35392/36450]\tLoss: 567.2346\n",
      "Training Epoch: 7 [35456/36450]\tLoss: 568.5578\n",
      "Training Epoch: 7 [35520/36450]\tLoss: 565.8778\n",
      "Training Epoch: 7 [35584/36450]\tLoss: 574.0001\n",
      "Training Epoch: 7 [35648/36450]\tLoss: 575.5115\n",
      "Training Epoch: 7 [35712/36450]\tLoss: 547.7759\n",
      "Training Epoch: 7 [35776/36450]\tLoss: 587.2194\n",
      "Training Epoch: 7 [35840/36450]\tLoss: 571.6574\n",
      "Training Epoch: 7 [35904/36450]\tLoss: 578.5681\n",
      "Training Epoch: 7 [35968/36450]\tLoss: 596.5128\n",
      "Training Epoch: 7 [36032/36450]\tLoss: 557.9342\n",
      "Training Epoch: 7 [36096/36450]\tLoss: 594.2147\n",
      "Training Epoch: 7 [36160/36450]\tLoss: 563.0750\n",
      "Training Epoch: 7 [36224/36450]\tLoss: 591.0377\n",
      "Training Epoch: 7 [36288/36450]\tLoss: 587.3679\n",
      "Training Epoch: 7 [36352/36450]\tLoss: 597.5794\n",
      "Training Epoch: 7 [36416/36450]\tLoss: 604.6019\n",
      "Training Epoch: 7 [36450/36450]\tLoss: 573.4084\n",
      "Training Epoch: 7 [4050/4050]\tLoss: 285.4596\n",
      "Training Epoch: 8 [64/36450]\tLoss: 593.5049\n",
      "Training Epoch: 8 [128/36450]\tLoss: 585.3843\n",
      "Training Epoch: 8 [192/36450]\tLoss: 558.7927\n",
      "Training Epoch: 8 [256/36450]\tLoss: 576.3226\n",
      "Training Epoch: 8 [320/36450]\tLoss: 555.2745\n",
      "Training Epoch: 8 [384/36450]\tLoss: 565.6510\n",
      "Training Epoch: 8 [448/36450]\tLoss: 579.7615\n",
      "Training Epoch: 8 [512/36450]\tLoss: 579.3360\n",
      "Training Epoch: 8 [576/36450]\tLoss: 532.5365\n",
      "Training Epoch: 8 [640/36450]\tLoss: 576.8376\n",
      "Training Epoch: 8 [704/36450]\tLoss: 582.1204\n",
      "Training Epoch: 8 [768/36450]\tLoss: 562.4457\n",
      "Training Epoch: 8 [832/36450]\tLoss: 584.3066\n",
      "Training Epoch: 8 [896/36450]\tLoss: 543.1262\n",
      "Training Epoch: 8 [960/36450]\tLoss: 591.0562\n",
      "Training Epoch: 8 [1024/36450]\tLoss: 536.6633\n",
      "Training Epoch: 8 [1088/36450]\tLoss: 541.5375\n",
      "Training Epoch: 8 [1152/36450]\tLoss: 546.0849\n",
      "Training Epoch: 8 [1216/36450]\tLoss: 561.4009\n",
      "Training Epoch: 8 [1280/36450]\tLoss: 573.5903\n",
      "Training Epoch: 8 [1344/36450]\tLoss: 559.8565\n",
      "Training Epoch: 8 [1408/36450]\tLoss: 543.2759\n",
      "Training Epoch: 8 [1472/36450]\tLoss: 560.9016\n",
      "Training Epoch: 8 [1536/36450]\tLoss: 571.2686\n",
      "Training Epoch: 8 [1600/36450]\tLoss: 554.6234\n",
      "Training Epoch: 8 [1664/36450]\tLoss: 533.8613\n",
      "Training Epoch: 8 [1728/36450]\tLoss: 591.9998\n",
      "Training Epoch: 8 [1792/36450]\tLoss: 576.7600\n",
      "Training Epoch: 8 [1856/36450]\tLoss: 620.7539\n",
      "Training Epoch: 8 [1920/36450]\tLoss: 589.8038\n",
      "Training Epoch: 8 [1984/36450]\tLoss: 555.3585\n",
      "Training Epoch: 8 [2048/36450]\tLoss: 564.8079\n",
      "Training Epoch: 8 [2112/36450]\tLoss: 554.4325\n",
      "Training Epoch: 8 [2176/36450]\tLoss: 559.3417\n",
      "Training Epoch: 8 [2240/36450]\tLoss: 633.6992\n",
      "Training Epoch: 8 [2304/36450]\tLoss: 585.6381\n",
      "Training Epoch: 8 [2368/36450]\tLoss: 601.5706\n",
      "Training Epoch: 8 [2432/36450]\tLoss: 558.1251\n",
      "Training Epoch: 8 [2496/36450]\tLoss: 561.4876\n",
      "Training Epoch: 8 [2560/36450]\tLoss: 575.5260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [2624/36450]\tLoss: 603.0389\n",
      "Training Epoch: 8 [2688/36450]\tLoss: 560.5827\n",
      "Training Epoch: 8 [2752/36450]\tLoss: 572.1390\n",
      "Training Epoch: 8 [2816/36450]\tLoss: 607.3296\n",
      "Training Epoch: 8 [2880/36450]\tLoss: 582.1835\n",
      "Training Epoch: 8 [2944/36450]\tLoss: 570.3568\n",
      "Training Epoch: 8 [3008/36450]\tLoss: 603.1226\n",
      "Training Epoch: 8 [3072/36450]\tLoss: 552.2198\n",
      "Training Epoch: 8 [3136/36450]\tLoss: 520.4093\n",
      "Training Epoch: 8 [3200/36450]\tLoss: 560.5157\n",
      "Training Epoch: 8 [3264/36450]\tLoss: 571.5488\n",
      "Training Epoch: 8 [3328/36450]\tLoss: 562.7562\n",
      "Training Epoch: 8 [3392/36450]\tLoss: 555.6326\n",
      "Training Epoch: 8 [3456/36450]\tLoss: 591.3999\n",
      "Training Epoch: 8 [3520/36450]\tLoss: 563.5737\n",
      "Training Epoch: 8 [3584/36450]\tLoss: 570.8292\n",
      "Training Epoch: 8 [3648/36450]\tLoss: 569.2386\n",
      "Training Epoch: 8 [3712/36450]\tLoss: 559.0415\n",
      "Training Epoch: 8 [3776/36450]\tLoss: 611.1311\n",
      "Training Epoch: 8 [3840/36450]\tLoss: 608.5081\n",
      "Training Epoch: 8 [3904/36450]\tLoss: 578.9781\n",
      "Training Epoch: 8 [3968/36450]\tLoss: 576.0414\n",
      "Training Epoch: 8 [4032/36450]\tLoss: 590.9023\n",
      "Training Epoch: 8 [4096/36450]\tLoss: 589.2520\n",
      "Training Epoch: 8 [4160/36450]\tLoss: 585.3651\n",
      "Training Epoch: 8 [4224/36450]\tLoss: 579.5535\n",
      "Training Epoch: 8 [4288/36450]\tLoss: 565.1711\n",
      "Training Epoch: 8 [4352/36450]\tLoss: 572.9269\n",
      "Training Epoch: 8 [4416/36450]\tLoss: 589.5975\n",
      "Training Epoch: 8 [4480/36450]\tLoss: 552.6025\n",
      "Training Epoch: 8 [4544/36450]\tLoss: 555.8070\n",
      "Training Epoch: 8 [4608/36450]\tLoss: 543.7813\n",
      "Training Epoch: 8 [4672/36450]\tLoss: 573.2076\n",
      "Training Epoch: 8 [4736/36450]\tLoss: 551.9333\n",
      "Training Epoch: 8 [4800/36450]\tLoss: 565.4572\n",
      "Training Epoch: 8 [4864/36450]\tLoss: 550.3604\n",
      "Training Epoch: 8 [4928/36450]\tLoss: 576.7578\n",
      "Training Epoch: 8 [4992/36450]\tLoss: 563.1687\n",
      "Training Epoch: 8 [5056/36450]\tLoss: 583.4025\n",
      "Training Epoch: 8 [5120/36450]\tLoss: 550.8209\n",
      "Training Epoch: 8 [5184/36450]\tLoss: 580.0785\n",
      "Training Epoch: 8 [5248/36450]\tLoss: 543.1814\n",
      "Training Epoch: 8 [5312/36450]\tLoss: 549.2498\n",
      "Training Epoch: 8 [5376/36450]\tLoss: 586.8843\n",
      "Training Epoch: 8 [5440/36450]\tLoss: 554.2468\n",
      "Training Epoch: 8 [5504/36450]\tLoss: 568.9174\n",
      "Training Epoch: 8 [5568/36450]\tLoss: 554.1465\n",
      "Training Epoch: 8 [5632/36450]\tLoss: 557.5195\n",
      "Training Epoch: 8 [5696/36450]\tLoss: 569.2162\n",
      "Training Epoch: 8 [5760/36450]\tLoss: 564.7943\n",
      "Training Epoch: 8 [5824/36450]\tLoss: 545.0322\n",
      "Training Epoch: 8 [5888/36450]\tLoss: 579.1088\n",
      "Training Epoch: 8 [5952/36450]\tLoss: 534.4733\n",
      "Training Epoch: 8 [6016/36450]\tLoss: 567.8768\n",
      "Training Epoch: 8 [6080/36450]\tLoss: 539.7528\n",
      "Training Epoch: 8 [6144/36450]\tLoss: 534.6048\n",
      "Training Epoch: 8 [6208/36450]\tLoss: 591.3402\n",
      "Training Epoch: 8 [6272/36450]\tLoss: 567.0580\n",
      "Training Epoch: 8 [6336/36450]\tLoss: 599.6891\n",
      "Training Epoch: 8 [6400/36450]\tLoss: 555.2394\n",
      "Training Epoch: 8 [6464/36450]\tLoss: 546.6848\n",
      "Training Epoch: 8 [6528/36450]\tLoss: 562.2844\n",
      "Training Epoch: 8 [6592/36450]\tLoss: 594.9521\n",
      "Training Epoch: 8 [6656/36450]\tLoss: 540.3318\n",
      "Training Epoch: 8 [6720/36450]\tLoss: 553.4959\n",
      "Training Epoch: 8 [6784/36450]\tLoss: 565.6589\n",
      "Training Epoch: 8 [6848/36450]\tLoss: 584.8239\n",
      "Training Epoch: 8 [6912/36450]\tLoss: 579.5526\n",
      "Training Epoch: 8 [6976/36450]\tLoss: 550.1496\n",
      "Training Epoch: 8 [7040/36450]\tLoss: 543.6262\n",
      "Training Epoch: 8 [7104/36450]\tLoss: 566.9294\n",
      "Training Epoch: 8 [7168/36450]\tLoss: 564.8865\n",
      "Training Epoch: 8 [7232/36450]\tLoss: 565.9120\n",
      "Training Epoch: 8 [7296/36450]\tLoss: 568.6295\n",
      "Training Epoch: 8 [7360/36450]\tLoss: 530.9659\n",
      "Training Epoch: 8 [7424/36450]\tLoss: 595.2775\n",
      "Training Epoch: 8 [7488/36450]\tLoss: 583.1078\n",
      "Training Epoch: 8 [7552/36450]\tLoss: 569.6518\n",
      "Training Epoch: 8 [7616/36450]\tLoss: 558.8063\n",
      "Training Epoch: 8 [7680/36450]\tLoss: 549.9210\n",
      "Training Epoch: 8 [7744/36450]\tLoss: 577.2156\n",
      "Training Epoch: 8 [7808/36450]\tLoss: 540.8615\n",
      "Training Epoch: 8 [7872/36450]\tLoss: 528.8086\n",
      "Training Epoch: 8 [7936/36450]\tLoss: 565.7342\n",
      "Training Epoch: 8 [8000/36450]\tLoss: 545.2403\n",
      "Training Epoch: 8 [8064/36450]\tLoss: 558.7020\n",
      "Training Epoch: 8 [8128/36450]\tLoss: 540.6769\n",
      "Training Epoch: 8 [8192/36450]\tLoss: 555.2128\n",
      "Training Epoch: 8 [8256/36450]\tLoss: 572.5439\n",
      "Training Epoch: 8 [8320/36450]\tLoss: 563.0472\n",
      "Training Epoch: 8 [8384/36450]\tLoss: 586.8696\n",
      "Training Epoch: 8 [8448/36450]\tLoss: 559.8780\n",
      "Training Epoch: 8 [8512/36450]\tLoss: 581.4968\n",
      "Training Epoch: 8 [8576/36450]\tLoss: 565.4933\n",
      "Training Epoch: 8 [8640/36450]\tLoss: 582.1923\n",
      "Training Epoch: 8 [8704/36450]\tLoss: 579.7394\n",
      "Training Epoch: 8 [8768/36450]\tLoss: 574.7045\n",
      "Training Epoch: 8 [8832/36450]\tLoss: 558.7704\n",
      "Training Epoch: 8 [8896/36450]\tLoss: 596.9512\n",
      "Training Epoch: 8 [8960/36450]\tLoss: 576.0290\n",
      "Training Epoch: 8 [9024/36450]\tLoss: 538.0569\n",
      "Training Epoch: 8 [9088/36450]\tLoss: 538.2651\n",
      "Training Epoch: 8 [9152/36450]\tLoss: 546.5944\n",
      "Training Epoch: 8 [9216/36450]\tLoss: 567.5135\n",
      "Training Epoch: 8 [9280/36450]\tLoss: 545.1738\n",
      "Training Epoch: 8 [9344/36450]\tLoss: 572.4451\n",
      "Training Epoch: 8 [9408/36450]\tLoss: 577.7137\n",
      "Training Epoch: 8 [9472/36450]\tLoss: 594.2239\n",
      "Training Epoch: 8 [9536/36450]\tLoss: 572.3981\n",
      "Training Epoch: 8 [9600/36450]\tLoss: 540.8500\n",
      "Training Epoch: 8 [9664/36450]\tLoss: 578.2111\n",
      "Training Epoch: 8 [9728/36450]\tLoss: 542.6697\n",
      "Training Epoch: 8 [9792/36450]\tLoss: 549.0806\n",
      "Training Epoch: 8 [9856/36450]\tLoss: 564.6971\n",
      "Training Epoch: 8 [9920/36450]\tLoss: 546.8542\n",
      "Training Epoch: 8 [9984/36450]\tLoss: 556.1741\n",
      "Training Epoch: 8 [10048/36450]\tLoss: 559.5730\n",
      "Training Epoch: 8 [10112/36450]\tLoss: 603.7678\n",
      "Training Epoch: 8 [10176/36450]\tLoss: 534.6871\n",
      "Training Epoch: 8 [10240/36450]\tLoss: 587.3717\n",
      "Training Epoch: 8 [10304/36450]\tLoss: 572.4587\n",
      "Training Epoch: 8 [10368/36450]\tLoss: 589.9824\n",
      "Training Epoch: 8 [10432/36450]\tLoss: 586.6513\n",
      "Training Epoch: 8 [10496/36450]\tLoss: 608.8459\n",
      "Training Epoch: 8 [10560/36450]\tLoss: 591.9337\n",
      "Training Epoch: 8 [10624/36450]\tLoss: 572.3727\n",
      "Training Epoch: 8 [10688/36450]\tLoss: 588.5143\n",
      "Training Epoch: 8 [10752/36450]\tLoss: 554.4964\n",
      "Training Epoch: 8 [10816/36450]\tLoss: 538.9806\n",
      "Training Epoch: 8 [10880/36450]\tLoss: 566.0405\n",
      "Training Epoch: 8 [10944/36450]\tLoss: 556.0057\n",
      "Training Epoch: 8 [11008/36450]\tLoss: 558.2493\n",
      "Training Epoch: 8 [11072/36450]\tLoss: 546.8004\n",
      "Training Epoch: 8 [11136/36450]\tLoss: 586.3602\n",
      "Training Epoch: 8 [11200/36450]\tLoss: 592.9113\n",
      "Training Epoch: 8 [11264/36450]\tLoss: 582.2216\n",
      "Training Epoch: 8 [11328/36450]\tLoss: 581.9459\n",
      "Training Epoch: 8 [11392/36450]\tLoss: 604.2122\n",
      "Training Epoch: 8 [11456/36450]\tLoss: 586.3918\n",
      "Training Epoch: 8 [11520/36450]\tLoss: 589.9710\n",
      "Training Epoch: 8 [11584/36450]\tLoss: 568.9201\n",
      "Training Epoch: 8 [11648/36450]\tLoss: 546.9333\n",
      "Training Epoch: 8 [11712/36450]\tLoss: 627.4363\n",
      "Training Epoch: 8 [11776/36450]\tLoss: 564.9067\n",
      "Training Epoch: 8 [11840/36450]\tLoss: 564.3164\n",
      "Training Epoch: 8 [11904/36450]\tLoss: 539.1339\n",
      "Training Epoch: 8 [11968/36450]\tLoss: 520.3737\n",
      "Training Epoch: 8 [12032/36450]\tLoss: 557.8808\n",
      "Training Epoch: 8 [12096/36450]\tLoss: 567.3564\n",
      "Training Epoch: 8 [12160/36450]\tLoss: 544.0391\n",
      "Training Epoch: 8 [12224/36450]\tLoss: 544.7337\n",
      "Training Epoch: 8 [12288/36450]\tLoss: 557.7347\n",
      "Training Epoch: 8 [12352/36450]\tLoss: 548.1069\n",
      "Training Epoch: 8 [12416/36450]\tLoss: 579.6618\n",
      "Training Epoch: 8 [12480/36450]\tLoss: 541.4235\n",
      "Training Epoch: 8 [12544/36450]\tLoss: 603.8494\n",
      "Training Epoch: 8 [12608/36450]\tLoss: 551.7320\n",
      "Training Epoch: 8 [12672/36450]\tLoss: 573.2686\n",
      "Training Epoch: 8 [12736/36450]\tLoss: 544.2571\n",
      "Training Epoch: 8 [12800/36450]\tLoss: 565.4759\n",
      "Training Epoch: 8 [12864/36450]\tLoss: 584.0273\n",
      "Training Epoch: 8 [12928/36450]\tLoss: 559.5511\n",
      "Training Epoch: 8 [12992/36450]\tLoss: 593.5433\n",
      "Training Epoch: 8 [13056/36450]\tLoss: 585.7261\n",
      "Training Epoch: 8 [13120/36450]\tLoss: 584.1273\n",
      "Training Epoch: 8 [13184/36450]\tLoss: 555.4040\n",
      "Training Epoch: 8 [13248/36450]\tLoss: 609.1841\n",
      "Training Epoch: 8 [13312/36450]\tLoss: 585.0190\n",
      "Training Epoch: 8 [13376/36450]\tLoss: 600.6471\n",
      "Training Epoch: 8 [13440/36450]\tLoss: 588.6278\n",
      "Training Epoch: 8 [13504/36450]\tLoss: 591.2249\n",
      "Training Epoch: 8 [13568/36450]\tLoss: 577.9608\n",
      "Training Epoch: 8 [13632/36450]\tLoss: 557.7245\n",
      "Training Epoch: 8 [13696/36450]\tLoss: 534.7313\n",
      "Training Epoch: 8 [13760/36450]\tLoss: 572.3254\n",
      "Training Epoch: 8 [13824/36450]\tLoss: 557.2701\n",
      "Training Epoch: 8 [13888/36450]\tLoss: 562.8030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [13952/36450]\tLoss: 567.7460\n",
      "Training Epoch: 8 [14016/36450]\tLoss: 549.9363\n",
      "Training Epoch: 8 [14080/36450]\tLoss: 569.9234\n",
      "Training Epoch: 8 [14144/36450]\tLoss: 576.4060\n",
      "Training Epoch: 8 [14208/36450]\tLoss: 565.5589\n",
      "Training Epoch: 8 [14272/36450]\tLoss: 535.4775\n",
      "Training Epoch: 8 [14336/36450]\tLoss: 541.3865\n",
      "Training Epoch: 8 [14400/36450]\tLoss: 567.0445\n",
      "Training Epoch: 8 [14464/36450]\tLoss: 570.1138\n",
      "Training Epoch: 8 [14528/36450]\tLoss: 588.0417\n",
      "Training Epoch: 8 [14592/36450]\tLoss: 560.4061\n",
      "Training Epoch: 8 [14656/36450]\tLoss: 551.7359\n",
      "Training Epoch: 8 [14720/36450]\tLoss: 581.3962\n",
      "Training Epoch: 8 [14784/36450]\tLoss: 575.6716\n",
      "Training Epoch: 8 [14848/36450]\tLoss: 535.1639\n",
      "Training Epoch: 8 [14912/36450]\tLoss: 565.0201\n",
      "Training Epoch: 8 [14976/36450]\tLoss: 557.2458\n",
      "Training Epoch: 8 [15040/36450]\tLoss: 569.9182\n",
      "Training Epoch: 8 [15104/36450]\tLoss: 593.6103\n",
      "Training Epoch: 8 [15168/36450]\tLoss: 559.9744\n",
      "Training Epoch: 8 [15232/36450]\tLoss: 542.1074\n",
      "Training Epoch: 8 [15296/36450]\tLoss: 548.2896\n",
      "Training Epoch: 8 [15360/36450]\tLoss: 554.0120\n",
      "Training Epoch: 8 [15424/36450]\tLoss: 581.3820\n",
      "Training Epoch: 8 [15488/36450]\tLoss: 568.5278\n",
      "Training Epoch: 8 [15552/36450]\tLoss: 597.7775\n",
      "Training Epoch: 8 [15616/36450]\tLoss: 585.4199\n",
      "Training Epoch: 8 [15680/36450]\tLoss: 558.1699\n",
      "Training Epoch: 8 [15744/36450]\tLoss: 566.4905\n",
      "Training Epoch: 8 [15808/36450]\tLoss: 606.4380\n",
      "Training Epoch: 8 [15872/36450]\tLoss: 582.3592\n",
      "Training Epoch: 8 [15936/36450]\tLoss: 540.6502\n",
      "Training Epoch: 8 [16000/36450]\tLoss: 557.7886\n",
      "Training Epoch: 8 [16064/36450]\tLoss: 580.6647\n",
      "Training Epoch: 8 [16128/36450]\tLoss: 556.7102\n",
      "Training Epoch: 8 [16192/36450]\tLoss: 568.3797\n",
      "Training Epoch: 8 [16256/36450]\tLoss: 555.7398\n",
      "Training Epoch: 8 [16320/36450]\tLoss: 564.8557\n",
      "Training Epoch: 8 [16384/36450]\tLoss: 601.1371\n",
      "Training Epoch: 8 [16448/36450]\tLoss: 593.2322\n",
      "Training Epoch: 8 [16512/36450]\tLoss: 559.9376\n",
      "Training Epoch: 8 [16576/36450]\tLoss: 550.7270\n",
      "Training Epoch: 8 [16640/36450]\tLoss: 543.2532\n",
      "Training Epoch: 8 [16704/36450]\tLoss: 543.2350\n",
      "Training Epoch: 8 [16768/36450]\tLoss: 563.4301\n",
      "Training Epoch: 8 [16832/36450]\tLoss: 560.8206\n",
      "Training Epoch: 8 [16896/36450]\tLoss: 548.9214\n",
      "Training Epoch: 8 [16960/36450]\tLoss: 553.4069\n",
      "Training Epoch: 8 [17024/36450]\tLoss: 540.2767\n",
      "Training Epoch: 8 [17088/36450]\tLoss: 571.2838\n",
      "Training Epoch: 8 [17152/36450]\tLoss: 554.3482\n",
      "Training Epoch: 8 [17216/36450]\tLoss: 573.8712\n",
      "Training Epoch: 8 [17280/36450]\tLoss: 565.6983\n",
      "Training Epoch: 8 [17344/36450]\tLoss: 570.6329\n",
      "Training Epoch: 8 [17408/36450]\tLoss: 574.7085\n",
      "Training Epoch: 8 [17472/36450]\tLoss: 572.4637\n",
      "Training Epoch: 8 [17536/36450]\tLoss: 570.4059\n",
      "Training Epoch: 8 [17600/36450]\tLoss: 560.8549\n",
      "Training Epoch: 8 [17664/36450]\tLoss: 572.2797\n",
      "Training Epoch: 8 [17728/36450]\tLoss: 575.2449\n",
      "Training Epoch: 8 [17792/36450]\tLoss: 543.2393\n",
      "Training Epoch: 8 [17856/36450]\tLoss: 605.4053\n",
      "Training Epoch: 8 [17920/36450]\tLoss: 543.1767\n",
      "Training Epoch: 8 [17984/36450]\tLoss: 573.7518\n",
      "Training Epoch: 8 [18048/36450]\tLoss: 538.3282\n",
      "Training Epoch: 8 [18112/36450]\tLoss: 571.3133\n",
      "Training Epoch: 8 [18176/36450]\tLoss: 575.6443\n",
      "Training Epoch: 8 [18240/36450]\tLoss: 529.8186\n",
      "Training Epoch: 8 [18304/36450]\tLoss: 576.8514\n",
      "Training Epoch: 8 [18368/36450]\tLoss: 535.6020\n",
      "Training Epoch: 8 [18432/36450]\tLoss: 563.2844\n",
      "Training Epoch: 8 [18496/36450]\tLoss: 558.8929\n",
      "Training Epoch: 8 [18560/36450]\tLoss: 549.4387\n",
      "Training Epoch: 8 [18624/36450]\tLoss: 557.5943\n",
      "Training Epoch: 8 [18688/36450]\tLoss: 576.3918\n",
      "Training Epoch: 8 [18752/36450]\tLoss: 576.1609\n",
      "Training Epoch: 8 [18816/36450]\tLoss: 574.9061\n",
      "Training Epoch: 8 [18880/36450]\tLoss: 546.1939\n",
      "Training Epoch: 8 [18944/36450]\tLoss: 583.1458\n",
      "Training Epoch: 8 [19008/36450]\tLoss: 579.8674\n",
      "Training Epoch: 8 [19072/36450]\tLoss: 565.7937\n",
      "Training Epoch: 8 [19136/36450]\tLoss: 556.6401\n",
      "Training Epoch: 8 [19200/36450]\tLoss: 545.5735\n",
      "Training Epoch: 8 [19264/36450]\tLoss: 558.0856\n",
      "Training Epoch: 8 [19328/36450]\tLoss: 607.3779\n",
      "Training Epoch: 8 [19392/36450]\tLoss: 573.7593\n",
      "Training Epoch: 8 [19456/36450]\tLoss: 556.4938\n",
      "Training Epoch: 8 [19520/36450]\tLoss: 559.6362\n",
      "Training Epoch: 8 [19584/36450]\tLoss: 604.4118\n",
      "Training Epoch: 8 [19648/36450]\tLoss: 569.6240\n",
      "Training Epoch: 8 [19712/36450]\tLoss: 581.1347\n",
      "Training Epoch: 8 [19776/36450]\tLoss: 559.9073\n",
      "Training Epoch: 8 [19840/36450]\tLoss: 547.7107\n",
      "Training Epoch: 8 [19904/36450]\tLoss: 573.3946\n",
      "Training Epoch: 8 [19968/36450]\tLoss: 553.9172\n",
      "Training Epoch: 8 [20032/36450]\tLoss: 590.8957\n",
      "Training Epoch: 8 [20096/36450]\tLoss: 583.5369\n",
      "Training Epoch: 8 [20160/36450]\tLoss: 595.8053\n",
      "Training Epoch: 8 [20224/36450]\tLoss: 586.7979\n",
      "Training Epoch: 8 [20288/36450]\tLoss: 614.8339\n",
      "Training Epoch: 8 [20352/36450]\tLoss: 590.4128\n",
      "Training Epoch: 8 [20416/36450]\tLoss: 564.2750\n",
      "Training Epoch: 8 [20480/36450]\tLoss: 530.2747\n",
      "Training Epoch: 8 [20544/36450]\tLoss: 559.7869\n",
      "Training Epoch: 8 [20608/36450]\tLoss: 570.8173\n",
      "Training Epoch: 8 [20672/36450]\tLoss: 551.4105\n",
      "Training Epoch: 8 [20736/36450]\tLoss: 591.4788\n",
      "Training Epoch: 8 [20800/36450]\tLoss: 576.7058\n",
      "Training Epoch: 8 [20864/36450]\tLoss: 586.3214\n",
      "Training Epoch: 8 [20928/36450]\tLoss: 579.8997\n",
      "Training Epoch: 8 [20992/36450]\tLoss: 547.7158\n",
      "Training Epoch: 8 [21056/36450]\tLoss: 583.4515\n",
      "Training Epoch: 8 [21120/36450]\tLoss: 547.8429\n",
      "Training Epoch: 8 [21184/36450]\tLoss: 531.9034\n",
      "Training Epoch: 8 [21248/36450]\tLoss: 530.0938\n",
      "Training Epoch: 8 [21312/36450]\tLoss: 562.1130\n",
      "Training Epoch: 8 [21376/36450]\tLoss: 564.7382\n",
      "Training Epoch: 8 [21440/36450]\tLoss: 573.4698\n",
      "Training Epoch: 8 [21504/36450]\tLoss: 532.4030\n",
      "Training Epoch: 8 [21568/36450]\tLoss: 560.2754\n",
      "Training Epoch: 8 [21632/36450]\tLoss: 565.2225\n",
      "Training Epoch: 8 [21696/36450]\tLoss: 555.0959\n",
      "Training Epoch: 8 [21760/36450]\tLoss: 563.7399\n",
      "Training Epoch: 8 [21824/36450]\tLoss: 557.4764\n",
      "Training Epoch: 8 [21888/36450]\tLoss: 560.8706\n",
      "Training Epoch: 8 [21952/36450]\tLoss: 559.0644\n",
      "Training Epoch: 8 [22016/36450]\tLoss: 574.0017\n",
      "Training Epoch: 8 [22080/36450]\tLoss: 547.6488\n",
      "Training Epoch: 8 [22144/36450]\tLoss: 563.9384\n",
      "Training Epoch: 8 [22208/36450]\tLoss: 551.4951\n",
      "Training Epoch: 8 [22272/36450]\tLoss: 525.3062\n",
      "Training Epoch: 8 [22336/36450]\tLoss: 553.1708\n",
      "Training Epoch: 8 [22400/36450]\tLoss: 588.4730\n",
      "Training Epoch: 8 [22464/36450]\tLoss: 565.0944\n",
      "Training Epoch: 8 [22528/36450]\tLoss: 586.6404\n",
      "Training Epoch: 8 [22592/36450]\tLoss: 567.9721\n",
      "Training Epoch: 8 [22656/36450]\tLoss: 567.6328\n",
      "Training Epoch: 8 [22720/36450]\tLoss: 550.1226\n",
      "Training Epoch: 8 [22784/36450]\tLoss: 536.0778\n",
      "Training Epoch: 8 [22848/36450]\tLoss: 563.5045\n",
      "Training Epoch: 8 [22912/36450]\tLoss: 559.6393\n",
      "Training Epoch: 8 [22976/36450]\tLoss: 542.8047\n",
      "Training Epoch: 8 [23040/36450]\tLoss: 562.1493\n",
      "Training Epoch: 8 [23104/36450]\tLoss: 552.7069\n",
      "Training Epoch: 8 [23168/36450]\tLoss: 566.5019\n",
      "Training Epoch: 8 [23232/36450]\tLoss: 567.8044\n",
      "Training Epoch: 8 [23296/36450]\tLoss: 579.6888\n",
      "Training Epoch: 8 [23360/36450]\tLoss: 555.0699\n",
      "Training Epoch: 8 [23424/36450]\tLoss: 570.8591\n",
      "Training Epoch: 8 [23488/36450]\tLoss: 556.0775\n",
      "Training Epoch: 8 [23552/36450]\tLoss: 563.8539\n",
      "Training Epoch: 8 [23616/36450]\tLoss: 506.4744\n",
      "Training Epoch: 8 [23680/36450]\tLoss: 571.7794\n",
      "Training Epoch: 8 [23744/36450]\tLoss: 539.6694\n",
      "Training Epoch: 8 [23808/36450]\tLoss: 547.9136\n",
      "Training Epoch: 8 [23872/36450]\tLoss: 580.8204\n",
      "Training Epoch: 8 [23936/36450]\tLoss: 555.7938\n",
      "Training Epoch: 8 [24000/36450]\tLoss: 579.9689\n",
      "Training Epoch: 8 [24064/36450]\tLoss: 536.4028\n",
      "Training Epoch: 8 [24128/36450]\tLoss: 577.5717\n",
      "Training Epoch: 8 [24192/36450]\tLoss: 574.2443\n",
      "Training Epoch: 8 [24256/36450]\tLoss: 521.3481\n",
      "Training Epoch: 8 [24320/36450]\tLoss: 540.7925\n",
      "Training Epoch: 8 [24384/36450]\tLoss: 569.2308\n",
      "Training Epoch: 8 [24448/36450]\tLoss: 570.3082\n",
      "Training Epoch: 8 [24512/36450]\tLoss: 573.9755\n",
      "Training Epoch: 8 [24576/36450]\tLoss: 551.6992\n",
      "Training Epoch: 8 [24640/36450]\tLoss: 585.8394\n",
      "Training Epoch: 8 [24704/36450]\tLoss: 562.6459\n",
      "Training Epoch: 8 [24768/36450]\tLoss: 556.2937\n",
      "Training Epoch: 8 [24832/36450]\tLoss: 585.7117\n",
      "Training Epoch: 8 [24896/36450]\tLoss: 533.6989\n",
      "Training Epoch: 8 [24960/36450]\tLoss: 564.9278\n",
      "Training Epoch: 8 [25024/36450]\tLoss: 539.4384\n",
      "Training Epoch: 8 [25088/36450]\tLoss: 572.0575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [25152/36450]\tLoss: 574.4111\n",
      "Training Epoch: 8 [25216/36450]\tLoss: 542.0392\n",
      "Training Epoch: 8 [25280/36450]\tLoss: 566.0746\n",
      "Training Epoch: 8 [25344/36450]\tLoss: 552.6320\n",
      "Training Epoch: 8 [25408/36450]\tLoss: 545.0105\n",
      "Training Epoch: 8 [25472/36450]\tLoss: 542.8281\n",
      "Training Epoch: 8 [25536/36450]\tLoss: 554.8738\n",
      "Training Epoch: 8 [25600/36450]\tLoss: 538.5986\n",
      "Training Epoch: 8 [25664/36450]\tLoss: 573.7923\n",
      "Training Epoch: 8 [25728/36450]\tLoss: 555.2621\n",
      "Training Epoch: 8 [25792/36450]\tLoss: 599.3578\n",
      "Training Epoch: 8 [25856/36450]\tLoss: 558.9714\n",
      "Training Epoch: 8 [25920/36450]\tLoss: 562.2272\n",
      "Training Epoch: 8 [25984/36450]\tLoss: 573.3697\n",
      "Training Epoch: 8 [26048/36450]\tLoss: 596.0425\n",
      "Training Epoch: 8 [26112/36450]\tLoss: 559.4691\n",
      "Training Epoch: 8 [26176/36450]\tLoss: 569.4995\n",
      "Training Epoch: 8 [26240/36450]\tLoss: 562.9153\n",
      "Training Epoch: 8 [26304/36450]\tLoss: 530.6552\n",
      "Training Epoch: 8 [26368/36450]\tLoss: 574.8830\n",
      "Training Epoch: 8 [26432/36450]\tLoss: 549.9463\n",
      "Training Epoch: 8 [26496/36450]\tLoss: 582.7651\n",
      "Training Epoch: 8 [26560/36450]\tLoss: 539.1244\n",
      "Training Epoch: 8 [26624/36450]\tLoss: 544.3488\n",
      "Training Epoch: 8 [26688/36450]\tLoss: 579.1428\n",
      "Training Epoch: 8 [26752/36450]\tLoss: 599.0769\n",
      "Training Epoch: 8 [26816/36450]\tLoss: 551.2137\n",
      "Training Epoch: 8 [26880/36450]\tLoss: 556.2887\n",
      "Training Epoch: 8 [26944/36450]\tLoss: 570.9894\n",
      "Training Epoch: 8 [27008/36450]\tLoss: 573.8177\n",
      "Training Epoch: 8 [27072/36450]\tLoss: 599.2834\n",
      "Training Epoch: 8 [27136/36450]\tLoss: 582.2515\n",
      "Training Epoch: 8 [27200/36450]\tLoss: 589.0140\n",
      "Training Epoch: 8 [27264/36450]\tLoss: 593.8540\n",
      "Training Epoch: 8 [27328/36450]\tLoss: 593.3812\n",
      "Training Epoch: 8 [27392/36450]\tLoss: 587.7836\n",
      "Training Epoch: 8 [27456/36450]\tLoss: 587.5881\n",
      "Training Epoch: 8 [27520/36450]\tLoss: 551.7115\n",
      "Training Epoch: 8 [27584/36450]\tLoss: 565.0482\n",
      "Training Epoch: 8 [27648/36450]\tLoss: 571.8997\n",
      "Training Epoch: 8 [27712/36450]\tLoss: 595.1278\n",
      "Training Epoch: 8 [27776/36450]\tLoss: 605.8129\n",
      "Training Epoch: 8 [27840/36450]\tLoss: 620.3170\n",
      "Training Epoch: 8 [27904/36450]\tLoss: 583.0065\n",
      "Training Epoch: 8 [27968/36450]\tLoss: 606.4712\n",
      "Training Epoch: 8 [28032/36450]\tLoss: 569.5445\n",
      "Training Epoch: 8 [28096/36450]\tLoss: 553.8782\n",
      "Training Epoch: 8 [28160/36450]\tLoss: 566.4245\n",
      "Training Epoch: 8 [28224/36450]\tLoss: 578.2590\n",
      "Training Epoch: 8 [28288/36450]\tLoss: 566.0568\n",
      "Training Epoch: 8 [28352/36450]\tLoss: 582.8431\n",
      "Training Epoch: 8 [28416/36450]\tLoss: 584.0844\n",
      "Training Epoch: 8 [28480/36450]\tLoss: 552.4190\n",
      "Training Epoch: 8 [28544/36450]\tLoss: 594.7062\n",
      "Training Epoch: 8 [28608/36450]\tLoss: 546.3936\n",
      "Training Epoch: 8 [28672/36450]\tLoss: 531.5473\n",
      "Training Epoch: 8 [28736/36450]\tLoss: 553.0402\n",
      "Training Epoch: 8 [28800/36450]\tLoss: 557.2308\n",
      "Training Epoch: 8 [28864/36450]\tLoss: 571.4137\n",
      "Training Epoch: 8 [28928/36450]\tLoss: 552.8341\n",
      "Training Epoch: 8 [28992/36450]\tLoss: 558.0312\n",
      "Training Epoch: 8 [29056/36450]\tLoss: 545.8600\n",
      "Training Epoch: 8 [29120/36450]\tLoss: 565.8800\n",
      "Training Epoch: 8 [29184/36450]\tLoss: 556.5198\n",
      "Training Epoch: 8 [29248/36450]\tLoss: 509.2513\n",
      "Training Epoch: 8 [29312/36450]\tLoss: 555.9146\n",
      "Training Epoch: 8 [29376/36450]\tLoss: 547.1184\n",
      "Training Epoch: 8 [29440/36450]\tLoss: 548.3931\n",
      "Training Epoch: 8 [29504/36450]\tLoss: 539.8908\n",
      "Training Epoch: 8 [29568/36450]\tLoss: 570.2309\n",
      "Training Epoch: 8 [29632/36450]\tLoss: 557.5699\n",
      "Training Epoch: 8 [29696/36450]\tLoss: 528.8530\n",
      "Training Epoch: 8 [29760/36450]\tLoss: 581.9081\n",
      "Training Epoch: 8 [29824/36450]\tLoss: 562.2389\n",
      "Training Epoch: 8 [29888/36450]\tLoss: 537.0942\n",
      "Training Epoch: 8 [29952/36450]\tLoss: 571.4011\n",
      "Training Epoch: 8 [30016/36450]\tLoss: 542.1252\n",
      "Training Epoch: 8 [30080/36450]\tLoss: 535.2845\n",
      "Training Epoch: 8 [30144/36450]\tLoss: 581.0261\n",
      "Training Epoch: 8 [30208/36450]\tLoss: 549.0305\n",
      "Training Epoch: 8 [30272/36450]\tLoss: 531.0771\n",
      "Training Epoch: 8 [30336/36450]\tLoss: 584.7439\n",
      "Training Epoch: 8 [30400/36450]\tLoss: 525.1396\n",
      "Training Epoch: 8 [30464/36450]\tLoss: 584.1039\n",
      "Training Epoch: 8 [30528/36450]\tLoss: 529.6105\n",
      "Training Epoch: 8 [30592/36450]\tLoss: 532.0237\n",
      "Training Epoch: 8 [30656/36450]\tLoss: 571.9735\n",
      "Training Epoch: 8 [30720/36450]\tLoss: 572.2510\n",
      "Training Epoch: 8 [30784/36450]\tLoss: 553.3646\n",
      "Training Epoch: 8 [30848/36450]\tLoss: 527.2434\n",
      "Training Epoch: 8 [30912/36450]\tLoss: 550.0704\n",
      "Training Epoch: 8 [30976/36450]\tLoss: 540.4338\n",
      "Training Epoch: 8 [31040/36450]\tLoss: 578.4994\n",
      "Training Epoch: 8 [31104/36450]\tLoss: 535.3188\n",
      "Training Epoch: 8 [31168/36450]\tLoss: 542.7071\n",
      "Training Epoch: 8 [31232/36450]\tLoss: 557.4507\n",
      "Training Epoch: 8 [31296/36450]\tLoss: 555.8617\n",
      "Training Epoch: 8 [31360/36450]\tLoss: 555.7399\n",
      "Training Epoch: 8 [31424/36450]\tLoss: 537.4409\n",
      "Training Epoch: 8 [31488/36450]\tLoss: 546.4728\n",
      "Training Epoch: 8 [31552/36450]\tLoss: 556.4792\n",
      "Training Epoch: 8 [31616/36450]\tLoss: 555.5574\n",
      "Training Epoch: 8 [31680/36450]\tLoss: 575.2288\n",
      "Training Epoch: 8 [31744/36450]\tLoss: 573.0239\n",
      "Training Epoch: 8 [31808/36450]\tLoss: 543.3151\n",
      "Training Epoch: 8 [31872/36450]\tLoss: 571.6053\n",
      "Training Epoch: 8 [31936/36450]\tLoss: 553.6718\n",
      "Training Epoch: 8 [32000/36450]\tLoss: 552.9849\n",
      "Training Epoch: 8 [32064/36450]\tLoss: 540.3763\n",
      "Training Epoch: 8 [32128/36450]\tLoss: 551.1810\n",
      "Training Epoch: 8 [32192/36450]\tLoss: 559.1930\n",
      "Training Epoch: 8 [32256/36450]\tLoss: 561.5453\n",
      "Training Epoch: 8 [32320/36450]\tLoss: 557.2155\n",
      "Training Epoch: 8 [32384/36450]\tLoss: 560.0524\n",
      "Training Epoch: 8 [32448/36450]\tLoss: 552.4796\n",
      "Training Epoch: 8 [32512/36450]\tLoss: 548.9891\n",
      "Training Epoch: 8 [32576/36450]\tLoss: 560.1717\n",
      "Training Epoch: 8 [32640/36450]\tLoss: 551.2610\n",
      "Training Epoch: 8 [32704/36450]\tLoss: 558.9486\n",
      "Training Epoch: 8 [32768/36450]\tLoss: 524.6849\n",
      "Training Epoch: 8 [32832/36450]\tLoss: 552.9435\n",
      "Training Epoch: 8 [32896/36450]\tLoss: 543.9579\n",
      "Training Epoch: 8 [32960/36450]\tLoss: 591.4859\n",
      "Training Epoch: 8 [33024/36450]\tLoss: 525.3392\n",
      "Training Epoch: 8 [33088/36450]\tLoss: 575.1228\n",
      "Training Epoch: 8 [33152/36450]\tLoss: 548.2786\n",
      "Training Epoch: 8 [33216/36450]\tLoss: 565.3895\n",
      "Training Epoch: 8 [33280/36450]\tLoss: 565.0004\n",
      "Training Epoch: 8 [33344/36450]\tLoss: 528.9255\n",
      "Training Epoch: 8 [33408/36450]\tLoss: 567.5811\n",
      "Training Epoch: 8 [33472/36450]\tLoss: 531.2805\n",
      "Training Epoch: 8 [33536/36450]\tLoss: 515.6753\n",
      "Training Epoch: 8 [33600/36450]\tLoss: 536.2692\n",
      "Training Epoch: 8 [33664/36450]\tLoss: 551.9677\n",
      "Training Epoch: 8 [33728/36450]\tLoss: 541.4364\n",
      "Training Epoch: 8 [33792/36450]\tLoss: 590.8810\n",
      "Training Epoch: 8 [33856/36450]\tLoss: 532.0358\n",
      "Training Epoch: 8 [33920/36450]\tLoss: 567.6677\n",
      "Training Epoch: 8 [33984/36450]\tLoss: 534.1431\n",
      "Training Epoch: 8 [34048/36450]\tLoss: 563.2183\n",
      "Training Epoch: 8 [34112/36450]\tLoss: 557.5521\n",
      "Training Epoch: 8 [34176/36450]\tLoss: 564.2966\n",
      "Training Epoch: 8 [34240/36450]\tLoss: 573.7797\n",
      "Training Epoch: 8 [34304/36450]\tLoss: 554.7786\n",
      "Training Epoch: 8 [34368/36450]\tLoss: 538.2933\n",
      "Training Epoch: 8 [34432/36450]\tLoss: 531.3580\n",
      "Training Epoch: 8 [34496/36450]\tLoss: 560.2921\n",
      "Training Epoch: 8 [34560/36450]\tLoss: 508.2173\n",
      "Training Epoch: 8 [34624/36450]\tLoss: 521.2582\n",
      "Training Epoch: 8 [34688/36450]\tLoss: 554.3716\n",
      "Training Epoch: 8 [34752/36450]\tLoss: 543.9481\n",
      "Training Epoch: 8 [34816/36450]\tLoss: 553.5654\n",
      "Training Epoch: 8 [34880/36450]\tLoss: 530.4095\n",
      "Training Epoch: 8 [34944/36450]\tLoss: 580.1276\n",
      "Training Epoch: 8 [35008/36450]\tLoss: 538.9620\n",
      "Training Epoch: 8 [35072/36450]\tLoss: 550.0001\n",
      "Training Epoch: 8 [35136/36450]\tLoss: 524.4457\n",
      "Training Epoch: 8 [35200/36450]\tLoss: 520.8805\n",
      "Training Epoch: 8 [35264/36450]\tLoss: 580.6126\n",
      "Training Epoch: 8 [35328/36450]\tLoss: 510.8030\n",
      "Training Epoch: 8 [35392/36450]\tLoss: 556.4854\n",
      "Training Epoch: 8 [35456/36450]\tLoss: 540.2340\n",
      "Training Epoch: 8 [35520/36450]\tLoss: 591.2145\n",
      "Training Epoch: 8 [35584/36450]\tLoss: 576.1423\n",
      "Training Epoch: 8 [35648/36450]\tLoss: 528.6599\n",
      "Training Epoch: 8 [35712/36450]\tLoss: 579.7208\n",
      "Training Epoch: 8 [35776/36450]\tLoss: 582.4794\n",
      "Training Epoch: 8 [35840/36450]\tLoss: 574.4201\n",
      "Training Epoch: 8 [35904/36450]\tLoss: 564.7301\n",
      "Training Epoch: 8 [35968/36450]\tLoss: 559.3915\n",
      "Training Epoch: 8 [36032/36450]\tLoss: 583.7872\n",
      "Training Epoch: 8 [36096/36450]\tLoss: 580.7681\n",
      "Training Epoch: 8 [36160/36450]\tLoss: 561.2131\n",
      "Training Epoch: 8 [36224/36450]\tLoss: 607.8640\n",
      "Training Epoch: 8 [36288/36450]\tLoss: 559.3311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [36352/36450]\tLoss: 568.3715\n",
      "Training Epoch: 8 [36416/36450]\tLoss: 508.9892\n",
      "Training Epoch: 8 [36450/36450]\tLoss: 582.3647\n",
      "Training Epoch: 8 [4050/4050]\tLoss: 274.5372\n",
      "Training Epoch: 9 [64/36450]\tLoss: 544.2316\n",
      "Training Epoch: 9 [128/36450]\tLoss: 543.9411\n",
      "Training Epoch: 9 [192/36450]\tLoss: 528.6543\n",
      "Training Epoch: 9 [256/36450]\tLoss: 519.9037\n",
      "Training Epoch: 9 [320/36450]\tLoss: 562.3889\n",
      "Training Epoch: 9 [384/36450]\tLoss: 561.3828\n",
      "Training Epoch: 9 [448/36450]\tLoss: 546.2360\n",
      "Training Epoch: 9 [512/36450]\tLoss: 570.2645\n",
      "Training Epoch: 9 [576/36450]\tLoss: 582.1391\n",
      "Training Epoch: 9 [640/36450]\tLoss: 562.0790\n",
      "Training Epoch: 9 [704/36450]\tLoss: 574.8071\n",
      "Training Epoch: 9 [768/36450]\tLoss: 544.7357\n",
      "Training Epoch: 9 [832/36450]\tLoss: 586.3102\n",
      "Training Epoch: 9 [896/36450]\tLoss: 555.1448\n",
      "Training Epoch: 9 [960/36450]\tLoss: 555.0444\n",
      "Training Epoch: 9 [1024/36450]\tLoss: 548.0054\n",
      "Training Epoch: 9 [1088/36450]\tLoss: 557.1757\n",
      "Training Epoch: 9 [1152/36450]\tLoss: 577.4407\n",
      "Training Epoch: 9 [1216/36450]\tLoss: 539.2162\n",
      "Training Epoch: 9 [1280/36450]\tLoss: 567.2595\n",
      "Training Epoch: 9 [1344/36450]\tLoss: 557.7978\n",
      "Training Epoch: 9 [1408/36450]\tLoss: 544.7106\n",
      "Training Epoch: 9 [1472/36450]\tLoss: 578.9156\n",
      "Training Epoch: 9 [1536/36450]\tLoss: 588.0643\n",
      "Training Epoch: 9 [1600/36450]\tLoss: 553.9540\n",
      "Training Epoch: 9 [1664/36450]\tLoss: 535.9720\n",
      "Training Epoch: 9 [1728/36450]\tLoss: 551.5560\n",
      "Training Epoch: 9 [1792/36450]\tLoss: 533.1694\n",
      "Training Epoch: 9 [1856/36450]\tLoss: 532.1649\n",
      "Training Epoch: 9 [1920/36450]\tLoss: 518.9907\n",
      "Training Epoch: 9 [1984/36450]\tLoss: 581.6175\n",
      "Training Epoch: 9 [2048/36450]\tLoss: 575.1428\n",
      "Training Epoch: 9 [2112/36450]\tLoss: 592.6818\n",
      "Training Epoch: 9 [2176/36450]\tLoss: 571.4688\n",
      "Training Epoch: 9 [2240/36450]\tLoss: 553.6500\n",
      "Training Epoch: 9 [2304/36450]\tLoss: 566.3358\n",
      "Training Epoch: 9 [2368/36450]\tLoss: 576.6099\n",
      "Training Epoch: 9 [2432/36450]\tLoss: 550.7336\n",
      "Training Epoch: 9 [2496/36450]\tLoss: 555.6237\n",
      "Training Epoch: 9 [2560/36450]\tLoss: 541.2167\n",
      "Training Epoch: 9 [2624/36450]\tLoss: 568.9137\n",
      "Training Epoch: 9 [2688/36450]\tLoss: 563.9824\n",
      "Training Epoch: 9 [2752/36450]\tLoss: 557.4801\n",
      "Training Epoch: 9 [2816/36450]\tLoss: 601.2666\n",
      "Training Epoch: 9 [2880/36450]\tLoss: 537.8945\n",
      "Training Epoch: 9 [2944/36450]\tLoss: 518.2677\n",
      "Training Epoch: 9 [3008/36450]\tLoss: 536.0308\n",
      "Training Epoch: 9 [3072/36450]\tLoss: 562.5752\n",
      "Training Epoch: 9 [3136/36450]\tLoss: 557.2003\n",
      "Training Epoch: 9 [3200/36450]\tLoss: 590.1393\n",
      "Training Epoch: 9 [3264/36450]\tLoss: 540.2226\n",
      "Training Epoch: 9 [3328/36450]\tLoss: 579.2905\n",
      "Training Epoch: 9 [3392/36450]\tLoss: 494.8429\n",
      "Training Epoch: 9 [3456/36450]\tLoss: 562.4162\n",
      "Training Epoch: 9 [3520/36450]\tLoss: 554.7868\n",
      "Training Epoch: 9 [3584/36450]\tLoss: 535.3066\n",
      "Training Epoch: 9 [3648/36450]\tLoss: 558.2462\n",
      "Training Epoch: 9 [3712/36450]\tLoss: 573.5338\n",
      "Training Epoch: 9 [3776/36450]\tLoss: 585.8950\n",
      "Training Epoch: 9 [3840/36450]\tLoss: 551.5680\n",
      "Training Epoch: 9 [3904/36450]\tLoss: 539.5417\n",
      "Training Epoch: 9 [3968/36450]\tLoss: 568.6858\n",
      "Training Epoch: 9 [4032/36450]\tLoss: 554.5665\n",
      "Training Epoch: 9 [4096/36450]\tLoss: 554.8730\n",
      "Training Epoch: 9 [4160/36450]\tLoss: 519.9245\n",
      "Training Epoch: 9 [4224/36450]\tLoss: 573.6791\n",
      "Training Epoch: 9 [4288/36450]\tLoss: 555.1380\n",
      "Training Epoch: 9 [4352/36450]\tLoss: 533.6041\n",
      "Training Epoch: 9 [4416/36450]\tLoss: 561.8727\n",
      "Training Epoch: 9 [4480/36450]\tLoss: 534.7665\n",
      "Training Epoch: 9 [4544/36450]\tLoss: 560.5541\n",
      "Training Epoch: 9 [4608/36450]\tLoss: 560.1679\n",
      "Training Epoch: 9 [4672/36450]\tLoss: 554.5205\n",
      "Training Epoch: 9 [4736/36450]\tLoss: 563.6262\n",
      "Training Epoch: 9 [4800/36450]\tLoss: 563.2791\n",
      "Training Epoch: 9 [4864/36450]\tLoss: 547.7483\n",
      "Training Epoch: 9 [4928/36450]\tLoss: 560.7779\n",
      "Training Epoch: 9 [4992/36450]\tLoss: 519.9701\n",
      "Training Epoch: 9 [5056/36450]\tLoss: 560.4133\n",
      "Training Epoch: 9 [5120/36450]\tLoss: 555.4677\n",
      "Training Epoch: 9 [5184/36450]\tLoss: 600.0199\n",
      "Training Epoch: 9 [5248/36450]\tLoss: 579.3880\n",
      "Training Epoch: 9 [5312/36450]\tLoss: 579.8242\n",
      "Training Epoch: 9 [5376/36450]\tLoss: 575.3365\n",
      "Training Epoch: 9 [5440/36450]\tLoss: 534.1265\n",
      "Training Epoch: 9 [5504/36450]\tLoss: 553.1104\n",
      "Training Epoch: 9 [5568/36450]\tLoss: 552.7131\n",
      "Training Epoch: 9 [5632/36450]\tLoss: 526.3267\n",
      "Training Epoch: 9 [5696/36450]\tLoss: 584.7424\n",
      "Training Epoch: 9 [5760/36450]\tLoss: 607.5528\n",
      "Training Epoch: 9 [5824/36450]\tLoss: 584.3689\n",
      "Training Epoch: 9 [5888/36450]\tLoss: 599.8770\n",
      "Training Epoch: 9 [5952/36450]\tLoss: 610.8462\n",
      "Training Epoch: 9 [6016/36450]\tLoss: 639.0378\n",
      "Training Epoch: 9 [6080/36450]\tLoss: 625.2521\n",
      "Training Epoch: 9 [6144/36450]\tLoss: 601.8109\n",
      "Training Epoch: 9 [6208/36450]\tLoss: 572.5354\n",
      "Training Epoch: 9 [6272/36450]\tLoss: 571.1351\n",
      "Training Epoch: 9 [6336/36450]\tLoss: 588.0991\n",
      "Training Epoch: 9 [6400/36450]\tLoss: 548.9929\n",
      "Training Epoch: 9 [6464/36450]\tLoss: 608.2700\n",
      "Training Epoch: 9 [6528/36450]\tLoss: 623.7283\n",
      "Training Epoch: 9 [6592/36450]\tLoss: 579.1239\n",
      "Training Epoch: 9 [6656/36450]\tLoss: 586.3425\n",
      "Training Epoch: 9 [6720/36450]\tLoss: 584.8065\n",
      "Training Epoch: 9 [6784/36450]\tLoss: 522.2554\n",
      "Training Epoch: 9 [6848/36450]\tLoss: 576.5980\n",
      "Training Epoch: 9 [6912/36450]\tLoss: 572.0684\n",
      "Training Epoch: 9 [6976/36450]\tLoss: 564.7342\n",
      "Training Epoch: 9 [7040/36450]\tLoss: 536.7935\n",
      "Training Epoch: 9 [7104/36450]\tLoss: 589.5001\n",
      "Training Epoch: 9 [7168/36450]\tLoss: 577.6589\n",
      "Training Epoch: 9 [7232/36450]\tLoss: 530.7594\n",
      "Training Epoch: 9 [7296/36450]\tLoss: 543.4143\n",
      "Training Epoch: 9 [7360/36450]\tLoss: 546.5400\n",
      "Training Epoch: 9 [7424/36450]\tLoss: 545.2967\n",
      "Training Epoch: 9 [7488/36450]\tLoss: 565.3956\n",
      "Training Epoch: 9 [7552/36450]\tLoss: 548.3828\n",
      "Training Epoch: 9 [7616/36450]\tLoss: 575.7828\n",
      "Training Epoch: 9 [7680/36450]\tLoss: 524.6191\n",
      "Training Epoch: 9 [7744/36450]\tLoss: 525.5218\n",
      "Training Epoch: 9 [7808/36450]\tLoss: 550.5178\n",
      "Training Epoch: 9 [7872/36450]\tLoss: 548.2210\n",
      "Training Epoch: 9 [7936/36450]\tLoss: 541.5035\n",
      "Training Epoch: 9 [8000/36450]\tLoss: 517.9316\n",
      "Training Epoch: 9 [8064/36450]\tLoss: 535.1242\n",
      "Training Epoch: 9 [8128/36450]\tLoss: 561.5184\n",
      "Training Epoch: 9 [8192/36450]\tLoss: 549.0971\n",
      "Training Epoch: 9 [8256/36450]\tLoss: 541.8340\n",
      "Training Epoch: 9 [8320/36450]\tLoss: 591.1893\n",
      "Training Epoch: 9 [8384/36450]\tLoss: 573.4101\n",
      "Training Epoch: 9 [8448/36450]\tLoss: 554.7378\n",
      "Training Epoch: 9 [8512/36450]\tLoss: 550.5172\n",
      "Training Epoch: 9 [8576/36450]\tLoss: 525.0447\n",
      "Training Epoch: 9 [8640/36450]\tLoss: 525.4509\n",
      "Training Epoch: 9 [8704/36450]\tLoss: 562.7242\n",
      "Training Epoch: 9 [8768/36450]\tLoss: 536.8332\n",
      "Training Epoch: 9 [8832/36450]\tLoss: 580.9194\n",
      "Training Epoch: 9 [8896/36450]\tLoss: 555.3729\n",
      "Training Epoch: 9 [8960/36450]\tLoss: 525.6196\n",
      "Training Epoch: 9 [9024/36450]\tLoss: 537.0078\n",
      "Training Epoch: 9 [9088/36450]\tLoss: 547.5271\n",
      "Training Epoch: 9 [9152/36450]\tLoss: 553.2776\n",
      "Training Epoch: 9 [9216/36450]\tLoss: 569.0770\n",
      "Training Epoch: 9 [9280/36450]\tLoss: 546.4772\n",
      "Training Epoch: 9 [9344/36450]\tLoss: 540.1657\n",
      "Training Epoch: 9 [9408/36450]\tLoss: 553.0594\n",
      "Training Epoch: 9 [9472/36450]\tLoss: 527.4508\n",
      "Training Epoch: 9 [9536/36450]\tLoss: 582.5852\n",
      "Training Epoch: 9 [9600/36450]\tLoss: 531.4648\n",
      "Training Epoch: 9 [9664/36450]\tLoss: 528.8102\n",
      "Training Epoch: 9 [9728/36450]\tLoss: 532.4394\n",
      "Training Epoch: 9 [9792/36450]\tLoss: 538.2773\n",
      "Training Epoch: 9 [9856/36450]\tLoss: 563.4226\n",
      "Training Epoch: 9 [9920/36450]\tLoss: 525.3600\n",
      "Training Epoch: 9 [9984/36450]\tLoss: 538.5262\n",
      "Training Epoch: 9 [10048/36450]\tLoss: 566.0634\n",
      "Training Epoch: 9 [10112/36450]\tLoss: 527.8938\n",
      "Training Epoch: 9 [10176/36450]\tLoss: 552.8186\n",
      "Training Epoch: 9 [10240/36450]\tLoss: 535.6033\n",
      "Training Epoch: 9 [10304/36450]\tLoss: 580.2573\n",
      "Training Epoch: 9 [10368/36450]\tLoss: 526.0895\n",
      "Training Epoch: 9 [10432/36450]\tLoss: 545.7103\n",
      "Training Epoch: 9 [10496/36450]\tLoss: 551.2373\n",
      "Training Epoch: 9 [10560/36450]\tLoss: 568.0430\n",
      "Training Epoch: 9 [10624/36450]\tLoss: 595.7054\n",
      "Training Epoch: 9 [10688/36450]\tLoss: 552.4610\n",
      "Training Epoch: 9 [10752/36450]\tLoss: 505.3463\n",
      "Training Epoch: 9 [10816/36450]\tLoss: 529.4938\n",
      "Training Epoch: 9 [10880/36450]\tLoss: 518.9686\n",
      "Training Epoch: 9 [10944/36450]\tLoss: 565.0859\n",
      "Training Epoch: 9 [11008/36450]\tLoss: 524.9512\n",
      "Training Epoch: 9 [11072/36450]\tLoss: 506.4923\n",
      "Training Epoch: 9 [11136/36450]\tLoss: 549.7172\n",
      "Training Epoch: 9 [11200/36450]\tLoss: 573.0337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [11264/36450]\tLoss: 560.1090\n",
      "Training Epoch: 9 [11328/36450]\tLoss: 555.1329\n",
      "Training Epoch: 9 [11392/36450]\tLoss: 544.4927\n",
      "Training Epoch: 9 [11456/36450]\tLoss: 550.9434\n",
      "Training Epoch: 9 [11520/36450]\tLoss: 569.1373\n",
      "Training Epoch: 9 [11584/36450]\tLoss: 537.4070\n",
      "Training Epoch: 9 [11648/36450]\tLoss: 540.4705\n",
      "Training Epoch: 9 [11712/36450]\tLoss: 513.7610\n",
      "Training Epoch: 9 [11776/36450]\tLoss: 529.1057\n",
      "Training Epoch: 9 [11840/36450]\tLoss: 548.3585\n",
      "Training Epoch: 9 [11904/36450]\tLoss: 566.0560\n",
      "Training Epoch: 9 [11968/36450]\tLoss: 540.1876\n",
      "Training Epoch: 9 [12032/36450]\tLoss: 554.8121\n",
      "Training Epoch: 9 [12096/36450]\tLoss: 551.1423\n",
      "Training Epoch: 9 [12160/36450]\tLoss: 532.8480\n",
      "Training Epoch: 9 [12224/36450]\tLoss: 550.0693\n",
      "Training Epoch: 9 [12288/36450]\tLoss: 551.7879\n",
      "Training Epoch: 9 [12352/36450]\tLoss: 546.0418\n",
      "Training Epoch: 9 [12416/36450]\tLoss: 531.8690\n",
      "Training Epoch: 9 [12480/36450]\tLoss: 549.3506\n",
      "Training Epoch: 9 [12544/36450]\tLoss: 549.9305\n",
      "Training Epoch: 9 [12608/36450]\tLoss: 569.6610\n",
      "Training Epoch: 9 [12672/36450]\tLoss: 553.2527\n",
      "Training Epoch: 9 [12736/36450]\tLoss: 575.3288\n",
      "Training Epoch: 9 [12800/36450]\tLoss: 567.0862\n",
      "Training Epoch: 9 [12864/36450]\tLoss: 527.1922\n",
      "Training Epoch: 9 [12928/36450]\tLoss: 550.4200\n",
      "Training Epoch: 9 [12992/36450]\tLoss: 568.9019\n",
      "Training Epoch: 9 [13056/36450]\tLoss: 548.9189\n",
      "Training Epoch: 9 [13120/36450]\tLoss: 576.6143\n",
      "Training Epoch: 9 [13184/36450]\tLoss: 497.9190\n",
      "Training Epoch: 9 [13248/36450]\tLoss: 523.5709\n",
      "Training Epoch: 9 [13312/36450]\tLoss: 539.5519\n",
      "Training Epoch: 9 [13376/36450]\tLoss: 542.2855\n",
      "Training Epoch: 9 [13440/36450]\tLoss: 544.4820\n",
      "Training Epoch: 9 [13504/36450]\tLoss: 530.6429\n",
      "Training Epoch: 9 [13568/36450]\tLoss: 556.4471\n",
      "Training Epoch: 9 [13632/36450]\tLoss: 554.2894\n",
      "Training Epoch: 9 [13696/36450]\tLoss: 496.6859\n",
      "Training Epoch: 9 [13760/36450]\tLoss: 541.2585\n",
      "Training Epoch: 9 [13824/36450]\tLoss: 522.7684\n",
      "Training Epoch: 9 [13888/36450]\tLoss: 576.1229\n",
      "Training Epoch: 9 [13952/36450]\tLoss: 510.7032\n",
      "Training Epoch: 9 [14016/36450]\tLoss: 541.6682\n",
      "Training Epoch: 9 [14080/36450]\tLoss: 548.8766\n",
      "Training Epoch: 9 [14144/36450]\tLoss: 520.7516\n",
      "Training Epoch: 9 [14208/36450]\tLoss: 593.6005\n",
      "Training Epoch: 9 [14272/36450]\tLoss: 539.1791\n",
      "Training Epoch: 9 [14336/36450]\tLoss: 563.9067\n",
      "Training Epoch: 9 [14400/36450]\tLoss: 548.6025\n",
      "Training Epoch: 9 [14464/36450]\tLoss: 540.9238\n",
      "Training Epoch: 9 [14528/36450]\tLoss: 570.5665\n",
      "Training Epoch: 9 [14592/36450]\tLoss: 546.5842\n",
      "Training Epoch: 9 [14656/36450]\tLoss: 568.7749\n",
      "Training Epoch: 9 [14720/36450]\tLoss: 544.4717\n",
      "Training Epoch: 9 [14784/36450]\tLoss: 571.9399\n",
      "Training Epoch: 9 [14848/36450]\tLoss: 529.5198\n",
      "Training Epoch: 9 [14912/36450]\tLoss: 554.4155\n",
      "Training Epoch: 9 [14976/36450]\tLoss: 522.5048\n",
      "Training Epoch: 9 [15040/36450]\tLoss: 533.5557\n",
      "Training Epoch: 9 [15104/36450]\tLoss: 530.5792\n",
      "Training Epoch: 9 [15168/36450]\tLoss: 558.1944\n",
      "Training Epoch: 9 [15232/36450]\tLoss: 506.0871\n",
      "Training Epoch: 9 [15296/36450]\tLoss: 558.5624\n",
      "Training Epoch: 9 [15360/36450]\tLoss: 571.6141\n",
      "Training Epoch: 9 [15424/36450]\tLoss: 592.4075\n",
      "Training Epoch: 9 [15488/36450]\tLoss: 567.0432\n",
      "Training Epoch: 9 [15552/36450]\tLoss: 584.6680\n",
      "Training Epoch: 9 [15616/36450]\tLoss: 563.2407\n",
      "Training Epoch: 9 [15680/36450]\tLoss: 555.6819\n",
      "Training Epoch: 9 [15744/36450]\tLoss: 542.2903\n",
      "Training Epoch: 9 [15808/36450]\tLoss: 574.3218\n",
      "Training Epoch: 9 [15872/36450]\tLoss: 538.6232\n",
      "Training Epoch: 9 [15936/36450]\tLoss: 543.3677\n",
      "Training Epoch: 9 [16000/36450]\tLoss: 530.0550\n",
      "Training Epoch: 9 [16064/36450]\tLoss: 504.8806\n",
      "Training Epoch: 9 [16128/36450]\tLoss: 521.4797\n",
      "Training Epoch: 9 [16192/36450]\tLoss: 576.7543\n",
      "Training Epoch: 9 [16256/36450]\tLoss: 532.3001\n",
      "Training Epoch: 9 [16320/36450]\tLoss: 543.0048\n",
      "Training Epoch: 9 [16384/36450]\tLoss: 549.9068\n",
      "Training Epoch: 9 [16448/36450]\tLoss: 579.6308\n",
      "Training Epoch: 9 [16512/36450]\tLoss: 532.0280\n",
      "Training Epoch: 9 [16576/36450]\tLoss: 538.6429\n",
      "Training Epoch: 9 [16640/36450]\tLoss: 538.5089\n",
      "Training Epoch: 9 [16704/36450]\tLoss: 538.6814\n",
      "Training Epoch: 9 [16768/36450]\tLoss: 550.6719\n",
      "Training Epoch: 9 [16832/36450]\tLoss: 543.0097\n",
      "Training Epoch: 9 [16896/36450]\tLoss: 548.6873\n",
      "Training Epoch: 9 [16960/36450]\tLoss: 545.9254\n",
      "Training Epoch: 9 [17024/36450]\tLoss: 541.9008\n",
      "Training Epoch: 9 [17088/36450]\tLoss: 570.3870\n",
      "Training Epoch: 9 [17152/36450]\tLoss: 544.6841\n",
      "Training Epoch: 9 [17216/36450]\tLoss: 536.0687\n",
      "Training Epoch: 9 [17280/36450]\tLoss: 521.5713\n",
      "Training Epoch: 9 [17344/36450]\tLoss: 534.3564\n",
      "Training Epoch: 9 [17408/36450]\tLoss: 523.8742\n",
      "Training Epoch: 9 [17472/36450]\tLoss: 522.3750\n",
      "Training Epoch: 9 [17536/36450]\tLoss: 539.1628\n",
      "Training Epoch: 9 [17600/36450]\tLoss: 534.8342\n",
      "Training Epoch: 9 [17664/36450]\tLoss: 517.0233\n",
      "Training Epoch: 9 [17728/36450]\tLoss: 553.1137\n",
      "Training Epoch: 9 [17792/36450]\tLoss: 528.5480\n",
      "Training Epoch: 9 [17856/36450]\tLoss: 511.5519\n",
      "Training Epoch: 9 [17920/36450]\tLoss: 580.4849\n",
      "Training Epoch: 9 [17984/36450]\tLoss: 532.4968\n",
      "Training Epoch: 9 [18048/36450]\tLoss: 553.2008\n",
      "Training Epoch: 9 [18112/36450]\tLoss: 564.9092\n",
      "Training Epoch: 9 [18176/36450]\tLoss: 528.5067\n",
      "Training Epoch: 9 [18240/36450]\tLoss: 586.0362\n",
      "Training Epoch: 9 [18304/36450]\tLoss: 561.0273\n",
      "Training Epoch: 9 [18368/36450]\tLoss: 566.4053\n",
      "Training Epoch: 9 [18432/36450]\tLoss: 554.8352\n",
      "Training Epoch: 9 [18496/36450]\tLoss: 559.0079\n",
      "Training Epoch: 9 [18560/36450]\tLoss: 552.5701\n",
      "Training Epoch: 9 [18624/36450]\tLoss: 534.5699\n",
      "Training Epoch: 9 [18688/36450]\tLoss: 543.3424\n",
      "Training Epoch: 9 [18752/36450]\tLoss: 575.7277\n",
      "Training Epoch: 9 [18816/36450]\tLoss: 586.3942\n",
      "Training Epoch: 9 [18880/36450]\tLoss: 593.9412\n",
      "Training Epoch: 9 [18944/36450]\tLoss: 556.8061\n",
      "Training Epoch: 9 [19008/36450]\tLoss: 564.3876\n",
      "Training Epoch: 9 [19072/36450]\tLoss: 562.0224\n",
      "Training Epoch: 9 [19136/36450]\tLoss: 529.4376\n",
      "Training Epoch: 9 [19200/36450]\tLoss: 567.0255\n",
      "Training Epoch: 9 [19264/36450]\tLoss: 523.9128\n",
      "Training Epoch: 9 [19328/36450]\tLoss: 544.4418\n",
      "Training Epoch: 9 [19392/36450]\tLoss: 576.5490\n",
      "Training Epoch: 9 [19456/36450]\tLoss: 541.6607\n",
      "Training Epoch: 9 [19520/36450]\tLoss: 572.8834\n",
      "Training Epoch: 9 [19584/36450]\tLoss: 562.5557\n",
      "Training Epoch: 9 [19648/36450]\tLoss: 567.8903\n",
      "Training Epoch: 9 [19712/36450]\tLoss: 598.4446\n",
      "Training Epoch: 9 [19776/36450]\tLoss: 552.8028\n",
      "Training Epoch: 9 [19840/36450]\tLoss: 584.7849\n",
      "Training Epoch: 9 [19904/36450]\tLoss: 581.8733\n",
      "Training Epoch: 9 [19968/36450]\tLoss: 627.9067\n",
      "Training Epoch: 9 [20032/36450]\tLoss: 574.3934\n",
      "Training Epoch: 9 [20096/36450]\tLoss: 600.6674\n",
      "Training Epoch: 9 [20160/36450]\tLoss: 586.1303\n",
      "Training Epoch: 9 [20224/36450]\tLoss: 563.0816\n",
      "Training Epoch: 9 [20288/36450]\tLoss: 532.1586\n",
      "Training Epoch: 9 [20352/36450]\tLoss: 545.8009\n",
      "Training Epoch: 9 [20416/36450]\tLoss: 552.2788\n",
      "Training Epoch: 9 [20480/36450]\tLoss: 556.8649\n",
      "Training Epoch: 9 [20544/36450]\tLoss: 546.5150\n",
      "Training Epoch: 9 [20608/36450]\tLoss: 573.8850\n",
      "Training Epoch: 9 [20672/36450]\tLoss: 596.1449\n",
      "Training Epoch: 9 [20736/36450]\tLoss: 583.8030\n",
      "Training Epoch: 9 [20800/36450]\tLoss: 568.7538\n",
      "Training Epoch: 9 [20864/36450]\tLoss: 565.2427\n",
      "Training Epoch: 9 [20928/36450]\tLoss: 534.5101\n",
      "Training Epoch: 9 [20992/36450]\tLoss: 563.6619\n",
      "Training Epoch: 9 [21056/36450]\tLoss: 522.6890\n",
      "Training Epoch: 9 [21120/36450]\tLoss: 564.8680\n",
      "Training Epoch: 9 [21184/36450]\tLoss: 523.3754\n",
      "Training Epoch: 9 [21248/36450]\tLoss: 552.8076\n",
      "Training Epoch: 9 [21312/36450]\tLoss: 556.6509\n",
      "Training Epoch: 9 [21376/36450]\tLoss: 546.7793\n",
      "Training Epoch: 9 [21440/36450]\tLoss: 529.5997\n",
      "Training Epoch: 9 [21504/36450]\tLoss: 548.8268\n",
      "Training Epoch: 9 [21568/36450]\tLoss: 548.9338\n",
      "Training Epoch: 9 [21632/36450]\tLoss: 518.7433\n",
      "Training Epoch: 9 [21696/36450]\tLoss: 578.1378\n",
      "Training Epoch: 9 [21760/36450]\tLoss: 553.6483\n",
      "Training Epoch: 9 [21824/36450]\tLoss: 566.1248\n",
      "Training Epoch: 9 [21888/36450]\tLoss: 548.9485\n",
      "Training Epoch: 9 [21952/36450]\tLoss: 570.9529\n",
      "Training Epoch: 9 [22016/36450]\tLoss: 535.9747\n",
      "Training Epoch: 9 [22080/36450]\tLoss: 539.0936\n",
      "Training Epoch: 9 [22144/36450]\tLoss: 542.8750\n",
      "Training Epoch: 9 [22208/36450]\tLoss: 541.4133\n",
      "Training Epoch: 9 [22272/36450]\tLoss: 516.3528\n",
      "Training Epoch: 9 [22336/36450]\tLoss: 537.4293\n",
      "Training Epoch: 9 [22400/36450]\tLoss: 544.5087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [22464/36450]\tLoss: 565.5243\n",
      "Training Epoch: 9 [22528/36450]\tLoss: 565.4035\n",
      "Training Epoch: 9 [22592/36450]\tLoss: 544.9177\n",
      "Training Epoch: 9 [22656/36450]\tLoss: 552.2925\n",
      "Training Epoch: 9 [22720/36450]\tLoss: 570.7335\n",
      "Training Epoch: 9 [22784/36450]\tLoss: 588.0464\n",
      "Training Epoch: 9 [22848/36450]\tLoss: 580.9506\n",
      "Training Epoch: 9 [22912/36450]\tLoss: 534.6908\n",
      "Training Epoch: 9 [22976/36450]\tLoss: 536.3911\n",
      "Training Epoch: 9 [23040/36450]\tLoss: 531.5775\n",
      "Training Epoch: 9 [23104/36450]\tLoss: 522.0592\n",
      "Training Epoch: 9 [23168/36450]\tLoss: 565.6030\n",
      "Training Epoch: 9 [23232/36450]\tLoss: 518.3244\n",
      "Training Epoch: 9 [23296/36450]\tLoss: 545.1370\n",
      "Training Epoch: 9 [23360/36450]\tLoss: 540.6026\n",
      "Training Epoch: 9 [23424/36450]\tLoss: 559.1907\n",
      "Training Epoch: 9 [23488/36450]\tLoss: 559.4886\n",
      "Training Epoch: 9 [23552/36450]\tLoss: 536.7802\n",
      "Training Epoch: 9 [23616/36450]\tLoss: 552.8217\n",
      "Training Epoch: 9 [23680/36450]\tLoss: 558.6161\n",
      "Training Epoch: 9 [23744/36450]\tLoss: 529.1965\n",
      "Training Epoch: 9 [23808/36450]\tLoss: 598.5288\n",
      "Training Epoch: 9 [23872/36450]\tLoss: 565.6526\n",
      "Training Epoch: 9 [23936/36450]\tLoss: 565.6284\n",
      "Training Epoch: 9 [24000/36450]\tLoss: 561.4773\n",
      "Training Epoch: 9 [24064/36450]\tLoss: 538.2008\n",
      "Training Epoch: 9 [24128/36450]\tLoss: 517.7056\n",
      "Training Epoch: 9 [24192/36450]\tLoss: 527.9921\n",
      "Training Epoch: 9 [24256/36450]\tLoss: 536.3979\n",
      "Training Epoch: 9 [24320/36450]\tLoss: 548.9411\n",
      "Training Epoch: 9 [24384/36450]\tLoss: 531.4811\n",
      "Training Epoch: 9 [24448/36450]\tLoss: 524.6851\n",
      "Training Epoch: 9 [24512/36450]\tLoss: 578.5419\n",
      "Training Epoch: 9 [24576/36450]\tLoss: 539.4365\n",
      "Training Epoch: 9 [24640/36450]\tLoss: 524.2810\n",
      "Training Epoch: 9 [24704/36450]\tLoss: 558.8258\n",
      "Training Epoch: 9 [24768/36450]\tLoss: 531.4052\n",
      "Training Epoch: 9 [24832/36450]\tLoss: 534.3391\n",
      "Training Epoch: 9 [24896/36450]\tLoss: 537.5256\n",
      "Training Epoch: 9 [24960/36450]\tLoss: 538.9106\n",
      "Training Epoch: 9 [25024/36450]\tLoss: 558.8369\n",
      "Training Epoch: 9 [25088/36450]\tLoss: 533.2629\n",
      "Training Epoch: 9 [25152/36450]\tLoss: 537.2169\n",
      "Training Epoch: 9 [25216/36450]\tLoss: 532.2723\n",
      "Training Epoch: 9 [25280/36450]\tLoss: 545.6101\n",
      "Training Epoch: 9 [25344/36450]\tLoss: 551.7438\n",
      "Training Epoch: 9 [25408/36450]\tLoss: 525.3953\n",
      "Training Epoch: 9 [25472/36450]\tLoss: 566.7421\n",
      "Training Epoch: 9 [25536/36450]\tLoss: 541.9563\n",
      "Training Epoch: 9 [25600/36450]\tLoss: 575.8739\n",
      "Training Epoch: 9 [25664/36450]\tLoss: 527.2476\n",
      "Training Epoch: 9 [25728/36450]\tLoss: 546.9641\n",
      "Training Epoch: 9 [25792/36450]\tLoss: 571.6945\n",
      "Training Epoch: 9 [25856/36450]\tLoss: 535.7407\n",
      "Training Epoch: 9 [25920/36450]\tLoss: 568.9454\n",
      "Training Epoch: 9 [25984/36450]\tLoss: 517.9167\n",
      "Training Epoch: 9 [26048/36450]\tLoss: 559.9154\n",
      "Training Epoch: 9 [26112/36450]\tLoss: 529.2601\n",
      "Training Epoch: 9 [26176/36450]\tLoss: 514.4956\n",
      "Training Epoch: 9 [26240/36450]\tLoss: 528.6329\n",
      "Training Epoch: 9 [26304/36450]\tLoss: 547.9081\n",
      "Training Epoch: 9 [26368/36450]\tLoss: 545.0674\n",
      "Training Epoch: 9 [26432/36450]\tLoss: 553.2058\n",
      "Training Epoch: 9 [26496/36450]\tLoss: 552.8000\n",
      "Training Epoch: 9 [26560/36450]\tLoss: 513.5173\n",
      "Training Epoch: 9 [26624/36450]\tLoss: 543.1663\n",
      "Training Epoch: 9 [26688/36450]\tLoss: 536.4349\n",
      "Training Epoch: 9 [26752/36450]\tLoss: 552.0320\n",
      "Training Epoch: 9 [26816/36450]\tLoss: 560.9829\n",
      "Training Epoch: 9 [26880/36450]\tLoss: 525.6964\n",
      "Training Epoch: 9 [26944/36450]\tLoss: 513.6007\n",
      "Training Epoch: 9 [27008/36450]\tLoss: 563.1441\n",
      "Training Epoch: 9 [27072/36450]\tLoss: 558.8077\n",
      "Training Epoch: 9 [27136/36450]\tLoss: 517.5730\n",
      "Training Epoch: 9 [27200/36450]\tLoss: 509.3404\n",
      "Training Epoch: 9 [27264/36450]\tLoss: 533.4177\n",
      "Training Epoch: 9 [27328/36450]\tLoss: 552.7186\n",
      "Training Epoch: 9 [27392/36450]\tLoss: 557.0175\n",
      "Training Epoch: 9 [27456/36450]\tLoss: 548.5601\n",
      "Training Epoch: 9 [27520/36450]\tLoss: 526.0734\n",
      "Training Epoch: 9 [27584/36450]\tLoss: 546.8970\n",
      "Training Epoch: 9 [27648/36450]\tLoss: 573.9427\n",
      "Training Epoch: 9 [27712/36450]\tLoss: 560.3130\n",
      "Training Epoch: 9 [27776/36450]\tLoss: 535.0685\n",
      "Training Epoch: 9 [27840/36450]\tLoss: 543.8721\n",
      "Training Epoch: 9 [27904/36450]\tLoss: 535.8472\n",
      "Training Epoch: 9 [27968/36450]\tLoss: 540.4996\n",
      "Training Epoch: 9 [28032/36450]\tLoss: 547.6041\n",
      "Training Epoch: 9 [28096/36450]\tLoss: 554.6087\n",
      "Training Epoch: 9 [28160/36450]\tLoss: 544.7034\n",
      "Training Epoch: 9 [28224/36450]\tLoss: 543.5164\n",
      "Training Epoch: 9 [28288/36450]\tLoss: 518.3918\n",
      "Training Epoch: 9 [28352/36450]\tLoss: 546.1635\n",
      "Training Epoch: 9 [28416/36450]\tLoss: 546.7318\n",
      "Training Epoch: 9 [28480/36450]\tLoss: 574.2558\n",
      "Training Epoch: 9 [28544/36450]\tLoss: 555.4890\n",
      "Training Epoch: 9 [28608/36450]\tLoss: 535.2412\n",
      "Training Epoch: 9 [28672/36450]\tLoss: 556.9435\n",
      "Training Epoch: 9 [28736/36450]\tLoss: 520.2654\n",
      "Training Epoch: 9 [28800/36450]\tLoss: 576.7787\n",
      "Training Epoch: 9 [28864/36450]\tLoss: 556.0192\n",
      "Training Epoch: 9 [28928/36450]\tLoss: 550.5140\n",
      "Training Epoch: 9 [28992/36450]\tLoss: 536.5020\n",
      "Training Epoch: 9 [29056/36450]\tLoss: 571.1791\n",
      "Training Epoch: 9 [29120/36450]\tLoss: 535.9883\n",
      "Training Epoch: 9 [29184/36450]\tLoss: 561.6946\n",
      "Training Epoch: 9 [29248/36450]\tLoss: 549.2892\n",
      "Training Epoch: 9 [29312/36450]\tLoss: 555.7051\n",
      "Training Epoch: 9 [29376/36450]\tLoss: 555.9455\n",
      "Training Epoch: 9 [29440/36450]\tLoss: 517.7656\n",
      "Training Epoch: 9 [29504/36450]\tLoss: 544.2364\n",
      "Training Epoch: 9 [29568/36450]\tLoss: 535.5114\n",
      "Training Epoch: 9 [29632/36450]\tLoss: 548.9892\n",
      "Training Epoch: 9 [29696/36450]\tLoss: 540.9036\n",
      "Training Epoch: 9 [29760/36450]\tLoss: 540.7690\n",
      "Training Epoch: 9 [29824/36450]\tLoss: 564.8986\n",
      "Training Epoch: 9 [29888/36450]\tLoss: 600.8133\n",
      "Training Epoch: 9 [29952/36450]\tLoss: 573.1229\n",
      "Training Epoch: 9 [30016/36450]\tLoss: 566.2834\n",
      "Training Epoch: 9 [30080/36450]\tLoss: 592.9515\n",
      "Training Epoch: 9 [30144/36450]\tLoss: 618.1157\n",
      "Training Epoch: 9 [30208/36450]\tLoss: 586.1776\n",
      "Training Epoch: 9 [30272/36450]\tLoss: 617.2804\n",
      "Training Epoch: 9 [30336/36450]\tLoss: 564.3456\n",
      "Training Epoch: 9 [30400/36450]\tLoss: 541.0393\n",
      "Training Epoch: 9 [30464/36450]\tLoss: 529.5432\n",
      "Training Epoch: 9 [30528/36450]\tLoss: 534.2728\n",
      "Training Epoch: 9 [30592/36450]\tLoss: 576.3057\n",
      "Training Epoch: 9 [30656/36450]\tLoss: 534.1368\n",
      "Training Epoch: 9 [30720/36450]\tLoss: 548.9513\n",
      "Training Epoch: 9 [30784/36450]\tLoss: 534.9825\n",
      "Training Epoch: 9 [30848/36450]\tLoss: 537.1224\n",
      "Training Epoch: 9 [30912/36450]\tLoss: 515.1152\n",
      "Training Epoch: 9 [30976/36450]\tLoss: 544.3214\n",
      "Training Epoch: 9 [31040/36450]\tLoss: 505.1993\n",
      "Training Epoch: 9 [31104/36450]\tLoss: 536.3234\n",
      "Training Epoch: 9 [31168/36450]\tLoss: 540.7559\n",
      "Training Epoch: 9 [31232/36450]\tLoss: 541.5347\n",
      "Training Epoch: 9 [31296/36450]\tLoss: 533.8361\n",
      "Training Epoch: 9 [31360/36450]\tLoss: 549.3222\n",
      "Training Epoch: 9 [31424/36450]\tLoss: 504.2268\n",
      "Training Epoch: 9 [31488/36450]\tLoss: 547.6690\n",
      "Training Epoch: 9 [31552/36450]\tLoss: 557.3790\n",
      "Training Epoch: 9 [31616/36450]\tLoss: 573.7527\n",
      "Training Epoch: 9 [31680/36450]\tLoss: 529.7758\n",
      "Training Epoch: 9 [31744/36450]\tLoss: 542.0638\n",
      "Training Epoch: 9 [31808/36450]\tLoss: 551.1959\n",
      "Training Epoch: 9 [31872/36450]\tLoss: 568.4949\n",
      "Training Epoch: 9 [31936/36450]\tLoss: 530.2214\n",
      "Training Epoch: 9 [32000/36450]\tLoss: 545.7535\n",
      "Training Epoch: 9 [32064/36450]\tLoss: 540.5437\n",
      "Training Epoch: 9 [32128/36450]\tLoss: 538.3582\n",
      "Training Epoch: 9 [32192/36450]\tLoss: 573.9976\n",
      "Training Epoch: 9 [32256/36450]\tLoss: 550.9091\n",
      "Training Epoch: 9 [32320/36450]\tLoss: 516.0566\n",
      "Training Epoch: 9 [32384/36450]\tLoss: 534.8449\n",
      "Training Epoch: 9 [32448/36450]\tLoss: 535.8853\n",
      "Training Epoch: 9 [32512/36450]\tLoss: 549.5220\n",
      "Training Epoch: 9 [32576/36450]\tLoss: 516.8796\n",
      "Training Epoch: 9 [32640/36450]\tLoss: 557.7659\n",
      "Training Epoch: 9 [32704/36450]\tLoss: 512.6703\n",
      "Training Epoch: 9 [32768/36450]\tLoss: 549.6167\n",
      "Training Epoch: 9 [32832/36450]\tLoss: 576.9612\n",
      "Training Epoch: 9 [32896/36450]\tLoss: 525.5679\n",
      "Training Epoch: 9 [32960/36450]\tLoss: 556.9699\n",
      "Training Epoch: 9 [33024/36450]\tLoss: 549.8998\n",
      "Training Epoch: 9 [33088/36450]\tLoss: 537.6137\n",
      "Training Epoch: 9 [33152/36450]\tLoss: 528.9228\n",
      "Training Epoch: 9 [33216/36450]\tLoss: 527.7388\n",
      "Training Epoch: 9 [33280/36450]\tLoss: 575.8086\n",
      "Training Epoch: 9 [33344/36450]\tLoss: 555.9107\n",
      "Training Epoch: 9 [33408/36450]\tLoss: 525.8338\n",
      "Training Epoch: 9 [33472/36450]\tLoss: 549.8576\n",
      "Training Epoch: 9 [33536/36450]\tLoss: 544.0689\n",
      "Training Epoch: 9 [33600/36450]\tLoss: 565.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [33664/36450]\tLoss: 538.1578\n",
      "Training Epoch: 9 [33728/36450]\tLoss: 564.3920\n",
      "Training Epoch: 9 [33792/36450]\tLoss: 534.6974\n",
      "Training Epoch: 9 [33856/36450]\tLoss: 551.4835\n",
      "Training Epoch: 9 [33920/36450]\tLoss: 532.3463\n",
      "Training Epoch: 9 [33984/36450]\tLoss: 556.8070\n",
      "Training Epoch: 9 [34048/36450]\tLoss: 536.0520\n",
      "Training Epoch: 9 [34112/36450]\tLoss: 529.5114\n",
      "Training Epoch: 9 [34176/36450]\tLoss: 555.0284\n",
      "Training Epoch: 9 [34240/36450]\tLoss: 532.0656\n",
      "Training Epoch: 9 [34304/36450]\tLoss: 554.0406\n",
      "Training Epoch: 9 [34368/36450]\tLoss: 559.2819\n",
      "Training Epoch: 9 [34432/36450]\tLoss: 541.4548\n",
      "Training Epoch: 9 [34496/36450]\tLoss: 571.2770\n",
      "Training Epoch: 9 [34560/36450]\tLoss: 542.7452\n",
      "Training Epoch: 9 [34624/36450]\tLoss: 560.4075\n",
      "Training Epoch: 9 [34688/36450]\tLoss: 524.4844\n",
      "Training Epoch: 9 [34752/36450]\tLoss: 544.6649\n",
      "Training Epoch: 9 [34816/36450]\tLoss: 555.4297\n",
      "Training Epoch: 9 [34880/36450]\tLoss: 533.4194\n",
      "Training Epoch: 9 [34944/36450]\tLoss: 555.4423\n",
      "Training Epoch: 9 [35008/36450]\tLoss: 532.2633\n",
      "Training Epoch: 9 [35072/36450]\tLoss: 573.1660\n",
      "Training Epoch: 9 [35136/36450]\tLoss: 578.6609\n",
      "Training Epoch: 9 [35200/36450]\tLoss: 562.6677\n",
      "Training Epoch: 9 [35264/36450]\tLoss: 531.7755\n",
      "Training Epoch: 9 [35328/36450]\tLoss: 535.9850\n",
      "Training Epoch: 9 [35392/36450]\tLoss: 550.3887\n",
      "Training Epoch: 9 [35456/36450]\tLoss: 543.2634\n",
      "Training Epoch: 9 [35520/36450]\tLoss: 546.4088\n",
      "Training Epoch: 9 [35584/36450]\tLoss: 534.7081\n",
      "Training Epoch: 9 [35648/36450]\tLoss: 538.5989\n",
      "Training Epoch: 9 [35712/36450]\tLoss: 548.6307\n",
      "Training Epoch: 9 [35776/36450]\tLoss: 531.3844\n",
      "Training Epoch: 9 [35840/36450]\tLoss: 511.9050\n",
      "Training Epoch: 9 [35904/36450]\tLoss: 538.6439\n",
      "Training Epoch: 9 [35968/36450]\tLoss: 511.7987\n",
      "Training Epoch: 9 [36032/36450]\tLoss: 539.8432\n",
      "Training Epoch: 9 [36096/36450]\tLoss: 543.4478\n",
      "Training Epoch: 9 [36160/36450]\tLoss: 547.0528\n",
      "Training Epoch: 9 [36224/36450]\tLoss: 552.1091\n",
      "Training Epoch: 9 [36288/36450]\tLoss: 544.6548\n",
      "Training Epoch: 9 [36352/36450]\tLoss: 566.1367\n",
      "Training Epoch: 9 [36416/36450]\tLoss: 546.2203\n",
      "Training Epoch: 9 [36450/36450]\tLoss: 551.6047\n",
      "Training Epoch: 9 [4050/4050]\tLoss: 270.5008\n",
      "Training Epoch: 10 [64/36450]\tLoss: 532.5560\n",
      "Training Epoch: 10 [128/36450]\tLoss: 578.6077\n",
      "Training Epoch: 10 [192/36450]\tLoss: 547.0108\n",
      "Training Epoch: 10 [256/36450]\tLoss: 545.9808\n",
      "Training Epoch: 10 [320/36450]\tLoss: 542.5637\n",
      "Training Epoch: 10 [384/36450]\tLoss: 545.5475\n",
      "Training Epoch: 10 [448/36450]\tLoss: 549.8832\n",
      "Training Epoch: 10 [512/36450]\tLoss: 546.0863\n",
      "Training Epoch: 10 [576/36450]\tLoss: 524.1126\n",
      "Training Epoch: 10 [640/36450]\tLoss: 567.7163\n",
      "Training Epoch: 10 [704/36450]\tLoss: 537.3422\n",
      "Training Epoch: 10 [768/36450]\tLoss: 538.9284\n",
      "Training Epoch: 10 [832/36450]\tLoss: 545.9259\n",
      "Training Epoch: 10 [896/36450]\tLoss: 527.3433\n",
      "Training Epoch: 10 [960/36450]\tLoss: 530.0436\n",
      "Training Epoch: 10 [1024/36450]\tLoss: 558.9742\n",
      "Training Epoch: 10 [1088/36450]\tLoss: 564.2047\n",
      "Training Epoch: 10 [1152/36450]\tLoss: 528.7592\n",
      "Training Epoch: 10 [1216/36450]\tLoss: 579.4161\n",
      "Training Epoch: 10 [1280/36450]\tLoss: 574.5856\n",
      "Training Epoch: 10 [1344/36450]\tLoss: 536.8783\n",
      "Training Epoch: 10 [1408/36450]\tLoss: 544.8945\n",
      "Training Epoch: 10 [1472/36450]\tLoss: 536.3494\n",
      "Training Epoch: 10 [1536/36450]\tLoss: 552.1630\n",
      "Training Epoch: 10 [1600/36450]\tLoss: 545.6277\n",
      "Training Epoch: 10 [1664/36450]\tLoss: 553.6785\n",
      "Training Epoch: 10 [1728/36450]\tLoss: 554.1512\n",
      "Training Epoch: 10 [1792/36450]\tLoss: 538.0904\n",
      "Training Epoch: 10 [1856/36450]\tLoss: 533.4934\n",
      "Training Epoch: 10 [1920/36450]\tLoss: 563.7083\n",
      "Training Epoch: 10 [1984/36450]\tLoss: 537.1885\n",
      "Training Epoch: 10 [2048/36450]\tLoss: 552.2427\n",
      "Training Epoch: 10 [2112/36450]\tLoss: 521.5013\n",
      "Training Epoch: 10 [2176/36450]\tLoss: 542.9194\n",
      "Training Epoch: 10 [2240/36450]\tLoss: 527.4932\n",
      "Training Epoch: 10 [2304/36450]\tLoss: 552.1185\n",
      "Training Epoch: 10 [2368/36450]\tLoss: 540.7211\n",
      "Training Epoch: 10 [2432/36450]\tLoss: 511.1544\n",
      "Training Epoch: 10 [2496/36450]\tLoss: 507.1894\n",
      "Training Epoch: 10 [2560/36450]\tLoss: 543.0827\n",
      "Training Epoch: 10 [2624/36450]\tLoss: 503.7333\n",
      "Training Epoch: 10 [2688/36450]\tLoss: 532.2904\n",
      "Training Epoch: 10 [2752/36450]\tLoss: 556.3550\n",
      "Training Epoch: 10 [2816/36450]\tLoss: 548.1946\n",
      "Training Epoch: 10 [2880/36450]\tLoss: 563.1496\n",
      "Training Epoch: 10 [2944/36450]\tLoss: 547.6918\n",
      "Training Epoch: 10 [3008/36450]\tLoss: 528.5634\n",
      "Training Epoch: 10 [3072/36450]\tLoss: 553.2609\n",
      "Training Epoch: 10 [3136/36450]\tLoss: 563.2581\n",
      "Training Epoch: 10 [3200/36450]\tLoss: 521.6207\n",
      "Training Epoch: 10 [3264/36450]\tLoss: 544.2792\n",
      "Training Epoch: 10 [3328/36450]\tLoss: 514.8046\n",
      "Training Epoch: 10 [3392/36450]\tLoss: 532.7568\n",
      "Training Epoch: 10 [3456/36450]\tLoss: 545.3649\n",
      "Training Epoch: 10 [3520/36450]\tLoss: 539.4467\n",
      "Training Epoch: 10 [3584/36450]\tLoss: 533.7688\n",
      "Training Epoch: 10 [3648/36450]\tLoss: 558.6848\n",
      "Training Epoch: 10 [3712/36450]\tLoss: 535.0479\n",
      "Training Epoch: 10 [3776/36450]\tLoss: 550.7411\n",
      "Training Epoch: 10 [3840/36450]\tLoss: 529.0957\n",
      "Training Epoch: 10 [3904/36450]\tLoss: 521.5293\n",
      "Training Epoch: 10 [3968/36450]\tLoss: 532.6428\n",
      "Training Epoch: 10 [4032/36450]\tLoss: 548.3984\n",
      "Training Epoch: 10 [4096/36450]\tLoss: 560.1584\n",
      "Training Epoch: 10 [4160/36450]\tLoss: 555.5082\n",
      "Training Epoch: 10 [4224/36450]\tLoss: 539.9022\n",
      "Training Epoch: 10 [4288/36450]\tLoss: 579.6058\n",
      "Training Epoch: 10 [4352/36450]\tLoss: 518.8976\n",
      "Training Epoch: 10 [4416/36450]\tLoss: 561.5480\n",
      "Training Epoch: 10 [4480/36450]\tLoss: 545.1152\n",
      "Training Epoch: 10 [4544/36450]\tLoss: 551.8447\n",
      "Training Epoch: 10 [4608/36450]\tLoss: 531.8248\n",
      "Training Epoch: 10 [4672/36450]\tLoss: 550.2055\n",
      "Training Epoch: 10 [4736/36450]\tLoss: 535.4783\n",
      "Training Epoch: 10 [4800/36450]\tLoss: 550.3502\n",
      "Training Epoch: 10 [4864/36450]\tLoss: 528.1827\n",
      "Training Epoch: 10 [4928/36450]\tLoss: 565.5322\n",
      "Training Epoch: 10 [4992/36450]\tLoss: 554.5311\n",
      "Training Epoch: 10 [5056/36450]\tLoss: 549.5816\n",
      "Training Epoch: 10 [5120/36450]\tLoss: 513.4639\n",
      "Training Epoch: 10 [5184/36450]\tLoss: 522.2825\n",
      "Training Epoch: 10 [5248/36450]\tLoss: 540.4655\n",
      "Training Epoch: 10 [5312/36450]\tLoss: 546.3436\n",
      "Training Epoch: 10 [5376/36450]\tLoss: 551.4954\n",
      "Training Epoch: 10 [5440/36450]\tLoss: 579.9896\n",
      "Training Epoch: 10 [5504/36450]\tLoss: 566.3549\n",
      "Training Epoch: 10 [5568/36450]\tLoss: 517.5874\n",
      "Training Epoch: 10 [5632/36450]\tLoss: 524.8310\n",
      "Training Epoch: 10 [5696/36450]\tLoss: 550.0510\n",
      "Training Epoch: 10 [5760/36450]\tLoss: 532.9531\n",
      "Training Epoch: 10 [5824/36450]\tLoss: 556.1556\n",
      "Training Epoch: 10 [5888/36450]\tLoss: 550.8549\n",
      "Training Epoch: 10 [5952/36450]\tLoss: 530.1939\n",
      "Training Epoch: 10 [6016/36450]\tLoss: 543.6360\n",
      "Training Epoch: 10 [6080/36450]\tLoss: 541.6518\n",
      "Training Epoch: 10 [6144/36450]\tLoss: 531.3601\n",
      "Training Epoch: 10 [6208/36450]\tLoss: 553.3929\n",
      "Training Epoch: 10 [6272/36450]\tLoss: 538.8342\n",
      "Training Epoch: 10 [6336/36450]\tLoss: 553.1704\n",
      "Training Epoch: 10 [6400/36450]\tLoss: 557.1573\n",
      "Training Epoch: 10 [6464/36450]\tLoss: 563.8066\n",
      "Training Epoch: 10 [6528/36450]\tLoss: 551.5616\n",
      "Training Epoch: 10 [6592/36450]\tLoss: 595.3330\n",
      "Training Epoch: 10 [6656/36450]\tLoss: 599.3744\n",
      "Training Epoch: 10 [6720/36450]\tLoss: 595.3004\n",
      "Training Epoch: 10 [6784/36450]\tLoss: 579.8048\n",
      "Training Epoch: 10 [6848/36450]\tLoss: 604.4178\n",
      "Training Epoch: 10 [6912/36450]\tLoss: 528.2728\n",
      "Training Epoch: 10 [6976/36450]\tLoss: 524.4029\n",
      "Training Epoch: 10 [7040/36450]\tLoss: 549.0029\n",
      "Training Epoch: 10 [7104/36450]\tLoss: 557.3094\n",
      "Training Epoch: 10 [7168/36450]\tLoss: 555.0567\n",
      "Training Epoch: 10 [7232/36450]\tLoss: 554.7983\n",
      "Training Epoch: 10 [7296/36450]\tLoss: 553.2723\n",
      "Training Epoch: 10 [7360/36450]\tLoss: 536.2425\n",
      "Training Epoch: 10 [7424/36450]\tLoss: 574.9488\n",
      "Training Epoch: 10 [7488/36450]\tLoss: 517.8060\n",
      "Training Epoch: 10 [7552/36450]\tLoss: 537.7066\n",
      "Training Epoch: 10 [7616/36450]\tLoss: 555.4933\n",
      "Training Epoch: 10 [7680/36450]\tLoss: 539.0609\n",
      "Training Epoch: 10 [7744/36450]\tLoss: 560.2690\n",
      "Training Epoch: 10 [7808/36450]\tLoss: 566.8940\n",
      "Training Epoch: 10 [7872/36450]\tLoss: 493.9563\n",
      "Training Epoch: 10 [7936/36450]\tLoss: 561.2678\n",
      "Training Epoch: 10 [8000/36450]\tLoss: 538.9708\n",
      "Training Epoch: 10 [8064/36450]\tLoss: 507.2114\n",
      "Training Epoch: 10 [8128/36450]\tLoss: 523.1312\n",
      "Training Epoch: 10 [8192/36450]\tLoss: 548.5710\n",
      "Training Epoch: 10 [8256/36450]\tLoss: 576.5115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [8320/36450]\tLoss: 555.9560\n",
      "Training Epoch: 10 [8384/36450]\tLoss: 567.3688\n",
      "Training Epoch: 10 [8448/36450]\tLoss: 507.5057\n",
      "Training Epoch: 10 [8512/36450]\tLoss: 527.1856\n",
      "Training Epoch: 10 [8576/36450]\tLoss: 553.2281\n",
      "Training Epoch: 10 [8640/36450]\tLoss: 519.2336\n",
      "Training Epoch: 10 [8704/36450]\tLoss: 574.2690\n",
      "Training Epoch: 10 [8768/36450]\tLoss: 546.0819\n",
      "Training Epoch: 10 [8832/36450]\tLoss: 544.4347\n",
      "Training Epoch: 10 [8896/36450]\tLoss: 554.7834\n",
      "Training Epoch: 10 [8960/36450]\tLoss: 568.4066\n",
      "Training Epoch: 10 [9024/36450]\tLoss: 509.5294\n",
      "Training Epoch: 10 [9088/36450]\tLoss: 557.9363\n",
      "Training Epoch: 10 [9152/36450]\tLoss: 565.8934\n",
      "Training Epoch: 10 [9216/36450]\tLoss: 519.0250\n",
      "Training Epoch: 10 [9280/36450]\tLoss: 528.1313\n",
      "Training Epoch: 10 [9344/36450]\tLoss: 515.6149\n",
      "Training Epoch: 10 [9408/36450]\tLoss: 532.9542\n",
      "Training Epoch: 10 [9472/36450]\tLoss: 515.9834\n",
      "Training Epoch: 10 [9536/36450]\tLoss: 556.7719\n",
      "Training Epoch: 10 [9600/36450]\tLoss: 539.2930\n",
      "Training Epoch: 10 [9664/36450]\tLoss: 520.1646\n",
      "Training Epoch: 10 [9728/36450]\tLoss: 524.2202\n",
      "Training Epoch: 10 [9792/36450]\tLoss: 524.9115\n",
      "Training Epoch: 10 [9856/36450]\tLoss: 520.9232\n",
      "Training Epoch: 10 [9920/36450]\tLoss: 509.6068\n",
      "Training Epoch: 10 [9984/36450]\tLoss: 551.1738\n",
      "Training Epoch: 10 [10048/36450]\tLoss: 526.2053\n",
      "Training Epoch: 10 [10112/36450]\tLoss: 510.5361\n",
      "Training Epoch: 10 [10176/36450]\tLoss: 534.7798\n",
      "Training Epoch: 10 [10240/36450]\tLoss: 529.6299\n",
      "Training Epoch: 10 [10304/36450]\tLoss: 539.5630\n",
      "Training Epoch: 10 [10368/36450]\tLoss: 556.8699\n",
      "Training Epoch: 10 [10432/36450]\tLoss: 522.9878\n",
      "Training Epoch: 10 [10496/36450]\tLoss: 538.9708\n",
      "Training Epoch: 10 [10560/36450]\tLoss: 515.3478\n",
      "Training Epoch: 10 [10624/36450]\tLoss: 536.1876\n",
      "Training Epoch: 10 [10688/36450]\tLoss: 539.0067\n",
      "Training Epoch: 10 [10752/36450]\tLoss: 552.2566\n",
      "Training Epoch: 10 [10816/36450]\tLoss: 523.9263\n",
      "Training Epoch: 10 [10880/36450]\tLoss: 520.7823\n",
      "Training Epoch: 10 [10944/36450]\tLoss: 551.6439\n",
      "Training Epoch: 10 [11008/36450]\tLoss: 550.0699\n",
      "Training Epoch: 10 [11072/36450]\tLoss: 537.1058\n",
      "Training Epoch: 10 [11136/36450]\tLoss: 523.7199\n",
      "Training Epoch: 10 [11200/36450]\tLoss: 549.9872\n",
      "Training Epoch: 10 [11264/36450]\tLoss: 579.4353\n",
      "Training Epoch: 10 [11328/36450]\tLoss: 548.2640\n",
      "Training Epoch: 10 [11392/36450]\tLoss: 551.8625\n",
      "Training Epoch: 10 [11456/36450]\tLoss: 570.5344\n",
      "Training Epoch: 10 [11520/36450]\tLoss: 534.0360\n",
      "Training Epoch: 10 [11584/36450]\tLoss: 517.1682\n",
      "Training Epoch: 10 [11648/36450]\tLoss: 529.3055\n",
      "Training Epoch: 10 [11712/36450]\tLoss: 517.3124\n",
      "Training Epoch: 10 [11776/36450]\tLoss: 529.9609\n",
      "Training Epoch: 10 [11840/36450]\tLoss: 571.5199\n",
      "Training Epoch: 10 [11904/36450]\tLoss: 528.4737\n",
      "Training Epoch: 10 [11968/36450]\tLoss: 551.5732\n",
      "Training Epoch: 10 [12032/36450]\tLoss: 535.2909\n",
      "Training Epoch: 10 [12096/36450]\tLoss: 535.4904\n",
      "Training Epoch: 10 [12160/36450]\tLoss: 534.9014\n",
      "Training Epoch: 10 [12224/36450]\tLoss: 533.3734\n",
      "Training Epoch: 10 [12288/36450]\tLoss: 493.6702\n",
      "Training Epoch: 10 [12352/36450]\tLoss: 540.7025\n",
      "Training Epoch: 10 [12416/36450]\tLoss: 527.1862\n",
      "Training Epoch: 10 [12480/36450]\tLoss: 510.9809\n",
      "Training Epoch: 10 [12544/36450]\tLoss: 512.1855\n",
      "Training Epoch: 10 [12608/36450]\tLoss: 540.5731\n",
      "Training Epoch: 10 [12672/36450]\tLoss: 568.0453\n",
      "Training Epoch: 10 [12736/36450]\tLoss: 522.5800\n",
      "Training Epoch: 10 [12800/36450]\tLoss: 501.0715\n",
      "Training Epoch: 10 [12864/36450]\tLoss: 526.8821\n",
      "Training Epoch: 10 [12928/36450]\tLoss: 585.9258\n",
      "Training Epoch: 10 [12992/36450]\tLoss: 533.4749\n",
      "Training Epoch: 10 [13056/36450]\tLoss: 531.2003\n",
      "Training Epoch: 10 [13120/36450]\tLoss: 515.8080\n",
      "Training Epoch: 10 [13184/36450]\tLoss: 501.1798\n",
      "Training Epoch: 10 [13248/36450]\tLoss: 562.2902\n",
      "Training Epoch: 10 [13312/36450]\tLoss: 516.8340\n",
      "Training Epoch: 10 [13376/36450]\tLoss: 523.4606\n",
      "Training Epoch: 10 [13440/36450]\tLoss: 521.7390\n",
      "Training Epoch: 10 [13504/36450]\tLoss: 540.7613\n",
      "Training Epoch: 10 [13568/36450]\tLoss: 546.5209\n",
      "Training Epoch: 10 [13632/36450]\tLoss: 525.9062\n",
      "Training Epoch: 10 [13696/36450]\tLoss: 556.3203\n",
      "Training Epoch: 10 [13760/36450]\tLoss: 556.4120\n",
      "Training Epoch: 10 [13824/36450]\tLoss: 513.4257\n",
      "Training Epoch: 10 [13888/36450]\tLoss: 535.4788\n",
      "Training Epoch: 10 [13952/36450]\tLoss: 546.0488\n",
      "Training Epoch: 10 [14016/36450]\tLoss: 533.6602\n",
      "Training Epoch: 10 [14080/36450]\tLoss: 537.4158\n",
      "Training Epoch: 10 [14144/36450]\tLoss: 516.4360\n",
      "Training Epoch: 10 [14208/36450]\tLoss: 528.1522\n",
      "Training Epoch: 10 [14272/36450]\tLoss: 565.0325\n",
      "Training Epoch: 10 [14336/36450]\tLoss: 556.2617\n",
      "Training Epoch: 10 [14400/36450]\tLoss: 491.3095\n",
      "Training Epoch: 10 [14464/36450]\tLoss: 549.1620\n",
      "Training Epoch: 10 [14528/36450]\tLoss: 548.7714\n",
      "Training Epoch: 10 [14592/36450]\tLoss: 518.9025\n",
      "Training Epoch: 10 [14656/36450]\tLoss: 535.9481\n",
      "Training Epoch: 10 [14720/36450]\tLoss: 522.3392\n",
      "Training Epoch: 10 [14784/36450]\tLoss: 561.0424\n",
      "Training Epoch: 10 [14848/36450]\tLoss: 520.9688\n",
      "Training Epoch: 10 [14912/36450]\tLoss: 548.3828\n",
      "Training Epoch: 10 [14976/36450]\tLoss: 547.1611\n",
      "Training Epoch: 10 [15040/36450]\tLoss: 543.0554\n",
      "Training Epoch: 10 [15104/36450]\tLoss: 553.3938\n",
      "Training Epoch: 10 [15168/36450]\tLoss: 589.1555\n",
      "Training Epoch: 10 [15232/36450]\tLoss: 560.4323\n",
      "Training Epoch: 10 [15296/36450]\tLoss: 564.5884\n",
      "Training Epoch: 10 [15360/36450]\tLoss: 594.8597\n",
      "Training Epoch: 10 [15424/36450]\tLoss: 614.5573\n",
      "Training Epoch: 10 [15488/36450]\tLoss: 590.2573\n",
      "Training Epoch: 10 [15552/36450]\tLoss: 569.9258\n",
      "Training Epoch: 10 [15616/36450]\tLoss: 543.0436\n",
      "Training Epoch: 10 [15680/36450]\tLoss: 542.5494\n",
      "Training Epoch: 10 [15744/36450]\tLoss: 525.3539\n",
      "Training Epoch: 10 [15808/36450]\tLoss: 585.4386\n",
      "Training Epoch: 10 [15872/36450]\tLoss: 570.1796\n",
      "Training Epoch: 10 [15936/36450]\tLoss: 530.1248\n",
      "Training Epoch: 10 [16000/36450]\tLoss: 560.3707\n",
      "Training Epoch: 10 [16064/36450]\tLoss: 580.3468\n",
      "Training Epoch: 10 [16128/36450]\tLoss: 594.8546\n",
      "Training Epoch: 10 [16192/36450]\tLoss: 562.5732\n",
      "Training Epoch: 10 [16256/36450]\tLoss: 541.3812\n",
      "Training Epoch: 10 [16320/36450]\tLoss: 546.3246\n",
      "Training Epoch: 10 [16384/36450]\tLoss: 553.8381\n",
      "Training Epoch: 10 [16448/36450]\tLoss: 549.3210\n",
      "Training Epoch: 10 [16512/36450]\tLoss: 530.2449\n",
      "Training Epoch: 10 [16576/36450]\tLoss: 537.2028\n",
      "Training Epoch: 10 [16640/36450]\tLoss: 538.9677\n",
      "Training Epoch: 10 [16704/36450]\tLoss: 555.1593\n",
      "Training Epoch: 10 [16768/36450]\tLoss: 559.8960\n",
      "Training Epoch: 10 [16832/36450]\tLoss: 545.8387\n",
      "Training Epoch: 10 [16896/36450]\tLoss: 520.7576\n",
      "Training Epoch: 10 [16960/36450]\tLoss: 582.9436\n",
      "Training Epoch: 10 [17024/36450]\tLoss: 542.1313\n",
      "Training Epoch: 10 [17088/36450]\tLoss: 557.3285\n",
      "Training Epoch: 10 [17152/36450]\tLoss: 551.2065\n",
      "Training Epoch: 10 [17216/36450]\tLoss: 541.3442\n",
      "Training Epoch: 10 [17280/36450]\tLoss: 538.2749\n",
      "Training Epoch: 10 [17344/36450]\tLoss: 495.3230\n",
      "Training Epoch: 10 [17408/36450]\tLoss: 561.4500\n",
      "Training Epoch: 10 [17472/36450]\tLoss: 574.1873\n",
      "Training Epoch: 10 [17536/36450]\tLoss: 540.8137\n",
      "Training Epoch: 10 [17600/36450]\tLoss: 544.6120\n",
      "Training Epoch: 10 [17664/36450]\tLoss: 537.7215\n",
      "Training Epoch: 10 [17728/36450]\tLoss: 524.0661\n",
      "Training Epoch: 10 [17792/36450]\tLoss: 526.0219\n",
      "Training Epoch: 10 [17856/36450]\tLoss: 552.1321\n",
      "Training Epoch: 10 [17920/36450]\tLoss: 526.8967\n",
      "Training Epoch: 10 [17984/36450]\tLoss: 547.0226\n",
      "Training Epoch: 10 [18048/36450]\tLoss: 529.3207\n",
      "Training Epoch: 10 [18112/36450]\tLoss: 503.5893\n",
      "Training Epoch: 10 [18176/36450]\tLoss: 538.4058\n",
      "Training Epoch: 10 [18240/36450]\tLoss: 546.8723\n",
      "Training Epoch: 10 [18304/36450]\tLoss: 534.2938\n",
      "Training Epoch: 10 [18368/36450]\tLoss: 563.3018\n",
      "Training Epoch: 10 [18432/36450]\tLoss: 512.2198\n",
      "Training Epoch: 10 [18496/36450]\tLoss: 528.5279\n",
      "Training Epoch: 10 [18560/36450]\tLoss: 532.5780\n",
      "Training Epoch: 10 [18624/36450]\tLoss: 536.6860\n",
      "Training Epoch: 10 [18688/36450]\tLoss: 540.2844\n",
      "Training Epoch: 10 [18752/36450]\tLoss: 535.5536\n",
      "Training Epoch: 10 [18816/36450]\tLoss: 490.3550\n",
      "Training Epoch: 10 [18880/36450]\tLoss: 544.6133\n",
      "Training Epoch: 10 [18944/36450]\tLoss: 556.7799\n",
      "Training Epoch: 10 [19008/36450]\tLoss: 536.3334\n",
      "Training Epoch: 10 [19072/36450]\tLoss: 541.3129\n",
      "Training Epoch: 10 [19136/36450]\tLoss: 543.6790\n",
      "Training Epoch: 10 [19200/36450]\tLoss: 531.7130\n",
      "Training Epoch: 10 [19264/36450]\tLoss: 513.0853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [19328/36450]\tLoss: 544.1495\n",
      "Training Epoch: 10 [19392/36450]\tLoss: 537.1798\n",
      "Training Epoch: 10 [19456/36450]\tLoss: 523.4947\n",
      "Training Epoch: 10 [19520/36450]\tLoss: 565.6600\n",
      "Training Epoch: 10 [19584/36450]\tLoss: 543.9934\n",
      "Training Epoch: 10 [19648/36450]\tLoss: 543.8899\n",
      "Training Epoch: 10 [19712/36450]\tLoss: 528.7732\n",
      "Training Epoch: 10 [19776/36450]\tLoss: 504.8077\n",
      "Training Epoch: 10 [19840/36450]\tLoss: 517.9789\n",
      "Training Epoch: 10 [19904/36450]\tLoss: 500.3245\n",
      "Training Epoch: 10 [19968/36450]\tLoss: 537.4400\n",
      "Training Epoch: 10 [20032/36450]\tLoss: 532.3440\n",
      "Training Epoch: 10 [20096/36450]\tLoss: 575.4283\n",
      "Training Epoch: 10 [20160/36450]\tLoss: 550.8221\n",
      "Training Epoch: 10 [20224/36450]\tLoss: 524.4131\n",
      "Training Epoch: 10 [20288/36450]\tLoss: 558.2507\n",
      "Training Epoch: 10 [20352/36450]\tLoss: 530.8830\n",
      "Training Epoch: 10 [20416/36450]\tLoss: 519.5461\n",
      "Training Epoch: 10 [20480/36450]\tLoss: 515.3869\n",
      "Training Epoch: 10 [20544/36450]\tLoss: 535.7252\n",
      "Training Epoch: 10 [20608/36450]\tLoss: 552.2736\n",
      "Training Epoch: 10 [20672/36450]\tLoss: 561.8919\n",
      "Training Epoch: 10 [20736/36450]\tLoss: 496.6352\n",
      "Training Epoch: 10 [20800/36450]\tLoss: 520.5886\n",
      "Training Epoch: 10 [20864/36450]\tLoss: 510.8369\n",
      "Training Epoch: 10 [20928/36450]\tLoss: 523.8089\n",
      "Training Epoch: 10 [20992/36450]\tLoss: 510.2117\n",
      "Training Epoch: 10 [21056/36450]\tLoss: 504.2505\n",
      "Training Epoch: 10 [21120/36450]\tLoss: 554.2153\n",
      "Training Epoch: 10 [21184/36450]\tLoss: 520.1491\n",
      "Training Epoch: 10 [21248/36450]\tLoss: 550.1937\n",
      "Training Epoch: 10 [21312/36450]\tLoss: 507.5897\n",
      "Training Epoch: 10 [21376/36450]\tLoss: 537.3885\n",
      "Training Epoch: 10 [21440/36450]\tLoss: 523.8302\n",
      "Training Epoch: 10 [21504/36450]\tLoss: 522.4177\n",
      "Training Epoch: 10 [21568/36450]\tLoss: 544.6541\n",
      "Training Epoch: 10 [21632/36450]\tLoss: 551.7946\n",
      "Training Epoch: 10 [21696/36450]\tLoss: 515.3929\n",
      "Training Epoch: 10 [21760/36450]\tLoss: 562.8786\n",
      "Training Epoch: 10 [21824/36450]\tLoss: 534.1051\n",
      "Training Epoch: 10 [21888/36450]\tLoss: 515.4302\n",
      "Training Epoch: 10 [21952/36450]\tLoss: 539.7854\n",
      "Training Epoch: 10 [22016/36450]\tLoss: 552.5546\n",
      "Training Epoch: 10 [22080/36450]\tLoss: 520.2187\n",
      "Training Epoch: 10 [22144/36450]\tLoss: 543.3035\n",
      "Training Epoch: 10 [22208/36450]\tLoss: 522.8177\n",
      "Training Epoch: 10 [22272/36450]\tLoss: 513.8729\n",
      "Training Epoch: 10 [22336/36450]\tLoss: 528.1174\n",
      "Training Epoch: 10 [22400/36450]\tLoss: 528.2347\n",
      "Training Epoch: 10 [22464/36450]\tLoss: 550.4899\n",
      "Training Epoch: 10 [22528/36450]\tLoss: 517.5529\n",
      "Training Epoch: 10 [22592/36450]\tLoss: 538.7319\n",
      "Training Epoch: 10 [22656/36450]\tLoss: 546.9541\n",
      "Training Epoch: 10 [22720/36450]\tLoss: 532.8008\n",
      "Training Epoch: 10 [22784/36450]\tLoss: 519.6827\n",
      "Training Epoch: 10 [22848/36450]\tLoss: 519.7261\n",
      "Training Epoch: 10 [22912/36450]\tLoss: 535.0585\n",
      "Training Epoch: 10 [22976/36450]\tLoss: 532.4458\n",
      "Training Epoch: 10 [23040/36450]\tLoss: 553.7457\n",
      "Training Epoch: 10 [23104/36450]\tLoss: 537.3927\n",
      "Training Epoch: 10 [23168/36450]\tLoss: 545.8078\n",
      "Training Epoch: 10 [23232/36450]\tLoss: 519.7615\n",
      "Training Epoch: 10 [23296/36450]\tLoss: 496.2901\n",
      "Training Epoch: 10 [23360/36450]\tLoss: 577.6747\n",
      "Training Epoch: 10 [23424/36450]\tLoss: 555.4217\n",
      "Training Epoch: 10 [23488/36450]\tLoss: 532.4679\n",
      "Training Epoch: 10 [23552/36450]\tLoss: 559.3434\n",
      "Training Epoch: 10 [23616/36450]\tLoss: 540.0692\n",
      "Training Epoch: 10 [23680/36450]\tLoss: 541.5302\n",
      "Training Epoch: 10 [23744/36450]\tLoss: 541.6932\n",
      "Training Epoch: 10 [23808/36450]\tLoss: 520.8013\n",
      "Training Epoch: 10 [23872/36450]\tLoss: 556.1296\n",
      "Training Epoch: 10 [23936/36450]\tLoss: 542.7552\n",
      "Training Epoch: 10 [24000/36450]\tLoss: 547.1411\n",
      "Training Epoch: 10 [24064/36450]\tLoss: 546.8109\n",
      "Training Epoch: 10 [24128/36450]\tLoss: 549.7587\n",
      "Training Epoch: 10 [24192/36450]\tLoss: 539.1255\n",
      "Training Epoch: 10 [24256/36450]\tLoss: 504.7333\n",
      "Training Epoch: 10 [24320/36450]\tLoss: 549.2710\n",
      "Training Epoch: 10 [24384/36450]\tLoss: 544.5284\n",
      "Training Epoch: 10 [24448/36450]\tLoss: 554.7893\n",
      "Training Epoch: 10 [24512/36450]\tLoss: 547.7493\n",
      "Training Epoch: 10 [24576/36450]\tLoss: 502.0226\n",
      "Training Epoch: 10 [24640/36450]\tLoss: 515.9374\n",
      "Training Epoch: 10 [24704/36450]\tLoss: 516.6590\n",
      "Training Epoch: 10 [24768/36450]\tLoss: 540.9500\n",
      "Training Epoch: 10 [24832/36450]\tLoss: 538.2921\n",
      "Training Epoch: 10 [24896/36450]\tLoss: 525.9815\n",
      "Training Epoch: 10 [24960/36450]\tLoss: 554.8556\n",
      "Training Epoch: 10 [25024/36450]\tLoss: 534.7463\n",
      "Training Epoch: 10 [25088/36450]\tLoss: 516.0227\n",
      "Training Epoch: 10 [25152/36450]\tLoss: 539.3177\n",
      "Training Epoch: 10 [25216/36450]\tLoss: 527.0790\n",
      "Training Epoch: 10 [25280/36450]\tLoss: 521.5560\n",
      "Training Epoch: 10 [25344/36450]\tLoss: 548.0179\n",
      "Training Epoch: 10 [25408/36450]\tLoss: 533.2352\n",
      "Training Epoch: 10 [25472/36450]\tLoss: 508.6353\n",
      "Training Epoch: 10 [25536/36450]\tLoss: 525.3165\n",
      "Training Epoch: 10 [25600/36450]\tLoss: 553.0596\n",
      "Training Epoch: 10 [25664/36450]\tLoss: 520.8584\n",
      "Training Epoch: 10 [25728/36450]\tLoss: 524.9902\n",
      "Training Epoch: 10 [25792/36450]\tLoss: 543.4742\n",
      "Training Epoch: 10 [25856/36450]\tLoss: 541.9748\n",
      "Training Epoch: 10 [25920/36450]\tLoss: 544.5909\n",
      "Training Epoch: 10 [25984/36450]\tLoss: 530.7671\n",
      "Training Epoch: 10 [26048/36450]\tLoss: 530.1372\n",
      "Training Epoch: 10 [26112/36450]\tLoss: 545.6261\n",
      "Training Epoch: 10 [26176/36450]\tLoss: 542.6995\n",
      "Training Epoch: 10 [26240/36450]\tLoss: 560.2794\n",
      "Training Epoch: 10 [26304/36450]\tLoss: 571.8290\n",
      "Training Epoch: 10 [26368/36450]\tLoss: 586.2069\n",
      "Training Epoch: 10 [26432/36450]\tLoss: 600.2440\n",
      "Training Epoch: 10 [26496/36450]\tLoss: 580.2083\n",
      "Training Epoch: 10 [26560/36450]\tLoss: 576.6710\n",
      "Training Epoch: 10 [26624/36450]\tLoss: 542.0576\n",
      "Training Epoch: 10 [26688/36450]\tLoss: 560.7729\n",
      "Training Epoch: 10 [26752/36450]\tLoss: 569.2020\n",
      "Training Epoch: 10 [26816/36450]\tLoss: 512.7276\n",
      "Training Epoch: 10 [26880/36450]\tLoss: 519.4778\n",
      "Training Epoch: 10 [26944/36450]\tLoss: 546.3695\n",
      "Training Epoch: 10 [27008/36450]\tLoss: 529.7194\n",
      "Training Epoch: 10 [27072/36450]\tLoss: 537.2549\n",
      "Training Epoch: 10 [27136/36450]\tLoss: 527.4722\n",
      "Training Epoch: 10 [27200/36450]\tLoss: 558.9547\n",
      "Training Epoch: 10 [27264/36450]\tLoss: 497.8728\n",
      "Training Epoch: 10 [27328/36450]\tLoss: 567.1997\n",
      "Training Epoch: 10 [27392/36450]\tLoss: 544.6915\n",
      "Training Epoch: 10 [27456/36450]\tLoss: 573.2815\n",
      "Training Epoch: 10 [27520/36450]\tLoss: 554.9236\n",
      "Training Epoch: 10 [27584/36450]\tLoss: 555.5554\n",
      "Training Epoch: 10 [27648/36450]\tLoss: 535.3961\n",
      "Training Epoch: 10 [27712/36450]\tLoss: 557.1698\n",
      "Training Epoch: 10 [27776/36450]\tLoss: 549.1978\n",
      "Training Epoch: 10 [27840/36450]\tLoss: 532.5772\n",
      "Training Epoch: 10 [27904/36450]\tLoss: 565.6642\n",
      "Training Epoch: 10 [27968/36450]\tLoss: 521.9911\n",
      "Training Epoch: 10 [28032/36450]\tLoss: 565.9849\n",
      "Training Epoch: 10 [28096/36450]\tLoss: 552.4901\n",
      "Training Epoch: 10 [28160/36450]\tLoss: 524.2324\n",
      "Training Epoch: 10 [28224/36450]\tLoss: 548.8857\n",
      "Training Epoch: 10 [28288/36450]\tLoss: 590.2360\n",
      "Training Epoch: 10 [28352/36450]\tLoss: 529.4333\n",
      "Training Epoch: 10 [28416/36450]\tLoss: 568.8060\n",
      "Training Epoch: 10 [28480/36450]\tLoss: 552.9728\n",
      "Training Epoch: 10 [28544/36450]\tLoss: 526.5674\n",
      "Training Epoch: 10 [28608/36450]\tLoss: 525.2335\n",
      "Training Epoch: 10 [28672/36450]\tLoss: 524.9542\n",
      "Training Epoch: 10 [28736/36450]\tLoss: 536.1482\n",
      "Training Epoch: 10 [28800/36450]\tLoss: 543.7217\n",
      "Training Epoch: 10 [28864/36450]\tLoss: 539.0009\n",
      "Training Epoch: 10 [28928/36450]\tLoss: 543.1008\n",
      "Training Epoch: 10 [28992/36450]\tLoss: 537.4189\n",
      "Training Epoch: 10 [29056/36450]\tLoss: 537.1140\n",
      "Training Epoch: 10 [29120/36450]\tLoss: 506.3013\n",
      "Training Epoch: 10 [29184/36450]\tLoss: 524.9551\n",
      "Training Epoch: 10 [29248/36450]\tLoss: 551.4203\n",
      "Training Epoch: 10 [29312/36450]\tLoss: 527.5345\n",
      "Training Epoch: 10 [29376/36450]\tLoss: 528.4880\n",
      "Training Epoch: 10 [29440/36450]\tLoss: 537.2532\n",
      "Training Epoch: 10 [29504/36450]\tLoss: 519.9689\n",
      "Training Epoch: 10 [29568/36450]\tLoss: 519.3782\n",
      "Training Epoch: 10 [29632/36450]\tLoss: 532.2672\n",
      "Training Epoch: 10 [29696/36450]\tLoss: 548.5869\n",
      "Training Epoch: 10 [29760/36450]\tLoss: 502.1371\n",
      "Training Epoch: 10 [29824/36450]\tLoss: 524.3012\n",
      "Training Epoch: 10 [29888/36450]\tLoss: 515.1235\n",
      "Training Epoch: 10 [29952/36450]\tLoss: 519.1134\n",
      "Training Epoch: 10 [30016/36450]\tLoss: 534.5439\n",
      "Training Epoch: 10 [30080/36450]\tLoss: 538.7338\n",
      "Training Epoch: 10 [30144/36450]\tLoss: 549.4172\n",
      "Training Epoch: 10 [30208/36450]\tLoss: 538.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [30272/36450]\tLoss: 522.5026\n",
      "Training Epoch: 10 [30336/36450]\tLoss: 538.5296\n",
      "Training Epoch: 10 [30400/36450]\tLoss: 549.7805\n",
      "Training Epoch: 10 [30464/36450]\tLoss: 553.7884\n",
      "Training Epoch: 10 [30528/36450]\tLoss: 547.7053\n",
      "Training Epoch: 10 [30592/36450]\tLoss: 533.4255\n",
      "Training Epoch: 10 [30656/36450]\tLoss: 546.6329\n",
      "Training Epoch: 10 [30720/36450]\tLoss: 556.9077\n",
      "Training Epoch: 10 [30784/36450]\tLoss: 530.1109\n",
      "Training Epoch: 10 [30848/36450]\tLoss: 526.5974\n",
      "Training Epoch: 10 [30912/36450]\tLoss: 537.9888\n",
      "Training Epoch: 10 [30976/36450]\tLoss: 512.3645\n",
      "Training Epoch: 10 [31040/36450]\tLoss: 548.6144\n",
      "Training Epoch: 10 [31104/36450]\tLoss: 561.5499\n",
      "Training Epoch: 10 [31168/36450]\tLoss: 516.2817\n",
      "Training Epoch: 10 [31232/36450]\tLoss: 507.6855\n",
      "Training Epoch: 10 [31296/36450]\tLoss: 535.9040\n",
      "Training Epoch: 10 [31360/36450]\tLoss: 530.3853\n",
      "Training Epoch: 10 [31424/36450]\tLoss: 522.1667\n",
      "Training Epoch: 10 [31488/36450]\tLoss: 528.1279\n",
      "Training Epoch: 10 [31552/36450]\tLoss: 488.1967\n",
      "Training Epoch: 10 [31616/36450]\tLoss: 541.6428\n",
      "Training Epoch: 10 [31680/36450]\tLoss: 529.7379\n",
      "Training Epoch: 10 [31744/36450]\tLoss: 501.9615\n",
      "Training Epoch: 10 [31808/36450]\tLoss: 550.0443\n",
      "Training Epoch: 10 [31872/36450]\tLoss: 535.7337\n",
      "Training Epoch: 10 [31936/36450]\tLoss: 518.5894\n",
      "Training Epoch: 10 [32000/36450]\tLoss: 509.3767\n",
      "Training Epoch: 10 [32064/36450]\tLoss: 540.8060\n",
      "Training Epoch: 10 [32128/36450]\tLoss: 498.7374\n",
      "Training Epoch: 10 [32192/36450]\tLoss: 497.6619\n",
      "Training Epoch: 10 [32256/36450]\tLoss: 503.2963\n",
      "Training Epoch: 10 [32320/36450]\tLoss: 556.1895\n",
      "Training Epoch: 10 [32384/36450]\tLoss: 528.0755\n",
      "Training Epoch: 10 [32448/36450]\tLoss: 544.9724\n",
      "Training Epoch: 10 [32512/36450]\tLoss: 559.3757\n",
      "Training Epoch: 10 [32576/36450]\tLoss: 554.8242\n",
      "Training Epoch: 10 [32640/36450]\tLoss: 530.5541\n",
      "Training Epoch: 10 [32704/36450]\tLoss: 554.1033\n",
      "Training Epoch: 10 [32768/36450]\tLoss: 511.1569\n",
      "Training Epoch: 10 [32832/36450]\tLoss: 535.5322\n",
      "Training Epoch: 10 [32896/36450]\tLoss: 552.0690\n",
      "Training Epoch: 10 [32960/36450]\tLoss: 545.4471\n",
      "Training Epoch: 10 [33024/36450]\tLoss: 514.3382\n",
      "Training Epoch: 10 [33088/36450]\tLoss: 537.7619\n",
      "Training Epoch: 10 [33152/36450]\tLoss: 515.2206\n",
      "Training Epoch: 10 [33216/36450]\tLoss: 562.1938\n",
      "Training Epoch: 10 [33280/36450]\tLoss: 515.5354\n",
      "Training Epoch: 10 [33344/36450]\tLoss: 532.2555\n",
      "Training Epoch: 10 [33408/36450]\tLoss: 542.2407\n",
      "Training Epoch: 10 [33472/36450]\tLoss: 546.4222\n",
      "Training Epoch: 10 [33536/36450]\tLoss: 527.5948\n",
      "Training Epoch: 10 [33600/36450]\tLoss: 536.7812\n",
      "Training Epoch: 10 [33664/36450]\tLoss: 556.7034\n",
      "Training Epoch: 10 [33728/36450]\tLoss: 540.7563\n",
      "Training Epoch: 10 [33792/36450]\tLoss: 545.8253\n",
      "Training Epoch: 10 [33856/36450]\tLoss: 528.9917\n",
      "Training Epoch: 10 [33920/36450]\tLoss: 545.8925\n",
      "Training Epoch: 10 [33984/36450]\tLoss: 557.2781\n",
      "Training Epoch: 10 [34048/36450]\tLoss: 547.1546\n",
      "Training Epoch: 10 [34112/36450]\tLoss: 536.1711\n",
      "Training Epoch: 10 [34176/36450]\tLoss: 526.6420\n",
      "Training Epoch: 10 [34240/36450]\tLoss: 527.1443\n",
      "Training Epoch: 10 [34304/36450]\tLoss: 541.0842\n",
      "Training Epoch: 10 [34368/36450]\tLoss: 548.0645\n",
      "Training Epoch: 10 [34432/36450]\tLoss: 549.2841\n",
      "Training Epoch: 10 [34496/36450]\tLoss: 556.7451\n",
      "Training Epoch: 10 [34560/36450]\tLoss: 550.8903\n",
      "Training Epoch: 10 [34624/36450]\tLoss: 531.0715\n",
      "Training Epoch: 10 [34688/36450]\tLoss: 546.0958\n",
      "Training Epoch: 10 [34752/36450]\tLoss: 535.3893\n",
      "Training Epoch: 10 [34816/36450]\tLoss: 552.0953\n",
      "Training Epoch: 10 [34880/36450]\tLoss: 535.2313\n",
      "Training Epoch: 10 [34944/36450]\tLoss: 544.1414\n",
      "Training Epoch: 10 [35008/36450]\tLoss: 542.1535\n",
      "Training Epoch: 10 [35072/36450]\tLoss: 533.2808\n",
      "Training Epoch: 10 [35136/36450]\tLoss: 526.3512\n",
      "Training Epoch: 10 [35200/36450]\tLoss: 527.1674\n",
      "Training Epoch: 10 [35264/36450]\tLoss: 577.1237\n",
      "Training Epoch: 10 [35328/36450]\tLoss: 556.6256\n",
      "Training Epoch: 10 [35392/36450]\tLoss: 543.1635\n",
      "Training Epoch: 10 [35456/36450]\tLoss: 532.0144\n",
      "Training Epoch: 10 [35520/36450]\tLoss: 545.5668\n",
      "Training Epoch: 10 [35584/36450]\tLoss: 527.4336\n",
      "Training Epoch: 10 [35648/36450]\tLoss: 531.7029\n",
      "Training Epoch: 10 [35712/36450]\tLoss: 552.7757\n",
      "Training Epoch: 10 [35776/36450]\tLoss: 523.4614\n",
      "Training Epoch: 10 [35840/36450]\tLoss: 524.5495\n",
      "Training Epoch: 10 [35904/36450]\tLoss: 557.9997\n",
      "Training Epoch: 10 [35968/36450]\tLoss: 548.1162\n",
      "Training Epoch: 10 [36032/36450]\tLoss: 521.2623\n",
      "Training Epoch: 10 [36096/36450]\tLoss: 516.8712\n",
      "Training Epoch: 10 [36160/36450]\tLoss: 521.4978\n",
      "Training Epoch: 10 [36224/36450]\tLoss: 542.2183\n",
      "Training Epoch: 10 [36288/36450]\tLoss: 560.6212\n",
      "Training Epoch: 10 [36352/36450]\tLoss: 493.1846\n",
      "Training Epoch: 10 [36416/36450]\tLoss: 526.5860\n",
      "Training Epoch: 10 [36450/36450]\tLoss: 528.6537\n",
      "Training Epoch: 10 [4050/4050]\tLoss: 263.9441\n",
      "Training Epoch: 11 [64/36450]\tLoss: 546.7182\n",
      "Training Epoch: 11 [128/36450]\tLoss: 539.3302\n",
      "Training Epoch: 11 [192/36450]\tLoss: 526.6110\n",
      "Training Epoch: 11 [256/36450]\tLoss: 560.1292\n",
      "Training Epoch: 11 [320/36450]\tLoss: 523.9097\n",
      "Training Epoch: 11 [384/36450]\tLoss: 518.1638\n",
      "Training Epoch: 11 [448/36450]\tLoss: 523.9877\n",
      "Training Epoch: 11 [512/36450]\tLoss: 511.9915\n",
      "Training Epoch: 11 [576/36450]\tLoss: 510.7139\n",
      "Training Epoch: 11 [640/36450]\tLoss: 547.1411\n",
      "Training Epoch: 11 [704/36450]\tLoss: 512.3244\n",
      "Training Epoch: 11 [768/36450]\tLoss: 509.5864\n",
      "Training Epoch: 11 [832/36450]\tLoss: 561.0918\n",
      "Training Epoch: 11 [896/36450]\tLoss: 518.1613\n",
      "Training Epoch: 11 [960/36450]\tLoss: 498.6087\n",
      "Training Epoch: 11 [1024/36450]\tLoss: 544.7182\n",
      "Training Epoch: 11 [1088/36450]\tLoss: 515.2152\n",
      "Training Epoch: 11 [1152/36450]\tLoss: 491.3618\n",
      "Training Epoch: 11 [1216/36450]\tLoss: 524.4669\n",
      "Training Epoch: 11 [1280/36450]\tLoss: 517.8856\n",
      "Training Epoch: 11 [1344/36450]\tLoss: 546.5103\n",
      "Training Epoch: 11 [1408/36450]\tLoss: 524.0864\n",
      "Training Epoch: 11 [1472/36450]\tLoss: 545.4738\n",
      "Training Epoch: 11 [1536/36450]\tLoss: 495.6638\n",
      "Training Epoch: 11 [1600/36450]\tLoss: 539.8669\n",
      "Training Epoch: 11 [1664/36450]\tLoss: 521.6589\n",
      "Training Epoch: 11 [1728/36450]\tLoss: 529.4387\n",
      "Training Epoch: 11 [1792/36450]\tLoss: 542.1382\n",
      "Training Epoch: 11 [1856/36450]\tLoss: 546.5885\n",
      "Training Epoch: 11 [1920/36450]\tLoss: 576.2981\n",
      "Training Epoch: 11 [1984/36450]\tLoss: 511.8257\n",
      "Training Epoch: 11 [2048/36450]\tLoss: 512.4038\n",
      "Training Epoch: 11 [2112/36450]\tLoss: 538.8376\n",
      "Training Epoch: 11 [2176/36450]\tLoss: 541.8044\n",
      "Training Epoch: 11 [2240/36450]\tLoss: 545.8859\n",
      "Training Epoch: 11 [2304/36450]\tLoss: 547.4393\n",
      "Training Epoch: 11 [2368/36450]\tLoss: 534.4288\n",
      "Training Epoch: 11 [2432/36450]\tLoss: 548.5212\n",
      "Training Epoch: 11 [2496/36450]\tLoss: 519.0949\n",
      "Training Epoch: 11 [2560/36450]\tLoss: 533.6534\n",
      "Training Epoch: 11 [2624/36450]\tLoss: 530.4789\n",
      "Training Epoch: 11 [2688/36450]\tLoss: 547.4109\n",
      "Training Epoch: 11 [2752/36450]\tLoss: 557.0074\n",
      "Training Epoch: 11 [2816/36450]\tLoss: 532.5413\n",
      "Training Epoch: 11 [2880/36450]\tLoss: 542.2369\n",
      "Training Epoch: 11 [2944/36450]\tLoss: 534.2842\n",
      "Training Epoch: 11 [3008/36450]\tLoss: 523.4318\n",
      "Training Epoch: 11 [3072/36450]\tLoss: 522.6201\n",
      "Training Epoch: 11 [3136/36450]\tLoss: 523.3123\n",
      "Training Epoch: 11 [3200/36450]\tLoss: 530.4650\n",
      "Training Epoch: 11 [3264/36450]\tLoss: 520.8304\n",
      "Training Epoch: 11 [3328/36450]\tLoss: 529.9707\n",
      "Training Epoch: 11 [3392/36450]\tLoss: 523.6669\n",
      "Training Epoch: 11 [3456/36450]\tLoss: 557.3102\n",
      "Training Epoch: 11 [3520/36450]\tLoss: 559.9003\n",
      "Training Epoch: 11 [3584/36450]\tLoss: 509.4686\n",
      "Training Epoch: 11 [3648/36450]\tLoss: 549.8118\n",
      "Training Epoch: 11 [3712/36450]\tLoss: 556.8165\n",
      "Training Epoch: 11 [3776/36450]\tLoss: 560.1782\n",
      "Training Epoch: 11 [3840/36450]\tLoss: 578.4219\n",
      "Training Epoch: 11 [3904/36450]\tLoss: 574.5182\n",
      "Training Epoch: 11 [3968/36450]\tLoss: 575.3261\n",
      "Training Epoch: 11 [4032/36450]\tLoss: 561.2582\n",
      "Training Epoch: 11 [4096/36450]\tLoss: 551.9109\n",
      "Training Epoch: 11 [4160/36450]\tLoss: 520.2350\n",
      "Training Epoch: 11 [4224/36450]\tLoss: 514.1985\n",
      "Training Epoch: 11 [4288/36450]\tLoss: 536.5453\n",
      "Training Epoch: 11 [4352/36450]\tLoss: 519.4435\n",
      "Training Epoch: 11 [4416/36450]\tLoss: 526.6361\n",
      "Training Epoch: 11 [4480/36450]\tLoss: 522.4028\n",
      "Training Epoch: 11 [4544/36450]\tLoss: 557.2538\n",
      "Training Epoch: 11 [4608/36450]\tLoss: 570.3696\n",
      "Training Epoch: 11 [4672/36450]\tLoss: 518.2282\n",
      "Training Epoch: 11 [4736/36450]\tLoss: 525.0159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [4800/36450]\tLoss: 500.6552\n",
      "Training Epoch: 11 [4864/36450]\tLoss: 548.3349\n",
      "Training Epoch: 11 [4928/36450]\tLoss: 545.0060\n",
      "Training Epoch: 11 [4992/36450]\tLoss: 521.7974\n",
      "Training Epoch: 11 [5056/36450]\tLoss: 536.6310\n",
      "Training Epoch: 11 [5120/36450]\tLoss: 542.4557\n",
      "Training Epoch: 11 [5184/36450]\tLoss: 517.4637\n",
      "Training Epoch: 11 [5248/36450]\tLoss: 566.8762\n",
      "Training Epoch: 11 [5312/36450]\tLoss: 523.2366\n",
      "Training Epoch: 11 [5376/36450]\tLoss: 516.3226\n",
      "Training Epoch: 11 [5440/36450]\tLoss: 557.8839\n",
      "Training Epoch: 11 [5504/36450]\tLoss: 493.3582\n",
      "Training Epoch: 11 [5568/36450]\tLoss: 527.1399\n",
      "Training Epoch: 11 [5632/36450]\tLoss: 526.1428\n",
      "Training Epoch: 11 [5696/36450]\tLoss: 524.0242\n",
      "Training Epoch: 11 [5760/36450]\tLoss: 547.4765\n",
      "Training Epoch: 11 [5824/36450]\tLoss: 559.0538\n",
      "Training Epoch: 11 [5888/36450]\tLoss: 544.7473\n",
      "Training Epoch: 11 [5952/36450]\tLoss: 501.5618\n",
      "Training Epoch: 11 [6016/36450]\tLoss: 522.5272\n",
      "Training Epoch: 11 [6080/36450]\tLoss: 492.7422\n",
      "Training Epoch: 11 [6144/36450]\tLoss: 537.5712\n",
      "Training Epoch: 11 [6208/36450]\tLoss: 500.7470\n",
      "Training Epoch: 11 [6272/36450]\tLoss: 510.5344\n",
      "Training Epoch: 11 [6336/36450]\tLoss: 504.6571\n",
      "Training Epoch: 11 [6400/36450]\tLoss: 517.3735\n",
      "Training Epoch: 11 [6464/36450]\tLoss: 507.5832\n",
      "Training Epoch: 11 [6528/36450]\tLoss: 491.0974\n",
      "Training Epoch: 11 [6592/36450]\tLoss: 529.3985\n",
      "Training Epoch: 11 [6656/36450]\tLoss: 524.6700\n",
      "Training Epoch: 11 [6720/36450]\tLoss: 549.5825\n",
      "Training Epoch: 11 [6784/36450]\tLoss: 499.5027\n",
      "Training Epoch: 11 [6848/36450]\tLoss: 526.3426\n",
      "Training Epoch: 11 [6912/36450]\tLoss: 519.9086\n",
      "Training Epoch: 11 [6976/36450]\tLoss: 509.6510\n",
      "Training Epoch: 11 [7040/36450]\tLoss: 541.7396\n",
      "Training Epoch: 11 [7104/36450]\tLoss: 533.4196\n",
      "Training Epoch: 11 [7168/36450]\tLoss: 514.2875\n",
      "Training Epoch: 11 [7232/36450]\tLoss: 546.1726\n",
      "Training Epoch: 11 [7296/36450]\tLoss: 547.9835\n",
      "Training Epoch: 11 [7360/36450]\tLoss: 521.2444\n",
      "Training Epoch: 11 [7424/36450]\tLoss: 537.0367\n",
      "Training Epoch: 11 [7488/36450]\tLoss: 530.3505\n",
      "Training Epoch: 11 [7552/36450]\tLoss: 519.3220\n",
      "Training Epoch: 11 [7616/36450]\tLoss: 511.0920\n",
      "Training Epoch: 11 [7680/36450]\tLoss: 544.1119\n",
      "Training Epoch: 11 [7744/36450]\tLoss: 533.0449\n",
      "Training Epoch: 11 [7808/36450]\tLoss: 522.9285\n",
      "Training Epoch: 11 [7872/36450]\tLoss: 539.8356\n",
      "Training Epoch: 11 [7936/36450]\tLoss: 521.3726\n",
      "Training Epoch: 11 [8000/36450]\tLoss: 546.9591\n",
      "Training Epoch: 11 [8064/36450]\tLoss: 528.8390\n",
      "Training Epoch: 11 [8128/36450]\tLoss: 524.6020\n",
      "Training Epoch: 11 [8192/36450]\tLoss: 497.2742\n",
      "Training Epoch: 11 [8256/36450]\tLoss: 524.4649\n",
      "Training Epoch: 11 [8320/36450]\tLoss: 517.8171\n",
      "Training Epoch: 11 [8384/36450]\tLoss: 510.9705\n",
      "Training Epoch: 11 [8448/36450]\tLoss: 527.5170\n",
      "Training Epoch: 11 [8512/36450]\tLoss: 534.5632\n",
      "Training Epoch: 11 [8576/36450]\tLoss: 511.1290\n",
      "Training Epoch: 11 [8640/36450]\tLoss: 514.8296\n",
      "Training Epoch: 11 [8704/36450]\tLoss: 495.2906\n",
      "Training Epoch: 11 [8768/36450]\tLoss: 537.5509\n",
      "Training Epoch: 11 [8832/36450]\tLoss: 509.1133\n",
      "Training Epoch: 11 [8896/36450]\tLoss: 536.1605\n",
      "Training Epoch: 11 [8960/36450]\tLoss: 538.6965\n",
      "Training Epoch: 11 [9024/36450]\tLoss: 549.3344\n",
      "Training Epoch: 11 [9088/36450]\tLoss: 548.4710\n",
      "Training Epoch: 11 [9152/36450]\tLoss: 505.3930\n",
      "Training Epoch: 11 [9216/36450]\tLoss: 552.6939\n",
      "Training Epoch: 11 [9280/36450]\tLoss: 550.5306\n",
      "Training Epoch: 11 [9344/36450]\tLoss: 523.8160\n",
      "Training Epoch: 11 [9408/36450]\tLoss: 548.0342\n",
      "Training Epoch: 11 [9472/36450]\tLoss: 536.0947\n",
      "Training Epoch: 11 [9536/36450]\tLoss: 499.1147\n",
      "Training Epoch: 11 [9600/36450]\tLoss: 536.4625\n",
      "Training Epoch: 11 [9664/36450]\tLoss: 532.5944\n",
      "Training Epoch: 11 [9728/36450]\tLoss: 565.9623\n",
      "Training Epoch: 11 [9792/36450]\tLoss: 531.8904\n",
      "Training Epoch: 11 [9856/36450]\tLoss: 539.9414\n",
      "Training Epoch: 11 [9920/36450]\tLoss: 499.0387\n",
      "Training Epoch: 11 [9984/36450]\tLoss: 561.5897\n",
      "Training Epoch: 11 [10048/36450]\tLoss: 547.5120\n",
      "Training Epoch: 11 [10112/36450]\tLoss: 565.8282\n",
      "Training Epoch: 11 [10176/36450]\tLoss: 586.9606\n",
      "Training Epoch: 11 [10240/36450]\tLoss: 607.2227\n",
      "Training Epoch: 11 [10304/36450]\tLoss: 609.6692\n",
      "Training Epoch: 11 [10368/36450]\tLoss: 636.8624\n",
      "Training Epoch: 11 [10432/36450]\tLoss: 644.6744\n",
      "Training Epoch: 11 [10496/36450]\tLoss: 581.3281\n",
      "Training Epoch: 11 [10560/36450]\tLoss: 540.2211\n",
      "Training Epoch: 11 [10624/36450]\tLoss: 541.8236\n",
      "Training Epoch: 11 [10688/36450]\tLoss: 525.4178\n",
      "Training Epoch: 11 [10752/36450]\tLoss: 561.1972\n",
      "Training Epoch: 11 [10816/36450]\tLoss: 554.9640\n",
      "Training Epoch: 11 [10880/36450]\tLoss: 565.6379\n",
      "Training Epoch: 11 [10944/36450]\tLoss: 527.4796\n",
      "Training Epoch: 11 [11008/36450]\tLoss: 522.7574\n",
      "Training Epoch: 11 [11072/36450]\tLoss: 543.1716\n",
      "Training Epoch: 11 [11136/36450]\tLoss: 555.6147\n",
      "Training Epoch: 11 [11200/36450]\tLoss: 533.1429\n",
      "Training Epoch: 11 [11264/36450]\tLoss: 539.3244\n",
      "Training Epoch: 11 [11328/36450]\tLoss: 538.0567\n",
      "Training Epoch: 11 [11392/36450]\tLoss: 536.4301\n",
      "Training Epoch: 11 [11456/36450]\tLoss: 524.8729\n",
      "Training Epoch: 11 [11520/36450]\tLoss: 557.2250\n",
      "Training Epoch: 11 [11584/36450]\tLoss: 541.2115\n",
      "Training Epoch: 11 [11648/36450]\tLoss: 513.3710\n",
      "Training Epoch: 11 [11712/36450]\tLoss: 488.8867\n",
      "Training Epoch: 11 [11776/36450]\tLoss: 499.3384\n",
      "Training Epoch: 11 [11840/36450]\tLoss: 522.3789\n",
      "Training Epoch: 11 [11904/36450]\tLoss: 523.3933\n",
      "Training Epoch: 11 [11968/36450]\tLoss: 538.3312\n",
      "Training Epoch: 11 [12032/36450]\tLoss: 540.8132\n",
      "Training Epoch: 11 [12096/36450]\tLoss: 529.8322\n",
      "Training Epoch: 11 [12160/36450]\tLoss: 527.4979\n",
      "Training Epoch: 11 [12224/36450]\tLoss: 525.8217\n",
      "Training Epoch: 11 [12288/36450]\tLoss: 559.4820\n",
      "Training Epoch: 11 [12352/36450]\tLoss: 523.1112\n",
      "Training Epoch: 11 [12416/36450]\tLoss: 516.5308\n",
      "Training Epoch: 11 [12480/36450]\tLoss: 547.5482\n",
      "Training Epoch: 11 [12544/36450]\tLoss: 565.0306\n",
      "Training Epoch: 11 [12608/36450]\tLoss: 552.4817\n",
      "Training Epoch: 11 [12672/36450]\tLoss: 511.9373\n",
      "Training Epoch: 11 [12736/36450]\tLoss: 493.7426\n",
      "Training Epoch: 11 [12800/36450]\tLoss: 539.4940\n",
      "Training Epoch: 11 [12864/36450]\tLoss: 521.5692\n",
      "Training Epoch: 11 [12928/36450]\tLoss: 532.5717\n",
      "Training Epoch: 11 [12992/36450]\tLoss: 538.2359\n",
      "Training Epoch: 11 [13056/36450]\tLoss: 506.3297\n",
      "Training Epoch: 11 [13120/36450]\tLoss: 523.1610\n",
      "Training Epoch: 11 [13184/36450]\tLoss: 540.8449\n",
      "Training Epoch: 11 [13248/36450]\tLoss: 562.6785\n",
      "Training Epoch: 11 [13312/36450]\tLoss: 478.9731\n",
      "Training Epoch: 11 [13376/36450]\tLoss: 522.0272\n",
      "Training Epoch: 11 [13440/36450]\tLoss: 521.3193\n",
      "Training Epoch: 11 [13504/36450]\tLoss: 551.2919\n",
      "Training Epoch: 11 [13568/36450]\tLoss: 525.8896\n",
      "Training Epoch: 11 [13632/36450]\tLoss: 550.1994\n",
      "Training Epoch: 11 [13696/36450]\tLoss: 534.5250\n",
      "Training Epoch: 11 [13760/36450]\tLoss: 543.3372\n",
      "Training Epoch: 11 [13824/36450]\tLoss: 546.7176\n",
      "Training Epoch: 11 [13888/36450]\tLoss: 519.9321\n",
      "Training Epoch: 11 [13952/36450]\tLoss: 522.0449\n",
      "Training Epoch: 11 [14016/36450]\tLoss: 554.3760\n",
      "Training Epoch: 11 [14080/36450]\tLoss: 526.5590\n",
      "Training Epoch: 11 [14144/36450]\tLoss: 516.4321\n",
      "Training Epoch: 11 [14208/36450]\tLoss: 536.1814\n",
      "Training Epoch: 11 [14272/36450]\tLoss: 520.3215\n",
      "Training Epoch: 11 [14336/36450]\tLoss: 523.7347\n",
      "Training Epoch: 11 [14400/36450]\tLoss: 542.4232\n",
      "Training Epoch: 11 [14464/36450]\tLoss: 536.7253\n",
      "Training Epoch: 11 [14528/36450]\tLoss: 501.2540\n",
      "Training Epoch: 11 [14592/36450]\tLoss: 548.4654\n",
      "Training Epoch: 11 [14656/36450]\tLoss: 533.1391\n",
      "Training Epoch: 11 [14720/36450]\tLoss: 533.3526\n",
      "Training Epoch: 11 [14784/36450]\tLoss: 529.4984\n",
      "Training Epoch: 11 [14848/36450]\tLoss: 521.9568\n",
      "Training Epoch: 11 [14912/36450]\tLoss: 547.5856\n",
      "Training Epoch: 11 [14976/36450]\tLoss: 514.7299\n",
      "Training Epoch: 11 [15040/36450]\tLoss: 550.4977\n",
      "Training Epoch: 11 [15104/36450]\tLoss: 516.9335\n",
      "Training Epoch: 11 [15168/36450]\tLoss: 501.7164\n",
      "Training Epoch: 11 [15232/36450]\tLoss: 532.1725\n",
      "Training Epoch: 11 [15296/36450]\tLoss: 533.8431\n",
      "Training Epoch: 11 [15360/36450]\tLoss: 549.4789\n",
      "Training Epoch: 11 [15424/36450]\tLoss: 498.8827\n",
      "Training Epoch: 11 [15488/36450]\tLoss: 550.1612\n",
      "Training Epoch: 11 [15552/36450]\tLoss: 526.4559\n",
      "Training Epoch: 11 [15616/36450]\tLoss: 564.8770\n",
      "Training Epoch: 11 [15680/36450]\tLoss: 511.0359\n",
      "Training Epoch: 11 [15744/36450]\tLoss: 529.7753\n",
      "Training Epoch: 11 [15808/36450]\tLoss: 504.8098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [15872/36450]\tLoss: 565.6588\n",
      "Training Epoch: 11 [15936/36450]\tLoss: 524.2064\n",
      "Training Epoch: 11 [16000/36450]\tLoss: 549.2371\n",
      "Training Epoch: 11 [16064/36450]\tLoss: 514.6833\n",
      "Training Epoch: 11 [16128/36450]\tLoss: 503.1638\n",
      "Training Epoch: 11 [16192/36450]\tLoss: 520.1313\n",
      "Training Epoch: 11 [16256/36450]\tLoss: 538.6902\n",
      "Training Epoch: 11 [16320/36450]\tLoss: 531.2725\n",
      "Training Epoch: 11 [16384/36450]\tLoss: 531.9996\n",
      "Training Epoch: 11 [16448/36450]\tLoss: 538.5501\n",
      "Training Epoch: 11 [16512/36450]\tLoss: 512.7894\n",
      "Training Epoch: 11 [16576/36450]\tLoss: 522.1461\n",
      "Training Epoch: 11 [16640/36450]\tLoss: 562.6230\n",
      "Training Epoch: 11 [16704/36450]\tLoss: 535.8790\n",
      "Training Epoch: 11 [16768/36450]\tLoss: 543.1233\n",
      "Training Epoch: 11 [16832/36450]\tLoss: 540.2798\n",
      "Training Epoch: 11 [16896/36450]\tLoss: 509.9462\n",
      "Training Epoch: 11 [16960/36450]\tLoss: 553.8549\n",
      "Training Epoch: 11 [17024/36450]\tLoss: 499.9167\n",
      "Training Epoch: 11 [17088/36450]\tLoss: 539.9573\n",
      "Training Epoch: 11 [17152/36450]\tLoss: 510.3292\n",
      "Training Epoch: 11 [17216/36450]\tLoss: 538.9749\n",
      "Training Epoch: 11 [17280/36450]\tLoss: 540.0289\n",
      "Training Epoch: 11 [17344/36450]\tLoss: 535.1274\n",
      "Training Epoch: 11 [17408/36450]\tLoss: 546.8004\n",
      "Training Epoch: 11 [17472/36450]\tLoss: 510.1061\n",
      "Training Epoch: 11 [17536/36450]\tLoss: 504.0496\n",
      "Training Epoch: 11 [17600/36450]\tLoss: 532.7355\n",
      "Training Epoch: 11 [17664/36450]\tLoss: 496.4176\n",
      "Training Epoch: 11 [17728/36450]\tLoss: 544.0371\n",
      "Training Epoch: 11 [17792/36450]\tLoss: 532.1579\n",
      "Training Epoch: 11 [17856/36450]\tLoss: 509.6020\n",
      "Training Epoch: 11 [17920/36450]\tLoss: 529.2537\n",
      "Training Epoch: 11 [17984/36450]\tLoss: 546.4199\n",
      "Training Epoch: 11 [18048/36450]\tLoss: 514.7430\n",
      "Training Epoch: 11 [18112/36450]\tLoss: 502.8904\n",
      "Training Epoch: 11 [18176/36450]\tLoss: 527.7743\n",
      "Training Epoch: 11 [18240/36450]\tLoss: 525.9041\n",
      "Training Epoch: 11 [18304/36450]\tLoss: 511.7276\n",
      "Training Epoch: 11 [18368/36450]\tLoss: 529.1420\n",
      "Training Epoch: 11 [18432/36450]\tLoss: 585.0264\n",
      "Training Epoch: 11 [18496/36450]\tLoss: 528.2107\n",
      "Training Epoch: 11 [18560/36450]\tLoss: 551.7610\n",
      "Training Epoch: 11 [18624/36450]\tLoss: 527.5205\n",
      "Training Epoch: 11 [18688/36450]\tLoss: 534.6039\n",
      "Training Epoch: 11 [18752/36450]\tLoss: 522.0810\n",
      "Training Epoch: 11 [18816/36450]\tLoss: 492.5007\n",
      "Training Epoch: 11 [18880/36450]\tLoss: 531.2794\n",
      "Training Epoch: 11 [18944/36450]\tLoss: 543.1389\n",
      "Training Epoch: 11 [19008/36450]\tLoss: 487.7038\n",
      "Training Epoch: 11 [19072/36450]\tLoss: 510.3011\n",
      "Training Epoch: 11 [19136/36450]\tLoss: 463.9061\n",
      "Training Epoch: 11 [19200/36450]\tLoss: 514.7030\n",
      "Training Epoch: 11 [19264/36450]\tLoss: 521.1494\n",
      "Training Epoch: 11 [19328/36450]\tLoss: 509.5041\n",
      "Training Epoch: 11 [19392/36450]\tLoss: 524.8540\n",
      "Training Epoch: 11 [19456/36450]\tLoss: 501.2841\n",
      "Training Epoch: 11 [19520/36450]\tLoss: 524.5371\n",
      "Training Epoch: 11 [19584/36450]\tLoss: 523.3474\n",
      "Training Epoch: 11 [19648/36450]\tLoss: 516.2089\n",
      "Training Epoch: 11 [19712/36450]\tLoss: 486.2616\n",
      "Training Epoch: 11 [19776/36450]\tLoss: 532.0890\n",
      "Training Epoch: 11 [19840/36450]\tLoss: 510.3719\n",
      "Training Epoch: 11 [19904/36450]\tLoss: 509.1664\n",
      "Training Epoch: 11 [19968/36450]\tLoss: 540.0026\n",
      "Training Epoch: 11 [20032/36450]\tLoss: 509.0172\n",
      "Training Epoch: 11 [20096/36450]\tLoss: 493.3777\n",
      "Training Epoch: 11 [20160/36450]\tLoss: 539.3276\n",
      "Training Epoch: 11 [20224/36450]\tLoss: 500.6837\n",
      "Training Epoch: 11 [20288/36450]\tLoss: 539.9987\n",
      "Training Epoch: 11 [20352/36450]\tLoss: 534.2550\n",
      "Training Epoch: 11 [20416/36450]\tLoss: 537.3628\n",
      "Training Epoch: 11 [20480/36450]\tLoss: 531.0267\n",
      "Training Epoch: 11 [20544/36450]\tLoss: 522.5443\n",
      "Training Epoch: 11 [20608/36450]\tLoss: 538.1687\n",
      "Training Epoch: 11 [20672/36450]\tLoss: 522.0404\n",
      "Training Epoch: 11 [20736/36450]\tLoss: 541.2501\n",
      "Training Epoch: 11 [20800/36450]\tLoss: 520.3173\n",
      "Training Epoch: 11 [20864/36450]\tLoss: 499.6643\n",
      "Training Epoch: 11 [20928/36450]\tLoss: 524.8293\n",
      "Training Epoch: 11 [20992/36450]\tLoss: 530.8969\n",
      "Training Epoch: 11 [21056/36450]\tLoss: 501.5847\n",
      "Training Epoch: 11 [21120/36450]\tLoss: 527.6577\n",
      "Training Epoch: 11 [21184/36450]\tLoss: 534.5702\n",
      "Training Epoch: 11 [21248/36450]\tLoss: 538.8737\n",
      "Training Epoch: 11 [21312/36450]\tLoss: 545.7233\n",
      "Training Epoch: 11 [21376/36450]\tLoss: 552.9879\n",
      "Training Epoch: 11 [21440/36450]\tLoss: 510.9683\n",
      "Training Epoch: 11 [21504/36450]\tLoss: 548.9828\n",
      "Training Epoch: 11 [21568/36450]\tLoss: 542.8748\n",
      "Training Epoch: 11 [21632/36450]\tLoss: 519.3534\n",
      "Training Epoch: 11 [21696/36450]\tLoss: 527.4005\n",
      "Training Epoch: 11 [21760/36450]\tLoss: 526.1823\n",
      "Training Epoch: 11 [21824/36450]\tLoss: 505.2789\n",
      "Training Epoch: 11 [21888/36450]\tLoss: 506.8928\n",
      "Training Epoch: 11 [21952/36450]\tLoss: 541.1035\n",
      "Training Epoch: 11 [22016/36450]\tLoss: 536.3056\n",
      "Training Epoch: 11 [22080/36450]\tLoss: 521.2219\n",
      "Training Epoch: 11 [22144/36450]\tLoss: 568.2707\n",
      "Training Epoch: 11 [22208/36450]\tLoss: 548.3906\n",
      "Training Epoch: 11 [22272/36450]\tLoss: 541.4577\n",
      "Training Epoch: 11 [22336/36450]\tLoss: 558.5699\n",
      "Training Epoch: 11 [22400/36450]\tLoss: 549.6594\n",
      "Training Epoch: 11 [22464/36450]\tLoss: 545.5283\n",
      "Training Epoch: 11 [22528/36450]\tLoss: 583.2709\n",
      "Training Epoch: 11 [22592/36450]\tLoss: 542.6485\n",
      "Training Epoch: 11 [22656/36450]\tLoss: 520.2451\n",
      "Training Epoch: 11 [22720/36450]\tLoss: 540.0889\n",
      "Training Epoch: 11 [22784/36450]\tLoss: 530.3525\n",
      "Training Epoch: 11 [22848/36450]\tLoss: 521.7018\n",
      "Training Epoch: 11 [22912/36450]\tLoss: 525.5518\n",
      "Training Epoch: 11 [22976/36450]\tLoss: 532.5792\n",
      "Training Epoch: 11 [23040/36450]\tLoss: 537.7841\n",
      "Training Epoch: 11 [23104/36450]\tLoss: 543.9023\n",
      "Training Epoch: 11 [23168/36450]\tLoss: 530.0807\n",
      "Training Epoch: 11 [23232/36450]\tLoss: 539.5549\n",
      "Training Epoch: 11 [23296/36450]\tLoss: 540.6205\n",
      "Training Epoch: 11 [23360/36450]\tLoss: 503.1744\n",
      "Training Epoch: 11 [23424/36450]\tLoss: 519.8527\n",
      "Training Epoch: 11 [23488/36450]\tLoss: 531.4143\n",
      "Training Epoch: 11 [23552/36450]\tLoss: 537.9766\n",
      "Training Epoch: 11 [23616/36450]\tLoss: 493.7925\n",
      "Training Epoch: 11 [23680/36450]\tLoss: 501.3994\n",
      "Training Epoch: 11 [23744/36450]\tLoss: 530.0609\n",
      "Training Epoch: 11 [23808/36450]\tLoss: 503.1946\n",
      "Training Epoch: 11 [23872/36450]\tLoss: 519.3782\n",
      "Training Epoch: 11 [23936/36450]\tLoss: 507.2896\n",
      "Training Epoch: 11 [24000/36450]\tLoss: 516.2142\n",
      "Training Epoch: 11 [24064/36450]\tLoss: 515.4822\n",
      "Training Epoch: 11 [24128/36450]\tLoss: 528.4982\n",
      "Training Epoch: 11 [24192/36450]\tLoss: 538.1108\n",
      "Training Epoch: 11 [24256/36450]\tLoss: 546.2841\n",
      "Training Epoch: 11 [24320/36450]\tLoss: 549.7292\n",
      "Training Epoch: 11 [24384/36450]\tLoss: 522.3137\n",
      "Training Epoch: 11 [24448/36450]\tLoss: 537.2615\n",
      "Training Epoch: 11 [24512/36450]\tLoss: 521.9905\n",
      "Training Epoch: 11 [24576/36450]\tLoss: 515.3748\n",
      "Training Epoch: 11 [24640/36450]\tLoss: 542.4553\n",
      "Training Epoch: 11 [24704/36450]\tLoss: 511.2688\n",
      "Training Epoch: 11 [24768/36450]\tLoss: 517.8374\n",
      "Training Epoch: 11 [24832/36450]\tLoss: 530.8239\n",
      "Training Epoch: 11 [24896/36450]\tLoss: 520.5463\n",
      "Training Epoch: 11 [24960/36450]\tLoss: 533.2697\n",
      "Training Epoch: 11 [25024/36450]\tLoss: 529.5682\n",
      "Training Epoch: 11 [25088/36450]\tLoss: 563.5598\n",
      "Training Epoch: 11 [25152/36450]\tLoss: 501.4096\n",
      "Training Epoch: 11 [25216/36450]\tLoss: 513.7833\n",
      "Training Epoch: 11 [25280/36450]\tLoss: 544.4416\n",
      "Training Epoch: 11 [25344/36450]\tLoss: 547.4352\n",
      "Training Epoch: 11 [25408/36450]\tLoss: 529.2995\n",
      "Training Epoch: 11 [25472/36450]\tLoss: 524.1631\n",
      "Training Epoch: 11 [25536/36450]\tLoss: 524.0926\n",
      "Training Epoch: 11 [25600/36450]\tLoss: 530.0198\n",
      "Training Epoch: 11 [25664/36450]\tLoss: 556.0702\n",
      "Training Epoch: 11 [25728/36450]\tLoss: 512.0424\n",
      "Training Epoch: 11 [25792/36450]\tLoss: 530.1312\n",
      "Training Epoch: 11 [25856/36450]\tLoss: 564.0443\n",
      "Training Epoch: 11 [25920/36450]\tLoss: 518.2853\n",
      "Training Epoch: 11 [25984/36450]\tLoss: 521.7149\n",
      "Training Epoch: 11 [26048/36450]\tLoss: 513.5657\n",
      "Training Epoch: 11 [26112/36450]\tLoss: 546.2856\n",
      "Training Epoch: 11 [26176/36450]\tLoss: 523.1990\n",
      "Training Epoch: 11 [26240/36450]\tLoss: 515.9899\n",
      "Training Epoch: 11 [26304/36450]\tLoss: 540.2311\n",
      "Training Epoch: 11 [26368/36450]\tLoss: 503.9878\n",
      "Training Epoch: 11 [26432/36450]\tLoss: 525.8447\n",
      "Training Epoch: 11 [26496/36450]\tLoss: 521.9570\n",
      "Training Epoch: 11 [26560/36450]\tLoss: 533.4506\n",
      "Training Epoch: 11 [26624/36450]\tLoss: 547.1535\n",
      "Training Epoch: 11 [26688/36450]\tLoss: 504.7187\n",
      "Training Epoch: 11 [26752/36450]\tLoss: 529.0354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [26816/36450]\tLoss: 502.8136\n",
      "Training Epoch: 11 [26880/36450]\tLoss: 517.6840\n",
      "Training Epoch: 11 [26944/36450]\tLoss: 522.9087\n",
      "Training Epoch: 11 [27008/36450]\tLoss: 506.9518\n",
      "Training Epoch: 11 [27072/36450]\tLoss: 490.6444\n",
      "Training Epoch: 11 [27136/36450]\tLoss: 515.4525\n",
      "Training Epoch: 11 [27200/36450]\tLoss: 485.1899\n",
      "Training Epoch: 11 [27264/36450]\tLoss: 520.2363\n",
      "Training Epoch: 11 [27328/36450]\tLoss: 538.9066\n",
      "Training Epoch: 11 [27392/36450]\tLoss: 515.3931\n",
      "Training Epoch: 11 [27456/36450]\tLoss: 512.7652\n",
      "Training Epoch: 11 [27520/36450]\tLoss: 546.8666\n",
      "Training Epoch: 11 [27584/36450]\tLoss: 522.3322\n",
      "Training Epoch: 11 [27648/36450]\tLoss: 533.0010\n",
      "Training Epoch: 11 [27712/36450]\tLoss: 504.6523\n",
      "Training Epoch: 11 [27776/36450]\tLoss: 567.4487\n",
      "Training Epoch: 11 [27840/36450]\tLoss: 533.1191\n",
      "Training Epoch: 11 [27904/36450]\tLoss: 551.2981\n",
      "Training Epoch: 11 [27968/36450]\tLoss: 548.4458\n",
      "Training Epoch: 11 [28032/36450]\tLoss: 541.5305\n",
      "Training Epoch: 11 [28096/36450]\tLoss: 570.4406\n",
      "Training Epoch: 11 [28160/36450]\tLoss: 584.7534\n",
      "Training Epoch: 11 [28224/36450]\tLoss: 548.5302\n",
      "Training Epoch: 11 [28288/36450]\tLoss: 597.2948\n",
      "Training Epoch: 11 [28352/36450]\tLoss: 568.9893\n",
      "Training Epoch: 11 [28416/36450]\tLoss: 555.4164\n",
      "Training Epoch: 11 [28480/36450]\tLoss: 564.7654\n",
      "Training Epoch: 11 [28544/36450]\tLoss: 555.9522\n",
      "Training Epoch: 11 [28608/36450]\tLoss: 544.9394\n",
      "Training Epoch: 11 [28672/36450]\tLoss: 508.3609\n",
      "Training Epoch: 11 [28736/36450]\tLoss: 523.4924\n",
      "Training Epoch: 11 [28800/36450]\tLoss: 491.8965\n",
      "Training Epoch: 11 [28864/36450]\tLoss: 548.4098\n",
      "Training Epoch: 11 [28928/36450]\tLoss: 533.6804\n",
      "Training Epoch: 11 [28992/36450]\tLoss: 516.7170\n",
      "Training Epoch: 11 [29056/36450]\tLoss: 517.4452\n",
      "Training Epoch: 11 [29120/36450]\tLoss: 547.7686\n",
      "Training Epoch: 11 [29184/36450]\tLoss: 512.6286\n",
      "Training Epoch: 11 [29248/36450]\tLoss: 570.9246\n",
      "Training Epoch: 11 [29312/36450]\tLoss: 504.6011\n",
      "Training Epoch: 11 [29376/36450]\tLoss: 561.9890\n",
      "Training Epoch: 11 [29440/36450]\tLoss: 542.7803\n",
      "Training Epoch: 11 [29504/36450]\tLoss: 563.9445\n",
      "Training Epoch: 11 [29568/36450]\tLoss: 541.4704\n",
      "Training Epoch: 11 [29632/36450]\tLoss: 570.0176\n",
      "Training Epoch: 11 [29696/36450]\tLoss: 558.4510\n",
      "Training Epoch: 11 [29760/36450]\tLoss: 544.5749\n",
      "Training Epoch: 11 [29824/36450]\tLoss: 506.0860\n",
      "Training Epoch: 11 [29888/36450]\tLoss: 550.3221\n",
      "Training Epoch: 11 [29952/36450]\tLoss: 541.5485\n",
      "Training Epoch: 11 [30016/36450]\tLoss: 509.6539\n",
      "Training Epoch: 11 [30080/36450]\tLoss: 529.7503\n",
      "Training Epoch: 11 [30144/36450]\tLoss: 525.6832\n",
      "Training Epoch: 11 [30208/36450]\tLoss: 531.6233\n",
      "Training Epoch: 11 [30272/36450]\tLoss: 549.5566\n",
      "Training Epoch: 11 [30336/36450]\tLoss: 525.9669\n",
      "Training Epoch: 11 [30400/36450]\tLoss: 547.1702\n",
      "Training Epoch: 11 [30464/36450]\tLoss: 546.9379\n",
      "Training Epoch: 11 [30528/36450]\tLoss: 502.4753\n",
      "Training Epoch: 11 [30592/36450]\tLoss: 525.9512\n",
      "Training Epoch: 11 [30656/36450]\tLoss: 525.1965\n",
      "Training Epoch: 11 [30720/36450]\tLoss: 505.9962\n",
      "Training Epoch: 11 [30784/36450]\tLoss: 540.0152\n",
      "Training Epoch: 11 [30848/36450]\tLoss: 531.5389\n",
      "Training Epoch: 11 [30912/36450]\tLoss: 542.0799\n",
      "Training Epoch: 11 [30976/36450]\tLoss: 544.1038\n",
      "Training Epoch: 11 [31040/36450]\tLoss: 516.9880\n",
      "Training Epoch: 11 [31104/36450]\tLoss: 540.3057\n",
      "Training Epoch: 11 [31168/36450]\tLoss: 523.3024\n",
      "Training Epoch: 11 [31232/36450]\tLoss: 510.5016\n",
      "Training Epoch: 11 [31296/36450]\tLoss: 511.2291\n",
      "Training Epoch: 11 [31360/36450]\tLoss: 549.0956\n",
      "Training Epoch: 11 [31424/36450]\tLoss: 524.6891\n",
      "Training Epoch: 11 [31488/36450]\tLoss: 482.5573\n",
      "Training Epoch: 11 [31552/36450]\tLoss: 545.3991\n",
      "Training Epoch: 11 [31616/36450]\tLoss: 524.4232\n",
      "Training Epoch: 11 [31680/36450]\tLoss: 521.1923\n",
      "Training Epoch: 11 [31744/36450]\tLoss: 523.8841\n",
      "Training Epoch: 11 [31808/36450]\tLoss: 524.8105\n",
      "Training Epoch: 11 [31872/36450]\tLoss: 514.9016\n",
      "Training Epoch: 11 [31936/36450]\tLoss: 584.0642\n",
      "Training Epoch: 11 [32000/36450]\tLoss: 560.4612\n",
      "Training Epoch: 11 [32064/36450]\tLoss: 496.7632\n",
      "Training Epoch: 11 [32128/36450]\tLoss: 525.5084\n",
      "Training Epoch: 11 [32192/36450]\tLoss: 540.7332\n",
      "Training Epoch: 11 [32256/36450]\tLoss: 528.8980\n",
      "Training Epoch: 11 [32320/36450]\tLoss: 518.6631\n",
      "Training Epoch: 11 [32384/36450]\tLoss: 519.0135\n",
      "Training Epoch: 11 [32448/36450]\tLoss: 512.0334\n",
      "Training Epoch: 11 [32512/36450]\tLoss: 522.9939\n",
      "Training Epoch: 11 [32576/36450]\tLoss: 519.7762\n",
      "Training Epoch: 11 [32640/36450]\tLoss: 526.6078\n",
      "Training Epoch: 11 [32704/36450]\tLoss: 542.0244\n",
      "Training Epoch: 11 [32768/36450]\tLoss: 501.5381\n",
      "Training Epoch: 11 [32832/36450]\tLoss: 524.2170\n",
      "Training Epoch: 11 [32896/36450]\tLoss: 530.3932\n",
      "Training Epoch: 11 [32960/36450]\tLoss: 526.8075\n",
      "Training Epoch: 11 [33024/36450]\tLoss: 511.0061\n",
      "Training Epoch: 11 [33088/36450]\tLoss: 529.9695\n",
      "Training Epoch: 11 [33152/36450]\tLoss: 509.2550\n",
      "Training Epoch: 11 [33216/36450]\tLoss: 524.8318\n",
      "Training Epoch: 11 [33280/36450]\tLoss: 508.4402\n",
      "Training Epoch: 11 [33344/36450]\tLoss: 553.5434\n",
      "Training Epoch: 11 [33408/36450]\tLoss: 538.8959\n",
      "Training Epoch: 11 [33472/36450]\tLoss: 554.9559\n",
      "Training Epoch: 11 [33536/36450]\tLoss: 510.7819\n",
      "Training Epoch: 11 [33600/36450]\tLoss: 516.8706\n",
      "Training Epoch: 11 [33664/36450]\tLoss: 516.4082\n",
      "Training Epoch: 11 [33728/36450]\tLoss: 492.7884\n",
      "Training Epoch: 11 [33792/36450]\tLoss: 523.0271\n",
      "Training Epoch: 11 [33856/36450]\tLoss: 509.2031\n",
      "Training Epoch: 11 [33920/36450]\tLoss: 513.3405\n",
      "Training Epoch: 11 [33984/36450]\tLoss: 518.5116\n",
      "Training Epoch: 11 [34048/36450]\tLoss: 525.3319\n",
      "Training Epoch: 11 [34112/36450]\tLoss: 530.2604\n",
      "Training Epoch: 11 [34176/36450]\tLoss: 508.4315\n",
      "Training Epoch: 11 [34240/36450]\tLoss: 545.5173\n",
      "Training Epoch: 11 [34304/36450]\tLoss: 519.8889\n",
      "Training Epoch: 11 [34368/36450]\tLoss: 494.7284\n",
      "Training Epoch: 11 [34432/36450]\tLoss: 553.0142\n",
      "Training Epoch: 11 [34496/36450]\tLoss: 535.1724\n",
      "Training Epoch: 11 [34560/36450]\tLoss: 541.6546\n",
      "Training Epoch: 11 [34624/36450]\tLoss: 547.2584\n",
      "Training Epoch: 11 [34688/36450]\tLoss: 493.5569\n",
      "Training Epoch: 11 [34752/36450]\tLoss: 497.3158\n",
      "Training Epoch: 11 [34816/36450]\tLoss: 530.7654\n",
      "Training Epoch: 11 [34880/36450]\tLoss: 522.1432\n",
      "Training Epoch: 11 [34944/36450]\tLoss: 524.1152\n",
      "Training Epoch: 11 [35008/36450]\tLoss: 508.0445\n",
      "Training Epoch: 11 [35072/36450]\tLoss: 506.5344\n",
      "Training Epoch: 11 [35136/36450]\tLoss: 508.4272\n",
      "Training Epoch: 11 [35200/36450]\tLoss: 502.9857\n",
      "Training Epoch: 11 [35264/36450]\tLoss: 536.1428\n",
      "Training Epoch: 11 [35328/36450]\tLoss: 526.5942\n",
      "Training Epoch: 11 [35392/36450]\tLoss: 512.9521\n",
      "Training Epoch: 11 [35456/36450]\tLoss: 523.8643\n",
      "Training Epoch: 11 [35520/36450]\tLoss: 517.8298\n",
      "Training Epoch: 11 [35584/36450]\tLoss: 538.4623\n",
      "Training Epoch: 11 [35648/36450]\tLoss: 528.1995\n",
      "Training Epoch: 11 [35712/36450]\tLoss: 508.3446\n",
      "Training Epoch: 11 [35776/36450]\tLoss: 526.4122\n",
      "Training Epoch: 11 [35840/36450]\tLoss: 502.5895\n",
      "Training Epoch: 11 [35904/36450]\tLoss: 541.2821\n",
      "Training Epoch: 11 [35968/36450]\tLoss: 514.1041\n",
      "Training Epoch: 11 [36032/36450]\tLoss: 524.4020\n",
      "Training Epoch: 11 [36096/36450]\tLoss: 542.8990\n",
      "Training Epoch: 11 [36160/36450]\tLoss: 570.7018\n",
      "Training Epoch: 11 [36224/36450]\tLoss: 553.7720\n",
      "Training Epoch: 11 [36288/36450]\tLoss: 555.0131\n",
      "Training Epoch: 11 [36352/36450]\tLoss: 536.7213\n",
      "Training Epoch: 11 [36416/36450]\tLoss: 575.0864\n",
      "Training Epoch: 11 [36450/36450]\tLoss: 517.5269\n",
      "Training Epoch: 11 [4050/4050]\tLoss: 265.4893\n",
      "Training Epoch: 12 [64/36450]\tLoss: 544.6044\n",
      "Training Epoch: 12 [128/36450]\tLoss: 527.7451\n",
      "Training Epoch: 12 [192/36450]\tLoss: 544.0350\n",
      "Training Epoch: 12 [256/36450]\tLoss: 538.4838\n",
      "Training Epoch: 12 [320/36450]\tLoss: 513.0776\n",
      "Training Epoch: 12 [384/36450]\tLoss: 522.2810\n",
      "Training Epoch: 12 [448/36450]\tLoss: 548.1296\n",
      "Training Epoch: 12 [512/36450]\tLoss: 491.2594\n",
      "Training Epoch: 12 [576/36450]\tLoss: 536.6533\n",
      "Training Epoch: 12 [640/36450]\tLoss: 543.1627\n",
      "Training Epoch: 12 [704/36450]\tLoss: 539.4559\n",
      "Training Epoch: 12 [768/36450]\tLoss: 542.5098\n",
      "Training Epoch: 12 [832/36450]\tLoss: 515.3818\n",
      "Training Epoch: 12 [896/36450]\tLoss: 521.7758\n",
      "Training Epoch: 12 [960/36450]\tLoss: 546.4989\n",
      "Training Epoch: 12 [1024/36450]\tLoss: 533.1107\n",
      "Training Epoch: 12 [1088/36450]\tLoss: 518.8090\n",
      "Training Epoch: 12 [1152/36450]\tLoss: 553.2592\n",
      "Training Epoch: 12 [1216/36450]\tLoss: 493.4283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [1280/36450]\tLoss: 517.5110\n",
      "Training Epoch: 12 [1344/36450]\tLoss: 512.9994\n",
      "Training Epoch: 12 [1408/36450]\tLoss: 512.8297\n",
      "Training Epoch: 12 [1472/36450]\tLoss: 555.5016\n",
      "Training Epoch: 12 [1536/36450]\tLoss: 523.3694\n",
      "Training Epoch: 12 [1600/36450]\tLoss: 503.4222\n",
      "Training Epoch: 12 [1664/36450]\tLoss: 511.0957\n",
      "Training Epoch: 12 [1728/36450]\tLoss: 509.1997\n",
      "Training Epoch: 12 [1792/36450]\tLoss: 542.7844\n",
      "Training Epoch: 12 [1856/36450]\tLoss: 539.9150\n",
      "Training Epoch: 12 [1920/36450]\tLoss: 523.0377\n",
      "Training Epoch: 12 [1984/36450]\tLoss: 501.9664\n",
      "Training Epoch: 12 [2048/36450]\tLoss: 530.0352\n",
      "Training Epoch: 12 [2112/36450]\tLoss: 522.8915\n",
      "Training Epoch: 12 [2176/36450]\tLoss: 535.5493\n",
      "Training Epoch: 12 [2240/36450]\tLoss: 523.6829\n",
      "Training Epoch: 12 [2304/36450]\tLoss: 506.5491\n",
      "Training Epoch: 12 [2368/36450]\tLoss: 519.2759\n",
      "Training Epoch: 12 [2432/36450]\tLoss: 522.5368\n",
      "Training Epoch: 12 [2496/36450]\tLoss: 515.1252\n",
      "Training Epoch: 12 [2560/36450]\tLoss: 525.1011\n",
      "Training Epoch: 12 [2624/36450]\tLoss: 519.8087\n",
      "Training Epoch: 12 [2688/36450]\tLoss: 517.8497\n",
      "Training Epoch: 12 [2752/36450]\tLoss: 492.1710\n",
      "Training Epoch: 12 [2816/36450]\tLoss: 512.4245\n",
      "Training Epoch: 12 [2880/36450]\tLoss: 537.2459\n",
      "Training Epoch: 12 [2944/36450]\tLoss: 528.0727\n",
      "Training Epoch: 12 [3008/36450]\tLoss: 485.0810\n",
      "Training Epoch: 12 [3072/36450]\tLoss: 527.8425\n",
      "Training Epoch: 12 [3136/36450]\tLoss: 496.2238\n",
      "Training Epoch: 12 [3200/36450]\tLoss: 502.1706\n",
      "Training Epoch: 12 [3264/36450]\tLoss: 509.3180\n",
      "Training Epoch: 12 [3328/36450]\tLoss: 517.5370\n",
      "Training Epoch: 12 [3392/36450]\tLoss: 507.2184\n",
      "Training Epoch: 12 [3456/36450]\tLoss: 498.7778\n",
      "Training Epoch: 12 [3520/36450]\tLoss: 519.7941\n",
      "Training Epoch: 12 [3584/36450]\tLoss: 499.1106\n",
      "Training Epoch: 12 [3648/36450]\tLoss: 544.4852\n",
      "Training Epoch: 12 [3712/36450]\tLoss: 521.4480\n",
      "Training Epoch: 12 [3776/36450]\tLoss: 508.4496\n",
      "Training Epoch: 12 [3840/36450]\tLoss: 526.6666\n",
      "Training Epoch: 12 [3904/36450]\tLoss: 506.9847\n",
      "Training Epoch: 12 [3968/36450]\tLoss: 519.5347\n",
      "Training Epoch: 12 [4032/36450]\tLoss: 495.3354\n",
      "Training Epoch: 12 [4096/36450]\tLoss: 513.9645\n",
      "Training Epoch: 12 [4160/36450]\tLoss: 517.0845\n",
      "Training Epoch: 12 [4224/36450]\tLoss: 531.5835\n",
      "Training Epoch: 12 [4288/36450]\tLoss: 496.3166\n",
      "Training Epoch: 12 [4352/36450]\tLoss: 521.7930\n",
      "Training Epoch: 12 [4416/36450]\tLoss: 542.0401\n",
      "Training Epoch: 12 [4480/36450]\tLoss: 517.4957\n",
      "Training Epoch: 12 [4544/36450]\tLoss: 535.8663\n",
      "Training Epoch: 12 [4608/36450]\tLoss: 503.2979\n",
      "Training Epoch: 12 [4672/36450]\tLoss: 527.7007\n",
      "Training Epoch: 12 [4736/36450]\tLoss: 510.3715\n",
      "Training Epoch: 12 [4800/36450]\tLoss: 528.2107\n",
      "Training Epoch: 12 [4864/36450]\tLoss: 518.9061\n",
      "Training Epoch: 12 [4928/36450]\tLoss: 542.5995\n",
      "Training Epoch: 12 [4992/36450]\tLoss: 558.2692\n",
      "Training Epoch: 12 [5056/36450]\tLoss: 531.0127\n",
      "Training Epoch: 12 [5120/36450]\tLoss: 582.3345\n",
      "Training Epoch: 12 [5184/36450]\tLoss: 589.4284\n",
      "Training Epoch: 12 [5248/36450]\tLoss: 596.2512\n",
      "Training Epoch: 12 [5312/36450]\tLoss: 604.9271\n",
      "Training Epoch: 12 [5376/36450]\tLoss: 611.0897\n",
      "Training Epoch: 12 [5440/36450]\tLoss: 570.4865\n",
      "Training Epoch: 12 [5504/36450]\tLoss: 557.2067\n",
      "Training Epoch: 12 [5568/36450]\tLoss: 530.1429\n",
      "Training Epoch: 12 [5632/36450]\tLoss: 521.7214\n",
      "Training Epoch: 12 [5696/36450]\tLoss: 569.8208\n",
      "Training Epoch: 12 [5760/36450]\tLoss: 555.9407\n",
      "Training Epoch: 12 [5824/36450]\tLoss: 595.5328\n",
      "Training Epoch: 12 [5888/36450]\tLoss: 572.9822\n",
      "Training Epoch: 12 [5952/36450]\tLoss: 530.5648\n",
      "Training Epoch: 12 [6016/36450]\tLoss: 525.0016\n",
      "Training Epoch: 12 [6080/36450]\tLoss: 524.2575\n",
      "Training Epoch: 12 [6144/36450]\tLoss: 542.5715\n",
      "Training Epoch: 12 [6208/36450]\tLoss: 564.4906\n",
      "Training Epoch: 12 [6272/36450]\tLoss: 547.4802\n",
      "Training Epoch: 12 [6336/36450]\tLoss: 531.8896\n",
      "Training Epoch: 12 [6400/36450]\tLoss: 536.0991\n",
      "Training Epoch: 12 [6464/36450]\tLoss: 527.9728\n",
      "Training Epoch: 12 [6528/36450]\tLoss: 552.8669\n",
      "Training Epoch: 12 [6592/36450]\tLoss: 551.8964\n",
      "Training Epoch: 12 [6656/36450]\tLoss: 489.6740\n",
      "Training Epoch: 12 [6720/36450]\tLoss: 521.3911\n",
      "Training Epoch: 12 [6784/36450]\tLoss: 515.6301\n",
      "Training Epoch: 12 [6848/36450]\tLoss: 513.6970\n",
      "Training Epoch: 12 [6912/36450]\tLoss: 560.1415\n",
      "Training Epoch: 12 [6976/36450]\tLoss: 532.5975\n",
      "Training Epoch: 12 [7040/36450]\tLoss: 512.1863\n",
      "Training Epoch: 12 [7104/36450]\tLoss: 535.4707\n",
      "Training Epoch: 12 [7168/36450]\tLoss: 522.2377\n",
      "Training Epoch: 12 [7232/36450]\tLoss: 534.2617\n",
      "Training Epoch: 12 [7296/36450]\tLoss: 519.2233\n",
      "Training Epoch: 12 [7360/36450]\tLoss: 521.6519\n",
      "Training Epoch: 12 [7424/36450]\tLoss: 522.4870\n",
      "Training Epoch: 12 [7488/36450]\tLoss: 530.2661\n",
      "Training Epoch: 12 [7552/36450]\tLoss: 521.2686\n",
      "Training Epoch: 12 [7616/36450]\tLoss: 528.8159\n",
      "Training Epoch: 12 [7680/36450]\tLoss: 507.9714\n",
      "Training Epoch: 12 [7744/36450]\tLoss: 538.8267\n",
      "Training Epoch: 12 [7808/36450]\tLoss: 521.5284\n",
      "Training Epoch: 12 [7872/36450]\tLoss: 548.0247\n",
      "Training Epoch: 12 [7936/36450]\tLoss: 525.5496\n",
      "Training Epoch: 12 [8000/36450]\tLoss: 496.6909\n",
      "Training Epoch: 12 [8064/36450]\tLoss: 500.1116\n",
      "Training Epoch: 12 [8128/36450]\tLoss: 532.1455\n",
      "Training Epoch: 12 [8192/36450]\tLoss: 536.5419\n",
      "Training Epoch: 12 [8256/36450]\tLoss: 550.9813\n",
      "Training Epoch: 12 [8320/36450]\tLoss: 531.6503\n",
      "Training Epoch: 12 [8384/36450]\tLoss: 515.7745\n",
      "Training Epoch: 12 [8448/36450]\tLoss: 525.6088\n",
      "Training Epoch: 12 [8512/36450]\tLoss: 534.8068\n",
      "Training Epoch: 12 [8576/36450]\tLoss: 543.4412\n",
      "Training Epoch: 12 [8640/36450]\tLoss: 517.5533\n",
      "Training Epoch: 12 [8704/36450]\tLoss: 532.7172\n",
      "Training Epoch: 12 [8768/36450]\tLoss: 523.2449\n",
      "Training Epoch: 12 [8832/36450]\tLoss: 545.6115\n",
      "Training Epoch: 12 [8896/36450]\tLoss: 526.1663\n",
      "Training Epoch: 12 [8960/36450]\tLoss: 519.8932\n",
      "Training Epoch: 12 [9024/36450]\tLoss: 509.5787\n",
      "Training Epoch: 12 [9088/36450]\tLoss: 520.8819\n",
      "Training Epoch: 12 [9152/36450]\tLoss: 491.5301\n",
      "Training Epoch: 12 [9216/36450]\tLoss: 517.2579\n",
      "Training Epoch: 12 [9280/36450]\tLoss: 529.9415\n",
      "Training Epoch: 12 [9344/36450]\tLoss: 547.5413\n",
      "Training Epoch: 12 [9408/36450]\tLoss: 503.5124\n",
      "Training Epoch: 12 [9472/36450]\tLoss: 535.1239\n",
      "Training Epoch: 12 [9536/36450]\tLoss: 525.4656\n",
      "Training Epoch: 12 [9600/36450]\tLoss: 506.9505\n",
      "Training Epoch: 12 [9664/36450]\tLoss: 544.3276\n",
      "Training Epoch: 12 [9728/36450]\tLoss: 527.6234\n",
      "Training Epoch: 12 [9792/36450]\tLoss: 533.3805\n",
      "Training Epoch: 12 [9856/36450]\tLoss: 560.9447\n",
      "Training Epoch: 12 [9920/36450]\tLoss: 519.9838\n",
      "Training Epoch: 12 [9984/36450]\tLoss: 513.5159\n",
      "Training Epoch: 12 [10048/36450]\tLoss: 495.2386\n",
      "Training Epoch: 12 [10112/36450]\tLoss: 542.0729\n",
      "Training Epoch: 12 [10176/36450]\tLoss: 520.2903\n",
      "Training Epoch: 12 [10240/36450]\tLoss: 517.8085\n",
      "Training Epoch: 12 [10304/36450]\tLoss: 492.0542\n",
      "Training Epoch: 12 [10368/36450]\tLoss: 509.7966\n",
      "Training Epoch: 12 [10432/36450]\tLoss: 518.8959\n",
      "Training Epoch: 12 [10496/36450]\tLoss: 537.7178\n",
      "Training Epoch: 12 [10560/36450]\tLoss: 527.8885\n",
      "Training Epoch: 12 [10624/36450]\tLoss: 489.8100\n",
      "Training Epoch: 12 [10688/36450]\tLoss: 536.7300\n",
      "Training Epoch: 12 [10752/36450]\tLoss: 511.0448\n",
      "Training Epoch: 12 [10816/36450]\tLoss: 487.5717\n",
      "Training Epoch: 12 [10880/36450]\tLoss: 524.4620\n",
      "Training Epoch: 12 [10944/36450]\tLoss: 510.4566\n",
      "Training Epoch: 12 [11008/36450]\tLoss: 520.8867\n",
      "Training Epoch: 12 [11072/36450]\tLoss: 510.5556\n",
      "Training Epoch: 12 [11136/36450]\tLoss: 543.5317\n",
      "Training Epoch: 12 [11200/36450]\tLoss: 524.8867\n",
      "Training Epoch: 12 [11264/36450]\tLoss: 505.8304\n",
      "Training Epoch: 12 [11328/36450]\tLoss: 507.4078\n",
      "Training Epoch: 12 [11392/36450]\tLoss: 493.2444\n",
      "Training Epoch: 12 [11456/36450]\tLoss: 505.8512\n",
      "Training Epoch: 12 [11520/36450]\tLoss: 506.9129\n",
      "Training Epoch: 12 [11584/36450]\tLoss: 534.4892\n",
      "Training Epoch: 12 [11648/36450]\tLoss: 565.9389\n",
      "Training Epoch: 12 [11712/36450]\tLoss: 508.8523\n",
      "Training Epoch: 12 [11776/36450]\tLoss: 533.9789\n",
      "Training Epoch: 12 [11840/36450]\tLoss: 538.2833\n",
      "Training Epoch: 12 [11904/36450]\tLoss: 524.2213\n",
      "Training Epoch: 12 [11968/36450]\tLoss: 499.5006\n",
      "Training Epoch: 12 [12032/36450]\tLoss: 545.3925\n",
      "Training Epoch: 12 [12096/36450]\tLoss: 527.7442\n",
      "Training Epoch: 12 [12160/36450]\tLoss: 530.4436\n",
      "Training Epoch: 12 [12224/36450]\tLoss: 512.0840\n",
      "Training Epoch: 12 [12288/36450]\tLoss: 497.0680\n",
      "Training Epoch: 12 [12352/36450]\tLoss: 490.6210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [12416/36450]\tLoss: 525.3683\n",
      "Training Epoch: 12 [12480/36450]\tLoss: 534.0090\n",
      "Training Epoch: 12 [12544/36450]\tLoss: 516.3747\n",
      "Training Epoch: 12 [12608/36450]\tLoss: 502.6847\n",
      "Training Epoch: 12 [12672/36450]\tLoss: 530.4276\n",
      "Training Epoch: 12 [12736/36450]\tLoss: 504.4936\n",
      "Training Epoch: 12 [12800/36450]\tLoss: 511.1722\n",
      "Training Epoch: 12 [12864/36450]\tLoss: 513.2616\n",
      "Training Epoch: 12 [12928/36450]\tLoss: 525.0758\n",
      "Training Epoch: 12 [12992/36450]\tLoss: 538.3419\n",
      "Training Epoch: 12 [13056/36450]\tLoss: 510.1667\n",
      "Training Epoch: 12 [13120/36450]\tLoss: 522.6121\n",
      "Training Epoch: 12 [13184/36450]\tLoss: 522.1537\n",
      "Training Epoch: 12 [13248/36450]\tLoss: 511.3102\n",
      "Training Epoch: 12 [13312/36450]\tLoss: 499.1314\n",
      "Training Epoch: 12 [13376/36450]\tLoss: 506.7665\n",
      "Training Epoch: 12 [13440/36450]\tLoss: 498.7794\n",
      "Training Epoch: 12 [13504/36450]\tLoss: 535.8243\n",
      "Training Epoch: 12 [13568/36450]\tLoss: 511.0956\n",
      "Training Epoch: 12 [13632/36450]\tLoss: 534.4389\n",
      "Training Epoch: 12 [13696/36450]\tLoss: 509.4924\n",
      "Training Epoch: 12 [13760/36450]\tLoss: 542.8696\n",
      "Training Epoch: 12 [13824/36450]\tLoss: 499.6050\n",
      "Training Epoch: 12 [13888/36450]\tLoss: 513.5471\n",
      "Training Epoch: 12 [13952/36450]\tLoss: 523.8442\n",
      "Training Epoch: 12 [14016/36450]\tLoss: 535.3566\n",
      "Training Epoch: 12 [14080/36450]\tLoss: 492.9611\n",
      "Training Epoch: 12 [14144/36450]\tLoss: 522.1317\n",
      "Training Epoch: 12 [14208/36450]\tLoss: 524.4633\n",
      "Training Epoch: 12 [14272/36450]\tLoss: 535.4284\n",
      "Training Epoch: 12 [14336/36450]\tLoss: 486.8311\n",
      "Training Epoch: 12 [14400/36450]\tLoss: 516.2432\n",
      "Training Epoch: 12 [14464/36450]\tLoss: 501.7328\n",
      "Training Epoch: 12 [14528/36450]\tLoss: 519.8661\n",
      "Training Epoch: 12 [14592/36450]\tLoss: 544.8151\n",
      "Training Epoch: 12 [14656/36450]\tLoss: 531.2945\n",
      "Training Epoch: 12 [14720/36450]\tLoss: 521.0138\n",
      "Training Epoch: 12 [14784/36450]\tLoss: 501.4050\n",
      "Training Epoch: 12 [14848/36450]\tLoss: 499.6000\n",
      "Training Epoch: 12 [14912/36450]\tLoss: 542.6054\n",
      "Training Epoch: 12 [14976/36450]\tLoss: 509.1812\n",
      "Training Epoch: 12 [15040/36450]\tLoss: 519.7633\n",
      "Training Epoch: 12 [15104/36450]\tLoss: 564.7261\n",
      "Training Epoch: 12 [15168/36450]\tLoss: 528.1838\n",
      "Training Epoch: 12 [15232/36450]\tLoss: 527.3342\n",
      "Training Epoch: 12 [15296/36450]\tLoss: 539.9829\n",
      "Training Epoch: 12 [15360/36450]\tLoss: 541.4605\n",
      "Training Epoch: 12 [15424/36450]\tLoss: 527.2384\n",
      "Training Epoch: 12 [15488/36450]\tLoss: 508.1649\n",
      "Training Epoch: 12 [15552/36450]\tLoss: 517.7074\n",
      "Training Epoch: 12 [15616/36450]\tLoss: 548.0443\n",
      "Training Epoch: 12 [15680/36450]\tLoss: 506.7253\n",
      "Training Epoch: 12 [15744/36450]\tLoss: 565.3571\n",
      "Training Epoch: 12 [15808/36450]\tLoss: 518.9009\n",
      "Training Epoch: 12 [15872/36450]\tLoss: 510.3076\n",
      "Training Epoch: 12 [15936/36450]\tLoss: 513.4549\n",
      "Training Epoch: 12 [16000/36450]\tLoss: 559.6381\n",
      "Training Epoch: 12 [16064/36450]\tLoss: 544.1635\n",
      "Training Epoch: 12 [16128/36450]\tLoss: 481.4182\n",
      "Training Epoch: 12 [16192/36450]\tLoss: 555.3548\n",
      "Training Epoch: 12 [16256/36450]\tLoss: 536.4918\n",
      "Training Epoch: 12 [16320/36450]\tLoss: 493.1724\n",
      "Training Epoch: 12 [16384/36450]\tLoss: 509.5864\n",
      "Training Epoch: 12 [16448/36450]\tLoss: 530.2191\n",
      "Training Epoch: 12 [16512/36450]\tLoss: 547.0261\n",
      "Training Epoch: 12 [16576/36450]\tLoss: 528.1310\n",
      "Training Epoch: 12 [16640/36450]\tLoss: 509.7482\n",
      "Training Epoch: 12 [16704/36450]\tLoss: 538.8752\n",
      "Training Epoch: 12 [16768/36450]\tLoss: 528.0612\n",
      "Training Epoch: 12 [16832/36450]\tLoss: 507.0015\n",
      "Training Epoch: 12 [16896/36450]\tLoss: 516.6713\n",
      "Training Epoch: 12 [16960/36450]\tLoss: 519.7661\n",
      "Training Epoch: 12 [17024/36450]\tLoss: 542.5350\n",
      "Training Epoch: 12 [17088/36450]\tLoss: 523.6038\n",
      "Training Epoch: 12 [17152/36450]\tLoss: 517.5776\n",
      "Training Epoch: 12 [17216/36450]\tLoss: 492.7945\n",
      "Training Epoch: 12 [17280/36450]\tLoss: 539.7307\n",
      "Training Epoch: 12 [17344/36450]\tLoss: 525.3547\n",
      "Training Epoch: 12 [17408/36450]\tLoss: 492.6501\n",
      "Training Epoch: 12 [17472/36450]\tLoss: 537.0570\n",
      "Training Epoch: 12 [17536/36450]\tLoss: 532.0795\n",
      "Training Epoch: 12 [17600/36450]\tLoss: 514.8552\n",
      "Training Epoch: 12 [17664/36450]\tLoss: 495.7820\n",
      "Training Epoch: 12 [17728/36450]\tLoss: 530.1676\n",
      "Training Epoch: 12 [17792/36450]\tLoss: 523.5060\n",
      "Training Epoch: 12 [17856/36450]\tLoss: 494.6712\n",
      "Training Epoch: 12 [17920/36450]\tLoss: 518.4842\n",
      "Training Epoch: 12 [17984/36450]\tLoss: 495.2548\n",
      "Training Epoch: 12 [18048/36450]\tLoss: 492.1301\n",
      "Training Epoch: 12 [18112/36450]\tLoss: 510.0616\n",
      "Training Epoch: 12 [18176/36450]\tLoss: 504.5851\n",
      "Training Epoch: 12 [18240/36450]\tLoss: 484.2242\n",
      "Training Epoch: 12 [18304/36450]\tLoss: 526.3051\n",
      "Training Epoch: 12 [18368/36450]\tLoss: 526.6453\n",
      "Training Epoch: 12 [18432/36450]\tLoss: 515.3048\n",
      "Training Epoch: 12 [18496/36450]\tLoss: 514.3251\n",
      "Training Epoch: 12 [18560/36450]\tLoss: 499.8054\n",
      "Training Epoch: 12 [18624/36450]\tLoss: 521.5628\n",
      "Training Epoch: 12 [18688/36450]\tLoss: 522.9418\n",
      "Training Epoch: 12 [18752/36450]\tLoss: 525.3356\n",
      "Training Epoch: 12 [18816/36450]\tLoss: 495.8867\n",
      "Training Epoch: 12 [18880/36450]\tLoss: 512.3817\n",
      "Training Epoch: 12 [18944/36450]\tLoss: 519.4868\n",
      "Training Epoch: 12 [19008/36450]\tLoss: 513.9557\n",
      "Training Epoch: 12 [19072/36450]\tLoss: 529.5633\n",
      "Training Epoch: 12 [19136/36450]\tLoss: 514.6396\n",
      "Training Epoch: 12 [19200/36450]\tLoss: 530.3625\n",
      "Training Epoch: 12 [19264/36450]\tLoss: 523.1747\n",
      "Training Epoch: 12 [19328/36450]\tLoss: 488.8077\n",
      "Training Epoch: 12 [19392/36450]\tLoss: 520.9078\n",
      "Training Epoch: 12 [19456/36450]\tLoss: 517.1927\n",
      "Training Epoch: 12 [19520/36450]\tLoss: 523.7324\n",
      "Training Epoch: 12 [19584/36450]\tLoss: 523.5266\n",
      "Training Epoch: 12 [19648/36450]\tLoss: 528.5205\n",
      "Training Epoch: 12 [19712/36450]\tLoss: 496.0031\n",
      "Training Epoch: 12 [19776/36450]\tLoss: 512.8693\n",
      "Training Epoch: 12 [19840/36450]\tLoss: 510.5435\n",
      "Training Epoch: 12 [19904/36450]\tLoss: 535.0972\n",
      "Training Epoch: 12 [19968/36450]\tLoss: 502.0843\n",
      "Training Epoch: 12 [20032/36450]\tLoss: 539.5873\n",
      "Training Epoch: 12 [20096/36450]\tLoss: 532.0629\n",
      "Training Epoch: 12 [20160/36450]\tLoss: 535.2327\n",
      "Training Epoch: 12 [20224/36450]\tLoss: 534.2314\n",
      "Training Epoch: 12 [20288/36450]\tLoss: 514.7730\n",
      "Training Epoch: 12 [20352/36450]\tLoss: 508.6334\n",
      "Training Epoch: 12 [20416/36450]\tLoss: 534.6486\n",
      "Training Epoch: 12 [20480/36450]\tLoss: 524.0246\n",
      "Training Epoch: 12 [20544/36450]\tLoss: 504.0592\n",
      "Training Epoch: 12 [20608/36450]\tLoss: 507.3591\n",
      "Training Epoch: 12 [20672/36450]\tLoss: 528.3403\n",
      "Training Epoch: 12 [20736/36450]\tLoss: 490.2245\n",
      "Training Epoch: 12 [20800/36450]\tLoss: 514.3513\n",
      "Training Epoch: 12 [20864/36450]\tLoss: 507.1975\n",
      "Training Epoch: 12 [20928/36450]\tLoss: 554.6341\n",
      "Training Epoch: 12 [20992/36450]\tLoss: 518.4878\n",
      "Training Epoch: 12 [21056/36450]\tLoss: 503.7531\n",
      "Training Epoch: 12 [21120/36450]\tLoss: 514.9523\n",
      "Training Epoch: 12 [21184/36450]\tLoss: 530.1596\n",
      "Training Epoch: 12 [21248/36450]\tLoss: 496.6105\n",
      "Training Epoch: 12 [21312/36450]\tLoss: 513.8769\n",
      "Training Epoch: 12 [21376/36450]\tLoss: 546.1827\n",
      "Training Epoch: 12 [21440/36450]\tLoss: 519.4708\n",
      "Training Epoch: 12 [21504/36450]\tLoss: 528.9536\n",
      "Training Epoch: 12 [21568/36450]\tLoss: 538.0430\n",
      "Training Epoch: 12 [21632/36450]\tLoss: 515.0671\n",
      "Training Epoch: 12 [21696/36450]\tLoss: 551.3278\n",
      "Training Epoch: 12 [21760/36450]\tLoss: 547.9814\n",
      "Training Epoch: 12 [21824/36450]\tLoss: 525.1879\n",
      "Training Epoch: 12 [21888/36450]\tLoss: 526.1540\n",
      "Training Epoch: 12 [21952/36450]\tLoss: 499.9387\n",
      "Training Epoch: 12 [22016/36450]\tLoss: 544.2315\n",
      "Training Epoch: 12 [22080/36450]\tLoss: 521.0925\n",
      "Training Epoch: 12 [22144/36450]\tLoss: 509.6025\n",
      "Training Epoch: 12 [22208/36450]\tLoss: 512.9318\n",
      "Training Epoch: 12 [22272/36450]\tLoss: 518.2104\n",
      "Training Epoch: 12 [22336/36450]\tLoss: 528.2511\n",
      "Training Epoch: 12 [22400/36450]\tLoss: 523.5773\n",
      "Training Epoch: 12 [22464/36450]\tLoss: 488.3475\n",
      "Training Epoch: 12 [22528/36450]\tLoss: 508.2327\n",
      "Training Epoch: 12 [22592/36450]\tLoss: 510.9137\n",
      "Training Epoch: 12 [22656/36450]\tLoss: 535.9497\n",
      "Training Epoch: 12 [22720/36450]\tLoss: 534.8140\n",
      "Training Epoch: 12 [22784/36450]\tLoss: 477.9729\n",
      "Training Epoch: 12 [22848/36450]\tLoss: 510.0864\n",
      "Training Epoch: 12 [22912/36450]\tLoss: 500.4964\n",
      "Training Epoch: 12 [22976/36450]\tLoss: 534.3254\n",
      "Training Epoch: 12 [23040/36450]\tLoss: 511.2005\n",
      "Training Epoch: 12 [23104/36450]\tLoss: 511.1082\n",
      "Training Epoch: 12 [23168/36450]\tLoss: 523.6497\n",
      "Training Epoch: 12 [23232/36450]\tLoss: 528.0169\n",
      "Training Epoch: 12 [23296/36450]\tLoss: 513.4892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [23360/36450]\tLoss: 534.2836\n",
      "Training Epoch: 12 [23424/36450]\tLoss: 524.0158\n",
      "Training Epoch: 12 [23488/36450]\tLoss: 518.6716\n",
      "Training Epoch: 12 [23552/36450]\tLoss: 506.2742\n",
      "Training Epoch: 12 [23616/36450]\tLoss: 550.1322\n",
      "Training Epoch: 12 [23680/36450]\tLoss: 545.4986\n",
      "Training Epoch: 12 [23744/36450]\tLoss: 559.1839\n",
      "Training Epoch: 12 [23808/36450]\tLoss: 608.4158\n",
      "Training Epoch: 12 [23872/36450]\tLoss: 610.2067\n",
      "Training Epoch: 12 [23936/36450]\tLoss: 617.4125\n",
      "Training Epoch: 12 [24000/36450]\tLoss: 659.9434\n",
      "Training Epoch: 12 [24064/36450]\tLoss: 590.8010\n",
      "Training Epoch: 12 [24128/36450]\tLoss: 549.7812\n",
      "Training Epoch: 12 [24192/36450]\tLoss: 511.0061\n",
      "Training Epoch: 12 [24256/36450]\tLoss: 532.1655\n",
      "Training Epoch: 12 [24320/36450]\tLoss: 525.7781\n",
      "Training Epoch: 12 [24384/36450]\tLoss: 527.9583\n",
      "Training Epoch: 12 [24448/36450]\tLoss: 548.5728\n",
      "Training Epoch: 12 [24512/36450]\tLoss: 546.5620\n",
      "Training Epoch: 12 [24576/36450]\tLoss: 504.7701\n",
      "Training Epoch: 12 [24640/36450]\tLoss: 498.6843\n",
      "Training Epoch: 12 [24704/36450]\tLoss: 574.5101\n",
      "Training Epoch: 12 [24768/36450]\tLoss: 523.6409\n",
      "Training Epoch: 12 [24832/36450]\tLoss: 529.7443\n",
      "Training Epoch: 12 [24896/36450]\tLoss: 541.7659\n",
      "Training Epoch: 12 [24960/36450]\tLoss: 532.2385\n",
      "Training Epoch: 12 [25024/36450]\tLoss: 525.3945\n",
      "Training Epoch: 12 [25088/36450]\tLoss: 527.9328\n",
      "Training Epoch: 12 [25152/36450]\tLoss: 547.4602\n",
      "Training Epoch: 12 [25216/36450]\tLoss: 528.2471\n",
      "Training Epoch: 12 [25280/36450]\tLoss: 522.0345\n",
      "Training Epoch: 12 [25344/36450]\tLoss: 524.0728\n",
      "Training Epoch: 12 [25408/36450]\tLoss: 508.9153\n",
      "Training Epoch: 12 [25472/36450]\tLoss: 523.3250\n",
      "Training Epoch: 12 [25536/36450]\tLoss: 526.9514\n",
      "Training Epoch: 12 [25600/36450]\tLoss: 530.6915\n",
      "Training Epoch: 12 [25664/36450]\tLoss: 543.9628\n",
      "Training Epoch: 12 [25728/36450]\tLoss: 548.8745\n",
      "Training Epoch: 12 [25792/36450]\tLoss: 522.0179\n",
      "Training Epoch: 12 [25856/36450]\tLoss: 525.6350\n",
      "Training Epoch: 12 [25920/36450]\tLoss: 496.6367\n",
      "Training Epoch: 12 [25984/36450]\tLoss: 514.7329\n",
      "Training Epoch: 12 [26048/36450]\tLoss: 495.4397\n",
      "Training Epoch: 12 [26112/36450]\tLoss: 517.2085\n",
      "Training Epoch: 12 [26176/36450]\tLoss: 522.6924\n",
      "Training Epoch: 12 [26240/36450]\tLoss: 520.4857\n",
      "Training Epoch: 12 [26304/36450]\tLoss: 542.4302\n",
      "Training Epoch: 12 [26368/36450]\tLoss: 531.2035\n",
      "Training Epoch: 12 [26432/36450]\tLoss: 527.8702\n",
      "Training Epoch: 12 [26496/36450]\tLoss: 515.5245\n",
      "Training Epoch: 12 [26560/36450]\tLoss: 496.5350\n",
      "Training Epoch: 12 [26624/36450]\tLoss: 531.5722\n",
      "Training Epoch: 12 [26688/36450]\tLoss: 505.3690\n",
      "Training Epoch: 12 [26752/36450]\tLoss: 507.4137\n",
      "Training Epoch: 12 [26816/36450]\tLoss: 534.3048\n",
      "Training Epoch: 12 [26880/36450]\tLoss: 529.7496\n",
      "Training Epoch: 12 [26944/36450]\tLoss: 521.5043\n",
      "Training Epoch: 12 [27008/36450]\tLoss: 484.1387\n",
      "Training Epoch: 12 [27072/36450]\tLoss: 533.0082\n",
      "Training Epoch: 12 [27136/36450]\tLoss: 520.1979\n",
      "Training Epoch: 12 [27200/36450]\tLoss: 525.5006\n",
      "Training Epoch: 12 [27264/36450]\tLoss: 541.7358\n",
      "Training Epoch: 12 [27328/36450]\tLoss: 524.7224\n",
      "Training Epoch: 12 [27392/36450]\tLoss: 522.3587\n",
      "Training Epoch: 12 [27456/36450]\tLoss: 477.7098\n",
      "Training Epoch: 12 [27520/36450]\tLoss: 500.3921\n",
      "Training Epoch: 12 [27584/36450]\tLoss: 519.9244\n",
      "Training Epoch: 12 [27648/36450]\tLoss: 512.0449\n",
      "Training Epoch: 12 [27712/36450]\tLoss: 540.3835\n",
      "Training Epoch: 12 [27776/36450]\tLoss: 533.3577\n",
      "Training Epoch: 12 [27840/36450]\tLoss: 524.7946\n",
      "Training Epoch: 12 [27904/36450]\tLoss: 500.9458\n",
      "Training Epoch: 12 [27968/36450]\tLoss: 480.2920\n",
      "Training Epoch: 12 [28032/36450]\tLoss: 518.7206\n",
      "Training Epoch: 12 [28096/36450]\tLoss: 526.5281\n",
      "Training Epoch: 12 [28160/36450]\tLoss: 528.6049\n",
      "Training Epoch: 12 [28224/36450]\tLoss: 501.4595\n",
      "Training Epoch: 12 [28288/36450]\tLoss: 513.8727\n",
      "Training Epoch: 12 [28352/36450]\tLoss: 526.3386\n",
      "Training Epoch: 12 [28416/36450]\tLoss: 519.9481\n",
      "Training Epoch: 12 [28480/36450]\tLoss: 504.4418\n",
      "Training Epoch: 12 [28544/36450]\tLoss: 504.2742\n",
      "Training Epoch: 12 [28608/36450]\tLoss: 501.0938\n",
      "Training Epoch: 12 [28672/36450]\tLoss: 554.1241\n",
      "Training Epoch: 12 [28736/36450]\tLoss: 527.8272\n",
      "Training Epoch: 12 [28800/36450]\tLoss: 498.1124\n",
      "Training Epoch: 12 [28864/36450]\tLoss: 535.2648\n",
      "Training Epoch: 12 [28928/36450]\tLoss: 507.0223\n",
      "Training Epoch: 12 [28992/36450]\tLoss: 514.5856\n",
      "Training Epoch: 12 [29056/36450]\tLoss: 548.8141\n",
      "Training Epoch: 12 [29120/36450]\tLoss: 526.1068\n",
      "Training Epoch: 12 [29184/36450]\tLoss: 459.7976\n",
      "Training Epoch: 12 [29248/36450]\tLoss: 523.7727\n",
      "Training Epoch: 12 [29312/36450]\tLoss: 498.4677\n",
      "Training Epoch: 12 [29376/36450]\tLoss: 491.2178\n",
      "Training Epoch: 12 [29440/36450]\tLoss: 513.8916\n",
      "Training Epoch: 12 [29504/36450]\tLoss: 521.9133\n",
      "Training Epoch: 12 [29568/36450]\tLoss: 549.6467\n",
      "Training Epoch: 12 [29632/36450]\tLoss: 530.4750\n",
      "Training Epoch: 12 [29696/36450]\tLoss: 539.2584\n",
      "Training Epoch: 12 [29760/36450]\tLoss: 530.4831\n",
      "Training Epoch: 12 [29824/36450]\tLoss: 513.3871\n",
      "Training Epoch: 12 [29888/36450]\tLoss: 522.4832\n",
      "Training Epoch: 12 [29952/36450]\tLoss: 531.9152\n",
      "Training Epoch: 12 [30016/36450]\tLoss: 523.6718\n",
      "Training Epoch: 12 [30080/36450]\tLoss: 524.3668\n",
      "Training Epoch: 12 [30144/36450]\tLoss: 493.3455\n",
      "Training Epoch: 12 [30208/36450]\tLoss: 512.0624\n",
      "Training Epoch: 12 [30272/36450]\tLoss: 497.9052\n",
      "Training Epoch: 12 [30336/36450]\tLoss: 498.7259\n",
      "Training Epoch: 12 [30400/36450]\tLoss: 568.9502\n",
      "Training Epoch: 12 [30464/36450]\tLoss: 505.3315\n",
      "Training Epoch: 12 [30528/36450]\tLoss: 521.6564\n",
      "Training Epoch: 12 [30592/36450]\tLoss: 493.7538\n",
      "Training Epoch: 12 [30656/36450]\tLoss: 519.3882\n",
      "Training Epoch: 12 [30720/36450]\tLoss: 510.7721\n",
      "Training Epoch: 12 [30784/36450]\tLoss: 502.9703\n",
      "Training Epoch: 12 [30848/36450]\tLoss: 537.3446\n",
      "Training Epoch: 12 [30912/36450]\tLoss: 518.4388\n",
      "Training Epoch: 12 [30976/36450]\tLoss: 537.1335\n",
      "Training Epoch: 12 [31040/36450]\tLoss: 557.5477\n",
      "Training Epoch: 12 [31104/36450]\tLoss: 513.9723\n",
      "Training Epoch: 12 [31168/36450]\tLoss: 540.7608\n",
      "Training Epoch: 12 [31232/36450]\tLoss: 512.8857\n",
      "Training Epoch: 12 [31296/36450]\tLoss: 503.5689\n",
      "Training Epoch: 12 [31360/36450]\tLoss: 488.6853\n",
      "Training Epoch: 12 [31424/36450]\tLoss: 521.0818\n",
      "Training Epoch: 12 [31488/36450]\tLoss: 533.0983\n",
      "Training Epoch: 12 [31552/36450]\tLoss: 513.7307\n",
      "Training Epoch: 12 [31616/36450]\tLoss: 532.5804\n",
      "Training Epoch: 12 [31680/36450]\tLoss: 502.7285\n",
      "Training Epoch: 12 [31744/36450]\tLoss: 510.7005\n",
      "Training Epoch: 12 [31808/36450]\tLoss: 514.3401\n",
      "Training Epoch: 12 [31872/36450]\tLoss: 514.5778\n",
      "Training Epoch: 12 [31936/36450]\tLoss: 520.8832\n",
      "Training Epoch: 12 [32000/36450]\tLoss: 519.5444\n",
      "Training Epoch: 12 [32064/36450]\tLoss: 516.5950\n",
      "Training Epoch: 12 [32128/36450]\tLoss: 508.8890\n",
      "Training Epoch: 12 [32192/36450]\tLoss: 516.2373\n",
      "Training Epoch: 12 [32256/36450]\tLoss: 526.0913\n",
      "Training Epoch: 12 [32320/36450]\tLoss: 488.0575\n",
      "Training Epoch: 12 [32384/36450]\tLoss: 491.4099\n",
      "Training Epoch: 12 [32448/36450]\tLoss: 506.5400\n",
      "Training Epoch: 12 [32512/36450]\tLoss: 535.4865\n",
      "Training Epoch: 12 [32576/36450]\tLoss: 513.5604\n",
      "Training Epoch: 12 [32640/36450]\tLoss: 524.9746\n",
      "Training Epoch: 12 [32704/36450]\tLoss: 503.5560\n",
      "Training Epoch: 12 [32768/36450]\tLoss: 504.9436\n",
      "Training Epoch: 12 [32832/36450]\tLoss: 467.9125\n",
      "Training Epoch: 12 [32896/36450]\tLoss: 501.6704\n",
      "Training Epoch: 12 [32960/36450]\tLoss: 543.4495\n",
      "Training Epoch: 12 [33024/36450]\tLoss: 520.3961\n",
      "Training Epoch: 12 [33088/36450]\tLoss: 516.8500\n",
      "Training Epoch: 12 [33152/36450]\tLoss: 510.1100\n",
      "Training Epoch: 12 [33216/36450]\tLoss: 523.3167\n",
      "Training Epoch: 12 [33280/36450]\tLoss: 542.3760\n",
      "Training Epoch: 12 [33344/36450]\tLoss: 508.7904\n",
      "Training Epoch: 12 [33408/36450]\tLoss: 536.8795\n",
      "Training Epoch: 12 [33472/36450]\tLoss: 520.1257\n",
      "Training Epoch: 12 [33536/36450]\tLoss: 541.3265\n",
      "Training Epoch: 12 [33600/36450]\tLoss: 528.2346\n",
      "Training Epoch: 12 [33664/36450]\tLoss: 521.6082\n",
      "Training Epoch: 12 [33728/36450]\tLoss: 518.4109\n",
      "Training Epoch: 12 [33792/36450]\tLoss: 514.5808\n",
      "Training Epoch: 12 [33856/36450]\tLoss: 518.9034\n",
      "Training Epoch: 12 [33920/36450]\tLoss: 501.1170\n",
      "Training Epoch: 12 [33984/36450]\tLoss: 505.8280\n",
      "Training Epoch: 12 [34048/36450]\tLoss: 557.0896\n",
      "Training Epoch: 12 [34112/36450]\tLoss: 508.1976\n",
      "Training Epoch: 12 [34176/36450]\tLoss: 537.2106\n",
      "Training Epoch: 12 [34240/36450]\tLoss: 475.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [34304/36450]\tLoss: 513.5839\n",
      "Training Epoch: 12 [34368/36450]\tLoss: 479.7855\n",
      "Training Epoch: 12 [34432/36450]\tLoss: 511.1722\n",
      "Training Epoch: 12 [34496/36450]\tLoss: 508.7757\n",
      "Training Epoch: 12 [34560/36450]\tLoss: 564.4666\n",
      "Training Epoch: 12 [34624/36450]\tLoss: 491.1085\n",
      "Training Epoch: 12 [34688/36450]\tLoss: 538.0483\n",
      "Training Epoch: 12 [34752/36450]\tLoss: 540.6687\n",
      "Training Epoch: 12 [34816/36450]\tLoss: 521.8599\n",
      "Training Epoch: 12 [34880/36450]\tLoss: 503.1202\n",
      "Training Epoch: 12 [34944/36450]\tLoss: 534.0404\n",
      "Training Epoch: 12 [35008/36450]\tLoss: 500.7375\n",
      "Training Epoch: 12 [35072/36450]\tLoss: 492.4825\n",
      "Training Epoch: 12 [35136/36450]\tLoss: 503.0164\n",
      "Training Epoch: 12 [35200/36450]\tLoss: 488.9422\n",
      "Training Epoch: 12 [35264/36450]\tLoss: 499.1411\n",
      "Training Epoch: 12 [35328/36450]\tLoss: 545.3416\n",
      "Training Epoch: 12 [35392/36450]\tLoss: 535.1674\n",
      "Training Epoch: 12 [35456/36450]\tLoss: 533.2488\n",
      "Training Epoch: 12 [35520/36450]\tLoss: 524.1548\n",
      "Training Epoch: 12 [35584/36450]\tLoss: 509.7077\n",
      "Training Epoch: 12 [35648/36450]\tLoss: 516.0341\n",
      "Training Epoch: 12 [35712/36450]\tLoss: 483.2638\n",
      "Training Epoch: 12 [35776/36450]\tLoss: 492.5815\n",
      "Training Epoch: 12 [35840/36450]\tLoss: 546.7225\n",
      "Training Epoch: 12 [35904/36450]\tLoss: 511.6040\n",
      "Training Epoch: 12 [35968/36450]\tLoss: 500.6362\n",
      "Training Epoch: 12 [36032/36450]\tLoss: 525.3787\n",
      "Training Epoch: 12 [36096/36450]\tLoss: 518.7448\n",
      "Training Epoch: 12 [36160/36450]\tLoss: 519.9944\n",
      "Training Epoch: 12 [36224/36450]\tLoss: 501.2479\n",
      "Training Epoch: 12 [36288/36450]\tLoss: 516.3693\n",
      "Training Epoch: 12 [36352/36450]\tLoss: 511.3416\n",
      "Training Epoch: 12 [36416/36450]\tLoss: 505.3607\n",
      "Training Epoch: 12 [36450/36450]\tLoss: 543.8850\n",
      "Training Epoch: 12 [4050/4050]\tLoss: 255.5648\n",
      "Training Epoch: 13 [64/36450]\tLoss: 523.3534\n",
      "Training Epoch: 13 [128/36450]\tLoss: 508.6338\n",
      "Training Epoch: 13 [192/36450]\tLoss: 520.6716\n",
      "Training Epoch: 13 [256/36450]\tLoss: 534.7390\n",
      "Training Epoch: 13 [320/36450]\tLoss: 505.1511\n",
      "Training Epoch: 13 [384/36450]\tLoss: 490.0945\n",
      "Training Epoch: 13 [448/36450]\tLoss: 535.0624\n",
      "Training Epoch: 13 [512/36450]\tLoss: 510.7949\n",
      "Training Epoch: 13 [576/36450]\tLoss: 497.0231\n",
      "Training Epoch: 13 [640/36450]\tLoss: 482.5236\n",
      "Training Epoch: 13 [704/36450]\tLoss: 522.9734\n",
      "Training Epoch: 13 [768/36450]\tLoss: 521.7531\n",
      "Training Epoch: 13 [832/36450]\tLoss: 477.7993\n",
      "Training Epoch: 13 [896/36450]\tLoss: 505.1117\n",
      "Training Epoch: 13 [960/36450]\tLoss: 506.9211\n",
      "Training Epoch: 13 [1024/36450]\tLoss: 480.5068\n",
      "Training Epoch: 13 [1088/36450]\tLoss: 510.7323\n",
      "Training Epoch: 13 [1152/36450]\tLoss: 511.3773\n",
      "Training Epoch: 13 [1216/36450]\tLoss: 514.8174\n",
      "Training Epoch: 13 [1280/36450]\tLoss: 522.5513\n",
      "Training Epoch: 13 [1344/36450]\tLoss: 494.2299\n",
      "Training Epoch: 13 [1408/36450]\tLoss: 543.6512\n",
      "Training Epoch: 13 [1472/36450]\tLoss: 514.1304\n",
      "Training Epoch: 13 [1536/36450]\tLoss: 517.4334\n",
      "Training Epoch: 13 [1600/36450]\tLoss: 470.7878\n",
      "Training Epoch: 13 [1664/36450]\tLoss: 496.9805\n",
      "Training Epoch: 13 [1728/36450]\tLoss: 519.8673\n",
      "Training Epoch: 13 [1792/36450]\tLoss: 509.8790\n",
      "Training Epoch: 13 [1856/36450]\tLoss: 498.3394\n",
      "Training Epoch: 13 [1920/36450]\tLoss: 515.9422\n",
      "Training Epoch: 13 [1984/36450]\tLoss: 506.2278\n",
      "Training Epoch: 13 [2048/36450]\tLoss: 511.8219\n",
      "Training Epoch: 13 [2112/36450]\tLoss: 506.3008\n",
      "Training Epoch: 13 [2176/36450]\tLoss: 528.4437\n",
      "Training Epoch: 13 [2240/36450]\tLoss: 512.9756\n",
      "Training Epoch: 13 [2304/36450]\tLoss: 488.1320\n",
      "Training Epoch: 13 [2368/36450]\tLoss: 493.4290\n",
      "Training Epoch: 13 [2432/36450]\tLoss: 520.4136\n",
      "Training Epoch: 13 [2496/36450]\tLoss: 521.7772\n",
      "Training Epoch: 13 [2560/36450]\tLoss: 527.1104\n",
      "Training Epoch: 13 [2624/36450]\tLoss: 498.4816\n",
      "Training Epoch: 13 [2688/36450]\tLoss: 495.5933\n",
      "Training Epoch: 13 [2752/36450]\tLoss: 542.0978\n",
      "Training Epoch: 13 [2816/36450]\tLoss: 510.6940\n",
      "Training Epoch: 13 [2880/36450]\tLoss: 529.7501\n",
      "Training Epoch: 13 [2944/36450]\tLoss: 570.2790\n",
      "Training Epoch: 13 [3008/36450]\tLoss: 572.9332\n",
      "Training Epoch: 13 [3072/36450]\tLoss: 516.3003\n",
      "Training Epoch: 13 [3136/36450]\tLoss: 535.9514\n",
      "Training Epoch: 13 [3200/36450]\tLoss: 596.2654\n",
      "Training Epoch: 13 [3264/36450]\tLoss: 605.8972\n",
      "Training Epoch: 13 [3328/36450]\tLoss: 581.0194\n",
      "Training Epoch: 13 [3392/36450]\tLoss: 634.0522\n",
      "Training Epoch: 13 [3456/36450]\tLoss: 557.1646\n",
      "Training Epoch: 13 [3520/36450]\tLoss: 544.9749\n",
      "Training Epoch: 13 [3584/36450]\tLoss: 518.3908\n",
      "Training Epoch: 13 [3648/36450]\tLoss: 512.8890\n",
      "Training Epoch: 13 [3712/36450]\tLoss: 514.0005\n",
      "Training Epoch: 13 [3776/36450]\tLoss: 535.1982\n",
      "Training Epoch: 13 [3840/36450]\tLoss: 572.7768\n",
      "Training Epoch: 13 [3904/36450]\tLoss: 547.5215\n",
      "Training Epoch: 13 [3968/36450]\tLoss: 546.4399\n",
      "Training Epoch: 13 [4032/36450]\tLoss: 517.7136\n",
      "Training Epoch: 13 [4096/36450]\tLoss: 529.9299\n",
      "Training Epoch: 13 [4160/36450]\tLoss: 497.7197\n",
      "Training Epoch: 13 [4224/36450]\tLoss: 503.9313\n",
      "Training Epoch: 13 [4288/36450]\tLoss: 525.7740\n",
      "Training Epoch: 13 [4352/36450]\tLoss: 539.6534\n",
      "Training Epoch: 13 [4416/36450]\tLoss: 519.0652\n",
      "Training Epoch: 13 [4480/36450]\tLoss: 484.6831\n",
      "Training Epoch: 13 [4544/36450]\tLoss: 515.6443\n",
      "Training Epoch: 13 [4608/36450]\tLoss: 535.2153\n",
      "Training Epoch: 13 [4672/36450]\tLoss: 523.8099\n",
      "Training Epoch: 13 [4736/36450]\tLoss: 527.2326\n",
      "Training Epoch: 13 [4800/36450]\tLoss: 526.7052\n",
      "Training Epoch: 13 [4864/36450]\tLoss: 515.9175\n",
      "Training Epoch: 13 [4928/36450]\tLoss: 504.6959\n",
      "Training Epoch: 13 [4992/36450]\tLoss: 566.4534\n",
      "Training Epoch: 13 [5056/36450]\tLoss: 525.0389\n",
      "Training Epoch: 13 [5120/36450]\tLoss: 525.0063\n",
      "Training Epoch: 13 [5184/36450]\tLoss: 517.5320\n",
      "Training Epoch: 13 [5248/36450]\tLoss: 511.3584\n",
      "Training Epoch: 13 [5312/36450]\tLoss: 497.2468\n",
      "Training Epoch: 13 [5376/36450]\tLoss: 549.8092\n",
      "Training Epoch: 13 [5440/36450]\tLoss: 514.7037\n",
      "Training Epoch: 13 [5504/36450]\tLoss: 518.6393\n",
      "Training Epoch: 13 [5568/36450]\tLoss: 509.5550\n",
      "Training Epoch: 13 [5632/36450]\tLoss: 522.0983\n",
      "Training Epoch: 13 [5696/36450]\tLoss: 509.7584\n",
      "Training Epoch: 13 [5760/36450]\tLoss: 515.1826\n",
      "Training Epoch: 13 [5824/36450]\tLoss: 524.2299\n",
      "Training Epoch: 13 [5888/36450]\tLoss: 492.4948\n",
      "Training Epoch: 13 [5952/36450]\tLoss: 502.1541\n",
      "Training Epoch: 13 [6016/36450]\tLoss: 539.2869\n",
      "Training Epoch: 13 [6080/36450]\tLoss: 499.7650\n",
      "Training Epoch: 13 [6144/36450]\tLoss: 491.5352\n",
      "Training Epoch: 13 [6208/36450]\tLoss: 505.2359\n",
      "Training Epoch: 13 [6272/36450]\tLoss: 568.5472\n",
      "Training Epoch: 13 [6336/36450]\tLoss: 525.8338\n",
      "Training Epoch: 13 [6400/36450]\tLoss: 517.7939\n",
      "Training Epoch: 13 [6464/36450]\tLoss: 525.6309\n",
      "Training Epoch: 13 [6528/36450]\tLoss: 519.4147\n",
      "Training Epoch: 13 [6592/36450]\tLoss: 482.1977\n",
      "Training Epoch: 13 [6656/36450]\tLoss: 510.6672\n",
      "Training Epoch: 13 [6720/36450]\tLoss: 501.8423\n",
      "Training Epoch: 13 [6784/36450]\tLoss: 510.2580\n",
      "Training Epoch: 13 [6848/36450]\tLoss: 490.8751\n",
      "Training Epoch: 13 [6912/36450]\tLoss: 519.7958\n",
      "Training Epoch: 13 [6976/36450]\tLoss: 523.6198\n",
      "Training Epoch: 13 [7040/36450]\tLoss: 524.4730\n",
      "Training Epoch: 13 [7104/36450]\tLoss: 530.0231\n",
      "Training Epoch: 13 [7168/36450]\tLoss: 520.6363\n",
      "Training Epoch: 13 [7232/36450]\tLoss: 512.8347\n",
      "Training Epoch: 13 [7296/36450]\tLoss: 515.6902\n",
      "Training Epoch: 13 [7360/36450]\tLoss: 549.1466\n",
      "Training Epoch: 13 [7424/36450]\tLoss: 505.1479\n",
      "Training Epoch: 13 [7488/36450]\tLoss: 513.0331\n",
      "Training Epoch: 13 [7552/36450]\tLoss: 536.3652\n",
      "Training Epoch: 13 [7616/36450]\tLoss: 509.7570\n",
      "Training Epoch: 13 [7680/36450]\tLoss: 508.6768\n",
      "Training Epoch: 13 [7744/36450]\tLoss: 484.7592\n",
      "Training Epoch: 13 [7808/36450]\tLoss: 512.9634\n",
      "Training Epoch: 13 [7872/36450]\tLoss: 477.9040\n",
      "Training Epoch: 13 [7936/36450]\tLoss: 527.2480\n",
      "Training Epoch: 13 [8000/36450]\tLoss: 521.0723\n",
      "Training Epoch: 13 [8064/36450]\tLoss: 502.3259\n",
      "Training Epoch: 13 [8128/36450]\tLoss: 490.8465\n",
      "Training Epoch: 13 [8192/36450]\tLoss: 520.5179\n",
      "Training Epoch: 13 [8256/36450]\tLoss: 512.6917\n",
      "Training Epoch: 13 [8320/36450]\tLoss: 522.0309\n",
      "Training Epoch: 13 [8384/36450]\tLoss: 514.2994\n",
      "Training Epoch: 13 [8448/36450]\tLoss: 488.6437\n",
      "Training Epoch: 13 [8512/36450]\tLoss: 501.3422\n",
      "Training Epoch: 13 [8576/36450]\tLoss: 492.2215\n",
      "Training Epoch: 13 [8640/36450]\tLoss: 500.4708\n",
      "Training Epoch: 13 [8704/36450]\tLoss: 517.5758\n",
      "Training Epoch: 13 [8768/36450]\tLoss: 502.9138\n",
      "Training Epoch: 13 [8832/36450]\tLoss: 498.9004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [8896/36450]\tLoss: 506.8630\n",
      "Training Epoch: 13 [8960/36450]\tLoss: 536.0153\n",
      "Training Epoch: 13 [9024/36450]\tLoss: 511.2173\n",
      "Training Epoch: 13 [9088/36450]\tLoss: 514.1526\n",
      "Training Epoch: 13 [9152/36450]\tLoss: 523.6784\n",
      "Training Epoch: 13 [9216/36450]\tLoss: 497.3154\n",
      "Training Epoch: 13 [9280/36450]\tLoss: 528.4039\n",
      "Training Epoch: 13 [9344/36450]\tLoss: 471.4849\n",
      "Training Epoch: 13 [9408/36450]\tLoss: 519.2714\n",
      "Training Epoch: 13 [9472/36450]\tLoss: 499.9357\n",
      "Training Epoch: 13 [9536/36450]\tLoss: 472.7569\n",
      "Training Epoch: 13 [9600/36450]\tLoss: 516.6806\n",
      "Training Epoch: 13 [9664/36450]\tLoss: 512.8742\n",
      "Training Epoch: 13 [9728/36450]\tLoss: 525.1694\n",
      "Training Epoch: 13 [9792/36450]\tLoss: 542.7948\n",
      "Training Epoch: 13 [9856/36450]\tLoss: 507.2513\n",
      "Training Epoch: 13 [9920/36450]\tLoss: 514.4522\n",
      "Training Epoch: 13 [9984/36450]\tLoss: 509.9131\n",
      "Training Epoch: 13 [10048/36450]\tLoss: 511.2374\n",
      "Training Epoch: 13 [10112/36450]\tLoss: 520.1218\n",
      "Training Epoch: 13 [10176/36450]\tLoss: 536.7466\n",
      "Training Epoch: 13 [10240/36450]\tLoss: 505.0965\n",
      "Training Epoch: 13 [10304/36450]\tLoss: 506.0915\n",
      "Training Epoch: 13 [10368/36450]\tLoss: 519.0859\n",
      "Training Epoch: 13 [10432/36450]\tLoss: 515.5506\n",
      "Training Epoch: 13 [10496/36450]\tLoss: 525.1102\n",
      "Training Epoch: 13 [10560/36450]\tLoss: 488.7057\n",
      "Training Epoch: 13 [10624/36450]\tLoss: 494.7167\n",
      "Training Epoch: 13 [10688/36450]\tLoss: 518.0406\n",
      "Training Epoch: 13 [10752/36450]\tLoss: 520.9012\n",
      "Training Epoch: 13 [10816/36450]\tLoss: 539.4897\n",
      "Training Epoch: 13 [10880/36450]\tLoss: 526.4624\n",
      "Training Epoch: 13 [10944/36450]\tLoss: 519.9807\n",
      "Training Epoch: 13 [11008/36450]\tLoss: 523.5287\n",
      "Training Epoch: 13 [11072/36450]\tLoss: 521.6459\n",
      "Training Epoch: 13 [11136/36450]\tLoss: 503.5242\n",
      "Training Epoch: 13 [11200/36450]\tLoss: 498.0179\n",
      "Training Epoch: 13 [11264/36450]\tLoss: 503.9200\n",
      "Training Epoch: 13 [11328/36450]\tLoss: 525.5173\n",
      "Training Epoch: 13 [11392/36450]\tLoss: 523.4094\n",
      "Training Epoch: 13 [11456/36450]\tLoss: 526.8888\n",
      "Training Epoch: 13 [11520/36450]\tLoss: 526.2094\n",
      "Training Epoch: 13 [11584/36450]\tLoss: 528.6575\n",
      "Training Epoch: 13 [11648/36450]\tLoss: 470.6414\n",
      "Training Epoch: 13 [11712/36450]\tLoss: 518.9230\n",
      "Training Epoch: 13 [11776/36450]\tLoss: 503.4147\n",
      "Training Epoch: 13 [11840/36450]\tLoss: 506.1475\n",
      "Training Epoch: 13 [11904/36450]\tLoss: 491.9302\n",
      "Training Epoch: 13 [11968/36450]\tLoss: 500.9405\n",
      "Training Epoch: 13 [12032/36450]\tLoss: 513.3729\n",
      "Training Epoch: 13 [12096/36450]\tLoss: 511.2767\n",
      "Training Epoch: 13 [12160/36450]\tLoss: 520.8974\n",
      "Training Epoch: 13 [12224/36450]\tLoss: 515.2712\n",
      "Training Epoch: 13 [12288/36450]\tLoss: 510.5622\n",
      "Training Epoch: 13 [12352/36450]\tLoss: 525.0726\n",
      "Training Epoch: 13 [12416/36450]\tLoss: 500.9759\n",
      "Training Epoch: 13 [12480/36450]\tLoss: 526.6521\n",
      "Training Epoch: 13 [12544/36450]\tLoss: 517.3671\n",
      "Training Epoch: 13 [12608/36450]\tLoss: 533.8804\n",
      "Training Epoch: 13 [12672/36450]\tLoss: 529.7308\n",
      "Training Epoch: 13 [12736/36450]\tLoss: 546.7158\n",
      "Training Epoch: 13 [12800/36450]\tLoss: 524.6439\n",
      "Training Epoch: 13 [12864/36450]\tLoss: 533.2075\n",
      "Training Epoch: 13 [12928/36450]\tLoss: 498.2340\n",
      "Training Epoch: 13 [12992/36450]\tLoss: 522.8371\n",
      "Training Epoch: 13 [13056/36450]\tLoss: 531.6437\n",
      "Training Epoch: 13 [13120/36450]\tLoss: 525.1660\n",
      "Training Epoch: 13 [13184/36450]\tLoss: 530.6347\n",
      "Training Epoch: 13 [13248/36450]\tLoss: 520.1387\n",
      "Training Epoch: 13 [13312/36450]\tLoss: 503.9407\n",
      "Training Epoch: 13 [13376/36450]\tLoss: 507.6883\n",
      "Training Epoch: 13 [13440/36450]\tLoss: 506.8109\n",
      "Training Epoch: 13 [13504/36450]\tLoss: 493.7086\n",
      "Training Epoch: 13 [13568/36450]\tLoss: 545.8726\n",
      "Training Epoch: 13 [13632/36450]\tLoss: 527.2422\n",
      "Training Epoch: 13 [13696/36450]\tLoss: 521.4389\n",
      "Training Epoch: 13 [13760/36450]\tLoss: 521.4683\n",
      "Training Epoch: 13 [13824/36450]\tLoss: 517.3177\n",
      "Training Epoch: 13 [13888/36450]\tLoss: 510.9703\n",
      "Training Epoch: 13 [13952/36450]\tLoss: 514.6930\n",
      "Training Epoch: 13 [14016/36450]\tLoss: 526.5593\n",
      "Training Epoch: 13 [14080/36450]\tLoss: 536.7551\n",
      "Training Epoch: 13 [14144/36450]\tLoss: 486.9872\n",
      "Training Epoch: 13 [14208/36450]\tLoss: 523.3638\n",
      "Training Epoch: 13 [14272/36450]\tLoss: 487.6933\n",
      "Training Epoch: 13 [14336/36450]\tLoss: 530.1803\n",
      "Training Epoch: 13 [14400/36450]\tLoss: 500.9146\n",
      "Training Epoch: 13 [14464/36450]\tLoss: 520.6646\n",
      "Training Epoch: 13 [14528/36450]\tLoss: 529.4576\n",
      "Training Epoch: 13 [14592/36450]\tLoss: 509.9641\n",
      "Training Epoch: 13 [14656/36450]\tLoss: 516.9866\n",
      "Training Epoch: 13 [14720/36450]\tLoss: 520.4888\n",
      "Training Epoch: 13 [14784/36450]\tLoss: 539.3577\n",
      "Training Epoch: 13 [14848/36450]\tLoss: 558.8171\n",
      "Training Epoch: 13 [14912/36450]\tLoss: 538.7443\n",
      "Training Epoch: 13 [14976/36450]\tLoss: 552.3126\n",
      "Training Epoch: 13 [15040/36450]\tLoss: 540.1614\n",
      "Training Epoch: 13 [15104/36450]\tLoss: 533.9929\n",
      "Training Epoch: 13 [15168/36450]\tLoss: 528.6639\n",
      "Training Epoch: 13 [15232/36450]\tLoss: 534.9113\n",
      "Training Epoch: 13 [15296/36450]\tLoss: 537.7880\n",
      "Training Epoch: 13 [15360/36450]\tLoss: 513.0754\n",
      "Training Epoch: 13 [15424/36450]\tLoss: 513.6636\n",
      "Training Epoch: 13 [15488/36450]\tLoss: 488.3169\n",
      "Training Epoch: 13 [15552/36450]\tLoss: 521.9697\n",
      "Training Epoch: 13 [15616/36450]\tLoss: 515.0624\n",
      "Training Epoch: 13 [15680/36450]\tLoss: 503.2155\n",
      "Training Epoch: 13 [15744/36450]\tLoss: 532.0205\n",
      "Training Epoch: 13 [15808/36450]\tLoss: 517.4282\n",
      "Training Epoch: 13 [15872/36450]\tLoss: 502.4348\n",
      "Training Epoch: 13 [15936/36450]\tLoss: 530.6494\n",
      "Training Epoch: 13 [16000/36450]\tLoss: 520.2769\n",
      "Training Epoch: 13 [16064/36450]\tLoss: 508.7596\n",
      "Training Epoch: 13 [16128/36450]\tLoss: 507.2193\n",
      "Training Epoch: 13 [16192/36450]\tLoss: 506.1990\n",
      "Training Epoch: 13 [16256/36450]\tLoss: 492.2525\n",
      "Training Epoch: 13 [16320/36450]\tLoss: 530.6957\n",
      "Training Epoch: 13 [16384/36450]\tLoss: 523.2692\n",
      "Training Epoch: 13 [16448/36450]\tLoss: 516.7045\n",
      "Training Epoch: 13 [16512/36450]\tLoss: 523.6378\n",
      "Training Epoch: 13 [16576/36450]\tLoss: 511.8201\n",
      "Training Epoch: 13 [16640/36450]\tLoss: 511.3130\n",
      "Training Epoch: 13 [16704/36450]\tLoss: 515.1896\n",
      "Training Epoch: 13 [16768/36450]\tLoss: 508.4225\n",
      "Training Epoch: 13 [16832/36450]\tLoss: 491.0101\n",
      "Training Epoch: 13 [16896/36450]\tLoss: 518.8114\n",
      "Training Epoch: 13 [16960/36450]\tLoss: 495.7038\n",
      "Training Epoch: 13 [17024/36450]\tLoss: 548.3262\n",
      "Training Epoch: 13 [17088/36450]\tLoss: 538.5547\n",
      "Training Epoch: 13 [17152/36450]\tLoss: 515.7928\n",
      "Training Epoch: 13 [17216/36450]\tLoss: 483.5558\n",
      "Training Epoch: 13 [17280/36450]\tLoss: 525.3453\n",
      "Training Epoch: 13 [17344/36450]\tLoss: 495.2504\n",
      "Training Epoch: 13 [17408/36450]\tLoss: 502.4289\n",
      "Training Epoch: 13 [17472/36450]\tLoss: 515.7107\n",
      "Training Epoch: 13 [17536/36450]\tLoss: 531.1529\n",
      "Training Epoch: 13 [17600/36450]\tLoss: 520.9874\n",
      "Training Epoch: 13 [17664/36450]\tLoss: 532.3261\n",
      "Training Epoch: 13 [17728/36450]\tLoss: 546.2915\n",
      "Training Epoch: 13 [17792/36450]\tLoss: 511.5341\n",
      "Training Epoch: 13 [17856/36450]\tLoss: 508.3278\n",
      "Training Epoch: 13 [17920/36450]\tLoss: 519.1230\n",
      "Training Epoch: 13 [17984/36450]\tLoss: 476.5735\n",
      "Training Epoch: 13 [18048/36450]\tLoss: 497.2413\n",
      "Training Epoch: 13 [18112/36450]\tLoss: 530.7634\n",
      "Training Epoch: 13 [18176/36450]\tLoss: 507.8949\n",
      "Training Epoch: 13 [18240/36450]\tLoss: 500.7299\n",
      "Training Epoch: 13 [18304/36450]\tLoss: 535.9515\n",
      "Training Epoch: 13 [18368/36450]\tLoss: 496.8235\n",
      "Training Epoch: 13 [18432/36450]\tLoss: 515.7556\n",
      "Training Epoch: 13 [18496/36450]\tLoss: 539.6221\n",
      "Training Epoch: 13 [18560/36450]\tLoss: 531.6240\n",
      "Training Epoch: 13 [18624/36450]\tLoss: 521.1334\n",
      "Training Epoch: 13 [18688/36450]\tLoss: 522.9474\n",
      "Training Epoch: 13 [18752/36450]\tLoss: 528.1421\n",
      "Training Epoch: 13 [18816/36450]\tLoss: 537.7321\n",
      "Training Epoch: 13 [18880/36450]\tLoss: 497.6216\n",
      "Training Epoch: 13 [18944/36450]\tLoss: 547.2733\n",
      "Training Epoch: 13 [19008/36450]\tLoss: 496.7014\n",
      "Training Epoch: 13 [19072/36450]\tLoss: 514.1688\n",
      "Training Epoch: 13 [19136/36450]\tLoss: 515.4553\n",
      "Training Epoch: 13 [19200/36450]\tLoss: 517.1337\n",
      "Training Epoch: 13 [19264/36450]\tLoss: 537.0037\n",
      "Training Epoch: 13 [19328/36450]\tLoss: 528.6327\n",
      "Training Epoch: 13 [19392/36450]\tLoss: 507.6032\n",
      "Training Epoch: 13 [19456/36450]\tLoss: 519.6361\n",
      "Training Epoch: 13 [19520/36450]\tLoss: 541.4432\n",
      "Training Epoch: 13 [19584/36450]\tLoss: 530.2493\n",
      "Training Epoch: 13 [19648/36450]\tLoss: 514.3751\n",
      "Training Epoch: 13 [19712/36450]\tLoss: 520.2615\n",
      "Training Epoch: 13 [19776/36450]\tLoss: 527.8091\n",
      "Training Epoch: 13 [19840/36450]\tLoss: 509.0994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [19904/36450]\tLoss: 509.9910\n",
      "Training Epoch: 13 [19968/36450]\tLoss: 486.4427\n",
      "Training Epoch: 13 [20032/36450]\tLoss: 504.7466\n",
      "Training Epoch: 13 [20096/36450]\tLoss: 491.9717\n",
      "Training Epoch: 13 [20160/36450]\tLoss: 523.8014\n",
      "Training Epoch: 13 [20224/36450]\tLoss: 484.4117\n",
      "Training Epoch: 13 [20288/36450]\tLoss: 492.8750\n",
      "Training Epoch: 13 [20352/36450]\tLoss: 530.4249\n",
      "Training Epoch: 13 [20416/36450]\tLoss: 542.7260\n",
      "Training Epoch: 13 [20480/36450]\tLoss: 535.5071\n",
      "Training Epoch: 13 [20544/36450]\tLoss: 532.2428\n",
      "Training Epoch: 13 [20608/36450]\tLoss: 537.0007\n",
      "Training Epoch: 13 [20672/36450]\tLoss: 513.8459\n",
      "Training Epoch: 13 [20736/36450]\tLoss: 515.5685\n",
      "Training Epoch: 13 [20800/36450]\tLoss: 512.0665\n",
      "Training Epoch: 13 [20864/36450]\tLoss: 500.4796\n",
      "Training Epoch: 13 [20928/36450]\tLoss: 549.8359\n",
      "Training Epoch: 13 [20992/36450]\tLoss: 515.3276\n",
      "Training Epoch: 13 [21056/36450]\tLoss: 528.0127\n",
      "Training Epoch: 13 [21120/36450]\tLoss: 519.0992\n",
      "Training Epoch: 13 [21184/36450]\tLoss: 487.9596\n",
      "Training Epoch: 13 [21248/36450]\tLoss: 499.2886\n",
      "Training Epoch: 13 [21312/36450]\tLoss: 492.4959\n",
      "Training Epoch: 13 [21376/36450]\tLoss: 498.9261\n",
      "Training Epoch: 13 [21440/36450]\tLoss: 525.3896\n",
      "Training Epoch: 13 [21504/36450]\tLoss: 517.8403\n",
      "Training Epoch: 13 [21568/36450]\tLoss: 517.1747\n",
      "Training Epoch: 13 [21632/36450]\tLoss: 512.4490\n",
      "Training Epoch: 13 [21696/36450]\tLoss: 486.6589\n",
      "Training Epoch: 13 [21760/36450]\tLoss: 515.3744\n",
      "Training Epoch: 13 [21824/36450]\tLoss: 503.4106\n",
      "Training Epoch: 13 [21888/36450]\tLoss: 505.0107\n",
      "Training Epoch: 13 [21952/36450]\tLoss: 509.8991\n",
      "Training Epoch: 13 [22016/36450]\tLoss: 515.0013\n",
      "Training Epoch: 13 [22080/36450]\tLoss: 501.1838\n",
      "Training Epoch: 13 [22144/36450]\tLoss: 516.4347\n",
      "Training Epoch: 13 [22208/36450]\tLoss: 536.6685\n",
      "Training Epoch: 13 [22272/36450]\tLoss: 503.4315\n",
      "Training Epoch: 13 [22336/36450]\tLoss: 515.4049\n",
      "Training Epoch: 13 [22400/36450]\tLoss: 505.9671\n",
      "Training Epoch: 13 [22464/36450]\tLoss: 510.1216\n",
      "Training Epoch: 13 [22528/36450]\tLoss: 531.0247\n",
      "Training Epoch: 13 [22592/36450]\tLoss: 519.9448\n",
      "Training Epoch: 13 [22656/36450]\tLoss: 524.7909\n",
      "Training Epoch: 13 [22720/36450]\tLoss: 501.4977\n",
      "Training Epoch: 13 [22784/36450]\tLoss: 529.6320\n",
      "Training Epoch: 13 [22848/36450]\tLoss: 497.4519\n",
      "Training Epoch: 13 [22912/36450]\tLoss: 517.7736\n",
      "Training Epoch: 13 [22976/36450]\tLoss: 509.1411\n",
      "Training Epoch: 13 [23040/36450]\tLoss: 498.2110\n",
      "Training Epoch: 13 [23104/36450]\tLoss: 511.5511\n",
      "Training Epoch: 13 [23168/36450]\tLoss: 507.7417\n",
      "Training Epoch: 13 [23232/36450]\tLoss: 534.0229\n",
      "Training Epoch: 13 [23296/36450]\tLoss: 528.1406\n",
      "Training Epoch: 13 [23360/36450]\tLoss: 497.2265\n",
      "Training Epoch: 13 [23424/36450]\tLoss: 517.2079\n",
      "Training Epoch: 13 [23488/36450]\tLoss: 492.6606\n",
      "Training Epoch: 13 [23552/36450]\tLoss: 513.4030\n",
      "Training Epoch: 13 [23616/36450]\tLoss: 497.6942\n",
      "Training Epoch: 13 [23680/36450]\tLoss: 512.5832\n",
      "Training Epoch: 13 [23744/36450]\tLoss: 555.3864\n",
      "Training Epoch: 13 [23808/36450]\tLoss: 535.2582\n",
      "Training Epoch: 13 [23872/36450]\tLoss: 497.7812\n",
      "Training Epoch: 13 [23936/36450]\tLoss: 518.3967\n",
      "Training Epoch: 13 [24000/36450]\tLoss: 522.5339\n",
      "Training Epoch: 13 [24064/36450]\tLoss: 519.6405\n",
      "Training Epoch: 13 [24128/36450]\tLoss: 528.0258\n",
      "Training Epoch: 13 [24192/36450]\tLoss: 531.4911\n",
      "Training Epoch: 13 [24256/36450]\tLoss: 528.9692\n",
      "Training Epoch: 13 [24320/36450]\tLoss: 484.0950\n",
      "Training Epoch: 13 [24384/36450]\tLoss: 513.7471\n",
      "Training Epoch: 13 [24448/36450]\tLoss: 504.4689\n",
      "Training Epoch: 13 [24512/36450]\tLoss: 501.2998\n",
      "Training Epoch: 13 [24576/36450]\tLoss: 473.0989\n",
      "Training Epoch: 13 [24640/36450]\tLoss: 489.0364\n",
      "Training Epoch: 13 [24704/36450]\tLoss: 480.2517\n",
      "Training Epoch: 13 [24768/36450]\tLoss: 518.0354\n",
      "Training Epoch: 13 [24832/36450]\tLoss: 495.4923\n",
      "Training Epoch: 13 [24896/36450]\tLoss: 523.7390\n",
      "Training Epoch: 13 [24960/36450]\tLoss: 530.9709\n",
      "Training Epoch: 13 [25024/36450]\tLoss: 513.1674\n",
      "Training Epoch: 13 [25088/36450]\tLoss: 530.5587\n",
      "Training Epoch: 13 [25152/36450]\tLoss: 511.0608\n",
      "Training Epoch: 13 [25216/36450]\tLoss: 499.4315\n",
      "Training Epoch: 13 [25280/36450]\tLoss: 529.8254\n",
      "Training Epoch: 13 [25344/36450]\tLoss: 511.7239\n",
      "Training Epoch: 13 [25408/36450]\tLoss: 502.6766\n",
      "Training Epoch: 13 [25472/36450]\tLoss: 538.3589\n",
      "Training Epoch: 13 [25536/36450]\tLoss: 513.6213\n",
      "Training Epoch: 13 [25600/36450]\tLoss: 512.9470\n",
      "Training Epoch: 13 [25664/36450]\tLoss: 490.5016\n",
      "Training Epoch: 13 [25728/36450]\tLoss: 500.5722\n",
      "Training Epoch: 13 [25792/36450]\tLoss: 528.2266\n",
      "Training Epoch: 13 [25856/36450]\tLoss: 491.3065\n",
      "Training Epoch: 13 [25920/36450]\tLoss: 496.1313\n",
      "Training Epoch: 13 [25984/36450]\tLoss: 534.9368\n",
      "Training Epoch: 13 [26048/36450]\tLoss: 534.4202\n",
      "Training Epoch: 13 [26112/36450]\tLoss: 499.4026\n",
      "Training Epoch: 13 [26176/36450]\tLoss: 494.5043\n",
      "Training Epoch: 13 [26240/36450]\tLoss: 518.1290\n",
      "Training Epoch: 13 [26304/36450]\tLoss: 517.1298\n",
      "Training Epoch: 13 [26368/36450]\tLoss: 535.7864\n",
      "Training Epoch: 13 [26432/36450]\tLoss: 524.5306\n",
      "Training Epoch: 13 [26496/36450]\tLoss: 485.4225\n",
      "Training Epoch: 13 [26560/36450]\tLoss: 506.6768\n",
      "Training Epoch: 13 [26624/36450]\tLoss: 510.0061\n",
      "Training Epoch: 13 [26688/36450]\tLoss: 489.8102\n",
      "Training Epoch: 13 [26752/36450]\tLoss: 471.9131\n",
      "Training Epoch: 13 [26816/36450]\tLoss: 546.4548\n",
      "Training Epoch: 13 [26880/36450]\tLoss: 519.3749\n",
      "Training Epoch: 13 [26944/36450]\tLoss: 526.4130\n",
      "Training Epoch: 13 [27008/36450]\tLoss: 497.5200\n",
      "Training Epoch: 13 [27072/36450]\tLoss: 536.8646\n",
      "Training Epoch: 13 [27136/36450]\tLoss: 533.4349\n",
      "Training Epoch: 13 [27200/36450]\tLoss: 538.5703\n",
      "Training Epoch: 13 [27264/36450]\tLoss: 533.6646\n",
      "Training Epoch: 13 [27328/36450]\tLoss: 539.1978\n",
      "Training Epoch: 13 [27392/36450]\tLoss: 504.2784\n",
      "Training Epoch: 13 [27456/36450]\tLoss: 506.6331\n",
      "Training Epoch: 13 [27520/36450]\tLoss: 515.4550\n",
      "Training Epoch: 13 [27584/36450]\tLoss: 545.5778\n",
      "Training Epoch: 13 [27648/36450]\tLoss: 547.1729\n",
      "Training Epoch: 13 [27712/36450]\tLoss: 531.5240\n",
      "Training Epoch: 13 [27776/36450]\tLoss: 520.5239\n",
      "Training Epoch: 13 [27840/36450]\tLoss: 556.2615\n",
      "Training Epoch: 13 [27904/36450]\tLoss: 519.4154\n",
      "Training Epoch: 13 [27968/36450]\tLoss: 542.5448\n",
      "Training Epoch: 13 [28032/36450]\tLoss: 501.4558\n",
      "Training Epoch: 13 [28096/36450]\tLoss: 525.5173\n",
      "Training Epoch: 13 [28160/36450]\tLoss: 518.6388\n",
      "Training Epoch: 13 [28224/36450]\tLoss: 570.7711\n",
      "Training Epoch: 13 [28288/36450]\tLoss: 547.4614\n",
      "Training Epoch: 13 [28352/36450]\tLoss: 502.1043\n",
      "Training Epoch: 13 [28416/36450]\tLoss: 535.1737\n",
      "Training Epoch: 13 [28480/36450]\tLoss: 533.8317\n",
      "Training Epoch: 13 [28544/36450]\tLoss: 531.4030\n",
      "Training Epoch: 13 [28608/36450]\tLoss: 524.0070\n",
      "Training Epoch: 13 [28672/36450]\tLoss: 516.8441\n",
      "Training Epoch: 13 [28736/36450]\tLoss: 504.2343\n",
      "Training Epoch: 13 [28800/36450]\tLoss: 529.6362\n",
      "Training Epoch: 13 [28864/36450]\tLoss: 519.4678\n",
      "Training Epoch: 13 [28928/36450]\tLoss: 521.6603\n",
      "Training Epoch: 13 [28992/36450]\tLoss: 533.9112\n",
      "Training Epoch: 13 [29056/36450]\tLoss: 490.5599\n",
      "Training Epoch: 13 [29120/36450]\tLoss: 501.9852\n",
      "Training Epoch: 13 [29184/36450]\tLoss: 510.7238\n",
      "Training Epoch: 13 [29248/36450]\tLoss: 511.8563\n",
      "Training Epoch: 13 [29312/36450]\tLoss: 544.2878\n",
      "Training Epoch: 13 [29376/36450]\tLoss: 461.2322\n",
      "Training Epoch: 13 [29440/36450]\tLoss: 517.4843\n",
      "Training Epoch: 13 [29504/36450]\tLoss: 506.7005\n",
      "Training Epoch: 13 [29568/36450]\tLoss: 522.7288\n",
      "Training Epoch: 13 [29632/36450]\tLoss: 521.4742\n",
      "Training Epoch: 13 [29696/36450]\tLoss: 488.5978\n",
      "Training Epoch: 13 [29760/36450]\tLoss: 518.2728\n",
      "Training Epoch: 13 [29824/36450]\tLoss: 516.9814\n",
      "Training Epoch: 13 [29888/36450]\tLoss: 530.6341\n",
      "Training Epoch: 13 [29952/36450]\tLoss: 516.5357\n",
      "Training Epoch: 13 [30016/36450]\tLoss: 494.0279\n",
      "Training Epoch: 13 [30080/36450]\tLoss: 504.9369\n",
      "Training Epoch: 13 [30144/36450]\tLoss: 503.1620\n",
      "Training Epoch: 13 [30208/36450]\tLoss: 491.3559\n",
      "Training Epoch: 13 [30272/36450]\tLoss: 516.4944\n",
      "Training Epoch: 13 [30336/36450]\tLoss: 508.2181\n",
      "Training Epoch: 13 [30400/36450]\tLoss: 519.4930\n",
      "Training Epoch: 13 [30464/36450]\tLoss: 510.7282\n",
      "Training Epoch: 13 [30528/36450]\tLoss: 513.1102\n",
      "Training Epoch: 13 [30592/36450]\tLoss: 510.2014\n",
      "Training Epoch: 13 [30656/36450]\tLoss: 510.3703\n",
      "Training Epoch: 13 [30720/36450]\tLoss: 505.5522\n",
      "Training Epoch: 13 [30784/36450]\tLoss: 513.1951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [30848/36450]\tLoss: 513.6727\n",
      "Training Epoch: 13 [30912/36450]\tLoss: 504.7995\n",
      "Training Epoch: 13 [30976/36450]\tLoss: 492.6963\n",
      "Training Epoch: 13 [31040/36450]\tLoss: 522.2572\n",
      "Training Epoch: 13 [31104/36450]\tLoss: 498.1754\n",
      "Training Epoch: 13 [31168/36450]\tLoss: 536.4986\n",
      "Training Epoch: 13 [31232/36450]\tLoss: 485.8549\n",
      "Training Epoch: 13 [31296/36450]\tLoss: 501.7974\n",
      "Training Epoch: 13 [31360/36450]\tLoss: 505.7697\n",
      "Training Epoch: 13 [31424/36450]\tLoss: 500.9650\n",
      "Training Epoch: 13 [31488/36450]\tLoss: 509.4308\n",
      "Training Epoch: 13 [31552/36450]\tLoss: 514.3043\n",
      "Training Epoch: 13 [31616/36450]\tLoss: 502.0772\n",
      "Training Epoch: 13 [31680/36450]\tLoss: 519.7531\n",
      "Training Epoch: 13 [31744/36450]\tLoss: 531.1526\n",
      "Training Epoch: 13 [31808/36450]\tLoss: 549.8555\n",
      "Training Epoch: 13 [31872/36450]\tLoss: 517.8732\n",
      "Training Epoch: 13 [31936/36450]\tLoss: 502.9905\n",
      "Training Epoch: 13 [32000/36450]\tLoss: 494.2621\n",
      "Training Epoch: 13 [32064/36450]\tLoss: 498.3322\n",
      "Training Epoch: 13 [32128/36450]\tLoss: 539.3354\n",
      "Training Epoch: 13 [32192/36450]\tLoss: 523.4570\n",
      "Training Epoch: 13 [32256/36450]\tLoss: 516.2855\n",
      "Training Epoch: 13 [32320/36450]\tLoss: 507.8473\n",
      "Training Epoch: 13 [32384/36450]\tLoss: 516.0857\n",
      "Training Epoch: 13 [32448/36450]\tLoss: 514.7824\n",
      "Training Epoch: 13 [32512/36450]\tLoss: 505.4290\n",
      "Training Epoch: 13 [32576/36450]\tLoss: 526.6927\n",
      "Training Epoch: 13 [32640/36450]\tLoss: 538.3478\n",
      "Training Epoch: 13 [32704/36450]\tLoss: 495.4926\n",
      "Training Epoch: 13 [32768/36450]\tLoss: 491.6555\n",
      "Training Epoch: 13 [32832/36450]\tLoss: 488.6861\n",
      "Training Epoch: 13 [32896/36450]\tLoss: 530.9686\n",
      "Training Epoch: 13 [32960/36450]\tLoss: 502.7852\n",
      "Training Epoch: 13 [33024/36450]\tLoss: 496.2981\n",
      "Training Epoch: 13 [33088/36450]\tLoss: 519.5350\n",
      "Training Epoch: 13 [33152/36450]\tLoss: 534.1723\n",
      "Training Epoch: 13 [33216/36450]\tLoss: 544.4767\n",
      "Training Epoch: 13 [33280/36450]\tLoss: 534.3090\n",
      "Training Epoch: 13 [33344/36450]\tLoss: 502.8441\n",
      "Training Epoch: 13 [33408/36450]\tLoss: 508.2656\n",
      "Training Epoch: 13 [33472/36450]\tLoss: 508.4574\n",
      "Training Epoch: 13 [33536/36450]\tLoss: 489.1227\n",
      "Training Epoch: 13 [33600/36450]\tLoss: 513.3386\n",
      "Training Epoch: 13 [33664/36450]\tLoss: 500.2672\n",
      "Training Epoch: 13 [33728/36450]\tLoss: 500.6092\n",
      "Training Epoch: 13 [33792/36450]\tLoss: 476.0527\n",
      "Training Epoch: 13 [33856/36450]\tLoss: 502.2845\n",
      "Training Epoch: 13 [33920/36450]\tLoss: 519.3264\n",
      "Training Epoch: 13 [33984/36450]\tLoss: 496.3779\n",
      "Training Epoch: 13 [34048/36450]\tLoss: 506.3938\n",
      "Training Epoch: 13 [34112/36450]\tLoss: 503.9808\n",
      "Training Epoch: 13 [34176/36450]\tLoss: 514.7999\n",
      "Training Epoch: 13 [34240/36450]\tLoss: 508.9495\n",
      "Training Epoch: 13 [34304/36450]\tLoss: 497.5274\n",
      "Training Epoch: 13 [34368/36450]\tLoss: 492.0640\n",
      "Training Epoch: 13 [34432/36450]\tLoss: 504.4427\n",
      "Training Epoch: 13 [34496/36450]\tLoss: 498.9689\n",
      "Training Epoch: 13 [34560/36450]\tLoss: 486.0872\n",
      "Training Epoch: 13 [34624/36450]\tLoss: 520.6788\n",
      "Training Epoch: 13 [34688/36450]\tLoss: 503.8930\n",
      "Training Epoch: 13 [34752/36450]\tLoss: 506.8768\n",
      "Training Epoch: 13 [34816/36450]\tLoss: 493.0901\n",
      "Training Epoch: 13 [34880/36450]\tLoss: 537.4874\n",
      "Training Epoch: 13 [34944/36450]\tLoss: 516.8021\n",
      "Training Epoch: 13 [35008/36450]\tLoss: 551.4891\n",
      "Training Epoch: 13 [35072/36450]\tLoss: 535.3152\n",
      "Training Epoch: 13 [35136/36450]\tLoss: 497.5518\n",
      "Training Epoch: 13 [35200/36450]\tLoss: 513.1153\n",
      "Training Epoch: 13 [35264/36450]\tLoss: 525.4484\n",
      "Training Epoch: 13 [35328/36450]\tLoss: 483.6220\n",
      "Training Epoch: 13 [35392/36450]\tLoss: 504.7610\n",
      "Training Epoch: 13 [35456/36450]\tLoss: 475.3401\n",
      "Training Epoch: 13 [35520/36450]\tLoss: 545.8880\n",
      "Training Epoch: 13 [35584/36450]\tLoss: 496.3946\n",
      "Training Epoch: 13 [35648/36450]\tLoss: 492.3539\n",
      "Training Epoch: 13 [35712/36450]\tLoss: 522.9324\n",
      "Training Epoch: 13 [35776/36450]\tLoss: 510.8792\n",
      "Training Epoch: 13 [35840/36450]\tLoss: 485.4366\n",
      "Training Epoch: 13 [35904/36450]\tLoss: 503.5841\n",
      "Training Epoch: 13 [35968/36450]\tLoss: 520.1022\n",
      "Training Epoch: 13 [36032/36450]\tLoss: 496.8957\n",
      "Training Epoch: 13 [36096/36450]\tLoss: 524.6504\n",
      "Training Epoch: 13 [36160/36450]\tLoss: 523.7141\n",
      "Training Epoch: 13 [36224/36450]\tLoss: 500.8879\n",
      "Training Epoch: 13 [36288/36450]\tLoss: 478.8198\n",
      "Training Epoch: 13 [36352/36450]\tLoss: 489.8746\n",
      "Training Epoch: 13 [36416/36450]\tLoss: 505.6659\n",
      "Training Epoch: 13 [36450/36450]\tLoss: 487.1030\n",
      "Training Epoch: 13 [4050/4050]\tLoss: 251.9424\n",
      "Training Epoch: 14 [64/36450]\tLoss: 476.0088\n",
      "Training Epoch: 14 [128/36450]\tLoss: 512.1147\n",
      "Training Epoch: 14 [192/36450]\tLoss: 499.1212\n",
      "Training Epoch: 14 [256/36450]\tLoss: 515.7040\n",
      "Training Epoch: 14 [320/36450]\tLoss: 512.4314\n",
      "Training Epoch: 14 [384/36450]\tLoss: 516.5115\n",
      "Training Epoch: 14 [448/36450]\tLoss: 487.1016\n",
      "Training Epoch: 14 [512/36450]\tLoss: 508.4393\n",
      "Training Epoch: 14 [576/36450]\tLoss: 471.6575\n",
      "Training Epoch: 14 [640/36450]\tLoss: 530.2651\n",
      "Training Epoch: 14 [704/36450]\tLoss: 485.7103\n",
      "Training Epoch: 14 [768/36450]\tLoss: 536.0312\n",
      "Training Epoch: 14 [832/36450]\tLoss: 494.1387\n",
      "Training Epoch: 14 [896/36450]\tLoss: 508.4612\n",
      "Training Epoch: 14 [960/36450]\tLoss: 510.2890\n",
      "Training Epoch: 14 [1024/36450]\tLoss: 492.5535\n",
      "Training Epoch: 14 [1088/36450]\tLoss: 508.7242\n",
      "Training Epoch: 14 [1152/36450]\tLoss: 508.1630\n",
      "Training Epoch: 14 [1216/36450]\tLoss: 529.8574\n",
      "Training Epoch: 14 [1280/36450]\tLoss: 523.0725\n",
      "Training Epoch: 14 [1344/36450]\tLoss: 547.8126\n",
      "Training Epoch: 14 [1408/36450]\tLoss: 534.0583\n",
      "Training Epoch: 14 [1472/36450]\tLoss: 525.8253\n",
      "Training Epoch: 14 [1536/36450]\tLoss: 543.7542\n",
      "Training Epoch: 14 [1600/36450]\tLoss: 493.3293\n",
      "Training Epoch: 14 [1664/36450]\tLoss: 514.5454\n",
      "Training Epoch: 14 [1728/36450]\tLoss: 512.4502\n",
      "Training Epoch: 14 [1792/36450]\tLoss: 514.7643\n",
      "Training Epoch: 14 [1856/36450]\tLoss: 523.7957\n",
      "Training Epoch: 14 [1920/36450]\tLoss: 513.9342\n",
      "Training Epoch: 14 [1984/36450]\tLoss: 497.7688\n",
      "Training Epoch: 14 [2048/36450]\tLoss: 468.7048\n",
      "Training Epoch: 14 [2112/36450]\tLoss: 503.5802\n",
      "Training Epoch: 14 [2176/36450]\tLoss: 506.5643\n",
      "Training Epoch: 14 [2240/36450]\tLoss: 508.3721\n",
      "Training Epoch: 14 [2304/36450]\tLoss: 504.1422\n",
      "Training Epoch: 14 [2368/36450]\tLoss: 501.4856\n",
      "Training Epoch: 14 [2432/36450]\tLoss: 508.1362\n",
      "Training Epoch: 14 [2496/36450]\tLoss: 532.4136\n",
      "Training Epoch: 14 [2560/36450]\tLoss: 507.0214\n",
      "Training Epoch: 14 [2624/36450]\tLoss: 543.8057\n",
      "Training Epoch: 14 [2688/36450]\tLoss: 512.8713\n",
      "Training Epoch: 14 [2752/36450]\tLoss: 519.3028\n",
      "Training Epoch: 14 [2816/36450]\tLoss: 497.4759\n",
      "Training Epoch: 14 [2880/36450]\tLoss: 525.0781\n",
      "Training Epoch: 14 [2944/36450]\tLoss: 485.9074\n",
      "Training Epoch: 14 [3008/36450]\tLoss: 518.8133\n",
      "Training Epoch: 14 [3072/36450]\tLoss: 512.9342\n",
      "Training Epoch: 14 [3136/36450]\tLoss: 508.9578\n",
      "Training Epoch: 14 [3200/36450]\tLoss: 504.1184\n",
      "Training Epoch: 14 [3264/36450]\tLoss: 511.2116\n",
      "Training Epoch: 14 [3328/36450]\tLoss: 512.7390\n",
      "Training Epoch: 14 [3392/36450]\tLoss: 527.5151\n",
      "Training Epoch: 14 [3456/36450]\tLoss: 519.6727\n",
      "Training Epoch: 14 [3520/36450]\tLoss: 475.8630\n",
      "Training Epoch: 14 [3584/36450]\tLoss: 482.5861\n",
      "Training Epoch: 14 [3648/36450]\tLoss: 489.9214\n",
      "Training Epoch: 14 [3712/36450]\tLoss: 473.0644\n",
      "Training Epoch: 14 [3776/36450]\tLoss: 524.5191\n",
      "Training Epoch: 14 [3840/36450]\tLoss: 517.8768\n",
      "Training Epoch: 14 [3904/36450]\tLoss: 530.6063\n",
      "Training Epoch: 14 [3968/36450]\tLoss: 527.5235\n",
      "Training Epoch: 14 [4032/36450]\tLoss: 475.2047\n",
      "Training Epoch: 14 [4096/36450]\tLoss: 520.7347\n",
      "Training Epoch: 14 [4160/36450]\tLoss: 505.8336\n",
      "Training Epoch: 14 [4224/36450]\tLoss: 507.9163\n",
      "Training Epoch: 14 [4288/36450]\tLoss: 488.9058\n",
      "Training Epoch: 14 [4352/36450]\tLoss: 519.0557\n",
      "Training Epoch: 14 [4416/36450]\tLoss: 503.5759\n",
      "Training Epoch: 14 [4480/36450]\tLoss: 515.1589\n",
      "Training Epoch: 14 [4544/36450]\tLoss: 482.7401\n",
      "Training Epoch: 14 [4608/36450]\tLoss: 482.7393\n",
      "Training Epoch: 14 [4672/36450]\tLoss: 500.6553\n",
      "Training Epoch: 14 [4736/36450]\tLoss: 527.6282\n",
      "Training Epoch: 14 [4800/36450]\tLoss: 508.8952\n",
      "Training Epoch: 14 [4864/36450]\tLoss: 494.9507\n",
      "Training Epoch: 14 [4928/36450]\tLoss: 505.6424\n",
      "Training Epoch: 14 [4992/36450]\tLoss: 528.1385\n",
      "Training Epoch: 14 [5056/36450]\tLoss: 486.7221\n",
      "Training Epoch: 14 [5120/36450]\tLoss: 525.9482\n",
      "Training Epoch: 14 [5184/36450]\tLoss: 492.4908\n",
      "Training Epoch: 14 [5248/36450]\tLoss: 495.1070\n",
      "Training Epoch: 14 [5312/36450]\tLoss: 514.5136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [5376/36450]\tLoss: 501.1876\n",
      "Training Epoch: 14 [5440/36450]\tLoss: 487.8269\n",
      "Training Epoch: 14 [5504/36450]\tLoss: 523.6065\n",
      "Training Epoch: 14 [5568/36450]\tLoss: 553.1214\n",
      "Training Epoch: 14 [5632/36450]\tLoss: 515.5281\n",
      "Training Epoch: 14 [5696/36450]\tLoss: 498.0241\n",
      "Training Epoch: 14 [5760/36450]\tLoss: 506.5316\n",
      "Training Epoch: 14 [5824/36450]\tLoss: 494.0695\n",
      "Training Epoch: 14 [5888/36450]\tLoss: 511.6151\n",
      "Training Epoch: 14 [5952/36450]\tLoss: 536.5352\n",
      "Training Epoch: 14 [6016/36450]\tLoss: 515.8589\n",
      "Training Epoch: 14 [6080/36450]\tLoss: 495.8155\n",
      "Training Epoch: 14 [6144/36450]\tLoss: 537.2975\n",
      "Training Epoch: 14 [6208/36450]\tLoss: 508.5329\n",
      "Training Epoch: 14 [6272/36450]\tLoss: 526.0577\n",
      "Training Epoch: 14 [6336/36450]\tLoss: 522.5060\n",
      "Training Epoch: 14 [6400/36450]\tLoss: 502.7576\n",
      "Training Epoch: 14 [6464/36450]\tLoss: 497.6182\n",
      "Training Epoch: 14 [6528/36450]\tLoss: 532.2214\n",
      "Training Epoch: 14 [6592/36450]\tLoss: 514.5587\n",
      "Training Epoch: 14 [6656/36450]\tLoss: 523.0633\n",
      "Training Epoch: 14 [6720/36450]\tLoss: 525.5737\n",
      "Training Epoch: 14 [6784/36450]\tLoss: 516.0206\n",
      "Training Epoch: 14 [6848/36450]\tLoss: 502.0721\n",
      "Training Epoch: 14 [6912/36450]\tLoss: 503.7781\n",
      "Training Epoch: 14 [6976/36450]\tLoss: 488.6152\n",
      "Training Epoch: 14 [7040/36450]\tLoss: 498.2947\n",
      "Training Epoch: 14 [7104/36450]\tLoss: 540.9877\n",
      "Training Epoch: 14 [7168/36450]\tLoss: 484.0413\n",
      "Training Epoch: 14 [7232/36450]\tLoss: 513.5039\n",
      "Training Epoch: 14 [7296/36450]\tLoss: 512.0034\n",
      "Training Epoch: 14 [7360/36450]\tLoss: 500.2715\n",
      "Training Epoch: 14 [7424/36450]\tLoss: 487.7472\n",
      "Training Epoch: 14 [7488/36450]\tLoss: 506.5214\n",
      "Training Epoch: 14 [7552/36450]\tLoss: 479.3697\n",
      "Training Epoch: 14 [7616/36450]\tLoss: 502.4317\n",
      "Training Epoch: 14 [7680/36450]\tLoss: 514.2592\n",
      "Training Epoch: 14 [7744/36450]\tLoss: 487.7790\n",
      "Training Epoch: 14 [7808/36450]\tLoss: 519.0078\n",
      "Training Epoch: 14 [7872/36450]\tLoss: 487.9515\n",
      "Training Epoch: 14 [7936/36450]\tLoss: 483.9986\n",
      "Training Epoch: 14 [8000/36450]\tLoss: 474.2515\n",
      "Training Epoch: 14 [8064/36450]\tLoss: 496.1318\n",
      "Training Epoch: 14 [8128/36450]\tLoss: 485.6721\n",
      "Training Epoch: 14 [8192/36450]\tLoss: 498.6206\n",
      "Training Epoch: 14 [8256/36450]\tLoss: 510.7706\n",
      "Training Epoch: 14 [8320/36450]\tLoss: 531.7981\n",
      "Training Epoch: 14 [8384/36450]\tLoss: 487.6395\n",
      "Training Epoch: 14 [8448/36450]\tLoss: 529.8190\n",
      "Training Epoch: 14 [8512/36450]\tLoss: 539.5868\n",
      "Training Epoch: 14 [8576/36450]\tLoss: 526.5549\n",
      "Training Epoch: 14 [8640/36450]\tLoss: 574.7980\n",
      "Training Epoch: 14 [8704/36450]\tLoss: 616.8036\n",
      "Training Epoch: 14 [8768/36450]\tLoss: 594.3928\n",
      "Training Epoch: 14 [8832/36450]\tLoss: 674.9537\n",
      "Training Epoch: 14 [8896/36450]\tLoss: 596.6887\n",
      "Training Epoch: 14 [8960/36450]\tLoss: 537.7479\n",
      "Training Epoch: 14 [9024/36450]\tLoss: 497.6386\n",
      "Training Epoch: 14 [9088/36450]\tLoss: 503.2727\n",
      "Training Epoch: 14 [9152/36450]\tLoss: 547.3459\n",
      "Training Epoch: 14 [9216/36450]\tLoss: 574.7526\n",
      "Training Epoch: 14 [9280/36450]\tLoss: 536.8017\n",
      "Training Epoch: 14 [9344/36450]\tLoss: 542.1188\n",
      "Training Epoch: 14 [9408/36450]\tLoss: 524.0213\n",
      "Training Epoch: 14 [9472/36450]\tLoss: 493.1873\n",
      "Training Epoch: 14 [9536/36450]\tLoss: 570.8126\n",
      "Training Epoch: 14 [9600/36450]\tLoss: 568.9336\n",
      "Training Epoch: 14 [9664/36450]\tLoss: 486.3814\n",
      "Training Epoch: 14 [9728/36450]\tLoss: 527.7396\n",
      "Training Epoch: 14 [9792/36450]\tLoss: 500.0695\n",
      "Training Epoch: 14 [9856/36450]\tLoss: 534.4934\n",
      "Training Epoch: 14 [9920/36450]\tLoss: 519.1208\n",
      "Training Epoch: 14 [9984/36450]\tLoss: 474.1367\n",
      "Training Epoch: 14 [10048/36450]\tLoss: 508.8328\n",
      "Training Epoch: 14 [10112/36450]\tLoss: 528.1422\n",
      "Training Epoch: 14 [10176/36450]\tLoss: 532.2249\n",
      "Training Epoch: 14 [10240/36450]\tLoss: 519.9700\n",
      "Training Epoch: 14 [10304/36450]\tLoss: 509.9807\n",
      "Training Epoch: 14 [10368/36450]\tLoss: 517.8543\n",
      "Training Epoch: 14 [10432/36450]\tLoss: 535.3281\n",
      "Training Epoch: 14 [10496/36450]\tLoss: 545.7974\n",
      "Training Epoch: 14 [10560/36450]\tLoss: 508.1421\n",
      "Training Epoch: 14 [10624/36450]\tLoss: 497.5216\n",
      "Training Epoch: 14 [10688/36450]\tLoss: 513.6575\n",
      "Training Epoch: 14 [10752/36450]\tLoss: 471.3728\n",
      "Training Epoch: 14 [10816/36450]\tLoss: 537.7341\n",
      "Training Epoch: 14 [10880/36450]\tLoss: 537.9950\n",
      "Training Epoch: 14 [10944/36450]\tLoss: 500.8816\n",
      "Training Epoch: 14 [11008/36450]\tLoss: 483.8927\n",
      "Training Epoch: 14 [11072/36450]\tLoss: 497.0916\n",
      "Training Epoch: 14 [11136/36450]\tLoss: 554.3432\n",
      "Training Epoch: 14 [11200/36450]\tLoss: 488.3456\n",
      "Training Epoch: 14 [11264/36450]\tLoss: 513.1307\n",
      "Training Epoch: 14 [11328/36450]\tLoss: 485.1491\n",
      "Training Epoch: 14 [11392/36450]\tLoss: 497.2871\n",
      "Training Epoch: 14 [11456/36450]\tLoss: 494.6544\n",
      "Training Epoch: 14 [11520/36450]\tLoss: 543.2821\n",
      "Training Epoch: 14 [11584/36450]\tLoss: 536.6091\n",
      "Training Epoch: 14 [11648/36450]\tLoss: 507.1436\n",
      "Training Epoch: 14 [11712/36450]\tLoss: 503.7411\n",
      "Training Epoch: 14 [11776/36450]\tLoss: 516.8101\n",
      "Training Epoch: 14 [11840/36450]\tLoss: 512.7264\n",
      "Training Epoch: 14 [11904/36450]\tLoss: 501.7630\n",
      "Training Epoch: 14 [11968/36450]\tLoss: 499.8436\n",
      "Training Epoch: 14 [12032/36450]\tLoss: 497.8397\n",
      "Training Epoch: 14 [12096/36450]\tLoss: 507.4566\n",
      "Training Epoch: 14 [12160/36450]\tLoss: 552.6533\n",
      "Training Epoch: 14 [12224/36450]\tLoss: 540.9412\n",
      "Training Epoch: 14 [12288/36450]\tLoss: 516.8784\n",
      "Training Epoch: 14 [12352/36450]\tLoss: 491.8971\n",
      "Training Epoch: 14 [12416/36450]\tLoss: 518.3420\n",
      "Training Epoch: 14 [12480/36450]\tLoss: 499.5409\n",
      "Training Epoch: 14 [12544/36450]\tLoss: 487.6884\n",
      "Training Epoch: 14 [12608/36450]\tLoss: 517.3303\n",
      "Training Epoch: 14 [12672/36450]\tLoss: 513.1503\n",
      "Training Epoch: 14 [12736/36450]\tLoss: 506.8508\n",
      "Training Epoch: 14 [12800/36450]\tLoss: 492.8287\n",
      "Training Epoch: 14 [12864/36450]\tLoss: 531.8596\n",
      "Training Epoch: 14 [12928/36450]\tLoss: 516.4097\n",
      "Training Epoch: 14 [12992/36450]\tLoss: 501.6866\n",
      "Training Epoch: 14 [13056/36450]\tLoss: 504.8797\n",
      "Training Epoch: 14 [13120/36450]\tLoss: 505.6027\n",
      "Training Epoch: 14 [13184/36450]\tLoss: 513.1570\n",
      "Training Epoch: 14 [13248/36450]\tLoss: 511.9912\n",
      "Training Epoch: 14 [13312/36450]\tLoss: 486.3004\n",
      "Training Epoch: 14 [13376/36450]\tLoss: 481.7573\n",
      "Training Epoch: 14 [13440/36450]\tLoss: 508.5727\n",
      "Training Epoch: 14 [13504/36450]\tLoss: 517.8188\n",
      "Training Epoch: 14 [13568/36450]\tLoss: 465.8494\n",
      "Training Epoch: 14 [13632/36450]\tLoss: 515.6851\n",
      "Training Epoch: 14 [13696/36450]\tLoss: 498.9850\n",
      "Training Epoch: 14 [13760/36450]\tLoss: 528.2375\n",
      "Training Epoch: 14 [13824/36450]\tLoss: 494.5415\n",
      "Training Epoch: 14 [13888/36450]\tLoss: 518.6854\n",
      "Training Epoch: 14 [13952/36450]\tLoss: 515.1542\n",
      "Training Epoch: 14 [14016/36450]\tLoss: 473.2932\n",
      "Training Epoch: 14 [14080/36450]\tLoss: 519.5004\n",
      "Training Epoch: 14 [14144/36450]\tLoss: 517.5408\n",
      "Training Epoch: 14 [14208/36450]\tLoss: 503.5417\n",
      "Training Epoch: 14 [14272/36450]\tLoss: 521.4193\n",
      "Training Epoch: 14 [14336/36450]\tLoss: 469.4084\n",
      "Training Epoch: 14 [14400/36450]\tLoss: 473.5528\n",
      "Training Epoch: 14 [14464/36450]\tLoss: 512.1847\n",
      "Training Epoch: 14 [14528/36450]\tLoss: 545.1063\n",
      "Training Epoch: 14 [14592/36450]\tLoss: 519.6781\n",
      "Training Epoch: 14 [14656/36450]\tLoss: 515.5934\n",
      "Training Epoch: 14 [14720/36450]\tLoss: 526.5324\n",
      "Training Epoch: 14 [14784/36450]\tLoss: 522.4890\n",
      "Training Epoch: 14 [14848/36450]\tLoss: 506.7910\n",
      "Training Epoch: 14 [14912/36450]\tLoss: 519.4675\n",
      "Training Epoch: 14 [14976/36450]\tLoss: 493.7324\n",
      "Training Epoch: 14 [15040/36450]\tLoss: 505.5145\n",
      "Training Epoch: 14 [15104/36450]\tLoss: 504.5779\n",
      "Training Epoch: 14 [15168/36450]\tLoss: 478.1418\n",
      "Training Epoch: 14 [15232/36450]\tLoss: 507.3477\n",
      "Training Epoch: 14 [15296/36450]\tLoss: 501.1312\n",
      "Training Epoch: 14 [15360/36450]\tLoss: 514.7493\n",
      "Training Epoch: 14 [15424/36450]\tLoss: 502.5621\n",
      "Training Epoch: 14 [15488/36450]\tLoss: 509.9860\n",
      "Training Epoch: 14 [15552/36450]\tLoss: 514.5689\n",
      "Training Epoch: 14 [15616/36450]\tLoss: 506.6776\n",
      "Training Epoch: 14 [15680/36450]\tLoss: 497.7054\n",
      "Training Epoch: 14 [15744/36450]\tLoss: 502.8272\n",
      "Training Epoch: 14 [15808/36450]\tLoss: 516.4442\n",
      "Training Epoch: 14 [15872/36450]\tLoss: 487.9532\n",
      "Training Epoch: 14 [15936/36450]\tLoss: 512.2442\n",
      "Training Epoch: 14 [16000/36450]\tLoss: 492.1278\n",
      "Training Epoch: 14 [16064/36450]\tLoss: 500.2457\n",
      "Training Epoch: 14 [16128/36450]\tLoss: 524.3698\n",
      "Training Epoch: 14 [16192/36450]\tLoss: 501.8825\n",
      "Training Epoch: 14 [16256/36450]\tLoss: 516.8213\n",
      "Training Epoch: 14 [16320/36450]\tLoss: 506.7533\n",
      "Training Epoch: 14 [16384/36450]\tLoss: 520.0790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [16448/36450]\tLoss: 499.1535\n",
      "Training Epoch: 14 [16512/36450]\tLoss: 509.9665\n",
      "Training Epoch: 14 [16576/36450]\tLoss: 495.9175\n",
      "Training Epoch: 14 [16640/36450]\tLoss: 500.6078\n",
      "Training Epoch: 14 [16704/36450]\tLoss: 522.5529\n",
      "Training Epoch: 14 [16768/36450]\tLoss: 497.8628\n",
      "Training Epoch: 14 [16832/36450]\tLoss: 486.4791\n",
      "Training Epoch: 14 [16896/36450]\tLoss: 522.5590\n",
      "Training Epoch: 14 [16960/36450]\tLoss: 482.3502\n",
      "Training Epoch: 14 [17024/36450]\tLoss: 475.7217\n",
      "Training Epoch: 14 [17088/36450]\tLoss: 520.6055\n",
      "Training Epoch: 14 [17152/36450]\tLoss: 509.0154\n",
      "Training Epoch: 14 [17216/36450]\tLoss: 487.3998\n",
      "Training Epoch: 14 [17280/36450]\tLoss: 511.5899\n",
      "Training Epoch: 14 [17344/36450]\tLoss: 513.5109\n",
      "Training Epoch: 14 [17408/36450]\tLoss: 499.9483\n",
      "Training Epoch: 14 [17472/36450]\tLoss: 493.1253\n",
      "Training Epoch: 14 [17536/36450]\tLoss: 528.2887\n",
      "Training Epoch: 14 [17600/36450]\tLoss: 527.5224\n",
      "Training Epoch: 14 [17664/36450]\tLoss: 508.9167\n",
      "Training Epoch: 14 [17728/36450]\tLoss: 508.0898\n",
      "Training Epoch: 14 [17792/36450]\tLoss: 509.0171\n",
      "Training Epoch: 14 [17856/36450]\tLoss: 530.6138\n",
      "Training Epoch: 14 [17920/36450]\tLoss: 513.3445\n",
      "Training Epoch: 14 [17984/36450]\tLoss: 495.7963\n",
      "Training Epoch: 14 [18048/36450]\tLoss: 547.0656\n",
      "Training Epoch: 14 [18112/36450]\tLoss: 494.2140\n",
      "Training Epoch: 14 [18176/36450]\tLoss: 508.2610\n",
      "Training Epoch: 14 [18240/36450]\tLoss: 552.2773\n",
      "Training Epoch: 14 [18304/36450]\tLoss: 518.0726\n",
      "Training Epoch: 14 [18368/36450]\tLoss: 516.2321\n",
      "Training Epoch: 14 [18432/36450]\tLoss: 475.8138\n",
      "Training Epoch: 14 [18496/36450]\tLoss: 514.0422\n",
      "Training Epoch: 14 [18560/36450]\tLoss: 502.7002\n",
      "Training Epoch: 14 [18624/36450]\tLoss: 515.3661\n",
      "Training Epoch: 14 [18688/36450]\tLoss: 500.4782\n",
      "Training Epoch: 14 [18752/36450]\tLoss: 470.4982\n",
      "Training Epoch: 14 [18816/36450]\tLoss: 490.9786\n",
      "Training Epoch: 14 [18880/36450]\tLoss: 516.5813\n",
      "Training Epoch: 14 [18944/36450]\tLoss: 529.7234\n",
      "Training Epoch: 14 [19008/36450]\tLoss: 514.3288\n",
      "Training Epoch: 14 [19072/36450]\tLoss: 508.4184\n",
      "Training Epoch: 14 [19136/36450]\tLoss: 479.8635\n",
      "Training Epoch: 14 [19200/36450]\tLoss: 510.4501\n",
      "Training Epoch: 14 [19264/36450]\tLoss: 510.2191\n",
      "Training Epoch: 14 [19328/36450]\tLoss: 511.6886\n",
      "Training Epoch: 14 [19392/36450]\tLoss: 486.9042\n",
      "Training Epoch: 14 [19456/36450]\tLoss: 498.8689\n",
      "Training Epoch: 14 [19520/36450]\tLoss: 527.5485\n",
      "Training Epoch: 14 [19584/36450]\tLoss: 526.0588\n",
      "Training Epoch: 14 [19648/36450]\tLoss: 501.8841\n",
      "Training Epoch: 14 [19712/36450]\tLoss: 515.8094\n",
      "Training Epoch: 14 [19776/36450]\tLoss: 496.3889\n",
      "Training Epoch: 14 [19840/36450]\tLoss: 523.8074\n",
      "Training Epoch: 14 [19904/36450]\tLoss: 489.3360\n",
      "Training Epoch: 14 [19968/36450]\tLoss: 493.0826\n",
      "Training Epoch: 14 [20032/36450]\tLoss: 519.6935\n",
      "Training Epoch: 14 [20096/36450]\tLoss: 520.9438\n",
      "Training Epoch: 14 [20160/36450]\tLoss: 492.5992\n",
      "Training Epoch: 14 [20224/36450]\tLoss: 499.9641\n",
      "Training Epoch: 14 [20288/36450]\tLoss: 512.6013\n",
      "Training Epoch: 14 [20352/36450]\tLoss: 493.6315\n",
      "Training Epoch: 14 [20416/36450]\tLoss: 508.5140\n",
      "Training Epoch: 14 [20480/36450]\tLoss: 506.7566\n",
      "Training Epoch: 14 [20544/36450]\tLoss: 498.9763\n",
      "Training Epoch: 14 [20608/36450]\tLoss: 516.3727\n",
      "Training Epoch: 14 [20672/36450]\tLoss: 494.9763\n",
      "Training Epoch: 14 [20736/36450]\tLoss: 509.6832\n",
      "Training Epoch: 14 [20800/36450]\tLoss: 497.6113\n",
      "Training Epoch: 14 [20864/36450]\tLoss: 510.7655\n",
      "Training Epoch: 14 [20928/36450]\tLoss: 518.1884\n",
      "Training Epoch: 14 [20992/36450]\tLoss: 507.2105\n",
      "Training Epoch: 14 [21056/36450]\tLoss: 518.8882\n",
      "Training Epoch: 14 [21120/36450]\tLoss: 487.9810\n",
      "Training Epoch: 14 [21184/36450]\tLoss: 468.8525\n",
      "Training Epoch: 14 [21248/36450]\tLoss: 526.4775\n",
      "Training Epoch: 14 [21312/36450]\tLoss: 495.7011\n",
      "Training Epoch: 14 [21376/36450]\tLoss: 533.6746\n",
      "Training Epoch: 14 [21440/36450]\tLoss: 493.2451\n",
      "Training Epoch: 14 [21504/36450]\tLoss: 520.2183\n",
      "Training Epoch: 14 [21568/36450]\tLoss: 511.6988\n",
      "Training Epoch: 14 [21632/36450]\tLoss: 521.8951\n",
      "Training Epoch: 14 [21696/36450]\tLoss: 507.5499\n",
      "Training Epoch: 14 [21760/36450]\tLoss: 506.1631\n",
      "Training Epoch: 14 [21824/36450]\tLoss: 541.9092\n",
      "Training Epoch: 14 [21888/36450]\tLoss: 518.7059\n",
      "Training Epoch: 14 [21952/36450]\tLoss: 506.5674\n",
      "Training Epoch: 14 [22016/36450]\tLoss: 494.8189\n",
      "Training Epoch: 14 [22080/36450]\tLoss: 527.5078\n",
      "Training Epoch: 14 [22144/36450]\tLoss: 503.3758\n",
      "Training Epoch: 14 [22208/36450]\tLoss: 515.1268\n",
      "Training Epoch: 14 [22272/36450]\tLoss: 518.4521\n",
      "Training Epoch: 14 [22336/36450]\tLoss: 526.2083\n",
      "Training Epoch: 14 [22400/36450]\tLoss: 504.4816\n",
      "Training Epoch: 14 [22464/36450]\tLoss: 487.3394\n",
      "Training Epoch: 14 [22528/36450]\tLoss: 497.5799\n",
      "Training Epoch: 14 [22592/36450]\tLoss: 518.6107\n",
      "Training Epoch: 14 [22656/36450]\tLoss: 472.9023\n",
      "Training Epoch: 14 [22720/36450]\tLoss: 516.9224\n",
      "Training Epoch: 14 [22784/36450]\tLoss: 504.4966\n",
      "Training Epoch: 14 [22848/36450]\tLoss: 514.8996\n",
      "Training Epoch: 14 [22912/36450]\tLoss: 520.7650\n",
      "Training Epoch: 14 [22976/36450]\tLoss: 548.9104\n",
      "Training Epoch: 14 [23040/36450]\tLoss: 473.1658\n",
      "Training Epoch: 14 [23104/36450]\tLoss: 547.8869\n",
      "Training Epoch: 14 [23168/36450]\tLoss: 501.2213\n",
      "Training Epoch: 14 [23232/36450]\tLoss: 524.3914\n",
      "Training Epoch: 14 [23296/36450]\tLoss: 544.4280\n",
      "Training Epoch: 14 [23360/36450]\tLoss: 522.8769\n",
      "Training Epoch: 14 [23424/36450]\tLoss: 520.2057\n",
      "Training Epoch: 14 [23488/36450]\tLoss: 501.4486\n",
      "Training Epoch: 14 [23552/36450]\tLoss: 535.3094\n",
      "Training Epoch: 14 [23616/36450]\tLoss: 514.0435\n",
      "Training Epoch: 14 [23680/36450]\tLoss: 495.3799\n",
      "Training Epoch: 14 [23744/36450]\tLoss: 495.7994\n",
      "Training Epoch: 14 [23808/36450]\tLoss: 501.4534\n",
      "Training Epoch: 14 [23872/36450]\tLoss: 520.1680\n",
      "Training Epoch: 14 [23936/36450]\tLoss: 501.8198\n",
      "Training Epoch: 14 [24000/36450]\tLoss: 506.8879\n",
      "Training Epoch: 14 [24064/36450]\tLoss: 534.1874\n",
      "Training Epoch: 14 [24128/36450]\tLoss: 538.2980\n",
      "Training Epoch: 14 [24192/36450]\tLoss: 515.2645\n",
      "Training Epoch: 14 [24256/36450]\tLoss: 511.3619\n",
      "Training Epoch: 14 [24320/36450]\tLoss: 506.7889\n",
      "Training Epoch: 14 [24384/36450]\tLoss: 508.7986\n",
      "Training Epoch: 14 [24448/36450]\tLoss: 535.1426\n",
      "Training Epoch: 14 [24512/36450]\tLoss: 524.8646\n",
      "Training Epoch: 14 [24576/36450]\tLoss: 527.7231\n",
      "Training Epoch: 14 [24640/36450]\tLoss: 490.1898\n",
      "Training Epoch: 14 [24704/36450]\tLoss: 526.2576\n",
      "Training Epoch: 14 [24768/36450]\tLoss: 542.4189\n",
      "Training Epoch: 14 [24832/36450]\tLoss: 504.4486\n",
      "Training Epoch: 14 [24896/36450]\tLoss: 517.3995\n",
      "Training Epoch: 14 [24960/36450]\tLoss: 525.5865\n",
      "Training Epoch: 14 [25024/36450]\tLoss: 496.3966\n",
      "Training Epoch: 14 [25088/36450]\tLoss: 532.2854\n",
      "Training Epoch: 14 [25152/36450]\tLoss: 510.9915\n",
      "Training Epoch: 14 [25216/36450]\tLoss: 480.5701\n",
      "Training Epoch: 14 [25280/36450]\tLoss: 485.7214\n",
      "Training Epoch: 14 [25344/36450]\tLoss: 499.3006\n",
      "Training Epoch: 14 [25408/36450]\tLoss: 501.7923\n",
      "Training Epoch: 14 [25472/36450]\tLoss: 507.0809\n",
      "Training Epoch: 14 [25536/36450]\tLoss: 489.8109\n",
      "Training Epoch: 14 [25600/36450]\tLoss: 496.0283\n",
      "Training Epoch: 14 [25664/36450]\tLoss: 496.4441\n",
      "Training Epoch: 14 [25728/36450]\tLoss: 498.6104\n",
      "Training Epoch: 14 [25792/36450]\tLoss: 492.6130\n",
      "Training Epoch: 14 [25856/36450]\tLoss: 511.8213\n",
      "Training Epoch: 14 [25920/36450]\tLoss: 517.1173\n",
      "Training Epoch: 14 [25984/36450]\tLoss: 487.3809\n",
      "Training Epoch: 14 [26048/36450]\tLoss: 513.8494\n",
      "Training Epoch: 14 [26112/36450]\tLoss: 487.4822\n",
      "Training Epoch: 14 [26176/36450]\tLoss: 486.3142\n",
      "Training Epoch: 14 [26240/36450]\tLoss: 502.4012\n",
      "Training Epoch: 14 [26304/36450]\tLoss: 509.2620\n",
      "Training Epoch: 14 [26368/36450]\tLoss: 502.7068\n",
      "Training Epoch: 14 [26432/36450]\tLoss: 469.5960\n",
      "Training Epoch: 14 [26496/36450]\tLoss: 492.5554\n",
      "Training Epoch: 14 [26560/36450]\tLoss: 484.9160\n",
      "Training Epoch: 14 [26624/36450]\tLoss: 517.2968\n",
      "Training Epoch: 14 [26688/36450]\tLoss: 519.9836\n",
      "Training Epoch: 14 [26752/36450]\tLoss: 494.7446\n",
      "Training Epoch: 14 [26816/36450]\tLoss: 481.1285\n",
      "Training Epoch: 14 [26880/36450]\tLoss: 495.4492\n",
      "Training Epoch: 14 [26944/36450]\tLoss: 493.4075\n",
      "Training Epoch: 14 [27008/36450]\tLoss: 509.9855\n",
      "Training Epoch: 14 [27072/36450]\tLoss: 488.3446\n",
      "Training Epoch: 14 [27136/36450]\tLoss: 517.8173\n",
      "Training Epoch: 14 [27200/36450]\tLoss: 520.0312\n",
      "Training Epoch: 14 [27264/36450]\tLoss: 508.9429\n",
      "Training Epoch: 14 [27328/36450]\tLoss: 487.8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [27392/36450]\tLoss: 480.4994\n",
      "Training Epoch: 14 [27456/36450]\tLoss: 556.2554\n",
      "Training Epoch: 14 [27520/36450]\tLoss: 506.0746\n",
      "Training Epoch: 14 [27584/36450]\tLoss: 500.5023\n",
      "Training Epoch: 14 [27648/36450]\tLoss: 482.9023\n",
      "Training Epoch: 14 [27712/36450]\tLoss: 497.1208\n",
      "Training Epoch: 14 [27776/36450]\tLoss: 513.2416\n",
      "Training Epoch: 14 [27840/36450]\tLoss: 507.6125\n",
      "Training Epoch: 14 [27904/36450]\tLoss: 497.8568\n",
      "Training Epoch: 14 [27968/36450]\tLoss: 520.3265\n",
      "Training Epoch: 14 [28032/36450]\tLoss: 497.3949\n",
      "Training Epoch: 14 [28096/36450]\tLoss: 499.5716\n",
      "Training Epoch: 14 [28160/36450]\tLoss: 535.0928\n",
      "Training Epoch: 14 [28224/36450]\tLoss: 503.9753\n",
      "Training Epoch: 14 [28288/36450]\tLoss: 510.6983\n",
      "Training Epoch: 14 [28352/36450]\tLoss: 501.9711\n",
      "Training Epoch: 14 [28416/36450]\tLoss: 507.5035\n",
      "Training Epoch: 14 [28480/36450]\tLoss: 531.6217\n",
      "Training Epoch: 14 [28544/36450]\tLoss: 489.8122\n",
      "Training Epoch: 14 [28608/36450]\tLoss: 533.0024\n",
      "Training Epoch: 14 [28672/36450]\tLoss: 499.0728\n",
      "Training Epoch: 14 [28736/36450]\tLoss: 501.5230\n",
      "Training Epoch: 14 [28800/36450]\tLoss: 514.1600\n",
      "Training Epoch: 14 [28864/36450]\tLoss: 491.5447\n",
      "Training Epoch: 14 [28928/36450]\tLoss: 518.4172\n",
      "Training Epoch: 14 [28992/36450]\tLoss: 474.1512\n",
      "Training Epoch: 14 [29056/36450]\tLoss: 531.3691\n",
      "Training Epoch: 14 [29120/36450]\tLoss: 505.0028\n",
      "Training Epoch: 14 [29184/36450]\tLoss: 517.9901\n",
      "Training Epoch: 14 [29248/36450]\tLoss: 483.7158\n",
      "Training Epoch: 14 [29312/36450]\tLoss: 518.4235\n",
      "Training Epoch: 14 [29376/36450]\tLoss: 500.6365\n",
      "Training Epoch: 14 [29440/36450]\tLoss: 501.7007\n",
      "Training Epoch: 14 [29504/36450]\tLoss: 499.4230\n",
      "Training Epoch: 14 [29568/36450]\tLoss: 486.4748\n",
      "Training Epoch: 14 [29632/36450]\tLoss: 506.4594\n",
      "Training Epoch: 14 [29696/36450]\tLoss: 501.4195\n",
      "Training Epoch: 14 [29760/36450]\tLoss: 520.0883\n",
      "Training Epoch: 14 [29824/36450]\tLoss: 545.4905\n",
      "Training Epoch: 14 [29888/36450]\tLoss: 537.2895\n",
      "Training Epoch: 14 [29952/36450]\tLoss: 537.9928\n",
      "Training Epoch: 14 [30016/36450]\tLoss: 543.6805\n",
      "Training Epoch: 14 [30080/36450]\tLoss: 526.1348\n",
      "Training Epoch: 14 [30144/36450]\tLoss: 558.8350\n",
      "Training Epoch: 14 [30208/36450]\tLoss: 556.2510\n",
      "Training Epoch: 14 [30272/36450]\tLoss: 529.7015\n",
      "Training Epoch: 14 [30336/36450]\tLoss: 511.4509\n",
      "Training Epoch: 14 [30400/36450]\tLoss: 535.5632\n",
      "Training Epoch: 14 [30464/36450]\tLoss: 497.3723\n",
      "Training Epoch: 14 [30528/36450]\tLoss: 523.8959\n",
      "Training Epoch: 14 [30592/36450]\tLoss: 496.4742\n",
      "Training Epoch: 14 [30656/36450]\tLoss: 522.3561\n",
      "Training Epoch: 14 [30720/36450]\tLoss: 520.4314\n",
      "Training Epoch: 14 [30784/36450]\tLoss: 546.2145\n",
      "Training Epoch: 14 [30848/36450]\tLoss: 506.7170\n",
      "Training Epoch: 14 [30912/36450]\tLoss: 514.6862\n",
      "Training Epoch: 14 [30976/36450]\tLoss: 536.9230\n",
      "Training Epoch: 14 [31040/36450]\tLoss: 503.5897\n",
      "Training Epoch: 14 [31104/36450]\tLoss: 499.8275\n",
      "Training Epoch: 14 [31168/36450]\tLoss: 512.7943\n",
      "Training Epoch: 14 [31232/36450]\tLoss: 519.6833\n",
      "Training Epoch: 14 [31296/36450]\tLoss: 525.9248\n",
      "Training Epoch: 14 [31360/36450]\tLoss: 528.8270\n",
      "Training Epoch: 14 [31424/36450]\tLoss: 496.0863\n",
      "Training Epoch: 14 [31488/36450]\tLoss: 516.5148\n",
      "Training Epoch: 14 [31552/36450]\tLoss: 506.9265\n",
      "Training Epoch: 14 [31616/36450]\tLoss: 525.0837\n",
      "Training Epoch: 14 [31680/36450]\tLoss: 508.6972\n",
      "Training Epoch: 14 [31744/36450]\tLoss: 471.2719\n",
      "Training Epoch: 14 [31808/36450]\tLoss: 492.7137\n",
      "Training Epoch: 14 [31872/36450]\tLoss: 504.3012\n",
      "Training Epoch: 14 [31936/36450]\tLoss: 520.2716\n",
      "Training Epoch: 14 [32000/36450]\tLoss: 510.0751\n",
      "Training Epoch: 14 [32064/36450]\tLoss: 568.0864\n",
      "Training Epoch: 14 [32128/36450]\tLoss: 500.9607\n",
      "Training Epoch: 14 [32192/36450]\tLoss: 493.6483\n",
      "Training Epoch: 14 [32256/36450]\tLoss: 482.5135\n",
      "Training Epoch: 14 [32320/36450]\tLoss: 517.7322\n",
      "Training Epoch: 14 [32384/36450]\tLoss: 508.0738\n",
      "Training Epoch: 14 [32448/36450]\tLoss: 527.4114\n",
      "Training Epoch: 14 [32512/36450]\tLoss: 493.3101\n",
      "Training Epoch: 14 [32576/36450]\tLoss: 515.7324\n",
      "Training Epoch: 14 [32640/36450]\tLoss: 484.4790\n",
      "Training Epoch: 14 [32704/36450]\tLoss: 496.7870\n",
      "Training Epoch: 14 [32768/36450]\tLoss: 491.3533\n",
      "Training Epoch: 14 [32832/36450]\tLoss: 484.9703\n",
      "Training Epoch: 14 [32896/36450]\tLoss: 515.0703\n",
      "Training Epoch: 14 [32960/36450]\tLoss: 481.0258\n",
      "Training Epoch: 14 [33024/36450]\tLoss: 508.1074\n",
      "Training Epoch: 14 [33088/36450]\tLoss: 499.5299\n",
      "Training Epoch: 14 [33152/36450]\tLoss: 523.0362\n",
      "Training Epoch: 14 [33216/36450]\tLoss: 499.0461\n",
      "Training Epoch: 14 [33280/36450]\tLoss: 515.7129\n",
      "Training Epoch: 14 [33344/36450]\tLoss: 482.1636\n",
      "Training Epoch: 14 [33408/36450]\tLoss: 474.9304\n",
      "Training Epoch: 14 [33472/36450]\tLoss: 500.5115\n",
      "Training Epoch: 14 [33536/36450]\tLoss: 520.4594\n",
      "Training Epoch: 14 [33600/36450]\tLoss: 487.0667\n",
      "Training Epoch: 14 [33664/36450]\tLoss: 505.0276\n",
      "Training Epoch: 14 [33728/36450]\tLoss: 492.4572\n",
      "Training Epoch: 14 [33792/36450]\tLoss: 508.8962\n",
      "Training Epoch: 14 [33856/36450]\tLoss: 506.4385\n",
      "Training Epoch: 14 [33920/36450]\tLoss: 476.1395\n",
      "Training Epoch: 14 [33984/36450]\tLoss: 498.2552\n",
      "Training Epoch: 14 [34048/36450]\tLoss: 508.4604\n",
      "Training Epoch: 14 [34112/36450]\tLoss: 496.0345\n",
      "Training Epoch: 14 [34176/36450]\tLoss: 499.1608\n",
      "Training Epoch: 14 [34240/36450]\tLoss: 521.5255\n",
      "Training Epoch: 14 [34304/36450]\tLoss: 514.6094\n",
      "Training Epoch: 14 [34368/36450]\tLoss: 503.5009\n",
      "Training Epoch: 14 [34432/36450]\tLoss: 479.9482\n",
      "Training Epoch: 14 [34496/36450]\tLoss: 469.0823\n",
      "Training Epoch: 14 [34560/36450]\tLoss: 504.7073\n",
      "Training Epoch: 14 [34624/36450]\tLoss: 518.6102\n",
      "Training Epoch: 14 [34688/36450]\tLoss: 516.6258\n",
      "Training Epoch: 14 [34752/36450]\tLoss: 511.1531\n",
      "Training Epoch: 14 [34816/36450]\tLoss: 513.8044\n",
      "Training Epoch: 14 [34880/36450]\tLoss: 500.7667\n",
      "Training Epoch: 14 [34944/36450]\tLoss: 496.6208\n",
      "Training Epoch: 14 [35008/36450]\tLoss: 498.3501\n",
      "Training Epoch: 14 [35072/36450]\tLoss: 523.2903\n",
      "Training Epoch: 14 [35136/36450]\tLoss: 487.7335\n",
      "Training Epoch: 14 [35200/36450]\tLoss: 495.1859\n",
      "Training Epoch: 14 [35264/36450]\tLoss: 526.8979\n",
      "Training Epoch: 14 [35328/36450]\tLoss: 479.9519\n",
      "Training Epoch: 14 [35392/36450]\tLoss: 510.6572\n",
      "Training Epoch: 14 [35456/36450]\tLoss: 514.0400\n",
      "Training Epoch: 14 [35520/36450]\tLoss: 467.9356\n",
      "Training Epoch: 14 [35584/36450]\tLoss: 512.1481\n",
      "Training Epoch: 14 [35648/36450]\tLoss: 489.5998\n",
      "Training Epoch: 14 [35712/36450]\tLoss: 517.1812\n",
      "Training Epoch: 14 [35776/36450]\tLoss: 509.4309\n",
      "Training Epoch: 14 [35840/36450]\tLoss: 517.5821\n",
      "Training Epoch: 14 [35904/36450]\tLoss: 485.5760\n",
      "Training Epoch: 14 [35968/36450]\tLoss: 521.6706\n",
      "Training Epoch: 14 [36032/36450]\tLoss: 522.4037\n",
      "Training Epoch: 14 [36096/36450]\tLoss: 496.6829\n",
      "Training Epoch: 14 [36160/36450]\tLoss: 494.9200\n",
      "Training Epoch: 14 [36224/36450]\tLoss: 508.6248\n",
      "Training Epoch: 14 [36288/36450]\tLoss: 505.0240\n",
      "Training Epoch: 14 [36352/36450]\tLoss: 534.5528\n",
      "Training Epoch: 14 [36416/36450]\tLoss: 485.4804\n",
      "Training Epoch: 14 [36450/36450]\tLoss: 476.9241\n",
      "Training Epoch: 14 [4050/4050]\tLoss: 249.2936\n",
      "Training Epoch: 15 [64/36450]\tLoss: 523.8961\n",
      "Training Epoch: 15 [128/36450]\tLoss: 500.5047\n",
      "Training Epoch: 15 [192/36450]\tLoss: 484.3820\n",
      "Training Epoch: 15 [256/36450]\tLoss: 503.2025\n",
      "Training Epoch: 15 [320/36450]\tLoss: 467.0920\n",
      "Training Epoch: 15 [384/36450]\tLoss: 480.8390\n",
      "Training Epoch: 15 [448/36450]\tLoss: 504.2692\n",
      "Training Epoch: 15 [512/36450]\tLoss: 481.7354\n",
      "Training Epoch: 15 [576/36450]\tLoss: 520.8340\n",
      "Training Epoch: 15 [640/36450]\tLoss: 500.8063\n",
      "Training Epoch: 15 [704/36450]\tLoss: 477.1223\n",
      "Training Epoch: 15 [768/36450]\tLoss: 497.2384\n",
      "Training Epoch: 15 [832/36450]\tLoss: 492.5370\n",
      "Training Epoch: 15 [896/36450]\tLoss: 532.3710\n",
      "Training Epoch: 15 [960/36450]\tLoss: 500.0134\n",
      "Training Epoch: 15 [1024/36450]\tLoss: 512.2703\n",
      "Training Epoch: 15 [1088/36450]\tLoss: 470.6190\n",
      "Training Epoch: 15 [1152/36450]\tLoss: 504.2135\n",
      "Training Epoch: 15 [1216/36450]\tLoss: 496.1911\n",
      "Training Epoch: 15 [1280/36450]\tLoss: 497.4962\n",
      "Training Epoch: 15 [1344/36450]\tLoss: 497.9675\n",
      "Training Epoch: 15 [1408/36450]\tLoss: 533.4238\n",
      "Training Epoch: 15 [1472/36450]\tLoss: 495.2790\n",
      "Training Epoch: 15 [1536/36450]\tLoss: 532.3602\n",
      "Training Epoch: 15 [1600/36450]\tLoss: 538.8112\n",
      "Training Epoch: 15 [1664/36450]\tLoss: 495.9933\n",
      "Training Epoch: 15 [1728/36450]\tLoss: 525.3610\n",
      "Training Epoch: 15 [1792/36450]\tLoss: 499.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [1856/36450]\tLoss: 539.1237\n",
      "Training Epoch: 15 [1920/36450]\tLoss: 493.9793\n",
      "Training Epoch: 15 [1984/36450]\tLoss: 488.0211\n",
      "Training Epoch: 15 [2048/36450]\tLoss: 499.3937\n",
      "Training Epoch: 15 [2112/36450]\tLoss: 504.8059\n",
      "Training Epoch: 15 [2176/36450]\tLoss: 481.4933\n",
      "Training Epoch: 15 [2240/36450]\tLoss: 492.3470\n",
      "Training Epoch: 15 [2304/36450]\tLoss: 511.8598\n",
      "Training Epoch: 15 [2368/36450]\tLoss: 503.7380\n",
      "Training Epoch: 15 [2432/36450]\tLoss: 476.5647\n",
      "Training Epoch: 15 [2496/36450]\tLoss: 525.5588\n",
      "Training Epoch: 15 [2560/36450]\tLoss: 497.6088\n",
      "Training Epoch: 15 [2624/36450]\tLoss: 489.0373\n",
      "Training Epoch: 15 [2688/36450]\tLoss: 483.5836\n",
      "Training Epoch: 15 [2752/36450]\tLoss: 487.7274\n",
      "Training Epoch: 15 [2816/36450]\tLoss: 511.0160\n",
      "Training Epoch: 15 [2880/36450]\tLoss: 491.8643\n",
      "Training Epoch: 15 [2944/36450]\tLoss: 500.0374\n",
      "Training Epoch: 15 [3008/36450]\tLoss: 487.0094\n",
      "Training Epoch: 15 [3072/36450]\tLoss: 477.4210\n",
      "Training Epoch: 15 [3136/36450]\tLoss: 478.3776\n",
      "Training Epoch: 15 [3200/36450]\tLoss: 494.5002\n",
      "Training Epoch: 15 [3264/36450]\tLoss: 509.5881\n",
      "Training Epoch: 15 [3328/36450]\tLoss: 530.6080\n",
      "Training Epoch: 15 [3392/36450]\tLoss: 510.7074\n",
      "Training Epoch: 15 [3456/36450]\tLoss: 489.7557\n",
      "Training Epoch: 15 [3520/36450]\tLoss: 522.7968\n",
      "Training Epoch: 15 [3584/36450]\tLoss: 517.3119\n",
      "Training Epoch: 15 [3648/36450]\tLoss: 465.6836\n",
      "Training Epoch: 15 [3712/36450]\tLoss: 524.7298\n",
      "Training Epoch: 15 [3776/36450]\tLoss: 515.9493\n",
      "Training Epoch: 15 [3840/36450]\tLoss: 501.1795\n",
      "Training Epoch: 15 [3904/36450]\tLoss: 491.3999\n",
      "Training Epoch: 15 [3968/36450]\tLoss: 505.5110\n",
      "Training Epoch: 15 [4032/36450]\tLoss: 503.4566\n",
      "Training Epoch: 15 [4096/36450]\tLoss: 502.9178\n",
      "Training Epoch: 15 [4160/36450]\tLoss: 516.9799\n",
      "Training Epoch: 15 [4224/36450]\tLoss: 548.0369\n",
      "Training Epoch: 15 [4288/36450]\tLoss: 525.3486\n",
      "Training Epoch: 15 [4352/36450]\tLoss: 534.7361\n",
      "Training Epoch: 15 [4416/36450]\tLoss: 520.6874\n",
      "Training Epoch: 15 [4480/36450]\tLoss: 541.6340\n",
      "Training Epoch: 15 [4544/36450]\tLoss: 527.7223\n",
      "Training Epoch: 15 [4608/36450]\tLoss: 556.4506\n",
      "Training Epoch: 15 [4672/36450]\tLoss: 564.3856\n",
      "Training Epoch: 15 [4736/36450]\tLoss: 536.3486\n",
      "Training Epoch: 15 [4800/36450]\tLoss: 499.6063\n",
      "Training Epoch: 15 [4864/36450]\tLoss: 511.6735\n",
      "Training Epoch: 15 [4928/36450]\tLoss: 520.7028\n",
      "Training Epoch: 15 [4992/36450]\tLoss: 482.7913\n",
      "Training Epoch: 15 [5056/36450]\tLoss: 551.0932\n",
      "Training Epoch: 15 [5120/36450]\tLoss: 529.4688\n",
      "Training Epoch: 15 [5184/36450]\tLoss: 516.6513\n",
      "Training Epoch: 15 [5248/36450]\tLoss: 527.3923\n",
      "Training Epoch: 15 [5312/36450]\tLoss: 538.3110\n",
      "Training Epoch: 15 [5376/36450]\tLoss: 503.5541\n",
      "Training Epoch: 15 [5440/36450]\tLoss: 522.8622\n",
      "Training Epoch: 15 [5504/36450]\tLoss: 524.1255\n",
      "Training Epoch: 15 [5568/36450]\tLoss: 481.1929\n",
      "Training Epoch: 15 [5632/36450]\tLoss: 506.2559\n",
      "Training Epoch: 15 [5696/36450]\tLoss: 509.3491\n",
      "Training Epoch: 15 [5760/36450]\tLoss: 506.3701\n",
      "Training Epoch: 15 [5824/36450]\tLoss: 478.6279\n",
      "Training Epoch: 15 [5888/36450]\tLoss: 486.1761\n",
      "Training Epoch: 15 [5952/36450]\tLoss: 489.2187\n",
      "Training Epoch: 15 [6016/36450]\tLoss: 485.7046\n",
      "Training Epoch: 15 [6080/36450]\tLoss: 488.6210\n",
      "Training Epoch: 15 [6144/36450]\tLoss: 497.5949\n",
      "Training Epoch: 15 [6208/36450]\tLoss: 490.9184\n",
      "Training Epoch: 15 [6272/36450]\tLoss: 519.7770\n",
      "Training Epoch: 15 [6336/36450]\tLoss: 529.3608\n",
      "Training Epoch: 15 [6400/36450]\tLoss: 471.3967\n",
      "Training Epoch: 15 [6464/36450]\tLoss: 509.7073\n",
      "Training Epoch: 15 [6528/36450]\tLoss: 508.9470\n",
      "Training Epoch: 15 [6592/36450]\tLoss: 492.3224\n",
      "Training Epoch: 15 [6656/36450]\tLoss: 508.4847\n",
      "Training Epoch: 15 [6720/36450]\tLoss: 501.7527\n",
      "Training Epoch: 15 [6784/36450]\tLoss: 478.9111\n",
      "Training Epoch: 15 [6848/36450]\tLoss: 524.2366\n",
      "Training Epoch: 15 [6912/36450]\tLoss: 504.5363\n",
      "Training Epoch: 15 [6976/36450]\tLoss: 488.4894\n",
      "Training Epoch: 15 [7040/36450]\tLoss: 493.1330\n",
      "Training Epoch: 15 [7104/36450]\tLoss: 503.8794\n",
      "Training Epoch: 15 [7168/36450]\tLoss: 508.5499\n",
      "Training Epoch: 15 [7232/36450]\tLoss: 461.9236\n",
      "Training Epoch: 15 [7296/36450]\tLoss: 490.1998\n",
      "Training Epoch: 15 [7360/36450]\tLoss: 499.2253\n",
      "Training Epoch: 15 [7424/36450]\tLoss: 535.7560\n",
      "Training Epoch: 15 [7488/36450]\tLoss: 515.0728\n",
      "Training Epoch: 15 [7552/36450]\tLoss: 518.3931\n",
      "Training Epoch: 15 [7616/36450]\tLoss: 492.3850\n",
      "Training Epoch: 15 [7680/36450]\tLoss: 501.2621\n",
      "Training Epoch: 15 [7744/36450]\tLoss: 487.7571\n",
      "Training Epoch: 15 [7808/36450]\tLoss: 497.9728\n",
      "Training Epoch: 15 [7872/36450]\tLoss: 518.4567\n",
      "Training Epoch: 15 [7936/36450]\tLoss: 508.2243\n",
      "Training Epoch: 15 [8000/36450]\tLoss: 480.9463\n",
      "Training Epoch: 15 [8064/36450]\tLoss: 506.6904\n",
      "Training Epoch: 15 [8128/36450]\tLoss: 505.3858\n",
      "Training Epoch: 15 [8192/36450]\tLoss: 507.4857\n",
      "Training Epoch: 15 [8256/36450]\tLoss: 503.1151\n",
      "Training Epoch: 15 [8320/36450]\tLoss: 479.1177\n",
      "Training Epoch: 15 [8384/36450]\tLoss: 520.0746\n",
      "Training Epoch: 15 [8448/36450]\tLoss: 522.5692\n",
      "Training Epoch: 15 [8512/36450]\tLoss: 494.8259\n",
      "Training Epoch: 15 [8576/36450]\tLoss: 515.9160\n",
      "Training Epoch: 15 [8640/36450]\tLoss: 476.9708\n",
      "Training Epoch: 15 [8704/36450]\tLoss: 525.8030\n",
      "Training Epoch: 15 [8768/36450]\tLoss: 480.1250\n",
      "Training Epoch: 15 [8832/36450]\tLoss: 478.6137\n",
      "Training Epoch: 15 [8896/36450]\tLoss: 501.0246\n",
      "Training Epoch: 15 [8960/36450]\tLoss: 487.5738\n",
      "Training Epoch: 15 [9024/36450]\tLoss: 496.0288\n",
      "Training Epoch: 15 [9088/36450]\tLoss: 484.2736\n",
      "Training Epoch: 15 [9152/36450]\tLoss: 510.0916\n",
      "Training Epoch: 15 [9216/36450]\tLoss: 492.3989\n",
      "Training Epoch: 15 [9280/36450]\tLoss: 524.3876\n",
      "Training Epoch: 15 [9344/36450]\tLoss: 506.6615\n",
      "Training Epoch: 15 [9408/36450]\tLoss: 478.3212\n",
      "Training Epoch: 15 [9472/36450]\tLoss: 518.5956\n",
      "Training Epoch: 15 [9536/36450]\tLoss: 533.0538\n",
      "Training Epoch: 15 [9600/36450]\tLoss: 504.9482\n",
      "Training Epoch: 15 [9664/36450]\tLoss: 504.2986\n",
      "Training Epoch: 15 [9728/36450]\tLoss: 503.0883\n",
      "Training Epoch: 15 [9792/36450]\tLoss: 508.1750\n",
      "Training Epoch: 15 [9856/36450]\tLoss: 494.7432\n",
      "Training Epoch: 15 [9920/36450]\tLoss: 517.8745\n",
      "Training Epoch: 15 [9984/36450]\tLoss: 490.2374\n",
      "Training Epoch: 15 [10048/36450]\tLoss: 472.6851\n",
      "Training Epoch: 15 [10112/36450]\tLoss: 486.5781\n",
      "Training Epoch: 15 [10176/36450]\tLoss: 486.8140\n",
      "Training Epoch: 15 [10240/36450]\tLoss: 508.0541\n",
      "Training Epoch: 15 [10304/36450]\tLoss: 506.2993\n",
      "Training Epoch: 15 [10368/36450]\tLoss: 481.5518\n",
      "Training Epoch: 15 [10432/36450]\tLoss: 504.8163\n",
      "Training Epoch: 15 [10496/36450]\tLoss: 546.7928\n",
      "Training Epoch: 15 [10560/36450]\tLoss: 507.8274\n",
      "Training Epoch: 15 [10624/36450]\tLoss: 487.8811\n",
      "Training Epoch: 15 [10688/36450]\tLoss: 524.1204\n",
      "Training Epoch: 15 [10752/36450]\tLoss: 480.2105\n",
      "Training Epoch: 15 [10816/36450]\tLoss: 523.4874\n",
      "Training Epoch: 15 [10880/36450]\tLoss: 512.8954\n",
      "Training Epoch: 15 [10944/36450]\tLoss: 498.1933\n",
      "Training Epoch: 15 [11008/36450]\tLoss: 481.6536\n",
      "Training Epoch: 15 [11072/36450]\tLoss: 485.8641\n",
      "Training Epoch: 15 [11136/36450]\tLoss: 521.0534\n",
      "Training Epoch: 15 [11200/36450]\tLoss: 491.8050\n",
      "Training Epoch: 15 [11264/36450]\tLoss: 486.3402\n",
      "Training Epoch: 15 [11328/36450]\tLoss: 496.8730\n",
      "Training Epoch: 15 [11392/36450]\tLoss: 501.4739\n",
      "Training Epoch: 15 [11456/36450]\tLoss: 494.5628\n",
      "Training Epoch: 15 [11520/36450]\tLoss: 502.7118\n",
      "Training Epoch: 15 [11584/36450]\tLoss: 505.2082\n",
      "Training Epoch: 15 [11648/36450]\tLoss: 516.6385\n",
      "Training Epoch: 15 [11712/36450]\tLoss: 479.2588\n",
      "Training Epoch: 15 [11776/36450]\tLoss: 520.4927\n",
      "Training Epoch: 15 [11840/36450]\tLoss: 494.9714\n",
      "Training Epoch: 15 [11904/36450]\tLoss: 482.1443\n",
      "Training Epoch: 15 [11968/36450]\tLoss: 502.5411\n",
      "Training Epoch: 15 [12032/36450]\tLoss: 538.9333\n",
      "Training Epoch: 15 [12096/36450]\tLoss: 498.6435\n",
      "Training Epoch: 15 [12160/36450]\tLoss: 498.9371\n",
      "Training Epoch: 15 [12224/36450]\tLoss: 550.2356\n",
      "Training Epoch: 15 [12288/36450]\tLoss: 495.5496\n",
      "Training Epoch: 15 [12352/36450]\tLoss: 459.7597\n",
      "Training Epoch: 15 [12416/36450]\tLoss: 520.1808\n",
      "Training Epoch: 15 [12480/36450]\tLoss: 515.1274\n",
      "Training Epoch: 15 [12544/36450]\tLoss: 492.3847\n",
      "Training Epoch: 15 [12608/36450]\tLoss: 489.0199\n",
      "Training Epoch: 15 [12672/36450]\tLoss: 541.3722\n",
      "Training Epoch: 15 [12736/36450]\tLoss: 494.7430\n",
      "Training Epoch: 15 [12800/36450]\tLoss: 482.5857\n",
      "Training Epoch: 15 [12864/36450]\tLoss: 516.6442\n",
      "Training Epoch: 15 [12928/36450]\tLoss: 507.6782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [12992/36450]\tLoss: 499.2058\n",
      "Training Epoch: 15 [13056/36450]\tLoss: 510.8893\n",
      "Training Epoch: 15 [13120/36450]\tLoss: 532.5549\n",
      "Training Epoch: 15 [13184/36450]\tLoss: 505.4864\n",
      "Training Epoch: 15 [13248/36450]\tLoss: 489.5575\n",
      "Training Epoch: 15 [13312/36450]\tLoss: 512.2489\n",
      "Training Epoch: 15 [13376/36450]\tLoss: 514.6463\n",
      "Training Epoch: 15 [13440/36450]\tLoss: 499.5754\n",
      "Training Epoch: 15 [13504/36450]\tLoss: 481.7117\n",
      "Training Epoch: 15 [13568/36450]\tLoss: 485.6016\n",
      "Training Epoch: 15 [13632/36450]\tLoss: 523.1491\n",
      "Training Epoch: 15 [13696/36450]\tLoss: 495.9507\n",
      "Training Epoch: 15 [13760/36450]\tLoss: 500.7881\n",
      "Training Epoch: 15 [13824/36450]\tLoss: 480.9346\n",
      "Training Epoch: 15 [13888/36450]\tLoss: 551.5382\n",
      "Training Epoch: 15 [13952/36450]\tLoss: 506.7936\n",
      "Training Epoch: 15 [14016/36450]\tLoss: 519.3055\n",
      "Training Epoch: 15 [14080/36450]\tLoss: 528.4158\n",
      "Training Epoch: 15 [14144/36450]\tLoss: 504.4094\n",
      "Training Epoch: 15 [14208/36450]\tLoss: 489.1786\n",
      "Training Epoch: 15 [14272/36450]\tLoss: 519.5049\n",
      "Training Epoch: 15 [14336/36450]\tLoss: 480.3857\n",
      "Training Epoch: 15 [14400/36450]\tLoss: 482.5917\n",
      "Training Epoch: 15 [14464/36450]\tLoss: 477.4089\n",
      "Training Epoch: 15 [14528/36450]\tLoss: 496.1447\n",
      "Training Epoch: 15 [14592/36450]\tLoss: 472.6014\n",
      "Training Epoch: 15 [14656/36450]\tLoss: 510.6968\n",
      "Training Epoch: 15 [14720/36450]\tLoss: 511.0212\n",
      "Training Epoch: 15 [14784/36450]\tLoss: 506.5891\n",
      "Training Epoch: 15 [14848/36450]\tLoss: 517.2370\n",
      "Training Epoch: 15 [14912/36450]\tLoss: 504.5568\n",
      "Training Epoch: 15 [14976/36450]\tLoss: 504.0685\n",
      "Training Epoch: 15 [15040/36450]\tLoss: 502.9163\n",
      "Training Epoch: 15 [15104/36450]\tLoss: 519.6251\n",
      "Training Epoch: 15 [15168/36450]\tLoss: 517.3011\n",
      "Training Epoch: 15 [15232/36450]\tLoss: 523.0910\n",
      "Training Epoch: 15 [15296/36450]\tLoss: 508.5396\n",
      "Training Epoch: 15 [15360/36450]\tLoss: 467.5291\n",
      "Training Epoch: 15 [15424/36450]\tLoss: 501.2420\n",
      "Training Epoch: 15 [15488/36450]\tLoss: 489.2277\n",
      "Training Epoch: 15 [15552/36450]\tLoss: 482.5400\n",
      "Training Epoch: 15 [15616/36450]\tLoss: 531.7527\n",
      "Training Epoch: 15 [15680/36450]\tLoss: 512.6204\n",
      "Training Epoch: 15 [15744/36450]\tLoss: 489.8958\n",
      "Training Epoch: 15 [15808/36450]\tLoss: 488.1095\n",
      "Training Epoch: 15 [15872/36450]\tLoss: 508.4877\n",
      "Training Epoch: 15 [15936/36450]\tLoss: 499.8377\n",
      "Training Epoch: 15 [16000/36450]\tLoss: 513.0184\n",
      "Training Epoch: 15 [16064/36450]\tLoss: 486.1026\n",
      "Training Epoch: 15 [16128/36450]\tLoss: 478.8015\n",
      "Training Epoch: 15 [16192/36450]\tLoss: 473.5630\n",
      "Training Epoch: 15 [16256/36450]\tLoss: 509.0253\n",
      "Training Epoch: 15 [16320/36450]\tLoss: 513.1213\n",
      "Training Epoch: 15 [16384/36450]\tLoss: 498.3168\n",
      "Training Epoch: 15 [16448/36450]\tLoss: 478.9890\n",
      "Training Epoch: 15 [16512/36450]\tLoss: 525.9061\n",
      "Training Epoch: 15 [16576/36450]\tLoss: 542.0749\n",
      "Training Epoch: 15 [16640/36450]\tLoss: 487.1764\n",
      "Training Epoch: 15 [16704/36450]\tLoss: 480.7401\n",
      "Training Epoch: 15 [16768/36450]\tLoss: 486.0385\n",
      "Training Epoch: 15 [16832/36450]\tLoss: 497.5084\n",
      "Training Epoch: 15 [16896/36450]\tLoss: 490.6408\n",
      "Training Epoch: 15 [16960/36450]\tLoss: 516.4907\n",
      "Training Epoch: 15 [17024/36450]\tLoss: 519.6586\n",
      "Training Epoch: 15 [17088/36450]\tLoss: 496.9910\n",
      "Training Epoch: 15 [17152/36450]\tLoss: 527.9391\n",
      "Training Epoch: 15 [17216/36450]\tLoss: 492.8056\n",
      "Training Epoch: 15 [17280/36450]\tLoss: 518.2913\n",
      "Training Epoch: 15 [17344/36450]\tLoss: 491.6637\n",
      "Training Epoch: 15 [17408/36450]\tLoss: 520.3745\n",
      "Training Epoch: 15 [17472/36450]\tLoss: 485.7382\n",
      "Training Epoch: 15 [17536/36450]\tLoss: 484.2759\n",
      "Training Epoch: 15 [17600/36450]\tLoss: 515.5811\n",
      "Training Epoch: 15 [17664/36450]\tLoss: 489.8802\n",
      "Training Epoch: 15 [17728/36450]\tLoss: 503.0111\n",
      "Training Epoch: 15 [17792/36450]\tLoss: 500.5918\n",
      "Training Epoch: 15 [17856/36450]\tLoss: 489.9767\n",
      "Training Epoch: 15 [17920/36450]\tLoss: 519.4337\n",
      "Training Epoch: 15 [17984/36450]\tLoss: 512.5141\n",
      "Training Epoch: 15 [18048/36450]\tLoss: 528.1482\n",
      "Training Epoch: 15 [18112/36450]\tLoss: 497.0393\n",
      "Training Epoch: 15 [18176/36450]\tLoss: 510.0344\n",
      "Training Epoch: 15 [18240/36450]\tLoss: 521.8726\n",
      "Training Epoch: 15 [18304/36450]\tLoss: 529.5603\n",
      "Training Epoch: 15 [18368/36450]\tLoss: 521.2344\n",
      "Training Epoch: 15 [18432/36450]\tLoss: 541.1901\n",
      "Training Epoch: 15 [18496/36450]\tLoss: 528.2455\n",
      "Training Epoch: 15 [18560/36450]\tLoss: 527.3380\n",
      "Training Epoch: 15 [18624/36450]\tLoss: 534.0669\n",
      "Training Epoch: 15 [18688/36450]\tLoss: 539.0998\n",
      "Training Epoch: 15 [18752/36450]\tLoss: 476.5570\n",
      "Training Epoch: 15 [18816/36450]\tLoss: 494.7338\n",
      "Training Epoch: 15 [18880/36450]\tLoss: 488.5624\n",
      "Training Epoch: 15 [18944/36450]\tLoss: 527.2906\n",
      "Training Epoch: 15 [19008/36450]\tLoss: 501.1069\n",
      "Training Epoch: 15 [19072/36450]\tLoss: 507.8813\n",
      "Training Epoch: 15 [19136/36450]\tLoss: 525.5251\n",
      "Training Epoch: 15 [19200/36450]\tLoss: 526.7828\n",
      "Training Epoch: 15 [19264/36450]\tLoss: 565.0367\n",
      "Training Epoch: 15 [19328/36450]\tLoss: 550.7521\n",
      "Training Epoch: 15 [19392/36450]\tLoss: 536.2883\n",
      "Training Epoch: 15 [19456/36450]\tLoss: 538.4209\n",
      "Training Epoch: 15 [19520/36450]\tLoss: 521.8128\n",
      "Training Epoch: 15 [19584/36450]\tLoss: 483.7365\n",
      "Training Epoch: 15 [19648/36450]\tLoss: 512.5390\n",
      "Training Epoch: 15 [19712/36450]\tLoss: 498.6793\n",
      "Training Epoch: 15 [19776/36450]\tLoss: 535.1542\n",
      "Training Epoch: 15 [19840/36450]\tLoss: 507.1050\n",
      "Training Epoch: 15 [19904/36450]\tLoss: 495.1761\n",
      "Training Epoch: 15 [19968/36450]\tLoss: 537.0320\n",
      "Training Epoch: 15 [20032/36450]\tLoss: 526.8945\n",
      "Training Epoch: 15 [20096/36450]\tLoss: 517.9168\n",
      "Training Epoch: 15 [20160/36450]\tLoss: 492.3660\n",
      "Training Epoch: 15 [20224/36450]\tLoss: 511.0246\n",
      "Training Epoch: 15 [20288/36450]\tLoss: 483.2331\n",
      "Training Epoch: 15 [20352/36450]\tLoss: 495.8906\n",
      "Training Epoch: 15 [20416/36450]\tLoss: 513.8226\n",
      "Training Epoch: 15 [20480/36450]\tLoss: 496.1999\n",
      "Training Epoch: 15 [20544/36450]\tLoss: 497.4754\n",
      "Training Epoch: 15 [20608/36450]\tLoss: 503.3731\n",
      "Training Epoch: 15 [20672/36450]\tLoss: 477.0610\n",
      "Training Epoch: 15 [20736/36450]\tLoss: 518.7543\n",
      "Training Epoch: 15 [20800/36450]\tLoss: 499.3156\n",
      "Training Epoch: 15 [20864/36450]\tLoss: 489.9444\n",
      "Training Epoch: 15 [20928/36450]\tLoss: 492.4168\n",
      "Training Epoch: 15 [20992/36450]\tLoss: 519.0117\n",
      "Training Epoch: 15 [21056/36450]\tLoss: 527.4470\n",
      "Training Epoch: 15 [21120/36450]\tLoss: 504.6744\n",
      "Training Epoch: 15 [21184/36450]\tLoss: 524.9223\n",
      "Training Epoch: 15 [21248/36450]\tLoss: 555.5248\n",
      "Training Epoch: 15 [21312/36450]\tLoss: 481.9883\n",
      "Training Epoch: 15 [21376/36450]\tLoss: 496.3359\n",
      "Training Epoch: 15 [21440/36450]\tLoss: 519.8006\n",
      "Training Epoch: 15 [21504/36450]\tLoss: 506.5294\n",
      "Training Epoch: 15 [21568/36450]\tLoss: 488.4062\n",
      "Training Epoch: 15 [21632/36450]\tLoss: 475.1220\n",
      "Training Epoch: 15 [21696/36450]\tLoss: 513.0076\n",
      "Training Epoch: 15 [21760/36450]\tLoss: 479.6328\n",
      "Training Epoch: 15 [21824/36450]\tLoss: 503.1804\n",
      "Training Epoch: 15 [21888/36450]\tLoss: 495.2964\n",
      "Training Epoch: 15 [21952/36450]\tLoss: 526.6197\n",
      "Training Epoch: 15 [22016/36450]\tLoss: 496.6532\n",
      "Training Epoch: 15 [22080/36450]\tLoss: 512.0248\n",
      "Training Epoch: 15 [22144/36450]\tLoss: 487.1575\n",
      "Training Epoch: 15 [22208/36450]\tLoss: 518.5734\n",
      "Training Epoch: 15 [22272/36450]\tLoss: 496.5912\n",
      "Training Epoch: 15 [22336/36450]\tLoss: 505.3862\n",
      "Training Epoch: 15 [22400/36450]\tLoss: 495.2646\n",
      "Training Epoch: 15 [22464/36450]\tLoss: 466.9461\n",
      "Training Epoch: 15 [22528/36450]\tLoss: 497.2786\n",
      "Training Epoch: 15 [22592/36450]\tLoss: 477.2192\n",
      "Training Epoch: 15 [22656/36450]\tLoss: 520.2019\n",
      "Training Epoch: 15 [22720/36450]\tLoss: 463.1708\n",
      "Training Epoch: 15 [22784/36450]\tLoss: 510.5652\n",
      "Training Epoch: 15 [22848/36450]\tLoss: 494.8543\n",
      "Training Epoch: 15 [22912/36450]\tLoss: 480.1657\n",
      "Training Epoch: 15 [22976/36450]\tLoss: 503.7313\n",
      "Training Epoch: 15 [23040/36450]\tLoss: 502.3185\n",
      "Training Epoch: 15 [23104/36450]\tLoss: 511.8920\n",
      "Training Epoch: 15 [23168/36450]\tLoss: 513.9769\n",
      "Training Epoch: 15 [23232/36450]\tLoss: 483.5259\n",
      "Training Epoch: 15 [23296/36450]\tLoss: 492.4443\n",
      "Training Epoch: 15 [23360/36450]\tLoss: 492.0073\n",
      "Training Epoch: 15 [23424/36450]\tLoss: 490.4678\n",
      "Training Epoch: 15 [23488/36450]\tLoss: 471.6933\n",
      "Training Epoch: 15 [23552/36450]\tLoss: 496.3529\n",
      "Training Epoch: 15 [23616/36450]\tLoss: 506.5272\n",
      "Training Epoch: 15 [23680/36450]\tLoss: 479.5591\n",
      "Training Epoch: 15 [23744/36450]\tLoss: 469.7194\n",
      "Training Epoch: 15 [23808/36450]\tLoss: 500.1561\n",
      "Training Epoch: 15 [23872/36450]\tLoss: 488.6090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [23936/36450]\tLoss: 521.3497\n",
      "Training Epoch: 15 [24000/36450]\tLoss: 505.6016\n",
      "Training Epoch: 15 [24064/36450]\tLoss: 518.3437\n",
      "Training Epoch: 15 [24128/36450]\tLoss: 480.3282\n",
      "Training Epoch: 15 [24192/36450]\tLoss: 513.4467\n",
      "Training Epoch: 15 [24256/36450]\tLoss: 504.2741\n",
      "Training Epoch: 15 [24320/36450]\tLoss: 501.7407\n",
      "Training Epoch: 15 [24384/36450]\tLoss: 502.6589\n",
      "Training Epoch: 15 [24448/36450]\tLoss: 509.4372\n",
      "Training Epoch: 15 [24512/36450]\tLoss: 505.8937\n",
      "Training Epoch: 15 [24576/36450]\tLoss: 478.3841\n",
      "Training Epoch: 15 [24640/36450]\tLoss: 494.1339\n",
      "Training Epoch: 15 [24704/36450]\tLoss: 485.7822\n",
      "Training Epoch: 15 [24768/36450]\tLoss: 485.8178\n",
      "Training Epoch: 15 [24832/36450]\tLoss: 483.8915\n",
      "Training Epoch: 15 [24896/36450]\tLoss: 485.2606\n",
      "Training Epoch: 15 [24960/36450]\tLoss: 504.8206\n",
      "Training Epoch: 15 [25024/36450]\tLoss: 532.0536\n",
      "Training Epoch: 15 [25088/36450]\tLoss: 471.7530\n",
      "Training Epoch: 15 [25152/36450]\tLoss: 528.4683\n",
      "Training Epoch: 15 [25216/36450]\tLoss: 496.5556\n",
      "Training Epoch: 15 [25280/36450]\tLoss: 484.0165\n",
      "Training Epoch: 15 [25344/36450]\tLoss: 488.0637\n",
      "Training Epoch: 15 [25408/36450]\tLoss: 531.5651\n",
      "Training Epoch: 15 [25472/36450]\tLoss: 493.2780\n",
      "Training Epoch: 15 [25536/36450]\tLoss: 528.9733\n",
      "Training Epoch: 15 [25600/36450]\tLoss: 488.6633\n",
      "Training Epoch: 15 [25664/36450]\tLoss: 484.0414\n",
      "Training Epoch: 15 [25728/36450]\tLoss: 502.2534\n",
      "Training Epoch: 15 [25792/36450]\tLoss: 512.9676\n",
      "Training Epoch: 15 [25856/36450]\tLoss: 505.4974\n",
      "Training Epoch: 15 [25920/36450]\tLoss: 501.3195\n",
      "Training Epoch: 15 [25984/36450]\tLoss: 492.1059\n",
      "Training Epoch: 15 [26048/36450]\tLoss: 510.5786\n",
      "Training Epoch: 15 [26112/36450]\tLoss: 501.3217\n",
      "Training Epoch: 15 [26176/36450]\tLoss: 499.0182\n",
      "Training Epoch: 15 [26240/36450]\tLoss: 512.3476\n",
      "Training Epoch: 15 [26304/36450]\tLoss: 516.9462\n",
      "Training Epoch: 15 [26368/36450]\tLoss: 508.8091\n",
      "Training Epoch: 15 [26432/36450]\tLoss: 505.9763\n",
      "Training Epoch: 15 [26496/36450]\tLoss: 489.0050\n",
      "Training Epoch: 15 [26560/36450]\tLoss: 480.9742\n",
      "Training Epoch: 15 [26624/36450]\tLoss: 507.9372\n",
      "Training Epoch: 15 [26688/36450]\tLoss: 511.5127\n",
      "Training Epoch: 15 [26752/36450]\tLoss: 504.7136\n",
      "Training Epoch: 15 [26816/36450]\tLoss: 508.7706\n",
      "Training Epoch: 15 [26880/36450]\tLoss: 491.2807\n",
      "Training Epoch: 15 [26944/36450]\tLoss: 516.0875\n",
      "Training Epoch: 15 [27008/36450]\tLoss: 467.8595\n",
      "Training Epoch: 15 [27072/36450]\tLoss: 511.7958\n",
      "Training Epoch: 15 [27136/36450]\tLoss: 518.0510\n",
      "Training Epoch: 15 [27200/36450]\tLoss: 484.1436\n",
      "Training Epoch: 15 [27264/36450]\tLoss: 518.3586\n",
      "Training Epoch: 15 [27328/36450]\tLoss: 482.1271\n",
      "Training Epoch: 15 [27392/36450]\tLoss: 504.2427\n",
      "Training Epoch: 15 [27456/36450]\tLoss: 487.2529\n",
      "Training Epoch: 15 [27520/36450]\tLoss: 491.2850\n",
      "Training Epoch: 15 [27584/36450]\tLoss: 509.5680\n",
      "Training Epoch: 15 [27648/36450]\tLoss: 489.2101\n",
      "Training Epoch: 15 [27712/36450]\tLoss: 517.1581\n",
      "Training Epoch: 15 [27776/36450]\tLoss: 480.1095\n",
      "Training Epoch: 15 [27840/36450]\tLoss: 505.8364\n",
      "Training Epoch: 15 [27904/36450]\tLoss: 508.0995\n",
      "Training Epoch: 15 [27968/36450]\tLoss: 516.3000\n",
      "Training Epoch: 15 [28032/36450]\tLoss: 499.4824\n",
      "Training Epoch: 15 [28096/36450]\tLoss: 490.7763\n",
      "Training Epoch: 15 [28160/36450]\tLoss: 496.9876\n",
      "Training Epoch: 15 [28224/36450]\tLoss: 495.5742\n",
      "Training Epoch: 15 [28288/36450]\tLoss: 513.4956\n",
      "Training Epoch: 15 [28352/36450]\tLoss: 494.6549\n",
      "Training Epoch: 15 [28416/36450]\tLoss: 492.2104\n",
      "Training Epoch: 15 [28480/36450]\tLoss: 509.2586\n",
      "Training Epoch: 15 [28544/36450]\tLoss: 500.4787\n",
      "Training Epoch: 15 [28608/36450]\tLoss: 506.2530\n",
      "Training Epoch: 15 [28672/36450]\tLoss: 521.4116\n",
      "Training Epoch: 15 [28736/36450]\tLoss: 471.2459\n",
      "Training Epoch: 15 [28800/36450]\tLoss: 506.3628\n",
      "Training Epoch: 15 [28864/36450]\tLoss: 463.7659\n",
      "Training Epoch: 15 [28928/36450]\tLoss: 507.2450\n",
      "Training Epoch: 15 [28992/36450]\tLoss: 493.2568\n",
      "Training Epoch: 15 [29056/36450]\tLoss: 522.3047\n",
      "Training Epoch: 15 [29120/36450]\tLoss: 516.6866\n",
      "Training Epoch: 15 [29184/36450]\tLoss: 535.7922\n",
      "Training Epoch: 15 [29248/36450]\tLoss: 490.7600\n",
      "Training Epoch: 15 [29312/36450]\tLoss: 499.9907\n",
      "Training Epoch: 15 [29376/36450]\tLoss: 529.4204\n",
      "Training Epoch: 15 [29440/36450]\tLoss: 479.2460\n",
      "Training Epoch: 15 [29504/36450]\tLoss: 464.7449\n",
      "Training Epoch: 15 [29568/36450]\tLoss: 474.0551\n",
      "Training Epoch: 15 [29632/36450]\tLoss: 474.7296\n",
      "Training Epoch: 15 [29696/36450]\tLoss: 476.8550\n",
      "Training Epoch: 15 [29760/36450]\tLoss: 517.6937\n",
      "Training Epoch: 15 [29824/36450]\tLoss: 479.2695\n",
      "Training Epoch: 15 [29888/36450]\tLoss: 507.0184\n",
      "Training Epoch: 15 [29952/36450]\tLoss: 493.0728\n",
      "Training Epoch: 15 [30016/36450]\tLoss: 508.2395\n",
      "Training Epoch: 15 [30080/36450]\tLoss: 514.7000\n",
      "Training Epoch: 15 [30144/36450]\tLoss: 503.6794\n",
      "Training Epoch: 15 [30208/36450]\tLoss: 479.2332\n",
      "Training Epoch: 15 [30272/36450]\tLoss: 467.0827\n",
      "Training Epoch: 15 [30336/36450]\tLoss: 518.3818\n",
      "Training Epoch: 15 [30400/36450]\tLoss: 501.7912\n",
      "Training Epoch: 15 [30464/36450]\tLoss: 504.2186\n",
      "Training Epoch: 15 [30528/36450]\tLoss: 520.3120\n",
      "Training Epoch: 15 [30592/36450]\tLoss: 498.8594\n",
      "Training Epoch: 15 [30656/36450]\tLoss: 528.0023\n",
      "Training Epoch: 15 [30720/36450]\tLoss: 586.9377\n",
      "Training Epoch: 15 [30784/36450]\tLoss: 564.8291\n",
      "Training Epoch: 15 [30848/36450]\tLoss: 617.4646\n",
      "Training Epoch: 15 [30912/36450]\tLoss: 609.3618\n",
      "Training Epoch: 15 [30976/36450]\tLoss: 549.8257\n",
      "Training Epoch: 15 [31040/36450]\tLoss: 553.9114\n",
      "Training Epoch: 15 [31104/36450]\tLoss: 475.3429\n",
      "Training Epoch: 15 [31168/36450]\tLoss: 480.9617\n",
      "Training Epoch: 15 [31232/36450]\tLoss: 511.1031\n",
      "Training Epoch: 15 [31296/36450]\tLoss: 548.0446\n",
      "Training Epoch: 15 [31360/36450]\tLoss: 558.5185\n",
      "Training Epoch: 15 [31424/36450]\tLoss: 563.0513\n",
      "Training Epoch: 15 [31488/36450]\tLoss: 534.3695\n",
      "Training Epoch: 15 [31552/36450]\tLoss: 522.1060\n",
      "Training Epoch: 15 [31616/36450]\tLoss: 525.7375\n",
      "Training Epoch: 15 [31680/36450]\tLoss: 513.3806\n",
      "Training Epoch: 15 [31744/36450]\tLoss: 487.5687\n",
      "Training Epoch: 15 [31808/36450]\tLoss: 494.9506\n",
      "Training Epoch: 15 [31872/36450]\tLoss: 502.6241\n",
      "Training Epoch: 15 [31936/36450]\tLoss: 482.8118\n",
      "Training Epoch: 15 [32000/36450]\tLoss: 487.0293\n",
      "Training Epoch: 15 [32064/36450]\tLoss: 484.4306\n",
      "Training Epoch: 15 [32128/36450]\tLoss: 529.0356\n",
      "Training Epoch: 15 [32192/36450]\tLoss: 500.0933\n",
      "Training Epoch: 15 [32256/36450]\tLoss: 484.2792\n",
      "Training Epoch: 15 [32320/36450]\tLoss: 497.6653\n",
      "Training Epoch: 15 [32384/36450]\tLoss: 528.5064\n",
      "Training Epoch: 15 [32448/36450]\tLoss: 467.9727\n",
      "Training Epoch: 15 [32512/36450]\tLoss: 504.3154\n",
      "Training Epoch: 15 [32576/36450]\tLoss: 513.5640\n",
      "Training Epoch: 15 [32640/36450]\tLoss: 500.2502\n",
      "Training Epoch: 15 [32704/36450]\tLoss: 491.2741\n",
      "Training Epoch: 15 [32768/36450]\tLoss: 503.2859\n",
      "Training Epoch: 15 [32832/36450]\tLoss: 503.4957\n",
      "Training Epoch: 15 [32896/36450]\tLoss: 510.2303\n",
      "Training Epoch: 15 [32960/36450]\tLoss: 506.8575\n",
      "Training Epoch: 15 [33024/36450]\tLoss: 521.2098\n",
      "Training Epoch: 15 [33088/36450]\tLoss: 482.0415\n",
      "Training Epoch: 15 [33152/36450]\tLoss: 527.4125\n",
      "Training Epoch: 15 [33216/36450]\tLoss: 487.3122\n",
      "Training Epoch: 15 [33280/36450]\tLoss: 486.4791\n",
      "Training Epoch: 15 [33344/36450]\tLoss: 511.7484\n",
      "Training Epoch: 15 [33408/36450]\tLoss: 517.2618\n",
      "Training Epoch: 15 [33472/36450]\tLoss: 500.1900\n",
      "Training Epoch: 15 [33536/36450]\tLoss: 529.4883\n",
      "Training Epoch: 15 [33600/36450]\tLoss: 512.7562\n",
      "Training Epoch: 15 [33664/36450]\tLoss: 501.8363\n",
      "Training Epoch: 15 [33728/36450]\tLoss: 506.2612\n",
      "Training Epoch: 15 [33792/36450]\tLoss: 475.0820\n",
      "Training Epoch: 15 [33856/36450]\tLoss: 486.0418\n",
      "Training Epoch: 15 [33920/36450]\tLoss: 479.8228\n",
      "Training Epoch: 15 [33984/36450]\tLoss: 498.8631\n",
      "Training Epoch: 15 [34048/36450]\tLoss: 516.6888\n",
      "Training Epoch: 15 [34112/36450]\tLoss: 494.6617\n",
      "Training Epoch: 15 [34176/36450]\tLoss: 503.2848\n",
      "Training Epoch: 15 [34240/36450]\tLoss: 484.6053\n",
      "Training Epoch: 15 [34304/36450]\tLoss: 498.5135\n",
      "Training Epoch: 15 [34368/36450]\tLoss: 498.2719\n",
      "Training Epoch: 15 [34432/36450]\tLoss: 508.5515\n",
      "Training Epoch: 15 [34496/36450]\tLoss: 476.3501\n",
      "Training Epoch: 15 [34560/36450]\tLoss: 514.9572\n",
      "Training Epoch: 15 [34624/36450]\tLoss: 559.9055\n",
      "Training Epoch: 15 [34688/36450]\tLoss: 506.6511\n",
      "Training Epoch: 15 [34752/36450]\tLoss: 488.1661\n",
      "Training Epoch: 15 [34816/36450]\tLoss: 490.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [34880/36450]\tLoss: 503.6549\n",
      "Training Epoch: 15 [34944/36450]\tLoss: 478.9168\n",
      "Training Epoch: 15 [35008/36450]\tLoss: 465.1057\n",
      "Training Epoch: 15 [35072/36450]\tLoss: 498.2076\n",
      "Training Epoch: 15 [35136/36450]\tLoss: 499.1801\n",
      "Training Epoch: 15 [35200/36450]\tLoss: 490.7168\n",
      "Training Epoch: 15 [35264/36450]\tLoss: 518.2911\n",
      "Training Epoch: 15 [35328/36450]\tLoss: 504.8477\n",
      "Training Epoch: 15 [35392/36450]\tLoss: 511.5226\n",
      "Training Epoch: 15 [35456/36450]\tLoss: 497.6324\n",
      "Training Epoch: 15 [35520/36450]\tLoss: 470.8314\n",
      "Training Epoch: 15 [35584/36450]\tLoss: 492.3971\n",
      "Training Epoch: 15 [35648/36450]\tLoss: 496.0593\n",
      "Training Epoch: 15 [35712/36450]\tLoss: 485.1150\n",
      "Training Epoch: 15 [35776/36450]\tLoss: 484.6288\n",
      "Training Epoch: 15 [35840/36450]\tLoss: 485.6528\n",
      "Training Epoch: 15 [35904/36450]\tLoss: 480.8963\n",
      "Training Epoch: 15 [35968/36450]\tLoss: 480.8052\n",
      "Training Epoch: 15 [36032/36450]\tLoss: 484.7109\n",
      "Training Epoch: 15 [36096/36450]\tLoss: 516.0709\n",
      "Training Epoch: 15 [36160/36450]\tLoss: 484.5829\n",
      "Training Epoch: 15 [36224/36450]\tLoss: 506.4058\n",
      "Training Epoch: 15 [36288/36450]\tLoss: 536.9208\n",
      "Training Epoch: 15 [36352/36450]\tLoss: 479.1013\n",
      "Training Epoch: 15 [36416/36450]\tLoss: 496.4308\n",
      "Training Epoch: 15 [36450/36450]\tLoss: 533.3608\n",
      "Training Epoch: 15 [4050/4050]\tLoss: 246.1170\n",
      "Training Epoch: 16 [64/36450]\tLoss: 471.1953\n",
      "Training Epoch: 16 [128/36450]\tLoss: 488.0514\n",
      "Training Epoch: 16 [192/36450]\tLoss: 477.9401\n",
      "Training Epoch: 16 [256/36450]\tLoss: 505.5741\n",
      "Training Epoch: 16 [320/36450]\tLoss: 495.7202\n",
      "Training Epoch: 16 [384/36450]\tLoss: 488.9254\n",
      "Training Epoch: 16 [448/36450]\tLoss: 489.3549\n",
      "Training Epoch: 16 [512/36450]\tLoss: 491.2401\n",
      "Training Epoch: 16 [576/36450]\tLoss: 504.9413\n",
      "Training Epoch: 16 [640/36450]\tLoss: 481.9015\n",
      "Training Epoch: 16 [704/36450]\tLoss: 490.0240\n",
      "Training Epoch: 16 [768/36450]\tLoss: 502.1475\n",
      "Training Epoch: 16 [832/36450]\tLoss: 483.1797\n",
      "Training Epoch: 16 [896/36450]\tLoss: 472.9698\n",
      "Training Epoch: 16 [960/36450]\tLoss: 485.7130\n",
      "Training Epoch: 16 [1024/36450]\tLoss: 492.5822\n",
      "Training Epoch: 16 [1088/36450]\tLoss: 481.2704\n",
      "Training Epoch: 16 [1152/36450]\tLoss: 474.7892\n",
      "Training Epoch: 16 [1216/36450]\tLoss: 527.8303\n",
      "Training Epoch: 16 [1280/36450]\tLoss: 492.0260\n",
      "Training Epoch: 16 [1344/36450]\tLoss: 501.3507\n",
      "Training Epoch: 16 [1408/36450]\tLoss: 484.9388\n",
      "Training Epoch: 16 [1472/36450]\tLoss: 491.0466\n",
      "Training Epoch: 16 [1536/36450]\tLoss: 502.3693\n",
      "Training Epoch: 16 [1600/36450]\tLoss: 490.2707\n",
      "Training Epoch: 16 [1664/36450]\tLoss: 502.6936\n",
      "Training Epoch: 16 [1728/36450]\tLoss: 501.7444\n",
      "Training Epoch: 16 [1792/36450]\tLoss: 526.6305\n",
      "Training Epoch: 16 [1856/36450]\tLoss: 519.8406\n",
      "Training Epoch: 16 [1920/36450]\tLoss: 489.8772\n",
      "Training Epoch: 16 [1984/36450]\tLoss: 489.6999\n",
      "Training Epoch: 16 [2048/36450]\tLoss: 484.0631\n",
      "Training Epoch: 16 [2112/36450]\tLoss: 486.4429\n",
      "Training Epoch: 16 [2176/36450]\tLoss: 465.3535\n",
      "Training Epoch: 16 [2240/36450]\tLoss: 497.0323\n",
      "Training Epoch: 16 [2304/36450]\tLoss: 508.0990\n",
      "Training Epoch: 16 [2368/36450]\tLoss: 498.4257\n",
      "Training Epoch: 16 [2432/36450]\tLoss: 491.8532\n",
      "Training Epoch: 16 [2496/36450]\tLoss: 494.6696\n",
      "Training Epoch: 16 [2560/36450]\tLoss: 502.3612\n",
      "Training Epoch: 16 [2624/36450]\tLoss: 523.0787\n",
      "Training Epoch: 16 [2688/36450]\tLoss: 491.5679\n",
      "Training Epoch: 16 [2752/36450]\tLoss: 495.8624\n",
      "Training Epoch: 16 [2816/36450]\tLoss: 537.2686\n",
      "Training Epoch: 16 [2880/36450]\tLoss: 511.9099\n",
      "Training Epoch: 16 [2944/36450]\tLoss: 499.1475\n",
      "Training Epoch: 16 [3008/36450]\tLoss: 509.6733\n",
      "Training Epoch: 16 [3072/36450]\tLoss: 495.9147\n",
      "Training Epoch: 16 [3136/36450]\tLoss: 505.3522\n",
      "Training Epoch: 16 [3200/36450]\tLoss: 495.2257\n",
      "Training Epoch: 16 [3264/36450]\tLoss: 493.4948\n",
      "Training Epoch: 16 [3328/36450]\tLoss: 520.0816\n",
      "Training Epoch: 16 [3392/36450]\tLoss: 477.6130\n",
      "Training Epoch: 16 [3456/36450]\tLoss: 511.1238\n",
      "Training Epoch: 16 [3520/36450]\tLoss: 506.1923\n",
      "Training Epoch: 16 [3584/36450]\tLoss: 521.0059\n",
      "Training Epoch: 16 [3648/36450]\tLoss: 488.3961\n",
      "Training Epoch: 16 [3712/36450]\tLoss: 520.7266\n",
      "Training Epoch: 16 [3776/36450]\tLoss: 498.4162\n",
      "Training Epoch: 16 [3840/36450]\tLoss: 517.9445\n",
      "Training Epoch: 16 [3904/36450]\tLoss: 475.2124\n",
      "Training Epoch: 16 [3968/36450]\tLoss: 512.0569\n",
      "Training Epoch: 16 [4032/36450]\tLoss: 515.5880\n",
      "Training Epoch: 16 [4096/36450]\tLoss: 522.8831\n",
      "Training Epoch: 16 [4160/36450]\tLoss: 516.7087\n",
      "Training Epoch: 16 [4224/36450]\tLoss: 518.0580\n",
      "Training Epoch: 16 [4288/36450]\tLoss: 524.7843\n",
      "Training Epoch: 16 [4352/36450]\tLoss: 492.8120\n",
      "Training Epoch: 16 [4416/36450]\tLoss: 497.8505\n",
      "Training Epoch: 16 [4480/36450]\tLoss: 480.7531\n",
      "Training Epoch: 16 [4544/36450]\tLoss: 508.5047\n",
      "Training Epoch: 16 [4608/36450]\tLoss: 508.5045\n",
      "Training Epoch: 16 [4672/36450]\tLoss: 537.3044\n",
      "Training Epoch: 16 [4736/36450]\tLoss: 484.9429\n",
      "Training Epoch: 16 [4800/36450]\tLoss: 510.2935\n",
      "Training Epoch: 16 [4864/36450]\tLoss: 501.1533\n",
      "Training Epoch: 16 [4928/36450]\tLoss: 479.2622\n",
      "Training Epoch: 16 [4992/36450]\tLoss: 473.1122\n",
      "Training Epoch: 16 [5056/36450]\tLoss: 499.6087\n",
      "Training Epoch: 16 [5120/36450]\tLoss: 509.4442\n",
      "Training Epoch: 16 [5184/36450]\tLoss: 497.1985\n",
      "Training Epoch: 16 [5248/36450]\tLoss: 496.2281\n",
      "Training Epoch: 16 [5312/36450]\tLoss: 488.6345\n",
      "Training Epoch: 16 [5376/36450]\tLoss: 495.5653\n",
      "Training Epoch: 16 [5440/36450]\tLoss: 458.9203\n",
      "Training Epoch: 16 [5504/36450]\tLoss: 512.1608\n",
      "Training Epoch: 16 [5568/36450]\tLoss: 501.4317\n",
      "Training Epoch: 16 [5632/36450]\tLoss: 488.5194\n",
      "Training Epoch: 16 [5696/36450]\tLoss: 504.4292\n",
      "Training Epoch: 16 [5760/36450]\tLoss: 482.6714\n",
      "Training Epoch: 16 [5824/36450]\tLoss: 495.3583\n",
      "Training Epoch: 16 [5888/36450]\tLoss: 477.3987\n",
      "Training Epoch: 16 [5952/36450]\tLoss: 472.2013\n",
      "Training Epoch: 16 [6016/36450]\tLoss: 492.2586\n",
      "Training Epoch: 16 [6080/36450]\tLoss: 488.5907\n",
      "Training Epoch: 16 [6144/36450]\tLoss: 524.0618\n",
      "Training Epoch: 16 [6208/36450]\tLoss: 516.2764\n",
      "Training Epoch: 16 [6272/36450]\tLoss: 465.8435\n",
      "Training Epoch: 16 [6336/36450]\tLoss: 488.1360\n",
      "Training Epoch: 16 [6400/36450]\tLoss: 482.3785\n",
      "Training Epoch: 16 [6464/36450]\tLoss: 497.9007\n",
      "Training Epoch: 16 [6528/36450]\tLoss: 488.4598\n",
      "Training Epoch: 16 [6592/36450]\tLoss: 514.4884\n",
      "Training Epoch: 16 [6656/36450]\tLoss: 471.0028\n",
      "Training Epoch: 16 [6720/36450]\tLoss: 521.7837\n",
      "Training Epoch: 16 [6784/36450]\tLoss: 500.1284\n",
      "Training Epoch: 16 [6848/36450]\tLoss: 509.2141\n",
      "Training Epoch: 16 [6912/36450]\tLoss: 508.5237\n",
      "Training Epoch: 16 [6976/36450]\tLoss: 503.4323\n",
      "Training Epoch: 16 [7040/36450]\tLoss: 525.5955\n",
      "Training Epoch: 16 [7104/36450]\tLoss: 523.1055\n",
      "Training Epoch: 16 [7168/36450]\tLoss: 497.8573\n",
      "Training Epoch: 16 [7232/36450]\tLoss: 493.7055\n",
      "Training Epoch: 16 [7296/36450]\tLoss: 496.6129\n",
      "Training Epoch: 16 [7360/36450]\tLoss: 487.2505\n",
      "Training Epoch: 16 [7424/36450]\tLoss: 480.3538\n",
      "Training Epoch: 16 [7488/36450]\tLoss: 504.1790\n",
      "Training Epoch: 16 [7552/36450]\tLoss: 523.7214\n",
      "Training Epoch: 16 [7616/36450]\tLoss: 492.5589\n",
      "Training Epoch: 16 [7680/36450]\tLoss: 518.2100\n",
      "Training Epoch: 16 [7744/36450]\tLoss: 487.2210\n",
      "Training Epoch: 16 [7808/36450]\tLoss: 527.4625\n",
      "Training Epoch: 16 [7872/36450]\tLoss: 488.7188\n",
      "Training Epoch: 16 [7936/36450]\tLoss: 557.4174\n",
      "Training Epoch: 16 [8000/36450]\tLoss: 463.6543\n",
      "Training Epoch: 16 [8064/36450]\tLoss: 499.1710\n",
      "Training Epoch: 16 [8128/36450]\tLoss: 478.6456\n",
      "Training Epoch: 16 [8192/36450]\tLoss: 485.3098\n",
      "Training Epoch: 16 [8256/36450]\tLoss: 499.1461\n",
      "Training Epoch: 16 [8320/36450]\tLoss: 492.8764\n",
      "Training Epoch: 16 [8384/36450]\tLoss: 501.7683\n",
      "Training Epoch: 16 [8448/36450]\tLoss: 501.3618\n",
      "Training Epoch: 16 [8512/36450]\tLoss: 528.1190\n",
      "Training Epoch: 16 [8576/36450]\tLoss: 516.7831\n",
      "Training Epoch: 16 [8640/36450]\tLoss: 497.6170\n",
      "Training Epoch: 16 [8704/36450]\tLoss: 488.3876\n",
      "Training Epoch: 16 [8768/36450]\tLoss: 538.4819\n",
      "Training Epoch: 16 [8832/36450]\tLoss: 507.7495\n",
      "Training Epoch: 16 [8896/36450]\tLoss: 512.5112\n",
      "Training Epoch: 16 [8960/36450]\tLoss: 485.9387\n",
      "Training Epoch: 16 [9024/36450]\tLoss: 481.1121\n",
      "Training Epoch: 16 [9088/36450]\tLoss: 482.6265\n",
      "Training Epoch: 16 [9152/36450]\tLoss: 498.1906\n",
      "Training Epoch: 16 [9216/36450]\tLoss: 492.2330\n",
      "Training Epoch: 16 [9280/36450]\tLoss: 494.0539\n",
      "Training Epoch: 16 [9344/36450]\tLoss: 524.3710\n",
      "Training Epoch: 16 [9408/36450]\tLoss: 491.9174\n",
      "Training Epoch: 16 [9472/36450]\tLoss: 527.1892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [9536/36450]\tLoss: 513.9661\n",
      "Training Epoch: 16 [9600/36450]\tLoss: 552.6147\n",
      "Training Epoch: 16 [9664/36450]\tLoss: 521.9664\n",
      "Training Epoch: 16 [9728/36450]\tLoss: 532.5501\n",
      "Training Epoch: 16 [9792/36450]\tLoss: 483.6076\n",
      "Training Epoch: 16 [9856/36450]\tLoss: 492.7347\n",
      "Training Epoch: 16 [9920/36450]\tLoss: 529.4357\n",
      "Training Epoch: 16 [9984/36450]\tLoss: 510.9875\n",
      "Training Epoch: 16 [10048/36450]\tLoss: 492.7884\n",
      "Training Epoch: 16 [10112/36450]\tLoss: 478.2901\n",
      "Training Epoch: 16 [10176/36450]\tLoss: 495.1507\n",
      "Training Epoch: 16 [10240/36450]\tLoss: 514.5543\n",
      "Training Epoch: 16 [10304/36450]\tLoss: 510.2526\n",
      "Training Epoch: 16 [10368/36450]\tLoss: 490.5571\n",
      "Training Epoch: 16 [10432/36450]\tLoss: 488.3369\n",
      "Training Epoch: 16 [10496/36450]\tLoss: 492.7313\n",
      "Training Epoch: 16 [10560/36450]\tLoss: 479.1734\n",
      "Training Epoch: 16 [10624/36450]\tLoss: 495.6673\n",
      "Training Epoch: 16 [10688/36450]\tLoss: 515.0628\n",
      "Training Epoch: 16 [10752/36450]\tLoss: 492.7178\n",
      "Training Epoch: 16 [10816/36450]\tLoss: 507.0909\n",
      "Training Epoch: 16 [10880/36450]\tLoss: 498.7681\n",
      "Training Epoch: 16 [10944/36450]\tLoss: 491.4005\n",
      "Training Epoch: 16 [11008/36450]\tLoss: 494.7597\n",
      "Training Epoch: 16 [11072/36450]\tLoss: 507.4880\n",
      "Training Epoch: 16 [11136/36450]\tLoss: 479.2799\n",
      "Training Epoch: 16 [11200/36450]\tLoss: 486.9796\n",
      "Training Epoch: 16 [11264/36450]\tLoss: 498.8804\n",
      "Training Epoch: 16 [11328/36450]\tLoss: 487.5763\n",
      "Training Epoch: 16 [11392/36450]\tLoss: 492.7556\n",
      "Training Epoch: 16 [11456/36450]\tLoss: 483.5089\n",
      "Training Epoch: 16 [11520/36450]\tLoss: 501.8937\n",
      "Training Epoch: 16 [11584/36450]\tLoss: 471.5157\n",
      "Training Epoch: 16 [11648/36450]\tLoss: 497.5817\n",
      "Training Epoch: 16 [11712/36450]\tLoss: 484.4065\n",
      "Training Epoch: 16 [11776/36450]\tLoss: 466.4676\n",
      "Training Epoch: 16 [11840/36450]\tLoss: 485.5241\n",
      "Training Epoch: 16 [11904/36450]\tLoss: 468.4453\n",
      "Training Epoch: 16 [11968/36450]\tLoss: 508.0229\n",
      "Training Epoch: 16 [12032/36450]\tLoss: 502.7024\n",
      "Training Epoch: 16 [12096/36450]\tLoss: 516.9097\n",
      "Training Epoch: 16 [12160/36450]\tLoss: 495.0569\n",
      "Training Epoch: 16 [12224/36450]\tLoss: 533.3094\n",
      "Training Epoch: 16 [12288/36450]\tLoss: 497.5094\n",
      "Training Epoch: 16 [12352/36450]\tLoss: 496.7075\n",
      "Training Epoch: 16 [12416/36450]\tLoss: 501.6529\n",
      "Training Epoch: 16 [12480/36450]\tLoss: 473.3448\n",
      "Training Epoch: 16 [12544/36450]\tLoss: 499.6467\n",
      "Training Epoch: 16 [12608/36450]\tLoss: 506.0276\n",
      "Training Epoch: 16 [12672/36450]\tLoss: 463.2959\n",
      "Training Epoch: 16 [12736/36450]\tLoss: 512.2704\n",
      "Training Epoch: 16 [12800/36450]\tLoss: 497.0718\n",
      "Training Epoch: 16 [12864/36450]\tLoss: 491.0002\n",
      "Training Epoch: 16 [12928/36450]\tLoss: 528.0434\n",
      "Training Epoch: 16 [12992/36450]\tLoss: 495.1292\n",
      "Training Epoch: 16 [13056/36450]\tLoss: 509.9662\n",
      "Training Epoch: 16 [13120/36450]\tLoss: 502.9179\n",
      "Training Epoch: 16 [13184/36450]\tLoss: 502.9321\n",
      "Training Epoch: 16 [13248/36450]\tLoss: 494.3422\n",
      "Training Epoch: 16 [13312/36450]\tLoss: 506.0976\n",
      "Training Epoch: 16 [13376/36450]\tLoss: 488.8522\n",
      "Training Epoch: 16 [13440/36450]\tLoss: 482.9873\n",
      "Training Epoch: 16 [13504/36450]\tLoss: 533.9735\n",
      "Training Epoch: 16 [13568/36450]\tLoss: 498.9418\n",
      "Training Epoch: 16 [13632/36450]\tLoss: 501.3875\n",
      "Training Epoch: 16 [13696/36450]\tLoss: 479.8577\n",
      "Training Epoch: 16 [13760/36450]\tLoss: 510.3344\n",
      "Training Epoch: 16 [13824/36450]\tLoss: 495.6217\n",
      "Training Epoch: 16 [13888/36450]\tLoss: 511.7599\n",
      "Training Epoch: 16 [13952/36450]\tLoss: 512.0425\n",
      "Training Epoch: 16 [14016/36450]\tLoss: 530.6653\n",
      "Training Epoch: 16 [14080/36450]\tLoss: 479.6775\n",
      "Training Epoch: 16 [14144/36450]\tLoss: 516.6797\n",
      "Training Epoch: 16 [14208/36450]\tLoss: 494.1674\n",
      "Training Epoch: 16 [14272/36450]\tLoss: 486.0458\n",
      "Training Epoch: 16 [14336/36450]\tLoss: 535.4682\n",
      "Training Epoch: 16 [14400/36450]\tLoss: 507.1027\n",
      "Training Epoch: 16 [14464/36450]\tLoss: 475.8095\n",
      "Training Epoch: 16 [14528/36450]\tLoss: 518.8735\n",
      "Training Epoch: 16 [14592/36450]\tLoss: 519.0714\n",
      "Training Epoch: 16 [14656/36450]\tLoss: 487.0546\n",
      "Training Epoch: 16 [14720/36450]\tLoss: 497.6212\n",
      "Training Epoch: 16 [14784/36450]\tLoss: 467.9399\n",
      "Training Epoch: 16 [14848/36450]\tLoss: 503.8622\n",
      "Training Epoch: 16 [14912/36450]\tLoss: 473.4998\n",
      "Training Epoch: 16 [14976/36450]\tLoss: 503.1790\n",
      "Training Epoch: 16 [15040/36450]\tLoss: 523.0700\n",
      "Training Epoch: 16 [15104/36450]\tLoss: 511.0967\n",
      "Training Epoch: 16 [15168/36450]\tLoss: 483.5968\n",
      "Training Epoch: 16 [15232/36450]\tLoss: 509.0959\n",
      "Training Epoch: 16 [15296/36450]\tLoss: 507.0504\n",
      "Training Epoch: 16 [15360/36450]\tLoss: 503.9684\n",
      "Training Epoch: 16 [15424/36450]\tLoss: 525.2249\n",
      "Training Epoch: 16 [15488/36450]\tLoss: 511.3224\n",
      "Training Epoch: 16 [15552/36450]\tLoss: 501.3226\n",
      "Training Epoch: 16 [15616/36450]\tLoss: 506.6901\n",
      "Training Epoch: 16 [15680/36450]\tLoss: 509.8007\n",
      "Training Epoch: 16 [15744/36450]\tLoss: 483.8464\n",
      "Training Epoch: 16 [15808/36450]\tLoss: 463.2621\n",
      "Training Epoch: 16 [15872/36450]\tLoss: 491.6177\n",
      "Training Epoch: 16 [15936/36450]\tLoss: 464.8219\n",
      "Training Epoch: 16 [16000/36450]\tLoss: 487.4177\n",
      "Training Epoch: 16 [16064/36450]\tLoss: 489.1718\n",
      "Training Epoch: 16 [16128/36450]\tLoss: 510.5990\n",
      "Training Epoch: 16 [16192/36450]\tLoss: 529.1163\n",
      "Training Epoch: 16 [16256/36450]\tLoss: 483.4886\n",
      "Training Epoch: 16 [16320/36450]\tLoss: 528.8924\n",
      "Training Epoch: 16 [16384/36450]\tLoss: 509.5514\n",
      "Training Epoch: 16 [16448/36450]\tLoss: 500.8716\n",
      "Training Epoch: 16 [16512/36450]\tLoss: 499.0694\n",
      "Training Epoch: 16 [16576/36450]\tLoss: 515.8785\n",
      "Training Epoch: 16 [16640/36450]\tLoss: 500.7479\n",
      "Training Epoch: 16 [16704/36450]\tLoss: 498.5042\n",
      "Training Epoch: 16 [16768/36450]\tLoss: 499.1658\n",
      "Training Epoch: 16 [16832/36450]\tLoss: 496.5518\n",
      "Training Epoch: 16 [16896/36450]\tLoss: 479.8112\n",
      "Training Epoch: 16 [16960/36450]\tLoss: 469.2782\n",
      "Training Epoch: 16 [17024/36450]\tLoss: 477.9326\n",
      "Training Epoch: 16 [17088/36450]\tLoss: 524.9628\n",
      "Training Epoch: 16 [17152/36450]\tLoss: 472.8518\n",
      "Training Epoch: 16 [17216/36450]\tLoss: 522.2593\n",
      "Training Epoch: 16 [17280/36450]\tLoss: 476.4693\n",
      "Training Epoch: 16 [17344/36450]\tLoss: 497.7247\n",
      "Training Epoch: 16 [17408/36450]\tLoss: 531.5743\n",
      "Training Epoch: 16 [17472/36450]\tLoss: 528.3270\n",
      "Training Epoch: 16 [17536/36450]\tLoss: 480.1317\n",
      "Training Epoch: 16 [17600/36450]\tLoss: 514.6799\n",
      "Training Epoch: 16 [17664/36450]\tLoss: 495.7882\n",
      "Training Epoch: 16 [17728/36450]\tLoss: 490.0688\n",
      "Training Epoch: 16 [17792/36450]\tLoss: 486.8349\n",
      "Training Epoch: 16 [17856/36450]\tLoss: 446.2932\n",
      "Training Epoch: 16 [17920/36450]\tLoss: 474.8098\n",
      "Training Epoch: 16 [17984/36450]\tLoss: 465.0108\n",
      "Training Epoch: 16 [18048/36450]\tLoss: 480.2990\n",
      "Training Epoch: 16 [18112/36450]\tLoss: 485.9529\n",
      "Training Epoch: 16 [18176/36450]\tLoss: 487.4419\n",
      "Training Epoch: 16 [18240/36450]\tLoss: 514.8972\n",
      "Training Epoch: 16 [18304/36450]\tLoss: 495.8688\n",
      "Training Epoch: 16 [18368/36450]\tLoss: 477.9820\n",
      "Training Epoch: 16 [18432/36450]\tLoss: 477.1429\n",
      "Training Epoch: 16 [18496/36450]\tLoss: 503.3651\n",
      "Training Epoch: 16 [18560/36450]\tLoss: 502.7373\n",
      "Training Epoch: 16 [18624/36450]\tLoss: 520.3393\n",
      "Training Epoch: 16 [18688/36450]\tLoss: 504.8285\n",
      "Training Epoch: 16 [18752/36450]\tLoss: 523.2932\n",
      "Training Epoch: 16 [18816/36450]\tLoss: 471.8376\n",
      "Training Epoch: 16 [18880/36450]\tLoss: 490.5784\n",
      "Training Epoch: 16 [18944/36450]\tLoss: 524.7826\n",
      "Training Epoch: 16 [19008/36450]\tLoss: 505.3497\n",
      "Training Epoch: 16 [19072/36450]\tLoss: 501.4660\n",
      "Training Epoch: 16 [19136/36450]\tLoss: 498.3338\n",
      "Training Epoch: 16 [19200/36450]\tLoss: 483.1066\n",
      "Training Epoch: 16 [19264/36450]\tLoss: 542.2549\n",
      "Training Epoch: 16 [19328/36450]\tLoss: 482.3478\n",
      "Training Epoch: 16 [19392/36450]\tLoss: 492.6458\n",
      "Training Epoch: 16 [19456/36450]\tLoss: 481.1651\n",
      "Training Epoch: 16 [19520/36450]\tLoss: 491.5016\n",
      "Training Epoch: 16 [19584/36450]\tLoss: 510.4482\n",
      "Training Epoch: 16 [19648/36450]\tLoss: 508.6569\n",
      "Training Epoch: 16 [19712/36450]\tLoss: 505.6006\n",
      "Training Epoch: 16 [19776/36450]\tLoss: 499.8851\n",
      "Training Epoch: 16 [19840/36450]\tLoss: 481.0490\n",
      "Training Epoch: 16 [19904/36450]\tLoss: 527.6068\n",
      "Training Epoch: 16 [19968/36450]\tLoss: 525.3887\n",
      "Training Epoch: 16 [20032/36450]\tLoss: 517.8388\n",
      "Training Epoch: 16 [20096/36450]\tLoss: 512.0199\n",
      "Training Epoch: 16 [20160/36450]\tLoss: 495.1970\n",
      "Training Epoch: 16 [20224/36450]\tLoss: 510.3151\n",
      "Training Epoch: 16 [20288/36450]\tLoss: 545.8500\n",
      "Training Epoch: 16 [20352/36450]\tLoss: 517.7448\n",
      "Training Epoch: 16 [20416/36450]\tLoss: 488.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [20480/36450]\tLoss: 483.0511\n",
      "Training Epoch: 16 [20544/36450]\tLoss: 502.9034\n",
      "Training Epoch: 16 [20608/36450]\tLoss: 497.5450\n",
      "Training Epoch: 16 [20672/36450]\tLoss: 534.8273\n",
      "Training Epoch: 16 [20736/36450]\tLoss: 490.5766\n",
      "Training Epoch: 16 [20800/36450]\tLoss: 494.1280\n",
      "Training Epoch: 16 [20864/36450]\tLoss: 505.8683\n",
      "Training Epoch: 16 [20928/36450]\tLoss: 511.3885\n",
      "Training Epoch: 16 [20992/36450]\tLoss: 507.6790\n",
      "Training Epoch: 16 [21056/36450]\tLoss: 518.7308\n",
      "Training Epoch: 16 [21120/36450]\tLoss: 469.9262\n",
      "Training Epoch: 16 [21184/36450]\tLoss: 509.9810\n",
      "Training Epoch: 16 [21248/36450]\tLoss: 502.7814\n",
      "Training Epoch: 16 [21312/36450]\tLoss: 509.0979\n",
      "Training Epoch: 16 [21376/36450]\tLoss: 491.1281\n",
      "Training Epoch: 16 [21440/36450]\tLoss: 503.9761\n",
      "Training Epoch: 16 [21504/36450]\tLoss: 515.9163\n",
      "Training Epoch: 16 [21568/36450]\tLoss: 489.8199\n",
      "Training Epoch: 16 [21632/36450]\tLoss: 511.2748\n",
      "Training Epoch: 16 [21696/36450]\tLoss: 485.8897\n",
      "Training Epoch: 16 [21760/36450]\tLoss: 501.7483\n",
      "Training Epoch: 16 [21824/36450]\tLoss: 518.8633\n",
      "Training Epoch: 16 [21888/36450]\tLoss: 508.4738\n",
      "Training Epoch: 16 [21952/36450]\tLoss: 472.7788\n",
      "Training Epoch: 16 [22016/36450]\tLoss: 489.2545\n",
      "Training Epoch: 16 [22080/36450]\tLoss: 511.0770\n",
      "Training Epoch: 16 [22144/36450]\tLoss: 488.3267\n",
      "Training Epoch: 16 [22208/36450]\tLoss: 508.3934\n",
      "Training Epoch: 16 [22272/36450]\tLoss: 483.8428\n",
      "Training Epoch: 16 [22336/36450]\tLoss: 496.3259\n",
      "Training Epoch: 16 [22400/36450]\tLoss: 489.7714\n",
      "Training Epoch: 16 [22464/36450]\tLoss: 465.9161\n",
      "Training Epoch: 16 [22528/36450]\tLoss: 486.2302\n",
      "Training Epoch: 16 [22592/36450]\tLoss: 498.7790\n",
      "Training Epoch: 16 [22656/36450]\tLoss: 489.8294\n",
      "Training Epoch: 16 [22720/36450]\tLoss: 496.6470\n",
      "Training Epoch: 16 [22784/36450]\tLoss: 498.4572\n",
      "Training Epoch: 16 [22848/36450]\tLoss: 479.5604\n",
      "Training Epoch: 16 [22912/36450]\tLoss: 498.4247\n",
      "Training Epoch: 16 [22976/36450]\tLoss: 499.2411\n",
      "Training Epoch: 16 [23040/36450]\tLoss: 488.2191\n",
      "Training Epoch: 16 [23104/36450]\tLoss: 512.7068\n",
      "Training Epoch: 16 [23168/36450]\tLoss: 506.1658\n",
      "Training Epoch: 16 [23232/36450]\tLoss: 506.4430\n",
      "Training Epoch: 16 [23296/36450]\tLoss: 507.7793\n",
      "Training Epoch: 16 [23360/36450]\tLoss: 517.5493\n",
      "Training Epoch: 16 [23424/36450]\tLoss: 490.3627\n",
      "Training Epoch: 16 [23488/36450]\tLoss: 516.9247\n",
      "Training Epoch: 16 [23552/36450]\tLoss: 502.2695\n",
      "Training Epoch: 16 [23616/36450]\tLoss: 504.2488\n",
      "Training Epoch: 16 [23680/36450]\tLoss: 469.8387\n",
      "Training Epoch: 16 [23744/36450]\tLoss: 503.8266\n",
      "Training Epoch: 16 [23808/36450]\tLoss: 463.3084\n",
      "Training Epoch: 16 [23872/36450]\tLoss: 491.5259\n",
      "Training Epoch: 16 [23936/36450]\tLoss: 534.3723\n",
      "Training Epoch: 16 [24000/36450]\tLoss: 470.0776\n",
      "Training Epoch: 16 [24064/36450]\tLoss: 509.8725\n",
      "Training Epoch: 16 [24128/36450]\tLoss: 507.0905\n",
      "Training Epoch: 16 [24192/36450]\tLoss: 502.5211\n",
      "Training Epoch: 16 [24256/36450]\tLoss: 508.4525\n",
      "Training Epoch: 16 [24320/36450]\tLoss: 511.3571\n",
      "Training Epoch: 16 [24384/36450]\tLoss: 479.2597\n",
      "Training Epoch: 16 [24448/36450]\tLoss: 495.1021\n",
      "Training Epoch: 16 [24512/36450]\tLoss: 475.5815\n",
      "Training Epoch: 16 [24576/36450]\tLoss: 498.1127\n",
      "Training Epoch: 16 [24640/36450]\tLoss: 513.2386\n",
      "Training Epoch: 16 [24704/36450]\tLoss: 497.5844\n",
      "Training Epoch: 16 [24768/36450]\tLoss: 480.5584\n",
      "Training Epoch: 16 [24832/36450]\tLoss: 484.6826\n",
      "Training Epoch: 16 [24896/36450]\tLoss: 490.0867\n",
      "Training Epoch: 16 [24960/36450]\tLoss: 501.9737\n",
      "Training Epoch: 16 [25024/36450]\tLoss: 465.9612\n",
      "Training Epoch: 16 [25088/36450]\tLoss: 492.5588\n",
      "Training Epoch: 16 [25152/36450]\tLoss: 496.8396\n",
      "Training Epoch: 16 [25216/36450]\tLoss: 503.0733\n",
      "Training Epoch: 16 [25280/36450]\tLoss: 512.3450\n",
      "Training Epoch: 16 [25344/36450]\tLoss: 471.3463\n",
      "Training Epoch: 16 [25408/36450]\tLoss: 512.7264\n",
      "Training Epoch: 16 [25472/36450]\tLoss: 473.2059\n",
      "Training Epoch: 16 [25536/36450]\tLoss: 477.3892\n",
      "Training Epoch: 16 [25600/36450]\tLoss: 498.7230\n",
      "Training Epoch: 16 [25664/36450]\tLoss: 489.5737\n",
      "Training Epoch: 16 [25728/36450]\tLoss: 461.1862\n",
      "Training Epoch: 16 [25792/36450]\tLoss: 479.0752\n",
      "Training Epoch: 16 [25856/36450]\tLoss: 501.2105\n",
      "Training Epoch: 16 [25920/36450]\tLoss: 481.3892\n",
      "Training Epoch: 16 [25984/36450]\tLoss: 508.9706\n",
      "Training Epoch: 16 [26048/36450]\tLoss: 515.3722\n",
      "Training Epoch: 16 [26112/36450]\tLoss: 516.2178\n",
      "Training Epoch: 16 [26176/36450]\tLoss: 478.7817\n",
      "Training Epoch: 16 [26240/36450]\tLoss: 518.9710\n",
      "Training Epoch: 16 [26304/36450]\tLoss: 479.6870\n",
      "Training Epoch: 16 [26368/36450]\tLoss: 486.5087\n",
      "Training Epoch: 16 [26432/36450]\tLoss: 467.0565\n",
      "Training Epoch: 16 [26496/36450]\tLoss: 500.7289\n",
      "Training Epoch: 16 [26560/36450]\tLoss: 501.9334\n",
      "Training Epoch: 16 [26624/36450]\tLoss: 482.4740\n",
      "Training Epoch: 16 [26688/36450]\tLoss: 481.2127\n",
      "Training Epoch: 16 [26752/36450]\tLoss: 495.4338\n",
      "Training Epoch: 16 [26816/36450]\tLoss: 493.4331\n",
      "Training Epoch: 16 [26880/36450]\tLoss: 492.5711\n",
      "Training Epoch: 16 [26944/36450]\tLoss: 504.2874\n",
      "Training Epoch: 16 [27008/36450]\tLoss: 513.4453\n",
      "Training Epoch: 16 [27072/36450]\tLoss: 518.3829\n",
      "Training Epoch: 16 [27136/36450]\tLoss: 523.0443\n",
      "Training Epoch: 16 [27200/36450]\tLoss: 518.8284\n",
      "Training Epoch: 16 [27264/36450]\tLoss: 494.8043\n",
      "Training Epoch: 16 [27328/36450]\tLoss: 510.9409\n",
      "Training Epoch: 16 [27392/36450]\tLoss: 551.1243\n",
      "Training Epoch: 16 [27456/36450]\tLoss: 497.4812\n",
      "Training Epoch: 16 [27520/36450]\tLoss: 514.7468\n",
      "Training Epoch: 16 [27584/36450]\tLoss: 510.6134\n",
      "Training Epoch: 16 [27648/36450]\tLoss: 521.6177\n",
      "Training Epoch: 16 [27712/36450]\tLoss: 510.4908\n",
      "Training Epoch: 16 [27776/36450]\tLoss: 533.4203\n",
      "Training Epoch: 16 [27840/36450]\tLoss: 522.7507\n",
      "Training Epoch: 16 [27904/36450]\tLoss: 543.7908\n",
      "Training Epoch: 16 [27968/36450]\tLoss: 548.8089\n",
      "Training Epoch: 16 [28032/36450]\tLoss: 536.4889\n",
      "Training Epoch: 16 [28096/36450]\tLoss: 534.3647\n",
      "Training Epoch: 16 [28160/36450]\tLoss: 533.0505\n",
      "Training Epoch: 16 [28224/36450]\tLoss: 496.0974\n",
      "Training Epoch: 16 [28288/36450]\tLoss: 491.2547\n",
      "Training Epoch: 16 [28352/36450]\tLoss: 481.6480\n",
      "Training Epoch: 16 [28416/36450]\tLoss: 483.7644\n",
      "Training Epoch: 16 [28480/36450]\tLoss: 502.8142\n",
      "Training Epoch: 16 [28544/36450]\tLoss: 495.3144\n",
      "Training Epoch: 16 [28608/36450]\tLoss: 477.1918\n",
      "Training Epoch: 16 [28672/36450]\tLoss: 492.1896\n",
      "Training Epoch: 16 [28736/36450]\tLoss: 487.4715\n",
      "Training Epoch: 16 [28800/36450]\tLoss: 483.3151\n",
      "Training Epoch: 16 [28864/36450]\tLoss: 499.1974\n",
      "Training Epoch: 16 [28928/36450]\tLoss: 493.2405\n",
      "Training Epoch: 16 [28992/36450]\tLoss: 483.0210\n",
      "Training Epoch: 16 [29056/36450]\tLoss: 520.6198\n",
      "Training Epoch: 16 [29120/36450]\tLoss: 486.4331\n",
      "Training Epoch: 16 [29184/36450]\tLoss: 481.4384\n",
      "Training Epoch: 16 [29248/36450]\tLoss: 480.5507\n",
      "Training Epoch: 16 [29312/36450]\tLoss: 529.6808\n",
      "Training Epoch: 16 [29376/36450]\tLoss: 514.1979\n",
      "Training Epoch: 16 [29440/36450]\tLoss: 510.6125\n",
      "Training Epoch: 16 [29504/36450]\tLoss: 484.3712\n",
      "Training Epoch: 16 [29568/36450]\tLoss: 485.9925\n",
      "Training Epoch: 16 [29632/36450]\tLoss: 516.8874\n",
      "Training Epoch: 16 [29696/36450]\tLoss: 492.9020\n",
      "Training Epoch: 16 [29760/36450]\tLoss: 498.7586\n",
      "Training Epoch: 16 [29824/36450]\tLoss: 508.8536\n",
      "Training Epoch: 16 [29888/36450]\tLoss: 489.9178\n",
      "Training Epoch: 16 [29952/36450]\tLoss: 500.4873\n",
      "Training Epoch: 16 [30016/36450]\tLoss: 514.5535\n",
      "Training Epoch: 16 [30080/36450]\tLoss: 488.1839\n",
      "Training Epoch: 16 [30144/36450]\tLoss: 513.6987\n",
      "Training Epoch: 16 [30208/36450]\tLoss: 515.5732\n",
      "Training Epoch: 16 [30272/36450]\tLoss: 477.0226\n",
      "Training Epoch: 16 [30336/36450]\tLoss: 509.0436\n",
      "Training Epoch: 16 [30400/36450]\tLoss: 510.9685\n",
      "Training Epoch: 16 [30464/36450]\tLoss: 483.8027\n",
      "Training Epoch: 16 [30528/36450]\tLoss: 469.5822\n",
      "Training Epoch: 16 [30592/36450]\tLoss: 501.2992\n",
      "Training Epoch: 16 [30656/36450]\tLoss: 507.3769\n",
      "Training Epoch: 16 [30720/36450]\tLoss: 502.4124\n",
      "Training Epoch: 16 [30784/36450]\tLoss: 492.1096\n",
      "Training Epoch: 16 [30848/36450]\tLoss: 483.0194\n",
      "Training Epoch: 16 [30912/36450]\tLoss: 521.5542\n",
      "Training Epoch: 16 [30976/36450]\tLoss: 469.0843\n",
      "Training Epoch: 16 [31040/36450]\tLoss: 480.2161\n",
      "Training Epoch: 16 [31104/36450]\tLoss: 479.1732\n",
      "Training Epoch: 16 [31168/36450]\tLoss: 494.0907\n",
      "Training Epoch: 16 [31232/36450]\tLoss: 486.3963\n",
      "Training Epoch: 16 [31296/36450]\tLoss: 498.3009\n",
      "Training Epoch: 16 [31360/36450]\tLoss: 493.3847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [31424/36450]\tLoss: 495.2065\n",
      "Training Epoch: 16 [31488/36450]\tLoss: 514.0001\n",
      "Training Epoch: 16 [31552/36450]\tLoss: 470.8332\n",
      "Training Epoch: 16 [31616/36450]\tLoss: 495.6717\n",
      "Training Epoch: 16 [31680/36450]\tLoss: 476.7075\n",
      "Training Epoch: 16 [31744/36450]\tLoss: 508.4178\n",
      "Training Epoch: 16 [31808/36450]\tLoss: 475.4793\n",
      "Training Epoch: 16 [31872/36450]\tLoss: 495.0358\n",
      "Training Epoch: 16 [31936/36450]\tLoss: 504.3866\n",
      "Training Epoch: 16 [32000/36450]\tLoss: 462.7179\n",
      "Training Epoch: 16 [32064/36450]\tLoss: 491.7425\n",
      "Training Epoch: 16 [32128/36450]\tLoss: 486.0155\n",
      "Training Epoch: 16 [32192/36450]\tLoss: 472.1614\n",
      "Training Epoch: 16 [32256/36450]\tLoss: 503.1778\n",
      "Training Epoch: 16 [32320/36450]\tLoss: 487.0705\n",
      "Training Epoch: 16 [32384/36450]\tLoss: 494.5071\n",
      "Training Epoch: 16 [32448/36450]\tLoss: 524.2311\n",
      "Training Epoch: 16 [32512/36450]\tLoss: 493.8756\n",
      "Training Epoch: 16 [32576/36450]\tLoss: 533.8177\n",
      "Training Epoch: 16 [32640/36450]\tLoss: 464.8194\n",
      "Training Epoch: 16 [32704/36450]\tLoss: 524.4243\n",
      "Training Epoch: 16 [32768/36450]\tLoss: 493.6339\n",
      "Training Epoch: 16 [32832/36450]\tLoss: 489.3986\n",
      "Training Epoch: 16 [32896/36450]\tLoss: 471.0253\n",
      "Training Epoch: 16 [32960/36450]\tLoss: 473.3718\n",
      "Training Epoch: 16 [33024/36450]\tLoss: 508.5305\n",
      "Training Epoch: 16 [33088/36450]\tLoss: 491.1212\n",
      "Training Epoch: 16 [33152/36450]\tLoss: 489.1341\n",
      "Training Epoch: 16 [33216/36450]\tLoss: 499.6104\n",
      "Training Epoch: 16 [33280/36450]\tLoss: 497.1406\n",
      "Training Epoch: 16 [33344/36450]\tLoss: 476.0355\n",
      "Training Epoch: 16 [33408/36450]\tLoss: 491.1811\n",
      "Training Epoch: 16 [33472/36450]\tLoss: 460.1573\n",
      "Training Epoch: 16 [33536/36450]\tLoss: 503.4500\n",
      "Training Epoch: 16 [33600/36450]\tLoss: 511.2660\n",
      "Training Epoch: 16 [33664/36450]\tLoss: 519.7120\n",
      "Training Epoch: 16 [33728/36450]\tLoss: 511.8491\n",
      "Training Epoch: 16 [33792/36450]\tLoss: 478.9787\n",
      "Training Epoch: 16 [33856/36450]\tLoss: 498.3060\n",
      "Training Epoch: 16 [33920/36450]\tLoss: 470.5795\n",
      "Training Epoch: 16 [33984/36450]\tLoss: 487.3619\n",
      "Training Epoch: 16 [34048/36450]\tLoss: 491.7706\n",
      "Training Epoch: 16 [34112/36450]\tLoss: 474.8897\n",
      "Training Epoch: 16 [34176/36450]\tLoss: 472.1182\n",
      "Training Epoch: 16 [34240/36450]\tLoss: 469.2339\n",
      "Training Epoch: 16 [34304/36450]\tLoss: 501.7053\n",
      "Training Epoch: 16 [34368/36450]\tLoss: 522.7737\n",
      "Training Epoch: 16 [34432/36450]\tLoss: 499.7350\n",
      "Training Epoch: 16 [34496/36450]\tLoss: 517.2048\n",
      "Training Epoch: 16 [34560/36450]\tLoss: 506.6054\n",
      "Training Epoch: 16 [34624/36450]\tLoss: 492.5305\n",
      "Training Epoch: 16 [34688/36450]\tLoss: 500.5483\n",
      "Training Epoch: 16 [34752/36450]\tLoss: 514.0510\n",
      "Training Epoch: 16 [34816/36450]\tLoss: 499.2272\n",
      "Training Epoch: 16 [34880/36450]\tLoss: 460.8607\n",
      "Training Epoch: 16 [34944/36450]\tLoss: 490.1484\n",
      "Training Epoch: 16 [35008/36450]\tLoss: 519.1517\n",
      "Training Epoch: 16 [35072/36450]\tLoss: 542.9732\n",
      "Training Epoch: 16 [35136/36450]\tLoss: 502.6800\n",
      "Training Epoch: 16 [35200/36450]\tLoss: 478.5364\n",
      "Training Epoch: 16 [35264/36450]\tLoss: 487.0985\n",
      "Training Epoch: 16 [35328/36450]\tLoss: 515.4121\n",
      "Training Epoch: 16 [35392/36450]\tLoss: 548.6730\n",
      "Training Epoch: 16 [35456/36450]\tLoss: 484.8208\n",
      "Training Epoch: 16 [35520/36450]\tLoss: 509.5027\n",
      "Training Epoch: 16 [35584/36450]\tLoss: 484.3971\n",
      "Training Epoch: 16 [35648/36450]\tLoss: 508.7234\n",
      "Training Epoch: 16 [35712/36450]\tLoss: 496.1657\n",
      "Training Epoch: 16 [35776/36450]\tLoss: 497.5740\n",
      "Training Epoch: 16 [35840/36450]\tLoss: 466.5831\n",
      "Training Epoch: 16 [35904/36450]\tLoss: 494.2462\n",
      "Training Epoch: 16 [35968/36450]\tLoss: 464.1811\n",
      "Training Epoch: 16 [36032/36450]\tLoss: 507.2937\n",
      "Training Epoch: 16 [36096/36450]\tLoss: 467.9669\n",
      "Training Epoch: 16 [36160/36450]\tLoss: 460.7686\n",
      "Training Epoch: 16 [36224/36450]\tLoss: 497.2393\n",
      "Training Epoch: 16 [36288/36450]\tLoss: 481.3738\n",
      "Training Epoch: 16 [36352/36450]\tLoss: 523.3201\n",
      "Training Epoch: 16 [36416/36450]\tLoss: 489.8665\n",
      "Training Epoch: 16 [36450/36450]\tLoss: 524.3081\n",
      "Training Epoch: 16 [4050/4050]\tLoss: 244.9058\n",
      "Training Epoch: 17 [64/36450]\tLoss: 500.5103\n",
      "Training Epoch: 17 [128/36450]\tLoss: 488.7597\n",
      "Training Epoch: 17 [192/36450]\tLoss: 507.8988\n",
      "Training Epoch: 17 [256/36450]\tLoss: 461.8994\n",
      "Training Epoch: 17 [320/36450]\tLoss: 510.7468\n",
      "Training Epoch: 17 [384/36450]\tLoss: 461.9381\n",
      "Training Epoch: 17 [448/36450]\tLoss: 448.3224\n",
      "Training Epoch: 17 [512/36450]\tLoss: 491.2494\n",
      "Training Epoch: 17 [576/36450]\tLoss: 518.9726\n",
      "Training Epoch: 17 [640/36450]\tLoss: 474.9355\n",
      "Training Epoch: 17 [704/36450]\tLoss: 496.6483\n",
      "Training Epoch: 17 [768/36450]\tLoss: 479.3860\n",
      "Training Epoch: 17 [832/36450]\tLoss: 457.9845\n",
      "Training Epoch: 17 [896/36450]\tLoss: 515.7140\n",
      "Training Epoch: 17 [960/36450]\tLoss: 491.6272\n",
      "Training Epoch: 17 [1024/36450]\tLoss: 484.8010\n",
      "Training Epoch: 17 [1088/36450]\tLoss: 474.5768\n",
      "Training Epoch: 17 [1152/36450]\tLoss: 512.6472\n",
      "Training Epoch: 17 [1216/36450]\tLoss: 488.8720\n",
      "Training Epoch: 17 [1280/36450]\tLoss: 494.9776\n",
      "Training Epoch: 17 [1344/36450]\tLoss: 490.2139\n",
      "Training Epoch: 17 [1408/36450]\tLoss: 521.9832\n",
      "Training Epoch: 17 [1472/36450]\tLoss: 486.6401\n",
      "Training Epoch: 17 [1536/36450]\tLoss: 510.8536\n",
      "Training Epoch: 17 [1600/36450]\tLoss: 505.2469\n",
      "Training Epoch: 17 [1664/36450]\tLoss: 490.7634\n",
      "Training Epoch: 17 [1728/36450]\tLoss: 514.8605\n",
      "Training Epoch: 17 [1792/36450]\tLoss: 543.6392\n",
      "Training Epoch: 17 [1856/36450]\tLoss: 557.9705\n",
      "Training Epoch: 17 [1920/36450]\tLoss: 539.3676\n",
      "Training Epoch: 17 [1984/36450]\tLoss: 532.4239\n",
      "Training Epoch: 17 [2048/36450]\tLoss: 538.6828\n",
      "Training Epoch: 17 [2112/36450]\tLoss: 535.5905\n",
      "Training Epoch: 17 [2176/36450]\tLoss: 495.9325\n",
      "Training Epoch: 17 [2240/36450]\tLoss: 492.0054\n",
      "Training Epoch: 17 [2304/36450]\tLoss: 508.3191\n",
      "Training Epoch: 17 [2368/36450]\tLoss: 487.3447\n",
      "Training Epoch: 17 [2432/36450]\tLoss: 494.9362\n",
      "Training Epoch: 17 [2496/36450]\tLoss: 525.8923\n",
      "Training Epoch: 17 [2560/36450]\tLoss: 536.8495\n",
      "Training Epoch: 17 [2624/36450]\tLoss: 533.7570\n",
      "Training Epoch: 17 [2688/36450]\tLoss: 516.2654\n",
      "Training Epoch: 17 [2752/36450]\tLoss: 488.6191\n",
      "Training Epoch: 17 [2816/36450]\tLoss: 481.0625\n",
      "Training Epoch: 17 [2880/36450]\tLoss: 468.5092\n",
      "Training Epoch: 17 [2944/36450]\tLoss: 493.6482\n",
      "Training Epoch: 17 [3008/36450]\tLoss: 505.5498\n",
      "Training Epoch: 17 [3072/36450]\tLoss: 468.9111\n",
      "Training Epoch: 17 [3136/36450]\tLoss: 485.9060\n",
      "Training Epoch: 17 [3200/36450]\tLoss: 491.9467\n",
      "Training Epoch: 17 [3264/36450]\tLoss: 484.7713\n",
      "Training Epoch: 17 [3328/36450]\tLoss: 509.5809\n",
      "Training Epoch: 17 [3392/36450]\tLoss: 497.3338\n",
      "Training Epoch: 17 [3456/36450]\tLoss: 484.1253\n",
      "Training Epoch: 17 [3520/36450]\tLoss: 482.9444\n",
      "Training Epoch: 17 [3584/36450]\tLoss: 494.9926\n",
      "Training Epoch: 17 [3648/36450]\tLoss: 499.5641\n",
      "Training Epoch: 17 [3712/36450]\tLoss: 482.1682\n",
      "Training Epoch: 17 [3776/36450]\tLoss: 521.7713\n",
      "Training Epoch: 17 [3840/36450]\tLoss: 490.0341\n",
      "Training Epoch: 17 [3904/36450]\tLoss: 489.1256\n",
      "Training Epoch: 17 [3968/36450]\tLoss: 488.1075\n",
      "Training Epoch: 17 [4032/36450]\tLoss: 462.7117\n",
      "Training Epoch: 17 [4096/36450]\tLoss: 507.4506\n",
      "Training Epoch: 17 [4160/36450]\tLoss: 503.4319\n",
      "Training Epoch: 17 [4224/36450]\tLoss: 459.3541\n",
      "Training Epoch: 17 [4288/36450]\tLoss: 488.9035\n",
      "Training Epoch: 17 [4352/36450]\tLoss: 474.4810\n",
      "Training Epoch: 17 [4416/36450]\tLoss: 501.1037\n",
      "Training Epoch: 17 [4480/36450]\tLoss: 490.2668\n",
      "Training Epoch: 17 [4544/36450]\tLoss: 496.8133\n",
      "Training Epoch: 17 [4608/36450]\tLoss: 490.4668\n",
      "Training Epoch: 17 [4672/36450]\tLoss: 479.1230\n",
      "Training Epoch: 17 [4736/36450]\tLoss: 481.9313\n",
      "Training Epoch: 17 [4800/36450]\tLoss: 474.2002\n",
      "Training Epoch: 17 [4864/36450]\tLoss: 475.0556\n",
      "Training Epoch: 17 [4928/36450]\tLoss: 517.7515\n",
      "Training Epoch: 17 [4992/36450]\tLoss: 478.0233\n",
      "Training Epoch: 17 [5056/36450]\tLoss: 498.3106\n",
      "Training Epoch: 17 [5120/36450]\tLoss: 463.4203\n",
      "Training Epoch: 17 [5184/36450]\tLoss: 488.3990\n",
      "Training Epoch: 17 [5248/36450]\tLoss: 531.2050\n",
      "Training Epoch: 17 [5312/36450]\tLoss: 501.0988\n",
      "Training Epoch: 17 [5376/36450]\tLoss: 495.9308\n",
      "Training Epoch: 17 [5440/36450]\tLoss: 518.8205\n",
      "Training Epoch: 17 [5504/36450]\tLoss: 509.1853\n",
      "Training Epoch: 17 [5568/36450]\tLoss: 512.0730\n",
      "Training Epoch: 17 [5632/36450]\tLoss: 469.4273\n",
      "Training Epoch: 17 [5696/36450]\tLoss: 499.8278\n",
      "Training Epoch: 17 [5760/36450]\tLoss: 499.9359\n",
      "Training Epoch: 17 [5824/36450]\tLoss: 511.1220\n",
      "Training Epoch: 17 [5888/36450]\tLoss: 507.4683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [5952/36450]\tLoss: 533.9272\n",
      "Training Epoch: 17 [6016/36450]\tLoss: 459.5966\n",
      "Training Epoch: 17 [6080/36450]\tLoss: 487.3089\n",
      "Training Epoch: 17 [6144/36450]\tLoss: 487.0395\n",
      "Training Epoch: 17 [6208/36450]\tLoss: 502.9289\n",
      "Training Epoch: 17 [6272/36450]\tLoss: 499.3109\n",
      "Training Epoch: 17 [6336/36450]\tLoss: 490.9636\n",
      "Training Epoch: 17 [6400/36450]\tLoss: 496.1577\n",
      "Training Epoch: 17 [6464/36450]\tLoss: 469.7575\n",
      "Training Epoch: 17 [6528/36450]\tLoss: 477.2656\n",
      "Training Epoch: 17 [6592/36450]\tLoss: 510.9389\n",
      "Training Epoch: 17 [6656/36450]\tLoss: 480.3473\n",
      "Training Epoch: 17 [6720/36450]\tLoss: 464.5044\n",
      "Training Epoch: 17 [6784/36450]\tLoss: 492.9655\n",
      "Training Epoch: 17 [6848/36450]\tLoss: 491.6720\n",
      "Training Epoch: 17 [6912/36450]\tLoss: 496.5848\n",
      "Training Epoch: 17 [6976/36450]\tLoss: 509.0831\n",
      "Training Epoch: 17 [7040/36450]\tLoss: 477.6602\n",
      "Training Epoch: 17 [7104/36450]\tLoss: 492.1837\n",
      "Training Epoch: 17 [7168/36450]\tLoss: 472.3568\n",
      "Training Epoch: 17 [7232/36450]\tLoss: 456.9470\n",
      "Training Epoch: 17 [7296/36450]\tLoss: 468.7962\n",
      "Training Epoch: 17 [7360/36450]\tLoss: 512.0685\n",
      "Training Epoch: 17 [7424/36450]\tLoss: 478.1216\n",
      "Training Epoch: 17 [7488/36450]\tLoss: 466.9982\n",
      "Training Epoch: 17 [7552/36450]\tLoss: 489.7981\n",
      "Training Epoch: 17 [7616/36450]\tLoss: 509.2483\n",
      "Training Epoch: 17 [7680/36450]\tLoss: 483.5200\n",
      "Training Epoch: 17 [7744/36450]\tLoss: 527.7728\n",
      "Training Epoch: 17 [7808/36450]\tLoss: 528.3344\n",
      "Training Epoch: 17 [7872/36450]\tLoss: 492.2701\n",
      "Training Epoch: 17 [7936/36450]\tLoss: 523.5309\n",
      "Training Epoch: 17 [8000/36450]\tLoss: 515.6117\n",
      "Training Epoch: 17 [8064/36450]\tLoss: 510.9942\n",
      "Training Epoch: 17 [8128/36450]\tLoss: 504.1116\n",
      "Training Epoch: 17 [8192/36450]\tLoss: 521.6970\n",
      "Training Epoch: 17 [8256/36450]\tLoss: 498.1259\n",
      "Training Epoch: 17 [8320/36450]\tLoss: 499.9319\n",
      "Training Epoch: 17 [8384/36450]\tLoss: 478.4482\n",
      "Training Epoch: 17 [8448/36450]\tLoss: 476.6170\n",
      "Training Epoch: 17 [8512/36450]\tLoss: 508.2675\n",
      "Training Epoch: 17 [8576/36450]\tLoss: 483.3391\n",
      "Training Epoch: 17 [8640/36450]\tLoss: 497.2497\n",
      "Training Epoch: 17 [8704/36450]\tLoss: 513.8188\n",
      "Training Epoch: 17 [8768/36450]\tLoss: 488.1219\n",
      "Training Epoch: 17 [8832/36450]\tLoss: 522.3410\n",
      "Training Epoch: 17 [8896/36450]\tLoss: 495.5776\n",
      "Training Epoch: 17 [8960/36450]\tLoss: 474.8733\n",
      "Training Epoch: 17 [9024/36450]\tLoss: 505.1823\n",
      "Training Epoch: 17 [9088/36450]\tLoss: 499.9448\n",
      "Training Epoch: 17 [9152/36450]\tLoss: 472.0899\n",
      "Training Epoch: 17 [9216/36450]\tLoss: 474.3169\n",
      "Training Epoch: 17 [9280/36450]\tLoss: 497.4327\n",
      "Training Epoch: 17 [9344/36450]\tLoss: 497.6153\n",
      "Training Epoch: 17 [9408/36450]\tLoss: 493.6143\n",
      "Training Epoch: 17 [9472/36450]\tLoss: 471.5776\n",
      "Training Epoch: 17 [9536/36450]\tLoss: 483.3369\n",
      "Training Epoch: 17 [9600/36450]\tLoss: 486.3888\n",
      "Training Epoch: 17 [9664/36450]\tLoss: 498.5514\n",
      "Training Epoch: 17 [9728/36450]\tLoss: 495.4226\n",
      "Training Epoch: 17 [9792/36450]\tLoss: 509.4122\n",
      "Training Epoch: 17 [9856/36450]\tLoss: 499.0177\n",
      "Training Epoch: 17 [9920/36450]\tLoss: 496.4869\n",
      "Training Epoch: 17 [9984/36450]\tLoss: 513.3900\n",
      "Training Epoch: 17 [10048/36450]\tLoss: 484.8394\n",
      "Training Epoch: 17 [10112/36450]\tLoss: 485.4124\n",
      "Training Epoch: 17 [10176/36450]\tLoss: 487.4770\n",
      "Training Epoch: 17 [10240/36450]\tLoss: 489.3714\n",
      "Training Epoch: 17 [10304/36450]\tLoss: 480.1050\n",
      "Training Epoch: 17 [10368/36450]\tLoss: 494.0428\n",
      "Training Epoch: 17 [10432/36450]\tLoss: 500.6739\n",
      "Training Epoch: 17 [10496/36450]\tLoss: 495.2521\n",
      "Training Epoch: 17 [10560/36450]\tLoss: 491.9134\n",
      "Training Epoch: 17 [10624/36450]\tLoss: 500.6358\n",
      "Training Epoch: 17 [10688/36450]\tLoss: 496.5538\n",
      "Training Epoch: 17 [10752/36450]\tLoss: 486.9061\n",
      "Training Epoch: 17 [10816/36450]\tLoss: 490.2885\n",
      "Training Epoch: 17 [10880/36450]\tLoss: 498.3722\n",
      "Training Epoch: 17 [10944/36450]\tLoss: 470.9879\n",
      "Training Epoch: 17 [11008/36450]\tLoss: 539.6442\n",
      "Training Epoch: 17 [11072/36450]\tLoss: 494.2781\n",
      "Training Epoch: 17 [11136/36450]\tLoss: 482.5033\n",
      "Training Epoch: 17 [11200/36450]\tLoss: 468.0680\n",
      "Training Epoch: 17 [11264/36450]\tLoss: 519.8430\n",
      "Training Epoch: 17 [11328/36450]\tLoss: 486.5001\n",
      "Training Epoch: 17 [11392/36450]\tLoss: 488.0412\n",
      "Training Epoch: 17 [11456/36450]\tLoss: 495.0910\n",
      "Training Epoch: 17 [11520/36450]\tLoss: 468.0289\n",
      "Training Epoch: 17 [11584/36450]\tLoss: 477.2253\n",
      "Training Epoch: 17 [11648/36450]\tLoss: 471.7800\n",
      "Training Epoch: 17 [11712/36450]\tLoss: 476.6168\n",
      "Training Epoch: 17 [11776/36450]\tLoss: 510.8358\n",
      "Training Epoch: 17 [11840/36450]\tLoss: 509.9285\n",
      "Training Epoch: 17 [11904/36450]\tLoss: 492.1399\n",
      "Training Epoch: 17 [11968/36450]\tLoss: 508.4977\n",
      "Training Epoch: 17 [12032/36450]\tLoss: 494.1191\n",
      "Training Epoch: 17 [12096/36450]\tLoss: 484.1550\n",
      "Training Epoch: 17 [12160/36450]\tLoss: 495.6472\n",
      "Training Epoch: 17 [12224/36450]\tLoss: 510.1908\n",
      "Training Epoch: 17 [12288/36450]\tLoss: 501.8829\n",
      "Training Epoch: 17 [12352/36450]\tLoss: 498.3745\n",
      "Training Epoch: 17 [12416/36450]\tLoss: 526.6559\n",
      "Training Epoch: 17 [12480/36450]\tLoss: 474.6898\n",
      "Training Epoch: 17 [12544/36450]\tLoss: 514.4070\n",
      "Training Epoch: 17 [12608/36450]\tLoss: 495.0341\n",
      "Training Epoch: 17 [12672/36450]\tLoss: 464.6477\n",
      "Training Epoch: 17 [12736/36450]\tLoss: 525.0012\n",
      "Training Epoch: 17 [12800/36450]\tLoss: 511.1892\n",
      "Training Epoch: 17 [12864/36450]\tLoss: 506.3101\n",
      "Training Epoch: 17 [12928/36450]\tLoss: 501.1259\n",
      "Training Epoch: 17 [12992/36450]\tLoss: 506.2238\n",
      "Training Epoch: 17 [13056/36450]\tLoss: 543.2111\n",
      "Training Epoch: 17 [13120/36450]\tLoss: 524.6529\n",
      "Training Epoch: 17 [13184/36450]\tLoss: 485.2715\n",
      "Training Epoch: 17 [13248/36450]\tLoss: 502.3724\n",
      "Training Epoch: 17 [13312/36450]\tLoss: 488.4929\n",
      "Training Epoch: 17 [13376/36450]\tLoss: 517.5809\n",
      "Training Epoch: 17 [13440/36450]\tLoss: 475.6311\n",
      "Training Epoch: 17 [13504/36450]\tLoss: 498.1245\n",
      "Training Epoch: 17 [13568/36450]\tLoss: 471.2200\n",
      "Training Epoch: 17 [13632/36450]\tLoss: 492.6950\n",
      "Training Epoch: 17 [13696/36450]\tLoss: 495.6729\n",
      "Training Epoch: 17 [13760/36450]\tLoss: 527.5472\n",
      "Training Epoch: 17 [13824/36450]\tLoss: 504.4762\n",
      "Training Epoch: 17 [13888/36450]\tLoss: 488.7402\n",
      "Training Epoch: 17 [13952/36450]\tLoss: 516.4813\n",
      "Training Epoch: 17 [14016/36450]\tLoss: 523.2719\n",
      "Training Epoch: 17 [14080/36450]\tLoss: 479.5090\n",
      "Training Epoch: 17 [14144/36450]\tLoss: 497.3135\n",
      "Training Epoch: 17 [14208/36450]\tLoss: 472.0814\n",
      "Training Epoch: 17 [14272/36450]\tLoss: 499.2123\n",
      "Training Epoch: 17 [14336/36450]\tLoss: 501.1025\n",
      "Training Epoch: 17 [14400/36450]\tLoss: 481.7490\n",
      "Training Epoch: 17 [14464/36450]\tLoss: 510.6178\n",
      "Training Epoch: 17 [14528/36450]\tLoss: 489.2750\n",
      "Training Epoch: 17 [14592/36450]\tLoss: 495.5186\n",
      "Training Epoch: 17 [14656/36450]\tLoss: 486.3626\n",
      "Training Epoch: 17 [14720/36450]\tLoss: 475.4299\n",
      "Training Epoch: 17 [14784/36450]\tLoss: 514.5714\n",
      "Training Epoch: 17 [14848/36450]\tLoss: 511.7139\n",
      "Training Epoch: 17 [14912/36450]\tLoss: 480.8191\n",
      "Training Epoch: 17 [14976/36450]\tLoss: 492.0198\n",
      "Training Epoch: 17 [15040/36450]\tLoss: 485.7501\n",
      "Training Epoch: 17 [15104/36450]\tLoss: 497.9335\n",
      "Training Epoch: 17 [15168/36450]\tLoss: 484.5852\n",
      "Training Epoch: 17 [15232/36450]\tLoss: 471.7885\n",
      "Training Epoch: 17 [15296/36450]\tLoss: 457.5992\n",
      "Training Epoch: 17 [15360/36450]\tLoss: 463.0984\n",
      "Training Epoch: 17 [15424/36450]\tLoss: 479.2288\n",
      "Training Epoch: 17 [15488/36450]\tLoss: 496.9394\n",
      "Training Epoch: 17 [15552/36450]\tLoss: 484.2908\n",
      "Training Epoch: 17 [15616/36450]\tLoss: 484.4404\n",
      "Training Epoch: 17 [15680/36450]\tLoss: 490.0666\n",
      "Training Epoch: 17 [15744/36450]\tLoss: 477.6153\n",
      "Training Epoch: 17 [15808/36450]\tLoss: 472.7805\n",
      "Training Epoch: 17 [15872/36450]\tLoss: 473.0298\n",
      "Training Epoch: 17 [15936/36450]\tLoss: 482.4201\n",
      "Training Epoch: 17 [16000/36450]\tLoss: 483.6887\n",
      "Training Epoch: 17 [16064/36450]\tLoss: 491.1377\n",
      "Training Epoch: 17 [16128/36450]\tLoss: 497.8445\n",
      "Training Epoch: 17 [16192/36450]\tLoss: 468.0515\n",
      "Training Epoch: 17 [16256/36450]\tLoss: 462.6881\n",
      "Training Epoch: 17 [16320/36450]\tLoss: 468.8155\n",
      "Training Epoch: 17 [16384/36450]\tLoss: 484.2648\n",
      "Training Epoch: 17 [16448/36450]\tLoss: 457.8158\n",
      "Training Epoch: 17 [16512/36450]\tLoss: 482.9156\n",
      "Training Epoch: 17 [16576/36450]\tLoss: 484.3023\n",
      "Training Epoch: 17 [16640/36450]\tLoss: 493.8061\n",
      "Training Epoch: 17 [16704/36450]\tLoss: 480.2440\n",
      "Training Epoch: 17 [16768/36450]\tLoss: 482.2721\n",
      "Training Epoch: 17 [16832/36450]\tLoss: 477.7393\n",
      "Training Epoch: 17 [16896/36450]\tLoss: 471.8488\n",
      "Training Epoch: 17 [16960/36450]\tLoss: 516.7104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [17024/36450]\tLoss: 519.9738\n",
      "Training Epoch: 17 [17088/36450]\tLoss: 504.1707\n",
      "Training Epoch: 17 [17152/36450]\tLoss: 527.1054\n",
      "Training Epoch: 17 [17216/36450]\tLoss: 489.4682\n",
      "Training Epoch: 17 [17280/36450]\tLoss: 506.2601\n",
      "Training Epoch: 17 [17344/36450]\tLoss: 484.6967\n",
      "Training Epoch: 17 [17408/36450]\tLoss: 485.0809\n",
      "Training Epoch: 17 [17472/36450]\tLoss: 489.7477\n",
      "Training Epoch: 17 [17536/36450]\tLoss: 503.6891\n",
      "Training Epoch: 17 [17600/36450]\tLoss: 486.6984\n",
      "Training Epoch: 17 [17664/36450]\tLoss: 486.8567\n",
      "Training Epoch: 17 [17728/36450]\tLoss: 461.4785\n",
      "Training Epoch: 17 [17792/36450]\tLoss: 494.5645\n",
      "Training Epoch: 17 [17856/36450]\tLoss: 463.4320\n",
      "Training Epoch: 17 [17920/36450]\tLoss: 490.2900\n",
      "Training Epoch: 17 [17984/36450]\tLoss: 514.7303\n",
      "Training Epoch: 17 [18048/36450]\tLoss: 495.1124\n",
      "Training Epoch: 17 [18112/36450]\tLoss: 471.1823\n",
      "Training Epoch: 17 [18176/36450]\tLoss: 503.9115\n",
      "Training Epoch: 17 [18240/36450]\tLoss: 524.8055\n",
      "Training Epoch: 17 [18304/36450]\tLoss: 495.4259\n",
      "Training Epoch: 17 [18368/36450]\tLoss: 529.7614\n",
      "Training Epoch: 17 [18432/36450]\tLoss: 501.1136\n",
      "Training Epoch: 17 [18496/36450]\tLoss: 522.8343\n",
      "Training Epoch: 17 [18560/36450]\tLoss: 516.2560\n",
      "Training Epoch: 17 [18624/36450]\tLoss: 470.7418\n",
      "Training Epoch: 17 [18688/36450]\tLoss: 499.7960\n",
      "Training Epoch: 17 [18752/36450]\tLoss: 483.7842\n",
      "Training Epoch: 17 [18816/36450]\tLoss: 516.1155\n",
      "Training Epoch: 17 [18880/36450]\tLoss: 493.9850\n",
      "Training Epoch: 17 [18944/36450]\tLoss: 493.1591\n",
      "Training Epoch: 17 [19008/36450]\tLoss: 516.8712\n",
      "Training Epoch: 17 [19072/36450]\tLoss: 498.5741\n",
      "Training Epoch: 17 [19136/36450]\tLoss: 509.7390\n",
      "Training Epoch: 17 [19200/36450]\tLoss: 480.1395\n",
      "Training Epoch: 17 [19264/36450]\tLoss: 484.7511\n",
      "Training Epoch: 17 [19328/36450]\tLoss: 503.7124\n",
      "Training Epoch: 17 [19392/36450]\tLoss: 499.9844\n",
      "Training Epoch: 17 [19456/36450]\tLoss: 505.4426\n",
      "Training Epoch: 17 [19520/36450]\tLoss: 481.6771\n",
      "Training Epoch: 17 [19584/36450]\tLoss: 471.6298\n",
      "Training Epoch: 17 [19648/36450]\tLoss: 492.7635\n",
      "Training Epoch: 17 [19712/36450]\tLoss: 487.6183\n",
      "Training Epoch: 17 [19776/36450]\tLoss: 472.5262\n",
      "Training Epoch: 17 [19840/36450]\tLoss: 469.4359\n",
      "Training Epoch: 17 [19904/36450]\tLoss: 546.3719\n",
      "Training Epoch: 17 [19968/36450]\tLoss: 498.6816\n",
      "Training Epoch: 17 [20032/36450]\tLoss: 521.9607\n",
      "Training Epoch: 17 [20096/36450]\tLoss: 496.9312\n",
      "Training Epoch: 17 [20160/36450]\tLoss: 519.0985\n",
      "Training Epoch: 17 [20224/36450]\tLoss: 497.7901\n",
      "Training Epoch: 17 [20288/36450]\tLoss: 512.8054\n",
      "Training Epoch: 17 [20352/36450]\tLoss: 528.9446\n",
      "Training Epoch: 17 [20416/36450]\tLoss: 517.3641\n",
      "Training Epoch: 17 [20480/36450]\tLoss: 526.7205\n",
      "Training Epoch: 17 [20544/36450]\tLoss: 534.3293\n",
      "Training Epoch: 17 [20608/36450]\tLoss: 509.6880\n",
      "Training Epoch: 17 [20672/36450]\tLoss: 531.5164\n",
      "Training Epoch: 17 [20736/36450]\tLoss: 552.0267\n",
      "Training Epoch: 17 [20800/36450]\tLoss: 550.5527\n",
      "Training Epoch: 17 [20864/36450]\tLoss: 555.2776\n",
      "Training Epoch: 17 [20928/36450]\tLoss: 499.1350\n",
      "Training Epoch: 17 [20992/36450]\tLoss: 542.1338\n",
      "Training Epoch: 17 [21056/36450]\tLoss: 488.0357\n",
      "Training Epoch: 17 [21120/36450]\tLoss: 517.7121\n",
      "Training Epoch: 17 [21184/36450]\tLoss: 493.0180\n",
      "Training Epoch: 17 [21248/36450]\tLoss: 516.0581\n",
      "Training Epoch: 17 [21312/36450]\tLoss: 495.1698\n",
      "Training Epoch: 17 [21376/36450]\tLoss: 486.4935\n",
      "Training Epoch: 17 [21440/36450]\tLoss: 519.2378\n",
      "Training Epoch: 17 [21504/36450]\tLoss: 457.2059\n",
      "Training Epoch: 17 [21568/36450]\tLoss: 499.6675\n",
      "Training Epoch: 17 [21632/36450]\tLoss: 532.7621\n",
      "Training Epoch: 17 [21696/36450]\tLoss: 488.8831\n",
      "Training Epoch: 17 [21760/36450]\tLoss: 518.5090\n",
      "Training Epoch: 17 [21824/36450]\tLoss: 508.6545\n",
      "Training Epoch: 17 [21888/36450]\tLoss: 512.2678\n",
      "Training Epoch: 17 [21952/36450]\tLoss: 490.6810\n",
      "Training Epoch: 17 [22016/36450]\tLoss: 493.8106\n",
      "Training Epoch: 17 [22080/36450]\tLoss: 517.0389\n",
      "Training Epoch: 17 [22144/36450]\tLoss: 497.2555\n",
      "Training Epoch: 17 [22208/36450]\tLoss: 533.4111\n",
      "Training Epoch: 17 [22272/36450]\tLoss: 475.8678\n",
      "Training Epoch: 17 [22336/36450]\tLoss: 473.0902\n",
      "Training Epoch: 17 [22400/36450]\tLoss: 487.9380\n",
      "Training Epoch: 17 [22464/36450]\tLoss: 470.0454\n",
      "Training Epoch: 17 [22528/36450]\tLoss: 480.9850\n",
      "Training Epoch: 17 [22592/36450]\tLoss: 530.0138\n",
      "Training Epoch: 17 [22656/36450]\tLoss: 492.4560\n",
      "Training Epoch: 17 [22720/36450]\tLoss: 464.3346\n",
      "Training Epoch: 17 [22784/36450]\tLoss: 515.3080\n",
      "Training Epoch: 17 [22848/36450]\tLoss: 485.3944\n",
      "Training Epoch: 17 [22912/36450]\tLoss: 489.9677\n",
      "Training Epoch: 17 [22976/36450]\tLoss: 456.9415\n",
      "Training Epoch: 17 [23040/36450]\tLoss: 500.7730\n",
      "Training Epoch: 17 [23104/36450]\tLoss: 494.2067\n",
      "Training Epoch: 17 [23168/36450]\tLoss: 501.6878\n",
      "Training Epoch: 17 [23232/36450]\tLoss: 502.0961\n",
      "Training Epoch: 17 [23296/36450]\tLoss: 478.7754\n",
      "Training Epoch: 17 [23360/36450]\tLoss: 485.9379\n",
      "Training Epoch: 17 [23424/36450]\tLoss: 474.2830\n",
      "Training Epoch: 17 [23488/36450]\tLoss: 495.6217\n",
      "Training Epoch: 17 [23552/36450]\tLoss: 497.0824\n",
      "Training Epoch: 17 [23616/36450]\tLoss: 529.4025\n",
      "Training Epoch: 17 [23680/36450]\tLoss: 513.7491\n",
      "Training Epoch: 17 [23744/36450]\tLoss: 473.0585\n",
      "Training Epoch: 17 [23808/36450]\tLoss: 487.4238\n",
      "Training Epoch: 17 [23872/36450]\tLoss: 487.6354\n",
      "Training Epoch: 17 [23936/36450]\tLoss: 479.0273\n",
      "Training Epoch: 17 [24000/36450]\tLoss: 473.8288\n",
      "Training Epoch: 17 [24064/36450]\tLoss: 473.0942\n",
      "Training Epoch: 17 [24128/36450]\tLoss: 492.8223\n",
      "Training Epoch: 17 [24192/36450]\tLoss: 520.0468\n",
      "Training Epoch: 17 [24256/36450]\tLoss: 479.3254\n",
      "Training Epoch: 17 [24320/36450]\tLoss: 479.5862\n",
      "Training Epoch: 17 [24384/36450]\tLoss: 478.5678\n",
      "Training Epoch: 17 [24448/36450]\tLoss: 516.6827\n",
      "Training Epoch: 17 [24512/36450]\tLoss: 531.4282\n",
      "Training Epoch: 17 [24576/36450]\tLoss: 500.6499\n",
      "Training Epoch: 17 [24640/36450]\tLoss: 472.9799\n",
      "Training Epoch: 17 [24704/36450]\tLoss: 467.0914\n",
      "Training Epoch: 17 [24768/36450]\tLoss: 476.2248\n",
      "Training Epoch: 17 [24832/36450]\tLoss: 505.3856\n",
      "Training Epoch: 17 [24896/36450]\tLoss: 483.8113\n",
      "Training Epoch: 17 [24960/36450]\tLoss: 487.5828\n",
      "Training Epoch: 17 [25024/36450]\tLoss: 479.0380\n",
      "Training Epoch: 17 [25088/36450]\tLoss: 484.8465\n",
      "Training Epoch: 17 [25152/36450]\tLoss: 508.0276\n",
      "Training Epoch: 17 [25216/36450]\tLoss: 497.6147\n",
      "Training Epoch: 17 [25280/36450]\tLoss: 487.8848\n",
      "Training Epoch: 17 [25344/36450]\tLoss: 489.7008\n",
      "Training Epoch: 17 [25408/36450]\tLoss: 491.4553\n",
      "Training Epoch: 17 [25472/36450]\tLoss: 511.0448\n",
      "Training Epoch: 17 [25536/36450]\tLoss: 502.6589\n",
      "Training Epoch: 17 [25600/36450]\tLoss: 486.5653\n",
      "Training Epoch: 17 [25664/36450]\tLoss: 489.0190\n",
      "Training Epoch: 17 [25728/36450]\tLoss: 492.0189\n",
      "Training Epoch: 17 [25792/36450]\tLoss: 488.7777\n",
      "Training Epoch: 17 [25856/36450]\tLoss: 471.1298\n",
      "Training Epoch: 17 [25920/36450]\tLoss: 512.9020\n",
      "Training Epoch: 17 [25984/36450]\tLoss: 496.1602\n",
      "Training Epoch: 17 [26048/36450]\tLoss: 481.2168\n",
      "Training Epoch: 17 [26112/36450]\tLoss: 517.3693\n",
      "Training Epoch: 17 [26176/36450]\tLoss: 491.8081\n",
      "Training Epoch: 17 [26240/36450]\tLoss: 460.7549\n",
      "Training Epoch: 17 [26304/36450]\tLoss: 481.8087\n",
      "Training Epoch: 17 [26368/36450]\tLoss: 495.8861\n",
      "Training Epoch: 17 [26432/36450]\tLoss: 474.1240\n",
      "Training Epoch: 17 [26496/36450]\tLoss: 485.9702\n",
      "Training Epoch: 17 [26560/36450]\tLoss: 487.6823\n",
      "Training Epoch: 17 [26624/36450]\tLoss: 517.5439\n",
      "Training Epoch: 17 [26688/36450]\tLoss: 473.0804\n",
      "Training Epoch: 17 [26752/36450]\tLoss: 504.1549\n",
      "Training Epoch: 17 [26816/36450]\tLoss: 491.0571\n",
      "Training Epoch: 17 [26880/36450]\tLoss: 474.3974\n",
      "Training Epoch: 17 [26944/36450]\tLoss: 489.5305\n",
      "Training Epoch: 17 [27008/36450]\tLoss: 494.4782\n",
      "Training Epoch: 17 [27072/36450]\tLoss: 484.6884\n",
      "Training Epoch: 17 [27136/36450]\tLoss: 528.9484\n",
      "Training Epoch: 17 [27200/36450]\tLoss: 482.1213\n",
      "Training Epoch: 17 [27264/36450]\tLoss: 474.7712\n",
      "Training Epoch: 17 [27328/36450]\tLoss: 487.1951\n",
      "Training Epoch: 17 [27392/36450]\tLoss: 519.6846\n",
      "Training Epoch: 17 [27456/36450]\tLoss: 487.5550\n",
      "Training Epoch: 17 [27520/36450]\tLoss: 501.1787\n",
      "Training Epoch: 17 [27584/36450]\tLoss: 495.4587\n",
      "Training Epoch: 17 [27648/36450]\tLoss: 441.6281\n",
      "Training Epoch: 17 [27712/36450]\tLoss: 492.6400\n",
      "Training Epoch: 17 [27776/36450]\tLoss: 510.1437\n",
      "Training Epoch: 17 [27840/36450]\tLoss: 512.0785\n",
      "Training Epoch: 17 [27904/36450]\tLoss: 483.6393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [27968/36450]\tLoss: 491.1284\n",
      "Training Epoch: 17 [28032/36450]\tLoss: 457.0268\n",
      "Training Epoch: 17 [28096/36450]\tLoss: 482.5271\n",
      "Training Epoch: 17 [28160/36450]\tLoss: 469.4101\n",
      "Training Epoch: 17 [28224/36450]\tLoss: 459.7397\n",
      "Training Epoch: 17 [28288/36450]\tLoss: 491.3287\n",
      "Training Epoch: 17 [28352/36450]\tLoss: 477.3310\n",
      "Training Epoch: 17 [28416/36450]\tLoss: 485.6060\n",
      "Training Epoch: 17 [28480/36450]\tLoss: 500.0483\n",
      "Training Epoch: 17 [28544/36450]\tLoss: 496.1757\n",
      "Training Epoch: 17 [28608/36450]\tLoss: 493.1650\n",
      "Training Epoch: 17 [28672/36450]\tLoss: 471.0292\n",
      "Training Epoch: 17 [28736/36450]\tLoss: 483.4101\n",
      "Training Epoch: 17 [28800/36450]\tLoss: 495.6486\n",
      "Training Epoch: 17 [28864/36450]\tLoss: 487.0632\n",
      "Training Epoch: 17 [28928/36450]\tLoss: 488.8680\n",
      "Training Epoch: 17 [28992/36450]\tLoss: 505.0391\n",
      "Training Epoch: 17 [29056/36450]\tLoss: 489.6467\n",
      "Training Epoch: 17 [29120/36450]\tLoss: 508.0580\n",
      "Training Epoch: 17 [29184/36450]\tLoss: 502.2827\n",
      "Training Epoch: 17 [29248/36450]\tLoss: 477.2392\n",
      "Training Epoch: 17 [29312/36450]\tLoss: 514.1137\n",
      "Training Epoch: 17 [29376/36450]\tLoss: 494.9231\n",
      "Training Epoch: 17 [29440/36450]\tLoss: 499.9091\n",
      "Training Epoch: 17 [29504/36450]\tLoss: 486.9567\n",
      "Training Epoch: 17 [29568/36450]\tLoss: 486.3671\n",
      "Training Epoch: 17 [29632/36450]\tLoss: 499.4110\n",
      "Training Epoch: 17 [29696/36450]\tLoss: 476.2925\n",
      "Training Epoch: 17 [29760/36450]\tLoss: 499.4265\n",
      "Training Epoch: 17 [29824/36450]\tLoss: 484.4158\n",
      "Training Epoch: 17 [29888/36450]\tLoss: 480.4014\n",
      "Training Epoch: 17 [29952/36450]\tLoss: 479.8919\n",
      "Training Epoch: 17 [30016/36450]\tLoss: 459.9032\n",
      "Training Epoch: 17 [30080/36450]\tLoss: 483.7242\n",
      "Training Epoch: 17 [30144/36450]\tLoss: 475.2839\n",
      "Training Epoch: 17 [30208/36450]\tLoss: 496.3568\n",
      "Training Epoch: 17 [30272/36450]\tLoss: 513.9203\n",
      "Training Epoch: 17 [30336/36450]\tLoss: 489.1996\n",
      "Training Epoch: 17 [30400/36450]\tLoss: 506.4560\n",
      "Training Epoch: 17 [30464/36450]\tLoss: 475.4718\n",
      "Training Epoch: 17 [30528/36450]\tLoss: 507.8838\n",
      "Training Epoch: 17 [30592/36450]\tLoss: 504.4130\n",
      "Training Epoch: 17 [30656/36450]\tLoss: 500.2613\n",
      "Training Epoch: 17 [30720/36450]\tLoss: 502.5818\n",
      "Training Epoch: 17 [30784/36450]\tLoss: 511.7854\n",
      "Training Epoch: 17 [30848/36450]\tLoss: 474.1561\n",
      "Training Epoch: 17 [30912/36450]\tLoss: 491.9286\n",
      "Training Epoch: 17 [30976/36450]\tLoss: 486.6993\n",
      "Training Epoch: 17 [31040/36450]\tLoss: 514.5768\n",
      "Training Epoch: 17 [31104/36450]\tLoss: 484.7930\n",
      "Training Epoch: 17 [31168/36450]\tLoss: 468.3366\n",
      "Training Epoch: 17 [31232/36450]\tLoss: 482.9452\n",
      "Training Epoch: 17 [31296/36450]\tLoss: 501.9921\n",
      "Training Epoch: 17 [31360/36450]\tLoss: 487.0525\n",
      "Training Epoch: 17 [31424/36450]\tLoss: 483.8720\n",
      "Training Epoch: 17 [31488/36450]\tLoss: 494.6396\n",
      "Training Epoch: 17 [31552/36450]\tLoss: 489.4269\n",
      "Training Epoch: 17 [31616/36450]\tLoss: 475.7579\n",
      "Training Epoch: 17 [31680/36450]\tLoss: 503.7640\n",
      "Training Epoch: 17 [31744/36450]\tLoss: 494.8526\n",
      "Training Epoch: 17 [31808/36450]\tLoss: 503.7090\n",
      "Training Epoch: 17 [31872/36450]\tLoss: 486.5180\n",
      "Training Epoch: 17 [31936/36450]\tLoss: 530.1580\n",
      "Training Epoch: 17 [32000/36450]\tLoss: 455.5230\n",
      "Training Epoch: 17 [32064/36450]\tLoss: 478.0454\n",
      "Training Epoch: 17 [32128/36450]\tLoss: 479.1771\n",
      "Training Epoch: 17 [32192/36450]\tLoss: 510.5548\n",
      "Training Epoch: 17 [32256/36450]\tLoss: 479.2873\n",
      "Training Epoch: 17 [32320/36450]\tLoss: 468.5977\n",
      "Training Epoch: 17 [32384/36450]\tLoss: 465.3037\n",
      "Training Epoch: 17 [32448/36450]\tLoss: 451.7866\n",
      "Training Epoch: 17 [32512/36450]\tLoss: 499.0852\n",
      "Training Epoch: 17 [32576/36450]\tLoss: 476.5486\n",
      "Training Epoch: 17 [32640/36450]\tLoss: 499.7719\n",
      "Training Epoch: 17 [32704/36450]\tLoss: 458.3626\n",
      "Training Epoch: 17 [32768/36450]\tLoss: 491.2537\n",
      "Training Epoch: 17 [32832/36450]\tLoss: 481.9474\n",
      "Training Epoch: 17 [32896/36450]\tLoss: 485.3900\n",
      "Training Epoch: 17 [32960/36450]\tLoss: 501.9845\n",
      "Training Epoch: 17 [33024/36450]\tLoss: 480.6746\n",
      "Training Epoch: 17 [33088/36450]\tLoss: 483.3072\n",
      "Training Epoch: 17 [33152/36450]\tLoss: 510.3126\n",
      "Training Epoch: 17 [33216/36450]\tLoss: 512.7584\n",
      "Training Epoch: 17 [33280/36450]\tLoss: 514.5633\n",
      "Training Epoch: 17 [33344/36450]\tLoss: 495.3531\n",
      "Training Epoch: 17 [33408/36450]\tLoss: 497.6876\n",
      "Training Epoch: 17 [33472/36450]\tLoss: 525.3969\n",
      "Training Epoch: 17 [33536/36450]\tLoss: 468.2998\n",
      "Training Epoch: 17 [33600/36450]\tLoss: 489.0990\n",
      "Training Epoch: 17 [33664/36450]\tLoss: 488.5130\n",
      "Training Epoch: 17 [33728/36450]\tLoss: 468.6412\n",
      "Training Epoch: 17 [33792/36450]\tLoss: 497.1118\n",
      "Training Epoch: 17 [33856/36450]\tLoss: 494.1790\n",
      "Training Epoch: 17 [33920/36450]\tLoss: 492.9998\n",
      "Training Epoch: 17 [33984/36450]\tLoss: 512.5213\n",
      "Training Epoch: 17 [34048/36450]\tLoss: 498.5097\n",
      "Training Epoch: 17 [34112/36450]\tLoss: 490.5530\n",
      "Training Epoch: 17 [34176/36450]\tLoss: 472.3618\n",
      "Training Epoch: 17 [34240/36450]\tLoss: 487.7959\n",
      "Training Epoch: 17 [34304/36450]\tLoss: 489.1714\n",
      "Training Epoch: 17 [34368/36450]\tLoss: 488.1084\n",
      "Training Epoch: 17 [34432/36450]\tLoss: 489.6374\n",
      "Training Epoch: 17 [34496/36450]\tLoss: 498.5052\n",
      "Training Epoch: 17 [34560/36450]\tLoss: 478.3885\n",
      "Training Epoch: 17 [34624/36450]\tLoss: 494.0558\n",
      "Training Epoch: 17 [34688/36450]\tLoss: 511.9597\n",
      "Training Epoch: 17 [34752/36450]\tLoss: 497.0766\n",
      "Training Epoch: 17 [34816/36450]\tLoss: 501.6513\n",
      "Training Epoch: 17 [34880/36450]\tLoss: 497.0636\n",
      "Training Epoch: 17 [34944/36450]\tLoss: 474.5468\n",
      "Training Epoch: 17 [35008/36450]\tLoss: 533.6890\n",
      "Training Epoch: 17 [35072/36450]\tLoss: 497.6325\n",
      "Training Epoch: 17 [35136/36450]\tLoss: 530.2772\n",
      "Training Epoch: 17 [35200/36450]\tLoss: 492.4795\n",
      "Training Epoch: 17 [35264/36450]\tLoss: 489.3404\n",
      "Training Epoch: 17 [35328/36450]\tLoss: 491.0354\n",
      "Training Epoch: 17 [35392/36450]\tLoss: 518.1901\n",
      "Training Epoch: 17 [35456/36450]\tLoss: 518.5168\n",
      "Training Epoch: 17 [35520/36450]\tLoss: 505.1183\n",
      "Training Epoch: 17 [35584/36450]\tLoss: 499.5764\n",
      "Training Epoch: 17 [35648/36450]\tLoss: 499.8177\n",
      "Training Epoch: 17 [35712/36450]\tLoss: 496.4512\n",
      "Training Epoch: 17 [35776/36450]\tLoss: 473.5854\n",
      "Training Epoch: 17 [35840/36450]\tLoss: 497.2526\n",
      "Training Epoch: 17 [35904/36450]\tLoss: 485.0307\n",
      "Training Epoch: 17 [35968/36450]\tLoss: 477.4649\n",
      "Training Epoch: 17 [36032/36450]\tLoss: 483.7715\n",
      "Training Epoch: 17 [36096/36450]\tLoss: 533.8352\n",
      "Training Epoch: 17 [36160/36450]\tLoss: 523.2558\n",
      "Training Epoch: 17 [36224/36450]\tLoss: 472.3291\n",
      "Training Epoch: 17 [36288/36450]\tLoss: 511.4299\n",
      "Training Epoch: 17 [36352/36450]\tLoss: 494.0145\n",
      "Training Epoch: 17 [36416/36450]\tLoss: 463.4352\n",
      "Training Epoch: 17 [36450/36450]\tLoss: 491.0985\n",
      "Training Epoch: 17 [4050/4050]\tLoss: 241.4258\n",
      "Training Epoch: 18 [64/36450]\tLoss: 474.9508\n",
      "Training Epoch: 18 [128/36450]\tLoss: 491.4905\n",
      "Training Epoch: 18 [192/36450]\tLoss: 458.1101\n",
      "Training Epoch: 18 [256/36450]\tLoss: 477.9349\n",
      "Training Epoch: 18 [320/36450]\tLoss: 515.0583\n",
      "Training Epoch: 18 [384/36450]\tLoss: 481.8296\n",
      "Training Epoch: 18 [448/36450]\tLoss: 481.4376\n",
      "Training Epoch: 18 [512/36450]\tLoss: 470.2150\n",
      "Training Epoch: 18 [576/36450]\tLoss: 495.1529\n",
      "Training Epoch: 18 [640/36450]\tLoss: 498.8695\n",
      "Training Epoch: 18 [704/36450]\tLoss: 465.4107\n",
      "Training Epoch: 18 [768/36450]\tLoss: 460.1908\n",
      "Training Epoch: 18 [832/36450]\tLoss: 482.3950\n",
      "Training Epoch: 18 [896/36450]\tLoss: 454.3708\n",
      "Training Epoch: 18 [960/36450]\tLoss: 505.2911\n",
      "Training Epoch: 18 [1024/36450]\tLoss: 486.1176\n",
      "Training Epoch: 18 [1088/36450]\tLoss: 500.2002\n",
      "Training Epoch: 18 [1152/36450]\tLoss: 478.8733\n",
      "Training Epoch: 18 [1216/36450]\tLoss: 495.4497\n",
      "Training Epoch: 18 [1280/36450]\tLoss: 500.4201\n",
      "Training Epoch: 18 [1344/36450]\tLoss: 509.6007\n",
      "Training Epoch: 18 [1408/36450]\tLoss: 490.4803\n",
      "Training Epoch: 18 [1472/36450]\tLoss: 512.0646\n",
      "Training Epoch: 18 [1536/36450]\tLoss: 468.3998\n",
      "Training Epoch: 18 [1600/36450]\tLoss: 483.1682\n",
      "Training Epoch: 18 [1664/36450]\tLoss: 501.1815\n",
      "Training Epoch: 18 [1728/36450]\tLoss: 480.2471\n",
      "Training Epoch: 18 [1792/36450]\tLoss: 509.7462\n",
      "Training Epoch: 18 [1856/36450]\tLoss: 507.8644\n",
      "Training Epoch: 18 [1920/36450]\tLoss: 477.2071\n",
      "Training Epoch: 18 [1984/36450]\tLoss: 498.9685\n",
      "Training Epoch: 18 [2048/36450]\tLoss: 497.9394\n",
      "Training Epoch: 18 [2112/36450]\tLoss: 491.4064\n",
      "Training Epoch: 18 [2176/36450]\tLoss: 495.8737\n",
      "Training Epoch: 18 [2240/36450]\tLoss: 501.3787\n",
      "Training Epoch: 18 [2304/36450]\tLoss: 506.7138\n",
      "Training Epoch: 18 [2368/36450]\tLoss: 492.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [2432/36450]\tLoss: 481.9218\n",
      "Training Epoch: 18 [2496/36450]\tLoss: 479.9966\n",
      "Training Epoch: 18 [2560/36450]\tLoss: 464.8196\n",
      "Training Epoch: 18 [2624/36450]\tLoss: 497.3509\n",
      "Training Epoch: 18 [2688/36450]\tLoss: 489.2599\n",
      "Training Epoch: 18 [2752/36450]\tLoss: 491.4691\n",
      "Training Epoch: 18 [2816/36450]\tLoss: 501.7284\n",
      "Training Epoch: 18 [2880/36450]\tLoss: 481.5089\n",
      "Training Epoch: 18 [2944/36450]\tLoss: 498.6441\n",
      "Training Epoch: 18 [3008/36450]\tLoss: 483.8890\n",
      "Training Epoch: 18 [3072/36450]\tLoss: 505.8849\n",
      "Training Epoch: 18 [3136/36450]\tLoss: 534.4259\n",
      "Training Epoch: 18 [3200/36450]\tLoss: 491.9118\n",
      "Training Epoch: 18 [3264/36450]\tLoss: 480.9702\n",
      "Training Epoch: 18 [3328/36450]\tLoss: 486.3993\n",
      "Training Epoch: 18 [3392/36450]\tLoss: 502.9165\n",
      "Training Epoch: 18 [3456/36450]\tLoss: 491.3142\n",
      "Training Epoch: 18 [3520/36450]\tLoss: 481.6968\n",
      "Training Epoch: 18 [3584/36450]\tLoss: 498.7180\n",
      "Training Epoch: 18 [3648/36450]\tLoss: 478.5837\n",
      "Training Epoch: 18 [3712/36450]\tLoss: 485.6489\n",
      "Training Epoch: 18 [3776/36450]\tLoss: 527.7639\n",
      "Training Epoch: 18 [3840/36450]\tLoss: 487.1488\n",
      "Training Epoch: 18 [3904/36450]\tLoss: 488.5246\n",
      "Training Epoch: 18 [3968/36450]\tLoss: 494.5211\n",
      "Training Epoch: 18 [4032/36450]\tLoss: 496.5094\n",
      "Training Epoch: 18 [4096/36450]\tLoss: 516.5458\n",
      "Training Epoch: 18 [4160/36450]\tLoss: 456.1220\n",
      "Training Epoch: 18 [4224/36450]\tLoss: 468.3395\n",
      "Training Epoch: 18 [4288/36450]\tLoss: 470.9353\n",
      "Training Epoch: 18 [4352/36450]\tLoss: 492.3216\n",
      "Training Epoch: 18 [4416/36450]\tLoss: 495.6150\n",
      "Training Epoch: 18 [4480/36450]\tLoss: 484.5527\n",
      "Training Epoch: 18 [4544/36450]\tLoss: 480.5971\n",
      "Training Epoch: 18 [4608/36450]\tLoss: 489.9754\n",
      "Training Epoch: 18 [4672/36450]\tLoss: 507.6045\n",
      "Training Epoch: 18 [4736/36450]\tLoss: 471.4789\n",
      "Training Epoch: 18 [4800/36450]\tLoss: 475.5441\n",
      "Training Epoch: 18 [4864/36450]\tLoss: 476.8132\n",
      "Training Epoch: 18 [4928/36450]\tLoss: 508.2040\n",
      "Training Epoch: 18 [4992/36450]\tLoss: 514.0084\n",
      "Training Epoch: 18 [5056/36450]\tLoss: 502.2398\n",
      "Training Epoch: 18 [5120/36450]\tLoss: 505.0439\n",
      "Training Epoch: 18 [5184/36450]\tLoss: 516.1607\n",
      "Training Epoch: 18 [5248/36450]\tLoss: 490.8879\n",
      "Training Epoch: 18 [5312/36450]\tLoss: 452.9447\n",
      "Training Epoch: 18 [5376/36450]\tLoss: 471.2663\n",
      "Training Epoch: 18 [5440/36450]\tLoss: 511.9199\n",
      "Training Epoch: 18 [5504/36450]\tLoss: 505.8784\n",
      "Training Epoch: 18 [5568/36450]\tLoss: 488.1814\n",
      "Training Epoch: 18 [5632/36450]\tLoss: 493.9575\n",
      "Training Epoch: 18 [5696/36450]\tLoss: 482.8801\n",
      "Training Epoch: 18 [5760/36450]\tLoss: 506.7059\n",
      "Training Epoch: 18 [5824/36450]\tLoss: 491.3438\n",
      "Training Epoch: 18 [5888/36450]\tLoss: 465.3620\n",
      "Training Epoch: 18 [5952/36450]\tLoss: 483.9697\n",
      "Training Epoch: 18 [6016/36450]\tLoss: 509.7470\n",
      "Training Epoch: 18 [6080/36450]\tLoss: 499.3270\n",
      "Training Epoch: 18 [6144/36450]\tLoss: 491.5773\n",
      "Training Epoch: 18 [6208/36450]\tLoss: 492.7556\n",
      "Training Epoch: 18 [6272/36450]\tLoss: 501.2556\n",
      "Training Epoch: 18 [6336/36450]\tLoss: 503.4829\n",
      "Training Epoch: 18 [6400/36450]\tLoss: 481.5095\n",
      "Training Epoch: 18 [6464/36450]\tLoss: 468.7007\n",
      "Training Epoch: 18 [6528/36450]\tLoss: 461.0629\n",
      "Training Epoch: 18 [6592/36450]\tLoss: 483.7784\n",
      "Training Epoch: 18 [6656/36450]\tLoss: 484.5023\n",
      "Training Epoch: 18 [6720/36450]\tLoss: 508.3555\n",
      "Training Epoch: 18 [6784/36450]\tLoss: 491.8632\n",
      "Training Epoch: 18 [6848/36450]\tLoss: 481.5106\n",
      "Training Epoch: 18 [6912/36450]\tLoss: 507.9745\n",
      "Training Epoch: 18 [6976/36450]\tLoss: 472.0269\n",
      "Training Epoch: 18 [7040/36450]\tLoss: 503.2493\n",
      "Training Epoch: 18 [7104/36450]\tLoss: 500.3994\n",
      "Training Epoch: 18 [7168/36450]\tLoss: 460.9234\n",
      "Training Epoch: 18 [7232/36450]\tLoss: 503.9034\n",
      "Training Epoch: 18 [7296/36450]\tLoss: 499.4991\n",
      "Training Epoch: 18 [7360/36450]\tLoss: 494.3451\n",
      "Training Epoch: 18 [7424/36450]\tLoss: 476.6114\n",
      "Training Epoch: 18 [7488/36450]\tLoss: 509.1056\n",
      "Training Epoch: 18 [7552/36450]\tLoss: 504.4158\n",
      "Training Epoch: 18 [7616/36450]\tLoss: 527.5710\n",
      "Training Epoch: 18 [7680/36450]\tLoss: 520.5408\n",
      "Training Epoch: 18 [7744/36450]\tLoss: 514.7557\n",
      "Training Epoch: 18 [7808/36450]\tLoss: 492.9660\n",
      "Training Epoch: 18 [7872/36450]\tLoss: 495.1213\n",
      "Training Epoch: 18 [7936/36450]\tLoss: 471.7702\n",
      "Training Epoch: 18 [8000/36450]\tLoss: 522.2423\n",
      "Training Epoch: 18 [8064/36450]\tLoss: 486.8391\n",
      "Training Epoch: 18 [8128/36450]\tLoss: 483.1133\n",
      "Training Epoch: 18 [8192/36450]\tLoss: 516.3857\n",
      "Training Epoch: 18 [8256/36450]\tLoss: 547.6913\n",
      "Training Epoch: 18 [8320/36450]\tLoss: 538.2690\n",
      "Training Epoch: 18 [8384/36450]\tLoss: 593.3314\n",
      "Training Epoch: 18 [8448/36450]\tLoss: 599.0085\n",
      "Training Epoch: 18 [8512/36450]\tLoss: 598.6169\n",
      "Training Epoch: 18 [8576/36450]\tLoss: 574.9575\n",
      "Training Epoch: 18 [8640/36450]\tLoss: 519.1050\n",
      "Training Epoch: 18 [8704/36450]\tLoss: 498.3520\n",
      "Training Epoch: 18 [8768/36450]\tLoss: 483.4807\n",
      "Training Epoch: 18 [8832/36450]\tLoss: 520.9397\n",
      "Training Epoch: 18 [8896/36450]\tLoss: 538.8728\n",
      "Training Epoch: 18 [8960/36450]\tLoss: 525.2299\n",
      "Training Epoch: 18 [9024/36450]\tLoss: 497.9457\n",
      "Training Epoch: 18 [9088/36450]\tLoss: 510.9173\n",
      "Training Epoch: 18 [9152/36450]\tLoss: 505.2724\n",
      "Training Epoch: 18 [9216/36450]\tLoss: 546.8715\n",
      "Training Epoch: 18 [9280/36450]\tLoss: 502.6104\n",
      "Training Epoch: 18 [9344/36450]\tLoss: 506.0695\n",
      "Training Epoch: 18 [9408/36450]\tLoss: 510.5389\n",
      "Training Epoch: 18 [9472/36450]\tLoss: 487.7845\n",
      "Training Epoch: 18 [9536/36450]\tLoss: 518.6312\n",
      "Training Epoch: 18 [9600/36450]\tLoss: 519.7024\n",
      "Training Epoch: 18 [9664/36450]\tLoss: 462.0724\n",
      "Training Epoch: 18 [9728/36450]\tLoss: 473.9648\n",
      "Training Epoch: 18 [9792/36450]\tLoss: 474.4602\n",
      "Training Epoch: 18 [9856/36450]\tLoss: 493.5271\n",
      "Training Epoch: 18 [9920/36450]\tLoss: 483.8015\n",
      "Training Epoch: 18 [9984/36450]\tLoss: 506.5772\n",
      "Training Epoch: 18 [10048/36450]\tLoss: 487.3660\n",
      "Training Epoch: 18 [10112/36450]\tLoss: 485.8008\n",
      "Training Epoch: 18 [10176/36450]\tLoss: 485.0898\n",
      "Training Epoch: 18 [10240/36450]\tLoss: 499.3641\n",
      "Training Epoch: 18 [10304/36450]\tLoss: 475.7092\n",
      "Training Epoch: 18 [10368/36450]\tLoss: 486.8286\n",
      "Training Epoch: 18 [10432/36450]\tLoss: 500.7944\n",
      "Training Epoch: 18 [10496/36450]\tLoss: 467.8398\n",
      "Training Epoch: 18 [10560/36450]\tLoss: 500.5780\n",
      "Training Epoch: 18 [10624/36450]\tLoss: 500.8700\n",
      "Training Epoch: 18 [10688/36450]\tLoss: 502.2141\n",
      "Training Epoch: 18 [10752/36450]\tLoss: 475.9167\n",
      "Training Epoch: 18 [10816/36450]\tLoss: 512.2982\n",
      "Training Epoch: 18 [10880/36450]\tLoss: 505.9437\n",
      "Training Epoch: 18 [10944/36450]\tLoss: 496.8778\n",
      "Training Epoch: 18 [11008/36450]\tLoss: 497.3387\n",
      "Training Epoch: 18 [11072/36450]\tLoss: 483.9714\n",
      "Training Epoch: 18 [11136/36450]\tLoss: 485.4772\n",
      "Training Epoch: 18 [11200/36450]\tLoss: 470.4352\n",
      "Training Epoch: 18 [11264/36450]\tLoss: 496.7668\n",
      "Training Epoch: 18 [11328/36450]\tLoss: 473.6932\n",
      "Training Epoch: 18 [11392/36450]\tLoss: 498.3628\n",
      "Training Epoch: 18 [11456/36450]\tLoss: 493.3991\n",
      "Training Epoch: 18 [11520/36450]\tLoss: 474.1508\n",
      "Training Epoch: 18 [11584/36450]\tLoss: 489.5018\n",
      "Training Epoch: 18 [11648/36450]\tLoss: 478.6692\n",
      "Training Epoch: 18 [11712/36450]\tLoss: 470.3347\n",
      "Training Epoch: 18 [11776/36450]\tLoss: 505.2855\n",
      "Training Epoch: 18 [11840/36450]\tLoss: 479.1400\n",
      "Training Epoch: 18 [11904/36450]\tLoss: 492.5969\n",
      "Training Epoch: 18 [11968/36450]\tLoss: 489.4318\n",
      "Training Epoch: 18 [12032/36450]\tLoss: 477.6532\n",
      "Training Epoch: 18 [12096/36450]\tLoss: 484.7344\n",
      "Training Epoch: 18 [12160/36450]\tLoss: 471.6185\n",
      "Training Epoch: 18 [12224/36450]\tLoss: 494.8246\n",
      "Training Epoch: 18 [12288/36450]\tLoss: 487.4808\n",
      "Training Epoch: 18 [12352/36450]\tLoss: 490.0238\n",
      "Training Epoch: 18 [12416/36450]\tLoss: 512.0425\n",
      "Training Epoch: 18 [12480/36450]\tLoss: 502.4876\n",
      "Training Epoch: 18 [12544/36450]\tLoss: 476.5207\n",
      "Training Epoch: 18 [12608/36450]\tLoss: 470.1163\n",
      "Training Epoch: 18 [12672/36450]\tLoss: 475.0482\n",
      "Training Epoch: 18 [12736/36450]\tLoss: 494.1346\n",
      "Training Epoch: 18 [12800/36450]\tLoss: 494.7570\n",
      "Training Epoch: 18 [12864/36450]\tLoss: 453.9747\n",
      "Training Epoch: 18 [12928/36450]\tLoss: 490.6244\n",
      "Training Epoch: 18 [12992/36450]\tLoss: 480.2614\n",
      "Training Epoch: 18 [13056/36450]\tLoss: 464.0660\n",
      "Training Epoch: 18 [13120/36450]\tLoss: 502.5458\n",
      "Training Epoch: 18 [13184/36450]\tLoss: 499.0327\n",
      "Training Epoch: 18 [13248/36450]\tLoss: 504.4008\n",
      "Training Epoch: 18 [13312/36450]\tLoss: 481.3840\n",
      "Training Epoch: 18 [13376/36450]\tLoss: 500.1013\n",
      "Training Epoch: 18 [13440/36450]\tLoss: 485.3395\n",
      "Training Epoch: 18 [13504/36450]\tLoss: 463.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [13568/36450]\tLoss: 505.8291\n",
      "Training Epoch: 18 [13632/36450]\tLoss: 464.6426\n",
      "Training Epoch: 18 [13696/36450]\tLoss: 466.1700\n",
      "Training Epoch: 18 [13760/36450]\tLoss: 482.1084\n",
      "Training Epoch: 18 [13824/36450]\tLoss: 478.0772\n",
      "Training Epoch: 18 [13888/36450]\tLoss: 458.7970\n",
      "Training Epoch: 18 [13952/36450]\tLoss: 501.9998\n",
      "Training Epoch: 18 [14016/36450]\tLoss: 478.6293\n",
      "Training Epoch: 18 [14080/36450]\tLoss: 498.0948\n",
      "Training Epoch: 18 [14144/36450]\tLoss: 476.2305\n",
      "Training Epoch: 18 [14208/36450]\tLoss: 527.1609\n",
      "Training Epoch: 18 [14272/36450]\tLoss: 467.1417\n",
      "Training Epoch: 18 [14336/36450]\tLoss: 491.8588\n",
      "Training Epoch: 18 [14400/36450]\tLoss: 490.7391\n",
      "Training Epoch: 18 [14464/36450]\tLoss: 484.7695\n",
      "Training Epoch: 18 [14528/36450]\tLoss: 481.3848\n",
      "Training Epoch: 18 [14592/36450]\tLoss: 486.8366\n",
      "Training Epoch: 18 [14656/36450]\tLoss: 503.9446\n",
      "Training Epoch: 18 [14720/36450]\tLoss: 467.8914\n",
      "Training Epoch: 18 [14784/36450]\tLoss: 481.9881\n",
      "Training Epoch: 18 [14848/36450]\tLoss: 493.8615\n",
      "Training Epoch: 18 [14912/36450]\tLoss: 531.2666\n",
      "Training Epoch: 18 [14976/36450]\tLoss: 469.4726\n",
      "Training Epoch: 18 [15040/36450]\tLoss: 490.2890\n",
      "Training Epoch: 18 [15104/36450]\tLoss: 465.8033\n",
      "Training Epoch: 18 [15168/36450]\tLoss: 477.9816\n",
      "Training Epoch: 18 [15232/36450]\tLoss: 482.5927\n",
      "Training Epoch: 18 [15296/36450]\tLoss: 489.2776\n",
      "Training Epoch: 18 [15360/36450]\tLoss: 493.7212\n",
      "Training Epoch: 18 [15424/36450]\tLoss: 496.5165\n",
      "Training Epoch: 18 [15488/36450]\tLoss: 507.2571\n",
      "Training Epoch: 18 [15552/36450]\tLoss: 515.4266\n",
      "Training Epoch: 18 [15616/36450]\tLoss: 486.7448\n",
      "Training Epoch: 18 [15680/36450]\tLoss: 466.1041\n",
      "Training Epoch: 18 [15744/36450]\tLoss: 487.7747\n",
      "Training Epoch: 18 [15808/36450]\tLoss: 486.8234\n",
      "Training Epoch: 18 [15872/36450]\tLoss: 481.1884\n",
      "Training Epoch: 18 [15936/36450]\tLoss: 480.5318\n",
      "Training Epoch: 18 [16000/36450]\tLoss: 476.2016\n",
      "Training Epoch: 18 [16064/36450]\tLoss: 470.0704\n",
      "Training Epoch: 18 [16128/36450]\tLoss: 477.7117\n",
      "Training Epoch: 18 [16192/36450]\tLoss: 483.4252\n",
      "Training Epoch: 18 [16256/36450]\tLoss: 498.9359\n",
      "Training Epoch: 18 [16320/36450]\tLoss: 468.1830\n",
      "Training Epoch: 18 [16384/36450]\tLoss: 492.5358\n",
      "Training Epoch: 18 [16448/36450]\tLoss: 519.9337\n",
      "Training Epoch: 18 [16512/36450]\tLoss: 510.8950\n",
      "Training Epoch: 18 [16576/36450]\tLoss: 462.6020\n",
      "Training Epoch: 18 [16640/36450]\tLoss: 492.0660\n",
      "Training Epoch: 18 [16704/36450]\tLoss: 489.3983\n",
      "Training Epoch: 18 [16768/36450]\tLoss: 511.6651\n",
      "Training Epoch: 18 [16832/36450]\tLoss: 479.9779\n",
      "Training Epoch: 18 [16896/36450]\tLoss: 482.2907\n",
      "Training Epoch: 18 [16960/36450]\tLoss: 488.5015\n",
      "Training Epoch: 18 [17024/36450]\tLoss: 487.2299\n",
      "Training Epoch: 18 [17088/36450]\tLoss: 496.8663\n",
      "Training Epoch: 18 [17152/36450]\tLoss: 482.9966\n",
      "Training Epoch: 18 [17216/36450]\tLoss: 471.6680\n",
      "Training Epoch: 18 [17280/36450]\tLoss: 493.5475\n",
      "Training Epoch: 18 [17344/36450]\tLoss: 498.4499\n",
      "Training Epoch: 18 [17408/36450]\tLoss: 467.4040\n",
      "Training Epoch: 18 [17472/36450]\tLoss: 493.8912\n",
      "Training Epoch: 18 [17536/36450]\tLoss: 493.9870\n",
      "Training Epoch: 18 [17600/36450]\tLoss: 510.6753\n",
      "Training Epoch: 18 [17664/36450]\tLoss: 438.5544\n",
      "Training Epoch: 18 [17728/36450]\tLoss: 501.6564\n",
      "Training Epoch: 18 [17792/36450]\tLoss: 491.5187\n",
      "Training Epoch: 18 [17856/36450]\tLoss: 478.4340\n",
      "Training Epoch: 18 [17920/36450]\tLoss: 469.6189\n",
      "Training Epoch: 18 [17984/36450]\tLoss: 481.0072\n",
      "Training Epoch: 18 [18048/36450]\tLoss: 461.2863\n",
      "Training Epoch: 18 [18112/36450]\tLoss: 469.4552\n",
      "Training Epoch: 18 [18176/36450]\tLoss: 469.9681\n",
      "Training Epoch: 18 [18240/36450]\tLoss: 484.7101\n",
      "Training Epoch: 18 [18304/36450]\tLoss: 507.1344\n",
      "Training Epoch: 18 [18368/36450]\tLoss: 481.4893\n",
      "Training Epoch: 18 [18432/36450]\tLoss: 487.3062\n",
      "Training Epoch: 18 [18496/36450]\tLoss: 493.9258\n",
      "Training Epoch: 18 [18560/36450]\tLoss: 490.6022\n",
      "Training Epoch: 18 [18624/36450]\tLoss: 466.6636\n",
      "Training Epoch: 18 [18688/36450]\tLoss: 493.4840\n",
      "Training Epoch: 18 [18752/36450]\tLoss: 469.4926\n",
      "Training Epoch: 18 [18816/36450]\tLoss: 483.4818\n",
      "Training Epoch: 18 [18880/36450]\tLoss: 480.6078\n",
      "Training Epoch: 18 [18944/36450]\tLoss: 487.8321\n",
      "Training Epoch: 18 [19008/36450]\tLoss: 498.8985\n",
      "Training Epoch: 18 [19072/36450]\tLoss: 451.4974\n",
      "Training Epoch: 18 [19136/36450]\tLoss: 465.2765\n",
      "Training Epoch: 18 [19200/36450]\tLoss: 491.2656\n",
      "Training Epoch: 18 [19264/36450]\tLoss: 471.7285\n",
      "Training Epoch: 18 [19328/36450]\tLoss: 496.2238\n",
      "Training Epoch: 18 [19392/36450]\tLoss: 500.7174\n",
      "Training Epoch: 18 [19456/36450]\tLoss: 496.0932\n",
      "Training Epoch: 18 [19520/36450]\tLoss: 491.4243\n",
      "Training Epoch: 18 [19584/36450]\tLoss: 468.9792\n",
      "Training Epoch: 18 [19648/36450]\tLoss: 471.2296\n",
      "Training Epoch: 18 [19712/36450]\tLoss: 466.1365\n",
      "Training Epoch: 18 [19776/36450]\tLoss: 486.2541\n",
      "Training Epoch: 18 [19840/36450]\tLoss: 472.9471\n",
      "Training Epoch: 18 [19904/36450]\tLoss: 500.4287\n",
      "Training Epoch: 18 [19968/36450]\tLoss: 484.3416\n",
      "Training Epoch: 18 [20032/36450]\tLoss: 466.0255\n",
      "Training Epoch: 18 [20096/36450]\tLoss: 483.0916\n",
      "Training Epoch: 18 [20160/36450]\tLoss: 465.9047\n",
      "Training Epoch: 18 [20224/36450]\tLoss: 513.3086\n",
      "Training Epoch: 18 [20288/36450]\tLoss: 502.0721\n",
      "Training Epoch: 18 [20352/36450]\tLoss: 468.1198\n",
      "Training Epoch: 18 [20416/36450]\tLoss: 481.8777\n",
      "Training Epoch: 18 [20480/36450]\tLoss: 479.6486\n",
      "Training Epoch: 18 [20544/36450]\tLoss: 492.0041\n",
      "Training Epoch: 18 [20608/36450]\tLoss: 460.5146\n",
      "Training Epoch: 18 [20672/36450]\tLoss: 499.5158\n",
      "Training Epoch: 18 [20736/36450]\tLoss: 489.1954\n",
      "Training Epoch: 18 [20800/36450]\tLoss: 492.5827\n",
      "Training Epoch: 18 [20864/36450]\tLoss: 488.4056\n",
      "Training Epoch: 18 [20928/36450]\tLoss: 484.6193\n",
      "Training Epoch: 18 [20992/36450]\tLoss: 484.1539\n",
      "Training Epoch: 18 [21056/36450]\tLoss: 462.5564\n",
      "Training Epoch: 18 [21120/36450]\tLoss: 472.9467\n",
      "Training Epoch: 18 [21184/36450]\tLoss: 488.5422\n",
      "Training Epoch: 18 [21248/36450]\tLoss: 489.1644\n",
      "Training Epoch: 18 [21312/36450]\tLoss: 500.7989\n",
      "Training Epoch: 18 [21376/36450]\tLoss: 512.7001\n",
      "Training Epoch: 18 [21440/36450]\tLoss: 511.7996\n",
      "Training Epoch: 18 [21504/36450]\tLoss: 462.9785\n",
      "Training Epoch: 18 [21568/36450]\tLoss: 471.0853\n",
      "Training Epoch: 18 [21632/36450]\tLoss: 485.7417\n",
      "Training Epoch: 18 [21696/36450]\tLoss: 502.2355\n",
      "Training Epoch: 18 [21760/36450]\tLoss: 481.0699\n",
      "Training Epoch: 18 [21824/36450]\tLoss: 474.2048\n",
      "Training Epoch: 18 [21888/36450]\tLoss: 479.9840\n",
      "Training Epoch: 18 [21952/36450]\tLoss: 458.6912\n",
      "Training Epoch: 18 [22016/36450]\tLoss: 482.7666\n",
      "Training Epoch: 18 [22080/36450]\tLoss: 486.3382\n",
      "Training Epoch: 18 [22144/36450]\tLoss: 485.2753\n",
      "Training Epoch: 18 [22208/36450]\tLoss: 486.6624\n",
      "Training Epoch: 18 [22272/36450]\tLoss: 502.9021\n",
      "Training Epoch: 18 [22336/36450]\tLoss: 474.1942\n",
      "Training Epoch: 18 [22400/36450]\tLoss: 492.4832\n",
      "Training Epoch: 18 [22464/36450]\tLoss: 496.3224\n",
      "Training Epoch: 18 [22528/36450]\tLoss: 508.6608\n",
      "Training Epoch: 18 [22592/36450]\tLoss: 504.0260\n",
      "Training Epoch: 18 [22656/36450]\tLoss: 506.0341\n",
      "Training Epoch: 18 [22720/36450]\tLoss: 501.5542\n",
      "Training Epoch: 18 [22784/36450]\tLoss: 504.2642\n",
      "Training Epoch: 18 [22848/36450]\tLoss: 498.1110\n",
      "Training Epoch: 18 [22912/36450]\tLoss: 505.2051\n",
      "Training Epoch: 18 [22976/36450]\tLoss: 524.2045\n",
      "Training Epoch: 18 [23040/36450]\tLoss: 487.4878\n",
      "Training Epoch: 18 [23104/36450]\tLoss: 464.8577\n",
      "Training Epoch: 18 [23168/36450]\tLoss: 497.7426\n",
      "Training Epoch: 18 [23232/36450]\tLoss: 490.5068\n",
      "Training Epoch: 18 [23296/36450]\tLoss: 504.3936\n",
      "Training Epoch: 18 [23360/36450]\tLoss: 503.2482\n",
      "Training Epoch: 18 [23424/36450]\tLoss: 489.7054\n",
      "Training Epoch: 18 [23488/36450]\tLoss: 487.5822\n",
      "Training Epoch: 18 [23552/36450]\tLoss: 482.7008\n",
      "Training Epoch: 18 [23616/36450]\tLoss: 495.9373\n",
      "Training Epoch: 18 [23680/36450]\tLoss: 493.2332\n",
      "Training Epoch: 18 [23744/36450]\tLoss: 483.8708\n",
      "Training Epoch: 18 [23808/36450]\tLoss: 477.7751\n",
      "Training Epoch: 18 [23872/36450]\tLoss: 486.6494\n",
      "Training Epoch: 18 [23936/36450]\tLoss: 511.2607\n",
      "Training Epoch: 18 [24000/36450]\tLoss: 486.7462\n",
      "Training Epoch: 18 [24064/36450]\tLoss: 455.9620\n",
      "Training Epoch: 18 [24128/36450]\tLoss: 483.9527\n",
      "Training Epoch: 18 [24192/36450]\tLoss: 485.7043\n",
      "Training Epoch: 18 [24256/36450]\tLoss: 510.4406\n",
      "Training Epoch: 18 [24320/36450]\tLoss: 508.3208\n",
      "Training Epoch: 18 [24384/36450]\tLoss: 490.3858\n",
      "Training Epoch: 18 [24448/36450]\tLoss: 502.0381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [24512/36450]\tLoss: 476.0751\n",
      "Training Epoch: 18 [24576/36450]\tLoss: 484.7101\n",
      "Training Epoch: 18 [24640/36450]\tLoss: 511.9995\n",
      "Training Epoch: 18 [24704/36450]\tLoss: 509.2969\n",
      "Training Epoch: 18 [24768/36450]\tLoss: 521.2061\n",
      "Training Epoch: 18 [24832/36450]\tLoss: 500.3554\n",
      "Training Epoch: 18 [24896/36450]\tLoss: 520.6396\n",
      "Training Epoch: 18 [24960/36450]\tLoss: 504.3419\n",
      "Training Epoch: 18 [25024/36450]\tLoss: 481.2584\n",
      "Training Epoch: 18 [25088/36450]\tLoss: 491.3792\n",
      "Training Epoch: 18 [25152/36450]\tLoss: 484.2895\n",
      "Training Epoch: 18 [25216/36450]\tLoss: 490.3264\n",
      "Training Epoch: 18 [25280/36450]\tLoss: 482.6744\n",
      "Training Epoch: 18 [25344/36450]\tLoss: 474.6493\n",
      "Training Epoch: 18 [25408/36450]\tLoss: 501.3815\n",
      "Training Epoch: 18 [25472/36450]\tLoss: 482.4324\n",
      "Training Epoch: 18 [25536/36450]\tLoss: 475.7665\n",
      "Training Epoch: 18 [25600/36450]\tLoss: 531.8249\n",
      "Training Epoch: 18 [25664/36450]\tLoss: 486.7675\n",
      "Training Epoch: 18 [25728/36450]\tLoss: 472.7838\n",
      "Training Epoch: 18 [25792/36450]\tLoss: 467.1766\n",
      "Training Epoch: 18 [25856/36450]\tLoss: 498.6795\n",
      "Training Epoch: 18 [25920/36450]\tLoss: 476.5505\n",
      "Training Epoch: 18 [25984/36450]\tLoss: 504.5882\n",
      "Training Epoch: 18 [26048/36450]\tLoss: 492.0583\n",
      "Training Epoch: 18 [26112/36450]\tLoss: 491.5410\n",
      "Training Epoch: 18 [26176/36450]\tLoss: 465.6060\n",
      "Training Epoch: 18 [26240/36450]\tLoss: 480.7968\n",
      "Training Epoch: 18 [26304/36450]\tLoss: 474.1434\n",
      "Training Epoch: 18 [26368/36450]\tLoss: 514.3179\n",
      "Training Epoch: 18 [26432/36450]\tLoss: 478.3137\n",
      "Training Epoch: 18 [26496/36450]\tLoss: 452.1880\n",
      "Training Epoch: 18 [26560/36450]\tLoss: 482.2745\n",
      "Training Epoch: 18 [26624/36450]\tLoss: 468.5319\n",
      "Training Epoch: 18 [26688/36450]\tLoss: 488.4757\n",
      "Training Epoch: 18 [26752/36450]\tLoss: 499.3375\n",
      "Training Epoch: 18 [26816/36450]\tLoss: 494.0341\n",
      "Training Epoch: 18 [26880/36450]\tLoss: 519.1388\n",
      "Training Epoch: 18 [26944/36450]\tLoss: 462.7018\n",
      "Training Epoch: 18 [27008/36450]\tLoss: 515.9789\n",
      "Training Epoch: 18 [27072/36450]\tLoss: 497.5756\n",
      "Training Epoch: 18 [27136/36450]\tLoss: 500.8303\n",
      "Training Epoch: 18 [27200/36450]\tLoss: 522.3323\n",
      "Training Epoch: 18 [27264/36450]\tLoss: 518.6284\n",
      "Training Epoch: 18 [27328/36450]\tLoss: 526.8996\n",
      "Training Epoch: 18 [27392/36450]\tLoss: 504.6238\n",
      "Training Epoch: 18 [27456/36450]\tLoss: 526.3691\n",
      "Training Epoch: 18 [27520/36450]\tLoss: 520.4269\n",
      "Training Epoch: 18 [27584/36450]\tLoss: 528.5832\n",
      "Training Epoch: 18 [27648/36450]\tLoss: 559.3946\n",
      "Training Epoch: 18 [27712/36450]\tLoss: 534.7499\n",
      "Training Epoch: 18 [27776/36450]\tLoss: 551.6992\n",
      "Training Epoch: 18 [27840/36450]\tLoss: 543.7786\n",
      "Training Epoch: 18 [27904/36450]\tLoss: 496.8745\n",
      "Training Epoch: 18 [27968/36450]\tLoss: 492.8233\n",
      "Training Epoch: 18 [28032/36450]\tLoss: 475.2221\n",
      "Training Epoch: 18 [28096/36450]\tLoss: 496.3512\n",
      "Training Epoch: 18 [28160/36450]\tLoss: 498.2296\n",
      "Training Epoch: 18 [28224/36450]\tLoss: 506.5750\n",
      "Training Epoch: 18 [28288/36450]\tLoss: 543.0346\n",
      "Training Epoch: 18 [28352/36450]\tLoss: 525.6472\n",
      "Training Epoch: 18 [28416/36450]\tLoss: 492.2615\n",
      "Training Epoch: 18 [28480/36450]\tLoss: 491.0765\n",
      "Training Epoch: 18 [28544/36450]\tLoss: 463.0147\n",
      "Training Epoch: 18 [28608/36450]\tLoss: 477.3827\n",
      "Training Epoch: 18 [28672/36450]\tLoss: 492.6447\n",
      "Training Epoch: 18 [28736/36450]\tLoss: 493.1531\n",
      "Training Epoch: 18 [28800/36450]\tLoss: 484.1158\n",
      "Training Epoch: 18 [28864/36450]\tLoss: 493.1674\n",
      "Training Epoch: 18 [28928/36450]\tLoss: 453.9852\n",
      "Training Epoch: 18 [28992/36450]\tLoss: 482.3899\n",
      "Training Epoch: 18 [29056/36450]\tLoss: 504.6599\n",
      "Training Epoch: 18 [29120/36450]\tLoss: 485.8727\n",
      "Training Epoch: 18 [29184/36450]\tLoss: 464.4734\n",
      "Training Epoch: 18 [29248/36450]\tLoss: 474.6621\n",
      "Training Epoch: 18 [29312/36450]\tLoss: 479.4901\n",
      "Training Epoch: 18 [29376/36450]\tLoss: 479.3684\n",
      "Training Epoch: 18 [29440/36450]\tLoss: 492.2670\n",
      "Training Epoch: 18 [29504/36450]\tLoss: 493.7312\n",
      "Training Epoch: 18 [29568/36450]\tLoss: 472.9106\n",
      "Training Epoch: 18 [29632/36450]\tLoss: 467.2010\n",
      "Training Epoch: 18 [29696/36450]\tLoss: 478.4227\n",
      "Training Epoch: 18 [29760/36450]\tLoss: 455.3221\n",
      "Training Epoch: 18 [29824/36450]\tLoss: 486.5549\n",
      "Training Epoch: 18 [29888/36450]\tLoss: 461.9712\n",
      "Training Epoch: 18 [29952/36450]\tLoss: 490.9525\n",
      "Training Epoch: 18 [30016/36450]\tLoss: 480.8028\n",
      "Training Epoch: 18 [30080/36450]\tLoss: 461.9101\n",
      "Training Epoch: 18 [30144/36450]\tLoss: 479.2253\n",
      "Training Epoch: 18 [30208/36450]\tLoss: 517.2918\n",
      "Training Epoch: 18 [30272/36450]\tLoss: 497.9781\n",
      "Training Epoch: 18 [30336/36450]\tLoss: 464.9116\n",
      "Training Epoch: 18 [30400/36450]\tLoss: 489.0966\n",
      "Training Epoch: 18 [30464/36450]\tLoss: 501.5122\n",
      "Training Epoch: 18 [30528/36450]\tLoss: 504.2250\n",
      "Training Epoch: 18 [30592/36450]\tLoss: 487.6748\n",
      "Training Epoch: 18 [30656/36450]\tLoss: 483.1561\n",
      "Training Epoch: 18 [30720/36450]\tLoss: 456.4266\n",
      "Training Epoch: 18 [30784/36450]\tLoss: 472.6464\n",
      "Training Epoch: 18 [30848/36450]\tLoss: 508.8535\n",
      "Training Epoch: 18 [30912/36450]\tLoss: 473.4225\n",
      "Training Epoch: 18 [30976/36450]\tLoss: 475.4529\n",
      "Training Epoch: 18 [31040/36450]\tLoss: 497.9690\n",
      "Training Epoch: 18 [31104/36450]\tLoss: 480.2329\n",
      "Training Epoch: 18 [31168/36450]\tLoss: 495.6590\n",
      "Training Epoch: 18 [31232/36450]\tLoss: 470.2215\n",
      "Training Epoch: 18 [31296/36450]\tLoss: 473.4843\n",
      "Training Epoch: 18 [31360/36450]\tLoss: 479.6107\n",
      "Training Epoch: 18 [31424/36450]\tLoss: 505.1460\n",
      "Training Epoch: 18 [31488/36450]\tLoss: 474.2647\n",
      "Training Epoch: 18 [31552/36450]\tLoss: 506.1305\n",
      "Training Epoch: 18 [31616/36450]\tLoss: 490.8950\n",
      "Training Epoch: 18 [31680/36450]\tLoss: 481.8825\n",
      "Training Epoch: 18 [31744/36450]\tLoss: 474.5912\n",
      "Training Epoch: 18 [31808/36450]\tLoss: 467.5852\n",
      "Training Epoch: 18 [31872/36450]\tLoss: 454.7097\n",
      "Training Epoch: 18 [31936/36450]\tLoss: 496.1779\n",
      "Training Epoch: 18 [32000/36450]\tLoss: 471.4902\n",
      "Training Epoch: 18 [32064/36450]\tLoss: 478.9626\n",
      "Training Epoch: 18 [32128/36450]\tLoss: 466.2830\n",
      "Training Epoch: 18 [32192/36450]\tLoss: 485.3424\n",
      "Training Epoch: 18 [32256/36450]\tLoss: 448.7286\n",
      "Training Epoch: 18 [32320/36450]\tLoss: 467.8685\n",
      "Training Epoch: 18 [32384/36450]\tLoss: 500.6036\n",
      "Training Epoch: 18 [32448/36450]\tLoss: 474.8784\n",
      "Training Epoch: 18 [32512/36450]\tLoss: 462.2124\n",
      "Training Epoch: 18 [32576/36450]\tLoss: 501.6234\n",
      "Training Epoch: 18 [32640/36450]\tLoss: 471.2771\n",
      "Training Epoch: 18 [32704/36450]\tLoss: 457.7551\n",
      "Training Epoch: 18 [32768/36450]\tLoss: 506.5307\n",
      "Training Epoch: 18 [32832/36450]\tLoss: 497.3825\n",
      "Training Epoch: 18 [32896/36450]\tLoss: 489.6879\n",
      "Training Epoch: 18 [32960/36450]\tLoss: 481.0472\n",
      "Training Epoch: 18 [33024/36450]\tLoss: 457.0238\n",
      "Training Epoch: 18 [33088/36450]\tLoss: 458.3749\n",
      "Training Epoch: 18 [33152/36450]\tLoss: 480.0045\n",
      "Training Epoch: 18 [33216/36450]\tLoss: 492.7177\n",
      "Training Epoch: 18 [33280/36450]\tLoss: 497.8758\n",
      "Training Epoch: 18 [33344/36450]\tLoss: 507.4877\n",
      "Training Epoch: 18 [33408/36450]\tLoss: 491.0749\n",
      "Training Epoch: 18 [33472/36450]\tLoss: 479.9037\n",
      "Training Epoch: 18 [33536/36450]\tLoss: 480.9088\n",
      "Training Epoch: 18 [33600/36450]\tLoss: 468.6032\n",
      "Training Epoch: 18 [33664/36450]\tLoss: 476.3917\n",
      "Training Epoch: 18 [33728/36450]\tLoss: 502.2569\n",
      "Training Epoch: 18 [33792/36450]\tLoss: 476.3327\n",
      "Training Epoch: 18 [33856/36450]\tLoss: 480.2914\n",
      "Training Epoch: 18 [33920/36450]\tLoss: 490.9055\n",
      "Training Epoch: 18 [33984/36450]\tLoss: 494.8710\n",
      "Training Epoch: 18 [34048/36450]\tLoss: 487.2485\n",
      "Training Epoch: 18 [34112/36450]\tLoss: 492.2261\n",
      "Training Epoch: 18 [34176/36450]\tLoss: 471.8651\n",
      "Training Epoch: 18 [34240/36450]\tLoss: 480.7388\n",
      "Training Epoch: 18 [34304/36450]\tLoss: 456.7625\n",
      "Training Epoch: 18 [34368/36450]\tLoss: 471.3805\n",
      "Training Epoch: 18 [34432/36450]\tLoss: 476.0813\n",
      "Training Epoch: 18 [34496/36450]\tLoss: 477.6302\n",
      "Training Epoch: 18 [34560/36450]\tLoss: 489.7814\n",
      "Training Epoch: 18 [34624/36450]\tLoss: 509.8586\n",
      "Training Epoch: 18 [34688/36450]\tLoss: 497.5374\n",
      "Training Epoch: 18 [34752/36450]\tLoss: 497.5005\n",
      "Training Epoch: 18 [34816/36450]\tLoss: 499.5598\n",
      "Training Epoch: 18 [34880/36450]\tLoss: 486.5099\n",
      "Training Epoch: 18 [34944/36450]\tLoss: 461.5696\n",
      "Training Epoch: 18 [35008/36450]\tLoss: 478.8171\n",
      "Training Epoch: 18 [35072/36450]\tLoss: 452.5071\n",
      "Training Epoch: 18 [35136/36450]\tLoss: 480.5541\n",
      "Training Epoch: 18 [35200/36450]\tLoss: 481.8824\n",
      "Training Epoch: 18 [35264/36450]\tLoss: 487.1874\n",
      "Training Epoch: 18 [35328/36450]\tLoss: 466.1819\n",
      "Training Epoch: 18 [35392/36450]\tLoss: 499.6096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [35456/36450]\tLoss: 526.0122\n",
      "Training Epoch: 18 [35520/36450]\tLoss: 478.7768\n",
      "Training Epoch: 18 [35584/36450]\tLoss: 459.2560\n",
      "Training Epoch: 18 [35648/36450]\tLoss: 467.9255\n",
      "Training Epoch: 18 [35712/36450]\tLoss: 469.7514\n",
      "Training Epoch: 18 [35776/36450]\tLoss: 498.0469\n",
      "Training Epoch: 18 [35840/36450]\tLoss: 488.8001\n",
      "Training Epoch: 18 [35904/36450]\tLoss: 492.2979\n",
      "Training Epoch: 18 [35968/36450]\tLoss: 499.9347\n",
      "Training Epoch: 18 [36032/36450]\tLoss: 488.7867\n",
      "Training Epoch: 18 [36096/36450]\tLoss: 484.4518\n",
      "Training Epoch: 18 [36160/36450]\tLoss: 486.0604\n",
      "Training Epoch: 18 [36224/36450]\tLoss: 497.2939\n",
      "Training Epoch: 18 [36288/36450]\tLoss: 484.3157\n",
      "Training Epoch: 18 [36352/36450]\tLoss: 466.8218\n",
      "Training Epoch: 18 [36416/36450]\tLoss: 465.4357\n",
      "Training Epoch: 18 [36450/36450]\tLoss: 504.9362\n",
      "Training Epoch: 18 [4050/4050]\tLoss: 240.5070\n",
      "Training Epoch: 19 [64/36450]\tLoss: 471.7031\n",
      "Training Epoch: 19 [128/36450]\tLoss: 479.7741\n",
      "Training Epoch: 19 [192/36450]\tLoss: 501.7838\n",
      "Training Epoch: 19 [256/36450]\tLoss: 485.4015\n",
      "Training Epoch: 19 [320/36450]\tLoss: 477.5687\n",
      "Training Epoch: 19 [384/36450]\tLoss: 486.8068\n",
      "Training Epoch: 19 [448/36450]\tLoss: 492.3417\n",
      "Training Epoch: 19 [512/36450]\tLoss: 466.6694\n",
      "Training Epoch: 19 [576/36450]\tLoss: 488.4362\n",
      "Training Epoch: 19 [640/36450]\tLoss: 472.1843\n",
      "Training Epoch: 19 [704/36450]\tLoss: 449.3918\n",
      "Training Epoch: 19 [768/36450]\tLoss: 458.6889\n",
      "Training Epoch: 19 [832/36450]\tLoss: 472.9493\n",
      "Training Epoch: 19 [896/36450]\tLoss: 486.7874\n",
      "Training Epoch: 19 [960/36450]\tLoss: 470.0115\n",
      "Training Epoch: 19 [1024/36450]\tLoss: 471.3673\n",
      "Training Epoch: 19 [1088/36450]\tLoss: 496.3720\n",
      "Training Epoch: 19 [1152/36450]\tLoss: 495.5595\n",
      "Training Epoch: 19 [1216/36450]\tLoss: 485.0818\n",
      "Training Epoch: 19 [1280/36450]\tLoss: 457.1279\n",
      "Training Epoch: 19 [1344/36450]\tLoss: 517.5217\n",
      "Training Epoch: 19 [1408/36450]\tLoss: 447.0995\n",
      "Training Epoch: 19 [1472/36450]\tLoss: 479.3527\n",
      "Training Epoch: 19 [1536/36450]\tLoss: 465.8179\n",
      "Training Epoch: 19 [1600/36450]\tLoss: 470.9272\n",
      "Training Epoch: 19 [1664/36450]\tLoss: 488.6310\n",
      "Training Epoch: 19 [1728/36450]\tLoss: 503.7966\n",
      "Training Epoch: 19 [1792/36450]\tLoss: 508.3997\n",
      "Training Epoch: 19 [1856/36450]\tLoss: 490.1205\n",
      "Training Epoch: 19 [1920/36450]\tLoss: 459.2323\n",
      "Training Epoch: 19 [1984/36450]\tLoss: 498.6693\n",
      "Training Epoch: 19 [2048/36450]\tLoss: 495.3157\n",
      "Training Epoch: 19 [2112/36450]\tLoss: 452.9960\n",
      "Training Epoch: 19 [2176/36450]\tLoss: 474.5706\n",
      "Training Epoch: 19 [2240/36450]\tLoss: 458.3800\n",
      "Training Epoch: 19 [2304/36450]\tLoss: 467.5844\n",
      "Training Epoch: 19 [2368/36450]\tLoss: 495.0103\n",
      "Training Epoch: 19 [2432/36450]\tLoss: 484.3629\n",
      "Training Epoch: 19 [2496/36450]\tLoss: 494.0717\n",
      "Training Epoch: 19 [2560/36450]\tLoss: 492.6479\n",
      "Training Epoch: 19 [2624/36450]\tLoss: 465.4449\n",
      "Training Epoch: 19 [2688/36450]\tLoss: 491.7137\n",
      "Training Epoch: 19 [2752/36450]\tLoss: 492.9317\n",
      "Training Epoch: 19 [2816/36450]\tLoss: 451.5426\n",
      "Training Epoch: 19 [2880/36450]\tLoss: 459.2525\n",
      "Training Epoch: 19 [2944/36450]\tLoss: 492.3137\n",
      "Training Epoch: 19 [3008/36450]\tLoss: 510.1340\n",
      "Training Epoch: 19 [3072/36450]\tLoss: 481.3105\n",
      "Training Epoch: 19 [3136/36450]\tLoss: 479.5609\n",
      "Training Epoch: 19 [3200/36450]\tLoss: 500.1458\n",
      "Training Epoch: 19 [3264/36450]\tLoss: 498.0201\n",
      "Training Epoch: 19 [3328/36450]\tLoss: 477.1245\n",
      "Training Epoch: 19 [3392/36450]\tLoss: 500.6602\n",
      "Training Epoch: 19 [3456/36450]\tLoss: 492.9883\n",
      "Training Epoch: 19 [3520/36450]\tLoss: 538.0104\n",
      "Training Epoch: 19 [3584/36450]\tLoss: 497.5046\n",
      "Training Epoch: 19 [3648/36450]\tLoss: 516.2509\n",
      "Training Epoch: 19 [3712/36450]\tLoss: 486.8411\n",
      "Training Epoch: 19 [3776/36450]\tLoss: 489.2936\n",
      "Training Epoch: 19 [3840/36450]\tLoss: 492.7599\n",
      "Training Epoch: 19 [3904/36450]\tLoss: 471.5485\n",
      "Training Epoch: 19 [3968/36450]\tLoss: 498.7222\n",
      "Training Epoch: 19 [4032/36450]\tLoss: 471.1961\n",
      "Training Epoch: 19 [4096/36450]\tLoss: 488.3916\n",
      "Training Epoch: 19 [4160/36450]\tLoss: 482.1023\n",
      "Training Epoch: 19 [4224/36450]\tLoss: 459.0707\n",
      "Training Epoch: 19 [4288/36450]\tLoss: 474.8031\n",
      "Training Epoch: 19 [4352/36450]\tLoss: 462.2426\n",
      "Training Epoch: 19 [4416/36450]\tLoss: 466.9110\n",
      "Training Epoch: 19 [4480/36450]\tLoss: 477.5668\n",
      "Training Epoch: 19 [4544/36450]\tLoss: 483.9265\n",
      "Training Epoch: 19 [4608/36450]\tLoss: 493.8476\n",
      "Training Epoch: 19 [4672/36450]\tLoss: 470.7735\n",
      "Training Epoch: 19 [4736/36450]\tLoss: 470.0575\n",
      "Training Epoch: 19 [4800/36450]\tLoss: 473.0044\n",
      "Training Epoch: 19 [4864/36450]\tLoss: 472.5115\n",
      "Training Epoch: 19 [4928/36450]\tLoss: 486.8741\n",
      "Training Epoch: 19 [4992/36450]\tLoss: 516.6731\n",
      "Training Epoch: 19 [5056/36450]\tLoss: 489.0327\n",
      "Training Epoch: 19 [5120/36450]\tLoss: 470.2845\n",
      "Training Epoch: 19 [5184/36450]\tLoss: 481.6358\n",
      "Training Epoch: 19 [5248/36450]\tLoss: 478.2234\n",
      "Training Epoch: 19 [5312/36450]\tLoss: 477.9337\n",
      "Training Epoch: 19 [5376/36450]\tLoss: 505.7033\n",
      "Training Epoch: 19 [5440/36450]\tLoss: 530.3752\n",
      "Training Epoch: 19 [5504/36450]\tLoss: 499.8621\n",
      "Training Epoch: 19 [5568/36450]\tLoss: 527.1703\n",
      "Training Epoch: 19 [5632/36450]\tLoss: 543.9494\n",
      "Training Epoch: 19 [5696/36450]\tLoss: 577.5407\n",
      "Training Epoch: 19 [5760/36450]\tLoss: 567.4019\n",
      "Training Epoch: 19 [5824/36450]\tLoss: 629.2589\n",
      "Training Epoch: 19 [5888/36450]\tLoss: 563.8307\n",
      "Training Epoch: 19 [5952/36450]\tLoss: 536.0311\n",
      "Training Epoch: 19 [6016/36450]\tLoss: 506.3488\n",
      "Training Epoch: 19 [6080/36450]\tLoss: 499.0718\n",
      "Training Epoch: 19 [6144/36450]\tLoss: 489.6450\n",
      "Training Epoch: 19 [6208/36450]\tLoss: 518.5895\n",
      "Training Epoch: 19 [6272/36450]\tLoss: 484.4770\n",
      "Training Epoch: 19 [6336/36450]\tLoss: 497.7795\n",
      "Training Epoch: 19 [6400/36450]\tLoss: 505.3638\n",
      "Training Epoch: 19 [6464/36450]\tLoss: 504.9544\n",
      "Training Epoch: 19 [6528/36450]\tLoss: 507.9258\n",
      "Training Epoch: 19 [6592/36450]\tLoss: 506.8597\n",
      "Training Epoch: 19 [6656/36450]\tLoss: 523.9304\n",
      "Training Epoch: 19 [6720/36450]\tLoss: 476.8370\n",
      "Training Epoch: 19 [6784/36450]\tLoss: 507.4971\n",
      "Training Epoch: 19 [6848/36450]\tLoss: 491.7435\n",
      "Training Epoch: 19 [6912/36450]\tLoss: 513.4575\n",
      "Training Epoch: 19 [6976/36450]\tLoss: 486.2491\n",
      "Training Epoch: 19 [7040/36450]\tLoss: 525.9041\n",
      "Training Epoch: 19 [7104/36450]\tLoss: 500.1577\n",
      "Training Epoch: 19 [7168/36450]\tLoss: 479.5166\n",
      "Training Epoch: 19 [7232/36450]\tLoss: 509.7693\n",
      "Training Epoch: 19 [7296/36450]\tLoss: 491.6209\n",
      "Training Epoch: 19 [7360/36450]\tLoss: 498.1024\n",
      "Training Epoch: 19 [7424/36450]\tLoss: 503.6348\n",
      "Training Epoch: 19 [7488/36450]\tLoss: 462.8502\n",
      "Training Epoch: 19 [7552/36450]\tLoss: 473.7486\n",
      "Training Epoch: 19 [7616/36450]\tLoss: 488.0514\n",
      "Training Epoch: 19 [7680/36450]\tLoss: 508.5177\n",
      "Training Epoch: 19 [7744/36450]\tLoss: 482.9978\n",
      "Training Epoch: 19 [7808/36450]\tLoss: 467.9055\n",
      "Training Epoch: 19 [7872/36450]\tLoss: 471.4308\n",
      "Training Epoch: 19 [7936/36450]\tLoss: 499.6462\n",
      "Training Epoch: 19 [8000/36450]\tLoss: 475.1050\n",
      "Training Epoch: 19 [8064/36450]\tLoss: 476.8509\n",
      "Training Epoch: 19 [8128/36450]\tLoss: 463.9939\n",
      "Training Epoch: 19 [8192/36450]\tLoss: 488.7028\n",
      "Training Epoch: 19 [8256/36450]\tLoss: 468.8177\n",
      "Training Epoch: 19 [8320/36450]\tLoss: 511.8237\n",
      "Training Epoch: 19 [8384/36450]\tLoss: 482.8031\n",
      "Training Epoch: 19 [8448/36450]\tLoss: 485.7186\n",
      "Training Epoch: 19 [8512/36450]\tLoss: 483.9415\n",
      "Training Epoch: 19 [8576/36450]\tLoss: 510.0611\n",
      "Training Epoch: 19 [8640/36450]\tLoss: 467.8063\n",
      "Training Epoch: 19 [8704/36450]\tLoss: 485.8382\n",
      "Training Epoch: 19 [8768/36450]\tLoss: 502.8849\n",
      "Training Epoch: 19 [8832/36450]\tLoss: 498.8880\n",
      "Training Epoch: 19 [8896/36450]\tLoss: 498.9686\n",
      "Training Epoch: 19 [8960/36450]\tLoss: 486.1416\n",
      "Training Epoch: 19 [9024/36450]\tLoss: 479.5640\n",
      "Training Epoch: 19 [9088/36450]\tLoss: 485.6278\n",
      "Training Epoch: 19 [9152/36450]\tLoss: 466.3446\n",
      "Training Epoch: 19 [9216/36450]\tLoss: 491.4619\n",
      "Training Epoch: 19 [9280/36450]\tLoss: 467.8300\n",
      "Training Epoch: 19 [9344/36450]\tLoss: 508.3326\n",
      "Training Epoch: 19 [9408/36450]\tLoss: 516.5610\n",
      "Training Epoch: 19 [9472/36450]\tLoss: 479.7240\n",
      "Training Epoch: 19 [9536/36450]\tLoss: 474.8869\n",
      "Training Epoch: 19 [9600/36450]\tLoss: 499.2562\n",
      "Training Epoch: 19 [9664/36450]\tLoss: 489.4266\n",
      "Training Epoch: 19 [9728/36450]\tLoss: 502.5857\n",
      "Training Epoch: 19 [9792/36450]\tLoss: 480.5035\n",
      "Training Epoch: 19 [9856/36450]\tLoss: 479.1761\n",
      "Training Epoch: 19 [9920/36450]\tLoss: 476.1779\n",
      "Training Epoch: 19 [9984/36450]\tLoss: 490.0740\n",
      "Training Epoch: 19 [10048/36450]\tLoss: 519.1676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [10112/36450]\tLoss: 492.0386\n",
      "Training Epoch: 19 [10176/36450]\tLoss: 468.2558\n",
      "Training Epoch: 19 [10240/36450]\tLoss: 500.3817\n",
      "Training Epoch: 19 [10304/36450]\tLoss: 472.3914\n",
      "Training Epoch: 19 [10368/36450]\tLoss: 484.9548\n",
      "Training Epoch: 19 [10432/36450]\tLoss: 477.0177\n",
      "Training Epoch: 19 [10496/36450]\tLoss: 506.7231\n",
      "Training Epoch: 19 [10560/36450]\tLoss: 506.0183\n",
      "Training Epoch: 19 [10624/36450]\tLoss: 473.1716\n",
      "Training Epoch: 19 [10688/36450]\tLoss: 515.4490\n",
      "Training Epoch: 19 [10752/36450]\tLoss: 464.4365\n",
      "Training Epoch: 19 [10816/36450]\tLoss: 441.4460\n",
      "Training Epoch: 19 [10880/36450]\tLoss: 482.3400\n",
      "Training Epoch: 19 [10944/36450]\tLoss: 480.9933\n",
      "Training Epoch: 19 [11008/36450]\tLoss: 465.6674\n",
      "Training Epoch: 19 [11072/36450]\tLoss: 511.7075\n",
      "Training Epoch: 19 [11136/36450]\tLoss: 490.6878\n",
      "Training Epoch: 19 [11200/36450]\tLoss: 466.1418\n",
      "Training Epoch: 19 [11264/36450]\tLoss: 514.5601\n",
      "Training Epoch: 19 [11328/36450]\tLoss: 515.1176\n",
      "Training Epoch: 19 [11392/36450]\tLoss: 476.7216\n",
      "Training Epoch: 19 [11456/36450]\tLoss: 491.0862\n",
      "Training Epoch: 19 [11520/36450]\tLoss: 501.8271\n",
      "Training Epoch: 19 [11584/36450]\tLoss: 511.0166\n",
      "Training Epoch: 19 [11648/36450]\tLoss: 463.8829\n",
      "Training Epoch: 19 [11712/36450]\tLoss: 499.6951\n",
      "Training Epoch: 19 [11776/36450]\tLoss: 438.3199\n",
      "Training Epoch: 19 [11840/36450]\tLoss: 479.7676\n",
      "Training Epoch: 19 [11904/36450]\tLoss: 496.3936\n",
      "Training Epoch: 19 [11968/36450]\tLoss: 467.7632\n",
      "Training Epoch: 19 [12032/36450]\tLoss: 499.2195\n",
      "Training Epoch: 19 [12096/36450]\tLoss: 456.4370\n",
      "Training Epoch: 19 [12160/36450]\tLoss: 453.3212\n",
      "Training Epoch: 19 [12224/36450]\tLoss: 517.4040\n",
      "Training Epoch: 19 [12288/36450]\tLoss: 488.7296\n",
      "Training Epoch: 19 [12352/36450]\tLoss: 483.2891\n",
      "Training Epoch: 19 [12416/36450]\tLoss: 457.7765\n",
      "Training Epoch: 19 [12480/36450]\tLoss: 441.0443\n",
      "Training Epoch: 19 [12544/36450]\tLoss: 488.6602\n",
      "Training Epoch: 19 [12608/36450]\tLoss: 494.1342\n",
      "Training Epoch: 19 [12672/36450]\tLoss: 469.5535\n",
      "Training Epoch: 19 [12736/36450]\tLoss: 479.8360\n",
      "Training Epoch: 19 [12800/36450]\tLoss: 482.8684\n",
      "Training Epoch: 19 [12864/36450]\tLoss: 472.5641\n",
      "Training Epoch: 19 [12928/36450]\tLoss: 481.6664\n",
      "Training Epoch: 19 [12992/36450]\tLoss: 477.3941\n",
      "Training Epoch: 19 [13056/36450]\tLoss: 490.1816\n",
      "Training Epoch: 19 [13120/36450]\tLoss: 491.2417\n",
      "Training Epoch: 19 [13184/36450]\tLoss: 481.0001\n",
      "Training Epoch: 19 [13248/36450]\tLoss: 464.1419\n",
      "Training Epoch: 19 [13312/36450]\tLoss: 487.7173\n",
      "Training Epoch: 19 [13376/36450]\tLoss: 487.7037\n",
      "Training Epoch: 19 [13440/36450]\tLoss: 474.9751\n",
      "Training Epoch: 19 [13504/36450]\tLoss: 478.2690\n",
      "Training Epoch: 19 [13568/36450]\tLoss: 474.1086\n",
      "Training Epoch: 19 [13632/36450]\tLoss: 521.6760\n",
      "Training Epoch: 19 [13696/36450]\tLoss: 481.0204\n",
      "Training Epoch: 19 [13760/36450]\tLoss: 469.2917\n",
      "Training Epoch: 19 [13824/36450]\tLoss: 512.5399\n",
      "Training Epoch: 19 [13888/36450]\tLoss: 463.4887\n",
      "Training Epoch: 19 [13952/36450]\tLoss: 462.2690\n",
      "Training Epoch: 19 [14016/36450]\tLoss: 479.0059\n",
      "Training Epoch: 19 [14080/36450]\tLoss: 490.3409\n",
      "Training Epoch: 19 [14144/36450]\tLoss: 495.8946\n",
      "Training Epoch: 19 [14208/36450]\tLoss: 446.2008\n",
      "Training Epoch: 19 [14272/36450]\tLoss: 481.6174\n",
      "Training Epoch: 19 [14336/36450]\tLoss: 523.3909\n",
      "Training Epoch: 19 [14400/36450]\tLoss: 493.2768\n",
      "Training Epoch: 19 [14464/36450]\tLoss: 500.4391\n",
      "Training Epoch: 19 [14528/36450]\tLoss: 456.6161\n",
      "Training Epoch: 19 [14592/36450]\tLoss: 477.4638\n",
      "Training Epoch: 19 [14656/36450]\tLoss: 501.7809\n",
      "Training Epoch: 19 [14720/36450]\tLoss: 470.8605\n",
      "Training Epoch: 19 [14784/36450]\tLoss: 457.1329\n",
      "Training Epoch: 19 [14848/36450]\tLoss: 462.3964\n",
      "Training Epoch: 19 [14912/36450]\tLoss: 484.7867\n",
      "Training Epoch: 19 [14976/36450]\tLoss: 484.4532\n",
      "Training Epoch: 19 [15040/36450]\tLoss: 474.3646\n",
      "Training Epoch: 19 [15104/36450]\tLoss: 483.9508\n",
      "Training Epoch: 19 [15168/36450]\tLoss: 472.7652\n",
      "Training Epoch: 19 [15232/36450]\tLoss: 514.1632\n",
      "Training Epoch: 19 [15296/36450]\tLoss: 531.5565\n",
      "Training Epoch: 19 [15360/36450]\tLoss: 469.0373\n",
      "Training Epoch: 19 [15424/36450]\tLoss: 466.9859\n",
      "Training Epoch: 19 [15488/36450]\tLoss: 482.4467\n",
      "Training Epoch: 19 [15552/36450]\tLoss: 482.4150\n",
      "Training Epoch: 19 [15616/36450]\tLoss: 454.9885\n",
      "Training Epoch: 19 [15680/36450]\tLoss: 506.2287\n",
      "Training Epoch: 19 [15744/36450]\tLoss: 482.4457\n",
      "Training Epoch: 19 [15808/36450]\tLoss: 470.7313\n",
      "Training Epoch: 19 [15872/36450]\tLoss: 491.4343\n",
      "Training Epoch: 19 [15936/36450]\tLoss: 479.0143\n",
      "Training Epoch: 19 [16000/36450]\tLoss: 502.7696\n",
      "Training Epoch: 19 [16064/36450]\tLoss: 480.2538\n",
      "Training Epoch: 19 [16128/36450]\tLoss: 495.5793\n",
      "Training Epoch: 19 [16192/36450]\tLoss: 492.8888\n",
      "Training Epoch: 19 [16256/36450]\tLoss: 523.6226\n",
      "Training Epoch: 19 [16320/36450]\tLoss: 465.4716\n",
      "Training Epoch: 19 [16384/36450]\tLoss: 505.3640\n",
      "Training Epoch: 19 [16448/36450]\tLoss: 481.7856\n",
      "Training Epoch: 19 [16512/36450]\tLoss: 495.3125\n",
      "Training Epoch: 19 [16576/36450]\tLoss: 471.3909\n",
      "Training Epoch: 19 [16640/36450]\tLoss: 471.2404\n",
      "Training Epoch: 19 [16704/36450]\tLoss: 472.6452\n",
      "Training Epoch: 19 [16768/36450]\tLoss: 479.6032\n",
      "Training Epoch: 19 [16832/36450]\tLoss: 444.5867\n",
      "Training Epoch: 19 [16896/36450]\tLoss: 493.1509\n",
      "Training Epoch: 19 [16960/36450]\tLoss: 479.0021\n",
      "Training Epoch: 19 [17024/36450]\tLoss: 450.6068\n",
      "Training Epoch: 19 [17088/36450]\tLoss: 480.2813\n",
      "Training Epoch: 19 [17152/36450]\tLoss: 495.8701\n",
      "Training Epoch: 19 [17216/36450]\tLoss: 469.8191\n",
      "Training Epoch: 19 [17280/36450]\tLoss: 504.7083\n",
      "Training Epoch: 19 [17344/36450]\tLoss: 488.4088\n",
      "Training Epoch: 19 [17408/36450]\tLoss: 488.1488\n",
      "Training Epoch: 19 [17472/36450]\tLoss: 467.5649\n",
      "Training Epoch: 19 [17536/36450]\tLoss: 470.1903\n",
      "Training Epoch: 19 [17600/36450]\tLoss: 451.8783\n",
      "Training Epoch: 19 [17664/36450]\tLoss: 490.0858\n",
      "Training Epoch: 19 [17728/36450]\tLoss: 458.0875\n",
      "Training Epoch: 19 [17792/36450]\tLoss: 457.9760\n",
      "Training Epoch: 19 [17856/36450]\tLoss: 464.2446\n",
      "Training Epoch: 19 [17920/36450]\tLoss: 468.8335\n",
      "Training Epoch: 19 [17984/36450]\tLoss: 487.7148\n",
      "Training Epoch: 19 [18048/36450]\tLoss: 475.0049\n",
      "Training Epoch: 19 [18112/36450]\tLoss: 468.4967\n",
      "Training Epoch: 19 [18176/36450]\tLoss: 493.3255\n",
      "Training Epoch: 19 [18240/36450]\tLoss: 485.9598\n",
      "Training Epoch: 19 [18304/36450]\tLoss: 469.4637\n",
      "Training Epoch: 19 [18368/36450]\tLoss: 493.8079\n",
      "Training Epoch: 19 [18432/36450]\tLoss: 505.1593\n",
      "Training Epoch: 19 [18496/36450]\tLoss: 494.2138\n",
      "Training Epoch: 19 [18560/36450]\tLoss: 497.9712\n",
      "Training Epoch: 19 [18624/36450]\tLoss: 498.1787\n",
      "Training Epoch: 19 [18688/36450]\tLoss: 481.0912\n",
      "Training Epoch: 19 [18752/36450]\tLoss: 472.2354\n",
      "Training Epoch: 19 [18816/36450]\tLoss: 480.4966\n",
      "Training Epoch: 19 [18880/36450]\tLoss: 470.1472\n",
      "Training Epoch: 19 [18944/36450]\tLoss: 469.4519\n",
      "Training Epoch: 19 [19008/36450]\tLoss: 483.9248\n",
      "Training Epoch: 19 [19072/36450]\tLoss: 488.5664\n",
      "Training Epoch: 19 [19136/36450]\tLoss: 518.2421\n",
      "Training Epoch: 19 [19200/36450]\tLoss: 475.8416\n",
      "Training Epoch: 19 [19264/36450]\tLoss: 475.7365\n",
      "Training Epoch: 19 [19328/36450]\tLoss: 478.5112\n",
      "Training Epoch: 19 [19392/36450]\tLoss: 491.7240\n",
      "Training Epoch: 19 [19456/36450]\tLoss: 519.6298\n",
      "Training Epoch: 19 [19520/36450]\tLoss: 496.4532\n",
      "Training Epoch: 19 [19584/36450]\tLoss: 513.3163\n",
      "Training Epoch: 19 [19648/36450]\tLoss: 537.2805\n",
      "Training Epoch: 19 [19712/36450]\tLoss: 579.3282\n",
      "Training Epoch: 19 [19776/36450]\tLoss: 622.1118\n",
      "Training Epoch: 19 [19840/36450]\tLoss: 643.2968\n",
      "Training Epoch: 19 [19904/36450]\tLoss: 603.1899\n",
      "Training Epoch: 19 [19968/36450]\tLoss: 532.4417\n",
      "Training Epoch: 19 [20032/36450]\tLoss: 500.3633\n",
      "Training Epoch: 19 [20096/36450]\tLoss: 483.7108\n",
      "Training Epoch: 19 [20160/36450]\tLoss: 502.6722\n",
      "Training Epoch: 19 [20224/36450]\tLoss: 545.3371\n",
      "Training Epoch: 19 [20288/36450]\tLoss: 491.5803\n",
      "Training Epoch: 19 [20352/36450]\tLoss: 489.7288\n",
      "Training Epoch: 19 [20416/36450]\tLoss: 503.6424\n",
      "Training Epoch: 19 [20480/36450]\tLoss: 523.6187\n",
      "Training Epoch: 19 [20544/36450]\tLoss: 553.3245\n",
      "Training Epoch: 19 [20608/36450]\tLoss: 542.8546\n",
      "Training Epoch: 19 [20672/36450]\tLoss: 493.2061\n",
      "Training Epoch: 19 [20736/36450]\tLoss: 454.7943\n",
      "Training Epoch: 19 [20800/36450]\tLoss: 510.9262\n",
      "Training Epoch: 19 [20864/36450]\tLoss: 510.6128\n",
      "Training Epoch: 19 [20928/36450]\tLoss: 477.5404\n",
      "Training Epoch: 19 [20992/36450]\tLoss: 535.9346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [21056/36450]\tLoss: 490.5219\n",
      "Training Epoch: 19 [21120/36450]\tLoss: 509.2138\n",
      "Training Epoch: 19 [21184/36450]\tLoss: 466.5018\n",
      "Training Epoch: 19 [21248/36450]\tLoss: 492.9765\n",
      "Training Epoch: 19 [21312/36450]\tLoss: 493.2541\n",
      "Training Epoch: 19 [21376/36450]\tLoss: 470.7733\n",
      "Training Epoch: 19 [21440/36450]\tLoss: 463.6631\n",
      "Training Epoch: 19 [21504/36450]\tLoss: 481.3003\n",
      "Training Epoch: 19 [21568/36450]\tLoss: 496.4631\n",
      "Training Epoch: 19 [21632/36450]\tLoss: 483.9863\n",
      "Training Epoch: 19 [21696/36450]\tLoss: 497.6510\n",
      "Training Epoch: 19 [21760/36450]\tLoss: 481.9457\n",
      "Training Epoch: 19 [21824/36450]\tLoss: 488.2118\n",
      "Training Epoch: 19 [21888/36450]\tLoss: 473.2746\n",
      "Training Epoch: 19 [21952/36450]\tLoss: 484.1134\n",
      "Training Epoch: 19 [22016/36450]\tLoss: 507.4607\n",
      "Training Epoch: 19 [22080/36450]\tLoss: 513.0394\n",
      "Training Epoch: 19 [22144/36450]\tLoss: 461.8699\n",
      "Training Epoch: 19 [22208/36450]\tLoss: 455.9613\n",
      "Training Epoch: 19 [22272/36450]\tLoss: 490.1142\n",
      "Training Epoch: 19 [22336/36450]\tLoss: 474.0168\n",
      "Training Epoch: 19 [22400/36450]\tLoss: 500.1248\n",
      "Training Epoch: 19 [22464/36450]\tLoss: 485.7205\n",
      "Training Epoch: 19 [22528/36450]\tLoss: 466.6243\n",
      "Training Epoch: 19 [22592/36450]\tLoss: 479.0998\n",
      "Training Epoch: 19 [22656/36450]\tLoss: 507.8257\n",
      "Training Epoch: 19 [22720/36450]\tLoss: 502.2099\n",
      "Training Epoch: 19 [22784/36450]\tLoss: 469.4048\n",
      "Training Epoch: 19 [22848/36450]\tLoss: 475.6756\n",
      "Training Epoch: 19 [22912/36450]\tLoss: 479.7317\n",
      "Training Epoch: 19 [22976/36450]\tLoss: 464.7098\n",
      "Training Epoch: 19 [23040/36450]\tLoss: 477.4444\n",
      "Training Epoch: 19 [23104/36450]\tLoss: 482.7245\n",
      "Training Epoch: 19 [23168/36450]\tLoss: 470.8069\n",
      "Training Epoch: 19 [23232/36450]\tLoss: 516.4783\n",
      "Training Epoch: 19 [23296/36450]\tLoss: 501.9629\n",
      "Training Epoch: 19 [23360/36450]\tLoss: 485.9213\n",
      "Training Epoch: 19 [23424/36450]\tLoss: 500.7764\n",
      "Training Epoch: 19 [23488/36450]\tLoss: 484.4719\n",
      "Training Epoch: 19 [23552/36450]\tLoss: 473.3851\n",
      "Training Epoch: 19 [23616/36450]\tLoss: 514.0629\n",
      "Training Epoch: 19 [23680/36450]\tLoss: 489.8504\n",
      "Training Epoch: 19 [23744/36450]\tLoss: 474.0804\n",
      "Training Epoch: 19 [23808/36450]\tLoss: 463.5531\n",
      "Training Epoch: 19 [23872/36450]\tLoss: 470.8093\n",
      "Training Epoch: 19 [23936/36450]\tLoss: 473.7614\n",
      "Training Epoch: 19 [24000/36450]\tLoss: 510.3801\n",
      "Training Epoch: 19 [24064/36450]\tLoss: 470.6173\n",
      "Training Epoch: 19 [24128/36450]\tLoss: 498.4238\n",
      "Training Epoch: 19 [24192/36450]\tLoss: 463.4308\n",
      "Training Epoch: 19 [24256/36450]\tLoss: 464.9306\n",
      "Training Epoch: 19 [24320/36450]\tLoss: 487.6314\n",
      "Training Epoch: 19 [24384/36450]\tLoss: 491.5986\n",
      "Training Epoch: 19 [24448/36450]\tLoss: 479.9976\n",
      "Training Epoch: 19 [24512/36450]\tLoss: 463.4472\n",
      "Training Epoch: 19 [24576/36450]\tLoss: 523.5212\n",
      "Training Epoch: 19 [24640/36450]\tLoss: 487.2484\n",
      "Training Epoch: 19 [24704/36450]\tLoss: 494.3796\n",
      "Training Epoch: 19 [24768/36450]\tLoss: 480.0489\n",
      "Training Epoch: 19 [24832/36450]\tLoss: 469.7717\n",
      "Training Epoch: 19 [24896/36450]\tLoss: 520.9644\n",
      "Training Epoch: 19 [24960/36450]\tLoss: 459.4120\n",
      "Training Epoch: 19 [25024/36450]\tLoss: 477.2205\n",
      "Training Epoch: 19 [25088/36450]\tLoss: 459.9713\n",
      "Training Epoch: 19 [25152/36450]\tLoss: 522.2573\n",
      "Training Epoch: 19 [25216/36450]\tLoss: 451.0823\n",
      "Training Epoch: 19 [25280/36450]\tLoss: 474.5032\n",
      "Training Epoch: 19 [25344/36450]\tLoss: 503.2709\n",
      "Training Epoch: 19 [25408/36450]\tLoss: 491.7635\n",
      "Training Epoch: 19 [25472/36450]\tLoss: 483.9754\n",
      "Training Epoch: 19 [25536/36450]\tLoss: 481.5071\n",
      "Training Epoch: 19 [25600/36450]\tLoss: 508.6357\n",
      "Training Epoch: 19 [25664/36450]\tLoss: 473.7009\n",
      "Training Epoch: 19 [25728/36450]\tLoss: 483.7489\n",
      "Training Epoch: 19 [25792/36450]\tLoss: 465.4635\n",
      "Training Epoch: 19 [25856/36450]\tLoss: 474.9608\n",
      "Training Epoch: 19 [25920/36450]\tLoss: 488.3070\n",
      "Training Epoch: 19 [25984/36450]\tLoss: 496.6445\n",
      "Training Epoch: 19 [26048/36450]\tLoss: 496.4982\n",
      "Training Epoch: 19 [26112/36450]\tLoss: 476.9872\n",
      "Training Epoch: 19 [26176/36450]\tLoss: 473.5581\n",
      "Training Epoch: 19 [26240/36450]\tLoss: 471.0148\n",
      "Training Epoch: 19 [26304/36450]\tLoss: 484.6462\n",
      "Training Epoch: 19 [26368/36450]\tLoss: 512.1092\n",
      "Training Epoch: 19 [26432/36450]\tLoss: 502.0840\n",
      "Training Epoch: 19 [26496/36450]\tLoss: 468.6285\n",
      "Training Epoch: 19 [26560/36450]\tLoss: 486.7138\n",
      "Training Epoch: 19 [26624/36450]\tLoss: 468.3603\n",
      "Training Epoch: 19 [26688/36450]\tLoss: 462.0137\n",
      "Training Epoch: 19 [26752/36450]\tLoss: 494.7673\n",
      "Training Epoch: 19 [26816/36450]\tLoss: 496.1912\n",
      "Training Epoch: 19 [26880/36450]\tLoss: 509.0832\n",
      "Training Epoch: 19 [26944/36450]\tLoss: 491.8094\n",
      "Training Epoch: 19 [27008/36450]\tLoss: 457.0444\n",
      "Training Epoch: 19 [27072/36450]\tLoss: 464.2859\n",
      "Training Epoch: 19 [27136/36450]\tLoss: 496.7216\n",
      "Training Epoch: 19 [27200/36450]\tLoss: 495.9585\n",
      "Training Epoch: 19 [27264/36450]\tLoss: 476.1172\n",
      "Training Epoch: 19 [27328/36450]\tLoss: 494.7360\n",
      "Training Epoch: 19 [27392/36450]\tLoss: 487.2108\n",
      "Training Epoch: 19 [27456/36450]\tLoss: 446.0752\n",
      "Training Epoch: 19 [27520/36450]\tLoss: 487.0010\n",
      "Training Epoch: 19 [27584/36450]\tLoss: 483.3346\n",
      "Training Epoch: 19 [27648/36450]\tLoss: 479.2122\n",
      "Training Epoch: 19 [27712/36450]\tLoss: 517.6669\n",
      "Training Epoch: 19 [27776/36450]\tLoss: 480.5028\n",
      "Training Epoch: 19 [27840/36450]\tLoss: 483.1920\n",
      "Training Epoch: 19 [27904/36450]\tLoss: 497.6852\n",
      "Training Epoch: 19 [27968/36450]\tLoss: 458.4494\n",
      "Training Epoch: 19 [28032/36450]\tLoss: 473.7918\n",
      "Training Epoch: 19 [28096/36450]\tLoss: 468.6931\n",
      "Training Epoch: 19 [28160/36450]\tLoss: 472.3674\n",
      "Training Epoch: 19 [28224/36450]\tLoss: 467.8337\n",
      "Training Epoch: 19 [28288/36450]\tLoss: 485.6467\n",
      "Training Epoch: 19 [28352/36450]\tLoss: 507.2278\n",
      "Training Epoch: 19 [28416/36450]\tLoss: 463.9663\n",
      "Training Epoch: 19 [28480/36450]\tLoss: 488.4973\n",
      "Training Epoch: 19 [28544/36450]\tLoss: 484.3155\n",
      "Training Epoch: 19 [28608/36450]\tLoss: 488.2934\n",
      "Training Epoch: 19 [28672/36450]\tLoss: 475.6282\n",
      "Training Epoch: 19 [28736/36450]\tLoss: 497.0208\n",
      "Training Epoch: 19 [28800/36450]\tLoss: 497.0033\n",
      "Training Epoch: 19 [28864/36450]\tLoss: 505.3779\n",
      "Training Epoch: 19 [28928/36450]\tLoss: 471.6957\n",
      "Training Epoch: 19 [28992/36450]\tLoss: 482.7263\n",
      "Training Epoch: 19 [29056/36450]\tLoss: 474.6631\n",
      "Training Epoch: 19 [29120/36450]\tLoss: 482.4584\n",
      "Training Epoch: 19 [29184/36450]\tLoss: 455.2012\n",
      "Training Epoch: 19 [29248/36450]\tLoss: 484.2959\n",
      "Training Epoch: 19 [29312/36450]\tLoss: 477.7899\n",
      "Training Epoch: 19 [29376/36450]\tLoss: 507.6151\n",
      "Training Epoch: 19 [29440/36450]\tLoss: 467.5418\n",
      "Training Epoch: 19 [29504/36450]\tLoss: 482.0283\n",
      "Training Epoch: 19 [29568/36450]\tLoss: 463.5216\n",
      "Training Epoch: 19 [29632/36450]\tLoss: 495.3114\n",
      "Training Epoch: 19 [29696/36450]\tLoss: 468.1888\n",
      "Training Epoch: 19 [29760/36450]\tLoss: 502.1218\n",
      "Training Epoch: 19 [29824/36450]\tLoss: 445.0367\n",
      "Training Epoch: 19 [29888/36450]\tLoss: 512.9904\n",
      "Training Epoch: 19 [29952/36450]\tLoss: 470.5791\n",
      "Training Epoch: 19 [30016/36450]\tLoss: 492.0636\n",
      "Training Epoch: 19 [30080/36450]\tLoss: 475.7310\n",
      "Training Epoch: 19 [30144/36450]\tLoss: 475.8909\n",
      "Training Epoch: 19 [30208/36450]\tLoss: 489.0603\n",
      "Training Epoch: 19 [30272/36450]\tLoss: 501.0583\n",
      "Training Epoch: 19 [30336/36450]\tLoss: 482.6441\n",
      "Training Epoch: 19 [30400/36450]\tLoss: 462.0652\n",
      "Training Epoch: 19 [30464/36450]\tLoss: 453.4559\n",
      "Training Epoch: 19 [30528/36450]\tLoss: 518.9565\n",
      "Training Epoch: 19 [30592/36450]\tLoss: 489.8565\n",
      "Training Epoch: 19 [30656/36450]\tLoss: 469.2324\n",
      "Training Epoch: 19 [30720/36450]\tLoss: 474.8174\n",
      "Training Epoch: 19 [30784/36450]\tLoss: 496.1405\n",
      "Training Epoch: 19 [30848/36450]\tLoss: 475.7771\n",
      "Training Epoch: 19 [30912/36450]\tLoss: 468.8530\n",
      "Training Epoch: 19 [30976/36450]\tLoss: 489.8450\n",
      "Training Epoch: 19 [31040/36450]\tLoss: 485.4785\n",
      "Training Epoch: 19 [31104/36450]\tLoss: 468.2875\n",
      "Training Epoch: 19 [31168/36450]\tLoss: 464.8449\n",
      "Training Epoch: 19 [31232/36450]\tLoss: 506.0109\n",
      "Training Epoch: 19 [31296/36450]\tLoss: 507.1315\n",
      "Training Epoch: 19 [31360/36450]\tLoss: 442.9352\n",
      "Training Epoch: 19 [31424/36450]\tLoss: 486.7786\n",
      "Training Epoch: 19 [31488/36450]\tLoss: 488.6195\n",
      "Training Epoch: 19 [31552/36450]\tLoss: 512.6395\n",
      "Training Epoch: 19 [31616/36450]\tLoss: 464.5322\n",
      "Training Epoch: 19 [31680/36450]\tLoss: 455.3782\n",
      "Training Epoch: 19 [31744/36450]\tLoss: 492.2396\n",
      "Training Epoch: 19 [31808/36450]\tLoss: 463.0906\n",
      "Training Epoch: 19 [31872/36450]\tLoss: 499.9358\n",
      "Training Epoch: 19 [31936/36450]\tLoss: 463.2751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [32000/36450]\tLoss: 459.8091\n",
      "Training Epoch: 19 [32064/36450]\tLoss: 474.8654\n",
      "Training Epoch: 19 [32128/36450]\tLoss: 490.0107\n",
      "Training Epoch: 19 [32192/36450]\tLoss: 476.0242\n",
      "Training Epoch: 19 [32256/36450]\tLoss: 487.5151\n",
      "Training Epoch: 19 [32320/36450]\tLoss: 484.3176\n",
      "Training Epoch: 19 [32384/36450]\tLoss: 459.9362\n",
      "Training Epoch: 19 [32448/36450]\tLoss: 466.7138\n",
      "Training Epoch: 19 [32512/36450]\tLoss: 478.8326\n",
      "Training Epoch: 19 [32576/36450]\tLoss: 498.1133\n",
      "Training Epoch: 19 [32640/36450]\tLoss: 492.8291\n",
      "Training Epoch: 19 [32704/36450]\tLoss: 495.7122\n",
      "Training Epoch: 19 [32768/36450]\tLoss: 489.6266\n",
      "Training Epoch: 19 [32832/36450]\tLoss: 474.6856\n",
      "Training Epoch: 19 [32896/36450]\tLoss: 494.8031\n",
      "Training Epoch: 19 [32960/36450]\tLoss: 455.4863\n",
      "Training Epoch: 19 [33024/36450]\tLoss: 495.5874\n",
      "Training Epoch: 19 [33088/36450]\tLoss: 483.6913\n",
      "Training Epoch: 19 [33152/36450]\tLoss: 502.7628\n",
      "Training Epoch: 19 [33216/36450]\tLoss: 482.5380\n",
      "Training Epoch: 19 [33280/36450]\tLoss: 446.5899\n",
      "Training Epoch: 19 [33344/36450]\tLoss: 473.4527\n",
      "Training Epoch: 19 [33408/36450]\tLoss: 485.4500\n",
      "Training Epoch: 19 [33472/36450]\tLoss: 492.1201\n",
      "Training Epoch: 19 [33536/36450]\tLoss: 449.3882\n",
      "Training Epoch: 19 [33600/36450]\tLoss: 455.9660\n",
      "Training Epoch: 19 [33664/36450]\tLoss: 479.7739\n",
      "Training Epoch: 19 [33728/36450]\tLoss: 482.9280\n",
      "Training Epoch: 19 [33792/36450]\tLoss: 490.8642\n",
      "Training Epoch: 19 [33856/36450]\tLoss: 476.2733\n",
      "Training Epoch: 19 [33920/36450]\tLoss: 464.2878\n",
      "Training Epoch: 19 [33984/36450]\tLoss: 454.7155\n",
      "Training Epoch: 19 [34048/36450]\tLoss: 467.7746\n",
      "Training Epoch: 19 [34112/36450]\tLoss: 494.5318\n",
      "Training Epoch: 19 [34176/36450]\tLoss: 451.3739\n",
      "Training Epoch: 19 [34240/36450]\tLoss: 457.3647\n",
      "Training Epoch: 19 [34304/36450]\tLoss: 494.6074\n",
      "Training Epoch: 19 [34368/36450]\tLoss: 511.3557\n",
      "Training Epoch: 19 [34432/36450]\tLoss: 479.4973\n",
      "Training Epoch: 19 [34496/36450]\tLoss: 496.2690\n",
      "Training Epoch: 19 [34560/36450]\tLoss: 481.1606\n",
      "Training Epoch: 19 [34624/36450]\tLoss: 468.0433\n",
      "Training Epoch: 19 [34688/36450]\tLoss: 509.5370\n",
      "Training Epoch: 19 [34752/36450]\tLoss: 475.9268\n",
      "Training Epoch: 19 [34816/36450]\tLoss: 470.5769\n",
      "Training Epoch: 19 [34880/36450]\tLoss: 516.9824\n",
      "Training Epoch: 19 [34944/36450]\tLoss: 466.7148\n",
      "Training Epoch: 19 [35008/36450]\tLoss: 497.8293\n",
      "Training Epoch: 19 [35072/36450]\tLoss: 504.5697\n",
      "Training Epoch: 19 [35136/36450]\tLoss: 485.3532\n",
      "Training Epoch: 19 [35200/36450]\tLoss: 487.8447\n",
      "Training Epoch: 19 [35264/36450]\tLoss: 514.8978\n",
      "Training Epoch: 19 [35328/36450]\tLoss: 482.0812\n",
      "Training Epoch: 19 [35392/36450]\tLoss: 502.1824\n",
      "Training Epoch: 19 [35456/36450]\tLoss: 488.4153\n",
      "Training Epoch: 19 [35520/36450]\tLoss: 476.6500\n",
      "Training Epoch: 19 [35584/36450]\tLoss: 482.4167\n",
      "Training Epoch: 19 [35648/36450]\tLoss: 480.5962\n",
      "Training Epoch: 19 [35712/36450]\tLoss: 476.5694\n",
      "Training Epoch: 19 [35776/36450]\tLoss: 490.9097\n",
      "Training Epoch: 19 [35840/36450]\tLoss: 461.7428\n",
      "Training Epoch: 19 [35904/36450]\tLoss: 484.4634\n",
      "Training Epoch: 19 [35968/36450]\tLoss: 493.8450\n",
      "Training Epoch: 19 [36032/36450]\tLoss: 499.6383\n",
      "Training Epoch: 19 [36096/36450]\tLoss: 462.4002\n",
      "Training Epoch: 19 [36160/36450]\tLoss: 492.8751\n",
      "Training Epoch: 19 [36224/36450]\tLoss: 480.8578\n",
      "Training Epoch: 19 [36288/36450]\tLoss: 455.9001\n",
      "Training Epoch: 19 [36352/36450]\tLoss: 497.4184\n",
      "Training Epoch: 19 [36416/36450]\tLoss: 488.2372\n",
      "Training Epoch: 19 [36450/36450]\tLoss: 450.3505\n",
      "Training Epoch: 19 [4050/4050]\tLoss: 243.0089\n",
      "Training Epoch: 20 [64/36450]\tLoss: 532.0327\n",
      "Training Epoch: 20 [128/36450]\tLoss: 448.1808\n",
      "Training Epoch: 20 [192/36450]\tLoss: 463.7184\n",
      "Training Epoch: 20 [256/36450]\tLoss: 475.1244\n",
      "Training Epoch: 20 [320/36450]\tLoss: 464.0195\n",
      "Training Epoch: 20 [384/36450]\tLoss: 465.8126\n",
      "Training Epoch: 20 [448/36450]\tLoss: 482.6072\n",
      "Training Epoch: 20 [512/36450]\tLoss: 506.0992\n",
      "Training Epoch: 20 [576/36450]\tLoss: 476.2737\n",
      "Training Epoch: 20 [640/36450]\tLoss: 483.6746\n",
      "Training Epoch: 20 [704/36450]\tLoss: 480.7477\n",
      "Training Epoch: 20 [768/36450]\tLoss: 497.2101\n",
      "Training Epoch: 20 [832/36450]\tLoss: 507.3718\n",
      "Training Epoch: 20 [896/36450]\tLoss: 487.9918\n",
      "Training Epoch: 20 [960/36450]\tLoss: 460.5497\n",
      "Training Epoch: 20 [1024/36450]\tLoss: 505.8226\n",
      "Training Epoch: 20 [1088/36450]\tLoss: 490.3548\n",
      "Training Epoch: 20 [1152/36450]\tLoss: 492.4615\n",
      "Training Epoch: 20 [1216/36450]\tLoss: 441.9415\n",
      "Training Epoch: 20 [1280/36450]\tLoss: 479.2819\n",
      "Training Epoch: 20 [1344/36450]\tLoss: 492.3342\n",
      "Training Epoch: 20 [1408/36450]\tLoss: 459.2396\n",
      "Training Epoch: 20 [1472/36450]\tLoss: 468.4162\n",
      "Training Epoch: 20 [1536/36450]\tLoss: 475.0634\n",
      "Training Epoch: 20 [1600/36450]\tLoss: 486.0795\n",
      "Training Epoch: 20 [1664/36450]\tLoss: 497.9497\n",
      "Training Epoch: 20 [1728/36450]\tLoss: 490.7495\n",
      "Training Epoch: 20 [1792/36450]\tLoss: 480.4732\n",
      "Training Epoch: 20 [1856/36450]\tLoss: 488.1119\n",
      "Training Epoch: 20 [1920/36450]\tLoss: 504.1976\n",
      "Training Epoch: 20 [1984/36450]\tLoss: 512.2249\n",
      "Training Epoch: 20 [2048/36450]\tLoss: 489.0764\n",
      "Training Epoch: 20 [2112/36450]\tLoss: 501.7103\n",
      "Training Epoch: 20 [2176/36450]\tLoss: 491.5302\n",
      "Training Epoch: 20 [2240/36450]\tLoss: 516.1050\n",
      "Training Epoch: 20 [2304/36450]\tLoss: 514.6750\n",
      "Training Epoch: 20 [2368/36450]\tLoss: 472.5141\n",
      "Training Epoch: 20 [2432/36450]\tLoss: 509.3303\n",
      "Training Epoch: 20 [2496/36450]\tLoss: 470.1988\n",
      "Training Epoch: 20 [2560/36450]\tLoss: 455.7622\n",
      "Training Epoch: 20 [2624/36450]\tLoss: 488.1495\n",
      "Training Epoch: 20 [2688/36450]\tLoss: 440.3746\n",
      "Training Epoch: 20 [2752/36450]\tLoss: 497.0166\n",
      "Training Epoch: 20 [2816/36450]\tLoss: 481.9093\n",
      "Training Epoch: 20 [2880/36450]\tLoss: 473.3347\n",
      "Training Epoch: 20 [2944/36450]\tLoss: 461.6379\n",
      "Training Epoch: 20 [3008/36450]\tLoss: 479.1173\n",
      "Training Epoch: 20 [3072/36450]\tLoss: 496.9138\n",
      "Training Epoch: 20 [3136/36450]\tLoss: 497.1954\n",
      "Training Epoch: 20 [3200/36450]\tLoss: 511.6603\n",
      "Training Epoch: 20 [3264/36450]\tLoss: 476.6437\n",
      "Training Epoch: 20 [3328/36450]\tLoss: 453.4906\n",
      "Training Epoch: 20 [3392/36450]\tLoss: 484.3771\n",
      "Training Epoch: 20 [3456/36450]\tLoss: 486.6678\n",
      "Training Epoch: 20 [3520/36450]\tLoss: 489.2639\n",
      "Training Epoch: 20 [3584/36450]\tLoss: 480.5148\n",
      "Training Epoch: 20 [3648/36450]\tLoss: 465.2976\n",
      "Training Epoch: 20 [3712/36450]\tLoss: 480.7936\n",
      "Training Epoch: 20 [3776/36450]\tLoss: 485.3143\n",
      "Training Epoch: 20 [3840/36450]\tLoss: 477.5195\n",
      "Training Epoch: 20 [3904/36450]\tLoss: 455.8351\n",
      "Training Epoch: 20 [3968/36450]\tLoss: 486.9794\n",
      "Training Epoch: 20 [4032/36450]\tLoss: 493.1821\n",
      "Training Epoch: 20 [4096/36450]\tLoss: 507.0434\n",
      "Training Epoch: 20 [4160/36450]\tLoss: 479.2834\n",
      "Training Epoch: 20 [4224/36450]\tLoss: 475.6137\n",
      "Training Epoch: 20 [4288/36450]\tLoss: 476.4602\n",
      "Training Epoch: 20 [4352/36450]\tLoss: 456.9652\n",
      "Training Epoch: 20 [4416/36450]\tLoss: 479.0311\n",
      "Training Epoch: 20 [4480/36450]\tLoss: 490.3336\n",
      "Training Epoch: 20 [4544/36450]\tLoss: 472.4216\n",
      "Training Epoch: 20 [4608/36450]\tLoss: 477.1373\n",
      "Training Epoch: 20 [4672/36450]\tLoss: 485.6918\n",
      "Training Epoch: 20 [4736/36450]\tLoss: 481.3934\n",
      "Training Epoch: 20 [4800/36450]\tLoss: 487.3180\n",
      "Training Epoch: 20 [4864/36450]\tLoss: 465.3375\n",
      "Training Epoch: 20 [4928/36450]\tLoss: 471.0313\n",
      "Training Epoch: 20 [4992/36450]\tLoss: 464.8950\n",
      "Training Epoch: 20 [5056/36450]\tLoss: 480.7285\n",
      "Training Epoch: 20 [5120/36450]\tLoss: 471.7830\n",
      "Training Epoch: 20 [5184/36450]\tLoss: 489.4229\n",
      "Training Epoch: 20 [5248/36450]\tLoss: 457.0327\n",
      "Training Epoch: 20 [5312/36450]\tLoss: 467.8748\n",
      "Training Epoch: 20 [5376/36450]\tLoss: 467.3336\n",
      "Training Epoch: 20 [5440/36450]\tLoss: 481.8098\n",
      "Training Epoch: 20 [5504/36450]\tLoss: 468.4756\n",
      "Training Epoch: 20 [5568/36450]\tLoss: 473.8167\n",
      "Training Epoch: 20 [5632/36450]\tLoss: 468.9901\n",
      "Training Epoch: 20 [5696/36450]\tLoss: 491.5301\n",
      "Training Epoch: 20 [5760/36450]\tLoss: 469.7618\n",
      "Training Epoch: 20 [5824/36450]\tLoss: 466.0926\n",
      "Training Epoch: 20 [5888/36450]\tLoss: 492.1497\n",
      "Training Epoch: 20 [5952/36450]\tLoss: 481.9064\n",
      "Training Epoch: 20 [6016/36450]\tLoss: 468.1187\n",
      "Training Epoch: 20 [6080/36450]\tLoss: 463.2108\n",
      "Training Epoch: 20 [6144/36450]\tLoss: 488.8565\n",
      "Training Epoch: 20 [6208/36450]\tLoss: 492.1996\n",
      "Training Epoch: 20 [6272/36450]\tLoss: 496.6396\n",
      "Training Epoch: 20 [6336/36450]\tLoss: 484.8960\n",
      "Training Epoch: 20 [6400/36450]\tLoss: 446.1129\n",
      "Training Epoch: 20 [6464/36450]\tLoss: 458.9434\n",
      "Training Epoch: 20 [6528/36450]\tLoss: 495.9067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [6592/36450]\tLoss: 496.4079\n",
      "Training Epoch: 20 [6656/36450]\tLoss: 495.4762\n",
      "Training Epoch: 20 [6720/36450]\tLoss: 465.5015\n",
      "Training Epoch: 20 [6784/36450]\tLoss: 507.8959\n",
      "Training Epoch: 20 [6848/36450]\tLoss: 485.1696\n",
      "Training Epoch: 20 [6912/36450]\tLoss: 465.7502\n",
      "Training Epoch: 20 [6976/36450]\tLoss: 467.1737\n",
      "Training Epoch: 20 [7040/36450]\tLoss: 520.8309\n",
      "Training Epoch: 20 [7104/36450]\tLoss: 478.2328\n",
      "Training Epoch: 20 [7168/36450]\tLoss: 470.4709\n",
      "Training Epoch: 20 [7232/36450]\tLoss: 449.1975\n",
      "Training Epoch: 20 [7296/36450]\tLoss: 499.2498\n",
      "Training Epoch: 20 [7360/36450]\tLoss: 451.9823\n",
      "Training Epoch: 20 [7424/36450]\tLoss: 453.1505\n",
      "Training Epoch: 20 [7488/36450]\tLoss: 488.5554\n",
      "Training Epoch: 20 [7552/36450]\tLoss: 491.0925\n",
      "Training Epoch: 20 [7616/36450]\tLoss: 443.0800\n",
      "Training Epoch: 20 [7680/36450]\tLoss: 488.4024\n",
      "Training Epoch: 20 [7744/36450]\tLoss: 487.9336\n",
      "Training Epoch: 20 [7808/36450]\tLoss: 494.9665\n",
      "Training Epoch: 20 [7872/36450]\tLoss: 480.0995\n",
      "Training Epoch: 20 [7936/36450]\tLoss: 483.8951\n",
      "Training Epoch: 20 [8000/36450]\tLoss: 479.6971\n",
      "Training Epoch: 20 [8064/36450]\tLoss: 505.4402\n",
      "Training Epoch: 20 [8128/36450]\tLoss: 502.2562\n",
      "Training Epoch: 20 [8192/36450]\tLoss: 498.4952\n",
      "Training Epoch: 20 [8256/36450]\tLoss: 491.9272\n",
      "Training Epoch: 20 [8320/36450]\tLoss: 501.2699\n",
      "Training Epoch: 20 [8384/36450]\tLoss: 513.8292\n",
      "Training Epoch: 20 [8448/36450]\tLoss: 535.4666\n",
      "Training Epoch: 20 [8512/36450]\tLoss: 508.0720\n",
      "Training Epoch: 20 [8576/36450]\tLoss: 508.2697\n",
      "Training Epoch: 20 [8640/36450]\tLoss: 489.9720\n",
      "Training Epoch: 20 [8704/36450]\tLoss: 479.4043\n",
      "Training Epoch: 20 [8768/36450]\tLoss: 476.5814\n",
      "Training Epoch: 20 [8832/36450]\tLoss: 484.8138\n",
      "Training Epoch: 20 [8896/36450]\tLoss: 485.2538\n",
      "Training Epoch: 20 [8960/36450]\tLoss: 487.2085\n",
      "Training Epoch: 20 [9024/36450]\tLoss: 478.2516\n",
      "Training Epoch: 20 [9088/36450]\tLoss: 503.0896\n",
      "Training Epoch: 20 [9152/36450]\tLoss: 468.1043\n",
      "Training Epoch: 20 [9216/36450]\tLoss: 483.4783\n",
      "Training Epoch: 20 [9280/36450]\tLoss: 475.5247\n",
      "Training Epoch: 20 [9344/36450]\tLoss: 481.5497\n",
      "Training Epoch: 20 [9408/36450]\tLoss: 476.0383\n",
      "Training Epoch: 20 [9472/36450]\tLoss: 491.3502\n",
      "Training Epoch: 20 [9536/36450]\tLoss: 467.2527\n",
      "Training Epoch: 20 [9600/36450]\tLoss: 497.9515\n",
      "Training Epoch: 20 [9664/36450]\tLoss: 469.7171\n",
      "Training Epoch: 20 [9728/36450]\tLoss: 484.8120\n",
      "Training Epoch: 20 [9792/36450]\tLoss: 488.3287\n",
      "Training Epoch: 20 [9856/36450]\tLoss: 482.6902\n",
      "Training Epoch: 20 [9920/36450]\tLoss: 475.0685\n",
      "Training Epoch: 20 [9984/36450]\tLoss: 482.0821\n",
      "Training Epoch: 20 [10048/36450]\tLoss: 500.3303\n",
      "Training Epoch: 20 [10112/36450]\tLoss: 498.8538\n",
      "Training Epoch: 20 [10176/36450]\tLoss: 492.9374\n",
      "Training Epoch: 20 [10240/36450]\tLoss: 459.6138\n",
      "Training Epoch: 20 [10304/36450]\tLoss: 502.8651\n",
      "Training Epoch: 20 [10368/36450]\tLoss: 458.8478\n",
      "Training Epoch: 20 [10432/36450]\tLoss: 486.6370\n",
      "Training Epoch: 20 [10496/36450]\tLoss: 485.3209\n",
      "Training Epoch: 20 [10560/36450]\tLoss: 495.8362\n",
      "Training Epoch: 20 [10624/36450]\tLoss: 503.1064\n",
      "Training Epoch: 20 [10688/36450]\tLoss: 491.6736\n",
      "Training Epoch: 20 [10752/36450]\tLoss: 516.5754\n",
      "Training Epoch: 20 [10816/36450]\tLoss: 483.7873\n",
      "Training Epoch: 20 [10880/36450]\tLoss: 478.0077\n",
      "Training Epoch: 20 [10944/36450]\tLoss: 485.0518\n",
      "Training Epoch: 20 [11008/36450]\tLoss: 492.6235\n",
      "Training Epoch: 20 [11072/36450]\tLoss: 476.6861\n",
      "Training Epoch: 20 [11136/36450]\tLoss: 498.1973\n",
      "Training Epoch: 20 [11200/36450]\tLoss: 488.6580\n",
      "Training Epoch: 20 [11264/36450]\tLoss: 497.5849\n",
      "Training Epoch: 20 [11328/36450]\tLoss: 499.1470\n",
      "Training Epoch: 20 [11392/36450]\tLoss: 476.8443\n",
      "Training Epoch: 20 [11456/36450]\tLoss: 476.6534\n",
      "Training Epoch: 20 [11520/36450]\tLoss: 471.9828\n",
      "Training Epoch: 20 [11584/36450]\tLoss: 475.5535\n",
      "Training Epoch: 20 [11648/36450]\tLoss: 479.2442\n",
      "Training Epoch: 20 [11712/36450]\tLoss: 496.4030\n",
      "Training Epoch: 20 [11776/36450]\tLoss: 507.9822\n",
      "Training Epoch: 20 [11840/36450]\tLoss: 493.9561\n",
      "Training Epoch: 20 [11904/36450]\tLoss: 496.6700\n",
      "Training Epoch: 20 [11968/36450]\tLoss: 482.5294\n",
      "Training Epoch: 20 [12032/36450]\tLoss: 492.7614\n",
      "Training Epoch: 20 [12096/36450]\tLoss: 477.3064\n",
      "Training Epoch: 20 [12160/36450]\tLoss: 464.5043\n",
      "Training Epoch: 20 [12224/36450]\tLoss: 505.0599\n",
      "Training Epoch: 20 [12288/36450]\tLoss: 490.1902\n",
      "Training Epoch: 20 [12352/36450]\tLoss: 480.5389\n",
      "Training Epoch: 20 [12416/36450]\tLoss: 492.7247\n",
      "Training Epoch: 20 [12480/36450]\tLoss: 511.2242\n",
      "Training Epoch: 20 [12544/36450]\tLoss: 492.5922\n",
      "Training Epoch: 20 [12608/36450]\tLoss: 474.2771\n",
      "Training Epoch: 20 [12672/36450]\tLoss: 476.9751\n",
      "Training Epoch: 20 [12736/36450]\tLoss: 528.9727\n",
      "Training Epoch: 20 [12800/36450]\tLoss: 492.7570\n",
      "Training Epoch: 20 [12864/36450]\tLoss: 552.0262\n",
      "Training Epoch: 20 [12928/36450]\tLoss: 502.1673\n",
      "Training Epoch: 20 [12992/36450]\tLoss: 536.1733\n",
      "Training Epoch: 20 [13056/36450]\tLoss: 551.6248\n",
      "Training Epoch: 20 [13120/36450]\tLoss: 526.5585\n",
      "Training Epoch: 20 [13184/36450]\tLoss: 511.6973\n",
      "Training Epoch: 20 [13248/36450]\tLoss: 493.6299\n",
      "Training Epoch: 20 [13312/36450]\tLoss: 504.8402\n",
      "Training Epoch: 20 [13376/36450]\tLoss: 469.4004\n",
      "Training Epoch: 20 [13440/36450]\tLoss: 496.0044\n",
      "Training Epoch: 20 [13504/36450]\tLoss: 490.5872\n",
      "Training Epoch: 20 [13568/36450]\tLoss: 522.0736\n",
      "Training Epoch: 20 [13632/36450]\tLoss: 520.7966\n",
      "Training Epoch: 20 [13696/36450]\tLoss: 480.3773\n",
      "Training Epoch: 20 [13760/36450]\tLoss: 509.1878\n",
      "Training Epoch: 20 [13824/36450]\tLoss: 478.5379\n",
      "Training Epoch: 20 [13888/36450]\tLoss: 485.6714\n",
      "Training Epoch: 20 [13952/36450]\tLoss: 467.1673\n",
      "Training Epoch: 20 [14016/36450]\tLoss: 479.1414\n",
      "Training Epoch: 20 [14080/36450]\tLoss: 478.8945\n",
      "Training Epoch: 20 [14144/36450]\tLoss: 518.4761\n",
      "Training Epoch: 20 [14208/36450]\tLoss: 484.4939\n",
      "Training Epoch: 20 [14272/36450]\tLoss: 447.8224\n",
      "Training Epoch: 20 [14336/36450]\tLoss: 503.7567\n",
      "Training Epoch: 20 [14400/36450]\tLoss: 529.3384\n",
      "Training Epoch: 20 [14464/36450]\tLoss: 469.8397\n",
      "Training Epoch: 20 [14528/36450]\tLoss: 504.4592\n",
      "Training Epoch: 20 [14592/36450]\tLoss: 473.2182\n",
      "Training Epoch: 20 [14656/36450]\tLoss: 494.8253\n",
      "Training Epoch: 20 [14720/36450]\tLoss: 494.5097\n",
      "Training Epoch: 20 [14784/36450]\tLoss: 485.6350\n",
      "Training Epoch: 20 [14848/36450]\tLoss: 476.3885\n",
      "Training Epoch: 20 [14912/36450]\tLoss: 490.3778\n",
      "Training Epoch: 20 [14976/36450]\tLoss: 484.7934\n",
      "Training Epoch: 20 [15040/36450]\tLoss: 459.8424\n",
      "Training Epoch: 20 [15104/36450]\tLoss: 457.8312\n",
      "Training Epoch: 20 [15168/36450]\tLoss: 454.1661\n",
      "Training Epoch: 20 [15232/36450]\tLoss: 483.4852\n",
      "Training Epoch: 20 [15296/36450]\tLoss: 472.8757\n",
      "Training Epoch: 20 [15360/36450]\tLoss: 495.8500\n",
      "Training Epoch: 20 [15424/36450]\tLoss: 455.5149\n",
      "Training Epoch: 20 [15488/36450]\tLoss: 496.7070\n",
      "Training Epoch: 20 [15552/36450]\tLoss: 497.5852\n",
      "Training Epoch: 20 [15616/36450]\tLoss: 470.3416\n",
      "Training Epoch: 20 [15680/36450]\tLoss: 511.6514\n",
      "Training Epoch: 20 [15744/36450]\tLoss: 472.3640\n",
      "Training Epoch: 20 [15808/36450]\tLoss: 478.7242\n",
      "Training Epoch: 20 [15872/36450]\tLoss: 456.4223\n",
      "Training Epoch: 20 [15936/36450]\tLoss: 500.7436\n",
      "Training Epoch: 20 [16000/36450]\tLoss: 478.2493\n",
      "Training Epoch: 20 [16064/36450]\tLoss: 488.3149\n",
      "Training Epoch: 20 [16128/36450]\tLoss: 465.3089\n",
      "Training Epoch: 20 [16192/36450]\tLoss: 474.8345\n",
      "Training Epoch: 20 [16256/36450]\tLoss: 504.8093\n",
      "Training Epoch: 20 [16320/36450]\tLoss: 481.3008\n",
      "Training Epoch: 20 [16384/36450]\tLoss: 462.7040\n",
      "Training Epoch: 20 [16448/36450]\tLoss: 478.6894\n",
      "Training Epoch: 20 [16512/36450]\tLoss: 491.8671\n",
      "Training Epoch: 20 [16576/36450]\tLoss: 490.4680\n",
      "Training Epoch: 20 [16640/36450]\tLoss: 449.4980\n",
      "Training Epoch: 20 [16704/36450]\tLoss: 454.8461\n",
      "Training Epoch: 20 [16768/36450]\tLoss: 477.6000\n",
      "Training Epoch: 20 [16832/36450]\tLoss: 513.5884\n",
      "Training Epoch: 20 [16896/36450]\tLoss: 482.6088\n",
      "Training Epoch: 20 [16960/36450]\tLoss: 449.8503\n",
      "Training Epoch: 20 [17024/36450]\tLoss: 495.2175\n",
      "Training Epoch: 20 [17088/36450]\tLoss: 465.5524\n",
      "Training Epoch: 20 [17152/36450]\tLoss: 499.6528\n",
      "Training Epoch: 20 [17216/36450]\tLoss: 482.8846\n",
      "Training Epoch: 20 [17280/36450]\tLoss: 488.8945\n",
      "Training Epoch: 20 [17344/36450]\tLoss: 457.0669\n",
      "Training Epoch: 20 [17408/36450]\tLoss: 475.1134\n",
      "Training Epoch: 20 [17472/36450]\tLoss: 454.6484\n",
      "Training Epoch: 20 [17536/36450]\tLoss: 494.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [17600/36450]\tLoss: 508.5437\n",
      "Training Epoch: 20 [17664/36450]\tLoss: 464.8615\n",
      "Training Epoch: 20 [17728/36450]\tLoss: 499.3744\n",
      "Training Epoch: 20 [17792/36450]\tLoss: 468.4121\n",
      "Training Epoch: 20 [17856/36450]\tLoss: 516.9742\n",
      "Training Epoch: 20 [17920/36450]\tLoss: 448.9382\n",
      "Training Epoch: 20 [17984/36450]\tLoss: 459.2543\n",
      "Training Epoch: 20 [18048/36450]\tLoss: 486.8914\n",
      "Training Epoch: 20 [18112/36450]\tLoss: 502.4834\n",
      "Training Epoch: 20 [18176/36450]\tLoss: 453.4332\n",
      "Training Epoch: 20 [18240/36450]\tLoss: 477.8468\n",
      "Training Epoch: 20 [18304/36450]\tLoss: 478.8470\n",
      "Training Epoch: 20 [18368/36450]\tLoss: 498.2411\n",
      "Training Epoch: 20 [18432/36450]\tLoss: 472.4548\n",
      "Training Epoch: 20 [18496/36450]\tLoss: 465.8032\n",
      "Training Epoch: 20 [18560/36450]\tLoss: 498.4285\n",
      "Training Epoch: 20 [18624/36450]\tLoss: 472.3718\n",
      "Training Epoch: 20 [18688/36450]\tLoss: 471.0480\n",
      "Training Epoch: 20 [18752/36450]\tLoss: 499.2217\n",
      "Training Epoch: 20 [18816/36450]\tLoss: 472.2079\n",
      "Training Epoch: 20 [18880/36450]\tLoss: 488.3824\n",
      "Training Epoch: 20 [18944/36450]\tLoss: 498.5796\n",
      "Training Epoch: 20 [19008/36450]\tLoss: 485.5621\n",
      "Training Epoch: 20 [19072/36450]\tLoss: 499.1355\n",
      "Training Epoch: 20 [19136/36450]\tLoss: 482.3110\n",
      "Training Epoch: 20 [19200/36450]\tLoss: 462.1242\n",
      "Training Epoch: 20 [19264/36450]\tLoss: 481.1557\n",
      "Training Epoch: 20 [19328/36450]\tLoss: 481.0963\n",
      "Training Epoch: 20 [19392/36450]\tLoss: 466.5813\n",
      "Training Epoch: 20 [19456/36450]\tLoss: 520.5060\n",
      "Training Epoch: 20 [19520/36450]\tLoss: 465.8170\n",
      "Training Epoch: 20 [19584/36450]\tLoss: 455.9372\n",
      "Training Epoch: 20 [19648/36450]\tLoss: 484.4586\n",
      "Training Epoch: 20 [19712/36450]\tLoss: 486.2939\n",
      "Training Epoch: 20 [19776/36450]\tLoss: 508.1143\n",
      "Training Epoch: 20 [19840/36450]\tLoss: 499.7880\n",
      "Training Epoch: 20 [19904/36450]\tLoss: 471.8600\n",
      "Training Epoch: 20 [19968/36450]\tLoss: 478.2744\n",
      "Training Epoch: 20 [20032/36450]\tLoss: 487.1280\n",
      "Training Epoch: 20 [20096/36450]\tLoss: 517.8115\n",
      "Training Epoch: 20 [20160/36450]\tLoss: 507.8850\n",
      "Training Epoch: 20 [20224/36450]\tLoss: 469.8130\n",
      "Training Epoch: 20 [20288/36450]\tLoss: 471.1152\n",
      "Training Epoch: 20 [20352/36450]\tLoss: 472.9134\n",
      "Training Epoch: 20 [20416/36450]\tLoss: 473.3483\n",
      "Training Epoch: 20 [20480/36450]\tLoss: 452.2851\n",
      "Training Epoch: 20 [20544/36450]\tLoss: 468.6216\n",
      "Training Epoch: 20 [20608/36450]\tLoss: 468.3780\n",
      "Training Epoch: 20 [20672/36450]\tLoss: 505.6298\n",
      "Training Epoch: 20 [20736/36450]\tLoss: 459.4402\n",
      "Training Epoch: 20 [20800/36450]\tLoss: 473.2105\n",
      "Training Epoch: 20 [20864/36450]\tLoss: 490.5677\n",
      "Training Epoch: 20 [20928/36450]\tLoss: 465.2270\n",
      "Training Epoch: 20 [20992/36450]\tLoss: 485.0536\n",
      "Training Epoch: 20 [21056/36450]\tLoss: 504.6021\n",
      "Training Epoch: 20 [21120/36450]\tLoss: 453.3821\n",
      "Training Epoch: 20 [21184/36450]\tLoss: 466.5856\n",
      "Training Epoch: 20 [21248/36450]\tLoss: 475.8211\n",
      "Training Epoch: 20 [21312/36450]\tLoss: 478.4695\n",
      "Training Epoch: 20 [21376/36450]\tLoss: 458.7142\n",
      "Training Epoch: 20 [21440/36450]\tLoss: 478.0302\n",
      "Training Epoch: 20 [21504/36450]\tLoss: 489.1360\n",
      "Training Epoch: 20 [21568/36450]\tLoss: 481.4752\n",
      "Training Epoch: 20 [21632/36450]\tLoss: 498.0563\n",
      "Training Epoch: 20 [21696/36450]\tLoss: 443.6689\n",
      "Training Epoch: 20 [21760/36450]\tLoss: 469.3094\n",
      "Training Epoch: 20 [21824/36450]\tLoss: 494.5229\n",
      "Training Epoch: 20 [21888/36450]\tLoss: 498.6221\n",
      "Training Epoch: 20 [21952/36450]\tLoss: 501.1010\n",
      "Training Epoch: 20 [22016/36450]\tLoss: 470.6910\n",
      "Training Epoch: 20 [22080/36450]\tLoss: 447.9304\n",
      "Training Epoch: 20 [22144/36450]\tLoss: 491.7160\n",
      "Training Epoch: 20 [22208/36450]\tLoss: 489.9770\n",
      "Training Epoch: 20 [22272/36450]\tLoss: 463.3069\n",
      "Training Epoch: 20 [22336/36450]\tLoss: 483.9716\n",
      "Training Epoch: 20 [22400/36450]\tLoss: 465.8285\n",
      "Training Epoch: 20 [22464/36450]\tLoss: 504.3773\n",
      "Training Epoch: 20 [22528/36450]\tLoss: 472.0277\n",
      "Training Epoch: 20 [22592/36450]\tLoss: 470.0669\n",
      "Training Epoch: 20 [22656/36450]\tLoss: 497.2801\n",
      "Training Epoch: 20 [22720/36450]\tLoss: 493.7179\n",
      "Training Epoch: 20 [22784/36450]\tLoss: 488.1358\n",
      "Training Epoch: 20 [22848/36450]\tLoss: 481.7404\n",
      "Training Epoch: 20 [22912/36450]\tLoss: 491.9719\n",
      "Training Epoch: 20 [22976/36450]\tLoss: 486.9324\n",
      "Training Epoch: 20 [23040/36450]\tLoss: 486.7580\n",
      "Training Epoch: 20 [23104/36450]\tLoss: 502.0375\n",
      "Training Epoch: 20 [23168/36450]\tLoss: 487.5619\n",
      "Training Epoch: 20 [23232/36450]\tLoss: 542.6019\n",
      "Training Epoch: 20 [23296/36450]\tLoss: 507.1944\n",
      "Training Epoch: 20 [23360/36450]\tLoss: 519.5826\n",
      "Training Epoch: 20 [23424/36450]\tLoss: 488.6504\n",
      "Training Epoch: 20 [23488/36450]\tLoss: 521.5896\n",
      "Training Epoch: 20 [23552/36450]\tLoss: 499.5584\n",
      "Training Epoch: 20 [23616/36450]\tLoss: 470.5521\n",
      "Training Epoch: 20 [23680/36450]\tLoss: 454.3569\n",
      "Training Epoch: 20 [23744/36450]\tLoss: 465.4464\n",
      "Training Epoch: 20 [23808/36450]\tLoss: 517.0952\n",
      "Training Epoch: 20 [23872/36450]\tLoss: 483.1282\n",
      "Training Epoch: 20 [23936/36450]\tLoss: 466.4463\n",
      "Training Epoch: 20 [24000/36450]\tLoss: 485.2207\n",
      "Training Epoch: 20 [24064/36450]\tLoss: 475.0790\n",
      "Training Epoch: 20 [24128/36450]\tLoss: 495.7704\n",
      "Training Epoch: 20 [24192/36450]\tLoss: 469.7315\n",
      "Training Epoch: 20 [24256/36450]\tLoss: 483.0627\n",
      "Training Epoch: 20 [24320/36450]\tLoss: 476.0931\n",
      "Training Epoch: 20 [24384/36450]\tLoss: 461.4999\n",
      "Training Epoch: 20 [24448/36450]\tLoss: 486.6675\n",
      "Training Epoch: 20 [24512/36450]\tLoss: 452.9824\n",
      "Training Epoch: 20 [24576/36450]\tLoss: 465.8843\n",
      "Training Epoch: 20 [24640/36450]\tLoss: 493.8558\n",
      "Training Epoch: 20 [24704/36450]\tLoss: 472.6077\n",
      "Training Epoch: 20 [24768/36450]\tLoss: 482.2745\n",
      "Training Epoch: 20 [24832/36450]\tLoss: 459.1102\n",
      "Training Epoch: 20 [24896/36450]\tLoss: 481.0073\n",
      "Training Epoch: 20 [24960/36450]\tLoss: 451.2030\n",
      "Training Epoch: 20 [25024/36450]\tLoss: 476.9175\n",
      "Training Epoch: 20 [25088/36450]\tLoss: 468.8216\n",
      "Training Epoch: 20 [25152/36450]\tLoss: 490.0522\n",
      "Training Epoch: 20 [25216/36450]\tLoss: 493.4272\n",
      "Training Epoch: 20 [25280/36450]\tLoss: 446.0242\n",
      "Training Epoch: 20 [25344/36450]\tLoss: 478.7715\n",
      "Training Epoch: 20 [25408/36450]\tLoss: 468.5184\n",
      "Training Epoch: 20 [25472/36450]\tLoss: 490.8701\n",
      "Training Epoch: 20 [25536/36450]\tLoss: 517.8502\n",
      "Training Epoch: 20 [25600/36450]\tLoss: 489.0841\n",
      "Training Epoch: 20 [25664/36450]\tLoss: 481.7916\n",
      "Training Epoch: 20 [25728/36450]\tLoss: 495.1049\n",
      "Training Epoch: 20 [25792/36450]\tLoss: 472.2346\n",
      "Training Epoch: 20 [25856/36450]\tLoss: 475.6557\n",
      "Training Epoch: 20 [25920/36450]\tLoss: 482.0546\n",
      "Training Epoch: 20 [25984/36450]\tLoss: 484.3213\n",
      "Training Epoch: 20 [26048/36450]\tLoss: 477.7058\n",
      "Training Epoch: 20 [26112/36450]\tLoss: 499.0060\n",
      "Training Epoch: 20 [26176/36450]\tLoss: 459.7525\n",
      "Training Epoch: 20 [26240/36450]\tLoss: 477.3159\n",
      "Training Epoch: 20 [26304/36450]\tLoss: 474.0997\n",
      "Training Epoch: 20 [26368/36450]\tLoss: 456.4435\n",
      "Training Epoch: 20 [26432/36450]\tLoss: 476.1823\n",
      "Training Epoch: 20 [26496/36450]\tLoss: 479.0543\n",
      "Training Epoch: 20 [26560/36450]\tLoss: 485.5834\n",
      "Training Epoch: 20 [26624/36450]\tLoss: 470.3589\n",
      "Training Epoch: 20 [26688/36450]\tLoss: 474.1022\n",
      "Training Epoch: 20 [26752/36450]\tLoss: 490.3624\n",
      "Training Epoch: 20 [26816/36450]\tLoss: 459.2086\n",
      "Training Epoch: 20 [26880/36450]\tLoss: 494.6454\n",
      "Training Epoch: 20 [26944/36450]\tLoss: 485.4154\n",
      "Training Epoch: 20 [27008/36450]\tLoss: 488.0345\n",
      "Training Epoch: 20 [27072/36450]\tLoss: 473.4844\n",
      "Training Epoch: 20 [27136/36450]\tLoss: 503.0857\n",
      "Training Epoch: 20 [27200/36450]\tLoss: 488.4421\n",
      "Training Epoch: 20 [27264/36450]\tLoss: 482.1198\n",
      "Training Epoch: 20 [27328/36450]\tLoss: 468.8376\n",
      "Training Epoch: 20 [27392/36450]\tLoss: 477.3366\n",
      "Training Epoch: 20 [27456/36450]\tLoss: 523.1931\n",
      "Training Epoch: 20 [27520/36450]\tLoss: 509.9682\n",
      "Training Epoch: 20 [27584/36450]\tLoss: 477.1146\n",
      "Training Epoch: 20 [27648/36450]\tLoss: 463.9608\n",
      "Training Epoch: 20 [27712/36450]\tLoss: 490.3351\n",
      "Training Epoch: 20 [27776/36450]\tLoss: 494.8146\n",
      "Training Epoch: 20 [27840/36450]\tLoss: 473.8002\n",
      "Training Epoch: 20 [27904/36450]\tLoss: 470.7938\n",
      "Training Epoch: 20 [27968/36450]\tLoss: 462.0357\n",
      "Training Epoch: 20 [28032/36450]\tLoss: 486.7156\n",
      "Training Epoch: 20 [28096/36450]\tLoss: 472.3096\n",
      "Training Epoch: 20 [28160/36450]\tLoss: 473.3652\n",
      "Training Epoch: 20 [28224/36450]\tLoss: 482.7126\n",
      "Training Epoch: 20 [28288/36450]\tLoss: 472.2281\n",
      "Training Epoch: 20 [28352/36450]\tLoss: 461.1880\n",
      "Training Epoch: 20 [28416/36450]\tLoss: 486.4483\n",
      "Training Epoch: 20 [28480/36450]\tLoss: 478.4361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [28544/36450]\tLoss: 485.8538\n",
      "Training Epoch: 20 [28608/36450]\tLoss: 486.0641\n",
      "Training Epoch: 20 [28672/36450]\tLoss: 470.5573\n",
      "Training Epoch: 20 [28736/36450]\tLoss: 508.7047\n",
      "Training Epoch: 20 [28800/36450]\tLoss: 506.7592\n",
      "Training Epoch: 20 [28864/36450]\tLoss: 474.0175\n",
      "Training Epoch: 20 [28928/36450]\tLoss: 472.5833\n",
      "Training Epoch: 20 [28992/36450]\tLoss: 485.6914\n",
      "Training Epoch: 20 [29056/36450]\tLoss: 458.6581\n",
      "Training Epoch: 20 [29120/36450]\tLoss: 463.9803\n",
      "Training Epoch: 20 [29184/36450]\tLoss: 471.0584\n",
      "Training Epoch: 20 [29248/36450]\tLoss: 449.5945\n",
      "Training Epoch: 20 [29312/36450]\tLoss: 479.8055\n",
      "Training Epoch: 20 [29376/36450]\tLoss: 483.6069\n",
      "Training Epoch: 20 [29440/36450]\tLoss: 492.4164\n",
      "Training Epoch: 20 [29504/36450]\tLoss: 477.0094\n",
      "Training Epoch: 20 [29568/36450]\tLoss: 483.4731\n",
      "Training Epoch: 20 [29632/36450]\tLoss: 453.5063\n",
      "Training Epoch: 20 [29696/36450]\tLoss: 488.0104\n",
      "Training Epoch: 20 [29760/36450]\tLoss: 497.2382\n",
      "Training Epoch: 20 [29824/36450]\tLoss: 484.9063\n",
      "Training Epoch: 20 [29888/36450]\tLoss: 486.4912\n",
      "Training Epoch: 20 [29952/36450]\tLoss: 464.6779\n",
      "Training Epoch: 20 [30016/36450]\tLoss: 481.6432\n",
      "Training Epoch: 20 [30080/36450]\tLoss: 470.2336\n",
      "Training Epoch: 20 [30144/36450]\tLoss: 493.6393\n",
      "Training Epoch: 20 [30208/36450]\tLoss: 480.6303\n",
      "Training Epoch: 20 [30272/36450]\tLoss: 465.9554\n",
      "Training Epoch: 20 [30336/36450]\tLoss: 491.6483\n",
      "Training Epoch: 20 [30400/36450]\tLoss: 461.6208\n",
      "Training Epoch: 20 [30464/36450]\tLoss: 481.8608\n",
      "Training Epoch: 20 [30528/36450]\tLoss: 465.8331\n",
      "Training Epoch: 20 [30592/36450]\tLoss: 494.8165\n",
      "Training Epoch: 20 [30656/36450]\tLoss: 477.8097\n",
      "Training Epoch: 20 [30720/36450]\tLoss: 491.1992\n",
      "Training Epoch: 20 [30784/36450]\tLoss: 481.9845\n",
      "Training Epoch: 20 [30848/36450]\tLoss: 496.2232\n",
      "Training Epoch: 20 [30912/36450]\tLoss: 474.0964\n",
      "Training Epoch: 20 [30976/36450]\tLoss: 473.0034\n",
      "Training Epoch: 20 [31040/36450]\tLoss: 502.4963\n",
      "Training Epoch: 20 [31104/36450]\tLoss: 465.1461\n",
      "Training Epoch: 20 [31168/36450]\tLoss: 456.1036\n",
      "Training Epoch: 20 [31232/36450]\tLoss: 487.5276\n",
      "Training Epoch: 20 [31296/36450]\tLoss: 489.5447\n",
      "Training Epoch: 20 [31360/36450]\tLoss: 470.7521\n",
      "Training Epoch: 20 [31424/36450]\tLoss: 483.4656\n",
      "Training Epoch: 20 [31488/36450]\tLoss: 457.0903\n",
      "Training Epoch: 20 [31552/36450]\tLoss: 479.1115\n",
      "Training Epoch: 20 [31616/36450]\tLoss: 482.1089\n",
      "Training Epoch: 20 [31680/36450]\tLoss: 466.9190\n",
      "Training Epoch: 20 [31744/36450]\tLoss: 447.1044\n",
      "Training Epoch: 20 [31808/36450]\tLoss: 486.4049\n",
      "Training Epoch: 20 [31872/36450]\tLoss: 517.4319\n",
      "Training Epoch: 20 [31936/36450]\tLoss: 481.3732\n",
      "Training Epoch: 20 [32000/36450]\tLoss: 452.0824\n",
      "Training Epoch: 20 [32064/36450]\tLoss: 484.6653\n",
      "Training Epoch: 20 [32128/36450]\tLoss: 483.1858\n",
      "Training Epoch: 20 [32192/36450]\tLoss: 461.8417\n",
      "Training Epoch: 20 [32256/36450]\tLoss: 477.7383\n",
      "Training Epoch: 20 [32320/36450]\tLoss: 456.8488\n",
      "Training Epoch: 20 [32384/36450]\tLoss: 463.6124\n",
      "Training Epoch: 20 [32448/36450]\tLoss: 462.1256\n",
      "Training Epoch: 20 [32512/36450]\tLoss: 439.1781\n",
      "Training Epoch: 20 [32576/36450]\tLoss: 486.7835\n",
      "Training Epoch: 20 [32640/36450]\tLoss: 507.3754\n",
      "Training Epoch: 20 [32704/36450]\tLoss: 505.3948\n",
      "Training Epoch: 20 [32768/36450]\tLoss: 497.1160\n",
      "Training Epoch: 20 [32832/36450]\tLoss: 466.2036\n",
      "Training Epoch: 20 [32896/36450]\tLoss: 475.7728\n",
      "Training Epoch: 20 [32960/36450]\tLoss: 468.5678\n",
      "Training Epoch: 20 [33024/36450]\tLoss: 492.5482\n",
      "Training Epoch: 20 [33088/36450]\tLoss: 494.2513\n",
      "Training Epoch: 20 [33152/36450]\tLoss: 499.5270\n",
      "Training Epoch: 20 [33216/36450]\tLoss: 465.5443\n",
      "Training Epoch: 20 [33280/36450]\tLoss: 476.7887\n",
      "Training Epoch: 20 [33344/36450]\tLoss: 502.2219\n",
      "Training Epoch: 20 [33408/36450]\tLoss: 499.9688\n",
      "Training Epoch: 20 [33472/36450]\tLoss: 485.5537\n",
      "Training Epoch: 20 [33536/36450]\tLoss: 491.2168\n",
      "Training Epoch: 20 [33600/36450]\tLoss: 529.3002\n",
      "Training Epoch: 20 [33664/36450]\tLoss: 506.7403\n",
      "Training Epoch: 20 [33728/36450]\tLoss: 511.6015\n",
      "Training Epoch: 20 [33792/36450]\tLoss: 531.7311\n",
      "Training Epoch: 20 [33856/36450]\tLoss: 499.3524\n",
      "Training Epoch: 20 [33920/36450]\tLoss: 526.7370\n",
      "Training Epoch: 20 [33984/36450]\tLoss: 515.9035\n",
      "Training Epoch: 20 [34048/36450]\tLoss: 515.4986\n",
      "Training Epoch: 20 [34112/36450]\tLoss: 491.9899\n",
      "Training Epoch: 20 [34176/36450]\tLoss: 480.5093\n",
      "Training Epoch: 20 [34240/36450]\tLoss: 480.2028\n",
      "Training Epoch: 20 [34304/36450]\tLoss: 465.7769\n",
      "Training Epoch: 20 [34368/36450]\tLoss: 503.5180\n",
      "Training Epoch: 20 [34432/36450]\tLoss: 513.0042\n",
      "Training Epoch: 20 [34496/36450]\tLoss: 460.7761\n",
      "Training Epoch: 20 [34560/36450]\tLoss: 485.9670\n",
      "Training Epoch: 20 [34624/36450]\tLoss: 484.7725\n",
      "Training Epoch: 20 [34688/36450]\tLoss: 481.0056\n",
      "Training Epoch: 20 [34752/36450]\tLoss: 471.8702\n",
      "Training Epoch: 20 [34816/36450]\tLoss: 497.5885\n",
      "Training Epoch: 20 [34880/36450]\tLoss: 460.3177\n",
      "Training Epoch: 20 [34944/36450]\tLoss: 488.2004\n",
      "Training Epoch: 20 [35008/36450]\tLoss: 512.2070\n",
      "Training Epoch: 20 [35072/36450]\tLoss: 465.7349\n",
      "Training Epoch: 20 [35136/36450]\tLoss: 482.4276\n",
      "Training Epoch: 20 [35200/36450]\tLoss: 473.5704\n",
      "Training Epoch: 20 [35264/36450]\tLoss: 477.8138\n",
      "Training Epoch: 20 [35328/36450]\tLoss: 479.4877\n",
      "Training Epoch: 20 [35392/36450]\tLoss: 456.6902\n",
      "Training Epoch: 20 [35456/36450]\tLoss: 452.2700\n",
      "Training Epoch: 20 [35520/36450]\tLoss: 466.8951\n",
      "Training Epoch: 20 [35584/36450]\tLoss: 462.8704\n",
      "Training Epoch: 20 [35648/36450]\tLoss: 483.1822\n",
      "Training Epoch: 20 [35712/36450]\tLoss: 483.0423\n",
      "Training Epoch: 20 [35776/36450]\tLoss: 452.3037\n",
      "Training Epoch: 20 [35840/36450]\tLoss: 454.2524\n",
      "Training Epoch: 20 [35904/36450]\tLoss: 491.1183\n",
      "Training Epoch: 20 [35968/36450]\tLoss: 507.3073\n",
      "Training Epoch: 20 [36032/36450]\tLoss: 469.0640\n",
      "Training Epoch: 20 [36096/36450]\tLoss: 476.7712\n",
      "Training Epoch: 20 [36160/36450]\tLoss: 475.8997\n",
      "Training Epoch: 20 [36224/36450]\tLoss: 471.6620\n",
      "Training Epoch: 20 [36288/36450]\tLoss: 485.7986\n",
      "Training Epoch: 20 [36352/36450]\tLoss: 480.1144\n",
      "Training Epoch: 20 [36416/36450]\tLoss: 476.1531\n",
      "Training Epoch: 20 [36450/36450]\tLoss: 487.6075\n",
      "Training Epoch: 20 [4050/4050]\tLoss: 235.9893\n",
      "Training Epoch: 21 [64/36450]\tLoss: 459.1056\n",
      "Training Epoch: 21 [128/36450]\tLoss: 488.6345\n",
      "Training Epoch: 21 [192/36450]\tLoss: 487.0754\n",
      "Training Epoch: 21 [256/36450]\tLoss: 467.2157\n",
      "Training Epoch: 21 [320/36450]\tLoss: 484.9053\n",
      "Training Epoch: 21 [384/36450]\tLoss: 480.1384\n",
      "Training Epoch: 21 [448/36450]\tLoss: 447.5245\n",
      "Training Epoch: 21 [512/36450]\tLoss: 502.0034\n",
      "Training Epoch: 21 [576/36450]\tLoss: 471.0931\n",
      "Training Epoch: 21 [640/36450]\tLoss: 493.8945\n",
      "Training Epoch: 21 [704/36450]\tLoss: 475.0272\n",
      "Training Epoch: 21 [768/36450]\tLoss: 502.1191\n",
      "Training Epoch: 21 [832/36450]\tLoss: 467.8708\n",
      "Training Epoch: 21 [896/36450]\tLoss: 480.7012\n",
      "Training Epoch: 21 [960/36450]\tLoss: 470.1455\n",
      "Training Epoch: 21 [1024/36450]\tLoss: 472.0547\n",
      "Training Epoch: 21 [1088/36450]\tLoss: 507.9546\n",
      "Training Epoch: 21 [1152/36450]\tLoss: 496.3756\n",
      "Training Epoch: 21 [1216/36450]\tLoss: 518.3374\n",
      "Training Epoch: 21 [1280/36450]\tLoss: 441.2993\n",
      "Training Epoch: 21 [1344/36450]\tLoss: 464.7513\n",
      "Training Epoch: 21 [1408/36450]\tLoss: 487.9729\n",
      "Training Epoch: 21 [1472/36450]\tLoss: 469.7844\n",
      "Training Epoch: 21 [1536/36450]\tLoss: 468.3680\n",
      "Training Epoch: 21 [1600/36450]\tLoss: 473.4995\n",
      "Training Epoch: 21 [1664/36450]\tLoss: 468.8952\n",
      "Training Epoch: 21 [1728/36450]\tLoss: 493.7738\n",
      "Training Epoch: 21 [1792/36450]\tLoss: 463.6897\n",
      "Training Epoch: 21 [1856/36450]\tLoss: 478.7412\n",
      "Training Epoch: 21 [1920/36450]\tLoss: 488.7860\n",
      "Training Epoch: 21 [1984/36450]\tLoss: 493.8553\n",
      "Training Epoch: 21 [2048/36450]\tLoss: 469.1268\n",
      "Training Epoch: 21 [2112/36450]\tLoss: 466.2278\n",
      "Training Epoch: 21 [2176/36450]\tLoss: 499.4199\n",
      "Training Epoch: 21 [2240/36450]\tLoss: 493.5405\n",
      "Training Epoch: 21 [2304/36450]\tLoss: 503.8871\n",
      "Training Epoch: 21 [2368/36450]\tLoss: 478.7307\n",
      "Training Epoch: 21 [2432/36450]\tLoss: 455.3754\n",
      "Training Epoch: 21 [2496/36450]\tLoss: 461.0616\n",
      "Training Epoch: 21 [2560/36450]\tLoss: 459.6570\n",
      "Training Epoch: 21 [2624/36450]\tLoss: 464.4120\n",
      "Training Epoch: 21 [2688/36450]\tLoss: 458.5302\n",
      "Training Epoch: 21 [2752/36450]\tLoss: 494.9384\n",
      "Training Epoch: 21 [2816/36450]\tLoss: 474.7346\n",
      "Training Epoch: 21 [2880/36450]\tLoss: 477.7354\n",
      "Training Epoch: 21 [2944/36450]\tLoss: 464.7421\n",
      "Training Epoch: 21 [3008/36450]\tLoss: 471.8648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [3072/36450]\tLoss: 492.6668\n",
      "Training Epoch: 21 [3136/36450]\tLoss: 487.7713\n",
      "Training Epoch: 21 [3200/36450]\tLoss: 454.0619\n",
      "Training Epoch: 21 [3264/36450]\tLoss: 477.6951\n",
      "Training Epoch: 21 [3328/36450]\tLoss: 542.0675\n",
      "Training Epoch: 21 [3392/36450]\tLoss: 474.1008\n",
      "Training Epoch: 21 [3456/36450]\tLoss: 482.0456\n",
      "Training Epoch: 21 [3520/36450]\tLoss: 461.3793\n",
      "Training Epoch: 21 [3584/36450]\tLoss: 467.0254\n",
      "Training Epoch: 21 [3648/36450]\tLoss: 474.1366\n",
      "Training Epoch: 21 [3712/36450]\tLoss: 465.6810\n",
      "Training Epoch: 21 [3776/36450]\tLoss: 495.8764\n",
      "Training Epoch: 21 [3840/36450]\tLoss: 495.4139\n",
      "Training Epoch: 21 [3904/36450]\tLoss: 471.0454\n",
      "Training Epoch: 21 [3968/36450]\tLoss: 487.2742\n",
      "Training Epoch: 21 [4032/36450]\tLoss: 488.9745\n",
      "Training Epoch: 21 [4096/36450]\tLoss: 483.8677\n",
      "Training Epoch: 21 [4160/36450]\tLoss: 446.2561\n",
      "Training Epoch: 21 [4224/36450]\tLoss: 464.4409\n",
      "Training Epoch: 21 [4288/36450]\tLoss: 474.5958\n",
      "Training Epoch: 21 [4352/36450]\tLoss: 505.1618\n",
      "Training Epoch: 21 [4416/36450]\tLoss: 495.1306\n",
      "Training Epoch: 21 [4480/36450]\tLoss: 515.3151\n",
      "Training Epoch: 21 [4544/36450]\tLoss: 508.5330\n",
      "Training Epoch: 21 [4608/36450]\tLoss: 492.5817\n",
      "Training Epoch: 21 [4672/36450]\tLoss: 504.6808\n",
      "Training Epoch: 21 [4736/36450]\tLoss: 500.7244\n",
      "Training Epoch: 21 [4800/36450]\tLoss: 508.8023\n",
      "Training Epoch: 21 [4864/36450]\tLoss: 494.3160\n",
      "Training Epoch: 21 [4928/36450]\tLoss: 482.3656\n",
      "Training Epoch: 21 [4992/36450]\tLoss: 473.5389\n",
      "Training Epoch: 21 [5056/36450]\tLoss: 496.3961\n",
      "Training Epoch: 21 [5120/36450]\tLoss: 454.1982\n",
      "Training Epoch: 21 [5184/36450]\tLoss: 493.2178\n",
      "Training Epoch: 21 [5248/36450]\tLoss: 482.7602\n",
      "Training Epoch: 21 [5312/36450]\tLoss: 454.3394\n",
      "Training Epoch: 21 [5376/36450]\tLoss: 481.4038\n",
      "Training Epoch: 21 [5440/36450]\tLoss: 480.0355\n",
      "Training Epoch: 21 [5504/36450]\tLoss: 485.6875\n",
      "Training Epoch: 21 [5568/36450]\tLoss: 481.6263\n",
      "Training Epoch: 21 [5632/36450]\tLoss: 478.6798\n",
      "Training Epoch: 21 [5696/36450]\tLoss: 506.1574\n",
      "Training Epoch: 21 [5760/36450]\tLoss: 475.6515\n",
      "Training Epoch: 21 [5824/36450]\tLoss: 476.8779\n",
      "Training Epoch: 21 [5888/36450]\tLoss: 474.5158\n",
      "Training Epoch: 21 [5952/36450]\tLoss: 454.6267\n",
      "Training Epoch: 21 [6016/36450]\tLoss: 468.7374\n",
      "Training Epoch: 21 [6080/36450]\tLoss: 472.5464\n",
      "Training Epoch: 21 [6144/36450]\tLoss: 471.1842\n",
      "Training Epoch: 21 [6208/36450]\tLoss: 477.4344\n",
      "Training Epoch: 21 [6272/36450]\tLoss: 487.9358\n",
      "Training Epoch: 21 [6336/36450]\tLoss: 496.8884\n",
      "Training Epoch: 21 [6400/36450]\tLoss: 489.1536\n",
      "Training Epoch: 21 [6464/36450]\tLoss: 498.6474\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
