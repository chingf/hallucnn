{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96589b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'pnet_noisy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a437651",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfdc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "lr = 1E-5\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}clean_reconstruction_training_set.hdf5'\n",
    "train_datafile = f'{engram_dir}hyperparameter_pooled_training_dataset_random_order_noNulls.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9766a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 18 00:46:50 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:23:00.0 Off |                  N/A |\r\n",
      "| 27%   24C    P8     5W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d748a",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b087e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae18933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6839214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23a4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779893ac",
   "metadata": {},
   "source": [
    "# Set up train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc69b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CleanSoundsDataset(train_datafile, .9)\n",
    "test_dataset = CleanSoundsDataset(train_datafile, .9, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a14c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026ae3a",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b449fc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/67482]\tLoss: 53174.1797\n",
      "Training Epoch: 1 [100/67482]\tLoss: 49671.3828\n",
      "Training Epoch: 1 [150/67482]\tLoss: 47478.8164\n",
      "Training Epoch: 1 [200/67482]\tLoss: 44750.2344\n",
      "Training Epoch: 1 [250/67482]\tLoss: 42351.6719\n",
      "Training Epoch: 1 [300/67482]\tLoss: 39993.8867\n",
      "Training Epoch: 1 [350/67482]\tLoss: 37114.4609\n",
      "Training Epoch: 1 [400/67482]\tLoss: 35507.5508\n",
      "Training Epoch: 1 [450/67482]\tLoss: 33815.5430\n",
      "Training Epoch: 1 [500/67482]\tLoss: 31897.8457\n",
      "Training Epoch: 1 [550/67482]\tLoss: 29236.1973\n",
      "Training Epoch: 1 [600/67482]\tLoss: 28484.2305\n",
      "Training Epoch: 1 [650/67482]\tLoss: 26955.4805\n",
      "Training Epoch: 1 [700/67482]\tLoss: 25102.0117\n",
      "Training Epoch: 1 [750/67482]\tLoss: 23256.7266\n",
      "Training Epoch: 1 [800/67482]\tLoss: 22697.1855\n",
      "Training Epoch: 1 [850/67482]\tLoss: 20755.6328\n",
      "Training Epoch: 1 [900/67482]\tLoss: 19294.5742\n",
      "Training Epoch: 1 [950/67482]\tLoss: 19178.0781\n",
      "Training Epoch: 1 [1000/67482]\tLoss: 18101.5742\n",
      "Training Epoch: 1 [1050/67482]\tLoss: 16837.5820\n",
      "Training Epoch: 1 [1100/67482]\tLoss: 16428.0234\n",
      "Training Epoch: 1 [1150/67482]\tLoss: 15725.7246\n",
      "Training Epoch: 1 [1200/67482]\tLoss: 15253.2715\n",
      "Training Epoch: 1 [1250/67482]\tLoss: 14843.8525\n",
      "Training Epoch: 1 [1300/67482]\tLoss: 14168.6328\n",
      "Training Epoch: 1 [1350/67482]\tLoss: 13771.0312\n",
      "Training Epoch: 1 [1400/67482]\tLoss: 13395.0186\n",
      "Training Epoch: 1 [1450/67482]\tLoss: 13265.2314\n",
      "Training Epoch: 1 [1500/67482]\tLoss: 12680.1465\n",
      "Training Epoch: 1 [1550/67482]\tLoss: 12497.7285\n",
      "Training Epoch: 1 [1600/67482]\tLoss: 12559.8164\n",
      "Training Epoch: 1 [1650/67482]\tLoss: 12276.1270\n",
      "Training Epoch: 1 [1700/67482]\tLoss: 12176.8555\n",
      "Training Epoch: 1 [1750/67482]\tLoss: 12422.1943\n",
      "Training Epoch: 1 [1800/67482]\tLoss: 12238.3301\n",
      "Training Epoch: 1 [1850/67482]\tLoss: 11785.1465\n",
      "Training Epoch: 1 [1900/67482]\tLoss: 11865.9297\n",
      "Training Epoch: 1 [1950/67482]\tLoss: 11627.8125\n",
      "Training Epoch: 1 [2000/67482]\tLoss: 12461.7646\n",
      "Training Epoch: 1 [2050/67482]\tLoss: 12426.7695\n",
      "Training Epoch: 1 [2100/67482]\tLoss: 11576.2783\n",
      "Training Epoch: 1 [2150/67482]\tLoss: 12124.4648\n",
      "Training Epoch: 1 [2200/67482]\tLoss: 12100.6914\n",
      "Training Epoch: 1 [2250/67482]\tLoss: 11912.5049\n",
      "Training Epoch: 1 [2300/67482]\tLoss: 11626.2607\n",
      "Training Epoch: 1 [2350/67482]\tLoss: 12412.8262\n",
      "Training Epoch: 1 [2400/67482]\tLoss: 11616.1133\n",
      "Training Epoch: 1 [2450/67482]\tLoss: 11754.8018\n",
      "Training Epoch: 1 [2500/67482]\tLoss: 11817.5537\n",
      "Training Epoch: 1 [2550/67482]\tLoss: 11692.0146\n",
      "Training Epoch: 1 [2600/67482]\tLoss: 11539.8555\n",
      "Training Epoch: 1 [2650/67482]\tLoss: 11523.6455\n",
      "Training Epoch: 1 [2700/67482]\tLoss: 11732.1699\n",
      "Training Epoch: 1 [2750/67482]\tLoss: 11459.6465\n",
      "Training Epoch: 1 [2800/67482]\tLoss: 11714.5303\n",
      "Training Epoch: 1 [2850/67482]\tLoss: 12018.5225\n",
      "Training Epoch: 1 [2900/67482]\tLoss: 11373.3496\n",
      "Training Epoch: 1 [2950/67482]\tLoss: 11628.3643\n",
      "Training Epoch: 1 [3000/67482]\tLoss: 11468.0498\n",
      "Training Epoch: 1 [3050/67482]\tLoss: 11330.6641\n",
      "Training Epoch: 1 [3100/67482]\tLoss: 11249.6562\n",
      "Training Epoch: 1 [3150/67482]\tLoss: 11211.3291\n",
      "Training Epoch: 1 [3200/67482]\tLoss: 11199.4980\n",
      "Training Epoch: 1 [3250/67482]\tLoss: 11314.0039\n",
      "Training Epoch: 1 [3300/67482]\tLoss: 11195.3271\n",
      "Training Epoch: 1 [3350/67482]\tLoss: 11325.3154\n",
      "Training Epoch: 1 [3400/67482]\tLoss: 11323.2412\n",
      "Training Epoch: 1 [3450/67482]\tLoss: 10849.8486\n",
      "Training Epoch: 1 [3500/67482]\tLoss: 10907.7637\n",
      "Training Epoch: 1 [3550/67482]\tLoss: 11489.3896\n",
      "Training Epoch: 1 [3600/67482]\tLoss: 10928.7480\n",
      "Training Epoch: 1 [3650/67482]\tLoss: 11000.2656\n",
      "Training Epoch: 1 [3700/67482]\tLoss: 10909.6191\n",
      "Training Epoch: 1 [3750/67482]\tLoss: 11038.0986\n",
      "Training Epoch: 1 [3800/67482]\tLoss: 10634.5312\n",
      "Training Epoch: 1 [3850/67482]\tLoss: 10813.8584\n",
      "Training Epoch: 1 [3900/67482]\tLoss: 10940.4150\n",
      "Training Epoch: 1 [3950/67482]\tLoss: 11056.5557\n",
      "Training Epoch: 1 [4000/67482]\tLoss: 11018.5449\n",
      "Training Epoch: 1 [4050/67482]\tLoss: 10754.2090\n",
      "Training Epoch: 1 [4100/67482]\tLoss: 10856.0508\n",
      "Training Epoch: 1 [4150/67482]\tLoss: 10740.4355\n",
      "Training Epoch: 1 [4200/67482]\tLoss: 10706.6826\n",
      "Training Epoch: 1 [4250/67482]\tLoss: 10985.9463\n",
      "Training Epoch: 1 [4300/67482]\tLoss: 10553.7188\n",
      "Training Epoch: 1 [4350/67482]\tLoss: 10459.3965\n",
      "Training Epoch: 1 [4400/67482]\tLoss: 10589.5615\n",
      "Training Epoch: 1 [4450/67482]\tLoss: 10377.0713\n",
      "Training Epoch: 1 [4500/67482]\tLoss: 10735.7480\n",
      "Training Epoch: 1 [4550/67482]\tLoss: 10205.5225\n",
      "Training Epoch: 1 [4600/67482]\tLoss: 10504.0146\n",
      "Training Epoch: 1 [4650/67482]\tLoss: 10634.9727\n",
      "Training Epoch: 1 [4700/67482]\tLoss: 10616.6230\n",
      "Training Epoch: 1 [4750/67482]\tLoss: 10404.2852\n",
      "Training Epoch: 1 [4800/67482]\tLoss: 10466.8857\n",
      "Training Epoch: 1 [4850/67482]\tLoss: 10563.3574\n",
      "Training Epoch: 1 [4900/67482]\tLoss: 10267.6895\n",
      "Training Epoch: 1 [4950/67482]\tLoss: 10265.4814\n",
      "Training Epoch: 1 [5000/67482]\tLoss: 10412.9062\n",
      "Training Epoch: 1 [5050/67482]\tLoss: 10194.5342\n",
      "Training Epoch: 1 [5100/67482]\tLoss: 10279.1338\n",
      "Training Epoch: 1 [5150/67482]\tLoss: 10477.8848\n",
      "Training Epoch: 1 [5200/67482]\tLoss: 10106.9609\n",
      "Training Epoch: 1 [5250/67482]\tLoss: 10083.7773\n",
      "Training Epoch: 1 [5300/67482]\tLoss: 10213.2227\n",
      "Training Epoch: 1 [5350/67482]\tLoss: 10257.5215\n",
      "Training Epoch: 1 [5400/67482]\tLoss: 10115.9756\n",
      "Training Epoch: 1 [5450/67482]\tLoss: 9960.0723\n",
      "Training Epoch: 1 [5500/67482]\tLoss: 10006.3057\n",
      "Training Epoch: 1 [5550/67482]\tLoss: 10097.1660\n",
      "Training Epoch: 1 [5600/67482]\tLoss: 10092.1250\n",
      "Training Epoch: 1 [5650/67482]\tLoss: 10079.8896\n",
      "Training Epoch: 1 [5700/67482]\tLoss: 9770.1025\n",
      "Training Epoch: 1 [5750/67482]\tLoss: 9898.8105\n",
      "Training Epoch: 1 [5800/67482]\tLoss: 9664.9053\n",
      "Training Epoch: 1 [5850/67482]\tLoss: 9906.0439\n",
      "Training Epoch: 1 [5900/67482]\tLoss: 9830.9893\n",
      "Training Epoch: 1 [5950/67482]\tLoss: 9856.7656\n",
      "Training Epoch: 1 [6000/67482]\tLoss: 9627.4648\n",
      "Training Epoch: 1 [6050/67482]\tLoss: 9774.2607\n",
      "Training Epoch: 1 [6100/67482]\tLoss: 9919.0977\n",
      "Training Epoch: 1 [6150/67482]\tLoss: 9584.2949\n",
      "Training Epoch: 1 [6200/67482]\tLoss: 9807.6992\n",
      "Training Epoch: 1 [6250/67482]\tLoss: 9620.4043\n",
      "Training Epoch: 1 [6300/67482]\tLoss: 9482.3730\n",
      "Training Epoch: 1 [6350/67482]\tLoss: 9752.0088\n",
      "Training Epoch: 1 [6400/67482]\tLoss: 9540.5186\n",
      "Training Epoch: 1 [6450/67482]\tLoss: 9467.7246\n",
      "Training Epoch: 1 [6500/67482]\tLoss: 9521.9355\n",
      "Training Epoch: 1 [6550/67482]\tLoss: 9614.7920\n",
      "Training Epoch: 1 [6600/67482]\tLoss: 9593.9258\n",
      "Training Epoch: 1 [6650/67482]\tLoss: 9311.0625\n",
      "Training Epoch: 1 [6700/67482]\tLoss: 9396.7012\n",
      "Training Epoch: 1 [6750/67482]\tLoss: 9321.9834\n",
      "Training Epoch: 1 [6800/67482]\tLoss: 9377.3047\n",
      "Training Epoch: 1 [6850/67482]\tLoss: 9342.8848\n",
      "Training Epoch: 1 [6900/67482]\tLoss: 9314.0449\n",
      "Training Epoch: 1 [6950/67482]\tLoss: 9065.8721\n",
      "Training Epoch: 1 [7000/67482]\tLoss: 9074.4717\n",
      "Training Epoch: 1 [7050/67482]\tLoss: 9457.6689\n",
      "Training Epoch: 1 [7100/67482]\tLoss: 9123.1826\n",
      "Training Epoch: 1 [7150/67482]\tLoss: 9145.4980\n",
      "Training Epoch: 1 [7200/67482]\tLoss: 9428.4307\n",
      "Training Epoch: 1 [7250/67482]\tLoss: 9398.0625\n",
      "Training Epoch: 1 [7300/67482]\tLoss: 8931.7578\n",
      "Training Epoch: 1 [7350/67482]\tLoss: 8947.3027\n",
      "Training Epoch: 1 [7400/67482]\tLoss: 9016.7061\n",
      "Training Epoch: 1 [7450/67482]\tLoss: 9062.9932\n",
      "Training Epoch: 1 [7500/67482]\tLoss: 9191.9297\n",
      "Training Epoch: 1 [7550/67482]\tLoss: 9060.2158\n",
      "Training Epoch: 1 [7600/67482]\tLoss: 8974.8252\n",
      "Training Epoch: 1 [7650/67482]\tLoss: 8766.2109\n",
      "Training Epoch: 1 [7700/67482]\tLoss: 8889.2656\n",
      "Training Epoch: 1 [7750/67482]\tLoss: 8988.3037\n",
      "Training Epoch: 1 [7800/67482]\tLoss: 8954.4805\n",
      "Training Epoch: 1 [7850/67482]\tLoss: 8802.1143\n",
      "Training Epoch: 1 [7900/67482]\tLoss: 8991.0566\n",
      "Training Epoch: 1 [7950/67482]\tLoss: 8772.6436\n",
      "Training Epoch: 1 [8000/67482]\tLoss: 8933.9385\n",
      "Training Epoch: 1 [8050/67482]\tLoss: 8739.9893\n",
      "Training Epoch: 1 [8100/67482]\tLoss: 8659.4141\n",
      "Training Epoch: 1 [8150/67482]\tLoss: 8746.2734\n",
      "Training Epoch: 1 [8200/67482]\tLoss: 8732.0117\n",
      "Training Epoch: 1 [8250/67482]\tLoss: 8580.4131\n",
      "Training Epoch: 1 [8300/67482]\tLoss: 8664.1934\n",
      "Training Epoch: 1 [8350/67482]\tLoss: 8704.2998\n",
      "Training Epoch: 1 [8400/67482]\tLoss: 8766.1055\n",
      "Training Epoch: 1 [8450/67482]\tLoss: 8404.6562\n",
      "Training Epoch: 1 [8500/67482]\tLoss: 8556.9355\n",
      "Training Epoch: 1 [8550/67482]\tLoss: 8585.2285\n",
      "Training Epoch: 1 [8600/67482]\tLoss: 8520.4971\n",
      "Training Epoch: 1 [8650/67482]\tLoss: 8609.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8700/67482]\tLoss: 8381.4375\n",
      "Training Epoch: 1 [8750/67482]\tLoss: 8420.2168\n",
      "Training Epoch: 1 [8800/67482]\tLoss: 8542.5039\n",
      "Training Epoch: 1 [8850/67482]\tLoss: 8397.6846\n",
      "Training Epoch: 1 [8900/67482]\tLoss: 8493.6592\n",
      "Training Epoch: 1 [8950/67482]\tLoss: 8210.4521\n",
      "Training Epoch: 1 [9000/67482]\tLoss: 8498.1367\n",
      "Training Epoch: 1 [9050/67482]\tLoss: 8350.3857\n",
      "Training Epoch: 1 [9100/67482]\tLoss: 8123.1777\n",
      "Training Epoch: 1 [9150/67482]\tLoss: 8434.9854\n",
      "Training Epoch: 1 [9200/67482]\tLoss: 8449.2695\n",
      "Training Epoch: 1 [9250/67482]\tLoss: 8298.5674\n",
      "Training Epoch: 1 [9300/67482]\tLoss: 8287.6650\n",
      "Training Epoch: 1 [9350/67482]\tLoss: 8187.1670\n",
      "Training Epoch: 1 [9400/67482]\tLoss: 8324.9424\n",
      "Training Epoch: 1 [9450/67482]\tLoss: 7989.1079\n",
      "Training Epoch: 1 [9500/67482]\tLoss: 8111.6782\n",
      "Training Epoch: 1 [9550/67482]\tLoss: 8179.5229\n",
      "Training Epoch: 1 [9600/67482]\tLoss: 8136.7632\n",
      "Training Epoch: 1 [9650/67482]\tLoss: 8082.7246\n",
      "Training Epoch: 1 [9700/67482]\tLoss: 7855.3062\n",
      "Training Epoch: 1 [9750/67482]\tLoss: 7931.9980\n",
      "Training Epoch: 1 [9800/67482]\tLoss: 8011.6270\n",
      "Training Epoch: 1 [9850/67482]\tLoss: 8020.9604\n",
      "Training Epoch: 1 [9900/67482]\tLoss: 7870.0781\n",
      "Training Epoch: 1 [9950/67482]\tLoss: 7949.2705\n",
      "Training Epoch: 1 [10000/67482]\tLoss: 7936.9731\n",
      "Training Epoch: 1 [10050/67482]\tLoss: 7875.6943\n",
      "Training Epoch: 1 [10100/67482]\tLoss: 7888.6528\n",
      "Training Epoch: 1 [10150/67482]\tLoss: 7797.0366\n",
      "Training Epoch: 1 [10200/67482]\tLoss: 7870.4385\n",
      "Training Epoch: 1 [10250/67482]\tLoss: 7896.2437\n",
      "Training Epoch: 1 [10300/67482]\tLoss: 7657.5361\n",
      "Training Epoch: 1 [10350/67482]\tLoss: 7948.9995\n",
      "Training Epoch: 1 [10400/67482]\tLoss: 7921.9897\n",
      "Training Epoch: 1 [10450/67482]\tLoss: 7769.8154\n",
      "Training Epoch: 1 [10500/67482]\tLoss: 7660.0537\n",
      "Training Epoch: 1 [10550/67482]\tLoss: 7840.8257\n",
      "Training Epoch: 1 [10600/67482]\tLoss: 7715.3008\n",
      "Training Epoch: 1 [10650/67482]\tLoss: 7663.2046\n",
      "Training Epoch: 1 [10700/67482]\tLoss: 7565.6895\n",
      "Training Epoch: 1 [10750/67482]\tLoss: 7764.2661\n",
      "Training Epoch: 1 [10800/67482]\tLoss: 7508.3882\n",
      "Training Epoch: 1 [10850/67482]\tLoss: 7366.9399\n",
      "Training Epoch: 1 [10900/67482]\tLoss: 7496.4761\n",
      "Training Epoch: 1 [10950/67482]\tLoss: 7612.2798\n",
      "Training Epoch: 1 [11000/67482]\tLoss: 7372.3311\n",
      "Training Epoch: 1 [11050/67482]\tLoss: 7522.9873\n",
      "Training Epoch: 1 [11100/67482]\tLoss: 7390.7231\n",
      "Training Epoch: 1 [11150/67482]\tLoss: 7642.3452\n",
      "Training Epoch: 1 [11200/67482]\tLoss: 7556.2764\n",
      "Training Epoch: 1 [11250/67482]\tLoss: 7435.0093\n",
      "Training Epoch: 1 [11300/67482]\tLoss: 7402.3271\n",
      "Training Epoch: 1 [11350/67482]\tLoss: 7363.8560\n",
      "Training Epoch: 1 [11400/67482]\tLoss: 7326.1904\n",
      "Training Epoch: 1 [11450/67482]\tLoss: 7255.5771\n",
      "Training Epoch: 1 [11500/67482]\tLoss: 7341.1196\n",
      "Training Epoch: 1 [11550/67482]\tLoss: 7211.3589\n",
      "Training Epoch: 1 [11600/67482]\tLoss: 7293.4385\n",
      "Training Epoch: 1 [11650/67482]\tLoss: 7265.2422\n",
      "Training Epoch: 1 [11700/67482]\tLoss: 7270.1987\n",
      "Training Epoch: 1 [11750/67482]\tLoss: 7260.0220\n",
      "Training Epoch: 1 [11800/67482]\tLoss: 7369.7441\n",
      "Training Epoch: 1 [11850/67482]\tLoss: 7171.9297\n",
      "Training Epoch: 1 [11900/67482]\tLoss: 7139.4819\n",
      "Training Epoch: 1 [11950/67482]\tLoss: 7287.6777\n",
      "Training Epoch: 1 [12000/67482]\tLoss: 6960.7378\n",
      "Training Epoch: 1 [12050/67482]\tLoss: 7055.8228\n",
      "Training Epoch: 1 [12100/67482]\tLoss: 7179.9478\n",
      "Training Epoch: 1 [12150/67482]\tLoss: 7058.7261\n",
      "Training Epoch: 1 [12200/67482]\tLoss: 7032.5620\n",
      "Training Epoch: 1 [12250/67482]\tLoss: 7065.7280\n",
      "Training Epoch: 1 [12300/67482]\tLoss: 7078.7871\n",
      "Training Epoch: 1 [12350/67482]\tLoss: 6831.6372\n",
      "Training Epoch: 1 [12400/67482]\tLoss: 7091.7710\n",
      "Training Epoch: 1 [12450/67482]\tLoss: 6992.2617\n",
      "Training Epoch: 1 [12500/67482]\tLoss: 6992.8247\n",
      "Training Epoch: 1 [12550/67482]\tLoss: 6978.5776\n",
      "Training Epoch: 1 [12600/67482]\tLoss: 6898.2485\n",
      "Training Epoch: 1 [12650/67482]\tLoss: 6776.4448\n",
      "Training Epoch: 1 [12700/67482]\tLoss: 6838.4326\n",
      "Training Epoch: 1 [12750/67482]\tLoss: 6877.5601\n",
      "Training Epoch: 1 [12800/67482]\tLoss: 7068.7158\n",
      "Training Epoch: 1 [12850/67482]\tLoss: 6728.0122\n",
      "Training Epoch: 1 [12900/67482]\tLoss: 6778.1372\n",
      "Training Epoch: 1 [12950/67482]\tLoss: 6870.7700\n",
      "Training Epoch: 1 [13000/67482]\tLoss: 6730.9883\n",
      "Training Epoch: 1 [13050/67482]\tLoss: 6810.4268\n",
      "Training Epoch: 1 [13100/67482]\tLoss: 6589.6055\n",
      "Training Epoch: 1 [13150/67482]\tLoss: 6866.8281\n",
      "Training Epoch: 1 [13200/67482]\tLoss: 6953.4858\n",
      "Training Epoch: 1 [13250/67482]\tLoss: 6669.2363\n",
      "Training Epoch: 1 [13300/67482]\tLoss: 6632.0068\n",
      "Training Epoch: 1 [13350/67482]\tLoss: 6788.1772\n",
      "Training Epoch: 1 [13400/67482]\tLoss: 6720.9673\n",
      "Training Epoch: 1 [13450/67482]\tLoss: 6777.9019\n",
      "Training Epoch: 1 [13500/67482]\tLoss: 6678.5044\n",
      "Training Epoch: 1 [13550/67482]\tLoss: 6516.7183\n",
      "Training Epoch: 1 [13600/67482]\tLoss: 6596.6846\n",
      "Training Epoch: 1 [13650/67482]\tLoss: 6758.5444\n",
      "Training Epoch: 1 [13700/67482]\tLoss: 6606.2656\n",
      "Training Epoch: 1 [13750/67482]\tLoss: 6564.0073\n",
      "Training Epoch: 1 [13800/67482]\tLoss: 6457.9136\n",
      "Training Epoch: 1 [13850/67482]\tLoss: 6700.1074\n",
      "Training Epoch: 1 [13900/67482]\tLoss: 6576.4175\n",
      "Training Epoch: 1 [13950/67482]\tLoss: 6471.0107\n",
      "Training Epoch: 1 [14000/67482]\tLoss: 6561.8545\n",
      "Training Epoch: 1 [14050/67482]\tLoss: 6445.9443\n",
      "Training Epoch: 1 [14100/67482]\tLoss: 6383.6855\n",
      "Training Epoch: 1 [14150/67482]\tLoss: 6511.4087\n",
      "Training Epoch: 1 [14200/67482]\tLoss: 6490.8730\n",
      "Training Epoch: 1 [14250/67482]\tLoss: 6315.8096\n",
      "Training Epoch: 1 [14300/67482]\tLoss: 6397.7881\n",
      "Training Epoch: 1 [14350/67482]\tLoss: 6391.8096\n",
      "Training Epoch: 1 [14400/67482]\tLoss: 6216.1802\n",
      "Training Epoch: 1 [14450/67482]\tLoss: 6420.0723\n",
      "Training Epoch: 1 [14500/67482]\tLoss: 6315.7998\n",
      "Training Epoch: 1 [14550/67482]\tLoss: 6475.2490\n",
      "Training Epoch: 1 [14600/67482]\tLoss: 6454.9531\n",
      "Training Epoch: 1 [14650/67482]\tLoss: 6432.6924\n",
      "Training Epoch: 1 [14700/67482]\tLoss: 6229.7573\n",
      "Training Epoch: 1 [14750/67482]\tLoss: 6252.2524\n",
      "Training Epoch: 1 [14800/67482]\tLoss: 6447.5869\n",
      "Training Epoch: 1 [14850/67482]\tLoss: 6308.7090\n",
      "Training Epoch: 1 [14900/67482]\tLoss: 6103.9741\n",
      "Training Epoch: 1 [14950/67482]\tLoss: 6167.1899\n",
      "Training Epoch: 1 [15000/67482]\tLoss: 6281.6641\n",
      "Training Epoch: 1 [15050/67482]\tLoss: 6242.6626\n",
      "Training Epoch: 1 [15100/67482]\tLoss: 6265.3682\n",
      "Training Epoch: 1 [15150/67482]\tLoss: 6114.5571\n",
      "Training Epoch: 1 [15200/67482]\tLoss: 6338.0605\n",
      "Training Epoch: 1 [15250/67482]\tLoss: 6097.0024\n",
      "Training Epoch: 1 [15300/67482]\tLoss: 6303.3154\n",
      "Training Epoch: 1 [15350/67482]\tLoss: 6149.1260\n",
      "Training Epoch: 1 [15400/67482]\tLoss: 6057.4204\n",
      "Training Epoch: 1 [15450/67482]\tLoss: 6091.6821\n",
      "Training Epoch: 1 [15500/67482]\tLoss: 5944.9707\n",
      "Training Epoch: 1 [15550/67482]\tLoss: 5987.5137\n",
      "Training Epoch: 1 [15600/67482]\tLoss: 6044.1484\n",
      "Training Epoch: 1 [15650/67482]\tLoss: 6160.0483\n",
      "Training Epoch: 1 [15700/67482]\tLoss: 5906.4785\n",
      "Training Epoch: 1 [15750/67482]\tLoss: 5940.6460\n",
      "Training Epoch: 1 [15800/67482]\tLoss: 6054.6685\n",
      "Training Epoch: 1 [15850/67482]\tLoss: 5857.9282\n",
      "Training Epoch: 1 [15900/67482]\tLoss: 5880.8975\n",
      "Training Epoch: 1 [15950/67482]\tLoss: 5920.1143\n",
      "Training Epoch: 1 [16000/67482]\tLoss: 5944.5430\n",
      "Training Epoch: 1 [16050/67482]\tLoss: 5988.9648\n",
      "Training Epoch: 1 [16100/67482]\tLoss: 5907.6445\n",
      "Training Epoch: 1 [16150/67482]\tLoss: 6086.6602\n",
      "Training Epoch: 1 [16200/67482]\tLoss: 6112.6431\n",
      "Training Epoch: 1 [16250/67482]\tLoss: 5877.6240\n",
      "Training Epoch: 1 [16300/67482]\tLoss: 5938.4419\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec27ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
