{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.ReconstructionTrainingDataset import CleanSoundsDataset\n",
    "from data.ReconstructionTrainingDataset import NoisySoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_name = 'pnet'\n",
    "_train_datafile = 'clean_reconstruction_training_set'\n",
    "SoundsDataset = CleanSoundsDataset\n",
    "dset_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4755a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = '3'\n",
    "pnet_name = 'pnet_snr3'\n",
    "_train_datafile = 'hyperparameter_pooled_training_dataset_random_order_noNulls'\n",
    "SoundsDataset = NoisySoundsDataset\n",
    "dset_kwargs = {'snr': snr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96589b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a437651",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfdc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "lr = 1E-5\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}{_train_datafile}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9766a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  9 16:24:51 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0 Off |                  N/A |\r\n",
      "|  0%   27C    P8     8W / 250W |      4MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d748a",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b087e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/ctn/users/cf2794/Code/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/ctn/users/cf2794/Code/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae18933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6839214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23a4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779893ac",
   "metadata": {},
   "source": [
    "# Set up train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc69b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SoundsDataset(\n",
    "    train_datafile, subset=.9, **dset_kwargs)\n",
    "test_dataset = SoundsDataset(\n",
    "    train_datafile, subset=.9,\n",
    "    train = False, **dset_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a14c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026ae3a",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b449fc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/13482]\tLoss: 35514.4609\n",
      "Training Epoch: 1 [100/13482]\tLoss: 33856.3945\n",
      "Training Epoch: 1 [150/13482]\tLoss: 32072.6426\n",
      "Training Epoch: 1 [200/13482]\tLoss: 30351.8027\n",
      "Training Epoch: 1 [250/13482]\tLoss: 27271.6016\n",
      "Training Epoch: 1 [300/13482]\tLoss: 26212.2793\n",
      "Training Epoch: 1 [350/13482]\tLoss: 22904.4570\n",
      "Training Epoch: 1 [400/13482]\tLoss: 22204.2480\n",
      "Training Epoch: 1 [450/13482]\tLoss: 21392.3672\n",
      "Training Epoch: 1 [500/13482]\tLoss: 19045.0059\n",
      "Training Epoch: 1 [550/13482]\tLoss: 17623.0371\n",
      "Training Epoch: 1 [600/13482]\tLoss: 16369.6934\n",
      "Training Epoch: 1 [650/13482]\tLoss: 16115.6201\n",
      "Training Epoch: 1 [700/13482]\tLoss: 14240.6357\n",
      "Training Epoch: 1 [750/13482]\tLoss: 13777.3047\n",
      "Training Epoch: 1 [800/13482]\tLoss: 12901.0420\n",
      "Training Epoch: 1 [850/13482]\tLoss: 12825.3184\n",
      "Training Epoch: 1 [900/13482]\tLoss: 11872.5615\n",
      "Training Epoch: 1 [950/13482]\tLoss: 11763.4502\n",
      "Training Epoch: 1 [1000/13482]\tLoss: 11647.2930\n",
      "Training Epoch: 1 [1050/13482]\tLoss: 11793.6016\n",
      "Training Epoch: 1 [1100/13482]\tLoss: 11238.6748\n",
      "Training Epoch: 1 [1150/13482]\tLoss: 10674.9014\n",
      "Training Epoch: 1 [1200/13482]\tLoss: 10186.9102\n",
      "Training Epoch: 1 [1250/13482]\tLoss: 10363.8516\n",
      "Training Epoch: 1 [1300/13482]\tLoss: 10076.4746\n",
      "Training Epoch: 1 [1350/13482]\tLoss: 10237.8701\n",
      "Training Epoch: 1 [1400/13482]\tLoss: 10132.7549\n",
      "Training Epoch: 1 [1450/13482]\tLoss: 10295.0479\n",
      "Training Epoch: 1 [1500/13482]\tLoss: 10824.8984\n",
      "Training Epoch: 1 [1550/13482]\tLoss: 10444.7295\n",
      "Training Epoch: 1 [1600/13482]\tLoss: 10304.6396\n",
      "Training Epoch: 1 [1650/13482]\tLoss: 10736.6094\n",
      "Training Epoch: 1 [1700/13482]\tLoss: 10342.5381\n",
      "Training Epoch: 1 [1750/13482]\tLoss: 10135.5527\n",
      "Training Epoch: 1 [1800/13482]\tLoss: 10515.5996\n",
      "Training Epoch: 1 [1850/13482]\tLoss: 10982.8008\n",
      "Training Epoch: 1 [1900/13482]\tLoss: 10407.9229\n",
      "Training Epoch: 1 [1950/13482]\tLoss: 10044.8926\n",
      "Training Epoch: 1 [2000/13482]\tLoss: 10776.2559\n",
      "Training Epoch: 1 [2050/13482]\tLoss: 10374.0859\n",
      "Training Epoch: 1 [2100/13482]\tLoss: 10813.1719\n",
      "Training Epoch: 1 [2150/13482]\tLoss: 10704.6328\n",
      "Training Epoch: 1 [2200/13482]\tLoss: 9944.2227\n",
      "Training Epoch: 1 [2250/13482]\tLoss: 10321.7402\n",
      "Training Epoch: 1 [2300/13482]\tLoss: 9880.2949\n",
      "Training Epoch: 1 [2350/13482]\tLoss: 9852.7158\n",
      "Training Epoch: 1 [2400/13482]\tLoss: 10033.2969\n",
      "Training Epoch: 1 [2450/13482]\tLoss: 9594.5723\n",
      "Training Epoch: 1 [2500/13482]\tLoss: 9827.1211\n",
      "Training Epoch: 1 [2550/13482]\tLoss: 9681.6689\n",
      "Training Epoch: 1 [2600/13482]\tLoss: 9636.0654\n",
      "Training Epoch: 1 [2650/13482]\tLoss: 9394.1758\n",
      "Training Epoch: 1 [2700/13482]\tLoss: 9721.3164\n",
      "Training Epoch: 1 [2750/13482]\tLoss: 9843.3311\n",
      "Training Epoch: 1 [2800/13482]\tLoss: 9639.8203\n",
      "Training Epoch: 1 [2850/13482]\tLoss: 9997.5781\n",
      "Training Epoch: 1 [2900/13482]\tLoss: 9709.2646\n",
      "Training Epoch: 1 [2950/13482]\tLoss: 9855.5391\n",
      "Training Epoch: 1 [3000/13482]\tLoss: 9393.8447\n",
      "Training Epoch: 1 [3050/13482]\tLoss: 9371.2227\n",
      "Training Epoch: 1 [3100/13482]\tLoss: 9269.8984\n",
      "Training Epoch: 1 [3150/13482]\tLoss: 9454.6045\n",
      "Training Epoch: 1 [3200/13482]\tLoss: 9157.3994\n",
      "Training Epoch: 1 [3250/13482]\tLoss: 9533.2510\n",
      "Training Epoch: 1 [3300/13482]\tLoss: 9482.7529\n",
      "Training Epoch: 1 [3350/13482]\tLoss: 9555.0166\n",
      "Training Epoch: 1 [3400/13482]\tLoss: 9638.7266\n",
      "Training Epoch: 1 [3450/13482]\tLoss: 9608.1426\n",
      "Training Epoch: 1 [3500/13482]\tLoss: 9125.2656\n",
      "Training Epoch: 1 [3550/13482]\tLoss: 9189.0605\n",
      "Training Epoch: 1 [3600/13482]\tLoss: 9551.0420\n",
      "Training Epoch: 1 [3650/13482]\tLoss: 9241.1699\n",
      "Training Epoch: 1 [3700/13482]\tLoss: 8820.2402\n",
      "Training Epoch: 1 [3750/13482]\tLoss: 9079.8047\n",
      "Training Epoch: 1 [3800/13482]\tLoss: 9330.9961\n",
      "Training Epoch: 1 [3850/13482]\tLoss: 9073.0430\n",
      "Training Epoch: 1 [3900/13482]\tLoss: 9218.8760\n",
      "Training Epoch: 1 [3950/13482]\tLoss: 9188.3369\n",
      "Training Epoch: 1 [4000/13482]\tLoss: 8940.4600\n",
      "Training Epoch: 1 [4050/13482]\tLoss: 9213.0488\n",
      "Training Epoch: 1 [4100/13482]\tLoss: 8822.6934\n",
      "Training Epoch: 1 [4150/13482]\tLoss: 8744.3564\n",
      "Training Epoch: 1 [4200/13482]\tLoss: 9002.3203\n",
      "Training Epoch: 1 [4250/13482]\tLoss: 9230.0391\n",
      "Training Epoch: 1 [4300/13482]\tLoss: 8952.1562\n",
      "Training Epoch: 1 [4350/13482]\tLoss: 9022.2920\n",
      "Training Epoch: 1 [4400/13482]\tLoss: 9129.7080\n",
      "Training Epoch: 1 [4450/13482]\tLoss: 8809.0537\n",
      "Training Epoch: 1 [4500/13482]\tLoss: 8676.3311\n",
      "Training Epoch: 1 [4550/13482]\tLoss: 8324.3008\n",
      "Training Epoch: 1 [4600/13482]\tLoss: 8177.8535\n",
      "Training Epoch: 1 [4650/13482]\tLoss: 8917.2188\n",
      "Training Epoch: 1 [4700/13482]\tLoss: 8417.6924\n",
      "Training Epoch: 1 [4750/13482]\tLoss: 8633.7354\n",
      "Training Epoch: 1 [4800/13482]\tLoss: 8111.9854\n",
      "Training Epoch: 1 [4850/13482]\tLoss: 8219.9434\n",
      "Training Epoch: 1 [4900/13482]\tLoss: 8564.5723\n",
      "Training Epoch: 1 [4950/13482]\tLoss: 8428.7100\n",
      "Training Epoch: 1 [5000/13482]\tLoss: 8753.8398\n",
      "Training Epoch: 1 [5050/13482]\tLoss: 8614.3203\n",
      "Training Epoch: 1 [5100/13482]\tLoss: 8135.0859\n",
      "Training Epoch: 1 [5150/13482]\tLoss: 8364.6328\n",
      "Training Epoch: 1 [5200/13482]\tLoss: 8612.1982\n",
      "Training Epoch: 1 [5250/13482]\tLoss: 8122.9072\n",
      "Training Epoch: 1 [5300/13482]\tLoss: 8170.0884\n",
      "Training Epoch: 1 [5350/13482]\tLoss: 8220.2041\n",
      "Training Epoch: 1 [5400/13482]\tLoss: 8536.7334\n",
      "Training Epoch: 1 [5450/13482]\tLoss: 8093.7822\n",
      "Training Epoch: 1 [5500/13482]\tLoss: 7883.5181\n",
      "Training Epoch: 1 [5550/13482]\tLoss: 7732.2959\n",
      "Training Epoch: 1 [5600/13482]\tLoss: 8238.6670\n",
      "Training Epoch: 1 [5650/13482]\tLoss: 8197.2021\n",
      "Training Epoch: 1 [5700/13482]\tLoss: 8288.6543\n",
      "Training Epoch: 1 [5750/13482]\tLoss: 8251.8027\n",
      "Training Epoch: 1 [5800/13482]\tLoss: 8074.8374\n",
      "Training Epoch: 1 [5850/13482]\tLoss: 8164.7178\n",
      "Training Epoch: 1 [5900/13482]\tLoss: 8117.2905\n",
      "Training Epoch: 1 [5950/13482]\tLoss: 8079.9292\n",
      "Training Epoch: 1 [6000/13482]\tLoss: 7798.8354\n",
      "Training Epoch: 1 [6050/13482]\tLoss: 7903.0083\n",
      "Training Epoch: 1 [6100/13482]\tLoss: 7782.7085\n",
      "Training Epoch: 1 [6150/13482]\tLoss: 7996.0059\n",
      "Training Epoch: 1 [6200/13482]\tLoss: 8126.4458\n",
      "Training Epoch: 1 [6250/13482]\tLoss: 7768.1670\n",
      "Training Epoch: 1 [6300/13482]\tLoss: 7885.4795\n",
      "Training Epoch: 1 [6350/13482]\tLoss: 8000.0332\n",
      "Training Epoch: 1 [6400/13482]\tLoss: 7996.5210\n",
      "Training Epoch: 1 [6450/13482]\tLoss: 7672.6689\n",
      "Training Epoch: 1 [6500/13482]\tLoss: 7861.7393\n",
      "Training Epoch: 1 [6550/13482]\tLoss: 7748.4780\n",
      "Training Epoch: 1 [6600/13482]\tLoss: 7324.7031\n",
      "Training Epoch: 1 [6650/13482]\tLoss: 7590.6328\n",
      "Training Epoch: 1 [6700/13482]\tLoss: 7547.5981\n",
      "Training Epoch: 1 [6750/13482]\tLoss: 7638.5469\n",
      "Training Epoch: 1 [6800/13482]\tLoss: 7537.2983\n",
      "Training Epoch: 1 [6850/13482]\tLoss: 7647.4795\n",
      "Training Epoch: 1 [6900/13482]\tLoss: 7502.2671\n",
      "Training Epoch: 1 [6950/13482]\tLoss: 7423.8101\n",
      "Training Epoch: 1 [7000/13482]\tLoss: 7454.5103\n",
      "Training Epoch: 1 [7050/13482]\tLoss: 7728.6187\n",
      "Training Epoch: 1 [7100/13482]\tLoss: 7434.3232\n",
      "Training Epoch: 1 [7150/13482]\tLoss: 7473.1245\n",
      "Training Epoch: 1 [7200/13482]\tLoss: 7822.8950\n",
      "Training Epoch: 1 [7250/13482]\tLoss: 7370.2744\n",
      "Training Epoch: 1 [7300/13482]\tLoss: 7375.4912\n",
      "Training Epoch: 1 [7350/13482]\tLoss: 7262.4131\n",
      "Training Epoch: 1 [7400/13482]\tLoss: 7411.2524\n",
      "Training Epoch: 1 [7450/13482]\tLoss: 7280.9775\n",
      "Training Epoch: 1 [7500/13482]\tLoss: 7115.6470\n",
      "Training Epoch: 1 [7550/13482]\tLoss: 7276.2661\n",
      "Training Epoch: 1 [7600/13482]\tLoss: 7372.9526\n",
      "Training Epoch: 1 [7650/13482]\tLoss: 7200.6816\n",
      "Training Epoch: 1 [7700/13482]\tLoss: 7189.8184\n",
      "Training Epoch: 1 [7750/13482]\tLoss: 7174.8071\n",
      "Training Epoch: 1 [7800/13482]\tLoss: 7386.7178\n",
      "Training Epoch: 1 [7850/13482]\tLoss: 7279.8677\n",
      "Training Epoch: 1 [7900/13482]\tLoss: 7318.1899\n",
      "Training Epoch: 1 [7950/13482]\tLoss: 7294.9043\n",
      "Training Epoch: 1 [8000/13482]\tLoss: 6969.4258\n",
      "Training Epoch: 1 [8050/13482]\tLoss: 7110.2153\n",
      "Training Epoch: 1 [8100/13482]\tLoss: 7116.7935\n",
      "Training Epoch: 1 [8150/13482]\tLoss: 7319.4561\n",
      "Training Epoch: 1 [8200/13482]\tLoss: 6989.8364\n",
      "Training Epoch: 1 [8250/13482]\tLoss: 6734.5518\n",
      "Training Epoch: 1 [8300/13482]\tLoss: 7053.7710\n",
      "Training Epoch: 1 [8350/13482]\tLoss: 6988.8794\n",
      "Training Epoch: 1 [8400/13482]\tLoss: 7079.0171\n",
      "Training Epoch: 1 [8450/13482]\tLoss: 7030.1714\n",
      "Training Epoch: 1 [8500/13482]\tLoss: 6935.4541\n",
      "Training Epoch: 1 [8550/13482]\tLoss: 6714.0288\n",
      "Training Epoch: 1 [8600/13482]\tLoss: 6769.5068\n",
      "Training Epoch: 1 [8650/13482]\tLoss: 7000.1333\n",
      "Training Epoch: 1 [8700/13482]\tLoss: 6803.4204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8750/13482]\tLoss: 6655.8267\n",
      "Training Epoch: 1 [8800/13482]\tLoss: 6865.2637\n",
      "Training Epoch: 1 [8850/13482]\tLoss: 6544.4727\n",
      "Training Epoch: 1 [8900/13482]\tLoss: 6970.1553\n",
      "Training Epoch: 1 [8950/13482]\tLoss: 6812.4385\n",
      "Training Epoch: 1 [9000/13482]\tLoss: 6214.8384\n",
      "Training Epoch: 1 [9050/13482]\tLoss: 6737.5078\n",
      "Training Epoch: 1 [9100/13482]\tLoss: 6338.9316\n",
      "Training Epoch: 1 [9150/13482]\tLoss: 6588.7588\n",
      "Training Epoch: 1 [9200/13482]\tLoss: 6802.5410\n",
      "Training Epoch: 1 [9250/13482]\tLoss: 6514.0654\n",
      "Training Epoch: 1 [9300/13482]\tLoss: 6610.1338\n",
      "Training Epoch: 1 [9350/13482]\tLoss: 6629.9448\n",
      "Training Epoch: 1 [9400/13482]\tLoss: 6396.2954\n",
      "Training Epoch: 1 [9450/13482]\tLoss: 6822.5708\n",
      "Training Epoch: 1 [9500/13482]\tLoss: 6459.7798\n",
      "Training Epoch: 1 [9550/13482]\tLoss: 6625.3062\n",
      "Training Epoch: 1 [9600/13482]\tLoss: 6703.5132\n",
      "Training Epoch: 1 [9650/13482]\tLoss: 6577.0278\n",
      "Training Epoch: 1 [9700/13482]\tLoss: 6533.3340\n",
      "Training Epoch: 1 [9750/13482]\tLoss: 6395.3457\n",
      "Training Epoch: 1 [9800/13482]\tLoss: 6394.5435\n",
      "Training Epoch: 1 [9850/13482]\tLoss: 6320.3457\n",
      "Training Epoch: 1 [9900/13482]\tLoss: 6278.9209\n",
      "Training Epoch: 1 [9950/13482]\tLoss: 6529.5161\n",
      "Training Epoch: 1 [10000/13482]\tLoss: 6385.8604\n",
      "Training Epoch: 1 [10050/13482]\tLoss: 6424.7827\n",
      "Training Epoch: 1 [10100/13482]\tLoss: 6000.6909\n",
      "Training Epoch: 1 [10150/13482]\tLoss: 6313.5298\n",
      "Training Epoch: 1 [10200/13482]\tLoss: 6543.0015\n",
      "Training Epoch: 1 [10250/13482]\tLoss: 6378.7754\n",
      "Training Epoch: 1 [10300/13482]\tLoss: 6245.8950\n",
      "Training Epoch: 1 [10350/13482]\tLoss: 6269.4126\n",
      "Training Epoch: 1 [10400/13482]\tLoss: 6162.0347\n",
      "Training Epoch: 1 [10450/13482]\tLoss: 6300.4653\n",
      "Training Epoch: 1 [10500/13482]\tLoss: 6498.7959\n",
      "Training Epoch: 1 [10550/13482]\tLoss: 6275.7271\n",
      "Training Epoch: 1 [10600/13482]\tLoss: 5919.5649\n",
      "Training Epoch: 1 [10650/13482]\tLoss: 5829.2168\n",
      "Training Epoch: 1 [10700/13482]\tLoss: 6211.7651\n",
      "Training Epoch: 1 [10750/13482]\tLoss: 6042.4824\n",
      "Training Epoch: 1 [10800/13482]\tLoss: 6037.7515\n",
      "Training Epoch: 1 [10850/13482]\tLoss: 5771.8374\n",
      "Training Epoch: 1 [10900/13482]\tLoss: 6033.6270\n",
      "Training Epoch: 1 [10950/13482]\tLoss: 5855.0771\n",
      "Training Epoch: 1 [11000/13482]\tLoss: 5904.4990\n",
      "Training Epoch: 1 [11050/13482]\tLoss: 5826.9849\n",
      "Training Epoch: 1 [11100/13482]\tLoss: 5825.1538\n",
      "Training Epoch: 1 [11150/13482]\tLoss: 5954.2661\n",
      "Training Epoch: 1 [11200/13482]\tLoss: 5963.5107\n",
      "Training Epoch: 1 [11250/13482]\tLoss: 6013.3667\n",
      "Training Epoch: 1 [11300/13482]\tLoss: 5811.6777\n",
      "Training Epoch: 1 [11350/13482]\tLoss: 6004.9194\n",
      "Training Epoch: 1 [11400/13482]\tLoss: 5993.1162\n",
      "Training Epoch: 1 [11450/13482]\tLoss: 5709.0625\n",
      "Training Epoch: 1 [11500/13482]\tLoss: 5720.2100\n",
      "Training Epoch: 1 [11550/13482]\tLoss: 5750.0586\n",
      "Training Epoch: 1 [11600/13482]\tLoss: 5782.1382\n",
      "Training Epoch: 1 [11650/13482]\tLoss: 5621.9419\n",
      "Training Epoch: 1 [11700/13482]\tLoss: 5680.5361\n",
      "Training Epoch: 1 [11750/13482]\tLoss: 5402.5181\n",
      "Training Epoch: 1 [11800/13482]\tLoss: 5828.1533\n",
      "Training Epoch: 1 [11850/13482]\tLoss: 5619.9067\n",
      "Training Epoch: 1 [11900/13482]\tLoss: 5572.3638\n",
      "Training Epoch: 1 [11950/13482]\tLoss: 5758.9712\n",
      "Training Epoch: 1 [12000/13482]\tLoss: 5634.9282\n",
      "Training Epoch: 1 [12050/13482]\tLoss: 5751.8628\n",
      "Training Epoch: 1 [12100/13482]\tLoss: 5683.0317\n",
      "Training Epoch: 1 [12150/13482]\tLoss: 5833.9019\n",
      "Training Epoch: 1 [12200/13482]\tLoss: 5537.7637\n",
      "Training Epoch: 1 [12250/13482]\tLoss: 5620.5986\n",
      "Training Epoch: 1 [12300/13482]\tLoss: 5561.5122\n",
      "Training Epoch: 1 [12350/13482]\tLoss: 5675.3350\n",
      "Training Epoch: 1 [12400/13482]\tLoss: 5628.0312\n",
      "Training Epoch: 1 [12450/13482]\tLoss: 5415.0405\n",
      "Training Epoch: 1 [12500/13482]\tLoss: 5618.5088\n",
      "Training Epoch: 1 [12550/13482]\tLoss: 5562.7104\n",
      "Training Epoch: 1 [12600/13482]\tLoss: 5381.3247\n",
      "Training Epoch: 1 [12650/13482]\tLoss: 5247.3628\n",
      "Training Epoch: 1 [12700/13482]\tLoss: 5296.6973\n",
      "Training Epoch: 1 [12750/13482]\tLoss: 5369.0703\n",
      "Training Epoch: 1 [12800/13482]\tLoss: 5446.3838\n",
      "Training Epoch: 1 [12850/13482]\tLoss: 5268.8643\n",
      "Training Epoch: 1 [12900/13482]\tLoss: 5476.5249\n",
      "Training Epoch: 1 [12950/13482]\tLoss: 5490.3262\n",
      "Training Epoch: 1 [13000/13482]\tLoss: 5220.0752\n",
      "Training Epoch: 1 [13050/13482]\tLoss: 5236.0703\n",
      "Training Epoch: 1 [13100/13482]\tLoss: 5257.3218\n",
      "Training Epoch: 1 [13150/13482]\tLoss: 5134.7783\n",
      "Training Epoch: 1 [13200/13482]\tLoss: 5234.7876\n",
      "Training Epoch: 1 [13250/13482]\tLoss: 5559.2778\n",
      "Training Epoch: 1 [13300/13482]\tLoss: 5302.4243\n",
      "Training Epoch: 1 [13350/13482]\tLoss: 5394.7280\n",
      "Training Epoch: 1 [13400/13482]\tLoss: 5425.7480\n",
      "Training Epoch: 1 [13450/13482]\tLoss: 5325.9790\n",
      "Training Epoch: 1 [13482/13482]\tLoss: 5255.2510\n",
      "Training Epoch: 1 [1497/1497]\tLoss: 5186.3701\n",
      "Training Epoch: 2 [50/13482]\tLoss: 5095.0293\n",
      "Training Epoch: 2 [100/13482]\tLoss: 5063.6191\n",
      "Training Epoch: 2 [150/13482]\tLoss: 5252.2275\n",
      "Training Epoch: 2 [200/13482]\tLoss: 5308.1938\n",
      "Training Epoch: 2 [250/13482]\tLoss: 5145.8672\n",
      "Training Epoch: 2 [300/13482]\tLoss: 5274.1953\n",
      "Training Epoch: 2 [350/13482]\tLoss: 5227.3262\n",
      "Training Epoch: 2 [400/13482]\tLoss: 4856.8599\n",
      "Training Epoch: 2 [450/13482]\tLoss: 5290.0474\n",
      "Training Epoch: 2 [500/13482]\tLoss: 5133.9922\n",
      "Training Epoch: 2 [550/13482]\tLoss: 5056.6797\n",
      "Training Epoch: 2 [600/13482]\tLoss: 4876.9507\n",
      "Training Epoch: 2 [650/13482]\tLoss: 5066.2500\n",
      "Training Epoch: 2 [700/13482]\tLoss: 5042.8252\n",
      "Training Epoch: 2 [750/13482]\tLoss: 4858.9346\n",
      "Training Epoch: 2 [800/13482]\tLoss: 4952.0562\n",
      "Training Epoch: 2 [850/13482]\tLoss: 5089.8394\n",
      "Training Epoch: 2 [900/13482]\tLoss: 4882.9648\n",
      "Training Epoch: 2 [950/13482]\tLoss: 4904.0967\n",
      "Training Epoch: 2 [1000/13482]\tLoss: 5195.5176\n",
      "Training Epoch: 2 [1050/13482]\tLoss: 5092.2002\n",
      "Training Epoch: 2 [1100/13482]\tLoss: 5133.1973\n",
      "Training Epoch: 2 [1150/13482]\tLoss: 4872.6855\n",
      "Training Epoch: 2 [1200/13482]\tLoss: 4687.5894\n",
      "Training Epoch: 2 [1250/13482]\tLoss: 4886.9111\n",
      "Training Epoch: 2 [1300/13482]\tLoss: 4607.5356\n",
      "Training Epoch: 2 [1350/13482]\tLoss: 4726.0322\n",
      "Training Epoch: 2 [1400/13482]\tLoss: 4842.5664\n",
      "Training Epoch: 2 [1450/13482]\tLoss: 4828.5200\n",
      "Training Epoch: 2 [1500/13482]\tLoss: 4835.5918\n",
      "Training Epoch: 2 [1550/13482]\tLoss: 4899.0117\n",
      "Training Epoch: 2 [1600/13482]\tLoss: 4773.3457\n",
      "Training Epoch: 2 [1650/13482]\tLoss: 4696.9463\n",
      "Training Epoch: 2 [1700/13482]\tLoss: 4705.3638\n",
      "Training Epoch: 2 [1750/13482]\tLoss: 4752.4175\n",
      "Training Epoch: 2 [1800/13482]\tLoss: 5102.8965\n",
      "Training Epoch: 2 [1850/13482]\tLoss: 4747.5439\n",
      "Training Epoch: 2 [1900/13482]\tLoss: 4740.1396\n",
      "Training Epoch: 2 [1950/13482]\tLoss: 4668.0195\n",
      "Training Epoch: 2 [2000/13482]\tLoss: 4706.8008\n",
      "Training Epoch: 2 [2050/13482]\tLoss: 4688.4492\n",
      "Training Epoch: 2 [2100/13482]\tLoss: 4880.2651\n",
      "Training Epoch: 2 [2150/13482]\tLoss: 4715.2476\n",
      "Training Epoch: 2 [2200/13482]\tLoss: 4535.2021\n",
      "Training Epoch: 2 [2250/13482]\tLoss: 4563.5005\n",
      "Training Epoch: 2 [2300/13482]\tLoss: 4420.0518\n",
      "Training Epoch: 2 [2350/13482]\tLoss: 4737.3638\n",
      "Training Epoch: 2 [2400/13482]\tLoss: 4706.6201\n",
      "Training Epoch: 2 [2450/13482]\tLoss: 4623.8574\n",
      "Training Epoch: 2 [2500/13482]\tLoss: 4622.8779\n",
      "Training Epoch: 2 [2550/13482]\tLoss: 4615.2759\n",
      "Training Epoch: 2 [2600/13482]\tLoss: 4571.0483\n",
      "Training Epoch: 2 [2650/13482]\tLoss: 4312.1577\n",
      "Training Epoch: 2 [2700/13482]\tLoss: 4722.5444\n",
      "Training Epoch: 2 [2750/13482]\tLoss: 4595.5981\n",
      "Training Epoch: 2 [2800/13482]\tLoss: 4445.8291\n",
      "Training Epoch: 2 [2850/13482]\tLoss: 4954.7812\n",
      "Training Epoch: 2 [2900/13482]\tLoss: 4575.7412\n",
      "Training Epoch: 2 [2950/13482]\tLoss: 4738.9346\n",
      "Training Epoch: 2 [3000/13482]\tLoss: 4514.9502\n",
      "Training Epoch: 2 [3050/13482]\tLoss: 4424.0991\n",
      "Training Epoch: 2 [3100/13482]\tLoss: 4339.7231\n",
      "Training Epoch: 2 [3150/13482]\tLoss: 4440.8057\n",
      "Training Epoch: 2 [3200/13482]\tLoss: 4382.3721\n",
      "Training Epoch: 2 [3250/13482]\tLoss: 4486.9414\n",
      "Training Epoch: 2 [3300/13482]\tLoss: 4537.5635\n",
      "Training Epoch: 2 [3350/13482]\tLoss: 4608.9702\n",
      "Training Epoch: 2 [3400/13482]\tLoss: 4599.1543\n",
      "Training Epoch: 2 [3450/13482]\tLoss: 4556.6792\n",
      "Training Epoch: 2 [3500/13482]\tLoss: 4380.3921\n",
      "Training Epoch: 2 [3550/13482]\tLoss: 4404.9272\n",
      "Training Epoch: 2 [3600/13482]\tLoss: 4604.7300\n",
      "Training Epoch: 2 [3650/13482]\tLoss: 4332.7959\n",
      "Training Epoch: 2 [3700/13482]\tLoss: 4186.8052\n",
      "Training Epoch: 2 [3750/13482]\tLoss: 4285.0181\n",
      "Training Epoch: 2 [3800/13482]\tLoss: 4499.0093\n",
      "Training Epoch: 2 [3850/13482]\tLoss: 4463.1885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [3900/13482]\tLoss: 4521.2158\n",
      "Training Epoch: 2 [3950/13482]\tLoss: 4411.3242\n",
      "Training Epoch: 2 [4000/13482]\tLoss: 4321.6499\n",
      "Training Epoch: 2 [4050/13482]\tLoss: 4349.6797\n",
      "Training Epoch: 2 [4100/13482]\tLoss: 4174.0342\n",
      "Training Epoch: 2 [4150/13482]\tLoss: 4254.9570\n",
      "Training Epoch: 2 [4200/13482]\tLoss: 4342.5776\n",
      "Training Epoch: 2 [4250/13482]\tLoss: 4499.0830\n",
      "Training Epoch: 2 [4300/13482]\tLoss: 4384.2261\n",
      "Training Epoch: 2 [4350/13482]\tLoss: 4330.2275\n",
      "Training Epoch: 2 [4400/13482]\tLoss: 4354.7412\n",
      "Training Epoch: 2 [4450/13482]\tLoss: 4184.8809\n",
      "Training Epoch: 2 [4500/13482]\tLoss: 4147.5303\n",
      "Training Epoch: 2 [4550/13482]\tLoss: 4009.1179\n",
      "Training Epoch: 2 [4600/13482]\tLoss: 4016.0684\n",
      "Training Epoch: 2 [4650/13482]\tLoss: 4291.1372\n",
      "Training Epoch: 2 [4700/13482]\tLoss: 4140.5488\n",
      "Training Epoch: 2 [4750/13482]\tLoss: 4340.7466\n",
      "Training Epoch: 2 [4800/13482]\tLoss: 3970.8330\n",
      "Training Epoch: 2 [4850/13482]\tLoss: 3922.4495\n",
      "Training Epoch: 2 [4900/13482]\tLoss: 4205.3379\n",
      "Training Epoch: 2 [4950/13482]\tLoss: 4167.0283\n",
      "Training Epoch: 2 [5000/13482]\tLoss: 4288.4316\n",
      "Training Epoch: 2 [5050/13482]\tLoss: 4287.4526\n",
      "Training Epoch: 2 [5100/13482]\tLoss: 3946.9121\n",
      "Training Epoch: 2 [5150/13482]\tLoss: 4077.6052\n",
      "Training Epoch: 2 [5200/13482]\tLoss: 4232.1719\n",
      "Training Epoch: 2 [5250/13482]\tLoss: 3995.6814\n",
      "Training Epoch: 2 [5300/13482]\tLoss: 4005.0222\n",
      "Training Epoch: 2 [5350/13482]\tLoss: 4042.0056\n",
      "Training Epoch: 2 [5400/13482]\tLoss: 4289.2759\n",
      "Training Epoch: 2 [5450/13482]\tLoss: 3984.8342\n",
      "Training Epoch: 2 [5500/13482]\tLoss: 3897.2754\n",
      "Training Epoch: 2 [5550/13482]\tLoss: 3829.5955\n",
      "Training Epoch: 2 [5600/13482]\tLoss: 4024.1365\n",
      "Training Epoch: 2 [5650/13482]\tLoss: 4066.1560\n",
      "Training Epoch: 2 [5700/13482]\tLoss: 4070.8103\n",
      "Training Epoch: 2 [5750/13482]\tLoss: 4092.8782\n",
      "Training Epoch: 2 [5800/13482]\tLoss: 4017.2788\n",
      "Training Epoch: 2 [5850/13482]\tLoss: 4110.4224\n",
      "Training Epoch: 2 [5900/13482]\tLoss: 4029.3655\n",
      "Training Epoch: 2 [5950/13482]\tLoss: 4094.4990\n",
      "Training Epoch: 2 [6000/13482]\tLoss: 3909.0081\n",
      "Training Epoch: 2 [6050/13482]\tLoss: 3985.7297\n",
      "Training Epoch: 2 [6100/13482]\tLoss: 3870.9312\n",
      "Training Epoch: 2 [6150/13482]\tLoss: 4062.6189\n",
      "Training Epoch: 2 [6200/13482]\tLoss: 4138.9912\n",
      "Training Epoch: 2 [6250/13482]\tLoss: 3927.4866\n",
      "Training Epoch: 2 [6300/13482]\tLoss: 3882.1138\n",
      "Training Epoch: 2 [6350/13482]\tLoss: 4156.0024\n",
      "Training Epoch: 2 [6400/13482]\tLoss: 3980.2432\n",
      "Training Epoch: 2 [6450/13482]\tLoss: 3867.6599\n",
      "Training Epoch: 2 [6500/13482]\tLoss: 3929.4656\n",
      "Training Epoch: 2 [6550/13482]\tLoss: 3948.8604\n",
      "Training Epoch: 2 [6600/13482]\tLoss: 3644.0579\n",
      "Training Epoch: 2 [6650/13482]\tLoss: 3920.3425\n",
      "Training Epoch: 2 [6700/13482]\tLoss: 3852.5386\n",
      "Training Epoch: 2 [6750/13482]\tLoss: 3865.8137\n",
      "Training Epoch: 2 [6800/13482]\tLoss: 3847.8599\n",
      "Training Epoch: 2 [6850/13482]\tLoss: 3873.8997\n",
      "Training Epoch: 2 [6900/13482]\tLoss: 3843.3074\n",
      "Training Epoch: 2 [6950/13482]\tLoss: 3807.4907\n",
      "Training Epoch: 2 [7000/13482]\tLoss: 3826.4568\n",
      "Training Epoch: 2 [7050/13482]\tLoss: 4031.4792\n",
      "Training Epoch: 2 [7100/13482]\tLoss: 3816.4714\n",
      "Training Epoch: 2 [7150/13482]\tLoss: 3856.9014\n",
      "Training Epoch: 2 [7200/13482]\tLoss: 4008.8667\n",
      "Training Epoch: 2 [7250/13482]\tLoss: 3808.4500\n",
      "Training Epoch: 2 [7300/13482]\tLoss: 3766.4268\n",
      "Training Epoch: 2 [7350/13482]\tLoss: 3736.0552\n",
      "Training Epoch: 2 [7400/13482]\tLoss: 3824.5120\n",
      "Training Epoch: 2 [7450/13482]\tLoss: 3780.9912\n",
      "Training Epoch: 2 [7500/13482]\tLoss: 3680.0457\n",
      "Training Epoch: 2 [7550/13482]\tLoss: 3773.4280\n",
      "Training Epoch: 2 [7600/13482]\tLoss: 3852.1340\n",
      "Training Epoch: 2 [7650/13482]\tLoss: 3748.8071\n",
      "Training Epoch: 2 [7700/13482]\tLoss: 3754.8047\n",
      "Training Epoch: 2 [7750/13482]\tLoss: 3760.9680\n",
      "Training Epoch: 2 [7800/13482]\tLoss: 3861.7595\n",
      "Training Epoch: 2 [7850/13482]\tLoss: 3794.1201\n",
      "Training Epoch: 2 [7900/13482]\tLoss: 3865.3647\n",
      "Training Epoch: 2 [7950/13482]\tLoss: 3841.9207\n",
      "Training Epoch: 2 [8000/13482]\tLoss: 3615.0559\n",
      "Training Epoch: 2 [8050/13482]\tLoss: 3766.6101\n",
      "Training Epoch: 2 [8100/13482]\tLoss: 3761.7725\n",
      "Training Epoch: 2 [8150/13482]\tLoss: 3866.9297\n",
      "Training Epoch: 2 [8200/13482]\tLoss: 3642.2964\n",
      "Training Epoch: 2 [8250/13482]\tLoss: 3579.9150\n",
      "Training Epoch: 2 [8300/13482]\tLoss: 3757.9141\n",
      "Training Epoch: 2 [8350/13482]\tLoss: 3689.3535\n",
      "Training Epoch: 2 [8400/13482]\tLoss: 3784.8901\n",
      "Training Epoch: 2 [8450/13482]\tLoss: 3721.4385\n",
      "Training Epoch: 2 [8500/13482]\tLoss: 3663.6846\n",
      "Training Epoch: 2 [8550/13482]\tLoss: 3560.4583\n",
      "Training Epoch: 2 [8600/13482]\tLoss: 3600.5203\n",
      "Training Epoch: 2 [8650/13482]\tLoss: 3746.2119\n",
      "Training Epoch: 2 [8700/13482]\tLoss: 3605.9565\n",
      "Training Epoch: 2 [8750/13482]\tLoss: 3586.8516\n",
      "Training Epoch: 2 [8800/13482]\tLoss: 3764.2358\n",
      "Training Epoch: 2 [8850/13482]\tLoss: 3477.6697\n",
      "Training Epoch: 2 [8900/13482]\tLoss: 3704.2432\n",
      "Training Epoch: 2 [8950/13482]\tLoss: 3693.0552\n",
      "Training Epoch: 2 [9000/13482]\tLoss: 3350.4346\n",
      "Training Epoch: 2 [9050/13482]\tLoss: 3650.5168\n",
      "Training Epoch: 2 [9100/13482]\tLoss: 3423.3672\n",
      "Training Epoch: 2 [9150/13482]\tLoss: 3598.6182\n",
      "Training Epoch: 2 [9200/13482]\tLoss: 3676.8960\n",
      "Training Epoch: 2 [9250/13482]\tLoss: 3521.4387\n",
      "Training Epoch: 2 [9300/13482]\tLoss: 3592.7058\n",
      "Training Epoch: 2 [9350/13482]\tLoss: 3655.7302\n",
      "Training Epoch: 2 [9400/13482]\tLoss: 3482.7493\n",
      "Training Epoch: 2 [9450/13482]\tLoss: 3714.8342\n",
      "Training Epoch: 2 [9500/13482]\tLoss: 3533.0037\n",
      "Training Epoch: 2 [9550/13482]\tLoss: 3645.2429\n",
      "Training Epoch: 2 [9600/13482]\tLoss: 3715.2434\n",
      "Training Epoch: 2 [9650/13482]\tLoss: 3620.9639\n",
      "Training Epoch: 2 [9700/13482]\tLoss: 3632.6101\n",
      "Training Epoch: 2 [9750/13482]\tLoss: 3536.8384\n",
      "Training Epoch: 2 [9800/13482]\tLoss: 3540.2168\n",
      "Training Epoch: 2 [9850/13482]\tLoss: 3507.5132\n",
      "Training Epoch: 2 [9900/13482]\tLoss: 3471.8962\n",
      "Training Epoch: 2 [9950/13482]\tLoss: 3638.7759\n",
      "Training Epoch: 2 [10000/13482]\tLoss: 3577.0557\n",
      "Training Epoch: 2 [10050/13482]\tLoss: 3583.7673\n",
      "Training Epoch: 2 [10100/13482]\tLoss: 3325.2017\n",
      "Training Epoch: 2 [10150/13482]\tLoss: 3530.2642\n",
      "Training Epoch: 2 [10200/13482]\tLoss: 3633.5466\n",
      "Training Epoch: 2 [10250/13482]\tLoss: 3600.7971\n",
      "Training Epoch: 2 [10300/13482]\tLoss: 3539.5857\n",
      "Training Epoch: 2 [10350/13482]\tLoss: 3491.8872\n",
      "Training Epoch: 2 [10400/13482]\tLoss: 3459.7229\n",
      "Training Epoch: 2 [10450/13482]\tLoss: 3536.6816\n",
      "Training Epoch: 2 [10500/13482]\tLoss: 3685.8555\n",
      "Training Epoch: 2 [10550/13482]\tLoss: 3530.4150\n",
      "Training Epoch: 2 [10600/13482]\tLoss: 3361.4482\n",
      "Training Epoch: 2 [10650/13482]\tLoss: 3317.2881\n",
      "Training Epoch: 2 [10700/13482]\tLoss: 3465.5171\n",
      "Training Epoch: 2 [10750/13482]\tLoss: 3493.7664\n",
      "Training Epoch: 2 [10800/13482]\tLoss: 3458.2505\n",
      "Training Epoch: 2 [10850/13482]\tLoss: 3295.9644\n",
      "Training Epoch: 2 [10900/13482]\tLoss: 3447.3638\n",
      "Training Epoch: 2 [10950/13482]\tLoss: 3328.1001\n",
      "Training Epoch: 2 [11000/13482]\tLoss: 3373.2632\n",
      "Training Epoch: 2 [11050/13482]\tLoss: 3386.2466\n",
      "Training Epoch: 2 [11100/13482]\tLoss: 3326.3157\n",
      "Training Epoch: 2 [11150/13482]\tLoss: 3406.9272\n",
      "Training Epoch: 2 [11200/13482]\tLoss: 3432.8123\n",
      "Training Epoch: 2 [11250/13482]\tLoss: 3466.4302\n",
      "Training Epoch: 2 [11300/13482]\tLoss: 3370.1919\n",
      "Training Epoch: 2 [11350/13482]\tLoss: 3449.1685\n",
      "Training Epoch: 2 [11400/13482]\tLoss: 3436.5032\n",
      "Training Epoch: 2 [11450/13482]\tLoss: 3291.2239\n",
      "Training Epoch: 2 [11500/13482]\tLoss: 3331.9604\n",
      "Training Epoch: 2 [11550/13482]\tLoss: 3363.7322\n",
      "Training Epoch: 2 [11600/13482]\tLoss: 3405.8530\n",
      "Training Epoch: 2 [11650/13482]\tLoss: 3284.6846\n",
      "Training Epoch: 2 [11700/13482]\tLoss: 3281.6868\n",
      "Training Epoch: 2 [11750/13482]\tLoss: 3145.4028\n",
      "Training Epoch: 2 [11800/13482]\tLoss: 3387.1074\n",
      "Training Epoch: 2 [11850/13482]\tLoss: 3258.3081\n",
      "Training Epoch: 2 [11900/13482]\tLoss: 3286.7336\n",
      "Training Epoch: 2 [11950/13482]\tLoss: 3354.1738\n",
      "Training Epoch: 2 [12000/13482]\tLoss: 3302.4141\n",
      "Training Epoch: 2 [12050/13482]\tLoss: 3377.0557\n",
      "Training Epoch: 2 [12100/13482]\tLoss: 3375.0806\n",
      "Training Epoch: 2 [12150/13482]\tLoss: 3441.9153\n",
      "Training Epoch: 2 [12200/13482]\tLoss: 3283.0134\n",
      "Training Epoch: 2 [12250/13482]\tLoss: 3323.5320\n",
      "Training Epoch: 2 [12300/13482]\tLoss: 3298.9592\n",
      "Training Epoch: 2 [12350/13482]\tLoss: 3349.1907\n",
      "Training Epoch: 2 [12400/13482]\tLoss: 3326.7251\n",
      "Training Epoch: 2 [12450/13482]\tLoss: 3225.7144\n",
      "Training Epoch: 2 [12500/13482]\tLoss: 3375.4126\n",
      "Training Epoch: 2 [12550/13482]\tLoss: 3295.6238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [12600/13482]\tLoss: 3219.5972\n",
      "Training Epoch: 2 [12650/13482]\tLoss: 3149.7546\n",
      "Training Epoch: 2 [12700/13482]\tLoss: 3181.9995\n",
      "Training Epoch: 2 [12750/13482]\tLoss: 3214.3687\n",
      "Training Epoch: 2 [12800/13482]\tLoss: 3298.6218\n",
      "Training Epoch: 2 [12850/13482]\tLoss: 3166.3269\n",
      "Training Epoch: 2 [12900/13482]\tLoss: 3310.7627\n",
      "Training Epoch: 2 [12950/13482]\tLoss: 3314.7268\n",
      "Training Epoch: 2 [13000/13482]\tLoss: 3145.5649\n",
      "Training Epoch: 2 [13050/13482]\tLoss: 3180.5891\n",
      "Training Epoch: 2 [13100/13482]\tLoss: 3173.1042\n",
      "Training Epoch: 2 [13150/13482]\tLoss: 3121.3225\n",
      "Training Epoch: 2 [13200/13482]\tLoss: 3215.6448\n",
      "Training Epoch: 2 [13250/13482]\tLoss: 3370.7686\n",
      "Training Epoch: 2 [13300/13482]\tLoss: 3256.5464\n",
      "Training Epoch: 2 [13350/13482]\tLoss: 3276.1116\n",
      "Training Epoch: 2 [13400/13482]\tLoss: 3341.3123\n",
      "Training Epoch: 2 [13450/13482]\tLoss: 3272.2568\n",
      "Training Epoch: 2 [13482/13482]\tLoss: 3237.9854\n",
      "Training Epoch: 2 [1497/1497]\tLoss: 3173.1875\n",
      "Training Epoch: 3 [50/13482]\tLoss: 3134.3833\n",
      "Training Epoch: 3 [100/13482]\tLoss: 3148.7759\n",
      "Training Epoch: 3 [150/13482]\tLoss: 3217.0439\n",
      "Training Epoch: 3 [200/13482]\tLoss: 3286.5610\n",
      "Training Epoch: 3 [250/13482]\tLoss: 3166.8118\n",
      "Training Epoch: 3 [300/13482]\tLoss: 3244.2129\n",
      "Training Epoch: 3 [350/13482]\tLoss: 3211.0735\n",
      "Training Epoch: 3 [400/13482]\tLoss: 3023.6865\n",
      "Training Epoch: 3 [450/13482]\tLoss: 3201.0181\n",
      "Training Epoch: 3 [500/13482]\tLoss: 3151.0747\n",
      "Training Epoch: 3 [550/13482]\tLoss: 3165.1970\n",
      "Training Epoch: 3 [600/13482]\tLoss: 3080.5911\n",
      "Training Epoch: 3 [650/13482]\tLoss: 3152.2854\n",
      "Training Epoch: 3 [700/13482]\tLoss: 3139.3250\n",
      "Training Epoch: 3 [750/13482]\tLoss: 3049.1570\n",
      "Training Epoch: 3 [800/13482]\tLoss: 3133.3792\n",
      "Training Epoch: 3 [850/13482]\tLoss: 3187.3198\n",
      "Training Epoch: 3 [900/13482]\tLoss: 3064.0444\n",
      "Training Epoch: 3 [950/13482]\tLoss: 3099.9968\n",
      "Training Epoch: 3 [1000/13482]\tLoss: 3250.6084\n",
      "Training Epoch: 3 [1050/13482]\tLoss: 3204.5896\n",
      "Training Epoch: 3 [1100/13482]\tLoss: 3196.6577\n",
      "Training Epoch: 3 [1150/13482]\tLoss: 3091.2234\n",
      "Training Epoch: 3 [1200/13482]\tLoss: 3005.2568\n",
      "Training Epoch: 3 [1250/13482]\tLoss: 3113.9304\n",
      "Training Epoch: 3 [1300/13482]\tLoss: 2934.8953\n",
      "Training Epoch: 3 [1350/13482]\tLoss: 2989.3716\n",
      "Training Epoch: 3 [1400/13482]\tLoss: 3084.8672\n",
      "Training Epoch: 3 [1450/13482]\tLoss: 3045.0212\n",
      "Training Epoch: 3 [1500/13482]\tLoss: 3036.6624\n",
      "Training Epoch: 3 [1550/13482]\tLoss: 3103.4514\n",
      "Training Epoch: 3 [1600/13482]\tLoss: 3013.1653\n",
      "Training Epoch: 3 [1650/13482]\tLoss: 2981.0957\n",
      "Training Epoch: 3 [1700/13482]\tLoss: 3011.4873\n",
      "Training Epoch: 3 [1750/13482]\tLoss: 3034.4165\n",
      "Training Epoch: 3 [1800/13482]\tLoss: 3244.1484\n",
      "Training Epoch: 3 [1850/13482]\tLoss: 3019.6406\n",
      "Training Epoch: 3 [1900/13482]\tLoss: 3028.3628\n",
      "Training Epoch: 3 [1950/13482]\tLoss: 3028.9888\n",
      "Training Epoch: 3 [2000/13482]\tLoss: 3021.8696\n",
      "Training Epoch: 3 [2050/13482]\tLoss: 3011.8940\n",
      "Training Epoch: 3 [2100/13482]\tLoss: 3099.7144\n",
      "Training Epoch: 3 [2150/13482]\tLoss: 3041.1477\n",
      "Training Epoch: 3 [2200/13482]\tLoss: 2917.9360\n",
      "Training Epoch: 3 [2250/13482]\tLoss: 2969.2795\n",
      "Training Epoch: 3 [2300/13482]\tLoss: 2886.2319\n",
      "Training Epoch: 3 [2350/13482]\tLoss: 3055.7644\n",
      "Training Epoch: 3 [2400/13482]\tLoss: 2998.4436\n",
      "Training Epoch: 3 [2450/13482]\tLoss: 3012.6458\n",
      "Training Epoch: 3 [2500/13482]\tLoss: 2984.7512\n",
      "Training Epoch: 3 [2550/13482]\tLoss: 2996.2217\n",
      "Training Epoch: 3 [2600/13482]\tLoss: 2981.9763\n",
      "Training Epoch: 3 [2650/13482]\tLoss: 2842.3047\n",
      "Training Epoch: 3 [2700/13482]\tLoss: 3081.4158\n",
      "Training Epoch: 3 [2750/13482]\tLoss: 2978.3296\n",
      "Training Epoch: 3 [2800/13482]\tLoss: 2902.3896\n",
      "Training Epoch: 3 [2850/13482]\tLoss: 3226.3376\n",
      "Training Epoch: 3 [2900/13482]\tLoss: 3002.5808\n",
      "Training Epoch: 3 [2950/13482]\tLoss: 3066.8667\n",
      "Training Epoch: 3 [3000/13482]\tLoss: 2954.3335\n",
      "Training Epoch: 3 [3050/13482]\tLoss: 2930.8450\n",
      "Training Epoch: 3 [3100/13482]\tLoss: 2847.7021\n",
      "Training Epoch: 3 [3150/13482]\tLoss: 2901.5115\n",
      "Training Epoch: 3 [3200/13482]\tLoss: 2916.0115\n",
      "Training Epoch: 3 [3250/13482]\tLoss: 2943.6899\n",
      "Training Epoch: 3 [3300/13482]\tLoss: 3004.3459\n",
      "Training Epoch: 3 [3350/13482]\tLoss: 3042.6836\n",
      "Training Epoch: 3 [3400/13482]\tLoss: 2995.4951\n",
      "Training Epoch: 3 [3450/13482]\tLoss: 2985.1313\n",
      "Training Epoch: 3 [3500/13482]\tLoss: 2925.3757\n",
      "Training Epoch: 3 [3550/13482]\tLoss: 2938.9041\n",
      "Training Epoch: 3 [3600/13482]\tLoss: 3065.6272\n",
      "Training Epoch: 3 [3650/13482]\tLoss: 2858.6074\n",
      "Training Epoch: 3 [3700/13482]\tLoss: 2808.1201\n",
      "Training Epoch: 3 [3750/13482]\tLoss: 2876.6887\n",
      "Training Epoch: 3 [3800/13482]\tLoss: 2978.0085\n",
      "Training Epoch: 3 [3850/13482]\tLoss: 2980.1379\n",
      "Training Epoch: 3 [3900/13482]\tLoss: 3006.0300\n",
      "Training Epoch: 3 [3950/13482]\tLoss: 2924.7415\n",
      "Training Epoch: 3 [4000/13482]\tLoss: 2902.0942\n",
      "Training Epoch: 3 [4050/13482]\tLoss: 2876.7791\n",
      "Training Epoch: 3 [4100/13482]\tLoss: 2841.6804\n",
      "Training Epoch: 3 [4150/13482]\tLoss: 2880.3403\n",
      "Training Epoch: 3 [4200/13482]\tLoss: 2907.5088\n",
      "Training Epoch: 3 [4250/13482]\tLoss: 3001.5056\n",
      "Training Epoch: 3 [4300/13482]\tLoss: 2944.0417\n",
      "Training Epoch: 3 [4350/13482]\tLoss: 2890.7825\n",
      "Training Epoch: 3 [4400/13482]\tLoss: 2934.7280\n",
      "Training Epoch: 3 [4450/13482]\tLoss: 2822.0728\n",
      "Training Epoch: 3 [4500/13482]\tLoss: 2832.4600\n",
      "Training Epoch: 3 [4550/13482]\tLoss: 2737.0925\n",
      "Training Epoch: 3 [4600/13482]\tLoss: 2760.4114\n",
      "Training Epoch: 3 [4650/13482]\tLoss: 2894.7222\n",
      "Training Epoch: 3 [4700/13482]\tLoss: 2834.2793\n",
      "Training Epoch: 3 [4750/13482]\tLoss: 2931.7385\n",
      "Training Epoch: 3 [4800/13482]\tLoss: 2760.1345\n",
      "Training Epoch: 3 [4850/13482]\tLoss: 2710.3521\n",
      "Training Epoch: 3 [4900/13482]\tLoss: 2886.7039\n",
      "Training Epoch: 3 [4950/13482]\tLoss: 2866.6187\n",
      "Training Epoch: 3 [5000/13482]\tLoss: 2911.7363\n",
      "Training Epoch: 3 [5050/13482]\tLoss: 2914.2383\n",
      "Training Epoch: 3 [5100/13482]\tLoss: 2726.2512\n",
      "Training Epoch: 3 [5150/13482]\tLoss: 2796.3662\n",
      "Training Epoch: 3 [5200/13482]\tLoss: 2894.2012\n",
      "Training Epoch: 3 [5250/13482]\tLoss: 2775.7205\n",
      "Training Epoch: 3 [5300/13482]\tLoss: 2774.0239\n",
      "Training Epoch: 3 [5350/13482]\tLoss: 2772.7000\n",
      "Training Epoch: 3 [5400/13482]\tLoss: 2931.7126\n",
      "Training Epoch: 3 [5450/13482]\tLoss: 2767.0911\n",
      "Training Epoch: 3 [5500/13482]\tLoss: 2752.8635\n",
      "Training Epoch: 3 [5550/13482]\tLoss: 2712.4258\n",
      "Training Epoch: 3 [5600/13482]\tLoss: 2762.0210\n",
      "Training Epoch: 3 [5650/13482]\tLoss: 2799.1531\n",
      "Training Epoch: 3 [5700/13482]\tLoss: 2791.3416\n",
      "Training Epoch: 3 [5750/13482]\tLoss: 2840.6707\n",
      "Training Epoch: 3 [5800/13482]\tLoss: 2806.0183\n",
      "Training Epoch: 3 [5850/13482]\tLoss: 2887.8347\n",
      "Training Epoch: 3 [5900/13482]\tLoss: 2792.5896\n",
      "Training Epoch: 3 [5950/13482]\tLoss: 2834.4717\n",
      "Training Epoch: 3 [6000/13482]\tLoss: 2752.4707\n",
      "Training Epoch: 3 [6050/13482]\tLoss: 2797.9954\n",
      "Training Epoch: 3 [6100/13482]\tLoss: 2740.6787\n",
      "Training Epoch: 3 [6150/13482]\tLoss: 2845.6968\n",
      "Training Epoch: 3 [6200/13482]\tLoss: 2906.9663\n",
      "Training Epoch: 3 [6250/13482]\tLoss: 2754.8447\n",
      "Training Epoch: 3 [6300/13482]\tLoss: 2688.7717\n",
      "Training Epoch: 3 [6350/13482]\tLoss: 2902.3328\n",
      "Training Epoch: 3 [6400/13482]\tLoss: 2778.6245\n",
      "Training Epoch: 3 [6450/13482]\tLoss: 2724.3640\n",
      "Training Epoch: 3 [6500/13482]\tLoss: 2733.6843\n",
      "Training Epoch: 3 [6550/13482]\tLoss: 2801.8096\n",
      "Training Epoch: 3 [6600/13482]\tLoss: 2606.8467\n",
      "Training Epoch: 3 [6650/13482]\tLoss: 2788.9575\n",
      "Training Epoch: 3 [6700/13482]\tLoss: 2714.0786\n",
      "Training Epoch: 3 [6750/13482]\tLoss: 2728.7153\n",
      "Training Epoch: 3 [6800/13482]\tLoss: 2717.3840\n",
      "Training Epoch: 3 [6850/13482]\tLoss: 2714.3042\n",
      "Training Epoch: 3 [6900/13482]\tLoss: 2722.0403\n",
      "Training Epoch: 3 [6950/13482]\tLoss: 2695.8521\n",
      "Training Epoch: 3 [7000/13482]\tLoss: 2727.6436\n",
      "Training Epoch: 3 [7050/13482]\tLoss: 2844.2676\n",
      "Training Epoch: 3 [7100/13482]\tLoss: 2726.1511\n",
      "Training Epoch: 3 [7150/13482]\tLoss: 2750.1570\n",
      "Training Epoch: 3 [7200/13482]\tLoss: 2808.7078\n",
      "Training Epoch: 3 [7250/13482]\tLoss: 2709.8730\n",
      "Training Epoch: 3 [7300/13482]\tLoss: 2682.8342\n",
      "Training Epoch: 3 [7350/13482]\tLoss: 2665.5469\n",
      "Training Epoch: 3 [7400/13482]\tLoss: 2741.2112\n",
      "Training Epoch: 3 [7450/13482]\tLoss: 2710.0959\n",
      "Training Epoch: 3 [7500/13482]\tLoss: 2636.4424\n",
      "Training Epoch: 3 [7550/13482]\tLoss: 2716.2136\n",
      "Training Epoch: 3 [7600/13482]\tLoss: 2744.7888\n",
      "Training Epoch: 3 [7650/13482]\tLoss: 2707.0962\n",
      "Training Epoch: 3 [7700/13482]\tLoss: 2682.4121\n",
      "Training Epoch: 3 [7750/13482]\tLoss: 2689.2410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [7800/13482]\tLoss: 2762.3594\n",
      "Training Epoch: 3 [7850/13482]\tLoss: 2732.4092\n",
      "Training Epoch: 3 [7900/13482]\tLoss: 2751.5593\n",
      "Training Epoch: 3 [7950/13482]\tLoss: 2752.1106\n",
      "Training Epoch: 3 [8000/13482]\tLoss: 2606.4763\n",
      "Training Epoch: 3 [8050/13482]\tLoss: 2730.6392\n",
      "Training Epoch: 3 [8100/13482]\tLoss: 2711.8989\n",
      "Training Epoch: 3 [8150/13482]\tLoss: 2768.2273\n",
      "Training Epoch: 3 [8200/13482]\tLoss: 2640.6934\n",
      "Training Epoch: 3 [8250/13482]\tLoss: 2603.0269\n",
      "Training Epoch: 3 [8300/13482]\tLoss: 2703.3542\n",
      "Training Epoch: 3 [8350/13482]\tLoss: 2672.4053\n",
      "Training Epoch: 3 [8400/13482]\tLoss: 2740.9822\n",
      "Training Epoch: 3 [8450/13482]\tLoss: 2675.0593\n",
      "Training Epoch: 3 [8500/13482]\tLoss: 2656.1238\n",
      "Training Epoch: 3 [8550/13482]\tLoss: 2597.9863\n",
      "Training Epoch: 3 [8600/13482]\tLoss: 2637.7417\n",
      "Training Epoch: 3 [8650/13482]\tLoss: 2726.5886\n",
      "Training Epoch: 3 [8700/13482]\tLoss: 2624.5857\n",
      "Training Epoch: 3 [8750/13482]\tLoss: 2630.8455\n",
      "Training Epoch: 3 [8800/13482]\tLoss: 2731.5029\n",
      "Training Epoch: 3 [8850/13482]\tLoss: 2559.7427\n",
      "Training Epoch: 3 [8900/13482]\tLoss: 2671.4907\n",
      "Training Epoch: 3 [8950/13482]\tLoss: 2685.6816\n",
      "Training Epoch: 3 [9000/13482]\tLoss: 2523.6936\n",
      "Training Epoch: 3 [9050/13482]\tLoss: 2668.5623\n",
      "Training Epoch: 3 [9100/13482]\tLoss: 2541.8057\n",
      "Training Epoch: 3 [9150/13482]\tLoss: 2658.3408\n",
      "Training Epoch: 3 [9200/13482]\tLoss: 2698.7004\n",
      "Training Epoch: 3 [9250/13482]\tLoss: 2579.7905\n",
      "Training Epoch: 3 [9300/13482]\tLoss: 2649.9670\n",
      "Training Epoch: 3 [9350/13482]\tLoss: 2680.5334\n",
      "Training Epoch: 3 [9400/13482]\tLoss: 2554.4597\n",
      "Training Epoch: 3 [9450/13482]\tLoss: 2702.1956\n",
      "Training Epoch: 3 [9500/13482]\tLoss: 2607.4397\n",
      "Training Epoch: 3 [9550/13482]\tLoss: 2670.5681\n",
      "Training Epoch: 3 [9600/13482]\tLoss: 2715.2593\n",
      "Training Epoch: 3 [9650/13482]\tLoss: 2654.0225\n",
      "Training Epoch: 3 [9700/13482]\tLoss: 2677.3640\n",
      "Training Epoch: 3 [9750/13482]\tLoss: 2609.3345\n",
      "Training Epoch: 3 [9800/13482]\tLoss: 2608.1609\n",
      "Training Epoch: 3 [9850/13482]\tLoss: 2594.0283\n",
      "Training Epoch: 3 [9900/13482]\tLoss: 2582.5818\n",
      "Training Epoch: 3 [9950/13482]\tLoss: 2679.0093\n",
      "Training Epoch: 3 [10000/13482]\tLoss: 2646.1580\n",
      "Training Epoch: 3 [10050/13482]\tLoss: 2657.7224\n",
      "Training Epoch: 3 [10100/13482]\tLoss: 2501.4231\n",
      "Training Epoch: 3 [10150/13482]\tLoss: 2619.9775\n",
      "Training Epoch: 3 [10200/13482]\tLoss: 2648.9460\n",
      "Training Epoch: 3 [10250/13482]\tLoss: 2693.9919\n",
      "Training Epoch: 3 [10300/13482]\tLoss: 2633.8032\n",
      "Training Epoch: 3 [10350/13482]\tLoss: 2590.5498\n",
      "Training Epoch: 3 [10400/13482]\tLoss: 2572.2351\n",
      "Training Epoch: 3 [10450/13482]\tLoss: 2619.5667\n",
      "Training Epoch: 3 [10500/13482]\tLoss: 2739.4788\n",
      "Training Epoch: 3 [10550/13482]\tLoss: 2625.8806\n",
      "Training Epoch: 3 [10600/13482]\tLoss: 2537.2866\n",
      "Training Epoch: 3 [10650/13482]\tLoss: 2511.3904\n",
      "Training Epoch: 3 [10700/13482]\tLoss: 2568.2664\n",
      "Training Epoch: 3 [10750/13482]\tLoss: 2624.3491\n",
      "Training Epoch: 3 [10800/13482]\tLoss: 2588.8975\n",
      "Training Epoch: 3 [10850/13482]\tLoss: 2499.0151\n",
      "Training Epoch: 3 [10900/13482]\tLoss: 2577.9836\n",
      "Training Epoch: 3 [10950/13482]\tLoss: 2507.8494\n",
      "Training Epoch: 3 [11000/13482]\tLoss: 2547.5168\n",
      "Training Epoch: 3 [11050/13482]\tLoss: 2568.2524\n",
      "Training Epoch: 3 [11100/13482]\tLoss: 2519.1030\n",
      "Training Epoch: 3 [11150/13482]\tLoss: 2569.1987\n",
      "Training Epoch: 3 [11200/13482]\tLoss: 2559.6631\n",
      "Training Epoch: 3 [11250/13482]\tLoss: 2613.1716\n",
      "Training Epoch: 3 [11300/13482]\tLoss: 2564.0415\n",
      "Training Epoch: 3 [11350/13482]\tLoss: 2606.2896\n",
      "Training Epoch: 3 [11400/13482]\tLoss: 2582.1992\n",
      "Training Epoch: 3 [11450/13482]\tLoss: 2496.3303\n",
      "Training Epoch: 3 [11500/13482]\tLoss: 2533.9729\n",
      "Training Epoch: 3 [11550/13482]\tLoss: 2573.3835\n",
      "Training Epoch: 3 [11600/13482]\tLoss: 2586.7693\n",
      "Training Epoch: 3 [11650/13482]\tLoss: 2513.5076\n",
      "Training Epoch: 3 [11700/13482]\tLoss: 2488.3701\n",
      "Training Epoch: 3 [11750/13482]\tLoss: 2421.9155\n",
      "Training Epoch: 3 [11800/13482]\tLoss: 2567.8298\n",
      "Training Epoch: 3 [11850/13482]\tLoss: 2467.3162\n",
      "Training Epoch: 3 [11900/13482]\tLoss: 2508.8938\n",
      "Training Epoch: 3 [11950/13482]\tLoss: 2554.1614\n",
      "Training Epoch: 3 [12000/13482]\tLoss: 2510.9900\n",
      "Training Epoch: 3 [12050/13482]\tLoss: 2546.3015\n",
      "Training Epoch: 3 [12100/13482]\tLoss: 2581.5803\n",
      "Training Epoch: 3 [12150/13482]\tLoss: 2602.0637\n",
      "Training Epoch: 3 [12200/13482]\tLoss: 2516.0010\n",
      "Training Epoch: 3 [12250/13482]\tLoss: 2529.0684\n",
      "Training Epoch: 3 [12300/13482]\tLoss: 2537.6902\n",
      "Training Epoch: 3 [12350/13482]\tLoss: 2541.6790\n",
      "Training Epoch: 3 [12400/13482]\tLoss: 2536.7979\n",
      "Training Epoch: 3 [12450/13482]\tLoss: 2478.0237\n",
      "Training Epoch: 3 [12500/13482]\tLoss: 2591.7815\n",
      "Training Epoch: 3 [12550/13482]\tLoss: 2520.8821\n",
      "Training Epoch: 3 [12600/13482]\tLoss: 2487.6174\n",
      "Training Epoch: 3 [12650/13482]\tLoss: 2453.4768\n",
      "Training Epoch: 3 [12700/13482]\tLoss: 2462.3950\n",
      "Training Epoch: 3 [12750/13482]\tLoss: 2484.8374\n",
      "Training Epoch: 3 [12800/13482]\tLoss: 2537.6990\n",
      "Training Epoch: 3 [12850/13482]\tLoss: 2468.7185\n",
      "Training Epoch: 3 [12900/13482]\tLoss: 2557.2239\n",
      "Training Epoch: 3 [12950/13482]\tLoss: 2541.6201\n",
      "Training Epoch: 3 [13000/13482]\tLoss: 2458.6482\n",
      "Training Epoch: 3 [13050/13482]\tLoss: 2487.1899\n",
      "Training Epoch: 3 [13100/13482]\tLoss: 2453.8506\n",
      "Training Epoch: 3 [13150/13482]\tLoss: 2441.2068\n",
      "Training Epoch: 3 [13200/13482]\tLoss: 2506.6870\n",
      "Training Epoch: 3 [13250/13482]\tLoss: 2582.3447\n",
      "Training Epoch: 3 [13300/13482]\tLoss: 2522.0713\n",
      "Training Epoch: 3 [13350/13482]\tLoss: 2511.2407\n",
      "Training Epoch: 3 [13400/13482]\tLoss: 2578.2329\n",
      "Training Epoch: 3 [13450/13482]\tLoss: 2544.6609\n",
      "Training Epoch: 3 [13482/13482]\tLoss: 2524.1980\n",
      "Training Epoch: 3 [1497/1497]\tLoss: 2467.4092\n",
      "Training Epoch: 4 [50/13482]\tLoss: 2454.6414\n",
      "Training Epoch: 4 [100/13482]\tLoss: 2478.2466\n",
      "Training Epoch: 4 [150/13482]\tLoss: 2498.1426\n",
      "Training Epoch: 4 [200/13482]\tLoss: 2548.9482\n",
      "Training Epoch: 4 [250/13482]\tLoss: 2470.9424\n",
      "Training Epoch: 4 [300/13482]\tLoss: 2505.0588\n",
      "Training Epoch: 4 [350/13482]\tLoss: 2486.3101\n",
      "Training Epoch: 4 [400/13482]\tLoss: 2392.7239\n",
      "Training Epoch: 4 [450/13482]\tLoss: 2467.1184\n",
      "Training Epoch: 4 [500/13482]\tLoss: 2452.2354\n",
      "Training Epoch: 4 [550/13482]\tLoss: 2481.8552\n",
      "Training Epoch: 4 [600/13482]\tLoss: 2444.4824\n",
      "Training Epoch: 4 [650/13482]\tLoss: 2470.9897\n",
      "Training Epoch: 4 [700/13482]\tLoss: 2473.6143\n",
      "Training Epoch: 4 [750/13482]\tLoss: 2413.7964\n",
      "Training Epoch: 4 [800/13482]\tLoss: 2475.2634\n",
      "Training Epoch: 4 [850/13482]\tLoss: 2485.6409\n",
      "Training Epoch: 4 [900/13482]\tLoss: 2418.7263\n",
      "Training Epoch: 4 [950/13482]\tLoss: 2458.1414\n",
      "Training Epoch: 4 [1000/13482]\tLoss: 2542.9463\n",
      "Training Epoch: 4 [1050/13482]\tLoss: 2497.5486\n",
      "Training Epoch: 4 [1100/13482]\tLoss: 2501.6248\n",
      "Training Epoch: 4 [1150/13482]\tLoss: 2448.7981\n",
      "Training Epoch: 4 [1200/13482]\tLoss: 2403.0881\n",
      "Training Epoch: 4 [1250/13482]\tLoss: 2455.2935\n",
      "Training Epoch: 4 [1300/13482]\tLoss: 2354.4631\n",
      "Training Epoch: 4 [1350/13482]\tLoss: 2375.3303\n",
      "Training Epoch: 4 [1400/13482]\tLoss: 2435.3862\n",
      "Training Epoch: 4 [1450/13482]\tLoss: 2401.0439\n",
      "Training Epoch: 4 [1500/13482]\tLoss: 2413.2092\n",
      "Training Epoch: 4 [1550/13482]\tLoss: 2440.8535\n",
      "Training Epoch: 4 [1600/13482]\tLoss: 2379.0691\n",
      "Training Epoch: 4 [1650/13482]\tLoss: 2380.3181\n",
      "Training Epoch: 4 [1700/13482]\tLoss: 2414.5364\n",
      "Training Epoch: 4 [1750/13482]\tLoss: 2402.1523\n",
      "Training Epoch: 4 [1800/13482]\tLoss: 2533.4429\n",
      "Training Epoch: 4 [1850/13482]\tLoss: 2406.1914\n",
      "Training Epoch: 4 [1900/13482]\tLoss: 2400.4116\n",
      "Training Epoch: 4 [1950/13482]\tLoss: 2420.2024\n",
      "Training Epoch: 4 [2000/13482]\tLoss: 2410.5300\n",
      "Training Epoch: 4 [2050/13482]\tLoss: 2402.1245\n",
      "Training Epoch: 4 [2100/13482]\tLoss: 2446.1047\n",
      "Training Epoch: 4 [2150/13482]\tLoss: 2431.1545\n",
      "Training Epoch: 4 [2200/13482]\tLoss: 2329.5891\n",
      "Training Epoch: 4 [2250/13482]\tLoss: 2394.3855\n",
      "Training Epoch: 4 [2300/13482]\tLoss: 2339.7366\n",
      "Training Epoch: 4 [2350/13482]\tLoss: 2414.4868\n",
      "Training Epoch: 4 [2400/13482]\tLoss: 2365.2283\n",
      "Training Epoch: 4 [2450/13482]\tLoss: 2405.6306\n",
      "Training Epoch: 4 [2500/13482]\tLoss: 2372.9458\n",
      "Training Epoch: 4 [2550/13482]\tLoss: 2385.3843\n",
      "Training Epoch: 4 [2600/13482]\tLoss: 2389.7820\n",
      "Training Epoch: 4 [2650/13482]\tLoss: 2319.9854\n",
      "Training Epoch: 4 [2700/13482]\tLoss: 2451.8555\n",
      "Training Epoch: 4 [2750/13482]\tLoss: 2389.6047\n",
      "Training Epoch: 4 [2800/13482]\tLoss: 2350.1755\n",
      "Training Epoch: 4 [2850/13482]\tLoss: 2547.5549\n",
      "Training Epoch: 4 [2900/13482]\tLoss: 2431.5256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [2950/13482]\tLoss: 2428.3049\n",
      "Training Epoch: 4 [3000/13482]\tLoss: 2357.0068\n",
      "Training Epoch: 4 [3050/13482]\tLoss: 2383.5093\n",
      "Training Epoch: 4 [3100/13482]\tLoss: 2305.2583\n",
      "Training Epoch: 4 [3150/13482]\tLoss: 2334.4221\n",
      "Training Epoch: 4 [3200/13482]\tLoss: 2362.2964\n",
      "Training Epoch: 4 [3250/13482]\tLoss: 2380.7124\n",
      "Training Epoch: 4 [3300/13482]\tLoss: 2431.0769\n",
      "Training Epoch: 4 [3350/13482]\tLoss: 2445.2886\n",
      "Training Epoch: 4 [3400/13482]\tLoss: 2388.6509\n",
      "Training Epoch: 4 [3450/13482]\tLoss: 2407.9624\n",
      "Training Epoch: 4 [3500/13482]\tLoss: 2368.9141\n",
      "Training Epoch: 4 [3550/13482]\tLoss: 2385.2058\n",
      "Training Epoch: 4 [3600/13482]\tLoss: 2482.8420\n",
      "Training Epoch: 4 [3650/13482]\tLoss: 2320.7466\n",
      "Training Epoch: 4 [3700/13482]\tLoss: 2284.5164\n",
      "Training Epoch: 4 [3750/13482]\tLoss: 2363.4392\n",
      "Training Epoch: 4 [3800/13482]\tLoss: 2389.6248\n",
      "Training Epoch: 4 [3850/13482]\tLoss: 2381.8413\n",
      "Training Epoch: 4 [3900/13482]\tLoss: 2401.1648\n",
      "Training Epoch: 4 [3950/13482]\tLoss: 2355.1941\n",
      "Training Epoch: 4 [4000/13482]\tLoss: 2353.7351\n",
      "Training Epoch: 4 [4050/13482]\tLoss: 2337.5762\n",
      "Training Epoch: 4 [4100/13482]\tLoss: 2358.6265\n",
      "Training Epoch: 4 [4150/13482]\tLoss: 2352.0908\n",
      "Training Epoch: 4 [4200/13482]\tLoss: 2353.8928\n",
      "Training Epoch: 4 [4250/13482]\tLoss: 2417.7737\n",
      "Training Epoch: 4 [4300/13482]\tLoss: 2383.5112\n",
      "Training Epoch: 4 [4350/13482]\tLoss: 2340.4702\n",
      "Training Epoch: 4 [4400/13482]\tLoss: 2402.7058\n",
      "Training Epoch: 4 [4450/13482]\tLoss: 2313.9329\n",
      "Training Epoch: 4 [4500/13482]\tLoss: 2336.1211\n",
      "Training Epoch: 4 [4550/13482]\tLoss: 2254.8315\n",
      "Training Epoch: 4 [4600/13482]\tLoss: 2266.8618\n",
      "Training Epoch: 4 [4650/13482]\tLoss: 2367.2598\n",
      "Training Epoch: 4 [4700/13482]\tLoss: 2324.6653\n",
      "Training Epoch: 4 [4750/13482]\tLoss: 2363.1204\n",
      "Training Epoch: 4 [4800/13482]\tLoss: 2298.6433\n",
      "Training Epoch: 4 [4850/13482]\tLoss: 2264.0583\n",
      "Training Epoch: 4 [4900/13482]\tLoss: 2370.8333\n",
      "Training Epoch: 4 [4950/13482]\tLoss: 2357.1724\n",
      "Training Epoch: 4 [5000/13482]\tLoss: 2375.6475\n",
      "Training Epoch: 4 [5050/13482]\tLoss: 2368.1682\n",
      "Training Epoch: 4 [5100/13482]\tLoss: 2264.8259\n",
      "Training Epoch: 4 [5150/13482]\tLoss: 2308.0627\n",
      "Training Epoch: 4 [5200/13482]\tLoss: 2375.6025\n",
      "Training Epoch: 4 [5250/13482]\tLoss: 2304.7778\n",
      "Training Epoch: 4 [5300/13482]\tLoss: 2300.3848\n",
      "Training Epoch: 4 [5350/13482]\tLoss: 2277.6760\n",
      "Training Epoch: 4 [5400/13482]\tLoss: 2381.7053\n",
      "Training Epoch: 4 [5450/13482]\tLoss: 2297.1072\n",
      "Training Epoch: 4 [5500/13482]\tLoss: 2308.4988\n",
      "Training Epoch: 4 [5550/13482]\tLoss: 2278.9170\n",
      "Training Epoch: 4 [5600/13482]\tLoss: 2281.0488\n",
      "Training Epoch: 4 [5650/13482]\tLoss: 2303.5122\n",
      "Training Epoch: 4 [5700/13482]\tLoss: 2299.6868\n",
      "Training Epoch: 4 [5750/13482]\tLoss: 2356.4866\n",
      "Training Epoch: 4 [5800/13482]\tLoss: 2337.3567\n",
      "Training Epoch: 4 [5850/13482]\tLoss: 2412.4648\n",
      "Training Epoch: 4 [5900/13482]\tLoss: 2312.7007\n",
      "Training Epoch: 4 [5950/13482]\tLoss: 2323.8479\n",
      "Training Epoch: 4 [6000/13482]\tLoss: 2296.5432\n",
      "Training Epoch: 4 [6050/13482]\tLoss: 2327.0059\n",
      "Training Epoch: 4 [6100/13482]\tLoss: 2302.9714\n",
      "Training Epoch: 4 [6150/13482]\tLoss: 2361.1990\n",
      "Training Epoch: 4 [6200/13482]\tLoss: 2413.9680\n",
      "Training Epoch: 4 [6250/13482]\tLoss: 2286.4170\n",
      "Training Epoch: 4 [6300/13482]\tLoss: 2232.5225\n",
      "Training Epoch: 4 [6350/13482]\tLoss: 2376.3618\n",
      "Training Epoch: 4 [6400/13482]\tLoss: 2314.3840\n",
      "Training Epoch: 4 [6450/13482]\tLoss: 2277.8213\n",
      "Training Epoch: 4 [6500/13482]\tLoss: 2267.5632\n",
      "Training Epoch: 4 [6550/13482]\tLoss: 2348.1716\n",
      "Training Epoch: 4 [6600/13482]\tLoss: 2211.6389\n",
      "Training Epoch: 4 [6650/13482]\tLoss: 2326.2520\n",
      "Training Epoch: 4 [6700/13482]\tLoss: 2258.9939\n",
      "Training Epoch: 4 [6750/13482]\tLoss: 2279.2622\n",
      "Training Epoch: 4 [6800/13482]\tLoss: 2264.1824\n",
      "Training Epoch: 4 [6850/13482]\tLoss: 2259.7332\n",
      "Training Epoch: 4 [6900/13482]\tLoss: 2276.7234\n",
      "Training Epoch: 4 [6950/13482]\tLoss: 2250.8169\n",
      "Training Epoch: 4 [7000/13482]\tLoss: 2287.5747\n",
      "Training Epoch: 4 [7050/13482]\tLoss: 2356.3735\n",
      "Training Epoch: 4 [7100/13482]\tLoss: 2290.0295\n",
      "Training Epoch: 4 [7150/13482]\tLoss: 2307.2278\n",
      "Training Epoch: 4 [7200/13482]\tLoss: 2330.0535\n",
      "Training Epoch: 4 [7250/13482]\tLoss: 2264.8147\n",
      "Training Epoch: 4 [7300/13482]\tLoss: 2262.2612\n",
      "Training Epoch: 4 [7350/13482]\tLoss: 2238.6814\n",
      "Training Epoch: 4 [7400/13482]\tLoss: 2313.0117\n",
      "Training Epoch: 4 [7450/13482]\tLoss: 2275.2083\n",
      "Training Epoch: 4 [7500/13482]\tLoss: 2215.1165\n",
      "Training Epoch: 4 [7550/13482]\tLoss: 2296.4172\n",
      "Training Epoch: 4 [7600/13482]\tLoss: 2298.7629\n",
      "Training Epoch: 4 [7650/13482]\tLoss: 2293.9883\n",
      "Training Epoch: 4 [7700/13482]\tLoss: 2245.6223\n",
      "Training Epoch: 4 [7750/13482]\tLoss: 2248.3416\n",
      "Training Epoch: 4 [7800/13482]\tLoss: 2320.9814\n",
      "Training Epoch: 4 [7850/13482]\tLoss: 2311.9814\n",
      "Training Epoch: 4 [7900/13482]\tLoss: 2291.6047\n",
      "Training Epoch: 4 [7950/13482]\tLoss: 2308.9644\n",
      "Training Epoch: 4 [8000/13482]\tLoss: 2206.6394\n",
      "Training Epoch: 4 [8050/13482]\tLoss: 2310.4019\n",
      "Training Epoch: 4 [8100/13482]\tLoss: 2282.2832\n",
      "Training Epoch: 4 [8150/13482]\tLoss: 2319.3538\n",
      "Training Epoch: 4 [8200/13482]\tLoss: 2246.9375\n",
      "Training Epoch: 4 [8250/13482]\tLoss: 2199.3984\n",
      "Training Epoch: 4 [8300/13482]\tLoss: 2266.8210\n",
      "Training Epoch: 4 [8350/13482]\tLoss: 2262.7083\n",
      "Training Epoch: 4 [8400/13482]\tLoss: 2314.1707\n",
      "Training Epoch: 4 [8450/13482]\tLoss: 2245.2798\n",
      "Training Epoch: 4 [8500/13482]\tLoss: 2250.5872\n",
      "Training Epoch: 4 [8550/13482]\tLoss: 2210.5452\n",
      "Training Epoch: 4 [8600/13482]\tLoss: 2250.5315\n",
      "Training Epoch: 4 [8650/13482]\tLoss: 2307.6738\n",
      "Training Epoch: 4 [8700/13482]\tLoss: 2230.6548\n",
      "Training Epoch: 4 [8750/13482]\tLoss: 2237.7720\n",
      "Training Epoch: 4 [8800/13482]\tLoss: 2288.6489\n",
      "Training Epoch: 4 [8850/13482]\tLoss: 2195.8606\n",
      "Training Epoch: 4 [8900/13482]\tLoss: 2256.0266\n",
      "Training Epoch: 4 [8950/13482]\tLoss: 2258.7002\n",
      "Training Epoch: 4 [9000/13482]\tLoss: 2192.0698\n",
      "Training Epoch: 4 [9050/13482]\tLoss: 2266.1914\n",
      "Training Epoch: 4 [9100/13482]\tLoss: 2180.0637\n",
      "Training Epoch: 4 [9150/13482]\tLoss: 2267.7896\n",
      "Training Epoch: 4 [9200/13482]\tLoss: 2304.9624\n",
      "Training Epoch: 4 [9250/13482]\tLoss: 2191.4128\n",
      "Training Epoch: 4 [9300/13482]\tLoss: 2264.6941\n",
      "Training Epoch: 4 [9350/13482]\tLoss: 2266.2080\n",
      "Training Epoch: 4 [9400/13482]\tLoss: 2171.9221\n",
      "Training Epoch: 4 [9450/13482]\tLoss: 2286.4282\n",
      "Training Epoch: 4 [9500/13482]\tLoss: 2223.4097\n",
      "Training Epoch: 4 [9550/13482]\tLoss: 2261.1338\n",
      "Training Epoch: 4 [9600/13482]\tLoss: 2291.8516\n",
      "Training Epoch: 4 [9650/13482]\tLoss: 2249.8181\n",
      "Training Epoch: 4 [9700/13482]\tLoss: 2269.3999\n",
      "Training Epoch: 4 [9750/13482]\tLoss: 2219.6218\n",
      "Training Epoch: 4 [9800/13482]\tLoss: 2212.2007\n",
      "Training Epoch: 4 [9850/13482]\tLoss: 2206.8020\n",
      "Training Epoch: 4 [9900/13482]\tLoss: 2213.3330\n",
      "Training Epoch: 4 [9950/13482]\tLoss: 2271.4912\n",
      "Training Epoch: 4 [10000/13482]\tLoss: 2246.8550\n",
      "Training Epoch: 4 [10050/13482]\tLoss: 2268.3359\n",
      "Training Epoch: 4 [10100/13482]\tLoss: 2166.8276\n",
      "Training Epoch: 4 [10150/13482]\tLoss: 2236.9177\n",
      "Training Epoch: 4 [10200/13482]\tLoss: 2232.8201\n",
      "Training Epoch: 4 [10250/13482]\tLoss: 2304.3223\n",
      "Training Epoch: 4 [10300/13482]\tLoss: 2243.2393\n",
      "Training Epoch: 4 [10350/13482]\tLoss: 2216.9417\n",
      "Training Epoch: 4 [10400/13482]\tLoss: 2195.0835\n",
      "Training Epoch: 4 [10450/13482]\tLoss: 2232.5479\n",
      "Training Epoch: 4 [10500/13482]\tLoss: 2336.0271\n",
      "Training Epoch: 4 [10550/13482]\tLoss: 2247.1987\n",
      "Training Epoch: 4 [10600/13482]\tLoss: 2190.9792\n",
      "Training Epoch: 4 [10650/13482]\tLoss: 2167.2786\n",
      "Training Epoch: 4 [10700/13482]\tLoss: 2196.3831\n",
      "Training Epoch: 4 [10750/13482]\tLoss: 2241.2227\n",
      "Training Epoch: 4 [10800/13482]\tLoss: 2210.2183\n",
      "Training Epoch: 4 [10850/13482]\tLoss: 2162.2686\n",
      "Training Epoch: 4 [10900/13482]\tLoss: 2203.0002\n",
      "Training Epoch: 4 [10950/13482]\tLoss: 2161.3218\n",
      "Training Epoch: 4 [11000/13482]\tLoss: 2195.4207\n",
      "Training Epoch: 4 [11050/13482]\tLoss: 2214.5022\n",
      "Training Epoch: 4 [11100/13482]\tLoss: 2179.4602\n",
      "Training Epoch: 4 [11150/13482]\tLoss: 2212.2639\n",
      "Training Epoch: 4 [11200/13482]\tLoss: 2183.4470\n",
      "Training Epoch: 4 [11250/13482]\tLoss: 2249.2358\n",
      "Training Epoch: 4 [11300/13482]\tLoss: 2214.7297\n",
      "Training Epoch: 4 [11350/13482]\tLoss: 2251.2959\n",
      "Training Epoch: 4 [11400/13482]\tLoss: 2219.3901\n",
      "Training Epoch: 4 [11450/13482]\tLoss: 2155.6089\n",
      "Training Epoch: 4 [11500/13482]\tLoss: 2188.8169\n",
      "Training Epoch: 4 [11550/13482]\tLoss: 2232.4829\n",
      "Training Epoch: 4 [11600/13482]\tLoss: 2223.5640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [11650/13482]\tLoss: 2183.7712\n",
      "Training Epoch: 4 [11700/13482]\tLoss: 2150.7112\n",
      "Training Epoch: 4 [11750/13482]\tLoss: 2113.9875\n",
      "Training Epoch: 4 [11800/13482]\tLoss: 2219.1614\n",
      "Training Epoch: 4 [11850/13482]\tLoss: 2128.2483\n",
      "Training Epoch: 4 [11900/13482]\tLoss: 2165.0571\n",
      "Training Epoch: 4 [11950/13482]\tLoss: 2219.8003\n",
      "Training Epoch: 4 [12000/13482]\tLoss: 2160.7839\n",
      "Training Epoch: 4 [12050/13482]\tLoss: 2178.6807\n",
      "Training Epoch: 4 [12100/13482]\tLoss: 2233.4573\n",
      "Training Epoch: 4 [12150/13482]\tLoss: 2229.5029\n",
      "Training Epoch: 4 [12200/13482]\tLoss: 2178.8286\n",
      "Training Epoch: 4 [12250/13482]\tLoss: 2179.1138\n",
      "Training Epoch: 4 [12300/13482]\tLoss: 2207.0645\n",
      "Training Epoch: 4 [12350/13482]\tLoss: 2188.7021\n",
      "Training Epoch: 4 [12400/13482]\tLoss: 2194.0547\n",
      "Training Epoch: 4 [12450/13482]\tLoss: 2149.2192\n",
      "Training Epoch: 4 [12500/13482]\tLoss: 2242.8430\n",
      "Training Epoch: 4 [12550/13482]\tLoss: 2184.5647\n",
      "Training Epoch: 4 [12600/13482]\tLoss: 2169.1982\n",
      "Training Epoch: 4 [12650/13482]\tLoss: 2148.9431\n",
      "Training Epoch: 4 [12700/13482]\tLoss: 2141.7939\n",
      "Training Epoch: 4 [12750/13482]\tLoss: 2164.9858\n",
      "Training Epoch: 4 [12800/13482]\tLoss: 2194.7017\n",
      "Training Epoch: 4 [12850/13482]\tLoss: 2168.2903\n",
      "Training Epoch: 4 [12900/13482]\tLoss: 2224.4197\n",
      "Training Epoch: 4 [12950/13482]\tLoss: 2195.2007\n",
      "Training Epoch: 4 [13000/13482]\tLoss: 2158.7593\n",
      "Training Epoch: 4 [13050/13482]\tLoss: 2187.3521\n",
      "Training Epoch: 4 [13100/13482]\tLoss: 2136.3025\n",
      "Training Epoch: 4 [13150/13482]\tLoss: 2141.5330\n",
      "Training Epoch: 4 [13200/13482]\tLoss: 2186.2192\n",
      "Training Epoch: 4 [13250/13482]\tLoss: 2229.5889\n",
      "Training Epoch: 4 [13300/13482]\tLoss: 2187.1169\n",
      "Training Epoch: 4 [13350/13482]\tLoss: 2163.0027\n",
      "Training Epoch: 4 [13400/13482]\tLoss: 2232.5337\n",
      "Training Epoch: 4 [13450/13482]\tLoss: 2218.6680\n",
      "Training Epoch: 4 [13482/13482]\tLoss: 2202.7012\n",
      "Training Epoch: 4 [1497/1497]\tLoss: 2152.4315\n",
      "Training Epoch: 5 [50/13482]\tLoss: 2150.8706\n",
      "Training Epoch: 5 [100/13482]\tLoss: 2172.9951\n",
      "Training Epoch: 5 [150/13482]\tLoss: 2177.4641\n",
      "Training Epoch: 5 [200/13482]\tLoss: 2212.8315\n",
      "Training Epoch: 5 [250/13482]\tLoss: 2159.5698\n",
      "Training Epoch: 5 [300/13482]\tLoss: 2170.6653\n",
      "Training Epoch: 5 [350/13482]\tLoss: 2160.6335\n",
      "Training Epoch: 5 [400/13482]\tLoss: 2107.7634\n",
      "Training Epoch: 5 [450/13482]\tLoss: 2147.0061\n",
      "Training Epoch: 5 [500/13482]\tLoss: 2143.1660\n",
      "Training Epoch: 5 [550/13482]\tLoss: 2169.7305\n",
      "Training Epoch: 5 [600/13482]\tLoss: 2156.2903\n",
      "Training Epoch: 5 [650/13482]\tLoss: 2164.9126\n",
      "Training Epoch: 5 [700/13482]\tLoss: 2177.3567\n",
      "Training Epoch: 5 [750/13482]\tLoss: 2126.9880\n",
      "Training Epoch: 5 [800/13482]\tLoss: 2172.3340\n",
      "Training Epoch: 5 [850/13482]\tLoss: 2163.7295\n",
      "Training Epoch: 5 [900/13482]\tLoss: 2125.6101\n",
      "Training Epoch: 5 [950/13482]\tLoss: 2167.4768\n",
      "Training Epoch: 5 [1000/13482]\tLoss: 2221.4109\n",
      "Training Epoch: 5 [1050/13482]\tLoss: 2166.6467\n",
      "Training Epoch: 5 [1100/13482]\tLoss: 2187.3396\n",
      "Training Epoch: 5 [1150/13482]\tLoss: 2152.0095\n",
      "Training Epoch: 5 [1200/13482]\tLoss: 2124.4009\n",
      "Training Epoch: 5 [1250/13482]\tLoss: 2146.5642\n",
      "Training Epoch: 5 [1300/13482]\tLoss: 2091.4451\n",
      "Training Epoch: 5 [1350/13482]\tLoss: 2094.8071\n",
      "Training Epoch: 5 [1400/13482]\tLoss: 2136.7312\n",
      "Training Epoch: 5 [1450/13482]\tLoss: 2106.0227\n",
      "Training Epoch: 5 [1500/13482]\tLoss: 2134.6531\n",
      "Training Epoch: 5 [1550/13482]\tLoss: 2132.5242\n",
      "Training Epoch: 5 [1600/13482]\tLoss: 2090.2764\n",
      "Training Epoch: 5 [1650/13482]\tLoss: 2108.4077\n",
      "Training Epoch: 5 [1700/13482]\tLoss: 2140.8066\n",
      "Training Epoch: 5 [1750/13482]\tLoss: 2106.8909\n",
      "Training Epoch: 5 [1800/13482]\tLoss: 2201.5208\n",
      "Training Epoch: 5 [1850/13482]\tLoss: 2129.3184\n",
      "Training Epoch: 5 [1900/13482]\tLoss: 2108.4382\n",
      "Training Epoch: 5 [1950/13482]\tLoss: 2135.2505\n",
      "Training Epoch: 5 [2000/13482]\tLoss: 2129.6770\n",
      "Training Epoch: 5 [2050/13482]\tLoss: 2118.0132\n",
      "Training Epoch: 5 [2100/13482]\tLoss: 2147.1094\n",
      "Training Epoch: 5 [2150/13482]\tLoss: 2152.2012\n",
      "Training Epoch: 5 [2200/13482]\tLoss: 2055.5889\n",
      "Training Epoch: 5 [2250/13482]\tLoss: 2127.4009\n",
      "Training Epoch: 5 [2300/13482]\tLoss: 2083.6914\n",
      "Training Epoch: 5 [2350/13482]\tLoss: 2107.9617\n",
      "Training Epoch: 5 [2400/13482]\tLoss: 2071.5024\n",
      "Training Epoch: 5 [2450/13482]\tLoss: 2114.6587\n",
      "Training Epoch: 5 [2500/13482]\tLoss: 2086.5754\n",
      "Training Epoch: 5 [2550/13482]\tLoss: 2097.1025\n",
      "Training Epoch: 5 [2600/13482]\tLoss: 2110.9854\n",
      "Training Epoch: 5 [2650/13482]\tLoss: 2075.6138\n",
      "Training Epoch: 5 [2700/13482]\tLoss: 2153.3030\n",
      "Training Epoch: 5 [2750/13482]\tLoss: 2115.0940\n",
      "Training Epoch: 5 [2800/13482]\tLoss: 2092.0198\n",
      "Training Epoch: 5 [2850/13482]\tLoss: 2225.5767\n",
      "Training Epoch: 5 [2900/13482]\tLoss: 2165.8311\n",
      "Training Epoch: 5 [2950/13482]\tLoss: 2128.6169\n",
      "Training Epoch: 5 [3000/13482]\tLoss: 2072.2114\n",
      "Training Epoch: 5 [3050/13482]\tLoss: 2121.8416\n",
      "Training Epoch: 5 [3100/13482]\tLoss: 2049.1917\n",
      "Training Epoch: 5 [3150/13482]\tLoss: 2065.6252\n",
      "Training Epoch: 5 [3200/13482]\tLoss: 2095.0247\n",
      "Training Epoch: 5 [3250/13482]\tLoss: 2121.0234\n",
      "Training Epoch: 5 [3300/13482]\tLoss: 2159.9246\n",
      "Training Epoch: 5 [3350/13482]\tLoss: 2160.3481\n",
      "Training Epoch: 5 [3400/13482]\tLoss: 2101.5847\n",
      "Training Epoch: 5 [3450/13482]\tLoss: 2141.7131\n",
      "Training Epoch: 5 [3500/13482]\tLoss: 2096.1455\n",
      "Training Epoch: 5 [3550/13482]\tLoss: 2119.6348\n",
      "Training Epoch: 5 [3600/13482]\tLoss: 2204.2234\n",
      "Training Epoch: 5 [3650/13482]\tLoss: 2068.3464\n",
      "Training Epoch: 5 [3700/13482]\tLoss: 2029.7312\n",
      "Training Epoch: 5 [3750/13482]\tLoss: 2119.5002\n",
      "Training Epoch: 5 [3800/13482]\tLoss: 2107.2061\n",
      "Training Epoch: 5 [3850/13482]\tLoss: 2088.5608\n",
      "Training Epoch: 5 [3900/13482]\tLoss: 2106.7080\n",
      "Training Epoch: 5 [3950/13482]\tLoss: 2080.5713\n",
      "Training Epoch: 5 [4000/13482]\tLoss: 2084.8816\n",
      "Training Epoch: 5 [4050/13482]\tLoss: 2083.4268\n",
      "Training Epoch: 5 [4100/13482]\tLoss: 2123.8242\n",
      "Training Epoch: 5 [4150/13482]\tLoss: 2094.5156\n",
      "Training Epoch: 5 [4200/13482]\tLoss: 2086.7361\n",
      "Training Epoch: 5 [4250/13482]\tLoss: 2136.2820\n",
      "Training Epoch: 5 [4300/13482]\tLoss: 2112.7263\n",
      "Training Epoch: 5 [4350/13482]\tLoss: 2073.3867\n",
      "Training Epoch: 5 [4400/13482]\tLoss: 2146.6799\n",
      "Training Epoch: 5 [4450/13482]\tLoss: 2069.9243\n",
      "Training Epoch: 5 [4500/13482]\tLoss: 2092.9702\n",
      "Training Epoch: 5 [4550/13482]\tLoss: 2018.0344\n",
      "Training Epoch: 5 [4600/13482]\tLoss: 2017.1390\n",
      "Training Epoch: 5 [4650/13482]\tLoss: 2115.1001\n",
      "Training Epoch: 5 [4700/13482]\tLoss: 2070.3108\n",
      "Training Epoch: 5 [4750/13482]\tLoss: 2079.3870\n",
      "Training Epoch: 5 [4800/13482]\tLoss: 2068.7434\n",
      "Training Epoch: 5 [4850/13482]\tLoss: 2045.6327\n",
      "Training Epoch: 5 [4900/13482]\tLoss: 2115.4351\n",
      "Training Epoch: 5 [4950/13482]\tLoss: 2103.1592\n",
      "Training Epoch: 5 [5000/13482]\tLoss: 2111.7896\n",
      "Training Epoch: 5 [5050/13482]\tLoss: 2099.3567\n",
      "Training Epoch: 5 [5100/13482]\tLoss: 2036.2622\n",
      "Training Epoch: 5 [5150/13482]\tLoss: 2069.2256\n",
      "Training Epoch: 5 [5200/13482]\tLoss: 2120.1875\n",
      "Training Epoch: 5 [5250/13482]\tLoss: 2069.8430\n",
      "Training Epoch: 5 [5300/13482]\tLoss: 2064.9189\n",
      "Training Epoch: 5 [5350/13482]\tLoss: 2030.0850\n",
      "Training Epoch: 5 [5400/13482]\tLoss: 2111.1394\n",
      "Training Epoch: 5 [5450/13482]\tLoss: 2061.6978\n",
      "Training Epoch: 5 [5500/13482]\tLoss: 2083.8699\n",
      "Training Epoch: 5 [5550/13482]\tLoss: 2058.1909\n",
      "Training Epoch: 5 [5600/13482]\tLoss: 2043.7670\n",
      "Training Epoch: 5 [5650/13482]\tLoss: 2058.0862\n",
      "Training Epoch: 5 [5700/13482]\tLoss: 2059.9595\n",
      "Training Epoch: 5 [5750/13482]\tLoss: 2115.5928\n",
      "Training Epoch: 5 [5800/13482]\tLoss: 2103.7383\n",
      "Training Epoch: 5 [5850/13482]\tLoss: 2175.9944\n",
      "Training Epoch: 5 [5900/13482]\tLoss: 2074.0198\n",
      "Training Epoch: 5 [5950/13482]\tLoss: 2064.6782\n",
      "Training Epoch: 5 [6000/13482]\tLoss: 2062.7939\n",
      "Training Epoch: 5 [6050/13482]\tLoss: 2086.1233\n",
      "Training Epoch: 5 [6100/13482]\tLoss: 2081.8174\n",
      "Training Epoch: 5 [6150/13482]\tLoss: 2118.2717\n",
      "Training Epoch: 5 [6200/13482]\tLoss: 2164.9277\n",
      "Training Epoch: 5 [6250/13482]\tLoss: 2047.2435\n",
      "Training Epoch: 5 [6300/13482]\tLoss: 2006.7197\n",
      "Training Epoch: 5 [6350/13482]\tLoss: 2106.3691\n",
      "Training Epoch: 5 [6400/13482]\tLoss: 2085.4490\n",
      "Training Epoch: 5 [6450/13482]\tLoss: 2053.3047\n",
      "Training Epoch: 5 [6500/13482]\tLoss: 2035.4176\n",
      "Training Epoch: 5 [6550/13482]\tLoss: 2119.3584\n",
      "Training Epoch: 5 [6600/13482]\tLoss: 2009.0116\n",
      "Training Epoch: 5 [6650/13482]\tLoss: 2085.7634\n",
      "Training Epoch: 5 [6700/13482]\tLoss: 2029.9351\n",
      "Training Epoch: 5 [6750/13482]\tLoss: 2051.4800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [6800/13482]\tLoss: 2034.1925\n",
      "Training Epoch: 5 [6850/13482]\tLoss: 2032.3756\n",
      "Training Epoch: 5 [6900/13482]\tLoss: 2052.5398\n",
      "Training Epoch: 5 [6950/13482]\tLoss: 2024.4847\n",
      "Training Epoch: 5 [7000/13482]\tLoss: 2061.3489\n",
      "Training Epoch: 5 [7050/13482]\tLoss: 2106.5891\n",
      "Training Epoch: 5 [7100/13482]\tLoss: 2067.1116\n",
      "Training Epoch: 5 [7150/13482]\tLoss: 2081.2305\n",
      "Training Epoch: 5 [7200/13482]\tLoss: 2087.9495\n",
      "Training Epoch: 5 [7250/13482]\tLoss: 2036.7474\n",
      "Training Epoch: 5 [7300/13482]\tLoss: 2050.4304\n",
      "Training Epoch: 5 [7350/13482]\tLoss: 2019.4785\n",
      "Training Epoch: 5 [7400/13482]\tLoss: 2092.5608\n",
      "Training Epoch: 5 [7450/13482]\tLoss: 2049.4414\n",
      "Training Epoch: 5 [7500/13482]\tLoss: 1995.3286\n",
      "Training Epoch: 5 [7550/13482]\tLoss: 2081.6262\n",
      "Training Epoch: 5 [7600/13482]\tLoss: 2073.9163\n",
      "Training Epoch: 5 [7650/13482]\tLoss: 2082.4885\n",
      "Training Epoch: 5 [7700/13482]\tLoss: 2021.9338\n",
      "Training Epoch: 5 [7750/13482]\tLoss: 2018.9082\n",
      "Training Epoch: 5 [7800/13482]\tLoss: 2096.7417\n",
      "Training Epoch: 5 [7850/13482]\tLoss: 2095.6206\n",
      "Training Epoch: 5 [7900/13482]\tLoss: 2056.7688\n",
      "Training Epoch: 5 [7950/13482]\tLoss: 2083.4688\n",
      "Training Epoch: 5 [8000/13482]\tLoss: 1998.8367\n",
      "Training Epoch: 5 [8050/13482]\tLoss: 2092.5952\n",
      "Training Epoch: 5 [8100/13482]\tLoss: 2060.2039\n",
      "Training Epoch: 5 [8150/13482]\tLoss: 2087.9624\n",
      "Training Epoch: 5 [8200/13482]\tLoss: 2045.3691\n",
      "Training Epoch: 5 [8250/13482]\tLoss: 1984.8193\n",
      "Training Epoch: 5 [8300/13482]\tLoss: 2039.9033\n",
      "Training Epoch: 5 [8350/13482]\tLoss: 2052.0056\n",
      "Training Epoch: 5 [8400/13482]\tLoss: 2092.0046\n",
      "Training Epoch: 5 [8450/13482]\tLoss: 2021.4116\n",
      "Training Epoch: 5 [8500/13482]\tLoss: 2039.9572\n",
      "Training Epoch: 5 [8550/13482]\tLoss: 2009.2654\n",
      "Training Epoch: 5 [8600/13482]\tLoss: 2049.9800\n",
      "Training Epoch: 5 [8650/13482]\tLoss: 2086.3660\n",
      "Training Epoch: 5 [8700/13482]\tLoss: 2026.6218\n",
      "Training Epoch: 5 [8750/13482]\tLoss: 2030.2394\n",
      "Training Epoch: 5 [8800/13482]\tLoss: 2053.7578\n",
      "Training Epoch: 5 [8850/13482]\tLoss: 2006.3384\n",
      "Training Epoch: 5 [8900/13482]\tLoss: 2042.9874\n",
      "Training Epoch: 5 [8950/13482]\tLoss: 2031.6023\n",
      "Training Epoch: 5 [9000/13482]\tLoss: 2012.0082\n",
      "Training Epoch: 5 [9050/13482]\tLoss: 2054.1428\n",
      "Training Epoch: 5 [9100/13482]\tLoss: 1986.0897\n",
      "Training Epoch: 5 [9150/13482]\tLoss: 2060.3625\n",
      "Training Epoch: 5 [9200/13482]\tLoss: 2100.0071\n",
      "Training Epoch: 5 [9250/13482]\tLoss: 1985.6421\n",
      "Training Epoch: 5 [9300/13482]\tLoss: 2061.3311\n",
      "Training Epoch: 5 [9350/13482]\tLoss: 2047.2814\n",
      "Training Epoch: 5 [9400/13482]\tLoss: 1970.3663\n",
      "Training Epoch: 5 [9450/13482]\tLoss: 2071.6438\n",
      "Training Epoch: 5 [9500/13482]\tLoss: 2018.0435\n",
      "Training Epoch: 5 [9550/13482]\tLoss: 2044.1334\n",
      "Training Epoch: 5 [9600/13482]\tLoss: 2068.5139\n",
      "Training Epoch: 5 [9650/13482]\tLoss: 2036.2644\n",
      "Training Epoch: 5 [9700/13482]\tLoss: 2050.6704\n",
      "Training Epoch: 5 [9750/13482]\tLoss: 2013.2081\n",
      "Training Epoch: 5 [9800/13482]\tLoss: 2000.4948\n",
      "Training Epoch: 5 [9850/13482]\tLoss: 1998.7424\n",
      "Training Epoch: 5 [9900/13482]\tLoss: 2018.4357\n",
      "Training Epoch: 5 [9950/13482]\tLoss: 2055.0120\n",
      "Training Epoch: 5 [10000/13482]\tLoss: 2031.6058\n",
      "Training Epoch: 5 [10050/13482]\tLoss: 2062.3757\n",
      "Training Epoch: 5 [10100/13482]\tLoss: 1987.2703\n",
      "Training Epoch: 5 [10150/13482]\tLoss: 2031.8427\n",
      "Training Epoch: 5 [10200/13482]\tLoss: 2013.6669\n",
      "Training Epoch: 5 [10250/13482]\tLoss: 2093.6697\n",
      "Training Epoch: 5 [10300/13482]\tLoss: 2032.1503\n",
      "Training Epoch: 5 [10350/13482]\tLoss: 2017.8073\n",
      "Training Epoch: 5 [10400/13482]\tLoss: 1991.3417\n",
      "Training Epoch: 5 [10450/13482]\tLoss: 2026.5272\n",
      "Training Epoch: 5 [10500/13482]\tLoss: 2118.7349\n",
      "Training Epoch: 5 [10550/13482]\tLoss: 2046.0490\n",
      "Training Epoch: 5 [10600/13482]\tLoss: 2004.9810\n",
      "Training Epoch: 5 [10650/13482]\tLoss: 1977.8048\n",
      "Training Epoch: 5 [10700/13482]\tLoss: 1999.3667\n",
      "Training Epoch: 5 [10750/13482]\tLoss: 2030.4440\n",
      "Training Epoch: 5 [10800/13482]\tLoss: 2004.5599\n",
      "Training Epoch: 5 [10850/13482]\tLoss: 1980.7474\n",
      "Training Epoch: 5 [10900/13482]\tLoss: 1998.3279\n",
      "Training Epoch: 5 [10950/13482]\tLoss: 1973.8420\n",
      "Training Epoch: 5 [11000/13482]\tLoss: 2002.3425\n",
      "Training Epoch: 5 [11050/13482]\tLoss: 2021.0063\n",
      "Training Epoch: 5 [11100/13482]\tLoss: 1994.4711\n",
      "Training Epoch: 5 [11150/13482]\tLoss: 2017.6365\n",
      "Training Epoch: 5 [11200/13482]\tLoss: 1981.9937\n",
      "Training Epoch: 5 [11250/13482]\tLoss: 2051.9146\n",
      "Training Epoch: 5 [11300/13482]\tLoss: 2020.2184\n",
      "Training Epoch: 5 [11350/13482]\tLoss: 2058.6580\n",
      "Training Epoch: 5 [11400/13482]\tLoss: 2022.7131\n",
      "Training Epoch: 5 [11450/13482]\tLoss: 1968.5927\n",
      "Training Epoch: 5 [11500/13482]\tLoss: 1999.7028\n",
      "Training Epoch: 5 [11550/13482]\tLoss: 2044.1094\n",
      "Training Epoch: 5 [11600/13482]\tLoss: 2022.9729\n",
      "Training Epoch: 5 [11650/13482]\tLoss: 2003.5168\n",
      "Training Epoch: 5 [11700/13482]\tLoss: 1965.6493\n",
      "Training Epoch: 5 [11750/13482]\tLoss: 1942.1368\n",
      "Training Epoch: 5 [11800/13482]\tLoss: 2028.6843\n",
      "Training Epoch: 5 [11850/13482]\tLoss: 1943.4441\n",
      "Training Epoch: 5 [11900/13482]\tLoss: 1974.6827\n",
      "Training Epoch: 5 [11950/13482]\tLoss: 2038.7281\n",
      "Training Epoch: 5 [12000/13482]\tLoss: 1964.9692\n",
      "Training Epoch: 5 [12050/13482]\tLoss: 1976.4146\n",
      "Training Epoch: 5 [12100/13482]\tLoss: 2041.5204\n",
      "Training Epoch: 5 [12150/13482]\tLoss: 2023.3967\n",
      "Training Epoch: 5 [12200/13482]\tLoss: 1991.2710\n",
      "Training Epoch: 5 [12250/13482]\tLoss: 1984.6169\n",
      "Training Epoch: 5 [12300/13482]\tLoss: 2022.1479\n",
      "Training Epoch: 5 [12350/13482]\tLoss: 1993.5129\n",
      "Training Epoch: 5 [12400/13482]\tLoss: 2005.5614\n",
      "Training Epoch: 5 [12450/13482]\tLoss: 1965.5269\n",
      "Training Epoch: 5 [12500/13482]\tLoss: 2046.1647\n",
      "Training Epoch: 5 [12550/13482]\tLoss: 1998.1416\n",
      "Training Epoch: 5 [12600/13482]\tLoss: 1992.0067\n",
      "Training Epoch: 5 [12650/13482]\tLoss: 1976.0369\n",
      "Training Epoch: 5 [12700/13482]\tLoss: 1958.7125\n",
      "Training Epoch: 5 [12750/13482]\tLoss: 1985.8224\n",
      "Training Epoch: 5 [12800/13482]\tLoss: 2001.2594\n",
      "Training Epoch: 5 [12850/13482]\tLoss: 1999.3232\n",
      "Training Epoch: 5 [12900/13482]\tLoss: 2037.2722\n",
      "Training Epoch: 5 [12950/13482]\tLoss: 1999.9264\n",
      "Training Epoch: 5 [13000/13482]\tLoss: 1988.1243\n",
      "Training Epoch: 5 [13050/13482]\tLoss: 2018.0986\n",
      "Training Epoch: 5 [13100/13482]\tLoss: 1957.7380\n",
      "Training Epoch: 5 [13150/13482]\tLoss: 1970.3147\n",
      "Training Epoch: 5 [13200/13482]\tLoss: 2003.3416\n",
      "Training Epoch: 5 [13250/13482]\tLoss: 2032.8846\n",
      "Training Epoch: 5 [13300/13482]\tLoss: 1996.9194\n",
      "Training Epoch: 5 [13350/13482]\tLoss: 1966.1375\n",
      "Training Epoch: 5 [13400/13482]\tLoss: 2038.3517\n",
      "Training Epoch: 5 [13450/13482]\tLoss: 2034.6907\n",
      "Training Epoch: 5 [13482/13482]\tLoss: 2018.6180\n",
      "Training Epoch: 5 [1497/1497]\tLoss: 1974.1566\n",
      "Training Epoch: 6 [50/13482]\tLoss: 1978.3756\n",
      "Training Epoch: 6 [100/13482]\tLoss: 1997.1232\n",
      "Training Epoch: 6 [150/13482]\tLoss: 1996.9220\n",
      "Training Epoch: 6 [200/13482]\tLoss: 2022.7819\n",
      "Training Epoch: 6 [250/13482]\tLoss: 1983.1328\n",
      "Training Epoch: 6 [300/13482]\tLoss: 1983.7479\n",
      "Training Epoch: 6 [350/13482]\tLoss: 1976.5892\n",
      "Training Epoch: 6 [400/13482]\tLoss: 1943.7029\n",
      "Training Epoch: 6 [450/13482]\tLoss: 1969.9004\n",
      "Training Epoch: 6 [500/13482]\tLoss: 1970.2102\n",
      "Training Epoch: 6 [550/13482]\tLoss: 1991.4862\n",
      "Training Epoch: 6 [600/13482]\tLoss: 1990.0951\n",
      "Training Epoch: 6 [650/13482]\tLoss: 1991.9309\n",
      "Training Epoch: 6 [700/13482]\tLoss: 2008.9188\n",
      "Training Epoch: 6 [750/13482]\tLoss: 1962.2701\n",
      "Training Epoch: 6 [800/13482]\tLoss: 1998.1697\n",
      "Training Epoch: 6 [850/13482]\tLoss: 1980.3999\n",
      "Training Epoch: 6 [900/13482]\tLoss: 1955.6970\n",
      "Training Epoch: 6 [950/13482]\tLoss: 2000.0234\n",
      "Training Epoch: 6 [1000/13482]\tLoss: 2036.9899\n",
      "Training Epoch: 6 [1050/13482]\tLoss: 1977.6786\n",
      "Training Epoch: 6 [1100/13482]\tLoss: 2007.6154\n",
      "Training Epoch: 6 [1150/13482]\tLoss: 1979.4614\n",
      "Training Epoch: 6 [1200/13482]\tLoss: 1961.8890\n",
      "Training Epoch: 6 [1250/13482]\tLoss: 1967.9473\n",
      "Training Epoch: 6 [1300/13482]\tLoss: 1937.3125\n",
      "Training Epoch: 6 [1350/13482]\tLoss: 1930.8580\n",
      "Training Epoch: 6 [1400/13482]\tLoss: 1965.4017\n",
      "Training Epoch: 6 [1450/13482]\tLoss: 1935.2235\n",
      "Training Epoch: 6 [1500/13482]\tLoss: 1972.7201\n",
      "Training Epoch: 6 [1550/13482]\tLoss: 1953.1526\n",
      "Training Epoch: 6 [1600/13482]\tLoss: 1922.6066\n",
      "Training Epoch: 6 [1650/13482]\tLoss: 1951.4319\n",
      "Training Epoch: 6 [1700/13482]\tLoss: 1980.1208\n",
      "Training Epoch: 6 [1750/13482]\tLoss: 1933.8878\n",
      "Training Epoch: 6 [1800/13482]\tLoss: 2010.1781\n",
      "Training Epoch: 6 [1850/13482]\tLoss: 1969.8856\n",
      "Training Epoch: 6 [1900/13482]\tLoss: 1939.3604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [1950/13482]\tLoss: 1968.6608\n",
      "Training Epoch: 6 [2000/13482]\tLoss: 1966.7654\n",
      "Training Epoch: 6 [2050/13482]\tLoss: 1950.0223\n",
      "Training Epoch: 6 [2100/13482]\tLoss: 1973.8484\n",
      "Training Epoch: 6 [2150/13482]\tLoss: 1989.1453\n",
      "Training Epoch: 6 [2200/13482]\tLoss: 1894.5925\n",
      "Training Epoch: 6 [2250/13482]\tLoss: 1970.0660\n",
      "Training Epoch: 6 [2300/13482]\tLoss: 1930.1793\n",
      "Training Epoch: 6 [2350/13482]\tLoss: 1929.8596\n",
      "Training Epoch: 6 [2400/13482]\tLoss: 1903.6930\n",
      "Training Epoch: 6 [2450/13482]\tLoss: 1943.4852\n",
      "Training Epoch: 6 [2500/13482]\tLoss: 1920.7341\n",
      "Training Epoch: 6 [2550/13482]\tLoss: 1928.5537\n",
      "Training Epoch: 6 [2600/13482]\tLoss: 1946.8018\n",
      "Training Epoch: 6 [2650/13482]\tLoss: 1928.8336\n",
      "Training Epoch: 6 [2700/13482]\tLoss: 1978.4603\n",
      "Training Epoch: 6 [2750/13482]\tLoss: 1951.5614\n",
      "Training Epoch: 6 [2800/13482]\tLoss: 1937.9100\n",
      "Training Epoch: 6 [2850/13482]\tLoss: 2037.8243\n",
      "Training Epoch: 6 [2900/13482]\tLoss: 2007.4604\n",
      "Training Epoch: 6 [2950/13482]\tLoss: 1954.7673\n",
      "Training Epoch: 6 [3000/13482]\tLoss: 1903.9270\n",
      "Training Epoch: 6 [3050/13482]\tLoss: 1963.9628\n",
      "Training Epoch: 6 [3100/13482]\tLoss: 1896.2001\n",
      "Training Epoch: 6 [3150/13482]\tLoss: 1906.0135\n",
      "Training Epoch: 6 [3200/13482]\tLoss: 1933.6555\n",
      "Training Epoch: 6 [3250/13482]\tLoss: 1967.5681\n",
      "Training Epoch: 6 [3300/13482]\tLoss: 1997.5875\n",
      "Training Epoch: 6 [3350/13482]\tLoss: 1991.8975\n",
      "Training Epoch: 6 [3400/13482]\tLoss: 1931.6438\n",
      "Training Epoch: 6 [3450/13482]\tLoss: 1984.2108\n",
      "Training Epoch: 6 [3500/13482]\tLoss: 1929.8713\n",
      "Training Epoch: 6 [3550/13482]\tLoss: 1960.7126\n",
      "Training Epoch: 6 [3600/13482]\tLoss: 2037.2203\n",
      "Training Epoch: 6 [3650/13482]\tLoss: 1918.1013\n",
      "Training Epoch: 6 [3700/13482]\tLoss: 1877.1332\n",
      "Training Epoch: 6 [3750/13482]\tLoss: 1971.0424\n",
      "Training Epoch: 6 [3800/13482]\tLoss: 1940.4441\n",
      "Training Epoch: 6 [3850/13482]\tLoss: 1915.6548\n",
      "Training Epoch: 6 [3900/13482]\tLoss: 1932.6731\n",
      "Training Epoch: 6 [3950/13482]\tLoss: 1916.3834\n",
      "Training Epoch: 6 [4000/13482]\tLoss: 1921.3522\n",
      "Training Epoch: 6 [4050/13482]\tLoss: 1930.4668\n",
      "Training Epoch: 6 [4100/13482]\tLoss: 1977.6722\n",
      "Training Epoch: 6 [4150/13482]\tLoss: 1938.0042\n",
      "Training Epoch: 6 [4200/13482]\tLoss: 1927.0840\n",
      "Training Epoch: 6 [4250/13482]\tLoss: 1968.9827\n",
      "Training Epoch: 6 [4300/13482]\tLoss: 1951.1772\n",
      "Training Epoch: 6 [4350/13482]\tLoss: 1912.8544\n",
      "Training Epoch: 6 [4400/13482]\tLoss: 1992.6464\n",
      "Training Epoch: 6 [4450/13482]\tLoss: 1922.5134\n",
      "Training Epoch: 6 [4500/13482]\tLoss: 1944.4105\n",
      "Training Epoch: 6 [4550/13482]\tLoss: 1873.3296\n",
      "Training Epoch: 6 [4600/13482]\tLoss: 1861.9614\n",
      "Training Epoch: 6 [4650/13482]\tLoss: 1961.6844\n",
      "Training Epoch: 6 [4700/13482]\tLoss: 1913.2676\n",
      "Training Epoch: 6 [4750/13482]\tLoss: 1907.5513\n",
      "Training Epoch: 6 [4800/13482]\tLoss: 1926.5784\n",
      "Training Epoch: 6 [4850/13482]\tLoss: 1910.3606\n",
      "Training Epoch: 6 [4900/13482]\tLoss: 1959.5385\n",
      "Training Epoch: 6 [4950/13482]\tLoss: 1947.0017\n",
      "Training Epoch: 6 [5000/13482]\tLoss: 1950.8256\n",
      "Training Epoch: 6 [5050/13482]\tLoss: 1937.4780\n",
      "Training Epoch: 6 [5100/13482]\tLoss: 1894.0782\n",
      "Training Epoch: 6 [5150/13482]\tLoss: 1922.0732\n",
      "Training Epoch: 6 [5200/13482]\tLoss: 1963.8553\n",
      "Training Epoch: 6 [5250/13482]\tLoss: 1923.8954\n",
      "Training Epoch: 6 [5300/13482]\tLoss: 1918.6128\n",
      "Training Epoch: 6 [5350/13482]\tLoss: 1876.6342\n",
      "Training Epoch: 6 [5400/13482]\tLoss: 1949.0723\n",
      "Training Epoch: 6 [5450/13482]\tLoss: 1914.9646\n",
      "Training Epoch: 6 [5500/13482]\tLoss: 1943.3452\n",
      "Training Epoch: 6 [5550/13482]\tLoss: 1919.9229\n",
      "Training Epoch: 6 [5600/13482]\tLoss: 1897.7874\n",
      "Training Epoch: 6 [5650/13482]\tLoss: 1907.5288\n",
      "Training Epoch: 6 [5700/13482]\tLoss: 1914.6917\n",
      "Training Epoch: 6 [5750/13482]\tLoss: 1965.9069\n",
      "Training Epoch: 6 [5800/13482]\tLoss: 1959.0212\n",
      "Training Epoch: 6 [5850/13482]\tLoss: 2028.7581\n",
      "Training Epoch: 6 [5900/13482]\tLoss: 1925.3959\n",
      "Training Epoch: 6 [5950/13482]\tLoss: 1904.8105\n",
      "Training Epoch: 6 [6000/13482]\tLoss: 1914.5787\n",
      "Training Epoch: 6 [6050/13482]\tLoss: 1934.2845\n",
      "Training Epoch: 6 [6100/13482]\tLoss: 1942.7948\n",
      "Training Epoch: 6 [6150/13482]\tLoss: 1969.2539\n",
      "Training Epoch: 6 [6200/13482]\tLoss: 2009.6465\n",
      "Training Epoch: 6 [6250/13482]\tLoss: 1897.7018\n",
      "Training Epoch: 6 [6300/13482]\tLoss: 1866.2769\n",
      "Training Epoch: 6 [6350/13482]\tLoss: 1940.3669\n",
      "Training Epoch: 6 [6400/13482]\tLoss: 1944.6405\n",
      "Training Epoch: 6 [6450/13482]\tLoss: 1912.8508\n",
      "Training Epoch: 6 [6500/13482]\tLoss: 1891.2274\n",
      "Training Epoch: 6 [6550/13482]\tLoss: 1976.9882\n",
      "Training Epoch: 6 [6600/13482]\tLoss: 1879.0553\n",
      "Training Epoch: 6 [6650/13482]\tLoss: 1933.4945\n",
      "Training Epoch: 6 [6700/13482]\tLoss: 1887.6293\n",
      "Training Epoch: 6 [6750/13482]\tLoss: 1908.5106\n",
      "Training Epoch: 6 [6800/13482]\tLoss: 1891.3129\n",
      "Training Epoch: 6 [6850/13482]\tLoss: 1890.7004\n",
      "Training Epoch: 6 [6900/13482]\tLoss: 1912.4379\n",
      "Training Epoch: 6 [6950/13482]\tLoss: 1881.7201\n",
      "Training Epoch: 6 [7000/13482]\tLoss: 1918.0529\n",
      "Training Epoch: 6 [7050/13482]\tLoss: 1950.0132\n",
      "Training Epoch: 6 [7100/13482]\tLoss: 1926.8706\n",
      "Training Epoch: 6 [7150/13482]\tLoss: 1939.4309\n",
      "Training Epoch: 6 [7200/13482]\tLoss: 1935.5305\n",
      "Training Epoch: 6 [7250/13482]\tLoss: 1893.6194\n",
      "Training Epoch: 6 [7300/13482]\tLoss: 1916.5236\n",
      "Training Epoch: 6 [7350/13482]\tLoss: 1880.1394\n",
      "Training Epoch: 6 [7400/13482]\tLoss: 1950.9982\n",
      "Training Epoch: 6 [7450/13482]\tLoss: 1906.2272\n",
      "Training Epoch: 6 [7500/13482]\tLoss: 1854.3785\n",
      "Training Epoch: 6 [7550/13482]\tLoss: 1945.4070\n",
      "Training Epoch: 6 [7600/13482]\tLoss: 1933.5917\n",
      "Training Epoch: 6 [7650/13482]\tLoss: 1947.3711\n",
      "Training Epoch: 6 [7700/13482]\tLoss: 1881.5303\n",
      "Training Epoch: 6 [7750/13482]\tLoss: 1872.9523\n",
      "Training Epoch: 6 [7800/13482]\tLoss: 1956.1160\n",
      "Training Epoch: 6 [7850/13482]\tLoss: 1956.6346\n",
      "Training Epoch: 6 [7900/13482]\tLoss: 1909.9573\n",
      "Training Epoch: 6 [7950/13482]\tLoss: 1942.4314\n",
      "Training Epoch: 6 [8000/13482]\tLoss: 1865.1766\n",
      "Training Epoch: 6 [8050/13482]\tLoss: 1953.2061\n",
      "Training Epoch: 6 [8100/13482]\tLoss: 1919.3026\n",
      "Training Epoch: 6 [8150/13482]\tLoss: 1941.7168\n",
      "Training Epoch: 6 [8200/13482]\tLoss: 1916.1688\n",
      "Training Epoch: 6 [8250/13482]\tLoss: 1845.6978\n",
      "Training Epoch: 6 [8300/13482]\tLoss: 1895.5167\n",
      "Training Epoch: 6 [8350/13482]\tLoss: 1918.2251\n",
      "Training Epoch: 6 [8400/13482]\tLoss: 1949.1714\n",
      "Training Epoch: 6 [8450/13482]\tLoss: 1878.8007\n",
      "Training Epoch: 6 [8500/13482]\tLoss: 1903.4901\n",
      "Training Epoch: 6 [8550/13482]\tLoss: 1879.4773\n",
      "Training Epoch: 6 [8600/13482]\tLoss: 1921.1506\n",
      "Training Epoch: 6 [8650/13482]\tLoss: 1943.3597\n",
      "Training Epoch: 6 [8700/13482]\tLoss: 1895.6361\n",
      "Training Epoch: 6 [8750/13482]\tLoss: 1895.1394\n",
      "Training Epoch: 6 [8800/13482]\tLoss: 1903.0791\n",
      "Training Epoch: 6 [8850/13482]\tLoss: 1882.9132\n",
      "Training Epoch: 6 [8900/13482]\tLoss: 1907.0226\n",
      "Training Epoch: 6 [8950/13482]\tLoss: 1885.6842\n",
      "Training Epoch: 6 [9000/13482]\tLoss: 1891.3292\n",
      "Training Epoch: 6 [9050/13482]\tLoss: 1916.0740\n",
      "Training Epoch: 6 [9100/13482]\tLoss: 1858.5500\n",
      "Training Epoch: 6 [9150/13482]\tLoss: 1925.6437\n",
      "Training Epoch: 6 [9200/13482]\tLoss: 1967.9769\n",
      "Training Epoch: 6 [9250/13482]\tLoss: 1851.5472\n",
      "Training Epoch: 6 [9300/13482]\tLoss: 1928.7673\n",
      "Training Epoch: 6 [9350/13482]\tLoss: 1906.2708\n",
      "Training Epoch: 6 [9400/13482]\tLoss: 1838.7799\n",
      "Training Epoch: 6 [9450/13482]\tLoss: 1934.4680\n",
      "Training Epoch: 6 [9500/13482]\tLoss: 1883.4930\n",
      "Training Epoch: 6 [9550/13482]\tLoss: 1903.7087\n",
      "Training Epoch: 6 [9600/13482]\tLoss: 1925.0225\n",
      "Training Epoch: 6 [9650/13482]\tLoss: 1897.3099\n",
      "Training Epoch: 6 [9700/13482]\tLoss: 1908.4729\n",
      "Training Epoch: 6 [9750/13482]\tLoss: 1879.6589\n",
      "Training Epoch: 6 [9800/13482]\tLoss: 1862.8612\n",
      "Training Epoch: 6 [9850/13482]\tLoss: 1862.5527\n",
      "Training Epoch: 6 [9900/13482]\tLoss: 1892.8030\n",
      "Training Epoch: 6 [9950/13482]\tLoss: 1914.8561\n",
      "Training Epoch: 6 [10000/13482]\tLoss: 1891.0088\n",
      "Training Epoch: 6 [10050/13482]\tLoss: 1929.7494\n",
      "Training Epoch: 6 [10100/13482]\tLoss: 1867.6986\n",
      "Training Epoch: 6 [10150/13482]\tLoss: 1897.5780\n",
      "Training Epoch: 6 [10200/13482]\tLoss: 1873.4021\n",
      "Training Epoch: 6 [10250/13482]\tLoss: 1955.9979\n",
      "Training Epoch: 6 [10300/13482]\tLoss: 1893.4885\n",
      "Training Epoch: 6 [10350/13482]\tLoss: 1887.5560\n",
      "Training Epoch: 6 [10400/13482]\tLoss: 1857.2559\n",
      "Training Epoch: 6 [10450/13482]\tLoss: 1893.1873\n",
      "Training Epoch: 6 [10500/13482]\tLoss: 1975.7617\n",
      "Training Epoch: 6 [10550/13482]\tLoss: 1915.2550\n",
      "Training Epoch: 6 [10600/13482]\tLoss: 1882.4849\n",
      "Training Epoch: 6 [10650/13482]\tLoss: 1851.8627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [10700/13482]\tLoss: 1870.9797\n",
      "Training Epoch: 6 [10750/13482]\tLoss: 1891.0082\n",
      "Training Epoch: 6 [10800/13482]\tLoss: 1870.3848\n",
      "Training Epoch: 6 [10850/13482]\tLoss: 1861.6077\n",
      "Training Epoch: 6 [10900/13482]\tLoss: 1863.2911\n",
      "Training Epoch: 6 [10950/13482]\tLoss: 1850.2834\n",
      "Training Epoch: 6 [11000/13482]\tLoss: 1874.0446\n",
      "Training Epoch: 6 [11050/13482]\tLoss: 1892.3451\n",
      "Training Epoch: 6 [11100/13482]\tLoss: 1871.6461\n",
      "Training Epoch: 6 [11150/13482]\tLoss: 1888.7268\n",
      "Training Epoch: 6 [11200/13482]\tLoss: 1851.1514\n",
      "Training Epoch: 6 [11250/13482]\tLoss: 1921.4844\n",
      "Training Epoch: 6 [11300/13482]\tLoss: 1889.5140\n",
      "Training Epoch: 6 [11350/13482]\tLoss: 1930.9240\n",
      "Training Epoch: 6 [11400/13482]\tLoss: 1892.5153\n",
      "Training Epoch: 6 [11450/13482]\tLoss: 1844.4310\n",
      "Training Epoch: 6 [11500/13482]\tLoss: 1874.5432\n",
      "Training Epoch: 6 [11550/13482]\tLoss: 1918.9224\n",
      "Training Epoch: 6 [11600/13482]\tLoss: 1890.4069\n",
      "Training Epoch: 6 [11650/13482]\tLoss: 1884.2643\n",
      "Training Epoch: 6 [11700/13482]\tLoss: 1842.5215\n",
      "Training Epoch: 6 [11750/13482]\tLoss: 1825.5817\n",
      "Training Epoch: 6 [11800/13482]\tLoss: 1902.0020\n",
      "Training Epoch: 6 [11850/13482]\tLoss: 1821.2034\n",
      "Training Epoch: 6 [11900/13482]\tLoss: 1848.8169\n",
      "Training Epoch: 6 [11950/13482]\tLoss: 1918.7947\n",
      "Training Epoch: 6 [12000/13482]\tLoss: 1834.5667\n",
      "Training Epoch: 6 [12050/13482]\tLoss: 1842.5181\n",
      "Training Epoch: 6 [12100/13482]\tLoss: 1913.5923\n",
      "Training Epoch: 6 [12150/13482]\tLoss: 1886.5022\n",
      "Training Epoch: 6 [12200/13482]\tLoss: 1865.8785\n",
      "Training Epoch: 6 [12250/13482]\tLoss: 1854.4362\n",
      "Training Epoch: 6 [12300/13482]\tLoss: 1898.1427\n",
      "Training Epoch: 6 [12350/13482]\tLoss: 1863.9470\n",
      "Training Epoch: 6 [12400/13482]\tLoss: 1880.4523\n",
      "Training Epoch: 6 [12450/13482]\tLoss: 1841.7081\n",
      "Training Epoch: 6 [12500/13482]\tLoss: 1913.6158\n",
      "Training Epoch: 6 [12550/13482]\tLoss: 1873.7363\n",
      "Training Epoch: 6 [12600/13482]\tLoss: 1873.0631\n",
      "Training Epoch: 6 [12650/13482]\tLoss: 1857.9520\n",
      "Training Epoch: 6 [12700/13482]\tLoss: 1833.9435\n",
      "Training Epoch: 6 [12750/13482]\tLoss: 1865.6998\n",
      "Training Epoch: 6 [12800/13482]\tLoss: 1871.7153\n",
      "Training Epoch: 6 [12850/13482]\tLoss: 1884.0825\n",
      "Training Epoch: 6 [12900/13482]\tLoss: 1910.9496\n",
      "Training Epoch: 6 [12950/13482]\tLoss: 1868.5151\n",
      "Training Epoch: 6 [13000/13482]\tLoss: 1872.3630\n",
      "Training Epoch: 6 [13050/13482]\tLoss: 1901.5437\n",
      "Training Epoch: 6 [13100/13482]\tLoss: 1837.1227\n",
      "Training Epoch: 6 [13150/13482]\tLoss: 1852.8517\n",
      "Training Epoch: 6 [13200/13482]\tLoss: 1879.6599\n",
      "Training Epoch: 6 [13250/13482]\tLoss: 1901.4922\n",
      "Training Epoch: 6 [13300/13482]\tLoss: 1868.7299\n",
      "Training Epoch: 6 [13350/13482]\tLoss: 1833.9556\n",
      "Training Epoch: 6 [13400/13482]\tLoss: 1907.8721\n",
      "Training Epoch: 6 [13450/13482]\tLoss: 1911.3990\n",
      "Training Epoch: 6 [13482/13482]\tLoss: 1893.7616\n",
      "Training Epoch: 6 [1497/1497]\tLoss: 1853.5435\n",
      "Training Epoch: 7 [50/13482]\tLoss: 1862.2697\n",
      "Training Epoch: 7 [100/13482]\tLoss: 1877.2025\n",
      "Training Epoch: 7 [150/13482]\tLoss: 1875.4983\n",
      "Training Epoch: 7 [200/13482]\tLoss: 1894.5903\n",
      "Training Epoch: 7 [250/13482]\tLoss: 1864.0439\n",
      "Training Epoch: 7 [300/13482]\tLoss: 1859.1324\n",
      "Training Epoch: 7 [350/13482]\tLoss: 1852.0547\n",
      "Training Epoch: 7 [400/13482]\tLoss: 1832.2775\n",
      "Training Epoch: 7 [450/13482]\tLoss: 1851.6802\n",
      "Training Epoch: 7 [500/13482]\tLoss: 1854.3198\n",
      "Training Epoch: 7 [550/13482]\tLoss: 1871.5803\n",
      "Training Epoch: 7 [600/13482]\tLoss: 1876.0748\n",
      "Training Epoch: 7 [650/13482]\tLoss: 1875.2972\n",
      "Training Epoch: 7 [700/13482]\tLoss: 1895.1226\n",
      "Training Epoch: 7 [750/13482]\tLoss: 1849.7550\n",
      "Training Epoch: 7 [800/13482]\tLoss: 1880.3595\n",
      "Training Epoch: 7 [850/13482]\tLoss: 1856.8385\n",
      "Training Epoch: 7 [900/13482]\tLoss: 1838.7936\n",
      "Training Epoch: 7 [950/13482]\tLoss: 1885.2498\n",
      "Training Epoch: 7 [1000/13482]\tLoss: 1911.6672\n",
      "Training Epoch: 7 [1050/13482]\tLoss: 1851.0005\n",
      "Training Epoch: 7 [1100/13482]\tLoss: 1885.7449\n",
      "Training Epoch: 7 [1150/13482]\tLoss: 1861.7198\n",
      "Training Epoch: 7 [1200/13482]\tLoss: 1850.5479\n",
      "Training Epoch: 7 [1250/13482]\tLoss: 1846.8224\n",
      "Training Epoch: 7 [1300/13482]\tLoss: 1830.1143\n",
      "Training Epoch: 7 [1350/13482]\tLoss: 1817.8762\n",
      "Training Epoch: 7 [1400/13482]\tLoss: 1848.2600\n",
      "Training Epoch: 7 [1450/13482]\tLoss: 1818.5477\n",
      "Training Epoch: 7 [1500/13482]\tLoss: 1860.8512\n",
      "Training Epoch: 7 [1550/13482]\tLoss: 1831.0479\n",
      "Training Epoch: 7 [1600/13482]\tLoss: 1807.0410\n",
      "Training Epoch: 7 [1650/13482]\tLoss: 1844.9183\n",
      "Training Epoch: 7 [1700/13482]\tLoss: 1868.8207\n",
      "Training Epoch: 7 [1750/13482]\tLoss: 1814.3215\n",
      "Training Epoch: 7 [1800/13482]\tLoss: 1879.4391\n",
      "Training Epoch: 7 [1850/13482]\tLoss: 1860.1173\n",
      "Training Epoch: 7 [1900/13482]\tLoss: 1824.4434\n",
      "Training Epoch: 7 [1950/13482]\tLoss: 1854.0427\n",
      "Training Epoch: 7 [2000/13482]\tLoss: 1855.0343\n",
      "Training Epoch: 7 [2050/13482]\tLoss: 1833.2013\n",
      "Training Epoch: 7 [2100/13482]\tLoss: 1855.6143\n",
      "Training Epoch: 7 [2150/13482]\tLoss: 1875.9957\n",
      "Training Epoch: 7 [2200/13482]\tLoss: 1783.7467\n",
      "Training Epoch: 7 [2250/13482]\tLoss: 1861.7125\n",
      "Training Epoch: 7 [2300/13482]\tLoss: 1822.7900\n",
      "Training Epoch: 7 [2350/13482]\tLoss: 1809.8134\n",
      "Training Epoch: 7 [2400/13482]\tLoss: 1791.4111\n",
      "Training Epoch: 7 [2450/13482]\tLoss: 1825.9863\n",
      "Training Epoch: 7 [2500/13482]\tLoss: 1808.1003\n",
      "Training Epoch: 7 [2550/13482]\tLoss: 1813.3079\n",
      "Training Epoch: 7 [2600/13482]\tLoss: 1833.0961\n",
      "Training Epoch: 7 [2650/13482]\tLoss: 1826.7994\n",
      "Training Epoch: 7 [2700/13482]\tLoss: 1858.2086\n",
      "Training Epoch: 7 [2750/13482]\tLoss: 1838.2628\n",
      "Training Epoch: 7 [2800/13482]\tLoss: 1831.7628\n",
      "Training Epoch: 7 [2850/13482]\tLoss: 1908.3384\n",
      "Training Epoch: 7 [2900/13482]\tLoss: 1896.6127\n",
      "Training Epoch: 7 [2950/13482]\tLoss: 1836.4417\n",
      "Training Epoch: 7 [3000/13482]\tLoss: 1787.9432\n",
      "Training Epoch: 7 [3050/13482]\tLoss: 1853.4071\n",
      "Training Epoch: 7 [3100/13482]\tLoss: 1789.7505\n",
      "Training Epoch: 7 [3150/13482]\tLoss: 1795.4990\n",
      "Training Epoch: 7 [3200/13482]\tLoss: 1820.2607\n",
      "Training Epoch: 7 [3250/13482]\tLoss: 1859.9220\n",
      "Training Epoch: 7 [3300/13482]\tLoss: 1883.6388\n",
      "Training Epoch: 7 [3350/13482]\tLoss: 1876.3464\n",
      "Training Epoch: 7 [3400/13482]\tLoss: 1814.8247\n",
      "Training Epoch: 7 [3450/13482]\tLoss: 1874.9340\n",
      "Training Epoch: 7 [3500/13482]\tLoss: 1813.3470\n",
      "Training Epoch: 7 [3550/13482]\tLoss: 1850.4025\n",
      "Training Epoch: 7 [3600/13482]\tLoss: 1921.0408\n",
      "Training Epoch: 7 [3650/13482]\tLoss: 1814.0438\n",
      "Training Epoch: 7 [3700/13482]\tLoss: 1771.1794\n",
      "Training Epoch: 7 [3750/13482]\tLoss: 1867.1521\n",
      "Training Epoch: 7 [3800/13482]\tLoss: 1825.7374\n",
      "Training Epoch: 7 [3850/13482]\tLoss: 1796.4719\n",
      "Training Epoch: 7 [3900/13482]\tLoss: 1812.6426\n",
      "Training Epoch: 7 [3950/13482]\tLoss: 1802.8998\n",
      "Training Epoch: 7 [4000/13482]\tLoss: 1807.0803\n",
      "Training Epoch: 7 [4050/13482]\tLoss: 1824.0017\n",
      "Training Epoch: 7 [4100/13482]\tLoss: 1874.2474\n",
      "Training Epoch: 7 [4150/13482]\tLoss: 1828.2336\n",
      "Training Epoch: 7 [4200/13482]\tLoss: 1817.0433\n",
      "Training Epoch: 7 [4250/13482]\tLoss: 1853.9333\n",
      "Training Epoch: 7 [4300/13482]\tLoss: 1839.6528\n",
      "Training Epoch: 7 [4350/13482]\tLoss: 1802.1401\n",
      "Training Epoch: 7 [4400/13482]\tLoss: 1886.1185\n",
      "Training Epoch: 7 [4450/13482]\tLoss: 1819.4028\n",
      "Training Epoch: 7 [4500/13482]\tLoss: 1840.4575\n",
      "Training Epoch: 7 [4550/13482]\tLoss: 1771.5780\n",
      "Training Epoch: 7 [4600/13482]\tLoss: 1752.4833\n",
      "Training Epoch: 7 [4650/13482]\tLoss: 1853.5879\n",
      "Training Epoch: 7 [4700/13482]\tLoss: 1802.9802\n",
      "Training Epoch: 7 [4750/13482]\tLoss: 1788.5206\n",
      "Training Epoch: 7 [4800/13482]\tLoss: 1825.8625\n",
      "Training Epoch: 7 [4850/13482]\tLoss: 1814.8528\n",
      "Training Epoch: 7 [4900/13482]\tLoss: 1850.4182\n",
      "Training Epoch: 7 [4950/13482]\tLoss: 1837.1377\n",
      "Training Epoch: 7 [5000/13482]\tLoss: 1838.3774\n",
      "Training Epoch: 7 [5050/13482]\tLoss: 1825.3378\n",
      "Training Epoch: 7 [5100/13482]\tLoss: 1792.9890\n",
      "Training Epoch: 7 [5150/13482]\tLoss: 1818.4807\n",
      "Training Epoch: 7 [5200/13482]\tLoss: 1854.1449\n",
      "Training Epoch: 7 [5250/13482]\tLoss: 1820.4139\n",
      "Training Epoch: 7 [5300/13482]\tLoss: 1814.8428\n",
      "Training Epoch: 7 [5350/13482]\tLoss: 1768.2318\n",
      "Training Epoch: 7 [5400/13482]\tLoss: 1836.1226\n",
      "Training Epoch: 7 [5450/13482]\tLoss: 1811.0934\n",
      "Training Epoch: 7 [5500/13482]\tLoss: 1843.1423\n",
      "Training Epoch: 7 [5550/13482]\tLoss: 1822.3223\n",
      "Training Epoch: 7 [5600/13482]\tLoss: 1795.1107\n",
      "Training Epoch: 7 [5650/13482]\tLoss: 1801.6241\n",
      "Training Epoch: 7 [5700/13482]\tLoss: 1813.0598\n",
      "Training Epoch: 7 [5750/13482]\tLoss: 1859.7831\n",
      "Training Epoch: 7 [5800/13482]\tLoss: 1856.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [5850/13482]\tLoss: 1923.4585\n",
      "Training Epoch: 7 [5900/13482]\tLoss: 1819.7964\n",
      "Training Epoch: 7 [5950/13482]\tLoss: 1793.3419\n",
      "Training Epoch: 7 [6000/13482]\tLoss: 1809.4080\n",
      "Training Epoch: 7 [6050/13482]\tLoss: 1826.6603\n",
      "Training Epoch: 7 [6100/13482]\tLoss: 1844.3436\n",
      "Training Epoch: 7 [6150/13482]\tLoss: 1864.7611\n",
      "Training Epoch: 7 [6200/13482]\tLoss: 1899.7821\n",
      "Training Epoch: 7 [6250/13482]\tLoss: 1791.3251\n",
      "Training Epoch: 7 [6300/13482]\tLoss: 1766.9969\n",
      "Training Epoch: 7 [6350/13482]\tLoss: 1823.9868\n",
      "Training Epoch: 7 [6400/13482]\tLoss: 1846.1332\n",
      "Training Epoch: 7 [6450/13482]\tLoss: 1813.6110\n",
      "Training Epoch: 7 [6500/13482]\tLoss: 1789.4008\n",
      "Training Epoch: 7 [6550/13482]\tLoss: 1876.7103\n",
      "Training Epoch: 7 [6600/13482]\tLoss: 1786.3708\n",
      "Training Epoch: 7 [6650/13482]\tLoss: 1825.0536\n",
      "Training Epoch: 7 [6700/13482]\tLoss: 1786.8291\n",
      "Training Epoch: 7 [6750/13482]\tLoss: 1807.0005\n",
      "Training Epoch: 7 [6800/13482]\tLoss: 1791.0028\n",
      "Training Epoch: 7 [6850/13482]\tLoss: 1790.1387\n",
      "Training Epoch: 7 [6900/13482]\tLoss: 1812.9148\n",
      "Training Epoch: 7 [6950/13482]\tLoss: 1779.3865\n",
      "Training Epoch: 7 [7000/13482]\tLoss: 1815.9331\n",
      "Training Epoch: 7 [7050/13482]\tLoss: 1839.1412\n",
      "Training Epoch: 7 [7100/13482]\tLoss: 1827.5232\n",
      "Training Epoch: 7 [7150/13482]\tLoss: 1839.2708\n",
      "Training Epoch: 7 [7200/13482]\tLoss: 1826.7526\n",
      "Training Epoch: 7 [7250/13482]\tLoss: 1792.5179\n",
      "Training Epoch: 7 [7300/13482]\tLoss: 1820.7046\n",
      "Training Epoch: 7 [7350/13482]\tLoss: 1781.2107\n",
      "Training Epoch: 7 [7400/13482]\tLoss: 1849.7656\n",
      "Training Epoch: 7 [7450/13482]\tLoss: 1804.5923\n",
      "Training Epoch: 7 [7500/13482]\tLoss: 1753.4608\n",
      "Training Epoch: 7 [7550/13482]\tLoss: 1848.5902\n",
      "Training Epoch: 7 [7600/13482]\tLoss: 1833.3376\n",
      "Training Epoch: 7 [7650/13482]\tLoss: 1850.6042\n",
      "Training Epoch: 7 [7700/13482]\tLoss: 1781.9258\n",
      "Training Epoch: 7 [7750/13482]\tLoss: 1768.8998\n",
      "Training Epoch: 7 [7800/13482]\tLoss: 1856.7422\n",
      "Training Epoch: 7 [7850/13482]\tLoss: 1857.0043\n",
      "Training Epoch: 7 [7900/13482]\tLoss: 1806.2880\n",
      "Training Epoch: 7 [7950/13482]\tLoss: 1843.0321\n",
      "Training Epoch: 7 [8000/13482]\tLoss: 1769.6655\n",
      "Training Epoch: 7 [8050/13482]\tLoss: 1853.4757\n",
      "Training Epoch: 7 [8100/13482]\tLoss: 1819.0675\n",
      "Training Epoch: 7 [8150/13482]\tLoss: 1838.0018\n",
      "Training Epoch: 7 [8200/13482]\tLoss: 1823.2972\n",
      "Training Epoch: 7 [8250/13482]\tLoss: 1745.9534\n",
      "Training Epoch: 7 [8300/13482]\tLoss: 1792.8884\n",
      "Training Epoch: 7 [8350/13482]\tLoss: 1822.7659\n",
      "Training Epoch: 7 [8400/13482]\tLoss: 1847.1422\n",
      "Training Epoch: 7 [8450/13482]\tLoss: 1777.7416\n",
      "Training Epoch: 7 [8500/13482]\tLoss: 1805.1337\n",
      "Training Epoch: 7 [8550/13482]\tLoss: 1785.8657\n",
      "Training Epoch: 7 [8600/13482]\tLoss: 1829.1853\n",
      "Training Epoch: 7 [8650/13482]\tLoss: 1841.1110\n",
      "Training Epoch: 7 [8700/13482]\tLoss: 1802.0040\n",
      "Training Epoch: 7 [8750/13482]\tLoss: 1797.8187\n",
      "Training Epoch: 7 [8800/13482]\tLoss: 1794.9766\n",
      "Training Epoch: 7 [8850/13482]\tLoss: 1793.7595\n",
      "Training Epoch: 7 [8900/13482]\tLoss: 1809.9368\n",
      "Training Epoch: 7 [8950/13482]\tLoss: 1781.7007\n",
      "Training Epoch: 7 [9000/13482]\tLoss: 1803.4260\n",
      "Training Epoch: 7 [9050/13482]\tLoss: 1816.7032\n",
      "Training Epoch: 7 [9100/13482]\tLoss: 1766.5092\n",
      "Training Epoch: 7 [9150/13482]\tLoss: 1829.0070\n",
      "Training Epoch: 7 [9200/13482]\tLoss: 1873.4624\n",
      "Training Epoch: 7 [9250/13482]\tLoss: 1754.8541\n",
      "Training Epoch: 7 [9300/13482]\tLoss: 1833.0441\n",
      "Training Epoch: 7 [9350/13482]\tLoss: 1805.3397\n",
      "Training Epoch: 7 [9400/13482]\tLoss: 1743.1172\n",
      "Training Epoch: 7 [9450/13482]\tLoss: 1836.2062\n",
      "Training Epoch: 7 [9500/13482]\tLoss: 1786.7151\n",
      "Training Epoch: 7 [9550/13482]\tLoss: 1803.1245\n",
      "Training Epoch: 7 [9600/13482]\tLoss: 1822.7185\n",
      "Training Epoch: 7 [9650/13482]\tLoss: 1796.7245\n",
      "Training Epoch: 7 [9700/13482]\tLoss: 1806.3660\n",
      "Training Epoch: 7 [9750/13482]\tLoss: 1783.8104\n",
      "Training Epoch: 7 [9800/13482]\tLoss: 1763.8353\n",
      "Training Epoch: 7 [9850/13482]\tLoss: 1764.0515\n",
      "Training Epoch: 7 [9900/13482]\tLoss: 1803.5114\n",
      "Training Epoch: 7 [9950/13482]\tLoss: 1814.1328\n",
      "Training Epoch: 7 [10000/13482]\tLoss: 1790.0573\n",
      "Training Epoch: 7 [10050/13482]\tLoss: 1835.5887\n",
      "Training Epoch: 7 [10100/13482]\tLoss: 1780.6171\n",
      "Training Epoch: 7 [10150/13482]\tLoss: 1800.8888\n",
      "Training Epoch: 7 [10200/13482]\tLoss: 1773.7336\n",
      "Training Epoch: 7 [10250/13482]\tLoss: 1856.5054\n",
      "Training Epoch: 7 [10300/13482]\tLoss: 1792.4749\n",
      "Training Epoch: 7 [10350/13482]\tLoss: 1793.4943\n",
      "Training Epoch: 7 [10400/13482]\tLoss: 1760.5280\n",
      "Training Epoch: 7 [10450/13482]\tLoss: 1797.9023\n",
      "Training Epoch: 7 [10500/13482]\tLoss: 1872.5575\n",
      "Training Epoch: 7 [10550/13482]\tLoss: 1821.2001\n",
      "Training Epoch: 7 [10600/13482]\tLoss: 1793.7555\n",
      "Training Epoch: 7 [10650/13482]\tLoss: 1760.6477\n",
      "Training Epoch: 7 [10700/13482]\tLoss: 1778.2363\n",
      "Training Epoch: 7 [10750/13482]\tLoss: 1789.5264\n",
      "Training Epoch: 7 [10800/13482]\tLoss: 1773.8678\n",
      "Training Epoch: 7 [10850/13482]\tLoss: 1775.3389\n",
      "Training Epoch: 7 [10900/13482]\tLoss: 1766.0819\n",
      "Training Epoch: 7 [10950/13482]\tLoss: 1760.9520\n",
      "Training Epoch: 7 [11000/13482]\tLoss: 1781.1909\n",
      "Training Epoch: 7 [11050/13482]\tLoss: 1798.6808\n",
      "Training Epoch: 7 [11100/13482]\tLoss: 1782.8688\n",
      "Training Epoch: 7 [11150/13482]\tLoss: 1794.7843\n",
      "Training Epoch: 7 [11200/13482]\tLoss: 1756.9462\n",
      "Training Epoch: 7 [11250/13482]\tLoss: 1826.4825\n",
      "Training Epoch: 7 [11300/13482]\tLoss: 1794.3469\n",
      "Training Epoch: 7 [11350/13482]\tLoss: 1838.5242\n",
      "Training Epoch: 7 [11400/13482]\tLoss: 1798.1195\n",
      "Training Epoch: 7 [11450/13482]\tLoss: 1754.8022\n",
      "Training Epoch: 7 [11500/13482]\tLoss: 1784.0948\n",
      "Training Epoch: 7 [11550/13482]\tLoss: 1828.1055\n",
      "Training Epoch: 7 [11600/13482]\tLoss: 1794.2789\n",
      "Training Epoch: 7 [11650/13482]\tLoss: 1797.2338\n",
      "Training Epoch: 7 [11700/13482]\tLoss: 1752.9913\n",
      "Training Epoch: 7 [11750/13482]\tLoss: 1739.7500\n",
      "Training Epoch: 7 [11800/13482]\tLoss: 1810.2500\n",
      "Training Epoch: 7 [11850/13482]\tLoss: 1732.9862\n",
      "Training Epoch: 7 [11900/13482]\tLoss: 1758.2212\n",
      "Training Epoch: 7 [11950/13482]\tLoss: 1832.2780\n",
      "Training Epoch: 7 [12000/13482]\tLoss: 1740.2211\n",
      "Training Epoch: 7 [12050/13482]\tLoss: 1744.9343\n",
      "Training Epoch: 7 [12100/13482]\tLoss: 1819.7379\n",
      "Training Epoch: 7 [12150/13482]\tLoss: 1786.8292\n",
      "Training Epoch: 7 [12200/13482]\tLoss: 1774.5439\n",
      "Training Epoch: 7 [12250/13482]\tLoss: 1759.6790\n",
      "Training Epoch: 7 [12300/13482]\tLoss: 1808.4565\n",
      "Training Epoch: 7 [12350/13482]\tLoss: 1770.5957\n",
      "Training Epoch: 7 [12400/13482]\tLoss: 1789.4801\n",
      "Training Epoch: 7 [12450/13482]\tLoss: 1750.9473\n",
      "Training Epoch: 7 [12500/13482]\tLoss: 1816.8477\n",
      "Training Epoch: 7 [12550/13482]\tLoss: 1783.4020\n",
      "Training Epoch: 7 [12600/13482]\tLoss: 1785.3884\n",
      "Training Epoch: 7 [12650/13482]\tLoss: 1770.8633\n",
      "Training Epoch: 7 [12700/13482]\tLoss: 1742.1741\n",
      "Training Epoch: 7 [12750/13482]\tLoss: 1778.6987\n",
      "Training Epoch: 7 [12800/13482]\tLoss: 1777.6730\n",
      "Training Epoch: 7 [12850/13482]\tLoss: 1799.3433\n",
      "Training Epoch: 7 [12900/13482]\tLoss: 1818.6638\n",
      "Training Epoch: 7 [12950/13482]\tLoss: 1771.7725\n",
      "Training Epoch: 7 [13000/13482]\tLoss: 1787.8739\n",
      "Training Epoch: 7 [13050/13482]\tLoss: 1814.3363\n",
      "Training Epoch: 7 [13100/13482]\tLoss: 1748.7041\n",
      "Training Epoch: 7 [13150/13482]\tLoss: 1765.9246\n",
      "Training Epoch: 7 [13200/13482]\tLoss: 1789.4209\n",
      "Training Epoch: 7 [13250/13482]\tLoss: 1805.8514\n",
      "Training Epoch: 7 [13300/13482]\tLoss: 1775.3409\n",
      "Training Epoch: 7 [13350/13482]\tLoss: 1738.1478\n",
      "Training Epoch: 7 [13400/13482]\tLoss: 1812.1301\n",
      "Training Epoch: 7 [13450/13482]\tLoss: 1821.5012\n",
      "Training Epoch: 7 [13482/13482]\tLoss: 1801.9977\n",
      "Training Epoch: 7 [1497/1497]\tLoss: 1764.9011\n",
      "Training Epoch: 8 [50/13482]\tLoss: 1777.7781\n",
      "Training Epoch: 8 [100/13482]\tLoss: 1789.0449\n",
      "Training Epoch: 8 [150/13482]\tLoss: 1786.9098\n",
      "Training Epoch: 8 [200/13482]\tLoss: 1800.4512\n",
      "Training Epoch: 8 [250/13482]\tLoss: 1777.4076\n",
      "Training Epoch: 8 [300/13482]\tLoss: 1768.4282\n",
      "Training Epoch: 8 [350/13482]\tLoss: 1760.5616\n",
      "Training Epoch: 8 [400/13482]\tLoss: 1751.1842\n",
      "Training Epoch: 8 [450/13482]\tLoss: 1765.1514\n",
      "Training Epoch: 8 [500/13482]\tLoss: 1769.8658\n",
      "Training Epoch: 8 [550/13482]\tLoss: 1784.7184\n",
      "Training Epoch: 8 [600/13482]\tLoss: 1791.7067\n",
      "Training Epoch: 8 [650/13482]\tLoss: 1790.0845\n",
      "Training Epoch: 8 [700/13482]\tLoss: 1812.0006\n",
      "Training Epoch: 8 [750/13482]\tLoss: 1766.9899\n",
      "Training Epoch: 8 [800/13482]\tLoss: 1794.2177\n",
      "Training Epoch: 8 [850/13482]\tLoss: 1766.6527\n",
      "Training Epoch: 8 [900/13482]\tLoss: 1752.2039\n",
      "Training Epoch: 8 [950/13482]\tLoss: 1800.2159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [1000/13482]\tLoss: 1820.1031\n",
      "Training Epoch: 8 [1050/13482]\tLoss: 1759.0089\n",
      "Training Epoch: 8 [1100/13482]\tLoss: 1796.3237\n",
      "Training Epoch: 8 [1150/13482]\tLoss: 1775.4374\n",
      "Training Epoch: 8 [1200/13482]\tLoss: 1768.7047\n",
      "Training Epoch: 8 [1250/13482]\tLoss: 1758.5216\n",
      "Training Epoch: 8 [1300/13482]\tLoss: 1750.3988\n",
      "Training Epoch: 8 [1350/13482]\tLoss: 1734.8259\n",
      "Training Epoch: 8 [1400/13482]\tLoss: 1761.3286\n",
      "Training Epoch: 8 [1450/13482]\tLoss: 1732.8929\n",
      "Training Epoch: 8 [1500/13482]\tLoss: 1777.8250\n",
      "Training Epoch: 8 [1550/13482]\tLoss: 1741.5074\n",
      "Training Epoch: 8 [1600/13482]\tLoss: 1721.3431\n",
      "Training Epoch: 8 [1650/13482]\tLoss: 1766.8071\n",
      "Training Epoch: 8 [1700/13482]\tLoss: 1786.1584\n",
      "Training Epoch: 8 [1750/13482]\tLoss: 1725.5735\n",
      "Training Epoch: 8 [1800/13482]\tLoss: 1782.8623\n",
      "Training Epoch: 8 [1850/13482]\tLoss: 1778.4620\n",
      "Training Epoch: 8 [1900/13482]\tLoss: 1740.4493\n",
      "Training Epoch: 8 [1950/13482]\tLoss: 1768.8677\n",
      "Training Epoch: 8 [2000/13482]\tLoss: 1772.1775\n",
      "Training Epoch: 8 [2050/13482]\tLoss: 1746.0420\n",
      "Training Epoch: 8 [2100/13482]\tLoss: 1769.2130\n",
      "Training Epoch: 8 [2150/13482]\tLoss: 1791.5994\n",
      "Training Epoch: 8 [2200/13482]\tLoss: 1701.8192\n",
      "Training Epoch: 8 [2250/13482]\tLoss: 1781.8478\n",
      "Training Epoch: 8 [2300/13482]\tLoss: 1742.8542\n",
      "Training Epoch: 8 [2350/13482]\tLoss: 1722.3132\n",
      "Training Epoch: 8 [2400/13482]\tLoss: 1709.7240\n",
      "Training Epoch: 8 [2450/13482]\tLoss: 1738.6311\n",
      "Training Epoch: 8 [2500/13482]\tLoss: 1725.5625\n",
      "Training Epoch: 8 [2550/13482]\tLoss: 1728.4264\n",
      "Training Epoch: 8 [2600/13482]\tLoss: 1748.4874\n",
      "Training Epoch: 8 [2650/13482]\tLoss: 1751.6216\n",
      "Training Epoch: 8 [2700/13482]\tLoss: 1769.1436\n",
      "Training Epoch: 8 [2750/13482]\tLoss: 1754.7067\n",
      "Training Epoch: 8 [2800/13482]\tLoss: 1753.5151\n",
      "Training Epoch: 8 [2850/13482]\tLoss: 1812.0391\n",
      "Training Epoch: 8 [2900/13482]\tLoss: 1813.1589\n",
      "Training Epoch: 8 [2950/13482]\tLoss: 1749.6101\n",
      "Training Epoch: 8 [3000/13482]\tLoss: 1701.8505\n",
      "Training Epoch: 8 [3050/13482]\tLoss: 1770.7152\n",
      "Training Epoch: 8 [3100/13482]\tLoss: 1710.3425\n",
      "Training Epoch: 8 [3150/13482]\tLoss: 1713.5526\n",
      "Training Epoch: 8 [3200/13482]\tLoss: 1735.2406\n",
      "Training Epoch: 8 [3250/13482]\tLoss: 1778.5587\n",
      "Training Epoch: 8 [3300/13482]\tLoss: 1798.2112\n",
      "Training Epoch: 8 [3350/13482]\tLoss: 1791.4492\n",
      "Training Epoch: 8 [3400/13482]\tLoss: 1728.8026\n",
      "Training Epoch: 8 [3450/13482]\tLoss: 1793.6694\n",
      "Training Epoch: 8 [3500/13482]\tLoss: 1725.9854\n",
      "Training Epoch: 8 [3550/13482]\tLoss: 1768.3008\n",
      "Training Epoch: 8 [3600/13482]\tLoss: 1834.7684\n",
      "Training Epoch: 8 [3650/13482]\tLoss: 1736.8114\n",
      "Training Epoch: 8 [3700/13482]\tLoss: 1692.4022\n",
      "Training Epoch: 8 [3750/13482]\tLoss: 1789.8486\n",
      "Training Epoch: 8 [3800/13482]\tLoss: 1740.8369\n",
      "Training Epoch: 8 [3850/13482]\tLoss: 1707.9263\n",
      "Training Epoch: 8 [3900/13482]\tLoss: 1723.6228\n",
      "Training Epoch: 8 [3950/13482]\tLoss: 1719.0492\n",
      "Training Epoch: 8 [4000/13482]\tLoss: 1721.9694\n",
      "Training Epoch: 8 [4050/13482]\tLoss: 1744.9401\n",
      "Training Epoch: 8 [4100/13482]\tLoss: 1796.7919\n",
      "Training Epoch: 8 [4150/13482]\tLoss: 1745.8129\n",
      "Training Epoch: 8 [4200/13482]\tLoss: 1735.7877\n",
      "Training Epoch: 8 [4250/13482]\tLoss: 1769.1423\n",
      "Training Epoch: 8 [4300/13482]\tLoss: 1757.1127\n",
      "Training Epoch: 8 [4350/13482]\tLoss: 1720.7102\n",
      "Training Epoch: 8 [4400/13482]\tLoss: 1807.5829\n",
      "Training Epoch: 8 [4450/13482]\tLoss: 1742.2134\n",
      "Training Epoch: 8 [4500/13482]\tLoss: 1763.2714\n",
      "Training Epoch: 8 [4550/13482]\tLoss: 1695.3512\n",
      "Training Epoch: 8 [4600/13482]\tLoss: 1670.6073\n",
      "Training Epoch: 8 [4650/13482]\tLoss: 1772.4513\n",
      "Training Epoch: 8 [4700/13482]\tLoss: 1720.9629\n",
      "Training Epoch: 8 [4750/13482]\tLoss: 1700.5553\n",
      "Training Epoch: 8 [4800/13482]\tLoss: 1749.6588\n",
      "Training Epoch: 8 [4850/13482]\tLoss: 1743.1566\n",
      "Training Epoch: 8 [4900/13482]\tLoss: 1768.6897\n",
      "Training Epoch: 8 [4950/13482]\tLoss: 1754.7611\n",
      "Training Epoch: 8 [5000/13482]\tLoss: 1754.8003\n",
      "Training Epoch: 8 [5050/13482]\tLoss: 1742.4000\n",
      "Training Epoch: 8 [5100/13482]\tLoss: 1716.7582\n",
      "Training Epoch: 8 [5150/13482]\tLoss: 1741.3903\n",
      "Training Epoch: 8 [5200/13482]\tLoss: 1772.1841\n",
      "Training Epoch: 8 [5250/13482]\tLoss: 1742.4962\n",
      "Training Epoch: 8 [5300/13482]\tLoss: 1736.8090\n",
      "Training Epoch: 8 [5350/13482]\tLoss: 1687.1277\n",
      "Training Epoch: 8 [5400/13482]\tLoss: 1751.4752\n",
      "Training Epoch: 8 [5450/13482]\tLoss: 1733.0203\n",
      "Training Epoch: 8 [5500/13482]\tLoss: 1767.1713\n",
      "Training Epoch: 8 [5550/13482]\tLoss: 1748.9108\n",
      "Training Epoch: 8 [5600/13482]\tLoss: 1718.4469\n",
      "Training Epoch: 8 [5650/13482]\tLoss: 1722.1989\n",
      "Training Epoch: 8 [5700/13482]\tLoss: 1737.0872\n",
      "Training Epoch: 8 [5750/13482]\tLoss: 1779.8577\n",
      "Training Epoch: 8 [5800/13482]\tLoss: 1779.8027\n",
      "Training Epoch: 8 [5850/13482]\tLoss: 1843.1755\n",
      "Training Epoch: 8 [5900/13482]\tLoss: 1740.3291\n",
      "Training Epoch: 8 [5950/13482]\tLoss: 1710.6008\n",
      "Training Epoch: 8 [6000/13482]\tLoss: 1730.8530\n",
      "Training Epoch: 8 [6050/13482]\tLoss: 1746.3223\n",
      "Training Epoch: 8 [6100/13482]\tLoss: 1770.5936\n",
      "Training Epoch: 8 [6150/13482]\tLoss: 1786.3630\n",
      "Training Epoch: 8 [6200/13482]\tLoss: 1817.4086\n",
      "Training Epoch: 8 [6250/13482]\tLoss: 1710.9630\n",
      "Training Epoch: 8 [6300/13482]\tLoss: 1692.9550\n",
      "Training Epoch: 8 [6350/13482]\tLoss: 1736.7050\n",
      "Training Epoch: 8 [6400/13482]\tLoss: 1772.5404\n",
      "Training Epoch: 8 [6450/13482]\tLoss: 1739.4198\n",
      "Training Epoch: 8 [6500/13482]\tLoss: 1713.1831\n",
      "Training Epoch: 8 [6550/13482]\tLoss: 1801.5552\n",
      "Training Epoch: 8 [6600/13482]\tLoss: 1716.2703\n",
      "Training Epoch: 8 [6650/13482]\tLoss: 1743.3911\n",
      "Training Epoch: 8 [6700/13482]\tLoss: 1710.7651\n",
      "Training Epoch: 8 [6750/13482]\tLoss: 1730.6638\n",
      "Training Epoch: 8 [6800/13482]\tLoss: 1716.2150\n",
      "Training Epoch: 8 [6850/13482]\tLoss: 1714.4906\n",
      "Training Epoch: 8 [6900/13482]\tLoss: 1738.0010\n",
      "Training Epoch: 8 [6950/13482]\tLoss: 1701.4736\n",
      "Training Epoch: 8 [7000/13482]\tLoss: 1738.9775\n",
      "Training Epoch: 8 [7050/13482]\tLoss: 1756.0131\n",
      "Training Epoch: 8 [7100/13482]\tLoss: 1752.4390\n",
      "Training Epoch: 8 [7150/13482]\tLoss: 1764.1676\n",
      "Training Epoch: 8 [7200/13482]\tLoss: 1744.3291\n",
      "Training Epoch: 8 [7250/13482]\tLoss: 1716.3550\n",
      "Training Epoch: 8 [7300/13482]\tLoss: 1748.2361\n",
      "Training Epoch: 8 [7350/13482]\tLoss: 1707.0167\n",
      "Training Epoch: 8 [7400/13482]\tLoss: 1773.3993\n",
      "Training Epoch: 8 [7450/13482]\tLoss: 1728.3630\n",
      "Training Epoch: 8 [7500/13482]\tLoss: 1677.2374\n",
      "Training Epoch: 8 [7550/13482]\tLoss: 1775.7385\n",
      "Training Epoch: 8 [7600/13482]\tLoss: 1756.9397\n",
      "Training Epoch: 8 [7650/13482]\tLoss: 1777.1155\n",
      "Training Epoch: 8 [7700/13482]\tLoss: 1706.8623\n",
      "Training Epoch: 8 [7750/13482]\tLoss: 1690.3618\n",
      "Training Epoch: 8 [7800/13482]\tLoss: 1781.9573\n",
      "Training Epoch: 8 [7850/13482]\tLoss: 1781.4988\n",
      "Training Epoch: 8 [7900/13482]\tLoss: 1728.3766\n",
      "Training Epoch: 8 [7950/13482]\tLoss: 1768.5569\n",
      "Training Epoch: 8 [8000/13482]\tLoss: 1697.7742\n",
      "Training Epoch: 8 [8050/13482]\tLoss: 1777.9476\n",
      "Training Epoch: 8 [8100/13482]\tLoss: 1743.4600\n",
      "Training Epoch: 8 [8150/13482]\tLoss: 1759.7261\n",
      "Training Epoch: 8 [8200/13482]\tLoss: 1752.7976\n",
      "Training Epoch: 8 [8250/13482]\tLoss: 1670.7360\n",
      "Training Epoch: 8 [8300/13482]\tLoss: 1715.6167\n",
      "Training Epoch: 8 [8350/13482]\tLoss: 1750.4064\n",
      "Training Epoch: 8 [8400/13482]\tLoss: 1770.3918\n",
      "Training Epoch: 8 [8450/13482]\tLoss: 1702.1027\n",
      "Training Epoch: 8 [8500/13482]\tLoss: 1730.5813\n",
      "Training Epoch: 8 [8550/13482]\tLoss: 1714.1403\n",
      "Training Epoch: 8 [8600/13482]\tLoss: 1760.0981\n",
      "Training Epoch: 8 [8650/13482]\tLoss: 1764.0162\n",
      "Training Epoch: 8 [8700/13482]\tLoss: 1731.3403\n",
      "Training Epoch: 8 [8750/13482]\tLoss: 1724.0237\n",
      "Training Epoch: 8 [8800/13482]\tLoss: 1712.7284\n",
      "Training Epoch: 8 [8850/13482]\tLoss: 1725.9177\n",
      "Training Epoch: 8 [8900/13482]\tLoss: 1736.7174\n",
      "Training Epoch: 8 [8950/13482]\tLoss: 1703.6757\n",
      "Training Epoch: 8 [9000/13482]\tLoss: 1736.2688\n",
      "Training Epoch: 8 [9050/13482]\tLoss: 1741.4924\n",
      "Training Epoch: 8 [9100/13482]\tLoss: 1696.7990\n",
      "Training Epoch: 8 [9150/13482]\tLoss: 1755.7574\n",
      "Training Epoch: 8 [9200/13482]\tLoss: 1801.9175\n",
      "Training Epoch: 8 [9250/13482]\tLoss: 1681.4796\n",
      "Training Epoch: 8 [9300/13482]\tLoss: 1760.1843\n",
      "Training Epoch: 8 [9350/13482]\tLoss: 1728.9327\n",
      "Training Epoch: 8 [9400/13482]\tLoss: 1670.0032\n",
      "Training Epoch: 8 [9450/13482]\tLoss: 1761.4310\n",
      "Training Epoch: 8 [9500/13482]\tLoss: 1713.7540\n",
      "Training Epoch: 8 [9550/13482]\tLoss: 1727.4177\n",
      "Training Epoch: 8 [9600/13482]\tLoss: 1745.5400\n",
      "Training Epoch: 8 [9650/13482]\tLoss: 1720.0042\n",
      "Training Epoch: 8 [9700/13482]\tLoss: 1728.9696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [9750/13482]\tLoss: 1711.0463\n",
      "Training Epoch: 8 [9800/13482]\tLoss: 1688.5802\n",
      "Training Epoch: 8 [9850/13482]\tLoss: 1688.8601\n",
      "Training Epoch: 8 [9900/13482]\tLoss: 1736.3202\n",
      "Training Epoch: 8 [9950/13482]\tLoss: 1737.6012\n",
      "Training Epoch: 8 [10000/13482]\tLoss: 1713.9028\n",
      "Training Epoch: 8 [10050/13482]\tLoss: 1764.9601\n",
      "Training Epoch: 8 [10100/13482]\tLoss: 1714.3596\n",
      "Training Epoch: 8 [10150/13482]\tLoss: 1727.5963\n",
      "Training Epoch: 8 [10200/13482]\tLoss: 1698.5624\n",
      "Training Epoch: 8 [10250/13482]\tLoss: 1780.5768\n",
      "Training Epoch: 8 [10300/13482]\tLoss: 1714.8025\n",
      "Training Epoch: 8 [10350/13482]\tLoss: 1721.9524\n",
      "Training Epoch: 8 [10400/13482]\tLoss: 1687.5831\n",
      "Training Epoch: 8 [10450/13482]\tLoss: 1726.0768\n",
      "Training Epoch: 8 [10500/13482]\tLoss: 1794.8572\n",
      "Training Epoch: 8 [10550/13482]\tLoss: 1749.6234\n",
      "Training Epoch: 8 [10600/13482]\tLoss: 1725.6390\n",
      "Training Epoch: 8 [10650/13482]\tLoss: 1691.0394\n",
      "Training Epoch: 8 [10700/13482]\tLoss: 1707.3700\n",
      "Training Epoch: 8 [10750/13482]\tLoss: 1711.6692\n",
      "Training Epoch: 8 [10800/13482]\tLoss: 1700.6953\n",
      "Training Epoch: 8 [10850/13482]\tLoss: 1709.5298\n",
      "Training Epoch: 8 [10900/13482]\tLoss: 1692.9957\n",
      "Training Epoch: 8 [10950/13482]\tLoss: 1692.8007\n",
      "Training Epoch: 8 [11000/13482]\tLoss: 1710.8054\n",
      "Training Epoch: 8 [11050/13482]\tLoss: 1726.8517\n",
      "Training Epoch: 8 [11100/13482]\tLoss: 1715.4921\n",
      "Training Epoch: 8 [11150/13482]\tLoss: 1722.1715\n",
      "Training Epoch: 8 [11200/13482]\tLoss: 1685.1464\n",
      "Training Epoch: 8 [11250/13482]\tLoss: 1753.6266\n",
      "Training Epoch: 8 [11300/13482]\tLoss: 1722.1633\n",
      "Training Epoch: 8 [11350/13482]\tLoss: 1768.6688\n",
      "Training Epoch: 8 [11400/13482]\tLoss: 1726.5790\n",
      "Training Epoch: 8 [11450/13482]\tLoss: 1686.7252\n",
      "Training Epoch: 8 [11500/13482]\tLoss: 1714.7395\n",
      "Training Epoch: 8 [11550/13482]\tLoss: 1758.0549\n",
      "Training Epoch: 8 [11600/13482]\tLoss: 1720.1360\n",
      "Training Epoch: 8 [11650/13482]\tLoss: 1730.0468\n",
      "Training Epoch: 8 [11700/13482]\tLoss: 1684.7922\n",
      "Training Epoch: 8 [11750/13482]\tLoss: 1674.0256\n",
      "Training Epoch: 8 [11800/13482]\tLoss: 1740.8613\n",
      "Training Epoch: 8 [11850/13482]\tLoss: 1665.9915\n",
      "Training Epoch: 8 [11900/13482]\tLoss: 1689.0913\n",
      "Training Epoch: 8 [11950/13482]\tLoss: 1766.9598\n",
      "Training Epoch: 8 [12000/13482]\tLoss: 1667.6479\n",
      "Training Epoch: 8 [12050/13482]\tLoss: 1669.7604\n",
      "Training Epoch: 8 [12100/13482]\tLoss: 1747.2014\n",
      "Training Epoch: 8 [12150/13482]\tLoss: 1710.7087\n",
      "Training Epoch: 8 [12200/13482]\tLoss: 1705.3257\n",
      "Training Epoch: 8 [12250/13482]\tLoss: 1687.7422\n",
      "Training Epoch: 8 [12300/13482]\tLoss: 1740.0050\n",
      "Training Epoch: 8 [12350/13482]\tLoss: 1699.2469\n",
      "Training Epoch: 8 [12400/13482]\tLoss: 1718.9197\n",
      "Training Epoch: 8 [12450/13482]\tLoss: 1680.8096\n",
      "Training Epoch: 8 [12500/13482]\tLoss: 1742.9257\n",
      "Training Epoch: 8 [12550/13482]\tLoss: 1714.5013\n",
      "Training Epoch: 8 [12600/13482]\tLoss: 1717.6782\n",
      "Training Epoch: 8 [12650/13482]\tLoss: 1703.8989\n",
      "Training Epoch: 8 [12700/13482]\tLoss: 1671.8912\n",
      "Training Epoch: 8 [12750/13482]\tLoss: 1711.9152\n",
      "Training Epoch: 8 [12800/13482]\tLoss: 1705.4521\n",
      "Training Epoch: 8 [12850/13482]\tLoss: 1734.2717\n",
      "Training Epoch: 8 [12900/13482]\tLoss: 1747.6512\n",
      "Training Epoch: 8 [12950/13482]\tLoss: 1696.7864\n",
      "Training Epoch: 8 [13000/13482]\tLoss: 1723.6017\n",
      "Training Epoch: 8 [13050/13482]\tLoss: 1746.0707\n",
      "Training Epoch: 8 [13100/13482]\tLoss: 1681.0778\n",
      "Training Epoch: 8 [13150/13482]\tLoss: 1699.2764\n",
      "Training Epoch: 8 [13200/13482]\tLoss: 1719.8809\n",
      "Training Epoch: 8 [13250/13482]\tLoss: 1733.3127\n",
      "Training Epoch: 8 [13300/13482]\tLoss: 1704.1758\n",
      "Training Epoch: 8 [13350/13482]\tLoss: 1665.8640\n",
      "Training Epoch: 8 [13400/13482]\tLoss: 1737.9657\n",
      "Training Epoch: 8 [13450/13482]\tLoss: 1752.0344\n",
      "Training Epoch: 8 [13482/13482]\tLoss: 1731.6692\n",
      "Training Epoch: 8 [1497/1497]\tLoss: 1696.6580\n",
      "Training Epoch: 9 [50/13482]\tLoss: 1713.5216\n",
      "Training Epoch: 9 [100/13482]\tLoss: 1720.9799\n",
      "Training Epoch: 9 [150/13482]\tLoss: 1718.7816\n",
      "Training Epoch: 9 [200/13482]\tLoss: 1727.7896\n",
      "Training Epoch: 9 [250/13482]\tLoss: 1711.3418\n",
      "Training Epoch: 9 [300/13482]\tLoss: 1698.9423\n",
      "Training Epoch: 9 [350/13482]\tLoss: 1690.2635\n",
      "Training Epoch: 9 [400/13482]\tLoss: 1689.3258\n",
      "Training Epoch: 9 [450/13482]\tLoss: 1698.4941\n",
      "Training Epoch: 9 [500/13482]\tLoss: 1705.2090\n",
      "Training Epoch: 9 [550/13482]\tLoss: 1718.5569\n",
      "Training Epoch: 9 [600/13482]\tLoss: 1725.5215\n",
      "Training Epoch: 9 [650/13482]\tLoss: 1724.7766\n",
      "Training Epoch: 9 [700/13482]\tLoss: 1748.2391\n",
      "Training Epoch: 9 [750/13482]\tLoss: 1703.2249\n",
      "Training Epoch: 9 [800/13482]\tLoss: 1727.7744\n",
      "Training Epoch: 9 [850/13482]\tLoss: 1697.6132\n",
      "Training Epoch: 9 [900/13482]\tLoss: 1685.1935\n",
      "Training Epoch: 9 [950/13482]\tLoss: 1733.7965\n",
      "Training Epoch: 9 [1000/13482]\tLoss: 1749.7996\n",
      "Training Epoch: 9 [1050/13482]\tLoss: 1688.1788\n",
      "Training Epoch: 9 [1100/13482]\tLoss: 1727.2858\n",
      "Training Epoch: 9 [1150/13482]\tLoss: 1709.5492\n",
      "Training Epoch: 9 [1200/13482]\tLoss: 1706.3191\n",
      "Training Epoch: 9 [1250/13482]\tLoss: 1691.5787\n",
      "Training Epoch: 9 [1300/13482]\tLoss: 1688.7352\n",
      "Training Epoch: 9 [1350/13482]\tLoss: 1670.7751\n",
      "Training Epoch: 9 [1400/13482]\tLoss: 1693.7700\n",
      "Training Epoch: 9 [1450/13482]\tLoss: 1667.3643\n",
      "Training Epoch: 9 [1500/13482]\tLoss: 1713.0325\n",
      "Training Epoch: 9 [1550/13482]\tLoss: 1672.9006\n",
      "Training Epoch: 9 [1600/13482]\tLoss: 1655.1041\n",
      "Training Epoch: 9 [1650/13482]\tLoss: 1706.6171\n",
      "Training Epoch: 9 [1700/13482]\tLoss: 1721.9840\n",
      "Training Epoch: 9 [1750/13482]\tLoss: 1656.7903\n",
      "Training Epoch: 9 [1800/13482]\tLoss: 1708.5121\n",
      "Training Epoch: 9 [1850/13482]\tLoss: 1714.6448\n",
      "Training Epoch: 9 [1900/13482]\tLoss: 1675.9988\n",
      "Training Epoch: 9 [1950/13482]\tLoss: 1702.5153\n",
      "Training Epoch: 9 [2000/13482]\tLoss: 1707.7164\n",
      "Training Epoch: 9 [2050/13482]\tLoss: 1678.3702\n",
      "Training Epoch: 9 [2100/13482]\tLoss: 1703.1227\n",
      "Training Epoch: 9 [2150/13482]\tLoss: 1725.9393\n",
      "Training Epoch: 9 [2200/13482]\tLoss: 1638.7034\n",
      "Training Epoch: 9 [2250/13482]\tLoss: 1720.0831\n",
      "Training Epoch: 9 [2300/13482]\tLoss: 1680.9052\n",
      "Training Epoch: 9 [2350/13482]\tLoss: 1655.3323\n",
      "Training Epoch: 9 [2400/13482]\tLoss: 1646.9418\n",
      "Training Epoch: 9 [2450/13482]\tLoss: 1670.6090\n",
      "Training Epoch: 9 [2500/13482]\tLoss: 1662.0660\n",
      "Training Epoch: 9 [2550/13482]\tLoss: 1663.3073\n",
      "Training Epoch: 9 [2600/13482]\tLoss: 1682.9524\n",
      "Training Epoch: 9 [2650/13482]\tLoss: 1693.4181\n",
      "Training Epoch: 9 [2700/13482]\tLoss: 1700.5498\n",
      "Training Epoch: 9 [2750/13482]\tLoss: 1690.2800\n",
      "Training Epoch: 9 [2800/13482]\tLoss: 1692.9579\n",
      "Training Epoch: 9 [2850/13482]\tLoss: 1737.5792\n",
      "Training Epoch: 9 [2900/13482]\tLoss: 1747.4537\n",
      "Training Epoch: 9 [2950/13482]\tLoss: 1683.1550\n",
      "Training Epoch: 9 [3000/13482]\tLoss: 1634.9136\n",
      "Training Epoch: 9 [3050/13482]\tLoss: 1706.0099\n",
      "Training Epoch: 9 [3100/13482]\tLoss: 1648.6248\n",
      "Training Epoch: 9 [3150/13482]\tLoss: 1650.1848\n",
      "Training Epoch: 9 [3200/13482]\tLoss: 1668.9108\n",
      "Training Epoch: 9 [3250/13482]\tLoss: 1714.2583\n",
      "Training Epoch: 9 [3300/13482]\tLoss: 1731.6272\n",
      "Training Epoch: 9 [3350/13482]\tLoss: 1726.2142\n",
      "Training Epoch: 9 [3400/13482]\tLoss: 1662.5392\n",
      "Training Epoch: 9 [3450/13482]\tLoss: 1730.6066\n",
      "Training Epoch: 9 [3500/13482]\tLoss: 1657.8634\n",
      "Training Epoch: 9 [3550/13482]\tLoss: 1704.3827\n",
      "Training Epoch: 9 [3600/13482]\tLoss: 1768.0604\n",
      "Training Epoch: 9 [3650/13482]\tLoss: 1676.6959\n",
      "Training Epoch: 9 [3700/13482]\tLoss: 1631.1821\n",
      "Training Epoch: 9 [3750/13482]\tLoss: 1729.3295\n",
      "Training Epoch: 9 [3800/13482]\tLoss: 1674.7103\n",
      "Training Epoch: 9 [3850/13482]\tLoss: 1639.0038\n",
      "Training Epoch: 9 [3900/13482]\tLoss: 1654.4906\n",
      "Training Epoch: 9 [3950/13482]\tLoss: 1654.3912\n",
      "Training Epoch: 9 [4000/13482]\tLoss: 1655.9082\n",
      "Training Epoch: 9 [4050/13482]\tLoss: 1683.9095\n",
      "Training Epoch: 9 [4100/13482]\tLoss: 1736.4266\n",
      "Training Epoch: 9 [4150/13482]\tLoss: 1681.2115\n",
      "Training Epoch: 9 [4200/13482]\tLoss: 1672.9283\n",
      "Training Epoch: 9 [4250/13482]\tLoss: 1703.6694\n",
      "Training Epoch: 9 [4300/13482]\tLoss: 1693.0798\n",
      "Training Epoch: 9 [4350/13482]\tLoss: 1658.1606\n",
      "Training Epoch: 9 [4400/13482]\tLoss: 1747.0510\n",
      "Training Epoch: 9 [4450/13482]\tLoss: 1681.7905\n",
      "Training Epoch: 9 [4500/13482]\tLoss: 1703.3943\n",
      "Training Epoch: 9 [4550/13482]\tLoss: 1635.6321\n",
      "Training Epoch: 9 [4600/13482]\tLoss: 1606.6903\n",
      "Training Epoch: 9 [4650/13482]\tLoss: 1709.1283\n",
      "Training Epoch: 9 [4700/13482]\tLoss: 1657.3209\n",
      "Training Epoch: 9 [4750/13482]\tLoss: 1632.7666\n",
      "Training Epoch: 9 [4800/13482]\tLoss: 1689.3170\n",
      "Training Epoch: 9 [4850/13482]\tLoss: 1686.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [4900/13482]\tLoss: 1704.6655\n",
      "Training Epoch: 9 [4950/13482]\tLoss: 1690.4585\n",
      "Training Epoch: 9 [5000/13482]\tLoss: 1689.6637\n",
      "Training Epoch: 9 [5050/13482]\tLoss: 1678.0936\n",
      "Training Epoch: 9 [5100/13482]\tLoss: 1656.7543\n",
      "Training Epoch: 9 [5150/13482]\tLoss: 1681.8302\n",
      "Training Epoch: 9 [5200/13482]\tLoss: 1708.4500\n",
      "Training Epoch: 9 [5250/13482]\tLoss: 1681.1047\n",
      "Training Epoch: 9 [5300/13482]\tLoss: 1675.6910\n",
      "Training Epoch: 9 [5350/13482]\tLoss: 1624.2231\n",
      "Training Epoch: 9 [5400/13482]\tLoss: 1685.2939\n",
      "Training Epoch: 9 [5450/13482]\tLoss: 1671.7468\n",
      "Training Epoch: 9 [5500/13482]\tLoss: 1706.8802\n",
      "Training Epoch: 9 [5550/13482]\tLoss: 1690.3662\n",
      "Training Epoch: 9 [5600/13482]\tLoss: 1658.7421\n",
      "Training Epoch: 9 [5650/13482]\tLoss: 1659.9806\n",
      "Training Epoch: 9 [5700/13482]\tLoss: 1677.5635\n",
      "Training Epoch: 9 [5750/13482]\tLoss: 1717.2540\n",
      "Training Epoch: 9 [5800/13482]\tLoss: 1719.4119\n",
      "Training Epoch: 9 [5850/13482]\tLoss: 1779.2937\n",
      "Training Epoch: 9 [5900/13482]\tLoss: 1678.1466\n",
      "Training Epoch: 9 [5950/13482]\tLoss: 1645.9536\n",
      "Training Epoch: 9 [6000/13482]\tLoss: 1669.7100\n",
      "Training Epoch: 9 [6050/13482]\tLoss: 1684.2047\n",
      "Training Epoch: 9 [6100/13482]\tLoss: 1712.8633\n",
      "Training Epoch: 9 [6150/13482]\tLoss: 1724.8530\n",
      "Training Epoch: 9 [6200/13482]\tLoss: 1753.1279\n",
      "Training Epoch: 9 [6250/13482]\tLoss: 1647.8384\n",
      "Training Epoch: 9 [6300/13482]\tLoss: 1635.5792\n",
      "Training Epoch: 9 [6350/13482]\tLoss: 1668.7964\n",
      "Training Epoch: 9 [6400/13482]\tLoss: 1714.8030\n",
      "Training Epoch: 9 [6450/13482]\tLoss: 1681.2686\n",
      "Training Epoch: 9 [6500/13482]\tLoss: 1653.6620\n",
      "Training Epoch: 9 [6550/13482]\tLoss: 1742.5945\n",
      "Training Epoch: 9 [6600/13482]\tLoss: 1660.9095\n",
      "Training Epoch: 9 [6650/13482]\tLoss: 1679.2441\n",
      "Training Epoch: 9 [6700/13482]\tLoss: 1650.7391\n",
      "Training Epoch: 9 [6750/13482]\tLoss: 1670.6335\n",
      "Training Epoch: 9 [6800/13482]\tLoss: 1658.0339\n",
      "Training Epoch: 9 [6850/13482]\tLoss: 1655.2993\n",
      "Training Epoch: 9 [6900/13482]\tLoss: 1679.0781\n",
      "Training Epoch: 9 [6950/13482]\tLoss: 1639.6906\n",
      "Training Epoch: 9 [7000/13482]\tLoss: 1678.7571\n",
      "Training Epoch: 9 [7050/13482]\tLoss: 1691.1716\n",
      "Training Epoch: 9 [7100/13482]\tLoss: 1693.1512\n",
      "Training Epoch: 9 [7150/13482]\tLoss: 1705.1078\n",
      "Training Epoch: 9 [7200/13482]\tLoss: 1679.6863\n",
      "Training Epoch: 9 [7250/13482]\tLoss: 1656.0227\n",
      "Training Epoch: 9 [7300/13482]\tLoss: 1690.9099\n",
      "Training Epoch: 9 [7350/13482]\tLoss: 1649.1034\n",
      "Training Epoch: 9 [7400/13482]\tLoss: 1713.3684\n",
      "Training Epoch: 9 [7450/13482]\tLoss: 1668.6665\n",
      "Training Epoch: 9 [7500/13482]\tLoss: 1617.6018\n",
      "Training Epoch: 9 [7550/13482]\tLoss: 1718.5996\n",
      "Training Epoch: 9 [7600/13482]\tLoss: 1695.8491\n",
      "Training Epoch: 9 [7650/13482]\tLoss: 1719.0150\n",
      "Training Epoch: 9 [7700/13482]\tLoss: 1647.2615\n",
      "Training Epoch: 9 [7750/13482]\tLoss: 1628.6556\n",
      "Training Epoch: 9 [7800/13482]\tLoss: 1722.9889\n",
      "Training Epoch: 9 [7850/13482]\tLoss: 1721.9364\n",
      "Training Epoch: 9 [7900/13482]\tLoss: 1667.4512\n",
      "Training Epoch: 9 [7950/13482]\tLoss: 1710.0540\n",
      "Training Epoch: 9 [8000/13482]\tLoss: 1641.4746\n",
      "Training Epoch: 9 [8050/13482]\tLoss: 1718.2158\n",
      "Training Epoch: 9 [8100/13482]\tLoss: 1683.7327\n",
      "Training Epoch: 9 [8150/13482]\tLoss: 1698.2498\n",
      "Training Epoch: 9 [8200/13482]\tLoss: 1697.1124\n",
      "Training Epoch: 9 [8250/13482]\tLoss: 1611.7971\n",
      "Training Epoch: 9 [8300/13482]\tLoss: 1654.9802\n",
      "Training Epoch: 9 [8350/13482]\tLoss: 1693.1183\n",
      "Training Epoch: 9 [8400/13482]\tLoss: 1710.2523\n",
      "Training Epoch: 9 [8450/13482]\tLoss: 1642.8442\n",
      "Training Epoch: 9 [8500/13482]\tLoss: 1672.0260\n",
      "Training Epoch: 9 [8550/13482]\tLoss: 1656.9319\n",
      "Training Epoch: 9 [8600/13482]\tLoss: 1706.0626\n",
      "Training Epoch: 9 [8650/13482]\tLoss: 1703.7242\n",
      "Training Epoch: 9 [8700/13482]\tLoss: 1675.7401\n",
      "Training Epoch: 9 [8750/13482]\tLoss: 1665.6082\n",
      "Training Epoch: 9 [8800/13482]\tLoss: 1647.8318\n",
      "Training Epoch: 9 [8850/13482]\tLoss: 1671.9595\n",
      "Training Epoch: 9 [8900/13482]\tLoss: 1679.3090\n",
      "Training Epoch: 9 [8950/13482]\tLoss: 1643.1058\n",
      "Training Epoch: 9 [9000/13482]\tLoss: 1682.7163\n",
      "Training Epoch: 9 [9050/13482]\tLoss: 1682.3811\n",
      "Training Epoch: 9 [9100/13482]\tLoss: 1641.8439\n",
      "Training Epoch: 9 [9150/13482]\tLoss: 1697.8724\n",
      "Training Epoch: 9 [9200/13482]\tLoss: 1745.2469\n",
      "Training Epoch: 9 [9250/13482]\tLoss: 1623.4749\n",
      "Training Epoch: 9 [9300/13482]\tLoss: 1702.1921\n",
      "Training Epoch: 9 [9350/13482]\tLoss: 1668.4102\n",
      "Training Epoch: 9 [9400/13482]\tLoss: 1612.1956\n",
      "Training Epoch: 9 [9450/13482]\tLoss: 1701.9541\n",
      "Training Epoch: 9 [9500/13482]\tLoss: 1656.6969\n",
      "Training Epoch: 9 [9550/13482]\tLoss: 1667.9944\n",
      "Training Epoch: 9 [9600/13482]\tLoss: 1684.6266\n",
      "Training Epoch: 9 [9650/13482]\tLoss: 1659.4501\n",
      "Training Epoch: 9 [9700/13482]\tLoss: 1667.7620\n",
      "Training Epoch: 9 [9750/13482]\tLoss: 1653.1500\n",
      "Training Epoch: 9 [9800/13482]\tLoss: 1629.1324\n",
      "Training Epoch: 9 [9850/13482]\tLoss: 1629.1267\n",
      "Training Epoch: 9 [9900/13482]\tLoss: 1683.2905\n",
      "Training Epoch: 9 [9950/13482]\tLoss: 1677.0298\n",
      "Training Epoch: 9 [10000/13482]\tLoss: 1654.1332\n",
      "Training Epoch: 9 [10050/13482]\tLoss: 1709.3149\n",
      "Training Epoch: 9 [10100/13482]\tLoss: 1662.0443\n",
      "Training Epoch: 9 [10150/13482]\tLoss: 1669.6664\n",
      "Training Epoch: 9 [10200/13482]\tLoss: 1638.9907\n",
      "Training Epoch: 9 [10250/13482]\tLoss: 1720.3400\n",
      "Training Epoch: 9 [10300/13482]\tLoss: 1652.9811\n",
      "Training Epoch: 9 [10350/13482]\tLoss: 1665.3207\n",
      "Training Epoch: 9 [10400/13482]\tLoss: 1630.7050\n",
      "Training Epoch: 9 [10450/13482]\tLoss: 1669.4661\n",
      "Training Epoch: 9 [10500/13482]\tLoss: 1733.9561\n",
      "Training Epoch: 9 [10550/13482]\tLoss: 1692.3354\n",
      "Training Epoch: 9 [10600/13482]\tLoss: 1670.5344\n",
      "Training Epoch: 9 [10650/13482]\tLoss: 1635.6072\n",
      "Training Epoch: 9 [10700/13482]\tLoss: 1651.3734\n",
      "Training Epoch: 9 [10750/13482]\tLoss: 1650.0336\n",
      "Training Epoch: 9 [10800/13482]\tLoss: 1643.0569\n",
      "Training Epoch: 9 [10850/13482]\tLoss: 1657.0615\n",
      "Training Epoch: 9 [10900/13482]\tLoss: 1635.5936\n",
      "Training Epoch: 9 [10950/13482]\tLoss: 1637.9077\n",
      "Training Epoch: 9 [11000/13482]\tLoss: 1655.2557\n",
      "Training Epoch: 9 [11050/13482]\tLoss: 1669.6501\n",
      "Training Epoch: 9 [11100/13482]\tLoss: 1662.3990\n",
      "Training Epoch: 9 [11150/13482]\tLoss: 1664.0508\n",
      "Training Epoch: 9 [11200/13482]\tLoss: 1628.6010\n",
      "Training Epoch: 9 [11250/13482]\tLoss: 1695.9027\n",
      "Training Epoch: 9 [11300/13482]\tLoss: 1665.1880\n",
      "Training Epoch: 9 [11350/13482]\tLoss: 1713.7135\n",
      "Training Epoch: 9 [11400/13482]\tLoss: 1670.0488\n",
      "Training Epoch: 9 [11450/13482]\tLoss: 1632.4067\n",
      "Training Epoch: 9 [11500/13482]\tLoss: 1659.0129\n",
      "Training Epoch: 9 [11550/13482]\tLoss: 1701.7336\n",
      "Training Epoch: 9 [11600/13482]\tLoss: 1660.7692\n",
      "Training Epoch: 9 [11650/13482]\tLoss: 1676.7655\n",
      "Training Epoch: 9 [11700/13482]\tLoss: 1630.8019\n",
      "Training Epoch: 9 [11750/13482]\tLoss: 1621.8274\n",
      "Training Epoch: 9 [11800/13482]\tLoss: 1685.5680\n",
      "Training Epoch: 9 [11850/13482]\tLoss: 1612.3403\n",
      "Training Epoch: 9 [11900/13482]\tLoss: 1633.5812\n",
      "Training Epoch: 9 [11950/13482]\tLoss: 1715.6841\n",
      "Training Epoch: 9 [12000/13482]\tLoss: 1610.0447\n",
      "Training Epoch: 9 [12050/13482]\tLoss: 1610.2845\n",
      "Training Epoch: 9 [12100/13482]\tLoss: 1689.4209\n",
      "Training Epoch: 9 [12150/13482]\tLoss: 1650.7928\n",
      "Training Epoch: 9 [12200/13482]\tLoss: 1650.6777\n",
      "Training Epoch: 9 [12250/13482]\tLoss: 1631.1255\n",
      "Training Epoch: 9 [12300/13482]\tLoss: 1685.0054\n",
      "Training Epoch: 9 [12350/13482]\tLoss: 1641.9021\n",
      "Training Epoch: 9 [12400/13482]\tLoss: 1662.2906\n",
      "Training Epoch: 9 [12450/13482]\tLoss: 1625.0337\n",
      "Training Epoch: 9 [12500/13482]\tLoss: 1684.7928\n",
      "Training Epoch: 9 [12550/13482]\tLoss: 1660.4341\n",
      "Training Epoch: 9 [12600/13482]\tLoss: 1663.8090\n",
      "Training Epoch: 9 [12650/13482]\tLoss: 1650.2240\n",
      "Training Epoch: 9 [12700/13482]\tLoss: 1615.8937\n",
      "Training Epoch: 9 [12750/13482]\tLoss: 1657.4669\n",
      "Training Epoch: 9 [12800/13482]\tLoss: 1647.6364\n",
      "Training Epoch: 9 [12850/13482]\tLoss: 1682.3645\n",
      "Training Epoch: 9 [12900/13482]\tLoss: 1691.2736\n",
      "Training Epoch: 9 [12950/13482]\tLoss: 1637.6411\n",
      "Training Epoch: 9 [13000/13482]\tLoss: 1672.5922\n",
      "Training Epoch: 9 [13050/13482]\tLoss: 1690.1743\n",
      "Training Epoch: 9 [13100/13482]\tLoss: 1626.9398\n",
      "Training Epoch: 9 [13150/13482]\tLoss: 1646.3032\n",
      "Training Epoch: 9 [13200/13482]\tLoss: 1663.7845\n",
      "Training Epoch: 9 [13250/13482]\tLoss: 1676.2842\n",
      "Training Epoch: 9 [13300/13482]\tLoss: 1648.0913\n",
      "Training Epoch: 9 [13350/13482]\tLoss: 1609.0283\n",
      "Training Epoch: 9 [13400/13482]\tLoss: 1678.0475\n",
      "Training Epoch: 9 [13450/13482]\tLoss: 1696.9883\n",
      "Training Epoch: 9 [13482/13482]\tLoss: 1675.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [1497/1497]\tLoss: 1642.3738\n",
      "Training Epoch: 10 [50/13482]\tLoss: 1662.9102\n",
      "Training Epoch: 10 [100/13482]\tLoss: 1666.3378\n",
      "Training Epoch: 10 [150/13482]\tLoss: 1664.0806\n",
      "Training Epoch: 10 [200/13482]\tLoss: 1669.6102\n",
      "Training Epoch: 10 [250/13482]\tLoss: 1658.7975\n",
      "Training Epoch: 10 [300/13482]\tLoss: 1643.5896\n",
      "Training Epoch: 10 [350/13482]\tLoss: 1633.6123\n",
      "Training Epoch: 10 [400/13482]\tLoss: 1639.5208\n",
      "Training Epoch: 10 [450/13482]\tLoss: 1645.4557\n",
      "Training Epoch: 10 [500/13482]\tLoss: 1653.8722\n",
      "Training Epoch: 10 [550/13482]\tLoss: 1665.6570\n",
      "Training Epoch: 10 [600/13482]\tLoss: 1671.1992\n",
      "Training Epoch: 10 [650/13482]\tLoss: 1672.7284\n",
      "Training Epoch: 10 [700/13482]\tLoss: 1697.2244\n",
      "Training Epoch: 10 [750/13482]\tLoss: 1651.6777\n",
      "Training Epoch: 10 [800/13482]\tLoss: 1673.8579\n",
      "Training Epoch: 10 [850/13482]\tLoss: 1642.2218\n",
      "Training Epoch: 10 [900/13482]\tLoss: 1631.1682\n",
      "Training Epoch: 10 [950/13482]\tLoss: 1679.6699\n",
      "Training Epoch: 10 [1000/13482]\tLoss: 1693.5829\n",
      "Training Epoch: 10 [1050/13482]\tLoss: 1631.4883\n",
      "Training Epoch: 10 [1100/13482]\tLoss: 1672.0305\n",
      "Training Epoch: 10 [1150/13482]\tLoss: 1657.0400\n",
      "Training Epoch: 10 [1200/13482]\tLoss: 1656.0039\n",
      "Training Epoch: 10 [1250/13482]\tLoss: 1638.1539\n",
      "Training Epoch: 10 [1300/13482]\tLoss: 1638.4427\n",
      "Training Epoch: 10 [1350/13482]\tLoss: 1619.0665\n",
      "Training Epoch: 10 [1400/13482]\tLoss: 1639.3627\n",
      "Training Epoch: 10 [1450/13482]\tLoss: 1615.4285\n",
      "Training Epoch: 10 [1500/13482]\tLoss: 1660.4958\n",
      "Training Epoch: 10 [1550/13482]\tLoss: 1617.9667\n",
      "Training Epoch: 10 [1600/13482]\tLoss: 1601.9270\n",
      "Training Epoch: 10 [1650/13482]\tLoss: 1658.0571\n",
      "Training Epoch: 10 [1700/13482]\tLoss: 1670.1898\n",
      "Training Epoch: 10 [1750/13482]\tLoss: 1601.5784\n",
      "Training Epoch: 10 [1800/13482]\tLoss: 1649.3049\n",
      "Training Epoch: 10 [1850/13482]\tLoss: 1662.9772\n",
      "Training Epoch: 10 [1900/13482]\tLoss: 1624.4198\n",
      "Training Epoch: 10 [1950/13482]\tLoss: 1648.2068\n",
      "Training Epoch: 10 [2000/13482]\tLoss: 1655.1338\n",
      "Training Epoch: 10 [2050/13482]\tLoss: 1623.7244\n",
      "Training Epoch: 10 [2100/13482]\tLoss: 1650.9567\n",
      "Training Epoch: 10 [2150/13482]\tLoss: 1673.0475\n",
      "Training Epoch: 10 [2200/13482]\tLoss: 1588.2531\n",
      "Training Epoch: 10 [2250/13482]\tLoss: 1670.4025\n",
      "Training Epoch: 10 [2300/13482]\tLoss: 1630.9209\n",
      "Training Epoch: 10 [2350/13482]\tLoss: 1601.3340\n",
      "Training Epoch: 10 [2400/13482]\tLoss: 1595.8588\n",
      "Training Epoch: 10 [2450/13482]\tLoss: 1615.2267\n",
      "Training Epoch: 10 [2500/13482]\tLoss: 1611.4344\n",
      "Training Epoch: 10 [2550/13482]\tLoss: 1611.7966\n",
      "Training Epoch: 10 [2600/13482]\tLoss: 1630.4760\n",
      "Training Epoch: 10 [2650/13482]\tLoss: 1645.9675\n",
      "Training Epoch: 10 [2700/13482]\tLoss: 1645.8043\n",
      "Training Epoch: 10 [2750/13482]\tLoss: 1638.5983\n",
      "Training Epoch: 10 [2800/13482]\tLoss: 1643.8291\n",
      "Training Epoch: 10 [2850/13482]\tLoss: 1678.8055\n",
      "Training Epoch: 10 [2900/13482]\tLoss: 1694.3572\n",
      "Training Epoch: 10 [2950/13482]\tLoss: 1630.4092\n",
      "Training Epoch: 10 [3000/13482]\tLoss: 1580.9847\n",
      "Training Epoch: 10 [3050/13482]\tLoss: 1653.4321\n",
      "Training Epoch: 10 [3100/13482]\tLoss: 1598.7981\n",
      "Training Epoch: 10 [3150/13482]\tLoss: 1599.6660\n",
      "Training Epoch: 10 [3200/13482]\tLoss: 1615.5109\n",
      "Training Epoch: 10 [3250/13482]\tLoss: 1661.5145\n",
      "Training Epoch: 10 [3300/13482]\tLoss: 1678.0232\n",
      "Training Epoch: 10 [3350/13482]\tLoss: 1673.8796\n",
      "Training Epoch: 10 [3400/13482]\tLoss: 1609.8467\n",
      "Training Epoch: 10 [3450/13482]\tLoss: 1679.9548\n",
      "Training Epoch: 10 [3500/13482]\tLoss: 1603.2859\n",
      "Training Epoch: 10 [3550/13482]\tLoss: 1652.8489\n",
      "Training Epoch: 10 [3600/13482]\tLoss: 1714.5521\n",
      "Training Epoch: 10 [3650/13482]\tLoss: 1627.7554\n",
      "Training Epoch: 10 [3700/13482]\tLoss: 1581.7142\n",
      "Training Epoch: 10 [3750/13482]\tLoss: 1680.0651\n",
      "Training Epoch: 10 [3800/13482]\tLoss: 1621.0096\n",
      "Training Epoch: 10 [3850/13482]\tLoss: 1583.1578\n",
      "Training Epoch: 10 [3900/13482]\tLoss: 1598.4587\n",
      "Training Epoch: 10 [3950/13482]\tLoss: 1602.5745\n",
      "Training Epoch: 10 [4000/13482]\tLoss: 1602.5217\n",
      "Training Epoch: 10 [4050/13482]\tLoss: 1634.9795\n",
      "Training Epoch: 10 [4100/13482]\tLoss: 1687.5627\n",
      "Training Epoch: 10 [4150/13482]\tLoss: 1628.6183\n",
      "Training Epoch: 10 [4200/13482]\tLoss: 1622.2020\n",
      "Training Epoch: 10 [4250/13482]\tLoss: 1651.1381\n",
      "Training Epoch: 10 [4300/13482]\tLoss: 1641.4202\n",
      "Training Epoch: 10 [4350/13482]\tLoss: 1608.0444\n",
      "Training Epoch: 10 [4400/13482]\tLoss: 1698.5953\n",
      "Training Epoch: 10 [4450/13482]\tLoss: 1632.7830\n",
      "Training Epoch: 10 [4500/13482]\tLoss: 1654.8480\n",
      "Training Epoch: 10 [4550/13482]\tLoss: 1586.9860\n",
      "Training Epoch: 10 [4600/13482]\tLoss: 1554.8289\n",
      "Training Epoch: 10 [4650/13482]\tLoss: 1657.7577\n",
      "Training Epoch: 10 [4700/13482]\tLoss: 1606.0889\n",
      "Training Epoch: 10 [4750/13482]\tLoss: 1578.2937\n",
      "Training Epoch: 10 [4800/13482]\tLoss: 1639.9835\n",
      "Training Epoch: 10 [4850/13482]\tLoss: 1640.8600\n",
      "Training Epoch: 10 [4900/13482]\tLoss: 1652.5909\n",
      "Training Epoch: 10 [4950/13482]\tLoss: 1638.3828\n",
      "Training Epoch: 10 [5000/13482]\tLoss: 1636.6891\n",
      "Training Epoch: 10 [5050/13482]\tLoss: 1626.1848\n",
      "Training Epoch: 10 [5100/13482]\tLoss: 1607.5996\n",
      "Training Epoch: 10 [5150/13482]\tLoss: 1634.0000\n",
      "Training Epoch: 10 [5200/13482]\tLoss: 1657.0983\n",
      "Training Epoch: 10 [5250/13482]\tLoss: 1630.8937\n",
      "Training Epoch: 10 [5300/13482]\tLoss: 1625.9762\n",
      "Training Epoch: 10 [5350/13482]\tLoss: 1573.6490\n",
      "Training Epoch: 10 [5400/13482]\tLoss: 1631.6705\n",
      "Training Epoch: 10 [5450/13482]\tLoss: 1621.6537\n",
      "Training Epoch: 10 [5500/13482]\tLoss: 1657.1566\n",
      "Training Epoch: 10 [5550/13482]\tLoss: 1641.7401\n",
      "Training Epoch: 10 [5600/13482]\tLoss: 1610.5240\n",
      "Training Epoch: 10 [5650/13482]\tLoss: 1609.4514\n",
      "Training Epoch: 10 [5700/13482]\tLoss: 1628.9044\n",
      "Training Epoch: 10 [5750/13482]\tLoss: 1666.2552\n",
      "Training Epoch: 10 [5800/13482]\tLoss: 1670.0405\n",
      "Training Epoch: 10 [5850/13482]\tLoss: 1726.4663\n",
      "Training Epoch: 10 [5900/13482]\tLoss: 1627.5240\n",
      "Training Epoch: 10 [5950/13482]\tLoss: 1593.4298\n",
      "Training Epoch: 10 [6000/13482]\tLoss: 1620.3118\n",
      "Training Epoch: 10 [6050/13482]\tLoss: 1634.3822\n",
      "Training Epoch: 10 [6100/13482]\tLoss: 1665.4447\n",
      "Training Epoch: 10 [6150/13482]\tLoss: 1674.4611\n",
      "Training Epoch: 10 [6200/13482]\tLoss: 1700.7927\n",
      "Training Epoch: 10 [6250/13482]\tLoss: 1596.1954\n",
      "Training Epoch: 10 [6300/13482]\tLoss: 1589.4309\n",
      "Training Epoch: 10 [6350/13482]\tLoss: 1613.8866\n",
      "Training Epoch: 10 [6400/13482]\tLoss: 1667.3936\n",
      "Training Epoch: 10 [6450/13482]\tLoss: 1633.9272\n",
      "Training Epoch: 10 [6500/13482]\tLoss: 1605.3877\n",
      "Training Epoch: 10 [6550/13482]\tLoss: 1694.2727\n",
      "Training Epoch: 10 [6600/13482]\tLoss: 1615.6333\n",
      "Training Epoch: 10 [6650/13482]\tLoss: 1626.7975\n",
      "Training Epoch: 10 [6700/13482]\tLoss: 1601.2996\n",
      "Training Epoch: 10 [6750/13482]\tLoss: 1621.4608\n",
      "Training Epoch: 10 [6800/13482]\tLoss: 1610.9510\n",
      "Training Epoch: 10 [6850/13482]\tLoss: 1607.2297\n",
      "Training Epoch: 10 [6900/13482]\tLoss: 1630.8152\n",
      "Training Epoch: 10 [6950/13482]\tLoss: 1588.9680\n",
      "Training Epoch: 10 [7000/13482]\tLoss: 1629.8695\n",
      "Training Epoch: 10 [7050/13482]\tLoss: 1638.6056\n",
      "Training Epoch: 10 [7100/13482]\tLoss: 1644.4567\n",
      "Training Epoch: 10 [7150/13482]\tLoss: 1656.6332\n",
      "Training Epoch: 10 [7200/13482]\tLoss: 1627.1842\n",
      "Training Epoch: 10 [7250/13482]\tLoss: 1606.2179\n",
      "Training Epoch: 10 [7300/13482]\tLoss: 1643.8466\n",
      "Training Epoch: 10 [7350/13482]\tLoss: 1602.1022\n",
      "Training Epoch: 10 [7400/13482]\tLoss: 1664.5406\n",
      "Training Epoch: 10 [7450/13482]\tLoss: 1619.9758\n",
      "Training Epoch: 10 [7500/13482]\tLoss: 1569.0547\n",
      "Training Epoch: 10 [7550/13482]\tLoss: 1671.9403\n",
      "Training Epoch: 10 [7600/13482]\tLoss: 1644.9863\n",
      "Training Epoch: 10 [7650/13482]\tLoss: 1671.0326\n",
      "Training Epoch: 10 [7700/13482]\tLoss: 1598.3961\n",
      "Training Epoch: 10 [7750/13482]\tLoss: 1578.3297\n",
      "Training Epoch: 10 [7800/13482]\tLoss: 1674.5443\n",
      "Training Epoch: 10 [7850/13482]\tLoss: 1673.0232\n",
      "Training Epoch: 10 [7900/13482]\tLoss: 1617.8427\n",
      "Training Epoch: 10 [7950/13482]\tLoss: 1662.0216\n",
      "Training Epoch: 10 [8000/13482]\tLoss: 1595.5120\n",
      "Training Epoch: 10 [8050/13482]\tLoss: 1668.9839\n",
      "Training Epoch: 10 [8100/13482]\tLoss: 1634.6915\n",
      "Training Epoch: 10 [8150/13482]\tLoss: 1648.0490\n",
      "Training Epoch: 10 [8200/13482]\tLoss: 1651.6188\n",
      "Training Epoch: 10 [8250/13482]\tLoss: 1563.9001\n",
      "Training Epoch: 10 [8300/13482]\tLoss: 1605.3219\n",
      "Training Epoch: 10 [8350/13482]\tLoss: 1645.9795\n",
      "Training Epoch: 10 [8400/13482]\tLoss: 1660.9784\n",
      "Training Epoch: 10 [8450/13482]\tLoss: 1594.4585\n",
      "Training Epoch: 10 [8500/13482]\tLoss: 1624.3541\n",
      "Training Epoch: 10 [8550/13482]\tLoss: 1609.6208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [8600/13482]\tLoss: 1661.8387\n",
      "Training Epoch: 10 [8650/13482]\tLoss: 1654.8163\n",
      "Training Epoch: 10 [8700/13482]\tLoss: 1630.1263\n",
      "Training Epoch: 10 [8750/13482]\tLoss: 1617.1995\n",
      "Training Epoch: 10 [8800/13482]\tLoss: 1594.7198\n",
      "Training Epoch: 10 [8850/13482]\tLoss: 1627.3236\n",
      "Training Epoch: 10 [8900/13482]\tLoss: 1632.5905\n",
      "Training Epoch: 10 [8950/13482]\tLoss: 1594.3083\n",
      "Training Epoch: 10 [9000/13482]\tLoss: 1638.3713\n",
      "Training Epoch: 10 [9050/13482]\tLoss: 1634.6487\n",
      "Training Epoch: 10 [9100/13482]\tLoss: 1596.7311\n",
      "Training Epoch: 10 [9150/13482]\tLoss: 1649.8961\n",
      "Training Epoch: 10 [9200/13482]\tLoss: 1698.3151\n",
      "Training Epoch: 10 [9250/13482]\tLoss: 1575.9255\n",
      "Training Epoch: 10 [9300/13482]\tLoss: 1654.2557\n",
      "Training Epoch: 10 [9350/13482]\tLoss: 1618.7057\n",
      "Training Epoch: 10 [9400/13482]\tLoss: 1564.9293\n",
      "Training Epoch: 10 [9450/13482]\tLoss: 1652.7073\n",
      "Training Epoch: 10 [9500/13482]\tLoss: 1610.0400\n",
      "Training Epoch: 10 [9550/13482]\tLoss: 1619.1729\n",
      "Training Epoch: 10 [9600/13482]\tLoss: 1634.3627\n",
      "Training Epoch: 10 [9650/13482]\tLoss: 1609.7356\n",
      "Training Epoch: 10 [9700/13482]\tLoss: 1617.5350\n",
      "Training Epoch: 10 [9750/13482]\tLoss: 1605.3809\n",
      "Training Epoch: 10 [9800/13482]\tLoss: 1580.5317\n",
      "Training Epoch: 10 [9850/13482]\tLoss: 1580.0907\n",
      "Training Epoch: 10 [9900/13482]\tLoss: 1639.6281\n",
      "Training Epoch: 10 [9950/13482]\tLoss: 1626.7567\n",
      "Training Epoch: 10 [10000/13482]\tLoss: 1605.2159\n",
      "Training Epoch: 10 [10050/13482]\tLoss: 1663.4569\n",
      "Training Epoch: 10 [10100/13482]\tLoss: 1618.9171\n",
      "Training Epoch: 10 [10150/13482]\tLoss: 1622.0389\n",
      "Training Epoch: 10 [10200/13482]\tLoss: 1590.3960\n",
      "Training Epoch: 10 [10250/13482]\tLoss: 1671.1896\n",
      "Training Epoch: 10 [10300/13482]\tLoss: 1602.3903\n",
      "Training Epoch: 10 [10350/13482]\tLoss: 1618.7344\n",
      "Training Epoch: 10 [10400/13482]\tLoss: 1584.3882\n",
      "Training Epoch: 10 [10450/13482]\tLoss: 1622.6725\n",
      "Training Epoch: 10 [10500/13482]\tLoss: 1684.3242\n",
      "Training Epoch: 10 [10550/13482]\tLoss: 1644.6561\n",
      "Training Epoch: 10 [10600/13482]\tLoss: 1624.2706\n",
      "Training Epoch: 10 [10650/13482]\tLoss: 1590.3524\n",
      "Training Epoch: 10 [10700/13482]\tLoss: 1605.6576\n",
      "Training Epoch: 10 [10750/13482]\tLoss: 1599.4983\n",
      "Training Epoch: 10 [10800/13482]\tLoss: 1595.7314\n",
      "Training Epoch: 10 [10850/13482]\tLoss: 1613.0476\n",
      "Training Epoch: 10 [10900/13482]\tLoss: 1588.2411\n",
      "Training Epoch: 10 [10950/13482]\tLoss: 1592.1417\n",
      "Training Epoch: 10 [11000/13482]\tLoss: 1609.7615\n",
      "Training Epoch: 10 [11050/13482]\tLoss: 1622.7083\n",
      "Training Epoch: 10 [11100/13482]\tLoss: 1618.8285\n",
      "Training Epoch: 10 [11150/13482]\tLoss: 1616.0941\n",
      "Training Epoch: 10 [11200/13482]\tLoss: 1582.2997\n",
      "Training Epoch: 10 [11250/13482]\tLoss: 1648.5024\n",
      "Training Epoch: 10 [11300/13482]\tLoss: 1618.0956\n",
      "Training Epoch: 10 [11350/13482]\tLoss: 1668.5470\n",
      "Training Epoch: 10 [11400/13482]\tLoss: 1623.5891\n",
      "Training Epoch: 10 [11450/13482]\tLoss: 1587.5806\n",
      "Training Epoch: 10 [11500/13482]\tLoss: 1612.8735\n",
      "Training Epoch: 10 [11550/13482]\tLoss: 1655.3414\n",
      "Training Epoch: 10 [11600/13482]\tLoss: 1611.6558\n",
      "Training Epoch: 10 [11650/13482]\tLoss: 1632.9873\n",
      "Training Epoch: 10 [11700/13482]\tLoss: 1586.0452\n",
      "Training Epoch: 10 [11750/13482]\tLoss: 1578.3313\n",
      "Training Epoch: 10 [11800/13482]\tLoss: 1639.5367\n",
      "Training Epoch: 10 [11850/13482]\tLoss: 1567.8773\n",
      "Training Epoch: 10 [11900/13482]\tLoss: 1587.7627\n",
      "Training Epoch: 10 [11950/13482]\tLoss: 1673.4823\n",
      "Training Epoch: 10 [12000/13482]\tLoss: 1563.3448\n",
      "Training Epoch: 10 [12050/13482]\tLoss: 1561.5151\n",
      "Training Epoch: 10 [12100/13482]\tLoss: 1641.4216\n",
      "Training Epoch: 10 [12150/13482]\tLoss: 1601.5179\n",
      "Training Epoch: 10 [12200/13482]\tLoss: 1605.3561\n",
      "Training Epoch: 10 [12250/13482]\tLoss: 1584.7028\n",
      "Training Epoch: 10 [12300/13482]\tLoss: 1639.5195\n",
      "Training Epoch: 10 [12350/13482]\tLoss: 1594.7529\n",
      "Training Epoch: 10 [12400/13482]\tLoss: 1615.7123\n",
      "Training Epoch: 10 [12450/13482]\tLoss: 1579.2795\n",
      "Training Epoch: 10 [12500/13482]\tLoss: 1637.1642\n",
      "Training Epoch: 10 [12550/13482]\tLoss: 1615.9768\n",
      "Training Epoch: 10 [12600/13482]\tLoss: 1619.0034\n",
      "Training Epoch: 10 [12650/13482]\tLoss: 1605.3638\n",
      "Training Epoch: 10 [12700/13482]\tLoss: 1569.6288\n",
      "Training Epoch: 10 [12750/13482]\tLoss: 1611.9429\n",
      "Training Epoch: 10 [12800/13482]\tLoss: 1599.9768\n",
      "Training Epoch: 10 [12850/13482]\tLoss: 1639.1012\n",
      "Training Epoch: 10 [12900/13482]\tLoss: 1644.8171\n",
      "Training Epoch: 10 [12950/13482]\tLoss: 1589.4259\n",
      "Training Epoch: 10 [13000/13482]\tLoss: 1630.1196\n",
      "Training Epoch: 10 [13050/13482]\tLoss: 1642.6738\n",
      "Training Epoch: 10 [13100/13482]\tLoss: 1581.6758\n",
      "Training Epoch: 10 [13150/13482]\tLoss: 1602.4637\n",
      "Training Epoch: 10 [13200/13482]\tLoss: 1617.0417\n",
      "Training Epoch: 10 [13250/13482]\tLoss: 1629.1172\n",
      "Training Epoch: 10 [13300/13482]\tLoss: 1601.8662\n",
      "Training Epoch: 10 [13350/13482]\tLoss: 1562.1145\n",
      "Training Epoch: 10 [13400/13482]\tLoss: 1627.6011\n",
      "Training Epoch: 10 [13450/13482]\tLoss: 1651.5442\n",
      "Training Epoch: 10 [13482/13482]\tLoss: 1630.1068\n",
      "Training Epoch: 10 [1497/1497]\tLoss: 1597.3928\n",
      "Training Epoch: 11 [50/13482]\tLoss: 1621.1006\n",
      "Training Epoch: 11 [100/13482]\tLoss: 1620.6025\n",
      "Training Epoch: 11 [150/13482]\tLoss: 1618.7782\n",
      "Training Epoch: 11 [200/13482]\tLoss: 1621.2930\n",
      "Training Epoch: 11 [250/13482]\tLoss: 1614.9631\n",
      "Training Epoch: 11 [300/13482]\tLoss: 1597.5598\n",
      "Training Epoch: 11 [350/13482]\tLoss: 1586.3503\n",
      "Training Epoch: 11 [400/13482]\tLoss: 1597.7740\n",
      "Training Epoch: 11 [450/13482]\tLoss: 1601.3695\n",
      "Training Epoch: 11 [500/13482]\tLoss: 1611.4226\n",
      "Training Epoch: 11 [550/13482]\tLoss: 1621.8788\n",
      "Training Epoch: 11 [600/13482]\tLoss: 1625.3618\n",
      "Training Epoch: 11 [650/13482]\tLoss: 1629.3754\n",
      "Training Epoch: 11 [700/13482]\tLoss: 1654.7212\n",
      "Training Epoch: 11 [750/13482]\tLoss: 1608.1666\n",
      "Training Epoch: 11 [800/13482]\tLoss: 1628.1681\n",
      "Training Epoch: 11 [850/13482]\tLoss: 1596.1887\n",
      "Training Epoch: 11 [900/13482]\tLoss: 1586.0787\n",
      "Training Epoch: 11 [950/13482]\tLoss: 1634.3025\n",
      "Training Epoch: 11 [1000/13482]\tLoss: 1647.1991\n",
      "Training Epoch: 11 [1050/13482]\tLoss: 1584.8340\n",
      "Training Epoch: 11 [1100/13482]\tLoss: 1626.1127\n",
      "Training Epoch: 11 [1150/13482]\tLoss: 1613.2646\n",
      "Training Epoch: 11 [1200/13482]\tLoss: 1613.7052\n",
      "Training Epoch: 11 [1250/13482]\tLoss: 1593.8699\n",
      "Training Epoch: 11 [1300/13482]\tLoss: 1596.3555\n",
      "Training Epoch: 11 [1350/13482]\tLoss: 1575.8346\n",
      "Training Epoch: 11 [1400/13482]\tLoss: 1593.6202\n",
      "Training Epoch: 11 [1450/13482]\tLoss: 1572.1727\n",
      "Training Epoch: 11 [1500/13482]\tLoss: 1616.2941\n",
      "Training Epoch: 11 [1550/13482]\tLoss: 1572.0431\n",
      "Training Epoch: 11 [1600/13482]\tLoss: 1557.6235\n",
      "Training Epoch: 11 [1650/13482]\tLoss: 1617.1974\n",
      "Training Epoch: 11 [1700/13482]\tLoss: 1626.8744\n",
      "Training Epoch: 11 [1750/13482]\tLoss: 1555.7112\n",
      "Training Epoch: 11 [1800/13482]\tLoss: 1600.6682\n",
      "Training Epoch: 11 [1850/13482]\tLoss: 1619.5098\n",
      "Training Epoch: 11 [1900/13482]\tLoss: 1581.0317\n",
      "Training Epoch: 11 [1950/13482]\tLoss: 1601.7163\n",
      "Training Epoch: 11 [2000/13482]\tLoss: 1610.6726\n",
      "Training Epoch: 11 [2050/13482]\tLoss: 1578.2355\n",
      "Training Epoch: 11 [2100/13482]\tLoss: 1607.9884\n",
      "Training Epoch: 11 [2150/13482]\tLoss: 1629.2418\n",
      "Training Epoch: 11 [2200/13482]\tLoss: 1546.1399\n",
      "Training Epoch: 11 [2250/13482]\tLoss: 1628.7490\n",
      "Training Epoch: 11 [2300/13482]\tLoss: 1588.6833\n",
      "Training Epoch: 11 [2350/13482]\tLoss: 1555.7080\n",
      "Training Epoch: 11 [2400/13482]\tLoss: 1552.6125\n",
      "Training Epoch: 11 [2450/13482]\tLoss: 1568.9812\n",
      "Training Epoch: 11 [2500/13482]\tLoss: 1569.5592\n",
      "Training Epoch: 11 [2550/13482]\tLoss: 1569.0099\n",
      "Training Epoch: 11 [2600/13482]\tLoss: 1586.3577\n",
      "Training Epoch: 11 [2650/13482]\tLoss: 1604.3120\n",
      "Training Epoch: 11 [2700/13482]\tLoss: 1600.2354\n",
      "Training Epoch: 11 [2750/13482]\tLoss: 1595.7317\n",
      "Training Epoch: 11 [2800/13482]\tLoss: 1602.7881\n",
      "Training Epoch: 11 [2850/13482]\tLoss: 1630.4941\n",
      "Training Epoch: 11 [2900/13482]\tLoss: 1650.1260\n",
      "Training Epoch: 11 [2950/13482]\tLoss: 1586.5204\n",
      "Training Epoch: 11 [3000/13482]\tLoss: 1535.7048\n",
      "Training Epoch: 11 [3050/13482]\tLoss: 1608.8387\n",
      "Training Epoch: 11 [3100/13482]\tLoss: 1557.2816\n",
      "Training Epoch: 11 [3150/13482]\tLoss: 1558.1101\n",
      "Training Epoch: 11 [3200/13482]\tLoss: 1570.9384\n",
      "Training Epoch: 11 [3250/13482]\tLoss: 1616.7019\n",
      "Training Epoch: 11 [3300/13482]\tLoss: 1632.8757\n",
      "Training Epoch: 11 [3350/13482]\tLoss: 1629.8406\n",
      "Training Epoch: 11 [3400/13482]\tLoss: 1567.1085\n",
      "Training Epoch: 11 [3450/13482]\tLoss: 1638.0320\n",
      "Training Epoch: 11 [3500/13482]\tLoss: 1558.2375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [3550/13482]\tLoss: 1609.4899\n",
      "Training Epoch: 11 [3600/13482]\tLoss: 1669.7120\n",
      "Training Epoch: 11 [3650/13482]\tLoss: 1586.3840\n",
      "Training Epoch: 11 [3700/13482]\tLoss: 1540.5010\n",
      "Training Epoch: 11 [3750/13482]\tLoss: 1639.3215\n",
      "Training Epoch: 11 [3800/13482]\tLoss: 1575.8428\n",
      "Training Epoch: 11 [3850/13482]\tLoss: 1536.2120\n",
      "Training Epoch: 11 [3900/13482]\tLoss: 1551.3182\n",
      "Training Epoch: 11 [3950/13482]\tLoss: 1559.4385\n",
      "Training Epoch: 11 [4000/13482]\tLoss: 1557.8295\n",
      "Training Epoch: 11 [4050/13482]\tLoss: 1594.0082\n",
      "Training Epoch: 11 [4100/13482]\tLoss: 1645.9125\n",
      "Training Epoch: 11 [4150/13482]\tLoss: 1584.0449\n",
      "Training Epoch: 11 [4200/13482]\tLoss: 1579.6005\n",
      "Training Epoch: 11 [4250/13482]\tLoss: 1607.1642\n",
      "Training Epoch: 11 [4300/13482]\tLoss: 1597.9463\n",
      "Training Epoch: 11 [4350/13482]\tLoss: 1566.1906\n",
      "Training Epoch: 11 [4400/13482]\tLoss: 1657.8622\n",
      "Training Epoch: 11 [4450/13482]\tLoss: 1591.3767\n",
      "Training Epoch: 11 [4500/13482]\tLoss: 1613.9297\n",
      "Training Epoch: 11 [4550/13482]\tLoss: 1545.8641\n",
      "Training Epoch: 11 [4600/13482]\tLoss: 1511.1565\n",
      "Training Epoch: 11 [4650/13482]\tLoss: 1614.5262\n",
      "Training Epoch: 11 [4700/13482]\tLoss: 1563.0740\n",
      "Training Epoch: 11 [4750/13482]\tLoss: 1532.9064\n",
      "Training Epoch: 11 [4800/13482]\tLoss: 1598.0474\n",
      "Training Epoch: 11 [4850/13482]\tLoss: 1601.6503\n",
      "Training Epoch: 11 [4900/13482]\tLoss: 1608.7126\n",
      "Training Epoch: 11 [4950/13482]\tLoss: 1594.5857\n",
      "Training Epoch: 11 [5000/13482]\tLoss: 1591.8142\n",
      "Training Epoch: 11 [5050/13482]\tLoss: 1582.4751\n",
      "Training Epoch: 11 [5100/13482]\tLoss: 1565.8021\n",
      "Training Epoch: 11 [5150/13482]\tLoss: 1594.1010\n",
      "Training Epoch: 11 [5200/13482]\tLoss: 1614.1956\n",
      "Training Epoch: 11 [5250/13482]\tLoss: 1588.1709\n",
      "Training Epoch: 11 [5300/13482]\tLoss: 1583.9539\n",
      "Training Epoch: 11 [5350/13482]\tLoss: 1531.2854\n",
      "Training Epoch: 11 [5400/13482]\tLoss: 1586.5087\n",
      "Training Epoch: 11 [5450/13482]\tLoss: 1579.1049\n",
      "Training Epoch: 11 [5500/13482]\tLoss: 1614.6805\n",
      "Training Epoch: 11 [5550/13482]\tLoss: 1599.7490\n",
      "Training Epoch: 11 [5600/13482]\tLoss: 1570.0187\n",
      "Training Epoch: 11 [5650/13482]\tLoss: 1566.7439\n",
      "Training Epoch: 11 [5700/13482]\tLoss: 1587.3196\n",
      "Training Epoch: 11 [5750/13482]\tLoss: 1622.9099\n",
      "Training Epoch: 11 [5800/13482]\tLoss: 1628.0333\n",
      "Training Epoch: 11 [5850/13482]\tLoss: 1681.0743\n",
      "Training Epoch: 11 [5900/13482]\tLoss: 1584.7333\n",
      "Training Epoch: 11 [5950/13482]\tLoss: 1549.2378\n",
      "Training Epoch: 11 [6000/13482]\tLoss: 1578.7324\n",
      "Training Epoch: 11 [6050/13482]\tLoss: 1592.6569\n",
      "Training Epoch: 11 [6100/13482]\tLoss: 1624.7277\n",
      "Training Epoch: 11 [6150/13482]\tLoss: 1631.2870\n",
      "Training Epoch: 11 [6200/13482]\tLoss: 1656.3533\n",
      "Training Epoch: 11 [6250/13482]\tLoss: 1552.4281\n",
      "Training Epoch: 11 [6300/13482]\tLoss: 1550.8019\n",
      "Training Epoch: 11 [6350/13482]\tLoss: 1567.4541\n",
      "Training Epoch: 11 [6400/13482]\tLoss: 1626.8335\n",
      "Training Epoch: 11 [6450/13482]\tLoss: 1593.8954\n",
      "Training Epoch: 11 [6500/13482]\tLoss: 1564.5756\n",
      "Training Epoch: 11 [6550/13482]\tLoss: 1653.0311\n",
      "Training Epoch: 11 [6600/13482]\tLoss: 1577.0990\n",
      "Training Epoch: 11 [6650/13482]\tLoss: 1582.1622\n",
      "Training Epoch: 11 [6700/13482]\tLoss: 1559.0531\n",
      "Training Epoch: 11 [6750/13482]\tLoss: 1579.8488\n",
      "Training Epoch: 11 [6800/13482]\tLoss: 1571.2299\n",
      "Training Epoch: 11 [6850/13482]\tLoss: 1566.6387\n",
      "Training Epoch: 11 [6900/13482]\tLoss: 1589.6837\n",
      "Training Epoch: 11 [6950/13482]\tLoss: 1545.7767\n",
      "Training Epoch: 11 [7000/13482]\tLoss: 1588.6665\n",
      "Training Epoch: 11 [7050/13482]\tLoss: 1594.2468\n",
      "Training Epoch: 11 [7100/13482]\tLoss: 1602.5532\n",
      "Training Epoch: 11 [7150/13482]\tLoss: 1615.4913\n",
      "Training Epoch: 11 [7200/13482]\tLoss: 1583.0183\n",
      "Training Epoch: 11 [7250/13482]\tLoss: 1563.5358\n",
      "Training Epoch: 11 [7300/13482]\tLoss: 1603.7079\n",
      "Training Epoch: 11 [7350/13482]\tLoss: 1562.4095\n",
      "Training Epoch: 11 [7400/13482]\tLoss: 1623.0865\n",
      "Training Epoch: 11 [7450/13482]\tLoss: 1578.5354\n",
      "Training Epoch: 11 [7500/13482]\tLoss: 1528.0641\n",
      "Training Epoch: 11 [7550/13482]\tLoss: 1631.8849\n",
      "Training Epoch: 11 [7600/13482]\tLoss: 1601.3345\n",
      "Training Epoch: 11 [7650/13482]\tLoss: 1629.7642\n",
      "Training Epoch: 11 [7700/13482]\tLoss: 1556.7883\n",
      "Training Epoch: 11 [7750/13482]\tLoss: 1535.6422\n",
      "Training Epoch: 11 [7800/13482]\tLoss: 1633.0419\n",
      "Training Epoch: 11 [7850/13482]\tLoss: 1631.1924\n",
      "Training Epoch: 11 [7900/13482]\tLoss: 1575.7440\n",
      "Training Epoch: 11 [7950/13482]\tLoss: 1621.0176\n",
      "Training Epoch: 11 [8000/13482]\tLoss: 1556.5508\n",
      "Training Epoch: 11 [8050/13482]\tLoss: 1626.7750\n",
      "Training Epoch: 11 [8100/13482]\tLoss: 1592.7649\n",
      "Training Epoch: 11 [8150/13482]\tLoss: 1605.4572\n",
      "Training Epoch: 11 [8200/13482]\tLoss: 1612.8713\n",
      "Training Epoch: 11 [8250/13482]\tLoss: 1523.3093\n",
      "Training Epoch: 11 [8300/13482]\tLoss: 1562.9572\n",
      "Training Epoch: 11 [8350/13482]\tLoss: 1605.5831\n",
      "Training Epoch: 11 [8400/13482]\tLoss: 1618.9910\n",
      "Training Epoch: 11 [8450/13482]\tLoss: 1553.3807\n",
      "Training Epoch: 11 [8500/13482]\tLoss: 1584.0582\n",
      "Training Epoch: 11 [8550/13482]\tLoss: 1568.9781\n",
      "Training Epoch: 11 [8600/13482]\tLoss: 1623.8463\n",
      "Training Epoch: 11 [8650/13482]\tLoss: 1613.3895\n",
      "Training Epoch: 11 [8700/13482]\tLoss: 1591.1267\n",
      "Training Epoch: 11 [8750/13482]\tLoss: 1575.4774\n",
      "Training Epoch: 11 [8800/13482]\tLoss: 1549.6659\n",
      "Training Epoch: 11 [8850/13482]\tLoss: 1588.9646\n",
      "Training Epoch: 11 [8900/13482]\tLoss: 1593.0216\n",
      "Training Epoch: 11 [8950/13482]\tLoss: 1553.0757\n",
      "Training Epoch: 11 [9000/13482]\tLoss: 1600.0320\n",
      "Training Epoch: 11 [9050/13482]\tLoss: 1594.8035\n",
      "Training Epoch: 11 [9100/13482]\tLoss: 1558.1299\n",
      "Training Epoch: 11 [9150/13482]\tLoss: 1608.4985\n",
      "Training Epoch: 11 [9200/13482]\tLoss: 1657.9722\n",
      "Training Epoch: 11 [9250/13482]\tLoss: 1535.5428\n",
      "Training Epoch: 11 [9300/13482]\tLoss: 1613.3170\n",
      "Training Epoch: 11 [9350/13482]\tLoss: 1576.3265\n",
      "Training Epoch: 11 [9400/13482]\tLoss: 1524.5095\n",
      "Training Epoch: 11 [9450/13482]\tLoss: 1610.2023\n",
      "Training Epoch: 11 [9500/13482]\tLoss: 1570.2566\n",
      "Training Epoch: 11 [9550/13482]\tLoss: 1577.3986\n",
      "Training Epoch: 11 [9600/13482]\tLoss: 1591.2891\n",
      "Training Epoch: 11 [9650/13482]\tLoss: 1567.1604\n",
      "Training Epoch: 11 [9700/13482]\tLoss: 1574.8989\n",
      "Training Epoch: 11 [9750/13482]\tLoss: 1564.4579\n",
      "Training Epoch: 11 [9800/13482]\tLoss: 1539.1943\n",
      "Training Epoch: 11 [9850/13482]\tLoss: 1538.3451\n",
      "Training Epoch: 11 [9900/13482]\tLoss: 1601.8478\n",
      "Training Epoch: 11 [9950/13482]\tLoss: 1583.4072\n",
      "Training Epoch: 11 [10000/13482]\tLoss: 1563.6881\n",
      "Training Epoch: 11 [10050/13482]\tLoss: 1623.8693\n",
      "Training Epoch: 11 [10100/13482]\tLoss: 1581.5903\n",
      "Training Epoch: 11 [10150/13482]\tLoss: 1581.3506\n",
      "Training Epoch: 11 [10200/13482]\tLoss: 1549.2710\n",
      "Training Epoch: 11 [10250/13482]\tLoss: 1629.2833\n",
      "Training Epoch: 11 [10300/13482]\tLoss: 1559.2756\n",
      "Training Epoch: 11 [10350/13482]\tLoss: 1578.8174\n",
      "Training Epoch: 11 [10400/13482]\tLoss: 1545.1190\n",
      "Training Epoch: 11 [10450/13482]\tLoss: 1582.4897\n",
      "Training Epoch: 11 [10500/13482]\tLoss: 1642.3182\n",
      "Training Epoch: 11 [10550/13482]\tLoss: 1603.5105\n",
      "Training Epoch: 11 [10600/13482]\tLoss: 1584.1272\n",
      "Training Epoch: 11 [10650/13482]\tLoss: 1551.7804\n",
      "Training Epoch: 11 [10700/13482]\tLoss: 1566.7861\n",
      "Training Epoch: 11 [10750/13482]\tLoss: 1556.5203\n",
      "Training Epoch: 11 [10800/13482]\tLoss: 1555.2743\n",
      "Training Epoch: 11 [10850/13482]\tLoss: 1574.6738\n",
      "Training Epoch: 11 [10900/13482]\tLoss: 1547.8541\n",
      "Training Epoch: 11 [10950/13482]\tLoss: 1552.6162\n",
      "Training Epoch: 11 [11000/13482]\tLoss: 1571.1570\n",
      "Training Epoch: 11 [11050/13482]\tLoss: 1582.8713\n",
      "Training Epoch: 11 [11100/13482]\tLoss: 1581.3694\n",
      "Training Epoch: 11 [11150/13482]\tLoss: 1574.9043\n",
      "Training Epoch: 11 [11200/13482]\tLoss: 1542.6742\n",
      "Training Epoch: 11 [11250/13482]\tLoss: 1607.7885\n",
      "Training Epoch: 11 [11300/13482]\tLoss: 1577.7573\n",
      "Training Epoch: 11 [11350/13482]\tLoss: 1630.1219\n",
      "Training Epoch: 11 [11400/13482]\tLoss: 1583.6615\n",
      "Training Epoch: 11 [11450/13482]\tLoss: 1549.2052\n",
      "Training Epoch: 11 [11500/13482]\tLoss: 1573.2997\n",
      "Training Epoch: 11 [11550/13482]\tLoss: 1615.4795\n",
      "Training Epoch: 11 [11600/13482]\tLoss: 1569.5181\n",
      "Training Epoch: 11 [11650/13482]\tLoss: 1595.2925\n",
      "Training Epoch: 11 [11700/13482]\tLoss: 1547.3369\n",
      "Training Epoch: 11 [11750/13482]\tLoss: 1540.8186\n",
      "Training Epoch: 11 [11800/13482]\tLoss: 1600.1741\n",
      "Training Epoch: 11 [11850/13482]\tLoss: 1529.7206\n",
      "Training Epoch: 11 [11900/13482]\tLoss: 1548.6309\n",
      "Training Epoch: 11 [11950/13482]\tLoss: 1636.9314\n",
      "Training Epoch: 11 [12000/13482]\tLoss: 1523.6990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [12050/13482]\tLoss: 1519.9669\n",
      "Training Epoch: 11 [12100/13482]\tLoss: 1599.9144\n",
      "Training Epoch: 11 [12150/13482]\tLoss: 1559.2706\n",
      "Training Epoch: 11 [12200/13482]\tLoss: 1566.5439\n",
      "Training Epoch: 11 [12250/13482]\tLoss: 1545.0242\n",
      "Training Epoch: 11 [12300/13482]\tLoss: 1600.4963\n",
      "Training Epoch: 11 [12350/13482]\tLoss: 1554.7013\n",
      "Training Epoch: 11 [12400/13482]\tLoss: 1575.6732\n",
      "Training Epoch: 11 [12450/13482]\tLoss: 1540.1234\n",
      "Training Epoch: 11 [12500/13482]\tLoss: 1596.5162\n",
      "Training Epoch: 11 [12550/13482]\tLoss: 1577.7640\n",
      "Training Epoch: 11 [12600/13482]\tLoss: 1580.0846\n",
      "Training Epoch: 11 [12650/13482]\tLoss: 1566.4845\n",
      "Training Epoch: 11 [12700/13482]\tLoss: 1529.9871\n",
      "Training Epoch: 11 [12750/13482]\tLoss: 1572.6018\n",
      "Training Epoch: 11 [12800/13482]\tLoss: 1559.0576\n",
      "Training Epoch: 11 [12850/13482]\tLoss: 1601.5394\n",
      "Training Epoch: 11 [12900/13482]\tLoss: 1604.8267\n",
      "Training Epoch: 11 [12950/13482]\tLoss: 1548.2803\n",
      "Training Epoch: 11 [13000/13482]\tLoss: 1593.3987\n",
      "Training Epoch: 11 [13050/13482]\tLoss: 1601.1958\n",
      "Training Epoch: 11 [13100/13482]\tLoss: 1542.3353\n",
      "Training Epoch: 11 [13150/13482]\tLoss: 1564.5955\n",
      "Training Epoch: 11 [13200/13482]\tLoss: 1576.4321\n",
      "Training Epoch: 11 [13250/13482]\tLoss: 1588.3596\n",
      "Training Epoch: 11 [13300/13482]\tLoss: 1562.1577\n",
      "Training Epoch: 11 [13350/13482]\tLoss: 1521.8121\n",
      "Training Epoch: 11 [13400/13482]\tLoss: 1583.6860\n",
      "Training Epoch: 11 [13450/13482]\tLoss: 1612.1967\n",
      "Training Epoch: 11 [13482/13482]\tLoss: 1590.5377\n",
      "Training Epoch: 11 [1497/1497]\tLoss: 1558.4585\n",
      "Training Epoch: 12 [50/13482]\tLoss: 1584.8265\n",
      "Training Epoch: 12 [100/13482]\tLoss: 1580.5370\n",
      "Training Epoch: 12 [150/13482]\tLoss: 1579.6508\n",
      "Training Epoch: 12 [200/13482]\tLoss: 1579.5084\n",
      "Training Epoch: 12 [250/13482]\tLoss: 1576.8315\n",
      "Training Epoch: 12 [300/13482]\tLoss: 1557.8525\n",
      "Training Epoch: 12 [350/13482]\tLoss: 1545.6927\n",
      "Training Epoch: 12 [400/13482]\tLoss: 1561.3809\n",
      "Training Epoch: 12 [450/13482]\tLoss: 1562.9728\n",
      "Training Epoch: 12 [500/13482]\tLoss: 1574.5778\n",
      "Training Epoch: 12 [550/13482]\tLoss: 1584.0427\n",
      "Training Epoch: 12 [600/13482]\tLoss: 1585.2690\n",
      "Training Epoch: 12 [650/13482]\tLoss: 1591.6479\n",
      "Training Epoch: 12 [700/13482]\tLoss: 1617.7323\n",
      "Training Epoch: 12 [750/13482]\tLoss: 1570.2106\n",
      "Training Epoch: 12 [800/13482]\tLoss: 1587.9617\n",
      "Training Epoch: 12 [850/13482]\tLoss: 1556.5192\n",
      "Training Epoch: 12 [900/13482]\tLoss: 1547.0638\n",
      "Training Epoch: 12 [950/13482]\tLoss: 1594.8276\n",
      "Training Epoch: 12 [1000/13482]\tLoss: 1607.1633\n",
      "Training Epoch: 12 [1050/13482]\tLoss: 1544.5901\n",
      "Training Epoch: 12 [1100/13482]\tLoss: 1586.2949\n",
      "Training Epoch: 12 [1150/13482]\tLoss: 1575.4670\n",
      "Training Epoch: 12 [1200/13482]\tLoss: 1577.0721\n",
      "Training Epoch: 12 [1250/13482]\tLoss: 1555.6603\n",
      "Training Epoch: 12 [1300/13482]\tLoss: 1559.7150\n",
      "Training Epoch: 12 [1350/13482]\tLoss: 1538.0023\n",
      "Training Epoch: 12 [1400/13482]\tLoss: 1553.8649\n",
      "Training Epoch: 12 [1450/13482]\tLoss: 1534.8912\n",
      "Training Epoch: 12 [1500/13482]\tLoss: 1577.6190\n",
      "Training Epoch: 12 [1550/13482]\tLoss: 1532.3135\n",
      "Training Epoch: 12 [1600/13482]\tLoss: 1519.3215\n",
      "Training Epoch: 12 [1650/13482]\tLoss: 1581.3623\n",
      "Training Epoch: 12 [1700/13482]\tLoss: 1589.0789\n",
      "Training Epoch: 12 [1750/13482]\tLoss: 1516.0623\n",
      "Training Epoch: 12 [1800/13482]\tLoss: 1559.0652\n",
      "Training Epoch: 12 [1850/13482]\tLoss: 1581.3662\n",
      "Training Epoch: 12 [1900/13482]\tLoss: 1543.0781\n",
      "Training Epoch: 12 [1950/13482]\tLoss: 1560.6927\n",
      "Training Epoch: 12 [2000/13482]\tLoss: 1571.8506\n",
      "Training Epoch: 12 [2050/13482]\tLoss: 1539.0844\n",
      "Training Epoch: 12 [2100/13482]\tLoss: 1570.9070\n",
      "Training Epoch: 12 [2150/13482]\tLoss: 1591.3588\n",
      "Training Epoch: 12 [2200/13482]\tLoss: 1509.3569\n",
      "Training Epoch: 12 [2250/13482]\tLoss: 1592.2747\n",
      "Training Epoch: 12 [2300/13482]\tLoss: 1551.6471\n",
      "Training Epoch: 12 [2350/13482]\tLoss: 1516.0968\n",
      "Training Epoch: 12 [2400/13482]\tLoss: 1514.9836\n",
      "Training Epoch: 12 [2450/13482]\tLoss: 1529.1141\n",
      "Training Epoch: 12 [2500/13482]\tLoss: 1533.0723\n",
      "Training Epoch: 12 [2550/13482]\tLoss: 1531.3640\n",
      "Training Epoch: 12 [2600/13482]\tLoss: 1547.4443\n",
      "Training Epoch: 12 [2650/13482]\tLoss: 1566.4977\n",
      "Training Epoch: 12 [2700/13482]\tLoss: 1560.8883\n",
      "Training Epoch: 12 [2750/13482]\tLoss: 1559.0598\n",
      "Training Epoch: 12 [2800/13482]\tLoss: 1567.2697\n",
      "Training Epoch: 12 [2850/13482]\tLoss: 1588.0530\n",
      "Training Epoch: 12 [2900/13482]\tLoss: 1611.4180\n",
      "Training Epoch: 12 [2950/13482]\tLoss: 1548.1595\n",
      "Training Epoch: 12 [3000/13482]\tLoss: 1496.5942\n",
      "Training Epoch: 12 [3050/13482]\tLoss: 1570.0127\n",
      "Training Epoch: 12 [3100/13482]\tLoss: 1521.3951\n",
      "Training Epoch: 12 [3150/13482]\tLoss: 1521.9142\n",
      "Training Epoch: 12 [3200/13482]\tLoss: 1531.4379\n",
      "Training Epoch: 12 [3250/13482]\tLoss: 1576.9961\n",
      "Training Epoch: 12 [3300/13482]\tLoss: 1593.1455\n",
      "Training Epoch: 12 [3350/13482]\tLoss: 1591.6782\n",
      "Training Epoch: 12 [3400/13482]\tLoss: 1531.1925\n",
      "Training Epoch: 12 [3450/13482]\tLoss: 1601.6326\n",
      "Training Epoch: 12 [3500/13482]\tLoss: 1518.6439\n",
      "Training Epoch: 12 [3550/13482]\tLoss: 1571.0505\n",
      "Training Epoch: 12 [3600/13482]\tLoss: 1630.5679\n",
      "Training Epoch: 12 [3650/13482]\tLoss: 1550.4849\n",
      "Training Epoch: 12 [3700/13482]\tLoss: 1505.3063\n",
      "Training Epoch: 12 [3750/13482]\tLoss: 1603.9413\n",
      "Training Epoch: 12 [3800/13482]\tLoss: 1536.0178\n",
      "Training Epoch: 12 [3850/13482]\tLoss: 1495.2073\n",
      "Training Epoch: 12 [3900/13482]\tLoss: 1510.7335\n",
      "Training Epoch: 12 [3950/13482]\tLoss: 1522.6747\n",
      "Training Epoch: 12 [4000/13482]\tLoss: 1519.5538\n",
      "Training Epoch: 12 [4050/13482]\tLoss: 1558.5217\n",
      "Training Epoch: 12 [4100/13482]\tLoss: 1609.3131\n",
      "Training Epoch: 12 [4150/13482]\tLoss: 1544.8898\n",
      "Training Epoch: 12 [4200/13482]\tLoss: 1541.9374\n",
      "Training Epoch: 12 [4250/13482]\tLoss: 1568.9849\n",
      "Training Epoch: 12 [4300/13482]\tLoss: 1560.0896\n",
      "Training Epoch: 12 [4350/13482]\tLoss: 1529.9077\n",
      "Training Epoch: 12 [4400/13482]\tLoss: 1622.0754\n",
      "Training Epoch: 12 [4450/13482]\tLoss: 1555.0254\n",
      "Training Epoch: 12 [4500/13482]\tLoss: 1577.6248\n",
      "Training Epoch: 12 [4550/13482]\tLoss: 1509.3324\n",
      "Training Epoch: 12 [4600/13482]\tLoss: 1473.0043\n",
      "Training Epoch: 12 [4650/13482]\tLoss: 1577.0835\n",
      "Training Epoch: 12 [4700/13482]\tLoss: 1525.4327\n",
      "Training Epoch: 12 [4750/13482]\tLoss: 1493.4260\n",
      "Training Epoch: 12 [4800/13482]\tLoss: 1561.3147\n",
      "Training Epoch: 12 [4850/13482]\tLoss: 1567.1748\n",
      "Training Epoch: 12 [4900/13482]\tLoss: 1570.0746\n",
      "Training Epoch: 12 [4950/13482]\tLoss: 1556.4214\n",
      "Training Epoch: 12 [5000/13482]\tLoss: 1552.1161\n",
      "Training Epoch: 12 [5050/13482]\tLoss: 1544.2035\n",
      "Training Epoch: 12 [5100/13482]\tLoss: 1528.7970\n",
      "Training Epoch: 12 [5150/13482]\tLoss: 1558.7990\n",
      "Training Epoch: 12 [5200/13482]\tLoss: 1576.6090\n",
      "Training Epoch: 12 [5250/13482]\tLoss: 1550.4954\n",
      "Training Epoch: 12 [5300/13482]\tLoss: 1546.9905\n",
      "Training Epoch: 12 [5350/13482]\tLoss: 1494.3384\n",
      "Training Epoch: 12 [5400/13482]\tLoss: 1546.8369\n",
      "Training Epoch: 12 [5450/13482]\tLoss: 1541.6195\n",
      "Training Epoch: 12 [5500/13482]\tLoss: 1577.0995\n",
      "Training Epoch: 12 [5550/13482]\tLoss: 1562.7238\n",
      "Training Epoch: 12 [5600/13482]\tLoss: 1534.5841\n",
      "Training Epoch: 12 [5650/13482]\tLoss: 1529.0865\n",
      "Training Epoch: 12 [5700/13482]\tLoss: 1550.3119\n",
      "Training Epoch: 12 [5750/13482]\tLoss: 1584.6029\n",
      "Training Epoch: 12 [5800/13482]\tLoss: 1590.8099\n",
      "Training Epoch: 12 [5850/13482]\tLoss: 1640.4578\n",
      "Training Epoch: 12 [5900/13482]\tLoss: 1547.1550\n",
      "Training Epoch: 12 [5950/13482]\tLoss: 1510.8165\n",
      "Training Epoch: 12 [6000/13482]\tLoss: 1542.2081\n",
      "Training Epoch: 12 [6050/13482]\tLoss: 1556.1945\n",
      "Training Epoch: 12 [6100/13482]\tLoss: 1588.4841\n",
      "Training Epoch: 12 [6150/13482]\tLoss: 1592.9929\n",
      "Training Epoch: 12 [6200/13482]\tLoss: 1617.2440\n",
      "Training Epoch: 12 [6250/13482]\tLoss: 1514.0580\n",
      "Training Epoch: 12 [6300/13482]\tLoss: 1516.8992\n",
      "Training Epoch: 12 [6350/13482]\tLoss: 1526.4944\n",
      "Training Epoch: 12 [6400/13482]\tLoss: 1590.7081\n",
      "Training Epoch: 12 [6450/13482]\tLoss: 1558.4337\n",
      "Training Epoch: 12 [6500/13482]\tLoss: 1528.6405\n",
      "Training Epoch: 12 [6550/13482]\tLoss: 1616.2327\n",
      "Training Epoch: 12 [6600/13482]\tLoss: 1542.6067\n",
      "Training Epoch: 12 [6650/13482]\tLoss: 1542.7007\n",
      "Training Epoch: 12 [6700/13482]\tLoss: 1521.5741\n",
      "Training Epoch: 12 [6750/13482]\tLoss: 1543.2595\n",
      "Training Epoch: 12 [6800/13482]\tLoss: 1536.3202\n",
      "Training Epoch: 12 [6850/13482]\tLoss: 1530.8528\n",
      "Training Epoch: 12 [6900/13482]\tLoss: 1553.0850\n",
      "Training Epoch: 12 [6950/13482]\tLoss: 1507.6272\n",
      "Training Epoch: 12 [7000/13482]\tLoss: 1552.4600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [7050/13482]\tLoss: 1555.2555\n",
      "Training Epoch: 12 [7100/13482]\tLoss: 1565.0819\n",
      "Training Epoch: 12 [7150/13482]\tLoss: 1579.0225\n",
      "Training Epoch: 12 [7200/13482]\tLoss: 1544.5808\n",
      "Training Epoch: 12 [7250/13482]\tLoss: 1525.5259\n",
      "Training Epoch: 12 [7300/13482]\tLoss: 1568.0299\n",
      "Training Epoch: 12 [7350/13482]\tLoss: 1527.3165\n",
      "Training Epoch: 12 [7400/13482]\tLoss: 1586.3434\n",
      "Training Epoch: 12 [7450/13482]\tLoss: 1541.8318\n",
      "Training Epoch: 12 [7500/13482]\tLoss: 1492.0571\n",
      "Training Epoch: 12 [7550/13482]\tLoss: 1596.0924\n",
      "Training Epoch: 12 [7600/13482]\tLoss: 1562.2295\n",
      "Training Epoch: 12 [7650/13482]\tLoss: 1592.8920\n",
      "Training Epoch: 12 [7700/13482]\tLoss: 1519.7118\n",
      "Training Epoch: 12 [7750/13482]\tLoss: 1497.9639\n",
      "Training Epoch: 12 [7800/13482]\tLoss: 1595.9951\n",
      "Training Epoch: 12 [7850/13482]\tLoss: 1593.8994\n",
      "Training Epoch: 12 [7900/13482]\tLoss: 1538.5590\n",
      "Training Epoch: 12 [7950/13482]\tLoss: 1584.3052\n",
      "Training Epoch: 12 [8000/13482]\tLoss: 1522.0311\n",
      "Training Epoch: 12 [8050/13482]\tLoss: 1589.0712\n",
      "Training Epoch: 12 [8100/13482]\tLoss: 1555.5917\n",
      "Training Epoch: 12 [8150/13482]\tLoss: 1567.8909\n",
      "Training Epoch: 12 [8200/13482]\tLoss: 1578.3617\n",
      "Training Epoch: 12 [8250/13482]\tLoss: 1487.4432\n",
      "Training Epoch: 12 [8300/13482]\tLoss: 1525.2780\n",
      "Training Epoch: 12 [8350/13482]\tLoss: 1569.6969\n",
      "Training Epoch: 12 [8400/13482]\tLoss: 1581.6772\n",
      "Training Epoch: 12 [8450/13482]\tLoss: 1517.1849\n",
      "Training Epoch: 12 [8500/13482]\tLoss: 1548.4827\n",
      "Training Epoch: 12 [8550/13482]\tLoss: 1532.5548\n",
      "Training Epoch: 12 [8600/13482]\tLoss: 1589.3831\n",
      "Training Epoch: 12 [8650/13482]\tLoss: 1576.7577\n",
      "Training Epoch: 12 [8700/13482]\tLoss: 1556.5544\n",
      "Training Epoch: 12 [8750/13482]\tLoss: 1538.3352\n",
      "Training Epoch: 12 [8800/13482]\tLoss: 1510.0260\n",
      "Training Epoch: 12 [8850/13482]\tLoss: 1554.5322\n",
      "Training Epoch: 12 [8900/13482]\tLoss: 1557.8735\n",
      "Training Epoch: 12 [8950/13482]\tLoss: 1516.2896\n",
      "Training Epoch: 12 [9000/13482]\tLoss: 1565.5708\n",
      "Training Epoch: 12 [9050/13482]\tLoss: 1560.2455\n",
      "Training Epoch: 12 [9100/13482]\tLoss: 1523.8466\n",
      "Training Epoch: 12 [9150/13482]\tLoss: 1571.5793\n",
      "Training Epoch: 12 [9200/13482]\tLoss: 1621.9207\n",
      "Training Epoch: 12 [9250/13482]\tLoss: 1499.8290\n",
      "Training Epoch: 12 [9300/13482]\tLoss: 1577.1812\n",
      "Training Epoch: 12 [9350/13482]\tLoss: 1538.6942\n",
      "Training Epoch: 12 [9400/13482]\tLoss: 1488.5547\n",
      "Training Epoch: 12 [9450/13482]\tLoss: 1572.2913\n",
      "Training Epoch: 12 [9500/13482]\tLoss: 1534.9865\n",
      "Training Epoch: 12 [9550/13482]\tLoss: 1540.3094\n",
      "Training Epoch: 12 [9600/13482]\tLoss: 1553.0956\n",
      "Training Epoch: 12 [9650/13482]\tLoss: 1529.1798\n",
      "Training Epoch: 12 [9700/13482]\tLoss: 1537.1991\n",
      "Training Epoch: 12 [9750/13482]\tLoss: 1527.9739\n",
      "Training Epoch: 12 [9800/13482]\tLoss: 1502.6495\n",
      "Training Epoch: 12 [9850/13482]\tLoss: 1501.1699\n",
      "Training Epoch: 12 [9900/13482]\tLoss: 1567.6887\n",
      "Training Epoch: 12 [9950/13482]\tLoss: 1545.0139\n",
      "Training Epoch: 12 [10000/13482]\tLoss: 1527.0275\n",
      "Training Epoch: 12 [10050/13482]\tLoss: 1588.1693\n",
      "Training Epoch: 12 [10100/13482]\tLoss: 1548.0581\n",
      "Training Epoch: 12 [10150/13482]\tLoss: 1545.1531\n",
      "Training Epoch: 12 [10200/13482]\tLoss: 1512.6122\n",
      "Training Epoch: 12 [10250/13482]\tLoss: 1591.7401\n",
      "Training Epoch: 12 [10300/13482]\tLoss: 1520.7966\n",
      "Training Epoch: 12 [10350/13482]\tLoss: 1543.3257\n",
      "Training Epoch: 12 [10400/13482]\tLoss: 1510.5781\n",
      "Training Epoch: 12 [10450/13482]\tLoss: 1546.5100\n",
      "Training Epoch: 12 [10500/13482]\tLoss: 1604.8755\n",
      "Training Epoch: 12 [10550/13482]\tLoss: 1566.5106\n",
      "Training Epoch: 12 [10600/13482]\tLoss: 1547.4825\n",
      "Training Epoch: 12 [10650/13482]\tLoss: 1517.4478\n",
      "Training Epoch: 12 [10700/13482]\tLoss: 1532.7782\n",
      "Training Epoch: 12 [10750/13482]\tLoss: 1518.5756\n",
      "Training Epoch: 12 [10800/13482]\tLoss: 1519.1481\n",
      "Training Epoch: 12 [10850/13482]\tLoss: 1539.8909\n",
      "Training Epoch: 12 [10900/13482]\tLoss: 1511.6401\n",
      "Training Epoch: 12 [10950/13482]\tLoss: 1517.3324\n",
      "Training Epoch: 12 [11000/13482]\tLoss: 1537.4130\n",
      "Training Epoch: 12 [11050/13482]\tLoss: 1547.5701\n",
      "Training Epoch: 12 [11100/13482]\tLoss: 1547.4261\n",
      "Training Epoch: 12 [11150/13482]\tLoss: 1538.3002\n",
      "Training Epoch: 12 [11200/13482]\tLoss: 1507.4935\n",
      "Training Epoch: 12 [11250/13482]\tLoss: 1571.8934\n",
      "Training Epoch: 12 [11300/13482]\tLoss: 1542.1044\n",
      "Training Epoch: 12 [11350/13482]\tLoss: 1595.9404\n",
      "Training Epoch: 12 [11400/13482]\tLoss: 1547.7880\n",
      "Training Epoch: 12 [11450/13482]\tLoss: 1515.1807\n",
      "Training Epoch: 12 [11500/13482]\tLoss: 1538.2401\n",
      "Training Epoch: 12 [11550/13482]\tLoss: 1580.2491\n",
      "Training Epoch: 12 [11600/13482]\tLoss: 1532.2230\n",
      "Training Epoch: 12 [11650/13482]\tLoss: 1561.3085\n",
      "Training Epoch: 12 [11700/13482]\tLoss: 1512.6947\n",
      "Training Epoch: 12 [11750/13482]\tLoss: 1507.4343\n",
      "Training Epoch: 12 [11800/13482]\tLoss: 1565.6636\n",
      "Training Epoch: 12 [11850/13482]\tLoss: 1495.8827\n",
      "Training Epoch: 12 [11900/13482]\tLoss: 1513.7852\n",
      "Training Epoch: 12 [11950/13482]\tLoss: 1604.0172\n",
      "Training Epoch: 12 [12000/13482]\tLoss: 1488.4406\n",
      "Training Epoch: 12 [12050/13482]\tLoss: 1483.1641\n",
      "Training Epoch: 12 [12100/13482]\tLoss: 1562.9460\n",
      "Training Epoch: 12 [12150/13482]\tLoss: 1521.7986\n",
      "Training Epoch: 12 [12200/13482]\tLoss: 1531.8690\n",
      "Training Epoch: 12 [12250/13482]\tLoss: 1509.7133\n",
      "Training Epoch: 12 [12300/13482]\tLoss: 1565.4717\n",
      "Training Epoch: 12 [12350/13482]\tLoss: 1518.9548\n",
      "Training Epoch: 12 [12400/13482]\tLoss: 1539.7834\n",
      "Training Epoch: 12 [12450/13482]\tLoss: 1505.2241\n",
      "Training Epoch: 12 [12500/13482]\tLoss: 1560.3307\n",
      "Training Epoch: 12 [12550/13482]\tLoss: 1543.5521\n",
      "Training Epoch: 12 [12600/13482]\tLoss: 1545.0048\n",
      "Training Epoch: 12 [12650/13482]\tLoss: 1531.4292\n",
      "Training Epoch: 12 [12700/13482]\tLoss: 1494.6881\n",
      "Training Epoch: 12 [12750/13482]\tLoss: 1537.0608\n",
      "Training Epoch: 12 [12800/13482]\tLoss: 1522.4950\n",
      "Training Epoch: 12 [12850/13482]\tLoss: 1567.6354\n",
      "Training Epoch: 12 [12900/13482]\tLoss: 1568.9056\n",
      "Training Epoch: 12 [12950/13482]\tLoss: 1511.7784\n",
      "Training Epoch: 12 [13000/13482]\tLoss: 1560.2296\n",
      "Training Epoch: 12 [13050/13482]\tLoss: 1563.6715\n",
      "Training Epoch: 12 [13100/13482]\tLoss: 1506.8561\n",
      "Training Epoch: 12 [13150/13482]\tLoss: 1530.3989\n",
      "Training Epoch: 12 [13200/13482]\tLoss: 1539.7791\n",
      "Training Epoch: 12 [13250/13482]\tLoss: 1551.6266\n",
      "Training Epoch: 12 [13300/13482]\tLoss: 1526.6117\n",
      "Training Epoch: 12 [13350/13482]\tLoss: 1485.9211\n",
      "Training Epoch: 12 [13400/13482]\tLoss: 1544.2113\n",
      "Training Epoch: 12 [13450/13482]\tLoss: 1576.7181\n",
      "Training Epoch: 12 [13482/13482]\tLoss: 1554.9487\n",
      "Training Epoch: 12 [1497/1497]\tLoss: 1523.2544\n",
      "Training Epoch: 13 [50/13482]\tLoss: 1551.8162\n",
      "Training Epoch: 13 [100/13482]\tLoss: 1544.1887\n",
      "Training Epoch: 13 [150/13482]\tLoss: 1544.5587\n",
      "Training Epoch: 13 [200/13482]\tLoss: 1542.0503\n",
      "Training Epoch: 13 [250/13482]\tLoss: 1542.4999\n",
      "Training Epoch: 13 [300/13482]\tLoss: 1522.2150\n",
      "Training Epoch: 13 [350/13482]\tLoss: 1509.3110\n",
      "Training Epoch: 13 [400/13482]\tLoss: 1528.3568\n",
      "Training Epoch: 13 [450/13482]\tLoss: 1528.1455\n",
      "Training Epoch: 13 [500/13482]\tLoss: 1541.4147\n",
      "Training Epoch: 13 [550/13482]\tLoss: 1549.9313\n",
      "Training Epoch: 13 [600/13482]\tLoss: 1548.7031\n",
      "Training Epoch: 13 [650/13482]\tLoss: 1557.3549\n",
      "Training Epoch: 13 [700/13482]\tLoss: 1584.0586\n",
      "Training Epoch: 13 [750/13482]\tLoss: 1535.9702\n",
      "Training Epoch: 13 [800/13482]\tLoss: 1551.4038\n",
      "Training Epoch: 13 [850/13482]\tLoss: 1520.9543\n",
      "Training Epoch: 13 [900/13482]\tLoss: 1511.9823\n",
      "Training Epoch: 13 [950/13482]\tLoss: 1558.9875\n",
      "Training Epoch: 13 [1000/13482]\tLoss: 1570.8564\n",
      "Training Epoch: 13 [1050/13482]\tLoss: 1508.2668\n",
      "Training Epoch: 13 [1100/13482]\tLoss: 1550.4766\n",
      "Training Epoch: 13 [1150/13482]\tLoss: 1541.4603\n",
      "Training Epoch: 13 [1200/13482]\tLoss: 1543.6238\n",
      "Training Epoch: 13 [1250/13482]\tLoss: 1520.8892\n",
      "Training Epoch: 13 [1300/13482]\tLoss: 1526.1658\n",
      "Training Epoch: 13 [1350/13482]\tLoss: 1503.7872\n",
      "Training Epoch: 13 [1400/13482]\tLoss: 1518.3674\n",
      "Training Epoch: 13 [1450/13482]\tLoss: 1501.5625\n",
      "Training Epoch: 13 [1500/13482]\tLoss: 1542.5375\n",
      "Training Epoch: 13 [1550/13482]\tLoss: 1496.3579\n",
      "Training Epoch: 13 [1600/13482]\tLoss: 1485.0074\n",
      "Training Epoch: 13 [1650/13482]\tLoss: 1548.8936\n",
      "Training Epoch: 13 [1700/13482]\tLoss: 1555.0093\n",
      "Training Epoch: 13 [1750/13482]\tLoss: 1480.5773\n",
      "Training Epoch: 13 [1800/13482]\tLoss: 1522.0029\n",
      "Training Epoch: 13 [1850/13482]\tLoss: 1546.6498\n",
      "Training Epoch: 13 [1900/13482]\tLoss: 1508.4786\n",
      "Training Epoch: 13 [1950/13482]\tLoss: 1523.1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [2000/13482]\tLoss: 1536.8812\n",
      "Training Epoch: 13 [2050/13482]\tLoss: 1504.0947\n",
      "Training Epoch: 13 [2100/13482]\tLoss: 1537.0967\n",
      "Training Epoch: 13 [2150/13482]\tLoss: 1557.1504\n",
      "Training Epoch: 13 [2200/13482]\tLoss: 1475.8503\n",
      "Training Epoch: 13 [2250/13482]\tLoss: 1559.3306\n",
      "Training Epoch: 13 [2300/13482]\tLoss: 1518.0878\n",
      "Training Epoch: 13 [2350/13482]\tLoss: 1480.7577\n",
      "Training Epoch: 13 [2400/13482]\tLoss: 1481.2345\n",
      "Training Epoch: 13 [2450/13482]\tLoss: 1493.2886\n",
      "Training Epoch: 13 [2500/13482]\tLoss: 1499.3296\n",
      "Training Epoch: 13 [2550/13482]\tLoss: 1496.4874\n",
      "Training Epoch: 13 [2600/13482]\tLoss: 1511.9733\n",
      "Training Epoch: 13 [2650/13482]\tLoss: 1531.7739\n",
      "Training Epoch: 13 [2700/13482]\tLoss: 1525.5411\n",
      "Training Epoch: 13 [2750/13482]\tLoss: 1525.8219\n",
      "Training Epoch: 13 [2800/13482]\tLoss: 1534.6141\n",
      "Training Epoch: 13 [2850/13482]\tLoss: 1548.1893\n",
      "Training Epoch: 13 [2900/13482]\tLoss: 1576.2467\n",
      "Training Epoch: 13 [2950/13482]\tLoss: 1513.7557\n",
      "Training Epoch: 13 [3000/13482]\tLoss: 1462.0278\n",
      "Training Epoch: 13 [3050/13482]\tLoss: 1534.9668\n",
      "Training Epoch: 13 [3100/13482]\tLoss: 1488.0519\n",
      "Training Epoch: 13 [3150/13482]\tLoss: 1487.8094\n",
      "Training Epoch: 13 [3200/13482]\tLoss: 1494.7112\n",
      "Training Epoch: 13 [3250/13482]\tLoss: 1541.0750\n",
      "Training Epoch: 13 [3300/13482]\tLoss: 1557.5468\n",
      "Training Epoch: 13 [3350/13482]\tLoss: 1557.4576\n",
      "Training Epoch: 13 [3400/13482]\tLoss: 1497.9702\n",
      "Training Epoch: 13 [3450/13482]\tLoss: 1567.2676\n",
      "Training Epoch: 13 [3500/13482]\tLoss: 1481.7129\n",
      "Training Epoch: 13 [3550/13482]\tLoss: 1536.2405\n",
      "Training Epoch: 13 [3600/13482]\tLoss: 1595.8105\n",
      "Training Epoch: 13 [3650/13482]\tLoss: 1518.0890\n",
      "Training Epoch: 13 [3700/13482]\tLoss: 1473.1050\n",
      "Training Epoch: 13 [3750/13482]\tLoss: 1569.6384\n",
      "Training Epoch: 13 [3800/13482]\tLoss: 1498.9413\n",
      "Training Epoch: 13 [3850/13482]\tLoss: 1458.5754\n",
      "Training Epoch: 13 [3900/13482]\tLoss: 1475.2848\n",
      "Training Epoch: 13 [3950/13482]\tLoss: 1490.2406\n",
      "Training Epoch: 13 [4000/13482]\tLoss: 1484.7250\n",
      "Training Epoch: 13 [4050/13482]\tLoss: 1525.8137\n",
      "Training Epoch: 13 [4100/13482]\tLoss: 1576.2086\n",
      "Training Epoch: 13 [4150/13482]\tLoss: 1509.9489\n",
      "Training Epoch: 13 [4200/13482]\tLoss: 1507.9929\n",
      "Training Epoch: 13 [4250/13482]\tLoss: 1534.8091\n",
      "Training Epoch: 13 [4300/13482]\tLoss: 1525.6166\n",
      "Training Epoch: 13 [4350/13482]\tLoss: 1496.6027\n",
      "Training Epoch: 13 [4400/13482]\tLoss: 1589.5789\n",
      "Training Epoch: 13 [4450/13482]\tLoss: 1522.3383\n",
      "Training Epoch: 13 [4500/13482]\tLoss: 1544.3728\n",
      "Training Epoch: 13 [4550/13482]\tLoss: 1476.1573\n",
      "Training Epoch: 13 [4600/13482]\tLoss: 1438.6118\n",
      "Training Epoch: 13 [4650/13482]\tLoss: 1542.4415\n",
      "Training Epoch: 13 [4700/13482]\tLoss: 1491.6116\n",
      "Training Epoch: 13 [4750/13482]\tLoss: 1457.4298\n",
      "Training Epoch: 13 [4800/13482]\tLoss: 1528.2428\n",
      "Training Epoch: 13 [4850/13482]\tLoss: 1535.6049\n",
      "Training Epoch: 13 [4900/13482]\tLoss: 1535.1304\n",
      "Training Epoch: 13 [4950/13482]\tLoss: 1521.7518\n",
      "Training Epoch: 13 [5000/13482]\tLoss: 1517.0417\n",
      "Training Epoch: 13 [5050/13482]\tLoss: 1509.7668\n",
      "Training Epoch: 13 [5100/13482]\tLoss: 1494.6316\n",
      "Training Epoch: 13 [5150/13482]\tLoss: 1526.1434\n",
      "Training Epoch: 13 [5200/13482]\tLoss: 1542.5164\n",
      "Training Epoch: 13 [5250/13482]\tLoss: 1515.8773\n",
      "Training Epoch: 13 [5300/13482]\tLoss: 1513.2540\n",
      "Training Epoch: 13 [5350/13482]\tLoss: 1460.7380\n",
      "Training Epoch: 13 [5400/13482]\tLoss: 1510.1245\n",
      "Training Epoch: 13 [5450/13482]\tLoss: 1507.3398\n",
      "Training Epoch: 13 [5500/13482]\tLoss: 1542.6550\n",
      "Training Epoch: 13 [5550/13482]\tLoss: 1528.2191\n",
      "Training Epoch: 13 [5600/13482]\tLoss: 1502.1766\n",
      "Training Epoch: 13 [5650/13482]\tLoss: 1494.9158\n",
      "Training Epoch: 13 [5700/13482]\tLoss: 1516.6484\n",
      "Training Epoch: 13 [5750/13482]\tLoss: 1549.5470\n",
      "Training Epoch: 13 [5800/13482]\tLoss: 1556.5393\n",
      "Training Epoch: 13 [5850/13482]\tLoss: 1602.8530\n",
      "Training Epoch: 13 [5900/13482]\tLoss: 1512.9497\n",
      "Training Epoch: 13 [5950/13482]\tLoss: 1475.5513\n",
      "Training Epoch: 13 [6000/13482]\tLoss: 1508.7996\n",
      "Training Epoch: 13 [6050/13482]\tLoss: 1523.0420\n",
      "Training Epoch: 13 [6100/13482]\tLoss: 1554.9398\n",
      "Training Epoch: 13 [6150/13482]\tLoss: 1557.6974\n",
      "Training Epoch: 13 [6200/13482]\tLoss: 1581.2867\n",
      "Training Epoch: 13 [6250/13482]\tLoss: 1479.1904\n",
      "Training Epoch: 13 [6300/13482]\tLoss: 1486.1423\n",
      "Training Epoch: 13 [6350/13482]\tLoss: 1489.6130\n",
      "Training Epoch: 13 [6400/13482]\tLoss: 1557.2090\n",
      "Training Epoch: 13 [6450/13482]\tLoss: 1525.7167\n",
      "Training Epoch: 13 [6500/13482]\tLoss: 1495.7766\n",
      "Training Epoch: 13 [6550/13482]\tLoss: 1582.2657\n",
      "Training Epoch: 13 [6600/13482]\tLoss: 1510.8853\n",
      "Training Epoch: 13 [6650/13482]\tLoss: 1506.5408\n",
      "Training Epoch: 13 [6700/13482]\tLoss: 1487.1666\n",
      "Training Epoch: 13 [6750/13482]\tLoss: 1509.8292\n",
      "Training Epoch: 13 [6800/13482]\tLoss: 1504.4073\n",
      "Training Epoch: 13 [6850/13482]\tLoss: 1498.0089\n",
      "Training Epoch: 13 [6900/13482]\tLoss: 1519.2615\n",
      "Training Epoch: 13 [6950/13482]\tLoss: 1472.7910\n",
      "Training Epoch: 13 [7000/13482]\tLoss: 1519.3378\n",
      "Training Epoch: 13 [7050/13482]\tLoss: 1519.7179\n",
      "Training Epoch: 13 [7100/13482]\tLoss: 1530.4879\n",
      "Training Epoch: 13 [7150/13482]\tLoss: 1545.3771\n",
      "Training Epoch: 13 [7200/13482]\tLoss: 1509.8423\n",
      "Training Epoch: 13 [7250/13482]\tLoss: 1490.6064\n",
      "Training Epoch: 13 [7300/13482]\tLoss: 1535.0927\n",
      "Training Epoch: 13 [7350/13482]\tLoss: 1495.0673\n",
      "Training Epoch: 13 [7400/13482]\tLoss: 1552.5208\n",
      "Training Epoch: 13 [7450/13482]\tLoss: 1508.0670\n",
      "Training Epoch: 13 [7500/13482]\tLoss: 1459.2000\n",
      "Training Epoch: 13 [7550/13482]\tLoss: 1562.8245\n",
      "Training Epoch: 13 [7600/13482]\tLoss: 1526.0883\n",
      "Training Epoch: 13 [7650/13482]\tLoss: 1558.7173\n",
      "Training Epoch: 13 [7700/13482]\tLoss: 1485.6901\n",
      "Training Epoch: 13 [7750/13482]\tLoss: 1463.5385\n",
      "Training Epoch: 13 [7800/13482]\tLoss: 1561.6052\n",
      "Training Epoch: 13 [7850/13482]\tLoss: 1559.2675\n",
      "Training Epoch: 13 [7900/13482]\tLoss: 1504.4540\n",
      "Training Epoch: 13 [7950/13482]\tLoss: 1550.2021\n",
      "Training Epoch: 13 [8000/13482]\tLoss: 1490.3323\n",
      "Training Epoch: 13 [8050/13482]\tLoss: 1554.2675\n",
      "Training Epoch: 13 [8100/13482]\tLoss: 1521.3608\n",
      "Training Epoch: 13 [8150/13482]\tLoss: 1533.3219\n",
      "Training Epoch: 13 [8200/13482]\tLoss: 1546.2184\n",
      "Training Epoch: 13 [8250/13482]\tLoss: 1454.5122\n",
      "Training Epoch: 13 [8300/13482]\tLoss: 1490.7070\n",
      "Training Epoch: 13 [8350/13482]\tLoss: 1536.6466\n",
      "Training Epoch: 13 [8400/13482]\tLoss: 1547.5261\n",
      "Training Epoch: 13 [8450/13482]\tLoss: 1484.0551\n",
      "Training Epoch: 13 [8500/13482]\tLoss: 1515.4780\n",
      "Training Epoch: 13 [8550/13482]\tLoss: 1498.5996\n",
      "Training Epoch: 13 [8600/13482]\tLoss: 1556.9481\n",
      "Training Epoch: 13 [8650/13482]\tLoss: 1543.1747\n",
      "Training Epoch: 13 [8700/13482]\tLoss: 1524.8619\n",
      "Training Epoch: 13 [8750/13482]\tLoss: 1504.2167\n",
      "Training Epoch: 13 [8800/13482]\tLoss: 1473.6306\n",
      "Training Epoch: 13 [8850/13482]\tLoss: 1522.1783\n",
      "Training Epoch: 13 [8900/13482]\tLoss: 1525.3162\n",
      "Training Epoch: 13 [8950/13482]\tLoss: 1482.4188\n",
      "Training Epoch: 13 [9000/13482]\tLoss: 1533.4896\n",
      "Training Epoch: 13 [9050/13482]\tLoss: 1528.3020\n",
      "Training Epoch: 13 [9100/13482]\tLoss: 1492.0851\n",
      "Training Epoch: 13 [9150/13482]\tLoss: 1537.4933\n",
      "Training Epoch: 13 [9200/13482]\tLoss: 1588.4237\n",
      "Training Epoch: 13 [9250/13482]\tLoss: 1467.1343\n",
      "Training Epoch: 13 [9300/13482]\tLoss: 1543.8700\n",
      "Training Epoch: 13 [9350/13482]\tLoss: 1503.9047\n",
      "Training Epoch: 13 [9400/13482]\tLoss: 1455.7291\n",
      "Training Epoch: 13 [9450/13482]\tLoss: 1537.2888\n",
      "Training Epoch: 13 [9500/13482]\tLoss: 1502.3342\n",
      "Training Epoch: 13 [9550/13482]\tLoss: 1506.2018\n",
      "Training Epoch: 13 [9600/13482]\tLoss: 1517.8115\n",
      "Training Epoch: 13 [9650/13482]\tLoss: 1494.3203\n",
      "Training Epoch: 13 [9700/13482]\tLoss: 1502.5068\n",
      "Training Epoch: 13 [9750/13482]\tLoss: 1493.9476\n",
      "Training Epoch: 13 [9800/13482]\tLoss: 1469.0139\n",
      "Training Epoch: 13 [9850/13482]\tLoss: 1466.8457\n",
      "Training Epoch: 13 [9900/13482]\tLoss: 1535.6396\n",
      "Training Epoch: 13 [9950/13482]\tLoss: 1509.5044\n",
      "Training Epoch: 13 [10000/13482]\tLoss: 1493.3973\n",
      "Training Epoch: 13 [10050/13482]\tLoss: 1554.7332\n",
      "Training Epoch: 13 [10100/13482]\tLoss: 1516.7668\n",
      "Training Epoch: 13 [10150/13482]\tLoss: 1511.6780\n",
      "Training Epoch: 13 [10200/13482]\tLoss: 1478.6471\n",
      "Training Epoch: 13 [10250/13482]\tLoss: 1557.0070\n",
      "Training Epoch: 13 [10300/13482]\tLoss: 1485.5591\n",
      "Training Epoch: 13 [10350/13482]\tLoss: 1510.2729\n",
      "Training Epoch: 13 [10400/13482]\tLoss: 1478.4634\n",
      "Training Epoch: 13 [10450/13482]\tLoss: 1512.9022\n",
      "Training Epoch: 13 [10500/13482]\tLoss: 1569.8793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [10550/13482]\tLoss: 1532.0734\n",
      "Training Epoch: 13 [10600/13482]\tLoss: 1513.2908\n",
      "Training Epoch: 13 [10650/13482]\tLoss: 1485.7692\n",
      "Training Epoch: 13 [10700/13482]\tLoss: 1501.2776\n",
      "Training Epoch: 13 [10750/13482]\tLoss: 1483.6371\n",
      "Training Epoch: 13 [10800/13482]\tLoss: 1485.2908\n",
      "Training Epoch: 13 [10850/13482]\tLoss: 1507.2706\n",
      "Training Epoch: 13 [10900/13482]\tLoss: 1478.3071\n",
      "Training Epoch: 13 [10950/13482]\tLoss: 1484.9973\n",
      "Training Epoch: 13 [11000/13482]\tLoss: 1506.0767\n",
      "Training Epoch: 13 [11050/13482]\tLoss: 1514.3499\n",
      "Training Epoch: 13 [11100/13482]\tLoss: 1514.7135\n",
      "Training Epoch: 13 [11150/13482]\tLoss: 1504.8612\n",
      "Training Epoch: 13 [11200/13482]\tLoss: 1475.3760\n",
      "Training Epoch: 13 [11250/13482]\tLoss: 1538.8278\n",
      "Training Epoch: 13 [11300/13482]\tLoss: 1509.1516\n",
      "Training Epoch: 13 [11350/13482]\tLoss: 1563.6666\n",
      "Training Epoch: 13 [11400/13482]\tLoss: 1513.9974\n",
      "Training Epoch: 13 [11450/13482]\tLoss: 1484.2275\n",
      "Training Epoch: 13 [11500/13482]\tLoss: 1506.2930\n",
      "Training Epoch: 13 [11550/13482]\tLoss: 1547.5875\n",
      "Training Epoch: 13 [11600/13482]\tLoss: 1497.5040\n",
      "Training Epoch: 13 [11650/13482]\tLoss: 1528.8628\n",
      "Training Epoch: 13 [11700/13482]\tLoss: 1480.8097\n",
      "Training Epoch: 13 [11750/13482]\tLoss: 1477.1505\n",
      "Training Epoch: 13 [11800/13482]\tLoss: 1534.2477\n",
      "Training Epoch: 13 [11850/13482]\tLoss: 1464.3181\n",
      "Training Epoch: 13 [11900/13482]\tLoss: 1480.9415\n",
      "Training Epoch: 13 [11950/13482]\tLoss: 1574.3003\n",
      "Training Epoch: 13 [12000/13482]\tLoss: 1455.6930\n",
      "Training Epoch: 13 [12050/13482]\tLoss: 1449.7360\n",
      "Training Epoch: 13 [12100/13482]\tLoss: 1528.7944\n",
      "Training Epoch: 13 [12150/13482]\tLoss: 1487.1400\n",
      "Training Epoch: 13 [12200/13482]\tLoss: 1499.4968\n",
      "Training Epoch: 13 [12250/13482]\tLoss: 1477.3958\n",
      "Training Epoch: 13 [12300/13482]\tLoss: 1532.9259\n",
      "Training Epoch: 13 [12350/13482]\tLoss: 1485.9866\n",
      "Training Epoch: 13 [12400/13482]\tLoss: 1506.4662\n",
      "Training Epoch: 13 [12450/13482]\tLoss: 1472.8265\n",
      "Training Epoch: 13 [12500/13482]\tLoss: 1526.7158\n",
      "Training Epoch: 13 [12550/13482]\tLoss: 1511.5533\n",
      "Training Epoch: 13 [12600/13482]\tLoss: 1512.6978\n",
      "Training Epoch: 13 [12650/13482]\tLoss: 1498.6915\n",
      "Training Epoch: 13 [12700/13482]\tLoss: 1462.0840\n",
      "Training Epoch: 13 [12750/13482]\tLoss: 1504.2255\n",
      "Training Epoch: 13 [12800/13482]\tLoss: 1488.6783\n",
      "Training Epoch: 13 [12850/13482]\tLoss: 1535.3652\n",
      "Training Epoch: 13 [12900/13482]\tLoss: 1535.3411\n",
      "Training Epoch: 13 [12950/13482]\tLoss: 1478.1630\n",
      "Training Epoch: 13 [13000/13482]\tLoss: 1529.1886\n",
      "Training Epoch: 13 [13050/13482]\tLoss: 1528.8702\n",
      "Training Epoch: 13 [13100/13482]\tLoss: 1473.8470\n",
      "Training Epoch: 13 [13150/13482]\tLoss: 1497.9199\n",
      "Training Epoch: 13 [13200/13482]\tLoss: 1505.4712\n",
      "Training Epoch: 13 [13250/13482]\tLoss: 1517.3468\n",
      "Training Epoch: 13 [13300/13482]\tLoss: 1493.7354\n",
      "Training Epoch: 13 [13350/13482]\tLoss: 1453.0133\n",
      "Training Epoch: 13 [13400/13482]\tLoss: 1507.4895\n",
      "Training Epoch: 13 [13450/13482]\tLoss: 1542.8591\n",
      "Training Epoch: 13 [13482/13482]\tLoss: 1521.3906\n",
      "Training Epoch: 13 [1497/1497]\tLoss: 1490.2533\n",
      "Training Epoch: 14 [50/13482]\tLoss: 1520.5829\n",
      "Training Epoch: 14 [100/13482]\tLoss: 1510.1949\n",
      "Training Epoch: 14 [150/13482]\tLoss: 1511.6484\n",
      "Training Epoch: 14 [200/13482]\tLoss: 1507.1129\n",
      "Training Epoch: 14 [250/13482]\tLoss: 1510.2527\n",
      "Training Epoch: 14 [300/13482]\tLoss: 1489.0098\n",
      "Training Epoch: 14 [350/13482]\tLoss: 1475.5488\n",
      "Training Epoch: 14 [400/13482]\tLoss: 1497.2496\n",
      "Training Epoch: 14 [450/13482]\tLoss: 1495.7203\n",
      "Training Epoch: 14 [500/13482]\tLoss: 1510.5270\n",
      "Training Epoch: 14 [550/13482]\tLoss: 1517.4519\n",
      "Training Epoch: 14 [600/13482]\tLoss: 1513.9354\n",
      "Training Epoch: 14 [650/13482]\tLoss: 1524.9857\n",
      "Training Epoch: 14 [700/13482]\tLoss: 1552.3647\n",
      "Training Epoch: 14 [750/13482]\tLoss: 1503.8518\n",
      "Training Epoch: 14 [800/13482]\tLoss: 1516.8571\n",
      "Training Epoch: 14 [850/13482]\tLoss: 1487.7272\n",
      "Training Epoch: 14 [900/13482]\tLoss: 1479.2954\n",
      "Training Epoch: 14 [950/13482]\tLoss: 1525.2463\n",
      "Training Epoch: 14 [1000/13482]\tLoss: 1536.8981\n",
      "Training Epoch: 14 [1050/13482]\tLoss: 1474.7510\n",
      "Training Epoch: 14 [1100/13482]\tLoss: 1517.1198\n",
      "Training Epoch: 14 [1150/13482]\tLoss: 1509.2018\n",
      "Training Epoch: 14 [1200/13482]\tLoss: 1511.1672\n",
      "Training Epoch: 14 [1250/13482]\tLoss: 1487.9425\n",
      "Training Epoch: 14 [1300/13482]\tLoss: 1494.5649\n",
      "Training Epoch: 14 [1350/13482]\tLoss: 1471.8663\n",
      "Training Epoch: 14 [1400/13482]\tLoss: 1485.0197\n",
      "Training Epoch: 14 [1450/13482]\tLoss: 1469.5902\n",
      "Training Epoch: 14 [1500/13482]\tLoss: 1509.4893\n",
      "Training Epoch: 14 [1550/13482]\tLoss: 1462.6813\n",
      "Training Epoch: 14 [1600/13482]\tLoss: 1453.4287\n",
      "Training Epoch: 14 [1650/13482]\tLoss: 1518.2662\n",
      "Training Epoch: 14 [1700/13482]\tLoss: 1522.6799\n",
      "Training Epoch: 14 [1750/13482]\tLoss: 1447.2244\n",
      "Training Epoch: 14 [1800/13482]\tLoss: 1487.7219\n",
      "Training Epoch: 14 [1850/13482]\tLoss: 1514.1979\n",
      "Training Epoch: 14 [1900/13482]\tLoss: 1476.1851\n",
      "Training Epoch: 14 [1950/13482]\tLoss: 1488.1696\n",
      "Training Epoch: 14 [2000/13482]\tLoss: 1504.2020\n",
      "Training Epoch: 14 [2050/13482]\tLoss: 1471.2664\n",
      "Training Epoch: 14 [2100/13482]\tLoss: 1504.1161\n",
      "Training Epoch: 14 [2150/13482]\tLoss: 1525.2340\n",
      "Training Epoch: 14 [2200/13482]\tLoss: 1444.6549\n",
      "Training Epoch: 14 [2250/13482]\tLoss: 1528.3801\n",
      "Training Epoch: 14 [2300/13482]\tLoss: 1486.5271\n",
      "Training Epoch: 14 [2350/13482]\tLoss: 1448.6257\n",
      "Training Epoch: 14 [2400/13482]\tLoss: 1450.1641\n",
      "Training Epoch: 14 [2450/13482]\tLoss: 1459.8979\n",
      "Training Epoch: 14 [2500/13482]\tLoss: 1467.2455\n",
      "Training Epoch: 14 [2550/13482]\tLoss: 1464.0391\n",
      "Training Epoch: 14 [2600/13482]\tLoss: 1479.0516\n",
      "Training Epoch: 14 [2650/13482]\tLoss: 1499.8173\n",
      "Training Epoch: 14 [2700/13482]\tLoss: 1491.9966\n",
      "Training Epoch: 14 [2750/13482]\tLoss: 1493.6826\n",
      "Training Epoch: 14 [2800/13482]\tLoss: 1503.2964\n",
      "Training Epoch: 14 [2850/13482]\tLoss: 1511.0151\n",
      "Training Epoch: 14 [2900/13482]\tLoss: 1543.3385\n",
      "Training Epoch: 14 [2950/13482]\tLoss: 1481.7294\n",
      "Training Epoch: 14 [3000/13482]\tLoss: 1429.5861\n",
      "Training Epoch: 14 [3050/13482]\tLoss: 1501.5757\n",
      "Training Epoch: 14 [3100/13482]\tLoss: 1455.7900\n",
      "Training Epoch: 14 [3150/13482]\tLoss: 1455.3405\n",
      "Training Epoch: 14 [3200/13482]\tLoss: 1460.5463\n",
      "Training Epoch: 14 [3250/13482]\tLoss: 1507.5165\n",
      "Training Epoch: 14 [3300/13482]\tLoss: 1524.0391\n",
      "Training Epoch: 14 [3350/13482]\tLoss: 1524.5093\n",
      "Training Epoch: 14 [3400/13482]\tLoss: 1465.3062\n",
      "Training Epoch: 14 [3450/13482]\tLoss: 1534.1810\n",
      "Training Epoch: 14 [3500/13482]\tLoss: 1447.7059\n",
      "Training Epoch: 14 [3550/13482]\tLoss: 1504.0781\n",
      "Training Epoch: 14 [3600/13482]\tLoss: 1562.7594\n",
      "Training Epoch: 14 [3650/13482]\tLoss: 1486.1941\n",
      "Training Epoch: 14 [3700/13482]\tLoss: 1441.6382\n",
      "Training Epoch: 14 [3750/13482]\tLoss: 1536.0142\n",
      "Training Epoch: 14 [3800/13482]\tLoss: 1464.6370\n",
      "Training Epoch: 14 [3850/13482]\tLoss: 1425.0356\n",
      "Training Epoch: 14 [3900/13482]\tLoss: 1441.6849\n",
      "Training Epoch: 14 [3950/13482]\tLoss: 1458.4603\n",
      "Training Epoch: 14 [4000/13482]\tLoss: 1450.5515\n",
      "Training Epoch: 14 [4050/13482]\tLoss: 1494.7347\n",
      "Training Epoch: 14 [4100/13482]\tLoss: 1545.6655\n",
      "Training Epoch: 14 [4150/13482]\tLoss: 1477.7330\n",
      "Training Epoch: 14 [4200/13482]\tLoss: 1476.1010\n",
      "Training Epoch: 14 [4250/13482]\tLoss: 1501.6057\n",
      "Training Epoch: 14 [4300/13482]\tLoss: 1492.1843\n",
      "Training Epoch: 14 [4350/13482]\tLoss: 1465.3824\n",
      "Training Epoch: 14 [4400/13482]\tLoss: 1559.6548\n",
      "Training Epoch: 14 [4450/13482]\tLoss: 1491.7679\n",
      "Training Epoch: 14 [4500/13482]\tLoss: 1512.9164\n",
      "Training Epoch: 14 [4550/13482]\tLoss: 1444.9240\n",
      "Training Epoch: 14 [4600/13482]\tLoss: 1406.3531\n",
      "Training Epoch: 14 [4650/13482]\tLoss: 1509.4478\n",
      "Training Epoch: 14 [4700/13482]\tLoss: 1460.4232\n",
      "Training Epoch: 14 [4750/13482]\tLoss: 1424.2979\n",
      "Training Epoch: 14 [4800/13482]\tLoss: 1496.3569\n",
      "Training Epoch: 14 [4850/13482]\tLoss: 1504.4159\n",
      "Training Epoch: 14 [4900/13482]\tLoss: 1502.7393\n",
      "Training Epoch: 14 [4950/13482]\tLoss: 1489.3700\n",
      "Training Epoch: 14 [5000/13482]\tLoss: 1484.6399\n",
      "Training Epoch: 14 [5050/13482]\tLoss: 1477.1239\n",
      "Training Epoch: 14 [5100/13482]\tLoss: 1462.6534\n",
      "Training Epoch: 14 [5150/13482]\tLoss: 1496.1642\n",
      "Training Epoch: 14 [5200/13482]\tLoss: 1511.0297\n",
      "Training Epoch: 14 [5250/13482]\tLoss: 1482.9994\n",
      "Training Epoch: 14 [5300/13482]\tLoss: 1481.3645\n",
      "Training Epoch: 14 [5350/13482]\tLoss: 1429.1147\n",
      "Training Epoch: 14 [5400/13482]\tLoss: 1476.3228\n",
      "Training Epoch: 14 [5450/13482]\tLoss: 1474.9009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [5500/13482]\tLoss: 1510.0514\n",
      "Training Epoch: 14 [5550/13482]\tLoss: 1494.7201\n",
      "Training Epoch: 14 [5600/13482]\tLoss: 1471.5193\n",
      "Training Epoch: 14 [5650/13482]\tLoss: 1462.5646\n",
      "Training Epoch: 14 [5700/13482]\tLoss: 1484.2953\n",
      "Training Epoch: 14 [5750/13482]\tLoss: 1515.9486\n",
      "Training Epoch: 14 [5800/13482]\tLoss: 1524.1588\n",
      "Training Epoch: 14 [5850/13482]\tLoss: 1567.0033\n",
      "Training Epoch: 14 [5900/13482]\tLoss: 1480.7257\n",
      "Training Epoch: 14 [5950/13482]\tLoss: 1442.6595\n",
      "Training Epoch: 14 [6000/13482]\tLoss: 1477.1831\n",
      "Training Epoch: 14 [6050/13482]\tLoss: 1491.3770\n",
      "Training Epoch: 14 [6100/13482]\tLoss: 1522.6396\n",
      "Training Epoch: 14 [6150/13482]\tLoss: 1523.9301\n",
      "Training Epoch: 14 [6200/13482]\tLoss: 1547.1469\n",
      "Training Epoch: 14 [6250/13482]\tLoss: 1446.4478\n",
      "Training Epoch: 14 [6300/13482]\tLoss: 1456.6877\n",
      "Training Epoch: 14 [6350/13482]\tLoss: 1454.3431\n",
      "Training Epoch: 14 [6400/13482]\tLoss: 1525.1084\n",
      "Training Epoch: 14 [6450/13482]\tLoss: 1494.5992\n",
      "Training Epoch: 14 [6500/13482]\tLoss: 1464.6075\n",
      "Training Epoch: 14 [6550/13482]\tLoss: 1549.5773\n",
      "Training Epoch: 14 [6600/13482]\tLoss: 1480.1515\n",
      "Training Epoch: 14 [6650/13482]\tLoss: 1472.3829\n",
      "Training Epoch: 14 [6700/13482]\tLoss: 1454.7203\n",
      "Training Epoch: 14 [6750/13482]\tLoss: 1478.3644\n",
      "Training Epoch: 14 [6800/13482]\tLoss: 1474.1234\n",
      "Training Epoch: 14 [6850/13482]\tLoss: 1466.6901\n",
      "Training Epoch: 14 [6900/13482]\tLoss: 1486.9728\n",
      "Training Epoch: 14 [6950/13482]\tLoss: 1440.0981\n",
      "Training Epoch: 14 [7000/13482]\tLoss: 1487.9110\n",
      "Training Epoch: 14 [7050/13482]\tLoss: 1486.2344\n",
      "Training Epoch: 14 [7100/13482]\tLoss: 1497.6755\n",
      "Training Epoch: 14 [7150/13482]\tLoss: 1512.8004\n",
      "Training Epoch: 14 [7200/13482]\tLoss: 1477.5730\n",
      "Training Epoch: 14 [7250/13482]\tLoss: 1457.5354\n",
      "Training Epoch: 14 [7300/13482]\tLoss: 1503.6733\n",
      "Training Epoch: 14 [7350/13482]\tLoss: 1464.3698\n",
      "Training Epoch: 14 [7400/13482]\tLoss: 1520.2985\n",
      "Training Epoch: 14 [7450/13482]\tLoss: 1475.9915\n",
      "Training Epoch: 14 [7500/13482]\tLoss: 1428.1610\n",
      "Training Epoch: 14 [7550/13482]\tLoss: 1531.0243\n",
      "Training Epoch: 14 [7600/13482]\tLoss: 1491.7338\n",
      "Training Epoch: 14 [7650/13482]\tLoss: 1525.9917\n",
      "Training Epoch: 14 [7700/13482]\tLoss: 1453.8561\n",
      "Training Epoch: 14 [7750/13482]\tLoss: 1431.1193\n",
      "Training Epoch: 14 [7800/13482]\tLoss: 1528.5146\n",
      "Training Epoch: 14 [7850/13482]\tLoss: 1525.9546\n",
      "Training Epoch: 14 [7900/13482]\tLoss: 1472.1631\n",
      "Training Epoch: 14 [7950/13482]\tLoss: 1517.7574\n",
      "Training Epoch: 14 [8000/13482]\tLoss: 1460.2174\n",
      "Training Epoch: 14 [8050/13482]\tLoss: 1521.1166\n",
      "Training Epoch: 14 [8100/13482]\tLoss: 1488.6437\n",
      "Training Epoch: 14 [8150/13482]\tLoss: 1500.3268\n",
      "Training Epoch: 14 [8200/13482]\tLoss: 1515.2194\n",
      "Training Epoch: 14 [8250/13482]\tLoss: 1423.3279\n",
      "Training Epoch: 14 [8300/13482]\tLoss: 1458.1033\n",
      "Training Epoch: 14 [8350/13482]\tLoss: 1504.9757\n",
      "Training Epoch: 14 [8400/13482]\tLoss: 1515.2271\n",
      "Training Epoch: 14 [8450/13482]\tLoss: 1452.5928\n",
      "Training Epoch: 14 [8500/13482]\tLoss: 1483.7561\n",
      "Training Epoch: 14 [8550/13482]\tLoss: 1466.2158\n",
      "Training Epoch: 14 [8600/13482]\tLoss: 1525.8342\n",
      "Training Epoch: 14 [8650/13482]\tLoss: 1511.2112\n",
      "Training Epoch: 14 [8700/13482]\tLoss: 1494.5138\n",
      "Training Epoch: 14 [8750/13482]\tLoss: 1471.7570\n",
      "Training Epoch: 14 [8800/13482]\tLoss: 1439.2137\n",
      "Training Epoch: 14 [8850/13482]\tLoss: 1490.9441\n",
      "Training Epoch: 14 [8900/13482]\tLoss: 1494.2344\n",
      "Training Epoch: 14 [8950/13482]\tLoss: 1450.5961\n",
      "Training Epoch: 14 [9000/13482]\tLoss: 1502.3435\n",
      "Training Epoch: 14 [9050/13482]\tLoss: 1497.0297\n",
      "Training Epoch: 14 [9100/13482]\tLoss: 1461.5945\n",
      "Training Epoch: 14 [9150/13482]\tLoss: 1505.0935\n",
      "Training Epoch: 14 [9200/13482]\tLoss: 1556.2009\n",
      "Training Epoch: 14 [9250/13482]\tLoss: 1436.0651\n",
      "Training Epoch: 14 [9300/13482]\tLoss: 1511.5918\n",
      "Training Epoch: 14 [9350/13482]\tLoss: 1470.7346\n",
      "Training Epoch: 14 [9400/13482]\tLoss: 1424.8619\n",
      "Training Epoch: 14 [9450/13482]\tLoss: 1504.0964\n",
      "Training Epoch: 14 [9500/13482]\tLoss: 1471.1893\n",
      "Training Epoch: 14 [9550/13482]\tLoss: 1473.5444\n",
      "Training Epoch: 14 [9600/13482]\tLoss: 1484.0005\n",
      "Training Epoch: 14 [9650/13482]\tLoss: 1461.6460\n",
      "Training Epoch: 14 [9700/13482]\tLoss: 1469.5490\n",
      "Training Epoch: 14 [9750/13482]\tLoss: 1461.3723\n",
      "Training Epoch: 14 [9800/13482]\tLoss: 1437.2997\n",
      "Training Epoch: 14 [9850/13482]\tLoss: 1434.5625\n",
      "Training Epoch: 14 [9900/13482]\tLoss: 1504.6133\n",
      "Training Epoch: 14 [9950/13482]\tLoss: 1475.4895\n",
      "Training Epoch: 14 [10000/13482]\tLoss: 1461.5120\n",
      "Training Epoch: 14 [10050/13482]\tLoss: 1522.5193\n",
      "Training Epoch: 14 [10100/13482]\tLoss: 1486.5524\n",
      "Training Epoch: 14 [10150/13482]\tLoss: 1479.7660\n",
      "Training Epoch: 14 [10200/13482]\tLoss: 1446.7798\n",
      "Training Epoch: 14 [10250/13482]\tLoss: 1524.1038\n",
      "Training Epoch: 14 [10300/13482]\tLoss: 1452.2505\n",
      "Training Epoch: 14 [10350/13482]\tLoss: 1478.5719\n",
      "Training Epoch: 14 [10400/13482]\tLoss: 1447.8855\n",
      "Training Epoch: 14 [10450/13482]\tLoss: 1480.7562\n",
      "Training Epoch: 14 [10500/13482]\tLoss: 1536.6019\n",
      "Training Epoch: 14 [10550/13482]\tLoss: 1499.2567\n",
      "Training Epoch: 14 [10600/13482]\tLoss: 1480.5474\n",
      "Training Epoch: 14 [10650/13482]\tLoss: 1455.4441\n",
      "Training Epoch: 14 [10700/13482]\tLoss: 1471.3322\n",
      "Training Epoch: 14 [10750/13482]\tLoss: 1450.5974\n",
      "Training Epoch: 14 [10800/13482]\tLoss: 1453.0634\n",
      "Training Epoch: 14 [10850/13482]\tLoss: 1476.0496\n",
      "Training Epoch: 14 [10900/13482]\tLoss: 1446.7208\n",
      "Training Epoch: 14 [10950/13482]\tLoss: 1454.3751\n",
      "Training Epoch: 14 [11000/13482]\tLoss: 1475.8818\n",
      "Training Epoch: 14 [11050/13482]\tLoss: 1482.1213\n",
      "Training Epoch: 14 [11100/13482]\tLoss: 1482.9193\n",
      "Training Epoch: 14 [11150/13482]\tLoss: 1473.5923\n",
      "Training Epoch: 14 [11200/13482]\tLoss: 1445.0198\n",
      "Training Epoch: 14 [11250/13482]\tLoss: 1506.8538\n",
      "Training Epoch: 14 [11300/13482]\tLoss: 1477.3152\n",
      "Training Epoch: 14 [11350/13482]\tLoss: 1532.2069\n",
      "Training Epoch: 14 [11400/13482]\tLoss: 1481.8391\n",
      "Training Epoch: 14 [11450/13482]\tLoss: 1455.2260\n",
      "Training Epoch: 14 [11500/13482]\tLoss: 1475.6875\n",
      "Training Epoch: 14 [11550/13482]\tLoss: 1515.5280\n",
      "Training Epoch: 14 [11600/13482]\tLoss: 1463.9336\n",
      "Training Epoch: 14 [11650/13482]\tLoss: 1497.5070\n",
      "Training Epoch: 14 [11700/13482]\tLoss: 1450.9556\n",
      "Training Epoch: 14 [11750/13482]\tLoss: 1448.4225\n",
      "Training Epoch: 14 [11800/13482]\tLoss: 1503.1810\n",
      "Training Epoch: 14 [11850/13482]\tLoss: 1433.3512\n",
      "Training Epoch: 14 [11900/13482]\tLoss: 1449.3427\n",
      "Training Epoch: 14 [11950/13482]\tLoss: 1547.1483\n",
      "Training Epoch: 14 [12000/13482]\tLoss: 1425.0176\n",
      "Training Epoch: 14 [12050/13482]\tLoss: 1418.0052\n",
      "Training Epoch: 14 [12100/13482]\tLoss: 1495.1823\n",
      "Training Epoch: 14 [12150/13482]\tLoss: 1454.0342\n",
      "Training Epoch: 14 [12200/13482]\tLoss: 1468.8246\n",
      "Training Epoch: 14 [12250/13482]\tLoss: 1447.2614\n",
      "Training Epoch: 14 [12300/13482]\tLoss: 1501.8325\n",
      "Training Epoch: 14 [12350/13482]\tLoss: 1454.5688\n",
      "Training Epoch: 14 [12400/13482]\tLoss: 1474.5006\n",
      "Training Epoch: 14 [12450/13482]\tLoss: 1441.9517\n",
      "Training Epoch: 14 [12500/13482]\tLoss: 1494.9841\n",
      "Training Epoch: 14 [12550/13482]\tLoss: 1480.9923\n",
      "Training Epoch: 14 [12600/13482]\tLoss: 1481.1886\n",
      "Training Epoch: 14 [12650/13482]\tLoss: 1467.2291\n",
      "Training Epoch: 14 [12700/13482]\tLoss: 1431.1545\n",
      "Training Epoch: 14 [12750/13482]\tLoss: 1473.5325\n",
      "Training Epoch: 14 [12800/13482]\tLoss: 1456.5912\n",
      "Training Epoch: 14 [12850/13482]\tLoss: 1504.3578\n",
      "Training Epoch: 14 [12900/13482]\tLoss: 1503.0792\n",
      "Training Epoch: 14 [12950/13482]\tLoss: 1445.8114\n",
      "Training Epoch: 14 [13000/13482]\tLoss: 1499.4447\n",
      "Training Epoch: 14 [13050/13482]\tLoss: 1495.8324\n",
      "Training Epoch: 14 [13100/13482]\tLoss: 1442.2180\n",
      "Training Epoch: 14 [13150/13482]\tLoss: 1466.6631\n",
      "Training Epoch: 14 [13200/13482]\tLoss: 1472.5458\n",
      "Training Epoch: 14 [13250/13482]\tLoss: 1485.5898\n",
      "Training Epoch: 14 [13300/13482]\tLoss: 1462.5724\n",
      "Training Epoch: 14 [13350/13482]\tLoss: 1421.8250\n",
      "Training Epoch: 14 [13400/13482]\tLoss: 1472.4534\n",
      "Training Epoch: 14 [13450/13482]\tLoss: 1510.6708\n",
      "Training Epoch: 14 [13482/13482]\tLoss: 1489.6501\n",
      "Training Epoch: 14 [1497/1497]\tLoss: 1458.8002\n",
      "Training Epoch: 15 [50/13482]\tLoss: 1490.5289\n",
      "Training Epoch: 15 [100/13482]\tLoss: 1477.4583\n",
      "Training Epoch: 15 [150/13482]\tLoss: 1479.8193\n",
      "Training Epoch: 15 [200/13482]\tLoss: 1473.7104\n",
      "Training Epoch: 15 [250/13482]\tLoss: 1479.1259\n",
      "Training Epoch: 15 [300/13482]\tLoss: 1457.0779\n",
      "Training Epoch: 15 [350/13482]\tLoss: 1443.3109\n",
      "Training Epoch: 15 [400/13482]\tLoss: 1466.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [450/13482]\tLoss: 1464.5565\n",
      "Training Epoch: 15 [500/13482]\tLoss: 1480.7336\n",
      "Training Epoch: 15 [550/13482]\tLoss: 1485.9802\n",
      "Training Epoch: 15 [600/13482]\tLoss: 1480.5686\n",
      "Training Epoch: 15 [650/13482]\tLoss: 1493.5890\n",
      "Training Epoch: 15 [700/13482]\tLoss: 1521.6321\n",
      "Training Epoch: 15 [750/13482]\tLoss: 1472.7831\n",
      "Training Epoch: 15 [800/13482]\tLoss: 1483.4192\n",
      "Training Epoch: 15 [850/13482]\tLoss: 1455.8435\n",
      "Training Epoch: 15 [900/13482]\tLoss: 1448.0127\n",
      "Training Epoch: 15 [950/13482]\tLoss: 1492.8282\n",
      "Training Epoch: 15 [1000/13482]\tLoss: 1504.2201\n",
      "Training Epoch: 15 [1050/13482]\tLoss: 1442.8646\n",
      "Training Epoch: 15 [1100/13482]\tLoss: 1485.0308\n",
      "Training Epoch: 15 [1150/13482]\tLoss: 1477.8347\n",
      "Training Epoch: 15 [1200/13482]\tLoss: 1479.4545\n",
      "Training Epoch: 15 [1250/13482]\tLoss: 1456.2241\n",
      "Training Epoch: 15 [1300/13482]\tLoss: 1464.2042\n",
      "Training Epoch: 15 [1350/13482]\tLoss: 1440.9965\n",
      "Training Epoch: 15 [1400/13482]\tLoss: 1452.5767\n",
      "Training Epoch: 15 [1450/13482]\tLoss: 1438.2526\n",
      "Training Epoch: 15 [1500/13482]\tLoss: 1477.7152\n",
      "Training Epoch: 15 [1550/13482]\tLoss: 1430.7366\n",
      "Training Epoch: 15 [1600/13482]\tLoss: 1423.3530\n",
      "Training Epoch: 15 [1650/13482]\tLoss: 1487.8561\n",
      "Training Epoch: 15 [1700/13482]\tLoss: 1490.7455\n",
      "Training Epoch: 15 [1750/13482]\tLoss: 1415.1908\n",
      "Training Epoch: 15 [1800/13482]\tLoss: 1455.3671\n",
      "Training Epoch: 15 [1850/13482]\tLoss: 1483.1489\n",
      "Training Epoch: 15 [1900/13482]\tLoss: 1445.2078\n",
      "Training Epoch: 15 [1950/13482]\tLoss: 1454.8280\n",
      "Training Epoch: 15 [2000/13482]\tLoss: 1472.3871\n",
      "Training Epoch: 15 [2050/13482]\tLoss: 1439.6823\n",
      "Training Epoch: 15 [2100/13482]\tLoss: 1472.1368\n",
      "Training Epoch: 15 [2150/13482]\tLoss: 1494.4497\n",
      "Training Epoch: 15 [2200/13482]\tLoss: 1414.6609\n",
      "Training Epoch: 15 [2250/13482]\tLoss: 1497.6553\n",
      "Training Epoch: 15 [2300/13482]\tLoss: 1455.7383\n",
      "Training Epoch: 15 [2350/13482]\tLoss: 1418.3640\n",
      "Training Epoch: 15 [2400/13482]\tLoss: 1420.3259\n",
      "Training Epoch: 15 [2450/13482]\tLoss: 1427.9722\n",
      "Training Epoch: 15 [2500/13482]\tLoss: 1436.7588\n",
      "Training Epoch: 15 [2550/13482]\tLoss: 1433.7424\n",
      "Training Epoch: 15 [2600/13482]\tLoss: 1447.5449\n",
      "Training Epoch: 15 [2650/13482]\tLoss: 1469.0052\n",
      "Training Epoch: 15 [2700/13482]\tLoss: 1459.6849\n",
      "Training Epoch: 15 [2750/13482]\tLoss: 1462.8668\n",
      "Training Epoch: 15 [2800/13482]\tLoss: 1472.8602\n",
      "Training Epoch: 15 [2850/13482]\tLoss: 1476.6875\n",
      "Training Epoch: 15 [2900/13482]\tLoss: 1511.3160\n",
      "Training Epoch: 15 [2950/13482]\tLoss: 1450.7590\n",
      "Training Epoch: 15 [3000/13482]\tLoss: 1398.6825\n",
      "Training Epoch: 15 [3050/13482]\tLoss: 1469.4800\n",
      "Training Epoch: 15 [3100/13482]\tLoss: 1424.9210\n",
      "Training Epoch: 15 [3150/13482]\tLoss: 1424.3477\n",
      "Training Epoch: 15 [3200/13482]\tLoss: 1427.9655\n",
      "Training Epoch: 15 [3250/13482]\tLoss: 1475.0886\n",
      "Training Epoch: 15 [3300/13482]\tLoss: 1491.4930\n",
      "Training Epoch: 15 [3350/13482]\tLoss: 1492.3256\n",
      "Training Epoch: 15 [3400/13482]\tLoss: 1433.8599\n",
      "Training Epoch: 15 [3450/13482]\tLoss: 1502.2163\n",
      "Training Epoch: 15 [3500/13482]\tLoss: 1415.8119\n",
      "Training Epoch: 15 [3550/13482]\tLoss: 1472.9674\n",
      "Training Epoch: 15 [3600/13482]\tLoss: 1529.9680\n",
      "Training Epoch: 15 [3650/13482]\tLoss: 1454.5421\n",
      "Training Epoch: 15 [3700/13482]\tLoss: 1411.2532\n",
      "Training Epoch: 15 [3750/13482]\tLoss: 1503.9476\n",
      "Training Epoch: 15 [3800/13482]\tLoss: 1432.2559\n",
      "Training Epoch: 15 [3850/13482]\tLoss: 1392.7306\n",
      "Training Epoch: 15 [3900/13482]\tLoss: 1408.3054\n",
      "Training Epoch: 15 [3950/13482]\tLoss: 1426.8241\n",
      "Training Epoch: 15 [4000/13482]\tLoss: 1417.7017\n",
      "Training Epoch: 15 [4050/13482]\tLoss: 1465.2123\n",
      "Training Epoch: 15 [4100/13482]\tLoss: 1515.6647\n",
      "Training Epoch: 15 [4150/13482]\tLoss: 1445.6768\n",
      "Training Epoch: 15 [4200/13482]\tLoss: 1444.5757\n",
      "Training Epoch: 15 [4250/13482]\tLoss: 1469.0037\n",
      "Training Epoch: 15 [4300/13482]\tLoss: 1460.3159\n",
      "Training Epoch: 15 [4350/13482]\tLoss: 1435.9253\n",
      "Training Epoch: 15 [4400/13482]\tLoss: 1529.8164\n",
      "Training Epoch: 15 [4450/13482]\tLoss: 1460.8990\n",
      "Training Epoch: 15 [4500/13482]\tLoss: 1482.1732\n",
      "Training Epoch: 15 [4550/13482]\tLoss: 1415.4482\n",
      "Training Epoch: 15 [4600/13482]\tLoss: 1376.1534\n",
      "Training Epoch: 15 [4650/13482]\tLoss: 1478.0491\n",
      "Training Epoch: 15 [4700/13482]\tLoss: 1429.2550\n",
      "Training Epoch: 15 [4750/13482]\tLoss: 1392.4779\n",
      "Training Epoch: 15 [4800/13482]\tLoss: 1464.9471\n",
      "Training Epoch: 15 [4850/13482]\tLoss: 1474.1891\n",
      "Training Epoch: 15 [4900/13482]\tLoss: 1471.7772\n",
      "Training Epoch: 15 [4950/13482]\tLoss: 1458.1334\n",
      "Training Epoch: 15 [5000/13482]\tLoss: 1451.5747\n",
      "Training Epoch: 15 [5050/13482]\tLoss: 1444.9620\n",
      "Training Epoch: 15 [5100/13482]\tLoss: 1432.4895\n",
      "Training Epoch: 15 [5150/13482]\tLoss: 1467.3956\n",
      "Training Epoch: 15 [5200/13482]\tLoss: 1480.2909\n",
      "Training Epoch: 15 [5250/13482]\tLoss: 1451.2386\n",
      "Training Epoch: 15 [5300/13482]\tLoss: 1450.4745\n",
      "Training Epoch: 15 [5350/13482]\tLoss: 1398.7701\n",
      "Training Epoch: 15 [5400/13482]\tLoss: 1444.3737\n",
      "Training Epoch: 15 [5450/13482]\tLoss: 1443.5182\n",
      "Training Epoch: 15 [5500/13482]\tLoss: 1478.3453\n",
      "Training Epoch: 15 [5550/13482]\tLoss: 1463.3516\n",
      "Training Epoch: 15 [5600/13482]\tLoss: 1441.9187\n",
      "Training Epoch: 15 [5650/13482]\tLoss: 1431.1439\n",
      "Training Epoch: 15 [5700/13482]\tLoss: 1452.8262\n",
      "Training Epoch: 15 [5750/13482]\tLoss: 1483.3212\n",
      "Training Epoch: 15 [5800/13482]\tLoss: 1492.2524\n",
      "Training Epoch: 15 [5850/13482]\tLoss: 1531.6584\n",
      "Training Epoch: 15 [5900/13482]\tLoss: 1449.4874\n",
      "Training Epoch: 15 [5950/13482]\tLoss: 1411.1771\n",
      "Training Epoch: 15 [6000/13482]\tLoss: 1446.3843\n",
      "Training Epoch: 15 [6050/13482]\tLoss: 1460.6682\n",
      "Training Epoch: 15 [6100/13482]\tLoss: 1491.1742\n",
      "Training Epoch: 15 [6150/13482]\tLoss: 1491.6133\n",
      "Training Epoch: 15 [6200/13482]\tLoss: 1513.9165\n",
      "Training Epoch: 15 [6250/13482]\tLoss: 1414.9269\n",
      "Training Epoch: 15 [6300/13482]\tLoss: 1428.1659\n",
      "Training Epoch: 15 [6350/13482]\tLoss: 1421.2325\n",
      "Training Epoch: 15 [6400/13482]\tLoss: 1493.5729\n",
      "Training Epoch: 15 [6450/13482]\tLoss: 1463.7374\n",
      "Training Epoch: 15 [6500/13482]\tLoss: 1434.1007\n",
      "Training Epoch: 15 [6550/13482]\tLoss: 1517.5474\n",
      "Training Epoch: 15 [6600/13482]\tLoss: 1450.1974\n",
      "Training Epoch: 15 [6650/13482]\tLoss: 1439.3333\n",
      "Training Epoch: 15 [6700/13482]\tLoss: 1423.1285\n",
      "Training Epoch: 15 [6750/13482]\tLoss: 1447.4465\n",
      "Training Epoch: 15 [6800/13482]\tLoss: 1444.5044\n",
      "Training Epoch: 15 [6850/13482]\tLoss: 1436.0378\n",
      "Training Epoch: 15 [6900/13482]\tLoss: 1455.4161\n",
      "Training Epoch: 15 [6950/13482]\tLoss: 1408.5244\n",
      "Training Epoch: 15 [7000/13482]\tLoss: 1457.1812\n",
      "Training Epoch: 15 [7050/13482]\tLoss: 1453.7900\n",
      "Training Epoch: 15 [7100/13482]\tLoss: 1465.8137\n",
      "Training Epoch: 15 [7150/13482]\tLoss: 1480.8580\n",
      "Training Epoch: 15 [7200/13482]\tLoss: 1446.3881\n",
      "Training Epoch: 15 [7250/13482]\tLoss: 1425.6562\n",
      "Training Epoch: 15 [7300/13482]\tLoss: 1472.8489\n",
      "Training Epoch: 15 [7350/13482]\tLoss: 1434.3153\n",
      "Training Epoch: 15 [7400/13482]\tLoss: 1488.7076\n",
      "Training Epoch: 15 [7450/13482]\tLoss: 1444.7505\n",
      "Training Epoch: 15 [7500/13482]\tLoss: 1398.1193\n",
      "Training Epoch: 15 [7550/13482]\tLoss: 1499.4254\n",
      "Training Epoch: 15 [7600/13482]\tLoss: 1458.7343\n",
      "Training Epoch: 15 [7650/13482]\tLoss: 1493.8708\n",
      "Training Epoch: 15 [7700/13482]\tLoss: 1423.0958\n",
      "Training Epoch: 15 [7750/13482]\tLoss: 1399.7594\n",
      "Training Epoch: 15 [7800/13482]\tLoss: 1495.9935\n",
      "Training Epoch: 15 [7850/13482]\tLoss: 1493.5031\n",
      "Training Epoch: 15 [7900/13482]\tLoss: 1440.8926\n",
      "Training Epoch: 15 [7950/13482]\tLoss: 1486.1466\n",
      "Training Epoch: 15 [8000/13482]\tLoss: 1430.7107\n",
      "Training Epoch: 15 [8050/13482]\tLoss: 1488.7056\n",
      "Training Epoch: 15 [8100/13482]\tLoss: 1456.9202\n",
      "Training Epoch: 15 [8150/13482]\tLoss: 1468.4174\n",
      "Training Epoch: 15 [8200/13482]\tLoss: 1484.5822\n",
      "Training Epoch: 15 [8250/13482]\tLoss: 1393.0651\n",
      "Training Epoch: 15 [8300/13482]\tLoss: 1426.5908\n",
      "Training Epoch: 15 [8350/13482]\tLoss: 1474.0192\n",
      "Training Epoch: 15 [8400/13482]\tLoss: 1483.5133\n",
      "Training Epoch: 15 [8450/13482]\tLoss: 1421.9852\n",
      "Training Epoch: 15 [8500/13482]\tLoss: 1452.5680\n",
      "Training Epoch: 15 [8550/13482]\tLoss: 1434.7147\n",
      "Training Epoch: 15 [8600/13482]\tLoss: 1495.1100\n",
      "Training Epoch: 15 [8650/13482]\tLoss: 1479.9563\n",
      "Training Epoch: 15 [8700/13482]\tLoss: 1464.5826\n",
      "Training Epoch: 15 [8750/13482]\tLoss: 1440.1188\n",
      "Training Epoch: 15 [8800/13482]\tLoss: 1406.1241\n",
      "Training Epoch: 15 [8850/13482]\tLoss: 1460.2003\n",
      "Training Epoch: 15 [8900/13482]\tLoss: 1463.7051\n",
      "Training Epoch: 15 [8950/13482]\tLoss: 1419.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [9000/13482]\tLoss: 1471.3302\n",
      "Training Epoch: 15 [9050/13482]\tLoss: 1465.8549\n",
      "Training Epoch: 15 [9100/13482]\tLoss: 1431.7549\n",
      "Training Epoch: 15 [9150/13482]\tLoss: 1473.5150\n",
      "Training Epoch: 15 [9200/13482]\tLoss: 1524.1969\n",
      "Training Epoch: 15 [9250/13482]\tLoss: 1405.6431\n",
      "Training Epoch: 15 [9300/13482]\tLoss: 1479.5132\n",
      "Training Epoch: 15 [9350/13482]\tLoss: 1438.6783\n",
      "Training Epoch: 15 [9400/13482]\tLoss: 1395.0214\n",
      "Training Epoch: 15 [9450/13482]\tLoss: 1471.7256\n",
      "Training Epoch: 15 [9500/13482]\tLoss: 1440.6649\n",
      "Training Epoch: 15 [9550/13482]\tLoss: 1441.5099\n",
      "Training Epoch: 15 [9600/13482]\tLoss: 1451.0543\n",
      "Training Epoch: 15 [9650/13482]\tLoss: 1430.1674\n",
      "Training Epoch: 15 [9700/13482]\tLoss: 1437.4922\n",
      "Training Epoch: 15 [9750/13482]\tLoss: 1429.6451\n",
      "Training Epoch: 15 [9800/13482]\tLoss: 1406.6462\n",
      "Training Epoch: 15 [9850/13482]\tLoss: 1403.5936\n",
      "Training Epoch: 15 [9900/13482]\tLoss: 1473.9526\n",
      "Training Epoch: 15 [9950/13482]\tLoss: 1442.5228\n",
      "Training Epoch: 15 [10000/13482]\tLoss: 1430.4868\n",
      "Training Epoch: 15 [10050/13482]\tLoss: 1490.7080\n",
      "Training Epoch: 15 [10100/13482]\tLoss: 1456.5680\n",
      "Training Epoch: 15 [10150/13482]\tLoss: 1448.6974\n",
      "Training Epoch: 15 [10200/13482]\tLoss: 1415.7954\n",
      "Training Epoch: 15 [10250/13482]\tLoss: 1491.7400\n",
      "Training Epoch: 15 [10300/13482]\tLoss: 1419.7944\n",
      "Training Epoch: 15 [10350/13482]\tLoss: 1447.6621\n",
      "Training Epoch: 15 [10400/13482]\tLoss: 1418.1658\n",
      "Training Epoch: 15 [10450/13482]\tLoss: 1449.2660\n",
      "Training Epoch: 15 [10500/13482]\tLoss: 1503.6239\n",
      "Training Epoch: 15 [10550/13482]\tLoss: 1467.1100\n",
      "Training Epoch: 15 [10600/13482]\tLoss: 1448.4259\n",
      "Training Epoch: 15 [10650/13482]\tLoss: 1425.7620\n",
      "Training Epoch: 15 [10700/13482]\tLoss: 1442.0098\n",
      "Training Epoch: 15 [10750/13482]\tLoss: 1418.4980\n",
      "Training Epoch: 15 [10800/13482]\tLoss: 1421.2440\n",
      "Training Epoch: 15 [10850/13482]\tLoss: 1445.5625\n",
      "Training Epoch: 15 [10900/13482]\tLoss: 1416.1082\n",
      "Training Epoch: 15 [10950/13482]\tLoss: 1424.5374\n",
      "Training Epoch: 15 [11000/13482]\tLoss: 1445.6238\n",
      "Training Epoch: 15 [11050/13482]\tLoss: 1449.8784\n",
      "Training Epoch: 15 [11100/13482]\tLoss: 1451.4032\n",
      "Training Epoch: 15 [11150/13482]\tLoss: 1443.9799\n",
      "Training Epoch: 15 [11200/13482]\tLoss: 1415.3271\n",
      "Training Epoch: 15 [11250/13482]\tLoss: 1474.4619\n",
      "Training Epoch: 15 [11300/13482]\tLoss: 1445.7957\n",
      "Training Epoch: 15 [11350/13482]\tLoss: 1501.3668\n",
      "Training Epoch: 15 [11400/13482]\tLoss: 1451.0463\n",
      "Training Epoch: 15 [11450/13482]\tLoss: 1426.7653\n",
      "Training Epoch: 15 [11500/13482]\tLoss: 1444.6003\n",
      "Training Epoch: 15 [11550/13482]\tLoss: 1483.0875\n",
      "Training Epoch: 15 [11600/13482]\tLoss: 1431.4714\n",
      "Training Epoch: 15 [11650/13482]\tLoss: 1467.4546\n",
      "Training Epoch: 15 [11700/13482]\tLoss: 1421.8429\n",
      "Training Epoch: 15 [11750/13482]\tLoss: 1418.8910\n",
      "Training Epoch: 15 [11800/13482]\tLoss: 1470.6564\n",
      "Training Epoch: 15 [11850/13482]\tLoss: 1403.2533\n",
      "Training Epoch: 15 [11900/13482]\tLoss: 1419.5918\n",
      "Training Epoch: 15 [11950/13482]\tLoss: 1520.2931\n",
      "Training Epoch: 15 [12000/13482]\tLoss: 1395.0809\n",
      "Training Epoch: 15 [12050/13482]\tLoss: 1386.3605\n",
      "Training Epoch: 15 [12100/13482]\tLoss: 1461.8411\n",
      "Training Epoch: 15 [12150/13482]\tLoss: 1423.0082\n",
      "Training Epoch: 15 [12200/13482]\tLoss: 1439.4547\n",
      "Training Epoch: 15 [12250/13482]\tLoss: 1417.0111\n",
      "Training Epoch: 15 [12300/13482]\tLoss: 1470.3116\n",
      "Training Epoch: 15 [12350/13482]\tLoss: 1424.1884\n",
      "Training Epoch: 15 [12400/13482]\tLoss: 1444.1654\n",
      "Training Epoch: 15 [12450/13482]\tLoss: 1412.4263\n",
      "Training Epoch: 15 [12500/13482]\tLoss: 1463.7852\n",
      "Training Epoch: 15 [12550/13482]\tLoss: 1450.4673\n",
      "Training Epoch: 15 [12600/13482]\tLoss: 1449.0610\n",
      "Training Epoch: 15 [12650/13482]\tLoss: 1437.2236\n",
      "Training Epoch: 15 [12700/13482]\tLoss: 1401.6520\n",
      "Training Epoch: 15 [12750/13482]\tLoss: 1442.8772\n",
      "Training Epoch: 15 [12800/13482]\tLoss: 1424.7759\n",
      "Training Epoch: 15 [12850/13482]\tLoss: 1474.8055\n",
      "Training Epoch: 15 [12900/13482]\tLoss: 1472.0140\n",
      "Training Epoch: 15 [12950/13482]\tLoss: 1414.6198\n",
      "Training Epoch: 15 [13000/13482]\tLoss: 1469.7327\n",
      "Training Epoch: 15 [13050/13482]\tLoss: 1462.6816\n",
      "Training Epoch: 15 [13100/13482]\tLoss: 1410.9364\n",
      "Training Epoch: 15 [13150/13482]\tLoss: 1436.8098\n",
      "Training Epoch: 15 [13200/13482]\tLoss: 1440.6284\n",
      "Training Epoch: 15 [13250/13482]\tLoss: 1454.1509\n",
      "Training Epoch: 15 [13300/13482]\tLoss: 1431.5444\n",
      "Training Epoch: 15 [13350/13482]\tLoss: 1391.3347\n",
      "Training Epoch: 15 [13400/13482]\tLoss: 1439.1024\n",
      "Training Epoch: 15 [13450/13482]\tLoss: 1479.8541\n",
      "Training Epoch: 15 [13482/13482]\tLoss: 1458.2850\n",
      "Training Epoch: 15 [1497/1497]\tLoss: 1427.5548\n",
      "Training Epoch: 16 [50/13482]\tLoss: 1460.2458\n",
      "Training Epoch: 16 [100/13482]\tLoss: 1445.9757\n",
      "Training Epoch: 16 [150/13482]\tLoss: 1449.6670\n",
      "Training Epoch: 16 [200/13482]\tLoss: 1441.4379\n",
      "Training Epoch: 16 [250/13482]\tLoss: 1448.3347\n",
      "Training Epoch: 16 [300/13482]\tLoss: 1425.8717\n",
      "Training Epoch: 16 [350/13482]\tLoss: 1412.0984\n",
      "Training Epoch: 16 [400/13482]\tLoss: 1437.6033\n",
      "Training Epoch: 16 [450/13482]\tLoss: 1434.0029\n",
      "Training Epoch: 16 [500/13482]\tLoss: 1451.3087\n",
      "Training Epoch: 16 [550/13482]\tLoss: 1454.2167\n",
      "Training Epoch: 16 [600/13482]\tLoss: 1447.4242\n",
      "Training Epoch: 16 [650/13482]\tLoss: 1462.8613\n",
      "Training Epoch: 16 [700/13482]\tLoss: 1491.3142\n",
      "Training Epoch: 16 [750/13482]\tLoss: 1442.0100\n",
      "Training Epoch: 16 [800/13482]\tLoss: 1450.1804\n",
      "Training Epoch: 16 [850/13482]\tLoss: 1424.7087\n",
      "Training Epoch: 16 [900/13482]\tLoss: 1417.8132\n",
      "Training Epoch: 16 [950/13482]\tLoss: 1461.0416\n",
      "Training Epoch: 16 [1000/13482]\tLoss: 1472.0148\n",
      "Training Epoch: 16 [1050/13482]\tLoss: 1411.8831\n",
      "Training Epoch: 16 [1100/13482]\tLoss: 1453.6470\n",
      "Training Epoch: 16 [1150/13482]\tLoss: 1446.7380\n",
      "Training Epoch: 16 [1200/13482]\tLoss: 1447.8953\n",
      "Training Epoch: 16 [1250/13482]\tLoss: 1425.0924\n",
      "Training Epoch: 16 [1300/13482]\tLoss: 1434.3362\n",
      "Training Epoch: 16 [1350/13482]\tLoss: 1410.1255\n",
      "Training Epoch: 16 [1400/13482]\tLoss: 1420.2667\n",
      "Training Epoch: 16 [1450/13482]\tLoss: 1407.1979\n",
      "Training Epoch: 16 [1500/13482]\tLoss: 1446.5597\n",
      "Training Epoch: 16 [1550/13482]\tLoss: 1399.6512\n",
      "Training Epoch: 16 [1600/13482]\tLoss: 1393.4216\n",
      "Training Epoch: 16 [1650/13482]\tLoss: 1456.7518\n",
      "Training Epoch: 16 [1700/13482]\tLoss: 1458.9181\n",
      "Training Epoch: 16 [1750/13482]\tLoss: 1384.2247\n",
      "Training Epoch: 16 [1800/13482]\tLoss: 1423.9187\n",
      "Training Epoch: 16 [1850/13482]\tLoss: 1452.0933\n",
      "Training Epoch: 16 [1900/13482]\tLoss: 1414.5405\n",
      "Training Epoch: 16 [1950/13482]\tLoss: 1422.4830\n",
      "Training Epoch: 16 [2000/13482]\tLoss: 1441.0162\n",
      "Training Epoch: 16 [2050/13482]\tLoss: 1408.9064\n",
      "Training Epoch: 16 [2100/13482]\tLoss: 1441.2969\n",
      "Training Epoch: 16 [2150/13482]\tLoss: 1463.3805\n",
      "Training Epoch: 16 [2200/13482]\tLoss: 1384.9937\n",
      "Training Epoch: 16 [2250/13482]\tLoss: 1466.9552\n",
      "Training Epoch: 16 [2300/13482]\tLoss: 1425.3171\n",
      "Training Epoch: 16 [2350/13482]\tLoss: 1388.3469\n",
      "Training Epoch: 16 [2400/13482]\tLoss: 1390.1271\n",
      "Training Epoch: 16 [2450/13482]\tLoss: 1396.7983\n",
      "Training Epoch: 16 [2500/13482]\tLoss: 1407.3208\n",
      "Training Epoch: 16 [2550/13482]\tLoss: 1404.2379\n",
      "Training Epoch: 16 [2600/13482]\tLoss: 1416.4937\n",
      "Training Epoch: 16 [2650/13482]\tLoss: 1437.8639\n",
      "Training Epoch: 16 [2700/13482]\tLoss: 1428.4485\n",
      "Training Epoch: 16 [2750/13482]\tLoss: 1433.1724\n",
      "Training Epoch: 16 [2800/13482]\tLoss: 1442.5863\n",
      "Training Epoch: 16 [2850/13482]\tLoss: 1442.7771\n",
      "Training Epoch: 16 [2900/13482]\tLoss: 1479.9673\n",
      "Training Epoch: 16 [2950/13482]\tLoss: 1420.9321\n",
      "Training Epoch: 16 [3000/13482]\tLoss: 1369.2638\n",
      "Training Epoch: 16 [3050/13482]\tLoss: 1437.9692\n",
      "Training Epoch: 16 [3100/13482]\tLoss: 1394.2383\n",
      "Training Epoch: 16 [3150/13482]\tLoss: 1393.7196\n",
      "Training Epoch: 16 [3200/13482]\tLoss: 1396.4529\n",
      "Training Epoch: 16 [3250/13482]\tLoss: 1443.6635\n",
      "Training Epoch: 16 [3300/13482]\tLoss: 1459.2239\n",
      "Training Epoch: 16 [3350/13482]\tLoss: 1460.0090\n",
      "Training Epoch: 16 [3400/13482]\tLoss: 1402.5422\n",
      "Training Epoch: 16 [3450/13482]\tLoss: 1470.8718\n",
      "Training Epoch: 16 [3500/13482]\tLoss: 1385.2407\n",
      "Training Epoch: 16 [3550/13482]\tLoss: 1441.8698\n",
      "Training Epoch: 16 [3600/13482]\tLoss: 1496.4075\n",
      "Training Epoch: 16 [3650/13482]\tLoss: 1422.8597\n",
      "Training Epoch: 16 [3700/13482]\tLoss: 1381.8429\n",
      "Training Epoch: 16 [3750/13482]\tLoss: 1472.8641\n",
      "Training Epoch: 16 [3800/13482]\tLoss: 1400.5480\n",
      "Training Epoch: 16 [3850/13482]\tLoss: 1360.4645\n",
      "Training Epoch: 16 [3900/13482]\tLoss: 1374.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [3950/13482]\tLoss: 1395.6584\n",
      "Training Epoch: 16 [4000/13482]\tLoss: 1386.3258\n",
      "Training Epoch: 16 [4050/13482]\tLoss: 1435.9570\n",
      "Training Epoch: 16 [4100/13482]\tLoss: 1484.1295\n",
      "Training Epoch: 16 [4150/13482]\tLoss: 1412.6451\n",
      "Training Epoch: 16 [4200/13482]\tLoss: 1413.6744\n",
      "Training Epoch: 16 [4250/13482]\tLoss: 1437.5920\n",
      "Training Epoch: 16 [4300/13482]\tLoss: 1429.4039\n",
      "Training Epoch: 16 [4350/13482]\tLoss: 1406.0792\n",
      "Training Epoch: 16 [4400/13482]\tLoss: 1498.0541\n",
      "Training Epoch: 16 [4450/13482]\tLoss: 1429.8444\n",
      "Training Epoch: 16 [4500/13482]\tLoss: 1452.5579\n",
      "Training Epoch: 16 [4550/13482]\tLoss: 1386.7744\n",
      "Training Epoch: 16 [4600/13482]\tLoss: 1346.2751\n",
      "Training Epoch: 16 [4650/13482]\tLoss: 1446.9124\n",
      "Training Epoch: 16 [4700/13482]\tLoss: 1397.5525\n",
      "Training Epoch: 16 [4750/13482]\tLoss: 1361.9817\n",
      "Training Epoch: 16 [4800/13482]\tLoss: 1434.5605\n",
      "Training Epoch: 16 [4850/13482]\tLoss: 1444.2896\n",
      "Training Epoch: 16 [4900/13482]\tLoss: 1440.0625\n",
      "Training Epoch: 16 [4950/13482]\tLoss: 1427.1838\n",
      "Training Epoch: 16 [5000/13482]\tLoss: 1418.7388\n",
      "Training Epoch: 16 [5050/13482]\tLoss: 1413.9292\n",
      "Training Epoch: 16 [5100/13482]\tLoss: 1402.4232\n",
      "Training Epoch: 16 [5150/13482]\tLoss: 1436.7997\n",
      "Training Epoch: 16 [5200/13482]\tLoss: 1448.7794\n",
      "Training Epoch: 16 [5250/13482]\tLoss: 1420.6583\n",
      "Training Epoch: 16 [5300/13482]\tLoss: 1420.5891\n",
      "Training Epoch: 16 [5350/13482]\tLoss: 1368.8910\n",
      "Training Epoch: 16 [5400/13482]\tLoss: 1411.6901\n",
      "Training Epoch: 16 [5450/13482]\tLoss: 1412.1428\n",
      "Training Epoch: 16 [5500/13482]\tLoss: 1447.3376\n",
      "Training Epoch: 16 [5550/13482]\tLoss: 1433.1042\n",
      "Training Epoch: 16 [5600/13482]\tLoss: 1412.5647\n",
      "Training Epoch: 16 [5650/13482]\tLoss: 1400.3584\n",
      "Training Epoch: 16 [5700/13482]\tLoss: 1422.3638\n",
      "Training Epoch: 16 [5750/13482]\tLoss: 1451.5490\n",
      "Training Epoch: 16 [5800/13482]\tLoss: 1460.6855\n",
      "Training Epoch: 16 [5850/13482]\tLoss: 1496.7467\n",
      "Training Epoch: 16 [5900/13482]\tLoss: 1418.6967\n",
      "Training Epoch: 16 [5950/13482]\tLoss: 1379.6228\n",
      "Training Epoch: 16 [6000/13482]\tLoss: 1415.7992\n",
      "Training Epoch: 16 [6050/13482]\tLoss: 1430.4779\n",
      "Training Epoch: 16 [6100/13482]\tLoss: 1459.7482\n",
      "Training Epoch: 16 [6150/13482]\tLoss: 1458.9978\n",
      "Training Epoch: 16 [6200/13482]\tLoss: 1480.8240\n",
      "Training Epoch: 16 [6250/13482]\tLoss: 1384.5439\n",
      "Training Epoch: 16 [6300/13482]\tLoss: 1400.3215\n",
      "Training Epoch: 16 [6350/13482]\tLoss: 1388.8146\n",
      "Training Epoch: 16 [6400/13482]\tLoss: 1462.1566\n",
      "Training Epoch: 16 [6450/13482]\tLoss: 1433.7139\n",
      "Training Epoch: 16 [6500/13482]\tLoss: 1404.5596\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec27ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
