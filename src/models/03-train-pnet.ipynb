{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96589b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'pnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a437651",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfdc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "lr = 1E-5\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "train_datafile = f'{engram_dir}clean_reconstruction_training_set.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9766a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 16 18:40:12 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:1D:00.0 Off |                  N/A |\r\n",
      "| 27%   26C    P8    21W / 250W |      3MiB / 11264MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d748a",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b087e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae18933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6839214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23a4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779893ac",
   "metadata": {},
   "source": [
    "# Set up train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc69b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CleanSoundsDataset(train_datafile, .9)\n",
    "test_dataset = CleanSoundsDataset(train_datafile, .9, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a14c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026ae3a",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b449fc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab3e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/36045]\tLoss: 82736.7109\n",
      "Training Epoch: 1 [100/36045]\tLoss: 78279.6328\n",
      "Training Epoch: 1 [150/36045]\tLoss: 73610.2422\n",
      "Training Epoch: 1 [200/36045]\tLoss: 69165.2031\n",
      "Training Epoch: 1 [250/36045]\tLoss: 75245.4688\n",
      "Training Epoch: 1 [300/36045]\tLoss: 73900.4688\n",
      "Training Epoch: 1 [350/36045]\tLoss: 69926.0000\n",
      "Training Epoch: 1 [400/36045]\tLoss: 66059.2812\n",
      "Training Epoch: 1 [450/36045]\tLoss: 63671.7773\n",
      "Training Epoch: 1 [500/36045]\tLoss: 60593.2617\n",
      "Training Epoch: 1 [550/36045]\tLoss: 56903.3672\n",
      "Training Epoch: 1 [600/36045]\tLoss: 51955.5898\n",
      "Training Epoch: 1 [650/36045]\tLoss: 50205.6562\n",
      "Training Epoch: 1 [700/36045]\tLoss: 45935.0391\n",
      "Training Epoch: 1 [750/36045]\tLoss: 41558.9258\n",
      "Training Epoch: 1 [800/36045]\tLoss: 39320.6914\n",
      "Training Epoch: 1 [850/36045]\tLoss: 38060.9570\n",
      "Training Epoch: 1 [900/36045]\tLoss: 33600.6523\n",
      "Training Epoch: 1 [950/36045]\tLoss: 30438.9316\n",
      "Training Epoch: 1 [1000/36045]\tLoss: 31007.9414\n",
      "Training Epoch: 1 [1050/36045]\tLoss: 30586.0371\n",
      "Training Epoch: 1 [1100/36045]\tLoss: 27856.7402\n",
      "Training Epoch: 1 [1150/36045]\tLoss: 27351.0977\n",
      "Training Epoch: 1 [1200/36045]\tLoss: 27527.1250\n",
      "Training Epoch: 1 [1250/36045]\tLoss: 27606.6289\n",
      "Training Epoch: 1 [1300/36045]\tLoss: 26269.1016\n",
      "Training Epoch: 1 [1350/36045]\tLoss: 26076.7598\n",
      "Training Epoch: 1 [1400/36045]\tLoss: 25059.0781\n",
      "Training Epoch: 1 [1450/36045]\tLoss: 24046.0820\n",
      "Training Epoch: 1 [1500/36045]\tLoss: 22603.1152\n",
      "Training Epoch: 1 [1550/36045]\tLoss: 22050.9062\n",
      "Training Epoch: 1 [1600/36045]\tLoss: 21901.5137\n",
      "Training Epoch: 1 [1650/36045]\tLoss: 20564.2559\n",
      "Training Epoch: 1 [1700/36045]\tLoss: 21215.4336\n",
      "Training Epoch: 1 [1750/36045]\tLoss: 21165.1895\n",
      "Training Epoch: 1 [1800/36045]\tLoss: 21938.7207\n",
      "Training Epoch: 1 [1850/36045]\tLoss: 21626.0469\n",
      "Training Epoch: 1 [1900/36045]\tLoss: 20336.1484\n",
      "Training Epoch: 1 [1950/36045]\tLoss: 19174.3008\n",
      "Training Epoch: 1 [2000/36045]\tLoss: 16164.7900\n",
      "Training Epoch: 1 [2050/36045]\tLoss: 15400.3438\n",
      "Training Epoch: 1 [2100/36045]\tLoss: 16501.9434\n",
      "Training Epoch: 1 [2150/36045]\tLoss: 16493.9863\n",
      "Training Epoch: 1 [2200/36045]\tLoss: 16680.7305\n",
      "Training Epoch: 1 [2250/36045]\tLoss: 15833.7754\n",
      "Training Epoch: 1 [2300/36045]\tLoss: 16073.9072\n",
      "Training Epoch: 1 [2350/36045]\tLoss: 16019.6162\n",
      "Training Epoch: 1 [2400/36045]\tLoss: 15968.8555\n",
      "Training Epoch: 1 [2450/36045]\tLoss: 17948.1172\n",
      "Training Epoch: 1 [2500/36045]\tLoss: 19082.3984\n",
      "Training Epoch: 1 [2550/36045]\tLoss: 19195.6289\n",
      "Training Epoch: 1 [2600/36045]\tLoss: 17822.1055\n",
      "Training Epoch: 1 [2650/36045]\tLoss: 19676.4453\n",
      "Training Epoch: 1 [2700/36045]\tLoss: 19682.5840\n",
      "Training Epoch: 1 [2750/36045]\tLoss: 20480.4824\n",
      "Training Epoch: 1 [2800/36045]\tLoss: 20488.7715\n",
      "Training Epoch: 1 [2850/36045]\tLoss: 17917.9199\n",
      "Training Epoch: 1 [2900/36045]\tLoss: 17861.5723\n",
      "Training Epoch: 1 [2950/36045]\tLoss: 17827.1094\n",
      "Training Epoch: 1 [3000/36045]\tLoss: 17628.6191\n",
      "Training Epoch: 1 [3050/36045]\tLoss: 17532.4453\n",
      "Training Epoch: 1 [3100/36045]\tLoss: 16645.6816\n",
      "Training Epoch: 1 [3150/36045]\tLoss: 12912.3984\n",
      "Training Epoch: 1 [3200/36045]\tLoss: 13180.5869\n",
      "Training Epoch: 1 [3250/36045]\tLoss: 11944.0029\n",
      "Training Epoch: 1 [3300/36045]\tLoss: 11544.2100\n",
      "Training Epoch: 1 [3350/36045]\tLoss: 12286.2090\n",
      "Training Epoch: 1 [3400/36045]\tLoss: 14298.1797\n",
      "Training Epoch: 1 [3450/36045]\tLoss: 15377.2148\n",
      "Training Epoch: 1 [3500/36045]\tLoss: 14978.7217\n",
      "Training Epoch: 1 [3550/36045]\tLoss: 15106.8281\n",
      "Training Epoch: 1 [3600/36045]\tLoss: 15728.1572\n",
      "Training Epoch: 1 [3650/36045]\tLoss: 17720.2363\n",
      "Training Epoch: 1 [3700/36045]\tLoss: 17746.5898\n",
      "Training Epoch: 1 [3750/36045]\tLoss: 17312.5840\n",
      "Training Epoch: 1 [3800/36045]\tLoss: 16558.0039\n",
      "Training Epoch: 1 [3850/36045]\tLoss: 14329.7148\n",
      "Training Epoch: 1 [3900/36045]\tLoss: 14838.2432\n",
      "Training Epoch: 1 [3950/36045]\tLoss: 14419.6074\n",
      "Training Epoch: 1 [4000/36045]\tLoss: 15005.0234\n",
      "Training Epoch: 1 [4050/36045]\tLoss: 14203.5605\n",
      "Training Epoch: 1 [4100/36045]\tLoss: 13943.3701\n",
      "Training Epoch: 1 [4150/36045]\tLoss: 14621.5186\n",
      "Training Epoch: 1 [4200/36045]\tLoss: 14944.4795\n",
      "Training Epoch: 1 [4250/36045]\tLoss: 15498.2969\n",
      "Training Epoch: 1 [4300/36045]\tLoss: 15735.1348\n",
      "Training Epoch: 1 [4350/36045]\tLoss: 15710.6338\n",
      "Training Epoch: 1 [4400/36045]\tLoss: 15167.9990\n",
      "Training Epoch: 1 [4450/36045]\tLoss: 15431.3721\n",
      "Training Epoch: 1 [4500/36045]\tLoss: 15532.4824\n",
      "Training Epoch: 1 [4550/36045]\tLoss: 15537.2236\n",
      "Training Epoch: 1 [4600/36045]\tLoss: 15544.0664\n",
      "Training Epoch: 1 [4650/36045]\tLoss: 15341.8730\n",
      "Training Epoch: 1 [4700/36045]\tLoss: 15557.1416\n",
      "Training Epoch: 1 [4750/36045]\tLoss: 15578.9014\n",
      "Training Epoch: 1 [4800/36045]\tLoss: 16271.8926\n",
      "Training Epoch: 1 [4850/36045]\tLoss: 15958.3213\n",
      "Training Epoch: 1 [4900/36045]\tLoss: 15596.7568\n",
      "Training Epoch: 1 [4950/36045]\tLoss: 16195.6465\n",
      "Training Epoch: 1 [5000/36045]\tLoss: 17768.0254\n",
      "Training Epoch: 1 [5050/36045]\tLoss: 17894.6836\n",
      "Training Epoch: 1 [5100/36045]\tLoss: 17711.4277\n",
      "Training Epoch: 1 [5150/36045]\tLoss: 17224.9824\n",
      "Training Epoch: 1 [5200/36045]\tLoss: 15124.5283\n",
      "Training Epoch: 1 [5250/36045]\tLoss: 15515.7832\n",
      "Training Epoch: 1 [5300/36045]\tLoss: 15688.7979\n",
      "Training Epoch: 1 [5350/36045]\tLoss: 15804.9902\n",
      "Training Epoch: 1 [5400/36045]\tLoss: 15118.5752\n",
      "Training Epoch: 1 [5450/36045]\tLoss: 14219.3271\n",
      "Training Epoch: 1 [5500/36045]\tLoss: 14154.2617\n",
      "Training Epoch: 1 [5550/36045]\tLoss: 14557.7168\n",
      "Training Epoch: 1 [5600/36045]\tLoss: 14814.1104\n",
      "Training Epoch: 1 [5650/36045]\tLoss: 14626.9424\n",
      "Training Epoch: 1 [5700/36045]\tLoss: 14078.0762\n",
      "Training Epoch: 1 [5750/36045]\tLoss: 14628.8262\n",
      "Training Epoch: 1 [5800/36045]\tLoss: 15011.6641\n",
      "Training Epoch: 1 [5850/36045]\tLoss: 14900.3584\n",
      "Training Epoch: 1 [5900/36045]\tLoss: 15811.7314\n",
      "Training Epoch: 1 [5950/36045]\tLoss: 16529.3164\n",
      "Training Epoch: 1 [6000/36045]\tLoss: 15585.4092\n",
      "Training Epoch: 1 [6050/36045]\tLoss: 15593.2578\n",
      "Training Epoch: 1 [6100/36045]\tLoss: 15507.3066\n",
      "Training Epoch: 1 [6150/36045]\tLoss: 15744.0332\n",
      "Training Epoch: 1 [6200/36045]\tLoss: 16243.1523\n",
      "Training Epoch: 1 [6250/36045]\tLoss: 16847.7148\n",
      "Training Epoch: 1 [6300/36045]\tLoss: 17081.6738\n",
      "Training Epoch: 1 [6350/36045]\tLoss: 17150.2812\n",
      "Training Epoch: 1 [6400/36045]\tLoss: 15798.4678\n",
      "Training Epoch: 1 [6450/36045]\tLoss: 14568.6494\n",
      "Training Epoch: 1 [6500/36045]\tLoss: 14825.0127\n",
      "Training Epoch: 1 [6550/36045]\tLoss: 14953.7031\n",
      "Training Epoch: 1 [6600/36045]\tLoss: 14775.8682\n",
      "Training Epoch: 1 [6650/36045]\tLoss: 15150.8467\n",
      "Training Epoch: 1 [6700/36045]\tLoss: 15942.4717\n",
      "Training Epoch: 1 [6750/36045]\tLoss: 15607.1455\n",
      "Training Epoch: 1 [6800/36045]\tLoss: 15399.7451\n",
      "Training Epoch: 1 [6850/36045]\tLoss: 15128.4238\n",
      "Training Epoch: 1 [6900/36045]\tLoss: 12301.6709\n",
      "Training Epoch: 1 [6950/36045]\tLoss: 11978.2637\n",
      "Training Epoch: 1 [7000/36045]\tLoss: 11921.4199\n",
      "Training Epoch: 1 [7050/36045]\tLoss: 12821.7109\n",
      "Training Epoch: 1 [7100/36045]\tLoss: 12659.1973\n",
      "Training Epoch: 1 [7150/36045]\tLoss: 12986.3730\n",
      "Training Epoch: 1 [7200/36045]\tLoss: 13356.1357\n",
      "Training Epoch: 1 [7250/36045]\tLoss: 13262.2275\n",
      "Training Epoch: 1 [7300/36045]\tLoss: 13056.4404\n",
      "Training Epoch: 1 [7350/36045]\tLoss: 12968.5312\n",
      "Training Epoch: 1 [7400/36045]\tLoss: 13271.8750\n",
      "Training Epoch: 1 [7450/36045]\tLoss: 13240.8066\n",
      "Training Epoch: 1 [7500/36045]\tLoss: 13085.1836\n",
      "Training Epoch: 1 [7550/36045]\tLoss: 12607.6660\n",
      "Training Epoch: 1 [7600/36045]\tLoss: 12884.5742\n",
      "Training Epoch: 1 [7650/36045]\tLoss: 13239.5449\n",
      "Training Epoch: 1 [7700/36045]\tLoss: 12479.8008\n",
      "Training Epoch: 1 [7750/36045]\tLoss: 12943.2734\n",
      "Training Epoch: 1 [7800/36045]\tLoss: 12525.3193\n",
      "Training Epoch: 1 [7850/36045]\tLoss: 11212.3926\n",
      "Training Epoch: 1 [7900/36045]\tLoss: 11804.6377\n",
      "Training Epoch: 1 [7950/36045]\tLoss: 11693.1934\n",
      "Training Epoch: 1 [8000/36045]\tLoss: 11643.6123\n",
      "Training Epoch: 1 [8050/36045]\tLoss: 11005.4648\n",
      "Training Epoch: 1 [8100/36045]\tLoss: 11734.4834\n",
      "Training Epoch: 1 [8150/36045]\tLoss: 14181.6406\n",
      "Training Epoch: 1 [8200/36045]\tLoss: 14042.5977\n",
      "Training Epoch: 1 [8250/36045]\tLoss: 13823.4004\n",
      "Training Epoch: 1 [8300/36045]\tLoss: 13957.3887\n",
      "Training Epoch: 1 [8350/36045]\tLoss: 13470.1201\n",
      "Training Epoch: 1 [8400/36045]\tLoss: 13212.3232\n",
      "Training Epoch: 1 [8450/36045]\tLoss: 12866.0986\n",
      "Training Epoch: 1 [8500/36045]\tLoss: 13038.9697\n",
      "Training Epoch: 1 [8550/36045]\tLoss: 12293.3965\n",
      "Training Epoch: 1 [8600/36045]\tLoss: 11933.5439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8650/36045]\tLoss: 12648.8457\n",
      "Training Epoch: 1 [8700/36045]\tLoss: 12898.6299\n",
      "Training Epoch: 1 [8750/36045]\tLoss: 12966.9814\n",
      "Training Epoch: 1 [8800/36045]\tLoss: 12886.6133\n",
      "Training Epoch: 1 [8850/36045]\tLoss: 12926.1797\n",
      "Training Epoch: 1 [8900/36045]\tLoss: 11865.7529\n",
      "Training Epoch: 1 [8950/36045]\tLoss: 11915.1445\n",
      "Training Epoch: 1 [9000/36045]\tLoss: 12267.1006\n",
      "Training Epoch: 1 [9050/36045]\tLoss: 12368.3799\n",
      "Training Epoch: 1 [9100/36045]\tLoss: 12250.6885\n",
      "Training Epoch: 1 [9150/36045]\tLoss: 8444.8945\n",
      "Training Epoch: 1 [9200/36045]\tLoss: 5920.9644\n",
      "Training Epoch: 1 [9250/36045]\tLoss: 6413.6704\n",
      "Training Epoch: 1 [9300/36045]\tLoss: 6657.3643\n",
      "Training Epoch: 1 [9350/36045]\tLoss: 5912.9863\n",
      "Training Epoch: 1 [9400/36045]\tLoss: 12149.0547\n",
      "Training Epoch: 1 [9450/36045]\tLoss: 12719.0029\n",
      "Training Epoch: 1 [9500/36045]\tLoss: 12443.7617\n",
      "Training Epoch: 1 [9550/36045]\tLoss: 12975.6104\n",
      "Training Epoch: 1 [9600/36045]\tLoss: 9593.9209\n",
      "Training Epoch: 1 [9650/36045]\tLoss: 9065.7158\n",
      "Training Epoch: 1 [9700/36045]\tLoss: 9392.5371\n",
      "Training Epoch: 1 [9750/36045]\tLoss: 8845.7920\n",
      "Training Epoch: 1 [9800/36045]\tLoss: 11427.5430\n",
      "Training Epoch: 1 [9850/36045]\tLoss: 12003.6680\n",
      "Training Epoch: 1 [9900/36045]\tLoss: 12305.4609\n",
      "Training Epoch: 1 [9950/36045]\tLoss: 11972.6670\n",
      "Training Epoch: 1 [10000/36045]\tLoss: 11386.1152\n",
      "Training Epoch: 1 [10050/36045]\tLoss: 10544.9053\n",
      "Training Epoch: 1 [10100/36045]\tLoss: 10520.0850\n",
      "Training Epoch: 1 [10150/36045]\tLoss: 10366.7646\n",
      "Training Epoch: 1 [10200/36045]\tLoss: 10375.1719\n",
      "Training Epoch: 1 [10250/36045]\tLoss: 13159.7051\n",
      "Training Epoch: 1 [10300/36045]\tLoss: 13294.2578\n",
      "Training Epoch: 1 [10350/36045]\tLoss: 13398.0713\n",
      "Training Epoch: 1 [10400/36045]\tLoss: 13513.3076\n",
      "Training Epoch: 1 [10450/36045]\tLoss: 12428.2832\n",
      "Training Epoch: 1 [10500/36045]\tLoss: 10192.5020\n",
      "Training Epoch: 1 [10550/36045]\tLoss: 9967.0625\n",
      "Training Epoch: 1 [10600/36045]\tLoss: 10195.0830\n",
      "Training Epoch: 1 [10650/36045]\tLoss: 10340.6143\n",
      "Training Epoch: 1 [10700/36045]\tLoss: 10441.5996\n",
      "Training Epoch: 1 [10750/36045]\tLoss: 10157.0488\n",
      "Training Epoch: 1 [10800/36045]\tLoss: 9948.6709\n",
      "Training Epoch: 1 [10850/36045]\tLoss: 10423.0684\n",
      "Training Epoch: 1 [10900/36045]\tLoss: 10561.5400\n",
      "Training Epoch: 1 [10950/36045]\tLoss: 8716.0791\n",
      "Training Epoch: 1 [11000/36045]\tLoss: 8770.2764\n",
      "Training Epoch: 1 [11050/36045]\tLoss: 9165.3271\n",
      "Training Epoch: 1 [11100/36045]\tLoss: 9161.9141\n",
      "Training Epoch: 1 [11150/36045]\tLoss: 9851.3115\n",
      "Training Epoch: 1 [11200/36045]\tLoss: 12202.6982\n",
      "Training Epoch: 1 [11250/36045]\tLoss: 12252.8408\n",
      "Training Epoch: 1 [11300/36045]\tLoss: 11682.9033\n",
      "Training Epoch: 1 [11350/36045]\tLoss: 11981.8350\n",
      "Training Epoch: 1 [11400/36045]\tLoss: 10888.8018\n",
      "Training Epoch: 1 [11450/36045]\tLoss: 10492.7822\n",
      "Training Epoch: 1 [11500/36045]\tLoss: 10554.6211\n",
      "Training Epoch: 1 [11550/36045]\tLoss: 10973.7393\n",
      "Training Epoch: 1 [11600/36045]\tLoss: 11745.3008\n",
      "Training Epoch: 1 [11650/36045]\tLoss: 12548.8115\n",
      "Training Epoch: 1 [11700/36045]\tLoss: 12558.0234\n",
      "Training Epoch: 1 [11750/36045]\tLoss: 11948.3164\n",
      "Training Epoch: 1 [11800/36045]\tLoss: 13000.0703\n",
      "Training Epoch: 1 [11850/36045]\tLoss: 12694.1113\n",
      "Training Epoch: 1 [11900/36045]\tLoss: 13298.9834\n",
      "Training Epoch: 1 [11950/36045]\tLoss: 12882.0479\n",
      "Training Epoch: 1 [12000/36045]\tLoss: 13479.0869\n",
      "Training Epoch: 1 [12050/36045]\tLoss: 13153.7070\n",
      "Training Epoch: 1 [12100/36045]\tLoss: 10116.5342\n",
      "Training Epoch: 1 [12150/36045]\tLoss: 9090.3711\n",
      "Training Epoch: 1 [12200/36045]\tLoss: 8890.0977\n",
      "Training Epoch: 1 [12250/36045]\tLoss: 9035.6416\n",
      "Training Epoch: 1 [12300/36045]\tLoss: 10610.6377\n",
      "Training Epoch: 1 [12350/36045]\tLoss: 11365.6768\n",
      "Training Epoch: 1 [12400/36045]\tLoss: 10916.7090\n",
      "Training Epoch: 1 [12450/36045]\tLoss: 10398.4180\n",
      "Training Epoch: 1 [12500/36045]\tLoss: 10991.4062\n",
      "Training Epoch: 1 [12550/36045]\tLoss: 10057.5332\n",
      "Training Epoch: 1 [12600/36045]\tLoss: 9064.1426\n",
      "Training Epoch: 1 [12650/36045]\tLoss: 9155.8857\n",
      "Training Epoch: 1 [12700/36045]\tLoss: 9078.0732\n",
      "Training Epoch: 1 [12750/36045]\tLoss: 9639.9570\n",
      "Training Epoch: 1 [12800/36045]\tLoss: 9763.3418\n",
      "Training Epoch: 1 [12850/36045]\tLoss: 11024.3174\n",
      "Training Epoch: 1 [12900/36045]\tLoss: 10834.9297\n",
      "Training Epoch: 1 [12950/36045]\tLoss: 10678.3643\n",
      "Training Epoch: 1 [13000/36045]\tLoss: 10412.6211\n",
      "Training Epoch: 1 [13050/36045]\tLoss: 10196.2881\n",
      "Training Epoch: 1 [13100/36045]\tLoss: 10395.8838\n",
      "Training Epoch: 1 [13150/36045]\tLoss: 10208.5244\n",
      "Training Epoch: 1 [13200/36045]\tLoss: 9843.4434\n",
      "Training Epoch: 1 [13250/36045]\tLoss: 10124.8955\n",
      "Training Epoch: 1 [13300/36045]\tLoss: 9857.2744\n",
      "Training Epoch: 1 [13350/36045]\tLoss: 9369.4580\n",
      "Training Epoch: 1 [13400/36045]\tLoss: 9316.5469\n",
      "Training Epoch: 1 [13450/36045]\tLoss: 9100.0127\n",
      "Training Epoch: 1 [13500/36045]\tLoss: 9528.9746\n",
      "Training Epoch: 1 [13550/36045]\tLoss: 9875.5137\n",
      "Training Epoch: 1 [13600/36045]\tLoss: 10211.9258\n",
      "Training Epoch: 1 [13650/36045]\tLoss: 9802.7246\n",
      "Training Epoch: 1 [13700/36045]\tLoss: 9040.1572\n",
      "Training Epoch: 1 [13750/36045]\tLoss: 9181.0928\n",
      "Training Epoch: 1 [13800/36045]\tLoss: 9314.2334\n",
      "Training Epoch: 1 [13850/36045]\tLoss: 9370.9609\n",
      "Training Epoch: 1 [13900/36045]\tLoss: 9314.6680\n",
      "Training Epoch: 1 [13950/36045]\tLoss: 8764.6514\n",
      "Training Epoch: 1 [14000/36045]\tLoss: 8514.5166\n",
      "Training Epoch: 1 [14050/36045]\tLoss: 8249.6514\n",
      "Training Epoch: 1 [14100/36045]\tLoss: 8268.6885\n",
      "Training Epoch: 1 [14150/36045]\tLoss: 8344.4072\n",
      "Training Epoch: 1 [14200/36045]\tLoss: 8807.8291\n",
      "Training Epoch: 1 [14250/36045]\tLoss: 10043.0156\n",
      "Training Epoch: 1 [14300/36045]\tLoss: 10089.2188\n",
      "Training Epoch: 1 [14350/36045]\tLoss: 9486.5869\n",
      "Training Epoch: 1 [14400/36045]\tLoss: 9718.1396\n",
      "Training Epoch: 1 [14450/36045]\tLoss: 9842.9258\n",
      "Training Epoch: 1 [14500/36045]\tLoss: 10097.6123\n",
      "Training Epoch: 1 [14550/36045]\tLoss: 10387.1602\n",
      "Training Epoch: 1 [14600/36045]\tLoss: 10538.6465\n",
      "Training Epoch: 1 [14650/36045]\tLoss: 10295.1191\n",
      "Training Epoch: 1 [14700/36045]\tLoss: 9789.3252\n",
      "Training Epoch: 1 [14750/36045]\tLoss: 9071.1348\n",
      "Training Epoch: 1 [14800/36045]\tLoss: 9077.6367\n",
      "Training Epoch: 1 [14850/36045]\tLoss: 9121.1777\n",
      "Training Epoch: 1 [14900/36045]\tLoss: 8862.6738\n",
      "Training Epoch: 1 [14950/36045]\tLoss: 9620.1826\n",
      "Training Epoch: 1 [15000/36045]\tLoss: 10613.9219\n",
      "Training Epoch: 1 [15050/36045]\tLoss: 10430.2129\n",
      "Training Epoch: 1 [15100/36045]\tLoss: 10660.7646\n",
      "Training Epoch: 1 [15150/36045]\tLoss: 8419.6377\n",
      "Training Epoch: 1 [15200/36045]\tLoss: 6579.4668\n",
      "Training Epoch: 1 [15250/36045]\tLoss: 6954.7705\n",
      "Training Epoch: 1 [15300/36045]\tLoss: 6785.2905\n",
      "Training Epoch: 1 [15350/36045]\tLoss: 6908.4077\n",
      "Training Epoch: 1 [15400/36045]\tLoss: 10789.3926\n",
      "Training Epoch: 1 [15450/36045]\tLoss: 10986.7236\n",
      "Training Epoch: 1 [15500/36045]\tLoss: 11298.9775\n",
      "Training Epoch: 1 [15550/36045]\tLoss: 10879.7139\n",
      "Training Epoch: 1 [15600/36045]\tLoss: 8558.2285\n",
      "Training Epoch: 1 [15650/36045]\tLoss: 8807.6777\n",
      "Training Epoch: 1 [15700/36045]\tLoss: 8292.4316\n",
      "Training Epoch: 1 [15750/36045]\tLoss: 8217.4443\n",
      "Training Epoch: 1 [15800/36045]\tLoss: 6130.2471\n",
      "Training Epoch: 1 [15850/36045]\tLoss: 5536.4834\n",
      "Training Epoch: 1 [15900/36045]\tLoss: 5601.2690\n",
      "Training Epoch: 1 [15950/36045]\tLoss: 5760.8228\n",
      "Training Epoch: 1 [16000/36045]\tLoss: 6470.9287\n",
      "Training Epoch: 1 [16050/36045]\tLoss: 6599.8330\n",
      "Training Epoch: 1 [16100/36045]\tLoss: 6015.5625\n",
      "Training Epoch: 1 [16150/36045]\tLoss: 5749.3789\n",
      "Training Epoch: 1 [16200/36045]\tLoss: 6830.6011\n",
      "Training Epoch: 1 [16250/36045]\tLoss: 6978.9893\n",
      "Training Epoch: 1 [16300/36045]\tLoss: 7363.9888\n",
      "Training Epoch: 1 [16350/36045]\tLoss: 7040.5317\n",
      "Training Epoch: 1 [16400/36045]\tLoss: 6853.6401\n",
      "Training Epoch: 1 [16450/36045]\tLoss: 7208.5459\n",
      "Training Epoch: 1 [16500/36045]\tLoss: 7310.1421\n",
      "Training Epoch: 1 [16550/36045]\tLoss: 7200.4121\n",
      "Training Epoch: 1 [16600/36045]\tLoss: 7536.6230\n",
      "Training Epoch: 1 [16650/36045]\tLoss: 7585.4761\n",
      "Training Epoch: 1 [16700/36045]\tLoss: 7490.3525\n",
      "Training Epoch: 1 [16750/36045]\tLoss: 7318.3857\n",
      "Training Epoch: 1 [16800/36045]\tLoss: 7531.4189\n",
      "Training Epoch: 1 [16850/36045]\tLoss: 7027.0688\n",
      "Training Epoch: 1 [16900/36045]\tLoss: 6966.5010\n",
      "Training Epoch: 1 [16950/36045]\tLoss: 6877.4951\n",
      "Training Epoch: 1 [17000/36045]\tLoss: 6909.3818\n",
      "Training Epoch: 1 [17050/36045]\tLoss: 7422.5820\n",
      "Training Epoch: 1 [17100/36045]\tLoss: 7601.0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [17150/36045]\tLoss: 7668.8857\n",
      "Training Epoch: 1 [17200/36045]\tLoss: 8256.3252\n",
      "Training Epoch: 1 [17250/36045]\tLoss: 8460.4502\n",
      "Training Epoch: 1 [17300/36045]\tLoss: 8833.4268\n",
      "Training Epoch: 1 [17350/36045]\tLoss: 7568.2695\n",
      "Training Epoch: 1 [17400/36045]\tLoss: 7355.5757\n",
      "Training Epoch: 1 [17450/36045]\tLoss: 7266.1738\n",
      "Training Epoch: 1 [17500/36045]\tLoss: 7098.9111\n",
      "Training Epoch: 1 [17550/36045]\tLoss: 7239.1631\n",
      "Training Epoch: 1 [17600/36045]\tLoss: 6958.1885\n",
      "Training Epoch: 1 [17650/36045]\tLoss: 7172.1357\n",
      "Training Epoch: 1 [17700/36045]\tLoss: 6800.3706\n",
      "Training Epoch: 1 [17750/36045]\tLoss: 7102.8706\n",
      "Training Epoch: 1 [17800/36045]\tLoss: 6928.6035\n",
      "Training Epoch: 1 [17850/36045]\tLoss: 6297.7661\n",
      "Training Epoch: 1 [17900/36045]\tLoss: 6402.6870\n",
      "Training Epoch: 1 [17950/36045]\tLoss: 6251.4019\n",
      "Training Epoch: 1 [18000/36045]\tLoss: 6194.7168\n",
      "Training Epoch: 1 [18050/36045]\tLoss: 7780.9214\n",
      "Training Epoch: 1 [18100/36045]\tLoss: 8124.3794\n",
      "Training Epoch: 1 [18150/36045]\tLoss: 8068.3369\n",
      "Training Epoch: 1 [18200/36045]\tLoss: 7830.3862\n",
      "Training Epoch: 1 [18250/36045]\tLoss: 8045.3931\n",
      "Training Epoch: 1 [18300/36045]\tLoss: 7797.1353\n",
      "Training Epoch: 1 [18350/36045]\tLoss: 7702.3892\n",
      "Training Epoch: 1 [18400/36045]\tLoss: 7959.6748\n",
      "Training Epoch: 1 [18450/36045]\tLoss: 7607.7002\n",
      "Training Epoch: 1 [18500/36045]\tLoss: 7130.6738\n",
      "Training Epoch: 1 [18550/36045]\tLoss: 6495.6567\n",
      "Training Epoch: 1 [18600/36045]\tLoss: 6109.5029\n",
      "Training Epoch: 1 [18650/36045]\tLoss: 6506.5747\n",
      "Training Epoch: 1 [18700/36045]\tLoss: 6572.3730\n",
      "Training Epoch: 1 [18750/36045]\tLoss: 6488.3652\n",
      "Training Epoch: 1 [18800/36045]\tLoss: 6809.1802\n",
      "Training Epoch: 1 [18850/36045]\tLoss: 6545.4966\n",
      "Training Epoch: 1 [18900/36045]\tLoss: 6914.7983\n",
      "Training Epoch: 1 [18950/36045]\tLoss: 7143.1885\n",
      "Training Epoch: 1 [19000/36045]\tLoss: 7776.5205\n",
      "Training Epoch: 1 [19050/36045]\tLoss: 7393.8506\n",
      "Training Epoch: 1 [19100/36045]\tLoss: 7597.7480\n",
      "Training Epoch: 1 [19150/36045]\tLoss: 7331.4180\n",
      "Training Epoch: 1 [19200/36045]\tLoss: 7205.1821\n",
      "Training Epoch: 1 [19250/36045]\tLoss: 6848.0610\n",
      "Training Epoch: 1 [19300/36045]\tLoss: 7130.0474\n",
      "Training Epoch: 1 [19350/36045]\tLoss: 6773.6084\n",
      "Training Epoch: 1 [19400/36045]\tLoss: 6728.3105\n",
      "Training Epoch: 1 [19450/36045]\tLoss: 6496.9165\n",
      "Training Epoch: 1 [19500/36045]\tLoss: 6701.6855\n",
      "Training Epoch: 1 [19550/36045]\tLoss: 6777.0283\n",
      "Training Epoch: 1 [19600/36045]\tLoss: 7056.1821\n",
      "Training Epoch: 1 [19650/36045]\tLoss: 8296.0293\n",
      "Training Epoch: 1 [19700/36045]\tLoss: 8232.7402\n",
      "Training Epoch: 1 [19750/36045]\tLoss: 8043.3926\n",
      "Training Epoch: 1 [19800/36045]\tLoss: 7868.8994\n",
      "Training Epoch: 1 [19850/36045]\tLoss: 6408.5576\n",
      "Training Epoch: 1 [19900/36045]\tLoss: 6194.1543\n",
      "Training Epoch: 1 [19950/36045]\tLoss: 6222.1167\n",
      "Training Epoch: 1 [20000/36045]\tLoss: 5799.5708\n",
      "Training Epoch: 1 [20050/36045]\tLoss: 6084.7856\n",
      "Training Epoch: 1 [20100/36045]\tLoss: 6069.0430\n",
      "Training Epoch: 1 [20150/36045]\tLoss: 6104.2617\n",
      "Training Epoch: 1 [20200/36045]\tLoss: 6187.1196\n",
      "Training Epoch: 1 [20250/36045]\tLoss: 6699.3228\n",
      "Training Epoch: 1 [20300/36045]\tLoss: 7262.1372\n",
      "Training Epoch: 1 [20350/36045]\tLoss: 7297.6084\n",
      "Training Epoch: 1 [20400/36045]\tLoss: 7459.8369\n",
      "Training Epoch: 1 [20450/36045]\tLoss: 7238.9468\n",
      "Training Epoch: 1 [20500/36045]\tLoss: 7122.0156\n",
      "Training Epoch: 1 [20550/36045]\tLoss: 6613.8403\n",
      "Training Epoch: 1 [20600/36045]\tLoss: 6583.9131\n",
      "Training Epoch: 1 [20650/36045]\tLoss: 6490.5703\n",
      "Training Epoch: 1 [20700/36045]\tLoss: 6578.7788\n",
      "Training Epoch: 1 [20750/36045]\tLoss: 6783.1021\n",
      "Training Epoch: 1 [20800/36045]\tLoss: 7063.1094\n",
      "Training Epoch: 1 [20850/36045]\tLoss: 7080.7046\n",
      "Training Epoch: 1 [20900/36045]\tLoss: 7276.4087\n",
      "Training Epoch: 1 [20950/36045]\tLoss: 7027.0415\n",
      "Training Epoch: 1 [21000/36045]\tLoss: 6732.5356\n",
      "Training Epoch: 1 [21050/36045]\tLoss: 5740.8174\n",
      "Training Epoch: 1 [21100/36045]\tLoss: 5526.8008\n",
      "Training Epoch: 1 [21150/36045]\tLoss: 5809.8315\n",
      "Training Epoch: 1 [21200/36045]\tLoss: 5926.7271\n",
      "Training Epoch: 1 [21250/36045]\tLoss: 5602.3813\n",
      "Training Epoch: 1 [21300/36045]\tLoss: 7514.4258\n",
      "Training Epoch: 1 [21350/36045]\tLoss: 7308.3296\n",
      "Training Epoch: 1 [21400/36045]\tLoss: 7494.7451\n",
      "Training Epoch: 1 [21450/36045]\tLoss: 7451.4863\n",
      "Training Epoch: 1 [21500/36045]\tLoss: 7887.5376\n",
      "Training Epoch: 1 [21550/36045]\tLoss: 7044.1802\n",
      "Training Epoch: 1 [21600/36045]\tLoss: 7320.7661\n",
      "Training Epoch: 1 [21650/36045]\tLoss: 7265.2983\n",
      "Training Epoch: 1 [21700/36045]\tLoss: 7267.0337\n",
      "Training Epoch: 1 [21750/36045]\tLoss: 7117.1968\n",
      "Training Epoch: 1 [21800/36045]\tLoss: 5711.2539\n",
      "Training Epoch: 1 [21850/36045]\tLoss: 5358.2490\n",
      "Training Epoch: 1 [21900/36045]\tLoss: 5530.5747\n",
      "Training Epoch: 1 [21950/36045]\tLoss: 5414.9727\n",
      "Training Epoch: 1 [22000/36045]\tLoss: 5457.6719\n",
      "Training Epoch: 1 [22050/36045]\tLoss: 6557.8169\n",
      "Training Epoch: 1 [22100/36045]\tLoss: 6207.1514\n",
      "Training Epoch: 1 [22150/36045]\tLoss: 6210.3091\n",
      "Training Epoch: 1 [22200/36045]\tLoss: 6049.9761\n",
      "Training Epoch: 1 [22250/36045]\tLoss: 6243.7803\n",
      "Training Epoch: 1 [22300/36045]\tLoss: 6426.5063\n",
      "Training Epoch: 1 [22350/36045]\tLoss: 6288.4551\n",
      "Training Epoch: 1 [22400/36045]\tLoss: 6343.5459\n",
      "Training Epoch: 1 [22450/36045]\tLoss: 6409.8999\n",
      "Training Epoch: 1 [22500/36045]\tLoss: 6293.4722\n",
      "Training Epoch: 1 [22550/36045]\tLoss: 6526.1118\n",
      "Training Epoch: 1 [22600/36045]\tLoss: 6937.0054\n",
      "Training Epoch: 1 [22650/36045]\tLoss: 7195.9185\n",
      "Training Epoch: 1 [22700/36045]\tLoss: 7320.1289\n",
      "Training Epoch: 1 [22750/36045]\tLoss: 7356.1919\n",
      "Training Epoch: 1 [22800/36045]\tLoss: 7639.0615\n",
      "Training Epoch: 1 [22850/36045]\tLoss: 6937.5132\n",
      "Training Epoch: 1 [22900/36045]\tLoss: 6855.0581\n",
      "Training Epoch: 1 [22950/36045]\tLoss: 6909.8345\n",
      "Training Epoch: 1 [23000/36045]\tLoss: 6878.9980\n",
      "Training Epoch: 1 [23050/36045]\tLoss: 6083.2310\n",
      "Training Epoch: 1 [23100/36045]\tLoss: 6174.4702\n",
      "Training Epoch: 1 [23150/36045]\tLoss: 6179.9595\n",
      "Training Epoch: 1 [23200/36045]\tLoss: 5944.5698\n",
      "Training Epoch: 1 [23250/36045]\tLoss: 5649.8301\n",
      "Training Epoch: 1 [23300/36045]\tLoss: 5751.3022\n",
      "Training Epoch: 1 [23350/36045]\tLoss: 5965.4834\n",
      "Training Epoch: 1 [23400/36045]\tLoss: 6284.4600\n",
      "Training Epoch: 1 [23450/36045]\tLoss: 6364.6719\n",
      "Training Epoch: 1 [23500/36045]\tLoss: 6029.4062\n",
      "Training Epoch: 1 [23550/36045]\tLoss: 6357.8750\n",
      "Training Epoch: 1 [23600/36045]\tLoss: 6830.2095\n",
      "Training Epoch: 1 [23650/36045]\tLoss: 7054.3428\n",
      "Training Epoch: 1 [23700/36045]\tLoss: 7015.5498\n",
      "Training Epoch: 1 [23750/36045]\tLoss: 7119.5669\n",
      "Training Epoch: 1 [23800/36045]\tLoss: 7054.0918\n",
      "Training Epoch: 1 [23850/36045]\tLoss: 6989.5942\n",
      "Training Epoch: 1 [23900/36045]\tLoss: 6996.1763\n",
      "Training Epoch: 1 [23950/36045]\tLoss: 7129.0898\n",
      "Training Epoch: 1 [24000/36045]\tLoss: 6914.9590\n",
      "Training Epoch: 1 [24050/36045]\tLoss: 5455.5737\n",
      "Training Epoch: 1 [24100/36045]\tLoss: 5383.8462\n",
      "Training Epoch: 1 [24150/36045]\tLoss: 5480.3667\n",
      "Training Epoch: 1 [24200/36045]\tLoss: 5520.1157\n",
      "Training Epoch: 1 [24250/36045]\tLoss: 5043.1392\n",
      "Training Epoch: 1 [24300/36045]\tLoss: 5592.9370\n",
      "Training Epoch: 1 [24350/36045]\tLoss: 5764.0225\n",
      "Training Epoch: 1 [24400/36045]\tLoss: 5710.4692\n",
      "Training Epoch: 1 [24450/36045]\tLoss: 5584.3618\n",
      "Training Epoch: 1 [24500/36045]\tLoss: 5944.9604\n",
      "Training Epoch: 1 [24550/36045]\tLoss: 6166.9712\n",
      "Training Epoch: 1 [24600/36045]\tLoss: 6117.3726\n",
      "Training Epoch: 1 [24650/36045]\tLoss: 5918.8042\n",
      "Training Epoch: 1 [24700/36045]\tLoss: 5966.9888\n",
      "Training Epoch: 1 [24750/36045]\tLoss: 5753.7021\n",
      "Training Epoch: 1 [24800/36045]\tLoss: 5999.2559\n",
      "Training Epoch: 1 [24850/36045]\tLoss: 6275.8970\n",
      "Training Epoch: 1 [24900/36045]\tLoss: 6241.6118\n",
      "Training Epoch: 1 [24950/36045]\tLoss: 6160.4722\n",
      "Training Epoch: 1 [25000/36045]\tLoss: 5838.1128\n",
      "Training Epoch: 1 [25050/36045]\tLoss: 5954.4375\n",
      "Training Epoch: 1 [25100/36045]\tLoss: 5299.5605\n",
      "Training Epoch: 1 [25150/36045]\tLoss: 4854.7749\n",
      "Training Epoch: 1 [25200/36045]\tLoss: 4734.2197\n",
      "Training Epoch: 1 [25250/36045]\tLoss: 4943.3135\n",
      "Training Epoch: 1 [25300/36045]\tLoss: 6086.9707\n",
      "Training Epoch: 1 [25350/36045]\tLoss: 5923.0049\n",
      "Training Epoch: 1 [25400/36045]\tLoss: 5237.5737\n",
      "Training Epoch: 1 [25450/36045]\tLoss: 5427.2344\n",
      "Training Epoch: 1 [25500/36045]\tLoss: 5829.3774\n",
      "Training Epoch: 1 [25550/36045]\tLoss: 6725.8979\n",
      "Training Epoch: 1 [25600/36045]\tLoss: 6811.6172\n",
      "Training Epoch: 1 [25650/36045]\tLoss: 6870.3745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [25700/36045]\tLoss: 7507.5098\n",
      "Training Epoch: 1 [25750/36045]\tLoss: 6855.3271\n",
      "Training Epoch: 1 [25800/36045]\tLoss: 3549.5154\n",
      "Training Epoch: 1 [25850/36045]\tLoss: 3381.6843\n",
      "Training Epoch: 1 [25900/36045]\tLoss: 3285.3953\n",
      "Training Epoch: 1 [25950/36045]\tLoss: 3321.1597\n",
      "Training Epoch: 1 [26000/36045]\tLoss: 4200.0815\n",
      "Training Epoch: 1 [26050/36045]\tLoss: 5998.1694\n",
      "Training Epoch: 1 [26100/36045]\tLoss: 5902.2954\n",
      "Training Epoch: 1 [26150/36045]\tLoss: 5871.8999\n",
      "Training Epoch: 1 [26200/36045]\tLoss: 5760.1812\n",
      "Training Epoch: 1 [26250/36045]\tLoss: 5870.8643\n",
      "Training Epoch: 1 [26300/36045]\tLoss: 4885.3589\n",
      "Training Epoch: 1 [26350/36045]\tLoss: 4763.1948\n",
      "Training Epoch: 1 [26400/36045]\tLoss: 4787.4053\n",
      "Training Epoch: 1 [26450/36045]\tLoss: 4662.1191\n",
      "Training Epoch: 1 [26500/36045]\tLoss: 5913.9458\n",
      "Training Epoch: 1 [26550/36045]\tLoss: 6142.2002\n",
      "Training Epoch: 1 [26600/36045]\tLoss: 6157.4907\n",
      "Training Epoch: 1 [26650/36045]\tLoss: 6186.4702\n",
      "Training Epoch: 1 [26700/36045]\tLoss: 6145.6689\n",
      "Training Epoch: 1 [26750/36045]\tLoss: 5803.5259\n",
      "Training Epoch: 1 [26800/36045]\tLoss: 4685.5928\n",
      "Training Epoch: 1 [26850/36045]\tLoss: 3993.8784\n",
      "Training Epoch: 1 [26900/36045]\tLoss: 4143.3154\n",
      "Training Epoch: 1 [26950/36045]\tLoss: 4457.6333\n",
      "Training Epoch: 1 [27000/36045]\tLoss: 6014.9360\n",
      "Training Epoch: 1 [27050/36045]\tLoss: 6079.3960\n",
      "Training Epoch: 1 [27100/36045]\tLoss: 6191.9790\n",
      "Training Epoch: 1 [27150/36045]\tLoss: 6144.1836\n",
      "Training Epoch: 1 [27200/36045]\tLoss: 5109.0742\n",
      "Training Epoch: 1 [27250/36045]\tLoss: 5235.9702\n",
      "Training Epoch: 1 [27300/36045]\tLoss: 5156.1011\n",
      "Training Epoch: 1 [27350/36045]\tLoss: 5157.8643\n",
      "Training Epoch: 1 [27400/36045]\tLoss: 4947.8721\n",
      "Training Epoch: 1 [27450/36045]\tLoss: 5792.1128\n",
      "Training Epoch: 1 [27500/36045]\tLoss: 6080.9507\n",
      "Training Epoch: 1 [27550/36045]\tLoss: 5964.3672\n",
      "Training Epoch: 1 [27600/36045]\tLoss: 5915.3755\n",
      "Training Epoch: 1 [27650/36045]\tLoss: 6039.1182\n",
      "Training Epoch: 1 [27700/36045]\tLoss: 6212.8579\n",
      "Training Epoch: 1 [27750/36045]\tLoss: 6253.5601\n",
      "Training Epoch: 1 [27800/36045]\tLoss: 6343.1064\n",
      "Training Epoch: 1 [27850/36045]\tLoss: 6171.4443\n",
      "Training Epoch: 1 [27900/36045]\tLoss: 5181.5522\n",
      "Training Epoch: 1 [27950/36045]\tLoss: 4188.1025\n",
      "Training Epoch: 1 [28000/36045]\tLoss: 4145.0171\n",
      "Training Epoch: 1 [28050/36045]\tLoss: 4346.6099\n",
      "Training Epoch: 1 [28100/36045]\tLoss: 4338.0425\n",
      "Training Epoch: 1 [28150/36045]\tLoss: 4968.1646\n",
      "Training Epoch: 1 [28200/36045]\tLoss: 5400.5269\n",
      "Training Epoch: 1 [28250/36045]\tLoss: 5023.0840\n",
      "Training Epoch: 1 [28300/36045]\tLoss: 5021.7168\n",
      "Training Epoch: 1 [28350/36045]\tLoss: 4899.3945\n",
      "Training Epoch: 1 [28400/36045]\tLoss: 5405.9966\n",
      "Training Epoch: 1 [28450/36045]\tLoss: 4411.9204\n",
      "Training Epoch: 1 [28500/36045]\tLoss: 3385.9951\n",
      "Training Epoch: 1 [28550/36045]\tLoss: 3100.8496\n",
      "Training Epoch: 1 [28600/36045]\tLoss: 4397.6724\n",
      "Training Epoch: 1 [28650/36045]\tLoss: 6113.6191\n",
      "Training Epoch: 1 [28700/36045]\tLoss: 6090.8711\n",
      "Training Epoch: 1 [28750/36045]\tLoss: 5981.4692\n",
      "Training Epoch: 1 [28800/36045]\tLoss: 6193.2007\n",
      "Training Epoch: 1 [28850/36045]\tLoss: 4992.3926\n",
      "Training Epoch: 1 [28900/36045]\tLoss: 3476.6736\n",
      "Training Epoch: 1 [28950/36045]\tLoss: 3428.6162\n",
      "Training Epoch: 1 [29000/36045]\tLoss: 3526.1653\n",
      "Training Epoch: 1 [29050/36045]\tLoss: 3608.9407\n",
      "Training Epoch: 1 [29100/36045]\tLoss: 3776.4143\n",
      "Training Epoch: 1 [29150/36045]\tLoss: 3640.5879\n",
      "Training Epoch: 1 [29200/36045]\tLoss: 3500.5601\n",
      "Training Epoch: 1 [29250/36045]\tLoss: 3346.9373\n",
      "Training Epoch: 1 [29300/36045]\tLoss: 4370.7368\n",
      "Training Epoch: 1 [29350/36045]\tLoss: 5739.2334\n",
      "Training Epoch: 1 [29400/36045]\tLoss: 5905.6548\n",
      "Training Epoch: 1 [29450/36045]\tLoss: 6306.6963\n",
      "Training Epoch: 1 [29500/36045]\tLoss: 6153.2764\n",
      "Training Epoch: 1 [29550/36045]\tLoss: 5892.3979\n",
      "Training Epoch: 1 [29600/36045]\tLoss: 5272.2368\n",
      "Training Epoch: 1 [29650/36045]\tLoss: 5133.8555\n",
      "Training Epoch: 1 [29700/36045]\tLoss: 4579.8247\n",
      "Training Epoch: 1 [29750/36045]\tLoss: 4697.4224\n",
      "Training Epoch: 1 [29800/36045]\tLoss: 4611.7798\n",
      "Training Epoch: 1 [29850/36045]\tLoss: 3849.3962\n",
      "Training Epoch: 1 [29900/36045]\tLoss: 4074.6956\n",
      "Training Epoch: 1 [29950/36045]\tLoss: 3967.0151\n",
      "Training Epoch: 1 [30000/36045]\tLoss: 4030.7466\n",
      "Training Epoch: 1 [30050/36045]\tLoss: 4078.1772\n",
      "Training Epoch: 1 [30100/36045]\tLoss: 6325.0732\n",
      "Training Epoch: 1 [30150/36045]\tLoss: 6206.3823\n",
      "Training Epoch: 1 [30200/36045]\tLoss: 6148.7832\n",
      "Training Epoch: 1 [30250/36045]\tLoss: 6293.1577\n",
      "Training Epoch: 1 [30300/36045]\tLoss: 6263.5273\n",
      "Training Epoch: 1 [30350/36045]\tLoss: 4983.1934\n",
      "Training Epoch: 1 [30400/36045]\tLoss: 4836.4917\n",
      "Training Epoch: 1 [30450/36045]\tLoss: 4841.3672\n",
      "Training Epoch: 1 [30500/36045]\tLoss: 4572.6240\n",
      "Training Epoch: 1 [30550/36045]\tLoss: 4437.6777\n",
      "Training Epoch: 1 [30600/36045]\tLoss: 4156.1021\n",
      "Training Epoch: 1 [30650/36045]\tLoss: 4060.6172\n",
      "Training Epoch: 1 [30700/36045]\tLoss: 4153.1758\n",
      "Training Epoch: 1 [30750/36045]\tLoss: 4140.5562\n",
      "Training Epoch: 1 [30800/36045]\tLoss: 3829.7461\n",
      "Training Epoch: 1 [30850/36045]\tLoss: 3780.9714\n",
      "Training Epoch: 1 [30900/36045]\tLoss: 3845.4395\n",
      "Training Epoch: 1 [30950/36045]\tLoss: 4023.5491\n",
      "Training Epoch: 1 [31000/36045]\tLoss: 4186.5386\n",
      "Training Epoch: 1 [31050/36045]\tLoss: 3688.3518\n",
      "Training Epoch: 1 [31100/36045]\tLoss: 3591.8684\n",
      "Training Epoch: 1 [31150/36045]\tLoss: 3585.3027\n",
      "Training Epoch: 1 [31200/36045]\tLoss: 4526.9468\n",
      "Training Epoch: 1 [31250/36045]\tLoss: 5761.2427\n",
      "Training Epoch: 1 [31300/36045]\tLoss: 5675.0215\n",
      "Training Epoch: 1 [31350/36045]\tLoss: 5712.8086\n",
      "Training Epoch: 1 [31400/36045]\tLoss: 5612.1606\n",
      "Training Epoch: 1 [31450/36045]\tLoss: 5675.4053\n",
      "Training Epoch: 1 [31500/36045]\tLoss: 6021.2256\n",
      "Training Epoch: 1 [31550/36045]\tLoss: 5995.1191\n",
      "Training Epoch: 1 [31600/36045]\tLoss: 5817.4155\n",
      "Training Epoch: 1 [31650/36045]\tLoss: 5946.9531\n",
      "Training Epoch: 1 [31700/36045]\tLoss: 4411.0229\n",
      "Training Epoch: 1 [31750/36045]\tLoss: 3623.2920\n",
      "Training Epoch: 1 [31800/36045]\tLoss: 3461.2822\n",
      "Training Epoch: 1 [31850/36045]\tLoss: 3572.4016\n",
      "Training Epoch: 1 [31900/36045]\tLoss: 5025.5342\n",
      "Training Epoch: 1 [31950/36045]\tLoss: 6012.9111\n",
      "Training Epoch: 1 [32000/36045]\tLoss: 6431.8789\n",
      "Training Epoch: 1 [32050/36045]\tLoss: 6188.7910\n",
      "Training Epoch: 1 [32100/36045]\tLoss: 6005.1172\n",
      "Training Epoch: 1 [32150/36045]\tLoss: 5690.4087\n",
      "Training Epoch: 1 [32200/36045]\tLoss: 5800.6401\n",
      "Training Epoch: 1 [32250/36045]\tLoss: 5874.0981\n",
      "Training Epoch: 1 [32300/36045]\tLoss: 5786.0552\n",
      "Training Epoch: 1 [32350/36045]\tLoss: 5676.7026\n",
      "Training Epoch: 1 [32400/36045]\tLoss: 5298.0249\n",
      "Training Epoch: 1 [32450/36045]\tLoss: 4342.9629\n",
      "Training Epoch: 1 [32500/36045]\tLoss: 4216.5845\n",
      "Training Epoch: 1 [32550/36045]\tLoss: 4212.1025\n",
      "Training Epoch: 1 [32600/36045]\tLoss: 4182.8423\n",
      "Training Epoch: 1 [32650/36045]\tLoss: 5201.2261\n",
      "Training Epoch: 1 [32700/36045]\tLoss: 5686.2358\n",
      "Training Epoch: 1 [32750/36045]\tLoss: 5622.3774\n",
      "Training Epoch: 1 [32800/36045]\tLoss: 5762.1655\n",
      "Training Epoch: 1 [32850/36045]\tLoss: 5316.3125\n",
      "Training Epoch: 1 [32900/36045]\tLoss: 4082.9619\n",
      "Training Epoch: 1 [32950/36045]\tLoss: 4265.4380\n",
      "Training Epoch: 1 [33000/36045]\tLoss: 4433.0703\n",
      "Training Epoch: 1 [33050/36045]\tLoss: 3915.0105\n",
      "Training Epoch: 1 [33100/36045]\tLoss: 4557.3618\n",
      "Training Epoch: 1 [33150/36045]\tLoss: 5938.0361\n",
      "Training Epoch: 1 [33200/36045]\tLoss: 5762.2261\n",
      "Training Epoch: 1 [33250/36045]\tLoss: 6048.9473\n",
      "Training Epoch: 1 [33300/36045]\tLoss: 6310.1045\n",
      "Training Epoch: 1 [33350/36045]\tLoss: 5245.7520\n",
      "Training Epoch: 1 [33400/36045]\tLoss: 4152.7749\n",
      "Training Epoch: 1 [33450/36045]\tLoss: 4054.4219\n",
      "Training Epoch: 1 [33500/36045]\tLoss: 4276.2544\n",
      "Training Epoch: 1 [33550/36045]\tLoss: 4518.0703\n",
      "Training Epoch: 1 [33600/36045]\tLoss: 4375.7632\n",
      "Training Epoch: 1 [33650/36045]\tLoss: 5116.0425\n",
      "Training Epoch: 1 [33700/36045]\tLoss: 4918.1865\n",
      "Training Epoch: 1 [33750/36045]\tLoss: 5207.0483\n",
      "Training Epoch: 1 [33800/36045]\tLoss: 5022.8374\n",
      "Training Epoch: 1 [33850/36045]\tLoss: 5117.5020\n",
      "Training Epoch: 1 [33900/36045]\tLoss: 5302.2803\n",
      "Training Epoch: 1 [33950/36045]\tLoss: 5407.1719\n",
      "Training Epoch: 1 [34000/36045]\tLoss: 5323.4575\n",
      "Training Epoch: 1 [34050/36045]\tLoss: 5508.6411\n",
      "Training Epoch: 1 [34100/36045]\tLoss: 5366.2695\n",
      "Training Epoch: 1 [34150/36045]\tLoss: 5103.0864\n",
      "Training Epoch: 1 [34200/36045]\tLoss: 4782.4385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [34250/36045]\tLoss: 4715.6641\n",
      "Training Epoch: 1 [34300/36045]\tLoss: 4332.1084\n",
      "Training Epoch: 1 [34350/36045]\tLoss: 4331.7637\n",
      "Training Epoch: 1 [34400/36045]\tLoss: 3979.4287\n",
      "Training Epoch: 1 [34450/36045]\tLoss: 3703.2161\n",
      "Training Epoch: 1 [34500/36045]\tLoss: 4019.2695\n",
      "Training Epoch: 1 [34550/36045]\tLoss: 3840.3931\n",
      "Training Epoch: 1 [34600/36045]\tLoss: 3561.2952\n",
      "Training Epoch: 1 [34650/36045]\tLoss: 3825.5950\n",
      "Training Epoch: 1 [34700/36045]\tLoss: 3976.3403\n",
      "Training Epoch: 1 [34750/36045]\tLoss: 3563.0139\n",
      "Training Epoch: 1 [34800/36045]\tLoss: 4112.2383\n",
      "Training Epoch: 1 [34850/36045]\tLoss: 4239.4736\n",
      "Training Epoch: 1 [34900/36045]\tLoss: 5625.4517\n",
      "Training Epoch: 1 [34950/36045]\tLoss: 5728.2236\n",
      "Training Epoch: 1 [35000/36045]\tLoss: 5700.8438\n",
      "Training Epoch: 1 [35050/36045]\tLoss: 5581.0586\n",
      "Training Epoch: 1 [35100/36045]\tLoss: 4237.3574\n",
      "Training Epoch: 1 [35150/36045]\tLoss: 4215.6851\n",
      "Training Epoch: 1 [35200/36045]\tLoss: 3862.2695\n",
      "Training Epoch: 1 [35250/36045]\tLoss: 3940.0344\n",
      "Training Epoch: 1 [35300/36045]\tLoss: 3865.2900\n",
      "Training Epoch: 1 [35350/36045]\tLoss: 4857.5991\n",
      "Training Epoch: 1 [35400/36045]\tLoss: 5226.3237\n",
      "Training Epoch: 1 [35450/36045]\tLoss: 5048.6489\n",
      "Training Epoch: 1 [35500/36045]\tLoss: 4895.2207\n",
      "Training Epoch: 1 [35550/36045]\tLoss: 4849.1436\n",
      "Training Epoch: 1 [35600/36045]\tLoss: 5087.2188\n",
      "Training Epoch: 1 [35650/36045]\tLoss: 5292.4590\n",
      "Training Epoch: 1 [35700/36045]\tLoss: 4947.9375\n",
      "Training Epoch: 1 [35750/36045]\tLoss: 4995.5747\n",
      "Training Epoch: 1 [35800/36045]\tLoss: 5109.1118\n",
      "Training Epoch: 1 [35850/36045]\tLoss: 4969.5151\n",
      "Training Epoch: 1 [35900/36045]\tLoss: 5171.2202\n",
      "Training Epoch: 1 [35950/36045]\tLoss: 5091.6685\n",
      "Training Epoch: 1 [36000/36045]\tLoss: 4959.5161\n",
      "Training Epoch: 1 [36045/36045]\tLoss: 4833.9004\n",
      "Training Epoch: 1 [4004/4004]\tLoss: 5114.8414\n",
      "Training Epoch: 2 [50/36045]\tLoss: 5389.5664\n",
      "Training Epoch: 2 [100/36045]\tLoss: 5345.9897\n",
      "Training Epoch: 2 [150/36045]\tLoss: 5324.4907\n",
      "Training Epoch: 2 [200/36045]\tLoss: 5329.7852\n",
      "Training Epoch: 2 [250/36045]\tLoss: 5412.3628\n",
      "Training Epoch: 2 [300/36045]\tLoss: 5426.7090\n",
      "Training Epoch: 2 [350/36045]\tLoss: 5186.2837\n",
      "Training Epoch: 2 [400/36045]\tLoss: 5349.9038\n",
      "Training Epoch: 2 [450/36045]\tLoss: 5164.2451\n",
      "Training Epoch: 2 [500/36045]\tLoss: 4936.2666\n",
      "Training Epoch: 2 [550/36045]\tLoss: 4900.6685\n",
      "Training Epoch: 2 [600/36045]\tLoss: 4678.5542\n",
      "Training Epoch: 2 [650/36045]\tLoss: 4946.7891\n",
      "Training Epoch: 2 [700/36045]\tLoss: 4968.3145\n",
      "Training Epoch: 2 [750/36045]\tLoss: 5510.3218\n",
      "Training Epoch: 2 [800/36045]\tLoss: 5603.4644\n",
      "Training Epoch: 2 [850/36045]\tLoss: 5640.3545\n",
      "Training Epoch: 2 [900/36045]\tLoss: 5163.6758\n",
      "Training Epoch: 2 [950/36045]\tLoss: 4941.4927\n",
      "Training Epoch: 2 [1000/36045]\tLoss: 4866.0137\n",
      "Training Epoch: 2 [1050/36045]\tLoss: 4911.7104\n",
      "Training Epoch: 2 [1100/36045]\tLoss: 4747.8999\n",
      "Training Epoch: 2 [1150/36045]\tLoss: 4733.9961\n",
      "Training Epoch: 2 [1200/36045]\tLoss: 4883.4478\n",
      "Training Epoch: 2 [1250/36045]\tLoss: 5130.8086\n",
      "Training Epoch: 2 [1300/36045]\tLoss: 5113.0190\n",
      "Training Epoch: 2 [1350/36045]\tLoss: 5121.8418\n",
      "Training Epoch: 2 [1400/36045]\tLoss: 5346.5210\n",
      "Training Epoch: 2 [1450/36045]\tLoss: 5185.7319\n",
      "Training Epoch: 2 [1500/36045]\tLoss: 4935.2070\n",
      "Training Epoch: 2 [1550/36045]\tLoss: 5216.4312\n",
      "Training Epoch: 2 [1600/36045]\tLoss: 5179.7769\n",
      "Training Epoch: 2 [1650/36045]\tLoss: 5184.6802\n",
      "Training Epoch: 2 [1700/36045]\tLoss: 5184.4395\n",
      "Training Epoch: 2 [1750/36045]\tLoss: 5039.7949\n",
      "Training Epoch: 2 [1800/36045]\tLoss: 5129.8442\n",
      "Training Epoch: 2 [1850/36045]\tLoss: 5279.0654\n",
      "Training Epoch: 2 [1900/36045]\tLoss: 5044.7896\n",
      "Training Epoch: 2 [1950/36045]\tLoss: 5017.0645\n",
      "Training Epoch: 2 [2000/36045]\tLoss: 4761.9585\n",
      "Training Epoch: 2 [2050/36045]\tLoss: 4671.1528\n",
      "Training Epoch: 2 [2100/36045]\tLoss: 5059.6167\n",
      "Training Epoch: 2 [2150/36045]\tLoss: 4983.7095\n",
      "Training Epoch: 2 [2200/36045]\tLoss: 4328.1152\n",
      "Training Epoch: 2 [2250/36045]\tLoss: 3831.2034\n",
      "Training Epoch: 2 [2300/36045]\tLoss: 3975.5415\n",
      "Training Epoch: 2 [2350/36045]\tLoss: 3912.0330\n",
      "Training Epoch: 2 [2400/36045]\tLoss: 3938.7886\n",
      "Training Epoch: 2 [2450/36045]\tLoss: 4860.9268\n",
      "Training Epoch: 2 [2500/36045]\tLoss: 5139.6118\n",
      "Training Epoch: 2 [2550/36045]\tLoss: 5031.1685\n",
      "Training Epoch: 2 [2600/36045]\tLoss: 4801.6699\n",
      "Training Epoch: 2 [2650/36045]\tLoss: 5240.8403\n",
      "Training Epoch: 2 [2700/36045]\tLoss: 5296.6367\n",
      "Training Epoch: 2 [2750/36045]\tLoss: 5498.7876\n",
      "Training Epoch: 2 [2800/36045]\tLoss: 5578.9678\n",
      "Training Epoch: 2 [2850/36045]\tLoss: 5273.8833\n",
      "Training Epoch: 2 [2900/36045]\tLoss: 5438.3232\n",
      "Training Epoch: 2 [2950/36045]\tLoss: 5388.0107\n",
      "Training Epoch: 2 [3000/36045]\tLoss: 5339.1235\n",
      "Training Epoch: 2 [3050/36045]\tLoss: 5427.2280\n",
      "Training Epoch: 2 [3100/36045]\tLoss: 5124.5898\n",
      "Training Epoch: 2 [3150/36045]\tLoss: 4087.1946\n",
      "Training Epoch: 2 [3200/36045]\tLoss: 4191.9619\n",
      "Training Epoch: 2 [3250/36045]\tLoss: 3824.0254\n",
      "Training Epoch: 2 [3300/36045]\tLoss: 3725.1362\n",
      "Training Epoch: 2 [3350/36045]\tLoss: 3966.6748\n",
      "Training Epoch: 2 [3400/36045]\tLoss: 4248.7007\n",
      "Training Epoch: 2 [3450/36045]\tLoss: 4507.2183\n",
      "Training Epoch: 2 [3500/36045]\tLoss: 4465.5122\n",
      "Training Epoch: 2 [3550/36045]\tLoss: 4408.9473\n",
      "Training Epoch: 2 [3600/36045]\tLoss: 4729.0723\n",
      "Training Epoch: 2 [3650/36045]\tLoss: 5217.2412\n",
      "Training Epoch: 2 [3700/36045]\tLoss: 5168.8452\n",
      "Training Epoch: 2 [3750/36045]\tLoss: 5123.0068\n",
      "Training Epoch: 2 [3800/36045]\tLoss: 4915.4268\n",
      "Training Epoch: 2 [3850/36045]\tLoss: 4364.4985\n",
      "Training Epoch: 2 [3900/36045]\tLoss: 4377.2256\n",
      "Training Epoch: 2 [3950/36045]\tLoss: 4329.4287\n",
      "Training Epoch: 2 [4000/36045]\tLoss: 4334.9780\n",
      "Training Epoch: 2 [4050/36045]\tLoss: 4153.6514\n",
      "Training Epoch: 2 [4100/36045]\tLoss: 4087.1147\n",
      "Training Epoch: 2 [4150/36045]\tLoss: 4270.5273\n",
      "Training Epoch: 2 [4200/36045]\tLoss: 4286.2354\n",
      "Training Epoch: 2 [4250/36045]\tLoss: 4550.8979\n",
      "Training Epoch: 2 [4300/36045]\tLoss: 4827.9609\n",
      "Training Epoch: 2 [4350/36045]\tLoss: 4935.5044\n",
      "Training Epoch: 2 [4400/36045]\tLoss: 4812.6597\n",
      "Training Epoch: 2 [4450/36045]\tLoss: 4796.3184\n",
      "Training Epoch: 2 [4500/36045]\tLoss: 4742.6387\n",
      "Training Epoch: 2 [4550/36045]\tLoss: 4778.6030\n",
      "Training Epoch: 2 [4600/36045]\tLoss: 4928.9150\n",
      "Training Epoch: 2 [4650/36045]\tLoss: 4788.8081\n",
      "Training Epoch: 2 [4700/36045]\tLoss: 4694.2681\n",
      "Training Epoch: 2 [4750/36045]\tLoss: 4808.5410\n",
      "Training Epoch: 2 [4800/36045]\tLoss: 4953.8789\n",
      "Training Epoch: 2 [4850/36045]\tLoss: 4899.2939\n",
      "Training Epoch: 2 [4900/36045]\tLoss: 4813.4243\n",
      "Training Epoch: 2 [4950/36045]\tLoss: 4736.3018\n",
      "Training Epoch: 2 [5000/36045]\tLoss: 4765.8398\n",
      "Training Epoch: 2 [5050/36045]\tLoss: 4581.5776\n",
      "Training Epoch: 2 [5100/36045]\tLoss: 4687.9238\n",
      "Training Epoch: 2 [5150/36045]\tLoss: 4550.8462\n",
      "Training Epoch: 2 [5200/36045]\tLoss: 4574.6392\n",
      "Training Epoch: 2 [5250/36045]\tLoss: 4635.9028\n",
      "Training Epoch: 2 [5300/36045]\tLoss: 4769.5400\n",
      "Training Epoch: 2 [5350/36045]\tLoss: 4779.0942\n",
      "Training Epoch: 2 [5400/36045]\tLoss: 4727.3066\n",
      "Training Epoch: 2 [5450/36045]\tLoss: 4473.7725\n",
      "Training Epoch: 2 [5500/36045]\tLoss: 4514.0566\n",
      "Training Epoch: 2 [5550/36045]\tLoss: 4665.8843\n",
      "Training Epoch: 2 [5600/36045]\tLoss: 4837.3638\n",
      "Training Epoch: 2 [5650/36045]\tLoss: 4661.0571\n",
      "Training Epoch: 2 [5700/36045]\tLoss: 4143.5669\n",
      "Training Epoch: 2 [5750/36045]\tLoss: 4352.2510\n",
      "Training Epoch: 2 [5800/36045]\tLoss: 4398.7539\n",
      "Training Epoch: 2 [5850/36045]\tLoss: 4342.9893\n",
      "Training Epoch: 2 [5900/36045]\tLoss: 5120.8535\n",
      "Training Epoch: 2 [5950/36045]\tLoss: 5212.5972\n",
      "Training Epoch: 2 [6000/36045]\tLoss: 5086.9312\n",
      "Training Epoch: 2 [6050/36045]\tLoss: 4967.7949\n",
      "Training Epoch: 2 [6100/36045]\tLoss: 4974.0278\n",
      "Training Epoch: 2 [6150/36045]\tLoss: 4808.0103\n",
      "Training Epoch: 2 [6200/36045]\tLoss: 4695.2563\n",
      "Training Epoch: 2 [6250/36045]\tLoss: 4812.8467\n",
      "Training Epoch: 2 [6300/36045]\tLoss: 4904.1133\n",
      "Training Epoch: 2 [6350/36045]\tLoss: 4925.1372\n",
      "Training Epoch: 2 [6400/36045]\tLoss: 4469.3960\n",
      "Training Epoch: 2 [6450/36045]\tLoss: 4163.0498\n",
      "Training Epoch: 2 [6500/36045]\tLoss: 4188.9014\n",
      "Training Epoch: 2 [6550/36045]\tLoss: 4304.8403\n",
      "Training Epoch: 2 [6600/36045]\tLoss: 4277.8789\n",
      "Training Epoch: 2 [6650/36045]\tLoss: 4485.4438\n",
      "Training Epoch: 2 [6700/36045]\tLoss: 4708.6055\n",
      "Training Epoch: 2 [6750/36045]\tLoss: 4554.1445\n",
      "Training Epoch: 2 [6800/36045]\tLoss: 4537.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [6850/36045]\tLoss: 4542.9766\n",
      "Training Epoch: 2 [6900/36045]\tLoss: 3996.4290\n",
      "Training Epoch: 2 [6950/36045]\tLoss: 3848.8586\n",
      "Training Epoch: 2 [7000/36045]\tLoss: 3917.6790\n",
      "Training Epoch: 2 [7050/36045]\tLoss: 3999.2888\n",
      "Training Epoch: 2 [7100/36045]\tLoss: 4506.4258\n",
      "Training Epoch: 2 [7150/36045]\tLoss: 4718.4370\n",
      "Training Epoch: 2 [7200/36045]\tLoss: 4827.6260\n",
      "Training Epoch: 2 [7250/36045]\tLoss: 4684.3818\n",
      "Training Epoch: 2 [7300/36045]\tLoss: 4526.5464\n",
      "Training Epoch: 2 [7350/36045]\tLoss: 4539.8472\n",
      "Training Epoch: 2 [7400/36045]\tLoss: 4086.5715\n",
      "Training Epoch: 2 [7450/36045]\tLoss: 4030.4380\n",
      "Training Epoch: 2 [7500/36045]\tLoss: 4018.4338\n",
      "Training Epoch: 2 [7550/36045]\tLoss: 3878.9387\n",
      "Training Epoch: 2 [7600/36045]\tLoss: 4080.4697\n",
      "Training Epoch: 2 [7650/36045]\tLoss: 4247.5259\n",
      "Training Epoch: 2 [7700/36045]\tLoss: 4025.3125\n",
      "Training Epoch: 2 [7750/36045]\tLoss: 4179.5449\n",
      "Training Epoch: 2 [7800/36045]\tLoss: 4095.0935\n",
      "Training Epoch: 2 [7850/36045]\tLoss: 3952.8259\n",
      "Training Epoch: 2 [7900/36045]\tLoss: 4237.9014\n",
      "Training Epoch: 2 [7950/36045]\tLoss: 4205.0635\n",
      "Training Epoch: 2 [8000/36045]\tLoss: 4214.0259\n",
      "Training Epoch: 2 [8050/36045]\tLoss: 4018.2285\n",
      "Training Epoch: 2 [8100/36045]\tLoss: 4104.5703\n",
      "Training Epoch: 2 [8150/36045]\tLoss: 4507.9868\n",
      "Training Epoch: 2 [8200/36045]\tLoss: 4499.7397\n",
      "Training Epoch: 2 [8250/36045]\tLoss: 4319.7070\n",
      "Training Epoch: 2 [8300/36045]\tLoss: 4495.2012\n",
      "Training Epoch: 2 [8350/36045]\tLoss: 4251.7681\n",
      "Training Epoch: 2 [8400/36045]\tLoss: 4227.0547\n",
      "Training Epoch: 2 [8450/36045]\tLoss: 4118.7026\n",
      "Training Epoch: 2 [8500/36045]\tLoss: 4170.6670\n",
      "Training Epoch: 2 [8550/36045]\tLoss: 3925.3669\n",
      "Training Epoch: 2 [8600/36045]\tLoss: 3840.8679\n",
      "Training Epoch: 2 [8650/36045]\tLoss: 4462.5640\n",
      "Training Epoch: 2 [8700/36045]\tLoss: 4651.0171\n",
      "Training Epoch: 2 [8750/36045]\tLoss: 4687.1763\n",
      "Training Epoch: 2 [8800/36045]\tLoss: 4692.1411\n",
      "Training Epoch: 2 [8850/36045]\tLoss: 4683.8433\n",
      "Training Epoch: 2 [8900/36045]\tLoss: 4221.9351\n",
      "Training Epoch: 2 [8950/36045]\tLoss: 4283.7075\n",
      "Training Epoch: 2 [9000/36045]\tLoss: 4303.3853\n",
      "Training Epoch: 2 [9050/36045]\tLoss: 4297.6055\n",
      "Training Epoch: 2 [9100/36045]\tLoss: 4216.0122\n",
      "Training Epoch: 2 [9150/36045]\tLoss: 3253.5979\n",
      "Training Epoch: 2 [9200/36045]\tLoss: 2583.4622\n",
      "Training Epoch: 2 [9250/36045]\tLoss: 2761.4189\n",
      "Training Epoch: 2 [9300/36045]\tLoss: 2768.9160\n",
      "Training Epoch: 2 [9350/36045]\tLoss: 2570.1270\n",
      "Training Epoch: 2 [9400/36045]\tLoss: 4575.8594\n",
      "Training Epoch: 2 [9450/36045]\tLoss: 4790.3408\n",
      "Training Epoch: 2 [9500/36045]\tLoss: 4665.5728\n",
      "Training Epoch: 2 [9550/36045]\tLoss: 5013.0435\n",
      "Training Epoch: 2 [9600/36045]\tLoss: 3481.0916\n",
      "Training Epoch: 2 [9650/36045]\tLoss: 3368.4966\n",
      "Training Epoch: 2 [9700/36045]\tLoss: 3430.6479\n",
      "Training Epoch: 2 [9750/36045]\tLoss: 3307.1284\n",
      "Training Epoch: 2 [9800/36045]\tLoss: 4637.7476\n",
      "Training Epoch: 2 [9850/36045]\tLoss: 5000.7974\n",
      "Training Epoch: 2 [9900/36045]\tLoss: 5042.7974\n",
      "Training Epoch: 2 [9950/36045]\tLoss: 4894.4316\n",
      "Training Epoch: 2 [10000/36045]\tLoss: 4578.2261\n",
      "Training Epoch: 2 [10050/36045]\tLoss: 4063.2495\n",
      "Training Epoch: 2 [10100/36045]\tLoss: 4078.3364\n",
      "Training Epoch: 2 [10150/36045]\tLoss: 4108.0928\n",
      "Training Epoch: 2 [10200/36045]\tLoss: 4017.4548\n",
      "Training Epoch: 2 [10250/36045]\tLoss: 4692.3018\n",
      "Training Epoch: 2 [10300/36045]\tLoss: 4707.5317\n",
      "Training Epoch: 2 [10350/36045]\tLoss: 4772.3027\n",
      "Training Epoch: 2 [10400/36045]\tLoss: 4798.8159\n",
      "Training Epoch: 2 [10450/36045]\tLoss: 4460.8613\n",
      "Training Epoch: 2 [10500/36045]\tLoss: 3803.1067\n",
      "Training Epoch: 2 [10550/36045]\tLoss: 3674.0291\n",
      "Training Epoch: 2 [10600/36045]\tLoss: 3835.8560\n",
      "Training Epoch: 2 [10650/36045]\tLoss: 3866.5476\n",
      "Training Epoch: 2 [10700/36045]\tLoss: 4190.6099\n",
      "Training Epoch: 2 [10750/36045]\tLoss: 4324.5210\n",
      "Training Epoch: 2 [10800/36045]\tLoss: 4220.1001\n",
      "Training Epoch: 2 [10850/36045]\tLoss: 4335.7065\n",
      "Training Epoch: 2 [10900/36045]\tLoss: 4534.3398\n",
      "Training Epoch: 2 [10950/36045]\tLoss: 3767.4570\n",
      "Training Epoch: 2 [11000/36045]\tLoss: 3810.1536\n",
      "Training Epoch: 2 [11050/36045]\tLoss: 3937.5725\n",
      "Training Epoch: 2 [11100/36045]\tLoss: 3887.6848\n",
      "Training Epoch: 2 [11150/36045]\tLoss: 4124.4517\n",
      "Training Epoch: 2 [11200/36045]\tLoss: 4157.1255\n",
      "Training Epoch: 2 [11250/36045]\tLoss: 4246.8887\n",
      "Training Epoch: 2 [11300/36045]\tLoss: 4107.9014\n",
      "Training Epoch: 2 [11350/36045]\tLoss: 4167.9185\n",
      "Training Epoch: 2 [11400/36045]\tLoss: 4032.3271\n",
      "Training Epoch: 2 [11450/36045]\tLoss: 3932.6272\n",
      "Training Epoch: 2 [11500/36045]\tLoss: 3885.5244\n",
      "Training Epoch: 2 [11550/36045]\tLoss: 4024.6831\n",
      "Training Epoch: 2 [11600/36045]\tLoss: 4263.6577\n",
      "Training Epoch: 2 [11650/36045]\tLoss: 4490.4023\n",
      "Training Epoch: 2 [11700/36045]\tLoss: 4464.1553\n",
      "Training Epoch: 2 [11750/36045]\tLoss: 4370.6299\n",
      "Training Epoch: 2 [11800/36045]\tLoss: 4640.1753\n",
      "Training Epoch: 2 [11850/36045]\tLoss: 4650.8120\n",
      "Training Epoch: 2 [11900/36045]\tLoss: 5016.7549\n",
      "Training Epoch: 2 [11950/36045]\tLoss: 4891.7246\n",
      "Training Epoch: 2 [12000/36045]\tLoss: 5090.7881\n",
      "Training Epoch: 2 [12050/36045]\tLoss: 4950.0645\n",
      "Training Epoch: 2 [12100/36045]\tLoss: 3892.8069\n",
      "Training Epoch: 2 [12150/36045]\tLoss: 3440.0459\n",
      "Training Epoch: 2 [12200/36045]\tLoss: 3391.0417\n",
      "Training Epoch: 2 [12250/36045]\tLoss: 3468.3284\n",
      "Training Epoch: 2 [12300/36045]\tLoss: 3938.8843\n",
      "Training Epoch: 2 [12350/36045]\tLoss: 4176.6167\n",
      "Training Epoch: 2 [12400/36045]\tLoss: 4108.4897\n",
      "Training Epoch: 2 [12450/36045]\tLoss: 4002.5259\n",
      "Training Epoch: 2 [12500/36045]\tLoss: 4221.4692\n",
      "Training Epoch: 2 [12550/36045]\tLoss: 4123.2656\n",
      "Training Epoch: 2 [12600/36045]\tLoss: 4075.4292\n",
      "Training Epoch: 2 [12650/36045]\tLoss: 4171.6499\n",
      "Training Epoch: 2 [12700/36045]\tLoss: 4208.4907\n",
      "Training Epoch: 2 [12750/36045]\tLoss: 4278.3408\n",
      "Training Epoch: 2 [12800/36045]\tLoss: 4010.9583\n",
      "Training Epoch: 2 [12850/36045]\tLoss: 4120.7568\n",
      "Training Epoch: 2 [12900/36045]\tLoss: 4053.4563\n",
      "Training Epoch: 2 [12950/36045]\tLoss: 4047.8862\n",
      "Training Epoch: 2 [13000/36045]\tLoss: 4023.0623\n",
      "Training Epoch: 2 [13050/36045]\tLoss: 4020.8962\n",
      "Training Epoch: 2 [13100/36045]\tLoss: 4117.8501\n",
      "Training Epoch: 2 [13150/36045]\tLoss: 4041.5374\n",
      "Training Epoch: 2 [13200/36045]\tLoss: 3892.4368\n",
      "Training Epoch: 2 [13250/36045]\tLoss: 3996.0493\n",
      "Training Epoch: 2 [13300/36045]\tLoss: 4042.8054\n",
      "Training Epoch: 2 [13350/36045]\tLoss: 3869.6301\n",
      "Training Epoch: 2 [13400/36045]\tLoss: 3885.2781\n",
      "Training Epoch: 2 [13450/36045]\tLoss: 3867.6160\n",
      "Training Epoch: 2 [13500/36045]\tLoss: 4028.4314\n",
      "Training Epoch: 2 [13550/36045]\tLoss: 4220.1597\n",
      "Training Epoch: 2 [13600/36045]\tLoss: 4304.3589\n",
      "Training Epoch: 2 [13650/36045]\tLoss: 4358.5059\n",
      "Training Epoch: 2 [13700/36045]\tLoss: 4012.2615\n",
      "Training Epoch: 2 [13750/36045]\tLoss: 3920.8162\n",
      "Training Epoch: 2 [13800/36045]\tLoss: 3920.4766\n",
      "Training Epoch: 2 [13850/36045]\tLoss: 3928.2898\n",
      "Training Epoch: 2 [13900/36045]\tLoss: 3938.4180\n",
      "Training Epoch: 2 [13950/36045]\tLoss: 3906.8279\n",
      "Training Epoch: 2 [14000/36045]\tLoss: 4050.3674\n",
      "Training Epoch: 2 [14050/36045]\tLoss: 3890.5886\n",
      "Training Epoch: 2 [14100/36045]\tLoss: 3882.1992\n",
      "Training Epoch: 2 [14150/36045]\tLoss: 3883.2324\n",
      "Training Epoch: 2 [14200/36045]\tLoss: 4064.9126\n",
      "Training Epoch: 2 [14250/36045]\tLoss: 4515.7622\n",
      "Training Epoch: 2 [14300/36045]\tLoss: 4541.9644\n",
      "Training Epoch: 2 [14350/36045]\tLoss: 4342.9502\n",
      "Training Epoch: 2 [14400/36045]\tLoss: 4331.5278\n",
      "Training Epoch: 2 [14450/36045]\tLoss: 4432.9971\n",
      "Training Epoch: 2 [14500/36045]\tLoss: 4540.1450\n",
      "Training Epoch: 2 [14550/36045]\tLoss: 4682.5640\n",
      "Training Epoch: 2 [14600/36045]\tLoss: 4709.7866\n",
      "Training Epoch: 2 [14650/36045]\tLoss: 4620.4106\n",
      "Training Epoch: 2 [14700/36045]\tLoss: 4284.7651\n",
      "Training Epoch: 2 [14750/36045]\tLoss: 3691.0137\n",
      "Training Epoch: 2 [14800/36045]\tLoss: 3673.9277\n",
      "Training Epoch: 2 [14850/36045]\tLoss: 3653.6348\n",
      "Training Epoch: 2 [14900/36045]\tLoss: 3611.4602\n",
      "Training Epoch: 2 [14950/36045]\tLoss: 3790.7808\n",
      "Training Epoch: 2 [15000/36045]\tLoss: 4077.6873\n",
      "Training Epoch: 2 [15050/36045]\tLoss: 4070.5969\n",
      "Training Epoch: 2 [15100/36045]\tLoss: 4172.2300\n",
      "Training Epoch: 2 [15150/36045]\tLoss: 3545.3132\n",
      "Training Epoch: 2 [15200/36045]\tLoss: 3006.3655\n",
      "Training Epoch: 2 [15250/36045]\tLoss: 3109.1619\n",
      "Training Epoch: 2 [15300/36045]\tLoss: 3080.8606\n",
      "Training Epoch: 2 [15350/36045]\tLoss: 3117.7800\n",
      "Training Epoch: 2 [15400/36045]\tLoss: 4178.8081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [15450/36045]\tLoss: 4161.3008\n",
      "Training Epoch: 2 [15500/36045]\tLoss: 4235.5986\n",
      "Training Epoch: 2 [15550/36045]\tLoss: 4133.0371\n",
      "Training Epoch: 2 [15600/36045]\tLoss: 3900.3708\n",
      "Training Epoch: 2 [15650/36045]\tLoss: 4012.9177\n",
      "Training Epoch: 2 [15700/36045]\tLoss: 3904.3848\n",
      "Training Epoch: 2 [15750/36045]\tLoss: 3831.7668\n",
      "Training Epoch: 2 [15800/36045]\tLoss: 2980.6086\n",
      "Training Epoch: 2 [15850/36045]\tLoss: 2850.9719\n",
      "Training Epoch: 2 [15900/36045]\tLoss: 2897.7119\n",
      "Training Epoch: 2 [15950/36045]\tLoss: 2978.7827\n",
      "Training Epoch: 2 [16000/36045]\tLoss: 3074.9734\n",
      "Training Epoch: 2 [16050/36045]\tLoss: 2955.6191\n",
      "Training Epoch: 2 [16100/36045]\tLoss: 2842.9067\n",
      "Training Epoch: 2 [16150/36045]\tLoss: 2727.2024\n",
      "Training Epoch: 2 [16200/36045]\tLoss: 3127.8547\n",
      "Training Epoch: 2 [16250/36045]\tLoss: 3270.9470\n",
      "Training Epoch: 2 [16300/36045]\tLoss: 3459.8960\n",
      "Training Epoch: 2 [16350/36045]\tLoss: 3430.7288\n",
      "Training Epoch: 2 [16400/36045]\tLoss: 3358.1187\n",
      "Training Epoch: 2 [16450/36045]\tLoss: 3480.9758\n",
      "Training Epoch: 2 [16500/36045]\tLoss: 3542.8269\n",
      "Training Epoch: 2 [16550/36045]\tLoss: 3433.4629\n",
      "Training Epoch: 2 [16600/36045]\tLoss: 3573.9102\n",
      "Training Epoch: 2 [16650/36045]\tLoss: 3639.4077\n",
      "Training Epoch: 2 [16700/36045]\tLoss: 3594.6619\n",
      "Training Epoch: 2 [16750/36045]\tLoss: 3516.3049\n",
      "Training Epoch: 2 [16800/36045]\tLoss: 3647.9348\n",
      "Training Epoch: 2 [16850/36045]\tLoss: 3510.9067\n",
      "Training Epoch: 2 [16900/36045]\tLoss: 3414.6633\n",
      "Training Epoch: 2 [16950/36045]\tLoss: 3299.7783\n",
      "Training Epoch: 2 [17000/36045]\tLoss: 3306.5977\n",
      "Training Epoch: 2 [17050/36045]\tLoss: 3510.7910\n",
      "Training Epoch: 2 [17100/36045]\tLoss: 3576.0884\n",
      "Training Epoch: 2 [17150/36045]\tLoss: 3472.0134\n",
      "Training Epoch: 2 [17200/36045]\tLoss: 3608.2046\n",
      "Training Epoch: 2 [17250/36045]\tLoss: 3660.0972\n",
      "Training Epoch: 2 [17300/36045]\tLoss: 3866.8596\n",
      "Training Epoch: 2 [17350/36045]\tLoss: 3415.7661\n",
      "Training Epoch: 2 [17400/36045]\tLoss: 3356.4451\n",
      "Training Epoch: 2 [17450/36045]\tLoss: 3446.6550\n",
      "Training Epoch: 2 [17500/36045]\tLoss: 3445.6001\n",
      "Training Epoch: 2 [17550/36045]\tLoss: 3466.0652\n",
      "Training Epoch: 2 [17600/36045]\tLoss: 3443.3208\n",
      "Training Epoch: 2 [17650/36045]\tLoss: 3598.5767\n",
      "Training Epoch: 2 [17700/36045]\tLoss: 3464.0161\n",
      "Training Epoch: 2 [17750/36045]\tLoss: 3549.8716\n",
      "Training Epoch: 2 [17800/36045]\tLoss: 3499.3042\n",
      "Training Epoch: 2 [17850/36045]\tLoss: 3096.9526\n",
      "Training Epoch: 2 [17900/36045]\tLoss: 3104.9890\n",
      "Training Epoch: 2 [17950/36045]\tLoss: 3097.8196\n",
      "Training Epoch: 2 [18000/36045]\tLoss: 3029.5076\n",
      "Training Epoch: 2 [18050/36045]\tLoss: 3561.7705\n",
      "Training Epoch: 2 [18100/36045]\tLoss: 3735.8298\n",
      "Training Epoch: 2 [18150/36045]\tLoss: 3699.8518\n",
      "Training Epoch: 2 [18200/36045]\tLoss: 3610.1082\n",
      "Training Epoch: 2 [18250/36045]\tLoss: 3748.6702\n",
      "Training Epoch: 2 [18300/36045]\tLoss: 3554.9146\n",
      "Training Epoch: 2 [18350/36045]\tLoss: 3543.6504\n",
      "Training Epoch: 2 [18400/36045]\tLoss: 3662.1172\n",
      "Training Epoch: 2 [18450/36045]\tLoss: 3493.2075\n",
      "Training Epoch: 2 [18500/36045]\tLoss: 3391.4905\n",
      "Training Epoch: 2 [18550/36045]\tLoss: 3195.7014\n",
      "Training Epoch: 2 [18600/36045]\tLoss: 3071.4517\n",
      "Training Epoch: 2 [18650/36045]\tLoss: 3245.1809\n",
      "Training Epoch: 2 [18700/36045]\tLoss: 3335.6907\n",
      "Training Epoch: 2 [18750/36045]\tLoss: 3347.1084\n",
      "Training Epoch: 2 [18800/36045]\tLoss: 3499.7671\n",
      "Training Epoch: 2 [18850/36045]\tLoss: 3320.6448\n",
      "Training Epoch: 2 [18900/36045]\tLoss: 3505.9456\n",
      "Training Epoch: 2 [18950/36045]\tLoss: 3528.2876\n",
      "Training Epoch: 2 [19000/36045]\tLoss: 3643.8770\n",
      "Training Epoch: 2 [19050/36045]\tLoss: 3511.3687\n",
      "Training Epoch: 2 [19100/36045]\tLoss: 3566.8555\n",
      "Training Epoch: 2 [19150/36045]\tLoss: 3494.3362\n",
      "Training Epoch: 2 [19200/36045]\tLoss: 3371.2471\n",
      "Training Epoch: 2 [19250/36045]\tLoss: 3277.3245\n",
      "Training Epoch: 2 [19300/36045]\tLoss: 3402.9993\n",
      "Training Epoch: 2 [19350/36045]\tLoss: 3266.4072\n",
      "Training Epoch: 2 [19400/36045]\tLoss: 3351.6802\n",
      "Training Epoch: 2 [19450/36045]\tLoss: 3298.3975\n",
      "Training Epoch: 2 [19500/36045]\tLoss: 3363.0444\n",
      "Training Epoch: 2 [19550/36045]\tLoss: 3392.9722\n",
      "Training Epoch: 2 [19600/36045]\tLoss: 3533.5334\n",
      "Training Epoch: 2 [19650/36045]\tLoss: 4085.3064\n",
      "Training Epoch: 2 [19700/36045]\tLoss: 4048.6733\n",
      "Training Epoch: 2 [19750/36045]\tLoss: 4004.9250\n",
      "Training Epoch: 2 [19800/36045]\tLoss: 3912.5261\n",
      "Training Epoch: 2 [19850/36045]\tLoss: 3104.2263\n",
      "Training Epoch: 2 [19900/36045]\tLoss: 3021.7771\n",
      "Training Epoch: 2 [19950/36045]\tLoss: 3027.3005\n",
      "Training Epoch: 2 [20000/36045]\tLoss: 2914.8850\n",
      "Training Epoch: 2 [20050/36045]\tLoss: 3318.5830\n",
      "Training Epoch: 2 [20100/36045]\tLoss: 3302.9592\n",
      "Training Epoch: 2 [20150/36045]\tLoss: 3332.8159\n",
      "Training Epoch: 2 [20200/36045]\tLoss: 3358.1646\n",
      "Training Epoch: 2 [20250/36045]\tLoss: 3477.3945\n",
      "Training Epoch: 2 [20300/36045]\tLoss: 3495.0198\n",
      "Training Epoch: 2 [20350/36045]\tLoss: 3556.9067\n",
      "Training Epoch: 2 [20400/36045]\tLoss: 3587.4077\n",
      "Training Epoch: 2 [20450/36045]\tLoss: 3562.7832\n",
      "Training Epoch: 2 [20500/36045]\tLoss: 3435.8472\n",
      "Training Epoch: 2 [20550/36045]\tLoss: 3403.9126\n",
      "Training Epoch: 2 [20600/36045]\tLoss: 3459.7988\n",
      "Training Epoch: 2 [20650/36045]\tLoss: 3429.4243\n",
      "Training Epoch: 2 [20700/36045]\tLoss: 3446.4028\n",
      "Training Epoch: 2 [20750/36045]\tLoss: 3586.4988\n",
      "Training Epoch: 2 [20800/36045]\tLoss: 3836.0664\n",
      "Training Epoch: 2 [20850/36045]\tLoss: 3789.9858\n",
      "Training Epoch: 2 [20900/36045]\tLoss: 3917.0242\n",
      "Training Epoch: 2 [20950/36045]\tLoss: 3753.4692\n",
      "Training Epoch: 2 [21000/36045]\tLoss: 3528.3545\n",
      "Training Epoch: 2 [21050/36045]\tLoss: 3062.4651\n",
      "Training Epoch: 2 [21100/36045]\tLoss: 3051.3894\n",
      "Training Epoch: 2 [21150/36045]\tLoss: 3173.9282\n",
      "Training Epoch: 2 [21200/36045]\tLoss: 3210.4255\n",
      "Training Epoch: 2 [21250/36045]\tLoss: 3038.8887\n",
      "Training Epoch: 2 [21300/36045]\tLoss: 3840.3931\n",
      "Training Epoch: 2 [21350/36045]\tLoss: 3708.0295\n",
      "Training Epoch: 2 [21400/36045]\tLoss: 3801.3965\n",
      "Training Epoch: 2 [21450/36045]\tLoss: 3784.3184\n",
      "Training Epoch: 2 [21500/36045]\tLoss: 3885.5391\n",
      "Training Epoch: 2 [21550/36045]\tLoss: 3769.7642\n",
      "Training Epoch: 2 [21600/36045]\tLoss: 3809.2236\n",
      "Training Epoch: 2 [21650/36045]\tLoss: 3834.6262\n",
      "Training Epoch: 2 [21700/36045]\tLoss: 3826.1257\n",
      "Training Epoch: 2 [21750/36045]\tLoss: 3633.1436\n",
      "Training Epoch: 2 [21800/36045]\tLoss: 3009.6973\n",
      "Training Epoch: 2 [21850/36045]\tLoss: 2872.8838\n",
      "Training Epoch: 2 [21900/36045]\tLoss: 2958.6123\n",
      "Training Epoch: 2 [21950/36045]\tLoss: 2956.0549\n",
      "Training Epoch: 2 [22000/36045]\tLoss: 2930.6807\n",
      "Training Epoch: 2 [22050/36045]\tLoss: 3311.6902\n",
      "Training Epoch: 2 [22100/36045]\tLoss: 3207.3347\n",
      "Training Epoch: 2 [22150/36045]\tLoss: 3181.9690\n",
      "Training Epoch: 2 [22200/36045]\tLoss: 3155.0068\n",
      "Training Epoch: 2 [22250/36045]\tLoss: 3254.0901\n",
      "Training Epoch: 2 [22300/36045]\tLoss: 3373.7429\n",
      "Training Epoch: 2 [22350/36045]\tLoss: 3304.9504\n",
      "Training Epoch: 2 [22400/36045]\tLoss: 3357.4409\n",
      "Training Epoch: 2 [22450/36045]\tLoss: 3359.4180\n",
      "Training Epoch: 2 [22500/36045]\tLoss: 3264.4419\n",
      "Training Epoch: 2 [22550/36045]\tLoss: 3439.4265\n",
      "Training Epoch: 2 [22600/36045]\tLoss: 3699.0081\n",
      "Training Epoch: 2 [22650/36045]\tLoss: 3886.8625\n",
      "Training Epoch: 2 [22700/36045]\tLoss: 3983.4092\n",
      "Training Epoch: 2 [22750/36045]\tLoss: 3967.2290\n",
      "Training Epoch: 2 [22800/36045]\tLoss: 4166.9854\n",
      "Training Epoch: 2 [22850/36045]\tLoss: 3729.4453\n",
      "Training Epoch: 2 [22900/36045]\tLoss: 3668.7585\n",
      "Training Epoch: 2 [22950/36045]\tLoss: 3679.9692\n",
      "Training Epoch: 2 [23000/36045]\tLoss: 3692.4253\n",
      "Training Epoch: 2 [23050/36045]\tLoss: 3235.7322\n",
      "Training Epoch: 2 [23100/36045]\tLoss: 3296.0913\n",
      "Training Epoch: 2 [23150/36045]\tLoss: 3292.9368\n",
      "Training Epoch: 2 [23200/36045]\tLoss: 3185.6299\n",
      "Training Epoch: 2 [23250/36045]\tLoss: 3079.6987\n",
      "Training Epoch: 2 [23300/36045]\tLoss: 3083.8608\n",
      "Training Epoch: 2 [23350/36045]\tLoss: 3182.6360\n",
      "Training Epoch: 2 [23400/36045]\tLoss: 3363.0273\n",
      "Training Epoch: 2 [23450/36045]\tLoss: 3380.4180\n",
      "Training Epoch: 2 [23500/36045]\tLoss: 3230.3213\n",
      "Training Epoch: 2 [23550/36045]\tLoss: 3461.6277\n",
      "Training Epoch: 2 [23600/36045]\tLoss: 3731.0659\n",
      "Training Epoch: 2 [23650/36045]\tLoss: 3923.3445\n",
      "Training Epoch: 2 [23700/36045]\tLoss: 3892.2512\n",
      "Training Epoch: 2 [23750/36045]\tLoss: 3879.0027\n",
      "Training Epoch: 2 [23800/36045]\tLoss: 3491.0916\n",
      "Training Epoch: 2 [23850/36045]\tLoss: 3479.0774\n",
      "Training Epoch: 2 [23900/36045]\tLoss: 3441.3667\n",
      "Training Epoch: 2 [23950/36045]\tLoss: 3461.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [24000/36045]\tLoss: 3282.2458\n",
      "Training Epoch: 2 [24050/36045]\tLoss: 2798.9158\n",
      "Training Epoch: 2 [24100/36045]\tLoss: 2831.4863\n",
      "Training Epoch: 2 [24150/36045]\tLoss: 2927.4438\n",
      "Training Epoch: 2 [24200/36045]\tLoss: 2934.5000\n",
      "Training Epoch: 2 [24250/36045]\tLoss: 2733.9016\n",
      "Training Epoch: 2 [24300/36045]\tLoss: 3024.2043\n",
      "Training Epoch: 2 [24350/36045]\tLoss: 3142.8374\n",
      "Training Epoch: 2 [24400/36045]\tLoss: 3149.6001\n",
      "Training Epoch: 2 [24450/36045]\tLoss: 3087.4551\n",
      "Training Epoch: 2 [24500/36045]\tLoss: 3288.8804\n",
      "Training Epoch: 2 [24550/36045]\tLoss: 3364.1736\n",
      "Training Epoch: 2 [24600/36045]\tLoss: 3332.3005\n",
      "Training Epoch: 2 [24650/36045]\tLoss: 3243.8862\n",
      "Training Epoch: 2 [24700/36045]\tLoss: 3327.7844\n",
      "Training Epoch: 2 [24750/36045]\tLoss: 3126.2019\n",
      "Training Epoch: 2 [24800/36045]\tLoss: 3080.8406\n",
      "Training Epoch: 2 [24850/36045]\tLoss: 3154.8496\n",
      "Training Epoch: 2 [24900/36045]\tLoss: 3129.3000\n",
      "Training Epoch: 2 [24950/36045]\tLoss: 3087.5132\n",
      "Training Epoch: 2 [25000/36045]\tLoss: 2960.9155\n",
      "Training Epoch: 2 [25050/36045]\tLoss: 2832.8611\n",
      "Training Epoch: 2 [25100/36045]\tLoss: 2560.0264\n",
      "Training Epoch: 2 [25150/36045]\tLoss: 2334.4194\n",
      "Training Epoch: 2 [25200/36045]\tLoss: 2330.5535\n",
      "Training Epoch: 2 [25250/36045]\tLoss: 2478.4402\n",
      "Training Epoch: 2 [25300/36045]\tLoss: 3196.6025\n",
      "Training Epoch: 2 [25350/36045]\tLoss: 3138.7991\n",
      "Training Epoch: 2 [25400/36045]\tLoss: 2886.7712\n",
      "Training Epoch: 2 [25450/36045]\tLoss: 3022.4229\n",
      "Training Epoch: 2 [25500/36045]\tLoss: 3233.8157\n",
      "Training Epoch: 2 [25550/36045]\tLoss: 3726.3787\n",
      "Training Epoch: 2 [25600/36045]\tLoss: 3723.6650\n",
      "Training Epoch: 2 [25650/36045]\tLoss: 3740.9773\n",
      "Training Epoch: 2 [25700/36045]\tLoss: 3858.8958\n",
      "Training Epoch: 2 [25750/36045]\tLoss: 3547.8352\n",
      "Training Epoch: 2 [25800/36045]\tLoss: 1982.8365\n",
      "Training Epoch: 2 [25850/36045]\tLoss: 1987.4420\n",
      "Training Epoch: 2 [25900/36045]\tLoss: 1885.1057\n",
      "Training Epoch: 2 [25950/36045]\tLoss: 1923.4410\n",
      "Training Epoch: 2 [26000/36045]\tLoss: 2392.4019\n",
      "Training Epoch: 2 [26050/36045]\tLoss: 3321.4661\n",
      "Training Epoch: 2 [26100/36045]\tLoss: 3320.5679\n",
      "Training Epoch: 2 [26150/36045]\tLoss: 3367.2056\n",
      "Training Epoch: 2 [26200/36045]\tLoss: 3294.5671\n",
      "Training Epoch: 2 [26250/36045]\tLoss: 3391.1038\n",
      "Training Epoch: 2 [26300/36045]\tLoss: 2811.3342\n",
      "Training Epoch: 2 [26350/36045]\tLoss: 2760.4011\n",
      "Training Epoch: 2 [26400/36045]\tLoss: 2747.8899\n",
      "Training Epoch: 2 [26450/36045]\tLoss: 2641.7351\n",
      "Training Epoch: 2 [26500/36045]\tLoss: 3316.7881\n",
      "Training Epoch: 2 [26550/36045]\tLoss: 3420.2534\n",
      "Training Epoch: 2 [26600/36045]\tLoss: 3401.4597\n",
      "Training Epoch: 2 [26650/36045]\tLoss: 3439.9646\n",
      "Training Epoch: 2 [26700/36045]\tLoss: 3430.7036\n",
      "Training Epoch: 2 [26750/36045]\tLoss: 3237.1406\n",
      "Training Epoch: 2 [26800/36045]\tLoss: 2490.4492\n",
      "Training Epoch: 2 [26850/36045]\tLoss: 2076.3950\n",
      "Training Epoch: 2 [26900/36045]\tLoss: 2119.3838\n",
      "Training Epoch: 2 [26950/36045]\tLoss: 2284.9431\n",
      "Training Epoch: 2 [27000/36045]\tLoss: 3286.9341\n",
      "Training Epoch: 2 [27050/36045]\tLoss: 3402.7358\n",
      "Training Epoch: 2 [27100/36045]\tLoss: 3370.5354\n",
      "Training Epoch: 2 [27150/36045]\tLoss: 3431.3154\n",
      "Training Epoch: 2 [27200/36045]\tLoss: 2797.3167\n",
      "Training Epoch: 2 [27250/36045]\tLoss: 2809.6450\n",
      "Training Epoch: 2 [27300/36045]\tLoss: 2742.9636\n",
      "Training Epoch: 2 [27350/36045]\tLoss: 2778.0005\n",
      "Training Epoch: 2 [27400/36045]\tLoss: 2700.6411\n",
      "Training Epoch: 2 [27450/36045]\tLoss: 3327.1709\n",
      "Training Epoch: 2 [27500/36045]\tLoss: 3511.5823\n",
      "Training Epoch: 2 [27550/36045]\tLoss: 3484.5403\n",
      "Training Epoch: 2 [27600/36045]\tLoss: 3450.0767\n",
      "Training Epoch: 2 [27650/36045]\tLoss: 3504.2556\n",
      "Training Epoch: 2 [27700/36045]\tLoss: 3568.7537\n",
      "Training Epoch: 2 [27750/36045]\tLoss: 3624.6411\n",
      "Training Epoch: 2 [27800/36045]\tLoss: 3607.7412\n",
      "Training Epoch: 2 [27850/36045]\tLoss: 3505.8340\n",
      "Training Epoch: 2 [27900/36045]\tLoss: 2978.2915\n",
      "Training Epoch: 2 [27950/36045]\tLoss: 2397.0784\n",
      "Training Epoch: 2 [28000/36045]\tLoss: 2330.9849\n",
      "Training Epoch: 2 [28050/36045]\tLoss: 2412.7510\n",
      "Training Epoch: 2 [28100/36045]\tLoss: 2400.7039\n",
      "Training Epoch: 2 [28150/36045]\tLoss: 2680.8577\n",
      "Training Epoch: 2 [28200/36045]\tLoss: 2778.5081\n",
      "Training Epoch: 2 [28250/36045]\tLoss: 2690.5063\n",
      "Training Epoch: 2 [28300/36045]\tLoss: 2628.1855\n",
      "Training Epoch: 2 [28350/36045]\tLoss: 2593.5103\n",
      "Training Epoch: 2 [28400/36045]\tLoss: 3177.9961\n",
      "Training Epoch: 2 [28450/36045]\tLoss: 2818.8875\n",
      "Training Epoch: 2 [28500/36045]\tLoss: 2314.3452\n",
      "Training Epoch: 2 [28550/36045]\tLoss: 2170.9937\n",
      "Training Epoch: 2 [28600/36045]\tLoss: 2662.6770\n",
      "Training Epoch: 2 [28650/36045]\tLoss: 3357.3384\n",
      "Training Epoch: 2 [28700/36045]\tLoss: 3400.9365\n",
      "Training Epoch: 2 [28750/36045]\tLoss: 3349.0706\n",
      "Training Epoch: 2 [28800/36045]\tLoss: 3426.8218\n",
      "Training Epoch: 2 [28850/36045]\tLoss: 2868.5923\n",
      "Training Epoch: 2 [28900/36045]\tLoss: 2205.0286\n",
      "Training Epoch: 2 [28950/36045]\tLoss: 2171.2939\n",
      "Training Epoch: 2 [29000/36045]\tLoss: 2192.4116\n",
      "Training Epoch: 2 [29050/36045]\tLoss: 2208.1433\n",
      "Training Epoch: 2 [29100/36045]\tLoss: 2251.7107\n",
      "Training Epoch: 2 [29150/36045]\tLoss: 2161.5115\n",
      "Training Epoch: 2 [29200/36045]\tLoss: 2088.7739\n",
      "Training Epoch: 2 [29250/36045]\tLoss: 2012.8682\n",
      "Training Epoch: 2 [29300/36045]\tLoss: 2505.9673\n",
      "Training Epoch: 2 [29350/36045]\tLoss: 3185.2827\n",
      "Training Epoch: 2 [29400/36045]\tLoss: 3332.7126\n",
      "Training Epoch: 2 [29450/36045]\tLoss: 3478.4709\n",
      "Training Epoch: 2 [29500/36045]\tLoss: 3467.4678\n",
      "Training Epoch: 2 [29550/36045]\tLoss: 3338.9231\n",
      "Training Epoch: 2 [29600/36045]\tLoss: 2892.9004\n",
      "Training Epoch: 2 [29650/36045]\tLoss: 2803.8167\n",
      "Training Epoch: 2 [29700/36045]\tLoss: 2491.8259\n",
      "Training Epoch: 2 [29750/36045]\tLoss: 2551.8838\n",
      "Training Epoch: 2 [29800/36045]\tLoss: 2594.1804\n",
      "Training Epoch: 2 [29850/36045]\tLoss: 2424.4832\n",
      "Training Epoch: 2 [29900/36045]\tLoss: 2498.1345\n",
      "Training Epoch: 2 [29950/36045]\tLoss: 2477.9700\n",
      "Training Epoch: 2 [30000/36045]\tLoss: 2517.5391\n",
      "Training Epoch: 2 [30050/36045]\tLoss: 2526.4697\n",
      "Training Epoch: 2 [30100/36045]\tLoss: 3540.1304\n",
      "Training Epoch: 2 [30150/36045]\tLoss: 3458.3765\n",
      "Training Epoch: 2 [30200/36045]\tLoss: 3372.5276\n",
      "Training Epoch: 2 [30250/36045]\tLoss: 3463.6492\n",
      "Training Epoch: 2 [30300/36045]\tLoss: 3478.4919\n",
      "Training Epoch: 2 [30350/36045]\tLoss: 2891.5413\n",
      "Training Epoch: 2 [30400/36045]\tLoss: 2810.2612\n",
      "Training Epoch: 2 [30450/36045]\tLoss: 2857.6599\n",
      "Training Epoch: 2 [30500/36045]\tLoss: 2695.2854\n",
      "Training Epoch: 2 [30550/36045]\tLoss: 2568.9670\n",
      "Training Epoch: 2 [30600/36045]\tLoss: 2361.8074\n",
      "Training Epoch: 2 [30650/36045]\tLoss: 2338.1670\n",
      "Training Epoch: 2 [30700/36045]\tLoss: 2382.5918\n",
      "Training Epoch: 2 [30750/36045]\tLoss: 2358.8245\n",
      "Training Epoch: 2 [30800/36045]\tLoss: 2274.0027\n",
      "Training Epoch: 2 [30850/36045]\tLoss: 2225.2554\n",
      "Training Epoch: 2 [30900/36045]\tLoss: 2310.2429\n",
      "Training Epoch: 2 [30950/36045]\tLoss: 2410.1418\n",
      "Training Epoch: 2 [31000/36045]\tLoss: 2432.0525\n",
      "Training Epoch: 2 [31050/36045]\tLoss: 2063.5576\n",
      "Training Epoch: 2 [31100/36045]\tLoss: 2031.2532\n",
      "Training Epoch: 2 [31150/36045]\tLoss: 2045.3882\n",
      "Training Epoch: 2 [31200/36045]\tLoss: 2605.1450\n",
      "Training Epoch: 2 [31250/36045]\tLoss: 3354.4482\n",
      "Training Epoch: 2 [31300/36045]\tLoss: 3292.2214\n",
      "Training Epoch: 2 [31350/36045]\tLoss: 3321.4312\n",
      "Training Epoch: 2 [31400/36045]\tLoss: 3265.0493\n",
      "Training Epoch: 2 [31450/36045]\tLoss: 3245.7109\n",
      "Training Epoch: 2 [31500/36045]\tLoss: 3369.1306\n",
      "Training Epoch: 2 [31550/36045]\tLoss: 3327.2139\n",
      "Training Epoch: 2 [31600/36045]\tLoss: 3208.7102\n",
      "Training Epoch: 2 [31650/36045]\tLoss: 3347.5852\n",
      "Training Epoch: 2 [31700/36045]\tLoss: 2529.9441\n",
      "Training Epoch: 2 [31750/36045]\tLoss: 2139.5588\n",
      "Training Epoch: 2 [31800/36045]\tLoss: 2055.2808\n",
      "Training Epoch: 2 [31850/36045]\tLoss: 2110.0979\n",
      "Training Epoch: 2 [31900/36045]\tLoss: 2944.6426\n",
      "Training Epoch: 2 [31950/36045]\tLoss: 3464.7788\n",
      "Training Epoch: 2 [32000/36045]\tLoss: 3748.1035\n",
      "Training Epoch: 2 [32050/36045]\tLoss: 3618.7268\n",
      "Training Epoch: 2 [32100/36045]\tLoss: 3537.0151\n",
      "Training Epoch: 2 [32150/36045]\tLoss: 3297.7690\n",
      "Training Epoch: 2 [32200/36045]\tLoss: 3360.4990\n",
      "Training Epoch: 2 [32250/36045]\tLoss: 3418.4243\n",
      "Training Epoch: 2 [32300/36045]\tLoss: 3364.3770\n",
      "Training Epoch: 2 [32350/36045]\tLoss: 3301.6199\n",
      "Training Epoch: 2 [32400/36045]\tLoss: 3081.0830\n",
      "Training Epoch: 2 [32450/36045]\tLoss: 2609.7925\n",
      "Training Epoch: 2 [32500/36045]\tLoss: 2535.0518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [32550/36045]\tLoss: 2530.0474\n",
      "Training Epoch: 2 [32600/36045]\tLoss: 2507.3513\n",
      "Training Epoch: 2 [32650/36045]\tLoss: 3084.0522\n",
      "Training Epoch: 2 [32700/36045]\tLoss: 3333.2952\n",
      "Training Epoch: 2 [32750/36045]\tLoss: 3252.2781\n",
      "Training Epoch: 2 [32800/36045]\tLoss: 3370.2185\n",
      "Training Epoch: 2 [32850/36045]\tLoss: 3071.2002\n",
      "Training Epoch: 2 [32900/36045]\tLoss: 2405.8789\n",
      "Training Epoch: 2 [32950/36045]\tLoss: 2517.8291\n",
      "Training Epoch: 2 [33000/36045]\tLoss: 2580.2544\n",
      "Training Epoch: 2 [33050/36045]\tLoss: 2338.2087\n",
      "Training Epoch: 2 [33100/36045]\tLoss: 2705.9036\n",
      "Training Epoch: 2 [33150/36045]\tLoss: 3456.8716\n",
      "Training Epoch: 2 [33200/36045]\tLoss: 3376.2485\n",
      "Training Epoch: 2 [33250/36045]\tLoss: 3504.5837\n",
      "Training Epoch: 2 [33300/36045]\tLoss: 3663.0845\n",
      "Training Epoch: 2 [33350/36045]\tLoss: 2989.8459\n",
      "Training Epoch: 2 [33400/36045]\tLoss: 2343.5500\n",
      "Training Epoch: 2 [33450/36045]\tLoss: 2314.2395\n",
      "Training Epoch: 2 [33500/36045]\tLoss: 2398.5208\n",
      "Training Epoch: 2 [33550/36045]\tLoss: 2540.5925\n",
      "Training Epoch: 2 [33600/36045]\tLoss: 2493.2815\n",
      "Training Epoch: 2 [33650/36045]\tLoss: 3058.0486\n",
      "Training Epoch: 2 [33700/36045]\tLoss: 2943.0356\n",
      "Training Epoch: 2 [33750/36045]\tLoss: 3142.5989\n",
      "Training Epoch: 2 [33800/36045]\tLoss: 3029.0249\n",
      "Training Epoch: 2 [33850/36045]\tLoss: 3069.3079\n",
      "Training Epoch: 2 [33900/36045]\tLoss: 3182.3037\n",
      "Training Epoch: 2 [33950/36045]\tLoss: 3229.6604\n",
      "Training Epoch: 2 [34000/36045]\tLoss: 3151.1299\n",
      "Training Epoch: 2 [34050/36045]\tLoss: 3227.2317\n",
      "Training Epoch: 2 [34100/36045]\tLoss: 3130.9709\n",
      "Training Epoch: 2 [34150/36045]\tLoss: 2945.4849\n",
      "Training Epoch: 2 [34200/36045]\tLoss: 2793.9412\n",
      "Training Epoch: 2 [34250/36045]\tLoss: 2802.4326\n",
      "Training Epoch: 2 [34300/36045]\tLoss: 2502.2166\n",
      "Training Epoch: 2 [34350/36045]\tLoss: 2532.5315\n",
      "Training Epoch: 2 [34400/36045]\tLoss: 2395.2483\n",
      "Training Epoch: 2 [34450/36045]\tLoss: 2226.0042\n",
      "Training Epoch: 2 [34500/36045]\tLoss: 2401.1938\n",
      "Training Epoch: 2 [34550/36045]\tLoss: 2333.5562\n",
      "Training Epoch: 2 [34600/36045]\tLoss: 2213.7878\n",
      "Training Epoch: 2 [34650/36045]\tLoss: 2458.4370\n",
      "Training Epoch: 2 [34700/36045]\tLoss: 2518.1130\n",
      "Training Epoch: 2 [34750/36045]\tLoss: 2259.3999\n",
      "Training Epoch: 2 [34800/36045]\tLoss: 2552.7222\n",
      "Training Epoch: 2 [34850/36045]\tLoss: 2660.3398\n",
      "Training Epoch: 2 [34900/36045]\tLoss: 3515.9902\n",
      "Training Epoch: 2 [34950/36045]\tLoss: 3536.7283\n",
      "Training Epoch: 2 [35000/36045]\tLoss: 3557.2910\n",
      "Training Epoch: 2 [35050/36045]\tLoss: 3450.1765\n",
      "Training Epoch: 2 [35100/36045]\tLoss: 2558.8074\n",
      "Training Epoch: 2 [35150/36045]\tLoss: 2518.5579\n",
      "Training Epoch: 2 [35200/36045]\tLoss: 2249.3723\n",
      "Training Epoch: 2 [35250/36045]\tLoss: 2352.5896\n",
      "Training Epoch: 2 [35300/36045]\tLoss: 2360.6160\n",
      "Training Epoch: 2 [35350/36045]\tLoss: 2947.1636\n",
      "Training Epoch: 2 [35400/36045]\tLoss: 3206.5305\n",
      "Training Epoch: 2 [35450/36045]\tLoss: 3058.6929\n",
      "Training Epoch: 2 [35500/36045]\tLoss: 2988.6260\n",
      "Training Epoch: 2 [35550/36045]\tLoss: 2954.0264\n",
      "Training Epoch: 2 [35600/36045]\tLoss: 3071.9087\n",
      "Training Epoch: 2 [35650/36045]\tLoss: 3245.3003\n",
      "Training Epoch: 2 [35700/36045]\tLoss: 3045.7300\n",
      "Training Epoch: 2 [35750/36045]\tLoss: 3130.6707\n",
      "Training Epoch: 2 [35800/36045]\tLoss: 3193.7786\n",
      "Training Epoch: 2 [35850/36045]\tLoss: 3138.9419\n",
      "Training Epoch: 2 [35900/36045]\tLoss: 3243.8474\n",
      "Training Epoch: 2 [35950/36045]\tLoss: 3225.2002\n",
      "Training Epoch: 2 [36000/36045]\tLoss: 3131.6958\n",
      "Training Epoch: 2 [36045/36045]\tLoss: 3060.8025\n",
      "Training Epoch: 2 [4004/4004]\tLoss: 3077.6538\n",
      "Training Epoch: 3 [50/36045]\tLoss: 3134.3259\n",
      "Training Epoch: 3 [100/36045]\tLoss: 3073.0791\n",
      "Training Epoch: 3 [150/36045]\tLoss: 3043.6458\n",
      "Training Epoch: 3 [200/36045]\tLoss: 3045.3811\n",
      "Training Epoch: 3 [250/36045]\tLoss: 3261.0972\n",
      "Training Epoch: 3 [300/36045]\tLoss: 3342.5664\n",
      "Training Epoch: 3 [350/36045]\tLoss: 3210.6577\n",
      "Training Epoch: 3 [400/36045]\tLoss: 3288.6650\n",
      "Training Epoch: 3 [450/36045]\tLoss: 3217.7422\n",
      "Training Epoch: 3 [500/36045]\tLoss: 3102.6516\n",
      "Training Epoch: 3 [550/36045]\tLoss: 3085.0044\n",
      "Training Epoch: 3 [600/36045]\tLoss: 2944.4739\n",
      "Training Epoch: 3 [650/36045]\tLoss: 3090.4402\n",
      "Training Epoch: 3 [700/36045]\tLoss: 3091.6284\n",
      "Training Epoch: 3 [750/36045]\tLoss: 3249.3840\n",
      "Training Epoch: 3 [800/36045]\tLoss: 3329.3179\n",
      "Training Epoch: 3 [850/36045]\tLoss: 3301.4060\n",
      "Training Epoch: 3 [900/36045]\tLoss: 3062.8242\n",
      "Training Epoch: 3 [950/36045]\tLoss: 2956.0950\n",
      "Training Epoch: 3 [1000/36045]\tLoss: 2855.7498\n",
      "Training Epoch: 3 [1050/36045]\tLoss: 2871.2810\n",
      "Training Epoch: 3 [1100/36045]\tLoss: 2777.5630\n",
      "Training Epoch: 3 [1150/36045]\tLoss: 2772.6228\n",
      "Training Epoch: 3 [1200/36045]\tLoss: 2874.5049\n",
      "Training Epoch: 3 [1250/36045]\tLoss: 3110.4512\n",
      "Training Epoch: 3 [1300/36045]\tLoss: 3089.7749\n",
      "Training Epoch: 3 [1350/36045]\tLoss: 3090.8088\n",
      "Training Epoch: 3 [1400/36045]\tLoss: 3250.0225\n",
      "Training Epoch: 3 [1450/36045]\tLoss: 3133.9866\n",
      "Training Epoch: 3 [1500/36045]\tLoss: 2953.8679\n",
      "Training Epoch: 3 [1550/36045]\tLoss: 3131.9355\n",
      "Training Epoch: 3 [1600/36045]\tLoss: 3135.0854\n",
      "Training Epoch: 3 [1650/36045]\tLoss: 3147.1389\n",
      "Training Epoch: 3 [1700/36045]\tLoss: 3156.9465\n",
      "Training Epoch: 3 [1750/36045]\tLoss: 3123.7659\n",
      "Training Epoch: 3 [1800/36045]\tLoss: 3111.9202\n",
      "Training Epoch: 3 [1850/36045]\tLoss: 3231.5134\n",
      "Training Epoch: 3 [1900/36045]\tLoss: 3079.9514\n",
      "Training Epoch: 3 [1950/36045]\tLoss: 3090.4692\n",
      "Training Epoch: 3 [2000/36045]\tLoss: 2899.7124\n",
      "Training Epoch: 3 [2050/36045]\tLoss: 2867.5461\n",
      "Training Epoch: 3 [2100/36045]\tLoss: 3054.7969\n",
      "Training Epoch: 3 [2150/36045]\tLoss: 2990.7639\n",
      "Training Epoch: 3 [2200/36045]\tLoss: 2679.6499\n",
      "Training Epoch: 3 [2250/36045]\tLoss: 2426.1624\n",
      "Training Epoch: 3 [2300/36045]\tLoss: 2521.4104\n",
      "Training Epoch: 3 [2350/36045]\tLoss: 2471.4087\n",
      "Training Epoch: 3 [2400/36045]\tLoss: 2513.0864\n",
      "Training Epoch: 3 [2450/36045]\tLoss: 3064.2410\n",
      "Training Epoch: 3 [2500/36045]\tLoss: 3200.0542\n",
      "Training Epoch: 3 [2550/36045]\tLoss: 3145.9358\n",
      "Training Epoch: 3 [2600/36045]\tLoss: 3037.2930\n",
      "Training Epoch: 3 [2650/36045]\tLoss: 3362.9741\n",
      "Training Epoch: 3 [2700/36045]\tLoss: 3463.7810\n",
      "Training Epoch: 3 [2750/36045]\tLoss: 3607.0613\n",
      "Training Epoch: 3 [2800/36045]\tLoss: 3656.4978\n",
      "Training Epoch: 3 [2850/36045]\tLoss: 3331.4294\n",
      "Training Epoch: 3 [2900/36045]\tLoss: 3388.7271\n",
      "Training Epoch: 3 [2950/36045]\tLoss: 3346.4409\n",
      "Training Epoch: 3 [3000/36045]\tLoss: 3325.9014\n",
      "Training Epoch: 3 [3050/36045]\tLoss: 3395.1584\n",
      "Training Epoch: 3 [3100/36045]\tLoss: 3156.7866\n",
      "Training Epoch: 3 [3150/36045]\tLoss: 2538.8032\n",
      "Training Epoch: 3 [3200/36045]\tLoss: 2608.5088\n",
      "Training Epoch: 3 [3250/36045]\tLoss: 2411.4722\n",
      "Training Epoch: 3 [3300/36045]\tLoss: 2313.4878\n",
      "Training Epoch: 3 [3350/36045]\tLoss: 2460.0745\n",
      "Training Epoch: 3 [3400/36045]\tLoss: 2601.2463\n",
      "Training Epoch: 3 [3450/36045]\tLoss: 2771.2944\n",
      "Training Epoch: 3 [3500/36045]\tLoss: 2722.7520\n",
      "Training Epoch: 3 [3550/36045]\tLoss: 2687.0256\n",
      "Training Epoch: 3 [3600/36045]\tLoss: 2891.4819\n",
      "Training Epoch: 3 [3650/36045]\tLoss: 3214.8000\n",
      "Training Epoch: 3 [3700/36045]\tLoss: 3210.4470\n",
      "Training Epoch: 3 [3750/36045]\tLoss: 3149.3953\n",
      "Training Epoch: 3 [3800/36045]\tLoss: 3048.8840\n",
      "Training Epoch: 3 [3850/36045]\tLoss: 2854.5857\n",
      "Training Epoch: 3 [3900/36045]\tLoss: 2845.9087\n",
      "Training Epoch: 3 [3950/36045]\tLoss: 2794.9399\n",
      "Training Epoch: 3 [4000/36045]\tLoss: 2820.3503\n",
      "Training Epoch: 3 [4050/36045]\tLoss: 2632.4150\n",
      "Training Epoch: 3 [4100/36045]\tLoss: 2585.6140\n",
      "Training Epoch: 3 [4150/36045]\tLoss: 2699.4209\n",
      "Training Epoch: 3 [4200/36045]\tLoss: 2679.8501\n",
      "Training Epoch: 3 [4250/36045]\tLoss: 2809.4919\n",
      "Training Epoch: 3 [4300/36045]\tLoss: 2987.2610\n",
      "Training Epoch: 3 [4350/36045]\tLoss: 3019.4207\n",
      "Training Epoch: 3 [4400/36045]\tLoss: 2906.7395\n",
      "Training Epoch: 3 [4450/36045]\tLoss: 2944.4106\n",
      "Training Epoch: 3 [4500/36045]\tLoss: 2964.1814\n",
      "Training Epoch: 3 [4550/36045]\tLoss: 3013.4521\n",
      "Training Epoch: 3 [4600/36045]\tLoss: 3077.4458\n",
      "Training Epoch: 3 [4650/36045]\tLoss: 3028.6421\n",
      "Training Epoch: 3 [4700/36045]\tLoss: 2909.1743\n",
      "Training Epoch: 3 [4750/36045]\tLoss: 2900.4236\n",
      "Training Epoch: 3 [4800/36045]\tLoss: 3004.5083\n",
      "Training Epoch: 3 [4850/36045]\tLoss: 2963.6782\n",
      "Training Epoch: 3 [4900/36045]\tLoss: 2893.3506\n",
      "Training Epoch: 3 [4950/36045]\tLoss: 2903.6636\n",
      "Training Epoch: 3 [5000/36045]\tLoss: 3020.9304\n",
      "Training Epoch: 3 [5050/36045]\tLoss: 2905.6155\n",
      "Training Epoch: 3 [5100/36045]\tLoss: 3004.6628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [5150/36045]\tLoss: 2894.4595\n",
      "Training Epoch: 3 [5200/36045]\tLoss: 2910.8542\n",
      "Training Epoch: 3 [5250/36045]\tLoss: 2913.1074\n",
      "Training Epoch: 3 [5300/36045]\tLoss: 2971.2434\n",
      "Training Epoch: 3 [5350/36045]\tLoss: 2997.5798\n",
      "Training Epoch: 3 [5400/36045]\tLoss: 2932.7771\n",
      "Training Epoch: 3 [5450/36045]\tLoss: 2684.5696\n",
      "Training Epoch: 3 [5500/36045]\tLoss: 2789.4534\n",
      "Training Epoch: 3 [5550/36045]\tLoss: 2809.0327\n",
      "Training Epoch: 3 [5600/36045]\tLoss: 3021.9626\n",
      "Training Epoch: 3 [5650/36045]\tLoss: 2945.9075\n",
      "Training Epoch: 3 [5700/36045]\tLoss: 2676.7874\n",
      "Training Epoch: 3 [5750/36045]\tLoss: 2755.2705\n",
      "Training Epoch: 3 [5800/36045]\tLoss: 2865.0334\n",
      "Training Epoch: 3 [5850/36045]\tLoss: 2762.1504\n",
      "Training Epoch: 3 [5900/36045]\tLoss: 3258.3999\n",
      "Training Epoch: 3 [5950/36045]\tLoss: 3316.4216\n",
      "Training Epoch: 3 [6000/36045]\tLoss: 3259.6082\n",
      "Training Epoch: 3 [6050/36045]\tLoss: 3155.6157\n",
      "Training Epoch: 3 [6100/36045]\tLoss: 3147.5000\n",
      "Training Epoch: 3 [6150/36045]\tLoss: 3019.2036\n",
      "Training Epoch: 3 [6200/36045]\tLoss: 2943.1792\n",
      "Training Epoch: 3 [6250/36045]\tLoss: 2987.2505\n",
      "Training Epoch: 3 [6300/36045]\tLoss: 3073.5679\n",
      "Training Epoch: 3 [6350/36045]\tLoss: 3129.2676\n",
      "Training Epoch: 3 [6400/36045]\tLoss: 2835.9976\n",
      "Training Epoch: 3 [6450/36045]\tLoss: 2649.9702\n",
      "Training Epoch: 3 [6500/36045]\tLoss: 2678.1885\n",
      "Training Epoch: 3 [6550/36045]\tLoss: 2755.5237\n",
      "Training Epoch: 3 [6600/36045]\tLoss: 2740.7288\n",
      "Training Epoch: 3 [6650/36045]\tLoss: 2951.0044\n",
      "Training Epoch: 3 [6700/36045]\tLoss: 3091.6489\n",
      "Training Epoch: 3 [6750/36045]\tLoss: 3012.4438\n",
      "Training Epoch: 3 [6800/36045]\tLoss: 2986.2588\n",
      "Training Epoch: 3 [6850/36045]\tLoss: 2978.6755\n",
      "Training Epoch: 3 [6900/36045]\tLoss: 2632.3999\n",
      "Training Epoch: 3 [6950/36045]\tLoss: 2517.2175\n",
      "Training Epoch: 3 [7000/36045]\tLoss: 2583.5876\n",
      "Training Epoch: 3 [7050/36045]\tLoss: 2650.0227\n",
      "Training Epoch: 3 [7100/36045]\tLoss: 2785.0747\n",
      "Training Epoch: 3 [7150/36045]\tLoss: 2867.2737\n",
      "Training Epoch: 3 [7200/36045]\tLoss: 2903.2236\n",
      "Training Epoch: 3 [7250/36045]\tLoss: 2846.7620\n",
      "Training Epoch: 3 [7300/36045]\tLoss: 2783.9795\n",
      "Training Epoch: 3 [7350/36045]\tLoss: 2807.8767\n",
      "Training Epoch: 3 [7400/36045]\tLoss: 2699.3223\n",
      "Training Epoch: 3 [7450/36045]\tLoss: 2652.9412\n",
      "Training Epoch: 3 [7500/36045]\tLoss: 2650.3213\n",
      "Training Epoch: 3 [7550/36045]\tLoss: 2544.9121\n",
      "Training Epoch: 3 [7600/36045]\tLoss: 2680.4871\n",
      "Training Epoch: 3 [7650/36045]\tLoss: 2783.5835\n",
      "Training Epoch: 3 [7700/36045]\tLoss: 2638.0627\n",
      "Training Epoch: 3 [7750/36045]\tLoss: 2713.2571\n",
      "Training Epoch: 3 [7800/36045]\tLoss: 2671.4319\n",
      "Training Epoch: 3 [7850/36045]\tLoss: 2563.7358\n",
      "Training Epoch: 3 [7900/36045]\tLoss: 2748.7556\n",
      "Training Epoch: 3 [7950/36045]\tLoss: 2729.0283\n",
      "Training Epoch: 3 [8000/36045]\tLoss: 2753.6323\n",
      "Training Epoch: 3 [8050/36045]\tLoss: 2626.9744\n",
      "Training Epoch: 3 [8100/36045]\tLoss: 2683.8264\n",
      "Training Epoch: 3 [8150/36045]\tLoss: 2921.2695\n",
      "Training Epoch: 3 [8200/36045]\tLoss: 2900.4041\n",
      "Training Epoch: 3 [8250/36045]\tLoss: 2797.9539\n",
      "Training Epoch: 3 [8300/36045]\tLoss: 2960.1516\n",
      "Training Epoch: 3 [8350/36045]\tLoss: 2765.3701\n",
      "Training Epoch: 3 [8400/36045]\tLoss: 2702.2427\n",
      "Training Epoch: 3 [8450/36045]\tLoss: 2604.8054\n",
      "Training Epoch: 3 [8500/36045]\tLoss: 2673.4961\n",
      "Training Epoch: 3 [8550/36045]\tLoss: 2538.7222\n",
      "Training Epoch: 3 [8600/36045]\tLoss: 2536.2603\n",
      "Training Epoch: 3 [8650/36045]\tLoss: 2804.3206\n",
      "Training Epoch: 3 [8700/36045]\tLoss: 2906.9587\n",
      "Training Epoch: 3 [8750/36045]\tLoss: 2897.5874\n",
      "Training Epoch: 3 [8800/36045]\tLoss: 2908.9202\n",
      "Training Epoch: 3 [8850/36045]\tLoss: 2868.3291\n",
      "Training Epoch: 3 [8900/36045]\tLoss: 2617.2666\n",
      "Training Epoch: 3 [8950/36045]\tLoss: 2662.1702\n",
      "Training Epoch: 3 [9000/36045]\tLoss: 2691.1611\n",
      "Training Epoch: 3 [9050/36045]\tLoss: 2678.3206\n",
      "Training Epoch: 3 [9100/36045]\tLoss: 2680.9102\n",
      "Training Epoch: 3 [9150/36045]\tLoss: 2036.8207\n",
      "Training Epoch: 3 [9200/36045]\tLoss: 1604.6215\n",
      "Training Epoch: 3 [9250/36045]\tLoss: 1727.2789\n",
      "Training Epoch: 3 [9300/36045]\tLoss: 1750.5148\n",
      "Training Epoch: 3 [9350/36045]\tLoss: 1622.2341\n",
      "Training Epoch: 3 [9400/36045]\tLoss: 3027.7961\n",
      "Training Epoch: 3 [9450/36045]\tLoss: 3163.3093\n",
      "Training Epoch: 3 [9500/36045]\tLoss: 3106.0820\n",
      "Training Epoch: 3 [9550/36045]\tLoss: 3337.5959\n",
      "Training Epoch: 3 [9600/36045]\tLoss: 2338.2085\n",
      "Training Epoch: 3 [9650/36045]\tLoss: 2291.9731\n",
      "Training Epoch: 3 [9700/36045]\tLoss: 2312.8545\n",
      "Training Epoch: 3 [9750/36045]\tLoss: 2242.0208\n",
      "Training Epoch: 3 [9800/36045]\tLoss: 3085.9468\n",
      "Training Epoch: 3 [9850/36045]\tLoss: 3297.1963\n",
      "Training Epoch: 3 [9900/36045]\tLoss: 3340.8635\n",
      "Training Epoch: 3 [9950/36045]\tLoss: 3226.0024\n",
      "Training Epoch: 3 [10000/36045]\tLoss: 3023.0981\n",
      "Training Epoch: 3 [10050/36045]\tLoss: 2630.0889\n",
      "Training Epoch: 3 [10100/36045]\tLoss: 2637.4561\n",
      "Training Epoch: 3 [10150/36045]\tLoss: 2658.0056\n",
      "Training Epoch: 3 [10200/36045]\tLoss: 2624.9084\n",
      "Training Epoch: 3 [10250/36045]\tLoss: 3099.5637\n",
      "Training Epoch: 3 [10300/36045]\tLoss: 3094.5913\n",
      "Training Epoch: 3 [10350/36045]\tLoss: 3171.8491\n",
      "Training Epoch: 3 [10400/36045]\tLoss: 3182.2544\n",
      "Training Epoch: 3 [10450/36045]\tLoss: 2938.4739\n",
      "Training Epoch: 3 [10500/36045]\tLoss: 2494.8589\n",
      "Training Epoch: 3 [10550/36045]\tLoss: 2433.6514\n",
      "Training Epoch: 3 [10600/36045]\tLoss: 2537.4392\n",
      "Training Epoch: 3 [10650/36045]\tLoss: 2559.6035\n",
      "Training Epoch: 3 [10700/36045]\tLoss: 2808.5481\n",
      "Training Epoch: 3 [10750/36045]\tLoss: 2957.2192\n",
      "Training Epoch: 3 [10800/36045]\tLoss: 2835.4111\n",
      "Training Epoch: 3 [10850/36045]\tLoss: 2913.4031\n",
      "Training Epoch: 3 [10900/36045]\tLoss: 3047.1270\n",
      "Training Epoch: 3 [10950/36045]\tLoss: 2401.4001\n",
      "Training Epoch: 3 [11000/36045]\tLoss: 2414.2358\n",
      "Training Epoch: 3 [11050/36045]\tLoss: 2522.4028\n",
      "Training Epoch: 3 [11100/36045]\tLoss: 2509.3401\n",
      "Training Epoch: 3 [11150/36045]\tLoss: 2682.4539\n",
      "Training Epoch: 3 [11200/36045]\tLoss: 2767.7803\n",
      "Training Epoch: 3 [11250/36045]\tLoss: 2836.9094\n",
      "Training Epoch: 3 [11300/36045]\tLoss: 2770.3325\n",
      "Training Epoch: 3 [11350/36045]\tLoss: 2792.4121\n",
      "Training Epoch: 3 [11400/36045]\tLoss: 2701.4521\n",
      "Training Epoch: 3 [11450/36045]\tLoss: 2645.7783\n",
      "Training Epoch: 3 [11500/36045]\tLoss: 2610.6414\n",
      "Training Epoch: 3 [11550/36045]\tLoss: 2660.4880\n",
      "Training Epoch: 3 [11600/36045]\tLoss: 2801.1216\n",
      "Training Epoch: 3 [11650/36045]\tLoss: 2938.6516\n",
      "Training Epoch: 3 [11700/36045]\tLoss: 2903.0166\n",
      "Training Epoch: 3 [11750/36045]\tLoss: 2902.8047\n",
      "Training Epoch: 3 [11800/36045]\tLoss: 3062.9121\n",
      "Training Epoch: 3 [11850/36045]\tLoss: 3089.0076\n",
      "Training Epoch: 3 [11900/36045]\tLoss: 3423.4917\n",
      "Training Epoch: 3 [11950/36045]\tLoss: 3380.2727\n",
      "Training Epoch: 3 [12000/36045]\tLoss: 3483.5100\n",
      "Training Epoch: 3 [12050/36045]\tLoss: 3370.9546\n",
      "Training Epoch: 3 [12100/36045]\tLoss: 2572.2480\n",
      "Training Epoch: 3 [12150/36045]\tLoss: 2235.0059\n",
      "Training Epoch: 3 [12200/36045]\tLoss: 2211.7930\n",
      "Training Epoch: 3 [12250/36045]\tLoss: 2242.4014\n",
      "Training Epoch: 3 [12300/36045]\tLoss: 2615.6941\n",
      "Training Epoch: 3 [12350/36045]\tLoss: 2756.4014\n",
      "Training Epoch: 3 [12400/36045]\tLoss: 2760.0193\n",
      "Training Epoch: 3 [12450/36045]\tLoss: 2702.1978\n",
      "Training Epoch: 3 [12500/36045]\tLoss: 2828.2747\n",
      "Training Epoch: 3 [12550/36045]\tLoss: 2744.7397\n",
      "Training Epoch: 3 [12600/36045]\tLoss: 2661.7476\n",
      "Training Epoch: 3 [12650/36045]\tLoss: 2704.9229\n",
      "Training Epoch: 3 [12700/36045]\tLoss: 2744.6040\n",
      "Training Epoch: 3 [12750/36045]\tLoss: 2775.4963\n",
      "Training Epoch: 3 [12800/36045]\tLoss: 2678.7314\n",
      "Training Epoch: 3 [12850/36045]\tLoss: 2792.2727\n",
      "Training Epoch: 3 [12900/36045]\tLoss: 2732.8979\n",
      "Training Epoch: 3 [12950/36045]\tLoss: 2710.8005\n",
      "Training Epoch: 3 [13000/36045]\tLoss: 2733.8020\n",
      "Training Epoch: 3 [13050/36045]\tLoss: 2647.4082\n",
      "Training Epoch: 3 [13100/36045]\tLoss: 2732.0122\n",
      "Training Epoch: 3 [13150/36045]\tLoss: 2689.1819\n",
      "Training Epoch: 3 [13200/36045]\tLoss: 2566.8569\n",
      "Training Epoch: 3 [13250/36045]\tLoss: 2648.8877\n",
      "Training Epoch: 3 [13300/36045]\tLoss: 2730.2363\n",
      "Training Epoch: 3 [13350/36045]\tLoss: 2667.8816\n",
      "Training Epoch: 3 [13400/36045]\tLoss: 2682.3352\n",
      "Training Epoch: 3 [13450/36045]\tLoss: 2662.6687\n",
      "Training Epoch: 3 [13500/36045]\tLoss: 2786.4949\n",
      "Training Epoch: 3 [13550/36045]\tLoss: 2973.3572\n",
      "Training Epoch: 3 [13600/36045]\tLoss: 3008.5913\n",
      "Training Epoch: 3 [13650/36045]\tLoss: 3096.4968\n",
      "Training Epoch: 3 [13700/36045]\tLoss: 2849.3481\n",
      "Training Epoch: 3 [13750/36045]\tLoss: 2663.8552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [13800/36045]\tLoss: 2636.7092\n",
      "Training Epoch: 3 [13850/36045]\tLoss: 2631.5017\n",
      "Training Epoch: 3 [13900/36045]\tLoss: 2665.7534\n",
      "Training Epoch: 3 [13950/36045]\tLoss: 2652.5129\n",
      "Training Epoch: 3 [14000/36045]\tLoss: 2760.4004\n",
      "Training Epoch: 3 [14050/36045]\tLoss: 2648.7568\n",
      "Training Epoch: 3 [14100/36045]\tLoss: 2637.4800\n",
      "Training Epoch: 3 [14150/36045]\tLoss: 2611.3977\n",
      "Training Epoch: 3 [14200/36045]\tLoss: 2761.2874\n",
      "Training Epoch: 3 [14250/36045]\tLoss: 3054.7856\n",
      "Training Epoch: 3 [14300/36045]\tLoss: 3066.7246\n",
      "Training Epoch: 3 [14350/36045]\tLoss: 2940.0808\n",
      "Training Epoch: 3 [14400/36045]\tLoss: 2901.5220\n",
      "Training Epoch: 3 [14450/36045]\tLoss: 2996.4285\n",
      "Training Epoch: 3 [14500/36045]\tLoss: 2989.7578\n",
      "Training Epoch: 3 [14550/36045]\tLoss: 3104.7722\n",
      "Training Epoch: 3 [14600/36045]\tLoss: 3078.6948\n",
      "Training Epoch: 3 [14650/36045]\tLoss: 3029.3105\n",
      "Training Epoch: 3 [14700/36045]\tLoss: 2816.7168\n",
      "Training Epoch: 3 [14750/36045]\tLoss: 2441.0249\n",
      "Training Epoch: 3 [14800/36045]\tLoss: 2405.2776\n",
      "Training Epoch: 3 [14850/36045]\tLoss: 2408.3821\n",
      "Training Epoch: 3 [14900/36045]\tLoss: 2396.4885\n",
      "Training Epoch: 3 [14950/36045]\tLoss: 2477.8770\n",
      "Training Epoch: 3 [15000/36045]\tLoss: 2625.0879\n",
      "Training Epoch: 3 [15050/36045]\tLoss: 2651.9961\n",
      "Training Epoch: 3 [15100/36045]\tLoss: 2654.6697\n",
      "Training Epoch: 3 [15150/36045]\tLoss: 2404.4377\n",
      "Training Epoch: 3 [15200/36045]\tLoss: 2118.0574\n",
      "Training Epoch: 3 [15250/36045]\tLoss: 2181.6841\n",
      "Training Epoch: 3 [15300/36045]\tLoss: 2156.2090\n",
      "Training Epoch: 3 [15350/36045]\tLoss: 2174.1677\n",
      "Training Epoch: 3 [15400/36045]\tLoss: 2610.3977\n",
      "Training Epoch: 3 [15450/36045]\tLoss: 2596.4570\n",
      "Training Epoch: 3 [15500/36045]\tLoss: 2635.8022\n",
      "Training Epoch: 3 [15550/36045]\tLoss: 2571.8572\n",
      "Training Epoch: 3 [15600/36045]\tLoss: 2596.3384\n",
      "Training Epoch: 3 [15650/36045]\tLoss: 2673.2244\n",
      "Training Epoch: 3 [15700/36045]\tLoss: 2616.2356\n",
      "Training Epoch: 3 [15750/36045]\tLoss: 2569.1401\n",
      "Training Epoch: 3 [15800/36045]\tLoss: 2129.8879\n",
      "Training Epoch: 3 [15850/36045]\tLoss: 2085.9888\n",
      "Training Epoch: 3 [15900/36045]\tLoss: 2097.1182\n",
      "Training Epoch: 3 [15950/36045]\tLoss: 2163.6094\n",
      "Training Epoch: 3 [16000/36045]\tLoss: 2177.3660\n",
      "Training Epoch: 3 [16050/36045]\tLoss: 2059.7224\n",
      "Training Epoch: 3 [16100/36045]\tLoss: 1972.3672\n",
      "Training Epoch: 3 [16150/36045]\tLoss: 1904.4209\n",
      "Training Epoch: 3 [16200/36045]\tLoss: 2201.3140\n",
      "Training Epoch: 3 [16250/36045]\tLoss: 2312.1965\n",
      "Training Epoch: 3 [16300/36045]\tLoss: 2461.4724\n",
      "Training Epoch: 3 [16350/36045]\tLoss: 2471.1199\n",
      "Training Epoch: 3 [16400/36045]\tLoss: 2418.2900\n",
      "Training Epoch: 3 [16450/36045]\tLoss: 2455.1853\n",
      "Training Epoch: 3 [16500/36045]\tLoss: 2487.9365\n",
      "Training Epoch: 3 [16550/36045]\tLoss: 2395.1514\n",
      "Training Epoch: 3 [16600/36045]\tLoss: 2490.8743\n",
      "Training Epoch: 3 [16650/36045]\tLoss: 2539.5879\n",
      "Training Epoch: 3 [16700/36045]\tLoss: 2518.1672\n",
      "Training Epoch: 3 [16750/36045]\tLoss: 2470.0156\n",
      "Training Epoch: 3 [16800/36045]\tLoss: 2545.4238\n",
      "Training Epoch: 3 [16850/36045]\tLoss: 2447.8145\n",
      "Training Epoch: 3 [16900/36045]\tLoss: 2400.3174\n",
      "Training Epoch: 3 [16950/36045]\tLoss: 2339.2500\n",
      "Training Epoch: 3 [17000/36045]\tLoss: 2332.4683\n",
      "Training Epoch: 3 [17050/36045]\tLoss: 2479.5930\n",
      "Training Epoch: 3 [17100/36045]\tLoss: 2502.0989\n",
      "Training Epoch: 3 [17150/36045]\tLoss: 2336.6521\n",
      "Training Epoch: 3 [17200/36045]\tLoss: 2348.0684\n",
      "Training Epoch: 3 [17250/36045]\tLoss: 2396.7104\n",
      "Training Epoch: 3 [17300/36045]\tLoss: 2556.3296\n",
      "Training Epoch: 3 [17350/36045]\tLoss: 2312.4724\n",
      "Training Epoch: 3 [17400/36045]\tLoss: 2293.3540\n",
      "Training Epoch: 3 [17450/36045]\tLoss: 2389.5273\n",
      "Training Epoch: 3 [17500/36045]\tLoss: 2388.3535\n",
      "Training Epoch: 3 [17550/36045]\tLoss: 2401.1775\n",
      "Training Epoch: 3 [17600/36045]\tLoss: 2375.4153\n",
      "Training Epoch: 3 [17650/36045]\tLoss: 2490.8164\n",
      "Training Epoch: 3 [17700/36045]\tLoss: 2390.2271\n",
      "Training Epoch: 3 [17750/36045]\tLoss: 2437.2405\n",
      "Training Epoch: 3 [17800/36045]\tLoss: 2418.2361\n",
      "Training Epoch: 3 [17850/36045]\tLoss: 2206.8032\n",
      "Training Epoch: 3 [17900/36045]\tLoss: 2239.7930\n",
      "Training Epoch: 3 [17950/36045]\tLoss: 2259.3691\n",
      "Training Epoch: 3 [18000/36045]\tLoss: 2207.8589\n",
      "Training Epoch: 3 [18050/36045]\tLoss: 2514.5417\n",
      "Training Epoch: 3 [18100/36045]\tLoss: 2608.9380\n",
      "Training Epoch: 3 [18150/36045]\tLoss: 2581.3904\n",
      "Training Epoch: 3 [18200/36045]\tLoss: 2549.6638\n",
      "Training Epoch: 3 [18250/36045]\tLoss: 2619.5212\n",
      "Training Epoch: 3 [18300/36045]\tLoss: 2471.7063\n",
      "Training Epoch: 3 [18350/36045]\tLoss: 2509.5764\n",
      "Training Epoch: 3 [18400/36045]\tLoss: 2563.0437\n",
      "Training Epoch: 3 [18450/36045]\tLoss: 2463.0229\n",
      "Training Epoch: 3 [18500/36045]\tLoss: 2395.2097\n",
      "Training Epoch: 3 [18550/36045]\tLoss: 2286.6802\n",
      "Training Epoch: 3 [18600/36045]\tLoss: 2226.2471\n",
      "Training Epoch: 3 [18650/36045]\tLoss: 2336.8625\n",
      "Training Epoch: 3 [18700/36045]\tLoss: 2431.5437\n",
      "Training Epoch: 3 [18750/36045]\tLoss: 2436.3174\n",
      "Training Epoch: 3 [18800/36045]\tLoss: 2546.7952\n",
      "Training Epoch: 3 [18850/36045]\tLoss: 2407.4556\n",
      "Training Epoch: 3 [18900/36045]\tLoss: 2544.9033\n",
      "Training Epoch: 3 [18950/36045]\tLoss: 2478.7939\n",
      "Training Epoch: 3 [19000/36045]\tLoss: 2364.7246\n",
      "Training Epoch: 3 [19050/36045]\tLoss: 2264.4194\n",
      "Training Epoch: 3 [19100/36045]\tLoss: 2326.4316\n",
      "Training Epoch: 3 [19150/36045]\tLoss: 2288.1592\n",
      "Training Epoch: 3 [19200/36045]\tLoss: 2310.7783\n",
      "Training Epoch: 3 [19250/36045]\tLoss: 2282.5457\n",
      "Training Epoch: 3 [19300/36045]\tLoss: 2366.5127\n",
      "Training Epoch: 3 [19350/36045]\tLoss: 2275.7576\n",
      "Training Epoch: 3 [19400/36045]\tLoss: 2351.4219\n",
      "Training Epoch: 3 [19450/36045]\tLoss: 2317.8027\n",
      "Training Epoch: 3 [19500/36045]\tLoss: 2341.7690\n",
      "Training Epoch: 3 [19550/36045]\tLoss: 2367.9282\n",
      "Training Epoch: 3 [19600/36045]\tLoss: 2452.4827\n",
      "Training Epoch: 3 [19650/36045]\tLoss: 2906.1929\n",
      "Training Epoch: 3 [19700/36045]\tLoss: 2862.2781\n",
      "Training Epoch: 3 [19750/36045]\tLoss: 2843.8669\n",
      "Training Epoch: 3 [19800/36045]\tLoss: 2785.7861\n",
      "Training Epoch: 3 [19850/36045]\tLoss: 2106.2214\n",
      "Training Epoch: 3 [19900/36045]\tLoss: 2049.9128\n",
      "Training Epoch: 3 [19950/36045]\tLoss: 2053.5935\n",
      "Training Epoch: 3 [20000/36045]\tLoss: 2018.2085\n",
      "Training Epoch: 3 [20050/36045]\tLoss: 2300.4224\n",
      "Training Epoch: 3 [20100/36045]\tLoss: 2285.4114\n",
      "Training Epoch: 3 [20150/36045]\tLoss: 2308.3008\n",
      "Training Epoch: 3 [20200/36045]\tLoss: 2308.0596\n",
      "Training Epoch: 3 [20250/36045]\tLoss: 2412.6433\n",
      "Training Epoch: 3 [20300/36045]\tLoss: 2437.1895\n",
      "Training Epoch: 3 [20350/36045]\tLoss: 2506.6826\n",
      "Training Epoch: 3 [20400/36045]\tLoss: 2530.9404\n",
      "Training Epoch: 3 [20450/36045]\tLoss: 2511.1541\n",
      "Training Epoch: 3 [20500/36045]\tLoss: 2419.0017\n",
      "Training Epoch: 3 [20550/36045]\tLoss: 2301.0854\n",
      "Training Epoch: 3 [20600/36045]\tLoss: 2350.9971\n",
      "Training Epoch: 3 [20650/36045]\tLoss: 2310.0957\n",
      "Training Epoch: 3 [20700/36045]\tLoss: 2313.1213\n",
      "Training Epoch: 3 [20750/36045]\tLoss: 2414.1272\n",
      "Training Epoch: 3 [20800/36045]\tLoss: 2608.0803\n",
      "Training Epoch: 3 [20850/36045]\tLoss: 2585.3547\n",
      "Training Epoch: 3 [20900/36045]\tLoss: 2682.5464\n",
      "Training Epoch: 3 [20950/36045]\tLoss: 2584.6245\n",
      "Training Epoch: 3 [21000/36045]\tLoss: 2414.3774\n",
      "Training Epoch: 3 [21050/36045]\tLoss: 2114.9077\n",
      "Training Epoch: 3 [21100/36045]\tLoss: 2120.5464\n",
      "Training Epoch: 3 [21150/36045]\tLoss: 2220.4849\n",
      "Training Epoch: 3 [21200/36045]\tLoss: 2238.4941\n",
      "Training Epoch: 3 [21250/36045]\tLoss: 2115.5588\n",
      "Training Epoch: 3 [21300/36045]\tLoss: 2582.6416\n",
      "Training Epoch: 3 [21350/36045]\tLoss: 2505.5601\n",
      "Training Epoch: 3 [21400/36045]\tLoss: 2565.4922\n",
      "Training Epoch: 3 [21450/36045]\tLoss: 2565.7312\n",
      "Training Epoch: 3 [21500/36045]\tLoss: 2607.3169\n",
      "Training Epoch: 3 [21550/36045]\tLoss: 2641.4617\n",
      "Training Epoch: 3 [21600/36045]\tLoss: 2648.4751\n",
      "Training Epoch: 3 [21650/36045]\tLoss: 2678.3367\n",
      "Training Epoch: 3 [21700/36045]\tLoss: 2667.8066\n",
      "Training Epoch: 3 [21750/36045]\tLoss: 2545.6938\n",
      "Training Epoch: 3 [21800/36045]\tLoss: 2058.5803\n",
      "Training Epoch: 3 [21850/36045]\tLoss: 1991.4280\n",
      "Training Epoch: 3 [21900/36045]\tLoss: 2046.5508\n",
      "Training Epoch: 3 [21950/36045]\tLoss: 2046.0498\n",
      "Training Epoch: 3 [22000/36045]\tLoss: 2037.9734\n",
      "Training Epoch: 3 [22050/36045]\tLoss: 2274.2065\n",
      "Training Epoch: 3 [22100/36045]\tLoss: 2216.2490\n",
      "Training Epoch: 3 [22150/36045]\tLoss: 2190.1165\n",
      "Training Epoch: 3 [22200/36045]\tLoss: 2198.8083\n",
      "Training Epoch: 3 [22250/36045]\tLoss: 2250.6270\n",
      "Training Epoch: 3 [22300/36045]\tLoss: 2348.9299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [22350/36045]\tLoss: 2340.3628\n",
      "Training Epoch: 3 [22400/36045]\tLoss: 2388.4192\n",
      "Training Epoch: 3 [22450/36045]\tLoss: 2376.9009\n",
      "Training Epoch: 3 [22500/36045]\tLoss: 2302.4114\n",
      "Training Epoch: 3 [22550/36045]\tLoss: 2425.5398\n",
      "Training Epoch: 3 [22600/36045]\tLoss: 2627.2112\n",
      "Training Epoch: 3 [22650/36045]\tLoss: 2737.2185\n",
      "Training Epoch: 3 [22700/36045]\tLoss: 2813.9072\n",
      "Training Epoch: 3 [22750/36045]\tLoss: 2830.3979\n",
      "Training Epoch: 3 [22800/36045]\tLoss: 2964.0898\n",
      "Training Epoch: 3 [22850/36045]\tLoss: 2571.0032\n",
      "Training Epoch: 3 [22900/36045]\tLoss: 2543.2205\n",
      "Training Epoch: 3 [22950/36045]\tLoss: 2525.4377\n",
      "Training Epoch: 3 [23000/36045]\tLoss: 2548.5447\n",
      "Training Epoch: 3 [23050/36045]\tLoss: 2259.1504\n",
      "Training Epoch: 3 [23100/36045]\tLoss: 2310.1455\n",
      "Training Epoch: 3 [23150/36045]\tLoss: 2294.9780\n",
      "Training Epoch: 3 [23200/36045]\tLoss: 2200.5015\n",
      "Training Epoch: 3 [23250/36045]\tLoss: 2154.9976\n",
      "Training Epoch: 3 [23300/36045]\tLoss: 2138.6499\n",
      "Training Epoch: 3 [23350/36045]\tLoss: 2183.7673\n",
      "Training Epoch: 3 [23400/36045]\tLoss: 2327.1499\n",
      "Training Epoch: 3 [23450/36045]\tLoss: 2316.7314\n",
      "Training Epoch: 3 [23500/36045]\tLoss: 2224.9680\n",
      "Training Epoch: 3 [23550/36045]\tLoss: 2397.7915\n",
      "Training Epoch: 3 [23600/36045]\tLoss: 2609.3513\n",
      "Training Epoch: 3 [23650/36045]\tLoss: 2738.1699\n",
      "Training Epoch: 3 [23700/36045]\tLoss: 2724.6946\n",
      "Training Epoch: 3 [23750/36045]\tLoss: 2685.3188\n",
      "Training Epoch: 3 [23800/36045]\tLoss: 2303.0212\n",
      "Training Epoch: 3 [23850/36045]\tLoss: 2335.6733\n",
      "Training Epoch: 3 [23900/36045]\tLoss: 2308.6814\n",
      "Training Epoch: 3 [23950/36045]\tLoss: 2302.4316\n",
      "Training Epoch: 3 [24000/36045]\tLoss: 2199.5742\n",
      "Training Epoch: 3 [24050/36045]\tLoss: 1947.0225\n",
      "Training Epoch: 3 [24100/36045]\tLoss: 2011.1611\n",
      "Training Epoch: 3 [24150/36045]\tLoss: 2071.0164\n",
      "Training Epoch: 3 [24200/36045]\tLoss: 2056.5688\n",
      "Training Epoch: 3 [24250/36045]\tLoss: 1949.3964\n",
      "Training Epoch: 3 [24300/36045]\tLoss: 2136.9460\n",
      "Training Epoch: 3 [24350/36045]\tLoss: 2212.3645\n",
      "Training Epoch: 3 [24400/36045]\tLoss: 2227.8962\n",
      "Training Epoch: 3 [24450/36045]\tLoss: 2174.4492\n",
      "Training Epoch: 3 [24500/36045]\tLoss: 2313.6694\n",
      "Training Epoch: 3 [24550/36045]\tLoss: 2414.5322\n",
      "Training Epoch: 3 [24600/36045]\tLoss: 2390.0901\n",
      "Training Epoch: 3 [24650/36045]\tLoss: 2324.4780\n",
      "Training Epoch: 3 [24700/36045]\tLoss: 2376.5261\n",
      "Training Epoch: 3 [24750/36045]\tLoss: 2193.1965\n",
      "Training Epoch: 3 [24800/36045]\tLoss: 1998.4753\n",
      "Training Epoch: 3 [24850/36045]\tLoss: 2038.4382\n",
      "Training Epoch: 3 [24900/36045]\tLoss: 2034.8892\n",
      "Training Epoch: 3 [24950/36045]\tLoss: 2033.3895\n",
      "Training Epoch: 3 [25000/36045]\tLoss: 1966.5079\n",
      "Training Epoch: 3 [25050/36045]\tLoss: 1848.2255\n",
      "Training Epoch: 3 [25100/36045]\tLoss: 1682.7562\n",
      "Training Epoch: 3 [25150/36045]\tLoss: 1550.9432\n",
      "Training Epoch: 3 [25200/36045]\tLoss: 1556.3268\n",
      "Training Epoch: 3 [25250/36045]\tLoss: 1676.0569\n",
      "Training Epoch: 3 [25300/36045]\tLoss: 2200.5149\n",
      "Training Epoch: 3 [25350/36045]\tLoss: 2174.7000\n",
      "Training Epoch: 3 [25400/36045]\tLoss: 2006.2771\n",
      "Training Epoch: 3 [25450/36045]\tLoss: 2063.0742\n",
      "Training Epoch: 3 [25500/36045]\tLoss: 2222.0754\n",
      "Training Epoch: 3 [25550/36045]\tLoss: 2462.8821\n",
      "Training Epoch: 3 [25600/36045]\tLoss: 2432.1396\n",
      "Training Epoch: 3 [25650/36045]\tLoss: 2410.5181\n",
      "Training Epoch: 3 [25700/36045]\tLoss: 2468.3323\n",
      "Training Epoch: 3 [25750/36045]\tLoss: 2321.8533\n",
      "Training Epoch: 3 [25800/36045]\tLoss: 1408.1327\n",
      "Training Epoch: 3 [25850/36045]\tLoss: 1446.7251\n",
      "Training Epoch: 3 [25900/36045]\tLoss: 1376.4285\n",
      "Training Epoch: 3 [25950/36045]\tLoss: 1411.4719\n",
      "Training Epoch: 3 [26000/36045]\tLoss: 1728.8021\n",
      "Training Epoch: 3 [26050/36045]\tLoss: 2351.5410\n",
      "Training Epoch: 3 [26100/36045]\tLoss: 2387.7878\n",
      "Training Epoch: 3 [26150/36045]\tLoss: 2401.4150\n",
      "Training Epoch: 3 [26200/36045]\tLoss: 2340.1790\n",
      "Training Epoch: 3 [26250/36045]\tLoss: 2401.9724\n",
      "Training Epoch: 3 [26300/36045]\tLoss: 2021.7230\n",
      "Training Epoch: 3 [26350/36045]\tLoss: 1997.5166\n",
      "Training Epoch: 3 [26400/36045]\tLoss: 1984.8678\n",
      "Training Epoch: 3 [26450/36045]\tLoss: 1885.2417\n",
      "Training Epoch: 3 [26500/36045]\tLoss: 2346.2834\n",
      "Training Epoch: 3 [26550/36045]\tLoss: 2429.2603\n",
      "Training Epoch: 3 [26600/36045]\tLoss: 2412.4822\n",
      "Training Epoch: 3 [26650/36045]\tLoss: 2448.1226\n",
      "Training Epoch: 3 [26700/36045]\tLoss: 2420.8657\n",
      "Training Epoch: 3 [26750/36045]\tLoss: 2257.9875\n",
      "Training Epoch: 3 [26800/36045]\tLoss: 1649.1565\n",
      "Training Epoch: 3 [26850/36045]\tLoss: 1377.7104\n",
      "Training Epoch: 3 [26900/36045]\tLoss: 1405.3831\n",
      "Training Epoch: 3 [26950/36045]\tLoss: 1536.3292\n",
      "Training Epoch: 3 [27000/36045]\tLoss: 2324.0266\n",
      "Training Epoch: 3 [27050/36045]\tLoss: 2441.8809\n",
      "Training Epoch: 3 [27100/36045]\tLoss: 2377.3999\n",
      "Training Epoch: 3 [27150/36045]\tLoss: 2450.7490\n",
      "Training Epoch: 3 [27200/36045]\tLoss: 1947.1284\n",
      "Training Epoch: 3 [27250/36045]\tLoss: 1943.2401\n",
      "Training Epoch: 3 [27300/36045]\tLoss: 1879.9927\n",
      "Training Epoch: 3 [27350/36045]\tLoss: 1913.1842\n",
      "Training Epoch: 3 [27400/36045]\tLoss: 1871.4049\n",
      "Training Epoch: 3 [27450/36045]\tLoss: 2309.6284\n",
      "Training Epoch: 3 [27500/36045]\tLoss: 2439.7922\n",
      "Training Epoch: 3 [27550/36045]\tLoss: 2431.8711\n",
      "Training Epoch: 3 [27600/36045]\tLoss: 2410.5088\n",
      "Training Epoch: 3 [27650/36045]\tLoss: 2444.5430\n",
      "Training Epoch: 3 [27700/36045]\tLoss: 2496.6689\n",
      "Training Epoch: 3 [27750/36045]\tLoss: 2529.8699\n",
      "Training Epoch: 3 [27800/36045]\tLoss: 2501.9956\n",
      "Training Epoch: 3 [27850/36045]\tLoss: 2433.6833\n",
      "Training Epoch: 3 [27900/36045]\tLoss: 2109.2427\n",
      "Training Epoch: 3 [27950/36045]\tLoss: 1726.6407\n",
      "Training Epoch: 3 [28000/36045]\tLoss: 1671.2899\n",
      "Training Epoch: 3 [28050/36045]\tLoss: 1727.4005\n",
      "Training Epoch: 3 [28100/36045]\tLoss: 1712.2849\n",
      "Training Epoch: 3 [28150/36045]\tLoss: 1890.3466\n",
      "Training Epoch: 3 [28200/36045]\tLoss: 1934.4941\n",
      "Training Epoch: 3 [28250/36045]\tLoss: 1903.9976\n",
      "Training Epoch: 3 [28300/36045]\tLoss: 1837.2839\n",
      "Training Epoch: 3 [28350/36045]\tLoss: 1829.0387\n",
      "Training Epoch: 3 [28400/36045]\tLoss: 2370.7092\n",
      "Training Epoch: 3 [28450/36045]\tLoss: 2138.5007\n",
      "Training Epoch: 3 [28500/36045]\tLoss: 1789.4545\n",
      "Training Epoch: 3 [28550/36045]\tLoss: 1660.0280\n",
      "Training Epoch: 3 [28600/36045]\tLoss: 1963.3687\n",
      "Training Epoch: 3 [28650/36045]\tLoss: 2431.2930\n",
      "Training Epoch: 3 [28700/36045]\tLoss: 2463.2249\n",
      "Training Epoch: 3 [28750/36045]\tLoss: 2420.7773\n",
      "Training Epoch: 3 [28800/36045]\tLoss: 2466.6499\n",
      "Training Epoch: 3 [28850/36045]\tLoss: 2073.0864\n",
      "Training Epoch: 3 [28900/36045]\tLoss: 1561.7030\n",
      "Training Epoch: 3 [28950/36045]\tLoss: 1543.5309\n",
      "Training Epoch: 3 [29000/36045]\tLoss: 1556.0227\n",
      "Training Epoch: 3 [29050/36045]\tLoss: 1570.9313\n",
      "Training Epoch: 3 [29100/36045]\tLoss: 1616.3141\n",
      "Training Epoch: 3 [29150/36045]\tLoss: 1564.5692\n",
      "Training Epoch: 3 [29200/36045]\tLoss: 1525.7603\n",
      "Training Epoch: 3 [29250/36045]\tLoss: 1477.2133\n",
      "Training Epoch: 3 [29300/36045]\tLoss: 1804.1437\n",
      "Training Epoch: 3 [29350/36045]\tLoss: 2264.0476\n",
      "Training Epoch: 3 [29400/36045]\tLoss: 2366.5857\n",
      "Training Epoch: 3 [29450/36045]\tLoss: 2451.7197\n",
      "Training Epoch: 3 [29500/36045]\tLoss: 2460.5569\n",
      "Training Epoch: 3 [29550/36045]\tLoss: 2341.8130\n",
      "Training Epoch: 3 [29600/36045]\tLoss: 2043.9685\n",
      "Training Epoch: 3 [29650/36045]\tLoss: 1980.2878\n",
      "Training Epoch: 3 [29700/36045]\tLoss: 1748.4683\n",
      "Training Epoch: 3 [29750/36045]\tLoss: 1778.7440\n",
      "Training Epoch: 3 [29800/36045]\tLoss: 1849.4207\n",
      "Training Epoch: 3 [29850/36045]\tLoss: 1839.7738\n",
      "Training Epoch: 3 [29900/36045]\tLoss: 1866.9561\n",
      "Training Epoch: 3 [29950/36045]\tLoss: 1878.4430\n",
      "Training Epoch: 3 [30000/36045]\tLoss: 1892.4177\n",
      "Training Epoch: 3 [30050/36045]\tLoss: 1895.6761\n",
      "Training Epoch: 3 [30100/36045]\tLoss: 2486.5649\n",
      "Training Epoch: 3 [30150/36045]\tLoss: 2454.9968\n",
      "Training Epoch: 3 [30200/36045]\tLoss: 2371.0371\n",
      "Training Epoch: 3 [30250/36045]\tLoss: 2456.9944\n",
      "Training Epoch: 3 [30300/36045]\tLoss: 2470.1526\n",
      "Training Epoch: 3 [30350/36045]\tLoss: 2009.8871\n",
      "Training Epoch: 3 [30400/36045]\tLoss: 1964.1368\n",
      "Training Epoch: 3 [30450/36045]\tLoss: 1983.0714\n",
      "Training Epoch: 3 [30500/36045]\tLoss: 1847.9703\n",
      "Training Epoch: 3 [30550/36045]\tLoss: 1738.9884\n",
      "Training Epoch: 3 [30600/36045]\tLoss: 1621.7446\n",
      "Training Epoch: 3 [30650/36045]\tLoss: 1613.6232\n",
      "Training Epoch: 3 [30700/36045]\tLoss: 1645.4232\n",
      "Training Epoch: 3 [30750/36045]\tLoss: 1626.1656\n",
      "Training Epoch: 3 [30800/36045]\tLoss: 1642.5961\n",
      "Training Epoch: 3 [30850/36045]\tLoss: 1618.5775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [30900/36045]\tLoss: 1677.5031\n",
      "Training Epoch: 3 [30950/36045]\tLoss: 1756.2678\n",
      "Training Epoch: 3 [31000/36045]\tLoss: 1751.6672\n",
      "Training Epoch: 3 [31050/36045]\tLoss: 1469.9818\n",
      "Training Epoch: 3 [31100/36045]\tLoss: 1445.7625\n",
      "Training Epoch: 3 [31150/36045]\tLoss: 1452.8925\n",
      "Training Epoch: 3 [31200/36045]\tLoss: 1855.2122\n",
      "Training Epoch: 3 [31250/36045]\tLoss: 2400.9980\n",
      "Training Epoch: 3 [31300/36045]\tLoss: 2335.9492\n",
      "Training Epoch: 3 [31350/36045]\tLoss: 2355.1721\n",
      "Training Epoch: 3 [31400/36045]\tLoss: 2328.4275\n",
      "Training Epoch: 3 [31450/36045]\tLoss: 2276.7563\n",
      "Training Epoch: 3 [31500/36045]\tLoss: 2315.5586\n",
      "Training Epoch: 3 [31550/36045]\tLoss: 2303.8569\n",
      "Training Epoch: 3 [31600/36045]\tLoss: 2199.8506\n",
      "Training Epoch: 3 [31650/36045]\tLoss: 2327.0085\n",
      "Training Epoch: 3 [31700/36045]\tLoss: 1791.9753\n",
      "Training Epoch: 3 [31750/36045]\tLoss: 1526.4473\n",
      "Training Epoch: 3 [31800/36045]\tLoss: 1453.4178\n",
      "Training Epoch: 3 [31850/36045]\tLoss: 1488.4661\n",
      "Training Epoch: 3 [31900/36045]\tLoss: 2120.3962\n",
      "Training Epoch: 3 [31950/36045]\tLoss: 2538.0452\n",
      "Training Epoch: 3 [32000/36045]\tLoss: 2767.4163\n",
      "Training Epoch: 3 [32050/36045]\tLoss: 2669.1968\n",
      "Training Epoch: 3 [32100/36045]\tLoss: 2627.8938\n",
      "Training Epoch: 3 [32150/36045]\tLoss: 2325.4983\n",
      "Training Epoch: 3 [32200/36045]\tLoss: 2364.9226\n",
      "Training Epoch: 3 [32250/36045]\tLoss: 2403.7317\n",
      "Training Epoch: 3 [32300/36045]\tLoss: 2369.4478\n",
      "Training Epoch: 3 [32350/36045]\tLoss: 2319.5212\n",
      "Training Epoch: 3 [32400/36045]\tLoss: 2178.5554\n",
      "Training Epoch: 3 [32450/36045]\tLoss: 1861.1561\n",
      "Training Epoch: 3 [32500/36045]\tLoss: 1807.2052\n",
      "Training Epoch: 3 [32550/36045]\tLoss: 1809.8778\n",
      "Training Epoch: 3 [32600/36045]\tLoss: 1793.6028\n",
      "Training Epoch: 3 [32650/36045]\tLoss: 2200.1064\n",
      "Training Epoch: 3 [32700/36045]\tLoss: 2370.2024\n",
      "Training Epoch: 3 [32750/36045]\tLoss: 2301.1772\n",
      "Training Epoch: 3 [32800/36045]\tLoss: 2373.1621\n",
      "Training Epoch: 3 [32850/36045]\tLoss: 2167.8982\n",
      "Training Epoch: 3 [32900/36045]\tLoss: 1724.4633\n",
      "Training Epoch: 3 [32950/36045]\tLoss: 1809.9276\n",
      "Training Epoch: 3 [33000/36045]\tLoss: 1841.1182\n",
      "Training Epoch: 3 [33050/36045]\tLoss: 1695.8156\n",
      "Training Epoch: 3 [33100/36045]\tLoss: 1948.3994\n",
      "Training Epoch: 3 [33150/36045]\tLoss: 2495.0886\n",
      "Training Epoch: 3 [33200/36045]\tLoss: 2443.3633\n",
      "Training Epoch: 3 [33250/36045]\tLoss: 2514.5120\n",
      "Training Epoch: 3 [33300/36045]\tLoss: 2651.4531\n",
      "Training Epoch: 3 [33350/36045]\tLoss: 2123.8989\n",
      "Training Epoch: 3 [33400/36045]\tLoss: 1642.5067\n",
      "Training Epoch: 3 [33450/36045]\tLoss: 1625.0356\n",
      "Training Epoch: 3 [33500/36045]\tLoss: 1674.6212\n",
      "Training Epoch: 3 [33550/36045]\tLoss: 1757.1548\n",
      "Training Epoch: 3 [33600/36045]\tLoss: 1746.4255\n",
      "Training Epoch: 3 [33650/36045]\tLoss: 2187.8843\n",
      "Training Epoch: 3 [33700/36045]\tLoss: 2104.3071\n",
      "Training Epoch: 3 [33750/36045]\tLoss: 2220.3262\n",
      "Training Epoch: 3 [33800/36045]\tLoss: 2165.4448\n",
      "Training Epoch: 3 [33850/36045]\tLoss: 2180.9370\n",
      "Training Epoch: 3 [33900/36045]\tLoss: 2263.6257\n",
      "Training Epoch: 3 [33950/36045]\tLoss: 2285.5757\n",
      "Training Epoch: 3 [34000/36045]\tLoss: 2240.5571\n",
      "Training Epoch: 3 [34050/36045]\tLoss: 2287.5815\n",
      "Training Epoch: 3 [34100/36045]\tLoss: 2200.5957\n",
      "Training Epoch: 3 [34150/36045]\tLoss: 2067.1338\n",
      "Training Epoch: 3 [34200/36045]\tLoss: 1966.1832\n",
      "Training Epoch: 3 [34250/36045]\tLoss: 1979.3660\n",
      "Training Epoch: 3 [34300/36045]\tLoss: 1745.4561\n",
      "Training Epoch: 3 [34350/36045]\tLoss: 1781.3890\n",
      "Training Epoch: 3 [34400/36045]\tLoss: 1712.6200\n",
      "Training Epoch: 3 [34450/36045]\tLoss: 1590.8241\n",
      "Training Epoch: 3 [34500/36045]\tLoss: 1709.9377\n",
      "Training Epoch: 3 [34550/36045]\tLoss: 1679.6178\n",
      "Training Epoch: 3 [34600/36045]\tLoss: 1611.3545\n",
      "Training Epoch: 3 [34650/36045]\tLoss: 1830.5656\n",
      "Training Epoch: 3 [34700/36045]\tLoss: 1893.7882\n",
      "Training Epoch: 3 [34750/36045]\tLoss: 1694.1682\n",
      "Training Epoch: 3 [34800/36045]\tLoss: 1906.0627\n",
      "Training Epoch: 3 [34850/36045]\tLoss: 1984.1858\n",
      "Training Epoch: 3 [34900/36045]\tLoss: 2540.9104\n",
      "Training Epoch: 3 [34950/36045]\tLoss: 2546.1191\n",
      "Training Epoch: 3 [35000/36045]\tLoss: 2569.8499\n",
      "Training Epoch: 3 [35050/36045]\tLoss: 2485.6023\n",
      "Training Epoch: 3 [35100/36045]\tLoss: 1863.4641\n",
      "Training Epoch: 3 [35150/36045]\tLoss: 1837.4189\n",
      "Training Epoch: 3 [35200/36045]\tLoss: 1624.2456\n",
      "Training Epoch: 3 [35250/36045]\tLoss: 1723.6844\n",
      "Training Epoch: 3 [35300/36045]\tLoss: 1745.4037\n",
      "Training Epoch: 3 [35350/36045]\tLoss: 2124.7039\n",
      "Training Epoch: 3 [35400/36045]\tLoss: 2301.0791\n",
      "Training Epoch: 3 [35450/36045]\tLoss: 2193.1829\n",
      "Training Epoch: 3 [35500/36045]\tLoss: 2146.3005\n",
      "Training Epoch: 3 [35550/36045]\tLoss: 2119.9570\n",
      "Training Epoch: 3 [35600/36045]\tLoss: 2223.8987\n",
      "Training Epoch: 3 [35650/36045]\tLoss: 2388.0952\n",
      "Training Epoch: 3 [35700/36045]\tLoss: 2228.3452\n",
      "Training Epoch: 3 [35750/36045]\tLoss: 2323.9551\n",
      "Training Epoch: 3 [35800/36045]\tLoss: 2356.5913\n",
      "Training Epoch: 3 [35850/36045]\tLoss: 2302.8774\n",
      "Training Epoch: 3 [35900/36045]\tLoss: 2356.9343\n",
      "Training Epoch: 3 [35950/36045]\tLoss: 2342.7126\n",
      "Training Epoch: 3 [36000/36045]\tLoss: 2291.7039\n",
      "Training Epoch: 3 [36045/36045]\tLoss: 2253.0698\n",
      "Training Epoch: 3 [4004/4004]\tLoss: 2224.5470\n",
      "Training Epoch: 4 [50/36045]\tLoss: 2234.0393\n",
      "Training Epoch: 4 [100/36045]\tLoss: 2187.4263\n",
      "Training Epoch: 4 [150/36045]\tLoss: 2176.3438\n",
      "Training Epoch: 4 [200/36045]\tLoss: 2167.7173\n",
      "Training Epoch: 4 [250/36045]\tLoss: 2384.4285\n",
      "Training Epoch: 4 [300/36045]\tLoss: 2475.0342\n",
      "Training Epoch: 4 [350/36045]\tLoss: 2378.4072\n",
      "Training Epoch: 4 [400/36045]\tLoss: 2425.7183\n",
      "Training Epoch: 4 [450/36045]\tLoss: 2376.7141\n",
      "Training Epoch: 4 [500/36045]\tLoss: 2271.5999\n",
      "Training Epoch: 4 [550/36045]\tLoss: 2271.4861\n",
      "Training Epoch: 4 [600/36045]\tLoss: 2165.9861\n",
      "Training Epoch: 4 [650/36045]\tLoss: 2264.2644\n",
      "Training Epoch: 4 [700/36045]\tLoss: 2266.9958\n",
      "Training Epoch: 4 [750/36045]\tLoss: 2302.4509\n",
      "Training Epoch: 4 [800/36045]\tLoss: 2362.5146\n",
      "Training Epoch: 4 [850/36045]\tLoss: 2323.2368\n",
      "Training Epoch: 4 [900/36045]\tLoss: 2171.1379\n",
      "Training Epoch: 4 [950/36045]\tLoss: 2100.8918\n",
      "Training Epoch: 4 [1000/36045]\tLoss: 2031.4221\n",
      "Training Epoch: 4 [1050/36045]\tLoss: 2035.3517\n",
      "Training Epoch: 4 [1100/36045]\tLoss: 1973.2056\n",
      "Training Epoch: 4 [1150/36045]\tLoss: 1977.3325\n",
      "Training Epoch: 4 [1200/36045]\tLoss: 2063.1926\n",
      "Training Epoch: 4 [1250/36045]\tLoss: 2235.9919\n",
      "Training Epoch: 4 [1300/36045]\tLoss: 2216.3638\n",
      "Training Epoch: 4 [1350/36045]\tLoss: 2226.2534\n",
      "Training Epoch: 4 [1400/36045]\tLoss: 2326.8313\n",
      "Training Epoch: 4 [1450/36045]\tLoss: 2245.2954\n",
      "Training Epoch: 4 [1500/36045]\tLoss: 2109.2598\n",
      "Training Epoch: 4 [1550/36045]\tLoss: 2223.6060\n",
      "Training Epoch: 4 [1600/36045]\tLoss: 2229.5664\n",
      "Training Epoch: 4 [1650/36045]\tLoss: 2232.4880\n",
      "Training Epoch: 4 [1700/36045]\tLoss: 2256.8201\n",
      "Training Epoch: 4 [1750/36045]\tLoss: 2291.4514\n",
      "Training Epoch: 4 [1800/36045]\tLoss: 2275.7656\n",
      "Training Epoch: 4 [1850/36045]\tLoss: 2361.3562\n",
      "Training Epoch: 4 [1900/36045]\tLoss: 2249.8870\n",
      "Training Epoch: 4 [1950/36045]\tLoss: 2243.7119\n",
      "Training Epoch: 4 [2000/36045]\tLoss: 2053.4717\n",
      "Training Epoch: 4 [2050/36045]\tLoss: 2047.2505\n",
      "Training Epoch: 4 [2100/36045]\tLoss: 2166.7026\n",
      "Training Epoch: 4 [2150/36045]\tLoss: 2117.9966\n",
      "Training Epoch: 4 [2200/36045]\tLoss: 1943.3170\n",
      "Training Epoch: 4 [2250/36045]\tLoss: 1793.9520\n",
      "Training Epoch: 4 [2300/36045]\tLoss: 1863.4146\n",
      "Training Epoch: 4 [2350/36045]\tLoss: 1810.3490\n",
      "Training Epoch: 4 [2400/36045]\tLoss: 1861.7804\n",
      "Training Epoch: 4 [2450/36045]\tLoss: 2249.1211\n",
      "Training Epoch: 4 [2500/36045]\tLoss: 2347.3730\n",
      "Training Epoch: 4 [2550/36045]\tLoss: 2322.5220\n",
      "Training Epoch: 4 [2600/36045]\tLoss: 2262.2014\n",
      "Training Epoch: 4 [2650/36045]\tLoss: 2523.4880\n",
      "Training Epoch: 4 [2700/36045]\tLoss: 2635.1484\n",
      "Training Epoch: 4 [2750/36045]\tLoss: 2761.2830\n",
      "Training Epoch: 4 [2800/36045]\tLoss: 2795.8064\n",
      "Training Epoch: 4 [2850/36045]\tLoss: 2478.2527\n",
      "Training Epoch: 4 [2900/36045]\tLoss: 2499.1157\n",
      "Training Epoch: 4 [2950/36045]\tLoss: 2448.6965\n",
      "Training Epoch: 4 [3000/36045]\tLoss: 2436.8608\n",
      "Training Epoch: 4 [3050/36045]\tLoss: 2493.2942\n",
      "Training Epoch: 4 [3100/36045]\tLoss: 2294.1970\n",
      "Training Epoch: 4 [3150/36045]\tLoss: 1814.7301\n",
      "Training Epoch: 4 [3200/36045]\tLoss: 1870.4049\n",
      "Training Epoch: 4 [3250/36045]\tLoss: 1738.0337\n",
      "Training Epoch: 4 [3300/36045]\tLoss: 1656.8560\n",
      "Training Epoch: 4 [3350/36045]\tLoss: 1758.9988\n",
      "Training Epoch: 4 [3400/36045]\tLoss: 1867.3497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [3450/36045]\tLoss: 2002.9337\n",
      "Training Epoch: 4 [3500/36045]\tLoss: 1955.9137\n",
      "Training Epoch: 4 [3550/36045]\tLoss: 1927.2855\n",
      "Training Epoch: 4 [3600/36045]\tLoss: 2065.1204\n",
      "Training Epoch: 4 [3650/36045]\tLoss: 2314.5706\n",
      "Training Epoch: 4 [3700/36045]\tLoss: 2315.6387\n",
      "Training Epoch: 4 [3750/36045]\tLoss: 2258.4961\n",
      "Training Epoch: 4 [3800/36045]\tLoss: 2200.6653\n",
      "Training Epoch: 4 [3850/36045]\tLoss: 2124.7578\n",
      "Training Epoch: 4 [3900/36045]\tLoss: 2127.4114\n",
      "Training Epoch: 4 [3950/36045]\tLoss: 2062.2832\n",
      "Training Epoch: 4 [4000/36045]\tLoss: 2107.0269\n",
      "Training Epoch: 4 [4050/36045]\tLoss: 1953.6587\n",
      "Training Epoch: 4 [4100/36045]\tLoss: 1913.9708\n",
      "Training Epoch: 4 [4150/36045]\tLoss: 1994.2064\n",
      "Training Epoch: 4 [4200/36045]\tLoss: 1966.7606\n",
      "Training Epoch: 4 [4250/36045]\tLoss: 2035.1820\n",
      "Training Epoch: 4 [4300/36045]\tLoss: 2134.6211\n",
      "Training Epoch: 4 [4350/36045]\tLoss: 2138.8650\n",
      "Training Epoch: 4 [4400/36045]\tLoss: 2042.1193\n",
      "Training Epoch: 4 [4450/36045]\tLoss: 2109.2998\n",
      "Training Epoch: 4 [4500/36045]\tLoss: 2161.3452\n",
      "Training Epoch: 4 [4550/36045]\tLoss: 2202.0806\n",
      "Training Epoch: 4 [4600/36045]\tLoss: 2234.4219\n",
      "Training Epoch: 4 [4650/36045]\tLoss: 2224.6509\n",
      "Training Epoch: 4 [4700/36045]\tLoss: 2110.4058\n",
      "Training Epoch: 4 [4750/36045]\tLoss: 2088.4309\n",
      "Training Epoch: 4 [4800/36045]\tLoss: 2165.5457\n",
      "Training Epoch: 4 [4850/36045]\tLoss: 2133.5063\n",
      "Training Epoch: 4 [4900/36045]\tLoss: 2071.1155\n",
      "Training Epoch: 4 [4950/36045]\tLoss: 2120.7249\n",
      "Training Epoch: 4 [5000/36045]\tLoss: 2240.5557\n",
      "Training Epoch: 4 [5050/36045]\tLoss: 2166.4014\n",
      "Training Epoch: 4 [5100/36045]\tLoss: 2231.4944\n",
      "Training Epoch: 4 [5150/36045]\tLoss: 2147.1626\n",
      "Training Epoch: 4 [5200/36045]\tLoss: 2122.8882\n",
      "Training Epoch: 4 [5250/36045]\tLoss: 2112.9993\n",
      "Training Epoch: 4 [5300/36045]\tLoss: 2142.6841\n",
      "Training Epoch: 4 [5350/36045]\tLoss: 2182.3708\n",
      "Training Epoch: 4 [5400/36045]\tLoss: 2116.1897\n",
      "Training Epoch: 4 [5450/36045]\tLoss: 1936.4867\n",
      "Training Epoch: 4 [5500/36045]\tLoss: 2035.5922\n",
      "Training Epoch: 4 [5550/36045]\tLoss: 2017.5393\n",
      "Training Epoch: 4 [5600/36045]\tLoss: 2205.8245\n",
      "Training Epoch: 4 [5650/36045]\tLoss: 2151.7551\n",
      "Training Epoch: 4 [5700/36045]\tLoss: 1995.5928\n",
      "Training Epoch: 4 [5750/36045]\tLoss: 2024.0033\n",
      "Training Epoch: 4 [5800/36045]\tLoss: 2137.6897\n",
      "Training Epoch: 4 [5850/36045]\tLoss: 2053.4390\n",
      "Training Epoch: 4 [5900/36045]\tLoss: 2380.7610\n",
      "Training Epoch: 4 [5950/36045]\tLoss: 2435.5647\n",
      "Training Epoch: 4 [6000/36045]\tLoss: 2399.7954\n",
      "Training Epoch: 4 [6050/36045]\tLoss: 2312.4587\n",
      "Training Epoch: 4 [6100/36045]\tLoss: 2303.7571\n",
      "Training Epoch: 4 [6150/36045]\tLoss: 2231.4746\n",
      "Training Epoch: 4 [6200/36045]\tLoss: 2188.9915\n",
      "Training Epoch: 4 [6250/36045]\tLoss: 2208.5525\n",
      "Training Epoch: 4 [6300/36045]\tLoss: 2284.0396\n",
      "Training Epoch: 4 [6350/36045]\tLoss: 2349.8638\n",
      "Training Epoch: 4 [6400/36045]\tLoss: 2099.0486\n",
      "Training Epoch: 4 [6450/36045]\tLoss: 1955.2881\n",
      "Training Epoch: 4 [6500/36045]\tLoss: 1985.4812\n",
      "Training Epoch: 4 [6550/36045]\tLoss: 2040.2136\n",
      "Training Epoch: 4 [6600/36045]\tLoss: 2030.7073\n",
      "Training Epoch: 4 [6650/36045]\tLoss: 2226.3042\n",
      "Training Epoch: 4 [6700/36045]\tLoss: 2333.8430\n",
      "Training Epoch: 4 [6750/36045]\tLoss: 2278.3711\n",
      "Training Epoch: 4 [6800/36045]\tLoss: 2258.8630\n",
      "Training Epoch: 4 [6850/36045]\tLoss: 2237.3511\n",
      "Training Epoch: 4 [6900/36045]\tLoss: 1967.4504\n",
      "Training Epoch: 4 [6950/36045]\tLoss: 1867.5811\n",
      "Training Epoch: 4 [7000/36045]\tLoss: 1933.7365\n",
      "Training Epoch: 4 [7050/36045]\tLoss: 1996.3650\n",
      "Training Epoch: 4 [7100/36045]\tLoss: 2027.4187\n",
      "Training Epoch: 4 [7150/36045]\tLoss: 2082.8328\n",
      "Training Epoch: 4 [7200/36045]\tLoss: 2102.4868\n",
      "Training Epoch: 4 [7250/36045]\tLoss: 2080.7844\n",
      "Training Epoch: 4 [7300/36045]\tLoss: 2047.7179\n",
      "Training Epoch: 4 [7350/36045]\tLoss: 2053.1392\n",
      "Training Epoch: 4 [7400/36045]\tLoss: 1979.0068\n",
      "Training Epoch: 4 [7450/36045]\tLoss: 1955.4222\n",
      "Training Epoch: 4 [7500/36045]\tLoss: 1949.0199\n",
      "Training Epoch: 4 [7550/36045]\tLoss: 1864.9531\n",
      "Training Epoch: 4 [7600/36045]\tLoss: 1994.2866\n",
      "Training Epoch: 4 [7650/36045]\tLoss: 2093.1868\n",
      "Training Epoch: 4 [7700/36045]\tLoss: 1997.9453\n",
      "Training Epoch: 4 [7750/36045]\tLoss: 2037.3727\n",
      "Training Epoch: 4 [7800/36045]\tLoss: 2013.9570\n",
      "Training Epoch: 4 [7850/36045]\tLoss: 1915.1957\n",
      "Training Epoch: 4 [7900/36045]\tLoss: 2039.7012\n",
      "Training Epoch: 4 [7950/36045]\tLoss: 2028.5984\n",
      "Training Epoch: 4 [8000/36045]\tLoss: 2047.4067\n",
      "Training Epoch: 4 [8050/36045]\tLoss: 1954.0142\n",
      "Training Epoch: 4 [8100/36045]\tLoss: 2004.1405\n",
      "Training Epoch: 4 [8150/36045]\tLoss: 2218.6189\n",
      "Training Epoch: 4 [8200/36045]\tLoss: 2194.8171\n",
      "Training Epoch: 4 [8250/36045]\tLoss: 2127.2490\n",
      "Training Epoch: 4 [8300/36045]\tLoss: 2261.7297\n",
      "Training Epoch: 4 [8350/36045]\tLoss: 2110.1843\n",
      "Training Epoch: 4 [8400/36045]\tLoss: 2019.7979\n",
      "Training Epoch: 4 [8450/36045]\tLoss: 1938.6263\n",
      "Training Epoch: 4 [8500/36045]\tLoss: 2001.1151\n",
      "Training Epoch: 4 [8550/36045]\tLoss: 1920.4950\n",
      "Training Epoch: 4 [8600/36045]\tLoss: 1919.6083\n",
      "Training Epoch: 4 [8650/36045]\tLoss: 2048.4275\n",
      "Training Epoch: 4 [8700/36045]\tLoss: 2126.9648\n",
      "Training Epoch: 4 [8750/36045]\tLoss: 2105.4917\n",
      "Training Epoch: 4 [8800/36045]\tLoss: 2118.9790\n",
      "Training Epoch: 4 [8850/36045]\tLoss: 2089.6260\n",
      "Training Epoch: 4 [8900/36045]\tLoss: 1908.3158\n",
      "Training Epoch: 4 [8950/36045]\tLoss: 1950.8257\n",
      "Training Epoch: 4 [9000/36045]\tLoss: 1972.2219\n",
      "Training Epoch: 4 [9050/36045]\tLoss: 1960.7163\n",
      "Training Epoch: 4 [9100/36045]\tLoss: 1987.7122\n",
      "Training Epoch: 4 [9150/36045]\tLoss: 1492.0560\n",
      "Training Epoch: 4 [9200/36045]\tLoss: 1164.9711\n",
      "Training Epoch: 4 [9250/36045]\tLoss: 1257.6472\n",
      "Training Epoch: 4 [9300/36045]\tLoss: 1287.0688\n",
      "Training Epoch: 4 [9350/36045]\tLoss: 1181.3169\n",
      "Training Epoch: 4 [9400/36045]\tLoss: 2254.4688\n",
      "Training Epoch: 4 [9450/36045]\tLoss: 2356.3987\n",
      "Training Epoch: 4 [9500/36045]\tLoss: 2322.8242\n",
      "Training Epoch: 4 [9550/36045]\tLoss: 2487.4531\n",
      "Training Epoch: 4 [9600/36045]\tLoss: 1785.0264\n",
      "Training Epoch: 4 [9650/36045]\tLoss: 1760.8475\n",
      "Training Epoch: 4 [9700/36045]\tLoss: 1768.3854\n",
      "Training Epoch: 4 [9750/36045]\tLoss: 1732.8761\n",
      "Training Epoch: 4 [9800/36045]\tLoss: 2305.7371\n",
      "Training Epoch: 4 [9850/36045]\tLoss: 2444.5728\n",
      "Training Epoch: 4 [9900/36045]\tLoss: 2488.9583\n",
      "Training Epoch: 4 [9950/36045]\tLoss: 2395.7383\n",
      "Training Epoch: 4 [10000/36045]\tLoss: 2245.7139\n",
      "Training Epoch: 4 [10050/36045]\tLoss: 1950.7219\n",
      "Training Epoch: 4 [10100/36045]\tLoss: 1953.5446\n",
      "Training Epoch: 4 [10150/36045]\tLoss: 1966.1475\n",
      "Training Epoch: 4 [10200/36045]\tLoss: 1958.7462\n",
      "Training Epoch: 4 [10250/36045]\tLoss: 2350.1418\n",
      "Training Epoch: 4 [10300/36045]\tLoss: 2327.7949\n",
      "Training Epoch: 4 [10350/36045]\tLoss: 2402.5173\n",
      "Training Epoch: 4 [10400/36045]\tLoss: 2406.5146\n",
      "Training Epoch: 4 [10450/36045]\tLoss: 2204.5249\n",
      "Training Epoch: 4 [10500/36045]\tLoss: 1856.6637\n",
      "Training Epoch: 4 [10550/36045]\tLoss: 1827.5778\n",
      "Training Epoch: 4 [10600/36045]\tLoss: 1895.1949\n",
      "Training Epoch: 4 [10650/36045]\tLoss: 1913.3823\n",
      "Training Epoch: 4 [10700/36045]\tLoss: 2102.2000\n",
      "Training Epoch: 4 [10750/36045]\tLoss: 2236.2483\n",
      "Training Epoch: 4 [10800/36045]\tLoss: 2118.7385\n",
      "Training Epoch: 4 [10850/36045]\tLoss: 2187.7991\n",
      "Training Epoch: 4 [10900/36045]\tLoss: 2274.8452\n",
      "Training Epoch: 4 [10950/36045]\tLoss: 1754.3033\n",
      "Training Epoch: 4 [11000/36045]\tLoss: 1760.0717\n",
      "Training Epoch: 4 [11050/36045]\tLoss: 1854.3284\n",
      "Training Epoch: 4 [11100/36045]\tLoss: 1860.7498\n",
      "Training Epoch: 4 [11150/36045]\tLoss: 2009.8232\n",
      "Training Epoch: 4 [11200/36045]\tLoss: 2106.0154\n",
      "Training Epoch: 4 [11250/36045]\tLoss: 2156.1392\n",
      "Training Epoch: 4 [11300/36045]\tLoss: 2111.1008\n",
      "Training Epoch: 4 [11350/36045]\tLoss: 2114.2957\n",
      "Training Epoch: 4 [11400/36045]\tLoss: 2025.4821\n",
      "Training Epoch: 4 [11450/36045]\tLoss: 1974.9445\n",
      "Training Epoch: 4 [11500/36045]\tLoss: 1949.8558\n",
      "Training Epoch: 4 [11550/36045]\tLoss: 1979.1554\n",
      "Training Epoch: 4 [11600/36045]\tLoss: 2099.2988\n",
      "Training Epoch: 4 [11650/36045]\tLoss: 2215.3562\n",
      "Training Epoch: 4 [11700/36045]\tLoss: 2209.7886\n",
      "Training Epoch: 4 [11750/36045]\tLoss: 2233.3108\n",
      "Training Epoch: 4 [11800/36045]\tLoss: 2346.3601\n",
      "Training Epoch: 4 [11850/36045]\tLoss: 2370.9167\n",
      "Training Epoch: 4 [11900/36045]\tLoss: 2671.5371\n",
      "Training Epoch: 4 [11950/36045]\tLoss: 2645.2983\n",
      "Training Epoch: 4 [12000/36045]\tLoss: 2698.0837\n",
      "Training Epoch: 4 [12050/36045]\tLoss: 2597.0073\n",
      "Training Epoch: 4 [12100/36045]\tLoss: 1942.8643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [12150/36045]\tLoss: 1672.2521\n",
      "Training Epoch: 4 [12200/36045]\tLoss: 1659.0337\n",
      "Training Epoch: 4 [12250/36045]\tLoss: 1684.6575\n",
      "Training Epoch: 4 [12300/36045]\tLoss: 1994.4373\n",
      "Training Epoch: 4 [12350/36045]\tLoss: 2092.2695\n",
      "Training Epoch: 4 [12400/36045]\tLoss: 2112.9177\n",
      "Training Epoch: 4 [12450/36045]\tLoss: 2072.9126\n",
      "Training Epoch: 4 [12500/36045]\tLoss: 2160.7119\n",
      "Training Epoch: 4 [12550/36045]\tLoss: 2076.7676\n",
      "Training Epoch: 4 [12600/36045]\tLoss: 1980.0278\n",
      "Training Epoch: 4 [12650/36045]\tLoss: 1995.9553\n",
      "Training Epoch: 4 [12700/36045]\tLoss: 2036.3816\n",
      "Training Epoch: 4 [12750/36045]\tLoss: 2057.3032\n",
      "Training Epoch: 4 [12800/36045]\tLoss: 2019.6129\n",
      "Training Epoch: 4 [12850/36045]\tLoss: 2123.2983\n",
      "Training Epoch: 4 [12900/36045]\tLoss: 2060.1355\n",
      "Training Epoch: 4 [12950/36045]\tLoss: 2037.0988\n",
      "Training Epoch: 4 [13000/36045]\tLoss: 2070.5303\n",
      "Training Epoch: 4 [13050/36045]\tLoss: 1953.9685\n",
      "Training Epoch: 4 [13100/36045]\tLoss: 2035.2566\n",
      "Training Epoch: 4 [13150/36045]\tLoss: 2017.6943\n",
      "Training Epoch: 4 [13200/36045]\tLoss: 1914.9563\n",
      "Training Epoch: 4 [13250/36045]\tLoss: 1996.4180\n",
      "Training Epoch: 4 [13300/36045]\tLoss: 2075.6042\n",
      "Training Epoch: 4 [13350/36045]\tLoss: 2039.8772\n",
      "Training Epoch: 4 [13400/36045]\tLoss: 2046.3582\n",
      "Training Epoch: 4 [13450/36045]\tLoss: 2020.0138\n",
      "Training Epoch: 4 [13500/36045]\tLoss: 2115.8982\n",
      "Training Epoch: 4 [13550/36045]\tLoss: 2278.6245\n",
      "Training Epoch: 4 [13600/36045]\tLoss: 2304.2332\n",
      "Training Epoch: 4 [13650/36045]\tLoss: 2390.7498\n",
      "Training Epoch: 4 [13700/36045]\tLoss: 2192.8350\n",
      "Training Epoch: 4 [13750/36045]\tLoss: 2037.0452\n",
      "Training Epoch: 4 [13800/36045]\tLoss: 2020.8724\n",
      "Training Epoch: 4 [13850/36045]\tLoss: 2019.5151\n",
      "Training Epoch: 4 [13900/36045]\tLoss: 2046.8701\n",
      "Training Epoch: 4 [13950/36045]\tLoss: 2028.0562\n",
      "Training Epoch: 4 [14000/36045]\tLoss: 2093.2036\n",
      "Training Epoch: 4 [14050/36045]\tLoss: 2011.0194\n",
      "Training Epoch: 4 [14100/36045]\tLoss: 1998.9259\n",
      "Training Epoch: 4 [14150/36045]\tLoss: 1965.9092\n",
      "Training Epoch: 4 [14200/36045]\tLoss: 2090.1826\n",
      "Training Epoch: 4 [14250/36045]\tLoss: 2306.2053\n",
      "Training Epoch: 4 [14300/36045]\tLoss: 2311.2029\n",
      "Training Epoch: 4 [14350/36045]\tLoss: 2216.2837\n",
      "Training Epoch: 4 [14400/36045]\tLoss: 2191.2285\n",
      "Training Epoch: 4 [14450/36045]\tLoss: 2270.4163\n",
      "Training Epoch: 4 [14500/36045]\tLoss: 2217.6616\n",
      "Training Epoch: 4 [14550/36045]\tLoss: 2323.5920\n",
      "Training Epoch: 4 [14600/36045]\tLoss: 2279.2544\n",
      "Training Epoch: 4 [14650/36045]\tLoss: 2255.4390\n",
      "Training Epoch: 4 [14700/36045]\tLoss: 2105.4036\n",
      "Training Epoch: 4 [14750/36045]\tLoss: 1833.7859\n",
      "Training Epoch: 4 [14800/36045]\tLoss: 1801.2241\n",
      "Training Epoch: 4 [14850/36045]\tLoss: 1808.7286\n",
      "Training Epoch: 4 [14900/36045]\tLoss: 1800.4091\n",
      "Training Epoch: 4 [14950/36045]\tLoss: 1858.9528\n",
      "Training Epoch: 4 [15000/36045]\tLoss: 1958.0157\n",
      "Training Epoch: 4 [15050/36045]\tLoss: 1985.3259\n",
      "Training Epoch: 4 [15100/36045]\tLoss: 1958.3922\n",
      "Training Epoch: 4 [15150/36045]\tLoss: 1821.6543\n",
      "Training Epoch: 4 [15200/36045]\tLoss: 1626.0612\n",
      "Training Epoch: 4 [15250/36045]\tLoss: 1679.8715\n",
      "Training Epoch: 4 [15300/36045]\tLoss: 1653.3982\n",
      "Training Epoch: 4 [15350/36045]\tLoss: 1675.2311\n",
      "Training Epoch: 4 [15400/36045]\tLoss: 1889.2091\n",
      "Training Epoch: 4 [15450/36045]\tLoss: 1888.5585\n",
      "Training Epoch: 4 [15500/36045]\tLoss: 1918.5530\n",
      "Training Epoch: 4 [15550/36045]\tLoss: 1866.1555\n",
      "Training Epoch: 4 [15600/36045]\tLoss: 1933.5385\n",
      "Training Epoch: 4 [15650/36045]\tLoss: 1990.1420\n",
      "Training Epoch: 4 [15700/36045]\tLoss: 1948.2371\n",
      "Training Epoch: 4 [15750/36045]\tLoss: 1915.1483\n",
      "Training Epoch: 4 [15800/36045]\tLoss: 1680.1189\n",
      "Training Epoch: 4 [15850/36045]\tLoss: 1659.0553\n",
      "Training Epoch: 4 [15900/36045]\tLoss: 1657.8063\n",
      "Training Epoch: 4 [15950/36045]\tLoss: 1706.7920\n",
      "Training Epoch: 4 [16000/36045]\tLoss: 1695.1750\n",
      "Training Epoch: 4 [16050/36045]\tLoss: 1605.6019\n",
      "Training Epoch: 4 [16100/36045]\tLoss: 1521.2782\n",
      "Training Epoch: 4 [16150/36045]\tLoss: 1479.5494\n",
      "Training Epoch: 4 [16200/36045]\tLoss: 1731.5376\n",
      "Training Epoch: 4 [16250/36045]\tLoss: 1814.2284\n",
      "Training Epoch: 4 [16300/36045]\tLoss: 1937.1136\n",
      "Training Epoch: 4 [16350/36045]\tLoss: 1953.3403\n",
      "Training Epoch: 4 [16400/36045]\tLoss: 1908.5494\n",
      "Training Epoch: 4 [16450/36045]\tLoss: 1909.5084\n",
      "Training Epoch: 4 [16500/36045]\tLoss: 1927.5227\n",
      "Training Epoch: 4 [16550/36045]\tLoss: 1844.1243\n",
      "Training Epoch: 4 [16600/36045]\tLoss: 1919.6301\n",
      "Training Epoch: 4 [16650/36045]\tLoss: 1960.1873\n",
      "Training Epoch: 4 [16700/36045]\tLoss: 1932.3248\n",
      "Training Epoch: 4 [16750/36045]\tLoss: 1898.8749\n",
      "Training Epoch: 4 [16800/36045]\tLoss: 1950.3654\n",
      "Training Epoch: 4 [16850/36045]\tLoss: 1874.4062\n",
      "Training Epoch: 4 [16900/36045]\tLoss: 1859.3136\n",
      "Training Epoch: 4 [16950/36045]\tLoss: 1848.0970\n",
      "Training Epoch: 4 [17000/36045]\tLoss: 1831.4072\n",
      "Training Epoch: 4 [17050/36045]\tLoss: 1948.9500\n",
      "Training Epoch: 4 [17100/36045]\tLoss: 1957.9755\n",
      "Training Epoch: 4 [17150/36045]\tLoss: 1787.0483\n",
      "Training Epoch: 4 [17200/36045]\tLoss: 1759.5834\n",
      "Training Epoch: 4 [17250/36045]\tLoss: 1813.9021\n",
      "Training Epoch: 4 [17300/36045]\tLoss: 1929.7133\n",
      "Training Epoch: 4 [17350/36045]\tLoss: 1770.2030\n",
      "Training Epoch: 4 [17400/36045]\tLoss: 1754.0309\n",
      "Training Epoch: 4 [17450/36045]\tLoss: 1830.6603\n",
      "Training Epoch: 4 [17500/36045]\tLoss: 1816.2167\n",
      "Training Epoch: 4 [17550/36045]\tLoss: 1837.2482\n",
      "Training Epoch: 4 [17600/36045]\tLoss: 1807.0057\n",
      "Training Epoch: 4 [17650/36045]\tLoss: 1884.9614\n",
      "Training Epoch: 4 [17700/36045]\tLoss: 1809.6617\n",
      "Training Epoch: 4 [17750/36045]\tLoss: 1847.8743\n",
      "Training Epoch: 4 [17800/36045]\tLoss: 1839.2909\n",
      "Training Epoch: 4 [17850/36045]\tLoss: 1714.7863\n",
      "Training Epoch: 4 [17900/36045]\tLoss: 1758.4534\n",
      "Training Epoch: 4 [17950/36045]\tLoss: 1778.9645\n",
      "Training Epoch: 4 [18000/36045]\tLoss: 1745.4978\n",
      "Training Epoch: 4 [18050/36045]\tLoss: 1975.0680\n",
      "Training Epoch: 4 [18100/36045]\tLoss: 2028.5681\n",
      "Training Epoch: 4 [18150/36045]\tLoss: 2012.2936\n",
      "Training Epoch: 4 [18200/36045]\tLoss: 1998.3148\n",
      "Training Epoch: 4 [18250/36045]\tLoss: 2043.2845\n",
      "Training Epoch: 4 [18300/36045]\tLoss: 1907.9941\n",
      "Training Epoch: 4 [18350/36045]\tLoss: 1965.0881\n",
      "Training Epoch: 4 [18400/36045]\tLoss: 1983.2412\n",
      "Training Epoch: 4 [18450/36045]\tLoss: 1919.2075\n",
      "Training Epoch: 4 [18500/36045]\tLoss: 1872.8468\n",
      "Training Epoch: 4 [18550/36045]\tLoss: 1806.3468\n",
      "Training Epoch: 4 [18600/36045]\tLoss: 1772.5438\n",
      "Training Epoch: 4 [18650/36045]\tLoss: 1856.1503\n",
      "Training Epoch: 4 [18700/36045]\tLoss: 1942.5833\n",
      "Training Epoch: 4 [18750/36045]\tLoss: 1936.8131\n",
      "Training Epoch: 4 [18800/36045]\tLoss: 2019.6212\n",
      "Training Epoch: 4 [18850/36045]\tLoss: 1908.5463\n",
      "Training Epoch: 4 [18900/36045]\tLoss: 2022.8992\n",
      "Training Epoch: 4 [18950/36045]\tLoss: 1939.6449\n",
      "Training Epoch: 4 [19000/36045]\tLoss: 1778.0640\n",
      "Training Epoch: 4 [19050/36045]\tLoss: 1697.2135\n",
      "Training Epoch: 4 [19100/36045]\tLoss: 1749.0031\n",
      "Training Epoch: 4 [19150/36045]\tLoss: 1715.1219\n",
      "Training Epoch: 4 [19200/36045]\tLoss: 1769.8218\n",
      "Training Epoch: 4 [19250/36045]\tLoss: 1761.1797\n",
      "Training Epoch: 4 [19300/36045]\tLoss: 1824.8270\n",
      "Training Epoch: 4 [19350/36045]\tLoss: 1763.7855\n",
      "Training Epoch: 4 [19400/36045]\tLoss: 1821.2133\n",
      "Training Epoch: 4 [19450/36045]\tLoss: 1792.3518\n",
      "Training Epoch: 4 [19500/36045]\tLoss: 1805.9055\n",
      "Training Epoch: 4 [19550/36045]\tLoss: 1825.2814\n",
      "Training Epoch: 4 [19600/36045]\tLoss: 1887.6460\n",
      "Training Epoch: 4 [19650/36045]\tLoss: 2299.7886\n",
      "Training Epoch: 4 [19700/36045]\tLoss: 2244.1816\n",
      "Training Epoch: 4 [19750/36045]\tLoss: 2236.0596\n",
      "Training Epoch: 4 [19800/36045]\tLoss: 2199.7925\n",
      "Training Epoch: 4 [19850/36045]\tLoss: 1619.0983\n",
      "Training Epoch: 4 [19900/36045]\tLoss: 1573.5522\n",
      "Training Epoch: 4 [19950/36045]\tLoss: 1579.7686\n",
      "Training Epoch: 4 [20000/36045]\tLoss: 1565.1710\n",
      "Training Epoch: 4 [20050/36045]\tLoss: 1763.0347\n",
      "Training Epoch: 4 [20100/36045]\tLoss: 1752.7645\n",
      "Training Epoch: 4 [20150/36045]\tLoss: 1772.2323\n",
      "Training Epoch: 4 [20200/36045]\tLoss: 1765.1707\n",
      "Training Epoch: 4 [20250/36045]\tLoss: 1863.7178\n",
      "Training Epoch: 4 [20300/36045]\tLoss: 1910.3262\n",
      "Training Epoch: 4 [20350/36045]\tLoss: 1970.6732\n",
      "Training Epoch: 4 [20400/36045]\tLoss: 1992.6925\n",
      "Training Epoch: 4 [20450/36045]\tLoss: 1965.9099\n",
      "Training Epoch: 4 [20500/36045]\tLoss: 1896.5239\n",
      "Training Epoch: 4 [20550/36045]\tLoss: 1749.2651\n",
      "Training Epoch: 4 [20600/36045]\tLoss: 1791.4060\n",
      "Training Epoch: 4 [20650/36045]\tLoss: 1759.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [20700/36045]\tLoss: 1756.1890\n",
      "Training Epoch: 4 [20750/36045]\tLoss: 1839.3896\n",
      "Training Epoch: 4 [20800/36045]\tLoss: 1989.6967\n",
      "Training Epoch: 4 [20850/36045]\tLoss: 1974.2295\n",
      "Training Epoch: 4 [20900/36045]\tLoss: 2060.9841\n",
      "Training Epoch: 4 [20950/36045]\tLoss: 1981.1670\n",
      "Training Epoch: 4 [21000/36045]\tLoss: 1848.0870\n",
      "Training Epoch: 4 [21050/36045]\tLoss: 1608.8391\n",
      "Training Epoch: 4 [21100/36045]\tLoss: 1610.0375\n",
      "Training Epoch: 4 [21150/36045]\tLoss: 1699.7715\n",
      "Training Epoch: 4 [21200/36045]\tLoss: 1711.4010\n",
      "Training Epoch: 4 [21250/36045]\tLoss: 1618.2981\n",
      "Training Epoch: 4 [21300/36045]\tLoss: 1957.0670\n",
      "Training Epoch: 4 [21350/36045]\tLoss: 1914.4132\n",
      "Training Epoch: 4 [21400/36045]\tLoss: 1949.6119\n",
      "Training Epoch: 4 [21450/36045]\tLoss: 1961.4705\n",
      "Training Epoch: 4 [21500/36045]\tLoss: 1983.8414\n",
      "Training Epoch: 4 [21550/36045]\tLoss: 2072.0503\n",
      "Training Epoch: 4 [21600/36045]\tLoss: 2071.7798\n",
      "Training Epoch: 4 [21650/36045]\tLoss: 2096.4934\n",
      "Training Epoch: 4 [21700/36045]\tLoss: 2088.2656\n",
      "Training Epoch: 4 [21750/36045]\tLoss: 2008.6682\n",
      "Training Epoch: 4 [21800/36045]\tLoss: 1580.6827\n",
      "Training Epoch: 4 [21850/36045]\tLoss: 1539.9977\n",
      "Training Epoch: 4 [21900/36045]\tLoss: 1577.2249\n",
      "Training Epoch: 4 [21950/36045]\tLoss: 1566.9241\n",
      "Training Epoch: 4 [22000/36045]\tLoss: 1572.6633\n",
      "Training Epoch: 4 [22050/36045]\tLoss: 1744.0791\n",
      "Training Epoch: 4 [22100/36045]\tLoss: 1705.1855\n",
      "Training Epoch: 4 [22150/36045]\tLoss: 1678.6080\n",
      "Training Epoch: 4 [22200/36045]\tLoss: 1697.1986\n",
      "Training Epoch: 4 [22250/36045]\tLoss: 1729.3170\n",
      "Training Epoch: 4 [22300/36045]\tLoss: 1815.2993\n",
      "Training Epoch: 4 [22350/36045]\tLoss: 1826.3810\n",
      "Training Epoch: 4 [22400/36045]\tLoss: 1868.3052\n",
      "Training Epoch: 4 [22450/36045]\tLoss: 1852.0042\n",
      "Training Epoch: 4 [22500/36045]\tLoss: 1792.8561\n",
      "Training Epoch: 4 [22550/36045]\tLoss: 1892.9603\n",
      "Training Epoch: 4 [22600/36045]\tLoss: 2069.8611\n",
      "Training Epoch: 4 [22650/36045]\tLoss: 2147.8435\n",
      "Training Epoch: 4 [22700/36045]\tLoss: 2205.6892\n",
      "Training Epoch: 4 [22750/36045]\tLoss: 2235.1614\n",
      "Training Epoch: 4 [22800/36045]\tLoss: 2335.4646\n",
      "Training Epoch: 4 [22850/36045]\tLoss: 1976.4678\n",
      "Training Epoch: 4 [22900/36045]\tLoss: 1963.6714\n",
      "Training Epoch: 4 [22950/36045]\tLoss: 1934.8058\n",
      "Training Epoch: 4 [23000/36045]\tLoss: 1955.4496\n",
      "Training Epoch: 4 [23050/36045]\tLoss: 1756.6642\n",
      "Training Epoch: 4 [23100/36045]\tLoss: 1793.3345\n",
      "Training Epoch: 4 [23150/36045]\tLoss: 1773.5363\n",
      "Training Epoch: 4 [23200/36045]\tLoss: 1687.7908\n",
      "Training Epoch: 4 [23250/36045]\tLoss: 1669.8572\n",
      "Training Epoch: 4 [23300/36045]\tLoss: 1650.6482\n",
      "Training Epoch: 4 [23350/36045]\tLoss: 1677.4709\n",
      "Training Epoch: 4 [23400/36045]\tLoss: 1803.5604\n",
      "Training Epoch: 4 [23450/36045]\tLoss: 1785.9250\n",
      "Training Epoch: 4 [23500/36045]\tLoss: 1716.7129\n",
      "Training Epoch: 4 [23550/36045]\tLoss: 1858.5349\n",
      "Training Epoch: 4 [23600/36045]\tLoss: 2042.4619\n",
      "Training Epoch: 4 [23650/36045]\tLoss: 2127.0222\n",
      "Training Epoch: 4 [23700/36045]\tLoss: 2126.1987\n",
      "Training Epoch: 4 [23750/36045]\tLoss: 2081.4377\n",
      "Training Epoch: 4 [23800/36045]\tLoss: 1745.9773\n",
      "Training Epoch: 4 [23850/36045]\tLoss: 1790.1937\n",
      "Training Epoch: 4 [23900/36045]\tLoss: 1771.0247\n",
      "Training Epoch: 4 [23950/36045]\tLoss: 1753.5917\n",
      "Training Epoch: 4 [24000/36045]\tLoss: 1689.0957\n",
      "Training Epoch: 4 [24050/36045]\tLoss: 1517.4570\n",
      "Training Epoch: 4 [24100/36045]\tLoss: 1582.2072\n",
      "Training Epoch: 4 [24150/36045]\tLoss: 1616.9718\n",
      "Training Epoch: 4 [24200/36045]\tLoss: 1591.3893\n",
      "Training Epoch: 4 [24250/36045]\tLoss: 1525.2894\n",
      "Training Epoch: 4 [24300/36045]\tLoss: 1661.5292\n",
      "Training Epoch: 4 [24350/36045]\tLoss: 1713.8142\n",
      "Training Epoch: 4 [24400/36045]\tLoss: 1735.2474\n",
      "Training Epoch: 4 [24450/36045]\tLoss: 1685.2839\n",
      "Training Epoch: 4 [24500/36045]\tLoss: 1790.1534\n",
      "Training Epoch: 4 [24550/36045]\tLoss: 1895.1692\n",
      "Training Epoch: 4 [24600/36045]\tLoss: 1877.5535\n",
      "Training Epoch: 4 [24650/36045]\tLoss: 1828.0294\n",
      "Training Epoch: 4 [24700/36045]\tLoss: 1863.3768\n",
      "Training Epoch: 4 [24750/36045]\tLoss: 1709.4031\n",
      "Training Epoch: 4 [24800/36045]\tLoss: 1527.6230\n",
      "Training Epoch: 4 [24850/36045]\tLoss: 1558.1759\n",
      "Training Epoch: 4 [24900/36045]\tLoss: 1560.3951\n",
      "Training Epoch: 4 [24950/36045]\tLoss: 1560.4039\n",
      "Training Epoch: 4 [25000/36045]\tLoss: 1509.3850\n",
      "Training Epoch: 4 [25050/36045]\tLoss: 1417.2833\n",
      "Training Epoch: 4 [25100/36045]\tLoss: 1286.8582\n",
      "Training Epoch: 4 [25150/36045]\tLoss: 1192.9795\n",
      "Training Epoch: 4 [25200/36045]\tLoss: 1195.0387\n",
      "Training Epoch: 4 [25250/36045]\tLoss: 1281.1572\n",
      "Training Epoch: 4 [25300/36045]\tLoss: 1681.8937\n",
      "Training Epoch: 4 [25350/36045]\tLoss: 1664.4545\n",
      "Training Epoch: 4 [25400/36045]\tLoss: 1544.6907\n",
      "Training Epoch: 4 [25450/36045]\tLoss: 1565.7434\n",
      "Training Epoch: 4 [25500/36045]\tLoss: 1696.1648\n",
      "Training Epoch: 4 [25550/36045]\tLoss: 1880.2668\n",
      "Training Epoch: 4 [25600/36045]\tLoss: 1866.0972\n",
      "Training Epoch: 4 [25650/36045]\tLoss: 1829.2859\n",
      "Training Epoch: 4 [25700/36045]\tLoss: 1887.3331\n",
      "Training Epoch: 4 [25750/36045]\tLoss: 1796.1035\n",
      "Training Epoch: 4 [25800/36045]\tLoss: 1111.0808\n",
      "Training Epoch: 4 [25850/36045]\tLoss: 1134.4768\n",
      "Training Epoch: 4 [25900/36045]\tLoss: 1075.9823\n",
      "Training Epoch: 4 [25950/36045]\tLoss: 1096.3560\n",
      "Training Epoch: 4 [26000/36045]\tLoss: 1348.1672\n",
      "Training Epoch: 4 [26050/36045]\tLoss: 1832.2522\n",
      "Training Epoch: 4 [26100/36045]\tLoss: 1879.9585\n",
      "Training Epoch: 4 [26150/36045]\tLoss: 1874.5466\n",
      "Training Epoch: 4 [26200/36045]\tLoss: 1828.1772\n",
      "Training Epoch: 4 [26250/36045]\tLoss: 1875.8574\n",
      "Training Epoch: 4 [26300/36045]\tLoss: 1596.3269\n",
      "Training Epoch: 4 [26350/36045]\tLoss: 1589.7885\n",
      "Training Epoch: 4 [26400/36045]\tLoss: 1582.4940\n",
      "Training Epoch: 4 [26450/36045]\tLoss: 1489.9757\n",
      "Training Epoch: 4 [26500/36045]\tLoss: 1825.9225\n",
      "Training Epoch: 4 [26550/36045]\tLoss: 1882.7816\n",
      "Training Epoch: 4 [26600/36045]\tLoss: 1862.3430\n",
      "Training Epoch: 4 [26650/36045]\tLoss: 1893.5206\n",
      "Training Epoch: 4 [26700/36045]\tLoss: 1862.7565\n",
      "Training Epoch: 4 [26750/36045]\tLoss: 1742.0673\n",
      "Training Epoch: 4 [26800/36045]\tLoss: 1278.3979\n",
      "Training Epoch: 4 [26850/36045]\tLoss: 1073.6788\n",
      "Training Epoch: 4 [26900/36045]\tLoss: 1091.5729\n",
      "Training Epoch: 4 [26950/36045]\tLoss: 1192.7219\n",
      "Training Epoch: 4 [27000/36045]\tLoss: 1822.5582\n",
      "Training Epoch: 4 [27050/36045]\tLoss: 1927.0356\n",
      "Training Epoch: 4 [27100/36045]\tLoss: 1863.2053\n",
      "Training Epoch: 4 [27150/36045]\tLoss: 1934.7275\n",
      "Training Epoch: 4 [27200/36045]\tLoss: 1504.7697\n",
      "Training Epoch: 4 [27250/36045]\tLoss: 1491.9407\n",
      "Training Epoch: 4 [27300/36045]\tLoss: 1435.8652\n",
      "Training Epoch: 4 [27350/36045]\tLoss: 1464.6515\n",
      "Training Epoch: 4 [27400/36045]\tLoss: 1441.7043\n",
      "Training Epoch: 4 [27450/36045]\tLoss: 1794.2477\n",
      "Training Epoch: 4 [27500/36045]\tLoss: 1901.9821\n",
      "Training Epoch: 4 [27550/36045]\tLoss: 1894.3545\n",
      "Training Epoch: 4 [27600/36045]\tLoss: 1883.4398\n",
      "Training Epoch: 4 [27650/36045]\tLoss: 1901.0745\n",
      "Training Epoch: 4 [27700/36045]\tLoss: 1941.7771\n",
      "Training Epoch: 4 [27750/36045]\tLoss: 1960.2385\n",
      "Training Epoch: 4 [27800/36045]\tLoss: 1936.0865\n",
      "Training Epoch: 4 [27850/36045]\tLoss: 1881.7324\n",
      "Training Epoch: 4 [27900/36045]\tLoss: 1657.0535\n",
      "Training Epoch: 4 [27950/36045]\tLoss: 1371.8258\n",
      "Training Epoch: 4 [28000/36045]\tLoss: 1325.1353\n",
      "Training Epoch: 4 [28050/36045]\tLoss: 1370.7119\n",
      "Training Epoch: 4 [28100/36045]\tLoss: 1353.2216\n",
      "Training Epoch: 4 [28150/36045]\tLoss: 1482.2369\n",
      "Training Epoch: 4 [28200/36045]\tLoss: 1506.8040\n",
      "Training Epoch: 4 [28250/36045]\tLoss: 1494.7058\n",
      "Training Epoch: 4 [28300/36045]\tLoss: 1431.8923\n",
      "Training Epoch: 4 [28350/36045]\tLoss: 1432.9075\n",
      "Training Epoch: 4 [28400/36045]\tLoss: 1912.8400\n",
      "Training Epoch: 4 [28450/36045]\tLoss: 1715.0731\n",
      "Training Epoch: 4 [28500/36045]\tLoss: 1444.0587\n",
      "Training Epoch: 4 [28550/36045]\tLoss: 1328.2075\n",
      "Training Epoch: 4 [28600/36045]\tLoss: 1560.8352\n",
      "Training Epoch: 4 [28650/36045]\tLoss: 1930.3931\n",
      "Training Epoch: 4 [28700/36045]\tLoss: 1954.5804\n",
      "Training Epoch: 4 [28750/36045]\tLoss: 1922.9675\n",
      "Training Epoch: 4 [28800/36045]\tLoss: 1948.7487\n",
      "Training Epoch: 4 [28850/36045]\tLoss: 1643.6343\n",
      "Training Epoch: 4 [28900/36045]\tLoss: 1233.4943\n",
      "Training Epoch: 4 [28950/36045]\tLoss: 1223.1449\n",
      "Training Epoch: 4 [29000/36045]\tLoss: 1235.5728\n",
      "Training Epoch: 4 [29050/36045]\tLoss: 1251.4745\n",
      "Training Epoch: 4 [29100/36045]\tLoss: 1290.7983\n",
      "Training Epoch: 4 [29150/36045]\tLoss: 1254.7054\n",
      "Training Epoch: 4 [29200/36045]\tLoss: 1228.7542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [29250/36045]\tLoss: 1190.8722\n",
      "Training Epoch: 4 [29300/36045]\tLoss: 1423.9260\n",
      "Training Epoch: 4 [29350/36045]\tLoss: 1754.9802\n",
      "Training Epoch: 4 [29400/36045]\tLoss: 1824.7009\n",
      "Training Epoch: 4 [29450/36045]\tLoss: 1892.9469\n",
      "Training Epoch: 4 [29500/36045]\tLoss: 1907.7850\n",
      "Training Epoch: 4 [29550/36045]\tLoss: 1811.3065\n",
      "Training Epoch: 4 [29600/36045]\tLoss: 1595.4658\n",
      "Training Epoch: 4 [29650/36045]\tLoss: 1550.9839\n",
      "Training Epoch: 4 [29700/36045]\tLoss: 1365.6963\n",
      "Training Epoch: 4 [29750/36045]\tLoss: 1388.5378\n",
      "Training Epoch: 4 [29800/36045]\tLoss: 1462.3667\n",
      "Training Epoch: 4 [29850/36045]\tLoss: 1510.0939\n",
      "Training Epoch: 4 [29900/36045]\tLoss: 1515.6281\n",
      "Training Epoch: 4 [29950/36045]\tLoss: 1528.2637\n",
      "Training Epoch: 4 [30000/36045]\tLoss: 1524.8983\n",
      "Training Epoch: 4 [30050/36045]\tLoss: 1525.0682\n",
      "Training Epoch: 4 [30100/36045]\tLoss: 1959.3658\n",
      "Training Epoch: 4 [30150/36045]\tLoss: 1954.6013\n",
      "Training Epoch: 4 [30200/36045]\tLoss: 1873.0320\n",
      "Training Epoch: 4 [30250/36045]\tLoss: 1951.8049\n",
      "Training Epoch: 4 [30300/36045]\tLoss: 1956.9547\n",
      "Training Epoch: 4 [30350/36045]\tLoss: 1573.4736\n",
      "Training Epoch: 4 [30400/36045]\tLoss: 1545.0092\n",
      "Training Epoch: 4 [30450/36045]\tLoss: 1541.8365\n",
      "Training Epoch: 4 [30500/36045]\tLoss: 1433.5854\n",
      "Training Epoch: 4 [30550/36045]\tLoss: 1343.3771\n",
      "Training Epoch: 4 [30600/36045]\tLoss: 1265.8463\n",
      "Training Epoch: 4 [30650/36045]\tLoss: 1261.1780\n",
      "Training Epoch: 4 [30700/36045]\tLoss: 1289.5546\n",
      "Training Epoch: 4 [30750/36045]\tLoss: 1270.9064\n",
      "Training Epoch: 4 [30800/36045]\tLoss: 1311.9113\n",
      "Training Epoch: 4 [30850/36045]\tLoss: 1299.8198\n",
      "Training Epoch: 4 [30900/36045]\tLoss: 1343.1423\n",
      "Training Epoch: 4 [30950/36045]\tLoss: 1405.2192\n",
      "Training Epoch: 4 [31000/36045]\tLoss: 1389.5930\n",
      "Training Epoch: 4 [31050/36045]\tLoss: 1163.7343\n",
      "Training Epoch: 4 [31100/36045]\tLoss: 1144.4178\n",
      "Training Epoch: 4 [31150/36045]\tLoss: 1147.7236\n",
      "Training Epoch: 4 [31200/36045]\tLoss: 1466.8975\n",
      "Training Epoch: 4 [31250/36045]\tLoss: 1901.7512\n",
      "Training Epoch: 4 [31300/36045]\tLoss: 1837.8870\n",
      "Training Epoch: 4 [31350/36045]\tLoss: 1858.2028\n",
      "Training Epoch: 4 [31400/36045]\tLoss: 1838.0691\n",
      "Training Epoch: 4 [31450/36045]\tLoss: 1785.2821\n",
      "Training Epoch: 4 [31500/36045]\tLoss: 1792.7338\n",
      "Training Epoch: 4 [31550/36045]\tLoss: 1798.3495\n",
      "Training Epoch: 4 [31600/36045]\tLoss: 1703.3912\n",
      "Training Epoch: 4 [31650/36045]\tLoss: 1815.5137\n",
      "Training Epoch: 4 [31700/36045]\tLoss: 1402.0884\n",
      "Training Epoch: 4 [31750/36045]\tLoss: 1189.6359\n",
      "Training Epoch: 4 [31800/36045]\tLoss: 1124.8165\n",
      "Training Epoch: 4 [31850/36045]\tLoss: 1155.6747\n",
      "Training Epoch: 4 [31900/36045]\tLoss: 1680.7396\n",
      "Training Epoch: 4 [31950/36045]\tLoss: 2040.7057\n",
      "Training Epoch: 4 [32000/36045]\tLoss: 2238.7869\n",
      "Training Epoch: 4 [32050/36045]\tLoss: 2157.5850\n",
      "Training Epoch: 4 [32100/36045]\tLoss: 2127.0271\n",
      "Training Epoch: 4 [32150/36045]\tLoss: 1826.6307\n",
      "Training Epoch: 4 [32200/36045]\tLoss: 1855.0247\n",
      "Training Epoch: 4 [32250/36045]\tLoss: 1880.9341\n",
      "Training Epoch: 4 [32300/36045]\tLoss: 1853.2032\n",
      "Training Epoch: 4 [32350/36045]\tLoss: 1807.9799\n",
      "Training Epoch: 4 [32400/36045]\tLoss: 1705.2120\n",
      "Training Epoch: 4 [32450/36045]\tLoss: 1459.1492\n",
      "Training Epoch: 4 [32500/36045]\tLoss: 1413.0220\n",
      "Training Epoch: 4 [32550/36045]\tLoss: 1422.2877\n",
      "Training Epoch: 4 [32600/36045]\tLoss: 1408.8097\n",
      "Training Epoch: 4 [32650/36045]\tLoss: 1722.2222\n",
      "Training Epoch: 4 [32700/36045]\tLoss: 1855.1996\n",
      "Training Epoch: 4 [32750/36045]\tLoss: 1795.6219\n",
      "Training Epoch: 4 [32800/36045]\tLoss: 1847.9822\n",
      "Training Epoch: 4 [32850/36045]\tLoss: 1699.1909\n",
      "Training Epoch: 4 [32900/36045]\tLoss: 1356.4006\n",
      "Training Epoch: 4 [32950/36045]\tLoss: 1421.5978\n",
      "Training Epoch: 4 [33000/36045]\tLoss: 1438.9939\n",
      "Training Epoch: 4 [33050/36045]\tLoss: 1335.7720\n",
      "Training Epoch: 4 [33100/36045]\tLoss: 1532.7581\n",
      "Training Epoch: 4 [33150/36045]\tLoss: 1993.9160\n",
      "Training Epoch: 4 [33200/36045]\tLoss: 1954.5126\n",
      "Training Epoch: 4 [33250/36045]\tLoss: 2004.5017\n",
      "Training Epoch: 4 [33300/36045]\tLoss: 2123.0620\n",
      "Training Epoch: 4 [33350/36045]\tLoss: 1683.4189\n",
      "Training Epoch: 4 [33400/36045]\tLoss: 1301.1650\n",
      "Training Epoch: 4 [33450/36045]\tLoss: 1288.9902\n",
      "Training Epoch: 4 [33500/36045]\tLoss: 1325.7513\n",
      "Training Epoch: 4 [33550/36045]\tLoss: 1375.9240\n",
      "Training Epoch: 4 [33600/36045]\tLoss: 1377.7970\n",
      "Training Epoch: 4 [33650/36045]\tLoss: 1727.1354\n",
      "Training Epoch: 4 [33700/36045]\tLoss: 1663.2185\n",
      "Training Epoch: 4 [33750/36045]\tLoss: 1738.1600\n",
      "Training Epoch: 4 [33800/36045]\tLoss: 1713.5432\n",
      "Training Epoch: 4 [33850/36045]\tLoss: 1718.1382\n",
      "Training Epoch: 4 [33900/36045]\tLoss: 1767.3776\n",
      "Training Epoch: 4 [33950/36045]\tLoss: 1788.2587\n",
      "Training Epoch: 4 [34000/36045]\tLoss: 1762.2360\n",
      "Training Epoch: 4 [34050/36045]\tLoss: 1796.6503\n",
      "Training Epoch: 4 [34100/36045]\tLoss: 1718.0919\n",
      "Training Epoch: 4 [34150/36045]\tLoss: 1617.5034\n",
      "Training Epoch: 4 [34200/36045]\tLoss: 1538.8091\n",
      "Training Epoch: 4 [34250/36045]\tLoss: 1548.3468\n",
      "Training Epoch: 4 [34300/36045]\tLoss: 1356.0709\n",
      "Training Epoch: 4 [34350/36045]\tLoss: 1391.7546\n",
      "Training Epoch: 4 [34400/36045]\tLoss: 1340.8003\n",
      "Training Epoch: 4 [34450/36045]\tLoss: 1250.1189\n",
      "Training Epoch: 4 [34500/36045]\tLoss: 1344.0750\n",
      "Training Epoch: 4 [34550/36045]\tLoss: 1330.8226\n",
      "Training Epoch: 4 [34600/36045]\tLoss: 1281.5039\n",
      "Training Epoch: 4 [34650/36045]\tLoss: 1477.9875\n",
      "Training Epoch: 4 [34700/36045]\tLoss: 1543.7485\n",
      "Training Epoch: 4 [34750/36045]\tLoss: 1379.5433\n",
      "Training Epoch: 4 [34800/36045]\tLoss: 1548.8124\n",
      "Training Epoch: 4 [34850/36045]\tLoss: 1606.3545\n",
      "Training Epoch: 4 [34900/36045]\tLoss: 1998.8590\n",
      "Training Epoch: 4 [34950/36045]\tLoss: 1995.2924\n",
      "Training Epoch: 4 [35000/36045]\tLoss: 2014.2202\n",
      "Training Epoch: 4 [35050/36045]\tLoss: 1952.3630\n",
      "Training Epoch: 4 [35100/36045]\tLoss: 1484.5490\n",
      "Training Epoch: 4 [35150/36045]\tLoss: 1469.9886\n",
      "Training Epoch: 4 [35200/36045]\tLoss: 1295.4019\n",
      "Training Epoch: 4 [35250/36045]\tLoss: 1389.8217\n",
      "Training Epoch: 4 [35300/36045]\tLoss: 1410.7122\n",
      "Training Epoch: 4 [35350/36045]\tLoss: 1686.1509\n",
      "Training Epoch: 4 [35400/36045]\tLoss: 1813.4220\n",
      "Training Epoch: 4 [35450/36045]\tLoss: 1733.1013\n",
      "Training Epoch: 4 [35500/36045]\tLoss: 1694.8566\n",
      "Training Epoch: 4 [35550/36045]\tLoss: 1672.6000\n",
      "Training Epoch: 4 [35600/36045]\tLoss: 1762.3448\n",
      "Training Epoch: 4 [35650/36045]\tLoss: 1907.5020\n",
      "Training Epoch: 4 [35700/36045]\tLoss: 1777.0806\n",
      "Training Epoch: 4 [35750/36045]\tLoss: 1870.9191\n",
      "Training Epoch: 4 [35800/36045]\tLoss: 1890.1844\n",
      "Training Epoch: 4 [35850/36045]\tLoss: 1841.5275\n",
      "Training Epoch: 4 [35900/36045]\tLoss: 1880.1936\n",
      "Training Epoch: 4 [35950/36045]\tLoss: 1872.1704\n",
      "Training Epoch: 4 [36000/36045]\tLoss: 1839.4834\n",
      "Training Epoch: 4 [36045/36045]\tLoss: 1812.5446\n",
      "Training Epoch: 4 [4004/4004]\tLoss: 1773.4498\n",
      "Training Epoch: 5 [50/36045]\tLoss: 1782.5455\n",
      "Training Epoch: 5 [100/36045]\tLoss: 1735.1746\n",
      "Training Epoch: 5 [150/36045]\tLoss: 1734.2936\n",
      "Training Epoch: 5 [200/36045]\tLoss: 1716.1255\n",
      "Training Epoch: 5 [250/36045]\tLoss: 1913.4672\n",
      "Training Epoch: 5 [300/36045]\tLoss: 2002.4187\n",
      "Training Epoch: 5 [350/36045]\tLoss: 1921.4374\n",
      "Training Epoch: 5 [400/36045]\tLoss: 1956.3221\n",
      "Training Epoch: 5 [450/36045]\tLoss: 1915.5125\n",
      "Training Epoch: 5 [500/36045]\tLoss: 1818.8604\n",
      "Training Epoch: 5 [550/36045]\tLoss: 1830.6855\n",
      "Training Epoch: 5 [600/36045]\tLoss: 1750.1976\n",
      "Training Epoch: 5 [650/36045]\tLoss: 1824.6083\n",
      "Training Epoch: 5 [700/36045]\tLoss: 1823.0684\n",
      "Training Epoch: 5 [750/36045]\tLoss: 1815.7556\n",
      "Training Epoch: 5 [800/36045]\tLoss: 1855.9121\n",
      "Training Epoch: 5 [850/36045]\tLoss: 1818.1064\n",
      "Training Epoch: 5 [900/36045]\tLoss: 1707.7267\n",
      "Training Epoch: 5 [950/36045]\tLoss: 1652.0497\n",
      "Training Epoch: 5 [1000/36045]\tLoss: 1604.7780\n",
      "Training Epoch: 5 [1050/36045]\tLoss: 1605.0900\n",
      "Training Epoch: 5 [1100/36045]\tLoss: 1561.0310\n",
      "Training Epoch: 5 [1150/36045]\tLoss: 1562.1589\n",
      "Training Epoch: 5 [1200/36045]\tLoss: 1636.6385\n",
      "Training Epoch: 5 [1250/36045]\tLoss: 1778.7875\n",
      "Training Epoch: 5 [1300/36045]\tLoss: 1760.5973\n",
      "Training Epoch: 5 [1350/36045]\tLoss: 1776.3326\n",
      "Training Epoch: 5 [1400/36045]\tLoss: 1847.3573\n",
      "Training Epoch: 5 [1450/36045]\tLoss: 1784.2476\n",
      "Training Epoch: 5 [1500/36045]\tLoss: 1674.1694\n",
      "Training Epoch: 5 [1550/36045]\tLoss: 1753.3818\n",
      "Training Epoch: 5 [1600/36045]\tLoss: 1758.2183\n",
      "Training Epoch: 5 [1650/36045]\tLoss: 1758.3577\n",
      "Training Epoch: 5 [1700/36045]\tLoss: 1781.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [1750/36045]\tLoss: 1835.5391\n",
      "Training Epoch: 5 [1800/36045]\tLoss: 1823.7904\n",
      "Training Epoch: 5 [1850/36045]\tLoss: 1892.1237\n",
      "Training Epoch: 5 [1900/36045]\tLoss: 1798.9438\n",
      "Training Epoch: 5 [1950/36045]\tLoss: 1789.2689\n",
      "Training Epoch: 5 [2000/36045]\tLoss: 1614.8002\n",
      "Training Epoch: 5 [2050/36045]\tLoss: 1617.5793\n",
      "Training Epoch: 5 [2100/36045]\tLoss: 1709.3380\n",
      "Training Epoch: 5 [2150/36045]\tLoss: 1669.1166\n",
      "Training Epoch: 5 [2200/36045]\tLoss: 1550.2931\n",
      "Training Epoch: 5 [2250/36045]\tLoss: 1450.7592\n",
      "Training Epoch: 5 [2300/36045]\tLoss: 1506.7076\n",
      "Training Epoch: 5 [2350/36045]\tLoss: 1453.8121\n",
      "Training Epoch: 5 [2400/36045]\tLoss: 1503.5935\n",
      "Training Epoch: 5 [2450/36045]\tLoss: 1810.2783\n",
      "Training Epoch: 5 [2500/36045]\tLoss: 1887.9679\n",
      "Training Epoch: 5 [2550/36045]\tLoss: 1878.4381\n",
      "Training Epoch: 5 [2600/36045]\tLoss: 1845.2268\n",
      "Training Epoch: 5 [2650/36045]\tLoss: 2061.0564\n",
      "Training Epoch: 5 [2700/36045]\tLoss: 2175.8467\n",
      "Training Epoch: 5 [2750/36045]\tLoss: 2288.8040\n",
      "Training Epoch: 5 [2800/36045]\tLoss: 2311.9746\n",
      "Training Epoch: 5 [2850/36045]\tLoss: 2015.3289\n",
      "Training Epoch: 5 [2900/36045]\tLoss: 2023.2666\n",
      "Training Epoch: 5 [2950/36045]\tLoss: 1971.0028\n",
      "Training Epoch: 5 [3000/36045]\tLoss: 1963.8733\n",
      "Training Epoch: 5 [3050/36045]\tLoss: 2016.2859\n",
      "Training Epoch: 5 [3100/36045]\tLoss: 1843.9027\n",
      "Training Epoch: 5 [3150/36045]\tLoss: 1440.8374\n",
      "Training Epoch: 5 [3200/36045]\tLoss: 1490.1007\n",
      "Training Epoch: 5 [3250/36045]\tLoss: 1388.9783\n",
      "Training Epoch: 5 [3300/36045]\tLoss: 1320.5712\n",
      "Training Epoch: 5 [3350/36045]\tLoss: 1398.1981\n",
      "Training Epoch: 5 [3400/36045]\tLoss: 1483.9518\n",
      "Training Epoch: 5 [3450/36045]\tLoss: 1599.1812\n",
      "Training Epoch: 5 [3500/36045]\tLoss: 1557.0680\n",
      "Training Epoch: 5 [3550/36045]\tLoss: 1525.7830\n",
      "Training Epoch: 5 [3600/36045]\tLoss: 1626.6685\n",
      "Training Epoch: 5 [3650/36045]\tLoss: 1841.1968\n",
      "Training Epoch: 5 [3700/36045]\tLoss: 1845.3357\n",
      "Training Epoch: 5 [3750/36045]\tLoss: 1793.8431\n",
      "Training Epoch: 5 [3800/36045]\tLoss: 1759.3350\n",
      "Training Epoch: 5 [3850/36045]\tLoss: 1737.8716\n",
      "Training Epoch: 5 [3900/36045]\tLoss: 1745.9813\n",
      "Training Epoch: 5 [3950/36045]\tLoss: 1675.5289\n",
      "Training Epoch: 5 [4000/36045]\tLoss: 1721.8304\n",
      "Training Epoch: 5 [4050/36045]\tLoss: 1588.3983\n",
      "Training Epoch: 5 [4100/36045]\tLoss: 1547.7664\n",
      "Training Epoch: 5 [4150/36045]\tLoss: 1608.9379\n",
      "Training Epoch: 5 [4200/36045]\tLoss: 1582.7679\n",
      "Training Epoch: 5 [4250/36045]\tLoss: 1629.9509\n",
      "Training Epoch: 5 [4300/36045]\tLoss: 1693.8175\n",
      "Training Epoch: 5 [4350/36045]\tLoss: 1686.6288\n",
      "Training Epoch: 5 [4400/36045]\tLoss: 1607.0437\n",
      "Training Epoch: 5 [4450/36045]\tLoss: 1680.4292\n",
      "Training Epoch: 5 [4500/36045]\tLoss: 1738.3762\n",
      "Training Epoch: 5 [4550/36045]\tLoss: 1772.3932\n",
      "Training Epoch: 5 [4600/36045]\tLoss: 1793.8680\n",
      "Training Epoch: 5 [4650/36045]\tLoss: 1796.2714\n",
      "Training Epoch: 5 [4700/36045]\tLoss: 1690.5161\n",
      "Training Epoch: 5 [4750/36045]\tLoss: 1670.9398\n",
      "Training Epoch: 5 [4800/36045]\tLoss: 1733.0389\n",
      "Training Epoch: 5 [4850/36045]\tLoss: 1704.1866\n",
      "Training Epoch: 5 [4900/36045]\tLoss: 1649.8517\n",
      "Training Epoch: 5 [4950/36045]\tLoss: 1706.9004\n",
      "Training Epoch: 5 [5000/36045]\tLoss: 1812.7155\n",
      "Training Epoch: 5 [5050/36045]\tLoss: 1756.1066\n",
      "Training Epoch: 5 [5100/36045]\tLoss: 1802.5692\n",
      "Training Epoch: 5 [5150/36045]\tLoss: 1739.1910\n",
      "Training Epoch: 5 [5200/36045]\tLoss: 1704.2560\n",
      "Training Epoch: 5 [5250/36045]\tLoss: 1691.5834\n",
      "Training Epoch: 5 [5300/36045]\tLoss: 1707.6449\n",
      "Training Epoch: 5 [5350/36045]\tLoss: 1752.5079\n",
      "Training Epoch: 5 [5400/36045]\tLoss: 1684.3394\n",
      "Training Epoch: 5 [5450/36045]\tLoss: 1552.4302\n",
      "Training Epoch: 5 [5500/36045]\tLoss: 1638.4033\n",
      "Training Epoch: 5 [5550/36045]\tLoss: 1609.1628\n",
      "Training Epoch: 5 [5600/36045]\tLoss: 1778.5416\n",
      "Training Epoch: 5 [5650/36045]\tLoss: 1728.8971\n",
      "Training Epoch: 5 [5700/36045]\tLoss: 1626.8031\n",
      "Training Epoch: 5 [5750/36045]\tLoss: 1631.2910\n",
      "Training Epoch: 5 [5800/36045]\tLoss: 1733.4084\n",
      "Training Epoch: 5 [5850/36045]\tLoss: 1669.0999\n",
      "Training Epoch: 5 [5900/36045]\tLoss: 1910.0541\n",
      "Training Epoch: 5 [5950/36045]\tLoss: 1960.8047\n",
      "Training Epoch: 5 [6000/36045]\tLoss: 1933.2479\n",
      "Training Epoch: 5 [6050/36045]\tLoss: 1858.0773\n",
      "Training Epoch: 5 [6100/36045]\tLoss: 1854.2832\n",
      "Training Epoch: 5 [6150/36045]\tLoss: 1804.9021\n",
      "Training Epoch: 5 [6200/36045]\tLoss: 1783.0802\n",
      "Training Epoch: 5 [6250/36045]\tLoss: 1795.2219\n",
      "Training Epoch: 5 [6300/36045]\tLoss: 1858.0928\n",
      "Training Epoch: 5 [6350/36045]\tLoss: 1923.3273\n",
      "Training Epoch: 5 [6400/36045]\tLoss: 1699.0563\n",
      "Training Epoch: 5 [6450/36045]\tLoss: 1579.3623\n",
      "Training Epoch: 5 [6500/36045]\tLoss: 1613.1445\n",
      "Training Epoch: 5 [6550/36045]\tLoss: 1651.5594\n",
      "Training Epoch: 5 [6600/36045]\tLoss: 1645.8505\n",
      "Training Epoch: 5 [6650/36045]\tLoss: 1821.9408\n",
      "Training Epoch: 5 [6700/36045]\tLoss: 1909.8673\n",
      "Training Epoch: 5 [6750/36045]\tLoss: 1865.7535\n",
      "Training Epoch: 5 [6800/36045]\tLoss: 1853.2457\n",
      "Training Epoch: 5 [6850/36045]\tLoss: 1830.0974\n",
      "Training Epoch: 5 [6900/36045]\tLoss: 1609.5300\n",
      "Training Epoch: 5 [6950/36045]\tLoss: 1520.8153\n",
      "Training Epoch: 5 [7000/36045]\tLoss: 1588.7217\n",
      "Training Epoch: 5 [7050/36045]\tLoss: 1647.0107\n",
      "Training Epoch: 5 [7100/36045]\tLoss: 1643.2434\n",
      "Training Epoch: 5 [7150/36045]\tLoss: 1684.5638\n",
      "Training Epoch: 5 [7200/36045]\tLoss: 1696.7451\n",
      "Training Epoch: 5 [7250/36045]\tLoss: 1688.7527\n",
      "Training Epoch: 5 [7300/36045]\tLoss: 1661.2295\n",
      "Training Epoch: 5 [7350/36045]\tLoss: 1658.7355\n",
      "Training Epoch: 5 [7400/36045]\tLoss: 1577.4414\n",
      "Training Epoch: 5 [7450/36045]\tLoss: 1568.6271\n",
      "Training Epoch: 5 [7500/36045]\tLoss: 1563.6515\n",
      "Training Epoch: 5 [7550/36045]\tLoss: 1495.0732\n",
      "Training Epoch: 5 [7600/36045]\tLoss: 1625.3691\n",
      "Training Epoch: 5 [7650/36045]\tLoss: 1722.5546\n",
      "Training Epoch: 5 [7700/36045]\tLoss: 1650.2938\n",
      "Training Epoch: 5 [7750/36045]\tLoss: 1676.1713\n",
      "Training Epoch: 5 [7800/36045]\tLoss: 1655.7690\n",
      "Training Epoch: 5 [7850/36045]\tLoss: 1559.8016\n",
      "Training Epoch: 5 [7900/36045]\tLoss: 1649.8813\n",
      "Training Epoch: 5 [7950/36045]\tLoss: 1641.6183\n",
      "Training Epoch: 5 [8000/36045]\tLoss: 1658.5911\n",
      "Training Epoch: 5 [8050/36045]\tLoss: 1582.7572\n",
      "Training Epoch: 5 [8100/36045]\tLoss: 1634.0659\n",
      "Training Epoch: 5 [8150/36045]\tLoss: 1834.4918\n",
      "Training Epoch: 5 [8200/36045]\tLoss: 1814.5979\n",
      "Training Epoch: 5 [8250/36045]\tLoss: 1760.0695\n",
      "Training Epoch: 5 [8300/36045]\tLoss: 1877.5168\n",
      "Training Epoch: 5 [8350/36045]\tLoss: 1749.8821\n",
      "Training Epoch: 5 [8400/36045]\tLoss: 1644.0032\n",
      "Training Epoch: 5 [8450/36045]\tLoss: 1571.3496\n",
      "Training Epoch: 5 [8500/36045]\tLoss: 1627.8252\n",
      "Training Epoch: 5 [8550/36045]\tLoss: 1576.8600\n",
      "Training Epoch: 5 [8600/36045]\tLoss: 1575.8215\n",
      "Training Epoch: 5 [8650/36045]\tLoss: 1654.8608\n",
      "Training Epoch: 5 [8700/36045]\tLoss: 1729.3921\n",
      "Training Epoch: 5 [8750/36045]\tLoss: 1701.3097\n",
      "Training Epoch: 5 [8800/36045]\tLoss: 1715.6393\n",
      "Training Epoch: 5 [8850/36045]\tLoss: 1694.1262\n",
      "Training Epoch: 5 [8900/36045]\tLoss: 1543.7925\n",
      "Training Epoch: 5 [8950/36045]\tLoss: 1583.4480\n",
      "Training Epoch: 5 [9000/36045]\tLoss: 1596.8619\n",
      "Training Epoch: 5 [9050/36045]\tLoss: 1586.4562\n",
      "Training Epoch: 5 [9100/36045]\tLoss: 1620.3207\n",
      "Training Epoch: 5 [9150/36045]\tLoss: 1204.8822\n",
      "Training Epoch: 5 [9200/36045]\tLoss: 932.3376\n",
      "Training Epoch: 5 [9250/36045]\tLoss: 1008.3486\n",
      "Training Epoch: 5 [9300/36045]\tLoss: 1040.4401\n",
      "Training Epoch: 5 [9350/36045]\tLoss: 951.0526\n",
      "Training Epoch: 5 [9400/36045]\tLoss: 1842.7570\n",
      "Training Epoch: 5 [9450/36045]\tLoss: 1928.1387\n",
      "Training Epoch: 5 [9500/36045]\tLoss: 1903.9022\n",
      "Training Epoch: 5 [9550/36045]\tLoss: 2028.6125\n",
      "Training Epoch: 5 [9600/36045]\tLoss: 1479.1163\n",
      "Training Epoch: 5 [9650/36045]\tLoss: 1464.0437\n",
      "Training Epoch: 5 [9700/36045]\tLoss: 1464.8472\n",
      "Training Epoch: 5 [9750/36045]\tLoss: 1449.1354\n",
      "Training Epoch: 5 [9800/36045]\tLoss: 1887.2588\n",
      "Training Epoch: 5 [9850/36045]\tLoss: 1991.0465\n",
      "Training Epoch: 5 [9900/36045]\tLoss: 2035.0588\n",
      "Training Epoch: 5 [9950/36045]\tLoss: 1958.5520\n",
      "Training Epoch: 5 [10000/36045]\tLoss: 1832.6903\n",
      "Training Epoch: 5 [10050/36045]\tLoss: 1587.3446\n",
      "Training Epoch: 5 [10100/36045]\tLoss: 1587.1362\n",
      "Training Epoch: 5 [10150/36045]\tLoss: 1598.7557\n",
      "Training Epoch: 5 [10200/36045]\tLoss: 1599.0027\n",
      "Training Epoch: 5 [10250/36045]\tLoss: 1932.5494\n",
      "Training Epoch: 5 [10300/36045]\tLoss: 1900.3230\n",
      "Training Epoch: 5 [10350/36045]\tLoss: 1972.6437\n",
      "Training Epoch: 5 [10400/36045]\tLoss: 1969.9552\n",
      "Training Epoch: 5 [10450/36045]\tLoss: 1801.9786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [10500/36045]\tLoss: 1515.6910\n",
      "Training Epoch: 5 [10550/36045]\tLoss: 1503.7798\n",
      "Training Epoch: 5 [10600/36045]\tLoss: 1553.9602\n",
      "Training Epoch: 5 [10650/36045]\tLoss: 1569.3827\n",
      "Training Epoch: 5 [10700/36045]\tLoss: 1719.6086\n",
      "Training Epoch: 5 [10750/36045]\tLoss: 1838.7939\n",
      "Training Epoch: 5 [10800/36045]\tLoss: 1728.8209\n",
      "Training Epoch: 5 [10850/36045]\tLoss: 1790.4633\n",
      "Training Epoch: 5 [10900/36045]\tLoss: 1857.2051\n",
      "Training Epoch: 5 [10950/36045]\tLoss: 1424.8066\n",
      "Training Epoch: 5 [11000/36045]\tLoss: 1429.4359\n",
      "Training Epoch: 5 [11050/36045]\tLoss: 1513.7455\n",
      "Training Epoch: 5 [11100/36045]\tLoss: 1529.8278\n",
      "Training Epoch: 5 [11150/36045]\tLoss: 1659.4135\n",
      "Training Epoch: 5 [11200/36045]\tLoss: 1737.1925\n",
      "Training Epoch: 5 [11250/36045]\tLoss: 1769.7056\n",
      "Training Epoch: 5 [11300/36045]\tLoss: 1730.1924\n",
      "Training Epoch: 5 [11350/36045]\tLoss: 1722.6907\n",
      "Training Epoch: 5 [11400/36045]\tLoss: 1639.9380\n",
      "Training Epoch: 5 [11450/36045]\tLoss: 1589.8750\n",
      "Training Epoch: 5 [11500/36045]\tLoss: 1577.9338\n",
      "Training Epoch: 5 [11550/36045]\tLoss: 1611.7021\n",
      "Training Epoch: 5 [11600/36045]\tLoss: 1733.0397\n",
      "Training Epoch: 5 [11650/36045]\tLoss: 1837.0188\n",
      "Training Epoch: 5 [11700/36045]\tLoss: 1842.3484\n",
      "Training Epoch: 5 [11750/36045]\tLoss: 1865.1548\n",
      "Training Epoch: 5 [11800/36045]\tLoss: 1941.8507\n",
      "Training Epoch: 5 [11850/36045]\tLoss: 1955.9768\n",
      "Training Epoch: 5 [11900/36045]\tLoss: 2220.2832\n",
      "Training Epoch: 5 [11950/36045]\tLoss: 2195.5984\n",
      "Training Epoch: 5 [12000/36045]\tLoss: 2230.7893\n",
      "Training Epoch: 5 [12050/36045]\tLoss: 2156.2324\n",
      "Training Epoch: 5 [12100/36045]\tLoss: 1623.6293\n",
      "Training Epoch: 5 [12150/36045]\tLoss: 1400.8008\n",
      "Training Epoch: 5 [12200/36045]\tLoss: 1390.2288\n",
      "Training Epoch: 5 [12250/36045]\tLoss: 1410.0513\n",
      "Training Epoch: 5 [12300/36045]\tLoss: 1658.7302\n",
      "Training Epoch: 5 [12350/36045]\tLoss: 1731.1479\n",
      "Training Epoch: 5 [12400/36045]\tLoss: 1753.2537\n",
      "Training Epoch: 5 [12450/36045]\tLoss: 1720.9585\n",
      "Training Epoch: 5 [12500/36045]\tLoss: 1791.8016\n",
      "Training Epoch: 5 [12550/36045]\tLoss: 1718.8391\n",
      "Training Epoch: 5 [12600/36045]\tLoss: 1629.7484\n",
      "Training Epoch: 5 [12650/36045]\tLoss: 1635.1605\n",
      "Training Epoch: 5 [12700/36045]\tLoss: 1676.0126\n",
      "Training Epoch: 5 [12750/36045]\tLoss: 1683.4309\n",
      "Training Epoch: 5 [12800/36045]\tLoss: 1659.4498\n",
      "Training Epoch: 5 [12850/36045]\tLoss: 1746.8516\n",
      "Training Epoch: 5 [12900/36045]\tLoss: 1687.2854\n",
      "Training Epoch: 5 [12950/36045]\tLoss: 1668.6090\n",
      "Training Epoch: 5 [13000/36045]\tLoss: 1706.6592\n",
      "Training Epoch: 5 [13050/36045]\tLoss: 1597.8116\n",
      "Training Epoch: 5 [13100/36045]\tLoss: 1669.2133\n",
      "Training Epoch: 5 [13150/36045]\tLoss: 1663.9978\n",
      "Training Epoch: 5 [13200/36045]\tLoss: 1577.0873\n",
      "Training Epoch: 5 [13250/36045]\tLoss: 1650.6648\n",
      "Training Epoch: 5 [13300/36045]\tLoss: 1706.0471\n",
      "Training Epoch: 5 [13350/36045]\tLoss: 1673.0850\n",
      "Training Epoch: 5 [13400/36045]\tLoss: 1678.0814\n",
      "Training Epoch: 5 [13450/36045]\tLoss: 1655.4696\n",
      "Training Epoch: 5 [13500/36045]\tLoss: 1735.0803\n",
      "Training Epoch: 5 [13550/36045]\tLoss: 1884.3088\n",
      "Training Epoch: 5 [13600/36045]\tLoss: 1911.4547\n",
      "Training Epoch: 5 [13650/36045]\tLoss: 2003.7301\n",
      "Training Epoch: 5 [13700/36045]\tLoss: 1829.2916\n",
      "Training Epoch: 5 [13750/36045]\tLoss: 1696.0270\n",
      "Training Epoch: 5 [13800/36045]\tLoss: 1679.7621\n",
      "Training Epoch: 5 [13850/36045]\tLoss: 1673.0651\n",
      "Training Epoch: 5 [13900/36045]\tLoss: 1689.2954\n",
      "Training Epoch: 5 [13950/36045]\tLoss: 1679.1592\n",
      "Training Epoch: 5 [14000/36045]\tLoss: 1728.6859\n",
      "Training Epoch: 5 [14050/36045]\tLoss: 1661.1372\n",
      "Training Epoch: 5 [14100/36045]\tLoss: 1655.4517\n",
      "Training Epoch: 5 [14150/36045]\tLoss: 1623.1284\n",
      "Training Epoch: 5 [14200/36045]\tLoss: 1732.9417\n",
      "Training Epoch: 5 [14250/36045]\tLoss: 1908.6152\n",
      "Training Epoch: 5 [14300/36045]\tLoss: 1911.3331\n",
      "Training Epoch: 5 [14350/36045]\tLoss: 1831.7629\n",
      "Training Epoch: 5 [14400/36045]\tLoss: 1814.6820\n",
      "Training Epoch: 5 [14450/36045]\tLoss: 1878.8789\n",
      "Training Epoch: 5 [14500/36045]\tLoss: 1807.1294\n",
      "Training Epoch: 5 [14550/36045]\tLoss: 1900.9332\n",
      "Training Epoch: 5 [14600/36045]\tLoss: 1854.7584\n",
      "Training Epoch: 5 [14650/36045]\tLoss: 1844.1866\n",
      "Training Epoch: 5 [14700/36045]\tLoss: 1725.5311\n",
      "Training Epoch: 5 [14750/36045]\tLoss: 1499.2379\n",
      "Training Epoch: 5 [14800/36045]\tLoss: 1473.6448\n",
      "Training Epoch: 5 [14850/36045]\tLoss: 1482.1797\n",
      "Training Epoch: 5 [14900/36045]\tLoss: 1473.9641\n",
      "Training Epoch: 5 [14950/36045]\tLoss: 1518.0348\n",
      "Training Epoch: 5 [15000/36045]\tLoss: 1585.7164\n",
      "Training Epoch: 5 [15050/36045]\tLoss: 1607.3658\n",
      "Training Epoch: 5 [15100/36045]\tLoss: 1574.3204\n",
      "Training Epoch: 5 [15150/36045]\tLoss: 1499.3053\n",
      "Training Epoch: 5 [15200/36045]\tLoss: 1359.1160\n",
      "Training Epoch: 5 [15250/36045]\tLoss: 1409.0785\n",
      "Training Epoch: 5 [15300/36045]\tLoss: 1381.0994\n",
      "Training Epoch: 5 [15350/36045]\tLoss: 1403.1482\n",
      "Training Epoch: 5 [15400/36045]\tLoss: 1502.8165\n",
      "Training Epoch: 5 [15450/36045]\tLoss: 1504.0818\n",
      "Training Epoch: 5 [15500/36045]\tLoss: 1534.7507\n",
      "Training Epoch: 5 [15550/36045]\tLoss: 1492.8802\n",
      "Training Epoch: 5 [15600/36045]\tLoss: 1571.9257\n",
      "Training Epoch: 5 [15650/36045]\tLoss: 1616.6866\n",
      "Training Epoch: 5 [15700/36045]\tLoss: 1581.1991\n",
      "Training Epoch: 5 [15750/36045]\tLoss: 1557.7144\n",
      "Training Epoch: 5 [15800/36045]\tLoss: 1396.5687\n",
      "Training Epoch: 5 [15850/36045]\tLoss: 1386.0613\n",
      "Training Epoch: 5 [15900/36045]\tLoss: 1384.8510\n",
      "Training Epoch: 5 [15950/36045]\tLoss: 1426.8467\n",
      "Training Epoch: 5 [16000/36045]\tLoss: 1419.6035\n",
      "Training Epoch: 5 [16050/36045]\tLoss: 1355.6987\n",
      "Training Epoch: 5 [16100/36045]\tLoss: 1276.7792\n",
      "Training Epoch: 5 [16150/36045]\tLoss: 1245.1405\n",
      "Training Epoch: 5 [16200/36045]\tLoss: 1464.9368\n",
      "Training Epoch: 5 [16250/36045]\tLoss: 1526.4276\n",
      "Training Epoch: 5 [16300/36045]\tLoss: 1630.1476\n",
      "Training Epoch: 5 [16350/36045]\tLoss: 1646.1140\n",
      "Training Epoch: 5 [16400/36045]\tLoss: 1606.2125\n",
      "Training Epoch: 5 [16450/36045]\tLoss: 1592.9216\n",
      "Training Epoch: 5 [16500/36045]\tLoss: 1604.4916\n",
      "Training Epoch: 5 [16550/36045]\tLoss: 1533.1847\n",
      "Training Epoch: 5 [16600/36045]\tLoss: 1604.0375\n",
      "Training Epoch: 5 [16650/36045]\tLoss: 1644.2506\n",
      "Training Epoch: 5 [16700/36045]\tLoss: 1610.0406\n",
      "Training Epoch: 5 [16750/36045]\tLoss: 1586.1578\n",
      "Training Epoch: 5 [16800/36045]\tLoss: 1625.3201\n",
      "Training Epoch: 5 [16850/36045]\tLoss: 1558.9117\n",
      "Training Epoch: 5 [16900/36045]\tLoss: 1556.5984\n",
      "Training Epoch: 5 [16950/36045]\tLoss: 1562.2318\n",
      "Training Epoch: 5 [17000/36045]\tLoss: 1540.2810\n",
      "Training Epoch: 5 [17050/36045]\tLoss: 1636.1497\n",
      "Training Epoch: 5 [17100/36045]\tLoss: 1646.6875\n",
      "Training Epoch: 5 [17150/36045]\tLoss: 1481.8525\n",
      "Training Epoch: 5 [17200/36045]\tLoss: 1442.7028\n",
      "Training Epoch: 5 [17250/36045]\tLoss: 1496.3508\n",
      "Training Epoch: 5 [17300/36045]\tLoss: 1586.6057\n",
      "Training Epoch: 5 [17350/36045]\tLoss: 1469.4303\n",
      "Training Epoch: 5 [17400/36045]\tLoss: 1460.1355\n",
      "Training Epoch: 5 [17450/36045]\tLoss: 1521.8068\n",
      "Training Epoch: 5 [17500/36045]\tLoss: 1502.9584\n",
      "Training Epoch: 5 [17550/36045]\tLoss: 1527.6145\n",
      "Training Epoch: 5 [17600/36045]\tLoss: 1498.1237\n",
      "Training Epoch: 5 [17650/36045]\tLoss: 1551.5199\n",
      "Training Epoch: 5 [17700/36045]\tLoss: 1497.1716\n",
      "Training Epoch: 5 [17750/36045]\tLoss: 1529.6051\n",
      "Training Epoch: 5 [17800/36045]\tLoss: 1524.0480\n",
      "Training Epoch: 5 [17850/36045]\tLoss: 1439.9464\n",
      "Training Epoch: 5 [17900/36045]\tLoss: 1484.8368\n",
      "Training Epoch: 5 [17950/36045]\tLoss: 1501.8414\n",
      "Training Epoch: 5 [18000/36045]\tLoss: 1475.2821\n",
      "Training Epoch: 5 [18050/36045]\tLoss: 1667.5500\n",
      "Training Epoch: 5 [18100/36045]\tLoss: 1701.4915\n",
      "Training Epoch: 5 [18150/36045]\tLoss: 1695.8264\n",
      "Training Epoch: 5 [18200/36045]\tLoss: 1687.0325\n",
      "Training Epoch: 5 [18250/36045]\tLoss: 1722.0081\n",
      "Training Epoch: 5 [18300/36045]\tLoss: 1593.5400\n",
      "Training Epoch: 5 [18350/36045]\tLoss: 1652.7386\n",
      "Training Epoch: 5 [18400/36045]\tLoss: 1652.4773\n",
      "Training Epoch: 5 [18450/36045]\tLoss: 1607.9073\n",
      "Training Epoch: 5 [18500/36045]\tLoss: 1578.6003\n",
      "Training Epoch: 5 [18550/36045]\tLoss: 1534.3146\n",
      "Training Epoch: 5 [18600/36045]\tLoss: 1511.5688\n",
      "Training Epoch: 5 [18650/36045]\tLoss: 1582.6948\n",
      "Training Epoch: 5 [18700/36045]\tLoss: 1662.7166\n",
      "Training Epoch: 5 [18750/36045]\tLoss: 1648.8683\n",
      "Training Epoch: 5 [18800/36045]\tLoss: 1713.9282\n",
      "Training Epoch: 5 [18850/36045]\tLoss: 1621.2039\n",
      "Training Epoch: 5 [18900/36045]\tLoss: 1722.9707\n",
      "Training Epoch: 5 [18950/36045]\tLoss: 1639.4491\n",
      "Training Epoch: 5 [19000/36045]\tLoss: 1479.8715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [19050/36045]\tLoss: 1411.3969\n",
      "Training Epoch: 5 [19100/36045]\tLoss: 1451.3817\n",
      "Training Epoch: 5 [19150/36045]\tLoss: 1419.4082\n",
      "Training Epoch: 5 [19200/36045]\tLoss: 1472.0907\n",
      "Training Epoch: 5 [19250/36045]\tLoss: 1473.3242\n",
      "Training Epoch: 5 [19300/36045]\tLoss: 1527.0262\n",
      "Training Epoch: 5 [19350/36045]\tLoss: 1481.6774\n",
      "Training Epoch: 5 [19400/36045]\tLoss: 1527.2383\n",
      "Training Epoch: 5 [19450/36045]\tLoss: 1502.3994\n",
      "Training Epoch: 5 [19500/36045]\tLoss: 1511.7340\n",
      "Training Epoch: 5 [19550/36045]\tLoss: 1524.8340\n",
      "Training Epoch: 5 [19600/36045]\tLoss: 1578.8397\n",
      "Training Epoch: 5 [19650/36045]\tLoss: 1948.5708\n",
      "Training Epoch: 5 [19700/36045]\tLoss: 1886.8994\n",
      "Training Epoch: 5 [19750/36045]\tLoss: 1882.5538\n",
      "Training Epoch: 5 [19800/36045]\tLoss: 1861.2150\n",
      "Training Epoch: 5 [19850/36045]\tLoss: 1356.4458\n",
      "Training Epoch: 5 [19900/36045]\tLoss: 1315.9218\n",
      "Training Epoch: 5 [19950/36045]\tLoss: 1324.5316\n",
      "Training Epoch: 5 [20000/36045]\tLoss: 1316.1628\n",
      "Training Epoch: 5 [20050/36045]\tLoss: 1474.9453\n",
      "Training Epoch: 5 [20100/36045]\tLoss: 1468.6683\n",
      "Training Epoch: 5 [20150/36045]\tLoss: 1484.8859\n",
      "Training Epoch: 5 [20200/36045]\tLoss: 1473.5367\n",
      "Training Epoch: 5 [20250/36045]\tLoss: 1560.4558\n",
      "Training Epoch: 5 [20300/36045]\tLoss: 1607.1719\n",
      "Training Epoch: 5 [20350/36045]\tLoss: 1659.9468\n",
      "Training Epoch: 5 [20400/36045]\tLoss: 1682.5293\n",
      "Training Epoch: 5 [20450/36045]\tLoss: 1653.4838\n",
      "Training Epoch: 5 [20500/36045]\tLoss: 1599.5580\n",
      "Training Epoch: 5 [20550/36045]\tLoss: 1457.5808\n",
      "Training Epoch: 5 [20600/36045]\tLoss: 1492.3439\n",
      "Training Epoch: 5 [20650/36045]\tLoss: 1469.2777\n",
      "Training Epoch: 5 [20700/36045]\tLoss: 1459.2213\n",
      "Training Epoch: 5 [20750/36045]\tLoss: 1533.5901\n",
      "Training Epoch: 5 [20800/36045]\tLoss: 1661.6150\n",
      "Training Epoch: 5 [20850/36045]\tLoss: 1648.2708\n",
      "Training Epoch: 5 [20900/36045]\tLoss: 1728.7531\n",
      "Training Epoch: 5 [20950/36045]\tLoss: 1654.1719\n",
      "Training Epoch: 5 [21000/36045]\tLoss: 1546.2230\n",
      "Training Epoch: 5 [21050/36045]\tLoss: 1335.4271\n",
      "Training Epoch: 5 [21100/36045]\tLoss: 1334.9978\n",
      "Training Epoch: 5 [21150/36045]\tLoss: 1416.4960\n",
      "Training Epoch: 5 [21200/36045]\tLoss: 1426.5112\n",
      "Training Epoch: 5 [21250/36045]\tLoss: 1350.5442\n",
      "Training Epoch: 5 [21300/36045]\tLoss: 1622.9172\n",
      "Training Epoch: 5 [21350/36045]\tLoss: 1598.0968\n",
      "Training Epoch: 5 [21400/36045]\tLoss: 1618.7056\n",
      "Training Epoch: 5 [21450/36045]\tLoss: 1637.8964\n",
      "Training Epoch: 5 [21500/36045]\tLoss: 1650.3892\n",
      "Training Epoch: 5 [21550/36045]\tLoss: 1755.6849\n",
      "Training Epoch: 5 [21600/36045]\tLoss: 1754.5950\n",
      "Training Epoch: 5 [21650/36045]\tLoss: 1775.8712\n",
      "Training Epoch: 5 [21700/36045]\tLoss: 1769.8740\n",
      "Training Epoch: 5 [21750/36045]\tLoss: 1709.8033\n",
      "Training Epoch: 5 [21800/36045]\tLoss: 1325.1514\n",
      "Training Epoch: 5 [21850/36045]\tLoss: 1294.6125\n",
      "Training Epoch: 5 [21900/36045]\tLoss: 1322.0542\n",
      "Training Epoch: 5 [21950/36045]\tLoss: 1309.5671\n",
      "Training Epoch: 5 [22000/36045]\tLoss: 1322.0967\n",
      "Training Epoch: 5 [22050/36045]\tLoss: 1451.2493\n",
      "Training Epoch: 5 [22100/36045]\tLoss: 1423.2675\n",
      "Training Epoch: 5 [22150/36045]\tLoss: 1395.4562\n",
      "Training Epoch: 5 [22200/36045]\tLoss: 1417.5013\n",
      "Training Epoch: 5 [22250/36045]\tLoss: 1440.6897\n",
      "Training Epoch: 5 [22300/36045]\tLoss: 1519.3834\n",
      "Training Epoch: 5 [22350/36045]\tLoss: 1538.7433\n",
      "Training Epoch: 5 [22400/36045]\tLoss: 1576.1945\n",
      "Training Epoch: 5 [22450/36045]\tLoss: 1557.3728\n",
      "Training Epoch: 5 [22500/36045]\tLoss: 1507.9919\n",
      "Training Epoch: 5 [22550/36045]\tLoss: 1596.4379\n",
      "Training Epoch: 5 [22600/36045]\tLoss: 1753.4056\n",
      "Training Epoch: 5 [22650/36045]\tLoss: 1821.0468\n",
      "Training Epoch: 5 [22700/36045]\tLoss: 1865.6442\n",
      "Training Epoch: 5 [22750/36045]\tLoss: 1897.6086\n",
      "Training Epoch: 5 [22800/36045]\tLoss: 1981.6372\n",
      "Training Epoch: 5 [22850/36045]\tLoss: 1659.8153\n",
      "Training Epoch: 5 [22900/36045]\tLoss: 1654.5470\n",
      "Training Epoch: 5 [22950/36045]\tLoss: 1622.8361\n",
      "Training Epoch: 5 [23000/36045]\tLoss: 1639.8016\n",
      "Training Epoch: 5 [23050/36045]\tLoss: 1481.2955\n",
      "Training Epoch: 5 [23100/36045]\tLoss: 1508.1637\n",
      "Training Epoch: 5 [23150/36045]\tLoss: 1487.0715\n",
      "Training Epoch: 5 [23200/36045]\tLoss: 1408.6532\n",
      "Training Epoch: 5 [23250/36045]\tLoss: 1403.4491\n",
      "Training Epoch: 5 [23300/36045]\tLoss: 1386.7554\n",
      "Training Epoch: 5 [23350/36045]\tLoss: 1410.6737\n",
      "Training Epoch: 5 [23400/36045]\tLoss: 1525.5199\n",
      "Training Epoch: 5 [23450/36045]\tLoss: 1507.5865\n",
      "Training Epoch: 5 [23500/36045]\tLoss: 1447.1989\n",
      "Training Epoch: 5 [23550/36045]\tLoss: 1568.1030\n",
      "Training Epoch: 5 [23600/36045]\tLoss: 1737.0925\n",
      "Training Epoch: 5 [23650/36045]\tLoss: 1796.2292\n",
      "Training Epoch: 5 [23700/36045]\tLoss: 1802.1155\n",
      "Training Epoch: 5 [23750/36045]\tLoss: 1757.2238\n",
      "Training Epoch: 5 [23800/36045]\tLoss: 1448.4695\n",
      "Training Epoch: 5 [23850/36045]\tLoss: 1495.2644\n",
      "Training Epoch: 5 [23900/36045]\tLoss: 1479.1184\n",
      "Training Epoch: 5 [23950/36045]\tLoss: 1459.2947\n",
      "Training Epoch: 5 [24000/36045]\tLoss: 1412.8328\n",
      "Training Epoch: 5 [24050/36045]\tLoss: 1280.9115\n",
      "Training Epoch: 5 [24100/36045]\tLoss: 1339.3545\n",
      "Training Epoch: 5 [24150/36045]\tLoss: 1359.0186\n",
      "Training Epoch: 5 [24200/36045]\tLoss: 1329.4589\n",
      "Training Epoch: 5 [24250/36045]\tLoss: 1282.7941\n",
      "Training Epoch: 5 [24300/36045]\tLoss: 1394.7289\n",
      "Training Epoch: 5 [24350/36045]\tLoss: 1437.5425\n",
      "Training Epoch: 5 [24400/36045]\tLoss: 1463.1667\n",
      "Training Epoch: 5 [24450/36045]\tLoss: 1417.9655\n",
      "Training Epoch: 5 [24500/36045]\tLoss: 1502.5105\n",
      "Training Epoch: 5 [24550/36045]\tLoss: 1604.1909\n",
      "Training Epoch: 5 [24600/36045]\tLoss: 1591.2906\n",
      "Training Epoch: 5 [24650/36045]\tLoss: 1550.8967\n",
      "Training Epoch: 5 [24700/36045]\tLoss: 1576.8652\n",
      "Training Epoch: 5 [24750/36045]\tLoss: 1445.0730\n",
      "Training Epoch: 5 [24800/36045]\tLoss: 1290.3680\n",
      "Training Epoch: 5 [24850/36045]\tLoss: 1314.8241\n",
      "Training Epoch: 5 [24900/36045]\tLoss: 1316.5754\n",
      "Training Epoch: 5 [24950/36045]\tLoss: 1314.5848\n",
      "Training Epoch: 5 [25000/36045]\tLoss: 1268.7496\n",
      "Training Epoch: 5 [25050/36045]\tLoss: 1193.6962\n",
      "Training Epoch: 5 [25100/36045]\tLoss: 1079.1660\n",
      "Training Epoch: 5 [25150/36045]\tLoss: 1003.9807\n",
      "Training Epoch: 5 [25200/36045]\tLoss: 1003.3582\n",
      "Training Epoch: 5 [25250/36045]\tLoss: 1071.2396\n",
      "Training Epoch: 5 [25300/36045]\tLoss: 1400.7705\n",
      "Training Epoch: 5 [25350/36045]\tLoss: 1390.3314\n",
      "Training Epoch: 5 [25400/36045]\tLoss: 1294.8160\n",
      "Training Epoch: 5 [25450/36045]\tLoss: 1304.5981\n",
      "Training Epoch: 5 [25500/36045]\tLoss: 1417.4257\n",
      "Training Epoch: 5 [25550/36045]\tLoss: 1593.2052\n",
      "Training Epoch: 5 [25600/36045]\tLoss: 1588.0967\n",
      "Training Epoch: 5 [25650/36045]\tLoss: 1545.2521\n",
      "Training Epoch: 5 [25700/36045]\tLoss: 1599.6151\n",
      "Training Epoch: 5 [25750/36045]\tLoss: 1522.3091\n",
      "Training Epoch: 5 [25800/36045]\tLoss: 936.7792\n",
      "Training Epoch: 5 [25850/36045]\tLoss: 952.1373\n",
      "Training Epoch: 5 [25900/36045]\tLoss: 905.0963\n",
      "Training Epoch: 5 [25950/36045]\tLoss: 922.8967\n",
      "Training Epoch: 5 [26000/36045]\tLoss: 1143.6807\n",
      "Training Epoch: 5 [26050/36045]\tLoss: 1557.0670\n",
      "Training Epoch: 5 [26100/36045]\tLoss: 1607.1792\n",
      "Training Epoch: 5 [26150/36045]\tLoss: 1594.0137\n",
      "Training Epoch: 5 [26200/36045]\tLoss: 1554.1333\n",
      "Training Epoch: 5 [26250/36045]\tLoss: 1593.6041\n",
      "Training Epoch: 5 [26300/36045]\tLoss: 1360.9534\n",
      "Training Epoch: 5 [26350/36045]\tLoss: 1362.1853\n",
      "Training Epoch: 5 [26400/36045]\tLoss: 1353.4568\n",
      "Training Epoch: 5 [26450/36045]\tLoss: 1264.5635\n",
      "Training Epoch: 5 [26500/36045]\tLoss: 1532.8264\n",
      "Training Epoch: 5 [26550/36045]\tLoss: 1576.0809\n",
      "Training Epoch: 5 [26600/36045]\tLoss: 1560.2327\n",
      "Training Epoch: 5 [26650/36045]\tLoss: 1592.1860\n",
      "Training Epoch: 5 [26700/36045]\tLoss: 1563.2228\n",
      "Training Epoch: 5 [26750/36045]\tLoss: 1465.4150\n",
      "Training Epoch: 5 [26800/36045]\tLoss: 1074.8053\n",
      "Training Epoch: 5 [26850/36045]\tLoss: 905.0001\n",
      "Training Epoch: 5 [26900/36045]\tLoss: 917.5560\n",
      "Training Epoch: 5 [26950/36045]\tLoss: 999.8903\n",
      "Training Epoch: 5 [27000/36045]\tLoss: 1539.7030\n",
      "Training Epoch: 5 [27050/36045]\tLoss: 1629.5457\n",
      "Training Epoch: 5 [27100/36045]\tLoss: 1576.9370\n",
      "Training Epoch: 5 [27150/36045]\tLoss: 1641.7422\n",
      "Training Epoch: 5 [27200/36045]\tLoss: 1266.9961\n",
      "Training Epoch: 5 [27250/36045]\tLoss: 1255.2363\n",
      "Training Epoch: 5 [27300/36045]\tLoss: 1208.5145\n",
      "Training Epoch: 5 [27350/36045]\tLoss: 1233.2064\n",
      "Training Epoch: 5 [27400/36045]\tLoss: 1217.2054\n",
      "Training Epoch: 5 [27450/36045]\tLoss: 1513.2435\n",
      "Training Epoch: 5 [27500/36045]\tLoss: 1607.9843\n",
      "Training Epoch: 5 [27550/36045]\tLoss: 1597.4630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [27600/36045]\tLoss: 1595.6600\n",
      "Training Epoch: 5 [27650/36045]\tLoss: 1607.4274\n",
      "Training Epoch: 5 [27700/36045]\tLoss: 1645.6903\n",
      "Training Epoch: 5 [27750/36045]\tLoss: 1659.8855\n",
      "Training Epoch: 5 [27800/36045]\tLoss: 1640.6549\n",
      "Training Epoch: 5 [27850/36045]\tLoss: 1596.3042\n",
      "Training Epoch: 5 [27900/36045]\tLoss: 1410.8473\n",
      "Training Epoch: 5 [27950/36045]\tLoss: 1166.3824\n",
      "Training Epoch: 5 [28000/36045]\tLoss: 1124.0522\n",
      "Training Epoch: 5 [28050/36045]\tLoss: 1162.5381\n",
      "Training Epoch: 5 [28100/36045]\tLoss: 1143.2360\n",
      "Training Epoch: 5 [28150/36045]\tLoss: 1256.4796\n",
      "Training Epoch: 5 [28200/36045]\tLoss: 1276.5156\n",
      "Training Epoch: 5 [28250/36045]\tLoss: 1270.3347\n",
      "Training Epoch: 5 [28300/36045]\tLoss: 1214.6300\n",
      "Training Epoch: 5 [28350/36045]\tLoss: 1216.8291\n",
      "Training Epoch: 5 [28400/36045]\tLoss: 1648.9065\n",
      "Training Epoch: 5 [28450/36045]\tLoss: 1470.4919\n",
      "Training Epoch: 5 [28500/36045]\tLoss: 1248.8882\n",
      "Training Epoch: 5 [28550/36045]\tLoss: 1144.7349\n",
      "Training Epoch: 5 [28600/36045]\tLoss: 1332.7225\n",
      "Training Epoch: 5 [28650/36045]\tLoss: 1637.2173\n",
      "Training Epoch: 5 [28700/36045]\tLoss: 1654.4576\n",
      "Training Epoch: 5 [28750/36045]\tLoss: 1628.4495\n",
      "Training Epoch: 5 [28800/36045]\tLoss: 1643.4812\n",
      "Training Epoch: 5 [28850/36045]\tLoss: 1395.1075\n",
      "Training Epoch: 5 [28900/36045]\tLoss: 1059.7229\n",
      "Training Epoch: 5 [28950/36045]\tLoss: 1051.1792\n",
      "Training Epoch: 5 [29000/36045]\tLoss: 1061.8033\n",
      "Training Epoch: 5 [29050/36045]\tLoss: 1076.5072\n",
      "Training Epoch: 5 [29100/36045]\tLoss: 1110.3158\n",
      "Training Epoch: 5 [29150/36045]\tLoss: 1081.1931\n",
      "Training Epoch: 5 [29200/36045]\tLoss: 1060.6605\n",
      "Training Epoch: 5 [29250/36045]\tLoss: 1028.4955\n",
      "Training Epoch: 5 [29300/36045]\tLoss: 1216.3215\n",
      "Training Epoch: 5 [29350/36045]\tLoss: 1486.0051\n",
      "Training Epoch: 5 [29400/36045]\tLoss: 1536.3654\n",
      "Training Epoch: 5 [29450/36045]\tLoss: 1599.5565\n",
      "Training Epoch: 5 [29500/36045]\tLoss: 1616.9802\n",
      "Training Epoch: 5 [29550/36045]\tLoss: 1534.9425\n",
      "Training Epoch: 5 [29600/36045]\tLoss: 1352.5688\n",
      "Training Epoch: 5 [29650/36045]\tLoss: 1315.4286\n",
      "Training Epoch: 5 [29700/36045]\tLoss: 1157.5707\n",
      "Training Epoch: 5 [29750/36045]\tLoss: 1177.0686\n",
      "Training Epoch: 5 [29800/36045]\tLoss: 1246.1912\n",
      "Training Epoch: 5 [29850/36045]\tLoss: 1306.2662\n",
      "Training Epoch: 5 [29900/36045]\tLoss: 1303.1646\n",
      "Training Epoch: 5 [29950/36045]\tLoss: 1315.4097\n",
      "Training Epoch: 5 [30000/36045]\tLoss: 1306.6569\n",
      "Training Epoch: 5 [30050/36045]\tLoss: 1308.4193\n",
      "Training Epoch: 5 [30100/36045]\tLoss: 1661.7117\n",
      "Training Epoch: 5 [30150/36045]\tLoss: 1667.9445\n",
      "Training Epoch: 5 [30200/36045]\tLoss: 1590.2310\n",
      "Training Epoch: 5 [30250/36045]\tLoss: 1660.8954\n",
      "Training Epoch: 5 [30300/36045]\tLoss: 1659.5103\n",
      "Training Epoch: 5 [30350/36045]\tLoss: 1340.8152\n",
      "Training Epoch: 5 [30400/36045]\tLoss: 1319.4216\n",
      "Training Epoch: 5 [30450/36045]\tLoss: 1310.4580\n",
      "Training Epoch: 5 [30500/36045]\tLoss: 1220.5042\n",
      "Training Epoch: 5 [30550/36045]\tLoss: 1144.4263\n",
      "Training Epoch: 5 [30600/36045]\tLoss: 1084.1914\n",
      "Training Epoch: 5 [30650/36045]\tLoss: 1079.8079\n",
      "Training Epoch: 5 [30700/36045]\tLoss: 1105.2235\n",
      "Training Epoch: 5 [30750/36045]\tLoss: 1084.3812\n",
      "Training Epoch: 5 [30800/36045]\tLoss: 1126.3807\n",
      "Training Epoch: 5 [30850/36045]\tLoss: 1117.6781\n",
      "Training Epoch: 5 [30900/36045]\tLoss: 1150.8582\n",
      "Training Epoch: 5 [30950/36045]\tLoss: 1207.9028\n",
      "Training Epoch: 5 [31000/36045]\tLoss: 1191.3726\n",
      "Training Epoch: 5 [31050/36045]\tLoss: 1001.6368\n",
      "Training Epoch: 5 [31100/36045]\tLoss: 985.5660\n",
      "Training Epoch: 5 [31150/36045]\tLoss: 989.4945\n",
      "Training Epoch: 5 [31200/36045]\tLoss: 1256.6660\n",
      "Training Epoch: 5 [31250/36045]\tLoss: 1623.8931\n",
      "Training Epoch: 5 [31300/36045]\tLoss: 1558.9485\n",
      "Training Epoch: 5 [31350/36045]\tLoss: 1577.7549\n",
      "Training Epoch: 5 [31400/36045]\tLoss: 1560.3745\n",
      "Training Epoch: 5 [31450/36045]\tLoss: 1518.5376\n",
      "Training Epoch: 5 [31500/36045]\tLoss: 1520.1931\n",
      "Training Epoch: 5 [31550/36045]\tLoss: 1536.6398\n",
      "Training Epoch: 5 [31600/36045]\tLoss: 1447.4275\n",
      "Training Epoch: 5 [31650/36045]\tLoss: 1548.8839\n",
      "Training Epoch: 5 [31700/36045]\tLoss: 1191.1146\n",
      "Training Epoch: 5 [31750/36045]\tLoss: 1003.9069\n",
      "Training Epoch: 5 [31800/36045]\tLoss: 944.4503\n",
      "Training Epoch: 5 [31850/36045]\tLoss: 973.2229\n",
      "Training Epoch: 5 [31900/36045]\tLoss: 1430.9652\n",
      "Training Epoch: 5 [31950/36045]\tLoss: 1753.6594\n",
      "Training Epoch: 5 [32000/36045]\tLoss: 1932.3152\n",
      "Training Epoch: 5 [32050/36045]\tLoss: 1863.6458\n",
      "Training Epoch: 5 [32100/36045]\tLoss: 1836.4042\n",
      "Training Epoch: 5 [32150/36045]\tLoss: 1564.2871\n",
      "Training Epoch: 5 [32200/36045]\tLoss: 1583.9265\n",
      "Training Epoch: 5 [32250/36045]\tLoss: 1599.7734\n",
      "Training Epoch: 5 [32300/36045]\tLoss: 1570.8359\n",
      "Training Epoch: 5 [32350/36045]\tLoss: 1530.6830\n",
      "Training Epoch: 5 [32400/36045]\tLoss: 1448.2108\n",
      "Training Epoch: 5 [32450/36045]\tLoss: 1238.1566\n",
      "Training Epoch: 5 [32500/36045]\tLoss: 1198.9857\n",
      "Training Epoch: 5 [32550/36045]\tLoss: 1212.1118\n",
      "Training Epoch: 5 [32600/36045]\tLoss: 1200.5885\n",
      "Training Epoch: 5 [32650/36045]\tLoss: 1464.7574\n",
      "Training Epoch: 5 [32700/36045]\tLoss: 1576.5377\n",
      "Training Epoch: 5 [32750/36045]\tLoss: 1520.6462\n",
      "Training Epoch: 5 [32800/36045]\tLoss: 1562.3746\n",
      "Training Epoch: 5 [32850/36045]\tLoss: 1442.3184\n",
      "Training Epoch: 5 [32900/36045]\tLoss: 1149.7485\n",
      "Training Epoch: 5 [32950/36045]\tLoss: 1204.3914\n",
      "Training Epoch: 5 [33000/36045]\tLoss: 1222.9307\n",
      "Training Epoch: 5 [33050/36045]\tLoss: 1142.5344\n",
      "Training Epoch: 5 [33100/36045]\tLoss: 1307.8823\n",
      "Training Epoch: 5 [33150/36045]\tLoss: 1711.1825\n",
      "Training Epoch: 5 [33200/36045]\tLoss: 1676.0276\n",
      "Training Epoch: 5 [33250/36045]\tLoss: 1716.7983\n",
      "Training Epoch: 5 [33300/36045]\tLoss: 1817.9523\n",
      "Training Epoch: 5 [33350/36045]\tLoss: 1437.0316\n",
      "Training Epoch: 5 [33400/36045]\tLoss: 1113.6179\n",
      "Training Epoch: 5 [33450/36045]\tLoss: 1103.0774\n",
      "Training Epoch: 5 [33500/36045]\tLoss: 1132.1862\n",
      "Training Epoch: 5 [33550/36045]\tLoss: 1169.0260\n",
      "Training Epoch: 5 [33600/36045]\tLoss: 1178.5894\n",
      "Training Epoch: 5 [33650/36045]\tLoss: 1486.2673\n",
      "Training Epoch: 5 [33700/36045]\tLoss: 1433.2046\n",
      "Training Epoch: 5 [33750/36045]\tLoss: 1487.5251\n",
      "Training Epoch: 5 [33800/36045]\tLoss: 1473.1703\n",
      "Training Epoch: 5 [33850/36045]\tLoss: 1472.6976\n",
      "Training Epoch: 5 [33900/36045]\tLoss: 1510.0470\n",
      "Training Epoch: 5 [33950/36045]\tLoss: 1530.7457\n",
      "Training Epoch: 5 [34000/36045]\tLoss: 1512.5398\n",
      "Training Epoch: 5 [34050/36045]\tLoss: 1540.2146\n",
      "Training Epoch: 5 [34100/36045]\tLoss: 1470.6663\n",
      "Training Epoch: 5 [34150/36045]\tLoss: 1383.7733\n",
      "Training Epoch: 5 [34200/36045]\tLoss: 1314.9972\n",
      "Training Epoch: 5 [34250/36045]\tLoss: 1323.3496\n",
      "Training Epoch: 5 [34300/36045]\tLoss: 1155.2083\n",
      "Training Epoch: 5 [34350/36045]\tLoss: 1192.8580\n",
      "Training Epoch: 5 [34400/36045]\tLoss: 1154.1042\n",
      "Training Epoch: 5 [34450/36045]\tLoss: 1079.4685\n",
      "Training Epoch: 5 [34500/36045]\tLoss: 1160.0773\n",
      "Training Epoch: 5 [34550/36045]\tLoss: 1150.4688\n",
      "Training Epoch: 5 [34600/36045]\tLoss: 1106.0743\n",
      "Training Epoch: 5 [34650/36045]\tLoss: 1280.8453\n",
      "Training Epoch: 5 [34700/36045]\tLoss: 1343.7037\n",
      "Training Epoch: 5 [34750/36045]\tLoss: 1198.4116\n",
      "Training Epoch: 5 [34800/36045]\tLoss: 1343.0118\n",
      "Training Epoch: 5 [34850/36045]\tLoss: 1388.5155\n",
      "Training Epoch: 5 [34900/36045]\tLoss: 1703.5402\n",
      "Training Epoch: 5 [34950/36045]\tLoss: 1698.3708\n",
      "Training Epoch: 5 [35000/36045]\tLoss: 1715.8232\n",
      "Training Epoch: 5 [35050/36045]\tLoss: 1667.1550\n",
      "Training Epoch: 5 [35100/36045]\tLoss: 1276.1079\n",
      "Training Epoch: 5 [35150/36045]\tLoss: 1263.2451\n",
      "Training Epoch: 5 [35200/36045]\tLoss: 1112.0875\n",
      "Training Epoch: 5 [35250/36045]\tLoss: 1199.2882\n",
      "Training Epoch: 5 [35300/36045]\tLoss: 1216.5763\n",
      "Training Epoch: 5 [35350/36045]\tLoss: 1446.7761\n",
      "Training Epoch: 5 [35400/36045]\tLoss: 1549.3927\n",
      "Training Epoch: 5 [35450/36045]\tLoss: 1484.3949\n",
      "Training Epoch: 5 [35500/36045]\tLoss: 1450.8094\n",
      "Training Epoch: 5 [35550/36045]\tLoss: 1432.1461\n",
      "Training Epoch: 5 [35600/36045]\tLoss: 1502.7262\n",
      "Training Epoch: 5 [35650/36045]\tLoss: 1628.2524\n",
      "Training Epoch: 5 [35700/36045]\tLoss: 1519.4585\n",
      "Training Epoch: 5 [35750/36045]\tLoss: 1608.6584\n",
      "Training Epoch: 5 [35800/36045]\tLoss: 1622.1614\n",
      "Training Epoch: 5 [35850/36045]\tLoss: 1581.8004\n",
      "Training Epoch: 5 [35900/36045]\tLoss: 1622.6719\n",
      "Training Epoch: 5 [35950/36045]\tLoss: 1618.0167\n",
      "Training Epoch: 5 [36000/36045]\tLoss: 1591.1497\n",
      "Training Epoch: 5 [36045/36045]\tLoss: 1568.4602\n",
      "Training Epoch: 5 [4004/4004]\tLoss: 1519.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [50/36045]\tLoss: 1532.8119\n",
      "Training Epoch: 6 [100/36045]\tLoss: 1485.0001\n",
      "Training Epoch: 6 [150/36045]\tLoss: 1490.0028\n",
      "Training Epoch: 6 [200/36045]\tLoss: 1470.2069\n",
      "Training Epoch: 6 [250/36045]\tLoss: 1649.8191\n",
      "Training Epoch: 6 [300/36045]\tLoss: 1732.4031\n",
      "Training Epoch: 6 [350/36045]\tLoss: 1659.5319\n",
      "Training Epoch: 6 [400/36045]\tLoss: 1687.5723\n",
      "Training Epoch: 6 [450/36045]\tLoss: 1651.6045\n",
      "Training Epoch: 6 [500/36045]\tLoss: 1570.1180\n",
      "Training Epoch: 6 [550/36045]\tLoss: 1584.8263\n",
      "Training Epoch: 6 [600/36045]\tLoss: 1517.9548\n",
      "Training Epoch: 6 [650/36045]\tLoss: 1576.8571\n",
      "Training Epoch: 6 [700/36045]\tLoss: 1573.2942\n",
      "Training Epoch: 6 [750/36045]\tLoss: 1553.6885\n",
      "Training Epoch: 6 [800/36045]\tLoss: 1583.5951\n",
      "Training Epoch: 6 [850/36045]\tLoss: 1544.8134\n",
      "Training Epoch: 6 [900/36045]\tLoss: 1460.3579\n",
      "Training Epoch: 6 [950/36045]\tLoss: 1409.8766\n",
      "Training Epoch: 6 [1000/36045]\tLoss: 1367.5071\n",
      "Training Epoch: 6 [1050/36045]\tLoss: 1368.3551\n",
      "Training Epoch: 6 [1100/36045]\tLoss: 1333.6456\n",
      "Training Epoch: 6 [1150/36045]\tLoss: 1331.7981\n",
      "Training Epoch: 6 [1200/36045]\tLoss: 1399.4668\n",
      "Training Epoch: 6 [1250/36045]\tLoss: 1535.2711\n",
      "Training Epoch: 6 [1300/36045]\tLoss: 1520.5562\n",
      "Training Epoch: 6 [1350/36045]\tLoss: 1536.9403\n",
      "Training Epoch: 6 [1400/36045]\tLoss: 1591.9728\n",
      "Training Epoch: 6 [1450/36045]\tLoss: 1538.1945\n",
      "Training Epoch: 6 [1500/36045]\tLoss: 1443.3552\n",
      "Training Epoch: 6 [1550/36045]\tLoss: 1504.4952\n",
      "Training Epoch: 6 [1600/36045]\tLoss: 1509.5189\n",
      "Training Epoch: 6 [1650/36045]\tLoss: 1505.6155\n",
      "Training Epoch: 6 [1700/36045]\tLoss: 1524.1462\n",
      "Training Epoch: 6 [1750/36045]\tLoss: 1580.3405\n",
      "Training Epoch: 6 [1800/36045]\tLoss: 1567.9862\n",
      "Training Epoch: 6 [1850/36045]\tLoss: 1627.2489\n",
      "Training Epoch: 6 [1900/36045]\tLoss: 1543.0719\n",
      "Training Epoch: 6 [1950/36045]\tLoss: 1536.7653\n",
      "Training Epoch: 6 [2000/36045]\tLoss: 1383.7926\n",
      "Training Epoch: 6 [2050/36045]\tLoss: 1390.7863\n",
      "Training Epoch: 6 [2100/36045]\tLoss: 1468.1376\n",
      "Training Epoch: 6 [2150/36045]\tLoss: 1432.3474\n",
      "Training Epoch: 6 [2200/36045]\tLoss: 1333.9332\n",
      "Training Epoch: 6 [2250/36045]\tLoss: 1256.0474\n",
      "Training Epoch: 6 [2300/36045]\tLoss: 1306.2676\n",
      "Training Epoch: 6 [2350/36045]\tLoss: 1254.3359\n",
      "Training Epoch: 6 [2400/36045]\tLoss: 1299.9534\n",
      "Training Epoch: 6 [2450/36045]\tLoss: 1568.0132\n",
      "Training Epoch: 6 [2500/36045]\tLoss: 1633.8807\n",
      "Training Epoch: 6 [2550/36045]\tLoss: 1630.1796\n",
      "Training Epoch: 6 [2600/36045]\tLoss: 1612.9054\n",
      "Training Epoch: 6 [2650/36045]\tLoss: 1794.7115\n",
      "Training Epoch: 6 [2700/36045]\tLoss: 1905.4674\n",
      "Training Epoch: 6 [2750/36045]\tLoss: 2006.8479\n",
      "Training Epoch: 6 [2800/36045]\tLoss: 2023.0636\n",
      "Training Epoch: 6 [2850/36045]\tLoss: 1755.5157\n",
      "Training Epoch: 6 [2900/36045]\tLoss: 1760.3582\n",
      "Training Epoch: 6 [2950/36045]\tLoss: 1708.9540\n",
      "Training Epoch: 6 [3000/36045]\tLoss: 1703.6892\n",
      "Training Epoch: 6 [3050/36045]\tLoss: 1754.1243\n",
      "Training Epoch: 6 [3100/36045]\tLoss: 1598.3109\n",
      "Training Epoch: 6 [3150/36045]\tLoss: 1243.3665\n",
      "Training Epoch: 6 [3200/36045]\tLoss: 1288.5402\n",
      "Training Epoch: 6 [3250/36045]\tLoss: 1203.9961\n",
      "Training Epoch: 6 [3300/36045]\tLoss: 1142.4453\n",
      "Training Epoch: 6 [3350/36045]\tLoss: 1206.6768\n",
      "Training Epoch: 6 [3400/36045]\tLoss: 1275.1976\n",
      "Training Epoch: 6 [3450/36045]\tLoss: 1377.5535\n",
      "Training Epoch: 6 [3500/36045]\tLoss: 1340.2561\n",
      "Training Epoch: 6 [3550/36045]\tLoss: 1308.0100\n",
      "Training Epoch: 6 [3600/36045]\tLoss: 1391.9241\n",
      "Training Epoch: 6 [3650/36045]\tLoss: 1590.5972\n",
      "Training Epoch: 6 [3700/36045]\tLoss: 1596.2185\n",
      "Training Epoch: 6 [3750/36045]\tLoss: 1546.8214\n",
      "Training Epoch: 6 [3800/36045]\tLoss: 1520.9541\n",
      "Training Epoch: 6 [3850/36045]\tLoss: 1519.9076\n",
      "Training Epoch: 6 [3900/36045]\tLoss: 1528.2803\n",
      "Training Epoch: 6 [3950/36045]\tLoss: 1459.9377\n",
      "Training Epoch: 6 [4000/36045]\tLoss: 1503.4625\n",
      "Training Epoch: 6 [4050/36045]\tLoss: 1383.1152\n",
      "Training Epoch: 6 [4100/36045]\tLoss: 1344.1605\n",
      "Training Epoch: 6 [4150/36045]\tLoss: 1395.7402\n",
      "Training Epoch: 6 [4200/36045]\tLoss: 1374.9298\n",
      "Training Epoch: 6 [4250/36045]\tLoss: 1408.5583\n",
      "Training Epoch: 6 [4300/36045]\tLoss: 1455.4271\n",
      "Training Epoch: 6 [4350/36045]\tLoss: 1440.3171\n",
      "Training Epoch: 6 [4400/36045]\tLoss: 1372.7623\n",
      "Training Epoch: 6 [4450/36045]\tLoss: 1448.0747\n",
      "Training Epoch: 6 [4500/36045]\tLoss: 1509.6021\n",
      "Training Epoch: 6 [4550/36045]\tLoss: 1540.9347\n",
      "Training Epoch: 6 [4600/36045]\tLoss: 1559.4164\n",
      "Training Epoch: 6 [4650/36045]\tLoss: 1565.6370\n",
      "Training Epoch: 6 [4700/36045]\tLoss: 1461.4095\n",
      "Training Epoch: 6 [4750/36045]\tLoss: 1441.1859\n",
      "Training Epoch: 6 [4800/36045]\tLoss: 1495.4305\n",
      "Training Epoch: 6 [4850/36045]\tLoss: 1469.1201\n",
      "Training Epoch: 6 [4900/36045]\tLoss: 1420.4271\n",
      "Training Epoch: 6 [4950/36045]\tLoss: 1477.4485\n",
      "Training Epoch: 6 [5000/36045]\tLoss: 1567.1659\n",
      "Training Epoch: 6 [5050/36045]\tLoss: 1518.8931\n",
      "Training Epoch: 6 [5100/36045]\tLoss: 1555.4321\n",
      "Training Epoch: 6 [5150/36045]\tLoss: 1505.5341\n",
      "Training Epoch: 6 [5200/36045]\tLoss: 1475.0271\n",
      "Training Epoch: 6 [5250/36045]\tLoss: 1461.4485\n",
      "Training Epoch: 6 [5300/36045]\tLoss: 1470.6884\n",
      "Training Epoch: 6 [5350/36045]\tLoss: 1517.6245\n",
      "Training Epoch: 6 [5400/36045]\tLoss: 1450.4541\n",
      "Training Epoch: 6 [5450/36045]\tLoss: 1345.6610\n",
      "Training Epoch: 6 [5500/36045]\tLoss: 1420.4541\n",
      "Training Epoch: 6 [5550/36045]\tLoss: 1387.6238\n",
      "Training Epoch: 6 [5600/36045]\tLoss: 1548.7499\n",
      "Training Epoch: 6 [5650/36045]\tLoss: 1496.7756\n",
      "Training Epoch: 6 [5700/36045]\tLoss: 1419.7629\n",
      "Training Epoch: 6 [5750/36045]\tLoss: 1411.2072\n",
      "Training Epoch: 6 [5800/36045]\tLoss: 1501.5576\n",
      "Training Epoch: 6 [5850/36045]\tLoss: 1450.1759\n",
      "Training Epoch: 6 [5900/36045]\tLoss: 1651.8901\n",
      "Training Epoch: 6 [5950/36045]\tLoss: 1699.1580\n",
      "Training Epoch: 6 [6000/36045]\tLoss: 1671.6270\n",
      "Training Epoch: 6 [6050/36045]\tLoss: 1608.0936\n",
      "Training Epoch: 6 [6100/36045]\tLoss: 1608.0459\n",
      "Training Epoch: 6 [6150/36045]\tLoss: 1563.6173\n",
      "Training Epoch: 6 [6200/36045]\tLoss: 1550.8767\n",
      "Training Epoch: 6 [6250/36045]\tLoss: 1561.0431\n",
      "Training Epoch: 6 [6300/36045]\tLoss: 1612.8475\n",
      "Training Epoch: 6 [6350/36045]\tLoss: 1674.7412\n",
      "Training Epoch: 6 [6400/36045]\tLoss: 1471.4979\n",
      "Training Epoch: 6 [6450/36045]\tLoss: 1368.4722\n",
      "Training Epoch: 6 [6500/36045]\tLoss: 1403.8516\n",
      "Training Epoch: 6 [6550/36045]\tLoss: 1432.8929\n",
      "Training Epoch: 6 [6600/36045]\tLoss: 1430.0571\n",
      "Training Epoch: 6 [6650/36045]\tLoss: 1586.7482\n",
      "Training Epoch: 6 [6700/36045]\tLoss: 1662.1431\n",
      "Training Epoch: 6 [6750/36045]\tLoss: 1624.5654\n",
      "Training Epoch: 6 [6800/36045]\tLoss: 1616.1670\n",
      "Training Epoch: 6 [6850/36045]\tLoss: 1594.5818\n",
      "Training Epoch: 6 [6900/36045]\tLoss: 1407.0173\n",
      "Training Epoch: 6 [6950/36045]\tLoss: 1326.3135\n",
      "Training Epoch: 6 [7000/36045]\tLoss: 1395.6465\n",
      "Training Epoch: 6 [7050/36045]\tLoss: 1447.6852\n",
      "Training Epoch: 6 [7100/36045]\tLoss: 1434.0779\n",
      "Training Epoch: 6 [7150/36045]\tLoss: 1466.5981\n",
      "Training Epoch: 6 [7200/36045]\tLoss: 1476.8401\n",
      "Training Epoch: 6 [7250/36045]\tLoss: 1474.3135\n",
      "Training Epoch: 6 [7300/36045]\tLoss: 1448.3237\n",
      "Training Epoch: 6 [7350/36045]\tLoss: 1444.4602\n",
      "Training Epoch: 6 [7400/36045]\tLoss: 1356.0851\n",
      "Training Epoch: 6 [7450/36045]\tLoss: 1352.9854\n",
      "Training Epoch: 6 [7500/36045]\tLoss: 1350.9298\n",
      "Training Epoch: 6 [7550/36045]\tLoss: 1290.7756\n",
      "Training Epoch: 6 [7600/36045]\tLoss: 1416.1543\n",
      "Training Epoch: 6 [7650/36045]\tLoss: 1506.8132\n",
      "Training Epoch: 6 [7700/36045]\tLoss: 1444.1755\n",
      "Training Epoch: 6 [7750/36045]\tLoss: 1464.7196\n",
      "Training Epoch: 6 [7800/36045]\tLoss: 1443.1062\n",
      "Training Epoch: 6 [7850/36045]\tLoss: 1357.3420\n",
      "Training Epoch: 6 [7900/36045]\tLoss: 1433.4523\n",
      "Training Epoch: 6 [7950/36045]\tLoss: 1428.7776\n",
      "Training Epoch: 6 [8000/36045]\tLoss: 1447.0375\n",
      "Training Epoch: 6 [8050/36045]\tLoss: 1382.3010\n",
      "Training Epoch: 6 [8100/36045]\tLoss: 1430.2550\n",
      "Training Epoch: 6 [8150/36045]\tLoss: 1610.2926\n",
      "Training Epoch: 6 [8200/36045]\tLoss: 1591.0972\n",
      "Training Epoch: 6 [8250/36045]\tLoss: 1538.9800\n",
      "Training Epoch: 6 [8300/36045]\tLoss: 1649.1366\n",
      "Training Epoch: 6 [8350/36045]\tLoss: 1534.2139\n",
      "Training Epoch: 6 [8400/36045]\tLoss: 1424.1699\n",
      "Training Epoch: 6 [8450/36045]\tLoss: 1357.6200\n",
      "Training Epoch: 6 [8500/36045]\tLoss: 1413.7485\n",
      "Training Epoch: 6 [8550/36045]\tLoss: 1379.0546\n",
      "Training Epoch: 6 [8600/36045]\tLoss: 1378.9193\n",
      "Training Epoch: 6 [8650/36045]\tLoss: 1439.1348\n",
      "Training Epoch: 6 [8700/36045]\tLoss: 1508.5486\n",
      "Training Epoch: 6 [8750/36045]\tLoss: 1479.9034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [8800/36045]\tLoss: 1492.4564\n",
      "Training Epoch: 6 [8850/36045]\tLoss: 1473.4124\n",
      "Training Epoch: 6 [8900/36045]\tLoss: 1339.9124\n",
      "Training Epoch: 6 [8950/36045]\tLoss: 1379.0486\n",
      "Training Epoch: 6 [9000/36045]\tLoss: 1387.8796\n",
      "Training Epoch: 6 [9050/36045]\tLoss: 1379.6530\n",
      "Training Epoch: 6 [9100/36045]\tLoss: 1413.7815\n",
      "Training Epoch: 6 [9150/36045]\tLoss: 1049.5939\n",
      "Training Epoch: 6 [9200/36045]\tLoss: 811.0657\n",
      "Training Epoch: 6 [9250/36045]\tLoss: 877.0308\n",
      "Training Epoch: 6 [9300/36045]\tLoss: 908.3502\n",
      "Training Epoch: 6 [9350/36045]\tLoss: 828.5427\n",
      "Training Epoch: 6 [9400/36045]\tLoss: 1606.3040\n",
      "Training Epoch: 6 [9450/36045]\tLoss: 1684.9547\n",
      "Training Epoch: 6 [9500/36045]\tLoss: 1664.6420\n",
      "Training Epoch: 6 [9550/36045]\tLoss: 1770.5016\n",
      "Training Epoch: 6 [9600/36045]\tLoss: 1300.3623\n",
      "Training Epoch: 6 [9650/36045]\tLoss: 1288.9116\n",
      "Training Epoch: 6 [9700/36045]\tLoss: 1285.7102\n",
      "Training Epoch: 6 [9750/36045]\tLoss: 1277.8553\n",
      "Training Epoch: 6 [9800/36045]\tLoss: 1653.0669\n",
      "Training Epoch: 6 [9850/36045]\tLoss: 1742.9460\n",
      "Training Epoch: 6 [9900/36045]\tLoss: 1784.1375\n",
      "Training Epoch: 6 [9950/36045]\tLoss: 1721.7294\n",
      "Training Epoch: 6 [10000/36045]\tLoss: 1606.5184\n",
      "Training Epoch: 6 [10050/36045]\tLoss: 1382.7064\n",
      "Training Epoch: 6 [10100/36045]\tLoss: 1379.9722\n",
      "Training Epoch: 6 [10150/36045]\tLoss: 1392.1863\n",
      "Training Epoch: 6 [10200/36045]\tLoss: 1392.9485\n",
      "Training Epoch: 6 [10250/36045]\tLoss: 1681.1770\n",
      "Training Epoch: 6 [10300/36045]\tLoss: 1643.6938\n",
      "Training Epoch: 6 [10350/36045]\tLoss: 1716.6256\n",
      "Training Epoch: 6 [10400/36045]\tLoss: 1710.9768\n",
      "Training Epoch: 6 [10450/36045]\tLoss: 1570.3550\n",
      "Training Epoch: 6 [10500/36045]\tLoss: 1325.8882\n",
      "Training Epoch: 6 [10550/36045]\tLoss: 1320.9774\n",
      "Training Epoch: 6 [10600/36045]\tLoss: 1363.0162\n",
      "Training Epoch: 6 [10650/36045]\tLoss: 1375.4761\n",
      "Training Epoch: 6 [10700/36045]\tLoss: 1500.0791\n",
      "Training Epoch: 6 [10750/36045]\tLoss: 1604.8599\n",
      "Training Epoch: 6 [10800/36045]\tLoss: 1504.1158\n",
      "Training Epoch: 6 [10850/36045]\tLoss: 1563.6738\n",
      "Training Epoch: 6 [10900/36045]\tLoss: 1625.4218\n",
      "Training Epoch: 6 [10950/36045]\tLoss: 1250.6157\n",
      "Training Epoch: 6 [11000/36045]\tLoss: 1253.3562\n",
      "Training Epoch: 6 [11050/36045]\tLoss: 1328.5178\n",
      "Training Epoch: 6 [11100/36045]\tLoss: 1344.5889\n",
      "Training Epoch: 6 [11150/36045]\tLoss: 1453.8463\n",
      "Training Epoch: 6 [11200/36045]\tLoss: 1508.6528\n",
      "Training Epoch: 6 [11250/36045]\tLoss: 1530.4447\n",
      "Training Epoch: 6 [11300/36045]\tLoss: 1499.1019\n",
      "Training Epoch: 6 [11350/36045]\tLoss: 1494.4352\n",
      "Training Epoch: 6 [11400/36045]\tLoss: 1424.1024\n",
      "Training Epoch: 6 [11450/36045]\tLoss: 1380.1390\n",
      "Training Epoch: 6 [11500/36045]\tLoss: 1378.7230\n",
      "Training Epoch: 6 [11550/36045]\tLoss: 1412.9521\n",
      "Training Epoch: 6 [11600/36045]\tLoss: 1521.3640\n",
      "Training Epoch: 6 [11650/36045]\tLoss: 1598.9106\n",
      "Training Epoch: 6 [11700/36045]\tLoss: 1596.9873\n",
      "Training Epoch: 6 [11750/36045]\tLoss: 1615.1367\n",
      "Training Epoch: 6 [11800/36045]\tLoss: 1675.9169\n",
      "Training Epoch: 6 [11850/36045]\tLoss: 1695.8759\n",
      "Training Epoch: 6 [11900/36045]\tLoss: 1948.4727\n",
      "Training Epoch: 6 [11950/36045]\tLoss: 1936.2601\n",
      "Training Epoch: 6 [12000/36045]\tLoss: 1974.5336\n",
      "Training Epoch: 6 [12050/36045]\tLoss: 1916.1438\n",
      "Training Epoch: 6 [12100/36045]\tLoss: 1441.2509\n",
      "Training Epoch: 6 [12150/36045]\tLoss: 1226.7186\n",
      "Training Epoch: 6 [12200/36045]\tLoss: 1206.0267\n",
      "Training Epoch: 6 [12250/36045]\tLoss: 1215.9452\n",
      "Training Epoch: 6 [12300/36045]\tLoss: 1442.1565\n",
      "Training Epoch: 6 [12350/36045]\tLoss: 1518.7068\n",
      "Training Epoch: 6 [12400/36045]\tLoss: 1552.0848\n",
      "Training Epoch: 6 [12450/36045]\tLoss: 1530.0616\n",
      "Training Epoch: 6 [12500/36045]\tLoss: 1591.2841\n",
      "Training Epoch: 6 [12550/36045]\tLoss: 1522.2263\n",
      "Training Epoch: 6 [12600/36045]\tLoss: 1434.0797\n",
      "Training Epoch: 6 [12650/36045]\tLoss: 1428.7384\n",
      "Training Epoch: 6 [12700/36045]\tLoss: 1462.4362\n",
      "Training Epoch: 6 [12750/36045]\tLoss: 1469.3992\n",
      "Training Epoch: 6 [12800/36045]\tLoss: 1456.4589\n",
      "Training Epoch: 6 [12850/36045]\tLoss: 1534.1528\n",
      "Training Epoch: 6 [12900/36045]\tLoss: 1481.4324\n",
      "Training Epoch: 6 [12950/36045]\tLoss: 1463.7461\n",
      "Training Epoch: 6 [13000/36045]\tLoss: 1494.8770\n",
      "Training Epoch: 6 [13050/36045]\tLoss: 1395.7019\n",
      "Training Epoch: 6 [13100/36045]\tLoss: 1456.5159\n",
      "Training Epoch: 6 [13150/36045]\tLoss: 1457.0270\n",
      "Training Epoch: 6 [13200/36045]\tLoss: 1379.5284\n",
      "Training Epoch: 6 [13250/36045]\tLoss: 1451.2733\n",
      "Training Epoch: 6 [13300/36045]\tLoss: 1506.6772\n",
      "Training Epoch: 6 [13350/36045]\tLoss: 1473.8259\n",
      "Training Epoch: 6 [13400/36045]\tLoss: 1477.8690\n",
      "Training Epoch: 6 [13450/36045]\tLoss: 1452.2853\n",
      "Training Epoch: 6 [13500/36045]\tLoss: 1516.5050\n",
      "Training Epoch: 6 [13550/36045]\tLoss: 1653.6138\n",
      "Training Epoch: 6 [13600/36045]\tLoss: 1679.2899\n",
      "Training Epoch: 6 [13650/36045]\tLoss: 1772.9476\n",
      "Training Epoch: 6 [13700/36045]\tLoss: 1613.3412\n",
      "Training Epoch: 6 [13750/36045]\tLoss: 1507.8688\n",
      "Training Epoch: 6 [13800/36045]\tLoss: 1491.3442\n",
      "Training Epoch: 6 [13850/36045]\tLoss: 1474.6173\n",
      "Training Epoch: 6 [13900/36045]\tLoss: 1480.1625\n",
      "Training Epoch: 6 [13950/36045]\tLoss: 1476.7991\n",
      "Training Epoch: 6 [14000/36045]\tLoss: 1523.9154\n",
      "Training Epoch: 6 [14050/36045]\tLoss: 1467.8364\n",
      "Training Epoch: 6 [14100/36045]\tLoss: 1470.9567\n",
      "Training Epoch: 6 [14150/36045]\tLoss: 1439.4285\n",
      "Training Epoch: 6 [14200/36045]\tLoss: 1538.4537\n",
      "Training Epoch: 6 [14250/36045]\tLoss: 1687.7305\n",
      "Training Epoch: 6 [14300/36045]\tLoss: 1684.3409\n",
      "Training Epoch: 6 [14350/36045]\tLoss: 1611.4424\n",
      "Training Epoch: 6 [14400/36045]\tLoss: 1596.0093\n",
      "Training Epoch: 6 [14450/36045]\tLoss: 1654.8114\n",
      "Training Epoch: 6 [14500/36045]\tLoss: 1581.6818\n",
      "Training Epoch: 6 [14550/36045]\tLoss: 1665.0247\n",
      "Training Epoch: 6 [14600/36045]\tLoss: 1620.4907\n",
      "Training Epoch: 6 [14650/36045]\tLoss: 1615.2354\n",
      "Training Epoch: 6 [14700/36045]\tLoss: 1511.1608\n",
      "Training Epoch: 6 [14750/36045]\tLoss: 1307.9172\n",
      "Training Epoch: 6 [14800/36045]\tLoss: 1286.9963\n",
      "Training Epoch: 6 [14850/36045]\tLoss: 1296.9945\n",
      "Training Epoch: 6 [14900/36045]\tLoss: 1290.9021\n",
      "Training Epoch: 6 [14950/36045]\tLoss: 1316.7465\n",
      "Training Epoch: 6 [15000/36045]\tLoss: 1362.9685\n",
      "Training Epoch: 6 [15050/36045]\tLoss: 1382.9167\n",
      "Training Epoch: 6 [15100/36045]\tLoss: 1352.5029\n",
      "Training Epoch: 6 [15150/36045]\tLoss: 1310.4136\n",
      "Training Epoch: 6 [15200/36045]\tLoss: 1198.1067\n",
      "Training Epoch: 6 [15250/36045]\tLoss: 1244.2017\n",
      "Training Epoch: 6 [15300/36045]\tLoss: 1214.6458\n",
      "Training Epoch: 6 [15350/36045]\tLoss: 1235.6620\n",
      "Training Epoch: 6 [15400/36045]\tLoss: 1291.0480\n",
      "Training Epoch: 6 [15450/36045]\tLoss: 1291.6249\n",
      "Training Epoch: 6 [15500/36045]\tLoss: 1320.5090\n",
      "Training Epoch: 6 [15550/36045]\tLoss: 1283.4403\n",
      "Training Epoch: 6 [15600/36045]\tLoss: 1364.4762\n",
      "Training Epoch: 6 [15650/36045]\tLoss: 1404.2649\n",
      "Training Epoch: 6 [15700/36045]\tLoss: 1375.2257\n",
      "Training Epoch: 6 [15750/36045]\tLoss: 1358.0306\n",
      "Training Epoch: 6 [15800/36045]\tLoss: 1232.4435\n",
      "Training Epoch: 6 [15850/36045]\tLoss: 1229.8319\n",
      "Training Epoch: 6 [15900/36045]\tLoss: 1227.8186\n",
      "Training Epoch: 6 [15950/36045]\tLoss: 1263.8186\n",
      "Training Epoch: 6 [16000/36045]\tLoss: 1256.7461\n",
      "Training Epoch: 6 [16050/36045]\tLoss: 1206.5663\n",
      "Training Epoch: 6 [16100/36045]\tLoss: 1133.6440\n",
      "Training Epoch: 6 [16150/36045]\tLoss: 1105.5566\n",
      "Training Epoch: 6 [16200/36045]\tLoss: 1302.4524\n",
      "Training Epoch: 6 [16250/36045]\tLoss: 1350.8719\n",
      "Training Epoch: 6 [16300/36045]\tLoss: 1444.6482\n",
      "Training Epoch: 6 [16350/36045]\tLoss: 1461.3835\n",
      "Training Epoch: 6 [16400/36045]\tLoss: 1427.6611\n",
      "Training Epoch: 6 [16450/36045]\tLoss: 1410.2990\n",
      "Training Epoch: 6 [16500/36045]\tLoss: 1418.6561\n",
      "Training Epoch: 6 [16550/36045]\tLoss: 1356.2943\n",
      "Training Epoch: 6 [16600/36045]\tLoss: 1421.9692\n",
      "Training Epoch: 6 [16650/36045]\tLoss: 1456.9867\n",
      "Training Epoch: 6 [16700/36045]\tLoss: 1423.4696\n",
      "Training Epoch: 6 [16750/36045]\tLoss: 1405.1823\n",
      "Training Epoch: 6 [16800/36045]\tLoss: 1437.2725\n",
      "Training Epoch: 6 [16850/36045]\tLoss: 1375.5614\n",
      "Training Epoch: 6 [16900/36045]\tLoss: 1379.9370\n",
      "Training Epoch: 6 [16950/36045]\tLoss: 1390.6763\n",
      "Training Epoch: 6 [17000/36045]\tLoss: 1366.2930\n",
      "Training Epoch: 6 [17050/36045]\tLoss: 1448.7881\n",
      "Training Epoch: 6 [17100/36045]\tLoss: 1459.7195\n",
      "Training Epoch: 6 [17150/36045]\tLoss: 1299.6007\n",
      "Training Epoch: 6 [17200/36045]\tLoss: 1251.6185\n",
      "Training Epoch: 6 [17250/36045]\tLoss: 1301.8494\n",
      "Training Epoch: 6 [17300/36045]\tLoss: 1378.9896\n",
      "Training Epoch: 6 [17350/36045]\tLoss: 1288.4991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [17400/36045]\tLoss: 1289.2305\n",
      "Training Epoch: 6 [17450/36045]\tLoss: 1339.0652\n",
      "Training Epoch: 6 [17500/36045]\tLoss: 1318.4220\n",
      "Training Epoch: 6 [17550/36045]\tLoss: 1341.5829\n",
      "Training Epoch: 6 [17600/36045]\tLoss: 1319.9219\n",
      "Training Epoch: 6 [17650/36045]\tLoss: 1362.6580\n",
      "Training Epoch: 6 [17700/36045]\tLoss: 1320.6261\n",
      "Training Epoch: 6 [17750/36045]\tLoss: 1350.5770\n",
      "Training Epoch: 6 [17800/36045]\tLoss: 1344.6432\n",
      "Training Epoch: 6 [17850/36045]\tLoss: 1272.5966\n",
      "Training Epoch: 6 [17900/36045]\tLoss: 1314.7451\n",
      "Training Epoch: 6 [17950/36045]\tLoss: 1330.2661\n",
      "Training Epoch: 6 [18000/36045]\tLoss: 1308.9044\n",
      "Training Epoch: 6 [18050/36045]\tLoss: 1479.8643\n",
      "Training Epoch: 6 [18100/36045]\tLoss: 1503.1459\n",
      "Training Epoch: 6 [18150/36045]\tLoss: 1502.9644\n",
      "Training Epoch: 6 [18200/36045]\tLoss: 1493.9789\n",
      "Training Epoch: 6 [18250/36045]\tLoss: 1522.5397\n",
      "Training Epoch: 6 [18300/36045]\tLoss: 1400.3584\n",
      "Training Epoch: 6 [18350/36045]\tLoss: 1462.3326\n",
      "Training Epoch: 6 [18400/36045]\tLoss: 1451.7439\n",
      "Training Epoch: 6 [18450/36045]\tLoss: 1418.7197\n",
      "Training Epoch: 6 [18500/36045]\tLoss: 1402.6282\n",
      "Training Epoch: 6 [18550/36045]\tLoss: 1369.4886\n",
      "Training Epoch: 6 [18600/36045]\tLoss: 1350.6569\n",
      "Training Epoch: 6 [18650/36045]\tLoss: 1414.3807\n",
      "Training Epoch: 6 [18700/36045]\tLoss: 1487.9487\n",
      "Training Epoch: 6 [18750/36045]\tLoss: 1469.4205\n",
      "Training Epoch: 6 [18800/36045]\tLoss: 1524.8679\n",
      "Training Epoch: 6 [18850/36045]\tLoss: 1445.9910\n",
      "Training Epoch: 6 [18900/36045]\tLoss: 1539.1907\n",
      "Training Epoch: 6 [18950/36045]\tLoss: 1458.8608\n",
      "Training Epoch: 6 [19000/36045]\tLoss: 1308.3640\n",
      "Training Epoch: 6 [19050/36045]\tLoss: 1246.4469\n",
      "Training Epoch: 6 [19100/36045]\tLoss: 1278.1031\n",
      "Training Epoch: 6 [19150/36045]\tLoss: 1250.6984\n",
      "Training Epoch: 6 [19200/36045]\tLoss: 1299.6293\n",
      "Training Epoch: 6 [19250/36045]\tLoss: 1306.9351\n",
      "Training Epoch: 6 [19300/36045]\tLoss: 1354.7571\n",
      "Training Epoch: 6 [19350/36045]\tLoss: 1315.3735\n",
      "Training Epoch: 6 [19400/36045]\tLoss: 1352.8793\n",
      "Training Epoch: 6 [19450/36045]\tLoss: 1330.4629\n",
      "Training Epoch: 6 [19500/36045]\tLoss: 1335.8318\n",
      "Training Epoch: 6 [19550/36045]\tLoss: 1345.4840\n",
      "Training Epoch: 6 [19600/36045]\tLoss: 1397.4865\n",
      "Training Epoch: 6 [19650/36045]\tLoss: 1732.0243\n",
      "Training Epoch: 6 [19700/36045]\tLoss: 1672.5889\n",
      "Training Epoch: 6 [19750/36045]\tLoss: 1669.7251\n",
      "Training Epoch: 6 [19800/36045]\tLoss: 1657.3248\n",
      "Training Epoch: 6 [19850/36045]\tLoss: 1201.4109\n",
      "Training Epoch: 6 [19900/36045]\tLoss: 1161.7698\n",
      "Training Epoch: 6 [19950/36045]\tLoss: 1169.1876\n",
      "Training Epoch: 6 [20000/36045]\tLoss: 1164.4872\n",
      "Training Epoch: 6 [20050/36045]\tLoss: 1304.5034\n",
      "Training Epoch: 6 [20100/36045]\tLoss: 1300.6254\n",
      "Training Epoch: 6 [20150/36045]\tLoss: 1316.2164\n",
      "Training Epoch: 6 [20200/36045]\tLoss: 1304.4570\n",
      "Training Epoch: 6 [20250/36045]\tLoss: 1381.9822\n",
      "Training Epoch: 6 [20300/36045]\tLoss: 1423.3955\n",
      "Training Epoch: 6 [20350/36045]\tLoss: 1469.7474\n",
      "Training Epoch: 6 [20400/36045]\tLoss: 1491.4358\n",
      "Training Epoch: 6 [20450/36045]\tLoss: 1462.7784\n",
      "Training Epoch: 6 [20500/36045]\tLoss: 1417.0416\n",
      "Training Epoch: 6 [20550/36045]\tLoss: 1293.4187\n",
      "Training Epoch: 6 [20600/36045]\tLoss: 1321.2776\n",
      "Training Epoch: 6 [20650/36045]\tLoss: 1303.4042\n",
      "Training Epoch: 6 [20700/36045]\tLoss: 1287.6731\n",
      "Training Epoch: 6 [20750/36045]\tLoss: 1357.8976\n",
      "Training Epoch: 6 [20800/36045]\tLoss: 1475.1450\n",
      "Training Epoch: 6 [20850/36045]\tLoss: 1462.4070\n",
      "Training Epoch: 6 [20900/36045]\tLoss: 1539.3441\n",
      "Training Epoch: 6 [20950/36045]\tLoss: 1465.9457\n",
      "Training Epoch: 6 [21000/36045]\tLoss: 1374.3656\n",
      "Training Epoch: 6 [21050/36045]\tLoss: 1177.9636\n",
      "Training Epoch: 6 [21100/36045]\tLoss: 1178.5023\n",
      "Training Epoch: 6 [21150/36045]\tLoss: 1253.7454\n",
      "Training Epoch: 6 [21200/36045]\tLoss: 1262.2885\n",
      "Training Epoch: 6 [21250/36045]\tLoss: 1197.2947\n",
      "Training Epoch: 6 [21300/36045]\tLoss: 1429.6575\n",
      "Training Epoch: 6 [21350/36045]\tLoss: 1414.4116\n",
      "Training Epoch: 6 [21400/36045]\tLoss: 1428.9259\n",
      "Training Epoch: 6 [21450/36045]\tLoss: 1450.8723\n",
      "Training Epoch: 6 [21500/36045]\tLoss: 1456.6385\n",
      "Training Epoch: 6 [21550/36045]\tLoss: 1562.3121\n",
      "Training Epoch: 6 [21600/36045]\tLoss: 1562.6245\n",
      "Training Epoch: 6 [21650/36045]\tLoss: 1582.2396\n",
      "Training Epoch: 6 [21700/36045]\tLoss: 1577.7412\n",
      "Training Epoch: 6 [21750/36045]\tLoss: 1527.0901\n",
      "Training Epoch: 6 [21800/36045]\tLoss: 1175.4718\n",
      "Training Epoch: 6 [21850/36045]\tLoss: 1148.6885\n",
      "Training Epoch: 6 [21900/36045]\tLoss: 1172.0015\n",
      "Training Epoch: 6 [21950/36045]\tLoss: 1160.1328\n",
      "Training Epoch: 6 [22000/36045]\tLoss: 1174.6672\n",
      "Training Epoch: 6 [22050/36045]\tLoss: 1273.1960\n",
      "Training Epoch: 6 [22100/36045]\tLoss: 1250.5756\n",
      "Training Epoch: 6 [22150/36045]\tLoss: 1222.0269\n",
      "Training Epoch: 6 [22200/36045]\tLoss: 1248.0582\n",
      "Training Epoch: 6 [22250/36045]\tLoss: 1266.2566\n",
      "Training Epoch: 6 [22300/36045]\tLoss: 1341.7533\n",
      "Training Epoch: 6 [22350/36045]\tLoss: 1366.8239\n",
      "Training Epoch: 6 [22400/36045]\tLoss: 1401.3203\n",
      "Training Epoch: 6 [22450/36045]\tLoss: 1379.0795\n",
      "Training Epoch: 6 [22500/36045]\tLoss: 1337.0496\n",
      "Training Epoch: 6 [22550/36045]\tLoss: 1418.2052\n",
      "Training Epoch: 6 [22600/36045]\tLoss: 1561.1589\n",
      "Training Epoch: 6 [22650/36045]\tLoss: 1624.9629\n",
      "Training Epoch: 6 [22700/36045]\tLoss: 1660.9849\n",
      "Training Epoch: 6 [22750/36045]\tLoss: 1692.6050\n",
      "Training Epoch: 6 [22800/36045]\tLoss: 1767.7631\n",
      "Training Epoch: 6 [22850/36045]\tLoss: 1476.7245\n",
      "Training Epoch: 6 [22900/36045]\tLoss: 1475.5205\n",
      "Training Epoch: 6 [22950/36045]\tLoss: 1442.8782\n",
      "Training Epoch: 6 [23000/36045]\tLoss: 1457.1923\n",
      "Training Epoch: 6 [23050/36045]\tLoss: 1315.2335\n",
      "Training Epoch: 6 [23100/36045]\tLoss: 1337.8746\n",
      "Training Epoch: 6 [23150/36045]\tLoss: 1316.2301\n",
      "Training Epoch: 6 [23200/36045]\tLoss: 1243.5232\n",
      "Training Epoch: 6 [23250/36045]\tLoss: 1244.7856\n",
      "Training Epoch: 6 [23300/36045]\tLoss: 1231.0580\n",
      "Training Epoch: 6 [23350/36045]\tLoss: 1256.2360\n",
      "Training Epoch: 6 [23400/36045]\tLoss: 1361.6156\n",
      "Training Epoch: 6 [23450/36045]\tLoss: 1344.6866\n",
      "Training Epoch: 6 [23500/36045]\tLoss: 1289.5154\n",
      "Training Epoch: 6 [23550/36045]\tLoss: 1397.0287\n",
      "Training Epoch: 6 [23600/36045]\tLoss: 1557.0521\n",
      "Training Epoch: 6 [23650/36045]\tLoss: 1603.2307\n",
      "Training Epoch: 6 [23700/36045]\tLoss: 1610.4204\n",
      "Training Epoch: 6 [23750/36045]\tLoss: 1567.3997\n",
      "Training Epoch: 6 [23800/36045]\tLoss: 1275.2487\n",
      "Training Epoch: 6 [23850/36045]\tLoss: 1320.5791\n",
      "Training Epoch: 6 [23900/36045]\tLoss: 1304.9884\n",
      "Training Epoch: 6 [23950/36045]\tLoss: 1285.8870\n",
      "Training Epoch: 6 [24000/36045]\tLoss: 1248.8533\n",
      "Training Epoch: 6 [24050/36045]\tLoss: 1138.3871\n",
      "Training Epoch: 6 [24100/36045]\tLoss: 1190.0431\n",
      "Training Epoch: 6 [24150/36045]\tLoss: 1199.7931\n",
      "Training Epoch: 6 [24200/36045]\tLoss: 1169.7543\n",
      "Training Epoch: 6 [24250/36045]\tLoss: 1133.0494\n",
      "Training Epoch: 6 [24300/36045]\tLoss: 1234.5995\n",
      "Training Epoch: 6 [24350/36045]\tLoss: 1274.1499\n",
      "Training Epoch: 6 [24400/36045]\tLoss: 1301.8546\n",
      "Training Epoch: 6 [24450/36045]\tLoss: 1260.2145\n",
      "Training Epoch: 6 [24500/36045]\tLoss: 1330.7416\n",
      "Training Epoch: 6 [24550/36045]\tLoss: 1427.4476\n",
      "Training Epoch: 6 [24600/36045]\tLoss: 1417.3215\n",
      "Training Epoch: 6 [24650/36045]\tLoss: 1379.3938\n",
      "Training Epoch: 6 [24700/36045]\tLoss: 1400.1842\n",
      "Training Epoch: 6 [24750/36045]\tLoss: 1284.9412\n",
      "Training Epoch: 6 [24800/36045]\tLoss: 1146.6921\n",
      "Training Epoch: 6 [24850/36045]\tLoss: 1168.6494\n",
      "Training Epoch: 6 [24900/36045]\tLoss: 1170.7537\n",
      "Training Epoch: 6 [24950/36045]\tLoss: 1170.3467\n",
      "Training Epoch: 6 [25000/36045]\tLoss: 1126.3235\n",
      "Training Epoch: 6 [25050/36045]\tLoss: 1060.5605\n",
      "Training Epoch: 6 [25100/36045]\tLoss: 954.5602\n",
      "Training Epoch: 6 [25150/36045]\tLoss: 889.0278\n",
      "Training Epoch: 6 [25200/36045]\tLoss: 886.0809\n",
      "Training Epoch: 6 [25250/36045]\tLoss: 944.0844\n",
      "Training Epoch: 6 [25300/36045]\tLoss: 1230.3047\n",
      "Training Epoch: 6 [25350/36045]\tLoss: 1225.4309\n",
      "Training Epoch: 6 [25400/36045]\tLoss: 1142.3011\n",
      "Training Epoch: 6 [25450/36045]\tLoss: 1148.1761\n",
      "Training Epoch: 6 [25500/36045]\tLoss: 1248.1292\n",
      "Training Epoch: 6 [25550/36045]\tLoss: 1427.0399\n",
      "Training Epoch: 6 [25600/36045]\tLoss: 1428.1871\n",
      "Training Epoch: 6 [25650/36045]\tLoss: 1383.6006\n",
      "Training Epoch: 6 [25700/36045]\tLoss: 1432.3413\n",
      "Training Epoch: 6 [25750/36045]\tLoss: 1361.0428\n",
      "Training Epoch: 6 [25800/36045]\tLoss: 834.3936\n",
      "Training Epoch: 6 [25850/36045]\tLoss: 847.2554\n",
      "Training Epoch: 6 [25900/36045]\tLoss: 807.5981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [25950/36045]\tLoss: 824.4852\n",
      "Training Epoch: 6 [26000/36045]\tLoss: 1023.4289\n",
      "Training Epoch: 6 [26050/36045]\tLoss: 1393.2947\n",
      "Training Epoch: 6 [26100/36045]\tLoss: 1440.9360\n",
      "Training Epoch: 6 [26150/36045]\tLoss: 1425.2083\n",
      "Training Epoch: 6 [26200/36045]\tLoss: 1389.9325\n",
      "Training Epoch: 6 [26250/36045]\tLoss: 1425.9863\n",
      "Training Epoch: 6 [26300/36045]\tLoss: 1219.9059\n",
      "Training Epoch: 6 [26350/36045]\tLoss: 1224.8982\n",
      "Training Epoch: 6 [26400/36045]\tLoss: 1214.3485\n",
      "Training Epoch: 6 [26450/36045]\tLoss: 1128.3556\n",
      "Training Epoch: 6 [26500/36045]\tLoss: 1362.2782\n",
      "Training Epoch: 6 [26550/36045]\tLoss: 1398.5765\n",
      "Training Epoch: 6 [26600/36045]\tLoss: 1387.5872\n",
      "Training Epoch: 6 [26650/36045]\tLoss: 1417.0698\n",
      "Training Epoch: 6 [26700/36045]\tLoss: 1387.7809\n",
      "Training Epoch: 6 [26750/36045]\tLoss: 1301.2983\n",
      "Training Epoch: 6 [26800/36045]\tLoss: 952.4953\n",
      "Training Epoch: 6 [26850/36045]\tLoss: 804.6053\n",
      "Training Epoch: 6 [26900/36045]\tLoss: 815.4100\n",
      "Training Epoch: 6 [26950/36045]\tLoss: 887.0518\n",
      "Training Epoch: 6 [27000/36045]\tLoss: 1369.4745\n",
      "Training Epoch: 6 [27050/36045]\tLoss: 1448.5845\n",
      "Training Epoch: 6 [27100/36045]\tLoss: 1404.3971\n",
      "Training Epoch: 6 [27150/36045]\tLoss: 1463.6091\n",
      "Training Epoch: 6 [27200/36045]\tLoss: 1123.9733\n",
      "Training Epoch: 6 [27250/36045]\tLoss: 1115.5435\n",
      "Training Epoch: 6 [27300/36045]\tLoss: 1074.2677\n",
      "Training Epoch: 6 [27350/36045]\tLoss: 1094.8342\n",
      "Training Epoch: 6 [27400/36045]\tLoss: 1082.6129\n",
      "Training Epoch: 6 [27450/36045]\tLoss: 1347.8551\n",
      "Training Epoch: 6 [27500/36045]\tLoss: 1436.6106\n",
      "Training Epoch: 6 [27550/36045]\tLoss: 1425.8116\n",
      "Training Epoch: 6 [27600/36045]\tLoss: 1428.5044\n",
      "Training Epoch: 6 [27650/36045]\tLoss: 1436.4862\n",
      "Training Epoch: 6 [27700/36045]\tLoss: 1473.0728\n",
      "Training Epoch: 6 [27750/36045]\tLoss: 1486.2970\n",
      "Training Epoch: 6 [27800/36045]\tLoss: 1467.9352\n",
      "Training Epoch: 6 [27850/36045]\tLoss: 1431.1442\n",
      "Training Epoch: 6 [27900/36045]\tLoss: 1261.7188\n",
      "Training Epoch: 6 [27950/36045]\tLoss: 1039.8781\n",
      "Training Epoch: 6 [28000/36045]\tLoss: 1001.8109\n",
      "Training Epoch: 6 [28050/36045]\tLoss: 1037.1785\n",
      "Training Epoch: 6 [28100/36045]\tLoss: 1018.7302\n",
      "Training Epoch: 6 [28150/36045]\tLoss: 1118.1624\n",
      "Training Epoch: 6 [28200/36045]\tLoss: 1132.8502\n",
      "Training Epoch: 6 [28250/36045]\tLoss: 1130.2212\n",
      "Training Epoch: 6 [28300/36045]\tLoss: 1077.6516\n",
      "Training Epoch: 6 [28350/36045]\tLoss: 1080.1576\n",
      "Training Epoch: 6 [28400/36045]\tLoss: 1489.0797\n",
      "Training Epoch: 6 [28450/36045]\tLoss: 1326.5687\n",
      "Training Epoch: 6 [28500/36045]\tLoss: 1132.6604\n",
      "Training Epoch: 6 [28550/36045]\tLoss: 1033.4071\n",
      "Training Epoch: 6 [28600/36045]\tLoss: 1192.6864\n",
      "Training Epoch: 6 [28650/36045]\tLoss: 1457.8273\n",
      "Training Epoch: 6 [28700/36045]\tLoss: 1471.4305\n",
      "Training Epoch: 6 [28750/36045]\tLoss: 1448.7412\n",
      "Training Epoch: 6 [28800/36045]\tLoss: 1458.9209\n",
      "Training Epoch: 6 [28850/36045]\tLoss: 1242.9972\n",
      "Training Epoch: 6 [28900/36045]\tLoss: 953.2203\n",
      "Training Epoch: 6 [28950/36045]\tLoss: 945.1328\n",
      "Training Epoch: 6 [29000/36045]\tLoss: 955.3345\n",
      "Training Epoch: 6 [29050/36045]\tLoss: 969.6844\n",
      "Training Epoch: 6 [29100/36045]\tLoss: 999.9536\n",
      "Training Epoch: 6 [29150/36045]\tLoss: 974.2899\n",
      "Training Epoch: 6 [29200/36045]\tLoss: 955.9246\n",
      "Training Epoch: 6 [29250/36045]\tLoss: 926.5215\n",
      "Training Epoch: 6 [29300/36045]\tLoss: 1090.6205\n",
      "Training Epoch: 6 [29350/36045]\tLoss: 1327.7570\n",
      "Training Epoch: 6 [29400/36045]\tLoss: 1367.4857\n",
      "Training Epoch: 6 [29450/36045]\tLoss: 1427.9290\n",
      "Training Epoch: 6 [29500/36045]\tLoss: 1446.6516\n",
      "Training Epoch: 6 [29550/36045]\tLoss: 1372.8501\n",
      "Training Epoch: 6 [29600/36045]\tLoss: 1203.6919\n",
      "Training Epoch: 6 [29650/36045]\tLoss: 1170.0850\n",
      "Training Epoch: 6 [29700/36045]\tLoss: 1030.5222\n",
      "Training Epoch: 6 [29750/36045]\tLoss: 1047.3027\n",
      "Training Epoch: 6 [29800/36045]\tLoss: 1112.1077\n",
      "Training Epoch: 6 [29850/36045]\tLoss: 1173.3604\n",
      "Training Epoch: 6 [29900/36045]\tLoss: 1166.6085\n",
      "Training Epoch: 6 [29950/36045]\tLoss: 1179.5717\n",
      "Training Epoch: 6 [30000/36045]\tLoss: 1169.7206\n",
      "Training Epoch: 6 [30050/36045]\tLoss: 1174.4740\n",
      "Training Epoch: 6 [30100/36045]\tLoss: 1479.2369\n",
      "Training Epoch: 6 [30150/36045]\tLoss: 1486.9952\n",
      "Training Epoch: 6 [30200/36045]\tLoss: 1411.8362\n",
      "Training Epoch: 6 [30250/36045]\tLoss: 1474.9590\n",
      "Training Epoch: 6 [30300/36045]\tLoss: 1468.0676\n",
      "Training Epoch: 6 [30350/36045]\tLoss: 1193.8467\n",
      "Training Epoch: 6 [30400/36045]\tLoss: 1178.6212\n",
      "Training Epoch: 6 [30450/36045]\tLoss: 1175.5487\n",
      "Training Epoch: 6 [30500/36045]\tLoss: 1098.9695\n",
      "Training Epoch: 6 [30550/36045]\tLoss: 1029.3943\n",
      "Training Epoch: 6 [30600/36045]\tLoss: 974.5424\n",
      "Training Epoch: 6 [30650/36045]\tLoss: 965.2003\n",
      "Training Epoch: 6 [30700/36045]\tLoss: 987.5355\n",
      "Training Epoch: 6 [30750/36045]\tLoss: 964.2128\n",
      "Training Epoch: 6 [30800/36045]\tLoss: 1004.3947\n",
      "Training Epoch: 6 [30850/36045]\tLoss: 999.2758\n",
      "Training Epoch: 6 [30900/36045]\tLoss: 1031.1737\n",
      "Training Epoch: 6 [30950/36045]\tLoss: 1086.9861\n",
      "Training Epoch: 6 [31000/36045]\tLoss: 1070.0438\n",
      "Training Epoch: 6 [31050/36045]\tLoss: 899.2560\n",
      "Training Epoch: 6 [31100/36045]\tLoss: 882.0994\n",
      "Training Epoch: 6 [31150/36045]\tLoss: 885.1624\n",
      "Training Epoch: 6 [31200/36045]\tLoss: 1117.3469\n",
      "Training Epoch: 6 [31250/36045]\tLoss: 1441.5127\n",
      "Training Epoch: 6 [31300/36045]\tLoss: 1381.5774\n",
      "Training Epoch: 6 [31350/36045]\tLoss: 1402.8180\n",
      "Training Epoch: 6 [31400/36045]\tLoss: 1390.4683\n",
      "Training Epoch: 6 [31450/36045]\tLoss: 1359.8267\n",
      "Training Epoch: 6 [31500/36045]\tLoss: 1359.4261\n",
      "Training Epoch: 6 [31550/36045]\tLoss: 1376.8973\n",
      "Training Epoch: 6 [31600/36045]\tLoss: 1292.4316\n",
      "Training Epoch: 6 [31650/36045]\tLoss: 1383.2737\n",
      "Training Epoch: 6 [31700/36045]\tLoss: 1052.8562\n",
      "Training Epoch: 6 [31750/36045]\tLoss: 882.9902\n",
      "Training Epoch: 6 [31800/36045]\tLoss: 833.6523\n",
      "Training Epoch: 6 [31850/36045]\tLoss: 865.0135\n",
      "Training Epoch: 6 [31900/36045]\tLoss: 1278.2377\n",
      "Training Epoch: 6 [31950/36045]\tLoss: 1573.3297\n",
      "Training Epoch: 6 [32000/36045]\tLoss: 1738.1272\n",
      "Training Epoch: 6 [32050/36045]\tLoss: 1673.4585\n",
      "Training Epoch: 6 [32100/36045]\tLoss: 1647.0704\n",
      "Training Epoch: 6 [32150/36045]\tLoss: 1388.5022\n",
      "Training Epoch: 6 [32200/36045]\tLoss: 1405.1233\n",
      "Training Epoch: 6 [32250/36045]\tLoss: 1421.7568\n",
      "Training Epoch: 6 [32300/36045]\tLoss: 1398.0098\n",
      "Training Epoch: 6 [32350/36045]\tLoss: 1367.4929\n",
      "Training Epoch: 6 [32400/36045]\tLoss: 1296.3093\n",
      "Training Epoch: 6 [32450/36045]\tLoss: 1102.2407\n",
      "Training Epoch: 6 [32500/36045]\tLoss: 1063.3789\n",
      "Training Epoch: 6 [32550/36045]\tLoss: 1073.7333\n",
      "Training Epoch: 6 [32600/36045]\tLoss: 1062.8529\n",
      "Training Epoch: 6 [32650/36045]\tLoss: 1300.7283\n",
      "Training Epoch: 6 [32700/36045]\tLoss: 1400.8186\n",
      "Training Epoch: 6 [32750/36045]\tLoss: 1349.2314\n",
      "Training Epoch: 6 [32800/36045]\tLoss: 1387.3004\n",
      "Training Epoch: 6 [32850/36045]\tLoss: 1286.3326\n",
      "Training Epoch: 6 [32900/36045]\tLoss: 1030.9539\n",
      "Training Epoch: 6 [32950/36045]\tLoss: 1079.8809\n",
      "Training Epoch: 6 [33000/36045]\tLoss: 1097.7860\n",
      "Training Epoch: 6 [33050/36045]\tLoss: 1027.4611\n",
      "Training Epoch: 6 [33100/36045]\tLoss: 1172.1248\n",
      "Training Epoch: 6 [33150/36045]\tLoss: 1534.0812\n",
      "Training Epoch: 6 [33200/36045]\tLoss: 1499.9991\n",
      "Training Epoch: 6 [33250/36045]\tLoss: 1536.5496\n",
      "Training Epoch: 6 [33300/36045]\tLoss: 1627.9117\n",
      "Training Epoch: 6 [33350/36045]\tLoss: 1284.1766\n",
      "Training Epoch: 6 [33400/36045]\tLoss: 994.7395\n",
      "Training Epoch: 6 [33450/36045]\tLoss: 987.2319\n",
      "Training Epoch: 6 [33500/36045]\tLoss: 1011.7540\n",
      "Training Epoch: 6 [33550/36045]\tLoss: 1044.0640\n",
      "Training Epoch: 6 [33600/36045]\tLoss: 1053.6007\n",
      "Training Epoch: 6 [33650/36045]\tLoss: 1333.6952\n",
      "Training Epoch: 6 [33700/36045]\tLoss: 1287.5286\n",
      "Training Epoch: 6 [33750/36045]\tLoss: 1330.9164\n",
      "Training Epoch: 6 [33800/36045]\tLoss: 1321.6130\n",
      "Training Epoch: 6 [33850/36045]\tLoss: 1322.2524\n",
      "Training Epoch: 6 [33900/36045]\tLoss: 1356.1627\n",
      "Training Epoch: 6 [33950/36045]\tLoss: 1375.5364\n",
      "Training Epoch: 6 [34000/36045]\tLoss: 1360.1771\n",
      "Training Epoch: 6 [34050/36045]\tLoss: 1381.8674\n",
      "Training Epoch: 6 [34100/36045]\tLoss: 1317.5403\n",
      "Training Epoch: 6 [34150/36045]\tLoss: 1236.4537\n",
      "Training Epoch: 6 [34200/36045]\tLoss: 1177.5389\n",
      "Training Epoch: 6 [34250/36045]\tLoss: 1189.2173\n",
      "Training Epoch: 6 [34300/36045]\tLoss: 1034.5032\n",
      "Training Epoch: 6 [34350/36045]\tLoss: 1073.2476\n",
      "Training Epoch: 6 [34400/36045]\tLoss: 1042.5688\n",
      "Training Epoch: 6 [34450/36045]\tLoss: 975.7056\n",
      "Training Epoch: 6 [34500/36045]\tLoss: 1046.0541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [34550/36045]\tLoss: 1036.7682\n",
      "Training Epoch: 6 [34600/36045]\tLoss: 993.6431\n",
      "Training Epoch: 6 [34650/36045]\tLoss: 1150.4246\n",
      "Training Epoch: 6 [34700/36045]\tLoss: 1210.4200\n",
      "Training Epoch: 6 [34750/36045]\tLoss: 1080.4055\n",
      "Training Epoch: 6 [34800/36045]\tLoss: 1212.1240\n",
      "Training Epoch: 6 [34850/36045]\tLoss: 1251.3765\n",
      "Training Epoch: 6 [34900/36045]\tLoss: 1524.4404\n",
      "Training Epoch: 6 [34950/36045]\tLoss: 1514.0784\n",
      "Training Epoch: 6 [35000/36045]\tLoss: 1526.7754\n",
      "Training Epoch: 6 [35050/36045]\tLoss: 1484.6390\n",
      "Training Epoch: 6 [35100/36045]\tLoss: 1146.9719\n",
      "Training Epoch: 6 [35150/36045]\tLoss: 1135.5769\n",
      "Training Epoch: 6 [35200/36045]\tLoss: 999.3962\n",
      "Training Epoch: 6 [35250/36045]\tLoss: 1082.0625\n",
      "Training Epoch: 6 [35300/36045]\tLoss: 1094.8104\n",
      "Training Epoch: 6 [35350/36045]\tLoss: 1300.0425\n",
      "Training Epoch: 6 [35400/36045]\tLoss: 1387.7668\n",
      "Training Epoch: 6 [35450/36045]\tLoss: 1330.1273\n",
      "Training Epoch: 6 [35500/36045]\tLoss: 1301.6184\n",
      "Training Epoch: 6 [35550/36045]\tLoss: 1287.6115\n",
      "Training Epoch: 6 [35600/36045]\tLoss: 1343.3882\n",
      "Training Epoch: 6 [35650/36045]\tLoss: 1453.5739\n",
      "Training Epoch: 6 [35700/36045]\tLoss: 1359.0043\n",
      "Training Epoch: 6 [35750/36045]\tLoss: 1440.9242\n",
      "Training Epoch: 6 [35800/36045]\tLoss: 1450.2542\n",
      "Training Epoch: 6 [35850/36045]\tLoss: 1416.3257\n",
      "Training Epoch: 6 [35900/36045]\tLoss: 1463.9149\n",
      "Training Epoch: 6 [35950/36045]\tLoss: 1461.1914\n",
      "Training Epoch: 6 [36000/36045]\tLoss: 1437.3931\n",
      "Training Epoch: 6 [36045/36045]\tLoss: 1417.1732\n",
      "Training Epoch: 6 [4004/4004]\tLoss: 1360.2200\n",
      "Training Epoch: 7 [50/36045]\tLoss: 1375.7659\n",
      "Training Epoch: 7 [100/36045]\tLoss: 1330.6284\n",
      "Training Epoch: 7 [150/36045]\tLoss: 1337.2131\n",
      "Training Epoch: 7 [200/36045]\tLoss: 1317.1029\n",
      "Training Epoch: 7 [250/36045]\tLoss: 1481.4540\n",
      "Training Epoch: 7 [300/36045]\tLoss: 1555.2271\n",
      "Training Epoch: 7 [350/36045]\tLoss: 1489.5981\n",
      "Training Epoch: 7 [400/36045]\tLoss: 1513.9834\n",
      "Training Epoch: 7 [450/36045]\tLoss: 1483.4021\n",
      "Training Epoch: 7 [500/36045]\tLoss: 1413.1549\n",
      "Training Epoch: 7 [550/36045]\tLoss: 1428.4493\n",
      "Training Epoch: 7 [600/36045]\tLoss: 1369.0385\n",
      "Training Epoch: 7 [650/36045]\tLoss: 1418.6886\n",
      "Training Epoch: 7 [700/36045]\tLoss: 1415.8512\n",
      "Training Epoch: 7 [750/36045]\tLoss: 1389.9233\n",
      "Training Epoch: 7 [800/36045]\tLoss: 1416.0244\n",
      "Training Epoch: 7 [850/36045]\tLoss: 1375.2583\n",
      "Training Epoch: 7 [900/36045]\tLoss: 1307.6263\n",
      "Training Epoch: 7 [950/36045]\tLoss: 1259.2555\n",
      "Training Epoch: 7 [1000/36045]\tLoss: 1219.1289\n",
      "Training Epoch: 7 [1050/36045]\tLoss: 1221.6936\n",
      "Training Epoch: 7 [1100/36045]\tLoss: 1191.2402\n",
      "Training Epoch: 7 [1150/36045]\tLoss: 1189.0237\n",
      "Training Epoch: 7 [1200/36045]\tLoss: 1252.3497\n",
      "Training Epoch: 7 [1250/36045]\tLoss: 1382.6790\n",
      "Training Epoch: 7 [1300/36045]\tLoss: 1371.3711\n",
      "Training Epoch: 7 [1350/36045]\tLoss: 1387.4652\n",
      "Training Epoch: 7 [1400/36045]\tLoss: 1435.4050\n",
      "Training Epoch: 7 [1450/36045]\tLoss: 1387.2390\n",
      "Training Epoch: 7 [1500/36045]\tLoss: 1301.0042\n",
      "Training Epoch: 7 [1550/36045]\tLoss: 1351.0665\n",
      "Training Epoch: 7 [1600/36045]\tLoss: 1356.3907\n",
      "Training Epoch: 7 [1650/36045]\tLoss: 1348.2549\n",
      "Training Epoch: 7 [1700/36045]\tLoss: 1364.3170\n",
      "Training Epoch: 7 [1750/36045]\tLoss: 1420.8754\n",
      "Training Epoch: 7 [1800/36045]\tLoss: 1405.8070\n",
      "Training Epoch: 7 [1850/36045]\tLoss: 1459.4292\n",
      "Training Epoch: 7 [1900/36045]\tLoss: 1380.2085\n",
      "Training Epoch: 7 [1950/36045]\tLoss: 1377.8384\n",
      "Training Epoch: 7 [2000/36045]\tLoss: 1242.5848\n",
      "Training Epoch: 7 [2050/36045]\tLoss: 1251.4232\n",
      "Training Epoch: 7 [2100/36045]\tLoss: 1320.6981\n",
      "Training Epoch: 7 [2150/36045]\tLoss: 1287.6084\n",
      "Training Epoch: 7 [2200/36045]\tLoss: 1198.2291\n",
      "Training Epoch: 7 [2250/36045]\tLoss: 1129.3412\n",
      "Training Epoch: 7 [2300/36045]\tLoss: 1176.7008\n",
      "Training Epoch: 7 [2350/36045]\tLoss: 1126.9740\n",
      "Training Epoch: 7 [2400/36045]\tLoss: 1167.5737\n",
      "Training Epoch: 7 [2450/36045]\tLoss: 1414.9178\n",
      "Training Epoch: 7 [2500/36045]\tLoss: 1473.4001\n",
      "Training Epoch: 7 [2550/36045]\tLoss: 1471.0433\n",
      "Training Epoch: 7 [2600/36045]\tLoss: 1463.6859\n",
      "Training Epoch: 7 [2650/36045]\tLoss: 1623.8435\n",
      "Training Epoch: 7 [2700/36045]\tLoss: 1728.1185\n",
      "Training Epoch: 7 [2750/36045]\tLoss: 1820.4906\n",
      "Training Epoch: 7 [2800/36045]\tLoss: 1834.9344\n",
      "Training Epoch: 7 [2850/36045]\tLoss: 1591.7886\n",
      "Training Epoch: 7 [2900/36045]\tLoss: 1595.0381\n",
      "Training Epoch: 7 [2950/36045]\tLoss: 1544.4270\n",
      "Training Epoch: 7 [3000/36045]\tLoss: 1538.4312\n",
      "Training Epoch: 7 [3050/36045]\tLoss: 1586.0558\n",
      "Training Epoch: 7 [3100/36045]\tLoss: 1441.5563\n",
      "Training Epoch: 7 [3150/36045]\tLoss: 1120.9094\n",
      "Training Epoch: 7 [3200/36045]\tLoss: 1164.5146\n",
      "Training Epoch: 7 [3250/36045]\tLoss: 1091.2084\n",
      "Training Epoch: 7 [3300/36045]\tLoss: 1033.5161\n",
      "Training Epoch: 7 [3350/36045]\tLoss: 1089.3250\n",
      "Training Epoch: 7 [3400/36045]\tLoss: 1145.8883\n",
      "Training Epoch: 7 [3450/36045]\tLoss: 1238.6116\n",
      "Training Epoch: 7 [3500/36045]\tLoss: 1204.3883\n",
      "Training Epoch: 7 [3550/36045]\tLoss: 1173.0540\n",
      "Training Epoch: 7 [3600/36045]\tLoss: 1248.5615\n",
      "Training Epoch: 7 [3650/36045]\tLoss: 1436.2611\n",
      "Training Epoch: 7 [3700/36045]\tLoss: 1441.3796\n",
      "Training Epoch: 7 [3750/36045]\tLoss: 1393.6986\n",
      "Training Epoch: 7 [3800/36045]\tLoss: 1370.4116\n",
      "Training Epoch: 7 [3850/36045]\tLoss: 1376.5470\n",
      "Training Epoch: 7 [3900/36045]\tLoss: 1386.0851\n",
      "Training Epoch: 7 [3950/36045]\tLoss: 1321.4200\n",
      "Training Epoch: 7 [4000/36045]\tLoss: 1361.2604\n",
      "Training Epoch: 7 [4050/36045]\tLoss: 1251.3134\n",
      "Training Epoch: 7 [4100/36045]\tLoss: 1215.1670\n",
      "Training Epoch: 7 [4150/36045]\tLoss: 1259.8741\n",
      "Training Epoch: 7 [4200/36045]\tLoss: 1242.4233\n",
      "Training Epoch: 7 [4250/36045]\tLoss: 1265.8586\n",
      "Training Epoch: 7 [4300/36045]\tLoss: 1304.9642\n",
      "Training Epoch: 7 [4350/36045]\tLoss: 1285.9058\n",
      "Training Epoch: 7 [4400/36045]\tLoss: 1226.6252\n",
      "Training Epoch: 7 [4450/36045]\tLoss: 1301.8500\n",
      "Training Epoch: 7 [4500/36045]\tLoss: 1365.9191\n",
      "Training Epoch: 7 [4550/36045]\tLoss: 1393.7042\n",
      "Training Epoch: 7 [4600/36045]\tLoss: 1411.3595\n",
      "Training Epoch: 7 [4650/36045]\tLoss: 1417.5038\n",
      "Training Epoch: 7 [4700/36045]\tLoss: 1314.5059\n",
      "Training Epoch: 7 [4750/36045]\tLoss: 1294.3042\n",
      "Training Epoch: 7 [4800/36045]\tLoss: 1344.6437\n",
      "Training Epoch: 7 [4850/36045]\tLoss: 1320.1598\n",
      "Training Epoch: 7 [4900/36045]\tLoss: 1275.4001\n",
      "Training Epoch: 7 [4950/36045]\tLoss: 1329.2603\n",
      "Training Epoch: 7 [5000/36045]\tLoss: 1406.2738\n",
      "Training Epoch: 7 [5050/36045]\tLoss: 1360.4521\n",
      "Training Epoch: 7 [5100/36045]\tLoss: 1392.8018\n",
      "Training Epoch: 7 [5150/36045]\tLoss: 1351.8365\n",
      "Training Epoch: 7 [5200/36045]\tLoss: 1329.5735\n",
      "Training Epoch: 7 [5250/36045]\tLoss: 1315.7145\n",
      "Training Epoch: 7 [5300/36045]\tLoss: 1320.7445\n",
      "Training Epoch: 7 [5350/36045]\tLoss: 1366.6312\n",
      "Training Epoch: 7 [5400/36045]\tLoss: 1301.8660\n",
      "Training Epoch: 7 [5450/36045]\tLoss: 1212.3705\n",
      "Training Epoch: 7 [5500/36045]\tLoss: 1279.5010\n",
      "Training Epoch: 7 [5550/36045]\tLoss: 1246.5201\n",
      "Training Epoch: 7 [5600/36045]\tLoss: 1403.4996\n",
      "Training Epoch: 7 [5650/36045]\tLoss: 1349.9290\n",
      "Training Epoch: 7 [5700/36045]\tLoss: 1284.2549\n",
      "Training Epoch: 7 [5750/36045]\tLoss: 1268.0645\n",
      "Training Epoch: 7 [5800/36045]\tLoss: 1349.5284\n",
      "Training Epoch: 7 [5850/36045]\tLoss: 1305.1737\n",
      "Training Epoch: 7 [5900/36045]\tLoss: 1488.2482\n",
      "Training Epoch: 7 [5950/36045]\tLoss: 1532.2118\n",
      "Training Epoch: 7 [6000/36045]\tLoss: 1504.8103\n",
      "Training Epoch: 7 [6050/36045]\tLoss: 1449.8191\n",
      "Training Epoch: 7 [6100/36045]\tLoss: 1451.0201\n",
      "Training Epoch: 7 [6150/36045]\tLoss: 1406.0735\n",
      "Training Epoch: 7 [6200/36045]\tLoss: 1396.4250\n",
      "Training Epoch: 7 [6250/36045]\tLoss: 1406.6648\n",
      "Training Epoch: 7 [6300/36045]\tLoss: 1450.3390\n",
      "Training Epoch: 7 [6350/36045]\tLoss: 1509.3334\n",
      "Training Epoch: 7 [6400/36045]\tLoss: 1324.6776\n",
      "Training Epoch: 7 [6450/36045]\tLoss: 1233.0778\n",
      "Training Epoch: 7 [6500/36045]\tLoss: 1266.1783\n",
      "Training Epoch: 7 [6550/36045]\tLoss: 1289.1615\n",
      "Training Epoch: 7 [6600/36045]\tLoss: 1289.4163\n",
      "Training Epoch: 7 [6650/36045]\tLoss: 1434.7993\n",
      "Training Epoch: 7 [6700/36045]\tLoss: 1502.5118\n",
      "Training Epoch: 7 [6750/36045]\tLoss: 1468.0848\n",
      "Training Epoch: 7 [6800/36045]\tLoss: 1461.2247\n",
      "Training Epoch: 7 [6850/36045]\tLoss: 1440.2390\n",
      "Training Epoch: 7 [6900/36045]\tLoss: 1272.6971\n",
      "Training Epoch: 7 [6950/36045]\tLoss: 1199.1407\n",
      "Training Epoch: 7 [7000/36045]\tLoss: 1267.4148\n",
      "Training Epoch: 7 [7050/36045]\tLoss: 1314.9115\n",
      "Training Epoch: 7 [7100/36045]\tLoss: 1299.5662\n",
      "Training Epoch: 7 [7150/36045]\tLoss: 1326.8409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [7200/36045]\tLoss: 1336.4685\n",
      "Training Epoch: 7 [7250/36045]\tLoss: 1335.3671\n",
      "Training Epoch: 7 [7300/36045]\tLoss: 1310.5735\n",
      "Training Epoch: 7 [7350/36045]\tLoss: 1306.9741\n",
      "Training Epoch: 7 [7400/36045]\tLoss: 1214.6660\n",
      "Training Epoch: 7 [7450/36045]\tLoss: 1213.7578\n",
      "Training Epoch: 7 [7500/36045]\tLoss: 1213.6864\n",
      "Training Epoch: 7 [7550/36045]\tLoss: 1157.7462\n",
      "Training Epoch: 7 [7600/36045]\tLoss: 1277.1920\n",
      "Training Epoch: 7 [7650/36045]\tLoss: 1360.9983\n",
      "Training Epoch: 7 [7700/36045]\tLoss: 1303.9521\n",
      "Training Epoch: 7 [7750/36045]\tLoss: 1323.0133\n",
      "Training Epoch: 7 [7800/36045]\tLoss: 1301.5653\n",
      "Training Epoch: 7 [7850/36045]\tLoss: 1227.3422\n",
      "Training Epoch: 7 [7900/36045]\tLoss: 1297.0625\n",
      "Training Epoch: 7 [7950/36045]\tLoss: 1293.5219\n",
      "Training Epoch: 7 [8000/36045]\tLoss: 1311.8652\n",
      "Training Epoch: 7 [8050/36045]\tLoss: 1252.5402\n",
      "Training Epoch: 7 [8100/36045]\tLoss: 1295.3556\n",
      "Training Epoch: 7 [8150/36045]\tLoss: 1458.5200\n",
      "Training Epoch: 7 [8200/36045]\tLoss: 1440.1410\n",
      "Training Epoch: 7 [8250/36045]\tLoss: 1390.3563\n",
      "Training Epoch: 7 [8300/36045]\tLoss: 1496.6005\n",
      "Training Epoch: 7 [8350/36045]\tLoss: 1390.4614\n",
      "Training Epoch: 7 [8400/36045]\tLoss: 1282.8221\n",
      "Training Epoch: 7 [8450/36045]\tLoss: 1219.4708\n",
      "Training Epoch: 7 [8500/36045]\tLoss: 1273.4253\n",
      "Training Epoch: 7 [8550/36045]\tLoss: 1244.4666\n",
      "Training Epoch: 7 [8600/36045]\tLoss: 1244.8044\n",
      "Training Epoch: 7 [8650/36045]\tLoss: 1299.6512\n",
      "Training Epoch: 7 [8700/36045]\tLoss: 1364.1140\n",
      "Training Epoch: 7 [8750/36045]\tLoss: 1337.8260\n",
      "Training Epoch: 7 [8800/36045]\tLoss: 1348.7406\n",
      "Training Epoch: 7 [8850/36045]\tLoss: 1331.5349\n",
      "Training Epoch: 7 [8900/36045]\tLoss: 1210.0302\n",
      "Training Epoch: 7 [8950/36045]\tLoss: 1247.8635\n",
      "Training Epoch: 7 [9000/36045]\tLoss: 1253.1155\n",
      "Training Epoch: 7 [9050/36045]\tLoss: 1247.0887\n",
      "Training Epoch: 7 [9100/36045]\tLoss: 1277.7183\n",
      "Training Epoch: 7 [9150/36045]\tLoss: 950.0465\n",
      "Training Epoch: 7 [9200/36045]\tLoss: 734.1541\n",
      "Training Epoch: 7 [9250/36045]\tLoss: 792.3486\n",
      "Training Epoch: 7 [9300/36045]\tLoss: 821.7332\n",
      "Training Epoch: 7 [9350/36045]\tLoss: 749.1171\n",
      "Training Epoch: 7 [9400/36045]\tLoss: 1455.7405\n",
      "Training Epoch: 7 [9450/36045]\tLoss: 1530.5542\n",
      "Training Epoch: 7 [9500/36045]\tLoss: 1511.6122\n",
      "Training Epoch: 7 [9550/36045]\tLoss: 1605.6218\n",
      "Training Epoch: 7 [9600/36045]\tLoss: 1179.3459\n",
      "Training Epoch: 7 [9650/36045]\tLoss: 1169.8864\n",
      "Training Epoch: 7 [9700/36045]\tLoss: 1164.4670\n",
      "Training Epoch: 7 [9750/36045]\tLoss: 1160.4868\n",
      "Training Epoch: 7 [9800/36045]\tLoss: 1500.4784\n",
      "Training Epoch: 7 [9850/36045]\tLoss: 1582.9520\n",
      "Training Epoch: 7 [9900/36045]\tLoss: 1621.7889\n",
      "Training Epoch: 7 [9950/36045]\tLoss: 1568.5601\n",
      "Training Epoch: 7 [10000/36045]\tLoss: 1458.5111\n",
      "Training Epoch: 7 [10050/36045]\tLoss: 1246.6047\n",
      "Training Epoch: 7 [10100/36045]\tLoss: 1242.8649\n",
      "Training Epoch: 7 [10150/36045]\tLoss: 1255.9076\n",
      "Training Epoch: 7 [10200/36045]\tLoss: 1255.2130\n",
      "Training Epoch: 7 [10250/36045]\tLoss: 1511.9843\n",
      "Training Epoch: 7 [10300/36045]\tLoss: 1474.1816\n",
      "Training Epoch: 7 [10350/36045]\tLoss: 1546.9506\n",
      "Training Epoch: 7 [10400/36045]\tLoss: 1539.0986\n",
      "Training Epoch: 7 [10450/36045]\tLoss: 1416.2887\n",
      "Training Epoch: 7 [10500/36045]\tLoss: 1197.9481\n",
      "Training Epoch: 7 [10550/36045]\tLoss: 1195.0826\n",
      "Training Epoch: 7 [10600/36045]\tLoss: 1232.8419\n",
      "Training Epoch: 7 [10650/36045]\tLoss: 1243.7104\n",
      "Training Epoch: 7 [10700/36045]\tLoss: 1354.1133\n",
      "Training Epoch: 7 [10750/36045]\tLoss: 1452.0468\n",
      "Training Epoch: 7 [10800/36045]\tLoss: 1360.8574\n",
      "Training Epoch: 7 [10850/36045]\tLoss: 1420.4183\n",
      "Training Epoch: 7 [10900/36045]\tLoss: 1478.3730\n",
      "Training Epoch: 7 [10950/36045]\tLoss: 1134.6436\n",
      "Training Epoch: 7 [11000/36045]\tLoss: 1132.1342\n",
      "Training Epoch: 7 [11050/36045]\tLoss: 1198.8883\n",
      "Training Epoch: 7 [11100/36045]\tLoss: 1214.4688\n",
      "Training Epoch: 7 [11150/36045]\tLoss: 1311.6962\n",
      "Training Epoch: 7 [11200/36045]\tLoss: 1357.6416\n",
      "Training Epoch: 7 [11250/36045]\tLoss: 1377.8759\n",
      "Training Epoch: 7 [11300/36045]\tLoss: 1353.4188\n",
      "Training Epoch: 7 [11350/36045]\tLoss: 1350.2266\n",
      "Training Epoch: 7 [11400/36045]\tLoss: 1288.0886\n",
      "Training Epoch: 7 [11450/36045]\tLoss: 1248.0382\n",
      "Training Epoch: 7 [11500/36045]\tLoss: 1247.4198\n",
      "Training Epoch: 7 [11550/36045]\tLoss: 1275.1338\n",
      "Training Epoch: 7 [11600/36045]\tLoss: 1368.7262\n",
      "Training Epoch: 7 [11650/36045]\tLoss: 1430.6073\n",
      "Training Epoch: 7 [11700/36045]\tLoss: 1428.5090\n",
      "Training Epoch: 7 [11750/36045]\tLoss: 1452.2587\n",
      "Training Epoch: 7 [11800/36045]\tLoss: 1510.4008\n",
      "Training Epoch: 7 [11850/36045]\tLoss: 1537.7513\n",
      "Training Epoch: 7 [11900/36045]\tLoss: 1784.3030\n",
      "Training Epoch: 7 [11950/36045]\tLoss: 1773.6467\n",
      "Training Epoch: 7 [12000/36045]\tLoss: 1807.5308\n",
      "Training Epoch: 7 [12050/36045]\tLoss: 1751.1661\n",
      "Training Epoch: 7 [12100/36045]\tLoss: 1295.7421\n",
      "Training Epoch: 7 [12150/36045]\tLoss: 1089.3115\n",
      "Training Epoch: 7 [12200/36045]\tLoss: 1073.8918\n",
      "Training Epoch: 7 [12250/36045]\tLoss: 1088.5337\n",
      "Training Epoch: 7 [12300/36045]\tLoss: 1308.4497\n",
      "Training Epoch: 7 [12350/36045]\tLoss: 1385.2101\n",
      "Training Epoch: 7 [12400/36045]\tLoss: 1414.1206\n",
      "Training Epoch: 7 [12450/36045]\tLoss: 1390.4077\n",
      "Training Epoch: 7 [12500/36045]\tLoss: 1437.6761\n",
      "Training Epoch: 7 [12550/36045]\tLoss: 1373.3695\n",
      "Training Epoch: 7 [12600/36045]\tLoss: 1296.4075\n",
      "Training Epoch: 7 [12650/36045]\tLoss: 1294.2629\n",
      "Training Epoch: 7 [12700/36045]\tLoss: 1330.4210\n",
      "Training Epoch: 7 [12750/36045]\tLoss: 1338.9800\n",
      "Training Epoch: 7 [12800/36045]\tLoss: 1322.5535\n",
      "Training Epoch: 7 [12850/36045]\tLoss: 1381.5782\n",
      "Training Epoch: 7 [12900/36045]\tLoss: 1326.2889\n",
      "Training Epoch: 7 [12950/36045]\tLoss: 1310.1298\n",
      "Training Epoch: 7 [13000/36045]\tLoss: 1346.6036\n",
      "Training Epoch: 7 [13050/36045]\tLoss: 1255.9851\n",
      "Training Epoch: 7 [13100/36045]\tLoss: 1318.0798\n",
      "Training Epoch: 7 [13150/36045]\tLoss: 1323.1292\n",
      "Training Epoch: 7 [13200/36045]\tLoss: 1251.3712\n",
      "Training Epoch: 7 [13250/36045]\tLoss: 1315.4166\n",
      "Training Epoch: 7 [13300/36045]\tLoss: 1359.4128\n",
      "Training Epoch: 7 [13350/36045]\tLoss: 1324.1287\n",
      "Training Epoch: 7 [13400/36045]\tLoss: 1327.7804\n",
      "Training Epoch: 7 [13450/36045]\tLoss: 1311.6515\n",
      "Training Epoch: 7 [13500/36045]\tLoss: 1369.8899\n",
      "Training Epoch: 7 [13550/36045]\tLoss: 1502.6533\n",
      "Training Epoch: 7 [13600/36045]\tLoss: 1530.0513\n",
      "Training Epoch: 7 [13650/36045]\tLoss: 1622.0018\n",
      "Training Epoch: 7 [13700/36045]\tLoss: 1469.0962\n",
      "Training Epoch: 7 [13750/36045]\tLoss: 1353.7836\n",
      "Training Epoch: 7 [13800/36045]\tLoss: 1329.2014\n",
      "Training Epoch: 7 [13850/36045]\tLoss: 1311.6326\n",
      "Training Epoch: 7 [13900/36045]\tLoss: 1325.8031\n",
      "Training Epoch: 7 [13950/36045]\tLoss: 1341.4012\n",
      "Training Epoch: 7 [14000/36045]\tLoss: 1392.0198\n",
      "Training Epoch: 7 [14050/36045]\tLoss: 1338.0719\n",
      "Training Epoch: 7 [14100/36045]\tLoss: 1337.9099\n",
      "Training Epoch: 7 [14150/36045]\tLoss: 1304.3264\n",
      "Training Epoch: 7 [14200/36045]\tLoss: 1394.3029\n",
      "Training Epoch: 7 [14250/36045]\tLoss: 1527.1067\n",
      "Training Epoch: 7 [14300/36045]\tLoss: 1528.4048\n",
      "Training Epoch: 7 [14350/36045]\tLoss: 1467.5302\n",
      "Training Epoch: 7 [14400/36045]\tLoss: 1453.0690\n",
      "Training Epoch: 7 [14450/36045]\tLoss: 1507.9807\n",
      "Training Epoch: 7 [14500/36045]\tLoss: 1425.0325\n",
      "Training Epoch: 7 [14550/36045]\tLoss: 1495.1089\n",
      "Training Epoch: 7 [14600/36045]\tLoss: 1453.1227\n",
      "Training Epoch: 7 [14650/36045]\tLoss: 1456.2421\n",
      "Training Epoch: 7 [14700/36045]\tLoss: 1367.4978\n",
      "Training Epoch: 7 [14750/36045]\tLoss: 1185.4845\n",
      "Training Epoch: 7 [14800/36045]\tLoss: 1165.8372\n",
      "Training Epoch: 7 [14850/36045]\tLoss: 1173.5530\n",
      "Training Epoch: 7 [14900/36045]\tLoss: 1164.1907\n",
      "Training Epoch: 7 [14950/36045]\tLoss: 1184.2668\n",
      "Training Epoch: 7 [15000/36045]\tLoss: 1225.7668\n",
      "Training Epoch: 7 [15050/36045]\tLoss: 1243.1760\n",
      "Training Epoch: 7 [15100/36045]\tLoss: 1214.3354\n",
      "Training Epoch: 7 [15150/36045]\tLoss: 1179.9453\n",
      "Training Epoch: 7 [15200/36045]\tLoss: 1082.7109\n",
      "Training Epoch: 7 [15250/36045]\tLoss: 1130.0747\n",
      "Training Epoch: 7 [15300/36045]\tLoss: 1102.7402\n",
      "Training Epoch: 7 [15350/36045]\tLoss: 1123.7661\n",
      "Training Epoch: 7 [15400/36045]\tLoss: 1145.0513\n",
      "Training Epoch: 7 [15450/36045]\tLoss: 1140.3846\n",
      "Training Epoch: 7 [15500/36045]\tLoss: 1169.1674\n",
      "Training Epoch: 7 [15550/36045]\tLoss: 1139.2850\n",
      "Training Epoch: 7 [15600/36045]\tLoss: 1227.9513\n",
      "Training Epoch: 7 [15650/36045]\tLoss: 1264.2872\n",
      "Training Epoch: 7 [15700/36045]\tLoss: 1238.7452\n",
      "Training Epoch: 7 [15750/36045]\tLoss: 1223.2998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [15800/36045]\tLoss: 1107.3385\n",
      "Training Epoch: 7 [15850/36045]\tLoss: 1107.1392\n",
      "Training Epoch: 7 [15900/36045]\tLoss: 1108.7714\n",
      "Training Epoch: 7 [15950/36045]\tLoss: 1145.1202\n",
      "Training Epoch: 7 [16000/36045]\tLoss: 1148.9292\n",
      "Training Epoch: 7 [16050/36045]\tLoss: 1108.2283\n",
      "Training Epoch: 7 [16100/36045]\tLoss: 1035.1604\n",
      "Training Epoch: 7 [16150/36045]\tLoss: 1003.0779\n",
      "Training Epoch: 7 [16200/36045]\tLoss: 1179.4430\n",
      "Training Epoch: 7 [16250/36045]\tLoss: 1223.1031\n",
      "Training Epoch: 7 [16300/36045]\tLoss: 1317.7158\n",
      "Training Epoch: 7 [16350/36045]\tLoss: 1341.1339\n",
      "Training Epoch: 7 [16400/36045]\tLoss: 1313.1475\n",
      "Training Epoch: 7 [16450/36045]\tLoss: 1290.0063\n",
      "Training Epoch: 7 [16500/36045]\tLoss: 1291.0944\n",
      "Training Epoch: 7 [16550/36045]\tLoss: 1231.0940\n",
      "Training Epoch: 7 [16600/36045]\tLoss: 1290.4927\n",
      "Training Epoch: 7 [16650/36045]\tLoss: 1322.3077\n",
      "Training Epoch: 7 [16700/36045]\tLoss: 1300.0912\n",
      "Training Epoch: 7 [16750/36045]\tLoss: 1287.2501\n",
      "Training Epoch: 7 [16800/36045]\tLoss: 1312.9287\n",
      "Training Epoch: 7 [16850/36045]\tLoss: 1251.0540\n",
      "Training Epoch: 7 [16900/36045]\tLoss: 1257.8723\n",
      "Training Epoch: 7 [16950/36045]\tLoss: 1272.8940\n",
      "Training Epoch: 7 [17000/36045]\tLoss: 1246.8108\n",
      "Training Epoch: 7 [17050/36045]\tLoss: 1321.8469\n",
      "Training Epoch: 7 [17100/36045]\tLoss: 1330.8074\n",
      "Training Epoch: 7 [17150/36045]\tLoss: 1176.6154\n",
      "Training Epoch: 7 [17200/36045]\tLoss: 1124.1687\n",
      "Training Epoch: 7 [17250/36045]\tLoss: 1171.3303\n",
      "Training Epoch: 7 [17300/36045]\tLoss: 1240.3320\n",
      "Training Epoch: 7 [17350/36045]\tLoss: 1164.9415\n",
      "Training Epoch: 7 [17400/36045]\tLoss: 1170.3392\n",
      "Training Epoch: 7 [17450/36045]\tLoss: 1210.8804\n",
      "Training Epoch: 7 [17500/36045]\tLoss: 1190.8274\n",
      "Training Epoch: 7 [17550/36045]\tLoss: 1212.5898\n",
      "Training Epoch: 7 [17600/36045]\tLoss: 1197.7145\n",
      "Training Epoch: 7 [17650/36045]\tLoss: 1234.0974\n",
      "Training Epoch: 7 [17700/36045]\tLoss: 1199.2523\n",
      "Training Epoch: 7 [17750/36045]\tLoss: 1226.3070\n",
      "Training Epoch: 7 [17800/36045]\tLoss: 1220.0554\n",
      "Training Epoch: 7 [17850/36045]\tLoss: 1159.9155\n",
      "Training Epoch: 7 [17900/36045]\tLoss: 1200.5778\n",
      "Training Epoch: 7 [17950/36045]\tLoss: 1212.2200\n",
      "Training Epoch: 7 [18000/36045]\tLoss: 1191.6862\n",
      "Training Epoch: 7 [18050/36045]\tLoss: 1349.6974\n",
      "Training Epoch: 7 [18100/36045]\tLoss: 1368.8127\n",
      "Training Epoch: 7 [18150/36045]\tLoss: 1371.7949\n",
      "Training Epoch: 7 [18200/36045]\tLoss: 1362.6140\n",
      "Training Epoch: 7 [18250/36045]\tLoss: 1386.4296\n",
      "Training Epoch: 7 [18300/36045]\tLoss: 1268.9675\n",
      "Training Epoch: 7 [18350/36045]\tLoss: 1331.7493\n",
      "Training Epoch: 7 [18400/36045]\tLoss: 1315.6947\n",
      "Training Epoch: 7 [18450/36045]\tLoss: 1288.8391\n",
      "Training Epoch: 7 [18500/36045]\tLoss: 1281.3146\n",
      "Training Epoch: 7 [18550/36045]\tLoss: 1253.4033\n",
      "Training Epoch: 7 [18600/36045]\tLoss: 1236.5162\n",
      "Training Epoch: 7 [18650/36045]\tLoss: 1296.0145\n",
      "Training Epoch: 7 [18700/36045]\tLoss: 1364.7913\n",
      "Training Epoch: 7 [18750/36045]\tLoss: 1344.9052\n",
      "Training Epoch: 7 [18800/36045]\tLoss: 1393.7394\n",
      "Training Epoch: 7 [18850/36045]\tLoss: 1323.1685\n",
      "Training Epoch: 7 [18900/36045]\tLoss: 1409.6975\n",
      "Training Epoch: 7 [18950/36045]\tLoss: 1328.9502\n",
      "Training Epoch: 7 [19000/36045]\tLoss: 1182.4624\n",
      "Training Epoch: 7 [19050/36045]\tLoss: 1128.2263\n",
      "Training Epoch: 7 [19100/36045]\tLoss: 1158.1113\n",
      "Training Epoch: 7 [19150/36045]\tLoss: 1138.1010\n",
      "Training Epoch: 7 [19200/36045]\tLoss: 1184.9432\n",
      "Training Epoch: 7 [19250/36045]\tLoss: 1192.2004\n",
      "Training Epoch: 7 [19300/36045]\tLoss: 1231.5022\n",
      "Training Epoch: 7 [19350/36045]\tLoss: 1193.6042\n",
      "Training Epoch: 7 [19400/36045]\tLoss: 1225.3271\n",
      "Training Epoch: 7 [19450/36045]\tLoss: 1208.7498\n",
      "Training Epoch: 7 [19500/36045]\tLoss: 1214.4619\n",
      "Training Epoch: 7 [19550/36045]\tLoss: 1224.4471\n",
      "Training Epoch: 7 [19600/36045]\tLoss: 1274.8956\n",
      "Training Epoch: 7 [19650/36045]\tLoss: 1584.9467\n",
      "Training Epoch: 7 [19700/36045]\tLoss: 1526.9683\n",
      "Training Epoch: 7 [19750/36045]\tLoss: 1523.5856\n",
      "Training Epoch: 7 [19800/36045]\tLoss: 1515.2283\n",
      "Training Epoch: 7 [19850/36045]\tLoss: 1088.5447\n",
      "Training Epoch: 7 [19900/36045]\tLoss: 1052.1627\n",
      "Training Epoch: 7 [19950/36045]\tLoss: 1060.4694\n",
      "Training Epoch: 7 [20000/36045]\tLoss: 1059.6962\n",
      "Training Epoch: 7 [20050/36045]\tLoss: 1186.8728\n",
      "Training Epoch: 7 [20100/36045]\tLoss: 1184.0624\n",
      "Training Epoch: 7 [20150/36045]\tLoss: 1195.8419\n",
      "Training Epoch: 7 [20200/36045]\tLoss: 1185.2057\n",
      "Training Epoch: 7 [20250/36045]\tLoss: 1255.1217\n",
      "Training Epoch: 7 [20300/36045]\tLoss: 1294.4706\n",
      "Training Epoch: 7 [20350/36045]\tLoss: 1336.1635\n",
      "Training Epoch: 7 [20400/36045]\tLoss: 1356.7129\n",
      "Training Epoch: 7 [20450/36045]\tLoss: 1330.9531\n",
      "Training Epoch: 7 [20500/36045]\tLoss: 1289.9927\n",
      "Training Epoch: 7 [20550/36045]\tLoss: 1176.9330\n",
      "Training Epoch: 7 [20600/36045]\tLoss: 1200.5750\n",
      "Training Epoch: 7 [20650/36045]\tLoss: 1186.4143\n",
      "Training Epoch: 7 [20700/36045]\tLoss: 1168.0507\n",
      "Training Epoch: 7 [20750/36045]\tLoss: 1237.6062\n",
      "Training Epoch: 7 [20800/36045]\tLoss: 1347.1128\n",
      "Training Epoch: 7 [20850/36045]\tLoss: 1334.3916\n",
      "Training Epoch: 7 [20900/36045]\tLoss: 1407.4795\n",
      "Training Epoch: 7 [20950/36045]\tLoss: 1335.1927\n",
      "Training Epoch: 7 [21000/36045]\tLoss: 1252.9548\n",
      "Training Epoch: 7 [21050/36045]\tLoss: 1070.8210\n",
      "Training Epoch: 7 [21100/36045]\tLoss: 1072.2068\n",
      "Training Epoch: 7 [21150/36045]\tLoss: 1142.5850\n",
      "Training Epoch: 7 [21200/36045]\tLoss: 1149.8419\n",
      "Training Epoch: 7 [21250/36045]\tLoss: 1092.0981\n",
      "Training Epoch: 7 [21300/36045]\tLoss: 1296.2324\n",
      "Training Epoch: 7 [21350/36045]\tLoss: 1285.7750\n",
      "Training Epoch: 7 [21400/36045]\tLoss: 1297.0911\n",
      "Training Epoch: 7 [21450/36045]\tLoss: 1319.5946\n",
      "Training Epoch: 7 [21500/36045]\tLoss: 1321.3809\n",
      "Training Epoch: 7 [21550/36045]\tLoss: 1430.0763\n",
      "Training Epoch: 7 [21600/36045]\tLoss: 1429.7770\n",
      "Training Epoch: 7 [21650/36045]\tLoss: 1447.4060\n",
      "Training Epoch: 7 [21700/36045]\tLoss: 1442.2495\n",
      "Training Epoch: 7 [21750/36045]\tLoss: 1396.9120\n",
      "Training Epoch: 7 [21800/36045]\tLoss: 1073.6682\n",
      "Training Epoch: 7 [21850/36045]\tLoss: 1048.3441\n",
      "Training Epoch: 7 [21900/36045]\tLoss: 1069.0188\n",
      "Training Epoch: 7 [21950/36045]\tLoss: 1057.1831\n",
      "Training Epoch: 7 [22000/36045]\tLoss: 1070.2831\n",
      "Training Epoch: 7 [22050/36045]\tLoss: 1148.2997\n",
      "Training Epoch: 7 [22100/36045]\tLoss: 1129.1981\n",
      "Training Epoch: 7 [22150/36045]\tLoss: 1101.4915\n",
      "Training Epoch: 7 [22200/36045]\tLoss: 1132.1477\n",
      "Training Epoch: 7 [22250/36045]\tLoss: 1146.6212\n",
      "Training Epoch: 7 [22300/36045]\tLoss: 1218.5427\n",
      "Training Epoch: 7 [22350/36045]\tLoss: 1245.1670\n",
      "Training Epoch: 7 [22400/36045]\tLoss: 1276.8219\n",
      "Training Epoch: 7 [22450/36045]\tLoss: 1252.4404\n",
      "Training Epoch: 7 [22500/36045]\tLoss: 1217.7219\n",
      "Training Epoch: 7 [22550/36045]\tLoss: 1294.9985\n",
      "Training Epoch: 7 [22600/36045]\tLoss: 1428.4706\n",
      "Training Epoch: 7 [22650/36045]\tLoss: 1488.9293\n",
      "Training Epoch: 7 [22700/36045]\tLoss: 1519.0071\n",
      "Training Epoch: 7 [22750/36045]\tLoss: 1547.8035\n",
      "Training Epoch: 7 [22800/36045]\tLoss: 1617.6219\n",
      "Training Epoch: 7 [22850/36045]\tLoss: 1350.1370\n",
      "Training Epoch: 7 [22900/36045]\tLoss: 1352.3280\n",
      "Training Epoch: 7 [22950/36045]\tLoss: 1319.1814\n",
      "Training Epoch: 7 [23000/36045]\tLoss: 1331.1497\n",
      "Training Epoch: 7 [23050/36045]\tLoss: 1197.2284\n",
      "Training Epoch: 7 [23100/36045]\tLoss: 1217.7856\n",
      "Training Epoch: 7 [23150/36045]\tLoss: 1196.3248\n",
      "Training Epoch: 7 [23200/36045]\tLoss: 1129.5178\n",
      "Training Epoch: 7 [23250/36045]\tLoss: 1133.3735\n",
      "Training Epoch: 7 [23300/36045]\tLoss: 1122.0284\n",
      "Training Epoch: 7 [23350/36045]\tLoss: 1147.7957\n",
      "Training Epoch: 7 [23400/36045]\tLoss: 1246.2341\n",
      "Training Epoch: 7 [23450/36045]\tLoss: 1230.5226\n",
      "Training Epoch: 7 [23500/36045]\tLoss: 1179.1263\n",
      "Training Epoch: 7 [23550/36045]\tLoss: 1276.8505\n",
      "Training Epoch: 7 [23600/36045]\tLoss: 1427.7389\n",
      "Training Epoch: 7 [23650/36045]\tLoss: 1467.3348\n",
      "Training Epoch: 7 [23700/36045]\tLoss: 1474.7316\n",
      "Training Epoch: 7 [23750/36045]\tLoss: 1434.0006\n",
      "Training Epoch: 7 [23800/36045]\tLoss: 1156.9563\n",
      "Training Epoch: 7 [23850/36045]\tLoss: 1200.6998\n",
      "Training Epoch: 7 [23900/36045]\tLoss: 1185.5645\n",
      "Training Epoch: 7 [23950/36045]\tLoss: 1167.0188\n",
      "Training Epoch: 7 [24000/36045]\tLoss: 1134.0330\n",
      "Training Epoch: 7 [24050/36045]\tLoss: 1034.3706\n",
      "Training Epoch: 7 [24100/36045]\tLoss: 1079.8634\n",
      "Training Epoch: 7 [24150/36045]\tLoss: 1084.9601\n",
      "Training Epoch: 7 [24200/36045]\tLoss: 1058.2184\n",
      "Training Epoch: 7 [24250/36045]\tLoss: 1028.7432\n",
      "Training Epoch: 7 [24300/36045]\tLoss: 1123.6189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [24350/36045]\tLoss: 1160.1490\n",
      "Training Epoch: 7 [24400/36045]\tLoss: 1186.5286\n",
      "Training Epoch: 7 [24450/36045]\tLoss: 1144.7670\n",
      "Training Epoch: 7 [24500/36045]\tLoss: 1205.7010\n",
      "Training Epoch: 7 [24550/36045]\tLoss: 1303.2140\n",
      "Training Epoch: 7 [24600/36045]\tLoss: 1295.4423\n",
      "Training Epoch: 7 [24650/36045]\tLoss: 1257.4832\n",
      "Training Epoch: 7 [24700/36045]\tLoss: 1275.9883\n",
      "Training Epoch: 7 [24750/36045]\tLoss: 1172.6211\n",
      "Training Epoch: 7 [24800/36045]\tLoss: 1043.5387\n",
      "Training Epoch: 7 [24850/36045]\tLoss: 1064.4290\n",
      "Training Epoch: 7 [24900/36045]\tLoss: 1066.1877\n",
      "Training Epoch: 7 [24950/36045]\tLoss: 1067.3312\n",
      "Training Epoch: 7 [25000/36045]\tLoss: 1022.8438\n",
      "Training Epoch: 7 [25050/36045]\tLoss: 963.9230\n",
      "Training Epoch: 7 [25100/36045]\tLoss: 865.1537\n",
      "Training Epoch: 7 [25150/36045]\tLoss: 806.2259\n",
      "Training Epoch: 7 [25200/36045]\tLoss: 803.3486\n",
      "Training Epoch: 7 [25250/36045]\tLoss: 854.7731\n",
      "Training Epoch: 7 [25300/36045]\tLoss: 1109.4882\n",
      "Training Epoch: 7 [25350/36045]\tLoss: 1107.7115\n",
      "Training Epoch: 7 [25400/36045]\tLoss: 1033.0109\n",
      "Training Epoch: 7 [25450/36045]\tLoss: 1037.1218\n",
      "Training Epoch: 7 [25500/36045]\tLoss: 1127.5205\n",
      "Training Epoch: 7 [25550/36045]\tLoss: 1308.4689\n",
      "Training Epoch: 7 [25600/36045]\tLoss: 1312.5413\n",
      "Training Epoch: 7 [25650/36045]\tLoss: 1268.2485\n",
      "Training Epoch: 7 [25700/36045]\tLoss: 1310.1602\n",
      "Training Epoch: 7 [25750/36045]\tLoss: 1243.9094\n",
      "Training Epoch: 7 [25800/36045]\tLoss: 761.9908\n",
      "Training Epoch: 7 [25850/36045]\tLoss: 774.5233\n",
      "Training Epoch: 7 [25900/36045]\tLoss: 740.3226\n",
      "Training Epoch: 7 [25950/36045]\tLoss: 756.1023\n",
      "Training Epoch: 7 [26000/36045]\tLoss: 936.5989\n",
      "Training Epoch: 7 [26050/36045]\tLoss: 1273.2643\n",
      "Training Epoch: 7 [26100/36045]\tLoss: 1317.9281\n",
      "Training Epoch: 7 [26150/36045]\tLoss: 1303.5724\n",
      "Training Epoch: 7 [26200/36045]\tLoss: 1272.5864\n",
      "Training Epoch: 7 [26250/36045]\tLoss: 1308.0354\n",
      "Training Epoch: 7 [26300/36045]\tLoss: 1122.1510\n",
      "Training Epoch: 7 [26350/36045]\tLoss: 1127.1101\n",
      "Training Epoch: 7 [26400/36045]\tLoss: 1113.4669\n",
      "Training Epoch: 7 [26450/36045]\tLoss: 1030.1681\n",
      "Training Epoch: 7 [26500/36045]\tLoss: 1243.9658\n",
      "Training Epoch: 7 [26550/36045]\tLoss: 1276.2803\n",
      "Training Epoch: 7 [26600/36045]\tLoss: 1268.2059\n",
      "Training Epoch: 7 [26650/36045]\tLoss: 1293.7822\n",
      "Training Epoch: 7 [26700/36045]\tLoss: 1264.2136\n",
      "Training Epoch: 7 [26750/36045]\tLoss: 1185.3960\n",
      "Training Epoch: 7 [26800/36045]\tLoss: 868.1587\n",
      "Training Epoch: 7 [26850/36045]\tLoss: 734.1902\n",
      "Training Epoch: 7 [26900/36045]\tLoss: 742.6930\n",
      "Training Epoch: 7 [26950/36045]\tLoss: 807.4528\n",
      "Training Epoch: 7 [27000/36045]\tLoss: 1250.8254\n",
      "Training Epoch: 7 [27050/36045]\tLoss: 1321.4784\n",
      "Training Epoch: 7 [27100/36045]\tLoss: 1282.4897\n",
      "Training Epoch: 7 [27150/36045]\tLoss: 1337.4445\n",
      "Training Epoch: 7 [27200/36045]\tLoss: 1023.7544\n",
      "Training Epoch: 7 [27250/36045]\tLoss: 1017.7006\n",
      "Training Epoch: 7 [27300/36045]\tLoss: 980.1495\n",
      "Training Epoch: 7 [27350/36045]\tLoss: 996.8047\n",
      "Training Epoch: 7 [27400/36045]\tLoss: 986.7583\n",
      "Training Epoch: 7 [27450/36045]\tLoss: 1231.9969\n",
      "Training Epoch: 7 [27500/36045]\tLoss: 1316.6726\n",
      "Training Epoch: 7 [27550/36045]\tLoss: 1306.4092\n",
      "Training Epoch: 7 [27600/36045]\tLoss: 1309.5361\n",
      "Training Epoch: 7 [27650/36045]\tLoss: 1314.4343\n",
      "Training Epoch: 7 [27700/36045]\tLoss: 1349.0215\n",
      "Training Epoch: 7 [27750/36045]\tLoss: 1363.3770\n",
      "Training Epoch: 7 [27800/36045]\tLoss: 1345.4908\n",
      "Training Epoch: 7 [27850/36045]\tLoss: 1314.6088\n",
      "Training Epoch: 7 [27900/36045]\tLoss: 1157.6263\n",
      "Training Epoch: 7 [27950/36045]\tLoss: 952.7402\n",
      "Training Epoch: 7 [28000/36045]\tLoss: 916.8583\n",
      "Training Epoch: 7 [28050/36045]\tLoss: 949.5421\n",
      "Training Epoch: 7 [28100/36045]\tLoss: 931.9243\n",
      "Training Epoch: 7 [28150/36045]\tLoss: 1017.4885\n",
      "Training Epoch: 7 [28200/36045]\tLoss: 1028.1952\n",
      "Training Epoch: 7 [28250/36045]\tLoss: 1029.1796\n",
      "Training Epoch: 7 [28300/36045]\tLoss: 979.5430\n",
      "Training Epoch: 7 [28350/36045]\tLoss: 981.9932\n",
      "Training Epoch: 7 [28400/36045]\tLoss: 1371.4812\n",
      "Training Epoch: 7 [28450/36045]\tLoss: 1218.5796\n",
      "Training Epoch: 7 [28500/36045]\tLoss: 1043.2065\n",
      "Training Epoch: 7 [28550/36045]\tLoss: 950.9015\n",
      "Training Epoch: 7 [28600/36045]\tLoss: 1096.5627\n",
      "Training Epoch: 7 [28650/36045]\tLoss: 1336.2737\n",
      "Training Epoch: 7 [28700/36045]\tLoss: 1344.3102\n",
      "Training Epoch: 7 [28750/36045]\tLoss: 1319.8265\n",
      "Training Epoch: 7 [28800/36045]\tLoss: 1324.8167\n",
      "Training Epoch: 7 [28850/36045]\tLoss: 1131.5520\n",
      "Training Epoch: 7 [28900/36045]\tLoss: 878.1570\n",
      "Training Epoch: 7 [28950/36045]\tLoss: 872.9188\n",
      "Training Epoch: 7 [29000/36045]\tLoss: 883.5355\n",
      "Training Epoch: 7 [29050/36045]\tLoss: 896.1106\n",
      "Training Epoch: 7 [29100/36045]\tLoss: 920.7023\n",
      "Training Epoch: 7 [29150/36045]\tLoss: 894.4102\n",
      "Training Epoch: 7 [29200/36045]\tLoss: 877.3275\n",
      "Training Epoch: 7 [29250/36045]\tLoss: 851.1836\n",
      "Training Epoch: 7 [29300/36045]\tLoss: 1003.5654\n",
      "Training Epoch: 7 [29350/36045]\tLoss: 1221.9333\n",
      "Training Epoch: 7 [29400/36045]\tLoss: 1253.4672\n",
      "Training Epoch: 7 [29450/36045]\tLoss: 1307.5426\n",
      "Training Epoch: 7 [29500/36045]\tLoss: 1324.5355\n",
      "Training Epoch: 7 [29550/36045]\tLoss: 1256.3367\n",
      "Training Epoch: 7 [29600/36045]\tLoss: 1093.6307\n",
      "Training Epoch: 7 [29650/36045]\tLoss: 1066.0037\n",
      "Training Epoch: 7 [29700/36045]\tLoss: 940.9861\n",
      "Training Epoch: 7 [29750/36045]\tLoss: 954.4343\n",
      "Training Epoch: 7 [29800/36045]\tLoss: 1014.6338\n",
      "Training Epoch: 7 [29850/36045]\tLoss: 1073.0197\n",
      "Training Epoch: 7 [29900/36045]\tLoss: 1062.7338\n",
      "Training Epoch: 7 [29950/36045]\tLoss: 1078.9132\n",
      "Training Epoch: 7 [30000/36045]\tLoss: 1072.4957\n",
      "Training Epoch: 7 [30050/36045]\tLoss: 1082.6517\n",
      "Training Epoch: 7 [30100/36045]\tLoss: 1354.8131\n",
      "Training Epoch: 7 [30150/36045]\tLoss: 1356.3821\n",
      "Training Epoch: 7 [30200/36045]\tLoss: 1279.7031\n",
      "Training Epoch: 7 [30250/36045]\tLoss: 1335.4033\n",
      "Training Epoch: 7 [30300/36045]\tLoss: 1327.5839\n",
      "Training Epoch: 7 [30350/36045]\tLoss: 1087.8502\n",
      "Training Epoch: 7 [30400/36045]\tLoss: 1081.1058\n",
      "Training Epoch: 7 [30450/36045]\tLoss: 1084.0270\n",
      "Training Epoch: 7 [30500/36045]\tLoss: 1010.2020\n",
      "Training Epoch: 7 [30550/36045]\tLoss: 936.8881\n",
      "Training Epoch: 7 [30600/36045]\tLoss: 883.8671\n",
      "Training Epoch: 7 [30650/36045]\tLoss: 873.7471\n",
      "Training Epoch: 7 [30700/36045]\tLoss: 901.4749\n",
      "Training Epoch: 7 [30750/36045]\tLoss: 881.7194\n",
      "Training Epoch: 7 [30800/36045]\tLoss: 922.6222\n",
      "Training Epoch: 7 [30850/36045]\tLoss: 917.2911\n",
      "Training Epoch: 7 [30900/36045]\tLoss: 944.0871\n",
      "Training Epoch: 7 [30950/36045]\tLoss: 992.9485\n",
      "Training Epoch: 7 [31000/36045]\tLoss: 974.8662\n",
      "Training Epoch: 7 [31050/36045]\tLoss: 818.6521\n",
      "Training Epoch: 7 [31100/36045]\tLoss: 803.0897\n",
      "Training Epoch: 7 [31150/36045]\tLoss: 809.0206\n",
      "Training Epoch: 7 [31200/36045]\tLoss: 1019.8760\n",
      "Training Epoch: 7 [31250/36045]\tLoss: 1315.4243\n",
      "Training Epoch: 7 [31300/36045]\tLoss: 1258.9934\n",
      "Training Epoch: 7 [31350/36045]\tLoss: 1278.3888\n",
      "Training Epoch: 7 [31400/36045]\tLoss: 1265.8647\n",
      "Training Epoch: 7 [31450/36045]\tLoss: 1242.5692\n",
      "Training Epoch: 7 [31500/36045]\tLoss: 1241.4833\n",
      "Training Epoch: 7 [31550/36045]\tLoss: 1259.6616\n",
      "Training Epoch: 7 [31600/36045]\tLoss: 1182.5552\n",
      "Training Epoch: 7 [31650/36045]\tLoss: 1266.0714\n",
      "Training Epoch: 7 [31700/36045]\tLoss: 959.1898\n",
      "Training Epoch: 7 [31750/36045]\tLoss: 804.6514\n",
      "Training Epoch: 7 [31800/36045]\tLoss: 761.9105\n",
      "Training Epoch: 7 [31850/36045]\tLoss: 789.2947\n",
      "Training Epoch: 7 [31900/36045]\tLoss: 1168.5983\n",
      "Training Epoch: 7 [31950/36045]\tLoss: 1441.9097\n",
      "Training Epoch: 7 [32000/36045]\tLoss: 1598.1777\n",
      "Training Epoch: 7 [32050/36045]\tLoss: 1536.5624\n",
      "Training Epoch: 7 [32100/36045]\tLoss: 1512.6550\n",
      "Training Epoch: 7 [32150/36045]\tLoss: 1271.5549\n",
      "Training Epoch: 7 [32200/36045]\tLoss: 1286.4271\n",
      "Training Epoch: 7 [32250/36045]\tLoss: 1302.1621\n",
      "Training Epoch: 7 [32300/36045]\tLoss: 1278.2805\n",
      "Training Epoch: 7 [32350/36045]\tLoss: 1251.1014\n",
      "Training Epoch: 7 [32400/36045]\tLoss: 1186.1010\n",
      "Training Epoch: 7 [32450/36045]\tLoss: 1003.2295\n",
      "Training Epoch: 7 [32500/36045]\tLoss: 965.8776\n",
      "Training Epoch: 7 [32550/36045]\tLoss: 975.5896\n",
      "Training Epoch: 7 [32600/36045]\tLoss: 966.2080\n",
      "Training Epoch: 7 [32650/36045]\tLoss: 1183.0374\n",
      "Training Epoch: 7 [32700/36045]\tLoss: 1274.1803\n",
      "Training Epoch: 7 [32750/36045]\tLoss: 1224.8574\n",
      "Training Epoch: 7 [32800/36045]\tLoss: 1259.1643\n",
      "Training Epoch: 7 [32850/36045]\tLoss: 1171.8378\n",
      "Training Epoch: 7 [32900/36045]\tLoss: 945.1051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [32950/36045]\tLoss: 989.5769\n",
      "Training Epoch: 7 [33000/36045]\tLoss: 1005.1046\n",
      "Training Epoch: 7 [33050/36045]\tLoss: 941.8369\n",
      "Training Epoch: 7 [33100/36045]\tLoss: 1073.1860\n",
      "Training Epoch: 7 [33150/36045]\tLoss: 1406.7092\n",
      "Training Epoch: 7 [33200/36045]\tLoss: 1373.5010\n",
      "Training Epoch: 7 [33250/36045]\tLoss: 1408.3374\n",
      "Training Epoch: 7 [33300/36045]\tLoss: 1492.2867\n",
      "Training Epoch: 7 [33350/36045]\tLoss: 1175.1681\n",
      "Training Epoch: 7 [33400/36045]\tLoss: 907.8453\n",
      "Training Epoch: 7 [33450/36045]\tLoss: 900.9565\n",
      "Training Epoch: 7 [33500/36045]\tLoss: 921.2473\n",
      "Training Epoch: 7 [33550/36045]\tLoss: 951.1832\n",
      "Training Epoch: 7 [33600/36045]\tLoss: 961.0962\n",
      "Training Epoch: 7 [33650/36045]\tLoss: 1225.9850\n",
      "Training Epoch: 7 [33700/36045]\tLoss: 1184.3838\n",
      "Training Epoch: 7 [33750/36045]\tLoss: 1220.8772\n",
      "Training Epoch: 7 [33800/36045]\tLoss: 1212.1631\n",
      "Training Epoch: 7 [33850/36045]\tLoss: 1214.0582\n",
      "Training Epoch: 7 [33900/36045]\tLoss: 1246.2848\n",
      "Training Epoch: 7 [33950/36045]\tLoss: 1264.4210\n",
      "Training Epoch: 7 [34000/36045]\tLoss: 1250.3081\n",
      "Training Epoch: 7 [34050/36045]\tLoss: 1266.4229\n",
      "Training Epoch: 7 [34100/36045]\tLoss: 1205.6985\n",
      "Training Epoch: 7 [34150/36045]\tLoss: 1127.4185\n",
      "Training Epoch: 7 [34200/36045]\tLoss: 1076.9458\n",
      "Training Epoch: 7 [34250/36045]\tLoss: 1093.1111\n",
      "Training Epoch: 7 [34300/36045]\tLoss: 947.4176\n",
      "Training Epoch: 7 [34350/36045]\tLoss: 985.1312\n",
      "Training Epoch: 7 [34400/36045]\tLoss: 956.9927\n",
      "Training Epoch: 7 [34450/36045]\tLoss: 894.9389\n",
      "Training Epoch: 7 [34500/36045]\tLoss: 958.8022\n",
      "Training Epoch: 7 [34550/36045]\tLoss: 951.5737\n",
      "Training Epoch: 7 [34600/36045]\tLoss: 912.1168\n",
      "Training Epoch: 7 [34650/36045]\tLoss: 1057.9272\n",
      "Training Epoch: 7 [34700/36045]\tLoss: 1114.7059\n",
      "Training Epoch: 7 [34750/36045]\tLoss: 994.9301\n",
      "Training Epoch: 7 [34800/36045]\tLoss: 1116.5331\n",
      "Training Epoch: 7 [34850/36045]\tLoss: 1148.8070\n",
      "Training Epoch: 7 [34900/36045]\tLoss: 1386.5212\n",
      "Training Epoch: 7 [34950/36045]\tLoss: 1375.5599\n",
      "Training Epoch: 7 [35000/36045]\tLoss: 1389.5520\n",
      "Training Epoch: 7 [35050/36045]\tLoss: 1355.7388\n",
      "Training Epoch: 7 [35100/36045]\tLoss: 1055.5574\n",
      "Training Epoch: 7 [35150/36045]\tLoss: 1042.2102\n",
      "Training Epoch: 7 [35200/36045]\tLoss: 913.3503\n",
      "Training Epoch: 7 [35250/36045]\tLoss: 988.2774\n",
      "Training Epoch: 7 [35300/36045]\tLoss: 997.6271\n",
      "Training Epoch: 7 [35350/36045]\tLoss: 1189.1556\n",
      "Training Epoch: 7 [35400/36045]\tLoss: 1271.8864\n",
      "Training Epoch: 7 [35450/36045]\tLoss: 1220.9872\n",
      "Training Epoch: 7 [35500/36045]\tLoss: 1195.4064\n",
      "Training Epoch: 7 [35550/36045]\tLoss: 1180.4578\n",
      "Training Epoch: 7 [35600/36045]\tLoss: 1225.9105\n",
      "Training Epoch: 7 [35650/36045]\tLoss: 1327.6172\n",
      "Training Epoch: 7 [35700/36045]\tLoss: 1238.9620\n",
      "Training Epoch: 7 [35750/36045]\tLoss: 1317.8607\n",
      "Training Epoch: 7 [35800/36045]\tLoss: 1327.1964\n",
      "Training Epoch: 7 [35850/36045]\tLoss: 1298.2035\n",
      "Training Epoch: 7 [35900/36045]\tLoss: 1348.3368\n",
      "Training Epoch: 7 [35950/36045]\tLoss: 1345.9633\n",
      "Training Epoch: 7 [36000/36045]\tLoss: 1322.7421\n",
      "Training Epoch: 7 [36045/36045]\tLoss: 1305.2445\n",
      "Training Epoch: 7 [4004/4004]\tLoss: 1248.5021\n",
      "Training Epoch: 8 [50/36045]\tLoss: 1261.1849\n",
      "Training Epoch: 8 [100/36045]\tLoss: 1220.5845\n",
      "Training Epoch: 8 [150/36045]\tLoss: 1224.1550\n",
      "Training Epoch: 8 [200/36045]\tLoss: 1202.7877\n",
      "Training Epoch: 8 [250/36045]\tLoss: 1359.0122\n",
      "Training Epoch: 8 [300/36045]\tLoss: 1429.2069\n",
      "Training Epoch: 8 [350/36045]\tLoss: 1369.8021\n",
      "Training Epoch: 8 [400/36045]\tLoss: 1390.6606\n",
      "Training Epoch: 8 [450/36045]\tLoss: 1362.5446\n",
      "Training Epoch: 8 [500/36045]\tLoss: 1295.0903\n",
      "Training Epoch: 8 [550/36045]\tLoss: 1312.2590\n",
      "Training Epoch: 8 [600/36045]\tLoss: 1259.1776\n",
      "Training Epoch: 8 [650/36045]\tLoss: 1303.2775\n",
      "Training Epoch: 8 [700/36045]\tLoss: 1298.9148\n",
      "Training Epoch: 8 [750/36045]\tLoss: 1269.9614\n",
      "Training Epoch: 8 [800/36045]\tLoss: 1292.9666\n",
      "Training Epoch: 8 [850/36045]\tLoss: 1252.8031\n",
      "Training Epoch: 8 [900/36045]\tLoss: 1194.9431\n",
      "Training Epoch: 8 [950/36045]\tLoss: 1148.7306\n",
      "Training Epoch: 8 [1000/36045]\tLoss: 1110.2896\n",
      "Training Epoch: 8 [1050/36045]\tLoss: 1114.0544\n",
      "Training Epoch: 8 [1100/36045]\tLoss: 1086.2943\n",
      "Training Epoch: 8 [1150/36045]\tLoss: 1084.4280\n",
      "Training Epoch: 8 [1200/36045]\tLoss: 1144.1836\n",
      "Training Epoch: 8 [1250/36045]\tLoss: 1270.4501\n",
      "Training Epoch: 8 [1300/36045]\tLoss: 1262.3071\n",
      "Training Epoch: 8 [1350/36045]\tLoss: 1276.6531\n",
      "Training Epoch: 8 [1400/36045]\tLoss: 1320.9062\n",
      "Training Epoch: 8 [1450/36045]\tLoss: 1275.9858\n",
      "Training Epoch: 8 [1500/36045]\tLoss: 1195.6854\n",
      "Training Epoch: 8 [1550/36045]\tLoss: 1238.7690\n",
      "Training Epoch: 8 [1600/36045]\tLoss: 1244.8490\n",
      "Training Epoch: 8 [1650/36045]\tLoss: 1233.4423\n",
      "Training Epoch: 8 [1700/36045]\tLoss: 1248.0889\n",
      "Training Epoch: 8 [1750/36045]\tLoss: 1304.5720\n",
      "Training Epoch: 8 [1800/36045]\tLoss: 1287.2922\n",
      "Training Epoch: 8 [1850/36045]\tLoss: 1336.6746\n",
      "Training Epoch: 8 [1900/36045]\tLoss: 1260.4272\n",
      "Training Epoch: 8 [1950/36045]\tLoss: 1261.7299\n",
      "Training Epoch: 8 [2000/36045]\tLoss: 1139.3511\n",
      "Training Epoch: 8 [2050/36045]\tLoss: 1148.8336\n",
      "Training Epoch: 8 [2100/36045]\tLoss: 1212.2040\n",
      "Training Epoch: 8 [2150/36045]\tLoss: 1180.3328\n",
      "Training Epoch: 8 [2200/36045]\tLoss: 1097.0800\n",
      "Training Epoch: 8 [2250/36045]\tLoss: 1034.5745\n",
      "Training Epoch: 8 [2300/36045]\tLoss: 1079.6566\n",
      "Training Epoch: 8 [2350/36045]\tLoss: 1032.5209\n",
      "Training Epoch: 8 [2400/36045]\tLoss: 1068.1373\n",
      "Training Epoch: 8 [2450/36045]\tLoss: 1301.5265\n",
      "Training Epoch: 8 [2500/36045]\tLoss: 1355.0424\n",
      "Training Epoch: 8 [2550/36045]\tLoss: 1352.6704\n",
      "Training Epoch: 8 [2600/36045]\tLoss: 1351.2795\n",
      "Training Epoch: 8 [2650/36045]\tLoss: 1496.7318\n",
      "Training Epoch: 8 [2700/36045]\tLoss: 1594.4878\n",
      "Training Epoch: 8 [2750/36045]\tLoss: 1682.8721\n",
      "Training Epoch: 8 [2800/36045]\tLoss: 1699.5459\n",
      "Training Epoch: 8 [2850/36045]\tLoss: 1470.7881\n",
      "Training Epoch: 8 [2900/36045]\tLoss: 1469.2054\n",
      "Training Epoch: 8 [2950/36045]\tLoss: 1417.5543\n",
      "Training Epoch: 8 [3000/36045]\tLoss: 1410.9596\n",
      "Training Epoch: 8 [3050/36045]\tLoss: 1458.3453\n",
      "Training Epoch: 8 [3100/36045]\tLoss: 1325.5691\n",
      "Training Epoch: 8 [3150/36045]\tLoss: 1032.1249\n",
      "Training Epoch: 8 [3200/36045]\tLoss: 1073.8989\n",
      "Training Epoch: 8 [3250/36045]\tLoss: 1006.0688\n",
      "Training Epoch: 8 [3300/36045]\tLoss: 950.1533\n",
      "Training Epoch: 8 [3350/36045]\tLoss: 999.2493\n",
      "Training Epoch: 8 [3400/36045]\tLoss: 1050.1639\n",
      "Training Epoch: 8 [3450/36045]\tLoss: 1136.6873\n",
      "Training Epoch: 8 [3500/36045]\tLoss: 1105.7496\n",
      "Training Epoch: 8 [3550/36045]\tLoss: 1074.9288\n",
      "Training Epoch: 8 [3600/36045]\tLoss: 1143.7784\n",
      "Training Epoch: 8 [3650/36045]\tLoss: 1320.8870\n",
      "Training Epoch: 8 [3700/36045]\tLoss: 1325.3232\n",
      "Training Epoch: 8 [3750/36045]\tLoss: 1279.5016\n",
      "Training Epoch: 8 [3800/36045]\tLoss: 1259.1113\n",
      "Training Epoch: 8 [3850/36045]\tLoss: 1268.1088\n",
      "Training Epoch: 8 [3900/36045]\tLoss: 1277.9818\n",
      "Training Epoch: 8 [3950/36045]\tLoss: 1216.8304\n",
      "Training Epoch: 8 [4000/36045]\tLoss: 1253.0641\n",
      "Training Epoch: 8 [4050/36045]\tLoss: 1150.8489\n",
      "Training Epoch: 8 [4100/36045]\tLoss: 1116.3671\n",
      "Training Epoch: 8 [4150/36045]\tLoss: 1157.3970\n",
      "Training Epoch: 8 [4200/36045]\tLoss: 1142.6743\n",
      "Training Epoch: 8 [4250/36045]\tLoss: 1159.3546\n",
      "Training Epoch: 8 [4300/36045]\tLoss: 1192.9862\n",
      "Training Epoch: 8 [4350/36045]\tLoss: 1171.8749\n",
      "Training Epoch: 8 [4400/36045]\tLoss: 1118.7567\n",
      "Training Epoch: 8 [4450/36045]\tLoss: 1193.5269\n",
      "Training Epoch: 8 [4500/36045]\tLoss: 1258.6815\n",
      "Training Epoch: 8 [4550/36045]\tLoss: 1282.9873\n",
      "Training Epoch: 8 [4600/36045]\tLoss: 1299.8004\n",
      "Training Epoch: 8 [4650/36045]\tLoss: 1304.0999\n",
      "Training Epoch: 8 [4700/36045]\tLoss: 1204.8743\n",
      "Training Epoch: 8 [4750/36045]\tLoss: 1186.4669\n",
      "Training Epoch: 8 [4800/36045]\tLoss: 1233.9001\n",
      "Training Epoch: 8 [4850/36045]\tLoss: 1209.8204\n",
      "Training Epoch: 8 [4900/36045]\tLoss: 1167.4177\n",
      "Training Epoch: 8 [4950/36045]\tLoss: 1217.6224\n",
      "Training Epoch: 8 [5000/36045]\tLoss: 1287.0981\n",
      "Training Epoch: 8 [5050/36045]\tLoss: 1242.7600\n",
      "Training Epoch: 8 [5100/36045]\tLoss: 1272.9485\n",
      "Training Epoch: 8 [5150/36045]\tLoss: 1237.4603\n",
      "Training Epoch: 8 [5200/36045]\tLoss: 1220.4969\n",
      "Training Epoch: 8 [5250/36045]\tLoss: 1205.6881\n",
      "Training Epoch: 8 [5300/36045]\tLoss: 1208.0128\n",
      "Training Epoch: 8 [5350/36045]\tLoss: 1252.3687\n",
      "Training Epoch: 8 [5400/36045]\tLoss: 1192.1799\n",
      "Training Epoch: 8 [5450/36045]\tLoss: 1113.5514\n",
      "Training Epoch: 8 [5500/36045]\tLoss: 1175.9957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [5550/36045]\tLoss: 1143.1289\n",
      "Training Epoch: 8 [5600/36045]\tLoss: 1294.1333\n",
      "Training Epoch: 8 [5650/36045]\tLoss: 1239.8170\n",
      "Training Epoch: 8 [5700/36045]\tLoss: 1178.1199\n",
      "Training Epoch: 8 [5750/36045]\tLoss: 1159.4296\n",
      "Training Epoch: 8 [5800/36045]\tLoss: 1236.4651\n",
      "Training Epoch: 8 [5850/36045]\tLoss: 1196.6213\n",
      "Training Epoch: 8 [5900/36045]\tLoss: 1365.9918\n",
      "Training Epoch: 8 [5950/36045]\tLoss: 1406.3517\n",
      "Training Epoch: 8 [6000/36045]\tLoss: 1378.8912\n",
      "Training Epoch: 8 [6050/36045]\tLoss: 1329.9927\n",
      "Training Epoch: 8 [6100/36045]\tLoss: 1332.7749\n",
      "Training Epoch: 8 [6150/36045]\tLoss: 1288.1759\n",
      "Training Epoch: 8 [6200/36045]\tLoss: 1280.1814\n",
      "Training Epoch: 8 [6250/36045]\tLoss: 1290.9269\n",
      "Training Epoch: 8 [6300/36045]\tLoss: 1328.4939\n",
      "Training Epoch: 8 [6350/36045]\tLoss: 1386.2526\n",
      "Training Epoch: 8 [6400/36045]\tLoss: 1210.7188\n",
      "Training Epoch: 8 [6450/36045]\tLoss: 1125.9314\n",
      "Training Epoch: 8 [6500/36045]\tLoss: 1157.2815\n",
      "Training Epoch: 8 [6550/36045]\tLoss: 1179.5225\n",
      "Training Epoch: 8 [6600/36045]\tLoss: 1183.8806\n",
      "Training Epoch: 8 [6650/36045]\tLoss: 1321.0541\n",
      "Training Epoch: 8 [6700/36045]\tLoss: 1381.2174\n",
      "Training Epoch: 8 [6750/36045]\tLoss: 1346.8517\n",
      "Training Epoch: 8 [6800/36045]\tLoss: 1341.6039\n",
      "Training Epoch: 8 [6850/36045]\tLoss: 1323.4255\n",
      "Training Epoch: 8 [6900/36045]\tLoss: 1170.8923\n",
      "Training Epoch: 8 [6950/36045]\tLoss: 1104.4091\n",
      "Training Epoch: 8 [7000/36045]\tLoss: 1169.4144\n",
      "Training Epoch: 8 [7050/36045]\tLoss: 1210.3413\n",
      "Training Epoch: 8 [7100/36045]\tLoss: 1195.1566\n",
      "Training Epoch: 8 [7150/36045]\tLoss: 1219.7069\n",
      "Training Epoch: 8 [7200/36045]\tLoss: 1230.2302\n",
      "Training Epoch: 8 [7250/36045]\tLoss: 1230.0989\n",
      "Training Epoch: 8 [7300/36045]\tLoss: 1207.2759\n",
      "Training Epoch: 8 [7350/36045]\tLoss: 1203.2007\n",
      "Training Epoch: 8 [7400/36045]\tLoss: 1109.8217\n",
      "Training Epoch: 8 [7450/36045]\tLoss: 1108.2451\n",
      "Training Epoch: 8 [7500/36045]\tLoss: 1109.1982\n",
      "Training Epoch: 8 [7550/36045]\tLoss: 1057.0214\n",
      "Training Epoch: 8 [7600/36045]\tLoss: 1170.7756\n",
      "Training Epoch: 8 [7650/36045]\tLoss: 1249.7872\n",
      "Training Epoch: 8 [7700/36045]\tLoss: 1198.1947\n",
      "Training Epoch: 8 [7750/36045]\tLoss: 1217.1140\n",
      "Training Epoch: 8 [7800/36045]\tLoss: 1196.0938\n",
      "Training Epoch: 8 [7850/36045]\tLoss: 1129.5300\n",
      "Training Epoch: 8 [7900/36045]\tLoss: 1192.9491\n",
      "Training Epoch: 8 [7950/36045]\tLoss: 1189.8468\n",
      "Training Epoch: 8 [8000/36045]\tLoss: 1208.7983\n",
      "Training Epoch: 8 [8050/36045]\tLoss: 1153.5902\n",
      "Training Epoch: 8 [8100/36045]\tLoss: 1193.1478\n",
      "Training Epoch: 8 [8150/36045]\tLoss: 1344.0128\n",
      "Training Epoch: 8 [8200/36045]\tLoss: 1326.6039\n",
      "Training Epoch: 8 [8250/36045]\tLoss: 1279.4604\n",
      "Training Epoch: 8 [8300/36045]\tLoss: 1379.0692\n",
      "Training Epoch: 8 [8350/36045]\tLoss: 1280.4712\n",
      "Training Epoch: 8 [8400/36045]\tLoss: 1177.1621\n",
      "Training Epoch: 8 [8450/36045]\tLoss: 1115.3694\n",
      "Training Epoch: 8 [8500/36045]\tLoss: 1167.4171\n",
      "Training Epoch: 8 [8550/36045]\tLoss: 1141.5603\n",
      "Training Epoch: 8 [8600/36045]\tLoss: 1140.9741\n",
      "Training Epoch: 8 [8650/36045]\tLoss: 1194.0120\n",
      "Training Epoch: 8 [8700/36045]\tLoss: 1254.7839\n",
      "Training Epoch: 8 [8750/36045]\tLoss: 1229.4210\n",
      "Training Epoch: 8 [8800/36045]\tLoss: 1239.6663\n",
      "Training Epoch: 8 [8850/36045]\tLoss: 1223.9106\n",
      "Training Epoch: 8 [8900/36045]\tLoss: 1111.8352\n",
      "Training Epoch: 8 [8950/36045]\tLoss: 1147.2701\n",
      "Training Epoch: 8 [9000/36045]\tLoss: 1152.0952\n",
      "Training Epoch: 8 [9050/36045]\tLoss: 1147.6621\n",
      "Training Epoch: 8 [9100/36045]\tLoss: 1176.3578\n",
      "Training Epoch: 8 [9150/36045]\tLoss: 874.1425\n",
      "Training Epoch: 8 [9200/36045]\tLoss: 675.0019\n",
      "Training Epoch: 8 [9250/36045]\tLoss: 727.7268\n",
      "Training Epoch: 8 [9300/36045]\tLoss: 755.5172\n",
      "Training Epoch: 8 [9350/36045]\tLoss: 688.6058\n",
      "Training Epoch: 8 [9400/36045]\tLoss: 1340.5902\n",
      "Training Epoch: 8 [9450/36045]\tLoss: 1412.3064\n",
      "Training Epoch: 8 [9500/36045]\tLoss: 1393.7271\n",
      "Training Epoch: 8 [9550/36045]\tLoss: 1480.7036\n",
      "Training Epoch: 8 [9600/36045]\tLoss: 1087.9333\n",
      "Training Epoch: 8 [9650/36045]\tLoss: 1080.2847\n",
      "Training Epoch: 8 [9700/36045]\tLoss: 1072.0931\n",
      "Training Epoch: 8 [9750/36045]\tLoss: 1070.0067\n",
      "Training Epoch: 8 [9800/36045]\tLoss: 1384.1464\n",
      "Training Epoch: 8 [9850/36045]\tLoss: 1462.2411\n",
      "Training Epoch: 8 [9900/36045]\tLoss: 1499.1827\n",
      "Training Epoch: 8 [9950/36045]\tLoss: 1451.9044\n",
      "Training Epoch: 8 [10000/36045]\tLoss: 1345.1636\n",
      "Training Epoch: 8 [10050/36045]\tLoss: 1142.2881\n",
      "Training Epoch: 8 [10100/36045]\tLoss: 1139.1183\n",
      "Training Epoch: 8 [10150/36045]\tLoss: 1153.8677\n",
      "Training Epoch: 8 [10200/36045]\tLoss: 1151.8260\n",
      "Training Epoch: 8 [10250/36045]\tLoss: 1385.8601\n",
      "Training Epoch: 8 [10300/36045]\tLoss: 1348.9678\n",
      "Training Epoch: 8 [10350/36045]\tLoss: 1418.5420\n",
      "Training Epoch: 8 [10400/36045]\tLoss: 1408.9478\n",
      "Training Epoch: 8 [10450/36045]\tLoss: 1298.5978\n",
      "Training Epoch: 8 [10500/36045]\tLoss: 1098.8018\n",
      "Training Epoch: 8 [10550/36045]\tLoss: 1097.0992\n",
      "Training Epoch: 8 [10600/36045]\tLoss: 1132.1343\n",
      "Training Epoch: 8 [10650/36045]\tLoss: 1142.2745\n",
      "Training Epoch: 8 [10700/36045]\tLoss: 1247.5327\n",
      "Training Epoch: 8 [10750/36045]\tLoss: 1342.4126\n",
      "Training Epoch: 8 [10800/36045]\tLoss: 1255.8490\n",
      "Training Epoch: 8 [10850/36045]\tLoss: 1312.6796\n",
      "Training Epoch: 8 [10900/36045]\tLoss: 1366.0560\n",
      "Training Epoch: 8 [10950/36045]\tLoss: 1043.5327\n",
      "Training Epoch: 8 [11000/36045]\tLoss: 1038.9143\n",
      "Training Epoch: 8 [11050/36045]\tLoss: 1101.5686\n",
      "Training Epoch: 8 [11100/36045]\tLoss: 1118.2734\n",
      "Training Epoch: 8 [11150/36045]\tLoss: 1208.6802\n",
      "Training Epoch: 8 [11200/36045]\tLoss: 1249.6075\n",
      "Training Epoch: 8 [11250/36045]\tLoss: 1267.8070\n",
      "Training Epoch: 8 [11300/36045]\tLoss: 1244.9954\n",
      "Training Epoch: 8 [11350/36045]\tLoss: 1240.6084\n",
      "Training Epoch: 8 [11400/36045]\tLoss: 1183.4480\n",
      "Training Epoch: 8 [11450/36045]\tLoss: 1145.4381\n",
      "Training Epoch: 8 [11500/36045]\tLoss: 1144.7913\n",
      "Training Epoch: 8 [11550/36045]\tLoss: 1169.8956\n",
      "Training Epoch: 8 [11600/36045]\tLoss: 1256.6995\n",
      "Training Epoch: 8 [11650/36045]\tLoss: 1313.3400\n",
      "Training Epoch: 8 [11700/36045]\tLoss: 1311.2643\n",
      "Training Epoch: 8 [11750/36045]\tLoss: 1337.1688\n",
      "Training Epoch: 8 [11800/36045]\tLoss: 1392.0371\n",
      "Training Epoch: 8 [11850/36045]\tLoss: 1422.1736\n",
      "Training Epoch: 8 [11900/36045]\tLoss: 1664.5837\n",
      "Training Epoch: 8 [11950/36045]\tLoss: 1653.9056\n",
      "Training Epoch: 8 [12000/36045]\tLoss: 1685.8848\n",
      "Training Epoch: 8 [12050/36045]\tLoss: 1632.5807\n",
      "Training Epoch: 8 [12100/36045]\tLoss: 1191.7223\n",
      "Training Epoch: 8 [12150/36045]\tLoss: 993.1622\n",
      "Training Epoch: 8 [12200/36045]\tLoss: 982.0593\n",
      "Training Epoch: 8 [12250/36045]\tLoss: 997.7952\n",
      "Training Epoch: 8 [12300/36045]\tLoss: 1206.5309\n",
      "Training Epoch: 8 [12350/36045]\tLoss: 1277.6208\n",
      "Training Epoch: 8 [12400/36045]\tLoss: 1301.1384\n",
      "Training Epoch: 8 [12450/36045]\tLoss: 1279.4459\n",
      "Training Epoch: 8 [12500/36045]\tLoss: 1323.0992\n",
      "Training Epoch: 8 [12550/36045]\tLoss: 1267.9196\n",
      "Training Epoch: 8 [12600/36045]\tLoss: 1199.9124\n",
      "Training Epoch: 8 [12650/36045]\tLoss: 1197.2529\n",
      "Training Epoch: 8 [12700/36045]\tLoss: 1230.6193\n",
      "Training Epoch: 8 [12750/36045]\tLoss: 1232.1703\n",
      "Training Epoch: 8 [12800/36045]\tLoss: 1211.2843\n",
      "Training Epoch: 8 [12850/36045]\tLoss: 1261.4561\n",
      "Training Epoch: 8 [12900/36045]\tLoss: 1211.3632\n",
      "Training Epoch: 8 [12950/36045]\tLoss: 1202.3901\n",
      "Training Epoch: 8 [13000/36045]\tLoss: 1240.9174\n",
      "Training Epoch: 8 [13050/36045]\tLoss: 1155.2582\n",
      "Training Epoch: 8 [13100/36045]\tLoss: 1210.3241\n",
      "Training Epoch: 8 [13150/36045]\tLoss: 1211.9282\n",
      "Training Epoch: 8 [13200/36045]\tLoss: 1147.8097\n",
      "Training Epoch: 8 [13250/36045]\tLoss: 1207.2389\n",
      "Training Epoch: 8 [13300/36045]\tLoss: 1247.8881\n",
      "Training Epoch: 8 [13350/36045]\tLoss: 1217.0945\n",
      "Training Epoch: 8 [13400/36045]\tLoss: 1222.5663\n",
      "Training Epoch: 8 [13450/36045]\tLoss: 1207.1121\n",
      "Training Epoch: 8 [13500/36045]\tLoss: 1257.5312\n",
      "Training Epoch: 8 [13550/36045]\tLoss: 1389.0369\n",
      "Training Epoch: 8 [13600/36045]\tLoss: 1418.6189\n",
      "Training Epoch: 8 [13650/36045]\tLoss: 1509.5735\n",
      "Training Epoch: 8 [13700/36045]\tLoss: 1362.1306\n",
      "Training Epoch: 8 [13750/36045]\tLoss: 1241.6418\n",
      "Training Epoch: 8 [13800/36045]\tLoss: 1219.3901\n",
      "Training Epoch: 8 [13850/36045]\tLoss: 1202.8660\n",
      "Training Epoch: 8 [13900/36045]\tLoss: 1216.7925\n",
      "Training Epoch: 8 [13950/36045]\tLoss: 1235.9723\n",
      "Training Epoch: 8 [14000/36045]\tLoss: 1281.3572\n",
      "Training Epoch: 8 [14050/36045]\tLoss: 1231.9752\n",
      "Training Epoch: 8 [14100/36045]\tLoss: 1233.2080\n",
      "Training Epoch: 8 [14150/36045]\tLoss: 1204.0765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [14200/36045]\tLoss: 1288.9069\n",
      "Training Epoch: 8 [14250/36045]\tLoss: 1411.7321\n",
      "Training Epoch: 8 [14300/36045]\tLoss: 1413.4447\n",
      "Training Epoch: 8 [14350/36045]\tLoss: 1355.6722\n",
      "Training Epoch: 8 [14400/36045]\tLoss: 1340.4701\n",
      "Training Epoch: 8 [14450/36045]\tLoss: 1392.3630\n",
      "Training Epoch: 8 [14500/36045]\tLoss: 1305.4327\n",
      "Training Epoch: 8 [14550/36045]\tLoss: 1370.7701\n",
      "Training Epoch: 8 [14600/36045]\tLoss: 1333.8287\n",
      "Training Epoch: 8 [14650/36045]\tLoss: 1340.1687\n",
      "Training Epoch: 8 [14700/36045]\tLoss: 1258.7134\n",
      "Training Epoch: 8 [14750/36045]\tLoss: 1087.0741\n",
      "Training Epoch: 8 [14800/36045]\tLoss: 1068.1987\n",
      "Training Epoch: 8 [14850/36045]\tLoss: 1075.7921\n",
      "Training Epoch: 8 [14900/36045]\tLoss: 1067.9650\n",
      "Training Epoch: 8 [14950/36045]\tLoss: 1086.2769\n",
      "Training Epoch: 8 [15000/36045]\tLoss: 1121.8582\n",
      "Training Epoch: 8 [15050/36045]\tLoss: 1133.3812\n",
      "Training Epoch: 8 [15100/36045]\tLoss: 1104.1217\n",
      "Training Epoch: 8 [15150/36045]\tLoss: 1082.1959\n",
      "Training Epoch: 8 [15200/36045]\tLoss: 1000.9097\n",
      "Training Epoch: 8 [15250/36045]\tLoss: 1046.6135\n",
      "Training Epoch: 8 [15300/36045]\tLoss: 1017.5703\n",
      "Training Epoch: 8 [15350/36045]\tLoss: 1035.3248\n",
      "Training Epoch: 8 [15400/36045]\tLoss: 1040.8684\n",
      "Training Epoch: 8 [15450/36045]\tLoss: 1036.2430\n",
      "Training Epoch: 8 [15500/36045]\tLoss: 1064.3042\n",
      "Training Epoch: 8 [15550/36045]\tLoss: 1036.8934\n",
      "Training Epoch: 8 [15600/36045]\tLoss: 1123.3452\n",
      "Training Epoch: 8 [15650/36045]\tLoss: 1157.6477\n",
      "Training Epoch: 8 [15700/36045]\tLoss: 1136.4921\n",
      "Training Epoch: 8 [15750/36045]\tLoss: 1123.0280\n",
      "Training Epoch: 8 [15800/36045]\tLoss: 1020.2328\n",
      "Training Epoch: 8 [15850/36045]\tLoss: 1022.3649\n",
      "Training Epoch: 8 [15900/36045]\tLoss: 1025.4575\n",
      "Training Epoch: 8 [15950/36045]\tLoss: 1059.8165\n",
      "Training Epoch: 8 [16000/36045]\tLoss: 1063.7606\n",
      "Training Epoch: 8 [16050/36045]\tLoss: 1026.1552\n",
      "Training Epoch: 8 [16100/36045]\tLoss: 953.3627\n",
      "Training Epoch: 8 [16150/36045]\tLoss: 921.1235\n",
      "Training Epoch: 8 [16200/36045]\tLoss: 1087.2339\n",
      "Training Epoch: 8 [16250/36045]\tLoss: 1131.5635\n",
      "Training Epoch: 8 [16300/36045]\tLoss: 1227.3118\n",
      "Training Epoch: 8 [16350/36045]\tLoss: 1249.3546\n",
      "Training Epoch: 8 [16400/36045]\tLoss: 1219.0924\n",
      "Training Epoch: 8 [16450/36045]\tLoss: 1189.9811\n",
      "Training Epoch: 8 [16500/36045]\tLoss: 1187.7988\n",
      "Training Epoch: 8 [16550/36045]\tLoss: 1135.3730\n",
      "Training Epoch: 8 [16600/36045]\tLoss: 1194.6245\n",
      "Training Epoch: 8 [16650/36045]\tLoss: 1225.5964\n",
      "Training Epoch: 8 [16700/36045]\tLoss: 1203.7178\n",
      "Training Epoch: 8 [16750/36045]\tLoss: 1186.6565\n",
      "Training Epoch: 8 [16800/36045]\tLoss: 1207.5170\n",
      "Training Epoch: 8 [16850/36045]\tLoss: 1151.0209\n",
      "Training Epoch: 8 [16900/36045]\tLoss: 1166.3861\n",
      "Training Epoch: 8 [16950/36045]\tLoss: 1186.1586\n",
      "Training Epoch: 8 [17000/36045]\tLoss: 1157.3180\n",
      "Training Epoch: 8 [17050/36045]\tLoss: 1221.8577\n",
      "Training Epoch: 8 [17100/36045]\tLoss: 1228.6864\n",
      "Training Epoch: 8 [17150/36045]\tLoss: 1079.4020\n",
      "Training Epoch: 8 [17200/36045]\tLoss: 1028.6504\n",
      "Training Epoch: 8 [17250/36045]\tLoss: 1075.2441\n",
      "Training Epoch: 8 [17300/36045]\tLoss: 1138.1676\n",
      "Training Epoch: 8 [17350/36045]\tLoss: 1071.3240\n",
      "Training Epoch: 8 [17400/36045]\tLoss: 1079.6277\n",
      "Training Epoch: 8 [17450/36045]\tLoss: 1114.2874\n",
      "Training Epoch: 8 [17500/36045]\tLoss: 1096.8737\n",
      "Training Epoch: 8 [17550/36045]\tLoss: 1116.6091\n",
      "Training Epoch: 8 [17600/36045]\tLoss: 1102.7729\n",
      "Training Epoch: 8 [17650/36045]\tLoss: 1134.9077\n",
      "Training Epoch: 8 [17700/36045]\tLoss: 1104.2477\n",
      "Training Epoch: 8 [17750/36045]\tLoss: 1129.2784\n",
      "Training Epoch: 8 [17800/36045]\tLoss: 1122.5483\n",
      "Training Epoch: 8 [17850/36045]\tLoss: 1066.2550\n",
      "Training Epoch: 8 [17900/36045]\tLoss: 1103.9921\n",
      "Training Epoch: 8 [17950/36045]\tLoss: 1114.7743\n",
      "Training Epoch: 8 [18000/36045]\tLoss: 1099.6582\n",
      "Training Epoch: 8 [18050/36045]\tLoss: 1250.7062\n",
      "Training Epoch: 8 [18100/36045]\tLoss: 1264.2418\n",
      "Training Epoch: 8 [18150/36045]\tLoss: 1266.0909\n",
      "Training Epoch: 8 [18200/36045]\tLoss: 1255.6909\n",
      "Training Epoch: 8 [18250/36045]\tLoss: 1278.6637\n",
      "Training Epoch: 8 [18300/36045]\tLoss: 1170.4788\n",
      "Training Epoch: 8 [18350/36045]\tLoss: 1235.2651\n",
      "Training Epoch: 8 [18400/36045]\tLoss: 1213.9146\n",
      "Training Epoch: 8 [18450/36045]\tLoss: 1187.9504\n",
      "Training Epoch: 8 [18500/36045]\tLoss: 1183.4794\n",
      "Training Epoch: 8 [18550/36045]\tLoss: 1158.9825\n",
      "Training Epoch: 8 [18600/36045]\tLoss: 1146.8757\n",
      "Training Epoch: 8 [18650/36045]\tLoss: 1205.6235\n",
      "Training Epoch: 8 [18700/36045]\tLoss: 1271.6736\n",
      "Training Epoch: 8 [18750/36045]\tLoss: 1250.5673\n",
      "Training Epoch: 8 [18800/36045]\tLoss: 1291.4462\n",
      "Training Epoch: 8 [18850/36045]\tLoss: 1223.1244\n",
      "Training Epoch: 8 [18900/36045]\tLoss: 1306.0284\n",
      "Training Epoch: 8 [18950/36045]\tLoss: 1225.3291\n",
      "Training Epoch: 8 [19000/36045]\tLoss: 1081.9728\n",
      "Training Epoch: 8 [19050/36045]\tLoss: 1038.3756\n",
      "Training Epoch: 8 [19100/36045]\tLoss: 1067.3474\n",
      "Training Epoch: 8 [19150/36045]\tLoss: 1048.2510\n",
      "Training Epoch: 8 [19200/36045]\tLoss: 1089.4851\n",
      "Training Epoch: 8 [19250/36045]\tLoss: 1095.2529\n",
      "Training Epoch: 8 [19300/36045]\tLoss: 1130.2406\n",
      "Training Epoch: 8 [19350/36045]\tLoss: 1099.2477\n",
      "Training Epoch: 8 [19400/36045]\tLoss: 1130.0327\n",
      "Training Epoch: 8 [19450/36045]\tLoss: 1116.6752\n",
      "Training Epoch: 8 [19500/36045]\tLoss: 1120.4142\n",
      "Training Epoch: 8 [19550/36045]\tLoss: 1126.7765\n",
      "Training Epoch: 8 [19600/36045]\tLoss: 1175.2876\n",
      "Training Epoch: 8 [19650/36045]\tLoss: 1473.2854\n",
      "Training Epoch: 8 [19700/36045]\tLoss: 1417.6163\n",
      "Training Epoch: 8 [19750/36045]\tLoss: 1414.8428\n",
      "Training Epoch: 8 [19800/36045]\tLoss: 1407.3989\n",
      "Training Epoch: 8 [19850/36045]\tLoss: 1005.5115\n",
      "Training Epoch: 8 [19900/36045]\tLoss: 970.9663\n",
      "Training Epoch: 8 [19950/36045]\tLoss: 979.2162\n",
      "Training Epoch: 8 [20000/36045]\tLoss: 978.1425\n",
      "Training Epoch: 8 [20050/36045]\tLoss: 1094.3219\n",
      "Training Epoch: 8 [20100/36045]\tLoss: 1093.1719\n",
      "Training Epoch: 8 [20150/36045]\tLoss: 1102.5217\n",
      "Training Epoch: 8 [20200/36045]\tLoss: 1094.3175\n",
      "Training Epoch: 8 [20250/36045]\tLoss: 1159.2909\n",
      "Training Epoch: 8 [20300/36045]\tLoss: 1196.7783\n",
      "Training Epoch: 8 [20350/36045]\tLoss: 1235.1779\n",
      "Training Epoch: 8 [20400/36045]\tLoss: 1254.1898\n",
      "Training Epoch: 8 [20450/36045]\tLoss: 1230.3629\n",
      "Training Epoch: 8 [20500/36045]\tLoss: 1193.0189\n",
      "Training Epoch: 8 [20550/36045]\tLoss: 1085.8827\n",
      "Training Epoch: 8 [20600/36045]\tLoss: 1107.1687\n",
      "Training Epoch: 8 [20650/36045]\tLoss: 1096.1467\n",
      "Training Epoch: 8 [20700/36045]\tLoss: 1076.0704\n",
      "Training Epoch: 8 [20750/36045]\tLoss: 1142.6440\n",
      "Training Epoch: 8 [20800/36045]\tLoss: 1245.0850\n",
      "Training Epoch: 8 [20850/36045]\tLoss: 1233.6053\n",
      "Training Epoch: 8 [20900/36045]\tLoss: 1303.4960\n",
      "Training Epoch: 8 [20950/36045]\tLoss: 1233.6267\n",
      "Training Epoch: 8 [21000/36045]\tLoss: 1158.4661\n",
      "Training Epoch: 8 [21050/36045]\tLoss: 987.9974\n",
      "Training Epoch: 8 [21100/36045]\tLoss: 989.7429\n",
      "Training Epoch: 8 [21150/36045]\tLoss: 1056.1547\n",
      "Training Epoch: 8 [21200/36045]\tLoss: 1061.4313\n",
      "Training Epoch: 8 [21250/36045]\tLoss: 1009.0551\n",
      "Training Epoch: 8 [21300/36045]\tLoss: 1192.9044\n",
      "Training Epoch: 8 [21350/36045]\tLoss: 1186.4338\n",
      "Training Epoch: 8 [21400/36045]\tLoss: 1196.4109\n",
      "Training Epoch: 8 [21450/36045]\tLoss: 1217.8290\n",
      "Training Epoch: 8 [21500/36045]\tLoss: 1216.5345\n",
      "Training Epoch: 8 [21550/36045]\tLoss: 1324.3447\n",
      "Training Epoch: 8 [21600/36045]\tLoss: 1324.2502\n",
      "Training Epoch: 8 [21650/36045]\tLoss: 1342.2678\n",
      "Training Epoch: 8 [21700/36045]\tLoss: 1338.1948\n",
      "Training Epoch: 8 [21750/36045]\tLoss: 1297.3107\n",
      "Training Epoch: 8 [21800/36045]\tLoss: 993.6859\n",
      "Training Epoch: 8 [21850/36045]\tLoss: 967.0262\n",
      "Training Epoch: 8 [21900/36045]\tLoss: 985.5093\n",
      "Training Epoch: 8 [21950/36045]\tLoss: 974.4042\n",
      "Training Epoch: 8 [22000/36045]\tLoss: 987.6387\n",
      "Training Epoch: 8 [22050/36045]\tLoss: 1054.6340\n",
      "Training Epoch: 8 [22100/36045]\tLoss: 1039.1223\n",
      "Training Epoch: 8 [22150/36045]\tLoss: 1011.3068\n",
      "Training Epoch: 8 [22200/36045]\tLoss: 1041.0105\n",
      "Training Epoch: 8 [22250/36045]\tLoss: 1052.0884\n",
      "Training Epoch: 8 [22300/36045]\tLoss: 1120.3121\n",
      "Training Epoch: 8 [22350/36045]\tLoss: 1148.0460\n",
      "Training Epoch: 8 [22400/36045]\tLoss: 1180.7323\n",
      "Training Epoch: 8 [22450/36045]\tLoss: 1157.2816\n",
      "Training Epoch: 8 [22500/36045]\tLoss: 1126.6495\n",
      "Training Epoch: 8 [22550/36045]\tLoss: 1198.2053\n",
      "Training Epoch: 8 [22600/36045]\tLoss: 1320.1309\n",
      "Training Epoch: 8 [22650/36045]\tLoss: 1378.4113\n",
      "Training Epoch: 8 [22700/36045]\tLoss: 1408.4816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [22750/36045]\tLoss: 1436.9108\n",
      "Training Epoch: 8 [22800/36045]\tLoss: 1503.3444\n",
      "Training Epoch: 8 [22850/36045]\tLoss: 1251.5055\n",
      "Training Epoch: 8 [22900/36045]\tLoss: 1253.1748\n",
      "Training Epoch: 8 [22950/36045]\tLoss: 1219.4828\n",
      "Training Epoch: 8 [23000/36045]\tLoss: 1230.7540\n",
      "Training Epoch: 8 [23050/36045]\tLoss: 1105.8510\n",
      "Training Epoch: 8 [23100/36045]\tLoss: 1126.2679\n",
      "Training Epoch: 8 [23150/36045]\tLoss: 1104.7750\n",
      "Training Epoch: 8 [23200/36045]\tLoss: 1041.4135\n",
      "Training Epoch: 8 [23250/36045]\tLoss: 1047.1010\n",
      "Training Epoch: 8 [23300/36045]\tLoss: 1037.8130\n",
      "Training Epoch: 8 [23350/36045]\tLoss: 1067.2889\n",
      "Training Epoch: 8 [23400/36045]\tLoss: 1158.7513\n",
      "Training Epoch: 8 [23450/36045]\tLoss: 1141.8761\n",
      "Training Epoch: 8 [23500/36045]\tLoss: 1093.2708\n",
      "Training Epoch: 8 [23550/36045]\tLoss: 1184.5043\n",
      "Training Epoch: 8 [23600/36045]\tLoss: 1328.0424\n",
      "Training Epoch: 8 [23650/36045]\tLoss: 1363.5020\n",
      "Training Epoch: 8 [23700/36045]\tLoss: 1370.6901\n",
      "Training Epoch: 8 [23750/36045]\tLoss: 1331.3029\n",
      "Training Epoch: 8 [23800/36045]\tLoss: 1066.8824\n",
      "Training Epoch: 8 [23850/36045]\tLoss: 1107.9343\n",
      "Training Epoch: 8 [23900/36045]\tLoss: 1093.8325\n",
      "Training Epoch: 8 [23950/36045]\tLoss: 1075.9066\n",
      "Training Epoch: 8 [24000/36045]\tLoss: 1046.9375\n",
      "Training Epoch: 8 [24050/36045]\tLoss: 955.8134\n",
      "Training Epoch: 8 [24100/36045]\tLoss: 996.5181\n",
      "Training Epoch: 8 [24150/36045]\tLoss: 998.2283\n",
      "Training Epoch: 8 [24200/36045]\tLoss: 974.8495\n",
      "Training Epoch: 8 [24250/36045]\tLoss: 950.4180\n",
      "Training Epoch: 8 [24300/36045]\tLoss: 1039.7855\n",
      "Training Epoch: 8 [24350/36045]\tLoss: 1072.0327\n",
      "Training Epoch: 8 [24400/36045]\tLoss: 1095.1029\n",
      "Training Epoch: 8 [24450/36045]\tLoss: 1052.8431\n",
      "Training Epoch: 8 [24500/36045]\tLoss: 1109.2002\n",
      "Training Epoch: 8 [24550/36045]\tLoss: 1209.9370\n",
      "Training Epoch: 8 [24600/36045]\tLoss: 1203.2906\n",
      "Training Epoch: 8 [24650/36045]\tLoss: 1163.6779\n",
      "Training Epoch: 8 [24700/36045]\tLoss: 1179.2522\n",
      "Training Epoch: 8 [24750/36045]\tLoss: 1084.4828\n",
      "Training Epoch: 8 [24800/36045]\tLoss: 963.4897\n",
      "Training Epoch: 8 [24850/36045]\tLoss: 984.2095\n",
      "Training Epoch: 8 [24900/36045]\tLoss: 984.7120\n",
      "Training Epoch: 8 [24950/36045]\tLoss: 985.5771\n",
      "Training Epoch: 8 [25000/36045]\tLoss: 941.3808\n",
      "Training Epoch: 8 [25050/36045]\tLoss: 888.6096\n",
      "Training Epoch: 8 [25100/36045]\tLoss: 796.9368\n",
      "Training Epoch: 8 [25150/36045]\tLoss: 742.9639\n",
      "Training Epoch: 8 [25200/36045]\tLoss: 740.7806\n",
      "Training Epoch: 8 [25250/36045]\tLoss: 786.3411\n",
      "Training Epoch: 8 [25300/36045]\tLoss: 1017.6301\n",
      "Training Epoch: 8 [25350/36045]\tLoss: 1017.5419\n",
      "Training Epoch: 8 [25400/36045]\tLoss: 950.5138\n",
      "Training Epoch: 8 [25450/36045]\tLoss: 953.0835\n",
      "Training Epoch: 8 [25500/36045]\tLoss: 1037.0256\n",
      "Training Epoch: 8 [25550/36045]\tLoss: 1212.2908\n",
      "Training Epoch: 8 [25600/36045]\tLoss: 1218.7731\n",
      "Training Epoch: 8 [25650/36045]\tLoss: 1176.2328\n",
      "Training Epoch: 8 [25700/36045]\tLoss: 1211.5520\n",
      "Training Epoch: 8 [25750/36045]\tLoss: 1150.8389\n",
      "Training Epoch: 8 [25800/36045]\tLoss: 705.5844\n",
      "Training Epoch: 8 [25850/36045]\tLoss: 717.6276\n",
      "Training Epoch: 8 [25900/36045]\tLoss: 687.0068\n",
      "Training Epoch: 8 [25950/36045]\tLoss: 702.1078\n",
      "Training Epoch: 8 [26000/36045]\tLoss: 867.9485\n",
      "Training Epoch: 8 [26050/36045]\tLoss: 1178.8199\n",
      "Training Epoch: 8 [26100/36045]\tLoss: 1221.1665\n",
      "Training Epoch: 8 [26150/36045]\tLoss: 1208.9677\n",
      "Training Epoch: 8 [26200/36045]\tLoss: 1180.3596\n",
      "Training Epoch: 8 [26250/36045]\tLoss: 1215.0645\n",
      "Training Epoch: 8 [26300/36045]\tLoss: 1044.9622\n",
      "Training Epoch: 8 [26350/36045]\tLoss: 1049.1356\n",
      "Training Epoch: 8 [26400/36045]\tLoss: 1033.7385\n",
      "Training Epoch: 8 [26450/36045]\tLoss: 953.8031\n",
      "Training Epoch: 8 [26500/36045]\tLoss: 1152.2231\n",
      "Training Epoch: 8 [26550/36045]\tLoss: 1180.1427\n",
      "Training Epoch: 8 [26600/36045]\tLoss: 1171.8647\n",
      "Training Epoch: 8 [26650/36045]\tLoss: 1194.6895\n",
      "Training Epoch: 8 [26700/36045]\tLoss: 1167.1418\n",
      "Training Epoch: 8 [26750/36045]\tLoss: 1095.5405\n",
      "Training Epoch: 8 [26800/36045]\tLoss: 803.3154\n",
      "Training Epoch: 8 [26850/36045]\tLoss: 677.9282\n",
      "Training Epoch: 8 [26900/36045]\tLoss: 683.6797\n",
      "Training Epoch: 8 [26950/36045]\tLoss: 743.1296\n",
      "Training Epoch: 8 [27000/36045]\tLoss: 1157.4141\n",
      "Training Epoch: 8 [27050/36045]\tLoss: 1223.3625\n",
      "Training Epoch: 8 [27100/36045]\tLoss: 1189.0262\n",
      "Training Epoch: 8 [27150/36045]\tLoss: 1241.0304\n",
      "Training Epoch: 8 [27200/36045]\tLoss: 944.3385\n",
      "Training Epoch: 8 [27250/36045]\tLoss: 938.6895\n",
      "Training Epoch: 8 [27300/36045]\tLoss: 904.0658\n",
      "Training Epoch: 8 [27350/36045]\tLoss: 918.3207\n",
      "Training Epoch: 8 [27400/36045]\tLoss: 911.9103\n",
      "Training Epoch: 8 [27450/36045]\tLoss: 1142.5391\n",
      "Training Epoch: 8 [27500/36045]\tLoss: 1221.5669\n",
      "Training Epoch: 8 [27550/36045]\tLoss: 1210.6333\n",
      "Training Epoch: 8 [27600/36045]\tLoss: 1212.8972\n",
      "Training Epoch: 8 [27650/36045]\tLoss: 1217.1664\n",
      "Training Epoch: 8 [27700/36045]\tLoss: 1251.5968\n",
      "Training Epoch: 8 [27750/36045]\tLoss: 1267.8551\n",
      "Training Epoch: 8 [27800/36045]\tLoss: 1248.6886\n",
      "Training Epoch: 8 [27850/36045]\tLoss: 1219.9773\n",
      "Training Epoch: 8 [27900/36045]\tLoss: 1075.9805\n",
      "Training Epoch: 8 [27950/36045]\tLoss: 886.8352\n",
      "Training Epoch: 8 [28000/36045]\tLoss: 852.8215\n",
      "Training Epoch: 8 [28050/36045]\tLoss: 882.7909\n",
      "Training Epoch: 8 [28100/36045]\tLoss: 865.4592\n",
      "Training Epoch: 8 [28150/36045]\tLoss: 939.8791\n",
      "Training Epoch: 8 [28200/36045]\tLoss: 947.9573\n",
      "Training Epoch: 8 [28250/36045]\tLoss: 951.1517\n",
      "Training Epoch: 8 [28300/36045]\tLoss: 904.4254\n",
      "Training Epoch: 8 [28350/36045]\tLoss: 905.3768\n",
      "Training Epoch: 8 [28400/36045]\tLoss: 1276.2673\n",
      "Training Epoch: 8 [28450/36045]\tLoss: 1134.1085\n",
      "Training Epoch: 8 [28500/36045]\tLoss: 975.8832\n",
      "Training Epoch: 8 [28550/36045]\tLoss: 890.8293\n",
      "Training Epoch: 8 [28600/36045]\tLoss: 1021.4322\n",
      "Training Epoch: 8 [28650/36045]\tLoss: 1234.8624\n",
      "Training Epoch: 8 [28700/36045]\tLoss: 1236.8762\n",
      "Training Epoch: 8 [28750/36045]\tLoss: 1213.3154\n",
      "Training Epoch: 8 [28800/36045]\tLoss: 1219.9374\n",
      "Training Epoch: 8 [28850/36045]\tLoss: 1047.5586\n",
      "Training Epoch: 8 [28900/36045]\tLoss: 821.4432\n",
      "Training Epoch: 8 [28950/36045]\tLoss: 815.0800\n",
      "Training Epoch: 8 [29000/36045]\tLoss: 822.1324\n",
      "Training Epoch: 8 [29050/36045]\tLoss: 831.2584\n",
      "Training Epoch: 8 [29100/36045]\tLoss: 853.0443\n",
      "Training Epoch: 8 [29150/36045]\tLoss: 830.4839\n",
      "Training Epoch: 8 [29200/36045]\tLoss: 818.6080\n",
      "Training Epoch: 8 [29250/36045]\tLoss: 795.4736\n",
      "Training Epoch: 8 [29300/36045]\tLoss: 934.6539\n",
      "Training Epoch: 8 [29350/36045]\tLoss: 1130.9985\n",
      "Training Epoch: 8 [29400/36045]\tLoss: 1156.3148\n",
      "Training Epoch: 8 [29450/36045]\tLoss: 1207.2751\n",
      "Training Epoch: 8 [29500/36045]\tLoss: 1229.3546\n",
      "Training Epoch: 8 [29550/36045]\tLoss: 1167.3965\n",
      "Training Epoch: 8 [29600/36045]\tLoss: 1009.1648\n",
      "Training Epoch: 8 [29650/36045]\tLoss: 982.3271\n",
      "Training Epoch: 8 [29700/36045]\tLoss: 865.6823\n",
      "Training Epoch: 8 [29750/36045]\tLoss: 876.1788\n",
      "Training Epoch: 8 [29800/36045]\tLoss: 932.9431\n",
      "Training Epoch: 8 [29850/36045]\tLoss: 990.8045\n",
      "Training Epoch: 8 [29900/36045]\tLoss: 985.1017\n",
      "Training Epoch: 8 [29950/36045]\tLoss: 1005.6414\n",
      "Training Epoch: 8 [30000/36045]\tLoss: 998.3766\n",
      "Training Epoch: 8 [30050/36045]\tLoss: 1005.8835\n",
      "Training Epoch: 8 [30100/36045]\tLoss: 1246.9313\n",
      "Training Epoch: 8 [30150/36045]\tLoss: 1244.0723\n",
      "Training Epoch: 8 [30200/36045]\tLoss: 1174.7887\n",
      "Training Epoch: 8 [30250/36045]\tLoss: 1232.6110\n",
      "Training Epoch: 8 [30300/36045]\tLoss: 1228.1075\n",
      "Training Epoch: 8 [30350/36045]\tLoss: 1007.8030\n",
      "Training Epoch: 8 [30400/36045]\tLoss: 998.8430\n",
      "Training Epoch: 8 [30450/36045]\tLoss: 996.4916\n",
      "Training Epoch: 8 [30500/36045]\tLoss: 925.5063\n",
      "Training Epoch: 8 [30550/36045]\tLoss: 858.2194\n",
      "Training Epoch: 8 [30600/36045]\tLoss: 816.1760\n",
      "Training Epoch: 8 [30650/36045]\tLoss: 808.6198\n",
      "Training Epoch: 8 [30700/36045]\tLoss: 835.6075\n",
      "Training Epoch: 8 [30750/36045]\tLoss: 812.6664\n",
      "Training Epoch: 8 [30800/36045]\tLoss: 853.0154\n",
      "Training Epoch: 8 [30850/36045]\tLoss: 848.2028\n",
      "Training Epoch: 8 [30900/36045]\tLoss: 874.0833\n",
      "Training Epoch: 8 [30950/36045]\tLoss: 921.0795\n",
      "Training Epoch: 8 [31000/36045]\tLoss: 904.3103\n",
      "Training Epoch: 8 [31050/36045]\tLoss: 757.9447\n",
      "Training Epoch: 8 [31100/36045]\tLoss: 741.7209\n",
      "Training Epoch: 8 [31150/36045]\tLoss: 747.2297\n",
      "Training Epoch: 8 [31200/36045]\tLoss: 942.6182\n",
      "Training Epoch: 8 [31250/36045]\tLoss: 1218.7087\n",
      "Training Epoch: 8 [31300/36045]\tLoss: 1165.2222\n",
      "Training Epoch: 8 [31350/36045]\tLoss: 1182.6232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [31400/36045]\tLoss: 1169.7814\n",
      "Training Epoch: 8 [31450/36045]\tLoss: 1152.1957\n",
      "Training Epoch: 8 [31500/36045]\tLoss: 1152.5452\n",
      "Training Epoch: 8 [31550/36045]\tLoss: 1169.9237\n",
      "Training Epoch: 8 [31600/36045]\tLoss: 1096.6770\n",
      "Training Epoch: 8 [31650/36045]\tLoss: 1173.6106\n",
      "Training Epoch: 8 [31700/36045]\tLoss: 885.6770\n",
      "Training Epoch: 8 [31750/36045]\tLoss: 742.6776\n",
      "Training Epoch: 8 [31800/36045]\tLoss: 704.7488\n",
      "Training Epoch: 8 [31850/36045]\tLoss: 729.7914\n",
      "Training Epoch: 8 [31900/36045]\tLoss: 1083.0953\n",
      "Training Epoch: 8 [31950/36045]\tLoss: 1340.1404\n",
      "Training Epoch: 8 [32000/36045]\tLoss: 1490.5809\n",
      "Training Epoch: 8 [32050/36045]\tLoss: 1431.1755\n",
      "Training Epoch: 8 [32100/36045]\tLoss: 1409.1084\n",
      "Training Epoch: 8 [32150/36045]\tLoss: 1175.5627\n",
      "Training Epoch: 8 [32200/36045]\tLoss: 1189.0559\n",
      "Training Epoch: 8 [32250/36045]\tLoss: 1206.1951\n",
      "Training Epoch: 8 [32300/36045]\tLoss: 1184.0411\n",
      "Training Epoch: 8 [32350/36045]\tLoss: 1160.0208\n",
      "Training Epoch: 8 [32400/36045]\tLoss: 1098.4780\n",
      "Training Epoch: 8 [32450/36045]\tLoss: 924.2951\n",
      "Training Epoch: 8 [32500/36045]\tLoss: 888.2939\n",
      "Training Epoch: 8 [32550/36045]\tLoss: 897.9822\n",
      "Training Epoch: 8 [32600/36045]\tLoss: 890.4875\n",
      "Training Epoch: 8 [32650/36045]\tLoss: 1092.0994\n",
      "Training Epoch: 8 [32700/36045]\tLoss: 1176.7085\n",
      "Training Epoch: 8 [32750/36045]\tLoss: 1128.7036\n",
      "Training Epoch: 8 [32800/36045]\tLoss: 1159.3304\n",
      "Training Epoch: 8 [32850/36045]\tLoss: 1081.2689\n",
      "Training Epoch: 8 [32900/36045]\tLoss: 876.3751\n",
      "Training Epoch: 8 [32950/36045]\tLoss: 918.0402\n",
      "Training Epoch: 8 [33000/36045]\tLoss: 930.4980\n",
      "Training Epoch: 8 [33050/36045]\tLoss: 872.6016\n",
      "Training Epoch: 8 [33100/36045]\tLoss: 993.8426\n",
      "Training Epoch: 8 [33150/36045]\tLoss: 1306.4840\n",
      "Training Epoch: 8 [33200/36045]\tLoss: 1275.6086\n",
      "Training Epoch: 8 [33250/36045]\tLoss: 1309.7065\n",
      "Training Epoch: 8 [33300/36045]\tLoss: 1388.4269\n",
      "Training Epoch: 8 [33350/36045]\tLoss: 1088.6051\n",
      "Training Epoch: 8 [33400/36045]\tLoss: 834.5766\n",
      "Training Epoch: 8 [33450/36045]\tLoss: 828.1125\n",
      "Training Epoch: 8 [33500/36045]\tLoss: 847.4545\n",
      "Training Epoch: 8 [33550/36045]\tLoss: 877.5421\n",
      "Training Epoch: 8 [33600/36045]\tLoss: 887.6560\n",
      "Training Epoch: 8 [33650/36045]\tLoss: 1137.3201\n",
      "Training Epoch: 8 [33700/36045]\tLoss: 1097.5208\n",
      "Training Epoch: 8 [33750/36045]\tLoss: 1129.6005\n",
      "Training Epoch: 8 [33800/36045]\tLoss: 1124.6351\n",
      "Training Epoch: 8 [33850/36045]\tLoss: 1131.4802\n",
      "Training Epoch: 8 [33900/36045]\tLoss: 1160.8337\n",
      "Training Epoch: 8 [33950/36045]\tLoss: 1173.9203\n",
      "Training Epoch: 8 [34000/36045]\tLoss: 1157.5258\n",
      "Training Epoch: 8 [34050/36045]\tLoss: 1170.4662\n",
      "Training Epoch: 8 [34100/36045]\tLoss: 1116.5398\n",
      "Training Epoch: 8 [34150/36045]\tLoss: 1044.7024\n",
      "Training Epoch: 8 [34200/36045]\tLoss: 1000.5424\n",
      "Training Epoch: 8 [34250/36045]\tLoss: 1015.4636\n",
      "Training Epoch: 8 [34300/36045]\tLoss: 874.3191\n",
      "Training Epoch: 8 [34350/36045]\tLoss: 910.2189\n",
      "Training Epoch: 8 [34400/36045]\tLoss: 885.2951\n",
      "Training Epoch: 8 [34450/36045]\tLoss: 831.7156\n",
      "Training Epoch: 8 [34500/36045]\tLoss: 892.4482\n",
      "Training Epoch: 8 [34550/36045]\tLoss: 884.3068\n",
      "Training Epoch: 8 [34600/36045]\tLoss: 847.9416\n",
      "Training Epoch: 8 [34650/36045]\tLoss: 988.9071\n",
      "Training Epoch: 8 [34700/36045]\tLoss: 1042.4733\n",
      "Training Epoch: 8 [34750/36045]\tLoss: 929.0328\n",
      "Training Epoch: 8 [34800/36045]\tLoss: 1042.7922\n",
      "Training Epoch: 8 [34850/36045]\tLoss: 1070.2245\n",
      "Training Epoch: 8 [34900/36045]\tLoss: 1285.0308\n",
      "Training Epoch: 8 [34950/36045]\tLoss: 1275.4988\n",
      "Training Epoch: 8 [35000/36045]\tLoss: 1288.9371\n",
      "Training Epoch: 8 [35050/36045]\tLoss: 1257.2784\n",
      "Training Epoch: 8 [35100/36045]\tLoss: 979.4886\n",
      "Training Epoch: 8 [35150/36045]\tLoss: 966.6689\n",
      "Training Epoch: 8 [35200/36045]\tLoss: 845.8443\n",
      "Training Epoch: 8 [35250/36045]\tLoss: 917.0251\n",
      "Training Epoch: 8 [35300/36045]\tLoss: 926.3405\n",
      "Training Epoch: 8 [35350/36045]\tLoss: 1105.0350\n",
      "Training Epoch: 8 [35400/36045]\tLoss: 1182.4318\n",
      "Training Epoch: 8 [35450/36045]\tLoss: 1133.4440\n",
      "Training Epoch: 8 [35500/36045]\tLoss: 1108.3047\n",
      "Training Epoch: 8 [35550/36045]\tLoss: 1093.0991\n",
      "Training Epoch: 8 [35600/36045]\tLoss: 1135.1740\n",
      "Training Epoch: 8 [35650/36045]\tLoss: 1232.2949\n",
      "Training Epoch: 8 [35700/36045]\tLoss: 1148.7933\n",
      "Training Epoch: 8 [35750/36045]\tLoss: 1223.5817\n",
      "Training Epoch: 8 [35800/36045]\tLoss: 1231.4264\n",
      "Training Epoch: 8 [35850/36045]\tLoss: 1203.7385\n",
      "Training Epoch: 8 [35900/36045]\tLoss: 1253.8018\n",
      "Training Epoch: 8 [35950/36045]\tLoss: 1254.0770\n",
      "Training Epoch: 8 [36000/36045]\tLoss: 1232.7728\n",
      "Training Epoch: 8 [36045/36045]\tLoss: 1216.5532\n",
      "Training Epoch: 8 [4004/4004]\tLoss: 1157.4347\n",
      "Training Epoch: 9 [50/36045]\tLoss: 1168.4039\n",
      "Training Epoch: 9 [100/36045]\tLoss: 1127.8406\n",
      "Training Epoch: 9 [150/36045]\tLoss: 1130.3834\n",
      "Training Epoch: 9 [200/36045]\tLoss: 1111.1935\n",
      "Training Epoch: 9 [250/36045]\tLoss: 1263.9705\n",
      "Training Epoch: 9 [300/36045]\tLoss: 1330.9298\n",
      "Training Epoch: 9 [350/36045]\tLoss: 1274.7852\n",
      "Training Epoch: 9 [400/36045]\tLoss: 1292.5111\n",
      "Training Epoch: 9 [450/36045]\tLoss: 1267.0605\n",
      "Training Epoch: 9 [500/36045]\tLoss: 1204.2814\n",
      "Training Epoch: 9 [550/36045]\tLoss: 1221.9261\n",
      "Training Epoch: 9 [600/36045]\tLoss: 1172.4786\n",
      "Training Epoch: 9 [650/36045]\tLoss: 1211.9991\n",
      "Training Epoch: 9 [700/36045]\tLoss: 1206.5096\n",
      "Training Epoch: 9 [750/36045]\tLoss: 1175.4161\n",
      "Training Epoch: 9 [800/36045]\tLoss: 1197.4828\n",
      "Training Epoch: 9 [850/36045]\tLoss: 1158.0012\n",
      "Training Epoch: 9 [900/36045]\tLoss: 1106.0098\n",
      "Training Epoch: 9 [950/36045]\tLoss: 1061.3925\n",
      "Training Epoch: 9 [1000/36045]\tLoss: 1025.2120\n",
      "Training Epoch: 9 [1050/36045]\tLoss: 1030.0050\n",
      "Training Epoch: 9 [1100/36045]\tLoss: 1003.9650\n",
      "Training Epoch: 9 [1150/36045]\tLoss: 1002.6267\n",
      "Training Epoch: 9 [1200/36045]\tLoss: 1059.1631\n",
      "Training Epoch: 9 [1250/36045]\tLoss: 1181.3744\n",
      "Training Epoch: 9 [1300/36045]\tLoss: 1176.3547\n",
      "Training Epoch: 9 [1350/36045]\tLoss: 1188.7361\n",
      "Training Epoch: 9 [1400/36045]\tLoss: 1230.6508\n",
      "Training Epoch: 9 [1450/36045]\tLoss: 1187.5758\n",
      "Training Epoch: 9 [1500/36045]\tLoss: 1111.2307\n",
      "Training Epoch: 9 [1550/36045]\tLoss: 1149.8052\n",
      "Training Epoch: 9 [1600/36045]\tLoss: 1157.0142\n",
      "Training Epoch: 9 [1650/36045]\tLoss: 1143.1562\n",
      "Training Epoch: 9 [1700/36045]\tLoss: 1156.9628\n",
      "Training Epoch: 9 [1750/36045]\tLoss: 1212.8844\n",
      "Training Epoch: 9 [1800/36045]\tLoss: 1194.2722\n",
      "Training Epoch: 9 [1850/36045]\tLoss: 1240.0533\n",
      "Training Epoch: 9 [1900/36045]\tLoss: 1166.2372\n",
      "Training Epoch: 9 [1950/36045]\tLoss: 1170.6888\n",
      "Training Epoch: 9 [2000/36045]\tLoss: 1057.6160\n",
      "Training Epoch: 9 [2050/36045]\tLoss: 1066.9266\n",
      "Training Epoch: 9 [2100/36045]\tLoss: 1125.2617\n",
      "Training Epoch: 9 [2150/36045]\tLoss: 1093.9924\n",
      "Training Epoch: 9 [2200/36045]\tLoss: 1016.6418\n",
      "Training Epoch: 9 [2250/36045]\tLoss: 959.8594\n",
      "Training Epoch: 9 [2300/36045]\tLoss: 1002.7298\n",
      "Training Epoch: 9 [2350/36045]\tLoss: 957.6875\n",
      "Training Epoch: 9 [2400/36045]\tLoss: 988.5477\n",
      "Training Epoch: 9 [2450/36045]\tLoss: 1211.1122\n",
      "Training Epoch: 9 [2500/36045]\tLoss: 1261.6182\n",
      "Training Epoch: 9 [2550/36045]\tLoss: 1259.2955\n",
      "Training Epoch: 9 [2600/36045]\tLoss: 1261.4502\n",
      "Training Epoch: 9 [2650/36045]\tLoss: 1398.4261\n",
      "Training Epoch: 9 [2700/36045]\tLoss: 1492.7521\n",
      "Training Epoch: 9 [2750/36045]\tLoss: 1578.7565\n",
      "Training Epoch: 9 [2800/36045]\tLoss: 1595.9386\n",
      "Training Epoch: 9 [2850/36045]\tLoss: 1372.6110\n",
      "Training Epoch: 9 [2900/36045]\tLoss: 1366.0214\n",
      "Training Epoch: 9 [2950/36045]\tLoss: 1315.7644\n",
      "Training Epoch: 9 [3000/36045]\tLoss: 1310.8226\n",
      "Training Epoch: 9 [3050/36045]\tLoss: 1358.1245\n",
      "Training Epoch: 9 [3100/36045]\tLoss: 1234.6625\n",
      "Training Epoch: 9 [3150/36045]\tLoss: 960.2558\n",
      "Training Epoch: 9 [3200/36045]\tLoss: 998.5684\n",
      "Training Epoch: 9 [3250/36045]\tLoss: 934.8961\n",
      "Training Epoch: 9 [3300/36045]\tLoss: 882.9269\n",
      "Training Epoch: 9 [3350/36045]\tLoss: 929.0746\n",
      "Training Epoch: 9 [3400/36045]\tLoss: 976.2929\n",
      "Training Epoch: 9 [3450/36045]\tLoss: 1056.1788\n",
      "Training Epoch: 9 [3500/36045]\tLoss: 1026.0802\n",
      "Training Epoch: 9 [3550/36045]\tLoss: 994.8126\n",
      "Training Epoch: 9 [3600/36045]\tLoss: 1060.1169\n",
      "Training Epoch: 9 [3650/36045]\tLoss: 1229.3711\n",
      "Training Epoch: 9 [3700/36045]\tLoss: 1233.9634\n",
      "Training Epoch: 9 [3750/36045]\tLoss: 1189.1633\n",
      "Training Epoch: 9 [3800/36045]\tLoss: 1169.8218\n",
      "Training Epoch: 9 [3850/36045]\tLoss: 1178.6317\n",
      "Training Epoch: 9 [3900/36045]\tLoss: 1188.9825\n",
      "Training Epoch: 9 [3950/36045]\tLoss: 1134.0515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [4000/36045]\tLoss: 1167.0388\n",
      "Training Epoch: 9 [4050/36045]\tLoss: 1070.7933\n",
      "Training Epoch: 9 [4100/36045]\tLoss: 1038.0376\n",
      "Training Epoch: 9 [4150/36045]\tLoss: 1075.7151\n",
      "Training Epoch: 9 [4200/36045]\tLoss: 1062.0598\n",
      "Training Epoch: 9 [4250/36045]\tLoss: 1073.3126\n",
      "Training Epoch: 9 [4300/36045]\tLoss: 1103.9227\n",
      "Training Epoch: 9 [4350/36045]\tLoss: 1083.3051\n",
      "Training Epoch: 9 [4400/36045]\tLoss: 1034.5648\n",
      "Training Epoch: 9 [4450/36045]\tLoss: 1106.3126\n",
      "Training Epoch: 9 [4500/36045]\tLoss: 1169.5343\n",
      "Training Epoch: 9 [4550/36045]\tLoss: 1190.7643\n",
      "Training Epoch: 9 [4600/36045]\tLoss: 1208.9878\n",
      "Training Epoch: 9 [4650/36045]\tLoss: 1212.6855\n",
      "Training Epoch: 9 [4700/36045]\tLoss: 1118.1384\n",
      "Training Epoch: 9 [4750/36045]\tLoss: 1098.9316\n",
      "Training Epoch: 9 [4800/36045]\tLoss: 1143.0150\n",
      "Training Epoch: 9 [4850/36045]\tLoss: 1119.1187\n",
      "Training Epoch: 9 [4900/36045]\tLoss: 1081.9291\n",
      "Training Epoch: 9 [4950/36045]\tLoss: 1129.9832\n",
      "Training Epoch: 9 [5000/36045]\tLoss: 1193.8545\n",
      "Training Epoch: 9 [5050/36045]\tLoss: 1151.0654\n",
      "Training Epoch: 9 [5100/36045]\tLoss: 1177.4146\n",
      "Training Epoch: 9 [5150/36045]\tLoss: 1144.9817\n",
      "Training Epoch: 9 [5200/36045]\tLoss: 1131.1602\n",
      "Training Epoch: 9 [5250/36045]\tLoss: 1117.6516\n",
      "Training Epoch: 9 [5300/36045]\tLoss: 1119.3250\n",
      "Training Epoch: 9 [5350/36045]\tLoss: 1161.9463\n",
      "Training Epoch: 9 [5400/36045]\tLoss: 1105.4092\n",
      "Training Epoch: 9 [5450/36045]\tLoss: 1035.7408\n",
      "Training Epoch: 9 [5500/36045]\tLoss: 1093.8354\n",
      "Training Epoch: 9 [5550/36045]\tLoss: 1061.2594\n",
      "Training Epoch: 9 [5600/36045]\tLoss: 1206.0591\n",
      "Training Epoch: 9 [5650/36045]\tLoss: 1152.7604\n",
      "Training Epoch: 9 [5700/36045]\tLoss: 1094.2314\n",
      "Training Epoch: 9 [5750/36045]\tLoss: 1075.1458\n",
      "Training Epoch: 9 [5800/36045]\tLoss: 1146.5867\n",
      "Training Epoch: 9 [5850/36045]\tLoss: 1109.7214\n",
      "Training Epoch: 9 [5900/36045]\tLoss: 1267.2235\n",
      "Training Epoch: 9 [5950/36045]\tLoss: 1305.6740\n",
      "Training Epoch: 9 [6000/36045]\tLoss: 1279.9076\n",
      "Training Epoch: 9 [6050/36045]\tLoss: 1234.9659\n",
      "Training Epoch: 9 [6100/36045]\tLoss: 1238.5455\n",
      "Training Epoch: 9 [6150/36045]\tLoss: 1194.8944\n",
      "Training Epoch: 9 [6200/36045]\tLoss: 1188.1636\n",
      "Training Epoch: 9 [6250/36045]\tLoss: 1199.2837\n",
      "Training Epoch: 9 [6300/36045]\tLoss: 1232.5010\n",
      "Training Epoch: 9 [6350/36045]\tLoss: 1289.5909\n",
      "Training Epoch: 9 [6400/36045]\tLoss: 1121.9657\n",
      "Training Epoch: 9 [6450/36045]\tLoss: 1043.3812\n",
      "Training Epoch: 9 [6500/36045]\tLoss: 1072.9088\n",
      "Training Epoch: 9 [6550/36045]\tLoss: 1093.5264\n",
      "Training Epoch: 9 [6600/36045]\tLoss: 1099.0977\n",
      "Training Epoch: 9 [6650/36045]\tLoss: 1229.9430\n",
      "Training Epoch: 9 [6700/36045]\tLoss: 1285.4690\n",
      "Training Epoch: 9 [6750/36045]\tLoss: 1251.7875\n",
      "Training Epoch: 9 [6800/36045]\tLoss: 1248.9139\n",
      "Training Epoch: 9 [6850/36045]\tLoss: 1232.4957\n",
      "Training Epoch: 9 [6900/36045]\tLoss: 1090.0588\n",
      "Training Epoch: 9 [6950/36045]\tLoss: 1027.1902\n",
      "Training Epoch: 9 [7000/36045]\tLoss: 1088.1896\n",
      "Training Epoch: 9 [7050/36045]\tLoss: 1123.3381\n",
      "Training Epoch: 9 [7100/36045]\tLoss: 1111.5912\n",
      "Training Epoch: 9 [7150/36045]\tLoss: 1135.7775\n",
      "Training Epoch: 9 [7200/36045]\tLoss: 1146.2657\n",
      "Training Epoch: 9 [7250/36045]\tLoss: 1144.6028\n",
      "Training Epoch: 9 [7300/36045]\tLoss: 1122.3021\n",
      "Training Epoch: 9 [7350/36045]\tLoss: 1118.2491\n",
      "Training Epoch: 9 [7400/36045]\tLoss: 1026.6149\n",
      "Training Epoch: 9 [7450/36045]\tLoss: 1025.1034\n",
      "Training Epoch: 9 [7500/36045]\tLoss: 1026.3247\n",
      "Training Epoch: 9 [7550/36045]\tLoss: 976.5227\n",
      "Training Epoch: 9 [7600/36045]\tLoss: 1086.5879\n",
      "Training Epoch: 9 [7650/36045]\tLoss: 1163.2766\n",
      "Training Epoch: 9 [7700/36045]\tLoss: 1115.0450\n",
      "Training Epoch: 9 [7750/36045]\tLoss: 1133.1106\n",
      "Training Epoch: 9 [7800/36045]\tLoss: 1112.7339\n",
      "Training Epoch: 9 [7850/36045]\tLoss: 1051.4209\n",
      "Training Epoch: 9 [7900/36045]\tLoss: 1110.3961\n",
      "Training Epoch: 9 [7950/36045]\tLoss: 1107.2181\n",
      "Training Epoch: 9 [8000/36045]\tLoss: 1127.2463\n",
      "Training Epoch: 9 [8050/36045]\tLoss: 1074.1458\n",
      "Training Epoch: 9 [8100/36045]\tLoss: 1111.4335\n",
      "Training Epoch: 9 [8150/36045]\tLoss: 1252.8563\n",
      "Training Epoch: 9 [8200/36045]\tLoss: 1236.0215\n",
      "Training Epoch: 9 [8250/36045]\tLoss: 1191.0686\n",
      "Training Epoch: 9 [8300/36045]\tLoss: 1285.9319\n",
      "Training Epoch: 9 [8350/36045]\tLoss: 1192.4298\n",
      "Training Epoch: 9 [8400/36045]\tLoss: 1092.4429\n",
      "Training Epoch: 9 [8450/36045]\tLoss: 1032.3237\n",
      "Training Epoch: 9 [8500/36045]\tLoss: 1082.9457\n",
      "Training Epoch: 9 [8550/36045]\tLoss: 1058.6370\n",
      "Training Epoch: 9 [8600/36045]\tLoss: 1057.1196\n",
      "Training Epoch: 9 [8650/36045]\tLoss: 1110.2837\n",
      "Training Epoch: 9 [8700/36045]\tLoss: 1167.6625\n",
      "Training Epoch: 9 [8750/36045]\tLoss: 1142.8328\n",
      "Training Epoch: 9 [8800/36045]\tLoss: 1152.4464\n",
      "Training Epoch: 9 [8850/36045]\tLoss: 1138.1215\n",
      "Training Epoch: 9 [8900/36045]\tLoss: 1033.7219\n",
      "Training Epoch: 9 [8950/36045]\tLoss: 1066.6686\n",
      "Training Epoch: 9 [9000/36045]\tLoss: 1071.9802\n",
      "Training Epoch: 9 [9050/36045]\tLoss: 1068.8708\n",
      "Training Epoch: 9 [9100/36045]\tLoss: 1096.2261\n",
      "Training Epoch: 9 [9150/36045]\tLoss: 813.6150\n",
      "Training Epoch: 9 [9200/36045]\tLoss: 627.4806\n",
      "Training Epoch: 9 [9250/36045]\tLoss: 676.1221\n",
      "Training Epoch: 9 [9300/36045]\tLoss: 702.1743\n",
      "Training Epoch: 9 [9350/36045]\tLoss: 639.9634\n",
      "Training Epoch: 9 [9400/36045]\tLoss: 1248.0292\n",
      "Training Epoch: 9 [9450/36045]\tLoss: 1317.1542\n",
      "Training Epoch: 9 [9500/36045]\tLoss: 1299.2362\n",
      "Training Epoch: 9 [9550/36045]\tLoss: 1380.7177\n",
      "Training Epoch: 9 [9600/36045]\tLoss: 1014.6412\n",
      "Training Epoch: 9 [9650/36045]\tLoss: 1008.2089\n",
      "Training Epoch: 9 [9700/36045]\tLoss: 998.1757\n",
      "Training Epoch: 9 [9750/36045]\tLoss: 998.0380\n",
      "Training Epoch: 9 [9800/36045]\tLoss: 1291.3014\n",
      "Training Epoch: 9 [9850/36045]\tLoss: 1365.1810\n",
      "Training Epoch: 9 [9900/36045]\tLoss: 1399.4845\n",
      "Training Epoch: 9 [9950/36045]\tLoss: 1355.3840\n",
      "Training Epoch: 9 [10000/36045]\tLoss: 1252.7854\n",
      "Training Epoch: 9 [10050/36045]\tLoss: 1059.9009\n",
      "Training Epoch: 9 [10100/36045]\tLoss: 1058.7153\n",
      "Training Epoch: 9 [10150/36045]\tLoss: 1073.9130\n",
      "Training Epoch: 9 [10200/36045]\tLoss: 1068.5133\n",
      "Training Epoch: 9 [10250/36045]\tLoss: 1286.4197\n",
      "Training Epoch: 9 [10300/36045]\tLoss: 1250.8782\n",
      "Training Epoch: 9 [10350/36045]\tLoss: 1316.5370\n",
      "Training Epoch: 9 [10400/36045]\tLoss: 1306.6698\n",
      "Training Epoch: 9 [10450/36045]\tLoss: 1206.1412\n",
      "Training Epoch: 9 [10500/36045]\tLoss: 1019.7470\n",
      "Training Epoch: 9 [10550/36045]\tLoss: 1018.1552\n",
      "Training Epoch: 9 [10600/36045]\tLoss: 1050.3287\n",
      "Training Epoch: 9 [10650/36045]\tLoss: 1060.2035\n",
      "Training Epoch: 9 [10700/36045]\tLoss: 1163.6138\n",
      "Training Epoch: 9 [10750/36045]\tLoss: 1253.6781\n",
      "Training Epoch: 9 [10800/36045]\tLoss: 1169.9243\n",
      "Training Epoch: 9 [10850/36045]\tLoss: 1226.5786\n",
      "Training Epoch: 9 [10900/36045]\tLoss: 1277.8761\n",
      "Training Epoch: 9 [10950/36045]\tLoss: 972.3480\n",
      "Training Epoch: 9 [11000/36045]\tLoss: 965.4281\n",
      "Training Epoch: 9 [11050/36045]\tLoss: 1024.0714\n",
      "Training Epoch: 9 [11100/36045]\tLoss: 1041.5889\n",
      "Training Epoch: 9 [11150/36045]\tLoss: 1127.1798\n",
      "Training Epoch: 9 [11200/36045]\tLoss: 1164.6399\n",
      "Training Epoch: 9 [11250/36045]\tLoss: 1181.7823\n",
      "Training Epoch: 9 [11300/36045]\tLoss: 1159.2964\n",
      "Training Epoch: 9 [11350/36045]\tLoss: 1153.3170\n",
      "Training Epoch: 9 [11400/36045]\tLoss: 1099.8276\n",
      "Training Epoch: 9 [11450/36045]\tLoss: 1063.3771\n",
      "Training Epoch: 9 [11500/36045]\tLoss: 1062.6351\n",
      "Training Epoch: 9 [11550/36045]\tLoss: 1085.6663\n",
      "Training Epoch: 9 [11600/36045]\tLoss: 1168.3807\n",
      "Training Epoch: 9 [11650/36045]\tLoss: 1224.2319\n",
      "Training Epoch: 9 [11700/36045]\tLoss: 1222.2533\n",
      "Training Epoch: 9 [11750/36045]\tLoss: 1248.0874\n",
      "Training Epoch: 9 [11800/36045]\tLoss: 1300.0883\n",
      "Training Epoch: 9 [11850/36045]\tLoss: 1332.1836\n",
      "Training Epoch: 9 [11900/36045]\tLoss: 1572.4623\n",
      "Training Epoch: 9 [11950/36045]\tLoss: 1562.5143\n",
      "Training Epoch: 9 [12000/36045]\tLoss: 1593.3855\n",
      "Training Epoch: 9 [12050/36045]\tLoss: 1542.3197\n",
      "Training Epoch: 9 [12100/36045]\tLoss: 1112.0164\n",
      "Training Epoch: 9 [12150/36045]\tLoss: 918.5912\n",
      "Training Epoch: 9 [12200/36045]\tLoss: 909.9457\n",
      "Training Epoch: 9 [12250/36045]\tLoss: 925.6780\n",
      "Training Epoch: 9 [12300/36045]\tLoss: 1125.7554\n",
      "Training Epoch: 9 [12350/36045]\tLoss: 1192.2848\n",
      "Training Epoch: 9 [12400/36045]\tLoss: 1212.2922\n",
      "Training Epoch: 9 [12450/36045]\tLoss: 1193.6373\n",
      "Training Epoch: 9 [12500/36045]\tLoss: 1237.0334\n",
      "Training Epoch: 9 [12550/36045]\tLoss: 1187.8007\n",
      "Training Epoch: 9 [12600/36045]\tLoss: 1121.9742\n",
      "Training Epoch: 9 [12650/36045]\tLoss: 1116.1255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [12700/36045]\tLoss: 1147.2643\n",
      "Training Epoch: 9 [12750/36045]\tLoss: 1146.0820\n",
      "Training Epoch: 9 [12800/36045]\tLoss: 1126.2596\n",
      "Training Epoch: 9 [12850/36045]\tLoss: 1173.0936\n",
      "Training Epoch: 9 [12900/36045]\tLoss: 1125.6520\n",
      "Training Epoch: 9 [12950/36045]\tLoss: 1117.2002\n",
      "Training Epoch: 9 [13000/36045]\tLoss: 1152.0640\n",
      "Training Epoch: 9 [13050/36045]\tLoss: 1071.9091\n",
      "Training Epoch: 9 [13100/36045]\tLoss: 1123.8832\n",
      "Training Epoch: 9 [13150/36045]\tLoss: 1125.0361\n",
      "Training Epoch: 9 [13200/36045]\tLoss: 1067.4382\n",
      "Training Epoch: 9 [13250/36045]\tLoss: 1122.2430\n",
      "Training Epoch: 9 [13300/36045]\tLoss: 1162.1046\n",
      "Training Epoch: 9 [13350/36045]\tLoss: 1131.8533\n",
      "Training Epoch: 9 [13400/36045]\tLoss: 1138.0382\n",
      "Training Epoch: 9 [13450/36045]\tLoss: 1122.5200\n",
      "Training Epoch: 9 [13500/36045]\tLoss: 1169.1379\n",
      "Training Epoch: 9 [13550/36045]\tLoss: 1300.0237\n",
      "Training Epoch: 9 [13600/36045]\tLoss: 1329.9514\n",
      "Training Epoch: 9 [13650/36045]\tLoss: 1416.8911\n",
      "Training Epoch: 9 [13700/36045]\tLoss: 1276.3514\n",
      "Training Epoch: 9 [13750/36045]\tLoss: 1157.4258\n",
      "Training Epoch: 9 [13800/36045]\tLoss: 1133.6835\n",
      "Training Epoch: 9 [13850/36045]\tLoss: 1113.6772\n",
      "Training Epoch: 9 [13900/36045]\tLoss: 1127.5494\n",
      "Training Epoch: 9 [13950/36045]\tLoss: 1153.8276\n",
      "Training Epoch: 9 [14000/36045]\tLoss: 1199.6617\n",
      "Training Epoch: 9 [14050/36045]\tLoss: 1151.4750\n",
      "Training Epoch: 9 [14100/36045]\tLoss: 1150.0482\n",
      "Training Epoch: 9 [14150/36045]\tLoss: 1124.1208\n",
      "Training Epoch: 9 [14200/36045]\tLoss: 1205.1989\n",
      "Training Epoch: 9 [14250/36045]\tLoss: 1320.2217\n",
      "Training Epoch: 9 [14300/36045]\tLoss: 1322.3719\n",
      "Training Epoch: 9 [14350/36045]\tLoss: 1266.1188\n",
      "Training Epoch: 9 [14400/36045]\tLoss: 1250.1317\n",
      "Training Epoch: 9 [14450/36045]\tLoss: 1300.6807\n",
      "Training Epoch: 9 [14500/36045]\tLoss: 1212.9414\n",
      "Training Epoch: 9 [14550/36045]\tLoss: 1274.4102\n",
      "Training Epoch: 9 [14600/36045]\tLoss: 1240.3176\n",
      "Training Epoch: 9 [14650/36045]\tLoss: 1246.4956\n",
      "Training Epoch: 9 [14700/36045]\tLoss: 1170.2460\n",
      "Training Epoch: 9 [14750/36045]\tLoss: 1008.8326\n",
      "Training Epoch: 9 [14800/36045]\tLoss: 992.0899\n",
      "Training Epoch: 9 [14850/36045]\tLoss: 1000.1820\n",
      "Training Epoch: 9 [14900/36045]\tLoss: 993.4891\n",
      "Training Epoch: 9 [14950/36045]\tLoss: 1006.3918\n",
      "Training Epoch: 9 [15000/36045]\tLoss: 1035.3239\n",
      "Training Epoch: 9 [15050/36045]\tLoss: 1044.3656\n",
      "Training Epoch: 9 [15100/36045]\tLoss: 1019.0797\n",
      "Training Epoch: 9 [15150/36045]\tLoss: 1007.3804\n",
      "Training Epoch: 9 [15200/36045]\tLoss: 932.7291\n",
      "Training Epoch: 9 [15250/36045]\tLoss: 972.4623\n",
      "Training Epoch: 9 [15300/36045]\tLoss: 943.8884\n",
      "Training Epoch: 9 [15350/36045]\tLoss: 962.7576\n",
      "Training Epoch: 9 [15400/36045]\tLoss: 965.0138\n",
      "Training Epoch: 9 [15450/36045]\tLoss: 957.1802\n",
      "Training Epoch: 9 [15500/36045]\tLoss: 979.5777\n",
      "Training Epoch: 9 [15550/36045]\tLoss: 953.6299\n",
      "Training Epoch: 9 [15600/36045]\tLoss: 1045.3429\n",
      "Training Epoch: 9 [15650/36045]\tLoss: 1079.9630\n",
      "Training Epoch: 9 [15700/36045]\tLoss: 1060.2468\n",
      "Training Epoch: 9 [15750/36045]\tLoss: 1043.6127\n",
      "Training Epoch: 9 [15800/36045]\tLoss: 946.5971\n",
      "Training Epoch: 9 [15850/36045]\tLoss: 950.2453\n",
      "Training Epoch: 9 [15900/36045]\tLoss: 960.5283\n",
      "Training Epoch: 9 [15950/36045]\tLoss: 996.5787\n",
      "Training Epoch: 9 [16000/36045]\tLoss: 999.6740\n",
      "Training Epoch: 9 [16050/36045]\tLoss: 955.9803\n",
      "Training Epoch: 9 [16100/36045]\tLoss: 880.4715\n",
      "Training Epoch: 9 [16150/36045]\tLoss: 853.7791\n",
      "Training Epoch: 9 [16200/36045]\tLoss: 1020.2264\n",
      "Training Epoch: 9 [16250/36045]\tLoss: 1067.4265\n",
      "Training Epoch: 9 [16300/36045]\tLoss: 1155.4060\n",
      "Training Epoch: 9 [16350/36045]\tLoss: 1166.0553\n",
      "Training Epoch: 9 [16400/36045]\tLoss: 1134.1648\n",
      "Training Epoch: 9 [16450/36045]\tLoss: 1111.6349\n",
      "Training Epoch: 9 [16500/36045]\tLoss: 1115.3640\n",
      "Training Epoch: 9 [16550/36045]\tLoss: 1067.1245\n",
      "Training Epoch: 9 [16600/36045]\tLoss: 1117.0260\n",
      "Training Epoch: 9 [16650/36045]\tLoss: 1140.7235\n",
      "Training Epoch: 9 [16700/36045]\tLoss: 1114.0500\n",
      "Training Epoch: 9 [16750/36045]\tLoss: 1103.3152\n",
      "Training Epoch: 9 [16800/36045]\tLoss: 1130.2241\n",
      "Training Epoch: 9 [16850/36045]\tLoss: 1076.0018\n",
      "Training Epoch: 9 [16900/36045]\tLoss: 1088.2738\n",
      "Training Epoch: 9 [16950/36045]\tLoss: 1106.3904\n",
      "Training Epoch: 9 [17000/36045]\tLoss: 1080.3110\n",
      "Training Epoch: 9 [17050/36045]\tLoss: 1141.8964\n",
      "Training Epoch: 9 [17100/36045]\tLoss: 1151.8315\n",
      "Training Epoch: 9 [17150/36045]\tLoss: 1005.1970\n",
      "Training Epoch: 9 [17200/36045]\tLoss: 952.1696\n",
      "Training Epoch: 9 [17250/36045]\tLoss: 994.3032\n",
      "Training Epoch: 9 [17300/36045]\tLoss: 1051.9254\n",
      "Training Epoch: 9 [17350/36045]\tLoss: 995.5690\n",
      "Training Epoch: 9 [17400/36045]\tLoss: 1008.1870\n",
      "Training Epoch: 9 [17450/36045]\tLoss: 1038.5874\n",
      "Training Epoch: 9 [17500/36045]\tLoss: 1021.1671\n",
      "Training Epoch: 9 [17550/36045]\tLoss: 1037.8092\n",
      "Training Epoch: 9 [17600/36045]\tLoss: 1025.7734\n",
      "Training Epoch: 9 [17650/36045]\tLoss: 1056.2122\n",
      "Training Epoch: 9 [17700/36045]\tLoss: 1027.0111\n",
      "Training Epoch: 9 [17750/36045]\tLoss: 1050.3330\n",
      "Training Epoch: 9 [17800/36045]\tLoss: 1043.3069\n",
      "Training Epoch: 9 [17850/36045]\tLoss: 997.5147\n",
      "Training Epoch: 9 [17900/36045]\tLoss: 1035.0862\n",
      "Training Epoch: 9 [17950/36045]\tLoss: 1043.3751\n",
      "Training Epoch: 9 [18000/36045]\tLoss: 1027.9618\n",
      "Training Epoch: 9 [18050/36045]\tLoss: 1171.1736\n",
      "Training Epoch: 9 [18100/36045]\tLoss: 1184.2776\n",
      "Training Epoch: 9 [18150/36045]\tLoss: 1186.8541\n",
      "Training Epoch: 9 [18200/36045]\tLoss: 1175.1456\n",
      "Training Epoch: 9 [18250/36045]\tLoss: 1196.2407\n",
      "Training Epoch: 9 [18300/36045]\tLoss: 1093.8525\n",
      "Training Epoch: 9 [18350/36045]\tLoss: 1158.7275\n",
      "Training Epoch: 9 [18400/36045]\tLoss: 1134.5288\n",
      "Training Epoch: 9 [18450/36045]\tLoss: 1109.4155\n",
      "Training Epoch: 9 [18500/36045]\tLoss: 1108.0160\n",
      "Training Epoch: 9 [18550/36045]\tLoss: 1086.9313\n",
      "Training Epoch: 9 [18600/36045]\tLoss: 1077.0674\n",
      "Training Epoch: 9 [18650/36045]\tLoss: 1134.8186\n",
      "Training Epoch: 9 [18700/36045]\tLoss: 1196.2081\n",
      "Training Epoch: 9 [18750/36045]\tLoss: 1173.8890\n",
      "Training Epoch: 9 [18800/36045]\tLoss: 1210.1735\n",
      "Training Epoch: 9 [18850/36045]\tLoss: 1145.4246\n",
      "Training Epoch: 9 [18900/36045]\tLoss: 1226.1924\n",
      "Training Epoch: 9 [18950/36045]\tLoss: 1146.8040\n",
      "Training Epoch: 9 [19000/36045]\tLoss: 1006.1758\n",
      "Training Epoch: 9 [19050/36045]\tLoss: 966.3862\n",
      "Training Epoch: 9 [19100/36045]\tLoss: 992.8571\n",
      "Training Epoch: 9 [19150/36045]\tLoss: 974.4799\n",
      "Training Epoch: 9 [19200/36045]\tLoss: 1014.2053\n",
      "Training Epoch: 9 [19250/36045]\tLoss: 1022.0023\n",
      "Training Epoch: 9 [19300/36045]\tLoss: 1053.8425\n",
      "Training Epoch: 9 [19350/36045]\tLoss: 1025.7051\n",
      "Training Epoch: 9 [19400/36045]\tLoss: 1053.6327\n",
      "Training Epoch: 9 [19450/36045]\tLoss: 1040.0570\n",
      "Training Epoch: 9 [19500/36045]\tLoss: 1044.2271\n",
      "Training Epoch: 9 [19550/36045]\tLoss: 1049.6141\n",
      "Training Epoch: 9 [19600/36045]\tLoss: 1097.3607\n",
      "Training Epoch: 9 [19650/36045]\tLoss: 1383.4634\n",
      "Training Epoch: 9 [19700/36045]\tLoss: 1329.8485\n",
      "Training Epoch: 9 [19750/36045]\tLoss: 1327.3571\n",
      "Training Epoch: 9 [19800/36045]\tLoss: 1321.0352\n",
      "Training Epoch: 9 [19850/36045]\tLoss: 939.5535\n",
      "Training Epoch: 9 [19900/36045]\tLoss: 905.4167\n",
      "Training Epoch: 9 [19950/36045]\tLoss: 912.4473\n",
      "Training Epoch: 9 [20000/36045]\tLoss: 911.9573\n",
      "Training Epoch: 9 [20050/36045]\tLoss: 1020.5294\n",
      "Training Epoch: 9 [20100/36045]\tLoss: 1020.3326\n",
      "Training Epoch: 9 [20150/36045]\tLoss: 1028.2273\n",
      "Training Epoch: 9 [20200/36045]\tLoss: 1021.1738\n",
      "Training Epoch: 9 [20250/36045]\tLoss: 1081.8849\n",
      "Training Epoch: 9 [20300/36045]\tLoss: 1119.6908\n",
      "Training Epoch: 9 [20350/36045]\tLoss: 1155.5516\n",
      "Training Epoch: 9 [20400/36045]\tLoss: 1173.2070\n",
      "Training Epoch: 9 [20450/36045]\tLoss: 1151.1069\n",
      "Training Epoch: 9 [20500/36045]\tLoss: 1116.0076\n",
      "Training Epoch: 9 [20550/36045]\tLoss: 1011.4615\n",
      "Training Epoch: 9 [20600/36045]\tLoss: 1031.0360\n",
      "Training Epoch: 9 [20650/36045]\tLoss: 1022.8564\n",
      "Training Epoch: 9 [20700/36045]\tLoss: 1002.7440\n",
      "Training Epoch: 9 [20750/36045]\tLoss: 1066.8275\n",
      "Training Epoch: 9 [20800/36045]\tLoss: 1162.3113\n",
      "Training Epoch: 9 [20850/36045]\tLoss: 1151.5090\n",
      "Training Epoch: 9 [20900/36045]\tLoss: 1218.7295\n",
      "Training Epoch: 9 [20950/36045]\tLoss: 1151.6682\n",
      "Training Epoch: 9 [21000/36045]\tLoss: 1082.3073\n",
      "Training Epoch: 9 [21050/36045]\tLoss: 921.9670\n",
      "Training Epoch: 9 [21100/36045]\tLoss: 923.1804\n",
      "Training Epoch: 9 [21150/36045]\tLoss: 986.2800\n",
      "Training Epoch: 9 [21200/36045]\tLoss: 990.1190\n",
      "Training Epoch: 9 [21250/36045]\tLoss: 942.1436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [21300/36045]\tLoss: 1110.7655\n",
      "Training Epoch: 9 [21350/36045]\tLoss: 1106.8190\n",
      "Training Epoch: 9 [21400/36045]\tLoss: 1115.4343\n",
      "Training Epoch: 9 [21450/36045]\tLoss: 1135.1021\n",
      "Training Epoch: 9 [21500/36045]\tLoss: 1132.3378\n",
      "Training Epoch: 9 [21550/36045]\tLoss: 1239.4695\n",
      "Training Epoch: 9 [21600/36045]\tLoss: 1240.6121\n",
      "Training Epoch: 9 [21650/36045]\tLoss: 1259.4148\n",
      "Training Epoch: 9 [21700/36045]\tLoss: 1255.3492\n",
      "Training Epoch: 9 [21750/36045]\tLoss: 1216.6980\n",
      "Training Epoch: 9 [21800/36045]\tLoss: 926.0516\n",
      "Training Epoch: 9 [21850/36045]\tLoss: 899.3937\n",
      "Training Epoch: 9 [21900/36045]\tLoss: 918.3179\n",
      "Training Epoch: 9 [21950/36045]\tLoss: 909.5393\n",
      "Training Epoch: 9 [22000/36045]\tLoss: 922.4102\n",
      "Training Epoch: 9 [22050/36045]\tLoss: 981.2219\n",
      "Training Epoch: 9 [22100/36045]\tLoss: 966.5114\n",
      "Training Epoch: 9 [22150/36045]\tLoss: 938.2595\n",
      "Training Epoch: 9 [22200/36045]\tLoss: 966.7176\n",
      "Training Epoch: 9 [22250/36045]\tLoss: 977.9309\n",
      "Training Epoch: 9 [22300/36045]\tLoss: 1044.3265\n",
      "Training Epoch: 9 [22350/36045]\tLoss: 1073.1852\n",
      "Training Epoch: 9 [22400/36045]\tLoss: 1104.6700\n",
      "Training Epoch: 9 [22450/36045]\tLoss: 1080.8568\n",
      "Training Epoch: 9 [22500/36045]\tLoss: 1051.4785\n",
      "Training Epoch: 9 [22550/36045]\tLoss: 1119.3806\n",
      "Training Epoch: 9 [22600/36045]\tLoss: 1233.8804\n",
      "Training Epoch: 9 [22650/36045]\tLoss: 1291.2627\n",
      "Training Epoch: 9 [22700/36045]\tLoss: 1320.1689\n",
      "Training Epoch: 9 [22750/36045]\tLoss: 1347.0481\n",
      "Training Epoch: 9 [22800/36045]\tLoss: 1408.3552\n",
      "Training Epoch: 9 [22850/36045]\tLoss: 1169.1311\n",
      "Training Epoch: 9 [22900/36045]\tLoss: 1172.3818\n",
      "Training Epoch: 9 [22950/36045]\tLoss: 1139.8777\n",
      "Training Epoch: 9 [23000/36045]\tLoss: 1149.7935\n",
      "Training Epoch: 9 [23050/36045]\tLoss: 1030.9150\n",
      "Training Epoch: 9 [23100/36045]\tLoss: 1050.5208\n",
      "Training Epoch: 9 [23150/36045]\tLoss: 1030.0811\n",
      "Training Epoch: 9 [23200/36045]\tLoss: 971.1976\n",
      "Training Epoch: 9 [23250/36045]\tLoss: 978.7599\n",
      "Training Epoch: 9 [23300/36045]\tLoss: 970.1506\n",
      "Training Epoch: 9 [23350/36045]\tLoss: 997.8923\n",
      "Training Epoch: 9 [23400/36045]\tLoss: 1083.6754\n",
      "Training Epoch: 9 [23450/36045]\tLoss: 1069.2478\n",
      "Training Epoch: 9 [23500/36045]\tLoss: 1024.9871\n",
      "Training Epoch: 9 [23550/36045]\tLoss: 1110.4858\n",
      "Training Epoch: 9 [23600/36045]\tLoss: 1245.1790\n",
      "Training Epoch: 9 [23650/36045]\tLoss: 1276.5321\n",
      "Training Epoch: 9 [23700/36045]\tLoss: 1284.6520\n",
      "Training Epoch: 9 [23750/36045]\tLoss: 1247.1830\n",
      "Training Epoch: 9 [23800/36045]\tLoss: 994.5340\n",
      "Training Epoch: 9 [23850/36045]\tLoss: 1034.0929\n",
      "Training Epoch: 9 [23900/36045]\tLoss: 1020.4786\n",
      "Training Epoch: 9 [23950/36045]\tLoss: 1001.3463\n",
      "Training Epoch: 9 [24000/36045]\tLoss: 973.8064\n",
      "Training Epoch: 9 [24050/36045]\tLoss: 889.1096\n",
      "Training Epoch: 9 [24100/36045]\tLoss: 928.5425\n",
      "Training Epoch: 9 [24150/36045]\tLoss: 930.6444\n",
      "Training Epoch: 9 [24200/36045]\tLoss: 909.9058\n",
      "Training Epoch: 9 [24250/36045]\tLoss: 887.1983\n",
      "Training Epoch: 9 [24300/36045]\tLoss: 966.8664\n",
      "Training Epoch: 9 [24350/36045]\tLoss: 995.4774\n",
      "Training Epoch: 9 [24400/36045]\tLoss: 1019.3763\n",
      "Training Epoch: 9 [24450/36045]\tLoss: 979.7211\n",
      "Training Epoch: 9 [24500/36045]\tLoss: 1033.5343\n",
      "Training Epoch: 9 [24550/36045]\tLoss: 1130.6338\n",
      "Training Epoch: 9 [24600/36045]\tLoss: 1122.4419\n",
      "Training Epoch: 9 [24650/36045]\tLoss: 1085.0577\n",
      "Training Epoch: 9 [24700/36045]\tLoss: 1101.4702\n",
      "Training Epoch: 9 [24750/36045]\tLoss: 1015.6809\n",
      "Training Epoch: 9 [24800/36045]\tLoss: 898.3476\n",
      "Training Epoch: 9 [24850/36045]\tLoss: 915.3872\n",
      "Training Epoch: 9 [24900/36045]\tLoss: 913.7971\n",
      "Training Epoch: 9 [24950/36045]\tLoss: 917.0883\n",
      "Training Epoch: 9 [25000/36045]\tLoss: 878.4374\n",
      "Training Epoch: 9 [25050/36045]\tLoss: 830.5723\n",
      "Training Epoch: 9 [25100/36045]\tLoss: 742.5311\n",
      "Training Epoch: 9 [25150/36045]\tLoss: 690.5457\n",
      "Training Epoch: 9 [25200/36045]\tLoss: 688.4916\n",
      "Training Epoch: 9 [25250/36045]\tLoss: 731.9305\n",
      "Training Epoch: 9 [25300/36045]\tLoss: 948.5428\n",
      "Training Epoch: 9 [25350/36045]\tLoss: 947.8969\n",
      "Training Epoch: 9 [25400/36045]\tLoss: 884.6632\n",
      "Training Epoch: 9 [25450/36045]\tLoss: 885.8143\n",
      "Training Epoch: 9 [25500/36045]\tLoss: 964.0001\n",
      "Training Epoch: 9 [25550/36045]\tLoss: 1137.0022\n",
      "Training Epoch: 9 [25600/36045]\tLoss: 1143.2368\n",
      "Training Epoch: 9 [25650/36045]\tLoss: 1099.4376\n",
      "Training Epoch: 9 [25700/36045]\tLoss: 1128.3715\n",
      "Training Epoch: 9 [25750/36045]\tLoss: 1074.6110\n",
      "Training Epoch: 9 [25800/36045]\tLoss: 661.9526\n",
      "Training Epoch: 9 [25850/36045]\tLoss: 674.0693\n",
      "Training Epoch: 9 [25900/36045]\tLoss: 644.8838\n",
      "Training Epoch: 9 [25950/36045]\tLoss: 658.2902\n",
      "Training Epoch: 9 [26000/36045]\tLoss: 811.9051\n",
      "Training Epoch: 9 [26050/36045]\tLoss: 1101.8214\n",
      "Training Epoch: 9 [26100/36045]\tLoss: 1143.5728\n",
      "Training Epoch: 9 [26150/36045]\tLoss: 1133.3660\n",
      "Training Epoch: 9 [26200/36045]\tLoss: 1105.0249\n",
      "Training Epoch: 9 [26250/36045]\tLoss: 1138.4446\n",
      "Training Epoch: 9 [26300/36045]\tLoss: 981.3203\n",
      "Training Epoch: 9 [26350/36045]\tLoss: 985.9058\n",
      "Training Epoch: 9 [26400/36045]\tLoss: 970.4331\n",
      "Training Epoch: 9 [26450/36045]\tLoss: 893.0215\n",
      "Training Epoch: 9 [26500/36045]\tLoss: 1076.8619\n",
      "Training Epoch: 9 [26550/36045]\tLoss: 1099.7736\n",
      "Training Epoch: 9 [26600/36045]\tLoss: 1091.9077\n",
      "Training Epoch: 9 [26650/36045]\tLoss: 1115.6278\n",
      "Training Epoch: 9 [26700/36045]\tLoss: 1090.9410\n",
      "Training Epoch: 9 [26750/36045]\tLoss: 1022.9106\n",
      "Training Epoch: 9 [26800/36045]\tLoss: 747.8958\n",
      "Training Epoch: 9 [26850/36045]\tLoss: 630.0728\n",
      "Training Epoch: 9 [26900/36045]\tLoss: 635.8653\n",
      "Training Epoch: 9 [26950/36045]\tLoss: 693.1716\n",
      "Training Epoch: 9 [27000/36045]\tLoss: 1084.8794\n",
      "Training Epoch: 9 [27050/36045]\tLoss: 1145.9816\n",
      "Training Epoch: 9 [27100/36045]\tLoss: 1112.3959\n",
      "Training Epoch: 9 [27150/36045]\tLoss: 1163.1659\n",
      "Training Epoch: 9 [27200/36045]\tLoss: 877.9084\n",
      "Training Epoch: 9 [27250/36045]\tLoss: 875.1227\n",
      "Training Epoch: 9 [27300/36045]\tLoss: 844.1011\n",
      "Training Epoch: 9 [27350/36045]\tLoss: 855.6005\n",
      "Training Epoch: 9 [27400/36045]\tLoss: 850.0793\n",
      "Training Epoch: 9 [27450/36045]\tLoss: 1066.1228\n",
      "Training Epoch: 9 [27500/36045]\tLoss: 1140.9709\n",
      "Training Epoch: 9 [27550/36045]\tLoss: 1132.6200\n",
      "Training Epoch: 9 [27600/36045]\tLoss: 1135.6384\n",
      "Training Epoch: 9 [27650/36045]\tLoss: 1138.3326\n",
      "Training Epoch: 9 [27700/36045]\tLoss: 1170.0603\n",
      "Training Epoch: 9 [27750/36045]\tLoss: 1186.4899\n",
      "Training Epoch: 9 [27800/36045]\tLoss: 1167.4548\n",
      "Training Epoch: 9 [27850/36045]\tLoss: 1142.4918\n",
      "Training Epoch: 9 [27900/36045]\tLoss: 1010.2665\n",
      "Training Epoch: 9 [27950/36045]\tLoss: 832.7462\n",
      "Training Epoch: 9 [28000/36045]\tLoss: 798.9065\n",
      "Training Epoch: 9 [28050/36045]\tLoss: 826.6153\n",
      "Training Epoch: 9 [28100/36045]\tLoss: 810.7885\n",
      "Training Epoch: 9 [28150/36045]\tLoss: 878.5343\n",
      "Training Epoch: 9 [28200/36045]\tLoss: 883.7967\n",
      "Training Epoch: 9 [28250/36045]\tLoss: 887.6331\n",
      "Training Epoch: 9 [28300/36045]\tLoss: 841.6299\n",
      "Training Epoch: 9 [28350/36045]\tLoss: 842.3531\n",
      "Training Epoch: 9 [28400/36045]\tLoss: 1206.7720\n",
      "Training Epoch: 9 [28450/36045]\tLoss: 1072.5504\n",
      "Training Epoch: 9 [28500/36045]\tLoss: 923.5400\n",
      "Training Epoch: 9 [28550/36045]\tLoss: 843.4876\n",
      "Training Epoch: 9 [28600/36045]\tLoss: 965.7687\n",
      "Training Epoch: 9 [28650/36045]\tLoss: 1161.6526\n",
      "Training Epoch: 9 [28700/36045]\tLoss: 1156.5978\n",
      "Training Epoch: 9 [28750/36045]\tLoss: 1132.0813\n",
      "Training Epoch: 9 [28800/36045]\tLoss: 1140.4178\n",
      "Training Epoch: 9 [28850/36045]\tLoss: 985.0507\n",
      "Training Epoch: 9 [28900/36045]\tLoss: 779.1354\n",
      "Training Epoch: 9 [28950/36045]\tLoss: 768.9035\n",
      "Training Epoch: 9 [29000/36045]\tLoss: 771.1292\n",
      "Training Epoch: 9 [29050/36045]\tLoss: 779.8563\n",
      "Training Epoch: 9 [29100/36045]\tLoss: 803.7915\n",
      "Training Epoch: 9 [29150/36045]\tLoss: 785.5191\n",
      "Training Epoch: 9 [29200/36045]\tLoss: 774.3786\n",
      "Training Epoch: 9 [29250/36045]\tLoss: 748.6606\n",
      "Training Epoch: 9 [29300/36045]\tLoss: 874.0258\n",
      "Training Epoch: 9 [29350/36045]\tLoss: 1055.2288\n",
      "Training Epoch: 9 [29400/36045]\tLoss: 1082.7897\n",
      "Training Epoch: 9 [29450/36045]\tLoss: 1132.7782\n",
      "Training Epoch: 9 [29500/36045]\tLoss: 1153.1564\n",
      "Training Epoch: 9 [29550/36045]\tLoss: 1090.3234\n",
      "Training Epoch: 9 [29600/36045]\tLoss: 940.4283\n",
      "Training Epoch: 9 [29650/36045]\tLoss: 916.5479\n",
      "Training Epoch: 9 [29700/36045]\tLoss: 808.9957\n",
      "Training Epoch: 9 [29750/36045]\tLoss: 817.3542\n",
      "Training Epoch: 9 [29800/36045]\tLoss: 871.2377\n",
      "Training Epoch: 9 [29850/36045]\tLoss: 935.9182\n",
      "Training Epoch: 9 [29900/36045]\tLoss: 929.9704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [29950/36045]\tLoss: 949.2393\n",
      "Training Epoch: 9 [30000/36045]\tLoss: 938.8443\n",
      "Training Epoch: 9 [30050/36045]\tLoss: 945.8383\n",
      "Training Epoch: 9 [30100/36045]\tLoss: 1168.9500\n",
      "Training Epoch: 9 [30150/36045]\tLoss: 1165.3939\n",
      "Training Epoch: 9 [30200/36045]\tLoss: 1099.9731\n",
      "Training Epoch: 9 [30250/36045]\tLoss: 1154.9537\n",
      "Training Epoch: 9 [30300/36045]\tLoss: 1149.4905\n",
      "Training Epoch: 9 [30350/36045]\tLoss: 942.2623\n",
      "Training Epoch: 9 [30400/36045]\tLoss: 933.3605\n",
      "Training Epoch: 9 [30450/36045]\tLoss: 930.3875\n",
      "Training Epoch: 9 [30500/36045]\tLoss: 863.5349\n",
      "Training Epoch: 9 [30550/36045]\tLoss: 800.7825\n",
      "Training Epoch: 9 [30600/36045]\tLoss: 763.8453\n",
      "Training Epoch: 9 [30650/36045]\tLoss: 755.7766\n",
      "Training Epoch: 9 [30700/36045]\tLoss: 780.8345\n",
      "Training Epoch: 9 [30750/36045]\tLoss: 757.7753\n",
      "Training Epoch: 9 [30800/36045]\tLoss: 799.6614\n",
      "Training Epoch: 9 [30850/36045]\tLoss: 795.6769\n",
      "Training Epoch: 9 [30900/36045]\tLoss: 819.3136\n",
      "Training Epoch: 9 [30950/36045]\tLoss: 862.4405\n",
      "Training Epoch: 9 [31000/36045]\tLoss: 845.8371\n",
      "Training Epoch: 9 [31050/36045]\tLoss: 707.9112\n",
      "Training Epoch: 9 [31100/36045]\tLoss: 692.9325\n",
      "Training Epoch: 9 [31150/36045]\tLoss: 698.4111\n",
      "Training Epoch: 9 [31200/36045]\tLoss: 881.0172\n",
      "Training Epoch: 9 [31250/36045]\tLoss: 1138.4409\n",
      "Training Epoch: 9 [31300/36045]\tLoss: 1087.7815\n",
      "Training Epoch: 9 [31350/36045]\tLoss: 1105.7589\n",
      "Training Epoch: 9 [31400/36045]\tLoss: 1094.1626\n",
      "Training Epoch: 9 [31450/36045]\tLoss: 1080.2067\n",
      "Training Epoch: 9 [31500/36045]\tLoss: 1079.3859\n",
      "Training Epoch: 9 [31550/36045]\tLoss: 1094.6144\n",
      "Training Epoch: 9 [31600/36045]\tLoss: 1027.2133\n",
      "Training Epoch: 9 [31650/36045]\tLoss: 1100.2896\n",
      "Training Epoch: 9 [31700/36045]\tLoss: 827.9241\n",
      "Training Epoch: 9 [31750/36045]\tLoss: 693.6817\n",
      "Training Epoch: 9 [31800/36045]\tLoss: 658.7062\n",
      "Training Epoch: 9 [31850/36045]\tLoss: 681.1052\n",
      "Training Epoch: 9 [31900/36045]\tLoss: 1015.7289\n",
      "Training Epoch: 9 [31950/36045]\tLoss: 1260.5381\n",
      "Training Epoch: 9 [32000/36045]\tLoss: 1405.8623\n",
      "Training Epoch: 9 [32050/36045]\tLoss: 1347.6166\n",
      "Training Epoch: 9 [32100/36045]\tLoss: 1327.2469\n",
      "Training Epoch: 9 [32150/36045]\tLoss: 1103.3654\n",
      "Training Epoch: 9 [32200/36045]\tLoss: 1114.5931\n",
      "Training Epoch: 9 [32250/36045]\tLoss: 1130.2020\n",
      "Training Epoch: 9 [32300/36045]\tLoss: 1108.1223\n",
      "Training Epoch: 9 [32350/36045]\tLoss: 1087.7825\n",
      "Training Epoch: 9 [32400/36045]\tLoss: 1029.4827\n",
      "Training Epoch: 9 [32450/36045]\tLoss: 862.6917\n",
      "Training Epoch: 9 [32500/36045]\tLoss: 827.9155\n",
      "Training Epoch: 9 [32550/36045]\tLoss: 836.9703\n",
      "Training Epoch: 9 [32600/36045]\tLoss: 830.3998\n",
      "Training Epoch: 9 [32650/36045]\tLoss: 1020.2327\n",
      "Training Epoch: 9 [32700/36045]\tLoss: 1100.3242\n",
      "Training Epoch: 9 [32750/36045]\tLoss: 1053.4821\n",
      "Training Epoch: 9 [32800/36045]\tLoss: 1081.3999\n",
      "Training Epoch: 9 [32850/36045]\tLoss: 1009.6152\n",
      "Training Epoch: 9 [32900/36045]\tLoss: 820.8896\n",
      "Training Epoch: 9 [32950/36045]\tLoss: 860.1044\n",
      "Training Epoch: 9 [33000/36045]\tLoss: 869.3945\n",
      "Training Epoch: 9 [33050/36045]\tLoss: 816.1621\n",
      "Training Epoch: 9 [33100/36045]\tLoss: 930.1215\n",
      "Training Epoch: 9 [33150/36045]\tLoss: 1227.4172\n",
      "Training Epoch: 9 [33200/36045]\tLoss: 1198.5559\n",
      "Training Epoch: 9 [33250/36045]\tLoss: 1230.6759\n",
      "Training Epoch: 9 [33300/36045]\tLoss: 1305.3793\n",
      "Training Epoch: 9 [33350/36045]\tLoss: 1018.2987\n",
      "Training Epoch: 9 [33400/36045]\tLoss: 775.9545\n",
      "Training Epoch: 9 [33450/36045]\tLoss: 770.8099\n",
      "Training Epoch: 9 [33500/36045]\tLoss: 789.4960\n",
      "Training Epoch: 9 [33550/36045]\tLoss: 817.8772\n",
      "Training Epoch: 9 [33600/36045]\tLoss: 825.8480\n",
      "Training Epoch: 9 [33650/36045]\tLoss: 1061.4241\n",
      "Training Epoch: 9 [33700/36045]\tLoss: 1026.6849\n",
      "Training Epoch: 9 [33750/36045]\tLoss: 1058.7900\n",
      "Training Epoch: 9 [33800/36045]\tLoss: 1056.0977\n",
      "Training Epoch: 9 [33850/36045]\tLoss: 1062.3914\n",
      "Training Epoch: 9 [33900/36045]\tLoss: 1083.5841\n",
      "Training Epoch: 9 [33950/36045]\tLoss: 1095.1509\n",
      "Training Epoch: 9 [34000/36045]\tLoss: 1082.3405\n",
      "Training Epoch: 9 [34050/36045]\tLoss: 1096.5621\n",
      "Training Epoch: 9 [34100/36045]\tLoss: 1047.4791\n",
      "Training Epoch: 9 [34150/36045]\tLoss: 976.1882\n",
      "Training Epoch: 9 [34200/36045]\tLoss: 931.4612\n",
      "Training Epoch: 9 [34250/36045]\tLoss: 946.5576\n",
      "Training Epoch: 9 [34300/36045]\tLoss: 816.1359\n",
      "Training Epoch: 9 [34350/36045]\tLoss: 853.2126\n",
      "Training Epoch: 9 [34400/36045]\tLoss: 830.1885\n",
      "Training Epoch: 9 [34450/36045]\tLoss: 778.3452\n",
      "Training Epoch: 9 [34500/36045]\tLoss: 833.6647\n",
      "Training Epoch: 9 [34550/36045]\tLoss: 826.8627\n",
      "Training Epoch: 9 [34600/36045]\tLoss: 797.6505\n",
      "Training Epoch: 9 [34650/36045]\tLoss: 934.3842\n",
      "Training Epoch: 9 [34700/36045]\tLoss: 982.2863\n",
      "Training Epoch: 9 [34750/36045]\tLoss: 874.3654\n",
      "Training Epoch: 9 [34800/36045]\tLoss: 983.7539\n",
      "Training Epoch: 9 [34850/36045]\tLoss: 1010.1840\n",
      "Training Epoch: 9 [34900/36045]\tLoss: 1208.4960\n",
      "Training Epoch: 9 [34950/36045]\tLoss: 1194.1973\n",
      "Training Epoch: 9 [35000/36045]\tLoss: 1204.7305\n",
      "Training Epoch: 9 [35050/36045]\tLoss: 1178.0184\n",
      "Training Epoch: 9 [35100/36045]\tLoss: 925.6782\n",
      "Training Epoch: 9 [35150/36045]\tLoss: 911.6816\n",
      "Training Epoch: 9 [35200/36045]\tLoss: 793.0594\n",
      "Training Epoch: 9 [35250/36045]\tLoss: 859.4318\n",
      "Training Epoch: 9 [35300/36045]\tLoss: 870.0701\n",
      "Training Epoch: 9 [35350/36045]\tLoss: 1040.6244\n",
      "Training Epoch: 9 [35400/36045]\tLoss: 1113.2877\n",
      "Training Epoch: 9 [35450/36045]\tLoss: 1062.9335\n",
      "Training Epoch: 9 [35500/36045]\tLoss: 1037.1954\n",
      "Training Epoch: 9 [35550/36045]\tLoss: 1022.9565\n",
      "Training Epoch: 9 [35600/36045]\tLoss: 1066.0515\n",
      "Training Epoch: 9 [35650/36045]\tLoss: 1160.1409\n",
      "Training Epoch: 9 [35700/36045]\tLoss: 1077.5221\n",
      "Training Epoch: 9 [35750/36045]\tLoss: 1147.8568\n",
      "Training Epoch: 9 [35800/36045]\tLoss: 1155.0446\n",
      "Training Epoch: 9 [35850/36045]\tLoss: 1129.1617\n",
      "Training Epoch: 9 [35900/36045]\tLoss: 1178.6567\n",
      "Training Epoch: 9 [35950/36045]\tLoss: 1180.2496\n",
      "Training Epoch: 9 [36000/36045]\tLoss: 1159.0090\n",
      "Training Epoch: 9 [36045/36045]\tLoss: 1142.0380\n",
      "Training Epoch: 9 [4004/4004]\tLoss: 1081.1739\n",
      "Training Epoch: 10 [50/36045]\tLoss: 1091.6656\n",
      "Training Epoch: 10 [100/36045]\tLoss: 1052.4607\n",
      "Training Epoch: 10 [150/36045]\tLoss: 1055.4369\n",
      "Training Epoch: 10 [200/36045]\tLoss: 1036.8326\n",
      "Training Epoch: 10 [250/36045]\tLoss: 1184.8912\n",
      "Training Epoch: 10 [300/36045]\tLoss: 1249.4402\n",
      "Training Epoch: 10 [350/36045]\tLoss: 1198.1084\n",
      "Training Epoch: 10 [400/36045]\tLoss: 1214.9310\n",
      "Training Epoch: 10 [450/36045]\tLoss: 1190.5919\n",
      "Training Epoch: 10 [500/36045]\tLoss: 1127.4867\n",
      "Training Epoch: 10 [550/36045]\tLoss: 1145.2344\n",
      "Training Epoch: 10 [600/36045]\tLoss: 1100.3075\n",
      "Training Epoch: 10 [650/36045]\tLoss: 1138.6549\n",
      "Training Epoch: 10 [700/36045]\tLoss: 1131.6422\n",
      "Training Epoch: 10 [750/36045]\tLoss: 1098.2808\n",
      "Training Epoch: 10 [800/36045]\tLoss: 1118.4465\n",
      "Training Epoch: 10 [850/36045]\tLoss: 1082.2338\n",
      "Training Epoch: 10 [900/36045]\tLoss: 1034.3900\n",
      "Training Epoch: 10 [950/36045]\tLoss: 991.5200\n",
      "Training Epoch: 10 [1000/36045]\tLoss: 955.9457\n",
      "Training Epoch: 10 [1050/36045]\tLoss: 960.5134\n",
      "Training Epoch: 10 [1100/36045]\tLoss: 936.3827\n",
      "Training Epoch: 10 [1150/36045]\tLoss: 936.3215\n",
      "Training Epoch: 10 [1200/36045]\tLoss: 990.5832\n",
      "Training Epoch: 10 [1250/36045]\tLoss: 1108.7472\n",
      "Training Epoch: 10 [1300/36045]\tLoss: 1104.9736\n",
      "Training Epoch: 10 [1350/36045]\tLoss: 1115.7490\n",
      "Training Epoch: 10 [1400/36045]\tLoss: 1157.0483\n",
      "Training Epoch: 10 [1450/36045]\tLoss: 1116.2203\n",
      "Training Epoch: 10 [1500/36045]\tLoss: 1042.7010\n",
      "Training Epoch: 10 [1550/36045]\tLoss: 1076.7145\n",
      "Training Epoch: 10 [1600/36045]\tLoss: 1084.8306\n",
      "Training Epoch: 10 [1650/36045]\tLoss: 1070.6820\n",
      "Training Epoch: 10 [1700/36045]\tLoss: 1084.6951\n",
      "Training Epoch: 10 [1750/36045]\tLoss: 1138.4957\n",
      "Training Epoch: 10 [1800/36045]\tLoss: 1118.1519\n",
      "Training Epoch: 10 [1850/36045]\tLoss: 1161.0541\n",
      "Training Epoch: 10 [1900/36045]\tLoss: 1090.6978\n",
      "Training Epoch: 10 [1950/36045]\tLoss: 1098.1548\n",
      "Training Epoch: 10 [2000/36045]\tLoss: 991.7646\n",
      "Training Epoch: 10 [2050/36045]\tLoss: 999.4764\n",
      "Training Epoch: 10 [2100/36045]\tLoss: 1053.8434\n",
      "Training Epoch: 10 [2150/36045]\tLoss: 1024.2109\n",
      "Training Epoch: 10 [2200/36045]\tLoss: 952.5618\n",
      "Training Epoch: 10 [2250/36045]\tLoss: 899.5663\n",
      "Training Epoch: 10 [2300/36045]\tLoss: 939.1628\n",
      "Training Epoch: 10 [2350/36045]\tLoss: 896.1151\n",
      "Training Epoch: 10 [2400/36045]\tLoss: 924.2366\n",
      "Training Epoch: 10 [2450/36045]\tLoss: 1139.0852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [2500/36045]\tLoss: 1186.9265\n",
      "Training Epoch: 10 [2550/36045]\tLoss: 1183.3601\n",
      "Training Epoch: 10 [2600/36045]\tLoss: 1187.9437\n",
      "Training Epoch: 10 [2650/36045]\tLoss: 1322.9495\n",
      "Training Epoch: 10 [2700/36045]\tLoss: 1415.4374\n",
      "Training Epoch: 10 [2750/36045]\tLoss: 1497.3059\n",
      "Training Epoch: 10 [2800/36045]\tLoss: 1513.8373\n",
      "Training Epoch: 10 [2850/36045]\tLoss: 1294.9388\n",
      "Training Epoch: 10 [2900/36045]\tLoss: 1285.5011\n",
      "Training Epoch: 10 [2950/36045]\tLoss: 1235.8705\n",
      "Training Epoch: 10 [3000/36045]\tLoss: 1230.2123\n",
      "Training Epoch: 10 [3050/36045]\tLoss: 1276.6646\n",
      "Training Epoch: 10 [3100/36045]\tLoss: 1161.8795\n",
      "Training Epoch: 10 [3150/36045]\tLoss: 902.4338\n",
      "Training Epoch: 10 [3200/36045]\tLoss: 937.3078\n",
      "Training Epoch: 10 [3250/36045]\tLoss: 877.1813\n",
      "Training Epoch: 10 [3300/36045]\tLoss: 829.4152\n",
      "Training Epoch: 10 [3350/36045]\tLoss: 873.9550\n",
      "Training Epoch: 10 [3400/36045]\tLoss: 917.4572\n",
      "Training Epoch: 10 [3450/36045]\tLoss: 990.1898\n",
      "Training Epoch: 10 [3500/36045]\tLoss: 961.1017\n",
      "Training Epoch: 10 [3550/36045]\tLoss: 931.7490\n",
      "Training Epoch: 10 [3600/36045]\tLoss: 995.4146\n",
      "Training Epoch: 10 [3650/36045]\tLoss: 1155.6619\n",
      "Training Epoch: 10 [3700/36045]\tLoss: 1158.0974\n",
      "Training Epoch: 10 [3750/36045]\tLoss: 1114.3301\n",
      "Training Epoch: 10 [3800/36045]\tLoss: 1097.6989\n",
      "Training Epoch: 10 [3850/36045]\tLoss: 1107.2168\n",
      "Training Epoch: 10 [3900/36045]\tLoss: 1117.6101\n",
      "Training Epoch: 10 [3950/36045]\tLoss: 1065.5143\n",
      "Training Epoch: 10 [4000/36045]\tLoss: 1094.6522\n",
      "Training Epoch: 10 [4050/36045]\tLoss: 1005.4050\n",
      "Training Epoch: 10 [4100/36045]\tLoss: 975.8420\n",
      "Training Epoch: 10 [4150/36045]\tLoss: 1010.2496\n",
      "Training Epoch: 10 [4200/36045]\tLoss: 996.4565\n",
      "Training Epoch: 10 [4250/36045]\tLoss: 1004.6645\n",
      "Training Epoch: 10 [4300/36045]\tLoss: 1033.7097\n",
      "Training Epoch: 10 [4350/36045]\tLoss: 1013.0803\n",
      "Training Epoch: 10 [4400/36045]\tLoss: 967.1932\n",
      "Training Epoch: 10 [4450/36045]\tLoss: 1036.3490\n",
      "Training Epoch: 10 [4500/36045]\tLoss: 1098.2588\n",
      "Training Epoch: 10 [4550/36045]\tLoss: 1117.6428\n",
      "Training Epoch: 10 [4600/36045]\tLoss: 1136.6925\n",
      "Training Epoch: 10 [4650/36045]\tLoss: 1137.9393\n",
      "Training Epoch: 10 [4700/36045]\tLoss: 1047.8403\n",
      "Training Epoch: 10 [4750/36045]\tLoss: 1029.7422\n",
      "Training Epoch: 10 [4800/36045]\tLoss: 1071.4785\n",
      "Training Epoch: 10 [4850/36045]\tLoss: 1048.3822\n",
      "Training Epoch: 10 [4900/36045]\tLoss: 1013.8054\n",
      "Training Epoch: 10 [4950/36045]\tLoss: 1058.9156\n",
      "Training Epoch: 10 [5000/36045]\tLoss: 1118.3170\n",
      "Training Epoch: 10 [5050/36045]\tLoss: 1077.5844\n",
      "Training Epoch: 10 [5100/36045]\tLoss: 1100.8497\n",
      "Training Epoch: 10 [5150/36045]\tLoss: 1071.0280\n",
      "Training Epoch: 10 [5200/36045]\tLoss: 1059.8890\n",
      "Training Epoch: 10 [5250/36045]\tLoss: 1046.8494\n",
      "Training Epoch: 10 [5300/36045]\tLoss: 1047.8452\n",
      "Training Epoch: 10 [5350/36045]\tLoss: 1088.4498\n",
      "Training Epoch: 10 [5400/36045]\tLoss: 1035.4890\n",
      "Training Epoch: 10 [5450/36045]\tLoss: 972.5059\n",
      "Training Epoch: 10 [5500/36045]\tLoss: 1027.2847\n",
      "Training Epoch: 10 [5550/36045]\tLoss: 995.8829\n",
      "Training Epoch: 10 [5600/36045]\tLoss: 1133.9706\n",
      "Training Epoch: 10 [5650/36045]\tLoss: 1082.1467\n",
      "Training Epoch: 10 [5700/36045]\tLoss: 1026.1326\n",
      "Training Epoch: 10 [5750/36045]\tLoss: 1007.0314\n",
      "Training Epoch: 10 [5800/36045]\tLoss: 1074.0210\n",
      "Training Epoch: 10 [5850/36045]\tLoss: 1039.4580\n",
      "Training Epoch: 10 [5900/36045]\tLoss: 1188.2301\n",
      "Training Epoch: 10 [5950/36045]\tLoss: 1224.3413\n",
      "Training Epoch: 10 [6000/36045]\tLoss: 1200.0182\n",
      "Training Epoch: 10 [6050/36045]\tLoss: 1157.8459\n",
      "Training Epoch: 10 [6100/36045]\tLoss: 1162.4578\n",
      "Training Epoch: 10 [6150/36045]\tLoss: 1120.1478\n",
      "Training Epoch: 10 [6200/36045]\tLoss: 1113.9955\n",
      "Training Epoch: 10 [6250/36045]\tLoss: 1125.9808\n",
      "Training Epoch: 10 [6300/36045]\tLoss: 1155.7299\n",
      "Training Epoch: 10 [6350/36045]\tLoss: 1212.2064\n",
      "Training Epoch: 10 [6400/36045]\tLoss: 1050.1698\n",
      "Training Epoch: 10 [6450/36045]\tLoss: 976.3535\n",
      "Training Epoch: 10 [6500/36045]\tLoss: 1004.3131\n",
      "Training Epoch: 10 [6550/36045]\tLoss: 1023.6823\n",
      "Training Epoch: 10 [6600/36045]\tLoss: 1029.7443\n",
      "Training Epoch: 10 [6650/36045]\tLoss: 1155.2467\n",
      "Training Epoch: 10 [6700/36045]\tLoss: 1207.3685\n",
      "Training Epoch: 10 [6750/36045]\tLoss: 1174.5497\n",
      "Training Epoch: 10 [6800/36045]\tLoss: 1174.2308\n",
      "Training Epoch: 10 [6850/36045]\tLoss: 1158.3907\n",
      "Training Epoch: 10 [6900/36045]\tLoss: 1022.9825\n",
      "Training Epoch: 10 [6950/36045]\tLoss: 962.7014\n",
      "Training Epoch: 10 [7000/36045]\tLoss: 1021.0556\n",
      "Training Epoch: 10 [7050/36045]\tLoss: 1052.5553\n",
      "Training Epoch: 10 [7100/36045]\tLoss: 1044.1226\n",
      "Training Epoch: 10 [7150/36045]\tLoss: 1066.5552\n",
      "Training Epoch: 10 [7200/36045]\tLoss: 1075.4240\n",
      "Training Epoch: 10 [7250/36045]\tLoss: 1073.1566\n",
      "Training Epoch: 10 [7300/36045]\tLoss: 1053.1201\n",
      "Training Epoch: 10 [7350/36045]\tLoss: 1049.3650\n",
      "Training Epoch: 10 [7400/36045]\tLoss: 957.8636\n",
      "Training Epoch: 10 [7450/36045]\tLoss: 956.4791\n",
      "Training Epoch: 10 [7500/36045]\tLoss: 957.1272\n",
      "Training Epoch: 10 [7550/36045]\tLoss: 910.5957\n",
      "Training Epoch: 10 [7600/36045]\tLoss: 1018.3497\n",
      "Training Epoch: 10 [7650/36045]\tLoss: 1092.7893\n",
      "Training Epoch: 10 [7700/36045]\tLoss: 1046.1780\n",
      "Training Epoch: 10 [7750/36045]\tLoss: 1063.4425\n",
      "Training Epoch: 10 [7800/36045]\tLoss: 1044.5082\n",
      "Training Epoch: 10 [7850/36045]\tLoss: 987.8809\n",
      "Training Epoch: 10 [7900/36045]\tLoss: 1043.3429\n",
      "Training Epoch: 10 [7950/36045]\tLoss: 1039.4788\n",
      "Training Epoch: 10 [8000/36045]\tLoss: 1059.8280\n",
      "Training Epoch: 10 [8050/36045]\tLoss: 1009.1293\n",
      "Training Epoch: 10 [8100/36045]\tLoss: 1044.6073\n",
      "Training Epoch: 10 [8150/36045]\tLoss: 1178.7490\n",
      "Training Epoch: 10 [8200/36045]\tLoss: 1162.3936\n",
      "Training Epoch: 10 [8250/36045]\tLoss: 1119.3340\n",
      "Training Epoch: 10 [8300/36045]\tLoss: 1209.2336\n",
      "Training Epoch: 10 [8350/36045]\tLoss: 1120.5851\n",
      "Training Epoch: 10 [8400/36045]\tLoss: 1023.2289\n",
      "Training Epoch: 10 [8450/36045]\tLoss: 964.5638\n",
      "Training Epoch: 10 [8500/36045]\tLoss: 1013.8784\n",
      "Training Epoch: 10 [8550/36045]\tLoss: 991.5224\n",
      "Training Epoch: 10 [8600/36045]\tLoss: 988.4479\n",
      "Training Epoch: 10 [8650/36045]\tLoss: 1041.3293\n",
      "Training Epoch: 10 [8700/36045]\tLoss: 1096.5699\n",
      "Training Epoch: 10 [8750/36045]\tLoss: 1072.5344\n",
      "Training Epoch: 10 [8800/36045]\tLoss: 1081.5112\n",
      "Training Epoch: 10 [8850/36045]\tLoss: 1068.1525\n",
      "Training Epoch: 10 [8900/36045]\tLoss: 969.0563\n",
      "Training Epoch: 10 [8950/36045]\tLoss: 999.9919\n",
      "Training Epoch: 10 [9000/36045]\tLoss: 1006.8536\n",
      "Training Epoch: 10 [9050/36045]\tLoss: 1004.7043\n",
      "Training Epoch: 10 [9100/36045]\tLoss: 1030.7061\n",
      "Training Epoch: 10 [9150/36045]\tLoss: 764.1198\n",
      "Training Epoch: 10 [9200/36045]\tLoss: 588.3535\n",
      "Training Epoch: 10 [9250/36045]\tLoss: 633.9641\n",
      "Training Epoch: 10 [9300/36045]\tLoss: 658.2125\n",
      "Training Epoch: 10 [9350/36045]\tLoss: 600.0497\n",
      "Training Epoch: 10 [9400/36045]\tLoss: 1172.6257\n",
      "Training Epoch: 10 [9450/36045]\tLoss: 1239.2579\n",
      "Training Epoch: 10 [9500/36045]\tLoss: 1221.9436\n",
      "Training Epoch: 10 [9550/36045]\tLoss: 1298.2466\n",
      "Training Epoch: 10 [9600/36045]\tLoss: 954.4185\n",
      "Training Epoch: 10 [9650/36045]\tLoss: 949.7587\n",
      "Training Epoch: 10 [9700/36045]\tLoss: 938.6765\n",
      "Training Epoch: 10 [9750/36045]\tLoss: 939.9360\n",
      "Training Epoch: 10 [9800/36045]\tLoss: 1214.9022\n",
      "Training Epoch: 10 [9850/36045]\tLoss: 1284.0072\n",
      "Training Epoch: 10 [9900/36045]\tLoss: 1316.3308\n",
      "Training Epoch: 10 [9950/36045]\tLoss: 1275.9054\n",
      "Training Epoch: 10 [10000/36045]\tLoss: 1178.4867\n",
      "Training Epoch: 10 [10050/36045]\tLoss: 994.5392\n",
      "Training Epoch: 10 [10100/36045]\tLoss: 993.4202\n",
      "Training Epoch: 10 [10150/36045]\tLoss: 1007.9528\n",
      "Training Epoch: 10 [10200/36045]\tLoss: 1000.6667\n",
      "Training Epoch: 10 [10250/36045]\tLoss: 1206.0499\n",
      "Training Epoch: 10 [10300/36045]\tLoss: 1171.5238\n",
      "Training Epoch: 10 [10350/36045]\tLoss: 1233.2695\n",
      "Training Epoch: 10 [10400/36045]\tLoss: 1223.5941\n",
      "Training Epoch: 10 [10450/36045]\tLoss: 1131.3092\n",
      "Training Epoch: 10 [10500/36045]\tLoss: 955.7432\n",
      "Training Epoch: 10 [10550/36045]\tLoss: 953.9324\n",
      "Training Epoch: 10 [10600/36045]\tLoss: 983.7659\n",
      "Training Epoch: 10 [10650/36045]\tLoss: 992.9327\n",
      "Training Epoch: 10 [10700/36045]\tLoss: 1093.2690\n",
      "Training Epoch: 10 [10750/36045]\tLoss: 1179.7501\n",
      "Training Epoch: 10 [10800/36045]\tLoss: 1099.9507\n",
      "Training Epoch: 10 [10850/36045]\tLoss: 1155.7421\n",
      "Training Epoch: 10 [10900/36045]\tLoss: 1203.3444\n",
      "Training Epoch: 10 [10950/36045]\tLoss: 911.3931\n",
      "Training Epoch: 10 [11000/36045]\tLoss: 904.6601\n",
      "Training Epoch: 10 [11050/36045]\tLoss: 961.8538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [11100/36045]\tLoss: 979.7982\n",
      "Training Epoch: 10 [11150/36045]\tLoss: 1060.4484\n",
      "Training Epoch: 10 [11200/36045]\tLoss: 1095.1875\n",
      "Training Epoch: 10 [11250/36045]\tLoss: 1111.7889\n",
      "Training Epoch: 10 [11300/36045]\tLoss: 1089.9460\n",
      "Training Epoch: 10 [11350/36045]\tLoss: 1083.7942\n",
      "Training Epoch: 10 [11400/36045]\tLoss: 1032.2495\n",
      "Training Epoch: 10 [11450/36045]\tLoss: 994.4265\n",
      "Training Epoch: 10 [11500/36045]\tLoss: 993.3283\n",
      "Training Epoch: 10 [11550/36045]\tLoss: 1016.4867\n",
      "Training Epoch: 10 [11600/36045]\tLoss: 1097.0496\n",
      "Training Epoch: 10 [11650/36045]\tLoss: 1152.0433\n",
      "Training Epoch: 10 [11700/36045]\tLoss: 1148.9835\n",
      "Training Epoch: 10 [11750/36045]\tLoss: 1174.6504\n",
      "Training Epoch: 10 [11800/36045]\tLoss: 1226.2202\n",
      "Training Epoch: 10 [11850/36045]\tLoss: 1260.9713\n",
      "Training Epoch: 10 [11900/36045]\tLoss: 1499.2253\n",
      "Training Epoch: 10 [11950/36045]\tLoss: 1489.3961\n",
      "Training Epoch: 10 [12000/36045]\tLoss: 1518.7129\n",
      "Training Epoch: 10 [12050/36045]\tLoss: 1469.7285\n",
      "Training Epoch: 10 [12100/36045]\tLoss: 1047.2515\n",
      "Training Epoch: 10 [12150/36045]\tLoss: 858.8299\n",
      "Training Epoch: 10 [12200/36045]\tLoss: 851.5621\n",
      "Training Epoch: 10 [12250/36045]\tLoss: 865.7991\n",
      "Training Epoch: 10 [12300/36045]\tLoss: 1057.1626\n",
      "Training Epoch: 10 [12350/36045]\tLoss: 1121.5510\n",
      "Training Epoch: 10 [12400/36045]\tLoss: 1141.4663\n",
      "Training Epoch: 10 [12450/36045]\tLoss: 1125.9338\n",
      "Training Epoch: 10 [12500/36045]\tLoss: 1167.6235\n",
      "Training Epoch: 10 [12550/36045]\tLoss: 1119.9227\n",
      "Training Epoch: 10 [12600/36045]\tLoss: 1053.8970\n",
      "Training Epoch: 10 [12650/36045]\tLoss: 1048.3219\n",
      "Training Epoch: 10 [12700/36045]\tLoss: 1080.2324\n",
      "Training Epoch: 10 [12750/36045]\tLoss: 1078.3878\n",
      "Training Epoch: 10 [12800/36045]\tLoss: 1058.2457\n",
      "Training Epoch: 10 [12850/36045]\tLoss: 1101.2697\n",
      "Training Epoch: 10 [12900/36045]\tLoss: 1055.4161\n",
      "Training Epoch: 10 [12950/36045]\tLoss: 1047.7812\n",
      "Training Epoch: 10 [13000/36045]\tLoss: 1081.8489\n",
      "Training Epoch: 10 [13050/36045]\tLoss: 1005.5962\n",
      "Training Epoch: 10 [13100/36045]\tLoss: 1054.5326\n",
      "Training Epoch: 10 [13150/36045]\tLoss: 1053.6857\n",
      "Training Epoch: 10 [13200/36045]\tLoss: 999.7039\n",
      "Training Epoch: 10 [13250/36045]\tLoss: 1051.8785\n",
      "Training Epoch: 10 [13300/36045]\tLoss: 1092.3009\n",
      "Training Epoch: 10 [13350/36045]\tLoss: 1062.5248\n",
      "Training Epoch: 10 [13400/36045]\tLoss: 1068.7585\n",
      "Training Epoch: 10 [13450/36045]\tLoss: 1054.1187\n",
      "Training Epoch: 10 [13500/36045]\tLoss: 1097.3468\n",
      "Training Epoch: 10 [13550/36045]\tLoss: 1228.1107\n",
      "Training Epoch: 10 [13600/36045]\tLoss: 1259.1528\n",
      "Training Epoch: 10 [13650/36045]\tLoss: 1344.4038\n",
      "Training Epoch: 10 [13700/36045]\tLoss: 1208.5707\n",
      "Training Epoch: 10 [13750/36045]\tLoss: 1084.1704\n",
      "Training Epoch: 10 [13800/36045]\tLoss: 1060.3979\n",
      "Training Epoch: 10 [13850/36045]\tLoss: 1041.1401\n",
      "Training Epoch: 10 [13900/36045]\tLoss: 1055.3511\n",
      "Training Epoch: 10 [13950/36045]\tLoss: 1084.9294\n",
      "Training Epoch: 10 [14000/36045]\tLoss: 1127.8301\n",
      "Training Epoch: 10 [14050/36045]\tLoss: 1082.4126\n",
      "Training Epoch: 10 [14100/36045]\tLoss: 1081.7288\n",
      "Training Epoch: 10 [14150/36045]\tLoss: 1060.0876\n",
      "Training Epoch: 10 [14200/36045]\tLoss: 1135.4891\n",
      "Training Epoch: 10 [14250/36045]\tLoss: 1241.9744\n",
      "Training Epoch: 10 [14300/36045]\tLoss: 1244.5599\n",
      "Training Epoch: 10 [14350/36045]\tLoss: 1191.7686\n",
      "Training Epoch: 10 [14400/36045]\tLoss: 1176.9879\n",
      "Training Epoch: 10 [14450/36045]\tLoss: 1225.7845\n",
      "Training Epoch: 10 [14500/36045]\tLoss: 1139.9688\n",
      "Training Epoch: 10 [14550/36045]\tLoss: 1195.9504\n",
      "Training Epoch: 10 [14600/36045]\tLoss: 1163.8406\n",
      "Training Epoch: 10 [14650/36045]\tLoss: 1170.3201\n",
      "Training Epoch: 10 [14700/36045]\tLoss: 1098.9857\n",
      "Training Epoch: 10 [14750/36045]\tLoss: 947.1312\n",
      "Training Epoch: 10 [14800/36045]\tLoss: 930.9955\n",
      "Training Epoch: 10 [14850/36045]\tLoss: 938.5184\n",
      "Training Epoch: 10 [14900/36045]\tLoss: 932.5579\n",
      "Training Epoch: 10 [14950/36045]\tLoss: 942.2764\n",
      "Training Epoch: 10 [15000/36045]\tLoss: 969.5275\n",
      "Training Epoch: 10 [15050/36045]\tLoss: 976.9839\n",
      "Training Epoch: 10 [15100/36045]\tLoss: 953.2943\n",
      "Training Epoch: 10 [15150/36045]\tLoss: 943.0529\n",
      "Training Epoch: 10 [15200/36045]\tLoss: 872.7341\n",
      "Training Epoch: 10 [15250/36045]\tLoss: 911.2549\n",
      "Training Epoch: 10 [15300/36045]\tLoss: 885.5824\n",
      "Training Epoch: 10 [15350/36045]\tLoss: 904.0898\n",
      "Training Epoch: 10 [15400/36045]\tLoss: 897.9731\n",
      "Training Epoch: 10 [15450/36045]\tLoss: 887.0256\n",
      "Training Epoch: 10 [15500/36045]\tLoss: 909.8360\n",
      "Training Epoch: 10 [15550/36045]\tLoss: 889.9355\n",
      "Training Epoch: 10 [15600/36045]\tLoss: 982.1241\n",
      "Training Epoch: 10 [15650/36045]\tLoss: 1011.6197\n",
      "Training Epoch: 10 [15700/36045]\tLoss: 990.6481\n",
      "Training Epoch: 10 [15750/36045]\tLoss: 976.7835\n",
      "Training Epoch: 10 [15800/36045]\tLoss: 888.5619\n",
      "Training Epoch: 10 [15850/36045]\tLoss: 897.1960\n",
      "Training Epoch: 10 [15900/36045]\tLoss: 909.0146\n",
      "Training Epoch: 10 [15950/36045]\tLoss: 939.1352\n",
      "Training Epoch: 10 [16000/36045]\tLoss: 931.8563\n",
      "Training Epoch: 10 [16050/36045]\tLoss: 890.5903\n",
      "Training Epoch: 10 [16100/36045]\tLoss: 825.6358\n",
      "Training Epoch: 10 [16150/36045]\tLoss: 806.1155\n",
      "Training Epoch: 10 [16200/36045]\tLoss: 963.7104\n",
      "Training Epoch: 10 [16250/36045]\tLoss: 1000.1931\n",
      "Training Epoch: 10 [16300/36045]\tLoss: 1081.8873\n",
      "Training Epoch: 10 [16350/36045]\tLoss: 1095.7281\n",
      "Training Epoch: 10 [16400/36045]\tLoss: 1073.2393\n",
      "Training Epoch: 10 [16450/36045]\tLoss: 1050.7378\n",
      "Training Epoch: 10 [16500/36045]\tLoss: 1047.1257\n",
      "Training Epoch: 10 [16550/36045]\tLoss: 997.3525\n",
      "Training Epoch: 10 [16600/36045]\tLoss: 1046.6447\n",
      "Training Epoch: 10 [16650/36045]\tLoss: 1075.7433\n",
      "Training Epoch: 10 [16700/36045]\tLoss: 1049.0267\n",
      "Training Epoch: 10 [16750/36045]\tLoss: 1037.1893\n",
      "Training Epoch: 10 [16800/36045]\tLoss: 1057.8384\n",
      "Training Epoch: 10 [16850/36045]\tLoss: 1006.2349\n",
      "Training Epoch: 10 [16900/36045]\tLoss: 1023.0393\n",
      "Training Epoch: 10 [16950/36045]\tLoss: 1045.0381\n",
      "Training Epoch: 10 [17000/36045]\tLoss: 1019.0403\n",
      "Training Epoch: 10 [17050/36045]\tLoss: 1074.0432\n",
      "Training Epoch: 10 [17100/36045]\tLoss: 1081.8888\n",
      "Training Epoch: 10 [17150/36045]\tLoss: 943.1151\n",
      "Training Epoch: 10 [17200/36045]\tLoss: 891.5815\n",
      "Training Epoch: 10 [17250/36045]\tLoss: 931.2369\n",
      "Training Epoch: 10 [17300/36045]\tLoss: 983.9480\n",
      "Training Epoch: 10 [17350/36045]\tLoss: 932.8047\n",
      "Training Epoch: 10 [17400/36045]\tLoss: 947.8333\n",
      "Training Epoch: 10 [17450/36045]\tLoss: 976.3024\n",
      "Training Epoch: 10 [17500/36045]\tLoss: 960.5144\n",
      "Training Epoch: 10 [17550/36045]\tLoss: 974.8555\n",
      "Training Epoch: 10 [17600/36045]\tLoss: 961.5266\n",
      "Training Epoch: 10 [17650/36045]\tLoss: 989.2944\n",
      "Training Epoch: 10 [17700/36045]\tLoss: 962.7611\n",
      "Training Epoch: 10 [17750/36045]\tLoss: 985.2587\n",
      "Training Epoch: 10 [17800/36045]\tLoss: 978.0787\n",
      "Training Epoch: 10 [17850/36045]\tLoss: 936.7929\n",
      "Training Epoch: 10 [17900/36045]\tLoss: 972.4822\n",
      "Training Epoch: 10 [17950/36045]\tLoss: 981.3199\n",
      "Training Epoch: 10 [18000/36045]\tLoss: 969.3284\n",
      "Training Epoch: 10 [18050/36045]\tLoss: 1105.1903\n",
      "Training Epoch: 10 [18100/36045]\tLoss: 1114.1824\n",
      "Training Epoch: 10 [18150/36045]\tLoss: 1117.5507\n",
      "Training Epoch: 10 [18200/36045]\tLoss: 1107.6042\n",
      "Training Epoch: 10 [18250/36045]\tLoss: 1129.5958\n",
      "Training Epoch: 10 [18300/36045]\tLoss: 1031.6508\n",
      "Training Epoch: 10 [18350/36045]\tLoss: 1093.1144\n",
      "Training Epoch: 10 [18400/36045]\tLoss: 1066.9395\n",
      "Training Epoch: 10 [18450/36045]\tLoss: 1045.3813\n",
      "Training Epoch: 10 [18500/36045]\tLoss: 1047.6628\n",
      "Training Epoch: 10 [18550/36045]\tLoss: 1028.5933\n",
      "Training Epoch: 10 [18600/36045]\tLoss: 1017.5794\n",
      "Training Epoch: 10 [18650/36045]\tLoss: 1073.7230\n",
      "Training Epoch: 10 [18700/36045]\tLoss: 1132.1019\n",
      "Training Epoch: 10 [18750/36045]\tLoss: 1111.4905\n",
      "Training Epoch: 10 [18800/36045]\tLoss: 1145.6700\n",
      "Training Epoch: 10 [18850/36045]\tLoss: 1082.6862\n",
      "Training Epoch: 10 [18900/36045]\tLoss: 1159.0410\n",
      "Training Epoch: 10 [18950/36045]\tLoss: 1081.7286\n",
      "Training Epoch: 10 [19000/36045]\tLoss: 946.2906\n",
      "Training Epoch: 10 [19050/36045]\tLoss: 908.4516\n",
      "Training Epoch: 10 [19100/36045]\tLoss: 932.6381\n",
      "Training Epoch: 10 [19150/36045]\tLoss: 916.0697\n",
      "Training Epoch: 10 [19200/36045]\tLoss: 954.4518\n",
      "Training Epoch: 10 [19250/36045]\tLoss: 963.0166\n",
      "Training Epoch: 10 [19300/36045]\tLoss: 991.3470\n",
      "Training Epoch: 10 [19350/36045]\tLoss: 965.6061\n",
      "Training Epoch: 10 [19400/36045]\tLoss: 992.2157\n",
      "Training Epoch: 10 [19450/36045]\tLoss: 979.0933\n",
      "Training Epoch: 10 [19500/36045]\tLoss: 982.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [19550/36045]\tLoss: 986.7328\n",
      "Training Epoch: 10 [19600/36045]\tLoss: 1033.7952\n",
      "Training Epoch: 10 [19650/36045]\tLoss: 1311.2805\n",
      "Training Epoch: 10 [19700/36045]\tLoss: 1259.5779\n",
      "Training Epoch: 10 [19750/36045]\tLoss: 1257.3359\n",
      "Training Epoch: 10 [19800/36045]\tLoss: 1251.1594\n",
      "Training Epoch: 10 [19850/36045]\tLoss: 884.6912\n",
      "Training Epoch: 10 [19900/36045]\tLoss: 851.4698\n",
      "Training Epoch: 10 [19950/36045]\tLoss: 857.9194\n",
      "Training Epoch: 10 [20000/36045]\tLoss: 857.6582\n",
      "Training Epoch: 10 [20050/36045]\tLoss: 960.0266\n",
      "Training Epoch: 10 [20100/36045]\tLoss: 960.4567\n",
      "Training Epoch: 10 [20150/36045]\tLoss: 966.7260\n",
      "Training Epoch: 10 [20200/36045]\tLoss: 960.9763\n",
      "Training Epoch: 10 [20250/36045]\tLoss: 1019.0248\n",
      "Training Epoch: 10 [20300/36045]\tLoss: 1057.5878\n",
      "Training Epoch: 10 [20350/36045]\tLoss: 1091.0532\n",
      "Training Epoch: 10 [20400/36045]\tLoss: 1107.2909\n",
      "Training Epoch: 10 [20450/36045]\tLoss: 1085.6692\n",
      "Training Epoch: 10 [20500/36045]\tLoss: 1053.2180\n",
      "Training Epoch: 10 [20550/36045]\tLoss: 950.3207\n",
      "Training Epoch: 10 [20600/36045]\tLoss: 969.0132\n",
      "Training Epoch: 10 [20650/36045]\tLoss: 962.4804\n",
      "Training Epoch: 10 [20700/36045]\tLoss: 942.2443\n",
      "Training Epoch: 10 [20750/36045]\tLoss: 1003.3160\n",
      "Training Epoch: 10 [20800/36045]\tLoss: 1093.5006\n",
      "Training Epoch: 10 [20850/36045]\tLoss: 1083.8873\n",
      "Training Epoch: 10 [20900/36045]\tLoss: 1148.4808\n",
      "Training Epoch: 10 [20950/36045]\tLoss: 1084.0127\n",
      "Training Epoch: 10 [21000/36045]\tLoss: 1019.1751\n",
      "Training Epoch: 10 [21050/36045]\tLoss: 867.3903\n",
      "Training Epoch: 10 [21100/36045]\tLoss: 868.7937\n",
      "Training Epoch: 10 [21150/36045]\tLoss: 929.3209\n",
      "Training Epoch: 10 [21200/36045]\tLoss: 931.8190\n",
      "Training Epoch: 10 [21250/36045]\tLoss: 887.0771\n",
      "Training Epoch: 10 [21300/36045]\tLoss: 1044.4907\n",
      "Training Epoch: 10 [21350/36045]\tLoss: 1041.5609\n",
      "Training Epoch: 10 [21400/36045]\tLoss: 1049.1888\n",
      "Training Epoch: 10 [21450/36045]\tLoss: 1067.3063\n",
      "Training Epoch: 10 [21500/36045]\tLoss: 1064.0380\n",
      "Training Epoch: 10 [21550/36045]\tLoss: 1171.7694\n",
      "Training Epoch: 10 [21600/36045]\tLoss: 1172.7249\n",
      "Training Epoch: 10 [21650/36045]\tLoss: 1191.1180\n",
      "Training Epoch: 10 [21700/36045]\tLoss: 1187.1412\n",
      "Training Epoch: 10 [21750/36045]\tLoss: 1150.2609\n",
      "Training Epoch: 10 [21800/36045]\tLoss: 871.7760\n",
      "Training Epoch: 10 [21850/36045]\tLoss: 845.2708\n",
      "Training Epoch: 10 [21900/36045]\tLoss: 864.0895\n",
      "Training Epoch: 10 [21950/36045]\tLoss: 856.8375\n",
      "Training Epoch: 10 [22000/36045]\tLoss: 868.6154\n",
      "Training Epoch: 10 [22050/36045]\tLoss: 921.4268\n",
      "Training Epoch: 10 [22100/36045]\tLoss: 907.5272\n",
      "Training Epoch: 10 [22150/36045]\tLoss: 880.1135\n",
      "Training Epoch: 10 [22200/36045]\tLoss: 908.0137\n",
      "Training Epoch: 10 [22250/36045]\tLoss: 918.7913\n",
      "Training Epoch: 10 [22300/36045]\tLoss: 982.4964\n",
      "Training Epoch: 10 [22350/36045]\tLoss: 1011.8906\n",
      "Training Epoch: 10 [22400/36045]\tLoss: 1042.0594\n",
      "Training Epoch: 10 [22450/36045]\tLoss: 1019.5366\n",
      "Training Epoch: 10 [22500/36045]\tLoss: 991.4253\n",
      "Training Epoch: 10 [22550/36045]\tLoss: 1055.8033\n",
      "Training Epoch: 10 [22600/36045]\tLoss: 1163.8751\n",
      "Training Epoch: 10 [22650/36045]\tLoss: 1218.7367\n",
      "Training Epoch: 10 [22700/36045]\tLoss: 1246.6136\n",
      "Training Epoch: 10 [22750/36045]\tLoss: 1273.0161\n",
      "Training Epoch: 10 [22800/36045]\tLoss: 1330.6141\n",
      "Training Epoch: 10 [22850/36045]\tLoss: 1102.2472\n",
      "Training Epoch: 10 [22900/36045]\tLoss: 1105.9421\n",
      "Training Epoch: 10 [22950/36045]\tLoss: 1073.7013\n",
      "Training Epoch: 10 [23000/36045]\tLoss: 1082.2671\n",
      "Training Epoch: 10 [23050/36045]\tLoss: 970.1393\n",
      "Training Epoch: 10 [23100/36045]\tLoss: 989.8531\n",
      "Training Epoch: 10 [23150/36045]\tLoss: 970.2355\n",
      "Training Epoch: 10 [23200/36045]\tLoss: 914.5322\n",
      "Training Epoch: 10 [23250/36045]\tLoss: 922.1232\n",
      "Training Epoch: 10 [23300/36045]\tLoss: 914.3858\n",
      "Training Epoch: 10 [23350/36045]\tLoss: 941.0610\n",
      "Training Epoch: 10 [23400/36045]\tLoss: 1022.9227\n",
      "Training Epoch: 10 [23450/36045]\tLoss: 1009.6309\n",
      "Training Epoch: 10 [23500/36045]\tLoss: 967.9491\n",
      "Training Epoch: 10 [23550/36045]\tLoss: 1047.8093\n",
      "Training Epoch: 10 [23600/36045]\tLoss: 1175.9178\n",
      "Training Epoch: 10 [23650/36045]\tLoss: 1205.0952\n",
      "Training Epoch: 10 [23700/36045]\tLoss: 1214.5718\n",
      "Training Epoch: 10 [23750/36045]\tLoss: 1177.9669\n",
      "Training Epoch: 10 [23800/36045]\tLoss: 935.4341\n",
      "Training Epoch: 10 [23850/36045]\tLoss: 972.8513\n",
      "Training Epoch: 10 [23900/36045]\tLoss: 960.2837\n",
      "Training Epoch: 10 [23950/36045]\tLoss: 940.5536\n",
      "Training Epoch: 10 [24000/36045]\tLoss: 914.6950\n",
      "Training Epoch: 10 [24050/36045]\tLoss: 836.6888\n",
      "Training Epoch: 10 [24100/36045]\tLoss: 875.2432\n",
      "Training Epoch: 10 [24150/36045]\tLoss: 875.9688\n",
      "Training Epoch: 10 [24200/36045]\tLoss: 856.6385\n",
      "Training Epoch: 10 [24250/36045]\tLoss: 835.4097\n",
      "Training Epoch: 10 [24300/36045]\tLoss: 908.7475\n",
      "Training Epoch: 10 [24350/36045]\tLoss: 935.2540\n",
      "Training Epoch: 10 [24400/36045]\tLoss: 958.7921\n",
      "Training Epoch: 10 [24450/36045]\tLoss: 920.0573\n",
      "Training Epoch: 10 [24500/36045]\tLoss: 970.6812\n",
      "Training Epoch: 10 [24550/36045]\tLoss: 1065.3689\n",
      "Training Epoch: 10 [24600/36045]\tLoss: 1058.0375\n",
      "Training Epoch: 10 [24650/36045]\tLoss: 1022.7668\n",
      "Training Epoch: 10 [24700/36045]\tLoss: 1039.0627\n",
      "Training Epoch: 10 [24750/36045]\tLoss: 958.2579\n",
      "Training Epoch: 10 [24800/36045]\tLoss: 840.4879\n",
      "Training Epoch: 10 [24850/36045]\tLoss: 857.5515\n",
      "Training Epoch: 10 [24900/36045]\tLoss: 857.6233\n",
      "Training Epoch: 10 [24950/36045]\tLoss: 862.7100\n",
      "Training Epoch: 10 [25000/36045]\tLoss: 824.6647\n",
      "Training Epoch: 10 [25050/36045]\tLoss: 778.9022\n",
      "Training Epoch: 10 [25100/36045]\tLoss: 696.5161\n",
      "Training Epoch: 10 [25150/36045]\tLoss: 649.2302\n",
      "Training Epoch: 10 [25200/36045]\tLoss: 648.0424\n",
      "Training Epoch: 10 [25250/36045]\tLoss: 687.8907\n",
      "Training Epoch: 10 [25300/36045]\tLoss: 890.0938\n",
      "Training Epoch: 10 [25350/36045]\tLoss: 889.3030\n",
      "Training Epoch: 10 [25400/36045]\tLoss: 832.1893\n",
      "Training Epoch: 10 [25450/36045]\tLoss: 833.6905\n",
      "Training Epoch: 10 [25500/36045]\tLoss: 906.8616\n",
      "Training Epoch: 10 [25550/36045]\tLoss: 1067.2229\n",
      "Training Epoch: 10 [25600/36045]\tLoss: 1073.0371\n",
      "Training Epoch: 10 [25650/36045]\tLoss: 1034.0841\n",
      "Training Epoch: 10 [25700/36045]\tLoss: 1059.5741\n",
      "Training Epoch: 10 [25750/36045]\tLoss: 1010.3276\n",
      "Training Epoch: 10 [25800/36045]\tLoss: 622.6059\n",
      "Training Epoch: 10 [25850/36045]\tLoss: 634.1234\n",
      "Training Epoch: 10 [25900/36045]\tLoss: 608.1307\n",
      "Training Epoch: 10 [25950/36045]\tLoss: 622.3597\n",
      "Training Epoch: 10 [26000/36045]\tLoss: 766.3593\n",
      "Training Epoch: 10 [26050/36045]\tLoss: 1038.4209\n",
      "Training Epoch: 10 [26100/36045]\tLoss: 1077.4241\n",
      "Training Epoch: 10 [26150/36045]\tLoss: 1068.3911\n",
      "Training Epoch: 10 [26200/36045]\tLoss: 1041.3585\n",
      "Training Epoch: 10 [26250/36045]\tLoss: 1075.0319\n",
      "Training Epoch: 10 [26300/36045]\tLoss: 930.0424\n",
      "Training Epoch: 10 [26350/36045]\tLoss: 934.6116\n",
      "Training Epoch: 10 [26400/36045]\tLoss: 917.2032\n",
      "Training Epoch: 10 [26450/36045]\tLoss: 840.6826\n",
      "Training Epoch: 10 [26500/36045]\tLoss: 1013.3179\n",
      "Training Epoch: 10 [26550/36045]\tLoss: 1034.8812\n",
      "Training Epoch: 10 [26600/36045]\tLoss: 1027.7003\n",
      "Training Epoch: 10 [26650/36045]\tLoss: 1049.5961\n",
      "Training Epoch: 10 [26700/36045]\tLoss: 1025.3621\n",
      "Training Epoch: 10 [26750/36045]\tLoss: 960.7331\n",
      "Training Epoch: 10 [26800/36045]\tLoss: 702.7979\n",
      "Training Epoch: 10 [26850/36045]\tLoss: 591.9802\n",
      "Training Epoch: 10 [26900/36045]\tLoss: 596.7183\n",
      "Training Epoch: 10 [26950/36045]\tLoss: 650.7592\n",
      "Training Epoch: 10 [27000/36045]\tLoss: 1023.2632\n",
      "Training Epoch: 10 [27050/36045]\tLoss: 1081.0520\n",
      "Training Epoch: 10 [27100/36045]\tLoss: 1049.1860\n",
      "Training Epoch: 10 [27150/36045]\tLoss: 1099.1243\n",
      "Training Epoch: 10 [27200/36045]\tLoss: 825.6143\n",
      "Training Epoch: 10 [27250/36045]\tLoss: 823.0923\n",
      "Training Epoch: 10 [27300/36045]\tLoss: 794.3910\n",
      "Training Epoch: 10 [27350/36045]\tLoss: 804.1003\n",
      "Training Epoch: 10 [27400/36045]\tLoss: 799.6588\n",
      "Training Epoch: 10 [27450/36045]\tLoss: 1004.2755\n",
      "Training Epoch: 10 [27500/36045]\tLoss: 1075.8418\n",
      "Training Epoch: 10 [27550/36045]\tLoss: 1068.0056\n",
      "Training Epoch: 10 [27600/36045]\tLoss: 1071.3629\n",
      "Training Epoch: 10 [27650/36045]\tLoss: 1072.7791\n",
      "Training Epoch: 10 [27700/36045]\tLoss: 1103.3651\n",
      "Training Epoch: 10 [27750/36045]\tLoss: 1120.0677\n",
      "Training Epoch: 10 [27800/36045]\tLoss: 1101.0796\n",
      "Training Epoch: 10 [27850/36045]\tLoss: 1078.3826\n",
      "Training Epoch: 10 [27900/36045]\tLoss: 955.4048\n",
      "Training Epoch: 10 [27950/36045]\tLoss: 788.0478\n",
      "Training Epoch: 10 [28000/36045]\tLoss: 755.0427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [28050/36045]\tLoss: 780.8436\n",
      "Training Epoch: 10 [28100/36045]\tLoss: 765.9736\n",
      "Training Epoch: 10 [28150/36045]\tLoss: 827.0750\n",
      "Training Epoch: 10 [28200/36045]\tLoss: 830.5417\n",
      "Training Epoch: 10 [28250/36045]\tLoss: 835.0803\n",
      "Training Epoch: 10 [28300/36045]\tLoss: 790.3501\n",
      "Training Epoch: 10 [28350/36045]\tLoss: 790.5559\n",
      "Training Epoch: 10 [28400/36045]\tLoss: 1147.4231\n",
      "Training Epoch: 10 [28450/36045]\tLoss: 1019.7733\n",
      "Training Epoch: 10 [28500/36045]\tLoss: 880.4508\n",
      "Training Epoch: 10 [28550/36045]\tLoss: 806.5997\n",
      "Training Epoch: 10 [28600/36045]\tLoss: 918.9921\n",
      "Training Epoch: 10 [28650/36045]\tLoss: 1094.3402\n",
      "Training Epoch: 10 [28700/36045]\tLoss: 1084.5697\n",
      "Training Epoch: 10 [28750/36045]\tLoss: 1065.6812\n",
      "Training Epoch: 10 [28800/36045]\tLoss: 1079.7893\n",
      "Training Epoch: 10 [28850/36045]\tLoss: 933.9625\n",
      "Training Epoch: 10 [28900/36045]\tLoss: 735.8467\n",
      "Training Epoch: 10 [28950/36045]\tLoss: 722.4310\n",
      "Training Epoch: 10 [29000/36045]\tLoss: 728.4326\n",
      "Training Epoch: 10 [29050/36045]\tLoss: 743.1515\n",
      "Training Epoch: 10 [29100/36045]\tLoss: 766.1314\n",
      "Training Epoch: 10 [29150/36045]\tLoss: 743.8255\n",
      "Training Epoch: 10 [29200/36045]\tLoss: 729.2115\n",
      "Training Epoch: 10 [29250/36045]\tLoss: 706.3915\n",
      "Training Epoch: 10 [29300/36045]\tLoss: 827.5850\n",
      "Training Epoch: 10 [29350/36045]\tLoss: 998.8087\n",
      "Training Epoch: 10 [29400/36045]\tLoss: 1021.8494\n",
      "Training Epoch: 10 [29450/36045]\tLoss: 1065.4294\n",
      "Training Epoch: 10 [29500/36045]\tLoss: 1085.8279\n",
      "Training Epoch: 10 [29550/36045]\tLoss: 1029.0912\n",
      "Training Epoch: 10 [29600/36045]\tLoss: 886.4733\n",
      "Training Epoch: 10 [29650/36045]\tLoss: 861.4418\n",
      "Training Epoch: 10 [29700/36045]\tLoss: 760.1739\n",
      "Training Epoch: 10 [29750/36045]\tLoss: 768.9344\n",
      "Training Epoch: 10 [29800/36045]\tLoss: 823.3932\n",
      "Training Epoch: 10 [29850/36045]\tLoss: 889.5862\n",
      "Training Epoch: 10 [29900/36045]\tLoss: 880.2125\n",
      "Training Epoch: 10 [29950/36045]\tLoss: 901.6636\n",
      "Training Epoch: 10 [30000/36045]\tLoss: 893.6083\n",
      "Training Epoch: 10 [30050/36045]\tLoss: 899.9634\n",
      "Training Epoch: 10 [30100/36045]\tLoss: 1104.3322\n",
      "Training Epoch: 10 [30150/36045]\tLoss: 1096.4304\n",
      "Training Epoch: 10 [30200/36045]\tLoss: 1037.0696\n",
      "Training Epoch: 10 [30250/36045]\tLoss: 1093.7899\n",
      "Training Epoch: 10 [30300/36045]\tLoss: 1086.9812\n",
      "Training Epoch: 10 [30350/36045]\tLoss: 886.5394\n",
      "Training Epoch: 10 [30400/36045]\tLoss: 875.9928\n",
      "Training Epoch: 10 [30450/36045]\tLoss: 874.1823\n",
      "Training Epoch: 10 [30500/36045]\tLoss: 813.5031\n",
      "Training Epoch: 10 [30550/36045]\tLoss: 754.2043\n",
      "Training Epoch: 10 [30600/36045]\tLoss: 718.0820\n",
      "Training Epoch: 10 [30650/36045]\tLoss: 708.8185\n",
      "Training Epoch: 10 [30700/36045]\tLoss: 734.5115\n",
      "Training Epoch: 10 [30750/36045]\tLoss: 714.1510\n",
      "Training Epoch: 10 [30800/36045]\tLoss: 755.7966\n",
      "Training Epoch: 10 [30850/36045]\tLoss: 749.5779\n",
      "Training Epoch: 10 [30900/36045]\tLoss: 771.1608\n",
      "Training Epoch: 10 [30950/36045]\tLoss: 813.2473\n",
      "Training Epoch: 10 [31000/36045]\tLoss: 798.2239\n",
      "Training Epoch: 10 [31050/36045]\tLoss: 666.3936\n",
      "Training Epoch: 10 [31100/36045]\tLoss: 651.0497\n",
      "Training Epoch: 10 [31150/36045]\tLoss: 656.0666\n",
      "Training Epoch: 10 [31200/36045]\tLoss: 828.0527\n",
      "Training Epoch: 10 [31250/36045]\tLoss: 1071.4120\n",
      "Training Epoch: 10 [31300/36045]\tLoss: 1025.3351\n",
      "Training Epoch: 10 [31350/36045]\tLoss: 1042.7100\n",
      "Training Epoch: 10 [31400/36045]\tLoss: 1029.0081\n",
      "Training Epoch: 10 [31450/36045]\tLoss: 1017.6034\n",
      "Training Epoch: 10 [31500/36045]\tLoss: 1018.2418\n",
      "Training Epoch: 10 [31550/36045]\tLoss: 1033.6931\n",
      "Training Epoch: 10 [31600/36045]\tLoss: 969.8718\n",
      "Training Epoch: 10 [31650/36045]\tLoss: 1037.4690\n",
      "Training Epoch: 10 [31700/36045]\tLoss: 778.2088\n",
      "Training Epoch: 10 [31750/36045]\tLoss: 652.4903\n",
      "Training Epoch: 10 [31800/36045]\tLoss: 620.6085\n",
      "Training Epoch: 10 [31850/36045]\tLoss: 640.9532\n",
      "Training Epoch: 10 [31900/36045]\tLoss: 959.2361\n",
      "Training Epoch: 10 [31950/36045]\tLoss: 1193.9854\n",
      "Training Epoch: 10 [32000/36045]\tLoss: 1335.6549\n",
      "Training Epoch: 10 [32050/36045]\tLoss: 1279.8813\n",
      "Training Epoch: 10 [32100/36045]\tLoss: 1260.6908\n",
      "Training Epoch: 10 [32150/36045]\tLoss: 1040.6115\n",
      "Training Epoch: 10 [32200/36045]\tLoss: 1050.0439\n",
      "Training Epoch: 10 [32250/36045]\tLoss: 1066.1611\n",
      "Training Epoch: 10 [32300/36045]\tLoss: 1045.6377\n",
      "Training Epoch: 10 [32350/36045]\tLoss: 1027.1393\n",
      "Training Epoch: 10 [32400/36045]\tLoss: 970.4167\n",
      "Training Epoch: 10 [32450/36045]\tLoss: 810.6703\n",
      "Training Epoch: 10 [32500/36045]\tLoss: 778.3699\n",
      "Training Epoch: 10 [32550/36045]\tLoss: 786.9687\n",
      "Training Epoch: 10 [32600/36045]\tLoss: 780.3989\n",
      "Training Epoch: 10 [32650/36045]\tLoss: 960.6433\n",
      "Training Epoch: 10 [32700/36045]\tLoss: 1037.6957\n",
      "Training Epoch: 10 [32750/36045]\tLoss: 992.5105\n",
      "Training Epoch: 10 [32800/36045]\tLoss: 1018.4985\n",
      "Training Epoch: 10 [32850/36045]\tLoss: 950.7172\n",
      "Training Epoch: 10 [32900/36045]\tLoss: 773.1917\n",
      "Training Epoch: 10 [32950/36045]\tLoss: 810.4310\n",
      "Training Epoch: 10 [33000/36045]\tLoss: 817.9433\n",
      "Training Epoch: 10 [33050/36045]\tLoss: 769.2114\n",
      "Training Epoch: 10 [33100/36045]\tLoss: 877.1070\n",
      "Training Epoch: 10 [33150/36045]\tLoss: 1161.9065\n",
      "Training Epoch: 10 [33200/36045]\tLoss: 1134.5388\n",
      "Training Epoch: 10 [33250/36045]\tLoss: 1164.8484\n",
      "Training Epoch: 10 [33300/36045]\tLoss: 1236.9974\n",
      "Training Epoch: 10 [33350/36045]\tLoss: 960.9224\n",
      "Training Epoch: 10 [33400/36045]\tLoss: 729.1033\n",
      "Training Epoch: 10 [33450/36045]\tLoss: 724.1051\n",
      "Training Epoch: 10 [33500/36045]\tLoss: 741.5472\n",
      "Training Epoch: 10 [33550/36045]\tLoss: 768.3255\n",
      "Training Epoch: 10 [33600/36045]\tLoss: 775.3744\n",
      "Training Epoch: 10 [33650/36045]\tLoss: 1001.5429\n",
      "Training Epoch: 10 [33700/36045]\tLoss: 970.0826\n",
      "Training Epoch: 10 [33750/36045]\tLoss: 1000.6779\n",
      "Training Epoch: 10 [33800/36045]\tLoss: 997.9021\n",
      "Training Epoch: 10 [33850/36045]\tLoss: 1003.7600\n",
      "Training Epoch: 10 [33900/36045]\tLoss: 1020.7063\n",
      "Training Epoch: 10 [33950/36045]\tLoss: 1032.8550\n",
      "Training Epoch: 10 [34000/36045]\tLoss: 1022.3669\n",
      "Training Epoch: 10 [34050/36045]\tLoss: 1034.6476\n",
      "Training Epoch: 10 [34100/36045]\tLoss: 987.9788\n",
      "Training Epoch: 10 [34150/36045]\tLoss: 918.3258\n",
      "Training Epoch: 10 [34200/36045]\tLoss: 875.9586\n",
      "Training Epoch: 10 [34250/36045]\tLoss: 892.7069\n",
      "Training Epoch: 10 [34300/36045]\tLoss: 769.1049\n",
      "Training Epoch: 10 [34350/36045]\tLoss: 803.8519\n",
      "Training Epoch: 10 [34400/36045]\tLoss: 782.7643\n",
      "Training Epoch: 10 [34450/36045]\tLoss: 734.3907\n",
      "Training Epoch: 10 [34500/36045]\tLoss: 787.3365\n",
      "Training Epoch: 10 [34550/36045]\tLoss: 780.5005\n",
      "Training Epoch: 10 [34600/36045]\tLoss: 754.0314\n",
      "Training Epoch: 10 [34650/36045]\tLoss: 885.5327\n",
      "Training Epoch: 10 [34700/36045]\tLoss: 931.9656\n",
      "Training Epoch: 10 [34750/36045]\tLoss: 830.7836\n",
      "Training Epoch: 10 [34800/36045]\tLoss: 935.8689\n",
      "Training Epoch: 10 [34850/36045]\tLoss: 958.2044\n",
      "Training Epoch: 10 [34900/36045]\tLoss: 1136.6777\n",
      "Training Epoch: 10 [34950/36045]\tLoss: 1124.5873\n",
      "Training Epoch: 10 [35000/36045]\tLoss: 1136.7926\n",
      "Training Epoch: 10 [35050/36045]\tLoss: 1112.1600\n",
      "Training Epoch: 10 [35100/36045]\tLoss: 873.2200\n",
      "Training Epoch: 10 [35150/36045]\tLoss: 859.0943\n",
      "Training Epoch: 10 [35200/36045]\tLoss: 748.5991\n",
      "Training Epoch: 10 [35250/36045]\tLoss: 814.4789\n",
      "Training Epoch: 10 [35300/36045]\tLoss: 825.5043\n",
      "Training Epoch: 10 [35350/36045]\tLoss: 982.2936\n",
      "Training Epoch: 10 [35400/36045]\tLoss: 1048.6985\n",
      "Training Epoch: 10 [35450/36045]\tLoss: 1002.5828\n",
      "Training Epoch: 10 [35500/36045]\tLoss: 980.8765\n",
      "Training Epoch: 10 [35550/36045]\tLoss: 965.9772\n",
      "Training Epoch: 10 [35600/36045]\tLoss: 1006.3502\n",
      "Training Epoch: 10 [35650/36045]\tLoss: 1095.4410\n",
      "Training Epoch: 10 [35700/36045]\tLoss: 1018.6247\n",
      "Training Epoch: 10 [35750/36045]\tLoss: 1087.5988\n",
      "Training Epoch: 10 [35800/36045]\tLoss: 1093.5706\n",
      "Training Epoch: 10 [35850/36045]\tLoss: 1066.8199\n",
      "Training Epoch: 10 [35900/36045]\tLoss: 1115.0386\n",
      "Training Epoch: 10 [35950/36045]\tLoss: 1117.0647\n",
      "Training Epoch: 10 [36000/36045]\tLoss: 1097.4326\n",
      "Training Epoch: 10 [36045/36045]\tLoss: 1079.9852\n",
      "Training Epoch: 10 [4004/4004]\tLoss: 1020.3236\n",
      "Training Epoch: 11 [50/36045]\tLoss: 1028.7762\n",
      "Training Epoch: 11 [100/36045]\tLoss: 990.9352\n",
      "Training Epoch: 11 [150/36045]\tLoss: 992.4228\n",
      "Training Epoch: 11 [200/36045]\tLoss: 974.4723\n",
      "Training Epoch: 11 [250/36045]\tLoss: 1120.4821\n",
      "Training Epoch: 11 [300/36045]\tLoss: 1184.7452\n",
      "Training Epoch: 11 [350/36045]\tLoss: 1135.6737\n",
      "Training Epoch: 11 [400/36045]\tLoss: 1149.7411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [450/36045]\tLoss: 1126.1289\n",
      "Training Epoch: 11 [500/36045]\tLoss: 1064.3213\n",
      "Training Epoch: 11 [550/36045]\tLoss: 1082.1101\n",
      "Training Epoch: 11 [600/36045]\tLoss: 1039.7836\n",
      "Training Epoch: 11 [650/36045]\tLoss: 1076.2438\n",
      "Training Epoch: 11 [700/36045]\tLoss: 1067.8259\n",
      "Training Epoch: 11 [750/36045]\tLoss: 1034.8325\n",
      "Training Epoch: 11 [800/36045]\tLoss: 1054.3324\n",
      "Training Epoch: 11 [850/36045]\tLoss: 1020.2917\n",
      "Training Epoch: 11 [900/36045]\tLoss: 975.0836\n",
      "Training Epoch: 11 [950/36045]\tLoss: 933.4780\n",
      "Training Epoch: 11 [1000/36045]\tLoss: 898.9559\n",
      "Training Epoch: 11 [1050/36045]\tLoss: 903.4911\n",
      "Training Epoch: 11 [1100/36045]\tLoss: 880.4479\n",
      "Training Epoch: 11 [1150/36045]\tLoss: 881.4332\n",
      "Training Epoch: 11 [1200/36045]\tLoss: 933.3729\n",
      "Training Epoch: 11 [1250/36045]\tLoss: 1047.8975\n",
      "Training Epoch: 11 [1300/36045]\tLoss: 1046.2140\n",
      "Training Epoch: 11 [1350/36045]\tLoss: 1055.6136\n",
      "Training Epoch: 11 [1400/36045]\tLoss: 1095.4486\n",
      "Training Epoch: 11 [1450/36045]\tLoss: 1056.0381\n",
      "Training Epoch: 11 [1500/36045]\tLoss: 984.9177\n",
      "Training Epoch: 11 [1550/36045]\tLoss: 1016.5682\n",
      "Training Epoch: 11 [1600/36045]\tLoss: 1025.7345\n",
      "Training Epoch: 11 [1650/36045]\tLoss: 1010.5194\n",
      "Training Epoch: 11 [1700/36045]\tLoss: 1023.2250\n",
      "Training Epoch: 11 [1750/36045]\tLoss: 1074.7588\n",
      "Training Epoch: 11 [1800/36045]\tLoss: 1055.6011\n",
      "Training Epoch: 11 [1850/36045]\tLoss: 1096.0413\n",
      "Training Epoch: 11 [1900/36045]\tLoss: 1027.3401\n",
      "Training Epoch: 11 [1950/36045]\tLoss: 1035.6538\n",
      "Training Epoch: 11 [2000/36045]\tLoss: 934.7733\n",
      "Training Epoch: 11 [2050/36045]\tLoss: 943.2322\n",
      "Training Epoch: 11 [2100/36045]\tLoss: 995.1628\n",
      "Training Epoch: 11 [2150/36045]\tLoss: 965.9558\n",
      "Training Epoch: 11 [2200/36045]\tLoss: 897.2325\n",
      "Training Epoch: 11 [2250/36045]\tLoss: 847.0105\n",
      "Training Epoch: 11 [2300/36045]\tLoss: 885.8102\n",
      "Training Epoch: 11 [2350/36045]\tLoss: 845.6182\n",
      "Training Epoch: 11 [2400/36045]\tLoss: 870.6238\n",
      "Training Epoch: 11 [2450/36045]\tLoss: 1076.4908\n",
      "Training Epoch: 11 [2500/36045]\tLoss: 1122.4091\n",
      "Training Epoch: 11 [2550/36045]\tLoss: 1120.2710\n",
      "Training Epoch: 11 [2600/36045]\tLoss: 1127.0645\n",
      "Training Epoch: 11 [2650/36045]\tLoss: 1257.9343\n",
      "Training Epoch: 11 [2700/36045]\tLoss: 1346.3223\n",
      "Training Epoch: 11 [2750/36045]\tLoss: 1428.4700\n",
      "Training Epoch: 11 [2800/36045]\tLoss: 1448.1345\n",
      "Training Epoch: 11 [2850/36045]\tLoss: 1229.0917\n",
      "Training Epoch: 11 [2900/36045]\tLoss: 1212.0663\n",
      "Training Epoch: 11 [2950/36045]\tLoss: 1164.5126\n",
      "Training Epoch: 11 [3000/36045]\tLoss: 1163.5731\n",
      "Training Epoch: 11 [3050/36045]\tLoss: 1209.5077\n",
      "Training Epoch: 11 [3100/36045]\tLoss: 1097.9093\n",
      "Training Epoch: 11 [3150/36045]\tLoss: 849.1400\n",
      "Training Epoch: 11 [3200/36045]\tLoss: 883.9644\n",
      "Training Epoch: 11 [3250/36045]\tLoss: 830.4260\n",
      "Training Epoch: 11 [3300/36045]\tLoss: 785.0591\n",
      "Training Epoch: 11 [3350/36045]\tLoss: 824.9702\n",
      "Training Epoch: 11 [3400/36045]\tLoss: 864.1552\n",
      "Training Epoch: 11 [3450/36045]\tLoss: 934.3263\n",
      "Training Epoch: 11 [3500/36045]\tLoss: 909.4501\n",
      "Training Epoch: 11 [3550/36045]\tLoss: 880.1729\n",
      "Training Epoch: 11 [3600/36045]\tLoss: 938.6027\n",
      "Training Epoch: 11 [3650/36045]\tLoss: 1090.3884\n",
      "Training Epoch: 11 [3700/36045]\tLoss: 1094.3530\n",
      "Training Epoch: 11 [3750/36045]\tLoss: 1053.2535\n",
      "Training Epoch: 11 [3800/36045]\tLoss: 1036.9972\n",
      "Training Epoch: 11 [3850/36045]\tLoss: 1044.7305\n",
      "Training Epoch: 11 [3900/36045]\tLoss: 1055.5167\n",
      "Training Epoch: 11 [3950/36045]\tLoss: 1007.3887\n",
      "Training Epoch: 11 [4000/36045]\tLoss: 1034.2770\n",
      "Training Epoch: 11 [4050/36045]\tLoss: 949.6006\n",
      "Training Epoch: 11 [4100/36045]\tLoss: 920.6009\n",
      "Training Epoch: 11 [4150/36045]\tLoss: 953.1608\n",
      "Training Epoch: 11 [4200/36045]\tLoss: 941.4575\n",
      "Training Epoch: 11 [4250/36045]\tLoss: 947.2443\n",
      "Training Epoch: 11 [4300/36045]\tLoss: 973.6238\n",
      "Training Epoch: 11 [4350/36045]\tLoss: 953.3481\n",
      "Training Epoch: 11 [4400/36045]\tLoss: 910.9009\n",
      "Training Epoch: 11 [4450/36045]\tLoss: 978.3908\n",
      "Training Epoch: 11 [4500/36045]\tLoss: 1037.5254\n",
      "Training Epoch: 11 [4550/36045]\tLoss: 1054.5034\n",
      "Training Epoch: 11 [4600/36045]\tLoss: 1075.2385\n",
      "Training Epoch: 11 [4650/36045]\tLoss: 1074.9055\n",
      "Training Epoch: 11 [4700/36045]\tLoss: 989.6771\n",
      "Training Epoch: 11 [4750/36045]\tLoss: 971.0784\n",
      "Training Epoch: 11 [4800/36045]\tLoss: 1010.6475\n",
      "Training Epoch: 11 [4850/36045]\tLoss: 989.6151\n",
      "Training Epoch: 11 [4900/36045]\tLoss: 958.1171\n",
      "Training Epoch: 11 [4950/36045]\tLoss: 999.0437\n",
      "Training Epoch: 11 [5000/36045]\tLoss: 1054.0830\n",
      "Training Epoch: 11 [5050/36045]\tLoss: 1016.9056\n",
      "Training Epoch: 11 [5100/36045]\tLoss: 1037.4189\n",
      "Training Epoch: 11 [5150/36045]\tLoss: 1009.4345\n",
      "Training Epoch: 11 [5200/36045]\tLoss: 1000.2535\n",
      "Training Epoch: 11 [5250/36045]\tLoss: 987.7292\n",
      "Training Epoch: 11 [5300/36045]\tLoss: 988.5475\n",
      "Training Epoch: 11 [5350/36045]\tLoss: 1027.0632\n",
      "Training Epoch: 11 [5400/36045]\tLoss: 977.4670\n",
      "Training Epoch: 11 [5450/36045]\tLoss: 918.8376\n",
      "Training Epoch: 11 [5500/36045]\tLoss: 970.9501\n",
      "Training Epoch: 11 [5550/36045]\tLoss: 941.8342\n",
      "Training Epoch: 11 [5600/36045]\tLoss: 1073.0282\n",
      "Training Epoch: 11 [5650/36045]\tLoss: 1022.6033\n",
      "Training Epoch: 11 [5700/36045]\tLoss: 968.4871\n",
      "Training Epoch: 11 [5750/36045]\tLoss: 950.0190\n",
      "Training Epoch: 11 [5800/36045]\tLoss: 1013.3454\n",
      "Training Epoch: 11 [5850/36045]\tLoss: 980.4949\n",
      "Training Epoch: 11 [5900/36045]\tLoss: 1121.5233\n",
      "Training Epoch: 11 [5950/36045]\tLoss: 1156.1475\n",
      "Training Epoch: 11 [6000/36045]\tLoss: 1133.2615\n",
      "Training Epoch: 11 [6050/36045]\tLoss: 1093.2257\n",
      "Training Epoch: 11 [6100/36045]\tLoss: 1098.3357\n",
      "Training Epoch: 11 [6150/36045]\tLoss: 1057.8727\n",
      "Training Epoch: 11 [6200/36045]\tLoss: 1052.6504\n",
      "Training Epoch: 11 [6250/36045]\tLoss: 1065.3495\n",
      "Training Epoch: 11 [6300/36045]\tLoss: 1092.0970\n",
      "Training Epoch: 11 [6350/36045]\tLoss: 1147.7690\n",
      "Training Epoch: 11 [6400/36045]\tLoss: 991.6500\n",
      "Training Epoch: 11 [6450/36045]\tLoss: 921.3744\n",
      "Training Epoch: 11 [6500/36045]\tLoss: 946.9534\n",
      "Training Epoch: 11 [6550/36045]\tLoss: 965.4069\n",
      "Training Epoch: 11 [6600/36045]\tLoss: 972.3219\n",
      "Training Epoch: 11 [6650/36045]\tLoss: 1093.1938\n",
      "Training Epoch: 11 [6700/36045]\tLoss: 1141.7255\n",
      "Training Epoch: 11 [6750/36045]\tLoss: 1109.7832\n",
      "Training Epoch: 11 [6800/36045]\tLoss: 1112.0784\n",
      "Training Epoch: 11 [6850/36045]\tLoss: 1095.8215\n",
      "Training Epoch: 11 [6900/36045]\tLoss: 965.8627\n",
      "Training Epoch: 11 [6950/36045]\tLoss: 908.6633\n",
      "Training Epoch: 11 [7000/36045]\tLoss: 965.7916\n",
      "Training Epoch: 11 [7050/36045]\tLoss: 993.9715\n",
      "Training Epoch: 11 [7100/36045]\tLoss: 986.5156\n",
      "Training Epoch: 11 [7150/36045]\tLoss: 1006.8844\n",
      "Training Epoch: 11 [7200/36045]\tLoss: 1015.7707\n",
      "Training Epoch: 11 [7250/36045]\tLoss: 1014.1919\n",
      "Training Epoch: 11 [7300/36045]\tLoss: 995.1574\n",
      "Training Epoch: 11 [7350/36045]\tLoss: 989.5381\n",
      "Training Epoch: 11 [7400/36045]\tLoss: 898.3022\n",
      "Training Epoch: 11 [7450/36045]\tLoss: 899.4436\n",
      "Training Epoch: 11 [7500/36045]\tLoss: 899.4880\n",
      "Training Epoch: 11 [7550/36045]\tLoss: 855.4550\n",
      "Training Epoch: 11 [7600/36045]\tLoss: 959.2797\n",
      "Training Epoch: 11 [7650/36045]\tLoss: 1031.7040\n",
      "Training Epoch: 11 [7700/36045]\tLoss: 987.7947\n",
      "Training Epoch: 11 [7750/36045]\tLoss: 1004.7549\n",
      "Training Epoch: 11 [7800/36045]\tLoss: 986.4359\n",
      "Training Epoch: 11 [7850/36045]\tLoss: 933.4093\n",
      "Training Epoch: 11 [7900/36045]\tLoss: 985.8532\n",
      "Training Epoch: 11 [7950/36045]\tLoss: 982.4781\n",
      "Training Epoch: 11 [8000/36045]\tLoss: 1003.0355\n",
      "Training Epoch: 11 [8050/36045]\tLoss: 953.7196\n",
      "Training Epoch: 11 [8100/36045]\tLoss: 987.7645\n",
      "Training Epoch: 11 [8150/36045]\tLoss: 1116.2109\n",
      "Training Epoch: 11 [8200/36045]\tLoss: 1100.3700\n",
      "Training Epoch: 11 [8250/36045]\tLoss: 1058.7389\n",
      "Training Epoch: 11 [8300/36045]\tLoss: 1144.8204\n",
      "Training Epoch: 11 [8350/36045]\tLoss: 1059.7355\n",
      "Training Epoch: 11 [8400/36045]\tLoss: 964.1508\n",
      "Training Epoch: 11 [8450/36045]\tLoss: 907.2964\n",
      "Training Epoch: 11 [8500/36045]\tLoss: 955.3464\n",
      "Training Epoch: 11 [8550/36045]\tLoss: 934.3837\n",
      "Training Epoch: 11 [8600/36045]\tLoss: 930.5168\n",
      "Training Epoch: 11 [8650/36045]\tLoss: 984.3432\n",
      "Training Epoch: 11 [8700/36045]\tLoss: 1037.2842\n",
      "Training Epoch: 11 [8750/36045]\tLoss: 1013.5731\n",
      "Training Epoch: 11 [8800/36045]\tLoss: 1022.4532\n",
      "Training Epoch: 11 [8850/36045]\tLoss: 1010.4916\n",
      "Training Epoch: 11 [8900/36045]\tLoss: 915.2305\n",
      "Training Epoch: 11 [8950/36045]\tLoss: 943.7404\n",
      "Training Epoch: 11 [9000/36045]\tLoss: 952.4391\n",
      "Training Epoch: 11 [9050/36045]\tLoss: 950.8547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [9100/36045]\tLoss: 975.7598\n",
      "Training Epoch: 11 [9150/36045]\tLoss: 722.2375\n",
      "Training Epoch: 11 [9200/36045]\tLoss: 555.2931\n",
      "Training Epoch: 11 [9250/36045]\tLoss: 598.5878\n",
      "Training Epoch: 11 [9300/36045]\tLoss: 620.8734\n",
      "Training Epoch: 11 [9350/36045]\tLoss: 566.0552\n",
      "Training Epoch: 11 [9400/36045]\tLoss: 1109.0845\n",
      "Training Epoch: 11 [9450/36045]\tLoss: 1173.2864\n",
      "Training Epoch: 11 [9500/36045]\tLoss: 1156.1846\n",
      "Training Epoch: 11 [9550/36045]\tLoss: 1227.9619\n",
      "Training Epoch: 11 [9600/36045]\tLoss: 904.0613\n",
      "Training Epoch: 11 [9650/36045]\tLoss: 900.9304\n",
      "Training Epoch: 11 [9700/36045]\tLoss: 888.5330\n",
      "Training Epoch: 11 [9750/36045]\tLoss: 890.6116\n",
      "Training Epoch: 11 [9800/36045]\tLoss: 1150.1327\n",
      "Training Epoch: 11 [9850/36045]\tLoss: 1215.0353\n",
      "Training Epoch: 11 [9900/36045]\tLoss: 1245.5085\n",
      "Training Epoch: 11 [9950/36045]\tLoss: 1208.6831\n",
      "Training Epoch: 11 [10000/36045]\tLoss: 1116.1228\n",
      "Training Epoch: 11 [10050/36045]\tLoss: 939.7042\n",
      "Training Epoch: 11 [10100/36045]\tLoss: 938.4594\n",
      "Training Epoch: 11 [10150/36045]\tLoss: 953.0092\n",
      "Training Epoch: 11 [10200/36045]\tLoss: 944.9988\n",
      "Training Epoch: 11 [10250/36045]\tLoss: 1137.7943\n",
      "Training Epoch: 11 [10300/36045]\tLoss: 1103.5756\n",
      "Training Epoch: 11 [10350/36045]\tLoss: 1162.6230\n",
      "Training Epoch: 11 [10400/36045]\tLoss: 1154.1122\n",
      "Training Epoch: 11 [10450/36045]\tLoss: 1068.3376\n",
      "Training Epoch: 11 [10500/36045]\tLoss: 901.2091\n",
      "Training Epoch: 11 [10550/36045]\tLoss: 899.1339\n",
      "Training Epoch: 11 [10600/36045]\tLoss: 928.1248\n",
      "Training Epoch: 11 [10650/36045]\tLoss: 936.7477\n",
      "Training Epoch: 11 [10700/36045]\tLoss: 1035.0903\n",
      "Training Epoch: 11 [10750/36045]\tLoss: 1118.8423\n",
      "Training Epoch: 11 [10800/36045]\tLoss: 1041.1626\n",
      "Training Epoch: 11 [10850/36045]\tLoss: 1095.0574\n",
      "Training Epoch: 11 [10900/36045]\tLoss: 1140.5736\n",
      "Training Epoch: 11 [10950/36045]\tLoss: 861.3915\n",
      "Training Epoch: 11 [11000/36045]\tLoss: 854.7615\n",
      "Training Epoch: 11 [11050/36045]\tLoss: 909.4092\n",
      "Training Epoch: 11 [11100/36045]\tLoss: 926.7667\n",
      "Training Epoch: 11 [11150/36045]\tLoss: 1003.6954\n",
      "Training Epoch: 11 [11200/36045]\tLoss: 1036.8438\n",
      "Training Epoch: 11 [11250/36045]\tLoss: 1053.1825\n",
      "Training Epoch: 11 [11300/36045]\tLoss: 1031.9594\n",
      "Training Epoch: 11 [11350/36045]\tLoss: 1025.7596\n",
      "Training Epoch: 11 [11400/36045]\tLoss: 975.2003\n",
      "Training Epoch: 11 [11450/36045]\tLoss: 936.6573\n",
      "Training Epoch: 11 [11500/36045]\tLoss: 935.7955\n",
      "Training Epoch: 11 [11550/36045]\tLoss: 958.1835\n",
      "Training Epoch: 11 [11600/36045]\tLoss: 1035.9005\n",
      "Training Epoch: 11 [11650/36045]\tLoss: 1091.0747\n",
      "Training Epoch: 11 [11700/36045]\tLoss: 1088.4117\n",
      "Training Epoch: 11 [11750/36045]\tLoss: 1113.7294\n",
      "Training Epoch: 11 [11800/36045]\tLoss: 1164.0828\n",
      "Training Epoch: 11 [11850/36045]\tLoss: 1200.6508\n",
      "Training Epoch: 11 [11900/36045]\tLoss: 1437.5410\n",
      "Training Epoch: 11 [11950/36045]\tLoss: 1429.0542\n",
      "Training Epoch: 11 [12000/36045]\tLoss: 1457.3679\n",
      "Training Epoch: 11 [12050/36045]\tLoss: 1409.6063\n",
      "Training Epoch: 11 [12100/36045]\tLoss: 993.9875\n",
      "Training Epoch: 11 [12150/36045]\tLoss: 809.1809\n",
      "Training Epoch: 11 [12200/36045]\tLoss: 802.2155\n",
      "Training Epoch: 11 [12250/36045]\tLoss: 815.5463\n",
      "Training Epoch: 11 [12300/36045]\tLoss: 1000.9935\n",
      "Training Epoch: 11 [12350/36045]\tLoss: 1064.7776\n",
      "Training Epoch: 11 [12400/36045]\tLoss: 1083.4417\n",
      "Training Epoch: 11 [12450/36045]\tLoss: 1068.4724\n",
      "Training Epoch: 11 [12500/36045]\tLoss: 1108.2417\n",
      "Training Epoch: 11 [12550/36045]\tLoss: 1062.9080\n",
      "Training Epoch: 11 [12600/36045]\tLoss: 997.7908\n",
      "Training Epoch: 11 [12650/36045]\tLoss: 992.5062\n",
      "Training Epoch: 11 [12700/36045]\tLoss: 1023.2705\n",
      "Training Epoch: 11 [12750/36045]\tLoss: 1020.7903\n",
      "Training Epoch: 11 [12800/36045]\tLoss: 1000.4694\n",
      "Training Epoch: 11 [12850/36045]\tLoss: 1041.5518\n",
      "Training Epoch: 11 [12900/36045]\tLoss: 997.6486\n",
      "Training Epoch: 11 [12950/36045]\tLoss: 990.2910\n",
      "Training Epoch: 11 [13000/36045]\tLoss: 1023.7864\n",
      "Training Epoch: 11 [13050/36045]\tLoss: 949.1671\n",
      "Training Epoch: 11 [13100/36045]\tLoss: 995.5487\n",
      "Training Epoch: 11 [13150/36045]\tLoss: 993.2802\n",
      "Training Epoch: 11 [13200/36045]\tLoss: 942.9076\n",
      "Training Epoch: 11 [13250/36045]\tLoss: 992.2661\n",
      "Training Epoch: 11 [13300/36045]\tLoss: 1032.5068\n",
      "Training Epoch: 11 [13350/36045]\tLoss: 1003.7006\n",
      "Training Epoch: 11 [13400/36045]\tLoss: 1010.3798\n",
      "Training Epoch: 11 [13450/36045]\tLoss: 996.2535\n",
      "Training Epoch: 11 [13500/36045]\tLoss: 1036.6351\n",
      "Training Epoch: 11 [13550/36045]\tLoss: 1168.2887\n",
      "Training Epoch: 11 [13600/36045]\tLoss: 1199.8829\n",
      "Training Epoch: 11 [13650/36045]\tLoss: 1283.5231\n",
      "Training Epoch: 11 [13700/36045]\tLoss: 1151.6792\n",
      "Training Epoch: 11 [13750/36045]\tLoss: 1024.5280\n",
      "Training Epoch: 11 [13800/36045]\tLoss: 1000.1843\n",
      "Training Epoch: 11 [13850/36045]\tLoss: 980.3936\n",
      "Training Epoch: 11 [13900/36045]\tLoss: 994.4637\n",
      "Training Epoch: 11 [13950/36045]\tLoss: 1027.5660\n",
      "Training Epoch: 11 [14000/36045]\tLoss: 1068.4995\n",
      "Training Epoch: 11 [14050/36045]\tLoss: 1025.4337\n",
      "Training Epoch: 11 [14100/36045]\tLoss: 1025.0699\n",
      "Training Epoch: 11 [14150/36045]\tLoss: 1006.1398\n",
      "Training Epoch: 11 [14200/36045]\tLoss: 1075.3223\n",
      "Training Epoch: 11 [14250/36045]\tLoss: 1175.1034\n",
      "Training Epoch: 11 [14300/36045]\tLoss: 1179.6873\n",
      "Training Epoch: 11 [14350/36045]\tLoss: 1129.6416\n",
      "Training Epoch: 11 [14400/36045]\tLoss: 1113.8270\n",
      "Training Epoch: 11 [14450/36045]\tLoss: 1160.7307\n",
      "Training Epoch: 11 [14500/36045]\tLoss: 1079.7440\n",
      "Training Epoch: 11 [14550/36045]\tLoss: 1130.9318\n",
      "Training Epoch: 11 [14600/36045]\tLoss: 1099.9503\n",
      "Training Epoch: 11 [14650/36045]\tLoss: 1105.8818\n",
      "Training Epoch: 11 [14700/36045]\tLoss: 1038.9272\n",
      "Training Epoch: 11 [14750/36045]\tLoss: 895.2328\n",
      "Training Epoch: 11 [14800/36045]\tLoss: 879.3246\n",
      "Training Epoch: 11 [14850/36045]\tLoss: 886.4166\n",
      "Training Epoch: 11 [14900/36045]\tLoss: 881.2321\n",
      "Training Epoch: 11 [14950/36045]\tLoss: 890.5338\n",
      "Training Epoch: 11 [15000/36045]\tLoss: 915.4221\n",
      "Training Epoch: 11 [15050/36045]\tLoss: 920.5288\n",
      "Training Epoch: 11 [15100/36045]\tLoss: 899.2719\n",
      "Training Epoch: 11 [15150/36045]\tLoss: 891.6142\n",
      "Training Epoch: 11 [15200/36045]\tLoss: 823.3142\n",
      "Training Epoch: 11 [15250/36045]\tLoss: 858.8789\n",
      "Training Epoch: 11 [15300/36045]\tLoss: 836.8174\n",
      "Training Epoch: 11 [15350/36045]\tLoss: 855.5161\n",
      "Training Epoch: 11 [15400/36045]\tLoss: 843.3535\n",
      "Training Epoch: 11 [15450/36045]\tLoss: 829.5844\n",
      "Training Epoch: 11 [15500/36045]\tLoss: 854.8889\n",
      "Training Epoch: 11 [15550/36045]\tLoss: 840.3036\n",
      "Training Epoch: 11 [15600/36045]\tLoss: 927.8616\n",
      "Training Epoch: 11 [15650/36045]\tLoss: 951.7317\n",
      "Training Epoch: 11 [15700/36045]\tLoss: 934.3325\n",
      "Training Epoch: 11 [15750/36045]\tLoss: 926.0386\n",
      "Training Epoch: 11 [15800/36045]\tLoss: 843.6046\n",
      "Training Epoch: 11 [15850/36045]\tLoss: 851.6276\n",
      "Training Epoch: 11 [15900/36045]\tLoss: 861.8588\n",
      "Training Epoch: 11 [15950/36045]\tLoss: 890.1497\n",
      "Training Epoch: 11 [16000/36045]\tLoss: 879.4010\n",
      "Training Epoch: 11 [16050/36045]\tLoss: 842.8696\n",
      "Training Epoch: 11 [16100/36045]\tLoss: 781.2425\n",
      "Training Epoch: 11 [16150/36045]\tLoss: 760.9382\n",
      "Training Epoch: 11 [16200/36045]\tLoss: 911.2151\n",
      "Training Epoch: 11 [16250/36045]\tLoss: 945.8149\n",
      "Training Epoch: 11 [16300/36045]\tLoss: 1027.4158\n",
      "Training Epoch: 11 [16350/36045]\tLoss: 1041.8644\n",
      "Training Epoch: 11 [16400/36045]\tLoss: 1017.8736\n",
      "Training Epoch: 11 [16450/36045]\tLoss: 993.4085\n",
      "Training Epoch: 11 [16500/36045]\tLoss: 990.4709\n",
      "Training Epoch: 11 [16550/36045]\tLoss: 944.6241\n",
      "Training Epoch: 11 [16600/36045]\tLoss: 990.8242\n",
      "Training Epoch: 11 [16650/36045]\tLoss: 1017.6909\n",
      "Training Epoch: 11 [16700/36045]\tLoss: 991.3069\n",
      "Training Epoch: 11 [16750/36045]\tLoss: 980.9274\n",
      "Training Epoch: 11 [16800/36045]\tLoss: 1000.1893\n",
      "Training Epoch: 11 [16850/36045]\tLoss: 950.6627\n",
      "Training Epoch: 11 [16900/36045]\tLoss: 967.8995\n",
      "Training Epoch: 11 [16950/36045]\tLoss: 991.5120\n",
      "Training Epoch: 11 [17000/36045]\tLoss: 966.6083\n",
      "Training Epoch: 11 [17050/36045]\tLoss: 1017.9293\n",
      "Training Epoch: 11 [17100/36045]\tLoss: 1024.8098\n",
      "Training Epoch: 11 [17150/36045]\tLoss: 890.6889\n",
      "Training Epoch: 11 [17200/36045]\tLoss: 839.9307\n",
      "Training Epoch: 11 [17250/36045]\tLoss: 878.0056\n",
      "Training Epoch: 11 [17300/36045]\tLoss: 927.1301\n",
      "Training Epoch: 11 [17350/36045]\tLoss: 880.5842\n",
      "Training Epoch: 11 [17400/36045]\tLoss: 897.1235\n",
      "Training Epoch: 11 [17450/36045]\tLoss: 924.0081\n",
      "Training Epoch: 11 [17500/36045]\tLoss: 909.1223\n",
      "Training Epoch: 11 [17550/36045]\tLoss: 921.5051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [17600/36045]\tLoss: 907.6707\n",
      "Training Epoch: 11 [17650/36045]\tLoss: 933.4908\n",
      "Training Epoch: 11 [17700/36045]\tLoss: 908.2815\n",
      "Training Epoch: 11 [17750/36045]\tLoss: 929.8079\n",
      "Training Epoch: 11 [17800/36045]\tLoss: 922.4088\n",
      "Training Epoch: 11 [17850/36045]\tLoss: 887.5754\n",
      "Training Epoch: 11 [17900/36045]\tLoss: 922.1425\n",
      "Training Epoch: 11 [17950/36045]\tLoss: 930.6249\n",
      "Training Epoch: 11 [18000/36045]\tLoss: 919.6641\n",
      "Training Epoch: 11 [18050/36045]\tLoss: 1049.1642\n",
      "Training Epoch: 11 [18100/36045]\tLoss: 1056.5603\n",
      "Training Epoch: 11 [18150/36045]\tLoss: 1060.8137\n",
      "Training Epoch: 11 [18200/36045]\tLoss: 1051.0593\n",
      "Training Epoch: 11 [18250/36045]\tLoss: 1072.1958\n",
      "Training Epoch: 11 [18300/36045]\tLoss: 978.2143\n",
      "Training Epoch: 11 [18350/36045]\tLoss: 1039.2743\n",
      "Training Epoch: 11 [18400/36045]\tLoss: 1013.4435\n",
      "Training Epoch: 11 [18450/36045]\tLoss: 993.6116\n",
      "Training Epoch: 11 [18500/36045]\tLoss: 995.9130\n",
      "Training Epoch: 11 [18550/36045]\tLoss: 977.1401\n",
      "Training Epoch: 11 [18600/36045]\tLoss: 966.6002\n",
      "Training Epoch: 11 [18650/36045]\tLoss: 1023.6093\n",
      "Training Epoch: 11 [18700/36045]\tLoss: 1079.2623\n",
      "Training Epoch: 11 [18750/36045]\tLoss: 1058.1676\n",
      "Training Epoch: 11 [18800/36045]\tLoss: 1089.8986\n",
      "Training Epoch: 11 [18850/36045]\tLoss: 1029.2572\n",
      "Training Epoch: 11 [18900/36045]\tLoss: 1102.5996\n",
      "Training Epoch: 11 [18950/36045]\tLoss: 1026.6852\n",
      "Training Epoch: 11 [19000/36045]\tLoss: 893.0684\n",
      "Training Epoch: 11 [19050/36045]\tLoss: 859.1452\n",
      "Training Epoch: 11 [19100/36045]\tLoss: 882.3389\n",
      "Training Epoch: 11 [19150/36045]\tLoss: 866.3815\n",
      "Training Epoch: 11 [19200/36045]\tLoss: 901.5846\n",
      "Training Epoch: 11 [19250/36045]\tLoss: 911.6775\n",
      "Training Epoch: 11 [19300/36045]\tLoss: 938.3466\n",
      "Training Epoch: 11 [19350/36045]\tLoss: 914.7216\n",
      "Training Epoch: 11 [19400/36045]\tLoss: 939.3651\n",
      "Training Epoch: 11 [19450/36045]\tLoss: 926.1823\n",
      "Training Epoch: 11 [19500/36045]\tLoss: 931.0433\n",
      "Training Epoch: 11 [19550/36045]\tLoss: 933.8578\n",
      "Training Epoch: 11 [19600/36045]\tLoss: 979.4234\n",
      "Training Epoch: 11 [19650/36045]\tLoss: 1248.1174\n",
      "Training Epoch: 11 [19700/36045]\tLoss: 1199.5051\n",
      "Training Epoch: 11 [19750/36045]\tLoss: 1198.0442\n",
      "Training Epoch: 11 [19800/36045]\tLoss: 1191.2040\n",
      "Training Epoch: 11 [19850/36045]\tLoss: 835.8483\n",
      "Training Epoch: 11 [19900/36045]\tLoss: 804.3777\n",
      "Training Epoch: 11 [19950/36045]\tLoss: 811.3138\n",
      "Training Epoch: 11 [20000/36045]\tLoss: 811.1623\n",
      "Training Epoch: 11 [20050/36045]\tLoss: 907.3143\n",
      "Training Epoch: 11 [20100/36045]\tLoss: 908.5438\n",
      "Training Epoch: 11 [20150/36045]\tLoss: 914.2985\n",
      "Training Epoch: 11 [20200/36045]\tLoss: 910.1337\n",
      "Training Epoch: 11 [20250/36045]\tLoss: 965.2868\n",
      "Training Epoch: 11 [20300/36045]\tLoss: 1003.9152\n",
      "Training Epoch: 11 [20350/36045]\tLoss: 1035.8347\n",
      "Training Epoch: 11 [20400/36045]\tLoss: 1051.5551\n",
      "Training Epoch: 11 [20450/36045]\tLoss: 1029.8798\n",
      "Training Epoch: 11 [20500/36045]\tLoss: 999.4307\n",
      "Training Epoch: 11 [20550/36045]\tLoss: 898.9417\n",
      "Training Epoch: 11 [20600/36045]\tLoss: 916.2558\n",
      "Training Epoch: 11 [20650/36045]\tLoss: 910.5108\n",
      "Training Epoch: 11 [20700/36045]\tLoss: 890.9560\n",
      "Training Epoch: 11 [20750/36045]\tLoss: 950.1182\n",
      "Training Epoch: 11 [20800/36045]\tLoss: 1035.2734\n",
      "Training Epoch: 11 [20850/36045]\tLoss: 1025.4856\n",
      "Training Epoch: 11 [20900/36045]\tLoss: 1087.7958\n",
      "Training Epoch: 11 [20950/36045]\tLoss: 1026.2407\n",
      "Training Epoch: 11 [21000/36045]\tLoss: 965.5351\n",
      "Training Epoch: 11 [21050/36045]\tLoss: 821.8427\n",
      "Training Epoch: 11 [21100/36045]\tLoss: 822.7925\n",
      "Training Epoch: 11 [21150/36045]\tLoss: 880.9721\n",
      "Training Epoch: 11 [21200/36045]\tLoss: 882.4824\n",
      "Training Epoch: 11 [21250/36045]\tLoss: 840.4780\n",
      "Training Epoch: 11 [21300/36045]\tLoss: 988.2841\n",
      "Training Epoch: 11 [21350/36045]\tLoss: 986.0530\n",
      "Training Epoch: 11 [21400/36045]\tLoss: 992.7178\n",
      "Training Epoch: 11 [21450/36045]\tLoss: 1009.2434\n",
      "Training Epoch: 11 [21500/36045]\tLoss: 1006.3695\n",
      "Training Epoch: 11 [21550/36045]\tLoss: 1113.2279\n",
      "Training Epoch: 11 [21600/36045]\tLoss: 1114.0046\n",
      "Training Epoch: 11 [21650/36045]\tLoss: 1132.1689\n",
      "Training Epoch: 11 [21700/36045]\tLoss: 1128.4966\n",
      "Training Epoch: 11 [21750/36045]\tLoss: 1092.7885\n",
      "Training Epoch: 11 [21800/36045]\tLoss: 824.5605\n",
      "Training Epoch: 11 [21850/36045]\tLoss: 799.0755\n",
      "Training Epoch: 11 [21900/36045]\tLoss: 818.0453\n",
      "Training Epoch: 11 [21950/36045]\tLoss: 811.4860\n",
      "Training Epoch: 11 [22000/36045]\tLoss: 821.4989\n",
      "Training Epoch: 11 [22050/36045]\tLoss: 870.0681\n",
      "Training Epoch: 11 [22100/36045]\tLoss: 857.9582\n",
      "Training Epoch: 11 [22150/36045]\tLoss: 831.7769\n",
      "Training Epoch: 11 [22200/36045]\tLoss: 858.2839\n",
      "Training Epoch: 11 [22250/36045]\tLoss: 867.5692\n",
      "Training Epoch: 11 [22300/36045]\tLoss: 929.6805\n",
      "Training Epoch: 11 [22350/36045]\tLoss: 960.2805\n",
      "Training Epoch: 11 [22400/36045]\tLoss: 988.7218\n",
      "Training Epoch: 11 [22450/36045]\tLoss: 967.3271\n",
      "Training Epoch: 11 [22500/36045]\tLoss: 940.3887\n",
      "Training Epoch: 11 [22550/36045]\tLoss: 1001.5629\n",
      "Training Epoch: 11 [22600/36045]\tLoss: 1103.8809\n",
      "Training Epoch: 11 [22650/36045]\tLoss: 1155.9559\n",
      "Training Epoch: 11 [22700/36045]\tLoss: 1183.7654\n",
      "Training Epoch: 11 [22750/36045]\tLoss: 1209.4843\n",
      "Training Epoch: 11 [22800/36045]\tLoss: 1263.4810\n",
      "Training Epoch: 11 [22850/36045]\tLoss: 1044.4780\n",
      "Training Epoch: 11 [22900/36045]\tLoss: 1048.5347\n",
      "Training Epoch: 11 [22950/36045]\tLoss: 1017.4338\n",
      "Training Epoch: 11 [23000/36045]\tLoss: 1024.6774\n",
      "Training Epoch: 11 [23050/36045]\tLoss: 918.6588\n",
      "Training Epoch: 11 [23100/36045]\tLoss: 938.2180\n",
      "Training Epoch: 11 [23150/36045]\tLoss: 919.8840\n",
      "Training Epoch: 11 [23200/36045]\tLoss: 867.0140\n",
      "Training Epoch: 11 [23250/36045]\tLoss: 873.9865\n",
      "Training Epoch: 11 [23300/36045]\tLoss: 866.8256\n",
      "Training Epoch: 11 [23350/36045]\tLoss: 891.9836\n",
      "Training Epoch: 11 [23400/36045]\tLoss: 970.7148\n",
      "Training Epoch: 11 [23450/36045]\tLoss: 958.2405\n",
      "Training Epoch: 11 [23500/36045]\tLoss: 918.5785\n",
      "Training Epoch: 11 [23550/36045]\tLoss: 993.6907\n",
      "Training Epoch: 11 [23600/36045]\tLoss: 1116.6814\n",
      "Training Epoch: 11 [23650/36045]\tLoss: 1143.4556\n",
      "Training Epoch: 11 [23700/36045]\tLoss: 1153.4821\n",
      "Training Epoch: 11 [23750/36045]\tLoss: 1117.6544\n",
      "Training Epoch: 11 [23800/36045]\tLoss: 884.9692\n",
      "Training Epoch: 11 [23850/36045]\tLoss: 920.4515\n",
      "Training Epoch: 11 [23900/36045]\tLoss: 908.8607\n",
      "Training Epoch: 11 [23950/36045]\tLoss: 888.6068\n",
      "Training Epoch: 11 [24000/36045]\tLoss: 864.1406\n",
      "Training Epoch: 11 [24050/36045]\tLoss: 792.1147\n",
      "Training Epoch: 11 [24100/36045]\tLoss: 829.4784\n",
      "Training Epoch: 11 [24150/36045]\tLoss: 829.3221\n",
      "Training Epoch: 11 [24200/36045]\tLoss: 811.9701\n",
      "Training Epoch: 11 [24250/36045]\tLoss: 791.6545\n",
      "Training Epoch: 11 [24300/36045]\tLoss: 859.2059\n",
      "Training Epoch: 11 [24350/36045]\tLoss: 883.4809\n",
      "Training Epoch: 11 [24400/36045]\tLoss: 907.1713\n",
      "Training Epoch: 11 [24450/36045]\tLoss: 869.4294\n",
      "Training Epoch: 11 [24500/36045]\tLoss: 916.8634\n",
      "Training Epoch: 11 [24550/36045]\tLoss: 1009.8151\n",
      "Training Epoch: 11 [24600/36045]\tLoss: 1003.8204\n",
      "Training Epoch: 11 [24650/36045]\tLoss: 970.8467\n",
      "Training Epoch: 11 [24700/36045]\tLoss: 985.9748\n",
      "Training Epoch: 11 [24750/36045]\tLoss: 908.3607\n",
      "Training Epoch: 11 [24800/36045]\tLoss: 790.3765\n",
      "Training Epoch: 11 [24850/36045]\tLoss: 810.0726\n",
      "Training Epoch: 11 [24900/36045]\tLoss: 810.8657\n",
      "Training Epoch: 11 [24950/36045]\tLoss: 814.1523\n",
      "Training Epoch: 11 [25000/36045]\tLoss: 776.2899\n",
      "Training Epoch: 11 [25050/36045]\tLoss: 735.8607\n",
      "Training Epoch: 11 [25100/36045]\tLoss: 659.8705\n",
      "Training Epoch: 11 [25150/36045]\tLoss: 613.7496\n",
      "Training Epoch: 11 [25200/36045]\tLoss: 611.2894\n",
      "Training Epoch: 11 [25250/36045]\tLoss: 649.3268\n",
      "Training Epoch: 11 [25300/36045]\tLoss: 842.5795\n",
      "Training Epoch: 11 [25350/36045]\tLoss: 842.0948\n",
      "Training Epoch: 11 [25400/36045]\tLoss: 787.3759\n",
      "Training Epoch: 11 [25450/36045]\tLoss: 787.7422\n",
      "Training Epoch: 11 [25500/36045]\tLoss: 859.2296\n",
      "Training Epoch: 11 [25550/36045]\tLoss: 1008.1013\n",
      "Training Epoch: 11 [25600/36045]\tLoss: 1014.9421\n",
      "Training Epoch: 11 [25650/36045]\tLoss: 977.0100\n",
      "Training Epoch: 11 [25700/36045]\tLoss: 997.4318\n",
      "Training Epoch: 11 [25750/36045]\tLoss: 953.2645\n",
      "Training Epoch: 11 [25800/36045]\tLoss: 589.8078\n",
      "Training Epoch: 11 [25850/36045]\tLoss: 601.7326\n",
      "Training Epoch: 11 [25900/36045]\tLoss: 577.1130\n",
      "Training Epoch: 11 [25950/36045]\tLoss: 590.2725\n",
      "Training Epoch: 11 [26000/36045]\tLoss: 726.4269\n",
      "Training Epoch: 11 [26050/36045]\tLoss: 984.4203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [26100/36045]\tLoss: 1021.6370\n",
      "Training Epoch: 11 [26150/36045]\tLoss: 1013.6395\n",
      "Training Epoch: 11 [26200/36045]\tLoss: 986.9957\n",
      "Training Epoch: 11 [26250/36045]\tLoss: 1020.4543\n",
      "Training Epoch: 11 [26300/36045]\tLoss: 885.5967\n",
      "Training Epoch: 11 [26350/36045]\tLoss: 890.2181\n",
      "Training Epoch: 11 [26400/36045]\tLoss: 872.2155\n",
      "Training Epoch: 11 [26450/36045]\tLoss: 797.1189\n",
      "Training Epoch: 11 [26500/36045]\tLoss: 960.7791\n",
      "Training Epoch: 11 [26550/36045]\tLoss: 980.0251\n",
      "Training Epoch: 11 [26600/36045]\tLoss: 973.1143\n",
      "Training Epoch: 11 [26650/36045]\tLoss: 994.3681\n",
      "Training Epoch: 11 [26700/36045]\tLoss: 970.8065\n",
      "Training Epoch: 11 [26750/36045]\tLoss: 908.4296\n",
      "Training Epoch: 11 [26800/36045]\tLoss: 664.4007\n",
      "Training Epoch: 11 [26850/36045]\tLoss: 559.0197\n",
      "Training Epoch: 11 [26900/36045]\tLoss: 563.0909\n",
      "Training Epoch: 11 [26950/36045]\tLoss: 614.7201\n",
      "Training Epoch: 11 [27000/36045]\tLoss: 970.6573\n",
      "Training Epoch: 11 [27050/36045]\tLoss: 1025.4277\n",
      "Training Epoch: 11 [27100/36045]\tLoss: 994.5239\n",
      "Training Epoch: 11 [27150/36045]\tLoss: 1043.6746\n",
      "Training Epoch: 11 [27200/36045]\tLoss: 780.8547\n",
      "Training Epoch: 11 [27250/36045]\tLoss: 778.8209\n",
      "Training Epoch: 11 [27300/36045]\tLoss: 752.1834\n",
      "Training Epoch: 11 [27350/36045]\tLoss: 760.0968\n",
      "Training Epoch: 11 [27400/36045]\tLoss: 756.4444\n",
      "Training Epoch: 11 [27450/36045]\tLoss: 950.9860\n",
      "Training Epoch: 11 [27500/36045]\tLoss: 1019.5575\n",
      "Training Epoch: 11 [27550/36045]\tLoss: 1012.1869\n",
      "Training Epoch: 11 [27600/36045]\tLoss: 1016.1597\n",
      "Training Epoch: 11 [27650/36045]\tLoss: 1016.1046\n",
      "Training Epoch: 11 [27700/36045]\tLoss: 1045.8403\n",
      "Training Epoch: 11 [27750/36045]\tLoss: 1062.4403\n",
      "Training Epoch: 11 [27800/36045]\tLoss: 1043.8907\n",
      "Training Epoch: 11 [27850/36045]\tLoss: 1022.9371\n",
      "Training Epoch: 11 [27900/36045]\tLoss: 907.5832\n",
      "Training Epoch: 11 [27950/36045]\tLoss: 748.8497\n",
      "Training Epoch: 11 [28000/36045]\tLoss: 716.8739\n",
      "Training Epoch: 11 [28050/36045]\tLoss: 741.0605\n",
      "Training Epoch: 11 [28100/36045]\tLoss: 726.8618\n",
      "Training Epoch: 11 [28150/36045]\tLoss: 781.8824\n",
      "Training Epoch: 11 [28200/36045]\tLoss: 784.4125\n",
      "Training Epoch: 11 [28250/36045]\tLoss: 789.6309\n",
      "Training Epoch: 11 [28300/36045]\tLoss: 745.9537\n",
      "Training Epoch: 11 [28350/36045]\tLoss: 745.0681\n",
      "Training Epoch: 11 [28400/36045]\tLoss: 1095.4900\n",
      "Training Epoch: 11 [28450/36045]\tLoss: 975.3550\n",
      "Training Epoch: 11 [28500/36045]\tLoss: 845.2229\n",
      "Training Epoch: 11 [28550/36045]\tLoss: 775.0735\n",
      "Training Epoch: 11 [28600/36045]\tLoss: 874.3710\n",
      "Training Epoch: 11 [28650/36045]\tLoss: 1032.3170\n",
      "Training Epoch: 11 [28700/36045]\tLoss: 1025.4302\n",
      "Training Epoch: 11 [28750/36045]\tLoss: 1012.9543\n",
      "Training Epoch: 11 [28800/36045]\tLoss: 1025.1749\n",
      "Training Epoch: 11 [28850/36045]\tLoss: 882.1187\n",
      "Training Epoch: 11 [28900/36045]\tLoss: 694.6248\n",
      "Training Epoch: 11 [28950/36045]\tLoss: 687.5406\n",
      "Training Epoch: 11 [29000/36045]\tLoss: 695.8909\n",
      "Training Epoch: 11 [29050/36045]\tLoss: 706.1874\n",
      "Training Epoch: 11 [29100/36045]\tLoss: 725.2882\n",
      "Training Epoch: 11 [29150/36045]\tLoss: 706.5654\n",
      "Training Epoch: 11 [29200/36045]\tLoss: 695.8604\n",
      "Training Epoch: 11 [29250/36045]\tLoss: 674.0141\n",
      "Training Epoch: 11 [29300/36045]\tLoss: 785.2996\n",
      "Training Epoch: 11 [29350/36045]\tLoss: 946.2441\n",
      "Training Epoch: 11 [29400/36045]\tLoss: 969.9977\n",
      "Training Epoch: 11 [29450/36045]\tLoss: 1011.5978\n",
      "Training Epoch: 11 [29500/36045]\tLoss: 1029.6595\n",
      "Training Epoch: 11 [29550/36045]\tLoss: 975.0790\n",
      "Training Epoch: 11 [29600/36045]\tLoss: 837.2127\n",
      "Training Epoch: 11 [29650/36045]\tLoss: 815.9226\n",
      "Training Epoch: 11 [29700/36045]\tLoss: 719.7812\n",
      "Training Epoch: 11 [29750/36045]\tLoss: 726.8680\n",
      "Training Epoch: 11 [29800/36045]\tLoss: 779.0637\n",
      "Training Epoch: 11 [29850/36045]\tLoss: 846.0963\n",
      "Training Epoch: 11 [29900/36045]\tLoss: 839.6168\n",
      "Training Epoch: 11 [29950/36045]\tLoss: 860.7177\n",
      "Training Epoch: 11 [30000/36045]\tLoss: 848.8651\n",
      "Training Epoch: 11 [30050/36045]\tLoss: 854.8085\n",
      "Training Epoch: 11 [30100/36045]\tLoss: 1048.2736\n",
      "Training Epoch: 11 [30150/36045]\tLoss: 1040.6941\n",
      "Training Epoch: 11 [30200/36045]\tLoss: 983.5600\n",
      "Training Epoch: 11 [30250/36045]\tLoss: 1037.3120\n",
      "Training Epoch: 11 [30300/36045]\tLoss: 1029.6810\n",
      "Training Epoch: 11 [30350/36045]\tLoss: 838.2530\n",
      "Training Epoch: 11 [30400/36045]\tLoss: 828.5410\n",
      "Training Epoch: 11 [30450/36045]\tLoss: 826.1628\n",
      "Training Epoch: 11 [30500/36045]\tLoss: 767.9045\n",
      "Training Epoch: 11 [30550/36045]\tLoss: 711.9261\n",
      "Training Epoch: 11 [30600/36045]\tLoss: 679.5652\n",
      "Training Epoch: 11 [30650/36045]\tLoss: 670.6801\n",
      "Training Epoch: 11 [30700/36045]\tLoss: 694.9609\n",
      "Training Epoch: 11 [30750/36045]\tLoss: 674.8295\n",
      "Training Epoch: 11 [30800/36045]\tLoss: 716.4971\n",
      "Training Epoch: 11 [30850/36045]\tLoss: 710.5322\n",
      "Training Epoch: 11 [30900/36045]\tLoss: 730.6384\n",
      "Training Epoch: 11 [30950/36045]\tLoss: 770.0696\n",
      "Training Epoch: 11 [31000/36045]\tLoss: 755.7337\n",
      "Training Epoch: 11 [31050/36045]\tLoss: 630.1906\n",
      "Training Epoch: 11 [31100/36045]\tLoss: 615.5784\n",
      "Training Epoch: 11 [31150/36045]\tLoss: 620.1647\n",
      "Training Epoch: 11 [31200/36045]\tLoss: 783.3887\n",
      "Training Epoch: 11 [31250/36045]\tLoss: 1015.4206\n",
      "Training Epoch: 11 [31300/36045]\tLoss: 971.9868\n",
      "Training Epoch: 11 [31350/36045]\tLoss: 988.9592\n",
      "Training Epoch: 11 [31400/36045]\tLoss: 974.6298\n",
      "Training Epoch: 11 [31450/36045]\tLoss: 965.8617\n",
      "Training Epoch: 11 [31500/36045]\tLoss: 967.2848\n",
      "Training Epoch: 11 [31550/36045]\tLoss: 981.6429\n",
      "Training Epoch: 11 [31600/36045]\tLoss: 921.3121\n",
      "Training Epoch: 11 [31650/36045]\tLoss: 984.9751\n",
      "Training Epoch: 11 [31700/36045]\tLoss: 736.5927\n",
      "Training Epoch: 11 [31750/36045]\tLoss: 617.4691\n",
      "Training Epoch: 11 [31800/36045]\tLoss: 587.7050\n",
      "Training Epoch: 11 [31850/36045]\tLoss: 606.2345\n",
      "Training Epoch: 11 [31900/36045]\tLoss: 911.3290\n",
      "Training Epoch: 11 [31950/36045]\tLoss: 1138.2642\n",
      "Training Epoch: 11 [32000/36045]\tLoss: 1276.6522\n",
      "Training Epoch: 11 [32050/36045]\tLoss: 1222.2970\n",
      "Training Epoch: 11 [32100/36045]\tLoss: 1204.0884\n",
      "Training Epoch: 11 [32150/36045]\tLoss: 987.0719\n",
      "Training Epoch: 11 [32200/36045]\tLoss: 995.7725\n",
      "Training Epoch: 11 [32250/36045]\tLoss: 1011.4059\n",
      "Training Epoch: 11 [32300/36045]\tLoss: 991.2202\n",
      "Training Epoch: 11 [32350/36045]\tLoss: 974.4501\n",
      "Training Epoch: 11 [32400/36045]\tLoss: 920.3412\n",
      "Training Epoch: 11 [32450/36045]\tLoss: 766.9967\n",
      "Training Epoch: 11 [32500/36045]\tLoss: 736.1265\n",
      "Training Epoch: 11 [32550/36045]\tLoss: 743.6668\n",
      "Training Epoch: 11 [32600/36045]\tLoss: 737.4346\n",
      "Training Epoch: 11 [32650/36045]\tLoss: 910.2573\n",
      "Training Epoch: 11 [32700/36045]\tLoss: 984.5829\n",
      "Training Epoch: 11 [32750/36045]\tLoss: 940.2373\n",
      "Training Epoch: 11 [32800/36045]\tLoss: 964.6154\n",
      "Training Epoch: 11 [32850/36045]\tLoss: 900.0147\n",
      "Training Epoch: 11 [32900/36045]\tLoss: 732.5439\n",
      "Training Epoch: 11 [32950/36045]\tLoss: 768.0727\n",
      "Training Epoch: 11 [33000/36045]\tLoss: 773.5354\n",
      "Training Epoch: 11 [33050/36045]\tLoss: 728.3762\n",
      "Training Epoch: 11 [33100/36045]\tLoss: 831.2821\n",
      "Training Epoch: 11 [33150/36045]\tLoss: 1105.3368\n",
      "Training Epoch: 11 [33200/36045]\tLoss: 1079.2843\n",
      "Training Epoch: 11 [33250/36045]\tLoss: 1108.1527\n",
      "Training Epoch: 11 [33300/36045]\tLoss: 1177.3914\n",
      "Training Epoch: 11 [33350/36045]\tLoss: 911.5491\n",
      "Training Epoch: 11 [33400/36045]\tLoss: 689.0380\n",
      "Training Epoch: 11 [33450/36045]\tLoss: 684.2129\n",
      "Training Epoch: 11 [33500/36045]\tLoss: 701.2758\n",
      "Training Epoch: 11 [33550/36045]\tLoss: 726.6954\n",
      "Training Epoch: 11 [33600/36045]\tLoss: 732.5013\n",
      "Training Epoch: 11 [33650/36045]\tLoss: 949.6428\n",
      "Training Epoch: 11 [33700/36045]\tLoss: 920.8438\n",
      "Training Epoch: 11 [33750/36045]\tLoss: 950.5167\n",
      "Training Epoch: 11 [33800/36045]\tLoss: 947.3790\n",
      "Training Epoch: 11 [33850/36045]\tLoss: 952.7914\n",
      "Training Epoch: 11 [33900/36045]\tLoss: 966.5333\n",
      "Training Epoch: 11 [33950/36045]\tLoss: 979.5550\n",
      "Training Epoch: 11 [34000/36045]\tLoss: 970.1470\n",
      "Training Epoch: 11 [34050/36045]\tLoss: 979.8472\n",
      "Training Epoch: 11 [34100/36045]\tLoss: 935.8636\n",
      "Training Epoch: 11 [34150/36045]\tLoss: 869.3547\n",
      "Training Epoch: 11 [34200/36045]\tLoss: 829.2338\n",
      "Training Epoch: 11 [34250/36045]\tLoss: 845.9116\n",
      "Training Epoch: 11 [34300/36045]\tLoss: 727.5009\n",
      "Training Epoch: 11 [34350/36045]\tLoss: 761.1072\n",
      "Training Epoch: 11 [34400/36045]\tLoss: 742.8931\n",
      "Training Epoch: 11 [34450/36045]\tLoss: 696.7819\n",
      "Training Epoch: 11 [34500/36045]\tLoss: 746.7130\n",
      "Training Epoch: 11 [34550/36045]\tLoss: 739.7375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [34600/36045]\tLoss: 717.0278\n",
      "Training Epoch: 11 [34650/36045]\tLoss: 845.3560\n",
      "Training Epoch: 11 [34700/36045]\tLoss: 889.8748\n",
      "Training Epoch: 11 [34750/36045]\tLoss: 792.5066\n",
      "Training Epoch: 11 [34800/36045]\tLoss: 893.7789\n",
      "Training Epoch: 11 [34850/36045]\tLoss: 914.2017\n",
      "Training Epoch: 11 [34900/36045]\tLoss: 1078.3629\n",
      "Training Epoch: 11 [34950/36045]\tLoss: 1066.7181\n",
      "Training Epoch: 11 [35000/36045]\tLoss: 1076.6180\n",
      "Training Epoch: 11 [35050/36045]\tLoss: 1053.4982\n",
      "Training Epoch: 11 [35100/36045]\tLoss: 829.7028\n",
      "Training Epoch: 11 [35150/36045]\tLoss: 818.0343\n",
      "Training Epoch: 11 [35200/36045]\tLoss: 711.7255\n",
      "Training Epoch: 11 [35250/36045]\tLoss: 774.5226\n",
      "Training Epoch: 11 [35300/36045]\tLoss: 785.4042\n",
      "Training Epoch: 11 [35350/36045]\tLoss: 931.9329\n",
      "Training Epoch: 11 [35400/36045]\tLoss: 996.0256\n",
      "Training Epoch: 11 [35450/36045]\tLoss: 951.7866\n",
      "Training Epoch: 11 [35500/36045]\tLoss: 929.7890\n",
      "Training Epoch: 11 [35550/36045]\tLoss: 914.2594\n",
      "Training Epoch: 11 [35600/36045]\tLoss: 956.1670\n",
      "Training Epoch: 11 [35650/36045]\tLoss: 1043.3121\n",
      "Training Epoch: 11 [35700/36045]\tLoss: 967.0161\n",
      "Training Epoch: 11 [35750/36045]\tLoss: 1033.5529\n",
      "Training Epoch: 11 [35800/36045]\tLoss: 1039.8726\n",
      "Training Epoch: 11 [35850/36045]\tLoss: 1014.2676\n",
      "Training Epoch: 11 [35900/36045]\tLoss: 1059.8831\n",
      "Training Epoch: 11 [35950/36045]\tLoss: 1061.3983\n",
      "Training Epoch: 11 [36000/36045]\tLoss: 1043.5419\n",
      "Training Epoch: 11 [36045/36045]\tLoss: 1026.0547\n",
      "Training Epoch: 11 [4004/4004]\tLoss: 967.1835\n",
      "Training Epoch: 12 [50/36045]\tLoss: 974.3624\n",
      "Training Epoch: 12 [100/36045]\tLoss: 936.4755\n",
      "Training Epoch: 12 [150/36045]\tLoss: 937.9848\n",
      "Training Epoch: 12 [200/36045]\tLoss: 921.6182\n",
      "Training Epoch: 12 [250/36045]\tLoss: 1064.7800\n",
      "Training Epoch: 12 [300/36045]\tLoss: 1128.2782\n",
      "Training Epoch: 12 [350/36045]\tLoss: 1082.1310\n",
      "Training Epoch: 12 [400/36045]\tLoss: 1094.1184\n",
      "Training Epoch: 12 [450/36045]\tLoss: 1070.0592\n",
      "Training Epoch: 12 [500/36045]\tLoss: 1008.2560\n",
      "Training Epoch: 12 [550/36045]\tLoss: 1026.5375\n",
      "Training Epoch: 12 [600/36045]\tLoss: 987.1747\n",
      "Training Epoch: 12 [650/36045]\tLoss: 1021.8347\n",
      "Training Epoch: 12 [700/36045]\tLoss: 1011.7742\n",
      "Training Epoch: 12 [750/36045]\tLoss: 980.3793\n",
      "Training Epoch: 12 [800/36045]\tLoss: 999.4991\n",
      "Training Epoch: 12 [850/36045]\tLoss: 967.2534\n",
      "Training Epoch: 12 [900/36045]\tLoss: 924.2227\n",
      "Training Epoch: 12 [950/36045]\tLoss: 883.9704\n",
      "Training Epoch: 12 [1000/36045]\tLoss: 850.1384\n",
      "Training Epoch: 12 [1050/36045]\tLoss: 854.0364\n",
      "Training Epoch: 12 [1100/36045]\tLoss: 831.9062\n",
      "Training Epoch: 12 [1150/36045]\tLoss: 834.4587\n",
      "Training Epoch: 12 [1200/36045]\tLoss: 883.9870\n",
      "Training Epoch: 12 [1250/36045]\tLoss: 995.5507\n",
      "Training Epoch: 12 [1300/36045]\tLoss: 995.7421\n",
      "Training Epoch: 12 [1350/36045]\tLoss: 1003.5723\n",
      "Training Epoch: 12 [1400/36045]\tLoss: 1041.8911\n",
      "Training Epoch: 12 [1450/36045]\tLoss: 1004.2932\n",
      "Training Epoch: 12 [1500/36045]\tLoss: 935.9189\n",
      "Training Epoch: 12 [1550/36045]\tLoss: 964.9916\n",
      "Training Epoch: 12 [1600/36045]\tLoss: 974.5226\n",
      "Training Epoch: 12 [1650/36045]\tLoss: 959.0543\n",
      "Training Epoch: 12 [1700/36045]\tLoss: 971.2795\n",
      "Training Epoch: 12 [1750/36045]\tLoss: 1020.7273\n",
      "Training Epoch: 12 [1800/36045]\tLoss: 1002.0776\n",
      "Training Epoch: 12 [1850/36045]\tLoss: 1039.6971\n",
      "Training Epoch: 12 [1900/36045]\tLoss: 972.9960\n",
      "Training Epoch: 12 [1950/36045]\tLoss: 982.2747\n",
      "Training Epoch: 12 [2000/36045]\tLoss: 886.5999\n",
      "Training Epoch: 12 [2050/36045]\tLoss: 895.0909\n",
      "Training Epoch: 12 [2100/36045]\tLoss: 943.9418\n",
      "Training Epoch: 12 [2150/36045]\tLoss: 915.3204\n",
      "Training Epoch: 12 [2200/36045]\tLoss: 849.7444\n",
      "Training Epoch: 12 [2250/36045]\tLoss: 802.6987\n",
      "Training Epoch: 12 [2300/36045]\tLoss: 840.1247\n",
      "Training Epoch: 12 [2350/36045]\tLoss: 801.2557\n",
      "Training Epoch: 12 [2400/36045]\tLoss: 823.7934\n",
      "Training Epoch: 12 [2450/36045]\tLoss: 1022.9111\n",
      "Training Epoch: 12 [2500/36045]\tLoss: 1068.1378\n",
      "Training Epoch: 12 [2550/36045]\tLoss: 1065.8452\n",
      "Training Epoch: 12 [2600/36045]\tLoss: 1072.9871\n",
      "Training Epoch: 12 [2650/36045]\tLoss: 1201.6039\n",
      "Training Epoch: 12 [2700/36045]\tLoss: 1289.6656\n",
      "Training Epoch: 12 [2750/36045]\tLoss: 1371.6051\n",
      "Training Epoch: 12 [2800/36045]\tLoss: 1389.7474\n",
      "Training Epoch: 12 [2850/36045]\tLoss: 1167.9897\n",
      "Training Epoch: 12 [2900/36045]\tLoss: 1148.6982\n",
      "Training Epoch: 12 [2950/36045]\tLoss: 1105.5873\n",
      "Training Epoch: 12 [3000/36045]\tLoss: 1104.0858\n",
      "Training Epoch: 12 [3050/36045]\tLoss: 1145.4121\n",
      "Training Epoch: 12 [3100/36045]\tLoss: 1041.1525\n",
      "Training Epoch: 12 [3150/36045]\tLoss: 805.4237\n",
      "Training Epoch: 12 [3200/36045]\tLoss: 838.6198\n",
      "Training Epoch: 12 [3250/36045]\tLoss: 786.9715\n",
      "Training Epoch: 12 [3300/36045]\tLoss: 743.4347\n",
      "Training Epoch: 12 [3350/36045]\tLoss: 783.1061\n",
      "Training Epoch: 12 [3400/36045]\tLoss: 821.1354\n",
      "Training Epoch: 12 [3450/36045]\tLoss: 886.8638\n",
      "Training Epoch: 12 [3500/36045]\tLoss: 862.5952\n",
      "Training Epoch: 12 [3550/36045]\tLoss: 834.3796\n",
      "Training Epoch: 12 [3600/36045]\tLoss: 890.7789\n",
      "Training Epoch: 12 [3650/36045]\tLoss: 1035.0337\n",
      "Training Epoch: 12 [3700/36045]\tLoss: 1038.6216\n",
      "Training Epoch: 12 [3750/36045]\tLoss: 998.9846\n",
      "Training Epoch: 12 [3800/36045]\tLoss: 984.4506\n",
      "Training Epoch: 12 [3850/36045]\tLoss: 990.7907\n",
      "Training Epoch: 12 [3900/36045]\tLoss: 1000.9245\n",
      "Training Epoch: 12 [3950/36045]\tLoss: 956.7703\n",
      "Training Epoch: 12 [4000/36045]\tLoss: 980.9300\n",
      "Training Epoch: 12 [4050/36045]\tLoss: 900.9998\n",
      "Training Epoch: 12 [4100/36045]\tLoss: 873.8845\n",
      "Training Epoch: 12 [4150/36045]\tLoss: 904.0216\n",
      "Training Epoch: 12 [4200/36045]\tLoss: 893.0298\n",
      "Training Epoch: 12 [4250/36045]\tLoss: 897.4529\n",
      "Training Epoch: 12 [4300/36045]\tLoss: 922.5749\n",
      "Training Epoch: 12 [4350/36045]\tLoss: 902.0330\n",
      "Training Epoch: 12 [4400/36045]\tLoss: 861.9370\n",
      "Training Epoch: 12 [4450/36045]\tLoss: 927.7908\n",
      "Training Epoch: 12 [4500/36045]\tLoss: 985.0073\n",
      "Training Epoch: 12 [4550/36045]\tLoss: 1000.3684\n",
      "Training Epoch: 12 [4600/36045]\tLoss: 1022.1635\n",
      "Training Epoch: 12 [4650/36045]\tLoss: 1019.6484\n",
      "Training Epoch: 12 [4700/36045]\tLoss: 939.1514\n",
      "Training Epoch: 12 [4750/36045]\tLoss: 921.4248\n",
      "Training Epoch: 12 [4800/36045]\tLoss: 958.8291\n",
      "Training Epoch: 12 [4850/36045]\tLoss: 938.9862\n",
      "Training Epoch: 12 [4900/36045]\tLoss: 909.1106\n",
      "Training Epoch: 12 [4950/36045]\tLoss: 947.0969\n",
      "Training Epoch: 12 [5000/36045]\tLoss: 998.7165\n",
      "Training Epoch: 12 [5050/36045]\tLoss: 963.6402\n",
      "Training Epoch: 12 [5100/36045]\tLoss: 981.3027\n",
      "Training Epoch: 12 [5150/36045]\tLoss: 955.6669\n",
      "Training Epoch: 12 [5200/36045]\tLoss: 948.5968\n",
      "Training Epoch: 12 [5250/36045]\tLoss: 936.4631\n",
      "Training Epoch: 12 [5300/36045]\tLoss: 937.1469\n",
      "Training Epoch: 12 [5350/36045]\tLoss: 973.4268\n",
      "Training Epoch: 12 [5400/36045]\tLoss: 927.5703\n",
      "Training Epoch: 12 [5450/36045]\tLoss: 872.4070\n",
      "Training Epoch: 12 [5500/36045]\tLoss: 921.6214\n",
      "Training Epoch: 12 [5550/36045]\tLoss: 894.8217\n",
      "Training Epoch: 12 [5600/36045]\tLoss: 1019.3967\n",
      "Training Epoch: 12 [5650/36045]\tLoss: 970.6378\n",
      "Training Epoch: 12 [5700/36045]\tLoss: 918.5875\n",
      "Training Epoch: 12 [5750/36045]\tLoss: 900.7317\n",
      "Training Epoch: 12 [5800/36045]\tLoss: 960.1086\n",
      "Training Epoch: 12 [5850/36045]\tLoss: 929.0747\n",
      "Training Epoch: 12 [5900/36045]\tLoss: 1064.1138\n",
      "Training Epoch: 12 [5950/36045]\tLoss: 1096.6552\n",
      "Training Epoch: 12 [6000/36045]\tLoss: 1075.0264\n",
      "Training Epoch: 12 [6050/36045]\tLoss: 1036.8268\n",
      "Training Epoch: 12 [6100/36045]\tLoss: 1042.6327\n",
      "Training Epoch: 12 [6150/36045]\tLoss: 1003.8627\n",
      "Training Epoch: 12 [6200/36045]\tLoss: 998.9724\n",
      "Training Epoch: 12 [6250/36045]\tLoss: 1012.8892\n",
      "Training Epoch: 12 [6300/36045]\tLoss: 1037.2432\n",
      "Training Epoch: 12 [6350/36045]\tLoss: 1092.3596\n",
      "Training Epoch: 12 [6400/36045]\tLoss: 939.6268\n",
      "Training Epoch: 12 [6450/36045]\tLoss: 872.4368\n",
      "Training Epoch: 12 [6500/36045]\tLoss: 896.8716\n",
      "Training Epoch: 12 [6550/36045]\tLoss: 914.6489\n",
      "Training Epoch: 12 [6600/36045]\tLoss: 920.6385\n",
      "Training Epoch: 12 [6650/36045]\tLoss: 1036.5317\n",
      "Training Epoch: 12 [6700/36045]\tLoss: 1084.0702\n",
      "Training Epoch: 12 [6750/36045]\tLoss: 1053.4850\n",
      "Training Epoch: 12 [6800/36045]\tLoss: 1055.9332\n",
      "Training Epoch: 12 [6850/36045]\tLoss: 1038.3136\n",
      "Training Epoch: 12 [6900/36045]\tLoss: 915.9670\n",
      "Training Epoch: 12 [6950/36045]\tLoss: 862.7225\n",
      "Training Epoch: 12 [7000/36045]\tLoss: 916.6987\n",
      "Training Epoch: 12 [7050/36045]\tLoss: 941.2199\n",
      "Training Epoch: 12 [7100/36045]\tLoss: 935.5221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [7150/36045]\tLoss: 955.9614\n",
      "Training Epoch: 12 [7200/36045]\tLoss: 963.6541\n",
      "Training Epoch: 12 [7250/36045]\tLoss: 961.0417\n",
      "Training Epoch: 12 [7300/36045]\tLoss: 943.8567\n",
      "Training Epoch: 12 [7350/36045]\tLoss: 937.7243\n",
      "Training Epoch: 12 [7400/36045]\tLoss: 846.7856\n",
      "Training Epoch: 12 [7450/36045]\tLoss: 848.7516\n",
      "Training Epoch: 12 [7500/36045]\tLoss: 848.0052\n",
      "Training Epoch: 12 [7550/36045]\tLoss: 807.2037\n",
      "Training Epoch: 12 [7600/36045]\tLoss: 907.7179\n",
      "Training Epoch: 12 [7650/36045]\tLoss: 978.4542\n",
      "Training Epoch: 12 [7700/36045]\tLoss: 936.0120\n",
      "Training Epoch: 12 [7750/36045]\tLoss: 952.0488\n",
      "Training Epoch: 12 [7800/36045]\tLoss: 934.9908\n",
      "Training Epoch: 12 [7850/36045]\tLoss: 885.9852\n",
      "Training Epoch: 12 [7900/36045]\tLoss: 936.0073\n",
      "Training Epoch: 12 [7950/36045]\tLoss: 932.2855\n",
      "Training Epoch: 12 [8000/36045]\tLoss: 953.0076\n",
      "Training Epoch: 12 [8050/36045]\tLoss: 905.4172\n",
      "Training Epoch: 12 [8100/36045]\tLoss: 937.9760\n",
      "Training Epoch: 12 [8150/36045]\tLoss: 1060.8234\n",
      "Training Epoch: 12 [8200/36045]\tLoss: 1045.6807\n",
      "Training Epoch: 12 [8250/36045]\tLoss: 1005.6224\n",
      "Training Epoch: 12 [8300/36045]\tLoss: 1087.8516\n",
      "Training Epoch: 12 [8350/36045]\tLoss: 1006.2499\n",
      "Training Epoch: 12 [8400/36045]\tLoss: 912.1891\n",
      "Training Epoch: 12 [8450/36045]\tLoss: 857.3283\n",
      "Training Epoch: 12 [8500/36045]\tLoss: 904.1879\n",
      "Training Epoch: 12 [8550/36045]\tLoss: 885.0953\n",
      "Training Epoch: 12 [8600/36045]\tLoss: 880.0266\n",
      "Training Epoch: 12 [8650/36045]\tLoss: 934.0981\n",
      "Training Epoch: 12 [8700/36045]\tLoss: 985.3472\n",
      "Training Epoch: 12 [8750/36045]\tLoss: 962.4767\n",
      "Training Epoch: 12 [8800/36045]\tLoss: 970.9709\n",
      "Training Epoch: 12 [8850/36045]\tLoss: 959.7162\n",
      "Training Epoch: 12 [8900/36045]\tLoss: 867.7977\n",
      "Training Epoch: 12 [8950/36045]\tLoss: 894.8593\n",
      "Training Epoch: 12 [9000/36045]\tLoss: 904.8957\n",
      "Training Epoch: 12 [9050/36045]\tLoss: 902.7531\n",
      "Training Epoch: 12 [9100/36045]\tLoss: 926.9431\n",
      "Training Epoch: 12 [9150/36045]\tLoss: 686.1573\n",
      "Training Epoch: 12 [9200/36045]\tLoss: 526.1210\n",
      "Training Epoch: 12 [9250/36045]\tLoss: 566.6703\n",
      "Training Epoch: 12 [9300/36045]\tLoss: 587.7001\n",
      "Training Epoch: 12 [9350/36045]\tLoss: 536.7017\n",
      "Training Epoch: 12 [9400/36045]\tLoss: 1053.5488\n",
      "Training Epoch: 12 [9450/36045]\tLoss: 1114.1074\n",
      "Training Epoch: 12 [9500/36045]\tLoss: 1098.6075\n",
      "Training Epoch: 12 [9550/36045]\tLoss: 1167.3029\n",
      "Training Epoch: 12 [9600/36045]\tLoss: 859.3845\n",
      "Training Epoch: 12 [9650/36045]\tLoss: 856.7177\n",
      "Training Epoch: 12 [9700/36045]\tLoss: 845.3555\n",
      "Training Epoch: 12 [9750/36045]\tLoss: 848.1190\n",
      "Training Epoch: 12 [9800/36045]\tLoss: 1092.3948\n",
      "Training Epoch: 12 [9850/36045]\tLoss: 1153.0735\n",
      "Training Epoch: 12 [9900/36045]\tLoss: 1184.2463\n",
      "Training Epoch: 12 [9950/36045]\tLoss: 1149.9847\n",
      "Training Epoch: 12 [10000/36045]\tLoss: 1060.0339\n",
      "Training Epoch: 12 [10050/36045]\tLoss: 891.9982\n",
      "Training Epoch: 12 [10100/36045]\tLoss: 891.6399\n",
      "Training Epoch: 12 [10150/36045]\tLoss: 905.5892\n",
      "Training Epoch: 12 [10200/36045]\tLoss: 896.3397\n",
      "Training Epoch: 12 [10250/36045]\tLoss: 1076.6152\n",
      "Training Epoch: 12 [10300/36045]\tLoss: 1044.5394\n",
      "Training Epoch: 12 [10350/36045]\tLoss: 1100.7950\n",
      "Training Epoch: 12 [10400/36045]\tLoss: 1091.4810\n",
      "Training Epoch: 12 [10450/36045]\tLoss: 1011.9991\n",
      "Training Epoch: 12 [10500/36045]\tLoss: 853.6956\n",
      "Training Epoch: 12 [10550/36045]\tLoss: 851.2726\n",
      "Training Epoch: 12 [10600/36045]\tLoss: 878.7460\n",
      "Training Epoch: 12 [10650/36045]\tLoss: 886.6700\n",
      "Training Epoch: 12 [10700/36045]\tLoss: 984.5673\n",
      "Training Epoch: 12 [10750/36045]\tLoss: 1064.6265\n",
      "Training Epoch: 12 [10800/36045]\tLoss: 989.0391\n",
      "Training Epoch: 12 [10850/36045]\tLoss: 1043.2058\n",
      "Training Epoch: 12 [10900/36045]\tLoss: 1086.2191\n",
      "Training Epoch: 12 [10950/36045]\tLoss: 817.4536\n",
      "Training Epoch: 12 [11000/36045]\tLoss: 811.2638\n",
      "Training Epoch: 12 [11050/36045]\tLoss: 864.0317\n",
      "Training Epoch: 12 [11100/36045]\tLoss: 880.4979\n",
      "Training Epoch: 12 [11150/36045]\tLoss: 953.9663\n",
      "Training Epoch: 12 [11200/36045]\tLoss: 986.3114\n",
      "Training Epoch: 12 [11250/36045]\tLoss: 1002.0273\n",
      "Training Epoch: 12 [11300/36045]\tLoss: 981.0272\n",
      "Training Epoch: 12 [11350/36045]\tLoss: 975.3326\n",
      "Training Epoch: 12 [11400/36045]\tLoss: 925.6713\n",
      "Training Epoch: 12 [11450/36045]\tLoss: 886.3323\n",
      "Training Epoch: 12 [11500/36045]\tLoss: 885.7700\n",
      "Training Epoch: 12 [11550/36045]\tLoss: 907.5331\n",
      "Training Epoch: 12 [11600/36045]\tLoss: 982.6993\n",
      "Training Epoch: 12 [11650/36045]\tLoss: 1038.2295\n",
      "Training Epoch: 12 [11700/36045]\tLoss: 1035.9952\n",
      "Training Epoch: 12 [11750/36045]\tLoss: 1060.3958\n",
      "Training Epoch: 12 [11800/36045]\tLoss: 1109.5544\n",
      "Training Epoch: 12 [11850/36045]\tLoss: 1148.5142\n",
      "Training Epoch: 12 [11900/36045]\tLoss: 1384.2778\n",
      "Training Epoch: 12 [11950/36045]\tLoss: 1377.0953\n",
      "Training Epoch: 12 [12000/36045]\tLoss: 1404.0817\n",
      "Training Epoch: 12 [12050/36045]\tLoss: 1357.1219\n",
      "Training Epoch: 12 [12100/36045]\tLoss: 947.4161\n",
      "Training Epoch: 12 [12150/36045]\tLoss: 765.8335\n",
      "Training Epoch: 12 [12200/36045]\tLoss: 759.0900\n",
      "Training Epoch: 12 [12250/36045]\tLoss: 771.7310\n",
      "Training Epoch: 12 [12300/36045]\tLoss: 951.9688\n",
      "Training Epoch: 12 [12350/36045]\tLoss: 1015.3318\n",
      "Training Epoch: 12 [12400/36045]\tLoss: 1032.5261\n",
      "Training Epoch: 12 [12450/36045]\tLoss: 1017.9362\n",
      "Training Epoch: 12 [12500/36045]\tLoss: 1056.0543\n",
      "Training Epoch: 12 [12550/36045]\tLoss: 1013.0558\n",
      "Training Epoch: 12 [12600/36045]\tLoss: 948.6794\n",
      "Training Epoch: 12 [12650/36045]\tLoss: 943.6018\n",
      "Training Epoch: 12 [12700/36045]\tLoss: 972.9724\n",
      "Training Epoch: 12 [12750/36045]\tLoss: 970.4619\n",
      "Training Epoch: 12 [12800/36045]\tLoss: 950.1420\n",
      "Training Epoch: 12 [12850/36045]\tLoss: 989.7681\n",
      "Training Epoch: 12 [12900/36045]\tLoss: 947.6333\n",
      "Training Epoch: 12 [12950/36045]\tLoss: 940.1457\n",
      "Training Epoch: 12 [13000/36045]\tLoss: 973.4142\n",
      "Training Epoch: 12 [13050/36045]\tLoss: 899.9240\n",
      "Training Epoch: 12 [13100/36045]\tLoss: 943.5385\n",
      "Training Epoch: 12 [13150/36045]\tLoss: 939.8186\n",
      "Training Epoch: 12 [13200/36045]\tLoss: 893.0291\n",
      "Training Epoch: 12 [13250/36045]\tLoss: 939.7632\n",
      "Training Epoch: 12 [13300/36045]\tLoss: 980.1855\n",
      "Training Epoch: 12 [13350/36045]\tLoss: 952.3002\n",
      "Training Epoch: 12 [13400/36045]\tLoss: 959.1446\n",
      "Training Epoch: 12 [13450/36045]\tLoss: 945.6981\n",
      "Training Epoch: 12 [13500/36045]\tLoss: 983.6179\n",
      "Training Epoch: 12 [13550/36045]\tLoss: 1116.0740\n",
      "Training Epoch: 12 [13600/36045]\tLoss: 1147.9652\n",
      "Training Epoch: 12 [13650/36045]\tLoss: 1230.5834\n",
      "Training Epoch: 12 [13700/36045]\tLoss: 1102.2405\n",
      "Training Epoch: 12 [13750/36045]\tLoss: 971.0031\n",
      "Training Epoch: 12 [13800/36045]\tLoss: 946.3839\n",
      "Training Epoch: 12 [13850/36045]\tLoss: 927.3924\n",
      "Training Epoch: 12 [13900/36045]\tLoss: 940.6090\n",
      "Training Epoch: 12 [13950/36045]\tLoss: 975.7890\n",
      "Training Epoch: 12 [14000/36045]\tLoss: 1015.8085\n",
      "Training Epoch: 12 [14050/36045]\tLoss: 976.3901\n",
      "Training Epoch: 12 [14100/36045]\tLoss: 975.0054\n",
      "Training Epoch: 12 [14150/36045]\tLoss: 956.6015\n",
      "Training Epoch: 12 [14200/36045]\tLoss: 1021.6949\n",
      "Training Epoch: 12 [14250/36045]\tLoss: 1117.3479\n",
      "Training Epoch: 12 [14300/36045]\tLoss: 1121.9672\n",
      "Training Epoch: 12 [14350/36045]\tLoss: 1072.9657\n",
      "Training Epoch: 12 [14400/36045]\tLoss: 1057.6775\n",
      "Training Epoch: 12 [14450/36045]\tLoss: 1104.5708\n",
      "Training Epoch: 12 [14500/36045]\tLoss: 1025.6313\n",
      "Training Epoch: 12 [14550/36045]\tLoss: 1072.0502\n",
      "Training Epoch: 12 [14600/36045]\tLoss: 1044.6282\n",
      "Training Epoch: 12 [14650/36045]\tLoss: 1049.8564\n",
      "Training Epoch: 12 [14700/36045]\tLoss: 985.0577\n",
      "Training Epoch: 12 [14750/36045]\tLoss: 847.8856\n",
      "Training Epoch: 12 [14800/36045]\tLoss: 834.1554\n",
      "Training Epoch: 12 [14850/36045]\tLoss: 841.1178\n",
      "Training Epoch: 12 [14900/36045]\tLoss: 835.4756\n",
      "Training Epoch: 12 [14950/36045]\tLoss: 843.3687\n",
      "Training Epoch: 12 [15000/36045]\tLoss: 867.3824\n",
      "Training Epoch: 12 [15050/36045]\tLoss: 872.1766\n",
      "Training Epoch: 12 [15100/36045]\tLoss: 851.4775\n",
      "Training Epoch: 12 [15150/36045]\tLoss: 841.9577\n",
      "Training Epoch: 12 [15200/36045]\tLoss: 776.9419\n",
      "Training Epoch: 12 [15250/36045]\tLoss: 813.2144\n",
      "Training Epoch: 12 [15300/36045]\tLoss: 791.9442\n",
      "Training Epoch: 12 [15350/36045]\tLoss: 807.8174\n",
      "Training Epoch: 12 [15400/36045]\tLoss: 792.2615\n",
      "Training Epoch: 12 [15450/36045]\tLoss: 782.1622\n",
      "Training Epoch: 12 [15500/36045]\tLoss: 807.5048\n",
      "Training Epoch: 12 [15550/36045]\tLoss: 791.5974\n",
      "Training Epoch: 12 [15600/36045]\tLoss: 874.8184\n",
      "Training Epoch: 12 [15650/36045]\tLoss: 902.1559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [15700/36045]\tLoss: 887.4268\n",
      "Training Epoch: 12 [15750/36045]\tLoss: 876.0830\n",
      "Training Epoch: 12 [15800/36045]\tLoss: 803.4933\n",
      "Training Epoch: 12 [15850/36045]\tLoss: 812.9630\n",
      "Training Epoch: 12 [15900/36045]\tLoss: 822.6341\n",
      "Training Epoch: 12 [15950/36045]\tLoss: 848.6037\n",
      "Training Epoch: 12 [16000/36045]\tLoss: 837.8339\n",
      "Training Epoch: 12 [16050/36045]\tLoss: 802.6956\n",
      "Training Epoch: 12 [16100/36045]\tLoss: 741.8198\n",
      "Training Epoch: 12 [16150/36045]\tLoss: 723.4393\n",
      "Training Epoch: 12 [16200/36045]\tLoss: 868.5080\n",
      "Training Epoch: 12 [16250/36045]\tLoss: 901.2434\n",
      "Training Epoch: 12 [16300/36045]\tLoss: 979.8174\n",
      "Training Epoch: 12 [16350/36045]\tLoss: 994.7414\n",
      "Training Epoch: 12 [16400/36045]\tLoss: 971.2957\n",
      "Training Epoch: 12 [16450/36045]\tLoss: 946.1915\n",
      "Training Epoch: 12 [16500/36045]\tLoss: 942.7523\n",
      "Training Epoch: 12 [16550/36045]\tLoss: 899.3151\n",
      "Training Epoch: 12 [16600/36045]\tLoss: 942.3530\n",
      "Training Epoch: 12 [16650/36045]\tLoss: 967.9843\n",
      "Training Epoch: 12 [16700/36045]\tLoss: 941.9942\n",
      "Training Epoch: 12 [16750/36045]\tLoss: 932.2881\n",
      "Training Epoch: 12 [16800/36045]\tLoss: 949.3667\n",
      "Training Epoch: 12 [16850/36045]\tLoss: 902.2570\n",
      "Training Epoch: 12 [16900/36045]\tLoss: 919.9377\n",
      "Training Epoch: 12 [16950/36045]\tLoss: 944.4387\n",
      "Training Epoch: 12 [17000/36045]\tLoss: 920.2340\n",
      "Training Epoch: 12 [17050/36045]\tLoss: 968.4086\n",
      "Training Epoch: 12 [17100/36045]\tLoss: 974.2552\n",
      "Training Epoch: 12 [17150/36045]\tLoss: 844.9135\n",
      "Training Epoch: 12 [17200/36045]\tLoss: 795.1552\n",
      "Training Epoch: 12 [17250/36045]\tLoss: 831.5950\n",
      "Training Epoch: 12 [17300/36045]\tLoss: 877.3623\n",
      "Training Epoch: 12 [17350/36045]\tLoss: 834.8003\n",
      "Training Epoch: 12 [17400/36045]\tLoss: 852.6636\n",
      "Training Epoch: 12 [17450/36045]\tLoss: 878.2239\n",
      "Training Epoch: 12 [17500/36045]\tLoss: 863.8200\n",
      "Training Epoch: 12 [17550/36045]\tLoss: 874.5660\n",
      "Training Epoch: 12 [17600/36045]\tLoss: 860.2764\n",
      "Training Epoch: 12 [17650/36045]\tLoss: 884.1903\n",
      "Training Epoch: 12 [17700/36045]\tLoss: 860.1091\n",
      "Training Epoch: 12 [17750/36045]\tLoss: 880.8111\n",
      "Training Epoch: 12 [17800/36045]\tLoss: 873.1440\n",
      "Training Epoch: 12 [17850/36045]\tLoss: 844.1673\n",
      "Training Epoch: 12 [17900/36045]\tLoss: 877.8369\n",
      "Training Epoch: 12 [17950/36045]\tLoss: 886.4384\n",
      "Training Epoch: 12 [18000/36045]\tLoss: 875.9802\n",
      "Training Epoch: 12 [18050/36045]\tLoss: 998.9402\n",
      "Training Epoch: 12 [18100/36045]\tLoss: 1005.4879\n",
      "Training Epoch: 12 [18150/36045]\tLoss: 1010.8027\n",
      "Training Epoch: 12 [18200/36045]\tLoss: 1000.4193\n",
      "Training Epoch: 12 [18250/36045]\tLoss: 1020.4105\n",
      "Training Epoch: 12 [18300/36045]\tLoss: 931.2778\n",
      "Training Epoch: 12 [18350/36045]\tLoss: 993.6840\n",
      "Training Epoch: 12 [18400/36045]\tLoss: 967.4839\n",
      "Training Epoch: 12 [18450/36045]\tLoss: 947.2473\n",
      "Training Epoch: 12 [18500/36045]\tLoss: 949.5701\n",
      "Training Epoch: 12 [18550/36045]\tLoss: 932.5616\n",
      "Training Epoch: 12 [18600/36045]\tLoss: 922.6407\n",
      "Training Epoch: 12 [18650/36045]\tLoss: 978.5181\n",
      "Training Epoch: 12 [18700/36045]\tLoss: 1031.2078\n",
      "Training Epoch: 12 [18750/36045]\tLoss: 1011.0451\n",
      "Training Epoch: 12 [18800/36045]\tLoss: 1041.4290\n",
      "Training Epoch: 12 [18850/36045]\tLoss: 981.2968\n",
      "Training Epoch: 12 [18900/36045]\tLoss: 1051.4198\n",
      "Training Epoch: 12 [18950/36045]\tLoss: 977.8071\n",
      "Training Epoch: 12 [19000/36045]\tLoss: 847.4384\n",
      "Training Epoch: 12 [19050/36045]\tLoss: 815.9989\n",
      "Training Epoch: 12 [19100/36045]\tLoss: 836.5297\n",
      "Training Epoch: 12 [19150/36045]\tLoss: 821.5322\n",
      "Training Epoch: 12 [19200/36045]\tLoss: 855.5035\n",
      "Training Epoch: 12 [19250/36045]\tLoss: 866.6126\n",
      "Training Epoch: 12 [19300/36045]\tLoss: 890.2910\n",
      "Training Epoch: 12 [19350/36045]\tLoss: 868.2294\n",
      "Training Epoch: 12 [19400/36045]\tLoss: 893.1310\n",
      "Training Epoch: 12 [19450/36045]\tLoss: 880.2250\n",
      "Training Epoch: 12 [19500/36045]\tLoss: 884.4724\n",
      "Training Epoch: 12 [19550/36045]\tLoss: 886.3052\n",
      "Training Epoch: 12 [19600/36045]\tLoss: 931.5419\n",
      "Training Epoch: 12 [19650/36045]\tLoss: 1193.1461\n",
      "Training Epoch: 12 [19700/36045]\tLoss: 1145.8251\n",
      "Training Epoch: 12 [19750/36045]\tLoss: 1144.5686\n",
      "Training Epoch: 12 [19800/36045]\tLoss: 1137.7047\n",
      "Training Epoch: 12 [19850/36045]\tLoss: 793.5349\n",
      "Training Epoch: 12 [19900/36045]\tLoss: 763.2858\n",
      "Training Epoch: 12 [19950/36045]\tLoss: 769.6177\n",
      "Training Epoch: 12 [20000/36045]\tLoss: 769.0805\n",
      "Training Epoch: 12 [20050/36045]\tLoss: 860.9840\n",
      "Training Epoch: 12 [20100/36045]\tLoss: 863.0894\n",
      "Training Epoch: 12 [20150/36045]\tLoss: 867.9418\n",
      "Training Epoch: 12 [20200/36045]\tLoss: 864.6466\n",
      "Training Epoch: 12 [20250/36045]\tLoss: 917.8910\n",
      "Training Epoch: 12 [20300/36045]\tLoss: 957.1524\n",
      "Training Epoch: 12 [20350/36045]\tLoss: 987.3260\n",
      "Training Epoch: 12 [20400/36045]\tLoss: 1002.3892\n",
      "Training Epoch: 12 [20450/36045]\tLoss: 980.4902\n",
      "Training Epoch: 12 [20500/36045]\tLoss: 952.1348\n",
      "Training Epoch: 12 [20550/36045]\tLoss: 852.4988\n",
      "Training Epoch: 12 [20600/36045]\tLoss: 869.0184\n",
      "Training Epoch: 12 [20650/36045]\tLoss: 864.4330\n",
      "Training Epoch: 12 [20700/36045]\tLoss: 845.7094\n",
      "Training Epoch: 12 [20750/36045]\tLoss: 902.1121\n",
      "Training Epoch: 12 [20800/36045]\tLoss: 982.8522\n",
      "Training Epoch: 12 [20850/36045]\tLoss: 973.9169\n",
      "Training Epoch: 12 [20900/36045]\tLoss: 1033.6818\n",
      "Training Epoch: 12 [20950/36045]\tLoss: 974.6100\n",
      "Training Epoch: 12 [21000/36045]\tLoss: 917.8120\n",
      "Training Epoch: 12 [21050/36045]\tLoss: 781.9020\n",
      "Training Epoch: 12 [21100/36045]\tLoss: 782.4158\n",
      "Training Epoch: 12 [21150/36045]\tLoss: 837.9469\n",
      "Training Epoch: 12 [21200/36045]\tLoss: 839.0598\n",
      "Training Epoch: 12 [21250/36045]\tLoss: 799.7734\n",
      "Training Epoch: 12 [21300/36045]\tLoss: 938.8638\n",
      "Training Epoch: 12 [21350/36045]\tLoss: 936.9652\n",
      "Training Epoch: 12 [21400/36045]\tLoss: 943.0833\n",
      "Training Epoch: 12 [21450/36045]\tLoss: 957.9650\n",
      "Training Epoch: 12 [21500/36045]\tLoss: 955.7502\n",
      "Training Epoch: 12 [21550/36045]\tLoss: 1060.5400\n",
      "Training Epoch: 12 [21600/36045]\tLoss: 1061.7776\n",
      "Training Epoch: 12 [21650/36045]\tLoss: 1079.8918\n",
      "Training Epoch: 12 [21700/36045]\tLoss: 1075.9362\n",
      "Training Epoch: 12 [21750/36045]\tLoss: 1041.2775\n",
      "Training Epoch: 12 [21800/36045]\tLoss: 782.8137\n",
      "Training Epoch: 12 [21850/36045]\tLoss: 758.9452\n",
      "Training Epoch: 12 [21900/36045]\tLoss: 776.8353\n",
      "Training Epoch: 12 [21950/36045]\tLoss: 770.1200\n",
      "Training Epoch: 12 [22000/36045]\tLoss: 779.6465\n",
      "Training Epoch: 12 [22050/36045]\tLoss: 825.7391\n",
      "Training Epoch: 12 [22100/36045]\tLoss: 814.4598\n",
      "Training Epoch: 12 [22150/36045]\tLoss: 788.4988\n",
      "Training Epoch: 12 [22200/36045]\tLoss: 814.3091\n",
      "Training Epoch: 12 [22250/36045]\tLoss: 822.9394\n",
      "Training Epoch: 12 [22300/36045]\tLoss: 883.5468\n",
      "Training Epoch: 12 [22350/36045]\tLoss: 913.9699\n",
      "Training Epoch: 12 [22400/36045]\tLoss: 940.8441\n",
      "Training Epoch: 12 [22450/36045]\tLoss: 921.5270\n",
      "Training Epoch: 12 [22500/36045]\tLoss: 895.1865\n",
      "Training Epoch: 12 [22550/36045]\tLoss: 952.8908\n",
      "Training Epoch: 12 [22600/36045]\tLoss: 1049.7162\n",
      "Training Epoch: 12 [22650/36045]\tLoss: 1099.9268\n",
      "Training Epoch: 12 [22700/36045]\tLoss: 1127.2354\n",
      "Training Epoch: 12 [22750/36045]\tLoss: 1151.8877\n",
      "Training Epoch: 12 [22800/36045]\tLoss: 1202.9308\n",
      "Training Epoch: 12 [22850/36045]\tLoss: 993.1617\n",
      "Training Epoch: 12 [22900/36045]\tLoss: 997.2144\n",
      "Training Epoch: 12 [22950/36045]\tLoss: 966.8574\n",
      "Training Epoch: 12 [23000/36045]\tLoss: 973.2471\n",
      "Training Epoch: 12 [23050/36045]\tLoss: 873.4006\n",
      "Training Epoch: 12 [23100/36045]\tLoss: 892.6920\n",
      "Training Epoch: 12 [23150/36045]\tLoss: 875.0340\n",
      "Training Epoch: 12 [23200/36045]\tLoss: 824.7906\n",
      "Training Epoch: 12 [23250/36045]\tLoss: 831.3941\n",
      "Training Epoch: 12 [23300/36045]\tLoss: 824.9730\n",
      "Training Epoch: 12 [23350/36045]\tLoss: 849.1450\n",
      "Training Epoch: 12 [23400/36045]\tLoss: 924.0670\n",
      "Training Epoch: 12 [23450/36045]\tLoss: 912.3856\n",
      "Training Epoch: 12 [23500/36045]\tLoss: 875.3470\n",
      "Training Epoch: 12 [23550/36045]\tLoss: 946.2930\n",
      "Training Epoch: 12 [23600/36045]\tLoss: 1064.1000\n",
      "Training Epoch: 12 [23650/36045]\tLoss: 1088.0944\n",
      "Training Epoch: 12 [23700/36045]\tLoss: 1099.0392\n",
      "Training Epoch: 12 [23750/36045]\tLoss: 1064.1415\n",
      "Training Epoch: 12 [23800/36045]\tLoss: 839.8937\n",
      "Training Epoch: 12 [23850/36045]\tLoss: 873.7941\n",
      "Training Epoch: 12 [23900/36045]\tLoss: 863.4539\n",
      "Training Epoch: 12 [23950/36045]\tLoss: 842.9103\n",
      "Training Epoch: 12 [24000/36045]\tLoss: 818.9062\n",
      "Training Epoch: 12 [24050/36045]\tLoss: 751.9390\n",
      "Training Epoch: 12 [24100/36045]\tLoss: 788.9226\n",
      "Training Epoch: 12 [24150/36045]\tLoss: 788.2530\n",
      "Training Epoch: 12 [24200/36045]\tLoss: 772.2122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [24250/36045]\tLoss: 752.4473\n",
      "Training Epoch: 12 [24300/36045]\tLoss: 814.9825\n",
      "Training Epoch: 12 [24350/36045]\tLoss: 838.1146\n",
      "Training Epoch: 12 [24400/36045]\tLoss: 861.1477\n",
      "Training Epoch: 12 [24450/36045]\tLoss: 823.7586\n",
      "Training Epoch: 12 [24500/36045]\tLoss: 868.8326\n",
      "Training Epoch: 12 [24550/36045]\tLoss: 961.8190\n",
      "Training Epoch: 12 [24600/36045]\tLoss: 956.7831\n",
      "Training Epoch: 12 [24650/36045]\tLoss: 923.4145\n",
      "Training Epoch: 12 [24700/36045]\tLoss: 937.5129\n",
      "Training Epoch: 12 [24750/36045]\tLoss: 864.7037\n",
      "Training Epoch: 12 [24800/36045]\tLoss: 748.1025\n",
      "Training Epoch: 12 [24850/36045]\tLoss: 768.1901\n",
      "Training Epoch: 12 [24900/36045]\tLoss: 767.6688\n",
      "Training Epoch: 12 [24950/36045]\tLoss: 770.7691\n",
      "Training Epoch: 12 [25000/36045]\tLoss: 735.6811\n",
      "Training Epoch: 12 [25050/36045]\tLoss: 698.4531\n",
      "Training Epoch: 12 [25100/36045]\tLoss: 625.9721\n",
      "Training Epoch: 12 [25150/36045]\tLoss: 581.7913\n",
      "Training Epoch: 12 [25200/36045]\tLoss: 579.8485\n",
      "Training Epoch: 12 [25250/36045]\tLoss: 616.1644\n",
      "Training Epoch: 12 [25300/36045]\tLoss: 801.4200\n",
      "Training Epoch: 12 [25350/36045]\tLoss: 800.6527\n",
      "Training Epoch: 12 [25400/36045]\tLoss: 748.5160\n",
      "Training Epoch: 12 [25450/36045]\tLoss: 748.9318\n",
      "Training Epoch: 12 [25500/36045]\tLoss: 816.9356\n",
      "Training Epoch: 12 [25550/36045]\tLoss: 956.4352\n",
      "Training Epoch: 12 [25600/36045]\tLoss: 962.8362\n",
      "Training Epoch: 12 [25650/36045]\tLoss: 926.3902\n",
      "Training Epoch: 12 [25700/36045]\tLoss: 943.5942\n",
      "Training Epoch: 12 [25750/36045]\tLoss: 903.7470\n",
      "Training Epoch: 12 [25800/36045]\tLoss: 561.0822\n",
      "Training Epoch: 12 [25850/36045]\tLoss: 573.1514\n",
      "Training Epoch: 12 [25900/36045]\tLoss: 549.7876\n",
      "Training Epoch: 12 [25950/36045]\tLoss: 562.0817\n",
      "Training Epoch: 12 [26000/36045]\tLoss: 691.1454\n",
      "Training Epoch: 12 [26050/36045]\tLoss: 936.5983\n",
      "Training Epoch: 12 [26100/36045]\tLoss: 972.0667\n",
      "Training Epoch: 12 [26150/36045]\tLoss: 965.0511\n",
      "Training Epoch: 12 [26200/36045]\tLoss: 938.7676\n",
      "Training Epoch: 12 [26250/36045]\tLoss: 972.1623\n",
      "Training Epoch: 12 [26300/36045]\tLoss: 846.0493\n",
      "Training Epoch: 12 [26350/36045]\tLoss: 851.2100\n",
      "Training Epoch: 12 [26400/36045]\tLoss: 832.7188\n",
      "Training Epoch: 12 [26450/36045]\tLoss: 758.3744\n",
      "Training Epoch: 12 [26500/36045]\tLoss: 914.0988\n",
      "Training Epoch: 12 [26550/36045]\tLoss: 931.6747\n",
      "Training Epoch: 12 [26600/36045]\tLoss: 924.7570\n",
      "Training Epoch: 12 [26650/36045]\tLoss: 944.5713\n",
      "Training Epoch: 12 [26700/36045]\tLoss: 921.9838\n",
      "Training Epoch: 12 [26750/36045]\tLoss: 862.3077\n",
      "Training Epoch: 12 [26800/36045]\tLoss: 630.6169\n",
      "Training Epoch: 12 [26850/36045]\tLoss: 529.2646\n",
      "Training Epoch: 12 [26900/36045]\tLoss: 533.0720\n",
      "Training Epoch: 12 [26950/36045]\tLoss: 583.1497\n",
      "Training Epoch: 12 [27000/36045]\tLoss: 923.4866\n",
      "Training Epoch: 12 [27050/36045]\tLoss: 975.2543\n",
      "Training Epoch: 12 [27100/36045]\tLoss: 945.5021\n",
      "Training Epoch: 12 [27150/36045]\tLoss: 994.1337\n",
      "Training Epoch: 12 [27200/36045]\tLoss: 741.1614\n",
      "Training Epoch: 12 [27250/36045]\tLoss: 739.3899\n",
      "Training Epoch: 12 [27300/36045]\tLoss: 714.9796\n",
      "Training Epoch: 12 [27350/36045]\tLoss: 721.2681\n",
      "Training Epoch: 12 [27400/36045]\tLoss: 717.9822\n",
      "Training Epoch: 12 [27450/36045]\tLoss: 903.1635\n",
      "Training Epoch: 12 [27500/36045]\tLoss: 969.2792\n",
      "Training Epoch: 12 [27550/36045]\tLoss: 962.1030\n",
      "Training Epoch: 12 [27600/36045]\tLoss: 966.6400\n",
      "Training Epoch: 12 [27650/36045]\tLoss: 965.1893\n",
      "Training Epoch: 12 [27700/36045]\tLoss: 994.7772\n",
      "Training Epoch: 12 [27750/36045]\tLoss: 1010.9438\n",
      "Training Epoch: 12 [27800/36045]\tLoss: 992.7729\n",
      "Training Epoch: 12 [27850/36045]\tLoss: 973.2900\n",
      "Training Epoch: 12 [27900/36045]\tLoss: 864.9476\n",
      "Training Epoch: 12 [27950/36045]\tLoss: 714.0579\n",
      "Training Epoch: 12 [28000/36045]\tLoss: 682.7373\n",
      "Training Epoch: 12 [28050/36045]\tLoss: 705.0137\n",
      "Training Epoch: 12 [28100/36045]\tLoss: 691.7368\n",
      "Training Epoch: 12 [28150/36045]\tLoss: 741.8696\n",
      "Training Epoch: 12 [28200/36045]\tLoss: 743.7913\n",
      "Training Epoch: 12 [28250/36045]\tLoss: 748.4161\n",
      "Training Epoch: 12 [28300/36045]\tLoss: 706.0262\n",
      "Training Epoch: 12 [28350/36045]\tLoss: 704.5054\n",
      "Training Epoch: 12 [28400/36045]\tLoss: 1051.7341\n",
      "Training Epoch: 12 [28450/36045]\tLoss: 938.8260\n",
      "Training Epoch: 12 [28500/36045]\tLoss: 814.6084\n",
      "Training Epoch: 12 [28550/36045]\tLoss: 746.5797\n",
      "Training Epoch: 12 [28600/36045]\tLoss: 835.0894\n",
      "Training Epoch: 12 [28650/36045]\tLoss: 981.4169\n",
      "Training Epoch: 12 [28700/36045]\tLoss: 976.2032\n",
      "Training Epoch: 12 [28750/36045]\tLoss: 964.2424\n",
      "Training Epoch: 12 [28800/36045]\tLoss: 973.5645\n",
      "Training Epoch: 12 [28850/36045]\tLoss: 838.4728\n",
      "Training Epoch: 12 [28900/36045]\tLoss: 663.8124\n",
      "Training Epoch: 12 [28950/36045]\tLoss: 657.1415\n",
      "Training Epoch: 12 [29000/36045]\tLoss: 661.5220\n",
      "Training Epoch: 12 [29050/36045]\tLoss: 671.6703\n",
      "Training Epoch: 12 [29100/36045]\tLoss: 693.5737\n",
      "Training Epoch: 12 [29150/36045]\tLoss: 675.9867\n",
      "Training Epoch: 12 [29200/36045]\tLoss: 663.1038\n",
      "Training Epoch: 12 [29250/36045]\tLoss: 642.3212\n",
      "Training Epoch: 12 [29300/36045]\tLoss: 749.7780\n",
      "Training Epoch: 12 [29350/36045]\tLoss: 901.8974\n",
      "Training Epoch: 12 [29400/36045]\tLoss: 922.8523\n",
      "Training Epoch: 12 [29450/36045]\tLoss: 962.5185\n",
      "Training Epoch: 12 [29500/36045]\tLoss: 980.9333\n",
      "Training Epoch: 12 [29550/36045]\tLoss: 927.8621\n",
      "Training Epoch: 12 [29600/36045]\tLoss: 795.5930\n",
      "Training Epoch: 12 [29650/36045]\tLoss: 775.6530\n",
      "Training Epoch: 12 [29700/36045]\tLoss: 684.2930\n",
      "Training Epoch: 12 [29750/36045]\tLoss: 690.8467\n",
      "Training Epoch: 12 [29800/36045]\tLoss: 742.1628\n",
      "Training Epoch: 12 [29850/36045]\tLoss: 811.2220\n",
      "Training Epoch: 12 [29900/36045]\tLoss: 803.8055\n",
      "Training Epoch: 12 [29950/36045]\tLoss: 825.9958\n",
      "Training Epoch: 12 [30000/36045]\tLoss: 812.9536\n",
      "Training Epoch: 12 [30050/36045]\tLoss: 817.5682\n",
      "Training Epoch: 12 [30100/36045]\tLoss: 1000.5781\n",
      "Training Epoch: 12 [30150/36045]\tLoss: 992.2207\n",
      "Training Epoch: 12 [30200/36045]\tLoss: 937.4487\n",
      "Training Epoch: 12 [30250/36045]\tLoss: 989.5551\n",
      "Training Epoch: 12 [30300/36045]\tLoss: 981.1066\n",
      "Training Epoch: 12 [30350/36045]\tLoss: 796.3331\n",
      "Training Epoch: 12 [30400/36045]\tLoss: 786.1228\n",
      "Training Epoch: 12 [30450/36045]\tLoss: 783.8455\n",
      "Training Epoch: 12 [30500/36045]\tLoss: 728.7238\n",
      "Training Epoch: 12 [30550/36045]\tLoss: 675.3912\n",
      "Training Epoch: 12 [30600/36045]\tLoss: 645.6882\n",
      "Training Epoch: 12 [30650/36045]\tLoss: 637.0483\n",
      "Training Epoch: 12 [30700/36045]\tLoss: 660.2653\n",
      "Training Epoch: 12 [30750/36045]\tLoss: 640.7814\n",
      "Training Epoch: 12 [30800/36045]\tLoss: 682.1755\n",
      "Training Epoch: 12 [30850/36045]\tLoss: 675.6056\n",
      "Training Epoch: 12 [30900/36045]\tLoss: 694.3068\n",
      "Training Epoch: 12 [30950/36045]\tLoss: 731.8184\n",
      "Training Epoch: 12 [31000/36045]\tLoss: 718.1795\n",
      "Training Epoch: 12 [31050/36045]\tLoss: 597.9243\n",
      "Training Epoch: 12 [31100/36045]\tLoss: 584.0560\n",
      "Training Epoch: 12 [31150/36045]\tLoss: 588.6223\n",
      "Training Epoch: 12 [31200/36045]\tLoss: 743.8095\n",
      "Training Epoch: 12 [31250/36045]\tLoss: 965.2316\n",
      "Training Epoch: 12 [31300/36045]\tLoss: 924.3916\n",
      "Training Epoch: 12 [31350/36045]\tLoss: 941.1489\n",
      "Training Epoch: 12 [31400/36045]\tLoss: 925.8270\n",
      "Training Epoch: 12 [31450/36045]\tLoss: 919.5505\n",
      "Training Epoch: 12 [31500/36045]\tLoss: 922.1701\n",
      "Training Epoch: 12 [31550/36045]\tLoss: 935.6702\n",
      "Training Epoch: 12 [31600/36045]\tLoss: 878.1050\n",
      "Training Epoch: 12 [31650/36045]\tLoss: 938.1760\n",
      "Training Epoch: 12 [31700/36045]\tLoss: 699.7914\n",
      "Training Epoch: 12 [31750/36045]\tLoss: 586.2982\n",
      "Training Epoch: 12 [31800/36045]\tLoss: 557.9342\n",
      "Training Epoch: 12 [31850/36045]\tLoss: 575.0907\n",
      "Training Epoch: 12 [31900/36045]\tLoss: 868.7365\n",
      "Training Epoch: 12 [31950/36045]\tLoss: 1089.0243\n",
      "Training Epoch: 12 [32000/36045]\tLoss: 1224.2393\n",
      "Training Epoch: 12 [32050/36045]\tLoss: 1170.8761\n",
      "Training Epoch: 12 [32100/36045]\tLoss: 1154.0619\n",
      "Training Epoch: 12 [32150/36045]\tLoss: 939.7156\n",
      "Training Epoch: 12 [32200/36045]\tLoss: 947.6180\n",
      "Training Epoch: 12 [32250/36045]\tLoss: 962.0563\n",
      "Training Epoch: 12 [32300/36045]\tLoss: 942.6035\n",
      "Training Epoch: 12 [32350/36045]\tLoss: 928.0920\n",
      "Training Epoch: 12 [32400/36045]\tLoss: 875.7328\n",
      "Training Epoch: 12 [32450/36045]\tLoss: 727.9611\n",
      "Training Epoch: 12 [32500/36045]\tLoss: 698.7740\n",
      "Training Epoch: 12 [32550/36045]\tLoss: 705.6461\n",
      "Training Epoch: 12 [32600/36045]\tLoss: 699.5970\n",
      "Training Epoch: 12 [32650/36045]\tLoss: 865.4012\n",
      "Training Epoch: 12 [32700/36045]\tLoss: 937.3403\n",
      "Training Epoch: 12 [32750/36045]\tLoss: 894.1002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [32800/36045]\tLoss: 916.9702\n",
      "Training Epoch: 12 [32850/36045]\tLoss: 854.9799\n",
      "Training Epoch: 12 [32900/36045]\tLoss: 696.5171\n",
      "Training Epoch: 12 [32950/36045]\tLoss: 730.2819\n",
      "Training Epoch: 12 [33000/36045]\tLoss: 734.0995\n",
      "Training Epoch: 12 [33050/36045]\tLoss: 692.1583\n",
      "Training Epoch: 12 [33100/36045]\tLoss: 790.3773\n",
      "Training Epoch: 12 [33150/36045]\tLoss: 1054.7246\n",
      "Training Epoch: 12 [33200/36045]\tLoss: 1029.7460\n",
      "Training Epoch: 12 [33250/36045]\tLoss: 1057.2627\n",
      "Training Epoch: 12 [33300/36045]\tLoss: 1124.1296\n",
      "Training Epoch: 12 [33350/36045]\tLoss: 867.7277\n",
      "Training Epoch: 12 [33400/36045]\tLoss: 653.8777\n",
      "Training Epoch: 12 [33450/36045]\tLoss: 649.1029\n",
      "Training Epoch: 12 [33500/36045]\tLoss: 665.7273\n",
      "Training Epoch: 12 [33550/36045]\tLoss: 689.9620\n",
      "Training Epoch: 12 [33600/36045]\tLoss: 694.7463\n",
      "Training Epoch: 12 [33650/36045]\tLoss: 904.2843\n",
      "Training Epoch: 12 [33700/36045]\tLoss: 877.1953\n",
      "Training Epoch: 12 [33750/36045]\tLoss: 905.8027\n",
      "Training Epoch: 12 [33800/36045]\tLoss: 902.3906\n",
      "Training Epoch: 12 [33850/36045]\tLoss: 907.8406\n",
      "Training Epoch: 12 [33900/36045]\tLoss: 919.1501\n",
      "Training Epoch: 12 [33950/36045]\tLoss: 932.1213\n",
      "Training Epoch: 12 [34000/36045]\tLoss: 923.0596\n",
      "Training Epoch: 12 [34050/36045]\tLoss: 931.1749\n",
      "Training Epoch: 12 [34100/36045]\tLoss: 890.1259\n",
      "Training Epoch: 12 [34150/36045]\tLoss: 826.2545\n",
      "Training Epoch: 12 [34200/36045]\tLoss: 787.6168\n",
      "Training Epoch: 12 [34250/36045]\tLoss: 804.2471\n",
      "Training Epoch: 12 [34300/36045]\tLoss: 691.3943\n",
      "Training Epoch: 12 [34350/36045]\tLoss: 723.8475\n",
      "Training Epoch: 12 [34400/36045]\tLoss: 707.0306\n",
      "Training Epoch: 12 [34450/36045]\tLoss: 663.1223\n",
      "Training Epoch: 12 [34500/36045]\tLoss: 710.8926\n",
      "Training Epoch: 12 [34550/36045]\tLoss: 703.5760\n",
      "Training Epoch: 12 [34600/36045]\tLoss: 684.4791\n",
      "Training Epoch: 12 [34650/36045]\tLoss: 809.8649\n",
      "Training Epoch: 12 [34700/36045]\tLoss: 852.4743\n",
      "Training Epoch: 12 [34750/36045]\tLoss: 759.0502\n",
      "Training Epoch: 12 [34800/36045]\tLoss: 857.5178\n",
      "Training Epoch: 12 [34850/36045]\tLoss: 875.7326\n",
      "Training Epoch: 12 [34900/36045]\tLoss: 1026.2896\n",
      "Training Epoch: 12 [34950/36045]\tLoss: 1015.1789\n",
      "Training Epoch: 12 [35000/36045]\tLoss: 1023.3301\n",
      "Training Epoch: 12 [35050/36045]\tLoss: 1001.5074\n",
      "Training Epoch: 12 [35100/36045]\tLoss: 791.7962\n",
      "Training Epoch: 12 [35150/36045]\tLoss: 782.4896\n",
      "Training Epoch: 12 [35200/36045]\tLoss: 679.1865\n",
      "Training Epoch: 12 [35250/36045]\tLoss: 738.9843\n",
      "Training Epoch: 12 [35300/36045]\tLoss: 750.6613\n",
      "Training Epoch: 12 [35350/36045]\tLoss: 888.4515\n",
      "Training Epoch: 12 [35400/36045]\tLoss: 949.0039\n",
      "Training Epoch: 12 [35450/36045]\tLoss: 905.4689\n",
      "Training Epoch: 12 [35500/36045]\tLoss: 884.4242\n",
      "Training Epoch: 12 [35550/36045]\tLoss: 869.4293\n",
      "Training Epoch: 12 [35600/36045]\tLoss: 911.8394\n",
      "Training Epoch: 12 [35650/36045]\tLoss: 996.6542\n",
      "Training Epoch: 12 [35700/36045]\tLoss: 920.7352\n",
      "Training Epoch: 12 [35750/36045]\tLoss: 986.6403\n",
      "Training Epoch: 12 [35800/36045]\tLoss: 992.5750\n",
      "Training Epoch: 12 [35850/36045]\tLoss: 966.6599\n",
      "Training Epoch: 12 [35900/36045]\tLoss: 1010.0058\n",
      "Training Epoch: 12 [35950/36045]\tLoss: 1011.8119\n",
      "Training Epoch: 12 [36000/36045]\tLoss: 994.9236\n",
      "Training Epoch: 12 [36045/36045]\tLoss: 976.3726\n",
      "Training Epoch: 12 [4004/4004]\tLoss: 919.0650\n",
      "Training Epoch: 13 [50/36045]\tLoss: 925.1707\n",
      "Training Epoch: 13 [100/36045]\tLoss: 888.4581\n",
      "Training Epoch: 13 [150/36045]\tLoss: 889.1120\n",
      "Training Epoch: 13 [200/36045]\tLoss: 873.4248\n",
      "Training Epoch: 13 [250/36045]\tLoss: 1014.8875\n",
      "Training Epoch: 13 [300/36045]\tLoss: 1078.3591\n",
      "Training Epoch: 13 [350/36045]\tLoss: 1033.8462\n",
      "Training Epoch: 13 [400/36045]\tLoss: 1043.1285\n",
      "Training Epoch: 13 [450/36045]\tLoss: 1019.5861\n",
      "Training Epoch: 13 [500/36045]\tLoss: 958.6354\n",
      "Training Epoch: 13 [550/36045]\tLoss: 975.9197\n",
      "Training Epoch: 13 [600/36045]\tLoss: 939.0411\n",
      "Training Epoch: 13 [650/36045]\tLoss: 972.8196\n",
      "Training Epoch: 13 [700/36045]\tLoss: 961.9426\n",
      "Training Epoch: 13 [750/36045]\tLoss: 931.4274\n",
      "Training Epoch: 13 [800/36045]\tLoss: 950.3347\n",
      "Training Epoch: 13 [850/36045]\tLoss: 920.6827\n",
      "Training Epoch: 13 [900/36045]\tLoss: 878.8751\n",
      "Training Epoch: 13 [950/36045]\tLoss: 839.3954\n",
      "Training Epoch: 13 [1000/36045]\tLoss: 806.7626\n",
      "Training Epoch: 13 [1050/36045]\tLoss: 810.1536\n",
      "Training Epoch: 13 [1100/36045]\tLoss: 788.7269\n",
      "Training Epoch: 13 [1150/36045]\tLoss: 792.4096\n",
      "Training Epoch: 13 [1200/36045]\tLoss: 839.7034\n",
      "Training Epoch: 13 [1250/36045]\tLoss: 948.7037\n",
      "Training Epoch: 13 [1300/36045]\tLoss: 950.0499\n",
      "Training Epoch: 13 [1350/36045]\tLoss: 956.7788\n",
      "Training Epoch: 13 [1400/36045]\tLoss: 993.9502\n",
      "Training Epoch: 13 [1450/36045]\tLoss: 957.9583\n",
      "Training Epoch: 13 [1500/36045]\tLoss: 891.3874\n",
      "Training Epoch: 13 [1550/36045]\tLoss: 918.5963\n",
      "Training Epoch: 13 [1600/36045]\tLoss: 928.8002\n",
      "Training Epoch: 13 [1650/36045]\tLoss: 912.8173\n",
      "Training Epoch: 13 [1700/36045]\tLoss: 924.2939\n",
      "Training Epoch: 13 [1750/36045]\tLoss: 972.1176\n",
      "Training Epoch: 13 [1800/36045]\tLoss: 954.5649\n",
      "Training Epoch: 13 [1850/36045]\tLoss: 988.6243\n",
      "Training Epoch: 13 [1900/36045]\tLoss: 923.9947\n",
      "Training Epoch: 13 [1950/36045]\tLoss: 934.7618\n",
      "Training Epoch: 13 [2000/36045]\tLoss: 843.7516\n",
      "Training Epoch: 13 [2050/36045]\tLoss: 851.1864\n",
      "Training Epoch: 13 [2100/36045]\tLoss: 897.2993\n",
      "Training Epoch: 13 [2150/36045]\tLoss: 870.3915\n",
      "Training Epoch: 13 [2200/36045]\tLoss: 807.3325\n",
      "Training Epoch: 13 [2250/36045]\tLoss: 762.9479\n",
      "Training Epoch: 13 [2300/36045]\tLoss: 798.8864\n",
      "Training Epoch: 13 [2350/36045]\tLoss: 761.7350\n",
      "Training Epoch: 13 [2400/36045]\tLoss: 782.4662\n",
      "Training Epoch: 13 [2450/36045]\tLoss: 975.4903\n",
      "Training Epoch: 13 [2500/36045]\tLoss: 1019.6745\n",
      "Training Epoch: 13 [2550/36045]\tLoss: 1017.2990\n",
      "Training Epoch: 13 [2600/36045]\tLoss: 1025.2894\n",
      "Training Epoch: 13 [2650/36045]\tLoss: 1153.1165\n",
      "Training Epoch: 13 [2700/36045]\tLoss: 1240.5032\n",
      "Training Epoch: 13 [2750/36045]\tLoss: 1321.5040\n",
      "Training Epoch: 13 [2800/36045]\tLoss: 1338.7408\n",
      "Training Epoch: 13 [2850/36045]\tLoss: 1114.9355\n",
      "Training Epoch: 13 [2900/36045]\tLoss: 1093.4180\n",
      "Training Epoch: 13 [2950/36045]\tLoss: 1052.8081\n",
      "Training Epoch: 13 [3000/36045]\tLoss: 1050.2128\n",
      "Training Epoch: 13 [3050/36045]\tLoss: 1089.5031\n",
      "Training Epoch: 13 [3100/36045]\tLoss: 992.1915\n",
      "Training Epoch: 13 [3150/36045]\tLoss: 765.8659\n",
      "Training Epoch: 13 [3200/36045]\tLoss: 797.1803\n",
      "Training Epoch: 13 [3250/36045]\tLoss: 749.1015\n",
      "Training Epoch: 13 [3300/36045]\tLoss: 707.7711\n",
      "Training Epoch: 13 [3350/36045]\tLoss: 745.5810\n",
      "Training Epoch: 13 [3400/36045]\tLoss: 782.9800\n",
      "Training Epoch: 13 [3450/36045]\tLoss: 845.0090\n",
      "Training Epoch: 13 [3500/36045]\tLoss: 821.9946\n",
      "Training Epoch: 13 [3550/36045]\tLoss: 794.4956\n",
      "Training Epoch: 13 [3600/36045]\tLoss: 848.2856\n",
      "Training Epoch: 13 [3650/36045]\tLoss: 985.3590\n",
      "Training Epoch: 13 [3700/36045]\tLoss: 989.3527\n",
      "Training Epoch: 13 [3750/36045]\tLoss: 951.1320\n",
      "Training Epoch: 13 [3800/36045]\tLoss: 937.3804\n",
      "Training Epoch: 13 [3850/36045]\tLoss: 942.2868\n",
      "Training Epoch: 13 [3900/36045]\tLoss: 951.9836\n",
      "Training Epoch: 13 [3950/36045]\tLoss: 911.0744\n",
      "Training Epoch: 13 [4000/36045]\tLoss: 932.5977\n",
      "Training Epoch: 13 [4050/36045]\tLoss: 857.0038\n",
      "Training Epoch: 13 [4100/36045]\tLoss: 831.5546\n",
      "Training Epoch: 13 [4150/36045]\tLoss: 859.7631\n",
      "Training Epoch: 13 [4200/36045]\tLoss: 849.7501\n",
      "Training Epoch: 13 [4250/36045]\tLoss: 852.9321\n",
      "Training Epoch: 13 [4300/36045]\tLoss: 876.9706\n",
      "Training Epoch: 13 [4350/36045]\tLoss: 856.4202\n",
      "Training Epoch: 13 [4400/36045]\tLoss: 818.4169\n",
      "Training Epoch: 13 [4450/36045]\tLoss: 882.5935\n",
      "Training Epoch: 13 [4500/36045]\tLoss: 937.8028\n",
      "Training Epoch: 13 [4550/36045]\tLoss: 951.9061\n",
      "Training Epoch: 13 [4600/36045]\tLoss: 974.7651\n",
      "Training Epoch: 13 [4650/36045]\tLoss: 970.1360\n",
      "Training Epoch: 13 [4700/36045]\tLoss: 894.1918\n",
      "Training Epoch: 13 [4750/36045]\tLoss: 877.2258\n",
      "Training Epoch: 13 [4800/36045]\tLoss: 912.4219\n",
      "Training Epoch: 13 [4850/36045]\tLoss: 893.6899\n",
      "Training Epoch: 13 [4900/36045]\tLoss: 865.5833\n",
      "Training Epoch: 13 [4950/36045]\tLoss: 900.7076\n",
      "Training Epoch: 13 [5000/36045]\tLoss: 948.7667\n",
      "Training Epoch: 13 [5050/36045]\tLoss: 915.6858\n",
      "Training Epoch: 13 [5100/36045]\tLoss: 931.2254\n",
      "Training Epoch: 13 [5150/36045]\tLoss: 907.7277\n",
      "Training Epoch: 13 [5200/36045]\tLoss: 901.8399\n",
      "Training Epoch: 13 [5250/36045]\tLoss: 890.4261\n",
      "Training Epoch: 13 [5300/36045]\tLoss: 891.5839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [5350/36045]\tLoss: 925.4420\n",
      "Training Epoch: 13 [5400/36045]\tLoss: 882.7347\n",
      "Training Epoch: 13 [5450/36045]\tLoss: 830.9218\n",
      "Training Epoch: 13 [5500/36045]\tLoss: 877.4280\n",
      "Training Epoch: 13 [5550/36045]\tLoss: 853.0857\n",
      "Training Epoch: 13 [5600/36045]\tLoss: 971.0536\n",
      "Training Epoch: 13 [5650/36045]\tLoss: 923.9160\n",
      "Training Epoch: 13 [5700/36045]\tLoss: 873.7708\n",
      "Training Epoch: 13 [5750/36045]\tLoss: 856.5967\n",
      "Training Epoch: 13 [5800/36045]\tLoss: 912.3320\n",
      "Training Epoch: 13 [5850/36045]\tLoss: 882.9559\n",
      "Training Epoch: 13 [5900/36045]\tLoss: 1012.5469\n",
      "Training Epoch: 13 [5950/36045]\tLoss: 1043.3384\n",
      "Training Epoch: 13 [6000/36045]\tLoss: 1022.7340\n",
      "Training Epoch: 13 [6050/36045]\tLoss: 986.1702\n",
      "Training Epoch: 13 [6100/36045]\tLoss: 992.4704\n",
      "Training Epoch: 13 [6150/36045]\tLoss: 955.7925\n",
      "Training Epoch: 13 [6200/36045]\tLoss: 951.3323\n",
      "Training Epoch: 13 [6250/36045]\tLoss: 966.4825\n",
      "Training Epoch: 13 [6300/36045]\tLoss: 988.6940\n",
      "Training Epoch: 13 [6350/36045]\tLoss: 1043.2365\n",
      "Training Epoch: 13 [6400/36045]\tLoss: 893.2368\n",
      "Training Epoch: 13 [6450/36045]\tLoss: 829.1570\n",
      "Training Epoch: 13 [6500/36045]\tLoss: 852.2553\n",
      "Training Epoch: 13 [6550/36045]\tLoss: 868.6476\n",
      "Training Epoch: 13 [6600/36045]\tLoss: 873.8625\n",
      "Training Epoch: 13 [6650/36045]\tLoss: 986.3146\n",
      "Training Epoch: 13 [6700/36045]\tLoss: 1032.5992\n",
      "Training Epoch: 13 [6750/36045]\tLoss: 1001.2783\n",
      "Training Epoch: 13 [6800/36045]\tLoss: 1004.3535\n",
      "Training Epoch: 13 [6850/36045]\tLoss: 987.7889\n",
      "Training Epoch: 13 [6900/36045]\tLoss: 871.4743\n",
      "Training Epoch: 13 [6950/36045]\tLoss: 820.1853\n",
      "Training Epoch: 13 [7000/36045]\tLoss: 872.0429\n",
      "Training Epoch: 13 [7050/36045]\tLoss: 895.0725\n",
      "Training Epoch: 13 [7100/36045]\tLoss: 890.2158\n",
      "Training Epoch: 13 [7150/36045]\tLoss: 909.4138\n",
      "Training Epoch: 13 [7200/36045]\tLoss: 916.8372\n",
      "Training Epoch: 13 [7250/36045]\tLoss: 914.2362\n",
      "Training Epoch: 13 [7300/36045]\tLoss: 897.7098\n",
      "Training Epoch: 13 [7350/36045]\tLoss: 891.0272\n",
      "Training Epoch: 13 [7400/36045]\tLoss: 800.8441\n",
      "Training Epoch: 13 [7450/36045]\tLoss: 804.0754\n",
      "Training Epoch: 13 [7500/36045]\tLoss: 802.2243\n",
      "Training Epoch: 13 [7550/36045]\tLoss: 764.3275\n",
      "Training Epoch: 13 [7600/36045]\tLoss: 861.1774\n",
      "Training Epoch: 13 [7650/36045]\tLoss: 929.9872\n",
      "Training Epoch: 13 [7700/36045]\tLoss: 888.9525\n",
      "Training Epoch: 13 [7750/36045]\tLoss: 904.4185\n",
      "Training Epoch: 13 [7800/36045]\tLoss: 888.3075\n",
      "Training Epoch: 13 [7850/36045]\tLoss: 843.3246\n",
      "Training Epoch: 13 [7900/36045]\tLoss: 891.0097\n",
      "Training Epoch: 13 [7950/36045]\tLoss: 887.4398\n",
      "Training Epoch: 13 [8000/36045]\tLoss: 907.9630\n",
      "Training Epoch: 13 [8050/36045]\tLoss: 861.7078\n",
      "Training Epoch: 13 [8100/36045]\tLoss: 893.2012\n",
      "Training Epoch: 13 [8150/36045]\tLoss: 1011.0986\n",
      "Training Epoch: 13 [8200/36045]\tLoss: 996.3840\n",
      "Training Epoch: 13 [8250/36045]\tLoss: 957.6022\n",
      "Training Epoch: 13 [8300/36045]\tLoss: 1036.6022\n",
      "Training Epoch: 13 [8350/36045]\tLoss: 958.1533\n",
      "Training Epoch: 13 [8400/36045]\tLoss: 865.4960\n",
      "Training Epoch: 13 [8450/36045]\tLoss: 812.6804\n",
      "Training Epoch: 13 [8500/36045]\tLoss: 858.3962\n",
      "Training Epoch: 13 [8550/36045]\tLoss: 841.2029\n",
      "Training Epoch: 13 [8600/36045]\tLoss: 835.0720\n",
      "Training Epoch: 13 [8650/36045]\tLoss: 888.7778\n",
      "Training Epoch: 13 [8700/36045]\tLoss: 938.4441\n",
      "Training Epoch: 13 [8750/36045]\tLoss: 916.9146\n",
      "Training Epoch: 13 [8800/36045]\tLoss: 924.6234\n",
      "Training Epoch: 13 [8850/36045]\tLoss: 913.9258\n",
      "Training Epoch: 13 [8900/36045]\tLoss: 825.8224\n",
      "Training Epoch: 13 [8950/36045]\tLoss: 851.1201\n",
      "Training Epoch: 13 [9000/36045]\tLoss: 861.3586\n",
      "Training Epoch: 13 [9050/36045]\tLoss: 859.4700\n",
      "Training Epoch: 13 [9100/36045]\tLoss: 884.0293\n",
      "Training Epoch: 13 [9150/36045]\tLoss: 653.1060\n",
      "Training Epoch: 13 [9200/36045]\tLoss: 498.6208\n",
      "Training Epoch: 13 [9250/36045]\tLoss: 538.6488\n",
      "Training Epoch: 13 [9300/36045]\tLoss: 558.7067\n",
      "Training Epoch: 13 [9350/36045]\tLoss: 509.6049\n",
      "Training Epoch: 13 [9400/36045]\tLoss: 1000.6311\n",
      "Training Epoch: 13 [9450/36045]\tLoss: 1061.3094\n",
      "Training Epoch: 13 [9500/36045]\tLoss: 1046.2354\n",
      "Training Epoch: 13 [9550/36045]\tLoss: 1109.1760\n",
      "Training Epoch: 13 [9600/36045]\tLoss: 818.1458\n",
      "Training Epoch: 13 [9650/36045]\tLoss: 817.8724\n",
      "Training Epoch: 13 [9700/36045]\tLoss: 804.7403\n",
      "Training Epoch: 13 [9750/36045]\tLoss: 806.7568\n",
      "Training Epoch: 13 [9800/36045]\tLoss: 1040.0045\n",
      "Training Epoch: 13 [9850/36045]\tLoss: 1098.2974\n",
      "Training Epoch: 13 [9900/36045]\tLoss: 1126.7194\n",
      "Training Epoch: 13 [9950/36045]\tLoss: 1095.1096\n",
      "Training Epoch: 13 [10000/36045]\tLoss: 1009.9877\n",
      "Training Epoch: 13 [10050/36045]\tLoss: 848.2589\n",
      "Training Epoch: 13 [10100/36045]\tLoss: 848.6201\n",
      "Training Epoch: 13 [10150/36045]\tLoss: 862.4934\n",
      "Training Epoch: 13 [10200/36045]\tLoss: 852.8268\n",
      "Training Epoch: 13 [10250/36045]\tLoss: 1022.6240\n",
      "Training Epoch: 13 [10300/36045]\tLoss: 991.7182\n",
      "Training Epoch: 13 [10350/36045]\tLoss: 1044.9846\n",
      "Training Epoch: 13 [10400/36045]\tLoss: 1035.9745\n",
      "Training Epoch: 13 [10450/36045]\tLoss: 961.5735\n",
      "Training Epoch: 13 [10500/36045]\tLoss: 810.7202\n",
      "Training Epoch: 13 [10550/36045]\tLoss: 808.1660\n",
      "Training Epoch: 13 [10600/36045]\tLoss: 834.7734\n",
      "Training Epoch: 13 [10650/36045]\tLoss: 841.8853\n",
      "Training Epoch: 13 [10700/36045]\tLoss: 938.7767\n",
      "Training Epoch: 13 [10750/36045]\tLoss: 1016.1031\n",
      "Training Epoch: 13 [10800/36045]\tLoss: 942.6150\n",
      "Training Epoch: 13 [10850/36045]\tLoss: 995.7535\n",
      "Training Epoch: 13 [10900/36045]\tLoss: 1036.7725\n",
      "Training Epoch: 13 [10950/36045]\tLoss: 778.3237\n",
      "Training Epoch: 13 [11000/36045]\tLoss: 772.1330\n",
      "Training Epoch: 13 [11050/36045]\tLoss: 822.5514\n",
      "Training Epoch: 13 [11100/36045]\tLoss: 838.8068\n",
      "Training Epoch: 13 [11150/36045]\tLoss: 909.3375\n",
      "Training Epoch: 13 [11200/36045]\tLoss: 939.9859\n",
      "Training Epoch: 13 [11250/36045]\tLoss: 955.4694\n",
      "Training Epoch: 13 [11300/36045]\tLoss: 935.5875\n",
      "Training Epoch: 13 [11350/36045]\tLoss: 929.4430\n",
      "Training Epoch: 13 [11400/36045]\tLoss: 880.5151\n",
      "Training Epoch: 13 [11450/36045]\tLoss: 841.9178\n",
      "Training Epoch: 13 [11500/36045]\tLoss: 840.9966\n",
      "Training Epoch: 13 [11550/36045]\tLoss: 861.6021\n",
      "Training Epoch: 13 [11600/36045]\tLoss: 935.7094\n",
      "Training Epoch: 13 [11650/36045]\tLoss: 991.2943\n",
      "Training Epoch: 13 [11700/36045]\tLoss: 988.8062\n",
      "Training Epoch: 13 [11750/36045]\tLoss: 1012.6907\n",
      "Training Epoch: 13 [11800/36045]\tLoss: 1061.1129\n",
      "Training Epoch: 13 [11850/36045]\tLoss: 1102.0563\n",
      "Training Epoch: 13 [11900/36045]\tLoss: 1336.5652\n",
      "Training Epoch: 13 [11950/36045]\tLoss: 1331.0284\n",
      "Training Epoch: 13 [12000/36045]\tLoss: 1356.3398\n",
      "Training Epoch: 13 [12050/36045]\tLoss: 1309.9020\n",
      "Training Epoch: 13 [12100/36045]\tLoss: 906.0578\n",
      "Training Epoch: 13 [12150/36045]\tLoss: 727.2864\n",
      "Training Epoch: 13 [12200/36045]\tLoss: 720.5688\n",
      "Training Epoch: 13 [12250/36045]\tLoss: 732.8464\n",
      "Training Epoch: 13 [12300/36045]\tLoss: 908.1818\n",
      "Training Epoch: 13 [12350/36045]\tLoss: 971.0927\n",
      "Training Epoch: 13 [12400/36045]\tLoss: 986.8697\n",
      "Training Epoch: 13 [12450/36045]\tLoss: 972.5120\n",
      "Training Epoch: 13 [12500/36045]\tLoss: 1009.0022\n",
      "Training Epoch: 13 [12550/36045]\tLoss: 968.2902\n",
      "Training Epoch: 13 [12600/36045]\tLoss: 904.7039\n",
      "Training Epoch: 13 [12650/36045]\tLoss: 899.7108\n",
      "Training Epoch: 13 [12700/36045]\tLoss: 927.7142\n",
      "Training Epoch: 13 [12750/36045]\tLoss: 925.8082\n",
      "Training Epoch: 13 [12800/36045]\tLoss: 905.2115\n",
      "Training Epoch: 13 [12850/36045]\tLoss: 943.4182\n",
      "Training Epoch: 13 [12900/36045]\tLoss: 903.3663\n",
      "Training Epoch: 13 [12950/36045]\tLoss: 895.4702\n",
      "Training Epoch: 13 [13000/36045]\tLoss: 928.1260\n",
      "Training Epoch: 13 [13050/36045]\tLoss: 855.9465\n",
      "Training Epoch: 13 [13100/36045]\tLoss: 896.7008\n",
      "Training Epoch: 13 [13150/36045]\tLoss: 891.5851\n",
      "Training Epoch: 13 [13200/36045]\tLoss: 848.2757\n",
      "Training Epoch: 13 [13250/36045]\tLoss: 892.5470\n",
      "Training Epoch: 13 [13300/36045]\tLoss: 933.0465\n",
      "Training Epoch: 13 [13350/36045]\tLoss: 906.1572\n",
      "Training Epoch: 13 [13400/36045]\tLoss: 913.1169\n",
      "Training Epoch: 13 [13450/36045]\tLoss: 900.5417\n",
      "Training Epoch: 13 [13500/36045]\tLoss: 936.0547\n",
      "Training Epoch: 13 [13550/36045]\tLoss: 1069.7317\n",
      "Training Epoch: 13 [13600/36045]\tLoss: 1101.6199\n",
      "Training Epoch: 13 [13650/36045]\tLoss: 1183.4011\n",
      "Training Epoch: 13 [13700/36045]\tLoss: 1058.3048\n",
      "Training Epoch: 13 [13750/36045]\tLoss: 923.2770\n",
      "Training Epoch: 13 [13800/36045]\tLoss: 898.2454\n",
      "Training Epoch: 13 [13850/36045]\tLoss: 880.2455\n",
      "Training Epoch: 13 [13900/36045]\tLoss: 891.7911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [13950/36045]\tLoss: 929.3511\n",
      "Training Epoch: 13 [14000/36045]\tLoss: 969.7188\n",
      "Training Epoch: 13 [14050/36045]\tLoss: 932.1244\n",
      "Training Epoch: 13 [14100/36045]\tLoss: 928.9491\n",
      "Training Epoch: 13 [14150/36045]\tLoss: 912.8092\n",
      "Training Epoch: 13 [14200/36045]\tLoss: 974.4326\n",
      "Training Epoch: 13 [14250/36045]\tLoss: 1064.1427\n",
      "Training Epoch: 13 [14300/36045]\tLoss: 1069.2528\n",
      "Training Epoch: 13 [14350/36045]\tLoss: 1022.9992\n",
      "Training Epoch: 13 [14400/36045]\tLoss: 1007.4457\n",
      "Training Epoch: 13 [14450/36045]\tLoss: 1052.8403\n",
      "Training Epoch: 13 [14500/36045]\tLoss: 976.4001\n",
      "Training Epoch: 13 [14550/36045]\tLoss: 1019.9446\n",
      "Training Epoch: 13 [14600/36045]\tLoss: 993.6307\n",
      "Training Epoch: 13 [14650/36045]\tLoss: 998.2156\n",
      "Training Epoch: 13 [14700/36045]\tLoss: 937.0941\n",
      "Training Epoch: 13 [14750/36045]\tLoss: 805.7944\n",
      "Training Epoch: 13 [14800/36045]\tLoss: 792.7333\n",
      "Training Epoch: 13 [14850/36045]\tLoss: 799.8759\n",
      "Training Epoch: 13 [14900/36045]\tLoss: 794.6367\n",
      "Training Epoch: 13 [14950/36045]\tLoss: 802.6931\n",
      "Training Epoch: 13 [15000/36045]\tLoss: 825.6012\n",
      "Training Epoch: 13 [15050/36045]\tLoss: 828.7234\n",
      "Training Epoch: 13 [15100/36045]\tLoss: 809.1511\n",
      "Training Epoch: 13 [15150/36045]\tLoss: 799.9142\n",
      "Training Epoch: 13 [15200/36045]\tLoss: 737.4533\n",
      "Training Epoch: 13 [15250/36045]\tLoss: 771.9586\n",
      "Training Epoch: 13 [15300/36045]\tLoss: 751.7637\n",
      "Training Epoch: 13 [15350/36045]\tLoss: 766.8168\n",
      "Training Epoch: 13 [15400/36045]\tLoss: 750.8668\n",
      "Training Epoch: 13 [15450/36045]\tLoss: 741.3766\n",
      "Training Epoch: 13 [15500/36045]\tLoss: 764.5388\n",
      "Training Epoch: 13 [15550/36045]\tLoss: 750.3593\n",
      "Training Epoch: 13 [15600/36045]\tLoss: 832.4250\n",
      "Training Epoch: 13 [15650/36045]\tLoss: 858.7612\n",
      "Training Epoch: 13 [15700/36045]\tLoss: 843.8532\n",
      "Training Epoch: 13 [15750/36045]\tLoss: 833.4197\n",
      "Training Epoch: 13 [15800/36045]\tLoss: 768.0980\n",
      "Training Epoch: 13 [15850/36045]\tLoss: 776.3722\n",
      "Training Epoch: 13 [15900/36045]\tLoss: 787.3887\n",
      "Training Epoch: 13 [15950/36045]\tLoss: 812.5519\n",
      "Training Epoch: 13 [16000/36045]\tLoss: 797.8300\n",
      "Training Epoch: 13 [16050/36045]\tLoss: 763.7277\n",
      "Training Epoch: 13 [16100/36045]\tLoss: 707.6408\n",
      "Training Epoch: 13 [16150/36045]\tLoss: 689.1309\n",
      "Training Epoch: 13 [16200/36045]\tLoss: 827.4810\n",
      "Training Epoch: 13 [16250/36045]\tLoss: 860.3787\n",
      "Training Epoch: 13 [16300/36045]\tLoss: 937.1883\n",
      "Training Epoch: 13 [16350/36045]\tLoss: 951.2078\n",
      "Training Epoch: 13 [16400/36045]\tLoss: 927.2023\n",
      "Training Epoch: 13 [16450/36045]\tLoss: 903.2856\n",
      "Training Epoch: 13 [16500/36045]\tLoss: 899.8856\n",
      "Training Epoch: 13 [16550/36045]\tLoss: 857.3422\n",
      "Training Epoch: 13 [16600/36045]\tLoss: 897.5429\n",
      "Training Epoch: 13 [16650/36045]\tLoss: 923.1125\n",
      "Training Epoch: 13 [16700/36045]\tLoss: 896.1336\n",
      "Training Epoch: 13 [16750/36045]\tLoss: 886.4664\n",
      "Training Epoch: 13 [16800/36045]\tLoss: 903.2633\n",
      "Training Epoch: 13 [16850/36045]\tLoss: 858.0383\n",
      "Training Epoch: 13 [16900/36045]\tLoss: 874.9887\n",
      "Training Epoch: 13 [16950/36045]\tLoss: 901.1563\n",
      "Training Epoch: 13 [17000/36045]\tLoss: 877.9305\n",
      "Training Epoch: 13 [17050/36045]\tLoss: 922.5900\n",
      "Training Epoch: 13 [17100/36045]\tLoss: 927.2862\n",
      "Training Epoch: 13 [17150/36045]\tLoss: 803.6790\n",
      "Training Epoch: 13 [17200/36045]\tLoss: 754.6848\n",
      "Training Epoch: 13 [17250/36045]\tLoss: 789.5309\n",
      "Training Epoch: 13 [17300/36045]\tLoss: 832.7727\n",
      "Training Epoch: 13 [17350/36045]\tLoss: 793.6332\n",
      "Training Epoch: 13 [17400/36045]\tLoss: 812.3085\n",
      "Training Epoch: 13 [17450/36045]\tLoss: 836.9057\n",
      "Training Epoch: 13 [17500/36045]\tLoss: 822.9903\n",
      "Training Epoch: 13 [17550/36045]\tLoss: 832.1457\n",
      "Training Epoch: 13 [17600/36045]\tLoss: 817.3925\n",
      "Training Epoch: 13 [17650/36045]\tLoss: 839.7245\n",
      "Training Epoch: 13 [17700/36045]\tLoss: 816.6628\n",
      "Training Epoch: 13 [17750/36045]\tLoss: 836.5089\n",
      "Training Epoch: 13 [17800/36045]\tLoss: 828.5925\n",
      "Training Epoch: 13 [17850/36045]\tLoss: 805.3448\n",
      "Training Epoch: 13 [17900/36045]\tLoss: 838.0273\n",
      "Training Epoch: 13 [17950/36045]\tLoss: 846.9611\n",
      "Training Epoch: 13 [18000/36045]\tLoss: 836.8533\n",
      "Training Epoch: 13 [18050/36045]\tLoss: 953.2512\n",
      "Training Epoch: 13 [18100/36045]\tLoss: 959.3963\n",
      "Training Epoch: 13 [18150/36045]\tLoss: 965.4568\n",
      "Training Epoch: 13 [18200/36045]\tLoss: 953.7969\n",
      "Training Epoch: 13 [18250/36045]\tLoss: 973.5386\n",
      "Training Epoch: 13 [18300/36045]\tLoss: 889.8811\n",
      "Training Epoch: 13 [18350/36045]\tLoss: 953.3494\n",
      "Training Epoch: 13 [18400/36045]\tLoss: 925.8957\n",
      "Training Epoch: 13 [18450/36045]\tLoss: 905.9195\n",
      "Training Epoch: 13 [18500/36045]\tLoss: 908.8087\n",
      "Training Epoch: 13 [18550/36045]\tLoss: 892.3585\n",
      "Training Epoch: 13 [18600/36045]\tLoss: 882.2214\n",
      "Training Epoch: 13 [18650/36045]\tLoss: 937.6429\n",
      "Training Epoch: 13 [18700/36045]\tLoss: 988.2509\n",
      "Training Epoch: 13 [18750/36045]\tLoss: 968.1140\n",
      "Training Epoch: 13 [18800/36045]\tLoss: 997.0651\n",
      "Training Epoch: 13 [18850/36045]\tLoss: 937.9675\n",
      "Training Epoch: 13 [18900/36045]\tLoss: 1005.4985\n",
      "Training Epoch: 13 [18950/36045]\tLoss: 933.7848\n",
      "Training Epoch: 13 [19000/36045]\tLoss: 807.0988\n",
      "Training Epoch: 13 [19050/36045]\tLoss: 777.6216\n",
      "Training Epoch: 13 [19100/36045]\tLoss: 796.4429\n",
      "Training Epoch: 13 [19150/36045]\tLoss: 782.3220\n",
      "Training Epoch: 13 [19200/36045]\tLoss: 814.0621\n",
      "Training Epoch: 13 [19250/36045]\tLoss: 826.1909\n",
      "Training Epoch: 13 [19300/36045]\tLoss: 847.4896\n",
      "Training Epoch: 13 [19350/36045]\tLoss: 826.8411\n",
      "Training Epoch: 13 [19400/36045]\tLoss: 851.6605\n",
      "Training Epoch: 13 [19450/36045]\tLoss: 838.9774\n",
      "Training Epoch: 13 [19500/36045]\tLoss: 842.9320\n",
      "Training Epoch: 13 [19550/36045]\tLoss: 843.9863\n",
      "Training Epoch: 13 [19600/36045]\tLoss: 888.4904\n",
      "Training Epoch: 13 [19650/36045]\tLoss: 1143.0919\n",
      "Training Epoch: 13 [19700/36045]\tLoss: 1097.2291\n",
      "Training Epoch: 13 [19750/36045]\tLoss: 1096.2837\n",
      "Training Epoch: 13 [19800/36045]\tLoss: 1089.1648\n",
      "Training Epoch: 13 [19850/36045]\tLoss: 755.2409\n",
      "Training Epoch: 13 [19900/36045]\tLoss: 726.2828\n",
      "Training Epoch: 13 [19950/36045]\tLoss: 732.2525\n",
      "Training Epoch: 13 [20000/36045]\tLoss: 731.4963\n",
      "Training Epoch: 13 [20050/36045]\tLoss: 819.3284\n",
      "Training Epoch: 13 [20100/36045]\tLoss: 822.1569\n",
      "Training Epoch: 13 [20150/36045]\tLoss: 826.4010\n",
      "Training Epoch: 13 [20200/36045]\tLoss: 823.9266\n",
      "Training Epoch: 13 [20250/36045]\tLoss: 875.3055\n",
      "Training Epoch: 13 [20300/36045]\tLoss: 915.0429\n",
      "Training Epoch: 13 [20350/36045]\tLoss: 943.6371\n",
      "Training Epoch: 13 [20400/36045]\tLoss: 958.4794\n",
      "Training Epoch: 13 [20450/36045]\tLoss: 935.7436\n",
      "Training Epoch: 13 [20500/36045]\tLoss: 909.3027\n",
      "Training Epoch: 13 [20550/36045]\tLoss: 810.5069\n",
      "Training Epoch: 13 [20600/36045]\tLoss: 826.6340\n",
      "Training Epoch: 13 [20650/36045]\tLoss: 822.6023\n",
      "Training Epoch: 13 [20700/36045]\tLoss: 804.5690\n",
      "Training Epoch: 13 [20750/36045]\tLoss: 858.9125\n",
      "Training Epoch: 13 [20800/36045]\tLoss: 936.0295\n",
      "Training Epoch: 13 [20850/36045]\tLoss: 926.7926\n",
      "Training Epoch: 13 [20900/36045]\tLoss: 984.2325\n",
      "Training Epoch: 13 [20950/36045]\tLoss: 928.2998\n",
      "Training Epoch: 13 [21000/36045]\tLoss: 874.5902\n",
      "Training Epoch: 13 [21050/36045]\tLoss: 746.0057\n",
      "Training Epoch: 13 [21100/36045]\tLoss: 746.0867\n",
      "Training Epoch: 13 [21150/36045]\tLoss: 799.3942\n",
      "Training Epoch: 13 [21200/36045]\tLoss: 799.9793\n",
      "Training Epoch: 13 [21250/36045]\tLoss: 763.0087\n",
      "Training Epoch: 13 [21300/36045]\tLoss: 894.6152\n",
      "Training Epoch: 13 [21350/36045]\tLoss: 892.7838\n",
      "Training Epoch: 13 [21400/36045]\tLoss: 897.8564\n",
      "Training Epoch: 13 [21450/36045]\tLoss: 911.2473\n",
      "Training Epoch: 13 [21500/36045]\tLoss: 910.2064\n",
      "Training Epoch: 13 [21550/36045]\tLoss: 1013.8840\n",
      "Training Epoch: 13 [21600/36045]\tLoss: 1014.9539\n",
      "Training Epoch: 13 [21650/36045]\tLoss: 1032.4882\n",
      "Training Epoch: 13 [21700/36045]\tLoss: 1028.8258\n",
      "Training Epoch: 13 [21750/36045]\tLoss: 995.1543\n",
      "Training Epoch: 13 [21800/36045]\tLoss: 745.8591\n",
      "Training Epoch: 13 [21850/36045]\tLoss: 722.8726\n",
      "Training Epoch: 13 [21900/36045]\tLoss: 739.7561\n",
      "Training Epoch: 13 [21950/36045]\tLoss: 733.5007\n",
      "Training Epoch: 13 [22000/36045]\tLoss: 742.6481\n",
      "Training Epoch: 13 [22050/36045]\tLoss: 786.0493\n",
      "Training Epoch: 13 [22100/36045]\tLoss: 775.3036\n",
      "Training Epoch: 13 [22150/36045]\tLoss: 750.3508\n",
      "Training Epoch: 13 [22200/36045]\tLoss: 775.0259\n",
      "Training Epoch: 13 [22250/36045]\tLoss: 782.7145\n",
      "Training Epoch: 13 [22300/36045]\tLoss: 842.1268\n",
      "Training Epoch: 13 [22350/36045]\tLoss: 872.7051\n",
      "Training Epoch: 13 [22400/36045]\tLoss: 897.6788\n",
      "Training Epoch: 13 [22450/36045]\tLoss: 879.8685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [22500/36045]\tLoss: 854.3352\n",
      "Training Epoch: 13 [22550/36045]\tLoss: 909.2028\n",
      "Training Epoch: 13 [22600/36045]\tLoss: 1000.8200\n",
      "Training Epoch: 13 [22650/36045]\tLoss: 1048.6112\n",
      "Training Epoch: 13 [22700/36045]\tLoss: 1075.5966\n",
      "Training Epoch: 13 [22750/36045]\tLoss: 1099.5525\n",
      "Training Epoch: 13 [22800/36045]\tLoss: 1147.5603\n",
      "Training Epoch: 13 [22850/36045]\tLoss: 946.7532\n",
      "Training Epoch: 13 [22900/36045]\tLoss: 950.8749\n",
      "Training Epoch: 13 [22950/36045]\tLoss: 921.7385\n",
      "Training Epoch: 13 [23000/36045]\tLoss: 927.1061\n",
      "Training Epoch: 13 [23050/36045]\tLoss: 832.7053\n",
      "Training Epoch: 13 [23100/36045]\tLoss: 851.3748\n",
      "Training Epoch: 13 [23150/36045]\tLoss: 834.7659\n",
      "Training Epoch: 13 [23200/36045]\tLoss: 786.9741\n",
      "Training Epoch: 13 [23250/36045]\tLoss: 792.9206\n",
      "Training Epoch: 13 [23300/36045]\tLoss: 787.1406\n",
      "Training Epoch: 13 [23350/36045]\tLoss: 810.4255\n",
      "Training Epoch: 13 [23400/36045]\tLoss: 881.9866\n",
      "Training Epoch: 13 [23450/36045]\tLoss: 870.9137\n",
      "Training Epoch: 13 [23500/36045]\tLoss: 836.1569\n",
      "Training Epoch: 13 [23550/36045]\tLoss: 903.2605\n",
      "Training Epoch: 13 [23600/36045]\tLoss: 1016.1238\n",
      "Training Epoch: 13 [23650/36045]\tLoss: 1037.4498\n",
      "Training Epoch: 13 [23700/36045]\tLoss: 1049.2047\n",
      "Training Epoch: 13 [23750/36045]\tLoss: 1015.3120\n",
      "Training Epoch: 13 [23800/36045]\tLoss: 799.4691\n",
      "Training Epoch: 13 [23850/36045]\tLoss: 832.1213\n",
      "Training Epoch: 13 [23900/36045]\tLoss: 822.8199\n",
      "Training Epoch: 13 [23950/36045]\tLoss: 802.0449\n",
      "Training Epoch: 13 [24000/36045]\tLoss: 778.1379\n",
      "Training Epoch: 13 [24050/36045]\tLoss: 716.0069\n",
      "Training Epoch: 13 [24100/36045]\tLoss: 752.5489\n",
      "Training Epoch: 13 [24150/36045]\tLoss: 750.8615\n",
      "Training Epoch: 13 [24200/36045]\tLoss: 736.3447\n",
      "Training Epoch: 13 [24250/36045]\tLoss: 717.4978\n",
      "Training Epoch: 13 [24300/36045]\tLoss: 775.6816\n",
      "Training Epoch: 13 [24350/36045]\tLoss: 797.1441\n",
      "Training Epoch: 13 [24400/36045]\tLoss: 819.3850\n",
      "Training Epoch: 13 [24450/36045]\tLoss: 783.1620\n",
      "Training Epoch: 13 [24500/36045]\tLoss: 825.9958\n",
      "Training Epoch: 13 [24550/36045]\tLoss: 919.6941\n",
      "Training Epoch: 13 [24600/36045]\tLoss: 914.3735\n",
      "Training Epoch: 13 [24650/36045]\tLoss: 881.3828\n",
      "Training Epoch: 13 [24700/36045]\tLoss: 895.1425\n",
      "Training Epoch: 13 [24750/36045]\tLoss: 826.0764\n",
      "Training Epoch: 13 [24800/36045]\tLoss: 710.4843\n",
      "Training Epoch: 13 [24850/36045]\tLoss: 730.9741\n",
      "Training Epoch: 13 [24900/36045]\tLoss: 729.8974\n",
      "Training Epoch: 13 [24950/36045]\tLoss: 732.6321\n",
      "Training Epoch: 13 [25000/36045]\tLoss: 699.6116\n",
      "Training Epoch: 13 [25050/36045]\tLoss: 665.0602\n",
      "Training Epoch: 13 [25100/36045]\tLoss: 596.5789\n",
      "Training Epoch: 13 [25150/36045]\tLoss: 554.1162\n",
      "Training Epoch: 13 [25200/36045]\tLoss: 551.5458\n",
      "Training Epoch: 13 [25250/36045]\tLoss: 586.6962\n",
      "Training Epoch: 13 [25300/36045]\tLoss: 765.1462\n",
      "Training Epoch: 13 [25350/36045]\tLoss: 763.8193\n",
      "Training Epoch: 13 [25400/36045]\tLoss: 713.8751\n",
      "Training Epoch: 13 [25450/36045]\tLoss: 714.4818\n",
      "Training Epoch: 13 [25500/36045]\tLoss: 779.4437\n",
      "Training Epoch: 13 [25550/36045]\tLoss: 909.1459\n",
      "Training Epoch: 13 [25600/36045]\tLoss: 915.5471\n",
      "Training Epoch: 13 [25650/36045]\tLoss: 880.6294\n",
      "Training Epoch: 13 [25700/36045]\tLoss: 894.9362\n",
      "Training Epoch: 13 [25750/36045]\tLoss: 859.1280\n",
      "Training Epoch: 13 [25800/36045]\tLoss: 535.4968\n",
      "Training Epoch: 13 [25850/36045]\tLoss: 547.3998\n",
      "Training Epoch: 13 [25900/36045]\tLoss: 525.0067\n",
      "Training Epoch: 13 [25950/36045]\tLoss: 536.6411\n",
      "Training Epoch: 13 [26000/36045]\tLoss: 659.2252\n",
      "Training Epoch: 13 [26050/36045]\tLoss: 892.9998\n",
      "Training Epoch: 13 [26100/36045]\tLoss: 927.4401\n",
      "Training Epoch: 13 [26150/36045]\tLoss: 921.4019\n",
      "Training Epoch: 13 [26200/36045]\tLoss: 894.9780\n",
      "Training Epoch: 13 [26250/36045]\tLoss: 928.5897\n",
      "Training Epoch: 13 [26300/36045]\tLoss: 810.4147\n",
      "Training Epoch: 13 [26350/36045]\tLoss: 816.1134\n",
      "Training Epoch: 13 [26400/36045]\tLoss: 796.9740\n",
      "Training Epoch: 13 [26450/36045]\tLoss: 723.4700\n",
      "Training Epoch: 13 [26500/36045]\tLoss: 872.2018\n",
      "Training Epoch: 13 [26550/36045]\tLoss: 887.6984\n",
      "Training Epoch: 13 [26600/36045]\tLoss: 880.6232\n",
      "Training Epoch: 13 [26650/36045]\tLoss: 899.6395\n",
      "Training Epoch: 13 [26700/36045]\tLoss: 877.9062\n",
      "Training Epoch: 13 [26750/36045]\tLoss: 820.3806\n",
      "Training Epoch: 13 [26800/36045]\tLoss: 600.2595\n",
      "Training Epoch: 13 [26850/36045]\tLoss: 502.9759\n",
      "Training Epoch: 13 [26900/36045]\tLoss: 506.5966\n",
      "Training Epoch: 13 [26950/36045]\tLoss: 554.9802\n",
      "Training Epoch: 13 [27000/36045]\tLoss: 880.7054\n",
      "Training Epoch: 13 [27050/36045]\tLoss: 929.8661\n",
      "Training Epoch: 13 [27100/36045]\tLoss: 901.0325\n",
      "Training Epoch: 13 [27150/36045]\tLoss: 949.0461\n",
      "Training Epoch: 13 [27200/36045]\tLoss: 705.7462\n",
      "Training Epoch: 13 [27250/36045]\tLoss: 704.0217\n",
      "Training Epoch: 13 [27300/36045]\tLoss: 681.3519\n",
      "Training Epoch: 13 [27350/36045]\tLoss: 686.1699\n",
      "Training Epoch: 13 [27400/36045]\tLoss: 683.3685\n",
      "Training Epoch: 13 [27450/36045]\tLoss: 860.1335\n",
      "Training Epoch: 13 [27500/36045]\tLoss: 923.6373\n",
      "Training Epoch: 13 [27550/36045]\tLoss: 916.4222\n",
      "Training Epoch: 13 [27600/36045]\tLoss: 921.9420\n",
      "Training Epoch: 13 [27650/36045]\tLoss: 919.1382\n",
      "Training Epoch: 13 [27700/36045]\tLoss: 948.6458\n",
      "Training Epoch: 13 [27750/36045]\tLoss: 964.1718\n",
      "Training Epoch: 13 [27800/36045]\tLoss: 946.5597\n",
      "Training Epoch: 13 [27850/36045]\tLoss: 928.3282\n",
      "Training Epoch: 13 [27900/36045]\tLoss: 826.4413\n",
      "Training Epoch: 13 [27950/36045]\tLoss: 682.5426\n",
      "Training Epoch: 13 [28000/36045]\tLoss: 651.9399\n",
      "Training Epoch: 13 [28050/36045]\tLoss: 672.5909\n",
      "Training Epoch: 13 [28100/36045]\tLoss: 660.0682\n",
      "Training Epoch: 13 [28150/36045]\tLoss: 705.7540\n",
      "Training Epoch: 13 [28200/36045]\tLoss: 707.5392\n",
      "Training Epoch: 13 [28250/36045]\tLoss: 711.2202\n",
      "Training Epoch: 13 [28300/36045]\tLoss: 670.3146\n",
      "Training Epoch: 13 [28350/36045]\tLoss: 668.2670\n",
      "Training Epoch: 13 [28400/36045]\tLoss: 1013.2921\n",
      "Training Epoch: 13 [28450/36045]\tLoss: 906.9747\n",
      "Training Epoch: 13 [28500/36045]\tLoss: 787.2205\n",
      "Training Epoch: 13 [28550/36045]\tLoss: 721.4119\n",
      "Training Epoch: 13 [28600/36045]\tLoss: 801.3788\n",
      "Training Epoch: 13 [28650/36045]\tLoss: 937.5745\n",
      "Training Epoch: 13 [28700/36045]\tLoss: 931.4374\n",
      "Training Epoch: 13 [28750/36045]\tLoss: 919.5486\n",
      "Training Epoch: 13 [28800/36045]\tLoss: 928.9588\n",
      "Training Epoch: 13 [28850/36045]\tLoss: 801.5418\n",
      "Training Epoch: 13 [28900/36045]\tLoss: 635.6716\n",
      "Training Epoch: 13 [28950/36045]\tLoss: 628.5982\n",
      "Training Epoch: 13 [29000/36045]\tLoss: 632.4216\n",
      "Training Epoch: 13 [29050/36045]\tLoss: 643.0764\n",
      "Training Epoch: 13 [29100/36045]\tLoss: 664.3586\n",
      "Training Epoch: 13 [29150/36045]\tLoss: 647.4085\n",
      "Training Epoch: 13 [29200/36045]\tLoss: 634.9722\n",
      "Training Epoch: 13 [29250/36045]\tLoss: 615.3895\n",
      "Training Epoch: 13 [29300/36045]\tLoss: 716.3474\n",
      "Training Epoch: 13 [29350/36045]\tLoss: 860.3708\n",
      "Training Epoch: 13 [29400/36045]\tLoss: 881.4471\n",
      "Training Epoch: 13 [29450/36045]\tLoss: 917.7481\n",
      "Training Epoch: 13 [29500/36045]\tLoss: 935.1244\n",
      "Training Epoch: 13 [29550/36045]\tLoss: 885.5691\n",
      "Training Epoch: 13 [29600/36045]\tLoss: 757.4057\n",
      "Training Epoch: 13 [29650/36045]\tLoss: 738.5550\n",
      "Training Epoch: 13 [29700/36045]\tLoss: 652.5618\n",
      "Training Epoch: 13 [29750/36045]\tLoss: 658.3755\n",
      "Training Epoch: 13 [29800/36045]\tLoss: 708.0803\n",
      "Training Epoch: 13 [29850/36045]\tLoss: 777.7925\n",
      "Training Epoch: 13 [29900/36045]\tLoss: 771.4007\n",
      "Training Epoch: 13 [29950/36045]\tLoss: 793.0485\n",
      "Training Epoch: 13 [30000/36045]\tLoss: 778.0677\n",
      "Training Epoch: 13 [30050/36045]\tLoss: 783.6417\n",
      "Training Epoch: 13 [30100/36045]\tLoss: 957.7258\n",
      "Training Epoch: 13 [30150/36045]\tLoss: 947.4946\n",
      "Training Epoch: 13 [30200/36045]\tLoss: 895.0916\n",
      "Training Epoch: 13 [30250/36045]\tLoss: 946.8757\n",
      "Training Epoch: 13 [30300/36045]\tLoss: 937.0396\n",
      "Training Epoch: 13 [30350/36045]\tLoss: 757.9413\n",
      "Training Epoch: 13 [30400/36045]\tLoss: 747.4023\n",
      "Training Epoch: 13 [30450/36045]\tLoss: 745.2230\n",
      "Training Epoch: 13 [30500/36045]\tLoss: 692.8470\n",
      "Training Epoch: 13 [30550/36045]\tLoss: 642.5509\n",
      "Training Epoch: 13 [30600/36045]\tLoss: 615.1474\n",
      "Training Epoch: 13 [30650/36045]\tLoss: 606.3618\n",
      "Training Epoch: 13 [30700/36045]\tLoss: 628.8849\n",
      "Training Epoch: 13 [30750/36045]\tLoss: 610.2382\n",
      "Training Epoch: 13 [30800/36045]\tLoss: 650.5112\n",
      "Training Epoch: 13 [30850/36045]\tLoss: 643.4790\n",
      "Training Epoch: 13 [30900/36045]\tLoss: 661.3105\n",
      "Training Epoch: 13 [30950/36045]\tLoss: 696.7068\n",
      "Training Epoch: 13 [31000/36045]\tLoss: 683.7037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [31050/36045]\tLoss: 568.8861\n",
      "Training Epoch: 13 [31100/36045]\tLoss: 555.8192\n",
      "Training Epoch: 13 [31150/36045]\tLoss: 560.3853\n",
      "Training Epoch: 13 [31200/36045]\tLoss: 708.1997\n",
      "Training Epoch: 13 [31250/36045]\tLoss: 919.5836\n",
      "Training Epoch: 13 [31300/36045]\tLoss: 881.1556\n",
      "Training Epoch: 13 [31350/36045]\tLoss: 897.8584\n",
      "Training Epoch: 13 [31400/36045]\tLoss: 881.1966\n",
      "Training Epoch: 13 [31450/36045]\tLoss: 877.8475\n",
      "Training Epoch: 13 [31500/36045]\tLoss: 881.8287\n",
      "Training Epoch: 13 [31550/36045]\tLoss: 894.2667\n",
      "Training Epoch: 13 [31600/36045]\tLoss: 839.5190\n",
      "Training Epoch: 13 [31650/36045]\tLoss: 896.4330\n",
      "Training Epoch: 13 [31700/36045]\tLoss: 666.6052\n",
      "Training Epoch: 13 [31750/36045]\tLoss: 558.0634\n",
      "Training Epoch: 13 [31800/36045]\tLoss: 531.2029\n",
      "Training Epoch: 13 [31850/36045]\tLoss: 546.9280\n",
      "Training Epoch: 13 [31900/36045]\tLoss: 830.0840\n",
      "Training Epoch: 13 [31950/36045]\tLoss: 1044.4705\n",
      "Training Epoch: 13 [32000/36045]\tLoss: 1176.6482\n",
      "Training Epoch: 13 [32050/36045]\tLoss: 1124.5142\n",
      "Training Epoch: 13 [32100/36045]\tLoss: 1108.6890\n",
      "Training Epoch: 13 [32150/36045]\tLoss: 896.6840\n",
      "Training Epoch: 13 [32200/36045]\tLoss: 903.9066\n",
      "Training Epoch: 13 [32250/36045]\tLoss: 917.5603\n",
      "Training Epoch: 13 [32300/36045]\tLoss: 898.6000\n",
      "Training Epoch: 13 [32350/36045]\tLoss: 886.0265\n",
      "Training Epoch: 13 [32400/36045]\tLoss: 835.4844\n",
      "Training Epoch: 13 [32450/36045]\tLoss: 693.0649\n",
      "Training Epoch: 13 [32500/36045]\tLoss: 665.4686\n",
      "Training Epoch: 13 [32550/36045]\tLoss: 671.6643\n",
      "Training Epoch: 13 [32600/36045]\tLoss: 665.6706\n",
      "Training Epoch: 13 [32650/36045]\tLoss: 825.3743\n",
      "Training Epoch: 13 [32700/36045]\tLoss: 894.9993\n",
      "Training Epoch: 13 [32750/36045]\tLoss: 852.7775\n",
      "Training Epoch: 13 [32800/36045]\tLoss: 874.4779\n",
      "Training Epoch: 13 [32850/36045]\tLoss: 814.8121\n",
      "Training Epoch: 13 [32900/36045]\tLoss: 664.0872\n",
      "Training Epoch: 13 [32950/36045]\tLoss: 696.1960\n",
      "Training Epoch: 13 [33000/36045]\tLoss: 698.7494\n",
      "Training Epoch: 13 [33050/36045]\tLoss: 659.5147\n",
      "Training Epoch: 13 [33100/36045]\tLoss: 753.4308\n",
      "Training Epoch: 13 [33150/36045]\tLoss: 1008.5298\n",
      "Training Epoch: 13 [33200/36045]\tLoss: 984.4672\n",
      "Training Epoch: 13 [33250/36045]\tLoss: 1010.9874\n",
      "Training Epoch: 13 [33300/36045]\tLoss: 1075.4136\n",
      "Training Epoch: 13 [33350/36045]\tLoss: 828.2892\n",
      "Training Epoch: 13 [33400/36045]\tLoss: 622.3779\n",
      "Training Epoch: 13 [33450/36045]\tLoss: 617.6637\n",
      "Training Epoch: 13 [33500/36045]\tLoss: 633.8167\n",
      "Training Epoch: 13 [33550/36045]\tLoss: 656.9604\n",
      "Training Epoch: 13 [33600/36045]\tLoss: 660.7977\n",
      "Training Epoch: 13 [33650/36045]\tLoss: 863.5110\n",
      "Training Epoch: 13 [33700/36045]\tLoss: 837.7285\n",
      "Training Epoch: 13 [33750/36045]\tLoss: 865.2968\n",
      "Training Epoch: 13 [33800/36045]\tLoss: 861.7166\n",
      "Training Epoch: 13 [33850/36045]\tLoss: 867.1682\n",
      "Training Epoch: 13 [33900/36045]\tLoss: 876.4827\n",
      "Training Epoch: 13 [33950/36045]\tLoss: 889.1622\n",
      "Training Epoch: 13 [34000/36045]\tLoss: 880.3795\n",
      "Training Epoch: 13 [34050/36045]\tLoss: 887.3312\n",
      "Training Epoch: 13 [34100/36045]\tLoss: 848.7695\n",
      "Training Epoch: 13 [34150/36045]\tLoss: 787.7924\n",
      "Training Epoch: 13 [34200/36045]\tLoss: 750.5179\n",
      "Training Epoch: 13 [34250/36045]\tLoss: 766.8607\n",
      "Training Epoch: 13 [34300/36045]\tLoss: 659.1871\n",
      "Training Epoch: 13 [34350/36045]\tLoss: 690.4904\n",
      "Training Epoch: 13 [34400/36045]\tLoss: 674.8660\n",
      "Training Epoch: 13 [34450/36045]\tLoss: 633.0145\n",
      "Training Epoch: 13 [34500/36045]\tLoss: 678.5901\n",
      "Training Epoch: 13 [34550/36045]\tLoss: 670.9356\n",
      "Training Epoch: 13 [34600/36045]\tLoss: 655.2273\n",
      "Training Epoch: 13 [34650/36045]\tLoss: 777.7457\n",
      "Training Epoch: 13 [34700/36045]\tLoss: 818.9394\n",
      "Training Epoch: 13 [34750/36045]\tLoss: 729.0490\n",
      "Training Epoch: 13 [34800/36045]\tLoss: 824.8551\n",
      "Training Epoch: 13 [34850/36045]\tLoss: 841.2717\n",
      "Training Epoch: 13 [34900/36045]\tLoss: 979.1567\n",
      "Training Epoch: 13 [34950/36045]\tLoss: 968.1916\n",
      "Training Epoch: 13 [35000/36045]\tLoss: 974.2549\n",
      "Training Epoch: 13 [35050/36045]\tLoss: 954.3254\n",
      "Training Epoch: 13 [35100/36045]\tLoss: 758.8500\n",
      "Training Epoch: 13 [35150/36045]\tLoss: 750.5896\n",
      "Training Epoch: 13 [35200/36045]\tLoss: 649.3396\n",
      "Training Epoch: 13 [35250/36045]\tLoss: 707.7031\n",
      "Training Epoch: 13 [35300/36045]\tLoss: 720.1225\n",
      "Training Epoch: 13 [35350/36045]\tLoss: 849.0510\n",
      "Training Epoch: 13 [35400/36045]\tLoss: 905.7783\n",
      "Training Epoch: 13 [35450/36045]\tLoss: 864.0538\n",
      "Training Epoch: 13 [35500/36045]\tLoss: 843.8134\n",
      "Training Epoch: 13 [35550/36045]\tLoss: 828.4169\n",
      "Training Epoch: 13 [35600/36045]\tLoss: 871.7476\n",
      "Training Epoch: 13 [35650/36045]\tLoss: 954.7421\n",
      "Training Epoch: 13 [35700/36045]\tLoss: 879.4478\n",
      "Training Epoch: 13 [35750/36045]\tLoss: 944.2653\n",
      "Training Epoch: 13 [35800/36045]\tLoss: 950.0141\n",
      "Training Epoch: 13 [35850/36045]\tLoss: 924.1746\n",
      "Training Epoch: 13 [35900/36045]\tLoss: 965.0194\n",
      "Training Epoch: 13 [35950/36045]\tLoss: 966.3698\n",
      "Training Epoch: 13 [36000/36045]\tLoss: 950.7720\n",
      "Training Epoch: 13 [36045/36045]\tLoss: 931.5837\n",
      "Training Epoch: 13 [4004/4004]\tLoss: 875.9262\n",
      "Training Epoch: 14 [50/36045]\tLoss: 880.7526\n",
      "Training Epoch: 14 [100/36045]\tLoss: 845.0622\n",
      "Training Epoch: 14 [150/36045]\tLoss: 845.2213\n",
      "Training Epoch: 14 [200/36045]\tLoss: 830.4546\n",
      "Training Epoch: 14 [250/36045]\tLoss: 969.5764\n",
      "Training Epoch: 14 [300/36045]\tLoss: 1032.8682\n",
      "Training Epoch: 14 [350/36045]\tLoss: 990.1875\n",
      "Training Epoch: 14 [400/36045]\tLoss: 996.8707\n",
      "Training Epoch: 14 [450/36045]\tLoss: 973.5074\n",
      "Training Epoch: 14 [500/36045]\tLoss: 913.6005\n",
      "Training Epoch: 14 [550/36045]\tLoss: 929.8918\n",
      "Training Epoch: 14 [600/36045]\tLoss: 895.3940\n",
      "Training Epoch: 14 [650/36045]\tLoss: 927.9132\n",
      "Training Epoch: 14 [700/36045]\tLoss: 916.3538\n",
      "Training Epoch: 14 [750/36045]\tLoss: 887.4684\n",
      "Training Epoch: 14 [800/36045]\tLoss: 906.1170\n",
      "Training Epoch: 14 [850/36045]\tLoss: 878.8206\n",
      "Training Epoch: 14 [900/36045]\tLoss: 838.0970\n",
      "Training Epoch: 14 [950/36045]\tLoss: 799.5251\n",
      "Training Epoch: 14 [1000/36045]\tLoss: 767.9155\n",
      "Training Epoch: 14 [1050/36045]\tLoss: 770.6442\n",
      "Training Epoch: 14 [1100/36045]\tLoss: 750.0638\n",
      "Training Epoch: 14 [1150/36045]\tLoss: 754.9740\n",
      "Training Epoch: 14 [1200/36045]\tLoss: 799.8886\n",
      "Training Epoch: 14 [1250/36045]\tLoss: 906.1099\n",
      "Training Epoch: 14 [1300/36045]\tLoss: 908.6926\n",
      "Training Epoch: 14 [1350/36045]\tLoss: 914.5527\n",
      "Training Epoch: 14 [1400/36045]\tLoss: 950.1655\n",
      "Training Epoch: 14 [1450/36045]\tLoss: 915.9468\n",
      "Training Epoch: 14 [1500/36045]\tLoss: 851.2636\n",
      "Training Epoch: 14 [1550/36045]\tLoss: 876.5633\n",
      "Training Epoch: 14 [1600/36045]\tLoss: 887.0158\n",
      "Training Epoch: 14 [1650/36045]\tLoss: 871.1024\n",
      "Training Epoch: 14 [1700/36045]\tLoss: 882.3328\n",
      "Training Epoch: 14 [1750/36045]\tLoss: 928.6014\n",
      "Training Epoch: 14 [1800/36045]\tLoss: 911.5186\n",
      "Training Epoch: 14 [1850/36045]\tLoss: 942.7241\n",
      "Training Epoch: 14 [1900/36045]\tLoss: 880.5898\n",
      "Training Epoch: 14 [1950/36045]\tLoss: 891.9650\n",
      "Training Epoch: 14 [2000/36045]\tLoss: 804.9102\n",
      "Training Epoch: 14 [2050/36045]\tLoss: 811.7065\n",
      "Training Epoch: 14 [2100/36045]\tLoss: 855.5955\n",
      "Training Epoch: 14 [2150/36045]\tLoss: 829.7262\n",
      "Training Epoch: 14 [2200/36045]\tLoss: 769.1522\n",
      "Training Epoch: 14 [2250/36045]\tLoss: 727.2872\n",
      "Training Epoch: 14 [2300/36045]\tLoss: 761.7885\n",
      "Training Epoch: 14 [2350/36045]\tLoss: 726.3168\n",
      "Training Epoch: 14 [2400/36045]\tLoss: 745.3036\n",
      "Training Epoch: 14 [2450/36045]\tLoss: 932.3630\n",
      "Training Epoch: 14 [2500/36045]\tLoss: 975.7643\n",
      "Training Epoch: 14 [2550/36045]\tLoss: 973.3837\n",
      "Training Epoch: 14 [2600/36045]\tLoss: 981.5615\n",
      "Training Epoch: 14 [2650/36045]\tLoss: 1108.7382\n",
      "Training Epoch: 14 [2700/36045]\tLoss: 1196.1997\n",
      "Training Epoch: 14 [2750/36045]\tLoss: 1275.8225\n",
      "Training Epoch: 14 [2800/36045]\tLoss: 1291.6423\n",
      "Training Epoch: 14 [2850/36045]\tLoss: 1066.6504\n",
      "Training Epoch: 14 [2900/36045]\tLoss: 1042.8373\n",
      "Training Epoch: 14 [2950/36045]\tLoss: 1003.5540\n",
      "Training Epoch: 14 [3000/36045]\tLoss: 1001.2686\n",
      "Training Epoch: 14 [3050/36045]\tLoss: 1038.9833\n",
      "Training Epoch: 14 [3100/36045]\tLoss: 946.5423\n",
      "Training Epoch: 14 [3150/36045]\tLoss: 730.1139\n",
      "Training Epoch: 14 [3200/36045]\tLoss: 760.0450\n",
      "Training Epoch: 14 [3250/36045]\tLoss: 714.3453\n",
      "Training Epoch: 14 [3300/36045]\tLoss: 675.3716\n",
      "Training Epoch: 14 [3350/36045]\tLoss: 711.8867\n",
      "Training Epoch: 14 [3400/36045]\tLoss: 748.0188\n",
      "Training Epoch: 14 [3450/36045]\tLoss: 806.6874\n",
      "Training Epoch: 14 [3500/36045]\tLoss: 785.3639\n",
      "Training Epoch: 14 [3550/36045]\tLoss: 758.0391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [3600/36045]\tLoss: 809.6202\n",
      "Training Epoch: 14 [3650/36045]\tLoss: 940.2285\n",
      "Training Epoch: 14 [3700/36045]\tLoss: 944.3661\n",
      "Training Epoch: 14 [3750/36045]\tLoss: 907.6588\n",
      "Training Epoch: 14 [3800/36045]\tLoss: 894.6633\n",
      "Training Epoch: 14 [3850/36045]\tLoss: 897.9887\n",
      "Training Epoch: 14 [3900/36045]\tLoss: 907.2341\n",
      "Training Epoch: 14 [3950/36045]\tLoss: 869.4281\n",
      "Training Epoch: 14 [4000/36045]\tLoss: 888.2585\n",
      "Training Epoch: 14 [4050/36045]\tLoss: 816.7995\n",
      "Training Epoch: 14 [4100/36045]\tLoss: 793.0118\n",
      "Training Epoch: 14 [4150/36045]\tLoss: 819.4514\n",
      "Training Epoch: 14 [4200/36045]\tLoss: 810.4018\n",
      "Training Epoch: 14 [4250/36045]\tLoss: 812.7073\n",
      "Training Epoch: 14 [4300/36045]\tLoss: 836.1011\n",
      "Training Epoch: 14 [4350/36045]\tLoss: 815.3057\n",
      "Training Epoch: 14 [4400/36045]\tLoss: 779.2014\n",
      "Training Epoch: 14 [4450/36045]\tLoss: 841.9569\n",
      "Training Epoch: 14 [4500/36045]\tLoss: 895.0546\n",
      "Training Epoch: 14 [4550/36045]\tLoss: 908.0308\n",
      "Training Epoch: 14 [4600/36045]\tLoss: 931.8298\n",
      "Training Epoch: 14 [4650/36045]\tLoss: 925.2955\n",
      "Training Epoch: 14 [4700/36045]\tLoss: 853.7407\n",
      "Training Epoch: 14 [4750/36045]\tLoss: 837.1528\n",
      "Training Epoch: 14 [4800/36045]\tLoss: 870.3785\n",
      "Training Epoch: 14 [4850/36045]\tLoss: 852.7806\n",
      "Training Epoch: 14 [4900/36045]\tLoss: 826.2997\n",
      "Training Epoch: 14 [4950/36045]\tLoss: 858.5565\n",
      "Training Epoch: 14 [5000/36045]\tLoss: 903.2527\n",
      "Training Epoch: 14 [5050/36045]\tLoss: 872.3194\n",
      "Training Epoch: 14 [5100/36045]\tLoss: 885.9505\n",
      "Training Epoch: 14 [5150/36045]\tLoss: 864.3127\n",
      "Training Epoch: 14 [5200/36045]\tLoss: 859.4296\n",
      "Training Epoch: 14 [5250/36045]\tLoss: 848.8608\n",
      "Training Epoch: 14 [5300/36045]\tLoss: 850.2651\n",
      "Training Epoch: 14 [5350/36045]\tLoss: 881.8963\n",
      "Training Epoch: 14 [5400/36045]\tLoss: 842.3796\n",
      "Training Epoch: 14 [5450/36045]\tLoss: 793.4948\n",
      "Training Epoch: 14 [5500/36045]\tLoss: 837.2340\n",
      "Training Epoch: 14 [5550/36045]\tLoss: 815.4960\n",
      "Training Epoch: 14 [5600/36045]\tLoss: 926.9422\n",
      "Training Epoch: 14 [5650/36045]\tLoss: 881.4723\n",
      "Training Epoch: 14 [5700/36045]\tLoss: 833.1271\n",
      "Training Epoch: 14 [5750/36045]\tLoss: 816.5395\n",
      "Training Epoch: 14 [5800/36045]\tLoss: 868.6641\n",
      "Training Epoch: 14 [5850/36045]\tLoss: 841.1442\n",
      "Training Epoch: 14 [5900/36045]\tLoss: 965.7523\n",
      "Training Epoch: 14 [5950/36045]\tLoss: 994.4535\n",
      "Training Epoch: 14 [6000/36045]\tLoss: 974.9857\n",
      "Training Epoch: 14 [6050/36045]\tLoss: 939.9852\n",
      "Training Epoch: 14 [6100/36045]\tLoss: 946.6555\n",
      "Training Epoch: 14 [6150/36045]\tLoss: 912.4089\n",
      "Training Epoch: 14 [6200/36045]\tLoss: 908.7285\n",
      "Training Epoch: 14 [6250/36045]\tLoss: 924.7993\n",
      "Training Epoch: 14 [6300/36045]\tLoss: 945.0264\n",
      "Training Epoch: 14 [6350/36045]\tLoss: 999.1959\n",
      "Training Epoch: 14 [6400/36045]\tLoss: 851.9401\n",
      "Training Epoch: 14 [6450/36045]\tLoss: 790.5211\n",
      "Training Epoch: 14 [6500/36045]\tLoss: 811.6226\n",
      "Training Epoch: 14 [6550/36045]\tLoss: 827.1324\n",
      "Training Epoch: 14 [6600/36045]\tLoss: 832.1865\n",
      "Training Epoch: 14 [6650/36045]\tLoss: 941.0366\n",
      "Training Epoch: 14 [6700/36045]\tLoss: 985.1964\n",
      "Training Epoch: 14 [6750/36045]\tLoss: 953.9760\n",
      "Training Epoch: 14 [6800/36045]\tLoss: 958.0256\n",
      "Training Epoch: 14 [6850/36045]\tLoss: 941.7113\n",
      "Training Epoch: 14 [6900/36045]\tLoss: 830.8165\n",
      "Training Epoch: 14 [6950/36045]\tLoss: 782.3934\n",
      "Training Epoch: 14 [7000/36045]\tLoss: 832.2267\n",
      "Training Epoch: 14 [7050/36045]\tLoss: 852.8242\n",
      "Training Epoch: 14 [7100/36045]\tLoss: 849.3747\n",
      "Training Epoch: 14 [7150/36045]\tLoss: 867.7482\n",
      "Training Epoch: 14 [7200/36045]\tLoss: 874.1255\n",
      "Training Epoch: 14 [7250/36045]\tLoss: 871.7188\n",
      "Training Epoch: 14 [7300/36045]\tLoss: 856.1415\n",
      "Training Epoch: 14 [7350/36045]\tLoss: 848.5379\n",
      "Training Epoch: 14 [7400/36045]\tLoss: 759.7971\n",
      "Training Epoch: 14 [7450/36045]\tLoss: 764.2613\n",
      "Training Epoch: 14 [7500/36045]\tLoss: 761.4423\n",
      "Training Epoch: 14 [7550/36045]\tLoss: 726.2219\n",
      "Training Epoch: 14 [7600/36045]\tLoss: 819.0555\n",
      "Training Epoch: 14 [7650/36045]\tLoss: 885.5321\n",
      "Training Epoch: 14 [7700/36045]\tLoss: 845.9417\n",
      "Training Epoch: 14 [7750/36045]\tLoss: 860.8937\n",
      "Training Epoch: 14 [7800/36045]\tLoss: 845.5074\n",
      "Training Epoch: 14 [7850/36045]\tLoss: 804.7030\n",
      "Training Epoch: 14 [7900/36045]\tLoss: 850.2167\n",
      "Training Epoch: 14 [7950/36045]\tLoss: 846.8359\n",
      "Training Epoch: 14 [8000/36045]\tLoss: 866.9279\n",
      "Training Epoch: 14 [8050/36045]\tLoss: 822.0788\n",
      "Training Epoch: 14 [8100/36045]\tLoss: 852.6243\n",
      "Training Epoch: 14 [8150/36045]\tLoss: 965.6467\n",
      "Training Epoch: 14 [8200/36045]\tLoss: 951.2789\n",
      "Training Epoch: 14 [8250/36045]\tLoss: 913.6786\n",
      "Training Epoch: 14 [8300/36045]\tLoss: 989.6602\n",
      "Training Epoch: 14 [8350/36045]\tLoss: 914.1550\n",
      "Training Epoch: 14 [8400/36045]\tLoss: 823.3171\n",
      "Training Epoch: 14 [8450/36045]\tLoss: 772.5854\n",
      "Training Epoch: 14 [8500/36045]\tLoss: 817.1133\n",
      "Training Epoch: 14 [8550/36045]\tLoss: 801.7587\n",
      "Training Epoch: 14 [8600/36045]\tLoss: 794.7505\n",
      "Training Epoch: 14 [8650/36045]\tLoss: 847.6289\n",
      "Training Epoch: 14 [8700/36045]\tLoss: 895.7247\n",
      "Training Epoch: 14 [8750/36045]\tLoss: 875.6835\n",
      "Training Epoch: 14 [8800/36045]\tLoss: 882.2797\n",
      "Training Epoch: 14 [8850/36045]\tLoss: 872.8514\n",
      "Training Epoch: 14 [8900/36045]\tLoss: 788.2864\n",
      "Training Epoch: 14 [8950/36045]\tLoss: 810.8194\n",
      "Training Epoch: 14 [9000/36045]\tLoss: 822.3342\n",
      "Training Epoch: 14 [9050/36045]\tLoss: 820.8362\n",
      "Training Epoch: 14 [9100/36045]\tLoss: 844.4231\n",
      "Training Epoch: 14 [9150/36045]\tLoss: 623.0670\n",
      "Training Epoch: 14 [9200/36045]\tLoss: 474.8170\n",
      "Training Epoch: 14 [9250/36045]\tLoss: 513.1777\n",
      "Training Epoch: 14 [9300/36045]\tLoss: 531.3019\n",
      "Training Epoch: 14 [9350/36045]\tLoss: 485.6472\n",
      "Training Epoch: 14 [9400/36045]\tLoss: 954.1888\n",
      "Training Epoch: 14 [9450/36045]\tLoss: 1012.3560\n",
      "Training Epoch: 14 [9500/36045]\tLoss: 997.6174\n",
      "Training Epoch: 14 [9550/36045]\tLoss: 1057.4430\n",
      "Training Epoch: 14 [9600/36045]\tLoss: 781.3264\n",
      "Training Epoch: 14 [9650/36045]\tLoss: 781.6381\n",
      "Training Epoch: 14 [9700/36045]\tLoss: 768.0414\n",
      "Training Epoch: 14 [9750/36045]\tLoss: 770.1077\n",
      "Training Epoch: 14 [9800/36045]\tLoss: 992.8012\n",
      "Training Epoch: 14 [9850/36045]\tLoss: 1048.4490\n",
      "Training Epoch: 14 [9900/36045]\tLoss: 1075.2958\n",
      "Training Epoch: 14 [9950/36045]\tLoss: 1045.7069\n",
      "Training Epoch: 14 [10000/36045]\tLoss: 964.2840\n",
      "Training Epoch: 14 [10050/36045]\tLoss: 808.8783\n",
      "Training Epoch: 14 [10100/36045]\tLoss: 810.0105\n",
      "Training Epoch: 14 [10150/36045]\tLoss: 823.3453\n",
      "Training Epoch: 14 [10200/36045]\tLoss: 813.4234\n",
      "Training Epoch: 14 [10250/36045]\tLoss: 973.2017\n",
      "Training Epoch: 14 [10300/36045]\tLoss: 943.7352\n",
      "Training Epoch: 14 [10350/36045]\tLoss: 994.2571\n",
      "Training Epoch: 14 [10400/36045]\tLoss: 984.8863\n",
      "Training Epoch: 14 [10450/36045]\tLoss: 915.7791\n",
      "Training Epoch: 14 [10500/36045]\tLoss: 771.7673\n",
      "Training Epoch: 14 [10550/36045]\tLoss: 768.6306\n",
      "Training Epoch: 14 [10600/36045]\tLoss: 794.8004\n",
      "Training Epoch: 14 [10650/36045]\tLoss: 801.3488\n",
      "Training Epoch: 14 [10700/36045]\tLoss: 897.0180\n",
      "Training Epoch: 14 [10750/36045]\tLoss: 971.8835\n",
      "Training Epoch: 14 [10800/36045]\tLoss: 900.8306\n",
      "Training Epoch: 14 [10850/36045]\tLoss: 952.3280\n",
      "Training Epoch: 14 [10900/36045]\tLoss: 991.9586\n",
      "Training Epoch: 14 [10950/36045]\tLoss: 742.8785\n",
      "Training Epoch: 14 [11000/36045]\tLoss: 736.3901\n",
      "Training Epoch: 14 [11050/36045]\tLoss: 785.2584\n",
      "Training Epoch: 14 [11100/36045]\tLoss: 801.0139\n",
      "Training Epoch: 14 [11150/36045]\tLoss: 868.0757\n",
      "Training Epoch: 14 [11200/36045]\tLoss: 898.0802\n",
      "Training Epoch: 14 [11250/36045]\tLoss: 913.3517\n",
      "Training Epoch: 14 [11300/36045]\tLoss: 893.0963\n",
      "Training Epoch: 14 [11350/36045]\tLoss: 887.5455\n",
      "Training Epoch: 14 [11400/36045]\tLoss: 840.0666\n",
      "Training Epoch: 14 [11450/36045]\tLoss: 801.1220\n",
      "Training Epoch: 14 [11500/36045]\tLoss: 800.1401\n",
      "Training Epoch: 14 [11550/36045]\tLoss: 820.3020\n",
      "Training Epoch: 14 [11600/36045]\tLoss: 892.3048\n",
      "Training Epoch: 14 [11650/36045]\tLoss: 947.9413\n",
      "Training Epoch: 14 [11700/36045]\tLoss: 946.2432\n",
      "Training Epoch: 14 [11750/36045]\tLoss: 968.9764\n",
      "Training Epoch: 14 [11800/36045]\tLoss: 1016.1119\n",
      "Training Epoch: 14 [11850/36045]\tLoss: 1060.2186\n",
      "Training Epoch: 14 [11900/36045]\tLoss: 1293.0334\n",
      "Training Epoch: 14 [11950/36045]\tLoss: 1288.5933\n",
      "Training Epoch: 14 [12000/36045]\tLoss: 1312.5911\n",
      "Training Epoch: 14 [12050/36045]\tLoss: 1266.7001\n",
      "Training Epoch: 14 [12100/36045]\tLoss: 868.0514\n",
      "Training Epoch: 14 [12150/36045]\tLoss: 692.4161\n",
      "Training Epoch: 14 [12200/36045]\tLoss: 685.9098\n",
      "Training Epoch: 14 [12250/36045]\tLoss: 697.5308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [12300/36045]\tLoss: 868.2040\n",
      "Training Epoch: 14 [12350/36045]\tLoss: 931.1208\n",
      "Training Epoch: 14 [12400/36045]\tLoss: 945.2384\n",
      "Training Epoch: 14 [12450/36045]\tLoss: 930.8982\n",
      "Training Epoch: 14 [12500/36045]\tLoss: 966.3066\n",
      "Training Epoch: 14 [12550/36045]\tLoss: 927.5178\n",
      "Training Epoch: 14 [12600/36045]\tLoss: 864.6227\n",
      "Training Epoch: 14 [12650/36045]\tLoss: 860.1148\n",
      "Training Epoch: 14 [12700/36045]\tLoss: 886.9620\n",
      "Training Epoch: 14 [12750/36045]\tLoss: 885.3433\n",
      "Training Epoch: 14 [12800/36045]\tLoss: 864.7520\n",
      "Training Epoch: 14 [12850/36045]\tLoss: 901.8090\n",
      "Training Epoch: 14 [12900/36045]\tLoss: 863.6052\n",
      "Training Epoch: 14 [12950/36045]\tLoss: 855.0387\n",
      "Training Epoch: 14 [13000/36045]\tLoss: 887.4479\n",
      "Training Epoch: 14 [13050/36045]\tLoss: 816.1562\n",
      "Training Epoch: 14 [13100/36045]\tLoss: 853.7187\n",
      "Training Epoch: 14 [13150/36045]\tLoss: 847.8530\n",
      "Training Epoch: 14 [13200/36045]\tLoss: 808.0241\n",
      "Training Epoch: 14 [13250/36045]\tLoss: 849.5716\n",
      "Training Epoch: 14 [13300/36045]\tLoss: 890.3333\n",
      "Training Epoch: 14 [13350/36045]\tLoss: 864.5267\n",
      "Training Epoch: 14 [13400/36045]\tLoss: 871.2724\n",
      "Training Epoch: 14 [13450/36045]\tLoss: 859.8462\n",
      "Training Epoch: 14 [13500/36045]\tLoss: 893.0278\n",
      "Training Epoch: 14 [13550/36045]\tLoss: 1027.4469\n",
      "Training Epoch: 14 [13600/36045]\tLoss: 1059.7017\n",
      "Training Epoch: 14 [13650/36045]\tLoss: 1140.9241\n",
      "Training Epoch: 14 [13700/36045]\tLoss: 1018.4108\n",
      "Training Epoch: 14 [13750/36045]\tLoss: 879.3588\n",
      "Training Epoch: 14 [13800/36045]\tLoss: 854.7432\n",
      "Training Epoch: 14 [13850/36045]\tLoss: 836.5490\n",
      "Training Epoch: 14 [13900/36045]\tLoss: 846.8386\n",
      "Training Epoch: 14 [13950/36045]\tLoss: 887.8311\n",
      "Training Epoch: 14 [14000/36045]\tLoss: 927.1979\n",
      "Training Epoch: 14 [14050/36045]\tLoss: 891.1357\n",
      "Training Epoch: 14 [14100/36045]\tLoss: 887.9918\n",
      "Training Epoch: 14 [14150/36045]\tLoss: 872.7977\n",
      "Training Epoch: 14 [14200/36045]\tLoss: 930.6541\n",
      "Training Epoch: 14 [14250/36045]\tLoss: 1016.3535\n",
      "Training Epoch: 14 [14300/36045]\tLoss: 1021.4434\n",
      "Training Epoch: 14 [14350/36045]\tLoss: 977.0906\n",
      "Training Epoch: 14 [14400/36045]\tLoss: 961.9370\n",
      "Training Epoch: 14 [14450/36045]\tLoss: 1006.2022\n",
      "Training Epoch: 14 [14500/36045]\tLoss: 932.2972\n",
      "Training Epoch: 14 [14550/36045]\tLoss: 972.5875\n",
      "Training Epoch: 14 [14600/36045]\tLoss: 948.3523\n",
      "Training Epoch: 14 [14650/36045]\tLoss: 951.9064\n",
      "Training Epoch: 14 [14700/36045]\tLoss: 893.9182\n",
      "Training Epoch: 14 [14750/36045]\tLoss: 767.7557\n",
      "Training Epoch: 14 [14800/36045]\tLoss: 755.7589\n",
      "Training Epoch: 14 [14850/36045]\tLoss: 763.0393\n",
      "Training Epoch: 14 [14900/36045]\tLoss: 757.6270\n",
      "Training Epoch: 14 [14950/36045]\tLoss: 765.6686\n",
      "Training Epoch: 14 [15000/36045]\tLoss: 787.5648\n",
      "Training Epoch: 14 [15050/36045]\tLoss: 789.4308\n",
      "Training Epoch: 14 [15100/36045]\tLoss: 770.4836\n",
      "Training Epoch: 14 [15150/36045]\tLoss: 761.0089\n",
      "Training Epoch: 14 [15200/36045]\tLoss: 701.6534\n",
      "Training Epoch: 14 [15250/36045]\tLoss: 734.2358\n",
      "Training Epoch: 14 [15300/36045]\tLoss: 714.3445\n",
      "Training Epoch: 14 [15350/36045]\tLoss: 729.9630\n",
      "Training Epoch: 14 [15400/36045]\tLoss: 714.1937\n",
      "Training Epoch: 14 [15450/36045]\tLoss: 703.7266\n",
      "Training Epoch: 14 [15500/36045]\tLoss: 725.6617\n",
      "Training Epoch: 14 [15550/36045]\tLoss: 714.0179\n",
      "Training Epoch: 14 [15600/36045]\tLoss: 794.0337\n",
      "Training Epoch: 14 [15650/36045]\tLoss: 818.8438\n",
      "Training Epoch: 14 [15700/36045]\tLoss: 805.1243\n",
      "Training Epoch: 14 [15750/36045]\tLoss: 795.1337\n",
      "Training Epoch: 14 [15800/36045]\tLoss: 734.2416\n",
      "Training Epoch: 14 [15850/36045]\tLoss: 744.0648\n",
      "Training Epoch: 14 [15900/36045]\tLoss: 754.9159\n",
      "Training Epoch: 14 [15950/36045]\tLoss: 778.0227\n",
      "Training Epoch: 14 [16000/36045]\tLoss: 761.4242\n",
      "Training Epoch: 14 [16050/36045]\tLoss: 729.3982\n",
      "Training Epoch: 14 [16100/36045]\tLoss: 673.9981\n",
      "Training Epoch: 14 [16150/36045]\tLoss: 656.7444\n",
      "Training Epoch: 14 [16200/36045]\tLoss: 791.1991\n",
      "Training Epoch: 14 [16250/36045]\tLoss: 822.5237\n",
      "Training Epoch: 14 [16300/36045]\tLoss: 896.8382\n",
      "Training Epoch: 14 [16350/36045]\tLoss: 912.0062\n",
      "Training Epoch: 14 [16400/36045]\tLoss: 887.1568\n",
      "Training Epoch: 14 [16450/36045]\tLoss: 864.2823\n",
      "Training Epoch: 14 [16500/36045]\tLoss: 861.1046\n",
      "Training Epoch: 14 [16550/36045]\tLoss: 819.6972\n",
      "Training Epoch: 14 [16600/36045]\tLoss: 857.1249\n",
      "Training Epoch: 14 [16650/36045]\tLoss: 882.1838\n",
      "Training Epoch: 14 [16700/36045]\tLoss: 855.0213\n",
      "Training Epoch: 14 [16750/36045]\tLoss: 845.1033\n",
      "Training Epoch: 14 [16800/36045]\tLoss: 861.3116\n",
      "Training Epoch: 14 [16850/36045]\tLoss: 818.1994\n",
      "Training Epoch: 14 [16900/36045]\tLoss: 834.2883\n",
      "Training Epoch: 14 [16950/36045]\tLoss: 861.7567\n",
      "Training Epoch: 14 [17000/36045]\tLoss: 839.3005\n",
      "Training Epoch: 14 [17050/36045]\tLoss: 880.9332\n",
      "Training Epoch: 14 [17100/36045]\tLoss: 884.4979\n",
      "Training Epoch: 14 [17150/36045]\tLoss: 766.2654\n",
      "Training Epoch: 14 [17200/36045]\tLoss: 718.3581\n",
      "Training Epoch: 14 [17250/36045]\tLoss: 751.5801\n",
      "Training Epoch: 14 [17300/36045]\tLoss: 792.4636\n",
      "Training Epoch: 14 [17350/36045]\tLoss: 756.3647\n",
      "Training Epoch: 14 [17400/36045]\tLoss: 775.5620\n",
      "Training Epoch: 14 [17450/36045]\tLoss: 799.5425\n",
      "Training Epoch: 14 [17500/36045]\tLoss: 785.9824\n",
      "Training Epoch: 14 [17550/36045]\tLoss: 793.4329\n",
      "Training Epoch: 14 [17600/36045]\tLoss: 778.6263\n",
      "Training Epoch: 14 [17650/36045]\tLoss: 799.6019\n",
      "Training Epoch: 14 [17700/36045]\tLoss: 777.1686\n",
      "Training Epoch: 14 [17750/36045]\tLoss: 796.5004\n",
      "Training Epoch: 14 [17800/36045]\tLoss: 788.2992\n",
      "Training Epoch: 14 [17850/36045]\tLoss: 770.7200\n",
      "Training Epoch: 14 [17900/36045]\tLoss: 802.4648\n",
      "Training Epoch: 14 [17950/36045]\tLoss: 811.6537\n",
      "Training Epoch: 14 [18000/36045]\tLoss: 801.9071\n",
      "Training Epoch: 14 [18050/36045]\tLoss: 911.9158\n",
      "Training Epoch: 14 [18100/36045]\tLoss: 917.6598\n",
      "Training Epoch: 14 [18150/36045]\tLoss: 923.9740\n",
      "Training Epoch: 14 [18200/36045]\tLoss: 911.3366\n",
      "Training Epoch: 14 [18250/36045]\tLoss: 931.4019\n",
      "Training Epoch: 14 [18300/36045]\tLoss: 852.4293\n",
      "Training Epoch: 14 [18350/36045]\tLoss: 917.1373\n",
      "Training Epoch: 14 [18400/36045]\tLoss: 889.1096\n",
      "Training Epoch: 14 [18450/36045]\tLoss: 869.4578\n",
      "Training Epoch: 14 [18500/36045]\tLoss: 871.9258\n",
      "Training Epoch: 14 [18550/36045]\tLoss: 856.0659\n",
      "Training Epoch: 14 [18600/36045]\tLoss: 845.9948\n",
      "Training Epoch: 14 [18650/36045]\tLoss: 900.5654\n",
      "Training Epoch: 14 [18700/36045]\tLoss: 948.8880\n",
      "Training Epoch: 14 [18750/36045]\tLoss: 929.2798\n",
      "Training Epoch: 14 [18800/36045]\tLoss: 957.1938\n",
      "Training Epoch: 14 [18850/36045]\tLoss: 898.8619\n",
      "Training Epoch: 14 [18900/36045]\tLoss: 963.7095\n",
      "Training Epoch: 14 [18950/36045]\tLoss: 894.0869\n",
      "Training Epoch: 14 [19000/36045]\tLoss: 770.5104\n",
      "Training Epoch: 14 [19050/36045]\tLoss: 743.1373\n",
      "Training Epoch: 14 [19100/36045]\tLoss: 759.9441\n",
      "Training Epoch: 14 [19150/36045]\tLoss: 746.4787\n",
      "Training Epoch: 14 [19200/36045]\tLoss: 776.5778\n",
      "Training Epoch: 14 [19250/36045]\tLoss: 789.3003\n",
      "Training Epoch: 14 [19300/36045]\tLoss: 808.3458\n",
      "Training Epoch: 14 [19350/36045]\tLoss: 789.1651\n",
      "Training Epoch: 14 [19400/36045]\tLoss: 813.7941\n",
      "Training Epoch: 14 [19450/36045]\tLoss: 801.3573\n",
      "Training Epoch: 14 [19500/36045]\tLoss: 805.2226\n",
      "Training Epoch: 14 [19550/36045]\tLoss: 805.4026\n",
      "Training Epoch: 14 [19600/36045]\tLoss: 849.3353\n",
      "Training Epoch: 14 [19650/36045]\tLoss: 1097.2836\n",
      "Training Epoch: 14 [19700/36045]\tLoss: 1052.6088\n",
      "Training Epoch: 14 [19750/36045]\tLoss: 1051.9803\n",
      "Training Epoch: 14 [19800/36045]\tLoss: 1044.7806\n",
      "Training Epoch: 14 [19850/36045]\tLoss: 720.4365\n",
      "Training Epoch: 14 [19900/36045]\tLoss: 692.9091\n",
      "Training Epoch: 14 [19950/36045]\tLoss: 698.4946\n",
      "Training Epoch: 14 [20000/36045]\tLoss: 697.5206\n",
      "Training Epoch: 14 [20050/36045]\tLoss: 781.5645\n",
      "Training Epoch: 14 [20100/36045]\tLoss: 784.9625\n",
      "Training Epoch: 14 [20150/36045]\tLoss: 788.7185\n",
      "Training Epoch: 14 [20200/36045]\tLoss: 786.9864\n",
      "Training Epoch: 14 [20250/36045]\tLoss: 836.5810\n",
      "Training Epoch: 14 [20300/36045]\tLoss: 876.5402\n",
      "Training Epoch: 14 [20350/36045]\tLoss: 903.7073\n",
      "Training Epoch: 14 [20400/36045]\tLoss: 918.4897\n",
      "Training Epoch: 14 [20450/36045]\tLoss: 894.5463\n",
      "Training Epoch: 14 [20500/36045]\tLoss: 870.0650\n",
      "Training Epoch: 14 [20550/36045]\tLoss: 772.7894\n",
      "Training Epoch: 14 [20600/36045]\tLoss: 788.3018\n",
      "Training Epoch: 14 [20650/36045]\tLoss: 784.4795\n",
      "Training Epoch: 14 [20700/36045]\tLoss: 767.5768\n",
      "Training Epoch: 14 [20750/36045]\tLoss: 820.0289\n",
      "Training Epoch: 14 [20800/36045]\tLoss: 893.5222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [20850/36045]\tLoss: 883.9626\n",
      "Training Epoch: 14 [20900/36045]\tLoss: 939.5153\n",
      "Training Epoch: 14 [20950/36045]\tLoss: 885.8961\n",
      "Training Epoch: 14 [21000/36045]\tLoss: 835.1401\n",
      "Training Epoch: 14 [21050/36045]\tLoss: 713.4374\n",
      "Training Epoch: 14 [21100/36045]\tLoss: 713.1951\n",
      "Training Epoch: 14 [21150/36045]\tLoss: 764.3378\n",
      "Training Epoch: 14 [21200/36045]\tLoss: 764.6012\n",
      "Training Epoch: 14 [21250/36045]\tLoss: 729.7313\n",
      "Training Epoch: 14 [21300/36045]\tLoss: 854.9864\n",
      "Training Epoch: 14 [21350/36045]\tLoss: 852.6033\n",
      "Training Epoch: 14 [21400/36045]\tLoss: 857.0218\n",
      "Training Epoch: 14 [21450/36045]\tLoss: 868.6714\n",
      "Training Epoch: 14 [21500/36045]\tLoss: 868.8778\n",
      "Training Epoch: 14 [21550/36045]\tLoss: 971.1874\n",
      "Training Epoch: 14 [21600/36045]\tLoss: 972.5886\n",
      "Training Epoch: 14 [21650/36045]\tLoss: 989.6462\n",
      "Training Epoch: 14 [21700/36045]\tLoss: 985.9705\n",
      "Training Epoch: 14 [21750/36045]\tLoss: 953.2241\n",
      "Training Epoch: 14 [21800/36045]\tLoss: 712.2946\n",
      "Training Epoch: 14 [21850/36045]\tLoss: 690.1754\n",
      "Training Epoch: 14 [21900/36045]\tLoss: 706.1768\n",
      "Training Epoch: 14 [21950/36045]\tLoss: 700.5558\n",
      "Training Epoch: 14 [22000/36045]\tLoss: 709.3059\n",
      "Training Epoch: 14 [22050/36045]\tLoss: 750.0958\n",
      "Training Epoch: 14 [22100/36045]\tLoss: 739.9509\n",
      "Training Epoch: 14 [22150/36045]\tLoss: 716.0591\n",
      "Training Epoch: 14 [22200/36045]\tLoss: 739.2359\n",
      "Training Epoch: 14 [22250/36045]\tLoss: 746.4673\n",
      "Training Epoch: 14 [22300/36045]\tLoss: 804.8733\n",
      "Training Epoch: 14 [22350/36045]\tLoss: 835.2098\n",
      "Training Epoch: 14 [22400/36045]\tLoss: 858.3149\n",
      "Training Epoch: 14 [22450/36045]\tLoss: 841.9757\n",
      "Training Epoch: 14 [22500/36045]\tLoss: 817.3621\n",
      "Training Epoch: 14 [22550/36045]\tLoss: 869.3005\n",
      "Training Epoch: 14 [22600/36045]\tLoss: 955.9890\n",
      "Training Epoch: 14 [22650/36045]\tLoss: 1001.6420\n",
      "Training Epoch: 14 [22700/36045]\tLoss: 1028.1610\n",
      "Training Epoch: 14 [22750/36045]\tLoss: 1051.4187\n",
      "Training Epoch: 14 [22800/36045]\tLoss: 1096.7245\n",
      "Training Epoch: 14 [22850/36045]\tLoss: 904.8126\n",
      "Training Epoch: 14 [22900/36045]\tLoss: 908.9832\n",
      "Training Epoch: 14 [22950/36045]\tLoss: 881.0776\n",
      "Training Epoch: 14 [23000/36045]\tLoss: 885.4724\n",
      "Training Epoch: 14 [23050/36045]\tLoss: 795.6686\n",
      "Training Epoch: 14 [23100/36045]\tLoss: 813.5223\n",
      "Training Epoch: 14 [23150/36045]\tLoss: 797.9491\n",
      "Training Epoch: 14 [23200/36045]\tLoss: 752.5121\n",
      "Training Epoch: 14 [23250/36045]\tLoss: 757.9277\n",
      "Training Epoch: 14 [23300/36045]\tLoss: 752.8106\n",
      "Training Epoch: 14 [23350/36045]\tLoss: 775.6407\n",
      "Training Epoch: 14 [23400/36045]\tLoss: 843.7816\n",
      "Training Epoch: 14 [23450/36045]\tLoss: 833.2631\n",
      "Training Epoch: 14 [23500/36045]\tLoss: 800.5523\n",
      "Training Epoch: 14 [23550/36045]\tLoss: 863.9924\n",
      "Training Epoch: 14 [23600/36045]\tLoss: 972.0187\n",
      "Training Epoch: 14 [23650/36045]\tLoss: 991.1867\n",
      "Training Epoch: 14 [23700/36045]\tLoss: 1003.3891\n",
      "Training Epoch: 14 [23750/36045]\tLoss: 970.5437\n",
      "Training Epoch: 14 [23800/36045]\tLoss: 763.5427\n",
      "Training Epoch: 14 [23850/36045]\tLoss: 795.0801\n",
      "Training Epoch: 14 [23900/36045]\tLoss: 786.3335\n",
      "Training Epoch: 14 [23950/36045]\tLoss: 765.6158\n",
      "Training Epoch: 14 [24000/36045]\tLoss: 741.6792\n",
      "Training Epoch: 14 [24050/36045]\tLoss: 683.8098\n",
      "Training Epoch: 14 [24100/36045]\tLoss: 719.4858\n",
      "Training Epoch: 14 [24150/36045]\tLoss: 717.0499\n",
      "Training Epoch: 14 [24200/36045]\tLoss: 704.2732\n",
      "Training Epoch: 14 [24250/36045]\tLoss: 685.8799\n",
      "Training Epoch: 14 [24300/36045]\tLoss: 740.2881\n",
      "Training Epoch: 14 [24350/36045]\tLoss: 760.2960\n",
      "Training Epoch: 14 [24400/36045]\tLoss: 781.8574\n",
      "Training Epoch: 14 [24450/36045]\tLoss: 746.6478\n",
      "Training Epoch: 14 [24500/36045]\tLoss: 787.3423\n",
      "Training Epoch: 14 [24550/36045]\tLoss: 881.5059\n",
      "Training Epoch: 14 [24600/36045]\tLoss: 876.0809\n",
      "Training Epoch: 14 [24650/36045]\tLoss: 843.5249\n",
      "Training Epoch: 14 [24700/36045]\tLoss: 856.8163\n",
      "Training Epoch: 14 [24750/36045]\tLoss: 791.3311\n",
      "Training Epoch: 14 [24800/36045]\tLoss: 676.8745\n",
      "Training Epoch: 14 [24850/36045]\tLoss: 697.6100\n",
      "Training Epoch: 14 [24900/36045]\tLoss: 696.0272\n",
      "Training Epoch: 14 [24950/36045]\tLoss: 698.2303\n",
      "Training Epoch: 14 [25000/36045]\tLoss: 667.2921\n",
      "Training Epoch: 14 [25050/36045]\tLoss: 635.1815\n",
      "Training Epoch: 14 [25100/36045]\tLoss: 570.3202\n",
      "Training Epoch: 14 [25150/36045]\tLoss: 529.3608\n",
      "Training Epoch: 14 [25200/36045]\tLoss: 526.2755\n",
      "Training Epoch: 14 [25250/36045]\tLoss: 560.4396\n",
      "Training Epoch: 14 [25300/36045]\tLoss: 732.0676\n",
      "Training Epoch: 14 [25350/36045]\tLoss: 730.6594\n",
      "Training Epoch: 14 [25400/36045]\tLoss: 682.2668\n",
      "Training Epoch: 14 [25450/36045]\tLoss: 683.2037\n",
      "Training Epoch: 14 [25500/36045]\tLoss: 745.5274\n",
      "Training Epoch: 14 [25550/36045]\tLoss: 866.7456\n",
      "Training Epoch: 14 [25600/36045]\tLoss: 872.9026\n",
      "Training Epoch: 14 [25650/36045]\tLoss: 839.4080\n",
      "Training Epoch: 14 [25700/36045]\tLoss: 851.9583\n",
      "Training Epoch: 14 [25750/36045]\tLoss: 819.3152\n",
      "Training Epoch: 14 [25800/36045]\tLoss: 512.6202\n",
      "Training Epoch: 14 [25850/36045]\tLoss: 524.4448\n",
      "Training Epoch: 14 [25900/36045]\tLoss: 502.7863\n",
      "Training Epoch: 14 [25950/36045]\tLoss: 513.5748\n",
      "Training Epoch: 14 [26000/36045]\tLoss: 630.2532\n",
      "Training Epoch: 14 [26050/36045]\tLoss: 853.7880\n",
      "Training Epoch: 14 [26100/36045]\tLoss: 887.0117\n",
      "Training Epoch: 14 [26150/36045]\tLoss: 881.8777\n",
      "Training Epoch: 14 [26200/36045]\tLoss: 855.6069\n",
      "Training Epoch: 14 [26250/36045]\tLoss: 889.2711\n",
      "Training Epoch: 14 [26300/36045]\tLoss: 778.5312\n",
      "Training Epoch: 14 [26350/36045]\tLoss: 784.8718\n",
      "Training Epoch: 14 [26400/36045]\tLoss: 765.2008\n",
      "Training Epoch: 14 [26450/36045]\tLoss: 692.3940\n",
      "Training Epoch: 14 [26500/36045]\tLoss: 834.2637\n",
      "Training Epoch: 14 [26550/36045]\tLoss: 847.6885\n",
      "Training Epoch: 14 [26600/36045]\tLoss: 840.8770\n",
      "Training Epoch: 14 [26650/36045]\tLoss: 858.8998\n",
      "Training Epoch: 14 [26700/36045]\tLoss: 837.9881\n",
      "Training Epoch: 14 [26750/36045]\tLoss: 782.5836\n",
      "Training Epoch: 14 [26800/36045]\tLoss: 573.2293\n",
      "Training Epoch: 14 [26850/36045]\tLoss: 479.7086\n",
      "Training Epoch: 14 [26900/36045]\tLoss: 483.0760\n",
      "Training Epoch: 14 [26950/36045]\tLoss: 530.0421\n",
      "Training Epoch: 14 [27000/36045]\tLoss: 842.0723\n",
      "Training Epoch: 14 [27050/36045]\tLoss: 888.6649\n",
      "Training Epoch: 14 [27100/36045]\tLoss: 860.7921\n",
      "Training Epoch: 14 [27150/36045]\tLoss: 907.8875\n",
      "Training Epoch: 14 [27200/36045]\tLoss: 674.1931\n",
      "Training Epoch: 14 [27250/36045]\tLoss: 672.0088\n",
      "Training Epoch: 14 [27300/36045]\tLoss: 651.0739\n",
      "Training Epoch: 14 [27350/36045]\tLoss: 654.4550\n",
      "Training Epoch: 14 [27400/36045]\tLoss: 652.0978\n",
      "Training Epoch: 14 [27450/36045]\tLoss: 821.0515\n",
      "Training Epoch: 14 [27500/36045]\tLoss: 881.9630\n",
      "Training Epoch: 14 [27550/36045]\tLoss: 874.6032\n",
      "Training Epoch: 14 [27600/36045]\tLoss: 881.2795\n",
      "Training Epoch: 14 [27650/36045]\tLoss: 877.2005\n",
      "Training Epoch: 14 [27700/36045]\tLoss: 906.8272\n",
      "Training Epoch: 14 [27750/36045]\tLoss: 921.6168\n",
      "Training Epoch: 14 [27800/36045]\tLoss: 904.7127\n",
      "Training Epoch: 14 [27850/36045]\tLoss: 887.4948\n",
      "Training Epoch: 14 [27900/36045]\tLoss: 791.4885\n",
      "Training Epoch: 14 [27950/36045]\tLoss: 653.9673\n",
      "Training Epoch: 14 [28000/36045]\tLoss: 624.1972\n",
      "Training Epoch: 14 [28050/36045]\tLoss: 643.1031\n",
      "Training Epoch: 14 [28100/36045]\tLoss: 631.4471\n",
      "Training Epoch: 14 [28150/36045]\tLoss: 673.3240\n",
      "Training Epoch: 14 [28200/36045]\tLoss: 675.0997\n",
      "Training Epoch: 14 [28250/36045]\tLoss: 677.4062\n",
      "Training Epoch: 14 [28300/36045]\tLoss: 638.5386\n",
      "Training Epoch: 14 [28350/36045]\tLoss: 635.7409\n",
      "Training Epoch: 14 [28400/36045]\tLoss: 979.5079\n",
      "Training Epoch: 14 [28450/36045]\tLoss: 879.1724\n",
      "Training Epoch: 14 [28500/36045]\tLoss: 763.3143\n",
      "Training Epoch: 14 [28550/36045]\tLoss: 699.9128\n",
      "Training Epoch: 14 [28600/36045]\tLoss: 771.9348\n",
      "Training Epoch: 14 [28650/36045]\tLoss: 898.1047\n",
      "Training Epoch: 14 [28700/36045]\tLoss: 891.4868\n",
      "Training Epoch: 14 [28750/36045]\tLoss: 879.5895\n",
      "Training Epoch: 14 [28800/36045]\tLoss: 889.5865\n",
      "Training Epoch: 14 [28850/36045]\tLoss: 768.0566\n",
      "Training Epoch: 14 [28900/36045]\tLoss: 610.1223\n",
      "Training Epoch: 14 [28950/36045]\tLoss: 603.9520\n",
      "Training Epoch: 14 [29000/36045]\tLoss: 606.6191\n",
      "Training Epoch: 14 [29050/36045]\tLoss: 616.9989\n",
      "Training Epoch: 14 [29100/36045]\tLoss: 638.5577\n",
      "Training Epoch: 14 [29150/36045]\tLoss: 622.3474\n",
      "Training Epoch: 14 [29200/36045]\tLoss: 609.5543\n",
      "Training Epoch: 14 [29250/36045]\tLoss: 591.1550\n",
      "Training Epoch: 14 [29300/36045]\tLoss: 686.7449\n",
      "Training Epoch: 14 [29350/36045]\tLoss: 823.4475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [29400/36045]\tLoss: 843.5649\n",
      "Training Epoch: 14 [29450/36045]\tLoss: 877.7043\n",
      "Training Epoch: 14 [29500/36045]\tLoss: 894.0590\n",
      "Training Epoch: 14 [29550/36045]\tLoss: 847.2371\n",
      "Training Epoch: 14 [29600/36045]\tLoss: 723.3226\n",
      "Training Epoch: 14 [29650/36045]\tLoss: 705.3538\n",
      "Training Epoch: 14 [29700/36045]\tLoss: 623.7087\n",
      "Training Epoch: 14 [29750/36045]\tLoss: 629.0485\n",
      "Training Epoch: 14 [29800/36045]\tLoss: 677.7869\n",
      "Training Epoch: 14 [29850/36045]\tLoss: 748.1146\n",
      "Training Epoch: 14 [29900/36045]\tLoss: 741.9212\n",
      "Training Epoch: 14 [29950/36045]\tLoss: 763.8161\n",
      "Training Epoch: 14 [30000/36045]\tLoss: 747.3157\n",
      "Training Epoch: 14 [30050/36045]\tLoss: 752.8439\n",
      "Training Epoch: 14 [30100/36045]\tLoss: 919.2721\n",
      "Training Epoch: 14 [30150/36045]\tLoss: 907.8275\n",
      "Training Epoch: 14 [30200/36045]\tLoss: 857.4025\n",
      "Training Epoch: 14 [30250/36045]\tLoss: 908.5825\n",
      "Training Epoch: 14 [30300/36045]\tLoss: 897.7732\n",
      "Training Epoch: 14 [30350/36045]\tLoss: 723.3309\n",
      "Training Epoch: 14 [30400/36045]\tLoss: 712.1124\n",
      "Training Epoch: 14 [30450/36045]\tLoss: 710.1990\n",
      "Training Epoch: 14 [30500/36045]\tLoss: 660.7296\n",
      "Training Epoch: 14 [30550/36045]\tLoss: 612.9695\n",
      "Training Epoch: 14 [30600/36045]\tLoss: 587.8001\n",
      "Training Epoch: 14 [30650/36045]\tLoss: 579.0431\n",
      "Training Epoch: 14 [30700/36045]\tLoss: 600.6748\n",
      "Training Epoch: 14 [30750/36045]\tLoss: 582.8241\n",
      "Training Epoch: 14 [30800/36045]\tLoss: 621.8816\n",
      "Training Epoch: 14 [30850/36045]\tLoss: 614.1581\n",
      "Training Epoch: 14 [30900/36045]\tLoss: 631.1486\n",
      "Training Epoch: 14 [30950/36045]\tLoss: 664.6797\n",
      "Training Epoch: 14 [31000/36045]\tLoss: 652.3110\n",
      "Training Epoch: 14 [31050/36045]\tLoss: 542.6560\n",
      "Training Epoch: 14 [31100/36045]\tLoss: 530.5511\n",
      "Training Epoch: 14 [31150/36045]\tLoss: 535.3189\n",
      "Training Epoch: 14 [31200/36045]\tLoss: 675.7933\n",
      "Training Epoch: 14 [31250/36045]\tLoss: 877.9032\n",
      "Training Epoch: 14 [31300/36045]\tLoss: 841.4644\n",
      "Training Epoch: 14 [31350/36045]\tLoss: 857.7983\n",
      "Training Epoch: 14 [31400/36045]\tLoss: 840.4091\n",
      "Training Epoch: 14 [31450/36045]\tLoss: 839.9635\n",
      "Training Epoch: 14 [31500/36045]\tLoss: 845.1267\n",
      "Training Epoch: 14 [31550/36045]\tLoss: 857.2110\n",
      "Training Epoch: 14 [31600/36045]\tLoss: 804.6505\n",
      "Training Epoch: 14 [31650/36045]\tLoss: 858.9268\n",
      "Training Epoch: 14 [31700/36045]\tLoss: 636.7886\n",
      "Training Epoch: 14 [31750/36045]\tLoss: 532.6997\n",
      "Training Epoch: 14 [31800/36045]\tLoss: 506.9006\n",
      "Training Epoch: 14 [31850/36045]\tLoss: 521.4542\n",
      "Training Epoch: 14 [31900/36045]\tLoss: 794.7554\n",
      "Training Epoch: 14 [31950/36045]\tLoss: 1003.6129\n",
      "Training Epoch: 14 [32000/36045]\tLoss: 1132.9700\n",
      "Training Epoch: 14 [32050/36045]\tLoss: 1081.7706\n",
      "Training Epoch: 14 [32100/36045]\tLoss: 1066.9711\n",
      "Training Epoch: 14 [32150/36045]\tLoss: 857.4725\n",
      "Training Epoch: 14 [32200/36045]\tLoss: 864.0624\n",
      "Training Epoch: 14 [32250/36045]\tLoss: 876.9905\n",
      "Training Epoch: 14 [32300/36045]\tLoss: 858.5623\n",
      "Training Epoch: 14 [32350/36045]\tLoss: 847.7468\n",
      "Training Epoch: 14 [32400/36045]\tLoss: 798.7191\n",
      "Training Epoch: 14 [32450/36045]\tLoss: 661.7792\n",
      "Training Epoch: 14 [32500/36045]\tLoss: 635.6075\n",
      "Training Epoch: 14 [32550/36045]\tLoss: 640.9685\n",
      "Training Epoch: 14 [32600/36045]\tLoss: 635.3194\n",
      "Training Epoch: 14 [32650/36045]\tLoss: 789.6971\n",
      "Training Epoch: 14 [32700/36045]\tLoss: 857.0455\n",
      "Training Epoch: 14 [32750/36045]\tLoss: 816.1524\n",
      "Training Epoch: 14 [32800/36045]\tLoss: 836.8394\n",
      "Training Epoch: 14 [32850/36045]\tLoss: 778.9373\n",
      "Training Epoch: 14 [32900/36045]\tLoss: 634.9507\n",
      "Training Epoch: 14 [32950/36045]\tLoss: 665.3618\n",
      "Training Epoch: 14 [33000/36045]\tLoss: 666.8271\n",
      "Training Epoch: 14 [33050/36045]\tLoss: 630.1340\n",
      "Training Epoch: 14 [33100/36045]\tLoss: 719.7673\n",
      "Training Epoch: 14 [33150/36045]\tLoss: 966.1667\n",
      "Training Epoch: 14 [33200/36045]\tLoss: 942.9520\n",
      "Training Epoch: 14 [33250/36045]\tLoss: 968.5375\n",
      "Training Epoch: 14 [33300/36045]\tLoss: 1030.8260\n",
      "Training Epoch: 14 [33350/36045]\tLoss: 792.7723\n",
      "Training Epoch: 14 [33400/36045]\tLoss: 594.1656\n",
      "Training Epoch: 14 [33450/36045]\tLoss: 589.3490\n",
      "Training Epoch: 14 [33500/36045]\tLoss: 605.0289\n",
      "Training Epoch: 14 [33550/36045]\tLoss: 627.2571\n",
      "Training Epoch: 14 [33600/36045]\tLoss: 630.3271\n",
      "Training Epoch: 14 [33650/36045]\tLoss: 826.7721\n",
      "Training Epoch: 14 [33700/36045]\tLoss: 802.0500\n",
      "Training Epoch: 14 [33750/36045]\tLoss: 828.7919\n",
      "Training Epoch: 14 [33800/36045]\tLoss: 824.8986\n",
      "Training Epoch: 14 [33850/36045]\tLoss: 830.0950\n",
      "Training Epoch: 14 [33900/36045]\tLoss: 838.0352\n",
      "Training Epoch: 14 [33950/36045]\tLoss: 850.5406\n",
      "Training Epoch: 14 [34000/36045]\tLoss: 841.6514\n",
      "Training Epoch: 14 [34050/36045]\tLoss: 847.6851\n",
      "Training Epoch: 14 [34100/36045]\tLoss: 811.5501\n",
      "Training Epoch: 14 [34150/36045]\tLoss: 753.7378\n",
      "Training Epoch: 14 [34200/36045]\tLoss: 717.4216\n",
      "Training Epoch: 14 [34250/36045]\tLoss: 733.3549\n",
      "Training Epoch: 14 [34300/36045]\tLoss: 630.4518\n",
      "Training Epoch: 14 [34350/36045]\tLoss: 660.6169\n",
      "Training Epoch: 14 [34400/36045]\tLoss: 645.9404\n",
      "Training Epoch: 14 [34450/36045]\tLoss: 606.0524\n",
      "Training Epoch: 14 [34500/36045]\tLoss: 649.6076\n",
      "Training Epoch: 14 [34550/36045]\tLoss: 641.5304\n",
      "Training Epoch: 14 [34600/36045]\tLoss: 629.0983\n",
      "Training Epoch: 14 [34650/36045]\tLoss: 748.9958\n",
      "Training Epoch: 14 [34700/36045]\tLoss: 788.9556\n",
      "Training Epoch: 14 [34750/36045]\tLoss: 702.3049\n",
      "Training Epoch: 14 [34800/36045]\tLoss: 795.8109\n",
      "Training Epoch: 14 [34850/36045]\tLoss: 810.6111\n",
      "Training Epoch: 14 [34900/36045]\tLoss: 936.3057\n",
      "Training Epoch: 14 [34950/36045]\tLoss: 925.3392\n",
      "Training Epoch: 14 [35000/36045]\tLoss: 929.8019\n",
      "Training Epoch: 14 [35050/36045]\tLoss: 911.4763\n",
      "Training Epoch: 14 [35100/36045]\tLoss: 729.7789\n",
      "Training Epoch: 14 [35150/36045]\tLoss: 722.2885\n",
      "Training Epoch: 14 [35200/36045]\tLoss: 623.1901\n",
      "Training Epoch: 14 [35250/36045]\tLoss: 680.1208\n",
      "Training Epoch: 14 [35300/36045]\tLoss: 692.7719\n",
      "Training Epoch: 14 [35350/36045]\tLoss: 813.3812\n",
      "Training Epoch: 14 [35400/36045]\tLoss: 866.5364\n",
      "Training Epoch: 14 [35450/36045]\tLoss: 826.4720\n",
      "Training Epoch: 14 [35500/36045]\tLoss: 806.9781\n",
      "Training Epoch: 14 [35550/36045]\tLoss: 791.2693\n",
      "Training Epoch: 14 [35600/36045]\tLoss: 835.4467\n",
      "Training Epoch: 14 [35650/36045]\tLoss: 916.8674\n",
      "Training Epoch: 14 [35700/36045]\tLoss: 842.0027\n",
      "Training Epoch: 14 [35750/36045]\tLoss: 905.8702\n",
      "Training Epoch: 14 [35800/36045]\tLoss: 911.5106\n",
      "Training Epoch: 14 [35850/36045]\tLoss: 885.6554\n",
      "Training Epoch: 14 [35900/36045]\tLoss: 924.0703\n",
      "Training Epoch: 14 [35950/36045]\tLoss: 924.8630\n",
      "Training Epoch: 14 [36000/36045]\tLoss: 910.4359\n",
      "Training Epoch: 14 [36045/36045]\tLoss: 890.9418\n",
      "Training Epoch: 14 [4004/4004]\tLoss: 836.9020\n",
      "Training Epoch: 15 [50/36045]\tLoss: 840.5134\n",
      "Training Epoch: 15 [100/36045]\tLoss: 805.8664\n",
      "Training Epoch: 15 [150/36045]\tLoss: 805.5853\n",
      "Training Epoch: 15 [200/36045]\tLoss: 791.6641\n",
      "Training Epoch: 15 [250/36045]\tLoss: 928.1232\n",
      "Training Epoch: 15 [300/36045]\tLoss: 991.2767\n",
      "Training Epoch: 15 [350/36045]\tLoss: 950.2770\n",
      "Training Epoch: 15 [400/36045]\tLoss: 954.4658\n",
      "Training Epoch: 15 [450/36045]\tLoss: 931.4187\n",
      "Training Epoch: 15 [500/36045]\tLoss: 872.8170\n",
      "Training Epoch: 15 [550/36045]\tLoss: 887.7743\n",
      "Training Epoch: 15 [600/36045]\tLoss: 855.5599\n",
      "Training Epoch: 15 [650/36045]\tLoss: 886.6918\n",
      "Training Epoch: 15 [700/36045]\tLoss: 874.6433\n",
      "Training Epoch: 15 [750/36045]\tLoss: 847.7574\n",
      "Training Epoch: 15 [800/36045]\tLoss: 866.1526\n",
      "Training Epoch: 15 [850/36045]\tLoss: 841.0764\n",
      "Training Epoch: 15 [900/36045]\tLoss: 801.3018\n",
      "Training Epoch: 15 [950/36045]\tLoss: 763.6252\n",
      "Training Epoch: 15 [1000/36045]\tLoss: 733.0315\n",
      "Training Epoch: 15 [1050/36045]\tLoss: 735.2640\n",
      "Training Epoch: 15 [1100/36045]\tLoss: 715.5326\n",
      "Training Epoch: 15 [1150/36045]\tLoss: 721.5837\n",
      "Training Epoch: 15 [1200/36045]\tLoss: 764.1240\n",
      "Training Epoch: 15 [1250/36045]\tLoss: 867.6484\n",
      "Training Epoch: 15 [1300/36045]\tLoss: 871.1989\n",
      "Training Epoch: 15 [1350/36045]\tLoss: 876.3373\n",
      "Training Epoch: 15 [1400/36045]\tLoss: 910.3818\n",
      "Training Epoch: 15 [1450/36045]\tLoss: 877.8837\n",
      "Training Epoch: 15 [1500/36045]\tLoss: 814.8608\n",
      "Training Epoch: 15 [1550/36045]\tLoss: 838.3961\n",
      "Training Epoch: 15 [1600/36045]\tLoss: 849.0278\n",
      "Training Epoch: 15 [1650/36045]\tLoss: 833.3978\n",
      "Training Epoch: 15 [1700/36045]\tLoss: 844.4713\n",
      "Training Epoch: 15 [1750/36045]\tLoss: 889.3521\n",
      "Training Epoch: 15 [1800/36045]\tLoss: 872.5802\n",
      "Training Epoch: 15 [1850/36045]\tLoss: 901.2416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [1900/36045]\tLoss: 841.6276\n",
      "Training Epoch: 15 [1950/36045]\tLoss: 853.2723\n",
      "Training Epoch: 15 [2000/36045]\tLoss: 769.9805\n",
      "Training Epoch: 15 [2050/36045]\tLoss: 776.1354\n",
      "Training Epoch: 15 [2100/36045]\tLoss: 817.9781\n",
      "Training Epoch: 15 [2150/36045]\tLoss: 793.1686\n",
      "Training Epoch: 15 [2200/36045]\tLoss: 734.9911\n",
      "Training Epoch: 15 [2250/36045]\tLoss: 695.1083\n",
      "Training Epoch: 15 [2300/36045]\tLoss: 728.4558\n",
      "Training Epoch: 15 [2350/36045]\tLoss: 694.4777\n",
      "Training Epoch: 15 [2400/36045]\tLoss: 711.8022\n",
      "Training Epoch: 15 [2450/36045]\tLoss: 893.3545\n",
      "Training Epoch: 15 [2500/36045]\tLoss: 935.9177\n",
      "Training Epoch: 15 [2550/36045]\tLoss: 933.1882\n",
      "Training Epoch: 15 [2600/36045]\tLoss: 941.9112\n",
      "Training Epoch: 15 [2650/36045]\tLoss: 1068.8573\n",
      "Training Epoch: 15 [2700/36045]\tLoss: 1156.0416\n",
      "Training Epoch: 15 [2750/36045]\tLoss: 1234.5574\n",
      "Training Epoch: 15 [2800/36045]\tLoss: 1249.4406\n",
      "Training Epoch: 15 [2850/36045]\tLoss: 1022.8802\n",
      "Training Epoch: 15 [2900/36045]\tLoss: 996.7534\n",
      "Training Epoch: 15 [2950/36045]\tLoss: 959.5014\n",
      "Training Epoch: 15 [3000/36045]\tLoss: 957.0416\n",
      "Training Epoch: 15 [3050/36045]\tLoss: 992.9399\n",
      "Training Epoch: 15 [3100/36045]\tLoss: 905.6680\n",
      "Training Epoch: 15 [3150/36045]\tLoss: 698.0981\n",
      "Training Epoch: 15 [3200/36045]\tLoss: 726.4461\n",
      "Training Epoch: 15 [3250/36045]\tLoss: 683.2645\n",
      "Training Epoch: 15 [3300/36045]\tLoss: 646.1943\n",
      "Training Epoch: 15 [3350/36045]\tLoss: 681.5202\n",
      "Training Epoch: 15 [3400/36045]\tLoss: 716.7071\n",
      "Training Epoch: 15 [3450/36045]\tLoss: 772.0020\n",
      "Training Epoch: 15 [3500/36045]\tLoss: 752.0396\n",
      "Training Epoch: 15 [3550/36045]\tLoss: 725.3444\n",
      "Training Epoch: 15 [3600/36045]\tLoss: 774.6496\n",
      "Training Epoch: 15 [3650/36045]\tLoss: 899.0230\n",
      "Training Epoch: 15 [3700/36045]\tLoss: 903.6231\n",
      "Training Epoch: 15 [3750/36045]\tLoss: 867.9902\n",
      "Training Epoch: 15 [3800/36045]\tLoss: 855.8860\n",
      "Training Epoch: 15 [3850/36045]\tLoss: 857.8812\n",
      "Training Epoch: 15 [3900/36045]\tLoss: 866.3979\n",
      "Training Epoch: 15 [3950/36045]\tLoss: 831.5227\n",
      "Training Epoch: 15 [4000/36045]\tLoss: 847.7524\n",
      "Training Epoch: 15 [4050/36045]\tLoss: 779.8809\n",
      "Training Epoch: 15 [4100/36045]\tLoss: 757.8964\n",
      "Training Epoch: 15 [4150/36045]\tLoss: 782.4690\n",
      "Training Epoch: 15 [4200/36045]\tLoss: 774.2878\n",
      "Training Epoch: 15 [4250/36045]\tLoss: 776.3353\n",
      "Training Epoch: 15 [4300/36045]\tLoss: 798.9799\n",
      "Training Epoch: 15 [4350/36045]\tLoss: 778.2042\n",
      "Training Epoch: 15 [4400/36045]\tLoss: 743.8761\n",
      "Training Epoch: 15 [4450/36045]\tLoss: 805.0580\n",
      "Training Epoch: 15 [4500/36045]\tLoss: 856.5789\n",
      "Training Epoch: 15 [4550/36045]\tLoss: 868.3833\n",
      "Training Epoch: 15 [4600/36045]\tLoss: 892.9630\n",
      "Training Epoch: 15 [4650/36045]\tLoss: 884.7472\n",
      "Training Epoch: 15 [4700/36045]\tLoss: 816.9383\n",
      "Training Epoch: 15 [4750/36045]\tLoss: 800.4808\n",
      "Training Epoch: 15 [4800/36045]\tLoss: 832.1013\n",
      "Training Epoch: 15 [4850/36045]\tLoss: 815.4524\n",
      "Training Epoch: 15 [4900/36045]\tLoss: 790.7172\n",
      "Training Epoch: 15 [4950/36045]\tLoss: 820.1224\n",
      "Training Epoch: 15 [5000/36045]\tLoss: 861.8544\n",
      "Training Epoch: 15 [5050/36045]\tLoss: 833.1222\n",
      "Training Epoch: 15 [5100/36045]\tLoss: 845.0402\n",
      "Training Epoch: 15 [5150/36045]\tLoss: 825.0598\n",
      "Training Epoch: 15 [5200/36045]\tLoss: 821.1088\n",
      "Training Epoch: 15 [5250/36045]\tLoss: 811.1530\n",
      "Training Epoch: 15 [5300/36045]\tLoss: 812.8976\n",
      "Training Epoch: 15 [5350/36045]\tLoss: 842.5724\n",
      "Training Epoch: 15 [5400/36045]\tLoss: 805.8925\n",
      "Training Epoch: 15 [5450/36045]\tLoss: 759.9773\n",
      "Training Epoch: 15 [5500/36045]\tLoss: 800.9623\n",
      "Training Epoch: 15 [5550/36045]\tLoss: 781.5307\n",
      "Training Epoch: 15 [5600/36045]\tLoss: 887.0432\n",
      "Training Epoch: 15 [5650/36045]\tLoss: 843.1351\n",
      "Training Epoch: 15 [5700/36045]\tLoss: 796.2540\n",
      "Training Epoch: 15 [5750/36045]\tLoss: 780.3539\n",
      "Training Epoch: 15 [5800/36045]\tLoss: 828.9053\n",
      "Training Epoch: 15 [5850/36045]\tLoss: 803.4488\n",
      "Training Epoch: 15 [5900/36045]\tLoss: 923.3171\n",
      "Training Epoch: 15 [5950/36045]\tLoss: 950.0450\n",
      "Training Epoch: 15 [6000/36045]\tLoss: 931.5149\n",
      "Training Epoch: 15 [6050/36045]\tLoss: 898.1547\n",
      "Training Epoch: 15 [6100/36045]\tLoss: 904.9719\n",
      "Training Epoch: 15 [6150/36045]\tLoss: 873.4384\n",
      "Training Epoch: 15 [6200/36045]\tLoss: 870.4254\n",
      "Training Epoch: 15 [6250/36045]\tLoss: 887.6198\n",
      "Training Epoch: 15 [6300/36045]\tLoss: 905.9812\n",
      "Training Epoch: 15 [6350/36045]\tLoss: 959.7880\n",
      "Training Epoch: 15 [6400/36045]\tLoss: 814.8837\n",
      "Training Epoch: 15 [6450/36045]\tLoss: 755.8185\n",
      "Training Epoch: 15 [6500/36045]\tLoss: 774.7367\n",
      "Training Epoch: 15 [6550/36045]\tLoss: 789.9919\n",
      "Training Epoch: 15 [6600/36045]\tLoss: 794.7008\n",
      "Training Epoch: 15 [6650/36045]\tLoss: 899.8442\n",
      "Training Epoch: 15 [6700/36045]\tLoss: 942.2156\n",
      "Training Epoch: 15 [6750/36045]\tLoss: 911.3964\n",
      "Training Epoch: 15 [6800/36045]\tLoss: 915.6003\n",
      "Training Epoch: 15 [6850/36045]\tLoss: 899.9595\n",
      "Training Epoch: 15 [6900/36045]\tLoss: 794.6319\n",
      "Training Epoch: 15 [6950/36045]\tLoss: 748.4302\n",
      "Training Epoch: 15 [7000/36045]\tLoss: 796.3791\n",
      "Training Epoch: 15 [7050/36045]\tLoss: 815.2982\n",
      "Training Epoch: 15 [7100/36045]\tLoss: 812.7633\n",
      "Training Epoch: 15 [7150/36045]\tLoss: 830.1287\n",
      "Training Epoch: 15 [7200/36045]\tLoss: 835.8249\n",
      "Training Epoch: 15 [7250/36045]\tLoss: 833.4231\n",
      "Training Epoch: 15 [7300/36045]\tLoss: 818.4963\n",
      "Training Epoch: 15 [7350/36045]\tLoss: 810.4158\n",
      "Training Epoch: 15 [7400/36045]\tLoss: 724.0919\n",
      "Training Epoch: 15 [7450/36045]\tLoss: 729.3292\n",
      "Training Epoch: 15 [7500/36045]\tLoss: 725.8007\n",
      "Training Epoch: 15 [7550/36045]\tLoss: 692.9799\n",
      "Training Epoch: 15 [7600/36045]\tLoss: 781.3464\n",
      "Training Epoch: 15 [7650/36045]\tLoss: 845.0705\n",
      "Training Epoch: 15 [7700/36045]\tLoss: 806.8530\n",
      "Training Epoch: 15 [7750/36045]\tLoss: 821.4827\n",
      "Training Epoch: 15 [7800/36045]\tLoss: 806.6317\n",
      "Training Epoch: 15 [7850/36045]\tLoss: 769.8891\n",
      "Training Epoch: 15 [7900/36045]\tLoss: 813.2850\n",
      "Training Epoch: 15 [7950/36045]\tLoss: 810.1400\n",
      "Training Epoch: 15 [8000/36045]\tLoss: 829.7582\n",
      "Training Epoch: 15 [8050/36045]\tLoss: 786.3148\n",
      "Training Epoch: 15 [8100/36045]\tLoss: 816.0080\n",
      "Training Epoch: 15 [8150/36045]\tLoss: 924.3123\n",
      "Training Epoch: 15 [8200/36045]\tLoss: 910.1948\n",
      "Training Epoch: 15 [8250/36045]\tLoss: 873.6357\n",
      "Training Epoch: 15 [8300/36045]\tLoss: 946.7891\n",
      "Training Epoch: 15 [8350/36045]\tLoss: 874.0629\n",
      "Training Epoch: 15 [8400/36045]\tLoss: 785.4984\n",
      "Training Epoch: 15 [8450/36045]\tLoss: 736.8466\n",
      "Training Epoch: 15 [8500/36045]\tLoss: 780.1964\n",
      "Training Epoch: 15 [8550/36045]\tLoss: 766.4227\n",
      "Training Epoch: 15 [8600/36045]\tLoss: 758.8651\n",
      "Training Epoch: 15 [8650/36045]\tLoss: 810.3396\n",
      "Training Epoch: 15 [8700/36045]\tLoss: 856.9810\n",
      "Training Epoch: 15 [8750/36045]\tLoss: 838.1515\n",
      "Training Epoch: 15 [8800/36045]\tLoss: 844.1044\n",
      "Training Epoch: 15 [8850/36045]\tLoss: 835.9261\n",
      "Training Epoch: 15 [8900/36045]\tLoss: 754.2556\n",
      "Training Epoch: 15 [8950/36045]\tLoss: 775.0468\n",
      "Training Epoch: 15 [9000/36045]\tLoss: 787.4744\n",
      "Training Epoch: 15 [9050/36045]\tLoss: 785.8881\n",
      "Training Epoch: 15 [9100/36045]\tLoss: 809.1823\n",
      "Training Epoch: 15 [9150/36045]\tLoss: 596.9354\n",
      "Training Epoch: 15 [9200/36045]\tLoss: 453.7193\n",
      "Training Epoch: 15 [9250/36045]\tLoss: 490.7035\n",
      "Training Epoch: 15 [9300/36045]\tLoss: 507.6244\n",
      "Training Epoch: 15 [9350/36045]\tLoss: 464.3865\n",
      "Training Epoch: 15 [9400/36045]\tLoss: 912.2362\n",
      "Training Epoch: 15 [9450/36045]\tLoss: 968.3726\n",
      "Training Epoch: 15 [9500/36045]\tLoss: 954.0389\n",
      "Training Epoch: 15 [9550/36045]\tLoss: 1010.4606\n",
      "Training Epoch: 15 [9600/36045]\tLoss: 748.0154\n",
      "Training Epoch: 15 [9650/36045]\tLoss: 749.0348\n",
      "Training Epoch: 15 [9700/36045]\tLoss: 734.8815\n",
      "Training Epoch: 15 [9750/36045]\tLoss: 736.7031\n",
      "Training Epoch: 15 [9800/36045]\tLoss: 950.4514\n",
      "Training Epoch: 15 [9850/36045]\tLoss: 1003.6459\n",
      "Training Epoch: 15 [9900/36045]\tLoss: 1028.7468\n",
      "Training Epoch: 15 [9950/36045]\tLoss: 1000.8667\n",
      "Training Epoch: 15 [10000/36045]\tLoss: 922.9271\n",
      "Training Epoch: 15 [10050/36045]\tLoss: 773.1417\n",
      "Training Epoch: 15 [10100/36045]\tLoss: 775.1616\n",
      "Training Epoch: 15 [10150/36045]\tLoss: 787.9380\n",
      "Training Epoch: 15 [10200/36045]\tLoss: 777.8587\n",
      "Training Epoch: 15 [10250/36045]\tLoss: 928.7302\n",
      "Training Epoch: 15 [10300/36045]\tLoss: 900.5940\n",
      "Training Epoch: 15 [10350/36045]\tLoss: 948.4018\n",
      "Training Epoch: 15 [10400/36045]\tLoss: 939.0222\n",
      "Training Epoch: 15 [10450/36045]\tLoss: 874.4521\n",
      "Training Epoch: 15 [10500/36045]\tLoss: 736.5850\n",
      "Training Epoch: 15 [10550/36045]\tLoss: 733.0909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [10600/36045]\tLoss: 758.7670\n",
      "Training Epoch: 15 [10650/36045]\tLoss: 764.9085\n",
      "Training Epoch: 15 [10700/36045]\tLoss: 859.7667\n",
      "Training Epoch: 15 [10750/36045]\tLoss: 932.1841\n",
      "Training Epoch: 15 [10800/36045]\tLoss: 863.5450\n",
      "Training Epoch: 15 [10850/36045]\tLoss: 913.7770\n",
      "Training Epoch: 15 [10900/36045]\tLoss: 951.6177\n",
      "Training Epoch: 15 [10950/36045]\tLoss: 711.1323\n",
      "Training Epoch: 15 [11000/36045]\tLoss: 704.7709\n",
      "Training Epoch: 15 [11050/36045]\tLoss: 751.7899\n",
      "Training Epoch: 15 [11100/36045]\tLoss: 766.9464\n",
      "Training Epoch: 15 [11150/36045]\tLoss: 831.1849\n",
      "Training Epoch: 15 [11200/36045]\tLoss: 860.6425\n",
      "Training Epoch: 15 [11250/36045]\tLoss: 875.3290\n",
      "Training Epoch: 15 [11300/36045]\tLoss: 855.1910\n",
      "Training Epoch: 15 [11350/36045]\tLoss: 850.0787\n",
      "Training Epoch: 15 [11400/36045]\tLoss: 803.8506\n",
      "Training Epoch: 15 [11450/36045]\tLoss: 765.3582\n",
      "Training Epoch: 15 [11500/36045]\tLoss: 764.0830\n",
      "Training Epoch: 15 [11550/36045]\tLoss: 783.2543\n",
      "Training Epoch: 15 [11600/36045]\tLoss: 853.8105\n",
      "Training Epoch: 15 [11650/36045]\tLoss: 909.2894\n",
      "Training Epoch: 15 [11700/36045]\tLoss: 907.9565\n",
      "Training Epoch: 15 [11750/36045]\tLoss: 929.9062\n",
      "Training Epoch: 15 [11800/36045]\tLoss: 976.0353\n",
      "Training Epoch: 15 [11850/36045]\tLoss: 1022.7441\n",
      "Training Epoch: 15 [11900/36045]\tLoss: 1253.6719\n",
      "Training Epoch: 15 [11950/36045]\tLoss: 1250.4597\n",
      "Training Epoch: 15 [12000/36045]\tLoss: 1272.7743\n",
      "Training Epoch: 15 [12050/36045]\tLoss: 1227.4475\n",
      "Training Epoch: 15 [12100/36045]\tLoss: 834.3466\n",
      "Training Epoch: 15 [12150/36045]\tLoss: 661.5285\n",
      "Training Epoch: 15 [12200/36045]\tLoss: 655.1326\n",
      "Training Epoch: 15 [12250/36045]\tLoss: 666.3621\n",
      "Training Epoch: 15 [12300/36045]\tLoss: 832.5789\n",
      "Training Epoch: 15 [12350/36045]\tLoss: 895.1871\n",
      "Training Epoch: 15 [12400/36045]\tLoss: 907.8814\n",
      "Training Epoch: 15 [12450/36045]\tLoss: 893.6394\n",
      "Training Epoch: 15 [12500/36045]\tLoss: 927.9182\n",
      "Training Epoch: 15 [12550/36045]\tLoss: 890.7354\n",
      "Training Epoch: 15 [12600/36045]\tLoss: 828.7610\n",
      "Training Epoch: 15 [12650/36045]\tLoss: 824.8298\n",
      "Training Epoch: 15 [12700/36045]\tLoss: 850.5061\n",
      "Training Epoch: 15 [12750/36045]\tLoss: 849.4217\n",
      "Training Epoch: 15 [12800/36045]\tLoss: 828.7731\n",
      "Training Epoch: 15 [12850/36045]\tLoss: 864.6808\n",
      "Training Epoch: 15 [12900/36045]\tLoss: 828.2563\n",
      "Training Epoch: 15 [12950/36045]\tLoss: 818.8823\n",
      "Training Epoch: 15 [13000/36045]\tLoss: 851.1273\n",
      "Training Epoch: 15 [13050/36045]\tLoss: 780.7808\n",
      "Training Epoch: 15 [13100/36045]\tLoss: 815.2581\n",
      "Training Epoch: 15 [13150/36045]\tLoss: 808.6719\n",
      "Training Epoch: 15 [13200/36045]\tLoss: 772.3146\n",
      "Training Epoch: 15 [13250/36045]\tLoss: 811.2147\n",
      "Training Epoch: 15 [13300/36045]\tLoss: 852.1147\n",
      "Training Epoch: 15 [13350/36045]\tLoss: 827.3924\n",
      "Training Epoch: 15 [13400/36045]\tLoss: 833.8866\n",
      "Training Epoch: 15 [13450/36045]\tLoss: 823.5537\n",
      "Training Epoch: 15 [13500/36045]\tLoss: 854.6024\n",
      "Training Epoch: 15 [13550/36045]\tLoss: 989.8594\n",
      "Training Epoch: 15 [13600/36045]\tLoss: 1022.2310\n",
      "Training Epoch: 15 [13650/36045]\tLoss: 1102.9845\n",
      "Training Epoch: 15 [13700/36045]\tLoss: 983.1272\n",
      "Training Epoch: 15 [13750/36045]\tLoss: 840.4850\n",
      "Training Epoch: 15 [13800/36045]\tLoss: 815.7492\n",
      "Training Epoch: 15 [13850/36045]\tLoss: 797.7705\n",
      "Training Epoch: 15 [13900/36045]\tLoss: 807.3514\n",
      "Training Epoch: 15 [13950/36045]\tLoss: 850.3926\n",
      "Training Epoch: 15 [14000/36045]\tLoss: 889.0814\n",
      "Training Epoch: 15 [14050/36045]\tLoss: 854.8149\n",
      "Training Epoch: 15 [14100/36045]\tLoss: 850.9523\n",
      "Training Epoch: 15 [14150/36045]\tLoss: 836.9698\n",
      "Training Epoch: 15 [14200/36045]\tLoss: 891.3946\n",
      "Training Epoch: 15 [14250/36045]\tLoss: 973.6030\n",
      "Training Epoch: 15 [14300/36045]\tLoss: 978.4081\n",
      "Training Epoch: 15 [14350/36045]\tLoss: 936.1722\n",
      "Training Epoch: 15 [14400/36045]\tLoss: 921.0489\n",
      "Training Epoch: 15 [14450/36045]\tLoss: 964.2423\n",
      "Training Epoch: 15 [14500/36045]\tLoss: 892.1211\n",
      "Training Epoch: 15 [14550/36045]\tLoss: 929.9822\n",
      "Training Epoch: 15 [14600/36045]\tLoss: 907.2886\n",
      "Training Epoch: 15 [14650/36045]\tLoss: 910.3663\n",
      "Training Epoch: 15 [14700/36045]\tLoss: 855.2219\n",
      "Training Epoch: 15 [14750/36045]\tLoss: 733.9019\n",
      "Training Epoch: 15 [14800/36045]\tLoss: 722.8226\n",
      "Training Epoch: 15 [14850/36045]\tLoss: 730.1819\n",
      "Training Epoch: 15 [14900/36045]\tLoss: 724.5121\n",
      "Training Epoch: 15 [14950/36045]\tLoss: 732.5090\n",
      "Training Epoch: 15 [15000/36045]\tLoss: 753.4345\n",
      "Training Epoch: 15 [15050/36045]\tLoss: 754.1591\n",
      "Training Epoch: 15 [15100/36045]\tLoss: 735.8896\n",
      "Training Epoch: 15 [15150/36045]\tLoss: 726.4754\n",
      "Training Epoch: 15 [15200/36045]\tLoss: 669.8806\n",
      "Training Epoch: 15 [15250/36045]\tLoss: 700.4760\n",
      "Training Epoch: 15 [15300/36045]\tLoss: 681.5469\n",
      "Training Epoch: 15 [15350/36045]\tLoss: 697.1631\n",
      "Training Epoch: 15 [15400/36045]\tLoss: 681.4966\n",
      "Training Epoch: 15 [15450/36045]\tLoss: 670.7521\n",
      "Training Epoch: 15 [15500/36045]\tLoss: 691.3498\n",
      "Training Epoch: 15 [15550/36045]\tLoss: 681.4636\n",
      "Training Epoch: 15 [15600/36045]\tLoss: 759.9261\n",
      "Training Epoch: 15 [15650/36045]\tLoss: 783.4999\n",
      "Training Epoch: 15 [15700/36045]\tLoss: 770.4708\n",
      "Training Epoch: 15 [15750/36045]\tLoss: 761.0331\n",
      "Training Epoch: 15 [15800/36045]\tLoss: 705.0469\n",
      "Training Epoch: 15 [15850/36045]\tLoss: 715.3392\n",
      "Training Epoch: 15 [15900/36045]\tLoss: 726.1450\n",
      "Training Epoch: 15 [15950/36045]\tLoss: 748.1124\n",
      "Training Epoch: 15 [16000/36045]\tLoss: 729.6825\n",
      "Training Epoch: 15 [16050/36045]\tLoss: 698.5069\n",
      "Training Epoch: 15 [16100/36045]\tLoss: 644.8843\n",
      "Training Epoch: 15 [16150/36045]\tLoss: 628.8690\n",
      "Training Epoch: 15 [16200/36045]\tLoss: 757.9451\n",
      "Training Epoch: 15 [16250/36045]\tLoss: 789.2405\n",
      "Training Epoch: 15 [16300/36045]\tLoss: 860.9697\n",
      "Training Epoch: 15 [16350/36045]\tLoss: 876.4096\n",
      "Training Epoch: 15 [16400/36045]\tLoss: 851.4340\n",
      "Training Epoch: 15 [16450/36045]\tLoss: 829.0114\n",
      "Training Epoch: 15 [16500/36045]\tLoss: 826.4446\n",
      "Training Epoch: 15 [16550/36045]\tLoss: 785.7720\n",
      "Training Epoch: 15 [16600/36045]\tLoss: 820.9185\n",
      "Training Epoch: 15 [16650/36045]\tLoss: 845.2604\n",
      "Training Epoch: 15 [16700/36045]\tLoss: 817.5696\n",
      "Training Epoch: 15 [16750/36045]\tLoss: 808.1342\n",
      "Training Epoch: 15 [16800/36045]\tLoss: 823.3209\n",
      "Training Epoch: 15 [16850/36045]\tLoss: 782.5046\n",
      "Training Epoch: 15 [16900/36045]\tLoss: 797.6794\n",
      "Training Epoch: 15 [16950/36045]\tLoss: 826.0870\n",
      "Training Epoch: 15 [17000/36045]\tLoss: 804.4808\n",
      "Training Epoch: 15 [17050/36045]\tLoss: 843.3974\n",
      "Training Epoch: 15 [17100/36045]\tLoss: 845.6411\n",
      "Training Epoch: 15 [17150/36045]\tLoss: 732.8768\n",
      "Training Epoch: 15 [17200/36045]\tLoss: 685.7878\n",
      "Training Epoch: 15 [17250/36045]\tLoss: 717.7038\n",
      "Training Epoch: 15 [17300/36045]\tLoss: 756.6259\n",
      "Training Epoch: 15 [17350/36045]\tLoss: 723.0236\n",
      "Training Epoch: 15 [17400/36045]\tLoss: 742.7255\n",
      "Training Epoch: 15 [17450/36045]\tLoss: 766.1970\n",
      "Training Epoch: 15 [17500/36045]\tLoss: 752.8699\n",
      "Training Epoch: 15 [17550/36045]\tLoss: 758.7729\n",
      "Training Epoch: 15 [17600/36045]\tLoss: 744.2449\n",
      "Training Epoch: 15 [17650/36045]\tLoss: 764.1072\n",
      "Training Epoch: 15 [17700/36045]\tLoss: 742.2327\n",
      "Training Epoch: 15 [17750/36045]\tLoss: 760.8872\n",
      "Training Epoch: 15 [17800/36045]\tLoss: 752.6404\n",
      "Training Epoch: 15 [17850/36045]\tLoss: 739.8287\n",
      "Training Epoch: 15 [17900/36045]\tLoss: 771.0554\n",
      "Training Epoch: 15 [17950/36045]\tLoss: 780.5602\n",
      "Training Epoch: 15 [18000/36045]\tLoss: 770.6519\n",
      "Training Epoch: 15 [18050/36045]\tLoss: 874.7657\n",
      "Training Epoch: 15 [18100/36045]\tLoss: 879.7239\n",
      "Training Epoch: 15 [18150/36045]\tLoss: 886.5105\n",
      "Training Epoch: 15 [18200/36045]\tLoss: 873.2399\n",
      "Training Epoch: 15 [18250/36045]\tLoss: 893.1366\n",
      "Training Epoch: 15 [18300/36045]\tLoss: 819.1800\n",
      "Training Epoch: 15 [18350/36045]\tLoss: 884.8144\n",
      "Training Epoch: 15 [18400/36045]\tLoss: 856.6540\n",
      "Training Epoch: 15 [18450/36045]\tLoss: 837.1457\n",
      "Training Epoch: 15 [18500/36045]\tLoss: 839.1619\n",
      "Training Epoch: 15 [18550/36045]\tLoss: 823.8549\n",
      "Training Epoch: 15 [18600/36045]\tLoss: 813.6678\n",
      "Training Epoch: 15 [18650/36045]\tLoss: 867.4628\n",
      "Training Epoch: 15 [18700/36045]\tLoss: 913.5761\n",
      "Training Epoch: 15 [18750/36045]\tLoss: 894.7061\n",
      "Training Epoch: 15 [18800/36045]\tLoss: 921.8248\n",
      "Training Epoch: 15 [18850/36045]\tLoss: 864.1335\n",
      "Training Epoch: 15 [18900/36045]\tLoss: 926.3505\n",
      "Training Epoch: 15 [18950/36045]\tLoss: 858.7119\n",
      "Training Epoch: 15 [19000/36045]\tLoss: 737.9581\n",
      "Training Epoch: 15 [19050/36045]\tLoss: 712.4056\n",
      "Training Epoch: 15 [19100/36045]\tLoss: 727.3923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [19150/36045]\tLoss: 714.5900\n",
      "Training Epoch: 15 [19200/36045]\tLoss: 743.2632\n",
      "Training Epoch: 15 [19250/36045]\tLoss: 756.4792\n",
      "Training Epoch: 15 [19300/36045]\tLoss: 773.6022\n",
      "Training Epoch: 15 [19350/36045]\tLoss: 755.4686\n",
      "Training Epoch: 15 [19400/36045]\tLoss: 779.9850\n",
      "Training Epoch: 15 [19450/36045]\tLoss: 767.8897\n",
      "Training Epoch: 15 [19500/36045]\tLoss: 771.4897\n",
      "Training Epoch: 15 [19550/36045]\tLoss: 771.0750\n",
      "Training Epoch: 15 [19600/36045]\tLoss: 814.4772\n",
      "Training Epoch: 15 [19650/36045]\tLoss: 1056.1287\n",
      "Training Epoch: 15 [19700/36045]\tLoss: 1012.3371\n",
      "Training Epoch: 15 [19750/36045]\tLoss: 1011.9689\n",
      "Training Epoch: 15 [19800/36045]\tLoss: 1004.9971\n",
      "Training Epoch: 15 [19850/36045]\tLoss: 689.6205\n",
      "Training Epoch: 15 [19900/36045]\tLoss: 663.3057\n",
      "Training Epoch: 15 [19950/36045]\tLoss: 668.6483\n",
      "Training Epoch: 15 [20000/36045]\tLoss: 667.5739\n",
      "Training Epoch: 15 [20050/36045]\tLoss: 747.9095\n",
      "Training Epoch: 15 [20100/36045]\tLoss: 751.7869\n",
      "Training Epoch: 15 [20150/36045]\tLoss: 755.0415\n",
      "Training Epoch: 15 [20200/36045]\tLoss: 754.0006\n",
      "Training Epoch: 15 [20250/36045]\tLoss: 802.0085\n",
      "Training Epoch: 15 [20300/36045]\tLoss: 842.0505\n",
      "Training Epoch: 15 [20350/36045]\tLoss: 867.8679\n",
      "Training Epoch: 15 [20400/36045]\tLoss: 882.6844\n",
      "Training Epoch: 15 [20450/36045]\tLoss: 857.5687\n",
      "Training Epoch: 15 [20500/36045]\tLoss: 834.7049\n",
      "Training Epoch: 15 [20550/36045]\tLoss: 739.2113\n",
      "Training Epoch: 15 [20600/36045]\tLoss: 754.1945\n",
      "Training Epoch: 15 [20650/36045]\tLoss: 750.5043\n",
      "Training Epoch: 15 [20700/36045]\tLoss: 734.6404\n",
      "Training Epoch: 15 [20750/36045]\tLoss: 785.5115\n",
      "Training Epoch: 15 [20800/36045]\tLoss: 855.6812\n",
      "Training Epoch: 15 [20850/36045]\tLoss: 845.8509\n",
      "Training Epoch: 15 [20900/36045]\tLoss: 899.3853\n",
      "Training Epoch: 15 [20950/36045]\tLoss: 848.0229\n",
      "Training Epoch: 15 [21000/36045]\tLoss: 799.8253\n",
      "Training Epoch: 15 [21050/36045]\tLoss: 684.0729\n",
      "Training Epoch: 15 [21100/36045]\tLoss: 684.1425\n",
      "Training Epoch: 15 [21150/36045]\tLoss: 733.0333\n",
      "Training Epoch: 15 [21200/36045]\tLoss: 733.2314\n",
      "Training Epoch: 15 [21250/36045]\tLoss: 700.1292\n",
      "Training Epoch: 15 [21300/36045]\tLoss: 819.4086\n",
      "Training Epoch: 15 [21350/36045]\tLoss: 816.5056\n",
      "Training Epoch: 15 [21400/36045]\tLoss: 820.0885\n",
      "Training Epoch: 15 [21450/36045]\tLoss: 830.4636\n",
      "Training Epoch: 15 [21500/36045]\tLoss: 831.5759\n",
      "Training Epoch: 15 [21550/36045]\tLoss: 933.5381\n",
      "Training Epoch: 15 [21600/36045]\tLoss: 934.9260\n",
      "Training Epoch: 15 [21650/36045]\tLoss: 951.7740\n",
      "Training Epoch: 15 [21700/36045]\tLoss: 947.7831\n",
      "Training Epoch: 15 [21750/36045]\tLoss: 915.9191\n",
      "Training Epoch: 15 [21800/36045]\tLoss: 682.4326\n",
      "Training Epoch: 15 [21850/36045]\tLoss: 661.4311\n",
      "Training Epoch: 15 [21900/36045]\tLoss: 676.7235\n",
      "Training Epoch: 15 [21950/36045]\tLoss: 671.6521\n",
      "Training Epoch: 15 [22000/36045]\tLoss: 680.0997\n",
      "Training Epoch: 15 [22050/36045]\tLoss: 718.3790\n",
      "Training Epoch: 15 [22100/36045]\tLoss: 708.5931\n",
      "Training Epoch: 15 [22150/36045]\tLoss: 685.6957\n",
      "Training Epoch: 15 [22200/36045]\tLoss: 707.6970\n",
      "Training Epoch: 15 [22250/36045]\tLoss: 714.2031\n",
      "Training Epoch: 15 [22300/36045]\tLoss: 771.9453\n",
      "Training Epoch: 15 [22350/36045]\tLoss: 801.8949\n",
      "Training Epoch: 15 [22400/36045]\tLoss: 823.2507\n",
      "Training Epoch: 15 [22450/36045]\tLoss: 808.1316\n",
      "Training Epoch: 15 [22500/36045]\tLoss: 784.4421\n",
      "Training Epoch: 15 [22550/36045]\tLoss: 833.6639\n",
      "Training Epoch: 15 [22600/36045]\tLoss: 915.5834\n",
      "Training Epoch: 15 [22650/36045]\tLoss: 959.3090\n",
      "Training Epoch: 15 [22700/36045]\tLoss: 985.2119\n",
      "Training Epoch: 15 [22750/36045]\tLoss: 1008.0265\n",
      "Training Epoch: 15 [22800/36045]\tLoss: 1050.8447\n",
      "Training Epoch: 15 [22850/36045]\tLoss: 867.7341\n",
      "Training Epoch: 15 [22900/36045]\tLoss: 871.9373\n",
      "Training Epoch: 15 [22950/36045]\tLoss: 845.1623\n",
      "Training Epoch: 15 [23000/36045]\tLoss: 848.5764\n",
      "Training Epoch: 15 [23050/36045]\tLoss: 762.3211\n",
      "Training Epoch: 15 [23100/36045]\tLoss: 779.4795\n",
      "Training Epoch: 15 [23150/36045]\tLoss: 764.7785\n",
      "Training Epoch: 15 [23200/36045]\tLoss: 721.6238\n",
      "Training Epoch: 15 [23250/36045]\tLoss: 726.7247\n",
      "Training Epoch: 15 [23300/36045]\tLoss: 722.1329\n",
      "Training Epoch: 15 [23350/36045]\tLoss: 744.9381\n",
      "Training Epoch: 15 [23400/36045]\tLoss: 809.6671\n",
      "Training Epoch: 15 [23450/36045]\tLoss: 799.6531\n",
      "Training Epoch: 15 [23500/36045]\tLoss: 768.6126\n",
      "Training Epoch: 15 [23550/36045]\tLoss: 828.7004\n",
      "Training Epoch: 15 [23600/36045]\tLoss: 932.2650\n",
      "Training Epoch: 15 [23650/36045]\tLoss: 949.7299\n",
      "Training Epoch: 15 [23700/36045]\tLoss: 962.1931\n",
      "Training Epoch: 15 [23750/36045]\tLoss: 930.4076\n",
      "Training Epoch: 15 [23800/36045]\tLoss: 732.3193\n",
      "Training Epoch: 15 [23850/36045]\tLoss: 762.7845\n",
      "Training Epoch: 15 [23900/36045]\tLoss: 754.3113\n",
      "Training Epoch: 15 [23950/36045]\tLoss: 733.7427\n",
      "Training Epoch: 15 [24000/36045]\tLoss: 709.7368\n",
      "Training Epoch: 15 [24050/36045]\tLoss: 655.3149\n",
      "Training Epoch: 15 [24100/36045]\tLoss: 690.0821\n",
      "Training Epoch: 15 [24150/36045]\tLoss: 686.9881\n",
      "Training Epoch: 15 [24200/36045]\tLoss: 675.8939\n",
      "Training Epoch: 15 [24250/36045]\tLoss: 657.8491\n",
      "Training Epoch: 15 [24300/36045]\tLoss: 709.1564\n",
      "Training Epoch: 15 [24350/36045]\tLoss: 727.8176\n",
      "Training Epoch: 15 [24400/36045]\tLoss: 748.6585\n",
      "Training Epoch: 15 [24450/36045]\tLoss: 714.4864\n",
      "Training Epoch: 15 [24500/36045]\tLoss: 753.2375\n",
      "Training Epoch: 15 [24550/36045]\tLoss: 847.7930\n",
      "Training Epoch: 15 [24600/36045]\tLoss: 842.1345\n",
      "Training Epoch: 15 [24650/36045]\tLoss: 810.0419\n",
      "Training Epoch: 15 [24700/36045]\tLoss: 823.1447\n",
      "Training Epoch: 15 [24750/36045]\tLoss: 760.4732\n",
      "Training Epoch: 15 [24800/36045]\tLoss: 647.3150\n",
      "Training Epoch: 15 [24850/36045]\tLoss: 668.2371\n",
      "Training Epoch: 15 [24900/36045]\tLoss: 665.9675\n",
      "Training Epoch: 15 [24950/36045]\tLoss: 667.8046\n",
      "Training Epoch: 15 [25000/36045]\tLoss: 638.7163\n",
      "Training Epoch: 15 [25050/36045]\tLoss: 608.7979\n",
      "Training Epoch: 15 [25100/36045]\tLoss: 547.0317\n",
      "Training Epoch: 15 [25150/36045]\tLoss: 507.6316\n",
      "Training Epoch: 15 [25200/36045]\tLoss: 504.0349\n",
      "Training Epoch: 15 [25250/36045]\tLoss: 537.3850\n",
      "Training Epoch: 15 [25300/36045]\tLoss: 702.7846\n",
      "Training Epoch: 15 [25350/36045]\tLoss: 701.2943\n",
      "Training Epoch: 15 [25400/36045]\tLoss: 654.1973\n",
      "Training Epoch: 15 [25450/36045]\tLoss: 655.7648\n",
      "Training Epoch: 15 [25500/36045]\tLoss: 715.2101\n",
      "Training Epoch: 15 [25550/36045]\tLoss: 829.5541\n",
      "Training Epoch: 15 [25600/36045]\tLoss: 835.1795\n",
      "Training Epoch: 15 [25650/36045]\tLoss: 803.5648\n",
      "Training Epoch: 15 [25700/36045]\tLoss: 814.6380\n",
      "Training Epoch: 15 [25750/36045]\tLoss: 784.5828\n",
      "Training Epoch: 15 [25800/36045]\tLoss: 492.6053\n",
      "Training Epoch: 15 [25850/36045]\tLoss: 503.9945\n",
      "Training Epoch: 15 [25900/36045]\tLoss: 483.1292\n",
      "Training Epoch: 15 [25950/36045]\tLoss: 492.9071\n",
      "Training Epoch: 15 [26000/36045]\tLoss: 604.4938\n",
      "Training Epoch: 15 [26050/36045]\tLoss: 819.0998\n",
      "Training Epoch: 15 [26100/36045]\tLoss: 851.0556\n",
      "Training Epoch: 15 [26150/36045]\tLoss: 847.0789\n",
      "Training Epoch: 15 [26200/36045]\tLoss: 820.7198\n",
      "Training Epoch: 15 [26250/36045]\tLoss: 854.4043\n",
      "Training Epoch: 15 [26300/36045]\tLoss: 750.4898\n",
      "Training Epoch: 15 [26350/36045]\tLoss: 757.5696\n",
      "Training Epoch: 15 [26400/36045]\tLoss: 737.4912\n",
      "Training Epoch: 15 [26450/36045]\tLoss: 665.0412\n",
      "Training Epoch: 15 [26500/36045]\tLoss: 800.4205\n",
      "Training Epoch: 15 [26550/36045]\tLoss: 811.8565\n",
      "Training Epoch: 15 [26600/36045]\tLoss: 805.1874\n",
      "Training Epoch: 15 [26650/36045]\tLoss: 822.8174\n",
      "Training Epoch: 15 [26700/36045]\tLoss: 802.1251\n",
      "Training Epoch: 15 [26750/36045]\tLoss: 749.1053\n",
      "Training Epoch: 15 [26800/36045]\tLoss: 549.1881\n",
      "Training Epoch: 15 [26850/36045]\tLoss: 459.1027\n",
      "Training Epoch: 15 [26900/36045]\tLoss: 462.4998\n",
      "Training Epoch: 15 [26950/36045]\tLoss: 507.7599\n",
      "Training Epoch: 15 [27000/36045]\tLoss: 808.0926\n",
      "Training Epoch: 15 [27050/36045]\tLoss: 852.0733\n",
      "Training Epoch: 15 [27100/36045]\tLoss: 825.1522\n",
      "Training Epoch: 15 [27150/36045]\tLoss: 871.3521\n",
      "Training Epoch: 15 [27200/36045]\tLoss: 646.0959\n",
      "Training Epoch: 15 [27250/36045]\tLoss: 643.5954\n",
      "Training Epoch: 15 [27300/36045]\tLoss: 623.9593\n",
      "Training Epoch: 15 [27350/36045]\tLoss: 626.2546\n",
      "Training Epoch: 15 [27400/36045]\tLoss: 624.3983\n",
      "Training Epoch: 15 [27450/36045]\tLoss: 786.2310\n",
      "Training Epoch: 15 [27500/36045]\tLoss: 844.7475\n",
      "Training Epoch: 15 [27550/36045]\tLoss: 837.2436\n",
      "Training Epoch: 15 [27600/36045]\tLoss: 845.0321\n",
      "Training Epoch: 15 [27650/36045]\tLoss: 839.8928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [27700/36045]\tLoss: 869.6612\n",
      "Training Epoch: 15 [27750/36045]\tLoss: 883.8075\n",
      "Training Epoch: 15 [27800/36045]\tLoss: 867.5978\n",
      "Training Epoch: 15 [27850/36045]\tLoss: 851.2772\n",
      "Training Epoch: 15 [27900/36045]\tLoss: 760.5610\n",
      "Training Epoch: 15 [27950/36045]\tLoss: 628.7503\n",
      "Training Epoch: 15 [28000/36045]\tLoss: 599.7841\n",
      "Training Epoch: 15 [28050/36045]\tLoss: 617.1566\n",
      "Training Epoch: 15 [28100/36045]\tLoss: 606.3018\n",
      "Training Epoch: 15 [28150/36045]\tLoss: 644.8851\n",
      "Training Epoch: 15 [28200/36045]\tLoss: 646.9465\n",
      "Training Epoch: 15 [28250/36045]\tLoss: 647.8954\n",
      "Training Epoch: 15 [28300/36045]\tLoss: 610.8951\n",
      "Training Epoch: 15 [28350/36045]\tLoss: 607.5120\n",
      "Training Epoch: 15 [28400/36045]\tLoss: 950.0890\n",
      "Training Epoch: 15 [28450/36045]\tLoss: 855.2562\n",
      "Training Epoch: 15 [28500/36045]\tLoss: 742.7269\n",
      "Training Epoch: 15 [28550/36045]\tLoss: 681.2686\n",
      "Training Epoch: 15 [28600/36045]\tLoss: 746.2189\n",
      "Training Epoch: 15 [28650/36045]\tLoss: 863.2904\n",
      "Training Epoch: 15 [28700/36045]\tLoss: 855.7198\n",
      "Training Epoch: 15 [28750/36045]\tLoss: 844.7009\n",
      "Training Epoch: 15 [28800/36045]\tLoss: 854.4016\n",
      "Training Epoch: 15 [28850/36045]\tLoss: 738.1697\n",
      "Training Epoch: 15 [28900/36045]\tLoss: 587.9011\n",
      "Training Epoch: 15 [28950/36045]\tLoss: 581.9068\n",
      "Training Epoch: 15 [29000/36045]\tLoss: 584.0865\n",
      "Training Epoch: 15 [29050/36045]\tLoss: 593.9169\n",
      "Training Epoch: 15 [29100/36045]\tLoss: 615.7911\n",
      "Training Epoch: 15 [29150/36045]\tLoss: 600.2793\n",
      "Training Epoch: 15 [29200/36045]\tLoss: 587.0242\n",
      "Training Epoch: 15 [29250/36045]\tLoss: 569.9116\n",
      "Training Epoch: 15 [29300/36045]\tLoss: 660.2100\n",
      "Training Epoch: 15 [29350/36045]\tLoss: 790.1996\n",
      "Training Epoch: 15 [29400/36045]\tLoss: 809.8943\n",
      "Training Epoch: 15 [29450/36045]\tLoss: 841.6817\n",
      "Training Epoch: 15 [29500/36045]\tLoss: 857.5359\n",
      "Training Epoch: 15 [29550/36045]\tLoss: 812.9410\n",
      "Training Epoch: 15 [29600/36045]\tLoss: 692.9164\n",
      "Training Epoch: 15 [29650/36045]\tLoss: 675.4875\n",
      "Training Epoch: 15 [29700/36045]\tLoss: 597.7968\n",
      "Training Epoch: 15 [29750/36045]\tLoss: 602.8712\n",
      "Training Epoch: 15 [29800/36045]\tLoss: 650.6141\n",
      "Training Epoch: 15 [29850/36045]\tLoss: 721.6132\n",
      "Training Epoch: 15 [29900/36045]\tLoss: 715.7809\n",
      "Training Epoch: 15 [29950/36045]\tLoss: 737.9572\n",
      "Training Epoch: 15 [30000/36045]\tLoss: 719.8578\n",
      "Training Epoch: 15 [30050/36045]\tLoss: 725.6584\n",
      "Training Epoch: 15 [30100/36045]\tLoss: 885.4592\n",
      "Training Epoch: 15 [30150/36045]\tLoss: 873.1348\n",
      "Training Epoch: 15 [30200/36045]\tLoss: 824.2973\n",
      "Training Epoch: 15 [30250/36045]\tLoss: 875.1253\n",
      "Training Epoch: 15 [30300/36045]\tLoss: 863.4070\n",
      "Training Epoch: 15 [30350/36045]\tLoss: 692.9573\n",
      "Training Epoch: 15 [30400/36045]\tLoss: 680.8402\n",
      "Training Epoch: 15 [30450/36045]\tLoss: 679.4542\n",
      "Training Epoch: 15 [30500/36045]\tLoss: 632.5001\n",
      "Training Epoch: 15 [30550/36045]\tLoss: 587.1706\n",
      "Training Epoch: 15 [30600/36045]\tLoss: 563.8127\n",
      "Training Epoch: 15 [30650/36045]\tLoss: 555.1619\n",
      "Training Epoch: 15 [30700/36045]\tLoss: 575.8207\n",
      "Training Epoch: 15 [30750/36045]\tLoss: 558.7921\n",
      "Training Epoch: 15 [30800/36045]\tLoss: 596.3472\n",
      "Training Epoch: 15 [30850/36045]\tLoss: 588.0502\n",
      "Training Epoch: 15 [30900/36045]\tLoss: 604.3419\n",
      "Training Epoch: 15 [30950/36045]\tLoss: 636.2416\n",
      "Training Epoch: 15 [31000/36045]\tLoss: 624.4543\n",
      "Training Epoch: 15 [31050/36045]\tLoss: 519.6641\n",
      "Training Epoch: 15 [31100/36045]\tLoss: 508.3969\n",
      "Training Epoch: 15 [31150/36045]\tLoss: 513.4802\n",
      "Training Epoch: 15 [31200/36045]\tLoss: 647.2515\n",
      "Training Epoch: 15 [31250/36045]\tLoss: 840.8579\n",
      "Training Epoch: 15 [31300/36045]\tLoss: 805.9952\n",
      "Training Epoch: 15 [31350/36045]\tLoss: 822.1328\n",
      "Training Epoch: 15 [31400/36045]\tLoss: 804.0035\n",
      "Training Epoch: 15 [31450/36045]\tLoss: 806.3154\n",
      "Training Epoch: 15 [31500/36045]\tLoss: 812.9507\n",
      "Training Epoch: 15 [31550/36045]\tLoss: 824.3267\n",
      "Training Epoch: 15 [31600/36045]\tLoss: 774.0770\n",
      "Training Epoch: 15 [31650/36045]\tLoss: 825.8403\n",
      "Training Epoch: 15 [31700/36045]\tLoss: 610.7214\n",
      "Training Epoch: 15 [31750/36045]\tLoss: 510.2990\n",
      "Training Epoch: 15 [31800/36045]\tLoss: 485.6404\n",
      "Training Epoch: 15 [31850/36045]\tLoss: 498.9821\n",
      "Training Epoch: 15 [31900/36045]\tLoss: 763.5892\n",
      "Training Epoch: 15 [31950/36045]\tLoss: 967.3334\n",
      "Training Epoch: 15 [32000/36045]\tLoss: 1093.9583\n",
      "Training Epoch: 15 [32050/36045]\tLoss: 1043.7073\n",
      "Training Epoch: 15 [32100/36045]\tLoss: 1029.7587\n",
      "Training Epoch: 15 [32150/36045]\tLoss: 822.7061\n",
      "Training Epoch: 15 [32200/36045]\tLoss: 828.9151\n",
      "Training Epoch: 15 [32250/36045]\tLoss: 841.1501\n",
      "Training Epoch: 15 [32300/36045]\tLoss: 823.2272\n",
      "Training Epoch: 15 [32350/36045]\tLoss: 813.7255\n",
      "Training Epoch: 15 [32400/36045]\tLoss: 766.2412\n",
      "Training Epoch: 15 [32450/36045]\tLoss: 634.1806\n",
      "Training Epoch: 15 [32500/36045]\tLoss: 609.3843\n",
      "Training Epoch: 15 [32550/36045]\tLoss: 614.1110\n",
      "Training Epoch: 15 [32600/36045]\tLoss: 608.6301\n",
      "Training Epoch: 15 [32650/36045]\tLoss: 758.7615\n",
      "Training Epoch: 15 [32700/36045]\tLoss: 824.0830\n",
      "Training Epoch: 15 [32750/36045]\tLoss: 784.5357\n",
      "Training Epoch: 15 [32800/36045]\tLoss: 804.4492\n",
      "Training Epoch: 15 [32850/36045]\tLoss: 747.8194\n",
      "Training Epoch: 15 [32900/36045]\tLoss: 609.4094\n",
      "Training Epoch: 15 [32950/36045]\tLoss: 638.2208\n",
      "Training Epoch: 15 [33000/36045]\tLoss: 638.9940\n",
      "Training Epoch: 15 [33050/36045]\tLoss: 604.3528\n",
      "Training Epoch: 15 [33100/36045]\tLoss: 690.1240\n",
      "Training Epoch: 15 [33150/36045]\tLoss: 928.3555\n",
      "Training Epoch: 15 [33200/36045]\tLoss: 905.8994\n",
      "Training Epoch: 15 [33250/36045]\tLoss: 930.8283\n",
      "Training Epoch: 15 [33300/36045]\tLoss: 991.0199\n",
      "Training Epoch: 15 [33350/36045]\tLoss: 761.4997\n",
      "Training Epoch: 15 [33400/36045]\tLoss: 569.3511\n",
      "Training Epoch: 15 [33450/36045]\tLoss: 564.4177\n",
      "Training Epoch: 15 [33500/36045]\tLoss: 579.5481\n",
      "Training Epoch: 15 [33550/36045]\tLoss: 601.0708\n",
      "Training Epoch: 15 [33600/36045]\tLoss: 603.5723\n",
      "Training Epoch: 15 [33650/36045]\tLoss: 794.4026\n",
      "Training Epoch: 15 [33700/36045]\tLoss: 770.5510\n",
      "Training Epoch: 15 [33750/36045]\tLoss: 796.5551\n",
      "Training Epoch: 15 [33800/36045]\tLoss: 792.2791\n",
      "Training Epoch: 15 [33850/36045]\tLoss: 797.0620\n",
      "Training Epoch: 15 [33900/36045]\tLoss: 804.2862\n",
      "Training Epoch: 15 [33950/36045]\tLoss: 816.6638\n",
      "Training Epoch: 15 [34000/36045]\tLoss: 807.4971\n",
      "Training Epoch: 15 [34050/36045]\tLoss: 812.8503\n",
      "Training Epoch: 15 [34100/36045]\tLoss: 778.9632\n",
      "Training Epoch: 15 [34150/36045]\tLoss: 724.1996\n",
      "Training Epoch: 15 [34200/36045]\tLoss: 688.5540\n",
      "Training Epoch: 15 [34250/36045]\tLoss: 704.0107\n",
      "Training Epoch: 15 [34300/36045]\tLoss: 605.2804\n",
      "Training Epoch: 15 [34350/36045]\tLoss: 634.4684\n",
      "Training Epoch: 15 [34400/36045]\tLoss: 620.6219\n",
      "Training Epoch: 15 [34450/36045]\tLoss: 582.5377\n",
      "Training Epoch: 15 [34500/36045]\tLoss: 624.1849\n",
      "Training Epoch: 15 [34550/36045]\tLoss: 615.7181\n",
      "Training Epoch: 15 [34600/36045]\tLoss: 606.2036\n",
      "Training Epoch: 15 [34650/36045]\tLoss: 723.8228\n",
      "Training Epoch: 15 [34700/36045]\tLoss: 762.7919\n",
      "Training Epoch: 15 [34750/36045]\tLoss: 679.0098\n",
      "Training Epoch: 15 [34800/36045]\tLoss: 770.6067\n",
      "Training Epoch: 15 [34850/36045]\tLoss: 783.8373\n",
      "Training Epoch: 15 [34900/36045]\tLoss: 898.5094\n",
      "Training Epoch: 15 [34950/36045]\tLoss: 887.4668\n",
      "Training Epoch: 15 [35000/36045]\tLoss: 890.7256\n",
      "Training Epoch: 15 [35050/36045]\tLoss: 873.6335\n",
      "Training Epoch: 15 [35100/36045]\tLoss: 704.5866\n",
      "Training Epoch: 15 [35150/36045]\tLoss: 697.6730\n",
      "Training Epoch: 15 [35200/36045]\tLoss: 600.3947\n",
      "Training Epoch: 15 [35250/36045]\tLoss: 656.0805\n",
      "Training Epoch: 15 [35300/36045]\tLoss: 668.7541\n",
      "Training Epoch: 15 [35350/36045]\tLoss: 781.9409\n",
      "Training Epoch: 15 [35400/36045]\tLoss: 831.6984\n",
      "Training Epoch: 15 [35450/36045]\tLoss: 793.3922\n",
      "Training Epoch: 15 [35500/36045]\tLoss: 774.2951\n",
      "Training Epoch: 15 [35550/36045]\tLoss: 758.5488\n",
      "Training Epoch: 15 [35600/36045]\tLoss: 803.3836\n",
      "Training Epoch: 15 [35650/36045]\tLoss: 883.5657\n",
      "Training Epoch: 15 [35700/36045]\tLoss: 809.0239\n",
      "Training Epoch: 15 [35750/36045]\tLoss: 872.0254\n",
      "Training Epoch: 15 [35800/36045]\tLoss: 877.5632\n",
      "Training Epoch: 15 [35850/36045]\tLoss: 851.7905\n",
      "Training Epoch: 15 [35900/36045]\tLoss: 887.8248\n",
      "Training Epoch: 15 [35950/36045]\tLoss: 888.1469\n",
      "Training Epoch: 15 [36000/36045]\tLoss: 874.7648\n",
      "Training Epoch: 15 [36045/36045]\tLoss: 855.1583\n",
      "Training Epoch: 15 [4004/4004]\tLoss: 802.5508\n",
      "Training Epoch: 16 [50/36045]\tLoss: 805.0907\n",
      "Training Epoch: 16 [100/36045]\tLoss: 771.5630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [150/36045]\tLoss: 770.8838\n",
      "Training Epoch: 16 [200/36045]\tLoss: 757.5756\n",
      "Training Epoch: 16 [250/36045]\tLoss: 891.3371\n",
      "Training Epoch: 16 [300/36045]\tLoss: 954.3770\n",
      "Training Epoch: 16 [350/36045]\tLoss: 914.8250\n",
      "Training Epoch: 16 [400/36045]\tLoss: 916.8105\n",
      "Training Epoch: 16 [450/36045]\tLoss: 894.1337\n",
      "Training Epoch: 16 [500/36045]\tLoss: 837.0176\n",
      "Training Epoch: 16 [550/36045]\tLoss: 850.4754\n",
      "Training Epoch: 16 [600/36045]\tLoss: 820.3660\n",
      "Training Epoch: 16 [650/36045]\tLoss: 850.0849\n",
      "Training Epoch: 16 [700/36045]\tLoss: 837.7335\n",
      "Training Epoch: 16 [750/36045]\tLoss: 812.7745\n",
      "Training Epoch: 16 [800/36045]\tLoss: 831.0009\n",
      "Training Epoch: 16 [850/36045]\tLoss: 807.7580\n",
      "Training Epoch: 16 [900/36045]\tLoss: 768.9682\n",
      "Training Epoch: 16 [950/36045]\tLoss: 732.0298\n",
      "Training Epoch: 16 [1000/36045]\tLoss: 702.5398\n",
      "Training Epoch: 16 [1050/36045]\tLoss: 704.3580\n",
      "Training Epoch: 16 [1100/36045]\tLoss: 685.4681\n",
      "Training Epoch: 16 [1150/36045]\tLoss: 692.4283\n",
      "Training Epoch: 16 [1200/36045]\tLoss: 732.8961\n",
      "Training Epoch: 16 [1250/36045]\tLoss: 833.7653\n",
      "Training Epoch: 16 [1300/36045]\tLoss: 838.0619\n",
      "Training Epoch: 16 [1350/36045]\tLoss: 842.6284\n",
      "Training Epoch: 16 [1400/36045]\tLoss: 875.1658\n",
      "Training Epoch: 16 [1450/36045]\tLoss: 844.2716\n",
      "Training Epoch: 16 [1500/36045]\tLoss: 782.8121\n",
      "Training Epoch: 16 [1550/36045]\tLoss: 804.6795\n",
      "Training Epoch: 16 [1600/36045]\tLoss: 815.5188\n",
      "Training Epoch: 16 [1650/36045]\tLoss: 800.2419\n",
      "Training Epoch: 16 [1700/36045]\tLoss: 811.2139\n",
      "Training Epoch: 16 [1750/36045]\tLoss: 854.9068\n",
      "Training Epoch: 16 [1800/36045]\tLoss: 838.1838\n",
      "Training Epoch: 16 [1850/36045]\tLoss: 864.6827\n",
      "Training Epoch: 16 [1900/36045]\tLoss: 807.5220\n",
      "Training Epoch: 16 [1950/36045]\tLoss: 819.2153\n",
      "Training Epoch: 16 [2000/36045]\tLoss: 739.4094\n",
      "Training Epoch: 16 [2050/36045]\tLoss: 744.8866\n",
      "Training Epoch: 16 [2100/36045]\tLoss: 785.0707\n",
      "Training Epoch: 16 [2150/36045]\tLoss: 761.1829\n",
      "Training Epoch: 16 [2200/36045]\tLoss: 705.2423\n",
      "Training Epoch: 16 [2250/36045]\tLoss: 667.0807\n",
      "Training Epoch: 16 [2300/36045]\tLoss: 699.2076\n",
      "Training Epoch: 16 [2350/36045]\tLoss: 666.7165\n",
      "Training Epoch: 16 [2400/36045]\tLoss: 682.5948\n",
      "Training Epoch: 16 [2450/36045]\tLoss: 859.0649\n",
      "Training Epoch: 16 [2500/36045]\tLoss: 900.7755\n",
      "Training Epoch: 16 [2550/36045]\tLoss: 897.8291\n",
      "Training Epoch: 16 [2600/36045]\tLoss: 906.8362\n",
      "Training Epoch: 16 [2650/36045]\tLoss: 1033.5840\n",
      "Training Epoch: 16 [2700/36045]\tLoss: 1120.6309\n",
      "Training Epoch: 16 [2750/36045]\tLoss: 1198.0760\n",
      "Training Epoch: 16 [2800/36045]\tLoss: 1212.0244\n",
      "Training Epoch: 16 [2850/36045]\tLoss: 984.1635\n",
      "Training Epoch: 16 [2900/36045]\tLoss: 956.0857\n",
      "Training Epoch: 16 [2950/36045]\tLoss: 920.7139\n",
      "Training Epoch: 16 [3000/36045]\tLoss: 917.9101\n",
      "Training Epoch: 16 [3050/36045]\tLoss: 952.3951\n",
      "Training Epoch: 16 [3100/36045]\tLoss: 869.5562\n",
      "Training Epoch: 16 [3150/36045]\tLoss: 670.2010\n",
      "Training Epoch: 16 [3200/36045]\tLoss: 697.1609\n",
      "Training Epoch: 16 [3250/36045]\tLoss: 655.9069\n",
      "Training Epoch: 16 [3300/36045]\tLoss: 620.8332\n",
      "Training Epoch: 16 [3350/36045]\tLoss: 654.8218\n",
      "Training Epoch: 16 [3400/36045]\tLoss: 689.0969\n",
      "Training Epoch: 16 [3450/36045]\tLoss: 741.4040\n",
      "Training Epoch: 16 [3500/36045]\tLoss: 722.8107\n",
      "Training Epoch: 16 [3550/36045]\tLoss: 696.5258\n",
      "Training Epoch: 16 [3600/36045]\tLoss: 744.0724\n",
      "Training Epoch: 16 [3650/36045]\tLoss: 862.7509\n",
      "Training Epoch: 16 [3700/36045]\tLoss: 867.9460\n",
      "Training Epoch: 16 [3750/36045]\tLoss: 833.0737\n",
      "Training Epoch: 16 [3800/36045]\tLoss: 821.9013\n",
      "Training Epoch: 16 [3850/36045]\tLoss: 822.7549\n",
      "Training Epoch: 16 [3900/36045]\tLoss: 830.6754\n",
      "Training Epoch: 16 [3950/36045]\tLoss: 798.0886\n",
      "Training Epoch: 16 [4000/36045]\tLoss: 812.1608\n",
      "Training Epoch: 16 [4050/36045]\tLoss: 747.4561\n",
      "Training Epoch: 16 [4100/36045]\tLoss: 726.9330\n",
      "Training Epoch: 16 [4150/36045]\tLoss: 749.9922\n",
      "Training Epoch: 16 [4200/36045]\tLoss: 742.5564\n",
      "Training Epoch: 16 [4250/36045]\tLoss: 744.4305\n",
      "Training Epoch: 16 [4300/36045]\tLoss: 766.5731\n",
      "Training Epoch: 16 [4350/36045]\tLoss: 745.7759\n",
      "Training Epoch: 16 [4400/36045]\tLoss: 712.9991\n",
      "Training Epoch: 16 [4450/36045]\tLoss: 772.9772\n",
      "Training Epoch: 16 [4500/36045]\tLoss: 822.9283\n",
      "Training Epoch: 16 [4550/36045]\tLoss: 833.8734\n",
      "Training Epoch: 16 [4600/36045]\tLoss: 858.7500\n",
      "Training Epoch: 16 [4650/36045]\tLoss: 849.2457\n",
      "Training Epoch: 16 [4700/36045]\tLoss: 784.6182\n",
      "Training Epoch: 16 [4750/36045]\tLoss: 768.1556\n",
      "Training Epoch: 16 [4800/36045]\tLoss: 798.4248\n",
      "Training Epoch: 16 [4850/36045]\tLoss: 782.6880\n",
      "Training Epoch: 16 [4900/36045]\tLoss: 759.4506\n",
      "Training Epoch: 16 [4950/36045]\tLoss: 786.4377\n",
      "Training Epoch: 16 [5000/36045]\tLoss: 825.5541\n",
      "Training Epoch: 16 [5050/36045]\tLoss: 798.5355\n",
      "Training Epoch: 16 [5100/36045]\tLoss: 809.3276\n",
      "Training Epoch: 16 [5150/36045]\tLoss: 790.6774\n",
      "Training Epoch: 16 [5200/36045]\tLoss: 787.5038\n",
      "Training Epoch: 16 [5250/36045]\tLoss: 778.1737\n",
      "Training Epoch: 16 [5300/36045]\tLoss: 780.1539\n",
      "Training Epoch: 16 [5350/36045]\tLoss: 808.1268\n",
      "Training Epoch: 16 [5400/36045]\tLoss: 773.9529\n",
      "Training Epoch: 16 [5450/36045]\tLoss: 730.6844\n",
      "Training Epoch: 16 [5500/36045]\tLoss: 769.1878\n",
      "Training Epoch: 16 [5550/36045]\tLoss: 751.7892\n",
      "Training Epoch: 16 [5600/36045]\tLoss: 852.1019\n",
      "Training Epoch: 16 [5650/36045]\tLoss: 809.7162\n",
      "Training Epoch: 16 [5700/36045]\tLoss: 764.0161\n",
      "Training Epoch: 16 [5750/36045]\tLoss: 748.6498\n",
      "Training Epoch: 16 [5800/36045]\tLoss: 794.0125\n",
      "Training Epoch: 16 [5850/36045]\tLoss: 770.5770\n",
      "Training Epoch: 16 [5900/36045]\tLoss: 886.0569\n",
      "Training Epoch: 16 [5950/36045]\tLoss: 910.9383\n",
      "Training Epoch: 16 [6000/36045]\tLoss: 893.2050\n",
      "Training Epoch: 16 [6050/36045]\tLoss: 861.4368\n",
      "Training Epoch: 16 [6100/36045]\tLoss: 868.2781\n",
      "Training Epoch: 16 [6150/36045]\tLoss: 839.4562\n",
      "Training Epoch: 16 [6200/36045]\tLoss: 837.2760\n",
      "Training Epoch: 16 [6250/36045]\tLoss: 855.2581\n",
      "Training Epoch: 16 [6300/36045]\tLoss: 872.1911\n",
      "Training Epoch: 16 [6350/36045]\tLoss: 925.4250\n",
      "Training Epoch: 16 [6400/36045]\tLoss: 782.7424\n",
      "Training Epoch: 16 [6450/36045]\tLoss: 725.6927\n",
      "Training Epoch: 16 [6500/36045]\tLoss: 742.7261\n",
      "Training Epoch: 16 [6550/36045]\tLoss: 757.9160\n",
      "Training Epoch: 16 [6600/36045]\tLoss: 762.1160\n",
      "Training Epoch: 16 [6650/36045]\tLoss: 863.6404\n",
      "Training Epoch: 16 [6700/36045]\tLoss: 904.4205\n",
      "Training Epoch: 16 [6750/36045]\tLoss: 874.0410\n",
      "Training Epoch: 16 [6800/36045]\tLoss: 878.1411\n",
      "Training Epoch: 16 [6850/36045]\tLoss: 863.2598\n",
      "Training Epoch: 16 [6900/36045]\tLoss: 763.1113\n",
      "Training Epoch: 16 [6950/36045]\tLoss: 718.9086\n",
      "Training Epoch: 16 [7000/36045]\tLoss: 765.1024\n",
      "Training Epoch: 16 [7050/36045]\tLoss: 782.7095\n",
      "Training Epoch: 16 [7100/36045]\tLoss: 780.9055\n",
      "Training Epoch: 16 [7150/36045]\tLoss: 797.2292\n",
      "Training Epoch: 16 [7200/36045]\tLoss: 802.3438\n",
      "Training Epoch: 16 [7250/36045]\tLoss: 799.8387\n",
      "Training Epoch: 16 [7300/36045]\tLoss: 785.4639\n",
      "Training Epoch: 16 [7350/36045]\tLoss: 777.1958\n",
      "Training Epoch: 16 [7400/36045]\tLoss: 694.2812\n",
      "Training Epoch: 16 [7450/36045]\tLoss: 699.8962\n",
      "Training Epoch: 16 [7500/36045]\tLoss: 695.8655\n",
      "Training Epoch: 16 [7550/36045]\tLoss: 665.0142\n",
      "Training Epoch: 16 [7600/36045]\tLoss: 748.8306\n",
      "Training Epoch: 16 [7650/36045]\tLoss: 809.4608\n",
      "Training Epoch: 16 [7700/36045]\tLoss: 772.6129\n",
      "Training Epoch: 16 [7750/36045]\tLoss: 786.9455\n",
      "Training Epoch: 16 [7800/36045]\tLoss: 772.6774\n",
      "Training Epoch: 16 [7850/36045]\tLoss: 739.5897\n",
      "Training Epoch: 16 [7900/36045]\tLoss: 780.9509\n",
      "Training Epoch: 16 [7950/36045]\tLoss: 778.1709\n",
      "Training Epoch: 16 [8000/36045]\tLoss: 797.2962\n",
      "Training Epoch: 16 [8050/36045]\tLoss: 755.1545\n",
      "Training Epoch: 16 [8100/36045]\tLoss: 784.0930\n",
      "Training Epoch: 16 [8150/36045]\tLoss: 888.0861\n",
      "Training Epoch: 16 [8200/36045]\tLoss: 874.0187\n",
      "Training Epoch: 16 [8250/36045]\tLoss: 838.3607\n",
      "Training Epoch: 16 [8300/36045]\tLoss: 909.0344\n",
      "Training Epoch: 16 [8350/36045]\tLoss: 838.7595\n",
      "Training Epoch: 16 [8400/36045]\tLoss: 752.7235\n",
      "Training Epoch: 16 [8450/36045]\tLoss: 706.0839\n",
      "Training Epoch: 16 [8500/36045]\tLoss: 748.2029\n",
      "Training Epoch: 16 [8550/36045]\tLoss: 735.8557\n",
      "Training Epoch: 16 [8600/36045]\tLoss: 727.9654\n",
      "Training Epoch: 16 [8650/36045]\tLoss: 778.0681\n",
      "Training Epoch: 16 [8700/36045]\tLoss: 822.9122\n",
      "Training Epoch: 16 [8750/36045]\tLoss: 805.4545\n",
      "Training Epoch: 16 [8800/36045]\tLoss: 811.1008\n",
      "Training Epoch: 16 [8850/36045]\tLoss: 803.2902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [8900/36045]\tLoss: 725.1054\n",
      "Training Epoch: 16 [8950/36045]\tLoss: 743.7821\n",
      "Training Epoch: 16 [9000/36045]\tLoss: 757.4103\n",
      "Training Epoch: 16 [9050/36045]\tLoss: 755.6774\n",
      "Training Epoch: 16 [9100/36045]\tLoss: 778.7816\n",
      "Training Epoch: 16 [9150/36045]\tLoss: 574.1262\n",
      "Training Epoch: 16 [9200/36045]\tLoss: 435.5827\n",
      "Training Epoch: 16 [9250/36045]\tLoss: 471.3421\n",
      "Training Epoch: 16 [9300/36045]\tLoss: 486.9954\n",
      "Training Epoch: 16 [9350/36045]\tLoss: 446.2819\n",
      "Training Epoch: 16 [9400/36045]\tLoss: 875.9124\n",
      "Training Epoch: 16 [9450/36045]\tLoss: 929.6683\n",
      "Training Epoch: 16 [9500/36045]\tLoss: 916.0091\n",
      "Training Epoch: 16 [9550/36045]\tLoss: 969.2192\n",
      "Training Epoch: 16 [9600/36045]\tLoss: 718.7945\n",
      "Training Epoch: 16 [9650/36045]\tLoss: 720.4207\n",
      "Training Epoch: 16 [9700/36045]\tLoss: 705.8158\n",
      "Training Epoch: 16 [9750/36045]\tLoss: 707.2791\n",
      "Training Epoch: 16 [9800/36045]\tLoss: 913.6816\n",
      "Training Epoch: 16 [9850/36045]\tLoss: 964.7294\n",
      "Training Epoch: 16 [9900/36045]\tLoss: 988.1928\n",
      "Training Epoch: 16 [9950/36045]\tLoss: 961.5496\n",
      "Training Epoch: 16 [10000/36045]\tLoss: 886.7744\n",
      "Training Epoch: 16 [10050/36045]\tLoss: 741.6847\n",
      "Training Epoch: 16 [10100/36045]\tLoss: 744.6581\n",
      "Training Epoch: 16 [10150/36045]\tLoss: 756.8674\n",
      "Training Epoch: 16 [10200/36045]\tLoss: 746.6829\n",
      "Training Epoch: 16 [10250/36045]\tLoss: 890.0603\n",
      "Training Epoch: 16 [10300/36045]\tLoss: 863.0386\n",
      "Training Epoch: 16 [10350/36045]\tLoss: 908.6246\n",
      "Training Epoch: 16 [10400/36045]\tLoss: 899.1891\n",
      "Training Epoch: 16 [10450/36045]\tLoss: 838.4886\n",
      "Training Epoch: 16 [10500/36045]\tLoss: 706.0131\n",
      "Training Epoch: 16 [10550/36045]\tLoss: 702.1657\n",
      "Training Epoch: 16 [10600/36045]\tLoss: 727.4728\n",
      "Training Epoch: 16 [10650/36045]\tLoss: 733.3165\n",
      "Training Epoch: 16 [10700/36045]\tLoss: 827.3400\n",
      "Training Epoch: 16 [10750/36045]\tLoss: 897.5901\n",
      "Training Epoch: 16 [10800/36045]\tLoss: 831.2969\n",
      "Training Epoch: 16 [10850/36045]\tLoss: 880.0919\n",
      "Training Epoch: 16 [10900/36045]\tLoss: 916.3826\n",
      "Training Epoch: 16 [10950/36045]\tLoss: 683.5605\n",
      "Training Epoch: 16 [11000/36045]\tLoss: 677.2682\n",
      "Training Epoch: 16 [11050/36045]\tLoss: 722.6511\n",
      "Training Epoch: 16 [11100/36045]\tLoss: 737.2372\n",
      "Training Epoch: 16 [11150/36045]\tLoss: 798.8997\n",
      "Training Epoch: 16 [11200/36045]\tLoss: 828.0684\n",
      "Training Epoch: 16 [11250/36045]\tLoss: 842.1940\n",
      "Training Epoch: 16 [11300/36045]\tLoss: 822.0873\n",
      "Training Epoch: 16 [11350/36045]\tLoss: 817.4742\n",
      "Training Epoch: 16 [11400/36045]\tLoss: 772.4879\n",
      "Training Epoch: 16 [11450/36045]\tLoss: 734.6727\n",
      "Training Epoch: 16 [11500/36045]\tLoss: 733.0811\n",
      "Training Epoch: 16 [11550/36045]\tLoss: 751.1893\n",
      "Training Epoch: 16 [11600/36045]\tLoss: 820.3597\n",
      "Training Epoch: 16 [11650/36045]\tLoss: 875.5428\n",
      "Training Epoch: 16 [11700/36045]\tLoss: 874.4795\n",
      "Training Epoch: 16 [11750/36045]\tLoss: 895.8188\n",
      "Training Epoch: 16 [11800/36045]\tLoss: 941.1031\n",
      "Training Epoch: 16 [11850/36045]\tLoss: 990.0900\n",
      "Training Epoch: 16 [11900/36045]\tLoss: 1219.0098\n",
      "Training Epoch: 16 [11950/36045]\tLoss: 1216.8412\n",
      "Training Epoch: 16 [12000/36045]\tLoss: 1237.5535\n",
      "Training Epoch: 16 [12050/36045]\tLoss: 1192.7402\n",
      "Training Epoch: 16 [12100/36045]\tLoss: 805.1277\n",
      "Training Epoch: 16 [12150/36045]\tLoss: 634.9108\n",
      "Training Epoch: 16 [12200/36045]\tLoss: 628.7205\n",
      "Training Epoch: 16 [12250/36045]\tLoss: 639.5717\n",
      "Training Epoch: 16 [12300/36045]\tLoss: 801.6470\n",
      "Training Epoch: 16 [12350/36045]\tLoss: 863.8154\n",
      "Training Epoch: 16 [12400/36045]\tLoss: 875.2324\n",
      "Training Epoch: 16 [12450/36045]\tLoss: 861.1554\n",
      "Training Epoch: 16 [12500/36045]\tLoss: 894.4185\n",
      "Training Epoch: 16 [12550/36045]\tLoss: 858.5822\n",
      "Training Epoch: 16 [12600/36045]\tLoss: 797.5764\n",
      "Training Epoch: 16 [12650/36045]\tLoss: 794.3317\n",
      "Training Epoch: 16 [12700/36045]\tLoss: 818.9380\n",
      "Training Epoch: 16 [12750/36045]\tLoss: 818.3553\n",
      "Training Epoch: 16 [12800/36045]\tLoss: 797.7761\n",
      "Training Epoch: 16 [12850/36045]\tLoss: 832.5970\n",
      "Training Epoch: 16 [12900/36045]\tLoss: 797.7509\n",
      "Training Epoch: 16 [12950/36045]\tLoss: 787.5782\n",
      "Training Epoch: 16 [13000/36045]\tLoss: 819.8716\n",
      "Training Epoch: 16 [13050/36045]\tLoss: 750.4565\n",
      "Training Epoch: 16 [13100/36045]\tLoss: 782.1180\n",
      "Training Epoch: 16 [13150/36045]\tLoss: 774.8751\n",
      "Training Epoch: 16 [13200/36045]\tLoss: 741.7870\n",
      "Training Epoch: 16 [13250/36045]\tLoss: 778.0336\n",
      "Training Epoch: 16 [13300/36045]\tLoss: 819.1833\n",
      "Training Epoch: 16 [13350/36045]\tLoss: 795.3576\n",
      "Training Epoch: 16 [13400/36045]\tLoss: 801.5463\n",
      "Training Epoch: 16 [13450/36045]\tLoss: 792.2958\n",
      "Training Epoch: 16 [13500/36045]\tLoss: 821.4163\n",
      "Training Epoch: 16 [13550/36045]\tLoss: 957.5175\n",
      "Training Epoch: 16 [13600/36045]\tLoss: 989.8203\n",
      "Training Epoch: 16 [13650/36045]\tLoss: 1070.2710\n",
      "Training Epoch: 16 [13700/36045]\tLoss: 952.7207\n",
      "Training Epoch: 16 [13750/36045]\tLoss: 806.8999\n",
      "Training Epoch: 16 [13800/36045]\tLoss: 781.8708\n",
      "Training Epoch: 16 [13850/36045]\tLoss: 764.2430\n",
      "Training Epoch: 16 [13900/36045]\tLoss: 773.2759\n",
      "Training Epoch: 16 [13950/36045]\tLoss: 817.5099\n",
      "Training Epoch: 16 [14000/36045]\tLoss: 855.7379\n",
      "Training Epoch: 16 [14050/36045]\tLoss: 822.7397\n",
      "Training Epoch: 16 [14100/36045]\tLoss: 818.7496\n",
      "Training Epoch: 16 [14150/36045]\tLoss: 805.1285\n",
      "Training Epoch: 16 [14200/36045]\tLoss: 857.1331\n",
      "Training Epoch: 16 [14250/36045]\tLoss: 936.0527\n",
      "Training Epoch: 16 [14300/36045]\tLoss: 940.8376\n",
      "Training Epoch: 16 [14350/36045]\tLoss: 900.2849\n",
      "Training Epoch: 16 [14400/36045]\tLoss: 885.4541\n",
      "Training Epoch: 16 [14450/36045]\tLoss: 927.5287\n",
      "Training Epoch: 16 [14500/36045]\tLoss: 856.8799\n",
      "Training Epoch: 16 [14550/36045]\tLoss: 892.7398\n",
      "Training Epoch: 16 [14600/36045]\tLoss: 871.5038\n",
      "Training Epoch: 16 [14650/36045]\tLoss: 874.2067\n",
      "Training Epoch: 16 [14700/36045]\tLoss: 821.7275\n",
      "Training Epoch: 16 [14750/36045]\tLoss: 704.8781\n",
      "Training Epoch: 16 [14800/36045]\tLoss: 694.3758\n",
      "Training Epoch: 16 [14850/36045]\tLoss: 701.7779\n",
      "Training Epoch: 16 [14900/36045]\tLoss: 695.8873\n",
      "Training Epoch: 16 [14950/36045]\tLoss: 703.7629\n",
      "Training Epoch: 16 [15000/36045]\tLoss: 723.8195\n",
      "Training Epoch: 16 [15050/36045]\tLoss: 723.5566\n",
      "Training Epoch: 16 [15100/36045]\tLoss: 706.0230\n",
      "Training Epoch: 16 [15150/36045]\tLoss: 696.6014\n",
      "Training Epoch: 16 [15200/36045]\tLoss: 642.7153\n",
      "Training Epoch: 16 [15250/36045]\tLoss: 671.5210\n",
      "Training Epoch: 16 [15300/36045]\tLoss: 653.5553\n",
      "Training Epoch: 16 [15350/36045]\tLoss: 668.7559\n",
      "Training Epoch: 16 [15400/36045]\tLoss: 653.5380\n",
      "Training Epoch: 16 [15450/36045]\tLoss: 642.0800\n",
      "Training Epoch: 16 [15500/36045]\tLoss: 661.5898\n",
      "Training Epoch: 16 [15550/36045]\tLoss: 653.1686\n",
      "Training Epoch: 16 [15600/36045]\tLoss: 730.2195\n",
      "Training Epoch: 16 [15650/36045]\tLoss: 752.6570\n",
      "Training Epoch: 16 [15700/36045]\tLoss: 740.4044\n",
      "Training Epoch: 16 [15750/36045]\tLoss: 731.3011\n",
      "Training Epoch: 16 [15800/36045]\tLoss: 679.2516\n",
      "Training Epoch: 16 [15850/36045]\tLoss: 690.3292\n",
      "Training Epoch: 16 [15900/36045]\tLoss: 700.6794\n",
      "Training Epoch: 16 [15950/36045]\tLoss: 722.2490\n",
      "Training Epoch: 16 [16000/36045]\tLoss: 702.0931\n",
      "Training Epoch: 16 [16050/36045]\tLoss: 671.5160\n",
      "Training Epoch: 16 [16100/36045]\tLoss: 619.8951\n",
      "Training Epoch: 16 [16150/36045]\tLoss: 604.5289\n",
      "Training Epoch: 16 [16200/36045]\tLoss: 729.1259\n",
      "Training Epoch: 16 [16250/36045]\tLoss: 759.9509\n",
      "Training Epoch: 16 [16300/36045]\tLoss: 829.5526\n",
      "Training Epoch: 16 [16350/36045]\tLoss: 845.4605\n",
      "Training Epoch: 16 [16400/36045]\tLoss: 819.8837\n",
      "Training Epoch: 16 [16450/36045]\tLoss: 798.3668\n",
      "Training Epoch: 16 [16500/36045]\tLoss: 795.9140\n",
      "Training Epoch: 16 [16550/36045]\tLoss: 756.2435\n",
      "Training Epoch: 16 [16600/36045]\tLoss: 789.0994\n",
      "Training Epoch: 16 [16650/36045]\tLoss: 812.8712\n",
      "Training Epoch: 16 [16700/36045]\tLoss: 785.2113\n",
      "Training Epoch: 16 [16750/36045]\tLoss: 775.9313\n",
      "Training Epoch: 16 [16800/36045]\tLoss: 790.5186\n",
      "Training Epoch: 16 [16850/36045]\tLoss: 751.6180\n",
      "Training Epoch: 16 [16900/36045]\tLoss: 765.7799\n",
      "Training Epoch: 16 [16950/36045]\tLoss: 794.6089\n",
      "Training Epoch: 16 [17000/36045]\tLoss: 773.8357\n",
      "Training Epoch: 16 [17050/36045]\tLoss: 810.2096\n",
      "Training Epoch: 16 [17100/36045]\tLoss: 811.4000\n",
      "Training Epoch: 16 [17150/36045]\tLoss: 703.4492\n",
      "Training Epoch: 16 [17200/36045]\tLoss: 657.6194\n",
      "Training Epoch: 16 [17250/36045]\tLoss: 688.0713\n",
      "Training Epoch: 16 [17300/36045]\tLoss: 725.5217\n",
      "Training Epoch: 16 [17350/36045]\tLoss: 693.9874\n",
      "Training Epoch: 16 [17400/36045]\tLoss: 713.9961\n",
      "Training Epoch: 16 [17450/36045]\tLoss: 737.1213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [17500/36045]\tLoss: 723.9005\n",
      "Training Epoch: 16 [17550/36045]\tLoss: 728.4415\n",
      "Training Epoch: 16 [17600/36045]\tLoss: 714.4370\n",
      "Training Epoch: 16 [17650/36045]\tLoss: 733.6569\n",
      "Training Epoch: 16 [17700/36045]\tLoss: 712.0128\n",
      "Training Epoch: 16 [17750/36045]\tLoss: 730.2900\n",
      "Training Epoch: 16 [17800/36045]\tLoss: 721.7278\n",
      "Training Epoch: 16 [17850/36045]\tLoss: 713.4019\n",
      "Training Epoch: 16 [17900/36045]\tLoss: 744.1702\n",
      "Training Epoch: 16 [17950/36045]\tLoss: 753.9207\n",
      "Training Epoch: 16 [18000/36045]\tLoss: 744.1824\n",
      "Training Epoch: 16 [18050/36045]\tLoss: 842.1693\n",
      "Training Epoch: 16 [18100/36045]\tLoss: 846.6461\n",
      "Training Epoch: 16 [18150/36045]\tLoss: 854.0709\n",
      "Training Epoch: 16 [18200/36045]\tLoss: 839.7349\n",
      "Training Epoch: 16 [18250/36045]\tLoss: 859.9727\n",
      "Training Epoch: 16 [18300/36045]\tLoss: 790.0571\n",
      "Training Epoch: 16 [18350/36045]\tLoss: 856.9888\n",
      "Training Epoch: 16 [18400/36045]\tLoss: 828.5790\n",
      "Training Epoch: 16 [18450/36045]\tLoss: 809.2759\n",
      "Training Epoch: 16 [18500/36045]\tLoss: 810.6307\n",
      "Training Epoch: 16 [18550/36045]\tLoss: 795.9689\n",
      "Training Epoch: 16 [18600/36045]\tLoss: 785.7198\n",
      "Training Epoch: 16 [18650/36045]\tLoss: 838.3912\n",
      "Training Epoch: 16 [18700/36045]\tLoss: 882.8942\n",
      "Training Epoch: 16 [18750/36045]\tLoss: 864.5869\n",
      "Training Epoch: 16 [18800/36045]\tLoss: 891.2437\n",
      "Training Epoch: 16 [18850/36045]\tLoss: 833.7126\n",
      "Training Epoch: 16 [18900/36045]\tLoss: 893.6144\n",
      "Training Epoch: 16 [18950/36045]\tLoss: 827.6713\n",
      "Training Epoch: 16 [19000/36045]\tLoss: 709.6474\n",
      "Training Epoch: 16 [19050/36045]\tLoss: 685.5772\n",
      "Training Epoch: 16 [19100/36045]\tLoss: 699.0498\n",
      "Training Epoch: 16 [19150/36045]\tLoss: 686.7501\n",
      "Training Epoch: 16 [19200/36045]\tLoss: 714.3799\n",
      "Training Epoch: 16 [19250/36045]\tLoss: 728.1110\n",
      "Training Epoch: 16 [19300/36045]\tLoss: 743.4216\n",
      "Training Epoch: 16 [19350/36045]\tLoss: 726.0687\n",
      "Training Epoch: 16 [19400/36045]\tLoss: 750.4711\n",
      "Training Epoch: 16 [19450/36045]\tLoss: 738.7286\n",
      "Training Epoch: 16 [19500/36045]\tLoss: 742.0927\n",
      "Training Epoch: 16 [19550/36045]\tLoss: 741.2032\n",
      "Training Epoch: 16 [19600/36045]\tLoss: 784.2314\n",
      "Training Epoch: 16 [19650/36045]\tLoss: 1020.0091\n",
      "Training Epoch: 16 [19700/36045]\tLoss: 976.6956\n",
      "Training Epoch: 16 [19750/36045]\tLoss: 976.5965\n",
      "Training Epoch: 16 [19800/36045]\tLoss: 970.1710\n",
      "Training Epoch: 16 [19850/36045]\tLoss: 663.1587\n",
      "Training Epoch: 16 [19900/36045]\tLoss: 638.0301\n",
      "Training Epoch: 16 [19950/36045]\tLoss: 643.0565\n",
      "Training Epoch: 16 [20000/36045]\tLoss: 641.8206\n",
      "Training Epoch: 16 [20050/36045]\tLoss: 718.5230\n",
      "Training Epoch: 16 [20100/36045]\tLoss: 722.7579\n",
      "Training Epoch: 16 [20150/36045]\tLoss: 725.7377\n",
      "Training Epoch: 16 [20200/36045]\tLoss: 725.2501\n",
      "Training Epoch: 16 [20250/36045]\tLoss: 771.9490\n",
      "Training Epoch: 16 [20300/36045]\tLoss: 811.8593\n",
      "Training Epoch: 16 [20350/36045]\tLoss: 836.2650\n",
      "Training Epoch: 16 [20400/36045]\tLoss: 851.1199\n",
      "Training Epoch: 16 [20450/36045]\tLoss: 824.8168\n",
      "Training Epoch: 16 [20500/36045]\tLoss: 803.3499\n",
      "Training Epoch: 16 [20550/36045]\tLoss: 710.0821\n",
      "Training Epoch: 16 [20600/36045]\tLoss: 724.6634\n",
      "Training Epoch: 16 [20650/36045]\tLoss: 720.9708\n",
      "Training Epoch: 16 [20700/36045]\tLoss: 706.2207\n",
      "Training Epoch: 16 [20750/36045]\tLoss: 755.4858\n",
      "Training Epoch: 16 [20800/36045]\tLoss: 822.9784\n",
      "Training Epoch: 16 [20850/36045]\tLoss: 812.3346\n",
      "Training Epoch: 16 [20900/36045]\tLoss: 864.3716\n",
      "Training Epoch: 16 [20950/36045]\tLoss: 814.8825\n",
      "Training Epoch: 16 [21000/36045]\tLoss: 768.8173\n",
      "Training Epoch: 16 [21050/36045]\tLoss: 658.4600\n",
      "Training Epoch: 16 [21100/36045]\tLoss: 658.7552\n",
      "Training Epoch: 16 [21150/36045]\tLoss: 705.7373\n",
      "Training Epoch: 16 [21200/36045]\tLoss: 705.6254\n",
      "Training Epoch: 16 [21250/36045]\tLoss: 674.2289\n",
      "Training Epoch: 16 [21300/36045]\tLoss: 788.4342\n",
      "Training Epoch: 16 [21350/36045]\tLoss: 784.6927\n",
      "Training Epoch: 16 [21400/36045]\tLoss: 787.8044\n",
      "Training Epoch: 16 [21450/36045]\tLoss: 797.0884\n",
      "Training Epoch: 16 [21500/36045]\tLoss: 799.1778\n",
      "Training Epoch: 16 [21550/36045]\tLoss: 900.5303\n",
      "Training Epoch: 16 [21600/36045]\tLoss: 902.1343\n",
      "Training Epoch: 16 [21650/36045]\tLoss: 918.2228\n",
      "Training Epoch: 16 [21700/36045]\tLoss: 914.4094\n",
      "Training Epoch: 16 [21750/36045]\tLoss: 882.8088\n",
      "Training Epoch: 16 [21800/36045]\tLoss: 656.4849\n",
      "Training Epoch: 16 [21850/36045]\tLoss: 636.7761\n",
      "Training Epoch: 16 [21900/36045]\tLoss: 651.3018\n",
      "Training Epoch: 16 [21950/36045]\tLoss: 647.1791\n",
      "Training Epoch: 16 [22000/36045]\tLoss: 654.9587\n",
      "Training Epoch: 16 [22050/36045]\tLoss: 690.8494\n",
      "Training Epoch: 16 [22100/36045]\tLoss: 681.0444\n",
      "Training Epoch: 16 [22150/36045]\tLoss: 659.3394\n",
      "Training Epoch: 16 [22200/36045]\tLoss: 680.1040\n",
      "Training Epoch: 16 [22250/36045]\tLoss: 686.4612\n",
      "Training Epoch: 16 [22300/36045]\tLoss: 743.6273\n",
      "Training Epoch: 16 [22350/36045]\tLoss: 773.0590\n",
      "Training Epoch: 16 [22400/36045]\tLoss: 792.8085\n",
      "Training Epoch: 16 [22450/36045]\tLoss: 778.5612\n",
      "Training Epoch: 16 [22500/36045]\tLoss: 755.7896\n",
      "Training Epoch: 16 [22550/36045]\tLoss: 802.3754\n",
      "Training Epoch: 16 [22600/36045]\tLoss: 880.0158\n",
      "Training Epoch: 16 [22650/36045]\tLoss: 921.9753\n",
      "Training Epoch: 16 [22700/36045]\tLoss: 947.4178\n",
      "Training Epoch: 16 [22750/36045]\tLoss: 969.8531\n",
      "Training Epoch: 16 [22800/36045]\tLoss: 1010.6143\n",
      "Training Epoch: 16 [22850/36045]\tLoss: 835.7361\n",
      "Training Epoch: 16 [22900/36045]\tLoss: 839.9525\n",
      "Training Epoch: 16 [22950/36045]\tLoss: 814.0899\n",
      "Training Epoch: 16 [23000/36045]\tLoss: 816.4431\n",
      "Training Epoch: 16 [23050/36045]\tLoss: 732.7958\n",
      "Training Epoch: 16 [23100/36045]\tLoss: 749.4173\n",
      "Training Epoch: 16 [23150/36045]\tLoss: 735.4894\n",
      "Training Epoch: 16 [23200/36045]\tLoss: 694.6716\n",
      "Training Epoch: 16 [23250/36045]\tLoss: 699.4512\n",
      "Training Epoch: 16 [23300/36045]\tLoss: 695.4100\n",
      "Training Epoch: 16 [23350/36045]\tLoss: 718.1343\n",
      "Training Epoch: 16 [23400/36045]\tLoss: 779.8097\n",
      "Training Epoch: 16 [23450/36045]\tLoss: 770.0695\n",
      "Training Epoch: 16 [23500/36045]\tLoss: 740.5582\n",
      "Training Epoch: 16 [23550/36045]\tLoss: 797.6715\n",
      "Training Epoch: 16 [23600/36045]\tLoss: 897.2855\n",
      "Training Epoch: 16 [23650/36045]\tLoss: 913.6866\n",
      "Training Epoch: 16 [23700/36045]\tLoss: 925.9828\n",
      "Training Epoch: 16 [23750/36045]\tLoss: 895.1932\n",
      "Training Epoch: 16 [23800/36045]\tLoss: 705.6630\n",
      "Training Epoch: 16 [23850/36045]\tLoss: 735.1973\n",
      "Training Epoch: 16 [23900/36045]\tLoss: 726.5302\n",
      "Training Epoch: 16 [23950/36045]\tLoss: 706.4672\n",
      "Training Epoch: 16 [24000/36045]\tLoss: 682.2674\n",
      "Training Epoch: 16 [24050/36045]\tLoss: 630.6989\n",
      "Training Epoch: 16 [24100/36045]\tLoss: 664.3351\n",
      "Training Epoch: 16 [24150/36045]\tLoss: 660.9087\n",
      "Training Epoch: 16 [24200/36045]\tLoss: 651.1934\n",
      "Training Epoch: 16 [24250/36045]\tLoss: 633.5170\n",
      "Training Epoch: 16 [24300/36045]\tLoss: 682.1841\n",
      "Training Epoch: 16 [24350/36045]\tLoss: 699.8374\n",
      "Training Epoch: 16 [24400/36045]\tLoss: 719.7504\n",
      "Training Epoch: 16 [24450/36045]\tLoss: 686.8446\n",
      "Training Epoch: 16 [24500/36045]\tLoss: 723.8441\n",
      "Training Epoch: 16 [24550/36045]\tLoss: 818.9465\n",
      "Training Epoch: 16 [24600/36045]\tLoss: 812.8180\n",
      "Training Epoch: 16 [24650/36045]\tLoss: 781.4664\n",
      "Training Epoch: 16 [24700/36045]\tLoss: 794.1043\n",
      "Training Epoch: 16 [24750/36045]\tLoss: 733.8068\n",
      "Training Epoch: 16 [24800/36045]\tLoss: 621.7037\n",
      "Training Epoch: 16 [24850/36045]\tLoss: 642.6833\n",
      "Training Epoch: 16 [24900/36045]\tLoss: 639.8781\n",
      "Training Epoch: 16 [24950/36045]\tLoss: 641.5328\n",
      "Training Epoch: 16 [25000/36045]\tLoss: 614.2639\n",
      "Training Epoch: 16 [25050/36045]\tLoss: 586.0128\n",
      "Training Epoch: 16 [25100/36045]\tLoss: 526.8976\n",
      "Training Epoch: 16 [25150/36045]\tLoss: 488.8492\n",
      "Training Epoch: 16 [25200/36045]\tLoss: 484.8986\n",
      "Training Epoch: 16 [25250/36045]\tLoss: 517.2606\n",
      "Training Epoch: 16 [25300/36045]\tLoss: 676.9786\n",
      "Training Epoch: 16 [25350/36045]\tLoss: 675.2436\n",
      "Training Epoch: 16 [25400/36045]\tLoss: 629.5717\n",
      "Training Epoch: 16 [25450/36045]\tLoss: 631.6902\n",
      "Training Epoch: 16 [25500/36045]\tLoss: 688.3094\n",
      "Training Epoch: 16 [25550/36045]\tLoss: 796.7064\n",
      "Training Epoch: 16 [25600/36045]\tLoss: 802.5735\n",
      "Training Epoch: 16 [25650/36045]\tLoss: 772.3171\n",
      "Training Epoch: 16 [25700/36045]\tLoss: 783.0063\n",
      "Training Epoch: 16 [25750/36045]\tLoss: 754.6474\n",
      "Training Epoch: 16 [25800/36045]\tLoss: 474.6507\n",
      "Training Epoch: 16 [25850/36045]\tLoss: 486.0220\n",
      "Training Epoch: 16 [25900/36045]\tLoss: 465.3680\n",
      "Training Epoch: 16 [25950/36045]\tLoss: 474.9203\n",
      "Training Epoch: 16 [26000/36045]\tLoss: 582.0671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [26050/36045]\tLoss: 789.2879\n",
      "Training Epoch: 16 [26100/36045]\tLoss: 820.5225\n",
      "Training Epoch: 16 [26150/36045]\tLoss: 817.1831\n",
      "Training Epoch: 16 [26200/36045]\tLoss: 791.0111\n",
      "Training Epoch: 16 [26250/36045]\tLoss: 824.1852\n",
      "Training Epoch: 16 [26300/36045]\tLoss: 726.9938\n",
      "Training Epoch: 16 [26350/36045]\tLoss: 734.3499\n",
      "Training Epoch: 16 [26400/36045]\tLoss: 714.0292\n",
      "Training Epoch: 16 [26450/36045]\tLoss: 641.7663\n",
      "Training Epoch: 16 [26500/36045]\tLoss: 771.2122\n",
      "Training Epoch: 16 [26550/36045]\tLoss: 780.7231\n",
      "Training Epoch: 16 [26600/36045]\tLoss: 774.4431\n",
      "Training Epoch: 16 [26650/36045]\tLoss: 791.7697\n",
      "Training Epoch: 16 [26700/36045]\tLoss: 771.1271\n",
      "Training Epoch: 16 [26750/36045]\tLoss: 720.3959\n",
      "Training Epoch: 16 [26800/36045]\tLoss: 528.6074\n",
      "Training Epoch: 16 [26850/36045]\tLoss: 441.6389\n",
      "Training Epoch: 16 [26900/36045]\tLoss: 444.8704\n",
      "Training Epoch: 16 [26950/36045]\tLoss: 488.8679\n",
      "Training Epoch: 16 [27000/36045]\tLoss: 779.0375\n",
      "Training Epoch: 16 [27050/36045]\tLoss: 820.6452\n",
      "Training Epoch: 16 [27100/36045]\tLoss: 794.5610\n",
      "Training Epoch: 16 [27150/36045]\tLoss: 839.8043\n",
      "Training Epoch: 16 [27200/36045]\tLoss: 622.2676\n",
      "Training Epoch: 16 [27250/36045]\tLoss: 619.0485\n",
      "Training Epoch: 16 [27300/36045]\tLoss: 600.6889\n",
      "Training Epoch: 16 [27350/36045]\tLoss: 601.9962\n",
      "Training Epoch: 16 [27400/36045]\tLoss: 600.6605\n",
      "Training Epoch: 16 [27450/36045]\tLoss: 755.9321\n",
      "Training Epoch: 16 [27500/36045]\tLoss: 812.2339\n",
      "Training Epoch: 16 [27550/36045]\tLoss: 804.5746\n",
      "Training Epoch: 16 [27600/36045]\tLoss: 813.3863\n",
      "Training Epoch: 16 [27650/36045]\tLoss: 807.3638\n",
      "Training Epoch: 16 [27700/36045]\tLoss: 837.3716\n",
      "Training Epoch: 16 [27750/36045]\tLoss: 851.0550\n",
      "Training Epoch: 16 [27800/36045]\tLoss: 835.3990\n",
      "Training Epoch: 16 [27850/36045]\tLoss: 819.9731\n",
      "Training Epoch: 16 [27900/36045]\tLoss: 733.8048\n",
      "Training Epoch: 16 [27950/36045]\tLoss: 606.9756\n",
      "Training Epoch: 16 [28000/36045]\tLoss: 578.9582\n",
      "Training Epoch: 16 [28050/36045]\tLoss: 594.8447\n",
      "Training Epoch: 16 [28100/36045]\tLoss: 584.7487\n",
      "Training Epoch: 16 [28150/36045]\tLoss: 620.5195\n",
      "Training Epoch: 16 [28200/36045]\tLoss: 623.1354\n",
      "Training Epoch: 16 [28250/36045]\tLoss: 622.5744\n",
      "Training Epoch: 16 [28300/36045]\tLoss: 587.4088\n",
      "Training Epoch: 16 [28350/36045]\tLoss: 583.6397\n",
      "Training Epoch: 16 [28400/36045]\tLoss: 925.2968\n",
      "Training Epoch: 16 [28450/36045]\tLoss: 835.4244\n",
      "Training Epoch: 16 [28500/36045]\tLoss: 725.3651\n",
      "Training Epoch: 16 [28550/36045]\tLoss: 665.6929\n",
      "Training Epoch: 16 [28600/36045]\tLoss: 724.4011\n",
      "Training Epoch: 16 [28650/36045]\tLoss: 833.0229\n",
      "Training Epoch: 16 [28700/36045]\tLoss: 825.4431\n",
      "Training Epoch: 16 [28750/36045]\tLoss: 814.3505\n",
      "Training Epoch: 16 [28800/36045]\tLoss: 823.9422\n",
      "Training Epoch: 16 [28850/36045]\tLoss: 712.4498\n",
      "Training Epoch: 16 [28900/36045]\tLoss: 568.5866\n",
      "Training Epoch: 16 [28950/36045]\tLoss: 563.2610\n",
      "Training Epoch: 16 [29000/36045]\tLoss: 564.5162\n",
      "Training Epoch: 16 [29050/36045]\tLoss: 574.2388\n",
      "Training Epoch: 16 [29100/36045]\tLoss: 596.1672\n",
      "Training Epoch: 16 [29150/36045]\tLoss: 581.2990\n",
      "Training Epoch: 16 [29200/36045]\tLoss: 567.6466\n",
      "Training Epoch: 16 [29250/36045]\tLoss: 551.6475\n",
      "Training Epoch: 16 [29300/36045]\tLoss: 637.1399\n",
      "Training Epoch: 16 [29350/36045]\tLoss: 761.0726\n",
      "Training Epoch: 16 [29400/36045]\tLoss: 780.4318\n",
      "Training Epoch: 16 [29450/36045]\tLoss: 810.2306\n",
      "Training Epoch: 16 [29500/36045]\tLoss: 825.6044\n",
      "Training Epoch: 16 [29550/36045]\tLoss: 783.0517\n",
      "Training Epoch: 16 [29600/36045]\tLoss: 666.5278\n",
      "Training Epoch: 16 [29650/36045]\tLoss: 649.3870\n",
      "Training Epoch: 16 [29700/36045]\tLoss: 575.4171\n",
      "Training Epoch: 16 [29750/36045]\tLoss: 580.0128\n",
      "Training Epoch: 16 [29800/36045]\tLoss: 627.1374\n",
      "Training Epoch: 16 [29850/36045]\tLoss: 698.4692\n",
      "Training Epoch: 16 [29900/36045]\tLoss: 692.9933\n",
      "Training Epoch: 16 [29950/36045]\tLoss: 715.4130\n",
      "Training Epoch: 16 [30000/36045]\tLoss: 695.9845\n",
      "Training Epoch: 16 [30050/36045]\tLoss: 701.8906\n",
      "Training Epoch: 16 [30100/36045]\tLoss: 856.2767\n",
      "Training Epoch: 16 [30150/36045]\tLoss: 842.9955\n",
      "Training Epoch: 16 [30200/36045]\tLoss: 795.7671\n",
      "Training Epoch: 16 [30250/36045]\tLoss: 846.1188\n",
      "Training Epoch: 16 [30300/36045]\tLoss: 833.9493\n",
      "Training Epoch: 16 [30350/36045]\tLoss: 666.4860\n",
      "Training Epoch: 16 [30400/36045]\tLoss: 653.9297\n",
      "Training Epoch: 16 [30450/36045]\tLoss: 652.5361\n",
      "Training Epoch: 16 [30500/36045]\tLoss: 608.4830\n",
      "Training Epoch: 16 [30550/36045]\tLoss: 564.5479\n",
      "Training Epoch: 16 [30600/36045]\tLoss: 543.5641\n",
      "Training Epoch: 16 [30650/36045]\tLoss: 534.4905\n",
      "Training Epoch: 16 [30700/36045]\tLoss: 554.6609\n",
      "Training Epoch: 16 [30750/36045]\tLoss: 538.1970\n",
      "Training Epoch: 16 [30800/36045]\tLoss: 574.2894\n",
      "Training Epoch: 16 [30850/36045]\tLoss: 565.5914\n",
      "Training Epoch: 16 [30900/36045]\tLoss: 581.2270\n",
      "Training Epoch: 16 [30950/36045]\tLoss: 611.9001\n",
      "Training Epoch: 16 [31000/36045]\tLoss: 600.2736\n",
      "Training Epoch: 16 [31050/36045]\tLoss: 500.0721\n",
      "Training Epoch: 16 [31100/36045]\tLoss: 489.3608\n",
      "Training Epoch: 16 [31150/36045]\tLoss: 494.9508\n",
      "Training Epoch: 16 [31200/36045]\tLoss: 622.4182\n",
      "Training Epoch: 16 [31250/36045]\tLoss: 808.6230\n",
      "Training Epoch: 16 [31300/36045]\tLoss: 774.8063\n",
      "Training Epoch: 16 [31350/36045]\tLoss: 790.8196\n",
      "Training Epoch: 16 [31400/36045]\tLoss: 772.0693\n",
      "Training Epoch: 16 [31450/36045]\tLoss: 777.1739\n",
      "Training Epoch: 16 [31500/36045]\tLoss: 785.0377\n",
      "Training Epoch: 16 [31550/36045]\tLoss: 795.9669\n",
      "Training Epoch: 16 [31600/36045]\tLoss: 747.6154\n",
      "Training Epoch: 16 [31650/36045]\tLoss: 797.3511\n",
      "Training Epoch: 16 [31700/36045]\tLoss: 588.0416\n",
      "Training Epoch: 16 [31750/36045]\tLoss: 490.8756\n",
      "Training Epoch: 16 [31800/36045]\tLoss: 467.1054\n",
      "Training Epoch: 16 [31850/36045]\tLoss: 479.4099\n",
      "Training Epoch: 16 [31900/36045]\tLoss: 736.3351\n",
      "Training Epoch: 16 [31950/36045]\tLoss: 935.4978\n",
      "Training Epoch: 16 [32000/36045]\tLoss: 1059.5754\n",
      "Training Epoch: 16 [32050/36045]\tLoss: 1010.1621\n",
      "Training Epoch: 16 [32100/36045]\tLoss: 996.8834\n",
      "Training Epoch: 16 [32150/36045]\tLoss: 792.8127\n",
      "Training Epoch: 16 [32200/36045]\tLoss: 798.3975\n",
      "Training Epoch: 16 [32250/36045]\tLoss: 810.7307\n",
      "Training Epoch: 16 [32300/36045]\tLoss: 792.4304\n",
      "Training Epoch: 16 [32350/36045]\tLoss: 784.5210\n",
      "Training Epoch: 16 [32400/36045]\tLoss: 738.1365\n",
      "Training Epoch: 16 [32450/36045]\tLoss: 610.5877\n",
      "Training Epoch: 16 [32500/36045]\tLoss: 587.0870\n",
      "Training Epoch: 16 [32550/36045]\tLoss: 591.0054\n",
      "Training Epoch: 16 [32600/36045]\tLoss: 585.8959\n",
      "Training Epoch: 16 [32650/36045]\tLoss: 732.5672\n",
      "Training Epoch: 16 [32700/36045]\tLoss: 796.2404\n",
      "Training Epoch: 16 [32750/36045]\tLoss: 757.9006\n",
      "Training Epoch: 16 [32800/36045]\tLoss: 777.3447\n",
      "Training Epoch: 16 [32850/36045]\tLoss: 721.4470\n",
      "Training Epoch: 16 [32900/36045]\tLoss: 587.5807\n",
      "Training Epoch: 16 [32950/36045]\tLoss: 614.8931\n",
      "Training Epoch: 16 [33000/36045]\tLoss: 615.3178\n",
      "Training Epoch: 16 [33050/36045]\tLoss: 582.3376\n",
      "Training Epoch: 16 [33100/36045]\tLoss: 664.6360\n",
      "Training Epoch: 16 [33150/36045]\tLoss: 895.3834\n",
      "Training Epoch: 16 [33200/36045]\tLoss: 873.6533\n",
      "Training Epoch: 16 [33250/36045]\tLoss: 898.1021\n",
      "Training Epoch: 16 [33300/36045]\tLoss: 956.2720\n",
      "Training Epoch: 16 [33350/36045]\tLoss: 734.5123\n",
      "Training Epoch: 16 [33400/36045]\tLoss: 547.9469\n",
      "Training Epoch: 16 [33450/36045]\tLoss: 542.8461\n",
      "Training Epoch: 16 [33500/36045]\tLoss: 557.5350\n",
      "Training Epoch: 16 [33550/36045]\tLoss: 578.5133\n",
      "Training Epoch: 16 [33600/36045]\tLoss: 580.6379\n",
      "Training Epoch: 16 [33650/36045]\tLoss: 766.4180\n",
      "Training Epoch: 16 [33700/36045]\tLoss: 743.0997\n",
      "Training Epoch: 16 [33750/36045]\tLoss: 768.4711\n",
      "Training Epoch: 16 [33800/36045]\tLoss: 763.7884\n",
      "Training Epoch: 16 [33850/36045]\tLoss: 768.0902\n",
      "Training Epoch: 16 [33900/36045]\tLoss: 775.3044\n",
      "Training Epoch: 16 [33950/36045]\tLoss: 787.6188\n",
      "Training Epoch: 16 [34000/36045]\tLoss: 777.9801\n",
      "Training Epoch: 16 [34050/36045]\tLoss: 782.9501\n",
      "Training Epoch: 16 [34100/36045]\tLoss: 751.0344\n",
      "Training Epoch: 16 [34150/36045]\tLoss: 699.0576\n",
      "Training Epoch: 16 [34200/36045]\tLoss: 663.8329\n",
      "Training Epoch: 16 [34250/36045]\tLoss: 678.8615\n",
      "Training Epoch: 16 [34300/36045]\tLoss: 583.5966\n",
      "Training Epoch: 16 [34350/36045]\tLoss: 612.1044\n",
      "Training Epoch: 16 [34400/36045]\tLoss: 599.0369\n",
      "Training Epoch: 16 [34450/36045]\tLoss: 562.5169\n",
      "Training Epoch: 16 [34500/36045]\tLoss: 602.4260\n",
      "Training Epoch: 16 [34550/36045]\tLoss: 593.5511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [34600/36045]\tLoss: 586.6345\n",
      "Training Epoch: 16 [34650/36045]\tLoss: 702.3722\n",
      "Training Epoch: 16 [34700/36045]\tLoss: 740.6549\n",
      "Training Epoch: 16 [34750/36045]\tLoss: 659.3400\n",
      "Training Epoch: 16 [34800/36045]\tLoss: 749.1786\n",
      "Training Epoch: 16 [34850/36045]\tLoss: 761.1048\n",
      "Training Epoch: 16 [34900/36045]\tLoss: 865.7849\n",
      "Training Epoch: 16 [34950/36045]\tLoss: 854.8169\n",
      "Training Epoch: 16 [35000/36045]\tLoss: 857.0159\n",
      "Training Epoch: 16 [35050/36045]\tLoss: 841.1688\n",
      "Training Epoch: 16 [35100/36045]\tLoss: 683.3586\n",
      "Training Epoch: 16 [35150/36045]\tLoss: 676.7034\n",
      "Training Epoch: 16 [35200/36045]\tLoss: 580.9855\n",
      "Training Epoch: 16 [35250/36045]\tLoss: 635.5023\n",
      "Training Epoch: 16 [35300/36045]\tLoss: 648.2578\n",
      "Training Epoch: 16 [35350/36045]\tLoss: 754.4589\n",
      "Training Epoch: 16 [35400/36045]\tLoss: 801.4515\n",
      "Training Epoch: 16 [35450/36045]\tLoss: 764.5435\n",
      "Training Epoch: 16 [35500/36045]\tLoss: 745.9357\n",
      "Training Epoch: 16 [35550/36045]\tLoss: 729.9373\n",
      "Training Epoch: 16 [35600/36045]\tLoss: 775.6178\n",
      "Training Epoch: 16 [35650/36045]\tLoss: 854.6504\n",
      "Training Epoch: 16 [35700/36045]\tLoss: 780.4661\n",
      "Training Epoch: 16 [35750/36045]\tLoss: 842.6201\n",
      "Training Epoch: 16 [35800/36045]\tLoss: 848.2707\n",
      "Training Epoch: 16 [35850/36045]\tLoss: 822.4639\n",
      "Training Epoch: 16 [35900/36045]\tLoss: 856.5197\n",
      "Training Epoch: 16 [35950/36045]\tLoss: 856.3072\n",
      "Training Epoch: 16 [36000/36045]\tLoss: 843.8997\n",
      "Training Epoch: 16 [36045/36045]\tLoss: 824.3348\n",
      "Training Epoch: 16 [4004/4004]\tLoss: 772.9658\n",
      "Training Epoch: 17 [50/36045]\tLoss: 774.6631\n",
      "Training Epoch: 17 [100/36045]\tLoss: 742.4395\n",
      "Training Epoch: 17 [150/36045]\tLoss: 741.3223\n",
      "Training Epoch: 17 [200/36045]\tLoss: 728.4545\n",
      "Training Epoch: 17 [250/36045]\tLoss: 859.4716\n",
      "Training Epoch: 17 [300/36045]\tLoss: 922.4003\n",
      "Training Epoch: 17 [350/36045]\tLoss: 884.0351\n",
      "Training Epoch: 17 [400/36045]\tLoss: 884.1931\n",
      "Training Epoch: 17 [450/36045]\tLoss: 862.0457\n",
      "Training Epoch: 17 [500/36045]\tLoss: 806.3939\n",
      "Training Epoch: 17 [550/36045]\tLoss: 818.2032\n",
      "Training Epoch: 17 [600/36045]\tLoss: 789.9915\n",
      "Training Epoch: 17 [650/36045]\tLoss: 818.3468\n",
      "Training Epoch: 17 [700/36045]\tLoss: 805.8671\n",
      "Training Epoch: 17 [750/36045]\tLoss: 782.7512\n",
      "Training Epoch: 17 [800/36045]\tLoss: 800.7745\n",
      "Training Epoch: 17 [850/36045]\tLoss: 779.0488\n",
      "Training Epoch: 17 [900/36045]\tLoss: 741.1832\n",
      "Training Epoch: 17 [950/36045]\tLoss: 704.9943\n",
      "Training Epoch: 17 [1000/36045]\tLoss: 676.3282\n",
      "Training Epoch: 17 [1050/36045]\tLoss: 678.0308\n",
      "Training Epoch: 17 [1100/36045]\tLoss: 659.8031\n",
      "Training Epoch: 17 [1150/36045]\tLoss: 667.6758\n",
      "Training Epoch: 17 [1200/36045]\tLoss: 706.1524\n",
      "Training Epoch: 17 [1250/36045]\tLoss: 804.7031\n",
      "Training Epoch: 17 [1300/36045]\tLoss: 809.3651\n",
      "Training Epoch: 17 [1350/36045]\tLoss: 813.5300\n",
      "Training Epoch: 17 [1400/36045]\tLoss: 844.6449\n",
      "Training Epoch: 17 [1450/36045]\tLoss: 815.2305\n",
      "Training Epoch: 17 [1500/36045]\tLoss: 755.0441\n",
      "Training Epoch: 17 [1550/36045]\tLoss: 775.5289\n",
      "Training Epoch: 17 [1600/36045]\tLoss: 786.4444\n",
      "Training Epoch: 17 [1650/36045]\tLoss: 771.6954\n",
      "Training Epoch: 17 [1700/36045]\tLoss: 782.6447\n",
      "Training Epoch: 17 [1750/36045]\tLoss: 825.4271\n",
      "Training Epoch: 17 [1800/36045]\tLoss: 808.5988\n",
      "Training Epoch: 17 [1850/36045]\tLoss: 833.3155\n",
      "Training Epoch: 17 [1900/36045]\tLoss: 778.4054\n",
      "Training Epoch: 17 [1950/36045]\tLoss: 790.0265\n",
      "Training Epoch: 17 [2000/36045]\tLoss: 713.2319\n",
      "Training Epoch: 17 [2050/36045]\tLoss: 718.2000\n",
      "Training Epoch: 17 [2100/36045]\tLoss: 757.0135\n",
      "Training Epoch: 17 [2150/36045]\tLoss: 733.9412\n",
      "Training Epoch: 17 [2200/36045]\tLoss: 680.0289\n",
      "Training Epoch: 17 [2250/36045]\tLoss: 643.1986\n",
      "Training Epoch: 17 [2300/36045]\tLoss: 674.2214\n",
      "Training Epoch: 17 [2350/36045]\tLoss: 643.0611\n",
      "Training Epoch: 17 [2400/36045]\tLoss: 657.6777\n",
      "Training Epoch: 17 [2450/36045]\tLoss: 829.5448\n",
      "Training Epoch: 17 [2500/36045]\tLoss: 870.3912\n",
      "Training Epoch: 17 [2550/36045]\tLoss: 867.2184\n",
      "Training Epoch: 17 [2600/36045]\tLoss: 876.4598\n",
      "Training Epoch: 17 [2650/36045]\tLoss: 1003.1043\n",
      "Training Epoch: 17 [2700/36045]\tLoss: 1090.0823\n",
      "Training Epoch: 17 [2750/36045]\tLoss: 1166.5931\n",
      "Training Epoch: 17 [2800/36045]\tLoss: 1179.6622\n",
      "Training Epoch: 17 [2850/36045]\tLoss: 950.8795\n",
      "Training Epoch: 17 [2900/36045]\tLoss: 921.2609\n",
      "Training Epoch: 17 [2950/36045]\tLoss: 887.4818\n",
      "Training Epoch: 17 [3000/36045]\tLoss: 884.3073\n",
      "Training Epoch: 17 [3050/36045]\tLoss: 917.6248\n",
      "Training Epoch: 17 [3100/36045]\tLoss: 838.6275\n",
      "Training Epoch: 17 [3150/36045]\tLoss: 646.5885\n",
      "Training Epoch: 17 [3200/36045]\tLoss: 672.3816\n",
      "Training Epoch: 17 [3250/36045]\tLoss: 632.6981\n",
      "Training Epoch: 17 [3300/36045]\tLoss: 599.3057\n",
      "Training Epoch: 17 [3350/36045]\tLoss: 632.0277\n",
      "Training Epoch: 17 [3400/36045]\tLoss: 665.1864\n",
      "Training Epoch: 17 [3450/36045]\tLoss: 715.0134\n",
      "Training Epoch: 17 [3500/36045]\tLoss: 697.5482\n",
      "Training Epoch: 17 [3550/36045]\tLoss: 671.8130\n",
      "Training Epoch: 17 [3600/36045]\tLoss: 717.7497\n",
      "Training Epoch: 17 [3650/36045]\tLoss: 831.7284\n",
      "Training Epoch: 17 [3700/36045]\tLoss: 837.1005\n",
      "Training Epoch: 17 [3750/36045]\tLoss: 803.0272\n",
      "Training Epoch: 17 [3800/36045]\tLoss: 792.3720\n",
      "Training Epoch: 17 [3850/36045]\tLoss: 792.6381\n",
      "Training Epoch: 17 [3900/36045]\tLoss: 799.7965\n",
      "Training Epoch: 17 [3950/36045]\tLoss: 769.2207\n",
      "Training Epoch: 17 [4000/36045]\tLoss: 781.4756\n",
      "Training Epoch: 17 [4050/36045]\tLoss: 719.5330\n",
      "Training Epoch: 17 [4100/36045]\tLoss: 700.2429\n",
      "Training Epoch: 17 [4150/36045]\tLoss: 721.9982\n",
      "Training Epoch: 17 [4200/36045]\tLoss: 715.0643\n",
      "Training Epoch: 17 [4250/36045]\tLoss: 716.8538\n",
      "Training Epoch: 17 [4300/36045]\tLoss: 738.5585\n",
      "Training Epoch: 17 [4350/36045]\tLoss: 717.8434\n",
      "Training Epoch: 17 [4400/36045]\tLoss: 686.6912\n",
      "Training Epoch: 17 [4450/36045]\tLoss: 745.6317\n",
      "Training Epoch: 17 [4500/36045]\tLoss: 794.3194\n",
      "Training Epoch: 17 [4550/36045]\tLoss: 804.3141\n",
      "Training Epoch: 17 [4600/36045]\tLoss: 829.2747\n",
      "Training Epoch: 17 [4650/36045]\tLoss: 818.5991\n",
      "Training Epoch: 17 [4700/36045]\tLoss: 756.7784\n",
      "Training Epoch: 17 [4750/36045]\tLoss: 739.9459\n",
      "Training Epoch: 17 [4800/36045]\tLoss: 769.5483\n",
      "Training Epoch: 17 [4850/36045]\tLoss: 754.4992\n",
      "Training Epoch: 17 [4900/36045]\tLoss: 732.6833\n",
      "Training Epoch: 17 [4950/36045]\tLoss: 757.3447\n",
      "Training Epoch: 17 [5000/36045]\tLoss: 794.2578\n",
      "Training Epoch: 17 [5050/36045]\tLoss: 768.7836\n",
      "Training Epoch: 17 [5100/36045]\tLoss: 778.7884\n",
      "Training Epoch: 17 [5150/36045]\tLoss: 761.3846\n",
      "Training Epoch: 17 [5200/36045]\tLoss: 758.8048\n",
      "Training Epoch: 17 [5250/36045]\tLoss: 750.0152\n",
      "Training Epoch: 17 [5300/36045]\tLoss: 752.1110\n",
      "Training Epoch: 17 [5350/36045]\tLoss: 778.6440\n",
      "Training Epoch: 17 [5400/36045]\tLoss: 746.5956\n",
      "Training Epoch: 17 [5450/36045]\tLoss: 705.6309\n",
      "Training Epoch: 17 [5500/36045]\tLoss: 742.0571\n",
      "Training Epoch: 17 [5550/36045]\tLoss: 726.3480\n",
      "Training Epoch: 17 [5600/36045]\tLoss: 822.3400\n",
      "Training Epoch: 17 [5650/36045]\tLoss: 781.4107\n",
      "Training Epoch: 17 [5700/36045]\tLoss: 736.4952\n",
      "Training Epoch: 17 [5750/36045]\tLoss: 721.6198\n",
      "Training Epoch: 17 [5800/36045]\tLoss: 764.0789\n",
      "Training Epoch: 17 [5850/36045]\tLoss: 742.6536\n",
      "Training Epoch: 17 [5900/36045]\tLoss: 854.3666\n",
      "Training Epoch: 17 [5950/36045]\tLoss: 877.3186\n",
      "Training Epoch: 17 [6000/36045]\tLoss: 860.5472\n",
      "Training Epoch: 17 [6050/36045]\tLoss: 830.0274\n",
      "Training Epoch: 17 [6100/36045]\tLoss: 836.7722\n",
      "Training Epoch: 17 [6150/36045]\tLoss: 810.6189\n",
      "Training Epoch: 17 [6200/36045]\tLoss: 808.9933\n",
      "Training Epoch: 17 [6250/36045]\tLoss: 828.1310\n",
      "Training Epoch: 17 [6300/36045]\tLoss: 843.4857\n",
      "Training Epoch: 17 [6350/36045]\tLoss: 896.4610\n",
      "Training Epoch: 17 [6400/36045]\tLoss: 755.5163\n",
      "Training Epoch: 17 [6450/36045]\tLoss: 700.4293\n",
      "Training Epoch: 17 [6500/36045]\tLoss: 715.6216\n",
      "Training Epoch: 17 [6550/36045]\tLoss: 731.1068\n",
      "Training Epoch: 17 [6600/36045]\tLoss: 734.6075\n",
      "Training Epoch: 17 [6650/36045]\tLoss: 832.6351\n",
      "Training Epoch: 17 [6700/36045]\tLoss: 872.0432\n",
      "Training Epoch: 17 [6750/36045]\tLoss: 842.2000\n",
      "Training Epoch: 17 [6800/36045]\tLoss: 846.0258\n",
      "Training Epoch: 17 [6850/36045]\tLoss: 831.7761\n",
      "Training Epoch: 17 [6900/36045]\tLoss: 736.4106\n",
      "Training Epoch: 17 [6950/36045]\tLoss: 693.8499\n",
      "Training Epoch: 17 [7000/36045]\tLoss: 738.5662\n",
      "Training Epoch: 17 [7050/36045]\tLoss: 755.1031\n",
      "Training Epoch: 17 [7100/36045]\tLoss: 753.7932\n",
      "Training Epoch: 17 [7150/36045]\tLoss: 769.1051\n",
      "Training Epoch: 17 [7200/36045]\tLoss: 773.7342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [7250/36045]\tLoss: 771.1530\n",
      "Training Epoch: 17 [7300/36045]\tLoss: 757.1329\n",
      "Training Epoch: 17 [7350/36045]\tLoss: 749.0943\n",
      "Training Epoch: 17 [7400/36045]\tLoss: 670.0446\n",
      "Training Epoch: 17 [7450/36045]\tLoss: 675.7981\n",
      "Training Epoch: 17 [7500/36045]\tLoss: 671.2175\n",
      "Training Epoch: 17 [7550/36045]\tLoss: 642.0825\n",
      "Training Epoch: 17 [7600/36045]\tLoss: 721.2683\n",
      "Training Epoch: 17 [7650/36045]\tLoss: 778.9829\n",
      "Training Epoch: 17 [7700/36045]\tLoss: 743.1508\n",
      "Training Epoch: 17 [7750/36045]\tLoss: 757.6164\n",
      "Training Epoch: 17 [7800/36045]\tLoss: 743.7178\n",
      "Training Epoch: 17 [7850/36045]\tLoss: 713.9614\n",
      "Training Epoch: 17 [7900/36045]\tLoss: 753.5320\n",
      "Training Epoch: 17 [7950/36045]\tLoss: 751.0088\n",
      "Training Epoch: 17 [8000/36045]\tLoss: 769.7449\n",
      "Training Epoch: 17 [8050/36045]\tLoss: 728.6127\n",
      "Training Epoch: 17 [8100/36045]\tLoss: 757.0294\n",
      "Training Epoch: 17 [8150/36045]\tLoss: 857.0433\n",
      "Training Epoch: 17 [8200/36045]\tLoss: 843.1081\n",
      "Training Epoch: 17 [8250/36045]\tLoss: 807.9930\n",
      "Training Epoch: 17 [8300/36045]\tLoss: 876.7873\n",
      "Training Epoch: 17 [8350/36045]\tLoss: 808.5115\n",
      "Training Epoch: 17 [8400/36045]\tLoss: 725.1268\n",
      "Training Epoch: 17 [8450/36045]\tLoss: 680.2440\n",
      "Training Epoch: 17 [8500/36045]\tLoss: 721.0956\n",
      "Training Epoch: 17 [8550/36045]\tLoss: 709.8459\n",
      "Training Epoch: 17 [8600/36045]\tLoss: 701.8440\n",
      "Training Epoch: 17 [8650/36045]\tLoss: 750.3419\n",
      "Training Epoch: 17 [8700/36045]\tLoss: 793.9432\n",
      "Training Epoch: 17 [8750/36045]\tLoss: 777.2868\n",
      "Training Epoch: 17 [8800/36045]\tLoss: 783.0750\n",
      "Training Epoch: 17 [8850/36045]\tLoss: 775.2379\n",
      "Training Epoch: 17 [8900/36045]\tLoss: 700.0312\n",
      "Training Epoch: 17 [8950/36045]\tLoss: 717.3308\n",
      "Training Epoch: 17 [9000/36045]\tLoss: 731.4890\n",
      "Training Epoch: 17 [9050/36045]\tLoss: 730.3228\n",
      "Training Epoch: 17 [9100/36045]\tLoss: 752.5011\n",
      "Training Epoch: 17 [9150/36045]\tLoss: 555.0145\n",
      "Training Epoch: 17 [9200/36045]\tLoss: 420.0366\n",
      "Training Epoch: 17 [9250/36045]\tLoss: 454.6593\n",
      "Training Epoch: 17 [9300/36045]\tLoss: 469.4188\n",
      "Training Epoch: 17 [9350/36045]\tLoss: 430.5605\n",
      "Training Epoch: 17 [9400/36045]\tLoss: 844.7396\n",
      "Training Epoch: 17 [9450/36045]\tLoss: 897.2068\n",
      "Training Epoch: 17 [9500/36045]\tLoss: 883.0965\n",
      "Training Epoch: 17 [9550/36045]\tLoss: 934.4324\n",
      "Training Epoch: 17 [9600/36045]\tLoss: 693.9260\n",
      "Training Epoch: 17 [9650/36045]\tLoss: 695.7473\n",
      "Training Epoch: 17 [9700/36045]\tLoss: 680.9935\n",
      "Training Epoch: 17 [9750/36045]\tLoss: 681.9627\n",
      "Training Epoch: 17 [9800/36045]\tLoss: 882.5161\n",
      "Training Epoch: 17 [9850/36045]\tLoss: 931.9792\n",
      "Training Epoch: 17 [9900/36045]\tLoss: 953.6245\n",
      "Training Epoch: 17 [9950/36045]\tLoss: 928.1464\n",
      "Training Epoch: 17 [10000/36045]\tLoss: 855.9617\n",
      "Training Epoch: 17 [10050/36045]\tLoss: 714.8168\n",
      "Training Epoch: 17 [10100/36045]\tLoss: 718.5155\n",
      "Training Epoch: 17 [10150/36045]\tLoss: 730.2096\n",
      "Training Epoch: 17 [10200/36045]\tLoss: 720.1124\n",
      "Training Epoch: 17 [10250/36045]\tLoss: 857.2288\n",
      "Training Epoch: 17 [10300/36045]\tLoss: 831.2838\n",
      "Training Epoch: 17 [10350/36045]\tLoss: 874.9666\n",
      "Training Epoch: 17 [10400/36045]\tLoss: 865.5303\n",
      "Training Epoch: 17 [10450/36045]\tLoss: 808.0790\n",
      "Training Epoch: 17 [10500/36045]\tLoss: 679.9915\n",
      "Training Epoch: 17 [10550/36045]\tLoss: 675.8737\n",
      "Training Epoch: 17 [10600/36045]\tLoss: 700.8964\n",
      "Training Epoch: 17 [10650/36045]\tLoss: 706.6171\n",
      "Training Epoch: 17 [10700/36045]\tLoss: 799.8069\n",
      "Training Epoch: 17 [10750/36045]\tLoss: 868.2870\n",
      "Training Epoch: 17 [10800/36045]\tLoss: 804.0316\n",
      "Training Epoch: 17 [10850/36045]\tLoss: 851.4647\n",
      "Training Epoch: 17 [10900/36045]\tLoss: 886.4177\n",
      "Training Epoch: 17 [10950/36045]\tLoss: 660.2488\n",
      "Training Epoch: 17 [11000/36045]\tLoss: 653.9969\n",
      "Training Epoch: 17 [11050/36045]\tLoss: 697.9816\n",
      "Training Epoch: 17 [11100/36045]\tLoss: 712.0032\n",
      "Training Epoch: 17 [11150/36045]\tLoss: 771.4683\n",
      "Training Epoch: 17 [11200/36045]\tLoss: 800.4655\n",
      "Training Epoch: 17 [11250/36045]\tLoss: 814.0862\n",
      "Training Epoch: 17 [11300/36045]\tLoss: 794.0189\n",
      "Training Epoch: 17 [11350/36045]\tLoss: 789.8156\n",
      "Training Epoch: 17 [11400/36045]\tLoss: 745.9786\n",
      "Training Epoch: 17 [11450/36045]\tLoss: 708.8864\n",
      "Training Epoch: 17 [11500/36045]\tLoss: 706.9966\n",
      "Training Epoch: 17 [11550/36045]\tLoss: 724.0319\n",
      "Training Epoch: 17 [11600/36045]\tLoss: 792.0397\n",
      "Training Epoch: 17 [11650/36045]\tLoss: 846.9145\n",
      "Training Epoch: 17 [11700/36045]\tLoss: 846.0117\n",
      "Training Epoch: 17 [11750/36045]\tLoss: 866.9240\n",
      "Training Epoch: 17 [11800/36045]\tLoss: 911.5469\n",
      "Training Epoch: 17 [11850/36045]\tLoss: 962.4078\n",
      "Training Epoch: 17 [11900/36045]\tLoss: 1189.2767\n",
      "Training Epoch: 17 [11950/36045]\tLoss: 1188.0139\n",
      "Training Epoch: 17 [12000/36045]\tLoss: 1207.2347\n",
      "Training Epoch: 17 [12050/36045]\tLoss: 1162.9611\n",
      "Training Epoch: 17 [12100/36045]\tLoss: 780.4339\n",
      "Training Epoch: 17 [12150/36045]\tLoss: 612.4991\n",
      "Training Epoch: 17 [12200/36045]\tLoss: 606.4724\n",
      "Training Epoch: 17 [12250/36045]\tLoss: 616.9931\n",
      "Training Epoch: 17 [12300/36045]\tLoss: 775.4552\n",
      "Training Epoch: 17 [12350/36045]\tLoss: 837.0797\n",
      "Training Epoch: 17 [12400/36045]\tLoss: 847.4899\n",
      "Training Epoch: 17 [12450/36045]\tLoss: 833.6893\n",
      "Training Epoch: 17 [12500/36045]\tLoss: 866.1118\n",
      "Training Epoch: 17 [12550/36045]\tLoss: 831.3337\n",
      "Training Epoch: 17 [12600/36045]\tLoss: 771.1819\n",
      "Training Epoch: 17 [12650/36045]\tLoss: 768.5455\n",
      "Training Epoch: 17 [12700/36045]\tLoss: 792.3202\n",
      "Training Epoch: 17 [12750/36045]\tLoss: 792.0038\n",
      "Training Epoch: 17 [12800/36045]\tLoss: 771.6213\n",
      "Training Epoch: 17 [12850/36045]\tLoss: 805.4462\n",
      "Training Epoch: 17 [12900/36045]\tLoss: 771.9352\n",
      "Training Epoch: 17 [12950/36045]\tLoss: 761.2198\n",
      "Training Epoch: 17 [13000/36045]\tLoss: 793.5414\n",
      "Training Epoch: 17 [13050/36045]\tLoss: 725.1754\n",
      "Training Epoch: 17 [13100/36045]\tLoss: 754.1722\n",
      "Training Epoch: 17 [13150/36045]\tLoss: 746.5872\n",
      "Training Epoch: 17 [13200/36045]\tLoss: 716.2413\n",
      "Training Epoch: 17 [13250/36045]\tLoss: 750.2679\n",
      "Training Epoch: 17 [13300/36045]\tLoss: 791.4520\n",
      "Training Epoch: 17 [13350/36045]\tLoss: 768.7079\n",
      "Training Epoch: 17 [13400/36045]\tLoss: 774.2558\n",
      "Training Epoch: 17 [13450/36045]\tLoss: 766.2161\n",
      "Training Epoch: 17 [13500/36045]\tLoss: 793.6124\n",
      "Training Epoch: 17 [13550/36045]\tLoss: 929.9030\n",
      "Training Epoch: 17 [13600/36045]\tLoss: 962.5242\n",
      "Training Epoch: 17 [13650/36045]\tLoss: 1042.3984\n",
      "Training Epoch: 17 [13700/36045]\tLoss: 927.2796\n",
      "Training Epoch: 17 [13750/36045]\tLoss: 778.8067\n",
      "Training Epoch: 17 [13800/36045]\tLoss: 753.4443\n",
      "Training Epoch: 17 [13850/36045]\tLoss: 736.2388\n",
      "Training Epoch: 17 [13900/36045]\tLoss: 744.6290\n",
      "Training Epoch: 17 [13950/36045]\tLoss: 789.8837\n",
      "Training Epoch: 17 [14000/36045]\tLoss: 827.3906\n",
      "Training Epoch: 17 [14050/36045]\tLoss: 795.5634\n",
      "Training Epoch: 17 [14100/36045]\tLoss: 791.5413\n",
      "Training Epoch: 17 [14150/36045]\tLoss: 778.3318\n",
      "Training Epoch: 17 [14200/36045]\tLoss: 828.2266\n",
      "Training Epoch: 17 [14250/36045]\tLoss: 904.7079\n",
      "Training Epoch: 17 [14300/36045]\tLoss: 909.2045\n",
      "Training Epoch: 17 [14350/36045]\tLoss: 870.2928\n",
      "Training Epoch: 17 [14400/36045]\tLoss: 855.4943\n",
      "Training Epoch: 17 [14450/36045]\tLoss: 896.6700\n",
      "Training Epoch: 17 [14500/36045]\tLoss: 826.7892\n",
      "Training Epoch: 17 [14550/36045]\tLoss: 861.2304\n",
      "Training Epoch: 17 [14600/36045]\tLoss: 841.3391\n",
      "Training Epoch: 17 [14650/36045]\tLoss: 843.6099\n",
      "Training Epoch: 17 [14700/36045]\tLoss: 793.7446\n",
      "Training Epoch: 17 [14750/36045]\tLoss: 680.4437\n",
      "Training Epoch: 17 [14800/36045]\tLoss: 670.6423\n",
      "Training Epoch: 17 [14850/36045]\tLoss: 677.7612\n",
      "Training Epoch: 17 [14900/36045]\tLoss: 671.8242\n",
      "Training Epoch: 17 [14950/36045]\tLoss: 679.6252\n",
      "Training Epoch: 17 [15000/36045]\tLoss: 698.8191\n",
      "Training Epoch: 17 [15050/36045]\tLoss: 698.0919\n",
      "Training Epoch: 17 [15100/36045]\tLoss: 680.8613\n",
      "Training Epoch: 17 [15150/36045]\tLoss: 671.7944\n",
      "Training Epoch: 17 [15200/36045]\tLoss: 619.9071\n",
      "Training Epoch: 17 [15250/36045]\tLoss: 647.4536\n",
      "Training Epoch: 17 [15300/36045]\tLoss: 630.1448\n",
      "Training Epoch: 17 [15350/36045]\tLoss: 644.9902\n",
      "Training Epoch: 17 [15400/36045]\tLoss: 630.0118\n",
      "Training Epoch: 17 [15450/36045]\tLoss: 618.1503\n",
      "Training Epoch: 17 [15500/36045]\tLoss: 636.6074\n",
      "Training Epoch: 17 [15550/36045]\tLoss: 629.4600\n",
      "Training Epoch: 17 [15600/36045]\tLoss: 705.1721\n",
      "Training Epoch: 17 [15650/36045]\tLoss: 726.9584\n",
      "Training Epoch: 17 [15700/36045]\tLoss: 715.0040\n",
      "Training Epoch: 17 [15750/36045]\tLoss: 706.3977\n",
      "Training Epoch: 17 [15800/36045]\tLoss: 657.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [15850/36045]\tLoss: 669.0634\n",
      "Training Epoch: 17 [15900/36045]\tLoss: 679.2953\n",
      "Training Epoch: 17 [15950/36045]\tLoss: 700.4175\n",
      "Training Epoch: 17 [16000/36045]\tLoss: 679.0862\n",
      "Training Epoch: 17 [16050/36045]\tLoss: 648.8710\n",
      "Training Epoch: 17 [16100/36045]\tLoss: 598.8464\n",
      "Training Epoch: 17 [16150/36045]\tLoss: 584.1877\n",
      "Training Epoch: 17 [16200/36045]\tLoss: 704.7693\n",
      "Training Epoch: 17 [16250/36045]\tLoss: 735.5453\n",
      "Training Epoch: 17 [16300/36045]\tLoss: 803.1526\n",
      "Training Epoch: 17 [16350/36045]\tLoss: 819.4810\n",
      "Training Epoch: 17 [16400/36045]\tLoss: 793.3986\n",
      "Training Epoch: 17 [16450/36045]\tLoss: 772.5381\n",
      "Training Epoch: 17 [16500/36045]\tLoss: 770.4177\n",
      "Training Epoch: 17 [16550/36045]\tLoss: 731.4719\n",
      "Training Epoch: 17 [16600/36045]\tLoss: 762.6729\n",
      "Training Epoch: 17 [16650/36045]\tLoss: 785.7405\n",
      "Training Epoch: 17 [16700/36045]\tLoss: 758.5590\n",
      "Training Epoch: 17 [16750/36045]\tLoss: 749.4248\n",
      "Training Epoch: 17 [16800/36045]\tLoss: 763.4485\n",
      "Training Epoch: 17 [16850/36045]\tLoss: 726.1315\n",
      "Training Epoch: 17 [16900/36045]\tLoss: 739.3544\n",
      "Training Epoch: 17 [16950/36045]\tLoss: 768.3064\n",
      "Training Epoch: 17 [17000/36045]\tLoss: 748.1830\n",
      "Training Epoch: 17 [17050/36045]\tLoss: 782.6053\n",
      "Training Epoch: 17 [17100/36045]\tLoss: 782.9098\n",
      "Training Epoch: 17 [17150/36045]\tLoss: 679.0934\n",
      "Training Epoch: 17 [17200/36045]\tLoss: 634.3680\n",
      "Training Epoch: 17 [17250/36045]\tLoss: 663.6818\n",
      "Training Epoch: 17 [17300/36045]\tLoss: 700.0261\n",
      "Training Epoch: 17 [17350/36045]\tLoss: 670.1850\n",
      "Training Epoch: 17 [17400/36045]\tLoss: 690.2178\n",
      "Training Epoch: 17 [17450/36045]\tLoss: 713.0575\n",
      "Training Epoch: 17 [17500/36045]\tLoss: 699.9309\n",
      "Training Epoch: 17 [17550/36045]\tLoss: 703.3362\n",
      "Training Epoch: 17 [17600/36045]\tLoss: 690.1612\n",
      "Training Epoch: 17 [17650/36045]\tLoss: 708.8651\n",
      "Training Epoch: 17 [17700/36045]\tLoss: 687.4077\n",
      "Training Epoch: 17 [17750/36045]\tLoss: 705.3597\n",
      "Training Epoch: 17 [17800/36045]\tLoss: 696.6503\n",
      "Training Epoch: 17 [17850/36045]\tLoss: 691.7824\n",
      "Training Epoch: 17 [17900/36045]\tLoss: 722.2173\n",
      "Training Epoch: 17 [17950/36045]\tLoss: 732.2242\n",
      "Training Epoch: 17 [18000/36045]\tLoss: 722.4709\n",
      "Training Epoch: 17 [18050/36045]\tLoss: 815.2295\n",
      "Training Epoch: 17 [18100/36045]\tLoss: 819.3544\n",
      "Training Epoch: 17 [18150/36045]\tLoss: 827.3351\n",
      "Training Epoch: 17 [18200/36045]\tLoss: 812.2064\n",
      "Training Epoch: 17 [18250/36045]\tLoss: 832.6686\n",
      "Training Epoch: 17 [18300/36045]\tLoss: 766.2895\n",
      "Training Epoch: 17 [18350/36045]\tLoss: 833.9456\n",
      "Training Epoch: 17 [18400/36045]\tLoss: 805.6381\n",
      "Training Epoch: 17 [18450/36045]\tLoss: 786.3816\n",
      "Training Epoch: 17 [18500/36045]\tLoss: 787.1637\n",
      "Training Epoch: 17 [18550/36045]\tLoss: 772.8148\n",
      "Training Epoch: 17 [18600/36045]\tLoss: 762.5364\n",
      "Training Epoch: 17 [18650/36045]\tLoss: 814.2590\n",
      "Training Epoch: 17 [18700/36045]\tLoss: 857.3152\n",
      "Training Epoch: 17 [18750/36045]\tLoss: 839.6652\n",
      "Training Epoch: 17 [18800/36045]\tLoss: 865.9826\n",
      "Training Epoch: 17 [18850/36045]\tLoss: 808.7331\n",
      "Training Epoch: 17 [18900/36045]\tLoss: 866.6041\n",
      "Training Epoch: 17 [18950/36045]\tLoss: 802.0794\n",
      "Training Epoch: 17 [19000/36045]\tLoss: 686.2078\n",
      "Training Epoch: 17 [19050/36045]\tLoss: 663.2936\n",
      "Training Epoch: 17 [19100/36045]\tLoss: 675.7131\n",
      "Training Epoch: 17 [19150/36045]\tLoss: 663.8013\n",
      "Training Epoch: 17 [19200/36045]\tLoss: 690.7937\n",
      "Training Epoch: 17 [19250/36045]\tLoss: 704.8272\n",
      "Training Epoch: 17 [19300/36045]\tLoss: 718.8152\n",
      "Training Epoch: 17 [19350/36045]\tLoss: 701.9182\n",
      "Training Epoch: 17 [19400/36045]\tLoss: 726.1833\n",
      "Training Epoch: 17 [19450/36045]\tLoss: 714.7469\n",
      "Training Epoch: 17 [19500/36045]\tLoss: 717.9490\n",
      "Training Epoch: 17 [19550/36045]\tLoss: 716.7245\n",
      "Training Epoch: 17 [19600/36045]\tLoss: 759.5193\n",
      "Training Epoch: 17 [19650/36045]\tLoss: 990.3281\n",
      "Training Epoch: 17 [19700/36045]\tLoss: 947.4072\n",
      "Training Epoch: 17 [19750/36045]\tLoss: 947.6603\n",
      "Training Epoch: 17 [19800/36045]\tLoss: 941.8246\n",
      "Training Epoch: 17 [19850/36045]\tLoss: 641.4438\n",
      "Training Epoch: 17 [19900/36045]\tLoss: 617.1622\n",
      "Training Epoch: 17 [19950/36045]\tLoss: 621.9661\n",
      "Training Epoch: 17 [20000/36045]\tLoss: 620.5770\n",
      "Training Epoch: 17 [20050/36045]\tLoss: 694.3325\n",
      "Training Epoch: 17 [20100/36045]\tLoss: 698.8325\n",
      "Training Epoch: 17 [20150/36045]\tLoss: 701.7087\n",
      "Training Epoch: 17 [20200/36045]\tLoss: 701.7101\n",
      "Training Epoch: 17 [20250/36045]\tLoss: 747.3408\n",
      "Training Epoch: 17 [20300/36045]\tLoss: 786.9976\n",
      "Training Epoch: 17 [20350/36045]\tLoss: 810.1644\n",
      "Training Epoch: 17 [20400/36045]\tLoss: 825.0938\n",
      "Training Epoch: 17 [20450/36045]\tLoss: 797.8215\n",
      "Training Epoch: 17 [20500/36045]\tLoss: 777.5121\n",
      "Training Epoch: 17 [20550/36045]\tLoss: 686.3984\n",
      "Training Epoch: 17 [20600/36045]\tLoss: 700.5461\n",
      "Training Epoch: 17 [20650/36045]\tLoss: 696.8830\n",
      "Training Epoch: 17 [20700/36045]\tLoss: 682.8851\n",
      "Training Epoch: 17 [20750/36045]\tLoss: 730.9426\n",
      "Training Epoch: 17 [20800/36045]\tLoss: 796.2378\n",
      "Training Epoch: 17 [20850/36045]\tLoss: 784.9251\n",
      "Training Epoch: 17 [20900/36045]\tLoss: 835.6276\n",
      "Training Epoch: 17 [20950/36045]\tLoss: 787.9802\n",
      "Training Epoch: 17 [21000/36045]\tLoss: 743.4884\n",
      "Training Epoch: 17 [21050/36045]\tLoss: 637.4869\n",
      "Training Epoch: 17 [21100/36045]\tLoss: 638.2175\n",
      "Training Epoch: 17 [21150/36045]\tLoss: 683.4539\n",
      "Training Epoch: 17 [21200/36045]\tLoss: 683.1750\n",
      "Training Epoch: 17 [21250/36045]\tLoss: 652.9406\n",
      "Training Epoch: 17 [21300/36045]\tLoss: 762.9083\n",
      "Training Epoch: 17 [21350/36045]\tLoss: 758.4089\n",
      "Training Epoch: 17 [21400/36045]\tLoss: 761.4545\n",
      "Training Epoch: 17 [21450/36045]\tLoss: 769.8536\n",
      "Training Epoch: 17 [21500/36045]\tLoss: 772.7310\n",
      "Training Epoch: 17 [21550/36045]\tLoss: 873.1044\n",
      "Training Epoch: 17 [21600/36045]\tLoss: 874.8584\n",
      "Training Epoch: 17 [21650/36045]\tLoss: 889.9868\n",
      "Training Epoch: 17 [21700/36045]\tLoss: 886.7808\n",
      "Training Epoch: 17 [21750/36045]\tLoss: 855.3541\n",
      "Training Epoch: 17 [21800/36045]\tLoss: 635.3770\n",
      "Training Epoch: 17 [21850/36045]\tLoss: 616.4589\n",
      "Training Epoch: 17 [21900/36045]\tLoss: 630.5681\n",
      "Training Epoch: 17 [21950/36045]\tLoss: 626.8266\n",
      "Training Epoch: 17 [22000/36045]\tLoss: 634.2197\n",
      "Training Epoch: 17 [22050/36045]\tLoss: 667.7587\n",
      "Training Epoch: 17 [22100/36045]\tLoss: 658.2557\n",
      "Training Epoch: 17 [22150/36045]\tLoss: 637.4080\n",
      "Training Epoch: 17 [22200/36045]\tLoss: 657.6043\n",
      "Training Epoch: 17 [22250/36045]\tLoss: 663.6727\n",
      "Training Epoch: 17 [22300/36045]\tLoss: 720.5150\n",
      "Training Epoch: 17 [22350/36045]\tLoss: 749.4458\n",
      "Training Epoch: 17 [22400/36045]\tLoss: 767.8741\n",
      "Training Epoch: 17 [22450/36045]\tLoss: 754.2233\n",
      "Training Epoch: 17 [22500/36045]\tLoss: 732.2172\n",
      "Training Epoch: 17 [22550/36045]\tLoss: 776.6825\n",
      "Training Epoch: 17 [22600/36045]\tLoss: 850.3177\n",
      "Training Epoch: 17 [22650/36045]\tLoss: 891.2311\n",
      "Training Epoch: 17 [22700/36045]\tLoss: 915.9132\n",
      "Training Epoch: 17 [22750/36045]\tLoss: 938.4745\n",
      "Training Epoch: 17 [22800/36045]\tLoss: 977.3058\n",
      "Training Epoch: 17 [22850/36045]\tLoss: 809.5154\n",
      "Training Epoch: 17 [22900/36045]\tLoss: 813.6390\n",
      "Training Epoch: 17 [22950/36045]\tLoss: 788.5826\n",
      "Training Epoch: 17 [23000/36045]\tLoss: 790.1168\n",
      "Training Epoch: 17 [23050/36045]\tLoss: 708.3646\n",
      "Training Epoch: 17 [23100/36045]\tLoss: 724.7211\n",
      "Training Epoch: 17 [23150/36045]\tLoss: 711.5147\n",
      "Training Epoch: 17 [23200/36045]\tLoss: 672.5974\n",
      "Training Epoch: 17 [23250/36045]\tLoss: 677.0260\n",
      "Training Epoch: 17 [23300/36045]\tLoss: 673.2663\n",
      "Training Epoch: 17 [23350/36045]\tLoss: 695.8305\n",
      "Training Epoch: 17 [23400/36045]\tLoss: 755.0074\n",
      "Training Epoch: 17 [23450/36045]\tLoss: 745.5329\n",
      "Training Epoch: 17 [23500/36045]\tLoss: 717.3729\n",
      "Training Epoch: 17 [23550/36045]\tLoss: 772.1234\n",
      "Training Epoch: 17 [23600/36045]\tLoss: 868.6807\n",
      "Training Epoch: 17 [23650/36045]\tLoss: 884.3166\n",
      "Training Epoch: 17 [23700/36045]\tLoss: 896.2985\n",
      "Training Epoch: 17 [23750/36045]\tLoss: 866.3355\n",
      "Training Epoch: 17 [23800/36045]\tLoss: 684.0792\n",
      "Training Epoch: 17 [23850/36045]\tLoss: 712.9553\n",
      "Training Epoch: 17 [23900/36045]\tLoss: 703.9705\n",
      "Training Epoch: 17 [23950/36045]\tLoss: 684.4921\n",
      "Training Epoch: 17 [24000/36045]\tLoss: 660.1143\n",
      "Training Epoch: 17 [24050/36045]\tLoss: 610.6721\n",
      "Training Epoch: 17 [24100/36045]\tLoss: 643.0970\n",
      "Training Epoch: 17 [24150/36045]\tLoss: 639.5191\n",
      "Training Epoch: 17 [24200/36045]\tLoss: 630.8902\n",
      "Training Epoch: 17 [24250/36045]\tLoss: 613.5118\n",
      "Training Epoch: 17 [24300/36045]\tLoss: 660.2957\n",
      "Training Epoch: 17 [24350/36045]\tLoss: 677.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [24400/36045]\tLoss: 696.4952\n",
      "Training Epoch: 17 [24450/36045]\tLoss: 664.3634\n",
      "Training Epoch: 17 [24500/36045]\tLoss: 700.3322\n",
      "Training Epoch: 17 [24550/36045]\tLoss: 795.0416\n",
      "Training Epoch: 17 [24600/36045]\tLoss: 788.9884\n",
      "Training Epoch: 17 [24650/36045]\tLoss: 757.9117\n",
      "Training Epoch: 17 [24700/36045]\tLoss: 770.4115\n",
      "Training Epoch: 17 [24750/36045]\tLoss: 711.8845\n",
      "Training Epoch: 17 [24800/36045]\tLoss: 600.7173\n",
      "Training Epoch: 17 [24850/36045]\tLoss: 621.9975\n",
      "Training Epoch: 17 [24900/36045]\tLoss: 618.3928\n",
      "Training Epoch: 17 [24950/36045]\tLoss: 620.4977\n",
      "Training Epoch: 17 [25000/36045]\tLoss: 594.4219\n",
      "Training Epoch: 17 [25050/36045]\tLoss: 567.4218\n",
      "Training Epoch: 17 [25100/36045]\tLoss: 510.3214\n",
      "Training Epoch: 17 [25150/36045]\tLoss: 473.2317\n",
      "Training Epoch: 17 [25200/36045]\tLoss: 469.0661\n",
      "Training Epoch: 17 [25250/36045]\tLoss: 500.6587\n",
      "Training Epoch: 17 [25300/36045]\tLoss: 655.8314\n",
      "Training Epoch: 17 [25350/36045]\tLoss: 654.1199\n",
      "Training Epoch: 17 [25400/36045]\tLoss: 609.3262\n",
      "Training Epoch: 17 [25450/36045]\tLoss: 612.0188\n",
      "Training Epoch: 17 [25500/36045]\tLoss: 666.3093\n",
      "Training Epoch: 17 [25550/36045]\tLoss: 770.7454\n",
      "Training Epoch: 17 [25600/36045]\tLoss: 776.5403\n",
      "Training Epoch: 17 [25650/36045]\tLoss: 747.5366\n",
      "Training Epoch: 17 [25700/36045]\tLoss: 758.1088\n",
      "Training Epoch: 17 [25750/36045]\tLoss: 730.5198\n",
      "Training Epoch: 17 [25800/36045]\tLoss: 460.0602\n",
      "Training Epoch: 17 [25850/36045]\tLoss: 470.9289\n",
      "Training Epoch: 17 [25900/36045]\tLoss: 450.9291\n",
      "Training Epoch: 17 [25950/36045]\tLoss: 460.0126\n",
      "Training Epoch: 17 [26000/36045]\tLoss: 563.9619\n",
      "Training Epoch: 17 [26050/36045]\tLoss: 764.8146\n",
      "Training Epoch: 17 [26100/36045]\tLoss: 795.6942\n",
      "Training Epoch: 17 [26150/36045]\tLoss: 792.9083\n",
      "Training Epoch: 17 [26200/36045]\tLoss: 766.5592\n",
      "Training Epoch: 17 [26250/36045]\tLoss: 799.9526\n",
      "Training Epoch: 17 [26300/36045]\tLoss: 707.0385\n",
      "Training Epoch: 17 [26350/36045]\tLoss: 715.9174\n",
      "Training Epoch: 17 [26400/36045]\tLoss: 694.3025\n",
      "Training Epoch: 17 [26450/36045]\tLoss: 623.2372\n",
      "Training Epoch: 17 [26500/36045]\tLoss: 747.2833\n",
      "Training Epoch: 17 [26550/36045]\tLoss: 755.5685\n",
      "Training Epoch: 17 [26600/36045]\tLoss: 749.4987\n",
      "Training Epoch: 17 [26650/36045]\tLoss: 766.3756\n",
      "Training Epoch: 17 [26700/36045]\tLoss: 746.3338\n",
      "Training Epoch: 17 [26750/36045]\tLoss: 696.9229\n",
      "Training Epoch: 17 [26800/36045]\tLoss: 512.0674\n",
      "Training Epoch: 17 [26850/36045]\tLoss: 427.0938\n",
      "Training Epoch: 17 [26900/36045]\tLoss: 430.6864\n",
      "Training Epoch: 17 [26950/36045]\tLoss: 473.1453\n",
      "Training Epoch: 17 [27000/36045]\tLoss: 755.8848\n",
      "Training Epoch: 17 [27050/36045]\tLoss: 795.3192\n",
      "Training Epoch: 17 [27100/36045]\tLoss: 769.8385\n",
      "Training Epoch: 17 [27150/36045]\tLoss: 814.5451\n",
      "Training Epoch: 17 [27200/36045]\tLoss: 602.7096\n",
      "Training Epoch: 17 [27250/36045]\tLoss: 599.1262\n",
      "Training Epoch: 17 [27300/36045]\tLoss: 581.5876\n",
      "Training Epoch: 17 [27350/36045]\tLoss: 582.3249\n",
      "Training Epoch: 17 [27400/36045]\tLoss: 581.2609\n",
      "Training Epoch: 17 [27450/36045]\tLoss: 731.3184\n",
      "Training Epoch: 17 [27500/36045]\tLoss: 785.7304\n",
      "Training Epoch: 17 [27550/36045]\tLoss: 777.9957\n",
      "Training Epoch: 17 [27600/36045]\tLoss: 787.6603\n",
      "Training Epoch: 17 [27650/36045]\tLoss: 780.9454\n",
      "Training Epoch: 17 [27700/36045]\tLoss: 811.1121\n",
      "Training Epoch: 17 [27750/36045]\tLoss: 824.3287\n",
      "Training Epoch: 17 [27800/36045]\tLoss: 809.2663\n",
      "Training Epoch: 17 [27850/36045]\tLoss: 794.5804\n",
      "Training Epoch: 17 [27900/36045]\tLoss: 711.9115\n",
      "Training Epoch: 17 [27950/36045]\tLoss: 589.0732\n",
      "Training Epoch: 17 [28000/36045]\tLoss: 561.7218\n",
      "Training Epoch: 17 [28050/36045]\tLoss: 576.5656\n",
      "Training Epoch: 17 [28100/36045]\tLoss: 566.9391\n",
      "Training Epoch: 17 [28150/36045]\tLoss: 600.8379\n",
      "Training Epoch: 17 [28200/36045]\tLoss: 603.9910\n",
      "Training Epoch: 17 [28250/36045]\tLoss: 602.2469\n",
      "Training Epoch: 17 [28300/36045]\tLoss: 568.6432\n",
      "Training Epoch: 17 [28350/36045]\tLoss: 564.6628\n",
      "Training Epoch: 17 [28400/36045]\tLoss: 905.1572\n",
      "Training Epoch: 17 [28450/36045]\tLoss: 819.2994\n",
      "Training Epoch: 17 [28500/36045]\tLoss: 711.1785\n",
      "Training Epoch: 17 [28550/36045]\tLoss: 652.8423\n",
      "Training Epoch: 17 [28600/36045]\tLoss: 706.5510\n",
      "Training Epoch: 17 [28650/36045]\tLoss: 808.3754\n",
      "Training Epoch: 17 [28700/36045]\tLoss: 800.8886\n",
      "Training Epoch: 17 [28750/36045]\tLoss: 789.7566\n",
      "Training Epoch: 17 [28800/36045]\tLoss: 799.2162\n",
      "Training Epoch: 17 [28850/36045]\tLoss: 691.4981\n",
      "Training Epoch: 17 [28900/36045]\tLoss: 552.7762\n",
      "Training Epoch: 17 [28950/36045]\tLoss: 548.0280\n",
      "Training Epoch: 17 [29000/36045]\tLoss: 548.5594\n",
      "Training Epoch: 17 [29050/36045]\tLoss: 558.0889\n",
      "Training Epoch: 17 [29100/36045]\tLoss: 579.9747\n",
      "Training Epoch: 17 [29150/36045]\tLoss: 565.6723\n",
      "Training Epoch: 17 [29200/36045]\tLoss: 551.6321\n",
      "Training Epoch: 17 [29250/36045]\tLoss: 536.5790\n",
      "Training Epoch: 17 [29300/36045]\tLoss: 618.2190\n",
      "Training Epoch: 17 [29350/36045]\tLoss: 737.3713\n",
      "Training Epoch: 17 [29400/36045]\tLoss: 756.6797\n",
      "Training Epoch: 17 [29450/36045]\tLoss: 784.7534\n",
      "Training Epoch: 17 [29500/36045]\tLoss: 799.7554\n",
      "Training Epoch: 17 [29550/36045]\tLoss: 758.9023\n",
      "Training Epoch: 17 [29600/36045]\tLoss: 644.9376\n",
      "Training Epoch: 17 [29650/36045]\tLoss: 627.9657\n",
      "Training Epoch: 17 [29700/36045]\tLoss: 557.0916\n",
      "Training Epoch: 17 [29750/36045]\tLoss: 561.1486\n",
      "Training Epoch: 17 [29800/36045]\tLoss: 607.9319\n",
      "Training Epoch: 17 [29850/36045]\tLoss: 679.4688\n",
      "Training Epoch: 17 [29900/36045]\tLoss: 674.3783\n",
      "Training Epoch: 17 [29950/36045]\tLoss: 696.9299\n",
      "Training Epoch: 17 [30000/36045]\tLoss: 676.4510\n",
      "Training Epoch: 17 [30050/36045]\tLoss: 682.5513\n",
      "Training Epoch: 17 [30100/36045]\tLoss: 832.6229\n",
      "Training Epoch: 17 [30150/36045]\tLoss: 818.7487\n",
      "Training Epoch: 17 [30200/36045]\tLoss: 772.6936\n",
      "Training Epoch: 17 [30250/36045]\tLoss: 822.7720\n",
      "Training Epoch: 17 [30300/36045]\tLoss: 810.1714\n",
      "Training Epoch: 17 [30350/36045]\tLoss: 645.1504\n",
      "Training Epoch: 17 [30400/36045]\tLoss: 632.1109\n",
      "Training Epoch: 17 [30450/36045]\tLoss: 630.8620\n",
      "Training Epoch: 17 [30500/36045]\tLoss: 588.8417\n",
      "Training Epoch: 17 [30550/36045]\tLoss: 546.2833\n",
      "Training Epoch: 17 [30600/36045]\tLoss: 526.8001\n",
      "Training Epoch: 17 [30650/36045]\tLoss: 517.8979\n",
      "Training Epoch: 17 [30700/36045]\tLoss: 537.1741\n",
      "Training Epoch: 17 [30750/36045]\tLoss: 521.6616\n",
      "Training Epoch: 17 [30800/36045]\tLoss: 555.9885\n",
      "Training Epoch: 17 [30850/36045]\tLoss: 547.3053\n",
      "Training Epoch: 17 [30900/36045]\tLoss: 562.4002\n",
      "Training Epoch: 17 [30950/36045]\tLoss: 591.8292\n",
      "Training Epoch: 17 [31000/36045]\tLoss: 580.9222\n",
      "Training Epoch: 17 [31050/36045]\tLoss: 483.9375\n",
      "Training Epoch: 17 [31100/36045]\tLoss: 474.0125\n",
      "Training Epoch: 17 [31150/36045]\tLoss: 479.5714\n",
      "Training Epoch: 17 [31200/36045]\tLoss: 602.6432\n",
      "Training Epoch: 17 [31250/36045]\tLoss: 782.2332\n",
      "Training Epoch: 17 [31300/36045]\tLoss: 749.6633\n",
      "Training Epoch: 17 [31350/36045]\tLoss: 765.6221\n",
      "Training Epoch: 17 [31400/36045]\tLoss: 746.0032\n",
      "Training Epoch: 17 [31450/36045]\tLoss: 753.7061\n",
      "Training Epoch: 17 [31500/36045]\tLoss: 762.1328\n",
      "Training Epoch: 17 [31550/36045]\tLoss: 773.0353\n",
      "Training Epoch: 17 [31600/36045]\tLoss: 726.0706\n",
      "Training Epoch: 17 [31650/36045]\tLoss: 774.2817\n",
      "Training Epoch: 17 [31700/36045]\tLoss: 569.3449\n",
      "Training Epoch: 17 [31750/36045]\tLoss: 474.9366\n",
      "Training Epoch: 17 [31800/36045]\tLoss: 451.7232\n",
      "Training Epoch: 17 [31850/36045]\tLoss: 463.5078\n",
      "Training Epoch: 17 [31900/36045]\tLoss: 713.8151\n",
      "Training Epoch: 17 [31950/36045]\tLoss: 909.5541\n",
      "Training Epoch: 17 [32000/36045]\tLoss: 1031.2764\n",
      "Training Epoch: 17 [32050/36045]\tLoss: 982.6295\n",
      "Training Epoch: 17 [32100/36045]\tLoss: 969.7791\n",
      "Training Epoch: 17 [32150/36045]\tLoss: 768.3661\n",
      "Training Epoch: 17 [32200/36045]\tLoss: 773.9054\n",
      "Training Epoch: 17 [32250/36045]\tLoss: 785.6440\n",
      "Training Epoch: 17 [32300/36045]\tLoss: 767.7950\n",
      "Training Epoch: 17 [32350/36045]\tLoss: 760.3801\n",
      "Training Epoch: 17 [32400/36045]\tLoss: 715.3403\n",
      "Training Epoch: 17 [32450/36045]\tLoss: 591.1110\n",
      "Training Epoch: 17 [32500/36045]\tLoss: 568.6826\n",
      "Training Epoch: 17 [32550/36045]\tLoss: 572.2265\n",
      "Training Epoch: 17 [32600/36045]\tLoss: 567.3418\n",
      "Training Epoch: 17 [32650/36045]\tLoss: 711.6366\n",
      "Training Epoch: 17 [32700/36045]\tLoss: 773.7721\n",
      "Training Epoch: 17 [32750/36045]\tLoss: 736.8523\n",
      "Training Epoch: 17 [32800/36045]\tLoss: 755.5944\n",
      "Training Epoch: 17 [32850/36045]\tLoss: 700.6607\n",
      "Training Epoch: 17 [32900/36045]\tLoss: 569.6729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [32950/36045]\tLoss: 596.1398\n",
      "Training Epoch: 17 [33000/36045]\tLoss: 596.2217\n",
      "Training Epoch: 17 [33050/36045]\tLoss: 564.5736\n",
      "Training Epoch: 17 [33100/36045]\tLoss: 644.0493\n",
      "Training Epoch: 17 [33150/36045]\tLoss: 868.6977\n",
      "Training Epoch: 17 [33200/36045]\tLoss: 847.4826\n",
      "Training Epoch: 17 [33250/36045]\tLoss: 871.5730\n",
      "Training Epoch: 17 [33300/36045]\tLoss: 928.0731\n",
      "Training Epoch: 17 [33350/36045]\tLoss: 712.7177\n",
      "Training Epoch: 17 [33400/36045]\tLoss: 530.4677\n",
      "Training Epoch: 17 [33450/36045]\tLoss: 525.3338\n",
      "Training Epoch: 17 [33500/36045]\tLoss: 539.7225\n",
      "Training Epoch: 17 [33550/36045]\tLoss: 560.2003\n",
      "Training Epoch: 17 [33600/36045]\tLoss: 562.0790\n",
      "Training Epoch: 17 [33650/36045]\tLoss: 743.5733\n",
      "Training Epoch: 17 [33700/36045]\tLoss: 720.7283\n",
      "Training Epoch: 17 [33750/36045]\tLoss: 745.5778\n",
      "Training Epoch: 17 [33800/36045]\tLoss: 740.5783\n",
      "Training Epoch: 17 [33850/36045]\tLoss: 744.6434\n",
      "Training Epoch: 17 [33900/36045]\tLoss: 752.0669\n",
      "Training Epoch: 17 [33950/36045]\tLoss: 764.3845\n",
      "Training Epoch: 17 [34000/36045]\tLoss: 754.2158\n",
      "Training Epoch: 17 [34050/36045]\tLoss: 758.9857\n",
      "Training Epoch: 17 [34100/36045]\tLoss: 728.5821\n",
      "Training Epoch: 17 [34150/36045]\tLoss: 678.5667\n",
      "Training Epoch: 17 [34200/36045]\tLoss: 643.7986\n",
      "Training Epoch: 17 [34250/36045]\tLoss: 658.4221\n",
      "Training Epoch: 17 [34300/36045]\tLoss: 565.9350\n",
      "Training Epoch: 17 [34350/36045]\tLoss: 593.8915\n",
      "Training Epoch: 17 [34400/36045]\tLoss: 581.6852\n",
      "Training Epoch: 17 [34450/36045]\tLoss: 546.2828\n",
      "Training Epoch: 17 [34500/36045]\tLoss: 584.7609\n",
      "Training Epoch: 17 [34550/36045]\tLoss: 575.6586\n",
      "Training Epoch: 17 [34600/36045]\tLoss: 570.5809\n",
      "Training Epoch: 17 [34650/36045]\tLoss: 684.9178\n",
      "Training Epoch: 17 [34700/36045]\tLoss: 722.5177\n",
      "Training Epoch: 17 [34750/36045]\tLoss: 643.3553\n",
      "Training Epoch: 17 [34800/36045]\tLoss: 731.6016\n",
      "Training Epoch: 17 [34850/36045]\tLoss: 742.6778\n",
      "Training Epoch: 17 [34900/36045]\tLoss: 839.3973\n",
      "Training Epoch: 17 [34950/36045]\tLoss: 828.3995\n",
      "Training Epoch: 17 [35000/36045]\tLoss: 830.1431\n",
      "Training Epoch: 17 [35050/36045]\tLoss: 814.7677\n",
      "Training Epoch: 17 [35100/36045]\tLoss: 666.2817\n",
      "Training Epoch: 17 [35150/36045]\tLoss: 659.6177\n",
      "Training Epoch: 17 [35200/36045]\tLoss: 565.2934\n",
      "Training Epoch: 17 [35250/36045]\tLoss: 618.6893\n",
      "Training Epoch: 17 [35300/36045]\tLoss: 631.7018\n",
      "Training Epoch: 17 [35350/36045]\tLoss: 732.3666\n",
      "Training Epoch: 17 [35400/36045]\tLoss: 777.0326\n",
      "Training Epoch: 17 [35450/36045]\tLoss: 741.4119\n",
      "Training Epoch: 17 [35500/36045]\tLoss: 722.9528\n",
      "Training Epoch: 17 [35550/36045]\tLoss: 707.0385\n",
      "Training Epoch: 17 [35600/36045]\tLoss: 753.1943\n",
      "Training Epoch: 17 [35650/36045]\tLoss: 831.2338\n",
      "Training Epoch: 17 [35700/36045]\tLoss: 757.3732\n",
      "Training Epoch: 17 [35750/36045]\tLoss: 818.8322\n",
      "Training Epoch: 17 [35800/36045]\tLoss: 824.6037\n",
      "Training Epoch: 17 [35850/36045]\tLoss: 798.7916\n",
      "Training Epoch: 17 [35900/36045]\tLoss: 831.2561\n",
      "Training Epoch: 17 [35950/36045]\tLoss: 830.6401\n",
      "Training Epoch: 17 [36000/36045]\tLoss: 818.9410\n",
      "Training Epoch: 17 [36045/36045]\tLoss: 799.6660\n",
      "Training Epoch: 17 [4004/4004]\tLoss: 749.2266\n",
      "Training Epoch: 18 [50/36045]\tLoss: 750.3599\n",
      "Training Epoch: 18 [100/36045]\tLoss: 719.2401\n",
      "Training Epoch: 18 [150/36045]\tLoss: 717.9506\n",
      "Training Epoch: 18 [200/36045]\tLoss: 705.1186\n",
      "Training Epoch: 18 [250/36045]\tLoss: 833.7638\n",
      "Training Epoch: 18 [300/36045]\tLoss: 896.6434\n",
      "Training Epoch: 18 [350/36045]\tLoss: 859.0720\n",
      "Training Epoch: 18 [400/36045]\tLoss: 858.0530\n",
      "Training Epoch: 18 [450/36045]\tLoss: 836.2596\n",
      "Training Epoch: 18 [500/36045]\tLoss: 781.9118\n",
      "Training Epoch: 18 [550/36045]\tLoss: 792.2220\n",
      "Training Epoch: 18 [600/36045]\tLoss: 765.5128\n",
      "Training Epoch: 18 [650/36045]\tLoss: 792.8731\n",
      "Training Epoch: 18 [700/36045]\tLoss: 780.2465\n",
      "Training Epoch: 18 [750/36045]\tLoss: 758.6640\n",
      "Training Epoch: 18 [800/36045]\tLoss: 776.2903\n",
      "Training Epoch: 18 [850/36045]\tLoss: 755.6890\n",
      "Training Epoch: 18 [900/36045]\tLoss: 718.6704\n",
      "Training Epoch: 18 [950/36045]\tLoss: 682.9670\n",
      "Training Epoch: 18 [1000/36045]\tLoss: 655.1881\n",
      "Training Epoch: 18 [1050/36045]\tLoss: 656.7167\n",
      "Training Epoch: 18 [1100/36045]\tLoss: 639.2518\n",
      "Training Epoch: 18 [1150/36045]\tLoss: 647.7401\n",
      "Training Epoch: 18 [1200/36045]\tLoss: 684.7689\n",
      "Training Epoch: 18 [1250/36045]\tLoss: 781.1607\n",
      "Training Epoch: 18 [1300/36045]\tLoss: 786.2026\n",
      "Training Epoch: 18 [1350/36045]\tLoss: 789.8585\n",
      "Training Epoch: 18 [1400/36045]\tLoss: 820.0271\n",
      "Training Epoch: 18 [1450/36045]\tLoss: 791.8384\n",
      "Training Epoch: 18 [1500/36045]\tLoss: 732.6998\n",
      "Training Epoch: 18 [1550/36045]\tLoss: 752.1003\n",
      "Training Epoch: 18 [1600/36045]\tLoss: 763.0922\n",
      "Training Epoch: 18 [1650/36045]\tLoss: 748.6974\n",
      "Training Epoch: 18 [1700/36045]\tLoss: 759.5601\n",
      "Training Epoch: 18 [1750/36045]\tLoss: 801.7353\n",
      "Training Epoch: 18 [1800/36045]\tLoss: 784.6356\n",
      "Training Epoch: 18 [1850/36045]\tLoss: 807.9533\n",
      "Training Epoch: 18 [1900/36045]\tLoss: 754.9714\n",
      "Training Epoch: 18 [1950/36045]\tLoss: 766.5118\n",
      "Training Epoch: 18 [2000/36045]\tLoss: 692.1252\n",
      "Training Epoch: 18 [2050/36045]\tLoss: 696.6912\n",
      "Training Epoch: 18 [2100/36045]\tLoss: 734.5092\n",
      "Training Epoch: 18 [2150/36045]\tLoss: 711.8989\n",
      "Training Epoch: 18 [2200/36045]\tLoss: 659.7849\n",
      "Training Epoch: 18 [2250/36045]\tLoss: 623.8249\n",
      "Training Epoch: 18 [2300/36045]\tLoss: 654.0804\n",
      "Training Epoch: 18 [2350/36045]\tLoss: 623.9846\n",
      "Training Epoch: 18 [2400/36045]\tLoss: 637.6310\n",
      "Training Epoch: 18 [2450/36045]\tLoss: 805.8796\n",
      "Training Epoch: 18 [2500/36045]\tLoss: 845.8714\n",
      "Training Epoch: 18 [2550/36045]\tLoss: 842.5784\n",
      "Training Epoch: 18 [2600/36045]\tLoss: 851.9512\n",
      "Training Epoch: 18 [2650/36045]\tLoss: 978.3143\n",
      "Training Epoch: 18 [2700/36045]\tLoss: 1065.2534\n",
      "Training Epoch: 18 [2750/36045]\tLoss: 1140.9913\n",
      "Training Epoch: 18 [2800/36045]\tLoss: 1153.3457\n",
      "Training Epoch: 18 [2850/36045]\tLoss: 923.9416\n",
      "Training Epoch: 18 [2900/36045]\tLoss: 893.1625\n",
      "Training Epoch: 18 [2950/36045]\tLoss: 860.6716\n",
      "Training Epoch: 18 [3000/36045]\tLoss: 857.1885\n",
      "Training Epoch: 18 [3050/36045]\tLoss: 889.7513\n",
      "Training Epoch: 18 [3100/36045]\tLoss: 813.6866\n",
      "Training Epoch: 18 [3150/36045]\tLoss: 627.6323\n",
      "Training Epoch: 18 [3200/36045]\tLoss: 652.3639\n",
      "Training Epoch: 18 [3250/36045]\tLoss: 613.9540\n",
      "Training Epoch: 18 [3300/36045]\tLoss: 581.7051\n",
      "Training Epoch: 18 [3350/36045]\tLoss: 613.4757\n",
      "Training Epoch: 18 [3400/36045]\tLoss: 645.5482\n",
      "Training Epoch: 18 [3450/36045]\tLoss: 693.5257\n",
      "Training Epoch: 18 [3500/36045]\tLoss: 677.0696\n",
      "Training Epoch: 18 [3550/36045]\tLoss: 651.7947\n",
      "Training Epoch: 18 [3600/36045]\tLoss: 696.6104\n",
      "Training Epoch: 18 [3650/36045]\tLoss: 806.7280\n",
      "Training Epoch: 18 [3700/36045]\tLoss: 812.2946\n",
      "Training Epoch: 18 [3750/36045]\tLoss: 778.6617\n",
      "Training Epoch: 18 [3800/36045]\tLoss: 768.6711\n",
      "Training Epoch: 18 [3850/36045]\tLoss: 768.3599\n",
      "Training Epoch: 18 [3900/36045]\tLoss: 775.1328\n",
      "Training Epoch: 18 [3950/36045]\tLoss: 746.0403\n",
      "Training Epoch: 18 [4000/36045]\tLoss: 757.0475\n",
      "Training Epoch: 18 [4050/36045]\tLoss: 697.1797\n",
      "Training Epoch: 18 [4100/36045]\tLoss: 678.7503\n",
      "Training Epoch: 18 [4150/36045]\tLoss: 699.4728\n",
      "Training Epoch: 18 [4200/36045]\tLoss: 692.8832\n",
      "Training Epoch: 18 [4250/36045]\tLoss: 694.6489\n",
      "Training Epoch: 18 [4300/36045]\tLoss: 715.8231\n",
      "Training Epoch: 18 [4350/36045]\tLoss: 695.4808\n",
      "Training Epoch: 18 [4400/36045]\tLoss: 665.6204\n",
      "Training Epoch: 18 [4450/36045]\tLoss: 723.7982\n",
      "Training Epoch: 18 [4500/36045]\tLoss: 771.5170\n",
      "Training Epoch: 18 [4550/36045]\tLoss: 780.6564\n",
      "Training Epoch: 18 [4600/36045]\tLoss: 805.4251\n",
      "Training Epoch: 18 [4650/36045]\tLoss: 794.1255\n",
      "Training Epoch: 18 [4700/36045]\tLoss: 734.3495\n",
      "Training Epoch: 18 [4750/36045]\tLoss: 717.3108\n",
      "Training Epoch: 18 [4800/36045]\tLoss: 746.4015\n",
      "Training Epoch: 18 [4850/36045]\tLoss: 731.9318\n",
      "Training Epoch: 18 [4900/36045]\tLoss: 711.1356\n",
      "Training Epoch: 18 [4950/36045]\tLoss: 734.0118\n",
      "Training Epoch: 18 [5000/36045]\tLoss: 769.2277\n",
      "Training Epoch: 18 [5050/36045]\tLoss: 744.8091\n",
      "Training Epoch: 18 [5100/36045]\tLoss: 754.5920\n",
      "Training Epoch: 18 [5150/36045]\tLoss: 737.9653\n",
      "Training Epoch: 18 [5200/36045]\tLoss: 735.9099\n",
      "Training Epoch: 18 [5250/36045]\tLoss: 727.5330\n",
      "Training Epoch: 18 [5300/36045]\tLoss: 729.4998\n",
      "Training Epoch: 18 [5350/36045]\tLoss: 755.0587\n",
      "Training Epoch: 18 [5400/36045]\tLoss: 724.4709\n",
      "Training Epoch: 18 [5450/36045]\tLoss: 685.4282\n",
      "Training Epoch: 18 [5500/36045]\tLoss: 720.1701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [5550/36045]\tLoss: 705.7927\n",
      "Training Epoch: 18 [5600/36045]\tLoss: 798.6062\n",
      "Training Epoch: 18 [5650/36045]\tLoss: 758.5897\n",
      "Training Epoch: 18 [5700/36045]\tLoss: 714.4623\n",
      "Training Epoch: 18 [5750/36045]\tLoss: 699.5558\n",
      "Training Epoch: 18 [5800/36045]\tLoss: 740.1490\n",
      "Training Epoch: 18 [5850/36045]\tLoss: 719.9893\n",
      "Training Epoch: 18 [5900/36045]\tLoss: 829.0121\n",
      "Training Epoch: 18 [5950/36045]\tLoss: 850.6652\n",
      "Training Epoch: 18 [6000/36045]\tLoss: 834.0087\n",
      "Training Epoch: 18 [6050/36045]\tLoss: 805.0576\n",
      "Training Epoch: 18 [6100/36045]\tLoss: 811.2377\n",
      "Training Epoch: 18 [6150/36045]\tLoss: 787.2912\n",
      "Training Epoch: 18 [6200/36045]\tLoss: 786.2460\n",
      "Training Epoch: 18 [6250/36045]\tLoss: 805.9892\n",
      "Training Epoch: 18 [6300/36045]\tLoss: 820.6289\n",
      "Training Epoch: 18 [6350/36045]\tLoss: 872.6800\n",
      "Training Epoch: 18 [6400/36045]\tLoss: 733.9518\n",
      "Training Epoch: 18 [6450/36045]\tLoss: 679.8572\n",
      "Training Epoch: 18 [6500/36045]\tLoss: 694.3021\n",
      "Training Epoch: 18 [6550/36045]\tLoss: 709.5627\n",
      "Training Epoch: 18 [6600/36045]\tLoss: 712.7928\n",
      "Training Epoch: 18 [6650/36045]\tLoss: 807.7568\n",
      "Training Epoch: 18 [6700/36045]\tLoss: 845.9373\n",
      "Training Epoch: 18 [6750/36045]\tLoss: 816.7716\n",
      "Training Epoch: 18 [6800/36045]\tLoss: 820.0392\n",
      "Training Epoch: 18 [6850/36045]\tLoss: 806.5961\n",
      "Training Epoch: 18 [6900/36045]\tLoss: 714.8168\n",
      "Training Epoch: 18 [6950/36045]\tLoss: 673.8384\n",
      "Training Epoch: 18 [7000/36045]\tLoss: 717.1678\n",
      "Training Epoch: 18 [7050/36045]\tLoss: 733.0021\n",
      "Training Epoch: 18 [7100/36045]\tLoss: 732.3190\n",
      "Training Epoch: 18 [7150/36045]\tLoss: 746.5527\n",
      "Training Epoch: 18 [7200/36045]\tLoss: 750.9543\n",
      "Training Epoch: 18 [7250/36045]\tLoss: 748.2015\n",
      "Training Epoch: 18 [7300/36045]\tLoss: 734.3502\n",
      "Training Epoch: 18 [7350/36045]\tLoss: 727.0128\n",
      "Training Epoch: 18 [7400/36045]\tLoss: 651.1564\n",
      "Training Epoch: 18 [7450/36045]\tLoss: 657.3228\n",
      "Training Epoch: 18 [7500/36045]\tLoss: 651.9377\n",
      "Training Epoch: 18 [7550/36045]\tLoss: 624.2585\n",
      "Training Epoch: 18 [7600/36045]\tLoss: 699.6219\n",
      "Training Epoch: 18 [7650/36045]\tLoss: 754.6757\n",
      "Training Epoch: 18 [7700/36045]\tLoss: 720.0886\n",
      "Training Epoch: 18 [7750/36045]\tLoss: 734.2113\n",
      "Training Epoch: 18 [7800/36045]\tLoss: 721.2538\n",
      "Training Epoch: 18 [7850/36045]\tLoss: 693.1882\n",
      "Training Epoch: 18 [7900/36045]\tLoss: 731.8822\n",
      "Training Epoch: 18 [7950/36045]\tLoss: 729.2570\n",
      "Training Epoch: 18 [8000/36045]\tLoss: 747.5582\n",
      "Training Epoch: 18 [8050/36045]\tLoss: 707.5641\n",
      "Training Epoch: 18 [8100/36045]\tLoss: 735.1238\n",
      "Training Epoch: 18 [8150/36045]\tLoss: 832.3534\n",
      "Training Epoch: 18 [8200/36045]\tLoss: 818.1634\n",
      "Training Epoch: 18 [8250/36045]\tLoss: 783.8251\n",
      "Training Epoch: 18 [8300/36045]\tLoss: 850.8602\n",
      "Training Epoch: 18 [8350/36045]\tLoss: 784.2661\n",
      "Training Epoch: 18 [8400/36045]\tLoss: 703.2404\n",
      "Training Epoch: 18 [8450/36045]\tLoss: 659.5676\n",
      "Training Epoch: 18 [8500/36045]\tLoss: 699.6407\n",
      "Training Epoch: 18 [8550/36045]\tLoss: 688.9751\n",
      "Training Epoch: 18 [8600/36045]\tLoss: 681.1536\n",
      "Training Epoch: 18 [8650/36045]\tLoss: 728.4620\n",
      "Training Epoch: 18 [8700/36045]\tLoss: 770.3989\n",
      "Training Epoch: 18 [8750/36045]\tLoss: 755.0934\n",
      "Training Epoch: 18 [8800/36045]\tLoss: 760.4251\n",
      "Training Epoch: 18 [8850/36045]\tLoss: 752.8847\n",
      "Training Epoch: 18 [8900/36045]\tLoss: 679.8201\n",
      "Training Epoch: 18 [8950/36045]\tLoss: 695.9789\n",
      "Training Epoch: 18 [9000/36045]\tLoss: 710.8472\n",
      "Training Epoch: 18 [9050/36045]\tLoss: 709.8820\n",
      "Training Epoch: 18 [9100/36045]\tLoss: 731.5857\n",
      "Training Epoch: 18 [9150/36045]\tLoss: 539.5417\n",
      "Training Epoch: 18 [9200/36045]\tLoss: 407.5414\n",
      "Training Epoch: 18 [9250/36045]\tLoss: 441.3511\n",
      "Training Epoch: 18 [9300/36045]\tLoss: 455.2771\n",
      "Training Epoch: 18 [9350/36045]\tLoss: 418.0641\n",
      "Training Epoch: 18 [9400/36045]\tLoss: 819.7945\n",
      "Training Epoch: 18 [9450/36045]\tLoss: 870.9983\n",
      "Training Epoch: 18 [9500/36045]\tLoss: 857.0545\n",
      "Training Epoch: 18 [9550/36045]\tLoss: 906.3956\n",
      "Training Epoch: 18 [9600/36045]\tLoss: 673.9131\n",
      "Training Epoch: 18 [9650/36045]\tLoss: 675.8600\n",
      "Training Epoch: 18 [9700/36045]\tLoss: 660.9574\n",
      "Training Epoch: 18 [9750/36045]\tLoss: 661.7260\n",
      "Training Epoch: 18 [9800/36045]\tLoss: 857.6982\n",
      "Training Epoch: 18 [9850/36045]\tLoss: 906.0711\n",
      "Training Epoch: 18 [9900/36045]\tLoss: 926.0538\n",
      "Training Epoch: 18 [9950/36045]\tLoss: 901.3552\n",
      "Training Epoch: 18 [10000/36045]\tLoss: 831.4005\n",
      "Training Epoch: 18 [10050/36045]\tLoss: 693.0320\n",
      "Training Epoch: 18 [10100/36045]\tLoss: 697.5393\n",
      "Training Epoch: 18 [10150/36045]\tLoss: 708.9758\n",
      "Training Epoch: 18 [10200/36045]\tLoss: 698.7006\n",
      "Training Epoch: 18 [10250/36045]\tLoss: 831.2697\n",
      "Training Epoch: 18 [10300/36045]\tLoss: 806.2122\n",
      "Training Epoch: 18 [10350/36045]\tLoss: 848.3889\n",
      "Training Epoch: 18 [10400/36045]\tLoss: 839.0520\n",
      "Training Epoch: 18 [10450/36045]\tLoss: 783.9805\n",
      "Training Epoch: 18 [10500/36045]\tLoss: 659.4036\n",
      "Training Epoch: 18 [10550/36045]\tLoss: 655.1597\n",
      "Training Epoch: 18 [10600/36045]\tLoss: 679.8138\n",
      "Training Epoch: 18 [10650/36045]\tLoss: 685.6022\n",
      "Training Epoch: 18 [10700/36045]\tLoss: 777.7426\n",
      "Training Epoch: 18 [10750/36045]\tLoss: 844.8689\n",
      "Training Epoch: 18 [10800/36045]\tLoss: 782.1692\n",
      "Training Epoch: 18 [10850/36045]\tLoss: 828.4254\n",
      "Training Epoch: 18 [10900/36045]\tLoss: 862.3671\n",
      "Training Epoch: 18 [10950/36045]\tLoss: 641.4954\n",
      "Training Epoch: 18 [11000/36045]\tLoss: 635.2991\n",
      "Training Epoch: 18 [11050/36045]\tLoss: 678.1989\n",
      "Training Epoch: 18 [11100/36045]\tLoss: 691.7844\n",
      "Training Epoch: 18 [11150/36045]\tLoss: 749.5438\n",
      "Training Epoch: 18 [11200/36045]\tLoss: 778.4086\n",
      "Training Epoch: 18 [11250/36045]\tLoss: 791.6797\n",
      "Training Epoch: 18 [11300/36045]\tLoss: 771.6198\n",
      "Training Epoch: 18 [11350/36045]\tLoss: 767.7201\n",
      "Training Epoch: 18 [11400/36045]\tLoss: 724.8210\n",
      "Training Epoch: 18 [11450/36045]\tLoss: 688.3998\n",
      "Training Epoch: 18 [11500/36045]\tLoss: 686.3270\n",
      "Training Epoch: 18 [11550/36045]\tLoss: 702.4166\n",
      "Training Epoch: 18 [11600/36045]\tLoss: 769.4061\n",
      "Training Epoch: 18 [11650/36045]\tLoss: 823.9099\n",
      "Training Epoch: 18 [11700/36045]\tLoss: 823.0690\n",
      "Training Epoch: 18 [11750/36045]\tLoss: 843.6299\n",
      "Training Epoch: 18 [11800/36045]\tLoss: 887.8085\n",
      "Training Epoch: 18 [11850/36045]\tLoss: 940.1061\n",
      "Training Epoch: 18 [11900/36045]\tLoss: 1165.2789\n",
      "Training Epoch: 18 [11950/36045]\tLoss: 1164.7206\n",
      "Training Epoch: 18 [12000/36045]\tLoss: 1182.7618\n",
      "Training Epoch: 18 [12050/36045]\tLoss: 1138.9575\n",
      "Training Epoch: 18 [12100/36045]\tLoss: 760.6833\n",
      "Training Epoch: 18 [12150/36045]\tLoss: 594.6232\n",
      "Training Epoch: 18 [12200/36045]\tLoss: 588.6801\n",
      "Training Epoch: 18 [12250/36045]\tLoss: 598.9550\n",
      "Training Epoch: 18 [12300/36045]\tLoss: 754.4980\n",
      "Training Epoch: 18 [12350/36045]\tLoss: 815.5984\n",
      "Training Epoch: 18 [12400/36045]\tLoss: 825.2715\n",
      "Training Epoch: 18 [12450/36045]\tLoss: 811.7081\n",
      "Training Epoch: 18 [12500/36045]\tLoss: 843.4728\n",
      "Training Epoch: 18 [12550/36045]\tLoss: 809.5025\n",
      "Training Epoch: 18 [12600/36045]\tLoss: 750.0433\n",
      "Training Epoch: 18 [12650/36045]\tLoss: 747.8773\n",
      "Training Epoch: 18 [12700/36045]\tLoss: 771.0222\n",
      "Training Epoch: 18 [12750/36045]\tLoss: 770.8841\n",
      "Training Epoch: 18 [12800/36045]\tLoss: 750.7977\n",
      "Training Epoch: 18 [12850/36045]\tLoss: 783.8593\n",
      "Training Epoch: 18 [12900/36045]\tLoss: 751.4061\n",
      "Training Epoch: 18 [12950/36045]\tLoss: 740.3016\n",
      "Training Epoch: 18 [13000/36045]\tLoss: 772.6450\n",
      "Training Epoch: 18 [13050/36045]\tLoss: 705.2357\n",
      "Training Epoch: 18 [13100/36045]\tLoss: 732.1119\n",
      "Training Epoch: 18 [13150/36045]\tLoss: 724.2297\n",
      "Training Epoch: 18 [13200/36045]\tLoss: 696.1390\n",
      "Training Epoch: 18 [13250/36045]\tLoss: 728.3218\n",
      "Training Epoch: 18 [13300/36045]\tLoss: 769.5563\n",
      "Training Epoch: 18 [13350/36045]\tLoss: 747.3930\n",
      "Training Epoch: 18 [13400/36045]\tLoss: 752.6451\n",
      "Training Epoch: 18 [13450/36045]\tLoss: 745.3734\n",
      "Training Epoch: 18 [13500/36045]\tLoss: 771.5858\n",
      "Training Epoch: 18 [13550/36045]\tLoss: 907.7665\n",
      "Training Epoch: 18 [13600/36045]\tLoss: 940.4793\n",
      "Training Epoch: 18 [13650/36045]\tLoss: 1020.1721\n",
      "Training Epoch: 18 [13700/36045]\tLoss: 906.7407\n",
      "Training Epoch: 18 [13750/36045]\tLoss: 756.6147\n",
      "Training Epoch: 18 [13800/36045]\tLoss: 730.8751\n",
      "Training Epoch: 18 [13850/36045]\tLoss: 713.9037\n",
      "Training Epoch: 18 [13900/36045]\tLoss: 721.9882\n",
      "Training Epoch: 18 [13950/36045]\tLoss: 767.6024\n",
      "Training Epoch: 18 [14000/36045]\tLoss: 804.5732\n",
      "Training Epoch: 18 [14050/36045]\tLoss: 773.6918\n",
      "Training Epoch: 18 [14100/36045]\tLoss: 769.6288\n",
      "Training Epoch: 18 [14150/36045]\tLoss: 756.8063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [14200/36045]\tLoss: 804.9533\n",
      "Training Epoch: 18 [14250/36045]\tLoss: 879.7319\n",
      "Training Epoch: 18 [14300/36045]\tLoss: 883.8506\n",
      "Training Epoch: 18 [14350/36045]\tLoss: 846.2695\n",
      "Training Epoch: 18 [14400/36045]\tLoss: 831.5072\n",
      "Training Epoch: 18 [14450/36045]\tLoss: 871.8881\n",
      "Training Epoch: 18 [14500/36045]\tLoss: 802.5414\n",
      "Training Epoch: 18 [14550/36045]\tLoss: 835.8307\n",
      "Training Epoch: 18 [14600/36045]\tLoss: 817.0870\n",
      "Training Epoch: 18 [14650/36045]\tLoss: 819.0674\n",
      "Training Epoch: 18 [14700/36045]\tLoss: 771.1382\n",
      "Training Epoch: 18 [14750/36045]\tLoss: 661.1376\n",
      "Training Epoch: 18 [14800/36045]\tLoss: 651.3588\n",
      "Training Epoch: 18 [14850/36045]\tLoss: 658.6583\n",
      "Training Epoch: 18 [14900/36045]\tLoss: 652.3372\n",
      "Training Epoch: 18 [14950/36045]\tLoss: 660.2939\n",
      "Training Epoch: 18 [15000/36045]\tLoss: 678.6426\n",
      "Training Epoch: 18 [15050/36045]\tLoss: 677.6047\n",
      "Training Epoch: 18 [15100/36045]\tLoss: 660.7050\n",
      "Training Epoch: 18 [15150/36045]\tLoss: 651.9669\n",
      "Training Epoch: 18 [15200/36045]\tLoss: 601.8845\n",
      "Training Epoch: 18 [15250/36045]\tLoss: 628.4521\n",
      "Training Epoch: 18 [15300/36045]\tLoss: 611.6868\n",
      "Training Epoch: 18 [15350/36045]\tLoss: 626.1047\n",
      "Training Epoch: 18 [15400/36045]\tLoss: 611.3335\n",
      "Training Epoch: 18 [15450/36045]\tLoss: 598.9810\n",
      "Training Epoch: 18 [15500/36045]\tLoss: 616.7324\n",
      "Training Epoch: 18 [15550/36045]\tLoss: 610.3896\n",
      "Training Epoch: 18 [15600/36045]\tLoss: 685.0638\n",
      "Training Epoch: 18 [15650/36045]\tLoss: 706.2154\n",
      "Training Epoch: 18 [15700/36045]\tLoss: 694.6955\n",
      "Training Epoch: 18 [15750/36045]\tLoss: 686.2679\n",
      "Training Epoch: 18 [15800/36045]\tLoss: 639.8621\n",
      "Training Epoch: 18 [15850/36045]\tLoss: 651.8813\n",
      "Training Epoch: 18 [15900/36045]\tLoss: 662.0483\n",
      "Training Epoch: 18 [15950/36045]\tLoss: 682.8563\n",
      "Training Epoch: 18 [16000/36045]\tLoss: 660.6306\n",
      "Training Epoch: 18 [16050/36045]\tLoss: 630.5755\n",
      "Training Epoch: 18 [16100/36045]\tLoss: 581.9721\n",
      "Training Epoch: 18 [16150/36045]\tLoss: 567.8029\n",
      "Training Epoch: 18 [16200/36045]\tLoss: 685.2651\n",
      "Training Epoch: 18 [16250/36045]\tLoss: 715.9792\n",
      "Training Epoch: 18 [16300/36045]\tLoss: 781.9355\n",
      "Training Epoch: 18 [16350/36045]\tLoss: 798.5812\n",
      "Training Epoch: 18 [16400/36045]\tLoss: 772.1328\n",
      "Training Epoch: 18 [16450/36045]\tLoss: 751.6925\n",
      "Training Epoch: 18 [16500/36045]\tLoss: 749.8974\n",
      "Training Epoch: 18 [16550/36045]\tLoss: 711.5469\n",
      "Training Epoch: 18 [16600/36045]\tLoss: 741.4727\n",
      "Training Epoch: 18 [16650/36045]\tLoss: 763.9141\n",
      "Training Epoch: 18 [16700/36045]\tLoss: 737.2442\n",
      "Training Epoch: 18 [16750/36045]\tLoss: 728.3111\n",
      "Training Epoch: 18 [16800/36045]\tLoss: 741.8007\n",
      "Training Epoch: 18 [16850/36045]\tLoss: 705.6914\n",
      "Training Epoch: 18 [16900/36045]\tLoss: 718.2039\n",
      "Training Epoch: 18 [16950/36045]\tLoss: 746.8674\n",
      "Training Epoch: 18 [17000/36045]\tLoss: 727.2902\n",
      "Training Epoch: 18 [17050/36045]\tLoss: 760.2240\n",
      "Training Epoch: 18 [17100/36045]\tLoss: 759.7637\n",
      "Training Epoch: 18 [17150/36045]\tLoss: 659.5113\n",
      "Training Epoch: 18 [17200/36045]\tLoss: 615.5875\n",
      "Training Epoch: 18 [17250/36045]\tLoss: 644.1432\n",
      "Training Epoch: 18 [17300/36045]\tLoss: 679.5312\n",
      "Training Epoch: 18 [17350/36045]\tLoss: 651.0814\n",
      "Training Epoch: 18 [17400/36045]\tLoss: 671.0896\n",
      "Training Epoch: 18 [17450/36045]\tLoss: 693.5919\n",
      "Training Epoch: 18 [17500/36045]\tLoss: 680.6598\n",
      "Training Epoch: 18 [17550/36045]\tLoss: 683.0677\n",
      "Training Epoch: 18 [17600/36045]\tLoss: 670.8359\n",
      "Training Epoch: 18 [17650/36045]\tLoss: 689.2184\n",
      "Training Epoch: 18 [17700/36045]\tLoss: 667.8086\n",
      "Training Epoch: 18 [17750/36045]\tLoss: 685.6203\n",
      "Training Epoch: 18 [17800/36045]\tLoss: 676.6135\n",
      "Training Epoch: 18 [17850/36045]\tLoss: 674.5563\n",
      "Training Epoch: 18 [17900/36045]\tLoss: 704.6028\n",
      "Training Epoch: 18 [17950/36045]\tLoss: 714.9568\n",
      "Training Epoch: 18 [18000/36045]\tLoss: 705.1627\n",
      "Training Epoch: 18 [18050/36045]\tLoss: 793.5092\n",
      "Training Epoch: 18 [18100/36045]\tLoss: 797.3238\n",
      "Training Epoch: 18 [18150/36045]\tLoss: 805.8722\n",
      "Training Epoch: 18 [18200/36045]\tLoss: 790.0446\n",
      "Training Epoch: 18 [18250/36045]\tLoss: 810.7305\n",
      "Training Epoch: 18 [18300/36045]\tLoss: 747.1948\n",
      "Training Epoch: 18 [18350/36045]\tLoss: 815.4158\n",
      "Training Epoch: 18 [18400/36045]\tLoss: 787.2801\n",
      "Training Epoch: 18 [18450/36045]\tLoss: 768.0219\n",
      "Training Epoch: 18 [18500/36045]\tLoss: 768.3503\n",
      "Training Epoch: 18 [18550/36045]\tLoss: 754.2454\n",
      "Training Epoch: 18 [18600/36045]\tLoss: 743.9505\n",
      "Training Epoch: 18 [18650/36045]\tLoss: 794.8071\n",
      "Training Epoch: 18 [18700/36045]\tLoss: 836.7241\n",
      "Training Epoch: 18 [18750/36045]\tLoss: 819.6875\n",
      "Training Epoch: 18 [18800/36045]\tLoss: 845.6848\n",
      "Training Epoch: 18 [18850/36045]\tLoss: 788.6505\n",
      "Training Epoch: 18 [18900/36045]\tLoss: 844.7996\n",
      "Training Epoch: 18 [18950/36045]\tLoss: 781.4296\n",
      "Training Epoch: 18 [19000/36045]\tLoss: 667.1063\n",
      "Training Epoch: 18 [19050/36045]\tLoss: 645.1340\n",
      "Training Epoch: 18 [19100/36045]\tLoss: 656.8049\n",
      "Training Epoch: 18 [19150/36045]\tLoss: 645.1548\n",
      "Training Epoch: 18 [19200/36045]\tLoss: 671.8909\n",
      "Training Epoch: 18 [19250/36045]\tLoss: 686.0848\n",
      "Training Epoch: 18 [19300/36045]\tLoss: 699.1290\n",
      "Training Epoch: 18 [19350/36045]\tLoss: 682.4827\n",
      "Training Epoch: 18 [19400/36045]\tLoss: 706.5611\n",
      "Training Epoch: 18 [19450/36045]\tLoss: 695.4547\n",
      "Training Epoch: 18 [19500/36045]\tLoss: 698.4921\n",
      "Training Epoch: 18 [19550/36045]\tLoss: 697.0707\n",
      "Training Epoch: 18 [19600/36045]\tLoss: 739.6629\n",
      "Training Epoch: 18 [19650/36045]\tLoss: 966.2887\n",
      "Training Epoch: 18 [19700/36045]\tLoss: 923.6812\n",
      "Training Epoch: 18 [19750/36045]\tLoss: 924.1927\n",
      "Training Epoch: 18 [19800/36045]\tLoss: 919.0956\n",
      "Training Epoch: 18 [19850/36045]\tLoss: 624.1030\n",
      "Training Epoch: 18 [19900/36045]\tLoss: 600.4421\n",
      "Training Epoch: 18 [19950/36045]\tLoss: 604.9906\n",
      "Training Epoch: 18 [20000/36045]\tLoss: 603.4307\n",
      "Training Epoch: 18 [20050/36045]\tLoss: 674.7589\n",
      "Training Epoch: 18 [20100/36045]\tLoss: 679.5347\n",
      "Training Epoch: 18 [20150/36045]\tLoss: 682.4133\n",
      "Training Epoch: 18 [20200/36045]\tLoss: 682.7977\n",
      "Training Epoch: 18 [20250/36045]\tLoss: 727.4905\n",
      "Training Epoch: 18 [20300/36045]\tLoss: 766.6992\n",
      "Training Epoch: 18 [20350/36045]\tLoss: 788.8016\n",
      "Training Epoch: 18 [20400/36045]\tLoss: 803.8090\n",
      "Training Epoch: 18 [20450/36045]\tLoss: 775.8404\n",
      "Training Epoch: 18 [20500/36045]\tLoss: 756.5117\n",
      "Training Epoch: 18 [20550/36045]\tLoss: 667.4064\n",
      "Training Epoch: 18 [20600/36045]\tLoss: 681.2197\n",
      "Training Epoch: 18 [20650/36045]\tLoss: 677.5281\n",
      "Training Epoch: 18 [20700/36045]\tLoss: 664.0259\n",
      "Training Epoch: 18 [20750/36045]\tLoss: 711.1473\n",
      "Training Epoch: 18 [20800/36045]\tLoss: 774.5478\n",
      "Training Epoch: 18 [20850/36045]\tLoss: 762.7391\n",
      "Training Epoch: 18 [20900/36045]\tLoss: 812.3927\n",
      "Training Epoch: 18 [20950/36045]\tLoss: 766.2792\n",
      "Training Epoch: 18 [21000/36045]\tLoss: 723.0866\n",
      "Training Epoch: 18 [21050/36045]\tLoss: 620.3758\n",
      "Training Epoch: 18 [21100/36045]\tLoss: 621.5989\n",
      "Training Epoch: 18 [21150/36045]\tLoss: 665.3118\n",
      "Training Epoch: 18 [21200/36045]\tLoss: 664.9169\n",
      "Training Epoch: 18 [21250/36045]\tLoss: 635.5983\n",
      "Training Epoch: 18 [21300/36045]\tLoss: 742.2803\n",
      "Training Epoch: 18 [21350/36045]\tLoss: 737.1535\n",
      "Training Epoch: 18 [21400/36045]\tLoss: 740.2376\n",
      "Training Epoch: 18 [21450/36045]\tLoss: 748.0991\n",
      "Training Epoch: 18 [21500/36045]\tLoss: 751.3928\n",
      "Training Epoch: 18 [21550/36045]\tLoss: 851.0142\n",
      "Training Epoch: 18 [21600/36045]\tLoss: 852.5081\n",
      "Training Epoch: 18 [21650/36045]\tLoss: 867.0433\n",
      "Training Epoch: 18 [21700/36045]\tLoss: 864.2974\n",
      "Training Epoch: 18 [21750/36045]\tLoss: 833.2330\n",
      "Training Epoch: 18 [21800/36045]\tLoss: 618.5607\n",
      "Training Epoch: 18 [21850/36045]\tLoss: 600.3835\n",
      "Training Epoch: 18 [21900/36045]\tLoss: 613.9178\n",
      "Training Epoch: 18 [21950/36045]\tLoss: 610.6617\n",
      "Training Epoch: 18 [22000/36045]\tLoss: 617.3776\n",
      "Training Epoch: 18 [22050/36045]\tLoss: 649.0146\n",
      "Training Epoch: 18 [22100/36045]\tLoss: 639.6677\n",
      "Training Epoch: 18 [22150/36045]\tLoss: 619.7689\n",
      "Training Epoch: 18 [22200/36045]\tLoss: 639.5529\n",
      "Training Epoch: 18 [22250/36045]\tLoss: 645.5465\n",
      "Training Epoch: 18 [22300/36045]\tLoss: 702.0172\n",
      "Training Epoch: 18 [22350/36045]\tLoss: 730.3748\n",
      "Training Epoch: 18 [22400/36045]\tLoss: 747.7705\n",
      "Training Epoch: 18 [22450/36045]\tLoss: 734.4808\n",
      "Training Epoch: 18 [22500/36045]\tLoss: 713.1324\n",
      "Training Epoch: 18 [22550/36045]\tLoss: 755.9506\n",
      "Training Epoch: 18 [22600/36045]\tLoss: 826.2830\n",
      "Training Epoch: 18 [22650/36045]\tLoss: 866.3395\n",
      "Training Epoch: 18 [22700/36045]\tLoss: 890.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [22750/36045]\tLoss: 913.0468\n",
      "Training Epoch: 18 [22800/36045]\tLoss: 950.4750\n",
      "Training Epoch: 18 [22850/36045]\tLoss: 788.2927\n",
      "Training Epoch: 18 [22900/36045]\tLoss: 792.3182\n",
      "Training Epoch: 18 [22950/36045]\tLoss: 767.9648\n",
      "Training Epoch: 18 [23000/36045]\tLoss: 768.7972\n",
      "Training Epoch: 18 [23050/36045]\tLoss: 688.4224\n",
      "Training Epoch: 18 [23100/36045]\tLoss: 704.7241\n",
      "Training Epoch: 18 [23150/36045]\tLoss: 692.0244\n",
      "Training Epoch: 18 [23200/36045]\tLoss: 654.6692\n",
      "Training Epoch: 18 [23250/36045]\tLoss: 658.6788\n",
      "Training Epoch: 18 [23300/36045]\tLoss: 655.1260\n",
      "Training Epoch: 18 [23350/36045]\tLoss: 677.4891\n",
      "Training Epoch: 18 [23400/36045]\tLoss: 734.6831\n",
      "Training Epoch: 18 [23450/36045]\tLoss: 725.5522\n",
      "Training Epoch: 18 [23500/36045]\tLoss: 698.5167\n",
      "Training Epoch: 18 [23550/36045]\tLoss: 751.3903\n",
      "Training Epoch: 18 [23600/36045]\tLoss: 845.5060\n",
      "Training Epoch: 18 [23650/36045]\tLoss: 860.5526\n",
      "Training Epoch: 18 [23700/36045]\tLoss: 872.1814\n",
      "Training Epoch: 18 [23750/36045]\tLoss: 842.9966\n",
      "Training Epoch: 18 [23800/36045]\tLoss: 666.6702\n",
      "Training Epoch: 18 [23850/36045]\tLoss: 695.2618\n",
      "Training Epoch: 18 [23900/36045]\tLoss: 685.8583\n",
      "Training Epoch: 18 [23950/36045]\tLoss: 666.9727\n",
      "Training Epoch: 18 [24000/36045]\tLoss: 642.5292\n",
      "Training Epoch: 18 [24050/36045]\tLoss: 594.4528\n",
      "Training Epoch: 18 [24100/36045]\tLoss: 626.0251\n",
      "Training Epoch: 18 [24150/36045]\tLoss: 621.9740\n",
      "Training Epoch: 18 [24200/36045]\tLoss: 614.5536\n",
      "Training Epoch: 18 [24250/36045]\tLoss: 597.1128\n",
      "Training Epoch: 18 [24300/36045]\tLoss: 642.8383\n",
      "Training Epoch: 18 [24350/36045]\tLoss: 658.6754\n",
      "Training Epoch: 18 [24400/36045]\tLoss: 677.7990\n",
      "Training Epoch: 18 [24450/36045]\tLoss: 646.6584\n",
      "Training Epoch: 18 [24500/36045]\tLoss: 681.2959\n",
      "Training Epoch: 18 [24550/36045]\tLoss: 776.0743\n",
      "Training Epoch: 18 [24600/36045]\tLoss: 769.3162\n",
      "Training Epoch: 18 [24650/36045]\tLoss: 739.1409\n",
      "Training Epoch: 18 [24700/36045]\tLoss: 751.0203\n",
      "Training Epoch: 18 [24750/36045]\tLoss: 694.1877\n",
      "Training Epoch: 18 [24800/36045]\tLoss: 583.7372\n",
      "Training Epoch: 18 [24850/36045]\tLoss: 604.9458\n",
      "Training Epoch: 18 [24900/36045]\tLoss: 601.2800\n",
      "Training Epoch: 18 [24950/36045]\tLoss: 603.1080\n",
      "Training Epoch: 18 [25000/36045]\tLoss: 578.5093\n",
      "Training Epoch: 18 [25050/36045]\tLoss: 552.2877\n",
      "Training Epoch: 18 [25100/36045]\tLoss: 496.6065\n",
      "Training Epoch: 18 [25150/36045]\tLoss: 460.5960\n",
      "Training Epoch: 18 [25200/36045]\tLoss: 455.9781\n",
      "Training Epoch: 18 [25250/36045]\tLoss: 487.2186\n",
      "Training Epoch: 18 [25300/36045]\tLoss: 638.2297\n",
      "Training Epoch: 18 [25350/36045]\tLoss: 636.5864\n",
      "Training Epoch: 18 [25400/36045]\tLoss: 592.9312\n",
      "Training Epoch: 18 [25450/36045]\tLoss: 595.6912\n",
      "Training Epoch: 18 [25500/36045]\tLoss: 648.3915\n",
      "Training Epoch: 18 [25550/36045]\tLoss: 749.5485\n",
      "Training Epoch: 18 [25600/36045]\tLoss: 755.7024\n",
      "Training Epoch: 18 [25650/36045]\tLoss: 727.7115\n",
      "Training Epoch: 18 [25700/36045]\tLoss: 738.0582\n",
      "Training Epoch: 18 [25750/36045]\tLoss: 711.3534\n",
      "Training Epoch: 18 [25800/36045]\tLoss: 447.9212\n",
      "Training Epoch: 18 [25850/36045]\tLoss: 458.6756\n",
      "Training Epoch: 18 [25900/36045]\tLoss: 438.8332\n",
      "Training Epoch: 18 [25950/36045]\tLoss: 448.0716\n",
      "Training Epoch: 18 [26000/36045]\tLoss: 548.9703\n",
      "Training Epoch: 18 [26050/36045]\tLoss: 745.2289\n",
      "Training Epoch: 18 [26100/36045]\tLoss: 774.9786\n",
      "Training Epoch: 18 [26150/36045]\tLoss: 773.3121\n",
      "Training Epoch: 18 [26200/36045]\tLoss: 746.5953\n",
      "Training Epoch: 18 [26250/36045]\tLoss: 779.8448\n",
      "Training Epoch: 18 [26300/36045]\tLoss: 691.2027\n",
      "Training Epoch: 18 [26350/36045]\tLoss: 700.2125\n",
      "Training Epoch: 18 [26400/36045]\tLoss: 678.7917\n",
      "Training Epoch: 18 [26450/36045]\tLoss: 607.4694\n",
      "Training Epoch: 18 [26500/36045]\tLoss: 728.2515\n",
      "Training Epoch: 18 [26550/36045]\tLoss: 734.9167\n",
      "Training Epoch: 18 [26600/36045]\tLoss: 729.2861\n",
      "Training Epoch: 18 [26650/36045]\tLoss: 745.9980\n",
      "Training Epoch: 18 [26700/36045]\tLoss: 725.7929\n",
      "Training Epoch: 18 [26750/36045]\tLoss: 678.2902\n",
      "Training Epoch: 18 [26800/36045]\tLoss: 498.2929\n",
      "Training Epoch: 18 [26850/36045]\tLoss: 415.4590\n",
      "Training Epoch: 18 [26900/36045]\tLoss: 418.8074\n",
      "Training Epoch: 18 [26950/36045]\tLoss: 460.5176\n",
      "Training Epoch: 18 [27000/36045]\tLoss: 736.9419\n",
      "Training Epoch: 18 [27050/36045]\tLoss: 774.8114\n",
      "Training Epoch: 18 [27100/36045]\tLoss: 749.9978\n",
      "Training Epoch: 18 [27150/36045]\tLoss: 793.9280\n",
      "Training Epoch: 18 [27200/36045]\tLoss: 587.0137\n",
      "Training Epoch: 18 [27250/36045]\tLoss: 582.6589\n",
      "Training Epoch: 18 [27300/36045]\tLoss: 566.0835\n",
      "Training Epoch: 18 [27350/36045]\tLoss: 566.3347\n",
      "Training Epoch: 18 [27400/36045]\tLoss: 565.3995\n",
      "Training Epoch: 18 [27450/36045]\tLoss: 711.5005\n",
      "Training Epoch: 18 [27500/36045]\tLoss: 764.2438\n",
      "Training Epoch: 18 [27550/36045]\tLoss: 756.5596\n",
      "Training Epoch: 18 [27600/36045]\tLoss: 766.6992\n",
      "Training Epoch: 18 [27650/36045]\tLoss: 759.5716\n",
      "Training Epoch: 18 [27700/36045]\tLoss: 789.9261\n",
      "Training Epoch: 18 [27750/36045]\tLoss: 802.8071\n",
      "Training Epoch: 18 [27800/36045]\tLoss: 788.1375\n",
      "Training Epoch: 18 [27850/36045]\tLoss: 774.1844\n",
      "Training Epoch: 18 [27900/36045]\tLoss: 694.3032\n",
      "Training Epoch: 18 [27950/36045]\tLoss: 574.6696\n",
      "Training Epoch: 18 [28000/36045]\tLoss: 548.0314\n",
      "Training Epoch: 18 [28050/36045]\tLoss: 561.8948\n",
      "Training Epoch: 18 [28100/36045]\tLoss: 552.7400\n",
      "Training Epoch: 18 [28150/36045]\tLoss: 585.1221\n",
      "Training Epoch: 18 [28200/36045]\tLoss: 588.8669\n",
      "Training Epoch: 18 [28250/36045]\tLoss: 586.0583\n",
      "Training Epoch: 18 [28300/36045]\tLoss: 553.7385\n",
      "Training Epoch: 18 [28350/36045]\tLoss: 549.6730\n",
      "Training Epoch: 18 [28400/36045]\tLoss: 889.0826\n",
      "Training Epoch: 18 [28450/36045]\tLoss: 806.4708\n",
      "Training Epoch: 18 [28500/36045]\tLoss: 699.7454\n",
      "Training Epoch: 18 [28550/36045]\tLoss: 642.4647\n",
      "Training Epoch: 18 [28600/36045]\tLoss: 692.1284\n",
      "Training Epoch: 18 [28650/36045]\tLoss: 788.4499\n",
      "Training Epoch: 18 [28700/36045]\tLoss: 781.1784\n",
      "Training Epoch: 18 [28750/36045]\tLoss: 769.9219\n",
      "Training Epoch: 18 [28800/36045]\tLoss: 779.2956\n",
      "Training Epoch: 18 [28850/36045]\tLoss: 674.4708\n",
      "Training Epoch: 18 [28900/36045]\tLoss: 540.0289\n",
      "Training Epoch: 18 [28950/36045]\tLoss: 535.6552\n",
      "Training Epoch: 18 [29000/36045]\tLoss: 535.7487\n",
      "Training Epoch: 18 [29050/36045]\tLoss: 544.9390\n",
      "Training Epoch: 18 [29100/36045]\tLoss: 566.8964\n",
      "Training Epoch: 18 [29150/36045]\tLoss: 552.9004\n",
      "Training Epoch: 18 [29200/36045]\tLoss: 538.6854\n",
      "Training Epoch: 18 [29250/36045]\tLoss: 524.1971\n",
      "Training Epoch: 18 [29300/36045]\tLoss: 602.8344\n",
      "Training Epoch: 18 [29350/36045]\tLoss: 718.1535\n",
      "Training Epoch: 18 [29400/36045]\tLoss: 737.3244\n",
      "Training Epoch: 18 [29450/36045]\tLoss: 764.1967\n",
      "Training Epoch: 18 [29500/36045]\tLoss: 778.7939\n",
      "Training Epoch: 18 [29550/36045]\tLoss: 739.2455\n",
      "Training Epoch: 18 [29600/36045]\tLoss: 627.4871\n",
      "Training Epoch: 18 [29650/36045]\tLoss: 610.4260\n",
      "Training Epoch: 18 [29700/36045]\tLoss: 542.2507\n",
      "Training Epoch: 18 [29750/36045]\tLoss: 545.7754\n",
      "Training Epoch: 18 [29800/36045]\tLoss: 592.3047\n",
      "Training Epoch: 18 [29850/36045]\tLoss: 664.1407\n",
      "Training Epoch: 18 [29900/36045]\tLoss: 659.3101\n",
      "Training Epoch: 18 [29950/36045]\tLoss: 682.0192\n",
      "Training Epoch: 18 [30000/36045]\tLoss: 660.6397\n",
      "Training Epoch: 18 [30050/36045]\tLoss: 666.9424\n",
      "Training Epoch: 18 [30100/36045]\tLoss: 813.5242\n",
      "Training Epoch: 18 [30150/36045]\tLoss: 799.1907\n",
      "Training Epoch: 18 [30200/36045]\tLoss: 754.1784\n",
      "Training Epoch: 18 [30250/36045]\tLoss: 803.8796\n",
      "Training Epoch: 18 [30300/36045]\tLoss: 791.1248\n",
      "Training Epoch: 18 [30350/36045]\tLoss: 627.7290\n",
      "Training Epoch: 18 [30400/36045]\tLoss: 614.6281\n",
      "Training Epoch: 18 [30450/36045]\tLoss: 613.4911\n",
      "Training Epoch: 18 [30500/36045]\tLoss: 572.7965\n",
      "Training Epoch: 18 [30550/36045]\tLoss: 531.6961\n",
      "Training Epoch: 18 [30600/36045]\tLoss: 513.0724\n",
      "Training Epoch: 18 [30650/36045]\tLoss: 504.5199\n",
      "Training Epoch: 18 [30700/36045]\tLoss: 523.1646\n",
      "Training Epoch: 18 [30750/36045]\tLoss: 508.0247\n",
      "Training Epoch: 18 [30800/36045]\tLoss: 541.4046\n",
      "Training Epoch: 18 [30850/36045]\tLoss: 532.1025\n",
      "Training Epoch: 18 [30900/36045]\tLoss: 547.4440\n",
      "Training Epoch: 18 [30950/36045]\tLoss: 575.3816\n",
      "Training Epoch: 18 [31000/36045]\tLoss: 565.1669\n",
      "Training Epoch: 18 [31050/36045]\tLoss: 471.0730\n",
      "Training Epoch: 18 [31100/36045]\tLoss: 461.3927\n",
      "Training Epoch: 18 [31150/36045]\tLoss: 467.2889\n",
      "Training Epoch: 18 [31200/36045]\tLoss: 586.1894\n",
      "Training Epoch: 18 [31250/36045]\tLoss: 761.2325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [31300/36045]\tLoss: 728.5883\n",
      "Training Epoch: 18 [31350/36045]\tLoss: 745.0920\n",
      "Training Epoch: 18 [31400/36045]\tLoss: 725.0279\n",
      "Training Epoch: 18 [31450/36045]\tLoss: 734.3313\n",
      "Training Epoch: 18 [31500/36045]\tLoss: 743.8005\n",
      "Training Epoch: 18 [31550/36045]\tLoss: 754.1805\n",
      "Training Epoch: 18 [31600/36045]\tLoss: 708.5283\n",
      "Training Epoch: 18 [31650/36045]\tLoss: 755.7968\n",
      "Training Epoch: 18 [31700/36045]\tLoss: 554.3867\n",
      "Training Epoch: 18 [31750/36045]\tLoss: 461.9562\n",
      "Training Epoch: 18 [31800/36045]\tLoss: 439.5546\n",
      "Training Epoch: 18 [31850/36045]\tLoss: 450.6321\n",
      "Training Epoch: 18 [31900/36045]\tLoss: 696.0268\n",
      "Training Epoch: 18 [31950/36045]\tLoss: 888.4474\n",
      "Training Epoch: 18 [32000/36045]\tLoss: 1008.4698\n",
      "Training Epoch: 18 [32050/36045]\tLoss: 960.2991\n",
      "Training Epoch: 18 [32100/36045]\tLoss: 947.7491\n",
      "Training Epoch: 18 [32150/36045]\tLoss: 748.9001\n",
      "Training Epoch: 18 [32200/36045]\tLoss: 754.1441\n",
      "Training Epoch: 18 [32250/36045]\tLoss: 765.8687\n",
      "Training Epoch: 18 [32300/36045]\tLoss: 747.9229\n",
      "Training Epoch: 18 [32350/36045]\tLoss: 741.0634\n",
      "Training Epoch: 18 [32400/36045]\tLoss: 696.9968\n",
      "Training Epoch: 18 [32450/36045]\tLoss: 575.4779\n",
      "Training Epoch: 18 [32500/36045]\tLoss: 553.8895\n",
      "Training Epoch: 18 [32550/36045]\tLoss: 557.0760\n",
      "Training Epoch: 18 [32600/36045]\tLoss: 552.4156\n",
      "Training Epoch: 18 [32650/36045]\tLoss: 694.9960\n",
      "Training Epoch: 18 [32700/36045]\tLoss: 755.8503\n",
      "Training Epoch: 18 [32750/36045]\tLoss: 720.1100\n",
      "Training Epoch: 18 [32800/36045]\tLoss: 738.3208\n",
      "Training Epoch: 18 [32850/36045]\tLoss: 684.0565\n",
      "Training Epoch: 18 [32900/36045]\tLoss: 555.3326\n",
      "Training Epoch: 18 [32950/36045]\tLoss: 580.9323\n",
      "Training Epoch: 18 [33000/36045]\tLoss: 581.0879\n",
      "Training Epoch: 18 [33050/36045]\tLoss: 550.2483\n",
      "Training Epoch: 18 [33100/36045]\tLoss: 627.5705\n",
      "Training Epoch: 18 [33150/36045]\tLoss: 847.1000\n",
      "Training Epoch: 18 [33200/36045]\tLoss: 826.2838\n",
      "Training Epoch: 18 [33250/36045]\tLoss: 850.1340\n",
      "Training Epoch: 18 [33300/36045]\tLoss: 905.0334\n",
      "Training Epoch: 18 [33350/36045]\tLoss: 695.1247\n",
      "Training Epoch: 18 [33400/36045]\tLoss: 516.2423\n",
      "Training Epoch: 18 [33450/36045]\tLoss: 511.1715\n",
      "Training Epoch: 18 [33500/36045]\tLoss: 525.3237\n",
      "Training Epoch: 18 [33550/36045]\tLoss: 545.3414\n",
      "Training Epoch: 18 [33600/36045]\tLoss: 547.0833\n",
      "Training Epoch: 18 [33650/36045]\tLoss: 724.9499\n",
      "Training Epoch: 18 [33700/36045]\tLoss: 702.4047\n",
      "Training Epoch: 18 [33750/36045]\tLoss: 726.8156\n",
      "Training Epoch: 18 [33800/36045]\tLoss: 721.6282\n",
      "Training Epoch: 18 [33850/36045]\tLoss: 725.4273\n",
      "Training Epoch: 18 [33900/36045]\tLoss: 733.3654\n",
      "Training Epoch: 18 [33950/36045]\tLoss: 745.5891\n",
      "Training Epoch: 18 [34000/36045]\tLoss: 735.0043\n",
      "Training Epoch: 18 [34050/36045]\tLoss: 739.5286\n",
      "Training Epoch: 18 [34100/36045]\tLoss: 710.4044\n",
      "Training Epoch: 18 [34150/36045]\tLoss: 661.8644\n",
      "Training Epoch: 18 [34200/36045]\tLoss: 627.4550\n",
      "Training Epoch: 18 [34250/36045]\tLoss: 641.8737\n",
      "Training Epoch: 18 [34300/36045]\tLoss: 551.4946\n",
      "Training Epoch: 18 [34350/36045]\tLoss: 579.1323\n",
      "Training Epoch: 18 [34400/36045]\tLoss: 567.5915\n",
      "Training Epoch: 18 [34450/36045]\tLoss: 533.1526\n",
      "Training Epoch: 18 [34500/36045]\tLoss: 570.3457\n",
      "Training Epoch: 18 [34550/36045]\tLoss: 561.0587\n",
      "Training Epoch: 18 [34600/36045]\tLoss: 557.4911\n",
      "Training Epoch: 18 [34650/36045]\tLoss: 670.6299\n",
      "Training Epoch: 18 [34700/36045]\tLoss: 707.8811\n",
      "Training Epoch: 18 [34750/36045]\tLoss: 630.2393\n",
      "Training Epoch: 18 [34800/36045]\tLoss: 717.2932\n",
      "Training Epoch: 18 [34850/36045]\tLoss: 727.6347\n",
      "Training Epoch: 18 [34900/36045]\tLoss: 818.2822\n",
      "Training Epoch: 18 [34950/36045]\tLoss: 806.9902\n",
      "Training Epoch: 18 [35000/36045]\tLoss: 808.5603\n",
      "Training Epoch: 18 [35050/36045]\tLoss: 793.5510\n",
      "Training Epoch: 18 [35100/36045]\tLoss: 652.3239\n",
      "Training Epoch: 18 [35150/36045]\tLoss: 645.7376\n",
      "Training Epoch: 18 [35200/36045]\tLoss: 552.4072\n",
      "Training Epoch: 18 [35250/36045]\tLoss: 605.0366\n",
      "Training Epoch: 18 [35300/36045]\tLoss: 618.1377\n",
      "Training Epoch: 18 [35350/36045]\tLoss: 714.3276\n",
      "Training Epoch: 18 [35400/36045]\tLoss: 757.1781\n",
      "Training Epoch: 18 [35450/36045]\tLoss: 722.5162\n",
      "Training Epoch: 18 [35500/36045]\tLoss: 704.2122\n",
      "Training Epoch: 18 [35550/36045]\tLoss: 688.3666\n",
      "Training Epoch: 18 [35600/36045]\tLoss: 734.9418\n",
      "Training Epoch: 18 [35650/36045]\tLoss: 812.1570\n",
      "Training Epoch: 18 [35700/36045]\tLoss: 738.5125\n",
      "Training Epoch: 18 [35750/36045]\tLoss: 799.4338\n",
      "Training Epoch: 18 [35800/36045]\tLoss: 805.2606\n",
      "Training Epoch: 18 [35850/36045]\tLoss: 779.5738\n",
      "Training Epoch: 18 [35900/36045]\tLoss: 810.7458\n",
      "Training Epoch: 18 [35950/36045]\tLoss: 809.8152\n",
      "Training Epoch: 18 [36000/36045]\tLoss: 798.7205\n",
      "Training Epoch: 18 [36045/36045]\tLoss: 779.6793\n",
      "Training Epoch: 18 [4004/4004]\tLoss: 730.1300\n",
      "Training Epoch: 19 [50/36045]\tLoss: 730.8893\n",
      "Training Epoch: 19 [100/36045]\tLoss: 700.5374\n",
      "Training Epoch: 19 [150/36045]\tLoss: 699.2218\n",
      "Training Epoch: 19 [200/36045]\tLoss: 686.4261\n",
      "Training Epoch: 19 [250/36045]\tLoss: 812.7155\n",
      "Training Epoch: 19 [300/36045]\tLoss: 875.7836\n",
      "Training Epoch: 19 [350/36045]\tLoss: 838.7861\n",
      "Training Epoch: 19 [400/36045]\tLoss: 836.9097\n",
      "Training Epoch: 19 [450/36045]\tLoss: 815.5776\n",
      "Training Epoch: 19 [500/36045]\tLoss: 762.1125\n",
      "Training Epoch: 19 [550/36045]\tLoss: 771.2371\n",
      "Training Epoch: 19 [600/36045]\tLoss: 745.7060\n",
      "Training Epoch: 19 [650/36045]\tLoss: 772.2056\n",
      "Training Epoch: 19 [700/36045]\tLoss: 759.6111\n",
      "Training Epoch: 19 [750/36045]\tLoss: 739.1711\n",
      "Training Epoch: 19 [800/36045]\tLoss: 756.5161\n",
      "Training Epoch: 19 [850/36045]\tLoss: 736.5805\n",
      "Training Epoch: 19 [900/36045]\tLoss: 700.3727\n",
      "Training Epoch: 19 [950/36045]\tLoss: 665.0009\n",
      "Training Epoch: 19 [1000/36045]\tLoss: 638.1135\n",
      "Training Epoch: 19 [1050/36045]\tLoss: 639.6207\n",
      "Training Epoch: 19 [1100/36045]\tLoss: 622.7677\n",
      "Training Epoch: 19 [1150/36045]\tLoss: 631.7338\n",
      "Training Epoch: 19 [1200/36045]\tLoss: 667.5626\n",
      "Training Epoch: 19 [1250/36045]\tLoss: 762.1362\n",
      "Training Epoch: 19 [1300/36045]\tLoss: 767.2838\n",
      "Training Epoch: 19 [1350/36045]\tLoss: 770.5739\n",
      "Training Epoch: 19 [1400/36045]\tLoss: 800.0494\n",
      "Training Epoch: 19 [1450/36045]\tLoss: 772.9102\n",
      "Training Epoch: 19 [1500/36045]\tLoss: 714.5980\n",
      "Training Epoch: 19 [1550/36045]\tLoss: 733.1290\n",
      "Training Epoch: 19 [1600/36045]\tLoss: 744.1162\n",
      "Training Epoch: 19 [1650/36045]\tLoss: 729.9672\n",
      "Training Epoch: 19 [1700/36045]\tLoss: 740.8265\n",
      "Training Epoch: 19 [1750/36045]\tLoss: 782.6399\n",
      "Training Epoch: 19 [1800/36045]\tLoss: 765.2972\n",
      "Training Epoch: 19 [1850/36045]\tLoss: 787.4393\n",
      "Training Epoch: 19 [1900/36045]\tLoss: 736.1367\n",
      "Training Epoch: 19 [1950/36045]\tLoss: 747.4835\n",
      "Training Epoch: 19 [2000/36045]\tLoss: 675.1334\n",
      "Training Epoch: 19 [2050/36045]\tLoss: 679.3825\n",
      "Training Epoch: 19 [2100/36045]\tLoss: 716.3561\n",
      "Training Epoch: 19 [2150/36045]\tLoss: 694.1514\n",
      "Training Epoch: 19 [2200/36045]\tLoss: 643.3834\n",
      "Training Epoch: 19 [2250/36045]\tLoss: 608.1981\n",
      "Training Epoch: 19 [2300/36045]\tLoss: 637.7088\n",
      "Training Epoch: 19 [2350/36045]\tLoss: 608.5908\n",
      "Training Epoch: 19 [2400/36045]\tLoss: 621.4101\n",
      "Training Epoch: 19 [2450/36045]\tLoss: 786.5518\n",
      "Training Epoch: 19 [2500/36045]\tLoss: 825.9465\n",
      "Training Epoch: 19 [2550/36045]\tLoss: 822.4191\n",
      "Training Epoch: 19 [2600/36045]\tLoss: 831.9340\n",
      "Training Epoch: 19 [2650/36045]\tLoss: 958.0757\n",
      "Training Epoch: 19 [2700/36045]\tLoss: 1044.9943\n",
      "Training Epoch: 19 [2750/36045]\tLoss: 1120.0990\n",
      "Training Epoch: 19 [2800/36045]\tLoss: 1131.8962\n",
      "Training Epoch: 19 [2850/36045]\tLoss: 902.0134\n",
      "Training Epoch: 19 [2900/36045]\tLoss: 870.2836\n",
      "Training Epoch: 19 [2950/36045]\tLoss: 838.9133\n",
      "Training Epoch: 19 [3000/36045]\tLoss: 835.1284\n",
      "Training Epoch: 19 [3050/36045]\tLoss: 867.2661\n",
      "Training Epoch: 19 [3100/36045]\tLoss: 793.5020\n",
      "Training Epoch: 19 [3150/36045]\tLoss: 612.2910\n",
      "Training Epoch: 19 [3200/36045]\tLoss: 636.1979\n",
      "Training Epoch: 19 [3250/36045]\tLoss: 598.7070\n",
      "Training Epoch: 19 [3300/36045]\tLoss: 567.3742\n",
      "Training Epoch: 19 [3350/36045]\tLoss: 598.2885\n",
      "Training Epoch: 19 [3400/36045]\tLoss: 629.4740\n",
      "Training Epoch: 19 [3450/36045]\tLoss: 675.9517\n",
      "Training Epoch: 19 [3500/36045]\tLoss: 660.4090\n",
      "Training Epoch: 19 [3550/36045]\tLoss: 635.4816\n",
      "Training Epoch: 19 [3600/36045]\tLoss: 679.4325\n",
      "Training Epoch: 19 [3650/36045]\tLoss: 786.4109\n",
      "Training Epoch: 19 [3700/36045]\tLoss: 792.0756\n",
      "Training Epoch: 19 [3750/36045]\tLoss: 758.7932\n",
      "Training Epoch: 19 [3800/36045]\tLoss: 749.3954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [3850/36045]\tLoss: 748.7627\n",
      "Training Epoch: 19 [3900/36045]\tLoss: 755.2476\n",
      "Training Epoch: 19 [3950/36045]\tLoss: 727.3248\n",
      "Training Epoch: 19 [4000/36045]\tLoss: 737.3737\n",
      "Training Epoch: 19 [4050/36045]\tLoss: 679.0633\n",
      "Training Epoch: 19 [4100/36045]\tLoss: 661.2886\n",
      "Training Epoch: 19 [4150/36045]\tLoss: 681.1382\n",
      "Training Epoch: 19 [4200/36045]\tLoss: 674.7968\n",
      "Training Epoch: 19 [4250/36045]\tLoss: 676.5948\n",
      "Training Epoch: 19 [4300/36045]\tLoss: 697.3478\n",
      "Training Epoch: 19 [4350/36045]\tLoss: 677.4197\n",
      "Training Epoch: 19 [4400/36045]\tLoss: 648.6329\n",
      "Training Epoch: 19 [4450/36045]\tLoss: 706.1191\n",
      "Training Epoch: 19 [4500/36045]\tLoss: 753.0314\n",
      "Training Epoch: 19 [4550/36045]\tLoss: 761.3787\n",
      "Training Epoch: 19 [4600/36045]\tLoss: 785.8607\n",
      "Training Epoch: 19 [4650/36045]\tLoss: 774.2742\n",
      "Training Epoch: 19 [4700/36045]\tLoss: 716.1105\n",
      "Training Epoch: 19 [4750/36045]\tLoss: 698.9664\n",
      "Training Epoch: 19 [4800/36045]\tLoss: 727.6544\n",
      "Training Epoch: 19 [4850/36045]\tLoss: 713.5773\n",
      "Training Epoch: 19 [4900/36045]\tLoss: 693.5202\n",
      "Training Epoch: 19 [4950/36045]\tLoss: 714.9604\n",
      "Training Epoch: 19 [5000/36045]\tLoss: 748.9695\n",
      "Training Epoch: 19 [5050/36045]\tLoss: 725.2975\n",
      "Training Epoch: 19 [5100/36045]\tLoss: 735.1530\n",
      "Training Epoch: 19 [5150/36045]\tLoss: 719.0989\n",
      "Training Epoch: 19 [5200/36045]\tLoss: 717.2465\n",
      "Training Epoch: 19 [5250/36045]\tLoss: 709.2396\n",
      "Training Epoch: 19 [5300/36045]\tLoss: 710.9360\n",
      "Training Epoch: 19 [5350/36045]\tLoss: 735.8203\n",
      "Training Epoch: 19 [5400/36045]\tLoss: 706.4745\n",
      "Training Epoch: 19 [5450/36045]\tLoss: 668.9409\n",
      "Training Epoch: 19 [5500/36045]\tLoss: 702.4853\n",
      "Training Epoch: 19 [5550/36045]\tLoss: 688.9805\n",
      "Training Epoch: 19 [5600/36045]\tLoss: 779.4755\n",
      "Training Epoch: 19 [5650/36045]\tLoss: 740.0901\n",
      "Training Epoch: 19 [5700/36045]\tLoss: 696.5352\n",
      "Training Epoch: 19 [5750/36045]\tLoss: 681.6525\n",
      "Training Epoch: 19 [5800/36045]\tLoss: 720.6736\n",
      "Training Epoch: 19 [5850/36045]\tLoss: 701.8053\n",
      "Training Epoch: 19 [5900/36045]\tLoss: 808.4180\n",
      "Training Epoch: 19 [5950/36045]\tLoss: 829.1078\n",
      "Training Epoch: 19 [6000/36045]\tLoss: 812.5938\n",
      "Training Epoch: 19 [6050/36045]\tLoss: 784.6122\n",
      "Training Epoch: 19 [6100/36045]\tLoss: 790.5634\n",
      "Training Epoch: 19 [6150/36045]\tLoss: 768.3582\n",
      "Training Epoch: 19 [6200/36045]\tLoss: 767.9763\n",
      "Training Epoch: 19 [6250/36045]\tLoss: 788.0858\n",
      "Training Epoch: 19 [6300/36045]\tLoss: 802.1200\n",
      "Training Epoch: 19 [6350/36045]\tLoss: 853.3893\n",
      "Training Epoch: 19 [6400/36045]\tLoss: 716.3009\n",
      "Training Epoch: 19 [6450/36045]\tLoss: 663.3013\n",
      "Training Epoch: 19 [6500/36045]\tLoss: 676.8572\n",
      "Training Epoch: 19 [6550/36045]\tLoss: 692.4807\n",
      "Training Epoch: 19 [6600/36045]\tLoss: 695.1202\n",
      "Training Epoch: 19 [6650/36045]\tLoss: 787.5631\n",
      "Training Epoch: 19 [6700/36045]\tLoss: 824.6690\n",
      "Training Epoch: 19 [6750/36045]\tLoss: 795.9976\n",
      "Training Epoch: 19 [6800/36045]\tLoss: 798.9746\n",
      "Training Epoch: 19 [6850/36045]\tLoss: 785.9045\n",
      "Training Epoch: 19 [6900/36045]\tLoss: 697.3622\n",
      "Training Epoch: 19 [6950/36045]\tLoss: 657.5615\n",
      "Training Epoch: 19 [7000/36045]\tLoss: 699.8398\n",
      "Training Epoch: 19 [7050/36045]\tLoss: 715.0544\n",
      "Training Epoch: 19 [7100/36045]\tLoss: 714.6570\n",
      "Training Epoch: 19 [7150/36045]\tLoss: 728.1865\n",
      "Training Epoch: 19 [7200/36045]\tLoss: 732.1458\n",
      "Training Epoch: 19 [7250/36045]\tLoss: 729.4512\n",
      "Training Epoch: 19 [7300/36045]\tLoss: 715.8107\n",
      "Training Epoch: 19 [7350/36045]\tLoss: 709.0745\n",
      "Training Epoch: 19 [7400/36045]\tLoss: 636.4210\n",
      "Training Epoch: 19 [7450/36045]\tLoss: 642.2037\n",
      "Training Epoch: 19 [7500/36045]\tLoss: 636.7201\n",
      "Training Epoch: 19 [7550/36045]\tLoss: 609.5801\n",
      "Training Epoch: 19 [7600/36045]\tLoss: 682.2011\n",
      "Training Epoch: 19 [7650/36045]\tLoss: 734.7999\n",
      "Training Epoch: 19 [7700/36045]\tLoss: 701.2405\n",
      "Training Epoch: 19 [7750/36045]\tLoss: 715.4138\n",
      "Training Epoch: 19 [7800/36045]\tLoss: 702.8087\n",
      "Training Epoch: 19 [7850/36045]\tLoss: 676.5115\n",
      "Training Epoch: 19 [7900/36045]\tLoss: 713.7593\n",
      "Training Epoch: 19 [7950/36045]\tLoss: 711.6151\n",
      "Training Epoch: 19 [8000/36045]\tLoss: 729.3579\n",
      "Training Epoch: 19 [8050/36045]\tLoss: 690.2654\n",
      "Training Epoch: 19 [8100/36045]\tLoss: 717.4579\n",
      "Training Epoch: 19 [8150/36045]\tLoss: 812.0232\n",
      "Training Epoch: 19 [8200/36045]\tLoss: 798.1125\n",
      "Training Epoch: 19 [8250/36045]\tLoss: 763.9896\n",
      "Training Epoch: 19 [8300/36045]\tLoss: 829.8940\n",
      "Training Epoch: 19 [8350/36045]\tLoss: 764.5319\n",
      "Training Epoch: 19 [8400/36045]\tLoss: 685.3978\n",
      "Training Epoch: 19 [8450/36045]\tLoss: 642.9091\n",
      "Training Epoch: 19 [8500/36045]\tLoss: 681.9786\n",
      "Training Epoch: 19 [8550/36045]\tLoss: 672.1108\n",
      "Training Epoch: 19 [8600/36045]\tLoss: 664.2748\n",
      "Training Epoch: 19 [8650/36045]\tLoss: 710.9495\n",
      "Training Epoch: 19 [8700/36045]\tLoss: 751.5282\n",
      "Training Epoch: 19 [8750/36045]\tLoss: 736.8107\n",
      "Training Epoch: 19 [8800/36045]\tLoss: 742.3540\n",
      "Training Epoch: 19 [8850/36045]\tLoss: 734.5897\n",
      "Training Epoch: 19 [8900/36045]\tLoss: 663.3513\n",
      "Training Epoch: 19 [8950/36045]\tLoss: 678.8829\n",
      "Training Epoch: 19 [9000/36045]\tLoss: 693.9236\n",
      "Training Epoch: 19 [9050/36045]\tLoss: 693.4958\n",
      "Training Epoch: 19 [9100/36045]\tLoss: 714.3810\n",
      "Training Epoch: 19 [9150/36045]\tLoss: 527.0155\n",
      "Training Epoch: 19 [9200/36045]\tLoss: 397.3559\n",
      "Training Epoch: 19 [9250/36045]\tLoss: 430.5676\n",
      "Training Epoch: 19 [9300/36045]\tLoss: 443.7781\n",
      "Training Epoch: 19 [9350/36045]\tLoss: 407.9568\n",
      "Training Epoch: 19 [9400/36045]\tLoss: 799.5626\n",
      "Training Epoch: 19 [9450/36045]\tLoss: 849.4820\n",
      "Training Epoch: 19 [9500/36045]\tLoss: 835.9829\n",
      "Training Epoch: 19 [9550/36045]\tLoss: 883.6066\n",
      "Training Epoch: 19 [9600/36045]\tLoss: 657.2116\n",
      "Training Epoch: 19 [9650/36045]\tLoss: 659.6964\n",
      "Training Epoch: 19 [9700/36045]\tLoss: 644.4283\n",
      "Training Epoch: 19 [9750/36045]\tLoss: 645.3303\n",
      "Training Epoch: 19 [9800/36045]\tLoss: 837.5907\n",
      "Training Epoch: 19 [9850/36045]\tLoss: 884.9039\n",
      "Training Epoch: 19 [9900/36045]\tLoss: 903.6823\n",
      "Training Epoch: 19 [9950/36045]\tLoss: 879.3937\n",
      "Training Epoch: 19 [10000/36045]\tLoss: 811.3271\n",
      "Training Epoch: 19 [10050/36045]\tLoss: 675.3573\n",
      "Training Epoch: 19 [10100/36045]\tLoss: 680.3084\n",
      "Training Epoch: 19 [10150/36045]\tLoss: 691.6884\n",
      "Training Epoch: 19 [10200/36045]\tLoss: 681.2982\n",
      "Training Epoch: 19 [10250/36045]\tLoss: 810.1875\n",
      "Training Epoch: 19 [10300/36045]\tLoss: 786.1367\n",
      "Training Epoch: 19 [10350/36045]\tLoss: 826.9888\n",
      "Training Epoch: 19 [10400/36045]\tLoss: 817.6435\n",
      "Training Epoch: 19 [10450/36045]\tLoss: 764.7517\n",
      "Training Epoch: 19 [10500/36045]\tLoss: 642.6537\n",
      "Training Epoch: 19 [10550/36045]\tLoss: 638.4496\n",
      "Training Epoch: 19 [10600/36045]\tLoss: 662.7568\n",
      "Training Epoch: 19 [10650/36045]\tLoss: 668.5024\n",
      "Training Epoch: 19 [10700/36045]\tLoss: 759.8046\n",
      "Training Epoch: 19 [10750/36045]\tLoss: 825.6730\n",
      "Training Epoch: 19 [10800/36045]\tLoss: 764.3856\n",
      "Training Epoch: 19 [10850/36045]\tLoss: 809.4375\n",
      "Training Epoch: 19 [10900/36045]\tLoss: 842.6146\n",
      "Training Epoch: 19 [10950/36045]\tLoss: 626.2365\n",
      "Training Epoch: 19 [11000/36045]\tLoss: 619.9158\n",
      "Training Epoch: 19 [11050/36045]\tLoss: 662.1055\n",
      "Training Epoch: 19 [11100/36045]\tLoss: 675.2244\n",
      "Training Epoch: 19 [11150/36045]\tLoss: 731.6780\n",
      "Training Epoch: 19 [11200/36045]\tLoss: 760.5410\n",
      "Training Epoch: 19 [11250/36045]\tLoss: 773.4675\n",
      "Training Epoch: 19 [11300/36045]\tLoss: 753.4194\n",
      "Training Epoch: 19 [11350/36045]\tLoss: 749.7268\n",
      "Training Epoch: 19 [11400/36045]\tLoss: 707.5924\n",
      "Training Epoch: 19 [11450/36045]\tLoss: 671.8049\n",
      "Training Epoch: 19 [11500/36045]\tLoss: 669.5622\n",
      "Training Epoch: 19 [11550/36045]\tLoss: 684.9196\n",
      "Training Epoch: 19 [11600/36045]\tLoss: 750.9827\n",
      "Training Epoch: 19 [11650/36045]\tLoss: 805.1655\n",
      "Training Epoch: 19 [11700/36045]\tLoss: 804.2975\n",
      "Training Epoch: 19 [11750/36045]\tLoss: 824.6002\n",
      "Training Epoch: 19 [11800/36045]\tLoss: 868.4695\n",
      "Training Epoch: 19 [11850/36045]\tLoss: 921.8466\n",
      "Training Epoch: 19 [11900/36045]\tLoss: 1145.5375\n",
      "Training Epoch: 19 [11950/36045]\tLoss: 1145.5361\n",
      "Training Epoch: 19 [12000/36045]\tLoss: 1162.6251\n",
      "Training Epoch: 19 [12050/36045]\tLoss: 1119.2216\n",
      "Training Epoch: 19 [12100/36045]\tLoss: 744.5752\n",
      "Training Epoch: 19 [12150/36045]\tLoss: 580.0627\n",
      "Training Epoch: 19 [12200/36045]\tLoss: 574.1854\n",
      "Training Epoch: 19 [12250/36045]\tLoss: 584.2332\n",
      "Training Epoch: 19 [12300/36045]\tLoss: 737.3894\n",
      "Training Epoch: 19 [12350/36045]\tLoss: 797.9535\n",
      "Training Epoch: 19 [12400/36045]\tLoss: 807.1484\n",
      "Training Epoch: 19 [12450/36045]\tLoss: 793.8005\n",
      "Training Epoch: 19 [12500/36045]\tLoss: 825.0477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [12550/36045]\tLoss: 791.6840\n",
      "Training Epoch: 19 [12600/36045]\tLoss: 732.7959\n",
      "Training Epoch: 19 [12650/36045]\tLoss: 730.9846\n",
      "Training Epoch: 19 [12700/36045]\tLoss: 753.6708\n",
      "Training Epoch: 19 [12750/36045]\tLoss: 753.5919\n",
      "Training Epoch: 19 [12800/36045]\tLoss: 733.8882\n",
      "Training Epoch: 19 [12850/36045]\tLoss: 766.2768\n",
      "Training Epoch: 19 [12900/36045]\tLoss: 734.7062\n",
      "Training Epoch: 19 [12950/36045]\tLoss: 723.3343\n",
      "Training Epoch: 19 [13000/36045]\tLoss: 755.6645\n",
      "Training Epoch: 19 [13050/36045]\tLoss: 689.1582\n",
      "Training Epoch: 19 [13100/36045]\tLoss: 714.2736\n",
      "Training Epoch: 19 [13150/36045]\tLoss: 706.2344\n",
      "Training Epoch: 19 [13200/36045]\tLoss: 679.9213\n",
      "Training Epoch: 19 [13250/36045]\tLoss: 710.6376\n",
      "Training Epoch: 19 [13300/36045]\tLoss: 751.8251\n",
      "Training Epoch: 19 [13350/36045]\tLoss: 730.0494\n",
      "Training Epoch: 19 [13400/36045]\tLoss: 735.0872\n",
      "Training Epoch: 19 [13450/36045]\tLoss: 728.3962\n",
      "Training Epoch: 19 [13500/36045]\tLoss: 753.6610\n",
      "Training Epoch: 19 [13550/36045]\tLoss: 889.6824\n",
      "Training Epoch: 19 [13600/36045]\tLoss: 922.3516\n",
      "Training Epoch: 19 [13650/36045]\tLoss: 1002.0912\n",
      "Training Epoch: 19 [13700/36045]\tLoss: 889.9565\n",
      "Training Epoch: 19 [13750/36045]\tLoss: 738.6491\n",
      "Training Epoch: 19 [13800/36045]\tLoss: 712.5746\n",
      "Training Epoch: 19 [13850/36045]\tLoss: 695.6647\n",
      "Training Epoch: 19 [13900/36045]\tLoss: 703.5922\n",
      "Training Epoch: 19 [13950/36045]\tLoss: 749.2889\n",
      "Training Epoch: 19 [14000/36045]\tLoss: 785.7405\n",
      "Training Epoch: 19 [14050/36045]\tLoss: 755.7510\n",
      "Training Epoch: 19 [14100/36045]\tLoss: 751.6376\n",
      "Training Epoch: 19 [14150/36045]\tLoss: 739.1503\n",
      "Training Epoch: 19 [14200/36045]\tLoss: 785.9235\n",
      "Training Epoch: 19 [14250/36045]\tLoss: 859.3616\n",
      "Training Epoch: 19 [14300/36045]\tLoss: 863.2448\n",
      "Training Epoch: 19 [14350/36045]\tLoss: 826.6073\n",
      "Training Epoch: 19 [14400/36045]\tLoss: 811.9721\n",
      "Training Epoch: 19 [14450/36045]\tLoss: 851.6644\n",
      "Training Epoch: 19 [14500/36045]\tLoss: 782.5289\n",
      "Training Epoch: 19 [14550/36045]\tLoss: 815.1476\n",
      "Training Epoch: 19 [14600/36045]\tLoss: 797.1972\n",
      "Training Epoch: 19 [14650/36045]\tLoss: 799.0349\n",
      "Training Epoch: 19 [14700/36045]\tLoss: 752.7034\n",
      "Training Epoch: 19 [14750/36045]\tLoss: 645.4478\n",
      "Training Epoch: 19 [14800/36045]\tLoss: 635.6582\n",
      "Training Epoch: 19 [14850/36045]\tLoss: 642.9819\n",
      "Training Epoch: 19 [14900/36045]\tLoss: 636.5286\n",
      "Training Epoch: 19 [14950/36045]\tLoss: 644.4488\n",
      "Training Epoch: 19 [15000/36045]\tLoss: 662.2274\n",
      "Training Epoch: 19 [15050/36045]\tLoss: 660.8974\n",
      "Training Epoch: 19 [15100/36045]\tLoss: 644.2769\n",
      "Training Epoch: 19 [15150/36045]\tLoss: 635.8522\n",
      "Training Epoch: 19 [15200/36045]\tLoss: 587.2224\n",
      "Training Epoch: 19 [15250/36045]\tLoss: 613.0798\n",
      "Training Epoch: 19 [15300/36045]\tLoss: 596.6955\n",
      "Training Epoch: 19 [15350/36045]\tLoss: 610.7546\n",
      "Training Epoch: 19 [15400/36045]\tLoss: 596.0401\n",
      "Training Epoch: 19 [15450/36045]\tLoss: 583.3179\n",
      "Training Epoch: 19 [15500/36045]\tLoss: 600.4958\n",
      "Training Epoch: 19 [15550/36045]\tLoss: 594.7107\n",
      "Training Epoch: 19 [15600/36045]\tLoss: 668.5515\n",
      "Training Epoch: 19 [15650/36045]\tLoss: 689.1746\n",
      "Training Epoch: 19 [15700/36045]\tLoss: 678.0100\n",
      "Training Epoch: 19 [15750/36045]\tLoss: 669.7167\n",
      "Training Epoch: 19 [15800/36045]\tLoss: 625.3935\n",
      "Training Epoch: 19 [15850/36045]\tLoss: 637.8303\n",
      "Training Epoch: 19 [15900/36045]\tLoss: 647.8909\n",
      "Training Epoch: 19 [15950/36045]\tLoss: 668.5328\n",
      "Training Epoch: 19 [16000/36045]\tLoss: 645.5501\n",
      "Training Epoch: 19 [16050/36045]\tLoss: 615.5196\n",
      "Training Epoch: 19 [16100/36045]\tLoss: 568.1677\n",
      "Training Epoch: 19 [16150/36045]\tLoss: 554.3702\n",
      "Training Epoch: 19 [16200/36045]\tLoss: 669.3348\n",
      "Training Epoch: 19 [16250/36045]\tLoss: 699.9048\n",
      "Training Epoch: 19 [16300/36045]\tLoss: 764.5596\n",
      "Training Epoch: 19 [16350/36045]\tLoss: 781.4021\n",
      "Training Epoch: 19 [16400/36045]\tLoss: 754.6678\n",
      "Training Epoch: 19 [16450/36045]\tLoss: 734.5630\n",
      "Training Epoch: 19 [16500/36045]\tLoss: 732.9988\n",
      "Training Epoch: 19 [16550/36045]\tLoss: 695.2518\n",
      "Training Epoch: 19 [16600/36045]\tLoss: 724.1190\n",
      "Training Epoch: 19 [16650/36045]\tLoss: 746.0244\n",
      "Training Epoch: 19 [16700/36045]\tLoss: 719.9122\n",
      "Training Epoch: 19 [16750/36045]\tLoss: 711.0446\n",
      "Training Epoch: 19 [16800/36045]\tLoss: 724.1870\n",
      "Training Epoch: 19 [16850/36045]\tLoss: 689.0135\n",
      "Training Epoch: 19 [16900/36045]\tLoss: 700.9407\n",
      "Training Epoch: 19 [16950/36045]\tLoss: 729.2380\n",
      "Training Epoch: 19 [17000/36045]\tLoss: 709.9490\n",
      "Training Epoch: 19 [17050/36045]\tLoss: 741.8727\n",
      "Training Epoch: 19 [17100/36045]\tLoss: 740.6932\n",
      "Training Epoch: 19 [17150/36045]\tLoss: 643.4172\n",
      "Training Epoch: 19 [17200/36045]\tLoss: 600.2706\n",
      "Training Epoch: 19 [17250/36045]\tLoss: 628.0978\n",
      "Training Epoch: 19 [17300/36045]\tLoss: 662.8907\n",
      "Training Epoch: 19 [17350/36045]\tLoss: 635.4160\n",
      "Training Epoch: 19 [17400/36045]\tLoss: 655.4795\n",
      "Training Epoch: 19 [17450/36045]\tLoss: 677.5951\n",
      "Training Epoch: 19 [17500/36045]\tLoss: 664.7493\n",
      "Training Epoch: 19 [17550/36045]\tLoss: 666.5178\n",
      "Training Epoch: 19 [17600/36045]\tLoss: 655.0994\n",
      "Training Epoch: 19 [17650/36045]\tLoss: 673.3781\n",
      "Training Epoch: 19 [17700/36045]\tLoss: 651.9351\n",
      "Training Epoch: 19 [17750/36045]\tLoss: 669.6069\n",
      "Training Epoch: 19 [17800/36045]\tLoss: 660.4254\n",
      "Training Epoch: 19 [17850/36045]\tLoss: 660.4843\n",
      "Training Epoch: 19 [17900/36045]\tLoss: 690.3672\n",
      "Training Epoch: 19 [17950/36045]\tLoss: 700.9730\n",
      "Training Epoch: 19 [18000/36045]\tLoss: 691.1442\n",
      "Training Epoch: 19 [18050/36045]\tLoss: 775.8398\n",
      "Training Epoch: 19 [18100/36045]\tLoss: 779.2976\n",
      "Training Epoch: 19 [18150/36045]\tLoss: 788.3510\n",
      "Training Epoch: 19 [18200/36045]\tLoss: 772.0378\n",
      "Training Epoch: 19 [18250/36045]\tLoss: 792.7906\n",
      "Training Epoch: 19 [18300/36045]\tLoss: 731.7104\n",
      "Training Epoch: 19 [18350/36045]\tLoss: 800.2604\n",
      "Training Epoch: 19 [18400/36045]\tLoss: 772.3840\n",
      "Training Epoch: 19 [18450/36045]\tLoss: 753.0862\n",
      "Training Epoch: 19 [18500/36045]\tLoss: 753.0096\n",
      "Training Epoch: 19 [18550/36045]\tLoss: 739.1625\n",
      "Training Epoch: 19 [18600/36045]\tLoss: 728.7594\n",
      "Training Epoch: 19 [18650/36045]\tLoss: 778.9896\n",
      "Training Epoch: 19 [18700/36045]\tLoss: 819.8816\n",
      "Training Epoch: 19 [18750/36045]\tLoss: 803.4508\n",
      "Training Epoch: 19 [18800/36045]\tLoss: 829.1231\n",
      "Training Epoch: 19 [18850/36045]\tLoss: 772.2354\n",
      "Training Epoch: 19 [18900/36045]\tLoss: 827.0089\n",
      "Training Epoch: 19 [18950/36045]\tLoss: 764.4956\n",
      "Training Epoch: 19 [19000/36045]\tLoss: 651.3926\n",
      "Training Epoch: 19 [19050/36045]\tLoss: 630.1499\n",
      "Training Epoch: 19 [19100/36045]\tLoss: 641.2431\n",
      "Training Epoch: 19 [19150/36045]\tLoss: 629.8573\n",
      "Training Epoch: 19 [19200/36045]\tLoss: 656.4810\n",
      "Training Epoch: 19 [19250/36045]\tLoss: 670.7457\n",
      "Training Epoch: 19 [19300/36045]\tLoss: 683.1308\n",
      "Training Epoch: 19 [19350/36045]\tLoss: 666.5948\n",
      "Training Epoch: 19 [19400/36045]\tLoss: 690.5004\n",
      "Training Epoch: 19 [19450/36045]\tLoss: 679.6876\n",
      "Training Epoch: 19 [19500/36045]\tLoss: 682.5797\n",
      "Training Epoch: 19 [19550/36045]\tLoss: 681.0221\n",
      "Training Epoch: 19 [19600/36045]\tLoss: 723.4225\n",
      "Training Epoch: 19 [19650/36045]\tLoss: 946.4984\n",
      "Training Epoch: 19 [19700/36045]\tLoss: 904.1507\n",
      "Training Epoch: 19 [19750/36045]\tLoss: 904.9546\n",
      "Training Epoch: 19 [19800/36045]\tLoss: 900.6003\n",
      "Training Epoch: 19 [19850/36045]\tLoss: 610.0056\n",
      "Training Epoch: 19 [19900/36045]\tLoss: 586.7452\n",
      "Training Epoch: 19 [19950/36045]\tLoss: 591.0391\n",
      "Training Epoch: 19 [20000/36045]\tLoss: 589.2999\n",
      "Training Epoch: 19 [20050/36045]\tLoss: 658.7068\n",
      "Training Epoch: 19 [20100/36045]\tLoss: 663.7797\n",
      "Training Epoch: 19 [20150/36045]\tLoss: 666.7189\n",
      "Training Epoch: 19 [20200/36045]\tLoss: 667.3921\n",
      "Training Epoch: 19 [20250/36045]\tLoss: 711.2076\n",
      "Training Epoch: 19 [20300/36045]\tLoss: 749.8605\n",
      "Training Epoch: 19 [20350/36045]\tLoss: 771.0612\n",
      "Training Epoch: 19 [20400/36045]\tLoss: 786.1794\n",
      "Training Epoch: 19 [20450/36045]\tLoss: 757.7494\n",
      "Training Epoch: 19 [20500/36045]\tLoss: 739.2785\n",
      "Training Epoch: 19 [20550/36045]\tLoss: 651.9700\n",
      "Training Epoch: 19 [20600/36045]\tLoss: 665.4643\n",
      "Training Epoch: 19 [20650/36045]\tLoss: 661.7235\n",
      "Training Epoch: 19 [20700/36045]\tLoss: 648.5117\n",
      "Training Epoch: 19 [20750/36045]\tLoss: 694.9017\n",
      "Training Epoch: 19 [20800/36045]\tLoss: 756.6945\n",
      "Training Epoch: 19 [20850/36045]\tLoss: 744.5040\n",
      "Training Epoch: 19 [20900/36045]\tLoss: 793.4068\n",
      "Training Epoch: 19 [20950/36045]\tLoss: 748.5488\n",
      "Training Epoch: 19 [21000/36045]\tLoss: 706.3959\n",
      "Training Epoch: 19 [21050/36045]\tLoss: 606.2199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [21100/36045]\tLoss: 607.8425\n",
      "Training Epoch: 19 [21150/36045]\tLoss: 650.3174\n",
      "Training Epoch: 19 [21200/36045]\tLoss: 649.7977\n",
      "Training Epoch: 19 [21250/36045]\tLoss: 621.2659\n",
      "Training Epoch: 19 [21300/36045]\tLoss: 725.3093\n",
      "Training Epoch: 19 [21350/36045]\tLoss: 719.6982\n",
      "Training Epoch: 19 [21400/36045]\tLoss: 722.9090\n",
      "Training Epoch: 19 [21450/36045]\tLoss: 730.3754\n",
      "Training Epoch: 19 [21500/36045]\tLoss: 733.8655\n",
      "Training Epoch: 19 [21550/36045]\tLoss: 832.7173\n",
      "Training Epoch: 19 [21600/36045]\tLoss: 833.9057\n",
      "Training Epoch: 19 [21650/36045]\tLoss: 847.9526\n",
      "Training Epoch: 19 [21700/36045]\tLoss: 845.8105\n",
      "Training Epoch: 19 [21750/36045]\tLoss: 815.0507\n",
      "Training Epoch: 19 [21800/36045]\tLoss: 604.9680\n",
      "Training Epoch: 19 [21850/36045]\tLoss: 587.2840\n",
      "Training Epoch: 19 [21900/36045]\tLoss: 600.2329\n",
      "Training Epoch: 19 [21950/36045]\tLoss: 597.3762\n",
      "Training Epoch: 19 [22000/36045]\tLoss: 603.3828\n",
      "Training Epoch: 19 [22050/36045]\tLoss: 633.4296\n",
      "Training Epoch: 19 [22100/36045]\tLoss: 624.2877\n",
      "Training Epoch: 19 [22150/36045]\tLoss: 605.3158\n",
      "Training Epoch: 19 [22200/36045]\tLoss: 624.8752\n",
      "Training Epoch: 19 [22250/36045]\tLoss: 630.7534\n",
      "Training Epoch: 19 [22300/36045]\tLoss: 686.8190\n",
      "Training Epoch: 19 [22350/36045]\tLoss: 714.5552\n",
      "Training Epoch: 19 [22400/36045]\tLoss: 731.1765\n",
      "Training Epoch: 19 [22450/36045]\tLoss: 718.1517\n",
      "Training Epoch: 19 [22500/36045]\tLoss: 697.4047\n",
      "Training Epoch: 19 [22550/36045]\tLoss: 738.9901\n",
      "Training Epoch: 19 [22600/36045]\tLoss: 806.5597\n",
      "Training Epoch: 19 [22650/36045]\tLoss: 845.9402\n",
      "Training Epoch: 19 [22700/36045]\tLoss: 869.9887\n",
      "Training Epoch: 19 [22750/36045]\tLoss: 892.1066\n",
      "Training Epoch: 19 [22800/36045]\tLoss: 928.4003\n",
      "Training Epoch: 19 [22850/36045]\tLoss: 770.7189\n",
      "Training Epoch: 19 [22900/36045]\tLoss: 774.6945\n",
      "Training Epoch: 19 [22950/36045]\tLoss: 750.9912\n",
      "Training Epoch: 19 [23000/36045]\tLoss: 751.3094\n",
      "Training Epoch: 19 [23050/36045]\tLoss: 671.9727\n",
      "Training Epoch: 19 [23100/36045]\tLoss: 688.3223\n",
      "Training Epoch: 19 [23150/36045]\tLoss: 675.9534\n",
      "Training Epoch: 19 [23200/36045]\tLoss: 639.7839\n",
      "Training Epoch: 19 [23250/36045]\tLoss: 643.4148\n",
      "Training Epoch: 19 [23300/36045]\tLoss: 639.9801\n",
      "Training Epoch: 19 [23350/36045]\tLoss: 662.2310\n",
      "Training Epoch: 19 [23400/36045]\tLoss: 717.8713\n",
      "Training Epoch: 19 [23450/36045]\tLoss: 709.0574\n",
      "Training Epoch: 19 [23500/36045]\tLoss: 682.9827\n",
      "Training Epoch: 19 [23550/36045]\tLoss: 734.2682\n",
      "Training Epoch: 19 [23600/36045]\tLoss: 826.4086\n",
      "Training Epoch: 19 [23650/36045]\tLoss: 841.0551\n",
      "Training Epoch: 19 [23700/36045]\tLoss: 852.2195\n",
      "Training Epoch: 19 [23750/36045]\tLoss: 823.7595\n",
      "Training Epoch: 19 [23800/36045]\tLoss: 652.4272\n",
      "Training Epoch: 19 [23850/36045]\tLoss: 680.7357\n",
      "Training Epoch: 19 [23900/36045]\tLoss: 671.0991\n",
      "Training Epoch: 19 [23950/36045]\tLoss: 652.5764\n",
      "Training Epoch: 19 [24000/36045]\tLoss: 628.2123\n",
      "Training Epoch: 19 [24050/36045]\tLoss: 581.0544\n",
      "Training Epoch: 19 [24100/36045]\tLoss: 611.9821\n",
      "Training Epoch: 19 [24150/36045]\tLoss: 607.4526\n",
      "Training Epoch: 19 [24200/36045]\tLoss: 600.9664\n",
      "Training Epoch: 19 [24250/36045]\tLoss: 583.5922\n",
      "Training Epoch: 19 [24300/36045]\tLoss: 628.4138\n",
      "Training Epoch: 19 [24350/36045]\tLoss: 643.8422\n",
      "Training Epoch: 19 [24400/36045]\tLoss: 662.3911\n",
      "Training Epoch: 19 [24450/36045]\tLoss: 632.2637\n",
      "Training Epoch: 19 [24500/36045]\tLoss: 665.7517\n",
      "Training Epoch: 19 [24550/36045]\tLoss: 760.3541\n",
      "Training Epoch: 19 [24600/36045]\tLoss: 753.1660\n",
      "Training Epoch: 19 [24650/36045]\tLoss: 723.5178\n",
      "Training Epoch: 19 [24700/36045]\tLoss: 735.2175\n",
      "Training Epoch: 19 [24750/36045]\tLoss: 679.4128\n",
      "Training Epoch: 19 [24800/36045]\tLoss: 570.0486\n",
      "Training Epoch: 19 [24850/36045]\tLoss: 590.8589\n",
      "Training Epoch: 19 [24900/36045]\tLoss: 587.2145\n",
      "Training Epoch: 19 [24950/36045]\tLoss: 589.0449\n",
      "Training Epoch: 19 [25000/36045]\tLoss: 565.2411\n",
      "Training Epoch: 19 [25050/36045]\tLoss: 539.9584\n",
      "Training Epoch: 19 [25100/36045]\tLoss: 485.1823\n",
      "Training Epoch: 19 [25150/36045]\tLoss: 450.0908\n",
      "Training Epoch: 19 [25200/36045]\tLoss: 445.2269\n",
      "Training Epoch: 19 [25250/36045]\tLoss: 475.9599\n",
      "Training Epoch: 19 [25300/36045]\tLoss: 623.7557\n",
      "Training Epoch: 19 [25350/36045]\tLoss: 621.8934\n",
      "Training Epoch: 19 [25400/36045]\tLoss: 579.3054\n",
      "Training Epoch: 19 [25450/36045]\tLoss: 582.2261\n",
      "Training Epoch: 19 [25500/36045]\tLoss: 633.4233\n",
      "Training Epoch: 19 [25550/36045]\tLoss: 732.5762\n",
      "Training Epoch: 19 [25600/36045]\tLoss: 738.5719\n",
      "Training Epoch: 19 [25650/36045]\tLoss: 711.7442\n",
      "Training Epoch: 19 [25700/36045]\tLoss: 721.7603\n",
      "Training Epoch: 19 [25750/36045]\tLoss: 695.6595\n",
      "Training Epoch: 19 [25800/36045]\tLoss: 438.0029\n",
      "Training Epoch: 19 [25850/36045]\tLoss: 448.6217\n",
      "Training Epoch: 19 [25900/36045]\tLoss: 428.9826\n",
      "Training Epoch: 19 [25950/36045]\tLoss: 438.2344\n",
      "Training Epoch: 19 [26000/36045]\tLoss: 536.8308\n",
      "Training Epoch: 19 [26050/36045]\tLoss: 729.1046\n",
      "Training Epoch: 19 [26100/36045]\tLoss: 758.3434\n",
      "Training Epoch: 19 [26150/36045]\tLoss: 757.1372\n",
      "Training Epoch: 19 [26200/36045]\tLoss: 730.4969\n",
      "Training Epoch: 19 [26250/36045]\tLoss: 763.3405\n",
      "Training Epoch: 19 [26300/36045]\tLoss: 678.2700\n",
      "Training Epoch: 19 [26350/36045]\tLoss: 687.4601\n",
      "Training Epoch: 19 [26400/36045]\tLoss: 665.9515\n",
      "Training Epoch: 19 [26450/36045]\tLoss: 594.7654\n",
      "Training Epoch: 19 [26500/36045]\tLoss: 712.3300\n",
      "Training Epoch: 19 [26550/36045]\tLoss: 718.1365\n",
      "Training Epoch: 19 [26600/36045]\tLoss: 712.6413\n",
      "Training Epoch: 19 [26650/36045]\tLoss: 729.3017\n",
      "Training Epoch: 19 [26700/36045]\tLoss: 709.0959\n",
      "Training Epoch: 19 [26750/36045]\tLoss: 662.8469\n",
      "Training Epoch: 19 [26800/36045]\tLoss: 487.1910\n",
      "Training Epoch: 19 [26850/36045]\tLoss: 405.8635\n",
      "Training Epoch: 19 [26900/36045]\tLoss: 409.2527\n",
      "Training Epoch: 19 [26950/36045]\tLoss: 450.0907\n",
      "Training Epoch: 19 [27000/36045]\tLoss: 721.7181\n",
      "Training Epoch: 19 [27050/36045]\tLoss: 758.0808\n",
      "Training Epoch: 19 [27100/36045]\tLoss: 733.8483\n",
      "Training Epoch: 19 [27150/36045]\tLoss: 777.2336\n",
      "Training Epoch: 19 [27200/36045]\tLoss: 574.0684\n",
      "Training Epoch: 19 [27250/36045]\tLoss: 569.2612\n",
      "Training Epoch: 19 [27300/36045]\tLoss: 553.2918\n",
      "Training Epoch: 19 [27350/36045]\tLoss: 553.2492\n",
      "Training Epoch: 19 [27400/36045]\tLoss: 552.4265\n",
      "Training Epoch: 19 [27450/36045]\tLoss: 695.1806\n",
      "Training Epoch: 19 [27500/36045]\tLoss: 746.5931\n",
      "Training Epoch: 19 [27550/36045]\tLoss: 738.8954\n",
      "Training Epoch: 19 [27600/36045]\tLoss: 749.4525\n",
      "Training Epoch: 19 [27650/36045]\tLoss: 742.0266\n",
      "Training Epoch: 19 [27700/36045]\tLoss: 772.4234\n",
      "Training Epoch: 19 [27750/36045]\tLoss: 785.1071\n",
      "Training Epoch: 19 [27800/36045]\tLoss: 770.7279\n",
      "Training Epoch: 19 [27850/36045]\tLoss: 757.3831\n",
      "Training Epoch: 19 [27900/36045]\tLoss: 679.7430\n",
      "Training Epoch: 19 [27950/36045]\tLoss: 562.8005\n",
      "Training Epoch: 19 [28000/36045]\tLoss: 536.6544\n",
      "Training Epoch: 19 [28050/36045]\tLoss: 549.8534\n",
      "Training Epoch: 19 [28100/36045]\tLoss: 540.9710\n",
      "Training Epoch: 19 [28150/36045]\tLoss: 572.2416\n",
      "Training Epoch: 19 [28200/36045]\tLoss: 576.4876\n",
      "Training Epoch: 19 [28250/36045]\tLoss: 572.8052\n",
      "Training Epoch: 19 [28300/36045]\tLoss: 541.5891\n",
      "Training Epoch: 19 [28350/36045]\tLoss: 537.4869\n",
      "Training Epoch: 19 [28400/36045]\tLoss: 875.8932\n",
      "Training Epoch: 19 [28450/36045]\tLoss: 795.8841\n",
      "Training Epoch: 19 [28500/36045]\tLoss: 690.2791\n",
      "Training Epoch: 19 [28550/36045]\tLoss: 633.7883\n",
      "Training Epoch: 19 [28600/36045]\tLoss: 680.2139\n",
      "Training Epoch: 19 [28650/36045]\tLoss: 772.0031\n",
      "Training Epoch: 19 [28700/36045]\tLoss: 765.0415\n",
      "Training Epoch: 19 [28750/36045]\tLoss: 753.5690\n",
      "Training Epoch: 19 [28800/36045]\tLoss: 762.9146\n",
      "Training Epoch: 19 [28850/36045]\tLoss: 660.3168\n",
      "Training Epoch: 19 [28900/36045]\tLoss: 529.4753\n",
      "Training Epoch: 19 [28950/36045]\tLoss: 525.4243\n",
      "Training Epoch: 19 [29000/36045]\tLoss: 525.2075\n",
      "Training Epoch: 19 [29050/36045]\tLoss: 534.1165\n",
      "Training Epoch: 19 [29100/36045]\tLoss: 556.0404\n",
      "Training Epoch: 19 [29150/36045]\tLoss: 542.3373\n",
      "Training Epoch: 19 [29200/36045]\tLoss: 527.9174\n",
      "Training Epoch: 19 [29250/36045]\tLoss: 513.9178\n",
      "Training Epoch: 19 [29300/36045]\tLoss: 590.0092\n",
      "Training Epoch: 19 [29350/36045]\tLoss: 702.3024\n",
      "Training Epoch: 19 [29400/36045]\tLoss: 721.3417\n",
      "Training Epoch: 19 [29450/36045]\tLoss: 747.0679\n",
      "Training Epoch: 19 [29500/36045]\tLoss: 761.5356\n",
      "Training Epoch: 19 [29550/36045]\tLoss: 722.9191\n",
      "Training Epoch: 19 [29600/36045]\tLoss: 612.9680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 19 [29650/36045]\tLoss: 595.9358\n",
      "Training Epoch: 19 [29700/36045]\tLoss: 529.8945\n",
      "Training Epoch: 19 [29750/36045]\tLoss: 532.9972\n",
      "Training Epoch: 19 [29800/36045]\tLoss: 579.3349\n",
      "Training Epoch: 19 [29850/36045]\tLoss: 651.3831\n",
      "Training Epoch: 19 [29900/36045]\tLoss: 646.8448\n",
      "Training Epoch: 19 [29950/36045]\tLoss: 669.5635\n",
      "Training Epoch: 19 [30000/36045]\tLoss: 647.5883\n",
      "Training Epoch: 19 [30050/36045]\tLoss: 653.9464\n",
      "Training Epoch: 19 [30100/36045]\tLoss: 797.7618\n",
      "Training Epoch: 19 [30150/36045]\tLoss: 783.0048\n",
      "Training Epoch: 19 [30200/36045]\tLoss: 738.8508\n",
      "Training Epoch: 19 [30250/36045]\tLoss: 788.3254\n",
      "Training Epoch: 19 [30300/36045]\tLoss: 775.3545\n",
      "Training Epoch: 19 [30350/36045]\tLoss: 613.4260\n",
      "Training Epoch: 19 [30400/36045]\tLoss: 600.0452\n",
      "Training Epoch: 19 [30450/36045]\tLoss: 599.1824\n",
      "Training Epoch: 19 [30500/36045]\tLoss: 559.5330\n",
      "Training Epoch: 19 [30550/36045]\tLoss: 519.4531\n",
      "Training Epoch: 19 [30600/36045]\tLoss: 501.9039\n",
      "Training Epoch: 19 [30650/36045]\tLoss: 493.2518\n",
      "Training Epoch: 19 [30700/36045]\tLoss: 511.7184\n",
      "Training Epoch: 19 [30750/36045]\tLoss: 496.7529\n",
      "Training Epoch: 19 [30800/36045]\tLoss: 529.2874\n",
      "Training Epoch: 19 [30850/36045]\tLoss: 519.7498\n",
      "Training Epoch: 19 [30900/36045]\tLoss: 534.9210\n",
      "Training Epoch: 19 [30950/36045]\tLoss: 562.0823\n",
      "Training Epoch: 19 [31000/36045]\tLoss: 552.0554\n",
      "Training Epoch: 19 [31050/36045]\tLoss: 460.5536\n",
      "Training Epoch: 19 [31100/36045]\tLoss: 450.8818\n",
      "Training Epoch: 19 [31150/36045]\tLoss: 457.1347\n",
      "Training Epoch: 19 [31200/36045]\tLoss: 572.5770\n",
      "Training Epoch: 19 [31250/36045]\tLoss: 743.6747\n",
      "Training Epoch: 19 [31300/36045]\tLoss: 711.4722\n",
      "Training Epoch: 19 [31350/36045]\tLoss: 727.7932\n",
      "Training Epoch: 19 [31400/36045]\tLoss: 707.7240\n",
      "Training Epoch: 19 [31450/36045]\tLoss: 718.3903\n",
      "Training Epoch: 19 [31500/36045]\tLoss: 728.3898\n",
      "Training Epoch: 19 [31550/36045]\tLoss: 738.6721\n",
      "Training Epoch: 19 [31600/36045]\tLoss: 693.8223\n",
      "Training Epoch: 19 [31650/36045]\tLoss: 740.3357\n",
      "Training Epoch: 19 [31700/36045]\tLoss: 542.1445\n",
      "Training Epoch: 19 [31750/36045]\tLoss: 451.1523\n",
      "Training Epoch: 19 [31800/36045]\tLoss: 429.5532\n",
      "Training Epoch: 19 [31850/36045]\tLoss: 439.9943\n",
      "Training Epoch: 19 [31900/36045]\tLoss: 681.1887\n",
      "Training Epoch: 19 [31950/36045]\tLoss: 870.9905\n",
      "Training Epoch: 19 [32000/36045]\tLoss: 989.3292\n",
      "Training Epoch: 19 [32050/36045]\tLoss: 941.8108\n",
      "Training Epoch: 19 [32100/36045]\tLoss: 929.3220\n",
      "Training Epoch: 19 [32150/36045]\tLoss: 732.8579\n",
      "Training Epoch: 19 [32200/36045]\tLoss: 737.8901\n",
      "Training Epoch: 19 [32250/36045]\tLoss: 749.3878\n",
      "Training Epoch: 19 [32300/36045]\tLoss: 731.5812\n",
      "Training Epoch: 19 [32350/36045]\tLoss: 724.9448\n",
      "Training Epoch: 19 [32400/36045]\tLoss: 681.6425\n",
      "Training Epoch: 19 [32450/36045]\tLoss: 562.5693\n",
      "Training Epoch: 19 [32500/36045]\tLoss: 541.3618\n",
      "Training Epoch: 19 [32550/36045]\tLoss: 544.6000\n",
      "Training Epoch: 19 [32600/36045]\tLoss: 539.9385\n",
      "Training Epoch: 19 [32650/36045]\tLoss: 681.3179\n",
      "Training Epoch: 19 [32700/36045]\tLoss: 741.0842\n",
      "Training Epoch: 19 [32750/36045]\tLoss: 706.2545\n",
      "Training Epoch: 19 [32800/36045]\tLoss: 724.1750\n",
      "Training Epoch: 19 [32850/36045]\tLoss: 670.3027\n",
      "Training Epoch: 19 [32900/36045]\tLoss: 543.5649\n",
      "Training Epoch: 19 [32950/36045]\tLoss: 568.3040\n",
      "Training Epoch: 19 [33000/36045]\tLoss: 568.6420\n",
      "Training Epoch: 19 [33050/36045]\tLoss: 538.5045\n",
      "Training Epoch: 19 [33100/36045]\tLoss: 613.9092\n",
      "Training Epoch: 19 [33150/36045]\tLoss: 829.2982\n",
      "Training Epoch: 19 [33200/36045]\tLoss: 808.6622\n",
      "Training Epoch: 19 [33250/36045]\tLoss: 832.4280\n",
      "Training Epoch: 19 [33300/36045]\tLoss: 885.8916\n",
      "Training Epoch: 19 [33350/36045]\tLoss: 680.5421\n",
      "Training Epoch: 19 [33400/36045]\tLoss: 504.4904\n",
      "Training Epoch: 19 [33450/36045]\tLoss: 499.4251\n",
      "Training Epoch: 19 [33500/36045]\tLoss: 513.4819\n",
      "Training Epoch: 19 [33550/36045]\tLoss: 533.0139\n",
      "Training Epoch: 19 [33600/36045]\tLoss: 534.6989\n",
      "Training Epoch: 19 [33650/36045]\tLoss: 709.4501\n",
      "Training Epoch: 19 [33700/36045]\tLoss: 687.1360\n",
      "Training Epoch: 19 [33750/36045]\tLoss: 711.1797\n",
      "Training Epoch: 19 [33800/36045]\tLoss: 705.8893\n",
      "Training Epoch: 19 [33850/36045]\tLoss: 709.4752\n",
      "Training Epoch: 19 [33900/36045]\tLoss: 718.0084\n",
      "Training Epoch: 19 [33950/36045]\tLoss: 730.1251\n",
      "Training Epoch: 19 [34000/36045]\tLoss: 719.1246\n",
      "Training Epoch: 19 [34050/36045]\tLoss: 723.5338\n",
      "Training Epoch: 19 [34100/36045]\tLoss: 695.3781\n",
      "Training Epoch: 19 [34150/36045]\tLoss: 647.9465\n",
      "Training Epoch: 19 [34200/36045]\tLoss: 613.8860\n",
      "Training Epoch: 19 [34250/36045]\tLoss: 628.1993\n",
      "Training Epoch: 19 [34300/36045]\tLoss: 539.5431\n",
      "Training Epoch: 19 [34350/36045]\tLoss: 566.8955\n",
      "Training Epoch: 19 [34400/36045]\tLoss: 555.9403\n",
      "Training Epoch: 19 [34450/36045]\tLoss: 522.2375\n",
      "Training Epoch: 19 [34500/36045]\tLoss: 558.3891\n",
      "Training Epoch: 19 [34550/36045]\tLoss: 548.9579\n",
      "Training Epoch: 19 [34600/36045]\tLoss: 546.5936\n",
      "Training Epoch: 19 [34650/36045]\tLoss: 658.7976\n",
      "Training Epoch: 19 [34700/36045]\tLoss: 695.7764\n",
      "Training Epoch: 19 [34750/36045]\tLoss: 619.3497\n",
      "Training Epoch: 19 [34800/36045]\tLoss: 705.3564\n",
      "Training Epoch: 19 [34850/36045]\tLoss: 715.1663\n",
      "Training Epoch: 19 [34900/36045]\tLoss: 800.9825\n",
      "Training Epoch: 19 [34950/36045]\tLoss: 789.4497\n",
      "Training Epoch: 19 [35000/36045]\tLoss: 790.8853\n",
      "Training Epoch: 19 [35050/36045]\tLoss: 776.1492\n",
      "Training Epoch: 19 [35100/36045]\tLoss: 640.8391\n",
      "Training Epoch: 19 [35150/36045]\tLoss: 634.1972\n",
      "Training Epoch: 19 [35200/36045]\tLoss: 541.7688\n",
      "Training Epoch: 19 [35250/36045]\tLoss: 593.6952\n",
      "Training Epoch: 19 [35300/36045]\tLoss: 606.9606\n",
      "Training Epoch: 19 [35350/36045]\tLoss: 699.2978\n",
      "Training Epoch: 19 [35400/36045]\tLoss: 740.6987\n",
      "Training Epoch: 19 [35450/36045]\tLoss: 706.8631\n",
      "Training Epoch: 19 [35500/36045]\tLoss: 688.6057\n",
      "Training Epoch: 19 [35550/36045]\tLoss: 672.9319\n",
      "Training Epoch: 19 [35600/36045]\tLoss: 719.7523\n",
      "Training Epoch: 19 [35650/36045]\tLoss: 796.3135\n",
      "Training Epoch: 19 [35700/36045]\tLoss: 722.8393\n",
      "Training Epoch: 19 [35750/36045]\tLoss: 783.2946\n",
      "Training Epoch: 19 [35800/36045]\tLoss: 789.2226\n",
      "Training Epoch: 19 [35850/36045]\tLoss: 763.6006\n",
      "Training Epoch: 19 [35900/36045]\tLoss: 793.8370\n",
      "Training Epoch: 19 [35950/36045]\tLoss: 792.5399\n",
      "Training Epoch: 19 [36000/36045]\tLoss: 781.9991\n",
      "Training Epoch: 19 [36045/36045]\tLoss: 763.1913\n",
      "Training Epoch: 19 [4004/4004]\tLoss: 714.3380\n",
      "Training Epoch: 20 [50/36045]\tLoss: 714.8969\n",
      "Training Epoch: 20 [100/36045]\tLoss: 685.2849\n",
      "Training Epoch: 20 [150/36045]\tLoss: 683.7958\n",
      "Training Epoch: 20 [200/36045]\tLoss: 671.0244\n",
      "Training Epoch: 20 [250/36045]\tLoss: 795.2893\n",
      "Training Epoch: 20 [300/36045]\tLoss: 858.4050\n",
      "Training Epoch: 20 [350/36045]\tLoss: 822.0068\n",
      "Training Epoch: 20 [400/36045]\tLoss: 819.4955\n",
      "Training Epoch: 20 [450/36045]\tLoss: 798.5583\n",
      "Training Epoch: 20 [500/36045]\tLoss: 745.8781\n",
      "Training Epoch: 20 [550/36045]\tLoss: 753.8647\n",
      "Training Epoch: 20 [600/36045]\tLoss: 729.3895\n",
      "Training Epoch: 20 [650/36045]\tLoss: 755.1683\n",
      "Training Epoch: 20 [700/36045]\tLoss: 742.6277\n",
      "Training Epoch: 20 [750/36045]\tLoss: 723.1192\n",
      "Training Epoch: 20 [800/36045]\tLoss: 740.1165\n",
      "Training Epoch: 20 [850/36045]\tLoss: 720.5759\n",
      "Training Epoch: 20 [900/36045]\tLoss: 685.0995\n",
      "Training Epoch: 20 [950/36045]\tLoss: 650.0165\n",
      "Training Epoch: 20 [1000/36045]\tLoss: 623.9750\n",
      "Training Epoch: 20 [1050/36045]\tLoss: 625.6317\n",
      "Training Epoch: 20 [1100/36045]\tLoss: 609.2669\n",
      "Training Epoch: 20 [1150/36045]\tLoss: 618.5592\n",
      "Training Epoch: 20 [1200/36045]\tLoss: 653.3630\n",
      "Training Epoch: 20 [1250/36045]\tLoss: 746.2898\n",
      "Training Epoch: 20 [1300/36045]\tLoss: 751.5317\n",
      "Training Epoch: 20 [1350/36045]\tLoss: 754.4830\n",
      "Training Epoch: 20 [1400/36045]\tLoss: 783.5289\n",
      "Training Epoch: 20 [1450/36045]\tLoss: 757.3295\n",
      "Training Epoch: 20 [1500/36045]\tLoss: 699.5985\n",
      "Training Epoch: 20 [1550/36045]\tLoss: 717.4633\n",
      "Training Epoch: 20 [1600/36045]\tLoss: 728.3363\n",
      "Training Epoch: 20 [1650/36045]\tLoss: 714.3589\n",
      "Training Epoch: 20 [1700/36045]\tLoss: 725.3051\n",
      "Training Epoch: 20 [1750/36045]\tLoss: 766.9620\n",
      "Training Epoch: 20 [1800/36045]\tLoss: 749.4241\n",
      "Training Epoch: 20 [1850/36045]\tLoss: 770.5630\n",
      "Training Epoch: 20 [1900/36045]\tLoss: 720.6576\n",
      "Training Epoch: 20 [1950/36045]\tLoss: 731.7641\n",
      "Training Epoch: 20 [2000/36045]\tLoss: 661.1326\n",
      "Training Epoch: 20 [2050/36045]\tLoss: 665.1588\n",
      "Training Epoch: 20 [2100/36045]\tLoss: 701.3739\n",
      "Training Epoch: 20 [2150/36045]\tLoss: 679.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [2200/36045]\tLoss: 629.8379\n",
      "Training Epoch: 20 [2250/36045]\tLoss: 595.2889\n",
      "Training Epoch: 20 [2300/36045]\tLoss: 624.1754\n",
      "Training Epoch: 20 [2350/36045]\tLoss: 595.8511\n",
      "Training Epoch: 20 [2400/36045]\tLoss: 608.0378\n",
      "Training Epoch: 20 [2450/36045]\tLoss: 770.4548\n",
      "Training Epoch: 20 [2500/36045]\tLoss: 809.3682\n",
      "Training Epoch: 20 [2550/36045]\tLoss: 805.6895\n",
      "Training Epoch: 20 [2600/36045]\tLoss: 815.2191\n",
      "Training Epoch: 20 [2650/36045]\tLoss: 941.2175\n",
      "Training Epoch: 20 [2700/36045]\tLoss: 1028.1331\n",
      "Training Epoch: 20 [2750/36045]\tLoss: 1102.7157\n",
      "Training Epoch: 20 [2800/36045]\tLoss: 1114.0800\n",
      "Training Epoch: 20 [2850/36045]\tLoss: 883.7902\n",
      "Training Epoch: 20 [2900/36045]\tLoss: 851.3303\n",
      "Training Epoch: 20 [2950/36045]\tLoss: 820.9158\n",
      "Training Epoch: 20 [3000/36045]\tLoss: 816.8889\n",
      "Training Epoch: 20 [3050/36045]\tLoss: 848.7861\n",
      "Training Epoch: 20 [3100/36045]\tLoss: 776.8256\n",
      "Training Epoch: 20 [3150/36045]\tLoss: 599.5404\n",
      "Training Epoch: 20 [3200/36045]\tLoss: 622.7605\n",
      "Training Epoch: 20 [3250/36045]\tLoss: 586.0213\n",
      "Training Epoch: 20 [3300/36045]\tLoss: 555.4073\n",
      "Training Epoch: 20 [3350/36045]\tLoss: 585.6397\n",
      "Training Epoch: 20 [3400/36045]\tLoss: 616.0321\n",
      "Training Epoch: 20 [3450/36045]\tLoss: 661.3524\n",
      "Training Epoch: 20 [3500/36045]\tLoss: 646.5234\n",
      "Training Epoch: 20 [3550/36045]\tLoss: 621.8669\n",
      "Training Epoch: 20 [3600/36045]\tLoss: 665.1090\n",
      "Training Epoch: 20 [3650/36045]\tLoss: 769.4874\n",
      "Training Epoch: 20 [3700/36045]\tLoss: 775.2394\n",
      "Training Epoch: 20 [3750/36045]\tLoss: 742.2429\n",
      "Training Epoch: 20 [3800/36045]\tLoss: 733.4241\n",
      "Training Epoch: 20 [3850/36045]\tLoss: 732.6075\n",
      "Training Epoch: 20 [3900/36045]\tLoss: 738.8936\n",
      "Training Epoch: 20 [3950/36045]\tLoss: 711.8908\n",
      "Training Epoch: 20 [4000/36045]\tLoss: 721.1539\n",
      "Training Epoch: 20 [4050/36045]\tLoss: 664.0367\n",
      "Training Epoch: 20 [4100/36045]\tLoss: 646.7706\n",
      "Training Epoch: 20 [4150/36045]\tLoss: 665.9069\n",
      "Training Epoch: 20 [4200/36045]\tLoss: 659.7736\n",
      "Training Epoch: 20 [4250/36045]\tLoss: 661.6465\n",
      "Training Epoch: 20 [4300/36045]\tLoss: 682.0389\n",
      "Training Epoch: 20 [4350/36045]\tLoss: 662.5406\n",
      "Training Epoch: 20 [4400/36045]\tLoss: 634.5969\n",
      "Training Epoch: 20 [4450/36045]\tLoss: 691.4174\n",
      "Training Epoch: 20 [4500/36045]\tLoss: 737.6563\n",
      "Training Epoch: 20 [4550/36045]\tLoss: 745.3167\n",
      "Training Epoch: 20 [4600/36045]\tLoss: 769.5071\n",
      "Training Epoch: 20 [4650/36045]\tLoss: 757.8593\n",
      "Training Epoch: 20 [4700/36045]\tLoss: 701.0175\n",
      "Training Epoch: 20 [4750/36045]\tLoss: 683.7962\n",
      "Training Epoch: 20 [4800/36045]\tLoss: 712.1420\n",
      "Training Epoch: 20 [4850/36045]\tLoss: 698.2818\n",
      "Training Epoch: 20 [4900/36045]\tLoss: 678.7369\n",
      "Training Epoch: 20 [4950/36045]\tLoss: 699.0847\n",
      "Training Epoch: 20 [5000/36045]\tLoss: 732.2438\n",
      "Training Epoch: 20 [5050/36045]\tLoss: 709.1778\n",
      "Training Epoch: 20 [5100/36045]\tLoss: 719.2123\n",
      "Training Epoch: 20 [5150/36045]\tLoss: 703.5170\n",
      "Training Epoch: 20 [5200/36045]\tLoss: 701.6997\n",
      "Training Epoch: 20 [5250/36045]\tLoss: 693.9297\n",
      "Training Epoch: 20 [5300/36045]\tLoss: 695.3937\n",
      "Training Epoch: 20 [5350/36045]\tLoss: 719.8190\n",
      "Training Epoch: 20 [5400/36045]\tLoss: 691.5298\n",
      "Training Epoch: 20 [5450/36045]\tLoss: 655.2522\n",
      "Training Epoch: 20 [5500/36045]\tLoss: 687.8356\n",
      "Training Epoch: 20 [5550/36045]\tLoss: 674.9213\n",
      "Training Epoch: 20 [5600/36045]\tLoss: 763.6400\n",
      "Training Epoch: 20 [5650/36045]\tLoss: 724.7443\n",
      "Training Epoch: 20 [5700/36045]\tLoss: 681.6670\n",
      "Training Epoch: 20 [5750/36045]\tLoss: 666.8061\n",
      "Training Epoch: 20 [5800/36045]\tLoss: 704.6182\n",
      "Training Epoch: 20 [5850/36045]\tLoss: 686.8259\n",
      "Training Epoch: 20 [5900/36045]\tLoss: 791.3932\n",
      "Training Epoch: 20 [5950/36045]\tLoss: 811.2279\n",
      "Training Epoch: 20 [6000/36045]\tLoss: 794.8157\n",
      "Training Epoch: 20 [6050/36045]\tLoss: 767.6611\n",
      "Training Epoch: 20 [6100/36045]\tLoss: 773.3766\n",
      "Training Epoch: 20 [6150/36045]\tLoss: 752.7708\n",
      "Training Epoch: 20 [6200/36045]\tLoss: 752.8983\n",
      "Training Epoch: 20 [6250/36045]\tLoss: 773.3511\n",
      "Training Epoch: 20 [6300/36045]\tLoss: 786.7538\n",
      "Training Epoch: 20 [6350/36045]\tLoss: 837.3659\n",
      "Training Epoch: 20 [6400/36045]\tLoss: 701.6044\n",
      "Training Epoch: 20 [6450/36045]\tLoss: 649.5656\n",
      "Training Epoch: 20 [6500/36045]\tLoss: 662.5167\n",
      "Training Epoch: 20 [6550/36045]\tLoss: 678.4755\n",
      "Training Epoch: 20 [6600/36045]\tLoss: 680.6644\n",
      "Training Epoch: 20 [6650/36045]\tLoss: 770.7148\n",
      "Training Epoch: 20 [6700/36045]\tLoss: 806.8969\n",
      "Training Epoch: 20 [6750/36045]\tLoss: 778.6814\n",
      "Training Epoch: 20 [6800/36045]\tLoss: 781.4764\n",
      "Training Epoch: 20 [6850/36045]\tLoss: 768.8006\n",
      "Training Epoch: 20 [6900/36045]\tLoss: 682.8685\n",
      "Training Epoch: 20 [6950/36045]\tLoss: 644.1132\n",
      "Training Epoch: 20 [7000/36045]\tLoss: 685.3901\n",
      "Training Epoch: 20 [7050/36045]\tLoss: 700.1769\n",
      "Training Epoch: 20 [7100/36045]\tLoss: 699.8737\n",
      "Training Epoch: 20 [7150/36045]\tLoss: 712.6411\n",
      "Training Epoch: 20 [7200/36045]\tLoss: 716.5132\n",
      "Training Epoch: 20 [7250/36045]\tLoss: 713.8264\n",
      "Training Epoch: 20 [7300/36045]\tLoss: 700.4478\n",
      "Training Epoch: 20 [7350/36045]\tLoss: 694.3891\n",
      "Training Epoch: 20 [7400/36045]\tLoss: 624.3871\n",
      "Training Epoch: 20 [7450/36045]\tLoss: 629.8107\n",
      "Training Epoch: 20 [7500/36045]\tLoss: 624.0791\n",
      "Training Epoch: 20 [7550/36045]\tLoss: 597.5350\n",
      "Training Epoch: 20 [7600/36045]\tLoss: 667.7244\n",
      "Training Epoch: 20 [7650/36045]\tLoss: 718.5258\n",
      "Training Epoch: 20 [7700/36045]\tLoss: 685.6990\n",
      "Training Epoch: 20 [7750/36045]\tLoss: 699.9694\n",
      "Training Epoch: 20 [7800/36045]\tLoss: 687.6529\n",
      "Training Epoch: 20 [7850/36045]\tLoss: 662.5778\n",
      "Training Epoch: 20 [7900/36045]\tLoss: 698.8591\n",
      "Training Epoch: 20 [7950/36045]\tLoss: 696.7551\n",
      "Training Epoch: 20 [8000/36045]\tLoss: 714.3090\n",
      "Training Epoch: 20 [8050/36045]\tLoss: 675.9565\n",
      "Training Epoch: 20 [8100/36045]\tLoss: 702.8083\n",
      "Training Epoch: 20 [8150/36045]\tLoss: 795.3037\n",
      "Training Epoch: 20 [8200/36045]\tLoss: 781.4495\n",
      "Training Epoch: 20 [8250/36045]\tLoss: 747.5888\n",
      "Training Epoch: 20 [8300/36045]\tLoss: 812.4475\n",
      "Training Epoch: 20 [8350/36045]\tLoss: 748.1503\n",
      "Training Epoch: 20 [8400/36045]\tLoss: 670.6956\n",
      "Training Epoch: 20 [8450/36045]\tLoss: 629.1135\n",
      "Training Epoch: 20 [8500/36045]\tLoss: 667.4290\n",
      "Training Epoch: 20 [8550/36045]\tLoss: 658.0105\n",
      "Training Epoch: 20 [8600/36045]\tLoss: 650.3717\n",
      "Training Epoch: 20 [8650/36045]\tLoss: 696.0770\n",
      "Training Epoch: 20 [8700/36045]\tLoss: 735.6067\n",
      "Training Epoch: 20 [8750/36045]\tLoss: 721.4838\n",
      "Training Epoch: 20 [8800/36045]\tLoss: 727.0133\n",
      "Training Epoch: 20 [8850/36045]\tLoss: 719.3485\n",
      "Training Epoch: 20 [8900/36045]\tLoss: 649.5800\n",
      "Training Epoch: 20 [8950/36045]\tLoss: 664.5459\n",
      "Training Epoch: 20 [9000/36045]\tLoss: 679.8641\n",
      "Training Epoch: 20 [9050/36045]\tLoss: 679.6703\n",
      "Training Epoch: 20 [9100/36045]\tLoss: 700.0185\n",
      "Training Epoch: 20 [9150/36045]\tLoss: 516.5007\n",
      "Training Epoch: 20 [9200/36045]\tLoss: 388.9111\n",
      "Training Epoch: 20 [9250/36045]\tLoss: 421.5939\n",
      "Training Epoch: 20 [9300/36045]\tLoss: 434.3236\n",
      "Training Epoch: 20 [9350/36045]\tLoss: 399.5943\n",
      "Training Epoch: 20 [9400/36045]\tLoss: 782.9594\n",
      "Training Epoch: 20 [9450/36045]\tLoss: 831.7902\n",
      "Training Epoch: 20 [9500/36045]\tLoss: 818.3863\n",
      "Training Epoch: 20 [9550/36045]\tLoss: 864.9067\n",
      "Training Epoch: 20 [9600/36045]\tLoss: 643.2950\n",
      "Training Epoch: 20 [9650/36045]\tLoss: 646.0977\n",
      "Training Epoch: 20 [9700/36045]\tLoss: 630.8242\n",
      "Training Epoch: 20 [9750/36045]\tLoss: 631.6167\n",
      "Training Epoch: 20 [9800/36045]\tLoss: 820.9268\n",
      "Training Epoch: 20 [9850/36045]\tLoss: 867.2314\n",
      "Training Epoch: 20 [9900/36045]\tLoss: 884.8844\n",
      "Training Epoch: 20 [9950/36045]\tLoss: 861.1786\n",
      "Training Epoch: 20 [10000/36045]\tLoss: 794.6669\n",
      "Training Epoch: 20 [10050/36045]\tLoss: 660.6530\n",
      "Training Epoch: 20 [10100/36045]\tLoss: 666.0453\n",
      "Training Epoch: 20 [10150/36045]\tLoss: 677.1501\n",
      "Training Epoch: 20 [10200/36045]\tLoss: 666.6024\n",
      "Training Epoch: 20 [10250/36045]\tLoss: 792.7542\n",
      "Training Epoch: 20 [10300/36045]\tLoss: 769.3951\n",
      "Training Epoch: 20 [10350/36045]\tLoss: 809.3836\n",
      "Training Epoch: 20 [10400/36045]\tLoss: 800.0112\n",
      "Training Epoch: 20 [10450/36045]\tLoss: 748.7064\n",
      "Training Epoch: 20 [10500/36045]\tLoss: 628.7812\n",
      "Training Epoch: 20 [10550/36045]\tLoss: 624.4577\n",
      "Training Epoch: 20 [10600/36045]\tLoss: 648.5510\n",
      "Training Epoch: 20 [10650/36045]\tLoss: 654.3015\n",
      "Training Epoch: 20 [10700/36045]\tLoss: 744.7746\n",
      "Training Epoch: 20 [10750/36045]\tLoss: 809.7562\n",
      "Training Epoch: 20 [10800/36045]\tLoss: 749.5150\n",
      "Training Epoch: 20 [10850/36045]\tLoss: 793.6372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [10900/36045]\tLoss: 826.0790\n",
      "Training Epoch: 20 [10950/36045]\tLoss: 613.4458\n",
      "Training Epoch: 20 [11000/36045]\tLoss: 607.0726\n",
      "Training Epoch: 20 [11050/36045]\tLoss: 648.6391\n",
      "Training Epoch: 20 [11100/36045]\tLoss: 661.4748\n",
      "Training Epoch: 20 [11150/36045]\tLoss: 716.7919\n",
      "Training Epoch: 20 [11200/36045]\tLoss: 745.6262\n",
      "Training Epoch: 20 [11250/36045]\tLoss: 758.3306\n",
      "Training Epoch: 20 [11300/36045]\tLoss: 738.2474\n",
      "Training Epoch: 20 [11350/36045]\tLoss: 734.7299\n",
      "Training Epoch: 20 [11400/36045]\tLoss: 693.1934\n",
      "Training Epoch: 20 [11450/36045]\tLoss: 657.9642\n",
      "Training Epoch: 20 [11500/36045]\tLoss: 655.6353\n",
      "Training Epoch: 20 [11550/36045]\tLoss: 670.3816\n",
      "Training Epoch: 20 [11600/36045]\tLoss: 735.6874\n",
      "Training Epoch: 20 [11650/36045]\tLoss: 789.5156\n",
      "Training Epoch: 20 [11700/36045]\tLoss: 788.6290\n",
      "Training Epoch: 20 [11750/36045]\tLoss: 808.7066\n",
      "Training Epoch: 20 [11800/36045]\tLoss: 852.3657\n",
      "Training Epoch: 20 [11850/36045]\tLoss: 906.5863\n",
      "Training Epoch: 20 [11900/36045]\tLoss: 1128.9767\n",
      "Training Epoch: 20 [11950/36045]\tLoss: 1129.4302\n",
      "Training Epoch: 20 [12000/36045]\tLoss: 1145.7838\n",
      "Training Epoch: 20 [12050/36045]\tLoss: 1102.7043\n",
      "Training Epoch: 20 [12100/36045]\tLoss: 731.1282\n",
      "Training Epoch: 20 [12150/36045]\tLoss: 567.9111\n",
      "Training Epoch: 20 [12200/36045]\tLoss: 562.0885\n",
      "Training Epoch: 20 [12250/36045]\tLoss: 571.9473\n",
      "Training Epoch: 20 [12300/36045]\tLoss: 723.0998\n",
      "Training Epoch: 20 [12350/36045]\tLoss: 783.1844\n",
      "Training Epoch: 20 [12400/36045]\tLoss: 792.0458\n",
      "Training Epoch: 20 [12450/36045]\tLoss: 778.9091\n",
      "Training Epoch: 20 [12500/36045]\tLoss: 809.7210\n",
      "Training Epoch: 20 [12550/36045]\tLoss: 776.8140\n",
      "Training Epoch: 20 [12600/36045]\tLoss: 718.4191\n",
      "Training Epoch: 20 [12650/36045]\tLoss: 716.8504\n",
      "Training Epoch: 20 [12700/36045]\tLoss: 739.2186\n",
      "Training Epoch: 20 [12750/36045]\tLoss: 739.1310\n",
      "Training Epoch: 20 [12800/36045]\tLoss: 719.8066\n",
      "Training Epoch: 20 [12850/36045]\tLoss: 751.6301\n",
      "Training Epoch: 20 [12900/36045]\tLoss: 720.7925\n",
      "Training Epoch: 20 [12950/36045]\tLoss: 709.2261\n",
      "Training Epoch: 20 [13000/36045]\tLoss: 741.5269\n",
      "Training Epoch: 20 [13050/36045]\tLoss: 675.8047\n",
      "Training Epoch: 20 [13100/36045]\tLoss: 699.5212\n",
      "Training Epoch: 20 [13150/36045]\tLoss: 691.3654\n",
      "Training Epoch: 20 [13200/36045]\tLoss: 666.5197\n",
      "Training Epoch: 20 [13250/36045]\tLoss: 696.0076\n",
      "Training Epoch: 20 [13300/36045]\tLoss: 737.1227\n",
      "Training Epoch: 20 [13350/36045]\tLoss: 715.6066\n",
      "Training Epoch: 20 [13400/36045]\tLoss: 720.4133\n",
      "Training Epoch: 20 [13450/36045]\tLoss: 714.2729\n",
      "Training Epoch: 20 [13500/36045]\tLoss: 738.6931\n",
      "Training Epoch: 20 [13550/36045]\tLoss: 874.6004\n",
      "Training Epoch: 20 [13600/36045]\tLoss: 907.2263\n",
      "Training Epoch: 20 [13650/36045]\tLoss: 987.0435\n",
      "Training Epoch: 20 [13700/36045]\tLoss: 876.0504\n",
      "Training Epoch: 20 [13750/36045]\tLoss: 723.7194\n",
      "Training Epoch: 20 [13800/36045]\tLoss: 697.3791\n",
      "Training Epoch: 20 [13850/36045]\tLoss: 680.4934\n",
      "Training Epoch: 20 [13900/36045]\tLoss: 688.2432\n",
      "Training Epoch: 20 [13950/36045]\tLoss: 734.0048\n",
      "Training Epoch: 20 [14000/36045]\tLoss: 769.9660\n",
      "Training Epoch: 20 [14050/36045]\tLoss: 740.7308\n",
      "Training Epoch: 20 [14100/36045]\tLoss: 736.6470\n",
      "Training Epoch: 20 [14150/36045]\tLoss: 724.3262\n",
      "Training Epoch: 20 [14200/36045]\tLoss: 770.0880\n",
      "Training Epoch: 20 [14250/36045]\tLoss: 842.4152\n",
      "Training Epoch: 20 [14300/36045]\tLoss: 846.1182\n",
      "Training Epoch: 20 [14350/36045]\tLoss: 810.2659\n",
      "Training Epoch: 20 [14400/36045]\tLoss: 795.6873\n",
      "Training Epoch: 20 [14450/36045]\tLoss: 834.8723\n",
      "Training Epoch: 20 [14500/36045]\tLoss: 765.7537\n",
      "Training Epoch: 20 [14550/36045]\tLoss: 797.9097\n",
      "Training Epoch: 20 [14600/36045]\tLoss: 780.6450\n",
      "Training Epoch: 20 [14650/36045]\tLoss: 782.2891\n",
      "Training Epoch: 20 [14700/36045]\tLoss: 737.3462\n",
      "Training Epoch: 20 [14750/36045]\tLoss: 632.3865\n",
      "Training Epoch: 20 [14800/36045]\tLoss: 622.5829\n",
      "Training Epoch: 20 [14850/36045]\tLoss: 629.8895\n",
      "Training Epoch: 20 [14900/36045]\tLoss: 623.3743\n",
      "Training Epoch: 20 [14950/36045]\tLoss: 631.2806\n",
      "Training Epoch: 20 [15000/36045]\tLoss: 648.5327\n",
      "Training Epoch: 20 [15050/36045]\tLoss: 647.0128\n",
      "Training Epoch: 20 [15100/36045]\tLoss: 630.5653\n",
      "Training Epoch: 20 [15150/36045]\tLoss: 622.4557\n",
      "Training Epoch: 20 [15200/36045]\tLoss: 575.0298\n",
      "Training Epoch: 20 [15250/36045]\tLoss: 600.3494\n",
      "Training Epoch: 20 [15300/36045]\tLoss: 584.2539\n",
      "Training Epoch: 20 [15350/36045]\tLoss: 597.9937\n",
      "Training Epoch: 20 [15400/36045]\tLoss: 583.2707\n",
      "Training Epoch: 20 [15450/36045]\tLoss: 570.2175\n",
      "Training Epoch: 20 [15500/36045]\tLoss: 586.9653\n",
      "Training Epoch: 20 [15550/36045]\tLoss: 581.5532\n",
      "Training Epoch: 20 [15600/36045]\tLoss: 654.6919\n",
      "Training Epoch: 20 [15650/36045]\tLoss: 674.8954\n",
      "Training Epoch: 20 [15700/36045]\tLoss: 664.0408\n",
      "Training Epoch: 20 [15750/36045]\tLoss: 655.8375\n",
      "Training Epoch: 20 [15800/36045]\tLoss: 613.3089\n",
      "Training Epoch: 20 [15850/36045]\tLoss: 626.0901\n",
      "Training Epoch: 20 [15900/36045]\tLoss: 636.0740\n",
      "Training Epoch: 20 [15950/36045]\tLoss: 656.5706\n",
      "Training Epoch: 20 [16000/36045]\tLoss: 632.9634\n",
      "Training Epoch: 20 [16050/36045]\tLoss: 602.8681\n",
      "Training Epoch: 20 [16100/36045]\tLoss: 556.6672\n",
      "Training Epoch: 20 [16150/36045]\tLoss: 543.1664\n",
      "Training Epoch: 20 [16200/36045]\tLoss: 656.0925\n",
      "Training Epoch: 20 [16250/36045]\tLoss: 686.5151\n",
      "Training Epoch: 20 [16300/36045]\tLoss: 749.9830\n",
      "Training Epoch: 20 [16350/36045]\tLoss: 767.0367\n",
      "Training Epoch: 20 [16400/36045]\tLoss: 740.0245\n",
      "Training Epoch: 20 [16450/36045]\tLoss: 720.2368\n",
      "Training Epoch: 20 [16500/36045]\tLoss: 718.8684\n",
      "Training Epoch: 20 [16550/36045]\tLoss: 681.6321\n",
      "Training Epoch: 20 [16600/36045]\tLoss: 709.6601\n",
      "Training Epoch: 20 [16650/36045]\tLoss: 731.0435\n",
      "Training Epoch: 20 [16700/36045]\tLoss: 705.5136\n",
      "Training Epoch: 20 [16750/36045]\tLoss: 696.6896\n",
      "Training Epoch: 20 [16800/36045]\tLoss: 709.4939\n",
      "Training Epoch: 20 [16850/36045]\tLoss: 675.1334\n",
      "Training Epoch: 20 [16900/36045]\tLoss: 686.6283\n",
      "Training Epoch: 20 [16950/36045]\tLoss: 714.5075\n",
      "Training Epoch: 20 [17000/36045]\tLoss: 695.4515\n",
      "Training Epoch: 20 [17050/36045]\tLoss: 726.5283\n",
      "Training Epoch: 20 [17100/36045]\tLoss: 724.8394\n",
      "Training Epoch: 20 [17150/36045]\tLoss: 629.9172\n",
      "Training Epoch: 20 [17200/36045]\tLoss: 587.5087\n",
      "Training Epoch: 20 [17250/36045]\tLoss: 614.7040\n",
      "Training Epoch: 20 [17300/36045]\tLoss: 649.0308\n",
      "Training Epoch: 20 [17350/36045]\tLoss: 622.3971\n",
      "Training Epoch: 20 [17400/36045]\tLoss: 642.4120\n",
      "Training Epoch: 20 [17450/36045]\tLoss: 664.2003\n",
      "Training Epoch: 20 [17500/36045]\tLoss: 651.3870\n",
      "Training Epoch: 20 [17550/36045]\tLoss: 652.6563\n",
      "Training Epoch: 20 [17600/36045]\tLoss: 642.0927\n",
      "Training Epoch: 20 [17650/36045]\tLoss: 660.2512\n",
      "Training Epoch: 20 [17700/36045]\tLoss: 638.8044\n",
      "Training Epoch: 20 [17750/36045]\tLoss: 656.3096\n",
      "Training Epoch: 20 [17800/36045]\tLoss: 646.9908\n",
      "Training Epoch: 20 [17850/36045]\tLoss: 648.8251\n",
      "Training Epoch: 20 [17900/36045]\tLoss: 678.5554\n",
      "Training Epoch: 20 [17950/36045]\tLoss: 689.4174\n",
      "Training Epoch: 20 [18000/36045]\tLoss: 679.5252\n",
      "Training Epoch: 20 [18050/36045]\tLoss: 761.0727\n",
      "Training Epoch: 20 [18100/36045]\tLoss: 764.2565\n",
      "Training Epoch: 20 [18150/36045]\tLoss: 773.6871\n",
      "Training Epoch: 20 [18200/36045]\tLoss: 757.0225\n",
      "Training Epoch: 20 [18250/36045]\tLoss: 777.8556\n",
      "Training Epoch: 20 [18300/36045]\tLoss: 718.7940\n",
      "Training Epoch: 20 [18350/36045]\tLoss: 787.6235\n",
      "Training Epoch: 20 [18400/36045]\tLoss: 759.9336\n",
      "Training Epoch: 20 [18450/36045]\tLoss: 740.5972\n",
      "Training Epoch: 20 [18500/36045]\tLoss: 740.1946\n",
      "Training Epoch: 20 [18550/36045]\tLoss: 726.5754\n",
      "Training Epoch: 20 [18600/36045]\tLoss: 716.1122\n",
      "Training Epoch: 20 [18650/36045]\tLoss: 765.7701\n",
      "Training Epoch: 20 [18700/36045]\tLoss: 805.8379\n",
      "Training Epoch: 20 [18750/36045]\tLoss: 789.9061\n",
      "Training Epoch: 20 [18800/36045]\tLoss: 815.2917\n",
      "Training Epoch: 20 [18850/36045]\tLoss: 758.5308\n",
      "Training Epoch: 20 [18900/36045]\tLoss: 812.1411\n",
      "Training Epoch: 20 [18950/36045]\tLoss: 750.3636\n",
      "Training Epoch: 20 [19000/36045]\tLoss: 638.1054\n",
      "Training Epoch: 20 [19050/36045]\tLoss: 617.4734\n",
      "Training Epoch: 20 [19100/36045]\tLoss: 628.1663\n",
      "Training Epoch: 20 [19150/36045]\tLoss: 616.9048\n",
      "Training Epoch: 20 [19200/36045]\tLoss: 643.5727\n",
      "Training Epoch: 20 [19250/36045]\tLoss: 657.8990\n",
      "Training Epoch: 20 [19300/36045]\tLoss: 669.8094\n",
      "Training Epoch: 20 [19350/36045]\tLoss: 653.3484\n",
      "Training Epoch: 20 [19400/36045]\tLoss: 677.0668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [19450/36045]\tLoss: 666.5228\n",
      "Training Epoch: 20 [19500/36045]\tLoss: 669.2670\n",
      "Training Epoch: 20 [19550/36045]\tLoss: 667.6152\n",
      "Training Epoch: 20 [19600/36045]\tLoss: 709.8336\n",
      "Training Epoch: 20 [19650/36045]\tLoss: 929.8511\n",
      "Training Epoch: 20 [19700/36045]\tLoss: 887.7792\n",
      "Training Epoch: 20 [19750/36045]\tLoss: 888.9175\n",
      "Training Epoch: 20 [19800/36045]\tLoss: 885.2255\n",
      "Training Epoch: 20 [19850/36045]\tLoss: 598.1879\n",
      "Training Epoch: 20 [19900/36045]\tLoss: 575.1729\n",
      "Training Epoch: 20 [19950/36045]\tLoss: 579.2021\n",
      "Training Epoch: 20 [20000/36045]\tLoss: 577.3532\n",
      "Training Epoch: 20 [20050/36045]\tLoss: 645.3144\n",
      "Training Epoch: 20 [20100/36045]\tLoss: 650.6995\n",
      "Training Epoch: 20 [20150/36045]\tLoss: 653.7330\n",
      "Training Epoch: 20 [20200/36045]\tLoss: 654.5666\n",
      "Training Epoch: 20 [20250/36045]\tLoss: 697.5246\n",
      "Training Epoch: 20 [20300/36045]\tLoss: 735.5767\n",
      "Training Epoch: 20 [20350/36045]\tLoss: 756.0651\n",
      "Training Epoch: 20 [20400/36045]\tLoss: 771.3451\n",
      "Training Epoch: 20 [20450/36045]\tLoss: 742.6733\n",
      "Training Epoch: 20 [20500/36045]\tLoss: 724.9372\n",
      "Training Epoch: 20 [20550/36045]\tLoss: 639.1771\n",
      "Training Epoch: 20 [20600/36045]\tLoss: 652.3264\n",
      "Training Epoch: 20 [20650/36045]\tLoss: 648.5164\n",
      "Training Epoch: 20 [20700/36045]\tLoss: 635.4723\n",
      "Training Epoch: 20 [20750/36045]\tLoss: 681.2900\n",
      "Training Epoch: 20 [20800/36045]\tLoss: 741.7400\n",
      "Training Epoch: 20 [20850/36045]\tLoss: 729.2658\n",
      "Training Epoch: 20 [20900/36045]\tLoss: 777.6337\n",
      "Training Epoch: 20 [20950/36045]\tLoss: 733.7954\n",
      "Training Epoch: 20 [21000/36045]\tLoss: 692.4381\n",
      "Training Epoch: 20 [21050/36045]\tLoss: 594.2357\n",
      "Training Epoch: 20 [21100/36045]\tLoss: 596.1495\n",
      "Training Epoch: 20 [21150/36045]\tLoss: 637.6783\n",
      "Training Epoch: 20 [21200/36045]\tLoss: 637.0557\n",
      "Training Epoch: 20 [21250/36045]\tLoss: 609.2222\n",
      "Training Epoch: 20 [21300/36045]\tLoss: 711.1224\n",
      "Training Epoch: 20 [21350/36045]\tLoss: 705.1203\n",
      "Training Epoch: 20 [21400/36045]\tLoss: 708.4602\n",
      "Training Epoch: 20 [21450/36045]\tLoss: 715.6071\n",
      "Training Epoch: 20 [21500/36045]\tLoss: 719.1444\n",
      "Training Epoch: 20 [21550/36045]\tLoss: 817.2247\n",
      "Training Epoch: 20 [21600/36045]\tLoss: 818.1273\n",
      "Training Epoch: 20 [21650/36045]\tLoss: 831.8494\n",
      "Training Epoch: 20 [21700/36045]\tLoss: 830.3768\n",
      "Training Epoch: 20 [21750/36045]\tLoss: 799.8888\n",
      "Training Epoch: 20 [21800/36045]\tLoss: 593.6682\n",
      "Training Epoch: 20 [21850/36045]\tLoss: 576.2049\n",
      "Training Epoch: 20 [21900/36045]\tLoss: 588.6216\n",
      "Training Epoch: 20 [21950/36045]\tLoss: 586.0819\n",
      "Training Epoch: 20 [22000/36045]\tLoss: 591.4933\n",
      "Training Epoch: 20 [22050/36045]\tLoss: 620.2811\n",
      "Training Epoch: 20 [22100/36045]\tLoss: 611.4000\n",
      "Training Epoch: 20 [22150/36045]\tLoss: 593.3110\n",
      "Training Epoch: 20 [22200/36045]\tLoss: 612.6226\n",
      "Training Epoch: 20 [22250/36045]\tLoss: 618.3706\n",
      "Training Epoch: 20 [22300/36045]\tLoss: 673.9055\n",
      "Training Epoch: 20 [22350/36045]\tLoss: 701.0989\n",
      "Training Epoch: 20 [22400/36045]\tLoss: 717.2314\n",
      "Training Epoch: 20 [22450/36045]\tLoss: 704.4236\n",
      "Training Epoch: 20 [22500/36045]\tLoss: 684.2845\n",
      "Training Epoch: 20 [22550/36045]\tLoss: 724.8500\n",
      "Training Epoch: 20 [22600/36045]\tLoss: 790.1624\n",
      "Training Epoch: 20 [22650/36045]\tLoss: 828.9302\n",
      "Training Epoch: 20 [22700/36045]\tLoss: 852.7444\n",
      "Training Epoch: 20 [22750/36045]\tLoss: 874.5646\n",
      "Training Epoch: 20 [22800/36045]\tLoss: 909.9269\n",
      "Training Epoch: 20 [22850/36045]\tLoss: 755.8887\n",
      "Training Epoch: 20 [22900/36045]\tLoss: 759.9141\n",
      "Training Epoch: 20 [22950/36045]\tLoss: 736.7758\n",
      "Training Epoch: 20 [23000/36045]\tLoss: 736.7126\n",
      "Training Epoch: 20 [23050/36045]\tLoss: 658.2252\n",
      "Training Epoch: 20 [23100/36045]\tLoss: 674.5997\n",
      "Training Epoch: 20 [23150/36045]\tLoss: 662.4599\n",
      "Training Epoch: 20 [23200/36045]\tLoss: 627.1591\n",
      "Training Epoch: 20 [23250/36045]\tLoss: 630.5038\n",
      "Training Epoch: 20 [23300/36045]\tLoss: 627.1725\n",
      "Training Epoch: 20 [23350/36045]\tLoss: 649.3527\n",
      "Training Epoch: 20 [23400/36045]\tLoss: 703.7601\n",
      "Training Epoch: 20 [23450/36045]\tLoss: 695.2224\n",
      "Training Epoch: 20 [23500/36045]\tLoss: 669.8871\n",
      "Training Epoch: 20 [23550/36045]\tLoss: 719.8586\n",
      "Training Epoch: 20 [23600/36045]\tLoss: 810.3681\n",
      "Training Epoch: 20 [23650/36045]\tLoss: 824.7545\n",
      "Training Epoch: 20 [23700/36045]\tLoss: 835.4924\n",
      "Training Epoch: 20 [23750/36045]\tLoss: 807.6271\n",
      "Training Epoch: 20 [23800/36045]\tLoss: 640.5579\n",
      "Training Epoch: 20 [23850/36045]\tLoss: 668.5510\n",
      "Training Epoch: 20 [23900/36045]\tLoss: 658.7723\n",
      "Training Epoch: 20 [23950/36045]\tLoss: 640.5297\n",
      "Training Epoch: 20 [24000/36045]\tLoss: 616.1885\n",
      "Training Epoch: 20 [24050/36045]\tLoss: 569.8655\n",
      "Training Epoch: 20 [24100/36045]\tLoss: 600.1403\n",
      "Training Epoch: 20 [24150/36045]\tLoss: 595.3347\n",
      "Training Epoch: 20 [24200/36045]\tLoss: 589.4229\n",
      "Training Epoch: 20 [24250/36045]\tLoss: 572.2603\n",
      "Training Epoch: 20 [24300/36045]\tLoss: 616.3196\n",
      "Training Epoch: 20 [24350/36045]\tLoss: 631.4435\n",
      "Training Epoch: 20 [24400/36045]\tLoss: 649.5917\n",
      "Training Epoch: 20 [24450/36045]\tLoss: 620.1035\n",
      "Training Epoch: 20 [24500/36045]\tLoss: 652.8391\n",
      "Training Epoch: 20 [24550/36045]\tLoss: 747.0455\n",
      "Training Epoch: 20 [24600/36045]\tLoss: 739.5863\n",
      "Training Epoch: 20 [24650/36045]\tLoss: 710.3883\n",
      "Training Epoch: 20 [24700/36045]\tLoss: 721.9022\n",
      "Training Epoch: 20 [24750/36045]\tLoss: 667.0435\n",
      "Training Epoch: 20 [24800/36045]\tLoss: 558.5388\n",
      "Training Epoch: 20 [24850/36045]\tLoss: 579.1053\n",
      "Training Epoch: 20 [24900/36045]\tLoss: 575.4161\n",
      "Training Epoch: 20 [24950/36045]\tLoss: 577.3229\n",
      "Training Epoch: 20 [25000/36045]\tLoss: 554.2291\n",
      "Training Epoch: 20 [25050/36045]\tLoss: 529.5446\n",
      "Training Epoch: 20 [25100/36045]\tLoss: 475.6285\n",
      "Training Epoch: 20 [25150/36045]\tLoss: 441.2101\n",
      "Training Epoch: 20 [25200/36045]\tLoss: 436.1881\n",
      "Training Epoch: 20 [25250/36045]\tLoss: 466.5031\n",
      "Training Epoch: 20 [25300/36045]\tLoss: 611.5227\n",
      "Training Epoch: 20 [25350/36045]\tLoss: 609.5912\n",
      "Training Epoch: 20 [25400/36045]\tLoss: 567.8018\n",
      "Training Epoch: 20 [25450/36045]\tLoss: 570.8305\n",
      "Training Epoch: 20 [25500/36045]\tLoss: 620.7872\n",
      "Training Epoch: 20 [25550/36045]\tLoss: 718.5020\n",
      "Training Epoch: 20 [25600/36045]\tLoss: 724.4091\n",
      "Training Epoch: 20 [25650/36045]\tLoss: 698.4534\n",
      "Training Epoch: 20 [25700/36045]\tLoss: 708.3260\n",
      "Training Epoch: 20 [25750/36045]\tLoss: 682.5388\n",
      "Training Epoch: 20 [25800/36045]\tLoss: 429.6658\n",
      "Training Epoch: 20 [25850/36045]\tLoss: 440.1731\n",
      "Training Epoch: 20 [25900/36045]\tLoss: 420.7335\n",
      "Training Epoch: 20 [25950/36045]\tLoss: 429.9900\n",
      "Training Epoch: 20 [26000/36045]\tLoss: 526.6983\n",
      "Training Epoch: 20 [26050/36045]\tLoss: 715.6128\n",
      "Training Epoch: 20 [26100/36045]\tLoss: 744.4865\n",
      "Training Epoch: 20 [26150/36045]\tLoss: 743.6504\n",
      "Training Epoch: 20 [26200/36045]\tLoss: 717.0306\n",
      "Training Epoch: 20 [26250/36045]\tLoss: 749.5530\n",
      "Training Epoch: 20 [26300/36045]\tLoss: 667.4171\n",
      "Training Epoch: 20 [26350/36045]\tLoss: 676.8225\n",
      "Training Epoch: 20 [26400/36045]\tLoss: 655.1749\n",
      "Training Epoch: 20 [26450/36045]\tLoss: 584.1451\n",
      "Training Epoch: 20 [26500/36045]\tLoss: 699.0049\n",
      "Training Epoch: 20 [26550/36045]\tLoss: 704.0668\n",
      "Training Epoch: 20 [26600/36045]\tLoss: 698.7524\n",
      "Training Epoch: 20 [26650/36045]\tLoss: 715.3307\n",
      "Training Epoch: 20 [26700/36045]\tLoss: 695.1459\n",
      "Training Epoch: 20 [26750/36045]\tLoss: 649.9503\n",
      "Training Epoch: 20 [26800/36045]\tLoss: 477.8968\n",
      "Training Epoch: 20 [26850/36045]\tLoss: 397.8478\n",
      "Training Epoch: 20 [26900/36045]\tLoss: 401.2636\n",
      "Training Epoch: 20 [26950/36045]\tLoss: 441.3236\n",
      "Training Epoch: 20 [27000/36045]\tLoss: 709.0060\n",
      "Training Epoch: 20 [27050/36045]\tLoss: 744.1114\n",
      "Training Epoch: 20 [27100/36045]\tLoss: 720.3647\n",
      "Training Epoch: 20 [27150/36045]\tLoss: 763.3021\n",
      "Training Epoch: 20 [27200/36045]\tLoss: 563.2176\n",
      "Training Epoch: 20 [27250/36045]\tLoss: 558.0298\n",
      "Training Epoch: 20 [27300/36045]\tLoss: 542.5366\n",
      "Training Epoch: 20 [27350/36045]\tLoss: 542.2814\n",
      "Training Epoch: 20 [27400/36045]\tLoss: 541.5186\n",
      "Training Epoch: 20 [27450/36045]\tLoss: 681.5068\n",
      "Training Epoch: 20 [27500/36045]\tLoss: 731.7946\n",
      "Training Epoch: 20 [27550/36045]\tLoss: 724.0784\n",
      "Training Epoch: 20 [27600/36045]\tLoss: 734.9503\n",
      "Training Epoch: 20 [27650/36045]\tLoss: 727.3220\n",
      "Training Epoch: 20 [27700/36045]\tLoss: 757.6984\n",
      "Training Epoch: 20 [27750/36045]\tLoss: 770.2542\n",
      "Training Epoch: 20 [27800/36045]\tLoss: 756.1008\n",
      "Training Epoch: 20 [27850/36045]\tLoss: 743.2514\n",
      "Training Epoch: 20 [27900/36045]\tLoss: 667.5020\n",
      "Training Epoch: 20 [27950/36045]\tLoss: 552.8541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 20 [28000/36045]\tLoss: 527.1055\n",
      "Training Epoch: 20 [28050/36045]\tLoss: 539.7794\n",
      "Training Epoch: 20 [28100/36045]\tLoss: 531.1023\n",
      "Training Epoch: 20 [28150/36045]\tLoss: 561.4534\n",
      "Training Epoch: 20 [28200/36045]\tLoss: 566.1349\n",
      "Training Epoch: 20 [28250/36045]\tLoss: 561.7217\n",
      "Training Epoch: 20 [28300/36045]\tLoss: 531.4561\n",
      "Training Epoch: 20 [28350/36045]\tLoss: 527.3454\n",
      "Training Epoch: 20 [28400/36045]\tLoss: 864.9038\n",
      "Training Epoch: 20 [28450/36045]\tLoss: 786.9584\n",
      "Training Epoch: 20 [28500/36045]\tLoss: 682.2962\n",
      "Training Epoch: 20 [28550/36045]\tLoss: 626.3715\n",
      "Training Epoch: 20 [28600/36045]\tLoss: 670.2078\n",
      "Training Epoch: 20 [28650/36045]\tLoss: 758.2094\n",
      "Training Epoch: 20 [28700/36045]\tLoss: 751.5832\n",
      "Training Epoch: 20 [28750/36045]\tLoss: 739.9182\n",
      "Training Epoch: 20 [28800/36045]\tLoss: 749.1425\n",
      "Training Epoch: 20 [28850/36045]\tLoss: 648.4232\n",
      "Training Epoch: 20 [28900/36045]\tLoss: 520.5496\n",
      "Training Epoch: 20 [28950/36045]\tLoss: 516.8929\n",
      "Training Epoch: 20 [29000/36045]\tLoss: 516.3767\n",
      "Training Epoch: 20 [29050/36045]\tLoss: 525.1094\n",
      "Training Epoch: 20 [29100/36045]\tLoss: 546.8928\n",
      "Training Epoch: 20 [29150/36045]\tLoss: 533.4321\n",
      "Training Epoch: 20 [29200/36045]\tLoss: 518.8250\n",
      "Training Epoch: 20 [29250/36045]\tLoss: 505.2138\n",
      "Training Epoch: 20 [29300/36045]\tLoss: 579.2411\n",
      "Training Epoch: 20 [29350/36045]\tLoss: 688.9346\n",
      "Training Epoch: 20 [29400/36045]\tLoss: 707.9776\n",
      "Training Epoch: 20 [29450/36045]\tLoss: 732.6096\n",
      "Training Epoch: 20 [29500/36045]\tLoss: 746.9291\n",
      "Training Epoch: 20 [29550/36045]\tLoss: 709.2015\n",
      "Training Epoch: 20 [29600/36045]\tLoss: 600.7335\n",
      "Training Epoch: 20 [29650/36045]\tLoss: 583.7393\n",
      "Training Epoch: 20 [29700/36045]\tLoss: 519.5579\n",
      "Training Epoch: 20 [29750/36045]\tLoss: 522.1449\n",
      "Training Epoch: 20 [29800/36045]\tLoss: 568.4241\n",
      "Training Epoch: 20 [29850/36045]\tLoss: 640.6440\n",
      "Training Epoch: 20 [29900/36045]\tLoss: 636.3782\n",
      "Training Epoch: 20 [29950/36045]\tLoss: 659.1147\n",
      "Training Epoch: 20 [30000/36045]\tLoss: 636.6275\n",
      "Training Epoch: 20 [30050/36045]\tLoss: 643.1038\n",
      "Training Epoch: 20 [30100/36045]\tLoss: 784.4956\n",
      "Training Epoch: 20 [30150/36045]\tLoss: 769.5391\n",
      "Training Epoch: 20 [30200/36045]\tLoss: 725.9633\n",
      "Training Epoch: 20 [30250/36045]\tLoss: 775.3659\n",
      "Training Epoch: 20 [30300/36045]\tLoss: 762.1736\n",
      "Training Epoch: 20 [30350/36045]\tLoss: 601.4777\n",
      "Training Epoch: 20 [30400/36045]\tLoss: 588.0039\n",
      "Training Epoch: 20 [30450/36045]\tLoss: 587.0936\n",
      "Training Epoch: 20 [30500/36045]\tLoss: 548.5447\n",
      "Training Epoch: 20 [30550/36045]\tLoss: 509.1166\n",
      "Training Epoch: 20 [30600/36045]\tLoss: 492.5648\n",
      "Training Epoch: 20 [30650/36045]\tLoss: 483.9596\n",
      "Training Epoch: 20 [30700/36045]\tLoss: 502.0015\n",
      "Training Epoch: 20 [30750/36045]\tLoss: 487.5423\n",
      "Training Epoch: 20 [30800/36045]\tLoss: 519.0376\n",
      "Training Epoch: 20 [30850/36045]\tLoss: 509.5515\n",
      "Training Epoch: 20 [30900/36045]\tLoss: 524.5269\n",
      "Training Epoch: 20 [30950/36045]\tLoss: 550.8793\n",
      "Training Epoch: 20 [31000/36045]\tLoss: 541.3307\n",
      "Training Epoch: 20 [31050/36045]\tLoss: 451.6541\n",
      "Training Epoch: 20 [31100/36045]\tLoss: 442.2465\n",
      "Training Epoch: 20 [31150/36045]\tLoss: 448.5084\n",
      "Training Epoch: 20 [31200/36045]\tLoss: 561.3552\n",
      "Training Epoch: 20 [31250/36045]\tLoss: 728.9362\n",
      "Training Epoch: 20 [31300/36045]\tLoss: 697.0338\n",
      "Training Epoch: 20 [31350/36045]\tLoss: 713.4694\n",
      "Training Epoch: 20 [31400/36045]\tLoss: 693.0286\n",
      "Training Epoch: 20 [31450/36045]\tLoss: 704.8610\n",
      "Training Epoch: 20 [31500/36045]\tLoss: 715.4097\n",
      "Training Epoch: 20 [31550/36045]\tLoss: 725.3030\n",
      "Training Epoch: 20 [31600/36045]\tLoss: 681.4378\n",
      "Training Epoch: 20 [31650/36045]\tLoss: 727.1833\n",
      "Training Epoch: 20 [31700/36045]\tLoss: 531.4862\n",
      "Training Epoch: 20 [31750/36045]\tLoss: 442.0884\n",
      "Training Epoch: 20 [31800/36045]\tLoss: 420.8113\n",
      "Training Epoch: 20 [31850/36045]\tLoss: 431.0406\n",
      "Training Epoch: 20 [31900/36045]\tLoss: 668.4843\n",
      "Training Epoch: 20 [31950/36045]\tLoss: 855.9284\n",
      "Training Epoch: 20 [32000/36045]\tLoss: 973.1747\n",
      "Training Epoch: 20 [32050/36045]\tLoss: 925.7691\n",
      "Training Epoch: 20 [32100/36045]\tLoss: 913.7796\n",
      "Training Epoch: 20 [32150/36045]\tLoss: 719.1479\n",
      "Training Epoch: 20 [32200/36045]\tLoss: 724.1221\n",
      "Training Epoch: 20 [32250/36045]\tLoss: 735.6235\n",
      "Training Epoch: 20 [32300/36045]\tLoss: 717.5149\n",
      "Training Epoch: 20 [32350/36045]\tLoss: 711.4276\n",
      "Training Epoch: 20 [32400/36045]\tLoss: 668.5845\n",
      "Training Epoch: 20 [32450/36045]\tLoss: 551.5648\n",
      "Training Epoch: 20 [32500/36045]\tLoss: 530.8998\n",
      "Training Epoch: 20 [32550/36045]\tLoss: 533.7652\n",
      "Training Epoch: 20 [32600/36045]\tLoss: 529.5790\n",
      "Training Epoch: 20 [32650/36045]\tLoss: 669.5462\n",
      "Training Epoch: 20 [32700/36045]\tLoss: 728.6873\n",
      "Training Epoch: 20 [32750/36045]\tLoss: 694.4915\n",
      "Training Epoch: 20 [32800/36045]\tLoss: 712.1689\n",
      "Training Epoch: 20 [32850/36045]\tLoss: 658.8756\n",
      "Training Epoch: 20 [32900/36045]\tLoss: 533.4113\n",
      "Training Epoch: 20 [32950/36045]\tLoss: 557.9022\n",
      "Training Epoch: 20 [33000/36045]\tLoss: 557.9945\n",
      "Training Epoch: 20 [33050/36045]\tLoss: 528.6449\n",
      "Training Epoch: 20 [33100/36045]\tLoss: 602.4258\n",
      "Training Epoch: 20 [33150/36045]\tLoss: 814.1807\n",
      "Training Epoch: 20 [33200/36045]\tLoss: 793.9471\n",
      "Training Epoch: 20 [33250/36045]\tLoss: 817.3103\n",
      "Training Epoch: 20 [33300/36045]\tLoss: 869.8669\n",
      "Training Epoch: 20 [33350/36045]\tLoss: 668.1478\n",
      "Training Epoch: 20 [33400/36045]\tLoss: 494.5732\n",
      "Training Epoch: 20 [33450/36045]\tLoss: 489.5814\n",
      "Training Epoch: 20 [33500/36045]\tLoss: 503.4488\n",
      "Training Epoch: 20 [33550/36045]\tLoss: 522.6736\n",
      "Training Epoch: 20 [33600/36045]\tLoss: 524.2242\n",
      "Training Epoch: 20 [33650/36045]\tLoss: 696.2435\n",
      "Training Epoch: 20 [33700/36045]\tLoss: 674.2043\n",
      "Training Epoch: 20 [33750/36045]\tLoss: 697.9221\n",
      "Training Epoch: 20 [33800/36045]\tLoss: 692.5955\n",
      "Training Epoch: 20 [33850/36045]\tLoss: 696.0805\n",
      "Training Epoch: 20 [33900/36045]\tLoss: 705.1199\n",
      "Training Epoch: 20 [33950/36045]\tLoss: 717.1915\n",
      "Training Epoch: 20 [34000/36045]\tLoss: 705.7471\n",
      "Training Epoch: 20 [34050/36045]\tLoss: 710.1212\n",
      "Training Epoch: 20 [34100/36045]\tLoss: 682.7758\n",
      "Training Epoch: 20 [34150/36045]\tLoss: 636.0664\n",
      "Training Epoch: 20 [34200/36045]\tLoss: 602.4507\n",
      "Training Epoch: 20 [34250/36045]\tLoss: 616.6772\n",
      "Training Epoch: 20 [34300/36045]\tLoss: 529.4676\n",
      "Training Epoch: 20 [34350/36045]\tLoss: 556.5457\n",
      "Training Epoch: 20 [34400/36045]\tLoss: 546.0674\n",
      "Training Epoch: 20 [34450/36045]\tLoss: 512.9731\n",
      "Training Epoch: 20 [34500/36045]\tLoss: 548.2392\n",
      "Training Epoch: 20 [34550/36045]\tLoss: 538.7373\n",
      "Training Epoch: 20 [34600/36045]\tLoss: 537.3531\n",
      "Training Epoch: 20 [34650/36045]\tLoss: 648.8173\n",
      "Training Epoch: 20 [34700/36045]\tLoss: 685.5661\n",
      "Training Epoch: 20 [34750/36045]\tLoss: 610.1060\n",
      "Training Epoch: 20 [34800/36045]\tLoss: 695.2150\n",
      "Training Epoch: 20 [34850/36045]\tLoss: 704.6325\n",
      "Training Epoch: 20 [34900/36045]\tLoss: 786.5396\n",
      "Training Epoch: 20 [34950/36045]\tLoss: 774.7812\n",
      "Training Epoch: 20 [35000/36045]\tLoss: 776.1295\n",
      "Training Epoch: 20 [35050/36045]\tLoss: 761.5747\n",
      "Training Epoch: 20 [35100/36045]\tLoss: 631.1671\n",
      "Training Epoch: 20 [35150/36045]\tLoss: 624.4306\n",
      "Training Epoch: 20 [35200/36045]\tLoss: 532.7882\n",
      "Training Epoch: 20 [35250/36045]\tLoss: 584.0997\n",
      "Training Epoch: 20 [35300/36045]\tLoss: 597.5443\n",
      "Training Epoch: 20 [35350/36045]\tLoss: 686.5916\n",
      "Training Epoch: 20 [35400/36045]\tLoss: 726.7809\n",
      "Training Epoch: 20 [35450/36045]\tLoss: 693.6588\n",
      "Training Epoch: 20 [35500/36045]\tLoss: 675.4097\n",
      "Training Epoch: 20 [35550/36045]\tLoss: 659.9093\n",
      "Training Epoch: 20 [35600/36045]\tLoss: 706.8996\n",
      "Training Epoch: 20 [35650/36045]\tLoss: 782.8980\n",
      "Training Epoch: 20 [35700/36045]\tLoss: 709.5891\n",
      "Training Epoch: 20 [35750/36045]\tLoss: 769.6483\n",
      "Training Epoch: 20 [35800/36045]\tLoss: 775.7020\n",
      "Training Epoch: 20 [35850/36045]\tLoss: 750.1105\n",
      "Training Epoch: 20 [35900/36045]\tLoss: 779.5844\n",
      "Training Epoch: 20 [35950/36045]\tLoss: 777.9773\n",
      "Training Epoch: 20 [36000/36045]\tLoss: 767.8951\n",
      "Training Epoch: 20 [36045/36045]\tLoss: 749.3662\n",
      "Training Epoch: 20 [4004/4004]\tLoss: 701.0465\n",
      "Training Epoch: 21 [50/36045]\tLoss: 701.4863\n",
      "Training Epoch: 21 [100/36045]\tLoss: 672.5337\n",
      "Training Epoch: 21 [150/36045]\tLoss: 670.8710\n",
      "Training Epoch: 21 [200/36045]\tLoss: 658.0259\n",
      "Training Epoch: 21 [250/36045]\tLoss: 780.5975\n",
      "Training Epoch: 21 [300/36045]\tLoss: 843.7426\n",
      "Training Epoch: 21 [350/36045]\tLoss: 807.8356\n",
      "Training Epoch: 21 [400/36045]\tLoss: 804.9529\n",
      "Training Epoch: 21 [450/36045]\tLoss: 784.2547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [500/36045]\tLoss: 732.2050\n",
      "Training Epoch: 21 [550/36045]\tLoss: 739.2254\n",
      "Training Epoch: 21 [600/36045]\tLoss: 715.6403\n",
      "Training Epoch: 21 [650/36045]\tLoss: 740.9241\n",
      "Training Epoch: 21 [700/36045]\tLoss: 728.4344\n",
      "Training Epoch: 21 [750/36045]\tLoss: 709.6010\n",
      "Training Epoch: 21 [800/36045]\tLoss: 726.2101\n",
      "Training Epoch: 21 [850/36045]\tLoss: 706.8475\n",
      "Training Epoch: 21 [900/36045]\tLoss: 672.0137\n",
      "Training Epoch: 21 [950/36045]\tLoss: 637.2420\n",
      "Training Epoch: 21 [1000/36045]\tLoss: 612.0837\n",
      "Training Epoch: 21 [1050/36045]\tLoss: 613.9235\n",
      "Training Epoch: 21 [1100/36045]\tLoss: 598.0046\n",
      "Training Epoch: 21 [1150/36045]\tLoss: 607.4229\n",
      "Training Epoch: 21 [1200/36045]\tLoss: 641.3657\n",
      "Training Epoch: 21 [1250/36045]\tLoss: 732.7810\n",
      "Training Epoch: 21 [1300/36045]\tLoss: 738.0892\n",
      "Training Epoch: 21 [1350/36045]\tLoss: 740.8695\n",
      "Training Epoch: 21 [1400/36045]\tLoss: 769.5963\n",
      "Training Epoch: 21 [1450/36045]\tLoss: 744.2285\n",
      "Training Epoch: 21 [1500/36045]\tLoss: 686.9210\n",
      "Training Epoch: 21 [1550/36045]\tLoss: 704.1564\n",
      "Training Epoch: 21 [1600/36045]\tLoss: 714.9296\n",
      "Training Epoch: 21 [1650/36045]\tLoss: 701.0802\n",
      "Training Epoch: 21 [1700/36045]\tLoss: 712.1750\n",
      "Training Epoch: 21 [1750/36045]\tLoss: 753.9138\n",
      "Training Epoch: 21 [1800/36045]\tLoss: 736.1240\n",
      "Training Epoch: 21 [1850/36045]\tLoss: 756.4451\n",
      "Training Epoch: 21 [1900/36045]\tLoss: 707.6384\n",
      "Training Epoch: 21 [1950/36045]\tLoss: 718.5455\n",
      "Training Epoch: 21 [2000/36045]\tLoss: 649.3957\n",
      "Training Epoch: 21 [2050/36045]\tLoss: 653.2347\n",
      "Training Epoch: 21 [2100/36045]\tLoss: 688.8123\n",
      "Training Epoch: 21 [2150/36045]\tLoss: 667.2083\n",
      "Training Epoch: 21 [2200/36045]\tLoss: 618.4500\n",
      "Training Epoch: 21 [2250/36045]\tLoss: 584.4218\n",
      "Training Epoch: 21 [2300/36045]\tLoss: 612.7892\n",
      "Training Epoch: 21 [2350/36045]\tLoss: 585.1371\n",
      "Training Epoch: 21 [2400/36045]\tLoss: 596.7952\n",
      "Training Epoch: 21 [2450/36045]\tLoss: 756.9235\n",
      "Training Epoch: 21 [2500/36045]\tLoss: 795.2972\n",
      "Training Epoch: 21 [2550/36045]\tLoss: 791.6007\n",
      "Training Epoch: 21 [2600/36045]\tLoss: 801.0975\n",
      "Training Epoch: 21 [2650/36045]\tLoss: 926.9050\n",
      "Training Epoch: 21 [2700/36045]\tLoss: 1013.9134\n",
      "Training Epoch: 21 [2750/36045]\tLoss: 1088.0316\n",
      "Training Epoch: 21 [2800/36045]\tLoss: 1099.0470\n",
      "Training Epoch: 21 [2850/36045]\tLoss: 868.4199\n",
      "Training Epoch: 21 [2900/36045]\tLoss: 835.3621\n",
      "Training Epoch: 21 [2950/36045]\tLoss: 805.8006\n",
      "Training Epoch: 21 [3000/36045]\tLoss: 801.5651\n",
      "Training Epoch: 21 [3050/36045]\tLoss: 833.3070\n",
      "Training Epoch: 21 [3100/36045]\tLoss: 762.7553\n",
      "Training Epoch: 21 [3150/36045]\tLoss: 588.6951\n",
      "Training Epoch: 21 [3200/36045]\tLoss: 611.3138\n",
      "Training Epoch: 21 [3250/36045]\tLoss: 575.2665\n",
      "Training Epoch: 21 [3300/36045]\tLoss: 545.2244\n",
      "Training Epoch: 21 [3350/36045]\tLoss: 574.9111\n",
      "Training Epoch: 21 [3400/36045]\tLoss: 604.6243\n",
      "Training Epoch: 21 [3450/36045]\tLoss: 648.9985\n",
      "Training Epoch: 21 [3500/36045]\tLoss: 634.7278\n",
      "Training Epoch: 21 [3550/36045]\tLoss: 610.2646\n",
      "Training Epoch: 21 [3600/36045]\tLoss: 652.9130\n",
      "Training Epoch: 21 [3650/36045]\tLoss: 755.1341\n",
      "Training Epoch: 21 [3700/36045]\tLoss: 760.9707\n",
      "Training Epoch: 21 [3750/36045]\tLoss: 728.2289\n",
      "Training Epoch: 21 [3800/36045]\tLoss: 719.9692\n",
      "Training Epoch: 21 [3850/36045]\tLoss: 719.0648\n",
      "Training Epoch: 21 [3900/36045]\tLoss: 725.1900\n",
      "Training Epoch: 21 [3950/36045]\tLoss: 698.9179\n",
      "Training Epoch: 21 [4000/36045]\tLoss: 707.5200\n",
      "Training Epoch: 21 [4050/36045]\tLoss: 651.3429\n",
      "Training Epoch: 21 [4100/36045]\tLoss: 634.4859\n",
      "Training Epoch: 21 [4150/36045]\tLoss: 653.0701\n",
      "Training Epoch: 21 [4200/36045]\tLoss: 647.1089\n",
      "Training Epoch: 21 [4250/36045]\tLoss: 649.0674\n",
      "Training Epoch: 21 [4300/36045]\tLoss: 669.1470\n",
      "Training Epoch: 21 [4350/36045]\tLoss: 650.0377\n",
      "Training Epoch: 21 [4400/36045]\tLoss: 622.7269\n",
      "Training Epoch: 21 [4450/36045]\tLoss: 678.9299\n",
      "Training Epoch: 21 [4500/36045]\tLoss: 724.6232\n",
      "Training Epoch: 21 [4550/36045]\tLoss: 731.7145\n",
      "Training Epoch: 21 [4600/36045]\tLoss: 755.6545\n",
      "Training Epoch: 21 [4650/36045]\tLoss: 744.0640\n",
      "Training Epoch: 21 [4700/36045]\tLoss: 688.2985\n",
      "Training Epoch: 21 [4750/36045]\tLoss: 671.0070\n",
      "Training Epoch: 21 [4800/36045]\tLoss: 699.0266\n",
      "Training Epoch: 21 [4850/36045]\tLoss: 685.2789\n",
      "Training Epoch: 21 [4900/36045]\tLoss: 666.1234\n",
      "Training Epoch: 21 [4950/36045]\tLoss: 685.6493\n",
      "Training Epoch: 21 [5000/36045]\tLoss: 718.2202\n",
      "Training Epoch: 21 [5050/36045]\tLoss: 695.6510\n",
      "Training Epoch: 21 [5100/36045]\tLoss: 705.8749\n",
      "Training Epoch: 21 [5150/36045]\tLoss: 690.3401\n",
      "Training Epoch: 21 [5200/36045]\tLoss: 688.4801\n",
      "Training Epoch: 21 [5250/36045]\tLoss: 680.8851\n",
      "Training Epoch: 21 [5300/36045]\tLoss: 682.1757\n",
      "Training Epoch: 21 [5350/36045]\tLoss: 706.3190\n",
      "Training Epoch: 21 [5400/36045]\tLoss: 678.9175\n",
      "Training Epoch: 21 [5450/36045]\tLoss: 643.6425\n",
      "Training Epoch: 21 [5500/36045]\tLoss: 675.4379\n",
      "Training Epoch: 21 [5550/36045]\tLoss: 662.8749\n",
      "Training Epoch: 21 [5600/36045]\tLoss: 750.2589\n",
      "Training Epoch: 21 [5650/36045]\tLoss: 711.7682\n",
      "Training Epoch: 21 [5700/36045]\tLoss: 669.1478\n",
      "Training Epoch: 21 [5750/36045]\tLoss: 654.2988\n",
      "Training Epoch: 21 [5800/36045]\tLoss: 691.1484\n",
      "Training Epoch: 21 [5850/36045]\tLoss: 674.2374\n",
      "Training Epoch: 21 [5900/36045]\tLoss: 776.9340\n",
      "Training Epoch: 21 [5950/36045]\tLoss: 796.0721\n",
      "Training Epoch: 21 [6000/36045]\tLoss: 779.7853\n",
      "Training Epoch: 21 [6050/36045]\tLoss: 753.3530\n",
      "Training Epoch: 21 [6100/36045]\tLoss: 758.8861\n",
      "Training Epoch: 21 [6150/36045]\tLoss: 739.6885\n",
      "Training Epoch: 21 [6200/36045]\tLoss: 740.1857\n",
      "Training Epoch: 21 [6250/36045]\tLoss: 760.8833\n",
      "Training Epoch: 21 [6300/36045]\tLoss: 773.7181\n",
      "Training Epoch: 21 [6350/36045]\tLoss: 823.7751\n",
      "Training Epoch: 21 [6400/36045]\tLoss: 689.1613\n",
      "Training Epoch: 21 [6450/36045]\tLoss: 637.9862\n",
      "Training Epoch: 21 [6500/36045]\tLoss: 650.5099\n",
      "Training Epoch: 21 [6550/36045]\tLoss: 666.7686\n",
      "Training Epoch: 21 [6600/36045]\tLoss: 668.5083\n",
      "Training Epoch: 21 [6650/36045]\tLoss: 756.3591\n",
      "Training Epoch: 21 [6700/36045]\tLoss: 791.7479\n",
      "Training Epoch: 21 [6750/36045]\tLoss: 763.9929\n",
      "Training Epoch: 21 [6800/36045]\tLoss: 766.7770\n",
      "Training Epoch: 21 [6850/36045]\tLoss: 754.4363\n",
      "Training Epoch: 21 [6900/36045]\tLoss: 670.6222\n",
      "Training Epoch: 21 [6950/36045]\tLoss: 632.7092\n",
      "Training Epoch: 21 [7000/36045]\tLoss: 673.0605\n",
      "Training Epoch: 21 [7050/36045]\tLoss: 687.5716\n",
      "Training Epoch: 21 [7100/36045]\tLoss: 687.2228\n",
      "Training Epoch: 21 [7150/36045]\tLoss: 699.3085\n",
      "Training Epoch: 21 [7200/36045]\tLoss: 703.3272\n",
      "Training Epoch: 21 [7250/36045]\tLoss: 700.6329\n",
      "Training Epoch: 21 [7300/36045]\tLoss: 687.4932\n",
      "Training Epoch: 21 [7350/36045]\tLoss: 682.0505\n",
      "Training Epoch: 21 [7400/36045]\tLoss: 614.1704\n",
      "Training Epoch: 21 [7450/36045]\tLoss: 619.2996\n",
      "Training Epoch: 21 [7500/36045]\tLoss: 613.2865\n",
      "Training Epoch: 21 [7550/36045]\tLoss: 587.4301\n",
      "Training Epoch: 21 [7600/36045]\tLoss: 655.4988\n",
      "Training Epoch: 21 [7650/36045]\tLoss: 704.9678\n",
      "Training Epoch: 21 [7700/36045]\tLoss: 672.6736\n",
      "Training Epoch: 21 [7750/36045]\tLoss: 686.9451\n",
      "Training Epoch: 21 [7800/36045]\tLoss: 674.9171\n",
      "Training Epoch: 21 [7850/36045]\tLoss: 650.6415\n",
      "Training Epoch: 21 [7900/36045]\tLoss: 686.4014\n",
      "Training Epoch: 21 [7950/36045]\tLoss: 684.0676\n",
      "Training Epoch: 21 [8000/36045]\tLoss: 701.6114\n",
      "Training Epoch: 21 [8050/36045]\tLoss: 663.9739\n",
      "Training Epoch: 21 [8100/36045]\tLoss: 690.3867\n",
      "Training Epoch: 21 [8150/36045]\tLoss: 781.3354\n",
      "Training Epoch: 21 [8200/36045]\tLoss: 767.3542\n",
      "Training Epoch: 21 [8250/36045]\tLoss: 733.7654\n",
      "Training Epoch: 21 [8300/36045]\tLoss: 797.7432\n",
      "Training Epoch: 21 [8350/36045]\tLoss: 734.2747\n",
      "Training Epoch: 21 [8400/36045]\tLoss: 658.4081\n",
      "Training Epoch: 21 [8450/36045]\tLoss: 617.4440\n",
      "Training Epoch: 21 [8500/36045]\tLoss: 655.2661\n",
      "Training Epoch: 21 [8550/36045]\tLoss: 646.0162\n",
      "Training Epoch: 21 [8600/36045]\tLoss: 638.7349\n",
      "Training Epoch: 21 [8650/36045]\tLoss: 683.2711\n",
      "Training Epoch: 21 [8700/36045]\tLoss: 721.9514\n",
      "Training Epoch: 21 [8750/36045]\tLoss: 708.5186\n",
      "Training Epoch: 21 [8800/36045]\tLoss: 713.8395\n",
      "Training Epoch: 21 [8850/36045]\tLoss: 706.4653\n",
      "Training Epoch: 21 [8900/36045]\tLoss: 637.9565\n",
      "Training Epoch: 21 [8950/36045]\tLoss: 652.3025\n",
      "Training Epoch: 21 [9000/36045]\tLoss: 667.9849\n",
      "Training Epoch: 21 [9050/36045]\tLoss: 667.8143\n",
      "Training Epoch: 21 [9100/36045]\tLoss: 687.8481\n",
      "Training Epoch: 21 [9150/36045]\tLoss: 507.5871\n",
      "Training Epoch: 21 [9200/36045]\tLoss: 381.8124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [9250/36045]\tLoss: 414.0417\n",
      "Training Epoch: 21 [9300/36045]\tLoss: 426.4051\n",
      "Training Epoch: 21 [9350/36045]\tLoss: 392.5676\n",
      "Training Epoch: 21 [9400/36045]\tLoss: 768.9730\n",
      "Training Epoch: 21 [9450/36045]\tLoss: 817.0275\n",
      "Training Epoch: 21 [9500/36045]\tLoss: 803.4667\n",
      "Training Epoch: 21 [9550/36045]\tLoss: 849.2461\n",
      "Training Epoch: 21 [9600/36045]\tLoss: 631.6519\n",
      "Training Epoch: 21 [9650/36045]\tLoss: 634.4909\n",
      "Training Epoch: 21 [9700/36045]\tLoss: 619.4362\n",
      "Training Epoch: 21 [9750/36045]\tLoss: 619.9803\n",
      "Training Epoch: 21 [9800/36045]\tLoss: 806.7070\n",
      "Training Epoch: 21 [9850/36045]\tLoss: 852.2147\n",
      "Training Epoch: 21 [9900/36045]\tLoss: 868.8422\n",
      "Training Epoch: 21 [9950/36045]\tLoss: 845.7639\n",
      "Training Epoch: 21 [10000/36045]\tLoss: 780.6556\n",
      "Training Epoch: 21 [10050/36045]\tLoss: 648.1563\n",
      "Training Epoch: 21 [10100/36045]\tLoss: 653.9269\n",
      "Training Epoch: 21 [10150/36045]\tLoss: 664.7373\n",
      "Training Epoch: 21 [10200/36045]\tLoss: 653.8931\n",
      "Training Epoch: 21 [10250/36045]\tLoss: 778.1351\n",
      "Training Epoch: 21 [10300/36045]\tLoss: 755.2532\n",
      "Training Epoch: 21 [10350/36045]\tLoss: 794.5615\n",
      "Training Epoch: 21 [10400/36045]\tLoss: 785.3473\n",
      "Training Epoch: 21 [10450/36045]\tLoss: 735.0191\n",
      "Training Epoch: 21 [10500/36045]\tLoss: 617.0498\n",
      "Training Epoch: 21 [10550/36045]\tLoss: 612.6135\n",
      "Training Epoch: 21 [10600/36045]\tLoss: 636.4367\n",
      "Training Epoch: 21 [10650/36045]\tLoss: 642.4268\n",
      "Training Epoch: 21 [10700/36045]\tLoss: 731.9511\n",
      "Training Epoch: 21 [10750/36045]\tLoss: 796.3884\n",
      "Training Epoch: 21 [10800/36045]\tLoss: 736.8721\n",
      "Training Epoch: 21 [10850/36045]\tLoss: 780.2485\n",
      "Training Epoch: 21 [10900/36045]\tLoss: 812.0805\n",
      "Training Epoch: 21 [10950/36045]\tLoss: 602.5120\n",
      "Training Epoch: 21 [11000/36045]\tLoss: 596.1988\n",
      "Training Epoch: 21 [11050/36045]\tLoss: 637.2127\n",
      "Training Epoch: 21 [11100/36045]\tLoss: 649.8484\n",
      "Training Epoch: 21 [11150/36045]\tLoss: 704.2171\n",
      "Training Epoch: 21 [11200/36045]\tLoss: 732.8937\n",
      "Training Epoch: 21 [11250/36045]\tLoss: 745.4451\n",
      "Training Epoch: 21 [11300/36045]\tLoss: 725.3624\n",
      "Training Epoch: 21 [11350/36045]\tLoss: 721.9401\n",
      "Training Epoch: 21 [11400/36045]\tLoss: 680.9777\n",
      "Training Epoch: 21 [11450/36045]\tLoss: 646.1769\n",
      "Training Epoch: 21 [11500/36045]\tLoss: 643.8745\n",
      "Training Epoch: 21 [11550/36045]\tLoss: 658.0608\n",
      "Training Epoch: 21 [11600/36045]\tLoss: 722.7393\n",
      "Training Epoch: 21 [11650/36045]\tLoss: 776.2438\n",
      "Training Epoch: 21 [11700/36045]\tLoss: 775.3234\n",
      "Training Epoch: 21 [11750/36045]\tLoss: 795.2393\n",
      "Training Epoch: 21 [11800/36045]\tLoss: 838.7186\n",
      "Training Epoch: 21 [11850/36045]\tLoss: 893.6321\n",
      "Training Epoch: 21 [11900/36045]\tLoss: 1114.8934\n",
      "Training Epoch: 21 [11950/36045]\tLoss: 1115.7047\n",
      "Training Epoch: 21 [12000/36045]\tLoss: 1131.4900\n",
      "Training Epoch: 21 [12050/36045]\tLoss: 1088.6652\n",
      "Training Epoch: 21 [12100/36045]\tLoss: 719.6793\n",
      "Training Epoch: 21 [12150/36045]\tLoss: 557.5603\n",
      "Training Epoch: 21 [12200/36045]\tLoss: 551.7990\n",
      "Training Epoch: 21 [12250/36045]\tLoss: 561.5076\n",
      "Training Epoch: 21 [12300/36045]\tLoss: 710.9684\n",
      "Training Epoch: 21 [12350/36045]\tLoss: 770.6383\n",
      "Training Epoch: 21 [12400/36045]\tLoss: 779.2453\n",
      "Training Epoch: 21 [12450/36045]\tLoss: 766.3040\n",
      "Training Epoch: 21 [12500/36045]\tLoss: 796.7307\n",
      "Training Epoch: 21 [12550/36045]\tLoss: 764.1934\n",
      "Training Epoch: 21 [12600/36045]\tLoss: 706.2131\n",
      "Training Epoch: 21 [12650/36045]\tLoss: 704.8288\n",
      "Training Epoch: 21 [12700/36045]\tLoss: 726.9582\n",
      "Training Epoch: 21 [12750/36045]\tLoss: 726.8264\n",
      "Training Epoch: 21 [12800/36045]\tLoss: 707.8826\n",
      "Training Epoch: 21 [12850/36045]\tLoss: 739.2106\n",
      "Training Epoch: 21 [12900/36045]\tLoss: 709.0187\n",
      "Training Epoch: 21 [12950/36045]\tLoss: 697.2499\n",
      "Training Epoch: 21 [13000/36045]\tLoss: 729.5586\n",
      "Training Epoch: 21 [13050/36045]\tLoss: 664.4837\n",
      "Training Epoch: 21 [13100/36045]\tLoss: 687.1206\n",
      "Training Epoch: 21 [13150/36045]\tLoss: 678.8709\n",
      "Training Epoch: 21 [13200/36045]\tLoss: 655.2123\n",
      "Training Epoch: 21 [13250/36045]\tLoss: 683.7031\n",
      "Training Epoch: 21 [13300/36045]\tLoss: 724.6742\n",
      "Training Epoch: 21 [13350/36045]\tLoss: 703.3876\n",
      "Training Epoch: 21 [13400/36045]\tLoss: 707.9204\n",
      "Training Epoch: 21 [13450/36045]\tLoss: 702.3118\n",
      "Training Epoch: 21 [13500/36045]\tLoss: 725.9940\n",
      "Training Epoch: 21 [13550/36045]\tLoss: 861.8145\n",
      "Training Epoch: 21 [13600/36045]\tLoss: 894.4657\n",
      "Training Epoch: 21 [13650/36045]\tLoss: 974.3254\n",
      "Training Epoch: 21 [13700/36045]\tLoss: 864.3480\n",
      "Training Epoch: 21 [13750/36045]\tLoss: 711.0880\n",
      "Training Epoch: 21 [13800/36045]\tLoss: 684.5490\n",
      "Training Epoch: 21 [13850/36045]\tLoss: 667.6570\n",
      "Training Epoch: 21 [13900/36045]\tLoss: 675.2379\n",
      "Training Epoch: 21 [13950/36045]\tLoss: 721.0751\n",
      "Training Epoch: 21 [14000/36045]\tLoss: 756.5915\n",
      "Training Epoch: 21 [14050/36045]\tLoss: 727.9958\n",
      "Training Epoch: 21 [14100/36045]\tLoss: 723.9541\n",
      "Training Epoch: 21 [14150/36045]\tLoss: 711.6794\n",
      "Training Epoch: 21 [14200/36045]\tLoss: 756.6957\n",
      "Training Epoch: 21 [14250/36045]\tLoss: 828.0854\n",
      "Training Epoch: 21 [14300/36045]\tLoss: 831.6484\n",
      "Training Epoch: 21 [14350/36045]\tLoss: 796.4651\n",
      "Training Epoch: 21 [14400/36045]\tLoss: 781.8813\n",
      "Training Epoch: 21 [14450/36045]\tLoss: 820.6907\n",
      "Training Epoch: 21 [14500/36045]\tLoss: 751.5123\n",
      "Training Epoch: 21 [14550/36045]\tLoss: 783.2693\n",
      "Training Epoch: 21 [14600/36045]\tLoss: 766.6327\n",
      "Training Epoch: 21 [14650/36045]\tLoss: 768.0474\n",
      "Training Epoch: 21 [14700/36045]\tLoss: 724.2811\n",
      "Training Epoch: 21 [14750/36045]\tLoss: 621.3170\n",
      "Training Epoch: 21 [14800/36045]\tLoss: 611.4741\n",
      "Training Epoch: 21 [14850/36045]\tLoss: 618.7727\n",
      "Training Epoch: 21 [14900/36045]\tLoss: 612.2178\n",
      "Training Epoch: 21 [14950/36045]\tLoss: 620.1134\n",
      "Training Epoch: 21 [15000/36045]\tLoss: 636.8967\n",
      "Training Epoch: 21 [15050/36045]\tLoss: 635.2252\n",
      "Training Epoch: 21 [15100/36045]\tLoss: 618.8975\n",
      "Training Epoch: 21 [15150/36045]\tLoss: 611.0991\n",
      "Training Epoch: 21 [15200/36045]\tLoss: 564.6863\n",
      "Training Epoch: 21 [15250/36045]\tLoss: 589.5959\n",
      "Training Epoch: 21 [15300/36045]\tLoss: 573.7179\n",
      "Training Epoch: 21 [15350/36045]\tLoss: 587.1756\n",
      "Training Epoch: 21 [15400/36045]\tLoss: 572.3975\n",
      "Training Epoch: 21 [15450/36045]\tLoss: 559.0651\n",
      "Training Epoch: 21 [15500/36045]\tLoss: 575.4623\n",
      "Training Epoch: 21 [15550/36045]\tLoss: 570.3173\n",
      "Training Epoch: 21 [15600/36045]\tLoss: 642.8690\n",
      "Training Epoch: 21 [15650/36045]\tLoss: 662.7255\n",
      "Training Epoch: 21 [15700/36045]\tLoss: 652.1477\n",
      "Training Epoch: 21 [15750/36045]\tLoss: 644.0021\n",
      "Training Epoch: 21 [15800/36045]\tLoss: 603.0947\n",
      "Training Epoch: 21 [15850/36045]\tLoss: 616.1180\n",
      "Training Epoch: 21 [15900/36045]\tLoss: 626.0838\n",
      "Training Epoch: 21 [15950/36045]\tLoss: 646.4133\n",
      "Training Epoch: 21 [16000/36045]\tLoss: 622.2714\n",
      "Training Epoch: 21 [16050/36045]\tLoss: 592.0934\n",
      "Training Epoch: 21 [16100/36045]\tLoss: 546.9131\n",
      "Training Epoch: 21 [16150/36045]\tLoss: 533.7014\n",
      "Training Epoch: 21 [16200/36045]\tLoss: 644.8869\n",
      "Training Epoch: 21 [16250/36045]\tLoss: 675.1800\n",
      "Training Epoch: 21 [16300/36045]\tLoss: 737.5497\n",
      "Training Epoch: 21 [16350/36045]\tLoss: 754.8130\n",
      "Training Epoch: 21 [16400/36045]\tLoss: 727.5902\n",
      "Training Epoch: 21 [16450/36045]\tLoss: 708.0476\n",
      "Training Epoch: 21 [16500/36045]\tLoss: 706.9147\n",
      "Training Epoch: 21 [16550/36045]\tLoss: 670.0527\n",
      "Training Epoch: 21 [16600/36045]\tLoss: 697.3986\n",
      "Training Epoch: 21 [16650/36045]\tLoss: 718.3137\n",
      "Training Epoch: 21 [16700/36045]\tLoss: 693.2868\n",
      "Training Epoch: 21 [16750/36045]\tLoss: 684.5698\n",
      "Training Epoch: 21 [16800/36045]\tLoss: 697.0524\n",
      "Training Epoch: 21 [16850/36045]\tLoss: 663.3439\n",
      "Training Epoch: 21 [16900/36045]\tLoss: 674.5889\n",
      "Training Epoch: 21 [16950/36045]\tLoss: 702.0076\n",
      "Training Epoch: 21 [17000/36045]\tLoss: 683.1862\n",
      "Training Epoch: 21 [17050/36045]\tLoss: 713.5177\n",
      "Training Epoch: 21 [17100/36045]\tLoss: 711.4465\n",
      "Training Epoch: 21 [17150/36045]\tLoss: 618.4722\n",
      "Training Epoch: 21 [17200/36045]\tLoss: 576.6594\n",
      "Training Epoch: 21 [17250/36045]\tLoss: 603.3725\n",
      "Training Epoch: 21 [17300/36045]\tLoss: 637.2750\n",
      "Training Epoch: 21 [17350/36045]\tLoss: 611.3754\n",
      "Training Epoch: 21 [17400/36045]\tLoss: 631.2913\n",
      "Training Epoch: 21 [17450/36045]\tLoss: 652.7494\n",
      "Training Epoch: 21 [17500/36045]\tLoss: 640.0344\n",
      "Training Epoch: 21 [17550/36045]\tLoss: 640.8624\n",
      "Training Epoch: 21 [17600/36045]\tLoss: 631.1199\n",
      "Training Epoch: 21 [17650/36045]\tLoss: 649.1669\n",
      "Training Epoch: 21 [17700/36045]\tLoss: 627.6722\n",
      "Training Epoch: 21 [17750/36045]\tLoss: 645.0600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [17800/36045]\tLoss: 635.6083\n",
      "Training Epoch: 21 [17850/36045]\tLoss: 639.0157\n",
      "Training Epoch: 21 [17900/36045]\tLoss: 668.5936\n",
      "Training Epoch: 21 [17950/36045]\tLoss: 679.6509\n",
      "Training Epoch: 21 [18000/36045]\tLoss: 669.7301\n",
      "Training Epoch: 21 [18050/36045]\tLoss: 748.4650\n",
      "Training Epoch: 21 [18100/36045]\tLoss: 751.4539\n",
      "Training Epoch: 21 [18150/36045]\tLoss: 761.2490\n",
      "Training Epoch: 21 [18200/36045]\tLoss: 744.2234\n",
      "Training Epoch: 21 [18250/36045]\tLoss: 765.2433\n",
      "Training Epoch: 21 [18300/36045]\tLoss: 707.7809\n",
      "Training Epoch: 21 [18350/36045]\tLoss: 776.8546\n",
      "Training Epoch: 21 [18400/36045]\tLoss: 749.3364\n",
      "Training Epoch: 21 [18450/36045]\tLoss: 729.8781\n",
      "Training Epoch: 21 [18500/36045]\tLoss: 729.3599\n",
      "Training Epoch: 21 [18550/36045]\tLoss: 715.8148\n",
      "Training Epoch: 21 [18600/36045]\tLoss: 705.4556\n",
      "Training Epoch: 21 [18650/36045]\tLoss: 754.4854\n",
      "Training Epoch: 21 [18700/36045]\tLoss: 793.9266\n",
      "Training Epoch: 21 [18750/36045]\tLoss: 778.4158\n",
      "Training Epoch: 21 [18800/36045]\tLoss: 803.4753\n",
      "Training Epoch: 21 [18850/36045]\tLoss: 746.9715\n",
      "Training Epoch: 21 [18900/36045]\tLoss: 799.4686\n",
      "Training Epoch: 21 [18950/36045]\tLoss: 738.4460\n",
      "Training Epoch: 21 [19000/36045]\tLoss: 626.6540\n",
      "Training Epoch: 21 [19050/36045]\tLoss: 606.5449\n",
      "Training Epoch: 21 [19100/36045]\tLoss: 617.0179\n",
      "Training Epoch: 21 [19150/36045]\tLoss: 605.6664\n",
      "Training Epoch: 21 [19200/36045]\tLoss: 632.5461\n",
      "Training Epoch: 21 [19250/36045]\tLoss: 646.9506\n",
      "Training Epoch: 21 [19300/36045]\tLoss: 658.5251\n",
      "Training Epoch: 21 [19350/36045]\tLoss: 642.1353\n",
      "Training Epoch: 21 [19400/36045]\tLoss: 665.6155\n",
      "Training Epoch: 21 [19450/36045]\tLoss: 655.3495\n",
      "Training Epoch: 21 [19500/36045]\tLoss: 657.9011\n",
      "Training Epoch: 21 [19550/36045]\tLoss: 656.1996\n",
      "Training Epoch: 21 [19600/36045]\tLoss: 698.2721\n",
      "Training Epoch: 21 [19650/36045]\tLoss: 915.6392\n",
      "Training Epoch: 21 [19700/36045]\tLoss: 873.8795\n",
      "Training Epoch: 21 [19750/36045]\tLoss: 875.3656\n",
      "Training Epoch: 21 [19800/36045]\tLoss: 872.2028\n",
      "Training Epoch: 21 [19850/36045]\tLoss: 587.9879\n",
      "Training Epoch: 21 [19900/36045]\tLoss: 565.1603\n",
      "Training Epoch: 21 [19950/36045]\tLoss: 568.9485\n",
      "Training Epoch: 21 [20000/36045]\tLoss: 567.1048\n",
      "Training Epoch: 21 [20050/36045]\tLoss: 634.0121\n",
      "Training Epoch: 21 [20100/36045]\tLoss: 639.6837\n",
      "Training Epoch: 21 [20150/36045]\tLoss: 642.7900\n",
      "Training Epoch: 21 [20200/36045]\tLoss: 643.6309\n",
      "Training Epoch: 21 [20250/36045]\tLoss: 685.7449\n",
      "Training Epoch: 21 [20300/36045]\tLoss: 723.2509\n",
      "Training Epoch: 21 [20350/36045]\tLoss: 743.2430\n",
      "Training Epoch: 21 [20400/36045]\tLoss: 758.7542\n",
      "Training Epoch: 21 [20450/36045]\tLoss: 729.9926\n",
      "Training Epoch: 21 [20500/36045]\tLoss: 712.8357\n",
      "Training Epoch: 21 [20550/36045]\tLoss: 628.3542\n",
      "Training Epoch: 21 [20600/36045]\tLoss: 641.1204\n",
      "Training Epoch: 21 [20650/36045]\tLoss: 637.2496\n",
      "Training Epoch: 21 [20700/36045]\tLoss: 624.3210\n",
      "Training Epoch: 21 [20750/36045]\tLoss: 669.7393\n",
      "Training Epoch: 21 [20800/36045]\tLoss: 729.0590\n",
      "Training Epoch: 21 [20850/36045]\tLoss: 716.3835\n",
      "Training Epoch: 21 [20900/36045]\tLoss: 764.3411\n",
      "Training Epoch: 21 [20950/36045]\tLoss: 721.2695\n",
      "Training Epoch: 21 [21000/36045]\tLoss: 680.5267\n",
      "Training Epoch: 21 [21050/36045]\tLoss: 583.8834\n",
      "Training Epoch: 21 [21100/36045]\tLoss: 586.0395\n",
      "Training Epoch: 21 [21150/36045]\tLoss: 626.8906\n",
      "Training Epoch: 21 [21200/36045]\tLoss: 626.1860\n",
      "Training Epoch: 21 [21250/36045]\tLoss: 598.9854\n",
      "Training Epoch: 21 [21300/36045]\tLoss: 699.0887\n",
      "Training Epoch: 21 [21350/36045]\tLoss: 692.7744\n",
      "Training Epoch: 21 [21400/36045]\tLoss: 696.1766\n",
      "Training Epoch: 21 [21450/36045]\tLoss: 703.0602\n",
      "Training Epoch: 21 [21500/36045]\tLoss: 706.5983\n",
      "Training Epoch: 21 [21550/36045]\tLoss: 803.9485\n",
      "Training Epoch: 21 [21600/36045]\tLoss: 804.6182\n",
      "Training Epoch: 21 [21650/36045]\tLoss: 818.1793\n",
      "Training Epoch: 21 [21700/36045]\tLoss: 817.3293\n",
      "Training Epoch: 21 [21750/36045]\tLoss: 787.0656\n",
      "Training Epoch: 21 [21800/36045]\tLoss: 583.9535\n",
      "Training Epoch: 21 [21850/36045]\tLoss: 566.5140\n",
      "Training Epoch: 21 [21900/36045]\tLoss: 578.5641\n",
      "Training Epoch: 21 [21950/36045]\tLoss: 576.2503\n",
      "Training Epoch: 21 [22000/36045]\tLoss: 581.3229\n",
      "Training Epoch: 21 [22050/36045]\tLoss: 609.0584\n",
      "Training Epoch: 21 [22100/36045]\tLoss: 600.4899\n",
      "Training Epoch: 21 [22150/36045]\tLoss: 583.1245\n",
      "Training Epoch: 21 [22200/36045]\tLoss: 602.0891\n",
      "Training Epoch: 21 [22250/36045]\tLoss: 607.7552\n",
      "Training Epoch: 21 [22300/36045]\tLoss: 662.6588\n",
      "Training Epoch: 21 [22350/36045]\tLoss: 689.5464\n",
      "Training Epoch: 21 [22400/36045]\tLoss: 705.3735\n",
      "Training Epoch: 21 [22450/36045]\tLoss: 692.7864\n",
      "Training Epoch: 21 [22500/36045]\tLoss: 673.1741\n",
      "Training Epoch: 21 [22550/36045]\tLoss: 712.8370\n",
      "Training Epoch: 21 [22600/36045]\tLoss: 776.3464\n",
      "Training Epoch: 21 [22650/36045]\tLoss: 814.4999\n",
      "Training Epoch: 21 [22700/36045]\tLoss: 838.1907\n",
      "Training Epoch: 21 [22750/36045]\tLoss: 859.6890\n",
      "Training Epoch: 21 [22800/36045]\tLoss: 894.2936\n",
      "Training Epoch: 21 [22850/36045]\tLoss: 743.2732\n",
      "Training Epoch: 21 [22900/36045]\tLoss: 747.3713\n",
      "Training Epoch: 21 [22950/36045]\tLoss: 724.7120\n",
      "Training Epoch: 21 [23000/36045]\tLoss: 724.3235\n",
      "Training Epoch: 21 [23050/36045]\tLoss: 646.5590\n",
      "Training Epoch: 21 [23100/36045]\tLoss: 662.9496\n",
      "Training Epoch: 21 [23150/36045]\tLoss: 650.9350\n",
      "Training Epoch: 21 [23200/36045]\tLoss: 616.3147\n",
      "Training Epoch: 21 [23250/36045]\tLoss: 619.4875\n",
      "Training Epoch: 21 [23300/36045]\tLoss: 616.2250\n",
      "Training Epoch: 21 [23350/36045]\tLoss: 638.3510\n",
      "Training Epoch: 21 [23400/36045]\tLoss: 691.7321\n",
      "Training Epoch: 21 [23450/36045]\tLoss: 683.4310\n",
      "Training Epoch: 21 [23500/36045]\tLoss: 658.6528\n",
      "Training Epoch: 21 [23550/36045]\tLoss: 707.5306\n",
      "Training Epoch: 21 [23600/36045]\tLoss: 796.7114\n",
      "Training Epoch: 21 [23650/36045]\tLoss: 810.9180\n",
      "Training Epoch: 21 [23700/36045]\tLoss: 821.2788\n",
      "Training Epoch: 21 [23750/36045]\tLoss: 793.9107\n",
      "Training Epoch: 21 [23800/36045]\tLoss: 630.4553\n",
      "Training Epoch: 21 [23850/36045]\tLoss: 658.1201\n",
      "Training Epoch: 21 [23900/36045]\tLoss: 648.2803\n",
      "Training Epoch: 21 [23950/36045]\tLoss: 630.2211\n",
      "Training Epoch: 21 [24000/36045]\tLoss: 605.9281\n",
      "Training Epoch: 21 [24050/36045]\tLoss: 560.3314\n",
      "Training Epoch: 21 [24100/36045]\tLoss: 590.0067\n",
      "Training Epoch: 21 [24150/36045]\tLoss: 585.0254\n",
      "Training Epoch: 21 [24200/36045]\tLoss: 579.4509\n",
      "Training Epoch: 21 [24250/36045]\tLoss: 562.5987\n",
      "Training Epoch: 21 [24300/36045]\tLoss: 605.9711\n",
      "Training Epoch: 21 [24350/36045]\tLoss: 620.8828\n",
      "Training Epoch: 21 [24400/36045]\tLoss: 638.7219\n",
      "Training Epoch: 21 [24450/36045]\tLoss: 609.6098\n",
      "Training Epoch: 21 [24500/36045]\tLoss: 641.9016\n",
      "Training Epoch: 21 [24550/36045]\tLoss: 735.5470\n",
      "Training Epoch: 21 [24600/36045]\tLoss: 727.9514\n",
      "Training Epoch: 21 [24650/36045]\tLoss: 699.1522\n",
      "Training Epoch: 21 [24700/36045]\tLoss: 710.4259\n",
      "Training Epoch: 21 [24750/36045]\tLoss: 656.4868\n",
      "Training Epoch: 21 [24800/36045]\tLoss: 548.5822\n",
      "Training Epoch: 21 [24850/36045]\tLoss: 569.0677\n",
      "Training Epoch: 21 [24900/36045]\tLoss: 565.3148\n",
      "Training Epoch: 21 [24950/36045]\tLoss: 567.3285\n",
      "Training Epoch: 21 [25000/36045]\tLoss: 544.9146\n",
      "Training Epoch: 21 [25050/36045]\tLoss: 520.5672\n",
      "Training Epoch: 21 [25100/36045]\tLoss: 467.4880\n",
      "Training Epoch: 21 [25150/36045]\tLoss: 433.5486\n",
      "Training Epoch: 21 [25200/36045]\tLoss: 428.4370\n",
      "Training Epoch: 21 [25250/36045]\tLoss: 458.4034\n",
      "Training Epoch: 21 [25300/36045]\tLoss: 601.0225\n",
      "Training Epoch: 21 [25350/36045]\tLoss: 599.0861\n",
      "Training Epoch: 21 [25400/36045]\tLoss: 557.9313\n",
      "Training Epoch: 21 [25450/36045]\tLoss: 561.0000\n",
      "Training Epoch: 21 [25500/36045]\tLoss: 609.9404\n",
      "Training Epoch: 21 [25550/36045]\tLoss: 706.5901\n",
      "Training Epoch: 21 [25600/36045]\tLoss: 712.4402\n",
      "Training Epoch: 21 [25650/36045]\tLoss: 687.1552\n",
      "Training Epoch: 21 [25700/36045]\tLoss: 696.9419\n",
      "Training Epoch: 21 [25750/36045]\tLoss: 671.3516\n",
      "Training Epoch: 21 [25800/36045]\tLoss: 422.5162\n",
      "Training Epoch: 21 [25850/36045]\tLoss: 432.9856\n",
      "Training Epoch: 21 [25900/36045]\tLoss: 413.7131\n",
      "Training Epoch: 21 [25950/36045]\tLoss: 422.9814\n",
      "Training Epoch: 21 [26000/36045]\tLoss: 518.0700\n",
      "Training Epoch: 21 [26050/36045]\tLoss: 704.1075\n",
      "Training Epoch: 21 [26100/36045]\tLoss: 732.7154\n",
      "Training Epoch: 21 [26150/36045]\tLoss: 732.1768\n",
      "Training Epoch: 21 [26200/36045]\tLoss: 705.5655\n",
      "Training Epoch: 21 [26250/36045]\tLoss: 737.8041\n",
      "Training Epoch: 21 [26300/36045]\tLoss: 658.1580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [26350/36045]\tLoss: 667.7623\n",
      "Training Epoch: 21 [26400/36045]\tLoss: 645.9745\n",
      "Training Epoch: 21 [26450/36045]\tLoss: 575.0903\n",
      "Training Epoch: 21 [26500/36045]\tLoss: 687.6547\n",
      "Training Epoch: 21 [26550/36045]\tLoss: 692.0795\n",
      "Training Epoch: 21 [26600/36045]\tLoss: 686.9344\n",
      "Training Epoch: 21 [26650/36045]\tLoss: 703.4360\n",
      "Training Epoch: 21 [26700/36045]\tLoss: 683.2648\n",
      "Training Epoch: 21 [26750/36045]\tLoss: 638.9765\n",
      "Training Epoch: 21 [26800/36045]\tLoss: 469.9813\n",
      "Training Epoch: 21 [26850/36045]\tLoss: 391.0096\n",
      "Training Epoch: 21 [26900/36045]\tLoss: 394.4415\n",
      "Training Epoch: 21 [26950/36045]\tLoss: 433.8060\n",
      "Training Epoch: 21 [27000/36045]\tLoss: 698.1774\n",
      "Training Epoch: 21 [27050/36045]\tLoss: 732.2332\n",
      "Training Epoch: 21 [27100/36045]\tLoss: 708.9056\n",
      "Training Epoch: 21 [27150/36045]\tLoss: 751.4590\n",
      "Training Epoch: 21 [27200/36045]\tLoss: 553.9590\n",
      "Training Epoch: 21 [27250/36045]\tLoss: 548.4363\n",
      "Training Epoch: 21 [27300/36045]\tLoss: 533.3177\n",
      "Training Epoch: 21 [27350/36045]\tLoss: 532.9197\n",
      "Training Epoch: 21 [27400/36045]\tLoss: 532.1570\n",
      "Training Epoch: 21 [27450/36045]\tLoss: 669.8600\n",
      "Training Epoch: 21 [27500/36045]\tLoss: 719.1830\n",
      "Training Epoch: 21 [27550/36045]\tLoss: 711.4391\n",
      "Training Epoch: 21 [27600/36045]\tLoss: 722.5751\n",
      "Training Epoch: 21 [27650/36045]\tLoss: 714.7685\n",
      "Training Epoch: 21 [27700/36045]\tLoss: 745.0840\n",
      "Training Epoch: 21 [27750/36045]\tLoss: 757.5790\n",
      "Training Epoch: 21 [27800/36045]\tLoss: 743.5698\n",
      "Training Epoch: 21 [27850/36045]\tLoss: 731.1343\n",
      "Training Epoch: 21 [27900/36045]\tLoss: 657.0472\n",
      "Training Epoch: 21 [27950/36045]\tLoss: 544.3704\n",
      "Training Epoch: 21 [28000/36045]\tLoss: 518.9586\n",
      "Training Epoch: 21 [28050/36045]\tLoss: 531.2217\n",
      "Training Epoch: 21 [28100/36045]\tLoss: 522.6801\n",
      "Training Epoch: 21 [28150/36045]\tLoss: 552.2465\n",
      "Training Epoch: 21 [28200/36045]\tLoss: 557.2627\n",
      "Training Epoch: 21 [28250/36045]\tLoss: 552.2872\n",
      "Training Epoch: 21 [28300/36045]\tLoss: 522.8079\n",
      "Training Epoch: 21 [28350/36045]\tLoss: 518.7325\n",
      "Training Epoch: 21 [28400/36045]\tLoss: 855.5356\n",
      "Training Epoch: 21 [28450/36045]\tLoss: 779.2693\n",
      "Training Epoch: 21 [28500/36045]\tLoss: 675.4052\n",
      "Training Epoch: 21 [28550/36045]\tLoss: 619.8904\n",
      "Training Epoch: 21 [28600/36045]\tLoss: 661.6642\n",
      "Training Epoch: 21 [28650/36045]\tLoss: 746.4471\n",
      "Training Epoch: 21 [28700/36045]\tLoss: 740.1824\n",
      "Training Epoch: 21 [28750/36045]\tLoss: 728.2700\n",
      "Training Epoch: 21 [28800/36045]\tLoss: 737.3774\n",
      "Training Epoch: 21 [28850/36045]\tLoss: 638.2170\n",
      "Training Epoch: 21 [28900/36045]\tLoss: 512.9009\n",
      "Training Epoch: 21 [28950/36045]\tLoss: 509.6479\n",
      "Training Epoch: 21 [29000/36045]\tLoss: 508.8758\n",
      "Training Epoch: 21 [29050/36045]\tLoss: 517.4860\n",
      "Training Epoch: 21 [29100/36045]\tLoss: 539.0355\n",
      "Training Epoch: 21 [29150/36045]\tLoss: 525.7933\n",
      "Training Epoch: 21 [29200/36045]\tLoss: 510.9960\n",
      "Training Epoch: 21 [29250/36045]\tLoss: 497.7625\n",
      "Training Epoch: 21 [29300/36045]\tLoss: 570.0463\n",
      "Training Epoch: 21 [29350/36045]\tLoss: 677.5131\n",
      "Training Epoch: 21 [29400/36045]\tLoss: 696.5693\n",
      "Training Epoch: 21 [29450/36045]\tLoss: 720.1677\n",
      "Training Epoch: 21 [29500/36045]\tLoss: 734.3835\n",
      "Training Epoch: 21 [29550/36045]\tLoss: 697.4283\n",
      "Training Epoch: 21 [29600/36045]\tLoss: 590.2795\n",
      "Training Epoch: 21 [29650/36045]\tLoss: 573.3881\n",
      "Training Epoch: 21 [29700/36045]\tLoss: 510.7316\n",
      "Training Epoch: 21 [29750/36045]\tLoss: 512.8338\n",
      "Training Epoch: 21 [29800/36045]\tLoss: 559.0783\n",
      "Training Epoch: 21 [29850/36045]\tLoss: 631.4695\n",
      "Training Epoch: 21 [29900/36045]\tLoss: 627.4896\n",
      "Training Epoch: 21 [29950/36045]\tLoss: 650.1751\n",
      "Training Epoch: 21 [30000/36045]\tLoss: 627.3488\n",
      "Training Epoch: 21 [30050/36045]\tLoss: 633.8524\n",
      "Training Epoch: 21 [30100/36045]\tLoss: 773.1749\n",
      "Training Epoch: 21 [30150/36045]\tLoss: 758.0491\n",
      "Training Epoch: 21 [30200/36045]\tLoss: 714.9189\n",
      "Training Epoch: 21 [30250/36045]\tLoss: 764.3809\n",
      "Training Epoch: 21 [30300/36045]\tLoss: 750.8930\n",
      "Training Epoch: 21 [30350/36045]\tLoss: 591.3616\n",
      "Training Epoch: 21 [30400/36045]\tLoss: 577.6699\n",
      "Training Epoch: 21 [30450/36045]\tLoss: 576.6982\n",
      "Training Epoch: 21 [30500/36045]\tLoss: 539.2186\n",
      "Training Epoch: 21 [30550/36045]\tLoss: 500.0887\n",
      "Training Epoch: 21 [30600/36045]\tLoss: 484.6749\n",
      "Training Epoch: 21 [30650/36045]\tLoss: 475.9649\n",
      "Training Epoch: 21 [30700/36045]\tLoss: 493.6657\n",
      "Training Epoch: 21 [30750/36045]\tLoss: 479.7914\n",
      "Training Epoch: 21 [30800/36045]\tLoss: 510.1472\n",
      "Training Epoch: 21 [30850/36045]\tLoss: 501.0294\n",
      "Training Epoch: 21 [30900/36045]\tLoss: 515.5612\n",
      "Training Epoch: 21 [30950/36045]\tLoss: 541.3879\n",
      "Training Epoch: 21 [31000/36045]\tLoss: 532.2686\n",
      "Training Epoch: 21 [31050/36045]\tLoss: 444.0025\n",
      "Training Epoch: 21 [31100/36045]\tLoss: 434.9472\n",
      "Training Epoch: 21 [31150/36045]\tLoss: 441.0071\n",
      "Training Epoch: 21 [31200/36045]\tLoss: 551.8832\n",
      "Training Epoch: 21 [31250/36045]\tLoss: 716.1595\n",
      "Training Epoch: 21 [31300/36045]\tLoss: 684.7657\n",
      "Training Epoch: 21 [31350/36045]\tLoss: 701.1974\n",
      "Training Epoch: 21 [31400/36045]\tLoss: 680.3210\n",
      "Training Epoch: 21 [31450/36045]\tLoss: 693.2880\n",
      "Training Epoch: 21 [31500/36045]\tLoss: 704.0977\n",
      "Training Epoch: 21 [31550/36045]\tLoss: 713.6874\n",
      "Training Epoch: 21 [31600/36045]\tLoss: 670.7449\n",
      "Training Epoch: 21 [31650/36045]\tLoss: 715.6425\n",
      "Training Epoch: 21 [31700/36045]\tLoss: 522.3358\n",
      "Training Epoch: 21 [31750/36045]\tLoss: 434.2590\n",
      "Training Epoch: 21 [31800/36045]\tLoss: 413.2508\n",
      "Training Epoch: 21 [31850/36045]\tLoss: 423.3930\n",
      "Training Epoch: 21 [31900/36045]\tLoss: 657.4581\n",
      "Training Epoch: 21 [31950/36045]\tLoss: 843.0527\n",
      "Training Epoch: 21 [32000/36045]\tLoss: 959.1927\n",
      "Training Epoch: 21 [32050/36045]\tLoss: 911.9990\n",
      "Training Epoch: 21 [32100/36045]\tLoss: 900.4247\n",
      "Training Epoch: 21 [32150/36045]\tLoss: 707.3702\n",
      "Training Epoch: 21 [32200/36045]\tLoss: 712.3710\n",
      "Training Epoch: 21 [32250/36045]\tLoss: 723.6478\n",
      "Training Epoch: 21 [32300/36045]\tLoss: 705.4770\n",
      "Training Epoch: 21 [32350/36045]\tLoss: 699.7574\n",
      "Training Epoch: 21 [32400/36045]\tLoss: 657.2051\n",
      "Training Epoch: 21 [32450/36045]\tLoss: 542.1715\n",
      "Training Epoch: 21 [32500/36045]\tLoss: 521.6860\n",
      "Training Epoch: 21 [32550/36045]\tLoss: 524.5002\n",
      "Training Epoch: 21 [32600/36045]\tLoss: 520.5585\n",
      "Training Epoch: 21 [32650/36045]\tLoss: 659.3864\n",
      "Training Epoch: 21 [32700/36045]\tLoss: 718.0387\n",
      "Training Epoch: 21 [32750/36045]\tLoss: 684.2533\n",
      "Training Epoch: 21 [32800/36045]\tLoss: 701.8804\n",
      "Training Epoch: 21 [32850/36045]\tLoss: 648.9377\n",
      "Training Epoch: 21 [32900/36045]\tLoss: 524.7549\n",
      "Training Epoch: 21 [32950/36045]\tLoss: 548.9113\n",
      "Training Epoch: 21 [33000/36045]\tLoss: 548.8341\n",
      "Training Epoch: 21 [33050/36045]\tLoss: 520.2518\n",
      "Training Epoch: 21 [33100/36045]\tLoss: 592.4946\n",
      "Training Epoch: 21 [33150/36045]\tLoss: 801.3236\n",
      "Training Epoch: 21 [33200/36045]\tLoss: 781.2619\n",
      "Training Epoch: 21 [33250/36045]\tLoss: 804.3965\n",
      "Training Epoch: 21 [33300/36045]\tLoss: 856.1325\n",
      "Training Epoch: 21 [33350/36045]\tLoss: 657.5054\n",
      "Training Epoch: 21 [33400/36045]\tLoss: 486.1196\n",
      "Training Epoch: 21 [33450/36045]\tLoss: 481.1189\n",
      "Training Epoch: 21 [33500/36045]\tLoss: 494.9105\n",
      "Training Epoch: 21 [33550/36045]\tLoss: 513.7639\n",
      "Training Epoch: 21 [33600/36045]\tLoss: 515.2673\n",
      "Training Epoch: 21 [33650/36045]\tLoss: 684.8705\n",
      "Training Epoch: 21 [33700/36045]\tLoss: 663.1039\n",
      "Training Epoch: 21 [33750/36045]\tLoss: 686.5003\n",
      "Training Epoch: 21 [33800/36045]\tLoss: 681.2154\n",
      "Training Epoch: 21 [33850/36045]\tLoss: 684.6005\n",
      "Training Epoch: 21 [33900/36045]\tLoss: 694.1063\n",
      "Training Epoch: 21 [33950/36045]\tLoss: 706.1370\n",
      "Training Epoch: 21 [34000/36045]\tLoss: 694.2773\n",
      "Training Epoch: 21 [34050/36045]\tLoss: 698.6790\n",
      "Training Epoch: 21 [34100/36045]\tLoss: 671.9302\n",
      "Training Epoch: 21 [34150/36045]\tLoss: 625.7997\n",
      "Training Epoch: 21 [34200/36045]\tLoss: 592.6145\n",
      "Training Epoch: 21 [34250/36045]\tLoss: 606.8051\n",
      "Training Epoch: 21 [34300/36045]\tLoss: 520.8270\n",
      "Training Epoch: 21 [34350/36045]\tLoss: 547.6289\n",
      "Training Epoch: 21 [34400/36045]\tLoss: 537.5578\n",
      "Training Epoch: 21 [34450/36045]\tLoss: 504.9462\n",
      "Training Epoch: 21 [34500/36045]\tLoss: 539.5012\n",
      "Training Epoch: 21 [34550/36045]\tLoss: 529.9620\n",
      "Training Epoch: 21 [34600/36045]\tLoss: 529.4142\n",
      "Training Epoch: 21 [34650/36045]\tLoss: 640.2943\n",
      "Training Epoch: 21 [34700/36045]\tLoss: 676.8115\n",
      "Training Epoch: 21 [34750/36045]\tLoss: 602.1381\n",
      "Training Epoch: 21 [34800/36045]\tLoss: 686.4613\n",
      "Training Epoch: 21 [34850/36045]\tLoss: 695.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 21 [34900/36045]\tLoss: 774.2623\n",
      "Training Epoch: 21 [34950/36045]\tLoss: 762.2745\n",
      "Training Epoch: 21 [35000/36045]\tLoss: 763.5713\n",
      "Training Epoch: 21 [35050/36045]\tLoss: 749.1204\n",
      "Training Epoch: 21 [35100/36045]\tLoss: 622.8605\n",
      "Training Epoch: 21 [35150/36045]\tLoss: 616.0516\n",
      "Training Epoch: 21 [35200/36045]\tLoss: 525.0751\n",
      "Training Epoch: 21 [35250/36045]\tLoss: 575.8544\n",
      "Training Epoch: 21 [35300/36045]\tLoss: 589.4656\n",
      "Training Epoch: 21 [35350/36045]\tLoss: 675.6639\n",
      "Training Epoch: 21 [35400/36045]\tLoss: 714.8328\n",
      "Training Epoch: 21 [35450/36045]\tLoss: 682.3188\n",
      "Training Epoch: 21 [35500/36045]\tLoss: 664.0715\n",
      "Training Epoch: 21 [35550/36045]\tLoss: 648.7114\n",
      "Training Epoch: 21 [35600/36045]\tLoss: 695.8369\n",
      "Training Epoch: 21 [35650/36045]\tLoss: 771.3679\n",
      "Training Epoch: 21 [35700/36045]\tLoss: 698.2004\n",
      "Training Epoch: 21 [35750/36045]\tLoss: 757.9460\n",
      "Training Epoch: 21 [35800/36045]\tLoss: 764.1002\n",
      "Training Epoch: 21 [35850/36045]\tLoss: 738.5128\n",
      "Training Epoch: 21 [35900/36045]\tLoss: 767.3287\n",
      "Training Epoch: 21 [35950/36045]\tLoss: 765.4731\n",
      "Training Epoch: 21 [36000/36045]\tLoss: 755.8049\n",
      "Training Epoch: 21 [36045/36045]\tLoss: 737.5588\n",
      "Training Epoch: 21 [4004/4004]\tLoss: 689.6654\n",
      "Training Epoch: 22 [50/36045]\tLoss: 690.0181\n",
      "Training Epoch: 22 [100/36045]\tLoss: 661.6063\n",
      "Training Epoch: 22 [150/36045]\tLoss: 659.7973\n",
      "Training Epoch: 22 [200/36045]\tLoss: 646.8622\n",
      "Training Epoch: 22 [250/36045]\tLoss: 767.9689\n",
      "Training Epoch: 22 [300/36045]\tLoss: 831.1880\n",
      "Training Epoch: 22 [350/36045]\tLoss: 795.7081\n",
      "Training Epoch: 22 [400/36045]\tLoss: 792.5555\n",
      "Training Epoch: 22 [450/36045]\tLoss: 772.0143\n",
      "Training Epoch: 22 [500/36045]\tLoss: 720.4174\n",
      "Training Epoch: 22 [550/36045]\tLoss: 726.6420\n",
      "Training Epoch: 22 [600/36045]\tLoss: 703.8738\n",
      "Training Epoch: 22 [650/36045]\tLoss: 728.8115\n",
      "Training Epoch: 22 [700/36045]\tLoss: 716.3798\n",
      "Training Epoch: 22 [750/36045]\tLoss: 697.9760\n",
      "Training Epoch: 22 [800/36045]\tLoss: 714.1403\n",
      "Training Epoch: 22 [850/36045]\tLoss: 694.8112\n",
      "Training Epoch: 22 [900/36045]\tLoss: 660.6162\n",
      "Training Epoch: 22 [950/36045]\tLoss: 626.2024\n",
      "Training Epoch: 22 [1000/36045]\tLoss: 601.9523\n",
      "Training Epoch: 22 [1050/36045]\tLoss: 603.9891\n",
      "Training Epoch: 22 [1100/36045]\tLoss: 588.3963\n",
      "Training Epoch: 22 [1150/36045]\tLoss: 597.7952\n",
      "Training Epoch: 22 [1200/36045]\tLoss: 630.9816\n",
      "Training Epoch: 22 [1250/36045]\tLoss: 721.0505\n",
      "Training Epoch: 22 [1300/36045]\tLoss: 726.4719\n",
      "Training Epoch: 22 [1350/36045]\tLoss: 729.1903\n",
      "Training Epoch: 22 [1400/36045]\tLoss: 757.7021\n",
      "Training Epoch: 22 [1450/36045]\tLoss: 732.9856\n",
      "Training Epoch: 22 [1500/36045]\tLoss: 675.9355\n",
      "Training Epoch: 22 [1550/36045]\tLoss: 692.6282\n",
      "Training Epoch: 22 [1600/36045]\tLoss: 703.3099\n",
      "Training Epoch: 22 [1650/36045]\tLoss: 689.5962\n",
      "Training Epoch: 22 [1700/36045]\tLoss: 700.9249\n",
      "Training Epoch: 22 [1750/36045]\tLoss: 742.8257\n",
      "Training Epoch: 22 [1800/36045]\tLoss: 724.7708\n",
      "Training Epoch: 22 [1850/36045]\tLoss: 744.3781\n",
      "Training Epoch: 22 [1900/36045]\tLoss: 696.4396\n",
      "Training Epoch: 22 [1950/36045]\tLoss: 707.2205\n",
      "Training Epoch: 22 [2000/36045]\tLoss: 639.3910\n",
      "Training Epoch: 22 [2050/36045]\tLoss: 643.0695\n",
      "Training Epoch: 22 [2100/36045]\tLoss: 678.0754\n",
      "Training Epoch: 22 [2150/36045]\tLoss: 656.6160\n",
      "Training Epoch: 22 [2200/36045]\tLoss: 608.6805\n",
      "Training Epoch: 22 [2250/36045]\tLoss: 575.0944\n",
      "Training Epoch: 22 [2300/36045]\tLoss: 603.0258\n",
      "Training Epoch: 22 [2350/36045]\tLoss: 575.9688\n",
      "Training Epoch: 22 [2400/36045]\tLoss: 587.1777\n",
      "Training Epoch: 22 [2450/36045]\tLoss: 745.3201\n",
      "Training Epoch: 22 [2500/36045]\tLoss: 783.1659\n",
      "Training Epoch: 22 [2550/36045]\tLoss: 779.5107\n",
      "Training Epoch: 22 [2600/36045]\tLoss: 788.9282\n",
      "Training Epoch: 22 [2650/36045]\tLoss: 914.6035\n",
      "Training Epoch: 22 [2700/36045]\tLoss: 1001.7224\n",
      "Training Epoch: 22 [2750/36045]\tLoss: 1075.4503\n",
      "Training Epoch: 22 [2800/36045]\tLoss: 1086.1843\n",
      "Training Epoch: 22 [2850/36045]\tLoss: 855.2558\n",
      "Training Epoch: 22 [2900/36045]\tLoss: 821.7266\n",
      "Training Epoch: 22 [2950/36045]\tLoss: 792.9088\n",
      "Training Epoch: 22 [3000/36045]\tLoss: 788.4758\n",
      "Training Epoch: 22 [3050/36045]\tLoss: 820.0983\n",
      "Training Epoch: 22 [3100/36045]\tLoss: 750.6757\n",
      "Training Epoch: 22 [3150/36045]\tLoss: 579.3094\n",
      "Training Epoch: 22 [3200/36045]\tLoss: 601.4422\n",
      "Training Epoch: 22 [3250/36045]\tLoss: 566.0270\n",
      "Training Epoch: 22 [3300/36045]\tLoss: 536.4533\n",
      "Training Epoch: 22 [3350/36045]\tLoss: 565.6855\n",
      "Training Epoch: 22 [3400/36045]\tLoss: 594.7797\n",
      "Training Epoch: 22 [3450/36045]\tLoss: 638.3779\n",
      "Training Epoch: 22 [3500/36045]\tLoss: 624.5176\n",
      "Training Epoch: 22 [3550/36045]\tLoss: 600.2072\n",
      "Training Epoch: 22 [3600/36045]\tLoss: 642.3547\n",
      "Training Epoch: 22 [3650/36045]\tLoss: 742.7527\n",
      "Training Epoch: 22 [3700/36045]\tLoss: 748.6771\n",
      "Training Epoch: 22 [3750/36045]\tLoss: 716.1664\n",
      "Training Epoch: 22 [3800/36045]\tLoss: 708.4199\n",
      "Training Epoch: 22 [3850/36045]\tLoss: 707.4907\n",
      "Training Epoch: 22 [3900/36045]\tLoss: 713.4801\n",
      "Training Epoch: 22 [3950/36045]\tLoss: 687.7935\n",
      "Training Epoch: 22 [4000/36045]\tLoss: 695.8542\n",
      "Training Epoch: 22 [4050/36045]\tLoss: 640.4341\n",
      "Training Epoch: 22 [4100/36045]\tLoss: 623.9269\n",
      "Training Epoch: 22 [4150/36045]\tLoss: 642.0729\n",
      "Training Epoch: 22 [4200/36045]\tLoss: 636.2501\n",
      "Training Epoch: 22 [4250/36045]\tLoss: 638.2929\n",
      "Training Epoch: 22 [4300/36045]\tLoss: 658.0858\n",
      "Training Epoch: 22 [4350/36045]\tLoss: 639.3060\n",
      "Training Epoch: 22 [4400/36045]\tLoss: 612.4835\n",
      "Training Epoch: 22 [4450/36045]\tLoss: 668.1203\n",
      "Training Epoch: 22 [4500/36045]\tLoss: 713.3765\n",
      "Training Epoch: 22 [4550/36045]\tLoss: 720.0004\n",
      "Training Epoch: 22 [4600/36045]\tLoss: 743.7400\n",
      "Training Epoch: 22 [4650/36045]\tLoss: 732.2584\n",
      "Training Epoch: 22 [4700/36045]\tLoss: 677.3607\n",
      "Training Epoch: 22 [4750/36045]\tLoss: 660.0095\n",
      "Training Epoch: 22 [4800/36045]\tLoss: 687.7129\n",
      "Training Epoch: 22 [4850/36045]\tLoss: 674.0132\n",
      "Training Epoch: 22 [4900/36045]\tLoss: 655.1877\n",
      "Training Epoch: 22 [4950/36045]\tLoss: 674.0979\n",
      "Training Epoch: 22 [5000/36045]\tLoss: 706.2571\n",
      "Training Epoch: 22 [5050/36045]\tLoss: 684.0953\n",
      "Training Epoch: 22 [5100/36045]\tLoss: 694.4650\n",
      "Training Epoch: 22 [5150/36045]\tLoss: 678.9611\n",
      "Training Epoch: 22 [5200/36045]\tLoss: 677.0287\n",
      "Training Epoch: 22 [5250/36045]\tLoss: 669.5916\n",
      "Training Epoch: 22 [5300/36045]\tLoss: 670.7697\n",
      "Training Epoch: 22 [5350/36045]\tLoss: 694.7373\n",
      "Training Epoch: 22 [5400/36045]\tLoss: 668.0679\n",
      "Training Epoch: 22 [5450/36045]\tLoss: 633.5638\n",
      "Training Epoch: 22 [5500/36045]\tLoss: 664.7108\n",
      "Training Epoch: 22 [5550/36045]\tLoss: 652.3404\n",
      "Training Epoch: 22 [5600/36045]\tLoss: 738.7332\n",
      "Training Epoch: 22 [5650/36045]\tLoss: 700.6061\n",
      "Training Epoch: 22 [5700/36045]\tLoss: 658.4380\n",
      "Training Epoch: 22 [5750/36045]\tLoss: 643.5677\n",
      "Training Epoch: 22 [5800/36045]\tLoss: 679.6216\n",
      "Training Epoch: 22 [5850/36045]\tLoss: 663.4208\n",
      "Training Epoch: 22 [5900/36045]\tLoss: 764.3572\n",
      "Training Epoch: 22 [5950/36045]\tLoss: 782.9811\n",
      "Training Epoch: 22 [6000/36045]\tLoss: 766.8722\n",
      "Training Epoch: 22 [6050/36045]\tLoss: 741.0679\n",
      "Training Epoch: 22 [6100/36045]\tLoss: 746.4797\n",
      "Training Epoch: 22 [6150/36045]\tLoss: 728.4421\n",
      "Training Epoch: 22 [6200/36045]\tLoss: 729.2081\n",
      "Training Epoch: 22 [6250/36045]\tLoss: 750.0524\n",
      "Training Epoch: 22 [6300/36045]\tLoss: 762.4532\n",
      "Training Epoch: 22 [6350/36045]\tLoss: 812.0540\n",
      "Training Epoch: 22 [6400/36045]\tLoss: 678.4803\n",
      "Training Epoch: 22 [6450/36045]\tLoss: 628.0574\n",
      "Training Epoch: 22 [6500/36045]\tLoss: 640.2330\n",
      "Training Epoch: 22 [6550/36045]\tLoss: 656.7383\n",
      "Training Epoch: 22 [6600/36045]\tLoss: 658.0022\n",
      "Training Epoch: 22 [6650/36045]\tLoss: 743.9009\n",
      "Training Epoch: 22 [6700/36045]\tLoss: 778.6196\n",
      "Training Epoch: 22 [6750/36045]\tLoss: 751.3678\n",
      "Training Epoch: 22 [6800/36045]\tLoss: 754.2626\n",
      "Training Epoch: 22 [6850/36045]\tLoss: 742.1188\n",
      "Training Epoch: 22 [6900/36045]\tLoss: 660.0591\n",
      "Training Epoch: 22 [6950/36045]\tLoss: 622.7623\n",
      "Training Epoch: 22 [7000/36045]\tLoss: 662.3147\n",
      "Training Epoch: 22 [7050/36045]\tLoss: 676.6666\n",
      "Training Epoch: 22 [7100/36045]\tLoss: 676.1808\n",
      "Training Epoch: 22 [7150/36045]\tLoss: 687.8196\n",
      "Training Epoch: 22 [7200/36045]\tLoss: 691.9987\n",
      "Training Epoch: 22 [7250/36045]\tLoss: 689.3004\n",
      "Training Epoch: 22 [7300/36045]\tLoss: 676.3643\n",
      "Training Epoch: 22 [7350/36045]\tLoss: 671.3124\n",
      "Training Epoch: 22 [7400/36045]\tLoss: 605.2462\n",
      "Training Epoch: 22 [7450/36045]\tLoss: 610.0604\n",
      "Training Epoch: 22 [7500/36045]\tLoss: 603.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [7550/36045]\tLoss: 578.7266\n",
      "Training Epoch: 22 [7600/36045]\tLoss: 644.9868\n",
      "Training Epoch: 22 [7650/36045]\tLoss: 693.4213\n",
      "Training Epoch: 22 [7700/36045]\tLoss: 661.4088\n",
      "Training Epoch: 22 [7750/36045]\tLoss: 675.7496\n",
      "Training Epoch: 22 [7800/36045]\tLoss: 663.8407\n",
      "Training Epoch: 22 [7850/36045]\tLoss: 640.2933\n",
      "Training Epoch: 22 [7900/36045]\tLoss: 675.6663\n",
      "Training Epoch: 22 [7950/36045]\tLoss: 673.0427\n",
      "Training Epoch: 22 [8000/36045]\tLoss: 690.7712\n",
      "Training Epoch: 22 [8050/36045]\tLoss: 653.5754\n",
      "Training Epoch: 22 [8100/36045]\tLoss: 679.7156\n",
      "Training Epoch: 22 [8150/36045]\tLoss: 769.3317\n",
      "Training Epoch: 22 [8200/36045]\tLoss: 755.2101\n",
      "Training Epoch: 22 [8250/36045]\tLoss: 721.9221\n",
      "Training Epoch: 22 [8300/36045]\tLoss: 785.0702\n",
      "Training Epoch: 22 [8350/36045]\tLoss: 722.4016\n",
      "Training Epoch: 22 [8400/36045]\tLoss: 647.8369\n",
      "Training Epoch: 22 [8450/36045]\tLoss: 607.4230\n",
      "Training Epoch: 22 [8500/36045]\tLoss: 644.7809\n",
      "Training Epoch: 22 [8550/36045]\tLoss: 635.6600\n",
      "Training Epoch: 22 [8600/36045]\tLoss: 628.6853\n",
      "Training Epoch: 22 [8650/36045]\tLoss: 672.1151\n",
      "Training Epoch: 22 [8700/36045]\tLoss: 710.2556\n",
      "Training Epoch: 22 [8750/36045]\tLoss: 697.2017\n",
      "Training Epoch: 22 [8800/36045]\tLoss: 702.4960\n",
      "Training Epoch: 22 [8850/36045]\tLoss: 695.3892\n",
      "Training Epoch: 22 [8900/36045]\tLoss: 627.8094\n",
      "Training Epoch: 22 [8950/36045]\tLoss: 641.8188\n",
      "Training Epoch: 22 [9000/36045]\tLoss: 657.6024\n",
      "Training Epoch: 22 [9050/36045]\tLoss: 657.5630\n",
      "Training Epoch: 22 [9100/36045]\tLoss: 677.3002\n",
      "Training Epoch: 22 [9150/36045]\tLoss: 499.8127\n",
      "Training Epoch: 22 [9200/36045]\tLoss: 375.7670\n",
      "Training Epoch: 22 [9250/36045]\tLoss: 407.4733\n",
      "Training Epoch: 22 [9300/36045]\tLoss: 419.6337\n",
      "Training Epoch: 22 [9350/36045]\tLoss: 386.4119\n",
      "Training Epoch: 22 [9400/36045]\tLoss: 756.9437\n",
      "Training Epoch: 22 [9450/36045]\tLoss: 804.1744\n",
      "Training Epoch: 22 [9500/36045]\tLoss: 790.6064\n",
      "Training Epoch: 22 [9550/36045]\tLoss: 835.8400\n",
      "Training Epoch: 22 [9600/36045]\tLoss: 621.4967\n",
      "Training Epoch: 22 [9650/36045]\tLoss: 624.5676\n",
      "Training Epoch: 22 [9700/36045]\tLoss: 609.6174\n",
      "Training Epoch: 22 [9750/36045]\tLoss: 609.9531\n",
      "Training Epoch: 22 [9800/36045]\tLoss: 794.3921\n",
      "Training Epoch: 22 [9850/36045]\tLoss: 839.1130\n",
      "Training Epoch: 22 [9900/36045]\tLoss: 855.0129\n",
      "Training Epoch: 22 [9950/36045]\tLoss: 832.4811\n",
      "Training Epoch: 22 [10000/36045]\tLoss: 768.5389\n",
      "Training Epoch: 22 [10050/36045]\tLoss: 637.3376\n",
      "Training Epoch: 22 [10100/36045]\tLoss: 643.3613\n",
      "Training Epoch: 22 [10150/36045]\tLoss: 653.8788\n",
      "Training Epoch: 22 [10200/36045]\tLoss: 642.8909\n",
      "Training Epoch: 22 [10250/36045]\tLoss: 765.5567\n",
      "Training Epoch: 22 [10300/36045]\tLoss: 743.1241\n",
      "Training Epoch: 22 [10350/36045]\tLoss: 781.9240\n",
      "Training Epoch: 22 [10400/36045]\tLoss: 772.6643\n",
      "Training Epoch: 22 [10450/36045]\tLoss: 723.2382\n",
      "Training Epoch: 22 [10500/36045]\tLoss: 606.9064\n",
      "Training Epoch: 22 [10550/36045]\tLoss: 602.3438\n",
      "Training Epoch: 22 [10600/36045]\tLoss: 626.0516\n",
      "Training Epoch: 22 [10650/36045]\tLoss: 632.1210\n",
      "Training Epoch: 22 [10700/36045]\tLoss: 720.8900\n",
      "Training Epoch: 22 [10750/36045]\tLoss: 784.7952\n",
      "Training Epoch: 22 [10800/36045]\tLoss: 725.8827\n",
      "Training Epoch: 22 [10850/36045]\tLoss: 768.6240\n",
      "Training Epoch: 22 [10900/36045]\tLoss: 799.8760\n",
      "Training Epoch: 22 [10950/36045]\tLoss: 593.0508\n",
      "Training Epoch: 22 [11000/36045]\tLoss: 586.7491\n",
      "Training Epoch: 22 [11050/36045]\tLoss: 627.3388\n",
      "Training Epoch: 22 [11100/36045]\tLoss: 639.7906\n",
      "Training Epoch: 22 [11150/36045]\tLoss: 693.3145\n",
      "Training Epoch: 22 [11200/36045]\tLoss: 721.8892\n",
      "Training Epoch: 22 [11250/36045]\tLoss: 734.3079\n",
      "Training Epoch: 22 [11300/36045]\tLoss: 714.2222\n",
      "Training Epoch: 22 [11350/36045]\tLoss: 710.9051\n",
      "Training Epoch: 22 [11400/36045]\tLoss: 670.4079\n",
      "Training Epoch: 22 [11450/36045]\tLoss: 636.0190\n",
      "Training Epoch: 22 [11500/36045]\tLoss: 633.7037\n",
      "Training Epoch: 22 [11550/36045]\tLoss: 647.4202\n",
      "Training Epoch: 22 [11600/36045]\tLoss: 711.5401\n",
      "Training Epoch: 22 [11650/36045]\tLoss: 764.7850\n",
      "Training Epoch: 22 [11700/36045]\tLoss: 763.8419\n",
      "Training Epoch: 22 [11750/36045]\tLoss: 783.6179\n",
      "Training Epoch: 22 [11800/36045]\tLoss: 826.9885\n",
      "Training Epoch: 22 [11850/36045]\tLoss: 882.4365\n",
      "Training Epoch: 22 [11900/36045]\tLoss: 1102.7355\n",
      "Training Epoch: 22 [11950/36045]\tLoss: 1103.8309\n",
      "Training Epoch: 22 [12000/36045]\tLoss: 1119.1295\n",
      "Training Epoch: 22 [12050/36045]\tLoss: 1076.5170\n",
      "Training Epoch: 22 [12100/36045]\tLoss: 709.7484\n",
      "Training Epoch: 22 [12150/36045]\tLoss: 548.5942\n",
      "Training Epoch: 22 [12200/36045]\tLoss: 542.8927\n",
      "Training Epoch: 22 [12250/36045]\tLoss: 552.4784\n",
      "Training Epoch: 22 [12300/36045]\tLoss: 700.5053\n",
      "Training Epoch: 22 [12350/36045]\tLoss: 759.7960\n",
      "Training Epoch: 22 [12400/36045]\tLoss: 768.2116\n",
      "Training Epoch: 22 [12450/36045]\tLoss: 755.4264\n",
      "Training Epoch: 22 [12500/36045]\tLoss: 785.5162\n",
      "Training Epoch: 22 [12550/36045]\tLoss: 753.2767\n",
      "Training Epoch: 22 [12600/36045]\tLoss: 695.6551\n",
      "Training Epoch: 22 [12650/36045]\tLoss: 694.4041\n",
      "Training Epoch: 22 [12700/36045]\tLoss: 716.3511\n",
      "Training Epoch: 22 [12750/36045]\tLoss: 716.1520\n",
      "Training Epoch: 22 [12800/36045]\tLoss: 697.5687\n",
      "Training Epoch: 22 [12850/36045]\tLoss: 728.4871\n",
      "Training Epoch: 22 [12900/36045]\tLoss: 698.8240\n",
      "Training Epoch: 22 [12950/36045]\tLoss: 686.8859\n",
      "Training Epoch: 22 [13000/36045]\tLoss: 719.1909\n",
      "Training Epoch: 22 [13050/36045]\tLoss: 654.6877\n",
      "Training Epoch: 22 [13100/36045]\tLoss: 676.4360\n",
      "Training Epoch: 22 [13150/36045]\tLoss: 668.1410\n",
      "Training Epoch: 22 [13200/36045]\tLoss: 645.4410\n",
      "Training Epoch: 22 [13250/36045]\tLoss: 673.1067\n",
      "Training Epoch: 22 [13300/36045]\tLoss: 713.8940\n",
      "Training Epoch: 22 [13350/36045]\tLoss: 692.7624\n",
      "Training Epoch: 22 [13400/36045]\tLoss: 697.0997\n",
      "Training Epoch: 22 [13450/36045]\tLoss: 691.9053\n",
      "Training Epoch: 22 [13500/36045]\tLoss: 714.9932\n",
      "Training Epoch: 22 [13550/36045]\tLoss: 850.7681\n",
      "Training Epoch: 22 [13600/36045]\tLoss: 883.4413\n",
      "Training Epoch: 22 [13650/36045]\tLoss: 963.4017\n",
      "Training Epoch: 22 [13700/36045]\tLoss: 854.2342\n",
      "Training Epoch: 22 [13750/36045]\tLoss: 700.1826\n",
      "Training Epoch: 22 [13800/36045]\tLoss: 673.4541\n",
      "Training Epoch: 22 [13850/36045]\tLoss: 656.5228\n",
      "Training Epoch: 22 [13900/36045]\tLoss: 664.0318\n",
      "Training Epoch: 22 [13950/36045]\tLoss: 709.8678\n",
      "Training Epoch: 22 [14000/36045]\tLoss: 745.0327\n",
      "Training Epoch: 22 [14050/36045]\tLoss: 716.9909\n",
      "Training Epoch: 22 [14100/36045]\tLoss: 712.9261\n",
      "Training Epoch: 22 [14150/36045]\tLoss: 700.6935\n",
      "Training Epoch: 22 [14200/36045]\tLoss: 745.0775\n",
      "Training Epoch: 22 [14250/36045]\tLoss: 815.7091\n",
      "Training Epoch: 22 [14300/36045]\tLoss: 819.1871\n",
      "Training Epoch: 22 [14350/36045]\tLoss: 784.5191\n",
      "Training Epoch: 22 [14400/36045]\tLoss: 769.9485\n",
      "Training Epoch: 22 [14450/36045]\tLoss: 808.4230\n",
      "Training Epoch: 22 [14500/36045]\tLoss: 739.1719\n",
      "Training Epoch: 22 [14550/36045]\tLoss: 770.6384\n",
      "Training Epoch: 22 [14600/36045]\tLoss: 754.4930\n",
      "Training Epoch: 22 [14650/36045]\tLoss: 755.6873\n",
      "Training Epoch: 22 [14700/36045]\tLoss: 712.9457\n",
      "Training Epoch: 22 [14750/36045]\tLoss: 611.7197\n",
      "Training Epoch: 22 [14800/36045]\tLoss: 601.8449\n",
      "Training Epoch: 22 [14850/36045]\tLoss: 609.1251\n",
      "Training Epoch: 22 [14900/36045]\tLoss: 602.5647\n",
      "Training Epoch: 22 [14950/36045]\tLoss: 610.4434\n",
      "Training Epoch: 22 [15000/36045]\tLoss: 626.8146\n",
      "Training Epoch: 22 [15050/36045]\tLoss: 625.0089\n",
      "Training Epoch: 22 [15100/36045]\tLoss: 608.7756\n",
      "Training Epoch: 22 [15150/36045]\tLoss: 601.2700\n",
      "Training Epoch: 22 [15200/36045]\tLoss: 555.7364\n",
      "Training Epoch: 22 [15250/36045]\tLoss: 580.3204\n",
      "Training Epoch: 22 [15300/36045]\tLoss: 564.6057\n",
      "Training Epoch: 22 [15350/36045]\tLoss: 577.8221\n",
      "Training Epoch: 22 [15400/36045]\tLoss: 562.9285\n",
      "Training Epoch: 22 [15450/36045]\tLoss: 549.3657\n",
      "Training Epoch: 22 [15500/36045]\tLoss: 565.4741\n",
      "Training Epoch: 22 [15550/36045]\tLoss: 560.5109\n",
      "Training Epoch: 22 [15600/36045]\tLoss: 632.5765\n",
      "Training Epoch: 22 [15650/36045]\tLoss: 652.1353\n",
      "Training Epoch: 22 [15700/36045]\tLoss: 641.8060\n",
      "Training Epoch: 22 [15750/36045]\tLoss: 633.7002\n",
      "Training Epoch: 22 [15800/36045]\tLoss: 594.2684\n",
      "Training Epoch: 22 [15850/36045]\tLoss: 607.5103\n",
      "Training Epoch: 22 [15900/36045]\tLoss: 617.4376\n",
      "Training Epoch: 22 [15950/36045]\tLoss: 637.6422\n",
      "Training Epoch: 22 [16000/36045]\tLoss: 613.0247\n",
      "Training Epoch: 22 [16050/36045]\tLoss: 582.7392\n",
      "Training Epoch: 22 [16100/36045]\tLoss: 538.4977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [16150/36045]\tLoss: 525.5187\n",
      "Training Epoch: 22 [16200/36045]\tLoss: 635.2300\n",
      "Training Epoch: 22 [16250/36045]\tLoss: 665.3486\n",
      "Training Epoch: 22 [16300/36045]\tLoss: 726.7413\n",
      "Training Epoch: 22 [16350/36045]\tLoss: 744.2145\n",
      "Training Epoch: 22 [16400/36045]\tLoss: 716.8107\n",
      "Training Epoch: 22 [16450/36045]\tLoss: 697.5025\n",
      "Training Epoch: 22 [16500/36045]\tLoss: 696.5691\n",
      "Training Epoch: 22 [16550/36045]\tLoss: 660.0168\n",
      "Training Epoch: 22 [16600/36045]\tLoss: 686.7841\n",
      "Training Epoch: 22 [16650/36045]\tLoss: 707.2518\n",
      "Training Epoch: 22 [16700/36045]\tLoss: 682.6943\n",
      "Training Epoch: 22 [16750/36045]\tLoss: 674.0746\n",
      "Training Epoch: 22 [16800/36045]\tLoss: 686.2820\n",
      "Training Epoch: 22 [16850/36045]\tLoss: 653.1638\n",
      "Training Epoch: 22 [16900/36045]\tLoss: 664.2102\n",
      "Training Epoch: 22 [16950/36045]\tLoss: 691.1848\n",
      "Training Epoch: 22 [17000/36045]\tLoss: 672.5515\n",
      "Training Epoch: 22 [17050/36045]\tLoss: 702.2483\n",
      "Training Epoch: 22 [17100/36045]\tLoss: 699.8646\n",
      "Training Epoch: 22 [17150/36045]\tLoss: 608.5391\n",
      "Training Epoch: 22 [17200/36045]\tLoss: 567.2617\n",
      "Training Epoch: 22 [17250/36045]\tLoss: 593.5443\n",
      "Training Epoch: 22 [17300/36045]\tLoss: 627.0891\n",
      "Training Epoch: 22 [17350/36045]\tLoss: 601.8098\n",
      "Training Epoch: 22 [17400/36045]\tLoss: 621.6046\n",
      "Training Epoch: 22 [17450/36045]\tLoss: 642.7684\n",
      "Training Epoch: 22 [17500/36045]\tLoss: 630.1458\n",
      "Training Epoch: 22 [17550/36045]\tLoss: 630.6664\n",
      "Training Epoch: 22 [17600/36045]\tLoss: 621.6203\n",
      "Training Epoch: 22 [17650/36045]\tLoss: 639.5253\n",
      "Training Epoch: 22 [17700/36045]\tLoss: 617.9915\n",
      "Training Epoch: 22 [17750/36045]\tLoss: 635.2629\n",
      "Training Epoch: 22 [17800/36045]\tLoss: 625.7427\n",
      "Training Epoch: 22 [17850/36045]\tLoss: 630.5385\n",
      "Training Epoch: 22 [17900/36045]\tLoss: 659.9879\n",
      "Training Epoch: 22 [17950/36045]\tLoss: 671.1989\n",
      "Training Epoch: 22 [18000/36045]\tLoss: 661.2252\n",
      "Training Epoch: 22 [18050/36045]\tLoss: 737.5363\n",
      "Training Epoch: 22 [18100/36045]\tLoss: 740.3726\n",
      "Training Epoch: 22 [18150/36045]\tLoss: 750.4651\n",
      "Training Epoch: 22 [18200/36045]\tLoss: 733.2147\n",
      "Training Epoch: 22 [18250/36045]\tLoss: 754.3291\n",
      "Training Epoch: 22 [18300/36045]\tLoss: 698.2151\n",
      "Training Epoch: 22 [18350/36045]\tLoss: 767.5099\n",
      "Training Epoch: 22 [18400/36045]\tLoss: 740.1093\n",
      "Training Epoch: 22 [18450/36045]\tLoss: 720.5989\n",
      "Training Epoch: 22 [18500/36045]\tLoss: 719.9761\n",
      "Training Epoch: 22 [18550/36045]\tLoss: 706.5401\n",
      "Training Epoch: 22 [18600/36045]\tLoss: 696.2230\n",
      "Training Epoch: 22 [18650/36045]\tLoss: 744.7208\n",
      "Training Epoch: 22 [18700/36045]\tLoss: 783.6229\n",
      "Training Epoch: 22 [18750/36045]\tLoss: 768.4409\n",
      "Training Epoch: 22 [18800/36045]\tLoss: 793.2742\n",
      "Training Epoch: 22 [18850/36045]\tLoss: 736.9547\n",
      "Training Epoch: 22 [18900/36045]\tLoss: 788.5859\n",
      "Training Epoch: 22 [18950/36045]\tLoss: 728.1268\n",
      "Training Epoch: 22 [19000/36045]\tLoss: 616.6140\n",
      "Training Epoch: 22 [19050/36045]\tLoss: 596.9924\n",
      "Training Epoch: 22 [19100/36045]\tLoss: 607.1851\n",
      "Training Epoch: 22 [19150/36045]\tLoss: 595.8513\n",
      "Training Epoch: 22 [19200/36045]\tLoss: 622.9705\n",
      "Training Epoch: 22 [19250/36045]\tLoss: 637.4275\n",
      "Training Epoch: 22 [19300/36045]\tLoss: 648.7956\n",
      "Training Epoch: 22 [19350/36045]\tLoss: 632.4079\n",
      "Training Epoch: 22 [19400/36045]\tLoss: 655.6700\n",
      "Training Epoch: 22 [19450/36045]\tLoss: 645.6162\n",
      "Training Epoch: 22 [19500/36045]\tLoss: 647.9783\n",
      "Training Epoch: 22 [19550/36045]\tLoss: 646.2727\n",
      "Training Epoch: 22 [19600/36045]\tLoss: 688.1907\n",
      "Training Epoch: 22 [19650/36045]\tLoss: 903.3526\n",
      "Training Epoch: 22 [19700/36045]\tLoss: 861.8774\n",
      "Training Epoch: 22 [19750/36045]\tLoss: 863.6583\n",
      "Training Epoch: 22 [19800/36045]\tLoss: 860.9363\n",
      "Training Epoch: 22 [19850/36045]\tLoss: 579.0146\n",
      "Training Epoch: 22 [19900/36045]\tLoss: 556.3196\n",
      "Training Epoch: 22 [19950/36045]\tLoss: 559.9781\n",
      "Training Epoch: 22 [20000/36045]\tLoss: 558.2040\n",
      "Training Epoch: 22 [20050/36045]\tLoss: 624.3113\n",
      "Training Epoch: 22 [20100/36045]\tLoss: 630.2231\n",
      "Training Epoch: 22 [20150/36045]\tLoss: 633.3043\n",
      "Training Epoch: 22 [20200/36045]\tLoss: 634.0382\n",
      "Training Epoch: 22 [20250/36045]\tLoss: 675.3540\n",
      "Training Epoch: 22 [20300/36045]\tLoss: 712.4311\n",
      "Training Epoch: 22 [20350/36045]\tLoss: 732.1255\n",
      "Training Epoch: 22 [20400/36045]\tLoss: 747.9070\n",
      "Training Epoch: 22 [20450/36045]\tLoss: 719.1052\n",
      "Training Epoch: 22 [20500/36045]\tLoss: 702.3680\n",
      "Training Epoch: 22 [20550/36045]\tLoss: 618.9423\n",
      "Training Epoch: 22 [20600/36045]\tLoss: 631.3312\n",
      "Training Epoch: 22 [20650/36045]\tLoss: 627.4344\n",
      "Training Epoch: 22 [20700/36045]\tLoss: 614.6255\n",
      "Training Epoch: 22 [20750/36045]\tLoss: 659.7618\n",
      "Training Epoch: 22 [20800/36045]\tLoss: 718.0778\n",
      "Training Epoch: 22 [20850/36045]\tLoss: 705.2787\n",
      "Training Epoch: 22 [20900/36045]\tLoss: 752.8298\n",
      "Training Epoch: 22 [20950/36045]\tLoss: 710.3589\n",
      "Training Epoch: 22 [21000/36045]\tLoss: 670.1064\n",
      "Training Epoch: 22 [21050/36045]\tLoss: 574.7678\n",
      "Training Epoch: 22 [21100/36045]\tLoss: 577.1732\n",
      "Training Epoch: 22 [21150/36045]\tLoss: 617.4897\n",
      "Training Epoch: 22 [21200/36045]\tLoss: 616.7508\n",
      "Training Epoch: 22 [21250/36045]\tLoss: 590.0736\n",
      "Training Epoch: 22 [21300/36045]\tLoss: 688.6459\n",
      "Training Epoch: 22 [21350/36045]\tLoss: 682.0722\n",
      "Training Epoch: 22 [21400/36045]\tLoss: 685.4719\n",
      "Training Epoch: 22 [21450/36045]\tLoss: 692.1763\n",
      "Training Epoch: 22 [21500/36045]\tLoss: 695.6853\n",
      "Training Epoch: 22 [21550/36045]\tLoss: 792.4099\n",
      "Training Epoch: 22 [21600/36045]\tLoss: 792.9132\n",
      "Training Epoch: 22 [21650/36045]\tLoss: 806.3704\n",
      "Training Epoch: 22 [21700/36045]\tLoss: 806.0624\n",
      "Training Epoch: 22 [21750/36045]\tLoss: 775.9495\n",
      "Training Epoch: 22 [21800/36045]\tLoss: 575.3747\n",
      "Training Epoch: 22 [21850/36045]\tLoss: 557.9494\n",
      "Training Epoch: 22 [21900/36045]\tLoss: 569.6918\n",
      "Training Epoch: 22 [21950/36045]\tLoss: 567.6666\n",
      "Training Epoch: 22 [22000/36045]\tLoss: 572.4883\n",
      "Training Epoch: 22 [22050/36045]\tLoss: 599.3574\n",
      "Training Epoch: 22 [22100/36045]\tLoss: 591.0735\n",
      "Training Epoch: 22 [22150/36045]\tLoss: 574.2214\n",
      "Training Epoch: 22 [22200/36045]\tLoss: 592.8735\n",
      "Training Epoch: 22 [22250/36045]\tLoss: 598.4017\n",
      "Training Epoch: 22 [22300/36045]\tLoss: 652.8008\n",
      "Training Epoch: 22 [22350/36045]\tLoss: 679.5017\n",
      "Training Epoch: 22 [22400/36045]\tLoss: 695.1456\n",
      "Training Epoch: 22 [22450/36045]\tLoss: 682.7537\n",
      "Training Epoch: 22 [22500/36045]\tLoss: 663.4899\n",
      "Training Epoch: 22 [22550/36045]\tLoss: 702.4451\n",
      "Training Epoch: 22 [22600/36045]\tLoss: 764.4086\n",
      "Training Epoch: 22 [22650/36045]\tLoss: 802.0263\n",
      "Training Epoch: 22 [22700/36045]\tLoss: 825.6309\n",
      "Training Epoch: 22 [22750/36045]\tLoss: 846.8105\n",
      "Training Epoch: 22 [22800/36045]\tLoss: 880.8287\n",
      "Training Epoch: 22 [22850/36045]\tLoss: 732.3328\n",
      "Training Epoch: 22 [22900/36045]\tLoss: 736.5078\n",
      "Training Epoch: 22 [22950/36045]\tLoss: 714.2611\n",
      "Training Epoch: 22 [23000/36045]\tLoss: 713.5494\n",
      "Training Epoch: 22 [23050/36045]\tLoss: 636.4687\n",
      "Training Epoch: 22 [23100/36045]\tLoss: 652.8383\n",
      "Training Epoch: 22 [23150/36045]\tLoss: 640.8669\n",
      "Training Epoch: 22 [23200/36045]\tLoss: 606.8583\n",
      "Training Epoch: 22 [23250/36045]\tLoss: 609.8907\n",
      "Training Epoch: 22 [23300/36045]\tLoss: 606.7052\n",
      "Training Epoch: 22 [23350/36045]\tLoss: 628.7516\n",
      "Training Epoch: 22 [23400/36045]\tLoss: 681.2528\n",
      "Training Epoch: 22 [23450/36045]\tLoss: 673.1608\n",
      "Training Epoch: 22 [23500/36045]\tLoss: 648.8099\n",
      "Training Epoch: 22 [23550/36045]\tLoss: 696.7651\n",
      "Training Epoch: 22 [23600/36045]\tLoss: 784.8487\n",
      "Training Epoch: 22 [23650/36045]\tLoss: 798.9128\n",
      "Training Epoch: 22 [23700/36045]\tLoss: 808.9491\n",
      "Training Epoch: 22 [23750/36045]\tLoss: 782.0154\n",
      "Training Epoch: 22 [23800/36045]\tLoss: 621.6176\n",
      "Training Epoch: 22 [23850/36045]\tLoss: 649.0688\n",
      "Training Epoch: 22 [23900/36045]\tLoss: 639.1316\n",
      "Training Epoch: 22 [23950/36045]\tLoss: 621.2358\n",
      "Training Epoch: 22 [24000/36045]\tLoss: 597.0318\n",
      "Training Epoch: 22 [24050/36045]\tLoss: 552.0195\n",
      "Training Epoch: 22 [24100/36045]\tLoss: 581.2123\n",
      "Training Epoch: 22 [24150/36045]\tLoss: 575.9878\n",
      "Training Epoch: 22 [24200/36045]\tLoss: 570.7709\n",
      "Training Epoch: 22 [24250/36045]\tLoss: 554.1215\n",
      "Training Epoch: 22 [24300/36045]\tLoss: 596.9937\n",
      "Training Epoch: 22 [24350/36045]\tLoss: 611.7279\n",
      "Training Epoch: 22 [24400/36045]\tLoss: 629.2234\n",
      "Training Epoch: 22 [24450/36045]\tLoss: 600.5165\n",
      "Training Epoch: 22 [24500/36045]\tLoss: 632.3544\n",
      "Training Epoch: 22 [24550/36045]\tLoss: 725.5029\n",
      "Training Epoch: 22 [24600/36045]\tLoss: 717.7997\n",
      "Training Epoch: 22 [24650/36045]\tLoss: 689.3115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [24700/36045]\tLoss: 700.4086\n",
      "Training Epoch: 22 [24750/36045]\tLoss: 647.1957\n",
      "Training Epoch: 22 [24800/36045]\tLoss: 539.9197\n",
      "Training Epoch: 22 [24850/36045]\tLoss: 560.2496\n",
      "Training Epoch: 22 [24900/36045]\tLoss: 556.5349\n",
      "Training Epoch: 22 [24950/36045]\tLoss: 558.6572\n",
      "Training Epoch: 22 [25000/36045]\tLoss: 536.7662\n",
      "Training Epoch: 22 [25050/36045]\tLoss: 512.7513\n",
      "Training Epoch: 22 [25100/36045]\tLoss: 460.3253\n",
      "Training Epoch: 22 [25150/36045]\tLoss: 426.8169\n",
      "Training Epoch: 22 [25200/36045]\tLoss: 421.6557\n",
      "Training Epoch: 22 [25250/36045]\tLoss: 451.3001\n",
      "Training Epoch: 22 [25300/36045]\tLoss: 591.8766\n",
      "Training Epoch: 22 [25350/36045]\tLoss: 589.8558\n",
      "Training Epoch: 22 [25400/36045]\tLoss: 549.3057\n",
      "Training Epoch: 22 [25450/36045]\tLoss: 552.3683\n",
      "Training Epoch: 22 [25500/36045]\tLoss: 600.4108\n",
      "Training Epoch: 22 [25550/36045]\tLoss: 696.2935\n",
      "Training Epoch: 22 [25600/36045]\tLoss: 702.0439\n",
      "Training Epoch: 22 [25650/36045]\tLoss: 677.3151\n",
      "Training Epoch: 22 [25700/36045]\tLoss: 686.9736\n",
      "Training Epoch: 22 [25750/36045]\tLoss: 661.5933\n",
      "Training Epoch: 22 [25800/36045]\tLoss: 416.3537\n",
      "Training Epoch: 22 [25850/36045]\tLoss: 426.8033\n",
      "Training Epoch: 22 [25900/36045]\tLoss: 407.6558\n",
      "Training Epoch: 22 [25950/36045]\tLoss: 416.8904\n",
      "Training Epoch: 22 [26000/36045]\tLoss: 510.5611\n",
      "Training Epoch: 22 [26050/36045]\tLoss: 694.0729\n",
      "Training Epoch: 22 [26100/36045]\tLoss: 722.4937\n",
      "Training Epoch: 22 [26150/36045]\tLoss: 722.1958\n",
      "Training Epoch: 22 [26200/36045]\tLoss: 695.5978\n",
      "Training Epoch: 22 [26250/36045]\tLoss: 727.5437\n",
      "Training Epoch: 22 [26300/36045]\tLoss: 650.0917\n",
      "Training Epoch: 22 [26350/36045]\tLoss: 659.8540\n",
      "Training Epoch: 22 [26400/36045]\tLoss: 637.9490\n",
      "Training Epoch: 22 [26450/36045]\tLoss: 567.1989\n",
      "Training Epoch: 22 [26500/36045]\tLoss: 677.7379\n",
      "Training Epoch: 22 [26550/36045]\tLoss: 681.6224\n",
      "Training Epoch: 22 [26600/36045]\tLoss: 676.6211\n",
      "Training Epoch: 22 [26650/36045]\tLoss: 693.0674\n",
      "Training Epoch: 22 [26700/36045]\tLoss: 672.9011\n",
      "Training Epoch: 22 [26750/36045]\tLoss: 629.4153\n",
      "Training Epoch: 22 [26800/36045]\tLoss: 463.0786\n",
      "Training Epoch: 22 [26850/36045]\tLoss: 385.0439\n",
      "Training Epoch: 22 [26900/36045]\tLoss: 388.4764\n",
      "Training Epoch: 22 [26950/36045]\tLoss: 427.2135\n",
      "Training Epoch: 22 [27000/36045]\tLoss: 688.7476\n",
      "Training Epoch: 22 [27050/36045]\tLoss: 721.9062\n",
      "Training Epoch: 22 [27100/36045]\tLoss: 698.9490\n",
      "Training Epoch: 22 [27150/36045]\tLoss: 741.1694\n",
      "Training Epoch: 22 [27200/36045]\tLoss: 545.8763\n",
      "Training Epoch: 22 [27250/36045]\tLoss: 540.0505\n",
      "Training Epoch: 22 [27300/36045]\tLoss: 525.2413\n",
      "Training Epoch: 22 [27350/36045]\tLoss: 524.7308\n",
      "Training Epoch: 22 [27400/36045]\tLoss: 523.9407\n",
      "Training Epoch: 22 [27450/36045]\tLoss: 659.7147\n",
      "Training Epoch: 22 [27500/36045]\tLoss: 708.1823\n",
      "Training Epoch: 22 [27550/36045]\tLoss: 700.4160\n",
      "Training Epoch: 22 [27600/36045]\tLoss: 711.7593\n",
      "Training Epoch: 22 [27650/36045]\tLoss: 703.8053\n",
      "Training Epoch: 22 [27700/36045]\tLoss: 734.0345\n",
      "Training Epoch: 22 [27750/36045]\tLoss: 746.5034\n",
      "Training Epoch: 22 [27800/36045]\tLoss: 732.5834\n",
      "Training Epoch: 22 [27850/36045]\tLoss: 720.5080\n",
      "Training Epoch: 22 [27900/36045]\tLoss: 647.9137\n",
      "Training Epoch: 22 [27950/36045]\tLoss: 536.9836\n",
      "Training Epoch: 22 [28000/36045]\tLoss: 511.8806\n",
      "Training Epoch: 22 [28050/36045]\tLoss: 523.7922\n",
      "Training Epoch: 22 [28100/36045]\tLoss: 515.3555\n",
      "Training Epoch: 22 [28150/36045]\tLoss: 544.1846\n",
      "Training Epoch: 22 [28200/36045]\tLoss: 549.4813\n",
      "Training Epoch: 22 [28250/36045]\tLoss: 544.0749\n",
      "Training Epoch: 22 [28300/36045]\tLoss: 515.2537\n",
      "Training Epoch: 22 [28350/36045]\tLoss: 511.2432\n",
      "Training Epoch: 22 [28400/36045]\tLoss: 847.3354\n",
      "Training Epoch: 22 [28450/36045]\tLoss: 772.5159\n",
      "Training Epoch: 22 [28500/36045]\tLoss: 669.2786\n",
      "Training Epoch: 22 [28550/36045]\tLoss: 614.1479\n",
      "Training Epoch: 22 [28600/36045]\tLoss: 654.1985\n",
      "Training Epoch: 22 [28650/36045]\tLoss: 736.2393\n",
      "Training Epoch: 22 [28700/36045]\tLoss: 730.2827\n",
      "Training Epoch: 22 [28750/36045]\tLoss: 718.0992\n",
      "Training Epoch: 22 [28800/36045]\tLoss: 727.0941\n",
      "Training Epoch: 22 [28850/36045]\tLoss: 629.2726\n",
      "Training Epoch: 22 [28900/36045]\tLoss: 506.2566\n",
      "Training Epoch: 22 [28950/36045]\tLoss: 503.3608\n",
      "Training Epoch: 22 [29000/36045]\tLoss: 502.4117\n",
      "Training Epoch: 22 [29050/36045]\tLoss: 510.8656\n",
      "Training Epoch: 22 [29100/36045]\tLoss: 532.1430\n",
      "Training Epoch: 22 [29150/36045]\tLoss: 519.0602\n",
      "Training Epoch: 22 [29200/36045]\tLoss: 504.1238\n",
      "Training Epoch: 22 [29250/36045]\tLoss: 491.2616\n",
      "Training Epoch: 22 [29300/36045]\tLoss: 562.0511\n",
      "Training Epoch: 22 [29350/36045]\tLoss: 667.5718\n",
      "Training Epoch: 22 [29400/36045]\tLoss: 686.5590\n",
      "Training Epoch: 22 [29450/36045]\tLoss: 709.2258\n",
      "Training Epoch: 22 [29500/36045]\tLoss: 723.3632\n",
      "Training Epoch: 22 [29550/36045]\tLoss: 687.0735\n",
      "Training Epoch: 22 [29600/36045]\tLoss: 581.2029\n",
      "Training Epoch: 22 [29650/36045]\tLoss: 564.4160\n",
      "Training Epoch: 22 [29700/36045]\tLoss: 503.0224\n",
      "Training Epoch: 22 [29750/36045]\tLoss: 504.6806\n",
      "Training Epoch: 22 [29800/36045]\tLoss: 550.8477\n",
      "Training Epoch: 22 [29850/36045]\tLoss: 623.4969\n",
      "Training Epoch: 22 [29900/36045]\tLoss: 619.7440\n",
      "Training Epoch: 22 [29950/36045]\tLoss: 642.3826\n",
      "Training Epoch: 22 [30000/36045]\tLoss: 619.3115\n",
      "Training Epoch: 22 [30050/36045]\tLoss: 625.7493\n",
      "Training Epoch: 22 [30100/36045]\tLoss: 763.2930\n",
      "Training Epoch: 22 [30150/36045]\tLoss: 747.9354\n",
      "Training Epoch: 22 [30200/36045]\tLoss: 705.2928\n",
      "Training Epoch: 22 [30250/36045]\tLoss: 754.7778\n",
      "Training Epoch: 22 [30300/36045]\tLoss: 741.0544\n",
      "Training Epoch: 22 [30350/36045]\tLoss: 582.4800\n",
      "Training Epoch: 22 [30400/36045]\tLoss: 568.4736\n",
      "Training Epoch: 22 [30450/36045]\tLoss: 567.6298\n",
      "Training Epoch: 22 [30500/36045]\tLoss: 530.9197\n",
      "Training Epoch: 22 [30550/36045]\tLoss: 492.1077\n",
      "Training Epoch: 22 [30600/36045]\tLoss: 477.7768\n",
      "Training Epoch: 22 [30650/36045]\tLoss: 468.8394\n",
      "Training Epoch: 22 [30700/36045]\tLoss: 486.4507\n",
      "Training Epoch: 22 [30750/36045]\tLoss: 472.8804\n",
      "Training Epoch: 22 [30800/36045]\tLoss: 502.3769\n",
      "Training Epoch: 22 [30850/36045]\tLoss: 493.5644\n",
      "Training Epoch: 22 [30900/36045]\tLoss: 507.6520\n",
      "Training Epoch: 22 [30950/36045]\tLoss: 533.1771\n",
      "Training Epoch: 22 [31000/36045]\tLoss: 524.2391\n",
      "Training Epoch: 22 [31050/36045]\tLoss: 437.3776\n",
      "Training Epoch: 22 [31100/36045]\tLoss: 428.4225\n",
      "Training Epoch: 22 [31150/36045]\tLoss: 434.4680\n",
      "Training Epoch: 22 [31200/36045]\tLoss: 543.4663\n",
      "Training Epoch: 22 [31250/36045]\tLoss: 704.9636\n",
      "Training Epoch: 22 [31300/36045]\tLoss: 674.0536\n",
      "Training Epoch: 22 [31350/36045]\tLoss: 690.3017\n",
      "Training Epoch: 22 [31400/36045]\tLoss: 669.3265\n",
      "Training Epoch: 22 [31450/36045]\tLoss: 683.0840\n",
      "Training Epoch: 22 [31500/36045]\tLoss: 694.0741\n",
      "Training Epoch: 22 [31550/36045]\tLoss: 703.5042\n",
      "Training Epoch: 22 [31600/36045]\tLoss: 661.1658\n",
      "Training Epoch: 22 [31650/36045]\tLoss: 705.5309\n",
      "Training Epoch: 22 [31700/36045]\tLoss: 514.4169\n",
      "Training Epoch: 22 [31750/36045]\tLoss: 427.3889\n",
      "Training Epoch: 22 [31800/36045]\tLoss: 406.7679\n",
      "Training Epoch: 22 [31850/36045]\tLoss: 416.6928\n",
      "Training Epoch: 22 [31900/36045]\tLoss: 647.9646\n",
      "Training Epoch: 22 [31950/36045]\tLoss: 831.8438\n",
      "Training Epoch: 22 [32000/36045]\tLoss: 947.0225\n",
      "Training Epoch: 22 [32050/36045]\tLoss: 900.0828\n",
      "Training Epoch: 22 [32100/36045]\tLoss: 888.7362\n",
      "Training Epoch: 22 [32150/36045]\tLoss: 697.1454\n",
      "Training Epoch: 22 [32200/36045]\tLoss: 702.0023\n",
      "Training Epoch: 22 [32250/36045]\tLoss: 713.1541\n",
      "Training Epoch: 22 [32300/36045]\tLoss: 694.9899\n",
      "Training Epoch: 22 [32350/36045]\tLoss: 689.4218\n",
      "Training Epoch: 22 [32400/36045]\tLoss: 647.3160\n",
      "Training Epoch: 22 [32450/36045]\tLoss: 533.9101\n",
      "Training Epoch: 22 [32500/36045]\tLoss: 513.6187\n",
      "Training Epoch: 22 [32550/36045]\tLoss: 516.4458\n",
      "Training Epoch: 22 [32600/36045]\tLoss: 512.5883\n",
      "Training Epoch: 22 [32650/36045]\tLoss: 650.5737\n",
      "Training Epoch: 22 [32700/36045]\tLoss: 708.6483\n",
      "Training Epoch: 22 [32750/36045]\tLoss: 675.3306\n",
      "Training Epoch: 22 [32800/36045]\tLoss: 692.8110\n",
      "Training Epoch: 22 [32850/36045]\tLoss: 640.2758\n",
      "Training Epoch: 22 [32900/36045]\tLoss: 517.2236\n",
      "Training Epoch: 22 [32950/36045]\tLoss: 541.0147\n",
      "Training Epoch: 22 [33000/36045]\tLoss: 540.9056\n",
      "Training Epoch: 22 [33050/36045]\tLoss: 512.8847\n",
      "Training Epoch: 22 [33100/36045]\tLoss: 583.8657\n",
      "Training Epoch: 22 [33150/36045]\tLoss: 790.1046\n",
      "Training Epoch: 22 [33200/36045]\tLoss: 770.1859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 22 [33250/36045]\tLoss: 793.1501\n",
      "Training Epoch: 22 [33300/36045]\tLoss: 844.1221\n",
      "Training Epoch: 22 [33350/36045]\tLoss: 648.2337\n",
      "Training Epoch: 22 [33400/36045]\tLoss: 478.7087\n",
      "Training Epoch: 22 [33450/36045]\tLoss: 473.7406\n",
      "Training Epoch: 22 [33500/36045]\tLoss: 487.4355\n",
      "Training Epoch: 22 [33550/36045]\tLoss: 505.9374\n",
      "Training Epoch: 22 [33600/36045]\tLoss: 507.4231\n",
      "Training Epoch: 22 [33650/36045]\tLoss: 674.8445\n",
      "Training Epoch: 22 [33700/36045]\tLoss: 653.3095\n",
      "Training Epoch: 22 [33750/36045]\tLoss: 676.4262\n",
      "Training Epoch: 22 [33800/36045]\tLoss: 671.2267\n",
      "Training Epoch: 22 [33850/36045]\tLoss: 674.4936\n",
      "Training Epoch: 22 [33900/36045]\tLoss: 684.4606\n",
      "Training Epoch: 22 [33950/36045]\tLoss: 696.3860\n",
      "Training Epoch: 22 [34000/36045]\tLoss: 684.2109\n",
      "Training Epoch: 22 [34050/36045]\tLoss: 688.6117\n",
      "Training Epoch: 22 [34100/36045]\tLoss: 662.3730\n",
      "Training Epoch: 22 [34150/36045]\tLoss: 616.6967\n",
      "Training Epoch: 22 [34200/36045]\tLoss: 583.9272\n",
      "Training Epoch: 22 [34250/36045]\tLoss: 598.1452\n",
      "Training Epoch: 22 [34300/36045]\tLoss: 513.1998\n",
      "Training Epoch: 22 [34350/36045]\tLoss: 539.7579\n",
      "Training Epoch: 22 [34400/36045]\tLoss: 530.0057\n",
      "Training Epoch: 22 [34450/36045]\tLoss: 497.8341\n",
      "Training Epoch: 22 [34500/36045]\tLoss: 531.7910\n",
      "Training Epoch: 22 [34550/36045]\tLoss: 522.2415\n",
      "Training Epoch: 22 [34600/36045]\tLoss: 522.4523\n",
      "Training Epoch: 22 [34650/36045]\tLoss: 632.8179\n",
      "Training Epoch: 22 [34700/36045]\tLoss: 669.1237\n",
      "Training Epoch: 22 [34750/36045]\tLoss: 595.0878\n",
      "Training Epoch: 22 [34800/36045]\tLoss: 678.7463\n",
      "Training Epoch: 22 [34850/36045]\tLoss: 687.6689\n",
      "Training Epoch: 22 [34900/36045]\tLoss: 763.5562\n",
      "Training Epoch: 22 [34950/36045]\tLoss: 751.3480\n",
      "Training Epoch: 22 [35000/36045]\tLoss: 752.5963\n",
      "Training Epoch: 22 [35050/36045]\tLoss: 738.2147\n",
      "Training Epoch: 22 [35100/36045]\tLoss: 615.5774\n",
      "Training Epoch: 22 [35150/36045]\tLoss: 608.7263\n",
      "Training Epoch: 22 [35200/36045]\tLoss: 518.3244\n",
      "Training Epoch: 22 [35250/36045]\tLoss: 568.6215\n",
      "Training Epoch: 22 [35300/36045]\tLoss: 582.3734\n",
      "Training Epoch: 22 [35350/36045]\tLoss: 666.0679\n",
      "Training Epoch: 22 [35400/36045]\tLoss: 704.3778\n",
      "Training Epoch: 22 [35450/36045]\tLoss: 672.3766\n",
      "Training Epoch: 22 [35500/36045]\tLoss: 654.1172\n",
      "Training Epoch: 22 [35550/36045]\tLoss: 638.8656\n",
      "Training Epoch: 22 [35600/36045]\tLoss: 686.1141\n",
      "Training Epoch: 22 [35650/36045]\tLoss: 761.2692\n",
      "Training Epoch: 22 [35700/36045]\tLoss: 688.2213\n",
      "Training Epoch: 22 [35750/36045]\tLoss: 747.7043\n",
      "Training Epoch: 22 [35800/36045]\tLoss: 753.9288\n",
      "Training Epoch: 22 [35850/36045]\tLoss: 728.3148\n",
      "Training Epoch: 22 [35900/36045]\tLoss: 756.5541\n",
      "Training Epoch: 22 [35950/36045]\tLoss: 754.5156\n",
      "Training Epoch: 22 [36000/36045]\tLoss: 745.2249\n",
      "Training Epoch: 22 [36045/36045]\tLoss: 727.2590\n",
      "Training Epoch: 22 [4004/4004]\tLoss: 679.6770\n",
      "Training Epoch: 23 [50/36045]\tLoss: 679.9398\n",
      "Training Epoch: 23 [100/36045]\tLoss: 651.9723\n",
      "Training Epoch: 23 [150/36045]\tLoss: 650.0591\n",
      "Training Epoch: 23 [200/36045]\tLoss: 637.0502\n",
      "Training Epoch: 23 [250/36045]\tLoss: 756.8975\n",
      "Training Epoch: 23 [300/36045]\tLoss: 820.2350\n",
      "Training Epoch: 23 [350/36045]\tLoss: 785.1077\n",
      "Training Epoch: 23 [400/36045]\tLoss: 781.7184\n",
      "Training Epoch: 23 [450/36045]\tLoss: 761.2748\n",
      "Training Epoch: 23 [500/36045]\tLoss: 710.0093\n",
      "Training Epoch: 23 [550/36045]\tLoss: 715.6032\n",
      "Training Epoch: 23 [600/36045]\tLoss: 693.6187\n",
      "Training Epoch: 23 [650/36045]\tLoss: 718.3040\n",
      "Training Epoch: 23 [700/36045]\tLoss: 705.8917\n",
      "Training Epoch: 23 [750/36045]\tLoss: 687.6660\n",
      "Training Epoch: 23 [800/36045]\tLoss: 703.3662\n",
      "Training Epoch: 23 [850/36045]\tLoss: 684.0174\n",
      "Training Epoch: 23 [900/36045]\tLoss: 650.5060\n",
      "Training Epoch: 23 [950/36045]\tLoss: 616.4974\n",
      "Training Epoch: 23 [1000/36045]\tLoss: 593.1349\n",
      "Training Epoch: 23 [1050/36045]\tLoss: 595.3281\n",
      "Training Epoch: 23 [1100/36045]\tLoss: 579.9324\n",
      "Training Epoch: 23 [1150/36045]\tLoss: 589.2220\n",
      "Training Epoch: 23 [1200/36045]\tLoss: 621.7714\n",
      "Training Epoch: 23 [1250/36045]\tLoss: 710.6699\n",
      "Training Epoch: 23 [1300/36045]\tLoss: 716.2684\n",
      "Training Epoch: 23 [1350/36045]\tLoss: 718.9830\n",
      "Training Epoch: 23 [1400/36045]\tLoss: 747.3029\n",
      "Training Epoch: 23 [1450/36045]\tLoss: 723.0438\n",
      "Training Epoch: 23 [1500/36045]\tLoss: 666.1377\n",
      "Training Epoch: 23 [1550/36045]\tLoss: 682.3940\n",
      "Training Epoch: 23 [1600/36045]\tLoss: 693.0405\n",
      "Training Epoch: 23 [1650/36045]\tLoss: 679.4911\n",
      "Training Epoch: 23 [1700/36045]\tLoss: 691.0905\n",
      "Training Epoch: 23 [1750/36045]\tLoss: 733.1249\n",
      "Training Epoch: 23 [1800/36045]\tLoss: 714.7975\n",
      "Training Epoch: 23 [1850/36045]\tLoss: 733.7727\n",
      "Training Epoch: 23 [1900/36045]\tLoss: 686.5635\n",
      "Training Epoch: 23 [1950/36045]\tLoss: 697.3143\n",
      "Training Epoch: 23 [2000/36045]\tLoss: 630.6765\n",
      "Training Epoch: 23 [2050/36045]\tLoss: 634.1957\n",
      "Training Epoch: 23 [2100/36045]\tLoss: 668.6441\n",
      "Training Epoch: 23 [2150/36045]\tLoss: 647.2542\n",
      "Training Epoch: 23 [2200/36045]\tLoss: 600.0994\n",
      "Training Epoch: 23 [2250/36045]\tLoss: 566.9141\n",
      "Training Epoch: 23 [2300/36045]\tLoss: 594.4832\n",
      "Training Epoch: 23 [2350/36045]\tLoss: 567.9551\n",
      "Training Epoch: 23 [2400/36045]\tLoss: 578.7677\n",
      "Training Epoch: 23 [2450/36045]\tLoss: 735.1384\n",
      "Training Epoch: 23 [2500/36045]\tLoss: 772.4766\n",
      "Training Epoch: 23 [2550/36045]\tLoss: 768.8875\n",
      "Training Epoch: 23 [2600/36045]\tLoss: 778.2095\n",
      "Training Epoch: 23 [2650/36045]\tLoss: 903.8294\n",
      "Training Epoch: 23 [2700/36045]\tLoss: 991.0518\n",
      "Training Epoch: 23 [2750/36045]\tLoss: 1064.4391\n",
      "Training Epoch: 23 [2800/36045]\tLoss: 1074.9299\n",
      "Training Epoch: 23 [2850/36045]\tLoss: 843.7690\n",
      "Training Epoch: 23 [2900/36045]\tLoss: 809.8749\n",
      "Training Epoch: 23 [2950/36045]\tLoss: 781.6698\n",
      "Training Epoch: 23 [3000/36045]\tLoss: 777.0521\n",
      "Training Epoch: 23 [3050/36045]\tLoss: 808.5507\n",
      "Training Epoch: 23 [3100/36045]\tLoss: 740.0684\n",
      "Training Epoch: 23 [3150/36045]\tLoss: 571.0584\n",
      "Training Epoch: 23 [3200/36045]\tLoss: 592.7927\n",
      "Training Epoch: 23 [3250/36045]\tLoss: 557.9609\n",
      "Training Epoch: 23 [3300/36045]\tLoss: 528.7662\n",
      "Training Epoch: 23 [3350/36045]\tLoss: 557.5900\n",
      "Training Epoch: 23 [3400/36045]\tLoss: 586.1042\n",
      "Training Epoch: 23 [3450/36045]\tLoss: 629.0400\n",
      "Training Epoch: 23 [3500/36045]\tLoss: 615.4979\n",
      "Training Epoch: 23 [3550/36045]\tLoss: 591.3271\n",
      "Training Epoch: 23 [3600/36045]\tLoss: 633.0403\n",
      "Training Epoch: 23 [3650/36045]\tLoss: 731.8618\n",
      "Training Epoch: 23 [3700/36045]\tLoss: 737.8513\n",
      "Training Epoch: 23 [3750/36045]\tLoss: 705.5518\n",
      "Training Epoch: 23 [3800/36045]\tLoss: 698.2735\n",
      "Training Epoch: 23 [3850/36045]\tLoss: 697.3659\n",
      "Training Epoch: 23 [3900/36045]\tLoss: 703.2391\n",
      "Training Epoch: 23 [3950/36045]\tLoss: 678.0385\n",
      "Training Epoch: 23 [4000/36045]\tLoss: 685.6526\n",
      "Training Epoch: 23 [4050/36045]\tLoss: 630.8632\n",
      "Training Epoch: 23 [4100/36045]\tLoss: 614.6631\n",
      "Training Epoch: 23 [4150/36045]\tLoss: 632.4493\n",
      "Training Epoch: 23 [4200/36045]\tLoss: 626.7341\n",
      "Training Epoch: 23 [4250/36045]\tLoss: 628.8484\n",
      "Training Epoch: 23 [4300/36045]\tLoss: 648.3587\n",
      "Training Epoch: 23 [4350/36045]\tLoss: 629.8740\n",
      "Training Epoch: 23 [4400/36045]\tLoss: 603.4349\n",
      "Training Epoch: 23 [4450/36045]\tLoss: 658.5575\n",
      "Training Epoch: 23 [4500/36045]\tLoss: 703.4590\n",
      "Training Epoch: 23 [4550/36045]\tLoss: 709.6902\n",
      "Training Epoch: 23 [4600/36045]\tLoss: 733.2635\n",
      "Training Epoch: 23 [4650/36045]\tLoss: 721.9180\n",
      "Training Epoch: 23 [4700/36045]\tLoss: 667.7292\n",
      "Training Epoch: 23 [4750/36045]\tLoss: 650.3304\n",
      "Training Epoch: 23 [4800/36045]\tLoss: 677.7241\n",
      "Training Epoch: 23 [4850/36045]\tLoss: 664.0425\n",
      "Training Epoch: 23 [4900/36045]\tLoss: 645.5184\n",
      "Training Epoch: 23 [4950/36045]\tLoss: 663.9599\n",
      "Training Epoch: 23 [5000/36045]\tLoss: 695.8171\n",
      "Training Epoch: 23 [5050/36045]\tLoss: 673.9915\n",
      "Training Epoch: 23 [5100/36045]\tLoss: 684.4512\n",
      "Training Epoch: 23 [5150/36045]\tLoss: 668.9105\n",
      "Training Epoch: 23 [5200/36045]\tLoss: 666.9129\n",
      "Training Epoch: 23 [5250/36045]\tLoss: 659.6336\n",
      "Training Epoch: 23 [5300/36045]\tLoss: 660.7343\n",
      "Training Epoch: 23 [5350/36045]\tLoss: 684.5749\n",
      "Training Epoch: 23 [5400/36045]\tLoss: 658.5080\n",
      "Training Epoch: 23 [5450/36045]\tLoss: 624.5830\n",
      "Training Epoch: 23 [5500/36045]\tLoss: 655.2025\n",
      "Training Epoch: 23 [5550/36045]\tLoss: 642.9443\n",
      "Training Epoch: 23 [5600/36045]\tLoss: 728.5888\n",
      "Training Epoch: 23 [5650/36045]\tLoss: 690.8092\n",
      "Training Epoch: 23 [5700/36045]\tLoss: 649.0699\n",
      "Training Epoch: 23 [5750/36045]\tLoss: 634.1475\n",
      "Training Epoch: 23 [5800/36045]\tLoss: 669.5104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [5850/36045]\tLoss: 653.8872\n",
      "Training Epoch: 23 [5900/36045]\tLoss: 753.1907\n",
      "Training Epoch: 23 [5950/36045]\tLoss: 771.4619\n",
      "Training Epoch: 23 [6000/36045]\tLoss: 755.5608\n",
      "Training Epoch: 23 [6050/36045]\tLoss: 730.3111\n",
      "Training Epoch: 23 [6100/36045]\tLoss: 735.6118\n",
      "Training Epoch: 23 [6150/36045]\tLoss: 718.5073\n",
      "Training Epoch: 23 [6200/36045]\tLoss: 719.4794\n",
      "Training Epoch: 23 [6250/36045]\tLoss: 740.4194\n",
      "Training Epoch: 23 [6300/36045]\tLoss: 752.5414\n",
      "Training Epoch: 23 [6350/36045]\tLoss: 801.7592\n",
      "Training Epoch: 23 [6400/36045]\tLoss: 669.1492\n",
      "Training Epoch: 23 [6450/36045]\tLoss: 619.3401\n",
      "Training Epoch: 23 [6500/36045]\tLoss: 631.2080\n",
      "Training Epoch: 23 [6550/36045]\tLoss: 647.8917\n",
      "Training Epoch: 23 [6600/36045]\tLoss: 648.6935\n",
      "Training Epoch: 23 [6650/36045]\tLoss: 732.9050\n",
      "Training Epoch: 23 [6700/36045]\tLoss: 767.0555\n",
      "Training Epoch: 23 [6750/36045]\tLoss: 740.3377\n",
      "Training Epoch: 23 [6800/36045]\tLoss: 743.3646\n",
      "Training Epoch: 23 [6850/36045]\tLoss: 731.2939\n",
      "Training Epoch: 23 [6900/36045]\tLoss: 650.7139\n",
      "Training Epoch: 23 [6950/36045]\tLoss: 613.8577\n",
      "Training Epoch: 23 [7000/36045]\tLoss: 652.7815\n",
      "Training Epoch: 23 [7050/36045]\tLoss: 667.0374\n",
      "Training Epoch: 23 [7100/36045]\tLoss: 666.4106\n",
      "Training Epoch: 23 [7150/36045]\tLoss: 677.7878\n",
      "Training Epoch: 23 [7200/36045]\tLoss: 682.0428\n",
      "Training Epoch: 23 [7250/36045]\tLoss: 679.3692\n",
      "Training Epoch: 23 [7300/36045]\tLoss: 666.5345\n",
      "Training Epoch: 23 [7350/36045]\tLoss: 661.7179\n",
      "Training Epoch: 23 [7400/36045]\tLoss: 597.2715\n",
      "Training Epoch: 23 [7450/36045]\tLoss: 601.7714\n",
      "Training Epoch: 23 [7500/36045]\tLoss: 595.6949\n",
      "Training Epoch: 23 [7550/36045]\tLoss: 571.0280\n",
      "Training Epoch: 23 [7600/36045]\tLoss: 635.7824\n",
      "Training Epoch: 23 [7650/36045]\tLoss: 683.2858\n",
      "Training Epoch: 23 [7700/36045]\tLoss: 651.4433\n",
      "Training Epoch: 23 [7750/36045]\tLoss: 665.8931\n",
      "Training Epoch: 23 [7800/36045]\tLoss: 654.0037\n",
      "Training Epoch: 23 [7850/36045]\tLoss: 631.1891\n",
      "Training Epoch: 23 [7900/36045]\tLoss: 666.1575\n",
      "Training Epoch: 23 [7950/36045]\tLoss: 663.3370\n",
      "Training Epoch: 23 [8000/36045]\tLoss: 681.2640\n",
      "Training Epoch: 23 [8050/36045]\tLoss: 644.3238\n",
      "Training Epoch: 23 [8100/36045]\tLoss: 670.3495\n",
      "Training Epoch: 23 [8150/36045]\tLoss: 758.7231\n",
      "Training Epoch: 23 [8200/36045]\tLoss: 744.5690\n",
      "Training Epoch: 23 [8250/36045]\tLoss: 711.5120\n",
      "Training Epoch: 23 [8300/36045]\tLoss: 773.9580\n",
      "Training Epoch: 23 [8350/36045]\tLoss: 712.0241\n",
      "Training Epoch: 23 [8400/36045]\tLoss: 638.5169\n",
      "Training Epoch: 23 [8450/36045]\tLoss: 598.6342\n",
      "Training Epoch: 23 [8500/36045]\tLoss: 635.5114\n",
      "Training Epoch: 23 [8550/36045]\tLoss: 626.5652\n",
      "Training Epoch: 23 [8600/36045]\tLoss: 619.7888\n",
      "Training Epoch: 23 [8650/36045]\tLoss: 662.3052\n",
      "Training Epoch: 23 [8700/36045]\tLoss: 699.9913\n",
      "Training Epoch: 23 [8750/36045]\tLoss: 687.1667\n",
      "Training Epoch: 23 [8800/36045]\tLoss: 692.5649\n",
      "Training Epoch: 23 [8850/36045]\tLoss: 685.5731\n",
      "Training Epoch: 23 [8900/36045]\tLoss: 618.8354\n",
      "Training Epoch: 23 [8950/36045]\tLoss: 632.5942\n",
      "Training Epoch: 23 [9000/36045]\tLoss: 648.3592\n",
      "Training Epoch: 23 [9050/36045]\tLoss: 648.5494\n",
      "Training Epoch: 23 [9100/36045]\tLoss: 667.9268\n",
      "Training Epoch: 23 [9150/36045]\tLoss: 492.9748\n",
      "Training Epoch: 23 [9200/36045]\tLoss: 370.4521\n",
      "Training Epoch: 23 [9250/36045]\tLoss: 401.6905\n",
      "Training Epoch: 23 [9300/36045]\tLoss: 413.6553\n",
      "Training Epoch: 23 [9350/36045]\tLoss: 380.9549\n",
      "Training Epoch: 23 [9400/36045]\tLoss: 746.3287\n",
      "Training Epoch: 23 [9450/36045]\tLoss: 792.7964\n",
      "Training Epoch: 23 [9500/36045]\tLoss: 779.3307\n",
      "Training Epoch: 23 [9550/36045]\tLoss: 824.0456\n",
      "Training Epoch: 23 [9600/36045]\tLoss: 612.5635\n",
      "Training Epoch: 23 [9650/36045]\tLoss: 615.9094\n",
      "Training Epoch: 23 [9700/36045]\tLoss: 600.9878\n",
      "Training Epoch: 23 [9750/36045]\tLoss: 601.1627\n",
      "Training Epoch: 23 [9800/36045]\tLoss: 783.4963\n",
      "Training Epoch: 23 [9850/36045]\tLoss: 827.5536\n",
      "Training Epoch: 23 [9900/36045]\tLoss: 842.8629\n",
      "Training Epoch: 23 [9950/36045]\tLoss: 820.7690\n",
      "Training Epoch: 23 [10000/36045]\tLoss: 757.8602\n",
      "Training Epoch: 23 [10050/36045]\tLoss: 627.7482\n",
      "Training Epoch: 23 [10100/36045]\tLoss: 633.9705\n",
      "Training Epoch: 23 [10150/36045]\tLoss: 644.2590\n",
      "Training Epoch: 23 [10200/36045]\tLoss: 633.1906\n",
      "Training Epoch: 23 [10250/36045]\tLoss: 754.5281\n",
      "Training Epoch: 23 [10300/36045]\tLoss: 732.5323\n",
      "Training Epoch: 23 [10350/36045]\tLoss: 770.8458\n",
      "Training Epoch: 23 [10400/36045]\tLoss: 761.5086\n",
      "Training Epoch: 23 [10450/36045]\tLoss: 712.8975\n",
      "Training Epoch: 23 [10500/36045]\tLoss: 597.9470\n",
      "Training Epoch: 23 [10550/36045]\tLoss: 593.3370\n",
      "Training Epoch: 23 [10600/36045]\tLoss: 616.9236\n",
      "Training Epoch: 23 [10650/36045]\tLoss: 623.0357\n",
      "Training Epoch: 23 [10700/36045]\tLoss: 711.1301\n",
      "Training Epoch: 23 [10750/36045]\tLoss: 774.5188\n",
      "Training Epoch: 23 [10800/36045]\tLoss: 716.1498\n",
      "Training Epoch: 23 [10850/36045]\tLoss: 758.3121\n",
      "Training Epoch: 23 [10900/36045]\tLoss: 789.0881\n",
      "Training Epoch: 23 [10950/36045]\tLoss: 584.7032\n",
      "Training Epoch: 23 [11000/36045]\tLoss: 578.3973\n",
      "Training Epoch: 23 [11050/36045]\tLoss: 618.6332\n",
      "Training Epoch: 23 [11100/36045]\tLoss: 630.8812\n",
      "Training Epoch: 23 [11150/36045]\tLoss: 683.6779\n",
      "Training Epoch: 23 [11200/36045]\tLoss: 712.1509\n",
      "Training Epoch: 23 [11250/36045]\tLoss: 724.4745\n",
      "Training Epoch: 23 [11300/36045]\tLoss: 704.3978\n",
      "Training Epoch: 23 [11350/36045]\tLoss: 701.1776\n",
      "Training Epoch: 23 [11400/36045]\tLoss: 661.0964\n",
      "Training Epoch: 23 [11450/36045]\tLoss: 627.0684\n",
      "Training Epoch: 23 [11500/36045]\tLoss: 624.7247\n",
      "Training Epoch: 23 [11550/36045]\tLoss: 638.0226\n",
      "Training Epoch: 23 [11600/36045]\tLoss: 701.6499\n",
      "Training Epoch: 23 [11650/36045]\tLoss: 754.6982\n",
      "Training Epoch: 23 [11700/36045]\tLoss: 753.7338\n",
      "Training Epoch: 23 [11750/36045]\tLoss: 773.3923\n",
      "Training Epoch: 23 [11800/36045]\tLoss: 816.6799\n",
      "Training Epoch: 23 [11850/36045]\tLoss: 872.5518\n",
      "Training Epoch: 23 [11900/36045]\tLoss: 1092.0121\n",
      "Training Epoch: 23 [11950/36045]\tLoss: 1093.3312\n",
      "Training Epoch: 23 [12000/36045]\tLoss: 1108.2092\n",
      "Training Epoch: 23 [12050/36045]\tLoss: 1065.7860\n",
      "Training Epoch: 23 [12100/36045]\tLoss: 700.9600\n",
      "Training Epoch: 23 [12150/36045]\tLoss: 540.6670\n",
      "Training Epoch: 23 [12200/36045]\tLoss: 535.0212\n",
      "Training Epoch: 23 [12250/36045]\tLoss: 544.5043\n",
      "Training Epoch: 23 [12300/36045]\tLoss: 691.2777\n",
      "Training Epoch: 23 [12350/36045]\tLoss: 750.2225\n",
      "Training Epoch: 23 [12400/36045]\tLoss: 758.4814\n",
      "Training Epoch: 23 [12450/36045]\tLoss: 745.8425\n",
      "Training Epoch: 23 [12500/36045]\tLoss: 775.6315\n",
      "Training Epoch: 23 [12550/36045]\tLoss: 743.6463\n",
      "Training Epoch: 23 [12600/36045]\tLoss: 686.3340\n",
      "Training Epoch: 23 [12650/36045]\tLoss: 685.1639\n",
      "Training Epoch: 23 [12700/36045]\tLoss: 706.9713\n",
      "Training Epoch: 23 [12750/36045]\tLoss: 706.6910\n",
      "Training Epoch: 23 [12800/36045]\tLoss: 688.4540\n",
      "Training Epoch: 23 [12850/36045]\tLoss: 719.0299\n",
      "Training Epoch: 23 [12900/36045]\tLoss: 689.8140\n",
      "Training Epoch: 23 [12950/36045]\tLoss: 677.7210\n",
      "Training Epoch: 23 [13000/36045]\tLoss: 710.0310\n",
      "Training Epoch: 23 [13050/36045]\tLoss: 646.0180\n",
      "Training Epoch: 23 [13100/36045]\tLoss: 667.0331\n",
      "Training Epoch: 23 [13150/36045]\tLoss: 658.7247\n",
      "Training Epoch: 23 [13200/36045]\tLoss: 636.8070\n",
      "Training Epoch: 23 [13250/36045]\tLoss: 663.7851\n",
      "Training Epoch: 23 [13300/36045]\tLoss: 704.3438\n",
      "Training Epoch: 23 [13350/36045]\tLoss: 683.3513\n",
      "Training Epoch: 23 [13400/36045]\tLoss: 687.5316\n",
      "Training Epoch: 23 [13450/36045]\tLoss: 682.6745\n",
      "Training Epoch: 23 [13500/36045]\tLoss: 705.2720\n",
      "Training Epoch: 23 [13550/36045]\tLoss: 841.0411\n",
      "Training Epoch: 23 [13600/36045]\tLoss: 873.7526\n",
      "Training Epoch: 23 [13650/36045]\tLoss: 953.8197\n",
      "Training Epoch: 23 [13700/36045]\tLoss: 845.3300\n",
      "Training Epoch: 23 [13750/36045]\tLoss: 690.5560\n",
      "Training Epoch: 23 [13800/36045]\tLoss: 663.6445\n",
      "Training Epoch: 23 [13850/36045]\tLoss: 646.6879\n",
      "Training Epoch: 23 [13900/36045]\tLoss: 654.1635\n",
      "Training Epoch: 23 [13950/36045]\tLoss: 699.9755\n",
      "Training Epoch: 23 [14000/36045]\tLoss: 734.8572\n",
      "Training Epoch: 23 [14050/36045]\tLoss: 707.2610\n",
      "Training Epoch: 23 [14100/36045]\tLoss: 703.1512\n",
      "Training Epoch: 23 [14150/36045]\tLoss: 690.9407\n",
      "Training Epoch: 23 [14200/36045]\tLoss: 734.7952\n",
      "Training Epoch: 23 [14250/36045]\tLoss: 804.8062\n",
      "Training Epoch: 23 [14300/36045]\tLoss: 808.2114\n",
      "Training Epoch: 23 [14350/36045]\tLoss: 773.9717\n",
      "Training Epoch: 23 [14400/36045]\tLoss: 759.4143\n",
      "Training Epoch: 23 [14450/36045]\tLoss: 797.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [14500/36045]\tLoss: 728.2956\n",
      "Training Epoch: 23 [14550/36045]\tLoss: 759.5079\n",
      "Training Epoch: 23 [14600/36045]\tLoss: 743.7598\n",
      "Training Epoch: 23 [14650/36045]\tLoss: 744.7507\n",
      "Training Epoch: 23 [14700/36045]\tLoss: 702.9102\n",
      "Training Epoch: 23 [14750/36045]\tLoss: 603.2357\n",
      "Training Epoch: 23 [14800/36045]\tLoss: 593.3394\n",
      "Training Epoch: 23 [14850/36045]\tLoss: 600.5943\n",
      "Training Epoch: 23 [14900/36045]\tLoss: 594.0452\n",
      "Training Epoch: 23 [14950/36045]\tLoss: 601.9036\n",
      "Training Epoch: 23 [15000/36045]\tLoss: 617.9005\n",
      "Training Epoch: 23 [15050/36045]\tLoss: 615.9816\n",
      "Training Epoch: 23 [15100/36045]\tLoss: 599.8178\n",
      "Training Epoch: 23 [15150/36045]\tLoss: 592.5839\n",
      "Training Epoch: 23 [15200/36045]\tLoss: 547.8418\n",
      "Training Epoch: 23 [15250/36045]\tLoss: 572.1428\n",
      "Training Epoch: 23 [15300/36045]\tLoss: 556.5601\n",
      "Training Epoch: 23 [15350/36045]\tLoss: 569.5615\n",
      "Training Epoch: 23 [15400/36045]\tLoss: 554.5136\n",
      "Training Epoch: 23 [15450/36045]\tLoss: 540.7759\n",
      "Training Epoch: 23 [15500/36045]\tLoss: 556.6191\n",
      "Training Epoch: 23 [15550/36045]\tLoss: 551.7993\n",
      "Training Epoch: 23 [15600/36045]\tLoss: 623.4410\n",
      "Training Epoch: 23 [15650/36045]\tLoss: 642.7320\n",
      "Training Epoch: 23 [15700/36045]\tLoss: 632.6344\n",
      "Training Epoch: 23 [15750/36045]\tLoss: 624.5497\n",
      "Training Epoch: 23 [15800/36045]\tLoss: 586.4937\n",
      "Training Epoch: 23 [15850/36045]\tLoss: 599.9364\n",
      "Training Epoch: 23 [15900/36045]\tLoss: 609.8065\n",
      "Training Epoch: 23 [15950/36045]\tLoss: 629.9221\n",
      "Training Epoch: 23 [16000/36045]\tLoss: 604.8660\n",
      "Training Epoch: 23 [16050/36045]\tLoss: 574.4772\n",
      "Training Epoch: 23 [16100/36045]\tLoss: 531.0914\n",
      "Training Epoch: 23 [16150/36045]\tLoss: 518.3089\n",
      "Training Epoch: 23 [16200/36045]\tLoss: 626.7358\n",
      "Training Epoch: 23 [16250/36045]\tLoss: 656.6537\n",
      "Training Epoch: 23 [16300/36045]\tLoss: 717.1746\n",
      "Training Epoch: 23 [16350/36045]\tLoss: 734.8528\n",
      "Training Epoch: 23 [16400/36045]\tLoss: 707.3068\n",
      "Training Epoch: 23 [16450/36045]\tLoss: 688.2101\n",
      "Training Epoch: 23 [16500/36045]\tLoss: 687.4465\n",
      "Training Epoch: 23 [16550/36045]\tLoss: 651.1459\n",
      "Training Epoch: 23 [16600/36045]\tLoss: 677.4050\n",
      "Training Epoch: 23 [16650/36045]\tLoss: 697.4543\n",
      "Training Epoch: 23 [16700/36045]\tLoss: 673.3260\n",
      "Training Epoch: 23 [16750/36045]\tLoss: 664.8173\n",
      "Training Epoch: 23 [16800/36045]\tLoss: 676.7823\n",
      "Training Epoch: 23 [16850/36045]\tLoss: 644.1902\n",
      "Training Epoch: 23 [16900/36045]\tLoss: 655.0696\n",
      "Training Epoch: 23 [16950/36045]\tLoss: 681.6201\n",
      "Training Epoch: 23 [17000/36045]\tLoss: 663.1549\n",
      "Training Epoch: 23 [17050/36045]\tLoss: 692.3041\n",
      "Training Epoch: 23 [17100/36045]\tLoss: 689.6661\n",
      "Training Epoch: 23 [17150/36045]\tLoss: 599.7736\n",
      "Training Epoch: 23 [17200/36045]\tLoss: 558.9617\n",
      "Training Epoch: 23 [17250/36045]\tLoss: 584.8568\n",
      "Training Epoch: 23 [17300/36045]\tLoss: 618.0784\n",
      "Training Epoch: 23 [17350/36045]\tLoss: 593.3273\n",
      "Training Epoch: 23 [17400/36045]\tLoss: 613.0126\n",
      "Training Epoch: 23 [17450/36045]\tLoss: 633.9140\n",
      "Training Epoch: 23 [17500/36045]\tLoss: 621.3975\n",
      "Training Epoch: 23 [17550/36045]\tLoss: 621.6926\n",
      "Training Epoch: 23 [17600/36045]\tLoss: 613.1990\n",
      "Training Epoch: 23 [17650/36045]\tLoss: 630.9328\n",
      "Training Epoch: 23 [17700/36045]\tLoss: 609.3701\n",
      "Training Epoch: 23 [17750/36045]\tLoss: 626.5592\n",
      "Training Epoch: 23 [17800/36045]\tLoss: 617.0190\n",
      "Training Epoch: 23 [17850/36045]\tLoss: 623.0483\n",
      "Training Epoch: 23 [17900/36045]\tLoss: 652.3868\n",
      "Training Epoch: 23 [17950/36045]\tLoss: 663.6888\n",
      "Training Epoch: 23 [18000/36045]\tLoss: 653.6707\n",
      "Training Epoch: 23 [18050/36045]\tLoss: 727.8869\n",
      "Training Epoch: 23 [18100/36045]\tLoss: 730.5998\n",
      "Training Epoch: 23 [18150/36045]\tLoss: 740.9633\n",
      "Training Epoch: 23 [18200/36045]\tLoss: 723.5424\n",
      "Training Epoch: 23 [18250/36045]\tLoss: 744.6840\n",
      "Training Epoch: 23 [18300/36045]\tLoss: 689.7279\n",
      "Training Epoch: 23 [18350/36045]\tLoss: 759.2330\n",
      "Training Epoch: 23 [18400/36045]\tLoss: 731.9493\n",
      "Training Epoch: 23 [18450/36045]\tLoss: 712.4131\n",
      "Training Epoch: 23 [18500/36045]\tLoss: 711.7207\n",
      "Training Epoch: 23 [18550/36045]\tLoss: 698.3734\n",
      "Training Epoch: 23 [18600/36045]\tLoss: 688.0504\n",
      "Training Epoch: 23 [18650/36045]\tLoss: 736.0999\n",
      "Training Epoch: 23 [18700/36045]\tLoss: 774.5267\n",
      "Training Epoch: 23 [18750/36045]\tLoss: 759.6492\n",
      "Training Epoch: 23 [18800/36045]\tLoss: 784.3057\n",
      "Training Epoch: 23 [18850/36045]\tLoss: 728.1497\n",
      "Training Epoch: 23 [18900/36045]\tLoss: 779.0595\n",
      "Training Epoch: 23 [18950/36045]\tLoss: 719.0283\n",
      "Training Epoch: 23 [19000/36045]\tLoss: 607.6492\n",
      "Training Epoch: 23 [19050/36045]\tLoss: 588.4565\n",
      "Training Epoch: 23 [19100/36045]\tLoss: 598.4022\n",
      "Training Epoch: 23 [19150/36045]\tLoss: 587.1267\n",
      "Training Epoch: 23 [19200/36045]\tLoss: 614.4929\n",
      "Training Epoch: 23 [19250/36045]\tLoss: 629.0103\n",
      "Training Epoch: 23 [19300/36045]\tLoss: 640.2091\n",
      "Training Epoch: 23 [19350/36045]\tLoss: 623.8156\n",
      "Training Epoch: 23 [19400/36045]\tLoss: 646.8344\n",
      "Training Epoch: 23 [19450/36045]\tLoss: 636.9611\n",
      "Training Epoch: 23 [19500/36045]\tLoss: 639.1677\n",
      "Training Epoch: 23 [19550/36045]\tLoss: 637.4738\n",
      "Training Epoch: 23 [19600/36045]\tLoss: 679.2678\n",
      "Training Epoch: 23 [19650/36045]\tLoss: 892.6063\n",
      "Training Epoch: 23 [19700/36045]\tLoss: 851.3293\n",
      "Training Epoch: 23 [19750/36045]\tLoss: 853.3506\n",
      "Training Epoch: 23 [19800/36045]\tLoss: 850.9831\n",
      "Training Epoch: 23 [19850/36045]\tLoss: 570.9918\n",
      "Training Epoch: 23 [19900/36045]\tLoss: 548.4422\n",
      "Training Epoch: 23 [19950/36045]\tLoss: 552.0569\n",
      "Training Epoch: 23 [20000/36045]\tLoss: 550.3846\n",
      "Training Epoch: 23 [20050/36045]\tLoss: 615.8135\n",
      "Training Epoch: 23 [20100/36045]\tLoss: 621.8741\n",
      "Training Epoch: 23 [20150/36045]\tLoss: 624.8562\n",
      "Training Epoch: 23 [20200/36045]\tLoss: 625.4330\n",
      "Training Epoch: 23 [20250/36045]\tLoss: 666.0513\n",
      "Training Epoch: 23 [20300/36045]\tLoss: 702.8388\n",
      "Training Epoch: 23 [20350/36045]\tLoss: 722.3605\n",
      "Training Epoch: 23 [20400/36045]\tLoss: 738.3968\n",
      "Training Epoch: 23 [20450/36045]\tLoss: 709.5403\n",
      "Training Epoch: 23 [20500/36045]\tLoss: 693.0703\n",
      "Training Epoch: 23 [20550/36045]\tLoss: 610.5772\n",
      "Training Epoch: 23 [20600/36045]\tLoss: 622.6206\n",
      "Training Epoch: 23 [20650/36045]\tLoss: 618.7603\n",
      "Training Epoch: 23 [20700/36045]\tLoss: 606.0881\n",
      "Training Epoch: 23 [20750/36045]\tLoss: 650.9930\n",
      "Training Epoch: 23 [20800/36045]\tLoss: 708.4019\n",
      "Training Epoch: 23 [20850/36045]\tLoss: 695.4988\n",
      "Training Epoch: 23 [20900/36045]\tLoss: 742.6381\n",
      "Training Epoch: 23 [20950/36045]\tLoss: 700.6721\n",
      "Training Epoch: 23 [21000/36045]\tLoss: 660.8333\n",
      "Training Epoch: 23 [21050/36045]\tLoss: 566.6652\n",
      "Training Epoch: 23 [21100/36045]\tLoss: 569.3032\n",
      "Training Epoch: 23 [21150/36045]\tLoss: 609.1580\n",
      "Training Epoch: 23 [21200/36045]\tLoss: 608.4031\n",
      "Training Epoch: 23 [21250/36045]\tLoss: 582.1458\n",
      "Training Epoch: 23 [21300/36045]\tLoss: 679.3853\n",
      "Training Epoch: 23 [21350/36045]\tLoss: 672.5988\n",
      "Training Epoch: 23 [21400/36045]\tLoss: 675.9781\n",
      "Training Epoch: 23 [21450/36045]\tLoss: 682.5626\n",
      "Training Epoch: 23 [21500/36045]\tLoss: 686.0184\n",
      "Training Epoch: 23 [21550/36045]\tLoss: 782.2598\n",
      "Training Epoch: 23 [21600/36045]\tLoss: 782.6139\n",
      "Training Epoch: 23 [21650/36045]\tLoss: 795.9819\n",
      "Training Epoch: 23 [21700/36045]\tLoss: 796.1308\n",
      "Training Epoch: 23 [21750/36045]\tLoss: 766.1116\n",
      "Training Epoch: 23 [21800/36045]\tLoss: 567.7111\n",
      "Training Epoch: 23 [21850/36045]\tLoss: 550.3207\n",
      "Training Epoch: 23 [21900/36045]\tLoss: 561.8099\n",
      "Training Epoch: 23 [21950/36045]\tLoss: 560.0942\n",
      "Training Epoch: 23 [22000/36045]\tLoss: 564.7014\n",
      "Training Epoch: 23 [22050/36045]\tLoss: 590.8239\n",
      "Training Epoch: 23 [22100/36045]\tLoss: 582.7544\n",
      "Training Epoch: 23 [22150/36045]\tLoss: 566.3026\n",
      "Training Epoch: 23 [22200/36045]\tLoss: 584.6575\n",
      "Training Epoch: 23 [22250/36045]\tLoss: 590.0661\n",
      "Training Epoch: 23 [22300/36045]\tLoss: 644.0692\n",
      "Training Epoch: 23 [22350/36045]\tLoss: 670.6496\n",
      "Training Epoch: 23 [22400/36045]\tLoss: 686.1723\n",
      "Training Epoch: 23 [22450/36045]\tLoss: 673.9014\n",
      "Training Epoch: 23 [22500/36045]\tLoss: 654.8846\n",
      "Training Epoch: 23 [22550/36045]\tLoss: 693.2565\n",
      "Training Epoch: 23 [22600/36045]\tLoss: 753.8882\n",
      "Training Epoch: 23 [22650/36045]\tLoss: 791.0549\n",
      "Training Epoch: 23 [22700/36045]\tLoss: 814.5794\n",
      "Training Epoch: 23 [22750/36045]\tLoss: 835.4886\n",
      "Training Epoch: 23 [22800/36045]\tLoss: 868.9894\n",
      "Training Epoch: 23 [22850/36045]\tLoss: 722.6538\n",
      "Training Epoch: 23 [22900/36045]\tLoss: 726.9064\n",
      "Training Epoch: 23 [22950/36045]\tLoss: 704.9988\n",
      "Training Epoch: 23 [23000/36045]\tLoss: 704.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [23050/36045]\tLoss: 627.5525\n",
      "Training Epoch: 23 [23100/36045]\tLoss: 643.8806\n",
      "Training Epoch: 23 [23150/36045]\tLoss: 631.9248\n",
      "Training Epoch: 23 [23200/36045]\tLoss: 598.4532\n",
      "Training Epoch: 23 [23250/36045]\tLoss: 601.3927\n",
      "Training Epoch: 23 [23300/36045]\tLoss: 598.2582\n",
      "Training Epoch: 23 [23350/36045]\tLoss: 620.1987\n",
      "Training Epoch: 23 [23400/36045]\tLoss: 671.9327\n",
      "Training Epoch: 23 [23450/36045]\tLoss: 664.0289\n",
      "Training Epoch: 23 [23500/36045]\tLoss: 640.0279\n",
      "Training Epoch: 23 [23550/36045]\tLoss: 687.1938\n",
      "Training Epoch: 23 [23600/36045]\tLoss: 774.3479\n",
      "Training Epoch: 23 [23650/36045]\tLoss: 788.2946\n",
      "Training Epoch: 23 [23700/36045]\tLoss: 798.0449\n",
      "Training Epoch: 23 [23750/36045]\tLoss: 771.4814\n",
      "Training Epoch: 23 [23800/36045]\tLoss: 613.7523\n",
      "Training Epoch: 23 [23850/36045]\tLoss: 641.0345\n",
      "Training Epoch: 23 [23900/36045]\tLoss: 630.9907\n",
      "Training Epoch: 23 [23950/36045]\tLoss: 613.2502\n",
      "Training Epoch: 23 [24000/36045]\tLoss: 589.1418\n",
      "Training Epoch: 23 [24050/36045]\tLoss: 544.6574\n",
      "Training Epoch: 23 [24100/36045]\tLoss: 573.4182\n",
      "Training Epoch: 23 [24150/36045]\tLoss: 567.9683\n",
      "Training Epoch: 23 [24200/36045]\tLoss: 563.0537\n",
      "Training Epoch: 23 [24250/36045]\tLoss: 546.5716\n",
      "Training Epoch: 23 [24300/36045]\tLoss: 589.0473\n",
      "Training Epoch: 23 [24350/36045]\tLoss: 603.6018\n",
      "Training Epoch: 23 [24400/36045]\tLoss: 620.7977\n",
      "Training Epoch: 23 [24450/36045]\tLoss: 592.4431\n",
      "Training Epoch: 23 [24500/36045]\tLoss: 623.8863\n",
      "Training Epoch: 23 [24550/36045]\tLoss: 716.5554\n",
      "Training Epoch: 23 [24600/36045]\tLoss: 708.7576\n",
      "Training Epoch: 23 [24650/36045]\tLoss: 680.5403\n",
      "Training Epoch: 23 [24700/36045]\tLoss: 691.4664\n",
      "Training Epoch: 23 [24750/36045]\tLoss: 638.9004\n",
      "Training Epoch: 23 [24800/36045]\tLoss: 532.1972\n",
      "Training Epoch: 23 [24850/36045]\tLoss: 552.3766\n",
      "Training Epoch: 23 [24900/36045]\tLoss: 548.7586\n",
      "Training Epoch: 23 [24950/36045]\tLoss: 550.9685\n",
      "Training Epoch: 23 [25000/36045]\tLoss: 529.5134\n",
      "Training Epoch: 23 [25050/36045]\tLoss: 505.7839\n",
      "Training Epoch: 23 [25100/36045]\tLoss: 453.9191\n",
      "Training Epoch: 23 [25150/36045]\tLoss: 420.7873\n",
      "Training Epoch: 23 [25200/36045]\tLoss: 415.6122\n",
      "Training Epoch: 23 [25250/36045]\tLoss: 444.9772\n",
      "Training Epoch: 23 [25300/36045]\tLoss: 583.7513\n",
      "Training Epoch: 23 [25350/36045]\tLoss: 581.6116\n",
      "Training Epoch: 23 [25400/36045]\tLoss: 541.6285\n",
      "Training Epoch: 23 [25450/36045]\tLoss: 544.6439\n",
      "Training Epoch: 23 [25500/36045]\tLoss: 591.9119\n",
      "Training Epoch: 23 [25550/36045]\tLoss: 687.2153\n",
      "Training Epoch: 23 [25600/36045]\tLoss: 692.8361\n",
      "Training Epoch: 23 [25650/36045]\tLoss: 668.5477\n",
      "Training Epoch: 23 [25700/36045]\tLoss: 678.0600\n",
      "Training Epoch: 23 [25750/36045]\tLoss: 652.9240\n",
      "Training Epoch: 23 [25800/36045]\tLoss: 410.9589\n",
      "Training Epoch: 23 [25850/36045]\tLoss: 421.3902\n",
      "Training Epoch: 23 [25900/36045]\tLoss: 402.3177\n",
      "Training Epoch: 23 [25950/36045]\tLoss: 411.4824\n",
      "Training Epoch: 23 [26000/36045]\tLoss: 503.9008\n",
      "Training Epoch: 23 [26050/36045]\tLoss: 685.1792\n",
      "Training Epoch: 23 [26100/36045]\tLoss: 713.4746\n",
      "Training Epoch: 23 [26150/36045]\tLoss: 713.3668\n",
      "Training Epoch: 23 [26200/36045]\tLoss: 686.7696\n",
      "Training Epoch: 23 [26250/36045]\tLoss: 718.4214\n",
      "Training Epoch: 23 [26300/36045]\tLoss: 642.9424\n",
      "Training Epoch: 23 [26350/36045]\tLoss: 652.8446\n",
      "Training Epoch: 23 [26400/36045]\tLoss: 630.8476\n",
      "Training Epoch: 23 [26450/36045]\tLoss: 560.2135\n",
      "Training Epoch: 23 [26500/36045]\tLoss: 668.9235\n",
      "Training Epoch: 23 [26550/36045]\tLoss: 672.3325\n",
      "Training Epoch: 23 [26600/36045]\tLoss: 667.4584\n",
      "Training Epoch: 23 [26650/36045]\tLoss: 683.8686\n",
      "Training Epoch: 23 [26700/36045]\tLoss: 663.6993\n",
      "Training Epoch: 23 [26750/36045]\tLoss: 620.9291\n",
      "Training Epoch: 23 [26800/36045]\tLoss: 456.9310\n",
      "Training Epoch: 23 [26850/36045]\tLoss: 379.7321\n",
      "Training Epoch: 23 [26900/36045]\tLoss: 383.1490\n",
      "Training Epoch: 23 [26950/36045]\tLoss: 421.3323\n",
      "Training Epoch: 23 [27000/36045]\tLoss: 680.3728\n",
      "Training Epoch: 23 [27050/36045]\tLoss: 712.7555\n",
      "Training Epoch: 23 [27100/36045]\tLoss: 690.1336\n",
      "Training Epoch: 23 [27150/36045]\tLoss: 732.0636\n",
      "Training Epoch: 23 [27200/36045]\tLoss: 538.6861\n",
      "Training Epoch: 23 [27250/36045]\tLoss: 532.5743\n",
      "Training Epoch: 23 [27300/36045]\tLoss: 518.0375\n",
      "Training Epoch: 23 [27350/36045]\tLoss: 517.4218\n",
      "Training Epoch: 23 [27400/36045]\tLoss: 516.5905\n",
      "Training Epoch: 23 [27450/36045]\tLoss: 650.7162\n",
      "Training Epoch: 23 [27500/36045]\tLoss: 698.4086\n",
      "Training Epoch: 23 [27550/36045]\tLoss: 690.6382\n",
      "Training Epoch: 23 [27600/36045]\tLoss: 702.1380\n",
      "Training Epoch: 23 [27650/36045]\tLoss: 694.0588\n",
      "Training Epoch: 23 [27700/36045]\tLoss: 724.1857\n",
      "Training Epoch: 23 [27750/36045]\tLoss: 736.6408\n",
      "Training Epoch: 23 [27800/36045]\tLoss: 722.7718\n",
      "Training Epoch: 23 [27850/36045]\tLoss: 711.0174\n",
      "Training Epoch: 23 [27900/36045]\tLoss: 639.7836\n",
      "Training Epoch: 23 [27950/36045]\tLoss: 530.4417\n",
      "Training Epoch: 23 [28000/36045]\tLoss: 505.6131\n",
      "Training Epoch: 23 [28050/36045]\tLoss: 517.2289\n",
      "Training Epoch: 23 [28100/36045]\tLoss: 508.8652\n",
      "Training Epoch: 23 [28150/36045]\tLoss: 536.9811\n",
      "Training Epoch: 23 [28200/36045]\tLoss: 542.5206\n",
      "Training Epoch: 23 [28250/36045]\tLoss: 536.7892\n",
      "Training Epoch: 23 [28300/36045]\tLoss: 508.5388\n",
      "Training Epoch: 23 [28350/36045]\tLoss: 504.5979\n",
      "Training Epoch: 23 [28400/36045]\tLoss: 840.0208\n",
      "Training Epoch: 23 [28450/36045]\tLoss: 766.4468\n",
      "Training Epoch: 23 [28500/36045]\tLoss: 663.7256\n",
      "Training Epoch: 23 [28550/36045]\tLoss: 608.9808\n",
      "Training Epoch: 23 [28600/36045]\tLoss: 647.5745\n",
      "Training Epoch: 23 [28650/36045]\tLoss: 727.2264\n",
      "Training Epoch: 23 [28700/36045]\tLoss: 721.4984\n",
      "Training Epoch: 23 [28750/36045]\tLoss: 709.0448\n",
      "Training Epoch: 23 [28800/36045]\tLoss: 717.9180\n",
      "Training Epoch: 23 [28850/36045]\tLoss: 621.3224\n",
      "Training Epoch: 23 [28900/36045]\tLoss: 500.3958\n",
      "Training Epoch: 23 [28950/36045]\tLoss: 497.8336\n",
      "Training Epoch: 23 [29000/36045]\tLoss: 496.7302\n",
      "Training Epoch: 23 [29050/36045]\tLoss: 504.9951\n",
      "Training Epoch: 23 [29100/36045]\tLoss: 525.9674\n",
      "Training Epoch: 23 [29150/36045]\tLoss: 513.0174\n",
      "Training Epoch: 23 [29200/36045]\tLoss: 498.0135\n",
      "Training Epoch: 23 [29250/36045]\tLoss: 485.5150\n",
      "Training Epoch: 23 [29300/36045]\tLoss: 554.9899\n",
      "Training Epoch: 23 [29350/36045]\tLoss: 658.7266\n",
      "Training Epoch: 23 [29400/36045]\tLoss: 677.5862\n",
      "Training Epoch: 23 [29450/36045]\tLoss: 699.4146\n",
      "Training Epoch: 23 [29500/36045]\tLoss: 713.5010\n",
      "Training Epoch: 23 [29550/36045]\tLoss: 677.8351\n",
      "Training Epoch: 23 [29600/36045]\tLoss: 573.1920\n",
      "Training Epoch: 23 [29650/36045]\tLoss: 556.4775\n",
      "Training Epoch: 23 [29700/36045]\tLoss: 496.1522\n",
      "Training Epoch: 23 [29750/36045]\tLoss: 497.3691\n",
      "Training Epoch: 23 [29800/36045]\tLoss: 543.4781\n",
      "Training Epoch: 23 [29850/36045]\tLoss: 616.4594\n",
      "Training Epoch: 23 [29900/36045]\tLoss: 612.8892\n",
      "Training Epoch: 23 [29950/36045]\tLoss: 635.4958\n",
      "Training Epoch: 23 [30000/36045]\tLoss: 612.2010\n",
      "Training Epoch: 23 [30050/36045]\tLoss: 618.5349\n",
      "Training Epoch: 23 [30100/36045]\tLoss: 754.4821\n",
      "Training Epoch: 23 [30150/36045]\tLoss: 738.9025\n",
      "Training Epoch: 23 [30200/36045]\tLoss: 696.7735\n",
      "Training Epoch: 23 [30250/36045]\tLoss: 746.2532\n",
      "Training Epoch: 23 [30300/36045]\tLoss: 732.3412\n",
      "Training Epoch: 23 [30350/36045]\tLoss: 574.5228\n",
      "Training Epoch: 23 [30400/36045]\tLoss: 560.1926\n",
      "Training Epoch: 23 [30450/36045]\tLoss: 559.5663\n",
      "Training Epoch: 23 [30500/36045]\tLoss: 523.4217\n",
      "Training Epoch: 23 [30550/36045]\tLoss: 484.9808\n",
      "Training Epoch: 23 [30600/36045]\tLoss: 471.6098\n",
      "Training Epoch: 23 [30650/36045]\tLoss: 462.4375\n",
      "Training Epoch: 23 [30700/36045]\tLoss: 480.0549\n",
      "Training Epoch: 23 [30750/36045]\tLoss: 466.6266\n",
      "Training Epoch: 23 [30800/36045]\tLoss: 495.4743\n",
      "Training Epoch: 23 [30850/36045]\tLoss: 486.8891\n",
      "Training Epoch: 23 [30900/36045]\tLoss: 500.6207\n",
      "Training Epoch: 23 [30950/36045]\tLoss: 525.8994\n",
      "Training Epoch: 23 [31000/36045]\tLoss: 517.0594\n",
      "Training Epoch: 23 [31050/36045]\tLoss: 431.5138\n",
      "Training Epoch: 23 [31100/36045]\tLoss: 422.5261\n",
      "Training Epoch: 23 [31150/36045]\tLoss: 428.6720\n",
      "Training Epoch: 23 [31200/36045]\tLoss: 535.8980\n",
      "Training Epoch: 23 [31250/36045]\tLoss: 695.0527\n",
      "Training Epoch: 23 [31300/36045]\tLoss: 664.4957\n",
      "Training Epoch: 23 [31350/36045]\tLoss: 680.5754\n",
      "Training Epoch: 23 [31400/36045]\tLoss: 659.6285\n",
      "Training Epoch: 23 [31450/36045]\tLoss: 673.9081\n",
      "Training Epoch: 23 [31500/36045]\tLoss: 685.0983\n",
      "Training Epoch: 23 [31550/36045]\tLoss: 694.3433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 23 [31600/36045]\tLoss: 652.5154\n",
      "Training Epoch: 23 [31650/36045]\tLoss: 696.5044\n",
      "Training Epoch: 23 [31700/36045]\tLoss: 507.3547\n",
      "Training Epoch: 23 [31750/36045]\tLoss: 421.2645\n",
      "Training Epoch: 23 [31800/36045]\tLoss: 401.0105\n",
      "Training Epoch: 23 [31850/36045]\tLoss: 410.7461\n",
      "Training Epoch: 23 [31900/36045]\tLoss: 639.5851\n",
      "Training Epoch: 23 [31950/36045]\tLoss: 821.8917\n",
      "Training Epoch: 23 [32000/36045]\tLoss: 936.2448\n",
      "Training Epoch: 23 [32050/36045]\tLoss: 889.5145\n",
      "Training Epoch: 23 [32100/36045]\tLoss: 878.3692\n",
      "Training Epoch: 23 [32150/36045]\tLoss: 688.0349\n",
      "Training Epoch: 23 [32200/36045]\tLoss: 692.7318\n",
      "Training Epoch: 23 [32250/36045]\tLoss: 703.8004\n",
      "Training Epoch: 23 [32300/36045]\tLoss: 685.6270\n",
      "Training Epoch: 23 [32350/36045]\tLoss: 680.1806\n",
      "Training Epoch: 23 [32400/36045]\tLoss: 638.5249\n",
      "Training Epoch: 23 [32450/36045]\tLoss: 526.5348\n",
      "Training Epoch: 23 [32500/36045]\tLoss: 506.4562\n",
      "Training Epoch: 23 [32550/36045]\tLoss: 509.2653\n",
      "Training Epoch: 23 [32600/36045]\tLoss: 505.4811\n",
      "Training Epoch: 23 [32650/36045]\tLoss: 642.7223\n",
      "Training Epoch: 23 [32700/36045]\tLoss: 700.2584\n",
      "Training Epoch: 23 [32750/36045]\tLoss: 667.3741\n",
      "Training Epoch: 23 [32800/36045]\tLoss: 684.6798\n",
      "Training Epoch: 23 [32850/36045]\tLoss: 632.5820\n",
      "Training Epoch: 23 [32900/36045]\tLoss: 510.5128\n",
      "Training Epoch: 23 [32950/36045]\tLoss: 534.0013\n",
      "Training Epoch: 23 [33000/36045]\tLoss: 533.8557\n",
      "Training Epoch: 23 [33050/36045]\tLoss: 506.3246\n",
      "Training Epoch: 23 [33100/36045]\tLoss: 576.2089\n",
      "Training Epoch: 23 [33150/36045]\tLoss: 780.1328\n",
      "Training Epoch: 23 [33200/36045]\tLoss: 760.3589\n",
      "Training Epoch: 23 [33250/36045]\tLoss: 783.1453\n",
      "Training Epoch: 23 [33300/36045]\tLoss: 833.4692\n",
      "Training Epoch: 23 [33350/36045]\tLoss: 639.9846\n",
      "Training Epoch: 23 [33400/36045]\tLoss: 472.1209\n",
      "Training Epoch: 23 [33450/36045]\tLoss: 467.1874\n",
      "Training Epoch: 23 [33500/36045]\tLoss: 480.7723\n",
      "Training Epoch: 23 [33550/36045]\tLoss: 498.9482\n",
      "Training Epoch: 23 [33600/36045]\tLoss: 500.4292\n",
      "Training Epoch: 23 [33650/36045]\tLoss: 665.8528\n",
      "Training Epoch: 23 [33700/36045]\tLoss: 644.5384\n",
      "Training Epoch: 23 [33750/36045]\tLoss: 667.4034\n",
      "Training Epoch: 23 [33800/36045]\tLoss: 662.3172\n",
      "Training Epoch: 23 [33850/36045]\tLoss: 665.4703\n",
      "Training Epoch: 23 [33900/36045]\tLoss: 675.8577\n",
      "Training Epoch: 23 [33950/36045]\tLoss: 687.6550\n",
      "Training Epoch: 23 [34000/36045]\tLoss: 675.2187\n",
      "Training Epoch: 23 [34050/36045]\tLoss: 679.6128\n",
      "Training Epoch: 23 [34100/36045]\tLoss: 653.8265\n",
      "Training Epoch: 23 [34150/36045]\tLoss: 608.5175\n",
      "Training Epoch: 23 [34200/36045]\tLoss: 576.1539\n",
      "Training Epoch: 23 [34250/36045]\tLoss: 590.4193\n",
      "Training Epoch: 23 [34300/36045]\tLoss: 506.3562\n",
      "Training Epoch: 23 [34350/36045]\tLoss: 532.6898\n",
      "Training Epoch: 23 [34400/36045]\tLoss: 523.1954\n",
      "Training Epoch: 23 [34450/36045]\tLoss: 491.4411\n",
      "Training Epoch: 23 [34500/36045]\tLoss: 524.8878\n",
      "Training Epoch: 23 [34550/36045]\tLoss: 515.3503\n",
      "Training Epoch: 23 [34600/36045]\tLoss: 516.2400\n",
      "Training Epoch: 23 [34650/36045]\tLoss: 626.1364\n",
      "Training Epoch: 23 [34700/36045]\tLoss: 662.2476\n",
      "Training Epoch: 23 [34750/36045]\tLoss: 588.7474\n",
      "Training Epoch: 23 [34800/36045]\tLoss: 671.8519\n",
      "Training Epoch: 23 [34850/36045]\tLoss: 680.6072\n",
      "Training Epoch: 23 [34900/36045]\tLoss: 754.0413\n",
      "Training Epoch: 23 [34950/36045]\tLoss: 741.6182\n",
      "Training Epoch: 23 [35000/36045]\tLoss: 742.8198\n",
      "Training Epoch: 23 [35050/36045]\tLoss: 728.4972\n",
      "Training Epoch: 23 [35100/36045]\tLoss: 609.0930\n",
      "Training Epoch: 23 [35150/36045]\tLoss: 602.2127\n",
      "Training Epoch: 23 [35200/36045]\tLoss: 512.3189\n",
      "Training Epoch: 23 [35250/36045]\tLoss: 562.1558\n",
      "Training Epoch: 23 [35300/36045]\tLoss: 576.0242\n",
      "Training Epoch: 23 [35350/36045]\tLoss: 657.5206\n",
      "Training Epoch: 23 [35400/36045]\tLoss: 695.0840\n",
      "Training Epoch: 23 [35450/36045]\tLoss: 663.5184\n",
      "Training Epoch: 23 [35500/36045]\tLoss: 645.2388\n",
      "Training Epoch: 23 [35550/36045]\tLoss: 630.0749\n",
      "Training Epoch: 23 [35600/36045]\tLoss: 677.4435\n",
      "Training Epoch: 23 [35650/36045]\tLoss: 752.2921\n",
      "Training Epoch: 23 [35700/36045]\tLoss: 679.3395\n",
      "Training Epoch: 23 [35750/36045]\tLoss: 738.5913\n",
      "Training Epoch: 23 [35800/36045]\tLoss: 744.8749\n",
      "Training Epoch: 23 [35850/36045]\tLoss: 719.2032\n",
      "Training Epoch: 23 [35900/36045]\tLoss: 746.9323\n",
      "Training Epoch: 23 [35950/36045]\tLoss: 744.7758\n",
      "Training Epoch: 23 [36000/36045]\tLoss: 735.8159\n",
      "Training Epoch: 23 [36045/36045]\tLoss: 718.1245\n",
      "Training Epoch: 23 [4004/4004]\tLoss: 670.7515\n",
      "Training Epoch: 24 [50/36045]\tLoss: 670.9047\n",
      "Training Epoch: 24 [100/36045]\tLoss: 643.3207\n",
      "Training Epoch: 24 [150/36045]\tLoss: 641.3522\n",
      "Training Epoch: 24 [200/36045]\tLoss: 628.2921\n",
      "Training Epoch: 24 [250/36045]\tLoss: 747.0662\n",
      "Training Epoch: 24 [300/36045]\tLoss: 810.5355\n",
      "Training Epoch: 24 [350/36045]\tLoss: 775.6790\n",
      "Training Epoch: 24 [400/36045]\tLoss: 772.0716\n",
      "Training Epoch: 24 [450/36045]\tLoss: 751.6894\n",
      "Training Epoch: 24 [500/36045]\tLoss: 700.6832\n",
      "Training Epoch: 24 [550/36045]\tLoss: 705.8037\n",
      "Training Epoch: 24 [600/36045]\tLoss: 684.5599\n",
      "Training Epoch: 24 [650/36045]\tLoss: 709.0345\n",
      "Training Epoch: 24 [700/36045]\tLoss: 696.5811\n",
      "Training Epoch: 24 [750/36045]\tLoss: 678.3243\n",
      "Training Epoch: 24 [800/36045]\tLoss: 693.5825\n",
      "Training Epoch: 24 [850/36045]\tLoss: 674.2239\n",
      "Training Epoch: 24 [900/36045]\tLoss: 641.4372\n",
      "Training Epoch: 24 [950/36045]\tLoss: 607.8621\n",
      "Training Epoch: 24 [1000/36045]\tLoss: 585.3099\n",
      "Training Epoch: 24 [1050/36045]\tLoss: 587.5819\n",
      "Training Epoch: 24 [1100/36045]\tLoss: 572.2899\n",
      "Training Epoch: 24 [1150/36045]\tLoss: 581.4470\n",
      "Training Epoch: 24 [1200/36045]\tLoss: 613.4973\n",
      "Training Epoch: 24 [1250/36045]\tLoss: 701.3879\n",
      "Training Epoch: 24 [1300/36045]\tLoss: 707.2017\n",
      "Training Epoch: 24 [1350/36045]\tLoss: 709.9261\n",
      "Training Epoch: 24 [1400/36045]\tLoss: 738.0018\n",
      "Training Epoch: 24 [1450/36045]\tLoss: 714.0383\n",
      "Training Epoch: 24 [1500/36045]\tLoss: 657.2453\n",
      "Training Epoch: 24 [1550/36045]\tLoss: 673.1904\n",
      "Training Epoch: 24 [1600/36045]\tLoss: 683.8743\n",
      "Training Epoch: 24 [1650/36045]\tLoss: 670.4844\n",
      "Training Epoch: 24 [1700/36045]\tLoss: 682.3285\n",
      "Training Epoch: 24 [1750/36045]\tLoss: 724.4052\n",
      "Training Epoch: 24 [1800/36045]\tLoss: 705.8047\n",
      "Training Epoch: 24 [1850/36045]\tLoss: 724.2667\n",
      "Training Epoch: 24 [1900/36045]\tLoss: 677.7081\n",
      "Training Epoch: 24 [1950/36045]\tLoss: 688.5295\n",
      "Training Epoch: 24 [2000/36045]\tLoss: 622.9236\n",
      "Training Epoch: 24 [2050/36045]\tLoss: 626.2685\n",
      "Training Epoch: 24 [2100/36045]\tLoss: 660.1619\n",
      "Training Epoch: 24 [2150/36045]\tLoss: 638.8082\n",
      "Training Epoch: 24 [2200/36045]\tLoss: 592.4484\n",
      "Training Epoch: 24 [2250/36045]\tLoss: 559.6263\n",
      "Training Epoch: 24 [2300/36045]\tLoss: 586.9041\n",
      "Training Epoch: 24 [2350/36045]\tLoss: 560.8268\n",
      "Training Epoch: 24 [2400/36045]\tLoss: 571.2733\n",
      "Training Epoch: 24 [2450/36045]\tLoss: 726.0386\n",
      "Training Epoch: 24 [2500/36045]\tLoss: 762.8932\n",
      "Training Epoch: 24 [2550/36045]\tLoss: 759.4072\n",
      "Training Epoch: 24 [2600/36045]\tLoss: 768.6356\n",
      "Training Epoch: 24 [2650/36045]\tLoss: 894.2430\n",
      "Training Epoch: 24 [2700/36045]\tLoss: 981.5284\n",
      "Training Epoch: 24 [2750/36045]\tLoss: 1054.6056\n",
      "Training Epoch: 24 [2800/36045]\tLoss: 1064.8955\n",
      "Training Epoch: 24 [2850/36045]\tLoss: 833.5866\n",
      "Training Epoch: 24 [2900/36045]\tLoss: 799.4128\n",
      "Training Epoch: 24 [2950/36045]\tLoss: 771.6886\n",
      "Training Epoch: 24 [3000/36045]\tLoss: 766.9036\n",
      "Training Epoch: 24 [3050/36045]\tLoss: 798.2640\n",
      "Training Epoch: 24 [3100/36045]\tLoss: 730.6034\n",
      "Training Epoch: 24 [3150/36045]\tLoss: 563.7274\n",
      "Training Epoch: 24 [3200/36045]\tLoss: 585.1225\n",
      "Training Epoch: 24 [3250/36045]\tLoss: 550.8147\n",
      "Training Epoch: 24 [3300/36045]\tLoss: 521.9188\n",
      "Training Epoch: 24 [3350/36045]\tLoss: 550.3571\n",
      "Training Epoch: 24 [3400/36045]\tLoss: 578.3385\n",
      "Training Epoch: 24 [3450/36045]\tLoss: 620.7087\n",
      "Training Epoch: 24 [3500/36045]\tLoss: 607.4257\n",
      "Training Epoch: 24 [3550/36045]\tLoss: 583.3776\n",
      "Training Epoch: 24 [3600/36045]\tLoss: 624.6996\n",
      "Training Epoch: 24 [3650/36045]\tLoss: 722.1308\n",
      "Training Epoch: 24 [3700/36045]\tLoss: 728.1710\n",
      "Training Epoch: 24 [3750/36045]\tLoss: 696.0634\n",
      "Training Epoch: 24 [3800/36045]\tLoss: 689.2211\n",
      "Training Epoch: 24 [3850/36045]\tLoss: 688.3564\n",
      "Training Epoch: 24 [3900/36045]\tLoss: 694.1263\n",
      "Training Epoch: 24 [3950/36045]\tLoss: 669.3403\n",
      "Training Epoch: 24 [4000/36045]\tLoss: 676.5736\n",
      "Training Epoch: 24 [4050/36045]\tLoss: 622.3351\n",
      "Training Epoch: 24 [4100/36045]\tLoss: 606.4116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [4150/36045]\tLoss: 623.8876\n",
      "Training Epoch: 24 [4200/36045]\tLoss: 618.2595\n",
      "Training Epoch: 24 [4250/36045]\tLoss: 620.4299\n",
      "Training Epoch: 24 [4300/36045]\tLoss: 639.6644\n",
      "Training Epoch: 24 [4350/36045]\tLoss: 621.4311\n",
      "Training Epoch: 24 [4400/36045]\tLoss: 595.3007\n",
      "Training Epoch: 24 [4450/36045]\tLoss: 649.9559\n",
      "Training Epoch: 24 [4500/36045]\tLoss: 694.5718\n",
      "Training Epoch: 24 [4550/36045]\tLoss: 700.4757\n",
      "Training Epoch: 24 [4600/36045]\tLoss: 723.9128\n",
      "Training Epoch: 24 [4650/36045]\tLoss: 712.7033\n",
      "Training Epoch: 24 [4700/36045]\tLoss: 659.1010\n",
      "Training Epoch: 24 [4750/36045]\tLoss: 641.6630\n",
      "Training Epoch: 24 [4800/36045]\tLoss: 668.7545\n",
      "Training Epoch: 24 [4850/36045]\tLoss: 655.0847\n",
      "Training Epoch: 24 [4900/36045]\tLoss: 636.8537\n",
      "Training Epoch: 24 [4950/36045]\tLoss: 654.9291\n",
      "Training Epoch: 24 [5000/36045]\tLoss: 686.5431\n",
      "Training Epoch: 24 [5050/36045]\tLoss: 664.9988\n",
      "Training Epoch: 24 [5100/36045]\tLoss: 675.5000\n",
      "Training Epoch: 24 [5150/36045]\tLoss: 659.8956\n",
      "Training Epoch: 24 [5200/36045]\tLoss: 657.8542\n",
      "Training Epoch: 24 [5250/36045]\tLoss: 650.7405\n",
      "Training Epoch: 24 [5300/36045]\tLoss: 651.7823\n",
      "Training Epoch: 24 [5350/36045]\tLoss: 675.4969\n",
      "Training Epoch: 24 [5400/36045]\tLoss: 649.9239\n",
      "Training Epoch: 24 [5450/36045]\tLoss: 616.4495\n",
      "Training Epoch: 24 [5500/36045]\tLoss: 646.6464\n",
      "Training Epoch: 24 [5550/36045]\tLoss: 634.4672\n",
      "Training Epoch: 24 [5600/36045]\tLoss: 719.5297\n",
      "Training Epoch: 24 [5650/36045]\tLoss: 682.0828\n",
      "Training Epoch: 24 [5700/36045]\tLoss: 640.7340\n",
      "Training Epoch: 24 [5750/36045]\tLoss: 625.7328\n",
      "Training Epoch: 24 [5800/36045]\tLoss: 660.4857\n",
      "Training Epoch: 24 [5850/36045]\tLoss: 645.3552\n",
      "Training Epoch: 24 [5900/36045]\tLoss: 743.1553\n",
      "Training Epoch: 24 [5950/36045]\tLoss: 761.2048\n",
      "Training Epoch: 24 [6000/36045]\tLoss: 745.5139\n",
      "Training Epoch: 24 [6050/36045]\tLoss: 720.7351\n",
      "Training Epoch: 24 [6100/36045]\tLoss: 725.9144\n",
      "Training Epoch: 24 [6150/36045]\tLoss: 709.5602\n",
      "Training Epoch: 24 [6200/36045]\tLoss: 710.7080\n",
      "Training Epoch: 24 [6250/36045]\tLoss: 731.7278\n",
      "Training Epoch: 24 [6300/36045]\tLoss: 743.7010\n",
      "Training Epoch: 24 [6350/36045]\tLoss: 792.5914\n",
      "Training Epoch: 24 [6400/36045]\tLoss: 660.8710\n",
      "Training Epoch: 24 [6450/36045]\tLoss: 611.5478\n",
      "Training Epoch: 24 [6500/36045]\tLoss: 623.1387\n",
      "Training Epoch: 24 [6550/36045]\tLoss: 639.9432\n",
      "Training Epoch: 24 [6600/36045]\tLoss: 640.3358\n",
      "Training Epoch: 24 [6650/36045]\tLoss: 723.1071\n",
      "Training Epoch: 24 [6700/36045]\tLoss: 756.7725\n",
      "Training Epoch: 24 [6750/36045]\tLoss: 730.5736\n",
      "Training Epoch: 24 [6800/36045]\tLoss: 733.6853\n",
      "Training Epoch: 24 [6850/36045]\tLoss: 721.6074\n",
      "Training Epoch: 24 [6900/36045]\tLoss: 642.3018\n",
      "Training Epoch: 24 [6950/36045]\tLoss: 605.7952\n",
      "Training Epoch: 24 [7000/36045]\tLoss: 644.2465\n",
      "Training Epoch: 24 [7050/36045]\tLoss: 658.4295\n",
      "Training Epoch: 24 [7100/36045]\tLoss: 657.6999\n",
      "Training Epoch: 24 [7150/36045]\tLoss: 668.9241\n",
      "Training Epoch: 24 [7200/36045]\tLoss: 673.1556\n",
      "Training Epoch: 24 [7250/36045]\tLoss: 670.5182\n",
      "Training Epoch: 24 [7300/36045]\tLoss: 657.7011\n",
      "Training Epoch: 24 [7350/36045]\tLoss: 653.0546\n",
      "Training Epoch: 24 [7400/36045]\tLoss: 590.0879\n",
      "Training Epoch: 24 [7450/36045]\tLoss: 594.2988\n",
      "Training Epoch: 24 [7500/36045]\tLoss: 588.3856\n",
      "Training Epoch: 24 [7550/36045]\tLoss: 564.1089\n",
      "Training Epoch: 24 [7600/36045]\tLoss: 627.5950\n",
      "Training Epoch: 24 [7650/36045]\tLoss: 674.2084\n",
      "Training Epoch: 24 [7700/36045]\tLoss: 642.5276\n",
      "Training Epoch: 24 [7750/36045]\tLoss: 657.1010\n",
      "Training Epoch: 24 [7800/36045]\tLoss: 645.2009\n",
      "Training Epoch: 24 [7850/36045]\tLoss: 623.0798\n",
      "Training Epoch: 24 [7900/36045]\tLoss: 657.6065\n",
      "Training Epoch: 24 [7950/36045]\tLoss: 654.6911\n",
      "Training Epoch: 24 [8000/36045]\tLoss: 672.7641\n",
      "Training Epoch: 24 [8050/36045]\tLoss: 636.0026\n",
      "Training Epoch: 24 [8100/36045]\tLoss: 661.9936\n",
      "Training Epoch: 24 [8150/36045]\tLoss: 749.2246\n",
      "Training Epoch: 24 [8200/36045]\tLoss: 735.1306\n",
      "Training Epoch: 24 [8250/36045]\tLoss: 702.2185\n",
      "Training Epoch: 24 [8300/36045]\tLoss: 764.0922\n",
      "Training Epoch: 24 [8350/36045]\tLoss: 702.7883\n",
      "Training Epoch: 24 [8400/36045]\tLoss: 630.1763\n",
      "Training Epoch: 24 [8450/36045]\tLoss: 590.7779\n",
      "Training Epoch: 24 [8500/36045]\tLoss: 627.2043\n",
      "Training Epoch: 24 [8550/36045]\tLoss: 618.4573\n",
      "Training Epoch: 24 [8600/36045]\tLoss: 611.8163\n",
      "Training Epoch: 24 [8650/36045]\tLoss: 653.5684\n",
      "Training Epoch: 24 [8700/36045]\tLoss: 690.8293\n",
      "Training Epoch: 24 [8750/36045]\tLoss: 678.1789\n",
      "Training Epoch: 24 [8800/36045]\tLoss: 683.6992\n",
      "Training Epoch: 24 [8850/36045]\tLoss: 676.7526\n",
      "Training Epoch: 24 [8900/36045]\tLoss: 610.8109\n",
      "Training Epoch: 24 [8950/36045]\tLoss: 624.3412\n",
      "Training Epoch: 24 [9000/36045]\tLoss: 640.0616\n",
      "Training Epoch: 24 [9050/36045]\tLoss: 640.4853\n",
      "Training Epoch: 24 [9100/36045]\tLoss: 659.5088\n",
      "Training Epoch: 24 [9150/36045]\tLoss: 486.8727\n",
      "Training Epoch: 24 [9200/36045]\tLoss: 365.6968\n",
      "Training Epoch: 24 [9250/36045]\tLoss: 396.5264\n",
      "Training Epoch: 24 [9300/36045]\tLoss: 408.2882\n",
      "Training Epoch: 24 [9350/36045]\tLoss: 376.0694\n",
      "Training Epoch: 24 [9400/36045]\tLoss: 736.8353\n",
      "Training Epoch: 24 [9450/36045]\tLoss: 782.6450\n",
      "Training Epoch: 24 [9500/36045]\tLoss: 769.2906\n",
      "Training Epoch: 24 [9550/36045]\tLoss: 813.5175\n",
      "Training Epoch: 24 [9600/36045]\tLoss: 604.6190\n",
      "Training Epoch: 24 [9650/36045]\tLoss: 608.2048\n",
      "Training Epoch: 24 [9700/36045]\tLoss: 593.3109\n",
      "Training Epoch: 24 [9750/36045]\tLoss: 593.3366\n",
      "Training Epoch: 24 [9800/36045]\tLoss: 773.7429\n",
      "Training Epoch: 24 [9850/36045]\tLoss: 817.2263\n",
      "Training Epoch: 24 [9900/36045]\tLoss: 832.0193\n",
      "Training Epoch: 24 [9950/36045]\tLoss: 810.3042\n",
      "Training Epoch: 24 [10000/36045]\tLoss: 748.3092\n",
      "Training Epoch: 24 [10050/36045]\tLoss: 619.1292\n",
      "Training Epoch: 24 [10100/36045]\tLoss: 625.5333\n",
      "Training Epoch: 24 [10150/36045]\tLoss: 635.6292\n",
      "Training Epoch: 24 [10200/36045]\tLoss: 624.5159\n",
      "Training Epoch: 24 [10250/36045]\tLoss: 744.7183\n",
      "Training Epoch: 24 [10300/36045]\tLoss: 723.1046\n",
      "Training Epoch: 24 [10350/36045]\tLoss: 760.9722\n",
      "Training Epoch: 24 [10400/36045]\tLoss: 751.5550\n",
      "Training Epoch: 24 [10450/36045]\tLoss: 703.6677\n",
      "Training Epoch: 24 [10500/36045]\tLoss: 589.9561\n",
      "Training Epoch: 24 [10550/36045]\tLoss: 585.3234\n",
      "Training Epoch: 24 [10600/36045]\tLoss: 608.7757\n",
      "Training Epoch: 24 [10650/36045]\tLoss: 614.9110\n",
      "Training Epoch: 24 [10700/36045]\tLoss: 702.3776\n",
      "Training Epoch: 24 [10750/36045]\tLoss: 765.2961\n",
      "Training Epoch: 24 [10800/36045]\tLoss: 707.4005\n",
      "Training Epoch: 24 [10850/36045]\tLoss: 749.0629\n",
      "Training Epoch: 24 [10900/36045]\tLoss: 779.4407\n",
      "Training Epoch: 24 [10950/36045]\tLoss: 577.2278\n",
      "Training Epoch: 24 [11000/36045]\tLoss: 570.9162\n",
      "Training Epoch: 24 [11050/36045]\tLoss: 610.8284\n",
      "Training Epoch: 24 [11100/36045]\tLoss: 622.8739\n",
      "Training Epoch: 24 [11150/36045]\tLoss: 675.0349\n",
      "Training Epoch: 24 [11200/36045]\tLoss: 703.4114\n",
      "Training Epoch: 24 [11250/36045]\tLoss: 715.6674\n",
      "Training Epoch: 24 [11300/36045]\tLoss: 695.6082\n",
      "Training Epoch: 24 [11350/36045]\tLoss: 692.4734\n",
      "Training Epoch: 24 [11400/36045]\tLoss: 652.7599\n",
      "Training Epoch: 24 [11450/36045]\tLoss: 619.0627\n",
      "Training Epoch: 24 [11500/36045]\tLoss: 616.6840\n",
      "Training Epoch: 24 [11550/36045]\tLoss: 629.6069\n",
      "Training Epoch: 24 [11600/36045]\tLoss: 692.8068\n",
      "Training Epoch: 24 [11650/36045]\tLoss: 745.6920\n",
      "Training Epoch: 24 [11700/36045]\tLoss: 744.7089\n",
      "Training Epoch: 24 [11750/36045]\tLoss: 764.2604\n",
      "Training Epoch: 24 [11800/36045]\tLoss: 807.4775\n",
      "Training Epoch: 24 [11850/36045]\tLoss: 863.6928\n",
      "Training Epoch: 24 [11900/36045]\tLoss: 1082.4175\n",
      "Training Epoch: 24 [11950/36045]\tLoss: 1083.9193\n",
      "Training Epoch: 24 [12000/36045]\tLoss: 1098.4342\n",
      "Training Epoch: 24 [12050/36045]\tLoss: 1056.1853\n",
      "Training Epoch: 24 [12100/36045]\tLoss: 693.0893\n",
      "Training Epoch: 24 [12150/36045]\tLoss: 533.5634\n",
      "Training Epoch: 24 [12200/36045]\tLoss: 527.9686\n",
      "Training Epoch: 24 [12250/36045]\tLoss: 537.3632\n",
      "Training Epoch: 24 [12300/36045]\tLoss: 683.0132\n",
      "Training Epoch: 24 [12350/36045]\tLoss: 741.6355\n",
      "Training Epoch: 24 [12400/36045]\tLoss: 749.7748\n",
      "Training Epoch: 24 [12450/36045]\tLoss: 737.2789\n",
      "Training Epoch: 24 [12500/36045]\tLoss: 766.7988\n",
      "Training Epoch: 24 [12550/36045]\tLoss: 735.0257\n",
      "Training Epoch: 24 [12600/36045]\tLoss: 677.9829\n",
      "Training Epoch: 24 [12650/36045]\tLoss: 676.8537\n",
      "Training Epoch: 24 [12700/36045]\tLoss: 698.5530\n",
      "Training Epoch: 24 [12750/36045]\tLoss: 698.1883\n",
      "Training Epoch: 24 [12800/36045]\tLoss: 680.2819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [12850/36045]\tLoss: 710.5732\n",
      "Training Epoch: 24 [12900/36045]\tLoss: 681.7434\n",
      "Training Epoch: 24 [12950/36045]\tLoss: 669.5001\n",
      "Training Epoch: 24 [13000/36045]\tLoss: 701.8204\n",
      "Training Epoch: 24 [13050/36045]\tLoss: 638.2216\n",
      "Training Epoch: 24 [13100/36045]\tLoss: 658.6349\n",
      "Training Epoch: 24 [13150/36045]\tLoss: 650.3305\n",
      "Training Epoch: 24 [13200/36045]\tLoss: 629.0667\n",
      "Training Epoch: 24 [13250/36045]\tLoss: 655.4625\n",
      "Training Epoch: 24 [13300/36045]\tLoss: 695.7620\n",
      "Training Epoch: 24 [13350/36045]\tLoss: 674.9060\n",
      "Training Epoch: 24 [13400/36045]\tLoss: 678.9525\n",
      "Training Epoch: 24 [13450/36045]\tLoss: 674.3932\n",
      "Training Epoch: 24 [13500/36045]\tLoss: 696.5641\n",
      "Training Epoch: 24 [13550/36045]\tLoss: 832.3692\n",
      "Training Epoch: 24 [13600/36045]\tLoss: 865.1259\n",
      "Training Epoch: 24 [13650/36045]\tLoss: 945.2943\n",
      "Training Epoch: 24 [13700/36045]\tLoss: 837.3825\n",
      "Training Epoch: 24 [13750/36045]\tLoss: 681.9197\n",
      "Training Epoch: 24 [13800/36045]\tLoss: 654.8527\n",
      "Training Epoch: 24 [13850/36045]\tLoss: 637.8836\n",
      "Training Epoch: 24 [13900/36045]\tLoss: 645.3475\n",
      "Training Epoch: 24 [13950/36045]\tLoss: 691.1353\n",
      "Training Epoch: 24 [14000/36045]\tLoss: 725.7753\n",
      "Training Epoch: 24 [14050/36045]\tLoss: 698.5387\n",
      "Training Epoch: 24 [14100/36045]\tLoss: 694.3638\n",
      "Training Epoch: 24 [14150/36045]\tLoss: 682.1681\n",
      "Training Epoch: 24 [14200/36045]\tLoss: 725.5976\n",
      "Training Epoch: 24 [14250/36045]\tLoss: 795.0495\n",
      "Training Epoch: 24 [14300/36045]\tLoss: 798.4048\n",
      "Training Epoch: 24 [14350/36045]\tLoss: 764.5244\n",
      "Training Epoch: 24 [14400/36045]\tLoss: 749.9804\n",
      "Training Epoch: 24 [14450/36045]\tLoss: 787.8771\n",
      "Training Epoch: 24 [14500/36045]\tLoss: 718.5782\n",
      "Training Epoch: 24 [14550/36045]\tLoss: 749.5627\n",
      "Training Epoch: 24 [14600/36045]\tLoss: 734.1387\n",
      "Training Epoch: 24 [14650/36045]\tLoss: 734.9435\n",
      "Training Epoch: 24 [14700/36045]\tLoss: 693.9156\n",
      "Training Epoch: 24 [14750/36045]\tLoss: 595.6363\n",
      "Training Epoch: 24 [14800/36045]\tLoss: 585.7338\n",
      "Training Epoch: 24 [14850/36045]\tLoss: 592.9559\n",
      "Training Epoch: 24 [14900/36045]\tLoss: 586.4148\n",
      "Training Epoch: 24 [14950/36045]\tLoss: 594.2515\n",
      "Training Epoch: 24 [15000/36045]\tLoss: 609.9153\n",
      "Training Epoch: 24 [15050/36045]\tLoss: 607.9081\n",
      "Training Epoch: 24 [15100/36045]\tLoss: 591.7907\n",
      "Training Epoch: 24 [15150/36045]\tLoss: 584.8088\n",
      "Training Epoch: 24 [15200/36045]\tLoss: 540.7775\n",
      "Training Epoch: 24 [15250/36045]\tLoss: 564.8223\n",
      "Training Epoch: 24 [15300/36045]\tLoss: 549.3484\n",
      "Training Epoch: 24 [15350/36045]\tLoss: 562.1642\n",
      "Training Epoch: 24 [15400/36045]\tLoss: 546.9581\n",
      "Training Epoch: 24 [15450/36045]\tLoss: 533.0717\n",
      "Training Epoch: 24 [15500/36045]\tLoss: 548.6715\n",
      "Training Epoch: 24 [15550/36045]\tLoss: 543.9727\n",
      "Training Epoch: 24 [15600/36045]\tLoss: 615.2245\n",
      "Training Epoch: 24 [15650/36045]\tLoss: 634.2817\n",
      "Training Epoch: 24 [15700/36045]\tLoss: 624.4002\n",
      "Training Epoch: 24 [15750/36045]\tLoss: 616.3238\n",
      "Training Epoch: 24 [15800/36045]\tLoss: 579.5533\n",
      "Training Epoch: 24 [15850/36045]\tLoss: 593.1765\n",
      "Training Epoch: 24 [15900/36045]\tLoss: 602.9832\n",
      "Training Epoch: 24 [15950/36045]\tLoss: 623.0189\n",
      "Training Epoch: 24 [16000/36045]\tLoss: 597.5734\n",
      "Training Epoch: 24 [16050/36045]\tLoss: 567.0977\n",
      "Training Epoch: 24 [16100/36045]\tLoss: 524.4966\n",
      "Training Epoch: 24 [16150/36045]\tLoss: 511.8763\n",
      "Training Epoch: 24 [16200/36045]\tLoss: 619.1530\n",
      "Training Epoch: 24 [16250/36045]\tLoss: 648.8563\n",
      "Training Epoch: 24 [16300/36045]\tLoss: 708.5972\n",
      "Training Epoch: 24 [16350/36045]\tLoss: 726.4854\n",
      "Training Epoch: 24 [16400/36045]\tLoss: 698.8245\n",
      "Training Epoch: 24 [16450/36045]\tLoss: 679.9153\n",
      "Training Epoch: 24 [16500/36045]\tLoss: 679.2952\n",
      "Training Epoch: 24 [16550/36045]\tLoss: 643.1888\n",
      "Training Epoch: 24 [16600/36045]\tLoss: 669.0052\n",
      "Training Epoch: 24 [16650/36045]\tLoss: 688.6619\n",
      "Training Epoch: 24 [16700/36045]\tLoss: 664.9296\n",
      "Training Epoch: 24 [16750/36045]\tLoss: 656.5511\n",
      "Training Epoch: 24 [16800/36045]\tLoss: 668.2916\n",
      "Training Epoch: 24 [16850/36045]\tLoss: 636.1686\n",
      "Training Epoch: 24 [16900/36045]\tLoss: 646.9060\n",
      "Training Epoch: 24 [16950/36045]\tLoss: 673.0432\n",
      "Training Epoch: 24 [17000/36045]\tLoss: 654.7579\n",
      "Training Epoch: 24 [17050/36045]\tLoss: 683.4189\n",
      "Training Epoch: 24 [17100/36045]\tLoss: 680.5840\n",
      "Training Epoch: 24 [17150/36045]\tLoss: 591.9440\n",
      "Training Epoch: 24 [17200/36045]\tLoss: 551.5287\n",
      "Training Epoch: 24 [17250/36045]\tLoss: 577.0787\n",
      "Training Epoch: 24 [17300/36045]\tLoss: 609.9903\n",
      "Training Epoch: 24 [17350/36045]\tLoss: 585.7067\n",
      "Training Epoch: 24 [17400/36045]\tLoss: 605.2945\n",
      "Training Epoch: 24 [17450/36045]\tLoss: 625.9729\n",
      "Training Epoch: 24 [17500/36045]\tLoss: 613.5743\n",
      "Training Epoch: 24 [17550/36045]\tLoss: 613.6781\n",
      "Training Epoch: 24 [17600/36045]\tLoss: 605.6144\n",
      "Training Epoch: 24 [17650/36045]\tLoss: 623.1476\n",
      "Training Epoch: 24 [17700/36045]\tLoss: 601.5953\n",
      "Training Epoch: 24 [17750/36045]\tLoss: 618.7411\n",
      "Training Epoch: 24 [17800/36045]\tLoss: 609.2184\n",
      "Training Epoch: 24 [17850/36045]\tLoss: 616.3235\n",
      "Training Epoch: 24 [17900/36045]\tLoss: 645.5301\n",
      "Training Epoch: 24 [17950/36045]\tLoss: 656.8859\n",
      "Training Epoch: 24 [18000/36045]\tLoss: 646.8552\n",
      "Training Epoch: 24 [18050/36045]\tLoss: 719.2646\n",
      "Training Epoch: 24 [18100/36045]\tLoss: 721.8860\n",
      "Training Epoch: 24 [18150/36045]\tLoss: 732.4725\n",
      "Training Epoch: 24 [18200/36045]\tLoss: 714.9039\n",
      "Training Epoch: 24 [18250/36045]\tLoss: 736.0228\n",
      "Training Epoch: 24 [18300/36045]\tLoss: 682.0709\n",
      "Training Epoch: 24 [18350/36045]\tLoss: 751.8184\n",
      "Training Epoch: 24 [18400/36045]\tLoss: 724.6494\n",
      "Training Epoch: 24 [18450/36045]\tLoss: 705.0899\n",
      "Training Epoch: 24 [18500/36045]\tLoss: 704.3552\n",
      "Training Epoch: 24 [18550/36045]\tLoss: 691.0607\n",
      "Training Epoch: 24 [18600/36045]\tLoss: 680.7010\n",
      "Training Epoch: 24 [18650/36045]\tLoss: 728.3833\n",
      "Training Epoch: 24 [18700/36045]\tLoss: 766.4135\n",
      "Training Epoch: 24 [18750/36045]\tLoss: 751.8135\n",
      "Training Epoch: 24 [18800/36045]\tLoss: 776.3239\n",
      "Training Epoch: 24 [18850/36045]\tLoss: 720.3038\n",
      "Training Epoch: 24 [18900/36045]\tLoss: 770.5894\n",
      "Training Epoch: 24 [18950/36045]\tLoss: 710.9058\n",
      "Training Epoch: 24 [19000/36045]\tLoss: 599.5444\n",
      "Training Epoch: 24 [19050/36045]\tLoss: 580.7562\n",
      "Training Epoch: 24 [19100/36045]\tLoss: 590.4858\n",
      "Training Epoch: 24 [19150/36045]\tLoss: 579.2972\n",
      "Training Epoch: 24 [19200/36045]\tLoss: 606.9316\n",
      "Training Epoch: 24 [19250/36045]\tLoss: 621.4915\n",
      "Training Epoch: 24 [19300/36045]\tLoss: 632.5310\n",
      "Training Epoch: 24 [19350/36045]\tLoss: 616.1238\n",
      "Training Epoch: 24 [19400/36045]\tLoss: 638.8994\n",
      "Training Epoch: 24 [19450/36045]\tLoss: 629.1899\n",
      "Training Epoch: 24 [19500/36045]\tLoss: 631.2769\n",
      "Training Epoch: 24 [19550/36045]\tLoss: 629.6141\n",
      "Training Epoch: 24 [19600/36045]\tLoss: 671.2919\n",
      "Training Epoch: 24 [19650/36045]\tLoss: 883.1099\n",
      "Training Epoch: 24 [19700/36045]\tLoss: 841.9349\n",
      "Training Epoch: 24 [19750/36045]\tLoss: 844.1545\n",
      "Training Epoch: 24 [19800/36045]\tLoss: 842.0969\n",
      "Training Epoch: 24 [19850/36045]\tLoss: 563.8348\n",
      "Training Epoch: 24 [19900/36045]\tLoss: 541.4358\n",
      "Training Epoch: 24 [19950/36045]\tLoss: 545.0352\n",
      "Training Epoch: 24 [20000/36045]\tLoss: 543.4551\n",
      "Training Epoch: 24 [20050/36045]\tLoss: 608.2417\n",
      "Training Epoch: 24 [20100/36045]\tLoss: 614.3930\n",
      "Training Epoch: 24 [20150/36045]\tLoss: 617.2451\n",
      "Training Epoch: 24 [20200/36045]\tLoss: 617.6686\n",
      "Training Epoch: 24 [20250/36045]\tLoss: 657.7006\n",
      "Training Epoch: 24 [20300/36045]\tLoss: 694.2844\n",
      "Training Epoch: 24 [20350/36045]\tLoss: 713.6868\n",
      "Training Epoch: 24 [20400/36045]\tLoss: 729.9308\n",
      "Training Epoch: 24 [20450/36045]\tLoss: 700.9993\n",
      "Training Epoch: 24 [20500/36045]\tLoss: 684.6895\n",
      "Training Epoch: 24 [20550/36045]\tLoss: 603.0430\n",
      "Training Epoch: 24 [20600/36045]\tLoss: 614.7939\n",
      "Training Epoch: 24 [20650/36045]\tLoss: 611.0260\n",
      "Training Epoch: 24 [20700/36045]\tLoss: 598.4808\n",
      "Training Epoch: 24 [20750/36045]\tLoss: 643.1649\n",
      "Training Epoch: 24 [20800/36045]\tLoss: 699.7519\n",
      "Training Epoch: 24 [20850/36045]\tLoss: 686.7473\n",
      "Training Epoch: 24 [20900/36045]\tLoss: 733.4989\n",
      "Training Epoch: 24 [20950/36045]\tLoss: 691.9805\n",
      "Training Epoch: 24 [21000/36045]\tLoss: 652.5065\n",
      "Training Epoch: 24 [21050/36045]\tLoss: 559.4064\n",
      "Training Epoch: 24 [21100/36045]\tLoss: 562.2571\n",
      "Training Epoch: 24 [21150/36045]\tLoss: 601.6748\n",
      "Training Epoch: 24 [21200/36045]\tLoss: 600.9081\n",
      "Training Epoch: 24 [21250/36045]\tLoss: 575.0013\n",
      "Training Epoch: 24 [21300/36045]\tLoss: 671.0760\n",
      "Training Epoch: 24 [21350/36045]\tLoss: 664.1151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [21400/36045]\tLoss: 667.4714\n",
      "Training Epoch: 24 [21450/36045]\tLoss: 673.9685\n",
      "Training Epoch: 24 [21500/36045]\tLoss: 677.3475\n",
      "Training Epoch: 24 [21550/36045]\tLoss: 773.2343\n",
      "Training Epoch: 24 [21600/36045]\tLoss: 773.4440\n",
      "Training Epoch: 24 [21650/36045]\tLoss: 786.7105\n",
      "Training Epoch: 24 [21700/36045]\tLoss: 787.2458\n",
      "Training Epoch: 24 [21750/36045]\tLoss: 757.3031\n",
      "Training Epoch: 24 [21800/36045]\tLoss: 560.8203\n",
      "Training Epoch: 24 [21850/36045]\tLoss: 543.4891\n",
      "Training Epoch: 24 [21900/36045]\tLoss: 554.7646\n",
      "Training Epoch: 24 [21950/36045]\tLoss: 553.3365\n",
      "Training Epoch: 24 [22000/36045]\tLoss: 557.7567\n",
      "Training Epoch: 24 [22050/36045]\tLoss: 583.2080\n",
      "Training Epoch: 24 [22100/36045]\tLoss: 575.3021\n",
      "Training Epoch: 24 [22150/36045]\tLoss: 559.1729\n",
      "Training Epoch: 24 [22200/36045]\tLoss: 577.2682\n",
      "Training Epoch: 24 [22250/36045]\tLoss: 582.5792\n",
      "Training Epoch: 24 [22300/36045]\tLoss: 636.2686\n",
      "Training Epoch: 24 [22350/36045]\tLoss: 662.7561\n",
      "Training Epoch: 24 [22400/36045]\tLoss: 678.1749\n",
      "Training Epoch: 24 [22450/36045]\tLoss: 665.9644\n",
      "Training Epoch: 24 [22500/36045]\tLoss: 647.1418\n",
      "Training Epoch: 24 [22550/36045]\tLoss: 685.0200\n",
      "Training Epoch: 24 [22600/36045]\tLoss: 744.4885\n",
      "Training Epoch: 24 [22650/36045]\tLoss: 781.2847\n",
      "Training Epoch: 24 [22700/36045]\tLoss: 804.7220\n",
      "Training Epoch: 24 [22750/36045]\tLoss: 825.3963\n",
      "Training Epoch: 24 [22800/36045]\tLoss: 858.4319\n",
      "Training Epoch: 24 [22850/36045]\tLoss: 713.9828\n",
      "Training Epoch: 24 [22900/36045]\tLoss: 718.3101\n",
      "Training Epoch: 24 [22950/36045]\tLoss: 696.6790\n",
      "Training Epoch: 24 [23000/36045]\tLoss: 695.4276\n",
      "Training Epoch: 24 [23050/36045]\tLoss: 619.5648\n",
      "Training Epoch: 24 [23100/36045]\tLoss: 635.8461\n",
      "Training Epoch: 24 [23150/36045]\tLoss: 623.9002\n",
      "Training Epoch: 24 [23200/36045]\tLoss: 590.8997\n",
      "Training Epoch: 24 [23250/36045]\tLoss: 593.7977\n",
      "Training Epoch: 24 [23300/36045]\tLoss: 590.6664\n",
      "Training Epoch: 24 [23350/36045]\tLoss: 612.4902\n",
      "Training Epoch: 24 [23400/36045]\tLoss: 663.5490\n",
      "Training Epoch: 24 [23450/36045]\tLoss: 655.8146\n",
      "Training Epoch: 24 [23500/36045]\tLoss: 632.1231\n",
      "Training Epoch: 24 [23550/36045]\tLoss: 678.5903\n",
      "Training Epoch: 24 [23600/36045]\tLoss: 764.9426\n",
      "Training Epoch: 24 [23650/36045]\tLoss: 778.7761\n",
      "Training Epoch: 24 [23700/36045]\tLoss: 788.2744\n",
      "Training Epoch: 24 [23750/36045]\tLoss: 762.0320\n",
      "Training Epoch: 24 [23800/36045]\tLoss: 606.6810\n",
      "Training Epoch: 24 [23850/36045]\tLoss: 633.8184\n",
      "Training Epoch: 24 [23900/36045]\tLoss: 623.6763\n",
      "Training Epoch: 24 [23950/36045]\tLoss: 606.0749\n",
      "Training Epoch: 24 [24000/36045]\tLoss: 582.0613\n",
      "Training Epoch: 24 [24050/36045]\tLoss: 538.0692\n",
      "Training Epoch: 24 [24100/36045]\tLoss: 566.4381\n",
      "Training Epoch: 24 [24150/36045]\tLoss: 560.7739\n",
      "Training Epoch: 24 [24200/36045]\tLoss: 556.1083\n",
      "Training Epoch: 24 [24250/36045]\tLoss: 539.7848\n",
      "Training Epoch: 24 [24300/36045]\tLoss: 581.9313\n",
      "Training Epoch: 24 [24350/36045]\tLoss: 596.3101\n",
      "Training Epoch: 24 [24400/36045]\tLoss: 613.2490\n",
      "Training Epoch: 24 [24450/36045]\tLoss: 585.1867\n",
      "Training Epoch: 24 [24500/36045]\tLoss: 616.2969\n",
      "Training Epoch: 24 [24550/36045]\tLoss: 708.4888\n",
      "Training Epoch: 24 [24600/36045]\tLoss: 700.6144\n",
      "Training Epoch: 24 [24650/36045]\tLoss: 672.6368\n",
      "Training Epoch: 24 [24700/36045]\tLoss: 683.3965\n",
      "Training Epoch: 24 [24750/36045]\tLoss: 631.4243\n",
      "Training Epoch: 24 [24800/36045]\tLoss: 525.2499\n",
      "Training Epoch: 24 [24850/36045]\tLoss: 545.2935\n",
      "Training Epoch: 24 [24900/36045]\tLoss: 541.7864\n",
      "Training Epoch: 24 [24950/36045]\tLoss: 544.0822\n",
      "Training Epoch: 24 [25000/36045]\tLoss: 522.9890\n",
      "Training Epoch: 24 [25050/36045]\tLoss: 499.5005\n",
      "Training Epoch: 24 [25100/36045]\tLoss: 448.1529\n",
      "Training Epoch: 24 [25150/36045]\tLoss: 415.3581\n",
      "Training Epoch: 24 [25200/36045]\tLoss: 410.1964\n",
      "Training Epoch: 24 [25250/36045]\tLoss: 439.2978\n",
      "Training Epoch: 24 [25300/36045]\tLoss: 576.4525\n",
      "Training Epoch: 24 [25350/36045]\tLoss: 574.1757\n",
      "Training Epoch: 24 [25400/36045]\tLoss: 534.7215\n",
      "Training Epoch: 24 [25450/36045]\tLoss: 537.6802\n",
      "Training Epoch: 24 [25500/36045]\tLoss: 584.2772\n",
      "Training Epoch: 24 [25550/36045]\tLoss: 679.0648\n",
      "Training Epoch: 24 [25600/36045]\tLoss: 684.5399\n",
      "Training Epoch: 24 [25650/36045]\tLoss: 660.6042\n",
      "Training Epoch: 24 [25700/36045]\tLoss: 669.9910\n",
      "Training Epoch: 24 [25750/36045]\tLoss: 645.1368\n",
      "Training Epoch: 24 [25800/36045]\tLoss: 406.1313\n",
      "Training Epoch: 24 [25850/36045]\tLoss: 416.5167\n",
      "Training Epoch: 24 [25900/36045]\tLoss: 397.5077\n",
      "Training Epoch: 24 [25950/36045]\tLoss: 406.6032\n",
      "Training Epoch: 24 [26000/36045]\tLoss: 497.9234\n",
      "Training Epoch: 24 [26050/36045]\tLoss: 677.1823\n",
      "Training Epoch: 24 [26100/36045]\tLoss: 705.3710\n",
      "Training Epoch: 24 [26150/36045]\tLoss: 705.4108\n",
      "Training Epoch: 24 [26200/36045]\tLoss: 678.8154\n",
      "Training Epoch: 24 [26250/36045]\tLoss: 710.1970\n",
      "Training Epoch: 24 [26300/36045]\tLoss: 636.5416\n",
      "Training Epoch: 24 [26350/36045]\tLoss: 646.5679\n",
      "Training Epoch: 24 [26400/36045]\tLoss: 624.4814\n",
      "Training Epoch: 24 [26450/36045]\tLoss: 553.9560\n",
      "Training Epoch: 24 [26500/36045]\tLoss: 660.9976\n",
      "Training Epoch: 24 [26550/36045]\tLoss: 663.9781\n",
      "Training Epoch: 24 [26600/36045]\tLoss: 659.2294\n",
      "Training Epoch: 24 [26650/36045]\tLoss: 675.6147\n",
      "Training Epoch: 24 [26700/36045]\tLoss: 655.4349\n",
      "Training Epoch: 24 [26750/36045]\tLoss: 613.3108\n",
      "Training Epoch: 24 [26800/36045]\tLoss: 451.3940\n",
      "Training Epoch: 24 [26850/36045]\tLoss: 374.9670\n",
      "Training Epoch: 24 [26900/36045]\tLoss: 378.3495\n",
      "Training Epoch: 24 [26950/36045]\tLoss: 416.0394\n",
      "Training Epoch: 24 [27000/36045]\tLoss: 672.8383\n",
      "Training Epoch: 24 [27050/36045]\tLoss: 704.5423\n",
      "Training Epoch: 24 [27100/36045]\tLoss: 682.2256\n",
      "Training Epoch: 24 [27150/36045]\tLoss: 723.9017\n",
      "Training Epoch: 24 [27200/36045]\tLoss: 532.2221\n",
      "Training Epoch: 24 [27250/36045]\tLoss: 525.8342\n",
      "Training Epoch: 24 [27300/36045]\tLoss: 511.5473\n",
      "Training Epoch: 24 [27350/36045]\tLoss: 510.8383\n",
      "Training Epoch: 24 [27400/36045]\tLoss: 509.9658\n",
      "Training Epoch: 24 [27450/36045]\tLoss: 642.6410\n",
      "Training Epoch: 24 [27500/36045]\tLoss: 689.6070\n",
      "Training Epoch: 24 [27550/36045]\tLoss: 681.8547\n",
      "Training Epoch: 24 [27600/36045]\tLoss: 693.4712\n",
      "Training Epoch: 24 [27650/36045]\tLoss: 685.2967\n",
      "Training Epoch: 24 [27700/36045]\tLoss: 715.3219\n",
      "Training Epoch: 24 [27750/36045]\tLoss: 727.7599\n",
      "Training Epoch: 24 [27800/36045]\tLoss: 713.9314\n",
      "Training Epoch: 24 [27850/36045]\tLoss: 702.4590\n",
      "Training Epoch: 24 [27900/36045]\tLoss: 632.4576\n",
      "Training Epoch: 24 [27950/36045]\tLoss: 524.5945\n",
      "Training Epoch: 24 [28000/36045]\tLoss: 500.0072\n",
      "Training Epoch: 24 [28050/36045]\tLoss: 511.3688\n",
      "Training Epoch: 24 [28100/36045]\tLoss: 503.0453\n",
      "Training Epoch: 24 [28150/36045]\tLoss: 530.4733\n",
      "Training Epoch: 24 [28200/36045]\tLoss: 536.2300\n",
      "Training Epoch: 24 [28250/36045]\tLoss: 530.2500\n",
      "Training Epoch: 24 [28300/36045]\tLoss: 502.5161\n",
      "Training Epoch: 24 [28350/36045]\tLoss: 498.6371\n",
      "Training Epoch: 24 [28400/36045]\tLoss: 833.4200\n",
      "Training Epoch: 24 [28450/36045]\tLoss: 760.9119\n",
      "Training Epoch: 24 [28500/36045]\tLoss: 658.6422\n",
      "Training Epoch: 24 [28550/36045]\tLoss: 604.3110\n",
      "Training Epoch: 24 [28600/36045]\tLoss: 641.6404\n",
      "Training Epoch: 24 [28650/36045]\tLoss: 719.1371\n",
      "Training Epoch: 24 [28700/36045]\tLoss: 713.5490\n",
      "Training Epoch: 24 [28750/36045]\tLoss: 700.8529\n",
      "Training Epoch: 24 [28800/36045]\tLoss: 709.6178\n",
      "Training Epoch: 24 [28850/36045]\tLoss: 614.1880\n",
      "Training Epoch: 24 [28900/36045]\tLoss: 495.1711\n",
      "Training Epoch: 24 [28950/36045]\tLoss: 492.9151\n",
      "Training Epoch: 24 [29000/36045]\tLoss: 491.6457\n",
      "Training Epoch: 24 [29050/36045]\tLoss: 499.7071\n",
      "Training Epoch: 24 [29100/36045]\tLoss: 520.3553\n",
      "Training Epoch: 24 [29150/36045]\tLoss: 507.5520\n",
      "Training Epoch: 24 [29200/36045]\tLoss: 492.5448\n",
      "Training Epoch: 24 [29250/36045]\tLoss: 480.4031\n",
      "Training Epoch: 24 [29300/36045]\tLoss: 548.6577\n",
      "Training Epoch: 24 [29350/36045]\tLoss: 650.7158\n",
      "Training Epoch: 24 [29400/36045]\tLoss: 669.4163\n",
      "Training Epoch: 24 [29450/36045]\tLoss: 690.5216\n",
      "Training Epoch: 24 [29500/36045]\tLoss: 704.6067\n",
      "Training Epoch: 24 [29550/36045]\tLoss: 669.5405\n",
      "Training Epoch: 24 [29600/36045]\tLoss: 566.0341\n",
      "Training Epoch: 24 [29650/36045]\tLoss: 549.3272\n",
      "Training Epoch: 24 [29700/36045]\tLoss: 489.9275\n",
      "Training Epoch: 24 [29750/36045]\tLoss: 490.7310\n",
      "Training Epoch: 24 [29800/36045]\tLoss: 536.8361\n",
      "Training Epoch: 24 [29850/36045]\tLoss: 610.1845\n",
      "Training Epoch: 24 [29900/36045]\tLoss: 606.7630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 24 [29950/36045]\tLoss: 629.3336\n",
      "Training Epoch: 24 [30000/36045]\tLoss: 605.7888\n",
      "Training Epoch: 24 [30050/36045]\tLoss: 612.0172\n",
      "Training Epoch: 24 [30100/36045]\tLoss: 746.5179\n",
      "Training Epoch: 24 [30150/36045]\tLoss: 730.7668\n",
      "Training Epoch: 24 [30200/36045]\tLoss: 689.1454\n",
      "Training Epoch: 24 [30250/36045]\tLoss: 738.5981\n",
      "Training Epoch: 24 [30300/36045]\tLoss: 724.5119\n",
      "Training Epoch: 24 [30350/36045]\tLoss: 567.3014\n",
      "Training Epoch: 24 [30400/36045]\tLoss: 552.6672\n",
      "Training Epoch: 24 [30450/36045]\tLoss: 552.3202\n",
      "Training Epoch: 24 [30500/36045]\tLoss: 516.6215\n",
      "Training Epoch: 24 [30550/36045]\tLoss: 478.5885\n",
      "Training Epoch: 24 [30600/36045]\tLoss: 466.0409\n",
      "Training Epoch: 24 [30650/36045]\tLoss: 456.6485\n",
      "Training Epoch: 24 [30700/36045]\tLoss: 474.3052\n",
      "Training Epoch: 24 [30750/36045]\tLoss: 460.9301\n",
      "Training Epoch: 24 [30800/36045]\tLoss: 489.2839\n",
      "Training Epoch: 24 [30850/36045]\tLoss: 480.8501\n",
      "Training Epoch: 24 [30900/36045]\tLoss: 494.3176\n",
      "Training Epoch: 24 [30950/36045]\tLoss: 519.3719\n",
      "Training Epoch: 24 [31000/36045]\tLoss: 510.5822\n",
      "Training Epoch: 24 [31050/36045]\tLoss: 426.2498\n",
      "Training Epoch: 24 [31100/36045]\tLoss: 417.1743\n",
      "Training Epoch: 24 [31150/36045]\tLoss: 423.4807\n",
      "Training Epoch: 24 [31200/36045]\tLoss: 529.0593\n",
      "Training Epoch: 24 [31250/36045]\tLoss: 686.1896\n",
      "Training Epoch: 24 [31300/36045]\tLoss: 655.8779\n",
      "Training Epoch: 24 [31350/36045]\tLoss: 671.8313\n",
      "Training Epoch: 24 [31400/36045]\tLoss: 650.9504\n",
      "Training Epoch: 24 [31450/36045]\tLoss: 665.5945\n",
      "Training Epoch: 24 [31500/36045]\tLoss: 676.9951\n",
      "Training Epoch: 24 [31550/36045]\tLoss: 686.0317\n",
      "Training Epoch: 24 [31600/36045]\tLoss: 644.6854\n",
      "Training Epoch: 24 [31650/36045]\tLoss: 688.3582\n",
      "Training Epoch: 24 [31700/36045]\tLoss: 500.9737\n",
      "Training Epoch: 24 [31750/36045]\tLoss: 415.7307\n",
      "Training Epoch: 24 [31800/36045]\tLoss: 395.8212\n",
      "Training Epoch: 24 [31850/36045]\tLoss: 405.4185\n",
      "Training Epoch: 24 [31900/36045]\tLoss: 632.0903\n",
      "Training Epoch: 24 [31950/36045]\tLoss: 812.9617\n",
      "Training Epoch: 24 [32000/36045]\tLoss: 926.5644\n",
      "Training Epoch: 24 [32050/36045]\tLoss: 880.0330\n",
      "Training Epoch: 24 [32100/36045]\tLoss: 869.0835\n",
      "Training Epoch: 24 [32150/36045]\tLoss: 679.8218\n",
      "Training Epoch: 24 [32200/36045]\tLoss: 684.3692\n",
      "Training Epoch: 24 [32250/36045]\tLoss: 695.3663\n",
      "Training Epoch: 24 [32300/36045]\tLoss: 677.1624\n",
      "Training Epoch: 24 [32350/36045]\tLoss: 671.8272\n",
      "Training Epoch: 24 [32400/36045]\tLoss: 630.6084\n",
      "Training Epoch: 24 [32450/36045]\tLoss: 519.9080\n",
      "Training Epoch: 24 [32500/36045]\tLoss: 500.0320\n",
      "Training Epoch: 24 [32550/36045]\tLoss: 502.8082\n",
      "Training Epoch: 24 [32600/36045]\tLoss: 499.0873\n",
      "Training Epoch: 24 [32650/36045]\tLoss: 635.6477\n",
      "Training Epoch: 24 [32700/36045]\tLoss: 692.6910\n",
      "Training Epoch: 24 [32750/36045]\tLoss: 660.1976\n",
      "Training Epoch: 24 [32800/36045]\tLoss: 677.3265\n",
      "Training Epoch: 24 [32850/36045]\tLoss: 625.6579\n",
      "Training Epoch: 24 [32900/36045]\tLoss: 504.4766\n",
      "Training Epoch: 24 [32950/36045]\tLoss: 527.7095\n",
      "Training Epoch: 24 [33000/36045]\tLoss: 527.5260\n",
      "Training Epoch: 24 [33050/36045]\tLoss: 500.4306\n",
      "Training Epoch: 24 [33100/36045]\tLoss: 569.3480\n",
      "Training Epoch: 24 [33150/36045]\tLoss: 771.1836\n",
      "Training Epoch: 24 [33200/36045]\tLoss: 751.5442\n",
      "Training Epoch: 24 [33250/36045]\tLoss: 774.1448\n",
      "Training Epoch: 24 [33300/36045]\tLoss: 823.9120\n",
      "Training Epoch: 24 [33350/36045]\tLoss: 632.5740\n",
      "Training Epoch: 24 [33400/36045]\tLoss: 466.2366\n",
      "Training Epoch: 24 [33450/36045]\tLoss: 461.3299\n",
      "Training Epoch: 24 [33500/36045]\tLoss: 474.7905\n",
      "Training Epoch: 24 [33550/36045]\tLoss: 492.6534\n",
      "Training Epoch: 24 [33600/36045]\tLoss: 494.1406\n",
      "Training Epoch: 24 [33650/36045]\tLoss: 657.7490\n",
      "Training Epoch: 24 [33700/36045]\tLoss: 636.6503\n",
      "Training Epoch: 24 [33750/36045]\tLoss: 659.2817\n",
      "Training Epoch: 24 [33800/36045]\tLoss: 654.3062\n",
      "Training Epoch: 24 [33850/36045]\tLoss: 657.3466\n",
      "Training Epoch: 24 [33900/36045]\tLoss: 668.0984\n",
      "Training Epoch: 24 [33950/36045]\tLoss: 679.7660\n",
      "Training Epoch: 24 [34000/36045]\tLoss: 667.1204\n",
      "Training Epoch: 24 [34050/36045]\tLoss: 671.5176\n",
      "Training Epoch: 24 [34100/36045]\tLoss: 646.1251\n",
      "Training Epoch: 24 [34150/36045]\tLoss: 601.1555\n",
      "Training Epoch: 24 [34200/36045]\tLoss: 569.1641\n",
      "Training Epoch: 24 [34250/36045]\tLoss: 583.4677\n",
      "Training Epoch: 24 [34300/36045]\tLoss: 500.1849\n",
      "Training Epoch: 24 [34350/36045]\tLoss: 526.3031\n",
      "Training Epoch: 24 [34400/36045]\tLoss: 517.0298\n",
      "Training Epoch: 24 [34450/36045]\tLoss: 485.6724\n",
      "Training Epoch: 24 [34500/36045]\tLoss: 518.6653\n",
      "Training Epoch: 24 [34550/36045]\tLoss: 509.1606\n",
      "Training Epoch: 24 [34600/36045]\tLoss: 510.6498\n",
      "Training Epoch: 24 [34650/36045]\tLoss: 620.1074\n",
      "Training Epoch: 24 [34700/36045]\tLoss: 656.0463\n",
      "Training Epoch: 24 [34750/36045]\tLoss: 583.0166\n",
      "Training Epoch: 24 [34800/36045]\tLoss: 665.6499\n",
      "Training Epoch: 24 [34850/36045]\tLoss: 674.2687\n",
      "Training Epoch: 24 [34900/36045]\tLoss: 745.4774\n",
      "Training Epoch: 24 [34950/36045]\tLoss: 732.8478\n",
      "Training Epoch: 24 [35000/36045]\tLoss: 733.9965\n",
      "Training Epoch: 24 [35050/36045]\tLoss: 719.7288\n",
      "Training Epoch: 24 [35100/36045]\tLoss: 603.2613\n",
      "Training Epoch: 24 [35150/36045]\tLoss: 596.3605\n",
      "Training Epoch: 24 [35200/36045]\tLoss: 506.9034\n",
      "Training Epoch: 24 [35250/36045]\tLoss: 556.3188\n",
      "Training Epoch: 24 [35300/36045]\tLoss: 570.2909\n",
      "Training Epoch: 24 [35350/36045]\tLoss: 649.8376\n",
      "Training Epoch: 24 [35400/36045]\tLoss: 686.7328\n",
      "Training Epoch: 24 [35450/36045]\tLoss: 655.5495\n",
      "Training Epoch: 24 [35500/36045]\tLoss: 637.2484\n",
      "Training Epoch: 24 [35550/36045]\tLoss: 622.1597\n",
      "Training Epoch: 24 [35600/36045]\tLoss: 669.6459\n",
      "Training Epoch: 24 [35650/36045]\tLoss: 744.2264\n",
      "Training Epoch: 24 [35700/36045]\tLoss: 671.3563\n",
      "Training Epoch: 24 [35750/36045]\tLoss: 730.3977\n",
      "Training Epoch: 24 [35800/36045]\tLoss: 736.7371\n",
      "Training Epoch: 24 [35850/36045]\tLoss: 710.9988\n",
      "Training Epoch: 24 [35900/36045]\tLoss: 738.2639\n",
      "Training Epoch: 24 [35950/36045]\tLoss: 736.0289\n",
      "Training Epoch: 24 [36000/36045]\tLoss: 727.3557\n",
      "Training Epoch: 24 [36045/36045]\tLoss: 709.9156\n",
      "Training Epoch: 24 [4004/4004]\tLoss: 662.7067\n",
      "Training Epoch: 25 [50/36045]\tLoss: 662.7486\n",
      "Training Epoch: 25 [100/36045]\tLoss: 635.5007\n",
      "Training Epoch: 25 [150/36045]\tLoss: 633.5167\n",
      "Training Epoch: 25 [200/36045]\tLoss: 620.4232\n",
      "Training Epoch: 25 [250/36045]\tLoss: 738.2409\n",
      "Training Epoch: 25 [300/36045]\tLoss: 801.8309\n",
      "Training Epoch: 25 [350/36045]\tLoss: 767.1829\n",
      "Training Epoch: 25 [400/36045]\tLoss: 763.3776\n",
      "Training Epoch: 25 [450/36045]\tLoss: 743.0539\n",
      "Training Epoch: 25 [500/36045]\tLoss: 692.2719\n",
      "Training Epoch: 25 [550/36045]\tLoss: 697.0317\n",
      "Training Epoch: 25 [600/36045]\tLoss: 676.4700\n",
      "Training Epoch: 25 [650/36045]\tLoss: 700.7427\n",
      "Training Epoch: 25 [700/36045]\tLoss: 688.2057\n",
      "Training Epoch: 25 [750/36045]\tLoss: 669.8097\n",
      "Training Epoch: 25 [800/36045]\tLoss: 684.6680\n",
      "Training Epoch: 25 [850/36045]\tLoss: 665.3344\n",
      "Training Epoch: 25 [900/36045]\tLoss: 633.2808\n",
      "Training Epoch: 25 [950/36045]\tLoss: 600.1260\n",
      "Training Epoch: 25 [1000/36045]\tLoss: 578.2754\n",
      "Training Epoch: 25 [1050/36045]\tLoss: 580.5653\n",
      "Training Epoch: 25 [1100/36045]\tLoss: 565.3414\n",
      "Training Epoch: 25 [1150/36045]\tLoss: 574.3736\n",
      "Training Epoch: 25 [1200/36045]\tLoss: 606.0456\n",
      "Training Epoch: 25 [1250/36045]\tLoss: 693.0511\n",
      "Training Epoch: 25 [1300/36045]\tLoss: 699.0762\n",
      "Training Epoch: 25 [1350/36045]\tLoss: 701.7822\n",
      "Training Epoch: 25 [1400/36045]\tLoss: 729.5538\n",
      "Training Epoch: 25 [1450/36045]\tLoss: 705.8065\n",
      "Training Epoch: 25 [1500/36045]\tLoss: 649.1407\n",
      "Training Epoch: 25 [1550/36045]\tLoss: 664.8807\n",
      "Training Epoch: 25 [1600/36045]\tLoss: 675.6432\n",
      "Training Epoch: 25 [1650/36045]\tLoss: 662.3849\n",
      "Training Epoch: 25 [1700/36045]\tLoss: 674.4078\n",
      "Training Epoch: 25 [1750/36045]\tLoss: 716.4344\n",
      "Training Epoch: 25 [1800/36045]\tLoss: 697.6172\n",
      "Training Epoch: 25 [1850/36045]\tLoss: 715.6917\n",
      "Training Epoch: 25 [1900/36045]\tLoss: 669.7400\n",
      "Training Epoch: 25 [1950/36045]\tLoss: 680.6666\n",
      "Training Epoch: 25 [2000/36045]\tLoss: 615.9161\n",
      "Training Epoch: 25 [2050/36045]\tLoss: 619.0783\n",
      "Training Epoch: 25 [2100/36045]\tLoss: 652.4388\n",
      "Training Epoch: 25 [2150/36045]\tLoss: 631.1383\n",
      "Training Epoch: 25 [2200/36045]\tLoss: 585.5762\n",
      "Training Epoch: 25 [2250/36045]\tLoss: 553.0915\n",
      "Training Epoch: 25 [2300/36045]\tLoss: 580.1197\n",
      "Training Epoch: 25 [2350/36045]\tLoss: 554.4008\n",
      "Training Epoch: 25 [2400/36045]\tLoss: 564.5044\n",
      "Training Epoch: 25 [2450/36045]\tLoss: 717.8129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [2500/36045]\tLoss: 754.2317\n",
      "Training Epoch: 25 [2550/36045]\tLoss: 750.8805\n",
      "Training Epoch: 25 [2600/36045]\tLoss: 760.0215\n",
      "Training Epoch: 25 [2650/36045]\tLoss: 885.6222\n",
      "Training Epoch: 25 [2700/36045]\tLoss: 972.9179\n",
      "Training Epoch: 25 [2750/36045]\tLoss: 1045.7156\n",
      "Training Epoch: 25 [2800/36045]\tLoss: 1055.8553\n",
      "Training Epoch: 25 [2850/36045]\tLoss: 824.4823\n",
      "Training Epoch: 25 [2900/36045]\tLoss: 790.0726\n",
      "Training Epoch: 25 [2950/36045]\tLoss: 762.7286\n",
      "Training Epoch: 25 [3000/36045]\tLoss: 757.7850\n",
      "Training Epoch: 25 [3050/36045]\tLoss: 789.0092\n",
      "Training Epoch: 25 [3100/36045]\tLoss: 722.0941\n",
      "Training Epoch: 25 [3150/36045]\tLoss: 557.1679\n",
      "Training Epoch: 25 [3200/36045]\tLoss: 578.2628\n",
      "Training Epoch: 25 [3250/36045]\tLoss: 544.4143\n",
      "Training Epoch: 25 [3300/36045]\tLoss: 515.7529\n",
      "Training Epoch: 25 [3350/36045]\tLoss: 543.8432\n",
      "Training Epoch: 25 [3400/36045]\tLoss: 571.3303\n",
      "Training Epoch: 25 [3450/36045]\tLoss: 613.2285\n",
      "Training Epoch: 25 [3500/36045]\tLoss: 600.1503\n",
      "Training Epoch: 25 [3550/36045]\tLoss: 576.2029\n",
      "Training Epoch: 25 [3600/36045]\tLoss: 617.1617\n",
      "Training Epoch: 25 [3650/36045]\tLoss: 713.3411\n",
      "Training Epoch: 25 [3700/36045]\tLoss: 719.4470\n",
      "Training Epoch: 25 [3750/36045]\tLoss: 687.5158\n",
      "Training Epoch: 25 [3800/36045]\tLoss: 681.0731\n",
      "Training Epoch: 25 [3850/36045]\tLoss: 680.2600\n",
      "Training Epoch: 25 [3900/36045]\tLoss: 685.9262\n",
      "Training Epoch: 25 [3950/36045]\tLoss: 661.4944\n",
      "Training Epoch: 25 [4000/36045]\tLoss: 668.4003\n",
      "Training Epoch: 25 [4050/36045]\tLoss: 614.6581\n",
      "Training Epoch: 25 [4100/36045]\tLoss: 598.9921\n",
      "Training Epoch: 25 [4150/36045]\tLoss: 616.1887\n",
      "Training Epoch: 25 [4200/36045]\tLoss: 610.6427\n",
      "Training Epoch: 25 [4250/36045]\tLoss: 612.8477\n",
      "Training Epoch: 25 [4300/36045]\tLoss: 631.8363\n",
      "Training Epoch: 25 [4350/36045]\tLoss: 613.8088\n",
      "Training Epoch: 25 [4400/36045]\tLoss: 587.9348\n",
      "Training Epoch: 25 [4450/36045]\tLoss: 642.1696\n",
      "Training Epoch: 25 [4500/36045]\tLoss: 686.5438\n",
      "Training Epoch: 25 [4550/36045]\tLoss: 692.1791\n",
      "Training Epoch: 25 [4600/36045]\tLoss: 715.5014\n",
      "Training Epoch: 25 [4650/36045]\tLoss: 704.4142\n",
      "Training Epoch: 25 [4700/36045]\tLoss: 651.3015\n",
      "Training Epoch: 25 [4750/36045]\tLoss: 633.8185\n",
      "Training Epoch: 25 [4800/36045]\tLoss: 660.6324\n",
      "Training Epoch: 25 [4850/36045]\tLoss: 646.9838\n",
      "Training Epoch: 25 [4900/36045]\tLoss: 629.0438\n",
      "Training Epoch: 25 [4950/36045]\tLoss: 646.8071\n",
      "Training Epoch: 25 [5000/36045]\tLoss: 678.2033\n",
      "Training Epoch: 25 [5050/36045]\tLoss: 656.9084\n",
      "Training Epoch: 25 [5100/36045]\tLoss: 667.4051\n",
      "Training Epoch: 25 [5150/36045]\tLoss: 651.7598\n",
      "Training Epoch: 25 [5200/36045]\tLoss: 649.6960\n",
      "Training Epoch: 25 [5250/36045]\tLoss: 642.7419\n",
      "Training Epoch: 25 [5300/36045]\tLoss: 643.7272\n",
      "Training Epoch: 25 [5350/36045]\tLoss: 667.2994\n",
      "Training Epoch: 25 [5400/36045]\tLoss: 642.1417\n",
      "Training Epoch: 25 [5450/36045]\tLoss: 609.0540\n",
      "Training Epoch: 25 [5500/36045]\tLoss: 638.9227\n",
      "Training Epoch: 25 [5550/36045]\tLoss: 626.8113\n",
      "Training Epoch: 25 [5600/36045]\tLoss: 711.3730\n",
      "Training Epoch: 25 [5650/36045]\tLoss: 674.2257\n",
      "Training Epoch: 25 [5700/36045]\tLoss: 633.2290\n",
      "Training Epoch: 25 [5750/36045]\tLoss: 618.1280\n",
      "Training Epoch: 25 [5800/36045]\tLoss: 652.3510\n",
      "Training Epoch: 25 [5850/36045]\tLoss: 637.6686\n",
      "Training Epoch: 25 [5900/36045]\tLoss: 734.1095\n",
      "Training Epoch: 25 [5950/36045]\tLoss: 752.0084\n",
      "Training Epoch: 25 [6000/36045]\tLoss: 736.5054\n",
      "Training Epoch: 25 [6050/36045]\tLoss: 712.1182\n",
      "Training Epoch: 25 [6100/36045]\tLoss: 717.1650\n",
      "Training Epoch: 25 [6150/36045]\tLoss: 701.4462\n",
      "Training Epoch: 25 [6200/36045]\tLoss: 702.7629\n",
      "Training Epoch: 25 [6250/36045]\tLoss: 723.8702\n",
      "Training Epoch: 25 [6300/36045]\tLoss: 735.7661\n",
      "Training Epoch: 25 [6350/36045]\tLoss: 784.3624\n",
      "Training Epoch: 25 [6400/36045]\tLoss: 653.4353\n",
      "Training Epoch: 25 [6450/36045]\tLoss: 604.4935\n",
      "Training Epoch: 25 [6500/36045]\tLoss: 615.8397\n",
      "Training Epoch: 25 [6550/36045]\tLoss: 632.7356\n",
      "Training Epoch: 25 [6600/36045]\tLoss: 632.7954\n",
      "Training Epoch: 25 [6650/36045]\tLoss: 714.3591\n",
      "Training Epoch: 25 [6700/36045]\tLoss: 747.5941\n",
      "Training Epoch: 25 [6750/36045]\tLoss: 721.8396\n",
      "Training Epoch: 25 [6800/36045]\tLoss: 724.9775\n",
      "Training Epoch: 25 [6850/36045]\tLoss: 712.8692\n",
      "Training Epoch: 25 [6900/36045]\tLoss: 634.6833\n",
      "Training Epoch: 25 [6950/36045]\tLoss: 598.5049\n",
      "Training Epoch: 25 [7000/36045]\tLoss: 636.5661\n",
      "Training Epoch: 25 [7050/36045]\tLoss: 650.6550\n",
      "Training Epoch: 25 [7100/36045]\tLoss: 649.8890\n",
      "Training Epoch: 25 [7150/36045]\tLoss: 661.0128\n",
      "Training Epoch: 25 [7200/36045]\tLoss: 665.1444\n",
      "Training Epoch: 25 [7250/36045]\tLoss: 662.5527\n",
      "Training Epoch: 25 [7300/36045]\tLoss: 649.7047\n",
      "Training Epoch: 25 [7350/36045]\tLoss: 645.2305\n",
      "Training Epoch: 25 [7400/36045]\tLoss: 583.6129\n",
      "Training Epoch: 25 [7450/36045]\tLoss: 587.5690\n",
      "Training Epoch: 25 [7500/36045]\tLoss: 581.8492\n",
      "Training Epoch: 25 [7550/36045]\tLoss: 557.8320\n",
      "Training Epoch: 25 [7600/36045]\tLoss: 620.2505\n",
      "Training Epoch: 25 [7650/36045]\tLoss: 666.0043\n",
      "Training Epoch: 25 [7700/36045]\tLoss: 634.5196\n",
      "Training Epoch: 25 [7750/36045]\tLoss: 649.1895\n",
      "Training Epoch: 25 [7800/36045]\tLoss: 637.2951\n",
      "Training Epoch: 25 [7850/36045]\tLoss: 615.7902\n",
      "Training Epoch: 25 [7900/36045]\tLoss: 649.8543\n",
      "Training Epoch: 25 [7950/36045]\tLoss: 646.9315\n",
      "Training Epoch: 25 [8000/36045]\tLoss: 665.0785\n",
      "Training Epoch: 25 [8050/36045]\tLoss: 628.4861\n",
      "Training Epoch: 25 [8100/36045]\tLoss: 654.4547\n",
      "Training Epoch: 25 [8150/36045]\tLoss: 740.6610\n",
      "Training Epoch: 25 [8200/36045]\tLoss: 726.6821\n",
      "Training Epoch: 25 [8250/36045]\tLoss: 693.8477\n",
      "Training Epoch: 25 [8300/36045]\tLoss: 755.2487\n",
      "Training Epoch: 25 [8350/36045]\tLoss: 694.4837\n",
      "Training Epoch: 25 [8400/36045]\tLoss: 622.6650\n",
      "Training Epoch: 25 [8450/36045]\tLoss: 583.6964\n",
      "Training Epoch: 25 [8500/36045]\tLoss: 619.7212\n",
      "Training Epoch: 25 [8550/36045]\tLoss: 611.1702\n",
      "Training Epoch: 25 [8600/36045]\tLoss: 604.6206\n",
      "Training Epoch: 25 [8650/36045]\tLoss: 645.7097\n",
      "Training Epoch: 25 [8700/36045]\tLoss: 682.5689\n",
      "Training Epoch: 25 [8750/36045]\tLoss: 670.0791\n",
      "Training Epoch: 25 [8800/36045]\tLoss: 675.6937\n",
      "Training Epoch: 25 [8850/36045]\tLoss: 668.7703\n",
      "Training Epoch: 25 [8900/36045]\tLoss: 603.5918\n",
      "Training Epoch: 25 [8950/36045]\tLoss: 616.9056\n",
      "Training Epoch: 25 [9000/36045]\tLoss: 632.5814\n",
      "Training Epoch: 25 [9050/36045]\tLoss: 633.1948\n",
      "Training Epoch: 25 [9100/36045]\tLoss: 651.9110\n",
      "Training Epoch: 25 [9150/36045]\tLoss: 481.3828\n",
      "Training Epoch: 25 [9200/36045]\tLoss: 361.4163\n",
      "Training Epoch: 25 [9250/36045]\tLoss: 391.8871\n",
      "Training Epoch: 25 [9300/36045]\tLoss: 403.4541\n",
      "Training Epoch: 25 [9350/36045]\tLoss: 371.6736\n",
      "Training Epoch: 25 [9400/36045]\tLoss: 728.2720\n",
      "Training Epoch: 25 [9450/36045]\tLoss: 773.5192\n",
      "Training Epoch: 25 [9500/36045]\tLoss: 760.2556\n",
      "Training Epoch: 25 [9550/36045]\tLoss: 804.0315\n",
      "Training Epoch: 25 [9600/36045]\tLoss: 597.4899\n",
      "Training Epoch: 25 [9650/36045]\tLoss: 601.2714\n",
      "Training Epoch: 25 [9700/36045]\tLoss: 586.4130\n",
      "Training Epoch: 25 [9750/36045]\tLoss: 586.2959\n",
      "Training Epoch: 25 [9800/36045]\tLoss: 764.9325\n",
      "Training Epoch: 25 [9850/36045]\tLoss: 807.9085\n",
      "Training Epoch: 25 [9900/36045]\tLoss: 822.2429\n",
      "Training Epoch: 25 [9950/36045]\tLoss: 800.8582\n",
      "Training Epoch: 25 [10000/36045]\tLoss: 739.6791\n",
      "Training Epoch: 25 [10050/36045]\tLoss: 611.3342\n",
      "Training Epoch: 25 [10100/36045]\tLoss: 617.9099\n",
      "Training Epoch: 25 [10150/36045]\tLoss: 627.8438\n",
      "Training Epoch: 25 [10200/36045]\tLoss: 616.7139\n",
      "Training Epoch: 25 [10250/36045]\tLoss: 735.8922\n",
      "Training Epoch: 25 [10300/36045]\tLoss: 714.6004\n",
      "Training Epoch: 25 [10350/36045]\tLoss: 752.0670\n",
      "Training Epoch: 25 [10400/36045]\tLoss: 742.5895\n",
      "Training Epoch: 25 [10450/36045]\tLoss: 695.3557\n",
      "Training Epoch: 25 [10500/36045]\tLoss: 582.7740\n",
      "Training Epoch: 25 [10550/36045]\tLoss: 578.1077\n",
      "Training Epoch: 25 [10600/36045]\tLoss: 601.4319\n",
      "Training Epoch: 25 [10650/36045]\tLoss: 607.5800\n",
      "Training Epoch: 25 [10700/36045]\tLoss: 694.4495\n",
      "Training Epoch: 25 [10750/36045]\tLoss: 756.9537\n",
      "Training Epoch: 25 [10800/36045]\tLoss: 699.4756\n",
      "Training Epoch: 25 [10850/36045]\tLoss: 740.7056\n",
      "Training Epoch: 25 [10900/36045]\tLoss: 770.7392\n",
      "Training Epoch: 25 [10950/36045]\tLoss: 570.4874\n",
      "Training Epoch: 25 [11000/36045]\tLoss: 564.1691\n",
      "Training Epoch: 25 [11050/36045]\tLoss: 603.7672\n",
      "Training Epoch: 25 [11100/36045]\tLoss: 615.6216\n",
      "Training Epoch: 25 [11150/36045]\tLoss: 667.2081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [11200/36045]\tLoss: 695.5030\n",
      "Training Epoch: 25 [11250/36045]\tLoss: 707.7101\n",
      "Training Epoch: 25 [11300/36045]\tLoss: 687.6740\n",
      "Training Epoch: 25 [11350/36045]\tLoss: 684.6171\n",
      "Training Epoch: 25 [11400/36045]\tLoss: 645.2250\n",
      "Training Epoch: 25 [11450/36045]\tLoss: 611.8224\n",
      "Training Epoch: 25 [11500/36045]\tLoss: 609.4039\n",
      "Training Epoch: 25 [11550/36045]\tLoss: 621.9926\n",
      "Training Epoch: 25 [11600/36045]\tLoss: 684.8337\n",
      "Training Epoch: 25 [11650/36045]\tLoss: 737.5856\n",
      "Training Epoch: 25 [11700/36045]\tLoss: 736.5829\n",
      "Training Epoch: 25 [11750/36045]\tLoss: 756.0358\n",
      "Training Epoch: 25 [11800/36045]\tLoss: 799.1877\n",
      "Training Epoch: 25 [11850/36045]\tLoss: 855.6934\n",
      "Training Epoch: 25 [11900/36045]\tLoss: 1073.7584\n",
      "Training Epoch: 25 [11950/36045]\tLoss: 1075.4148\n",
      "Training Epoch: 25 [12000/36045]\tLoss: 1089.6169\n",
      "Training Epoch: 25 [12050/36045]\tLoss: 1047.5195\n",
      "Training Epoch: 25 [12100/36045]\tLoss: 685.9822\n",
      "Training Epoch: 25 [12150/36045]\tLoss: 527.1459\n",
      "Training Epoch: 25 [12200/36045]\tLoss: 521.5924\n",
      "Training Epoch: 25 [12250/36045]\tLoss: 530.9153\n",
      "Training Epoch: 25 [12300/36045]\tLoss: 675.5596\n",
      "Training Epoch: 25 [12350/36045]\tLoss: 733.8737\n",
      "Training Epoch: 25 [12400/36045]\tLoss: 741.9188\n",
      "Training Epoch: 25 [12450/36045]\tLoss: 729.5640\n",
      "Training Epoch: 25 [12500/36045]\tLoss: 758.8428\n",
      "Training Epoch: 25 [12550/36045]\tLoss: 727.2477\n",
      "Training Epoch: 25 [12600/36045]\tLoss: 670.4271\n",
      "Training Epoch: 25 [12650/36045]\tLoss: 669.3185\n",
      "Training Epoch: 25 [12700/36045]\tLoss: 690.9356\n",
      "Training Epoch: 25 [12750/36045]\tLoss: 690.4850\n",
      "Training Epoch: 25 [12800/36045]\tLoss: 672.9025\n",
      "Training Epoch: 25 [12850/36045]\tLoss: 702.9512\n",
      "Training Epoch: 25 [12900/36045]\tLoss: 674.4506\n",
      "Training Epoch: 25 [12950/36045]\tLoss: 662.0655\n",
      "Training Epoch: 25 [13000/36045]\tLoss: 694.3993\n",
      "Training Epoch: 25 [13050/36045]\tLoss: 631.1724\n",
      "Training Epoch: 25 [13100/36045]\tLoss: 651.0720\n",
      "Training Epoch: 25 [13150/36045]\tLoss: 642.7793\n",
      "Training Epoch: 25 [13200/36045]\tLoss: 622.0704\n",
      "Training Epoch: 25 [13250/36045]\tLoss: 647.9539\n",
      "Training Epoch: 25 [13300/36045]\tLoss: 687.9953\n",
      "Training Epoch: 25 [13350/36045]\tLoss: 667.2678\n",
      "Training Epoch: 25 [13400/36045]\tLoss: 671.1973\n",
      "Training Epoch: 25 [13450/36045]\tLoss: 666.9038\n",
      "Training Epoch: 25 [13500/36045]\tLoss: 688.7029\n",
      "Training Epoch: 25 [13550/36045]\tLoss: 824.5707\n",
      "Training Epoch: 25 [13600/36045]\tLoss: 857.3713\n",
      "Training Epoch: 25 [13650/36045]\tLoss: 937.6274\n",
      "Training Epoch: 25 [13700/36045]\tLoss: 830.2213\n",
      "Training Epoch: 25 [13750/36045]\tLoss: 674.1166\n",
      "Training Epoch: 25 [13800/36045]\tLoss: 646.9155\n",
      "Training Epoch: 25 [13850/36045]\tLoss: 629.9411\n",
      "Training Epoch: 25 [13900/36045]\tLoss: 637.3949\n",
      "Training Epoch: 25 [13950/36045]\tLoss: 683.1505\n",
      "Training Epoch: 25 [14000/36045]\tLoss: 717.5860\n",
      "Training Epoch: 25 [14050/36045]\tLoss: 690.6534\n",
      "Training Epoch: 25 [14100/36045]\tLoss: 686.4040\n",
      "Training Epoch: 25 [14150/36045]\tLoss: 674.2213\n",
      "Training Epoch: 25 [14200/36045]\tLoss: 717.3033\n",
      "Training Epoch: 25 [14250/36045]\tLoss: 786.2374\n",
      "Training Epoch: 25 [14300/36045]\tLoss: 789.5630\n",
      "Training Epoch: 25 [14350/36045]\tLoss: 755.9838\n",
      "Training Epoch: 25 [14400/36045]\tLoss: 741.4546\n",
      "Training Epoch: 25 [14450/36045]\tLoss: 779.1064\n",
      "Training Epoch: 25 [14500/36045]\tLoss: 709.8113\n",
      "Training Epoch: 25 [14550/36045]\tLoss: 740.5805\n",
      "Training Epoch: 25 [14600/36045]\tLoss: 725.4293\n",
      "Training Epoch: 25 [14650/36045]\tLoss: 726.0716\n",
      "Training Epoch: 25 [14700/36045]\tLoss: 685.7804\n",
      "Training Epoch: 25 [14750/36045]\tLoss: 588.7652\n",
      "Training Epoch: 25 [14800/36045]\tLoss: 578.8618\n",
      "Training Epoch: 25 [14850/36045]\tLoss: 586.0488\n",
      "Training Epoch: 25 [14900/36045]\tLoss: 579.5190\n",
      "Training Epoch: 25 [14950/36045]\tLoss: 587.3350\n",
      "Training Epoch: 25 [15000/36045]\tLoss: 602.7040\n",
      "Training Epoch: 25 [15050/36045]\tLoss: 600.6185\n",
      "Training Epoch: 25 [15100/36045]\tLoss: 584.5419\n",
      "Training Epoch: 25 [15150/36045]\tLoss: 577.7861\n",
      "Training Epoch: 25 [15200/36045]\tLoss: 534.3901\n",
      "Training Epoch: 25 [15250/36045]\tLoss: 558.2116\n",
      "Training Epoch: 25 [15300/36045]\tLoss: 542.8232\n",
      "Training Epoch: 25 [15350/36045]\tLoss: 555.4866\n",
      "Training Epoch: 25 [15400/36045]\tLoss: 540.1311\n",
      "Training Epoch: 25 [15450/36045]\tLoss: 526.1098\n",
      "Training Epoch: 25 [15500/36045]\tLoss: 541.4946\n",
      "Training Epoch: 25 [15550/36045]\tLoss: 536.8942\n",
      "Training Epoch: 25 [15600/36045]\tLoss: 607.7895\n",
      "Training Epoch: 25 [15650/36045]\tLoss: 626.6371\n",
      "Training Epoch: 25 [15700/36045]\tLoss: 616.9579\n",
      "Training Epoch: 25 [15750/36045]\tLoss: 608.8749\n",
      "Training Epoch: 25 [15800/36045]\tLoss: 573.3046\n",
      "Training Epoch: 25 [15850/36045]\tLoss: 587.0848\n",
      "Training Epoch: 25 [15900/36045]\tLoss: 596.8265\n",
      "Training Epoch: 25 [15950/36045]\tLoss: 616.7909\n",
      "Training Epoch: 25 [16000/36045]\tLoss: 590.9978\n",
      "Training Epoch: 25 [16050/36045]\tLoss: 560.4631\n",
      "Training Epoch: 25 [16100/36045]\tLoss: 518.5811\n",
      "Training Epoch: 25 [16150/36045]\tLoss: 506.0868\n",
      "Training Epoch: 25 [16200/36045]\tLoss: 612.3203\n",
      "Training Epoch: 25 [16250/36045]\tLoss: 641.8091\n",
      "Training Epoch: 25 [16300/36045]\tLoss: 700.8488\n",
      "Training Epoch: 25 [16350/36045]\tLoss: 718.9531\n",
      "Training Epoch: 25 [16400/36045]\tLoss: 691.1961\n",
      "Training Epoch: 25 [16450/36045]\tLoss: 672.4508\n",
      "Training Epoch: 25 [16500/36045]\tLoss: 671.9415\n",
      "Training Epoch: 25 [16550/36045]\tLoss: 635.9882\n",
      "Training Epoch: 25 [16600/36045]\tLoss: 661.4152\n",
      "Training Epoch: 25 [16650/36045]\tLoss: 680.7133\n",
      "Training Epoch: 25 [16700/36045]\tLoss: 657.3419\n",
      "Training Epoch: 25 [16750/36045]\tLoss: 649.0978\n",
      "Training Epoch: 25 [16800/36045]\tLoss: 660.6229\n",
      "Training Epoch: 25 [16850/36045]\tLoss: 628.9257\n",
      "Training Epoch: 25 [16900/36045]\tLoss: 639.5342\n",
      "Training Epoch: 25 [16950/36045]\tLoss: 665.2790\n",
      "Training Epoch: 25 [17000/36045]\tLoss: 647.1957\n",
      "Training Epoch: 25 [17050/36045]\tLoss: 675.4174\n",
      "Training Epoch: 25 [17100/36045]\tLoss: 672.4262\n",
      "Training Epoch: 25 [17150/36045]\tLoss: 584.8801\n",
      "Training Epoch: 25 [17200/36045]\tLoss: 544.8052\n",
      "Training Epoch: 25 [17250/36045]\tLoss: 570.0409\n",
      "Training Epoch: 25 [17300/36045]\tLoss: 602.6567\n",
      "Training Epoch: 25 [17350/36045]\tLoss: 578.8096\n",
      "Training Epoch: 25 [17400/36045]\tLoss: 598.3171\n",
      "Training Epoch: 25 [17450/36045]\tLoss: 618.8102\n",
      "Training Epoch: 25 [17500/36045]\tLoss: 606.5215\n",
      "Training Epoch: 25 [17550/36045]\tLoss: 606.4416\n",
      "Training Epoch: 25 [17600/36045]\tLoss: 598.7148\n",
      "Training Epoch: 25 [17650/36045]\tLoss: 616.0574\n",
      "Training Epoch: 25 [17700/36045]\tLoss: 594.5524\n",
      "Training Epoch: 25 [17750/36045]\tLoss: 611.6689\n",
      "Training Epoch: 25 [17800/36045]\tLoss: 602.1794\n",
      "Training Epoch: 25 [17850/36045]\tLoss: 610.2122\n",
      "Training Epoch: 25 [17900/36045]\tLoss: 639.2812\n",
      "Training Epoch: 25 [17950/36045]\tLoss: 650.6869\n",
      "Training Epoch: 25 [18000/36045]\tLoss: 640.6721\n",
      "Training Epoch: 25 [18050/36045]\tLoss: 711.5046\n",
      "Training Epoch: 25 [18100/36045]\tLoss: 714.0435\n",
      "Training Epoch: 25 [18150/36045]\tLoss: 724.7954\n",
      "Training Epoch: 25 [18200/36045]\tLoss: 707.1034\n",
      "Training Epoch: 25 [18250/36045]\tLoss: 728.1787\n",
      "Training Epoch: 25 [18300/36045]\tLoss: 675.1272\n",
      "Training Epoch: 25 [18350/36045]\tLoss: 745.1407\n",
      "Training Epoch: 25 [18400/36045]\tLoss: 718.0733\n",
      "Training Epoch: 25 [18450/36045]\tLoss: 698.4785\n",
      "Training Epoch: 25 [18500/36045]\tLoss: 697.6985\n",
      "Training Epoch: 25 [18550/36045]\tLoss: 684.4458\n",
      "Training Epoch: 25 [18600/36045]\tLoss: 674.0413\n",
      "Training Epoch: 25 [18650/36045]\tLoss: 721.4332\n",
      "Training Epoch: 25 [18700/36045]\tLoss: 759.1303\n",
      "Training Epoch: 25 [18750/36045]\tLoss: 744.7701\n",
      "Training Epoch: 25 [18800/36045]\tLoss: 769.1501\n",
      "Training Epoch: 25 [18850/36045]\tLoss: 713.2291\n",
      "Training Epoch: 25 [18900/36045]\tLoss: 762.9652\n",
      "Training Epoch: 25 [18950/36045]\tLoss: 703.5846\n",
      "Training Epoch: 25 [19000/36045]\tLoss: 592.1771\n",
      "Training Epoch: 25 [19050/36045]\tLoss: 573.7766\n",
      "Training Epoch: 25 [19100/36045]\tLoss: 583.3055\n",
      "Training Epoch: 25 [19150/36045]\tLoss: 572.2189\n",
      "Training Epoch: 25 [19200/36045]\tLoss: 600.1290\n",
      "Training Epoch: 25 [19250/36045]\tLoss: 614.7070\n",
      "Training Epoch: 25 [19300/36045]\tLoss: 625.6033\n",
      "Training Epoch: 25 [19350/36045]\tLoss: 609.1759\n",
      "Training Epoch: 25 [19400/36045]\tLoss: 631.7280\n",
      "Training Epoch: 25 [19450/36045]\tLoss: 622.1656\n",
      "Training Epoch: 25 [19500/36045]\tLoss: 624.1530\n",
      "Training Epoch: 25 [19550/36045]\tLoss: 622.5316\n",
      "Training Epoch: 25 [19600/36045]\tLoss: 664.0976\n",
      "Training Epoch: 25 [19650/36045]\tLoss: 874.6174\n",
      "Training Epoch: 25 [19700/36045]\tLoss: 833.4818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [19750/36045]\tLoss: 835.8798\n",
      "Training Epoch: 25 [19800/36045]\tLoss: 834.1036\n",
      "Training Epoch: 25 [19850/36045]\tLoss: 557.4254\n",
      "Training Epoch: 25 [19900/36045]\tLoss: 535.1680\n",
      "Training Epoch: 25 [19950/36045]\tLoss: 538.7548\n",
      "Training Epoch: 25 [20000/36045]\tLoss: 537.2540\n",
      "Training Epoch: 25 [20050/36045]\tLoss: 601.4232\n",
      "Training Epoch: 25 [20100/36045]\tLoss: 607.6334\n",
      "Training Epoch: 25 [20150/36045]\tLoss: 610.3438\n",
      "Training Epoch: 25 [20200/36045]\tLoss: 610.6420\n",
      "Training Epoch: 25 [20250/36045]\tLoss: 650.1766\n",
      "Training Epoch: 25 [20300/36045]\tLoss: 686.6066\n",
      "Training Epoch: 25 [20350/36045]\tLoss: 705.9172\n",
      "Training Epoch: 25 [20400/36045]\tLoss: 722.3140\n",
      "Training Epoch: 25 [20450/36045]\tLoss: 693.2978\n",
      "Training Epoch: 25 [20500/36045]\tLoss: 677.0858\n",
      "Training Epoch: 25 [20550/36045]\tLoss: 596.2100\n",
      "Training Epoch: 25 [20600/36045]\tLoss: 607.7238\n",
      "Training Epoch: 25 [20650/36045]\tLoss: 604.0710\n",
      "Training Epoch: 25 [20700/36045]\tLoss: 591.6395\n",
      "Training Epoch: 25 [20750/36045]\tLoss: 636.0902\n",
      "Training Epoch: 25 [20800/36045]\tLoss: 691.9274\n",
      "Training Epoch: 25 [20850/36045]\tLoss: 678.8371\n",
      "Training Epoch: 25 [20900/36045]\tLoss: 725.2310\n",
      "Training Epoch: 25 [20950/36045]\tLoss: 684.1295\n",
      "Training Epoch: 25 [21000/36045]\tLoss: 644.9871\n",
      "Training Epoch: 25 [21050/36045]\tLoss: 552.8510\n",
      "Training Epoch: 25 [21100/36045]\tLoss: 555.8934\n",
      "Training Epoch: 25 [21150/36045]\tLoss: 594.8922\n",
      "Training Epoch: 25 [21200/36045]\tLoss: 594.1168\n",
      "Training Epoch: 25 [21250/36045]\tLoss: 568.5087\n",
      "Training Epoch: 25 [21300/36045]\tLoss: 663.5573\n",
      "Training Epoch: 25 [21350/36045]\tLoss: 656.4405\n",
      "Training Epoch: 25 [21400/36045]\tLoss: 659.7849\n",
      "Training Epoch: 25 [21450/36045]\tLoss: 666.2173\n",
      "Training Epoch: 25 [21500/36045]\tLoss: 669.4924\n",
      "Training Epoch: 25 [21550/36045]\tLoss: 765.1265\n",
      "Training Epoch: 25 [21600/36045]\tLoss: 765.1917\n",
      "Training Epoch: 25 [21650/36045]\tLoss: 778.3524\n",
      "Training Epoch: 25 [21700/36045]\tLoss: 779.2125\n",
      "Training Epoch: 25 [21750/36045]\tLoss: 749.3533\n",
      "Training Epoch: 25 [21800/36045]\tLoss: 554.5918\n",
      "Training Epoch: 25 [21850/36045]\tLoss: 537.3282\n",
      "Training Epoch: 25 [21900/36045]\tLoss: 548.4098\n",
      "Training Epoch: 25 [21950/36045]\tLoss: 547.2372\n",
      "Training Epoch: 25 [22000/36045]\tLoss: 551.4879\n",
      "Training Epoch: 25 [22050/36045]\tLoss: 576.3485\n",
      "Training Epoch: 25 [22100/36045]\tLoss: 568.5698\n",
      "Training Epoch: 25 [22150/36045]\tLoss: 552.7150\n",
      "Training Epoch: 25 [22200/36045]\tLoss: 570.5796\n",
      "Training Epoch: 25 [22250/36045]\tLoss: 575.8113\n",
      "Training Epoch: 25 [22300/36045]\tLoss: 629.2491\n",
      "Training Epoch: 25 [22350/36045]\tLoss: 655.6494\n",
      "Training Epoch: 25 [22400/36045]\tLoss: 670.9690\n",
      "Training Epoch: 25 [22450/36045]\tLoss: 658.7819\n",
      "Training Epoch: 25 [22500/36045]\tLoss: 640.1232\n",
      "Training Epoch: 25 [22550/36045]\tLoss: 677.5844\n",
      "Training Epoch: 25 [22600/36045]\tLoss: 736.0226\n",
      "Training Epoch: 25 [22650/36045]\tLoss: 772.4995\n",
      "Training Epoch: 25 [22700/36045]\tLoss: 795.8409\n",
      "Training Epoch: 25 [22750/36045]\tLoss: 816.3071\n",
      "Training Epoch: 25 [22800/36045]\tLoss: 848.9227\n",
      "Training Epoch: 25 [22850/36045]\tLoss: 706.1432\n",
      "Training Epoch: 25 [22900/36045]\tLoss: 710.5455\n",
      "Training Epoch: 25 [22950/36045]\tLoss: 689.1388\n",
      "Training Epoch: 25 [23000/36045]\tLoss: 687.6552\n",
      "Training Epoch: 25 [23050/36045]\tLoss: 612.3390\n",
      "Training Epoch: 25 [23100/36045]\tLoss: 628.5742\n",
      "Training Epoch: 25 [23150/36045]\tLoss: 616.6382\n",
      "Training Epoch: 25 [23200/36045]\tLoss: 584.0553\n",
      "Training Epoch: 25 [23250/36045]\tLoss: 586.9448\n",
      "Training Epoch: 25 [23300/36045]\tLoss: 583.7873\n",
      "Training Epoch: 25 [23350/36045]\tLoss: 605.4963\n",
      "Training Epoch: 25 [23400/36045]\tLoss: 655.9590\n",
      "Training Epoch: 25 [23450/36045]\tLoss: 648.3734\n",
      "Training Epoch: 25 [23500/36045]\tLoss: 624.9648\n",
      "Training Epoch: 25 [23550/36045]\tLoss: 670.7944\n",
      "Training Epoch: 25 [23600/36045]\tLoss: 756.4472\n",
      "Training Epoch: 25 [23650/36045]\tLoss: 770.1682\n",
      "Training Epoch: 25 [23700/36045]\tLoss: 779.4458\n",
      "Training Epoch: 25 [23750/36045]\tLoss: 753.4898\n",
      "Training Epoch: 25 [23800/36045]\tLoss: 600.2884\n",
      "Training Epoch: 25 [23850/36045]\tLoss: 627.2916\n",
      "Training Epoch: 25 [23900/36045]\tLoss: 617.0694\n",
      "Training Epoch: 25 [23950/36045]\tLoss: 599.5918\n",
      "Training Epoch: 25 [24000/36045]\tLoss: 575.6651\n",
      "Training Epoch: 25 [24050/36045]\tLoss: 532.1251\n",
      "Training Epoch: 25 [24100/36045]\tLoss: 560.1316\n",
      "Training Epoch: 25 [24150/36045]\tLoss: 554.2673\n",
      "Training Epoch: 25 [24200/36045]\tLoss: 549.8116\n",
      "Training Epoch: 25 [24250/36045]\tLoss: 533.6400\n",
      "Training Epoch: 25 [24300/36045]\tLoss: 575.5064\n",
      "Training Epoch: 25 [24350/36045]\tLoss: 589.7144\n",
      "Training Epoch: 25 [24400/36045]\tLoss: 606.4268\n",
      "Training Epoch: 25 [24450/36045]\tLoss: 578.6017\n",
      "Training Epoch: 25 [24500/36045]\tLoss: 609.4302\n",
      "Training Epoch: 25 [24550/36045]\tLoss: 701.1674\n",
      "Training Epoch: 25 [24600/36045]\tLoss: 693.2390\n",
      "Training Epoch: 25 [24650/36045]\tLoss: 665.4641\n",
      "Training Epoch: 25 [24700/36045]\tLoss: 676.0682\n",
      "Training Epoch: 25 [24750/36045]\tLoss: 624.6394\n",
      "Training Epoch: 25 [24800/36045]\tLoss: 518.9399\n",
      "Training Epoch: 25 [24850/36045]\tLoss: 538.8682\n",
      "Training Epoch: 25 [24900/36045]\tLoss: 535.4746\n",
      "Training Epoch: 25 [24950/36045]\tLoss: 537.8481\n",
      "Training Epoch: 25 [25000/36045]\tLoss: 517.0588\n",
      "Training Epoch: 25 [25050/36045]\tLoss: 493.7895\n",
      "Training Epoch: 25 [25100/36045]\tLoss: 442.9280\n",
      "Training Epoch: 25 [25150/36045]\tLoss: 410.4412\n",
      "Training Epoch: 25 [25200/36045]\tLoss: 405.3132\n",
      "Training Epoch: 25 [25250/36045]\tLoss: 434.1554\n",
      "Training Epoch: 25 [25300/36045]\tLoss: 569.8246\n",
      "Training Epoch: 25 [25350/36045]\tLoss: 567.4001\n",
      "Training Epoch: 25 [25400/36045]\tLoss: 528.4504\n",
      "Training Epoch: 25 [25450/36045]\tLoss: 531.3461\n",
      "Training Epoch: 25 [25500/36045]\tLoss: 577.3585\n",
      "Training Epoch: 25 [25550/36045]\tLoss: 671.6742\n",
      "Training Epoch: 25 [25600/36045]\tLoss: 676.9990\n",
      "Training Epoch: 25 [25650/36045]\tLoss: 653.3530\n",
      "Training Epoch: 25 [25700/36045]\tLoss: 662.6430\n",
      "Training Epoch: 25 [25750/36045]\tLoss: 638.0965\n",
      "Training Epoch: 25 [25800/36045]\tLoss: 401.7942\n",
      "Training Epoch: 25 [25850/36045]\tLoss: 412.1172\n",
      "Training Epoch: 25 [25900/36045]\tLoss: 393.1395\n",
      "Training Epoch: 25 [25950/36045]\tLoss: 402.1680\n",
      "Training Epoch: 25 [26000/36045]\tLoss: 492.5094\n",
      "Training Epoch: 25 [26050/36045]\tLoss: 669.9479\n",
      "Training Epoch: 25 [26100/36045]\tLoss: 698.0450\n",
      "Training Epoch: 25 [26150/36045]\tLoss: 698.2001\n",
      "Training Epoch: 25 [26200/36045]\tLoss: 671.5864\n",
      "Training Epoch: 25 [26250/36045]\tLoss: 702.7173\n",
      "Training Epoch: 25 [26300/36045]\tLoss: 630.7773\n",
      "Training Epoch: 25 [26350/36045]\tLoss: 640.9221\n",
      "Training Epoch: 25 [26400/36045]\tLoss: 618.7505\n",
      "Training Epoch: 25 [26450/36045]\tLoss: 548.3148\n",
      "Training Epoch: 25 [26500/36045]\tLoss: 653.8187\n",
      "Training Epoch: 25 [26550/36045]\tLoss: 656.4068\n",
      "Training Epoch: 25 [26600/36045]\tLoss: 651.7849\n",
      "Training Epoch: 25 [26650/36045]\tLoss: 668.1512\n",
      "Training Epoch: 25 [26700/36045]\tLoss: 647.9571\n",
      "Training Epoch: 25 [26750/36045]\tLoss: 606.4166\n",
      "Training Epoch: 25 [26800/36045]\tLoss: 446.3751\n",
      "Training Epoch: 25 [26850/36045]\tLoss: 370.6640\n",
      "Training Epoch: 25 [26900/36045]\tLoss: 374.0004\n",
      "Training Epoch: 25 [26950/36045]\tLoss: 411.2424\n",
      "Training Epoch: 25 [27000/36045]\tLoss: 666.0101\n",
      "Training Epoch: 25 [27050/36045]\tLoss: 697.1110\n",
      "Training Epoch: 25 [27100/36045]\tLoss: 675.0805\n",
      "Training Epoch: 25 [27150/36045]\tLoss: 716.5265\n",
      "Training Epoch: 25 [27200/36045]\tLoss: 526.3619\n",
      "Training Epoch: 25 [27250/36045]\tLoss: 519.7034\n",
      "Training Epoch: 25 [27300/36045]\tLoss: 505.6552\n",
      "Training Epoch: 25 [27350/36045]\tLoss: 504.8564\n",
      "Training Epoch: 25 [27400/36045]\tLoss: 503.9527\n",
      "Training Epoch: 25 [27450/36045]\tLoss: 635.3206\n",
      "Training Epoch: 25 [27500/36045]\tLoss: 681.6019\n",
      "Training Epoch: 25 [27550/36045]\tLoss: 673.8965\n",
      "Training Epoch: 25 [27600/36045]\tLoss: 685.5939\n",
      "Training Epoch: 25 [27650/36045]\tLoss: 677.3525\n",
      "Training Epoch: 25 [27700/36045]\tLoss: 707.2788\n",
      "Training Epoch: 25 [27750/36045]\tLoss: 719.6835\n",
      "Training Epoch: 25 [27800/36045]\tLoss: 705.9041\n",
      "Training Epoch: 25 [27850/36045]\tLoss: 694.6682\n",
      "Training Epoch: 25 [27900/36045]\tLoss: 625.8132\n",
      "Training Epoch: 25 [27950/36045]\tLoss: 519.3303\n",
      "Training Epoch: 25 [28000/36045]\tLoss: 494.9476\n",
      "Training Epoch: 25 [28050/36045]\tLoss: 506.0911\n",
      "Training Epoch: 25 [28100/36045]\tLoss: 497.7865\n",
      "Training Epoch: 25 [28150/36045]\tLoss: 524.5604\n",
      "Training Epoch: 25 [28200/36045]\tLoss: 530.5186\n",
      "Training Epoch: 25 [28250/36045]\tLoss: 524.3456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 25 [28300/36045]\tLoss: 497.0680\n",
      "Training Epoch: 25 [28350/36045]\tLoss: 493.2372\n",
      "Training Epoch: 25 [28400/36045]\tLoss: 827.4101\n",
      "Training Epoch: 25 [28450/36045]\tLoss: 755.8446\n",
      "Training Epoch: 25 [28500/36045]\tLoss: 653.9937\n",
      "Training Epoch: 25 [28550/36045]\tLoss: 600.0816\n",
      "Training Epoch: 25 [28600/36045]\tLoss: 636.2877\n",
      "Training Epoch: 25 [28650/36045]\tLoss: 711.8086\n",
      "Training Epoch: 25 [28700/36045]\tLoss: 706.2956\n",
      "Training Epoch: 25 [28750/36045]\tLoss: 693.3896\n",
      "Training Epoch: 25 [28800/36045]\tLoss: 702.0675\n",
      "Training Epoch: 25 [28850/36045]\tLoss: 607.7439\n",
      "Training Epoch: 25 [28900/36045]\tLoss: 490.4719\n",
      "Training Epoch: 25 [28950/36045]\tLoss: 488.4853\n",
      "Training Epoch: 25 [29000/36045]\tLoss: 487.0356\n",
      "Training Epoch: 25 [29050/36045]\tLoss: 494.8947\n",
      "Training Epoch: 25 [29100/36045]\tLoss: 515.2240\n",
      "Training Epoch: 25 [29150/36045]\tLoss: 502.5848\n",
      "Training Epoch: 25 [29200/36045]\tLoss: 487.6208\n",
      "Training Epoch: 25 [29250/36045]\tLoss: 475.8118\n",
      "Training Epoch: 25 [29300/36045]\tLoss: 542.9242\n",
      "Training Epoch: 25 [29350/36045]\tLoss: 643.4088\n",
      "Training Epoch: 25 [29400/36045]\tLoss: 661.9363\n",
      "Training Epoch: 25 [29450/36045]\tLoss: 682.4183\n",
      "Training Epoch: 25 [29500/36045]\tLoss: 696.5336\n",
      "Training Epoch: 25 [29550/36045]\tLoss: 662.0382\n",
      "Training Epoch: 25 [29600/36045]\tLoss: 559.5683\n",
      "Training Epoch: 25 [29650/36045]\tLoss: 542.8162\n",
      "Training Epoch: 25 [29700/36045]\tLoss: 484.2489\n",
      "Training Epoch: 25 [29750/36045]\tLoss: 484.6764\n",
      "Training Epoch: 25 [29800/36045]\tLoss: 530.8165\n",
      "Training Epoch: 25 [29850/36045]\tLoss: 604.5419\n",
      "Training Epoch: 25 [29900/36045]\tLoss: 601.2284\n",
      "Training Epoch: 25 [29950/36045]\tLoss: 623.7511\n",
      "Training Epoch: 25 [30000/36045]\tLoss: 599.9497\n",
      "Training Epoch: 25 [30050/36045]\tLoss: 606.0930\n",
      "Training Epoch: 25 [30100/36045]\tLoss: 739.2950\n",
      "Training Epoch: 25 [30150/36045]\tLoss: 723.3942\n",
      "Training Epoch: 25 [30200/36045]\tLoss: 682.2585\n",
      "Training Epoch: 25 [30250/36045]\tLoss: 731.6542\n",
      "Training Epoch: 25 [30300/36045]\tLoss: 717.4151\n",
      "Training Epoch: 25 [30350/36045]\tLoss: 560.6932\n",
      "Training Epoch: 25 [30400/36045]\tLoss: 545.7933\n",
      "Training Epoch: 25 [30450/36045]\tLoss: 545.7616\n",
      "Training Epoch: 25 [30500/36045]\tLoss: 510.4280\n",
      "Training Epoch: 25 [30550/36045]\tLoss: 472.8243\n",
      "Training Epoch: 25 [30600/36045]\tLoss: 460.9601\n",
      "Training Epoch: 25 [30650/36045]\tLoss: 451.3740\n",
      "Training Epoch: 25 [30700/36045]\tLoss: 469.0832\n",
      "Training Epoch: 25 [30750/36045]\tLoss: 455.7124\n",
      "Training Epoch: 25 [30800/36045]\tLoss: 483.6954\n",
      "Training Epoch: 25 [30850/36045]\tLoss: 475.3558\n",
      "Training Epoch: 25 [30900/36045]\tLoss: 488.6302\n",
      "Training Epoch: 25 [30950/36045]\tLoss: 513.4592\n",
      "Training Epoch: 25 [31000/36045]\tLoss: 504.7019\n",
      "Training Epoch: 25 [31050/36045]\tLoss: 421.4711\n",
      "Training Epoch: 25 [31100/36045]\tLoss: 412.2933\n",
      "Training Epoch: 25 [31150/36045]\tLoss: 418.7839\n",
      "Training Epoch: 25 [31200/36045]\tLoss: 522.8436\n",
      "Training Epoch: 25 [31250/36045]\tLoss: 678.1875\n",
      "Training Epoch: 25 [31300/36045]\tLoss: 648.0381\n",
      "Training Epoch: 25 [31350/36045]\tLoss: 663.9122\n",
      "Training Epoch: 25 [31400/36045]\tLoss: 643.0986\n",
      "Training Epoch: 25 [31450/36045]\tLoss: 658.0250\n",
      "Training Epoch: 25 [31500/36045]\tLoss: 669.6379\n",
      "Training Epoch: 25 [31550/36045]\tLoss: 678.4556\n",
      "Training Epoch: 25 [31600/36045]\tLoss: 637.5711\n",
      "Training Epoch: 25 [31650/36045]\tLoss: 680.9462\n",
      "Training Epoch: 25 [31700/36045]\tLoss: 495.1786\n",
      "Training Epoch: 25 [31750/36045]\tLoss: 410.7149\n",
      "Training Epoch: 25 [31800/36045]\tLoss: 391.1279\n",
      "Training Epoch: 25 [31850/36045]\tLoss: 400.6129\n",
      "Training Epoch: 25 [31900/36045]\tLoss: 625.3258\n",
      "Training Epoch: 25 [31950/36045]\tLoss: 804.8690\n",
      "Training Epoch: 25 [32000/36045]\tLoss: 917.7950\n",
      "Training Epoch: 25 [32050/36045]\tLoss: 871.4562\n",
      "Training Epoch: 25 [32100/36045]\tLoss: 860.6907\n",
      "Training Epoch: 25 [32150/36045]\tLoss: 672.3615\n",
      "Training Epoch: 25 [32200/36045]\tLoss: 676.7555\n",
      "Training Epoch: 25 [32250/36045]\tLoss: 687.6915\n",
      "Training Epoch: 25 [32300/36045]\tLoss: 669.4517\n",
      "Training Epoch: 25 [32350/36045]\tLoss: 664.2214\n",
      "Training Epoch: 25 [32400/36045]\tLoss: 623.4258\n",
      "Training Epoch: 25 [32450/36045]\tLoss: 513.9040\n",
      "Training Epoch: 25 [32500/36045]\tLoss: 494.2193\n",
      "Training Epoch: 25 [32550/36045]\tLoss: 496.9461\n",
      "Training Epoch: 25 [32600/36045]\tLoss: 493.2835\n",
      "Training Epoch: 25 [32650/36045]\tLoss: 629.2190\n",
      "Training Epoch: 25 [32700/36045]\tLoss: 685.8191\n",
      "Training Epoch: 25 [32750/36045]\tLoss: 653.6707\n",
      "Training Epoch: 25 [32800/36045]\tLoss: 670.6279\n",
      "Training Epoch: 25 [32850/36045]\tLoss: 619.3768\n",
      "Training Epoch: 25 [32900/36045]\tLoss: 499.0022\n",
      "Training Epoch: 25 [32950/36045]\tLoss: 522.0208\n",
      "Training Epoch: 25 [33000/36045]\tLoss: 521.7889\n",
      "Training Epoch: 25 [33050/36045]\tLoss: 495.0944\n",
      "Training Epoch: 25 [33100/36045]\tLoss: 563.1464\n",
      "Training Epoch: 25 [33150/36045]\tLoss: 763.0825\n",
      "Training Epoch: 25 [33200/36045]\tLoss: 743.5698\n",
      "Training Epoch: 25 [33250/36045]\tLoss: 765.9850\n",
      "Training Epoch: 25 [33300/36045]\tLoss: 815.2713\n",
      "Training Epoch: 25 [33350/36045]\tLoss: 625.8646\n",
      "Training Epoch: 25 [33400/36045]\tLoss: 460.9180\n",
      "Training Epoch: 25 [33450/36045]\tLoss: 456.0283\n",
      "Training Epoch: 25 [33500/36045]\tLoss: 469.3610\n",
      "Training Epoch: 25 [33550/36045]\tLoss: 486.9316\n",
      "Training Epoch: 25 [33600/36045]\tLoss: 488.4418\n",
      "Training Epoch: 25 [33650/36045]\tLoss: 650.4079\n",
      "Training Epoch: 25 [33700/36045]\tLoss: 629.5103\n",
      "Training Epoch: 25 [33750/36045]\tLoss: 651.9114\n",
      "Training Epoch: 25 [33800/36045]\tLoss: 647.0475\n",
      "Training Epoch: 25 [33850/36045]\tLoss: 649.9720\n",
      "Training Epoch: 25 [33900/36045]\tLoss: 661.0538\n",
      "Training Epoch: 25 [33950/36045]\tLoss: 672.5958\n",
      "Training Epoch: 25 [34000/36045]\tLoss: 659.7808\n",
      "Training Epoch: 25 [34050/36045]\tLoss: 664.1818\n",
      "Training Epoch: 25 [34100/36045]\tLoss: 639.1270\n",
      "Training Epoch: 25 [34150/36045]\tLoss: 594.4744\n",
      "Training Epoch: 25 [34200/36045]\tLoss: 562.8221\n",
      "Training Epoch: 25 [34250/36045]\tLoss: 577.1584\n",
      "Training Epoch: 25 [34300/36045]\tLoss: 494.5754\n",
      "Training Epoch: 25 [34350/36045]\tLoss: 520.4984\n",
      "Training Epoch: 25 [34400/36045]\tLoss: 511.4138\n",
      "Training Epoch: 25 [34450/36045]\tLoss: 480.4308\n",
      "Training Epoch: 25 [34500/36045]\tLoss: 513.0178\n",
      "Training Epoch: 25 [34550/36045]\tLoss: 503.5518\n",
      "Training Epoch: 25 [34600/36045]\tLoss: 505.5826\n",
      "Training Epoch: 25 [34650/36045]\tLoss: 614.6371\n",
      "Training Epoch: 25 [34700/36045]\tLoss: 650.4247\n",
      "Training Epoch: 25 [34750/36045]\tLoss: 577.8170\n",
      "Training Epoch: 25 [34800/36045]\tLoss: 660.0417\n",
      "Training Epoch: 25 [34850/36045]\tLoss: 668.5369\n",
      "Training Epoch: 25 [34900/36045]\tLoss: 737.7068\n",
      "Training Epoch: 25 [34950/36045]\tLoss: 724.8812\n",
      "Training Epoch: 25 [35000/36045]\tLoss: 725.9797\n",
      "Training Epoch: 25 [35050/36045]\tLoss: 711.7668\n",
      "Training Epoch: 25 [35100/36045]\tLoss: 597.9864\n",
      "Training Epoch: 25 [35150/36045]\tLoss: 591.0775\n",
      "Training Epoch: 25 [35200/36045]\tLoss: 501.9975\n",
      "Training Epoch: 25 [35250/36045]\tLoss: 551.0240\n",
      "Training Epoch: 25 [35300/36045]\tLoss: 565.0881\n",
      "Training Epoch: 25 [35350/36045]\tLoss: 642.8807\n",
      "Training Epoch: 25 [35400/36045]\tLoss: 679.1830\n",
      "Training Epoch: 25 [35450/36045]\tLoss: 648.3344\n",
      "Training Epoch: 25 [35500/36045]\tLoss: 629.9993\n",
      "Training Epoch: 25 [35550/36045]\tLoss: 614.9737\n",
      "Training Epoch: 25 [35600/36045]\tLoss: 662.5898\n",
      "Training Epoch: 25 [35650/36045]\tLoss: 736.9379\n",
      "Training Epoch: 25 [35700/36045]\tLoss: 664.1426\n",
      "Training Epoch: 25 [35750/36045]\tLoss: 722.9849\n",
      "Training Epoch: 25 [35800/36045]\tLoss: 729.3702\n",
      "Training Epoch: 25 [35850/36045]\tLoss: 703.5683\n",
      "Training Epoch: 25 [35900/36045]\tLoss: 730.4150\n",
      "Training Epoch: 25 [35950/36045]\tLoss: 728.1230\n",
      "Training Epoch: 25 [36000/36045]\tLoss: 719.6931\n",
      "Training Epoch: 25 [36045/36045]\tLoss: 702.4800\n",
      "Training Epoch: 25 [4004/4004]\tLoss: 655.4042\n",
      "Training Epoch: 26 [50/36045]\tLoss: 655.3295\n",
      "Training Epoch: 26 [100/36045]\tLoss: 628.3883\n",
      "Training Epoch: 26 [150/36045]\tLoss: 626.4172\n",
      "Training Epoch: 26 [200/36045]\tLoss: 613.2994\n",
      "Training Epoch: 26 [250/36045]\tLoss: 730.2659\n",
      "Training Epoch: 26 [300/36045]\tLoss: 793.9636\n",
      "Training Epoch: 26 [350/36045]\tLoss: 759.4698\n",
      "Training Epoch: 26 [400/36045]\tLoss: 755.4886\n",
      "Training Epoch: 26 [450/36045]\tLoss: 735.2372\n",
      "Training Epoch: 26 [500/36045]\tLoss: 684.6646\n",
      "Training Epoch: 26 [550/36045]\tLoss: 689.1422\n",
      "Training Epoch: 26 [600/36045]\tLoss: 669.1838\n",
      "Training Epoch: 26 [650/36045]\tLoss: 693.2501\n",
      "Training Epoch: 26 [700/36045]\tLoss: 680.5961\n",
      "Training Epoch: 26 [750/36045]\tLoss: 661.9966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [800/36045]\tLoss: 676.5184\n",
      "Training Epoch: 26 [850/36045]\tLoss: 657.2468\n",
      "Training Epoch: 26 [900/36045]\tLoss: 625.8916\n",
      "Training Epoch: 26 [950/36045]\tLoss: 593.1108\n",
      "Training Epoch: 26 [1000/36045]\tLoss: 571.8745\n",
      "Training Epoch: 26 [1050/36045]\tLoss: 574.1424\n",
      "Training Epoch: 26 [1100/36045]\tLoss: 558.9884\n",
      "Training Epoch: 26 [1150/36045]\tLoss: 567.9225\n",
      "Training Epoch: 26 [1200/36045]\tLoss: 599.3079\n",
      "Training Epoch: 26 [1250/36045]\tLoss: 685.5114\n",
      "Training Epoch: 26 [1300/36045]\tLoss: 691.7153\n",
      "Training Epoch: 26 [1350/36045]\tLoss: 694.3698\n",
      "Training Epoch: 26 [1400/36045]\tLoss: 721.8113\n",
      "Training Epoch: 26 [1450/36045]\tLoss: 698.2518\n",
      "Training Epoch: 26 [1500/36045]\tLoss: 641.7316\n",
      "Training Epoch: 26 [1550/36045]\tLoss: 657.3453\n",
      "Training Epoch: 26 [1600/36045]\tLoss: 668.1873\n",
      "Training Epoch: 26 [1650/36045]\tLoss: 655.0267\n",
      "Training Epoch: 26 [1700/36045]\tLoss: 667.1619\n",
      "Training Epoch: 26 [1750/36045]\tLoss: 709.0925\n",
      "Training Epoch: 26 [1800/36045]\tLoss: 690.1439\n",
      "Training Epoch: 26 [1850/36045]\tLoss: 707.9252\n",
      "Training Epoch: 26 [1900/36045]\tLoss: 662.5361\n",
      "Training Epoch: 26 [1950/36045]\tLoss: 673.5562\n",
      "Training Epoch: 26 [2000/36045]\tLoss: 609.4871\n",
      "Training Epoch: 26 [2050/36045]\tLoss: 612.4816\n",
      "Training Epoch: 26 [2100/36045]\tLoss: 645.3519\n",
      "Training Epoch: 26 [2150/36045]\tLoss: 624.1412\n",
      "Training Epoch: 26 [2200/36045]\tLoss: 579.3514\n",
      "Training Epoch: 26 [2250/36045]\tLoss: 547.1829\n",
      "Training Epoch: 26 [2300/36045]\tLoss: 573.9726\n",
      "Training Epoch: 26 [2350/36045]\tLoss: 548.5360\n",
      "Training Epoch: 26 [2400/36045]\tLoss: 558.3271\n",
      "Training Epoch: 26 [2450/36045]\tLoss: 710.3148\n",
      "Training Epoch: 26 [2500/36045]\tLoss: 746.3544\n",
      "Training Epoch: 26 [2550/36045]\tLoss: 743.1490\n",
      "Training Epoch: 26 [2600/36045]\tLoss: 752.2074\n",
      "Training Epoch: 26 [2650/36045]\tLoss: 877.7946\n",
      "Training Epoch: 26 [2700/36045]\tLoss: 965.0773\n",
      "Training Epoch: 26 [2750/36045]\tLoss: 1037.6238\n",
      "Training Epoch: 26 [2800/36045]\tLoss: 1047.6548\n",
      "Training Epoch: 26 [2850/36045]\tLoss: 816.2709\n",
      "Training Epoch: 26 [2900/36045]\tLoss: 781.6284\n",
      "Training Epoch: 26 [2950/36045]\tLoss: 754.5992\n",
      "Training Epoch: 26 [3000/36045]\tLoss: 749.5037\n",
      "Training Epoch: 26 [3050/36045]\tLoss: 780.6048\n",
      "Training Epoch: 26 [3100/36045]\tLoss: 714.3859\n",
      "Training Epoch: 26 [3150/36045]\tLoss: 551.2450\n",
      "Training Epoch: 26 [3200/36045]\tLoss: 572.0555\n",
      "Training Epoch: 26 [3250/36045]\tLoss: 538.6094\n",
      "Training Epoch: 26 [3300/36045]\tLoss: 510.1386\n",
      "Training Epoch: 26 [3350/36045]\tLoss: 537.9244\n",
      "Training Epoch: 26 [3400/36045]\tLoss: 564.9637\n",
      "Training Epoch: 26 [3450/36045]\tLoss: 606.4659\n",
      "Training Epoch: 26 [3500/36045]\tLoss: 593.5413\n",
      "Training Epoch: 26 [3550/36045]\tLoss: 569.6769\n",
      "Training Epoch: 26 [3600/36045]\tLoss: 610.2883\n",
      "Training Epoch: 26 [3650/36045]\tLoss: 705.3259\n",
      "Training Epoch: 26 [3700/36045]\tLoss: 711.5202\n",
      "Training Epoch: 26 [3750/36045]\tLoss: 679.7565\n",
      "Training Epoch: 26 [3800/36045]\tLoss: 673.6810\n",
      "Training Epoch: 26 [3850/36045]\tLoss: 672.9238\n",
      "Training Epoch: 26 [3900/36045]\tLoss: 678.4838\n",
      "Training Epoch: 26 [3950/36045]\tLoss: 654.3651\n",
      "Training Epoch: 26 [4000/36045]\tLoss: 660.9812\n",
      "Training Epoch: 26 [4050/36045]\tLoss: 607.6987\n",
      "Training Epoch: 26 [4100/36045]\tLoss: 592.2704\n",
      "Training Epoch: 26 [4150/36045]\tLoss: 609.2142\n",
      "Training Epoch: 26 [4200/36045]\tLoss: 603.7401\n",
      "Training Epoch: 26 [4250/36045]\tLoss: 605.9562\n",
      "Training Epoch: 26 [4300/36045]\tLoss: 624.7227\n",
      "Training Epoch: 26 [4350/36045]\tLoss: 606.8757\n",
      "Training Epoch: 26 [4400/36045]\tLoss: 581.2221\n",
      "Training Epoch: 26 [4450/36045]\tLoss: 635.0788\n",
      "Training Epoch: 26 [4500/36045]\tLoss: 679.2393\n",
      "Training Epoch: 26 [4550/36045]\tLoss: 684.6519\n",
      "Training Epoch: 26 [4600/36045]\tLoss: 707.8691\n",
      "Training Epoch: 26 [4650/36045]\tLoss: 696.8928\n",
      "Training Epoch: 26 [4700/36045]\tLoss: 644.1951\n",
      "Training Epoch: 26 [4750/36045]\tLoss: 626.6642\n",
      "Training Epoch: 26 [4800/36045]\tLoss: 653.2317\n",
      "Training Epoch: 26 [4850/36045]\tLoss: 639.6141\n",
      "Training Epoch: 26 [4900/36045]\tLoss: 621.9534\n",
      "Training Epoch: 26 [4950/36045]\tLoss: 639.4409\n",
      "Training Epoch: 26 [5000/36045]\tLoss: 670.6274\n",
      "Training Epoch: 26 [5050/36045]\tLoss: 649.5562\n",
      "Training Epoch: 26 [5100/36045]\tLoss: 660.0337\n",
      "Training Epoch: 26 [5150/36045]\tLoss: 644.3729\n",
      "Training Epoch: 26 [5200/36045]\tLoss: 642.3104\n",
      "Training Epoch: 26 [5250/36045]\tLoss: 635.4918\n",
      "Training Epoch: 26 [5300/36045]\tLoss: 636.4113\n",
      "Training Epoch: 26 [5350/36045]\tLoss: 659.8220\n",
      "Training Epoch: 26 [5400/36045]\tLoss: 635.0305\n",
      "Training Epoch: 26 [5450/36045]\tLoss: 602.2894\n",
      "Training Epoch: 26 [5500/36045]\tLoss: 631.9094\n",
      "Training Epoch: 26 [5550/36045]\tLoss: 619.8571\n",
      "Training Epoch: 26 [5600/36045]\tLoss: 703.9630\n",
      "Training Epoch: 26 [5650/36045]\tLoss: 667.0751\n",
      "Training Epoch: 26 [5700/36045]\tLoss: 626.3942\n",
      "Training Epoch: 26 [5750/36045]\tLoss: 611.1960\n",
      "Training Epoch: 26 [5800/36045]\tLoss: 644.9620\n",
      "Training Epoch: 26 [5850/36045]\tLoss: 630.6945\n",
      "Training Epoch: 26 [5900/36045]\tLoss: 725.9197\n",
      "Training Epoch: 26 [5950/36045]\tLoss: 743.6938\n",
      "Training Epoch: 26 [6000/36045]\tLoss: 728.3497\n",
      "Training Epoch: 26 [6050/36045]\tLoss: 704.2883\n",
      "Training Epoch: 26 [6100/36045]\tLoss: 709.2078\n",
      "Training Epoch: 26 [6150/36045]\tLoss: 694.0541\n",
      "Training Epoch: 26 [6200/36045]\tLoss: 695.5538\n",
      "Training Epoch: 26 [6250/36045]\tLoss: 716.7441\n",
      "Training Epoch: 26 [6300/36045]\tLoss: 728.5936\n",
      "Training Epoch: 26 [6350/36045]\tLoss: 776.9103\n",
      "Training Epoch: 26 [6400/36045]\tLoss: 646.6871\n",
      "Training Epoch: 26 [6450/36045]\tLoss: 598.0495\n",
      "Training Epoch: 26 [6500/36045]\tLoss: 609.1904\n",
      "Training Epoch: 26 [6550/36045]\tLoss: 626.1661\n",
      "Training Epoch: 26 [6600/36045]\tLoss: 625.9568\n",
      "Training Epoch: 26 [6650/36045]\tLoss: 706.4898\n",
      "Training Epoch: 26 [6700/36045]\tLoss: 739.3254\n",
      "Training Epoch: 26 [6750/36045]\tLoss: 713.9477\n",
      "Training Epoch: 26 [6800/36045]\tLoss: 717.0746\n",
      "Training Epoch: 26 [6850/36045]\tLoss: 704.9429\n",
      "Training Epoch: 26 [6900/36045]\tLoss: 627.7505\n",
      "Training Epoch: 26 [6950/36045]\tLoss: 591.8916\n",
      "Training Epoch: 26 [7000/36045]\tLoss: 629.6046\n",
      "Training Epoch: 26 [7050/36045]\tLoss: 643.5699\n",
      "Training Epoch: 26 [7100/36045]\tLoss: 642.8084\n",
      "Training Epoch: 26 [7150/36045]\tLoss: 653.8502\n",
      "Training Epoch: 26 [7200/36045]\tLoss: 657.8498\n",
      "Training Epoch: 26 [7250/36045]\tLoss: 655.3186\n",
      "Training Epoch: 26 [7300/36045]\tLoss: 642.4150\n",
      "Training Epoch: 26 [7350/36045]\tLoss: 638.1234\n",
      "Training Epoch: 26 [7400/36045]\tLoss: 577.7058\n",
      "Training Epoch: 26 [7450/36045]\tLoss: 581.4498\n",
      "Training Epoch: 26 [7500/36045]\tLoss: 575.9162\n",
      "Training Epoch: 26 [7550/36045]\tLoss: 552.0892\n",
      "Training Epoch: 26 [7600/36045]\tLoss: 613.5959\n",
      "Training Epoch: 26 [7650/36045]\tLoss: 658.5373\n",
      "Training Epoch: 26 [7700/36045]\tLoss: 627.2786\n",
      "Training Epoch: 26 [7750/36045]\tLoss: 642.0065\n",
      "Training Epoch: 26 [7800/36045]\tLoss: 630.1462\n",
      "Training Epoch: 26 [7850/36045]\tLoss: 609.1628\n",
      "Training Epoch: 26 [7900/36045]\tLoss: 642.7833\n",
      "Training Epoch: 26 [7950/36045]\tLoss: 639.9000\n",
      "Training Epoch: 26 [8000/36045]\tLoss: 658.0709\n",
      "Training Epoch: 26 [8050/36045]\tLoss: 621.6547\n",
      "Training Epoch: 26 [8100/36045]\tLoss: 647.5872\n",
      "Training Epoch: 26 [8150/36045]\tLoss: 732.8814\n",
      "Training Epoch: 26 [8200/36045]\tLoss: 719.0363\n",
      "Training Epoch: 26 [8250/36045]\tLoss: 686.2492\n",
      "Training Epoch: 26 [8300/36045]\tLoss: 747.2476\n",
      "Training Epoch: 26 [8350/36045]\tLoss: 686.9510\n",
      "Training Epoch: 26 [8400/36045]\tLoss: 615.8528\n",
      "Training Epoch: 26 [8450/36045]\tLoss: 577.2574\n",
      "Training Epoch: 26 [8500/36045]\tLoss: 612.9337\n",
      "Training Epoch: 26 [8550/36045]\tLoss: 604.5601\n",
      "Training Epoch: 26 [8600/36045]\tLoss: 598.0862\n",
      "Training Epoch: 26 [8650/36045]\tLoss: 638.5629\n",
      "Training Epoch: 26 [8700/36045]\tLoss: 675.0512\n",
      "Training Epoch: 26 [8750/36045]\tLoss: 662.7179\n",
      "Training Epoch: 26 [8800/36045]\tLoss: 668.3989\n",
      "Training Epoch: 26 [8850/36045]\tLoss: 661.5003\n",
      "Training Epoch: 26 [8900/36045]\tLoss: 597.0338\n",
      "Training Epoch: 26 [8950/36045]\tLoss: 610.1378\n",
      "Training Epoch: 26 [9000/36045]\tLoss: 625.7839\n",
      "Training Epoch: 26 [9050/36045]\tLoss: 626.5439\n",
      "Training Epoch: 26 [9100/36045]\tLoss: 645.0029\n",
      "Training Epoch: 26 [9150/36045]\tLoss: 476.4028\n",
      "Training Epoch: 26 [9200/36045]\tLoss: 357.5331\n",
      "Training Epoch: 26 [9250/36045]\tLoss: 387.6821\n",
      "Training Epoch: 26 [9300/36045]\tLoss: 399.0657\n",
      "Training Epoch: 26 [9350/36045]\tLoss: 367.6852\n",
      "Training Epoch: 26 [9400/36045]\tLoss: 720.4869\n",
      "Training Epoch: 26 [9450/36045]\tLoss: 765.2449\n",
      "Training Epoch: 26 [9500/36045]\tLoss: 752.0522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [9550/36045]\tLoss: 795.4122\n",
      "Training Epoch: 26 [9600/36045]\tLoss: 591.0462\n",
      "Training Epoch: 26 [9650/36045]\tLoss: 594.9875\n",
      "Training Epoch: 26 [9700/36045]\tLoss: 580.1708\n",
      "Training Epoch: 26 [9750/36045]\tLoss: 579.9136\n",
      "Training Epoch: 26 [9800/36045]\tLoss: 756.9207\n",
      "Training Epoch: 26 [9850/36045]\tLoss: 799.4445\n",
      "Training Epoch: 26 [9900/36045]\tLoss: 813.3555\n",
      "Training Epoch: 26 [9950/36045]\tLoss: 792.2636\n",
      "Training Epoch: 26 [10000/36045]\tLoss: 731.8224\n",
      "Training Epoch: 26 [10050/36045]\tLoss: 604.2467\n",
      "Training Epoch: 26 [10100/36045]\tLoss: 610.9838\n",
      "Training Epoch: 26 [10150/36045]\tLoss: 620.7783\n",
      "Training Epoch: 26 [10200/36045]\tLoss: 609.6424\n",
      "Training Epoch: 26 [10250/36045]\tLoss: 727.8788\n",
      "Training Epoch: 26 [10300/36045]\tLoss: 706.8629\n",
      "Training Epoch: 26 [10350/36045]\tLoss: 743.9726\n",
      "Training Epoch: 26 [10400/36045]\tLoss: 734.4579\n",
      "Training Epoch: 26 [10450/36045]\tLoss: 687.8127\n",
      "Training Epoch: 26 [10500/36045]\tLoss: 576.2602\n",
      "Training Epoch: 26 [10550/36045]\tLoss: 571.5474\n",
      "Training Epoch: 26 [10600/36045]\tLoss: 594.7470\n",
      "Training Epoch: 26 [10650/36045]\tLoss: 600.9088\n",
      "Training Epoch: 26 [10700/36045]\tLoss: 687.2102\n",
      "Training Epoch: 26 [10750/36045]\tLoss: 749.3569\n",
      "Training Epoch: 26 [10800/36045]\tLoss: 692.2491\n",
      "Training Epoch: 26 [10850/36045]\tLoss: 733.0935\n",
      "Training Epoch: 26 [10900/36045]\tLoss: 762.8258\n",
      "Training Epoch: 26 [10950/36045]\tLoss: 564.3630\n",
      "Training Epoch: 26 [11000/36045]\tLoss: 558.0356\n",
      "Training Epoch: 26 [11050/36045]\tLoss: 597.3325\n",
      "Training Epoch: 26 [11100/36045]\tLoss: 609.0124\n",
      "Training Epoch: 26 [11150/36045]\tLoss: 660.0803\n",
      "Training Epoch: 26 [11200/36045]\tLoss: 688.3036\n",
      "Training Epoch: 26 [11250/36045]\tLoss: 700.4730\n",
      "Training Epoch: 26 [11300/36045]\tLoss: 680.4554\n",
      "Training Epoch: 26 [11350/36045]\tLoss: 677.4712\n",
      "Training Epoch: 26 [11400/36045]\tLoss: 638.3641\n",
      "Training Epoch: 26 [11450/36045]\tLoss: 605.2228\n",
      "Training Epoch: 26 [11500/36045]\tLoss: 602.7695\n",
      "Training Epoch: 26 [11550/36045]\tLoss: 615.0587\n",
      "Training Epoch: 26 [11600/36045]\tLoss: 677.5949\n",
      "Training Epoch: 26 [11650/36045]\tLoss: 730.2286\n",
      "Training Epoch: 26 [11700/36045]\tLoss: 729.2079\n",
      "Training Epoch: 26 [11750/36045]\tLoss: 748.5687\n",
      "Training Epoch: 26 [11800/36045]\tLoss: 791.6600\n",
      "Training Epoch: 26 [11850/36045]\tLoss: 848.4107\n",
      "Training Epoch: 26 [11900/36045]\tLoss: 1065.8762\n",
      "Training Epoch: 26 [11950/36045]\tLoss: 1067.6663\n",
      "Training Epoch: 26 [12000/36045]\tLoss: 1081.5946\n",
      "Training Epoch: 26 [12050/36045]\tLoss: 1039.6366\n",
      "Training Epoch: 26 [12100/36045]\tLoss: 679.5110\n",
      "Training Epoch: 26 [12150/36045]\tLoss: 521.3008\n",
      "Training Epoch: 26 [12200/36045]\tLoss: 515.7822\n",
      "Training Epoch: 26 [12250/36045]\tLoss: 525.0388\n",
      "Training Epoch: 26 [12300/36045]\tLoss: 668.7767\n",
      "Training Epoch: 26 [12350/36045]\tLoss: 726.8073\n",
      "Training Epoch: 26 [12400/36045]\tLoss: 734.7763\n",
      "Training Epoch: 26 [12450/36045]\tLoss: 722.5596\n",
      "Training Epoch: 26 [12500/36045]\tLoss: 751.6136\n",
      "Training Epoch: 26 [12550/36045]\tLoss: 720.1747\n",
      "Training Epoch: 26 [12600/36045]\tLoss: 663.5390\n",
      "Training Epoch: 26 [12650/36045]\tLoss: 662.4320\n",
      "Training Epoch: 26 [12700/36045]\tLoss: 683.9904\n",
      "Training Epoch: 26 [12750/36045]\tLoss: 683.4536\n",
      "Training Epoch: 26 [12800/36045]\tLoss: 666.1796\n",
      "Training Epoch: 26 [12850/36045]\tLoss: 696.0303\n",
      "Training Epoch: 26 [12900/36045]\tLoss: 667.8129\n",
      "Training Epoch: 26 [12950/36045]\tLoss: 655.2889\n",
      "Training Epoch: 26 [13000/36045]\tLoss: 687.6470\n",
      "Training Epoch: 26 [13050/36045]\tLoss: 624.7582\n",
      "Training Epoch: 26 [13100/36045]\tLoss: 644.2165\n",
      "Training Epoch: 26 [13150/36045]\tLoss: 635.9337\n",
      "Training Epoch: 26 [13200/36045]\tLoss: 615.6906\n",
      "Training Epoch: 26 [13250/36045]\tLoss: 641.1199\n",
      "Training Epoch: 26 [13300/36045]\tLoss: 680.9109\n",
      "Training Epoch: 26 [13350/36045]\tLoss: 660.3110\n",
      "Training Epoch: 26 [13400/36045]\tLoss: 664.1389\n",
      "Training Epoch: 26 [13450/36045]\tLoss: 660.0754\n",
      "Training Epoch: 26 [13500/36045]\tLoss: 681.5499\n",
      "Training Epoch: 26 [13550/36045]\tLoss: 817.5121\n",
      "Training Epoch: 26 [13600/36045]\tLoss: 850.3454\n",
      "Training Epoch: 26 [13650/36045]\tLoss: 930.6801\n",
      "Training Epoch: 26 [13700/36045]\tLoss: 823.7201\n",
      "Training Epoch: 26 [13750/36045]\tLoss: 667.0398\n",
      "Training Epoch: 26 [13800/36045]\tLoss: 639.7171\n",
      "Training Epoch: 26 [13850/36045]\tLoss: 622.7353\n",
      "Training Epoch: 26 [13900/36045]\tLoss: 630.1729\n",
      "Training Epoch: 26 [13950/36045]\tLoss: 675.8839\n",
      "Training Epoch: 26 [14000/36045]\tLoss: 710.1420\n",
      "Training Epoch: 26 [14050/36045]\tLoss: 683.4729\n",
      "Training Epoch: 26 [14100/36045]\tLoss: 679.1531\n",
      "Training Epoch: 26 [14150/36045]\tLoss: 666.9880\n",
      "Training Epoch: 26 [14200/36045]\tLoss: 709.7697\n",
      "Training Epoch: 26 [14250/36045]\tLoss: 778.2125\n",
      "Training Epoch: 26 [14300/36045]\tLoss: 781.5211\n",
      "Training Epoch: 26 [14350/36045]\tLoss: 748.2038\n",
      "Training Epoch: 26 [14400/36045]\tLoss: 733.6958\n",
      "Training Epoch: 26 [14450/36045]\tLoss: 771.1237\n",
      "Training Epoch: 26 [14500/36045]\tLoss: 701.8296\n",
      "Training Epoch: 26 [14550/36045]\tLoss: 732.4002\n",
      "Training Epoch: 26 [14600/36045]\tLoss: 717.4828\n",
      "Training Epoch: 26 [14650/36045]\tLoss: 717.9858\n",
      "Training Epoch: 26 [14700/36045]\tLoss: 678.3677\n",
      "Training Epoch: 26 [14750/36045]\tLoss: 582.5021\n",
      "Training Epoch: 26 [14800/36045]\tLoss: 572.5987\n",
      "Training Epoch: 26 [14850/36045]\tLoss: 579.7535\n",
      "Training Epoch: 26 [14900/36045]\tLoss: 573.2365\n",
      "Training Epoch: 26 [14950/36045]\tLoss: 581.0413\n",
      "Training Epoch: 26 [15000/36045]\tLoss: 596.1425\n",
      "Training Epoch: 26 [15050/36045]\tLoss: 593.9789\n",
      "Training Epoch: 26 [15100/36045]\tLoss: 577.9411\n",
      "Training Epoch: 26 [15150/36045]\tLoss: 571.3936\n",
      "Training Epoch: 26 [15200/36045]\tLoss: 528.5699\n",
      "Training Epoch: 26 [15250/36045]\tLoss: 552.1953\n",
      "Training Epoch: 26 [15300/36045]\tLoss: 536.8768\n",
      "Training Epoch: 26 [15350/36045]\tLoss: 549.4122\n",
      "Training Epoch: 26 [15400/36045]\tLoss: 533.9119\n",
      "Training Epoch: 26 [15450/36045]\tLoss: 519.7714\n",
      "Training Epoch: 26 [15500/36045]\tLoss: 534.9616\n",
      "Training Epoch: 26 [15550/36045]\tLoss: 530.4420\n",
      "Training Epoch: 26 [15600/36045]\tLoss: 601.0142\n",
      "Training Epoch: 26 [15650/36045]\tLoss: 619.6730\n",
      "Training Epoch: 26 [15700/36045]\tLoss: 610.1848\n",
      "Training Epoch: 26 [15750/36045]\tLoss: 602.0837\n",
      "Training Epoch: 26 [15800/36045]\tLoss: 567.6295\n",
      "Training Epoch: 26 [15850/36045]\tLoss: 581.5474\n",
      "Training Epoch: 26 [15900/36045]\tLoss: 591.2261\n",
      "Training Epoch: 26 [15950/36045]\tLoss: 611.1307\n",
      "Training Epoch: 26 [16000/36045]\tLoss: 585.0332\n",
      "Training Epoch: 26 [16050/36045]\tLoss: 554.4551\n",
      "Training Epoch: 26 [16100/36045]\tLoss: 513.2242\n",
      "Training Epoch: 26 [16150/36045]\tLoss: 500.8251\n",
      "Training Epoch: 26 [16200/36045]\tLoss: 606.1097\n",
      "Training Epoch: 26 [16250/36045]\tLoss: 635.4001\n",
      "Training Epoch: 26 [16300/36045]\tLoss: 693.8069\n",
      "Training Epoch: 26 [16350/36045]\tLoss: 712.1255\n",
      "Training Epoch: 26 [16400/36045]\tLoss: 684.2864\n",
      "Training Epoch: 26 [16450/36045]\tLoss: 665.6776\n",
      "Training Epoch: 26 [16500/36045]\tLoss: 665.2522\n",
      "Training Epoch: 26 [16550/36045]\tLoss: 629.4269\n",
      "Training Epoch: 26 [16600/36045]\tLoss: 654.5137\n",
      "Training Epoch: 26 [16650/36045]\tLoss: 673.4863\n",
      "Training Epoch: 26 [16700/36045]\tLoss: 650.4425\n",
      "Training Epoch: 26 [16750/36045]\tLoss: 642.3303\n",
      "Training Epoch: 26 [16800/36045]\tLoss: 653.6375\n",
      "Training Epoch: 26 [16850/36045]\tLoss: 622.3276\n",
      "Training Epoch: 26 [16900/36045]\tLoss: 632.8198\n",
      "Training Epoch: 26 [16950/36045]\tLoss: 658.2034\n",
      "Training Epoch: 26 [17000/36045]\tLoss: 640.3345\n",
      "Training Epoch: 26 [17050/36045]\tLoss: 668.1584\n",
      "Training Epoch: 26 [17100/36045]\tLoss: 665.0385\n",
      "Training Epoch: 26 [17150/36045]\tLoss: 578.4542\n",
      "Training Epoch: 26 [17200/36045]\tLoss: 538.6697\n",
      "Training Epoch: 26 [17250/36045]\tLoss: 563.6247\n",
      "Training Epoch: 26 [17300/36045]\tLoss: 595.9625\n",
      "Training Epoch: 26 [17350/36045]\tLoss: 572.5322\n",
      "Training Epoch: 26 [17400/36045]\tLoss: 591.9841\n",
      "Training Epoch: 26 [17450/36045]\tLoss: 612.3078\n",
      "Training Epoch: 26 [17500/36045]\tLoss: 600.1071\n",
      "Training Epoch: 26 [17550/36045]\tLoss: 599.8562\n",
      "Training Epoch: 26 [17600/36045]\tLoss: 592.4075\n",
      "Training Epoch: 26 [17650/36045]\tLoss: 609.5822\n",
      "Training Epoch: 26 [17700/36045]\tLoss: 588.1457\n",
      "Training Epoch: 26 [17750/36045]\tLoss: 605.2222\n",
      "Training Epoch: 26 [17800/36045]\tLoss: 595.7704\n",
      "Training Epoch: 26 [17850/36045]\tLoss: 604.6113\n",
      "Training Epoch: 26 [17900/36045]\tLoss: 633.5535\n",
      "Training Epoch: 26 [17950/36045]\tLoss: 645.0207\n",
      "Training Epoch: 26 [18000/36045]\tLoss: 635.0406\n",
      "Training Epoch: 26 [18050/36045]\tLoss: 704.4684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [18100/36045]\tLoss: 706.9176\n",
      "Training Epoch: 26 [18150/36045]\tLoss: 717.7919\n",
      "Training Epoch: 26 [18200/36045]\tLoss: 700.0047\n",
      "Training Epoch: 26 [18250/36045]\tLoss: 721.0328\n",
      "Training Epoch: 26 [18300/36045]\tLoss: 668.8131\n",
      "Training Epoch: 26 [18350/36045]\tLoss: 739.0889\n",
      "Training Epoch: 26 [18400/36045]\tLoss: 712.1038\n",
      "Training Epoch: 26 [18450/36045]\tLoss: 692.4616\n",
      "Training Epoch: 26 [18500/36045]\tLoss: 691.6279\n",
      "Training Epoch: 26 [18550/36045]\tLoss: 678.4213\n",
      "Training Epoch: 26 [18600/36045]\tLoss: 667.9767\n",
      "Training Epoch: 26 [18650/36045]\tLoss: 715.1408\n",
      "Training Epoch: 26 [18700/36045]\tLoss: 752.5390\n",
      "Training Epoch: 26 [18750/36045]\tLoss: 738.3834\n",
      "Training Epoch: 26 [18800/36045]\tLoss: 762.6420\n",
      "Training Epoch: 26 [18850/36045]\tLoss: 706.7947\n",
      "Training Epoch: 26 [18900/36045]\tLoss: 756.0489\n",
      "Training Epoch: 26 [18950/36045]\tLoss: 696.9316\n",
      "Training Epoch: 26 [19000/36045]\tLoss: 585.4493\n",
      "Training Epoch: 26 [19050/36045]\tLoss: 567.4068\n",
      "Training Epoch: 26 [19100/36045]\tLoss: 576.7474\n",
      "Training Epoch: 26 [19150/36045]\tLoss: 565.7705\n",
      "Training Epoch: 26 [19200/36045]\tLoss: 593.9439\n",
      "Training Epoch: 26 [19250/36045]\tLoss: 608.5355\n",
      "Training Epoch: 26 [19300/36045]\tLoss: 619.2999\n",
      "Training Epoch: 26 [19350/36045]\tLoss: 602.8541\n",
      "Training Epoch: 26 [19400/36045]\tLoss: 625.2009\n",
      "Training Epoch: 26 [19450/36045]\tLoss: 615.7672\n",
      "Training Epoch: 26 [19500/36045]\tLoss: 617.6689\n",
      "Training Epoch: 26 [19550/36045]\tLoss: 616.0858\n",
      "Training Epoch: 26 [19600/36045]\tLoss: 657.5497\n",
      "Training Epoch: 26 [19650/36045]\tLoss: 866.9404\n",
      "Training Epoch: 26 [19700/36045]\tLoss: 825.8121\n",
      "Training Epoch: 26 [19750/36045]\tLoss: 828.3781\n",
      "Training Epoch: 26 [19800/36045]\tLoss: 826.8589\n",
      "Training Epoch: 26 [19850/36045]\tLoss: 551.6230\n",
      "Training Epoch: 26 [19900/36045]\tLoss: 529.4888\n",
      "Training Epoch: 26 [19950/36045]\tLoss: 533.0645\n",
      "Training Epoch: 26 [20000/36045]\tLoss: 531.6402\n",
      "Training Epoch: 26 [20050/36045]\tLoss: 595.2222\n",
      "Training Epoch: 26 [20100/36045]\tLoss: 601.4689\n",
      "Training Epoch: 26 [20150/36045]\tLoss: 604.0377\n",
      "Training Epoch: 26 [20200/36045]\tLoss: 604.2348\n",
      "Training Epoch: 26 [20250/36045]\tLoss: 643.3405\n",
      "Training Epoch: 26 [20300/36045]\tLoss: 679.6520\n",
      "Training Epoch: 26 [20350/36045]\tLoss: 698.8852\n",
      "Training Epoch: 26 [20400/36045]\tLoss: 715.3917\n",
      "Training Epoch: 26 [20450/36045]\tLoss: 686.2894\n",
      "Training Epoch: 26 [20500/36045]\tLoss: 670.1415\n",
      "Training Epoch: 26 [20550/36045]\tLoss: 589.9797\n",
      "Training Epoch: 26 [20600/36045]\tLoss: 601.2958\n",
      "Training Epoch: 26 [20650/36045]\tLoss: 597.7627\n",
      "Training Epoch: 26 [20700/36045]\tLoss: 585.4245\n",
      "Training Epoch: 26 [20750/36045]\tLoss: 629.6373\n",
      "Training Epoch: 26 [20800/36045]\tLoss: 684.7883\n",
      "Training Epoch: 26 [20850/36045]\tLoss: 671.6286\n",
      "Training Epoch: 26 [20900/36045]\tLoss: 717.6990\n",
      "Training Epoch: 26 [20950/36045]\tLoss: 676.9856\n",
      "Training Epoch: 26 [21000/36045]\tLoss: 638.1417\n",
      "Training Epoch: 26 [21050/36045]\tLoss: 546.8791\n",
      "Training Epoch: 26 [21100/36045]\tLoss: 550.0910\n",
      "Training Epoch: 26 [21150/36045]\tLoss: 588.6971\n",
      "Training Epoch: 26 [21200/36045]\tLoss: 587.9158\n",
      "Training Epoch: 26 [21250/36045]\tLoss: 562.5721\n",
      "Training Epoch: 26 [21300/36045]\tLoss: 656.6964\n",
      "Training Epoch: 26 [21350/36045]\tLoss: 649.4373\n",
      "Training Epoch: 26 [21400/36045]\tLoss: 652.7809\n",
      "Training Epoch: 26 [21450/36045]\tLoss: 659.1645\n",
      "Training Epoch: 26 [21500/36045]\tLoss: 662.3221\n",
      "Training Epoch: 26 [21550/36045]\tLoss: 757.7714\n",
      "Training Epoch: 26 [21600/36045]\tLoss: 757.6917\n",
      "Training Epoch: 26 [21650/36045]\tLoss: 770.7518\n",
      "Training Epoch: 26 [21700/36045]\tLoss: 771.8913\n",
      "Training Epoch: 26 [21750/36045]\tLoss: 742.1226\n",
      "Training Epoch: 26 [21800/36045]\tLoss: 548.9280\n",
      "Training Epoch: 26 [21850/36045]\tLoss: 531.7277\n",
      "Training Epoch: 26 [21900/36045]\tLoss: 542.6263\n",
      "Training Epoch: 26 [21950/36045]\tLoss: 541.6807\n",
      "Training Epoch: 26 [22000/36045]\tLoss: 545.7784\n",
      "Training Epoch: 26 [22050/36045]\tLoss: 570.1213\n",
      "Training Epoch: 26 [22100/36045]\tLoss: 562.4388\n",
      "Training Epoch: 26 [22150/36045]\tLoss: 546.8295\n",
      "Training Epoch: 26 [22200/36045]\tLoss: 564.4840\n",
      "Training Epoch: 26 [22250/36045]\tLoss: 569.6504\n",
      "Training Epoch: 26 [22300/36045]\tLoss: 622.8779\n",
      "Training Epoch: 26 [22350/36045]\tLoss: 649.1976\n",
      "Training Epoch: 26 [22400/36045]\tLoss: 664.4246\n",
      "Training Epoch: 26 [22450/36045]\tLoss: 652.2349\n",
      "Training Epoch: 26 [22500/36045]\tLoss: 633.7194\n",
      "Training Epoch: 26 [22550/36045]\tLoss: 670.8276\n",
      "Training Epoch: 26 [22600/36045]\tLoss: 728.3415\n",
      "Training Epoch: 26 [22650/36045]\tLoss: 764.5347\n",
      "Training Epoch: 26 [22700/36045]\tLoss: 787.7745\n",
      "Training Epoch: 26 [22750/36045]\tLoss: 808.0554\n",
      "Training Epoch: 26 [22800/36045]\tLoss: 840.2892\n",
      "Training Epoch: 26 [22850/36045]\tLoss: 699.0005\n",
      "Training Epoch: 26 [22900/36045]\tLoss: 703.4730\n",
      "Training Epoch: 26 [22950/36045]\tLoss: 682.2512\n",
      "Training Epoch: 26 [23000/36045]\tLoss: 680.5565\n",
      "Training Epoch: 26 [23050/36045]\tLoss: 605.7543\n",
      "Training Epoch: 26 [23100/36045]\tLoss: 621.9428\n",
      "Training Epoch: 26 [23150/36045]\tLoss: 610.0201\n",
      "Training Epoch: 26 [23200/36045]\tLoss: 577.8087\n",
      "Training Epoch: 26 [23250/36045]\tLoss: 580.7082\n",
      "Training Epoch: 26 [23300/36045]\tLoss: 577.5044\n",
      "Training Epoch: 26 [23350/36045]\tLoss: 599.1034\n",
      "Training Epoch: 26 [23400/36045]\tLoss: 649.0367\n",
      "Training Epoch: 26 [23450/36045]\tLoss: 641.5818\n",
      "Training Epoch: 26 [23500/36045]\tLoss: 618.4313\n",
      "Training Epoch: 26 [23550/36045]\tLoss: 663.6762\n",
      "Training Epoch: 26 [23600/36045]\tLoss: 748.7158\n",
      "Training Epoch: 26 [23650/36045]\tLoss: 762.3227\n",
      "Training Epoch: 26 [23700/36045]\tLoss: 771.4076\n",
      "Training Epoch: 26 [23750/36045]\tLoss: 745.7114\n",
      "Training Epoch: 26 [23800/36045]\tLoss: 594.4616\n",
      "Training Epoch: 26 [23850/36045]\tLoss: 621.3422\n",
      "Training Epoch: 26 [23900/36045]\tLoss: 611.0535\n",
      "Training Epoch: 26 [23950/36045]\tLoss: 593.6851\n",
      "Training Epoch: 26 [24000/36045]\tLoss: 569.8430\n",
      "Training Epoch: 26 [24050/36045]\tLoss: 526.7141\n",
      "Training Epoch: 26 [24100/36045]\tLoss: 554.3821\n",
      "Training Epoch: 26 [24150/36045]\tLoss: 548.3367\n",
      "Training Epoch: 26 [24200/36045]\tLoss: 544.0615\n",
      "Training Epoch: 26 [24250/36045]\tLoss: 528.0416\n",
      "Training Epoch: 26 [24300/36045]\tLoss: 569.6535\n",
      "Training Epoch: 26 [24350/36045]\tLoss: 583.6972\n",
      "Training Epoch: 26 [24400/36045]\tLoss: 600.2053\n",
      "Training Epoch: 26 [24450/36045]\tLoss: 572.5818\n",
      "Training Epoch: 26 [24500/36045]\tLoss: 603.1717\n",
      "Training Epoch: 26 [24550/36045]\tLoss: 694.4766\n",
      "Training Epoch: 26 [24600/36045]\tLoss: 686.5115\n",
      "Training Epoch: 26 [24650/36045]\tLoss: 658.9006\n",
      "Training Epoch: 26 [24700/36045]\tLoss: 669.3645\n",
      "Training Epoch: 26 [24750/36045]\tLoss: 618.4360\n",
      "Training Epoch: 26 [24800/36045]\tLoss: 513.1662\n",
      "Training Epoch: 26 [24850/36045]\tLoss: 532.9996\n",
      "Training Epoch: 26 [24900/36045]\tLoss: 529.7145\n",
      "Training Epoch: 26 [24950/36045]\tLoss: 532.1547\n",
      "Training Epoch: 26 [25000/36045]\tLoss: 511.6182\n",
      "Training Epoch: 26 [25050/36045]\tLoss: 488.5626\n",
      "Training Epoch: 26 [25100/36045]\tLoss: 438.1564\n",
      "Training Epoch: 26 [25150/36045]\tLoss: 405.9573\n",
      "Training Epoch: 26 [25200/36045]\tLoss: 400.8712\n",
      "Training Epoch: 26 [25250/36045]\tLoss: 429.4619\n",
      "Training Epoch: 26 [25300/36045]\tLoss: 563.7585\n",
      "Training Epoch: 26 [25350/36045]\tLoss: 561.1819\n",
      "Training Epoch: 26 [25400/36045]\tLoss: 522.7230\n",
      "Training Epoch: 26 [25450/36045]\tLoss: 525.5541\n",
      "Training Epoch: 26 [25500/36045]\tLoss: 571.0506\n",
      "Training Epoch: 26 [25550/36045]\tLoss: 664.9183\n",
      "Training Epoch: 26 [25600/36045]\tLoss: 670.0924\n",
      "Training Epoch: 26 [25650/36045]\tLoss: 646.6925\n",
      "Training Epoch: 26 [25700/36045]\tLoss: 655.9102\n",
      "Training Epoch: 26 [25750/36045]\tLoss: 631.6878\n",
      "Training Epoch: 26 [25800/36045]\tLoss: 397.8740\n",
      "Training Epoch: 26 [25850/36045]\tLoss: 408.1156\n",
      "Training Epoch: 26 [25900/36045]\tLoss: 389.1449\n",
      "Training Epoch: 26 [25950/36045]\tLoss: 398.1074\n",
      "Training Epoch: 26 [26000/36045]\tLoss: 487.5747\n",
      "Training Epoch: 26 [26050/36045]\tLoss: 663.3651\n",
      "Training Epoch: 26 [26100/36045]\tLoss: 691.3796\n",
      "Training Epoch: 26 [26150/36045]\tLoss: 691.6202\n",
      "Training Epoch: 26 [26200/36045]\tLoss: 664.9714\n",
      "Training Epoch: 26 [26250/36045]\tLoss: 695.8677\n",
      "Training Epoch: 26 [26300/36045]\tLoss: 625.5519\n",
      "Training Epoch: 26 [26350/36045]\tLoss: 635.8137\n",
      "Training Epoch: 26 [26400/36045]\tLoss: 613.5582\n",
      "Training Epoch: 26 [26450/36045]\tLoss: 543.1907\n",
      "Training Epoch: 26 [26500/36045]\tLoss: 647.2683\n",
      "Training Epoch: 26 [26550/36045]\tLoss: 649.5002\n",
      "Training Epoch: 26 [26600/36045]\tLoss: 645.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [26650/36045]\tLoss: 661.3564\n",
      "Training Epoch: 26 [26700/36045]\tLoss: 641.1393\n",
      "Training Epoch: 26 [26750/36045]\tLoss: 600.1280\n",
      "Training Epoch: 26 [26800/36045]\tLoss: 441.7939\n",
      "Training Epoch: 26 [26850/36045]\tLoss: 366.7494\n",
      "Training Epoch: 26 [26900/36045]\tLoss: 370.0306\n",
      "Training Epoch: 26 [26950/36045]\tLoss: 406.8658\n",
      "Training Epoch: 26 [27000/36045]\tLoss: 659.7794\n",
      "Training Epoch: 26 [27050/36045]\tLoss: 690.3445\n",
      "Training Epoch: 26 [27100/36045]\tLoss: 668.5779\n",
      "Training Epoch: 26 [27150/36045]\tLoss: 709.8188\n",
      "Training Epoch: 26 [27200/36045]\tLoss: 521.0069\n",
      "Training Epoch: 26 [27250/36045]\tLoss: 514.0833\n",
      "Training Epoch: 26 [27300/36045]\tLoss: 500.2676\n",
      "Training Epoch: 26 [27350/36045]\tLoss: 499.3762\n",
      "Training Epoch: 26 [27400/36045]\tLoss: 498.4540\n",
      "Training Epoch: 26 [27450/36045]\tLoss: 628.6277\n",
      "Training Epoch: 26 [27500/36045]\tLoss: 674.2691\n",
      "Training Epoch: 26 [27550/36045]\tLoss: 666.6369\n",
      "Training Epoch: 26 [27600/36045]\tLoss: 678.3839\n",
      "Training Epoch: 26 [27650/36045]\tLoss: 670.1043\n",
      "Training Epoch: 26 [27700/36045]\tLoss: 699.9232\n",
      "Training Epoch: 26 [27750/36045]\tLoss: 712.2819\n",
      "Training Epoch: 26 [27800/36045]\tLoss: 698.5621\n",
      "Training Epoch: 26 [27850/36045]\tLoss: 687.5237\n",
      "Training Epoch: 26 [27900/36045]\tLoss: 619.7440\n",
      "Training Epoch: 26 [27950/36045]\tLoss: 514.5519\n",
      "Training Epoch: 26 [28000/36045]\tLoss: 490.3434\n",
      "Training Epoch: 26 [28050/36045]\tLoss: 501.2973\n",
      "Training Epoch: 26 [28100/36045]\tLoss: 492.9957\n",
      "Training Epoch: 26 [28150/36045]\tLoss: 519.1473\n",
      "Training Epoch: 26 [28200/36045]\tLoss: 525.2946\n",
      "Training Epoch: 26 [28250/36045]\tLoss: 518.9752\n",
      "Training Epoch: 26 [28300/36045]\tLoss: 492.1022\n",
      "Training Epoch: 26 [28350/36045]\tLoss: 488.3052\n",
      "Training Epoch: 26 [28400/36045]\tLoss: 821.8965\n",
      "Training Epoch: 26 [28450/36045]\tLoss: 751.1701\n",
      "Training Epoch: 26 [28500/36045]\tLoss: 649.7242\n",
      "Training Epoch: 26 [28550/36045]\tLoss: 596.2280\n",
      "Training Epoch: 26 [28600/36045]\tLoss: 631.4227\n",
      "Training Epoch: 26 [28650/36045]\tLoss: 705.1108\n",
      "Training Epoch: 26 [28700/36045]\tLoss: 699.6223\n",
      "Training Epoch: 26 [28750/36045]\tLoss: 686.5410\n",
      "Training Epoch: 26 [28800/36045]\tLoss: 695.1588\n",
      "Training Epoch: 26 [28850/36045]\tLoss: 601.8880\n",
      "Training Epoch: 26 [28900/36045]\tLoss: 486.2093\n",
      "Training Epoch: 26 [28950/36045]\tLoss: 484.4537\n",
      "Training Epoch: 26 [29000/36045]\tLoss: 482.8104\n",
      "Training Epoch: 26 [29050/36045]\tLoss: 490.4771\n",
      "Training Epoch: 26 [29100/36045]\tLoss: 510.5082\n",
      "Training Epoch: 26 [29150/36045]\tLoss: 498.0537\n",
      "Training Epoch: 26 [29200/36045]\tLoss: 483.1613\n",
      "Training Epoch: 26 [29250/36045]\tLoss: 471.6504\n",
      "Training Epoch: 26 [29300/36045]\tLoss: 537.6830\n",
      "Training Epoch: 26 [29350/36045]\tLoss: 636.6943\n",
      "Training Epoch: 26 [29400/36045]\tLoss: 655.0475\n",
      "Training Epoch: 26 [29450/36045]\tLoss: 674.9945\n",
      "Training Epoch: 26 [29500/36045]\tLoss: 689.1639\n",
      "Training Epoch: 26 [29550/36045]\tLoss: 655.2006\n",
      "Training Epoch: 26 [29600/36045]\tLoss: 553.6733\n",
      "Training Epoch: 26 [29650/36045]\tLoss: 536.8354\n",
      "Training Epoch: 26 [29700/36045]\tLoss: 479.0305\n",
      "Training Epoch: 26 [29750/36045]\tLoss: 479.1241\n",
      "Training Epoch: 26 [29800/36045]\tLoss: 525.3276\n",
      "Training Epoch: 26 [29850/36045]\tLoss: 599.4286\n",
      "Training Epoch: 26 [29900/36045]\tLoss: 596.1843\n",
      "Training Epoch: 26 [29950/36045]\tLoss: 618.6476\n",
      "Training Epoch: 26 [30000/36045]\tLoss: 594.5936\n",
      "Training Epoch: 26 [30050/36045]\tLoss: 600.6755\n",
      "Training Epoch: 26 [30100/36045]\tLoss: 732.7025\n",
      "Training Epoch: 26 [30150/36045]\tLoss: 716.6695\n",
      "Training Epoch: 26 [30200/36045]\tLoss: 675.9912\n",
      "Training Epoch: 26 [30250/36045]\tLoss: 725.3065\n",
      "Training Epoch: 26 [30300/36045]\tLoss: 710.9279\n",
      "Training Epoch: 26 [30350/36045]\tLoss: 554.6035\n",
      "Training Epoch: 26 [30400/36045]\tLoss: 539.4915\n",
      "Training Epoch: 26 [30450/36045]\tLoss: 539.7817\n",
      "Training Epoch: 26 [30500/36045]\tLoss: 504.7531\n",
      "Training Epoch: 26 [30550/36045]\tLoss: 467.5859\n",
      "Training Epoch: 26 [30600/36045]\tLoss: 456.2817\n",
      "Training Epoch: 26 [30650/36045]\tLoss: 446.5388\n",
      "Training Epoch: 26 [30700/36045]\tLoss: 464.2962\n",
      "Training Epoch: 26 [30750/36045]\tLoss: 450.9158\n",
      "Training Epoch: 26 [30800/36045]\tLoss: 478.6179\n",
      "Training Epoch: 26 [30850/36045]\tLoss: 470.3295\n",
      "Training Epoch: 26 [30900/36045]\tLoss: 483.4635\n",
      "Training Epoch: 26 [30950/36045]\tLoss: 508.0539\n",
      "Training Epoch: 26 [31000/36045]\tLoss: 499.3386\n",
      "Training Epoch: 26 [31050/36045]\tLoss: 417.0952\n",
      "Training Epoch: 26 [31100/36045]\tLoss: 407.8259\n",
      "Training Epoch: 26 [31150/36045]\tLoss: 414.4969\n",
      "Training Epoch: 26 [31200/36045]\tLoss: 517.1678\n",
      "Training Epoch: 26 [31250/36045]\tLoss: 670.8998\n",
      "Training Epoch: 26 [31300/36045]\tLoss: 640.8585\n",
      "Training Epoch: 26 [31350/36045]\tLoss: 656.6946\n",
      "Training Epoch: 26 [31400/36045]\tLoss: 635.9363\n",
      "Training Epoch: 26 [31450/36045]\tLoss: 651.1021\n",
      "Training Epoch: 26 [31500/36045]\tLoss: 662.9100\n",
      "Training Epoch: 26 [31550/36045]\tLoss: 671.5065\n",
      "Training Epoch: 26 [31600/36045]\tLoss: 631.0668\n",
      "Training Epoch: 26 [31650/36045]\tLoss: 674.1522\n",
      "Training Epoch: 26 [31700/36045]\tLoss: 489.8865\n",
      "Training Epoch: 26 [31750/36045]\tLoss: 406.1407\n",
      "Training Epoch: 26 [31800/36045]\tLoss: 386.8615\n",
      "Training Epoch: 26 [31850/36045]\tLoss: 396.2412\n",
      "Training Epoch: 26 [31900/36045]\tLoss: 619.1675\n",
      "Training Epoch: 26 [31950/36045]\tLoss: 797.4722\n",
      "Training Epoch: 26 [32000/36045]\tLoss: 909.7952\n",
      "Training Epoch: 26 [32050/36045]\tLoss: 863.6444\n",
      "Training Epoch: 26 [32100/36045]\tLoss: 853.0509\n",
      "Training Epoch: 26 [32150/36045]\tLoss: 665.5403\n",
      "Training Epoch: 26 [32200/36045]\tLoss: 669.7742\n",
      "Training Epoch: 26 [32250/36045]\tLoss: 680.6537\n",
      "Training Epoch: 26 [32300/36045]\tLoss: 662.3831\n",
      "Training Epoch: 26 [32350/36045]\tLoss: 657.2568\n",
      "Training Epoch: 26 [32400/36045]\tLoss: 616.8644\n",
      "Training Epoch: 26 [32450/36045]\tLoss: 508.4208\n",
      "Training Epoch: 26 [32500/36045]\tLoss: 488.9121\n",
      "Training Epoch: 26 [32550/36045]\tLoss: 491.5791\n",
      "Training Epoch: 26 [32600/36045]\tLoss: 487.9755\n",
      "Training Epoch: 26 [32650/36045]\tLoss: 623.3368\n",
      "Training Epoch: 26 [32700/36045]\tLoss: 679.5372\n",
      "Training Epoch: 26 [32750/36045]\tLoss: 647.6907\n",
      "Training Epoch: 26 [32800/36045]\tLoss: 664.4817\n",
      "Training Epoch: 26 [32850/36045]\tLoss: 613.6322\n",
      "Training Epoch: 26 [32900/36045]\tLoss: 493.9988\n",
      "Training Epoch: 26 [32950/36045]\tLoss: 516.8319\n",
      "Training Epoch: 26 [33000/36045]\tLoss: 516.5446\n",
      "Training Epoch: 26 [33050/36045]\tLoss: 490.2242\n",
      "Training Epoch: 26 [33100/36045]\tLoss: 557.4924\n",
      "Training Epoch: 26 [33150/36045]\tLoss: 755.6948\n",
      "Training Epoch: 26 [33200/36045]\tLoss: 736.2983\n",
      "Training Epoch: 26 [33250/36045]\tLoss: 758.5375\n",
      "Training Epoch: 26 [33300/36045]\tLoss: 807.4047\n",
      "Training Epoch: 26 [33350/36045]\tLoss: 619.7454\n",
      "Training Epoch: 26 [33400/36045]\tLoss: 456.0592\n",
      "Training Epoch: 26 [33450/36045]\tLoss: 451.1817\n",
      "Training Epoch: 26 [33500/36045]\tLoss: 464.3901\n",
      "Training Epoch: 26 [33550/36045]\tLoss: 481.6911\n",
      "Training Epoch: 26 [33600/36045]\tLoss: 483.2416\n",
      "Training Epoch: 26 [33650/36045]\tLoss: 643.7079\n",
      "Training Epoch: 26 [33700/36045]\tLoss: 622.9949\n",
      "Training Epoch: 26 [33750/36045]\tLoss: 645.1628\n",
      "Training Epoch: 26 [33800/36045]\tLoss: 640.4194\n",
      "Training Epoch: 26 [33850/36045]\tLoss: 643.2296\n",
      "Training Epoch: 26 [33900/36045]\tLoss: 654.6179\n",
      "Training Epoch: 26 [33950/36045]\tLoss: 666.0343\n",
      "Training Epoch: 26 [34000/36045]\tLoss: 653.0833\n",
      "Training Epoch: 26 [34050/36045]\tLoss: 657.4748\n",
      "Training Epoch: 26 [34100/36045]\tLoss: 632.7212\n",
      "Training Epoch: 26 [34150/36045]\tLoss: 588.3551\n",
      "Training Epoch: 26 [34200/36045]\tLoss: 557.0176\n",
      "Training Epoch: 26 [34250/36045]\tLoss: 571.3888\n",
      "Training Epoch: 26 [34300/36045]\tLoss: 489.4380\n",
      "Training Epoch: 26 [34350/36045]\tLoss: 515.1833\n",
      "Training Epoch: 26 [34400/36045]\tLoss: 506.2585\n",
      "Training Epoch: 26 [34450/36045]\tLoss: 475.6335\n",
      "Training Epoch: 26 [34500/36045]\tLoss: 507.8489\n",
      "Training Epoch: 26 [34550/36045]\tLoss: 498.4307\n",
      "Training Epoch: 26 [34600/36045]\tLoss: 500.9543\n",
      "Training Epoch: 26 [34650/36045]\tLoss: 609.6433\n",
      "Training Epoch: 26 [34700/36045]\tLoss: 645.2939\n",
      "Training Epoch: 26 [34750/36045]\tLoss: 573.0685\n",
      "Training Epoch: 26 [34800/36045]\tLoss: 654.9329\n",
      "Training Epoch: 26 [34850/36045]\tLoss: 663.3130\n",
      "Training Epoch: 26 [34900/36045]\tLoss: 730.6132\n",
      "Training Epoch: 26 [34950/36045]\tLoss: 717.5994\n",
      "Training Epoch: 26 [35000/36045]\tLoss: 718.6510\n",
      "Training Epoch: 26 [35050/36045]\tLoss: 704.4971\n",
      "Training Epoch: 26 [35100/36045]\tLoss: 593.1890\n",
      "Training Epoch: 26 [35150/36045]\tLoss: 586.2734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 26 [35200/36045]\tLoss: 497.5288\n",
      "Training Epoch: 26 [35250/36045]\tLoss: 546.1864\n",
      "Training Epoch: 26 [35300/36045]\tLoss: 560.3390\n",
      "Training Epoch: 26 [35350/36045]\tLoss: 636.5367\n",
      "Training Epoch: 26 [35400/36045]\tLoss: 672.3075\n",
      "Training Epoch: 26 [35450/36045]\tLoss: 641.7531\n",
      "Training Epoch: 26 [35500/36045]\tLoss: 623.3734\n",
      "Training Epoch: 26 [35550/36045]\tLoss: 608.4060\n",
      "Training Epoch: 26 [35600/36045]\tLoss: 656.1586\n",
      "Training Epoch: 26 [35650/36045]\tLoss: 730.3073\n",
      "Training Epoch: 26 [35700/36045]\tLoss: 657.5753\n",
      "Training Epoch: 26 [35750/36045]\tLoss: 716.2321\n",
      "Training Epoch: 26 [35800/36045]\tLoss: 722.6549\n",
      "Training Epoch: 26 [35850/36045]\tLoss: 696.7926\n",
      "Training Epoch: 26 [35900/36045]\tLoss: 723.2628\n",
      "Training Epoch: 26 [35950/36045]\tLoss: 720.9273\n",
      "Training Epoch: 26 [36000/36045]\tLoss: 712.7010\n",
      "Training Epoch: 26 [36045/36045]\tLoss: 695.6991\n",
      "Training Epoch: 26 [4004/4004]\tLoss: 648.7353\n",
      "Training Epoch: 27 [50/36045]\tLoss: 648.5413\n",
      "Training Epoch: 27 [100/36045]\tLoss: 621.8826\n",
      "Training Epoch: 27 [150/36045]\tLoss: 619.9429\n",
      "Training Epoch: 27 [200/36045]\tLoss: 606.8016\n",
      "Training Epoch: 27 [250/36045]\tLoss: 723.0095\n",
      "Training Epoch: 27 [300/36045]\tLoss: 786.8034\n",
      "Training Epoch: 27 [350/36045]\tLoss: 752.4247\n",
      "Training Epoch: 27 [400/36045]\tLoss: 748.2922\n",
      "Training Epoch: 27 [450/36045]\tLoss: 728.1252\n",
      "Training Epoch: 27 [500/36045]\tLoss: 677.7488\n",
      "Training Epoch: 27 [550/36045]\tLoss: 681.9950\n",
      "Training Epoch: 27 [600/36045]\tLoss: 662.5711\n",
      "Training Epoch: 27 [650/36045]\tLoss: 686.4255\n",
      "Training Epoch: 27 [700/36045]\tLoss: 673.6395\n",
      "Training Epoch: 27 [750/36045]\tLoss: 654.7993\n",
      "Training Epoch: 27 [800/36045]\tLoss: 669.0378\n",
      "Training Epoch: 27 [850/36045]\tLoss: 649.8522\n",
      "Training Epoch: 27 [900/36045]\tLoss: 619.1406\n",
      "Training Epoch: 27 [950/36045]\tLoss: 586.6902\n",
      "Training Epoch: 27 [1000/36045]\tLoss: 565.9968\n",
      "Training Epoch: 27 [1050/36045]\tLoss: 568.2299\n",
      "Training Epoch: 27 [1100/36045]\tLoss: 553.1533\n",
      "Training Epoch: 27 [1150/36045]\tLoss: 562.0146\n",
      "Training Epoch: 27 [1200/36045]\tLoss: 593.1775\n",
      "Training Epoch: 27 [1250/36045]\tLoss: 678.6346\n",
      "Training Epoch: 27 [1300/36045]\tLoss: 684.9897\n",
      "Training Epoch: 27 [1350/36045]\tLoss: 687.5663\n",
      "Training Epoch: 27 [1400/36045]\tLoss: 714.6793\n",
      "Training Epoch: 27 [1450/36045]\tLoss: 691.2985\n",
      "Training Epoch: 27 [1500/36045]\tLoss: 634.9354\n",
      "Training Epoch: 27 [1550/36045]\tLoss: 650.4754\n",
      "Training Epoch: 27 [1600/36045]\tLoss: 661.3751\n",
      "Training Epoch: 27 [1650/36045]\tLoss: 648.2838\n",
      "Training Epoch: 27 [1700/36045]\tLoss: 660.4915\n",
      "Training Epoch: 27 [1750/36045]\tLoss: 702.3087\n",
      "Training Epoch: 27 [1800/36045]\tLoss: 683.3041\n",
      "Training Epoch: 27 [1850/36045]\tLoss: 700.8525\n",
      "Training Epoch: 27 [1900/36045]\tLoss: 655.9784\n",
      "Training Epoch: 27 [1950/36045]\tLoss: 667.0578\n",
      "Training Epoch: 27 [2000/36045]\tLoss: 603.5410\n",
      "Training Epoch: 27 [2050/36045]\tLoss: 606.3911\n",
      "Training Epoch: 27 [2100/36045]\tLoss: 638.8204\n",
      "Training Epoch: 27 [2150/36045]\tLoss: 617.7350\n",
      "Training Epoch: 27 [2200/36045]\tLoss: 573.6672\n",
      "Training Epoch: 27 [2250/36045]\tLoss: 541.7938\n",
      "Training Epoch: 27 [2300/36045]\tLoss: 568.3464\n",
      "Training Epoch: 27 [2350/36045]\tLoss: 543.1413\n",
      "Training Epoch: 27 [2400/36045]\tLoss: 552.6532\n",
      "Training Epoch: 27 [2450/36045]\tLoss: 703.4443\n",
      "Training Epoch: 27 [2500/36045]\tLoss: 739.1537\n",
      "Training Epoch: 27 [2550/36045]\tLoss: 736.0865\n",
      "Training Epoch: 27 [2600/36045]\tLoss: 745.0726\n",
      "Training Epoch: 27 [2650/36045]\tLoss: 870.6264\n",
      "Training Epoch: 27 [2700/36045]\tLoss: 957.8873\n",
      "Training Epoch: 27 [2750/36045]\tLoss: 1030.2147\n",
      "Training Epoch: 27 [2800/36045]\tLoss: 1040.1710\n",
      "Training Epoch: 27 [2850/36045]\tLoss: 808.7980\n",
      "Training Epoch: 27 [2900/36045]\tLoss: 773.9091\n",
      "Training Epoch: 27 [2950/36045]\tLoss: 747.1573\n",
      "Training Epoch: 27 [3000/36045]\tLoss: 741.9173\n",
      "Training Epoch: 27 [3050/36045]\tLoss: 772.9200\n",
      "Training Epoch: 27 [3100/36045]\tLoss: 707.3521\n",
      "Training Epoch: 27 [3150/36045]\tLoss: 545.8420\n",
      "Training Epoch: 27 [3200/36045]\tLoss: 566.3783\n",
      "Training Epoch: 27 [3250/36045]\tLoss: 533.2845\n",
      "Training Epoch: 27 [3300/36045]\tLoss: 504.9839\n",
      "Training Epoch: 27 [3350/36045]\tLoss: 532.5026\n",
      "Training Epoch: 27 [3400/36045]\tLoss: 559.1465\n",
      "Training Epoch: 27 [3450/36045]\tLoss: 600.3066\n",
      "Training Epoch: 27 [3500/36045]\tLoss: 587.4899\n",
      "Training Epoch: 27 [3550/36045]\tLoss: 563.6906\n",
      "Training Epoch: 27 [3600/36045]\tLoss: 603.9662\n",
      "Training Epoch: 27 [3650/36045]\tLoss: 697.9662\n",
      "Training Epoch: 27 [3700/36045]\tLoss: 704.2704\n",
      "Training Epoch: 27 [3750/36045]\tLoss: 672.6660\n",
      "Training Epoch: 27 [3800/36045]\tLoss: 666.9264\n",
      "Training Epoch: 27 [3850/36045]\tLoss: 666.2253\n",
      "Training Epoch: 27 [3900/36045]\tLoss: 671.6789\n",
      "Training Epoch: 27 [3950/36045]\tLoss: 647.8396\n",
      "Training Epoch: 27 [4000/36045]\tLoss: 654.2031\n",
      "Training Epoch: 27 [4050/36045]\tLoss: 601.3529\n",
      "Training Epoch: 27 [4100/36045]\tLoss: 586.1414\n",
      "Training Epoch: 27 [4150/36045]\tLoss: 602.8540\n",
      "Training Epoch: 27 [4200/36045]\tLoss: 597.4351\n",
      "Training Epoch: 27 [4250/36045]\tLoss: 599.6512\n",
      "Training Epoch: 27 [4300/36045]\tLoss: 618.2183\n",
      "Training Epoch: 27 [4350/36045]\tLoss: 600.5249\n",
      "Training Epoch: 27 [4400/36045]\tLoss: 575.0711\n",
      "Training Epoch: 27 [4450/36045]\tLoss: 628.5829\n",
      "Training Epoch: 27 [4500/36045]\tLoss: 672.5580\n",
      "Training Epoch: 27 [4550/36045]\tLoss: 677.7748\n",
      "Training Epoch: 27 [4600/36045]\tLoss: 700.8950\n",
      "Training Epoch: 27 [4650/36045]\tLoss: 690.0178\n",
      "Training Epoch: 27 [4700/36045]\tLoss: 637.6794\n",
      "Training Epoch: 27 [4750/36045]\tLoss: 620.1066\n",
      "Training Epoch: 27 [4800/36045]\tLoss: 646.4549\n",
      "Training Epoch: 27 [4850/36045]\tLoss: 632.8732\n",
      "Training Epoch: 27 [4900/36045]\tLoss: 615.4762\n",
      "Training Epoch: 27 [4950/36045]\tLoss: 632.7108\n",
      "Training Epoch: 27 [5000/36045]\tLoss: 663.6950\n",
      "Training Epoch: 27 [5050/36045]\tLoss: 642.8280\n",
      "Training Epoch: 27 [5100/36045]\tLoss: 653.2905\n",
      "Training Epoch: 27 [5150/36045]\tLoss: 637.6275\n",
      "Training Epoch: 27 [5200/36045]\tLoss: 635.5790\n",
      "Training Epoch: 27 [5250/36045]\tLoss: 628.8694\n",
      "Training Epoch: 27 [5300/36045]\tLoss: 629.7116\n",
      "Training Epoch: 27 [5350/36045]\tLoss: 652.9561\n",
      "Training Epoch: 27 [5400/36045]\tLoss: 628.4995\n",
      "Training Epoch: 27 [5450/36045]\tLoss: 596.0748\n",
      "Training Epoch: 27 [5500/36045]\tLoss: 625.5038\n",
      "Training Epoch: 27 [5550/36045]\tLoss: 613.5007\n",
      "Training Epoch: 27 [5600/36045]\tLoss: 697.1738\n",
      "Training Epoch: 27 [5650/36045]\tLoss: 660.5120\n",
      "Training Epoch: 27 [5700/36045]\tLoss: 620.1242\n",
      "Training Epoch: 27 [5750/36045]\tLoss: 604.8417\n",
      "Training Epoch: 27 [5800/36045]\tLoss: 638.2107\n",
      "Training Epoch: 27 [5850/36045]\tLoss: 624.3230\n",
      "Training Epoch: 27 [5900/36045]\tLoss: 718.4520\n",
      "Training Epoch: 27 [5950/36045]\tLoss: 736.1125\n",
      "Training Epoch: 27 [6000/36045]\tLoss: 720.9024\n",
      "Training Epoch: 27 [6050/36045]\tLoss: 697.1187\n",
      "Training Epoch: 27 [6100/36045]\tLoss: 701.9258\n",
      "Training Epoch: 27 [6150/36045]\tLoss: 687.2841\n",
      "Training Epoch: 27 [6200/36045]\tLoss: 688.9767\n",
      "Training Epoch: 27 [6250/36045]\tLoss: 710.2369\n",
      "Training Epoch: 27 [6300/36045]\tLoss: 722.0553\n",
      "Training Epoch: 27 [6350/36045]\tLoss: 770.1046\n",
      "Training Epoch: 27 [6400/36045]\tLoss: 640.5174\n",
      "Training Epoch: 27 [6450/36045]\tLoss: 592.1282\n",
      "Training Epoch: 27 [6500/36045]\tLoss: 603.1013\n",
      "Training Epoch: 27 [6550/36045]\tLoss: 620.1448\n",
      "Training Epoch: 27 [6600/36045]\tLoss: 619.7111\n",
      "Training Epoch: 27 [6650/36045]\tLoss: 699.3446\n",
      "Training Epoch: 27 [6700/36045]\tLoss: 731.8069\n",
      "Training Epoch: 27 [6750/36045]\tLoss: 706.7569\n",
      "Training Epoch: 27 [6800/36045]\tLoss: 709.8542\n",
      "Training Epoch: 27 [6850/36045]\tLoss: 697.7156\n",
      "Training Epoch: 27 [6900/36045]\tLoss: 621.4063\n",
      "Training Epoch: 27 [6950/36045]\tLoss: 585.8566\n",
      "Training Epoch: 27 [7000/36045]\tLoss: 623.2462\n",
      "Training Epoch: 27 [7050/36045]\tLoss: 637.0723\n",
      "Training Epoch: 27 [7100/36045]\tLoss: 636.3389\n",
      "Training Epoch: 27 [7150/36045]\tLoss: 647.3040\n",
      "Training Epoch: 27 [7200/36045]\tLoss: 651.1642\n",
      "Training Epoch: 27 [7250/36045]\tLoss: 648.6992\n",
      "Training Epoch: 27 [7300/36045]\tLoss: 635.7289\n",
      "Training Epoch: 27 [7350/36045]\tLoss: 631.6243\n",
      "Training Epoch: 27 [7400/36045]\tLoss: 572.2658\n",
      "Training Epoch: 27 [7450/36045]\tLoss: 575.8476\n",
      "Training Epoch: 27 [7500/36045]\tLoss: 570.4749\n",
      "Training Epoch: 27 [7550/36045]\tLoss: 546.8062\n",
      "Training Epoch: 27 [7600/36045]\tLoss: 607.5150\n",
      "Training Epoch: 27 [7650/36045]\tLoss: 651.7042\n",
      "Training Epoch: 27 [7700/36045]\tLoss: 620.6876\n",
      "Training Epoch: 27 [7750/36045]\tLoss: 635.4377\n",
      "Training Epoch: 27 [7800/36045]\tLoss: 623.6409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [7850/36045]\tLoss: 603.0901\n",
      "Training Epoch: 27 [7900/36045]\tLoss: 636.3055\n",
      "Training Epoch: 27 [7950/36045]\tLoss: 633.4788\n",
      "Training Epoch: 27 [8000/36045]\tLoss: 651.6455\n",
      "Training Epoch: 27 [8050/36045]\tLoss: 615.4115\n",
      "Training Epoch: 27 [8100/36045]\tLoss: 641.2869\n",
      "Training Epoch: 27 [8150/36045]\tLoss: 725.7739\n",
      "Training Epoch: 27 [8200/36045]\tLoss: 712.0604\n",
      "Training Epoch: 27 [8250/36045]\tLoss: 679.3129\n",
      "Training Epoch: 27 [8300/36045]\tLoss: 739.9534\n",
      "Training Epoch: 27 [8350/36045]\tLoss: 680.0742\n",
      "Training Epoch: 27 [8400/36045]\tLoss: 609.6332\n",
      "Training Epoch: 27 [8450/36045]\tLoss: 571.3613\n",
      "Training Epoch: 27 [8500/36045]\tLoss: 606.7390\n",
      "Training Epoch: 27 [8550/36045]\tLoss: 598.5201\n",
      "Training Epoch: 27 [8600/36045]\tLoss: 592.1196\n",
      "Training Epoch: 27 [8650/36045]\tLoss: 632.0140\n",
      "Training Epoch: 27 [8700/36045]\tLoss: 668.1638\n",
      "Training Epoch: 27 [8750/36045]\tLoss: 655.9821\n",
      "Training Epoch: 27 [8800/36045]\tLoss: 661.7065\n",
      "Training Epoch: 27 [8850/36045]\tLoss: 654.8403\n",
      "Training Epoch: 27 [8900/36045]\tLoss: 591.0306\n",
      "Training Epoch: 27 [8950/36045]\tLoss: 603.9347\n",
      "Training Epoch: 27 [9000/36045]\tLoss: 619.5634\n",
      "Training Epoch: 27 [9050/36045]\tLoss: 620.4396\n",
      "Training Epoch: 27 [9100/36045]\tLoss: 638.6837\n",
      "Training Epoch: 27 [9150/36045]\tLoss: 471.8510\n",
      "Training Epoch: 27 [9200/36045]\tLoss: 353.9846\n",
      "Training Epoch: 27 [9250/36045]\tLoss: 383.8417\n",
      "Training Epoch: 27 [9300/36045]\tLoss: 395.0547\n",
      "Training Epoch: 27 [9350/36045]\tLoss: 364.0407\n",
      "Training Epoch: 27 [9400/36045]\tLoss: 713.3641\n",
      "Training Epoch: 27 [9450/36045]\tLoss: 757.6868\n",
      "Training Epoch: 27 [9500/36045]\tLoss: 744.5496\n",
      "Training Epoch: 27 [9550/36045]\tLoss: 787.5237\n",
      "Training Epoch: 27 [9600/36045]\tLoss: 585.1773\n",
      "Training Epoch: 27 [9650/36045]\tLoss: 589.2532\n",
      "Training Epoch: 27 [9700/36045]\tLoss: 574.4821\n",
      "Training Epoch: 27 [9750/36045]\tLoss: 574.0911\n",
      "Training Epoch: 27 [9800/36045]\tLoss: 749.5857\n",
      "Training Epoch: 27 [9850/36045]\tLoss: 791.7013\n",
      "Training Epoch: 27 [9900/36045]\tLoss: 805.2189\n",
      "Training Epoch: 27 [9950/36045]\tLoss: 784.3909\n",
      "Training Epoch: 27 [10000/36045]\tLoss: 724.6221\n",
      "Training Epoch: 27 [10050/36045]\tLoss: 597.7636\n",
      "Training Epoch: 27 [10100/36045]\tLoss: 604.6494\n",
      "Training Epoch: 27 [10150/36045]\tLoss: 614.3204\n",
      "Training Epoch: 27 [10200/36045]\tLoss: 603.1836\n",
      "Training Epoch: 27 [10250/36045]\tLoss: 720.5499\n",
      "Training Epoch: 27 [10300/36045]\tLoss: 699.7779\n",
      "Training Epoch: 27 [10350/36045]\tLoss: 736.5673\n",
      "Training Epoch: 27 [10400/36045]\tLoss: 727.0345\n",
      "Training Epoch: 27 [10450/36045]\tLoss: 680.9202\n",
      "Training Epoch: 27 [10500/36045]\tLoss: 570.3070\n",
      "Training Epoch: 27 [10550/36045]\tLoss: 565.5388\n",
      "Training Epoch: 27 [10600/36045]\tLoss: 588.6195\n",
      "Training Epoch: 27 [10650/36045]\tLoss: 594.7961\n",
      "Training Epoch: 27 [10700/36045]\tLoss: 680.5613\n",
      "Training Epoch: 27 [10750/36045]\tLoss: 742.3987\n",
      "Training Epoch: 27 [10800/36045]\tLoss: 685.6170\n",
      "Training Epoch: 27 [10850/36045]\tLoss: 726.1138\n",
      "Training Epoch: 27 [10900/36045]\tLoss: 755.5782\n",
      "Training Epoch: 27 [10950/36045]\tLoss: 558.7565\n",
      "Training Epoch: 27 [11000/36045]\tLoss: 552.4189\n",
      "Training Epoch: 27 [11050/36045]\tLoss: 591.4312\n",
      "Training Epoch: 27 [11100/36045]\tLoss: 602.9556\n",
      "Training Epoch: 27 [11150/36045]\tLoss: 653.5524\n",
      "Training Epoch: 27 [11200/36045]\tLoss: 681.7167\n",
      "Training Epoch: 27 [11250/36045]\tLoss: 693.8484\n",
      "Training Epoch: 27 [11300/36045]\tLoss: 673.8459\n",
      "Training Epoch: 27 [11350/36045]\tLoss: 670.9290\n",
      "Training Epoch: 27 [11400/36045]\tLoss: 632.0782\n",
      "Training Epoch: 27 [11450/36045]\tLoss: 599.1719\n",
      "Training Epoch: 27 [11500/36045]\tLoss: 596.6884\n",
      "Training Epoch: 27 [11550/36045]\tLoss: 608.7100\n",
      "Training Epoch: 27 [11600/36045]\tLoss: 670.9793\n",
      "Training Epoch: 27 [11650/36045]\tLoss: 723.5061\n",
      "Training Epoch: 27 [11700/36045]\tLoss: 722.4698\n",
      "Training Epoch: 27 [11750/36045]\tLoss: 741.7416\n",
      "Training Epoch: 27 [11800/36045]\tLoss: 784.7780\n",
      "Training Epoch: 27 [11850/36045]\tLoss: 841.7374\n",
      "Training Epoch: 27 [11900/36045]\tLoss: 1058.6534\n",
      "Training Epoch: 27 [11950/36045]\tLoss: 1060.5625\n",
      "Training Epoch: 27 [12000/36045]\tLoss: 1074.2491\n",
      "Training Epoch: 27 [12050/36045]\tLoss: 1032.4219\n",
      "Training Epoch: 27 [12100/36045]\tLoss: 673.5828\n",
      "Training Epoch: 27 [12150/36045]\tLoss: 515.9439\n",
      "Training Epoch: 27 [12200/36045]\tLoss: 510.4556\n",
      "Training Epoch: 27 [12250/36045]\tLoss: 519.6499\n",
      "Training Epoch: 27 [12300/36045]\tLoss: 662.5649\n",
      "Training Epoch: 27 [12350/36045]\tLoss: 720.3376\n",
      "Training Epoch: 27 [12400/36045]\tLoss: 728.2426\n",
      "Training Epoch: 27 [12450/36045]\tLoss: 716.1586\n",
      "Training Epoch: 27 [12500/36045]\tLoss: 744.9978\n",
      "Training Epoch: 27 [12550/36045]\tLoss: 713.7005\n",
      "Training Epoch: 27 [12600/36045]\tLoss: 657.2236\n",
      "Training Epoch: 27 [12650/36045]\tLoss: 656.1018\n",
      "Training Epoch: 27 [12700/36045]\tLoss: 677.6220\n",
      "Training Epoch: 27 [12750/36045]\tLoss: 676.9981\n",
      "Training Epoch: 27 [12800/36045]\tLoss: 660.0133\n",
      "Training Epoch: 27 [12850/36045]\tLoss: 689.7050\n",
      "Training Epoch: 27 [12900/36045]\tLoss: 661.7309\n",
      "Training Epoch: 27 [12950/36045]\tLoss: 649.0739\n",
      "Training Epoch: 27 [13000/36045]\tLoss: 681.4669\n",
      "Training Epoch: 27 [13050/36045]\tLoss: 618.8868\n",
      "Training Epoch: 27 [13100/36045]\tLoss: 637.9596\n",
      "Training Epoch: 27 [13150/36045]\tLoss: 629.6807\n",
      "Training Epoch: 27 [13200/36045]\tLoss: 609.8327\n",
      "Training Epoch: 27 [13250/36045]\tLoss: 634.8589\n",
      "Training Epoch: 27 [13300/36045]\tLoss: 674.4103\n",
      "Training Epoch: 27 [13350/36045]\tLoss: 653.9369\n",
      "Training Epoch: 27 [13400/36045]\tLoss: 657.6784\n",
      "Training Epoch: 27 [13450/36045]\tLoss: 653.8109\n",
      "Training Epoch: 27 [13500/36045]\tLoss: 674.9971\n",
      "Training Epoch: 27 [13550/36045]\tLoss: 811.0789\n",
      "Training Epoch: 27 [13600/36045]\tLoss: 843.9348\n",
      "Training Epoch: 27 [13650/36045]\tLoss: 924.3425\n",
      "Training Epoch: 27 [13700/36045]\tLoss: 817.7799\n",
      "Training Epoch: 27 [13750/36045]\tLoss: 660.5856\n",
      "Training Epoch: 27 [13800/36045]\tLoss: 633.1478\n",
      "Training Epoch: 27 [13850/36045]\tLoss: 616.1538\n",
      "Training Epoch: 27 [13900/36045]\tLoss: 623.5702\n",
      "Training Epoch: 27 [13950/36045]\tLoss: 669.2289\n",
      "Training Epoch: 27 [14000/36045]\tLoss: 703.3307\n",
      "Training Epoch: 27 [14050/36045]\tLoss: 676.8915\n",
      "Training Epoch: 27 [14100/36045]\tLoss: 672.5111\n",
      "Training Epoch: 27 [14150/36045]\tLoss: 660.3658\n",
      "Training Epoch: 27 [14200/36045]\tLoss: 702.8802\n",
      "Training Epoch: 27 [14250/36045]\tLoss: 770.8542\n",
      "Training Epoch: 27 [14300/36045]\tLoss: 774.1548\n",
      "Training Epoch: 27 [14350/36045]\tLoss: 741.0710\n",
      "Training Epoch: 27 [14400/36045]\tLoss: 726.5900\n",
      "Training Epoch: 27 [14450/36045]\tLoss: 763.8102\n",
      "Training Epoch: 27 [14500/36045]\tLoss: 694.5168\n",
      "Training Epoch: 27 [14550/36045]\tLoss: 724.9036\n",
      "Training Epoch: 27 [14600/36045]\tLoss: 710.1891\n",
      "Training Epoch: 27 [14650/36045]\tLoss: 710.5720\n",
      "Training Epoch: 27 [14700/36045]\tLoss: 671.5717\n",
      "Training Epoch: 27 [14750/36045]\tLoss: 576.7567\n",
      "Training Epoch: 27 [14800/36045]\tLoss: 566.8542\n",
      "Training Epoch: 27 [14850/36045]\tLoss: 573.9808\n",
      "Training Epoch: 27 [14900/36045]\tLoss: 567.4774\n",
      "Training Epoch: 27 [14950/36045]\tLoss: 575.2802\n",
      "Training Epoch: 27 [15000/36045]\tLoss: 590.1347\n",
      "Training Epoch: 27 [15050/36045]\tLoss: 587.8932\n",
      "Training Epoch: 27 [15100/36045]\tLoss: 571.8898\n",
      "Training Epoch: 27 [15150/36045]\tLoss: 565.5353\n",
      "Training Epoch: 27 [15200/36045]\tLoss: 523.2348\n",
      "Training Epoch: 27 [15250/36045]\tLoss: 546.6824\n",
      "Training Epoch: 27 [15300/36045]\tLoss: 531.4268\n",
      "Training Epoch: 27 [15350/36045]\tLoss: 543.8499\n",
      "Training Epoch: 27 [15400/36045]\tLoss: 528.2067\n",
      "Training Epoch: 27 [15450/36045]\tLoss: 513.9636\n",
      "Training Epoch: 27 [15500/36045]\tLoss: 528.9741\n",
      "Training Epoch: 27 [15550/36045]\tLoss: 524.5277\n",
      "Training Epoch: 27 [15600/36045]\tLoss: 594.8025\n",
      "Training Epoch: 27 [15650/36045]\tLoss: 613.2918\n",
      "Training Epoch: 27 [15700/36045]\tLoss: 603.9813\n",
      "Training Epoch: 27 [15750/36045]\tLoss: 595.8530\n",
      "Training Epoch: 27 [15800/36045]\tLoss: 562.4406\n",
      "Training Epoch: 27 [15850/36045]\tLoss: 576.4805\n",
      "Training Epoch: 27 [15900/36045]\tLoss: 586.0995\n",
      "Training Epoch: 27 [15950/36045]\tLoss: 605.9582\n",
      "Training Epoch: 27 [16000/36045]\tLoss: 579.5905\n",
      "Training Epoch: 27 [16050/36045]\tLoss: 548.9802\n",
      "Training Epoch: 27 [16100/36045]\tLoss: 508.3360\n",
      "Training Epoch: 27 [16150/36045]\tLoss: 496.0073\n",
      "Training Epoch: 27 [16200/36045]\tLoss: 600.4272\n",
      "Training Epoch: 27 [16250/36045]\tLoss: 629.5379\n",
      "Training Epoch: 27 [16300/36045]\tLoss: 687.3739\n",
      "Training Epoch: 27 [16350/36045]\tLoss: 705.8973\n",
      "Training Epoch: 27 [16400/36045]\tLoss: 677.9850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [16450/36045]\tLoss: 659.4881\n",
      "Training Epoch: 27 [16500/36045]\tLoss: 659.1295\n",
      "Training Epoch: 27 [16550/36045]\tLoss: 623.4153\n",
      "Training Epoch: 27 [16600/36045]\tLoss: 648.2019\n",
      "Training Epoch: 27 [16650/36045]\tLoss: 666.8770\n",
      "Training Epoch: 27 [16700/36045]\tLoss: 644.1357\n",
      "Training Epoch: 27 [16750/36045]\tLoss: 636.1440\n",
      "Training Epoch: 27 [16800/36045]\tLoss: 647.2336\n",
      "Training Epoch: 27 [16850/36045]\tLoss: 616.2784\n",
      "Training Epoch: 27 [16900/36045]\tLoss: 626.6688\n",
      "Training Epoch: 27 [16950/36045]\tLoss: 651.7249\n",
      "Training Epoch: 27 [17000/36045]\tLoss: 634.0722\n",
      "Training Epoch: 27 [17050/36045]\tLoss: 661.5313\n",
      "Training Epoch: 27 [17100/36045]\tLoss: 658.2987\n",
      "Training Epoch: 27 [17150/36045]\tLoss: 572.5677\n",
      "Training Epoch: 27 [17200/36045]\tLoss: 533.0369\n",
      "Training Epoch: 27 [17250/36045]\tLoss: 557.7429\n",
      "Training Epoch: 27 [17300/36045]\tLoss: 589.8199\n",
      "Training Epoch: 27 [17350/36045]\tLoss: 566.7849\n",
      "Training Epoch: 27 [17400/36045]\tLoss: 586.2029\n",
      "Training Epoch: 27 [17450/36045]\tLoss: 606.3629\n",
      "Training Epoch: 27 [17500/36045]\tLoss: 594.2336\n",
      "Training Epoch: 27 [17550/36045]\tLoss: 593.8301\n",
      "Training Epoch: 27 [17600/36045]\tLoss: 586.6172\n",
      "Training Epoch: 27 [17650/36045]\tLoss: 603.6451\n",
      "Training Epoch: 27 [17700/36045]\tLoss: 582.2853\n",
      "Training Epoch: 27 [17750/36045]\tLoss: 599.3076\n",
      "Training Epoch: 27 [17800/36045]\tLoss: 589.8948\n",
      "Training Epoch: 27 [17850/36045]\tLoss: 599.4487\n",
      "Training Epoch: 27 [17900/36045]\tLoss: 628.2826\n",
      "Training Epoch: 27 [17950/36045]\tLoss: 639.8197\n",
      "Training Epoch: 27 [18000/36045]\tLoss: 629.8838\n",
      "Training Epoch: 27 [18050/36045]\tLoss: 698.0427\n",
      "Training Epoch: 27 [18100/36045]\tLoss: 700.3929\n",
      "Training Epoch: 27 [18150/36045]\tLoss: 711.3639\n",
      "Training Epoch: 27 [18200/36045]\tLoss: 693.5054\n",
      "Training Epoch: 27 [18250/36045]\tLoss: 714.4897\n",
      "Training Epoch: 27 [18300/36045]\tLoss: 663.0464\n",
      "Training Epoch: 27 [18350/36045]\tLoss: 733.5668\n",
      "Training Epoch: 27 [18400/36045]\tLoss: 706.6466\n",
      "Training Epoch: 27 [18450/36045]\tLoss: 686.9453\n",
      "Training Epoch: 27 [18500/36045]\tLoss: 686.0592\n",
      "Training Epoch: 27 [18550/36045]\tLoss: 672.9058\n",
      "Training Epoch: 27 [18600/36045]\tLoss: 662.4260\n",
      "Training Epoch: 27 [18650/36045]\tLoss: 709.4127\n",
      "Training Epoch: 27 [18700/36045]\tLoss: 746.5279\n",
      "Training Epoch: 27 [18750/36045]\tLoss: 732.5515\n",
      "Training Epoch: 27 [18800/36045]\tLoss: 756.6940\n",
      "Training Epoch: 27 [18850/36045]\tLoss: 700.9062\n",
      "Training Epoch: 27 [18900/36045]\tLoss: 749.7374\n",
      "Training Epoch: 27 [18950/36045]\tLoss: 690.8461\n",
      "Training Epoch: 27 [19000/36045]\tLoss: 579.2745\n",
      "Training Epoch: 27 [19050/36045]\tLoss: 561.5547\n",
      "Training Epoch: 27 [19100/36045]\tLoss: 570.7220\n",
      "Training Epoch: 27 [19150/36045]\tLoss: 559.8584\n",
      "Training Epoch: 27 [19200/36045]\tLoss: 588.2797\n",
      "Training Epoch: 27 [19250/36045]\tLoss: 602.8832\n",
      "Training Epoch: 27 [19300/36045]\tLoss: 613.5261\n",
      "Training Epoch: 27 [19350/36045]\tLoss: 597.0671\n",
      "Training Epoch: 27 [19400/36045]\tLoss: 619.2227\n",
      "Training Epoch: 27 [19450/36045]\tLoss: 609.9026\n",
      "Training Epoch: 27 [19500/36045]\tLoss: 611.7285\n",
      "Training Epoch: 27 [19550/36045]\tLoss: 610.1767\n",
      "Training Epoch: 27 [19600/36045]\tLoss: 651.5490\n",
      "Training Epoch: 27 [19650/36045]\tLoss: 859.9415\n",
      "Training Epoch: 27 [19700/36045]\tLoss: 818.8046\n",
      "Training Epoch: 27 [19750/36045]\tLoss: 821.5311\n",
      "Training Epoch: 27 [19800/36045]\tLoss: 820.2460\n",
      "Training Epoch: 27 [19850/36045]\tLoss: 546.3248\n",
      "Training Epoch: 27 [19900/36045]\tLoss: 524.2927\n",
      "Training Epoch: 27 [19950/36045]\tLoss: 527.8622\n",
      "Training Epoch: 27 [20000/36045]\tLoss: 526.5145\n",
      "Training Epoch: 27 [20050/36045]\tLoss: 589.5425\n",
      "Training Epoch: 27 [20100/36045]\tLoss: 595.8091\n",
      "Training Epoch: 27 [20150/36045]\tLoss: 598.2401\n",
      "Training Epoch: 27 [20200/36045]\tLoss: 598.3525\n",
      "Training Epoch: 27 [20250/36045]\tLoss: 637.0820\n",
      "Training Epoch: 27 [20300/36045]\tLoss: 673.3024\n",
      "Training Epoch: 27 [20350/36045]\tLoss: 692.4648\n",
      "Training Epoch: 27 [20400/36045]\tLoss: 709.0511\n",
      "Training Epoch: 27 [20450/36045]\tLoss: 679.8658\n",
      "Training Epoch: 27 [20500/36045]\tLoss: 663.7640\n",
      "Training Epoch: 27 [20550/36045]\tLoss: 584.2638\n",
      "Training Epoch: 27 [20600/36045]\tLoss: 595.4118\n",
      "Training Epoch: 27 [20650/36045]\tLoss: 591.9970\n",
      "Training Epoch: 27 [20700/36045]\tLoss: 579.7335\n",
      "Training Epoch: 27 [20750/36045]\tLoss: 623.7101\n",
      "Training Epoch: 27 [20800/36045]\tLoss: 678.2320\n",
      "Training Epoch: 27 [20850/36045]\tLoss: 665.0178\n",
      "Training Epoch: 27 [20900/36045]\tLoss: 710.7960\n",
      "Training Epoch: 27 [20950/36045]\tLoss: 670.4413\n",
      "Training Epoch: 27 [21000/36045]\tLoss: 631.8686\n",
      "Training Epoch: 27 [21050/36045]\tLoss: 541.4030\n",
      "Training Epoch: 27 [21100/36045]\tLoss: 544.7647\n",
      "Training Epoch: 27 [21150/36045]\tLoss: 583.0070\n",
      "Training Epoch: 27 [21200/36045]\tLoss: 582.2222\n",
      "Training Epoch: 27 [21250/36045]\tLoss: 557.1174\n",
      "Training Epoch: 27 [21300/36045]\tLoss: 650.3947\n",
      "Training Epoch: 27 [21350/36045]\tLoss: 643.0079\n",
      "Training Epoch: 27 [21400/36045]\tLoss: 646.3587\n",
      "Training Epoch: 27 [21450/36045]\tLoss: 652.7020\n",
      "Training Epoch: 27 [21500/36045]\tLoss: 655.7375\n",
      "Training Epoch: 27 [21550/36045]\tLoss: 751.0497\n",
      "Training Epoch: 27 [21600/36045]\tLoss: 750.8274\n",
      "Training Epoch: 27 [21650/36045]\tLoss: 763.7927\n",
      "Training Epoch: 27 [21700/36045]\tLoss: 765.1774\n",
      "Training Epoch: 27 [21750/36045]\tLoss: 735.5054\n",
      "Training Epoch: 27 [21800/36045]\tLoss: 543.7465\n",
      "Training Epoch: 27 [21850/36045]\tLoss: 526.6016\n",
      "Training Epoch: 27 [21900/36045]\tLoss: 537.3278\n",
      "Training Epoch: 27 [21950/36045]\tLoss: 536.5872\n",
      "Training Epoch: 27 [22000/36045]\tLoss: 540.5478\n",
      "Training Epoch: 27 [22050/36045]\tLoss: 564.4291\n",
      "Training Epoch: 27 [22100/36045]\tLoss: 556.8229\n",
      "Training Epoch: 27 [22150/36045]\tLoss: 541.4325\n",
      "Training Epoch: 27 [22200/36045]\tLoss: 558.8929\n",
      "Training Epoch: 27 [22250/36045]\tLoss: 564.0068\n",
      "Training Epoch: 27 [22300/36045]\tLoss: 617.0530\n",
      "Training Epoch: 27 [22350/36045]\tLoss: 643.3031\n",
      "Training Epoch: 27 [22400/36045]\tLoss: 658.4410\n",
      "Training Epoch: 27 [22450/36045]\tLoss: 646.2308\n",
      "Training Epoch: 27 [22500/36045]\tLoss: 627.8437\n",
      "Training Epoch: 27 [22550/36045]\tLoss: 664.6514\n",
      "Training Epoch: 27 [22600/36045]\tLoss: 721.3278\n",
      "Training Epoch: 27 [22650/36045]\tLoss: 757.2639\n",
      "Training Epoch: 27 [22700/36045]\tLoss: 780.3961\n",
      "Training Epoch: 27 [22750/36045]\tLoss: 800.5132\n",
      "Training Epoch: 27 [22800/36045]\tLoss: 832.3968\n",
      "Training Epoch: 27 [22850/36045]\tLoss: 692.4528\n",
      "Training Epoch: 27 [22900/36045]\tLoss: 696.9902\n",
      "Training Epoch: 27 [22950/36045]\tLoss: 675.9216\n",
      "Training Epoch: 27 [23000/36045]\tLoss: 674.0338\n",
      "Training Epoch: 27 [23050/36045]\tLoss: 599.7181\n",
      "Training Epoch: 27 [23100/36045]\tLoss: 615.8610\n",
      "Training Epoch: 27 [23150/36045]\tLoss: 603.9550\n",
      "Training Epoch: 27 [23200/36045]\tLoss: 572.0747\n",
      "Training Epoch: 27 [23250/36045]\tLoss: 574.9943\n",
      "Training Epoch: 27 [23300/36045]\tLoss: 571.7308\n",
      "Training Epoch: 27 [23350/36045]\tLoss: 593.2253\n",
      "Training Epoch: 27 [23400/36045]\tLoss: 642.6829\n",
      "Training Epoch: 27 [23450/36045]\tLoss: 635.3478\n",
      "Training Epoch: 27 [23500/36045]\tLoss: 612.4310\n",
      "Training Epoch: 27 [23550/36045]\tLoss: 657.1373\n",
      "Training Epoch: 27 [23600/36045]\tLoss: 741.6361\n",
      "Training Epoch: 27 [23650/36045]\tLoss: 755.1269\n",
      "Training Epoch: 27 [23700/36045]\tLoss: 764.0454\n",
      "Training Epoch: 27 [23750/36045]\tLoss: 738.5837\n",
      "Training Epoch: 27 [23800/36045]\tLoss: 589.1134\n",
      "Training Epoch: 27 [23850/36045]\tLoss: 615.8817\n",
      "Training Epoch: 27 [23900/36045]\tLoss: 605.5410\n",
      "Training Epoch: 27 [23950/36045]\tLoss: 588.2682\n",
      "Training Epoch: 27 [24000/36045]\tLoss: 564.5076\n",
      "Training Epoch: 27 [24050/36045]\tLoss: 521.7567\n",
      "Training Epoch: 27 [24100/36045]\tLoss: 549.1086\n",
      "Training Epoch: 27 [24150/36045]\tLoss: 542.8972\n",
      "Training Epoch: 27 [24200/36045]\tLoss: 538.7801\n",
      "Training Epoch: 27 [24250/36045]\tLoss: 522.9125\n",
      "Training Epoch: 27 [24300/36045]\tLoss: 564.2850\n",
      "Training Epoch: 27 [24350/36045]\tLoss: 578.1705\n",
      "Training Epoch: 27 [24400/36045]\tLoss: 594.4916\n",
      "Training Epoch: 27 [24450/36045]\tLoss: 567.0469\n",
      "Training Epoch: 27 [24500/36045]\tLoss: 597.4315\n",
      "Training Epoch: 27 [24550/36045]\tLoss: 688.3295\n",
      "Training Epoch: 27 [24600/36045]\tLoss: 680.3354\n",
      "Training Epoch: 27 [24650/36045]\tLoss: 652.8563\n",
      "Training Epoch: 27 [24700/36045]\tLoss: 663.1974\n",
      "Training Epoch: 27 [24750/36045]\tLoss: 612.7319\n",
      "Training Epoch: 27 [24800/36045]\tLoss: 507.8609\n",
      "Training Epoch: 27 [24850/36045]\tLoss: 527.6104\n",
      "Training Epoch: 27 [24900/36045]\tLoss: 524.4257\n",
      "Training Epoch: 27 [24950/36045]\tLoss: 526.9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [25000/36045]\tLoss: 506.5953\n",
      "Training Epoch: 27 [25050/36045]\tLoss: 483.7553\n",
      "Training Epoch: 27 [25100/36045]\tLoss: 433.7731\n",
      "Training Epoch: 27 [25150/36045]\tLoss: 401.8478\n",
      "Training Epoch: 27 [25200/36045]\tLoss: 396.8033\n",
      "Training Epoch: 27 [25250/36045]\tLoss: 425.1515\n",
      "Training Epoch: 27 [25300/36045]\tLoss: 558.1725\n",
      "Training Epoch: 27 [25350/36045]\tLoss: 555.4475\n",
      "Training Epoch: 27 [25400/36045]\tLoss: 517.4672\n",
      "Training Epoch: 27 [25450/36045]\tLoss: 520.2349\n",
      "Training Epoch: 27 [25500/36045]\tLoss: 565.2687\n",
      "Training Epoch: 27 [25550/36045]\tLoss: 658.6960\n",
      "Training Epoch: 27 [25600/36045]\tLoss: 663.7254\n",
      "Training Epoch: 27 [25650/36045]\tLoss: 640.5420\n",
      "Training Epoch: 27 [25700/36045]\tLoss: 649.7089\n",
      "Training Epoch: 27 [25750/36045]\tLoss: 625.8184\n",
      "Training Epoch: 27 [25800/36045]\tLoss: 394.2946\n",
      "Training Epoch: 27 [25850/36045]\tLoss: 404.4366\n",
      "Training Epoch: 27 [25900/36045]\tLoss: 385.4623\n",
      "Training Epoch: 27 [25950/36045]\tLoss: 394.3676\n",
      "Training Epoch: 27 [26000/36045]\tLoss: 483.0553\n",
      "Training Epoch: 27 [26050/36045]\tLoss: 657.3348\n",
      "Training Epoch: 27 [26100/36045]\tLoss: 685.2698\n",
      "Training Epoch: 27 [26150/36045]\tLoss: 685.5704\n",
      "Training Epoch: 27 [26200/36045]\tLoss: 658.8758\n",
      "Training Epoch: 27 [26250/36045]\tLoss: 689.5603\n",
      "Training Epoch: 27 [26300/36045]\tLoss: 620.7858\n",
      "Training Epoch: 27 [26350/36045]\tLoss: 631.1616\n",
      "Training Epoch: 27 [26400/36045]\tLoss: 608.8187\n",
      "Training Epoch: 27 [26450/36045]\tLoss: 538.5014\n",
      "Training Epoch: 27 [26500/36045]\tLoss: 641.2555\n",
      "Training Epoch: 27 [26550/36045]\tLoss: 643.1642\n",
      "Training Epoch: 27 [26600/36045]\tLoss: 638.7947\n",
      "Training Epoch: 27 [26650/36045]\tLoss: 655.1317\n",
      "Training Epoch: 27 [26700/36045]\tLoss: 634.8836\n",
      "Training Epoch: 27 [26750/36045]\tLoss: 594.3531\n",
      "Training Epoch: 27 [26800/36045]\tLoss: 437.5889\n",
      "Training Epoch: 27 [26850/36045]\tLoss: 363.1688\n",
      "Training Epoch: 27 [26900/36045]\tLoss: 366.3867\n",
      "Training Epoch: 27 [26950/36045]\tLoss: 402.8496\n",
      "Training Epoch: 27 [27000/36045]\tLoss: 654.0610\n",
      "Training Epoch: 27 [27050/36045]\tLoss: 684.1493\n",
      "Training Epoch: 27 [27100/36045]\tLoss: 662.6248\n",
      "Training Epoch: 27 [27150/36045]\tLoss: 703.6833\n",
      "Training Epoch: 27 [27200/36045]\tLoss: 516.0839\n",
      "Training Epoch: 27 [27250/36045]\tLoss: 508.9042\n",
      "Training Epoch: 27 [27300/36045]\tLoss: 495.3146\n",
      "Training Epoch: 27 [27350/36045]\tLoss: 494.3264\n",
      "Training Epoch: 27 [27400/36045]\tLoss: 493.3977\n",
      "Training Epoch: 27 [27450/36045]\tLoss: 622.4686\n",
      "Training Epoch: 27 [27500/36045]\tLoss: 667.5161\n",
      "Training Epoch: 27 [27550/36045]\tLoss: 659.9787\n",
      "Training Epoch: 27 [27600/36045]\tLoss: 671.7495\n",
      "Training Epoch: 27 [27650/36045]\tLoss: 663.4553\n",
      "Training Epoch: 27 [27700/36045]\tLoss: 693.1539\n",
      "Training Epoch: 27 [27750/36045]\tLoss: 705.4605\n",
      "Training Epoch: 27 [27800/36045]\tLoss: 691.8096\n",
      "Training Epoch: 27 [27850/36045]\tLoss: 680.9359\n",
      "Training Epoch: 27 [27900/36045]\tLoss: 614.1685\n",
      "Training Epoch: 27 [27950/36045]\tLoss: 510.1846\n",
      "Training Epoch: 27 [28000/36045]\tLoss: 486.1253\n",
      "Training Epoch: 27 [28050/36045]\tLoss: 496.9118\n",
      "Training Epoch: 27 [28100/36045]\tLoss: 488.6033\n",
      "Training Epoch: 27 [28150/36045]\tLoss: 514.1605\n",
      "Training Epoch: 27 [28200/36045]\tLoss: 520.4883\n",
      "Training Epoch: 27 [28250/36045]\tLoss: 514.0584\n",
      "Training Epoch: 27 [28300/36045]\tLoss: 487.5482\n",
      "Training Epoch: 27 [28350/36045]\tLoss: 483.7677\n",
      "Training Epoch: 27 [28400/36045]\tLoss: 816.8082\n",
      "Training Epoch: 27 [28450/36045]\tLoss: 746.8325\n",
      "Training Epoch: 27 [28500/36045]\tLoss: 645.7875\n",
      "Training Epoch: 27 [28550/36045]\tLoss: 592.6956\n",
      "Training Epoch: 27 [28600/36045]\tLoss: 626.9706\n",
      "Training Epoch: 27 [28650/36045]\tLoss: 698.9431\n",
      "Training Epoch: 27 [28700/36045]\tLoss: 693.4436\n",
      "Training Epoch: 27 [28750/36045]\tLoss: 680.2188\n",
      "Training Epoch: 27 [28800/36045]\tLoss: 688.8047\n",
      "Training Epoch: 27 [28850/36045]\tLoss: 596.5346\n",
      "Training Epoch: 27 [28900/36045]\tLoss: 482.3132\n",
      "Training Epoch: 27 [28950/36045]\tLoss: 480.7514\n",
      "Training Epoch: 27 [29000/36045]\tLoss: 478.9050\n",
      "Training Epoch: 27 [29050/36045]\tLoss: 486.3962\n",
      "Training Epoch: 27 [29100/36045]\tLoss: 506.1560\n",
      "Training Epoch: 27 [29150/36045]\tLoss: 493.9031\n",
      "Training Epoch: 27 [29200/36045]\tLoss: 479.0976\n",
      "Training Epoch: 27 [29250/36045]\tLoss: 467.8485\n",
      "Training Epoch: 27 [29300/36045]\tLoss: 532.8535\n",
      "Training Epoch: 27 [29350/36045]\tLoss: 630.4846\n",
      "Training Epoch: 27 [29400/36045]\tLoss: 648.6714\n",
      "Training Epoch: 27 [29450/36045]\tLoss: 668.1625\n",
      "Training Epoch: 27 [29500/36045]\tLoss: 682.4030\n",
      "Training Epoch: 27 [29550/36045]\tLoss: 648.9301\n",
      "Training Epoch: 27 [29600/36045]\tLoss: 548.2563\n",
      "Training Epoch: 27 [29650/36045]\tLoss: 531.3029\n",
      "Training Epoch: 27 [29700/36045]\tLoss: 474.2080\n",
      "Training Epoch: 27 [29750/36045]\tLoss: 474.0086\n",
      "Training Epoch: 27 [29800/36045]\tLoss: 520.2990\n",
      "Training Epoch: 27 [29850/36045]\tLoss: 594.7632\n",
      "Training Epoch: 27 [29900/36045]\tLoss: 591.5522\n",
      "Training Epoch: 27 [29950/36045]\tLoss: 613.9478\n",
      "Training Epoch: 27 [30000/36045]\tLoss: 589.6471\n",
      "Training Epoch: 27 [30050/36045]\tLoss: 595.6946\n",
      "Training Epoch: 27 [30100/36045]\tLoss: 726.6494\n",
      "Training Epoch: 27 [30150/36045]\tLoss: 710.5029\n",
      "Training Epoch: 27 [30200/36045]\tLoss: 670.2476\n",
      "Training Epoch: 27 [30250/36045]\tLoss: 719.4650\n",
      "Training Epoch: 27 [30300/36045]\tLoss: 704.9565\n",
      "Training Epoch: 27 [30350/36045]\tLoss: 548.9634\n",
      "Training Epoch: 27 [30400/36045]\tLoss: 533.6952\n",
      "Training Epoch: 27 [30450/36045]\tLoss: 534.2957\n",
      "Training Epoch: 27 [30500/36045]\tLoss: 499.5290\n",
      "Training Epoch: 27 [30550/36045]\tLoss: 462.7928\n",
      "Training Epoch: 27 [30600/36045]\tLoss: 451.9464\n",
      "Training Epoch: 27 [30650/36045]\tLoss: 442.0863\n",
      "Training Epoch: 27 [30700/36045]\tLoss: 459.8786\n",
      "Training Epoch: 27 [30750/36045]\tLoss: 446.4937\n",
      "Training Epoch: 27 [30800/36045]\tLoss: 473.9768\n",
      "Training Epoch: 27 [30850/36045]\tLoss: 465.7103\n",
      "Training Epoch: 27 [30900/36045]\tLoss: 478.7387\n",
      "Training Epoch: 27 [30950/36045]\tLoss: 503.0775\n",
      "Training Epoch: 27 [31000/36045]\tLoss: 494.4250\n",
      "Training Epoch: 27 [31050/36045]\tLoss: 413.0626\n",
      "Training Epoch: 27 [31100/36045]\tLoss: 403.7249\n",
      "Training Epoch: 27 [31150/36045]\tLoss: 410.5565\n",
      "Training Epoch: 27 [31200/36045]\tLoss: 511.9629\n",
      "Training Epoch: 27 [31250/36045]\tLoss: 664.2148\n",
      "Training Epoch: 27 [31300/36045]\tLoss: 634.2510\n",
      "Training Epoch: 27 [31350/36045]\tLoss: 650.0806\n",
      "Training Epoch: 27 [31400/36045]\tLoss: 629.3626\n",
      "Training Epoch: 27 [31450/36045]\tLoss: 644.7433\n",
      "Training Epoch: 27 [31500/36045]\tLoss: 656.7164\n",
      "Training Epoch: 27 [31550/36045]\tLoss: 665.1023\n",
      "Training Epoch: 27 [31600/36045]\tLoss: 625.0854\n",
      "Training Epoch: 27 [31650/36045]\tLoss: 667.8890\n",
      "Training Epoch: 27 [31700/36045]\tLoss: 485.0282\n",
      "Training Epoch: 27 [31750/36045]\tLoss: 401.9459\n",
      "Training Epoch: 27 [31800/36045]\tLoss: 382.9603\n",
      "Training Epoch: 27 [31850/36045]\tLoss: 392.2335\n",
      "Training Epoch: 27 [31900/36045]\tLoss: 613.5215\n",
      "Training Epoch: 27 [31950/36045]\tLoss: 790.6677\n",
      "Training Epoch: 27 [32000/36045]\tLoss: 902.4544\n",
      "Training Epoch: 27 [32050/36045]\tLoss: 856.4869\n",
      "Training Epoch: 27 [32100/36045]\tLoss: 846.0568\n",
      "Training Epoch: 27 [32150/36045]\tLoss: 659.2646\n",
      "Training Epoch: 27 [32200/36045]\tLoss: 663.3350\n",
      "Training Epoch: 27 [32250/36045]\tLoss: 674.1622\n",
      "Training Epoch: 27 [32300/36045]\tLoss: 655.8690\n",
      "Training Epoch: 27 [32350/36045]\tLoss: 650.8480\n",
      "Training Epoch: 27 [32400/36045]\tLoss: 610.8342\n",
      "Training Epoch: 27 [32450/36045]\tLoss: 503.3825\n",
      "Training Epoch: 27 [32500/36045]\tLoss: 484.0333\n",
      "Training Epoch: 27 [32550/36045]\tLoss: 486.6367\n",
      "Training Epoch: 27 [32600/36045]\tLoss: 483.0927\n",
      "Training Epoch: 27 [32650/36045]\tLoss: 617.9245\n",
      "Training Epoch: 27 [32700/36045]\tLoss: 673.7590\n",
      "Training Epoch: 27 [32750/36045]\tLoss: 642.1795\n",
      "Training Epoch: 27 [32800/36045]\tLoss: 658.8094\n",
      "Training Epoch: 27 [32850/36045]\tLoss: 608.3426\n",
      "Training Epoch: 27 [32900/36045]\tLoss: 489.3997\n",
      "Training Epoch: 27 [32950/36045]\tLoss: 512.0676\n",
      "Training Epoch: 27 [33000/36045]\tLoss: 511.7215\n",
      "Training Epoch: 27 [33050/36045]\tLoss: 485.7499\n",
      "Training Epoch: 27 [33100/36045]\tLoss: 552.3033\n",
      "Training Epoch: 27 [33150/36045]\tLoss: 748.9158\n",
      "Training Epoch: 27 [33200/36045]\tLoss: 729.6254\n",
      "Training Epoch: 27 [33250/36045]\tLoss: 751.7012\n",
      "Training Epoch: 27 [33300/36045]\tLoss: 800.1960\n",
      "Training Epoch: 27 [33350/36045]\tLoss: 614.1290\n",
      "Training Epoch: 27 [33400/36045]\tLoss: 451.5878\n",
      "Training Epoch: 27 [33450/36045]\tLoss: 446.7224\n",
      "Training Epoch: 27 [33500/36045]\tLoss: 459.8108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 27 [33550/36045]\tLoss: 476.8664\n",
      "Training Epoch: 27 [33600/36045]\tLoss: 478.4666\n",
      "Training Epoch: 27 [33650/36045]\tLoss: 637.5519\n",
      "Training Epoch: 27 [33700/36045]\tLoss: 617.0061\n",
      "Training Epoch: 27 [33750/36045]\tLoss: 638.9429\n",
      "Training Epoch: 27 [33800/36045]\tLoss: 634.3275\n",
      "Training Epoch: 27 [33850/36045]\tLoss: 637.0283\n",
      "Training Epoch: 27 [33900/36045]\tLoss: 648.7042\n",
      "Training Epoch: 27 [33950/36045]\tLoss: 659.9922\n",
      "Training Epoch: 27 [34000/36045]\tLoss: 646.9320\n",
      "Training Epoch: 27 [34050/36045]\tLoss: 651.2993\n",
      "Training Epoch: 27 [34100/36045]\tLoss: 626.8237\n",
      "Training Epoch: 27 [34150/36045]\tLoss: 582.7142\n",
      "Training Epoch: 27 [34200/36045]\tLoss: 551.6759\n",
      "Training Epoch: 27 [34250/36045]\tLoss: 566.0795\n",
      "Training Epoch: 27 [34300/36045]\tLoss: 484.7078\n",
      "Training Epoch: 27 [34350/36045]\tLoss: 510.2870\n",
      "Training Epoch: 27 [34400/36045]\tLoss: 501.4985\n",
      "Training Epoch: 27 [34450/36045]\tLoss: 471.2189\n",
      "Training Epoch: 27 [34500/36045]\tLoss: 503.0871\n",
      "Training Epoch: 27 [34550/36045]\tLoss: 493.7284\n",
      "Training Epoch: 27 [34600/36045]\tLoss: 496.7015\n",
      "Training Epoch: 27 [34650/36045]\tLoss: 605.0622\n",
      "Training Epoch: 27 [34700/36045]\tLoss: 640.5826\n",
      "Training Epoch: 27 [34750/36045]\tLoss: 568.7083\n",
      "Training Epoch: 27 [34800/36045]\tLoss: 650.2507\n",
      "Training Epoch: 27 [34850/36045]\tLoss: 658.5240\n",
      "Training Epoch: 27 [34900/36045]\tLoss: 724.1050\n",
      "Training Epoch: 27 [34950/36045]\tLoss: 710.9062\n",
      "Training Epoch: 27 [35000/36045]\tLoss: 711.9140\n",
      "Training Epoch: 27 [35050/36045]\tLoss: 697.8234\n",
      "Training Epoch: 27 [35100/36045]\tLoss: 588.8021\n",
      "Training Epoch: 27 [35150/36045]\tLoss: 581.8776\n",
      "Training Epoch: 27 [35200/36045]\tLoss: 493.4348\n",
      "Training Epoch: 27 [35250/36045]\tLoss: 541.7379\n",
      "Training Epoch: 27 [35300/36045]\tLoss: 555.9789\n",
      "Training Epoch: 27 [35350/36045]\tLoss: 630.7163\n",
      "Training Epoch: 27 [35400/36045]\tLoss: 666.0060\n",
      "Training Epoch: 27 [35450/36045]\tLoss: 635.7120\n",
      "Training Epoch: 27 [35500/36045]\tLoss: 617.2799\n",
      "Training Epoch: 27 [35550/36045]\tLoss: 602.3693\n",
      "Training Epoch: 27 [35600/36045]\tLoss: 650.2603\n",
      "Training Epoch: 27 [35650/36045]\tLoss: 724.2372\n",
      "Training Epoch: 27 [35700/36045]\tLoss: 651.5571\n",
      "Training Epoch: 27 [35750/36045]\tLoss: 710.0451\n",
      "Training Epoch: 27 [35800/36045]\tLoss: 716.4962\n",
      "Training Epoch: 27 [35850/36045]\tLoss: 690.5782\n",
      "Training Epoch: 27 [35900/36045]\tLoss: 716.7040\n",
      "Training Epoch: 27 [35950/36045]\tLoss: 714.3346\n",
      "Training Epoch: 27 [36000/36045]\tLoss: 706.2814\n",
      "Training Epoch: 27 [36045/36045]\tLoss: 689.4772\n",
      "Training Epoch: 27 [4004/4004]\tLoss: 642.6133\n",
      "Training Epoch: 28 [50/36045]\tLoss: 642.3016\n",
      "Training Epoch: 28 [100/36045]\tLoss: 615.9039\n",
      "Training Epoch: 28 [150/36045]\tLoss: 614.0049\n",
      "Training Epoch: 28 [200/36045]\tLoss: 600.8361\n",
      "Training Epoch: 28 [250/36045]\tLoss: 716.3619\n",
      "Training Epoch: 28 [300/36045]\tLoss: 780.2438\n",
      "Training Epoch: 28 [350/36045]\tLoss: 745.9561\n",
      "Training Epoch: 28 [400/36045]\tLoss: 741.6945\n",
      "Training Epoch: 28 [450/36045]\tLoss: 721.6171\n",
      "Training Epoch: 28 [500/36045]\tLoss: 671.4216\n",
      "Training Epoch: 28 [550/36045]\tLoss: 675.4724\n",
      "Training Epoch: 28 [600/36045]\tLoss: 656.5267\n",
      "Training Epoch: 28 [650/36045]\tLoss: 680.1717\n",
      "Training Epoch: 28 [700/36045]\tLoss: 667.2508\n",
      "Training Epoch: 28 [750/36045]\tLoss: 648.1511\n",
      "Training Epoch: 28 [800/36045]\tLoss: 662.1446\n",
      "Training Epoch: 28 [850/36045]\tLoss: 643.0544\n",
      "Training Epoch: 28 [900/36045]\tLoss: 612.9329\n",
      "Training Epoch: 28 [950/36045]\tLoss: 580.7803\n",
      "Training Epoch: 28 [1000/36045]\tLoss: 560.5705\n",
      "Training Epoch: 28 [1050/36045]\tLoss: 562.7703\n",
      "Training Epoch: 28 [1100/36045]\tLoss: 547.7729\n",
      "Training Epoch: 28 [1150/36045]\tLoss: 556.5811\n",
      "Training Epoch: 28 [1200/36045]\tLoss: 587.5635\n",
      "Training Epoch: 28 [1250/36045]\tLoss: 672.3192\n",
      "Training Epoch: 28 [1300/36045]\tLoss: 678.8052\n",
      "Training Epoch: 28 [1350/36045]\tLoss: 681.2877\n",
      "Training Epoch: 28 [1400/36045]\tLoss: 708.0874\n",
      "Training Epoch: 28 [1450/36045]\tLoss: 684.8798\n",
      "Training Epoch: 28 [1500/36045]\tLoss: 628.6803\n",
      "Training Epoch: 28 [1550/36045]\tLoss: 644.1766\n",
      "Training Epoch: 28 [1600/36045]\tLoss: 655.1074\n",
      "Training Epoch: 28 [1650/36045]\tLoss: 642.0685\n",
      "Training Epoch: 28 [1700/36045]\tLoss: 654.3222\n",
      "Training Epoch: 28 [1750/36045]\tLoss: 696.0282\n",
      "Training Epoch: 28 [1800/36045]\tLoss: 677.0208\n",
      "Training Epoch: 28 [1850/36045]\tLoss: 694.3755\n",
      "Training Epoch: 28 [1900/36045]\tLoss: 649.9708\n",
      "Training Epoch: 28 [1950/36045]\tLoss: 661.0746\n",
      "Training Epoch: 28 [2000/36045]\tLoss: 598.0220\n",
      "Training Epoch: 28 [2050/36045]\tLoss: 600.7466\n",
      "Training Epoch: 28 [2100/36045]\tLoss: 632.7853\n",
      "Training Epoch: 28 [2150/36045]\tLoss: 611.8458\n",
      "Training Epoch: 28 [2200/36045]\tLoss: 568.4432\n",
      "Training Epoch: 28 [2250/36045]\tLoss: 536.8461\n",
      "Training Epoch: 28 [2300/36045]\tLoss: 563.1624\n",
      "Training Epoch: 28 [2350/36045]\tLoss: 538.1591\n",
      "Training Epoch: 28 [2400/36045]\tLoss: 547.4181\n",
      "Training Epoch: 28 [2450/36045]\tLoss: 697.1269\n",
      "Training Epoch: 28 [2500/36045]\tLoss: 732.5394\n",
      "Training Epoch: 28 [2550/36045]\tLoss: 729.5972\n",
      "Training Epoch: 28 [2600/36045]\tLoss: 738.5231\n",
      "Training Epoch: 28 [2650/36045]\tLoss: 864.0197\n",
      "Training Epoch: 28 [2700/36045]\tLoss: 951.2604\n",
      "Training Epoch: 28 [2750/36045]\tLoss: 1023.3973\n",
      "Training Epoch: 28 [2800/36045]\tLoss: 1033.3063\n",
      "Training Epoch: 28 [2850/36045]\tLoss: 801.9466\n",
      "Training Epoch: 28 [2900/36045]\tLoss: 766.7988\n",
      "Training Epoch: 28 [2950/36045]\tLoss: 740.3019\n",
      "Training Epoch: 28 [3000/36045]\tLoss: 734.9257\n",
      "Training Epoch: 28 [3050/36045]\tLoss: 765.8568\n",
      "Training Epoch: 28 [3100/36045]\tLoss: 700.8936\n",
      "Training Epoch: 28 [3150/36045]\tLoss: 540.8742\n",
      "Training Epoch: 28 [3200/36045]\tLoss: 561.1423\n",
      "Training Epoch: 28 [3250/36045]\tLoss: 528.3625\n",
      "Training Epoch: 28 [3300/36045]\tLoss: 500.2223\n",
      "Training Epoch: 28 [3350/36045]\tLoss: 527.5092\n",
      "Training Epoch: 28 [3400/36045]\tLoss: 553.8065\n",
      "Training Epoch: 28 [3450/36045]\tLoss: 594.6572\n",
      "Training Epoch: 28 [3500/36045]\tLoss: 581.9146\n",
      "Training Epoch: 28 [3550/36045]\tLoss: 558.1619\n",
      "Training Epoch: 28 [3600/36045]\tLoss: 598.1165\n",
      "Training Epoch: 28 [3650/36045]\tLoss: 691.1750\n",
      "Training Epoch: 28 [3700/36045]\tLoss: 697.6055\n",
      "Training Epoch: 28 [3750/36045]\tLoss: 666.1532\n",
      "Training Epoch: 28 [3800/36045]\tLoss: 660.7141\n",
      "Training Epoch: 28 [3850/36045]\tLoss: 660.0700\n",
      "Training Epoch: 28 [3900/36045]\tLoss: 665.4199\n",
      "Training Epoch: 28 [3950/36045]\tLoss: 641.8318\n",
      "Training Epoch: 28 [4000/36045]\tLoss: 647.9809\n",
      "Training Epoch: 28 [4050/36045]\tLoss: 595.5375\n",
      "Training Epoch: 28 [4100/36045]\tLoss: 580.5226\n",
      "Training Epoch: 28 [4150/36045]\tLoss: 597.0188\n",
      "Training Epoch: 28 [4200/36045]\tLoss: 591.6368\n",
      "Training Epoch: 28 [4250/36045]\tLoss: 593.8528\n",
      "Training Epoch: 28 [4300/36045]\tLoss: 612.2366\n",
      "Training Epoch: 28 [4350/36045]\tLoss: 594.6770\n",
      "Training Epoch: 28 [4400/36045]\tLoss: 569.4064\n",
      "Training Epoch: 28 [4450/36045]\tLoss: 622.6014\n",
      "Training Epoch: 28 [4500/36045]\tLoss: 666.4179\n",
      "Training Epoch: 28 [4550/36045]\tLoss: 671.4562\n",
      "Training Epoch: 28 [4600/36045]\tLoss: 694.4875\n",
      "Training Epoch: 28 [4650/36045]\tLoss: 683.6965\n",
      "Training Epoch: 28 [4700/36045]\tLoss: 631.6763\n",
      "Training Epoch: 28 [4750/36045]\tLoss: 614.0697\n",
      "Training Epoch: 28 [4800/36045]\tLoss: 640.2216\n",
      "Training Epoch: 28 [4850/36045]\tLoss: 626.6770\n",
      "Training Epoch: 28 [4900/36045]\tLoss: 609.5245\n",
      "Training Epoch: 28 [4950/36045]\tLoss: 626.5254\n",
      "Training Epoch: 28 [5000/36045]\tLoss: 657.3151\n",
      "Training Epoch: 28 [5050/36045]\tLoss: 636.6413\n",
      "Training Epoch: 28 [5100/36045]\tLoss: 647.0944\n",
      "Training Epoch: 28 [5150/36045]\tLoss: 631.4371\n",
      "Training Epoch: 28 [5200/36045]\tLoss: 629.4075\n",
      "Training Epoch: 28 [5250/36045]\tLoss: 622.7819\n",
      "Training Epoch: 28 [5300/36045]\tLoss: 623.5410\n",
      "Training Epoch: 28 [5350/36045]\tLoss: 646.6218\n",
      "Training Epoch: 28 [5400/36045]\tLoss: 622.4775\n",
      "Training Epoch: 28 [5450/36045]\tLoss: 590.3469\n",
      "Training Epoch: 28 [5500/36045]\tLoss: 619.6193\n",
      "Training Epoch: 28 [5550/36045]\tLoss: 607.6571\n",
      "Training Epoch: 28 [5600/36045]\tLoss: 690.9146\n",
      "Training Epoch: 28 [5650/36045]\tLoss: 654.4521\n",
      "Training Epoch: 28 [5700/36045]\tLoss: 614.3461\n",
      "Training Epoch: 28 [5750/36045]\tLoss: 598.9902\n",
      "Training Epoch: 28 [5800/36045]\tLoss: 632.0109\n",
      "Training Epoch: 28 [5850/36045]\tLoss: 618.4642\n",
      "Training Epoch: 28 [5900/36045]\tLoss: 711.5989\n",
      "Training Epoch: 28 [5950/36045]\tLoss: 729.1528\n",
      "Training Epoch: 28 [6000/36045]\tLoss: 714.0578\n",
      "Training Epoch: 28 [6050/36045]\tLoss: 690.5164\n",
      "Training Epoch: 28 [6100/36045]\tLoss: 695.2274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [6150/36045]\tLoss: 681.0553\n",
      "Training Epoch: 28 [6200/36045]\tLoss: 682.9426\n",
      "Training Epoch: 28 [6250/36045]\tLoss: 704.2586\n",
      "Training Epoch: 28 [6300/36045]\tLoss: 716.0537\n",
      "Training Epoch: 28 [6350/36045]\tLoss: 763.8511\n",
      "Training Epoch: 28 [6400/36045]\tLoss: 634.8444\n",
      "Training Epoch: 28 [6450/36045]\tLoss: 586.6620\n",
      "Training Epoch: 28 [6500/36045]\tLoss: 597.4973\n",
      "Training Epoch: 28 [6550/36045]\tLoss: 614.5953\n",
      "Training Epoch: 28 [6600/36045]\tLoss: 613.9738\n",
      "Training Epoch: 28 [6650/36045]\tLoss: 692.8065\n",
      "Training Epoch: 28 [6700/36045]\tLoss: 724.9227\n",
      "Training Epoch: 28 [6750/36045]\tLoss: 700.1622\n",
      "Training Epoch: 28 [6800/36045]\tLoss: 703.2233\n",
      "Training Epoch: 28 [6850/36045]\tLoss: 691.0905\n",
      "Training Epoch: 28 [6900/36045]\tLoss: 615.5701\n",
      "Training Epoch: 28 [6950/36045]\tLoss: 580.3201\n",
      "Training Epoch: 28 [7000/36045]\tLoss: 617.4015\n",
      "Training Epoch: 28 [7050/36045]\tLoss: 631.0856\n",
      "Training Epoch: 28 [7100/36045]\tLoss: 630.3904\n",
      "Training Epoch: 28 [7150/36045]\tLoss: 641.2814\n",
      "Training Epoch: 28 [7200/36045]\tLoss: 645.0065\n",
      "Training Epoch: 28 [7250/36045]\tLoss: 642.6050\n",
      "Training Epoch: 28 [7300/36045]\tLoss: 629.5671\n",
      "Training Epoch: 28 [7350/36045]\tLoss: 625.6462\n",
      "Training Epoch: 28 [7400/36045]\tLoss: 567.2252\n",
      "Training Epoch: 28 [7450/36045]\tLoss: 570.6909\n",
      "Training Epoch: 28 [7500/36045]\tLoss: 565.4495\n",
      "Training Epoch: 28 [7550/36045]\tLoss: 541.9267\n",
      "Training Epoch: 28 [7600/36045]\tLoss: 601.9216\n",
      "Training Epoch: 28 [7650/36045]\tLoss: 645.4241\n",
      "Training Epoch: 28 [7700/36045]\tLoss: 614.6505\n",
      "Training Epoch: 28 [7750/36045]\tLoss: 629.3975\n",
      "Training Epoch: 28 [7800/36045]\tLoss: 617.6865\n",
      "Training Epoch: 28 [7850/36045]\tLoss: 597.4943\n",
      "Training Epoch: 28 [7900/36045]\tLoss: 630.3476\n",
      "Training Epoch: 28 [7950/36045]\tLoss: 627.5764\n",
      "Training Epoch: 28 [8000/36045]\tLoss: 645.7286\n",
      "Training Epoch: 28 [8050/36045]\tLoss: 609.6727\n",
      "Training Epoch: 28 [8100/36045]\tLoss: 635.4781\n",
      "Training Epoch: 28 [8150/36045]\tLoss: 719.2480\n",
      "Training Epoch: 28 [8200/36045]\tLoss: 705.6537\n",
      "Training Epoch: 28 [8250/36045]\tLoss: 672.9515\n",
      "Training Epoch: 28 [8300/36045]\tLoss: 733.2607\n",
      "Training Epoch: 28 [8350/36045]\tLoss: 673.7635\n",
      "Training Epoch: 28 [8400/36045]\tLoss: 603.9198\n",
      "Training Epoch: 28 [8450/36045]\tLoss: 565.9335\n",
      "Training Epoch: 28 [8500/36045]\tLoss: 601.0546\n",
      "Training Epoch: 28 [8550/36045]\tLoss: 592.9706\n",
      "Training Epoch: 28 [8600/36045]\tLoss: 586.6419\n",
      "Training Epoch: 28 [8650/36045]\tLoss: 625.9799\n",
      "Training Epoch: 28 [8700/36045]\tLoss: 661.8194\n",
      "Training Epoch: 28 [8750/36045]\tLoss: 649.7828\n",
      "Training Epoch: 28 [8800/36045]\tLoss: 655.5330\n",
      "Training Epoch: 28 [8850/36045]\tLoss: 648.7058\n",
      "Training Epoch: 28 [8900/36045]\tLoss: 585.5034\n",
      "Training Epoch: 28 [8950/36045]\tLoss: 598.2175\n",
      "Training Epoch: 28 [9000/36045]\tLoss: 613.8384\n",
      "Training Epoch: 28 [9050/36045]\tLoss: 614.8092\n",
      "Training Epoch: 28 [9100/36045]\tLoss: 632.8710\n",
      "Training Epoch: 28 [9150/36045]\tLoss: 467.6647\n",
      "Training Epoch: 28 [9200/36045]\tLoss: 350.7198\n",
      "Training Epoch: 28 [9250/36045]\tLoss: 380.3111\n",
      "Training Epoch: 28 [9300/36045]\tLoss: 391.3659\n",
      "Training Epoch: 28 [9350/36045]\tLoss: 360.6908\n",
      "Training Epoch: 28 [9400/36045]\tLoss: 706.8105\n",
      "Training Epoch: 28 [9450/36045]\tLoss: 750.7384\n",
      "Training Epoch: 28 [9500/36045]\tLoss: 737.6479\n",
      "Training Epoch: 28 [9550/36045]\tLoss: 780.2589\n",
      "Training Epoch: 28 [9600/36045]\tLoss: 579.7960\n",
      "Training Epoch: 28 [9650/36045]\tLoss: 583.9904\n",
      "Training Epoch: 28 [9700/36045]\tLoss: 569.2653\n",
      "Training Epoch: 28 [9750/36045]\tLoss: 568.7500\n",
      "Training Epoch: 28 [9800/36045]\tLoss: 742.8317\n",
      "Training Epoch: 28 [9850/36045]\tLoss: 784.5759\n",
      "Training Epoch: 28 [9900/36045]\tLoss: 797.7271\n",
      "Training Epoch: 28 [9950/36045]\tLoss: 777.1396\n",
      "Training Epoch: 28 [10000/36045]\tLoss: 717.9882\n",
      "Training Epoch: 28 [10050/36045]\tLoss: 591.8047\n",
      "Training Epoch: 28 [10100/36045]\tLoss: 598.8231\n",
      "Training Epoch: 28 [10150/36045]\tLoss: 608.3823\n",
      "Training Epoch: 28 [10200/36045]\tLoss: 597.2480\n",
      "Training Epoch: 28 [10250/36045]\tLoss: 713.8076\n",
      "Training Epoch: 28 [10300/36045]\tLoss: 693.2550\n",
      "Training Epoch: 28 [10350/36045]\tLoss: 729.7565\n",
      "Training Epoch: 28 [10400/36045]\tLoss: 720.2172\n",
      "Training Epoch: 28 [10450/36045]\tLoss: 674.5845\n",
      "Training Epoch: 28 [10500/36045]\tLoss: 564.8328\n",
      "Training Epoch: 28 [10550/36045]\tLoss: 560.0043\n",
      "Training Epoch: 28 [10600/36045]\tLoss: 582.9758\n",
      "Training Epoch: 28 [10650/36045]\tLoss: 589.1652\n",
      "Training Epoch: 28 [10700/36045]\tLoss: 674.4260\n",
      "Training Epoch: 28 [10750/36045]\tLoss: 735.9905\n",
      "Training Epoch: 28 [10800/36045]\tLoss: 679.4962\n",
      "Training Epoch: 28 [10850/36045]\tLoss: 719.6785\n",
      "Training Epoch: 28 [10900/36045]\tLoss: 748.9016\n",
      "Training Epoch: 28 [10950/36045]\tLoss: 553.5936\n",
      "Training Epoch: 28 [11000/36045]\tLoss: 547.2455\n",
      "Training Epoch: 28 [11050/36045]\tLoss: 585.9909\n",
      "Training Epoch: 28 [11100/36045]\tLoss: 597.3760\n",
      "Training Epoch: 28 [11150/36045]\tLoss: 647.5420\n",
      "Training Epoch: 28 [11200/36045]\tLoss: 675.6586\n",
      "Training Epoch: 28 [11250/36045]\tLoss: 687.7515\n",
      "Training Epoch: 28 [11300/36045]\tLoss: 667.7609\n",
      "Training Epoch: 28 [11350/36045]\tLoss: 664.9066\n",
      "Training Epoch: 28 [11400/36045]\tLoss: 626.2884\n",
      "Training Epoch: 28 [11450/36045]\tLoss: 593.5956\n",
      "Training Epoch: 28 [11500/36045]\tLoss: 591.0864\n",
      "Training Epoch: 28 [11550/36045]\tLoss: 602.8678\n",
      "Training Epoch: 28 [11600/36045]\tLoss: 664.9000\n",
      "Training Epoch: 28 [11650/36045]\tLoss: 717.3281\n",
      "Training Epoch: 28 [11700/36045]\tLoss: 716.2796\n",
      "Training Epoch: 28 [11750/36045]\tLoss: 735.4648\n",
      "Training Epoch: 28 [11800/36045]\tLoss: 778.4495\n",
      "Training Epoch: 28 [11850/36045]\tLoss: 835.5899\n",
      "Training Epoch: 28 [11900/36045]\tLoss: 1052.0013\n",
      "Training Epoch: 28 [11950/36045]\tLoss: 1054.0197\n",
      "Training Epoch: 28 [12000/36045]\tLoss: 1067.4901\n",
      "Training Epoch: 28 [12050/36045]\tLoss: 1025.7855\n",
      "Training Epoch: 28 [12100/36045]\tLoss: 668.1250\n",
      "Training Epoch: 28 [12150/36045]\tLoss: 511.0092\n",
      "Training Epoch: 28 [12200/36045]\tLoss: 505.5489\n",
      "Training Epoch: 28 [12250/36045]\tLoss: 514.6826\n",
      "Training Epoch: 28 [12300/36045]\tLoss: 656.8483\n",
      "Training Epoch: 28 [12350/36045]\tLoss: 714.3856\n",
      "Training Epoch: 28 [12400/36045]\tLoss: 722.2333\n",
      "Training Epoch: 28 [12450/36045]\tLoss: 710.2744\n",
      "Training Epoch: 28 [12500/36045]\tLoss: 738.9094\n",
      "Training Epoch: 28 [12550/36045]\tLoss: 707.7416\n",
      "Training Epoch: 28 [12600/36045]\tLoss: 651.4044\n",
      "Training Epoch: 28 [12650/36045]\tLoss: 650.2549\n",
      "Training Epoch: 28 [12700/36045]\tLoss: 671.7524\n",
      "Training Epoch: 28 [12750/36045]\tLoss: 671.0427\n",
      "Training Epoch: 28 [12800/36045]\tLoss: 654.3253\n",
      "Training Epoch: 28 [12850/36045]\tLoss: 683.8918\n",
      "Training Epoch: 28 [12900/36045]\tLoss: 656.1261\n",
      "Training Epoch: 28 [12950/36045]\tLoss: 643.3439\n",
      "Training Epoch: 28 [13000/36045]\tLoss: 675.7812\n",
      "Training Epoch: 28 [13050/36045]\tLoss: 613.4853\n",
      "Training Epoch: 28 [13100/36045]\tLoss: 632.2154\n",
      "Training Epoch: 28 [13150/36045]\tLoss: 623.9351\n",
      "Training Epoch: 28 [13200/36045]\tLoss: 604.4250\n",
      "Training Epoch: 28 [13250/36045]\tLoss: 629.0942\n",
      "Training Epoch: 28 [13300/36045]\tLoss: 668.4167\n",
      "Training Epoch: 28 [13350/36045]\tLoss: 648.0684\n",
      "Training Epoch: 28 [13400/36045]\tLoss: 651.7346\n",
      "Training Epoch: 28 [13450/36045]\tLoss: 648.0333\n",
      "Training Epoch: 28 [13500/36045]\tLoss: 668.9618\n",
      "Training Epoch: 28 [13550/36045]\tLoss: 805.1788\n",
      "Training Epoch: 28 [13600/36045]\tLoss: 838.0471\n",
      "Training Epoch: 28 [13650/36045]\tLoss: 918.5264\n",
      "Training Epoch: 28 [13700/36045]\tLoss: 812.3216\n",
      "Training Epoch: 28 [13750/36045]\tLoss: 654.6664\n",
      "Training Epoch: 28 [13800/36045]\tLoss: 627.1175\n",
      "Training Epoch: 28 [13850/36045]\tLoss: 610.1037\n",
      "Training Epoch: 28 [13900/36045]\tLoss: 617.4967\n",
      "Training Epoch: 28 [13950/36045]\tLoss: 663.1001\n",
      "Training Epoch: 28 [14000/36045]\tLoss: 697.0656\n",
      "Training Epoch: 28 [14050/36045]\tLoss: 670.8273\n",
      "Training Epoch: 28 [14100/36045]\tLoss: 666.3962\n",
      "Training Epoch: 28 [14150/36045]\tLoss: 654.2694\n",
      "Training Epoch: 28 [14200/36045]\tLoss: 696.5441\n",
      "Training Epoch: 28 [14250/36045]\tLoss: 764.0712\n",
      "Training Epoch: 28 [14300/36045]\tLoss: 767.3718\n",
      "Training Epoch: 28 [14350/36045]\tLoss: 734.4965\n",
      "Training Epoch: 28 [14400/36045]\tLoss: 720.0467\n",
      "Training Epoch: 28 [14450/36045]\tLoss: 757.0746\n",
      "Training Epoch: 28 [14500/36045]\tLoss: 687.7851\n",
      "Training Epoch: 28 [14550/36045]\tLoss: 718.0013\n",
      "Training Epoch: 28 [14600/36045]\tLoss: 703.4623\n",
      "Training Epoch: 28 [14650/36045]\tLoss: 703.7402\n",
      "Training Epoch: 28 [14700/36045]\tLoss: 665.3102\n",
      "Training Epoch: 28 [14750/36045]\tLoss: 571.4598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [14800/36045]\tLoss: 561.5592\n",
      "Training Epoch: 28 [14850/36045]\tLoss: 568.6631\n",
      "Training Epoch: 28 [14900/36045]\tLoss: 562.1716\n",
      "Training Epoch: 28 [14950/36045]\tLoss: 569.9808\n",
      "Training Epoch: 28 [15000/36045]\tLoss: 584.6061\n",
      "Training Epoch: 28 [15050/36045]\tLoss: 582.2878\n",
      "Training Epoch: 28 [15100/36045]\tLoss: 566.3119\n",
      "Training Epoch: 28 [15150/36045]\tLoss: 560.1400\n",
      "Training Epoch: 28 [15200/36045]\tLoss: 518.3192\n",
      "Training Epoch: 28 [15250/36045]\tLoss: 541.6038\n",
      "Training Epoch: 28 [15300/36045]\tLoss: 526.4071\n",
      "Training Epoch: 28 [15350/36045]\tLoss: 538.7283\n",
      "Training Epoch: 28 [15400/36045]\tLoss: 522.9456\n",
      "Training Epoch: 28 [15450/36045]\tLoss: 508.6151\n",
      "Training Epoch: 28 [15500/36045]\tLoss: 523.4576\n",
      "Training Epoch: 28 [15550/36045]\tLoss: 519.0815\n",
      "Training Epoch: 28 [15600/36045]\tLoss: 589.0793\n",
      "Training Epoch: 28 [15650/36045]\tLoss: 607.4135\n",
      "Training Epoch: 28 [15700/36045]\tLoss: 598.2695\n",
      "Training Epoch: 28 [15750/36045]\tLoss: 590.1053\n",
      "Training Epoch: 28 [15800/36045]\tLoss: 557.6711\n",
      "Training Epoch: 28 [15850/36045]\tLoss: 571.8195\n",
      "Training Epoch: 28 [15900/36045]\tLoss: 581.3831\n",
      "Training Epoch: 28 [15950/36045]\tLoss: 601.2076\n",
      "Training Epoch: 28 [16000/36045]\tLoss: 574.5954\n",
      "Training Epoch: 28 [16050/36045]\tLoss: 543.9632\n",
      "Training Epoch: 28 [16100/36045]\tLoss: 503.8474\n",
      "Training Epoch: 28 [16150/36045]\tLoss: 491.5713\n",
      "Training Epoch: 28 [16200/36045]\tLoss: 595.2005\n",
      "Training Epoch: 28 [16250/36045]\tLoss: 624.1493\n",
      "Training Epoch: 28 [16300/36045]\tLoss: 681.4659\n",
      "Training Epoch: 28 [16350/36045]\tLoss: 700.1813\n",
      "Training Epoch: 28 [16400/36045]\tLoss: 672.2041\n",
      "Training Epoch: 28 [16450/36045]\tLoss: 653.7985\n",
      "Training Epoch: 28 [16500/36045]\tLoss: 653.4965\n",
      "Training Epoch: 28 [16550/36045]\tLoss: 617.8815\n",
      "Training Epoch: 28 [16600/36045]\tLoss: 642.3994\n",
      "Training Epoch: 28 [16650/36045]\tLoss: 660.8015\n",
      "Training Epoch: 28 [16700/36045]\tLoss: 638.3388\n",
      "Training Epoch: 28 [16750/36045]\tLoss: 630.4562\n",
      "Training Epoch: 28 [16800/36045]\tLoss: 641.3336\n",
      "Training Epoch: 28 [16850/36045]\tLoss: 610.7048\n",
      "Training Epoch: 28 [16900/36045]\tLoss: 621.0076\n",
      "Training Epoch: 28 [16950/36045]\tLoss: 645.7675\n",
      "Training Epoch: 28 [17000/36045]\tLoss: 628.3261\n",
      "Training Epoch: 28 [17050/36045]\tLoss: 655.4470\n",
      "Training Epoch: 28 [17100/36045]\tLoss: 652.1129\n",
      "Training Epoch: 28 [17150/36045]\tLoss: 567.1471\n",
      "Training Epoch: 28 [17200/36045]\tLoss: 527.8412\n",
      "Training Epoch: 28 [17250/36045]\tLoss: 552.3255\n",
      "Training Epoch: 28 [17300/36045]\tLoss: 584.1588\n",
      "Training Epoch: 28 [17350/36045]\tLoss: 561.4955\n",
      "Training Epoch: 28 [17400/36045]\tLoss: 580.8960\n",
      "Training Epoch: 28 [17450/36045]\tLoss: 600.8962\n",
      "Training Epoch: 28 [17500/36045]\tLoss: 588.8268\n",
      "Training Epoch: 28 [17550/36045]\tLoss: 588.2891\n",
      "Training Epoch: 28 [17600/36045]\tLoss: 581.2811\n",
      "Training Epoch: 28 [17650/36045]\tLoss: 598.1779\n",
      "Training Epoch: 28 [17700/36045]\tLoss: 576.8939\n",
      "Training Epoch: 28 [17750/36045]\tLoss: 593.8508\n",
      "Training Epoch: 28 [17800/36045]\tLoss: 584.4784\n",
      "Training Epoch: 28 [17850/36045]\tLoss: 594.6702\n",
      "Training Epoch: 28 [17900/36045]\tLoss: 623.4164\n",
      "Training Epoch: 28 [17950/36045]\tLoss: 635.0248\n",
      "Training Epoch: 28 [18000/36045]\tLoss: 625.1372\n",
      "Training Epoch: 28 [18050/36045]\tLoss: 692.1387\n",
      "Training Epoch: 28 [18100/36045]\tLoss: 694.3820\n",
      "Training Epoch: 28 [18150/36045]\tLoss: 705.4353\n",
      "Training Epoch: 28 [18200/36045]\tLoss: 687.5239\n",
      "Training Epoch: 28 [18250/36045]\tLoss: 708.4722\n",
      "Training Epoch: 28 [18300/36045]\tLoss: 657.7524\n",
      "Training Epoch: 28 [18350/36045]\tLoss: 728.4946\n",
      "Training Epoch: 28 [18400/36045]\tLoss: 701.6270\n",
      "Training Epoch: 28 [18450/36045]\tLoss: 681.8580\n",
      "Training Epoch: 28 [18500/36045]\tLoss: 680.9267\n",
      "Training Epoch: 28 [18550/36045]\tLoss: 667.8330\n",
      "Training Epoch: 28 [18600/36045]\tLoss: 657.3224\n",
      "Training Epoch: 28 [18650/36045]\tLoss: 704.1686\n",
      "Training Epoch: 28 [18700/36045]\tLoss: 741.0091\n",
      "Training Epoch: 28 [18750/36045]\tLoss: 727.1934\n",
      "Training Epoch: 28 [18800/36045]\tLoss: 751.2248\n",
      "Training Epoch: 28 [18850/36045]\tLoss: 695.4887\n",
      "Training Epoch: 28 [18900/36045]\tLoss: 743.9464\n",
      "Training Epoch: 28 [18950/36045]\tLoss: 685.2466\n",
      "Training Epoch: 28 [19000/36045]\tLoss: 573.5790\n",
      "Training Epoch: 28 [19050/36045]\tLoss: 556.1494\n",
      "Training Epoch: 28 [19100/36045]\tLoss: 565.1573\n",
      "Training Epoch: 28 [19150/36045]\tLoss: 554.4089\n",
      "Training Epoch: 28 [19200/36045]\tLoss: 583.0622\n",
      "Training Epoch: 28 [19250/36045]\tLoss: 597.6785\n",
      "Training Epoch: 28 [19300/36045]\tLoss: 608.2089\n",
      "Training Epoch: 28 [19350/36045]\tLoss: 591.7404\n",
      "Training Epoch: 28 [19400/36045]\tLoss: 613.7187\n",
      "Training Epoch: 28 [19450/36045]\tLoss: 604.4990\n",
      "Training Epoch: 28 [19500/36045]\tLoss: 606.2561\n",
      "Training Epoch: 28 [19550/36045]\tLoss: 604.7289\n",
      "Training Epoch: 28 [19600/36045]\tLoss: 646.0182\n",
      "Training Epoch: 28 [19650/36045]\tLoss: 853.5158\n",
      "Training Epoch: 28 [19700/36045]\tLoss: 812.3682\n",
      "Training Epoch: 28 [19750/36045]\tLoss: 815.2460\n",
      "Training Epoch: 28 [19800/36045]\tLoss: 814.1714\n",
      "Training Epoch: 28 [19850/36045]\tLoss: 541.4523\n",
      "Training Epoch: 28 [19900/36045]\tLoss: 519.5049\n",
      "Training Epoch: 28 [19950/36045]\tLoss: 523.0745\n",
      "Training Epoch: 28 [20000/36045]\tLoss: 521.8048\n",
      "Training Epoch: 28 [20050/36045]\tLoss: 584.3108\n",
      "Training Epoch: 28 [20100/36045]\tLoss: 590.5836\n",
      "Training Epoch: 28 [20150/36045]\tLoss: 592.8809\n",
      "Training Epoch: 28 [20200/36045]\tLoss: 592.9209\n",
      "Training Epoch: 28 [20250/36045]\tLoss: 631.3179\n",
      "Training Epoch: 28 [20300/36045]\tLoss: 667.4674\n",
      "Training Epoch: 28 [20350/36045]\tLoss: 686.5652\n",
      "Training Epoch: 28 [20400/36045]\tLoss: 703.2087\n",
      "Training Epoch: 28 [20450/36045]\tLoss: 673.9444\n",
      "Training Epoch: 28 [20500/36045]\tLoss: 657.8778\n",
      "Training Epoch: 28 [20550/36045]\tLoss: 578.9921\n",
      "Training Epoch: 28 [20600/36045]\tLoss: 589.9961\n",
      "Training Epoch: 28 [20650/36045]\tLoss: 586.6943\n",
      "Training Epoch: 28 [20700/36045]\tLoss: 574.4889\n",
      "Training Epoch: 28 [20750/36045]\tLoss: 618.2354\n",
      "Training Epoch: 28 [20800/36045]\tLoss: 672.1780\n",
      "Training Epoch: 28 [20850/36045]\tLoss: 658.9229\n",
      "Training Epoch: 28 [20900/36045]\tLoss: 704.4357\n",
      "Training Epoch: 28 [20950/36045]\tLoss: 664.4123\n",
      "Training Epoch: 28 [21000/36045]\tLoss: 626.0882\n",
      "Training Epoch: 28 [21050/36045]\tLoss: 536.3550\n",
      "Training Epoch: 28 [21100/36045]\tLoss: 539.8501\n",
      "Training Epoch: 28 [21150/36045]\tLoss: 577.7571\n",
      "Training Epoch: 28 [21200/36045]\tLoss: 576.9703\n",
      "Training Epoch: 28 [21250/36045]\tLoss: 552.0835\n",
      "Training Epoch: 28 [21300/36045]\tLoss: 644.5759\n",
      "Training Epoch: 28 [21350/36045]\tLoss: 637.0746\n",
      "Training Epoch: 28 [21400/36045]\tLoss: 640.4390\n",
      "Training Epoch: 28 [21450/36045]\tLoss: 646.7469\n",
      "Training Epoch: 28 [21500/36045]\tLoss: 649.6633\n",
      "Training Epoch: 28 [21550/36045]\tLoss: 744.8699\n",
      "Training Epoch: 28 [21600/36045]\tLoss: 744.5089\n",
      "Training Epoch: 28 [21650/36045]\tLoss: 757.3856\n",
      "Training Epoch: 28 [21700/36045]\tLoss: 758.9880\n",
      "Training Epoch: 28 [21750/36045]\tLoss: 729.4184\n",
      "Training Epoch: 28 [21800/36045]\tLoss: 538.9801\n",
      "Training Epoch: 28 [21850/36045]\tLoss: 521.8816\n",
      "Training Epoch: 28 [21900/36045]\tLoss: 532.4451\n",
      "Training Epoch: 28 [21950/36045]\tLoss: 531.8920\n",
      "Training Epoch: 28 [22000/36045]\tLoss: 535.7314\n",
      "Training Epoch: 28 [22050/36045]\tLoss: 559.1967\n",
      "Training Epoch: 28 [22100/36045]\tLoss: 551.6524\n",
      "Training Epoch: 28 [22150/36045]\tLoss: 536.4572\n",
      "Training Epoch: 28 [22200/36045]\tLoss: 553.7379\n",
      "Training Epoch: 28 [22250/36045]\tLoss: 558.8098\n",
      "Training Epoch: 28 [22300/36045]\tLoss: 611.6982\n",
      "Training Epoch: 28 [22350/36045]\tLoss: 637.8868\n",
      "Training Epoch: 28 [22400/36045]\tLoss: 652.9396\n",
      "Training Epoch: 28 [22450/36045]\tLoss: 640.6975\n",
      "Training Epoch: 28 [22500/36045]\tLoss: 622.4276\n",
      "Training Epoch: 28 [22550/36045]\tLoss: 658.9784\n",
      "Training Epoch: 28 [22600/36045]\tLoss: 714.8893\n",
      "Training Epoch: 28 [22650/36045]\tLoss: 750.5883\n",
      "Training Epoch: 28 [22700/36045]\tLoss: 773.6068\n",
      "Training Epoch: 28 [22750/36045]\tLoss: 793.5808\n",
      "Training Epoch: 28 [22800/36045]\tLoss: 825.1433\n",
      "Training Epoch: 28 [22850/36045]\tLoss: 686.4187\n",
      "Training Epoch: 28 [22900/36045]\tLoss: 691.0168\n",
      "Training Epoch: 28 [22950/36045]\tLoss: 670.0739\n",
      "Training Epoch: 28 [23000/36045]\tLoss: 668.0090\n",
      "Training Epoch: 28 [23050/36045]\tLoss: 594.1567\n",
      "Training Epoch: 28 [23100/36045]\tLoss: 610.2561\n",
      "Training Epoch: 28 [23150/36045]\tLoss: 598.3691\n",
      "Training Epoch: 28 [23200/36045]\tLoss: 566.7859\n",
      "Training Epoch: 28 [23250/36045]\tLoss: 569.7296\n",
      "Training Epoch: 28 [23300/36045]\tLoss: 566.3986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [23350/36045]\tLoss: 587.7946\n",
      "Training Epoch: 28 [23400/36045]\tLoss: 636.8228\n",
      "Training Epoch: 28 [23450/36045]\tLoss: 629.5972\n",
      "Training Epoch: 28 [23500/36045]\tLoss: 606.8936\n",
      "Training Epoch: 28 [23550/36045]\tLoss: 651.0997\n",
      "Training Epoch: 28 [23600/36045]\tLoss: 735.1188\n",
      "Training Epoch: 28 [23650/36045]\tLoss: 748.4913\n",
      "Training Epoch: 28 [23700/36045]\tLoss: 757.2664\n",
      "Training Epoch: 28 [23750/36045]\tLoss: 732.0175\n",
      "Training Epoch: 28 [23800/36045]\tLoss: 584.1785\n",
      "Training Epoch: 28 [23850/36045]\tLoss: 610.8436\n",
      "Training Epoch: 28 [23900/36045]\tLoss: 600.4639\n",
      "Training Epoch: 28 [23950/36045]\tLoss: 583.2729\n",
      "Training Epoch: 28 [24000/36045]\tLoss: 559.5930\n",
      "Training Epoch: 28 [24050/36045]\tLoss: 517.1884\n",
      "Training Epoch: 28 [24100/36045]\tLoss: 544.2454\n",
      "Training Epoch: 28 [24150/36045]\tLoss: 537.8812\n",
      "Training Epoch: 28 [24200/36045]\tLoss: 533.9066\n",
      "Training Epoch: 28 [24250/36045]\tLoss: 518.1895\n",
      "Training Epoch: 28 [24300/36045]\tLoss: 559.3327\n",
      "Training Epoch: 28 [24350/36045]\tLoss: 573.0675\n",
      "Training Epoch: 28 [24400/36045]\tLoss: 589.2154\n",
      "Training Epoch: 28 [24450/36045]\tLoss: 561.9350\n",
      "Training Epoch: 28 [24500/36045]\tLoss: 592.1391\n",
      "Training Epoch: 28 [24550/36045]\tLoss: 682.6566\n",
      "Training Epoch: 28 [24600/36045]\tLoss: 674.6364\n",
      "Training Epoch: 28 [24650/36045]\tLoss: 647.2625\n",
      "Training Epoch: 28 [24700/36045]\tLoss: 657.4989\n",
      "Training Epoch: 28 [24750/36045]\tLoss: 607.4628\n",
      "Training Epoch: 28 [24800/36045]\tLoss: 502.9683\n",
      "Training Epoch: 28 [24850/36045]\tLoss: 522.6377\n",
      "Training Epoch: 28 [24900/36045]\tLoss: 519.5446\n",
      "Training Epoch: 28 [24950/36045]\tLoss: 522.0776\n",
      "Training Epoch: 28 [25000/36045]\tLoss: 501.9382\n",
      "Training Epoch: 28 [25050/36045]\tLoss: 479.3148\n",
      "Training Epoch: 28 [25100/36045]\tLoss: 429.7280\n",
      "Training Epoch: 28 [25150/36045]\tLoss: 398.0650\n",
      "Training Epoch: 28 [25200/36045]\tLoss: 393.0571\n",
      "Training Epoch: 28 [25250/36045]\tLoss: 421.1731\n",
      "Training Epoch: 28 [25300/36045]\tLoss: 553.0039\n",
      "Training Epoch: 28 [25350/36045]\tLoss: 550.1401\n",
      "Training Epoch: 28 [25400/36045]\tLoss: 512.6242\n",
      "Training Epoch: 28 [25450/36045]\tLoss: 515.3296\n",
      "Training Epoch: 28 [25500/36045]\tLoss: 559.9423\n",
      "Training Epoch: 28 [25550/36045]\tLoss: 652.9331\n",
      "Training Epoch: 28 [25600/36045]\tLoss: 657.8268\n",
      "Training Epoch: 28 [25650/36045]\tLoss: 634.8378\n",
      "Training Epoch: 28 [25700/36045]\tLoss: 643.9732\n",
      "Training Epoch: 28 [25750/36045]\tLoss: 620.4131\n",
      "Training Epoch: 28 [25800/36045]\tLoss: 391.0018\n",
      "Training Epoch: 28 [25850/36045]\tLoss: 401.0300\n",
      "Training Epoch: 28 [25900/36045]\tLoss: 382.0492\n",
      "Training Epoch: 28 [25950/36045]\tLoss: 390.9079\n",
      "Training Epoch: 28 [26000/36045]\tLoss: 478.8978\n",
      "Training Epoch: 28 [26050/36045]\tLoss: 651.7802\n",
      "Training Epoch: 28 [26100/36045]\tLoss: 679.6349\n",
      "Training Epoch: 28 [26150/36045]\tLoss: 679.9731\n",
      "Training Epoch: 28 [26200/36045]\tLoss: 653.2283\n",
      "Training Epoch: 28 [26250/36045]\tLoss: 683.7262\n",
      "Training Epoch: 28 [26300/36045]\tLoss: 616.4160\n",
      "Training Epoch: 28 [26350/36045]\tLoss: 626.9014\n",
      "Training Epoch: 28 [26400/36045]\tLoss: 604.4652\n",
      "Training Epoch: 28 [26450/36045]\tLoss: 534.1846\n",
      "Training Epoch: 28 [26500/36045]\tLoss: 635.7088\n",
      "Training Epoch: 28 [26550/36045]\tLoss: 637.3251\n",
      "Training Epoch: 28 [26600/36045]\tLoss: 633.0812\n",
      "Training Epoch: 28 [26650/36045]\tLoss: 649.3980\n",
      "Training Epoch: 28 [26700/36045]\tLoss: 629.1135\n",
      "Training Epoch: 28 [26750/36045]\tLoss: 589.0204\n",
      "Training Epoch: 28 [26800/36045]\tLoss: 433.7124\n",
      "Training Epoch: 28 [26850/36045]\tLoss: 359.8763\n",
      "Training Epoch: 28 [26900/36045]\tLoss: 363.0269\n",
      "Training Epoch: 28 [26950/36045]\tLoss: 399.1447\n",
      "Training Epoch: 28 [27000/36045]\tLoss: 648.7895\n",
      "Training Epoch: 28 [27050/36045]\tLoss: 678.4492\n",
      "Training Epoch: 28 [27100/36045]\tLoss: 657.1454\n",
      "Training Epoch: 28 [27150/36045]\tLoss: 698.0425\n",
      "Training Epoch: 28 [27200/36045]\tLoss: 511.5343\n",
      "Training Epoch: 28 [27250/36045]\tLoss: 504.1141\n",
      "Training Epoch: 28 [27300/36045]\tLoss: 490.7410\n",
      "Training Epoch: 28 [27350/36045]\tLoss: 489.6500\n",
      "Training Epoch: 28 [27400/36045]\tLoss: 488.7275\n",
      "Training Epoch: 28 [27450/36045]\tLoss: 616.7687\n",
      "Training Epoch: 28 [27500/36045]\tLoss: 661.2722\n",
      "Training Epoch: 28 [27550/36045]\tLoss: 653.8408\n",
      "Training Epoch: 28 [27600/36045]\tLoss: 665.6163\n",
      "Training Epoch: 28 [27650/36045]\tLoss: 657.3269\n",
      "Training Epoch: 28 [27700/36045]\tLoss: 686.8890\n",
      "Training Epoch: 28 [27750/36045]\tLoss: 699.1470\n",
      "Training Epoch: 28 [27800/36045]\tLoss: 685.5690\n",
      "Training Epoch: 28 [27850/36045]\tLoss: 674.8333\n",
      "Training Epoch: 28 [27900/36045]\tLoss: 609.0234\n",
      "Training Epoch: 28 [27950/36045]\tLoss: 506.1693\n",
      "Training Epoch: 28 [28000/36045]\tLoss: 482.2414\n",
      "Training Epoch: 28 [28050/36045]\tLoss: 492.8763\n",
      "Training Epoch: 28 [28100/36045]\tLoss: 484.5571\n",
      "Training Epoch: 28 [28150/36045]\tLoss: 509.5452\n",
      "Training Epoch: 28 [28200/36045]\tLoss: 516.0479\n",
      "Training Epoch: 28 [28250/36045]\tLoss: 509.5344\n",
      "Training Epoch: 28 [28300/36045]\tLoss: 483.3502\n",
      "Training Epoch: 28 [28350/36045]\tLoss: 479.5710\n",
      "Training Epoch: 28 [28400/36045]\tLoss: 812.0917\n",
      "Training Epoch: 28 [28450/36045]\tLoss: 742.7917\n",
      "Training Epoch: 28 [28500/36045]\tLoss: 642.1477\n",
      "Training Epoch: 28 [28550/36045]\tLoss: 589.4431\n",
      "Training Epoch: 28 [28600/36045]\tLoss: 622.8721\n",
      "Training Epoch: 28 [28650/36045]\tLoss: 693.2303\n",
      "Training Epoch: 28 [28700/36045]\tLoss: 687.6948\n",
      "Training Epoch: 28 [28750/36045]\tLoss: 674.3551\n",
      "Training Epoch: 28 [28800/36045]\tLoss: 682.9371\n",
      "Training Epoch: 28 [28850/36045]\tLoss: 591.6145\n",
      "Training Epoch: 28 [28900/36045]\tLoss: 478.7284\n",
      "Training Epoch: 28 [28950/36045]\tLoss: 477.3261\n",
      "Training Epoch: 28 [29000/36045]\tLoss: 475.2731\n",
      "Training Epoch: 28 [29050/36045]\tLoss: 482.6073\n",
      "Training Epoch: 28 [29100/36045]\tLoss: 502.1259\n",
      "Training Epoch: 28 [29150/36045]\tLoss: 490.0861\n",
      "Training Epoch: 28 [29200/36045]\tLoss: 475.3739\n",
      "Training Epoch: 28 [29250/36045]\tLoss: 464.3513\n",
      "Training Epoch: 28 [29300/36045]\tLoss: 528.3749\n",
      "Training Epoch: 28 [29350/36045]\tLoss: 624.7128\n",
      "Training Epoch: 28 [29400/36045]\tLoss: 642.7463\n",
      "Training Epoch: 28 [29450/36045]\tLoss: 661.8521\n",
      "Training Epoch: 28 [29500/36045]\tLoss: 676.1747\n",
      "Training Epoch: 28 [29550/36045]\tLoss: 643.1479\n",
      "Training Epoch: 28 [29600/36045]\tLoss: 543.2480\n",
      "Training Epoch: 28 [29650/36045]\tLoss: 526.1572\n",
      "Training Epoch: 28 [29700/36045]\tLoss: 469.7318\n",
      "Training Epoch: 28 [29750/36045]\tLoss: 469.2790\n",
      "Training Epoch: 28 [29800/36045]\tLoss: 515.6744\n",
      "Training Epoch: 28 [29850/36045]\tLoss: 590.4815\n",
      "Training Epoch: 28 [29900/36045]\tLoss: 587.2715\n",
      "Training Epoch: 28 [29950/36045]\tLoss: 609.5932\n",
      "Training Epoch: 28 [30000/36045]\tLoss: 585.0549\n",
      "Training Epoch: 28 [30050/36045]\tLoss: 591.0956\n",
      "Training Epoch: 28 [30100/36045]\tLoss: 721.0656\n",
      "Training Epoch: 28 [30150/36045]\tLoss: 704.8232\n",
      "Training Epoch: 28 [30200/36045]\tLoss: 664.9510\n",
      "Training Epoch: 28 [30250/36045]\tLoss: 714.0635\n",
      "Training Epoch: 28 [30300/36045]\tLoss: 699.4281\n",
      "Training Epoch: 28 [30350/36045]\tLoss: 543.7223\n",
      "Training Epoch: 28 [30400/36045]\tLoss: 528.3498\n",
      "Training Epoch: 28 [30450/36045]\tLoss: 529.2361\n",
      "Training Epoch: 28 [30500/36045]\tLoss: 494.7036\n",
      "Training Epoch: 28 [30550/36045]\tLoss: 458.3784\n",
      "Training Epoch: 28 [30600/36045]\tLoss: 447.9114\n",
      "Training Epoch: 28 [30650/36045]\tLoss: 437.9707\n",
      "Training Epoch: 28 [30700/36045]\tLoss: 455.7825\n",
      "Training Epoch: 28 [30750/36045]\tLoss: 442.4103\n",
      "Training Epoch: 28 [30800/36045]\tLoss: 469.7117\n",
      "Training Epoch: 28 [30850/36045]\tLoss: 461.4510\n",
      "Training Epoch: 28 [30900/36045]\tLoss: 474.3900\n",
      "Training Epoch: 28 [30950/36045]\tLoss: 498.4760\n",
      "Training Epoch: 28 [31000/36045]\tLoss: 489.9052\n",
      "Training Epoch: 28 [31050/36045]\tLoss: 409.3284\n",
      "Training Epoch: 28 [31100/36045]\tLoss: 399.9489\n",
      "Training Epoch: 28 [31150/36045]\tLoss: 406.9154\n",
      "Training Epoch: 28 [31200/36045]\tLoss: 507.1695\n",
      "Training Epoch: 28 [31250/36045]\tLoss: 658.0434\n",
      "Training Epoch: 28 [31300/36045]\tLoss: 628.1488\n",
      "Training Epoch: 28 [31350/36045]\tLoss: 643.9901\n",
      "Training Epoch: 28 [31400/36045]\tLoss: 623.3007\n",
      "Training Epoch: 28 [31450/36045]\tLoss: 638.8779\n",
      "Training Epoch: 28 [31500/36045]\tLoss: 650.9835\n",
      "Training Epoch: 28 [31550/36045]\tLoss: 659.1777\n",
      "Training Epoch: 28 [31600/36045]\tLoss: 619.5550\n",
      "Training Epoch: 28 [31650/36045]\tLoss: 662.0909\n",
      "Training Epoch: 28 [31700/36045]\tLoss: 480.5473\n",
      "Training Epoch: 28 [31750/36045]\tLoss: 398.0824\n",
      "Training Epoch: 28 [31800/36045]\tLoss: 379.3737\n",
      "Training Epoch: 28 [31850/36045]\tLoss: 388.5380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 28 [31900/36045]\tLoss: 608.3166\n",
      "Training Epoch: 28 [31950/36045]\tLoss: 784.3783\n",
      "Training Epoch: 28 [32000/36045]\tLoss: 895.6898\n",
      "Training Epoch: 28 [32050/36045]\tLoss: 849.8967\n",
      "Training Epoch: 28 [32100/36045]\tLoss: 839.6235\n",
      "Training Epoch: 28 [32150/36045]\tLoss: 653.4612\n",
      "Training Epoch: 28 [32200/36045]\tLoss: 657.3688\n",
      "Training Epoch: 28 [32250/36045]\tLoss: 668.1471\n",
      "Training Epoch: 28 [32300/36045]\tLoss: 649.8400\n",
      "Training Epoch: 28 [32350/36045]\tLoss: 644.9237\n",
      "Training Epoch: 28 [32400/36045]\tLoss: 605.2618\n",
      "Training Epoch: 28 [32450/36045]\tLoss: 498.7299\n",
      "Training Epoch: 28 [32500/36045]\tLoss: 479.5240\n",
      "Training Epoch: 28 [32550/36045]\tLoss: 482.0647\n",
      "Training Epoch: 28 [32600/36045]\tLoss: 478.5798\n",
      "Training Epoch: 28 [32650/36045]\tLoss: 612.9218\n",
      "Training Epoch: 28 [32700/36045]\tLoss: 668.4191\n",
      "Training Epoch: 28 [32750/36045]\tLoss: 637.0754\n",
      "Training Epoch: 28 [32800/36045]\tLoss: 653.5496\n",
      "Training Epoch: 28 [32850/36045]\tLoss: 603.4467\n",
      "Training Epoch: 28 [32900/36045]\tLoss: 485.1514\n",
      "Training Epoch: 28 [32950/36045]\tLoss: 507.6700\n",
      "Training Epoch: 28 [33000/36045]\tLoss: 507.2660\n",
      "Training Epoch: 28 [33050/36045]\tLoss: 481.6154\n",
      "Training Epoch: 28 [33100/36045]\tLoss: 547.5162\n",
      "Training Epoch: 28 [33150/36045]\tLoss: 742.6613\n",
      "Training Epoch: 28 [33200/36045]\tLoss: 723.4709\n",
      "Training Epoch: 28 [33250/36045]\tLoss: 745.3956\n",
      "Training Epoch: 28 [33300/36045]\tLoss: 793.5549\n",
      "Training Epoch: 28 [33350/36045]\tLoss: 608.9462\n",
      "Training Epoch: 28 [33400/36045]\tLoss: 447.4468\n",
      "Training Epoch: 28 [33450/36045]\tLoss: 442.5974\n",
      "Training Epoch: 28 [33500/36045]\tLoss: 455.5706\n",
      "Training Epoch: 28 [33550/36045]\tLoss: 472.4066\n",
      "Training Epoch: 28 [33600/36045]\tLoss: 474.0582\n",
      "Training Epoch: 28 [33650/36045]\tLoss: 631.8642\n",
      "Training Epoch: 28 [33700/36045]\tLoss: 611.4663\n",
      "Training Epoch: 28 [33750/36045]\tLoss: 633.1810\n",
      "Training Epoch: 28 [33800/36045]\tLoss: 628.6979\n",
      "Training Epoch: 28 [33850/36045]\tLoss: 631.2991\n",
      "Training Epoch: 28 [33900/36045]\tLoss: 643.2412\n",
      "Training Epoch: 28 [33950/36045]\tLoss: 654.3983\n",
      "Training Epoch: 28 [34000/36045]\tLoss: 641.2491\n",
      "Training Epoch: 28 [34050/36045]\tLoss: 645.5781\n",
      "Training Epoch: 28 [34100/36045]\tLoss: 621.3683\n",
      "Training Epoch: 28 [34150/36045]\tLoss: 577.4872\n",
      "Training Epoch: 28 [34200/36045]\tLoss: 546.7397\n",
      "Training Epoch: 28 [34250/36045]\tLoss: 561.1666\n",
      "Training Epoch: 28 [34300/36045]\tLoss: 480.3319\n",
      "Training Epoch: 28 [34350/36045]\tLoss: 505.7492\n",
      "Training Epoch: 28 [34400/36045]\tLoss: 497.0830\n",
      "Training Epoch: 28 [34450/36045]\tLoss: 467.1349\n",
      "Training Epoch: 28 [34500/36045]\tLoss: 498.6789\n",
      "Training Epoch: 28 [34550/36045]\tLoss: 489.3903\n",
      "Training Epoch: 28 [34600/36045]\tLoss: 492.7721\n",
      "Training Epoch: 28 [34650/36045]\tLoss: 600.8392\n",
      "Training Epoch: 28 [34700/36045]\tLoss: 636.2305\n",
      "Training Epoch: 28 [34750/36045]\tLoss: 564.6876\n",
      "Training Epoch: 28 [34800/36045]\tLoss: 645.9371\n",
      "Training Epoch: 28 [34850/36045]\tLoss: 654.1141\n",
      "Training Epoch: 28 [34900/36045]\tLoss: 718.1035\n",
      "Training Epoch: 28 [34950/36045]\tLoss: 704.7221\n",
      "Training Epoch: 28 [35000/36045]\tLoss: 705.6891\n",
      "Training Epoch: 28 [35050/36045]\tLoss: 691.6660\n",
      "Training Epoch: 28 [35100/36045]\tLoss: 584.7717\n",
      "Training Epoch: 28 [35150/36045]\tLoss: 577.8345\n",
      "Training Epoch: 28 [35200/36045]\tLoss: 489.6642\n",
      "Training Epoch: 28 [35250/36045]\tLoss: 537.6260\n",
      "Training Epoch: 28 [35300/36045]\tLoss: 551.9553\n",
      "Training Epoch: 28 [35350/36045]\tLoss: 625.3511\n",
      "Training Epoch: 28 [35400/36045]\tLoss: 660.2031\n",
      "Training Epoch: 28 [35450/36045]\tLoss: 630.1379\n",
      "Training Epoch: 28 [35500/36045]\tLoss: 611.6483\n",
      "Training Epoch: 28 [35550/36045]\tLoss: 596.7929\n",
      "Training Epoch: 28 [35600/36045]\tLoss: 644.8242\n",
      "Training Epoch: 28 [35650/36045]\tLoss: 718.6529\n",
      "Training Epoch: 28 [35700/36045]\tLoss: 646.0150\n",
      "Training Epoch: 28 [35750/36045]\tLoss: 704.3500\n",
      "Training Epoch: 28 [35800/36045]\tLoss: 710.8183\n",
      "Training Epoch: 28 [35850/36045]\tLoss: 684.8517\n",
      "Training Epoch: 28 [35900/36045]\tLoss: 710.6586\n",
      "Training Epoch: 28 [35950/36045]\tLoss: 708.2620\n",
      "Training Epoch: 28 [36000/36045]\tLoss: 700.3571\n",
      "Training Epoch: 28 [36045/36045]\tLoss: 683.7385\n",
      "Training Epoch: 28 [4004/4004]\tLoss: 636.9666\n",
      "Training Epoch: 29 [50/36045]\tLoss: 636.5427\n",
      "Training Epoch: 29 [100/36045]\tLoss: 610.3859\n",
      "Training Epoch: 29 [150/36045]\tLoss: 608.5316\n",
      "Training Epoch: 29 [200/36045]\tLoss: 595.3294\n",
      "Training Epoch: 29 [250/36045]\tLoss: 710.2404\n",
      "Training Epoch: 29 [300/36045]\tLoss: 774.2007\n",
      "Training Epoch: 29 [350/36045]\tLoss: 739.9889\n",
      "Training Epoch: 29 [400/36045]\tLoss: 735.6183\n",
      "Training Epoch: 29 [450/36045]\tLoss: 715.6315\n",
      "Training Epoch: 29 [500/36045]\tLoss: 665.6010\n",
      "Training Epoch: 29 [550/36045]\tLoss: 669.4827\n",
      "Training Epoch: 29 [600/36045]\tLoss: 650.9677\n",
      "Training Epoch: 29 [650/36045]\tLoss: 674.4109\n",
      "Training Epoch: 29 [700/36045]\tLoss: 661.3599\n",
      "Training Epoch: 29 [750/36045]\tLoss: 641.9922\n",
      "Training Epoch: 29 [800/36045]\tLoss: 655.7668\n",
      "Training Epoch: 29 [850/36045]\tLoss: 636.7757\n",
      "Training Epoch: 29 [900/36045]\tLoss: 607.1961\n",
      "Training Epoch: 29 [950/36045]\tLoss: 575.3139\n",
      "Training Epoch: 29 [1000/36045]\tLoss: 555.5398\n",
      "Training Epoch: 29 [1050/36045]\tLoss: 557.7131\n",
      "Training Epoch: 29 [1100/36045]\tLoss: 542.7940\n",
      "Training Epoch: 29 [1150/36045]\tLoss: 551.5652\n",
      "Training Epoch: 29 [1200/36045]\tLoss: 582.3922\n",
      "Training Epoch: 29 [1250/36045]\tLoss: 666.4886\n",
      "Training Epoch: 29 [1300/36045]\tLoss: 673.0893\n",
      "Training Epoch: 29 [1350/36045]\tLoss: 675.4689\n",
      "Training Epoch: 29 [1400/36045]\tLoss: 701.9755\n",
      "Training Epoch: 29 [1450/36045]\tLoss: 678.9341\n",
      "Training Epoch: 29 [1500/36045]\tLoss: 622.9031\n",
      "Training Epoch: 29 [1550/36045]\tLoss: 638.3701\n",
      "Training Epoch: 29 [1600/36045]\tLoss: 649.3079\n",
      "Training Epoch: 29 [1650/36045]\tLoss: 636.3136\n",
      "Training Epoch: 29 [1700/36045]\tLoss: 648.5934\n",
      "Training Epoch: 29 [1750/36045]\tLoss: 690.2007\n",
      "Training Epoch: 29 [1800/36045]\tLoss: 671.2266\n",
      "Training Epoch: 29 [1850/36045]\tLoss: 688.4144\n",
      "Training Epoch: 29 [1900/36045]\tLoss: 644.4366\n",
      "Training Epoch: 29 [1950/36045]\tLoss: 655.5345\n",
      "Training Epoch: 29 [2000/36045]\tLoss: 592.8851\n",
      "Training Epoch: 29 [2050/36045]\tLoss: 595.4981\n",
      "Training Epoch: 29 [2100/36045]\tLoss: 627.1922\n",
      "Training Epoch: 29 [2150/36045]\tLoss: 606.4096\n",
      "Training Epoch: 29 [2200/36045]\tLoss: 563.6175\n",
      "Training Epoch: 29 [2250/36045]\tLoss: 532.2786\n",
      "Training Epoch: 29 [2300/36045]\tLoss: 558.3616\n",
      "Training Epoch: 29 [2350/36045]\tLoss: 533.5416\n",
      "Training Epoch: 29 [2400/36045]\tLoss: 542.5711\n",
      "Training Epoch: 29 [2450/36045]\tLoss: 691.2982\n",
      "Training Epoch: 29 [2500/36045]\tLoss: 726.4363\n",
      "Training Epoch: 29 [2550/36045]\tLoss: 723.6077\n",
      "Training Epoch: 29 [2600/36045]\tLoss: 732.4810\n",
      "Training Epoch: 29 [2650/36045]\tLoss: 857.8989\n",
      "Training Epoch: 29 [2700/36045]\tLoss: 945.1268\n",
      "Training Epoch: 29 [2750/36045]\tLoss: 1017.0986\n",
      "Training Epoch: 29 [2800/36045]\tLoss: 1026.9817\n",
      "Training Epoch: 29 [2850/36045]\tLoss: 795.6266\n",
      "Training Epoch: 29 [2900/36045]\tLoss: 760.2111\n",
      "Training Epoch: 29 [2950/36045]\tLoss: 733.9529\n",
      "Training Epoch: 29 [3000/36045]\tLoss: 728.4515\n",
      "Training Epoch: 29 [3050/36045]\tLoss: 759.3375\n",
      "Training Epoch: 29 [3100/36045]\tLoss: 694.9315\n",
      "Training Epoch: 29 [3150/36045]\tLoss: 536.2792\n",
      "Training Epoch: 29 [3200/36045]\tLoss: 556.2804\n",
      "Training Epoch: 29 [3250/36045]\tLoss: 523.7870\n",
      "Training Epoch: 29 [3300/36045]\tLoss: 495.8018\n",
      "Training Epoch: 29 [3350/36045]\tLoss: 522.8903\n",
      "Training Epoch: 29 [3400/36045]\tLoss: 548.8849\n",
      "Training Epoch: 29 [3450/36045]\tLoss: 589.4466\n",
      "Training Epoch: 29 [3500/36045]\tLoss: 576.7492\n",
      "Training Epoch: 29 [3550/36045]\tLoss: 553.0268\n",
      "Training Epoch: 29 [3600/36045]\tLoss: 592.6786\n",
      "Training Epoch: 29 [3650/36045]\tLoss: 684.8817\n",
      "Training Epoch: 29 [3700/36045]\tLoss: 691.4535\n",
      "Training Epoch: 29 [3750/36045]\tLoss: 660.1434\n",
      "Training Epoch: 29 [3800/36045]\tLoss: 654.9679\n",
      "Training Epoch: 29 [3850/36045]\tLoss: 654.3829\n",
      "Training Epoch: 29 [3900/36045]\tLoss: 659.6336\n",
      "Training Epoch: 29 [3950/36045]\tLoss: 636.2755\n",
      "Training Epoch: 29 [4000/36045]\tLoss: 642.2473\n",
      "Training Epoch: 29 [4050/36045]\tLoss: 590.1840\n",
      "Training Epoch: 29 [4100/36045]\tLoss: 575.3472\n",
      "Training Epoch: 29 [4150/36045]\tLoss: 591.6340\n",
      "Training Epoch: 29 [4200/36045]\tLoss: 586.2759\n",
      "Training Epoch: 29 [4250/36045]\tLoss: 588.4959\n",
      "Training Epoch: 29 [4300/36045]\tLoss: 606.7102\n",
      "Training Epoch: 29 [4350/36045]\tLoss: 589.2693\n",
      "Training Epoch: 29 [4400/36045]\tLoss: 564.1659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [4450/36045]\tLoss: 617.0673\n",
      "Training Epoch: 29 [4500/36045]\tLoss: 660.7494\n",
      "Training Epoch: 29 [4550/36045]\tLoss: 665.6222\n",
      "Training Epoch: 29 [4600/36045]\tLoss: 688.5738\n",
      "Training Epoch: 29 [4650/36045]\tLoss: 677.8547\n",
      "Training Epoch: 29 [4700/36045]\tLoss: 626.1221\n",
      "Training Epoch: 29 [4750/36045]\tLoss: 608.4880\n",
      "Training Epoch: 29 [4800/36045]\tLoss: 634.4634\n",
      "Training Epoch: 29 [4850/36045]\tLoss: 620.9553\n",
      "Training Epoch: 29 [4900/36045]\tLoss: 604.0283\n",
      "Training Epoch: 29 [4950/36045]\tLoss: 620.8108\n",
      "Training Epoch: 29 [5000/36045]\tLoss: 651.4169\n",
      "Training Epoch: 29 [5050/36045]\tLoss: 630.9290\n",
      "Training Epoch: 29 [5100/36045]\tLoss: 641.3765\n",
      "Training Epoch: 29 [5150/36045]\tLoss: 625.7303\n",
      "Training Epoch: 29 [5200/36045]\tLoss: 623.7197\n",
      "Training Epoch: 29 [5250/36045]\tLoss: 617.1561\n",
      "Training Epoch: 29 [5300/36045]\tLoss: 617.8306\n",
      "Training Epoch: 29 [5350/36045]\tLoss: 640.7565\n",
      "Training Epoch: 29 [5400/36045]\tLoss: 616.9056\n",
      "Training Epoch: 29 [5450/36045]\tLoss: 585.0490\n",
      "Training Epoch: 29 [5500/36045]\tLoss: 614.1866\n",
      "Training Epoch: 29 [5550/36045]\tLoss: 602.2563\n",
      "Training Epoch: 29 [5600/36045]\tLoss: 685.1145\n",
      "Training Epoch: 29 [5650/36045]\tLoss: 648.8347\n",
      "Training Epoch: 29 [5700/36045]\tLoss: 608.9996\n",
      "Training Epoch: 29 [5750/36045]\tLoss: 593.5804\n",
      "Training Epoch: 29 [5800/36045]\tLoss: 626.2896\n",
      "Training Epoch: 29 [5850/36045]\tLoss: 613.0489\n",
      "Training Epoch: 29 [5900/36045]\tLoss: 705.2756\n",
      "Training Epoch: 29 [5950/36045]\tLoss: 722.7284\n",
      "Training Epoch: 29 [6000/36045]\tLoss: 707.7355\n",
      "Training Epoch: 29 [6050/36045]\tLoss: 684.4098\n",
      "Training Epoch: 29 [6100/36045]\tLoss: 689.0367\n",
      "Training Epoch: 29 [6150/36045]\tLoss: 675.2999\n",
      "Training Epoch: 29 [6200/36045]\tLoss: 677.3791\n",
      "Training Epoch: 29 [6250/36045]\tLoss: 698.7382\n",
      "Training Epoch: 29 [6300/36045]\tLoss: 710.5150\n",
      "Training Epoch: 29 [6350/36045]\tLoss: 758.0773\n",
      "Training Epoch: 29 [6400/36045]\tLoss: 629.6018\n",
      "Training Epoch: 29 [6450/36045]\tLoss: 581.5981\n",
      "Training Epoch: 29 [6500/36045]\tLoss: 592.3153\n",
      "Training Epoch: 29 [6550/36045]\tLoss: 609.4568\n",
      "Training Epoch: 29 [6600/36045]\tLoss: 608.6770\n",
      "Training Epoch: 29 [6650/36045]\tLoss: 686.7858\n",
      "Training Epoch: 29 [6700/36045]\tLoss: 718.5845\n",
      "Training Epoch: 29 [6750/36045]\tLoss: 694.0799\n",
      "Training Epoch: 29 [6800/36045]\tLoss: 697.1058\n",
      "Training Epoch: 29 [6850/36045]\tLoss: 684.9876\n",
      "Training Epoch: 29 [6900/36045]\tLoss: 610.1760\n",
      "Training Epoch: 29 [6950/36045]\tLoss: 575.2177\n",
      "Training Epoch: 29 [7000/36045]\tLoss: 611.9995\n",
      "Training Epoch: 29 [7050/36045]\tLoss: 625.5486\n",
      "Training Epoch: 29 [7100/36045]\tLoss: 624.8929\n",
      "Training Epoch: 29 [7150/36045]\tLoss: 635.7126\n",
      "Training Epoch: 29 [7200/36045]\tLoss: 639.3105\n",
      "Training Epoch: 29 [7250/36045]\tLoss: 636.9669\n",
      "Training Epoch: 29 [7300/36045]\tLoss: 623.8652\n",
      "Training Epoch: 29 [7350/36045]\tLoss: 620.1169\n",
      "Training Epoch: 29 [7400/36045]\tLoss: 562.5353\n",
      "Training Epoch: 29 [7450/36045]\tLoss: 565.9219\n",
      "Training Epoch: 29 [7500/36045]\tLoss: 560.7854\n",
      "Training Epoch: 29 [7550/36045]\tLoss: 537.4036\n",
      "Training Epoch: 29 [7600/36045]\tLoss: 596.7507\n",
      "Training Epoch: 29 [7650/36045]\tLoss: 639.6298\n",
      "Training Epoch: 29 [7700/36045]\tLoss: 609.0894\n",
      "Training Epoch: 29 [7750/36045]\tLoss: 623.8191\n",
      "Training Epoch: 29 [7800/36045]\tLoss: 612.2071\n",
      "Training Epoch: 29 [7850/36045]\tLoss: 592.3148\n",
      "Training Epoch: 29 [7900/36045]\tLoss: 624.8450\n",
      "Training Epoch: 29 [7950/36045]\tLoss: 622.1212\n",
      "Training Epoch: 29 [8000/36045]\tLoss: 640.2579\n",
      "Training Epoch: 29 [8050/36045]\tLoss: 604.3693\n",
      "Training Epoch: 29 [8100/36045]\tLoss: 630.1016\n",
      "Training Epoch: 29 [8150/36045]\tLoss: 713.2281\n",
      "Training Epoch: 29 [8200/36045]\tLoss: 699.7411\n",
      "Training Epoch: 29 [8250/36045]\tLoss: 667.0908\n",
      "Training Epoch: 29 [8300/36045]\tLoss: 727.0874\n",
      "Training Epoch: 29 [8350/36045]\tLoss: 667.9443\n",
      "Training Epoch: 29 [8400/36045]\tLoss: 598.6450\n",
      "Training Epoch: 29 [8450/36045]\tLoss: 560.9152\n",
      "Training Epoch: 29 [8500/36045]\tLoss: 595.8129\n",
      "Training Epoch: 29 [8550/36045]\tLoss: 587.8495\n",
      "Training Epoch: 29 [8600/36045]\tLoss: 581.5889\n",
      "Training Epoch: 29 [8650/36045]\tLoss: 620.3925\n",
      "Training Epoch: 29 [8700/36045]\tLoss: 655.9470\n",
      "Training Epoch: 29 [8750/36045]\tLoss: 644.0486\n",
      "Training Epoch: 29 [8800/36045]\tLoss: 649.8134\n",
      "Training Epoch: 29 [8850/36045]\tLoss: 643.0280\n",
      "Training Epoch: 29 [8900/36045]\tLoss: 580.3901\n",
      "Training Epoch: 29 [8950/36045]\tLoss: 592.9236\n",
      "Training Epoch: 29 [9000/36045]\tLoss: 608.5441\n",
      "Training Epoch: 29 [9050/36045]\tLoss: 609.5944\n",
      "Training Epoch: 29 [9100/36045]\tLoss: 627.4979\n",
      "Training Epoch: 29 [9150/36045]\tLoss: 463.7949\n",
      "Training Epoch: 29 [9200/36045]\tLoss: 347.6995\n",
      "Training Epoch: 29 [9250/36045]\tLoss: 377.0480\n",
      "Training Epoch: 29 [9300/36045]\tLoss: 387.9568\n",
      "Training Epoch: 29 [9350/36045]\tLoss: 357.5957\n",
      "Training Epoch: 29 [9400/36045]\tLoss: 700.7507\n",
      "Training Epoch: 29 [9450/36045]\tLoss: 744.3153\n",
      "Training Epoch: 29 [9500/36045]\tLoss: 731.2659\n",
      "Training Epoch: 29 [9550/36045]\tLoss: 773.5341\n",
      "Training Epoch: 29 [9600/36045]\tLoss: 574.8354\n",
      "Training Epoch: 29 [9650/36045]\tLoss: 579.1369\n",
      "Training Epoch: 29 [9700/36045]\tLoss: 564.4550\n",
      "Training Epoch: 29 [9750/36045]\tLoss: 563.8246\n",
      "Training Epoch: 29 [9800/36045]\tLoss: 736.5820\n",
      "Training Epoch: 29 [9850/36045]\tLoss: 777.9852\n",
      "Training Epoch: 29 [9900/36045]\tLoss: 790.7955\n",
      "Training Epoch: 29 [9950/36045]\tLoss: 770.4271\n",
      "Training Epoch: 29 [10000/36045]\tLoss: 711.8488\n",
      "Training Epoch: 29 [10050/36045]\tLoss: 586.3049\n",
      "Training Epoch: 29 [10100/36045]\tLoss: 593.4402\n",
      "Training Epoch: 29 [10150/36045]\tLoss: 602.8954\n",
      "Training Epoch: 29 [10200/36045]\tLoss: 591.7667\n",
      "Training Epoch: 29 [10250/36045]\tLoss: 707.5754\n",
      "Training Epoch: 29 [10300/36045]\tLoss: 687.2235\n",
      "Training Epoch: 29 [10350/36045]\tLoss: 723.4645\n",
      "Training Epoch: 29 [10400/36045]\tLoss: 713.9232\n",
      "Training Epoch: 29 [10450/36045]\tLoss: 668.7316\n",
      "Training Epoch: 29 [10500/36045]\tLoss: 559.7723\n",
      "Training Epoch: 29 [10550/36045]\tLoss: 554.8840\n",
      "Training Epoch: 29 [10600/36045]\tLoss: 577.7554\n",
      "Training Epoch: 29 [10650/36045]\tLoss: 583.9539\n",
      "Training Epoch: 29 [10700/36045]\tLoss: 668.7413\n",
      "Training Epoch: 29 [10750/36045]\tLoss: 730.0596\n",
      "Training Epoch: 29 [10800/36045]\tLoss: 673.8204\n",
      "Training Epoch: 29 [10850/36045]\tLoss: 713.7169\n",
      "Training Epoch: 29 [10900/36045]\tLoss: 742.7222\n",
      "Training Epoch: 29 [10950/36045]\tLoss: 548.8144\n",
      "Training Epoch: 29 [11000/36045]\tLoss: 542.4568\n",
      "Training Epoch: 29 [11050/36045]\tLoss: 580.9525\n",
      "Training Epoch: 29 [11100/36045]\tLoss: 592.2122\n",
      "Training Epoch: 29 [11150/36045]\tLoss: 641.9828\n",
      "Training Epoch: 29 [11200/36045]\tLoss: 670.0602\n",
      "Training Epoch: 29 [11250/36045]\tLoss: 682.1145\n",
      "Training Epoch: 29 [11300/36045]\tLoss: 662.1346\n",
      "Training Epoch: 29 [11350/36045]\tLoss: 659.3381\n",
      "Training Epoch: 29 [11400/36045]\tLoss: 620.9315\n",
      "Training Epoch: 29 [11450/36045]\tLoss: 588.4346\n",
      "Training Epoch: 29 [11500/36045]\tLoss: 585.9028\n",
      "Training Epoch: 29 [11550/36045]\tLoss: 597.4673\n",
      "Training Epoch: 29 [11600/36045]\tLoss: 659.2876\n",
      "Training Epoch: 29 [11650/36045]\tLoss: 711.6223\n",
      "Training Epoch: 29 [11700/36045]\tLoss: 710.5655\n",
      "Training Epoch: 29 [11750/36045]\tLoss: 729.6658\n",
      "Training Epoch: 29 [11800/36045]\tLoss: 772.6027\n",
      "Training Epoch: 29 [11850/36045]\tLoss: 829.9015\n",
      "Training Epoch: 29 [11900/36045]\tLoss: 1045.8490\n",
      "Training Epoch: 29 [11950/36045]\tLoss: 1047.9686\n",
      "Training Epoch: 29 [12000/36045]\tLoss: 1061.2433\n",
      "Training Epoch: 29 [12050/36045]\tLoss: 1019.6540\n",
      "Training Epoch: 29 [12100/36045]\tLoss: 663.0796\n",
      "Training Epoch: 29 [12150/36045]\tLoss: 506.4427\n",
      "Training Epoch: 29 [12200/36045]\tLoss: 501.0098\n",
      "Training Epoch: 29 [12250/36045]\tLoss: 510.0862\n",
      "Training Epoch: 29 [12300/36045]\tLoss: 651.5650\n",
      "Training Epoch: 29 [12350/36045]\tLoss: 708.8880\n",
      "Training Epoch: 29 [12400/36045]\tLoss: 716.6801\n",
      "Training Epoch: 29 [12450/36045]\tLoss: 704.8378\n",
      "Training Epoch: 29 [12500/36045]\tLoss: 733.2808\n",
      "Training Epoch: 29 [12550/36045]\tLoss: 702.2313\n",
      "Training Epoch: 29 [12600/36045]\tLoss: 646.0189\n",
      "Training Epoch: 29 [12650/36045]\tLoss: 644.8334\n",
      "Training Epoch: 29 [12700/36045]\tLoss: 666.3162\n",
      "Training Epoch: 29 [12750/36045]\tLoss: 665.5241\n",
      "Training Epoch: 29 [12800/36045]\tLoss: 649.0519\n",
      "Training Epoch: 29 [12850/36045]\tLoss: 678.5229\n",
      "Training Epoch: 29 [12900/36045]\tLoss: 650.9387\n",
      "Training Epoch: 29 [12950/36045]\tLoss: 638.0375\n",
      "Training Epoch: 29 [13000/36045]\tLoss: 670.5275\n",
      "Training Epoch: 29 [13050/36045]\tLoss: 608.4940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [13100/36045]\tLoss: 626.9147\n",
      "Training Epoch: 29 [13150/36045]\tLoss: 618.6295\n",
      "Training Epoch: 29 [13200/36045]\tLoss: 599.4105\n",
      "Training Epoch: 29 [13250/36045]\tLoss: 623.7635\n",
      "Training Epoch: 29 [13300/36045]\tLoss: 662.8693\n",
      "Training Epoch: 29 [13350/36045]\tLoss: 642.6412\n",
      "Training Epoch: 29 [13400/36045]\tLoss: 646.2420\n",
      "Training Epoch: 29 [13450/36045]\tLoss: 642.6811\n",
      "Training Epoch: 29 [13500/36045]\tLoss: 663.3780\n",
      "Training Epoch: 29 [13550/36045]\tLoss: 799.7372\n",
      "Training Epoch: 29 [13600/36045]\tLoss: 832.6092\n",
      "Training Epoch: 29 [13650/36045]\tLoss: 913.1605\n",
      "Training Epoch: 29 [13700/36045]\tLoss: 807.2811\n",
      "Training Epoch: 29 [13750/36045]\tLoss: 649.2105\n",
      "Training Epoch: 29 [13800/36045]\tLoss: 621.5536\n",
      "Training Epoch: 29 [13850/36045]\tLoss: 604.5129\n",
      "Training Epoch: 29 [13900/36045]\tLoss: 611.8812\n",
      "Training Epoch: 29 [13950/36045]\tLoss: 657.4316\n",
      "Training Epoch: 29 [14000/36045]\tLoss: 691.2758\n",
      "Training Epoch: 29 [14050/36045]\tLoss: 665.2156\n",
      "Training Epoch: 29 [14100/36045]\tLoss: 660.7407\n",
      "Training Epoch: 29 [14150/36045]\tLoss: 648.6285\n",
      "Training Epoch: 29 [14200/36045]\tLoss: 690.6888\n",
      "Training Epoch: 29 [14250/36045]\tLoss: 757.7924\n",
      "Training Epoch: 29 [14300/36045]\tLoss: 761.1000\n",
      "Training Epoch: 29 [14350/36045]\tLoss: 728.4113\n",
      "Training Epoch: 29 [14400/36045]\tLoss: 713.9928\n",
      "Training Epoch: 29 [14450/36045]\tLoss: 750.8434\n",
      "Training Epoch: 29 [14500/36045]\tLoss: 681.5627\n",
      "Training Epoch: 29 [14550/36045]\tLoss: 711.6191\n",
      "Training Epoch: 29 [14600/36045]\tLoss: 697.2339\n",
      "Training Epoch: 29 [14650/36045]\tLoss: 697.4173\n",
      "Training Epoch: 29 [14700/36045]\tLoss: 659.5148\n",
      "Training Epoch: 29 [14750/36045]\tLoss: 566.5549\n",
      "Training Epoch: 29 [14800/36045]\tLoss: 556.6576\n",
      "Training Epoch: 29 [14850/36045]\tLoss: 563.7452\n",
      "Training Epoch: 29 [14900/36045]\tLoss: 557.2618\n",
      "Training Epoch: 29 [14950/36045]\tLoss: 565.0829\n",
      "Training Epoch: 29 [15000/36045]\tLoss: 579.4965\n",
      "Training Epoch: 29 [15050/36045]\tLoss: 577.1011\n",
      "Training Epoch: 29 [15100/36045]\tLoss: 561.1470\n",
      "Training Epoch: 29 [15150/36045]\tLoss: 555.1494\n",
      "Training Epoch: 29 [15200/36045]\tLoss: 513.7704\n",
      "Training Epoch: 29 [15250/36045]\tLoss: 536.9026\n",
      "Training Epoch: 29 [15300/36045]\tLoss: 521.7637\n",
      "Training Epoch: 29 [15350/36045]\tLoss: 533.9911\n",
      "Training Epoch: 29 [15400/36045]\tLoss: 518.0744\n",
      "Training Epoch: 29 [15450/36045]\tLoss: 503.6681\n",
      "Training Epoch: 29 [15500/36045]\tLoss: 518.3517\n",
      "Training Epoch: 29 [15550/36045]\tLoss: 514.0457\n",
      "Training Epoch: 29 [15600/36045]\tLoss: 583.7823\n",
      "Training Epoch: 29 [15650/36045]\tLoss: 601.9736\n",
      "Training Epoch: 29 [15700/36045]\tLoss: 592.9863\n",
      "Training Epoch: 29 [15750/36045]\tLoss: 584.7795\n",
      "Training Epoch: 29 [15800/36045]\tLoss: 553.2665\n",
      "Training Epoch: 29 [15850/36045]\tLoss: 567.5114\n",
      "Training Epoch: 29 [15900/36045]\tLoss: 577.0235\n",
      "Training Epoch: 29 [15950/36045]\tLoss: 596.8251\n",
      "Training Epoch: 29 [16000/36045]\tLoss: 569.9876\n",
      "Training Epoch: 29 [16050/36045]\tLoss: 539.3424\n",
      "Training Epoch: 29 [16100/36045]\tLoss: 499.7024\n",
      "Training Epoch: 29 [16150/36045]\tLoss: 487.4670\n",
      "Training Epoch: 29 [16200/36045]\tLoss: 590.3708\n",
      "Training Epoch: 29 [16250/36045]\tLoss: 619.1734\n",
      "Training Epoch: 29 [16300/36045]\tLoss: 676.0151\n",
      "Training Epoch: 29 [16350/36045]\tLoss: 694.9075\n",
      "Training Epoch: 29 [16400/36045]\tLoss: 666.8721\n",
      "Training Epoch: 29 [16450/36045]\tLoss: 648.5430\n",
      "Training Epoch: 29 [16500/36045]\tLoss: 648.2908\n",
      "Training Epoch: 29 [16550/36045]\tLoss: 612.7668\n",
      "Training Epoch: 29 [16600/36045]\tLoss: 637.0396\n",
      "Training Epoch: 29 [16650/36045]\tLoss: 655.1906\n",
      "Training Epoch: 29 [16700/36045]\tLoss: 632.9854\n",
      "Training Epoch: 29 [16750/36045]\tLoss: 625.2007\n",
      "Training Epoch: 29 [16800/36045]\tLoss: 635.8741\n",
      "Training Epoch: 29 [16850/36045]\tLoss: 605.5469\n",
      "Training Epoch: 29 [16900/36045]\tLoss: 615.7749\n",
      "Training Epoch: 29 [16950/36045]\tLoss: 640.2686\n",
      "Training Epoch: 29 [17000/36045]\tLoss: 623.0286\n",
      "Training Epoch: 29 [17050/36045]\tLoss: 649.8342\n",
      "Training Epoch: 29 [17100/36045]\tLoss: 646.4075\n",
      "Training Epoch: 29 [17150/36045]\tLoss: 562.1348\n",
      "Training Epoch: 29 [17200/36045]\tLoss: 523.0289\n",
      "Training Epoch: 29 [17250/36045]\tLoss: 547.3149\n",
      "Training Epoch: 29 [17300/36045]\tLoss: 578.9194\n",
      "Training Epoch: 29 [17350/36045]\tLoss: 556.6057\n",
      "Training Epoch: 29 [17400/36045]\tLoss: 576.0002\n",
      "Training Epoch: 29 [17450/36045]\tLoss: 595.8450\n",
      "Training Epoch: 29 [17500/36045]\tLoss: 583.8280\n",
      "Training Epoch: 29 [17550/36045]\tLoss: 583.1719\n",
      "Training Epoch: 29 [17600/36045]\tLoss: 576.3440\n",
      "Training Epoch: 29 [17650/36045]\tLoss: 593.1221\n",
      "Training Epoch: 29 [17700/36045]\tLoss: 571.9081\n",
      "Training Epoch: 29 [17750/36045]\tLoss: 588.7933\n",
      "Training Epoch: 29 [17800/36045]\tLoss: 579.4619\n",
      "Training Epoch: 29 [17850/36045]\tLoss: 590.2328\n",
      "Training Epoch: 29 [17900/36045]\tLoss: 618.9094\n",
      "Training Epoch: 29 [17950/36045]\tLoss: 630.5851\n",
      "Training Epoch: 29 [18000/36045]\tLoss: 620.7470\n",
      "Training Epoch: 29 [18050/36045]\tLoss: 686.6865\n",
      "Training Epoch: 29 [18100/36045]\tLoss: 688.8185\n",
      "Training Epoch: 29 [18150/36045]\tLoss: 699.9460\n",
      "Training Epoch: 29 [18200/36045]\tLoss: 681.9942\n",
      "Training Epoch: 29 [18250/36045]\tLoss: 702.9139\n",
      "Training Epoch: 29 [18300/36045]\tLoss: 652.8682\n",
      "Training Epoch: 29 [18350/36045]\tLoss: 723.8099\n",
      "Training Epoch: 29 [18400/36045]\tLoss: 696.9868\n",
      "Training Epoch: 29 [18450/36045]\tLoss: 677.1447\n",
      "Training Epoch: 29 [18500/36045]\tLoss: 676.1785\n",
      "Training Epoch: 29 [18550/36045]\tLoss: 663.1484\n",
      "Training Epoch: 29 [18600/36045]\tLoss: 652.6090\n",
      "Training Epoch: 29 [18650/36045]\tLoss: 699.3433\n",
      "Training Epoch: 29 [18700/36045]\tLoss: 735.9146\n",
      "Training Epoch: 29 [18750/36045]\tLoss: 722.2462\n",
      "Training Epoch: 29 [18800/36045]\tLoss: 746.1725\n",
      "Training Epoch: 29 [18850/36045]\tLoss: 690.4824\n",
      "Training Epoch: 29 [18900/36045]\tLoss: 738.6067\n",
      "Training Epoch: 29 [18950/36045]\tLoss: 680.0684\n",
      "Training Epoch: 29 [19000/36045]\tLoss: 568.3029\n",
      "Training Epoch: 29 [19050/36045]\tLoss: 551.1367\n",
      "Training Epoch: 29 [19100/36045]\tLoss: 559.9963\n",
      "Training Epoch: 29 [19150/36045]\tLoss: 549.3634\n",
      "Training Epoch: 29 [19200/36045]\tLoss: 578.2324\n",
      "Training Epoch: 29 [19250/36045]\tLoss: 592.8630\n",
      "Training Epoch: 29 [19300/36045]\tLoss: 603.2906\n",
      "Training Epoch: 29 [19350/36045]\tLoss: 586.8145\n",
      "Training Epoch: 29 [19400/36045]\tLoss: 608.6287\n",
      "Training Epoch: 29 [19450/36045]\tLoss: 599.4983\n",
      "Training Epoch: 29 [19500/36045]\tLoss: 601.1926\n",
      "Training Epoch: 29 [19550/36045]\tLoss: 599.6836\n",
      "Training Epoch: 29 [19600/36045]\tLoss: 640.8959\n",
      "Training Epoch: 29 [19650/36045]\tLoss: 847.5867\n",
      "Training Epoch: 29 [19700/36045]\tLoss: 806.4286\n",
      "Training Epoch: 29 [19750/36045]\tLoss: 809.4465\n",
      "Training Epoch: 29 [19800/36045]\tLoss: 808.5623\n",
      "Training Epoch: 29 [19850/36045]\tLoss: 536.9431\n",
      "Training Epoch: 29 [19900/36045]\tLoss: 515.0698\n",
      "Training Epoch: 29 [19950/36045]\tLoss: 518.6457\n",
      "Training Epoch: 29 [20000/36045]\tLoss: 517.4547\n",
      "Training Epoch: 29 [20050/36045]\tLoss: 579.4685\n",
      "Training Epoch: 29 [20100/36045]\tLoss: 585.7346\n",
      "Training Epoch: 29 [20150/36045]\tLoss: 587.9033\n",
      "Training Epoch: 29 [20200/36045]\tLoss: 587.8809\n",
      "Training Epoch: 29 [20250/36045]\tLoss: 625.9839\n",
      "Training Epoch: 29 [20300/36045]\tLoss: 662.0782\n",
      "Training Epoch: 29 [20350/36045]\tLoss: 681.1159\n",
      "Training Epoch: 29 [20400/36045]\tLoss: 697.7972\n",
      "Training Epoch: 29 [20450/36045]\tLoss: 668.4596\n",
      "Training Epoch: 29 [20500/36045]\tLoss: 652.4227\n",
      "Training Epoch: 29 [20550/36045]\tLoss: 574.1083\n",
      "Training Epoch: 29 [20600/36045]\tLoss: 584.9889\n",
      "Training Epoch: 29 [20650/36045]\tLoss: 581.7907\n",
      "Training Epoch: 29 [20700/36045]\tLoss: 569.6314\n",
      "Training Epoch: 29 [20750/36045]\tLoss: 613.1554\n",
      "Training Epoch: 29 [20800/36045]\tLoss: 666.5635\n",
      "Training Epoch: 29 [20850/36045]\tLoss: 653.2791\n",
      "Training Epoch: 29 [20900/36045]\tLoss: 698.5495\n",
      "Training Epoch: 29 [20950/36045]\tLoss: 658.8312\n",
      "Training Epoch: 29 [21000/36045]\tLoss: 620.7382\n",
      "Training Epoch: 29 [21050/36045]\tLoss: 531.6795\n",
      "Training Epoch: 29 [21100/36045]\tLoss: 535.2965\n",
      "Training Epoch: 29 [21150/36045]\tLoss: 572.8946\n",
      "Training Epoch: 29 [21200/36045]\tLoss: 572.1060\n",
      "Training Epoch: 29 [21250/36045]\tLoss: 547.4200\n",
      "Training Epoch: 29 [21300/36045]\tLoss: 639.1791\n",
      "Training Epoch: 29 [21350/36045]\tLoss: 631.5750\n",
      "Training Epoch: 29 [21400/36045]\tLoss: 634.9567\n",
      "Training Epoch: 29 [21450/36045]\tLoss: 641.2339\n",
      "Training Epoch: 29 [21500/36045]\tLoss: 644.0371\n",
      "Training Epoch: 29 [21550/36045]\tLoss: 739.1567\n",
      "Training Epoch: 29 [21600/36045]\tLoss: 738.6633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [21650/36045]\tLoss: 751.4583\n",
      "Training Epoch: 29 [21700/36045]\tLoss: 753.2549\n",
      "Training Epoch: 29 [21750/36045]\tLoss: 723.7931\n",
      "Training Epoch: 29 [21800/36045]\tLoss: 534.5734\n",
      "Training Epoch: 29 [21850/36045]\tLoss: 517.5126\n",
      "Training Epoch: 29 [21900/36045]\tLoss: 527.9229\n",
      "Training Epoch: 29 [21950/36045]\tLoss: 527.5428\n",
      "Training Epoch: 29 [22000/36045]\tLoss: 531.2769\n",
      "Training Epoch: 29 [22050/36045]\tLoss: 554.3648\n",
      "Training Epoch: 29 [22100/36045]\tLoss: 546.8710\n",
      "Training Epoch: 29 [22150/36045]\tLoss: 531.8505\n",
      "Training Epoch: 29 [22200/36045]\tLoss: 548.9635\n",
      "Training Epoch: 29 [22250/36045]\tLoss: 554.0016\n",
      "Training Epoch: 29 [22300/36045]\tLoss: 606.7519\n",
      "Training Epoch: 29 [22350/36045]\tLoss: 632.8867\n",
      "Training Epoch: 29 [22400/36045]\tLoss: 647.8584\n",
      "Training Epoch: 29 [22450/36045]\tLoss: 635.5756\n",
      "Training Epoch: 29 [22500/36045]\tLoss: 617.4146\n",
      "Training Epoch: 29 [22550/36045]\tLoss: 653.7452\n",
      "Training Epoch: 29 [22600/36045]\tLoss: 708.9504\n",
      "Training Epoch: 29 [22650/36045]\tLoss: 744.4296\n",
      "Training Epoch: 29 [22700/36045]\tLoss: 767.3296\n",
      "Training Epoch: 29 [22750/36045]\tLoss: 787.1786\n",
      "Training Epoch: 29 [22800/36045]\tLoss: 818.4459\n",
      "Training Epoch: 29 [22850/36045]\tLoss: 680.8335\n",
      "Training Epoch: 29 [22900/36045]\tLoss: 685.4879\n",
      "Training Epoch: 29 [22950/36045]\tLoss: 664.6479\n",
      "Training Epoch: 29 [23000/36045]\tLoss: 662.4188\n",
      "Training Epoch: 29 [23050/36045]\tLoss: 589.0116\n",
      "Training Epoch: 29 [23100/36045]\tLoss: 605.0683\n",
      "Training Epoch: 29 [23150/36045]\tLoss: 593.2022\n",
      "Training Epoch: 29 [23200/36045]\tLoss: 561.8865\n",
      "Training Epoch: 29 [23250/36045]\tLoss: 564.8561\n",
      "Training Epoch: 29 [23300/36045]\tLoss: 561.4524\n",
      "Training Epoch: 29 [23350/36045]\tLoss: 582.7582\n",
      "Training Epoch: 29 [23400/36045]\tLoss: 631.3953\n",
      "Training Epoch: 29 [23450/36045]\tLoss: 624.2699\n",
      "Training Epoch: 29 [23500/36045]\tLoss: 601.7617\n",
      "Training Epoch: 29 [23550/36045]\tLoss: 645.5017\n",
      "Training Epoch: 29 [23600/36045]\tLoss: 729.0934\n",
      "Training Epoch: 29 [23650/36045]\tLoss: 742.3444\n",
      "Training Epoch: 29 [23700/36045]\tLoss: 750.9960\n",
      "Training Epoch: 29 [23750/36045]\tLoss: 725.9399\n",
      "Training Epoch: 29 [23800/36045]\tLoss: 579.6035\n",
      "Training Epoch: 29 [23850/36045]\tLoss: 606.1742\n",
      "Training Epoch: 29 [23900/36045]\tLoss: 595.7681\n",
      "Training Epoch: 29 [23950/36045]\tLoss: 578.6459\n",
      "Training Epoch: 29 [24000/36045]\tLoss: 555.0458\n",
      "Training Epoch: 29 [24050/36045]\tLoss: 512.9590\n",
      "Training Epoch: 29 [24100/36045]\tLoss: 539.7407\n",
      "Training Epoch: 29 [24150/36045]\tLoss: 533.2353\n",
      "Training Epoch: 29 [24200/36045]\tLoss: 529.3915\n",
      "Training Epoch: 29 [24250/36045]\tLoss: 513.8220\n",
      "Training Epoch: 29 [24300/36045]\tLoss: 554.7427\n",
      "Training Epoch: 29 [24350/36045]\tLoss: 568.3341\n",
      "Training Epoch: 29 [24400/36045]\tLoss: 584.3224\n",
      "Training Epoch: 29 [24450/36045]\tLoss: 557.1964\n",
      "Training Epoch: 29 [24500/36045]\tLoss: 587.2382\n",
      "Training Epoch: 29 [24550/36045]\tLoss: 677.4005\n",
      "Training Epoch: 29 [24600/36045]\tLoss: 669.3540\n",
      "Training Epoch: 29 [24650/36045]\tLoss: 642.0646\n",
      "Training Epoch: 29 [24700/36045]\tLoss: 652.2131\n",
      "Training Epoch: 29 [24750/36045]\tLoss: 602.5776\n",
      "Training Epoch: 29 [24800/36045]\tLoss: 498.4396\n",
      "Training Epoch: 29 [24850/36045]\tLoss: 518.0305\n",
      "Training Epoch: 29 [24900/36045]\tLoss: 515.0202\n",
      "Training Epoch: 29 [24950/36045]\tLoss: 517.5810\n",
      "Training Epoch: 29 [25000/36045]\tLoss: 497.6049\n",
      "Training Epoch: 29 [25050/36045]\tLoss: 475.1962\n",
      "Training Epoch: 29 [25100/36045]\tLoss: 425.9801\n",
      "Training Epoch: 29 [25150/36045]\tLoss: 394.5679\n",
      "Training Epoch: 29 [25200/36045]\tLoss: 389.5903\n",
      "Training Epoch: 29 [25250/36045]\tLoss: 417.4846\n",
      "Training Epoch: 29 [25300/36045]\tLoss: 548.2025\n",
      "Training Epoch: 29 [25350/36045]\tLoss: 545.2127\n",
      "Training Epoch: 29 [25400/36045]\tLoss: 508.1428\n",
      "Training Epoch: 29 [25450/36045]\tLoss: 510.7882\n",
      "Training Epoch: 29 [25500/36045]\tLoss: 555.0145\n",
      "Training Epoch: 29 [25550/36045]\tLoss: 647.5719\n",
      "Training Epoch: 29 [25600/36045]\tLoss: 652.3408\n",
      "Training Epoch: 29 [25650/36045]\tLoss: 629.5284\n",
      "Training Epoch: 29 [25700/36045]\tLoss: 638.6486\n",
      "Training Epoch: 29 [25750/36045]\tLoss: 615.4119\n",
      "Training Epoch: 29 [25800/36045]\tLoss: 387.9569\n",
      "Training Epoch: 29 [25850/36045]\tLoss: 397.8593\n",
      "Training Epoch: 29 [25900/36045]\tLoss: 378.8721\n",
      "Training Epoch: 29 [25950/36045]\tLoss: 387.6954\n",
      "Training Epoch: 29 [26000/36045]\tLoss: 475.0576\n",
      "Training Epoch: 29 [26050/36045]\tLoss: 646.6414\n",
      "Training Epoch: 29 [26100/36045]\tLoss: 674.4136\n",
      "Training Epoch: 29 [26150/36045]\tLoss: 674.7689\n",
      "Training Epoch: 29 [26200/36045]\tLoss: 647.9734\n",
      "Training Epoch: 29 [26250/36045]\tLoss: 678.3091\n",
      "Training Epoch: 29 [26300/36045]\tLoss: 612.3926\n",
      "Training Epoch: 29 [26350/36045]\tLoss: 622.9827\n",
      "Training Epoch: 29 [26400/36045]\tLoss: 600.4460\n",
      "Training Epoch: 29 [26450/36045]\tLoss: 530.1923\n",
      "Training Epoch: 29 [26500/36045]\tLoss: 630.5721\n",
      "Training Epoch: 29 [26550/36045]\tLoss: 631.9255\n",
      "Training Epoch: 29 [26600/36045]\tLoss: 627.8044\n",
      "Training Epoch: 29 [26650/36045]\tLoss: 644.0932\n",
      "Training Epoch: 29 [26700/36045]\tLoss: 623.7690\n",
      "Training Epoch: 29 [26750/36045]\tLoss: 584.0738\n",
      "Training Epoch: 29 [26800/36045]\tLoss: 430.1243\n",
      "Training Epoch: 29 [26850/36045]\tLoss: 356.8359\n",
      "Training Epoch: 29 [26900/36045]\tLoss: 359.9177\n",
      "Training Epoch: 29 [26950/36045]\tLoss: 395.7122\n",
      "Training Epoch: 29 [27000/36045]\tLoss: 643.9107\n",
      "Training Epoch: 29 [27050/36045]\tLoss: 673.1827\n",
      "Training Epoch: 29 [27100/36045]\tLoss: 652.0806\n",
      "Training Epoch: 29 [27150/36045]\tLoss: 692.8325\n",
      "Training Epoch: 29 [27200/36045]\tLoss: 507.3118\n",
      "Training Epoch: 29 [27250/36045]\tLoss: 499.6692\n",
      "Training Epoch: 29 [27300/36045]\tLoss: 486.5010\n",
      "Training Epoch: 29 [27350/36045]\tLoss: 485.3033\n",
      "Training Epoch: 29 [27400/36045]\tLoss: 484.3958\n",
      "Training Epoch: 29 [27450/36045]\tLoss: 611.4696\n",
      "Training Epoch: 29 [27500/36045]\tLoss: 655.4803\n",
      "Training Epoch: 29 [27550/36045]\tLoss: 648.1570\n",
      "Training Epoch: 29 [27600/36045]\tLoss: 659.9250\n",
      "Training Epoch: 29 [27650/36045]\tLoss: 651.6539\n",
      "Training Epoch: 29 [27700/36045]\tLoss: 681.0674\n",
      "Training Epoch: 29 [27750/36045]\tLoss: 693.2812\n",
      "Training Epoch: 29 [27800/36045]\tLoss: 679.7760\n",
      "Training Epoch: 29 [27850/36045]\tLoss: 669.1603\n",
      "Training Epoch: 29 [27900/36045]\tLoss: 604.2562\n",
      "Training Epoch: 29 [27950/36045]\tLoss: 502.4604\n",
      "Training Epoch: 29 [28000/36045]\tLoss: 478.6479\n",
      "Training Epoch: 29 [28050/36045]\tLoss: 489.1442\n",
      "Training Epoch: 29 [28100/36045]\tLoss: 480.8134\n",
      "Training Epoch: 29 [28150/36045]\tLoss: 505.2563\n",
      "Training Epoch: 29 [28200/36045]\tLoss: 511.9312\n",
      "Training Epoch: 29 [28250/36045]\tLoss: 505.3520\n",
      "Training Epoch: 29 [28300/36045]\tLoss: 479.4623\n",
      "Training Epoch: 29 [28350/36045]\tLoss: 475.6713\n",
      "Training Epoch: 29 [28400/36045]\tLoss: 807.7003\n",
      "Training Epoch: 29 [28450/36045]\tLoss: 739.0154\n",
      "Training Epoch: 29 [28500/36045]\tLoss: 638.7719\n",
      "Training Epoch: 29 [28550/36045]\tLoss: 586.4366\n",
      "Training Epoch: 29 [28600/36045]\tLoss: 619.0811\n",
      "Training Epoch: 29 [28650/36045]\tLoss: 687.9133\n",
      "Training Epoch: 29 [28700/36045]\tLoss: 682.3251\n",
      "Training Epoch: 29 [28750/36045]\tLoss: 668.8960\n",
      "Training Epoch: 29 [28800/36045]\tLoss: 677.5018\n",
      "Training Epoch: 29 [28850/36045]\tLoss: 587.0715\n",
      "Training Epoch: 29 [28900/36045]\tLoss: 475.4122\n",
      "Training Epoch: 29 [28950/36045]\tLoss: 474.1369\n",
      "Training Epoch: 29 [29000/36045]\tLoss: 471.8783\n",
      "Training Epoch: 29 [29050/36045]\tLoss: 479.0758\n",
      "Training Epoch: 29 [29100/36045]\tLoss: 498.3829\n",
      "Training Epoch: 29 [29150/36045]\tLoss: 486.5623\n",
      "Training Epoch: 29 [29200/36045]\tLoss: 471.9440\n",
      "Training Epoch: 29 [29250/36045]\tLoss: 461.1139\n",
      "Training Epoch: 29 [29300/36045]\tLoss: 524.1993\n",
      "Training Epoch: 29 [29350/36045]\tLoss: 619.3276\n",
      "Training Epoch: 29 [29400/36045]\tLoss: 637.2231\n",
      "Training Epoch: 29 [29450/36045]\tLoss: 656.0049\n",
      "Training Epoch: 29 [29500/36045]\tLoss: 670.4142\n",
      "Training Epoch: 29 [29550/36045]\tLoss: 637.7903\n",
      "Training Epoch: 29 [29600/36045]\tLoss: 538.5914\n",
      "Training Epoch: 29 [29650/36045]\tLoss: 521.3483\n",
      "Training Epoch: 29 [29700/36045]\tLoss: 465.5629\n",
      "Training Epoch: 29 [29750/36045]\tLoss: 464.8949\n",
      "Training Epoch: 29 [29800/36045]\tLoss: 511.4064\n",
      "Training Epoch: 29 [29850/36045]\tLoss: 586.5298\n",
      "Training Epoch: 29 [29900/36045]\tLoss: 583.2943\n",
      "Training Epoch: 29 [29950/36045]\tLoss: 605.5377\n",
      "Training Epoch: 29 [30000/36045]\tLoss: 580.7752\n",
      "Training Epoch: 29 [30050/36045]\tLoss: 586.8354\n",
      "Training Epoch: 29 [30100/36045]\tLoss: 715.8964\n",
      "Training Epoch: 29 [30150/36045]\tLoss: 699.5719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 29 [30200/36045]\tLoss: 660.0406\n",
      "Training Epoch: 29 [30250/36045]\tLoss: 709.0463\n",
      "Training Epoch: 29 [30300/36045]\tLoss: 694.2863\n",
      "Training Epoch: 29 [30350/36045]\tLoss: 538.8405\n",
      "Training Epoch: 29 [30400/36045]\tLoss: 523.4069\n",
      "Training Epoch: 29 [30450/36045]\tLoss: 524.5487\n",
      "Training Epoch: 29 [30500/36045]\tLoss: 490.2316\n",
      "Training Epoch: 29 [30550/36045]\tLoss: 454.2896\n",
      "Training Epoch: 29 [30600/36045]\tLoss: 444.1460\n",
      "Training Epoch: 29 [30650/36045]\tLoss: 434.1530\n",
      "Training Epoch: 29 [30700/36045]\tLoss: 451.9731\n",
      "Training Epoch: 29 [30750/36045]\tLoss: 438.6302\n",
      "Training Epoch: 29 [30800/36045]\tLoss: 465.7726\n",
      "Training Epoch: 29 [30850/36045]\tLoss: 457.5108\n",
      "Training Epoch: 29 [30900/36045]\tLoss: 470.3643\n",
      "Training Epoch: 29 [30950/36045]\tLoss: 494.2077\n",
      "Training Epoch: 29 [31000/36045]\tLoss: 485.7303\n",
      "Training Epoch: 29 [31050/36045]\tLoss: 405.8606\n",
      "Training Epoch: 29 [31100/36045]\tLoss: 396.4605\n",
      "Training Epoch: 29 [31150/36045]\tLoss: 403.5370\n",
      "Training Epoch: 29 [31200/36045]\tLoss: 502.7359\n",
      "Training Epoch: 29 [31250/36045]\tLoss: 652.3176\n",
      "Training Epoch: 29 [31300/36045]\tLoss: 622.4955\n",
      "Training Epoch: 29 [31350/36045]\tLoss: 638.3561\n",
      "Training Epoch: 29 [31400/36045]\tLoss: 617.6885\n",
      "Training Epoch: 29 [31450/36045]\tLoss: 633.4437\n",
      "Training Epoch: 29 [31500/36045]\tLoss: 645.6519\n",
      "Training Epoch: 29 [31550/36045]\tLoss: 653.6763\n",
      "Training Epoch: 29 [31600/36045]\tLoss: 614.4180\n",
      "Training Epoch: 29 [31650/36045]\tLoss: 656.7038\n",
      "Training Epoch: 29 [31700/36045]\tLoss: 476.3984\n",
      "Training Epoch: 29 [31750/36045]\tLoss: 394.5108\n",
      "Training Epoch: 29 [31800/36045]\tLoss: 376.0599\n",
      "Training Epoch: 29 [31850/36045]\tLoss: 385.1133\n",
      "Training Epoch: 29 [31900/36045]\tLoss: 603.4976\n",
      "Training Epoch: 29 [31950/36045]\tLoss: 778.5444\n",
      "Training Epoch: 29 [32000/36045]\tLoss: 889.4324\n",
      "Training Epoch: 29 [32050/36045]\tLoss: 843.8040\n",
      "Training Epoch: 29 [32100/36045]\tLoss: 833.6806\n",
      "Training Epoch: 29 [32150/36045]\tLoss: 648.0706\n",
      "Training Epoch: 29 [32200/36045]\tLoss: 651.8203\n",
      "Training Epoch: 29 [32250/36045]\tLoss: 662.5518\n",
      "Training Epoch: 29 [32300/36045]\tLoss: 644.2385\n",
      "Training Epoch: 29 [32350/36045]\tLoss: 639.4266\n",
      "Training Epoch: 29 [32400/36045]\tLoss: 600.0902\n",
      "Training Epoch: 29 [32450/36045]\tLoss: 494.4139\n",
      "Training Epoch: 29 [32500/36045]\tLoss: 475.3378\n",
      "Training Epoch: 29 [32550/36045]\tLoss: 477.8196\n",
      "Training Epoch: 29 [32600/36045]\tLoss: 474.3926\n",
      "Training Epoch: 29 [32650/36045]\tLoss: 608.2798\n",
      "Training Epoch: 29 [32700/36045]\tLoss: 663.4629\n",
      "Training Epoch: 29 [32750/36045]\tLoss: 632.3298\n",
      "Training Epoch: 29 [32800/36045]\tLoss: 648.6534\n",
      "Training Epoch: 29 [32850/36045]\tLoss: 598.8956\n",
      "Training Epoch: 29 [32900/36045]\tLoss: 481.2102\n",
      "Training Epoch: 29 [32950/36045]\tLoss: 503.5937\n",
      "Training Epoch: 29 [33000/36045]\tLoss: 503.1323\n",
      "Training Epoch: 29 [33050/36045]\tLoss: 477.7753\n",
      "Training Epoch: 29 [33100/36045]\tLoss: 543.0794\n",
      "Training Epoch: 29 [33150/36045]\tLoss: 736.8652\n",
      "Training Epoch: 29 [33200/36045]\tLoss: 717.7697\n",
      "Training Epoch: 29 [33250/36045]\tLoss: 739.5554\n",
      "Training Epoch: 29 [33300/36045]\tLoss: 787.4092\n",
      "Training Epoch: 29 [33350/36045]\tLoss: 604.1414\n",
      "Training Epoch: 29 [33400/36045]\tLoss: 443.5919\n",
      "Training Epoch: 29 [33450/36045]\tLoss: 438.7648\n",
      "Training Epoch: 29 [33500/36045]\tLoss: 451.6276\n",
      "Training Epoch: 29 [33550/36045]\tLoss: 468.2692\n",
      "Training Epoch: 29 [33600/36045]\tLoss: 469.9695\n",
      "Training Epoch: 29 [33650/36045]\tLoss: 626.5827\n",
      "Training Epoch: 29 [33700/36045]\tLoss: 606.3150\n",
      "Training Epoch: 29 [33750/36045]\tLoss: 627.8214\n",
      "Training Epoch: 29 [33800/36045]\tLoss: 623.4730\n",
      "Training Epoch: 29 [33850/36045]\tLoss: 625.9868\n",
      "Training Epoch: 29 [33900/36045]\tLoss: 638.1725\n",
      "Training Epoch: 29 [33950/36045]\tLoss: 649.1976\n",
      "Training Epoch: 29 [34000/36045]\tLoss: 635.9724\n",
      "Training Epoch: 29 [34050/36045]\tLoss: 640.2545\n",
      "Training Epoch: 29 [34100/36045]\tLoss: 616.3017\n",
      "Training Epoch: 29 [34150/36045]\tLoss: 572.6251\n",
      "Training Epoch: 29 [34200/36045]\tLoss: 542.1614\n",
      "Training Epoch: 29 [34250/36045]\tLoss: 556.5992\n",
      "Training Epoch: 29 [34300/36045]\tLoss: 476.2680\n",
      "Training Epoch: 29 [34350/36045]\tLoss: 501.5225\n",
      "Training Epoch: 29 [34400/36045]\tLoss: 492.9716\n",
      "Training Epoch: 29 [34450/36045]\tLoss: 463.3400\n",
      "Training Epoch: 29 [34500/36045]\tLoss: 494.5839\n",
      "Training Epoch: 29 [34550/36045]\tLoss: 485.3722\n",
      "Training Epoch: 29 [34600/36045]\tLoss: 489.1250\n",
      "Training Epoch: 29 [34650/36045]\tLoss: 596.9291\n",
      "Training Epoch: 29 [34700/36045]\tLoss: 632.1916\n",
      "Training Epoch: 29 [34750/36045]\tLoss: 560.9658\n",
      "Training Epoch: 29 [34800/36045]\tLoss: 641.9465\n",
      "Training Epoch: 29 [34850/36045]\tLoss: 650.0370\n",
      "Training Epoch: 29 [34900/36045]\tLoss: 712.5436\n",
      "Training Epoch: 29 [34950/36045]\tLoss: 698.9813\n",
      "Training Epoch: 29 [35000/36045]\tLoss: 699.9113\n",
      "Training Epoch: 29 [35050/36045]\tLoss: 685.9602\n",
      "Training Epoch: 29 [35100/36045]\tLoss: 581.0542\n",
      "Training Epoch: 29 [35150/36045]\tLoss: 574.0997\n",
      "Training Epoch: 29 [35200/36045]\tLoss: 486.1739\n",
      "Training Epoch: 29 [35250/36045]\tLoss: 533.8074\n",
      "Training Epoch: 29 [35300/36045]\tLoss: 548.2262\n",
      "Training Epoch: 29 [35350/36045]\tLoss: 620.3859\n",
      "Training Epoch: 29 [35400/36045]\tLoss: 654.8372\n",
      "Training Epoch: 29 [35450/36045]\tLoss: 624.9721\n",
      "Training Epoch: 29 [35500/36045]\tLoss: 606.4204\n",
      "Training Epoch: 29 [35550/36045]\tLoss: 591.6185\n",
      "Training Epoch: 29 [35600/36045]\tLoss: 639.7947\n",
      "Training Epoch: 29 [35650/36045]\tLoss: 713.4949\n",
      "Training Epoch: 29 [35700/36045]\tLoss: 640.8910\n",
      "Training Epoch: 29 [35750/36045]\tLoss: 699.0862\n",
      "Training Epoch: 29 [35800/36045]\tLoss: 705.5613\n",
      "Training Epoch: 29 [35850/36045]\tLoss: 679.5548\n",
      "Training Epoch: 29 [35900/36045]\tLoss: 705.0631\n",
      "Training Epoch: 29 [35950/36045]\tLoss: 702.6453\n",
      "Training Epoch: 29 [36000/36045]\tLoss: 694.8653\n",
      "Training Epoch: 29 [36045/36045]\tLoss: 678.4222\n",
      "Training Epoch: 29 [4004/4004]\tLoss: 631.7366\n",
      "Training Epoch: 30 [50/36045]\tLoss: 631.2070\n",
      "Training Epoch: 30 [100/36045]\tLoss: 605.2739\n",
      "Training Epoch: 30 [150/36045]\tLoss: 603.4640\n",
      "Training Epoch: 30 [200/36045]\tLoss: 590.2239\n",
      "Training Epoch: 30 [250/36045]\tLoss: 704.5765\n",
      "Training Epoch: 30 [300/36045]\tLoss: 768.6087\n",
      "Training Epoch: 30 [350/36045]\tLoss: 734.4635\n",
      "Training Epoch: 30 [400/36045]\tLoss: 729.9990\n",
      "Training Epoch: 30 [450/36045]\tLoss: 710.1027\n",
      "Training Epoch: 30 [500/36045]\tLoss: 660.2214\n",
      "Training Epoch: 30 [550/36045]\tLoss: 663.9545\n",
      "Training Epoch: 30 [600/36045]\tLoss: 645.8316\n",
      "Training Epoch: 30 [650/36045]\tLoss: 669.0809\n",
      "Training Epoch: 30 [700/36045]\tLoss: 655.9080\n",
      "Training Epoch: 30 [750/36045]\tLoss: 636.2686\n",
      "Training Epoch: 30 [800/36045]\tLoss: 649.8437\n",
      "Training Epoch: 30 [850/36045]\tLoss: 630.9534\n",
      "Training Epoch: 30 [900/36045]\tLoss: 601.8723\n",
      "Training Epoch: 30 [950/36045]\tLoss: 570.2364\n",
      "Training Epoch: 30 [1000/36045]\tLoss: 550.8602\n",
      "Training Epoch: 30 [1050/36045]\tLoss: 553.0148\n",
      "Training Epoch: 30 [1100/36045]\tLoss: 538.1700\n",
      "Training Epoch: 30 [1150/36045]\tLoss: 546.9174\n",
      "Training Epoch: 30 [1200/36045]\tLoss: 577.6063\n",
      "Training Epoch: 30 [1250/36045]\tLoss: 661.0815\n",
      "Training Epoch: 30 [1300/36045]\tLoss: 667.7834\n",
      "Training Epoch: 30 [1350/36045]\tLoss: 670.0583\n",
      "Training Epoch: 30 [1400/36045]\tLoss: 696.2913\n",
      "Training Epoch: 30 [1450/36045]\tLoss: 673.4085\n",
      "Training Epoch: 30 [1500/36045]\tLoss: 617.5481\n",
      "Training Epoch: 30 [1550/36045]\tLoss: 632.9908\n",
      "Training Epoch: 30 [1600/36045]\tLoss: 643.9188\n",
      "Training Epoch: 30 [1650/36045]\tLoss: 630.9640\n",
      "Training Epoch: 30 [1700/36045]\tLoss: 643.2555\n",
      "Training Epoch: 30 [1750/36045]\tLoss: 684.7786\n",
      "Training Epoch: 30 [1800/36045]\tLoss: 665.8622\n",
      "Training Epoch: 30 [1850/36045]\tLoss: 682.9037\n",
      "Training Epoch: 30 [1900/36045]\tLoss: 639.3126\n",
      "Training Epoch: 30 [1950/36045]\tLoss: 650.3823\n",
      "Training Epoch: 30 [2000/36045]\tLoss: 588.0878\n",
      "Training Epoch: 30 [2050/36045]\tLoss: 590.6025\n",
      "Training Epoch: 30 [2100/36045]\tLoss: 621.9932\n",
      "Training Epoch: 30 [2150/36045]\tLoss: 601.3713\n",
      "Training Epoch: 30 [2200/36045]\tLoss: 559.1402\n",
      "Training Epoch: 30 [2250/36045]\tLoss: 528.0432\n",
      "Training Epoch: 30 [2300/36045]\tLoss: 553.8984\n",
      "Training Epoch: 30 [2350/36045]\tLoss: 529.2480\n",
      "Training Epoch: 30 [2400/36045]\tLoss: 538.0699\n",
      "Training Epoch: 30 [2450/36045]\tLoss: 685.9006\n",
      "Training Epoch: 30 [2500/36045]\tLoss: 720.7819\n",
      "Training Epoch: 30 [2550/36045]\tLoss: 718.0579\n",
      "Training Epoch: 30 [2600/36045]\tLoss: 726.8810\n",
      "Training Epoch: 30 [2650/36045]\tLoss: 852.2061\n",
      "Training Epoch: 30 [2700/36045]\tLoss: 939.4285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [2750/36045]\tLoss: 1011.2587\n",
      "Training Epoch: 30 [2800/36045]\tLoss: 1021.1313\n",
      "Training Epoch: 30 [2850/36045]\tLoss: 789.7689\n",
      "Training Epoch: 30 [2900/36045]\tLoss: 754.0795\n",
      "Training Epoch: 30 [2950/36045]\tLoss: 728.0483\n",
      "Training Epoch: 30 [3000/36045]\tLoss: 722.4340\n",
      "Training Epoch: 30 [3050/36045]\tLoss: 753.2974\n",
      "Training Epoch: 30 [3100/36045]\tLoss: 689.4035\n",
      "Training Epoch: 30 [3150/36045]\tLoss: 532.0079\n",
      "Training Epoch: 30 [3200/36045]\tLoss: 551.7414\n",
      "Training Epoch: 30 [3250/36045]\tLoss: 519.5156\n",
      "Training Epoch: 30 [3300/36045]\tLoss: 491.6824\n",
      "Training Epoch: 30 [3350/36045]\tLoss: 518.6053\n",
      "Training Epoch: 30 [3400/36045]\tLoss: 544.3320\n",
      "Training Epoch: 30 [3450/36045]\tLoss: 584.6158\n",
      "Training Epoch: 30 [3500/36045]\tLoss: 571.9409\n",
      "Training Epoch: 30 [3550/36045]\tLoss: 548.2346\n",
      "Training Epoch: 30 [3600/36045]\tLoss: 587.6064\n",
      "Training Epoch: 30 [3650/36045]\tLoss: 679.0311\n",
      "Training Epoch: 30 [3700/36045]\tLoss: 685.7543\n",
      "Training Epoch: 30 [3750/36045]\tLoss: 654.5737\n",
      "Training Epoch: 30 [3800/36045]\tLoss: 649.6259\n",
      "Training Epoch: 30 [3850/36045]\tLoss: 649.1031\n",
      "Training Epoch: 30 [3900/36045]\tLoss: 654.2615\n",
      "Training Epoch: 30 [3950/36045]\tLoss: 631.1191\n",
      "Training Epoch: 30 [4000/36045]\tLoss: 636.9461\n",
      "Training Epoch: 30 [4050/36045]\tLoss: 585.2355\n",
      "Training Epoch: 30 [4100/36045]\tLoss: 570.5588\n",
      "Training Epoch: 30 [4150/36045]\tLoss: 586.6396\n",
      "Training Epoch: 30 [4200/36045]\tLoss: 581.2991\n",
      "Training Epoch: 30 [4250/36045]\tLoss: 583.5291\n",
      "Training Epoch: 30 [4300/36045]\tLoss: 601.5844\n",
      "Training Epoch: 30 [4350/36045]\tLoss: 584.2501\n",
      "Training Epoch: 30 [4400/36045]\tLoss: 559.2960\n",
      "Training Epoch: 30 [4450/36045]\tLoss: 611.9269\n",
      "Training Epoch: 30 [4500/36045]\tLoss: 655.4945\n",
      "Training Epoch: 30 [4550/36045]\tLoss: 660.2120\n",
      "Training Epoch: 30 [4600/36045]\tLoss: 683.0936\n",
      "Training Epoch: 30 [4650/36045]\tLoss: 672.4329\n",
      "Training Epoch: 30 [4700/36045]\tLoss: 620.9652\n",
      "Training Epoch: 30 [4750/36045]\tLoss: 603.3072\n",
      "Training Epoch: 30 [4800/36045]\tLoss: 629.1237\n",
      "Training Epoch: 30 [4850/36045]\tLoss: 615.6497\n",
      "Training Epoch: 30 [4900/36045]\tLoss: 598.9308\n",
      "Training Epoch: 30 [4950/36045]\tLoss: 615.5084\n",
      "Training Epoch: 30 [5000/36045]\tLoss: 645.9432\n",
      "Training Epoch: 30 [5050/36045]\tLoss: 625.6360\n",
      "Training Epoch: 30 [5100/36045]\tLoss: 636.0821\n",
      "Training Epoch: 30 [5150/36045]\tLoss: 620.4495\n",
      "Training Epoch: 30 [5200/36045]\tLoss: 618.4546\n",
      "Training Epoch: 30 [5250/36045]\tLoss: 611.9331\n",
      "Training Epoch: 30 [5300/36045]\tLoss: 612.5262\n",
      "Training Epoch: 30 [5350/36045]\tLoss: 635.3072\n",
      "Training Epoch: 30 [5400/36045]\tLoss: 611.7324\n",
      "Training Epoch: 30 [5450/36045]\tLoss: 580.1317\n",
      "Training Epoch: 30 [5500/36045]\tLoss: 609.1488\n",
      "Training Epoch: 30 [5550/36045]\tLoss: 597.2432\n",
      "Training Epoch: 30 [5600/36045]\tLoss: 679.7180\n",
      "Training Epoch: 30 [5650/36045]\tLoss: 643.6106\n",
      "Training Epoch: 30 [5700/36045]\tLoss: 604.0358\n",
      "Training Epoch: 30 [5750/36045]\tLoss: 588.5622\n",
      "Training Epoch: 30 [5800/36045]\tLoss: 620.9865\n",
      "Training Epoch: 30 [5850/36045]\tLoss: 608.0215\n",
      "Training Epoch: 30 [5900/36045]\tLoss: 699.4147\n",
      "Training Epoch: 30 [5950/36045]\tLoss: 716.7717\n",
      "Training Epoch: 30 [6000/36045]\tLoss: 701.8727\n",
      "Training Epoch: 30 [6050/36045]\tLoss: 678.7397\n",
      "Training Epoch: 30 [6100/36045]\tLoss: 683.2929\n",
      "Training Epoch: 30 [6150/36045]\tLoss: 669.9608\n",
      "Training Epoch: 30 [6200/36045]\tLoss: 672.2272\n",
      "Training Epoch: 30 [6250/36045]\tLoss: 693.6209\n",
      "Training Epoch: 30 [6300/36045]\tLoss: 705.3818\n",
      "Training Epoch: 30 [6350/36045]\tLoss: 752.7254\n",
      "Training Epoch: 30 [6400/36045]\tLoss: 624.7387\n",
      "Training Epoch: 30 [6450/36045]\tLoss: 576.8924\n",
      "Training Epoch: 30 [6500/36045]\tLoss: 587.5048\n",
      "Training Epoch: 30 [6550/36045]\tLoss: 604.6823\n",
      "Training Epoch: 30 [6600/36045]\tLoss: 603.7665\n",
      "Training Epoch: 30 [6650/36045]\tLoss: 681.2162\n",
      "Training Epoch: 30 [6700/36045]\tLoss: 712.7233\n",
      "Training Epoch: 30 [6750/36045]\tLoss: 688.4466\n",
      "Training Epoch: 30 [6800/36045]\tLoss: 691.4417\n",
      "Training Epoch: 30 [6850/36045]\tLoss: 679.3414\n",
      "Training Epoch: 30 [6900/36045]\tLoss: 605.1730\n",
      "Training Epoch: 30 [6950/36045]\tLoss: 570.4948\n",
      "Training Epoch: 30 [7000/36045]\tLoss: 606.9869\n",
      "Training Epoch: 30 [7050/36045]\tLoss: 620.4101\n",
      "Training Epoch: 30 [7100/36045]\tLoss: 619.7895\n",
      "Training Epoch: 30 [7150/36045]\tLoss: 630.5419\n",
      "Training Epoch: 30 [7200/36045]\tLoss: 634.0193\n",
      "Training Epoch: 30 [7250/36045]\tLoss: 631.7305\n",
      "Training Epoch: 30 [7300/36045]\tLoss: 618.5696\n",
      "Training Epoch: 30 [7350/36045]\tLoss: 614.9802\n",
      "Training Epoch: 30 [7400/36045]\tLoss: 558.1580\n",
      "Training Epoch: 30 [7450/36045]\tLoss: 561.4910\n",
      "Training Epoch: 30 [7500/36045]\tLoss: 556.4404\n",
      "Training Epoch: 30 [7550/36045]\tLoss: 533.1949\n",
      "Training Epoch: 30 [7600/36045]\tLoss: 591.9503\n",
      "Training Epoch: 30 [7650/36045]\tLoss: 634.2623\n",
      "Training Epoch: 30 [7700/36045]\tLoss: 603.9422\n",
      "Training Epoch: 30 [7750/36045]\tLoss: 618.6477\n",
      "Training Epoch: 30 [7800/36045]\tLoss: 607.1401\n",
      "Training Epoch: 30 [7850/36045]\tLoss: 587.5024\n",
      "Training Epoch: 30 [7900/36045]\tLoss: 619.7408\n",
      "Training Epoch: 30 [7950/36045]\tLoss: 617.0571\n",
      "Training Epoch: 30 [8000/36045]\tLoss: 635.1807\n",
      "Training Epoch: 30 [8050/36045]\tLoss: 599.4460\n",
      "Training Epoch: 30 [8100/36045]\tLoss: 625.1069\n",
      "Training Epoch: 30 [8150/36045]\tLoss: 707.6512\n",
      "Training Epoch: 30 [8200/36045]\tLoss: 694.2622\n",
      "Training Epoch: 30 [8250/36045]\tLoss: 661.6688\n",
      "Training Epoch: 30 [8300/36045]\tLoss: 721.3693\n",
      "Training Epoch: 30 [8350/36045]\tLoss: 662.5575\n",
      "Training Epoch: 30 [8400/36045]\tLoss: 593.7529\n",
      "Training Epoch: 30 [8450/36045]\tLoss: 556.2584\n",
      "Training Epoch: 30 [8500/36045]\tLoss: 590.9578\n",
      "Training Epoch: 30 [8550/36045]\tLoss: 583.1061\n",
      "Training Epoch: 30 [8600/36045]\tLoss: 576.9073\n",
      "Training Epoch: 30 [8650/36045]\tLoss: 615.1993\n",
      "Training Epoch: 30 [8700/36045]\tLoss: 650.4905\n",
      "Training Epoch: 30 [8750/36045]\tLoss: 638.7225\n",
      "Training Epoch: 30 [8800/36045]\tLoss: 644.4956\n",
      "Training Epoch: 30 [8850/36045]\tLoss: 637.7516\n",
      "Training Epoch: 30 [8900/36045]\tLoss: 575.6404\n",
      "Training Epoch: 30 [8950/36045]\tLoss: 588.0010\n",
      "Training Epoch: 30 [9000/36045]\tLoss: 603.6263\n",
      "Training Epoch: 30 [9050/36045]\tLoss: 604.7467\n",
      "Training Epoch: 30 [9100/36045]\tLoss: 622.5106\n",
      "Training Epoch: 30 [9150/36045]\tLoss: 460.2003\n",
      "Training Epoch: 30 [9200/36045]\tLoss: 344.8919\n",
      "Training Epoch: 30 [9250/36045]\tLoss: 374.0183\n",
      "Training Epoch: 30 [9300/36045]\tLoss: 384.7924\n",
      "Training Epoch: 30 [9350/36045]\tLoss: 354.7230\n",
      "Training Epoch: 30 [9400/36045]\tLoss: 695.1245\n",
      "Training Epoch: 30 [9450/36045]\tLoss: 738.3494\n",
      "Training Epoch: 30 [9500/36045]\tLoss: 725.3387\n",
      "Training Epoch: 30 [9550/36045]\tLoss: 767.2839\n",
      "Training Epoch: 30 [9600/36045]\tLoss: 570.2423\n",
      "Training Epoch: 30 [9650/36045]\tLoss: 574.6422\n",
      "Training Epoch: 30 [9700/36045]\tLoss: 559.9990\n",
      "Training Epoch: 30 [9750/36045]\tLoss: 559.2635\n",
      "Training Epoch: 30 [9800/36045]\tLoss: 730.7766\n",
      "Training Epoch: 30 [9850/36045]\tLoss: 771.8649\n",
      "Training Epoch: 30 [9900/36045]\tLoss: 784.3578\n",
      "Training Epoch: 30 [9950/36045]\tLoss: 764.1881\n",
      "Training Epoch: 30 [10000/36045]\tLoss: 706.1456\n",
      "Training Epoch: 30 [10050/36045]\tLoss: 581.2090\n",
      "Training Epoch: 30 [10100/36045]\tLoss: 588.4473\n",
      "Training Epoch: 30 [10150/36045]\tLoss: 597.8049\n",
      "Training Epoch: 30 [10200/36045]\tLoss: 586.6837\n",
      "Training Epoch: 30 [10250/36045]\tLoss: 701.7906\n",
      "Training Epoch: 30 [10300/36045]\tLoss: 681.6257\n",
      "Training Epoch: 30 [10350/36045]\tLoss: 717.6293\n",
      "Training Epoch: 30 [10400/36045]\tLoss: 708.0870\n",
      "Training Epoch: 30 [10450/36045]\tLoss: 663.3010\n",
      "Training Epoch: 30 [10500/36045]\tLoss: 555.0728\n",
      "Training Epoch: 30 [10550/36045]\tLoss: 550.1287\n",
      "Training Epoch: 30 [10600/36045]\tLoss: 572.9092\n",
      "Training Epoch: 30 [10650/36045]\tLoss: 579.1119\n",
      "Training Epoch: 30 [10700/36045]\tLoss: 663.4556\n",
      "Training Epoch: 30 [10750/36045]\tLoss: 724.5473\n",
      "Training Epoch: 30 [10800/36045]\tLoss: 668.5367\n",
      "Training Epoch: 30 [10850/36045]\tLoss: 708.1721\n",
      "Training Epoch: 30 [10900/36045]\tLoss: 736.9802\n",
      "Training Epoch: 30 [10950/36045]\tLoss: 544.3715\n",
      "Training Epoch: 30 [11000/36045]\tLoss: 538.0063\n",
      "Training Epoch: 30 [11050/36045]\tLoss: 576.2684\n",
      "Training Epoch: 30 [11100/36045]\tLoss: 587.4137\n",
      "Training Epoch: 30 [11150/36045]\tLoss: 636.8199\n",
      "Training Epoch: 30 [11200/36045]\tLoss: 664.8676\n",
      "Training Epoch: 30 [11250/36045]\tLoss: 676.8811\n",
      "Training Epoch: 30 [11300/36045]\tLoss: 656.9124\n",
      "Training Epoch: 30 [11350/36045]\tLoss: 654.1698\n",
      "Training Epoch: 30 [11400/36045]\tLoss: 615.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [11450/36045]\tLoss: 583.6395\n",
      "Training Epoch: 30 [11500/36045]\tLoss: 581.0879\n",
      "Training Epoch: 30 [11550/36045]\tLoss: 592.4556\n",
      "Training Epoch: 30 [11600/36045]\tLoss: 654.0840\n",
      "Training Epoch: 30 [11650/36045]\tLoss: 706.3297\n",
      "Training Epoch: 30 [11700/36045]\tLoss: 705.2689\n",
      "Training Epoch: 30 [11750/36045]\tLoss: 724.2869\n",
      "Training Epoch: 30 [11800/36045]\tLoss: 767.1786\n",
      "Training Epoch: 30 [11850/36045]\tLoss: 824.6174\n",
      "Training Epoch: 30 [11900/36045]\tLoss: 1040.1382\n",
      "Training Epoch: 30 [11950/36045]\tLoss: 1042.3534\n",
      "Training Epoch: 30 [12000/36045]\tLoss: 1055.4481\n",
      "Training Epoch: 30 [12050/36045]\tLoss: 1013.9680\n",
      "Training Epoch: 30 [12100/36045]\tLoss: 658.3990\n",
      "Training Epoch: 30 [12150/36045]\tLoss: 502.2025\n",
      "Training Epoch: 30 [12200/36045]\tLoss: 496.7953\n",
      "Training Epoch: 30 [12250/36045]\tLoss: 505.8182\n",
      "Training Epoch: 30 [12300/36045]\tLoss: 646.6652\n",
      "Training Epoch: 30 [12350/36045]\tLoss: 703.7914\n",
      "Training Epoch: 30 [12400/36045]\tLoss: 711.5281\n",
      "Training Epoch: 30 [12450/36045]\tLoss: 699.7923\n",
      "Training Epoch: 30 [12500/36045]\tLoss: 728.0564\n",
      "Training Epoch: 30 [12550/36045]\tLoss: 697.1154\n",
      "Training Epoch: 30 [12600/36045]\tLoss: 641.0159\n",
      "Training Epoch: 30 [12650/36045]\tLoss: 639.7883\n",
      "Training Epoch: 30 [12700/36045]\tLoss: 661.2610\n",
      "Training Epoch: 30 [12750/36045]\tLoss: 660.3913\n",
      "Training Epoch: 30 [12800/36045]\tLoss: 644.1437\n",
      "Training Epoch: 30 [12850/36045]\tLoss: 673.5438\n",
      "Training Epoch: 30 [12900/36045]\tLoss: 646.1196\n",
      "Training Epoch: 30 [12950/36045]\tLoss: 633.1064\n",
      "Training Epoch: 30 [13000/36045]\tLoss: 665.6536\n",
      "Training Epoch: 30 [13050/36045]\tLoss: 603.8646\n",
      "Training Epoch: 30 [13100/36045]\tLoss: 622.0007\n",
      "Training Epoch: 30 [13150/36045]\tLoss: 613.7104\n",
      "Training Epoch: 30 [13200/36045]\tLoss: 594.7457\n",
      "Training Epoch: 30 [13250/36045]\tLoss: 618.8159\n",
      "Training Epoch: 30 [13300/36045]\tLoss: 657.7164\n",
      "Training Epoch: 30 [13350/36045]\tLoss: 637.6017\n",
      "Training Epoch: 30 [13400/36045]\tLoss: 641.1463\n",
      "Training Epoch: 30 [13450/36045]\tLoss: 637.7047\n",
      "Training Epoch: 30 [13500/36045]\tLoss: 658.1924\n",
      "Training Epoch: 30 [13550/36045]\tLoss: 794.6953\n",
      "Training Epoch: 30 [13600/36045]\tLoss: 827.5626\n",
      "Training Epoch: 30 [13650/36045]\tLoss: 908.1874\n",
      "Training Epoch: 30 [13700/36045]\tLoss: 802.6074\n",
      "Training Epoch: 30 [13750/36045]\tLoss: 644.1604\n",
      "Training Epoch: 30 [13800/36045]\tLoss: 616.3970\n",
      "Training Epoch: 30 [13850/36045]\tLoss: 599.3239\n",
      "Training Epoch: 30 [13900/36045]\tLoss: 606.6671\n",
      "Training Epoch: 30 [13950/36045]\tLoss: 652.1688\n",
      "Training Epoch: 30 [14000/36045]\tLoss: 685.9044\n",
      "Training Epoch: 30 [14050/36045]\tLoss: 660.0035\n",
      "Training Epoch: 30 [14100/36045]\tLoss: 655.4888\n",
      "Training Epoch: 30 [14150/36045]\tLoss: 643.3870\n",
      "Training Epoch: 30 [14200/36045]\tLoss: 685.2562\n",
      "Training Epoch: 30 [14250/36045]\tLoss: 751.9587\n",
      "Training Epoch: 30 [14300/36045]\tLoss: 755.2780\n",
      "Training Epoch: 30 [14350/36045]\tLoss: 722.7565\n",
      "Training Epoch: 30 [14400/36045]\tLoss: 708.3690\n",
      "Training Epoch: 30 [14450/36045]\tLoss: 745.0555\n",
      "Training Epoch: 30 [14500/36045]\tLoss: 675.7911\n",
      "Training Epoch: 30 [14550/36045]\tLoss: 705.6973\n",
      "Training Epoch: 30 [14600/36045]\tLoss: 691.4471\n",
      "Training Epoch: 30 [14650/36045]\tLoss: 691.5436\n",
      "Training Epoch: 30 [14700/36045]\tLoss: 654.1301\n",
      "Training Epoch: 30 [14750/36045]\tLoss: 561.9969\n",
      "Training Epoch: 30 [14800/36045]\tLoss: 552.1050\n",
      "Training Epoch: 30 [14850/36045]\tLoss: 559.1813\n",
      "Training Epoch: 30 [14900/36045]\tLoss: 552.7017\n",
      "Training Epoch: 30 [14950/36045]\tLoss: 560.5386\n",
      "Training Epoch: 30 [15000/36045]\tLoss: 574.7560\n",
      "Training Epoch: 30 [15050/36045]\tLoss: 572.2844\n",
      "Training Epoch: 30 [15100/36045]\tLoss: 556.3464\n",
      "Training Epoch: 30 [15150/36045]\tLoss: 550.5171\n",
      "Training Epoch: 30 [15200/36045]\tLoss: 509.5453\n",
      "Training Epoch: 30 [15250/36045]\tLoss: 532.5323\n",
      "Training Epoch: 30 [15300/36045]\tLoss: 517.4518\n",
      "Training Epoch: 30 [15350/36045]\tLoss: 529.5920\n",
      "Training Epoch: 30 [15400/36045]\tLoss: 513.5483\n",
      "Training Epoch: 30 [15450/36045]\tLoss: 499.0778\n",
      "Training Epoch: 30 [15500/36045]\tLoss: 513.6079\n",
      "Training Epoch: 30 [15550/36045]\tLoss: 509.3738\n",
      "Training Epoch: 30 [15600/36045]\tLoss: 578.8616\n",
      "Training Epoch: 30 [15650/36045]\tLoss: 596.9196\n",
      "Training Epoch: 30 [15700/36045]\tLoss: 588.0801\n",
      "Training Epoch: 30 [15750/36045]\tLoss: 579.8262\n",
      "Training Epoch: 30 [15800/36045]\tLoss: 549.1824\n",
      "Training Epoch: 30 [15850/36045]\tLoss: 563.5137\n",
      "Training Epoch: 30 [15900/36045]\tLoss: 572.9777\n",
      "Training Epoch: 30 [15950/36045]\tLoss: 592.7661\n",
      "Training Epoch: 30 [16000/36045]\tLoss: 565.7185\n",
      "Training Epoch: 30 [16050/36045]\tLoss: 535.0672\n",
      "Training Epoch: 30 [16100/36045]\tLoss: 495.8572\n",
      "Training Epoch: 30 [16150/36045]\tLoss: 483.6543\n",
      "Training Epoch: 30 [16200/36045]\tLoss: 585.8906\n",
      "Training Epoch: 30 [16250/36045]\tLoss: 614.5600\n",
      "Training Epoch: 30 [16300/36045]\tLoss: 670.9650\n",
      "Training Epoch: 30 [16350/36045]\tLoss: 690.0187\n",
      "Training Epoch: 30 [16400/36045]\tLoss: 661.9325\n",
      "Training Epoch: 30 [16450/36045]\tLoss: 643.6694\n",
      "Training Epoch: 30 [16500/36045]\tLoss: 643.4614\n",
      "Training Epoch: 30 [16550/36045]\tLoss: 608.0222\n",
      "Training Epoch: 30 [16600/36045]\tLoss: 632.0684\n",
      "Training Epoch: 30 [16650/36045]\tLoss: 649.9874\n",
      "Training Epoch: 30 [16700/36045]\tLoss: 628.0218\n",
      "Training Epoch: 30 [16750/36045]\tLoss: 620.3242\n",
      "Training Epoch: 30 [16800/36045]\tLoss: 630.8025\n",
      "Training Epoch: 30 [16850/36045]\tLoss: 600.7557\n",
      "Training Epoch: 30 [16900/36045]\tLoss: 610.9203\n",
      "Training Epoch: 30 [16950/36045]\tLoss: 635.1749\n",
      "Training Epoch: 30 [17000/36045]\tLoss: 618.1254\n",
      "Training Epoch: 30 [17050/36045]\tLoss: 644.6354\n",
      "Training Epoch: 30 [17100/36045]\tLoss: 641.1233\n",
      "Training Epoch: 30 [17150/36045]\tLoss: 557.4828\n",
      "Training Epoch: 30 [17200/36045]\tLoss: 518.5558\n",
      "Training Epoch: 30 [17250/36045]\tLoss: 542.6631\n",
      "Training Epoch: 30 [17300/36045]\tLoss: 574.0525\n",
      "Training Epoch: 30 [17350/36045]\tLoss: 552.0675\n",
      "Training Epoch: 30 [17400/36045]\tLoss: 571.4639\n",
      "Training Epoch: 30 [17450/36045]\tLoss: 591.1599\n",
      "Training Epoch: 30 [17500/36045]\tLoss: 579.1899\n",
      "Training Epoch: 30 [17550/36045]\tLoss: 578.4283\n",
      "Training Epoch: 30 [17600/36045]\tLoss: 571.7624\n",
      "Training Epoch: 30 [17650/36045]\tLoss: 588.4283\n",
      "Training Epoch: 30 [17700/36045]\tLoss: 567.2791\n",
      "Training Epoch: 30 [17750/36045]\tLoss: 584.0890\n",
      "Training Epoch: 30 [17800/36045]\tLoss: 574.7975\n",
      "Training Epoch: 30 [17850/36045]\tLoss: 586.1034\n",
      "Training Epoch: 30 [17900/36045]\tLoss: 614.7208\n",
      "Training Epoch: 30 [17950/36045]\tLoss: 626.4598\n",
      "Training Epoch: 30 [18000/36045]\tLoss: 616.6695\n",
      "Training Epoch: 30 [18050/36045]\tLoss: 681.6291\n",
      "Training Epoch: 30 [18100/36045]\tLoss: 683.6516\n",
      "Training Epoch: 30 [18150/36045]\tLoss: 694.8456\n",
      "Training Epoch: 30 [18200/36045]\tLoss: 676.8632\n",
      "Training Epoch: 30 [18250/36045]\tLoss: 697.7607\n",
      "Training Epoch: 30 [18300/36045]\tLoss: 648.3414\n",
      "Training Epoch: 30 [18350/36045]\tLoss: 719.4648\n",
      "Training Epoch: 30 [18400/36045]\tLoss: 692.6802\n",
      "Training Epoch: 30 [18450/36045]\tLoss: 672.7619\n",
      "Training Epoch: 30 [18500/36045]\tLoss: 671.7705\n",
      "Training Epoch: 30 [18550/36045]\tLoss: 658.8046\n",
      "Training Epoch: 30 [18600/36045]\tLoss: 648.2401\n",
      "Training Epoch: 30 [18650/36045]\tLoss: 694.8820\n",
      "Training Epoch: 30 [18700/36045]\tLoss: 731.1912\n",
      "Training Epoch: 30 [18750/36045]\tLoss: 717.6602\n",
      "Training Epoch: 30 [18800/36045]\tLoss: 741.4866\n",
      "Training Epoch: 30 [18850/36045]\tLoss: 685.8397\n",
      "Training Epoch: 30 [18900/36045]\tLoss: 733.6616\n",
      "Training Epoch: 30 [18950/36045]\tLoss: 675.2602\n",
      "Training Epoch: 30 [19000/36045]\tLoss: 563.3975\n",
      "Training Epoch: 30 [19050/36045]\tLoss: 546.4700\n",
      "Training Epoch: 30 [19100/36045]\tLoss: 555.1928\n",
      "Training Epoch: 30 [19150/36045]\tLoss: 544.6737\n",
      "Training Epoch: 30 [19200/36045]\tLoss: 573.7418\n",
      "Training Epoch: 30 [19250/36045]\tLoss: 588.3898\n",
      "Training Epoch: 30 [19300/36045]\tLoss: 598.7219\n",
      "Training Epoch: 30 [19350/36045]\tLoss: 582.2408\n",
      "Training Epoch: 30 [19400/36045]\tLoss: 603.9037\n",
      "Training Epoch: 30 [19450/36045]\tLoss: 594.8509\n",
      "Training Epoch: 30 [19500/36045]\tLoss: 596.4879\n",
      "Training Epoch: 30 [19550/36045]\tLoss: 594.9921\n",
      "Training Epoch: 30 [19600/36045]\tLoss: 636.1330\n",
      "Training Epoch: 30 [19650/36045]\tLoss: 842.0934\n",
      "Training Epoch: 30 [19700/36045]\tLoss: 800.9245\n",
      "Training Epoch: 30 [19750/36045]\tLoss: 804.0719\n",
      "Training Epoch: 30 [19800/36045]\tLoss: 803.3613\n",
      "Training Epoch: 30 [19850/36045]\tLoss: 532.7508\n",
      "Training Epoch: 30 [19900/36045]\tLoss: 510.9460\n",
      "Training Epoch: 30 [19950/36045]\tLoss: 514.5331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [20000/36045]\tLoss: 513.4208\n",
      "Training Epoch: 30 [20050/36045]\tLoss: 574.9683\n",
      "Training Epoch: 30 [20100/36045]\tLoss: 581.2159\n",
      "Training Epoch: 30 [20150/36045]\tLoss: 583.2635\n",
      "Training Epoch: 30 [20200/36045]\tLoss: 583.1868\n",
      "Training Epoch: 30 [20250/36045]\tLoss: 621.0301\n",
      "Training Epoch: 30 [20300/36045]\tLoss: 657.0801\n",
      "Training Epoch: 30 [20350/36045]\tLoss: 676.0621\n",
      "Training Epoch: 30 [20400/36045]\tLoss: 692.7644\n",
      "Training Epoch: 30 [20450/36045]\tLoss: 663.3586\n",
      "Training Epoch: 30 [20500/36045]\tLoss: 647.3501\n",
      "Training Epoch: 30 [20550/36045]\tLoss: 569.5671\n",
      "Training Epoch: 30 [20600/36045]\tLoss: 580.3401\n",
      "Training Epoch: 30 [20650/36045]\tLoss: 577.2347\n",
      "Training Epoch: 30 [20700/36045]\tLoss: 565.1140\n",
      "Training Epoch: 30 [20750/36045]\tLoss: 608.4218\n",
      "Training Epoch: 30 [20800/36045]\tLoss: 661.3350\n",
      "Training Epoch: 30 [20850/36045]\tLoss: 648.0333\n",
      "Training Epoch: 30 [20900/36045]\tLoss: 693.0805\n",
      "Training Epoch: 30 [20950/36045]\tLoss: 653.6427\n",
      "Training Epoch: 30 [21000/36045]\tLoss: 615.7660\n",
      "Training Epoch: 30 [21050/36045]\tLoss: 527.3315\n",
      "Training Epoch: 30 [21100/36045]\tLoss: 531.0628\n",
      "Training Epoch: 30 [21150/36045]\tLoss: 568.3748\n",
      "Training Epoch: 30 [21200/36045]\tLoss: 567.5853\n",
      "Training Epoch: 30 [21250/36045]\tLoss: 543.0845\n",
      "Training Epoch: 30 [21300/36045]\tLoss: 634.1550\n",
      "Training Epoch: 30 [21350/36045]\tLoss: 626.4581\n",
      "Training Epoch: 30 [21400/36045]\tLoss: 629.8606\n",
      "Training Epoch: 30 [21450/36045]\tLoss: 636.1103\n",
      "Training Epoch: 30 [21500/36045]\tLoss: 638.8078\n",
      "Training Epoch: 30 [21550/36045]\tLoss: 733.8520\n",
      "Training Epoch: 30 [21600/36045]\tLoss: 733.2317\n",
      "Training Epoch: 30 [21650/36045]\tLoss: 745.9537\n",
      "Training Epoch: 30 [21700/36045]\tLoss: 747.9241\n",
      "Training Epoch: 30 [21750/36045]\tLoss: 718.5748\n",
      "Training Epoch: 30 [21800/36045]\tLoss: 530.4820\n",
      "Training Epoch: 30 [21850/36045]\tLoss: 513.4507\n",
      "Training Epoch: 30 [21900/36045]\tLoss: 523.7177\n",
      "Training Epoch: 30 [21950/36045]\tLoss: 523.4977\n",
      "Training Epoch: 30 [22000/36045]\tLoss: 527.1417\n",
      "Training Epoch: 30 [22050/36045]\tLoss: 549.8853\n",
      "Training Epoch: 30 [22100/36045]\tLoss: 542.4314\n",
      "Training Epoch: 30 [22150/36045]\tLoss: 527.5692\n",
      "Training Epoch: 30 [22200/36045]\tLoss: 544.5246\n",
      "Training Epoch: 30 [22250/36045]\tLoss: 549.5361\n",
      "Training Epoch: 30 [22300/36045]\tLoss: 602.1655\n",
      "Training Epoch: 30 [22350/36045]\tLoss: 628.2509\n",
      "Training Epoch: 30 [22400/36045]\tLoss: 643.1448\n",
      "Training Epoch: 30 [22450/36045]\tLoss: 630.8171\n",
      "Training Epoch: 30 [22500/36045]\tLoss: 612.7576\n",
      "Training Epoch: 30 [22550/36045]\tLoss: 648.9000\n",
      "Training Epoch: 30 [22600/36045]\tLoss: 703.4500\n",
      "Training Epoch: 30 [22650/36045]\tLoss: 738.7245\n",
      "Training Epoch: 30 [22700/36045]\tLoss: 761.5008\n",
      "Training Epoch: 30 [22750/36045]\tLoss: 781.2415\n",
      "Training Epoch: 30 [22800/36045]\tLoss: 812.2366\n",
      "Training Epoch: 30 [22850/36045]\tLoss: 675.6429\n",
      "Training Epoch: 30 [22900/36045]\tLoss: 680.3490\n",
      "Training Epoch: 30 [22950/36045]\tLoss: 659.5939\n",
      "Training Epoch: 30 [23000/36045]\tLoss: 657.2112\n",
      "Training Epoch: 30 [23050/36045]\tLoss: 584.2346\n",
      "Training Epoch: 30 [23100/36045]\tLoss: 600.2495\n",
      "Training Epoch: 30 [23150/36045]\tLoss: 588.4047\n",
      "Training Epoch: 30 [23200/36045]\tLoss: 557.3316\n",
      "Training Epoch: 30 [23250/36045]\tLoss: 560.3265\n",
      "Training Epoch: 30 [23300/36045]\tLoss: 556.8486\n",
      "Training Epoch: 30 [23350/36045]\tLoss: 578.0703\n",
      "Training Epoch: 30 [23400/36045]\tLoss: 626.3502\n",
      "Training Epoch: 30 [23450/36045]\tLoss: 619.3154\n",
      "Training Epoch: 30 [23500/36045]\tLoss: 596.9881\n",
      "Training Epoch: 30 [23550/36045]\tLoss: 640.2925\n",
      "Training Epoch: 30 [23600/36045]\tLoss: 723.5010\n",
      "Training Epoch: 30 [23650/36045]\tLoss: 736.6280\n",
      "Training Epoch: 30 [23700/36045]\tLoss: 745.1719\n",
      "Training Epoch: 30 [23750/36045]\tLoss: 720.2922\n",
      "Training Epoch: 30 [23800/36045]\tLoss: 575.3453\n",
      "Training Epoch: 30 [23850/36045]\tLoss: 601.8292\n",
      "Training Epoch: 30 [23900/36045]\tLoss: 591.4089\n",
      "Training Epoch: 30 [23950/36045]\tLoss: 574.3434\n",
      "Training Epoch: 30 [24000/36045]\tLoss: 550.8215\n",
      "Training Epoch: 30 [24050/36045]\tLoss: 509.0267\n",
      "Training Epoch: 30 [24100/36045]\tLoss: 535.5529\n",
      "Training Epoch: 30 [24150/36045]\tLoss: 528.9162\n",
      "Training Epoch: 30 [24200/36045]\tLoss: 525.1937\n",
      "Training Epoch: 30 [24250/36045]\tLoss: 509.7684\n",
      "Training Epoch: 30 [24300/36045]\tLoss: 550.4722\n",
      "Training Epoch: 30 [24350/36045]\tLoss: 563.9267\n",
      "Training Epoch: 30 [24400/36045]\tLoss: 579.7681\n",
      "Training Epoch: 30 [24450/36045]\tLoss: 552.7897\n",
      "Training Epoch: 30 [24500/36045]\tLoss: 582.6821\n",
      "Training Epoch: 30 [24550/36045]\tLoss: 672.5148\n",
      "Training Epoch: 30 [24600/36045]\tLoss: 664.4402\n",
      "Training Epoch: 30 [24650/36045]\tLoss: 637.2192\n",
      "Training Epoch: 30 [24700/36045]\tLoss: 647.2952\n",
      "Training Epoch: 30 [24750/36045]\tLoss: 598.0345\n",
      "Training Epoch: 30 [24800/36045]\tLoss: 494.2341\n",
      "Training Epoch: 30 [24850/36045]\tLoss: 513.7461\n",
      "Training Epoch: 30 [24900/36045]\tLoss: 510.8106\n",
      "Training Epoch: 30 [24950/36045]\tLoss: 513.3889\n",
      "Training Epoch: 30 [25000/36045]\tLoss: 493.5609\n",
      "Training Epoch: 30 [25050/36045]\tLoss: 471.3633\n",
      "Training Epoch: 30 [25100/36045]\tLoss: 422.4957\n",
      "Training Epoch: 30 [25150/36045]\tLoss: 391.3227\n",
      "Training Epoch: 30 [25200/36045]\tLoss: 386.3681\n",
      "Training Epoch: 30 [25250/36045]\tLoss: 414.0524\n",
      "Training Epoch: 30 [25300/36045]\tLoss: 543.7275\n",
      "Training Epoch: 30 [25350/36045]\tLoss: 540.6249\n",
      "Training Epoch: 30 [25400/36045]\tLoss: 503.9803\n",
      "Training Epoch: 30 [25450/36045]\tLoss: 506.5695\n",
      "Training Epoch: 30 [25500/36045]\tLoss: 550.4381\n",
      "Training Epoch: 30 [25550/36045]\tLoss: 642.5657\n",
      "Training Epoch: 30 [25600/36045]\tLoss: 647.2213\n",
      "Training Epoch: 30 [25650/36045]\tLoss: 624.5719\n",
      "Training Epoch: 30 [25700/36045]\tLoss: 633.6888\n",
      "Training Epoch: 30 [25750/36045]\tLoss: 610.7665\n",
      "Training Epoch: 30 [25800/36045]\tLoss: 385.1293\n",
      "Training Epoch: 30 [25850/36045]\tLoss: 394.8979\n",
      "Training Epoch: 30 [25900/36045]\tLoss: 375.9059\n",
      "Training Epoch: 30 [25950/36045]\tLoss: 384.7045\n",
      "Training Epoch: 30 [26000/36045]\tLoss: 471.4984\n",
      "Training Epoch: 30 [26050/36045]\tLoss: 641.8690\n",
      "Training Epoch: 30 [26100/36045]\tLoss: 669.5563\n",
      "Training Epoch: 30 [26150/36045]\tLoss: 669.9108\n",
      "Training Epoch: 30 [26200/36045]\tLoss: 643.0665\n",
      "Training Epoch: 30 [26250/36045]\tLoss: 673.2628\n",
      "Training Epoch: 30 [26300/36045]\tLoss: 608.6757\n",
      "Training Epoch: 30 [26350/36045]\tLoss: 619.3649\n",
      "Training Epoch: 30 [26400/36045]\tLoss: 596.7197\n",
      "Training Epoch: 30 [26450/36045]\tLoss: 526.4854\n",
      "Training Epoch: 30 [26500/36045]\tLoss: 625.7970\n",
      "Training Epoch: 30 [26550/36045]\tLoss: 626.9170\n",
      "Training Epoch: 30 [26600/36045]\tLoss: 622.9152\n",
      "Training Epoch: 30 [26650/36045]\tLoss: 639.1660\n",
      "Training Epoch: 30 [26700/36045]\tLoss: 618.7991\n",
      "Training Epoch: 30 [26750/36045]\tLoss: 579.4683\n",
      "Training Epoch: 30 [26800/36045]\tLoss: 426.7940\n",
      "Training Epoch: 30 [26850/36045]\tLoss: 354.0178\n",
      "Training Epoch: 30 [26900/36045]\tLoss: 357.0310\n",
      "Training Epoch: 30 [26950/36045]\tLoss: 392.5195\n",
      "Training Epoch: 30 [27000/36045]\tLoss: 639.3804\n",
      "Training Epoch: 30 [27050/36045]\tLoss: 668.2980\n",
      "Training Epoch: 30 [27100/36045]\tLoss: 647.3832\n",
      "Training Epoch: 30 [27150/36045]\tLoss: 688.0018\n",
      "Training Epoch: 30 [27200/36045]\tLoss: 503.3788\n",
      "Training Epoch: 30 [27250/36045]\tLoss: 495.5335\n",
      "Training Epoch: 30 [27300/36045]\tLoss: 482.5559\n",
      "Training Epoch: 30 [27350/36045]\tLoss: 481.2498\n",
      "Training Epoch: 30 [27400/36045]\tLoss: 480.3624\n",
      "Training Epoch: 30 [27450/36045]\tLoss: 606.5257\n",
      "Training Epoch: 30 [27500/36045]\tLoss: 650.0908\n",
      "Training Epoch: 30 [27550/36045]\tLoss: 642.8735\n",
      "Training Epoch: 30 [27600/36045]\tLoss: 654.6268\n",
      "Training Epoch: 30 [27650/36045]\tLoss: 646.3808\n",
      "Training Epoch: 30 [27700/36045]\tLoss: 675.6376\n",
      "Training Epoch: 30 [27750/36045]\tLoss: 687.8131\n",
      "Training Epoch: 30 [27800/36045]\tLoss: 674.3787\n",
      "Training Epoch: 30 [27850/36045]\tLoss: 663.8696\n",
      "Training Epoch: 30 [27900/36045]\tLoss: 599.8229\n",
      "Training Epoch: 30 [27950/36045]\tLoss: 499.0205\n",
      "Training Epoch: 30 [28000/36045]\tLoss: 475.3090\n",
      "Training Epoch: 30 [28050/36045]\tLoss: 485.6788\n",
      "Training Epoch: 30 [28100/36045]\tLoss: 477.3364\n",
      "Training Epoch: 30 [28150/36045]\tLoss: 501.2586\n",
      "Training Epoch: 30 [28200/36045]\tLoss: 508.1030\n",
      "Training Epoch: 30 [28250/36045]\tLoss: 501.4711\n",
      "Training Epoch: 30 [28300/36045]\tLoss: 475.8473\n",
      "Training Epoch: 30 [28350/36045]\tLoss: 472.0324\n",
      "Training Epoch: 30 [28400/36045]\tLoss: 803.5972\n",
      "Training Epoch: 30 [28450/36045]\tLoss: 735.4779\n",
      "Training Epoch: 30 [28500/36045]\tLoss: 635.6331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 30 [28550/36045]\tLoss: 583.6478\n",
      "Training Epoch: 30 [28600/36045]\tLoss: 615.5579\n",
      "Training Epoch: 30 [28650/36045]\tLoss: 682.9438\n",
      "Training Epoch: 30 [28700/36045]\tLoss: 677.2929\n",
      "Training Epoch: 30 [28750/36045]\tLoss: 663.7979\n",
      "Training Epoch: 30 [28800/36045]\tLoss: 672.4524\n",
      "Training Epoch: 30 [28850/36045]\tLoss: 582.8596\n",
      "Training Epoch: 30 [28900/36045]\tLoss: 472.3292\n",
      "Training Epoch: 30 [28950/36045]\tLoss: 471.1510\n",
      "Training Epoch: 30 [29000/36045]\tLoss: 468.6928\n",
      "Training Epoch: 30 [29050/36045]\tLoss: 475.7745\n",
      "Training Epoch: 30 [29100/36045]\tLoss: 494.8981\n",
      "Training Epoch: 30 [29150/36045]\tLoss: 483.2982\n",
      "Training Epoch: 30 [29200/36045]\tLoss: 468.7697\n",
      "Training Epoch: 30 [29250/36045]\tLoss: 458.0996\n",
      "Training Epoch: 30 [29300/36045]\tLoss: 520.2894\n",
      "Training Epoch: 30 [29350/36045]\tLoss: 614.2855\n",
      "Training Epoch: 30 [29400/36045]\tLoss: 632.0625\n",
      "Training Epoch: 30 [29450/36045]\tLoss: 650.5714\n",
      "Training Epoch: 30 [29500/36045]\tLoss: 665.0679\n",
      "Training Epoch: 30 [29550/36045]\tLoss: 632.8037\n",
      "Training Epoch: 30 [29600/36045]\tLoss: 534.2412\n",
      "Training Epoch: 30 [29650/36045]\tLoss: 516.8391\n",
      "Training Epoch: 30 [29700/36045]\tLoss: 461.6693\n",
      "Training Epoch: 30 [29750/36045]\tLoss: 460.8221\n",
      "Training Epoch: 30 [29800/36045]\tLoss: 507.4549\n",
      "Training Epoch: 30 [29850/36045]\tLoss: 582.8655\n",
      "Training Epoch: 30 [29900/36045]\tLoss: 579.5819\n",
      "Training Epoch: 30 [29950/36045]\tLoss: 601.7430\n",
      "Training Epoch: 30 [30000/36045]\tLoss: 576.7750\n",
      "Training Epoch: 30 [30050/36045]\tLoss: 582.8782\n",
      "Training Epoch: 30 [30100/36045]\tLoss: 711.0976\n",
      "Training Epoch: 30 [30150/36045]\tLoss: 694.6993\n",
      "Training Epoch: 30 [30200/36045]\tLoss: 655.4673\n",
      "Training Epoch: 30 [30250/36045]\tLoss: 704.3691\n",
      "Training Epoch: 30 [30300/36045]\tLoss: 689.4867\n",
      "Training Epoch: 30 [30350/36045]\tLoss: 534.2864\n",
      "Training Epoch: 30 [30400/36045]\tLoss: 518.8232\n",
      "Training Epoch: 30 [30450/36045]\tLoss: 520.1879\n",
      "Training Epoch: 30 [30500/36045]\tLoss: 486.0730\n",
      "Training Epoch: 30 [30550/36045]\tLoss: 450.4829\n",
      "Training Epoch: 30 [30600/36045]\tLoss: 440.6235\n",
      "Training Epoch: 30 [30650/36045]\tLoss: 430.5993\n",
      "Training Epoch: 30 [30700/36045]\tLoss: 448.4230\n",
      "Training Epoch: 30 [30750/36045]\tLoss: 435.1220\n",
      "Training Epoch: 30 [30800/36045]\tLoss: 462.1199\n",
      "Training Epoch: 30 [30850/36045]\tLoss: 453.8530\n",
      "Training Epoch: 30 [30900/36045]\tLoss: 466.6199\n",
      "Training Epoch: 30 [30950/36045]\tLoss: 490.2386\n",
      "Training Epoch: 30 [31000/36045]\tLoss: 481.8592\n",
      "Training Epoch: 30 [31050/36045]\tLoss: 402.6329\n",
      "Training Epoch: 30 [31100/36045]\tLoss: 393.2253\n",
      "Training Epoch: 30 [31150/36045]\tLoss: 400.3907\n",
      "Training Epoch: 30 [31200/36045]\tLoss: 498.6171\n",
      "Training Epoch: 30 [31250/36045]\tLoss: 646.9844\n",
      "Training Epoch: 30 [31300/36045]\tLoss: 617.2437\n",
      "Training Epoch: 30 [31350/36045]\tLoss: 633.1240\n",
      "Training Epoch: 30 [31400/36045]\tLoss: 612.4761\n",
      "Training Epoch: 30 [31450/36045]\tLoss: 628.3890\n",
      "Training Epoch: 30 [31500/36045]\tLoss: 640.6751\n",
      "Training Epoch: 30 [31550/36045]\tLoss: 648.5511\n",
      "Training Epoch: 30 [31600/36045]\tLoss: 609.6290\n",
      "Training Epoch: 30 [31650/36045]\tLoss: 651.6841\n",
      "Training Epoch: 30 [31700/36045]\tLoss: 472.5431\n",
      "Training Epoch: 30 [31750/36045]\tLoss: 391.1989\n",
      "Training Epoch: 30 [31800/36045]\tLoss: 372.9857\n",
      "Training Epoch: 30 [31850/36045]\tLoss: 381.9266\n",
      "Training Epoch: 30 [31900/36045]\tLoss: 599.0199\n",
      "Training Epoch: 30 [31950/36045]\tLoss: 773.1171\n",
      "Training Epoch: 30 [32000/36045]\tLoss: 883.6260\n",
      "Training Epoch: 30 [32050/36045]\tLoss: 838.1519\n",
      "Training Epoch: 30 [32100/36045]\tLoss: 828.1694\n",
      "Training Epoch: 30 [32150/36045]\tLoss: 643.0453\n",
      "Training Epoch: 30 [32200/36045]\tLoss: 646.6436\n",
      "Training Epoch: 30 [32250/36045]\tLoss: 657.3300\n",
      "Training Epoch: 30 [32300/36045]\tLoss: 639.0206\n",
      "Training Epoch: 30 [32350/36045]\tLoss: 634.3082\n",
      "Training Epoch: 30 [32400/36045]\tLoss: 595.2729\n",
      "Training Epoch: 30 [32450/36045]\tLoss: 490.3948\n",
      "Training Epoch: 30 [32500/36045]\tLoss: 471.4385\n",
      "Training Epoch: 30 [32550/36045]\tLoss: 473.8663\n",
      "Training Epoch: 30 [32600/36045]\tLoss: 470.4969\n",
      "Training Epoch: 30 [32650/36045]\tLoss: 603.9587\n",
      "Training Epoch: 30 [32700/36045]\tLoss: 658.8456\n",
      "Training Epoch: 30 [32750/36045]\tLoss: 627.9020\n",
      "Training Epoch: 30 [32800/36045]\tLoss: 644.0805\n",
      "Training Epoch: 30 [32850/36045]\tLoss: 594.6517\n",
      "Training Epoch: 30 [32900/36045]\tLoss: 477.5410\n",
      "Training Epoch: 30 [32950/36045]\tLoss: 499.8022\n",
      "Training Epoch: 30 [33000/36045]\tLoss: 499.2830\n",
      "Training Epoch: 30 [33050/36045]\tLoss: 474.1938\n",
      "Training Epoch: 30 [33100/36045]\tLoss: 538.9525\n",
      "Training Epoch: 30 [33150/36045]\tLoss: 731.4738\n",
      "Training Epoch: 30 [33200/36045]\tLoss: 712.4707\n",
      "Training Epoch: 30 [33250/36045]\tLoss: 734.1256\n",
      "Training Epoch: 30 [33300/36045]\tLoss: 781.6990\n",
      "Training Epoch: 30 [33350/36045]\tLoss: 599.6688\n",
      "Training Epoch: 30 [33400/36045]\tLoss: 439.9878\n",
      "Training Epoch: 30 [33450/36045]\tLoss: 435.1906\n",
      "Training Epoch: 30 [33500/36045]\tLoss: 447.9501\n",
      "Training Epoch: 30 [33550/36045]\tLoss: 464.4193\n",
      "Training Epoch: 30 [33600/36045]\tLoss: 466.1618\n",
      "Training Epoch: 30 [33650/36045]\tLoss: 621.6561\n",
      "Training Epoch: 30 [33700/36045]\tLoss: 601.5049\n",
      "Training Epoch: 30 [33750/36045]\tLoss: 622.8188\n",
      "Training Epoch: 30 [33800/36045]\tLoss: 618.6060\n",
      "Training Epoch: 30 [33850/36045]\tLoss: 621.0459\n",
      "Training Epoch: 30 [33900/36045]\tLoss: 633.4512\n",
      "Training Epoch: 30 [33950/36045]\tLoss: 644.3446\n",
      "Training Epoch: 30 [34000/36045]\tLoss: 631.0515\n",
      "Training Epoch: 30 [34050/36045]\tLoss: 635.2834\n",
      "Training Epoch: 30 [34100/36045]\tLoss: 611.5814\n",
      "Training Epoch: 30 [34150/36045]\tLoss: 568.0903\n",
      "Training Epoch: 30 [34200/36045]\tLoss: 537.9001\n",
      "Training Epoch: 30 [34250/36045]\tLoss: 552.3374\n",
      "Training Epoch: 30 [34300/36045]\tLoss: 472.4795\n",
      "Training Epoch: 30 [34350/36045]\tLoss: 497.5698\n",
      "Training Epoch: 30 [34400/36045]\tLoss: 489.1317\n",
      "Training Epoch: 30 [34450/36045]\tLoss: 459.8019\n",
      "Training Epoch: 30 [34500/36045]\tLoss: 490.7687\n",
      "Training Epoch: 30 [34550/36045]\tLoss: 481.6369\n",
      "Training Epoch: 30 [34600/36045]\tLoss: 485.7280\n",
      "Training Epoch: 30 [34650/36045]\tLoss: 593.2936\n",
      "Training Epoch: 30 [34700/36045]\tLoss: 628.4285\n",
      "Training Epoch: 30 [34750/36045]\tLoss: 557.5094\n",
      "Training Epoch: 30 [34800/36045]\tLoss: 638.2421\n",
      "Training Epoch: 30 [34850/36045]\tLoss: 646.2535\n",
      "Training Epoch: 30 [34900/36045]\tLoss: 707.3701\n",
      "Training Epoch: 30 [34950/36045]\tLoss: 693.6302\n",
      "Training Epoch: 30 [35000/36045]\tLoss: 694.5275\n",
      "Training Epoch: 30 [35050/36045]\tLoss: 680.6550\n",
      "Training Epoch: 30 [35100/36045]\tLoss: 577.6138\n",
      "Training Epoch: 30 [35150/36045]\tLoss: 570.6362\n",
      "Training Epoch: 30 [35200/36045]\tLoss: 482.9278\n",
      "Training Epoch: 30 [35250/36045]\tLoss: 530.2467\n",
      "Training Epoch: 30 [35300/36045]\tLoss: 544.7576\n",
      "Training Epoch: 30 [35350/36045]\tLoss: 615.7773\n",
      "Training Epoch: 30 [35400/36045]\tLoss: 649.8564\n",
      "Training Epoch: 30 [35450/36045]\tLoss: 620.1655\n",
      "Training Epoch: 30 [35500/36045]\tLoss: 601.5495\n",
      "Training Epoch: 30 [35550/36045]\tLoss: 586.7990\n",
      "Training Epoch: 30 [35600/36045]\tLoss: 635.1274\n",
      "Training Epoch: 30 [35650/36045]\tLoss: 708.7150\n",
      "Training Epoch: 30 [35700/36045]\tLoss: 636.1370\n",
      "Training Epoch: 30 [35750/36045]\tLoss: 694.2023\n",
      "Training Epoch: 30 [35800/36045]\tLoss: 700.6753\n",
      "Training Epoch: 30 [35850/36045]\tLoss: 674.6376\n",
      "Training Epoch: 30 [35900/36045]\tLoss: 699.8672\n",
      "Training Epoch: 30 [35950/36045]\tLoss: 697.4314\n",
      "Training Epoch: 30 [36000/36045]\tLoss: 689.7574\n",
      "Training Epoch: 30 [36045/36045]\tLoss: 673.4781\n",
      "Training Epoch: 30 [4004/4004]\tLoss: 626.8752\n",
      "Training Epoch: 31 [50/36045]\tLoss: 626.2469\n",
      "Training Epoch: 31 [100/36045]\tLoss: 600.5225\n",
      "Training Epoch: 31 [150/36045]\tLoss: 598.7545\n",
      "Training Epoch: 31 [200/36045]\tLoss: 585.4737\n",
      "Training Epoch: 31 [250/36045]\tLoss: 699.3154\n",
      "Training Epoch: 31 [300/36045]\tLoss: 763.4141\n",
      "Training Epoch: 31 [350/36045]\tLoss: 729.3317\n",
      "Training Epoch: 31 [400/36045]\tLoss: 724.7838\n",
      "Training Epoch: 31 [450/36045]\tLoss: 704.9779\n",
      "Training Epoch: 31 [500/36045]\tLoss: 655.2296\n",
      "Training Epoch: 31 [550/36045]\tLoss: 658.8286\n",
      "Training Epoch: 31 [600/36045]\tLoss: 641.0666\n",
      "Training Epoch: 31 [650/36045]\tLoss: 664.1310\n",
      "Training Epoch: 31 [700/36045]\tLoss: 650.8448\n",
      "Training Epoch: 31 [750/36045]\tLoss: 630.9362\n",
      "Training Epoch: 31 [800/36045]\tLoss: 644.3240\n",
      "Training Epoch: 31 [850/36045]\tLoss: 625.5361\n",
      "Training Epoch: 31 [900/36045]\tLoss: 596.9135\n",
      "Training Epoch: 31 [950/36045]\tLoss: 565.5036\n",
      "Training Epoch: 31 [1000/36045]\tLoss: 546.4948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31 [1050/36045]\tLoss: 548.6365\n",
      "Training Epoch: 31 [1100/36045]\tLoss: 533.8625\n",
      "Training Epoch: 31 [1150/36045]\tLoss: 542.5948\n",
      "Training Epoch: 31 [1200/36045]\tLoss: 573.1582\n",
      "Training Epoch: 31 [1250/36045]\tLoss: 656.0491\n",
      "Training Epoch: 31 [1300/36045]\tLoss: 662.8394\n",
      "Training Epoch: 31 [1350/36045]\tLoss: 665.0121\n",
      "Training Epoch: 31 [1400/36045]\tLoss: 690.9882\n",
      "Training Epoch: 31 [1450/36045]\tLoss: 668.2570\n",
      "Training Epoch: 31 [1500/36045]\tLoss: 612.5674\n",
      "Training Epoch: 31 [1550/36045]\tLoss: 627.9869\n",
      "Training Epoch: 31 [1600/36045]\tLoss: 638.8929\n",
      "Training Epoch: 31 [1650/36045]\tLoss: 625.9738\n",
      "Training Epoch: 31 [1700/36045]\tLoss: 638.2666\n",
      "Training Epoch: 31 [1750/36045]\tLoss: 679.7184\n",
      "Training Epoch: 31 [1800/36045]\tLoss: 660.8766\n",
      "Training Epoch: 31 [1850/36045]\tLoss: 677.7900\n",
      "Training Epoch: 31 [1900/36045]\tLoss: 634.5494\n",
      "Training Epoch: 31 [1950/36045]\tLoss: 645.5741\n",
      "Training Epoch: 31 [2000/36045]\tLoss: 583.5938\n",
      "Training Epoch: 31 [2050/36045]\tLoss: 586.0238\n",
      "Training Epoch: 31 [2100/36045]\tLoss: 617.1464\n",
      "Training Epoch: 31 [2150/36045]\tLoss: 596.6860\n",
      "Training Epoch: 31 [2200/36045]\tLoss: 554.9711\n",
      "Training Epoch: 31 [2250/36045]\tLoss: 524.1000\n",
      "Training Epoch: 31 [2300/36045]\tLoss: 549.7343\n",
      "Training Epoch: 31 [2350/36045]\tLoss: 525.2432\n",
      "Training Epoch: 31 [2400/36045]\tLoss: 533.8792\n",
      "Training Epoch: 31 [2450/36045]\tLoss: 680.8868\n",
      "Training Epoch: 31 [2500/36045]\tLoss: 715.5251\n",
      "Training Epoch: 31 [2550/36045]\tLoss: 712.8953\n",
      "Training Epoch: 31 [2600/36045]\tLoss: 721.6702\n",
      "Training Epoch: 31 [2650/36045]\tLoss: 846.8930\n",
      "Training Epoch: 31 [2700/36045]\tLoss: 934.1176\n",
      "Training Epoch: 31 [2750/36045]\tLoss: 1005.8286\n",
      "Training Epoch: 31 [2800/36045]\tLoss: 1015.7006\n",
      "Training Epoch: 31 [2850/36045]\tLoss: 784.3163\n",
      "Training Epoch: 31 [2900/36045]\tLoss: 748.3498\n",
      "Training Epoch: 31 [2950/36045]\tLoss: 722.5358\n",
      "Training Epoch: 31 [3000/36045]\tLoss: 716.8250\n",
      "Training Epoch: 31 [3050/36045]\tLoss: 747.6821\n",
      "Training Epoch: 31 [3100/36045]\tLoss: 684.2606\n",
      "Training Epoch: 31 [3150/36045]\tLoss: 528.0176\n",
      "Training Epoch: 31 [3200/36045]\tLoss: 547.4852\n",
      "Training Epoch: 31 [3250/36045]\tLoss: 515.5139\n",
      "Training Epoch: 31 [3300/36045]\tLoss: 487.8327\n",
      "Training Epoch: 31 [3350/36045]\tLoss: 514.6197\n",
      "Training Epoch: 31 [3400/36045]\tLoss: 540.1049\n",
      "Training Epoch: 31 [3450/36045]\tLoss: 580.1168\n",
      "Training Epoch: 31 [3500/36045]\tLoss: 567.4456\n",
      "Training Epoch: 31 [3550/36045]\tLoss: 543.7464\n",
      "Training Epoch: 31 [3600/36045]\tLoss: 582.8629\n",
      "Training Epoch: 31 [3650/36045]\tLoss: 673.5773\n",
      "Training Epoch: 31 [3700/36045]\tLoss: 680.4583\n",
      "Training Epoch: 31 [3750/36045]\tLoss: 649.3909\n",
      "Training Epoch: 31 [3800/36045]\tLoss: 644.6379\n",
      "Training Epoch: 31 [3850/36045]\tLoss: 644.1819\n",
      "Training Epoch: 31 [3900/36045]\tLoss: 649.2576\n",
      "Training Epoch: 31 [3950/36045]\tLoss: 626.3217\n",
      "Training Epoch: 31 [4000/36045]\tLoss: 632.0298\n",
      "Training Epoch: 31 [4050/36045]\tLoss: 580.6447\n",
      "Training Epoch: 31 [4100/36045]\tLoss: 566.1094\n",
      "Training Epoch: 31 [4150/36045]\tLoss: 581.9883\n",
      "Training Epoch: 31 [4200/36045]\tLoss: 576.6618\n",
      "Training Epoch: 31 [4250/36045]\tLoss: 578.9100\n",
      "Training Epoch: 31 [4300/36045]\tLoss: 596.8132\n",
      "Training Epoch: 31 [4350/36045]\tLoss: 579.5743\n",
      "Training Epoch: 31 [4400/36045]\tLoss: 554.7535\n",
      "Training Epoch: 31 [4450/36045]\tLoss: 607.1337\n",
      "Training Epoch: 31 [4500/36045]\tLoss: 650.6072\n",
      "Training Epoch: 31 [4550/36045]\tLoss: 655.1774\n",
      "Training Epoch: 31 [4600/36045]\tLoss: 677.9959\n",
      "Training Epoch: 31 [4650/36045]\tLoss: 667.3820\n",
      "Training Epoch: 31 [4700/36045]\tLoss: 616.1608\n",
      "Training Epoch: 31 [4750/36045]\tLoss: 598.4813\n",
      "Training Epoch: 31 [4800/36045]\tLoss: 624.1554\n",
      "Training Epoch: 31 [4850/36045]\tLoss: 610.7128\n",
      "Training Epoch: 31 [4900/36045]\tLoss: 594.1839\n",
      "Training Epoch: 31 [4950/36045]\tLoss: 610.5717\n",
      "Training Epoch: 31 [5000/36045]\tLoss: 640.8478\n",
      "Training Epoch: 31 [5050/36045]\tLoss: 620.7155\n",
      "Training Epoch: 31 [5100/36045]\tLoss: 631.1635\n",
      "Training Epoch: 31 [5150/36045]\tLoss: 615.5450\n",
      "Training Epoch: 31 [5200/36045]\tLoss: 613.5606\n",
      "Training Epoch: 31 [5250/36045]\tLoss: 607.0654\n",
      "Training Epoch: 31 [5300/36045]\tLoss: 607.5837\n",
      "Training Epoch: 31 [5350/36045]\tLoss: 630.2303\n",
      "Training Epoch: 31 [5400/36045]\tLoss: 606.9161\n",
      "Training Epoch: 31 [5450/36045]\tLoss: 575.5521\n",
      "Training Epoch: 31 [5500/36045]\tLoss: 604.4583\n",
      "Training Epoch: 31 [5550/36045]\tLoss: 592.5716\n",
      "Training Epoch: 31 [5600/36045]\tLoss: 674.6819\n",
      "Training Epoch: 31 [5650/36045]\tLoss: 638.7385\n",
      "Training Epoch: 31 [5700/36045]\tLoss: 599.4139\n",
      "Training Epoch: 31 [5750/36045]\tLoss: 583.8928\n",
      "Training Epoch: 31 [5800/36045]\tLoss: 616.0515\n",
      "Training Epoch: 31 [5850/36045]\tLoss: 603.3378\n",
      "Training Epoch: 31 [5900/36045]\tLoss: 693.9614\n",
      "Training Epoch: 31 [5950/36045]\tLoss: 711.2272\n",
      "Training Epoch: 31 [6000/36045]\tLoss: 696.4176\n",
      "Training Epoch: 31 [6050/36045]\tLoss: 673.4572\n",
      "Training Epoch: 31 [6100/36045]\tLoss: 677.9446\n",
      "Training Epoch: 31 [6150/36045]\tLoss: 664.9910\n",
      "Training Epoch: 31 [6200/36045]\tLoss: 667.4395\n",
      "Training Epoch: 31 [6250/36045]\tLoss: 688.8608\n",
      "Training Epoch: 31 [6300/36045]\tLoss: 700.6084\n",
      "Training Epoch: 31 [6350/36045]\tLoss: 747.7483\n",
      "Training Epoch: 31 [6400/36045]\tLoss: 620.2120\n",
      "Training Epoch: 31 [6450/36045]\tLoss: 572.5060\n",
      "Training Epoch: 31 [6500/36045]\tLoss: 583.0229\n",
      "Training Epoch: 31 [6550/36045]\tLoss: 600.2306\n",
      "Training Epoch: 31 [6600/36045]\tLoss: 599.1970\n",
      "Training Epoch: 31 [6650/36045]\tLoss: 676.0436\n",
      "Training Epoch: 31 [6700/36045]\tLoss: 707.2827\n",
      "Training Epoch: 31 [6750/36045]\tLoss: 683.2098\n",
      "Training Epoch: 31 [6800/36045]\tLoss: 686.1796\n",
      "Training Epoch: 31 [6850/36045]\tLoss: 674.0959\n",
      "Training Epoch: 31 [6900/36045]\tLoss: 600.5174\n",
      "Training Epoch: 31 [6950/36045]\tLoss: 566.1073\n",
      "Training Epoch: 31 [7000/36045]\tLoss: 602.3203\n",
      "Training Epoch: 31 [7050/36045]\tLoss: 615.6255\n",
      "Training Epoch: 31 [7100/36045]\tLoss: 615.0338\n",
      "Training Epoch: 31 [7150/36045]\tLoss: 625.7234\n",
      "Training Epoch: 31 [7200/36045]\tLoss: 629.0863\n",
      "Training Epoch: 31 [7250/36045]\tLoss: 626.8494\n",
      "Training Epoch: 31 [7300/36045]\tLoss: 613.6345\n",
      "Training Epoch: 31 [7350/36045]\tLoss: 610.1896\n",
      "Training Epoch: 31 [7400/36045]\tLoss: 554.0616\n",
      "Training Epoch: 31 [7450/36045]\tLoss: 557.3596\n",
      "Training Epoch: 31 [7500/36045]\tLoss: 552.3806\n",
      "Training Epoch: 31 [7550/36045]\tLoss: 529.2668\n",
      "Training Epoch: 31 [7600/36045]\tLoss: 587.4775\n",
      "Training Epoch: 31 [7650/36045]\tLoss: 629.2737\n",
      "Training Epoch: 31 [7700/36045]\tLoss: 599.1590\n",
      "Training Epoch: 31 [7750/36045]\tLoss: 613.8365\n",
      "Training Epoch: 31 [7800/36045]\tLoss: 602.4357\n",
      "Training Epoch: 31 [7850/36045]\tLoss: 583.0170\n",
      "Training Epoch: 31 [7900/36045]\tLoss: 614.9892\n",
      "Training Epoch: 31 [7950/36045]\tLoss: 612.3367\n",
      "Training Epoch: 31 [8000/36045]\tLoss: 630.4528\n",
      "Training Epoch: 31 [8050/36045]\tLoss: 594.8593\n",
      "Training Epoch: 31 [8100/36045]\tLoss: 620.4518\n",
      "Training Epoch: 31 [8150/36045]\tLoss: 702.4658\n",
      "Training Epoch: 31 [8200/36045]\tLoss: 689.1669\n",
      "Training Epoch: 31 [8250/36045]\tLoss: 656.6340\n",
      "Training Epoch: 31 [8300/36045]\tLoss: 716.0546\n",
      "Training Epoch: 31 [8350/36045]\tLoss: 657.5532\n",
      "Training Epoch: 31 [8400/36045]\tLoss: 589.2007\n",
      "Training Epoch: 31 [8450/36045]\tLoss: 551.9224\n",
      "Training Epoch: 31 [8500/36045]\tLoss: 586.4443\n",
      "Training Epoch: 31 [8550/36045]\tLoss: 578.6979\n",
      "Training Epoch: 31 [8600/36045]\tLoss: 572.5555\n",
      "Training Epoch: 31 [8650/36045]\tLoss: 610.3564\n",
      "Training Epoch: 31 [8700/36045]\tLoss: 645.4025\n",
      "Training Epoch: 31 [8750/36045]\tLoss: 633.7585\n",
      "Training Epoch: 31 [8800/36045]\tLoss: 639.5354\n",
      "Training Epoch: 31 [8850/36045]\tLoss: 632.8296\n",
      "Training Epoch: 31 [8900/36045]\tLoss: 571.2121\n",
      "Training Epoch: 31 [8950/36045]\tLoss: 583.4084\n",
      "Training Epoch: 31 [9000/36045]\tLoss: 599.0419\n",
      "Training Epoch: 31 [9050/36045]\tLoss: 600.2265\n",
      "Training Epoch: 31 [9100/36045]\tLoss: 617.8644\n",
      "Training Epoch: 31 [9150/36045]\tLoss: 456.8477\n",
      "Training Epoch: 31 [9200/36045]\tLoss: 342.2718\n",
      "Training Epoch: 31 [9250/36045]\tLoss: 371.1942\n",
      "Training Epoch: 31 [9300/36045]\tLoss: 381.8446\n",
      "Training Epoch: 31 [9350/36045]\tLoss: 352.0468\n",
      "Training Epoch: 31 [9400/36045]\tLoss: 689.8821\n",
      "Training Epoch: 31 [9450/36045]\tLoss: 732.7861\n",
      "Training Epoch: 31 [9500/36045]\tLoss: 719.8143\n",
      "Training Epoch: 31 [9550/36045]\tLoss: 761.4535\n",
      "Training Epoch: 31 [9600/36045]\tLoss: 565.9738\n",
      "Training Epoch: 31 [9650/36045]\tLoss: 570.4659\n",
      "Training Epoch: 31 [9700/36045]\tLoss: 555.8553\n",
      "Training Epoch: 31 [9750/36045]\tLoss: 555.0233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31 [9800/36045]\tLoss: 725.3667\n",
      "Training Epoch: 31 [9850/36045]\tLoss: 766.1625\n",
      "Training Epoch: 31 [9900/36045]\tLoss: 778.3584\n",
      "Training Epoch: 31 [9950/36045]\tLoss: 758.3688\n",
      "Training Epoch: 31 [10000/36045]\tLoss: 700.8301\n",
      "Training Epoch: 31 [10050/36045]\tLoss: 576.4733\n",
      "Training Epoch: 31 [10100/36045]\tLoss: 583.8013\n",
      "Training Epoch: 31 [10150/36045]\tLoss: 593.0667\n",
      "Training Epoch: 31 [10200/36045]\tLoss: 581.9532\n",
      "Training Epoch: 31 [10250/36045]\tLoss: 696.4035\n",
      "Training Epoch: 31 [10300/36045]\tLoss: 676.4142\n",
      "Training Epoch: 31 [10350/36045]\tLoss: 712.2009\n",
      "Training Epoch: 31 [10400/36045]\tLoss: 702.6546\n",
      "Training Epoch: 31 [10450/36045]\tLoss: 658.2432\n",
      "Training Epoch: 31 [10500/36045]\tLoss: 550.6928\n",
      "Training Epoch: 31 [10550/36045]\tLoss: 545.6984\n",
      "Training Epoch: 31 [10600/36045]\tLoss: 568.3981\n",
      "Training Epoch: 31 [10650/36045]\tLoss: 574.5995\n",
      "Training Epoch: 31 [10700/36045]\tLoss: 658.5252\n",
      "Training Epoch: 31 [10750/36045]\tLoss: 719.4067\n",
      "Training Epoch: 31 [10800/36045]\tLoss: 663.6028\n",
      "Training Epoch: 31 [10850/36045]\tLoss: 702.9989\n",
      "Training Epoch: 31 [10900/36045]\tLoss: 731.6255\n",
      "Training Epoch: 31 [10950/36045]\tLoss: 540.2276\n",
      "Training Epoch: 31 [11000/36045]\tLoss: 533.8558\n",
      "Training Epoch: 31 [11050/36045]\tLoss: 571.8995\n",
      "Training Epoch: 31 [11100/36045]\tLoss: 582.9393\n",
      "Training Epoch: 31 [11150/36045]\tLoss: 632.0081\n",
      "Training Epoch: 31 [11200/36045]\tLoss: 660.0350\n",
      "Training Epoch: 31 [11250/36045]\tLoss: 672.0056\n",
      "Training Epoch: 31 [11300/36045]\tLoss: 652.0493\n",
      "Training Epoch: 31 [11350/36045]\tLoss: 649.3564\n",
      "Training Epoch: 31 [11400/36045]\tLoss: 611.3217\n",
      "Training Epoch: 31 [11450/36045]\tLoss: 579.1700\n",
      "Training Epoch: 31 [11500/36045]\tLoss: 576.6013\n",
      "Training Epoch: 31 [11550/36045]\tLoss: 587.7898\n",
      "Training Epoch: 31 [11600/36045]\tLoss: 649.2433\n",
      "Training Epoch: 31 [11650/36045]\tLoss: 701.4041\n",
      "Training Epoch: 31 [11700/36045]\tLoss: 700.3419\n",
      "Training Epoch: 31 [11750/36045]\tLoss: 719.2794\n",
      "Training Epoch: 31 [11800/36045]\tLoss: 762.1292\n",
      "Training Epoch: 31 [11850/36045]\tLoss: 819.6926\n",
      "Training Epoch: 31 [11900/36045]\tLoss: 1034.8208\n",
      "Training Epoch: 31 [11950/36045]\tLoss: 1037.1267\n",
      "Training Epoch: 31 [12000/36045]\tLoss: 1050.0535\n",
      "Training Epoch: 31 [12050/36045]\tLoss: 1008.6774\n",
      "Training Epoch: 31 [12100/36045]\tLoss: 654.0440\n",
      "Training Epoch: 31 [12150/36045]\tLoss: 498.2527\n",
      "Training Epoch: 31 [12200/36045]\tLoss: 492.8692\n",
      "Training Epoch: 31 [12250/36045]\tLoss: 501.8434\n",
      "Training Epoch: 31 [12300/36045]\tLoss: 642.1065\n",
      "Training Epoch: 31 [12350/36045]\tLoss: 699.0499\n",
      "Training Epoch: 31 [12400/36045]\tLoss: 706.7330\n",
      "Training Epoch: 31 [12450/36045]\tLoss: 695.0926\n",
      "Training Epoch: 31 [12500/36045]\tLoss: 723.1921\n",
      "Training Epoch: 31 [12550/36045]\tLoss: 692.3499\n",
      "Training Epoch: 31 [12600/36045]\tLoss: 636.3519\n",
      "Training Epoch: 31 [12650/36045]\tLoss: 635.0799\n",
      "Training Epoch: 31 [12700/36045]\tLoss: 656.5428\n",
      "Training Epoch: 31 [12750/36045]\tLoss: 655.6015\n",
      "Training Epoch: 31 [12800/36045]\tLoss: 639.5628\n",
      "Training Epoch: 31 [12850/36045]\tLoss: 668.9086\n",
      "Training Epoch: 31 [12900/36045]\tLoss: 641.6299\n",
      "Training Epoch: 31 [12950/36045]\tLoss: 628.5100\n",
      "Training Epoch: 31 [13000/36045]\tLoss: 661.1165\n",
      "Training Epoch: 31 [13050/36045]\tLoss: 599.5578\n",
      "Training Epoch: 31 [13100/36045]\tLoss: 617.4282\n",
      "Training Epoch: 31 [13150/36045]\tLoss: 609.1340\n",
      "Training Epoch: 31 [13200/36045]\tLoss: 590.3950\n",
      "Training Epoch: 31 [13250/36045]\tLoss: 614.2084\n",
      "Training Epoch: 31 [13300/36045]\tLoss: 652.9150\n",
      "Training Epoch: 31 [13350/36045]\tLoss: 632.9070\n",
      "Training Epoch: 31 [13400/36045]\tLoss: 636.4018\n",
      "Training Epoch: 31 [13450/36045]\tLoss: 633.0644\n",
      "Training Epoch: 31 [13500/36045]\tLoss: 653.3607\n",
      "Training Epoch: 31 [13550/36045]\tLoss: 790.0047\n",
      "Training Epoch: 31 [13600/36045]\tLoss: 822.8611\n",
      "Training Epoch: 31 [13650/36045]\tLoss: 903.5609\n",
      "Training Epoch: 31 [13700/36045]\tLoss: 798.2581\n",
      "Training Epoch: 31 [13750/36045]\tLoss: 639.4678\n",
      "Training Epoch: 31 [13800/36045]\tLoss: 611.5999\n",
      "Training Epoch: 31 [13850/36045]\tLoss: 594.4895\n",
      "Training Epoch: 31 [13900/36045]\tLoss: 601.8087\n",
      "Training Epoch: 31 [13950/36045]\tLoss: 647.2657\n",
      "Training Epoch: 31 [14000/36045]\tLoss: 680.9041\n",
      "Training Epoch: 31 [14050/36045]\tLoss: 655.1461\n",
      "Training Epoch: 31 [14100/36045]\tLoss: 650.5947\n",
      "Training Epoch: 31 [14150/36045]\tLoss: 638.4998\n",
      "Training Epoch: 31 [14200/36045]\tLoss: 680.1960\n",
      "Training Epoch: 31 [14250/36045]\tLoss: 746.5213\n",
      "Training Epoch: 31 [14300/36045]\tLoss: 749.8555\n",
      "Training Epoch: 31 [14350/36045]\tLoss: 717.4854\n",
      "Training Epoch: 31 [14400/36045]\tLoss: 703.1270\n",
      "Training Epoch: 31 [14450/36045]\tLoss: 739.6620\n",
      "Training Epoch: 31 [14500/36045]\tLoss: 670.4225\n",
      "Training Epoch: 31 [14550/36045]\tLoss: 700.1853\n",
      "Training Epoch: 31 [14600/36045]\tLoss: 686.0537\n",
      "Training Epoch: 31 [14650/36045]\tLoss: 686.0710\n",
      "Training Epoch: 31 [14700/36045]\tLoss: 649.1096\n",
      "Training Epoch: 31 [14750/36045]\tLoss: 557.7486\n",
      "Training Epoch: 31 [14800/36045]\tLoss: 547.8634\n",
      "Training Epoch: 31 [14850/36045]\tLoss: 554.9321\n",
      "Training Epoch: 31 [14900/36045]\tLoss: 548.4532\n",
      "Training Epoch: 31 [14950/36045]\tLoss: 556.3082\n",
      "Training Epoch: 31 [15000/36045]\tLoss: 570.3441\n",
      "Training Epoch: 31 [15050/36045]\tLoss: 567.7977\n",
      "Training Epoch: 31 [15100/36045]\tLoss: 551.8702\n",
      "Training Epoch: 31 [15150/36045]\tLoss: 546.2027\n",
      "Training Epoch: 31 [15200/36045]\tLoss: 505.6082\n",
      "Training Epoch: 31 [15250/36045]\tLoss: 528.4556\n",
      "Training Epoch: 31 [15300/36045]\tLoss: 513.4349\n",
      "Training Epoch: 31 [15350/36045]\tLoss: 525.4933\n",
      "Training Epoch: 31 [15400/36045]\tLoss: 509.3303\n",
      "Training Epoch: 31 [15450/36045]\tLoss: 494.8069\n",
      "Training Epoch: 31 [15500/36045]\tLoss: 509.1873\n",
      "Training Epoch: 31 [15550/36045]\tLoss: 505.0250\n",
      "Training Epoch: 31 [15600/36045]\tLoss: 574.2759\n",
      "Training Epoch: 31 [15650/36045]\tLoss: 592.2089\n",
      "Training Epoch: 31 [15700/36045]\tLoss: 583.5093\n",
      "Training Epoch: 31 [15750/36045]\tLoss: 575.2052\n",
      "Training Epoch: 31 [15800/36045]\tLoss: 545.3818\n",
      "Training Epoch: 31 [15850/36045]\tLoss: 559.7927\n",
      "Training Epoch: 31 [15900/36045]\tLoss: 569.2109\n",
      "Training Epoch: 31 [15950/36045]\tLoss: 588.9943\n",
      "Training Epoch: 31 [16000/36045]\tLoss: 561.7484\n",
      "Training Epoch: 31 [16050/36045]\tLoss: 531.0974\n",
      "Training Epoch: 31 [16100/36045]\tLoss: 492.2768\n",
      "Training Epoch: 31 [16150/36045]\tLoss: 480.1004\n",
      "Training Epoch: 31 [16200/36045]\tLoss: 581.7213\n",
      "Training Epoch: 31 [16250/36045]\tLoss: 610.2673\n",
      "Training Epoch: 31 [16300/36045]\tLoss: 666.2693\n",
      "Training Epoch: 31 [16350/36045]\tLoss: 685.4680\n",
      "Training Epoch: 31 [16400/36045]\tLoss: 657.3390\n",
      "Training Epoch: 31 [16450/36045]\tLoss: 639.1346\n",
      "Training Epoch: 31 [16500/36045]\tLoss: 638.9656\n",
      "Training Epoch: 31 [16550/36045]\tLoss: 603.6067\n",
      "Training Epoch: 31 [16600/36045]\tLoss: 627.4419\n",
      "Training Epoch: 31 [16650/36045]\tLoss: 645.1455\n",
      "Training Epoch: 31 [16700/36045]\tLoss: 623.4037\n",
      "Training Epoch: 31 [16750/36045]\tLoss: 615.7820\n",
      "Training Epoch: 31 [16800/36045]\tLoss: 626.0747\n",
      "Training Epoch: 31 [16850/36045]\tLoss: 596.2903\n",
      "Training Epoch: 31 [16900/36045]\tLoss: 606.4008\n",
      "Training Epoch: 31 [16950/36045]\tLoss: 630.4411\n",
      "Training Epoch: 31 [17000/36045]\tLoss: 613.5709\n",
      "Training Epoch: 31 [17050/36045]\tLoss: 639.8025\n",
      "Training Epoch: 31 [17100/36045]\tLoss: 636.2135\n",
      "Training Epoch: 31 [17150/36045]\tLoss: 553.1502\n",
      "Training Epoch: 31 [17200/36045]\tLoss: 514.3853\n",
      "Training Epoch: 31 [17250/36045]\tLoss: 538.3311\n",
      "Training Epoch: 31 [17300/36045]\tLoss: 569.5159\n",
      "Training Epoch: 31 [17350/36045]\tLoss: 547.8427\n",
      "Training Epoch: 31 [17400/36045]\tLoss: 567.2452\n",
      "Training Epoch: 31 [17450/36045]\tLoss: 586.7997\n",
      "Training Epoch: 31 [17500/36045]\tLoss: 574.8721\n",
      "Training Epoch: 31 [17550/36045]\tLoss: 574.0161\n",
      "Training Epoch: 31 [17600/36045]\tLoss: 567.4977\n",
      "Training Epoch: 31 [17650/36045]\tLoss: 584.0547\n",
      "Training Epoch: 31 [17700/36045]\tLoss: 562.9655\n",
      "Training Epoch: 31 [17750/36045]\tLoss: 579.6985\n",
      "Training Epoch: 31 [17800/36045]\tLoss: 570.4460\n",
      "Training Epoch: 31 [17850/36045]\tLoss: 582.2509\n",
      "Training Epoch: 31 [17900/36045]\tLoss: 610.8148\n",
      "Training Epoch: 31 [17950/36045]\tLoss: 622.6138\n",
      "Training Epoch: 31 [18000/36045]\tLoss: 612.8690\n",
      "Training Epoch: 31 [18050/36045]\tLoss: 676.9203\n",
      "Training Epoch: 31 [18100/36045]\tLoss: 678.8378\n",
      "Training Epoch: 31 [18150/36045]\tLoss: 690.0900\n",
      "Training Epoch: 31 [18200/36045]\tLoss: 672.0870\n",
      "Training Epoch: 31 [18250/36045]\tLoss: 692.9675\n",
      "Training Epoch: 31 [18300/36045]\tLoss: 644.1290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31 [18350/36045]\tLoss: 715.4225\n",
      "Training Epoch: 31 [18400/36045]\tLoss: 688.6669\n",
      "Training Epoch: 31 [18450/36045]\tLoss: 668.6749\n",
      "Training Epoch: 31 [18500/36045]\tLoss: 667.6664\n",
      "Training Epoch: 31 [18550/36045]\tLoss: 654.7623\n",
      "Training Epoch: 31 [18600/36045]\tLoss: 644.1777\n",
      "Training Epoch: 31 [18650/36045]\tLoss: 690.7390\n",
      "Training Epoch: 31 [18700/36045]\tLoss: 726.7952\n",
      "Training Epoch: 31 [18750/36045]\tLoss: 713.3944\n",
      "Training Epoch: 31 [18800/36045]\tLoss: 737.1234\n",
      "Training Epoch: 31 [18850/36045]\tLoss: 681.5210\n",
      "Training Epoch: 31 [18900/36045]\tLoss: 729.0636\n",
      "Training Epoch: 31 [18950/36045]\tLoss: 670.7785\n",
      "Training Epoch: 31 [19000/36045]\tLoss: 558.8234\n",
      "Training Epoch: 31 [19050/36045]\tLoss: 542.1097\n",
      "Training Epoch: 31 [19100/36045]\tLoss: 550.7086\n",
      "Training Epoch: 31 [19150/36045]\tLoss: 540.2999\n",
      "Training Epoch: 31 [19200/36045]\tLoss: 569.5501\n",
      "Training Epoch: 31 [19250/36045]\tLoss: 584.2219\n",
      "Training Epoch: 31 [19300/36045]\tLoss: 594.4636\n",
      "Training Epoch: 31 [19350/36045]\tLoss: 577.9801\n",
      "Training Epoch: 31 [19400/36045]\tLoss: 599.5041\n",
      "Training Epoch: 31 [19450/36045]\tLoss: 590.5167\n",
      "Training Epoch: 31 [19500/36045]\tLoss: 592.1035\n",
      "Training Epoch: 31 [19550/36045]\tLoss: 590.6148\n",
      "Training Epoch: 31 [19600/36045]\tLoss: 631.6895\n",
      "Training Epoch: 31 [19650/36045]\tLoss: 836.9860\n",
      "Training Epoch: 31 [19700/36045]\tLoss: 795.8062\n",
      "Training Epoch: 31 [19750/36045]\tLoss: 799.0733\n",
      "Training Epoch: 31 [19800/36045]\tLoss: 798.5205\n",
      "Training Epoch: 31 [19850/36045]\tLoss: 528.8354\n",
      "Training Epoch: 31 [19900/36045]\tLoss: 507.0993\n",
      "Training Epoch: 31 [19950/36045]\tLoss: 510.7011\n",
      "Training Epoch: 31 [20000/36045]\tLoss: 509.6670\n",
      "Training Epoch: 31 [20050/36045]\tLoss: 570.7715\n",
      "Training Epoch: 31 [20100/36045]\tLoss: 576.9891\n",
      "Training Epoch: 31 [20150/36045]\tLoss: 578.9238\n",
      "Training Epoch: 31 [20200/36045]\tLoss: 578.8018\n",
      "Training Epoch: 31 [20250/36045]\tLoss: 616.4130\n",
      "Training Epoch: 31 [20300/36045]\tLoss: 652.4293\n",
      "Training Epoch: 31 [20350/36045]\tLoss: 671.3571\n",
      "Training Epoch: 31 [20400/36045]\tLoss: 688.0671\n",
      "Training Epoch: 31 [20450/36045]\tLoss: 658.5973\n",
      "Training Epoch: 31 [20500/36045]\tLoss: 642.6187\n",
      "Training Epoch: 31 [20550/36045]\tLoss: 565.3322\n",
      "Training Epoch: 31 [20600/36045]\tLoss: 576.0086\n",
      "Training Epoch: 31 [20650/36045]\tLoss: 572.9845\n",
      "Training Epoch: 31 [20700/36045]\tLoss: 560.8965\n",
      "Training Epoch: 31 [20750/36045]\tLoss: 603.9961\n",
      "Training Epoch: 31 [20800/36045]\tLoss: 656.4501\n",
      "Training Epoch: 31 [20850/36045]\tLoss: 643.1407\n",
      "Training Epoch: 31 [20900/36045]\tLoss: 687.9808\n",
      "Training Epoch: 31 [20950/36045]\tLoss: 648.8019\n",
      "Training Epoch: 31 [21000/36045]\tLoss: 611.1279\n",
      "Training Epoch: 31 [21050/36045]\tLoss: 523.2737\n",
      "Training Epoch: 31 [21100/36045]\tLoss: 527.1149\n",
      "Training Epoch: 31 [21150/36045]\tLoss: 564.1601\n",
      "Training Epoch: 31 [21200/36045]\tLoss: 563.3704\n",
      "Training Epoch: 31 [21250/36045]\tLoss: 539.0414\n",
      "Training Epoch: 31 [21300/36045]\tLoss: 629.4619\n",
      "Training Epoch: 31 [21350/36045]\tLoss: 621.6815\n",
      "Training Epoch: 31 [21400/36045]\tLoss: 625.1074\n",
      "Training Epoch: 31 [21450/36045]\tLoss: 631.3341\n",
      "Training Epoch: 31 [21500/36045]\tLoss: 633.9322\n",
      "Training Epoch: 31 [21550/36045]\tLoss: 728.9086\n",
      "Training Epoch: 31 [21600/36045]\tLoss: 728.1680\n",
      "Training Epoch: 31 [21650/36045]\tLoss: 740.8239\n",
      "Training Epoch: 31 [21700/36045]\tLoss: 742.9512\n",
      "Training Epoch: 31 [21750/36045]\tLoss: 713.7172\n",
      "Training Epoch: 31 [21800/36045]\tLoss: 526.6701\n",
      "Training Epoch: 31 [21850/36045]\tLoss: 509.6603\n",
      "Training Epoch: 31 [21900/36045]\tLoss: 519.7942\n",
      "Training Epoch: 31 [21950/36045]\tLoss: 519.7234\n",
      "Training Epoch: 31 [22000/36045]\tLoss: 523.2912\n",
      "Training Epoch: 31 [22050/36045]\tLoss: 545.7193\n",
      "Training Epoch: 31 [22100/36045]\tLoss: 538.2950\n",
      "Training Epoch: 31 [22150/36045]\tLoss: 523.5765\n",
      "Training Epoch: 31 [22200/36045]\tLoss: 540.3842\n",
      "Training Epoch: 31 [22250/36045]\tLoss: 545.3745\n",
      "Training Epoch: 31 [22300/36045]\tLoss: 597.8986\n",
      "Training Epoch: 31 [22350/36045]\tLoss: 623.9390\n",
      "Training Epoch: 31 [22400/36045]\tLoss: 638.7565\n",
      "Training Epoch: 31 [22450/36045]\tLoss: 626.3817\n",
      "Training Epoch: 31 [22500/36045]\tLoss: 608.4174\n",
      "Training Epoch: 31 [22550/36045]\tLoss: 644.3977\n",
      "Training Epoch: 31 [22600/36045]\tLoss: 698.3376\n",
      "Training Epoch: 31 [22650/36045]\tLoss: 733.4181\n",
      "Training Epoch: 31 [22700/36045]\tLoss: 756.0689\n",
      "Training Epoch: 31 [22750/36045]\tLoss: 775.7158\n",
      "Training Epoch: 31 [22800/36045]\tLoss: 806.4604\n",
      "Training Epoch: 31 [22850/36045]\tLoss: 670.8035\n",
      "Training Epoch: 31 [22900/36045]\tLoss: 675.5552\n",
      "Training Epoch: 31 [22950/36045]\tLoss: 654.8715\n",
      "Training Epoch: 31 [23000/36045]\tLoss: 652.3432\n",
      "Training Epoch: 31 [23050/36045]\tLoss: 579.7859\n",
      "Training Epoch: 31 [23100/36045]\tLoss: 595.7578\n",
      "Training Epoch: 31 [23150/36045]\tLoss: 583.9355\n",
      "Training Epoch: 31 [23200/36045]\tLoss: 553.0851\n",
      "Training Epoch: 31 [23250/36045]\tLoss: 556.1009\n",
      "Training Epoch: 31 [23300/36045]\tLoss: 552.5515\n",
      "Training Epoch: 31 [23350/36045]\tLoss: 573.6931\n",
      "Training Epoch: 31 [23400/36045]\tLoss: 621.6458\n",
      "Training Epoch: 31 [23450/36045]\tLoss: 614.6928\n",
      "Training Epoch: 31 [23500/36045]\tLoss: 592.5325\n",
      "Training Epoch: 31 [23550/36045]\tLoss: 635.4305\n",
      "Training Epoch: 31 [23600/36045]\tLoss: 718.2925\n",
      "Training Epoch: 31 [23650/36045]\tLoss: 731.2949\n",
      "Training Epoch: 31 [23700/36045]\tLoss: 739.7429\n",
      "Training Epoch: 31 [23750/36045]\tLoss: 715.0262\n",
      "Training Epoch: 31 [23800/36045]\tLoss: 571.3700\n",
      "Training Epoch: 31 [23850/36045]\tLoss: 597.7737\n",
      "Training Epoch: 31 [23900/36045]\tLoss: 587.3497\n",
      "Training Epoch: 31 [23950/36045]\tLoss: 570.3292\n",
      "Training Epoch: 31 [24000/36045]\tLoss: 546.8832\n",
      "Training Epoch: 31 [24050/36045]\tLoss: 505.3571\n",
      "Training Epoch: 31 [24100/36045]\tLoss: 531.6481\n",
      "Training Epoch: 31 [24150/36045]\tLoss: 524.8882\n",
      "Training Epoch: 31 [24200/36045]\tLoss: 521.2798\n",
      "Training Epoch: 31 [24250/36045]\tLoss: 505.9945\n",
      "Training Epoch: 31 [24300/36045]\tLoss: 546.4855\n",
      "Training Epoch: 31 [24350/36045]\tLoss: 559.8094\n",
      "Training Epoch: 31 [24400/36045]\tLoss: 575.5157\n",
      "Training Epoch: 31 [24450/36045]\tLoss: 548.6799\n",
      "Training Epoch: 31 [24500/36045]\tLoss: 578.4337\n",
      "Training Epoch: 31 [24550/36045]\tLoss: 667.9600\n",
      "Training Epoch: 31 [24600/36045]\tLoss: 659.8562\n",
      "Training Epoch: 31 [24650/36045]\tLoss: 632.6907\n",
      "Training Epoch: 31 [24700/36045]\tLoss: 642.7056\n",
      "Training Epoch: 31 [24750/36045]\tLoss: 593.7989\n",
      "Training Epoch: 31 [24800/36045]\tLoss: 490.3176\n",
      "Training Epoch: 31 [24850/36045]\tLoss: 509.7504\n",
      "Training Epoch: 31 [24900/36045]\tLoss: 506.8798\n",
      "Training Epoch: 31 [24950/36045]\tLoss: 509.4689\n",
      "Training Epoch: 31 [25000/36045]\tLoss: 489.7772\n",
      "Training Epoch: 31 [25050/36045]\tLoss: 467.7850\n",
      "Training Epoch: 31 [25100/36045]\tLoss: 419.2467\n",
      "Training Epoch: 31 [25150/36045]\tLoss: 388.3006\n",
      "Training Epoch: 31 [25200/36045]\tLoss: 383.3620\n",
      "Training Epoch: 31 [25250/36045]\tLoss: 410.8475\n",
      "Training Epoch: 31 [25300/36045]\tLoss: 539.5459\n",
      "Training Epoch: 31 [25350/36045]\tLoss: 536.3422\n",
      "Training Epoch: 31 [25400/36045]\tLoss: 500.1018\n",
      "Training Epoch: 31 [25450/36045]\tLoss: 502.6390\n",
      "Training Epoch: 31 [25500/36045]\tLoss: 546.1735\n",
      "Training Epoch: 31 [25550/36045]\tLoss: 637.8761\n",
      "Training Epoch: 31 [25600/36045]\tLoss: 642.4293\n",
      "Training Epoch: 31 [25650/36045]\tLoss: 619.9320\n",
      "Training Epoch: 31 [25700/36045]\tLoss: 629.0551\n",
      "Training Epoch: 31 [25750/36045]\tLoss: 606.4365\n",
      "Training Epoch: 31 [25800/36045]\tLoss: 382.4936\n",
      "Training Epoch: 31 [25850/36045]\tLoss: 392.1221\n",
      "Training Epoch: 31 [25900/36045]\tLoss: 373.1281\n",
      "Training Epoch: 31 [25950/36045]\tLoss: 381.9117\n",
      "Training Epoch: 31 [26000/36045]\tLoss: 468.1896\n",
      "Training Epoch: 31 [26050/36045]\tLoss: 637.4228\n",
      "Training Epoch: 31 [26100/36045]\tLoss: 665.0227\n",
      "Training Epoch: 31 [26150/36045]\tLoss: 665.3602\n",
      "Training Epoch: 31 [26200/36045]\tLoss: 638.4697\n",
      "Training Epoch: 31 [26250/36045]\tLoss: 668.5484\n",
      "Training Epoch: 31 [26300/36045]\tLoss: 605.2313\n",
      "Training Epoch: 31 [26350/36045]\tLoss: 616.0126\n",
      "Training Epoch: 31 [26400/36045]\tLoss: 593.2521\n",
      "Training Epoch: 31 [26450/36045]\tLoss: 523.0317\n",
      "Training Epoch: 31 [26500/36045]\tLoss: 621.3459\n",
      "Training Epoch: 31 [26550/36045]\tLoss: 622.2593\n",
      "Training Epoch: 31 [26600/36045]\tLoss: 618.3716\n",
      "Training Epoch: 31 [26650/36045]\tLoss: 634.5752\n",
      "Training Epoch: 31 [26700/36045]\tLoss: 614.1622\n",
      "Training Epoch: 31 [26750/36045]\tLoss: 575.1671\n",
      "Training Epoch: 31 [26800/36045]\tLoss: 423.6940\n",
      "Training Epoch: 31 [26850/36045]\tLoss: 351.3977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31 [26900/36045]\tLoss: 354.3442\n",
      "Training Epoch: 31 [26950/36045]\tLoss: 389.5402\n",
      "Training Epoch: 31 [27000/36045]\tLoss: 635.1617\n",
      "Training Epoch: 31 [27050/36045]\tLoss: 663.7521\n",
      "Training Epoch: 31 [27100/36045]\tLoss: 643.0136\n",
      "Training Epoch: 31 [27150/36045]\tLoss: 683.5088\n",
      "Training Epoch: 31 [27200/36045]\tLoss: 499.7050\n",
      "Training Epoch: 31 [27250/36045]\tLoss: 491.6753\n",
      "Training Epoch: 31 [27300/36045]\tLoss: 478.8737\n",
      "Training Epoch: 31 [27350/36045]\tLoss: 477.4584\n",
      "Training Epoch: 31 [27400/36045]\tLoss: 476.5949\n",
      "Training Epoch: 31 [27450/36045]\tLoss: 601.9006\n",
      "Training Epoch: 31 [27500/36045]\tLoss: 645.0627\n",
      "Training Epoch: 31 [27550/36045]\tLoss: 637.9457\n",
      "Training Epoch: 31 [27600/36045]\tLoss: 649.6804\n",
      "Training Epoch: 31 [27650/36045]\tLoss: 641.4632\n",
      "Training Epoch: 31 [27700/36045]\tLoss: 670.5591\n",
      "Training Epoch: 31 [27750/36045]\tLoss: 682.6991\n",
      "Training Epoch: 31 [27800/36045]\tLoss: 669.3328\n",
      "Training Epoch: 31 [27850/36045]\tLoss: 658.9237\n",
      "Training Epoch: 31 [27900/36045]\tLoss: 595.6882\n",
      "Training Epoch: 31 [27950/36045]\tLoss: 495.8204\n",
      "Training Epoch: 31 [28000/36045]\tLoss: 472.1956\n",
      "Training Epoch: 31 [28050/36045]\tLoss: 482.4501\n",
      "Training Epoch: 31 [28100/36045]\tLoss: 474.0968\n",
      "Training Epoch: 31 [28150/36045]\tLoss: 497.5224\n",
      "Training Epoch: 31 [28200/36045]\tLoss: 504.5334\n",
      "Training Epoch: 31 [28250/36045]\tLoss: 497.8580\n",
      "Training Epoch: 31 [28300/36045]\tLoss: 472.4741\n",
      "Training Epoch: 31 [28350/36045]\tLoss: 468.6259\n",
      "Training Epoch: 31 [28400/36045]\tLoss: 799.7518\n",
      "Training Epoch: 31 [28450/36045]\tLoss: 732.1585\n",
      "Training Epoch: 31 [28500/36045]\tLoss: 632.7073\n",
      "Training Epoch: 31 [28550/36045]\tLoss: 581.0545\n",
      "Training Epoch: 31 [28600/36045]\tLoss: 612.2704\n",
      "Training Epoch: 31 [28650/36045]\tLoss: 678.2833\n",
      "Training Epoch: 31 [28700/36045]\tLoss: 672.5649\n",
      "Training Epoch: 31 [28750/36045]\tLoss: 659.0276\n",
      "Training Epoch: 31 [28800/36045]\tLoss: 667.7509\n",
      "Training Epoch: 31 [28850/36045]\tLoss: 578.9420\n",
      "Training Epoch: 31 [28900/36045]\tLoss: 469.4500\n",
      "Training Epoch: 31 [28950/36045]\tLoss: 468.3446\n",
      "Training Epoch: 31 [29000/36045]\tLoss: 465.6951\n",
      "Training Epoch: 31 [29050/36045]\tLoss: 472.6824\n",
      "Training Epoch: 31 [29100/36045]\tLoss: 491.6472\n",
      "Training Epoch: 31 [29150/36045]\tLoss: 480.2661\n",
      "Training Epoch: 31 [29200/36045]\tLoss: 465.8199\n",
      "Training Epoch: 31 [29250/36045]\tLoss: 455.2803\n",
      "Training Epoch: 31 [29300/36045]\tLoss: 516.6146\n",
      "Training Epoch: 31 [29350/36045]\tLoss: 609.5527\n",
      "Training Epoch: 31 [29400/36045]\tLoss: 627.2316\n",
      "Training Epoch: 31 [29450/36045]\tLoss: 645.5082\n",
      "Training Epoch: 31 [29500/36045]\tLoss: 660.0897\n",
      "Training Epoch: 31 [29550/36045]\tLoss: 628.1437\n",
      "Training Epoch: 31 [29600/36045]\tLoss: 530.1602\n",
      "Training Epoch: 31 [29650/36045]\tLoss: 512.5997\n",
      "Training Epoch: 31 [29700/36045]\tLoss: 458.0250\n",
      "Training Epoch: 31 [29750/36045]\tLoss: 457.0320\n",
      "Training Epoch: 31 [29800/36045]\tLoss: 503.7845\n",
      "Training Epoch: 31 [29850/36045]\tLoss: 579.4523\n",
      "Training Epoch: 31 [29900/36045]\tLoss: 576.1018\n",
      "Training Epoch: 31 [29950/36045]\tLoss: 598.1803\n",
      "Training Epoch: 31 [30000/36045]\tLoss: 573.0271\n",
      "Training Epoch: 31 [30050/36045]\tLoss: 579.1934\n",
      "Training Epoch: 31 [30100/36045]\tLoss: 706.6324\n",
      "Training Epoch: 31 [30150/36045]\tLoss: 690.1622\n",
      "Training Epoch: 31 [30200/36045]\tLoss: 651.1919\n",
      "Training Epoch: 31 [30250/36045]\tLoss: 699.9946\n",
      "Training Epoch: 31 [30300/36045]\tLoss: 684.9949\n",
      "Training Epoch: 31 [30350/36045]\tLoss: 530.0298\n",
      "Training Epoch: 31 [30400/36045]\tLoss: 514.5604\n",
      "Training Epoch: 31 [30450/36045]\tLoss: 516.1154\n",
      "Training Epoch: 31 [30500/36045]\tLoss: 482.1911\n",
      "Training Epoch: 31 [30550/36045]\tLoss: 446.9235\n",
      "Training Epoch: 31 [30600/36045]\tLoss: 437.3217\n",
      "Training Epoch: 31 [30650/36045]\tLoss: 427.2816\n",
      "Training Epoch: 31 [30700/36045]\tLoss: 445.1089\n",
      "Training Epoch: 31 [30750/36045]\tLoss: 431.8546\n",
      "Training Epoch: 31 [30800/36045]\tLoss: 458.7202\n",
      "Training Epoch: 31 [30850/36045]\tLoss: 450.4466\n",
      "Training Epoch: 31 [30900/36045]\tLoss: 463.1237\n",
      "Training Epoch: 31 [30950/36045]\tLoss: 486.5403\n",
      "Training Epoch: 31 [31000/36045]\tLoss: 478.2578\n",
      "Training Epoch: 31 [31050/36045]\tLoss: 399.6226\n",
      "Training Epoch: 31 [31100/36045]\tLoss: 390.2131\n",
      "Training Epoch: 31 [31150/36045]\tLoss: 397.4518\n",
      "Training Epoch: 31 [31200/36045]\tLoss: 494.7767\n",
      "Training Epoch: 31 [31250/36045]\tLoss: 642.0016\n",
      "Training Epoch: 31 [31300/36045]\tLoss: 612.3528\n",
      "Training Epoch: 31 [31350/36045]\tLoss: 628.2491\n",
      "Training Epoch: 31 [31400/36045]\tLoss: 607.6206\n",
      "Training Epoch: 31 [31450/36045]\tLoss: 623.6700\n",
      "Training Epoch: 31 [31500/36045]\tLoss: 636.0148\n",
      "Training Epoch: 31 [31550/36045]\tLoss: 643.7634\n",
      "Training Epoch: 31 [31600/36045]\tLoss: 605.1501\n",
      "Training Epoch: 31 [31650/36045]\tLoss: 646.9934\n",
      "Training Epoch: 31 [31700/36045]\tLoss: 468.9499\n",
      "Training Epoch: 31 [31750/36045]\tLoss: 388.1176\n",
      "Training Epoch: 31 [31800/36045]\tLoss: 370.1229\n",
      "Training Epoch: 31 [31850/36045]\tLoss: 378.9508\n",
      "Training Epoch: 31 [31900/36045]\tLoss: 594.8466\n",
      "Training Epoch: 31 [31950/36045]\tLoss: 768.0565\n",
      "Training Epoch: 31 [32000/36045]\tLoss: 878.2230\n",
      "Training Epoch: 31 [32050/36045]\tLoss: 832.8923\n",
      "Training Epoch: 31 [32100/36045]\tLoss: 823.0402\n",
      "Training Epoch: 31 [32150/36045]\tLoss: 638.3472\n",
      "Training Epoch: 31 [32200/36045]\tLoss: 641.8008\n",
      "Training Epoch: 31 [32250/36045]\tLoss: 652.4439\n",
      "Training Epoch: 31 [32300/36045]\tLoss: 634.1462\n",
      "Training Epoch: 31 [32350/36045]\tLoss: 629.5276\n",
      "Training Epoch: 31 [32400/36045]\tLoss: 590.7709\n",
      "Training Epoch: 31 [32450/36045]\tLoss: 486.6400\n",
      "Training Epoch: 31 [32500/36045]\tLoss: 467.7960\n",
      "Training Epoch: 31 [32550/36045]\tLoss: 470.1751\n",
      "Training Epoch: 31 [32600/36045]\tLoss: 466.8632\n",
      "Training Epoch: 31 [32650/36045]\tLoss: 599.9244\n",
      "Training Epoch: 31 [32700/36045]\tLoss: 654.5312\n",
      "Training Epoch: 31 [32750/36045]\tLoss: 623.7596\n",
      "Training Epoch: 31 [32800/36045]\tLoss: 639.7985\n",
      "Training Epoch: 31 [32850/36045]\tLoss: 590.6838\n",
      "Training Epoch: 31 [32900/36045]\tLoss: 474.1143\n",
      "Training Epoch: 31 [32950/36045]\tLoss: 496.2646\n",
      "Training Epoch: 31 [33000/36045]\tLoss: 495.6852\n",
      "Training Epoch: 31 [33050/36045]\tLoss: 470.8418\n",
      "Training Epoch: 31 [33100/36045]\tLoss: 535.1016\n",
      "Training Epoch: 31 [33150/36045]\tLoss: 726.4427\n",
      "Training Epoch: 31 [33200/36045]\tLoss: 707.5298\n",
      "Training Epoch: 31 [33250/36045]\tLoss: 729.0624\n",
      "Training Epoch: 31 [33300/36045]\tLoss: 776.3759\n",
      "Training Epoch: 31 [33350/36045]\tLoss: 595.4894\n",
      "Training Epoch: 31 [33400/36045]\tLoss: 436.6074\n",
      "Training Epoch: 31 [33450/36045]\tLoss: 431.8465\n",
      "Training Epoch: 31 [33500/36045]\tLoss: 444.5125\n",
      "Training Epoch: 31 [33550/36045]\tLoss: 460.8265\n",
      "Training Epoch: 31 [33600/36045]\tLoss: 462.6021\n",
      "Training Epoch: 31 [33650/36045]\tLoss: 617.0430\n",
      "Training Epoch: 31 [33700/36045]\tLoss: 596.9982\n",
      "Training Epoch: 31 [33750/36045]\tLoss: 618.1376\n",
      "Training Epoch: 31 [33800/36045]\tLoss: 614.0600\n",
      "Training Epoch: 31 [33850/36045]\tLoss: 616.4382\n",
      "Training Epoch: 31 [33900/36045]\tLoss: 629.0370\n",
      "Training Epoch: 31 [33950/36045]\tLoss: 639.8004\n",
      "Training Epoch: 31 [34000/36045]\tLoss: 626.4469\n",
      "Training Epoch: 31 [34050/36045]\tLoss: 630.6272\n",
      "Training Epoch: 31 [34100/36045]\tLoss: 607.1708\n",
      "Training Epoch: 31 [34150/36045]\tLoss: 563.8510\n",
      "Training Epoch: 31 [34200/36045]\tLoss: 533.9218\n",
      "Training Epoch: 31 [34250/36045]\tLoss: 548.3466\n",
      "Training Epoch: 31 [34300/36045]\tLoss: 468.9349\n",
      "Training Epoch: 31 [34350/36045]\tLoss: 493.8614\n",
      "Training Epoch: 31 [34400/36045]\tLoss: 485.5367\n",
      "Training Epoch: 31 [34450/36045]\tLoss: 456.4943\n",
      "Training Epoch: 31 [34500/36045]\tLoss: 487.2053\n",
      "Training Epoch: 31 [34550/36045]\tLoss: 478.1525\n",
      "Training Epoch: 31 [34600/36045]\tLoss: 482.5529\n",
      "Training Epoch: 31 [34650/36045]\tLoss: 589.8997\n",
      "Training Epoch: 31 [34700/36045]\tLoss: 624.9115\n",
      "Training Epoch: 31 [34750/36045]\tLoss: 554.2902\n",
      "Training Epoch: 31 [34800/36045]\tLoss: 634.7931\n",
      "Training Epoch: 31 [34850/36045]\tLoss: 642.7299\n",
      "Training Epoch: 31 [34900/36045]\tLoss: 702.5369\n",
      "Training Epoch: 31 [34950/36045]\tLoss: 688.6241\n",
      "Training Epoch: 31 [35000/36045]\tLoss: 689.4938\n",
      "Training Epoch: 31 [35050/36045]\tLoss: 675.7088\n",
      "Training Epoch: 31 [35100/36045]\tLoss: 574.4196\n",
      "Training Epoch: 31 [35150/36045]\tLoss: 567.4115\n",
      "Training Epoch: 31 [35200/36045]\tLoss: 479.8954\n",
      "Training Epoch: 31 [35250/36045]\tLoss: 526.9158\n",
      "Training Epoch: 31 [35300/36045]\tLoss: 541.5226\n",
      "Training Epoch: 31 [35350/36045]\tLoss: 611.4894\n",
      "Training Epoch: 31 [35400/36045]\tLoss: 645.2178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31 [35450/36045]\tLoss: 615.6754\n",
      "Training Epoch: 31 [35500/36045]\tLoss: 596.9943\n",
      "Training Epoch: 31 [35550/36045]\tLoss: 582.2960\n",
      "Training Epoch: 31 [35600/36045]\tLoss: 630.7847\n",
      "Training Epoch: 31 [35650/36045]\tLoss: 704.2736\n",
      "Training Epoch: 31 [35700/36045]\tLoss: 631.7131\n",
      "Training Epoch: 31 [35750/36045]\tLoss: 689.6546\n",
      "Training Epoch: 31 [35800/36045]\tLoss: 696.1191\n",
      "Training Epoch: 31 [35850/36045]\tLoss: 670.0596\n",
      "Training Epoch: 31 [35900/36045]\tLoss: 695.0294\n",
      "Training Epoch: 31 [35950/36045]\tLoss: 692.5760\n",
      "Training Epoch: 31 [36000/36045]\tLoss: 684.9902\n",
      "Training Epoch: 31 [36045/36045]\tLoss: 668.8635\n",
      "Training Epoch: 31 [4004/4004]\tLoss: 622.3419\n",
      "Training Epoch: 32 [50/36045]\tLoss: 621.6226\n",
      "Training Epoch: 32 [100/36045]\tLoss: 596.0941\n",
      "Training Epoch: 32 [150/36045]\tLoss: 594.3623\n",
      "Training Epoch: 32 [200/36045]\tLoss: 581.0396\n",
      "Training Epoch: 32 [250/36045]\tLoss: 694.4114\n",
      "Training Epoch: 32 [300/36045]\tLoss: 758.5715\n",
      "Training Epoch: 32 [350/36045]\tLoss: 724.5515\n",
      "Training Epoch: 32 [400/36045]\tLoss: 719.9293\n",
      "Training Epoch: 32 [450/36045]\tLoss: 700.2124\n",
      "Training Epoch: 32 [500/36045]\tLoss: 650.5818\n",
      "Training Epoch: 32 [550/36045]\tLoss: 654.0577\n",
      "Training Epoch: 32 [600/36045]\tLoss: 636.6304\n",
      "Training Epoch: 32 [650/36045]\tLoss: 659.5188\n",
      "Training Epoch: 32 [700/36045]\tLoss: 646.1293\n",
      "Training Epoch: 32 [750/36045]\tLoss: 625.9545\n",
      "Training Epoch: 32 [800/36045]\tLoss: 639.1638\n",
      "Training Epoch: 32 [850/36045]\tLoss: 620.4779\n",
      "Training Epoch: 32 [900/36045]\tLoss: 592.2803\n",
      "Training Epoch: 32 [950/36045]\tLoss: 561.0780\n",
      "Training Epoch: 32 [1000/36045]\tLoss: 542.4120\n",
      "Training Epoch: 32 [1050/36045]\tLoss: 544.5453\n",
      "Training Epoch: 32 [1100/36045]\tLoss: 529.8375\n",
      "Training Epoch: 32 [1150/36045]\tLoss: 538.5618\n",
      "Training Epoch: 32 [1200/36045]\tLoss: 569.0092\n",
      "Training Epoch: 32 [1250/36045]\tLoss: 651.3508\n",
      "Training Epoch: 32 [1300/36045]\tLoss: 658.2180\n",
      "Training Epoch: 32 [1350/36045]\tLoss: 660.2911\n",
      "Training Epoch: 32 [1400/36045]\tLoss: 686.0266\n",
      "Training Epoch: 32 [1450/36045]\tLoss: 663.4407\n",
      "Training Epoch: 32 [1500/36045]\tLoss: 607.9218\n",
      "Training Epoch: 32 [1550/36045]\tLoss: 623.3173\n",
      "Training Epoch: 32 [1600/36045]\tLoss: 634.1918\n",
      "Training Epoch: 32 [1650/36045]\tLoss: 621.3054\n",
      "Training Epoch: 32 [1700/36045]\tLoss: 633.5902\n",
      "Training Epoch: 32 [1750/36045]\tLoss: 674.9832\n",
      "Training Epoch: 32 [1800/36045]\tLoss: 656.2285\n",
      "Training Epoch: 32 [1850/36045]\tLoss: 673.0294\n",
      "Training Epoch: 32 [1900/36045]\tLoss: 630.1069\n",
      "Training Epoch: 32 [1950/36045]\tLoss: 641.0728\n",
      "Training Epoch: 32 [2000/36045]\tLoss: 579.3726\n",
      "Training Epoch: 32 [2050/36045]\tLoss: 581.7300\n",
      "Training Epoch: 32 [2100/36045]\tLoss: 612.6166\n",
      "Training Epoch: 32 [2150/36045]\tLoss: 592.3154\n",
      "Training Epoch: 32 [2200/36045]\tLoss: 551.0765\n",
      "Training Epoch: 32 [2250/36045]\tLoss: 520.4172\n",
      "Training Epoch: 32 [2300/36045]\tLoss: 545.8375\n",
      "Training Epoch: 32 [2350/36045]\tLoss: 521.4969\n",
      "Training Epoch: 32 [2400/36045]\tLoss: 529.9677\n",
      "Training Epoch: 32 [2450/36045]\tLoss: 676.2163\n",
      "Training Epoch: 32 [2500/36045]\tLoss: 710.6252\n",
      "Training Epoch: 32 [2550/36045]\tLoss: 708.0787\n",
      "Training Epoch: 32 [2600/36045]\tLoss: 716.8036\n",
      "Training Epoch: 32 [2650/36045]\tLoss: 841.9199\n",
      "Training Epoch: 32 [2700/36045]\tLoss: 929.1552\n",
      "Training Epoch: 32 [2750/36045]\tLoss: 1000.7688\n",
      "Training Epoch: 32 [2800/36045]\tLoss: 1010.6437\n",
      "Training Epoch: 32 [2850/36045]\tLoss: 779.2228\n",
      "Training Epoch: 32 [2900/36045]\tLoss: 742.9763\n",
      "Training Epoch: 32 [2950/36045]\tLoss: 717.3737\n",
      "Training Epoch: 32 [3000/36045]\tLoss: 711.5832\n",
      "Training Epoch: 32 [3050/36045]\tLoss: 742.4474\n",
      "Training Epoch: 32 [3100/36045]\tLoss: 679.4598\n",
      "Training Epoch: 32 [3150/36045]\tLoss: 524.2744\n",
      "Training Epoch: 32 [3200/36045]\tLoss: 543.4793\n",
      "Training Epoch: 32 [3250/36045]\tLoss: 511.7539\n",
      "Training Epoch: 32 [3300/36045]\tLoss: 484.2285\n",
      "Training Epoch: 32 [3350/36045]\tLoss: 510.9031\n",
      "Training Epoch: 32 [3400/36045]\tLoss: 536.1676\n",
      "Training Epoch: 32 [3450/36045]\tLoss: 575.9084\n",
      "Training Epoch: 32 [3500/36045]\tLoss: 563.2254\n",
      "Training Epoch: 32 [3550/36045]\tLoss: 539.5308\n",
      "Training Epoch: 32 [3600/36045]\tLoss: 578.4164\n",
      "Training Epoch: 32 [3650/36045]\tLoss: 668.4833\n",
      "Training Epoch: 32 [3700/36045]\tLoss: 675.5228\n",
      "Training Epoch: 32 [3750/36045]\tLoss: 644.5486\n",
      "Training Epoch: 32 [3800/36045]\tLoss: 639.9617\n",
      "Training Epoch: 32 [3850/36045]\tLoss: 639.5792\n",
      "Training Epoch: 32 [3900/36045]\tLoss: 644.5844\n",
      "Training Epoch: 32 [3950/36045]\tLoss: 621.8480\n",
      "Training Epoch: 32 [4000/36045]\tLoss: 627.4572\n",
      "Training Epoch: 32 [4050/36045]\tLoss: 576.3707\n",
      "Training Epoch: 32 [4100/36045]\tLoss: 561.9586\n",
      "Training Epoch: 32 [4150/36045]\tLoss: 577.6406\n",
      "Training Epoch: 32 [4200/36045]\tLoss: 572.3287\n",
      "Training Epoch: 32 [4250/36045]\tLoss: 574.6025\n",
      "Training Epoch: 32 [4300/36045]\tLoss: 592.3576\n",
      "Training Epoch: 32 [4350/36045]\tLoss: 575.2036\n",
      "Training Epoch: 32 [4400/36045]\tLoss: 550.5019\n",
      "Training Epoch: 32 [4450/36045]\tLoss: 602.6510\n",
      "Training Epoch: 32 [4500/36045]\tLoss: 646.0483\n",
      "Training Epoch: 32 [4550/36045]\tLoss: 650.4791\n",
      "Training Epoch: 32 [4600/36045]\tLoss: 673.2380\n",
      "Training Epoch: 32 [4650/36045]\tLoss: 662.6594\n",
      "Training Epoch: 32 [4700/36045]\tLoss: 611.6716\n",
      "Training Epoch: 32 [4750/36045]\tLoss: 593.9734\n",
      "Training Epoch: 32 [4800/36045]\tLoss: 619.5203\n",
      "Training Epoch: 32 [4850/36045]\tLoss: 606.1047\n",
      "Training Epoch: 32 [4900/36045]\tLoss: 589.7479\n",
      "Training Epoch: 32 [4950/36045]\tLoss: 605.9600\n",
      "Training Epoch: 32 [5000/36045]\tLoss: 636.0925\n",
      "Training Epoch: 32 [5050/36045]\tLoss: 616.1296\n",
      "Training Epoch: 32 [5100/36045]\tLoss: 626.5820\n",
      "Training Epoch: 32 [5150/36045]\tLoss: 610.9765\n",
      "Training Epoch: 32 [5200/36045]\tLoss: 608.9943\n",
      "Training Epoch: 32 [5250/36045]\tLoss: 602.5137\n",
      "Training Epoch: 32 [5300/36045]\tLoss: 602.9669\n",
      "Training Epoch: 32 [5350/36045]\tLoss: 625.4890\n",
      "Training Epoch: 32 [5400/36045]\tLoss: 602.4196\n",
      "Training Epoch: 32 [5450/36045]\tLoss: 571.2745\n",
      "Training Epoch: 32 [5500/36045]\tLoss: 600.0754\n",
      "Training Epoch: 32 [5550/36045]\tLoss: 588.2032\n",
      "Training Epoch: 32 [5600/36045]\tLoss: 669.9696\n",
      "Training Epoch: 32 [5650/36045]\tLoss: 634.1844\n",
      "Training Epoch: 32 [5700/36045]\tLoss: 595.1000\n",
      "Training Epoch: 32 [5750/36045]\tLoss: 579.5361\n",
      "Training Epoch: 32 [5800/36045]\tLoss: 611.4431\n",
      "Training Epoch: 32 [5850/36045]\tLoss: 598.9607\n",
      "Training Epoch: 32 [5900/36045]\tLoss: 688.8696\n",
      "Training Epoch: 32 [5950/36045]\tLoss: 706.0504\n",
      "Training Epoch: 32 [6000/36045]\tLoss: 691.3273\n",
      "Training Epoch: 32 [6050/36045]\tLoss: 668.5202\n",
      "Training Epoch: 32 [6100/36045]\tLoss: 672.9489\n",
      "Training Epoch: 32 [6150/36045]\tLoss: 660.3510\n",
      "Training Epoch: 32 [6200/36045]\tLoss: 662.9779\n",
      "Training Epoch: 32 [6250/36045]\tLoss: 684.4219\n",
      "Training Epoch: 32 [6300/36045]\tLoss: 696.1568\n",
      "Training Epoch: 32 [6350/36045]\tLoss: 743.1068\n",
      "Training Epoch: 32 [6400/36045]\tLoss: 615.9862\n",
      "Training Epoch: 32 [6450/36045]\tLoss: 568.4064\n",
      "Training Epoch: 32 [6500/36045]\tLoss: 578.8351\n",
      "Training Epoch: 32 [6550/36045]\tLoss: 596.0687\n",
      "Training Epoch: 32 [6600/36045]\tLoss: 594.9312\n",
      "Training Epoch: 32 [6650/36045]\tLoss: 671.2255\n",
      "Training Epoch: 32 [6700/36045]\tLoss: 702.2159\n",
      "Training Epoch: 32 [6750/36045]\tLoss: 678.3262\n",
      "Training Epoch: 32 [6800/36045]\tLoss: 681.2756\n",
      "Training Epoch: 32 [6850/36045]\tLoss: 669.2070\n",
      "Training Epoch: 32 [6900/36045]\tLoss: 596.1742\n",
      "Training Epoch: 32 [6950/36045]\tLoss: 562.0190\n",
      "Training Epoch: 32 [7000/36045]\tLoss: 597.9637\n",
      "Training Epoch: 32 [7050/36045]\tLoss: 611.1578\n",
      "Training Epoch: 32 [7100/36045]\tLoss: 610.5876\n",
      "Training Epoch: 32 [7150/36045]\tLoss: 621.2199\n",
      "Training Epoch: 32 [7200/36045]\tLoss: 624.4742\n",
      "Training Epoch: 32 [7250/36045]\tLoss: 622.2858\n",
      "Training Epoch: 32 [7300/36045]\tLoss: 609.0203\n",
      "Training Epoch: 32 [7350/36045]\tLoss: 605.7068\n",
      "Training Epoch: 32 [7400/36045]\tLoss: 550.2200\n",
      "Training Epoch: 32 [7450/36045]\tLoss: 553.4951\n",
      "Training Epoch: 32 [7500/36045]\tLoss: 548.5787\n",
      "Training Epoch: 32 [7550/36045]\tLoss: 525.5906\n",
      "Training Epoch: 32 [7600/36045]\tLoss: 583.2975\n",
      "Training Epoch: 32 [7650/36045]\tLoss: 624.6232\n",
      "Training Epoch: 32 [7700/36045]\tLoss: 594.6998\n",
      "Training Epoch: 32 [7750/36045]\tLoss: 609.3482\n",
      "Training Epoch: 32 [7800/36045]\tLoss: 598.0529\n",
      "Training Epoch: 32 [7850/36045]\tLoss: 578.8239\n",
      "Training Epoch: 32 [7900/36045]\tLoss: 610.5502\n",
      "Training Epoch: 32 [7950/36045]\tLoss: 607.9239\n",
      "Training Epoch: 32 [8000/36045]\tLoss: 626.0363\n",
      "Training Epoch: 32 [8050/36045]\tLoss: 590.5730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 32 [8100/36045]\tLoss: 616.1003\n",
      "Training Epoch: 32 [8150/36045]\tLoss: 697.6292\n",
      "Training Epoch: 32 [8200/36045]\tLoss: 684.4123\n",
      "Training Epoch: 32 [8250/36045]\tLoss: 651.9425\n",
      "Training Epoch: 32 [8300/36045]\tLoss: 711.0989\n",
      "Training Epoch: 32 [8350/36045]\tLoss: 652.8890\n",
      "Training Epoch: 32 [8400/36045]\tLoss: 584.9512\n",
      "Training Epoch: 32 [8450/36045]\tLoss: 547.8740\n",
      "Training Epoch: 32 [8500/36045]\tLoss: 582.2344\n",
      "Training Epoch: 32 [8550/36045]\tLoss: 574.5890\n",
      "Training Epoch: 32 [8600/36045]\tLoss: 568.4983\n",
      "Training Epoch: 32 [8650/36045]\tLoss: 605.8276\n",
      "Training Epoch: 32 [8700/36045]\tLoss: 640.6432\n",
      "Training Epoch: 32 [8750/36045]\tLoss: 629.1167\n",
      "Training Epoch: 32 [8800/36045]\tLoss: 634.8958\n",
      "Training Epoch: 32 [8850/36045]\tLoss: 628.2249\n",
      "Training Epoch: 32 [8900/36045]\tLoss: 567.0708\n",
      "Training Epoch: 32 [8950/36045]\tLoss: 579.1126\n",
      "Training Epoch: 32 [9000/36045]\tLoss: 594.7552\n",
      "Training Epoch: 32 [9050/36045]\tLoss: 596.0000\n",
      "Training Epoch: 32 [9100/36045]\tLoss: 613.5235\n",
      "Training Epoch: 32 [9150/36045]\tLoss: 453.7098\n",
      "Training Epoch: 32 [9200/36045]\tLoss: 339.8196\n",
      "Training Epoch: 32 [9250/36045]\tLoss: 368.5528\n",
      "Training Epoch: 32 [9300/36045]\tLoss: 379.0904\n",
      "Training Epoch: 32 [9350/36045]\tLoss: 349.5456\n",
      "Training Epoch: 32 [9400/36045]\tLoss: 684.9820\n",
      "Training Epoch: 32 [9450/36045]\tLoss: 727.5816\n",
      "Training Epoch: 32 [9500/36045]\tLoss: 714.6483\n",
      "Training Epoch: 32 [9550/36045]\tLoss: 755.9979\n",
      "Training Epoch: 32 [9600/36045]\tLoss: 561.9952\n",
      "Training Epoch: 32 [9650/36045]\tLoss: 566.5743\n",
      "Training Epoch: 32 [9700/36045]\tLoss: 551.9895\n",
      "Training Epoch: 32 [9750/36045]\tLoss: 551.0690\n",
      "Training Epoch: 32 [9800/36045]\tLoss: 720.3104\n",
      "Training Epoch: 32 [9850/36045]\tLoss: 760.8328\n",
      "Training Epoch: 32 [9900/36045]\tLoss: 772.7503\n",
      "Training Epoch: 32 [9950/36045]\tLoss: 752.9249\n",
      "Training Epoch: 32 [10000/36045]\tLoss: 695.8611\n",
      "Training Epoch: 32 [10050/36045]\tLoss: 572.0582\n",
      "Training Epoch: 32 [10100/36045]\tLoss: 579.4643\n",
      "Training Epoch: 32 [10150/36045]\tLoss: 588.6428\n",
      "Training Epoch: 32 [10200/36045]\tLoss: 577.5370\n",
      "Training Epoch: 32 [10250/36045]\tLoss: 691.3714\n",
      "Training Epoch: 32 [10300/36045]\tLoss: 671.5490\n",
      "Training Epoch: 32 [10350/36045]\tLoss: 707.1357\n",
      "Training Epoch: 32 [10400/36045]\tLoss: 697.5823\n",
      "Training Epoch: 32 [10450/36045]\tLoss: 653.5179\n",
      "Training Epoch: 32 [10500/36045]\tLoss: 546.5992\n",
      "Training Epoch: 32 [10550/36045]\tLoss: 541.5593\n",
      "Training Epoch: 32 [10600/36045]\tLoss: 564.1870\n",
      "Training Epoch: 32 [10650/36045]\tLoss: 570.3829\n",
      "Training Epoch: 32 [10700/36045]\tLoss: 653.9124\n",
      "Training Epoch: 32 [10750/36045]\tLoss: 714.5976\n",
      "Training Epoch: 32 [10800/36045]\tLoss: 658.9816\n",
      "Training Epoch: 32 [10850/36045]\tLoss: 698.1591\n",
      "Training Epoch: 32 [10900/36045]\tLoss: 726.6172\n",
      "Training Epoch: 32 [10950/36045]\tLoss: 536.3496\n",
      "Training Epoch: 32 [11000/36045]\tLoss: 529.9734\n",
      "Training Epoch: 32 [11050/36045]\tLoss: 567.8115\n",
      "Training Epoch: 32 [11100/36045]\tLoss: 578.7542\n",
      "Training Epoch: 32 [11150/36045]\tLoss: 627.5099\n",
      "Training Epoch: 32 [11200/36045]\tLoss: 655.5237\n",
      "Training Epoch: 32 [11250/36045]\tLoss: 667.4507\n",
      "Training Epoch: 32 [11300/36045]\tLoss: 647.5086\n",
      "Training Epoch: 32 [11350/36045]\tLoss: 644.8612\n",
      "Training Epoch: 32 [11400/36045]\tLoss: 606.9917\n",
      "Training Epoch: 32 [11450/36045]\tLoss: 574.9931\n",
      "Training Epoch: 32 [11500/36045]\tLoss: 572.4094\n",
      "Training Epoch: 32 [11550/36045]\tLoss: 583.4322\n",
      "Training Epoch: 32 [11600/36045]\tLoss: 644.7252\n",
      "Training Epoch: 32 [11650/36045]\tLoss: 696.8062\n",
      "Training Epoch: 32 [11700/36045]\tLoss: 695.7462\n",
      "Training Epoch: 32 [11750/36045]\tLoss: 714.6038\n",
      "Training Epoch: 32 [11800/36045]\tLoss: 757.4139\n",
      "Training Epoch: 32 [11850/36045]\tLoss: 815.0887\n",
      "Training Epoch: 32 [11900/36045]\tLoss: 1029.8557\n",
      "Training Epoch: 32 [11950/36045]\tLoss: 1032.2487\n",
      "Training Epoch: 32 [12000/36045]\tLoss: 1045.0182\n",
      "Training Epoch: 32 [12050/36045]\tLoss: 1003.7405\n",
      "Training Epoch: 32 [12100/36045]\tLoss: 649.9791\n",
      "Training Epoch: 32 [12150/36045]\tLoss: 494.5627\n",
      "Training Epoch: 32 [12200/36045]\tLoss: 489.2007\n",
      "Training Epoch: 32 [12250/36045]\tLoss: 498.1320\n",
      "Training Epoch: 32 [12300/36045]\tLoss: 637.8530\n",
      "Training Epoch: 32 [12350/36045]\tLoss: 694.6247\n",
      "Training Epoch: 32 [12400/36045]\tLoss: 702.2579\n",
      "Training Epoch: 32 [12450/36045]\tLoss: 690.7009\n",
      "Training Epoch: 32 [12500/36045]\tLoss: 718.6495\n",
      "Training Epoch: 32 [12550/36045]\tLoss: 687.8991\n",
      "Training Epoch: 32 [12600/36045]\tLoss: 631.9899\n",
      "Training Epoch: 32 [12650/36045]\tLoss: 630.6736\n",
      "Training Epoch: 32 [12700/36045]\tLoss: 652.1255\n",
      "Training Epoch: 32 [12750/36045]\tLoss: 651.1185\n",
      "Training Epoch: 32 [12800/36045]\tLoss: 635.2771\n",
      "Training Epoch: 32 [12850/36045]\tLoss: 664.5798\n",
      "Training Epoch: 32 [12900/36045]\tLoss: 637.4373\n",
      "Training Epoch: 32 [12950/36045]\tLoss: 624.2139\n",
      "Training Epoch: 32 [13000/36045]\tLoss: 656.8798\n",
      "Training Epoch: 32 [13050/36045]\tLoss: 595.5396\n",
      "Training Epoch: 32 [13100/36045]\tLoss: 613.1606\n",
      "Training Epoch: 32 [13150/36045]\tLoss: 604.8627\n",
      "Training Epoch: 32 [13200/36045]\tLoss: 586.3267\n",
      "Training Epoch: 32 [13250/36045]\tLoss: 609.9055\n",
      "Training Epoch: 32 [13300/36045]\tLoss: 648.4289\n",
      "Training Epoch: 32 [13350/36045]\tLoss: 628.5202\n",
      "Training Epoch: 32 [13400/36045]\tLoss: 631.9704\n",
      "Training Epoch: 32 [13450/36045]\tLoss: 628.7257\n",
      "Training Epoch: 32 [13500/36045]\tLoss: 648.8466\n",
      "Training Epoch: 32 [13550/36045]\tLoss: 785.6254\n",
      "Training Epoch: 32 [13600/36045]\tLoss: 818.4669\n",
      "Training Epoch: 32 [13650/36045]\tLoss: 899.2431\n",
      "Training Epoch: 32 [13700/36045]\tLoss: 794.1980\n",
      "Training Epoch: 32 [13750/36045]\tLoss: 635.0912\n",
      "Training Epoch: 32 [13800/36045]\tLoss: 607.1218\n",
      "Training Epoch: 32 [13850/36045]\tLoss: 589.9713\n",
      "Training Epoch: 32 [13900/36045]\tLoss: 597.2685\n",
      "Training Epoch: 32 [13950/36045]\tLoss: 642.6855\n",
      "Training Epoch: 32 [14000/36045]\tLoss: 676.2360\n",
      "Training Epoch: 32 [14050/36045]\tLoss: 650.6046\n",
      "Training Epoch: 32 [14100/36045]\tLoss: 646.0182\n",
      "Training Epoch: 32 [14150/36045]\tLoss: 633.9281\n",
      "Training Epoch: 32 [14200/36045]\tLoss: 675.4688\n",
      "Training Epoch: 32 [14250/36045]\tLoss: 741.4405\n",
      "Training Epoch: 32 [14300/36045]\tLoss: 744.7896\n",
      "Training Epoch: 32 [14350/36045]\tLoss: 712.5567\n",
      "Training Epoch: 32 [14400/36045]\tLoss: 698.2263\n",
      "Training Epoch: 32 [14450/36045]\tLoss: 734.6214\n",
      "Training Epoch: 32 [14500/36045]\tLoss: 665.4146\n",
      "Training Epoch: 32 [14550/36045]\tLoss: 695.0386\n",
      "Training Epoch: 32 [14600/36045]\tLoss: 681.0125\n",
      "Training Epoch: 32 [14650/36045]\tLoss: 680.9579\n",
      "Training Epoch: 32 [14700/36045]\tLoss: 644.4141\n",
      "Training Epoch: 32 [14750/36045]\tLoss: 553.7776\n",
      "Training Epoch: 32 [14800/36045]\tLoss: 543.9003\n",
      "Training Epoch: 32 [14850/36045]\tLoss: 550.9633\n",
      "Training Epoch: 32 [14900/36045]\tLoss: 544.4850\n",
      "Training Epoch: 32 [14950/36045]\tLoss: 552.3571\n",
      "Training Epoch: 32 [15000/36045]\tLoss: 566.2250\n",
      "Training Epoch: 32 [15050/36045]\tLoss: 563.6064\n",
      "Training Epoch: 32 [15100/36045]\tLoss: 547.6854\n",
      "Training Epoch: 32 [15150/36045]\tLoss: 542.1708\n",
      "Training Epoch: 32 [15200/36045]\tLoss: 501.9283\n",
      "Training Epoch: 32 [15250/36045]\tLoss: 524.6402\n",
      "Training Epoch: 32 [15300/36045]\tLoss: 509.6814\n",
      "Training Epoch: 32 [15350/36045]\tLoss: 521.6638\n",
      "Training Epoch: 32 [15400/36045]\tLoss: 505.3904\n",
      "Training Epoch: 32 [15450/36045]\tLoss: 490.8229\n",
      "Training Epoch: 32 [15500/36045]\tLoss: 505.0571\n",
      "Training Epoch: 32 [15550/36045]\tLoss: 500.9657\n",
      "Training Epoch: 32 [15600/36045]\tLoss: 569.9923\n",
      "Training Epoch: 32 [15650/36045]\tLoss: 587.8063\n",
      "Training Epoch: 32 [15700/36045]\tLoss: 579.2379\n",
      "Training Epoch: 32 [15750/36045]\tLoss: 570.8835\n",
      "Training Epoch: 32 [15800/36045]\tLoss: 541.8336\n",
      "Training Epoch: 32 [15850/36045]\tLoss: 556.3190\n",
      "Training Epoch: 32 [15900/36045]\tLoss: 565.6930\n",
      "Training Epoch: 32 [15950/36045]\tLoss: 585.4781\n",
      "Training Epoch: 32 [16000/36045]\tLoss: 558.0433\n",
      "Training Epoch: 32 [16050/36045]\tLoss: 527.3983\n",
      "Training Epoch: 32 [16100/36045]\tLoss: 488.9317\n",
      "Training Epoch: 32 [16150/36045]\tLoss: 476.7773\n",
      "Training Epoch: 32 [16200/36045]\tLoss: 577.8298\n",
      "Training Epoch: 32 [16250/36045]\tLoss: 606.2606\n",
      "Training Epoch: 32 [16300/36045]\tLoss: 661.8884\n",
      "Training Epoch: 32 [16350/36045]\tLoss: 681.2190\n",
      "Training Epoch: 32 [16400/36045]\tLoss: 653.0528\n",
      "Training Epoch: 32 [16450/36045]\tLoss: 634.9035\n",
      "Training Epoch: 32 [16500/36045]\tLoss: 634.7662\n",
      "Training Epoch: 32 [16550/36045]\tLoss: 599.4854\n",
      "Training Epoch: 32 [16600/36045]\tLoss: 623.1224\n",
      "Training Epoch: 32 [16650/36045]\tLoss: 640.6255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 32 [16700/36045]\tLoss: 619.0949\n",
      "Training Epoch: 32 [16750/36045]\tLoss: 611.5383\n",
      "Training Epoch: 32 [16800/36045]\tLoss: 621.6558\n",
      "Training Epoch: 32 [16850/36045]\tLoss: 592.1179\n",
      "Training Epoch: 32 [16900/36045]\tLoss: 602.1788\n",
      "Training Epoch: 32 [16950/36045]\tLoss: 626.0305\n",
      "Training Epoch: 32 [17000/36045]\tLoss: 609.3264\n",
      "Training Epoch: 32 [17050/36045]\tLoss: 635.2947\n",
      "Training Epoch: 32 [17100/36045]\tLoss: 631.6379\n",
      "Training Epoch: 32 [17150/36045]\tLoss: 549.1044\n",
      "Training Epoch: 32 [17200/36045]\tLoss: 510.4865\n",
      "Training Epoch: 32 [17250/36045]\tLoss: 534.2855\n",
      "Training Epoch: 32 [17300/36045]\tLoss: 565.2748\n",
      "Training Epoch: 32 [17350/36045]\tLoss: 543.8979\n",
      "Training Epoch: 32 [17400/36045]\tLoss: 563.3085\n",
      "Training Epoch: 32 [17450/36045]\tLoss: 582.7297\n",
      "Training Epoch: 32 [17500/36045]\tLoss: 570.8419\n",
      "Training Epoch: 32 [17550/36045]\tLoss: 569.9008\n",
      "Training Epoch: 32 [17600/36045]\tLoss: 563.5179\n",
      "Training Epoch: 32 [17650/36045]\tLoss: 579.9670\n",
      "Training Epoch: 32 [17700/36045]\tLoss: 558.9330\n",
      "Training Epoch: 32 [17750/36045]\tLoss: 575.5906\n",
      "Training Epoch: 32 [17800/36045]\tLoss: 566.3748\n",
      "Training Epoch: 32 [17850/36045]\tLoss: 578.6495\n",
      "Training Epoch: 32 [17900/36045]\tLoss: 607.1624\n",
      "Training Epoch: 32 [17950/36045]\tLoss: 619.0176\n",
      "Training Epoch: 32 [18000/36045]\tLoss: 609.3162\n",
      "Training Epoch: 32 [18050/36045]\tLoss: 672.5212\n",
      "Training Epoch: 32 [18100/36045]\tLoss: 674.3432\n",
      "Training Epoch: 32 [18150/36045]\tLoss: 685.6421\n",
      "Training Epoch: 32 [18200/36045]\tLoss: 667.6282\n",
      "Training Epoch: 32 [18250/36045]\tLoss: 688.4965\n",
      "Training Epoch: 32 [18300/36045]\tLoss: 640.1945\n",
      "Training Epoch: 32 [18350/36045]\tLoss: 711.6531\n",
      "Training Epoch: 32 [18400/36045]\tLoss: 684.9147\n",
      "Training Epoch: 32 [18450/36045]\tLoss: 664.8549\n",
      "Training Epoch: 32 [18500/36045]\tLoss: 663.8369\n",
      "Training Epoch: 32 [18550/36045]\tLoss: 650.9877\n",
      "Training Epoch: 32 [18600/36045]\tLoss: 640.3909\n",
      "Training Epoch: 32 [18650/36045]\tLoss: 686.8765\n",
      "Training Epoch: 32 [18700/36045]\tLoss: 722.6913\n",
      "Training Epoch: 32 [18750/36045]\tLoss: 709.4157\n",
      "Training Epoch: 32 [18800/36045]\tLoss: 733.0468\n",
      "Training Epoch: 32 [18850/36045]\tLoss: 677.4932\n",
      "Training Epoch: 32 [18900/36045]\tLoss: 724.7737\n",
      "Training Epoch: 32 [18950/36045]\tLoss: 666.5880\n",
      "Training Epoch: 32 [19000/36045]\tLoss: 554.5467\n",
      "Training Epoch: 32 [19050/36045]\tLoss: 538.0229\n",
      "Training Epoch: 32 [19100/36045]\tLoss: 546.5114\n",
      "Training Epoch: 32 [19150/36045]\tLoss: 536.2090\n",
      "Training Epoch: 32 [19200/36045]\tLoss: 565.6235\n",
      "Training Epoch: 32 [19250/36045]\tLoss: 580.3282\n",
      "Training Epoch: 32 [19300/36045]\tLoss: 590.4827\n",
      "Training Epoch: 32 [19350/36045]\tLoss: 573.9987\n",
      "Training Epoch: 32 [19400/36045]\tLoss: 595.3959\n",
      "Training Epoch: 32 [19450/36045]\tLoss: 586.4601\n",
      "Training Epoch: 32 [19500/36045]\tLoss: 588.0056\n",
      "Training Epoch: 32 [19550/36045]\tLoss: 586.5180\n",
      "Training Epoch: 32 [19600/36045]\tLoss: 627.5314\n",
      "Training Epoch: 32 [19650/36045]\tLoss: 832.2242\n",
      "Training Epoch: 32 [19700/36045]\tLoss: 791.0321\n",
      "Training Epoch: 32 [19750/36045]\tLoss: 794.4086\n",
      "Training Epoch: 32 [19800/36045]\tLoss: 794.0002\n",
      "Training Epoch: 32 [19850/36045]\tLoss: 525.1662\n",
      "Training Epoch: 32 [19900/36045]\tLoss: 503.5028\n",
      "Training Epoch: 32 [19950/36045]\tLoss: 507.1199\n",
      "Training Epoch: 32 [20000/36045]\tLoss: 506.1624\n",
      "Training Epoch: 32 [20050/36045]\tLoss: 566.8458\n",
      "Training Epoch: 32 [20100/36045]\tLoss: 573.0229\n",
      "Training Epoch: 32 [20150/36045]\tLoss: 574.8533\n",
      "Training Epoch: 32 [20200/36045]\tLoss: 574.6951\n",
      "Training Epoch: 32 [20250/36045]\tLoss: 612.0981\n",
      "Training Epoch: 32 [20300/36045]\tLoss: 648.0896\n",
      "Training Epoch: 32 [20350/36045]\tLoss: 666.9626\n",
      "Training Epoch: 32 [20400/36045]\tLoss: 683.6704\n",
      "Training Epoch: 32 [20450/36045]\tLoss: 654.1407\n",
      "Training Epoch: 32 [20500/36045]\tLoss: 638.1948\n",
      "Training Epoch: 32 [20550/36045]\tLoss: 561.3719\n",
      "Training Epoch: 32 [20600/36045]\tLoss: 571.9594\n",
      "Training Epoch: 32 [20650/36045]\tLoss: 569.0065\n",
      "Training Epoch: 32 [20700/36045]\tLoss: 556.9475\n",
      "Training Epoch: 32 [20750/36045]\tLoss: 599.8456\n",
      "Training Epoch: 32 [20800/36045]\tLoss: 651.8740\n",
      "Training Epoch: 32 [20850/36045]\tLoss: 638.5645\n",
      "Training Epoch: 32 [20900/36045]\tLoss: 683.2105\n",
      "Training Epoch: 32 [20950/36045]\tLoss: 644.2727\n",
      "Training Epoch: 32 [21000/36045]\tLoss: 606.7886\n",
      "Training Epoch: 32 [21050/36045]\tLoss: 519.4768\n",
      "Training Epoch: 32 [21100/36045]\tLoss: 523.4239\n",
      "Training Epoch: 32 [21150/36045]\tLoss: 560.2195\n",
      "Training Epoch: 32 [21200/36045]\tLoss: 559.4301\n",
      "Training Epoch: 32 [21250/36045]\tLoss: 535.2603\n",
      "Training Epoch: 32 [21300/36045]\tLoss: 625.0646\n",
      "Training Epoch: 32 [21350/36045]\tLoss: 617.2101\n",
      "Training Epoch: 32 [21400/36045]\tLoss: 620.6614\n",
      "Training Epoch: 32 [21450/36045]\tLoss: 626.8690\n",
      "Training Epoch: 32 [21500/36045]\tLoss: 629.3739\n",
      "Training Epoch: 32 [21550/36045]\tLoss: 724.2861\n",
      "Training Epoch: 32 [21600/36045]\tLoss: 723.4323\n",
      "Training Epoch: 32 [21650/36045]\tLoss: 736.0297\n",
      "Training Epoch: 32 [21700/36045]\tLoss: 738.2993\n",
      "Training Epoch: 32 [21750/36045]\tLoss: 709.1825\n",
      "Training Epoch: 32 [21800/36045]\tLoss: 523.1068\n",
      "Training Epoch: 32 [21850/36045]\tLoss: 506.1121\n",
      "Training Epoch: 32 [21900/36045]\tLoss: 516.1223\n",
      "Training Epoch: 32 [21950/36045]\tLoss: 516.1910\n",
      "Training Epoch: 32 [22000/36045]\tLoss: 519.6960\n",
      "Training Epoch: 32 [22050/36045]\tLoss: 541.8333\n",
      "Training Epoch: 32 [22100/36045]\tLoss: 534.4303\n",
      "Training Epoch: 32 [22150/36045]\tLoss: 519.8432\n",
      "Training Epoch: 32 [22200/36045]\tLoss: 536.5107\n",
      "Training Epoch: 32 [22250/36045]\tLoss: 541.4854\n",
      "Training Epoch: 32 [22300/36045]\tLoss: 593.9179\n",
      "Training Epoch: 32 [22350/36045]\tLoss: 619.9162\n",
      "Training Epoch: 32 [22400/36045]\tLoss: 634.6577\n",
      "Training Epoch: 32 [22450/36045]\tLoss: 622.2353\n",
      "Training Epoch: 32 [22500/36045]\tLoss: 604.3613\n",
      "Training Epoch: 32 [22550/36045]\tLoss: 640.2022\n",
      "Training Epoch: 32 [22600/36045]\tLoss: 693.5720\n",
      "Training Epoch: 32 [22650/36045]\tLoss: 728.4666\n",
      "Training Epoch: 32 [22700/36045]\tLoss: 750.9907\n",
      "Training Epoch: 32 [22750/36045]\tLoss: 770.5580\n",
      "Training Epoch: 32 [22800/36045]\tLoss: 801.0726\n",
      "Training Epoch: 32 [22850/36045]\tLoss: 666.2795\n",
      "Training Epoch: 32 [22900/36045]\tLoss: 671.0692\n",
      "Training Epoch: 32 [22950/36045]\tLoss: 650.4453\n",
      "Training Epoch: 32 [23000/36045]\tLoss: 647.7797\n",
      "Training Epoch: 32 [23050/36045]\tLoss: 575.6320\n",
      "Training Epoch: 32 [23100/36045]\tLoss: 591.5589\n",
      "Training Epoch: 32 [23150/36045]\tLoss: 579.7609\n",
      "Training Epoch: 32 [23200/36045]\tLoss: 549.1154\n",
      "Training Epoch: 32 [23250/36045]\tLoss: 552.1456\n",
      "Training Epoch: 32 [23300/36045]\tLoss: 548.5295\n",
      "Training Epoch: 32 [23350/36045]\tLoss: 569.5948\n",
      "Training Epoch: 32 [23400/36045]\tLoss: 617.2453\n",
      "Training Epoch: 32 [23450/36045]\tLoss: 610.3685\n",
      "Training Epoch: 32 [23500/36045]\tLoss: 588.3622\n",
      "Training Epoch: 32 [23550/36045]\tLoss: 630.8813\n",
      "Training Epoch: 32 [23600/36045]\tLoss: 713.4284\n",
      "Training Epoch: 32 [23650/36045]\tLoss: 726.3053\n",
      "Training Epoch: 32 [23700/36045]\tLoss: 734.6671\n",
      "Training Epoch: 32 [23750/36045]\tLoss: 710.1012\n",
      "Training Epoch: 32 [23800/36045]\tLoss: 567.6481\n",
      "Training Epoch: 32 [23850/36045]\tLoss: 593.9772\n",
      "Training Epoch: 32 [23900/36045]\tLoss: 583.5585\n",
      "Training Epoch: 32 [23950/36045]\tLoss: 566.5740\n",
      "Training Epoch: 32 [24000/36045]\tLoss: 543.2006\n",
      "Training Epoch: 32 [24050/36045]\tLoss: 501.9213\n",
      "Training Epoch: 32 [24100/36045]\tLoss: 527.9967\n",
      "Training Epoch: 32 [24150/36045]\tLoss: 521.1192\n",
      "Training Epoch: 32 [24200/36045]\tLoss: 517.6205\n",
      "Training Epoch: 32 [24250/36045]\tLoss: 502.4716\n",
      "Training Epoch: 32 [24300/36045]\tLoss: 542.7524\n",
      "Training Epoch: 32 [24350/36045]\tLoss: 555.9525\n",
      "Training Epoch: 32 [24400/36045]\tLoss: 571.5355\n",
      "Training Epoch: 32 [24450/36045]\tLoss: 544.8374\n",
      "Training Epoch: 32 [24500/36045]\tLoss: 574.4614\n",
      "Training Epoch: 32 [24550/36045]\tLoss: 663.7021\n",
      "Training Epoch: 32 [24600/36045]\tLoss: 655.5672\n",
      "Training Epoch: 32 [24650/36045]\tLoss: 628.4477\n",
      "Training Epoch: 32 [24700/36045]\tLoss: 638.4129\n",
      "Training Epoch: 32 [24750/36045]\tLoss: 589.8398\n",
      "Training Epoch: 32 [24800/36045]\tLoss: 486.6591\n",
      "Training Epoch: 32 [24850/36045]\tLoss: 506.0122\n",
      "Training Epoch: 32 [24900/36045]\tLoss: 503.1984\n",
      "Training Epoch: 32 [24950/36045]\tLoss: 505.7932\n",
      "Training Epoch: 32 [25000/36045]\tLoss: 486.2281\n",
      "Training Epoch: 32 [25050/36045]\tLoss: 464.4341\n",
      "Training Epoch: 32 [25100/36045]\tLoss: 416.2080\n",
      "Training Epoch: 32 [25150/36045]\tLoss: 385.4768\n",
      "Training Epoch: 32 [25200/36045]\tLoss: 380.5493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 32 [25250/36045]\tLoss: 407.8463\n",
      "Training Epoch: 32 [25300/36045]\tLoss: 535.6293\n",
      "Training Epoch: 32 [25350/36045]\tLoss: 532.3349\n",
      "Training Epoch: 32 [25400/36045]\tLoss: 496.4785\n",
      "Training Epoch: 32 [25450/36045]\tLoss: 498.9669\n",
      "Training Epoch: 32 [25500/36045]\tLoss: 542.1885\n",
      "Training Epoch: 32 [25550/36045]\tLoss: 633.4716\n",
      "Training Epoch: 32 [25600/36045]\tLoss: 637.9320\n",
      "Training Epoch: 32 [25650/36045]\tLoss: 615.5786\n",
      "Training Epoch: 32 [25700/36045]\tLoss: 624.7144\n",
      "Training Epoch: 32 [25750/36045]\tLoss: 602.3888\n",
      "Training Epoch: 32 [25800/36045]\tLoss: 380.0294\n",
      "Training Epoch: 32 [25850/36045]\tLoss: 389.5131\n",
      "Training Epoch: 32 [25900/36045]\tLoss: 370.5187\n",
      "Training Epoch: 32 [25950/36045]\tLoss: 379.2971\n",
      "Training Epoch: 32 [26000/36045]\tLoss: 465.1052\n",
      "Training Epoch: 32 [26050/36045]\tLoss: 633.2701\n",
      "Training Epoch: 32 [26100/36045]\tLoss: 660.7798\n",
      "Training Epoch: 32 [26150/36045]\tLoss: 661.0839\n",
      "Training Epoch: 32 [26200/36045]\tLoss: 634.1526\n",
      "Training Epoch: 32 [26250/36045]\tLoss: 664.1334\n",
      "Training Epoch: 32 [26300/36045]\tLoss: 602.0317\n",
      "Training Epoch: 32 [26350/36045]\tLoss: 612.8970\n",
      "Training Epoch: 32 [26400/36045]\tLoss: 590.0142\n",
      "Training Epoch: 32 [26450/36045]\tLoss: 519.8039\n",
      "Training Epoch: 32 [26500/36045]\tLoss: 617.1879\n",
      "Training Epoch: 32 [26550/36045]\tLoss: 617.9197\n",
      "Training Epoch: 32 [26600/36045]\tLoss: 614.1385\n",
      "Training Epoch: 32 [26650/36045]\tLoss: 630.2850\n",
      "Training Epoch: 32 [26700/36045]\tLoss: 609.8247\n",
      "Training Epoch: 32 [26750/36045]\tLoss: 571.1398\n",
      "Training Epoch: 32 [26800/36045]\tLoss: 420.8010\n",
      "Training Epoch: 32 [26850/36045]\tLoss: 348.9554\n",
      "Training Epoch: 32 [26900/36045]\tLoss: 351.8364\n",
      "Training Epoch: 32 [26950/36045]\tLoss: 386.7512\n",
      "Training Epoch: 32 [27000/36045]\tLoss: 631.2236\n",
      "Training Epoch: 32 [27050/36045]\tLoss: 659.5093\n",
      "Training Epoch: 32 [27100/36045]\tLoss: 638.9388\n",
      "Training Epoch: 32 [27150/36045]\tLoss: 679.3170\n",
      "Training Epoch: 32 [27200/36045]\tLoss: 496.2641\n",
      "Training Epoch: 32 [27250/36045]\tLoss: 488.0666\n",
      "Training Epoch: 32 [27300/36045]\tLoss: 475.4272\n",
      "Training Epoch: 32 [27350/36045]\tLoss: 473.9023\n",
      "Training Epoch: 32 [27400/36045]\tLoss: 473.0645\n",
      "Training Epoch: 32 [27450/36045]\tLoss: 597.5629\n",
      "Training Epoch: 32 [27500/36045]\tLoss: 640.3615\n",
      "Training Epoch: 32 [27550/36045]\tLoss: 633.3366\n",
      "Training Epoch: 32 [27600/36045]\tLoss: 645.0508\n",
      "Training Epoch: 32 [27650/36045]\tLoss: 636.8622\n",
      "Training Epoch: 32 [27700/36045]\tLoss: 665.7958\n",
      "Training Epoch: 32 [27750/36045]\tLoss: 677.9044\n",
      "Training Epoch: 32 [27800/36045]\tLoss: 664.6041\n",
      "Training Epoch: 32 [27850/36045]\tLoss: 654.2886\n",
      "Training Epoch: 32 [27900/36045]\tLoss: 591.8210\n",
      "Training Epoch: 32 [27950/36045]\tLoss: 492.8354\n",
      "Training Epoch: 32 [28000/36045]\tLoss: 469.2838\n",
      "Training Epoch: 32 [28050/36045]\tLoss: 479.4327\n",
      "Training Epoch: 32 [28100/36045]\tLoss: 471.0708\n",
      "Training Epoch: 32 [28150/36045]\tLoss: 494.0227\n",
      "Training Epoch: 32 [28200/36045]\tLoss: 501.1974\n",
      "Training Epoch: 32 [28250/36045]\tLoss: 494.4843\n",
      "Training Epoch: 32 [28300/36045]\tLoss: 469.3166\n",
      "Training Epoch: 32 [28350/36045]\tLoss: 465.4288\n",
      "Training Epoch: 32 [28400/36045]\tLoss: 796.1386\n",
      "Training Epoch: 32 [28450/36045]\tLoss: 729.0397\n",
      "Training Epoch: 32 [28500/36045]\tLoss: 629.9755\n",
      "Training Epoch: 32 [28550/36045]\tLoss: 578.6373\n",
      "Training Epoch: 32 [28600/36045]\tLoss: 609.1918\n",
      "Training Epoch: 32 [28650/36045]\tLoss: 673.8995\n",
      "Training Epoch: 32 [28700/36045]\tLoss: 668.1143\n",
      "Training Epoch: 32 [28750/36045]\tLoss: 654.5558\n",
      "Training Epoch: 32 [28800/36045]\tLoss: 663.3656\n",
      "Training Epoch: 32 [28850/36045]\tLoss: 575.2861\n",
      "Training Epoch: 32 [28900/36045]\tLoss: 466.7492\n",
      "Training Epoch: 32 [28950/36045]\tLoss: 465.6966\n",
      "Training Epoch: 32 [29000/36045]\tLoss: 462.8660\n",
      "Training Epoch: 32 [29050/36045]\tLoss: 469.7823\n",
      "Training Epoch: 32 [29100/36045]\tLoss: 488.6094\n",
      "Training Epoch: 32 [29150/36045]\tLoss: 477.4419\n",
      "Training Epoch: 32 [29200/36045]\tLoss: 463.0676\n",
      "Training Epoch: 32 [29250/36045]\tLoss: 452.6312\n",
      "Training Epoch: 32 [29300/36045]\tLoss: 513.1503\n",
      "Training Epoch: 32 [29350/36045]\tLoss: 605.1005\n",
      "Training Epoch: 32 [29400/36045]\tLoss: 622.7033\n",
      "Training Epoch: 32 [29450/36045]\tLoss: 640.7802\n",
      "Training Epoch: 32 [29500/36045]\tLoss: 655.4391\n",
      "Training Epoch: 32 [29550/36045]\tLoss: 623.7723\n",
      "Training Epoch: 32 [29600/36045]\tLoss: 526.3175\n",
      "Training Epoch: 32 [29650/36045]\tLoss: 508.6066\n",
      "Training Epoch: 32 [29700/36045]\tLoss: 454.6087\n",
      "Training Epoch: 32 [29750/36045]\tLoss: 453.4988\n",
      "Training Epoch: 32 [29800/36045]\tLoss: 500.3633\n",
      "Training Epoch: 32 [29850/36045]\tLoss: 576.2592\n",
      "Training Epoch: 32 [29900/36045]\tLoss: 572.8277\n",
      "Training Epoch: 32 [29950/36045]\tLoss: 594.8268\n",
      "Training Epoch: 32 [30000/36045]\tLoss: 569.5110\n",
      "Training Epoch: 32 [30050/36045]\tLoss: 575.7559\n",
      "Training Epoch: 32 [30100/36045]\tLoss: 702.4672\n",
      "Training Epoch: 32 [30150/36045]\tLoss: 685.9232\n",
      "Training Epoch: 32 [30200/36045]\tLoss: 647.1812\n",
      "Training Epoch: 32 [30250/36045]\tLoss: 695.8927\n",
      "Training Epoch: 32 [30300/36045]\tLoss: 680.7836\n",
      "Training Epoch: 32 [30350/36045]\tLoss: 526.0455\n",
      "Training Epoch: 32 [30400/36045]\tLoss: 510.5838\n",
      "Training Epoch: 32 [30450/36045]\tLoss: 512.3004\n",
      "Training Epoch: 32 [30500/36045]\tLoss: 478.5545\n",
      "Training Epoch: 32 [30550/36045]\tLoss: 443.5850\n",
      "Training Epoch: 32 [30600/36045]\tLoss: 434.2211\n",
      "Training Epoch: 32 [30650/36045]\tLoss: 424.1776\n",
      "Training Epoch: 32 [30700/36045]\tLoss: 442.0092\n",
      "Training Epoch: 32 [30750/36045]\tLoss: 428.8014\n",
      "Training Epoch: 32 [30800/36045]\tLoss: 455.5456\n",
      "Training Epoch: 32 [30850/36045]\tLoss: 447.2632\n",
      "Training Epoch: 32 [30900/36045]\tLoss: 459.8492\n",
      "Training Epoch: 32 [30950/36045]\tLoss: 483.0874\n",
      "Training Epoch: 32 [31000/36045]\tLoss: 474.8979\n",
      "Training Epoch: 32 [31050/36045]\tLoss: 396.8083\n",
      "Training Epoch: 32 [31100/36045]\tLoss: 387.3989\n",
      "Training Epoch: 32 [31150/36045]\tLoss: 394.6983\n",
      "Training Epoch: 32 [31200/36045]\tLoss: 491.1837\n",
      "Training Epoch: 32 [31250/36045]\tLoss: 637.3348\n",
      "Training Epoch: 32 [31300/36045]\tLoss: 607.7872\n",
      "Training Epoch: 32 [31350/36045]\tLoss: 623.6934\n",
      "Training Epoch: 32 [31400/36045]\tLoss: 603.0829\n",
      "Training Epoch: 32 [31450/36045]\tLoss: 619.2500\n",
      "Training Epoch: 32 [31500/36045]\tLoss: 631.6398\n",
      "Training Epoch: 32 [31550/36045]\tLoss: 639.2786\n",
      "Training Epoch: 32 [31600/36045]\tLoss: 600.9512\n",
      "Training Epoch: 32 [31650/36045]\tLoss: 642.5988\n",
      "Training Epoch: 32 [31700/36045]\tLoss: 465.5920\n",
      "Training Epoch: 32 [31750/36045]\tLoss: 385.2426\n",
      "Training Epoch: 32 [31800/36045]\tLoss: 367.4496\n",
      "Training Epoch: 32 [31850/36045]\tLoss: 376.1645\n",
      "Training Epoch: 32 [31900/36045]\tLoss: 590.9465\n",
      "Training Epoch: 32 [31950/36045]\tLoss: 763.3282\n",
      "Training Epoch: 32 [32000/36045]\tLoss: 873.1819\n",
      "Training Epoch: 32 [32050/36045]\tLoss: 827.9857\n",
      "Training Epoch: 32 [32100/36045]\tLoss: 818.2518\n",
      "Training Epoch: 32 [32150/36045]\tLoss: 633.9440\n",
      "Training Epoch: 32 [32200/36045]\tLoss: 637.2606\n",
      "Training Epoch: 32 [32250/36045]\tLoss: 647.8615\n",
      "Training Epoch: 32 [32300/36045]\tLoss: 629.5829\n",
      "Training Epoch: 32 [32350/36045]\tLoss: 625.0505\n",
      "Training Epoch: 32 [32400/36045]\tLoss: 586.5518\n",
      "Training Epoch: 32 [32450/36045]\tLoss: 483.1230\n",
      "Training Epoch: 32 [32500/36045]\tLoss: 464.3850\n",
      "Training Epoch: 32 [32550/36045]\tLoss: 466.7204\n",
      "Training Epoch: 32 [32600/36045]\tLoss: 463.4655\n",
      "Training Epoch: 32 [32650/36045]\tLoss: 596.1465\n",
      "Training Epoch: 32 [32700/36045]\tLoss: 650.4895\n",
      "Training Epoch: 32 [32750/36045]\tLoss: 619.8725\n",
      "Training Epoch: 32 [32800/36045]\tLoss: 635.7797\n",
      "Training Epoch: 32 [32850/36045]\tLoss: 586.9653\n",
      "Training Epoch: 32 [32900/36045]\tLoss: 470.9051\n",
      "Training Epoch: 32 [32950/36045]\tLoss: 492.9543\n",
      "Training Epoch: 32 [33000/36045]\tLoss: 492.3113\n",
      "Training Epoch: 32 [33050/36045]\tLoss: 467.6956\n",
      "Training Epoch: 32 [33100/36045]\tLoss: 531.4985\n",
      "Training Epoch: 32 [33150/36045]\tLoss: 721.7363\n",
      "Training Epoch: 32 [33200/36045]\tLoss: 702.9113\n",
      "Training Epoch: 32 [33250/36045]\tLoss: 724.3279\n",
      "Training Epoch: 32 [33300/36045]\tLoss: 771.3968\n",
      "Training Epoch: 32 [33350/36045]\tLoss: 591.5724\n",
      "Training Epoch: 32 [33400/36045]\tLoss: 433.4294\n",
      "Training Epoch: 32 [33450/36045]\tLoss: 428.7110\n",
      "Training Epoch: 32 [33500/36045]\tLoss: 441.2926\n",
      "Training Epoch: 32 [33550/36045]\tLoss: 457.4637\n",
      "Training Epoch: 32 [33600/36045]\tLoss: 459.2638\n",
      "Training Epoch: 32 [33650/36045]\tLoss: 612.7086\n",
      "Training Epoch: 32 [33700/36045]\tLoss: 592.7648\n",
      "Training Epoch: 32 [33750/36045]\tLoss: 613.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 32 [33800/36045]\tLoss: 609.8047\n",
      "Training Epoch: 32 [33850/36045]\tLoss: 612.1297\n",
      "Training Epoch: 32 [33900/36045]\tLoss: 624.8975\n",
      "Training Epoch: 32 [33950/36045]\tLoss: 635.5317\n",
      "Training Epoch: 32 [34000/36045]\tLoss: 622.1260\n",
      "Training Epoch: 32 [34050/36045]\tLoss: 626.2576\n",
      "Training Epoch: 32 [34100/36045]\tLoss: 603.0405\n",
      "Training Epoch: 32 [34150/36045]\tLoss: 559.8806\n",
      "Training Epoch: 32 [34200/36045]\tLoss: 530.1964\n",
      "Training Epoch: 32 [34250/36045]\tLoss: 544.5995\n",
      "Training Epoch: 32 [34300/36045]\tLoss: 465.6083\n",
      "Training Epoch: 32 [34350/36045]\tLoss: 490.3740\n",
      "Training Epoch: 32 [34400/36045]\tLoss: 482.1628\n",
      "Training Epoch: 32 [34450/36045]\tLoss: 453.3960\n",
      "Training Epoch: 32 [34500/36045]\tLoss: 483.8695\n",
      "Training Epoch: 32 [34550/36045]\tLoss: 474.8925\n",
      "Training Epoch: 32 [34600/36045]\tLoss: 479.5760\n",
      "Training Epoch: 32 [34650/36045]\tLoss: 586.7204\n",
      "Training Epoch: 32 [34700/36045]\tLoss: 621.6167\n",
      "Training Epoch: 32 [34750/36045]\tLoss: 551.2853\n",
      "Training Epoch: 32 [34800/36045]\tLoss: 631.5729\n",
      "Training Epoch: 32 [34850/36045]\tLoss: 639.4367\n",
      "Training Epoch: 32 [34900/36045]\tLoss: 698.0055\n",
      "Training Epoch: 32 [34950/36045]\tLoss: 683.9265\n",
      "Training Epoch: 32 [35000/36045]\tLoss: 684.7760\n",
      "Training Epoch: 32 [35050/36045]\tLoss: 671.0866\n",
      "Training Epoch: 32 [35100/36045]\tLoss: 571.4439\n",
      "Training Epoch: 32 [35150/36045]\tLoss: 564.3968\n",
      "Training Epoch: 32 [35200/36045]\tLoss: 477.0515\n",
      "Training Epoch: 32 [35250/36045]\tLoss: 523.7914\n",
      "Training Epoch: 32 [35300/36045]\tLoss: 538.4988\n",
      "Training Epoch: 32 [35350/36045]\tLoss: 607.4891\n",
      "Training Epoch: 32 [35400/36045]\tLoss: 640.8827\n",
      "Training Epoch: 32 [35450/36045]\tLoss: 611.4663\n",
      "Training Epoch: 32 [35500/36045]\tLoss: 592.7200\n",
      "Training Epoch: 32 [35550/36045]\tLoss: 578.0771\n",
      "Training Epoch: 32 [35600/36045]\tLoss: 626.7352\n",
      "Training Epoch: 32 [35650/36045]\tLoss: 700.1382\n",
      "Training Epoch: 32 [35700/36045]\tLoss: 627.5849\n",
      "Training Epoch: 32 [35750/36045]\tLoss: 685.4068\n",
      "Training Epoch: 32 [35800/36045]\tLoss: 691.8588\n",
      "Training Epoch: 32 [35850/36045]\tLoss: 665.7873\n",
      "Training Epoch: 32 [35900/36045]\tLoss: 690.5168\n",
      "Training Epoch: 32 [35950/36045]\tLoss: 688.0413\n",
      "Training Epoch: 32 [36000/36045]\tLoss: 680.5291\n",
      "Training Epoch: 32 [36045/36045]\tLoss: 664.5421\n",
      "Training Epoch: 32 [4004/4004]\tLoss: 618.1038\n",
      "Training Epoch: 33 [50/36045]\tLoss: 617.3000\n",
      "Training Epoch: 33 [100/36045]\tLoss: 591.9564\n",
      "Training Epoch: 33 [150/36045]\tLoss: 590.2543\n",
      "Training Epoch: 33 [200/36045]\tLoss: 576.8884\n",
      "Training Epoch: 33 [250/36045]\tLoss: 689.8274\n",
      "Training Epoch: 33 [300/36045]\tLoss: 754.0441\n",
      "Training Epoch: 33 [350/36045]\tLoss: 720.0881\n",
      "Training Epoch: 33 [400/36045]\tLoss: 715.3983\n",
      "Training Epoch: 33 [450/36045]\tLoss: 695.7674\n",
      "Training Epoch: 33 [500/36045]\tLoss: 646.2402\n",
      "Training Epoch: 33 [550/36045]\tLoss: 649.6003\n",
      "Training Epoch: 33 [600/36045]\tLoss: 632.4866\n",
      "Training Epoch: 33 [650/36045]\tLoss: 655.2105\n",
      "Training Epoch: 33 [700/36045]\tLoss: 641.7256\n",
      "Training Epoch: 33 [750/36045]\tLoss: 621.2887\n",
      "Training Epoch: 33 [800/36045]\tLoss: 634.3245\n",
      "Training Epoch: 33 [850/36045]\tLoss: 615.7407\n",
      "Training Epoch: 33 [900/36045]\tLoss: 587.9399\n",
      "Training Epoch: 33 [950/36045]\tLoss: 556.9277\n",
      "Training Epoch: 33 [1000/36045]\tLoss: 538.5838\n",
      "Training Epoch: 33 [1050/36045]\tLoss: 540.7114\n",
      "Training Epoch: 33 [1100/36045]\tLoss: 526.0657\n",
      "Training Epoch: 33 [1150/36045]\tLoss: 534.7879\n",
      "Training Epoch: 33 [1200/36045]\tLoss: 565.1271\n",
      "Training Epoch: 33 [1250/36045]\tLoss: 646.9529\n",
      "Training Epoch: 33 [1300/36045]\tLoss: 653.8855\n",
      "Training Epoch: 33 [1350/36045]\tLoss: 655.8633\n",
      "Training Epoch: 33 [1400/36045]\tLoss: 681.3712\n",
      "Training Epoch: 33 [1450/36045]\tLoss: 658.9250\n",
      "Training Epoch: 33 [1500/36045]\tLoss: 603.5760\n",
      "Training Epoch: 33 [1550/36045]\tLoss: 618.9470\n",
      "Training Epoch: 33 [1600/36045]\tLoss: 629.7831\n",
      "Training Epoch: 33 [1650/36045]\tLoss: 616.9239\n",
      "Training Epoch: 33 [1700/36045]\tLoss: 629.1938\n",
      "Training Epoch: 33 [1750/36045]\tLoss: 670.5383\n",
      "Training Epoch: 33 [1800/36045]\tLoss: 651.8823\n",
      "Training Epoch: 33 [1850/36045]\tLoss: 668.5852\n",
      "Training Epoch: 33 [1900/36045]\tLoss: 625.9501\n",
      "Training Epoch: 33 [1950/36045]\tLoss: 636.8456\n",
      "Training Epoch: 33 [2000/36045]\tLoss: 575.3954\n",
      "Training Epoch: 33 [2050/36045]\tLoss: 577.6923\n",
      "Training Epoch: 33 [2100/36045]\tLoss: 608.3708\n",
      "Training Epoch: 33 [2150/36045]\tLoss: 588.2279\n",
      "Training Epoch: 33 [2200/36045]\tLoss: 547.4271\n",
      "Training Epoch: 33 [2250/36045]\tLoss: 516.9654\n",
      "Training Epoch: 33 [2300/36045]\tLoss: 542.1794\n",
      "Training Epoch: 33 [2350/36045]\tLoss: 517.9835\n",
      "Training Epoch: 33 [2400/36045]\tLoss: 526.3101\n",
      "Training Epoch: 33 [2450/36045]\tLoss: 671.8546\n",
      "Training Epoch: 33 [2500/36045]\tLoss: 706.0453\n",
      "Training Epoch: 33 [2550/36045]\tLoss: 703.5702\n",
      "Training Epoch: 33 [2600/36045]\tLoss: 712.2435\n",
      "Training Epoch: 33 [2650/36045]\tLoss: 837.2554\n",
      "Training Epoch: 33 [2700/36045]\tLoss: 924.5084\n",
      "Training Epoch: 33 [2750/36045]\tLoss: 996.0445\n",
      "Training Epoch: 33 [2800/36045]\tLoss: 1005.9211\n",
      "Training Epoch: 33 [2850/36045]\tLoss: 774.4490\n",
      "Training Epoch: 33 [2900/36045]\tLoss: 737.9214\n",
      "Training Epoch: 33 [2950/36045]\tLoss: 712.5279\n",
      "Training Epoch: 33 [3000/36045]\tLoss: 706.6751\n",
      "Training Epoch: 33 [3050/36045]\tLoss: 737.5548\n",
      "Training Epoch: 33 [3100/36045]\tLoss: 674.9655\n",
      "Training Epoch: 33 [3150/36045]\tLoss: 520.7491\n",
      "Training Epoch: 33 [3200/36045]\tLoss: 539.6974\n",
      "Training Epoch: 33 [3250/36045]\tLoss: 508.2126\n",
      "Training Epoch: 33 [3300/36045]\tLoss: 480.8488\n",
      "Training Epoch: 33 [3350/36045]\tLoss: 507.4308\n",
      "Training Epoch: 33 [3400/36045]\tLoss: 532.4883\n",
      "Training Epoch: 33 [3450/36045]\tLoss: 571.9574\n",
      "Training Epoch: 33 [3500/36045]\tLoss: 559.2494\n",
      "Training Epoch: 33 [3550/36045]\tLoss: 535.5616\n",
      "Training Epoch: 33 [3600/36045]\tLoss: 574.2399\n",
      "Training Epoch: 33 [3650/36045]\tLoss: 663.7160\n",
      "Training Epoch: 33 [3700/36045]\tLoss: 670.9084\n",
      "Training Epoch: 33 [3750/36045]\tLoss: 640.0062\n",
      "Training Epoch: 33 [3800/36045]\tLoss: 635.5623\n",
      "Training Epoch: 33 [3850/36045]\tLoss: 635.2618\n",
      "Training Epoch: 33 [3900/36045]\tLoss: 640.2104\n",
      "Training Epoch: 33 [3950/36045]\tLoss: 617.6676\n",
      "Training Epoch: 33 [4000/36045]\tLoss: 623.1913\n",
      "Training Epoch: 33 [4050/36045]\tLoss: 572.3779\n",
      "Training Epoch: 33 [4100/36045]\tLoss: 558.0717\n",
      "Training Epoch: 33 [4150/36045]\tLoss: 573.5645\n",
      "Training Epoch: 33 [4200/36045]\tLoss: 568.2710\n",
      "Training Epoch: 33 [4250/36045]\tLoss: 570.5767\n",
      "Training Epoch: 33 [4300/36045]\tLoss: 588.1838\n",
      "Training Epoch: 33 [4350/36045]\tLoss: 571.1047\n",
      "Training Epoch: 33 [4400/36045]\tLoss: 546.5090\n",
      "Training Epoch: 33 [4450/36045]\tLoss: 598.4487\n",
      "Training Epoch: 33 [4500/36045]\tLoss: 641.7852\n",
      "Training Epoch: 33 [4550/36045]\tLoss: 646.0834\n",
      "Training Epoch: 33 [4600/36045]\tLoss: 668.7833\n",
      "Training Epoch: 33 [4650/36045]\tLoss: 658.2299\n",
      "Training Epoch: 33 [4700/36045]\tLoss: 607.4641\n",
      "Training Epoch: 33 [4750/36045]\tLoss: 589.7521\n",
      "Training Epoch: 33 [4800/36045]\tLoss: 615.1839\n",
      "Training Epoch: 33 [4850/36045]\tLoss: 601.7915\n",
      "Training Epoch: 33 [4900/36045]\tLoss: 585.5885\n",
      "Training Epoch: 33 [4950/36045]\tLoss: 601.6390\n",
      "Training Epoch: 33 [5000/36045]\tLoss: 631.6450\n",
      "Training Epoch: 33 [5050/36045]\tLoss: 611.8456\n",
      "Training Epoch: 33 [5100/36045]\tLoss: 622.3036\n",
      "Training Epoch: 33 [5150/36045]\tLoss: 606.7079\n",
      "Training Epoch: 33 [5200/36045]\tLoss: 604.7200\n",
      "Training Epoch: 33 [5250/36045]\tLoss: 598.2459\n",
      "Training Epoch: 33 [5300/36045]\tLoss: 598.6443\n",
      "Training Epoch: 33 [5350/36045]\tLoss: 621.0508\n",
      "Training Epoch: 33 [5400/36045]\tLoss: 598.2112\n",
      "Training Epoch: 33 [5450/36045]\tLoss: 567.2669\n",
      "Training Epoch: 33 [5500/36045]\tLoss: 595.9667\n",
      "Training Epoch: 33 [5550/36045]\tLoss: 584.1064\n",
      "Training Epoch: 33 [5600/36045]\tLoss: 665.5502\n",
      "Training Epoch: 33 [5650/36045]\tLoss: 629.9188\n",
      "Training Epoch: 33 [5700/36045]\tLoss: 591.0636\n",
      "Training Epoch: 33 [5750/36045]\tLoss: 575.4594\n",
      "Training Epoch: 33 [5800/36045]\tLoss: 607.1267\n",
      "Training Epoch: 33 [5850/36045]\tLoss: 594.8592\n",
      "Training Epoch: 33 [5900/36045]\tLoss: 684.1014\n",
      "Training Epoch: 33 [5950/36045]\tLoss: 701.2039\n",
      "Training Epoch: 33 [6000/36045]\tLoss: 686.5645\n",
      "Training Epoch: 33 [6050/36045]\tLoss: 663.8935\n",
      "Training Epoch: 33 [6100/36045]\tLoss: 668.2696\n",
      "Training Epoch: 33 [6150/36045]\tLoss: 656.0067\n",
      "Training Epoch: 33 [6200/36045]\tLoss: 658.8083\n",
      "Training Epoch: 33 [6250/36045]\tLoss: 680.2710\n",
      "Training Epoch: 33 [6300/36045]\tLoss: 691.9933\n",
      "Training Epoch: 33 [6350/36045]\tLoss: 738.7661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 33 [6400/36045]\tLoss: 612.0306\n",
      "Training Epoch: 33 [6450/36045]\tLoss: 564.5646\n",
      "Training Epoch: 33 [6500/36045]\tLoss: 574.9114\n",
      "Training Epoch: 33 [6550/36045]\tLoss: 592.1683\n",
      "Training Epoch: 33 [6600/36045]\tLoss: 590.9376\n",
      "Training Epoch: 33 [6650/36045]\tLoss: 666.7253\n",
      "Training Epoch: 33 [6700/36045]\tLoss: 697.4833\n",
      "Training Epoch: 33 [6750/36045]\tLoss: 673.7585\n",
      "Training Epoch: 33 [6800/36045]\tLoss: 676.6924\n",
      "Training Epoch: 33 [6850/36045]\tLoss: 664.6377\n",
      "Training Epoch: 33 [6900/36045]\tLoss: 592.1125\n",
      "Training Epoch: 33 [6950/36045]\tLoss: 558.1992\n",
      "Training Epoch: 33 [7000/36045]\tLoss: 593.8864\n",
      "Training Epoch: 33 [7050/36045]\tLoss: 606.9752\n",
      "Training Epoch: 33 [7100/36045]\tLoss: 606.4199\n",
      "Training Epoch: 33 [7150/36045]\tLoss: 616.9996\n",
      "Training Epoch: 33 [7200/36045]\tLoss: 620.1500\n",
      "Training Epoch: 33 [7250/36045]\tLoss: 618.0073\n",
      "Training Epoch: 33 [7300/36045]\tLoss: 604.6947\n",
      "Training Epoch: 33 [7350/36045]\tLoss: 601.5012\n",
      "Training Epoch: 33 [7400/36045]\tLoss: 546.6096\n",
      "Training Epoch: 33 [7450/36045]\tLoss: 549.8707\n",
      "Training Epoch: 33 [7500/36045]\tLoss: 545.0098\n",
      "Training Epoch: 33 [7550/36045]\tLoss: 522.1412\n",
      "Training Epoch: 33 [7600/36045]\tLoss: 579.3813\n",
      "Training Epoch: 33 [7650/36045]\tLoss: 620.2747\n",
      "Training Epoch: 33 [7700/36045]\tLoss: 590.5298\n",
      "Training Epoch: 33 [7750/36045]\tLoss: 605.1499\n",
      "Training Epoch: 33 [7800/36045]\tLoss: 593.9582\n",
      "Training Epoch: 33 [7850/36045]\tLoss: 574.8937\n",
      "Training Epoch: 33 [7900/36045]\tLoss: 606.3914\n",
      "Training Epoch: 33 [7950/36045]\tLoss: 603.7875\n",
      "Training Epoch: 33 [8000/36045]\tLoss: 621.8981\n",
      "Training Epoch: 33 [8050/36045]\tLoss: 586.5563\n",
      "Training Epoch: 33 [8100/36045]\tLoss: 612.0228\n",
      "Training Epoch: 33 [8150/36045]\tLoss: 693.1056\n",
      "Training Epoch: 33 [8200/36045]\tLoss: 679.9642\n",
      "Training Epoch: 33 [8250/36045]\tLoss: 647.5588\n",
      "Training Epoch: 33 [8300/36045]\tLoss: 706.4672\n",
      "Training Epoch: 33 [8350/36045]\tLoss: 648.5296\n",
      "Training Epoch: 33 [8400/36045]\tLoss: 580.9734\n",
      "Training Epoch: 33 [8450/36045]\tLoss: 544.0853\n",
      "Training Epoch: 33 [8500/36045]\tLoss: 578.2976\n",
      "Training Epoch: 33 [8550/36045]\tLoss: 570.7487\n",
      "Training Epoch: 33 [8600/36045]\tLoss: 564.7056\n",
      "Training Epoch: 33 [8650/36045]\tLoss: 601.5798\n",
      "Training Epoch: 33 [8700/36045]\tLoss: 636.1812\n",
      "Training Epoch: 33 [8750/36045]\tLoss: 624.7646\n",
      "Training Epoch: 33 [8800/36045]\tLoss: 630.5445\n",
      "Training Epoch: 33 [8850/36045]\tLoss: 623.9061\n",
      "Training Epoch: 33 [8900/36045]\tLoss: 563.1870\n",
      "Training Epoch: 33 [8950/36045]\tLoss: 575.0829\n",
      "Training Epoch: 33 [9000/36045]\tLoss: 590.7357\n",
      "Training Epoch: 33 [9050/36045]\tLoss: 592.0372\n",
      "Training Epoch: 33 [9100/36045]\tLoss: 609.4566\n",
      "Training Epoch: 33 [9150/36045]\tLoss: 450.7636\n",
      "Training Epoch: 33 [9200/36045]\tLoss: 337.5185\n",
      "Training Epoch: 33 [9250/36045]\tLoss: 366.0752\n",
      "Training Epoch: 33 [9300/36045]\tLoss: 376.5090\n",
      "Training Epoch: 33 [9350/36045]\tLoss: 347.2017\n",
      "Training Epoch: 33 [9400/36045]\tLoss: 680.3895\n",
      "Training Epoch: 33 [9450/36045]\tLoss: 722.6989\n",
      "Training Epoch: 33 [9500/36045]\tLoss: 709.8033\n",
      "Training Epoch: 33 [9550/36045]\tLoss: 750.8793\n",
      "Training Epoch: 33 [9600/36045]\tLoss: 558.2773\n",
      "Training Epoch: 33 [9650/36045]\tLoss: 562.9365\n",
      "Training Epoch: 33 [9700/36045]\tLoss: 548.3721\n",
      "Training Epoch: 33 [9750/36045]\tLoss: 547.3707\n",
      "Training Epoch: 33 [9800/36045]\tLoss: 715.5723\n",
      "Training Epoch: 33 [9850/36045]\tLoss: 755.8386\n",
      "Training Epoch: 33 [9900/36045]\tLoss: 767.4934\n",
      "Training Epoch: 33 [9950/36045]\tLoss: 747.8187\n",
      "Training Epoch: 33 [10000/36045]\tLoss: 691.2043\n",
      "Training Epoch: 33 [10050/36045]\tLoss: 567.9317\n",
      "Training Epoch: 33 [10100/36045]\tLoss: 575.4053\n",
      "Training Epoch: 33 [10150/36045]\tLoss: 584.5010\n",
      "Training Epoch: 33 [10200/36045]\tLoss: 573.4022\n",
      "Training Epoch: 33 [10250/36045]\tLoss: 686.6601\n",
      "Training Epoch: 33 [10300/36045]\tLoss: 666.9979\n",
      "Training Epoch: 33 [10350/36045]\tLoss: 702.3969\n",
      "Training Epoch: 33 [10400/36045]\tLoss: 692.8348\n",
      "Training Epoch: 33 [10450/36045]\tLoss: 649.0910\n",
      "Training Epoch: 33 [10500/36045]\tLoss: 542.7645\n",
      "Training Epoch: 33 [10550/36045]\tLoss: 537.6840\n",
      "Training Epoch: 33 [10600/36045]\tLoss: 560.2460\n",
      "Training Epoch: 33 [10650/36045]\tLoss: 566.4345\n",
      "Training Epoch: 33 [10700/36045]\tLoss: 649.5859\n",
      "Training Epoch: 33 [10750/36045]\tLoss: 710.0873\n",
      "Training Epoch: 33 [10800/36045]\tLoss: 654.6420\n",
      "Training Epoch: 33 [10850/36045]\tLoss: 693.6205\n",
      "Training Epoch: 33 [10900/36045]\tLoss: 721.9218\n",
      "Training Epoch: 33 [10950/36045]\tLoss: 532.7103\n",
      "Training Epoch: 33 [11000/36045]\tLoss: 526.3309\n",
      "Training Epoch: 33 [11050/36045]\tLoss: 563.9775\n",
      "Training Epoch: 33 [11100/36045]\tLoss: 574.8298\n",
      "Training Epoch: 33 [11150/36045]\tLoss: 623.2935\n",
      "Training Epoch: 33 [11200/36045]\tLoss: 651.3012\n",
      "Training Epoch: 33 [11250/36045]\tLoss: 663.1838\n",
      "Training Epoch: 33 [11300/36045]\tLoss: 643.2585\n",
      "Training Epoch: 33 [11350/36045]\tLoss: 640.6534\n",
      "Training Epoch: 33 [11400/36045]\tLoss: 602.9365\n",
      "Training Epoch: 33 [11450/36045]\tLoss: 571.0804\n",
      "Training Epoch: 33 [11500/36045]\tLoss: 568.4844\n",
      "Training Epoch: 33 [11550/36045]\tLoss: 579.3522\n",
      "Training Epoch: 33 [11600/36045]\tLoss: 640.4966\n",
      "Training Epoch: 33 [11650/36045]\tLoss: 692.5021\n",
      "Training Epoch: 33 [11700/36045]\tLoss: 691.4470\n",
      "Training Epoch: 33 [11750/36045]\tLoss: 710.2252\n",
      "Training Epoch: 33 [11800/36045]\tLoss: 752.9982\n",
      "Training Epoch: 33 [11850/36045]\tLoss: 810.7727\n",
      "Training Epoch: 33 [11900/36045]\tLoss: 1025.2075\n",
      "Training Epoch: 33 [11950/36045]\tLoss: 1027.6852\n",
      "Training Epoch: 33 [12000/36045]\tLoss: 1040.3059\n",
      "Training Epoch: 33 [12050/36045]\tLoss: 999.1214\n",
      "Training Epoch: 33 [12100/36045]\tLoss: 646.1744\n",
      "Training Epoch: 33 [12150/36045]\tLoss: 491.1072\n",
      "Training Epoch: 33 [12200/36045]\tLoss: 485.7653\n",
      "Training Epoch: 33 [12250/36045]\tLoss: 494.6577\n",
      "Training Epoch: 33 [12300/36045]\tLoss: 633.8743\n",
      "Training Epoch: 33 [12350/36045]\tLoss: 690.4827\n",
      "Training Epoch: 33 [12400/36045]\tLoss: 698.0701\n",
      "Training Epoch: 33 [12450/36045]\tLoss: 686.5856\n",
      "Training Epoch: 33 [12500/36045]\tLoss: 714.3963\n",
      "Training Epoch: 33 [12550/36045]\tLoss: 683.7314\n",
      "Training Epoch: 33 [12600/36045]\tLoss: 627.9000\n",
      "Training Epoch: 33 [12650/36045]\tLoss: 626.5392\n",
      "Training Epoch: 33 [12700/36045]\tLoss: 647.9795\n",
      "Training Epoch: 33 [12750/36045]\tLoss: 646.9122\n",
      "Training Epoch: 33 [12800/36045]\tLoss: 631.2590\n",
      "Training Epoch: 33 [12850/36045]\tLoss: 660.5269\n",
      "Training Epoch: 33 [12900/36045]\tLoss: 633.5132\n",
      "Training Epoch: 33 [12950/36045]\tLoss: 620.1898\n",
      "Training Epoch: 33 [13000/36045]\tLoss: 652.9120\n",
      "Training Epoch: 33 [13050/36045]\tLoss: 591.7797\n",
      "Training Epoch: 33 [13100/36045]\tLoss: 609.1664\n",
      "Training Epoch: 33 [13150/36045]\tLoss: 600.8648\n",
      "Training Epoch: 33 [13200/36045]\tLoss: 582.5131\n",
      "Training Epoch: 33 [13250/36045]\tLoss: 605.8770\n",
      "Training Epoch: 33 [13300/36045]\tLoss: 644.2253\n",
      "Training Epoch: 33 [13350/36045]\tLoss: 624.4111\n",
      "Training Epoch: 33 [13400/36045]\tLoss: 627.8208\n",
      "Training Epoch: 33 [13450/36045]\tLoss: 624.6588\n",
      "Training Epoch: 33 [13500/36045]\tLoss: 644.6186\n",
      "Training Epoch: 33 [13550/36045]\tLoss: 781.5252\n",
      "Training Epoch: 33 [13600/36045]\tLoss: 814.3481\n",
      "Training Epoch: 33 [13650/36045]\tLoss: 895.2028\n",
      "Training Epoch: 33 [13700/36045]\tLoss: 790.3979\n",
      "Training Epoch: 33 [13750/36045]\tLoss: 630.9963\n",
      "Training Epoch: 33 [13800/36045]\tLoss: 602.9290\n",
      "Training Epoch: 33 [13850/36045]\tLoss: 585.7364\n",
      "Training Epoch: 33 [13900/36045]\tLoss: 593.0143\n",
      "Training Epoch: 33 [13950/36045]\tLoss: 638.3961\n",
      "Training Epoch: 33 [14000/36045]\tLoss: 671.8666\n",
      "Training Epoch: 33 [14050/36045]\tLoss: 646.3470\n",
      "Training Epoch: 33 [14100/36045]\tLoss: 641.7259\n",
      "Training Epoch: 33 [14150/36045]\tLoss: 629.6404\n",
      "Training Epoch: 33 [14200/36045]\tLoss: 671.0404\n",
      "Training Epoch: 33 [14250/36045]\tLoss: 736.6802\n",
      "Training Epoch: 33 [14300/36045]\tLoss: 740.0435\n",
      "Training Epoch: 33 [14350/36045]\tLoss: 707.9353\n",
      "Training Epoch: 33 [14400/36045]\tLoss: 693.6339\n",
      "Training Epoch: 33 [14450/36045]\tLoss: 729.8986\n",
      "Training Epoch: 33 [14500/36045]\tLoss: 660.7321\n",
      "Training Epoch: 33 [14550/36045]\tLoss: 690.2219\n",
      "Training Epoch: 33 [14600/36045]\tLoss: 676.2877\n",
      "Training Epoch: 33 [14650/36045]\tLoss: 676.1675\n",
      "Training Epoch: 33 [14700/36045]\tLoss: 640.0116\n",
      "Training Epoch: 33 [14750/36045]\tLoss: 550.0568\n",
      "Training Epoch: 33 [14800/36045]\tLoss: 540.1889\n",
      "Training Epoch: 33 [14850/36045]\tLoss: 547.2460\n",
      "Training Epoch: 33 [14900/36045]\tLoss: 540.7686\n",
      "Training Epoch: 33 [14950/36045]\tLoss: 548.6570\n",
      "Training Epoch: 33 [15000/36045]\tLoss: 562.3683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 33 [15050/36045]\tLoss: 559.6817\n",
      "Training Epoch: 33 [15100/36045]\tLoss: 543.7637\n",
      "Training Epoch: 33 [15150/36045]\tLoss: 538.3938\n",
      "Training Epoch: 33 [15200/36045]\tLoss: 498.4794\n",
      "Training Epoch: 33 [15250/36045]\tLoss: 521.0609\n",
      "Training Epoch: 33 [15300/36045]\tLoss: 506.1651\n",
      "Training Epoch: 33 [15350/36045]\tLoss: 518.0779\n",
      "Training Epoch: 33 [15400/36045]\tLoss: 501.7011\n",
      "Training Epoch: 33 [15450/36045]\tLoss: 487.0963\n",
      "Training Epoch: 33 [15500/36045]\tLoss: 501.1901\n",
      "Training Epoch: 33 [15550/36045]\tLoss: 497.1663\n",
      "Training Epoch: 33 [15600/36045]\tLoss: 565.9811\n",
      "Training Epoch: 33 [15650/36045]\tLoss: 583.6827\n",
      "Training Epoch: 33 [15700/36045]\tLoss: 575.2344\n",
      "Training Epoch: 33 [15750/36045]\tLoss: 566.8323\n",
      "Training Epoch: 33 [15800/36045]\tLoss: 538.5119\n",
      "Training Epoch: 33 [15850/36045]\tLoss: 553.0679\n",
      "Training Epoch: 33 [15900/36045]\tLoss: 562.3997\n",
      "Training Epoch: 33 [15950/36045]\tLoss: 582.1907\n",
      "Training Epoch: 33 [16000/36045]\tLoss: 554.5762\n",
      "Training Epoch: 33 [16050/36045]\tLoss: 523.9401\n",
      "Training Epoch: 33 [16100/36045]\tLoss: 485.7977\n",
      "Training Epoch: 33 [16150/36045]\tLoss: 473.6623\n",
      "Training Epoch: 33 [16200/36045]\tLoss: 574.1882\n",
      "Training Epoch: 33 [16250/36045]\tLoss: 602.5107\n",
      "Training Epoch: 33 [16300/36045]\tLoss: 657.7889\n",
      "Training Epoch: 33 [16350/36045]\tLoss: 677.2394\n",
      "Training Epoch: 33 [16400/36045]\tLoss: 649.0410\n",
      "Training Epoch: 33 [16450/36045]\tLoss: 630.9467\n",
      "Training Epoch: 33 [16500/36045]\tLoss: 630.8339\n",
      "Training Epoch: 33 [16550/36045]\tLoss: 595.6278\n",
      "Training Epoch: 33 [16600/36045]\tLoss: 619.0788\n",
      "Training Epoch: 33 [16650/36045]\tLoss: 636.3938\n",
      "Training Epoch: 33 [16700/36045]\tLoss: 615.0644\n",
      "Training Epoch: 33 [16750/36045]\tLoss: 607.5619\n",
      "Training Epoch: 33 [16800/36045]\tLoss: 617.5150\n",
      "Training Epoch: 33 [16850/36045]\tLoss: 588.2102\n",
      "Training Epoch: 33 [16900/36045]\tLoss: 598.2242\n",
      "Training Epoch: 33 [16950/36045]\tLoss: 621.9108\n",
      "Training Epoch: 33 [17000/36045]\tLoss: 605.3592\n",
      "Training Epoch: 33 [17050/36045]\tLoss: 631.0776\n",
      "Training Epoch: 33 [17100/36045]\tLoss: 627.3618\n",
      "Training Epoch: 33 [17150/36045]\tLoss: 545.3157\n",
      "Training Epoch: 33 [17200/36045]\tLoss: 506.8329\n",
      "Training Epoch: 33 [17250/36045]\tLoss: 530.4977\n",
      "Training Epoch: 33 [17300/36045]\tLoss: 561.2993\n",
      "Training Epoch: 33 [17350/36045]\tLoss: 540.2054\n",
      "Training Epoch: 33 [17400/36045]\tLoss: 559.6242\n",
      "Training Epoch: 33 [17450/36045]\tLoss: 578.9217\n",
      "Training Epoch: 33 [17500/36045]\tLoss: 567.0707\n",
      "Training Epoch: 33 [17550/36045]\tLoss: 566.0513\n",
      "Training Epoch: 33 [17600/36045]\tLoss: 559.7941\n",
      "Training Epoch: 33 [17650/36045]\tLoss: 576.1368\n",
      "Training Epoch: 33 [17700/36045]\tLoss: 555.1535\n",
      "Training Epoch: 33 [17750/36045]\tLoss: 571.7379\n",
      "Training Epoch: 33 [17800/36045]\tLoss: 562.5552\n",
      "Training Epoch: 33 [17850/36045]\tLoss: 575.2769\n",
      "Training Epoch: 33 [17900/36045]\tLoss: 603.7388\n",
      "Training Epoch: 33 [17950/36045]\tLoss: 615.6460\n",
      "Training Epoch: 33 [18000/36045]\tLoss: 605.9867\n",
      "Training Epoch: 33 [18050/36045]\tLoss: 668.3995\n",
      "Training Epoch: 33 [18100/36045]\tLoss: 670.1369\n",
      "Training Epoch: 33 [18150/36045]\tLoss: 681.4709\n",
      "Training Epoch: 33 [18200/36045]\tLoss: 663.4542\n",
      "Training Epoch: 33 [18250/36045]\tLoss: 684.3154\n",
      "Training Epoch: 33 [18300/36045]\tLoss: 636.5068\n",
      "Training Epoch: 33 [18350/36045]\tLoss: 708.1298\n",
      "Training Epoch: 33 [18400/36045]\tLoss: 681.3966\n",
      "Training Epoch: 33 [18450/36045]\tLoss: 661.2764\n",
      "Training Epoch: 33 [18500/36045]\tLoss: 660.2552\n",
      "Training Epoch: 33 [18550/36045]\tLoss: 647.4511\n",
      "Training Epoch: 33 [18600/36045]\tLoss: 636.8536\n",
      "Training Epoch: 33 [18650/36045]\tLoss: 683.2634\n",
      "Training Epoch: 33 [18700/36045]\tLoss: 718.8493\n",
      "Training Epoch: 33 [18750/36045]\tLoss: 705.6962\n",
      "Training Epoch: 33 [18800/36045]\tLoss: 729.2259\n",
      "Training Epoch: 33 [18850/36045]\tLoss: 673.7286\n",
      "Training Epoch: 33 [18900/36045]\tLoss: 720.7585\n",
      "Training Epoch: 33 [18950/36045]\tLoss: 662.6580\n",
      "Training Epoch: 33 [19000/36045]\tLoss: 550.5398\n",
      "Training Epoch: 33 [19050/36045]\tLoss: 534.1804\n",
      "Training Epoch: 33 [19100/36045]\tLoss: 542.5743\n",
      "Training Epoch: 33 [19150/36045]\tLoss: 532.3723\n",
      "Training Epoch: 33 [19200/36045]\tLoss: 561.9349\n",
      "Training Epoch: 33 [19250/36045]\tLoss: 576.6832\n",
      "Training Epoch: 33 [19300/36045]\tLoss: 586.7497\n",
      "Training Epoch: 33 [19350/36045]\tLoss: 570.2681\n",
      "Training Epoch: 33 [19400/36045]\tLoss: 591.5499\n",
      "Training Epoch: 33 [19450/36045]\tLoss: 582.6519\n",
      "Training Epoch: 33 [19500/36045]\tLoss: 584.1672\n",
      "Training Epoch: 33 [19550/36045]\tLoss: 582.6733\n",
      "Training Epoch: 33 [19600/36045]\tLoss: 623.6302\n",
      "Training Epoch: 33 [19650/36045]\tLoss: 827.7740\n",
      "Training Epoch: 33 [19700/36045]\tLoss: 786.5656\n",
      "Training Epoch: 33 [19750/36045]\tLoss: 790.0422\n",
      "Training Epoch: 33 [19800/36045]\tLoss: 789.7669\n",
      "Training Epoch: 33 [19850/36045]\tLoss: 521.7173\n",
      "Training Epoch: 33 [19900/36045]\tLoss: 500.1314\n",
      "Training Epoch: 33 [19950/36045]\tLoss: 503.7630\n",
      "Training Epoch: 33 [20000/36045]\tLoss: 502.8811\n",
      "Training Epoch: 33 [20050/36045]\tLoss: 563.1630\n",
      "Training Epoch: 33 [20100/36045]\tLoss: 569.2902\n",
      "Training Epoch: 33 [20150/36045]\tLoss: 571.0270\n",
      "Training Epoch: 33 [20200/36045]\tLoss: 570.8396\n",
      "Training Epoch: 33 [20250/36045]\tLoss: 608.0554\n",
      "Training Epoch: 33 [20300/36045]\tLoss: 644.0303\n",
      "Training Epoch: 33 [20350/36045]\tLoss: 662.8463\n",
      "Training Epoch: 33 [20400/36045]\tLoss: 679.5449\n",
      "Training Epoch: 33 [20450/36045]\tLoss: 649.9573\n",
      "Training Epoch: 33 [20500/36045]\tLoss: 634.0482\n",
      "Training Epoch: 33 [20550/36045]\tLoss: 557.6592\n",
      "Training Epoch: 33 [20600/36045]\tLoss: 568.1619\n",
      "Training Epoch: 33 [20650/36045]\tLoss: 565.2711\n",
      "Training Epoch: 33 [20700/36045]\tLoss: 553.2393\n",
      "Training Epoch: 33 [20750/36045]\tLoss: 595.9429\n",
      "Training Epoch: 33 [20800/36045]\tLoss: 647.5777\n",
      "Training Epoch: 33 [20850/36045]\tLoss: 634.2728\n",
      "Training Epoch: 33 [20900/36045]\tLoss: 678.7363\n",
      "Training Epoch: 33 [20950/36045]\tLoss: 640.0230\n",
      "Training Epoch: 33 [21000/36045]\tLoss: 602.7181\n",
      "Training Epoch: 33 [21050/36045]\tLoss: 515.9158\n",
      "Training Epoch: 33 [21100/36045]\tLoss: 519.9659\n",
      "Training Epoch: 33 [21150/36045]\tLoss: 556.5264\n",
      "Training Epoch: 33 [21200/36045]\tLoss: 555.7373\n",
      "Training Epoch: 33 [21250/36045]\tLoss: 531.7149\n",
      "Training Epoch: 33 [21300/36045]\tLoss: 620.9321\n",
      "Training Epoch: 33 [21350/36045]\tLoss: 613.0140\n",
      "Training Epoch: 33 [21400/36045]\tLoss: 616.4909\n",
      "Training Epoch: 33 [21450/36045]\tLoss: 622.6837\n",
      "Training Epoch: 33 [21500/36045]\tLoss: 625.1024\n",
      "Training Epoch: 33 [21550/36045]\tLoss: 719.9497\n",
      "Training Epoch: 33 [21600/36045]\tLoss: 718.9910\n",
      "Training Epoch: 33 [21650/36045]\tLoss: 731.5367\n",
      "Training Epoch: 33 [21700/36045]\tLoss: 733.9363\n",
      "Training Epoch: 33 [21750/36045]\tLoss: 704.9377\n",
      "Training Epoch: 33 [21800/36045]\tLoss: 519.7650\n",
      "Training Epoch: 33 [21850/36045]\tLoss: 502.7805\n",
      "Training Epoch: 33 [21900/36045]\tLoss: 512.6754\n",
      "Training Epoch: 33 [21950/36045]\tLoss: 512.8767\n",
      "Training Epoch: 33 [22000/36045]\tLoss: 516.3306\n",
      "Training Epoch: 33 [22050/36045]\tLoss: 538.1984\n",
      "Training Epoch: 33 [22100/36045]\tLoss: 530.8093\n",
      "Training Epoch: 33 [22150/36045]\tLoss: 516.3418\n",
      "Training Epoch: 33 [22200/36045]\tLoss: 532.8779\n",
      "Training Epoch: 33 [22250/36045]\tLoss: 537.8412\n",
      "Training Epoch: 33 [22300/36045]\tLoss: 590.1937\n",
      "Training Epoch: 33 [22350/36045]\tLoss: 616.1531\n",
      "Training Epoch: 33 [22400/36045]\tLoss: 630.8187\n",
      "Training Epoch: 33 [22450/36045]\tLoss: 618.3488\n",
      "Training Epoch: 33 [22500/36045]\tLoss: 600.5630\n",
      "Training Epoch: 33 [22550/36045]\tLoss: 636.2808\n",
      "Training Epoch: 33 [22600/36045]\tLoss: 689.1187\n",
      "Training Epoch: 33 [22650/36045]\tLoss: 723.8325\n",
      "Training Epoch: 33 [22700/36045]\tLoss: 746.2313\n",
      "Training Epoch: 33 [22750/36045]\tLoss: 765.7308\n",
      "Training Epoch: 33 [22800/36045]\tLoss: 796.0337\n",
      "Training Epoch: 33 [22850/36045]\tLoss: 662.0379\n",
      "Training Epoch: 33 [22900/36045]\tLoss: 666.8597\n",
      "Training Epoch: 33 [22950/36045]\tLoss: 646.2843\n",
      "Training Epoch: 33 [23000/36045]\tLoss: 643.4904\n",
      "Training Epoch: 33 [23050/36045]\tLoss: 571.7430\n",
      "Training Epoch: 33 [23100/36045]\tLoss: 587.6235\n",
      "Training Epoch: 33 [23150/36045]\tLoss: 575.8508\n",
      "Training Epoch: 33 [23200/36045]\tLoss: 545.3953\n",
      "Training Epoch: 33 [23250/36045]\tLoss: 548.4332\n",
      "Training Epoch: 33 [23300/36045]\tLoss: 544.7570\n",
      "Training Epoch: 33 [23350/36045]\tLoss: 565.7480\n",
      "Training Epoch: 33 [23400/36045]\tLoss: 613.1177\n",
      "Training Epoch: 33 [23450/36045]\tLoss: 606.3135\n",
      "Training Epoch: 33 [23500/36045]\tLoss: 584.4476\n",
      "Training Epoch: 33 [23550/36045]\tLoss: 626.6141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 33 [23600/36045]\tLoss: 708.8727\n",
      "Training Epoch: 33 [23650/36045]\tLoss: 721.6237\n",
      "Training Epoch: 33 [23700/36045]\tLoss: 729.9094\n",
      "Training Epoch: 33 [23750/36045]\tLoss: 705.4828\n",
      "Training Epoch: 33 [23800/36045]\tLoss: 564.1546\n",
      "Training Epoch: 33 [23850/36045]\tLoss: 590.4155\n",
      "Training Epoch: 33 [23900/36045]\tLoss: 580.0077\n",
      "Training Epoch: 33 [23950/36045]\tLoss: 563.0517\n",
      "Training Epoch: 33 [24000/36045]\tLoss: 539.7489\n",
      "Training Epoch: 33 [24050/36045]\tLoss: 498.6956\n",
      "Training Epoch: 33 [24100/36045]\tLoss: 524.5739\n",
      "Training Epoch: 33 [24150/36045]\tLoss: 517.5840\n",
      "Training Epoch: 33 [24200/36045]\tLoss: 514.1905\n",
      "Training Epoch: 33 [24250/36045]\tLoss: 499.1737\n",
      "Training Epoch: 33 [24300/36045]\tLoss: 539.2487\n",
      "Training Epoch: 33 [24350/36045]\tLoss: 552.3311\n",
      "Training Epoch: 33 [24400/36045]\tLoss: 567.8010\n",
      "Training Epoch: 33 [24450/36045]\tLoss: 541.2362\n",
      "Training Epoch: 33 [24500/36045]\tLoss: 570.7380\n",
      "Training Epoch: 33 [24550/36045]\tLoss: 659.7119\n",
      "Training Epoch: 33 [24600/36045]\tLoss: 651.5441\n",
      "Training Epoch: 33 [24650/36045]\tLoss: 624.4647\n",
      "Training Epoch: 33 [24700/36045]\tLoss: 634.3887\n",
      "Training Epoch: 33 [24750/36045]\tLoss: 586.1310\n",
      "Training Epoch: 33 [24800/36045]\tLoss: 483.2335\n",
      "Training Epoch: 33 [24850/36045]\tLoss: 502.5062\n",
      "Training Epoch: 33 [24900/36045]\tLoss: 499.7422\n",
      "Training Epoch: 33 [24950/36045]\tLoss: 502.3385\n",
      "Training Epoch: 33 [25000/36045]\tLoss: 482.8920\n",
      "Training Epoch: 33 [25050/36045]\tLoss: 461.2883\n",
      "Training Epoch: 33 [25100/36045]\tLoss: 413.3597\n",
      "Training Epoch: 33 [25150/36045]\tLoss: 382.8317\n",
      "Training Epoch: 33 [25200/36045]\tLoss: 377.9110\n",
      "Training Epoch: 33 [25250/36045]\tLoss: 405.0290\n",
      "Training Epoch: 33 [25300/36045]\tLoss: 531.9527\n",
      "Training Epoch: 33 [25350/36045]\tLoss: 528.5773\n",
      "Training Epoch: 33 [25400/36045]\tLoss: 493.0845\n",
      "Training Epoch: 33 [25450/36045]\tLoss: 495.5284\n",
      "Training Epoch: 33 [25500/36045]\tLoss: 538.4541\n",
      "Training Epoch: 33 [25550/36045]\tLoss: 629.3223\n",
      "Training Epoch: 33 [25600/36045]\tLoss: 633.7007\n",
      "Training Epoch: 33 [25650/36045]\tLoss: 611.4830\n",
      "Training Epoch: 33 [25700/36045]\tLoss: 620.6374\n",
      "Training Epoch: 33 [25750/36045]\tLoss: 598.5944\n",
      "Training Epoch: 33 [25800/36045]\tLoss: 377.7172\n",
      "Training Epoch: 33 [25850/36045]\tLoss: 387.0526\n",
      "Training Epoch: 33 [25900/36045]\tLoss: 368.0615\n",
      "Training Epoch: 33 [25950/36045]\tLoss: 376.8445\n",
      "Training Epoch: 33 [26000/36045]\tLoss: 462.2234\n",
      "Training Epoch: 33 [26050/36045]\tLoss: 629.3821\n",
      "Training Epoch: 33 [26100/36045]\tLoss: 656.7976\n",
      "Training Epoch: 33 [26150/36045]\tLoss: 657.0525\n",
      "Training Epoch: 33 [26200/36045]\tLoss: 630.0881\n",
      "Training Epoch: 33 [26250/36045]\tLoss: 659.9893\n",
      "Training Epoch: 33 [26300/36045]\tLoss: 599.0534\n",
      "Training Epoch: 33 [26350/36045]\tLoss: 609.9937\n",
      "Training Epoch: 33 [26400/36045]\tLoss: 586.9817\n",
      "Training Epoch: 33 [26450/36045]\tLoss: 516.7786\n",
      "Training Epoch: 33 [26500/36045]\tLoss: 613.2957\n",
      "Training Epoch: 33 [26550/36045]\tLoss: 613.8696\n",
      "Training Epoch: 33 [26600/36045]\tLoss: 610.1862\n",
      "Training Epoch: 33 [26650/36045]\tLoss: 626.2664\n",
      "Training Epoch: 33 [26700/36045]\tLoss: 605.7562\n",
      "Training Epoch: 33 [26750/36045]\tLoss: 567.3604\n",
      "Training Epoch: 33 [26800/36045]\tLoss: 418.0954\n",
      "Training Epoch: 33 [26850/36045]\tLoss: 346.6739\n",
      "Training Epoch: 33 [26900/36045]\tLoss: 349.4899\n",
      "Training Epoch: 33 [26950/36045]\tLoss: 384.1340\n",
      "Training Epoch: 33 [27000/36045]\tLoss: 627.5383\n",
      "Training Epoch: 33 [27050/36045]\tLoss: 655.5386\n",
      "Training Epoch: 33 [27100/36045]\tLoss: 635.1315\n",
      "Training Epoch: 33 [27150/36045]\tLoss: 675.3961\n",
      "Training Epoch: 33 [27200/36045]\tLoss: 493.0344\n",
      "Training Epoch: 33 [27250/36045]\tLoss: 484.6837\n",
      "Training Epoch: 33 [27300/36045]\tLoss: 472.1926\n",
      "Training Epoch: 33 [27350/36045]\tLoss: 470.5589\n",
      "Training Epoch: 33 [27400/36045]\tLoss: 469.7472\n",
      "Training Epoch: 33 [27450/36045]\tLoss: 593.4853\n",
      "Training Epoch: 33 [27500/36045]\tLoss: 635.9570\n",
      "Training Epoch: 33 [27550/36045]\tLoss: 629.0155\n",
      "Training Epoch: 33 [27600/36045]\tLoss: 640.7065\n",
      "Training Epoch: 33 [27650/36045]\tLoss: 632.5453\n",
      "Training Epoch: 33 [27700/36045]\tLoss: 661.3163\n",
      "Training Epoch: 33 [27750/36045]\tLoss: 673.3978\n",
      "Training Epoch: 33 [27800/36045]\tLoss: 660.1617\n",
      "Training Epoch: 33 [27850/36045]\tLoss: 649.9338\n",
      "Training Epoch: 33 [27900/36045]\tLoss: 588.1945\n",
      "Training Epoch: 33 [27950/36045]\tLoss: 490.0423\n",
      "Training Epoch: 33 [28000/36045]\tLoss: 466.5533\n",
      "Training Epoch: 33 [28050/36045]\tLoss: 476.6063\n",
      "Training Epoch: 33 [28100/36045]\tLoss: 468.2375\n",
      "Training Epoch: 33 [28150/36045]\tLoss: 490.7394\n",
      "Training Epoch: 33 [28200/36045]\tLoss: 498.0737\n",
      "Training Epoch: 33 [28250/36045]\tLoss: 491.3267\n",
      "Training Epoch: 33 [28300/36045]\tLoss: 466.3534\n",
      "Training Epoch: 33 [28350/36045]\tLoss: 462.4210\n",
      "Training Epoch: 33 [28400/36045]\tLoss: 792.7377\n",
      "Training Epoch: 33 [28450/36045]\tLoss: 726.1061\n",
      "Training Epoch: 33 [28500/36045]\tLoss: 627.4207\n",
      "Training Epoch: 33 [28550/36045]\tLoss: 576.3782\n",
      "Training Epoch: 33 [28600/36045]\tLoss: 606.3000\n",
      "Training Epoch: 33 [28650/36045]\tLoss: 669.7657\n",
      "Training Epoch: 33 [28700/36045]\tLoss: 663.9164\n",
      "Training Epoch: 33 [28750/36045]\tLoss: 650.3579\n",
      "Training Epoch: 33 [28800/36045]\tLoss: 659.2676\n",
      "Training Epoch: 33 [28850/36045]\tLoss: 571.8656\n",
      "Training Epoch: 33 [28900/36045]\tLoss: 464.2065\n",
      "Training Epoch: 33 [28950/36045]\tLoss: 463.1905\n",
      "Training Epoch: 33 [29000/36045]\tLoss: 460.1919\n",
      "Training Epoch: 33 [29050/36045]\tLoss: 467.0591\n",
      "Training Epoch: 33 [29100/36045]\tLoss: 485.7665\n",
      "Training Epoch: 33 [29150/36045]\tLoss: 474.8038\n",
      "Training Epoch: 33 [29200/36045]\tLoss: 460.4897\n",
      "Training Epoch: 33 [29250/36045]\tLoss: 450.1337\n",
      "Training Epoch: 33 [29300/36045]\tLoss: 509.8759\n",
      "Training Epoch: 33 [29350/36045]\tLoss: 600.9042\n",
      "Training Epoch: 33 [29400/36045]\tLoss: 618.4514\n",
      "Training Epoch: 33 [29450/36045]\tLoss: 636.3544\n",
      "Training Epoch: 33 [29500/36045]\tLoss: 651.0798\n",
      "Training Epoch: 33 [29550/36045]\tLoss: 619.6584\n",
      "Training Epoch: 33 [29600/36045]\tLoss: 522.6877\n",
      "Training Epoch: 33 [29650/36045]\tLoss: 504.8388\n",
      "Training Epoch: 33 [29700/36045]\tLoss: 451.4016\n",
      "Training Epoch: 33 [29750/36045]\tLoss: 450.1995\n",
      "Training Epoch: 33 [29800/36045]\tLoss: 497.1641\n",
      "Training Epoch: 33 [29850/36045]\tLoss: 573.2604\n",
      "Training Epoch: 33 [29900/36045]\tLoss: 569.7376\n",
      "Training Epoch: 33 [29950/36045]\tLoss: 591.6627\n",
      "Training Epoch: 33 [30000/36045]\tLoss: 566.2084\n",
      "Training Epoch: 33 [30050/36045]\tLoss: 572.5422\n",
      "Training Epoch: 33 [30100/36045]\tLoss: 698.5722\n",
      "Training Epoch: 33 [30150/36045]\tLoss: 681.9497\n",
      "Training Epoch: 33 [30200/36045]\tLoss: 643.4077\n",
      "Training Epoch: 33 [30250/36045]\tLoss: 692.0381\n",
      "Training Epoch: 33 [30300/36045]\tLoss: 676.8307\n",
      "Training Epoch: 33 [30350/36045]\tLoss: 522.3086\n",
      "Training Epoch: 33 [30400/36045]\tLoss: 506.8641\n",
      "Training Epoch: 33 [30450/36045]\tLoss: 508.7148\n",
      "Training Epoch: 33 [30500/36045]\tLoss: 475.1357\n",
      "Training Epoch: 33 [30550/36045]\tLoss: 440.4448\n",
      "Training Epoch: 33 [30600/36045]\tLoss: 431.3039\n",
      "Training Epoch: 33 [30650/36045]\tLoss: 421.2664\n",
      "Training Epoch: 33 [30700/36045]\tLoss: 439.1035\n",
      "Training Epoch: 33 [30750/36045]\tLoss: 425.9386\n",
      "Training Epoch: 33 [30800/36045]\tLoss: 452.5719\n",
      "Training Epoch: 33 [30850/36045]\tLoss: 444.2790\n",
      "Training Epoch: 33 [30900/36045]\tLoss: 456.7763\n",
      "Training Epoch: 33 [30950/36045]\tLoss: 479.8573\n",
      "Training Epoch: 33 [31000/36045]\tLoss: 471.7543\n",
      "Training Epoch: 33 [31050/36045]\tLoss: 394.1706\n",
      "Training Epoch: 33 [31100/36045]\tLoss: 384.7611\n",
      "Training Epoch: 33 [31150/36045]\tLoss: 392.1121\n",
      "Training Epoch: 33 [31200/36045]\tLoss: 487.8140\n",
      "Training Epoch: 33 [31250/36045]\tLoss: 632.9548\n",
      "Training Epoch: 33 [31300/36045]\tLoss: 603.5142\n",
      "Training Epoch: 33 [31350/36045]\tLoss: 619.4241\n",
      "Training Epoch: 33 [31400/36045]\tLoss: 598.8312\n",
      "Training Epoch: 33 [31450/36045]\tLoss: 615.0982\n",
      "Training Epoch: 33 [31500/36045]\tLoss: 627.5237\n",
      "Training Epoch: 33 [31550/36045]\tLoss: 635.0673\n",
      "Training Epoch: 33 [31600/36045]\tLoss: 597.0052\n",
      "Training Epoch: 33 [31650/36045]\tLoss: 638.4710\n",
      "Training Epoch: 33 [31700/36045]\tLoss: 462.4461\n",
      "Training Epoch: 33 [31750/36045]\tLoss: 382.5532\n",
      "Training Epoch: 33 [31800/36045]\tLoss: 364.9471\n",
      "Training Epoch: 33 [31850/36045]\tLoss: 373.5488\n",
      "Training Epoch: 33 [31900/36045]\tLoss: 587.2946\n",
      "Training Epoch: 33 [31950/36045]\tLoss: 758.9029\n",
      "Training Epoch: 33 [32000/36045]\tLoss: 868.4683\n",
      "Training Epoch: 33 [32050/36045]\tLoss: 823.3989\n",
      "Training Epoch: 33 [32100/36045]\tLoss: 813.7703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 33 [32150/36045]\tLoss: 629.8088\n",
      "Training Epoch: 33 [32200/36045]\tLoss: 632.9957\n",
      "Training Epoch: 33 [32250/36045]\tLoss: 643.5544\n",
      "Training Epoch: 33 [32300/36045]\tLoss: 625.3015\n",
      "Training Epoch: 33 [32350/36045]\tLoss: 620.8474\n",
      "Training Epoch: 33 [32400/36045]\tLoss: 582.5887\n",
      "Training Epoch: 33 [32450/36045]\tLoss: 479.8223\n",
      "Training Epoch: 33 [32500/36045]\tLoss: 461.1830\n",
      "Training Epoch: 33 [32550/36045]\tLoss: 463.4815\n",
      "Training Epoch: 33 [32600/36045]\tLoss: 460.2821\n",
      "Training Epoch: 33 [32650/36045]\tLoss: 592.5988\n",
      "Training Epoch: 33 [32700/36045]\tLoss: 646.6949\n",
      "Training Epoch: 33 [32750/36045]\tLoss: 616.2165\n",
      "Training Epoch: 33 [32800/36045]\tLoss: 632.0009\n",
      "Training Epoch: 33 [32850/36045]\tLoss: 583.4719\n",
      "Training Epoch: 33 [32900/36045]\tLoss: 467.8912\n",
      "Training Epoch: 33 [32950/36045]\tLoss: 489.8479\n",
      "Training Epoch: 33 [33000/36045]\tLoss: 489.1382\n",
      "Training Epoch: 33 [33050/36045]\tLoss: 464.7358\n",
      "Training Epoch: 33 [33100/36045]\tLoss: 528.1196\n",
      "Training Epoch: 33 [33150/36045]\tLoss: 717.3243\n",
      "Training Epoch: 33 [33200/36045]\tLoss: 698.5842\n",
      "Training Epoch: 33 [33250/36045]\tLoss: 719.8888\n",
      "Training Epoch: 33 [33300/36045]\tLoss: 766.7275\n",
      "Training Epoch: 33 [33350/36045]\tLoss: 587.8922\n",
      "Training Epoch: 33 [33400/36045]\tLoss: 430.4353\n",
      "Training Epoch: 33 [33450/36045]\tLoss: 425.7655\n",
      "Training Epoch: 33 [33500/36045]\tLoss: 438.2715\n",
      "Training Epoch: 33 [33550/36045]\tLoss: 454.3072\n",
      "Training Epoch: 33 [33600/36045]\tLoss: 456.1232\n",
      "Training Epoch: 33 [33650/36045]\tLoss: 608.6247\n",
      "Training Epoch: 33 [33700/36045]\tLoss: 588.7794\n",
      "Training Epoch: 33 [33750/36045]\tLoss: 609.6211\n",
      "Training Epoch: 33 [33800/36045]\tLoss: 605.8120\n",
      "Training Epoch: 33 [33850/36045]\tLoss: 608.0903\n",
      "Training Epoch: 33 [33900/36045]\tLoss: 621.0032\n",
      "Training Epoch: 33 [33950/36045]\tLoss: 631.5101\n",
      "Training Epoch: 33 [34000/36045]\tLoss: 618.0613\n",
      "Training Epoch: 33 [34050/36045]\tLoss: 622.1476\n",
      "Training Epoch: 33 [34100/36045]\tLoss: 599.1650\n",
      "Training Epoch: 33 [34150/36045]\tLoss: 556.1550\n",
      "Training Epoch: 33 [34200/36045]\tLoss: 526.6982\n",
      "Training Epoch: 33 [34250/36045]\tLoss: 541.0728\n",
      "Training Epoch: 33 [34300/36045]\tLoss: 462.4782\n",
      "Training Epoch: 33 [34350/36045]\tLoss: 487.0880\n",
      "Training Epoch: 33 [34400/36045]\tLoss: 478.9914\n",
      "Training Epoch: 33 [34450/36045]\tLoss: 450.4875\n",
      "Training Epoch: 33 [34500/36045]\tLoss: 480.7399\n",
      "Training Epoch: 33 [34550/36045]\tLoss: 471.8337\n",
      "Training Epoch: 33 [34600/36045]\tLoss: 476.7771\n",
      "Training Epoch: 33 [34650/36045]\tLoss: 583.7344\n",
      "Training Epoch: 33 [34700/36045]\tLoss: 618.5231\n",
      "Training Epoch: 33 [34750/36045]\tLoss: 548.4744\n",
      "Training Epoch: 33 [34800/36045]\tLoss: 628.5585\n",
      "Training Epoch: 33 [34850/36045]\tLoss: 636.3487\n",
      "Training Epoch: 33 [34900/36045]\tLoss: 693.7433\n",
      "Training Epoch: 33 [34950/36045]\tLoss: 679.5065\n",
      "Training Epoch: 33 [35000/36045]\tLoss: 680.3438\n",
      "Training Epoch: 33 [35050/36045]\tLoss: 666.7578\n",
      "Training Epoch: 33 [35100/36045]\tLoss: 568.6619\n",
      "Training Epoch: 33 [35150/36045]\tLoss: 561.5674\n",
      "Training Epoch: 33 [35200/36045]\tLoss: 474.3745\n",
      "Training Epoch: 33 [35250/36045]\tLoss: 520.8532\n",
      "Training Epoch: 33 [35300/36045]\tLoss: 535.6678\n",
      "Training Epoch: 33 [35350/36045]\tLoss: 603.7482\n",
      "Training Epoch: 33 [35400/36045]\tLoss: 636.8176\n",
      "Training Epoch: 33 [35450/36045]\tLoss: 607.5065\n",
      "Training Epoch: 33 [35500/36045]\tLoss: 588.6964\n",
      "Training Epoch: 33 [35550/36045]\tLoss: 574.1151\n",
      "Training Epoch: 33 [35600/36045]\tLoss: 622.9509\n",
      "Training Epoch: 33 [35650/36045]\tLoss: 696.2780\n",
      "Training Epoch: 33 [35700/36045]\tLoss: 623.7200\n",
      "Training Epoch: 33 [35750/36045]\tLoss: 681.4266\n",
      "Training Epoch: 33 [35800/36045]\tLoss: 687.8650\n",
      "Training Epoch: 33 [35850/36045]\tLoss: 661.7919\n",
      "Training Epoch: 33 [35900/36045]\tLoss: 686.2977\n",
      "Training Epoch: 33 [35950/36045]\tLoss: 683.7936\n",
      "Training Epoch: 33 [36000/36045]\tLoss: 676.3424\n",
      "Training Epoch: 33 [36045/36045]\tLoss: 660.4834\n",
      "Training Epoch: 33 [4004/4004]\tLoss: 614.1331\n",
      "Training Epoch: 34 [50/36045]\tLoss: 613.2518\n",
      "Training Epoch: 34 [100/36045]\tLoss: 588.0818\n",
      "Training Epoch: 34 [150/36045]\tLoss: 586.4022\n",
      "Training Epoch: 34 [200/36045]\tLoss: 572.9922\n",
      "Training Epoch: 34 [250/36045]\tLoss: 685.5297\n",
      "Training Epoch: 34 [300/36045]\tLoss: 749.7992\n",
      "Training Epoch: 34 [350/36045]\tLoss: 715.9119\n",
      "Training Epoch: 34 [400/36045]\tLoss: 711.1597\n",
      "Training Epoch: 34 [450/36045]\tLoss: 691.6103\n",
      "Training Epoch: 34 [500/36045]\tLoss: 642.1711\n",
      "Training Epoch: 34 [550/36045]\tLoss: 645.4221\n",
      "Training Epoch: 34 [600/36045]\tLoss: 628.6052\n",
      "Training Epoch: 34 [650/36045]\tLoss: 651.1773\n",
      "Training Epoch: 34 [700/36045]\tLoss: 637.6030\n",
      "Training Epoch: 34 [750/36045]\tLoss: 616.9071\n",
      "Training Epoch: 34 [800/36045]\tLoss: 629.7725\n",
      "Training Epoch: 34 [850/36045]\tLoss: 611.2918\n",
      "Training Epoch: 34 [900/36045]\tLoss: 583.8639\n",
      "Training Epoch: 34 [950/36045]\tLoss: 553.0280\n",
      "Training Epoch: 34 [1000/36045]\tLoss: 534.9874\n",
      "Training Epoch: 34 [1050/36045]\tLoss: 537.1080\n",
      "Training Epoch: 34 [1100/36045]\tLoss: 522.5195\n",
      "Training Epoch: 34 [1150/36045]\tLoss: 531.2457\n",
      "Training Epoch: 34 [1200/36045]\tLoss: 561.4856\n",
      "Training Epoch: 34 [1250/36045]\tLoss: 642.8255\n",
      "Training Epoch: 34 [1300/36045]\tLoss: 649.8135\n",
      "Training Epoch: 34 [1350/36045]\tLoss: 651.6989\n",
      "Training Epoch: 34 [1400/36045]\tLoss: 676.9910\n",
      "Training Epoch: 34 [1450/36045]\tLoss: 654.6790\n",
      "Training Epoch: 34 [1500/36045]\tLoss: 599.5009\n",
      "Training Epoch: 34 [1550/36045]\tLoss: 614.8453\n",
      "Training Epoch: 34 [1600/36045]\tLoss: 625.6393\n",
      "Training Epoch: 34 [1650/36045]\tLoss: 612.8017\n",
      "Training Epoch: 34 [1700/36045]\tLoss: 625.0490\n",
      "Training Epoch: 34 [1750/36045]\tLoss: 666.3568\n",
      "Training Epoch: 34 [1800/36045]\tLoss: 647.8091\n",
      "Training Epoch: 34 [1850/36045]\tLoss: 664.4264\n",
      "Training Epoch: 34 [1900/36045]\tLoss: 622.0511\n",
      "Training Epoch: 34 [1950/36045]\tLoss: 632.8656\n",
      "Training Epoch: 34 [2000/36045]\tLoss: 571.6393\n",
      "Training Epoch: 34 [2050/36045]\tLoss: 573.8878\n",
      "Training Epoch: 34 [2100/36045]\tLoss: 604.3835\n",
      "Training Epoch: 34 [2150/36045]\tLoss: 584.3961\n",
      "Training Epoch: 34 [2200/36045]\tLoss: 543.9982\n",
      "Training Epoch: 34 [2250/36045]\tLoss: 513.7217\n",
      "Training Epoch: 34 [2300/36045]\tLoss: 538.7361\n",
      "Training Epoch: 34 [2350/36045]\tLoss: 514.6815\n",
      "Training Epoch: 34 [2400/36045]\tLoss: 522.8840\n",
      "Training Epoch: 34 [2450/36045]\tLoss: 667.7726\n",
      "Training Epoch: 34 [2500/36045]\tLoss: 701.7540\n",
      "Training Epoch: 34 [2550/36045]\tLoss: 699.3371\n",
      "Training Epoch: 34 [2600/36045]\tLoss: 707.9600\n",
      "Training Epoch: 34 [2650/36045]\tLoss: 832.8709\n",
      "Training Epoch: 34 [2700/36045]\tLoss: 920.1492\n",
      "Training Epoch: 34 [2750/36045]\tLoss: 991.6250\n",
      "Training Epoch: 34 [2800/36045]\tLoss: 1001.4972\n",
      "Training Epoch: 34 [2850/36045]\tLoss: 769.9616\n",
      "Training Epoch: 34 [2900/36045]\tLoss: 733.1531\n",
      "Training Epoch: 34 [2950/36045]\tLoss: 707.9697\n",
      "Training Epoch: 34 [3000/36045]\tLoss: 702.0702\n",
      "Training Epoch: 34 [3050/36045]\tLoss: 732.9707\n",
      "Training Epoch: 34 [3100/36045]\tLoss: 670.7459\n",
      "Training Epoch: 34 [3150/36045]\tLoss: 517.4155\n",
      "Training Epoch: 34 [3200/36045]\tLoss: 536.1176\n",
      "Training Epoch: 34 [3250/36045]\tLoss: 504.8709\n",
      "Training Epoch: 34 [3300/36045]\tLoss: 477.6749\n",
      "Training Epoch: 34 [3350/36045]\tLoss: 504.1801\n",
      "Training Epoch: 34 [3400/36045]\tLoss: 529.0389\n",
      "Training Epoch: 34 [3450/36045]\tLoss: 568.2330\n",
      "Training Epoch: 34 [3500/36045]\tLoss: 555.4922\n",
      "Training Epoch: 34 [3550/36045]\tLoss: 531.8161\n",
      "Training Epoch: 34 [3600/36045]\tLoss: 570.3113\n",
      "Training Epoch: 34 [3650/36045]\tLoss: 659.2462\n",
      "Training Epoch: 34 [3700/36045]\tLoss: 666.5817\n",
      "Training Epoch: 34 [3750/36045]\tLoss: 635.7304\n",
      "Training Epoch: 34 [3800/36045]\tLoss: 631.4107\n",
      "Training Epoch: 34 [3850/36045]\tLoss: 631.2037\n",
      "Training Epoch: 34 [3900/36045]\tLoss: 636.1097\n",
      "Training Epoch: 34 [3950/36045]\tLoss: 613.7546\n",
      "Training Epoch: 34 [4000/36045]\tLoss: 619.2003\n",
      "Training Epoch: 34 [4050/36045]\tLoss: 568.6360\n",
      "Training Epoch: 34 [4100/36045]\tLoss: 554.4211\n",
      "Training Epoch: 34 [4150/36045]\tLoss: 569.7339\n",
      "Training Epoch: 34 [4200/36045]\tLoss: 564.4643\n",
      "Training Epoch: 34 [4250/36045]\tLoss: 566.8062\n",
      "Training Epoch: 34 [4300/36045]\tLoss: 584.2621\n",
      "Training Epoch: 34 [4350/36045]\tLoss: 567.2480\n",
      "Training Epoch: 34 [4400/36045]\tLoss: 542.7490\n",
      "Training Epoch: 34 [4450/36045]\tLoss: 594.4999\n",
      "Training Epoch: 34 [4500/36045]\tLoss: 637.7903\n",
      "Training Epoch: 34 [4550/36045]\tLoss: 641.9616\n",
      "Training Epoch: 34 [4600/36045]\tLoss: 664.6002\n",
      "Training Epoch: 34 [4650/36045]\tLoss: 654.0635\n",
      "Training Epoch: 34 [4700/36045]\tLoss: 603.5118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 34 [4750/36045]\tLoss: 585.7893\n",
      "Training Epoch: 34 [4800/36045]\tLoss: 611.1187\n",
      "Training Epoch: 34 [4850/36045]\tLoss: 597.7430\n",
      "Training Epoch: 34 [4900/36045]\tLoss: 581.6772\n",
      "Training Epoch: 34 [4950/36045]\tLoss: 597.5806\n",
      "Training Epoch: 34 [5000/36045]\tLoss: 627.4778\n",
      "Training Epoch: 34 [5050/36045]\tLoss: 607.8359\n",
      "Training Epoch: 34 [5100/36045]\tLoss: 618.2986\n",
      "Training Epoch: 34 [5150/36045]\tLoss: 602.7083\n",
      "Training Epoch: 34 [5200/36045]\tLoss: 600.7081\n",
      "Training Epoch: 34 [5250/36045]\tLoss: 594.2352\n",
      "Training Epoch: 34 [5300/36045]\tLoss: 594.5886\n",
      "Training Epoch: 34 [5350/36045]\tLoss: 616.8872\n",
      "Training Epoch: 34 [5400/36045]\tLoss: 594.2632\n",
      "Training Epoch: 34 [5450/36045]\tLoss: 563.4999\n",
      "Training Epoch: 34 [5500/36045]\tLoss: 592.1041\n",
      "Training Epoch: 34 [5550/36045]\tLoss: 580.2543\n",
      "Training Epoch: 34 [5600/36045]\tLoss: 661.3971\n",
      "Training Epoch: 34 [5650/36045]\tLoss: 625.9158\n",
      "Training Epoch: 34 [5700/36045]\tLoss: 587.2781\n",
      "Training Epoch: 34 [5750/36045]\tLoss: 571.6337\n",
      "Training Epoch: 34 [5800/36045]\tLoss: 603.0739\n",
      "Training Epoch: 34 [5850/36045]\tLoss: 591.0082\n",
      "Training Epoch: 34 [5900/36045]\tLoss: 679.6257\n",
      "Training Epoch: 34 [5950/36045]\tLoss: 696.6567\n",
      "Training Epoch: 34 [6000/36045]\tLoss: 682.0962\n",
      "Training Epoch: 34 [6050/36045]\tLoss: 659.5448\n",
      "Training Epoch: 34 [6100/36045]\tLoss: 663.8751\n",
      "Training Epoch: 34 [6150/36045]\tLoss: 651.9288\n",
      "Training Epoch: 34 [6200/36045]\tLoss: 654.9019\n",
      "Training Epoch: 34 [6250/36045]\tLoss: 676.3810\n",
      "Training Epoch: 34 [6300/36045]\tLoss: 688.0917\n",
      "Training Epoch: 34 [6350/36045]\tLoss: 734.6968\n",
      "Training Epoch: 34 [6400/36045]\tLoss: 608.3185\n",
      "Training Epoch: 34 [6450/36045]\tLoss: 560.9564\n",
      "Training Epoch: 34 [6500/36045]\tLoss: 571.2263\n",
      "Training Epoch: 34 [6550/36045]\tLoss: 588.5050\n",
      "Training Epoch: 34 [6600/36045]\tLoss: 587.1899\n",
      "Training Epoch: 34 [6650/36045]\tLoss: 662.5118\n",
      "Training Epoch: 34 [6700/36045]\tLoss: 693.0522\n",
      "Training Epoch: 34 [6750/36045]\tLoss: 669.4756\n",
      "Training Epoch: 34 [6800/36045]\tLoss: 672.3979\n",
      "Training Epoch: 34 [6850/36045]\tLoss: 660.3563\n",
      "Training Epoch: 34 [6900/36045]\tLoss: 588.3066\n",
      "Training Epoch: 34 [6950/36045]\tLoss: 554.6210\n",
      "Training Epoch: 34 [7000/36045]\tLoss: 590.0620\n",
      "Training Epoch: 34 [7050/36045]\tLoss: 603.0508\n",
      "Training Epoch: 34 [7100/36045]\tLoss: 602.5039\n",
      "Training Epoch: 34 [7150/36045]\tLoss: 613.0341\n",
      "Training Epoch: 34 [7200/36045]\tLoss: 616.0866\n",
      "Training Epoch: 34 [7250/36045]\tLoss: 613.9870\n",
      "Training Epoch: 34 [7300/36045]\tLoss: 600.6300\n",
      "Training Epoch: 34 [7350/36045]\tLoss: 597.5461\n",
      "Training Epoch: 34 [7400/36045]\tLoss: 543.2107\n",
      "Training Epoch: 34 [7450/36045]\tLoss: 546.4648\n",
      "Training Epoch: 34 [7500/36045]\tLoss: 541.6516\n",
      "Training Epoch: 34 [7550/36045]\tLoss: 518.8977\n",
      "Training Epoch: 34 [7600/36045]\tLoss: 575.7023\n",
      "Training Epoch: 34 [7650/36045]\tLoss: 616.1979\n",
      "Training Epoch: 34 [7700/36045]\tLoss: 586.6200\n",
      "Training Epoch: 34 [7750/36045]\tLoss: 601.2133\n",
      "Training Epoch: 34 [7800/36045]\tLoss: 590.1223\n",
      "Training Epoch: 34 [7850/36045]\tLoss: 571.2001\n",
      "Training Epoch: 34 [7900/36045]\tLoss: 602.4854\n",
      "Training Epoch: 34 [7950/36045]\tLoss: 599.9007\n",
      "Training Epoch: 34 [8000/36045]\tLoss: 618.0104\n",
      "Training Epoch: 34 [8050/36045]\tLoss: 582.7833\n",
      "Training Epoch: 34 [8100/36045]\tLoss: 608.1931\n",
      "Training Epoch: 34 [8150/36045]\tLoss: 688.8644\n",
      "Training Epoch: 34 [8200/36045]\tLoss: 675.7922\n",
      "Training Epoch: 34 [8250/36045]\tLoss: 643.4519\n",
      "Training Epoch: 34 [8300/36045]\tLoss: 702.1281\n",
      "Training Epoch: 34 [8350/36045]\tLoss: 644.4448\n",
      "Training Epoch: 34 [8400/36045]\tLoss: 577.2404\n",
      "Training Epoch: 34 [8450/36045]\tLoss: 540.5315\n",
      "Training Epoch: 34 [8500/36045]\tLoss: 574.6068\n",
      "Training Epoch: 34 [8550/36045]\tLoss: 567.1501\n",
      "Training Epoch: 34 [8600/36045]\tLoss: 561.1521\n",
      "Training Epoch: 34 [8650/36045]\tLoss: 597.5867\n",
      "Training Epoch: 34 [8700/36045]\tLoss: 631.9875\n",
      "Training Epoch: 34 [8750/36045]\tLoss: 620.6741\n",
      "Training Epoch: 34 [8800/36045]\tLoss: 626.4537\n",
      "Training Epoch: 34 [8850/36045]\tLoss: 619.8451\n",
      "Training Epoch: 34 [8900/36045]\tLoss: 559.5351\n",
      "Training Epoch: 34 [8950/36045]\tLoss: 571.2933\n",
      "Training Epoch: 34 [9000/36045]\tLoss: 586.9586\n",
      "Training Epoch: 34 [9050/36045]\tLoss: 588.3124\n",
      "Training Epoch: 34 [9100/36045]\tLoss: 605.6359\n",
      "Training Epoch: 34 [9150/36045]\tLoss: 447.9904\n",
      "Training Epoch: 34 [9200/36045]\tLoss: 335.3537\n",
      "Training Epoch: 34 [9250/36045]\tLoss: 363.7457\n",
      "Training Epoch: 34 [9300/36045]\tLoss: 374.0844\n",
      "Training Epoch: 34 [9350/36045]\tLoss: 344.9997\n",
      "Training Epoch: 34 [9400/36045]\tLoss: 676.0726\n",
      "Training Epoch: 34 [9450/36045]\tLoss: 718.1060\n",
      "Training Epoch: 34 [9500/36045]\tLoss: 705.2474\n",
      "Training Epoch: 34 [9550/36045]\tLoss: 746.0659\n",
      "Training Epoch: 34 [9600/36045]\tLoss: 554.7947\n",
      "Training Epoch: 34 [9650/36045]\tLoss: 559.5267\n",
      "Training Epoch: 34 [9700/36045]\tLoss: 544.9772\n",
      "Training Epoch: 34 [9750/36045]\tLoss: 543.9027\n",
      "Training Epoch: 34 [9800/36045]\tLoss: 711.1216\n",
      "Training Epoch: 34 [9850/36045]\tLoss: 751.1479\n",
      "Training Epoch: 34 [9900/36045]\tLoss: 762.5549\n",
      "Training Epoch: 34 [9950/36045]\tLoss: 743.0167\n",
      "Training Epoch: 34 [10000/36045]\tLoss: 686.8289\n",
      "Training Epoch: 34 [10050/36045]\tLoss: 564.0663\n",
      "Training Epoch: 34 [10100/36045]\tLoss: 571.5967\n",
      "Training Epoch: 34 [10150/36045]\tLoss: 580.6140\n",
      "Training Epoch: 34 [10200/36045]\tLoss: 569.5210\n",
      "Training Epoch: 34 [10250/36045]\tLoss: 682.2399\n",
      "Training Epoch: 34 [10300/36045]\tLoss: 662.7313\n",
      "Training Epoch: 34 [10350/36045]\tLoss: 697.9538\n",
      "Training Epoch: 34 [10400/36045]\tLoss: 688.3792\n",
      "Training Epoch: 34 [10450/36045]\tLoss: 644.9324\n",
      "Training Epoch: 34 [10500/36045]\tLoss: 539.1647\n",
      "Training Epoch: 34 [10550/36045]\tLoss: 534.0477\n",
      "Training Epoch: 34 [10600/36045]\tLoss: 556.5505\n",
      "Training Epoch: 34 [10650/36045]\tLoss: 562.7295\n",
      "Training Epoch: 34 [10700/36045]\tLoss: 645.5192\n",
      "Training Epoch: 34 [10750/36045]\tLoss: 705.8477\n",
      "Training Epoch: 34 [10800/36045]\tLoss: 650.5580\n",
      "Training Epoch: 34 [10850/36045]\tLoss: 689.3551\n",
      "Training Epoch: 34 [10900/36045]\tLoss: 717.5089\n",
      "Training Epoch: 34 [10950/36045]\tLoss: 529.2867\n",
      "Training Epoch: 34 [11000/36045]\tLoss: 522.9054\n",
      "Training Epoch: 34 [11050/36045]\tLoss: 560.3726\n",
      "Training Epoch: 34 [11100/36045]\tLoss: 571.1406\n",
      "Training Epoch: 34 [11150/36045]\tLoss: 619.3312\n",
      "Training Epoch: 34 [11200/36045]\tLoss: 647.3391\n",
      "Training Epoch: 34 [11250/36045]\tLoss: 659.1776\n",
      "Training Epoch: 34 [11300/36045]\tLoss: 639.2719\n",
      "Training Epoch: 34 [11350/36045]\tLoss: 636.7054\n",
      "Training Epoch: 34 [11400/36045]\tLoss: 599.1305\n",
      "Training Epoch: 34 [11450/36045]\tLoss: 567.4076\n",
      "Training Epoch: 34 [11500/36045]\tLoss: 564.8017\n",
      "Training Epoch: 34 [11550/36045]\tLoss: 575.5234\n",
      "Training Epoch: 34 [11600/36045]\tLoss: 636.5287\n",
      "Training Epoch: 34 [11650/36045]\tLoss: 688.4645\n",
      "Training Epoch: 34 [11700/36045]\tLoss: 687.4164\n",
      "Training Epoch: 34 [11750/36045]\tLoss: 706.1151\n",
      "Training Epoch: 34 [11800/36045]\tLoss: 748.8543\n",
      "Training Epoch: 34 [11850/36045]\tLoss: 806.7176\n",
      "Training Epoch: 34 [11900/36045]\tLoss: 1020.8471\n",
      "Training Epoch: 34 [11950/36045]\tLoss: 1023.4055\n",
      "Training Epoch: 34 [12000/36045]\tLoss: 1035.8862\n",
      "Training Epoch: 34 [12050/36045]\tLoss: 994.7903\n",
      "Training Epoch: 34 [12100/36045]\tLoss: 642.6052\n",
      "Training Epoch: 34 [12150/36045]\tLoss: 487.8644\n",
      "Training Epoch: 34 [12200/36045]\tLoss: 482.5401\n",
      "Training Epoch: 34 [12250/36045]\tLoss: 491.3974\n",
      "Training Epoch: 34 [12300/36045]\tLoss: 630.1437\n",
      "Training Epoch: 34 [12350/36045]\tLoss: 686.5955\n",
      "Training Epoch: 34 [12400/36045]\tLoss: 694.1421\n",
      "Training Epoch: 34 [12450/36045]\tLoss: 682.7196\n",
      "Training Epoch: 34 [12500/36045]\tLoss: 710.4055\n",
      "Training Epoch: 34 [12550/36045]\tLoss: 679.8203\n",
      "Training Epoch: 34 [12600/36045]\tLoss: 624.0552\n",
      "Training Epoch: 34 [12650/36045]\tLoss: 622.6512\n",
      "Training Epoch: 34 [12700/36045]\tLoss: 644.0788\n",
      "Training Epoch: 34 [12750/36045]\tLoss: 642.9562\n",
      "Training Epoch: 34 [12800/36045]\tLoss: 627.4839\n",
      "Training Epoch: 34 [12850/36045]\tLoss: 656.7247\n",
      "Training Epoch: 34 [12900/36045]\tLoss: 629.8315\n",
      "Training Epoch: 34 [12950/36045]\tLoss: 616.4124\n",
      "Training Epoch: 34 [13000/36045]\tLoss: 649.1876\n",
      "Training Epoch: 34 [13050/36045]\tLoss: 588.2528\n",
      "Training Epoch: 34 [13100/36045]\tLoss: 605.4192\n",
      "Training Epoch: 34 [13150/36045]\tLoss: 597.1129\n",
      "Training Epoch: 34 [13200/36045]\tLoss: 578.9308\n",
      "Training Epoch: 34 [13250/36045]\tLoss: 602.0958\n",
      "Training Epoch: 34 [13300/36045]\tLoss: 640.2781\n",
      "Training Epoch: 34 [13350/36045]\tLoss: 620.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 34 [13400/36045]\tLoss: 623.9265\n",
      "Training Epoch: 34 [13450/36045]\tLoss: 620.8392\n",
      "Training Epoch: 34 [13500/36045]\tLoss: 640.6493\n",
      "Training Epoch: 34 [13550/36045]\tLoss: 777.6755\n",
      "Training Epoch: 34 [13600/36045]\tLoss: 810.4788\n",
      "Training Epoch: 34 [13650/36045]\tLoss: 891.4114\n",
      "Training Epoch: 34 [13700/36045]\tLoss: 786.8322\n",
      "Training Epoch: 34 [13750/36045]\tLoss: 627.1550\n",
      "Training Epoch: 34 [13800/36045]\tLoss: 598.9911\n",
      "Training Epoch: 34 [13850/36045]\tLoss: 581.7572\n",
      "Training Epoch: 34 [13900/36045]\tLoss: 589.0197\n",
      "Training Epoch: 34 [13950/36045]\tLoss: 634.3692\n",
      "Training Epoch: 34 [14000/36045]\tLoss: 667.7669\n",
      "Training Epoch: 34 [14050/36045]\tLoss: 642.3460\n",
      "Training Epoch: 34 [14100/36045]\tLoss: 637.6893\n",
      "Training Epoch: 34 [14150/36045]\tLoss: 625.6088\n",
      "Training Epoch: 34 [14200/36045]\tLoss: 666.8816\n",
      "Training Epoch: 34 [14250/36045]\tLoss: 732.2111\n",
      "Training Epoch: 34 [14300/36045]\tLoss: 735.5864\n",
      "Training Epoch: 34 [14350/36045]\tLoss: 703.5907\n",
      "Training Epoch: 34 [14400/36045]\tLoss: 689.3207\n",
      "Training Epoch: 34 [14450/36045]\tLoss: 725.4620\n",
      "Training Epoch: 34 [14500/36045]\tLoss: 656.3441\n",
      "Training Epoch: 34 [14550/36045]\tLoss: 685.7039\n",
      "Training Epoch: 34 [14600/36045]\tLoss: 671.8496\n",
      "Training Epoch: 34 [14650/36045]\tLoss: 671.6701\n",
      "Training Epoch: 34 [14700/36045]\tLoss: 635.8755\n",
      "Training Epoch: 34 [14750/36045]\tLoss: 546.5636\n",
      "Training Epoch: 34 [14800/36045]\tLoss: 536.7061\n",
      "Training Epoch: 34 [14850/36045]\tLoss: 543.7565\n",
      "Training Epoch: 34 [14900/36045]\tLoss: 537.2794\n",
      "Training Epoch: 34 [14950/36045]\tLoss: 545.1829\n",
      "Training Epoch: 34 [15000/36045]\tLoss: 558.7493\n",
      "Training Epoch: 34 [15050/36045]\tLoss: 555.9988\n",
      "Training Epoch: 34 [15100/36045]\tLoss: 540.0809\n",
      "Training Epoch: 34 [15150/36045]\tLoss: 534.8477\n",
      "Training Epoch: 34 [15200/36045]\tLoss: 495.2390\n",
      "Training Epoch: 34 [15250/36045]\tLoss: 517.6952\n",
      "Training Epoch: 34 [15300/36045]\tLoss: 502.8622\n",
      "Training Epoch: 34 [15350/36045]\tLoss: 514.7124\n",
      "Training Epoch: 34 [15400/36045]\tLoss: 498.2394\n",
      "Training Epoch: 34 [15450/36045]\tLoss: 483.6000\n",
      "Training Epoch: 34 [15500/36045]\tLoss: 497.5609\n",
      "Training Epoch: 34 [15550/36045]\tLoss: 493.6018\n",
      "Training Epoch: 34 [15600/36045]\tLoss: 562.2159\n",
      "Training Epoch: 34 [15650/36045]\tLoss: 579.8124\n",
      "Training Epoch: 34 [15700/36045]\tLoss: 571.4728\n",
      "Training Epoch: 34 [15750/36045]\tLoss: 563.0265\n",
      "Training Epoch: 34 [15800/36045]\tLoss: 535.3947\n",
      "Training Epoch: 34 [15850/36045]\tLoss: 550.0173\n",
      "Training Epoch: 34 [15900/36045]\tLoss: 559.3104\n",
      "Training Epoch: 34 [15950/36045]\tLoss: 579.1083\n",
      "Training Epoch: 34 [16000/36045]\tLoss: 551.3236\n",
      "Training Epoch: 34 [16050/36045]\tLoss: 520.6992\n",
      "Training Epoch: 34 [16100/36045]\tLoss: 482.8537\n",
      "Training Epoch: 34 [16150/36045]\tLoss: 470.7359\n",
      "Training Epoch: 34 [16200/36045]\tLoss: 570.7725\n",
      "Training Epoch: 34 [16250/36045]\tLoss: 598.9929\n",
      "Training Epoch: 34 [16300/36045]\tLoss: 653.9451\n",
      "Training Epoch: 34 [16350/36045]\tLoss: 673.5023\n",
      "Training Epoch: 34 [16400/36045]\tLoss: 645.2770\n",
      "Training Epoch: 34 [16450/36045]\tLoss: 627.2376\n",
      "Training Epoch: 34 [16500/36045]\tLoss: 627.1421\n",
      "Training Epoch: 34 [16550/36045]\tLoss: 592.0085\n",
      "Training Epoch: 34 [16600/36045]\tLoss: 615.2831\n",
      "Training Epoch: 34 [16650/36045]\tLoss: 632.4227\n",
      "Training Epoch: 34 [16700/36045]\tLoss: 611.2864\n",
      "Training Epoch: 34 [16750/36045]\tLoss: 603.8283\n",
      "Training Epoch: 34 [16800/36045]\tLoss: 613.6253\n",
      "Training Epoch: 34 [16850/36045]\tLoss: 584.5417\n",
      "Training Epoch: 34 [16900/36045]\tLoss: 594.5120\n",
      "Training Epoch: 34 [16950/36045]\tLoss: 618.0538\n",
      "Training Epoch: 34 [17000/36045]\tLoss: 601.6413\n",
      "Training Epoch: 34 [17050/36045]\tLoss: 627.1234\n",
      "Training Epoch: 34 [17100/36045]\tLoss: 623.3562\n",
      "Training Epoch: 34 [17150/36045]\tLoss: 541.7596\n",
      "Training Epoch: 34 [17200/36045]\tLoss: 503.4010\n",
      "Training Epoch: 34 [17250/36045]\tLoss: 526.9425\n",
      "Training Epoch: 34 [17300/36045]\tLoss: 557.5631\n",
      "Training Epoch: 34 [17350/36045]\tLoss: 536.7404\n",
      "Training Epoch: 34 [17400/36045]\tLoss: 556.1671\n",
      "Training Epoch: 34 [17450/36045]\tLoss: 575.3510\n",
      "Training Epoch: 34 [17500/36045]\tLoss: 563.5327\n",
      "Training Epoch: 34 [17550/36045]\tLoss: 562.4420\n",
      "Training Epoch: 34 [17600/36045]\tLoss: 556.3021\n",
      "Training Epoch: 34 [17650/36045]\tLoss: 572.5393\n",
      "Training Epoch: 34 [17700/36045]\tLoss: 551.6038\n",
      "Training Epoch: 34 [17750/36045]\tLoss: 568.1157\n",
      "Training Epoch: 34 [17800/36045]\tLoss: 558.9638\n",
      "Training Epoch: 34 [17850/36045]\tLoss: 572.1115\n",
      "Training Epoch: 34 [17900/36045]\tLoss: 600.5208\n",
      "Training Epoch: 34 [17950/36045]\tLoss: 612.4772\n",
      "Training Epoch: 34 [18000/36045]\tLoss: 602.8583\n",
      "Training Epoch: 34 [18050/36045]\tLoss: 664.5287\n",
      "Training Epoch: 34 [18100/36045]\tLoss: 666.1929\n",
      "Training Epoch: 34 [18150/36045]\tLoss: 677.5492\n",
      "Training Epoch: 34 [18200/36045]\tLoss: 659.5373\n",
      "Training Epoch: 34 [18250/36045]\tLoss: 680.3968\n",
      "Training Epoch: 34 [18300/36045]\tLoss: 633.0396\n",
      "Training Epoch: 34 [18350/36045]\tLoss: 704.8294\n",
      "Training Epoch: 34 [18400/36045]\tLoss: 678.0883\n",
      "Training Epoch: 34 [18450/36045]\tLoss: 657.9165\n",
      "Training Epoch: 34 [18500/36045]\tLoss: 656.8970\n",
      "Training Epoch: 34 [18550/36045]\tLoss: 644.1283\n",
      "Training Epoch: 34 [18600/36045]\tLoss: 633.5413\n",
      "Training Epoch: 34 [18650/36045]\tLoss: 679.8718\n",
      "Training Epoch: 34 [18700/36045]\tLoss: 715.2428\n",
      "Training Epoch: 34 [18750/36045]\tLoss: 702.2103\n",
      "Training Epoch: 34 [18800/36045]\tLoss: 725.6327\n",
      "Training Epoch: 34 [18850/36045]\tLoss: 670.2006\n",
      "Training Epoch: 34 [18900/36045]\tLoss: 716.9880\n",
      "Training Epoch: 34 [18950/36045]\tLoss: 658.9637\n",
      "Training Epoch: 34 [19000/36045]\tLoss: 546.7779\n",
      "Training Epoch: 34 [19050/36045]\tLoss: 530.5562\n",
      "Training Epoch: 34 [19100/36045]\tLoss: 538.8743\n",
      "Training Epoch: 34 [19150/36045]\tLoss: 528.7631\n",
      "Training Epoch: 34 [19200/36045]\tLoss: 558.4615\n",
      "Training Epoch: 34 [19250/36045]\tLoss: 573.2654\n",
      "Training Epoch: 34 [19300/36045]\tLoss: 583.2377\n",
      "Training Epoch: 34 [19350/36045]\tLoss: 566.7652\n",
      "Training Epoch: 34 [19400/36045]\tLoss: 587.9399\n",
      "Training Epoch: 34 [19450/36045]\tLoss: 579.0678\n",
      "Training Epoch: 34 [19500/36045]\tLoss: 580.5655\n",
      "Training Epoch: 34 [19550/36045]\tLoss: 579.0543\n",
      "Training Epoch: 34 [19600/36045]\tLoss: 619.9626\n",
      "Training Epoch: 34 [19650/36045]\tLoss: 823.6050\n",
      "Training Epoch: 34 [19700/36045]\tLoss: 782.3757\n",
      "Training Epoch: 34 [19750/36045]\tLoss: 785.9463\n",
      "Training Epoch: 34 [19800/36045]\tLoss: 785.7921\n",
      "Training Epoch: 34 [19850/36045]\tLoss: 518.4677\n",
      "Training Epoch: 34 [19900/36045]\tLoss: 496.9662\n",
      "Training Epoch: 34 [19950/36045]\tLoss: 500.6073\n",
      "Training Epoch: 34 [20000/36045]\tLoss: 499.8026\n",
      "Training Epoch: 34 [20050/36045]\tLoss: 559.7008\n",
      "Training Epoch: 34 [20100/36045]\tLoss: 565.7695\n",
      "Training Epoch: 34 [20150/36045]\tLoss: 567.4247\n",
      "Training Epoch: 34 [20200/36045]\tLoss: 567.2117\n",
      "Training Epoch: 34 [20250/36045]\tLoss: 604.2580\n",
      "Training Epoch: 34 [20300/36045]\tLoss: 640.2252\n",
      "Training Epoch: 34 [20350/36045]\tLoss: 658.9800\n",
      "Training Epoch: 34 [20400/36045]\tLoss: 675.6656\n",
      "Training Epoch: 34 [20450/36045]\tLoss: 646.0217\n",
      "Training Epoch: 34 [20500/36045]\tLoss: 630.1537\n",
      "Training Epoch: 34 [20550/36045]\tLoss: 554.1716\n",
      "Training Epoch: 34 [20600/36045]\tLoss: 564.5908\n",
      "Training Epoch: 34 [20650/36045]\tLoss: 561.7551\n",
      "Training Epoch: 34 [20700/36045]\tLoss: 549.7487\n",
      "Training Epoch: 34 [20750/36045]\tLoss: 592.2640\n",
      "Training Epoch: 34 [20800/36045]\tLoss: 643.5355\n",
      "Training Epoch: 34 [20850/36045]\tLoss: 630.2386\n",
      "Training Epoch: 34 [20900/36045]\tLoss: 674.5302\n",
      "Training Epoch: 34 [20950/36045]\tLoss: 636.0266\n",
      "Training Epoch: 34 [21000/36045]\tLoss: 598.8895\n",
      "Training Epoch: 34 [21050/36045]\tLoss: 512.5688\n",
      "Training Epoch: 34 [21100/36045]\tLoss: 516.7189\n",
      "Training Epoch: 34 [21150/36045]\tLoss: 553.0577\n",
      "Training Epoch: 34 [21200/36045]\tLoss: 552.2693\n",
      "Training Epoch: 34 [21250/36045]\tLoss: 528.3829\n",
      "Training Epoch: 34 [21300/36045]\tLoss: 617.0404\n",
      "Training Epoch: 34 [21350/36045]\tLoss: 609.0678\n",
      "Training Epoch: 34 [21400/36045]\tLoss: 612.5696\n",
      "Training Epoch: 34 [21450/36045]\tLoss: 618.7529\n",
      "Training Epoch: 34 [21500/36045]\tLoss: 621.0906\n",
      "Training Epoch: 34 [21550/36045]\tLoss: 715.8724\n",
      "Training Epoch: 34 [21600/36045]\tLoss: 714.8175\n",
      "Training Epoch: 34 [21650/36045]\tLoss: 727.3173\n",
      "Training Epoch: 34 [21700/36045]\tLoss: 729.8364\n",
      "Training Epoch: 34 [21750/36045]\tLoss: 700.9550\n",
      "Training Epoch: 34 [21800/36045]\tLoss: 516.6226\n",
      "Training Epoch: 34 [21850/36045]\tLoss: 499.6453\n",
      "Training Epoch: 34 [21900/36045]\tLoss: 509.4323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 34 [21950/36045]\tLoss: 509.7610\n",
      "Training Epoch: 34 [22000/36045]\tLoss: 513.1742\n",
      "Training Epoch: 34 [22050/36045]\tLoss: 534.7894\n",
      "Training Epoch: 34 [22100/36045]\tLoss: 527.4081\n",
      "Training Epoch: 34 [22150/36045]\tLoss: 513.0508\n",
      "Training Epoch: 34 [22200/36045]\tLoss: 529.4632\n",
      "Training Epoch: 34 [22250/36045]\tLoss: 534.4203\n",
      "Training Epoch: 34 [22300/36045]\tLoss: 586.7004\n",
      "Training Epoch: 34 [22350/36045]\tLoss: 612.6233\n",
      "Training Epoch: 34 [22400/36045]\tLoss: 627.2136\n",
      "Training Epoch: 34 [22450/36045]\tLoss: 614.6965\n",
      "Training Epoch: 34 [22500/36045]\tLoss: 596.9987\n",
      "Training Epoch: 34 [22550/36045]\tLoss: 632.6072\n",
      "Training Epoch: 34 [22600/36045]\tLoss: 684.9464\n",
      "Training Epoch: 34 [22650/36045]\tLoss: 719.4853\n",
      "Training Epoch: 34 [22700/36045]\tLoss: 741.7593\n",
      "Training Epoch: 34 [22750/36045]\tLoss: 761.2032\n",
      "Training Epoch: 34 [22800/36045]\tLoss: 791.3095\n",
      "Training Epoch: 34 [22850/36045]\tLoss: 658.0502\n",
      "Training Epoch: 34 [22900/36045]\tLoss: 662.9005\n",
      "Training Epoch: 34 [22950/36045]\tLoss: 642.3629\n",
      "Training Epoch: 34 [23000/36045]\tLoss: 639.4496\n",
      "Training Epoch: 34 [23050/36045]\tLoss: 568.0927\n",
      "Training Epoch: 34 [23100/36045]\tLoss: 583.9277\n",
      "Training Epoch: 34 [23150/36045]\tLoss: 572.1808\n",
      "Training Epoch: 34 [23200/36045]\tLoss: 541.9003\n",
      "Training Epoch: 34 [23250/36045]\tLoss: 544.9411\n",
      "Training Epoch: 34 [23300/36045]\tLoss: 541.2101\n",
      "Training Epoch: 34 [23350/36045]\tLoss: 562.1274\n",
      "Training Epoch: 34 [23400/36045]\tLoss: 609.2377\n",
      "Training Epoch: 34 [23450/36045]\tLoss: 602.5024\n",
      "Training Epoch: 34 [23500/36045]\tLoss: 580.7648\n",
      "Training Epoch: 34 [23550/36045]\tLoss: 622.6022\n",
      "Training Epoch: 34 [23600/36045]\tLoss: 704.5953\n",
      "Training Epoch: 34 [23650/36045]\tLoss: 717.2201\n",
      "Training Epoch: 34 [23700/36045]\tLoss: 725.4404\n",
      "Training Epoch: 34 [23750/36045]\tLoss: 701.1415\n",
      "Training Epoch: 34 [23800/36045]\tLoss: 560.8671\n",
      "Training Epoch: 34 [23850/36045]\tLoss: 587.0659\n",
      "Training Epoch: 34 [23900/36045]\tLoss: 576.6725\n",
      "Training Epoch: 34 [23950/36045]\tLoss: 559.7414\n",
      "Training Epoch: 34 [24000/36045]\tLoss: 536.5055\n",
      "Training Epoch: 34 [24050/36045]\tLoss: 495.6604\n",
      "Training Epoch: 34 [24100/36045]\tLoss: 521.3580\n",
      "Training Epoch: 34 [24150/36045]\tLoss: 514.2592\n",
      "Training Epoch: 34 [24200/36045]\tLoss: 510.9689\n",
      "Training Epoch: 34 [24250/36045]\tLoss: 496.0786\n",
      "Training Epoch: 34 [24300/36045]\tLoss: 535.9528\n",
      "Training Epoch: 34 [24350/36045]\tLoss: 548.9233\n",
      "Training Epoch: 34 [24400/36045]\tLoss: 564.2898\n",
      "Training Epoch: 34 [24450/36045]\tLoss: 537.8541\n",
      "Training Epoch: 34 [24500/36045]\tLoss: 567.2413\n",
      "Training Epoch: 34 [24550/36045]\tLoss: 655.9650\n",
      "Training Epoch: 34 [24600/36045]\tLoss: 647.7619\n",
      "Training Epoch: 34 [24650/36045]\tLoss: 620.7180\n",
      "Training Epoch: 34 [24700/36045]\tLoss: 630.6078\n",
      "Training Epoch: 34 [24750/36045]\tLoss: 582.6503\n",
      "Training Epoch: 34 [24800/36045]\tLoss: 480.0187\n",
      "Training Epoch: 34 [24850/36045]\tLoss: 499.2101\n",
      "Training Epoch: 34 [24900/36045]\tLoss: 496.4898\n",
      "Training Epoch: 34 [24950/36045]\tLoss: 499.0832\n",
      "Training Epoch: 34 [25000/36045]\tLoss: 479.7504\n",
      "Training Epoch: 34 [25050/36045]\tLoss: 458.3282\n",
      "Training Epoch: 34 [25100/36045]\tLoss: 410.6840\n",
      "Training Epoch: 34 [25150/36045]\tLoss: 380.3479\n",
      "Training Epoch: 34 [25200/36045]\tLoss: 375.4296\n",
      "Training Epoch: 34 [25250/36045]\tLoss: 402.3777\n",
      "Training Epoch: 34 [25300/36045]\tLoss: 528.4949\n",
      "Training Epoch: 34 [25350/36045]\tLoss: 525.0461\n",
      "Training Epoch: 34 [25400/36045]\tLoss: 489.8986\n",
      "Training Epoch: 34 [25450/36045]\tLoss: 492.3021\n",
      "Training Epoch: 34 [25500/36045]\tLoss: 534.9466\n",
      "Training Epoch: 34 [25550/36045]\tLoss: 625.4046\n",
      "Training Epoch: 34 [25600/36045]\tLoss: 629.7100\n",
      "Training Epoch: 34 [25650/36045]\tLoss: 607.6219\n",
      "Training Epoch: 34 [25700/36045]\tLoss: 616.7997\n",
      "Training Epoch: 34 [25750/36045]\tLoss: 595.0291\n",
      "Training Epoch: 34 [25800/36045]\tLoss: 375.5408\n",
      "Training Epoch: 34 [25850/36045]\tLoss: 384.7240\n",
      "Training Epoch: 34 [25900/36045]\tLoss: 365.7411\n",
      "Training Epoch: 34 [25950/36045]\tLoss: 374.5399\n",
      "Training Epoch: 34 [26000/36045]\tLoss: 459.5254\n",
      "Training Epoch: 34 [26050/36045]\tLoss: 625.7346\n",
      "Training Epoch: 34 [26100/36045]\tLoss: 653.0494\n",
      "Training Epoch: 34 [26150/36045]\tLoss: 653.2413\n",
      "Training Epoch: 34 [26200/36045]\tLoss: 626.2523\n",
      "Training Epoch: 34 [26250/36045]\tLoss: 656.0926\n",
      "Training Epoch: 34 [26300/36045]\tLoss: 596.2768\n",
      "Training Epoch: 34 [26350/36045]\tLoss: 607.2805\n",
      "Training Epoch: 34 [26400/36045]\tLoss: 584.1324\n",
      "Training Epoch: 34 [26450/36045]\tLoss: 513.9346\n",
      "Training Epoch: 34 [26500/36045]\tLoss: 609.6462\n",
      "Training Epoch: 34 [26550/36045]\tLoss: 610.0836\n",
      "Training Epoch: 34 [26600/36045]\tLoss: 606.4874\n",
      "Training Epoch: 34 [26650/36045]\tLoss: 622.4929\n",
      "Training Epoch: 34 [26700/36045]\tLoss: 601.9300\n",
      "Training Epoch: 34 [26750/36045]\tLoss: 563.8057\n",
      "Training Epoch: 34 [26800/36045]\tLoss: 415.5616\n",
      "Training Epoch: 34 [26850/36045]\tLoss: 344.5384\n",
      "Training Epoch: 34 [26900/36045]\tLoss: 347.2885\n",
      "Training Epoch: 34 [26950/36045]\tLoss: 381.6721\n",
      "Training Epoch: 34 [27000/36045]\tLoss: 624.0802\n",
      "Training Epoch: 34 [27050/36045]\tLoss: 651.8145\n",
      "Training Epoch: 34 [27100/36045]\tLoss: 631.5663\n",
      "Training Epoch: 34 [27150/36045]\tLoss: 671.7194\n",
      "Training Epoch: 34 [27200/36045]\tLoss: 489.9960\n",
      "Training Epoch: 34 [27250/36045]\tLoss: 481.5063\n",
      "Training Epoch: 34 [27300/36045]\tLoss: 469.1489\n",
      "Training Epoch: 34 [27350/36045]\tLoss: 467.4084\n",
      "Training Epoch: 34 [27400/36045]\tLoss: 466.6222\n",
      "Training Epoch: 34 [27450/36045]\tLoss: 589.6455\n",
      "Training Epoch: 34 [27500/36045]\tLoss: 631.8229\n",
      "Training Epoch: 34 [27550/36045]\tLoss: 624.9542\n",
      "Training Epoch: 34 [27600/36045]\tLoss: 636.6208\n",
      "Training Epoch: 34 [27650/36045]\tLoss: 628.4854\n",
      "Training Epoch: 34 [27700/36045]\tLoss: 657.0948\n",
      "Training Epoch: 34 [27750/36045]\tLoss: 669.1525\n",
      "Training Epoch: 34 [27800/36045]\tLoss: 655.9789\n",
      "Training Epoch: 34 [27850/36045]\tLoss: 645.8335\n",
      "Training Epoch: 34 [27900/36045]\tLoss: 584.7856\n",
      "Training Epoch: 34 [27950/36045]\tLoss: 487.4211\n",
      "Training Epoch: 34 [28000/36045]\tLoss: 463.9868\n",
      "Training Epoch: 34 [28050/36045]\tLoss: 473.9524\n",
      "Training Epoch: 34 [28100/36045]\tLoss: 465.5791\n",
      "Training Epoch: 34 [28150/36045]\tLoss: 487.6542\n",
      "Training Epoch: 34 [28200/36045]\tLoss: 495.1423\n",
      "Training Epoch: 34 [28250/36045]\tLoss: 488.3644\n",
      "Training Epoch: 34 [28300/36045]\tLoss: 463.5646\n",
      "Training Epoch: 34 [28350/36045]\tLoss: 459.5854\n",
      "Training Epoch: 34 [28400/36045]\tLoss: 789.5315\n",
      "Training Epoch: 34 [28450/36045]\tLoss: 723.3434\n",
      "Training Epoch: 34 [28500/36045]\tLoss: 625.0280\n",
      "Training Epoch: 34 [28550/36045]\tLoss: 574.2623\n",
      "Training Epoch: 34 [28600/36045]\tLoss: 603.5750\n",
      "Training Epoch: 34 [28650/36045]\tLoss: 665.8580\n",
      "Training Epoch: 34 [28700/36045]\tLoss: 659.9514\n",
      "Training Epoch: 34 [28750/36045]\tLoss: 646.4118\n",
      "Training Epoch: 34 [28800/36045]\tLoss: 655.4305\n",
      "Training Epoch: 34 [28850/36045]\tLoss: 568.6561\n",
      "Training Epoch: 34 [28900/36045]\tLoss: 461.8041\n",
      "Training Epoch: 34 [28950/36045]\tLoss: 460.8127\n",
      "Training Epoch: 34 [29000/36045]\tLoss: 457.6620\n",
      "Training Epoch: 34 [29050/36045]\tLoss: 464.4995\n",
      "Training Epoch: 34 [29100/36045]\tLoss: 483.1028\n",
      "Training Epoch: 34 [29150/36045]\tLoss: 472.3334\n",
      "Training Epoch: 34 [29200/36045]\tLoss: 458.0677\n",
      "Training Epoch: 34 [29250/36045]\tLoss: 447.7718\n",
      "Training Epoch: 34 [29300/36045]\tLoss: 506.7748\n",
      "Training Epoch: 34 [29350/36045]\tLoss: 596.9429\n",
      "Training Epoch: 34 [29400/36045]\tLoss: 614.4534\n",
      "Training Epoch: 34 [29450/36045]\tLoss: 632.2007\n",
      "Training Epoch: 34 [29500/36045]\tLoss: 646.9823\n",
      "Training Epoch: 34 [29550/36045]\tLoss: 615.7759\n",
      "Training Epoch: 34 [29600/36045]\tLoss: 519.2510\n",
      "Training Epoch: 34 [29650/36045]\tLoss: 501.2787\n",
      "Training Epoch: 34 [29700/36045]\tLoss: 448.3869\n",
      "Training Epoch: 34 [29750/36045]\tLoss: 447.1129\n",
      "Training Epoch: 34 [29800/36045]\tLoss: 494.1631\n",
      "Training Epoch: 34 [29850/36045]\tLoss: 570.4338\n",
      "Training Epoch: 34 [29900/36045]\tLoss: 566.8142\n",
      "Training Epoch: 34 [29950/36045]\tLoss: 588.6732\n",
      "Training Epoch: 34 [30000/36045]\tLoss: 563.1028\n",
      "Training Epoch: 34 [30050/36045]\tLoss: 569.5326\n",
      "Training Epoch: 34 [30100/36045]\tLoss: 694.9210\n",
      "Training Epoch: 34 [30150/36045]\tLoss: 678.2145\n",
      "Training Epoch: 34 [30200/36045]\tLoss: 639.8483\n",
      "Training Epoch: 34 [30250/36045]\tLoss: 688.4089\n",
      "Training Epoch: 34 [30300/36045]\tLoss: 673.1160\n",
      "Training Epoch: 34 [30350/36045]\tLoss: 518.7977\n",
      "Training Epoch: 34 [30400/36045]\tLoss: 503.3750\n",
      "Training Epoch: 34 [30450/36045]\tLoss: 505.3346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 34 [30500/36045]\tLoss: 471.9132\n",
      "Training Epoch: 34 [30550/36045]\tLoss: 437.4843\n",
      "Training Epoch: 34 [30600/36045]\tLoss: 428.5556\n",
      "Training Epoch: 34 [30650/36045]\tLoss: 418.5316\n",
      "Training Epoch: 34 [30700/36045]\tLoss: 436.3732\n",
      "Training Epoch: 34 [30750/36045]\tLoss: 423.2460\n",
      "Training Epoch: 34 [30800/36045]\tLoss: 449.7781\n",
      "Training Epoch: 34 [30850/36045]\tLoss: 441.4740\n",
      "Training Epoch: 34 [30900/36045]\tLoss: 453.8870\n",
      "Training Epoch: 34 [30950/36045]\tLoss: 476.8299\n",
      "Training Epoch: 34 [31000/36045]\tLoss: 468.8060\n",
      "Training Epoch: 34 [31050/36045]\tLoss: 391.6930\n",
      "Training Epoch: 34 [31100/36045]\tLoss: 382.2812\n",
      "Training Epoch: 34 [31150/36045]\tLoss: 389.6776\n",
      "Training Epoch: 34 [31200/36045]\tLoss: 484.6473\n",
      "Training Epoch: 34 [31250/36045]\tLoss: 628.8370\n",
      "Training Epoch: 34 [31300/36045]\tLoss: 599.5059\n",
      "Training Epoch: 34 [31350/36045]\tLoss: 615.4135\n",
      "Training Epoch: 34 [31400/36045]\tLoss: 594.8386\n",
      "Training Epoch: 34 [31450/36045]\tLoss: 611.1889\n",
      "Training Epoch: 34 [31500/36045]\tLoss: 623.6438\n",
      "Training Epoch: 34 [31550/36045]\tLoss: 631.1037\n",
      "Training Epoch: 34 [31600/36045]\tLoss: 593.2870\n",
      "Training Epoch: 34 [31650/36045]\tLoss: 634.5859\n",
      "Training Epoch: 34 [31700/36045]\tLoss: 459.4911\n",
      "Training Epoch: 34 [31750/36045]\tLoss: 380.0320\n",
      "Training Epoch: 34 [31800/36045]\tLoss: 362.5992\n",
      "Training Epoch: 34 [31850/36045]\tLoss: 371.0885\n",
      "Training Epoch: 34 [31900/36045]\tLoss: 583.8684\n",
      "Training Epoch: 34 [31950/36045]\tLoss: 754.7533\n",
      "Training Epoch: 34 [32000/36045]\tLoss: 864.0516\n",
      "Training Epoch: 34 [32050/36045]\tLoss: 819.1027\n",
      "Training Epoch: 34 [32100/36045]\tLoss: 809.5666\n",
      "Training Epoch: 34 [32150/36045]\tLoss: 625.9188\n",
      "Training Epoch: 34 [32200/36045]\tLoss: 628.9810\n",
      "Training Epoch: 34 [32250/36045]\tLoss: 639.4970\n",
      "Training Epoch: 34 [32300/36045]\tLoss: 621.2762\n",
      "Training Epoch: 34 [32350/36045]\tLoss: 616.8906\n",
      "Training Epoch: 34 [32400/36045]\tLoss: 578.8582\n",
      "Training Epoch: 34 [32450/36045]\tLoss: 476.7184\n",
      "Training Epoch: 34 [32500/36045]\tLoss: 458.1717\n",
      "Training Epoch: 34 [32550/36045]\tLoss: 460.4400\n",
      "Training Epoch: 34 [32600/36045]\tLoss: 457.2926\n",
      "Training Epoch: 34 [32650/36045]\tLoss: 589.2596\n",
      "Training Epoch: 34 [32700/36045]\tLoss: 643.1247\n",
      "Training Epoch: 34 [32750/36045]\tLoss: 612.7715\n",
      "Training Epoch: 34 [32800/36045]\tLoss: 628.4427\n",
      "Training Epoch: 34 [32850/36045]\tLoss: 580.1837\n",
      "Training Epoch: 34 [32900/36045]\tLoss: 465.0541\n",
      "Training Epoch: 34 [32950/36045]\tLoss: 486.9261\n",
      "Training Epoch: 34 [33000/36045]\tLoss: 486.1457\n",
      "Training Epoch: 34 [33050/36045]\tLoss: 461.9457\n",
      "Training Epoch: 34 [33100/36045]\tLoss: 524.9446\n",
      "Training Epoch: 34 [33150/36045]\tLoss: 713.1796\n",
      "Training Epoch: 34 [33200/36045]\tLoss: 694.5203\n",
      "Training Epoch: 34 [33250/36045]\tLoss: 715.7177\n",
      "Training Epoch: 34 [33300/36045]\tLoss: 762.3386\n",
      "Training Epoch: 34 [33350/36045]\tLoss: 584.4272\n",
      "Training Epoch: 34 [33400/36045]\tLoss: 427.6105\n",
      "Training Epoch: 34 [33450/36045]\tLoss: 422.9941\n",
      "Training Epoch: 34 [33500/36045]\tLoss: 435.4326\n",
      "Training Epoch: 34 [33550/36045]\tLoss: 451.3370\n",
      "Training Epoch: 34 [33600/36045]\tLoss: 453.1613\n",
      "Training Epoch: 34 [33650/36045]\tLoss: 604.7679\n",
      "Training Epoch: 34 [33700/36045]\tLoss: 585.0200\n",
      "Training Epoch: 34 [33750/36045]\tLoss: 605.7370\n",
      "Training Epoch: 34 [33800/36045]\tLoss: 602.0585\n",
      "Training Epoch: 34 [33850/36045]\tLoss: 604.2942\n",
      "Training Epoch: 34 [33900/36045]\tLoss: 617.3287\n",
      "Training Epoch: 34 [33950/36045]\tLoss: 627.7117\n",
      "Training Epoch: 34 [34000/36045]\tLoss: 614.2306\n",
      "Training Epoch: 34 [34050/36045]\tLoss: 618.2752\n",
      "Training Epoch: 34 [34100/36045]\tLoss: 595.5217\n",
      "Training Epoch: 34 [34150/36045]\tLoss: 552.6507\n",
      "Training Epoch: 34 [34200/36045]\tLoss: 523.4034\n",
      "Training Epoch: 34 [34250/36045]\tLoss: 537.7451\n",
      "Training Epoch: 34 [34300/36045]\tLoss: 459.5267\n",
      "Training Epoch: 34 [34350/36045]\tLoss: 483.9856\n",
      "Training Epoch: 34 [34400/36045]\tLoss: 476.0048\n",
      "Training Epoch: 34 [34450/36045]\tLoss: 447.7508\n",
      "Training Epoch: 34 [34500/36045]\tLoss: 477.7970\n",
      "Training Epoch: 34 [34550/36045]\tLoss: 468.9556\n",
      "Training Epoch: 34 [34600/36045]\tLoss: 474.1384\n",
      "Training Epoch: 34 [34650/36045]\tLoss: 580.9226\n",
      "Training Epoch: 34 [34700/36045]\tLoss: 615.6128\n",
      "Training Epoch: 34 [34750/36045]\tLoss: 545.8391\n",
      "Training Epoch: 34 [34800/36045]\tLoss: 625.7277\n",
      "Training Epoch: 34 [34850/36045]\tLoss: 633.4446\n",
      "Training Epoch: 34 [34900/36045]\tLoss: 689.7211\n",
      "Training Epoch: 34 [34950/36045]\tLoss: 675.3367\n",
      "Training Epoch: 34 [35000/36045]\tLoss: 676.1727\n",
      "Training Epoch: 34 [35050/36045]\tLoss: 662.6943\n",
      "Training Epoch: 34 [35100/36045]\tLoss: 566.0517\n",
      "Training Epoch: 34 [35150/36045]\tLoss: 558.9009\n",
      "Training Epoch: 34 [35200/36045]\tLoss: 471.8462\n",
      "Training Epoch: 34 [35250/36045]\tLoss: 518.0843\n",
      "Training Epoch: 34 [35300/36045]\tLoss: 533.0141\n",
      "Training Epoch: 34 [35350/36045]\tLoss: 600.2406\n",
      "Training Epoch: 34 [35400/36045]\tLoss: 632.9922\n",
      "Training Epoch: 34 [35450/36045]\tLoss: 603.7675\n",
      "Training Epoch: 34 [35500/36045]\tLoss: 584.8986\n",
      "Training Epoch: 34 [35550/36045]\tLoss: 570.3880\n",
      "Training Epoch: 34 [35600/36045]\tLoss: 619.4079\n",
      "Training Epoch: 34 [35650/36045]\tLoss: 692.6675\n",
      "Training Epoch: 34 [35700/36045]\tLoss: 620.0891\n",
      "Training Epoch: 34 [35750/36045]\tLoss: 677.6860\n",
      "Training Epoch: 34 [35800/36045]\tLoss: 684.1136\n",
      "Training Epoch: 34 [35850/36045]\tLoss: 658.0489\n",
      "Training Epoch: 34 [35900/36045]\tLoss: 682.3457\n",
      "Training Epoch: 34 [35950/36045]\tLoss: 679.8035\n",
      "Training Epoch: 34 [36000/36045]\tLoss: 672.4030\n",
      "Training Epoch: 34 [36045/36045]\tLoss: 656.6617\n",
      "Training Epoch: 34 [4004/4004]\tLoss: 610.4059\n",
      "Training Epoch: 35 [50/36045]\tLoss: 609.4531\n",
      "Training Epoch: 35 [100/36045]\tLoss: 584.4462\n",
      "Training Epoch: 35 [150/36045]\tLoss: 582.7806\n",
      "Training Epoch: 35 [200/36045]\tLoss: 569.3256\n",
      "Training Epoch: 35 [250/36045]\tLoss: 681.4899\n",
      "Training Epoch: 35 [300/36045]\tLoss: 745.8098\n",
      "Training Epoch: 35 [350/36045]\tLoss: 711.9979\n",
      "Training Epoch: 35 [400/36045]\tLoss: 707.1869\n",
      "Training Epoch: 35 [450/36045]\tLoss: 687.7126\n",
      "Training Epoch: 35 [500/36045]\tLoss: 638.3463\n",
      "Training Epoch: 35 [550/36045]\tLoss: 641.4936\n",
      "Training Epoch: 35 [600/36045]\tLoss: 624.9615\n",
      "Training Epoch: 35 [650/36045]\tLoss: 647.3931\n",
      "Training Epoch: 35 [700/36045]\tLoss: 633.7347\n",
      "Training Epoch: 35 [750/36045]\tLoss: 612.7810\n",
      "Training Epoch: 35 [800/36045]\tLoss: 625.4769\n",
      "Training Epoch: 35 [850/36045]\tLoss: 607.1030\n",
      "Training Epoch: 35 [900/36045]\tLoss: 580.0281\n",
      "Training Epoch: 35 [950/36045]\tLoss: 549.3564\n",
      "Training Epoch: 35 [1000/36045]\tLoss: 531.6002\n",
      "Training Epoch: 35 [1050/36045]\tLoss: 533.7098\n",
      "Training Epoch: 35 [1100/36045]\tLoss: 519.1750\n",
      "Training Epoch: 35 [1150/36045]\tLoss: 527.9116\n",
      "Training Epoch: 35 [1200/36045]\tLoss: 558.0621\n",
      "Training Epoch: 35 [1250/36045]\tLoss: 638.9433\n",
      "Training Epoch: 35 [1300/36045]\tLoss: 645.9755\n",
      "Training Epoch: 35 [1350/36045]\tLoss: 647.7711\n",
      "Training Epoch: 35 [1400/36045]\tLoss: 672.8577\n",
      "Training Epoch: 35 [1450/36045]\tLoss: 650.6772\n",
      "Training Epoch: 35 [1500/36045]\tLoss: 595.6724\n",
      "Training Epoch: 35 [1550/36045]\tLoss: 610.9875\n",
      "Training Epoch: 35 [1600/36045]\tLoss: 621.7354\n",
      "Training Epoch: 35 [1650/36045]\tLoss: 608.9126\n",
      "Training Epoch: 35 [1700/36045]\tLoss: 621.1303\n",
      "Training Epoch: 35 [1750/36045]\tLoss: 662.4125\n",
      "Training Epoch: 35 [1800/36045]\tLoss: 643.9827\n",
      "Training Epoch: 35 [1850/36045]\tLoss: 660.5253\n",
      "Training Epoch: 35 [1900/36045]\tLoss: 618.3828\n",
      "Training Epoch: 35 [1950/36045]\tLoss: 629.1073\n",
      "Training Epoch: 35 [2000/36045]\tLoss: 568.0811\n",
      "Training Epoch: 35 [2050/36045]\tLoss: 570.2941\n",
      "Training Epoch: 35 [2100/36045]\tLoss: 600.6323\n",
      "Training Epoch: 35 [2150/36045]\tLoss: 580.7958\n",
      "Training Epoch: 35 [2200/36045]\tLoss: 540.7669\n",
      "Training Epoch: 35 [2250/36045]\tLoss: 510.6649\n",
      "Training Epoch: 35 [2300/36045]\tLoss: 535.4850\n",
      "Training Epoch: 35 [2350/36045]\tLoss: 511.5729\n",
      "Training Epoch: 35 [2400/36045]\tLoss: 519.6696\n",
      "Training Epoch: 35 [2450/36045]\tLoss: 663.9456\n",
      "Training Epoch: 35 [2500/36045]\tLoss: 697.7233\n",
      "Training Epoch: 35 [2550/36045]\tLoss: 695.3506\n",
      "Training Epoch: 35 [2600/36045]\tLoss: 703.9266\n",
      "Training Epoch: 35 [2650/36045]\tLoss: 828.7415\n",
      "Training Epoch: 35 [2700/36045]\tLoss: 916.0541\n",
      "Training Epoch: 35 [2750/36045]\tLoss: 987.4828\n",
      "Training Epoch: 35 [2800/36045]\tLoss: 997.3424\n",
      "Training Epoch: 35 [2850/36045]\tLoss: 765.7314\n",
      "Training Epoch: 35 [2900/36045]\tLoss: 728.6431\n",
      "Training Epoch: 35 [2950/36045]\tLoss: 703.6758\n",
      "Training Epoch: 35 [3000/36045]\tLoss: 697.7424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 35 [3050/36045]\tLoss: 728.6655\n",
      "Training Epoch: 35 [3100/36045]\tLoss: 666.7733\n",
      "Training Epoch: 35 [3150/36045]\tLoss: 514.2502\n",
      "Training Epoch: 35 [3200/36045]\tLoss: 532.7220\n",
      "Training Epoch: 35 [3250/36045]\tLoss: 501.7129\n",
      "Training Epoch: 35 [3300/36045]\tLoss: 474.6908\n",
      "Training Epoch: 35 [3350/36045]\tLoss: 501.1310\n",
      "Training Epoch: 35 [3400/36045]\tLoss: 525.7950\n",
      "Training Epoch: 35 [3450/36045]\tLoss: 564.7106\n",
      "Training Epoch: 35 [3500/36045]\tLoss: 551.9325\n",
      "Training Epoch: 35 [3550/36045]\tLoss: 528.2765\n",
      "Training Epoch: 35 [3600/36045]\tLoss: 566.6104\n",
      "Training Epoch: 35 [3650/36045]\tLoss: 655.0484\n",
      "Training Epoch: 35 [3700/36045]\tLoss: 662.5117\n",
      "Training Epoch: 35 [3750/36045]\tLoss: 631.6923\n",
      "Training Epoch: 35 [3800/36045]\tLoss: 627.4830\n",
      "Training Epoch: 35 [3850/36045]\tLoss: 627.3824\n",
      "Training Epoch: 35 [3900/36045]\tLoss: 632.2612\n",
      "Training Epoch: 35 [3950/36045]\tLoss: 610.0850\n",
      "Training Epoch: 35 [4000/36045]\tLoss: 615.4564\n",
      "Training Epoch: 35 [4050/36045]\tLoss: 565.1189\n",
      "Training Epoch: 35 [4100/36045]\tLoss: 550.9822\n",
      "Training Epoch: 35 [4150/36045]\tLoss: 566.1275\n",
      "Training Epoch: 35 [4200/36045]\tLoss: 560.8880\n",
      "Training Epoch: 35 [4250/36045]\tLoss: 563.2668\n",
      "Training Epoch: 35 [4300/36045]\tLoss: 580.5659\n",
      "Training Epoch: 35 [4350/36045]\tLoss: 563.6092\n",
      "Training Epoch: 35 [4400/36045]\tLoss: 539.2006\n",
      "Training Epoch: 35 [4450/36045]\tLoss: 590.7823\n",
      "Training Epoch: 35 [4500/36045]\tLoss: 634.0391\n",
      "Training Epoch: 35 [4550/36045]\tLoss: 638.0869\n",
      "Training Epoch: 35 [4600/36045]\tLoss: 660.6605\n",
      "Training Epoch: 35 [4650/36045]\tLoss: 650.1351\n",
      "Training Epoch: 35 [4700/36045]\tLoss: 599.7924\n",
      "Training Epoch: 35 [4750/36045]\tLoss: 582.0630\n",
      "Training Epoch: 35 [4800/36045]\tLoss: 607.2982\n",
      "Training Epoch: 35 [4850/36045]\tLoss: 593.9327\n",
      "Training Epoch: 35 [4900/36045]\tLoss: 577.9877\n",
      "Training Epoch: 35 [4950/36045]\tLoss: 593.7601\n",
      "Training Epoch: 35 [5000/36045]\tLoss: 623.5672\n",
      "Training Epoch: 35 [5050/36045]\tLoss: 604.0768\n",
      "Training Epoch: 35 [5100/36045]\tLoss: 614.5401\n",
      "Training Epoch: 35 [5150/36045]\tLoss: 598.9496\n",
      "Training Epoch: 35 [5200/36045]\tLoss: 596.9323\n",
      "Training Epoch: 35 [5250/36045]\tLoss: 590.4590\n",
      "Training Epoch: 35 [5300/36045]\tLoss: 590.7775\n",
      "Training Epoch: 35 [5350/36045]\tLoss: 612.9742\n",
      "Training Epoch: 35 [5400/36045]\tLoss: 590.5493\n",
      "Training Epoch: 35 [5450/36045]\tLoss: 559.9490\n",
      "Training Epoch: 35 [5500/36045]\tLoss: 588.4642\n",
      "Training Epoch: 35 [5550/36045]\tLoss: 576.6239\n",
      "Training Epoch: 35 [5600/36045]\tLoss: 657.4882\n",
      "Training Epoch: 35 [5650/36045]\tLoss: 622.1519\n",
      "Training Epoch: 35 [5700/36045]\tLoss: 583.7190\n",
      "Training Epoch: 35 [5750/36045]\tLoss: 568.0349\n",
      "Training Epoch: 35 [5800/36045]\tLoss: 599.2601\n",
      "Training Epoch: 35 [5850/36045]\tLoss: 587.3863\n",
      "Training Epoch: 35 [5900/36045]\tLoss: 675.4149\n",
      "Training Epoch: 35 [5950/36045]\tLoss: 692.3806\n",
      "Training Epoch: 35 [6000/36045]\tLoss: 677.8948\n",
      "Training Epoch: 35 [6050/36045]\tLoss: 655.4484\n",
      "Training Epoch: 35 [6100/36045]\tLoss: 659.7379\n",
      "Training Epoch: 35 [6150/36045]\tLoss: 648.0920\n",
      "Training Epoch: 35 [6200/36045]\tLoss: 651.2333\n",
      "Training Epoch: 35 [6250/36045]\tLoss: 672.7259\n",
      "Training Epoch: 35 [6300/36045]\tLoss: 684.4268\n",
      "Training Epoch: 35 [6350/36045]\tLoss: 730.8733\n",
      "Training Epoch: 35 [6400/36045]\tLoss: 604.8262\n",
      "Training Epoch: 35 [6450/36045]\tLoss: 557.5601\n",
      "Training Epoch: 35 [6500/36045]\tLoss: 567.7578\n",
      "Training Epoch: 35 [6550/36045]\tLoss: 585.0574\n",
      "Training Epoch: 35 [6600/36045]\tLoss: 583.6652\n",
      "Training Epoch: 35 [6650/36045]\tLoss: 658.5569\n",
      "Training Epoch: 35 [6700/36045]\tLoss: 688.8938\n",
      "Training Epoch: 35 [6750/36045]\tLoss: 665.4506\n",
      "Training Epoch: 35 [6800/36045]\tLoss: 668.3644\n",
      "Training Epoch: 35 [6850/36045]\tLoss: 656.3361\n",
      "Training Epoch: 35 [6900/36045]\tLoss: 584.7341\n",
      "Training Epoch: 35 [6950/36045]\tLoss: 551.2620\n",
      "Training Epoch: 35 [7000/36045]\tLoss: 586.4669\n",
      "Training Epoch: 35 [7050/36045]\tLoss: 599.3613\n",
      "Training Epoch: 35 [7100/36045]\tLoss: 598.8167\n",
      "Training Epoch: 35 [7150/36045]\tLoss: 609.3002\n",
      "Training Epoch: 35 [7200/36045]\tLoss: 612.2594\n",
      "Training Epoch: 35 [7250/36045]\tLoss: 610.2004\n",
      "Training Epoch: 35 [7300/36045]\tLoss: 596.8013\n",
      "Training Epoch: 35 [7350/36045]\tLoss: 593.8192\n",
      "Training Epoch: 35 [7400/36045]\tLoss: 540.0052\n",
      "Training Epoch: 35 [7450/36045]\tLoss: 543.2559\n",
      "Training Epoch: 35 [7500/36045]\tLoss: 538.4851\n",
      "Training Epoch: 35 [7550/36045]\tLoss: 515.8419\n",
      "Training Epoch: 35 [7600/36045]\tLoss: 572.2380\n",
      "Training Epoch: 35 [7650/36045]\tLoss: 612.3666\n",
      "Training Epoch: 35 [7700/36045]\tLoss: 582.9460\n",
      "Training Epoch: 35 [7750/36045]\tLoss: 597.5140\n",
      "Training Epoch: 35 [7800/36045]\tLoss: 586.5198\n",
      "Training Epoch: 35 [7850/36045]\tLoss: 567.7212\n",
      "Training Epoch: 35 [7900/36045]\tLoss: 598.8075\n",
      "Training Epoch: 35 [7950/36045]\tLoss: 596.2388\n",
      "Training Epoch: 35 [8000/36045]\tLoss: 614.3493\n",
      "Training Epoch: 35 [8050/36045]\tLoss: 579.2307\n",
      "Training Epoch: 35 [8100/36045]\tLoss: 604.5886\n",
      "Training Epoch: 35 [8150/36045]\tLoss: 684.8796\n",
      "Training Epoch: 35 [8200/36045]\tLoss: 671.8690\n",
      "Training Epoch: 35 [8250/36045]\tLoss: 639.5969\n",
      "Training Epoch: 35 [8300/36045]\tLoss: 698.0532\n",
      "Training Epoch: 35 [8350/36045]\tLoss: 640.6102\n",
      "Training Epoch: 35 [8400/36045]\tLoss: 573.7299\n",
      "Training Epoch: 35 [8450/36045]\tLoss: 537.1922\n",
      "Training Epoch: 35 [8500/36045]\tLoss: 571.1384\n",
      "Training Epoch: 35 [8550/36045]\tLoss: 563.7717\n",
      "Training Epoch: 35 [8600/36045]\tLoss: 557.8160\n",
      "Training Epoch: 35 [8650/36045]\tLoss: 593.8263\n",
      "Training Epoch: 35 [8700/36045]\tLoss: 628.0377\n",
      "Training Epoch: 35 [8750/36045]\tLoss: 616.8208\n",
      "Training Epoch: 35 [8800/36045]\tLoss: 622.5996\n",
      "Training Epoch: 35 [8850/36045]\tLoss: 616.0180\n",
      "Training Epoch: 35 [8900/36045]\tLoss: 556.0936\n",
      "Training Epoch: 35 [8950/36045]\tLoss: 567.7219\n",
      "Training Epoch: 35 [9000/36045]\tLoss: 583.4016\n",
      "Training Epoch: 35 [9050/36045]\tLoss: 584.8033\n",
      "Training Epoch: 35 [9100/36045]\tLoss: 602.0387\n",
      "Training Epoch: 35 [9150/36045]\tLoss: 445.3744\n",
      "Training Epoch: 35 [9200/36045]\tLoss: 333.3132\n",
      "Training Epoch: 35 [9250/36045]\tLoss: 361.5504\n",
      "Training Epoch: 35 [9300/36045]\tLoss: 371.8017\n",
      "Training Epoch: 35 [9350/36045]\tLoss: 342.9269\n",
      "Training Epoch: 35 [9400/36045]\tLoss: 672.0061\n",
      "Training Epoch: 35 [9450/36045]\tLoss: 713.7758\n",
      "Training Epoch: 35 [9500/36045]\tLoss: 700.9550\n",
      "Training Epoch: 35 [9550/36045]\tLoss: 741.5305\n",
      "Training Epoch: 35 [9600/36045]\tLoss: 551.5255\n",
      "Training Epoch: 35 [9650/36045]\tLoss: 556.3223\n",
      "Training Epoch: 35 [9700/36045]\tLoss: 541.7823\n",
      "Training Epoch: 35 [9750/36045]\tLoss: 540.6432\n",
      "Training Epoch: 35 [9800/36045]\tLoss: 706.9323\n",
      "Training Epoch: 35 [9850/36045]\tLoss: 746.7329\n",
      "Training Epoch: 35 [9900/36045]\tLoss: 757.9045\n",
      "Training Epoch: 35 [9950/36045]\tLoss: 738.4911\n",
      "Training Epoch: 35 [10000/36045]\tLoss: 682.7088\n",
      "Training Epoch: 35 [10050/36045]\tLoss: 560.4373\n",
      "Training Epoch: 35 [10100/36045]\tLoss: 568.0155\n",
      "Training Epoch: 35 [10150/36045]\tLoss: 576.9586\n",
      "Training Epoch: 35 [10200/36045]\tLoss: 565.8685\n",
      "Training Epoch: 35 [10250/36045]\tLoss: 678.0844\n",
      "Training Epoch: 35 [10300/36045]\tLoss: 658.7245\n",
      "Training Epoch: 35 [10350/36045]\tLoss: 693.7796\n",
      "Training Epoch: 35 [10400/36045]\tLoss: 684.1888\n",
      "Training Epoch: 35 [10450/36045]\tLoss: 641.0174\n",
      "Training Epoch: 35 [10500/36045]\tLoss: 535.7788\n",
      "Training Epoch: 35 [10550/36045]\tLoss: 530.6306\n",
      "Training Epoch: 35 [10600/36045]\tLoss: 553.0783\n",
      "Training Epoch: 35 [10650/36045]\tLoss: 559.2454\n",
      "Training Epoch: 35 [10700/36045]\tLoss: 641.6877\n",
      "Training Epoch: 35 [10750/36045]\tLoss: 701.8535\n",
      "Training Epoch: 35 [10800/36045]\tLoss: 646.7071\n",
      "Training Epoch: 35 [10850/36045]\tLoss: 685.3380\n",
      "Training Epoch: 35 [10900/36045]\tLoss: 713.3521\n",
      "Training Epoch: 35 [10950/36045]\tLoss: 526.0593\n",
      "Training Epoch: 35 [11000/36045]\tLoss: 519.6768\n",
      "Training Epoch: 35 [11050/36045]\tLoss: 556.9760\n",
      "Training Epoch: 35 [11100/36045]\tLoss: 567.6644\n",
      "Training Epoch: 35 [11150/36045]\tLoss: 615.6000\n",
      "Training Epoch: 35 [11200/36045]\tLoss: 643.6138\n",
      "Training Epoch: 35 [11250/36045]\tLoss: 655.4072\n",
      "Training Epoch: 35 [11300/36045]\tLoss: 635.5237\n",
      "Training Epoch: 35 [11350/36045]\tLoss: 632.9924\n",
      "Training Epoch: 35 [11400/36045]\tLoss: 595.5512\n",
      "Training Epoch: 35 [11450/36045]\tLoss: 563.9531\n",
      "Training Epoch: 35 [11500/36045]\tLoss: 561.3395\n",
      "Training Epoch: 35 [11550/36045]\tLoss: 571.9238\n",
      "Training Epoch: 35 [11600/36045]\tLoss: 632.7986\n",
      "Training Epoch: 35 [11650/36045]\tLoss: 684.6705\n",
      "Training Epoch: 35 [11700/36045]\tLoss: 683.6296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 35 [11750/36045]\tLoss: 702.2486\n",
      "Training Epoch: 35 [11800/36045]\tLoss: 744.9567\n",
      "Training Epoch: 35 [11850/36045]\tLoss: 802.8989\n",
      "Training Epoch: 35 [11900/36045]\tLoss: 1016.7486\n",
      "Training Epoch: 35 [11950/36045]\tLoss: 1019.3838\n",
      "Training Epoch: 35 [12000/36045]\tLoss: 1031.7319\n",
      "Training Epoch: 35 [12050/36045]\tLoss: 990.7206\n",
      "Training Epoch: 35 [12100/36045]\tLoss: 639.2490\n",
      "Training Epoch: 35 [12150/36045]\tLoss: 484.8143\n",
      "Training Epoch: 35 [12200/36045]\tLoss: 479.5051\n",
      "Training Epoch: 35 [12250/36045]\tLoss: 488.3318\n",
      "Training Epoch: 35 [12300/36045]\tLoss: 626.6379\n",
      "Training Epoch: 35 [12350/36045]\tLoss: 682.9388\n",
      "Training Epoch: 35 [12400/36045]\tLoss: 690.4504\n",
      "Training Epoch: 35 [12450/36045]\tLoss: 679.0789\n",
      "Training Epoch: 35 [12500/36045]\tLoss: 706.6533\n",
      "Training Epoch: 35 [12550/36045]\tLoss: 676.1422\n",
      "Training Epoch: 35 [12600/36045]\tLoss: 620.4325\n",
      "Training Epoch: 35 [12650/36045]\tLoss: 618.9863\n",
      "Training Epoch: 35 [12700/36045]\tLoss: 640.4003\n",
      "Training Epoch: 35 [12750/36045]\tLoss: 639.2272\n",
      "Training Epoch: 35 [12800/36045]\tLoss: 623.9301\n",
      "Training Epoch: 35 [12850/36045]\tLoss: 653.1494\n",
      "Training Epoch: 35 [12900/36045]\tLoss: 626.3709\n",
      "Training Epoch: 35 [12950/36045]\tLoss: 612.8581\n",
      "Training Epoch: 35 [13000/36045]\tLoss: 645.6846\n",
      "Training Epoch: 35 [13050/36045]\tLoss: 584.9376\n",
      "Training Epoch: 35 [13100/36045]\tLoss: 601.8959\n",
      "Training Epoch: 35 [13150/36045]\tLoss: 593.5842\n",
      "Training Epoch: 35 [13200/36045]\tLoss: 575.5593\n",
      "Training Epoch: 35 [13250/36045]\tLoss: 598.5393\n",
      "Training Epoch: 35 [13300/36045]\tLoss: 636.5648\n",
      "Training Epoch: 35 [13350/36045]\tLoss: 616.9255\n",
      "Training Epoch: 35 [13400/36045]\tLoss: 620.2637\n",
      "Training Epoch: 35 [13450/36045]\tLoss: 617.2445\n",
      "Training Epoch: 35 [13500/36045]\tLoss: 636.9146\n",
      "Training Epoch: 35 [13550/36045]\tLoss: 774.0522\n",
      "Training Epoch: 35 [13600/36045]\tLoss: 806.8358\n",
      "Training Epoch: 35 [13650/36045]\tLoss: 887.8461\n",
      "Training Epoch: 35 [13700/36045]\tLoss: 783.4791\n",
      "Training Epoch: 35 [13750/36045]\tLoss: 623.5427\n",
      "Training Epoch: 35 [13800/36045]\tLoss: 595.2827\n",
      "Training Epoch: 35 [13850/36045]\tLoss: 578.0096\n",
      "Training Epoch: 35 [13900/36045]\tLoss: 585.2601\n",
      "Training Epoch: 35 [13950/36045]\tLoss: 630.5812\n",
      "Training Epoch: 35 [14000/36045]\tLoss: 663.9114\n",
      "Training Epoch: 35 [14050/36045]\tLoss: 638.5781\n",
      "Training Epoch: 35 [14100/36045]\tLoss: 633.8843\n",
      "Training Epoch: 35 [14150/36045]\tLoss: 621.8101\n",
      "Training Epoch: 35 [14200/36045]\tLoss: 662.9666\n",
      "Training Epoch: 35 [14250/36045]\tLoss: 728.0075\n",
      "Training Epoch: 35 [14300/36045]\tLoss: 731.3907\n",
      "Training Epoch: 35 [14350/36045]\tLoss: 699.4981\n",
      "Training Epoch: 35 [14400/36045]\tLoss: 685.2612\n",
      "Training Epoch: 35 [14450/36045]\tLoss: 721.2856\n",
      "Training Epoch: 35 [14500/36045]\tLoss: 652.2238\n",
      "Training Epoch: 35 [14550/36045]\tLoss: 681.4550\n",
      "Training Epoch: 35 [14600/36045]\tLoss: 667.6722\n",
      "Training Epoch: 35 [14650/36045]\tLoss: 667.4390\n",
      "Training Epoch: 35 [14700/36045]\tLoss: 631.9809\n",
      "Training Epoch: 35 [14750/36045]\tLoss: 543.2777\n",
      "Training Epoch: 35 [14800/36045]\tLoss: 533.4296\n",
      "Training Epoch: 35 [14850/36045]\tLoss: 540.4725\n",
      "Training Epoch: 35 [14900/36045]\tLoss: 533.9965\n",
      "Training Epoch: 35 [14950/36045]\tLoss: 541.9128\n",
      "Training Epoch: 35 [15000/36045]\tLoss: 555.3455\n",
      "Training Epoch: 35 [15050/36045]\tLoss: 552.5349\n",
      "Training Epoch: 35 [15100/36045]\tLoss: 536.6158\n",
      "Training Epoch: 35 [15150/36045]\tLoss: 531.5118\n",
      "Training Epoch: 35 [15200/36045]\tLoss: 492.1875\n",
      "Training Epoch: 35 [15250/36045]\tLoss: 514.5240\n",
      "Training Epoch: 35 [15300/36045]\tLoss: 499.7536\n",
      "Training Epoch: 35 [15350/36045]\tLoss: 511.5481\n",
      "Training Epoch: 35 [15400/36045]\tLoss: 494.9851\n",
      "Training Epoch: 35 [15450/36045]\tLoss: 480.3121\n",
      "Training Epoch: 35 [15500/36045]\tLoss: 494.1477\n",
      "Training Epoch: 35 [15550/36045]\tLoss: 490.2496\n",
      "Training Epoch: 35 [15600/36045]\tLoss: 558.6748\n",
      "Training Epoch: 35 [15650/36045]\tLoss: 576.1727\n",
      "Training Epoch: 35 [15700/36045]\tLoss: 567.9308\n",
      "Training Epoch: 35 [15750/36045]\tLoss: 559.4436\n",
      "Training Epoch: 35 [15800/36045]\tLoss: 532.4623\n",
      "Training Epoch: 35 [15850/36045]\tLoss: 547.1489\n",
      "Training Epoch: 35 [15900/36045]\tLoss: 556.4064\n",
      "Training Epoch: 35 [15950/36045]\tLoss: 576.2113\n",
      "Training Epoch: 35 [16000/36045]\tLoss: 548.2643\n",
      "Training Epoch: 35 [16050/36045]\tLoss: 517.6541\n",
      "Training Epoch: 35 [16100/36045]\tLoss: 480.0817\n",
      "Training Epoch: 35 [16150/36045]\tLoss: 467.9810\n",
      "Training Epoch: 35 [16200/36045]\tLoss: 567.5610\n",
      "Training Epoch: 35 [16250/36045]\tLoss: 595.6848\n",
      "Training Epoch: 35 [16300/36045]\tLoss: 650.3315\n",
      "Training Epoch: 35 [16350/36045]\tLoss: 669.9839\n",
      "Training Epoch: 35 [16400/36045]\tLoss: 641.7382\n",
      "Training Epoch: 35 [16450/36045]\tLoss: 623.7526\n",
      "Training Epoch: 35 [16500/36045]\tLoss: 623.6680\n",
      "Training Epoch: 35 [16550/36045]\tLoss: 588.6056\n",
      "Training Epoch: 35 [16600/36045]\tLoss: 611.7115\n",
      "Training Epoch: 35 [16650/36045]\tLoss: 628.6878\n",
      "Training Epoch: 35 [16700/36045]\tLoss: 607.7366\n",
      "Training Epoch: 35 [16750/36045]\tLoss: 600.3143\n",
      "Training Epoch: 35 [16800/36045]\tLoss: 609.9645\n",
      "Training Epoch: 35 [16850/36045]\tLoss: 581.0900\n",
      "Training Epoch: 35 [16900/36045]\tLoss: 591.0196\n",
      "Training Epoch: 35 [16950/36045]\tLoss: 614.4352\n",
      "Training Epoch: 35 [17000/36045]\tLoss: 598.1484\n",
      "Training Epoch: 35 [17050/36045]\tLoss: 623.4084\n",
      "Training Epoch: 35 [17100/36045]\tLoss: 619.5956\n",
      "Training Epoch: 35 [17150/36045]\tLoss: 538.4144\n",
      "Training Epoch: 35 [17200/36045]\tLoss: 500.1710\n",
      "Training Epoch: 35 [17250/36045]\tLoss: 523.5976\n",
      "Training Epoch: 35 [17300/36045]\tLoss: 554.0463\n",
      "Training Epoch: 35 [17350/36045]\tLoss: 533.4819\n",
      "Training Epoch: 35 [17400/36045]\tLoss: 552.9161\n",
      "Training Epoch: 35 [17450/36045]\tLoss: 571.9960\n",
      "Training Epoch: 35 [17500/36045]\tLoss: 560.2062\n",
      "Training Epoch: 35 [17550/36045]\tLoss: 559.0522\n",
      "Training Epoch: 35 [17600/36045]\tLoss: 553.0195\n",
      "Training Epoch: 35 [17650/36045]\tLoss: 569.1514\n",
      "Training Epoch: 35 [17700/36045]\tLoss: 548.2637\n",
      "Training Epoch: 35 [17750/36045]\tLoss: 564.7014\n",
      "Training Epoch: 35 [17800/36045]\tLoss: 555.5801\n",
      "Training Epoch: 35 [17850/36045]\tLoss: 569.1355\n",
      "Training Epoch: 35 [17900/36045]\tLoss: 597.4893\n",
      "Training Epoch: 35 [17950/36045]\tLoss: 609.4933\n",
      "Training Epoch: 35 [18000/36045]\tLoss: 599.9114\n",
      "Training Epoch: 35 [18050/36045]\tLoss: 660.8844\n",
      "Training Epoch: 35 [18100/36045]\tLoss: 662.4875\n",
      "Training Epoch: 35 [18150/36045]\tLoss: 673.8525\n",
      "Training Epoch: 35 [18200/36045]\tLoss: 655.8544\n",
      "Training Epoch: 35 [18250/36045]\tLoss: 676.7148\n",
      "Training Epoch: 35 [18300/36045]\tLoss: 629.7716\n",
      "Training Epoch: 35 [18350/36045]\tLoss: 701.7324\n",
      "Training Epoch: 35 [18400/36045]\tLoss: 674.9681\n",
      "Training Epoch: 35 [18450/36045]\tLoss: 654.7574\n",
      "Training Epoch: 35 [18500/36045]\tLoss: 653.7401\n",
      "Training Epoch: 35 [18550/36045]\tLoss: 640.9971\n",
      "Training Epoch: 35 [18600/36045]\tLoss: 630.4345\n",
      "Training Epoch: 35 [18650/36045]\tLoss: 676.6765\n",
      "Training Epoch: 35 [18700/36045]\tLoss: 711.8515\n",
      "Training Epoch: 35 [18750/36045]\tLoss: 698.9349\n",
      "Training Epoch: 35 [18800/36045]\tLoss: 722.2434\n",
      "Training Epoch: 35 [18850/36045]\tLoss: 666.8891\n",
      "Training Epoch: 35 [18900/36045]\tLoss: 713.4368\n",
      "Training Epoch: 35 [18950/36045]\tLoss: 655.4865\n",
      "Training Epoch: 35 [19000/36045]\tLoss: 543.2368\n",
      "Training Epoch: 35 [19050/36045]\tLoss: 527.1285\n",
      "Training Epoch: 35 [19100/36045]\tLoss: 535.3931\n",
      "Training Epoch: 35 [19150/36045]\tLoss: 525.3572\n",
      "Training Epoch: 35 [19200/36045]\tLoss: 555.1861\n",
      "Training Epoch: 35 [19250/36045]\tLoss: 570.0544\n",
      "Training Epoch: 35 [19300/36045]\tLoss: 579.9246\n",
      "Training Epoch: 35 [19350/36045]\tLoss: 563.4724\n",
      "Training Epoch: 35 [19400/36045]\tLoss: 584.5419\n",
      "Training Epoch: 35 [19450/36045]\tLoss: 575.6879\n",
      "Training Epoch: 35 [19500/36045]\tLoss: 577.1804\n",
      "Training Epoch: 35 [19550/36045]\tLoss: 575.6376\n",
      "Training Epoch: 35 [19600/36045]\tLoss: 616.5096\n",
      "Training Epoch: 35 [19650/36045]\tLoss: 819.6907\n",
      "Training Epoch: 35 [19700/36045]\tLoss: 778.4351\n",
      "Training Epoch: 35 [19750/36045]\tLoss: 782.0974\n",
      "Training Epoch: 35 [19800/36045]\tLoss: 782.0488\n",
      "Training Epoch: 35 [19850/36045]\tLoss: 515.3997\n",
      "Training Epoch: 35 [19900/36045]\tLoss: 493.9869\n",
      "Training Epoch: 35 [19950/36045]\tLoss: 497.6320\n",
      "Training Epoch: 35 [20000/36045]\tLoss: 496.9085\n",
      "Training Epoch: 35 [20050/36045]\tLoss: 556.4391\n",
      "Training Epoch: 35 [20100/36045]\tLoss: 562.4425\n",
      "Training Epoch: 35 [20150/36045]\tLoss: 564.0278\n",
      "Training Epoch: 35 [20200/36045]\tLoss: 563.7902\n",
      "Training Epoch: 35 [20250/36045]\tLoss: 600.6841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 35 [20300/36045]\tLoss: 636.6516\n",
      "Training Epoch: 35 [20350/36045]\tLoss: 655.3424\n",
      "Training Epoch: 35 [20400/36045]\tLoss: 672.0123\n",
      "Training Epoch: 35 [20450/36045]\tLoss: 642.3097\n",
      "Training Epoch: 35 [20500/36045]\tLoss: 626.4896\n",
      "Training Epoch: 35 [20550/36045]\tLoss: 550.8894\n",
      "Training Epoch: 35 [20600/36045]\tLoss: 561.2249\n",
      "Training Epoch: 35 [20650/36045]\tLoss: 558.4389\n",
      "Training Epoch: 35 [20700/36045]\tLoss: 546.4548\n",
      "Training Epoch: 35 [20750/36045]\tLoss: 588.7899\n",
      "Training Epoch: 35 [20800/36045]\tLoss: 639.7266\n",
      "Training Epoch: 35 [20850/36045]\tLoss: 626.4361\n",
      "Training Epoch: 35 [20900/36045]\tLoss: 670.5684\n",
      "Training Epoch: 35 [20950/36045]\tLoss: 632.2597\n",
      "Training Epoch: 35 [21000/36045]\tLoss: 595.2800\n",
      "Training Epoch: 35 [21050/36045]\tLoss: 509.4178\n",
      "Training Epoch: 35 [21100/36045]\tLoss: 513.6636\n",
      "Training Epoch: 35 [21150/36045]\tLoss: 549.7936\n",
      "Training Epoch: 35 [21200/36045]\tLoss: 549.0061\n",
      "Training Epoch: 35 [21250/36045]\tLoss: 525.2441\n",
      "Training Epoch: 35 [21300/36045]\tLoss: 613.3687\n",
      "Training Epoch: 35 [21350/36045]\tLoss: 605.3481\n",
      "Training Epoch: 35 [21400/36045]\tLoss: 608.8743\n",
      "Training Epoch: 35 [21450/36045]\tLoss: 615.0530\n",
      "Training Epoch: 35 [21500/36045]\tLoss: 617.3138\n",
      "Training Epoch: 35 [21550/36045]\tLoss: 712.0274\n",
      "Training Epoch: 35 [21600/36045]\tLoss: 710.8869\n",
      "Training Epoch: 35 [21650/36045]\tLoss: 723.3461\n",
      "Training Epoch: 35 [21700/36045]\tLoss: 725.9771\n",
      "Training Epoch: 35 [21750/36045]\tLoss: 697.2094\n",
      "Training Epoch: 35 [21800/36045]\tLoss: 513.6592\n",
      "Training Epoch: 35 [21850/36045]\tLoss: 496.6868\n",
      "Training Epoch: 35 [21900/36045]\tLoss: 506.3730\n",
      "Training Epoch: 35 [21950/36045]\tLoss: 506.8270\n",
      "Training Epoch: 35 [22000/36045]\tLoss: 510.2074\n",
      "Training Epoch: 35 [22050/36045]\tLoss: 531.5847\n",
      "Training Epoch: 35 [22100/36045]\tLoss: 524.2070\n",
      "Training Epoch: 35 [22150/36045]\tLoss: 509.9505\n",
      "Training Epoch: 35 [22200/36045]\tLoss: 526.2473\n",
      "Training Epoch: 35 [22250/36045]\tLoss: 531.2031\n",
      "Training Epoch: 35 [22300/36045]\tLoss: 583.4175\n",
      "Training Epoch: 35 [22350/36045]\tLoss: 609.3059\n",
      "Training Epoch: 35 [22400/36045]\tLoss: 623.8194\n",
      "Training Epoch: 35 [22450/36045]\tLoss: 611.2570\n",
      "Training Epoch: 35 [22500/36045]\tLoss: 593.6469\n",
      "Training Epoch: 35 [22550/36045]\tLoss: 629.1575\n",
      "Training Epoch: 35 [22600/36045]\tLoss: 681.0289\n",
      "Training Epoch: 35 [22650/36045]\tLoss: 715.3986\n",
      "Training Epoch: 35 [22700/36045]\tLoss: 737.5464\n",
      "Training Epoch: 35 [22750/36045]\tLoss: 756.9495\n",
      "Training Epoch: 35 [22800/36045]\tLoss: 786.8715\n",
      "Training Epoch: 35 [22850/36045]\tLoss: 654.2924\n",
      "Training Epoch: 35 [22900/36045]\tLoss: 659.1677\n",
      "Training Epoch: 35 [22950/36045]\tLoss: 638.6577\n",
      "Training Epoch: 35 [23000/36045]\tLoss: 635.6348\n",
      "Training Epoch: 35 [23050/36045]\tLoss: 564.6585\n",
      "Training Epoch: 35 [23100/36045]\tLoss: 580.4498\n",
      "Training Epoch: 35 [23150/36045]\tLoss: 568.7300\n",
      "Training Epoch: 35 [23200/36045]\tLoss: 538.6083\n",
      "Training Epoch: 35 [23250/36045]\tLoss: 541.6489\n",
      "Training Epoch: 35 [23300/36045]\tLoss: 537.8685\n",
      "Training Epoch: 35 [23350/36045]\tLoss: 558.7129\n",
      "Training Epoch: 35 [23400/36045]\tLoss: 605.5832\n",
      "Training Epoch: 35 [23450/36045]\tLoss: 598.9125\n",
      "Training Epoch: 35 [23500/36045]\tLoss: 577.2917\n",
      "Training Epoch: 35 [23550/36045]\tLoss: 618.8223\n",
      "Training Epoch: 35 [23600/36045]\tLoss: 700.5702\n",
      "Training Epoch: 35 [23650/36045]\tLoss: 713.0687\n",
      "Training Epoch: 35 [23700/36045]\tLoss: 721.2339\n",
      "Training Epoch: 35 [23750/36045]\tLoss: 697.0514\n",
      "Training Epoch: 35 [23800/36045]\tLoss: 557.7677\n",
      "Training Epoch: 35 [23850/36045]\tLoss: 583.9088\n",
      "Training Epoch: 35 [23900/36045]\tLoss: 573.5329\n",
      "Training Epoch: 35 [23950/36045]\tLoss: 556.6247\n",
      "Training Epoch: 35 [24000/36045]\tLoss: 533.4510\n",
      "Training Epoch: 35 [24050/36045]\tLoss: 492.7984\n",
      "Training Epoch: 35 [24100/36045]\tLoss: 518.3302\n",
      "Training Epoch: 35 [24150/36045]\tLoss: 511.1260\n",
      "Training Epoch: 35 [24200/36045]\tLoss: 507.9364\n",
      "Training Epoch: 35 [24250/36045]\tLoss: 493.1686\n",
      "Training Epoch: 35 [24300/36045]\tLoss: 532.8472\n",
      "Training Epoch: 35 [24350/36045]\tLoss: 545.7107\n",
      "Training Epoch: 35 [24400/36045]\tLoss: 560.9822\n",
      "Training Epoch: 35 [24450/36045]\tLoss: 534.6705\n",
      "Training Epoch: 35 [24500/36045]\tLoss: 563.9525\n",
      "Training Epoch: 35 [24550/36045]\tLoss: 652.4402\n",
      "Training Epoch: 35 [24600/36045]\tLoss: 644.1998\n",
      "Training Epoch: 35 [24650/36045]\tLoss: 617.1866\n",
      "Training Epoch: 35 [24700/36045]\tLoss: 627.0478\n",
      "Training Epoch: 35 [24750/36045]\tLoss: 579.3773\n",
      "Training Epoch: 35 [24800/36045]\tLoss: 476.9957\n",
      "Training Epoch: 35 [24850/36045]\tLoss: 496.1054\n",
      "Training Epoch: 35 [24900/36045]\tLoss: 493.4224\n",
      "Training Epoch: 35 [24950/36045]\tLoss: 496.0085\n",
      "Training Epoch: 35 [25000/36045]\tLoss: 476.7860\n",
      "Training Epoch: 35 [25050/36045]\tLoss: 455.5371\n",
      "Training Epoch: 35 [25100/36045]\tLoss: 408.1653\n",
      "Training Epoch: 35 [25150/36045]\tLoss: 378.0107\n",
      "Training Epoch: 35 [25200/36045]\tLoss: 373.0903\n",
      "Training Epoch: 35 [25250/36045]\tLoss: 399.8776\n",
      "Training Epoch: 35 [25300/36045]\tLoss: 525.2358\n",
      "Training Epoch: 35 [25350/36045]\tLoss: 521.7218\n",
      "Training Epoch: 35 [25400/36045]\tLoss: 486.9018\n",
      "Training Epoch: 35 [25450/36045]\tLoss: 489.2687\n",
      "Training Epoch: 35 [25500/36045]\tLoss: 531.6439\n",
      "Training Epoch: 35 [25550/36045]\tLoss: 621.6941\n",
      "Training Epoch: 35 [25600/36045]\tLoss: 625.9361\n",
      "Training Epoch: 35 [25650/36045]\tLoss: 603.9739\n",
      "Training Epoch: 35 [25700/36045]\tLoss: 613.1807\n",
      "Training Epoch: 35 [25750/36045]\tLoss: 591.6713\n",
      "Training Epoch: 35 [25800/36045]\tLoss: 373.4841\n",
      "Training Epoch: 35 [25850/36045]\tLoss: 382.5096\n",
      "Training Epoch: 35 [25900/36045]\tLoss: 363.5421\n",
      "Training Epoch: 35 [25950/36045]\tLoss: 372.3694\n",
      "Training Epoch: 35 [26000/36045]\tLoss: 456.9950\n",
      "Training Epoch: 35 [26050/36045]\tLoss: 622.3054\n",
      "Training Epoch: 35 [26100/36045]\tLoss: 649.5109\n",
      "Training Epoch: 35 [26150/36045]\tLoss: 649.6262\n",
      "Training Epoch: 35 [26200/36045]\tLoss: 622.6243\n",
      "Training Epoch: 35 [26250/36045]\tLoss: 652.4220\n",
      "Training Epoch: 35 [26300/36045]\tLoss: 593.6840\n",
      "Training Epoch: 35 [26350/36045]\tLoss: 604.7374\n",
      "Training Epoch: 35 [26400/36045]\tLoss: 581.4468\n",
      "Training Epoch: 35 [26450/36045]\tLoss: 511.2537\n",
      "Training Epoch: 35 [26500/36045]\tLoss: 606.2184\n",
      "Training Epoch: 35 [26550/36045]\tLoss: 606.5410\n",
      "Training Epoch: 35 [26600/36045]\tLoss: 603.0187\n",
      "Training Epoch: 35 [26650/36045]\tLoss: 618.9410\n",
      "Training Epoch: 35 [26700/36045]\tLoss: 598.3234\n",
      "Training Epoch: 35 [26750/36045]\tLoss: 560.4554\n",
      "Training Epoch: 35 [26800/36045]\tLoss: 413.1843\n",
      "Training Epoch: 35 [26850/36045]\tLoss: 342.5357\n",
      "Training Epoch: 35 [26900/36045]\tLoss: 345.2179\n",
      "Training Epoch: 35 [26950/36045]\tLoss: 379.3500\n",
      "Training Epoch: 35 [27000/36045]\tLoss: 620.8270\n",
      "Training Epoch: 35 [27050/36045]\tLoss: 648.3148\n",
      "Training Epoch: 35 [27100/36045]\tLoss: 628.2224\n",
      "Training Epoch: 35 [27150/36045]\tLoss: 668.2632\n",
      "Training Epoch: 35 [27200/36045]\tLoss: 487.1321\n",
      "Training Epoch: 35 [27250/36045]\tLoss: 478.5141\n",
      "Training Epoch: 35 [27300/36045]\tLoss: 466.2779\n",
      "Training Epoch: 35 [27350/36045]\tLoss: 464.4333\n",
      "Training Epoch: 35 [27400/36045]\tLoss: 463.6724\n",
      "Training Epoch: 35 [27450/36045]\tLoss: 586.0236\n",
      "Training Epoch: 35 [27500/36045]\tLoss: 627.9360\n",
      "Training Epoch: 35 [27550/36045]\tLoss: 621.1285\n",
      "Training Epoch: 35 [27600/36045]\tLoss: 632.7706\n",
      "Training Epoch: 35 [27650/36045]\tLoss: 624.6589\n",
      "Training Epoch: 35 [27700/36045]\tLoss: 653.1082\n",
      "Training Epoch: 35 [27750/36045]\tLoss: 665.1450\n",
      "Training Epoch: 35 [27800/36045]\tLoss: 652.0317\n",
      "Training Epoch: 35 [27850/36045]\tLoss: 641.9656\n",
      "Training Epoch: 35 [27900/36045]\tLoss: 581.5740\n",
      "Training Epoch: 35 [27950/36045]\tLoss: 484.9549\n",
      "Training Epoch: 35 [28000/36045]\tLoss: 461.5692\n",
      "Training Epoch: 35 [28050/36045]\tLoss: 471.4555\n",
      "Training Epoch: 35 [28100/36045]\tLoss: 463.0801\n",
      "Training Epoch: 35 [28150/36045]\tLoss: 484.7517\n",
      "Training Epoch: 35 [28200/36045]\tLoss: 492.3867\n",
      "Training Epoch: 35 [28250/36045]\tLoss: 485.5783\n",
      "Training Epoch: 35 [28300/36045]\tLoss: 460.9334\n",
      "Training Epoch: 35 [28350/36045]\tLoss: 456.9086\n",
      "Training Epoch: 35 [28400/36045]\tLoss: 786.5056\n",
      "Training Epoch: 35 [28450/36045]\tLoss: 720.7400\n",
      "Training Epoch: 35 [28500/36045]\tLoss: 622.7840\n",
      "Training Epoch: 35 [28550/36045]\tLoss: 572.2760\n",
      "Training Epoch: 35 [28600/36045]\tLoss: 600.9991\n",
      "Training Epoch: 35 [28650/36045]\tLoss: 662.1575\n",
      "Training Epoch: 35 [28700/36045]\tLoss: 656.2023\n",
      "Training Epoch: 35 [28750/36045]\tLoss: 642.6996\n",
      "Training Epoch: 35 [28800/36045]\tLoss: 651.8320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 35 [28850/36045]\tLoss: 565.6365\n",
      "Training Epoch: 35 [28900/36045]\tLoss: 459.5273\n",
      "Training Epoch: 35 [28950/36045]\tLoss: 458.5529\n",
      "Training Epoch: 35 [29000/36045]\tLoss: 455.2651\n",
      "Training Epoch: 35 [29050/36045]\tLoss: 462.0913\n",
      "Training Epoch: 35 [29100/36045]\tLoss: 480.6033\n",
      "Training Epoch: 35 [29150/36045]\tLoss: 470.0137\n",
      "Training Epoch: 35 [29200/36045]\tLoss: 455.7839\n",
      "Training Epoch: 35 [29250/36045]\tLoss: 445.5330\n",
      "Training Epoch: 35 [29300/36045]\tLoss: 503.8343\n",
      "Training Epoch: 35 [29350/36045]\tLoss: 593.1988\n",
      "Training Epoch: 35 [29400/36045]\tLoss: 610.6892\n",
      "Training Epoch: 35 [29450/36045]\tLoss: 628.2938\n",
      "Training Epoch: 35 [29500/36045]\tLoss: 643.1213\n",
      "Training Epoch: 35 [29550/36045]\tLoss: 612.1017\n",
      "Training Epoch: 35 [29600/36045]\tLoss: 515.9890\n",
      "Training Epoch: 35 [29650/36045]\tLoss: 497.9112\n",
      "Training Epoch: 35 [29700/36045]\tLoss: 445.5497\n",
      "Training Epoch: 35 [29750/36045]\tLoss: 444.2201\n",
      "Training Epoch: 35 [29800/36045]\tLoss: 491.3391\n",
      "Training Epoch: 35 [29850/36045]\tLoss: 567.7619\n",
      "Training Epoch: 35 [29900/36045]\tLoss: 564.0422\n",
      "Training Epoch: 35 [29950/36045]\tLoss: 585.8461\n",
      "Training Epoch: 35 [30000/36045]\tLoss: 560.1788\n",
      "Training Epoch: 35 [30050/36045]\tLoss: 566.7078\n",
      "Training Epoch: 35 [30100/36045]\tLoss: 691.4896\n",
      "Training Epoch: 35 [30150/36045]\tLoss: 674.6924\n",
      "Training Epoch: 35 [30200/36045]\tLoss: 636.4847\n",
      "Training Epoch: 35 [30250/36045]\tLoss: 684.9871\n",
      "Training Epoch: 35 [30300/36045]\tLoss: 669.6216\n",
      "Training Epoch: 35 [30350/36045]\tLoss: 515.4933\n",
      "Training Epoch: 35 [30400/36045]\tLoss: 500.0926\n",
      "Training Epoch: 35 [30450/36045]\tLoss: 502.1397\n",
      "Training Epoch: 35 [30500/36045]\tLoss: 468.8675\n",
      "Training Epoch: 35 [30550/36045]\tLoss: 434.6881\n",
      "Training Epoch: 35 [30600/36045]\tLoss: 425.9626\n",
      "Training Epoch: 35 [30650/36045]\tLoss: 415.9577\n",
      "Training Epoch: 35 [30700/36045]\tLoss: 433.8019\n",
      "Training Epoch: 35 [30750/36045]\tLoss: 420.7055\n",
      "Training Epoch: 35 [30800/36045]\tLoss: 447.1474\n",
      "Training Epoch: 35 [30850/36045]\tLoss: 438.8313\n",
      "Training Epoch: 35 [30900/36045]\tLoss: 451.1655\n",
      "Training Epoch: 35 [30950/36045]\tLoss: 473.9862\n",
      "Training Epoch: 35 [31000/36045]\tLoss: 466.0352\n",
      "Training Epoch: 35 [31050/36045]\tLoss: 389.3602\n",
      "Training Epoch: 35 [31100/36045]\tLoss: 379.9433\n",
      "Training Epoch: 35 [31150/36045]\tLoss: 387.3821\n",
      "Training Epoch: 35 [31200/36045]\tLoss: 481.6647\n",
      "Training Epoch: 35 [31250/36045]\tLoss: 624.9620\n",
      "Training Epoch: 35 [31300/36045]\tLoss: 595.7379\n",
      "Training Epoch: 35 [31350/36045]\tLoss: 611.6372\n",
      "Training Epoch: 35 [31400/36045]\tLoss: 591.0826\n",
      "Training Epoch: 35 [31450/36045]\tLoss: 607.4995\n",
      "Training Epoch: 35 [31500/36045]\tLoss: 619.9803\n",
      "Training Epoch: 35 [31550/36045]\tLoss: 627.3646\n",
      "Training Epoch: 35 [31600/36045]\tLoss: 589.7751\n",
      "Training Epoch: 35 [31650/36045]\tLoss: 630.9215\n",
      "Training Epoch: 35 [31700/36045]\tLoss: 456.7082\n",
      "Training Epoch: 35 [31750/36045]\tLoss: 377.6642\n",
      "Training Epoch: 35 [31800/36045]\tLoss: 360.3916\n",
      "Training Epoch: 35 [31850/36045]\tLoss: 368.7683\n",
      "Training Epoch: 35 [31900/36045]\tLoss: 580.6486\n",
      "Training Epoch: 35 [31950/36045]\tLoss: 750.8549\n",
      "Training Epoch: 35 [32000/36045]\tLoss: 859.9053\n",
      "Training Epoch: 35 [32050/36045]\tLoss: 815.0703\n",
      "Training Epoch: 35 [32100/36045]\tLoss: 805.6155\n",
      "Training Epoch: 35 [32150/36045]\tLoss: 622.2540\n",
      "Training Epoch: 35 [32200/36045]\tLoss: 625.1947\n",
      "Training Epoch: 35 [32250/36045]\tLoss: 635.6671\n",
      "Training Epoch: 35 [32300/36045]\tLoss: 617.4849\n",
      "Training Epoch: 35 [32350/36045]\tLoss: 613.1580\n",
      "Training Epoch: 35 [32400/36045]\tLoss: 575.3409\n",
      "Training Epoch: 35 [32450/36045]\tLoss: 473.7939\n",
      "Training Epoch: 35 [32500/36045]\tLoss: 455.3339\n",
      "Training Epoch: 35 [32550/36045]\tLoss: 457.5796\n",
      "Training Epoch: 35 [32600/36045]\tLoss: 454.4784\n",
      "Training Epoch: 35 [32650/36045]\tLoss: 586.1096\n",
      "Training Epoch: 35 [32700/36045]\tLoss: 639.7588\n",
      "Training Epoch: 35 [32750/36045]\tLoss: 609.5183\n",
      "Training Epoch: 35 [32800/36045]\tLoss: 625.0863\n",
      "Training Epoch: 35 [32850/36045]\tLoss: 577.0807\n",
      "Training Epoch: 35 [32900/36045]\tLoss: 462.3784\n",
      "Training Epoch: 35 [32950/36045]\tLoss: 484.1701\n",
      "Training Epoch: 35 [33000/36045]\tLoss: 483.3168\n",
      "Training Epoch: 35 [33050/36045]\tLoss: 459.3119\n",
      "Training Epoch: 35 [33100/36045]\tLoss: 521.9543\n",
      "Training Epoch: 35 [33150/36045]\tLoss: 709.2780\n",
      "Training Epoch: 35 [33200/36045]\tLoss: 690.6968\n",
      "Training Epoch: 35 [33250/36045]\tLoss: 711.7898\n",
      "Training Epoch: 35 [33300/36045]\tLoss: 758.2045\n",
      "Training Epoch: 35 [33350/36045]\tLoss: 581.1599\n",
      "Training Epoch: 35 [33400/36045]\tLoss: 424.9415\n",
      "Training Epoch: 35 [33450/36045]\tLoss: 420.3830\n",
      "Training Epoch: 35 [33500/36045]\tLoss: 432.7595\n",
      "Training Epoch: 35 [33550/36045]\tLoss: 448.5349\n",
      "Training Epoch: 35 [33600/36045]\tLoss: 450.3618\n",
      "Training Epoch: 35 [33650/36045]\tLoss: 601.1169\n",
      "Training Epoch: 35 [33700/36045]\tLoss: 581.4683\n",
      "Training Epoch: 35 [33750/36045]\tLoss: 602.0735\n",
      "Training Epoch: 35 [33800/36045]\tLoss: 598.5220\n",
      "Training Epoch: 35 [33850/36045]\tLoss: 600.7169\n",
      "Training Epoch: 35 [33900/36045]\tLoss: 613.8505\n",
      "Training Epoch: 35 [33950/36045]\tLoss: 624.1169\n",
      "Training Epoch: 35 [34000/36045]\tLoss: 610.6138\n",
      "Training Epoch: 35 [34050/36045]\tLoss: 614.6200\n",
      "Training Epoch: 35 [34100/36045]\tLoss: 592.0908\n",
      "Training Epoch: 35 [34150/36045]\tLoss: 549.3471\n",
      "Training Epoch: 35 [34200/36045]\tLoss: 520.2930\n",
      "Training Epoch: 35 [34250/36045]\tLoss: 534.5989\n",
      "Training Epoch: 35 [34300/36045]\tLoss: 456.7380\n",
      "Training Epoch: 35 [34350/36045]\tLoss: 481.0529\n",
      "Training Epoch: 35 [34400/36045]\tLoss: 473.1879\n",
      "Training Epoch: 35 [34450/36045]\tLoss: 445.1703\n",
      "Training Epoch: 35 [34500/36045]\tLoss: 475.0236\n",
      "Training Epoch: 35 [34550/36045]\tLoss: 466.2402\n",
      "Training Epoch: 35 [34600/36045]\tLoss: 471.6459\n",
      "Training Epoch: 35 [34650/36045]\tLoss: 578.2703\n",
      "Training Epoch: 35 [34700/36045]\tLoss: 612.8696\n",
      "Training Epoch: 35 [34750/36045]\tLoss: 543.3632\n",
      "Training Epoch: 35 [34800/36045]\tLoss: 623.0632\n",
      "Training Epoch: 35 [34850/36045]\tLoss: 630.7057\n",
      "Training Epoch: 35 [34900/36045]\tLoss: 685.9163\n",
      "Training Epoch: 35 [34950/36045]\tLoss: 671.3941\n",
      "Training Epoch: 35 [35000/36045]\tLoss: 672.2419\n",
      "Training Epoch: 35 [35050/36045]\tLoss: 658.8716\n",
      "Training Epoch: 35 [35100/36045]\tLoss: 563.5948\n",
      "Training Epoch: 35 [35150/36045]\tLoss: 556.3801\n",
      "Training Epoch: 35 [35200/36045]\tLoss: 469.4529\n",
      "Training Epoch: 35 [35250/36045]\tLoss: 515.4716\n",
      "Training Epoch: 35 [35300/36045]\tLoss: 530.5240\n",
      "Training Epoch: 35 [35350/36045]\tLoss: 596.9426\n",
      "Training Epoch: 35 [35400/36045]\tLoss: 629.3806\n",
      "Training Epoch: 35 [35450/36045]\tLoss: 600.2261\n",
      "Training Epoch: 35 [35500/36045]\tLoss: 581.3067\n",
      "Training Epoch: 35 [35550/36045]\tLoss: 566.8764\n",
      "Training Epoch: 35 [35600/36045]\tLoss: 616.0842\n",
      "Training Epoch: 35 [35650/36045]\tLoss: 689.2820\n",
      "Training Epoch: 35 [35700/36045]\tLoss: 616.6657\n",
      "Training Epoch: 35 [35750/36045]\tLoss: 674.1621\n",
      "Training Epoch: 35 [35800/36045]\tLoss: 680.5833\n",
      "Training Epoch: 35 [35850/36045]\tLoss: 654.5370\n",
      "Training Epoch: 35 [35900/36045]\tLoss: 678.6346\n",
      "Training Epoch: 35 [35950/36045]\tLoss: 676.0434\n",
      "Training Epoch: 35 [36000/36045]\tLoss: 668.6866\n",
      "Training Epoch: 35 [36045/36045]\tLoss: 653.0550\n",
      "Training Epoch: 35 [4004/4004]\tLoss: 606.9028\n",
      "Training Epoch: 36 [50/36045]\tLoss: 605.8839\n",
      "Training Epoch: 36 [100/36045]\tLoss: 581.0282\n",
      "Training Epoch: 36 [150/36045]\tLoss: 579.3665\n",
      "Training Epoch: 36 [200/36045]\tLoss: 565.8651\n",
      "Training Epoch: 36 [250/36045]\tLoss: 677.6816\n",
      "Training Epoch: 36 [300/36045]\tLoss: 742.0536\n",
      "Training Epoch: 36 [350/36045]\tLoss: 708.3229\n",
      "Training Epoch: 36 [400/36045]\tLoss: 703.4537\n",
      "Training Epoch: 36 [450/36045]\tLoss: 684.0470\n",
      "Training Epoch: 36 [500/36045]\tLoss: 634.7388\n",
      "Training Epoch: 36 [550/36045]\tLoss: 637.7894\n",
      "Training Epoch: 36 [600/36045]\tLoss: 621.5346\n",
      "Training Epoch: 36 [650/36045]\tLoss: 643.8363\n",
      "Training Epoch: 36 [700/36045]\tLoss: 630.0953\n",
      "Training Epoch: 36 [750/36045]\tLoss: 608.8830\n",
      "Training Epoch: 36 [800/36045]\tLoss: 621.4102\n",
      "Training Epoch: 36 [850/36045]\tLoss: 603.1508\n",
      "Training Epoch: 36 [900/36045]\tLoss: 576.4125\n",
      "Training Epoch: 36 [950/36045]\tLoss: 545.8938\n",
      "Training Epoch: 36 [1000/36045]\tLoss: 528.4028\n",
      "Training Epoch: 36 [1050/36045]\tLoss: 530.4944\n",
      "Training Epoch: 36 [1100/36045]\tLoss: 516.0107\n",
      "Training Epoch: 36 [1150/36045]\tLoss: 524.7664\n",
      "Training Epoch: 36 [1200/36045]\tLoss: 554.8390\n",
      "Training Epoch: 36 [1250/36045]\tLoss: 635.2850\n",
      "Training Epoch: 36 [1300/36045]\tLoss: 642.3486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 36 [1350/36045]\tLoss: 644.0569\n",
      "Training Epoch: 36 [1400/36045]\tLoss: 668.9465\n",
      "Training Epoch: 36 [1450/36045]\tLoss: 646.8986\n",
      "Training Epoch: 36 [1500/36045]\tLoss: 592.0695\n",
      "Training Epoch: 36 [1550/36045]\tLoss: 607.3502\n",
      "Training Epoch: 36 [1600/36045]\tLoss: 618.0485\n",
      "Training Epoch: 36 [1650/36045]\tLoss: 605.2333\n",
      "Training Epoch: 36 [1700/36045]\tLoss: 617.4164\n",
      "Training Epoch: 36 [1750/36045]\tLoss: 658.6868\n",
      "Training Epoch: 36 [1800/36045]\tLoss: 640.3806\n",
      "Training Epoch: 36 [1850/36045]\tLoss: 656.8586\n",
      "Training Epoch: 36 [1900/36045]\tLoss: 614.9211\n",
      "Training Epoch: 36 [1950/36045]\tLoss: 625.5486\n",
      "Training Epoch: 36 [2000/36045]\tLoss: 564.7023\n",
      "Training Epoch: 36 [2050/36045]\tLoss: 566.8943\n",
      "Training Epoch: 36 [2100/36045]\tLoss: 597.0970\n",
      "Training Epoch: 36 [2150/36045]\tLoss: 577.4048\n",
      "Training Epoch: 36 [2200/36045]\tLoss: 537.7133\n",
      "Training Epoch: 36 [2250/36045]\tLoss: 507.7758\n",
      "Training Epoch: 36 [2300/36045]\tLoss: 532.4075\n",
      "Training Epoch: 36 [2350/36045]\tLoss: 508.6424\n",
      "Training Epoch: 36 [2400/36045]\tLoss: 516.6494\n",
      "Training Epoch: 36 [2450/36045]\tLoss: 660.3506\n",
      "Training Epoch: 36 [2500/36045]\tLoss: 693.9282\n",
      "Training Epoch: 36 [2550/36045]\tLoss: 691.5849\n",
      "Training Epoch: 36 [2600/36045]\tLoss: 700.1204\n",
      "Training Epoch: 36 [2650/36045]\tLoss: 824.8451\n",
      "Training Epoch: 36 [2700/36045]\tLoss: 912.2028\n",
      "Training Epoch: 36 [2750/36045]\tLoss: 983.5913\n",
      "Training Epoch: 36 [2800/36045]\tLoss: 993.4290\n",
      "Training Epoch: 36 [2850/36045]\tLoss: 761.7316\n",
      "Training Epoch: 36 [2900/36045]\tLoss: 724.3676\n",
      "Training Epoch: 36 [2950/36045]\tLoss: 699.6259\n",
      "Training Epoch: 36 [3000/36045]\tLoss: 693.6683\n",
      "Training Epoch: 36 [3050/36045]\tLoss: 724.6129\n",
      "Training Epoch: 36 [3100/36045]\tLoss: 663.0220\n",
      "Training Epoch: 36 [3150/36045]\tLoss: 511.2327\n",
      "Training Epoch: 36 [3200/36045]\tLoss: 529.4957\n",
      "Training Epoch: 36 [3250/36045]\tLoss: 498.7238\n",
      "Training Epoch: 36 [3300/36045]\tLoss: 471.8827\n",
      "Training Epoch: 36 [3350/36045]\tLoss: 498.2657\n",
      "Training Epoch: 36 [3400/36045]\tLoss: 522.7351\n",
      "Training Epoch: 36 [3450/36045]\tLoss: 561.3684\n",
      "Training Epoch: 36 [3500/36045]\tLoss: 548.5527\n",
      "Training Epoch: 36 [3550/36045]\tLoss: 524.9283\n",
      "Training Epoch: 36 [3600/36045]\tLoss: 563.1206\n",
      "Training Epoch: 36 [3650/36045]\tLoss: 651.0996\n",
      "Training Epoch: 36 [3700/36045]\tLoss: 658.6719\n",
      "Training Epoch: 36 [3750/36045]\tLoss: 627.8658\n",
      "Training Epoch: 36 [3800/36045]\tLoss: 623.7586\n",
      "Training Epoch: 36 [3850/36045]\tLoss: 623.7794\n",
      "Training Epoch: 36 [3900/36045]\tLoss: 628.6435\n",
      "Training Epoch: 36 [3950/36045]\tLoss: 606.6374\n",
      "Training Epoch: 36 [4000/36045]\tLoss: 611.9351\n",
      "Training Epoch: 36 [4050/36045]\tLoss: 561.8035\n",
      "Training Epoch: 36 [4100/36045]\tLoss: 547.7352\n",
      "Training Epoch: 36 [4150/36045]\tLoss: 562.7267\n",
      "Training Epoch: 36 [4200/36045]\tLoss: 557.5240\n",
      "Training Epoch: 36 [4250/36045]\tLoss: 559.9363\n",
      "Training Epoch: 36 [4300/36045]\tLoss: 577.0706\n",
      "Training Epoch: 36 [4350/36045]\tLoss: 560.1659\n",
      "Training Epoch: 36 [4400/36045]\tLoss: 535.8445\n",
      "Training Epoch: 36 [4450/36045]\tLoss: 587.2766\n",
      "Training Epoch: 36 [4500/36045]\tLoss: 630.5103\n",
      "Training Epoch: 36 [4550/36045]\tLoss: 634.4354\n",
      "Training Epoch: 36 [4600/36045]\tLoss: 656.9379\n",
      "Training Epoch: 36 [4650/36045]\tLoss: 646.4224\n",
      "Training Epoch: 36 [4700/36045]\tLoss: 596.2853\n",
      "Training Epoch: 36 [4750/36045]\tLoss: 578.5533\n",
      "Training Epoch: 36 [4800/36045]\tLoss: 603.6996\n",
      "Training Epoch: 36 [4850/36045]\tLoss: 590.3367\n",
      "Training Epoch: 36 [4900/36045]\tLoss: 574.4984\n",
      "Training Epoch: 36 [4950/36045]\tLoss: 590.1572\n",
      "Training Epoch: 36 [5000/36045]\tLoss: 619.8911\n",
      "Training Epoch: 36 [5050/36045]\tLoss: 600.5463\n",
      "Training Epoch: 36 [5100/36045]\tLoss: 611.0060\n",
      "Training Epoch: 36 [5150/36045]\tLoss: 595.4079\n",
      "Training Epoch: 36 [5200/36045]\tLoss: 593.3694\n",
      "Training Epoch: 36 [5250/36045]\tLoss: 586.8974\n",
      "Training Epoch: 36 [5300/36045]\tLoss: 587.1896\n",
      "Training Epoch: 36 [5350/36045]\tLoss: 609.2900\n",
      "Training Epoch: 36 [5400/36045]\tLoss: 587.0472\n",
      "Training Epoch: 36 [5450/36045]\tLoss: 556.5943\n",
      "Training Epoch: 36 [5500/36045]\tLoss: 585.0266\n",
      "Training Epoch: 36 [5550/36045]\tLoss: 573.1956\n",
      "Training Epoch: 36 [5600/36045]\tLoss: 653.8036\n",
      "Training Epoch: 36 [5650/36045]\tLoss: 618.6065\n",
      "Training Epoch: 36 [5700/36045]\tLoss: 580.3654\n",
      "Training Epoch: 36 [5750/36045]\tLoss: 564.6406\n",
      "Training Epoch: 36 [5800/36045]\tLoss: 595.6644\n",
      "Training Epoch: 36 [5850/36045]\tLoss: 583.9745\n",
      "Training Epoch: 36 [5900/36045]\tLoss: 671.4449\n",
      "Training Epoch: 36 [5950/36045]\tLoss: 688.3518\n",
      "Training Epoch: 36 [6000/36045]\tLoss: 673.9343\n",
      "Training Epoch: 36 [6050/36045]\tLoss: 651.5795\n",
      "Training Epoch: 36 [6100/36045]\tLoss: 655.8354\n",
      "Training Epoch: 36 [6150/36045]\tLoss: 644.4754\n",
      "Training Epoch: 36 [6200/36045]\tLoss: 647.7814\n",
      "Training Epoch: 36 [6250/36045]\tLoss: 669.2852\n",
      "Training Epoch: 36 [6300/36045]\tLoss: 680.9769\n",
      "Training Epoch: 36 [6350/36045]\tLoss: 727.2728\n",
      "Training Epoch: 36 [6400/36045]\tLoss: 601.5332\n",
      "Training Epoch: 36 [6450/36045]\tLoss: 554.3549\n",
      "Training Epoch: 36 [6500/36045]\tLoss: 564.4862\n",
      "Training Epoch: 36 [6550/36045]\tLoss: 581.8060\n",
      "Training Epoch: 36 [6600/36045]\tLoss: 580.3427\n",
      "Training Epoch: 36 [6650/36045]\tLoss: 654.8368\n",
      "Training Epoch: 36 [6700/36045]\tLoss: 684.9811\n",
      "Training Epoch: 36 [6750/36045]\tLoss: 661.6581\n",
      "Training Epoch: 36 [6800/36045]\tLoss: 664.5676\n",
      "Training Epoch: 36 [6850/36045]\tLoss: 652.5529\n",
      "Training Epoch: 36 [6900/36045]\tLoss: 581.3734\n",
      "Training Epoch: 36 [6950/36045]\tLoss: 548.1014\n",
      "Training Epoch: 36 [7000/36045]\tLoss: 583.0800\n",
      "Training Epoch: 36 [7050/36045]\tLoss: 595.8845\n",
      "Training Epoch: 36 [7100/36045]\tLoss: 595.3376\n",
      "Training Epoch: 36 [7150/36045]\tLoss: 605.7774\n",
      "Training Epoch: 36 [7200/36045]\tLoss: 608.6466\n",
      "Training Epoch: 36 [7250/36045]\tLoss: 606.6251\n",
      "Training Epoch: 36 [7300/36045]\tLoss: 593.1877\n",
      "Training Epoch: 36 [7350/36045]\tLoss: 590.3002\n",
      "Training Epoch: 36 [7400/36045]\tLoss: 536.9774\n",
      "Training Epoch: 36 [7450/36045]\tLoss: 540.2263\n",
      "Training Epoch: 36 [7500/36045]\tLoss: 535.4941\n",
      "Training Epoch: 36 [7550/36045]\tLoss: 512.9582\n",
      "Training Epoch: 36 [7600/36045]\tLoss: 568.9685\n",
      "Training Epoch: 36 [7650/36045]\tLoss: 608.7594\n",
      "Training Epoch: 36 [7700/36045]\tLoss: 579.4848\n",
      "Training Epoch: 36 [7750/36045]\tLoss: 594.0309\n",
      "Training Epoch: 36 [7800/36045]\tLoss: 583.1284\n",
      "Training Epoch: 36 [7850/36045]\tLoss: 564.4367\n",
      "Training Epoch: 36 [7900/36045]\tLoss: 595.3379\n",
      "Training Epoch: 36 [7950/36045]\tLoss: 592.7816\n",
      "Training Epoch: 36 [8000/36045]\tLoss: 610.8948\n",
      "Training Epoch: 36 [8050/36045]\tLoss: 575.8787\n",
      "Training Epoch: 36 [8100/36045]\tLoss: 601.1899\n",
      "Training Epoch: 36 [8150/36045]\tLoss: 681.1279\n",
      "Training Epoch: 36 [8200/36045]\tLoss: 668.1721\n",
      "Training Epoch: 36 [8250/36045]\tLoss: 635.9703\n",
      "Training Epoch: 36 [8300/36045]\tLoss: 694.2192\n",
      "Training Epoch: 36 [8350/36045]\tLoss: 637.0022\n",
      "Training Epoch: 36 [8400/36045]\tLoss: 570.4221\n",
      "Training Epoch: 36 [8450/36045]\tLoss: 534.0485\n",
      "Training Epoch: 36 [8500/36045]\tLoss: 567.8719\n",
      "Training Epoch: 36 [8550/36045]\tLoss: 560.5928\n",
      "Training Epoch: 36 [8600/36045]\tLoss: 554.6790\n",
      "Training Epoch: 36 [8650/36045]\tLoss: 590.2794\n",
      "Training Epoch: 36 [8700/36045]\tLoss: 624.3113\n",
      "Training Epoch: 36 [8750/36045]\tLoss: 613.1840\n",
      "Training Epoch: 36 [8800/36045]\tLoss: 618.9624\n",
      "Training Epoch: 36 [8850/36045]\tLoss: 612.4048\n",
      "Training Epoch: 36 [8900/36045]\tLoss: 552.8439\n",
      "Training Epoch: 36 [8950/36045]\tLoss: 564.3500\n",
      "Training Epoch: 36 [9000/36045]\tLoss: 580.0452\n",
      "Training Epoch: 36 [9050/36045]\tLoss: 581.4907\n",
      "Training Epoch: 36 [9100/36045]\tLoss: 598.6443\n",
      "Training Epoch: 36 [9150/36045]\tLoss: 442.9024\n",
      "Training Epoch: 36 [9200/36045]\tLoss: 331.3855\n",
      "Training Epoch: 36 [9250/36045]\tLoss: 359.4781\n",
      "Training Epoch: 36 [9300/36045]\tLoss: 369.6492\n",
      "Training Epoch: 36 [9350/36045]\tLoss: 340.9722\n",
      "Training Epoch: 36 [9400/36045]\tLoss: 668.1671\n",
      "Training Epoch: 36 [9450/36045]\tLoss: 709.6845\n",
      "Training Epoch: 36 [9500/36045]\tLoss: 696.9017\n",
      "Training Epoch: 36 [9550/36045]\tLoss: 737.2481\n",
      "Training Epoch: 36 [9600/36045]\tLoss: 548.4496\n",
      "Training Epoch: 36 [9650/36045]\tLoss: 553.3024\n",
      "Training Epoch: 36 [9700/36045]\tLoss: 538.7690\n",
      "Training Epoch: 36 [9750/36045]\tLoss: 537.5738\n",
      "Training Epoch: 36 [9800/36045]\tLoss: 702.9812\n",
      "Training Epoch: 36 [9850/36045]\tLoss: 742.5677\n",
      "Training Epoch: 36 [9900/36045]\tLoss: 753.5156\n",
      "Training Epoch: 36 [9950/36045]\tLoss: 734.2156\n",
      "Training Epoch: 36 [10000/36045]\tLoss: 678.8210\n",
      "Training Epoch: 36 [10050/36045]\tLoss: 557.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 36 [10100/36045]\tLoss: 564.6415\n",
      "Training Epoch: 36 [10150/36045]\tLoss: 573.5131\n",
      "Training Epoch: 36 [10200/36045]\tLoss: 562.4232\n",
      "Training Epoch: 36 [10250/36045]\tLoss: 674.1704\n",
      "Training Epoch: 36 [10300/36045]\tLoss: 654.9554\n",
      "Training Epoch: 36 [10350/36045]\tLoss: 689.8504\n",
      "Training Epoch: 36 [10400/36045]\tLoss: 680.2397\n",
      "Training Epoch: 36 [10450/36045]\tLoss: 637.3243\n",
      "Training Epoch: 36 [10500/36045]\tLoss: 532.5893\n",
      "Training Epoch: 36 [10550/36045]\tLoss: 527.4144\n",
      "Training Epoch: 36 [10600/36045]\tLoss: 549.8103\n",
      "Training Epoch: 36 [10650/36045]\tLoss: 555.9622\n",
      "Training Epoch: 36 [10700/36045]\tLoss: 638.0707\n",
      "Training Epoch: 36 [10750/36045]\tLoss: 698.0835\n",
      "Training Epoch: 36 [10800/36045]\tLoss: 643.0708\n",
      "Training Epoch: 36 [10850/36045]\tLoss: 681.5479\n",
      "Training Epoch: 36 [10900/36045]\tLoss: 709.4294\n",
      "Training Epoch: 36 [10950/36045]\tLoss: 523.0107\n",
      "Training Epoch: 36 [11000/36045]\tLoss: 516.6282\n",
      "Training Epoch: 36 [11050/36045]\tLoss: 553.7683\n",
      "Training Epoch: 36 [11100/36045]\tLoss: 564.3832\n",
      "Training Epoch: 36 [11150/36045]\tLoss: 612.0779\n",
      "Training Epoch: 36 [11200/36045]\tLoss: 640.1046\n",
      "Training Epoch: 36 [11250/36045]\tLoss: 651.8528\n",
      "Training Epoch: 36 [11300/36045]\tLoss: 631.9929\n",
      "Training Epoch: 36 [11350/36045]\tLoss: 629.4940\n",
      "Training Epoch: 36 [11400/36045]\tLoss: 592.1792\n",
      "Training Epoch: 36 [11450/36045]\tLoss: 560.6977\n",
      "Training Epoch: 36 [11500/36045]\tLoss: 558.0777\n",
      "Training Epoch: 36 [11550/36045]\tLoss: 568.5321\n",
      "Training Epoch: 36 [11600/36045]\tLoss: 629.2839\n",
      "Training Epoch: 36 [11650/36045]\tLoss: 681.0986\n",
      "Training Epoch: 36 [11700/36045]\tLoss: 680.0638\n",
      "Training Epoch: 36 [11750/36045]\tLoss: 698.6035\n",
      "Training Epoch: 36 [11800/36045]\tLoss: 741.2836\n",
      "Training Epoch: 36 [11850/36045]\tLoss: 799.2949\n",
      "Training Epoch: 36 [11900/36045]\tLoss: 1012.8895\n",
      "Training Epoch: 36 [11950/36045]\tLoss: 1015.5964\n",
      "Training Epoch: 36 [12000/36045]\tLoss: 1027.8187\n",
      "Training Epoch: 36 [12050/36045]\tLoss: 986.8882\n",
      "Training Epoch: 36 [12100/36045]\tLoss: 636.0865\n",
      "Training Epoch: 36 [12150/36045]\tLoss: 481.9399\n",
      "Training Epoch: 36 [12200/36045]\tLoss: 476.6437\n",
      "Training Epoch: 36 [12250/36045]\tLoss: 485.4437\n",
      "Training Epoch: 36 [12300/36045]\tLoss: 623.3363\n",
      "Training Epoch: 36 [12350/36045]\tLoss: 679.4920\n",
      "Training Epoch: 36 [12400/36045]\tLoss: 686.9738\n",
      "Training Epoch: 36 [12450/36045]\tLoss: 675.6431\n",
      "Training Epoch: 36 [12500/36045]\tLoss: 703.1179\n",
      "Training Epoch: 36 [12550/36045]\tLoss: 672.6769\n",
      "Training Epoch: 36 [12600/36045]\tLoss: 617.0124\n",
      "Training Epoch: 36 [12650/36045]\tLoss: 615.5251\n",
      "Training Epoch: 36 [12700/36045]\tLoss: 636.9241\n",
      "Training Epoch: 36 [12750/36045]\tLoss: 635.7051\n",
      "Training Epoch: 36 [12800/36045]\tLoss: 620.5780\n",
      "Training Epoch: 36 [12850/36045]\tLoss: 649.7813\n",
      "Training Epoch: 36 [12900/36045]\tLoss: 623.1119\n",
      "Training Epoch: 36 [12950/36045]\tLoss: 609.5065\n",
      "Training Epoch: 36 [13000/36045]\tLoss: 642.3837\n",
      "Training Epoch: 36 [13050/36045]\tLoss: 581.8153\n",
      "Training Epoch: 36 [13100/36045]\tLoss: 598.5761\n",
      "Training Epoch: 36 [13150/36045]\tLoss: 590.2589\n",
      "Training Epoch: 36 [13200/36045]\tLoss: 572.3795\n",
      "Training Epoch: 36 [13250/36045]\tLoss: 595.1860\n",
      "Training Epoch: 36 [13300/36045]\tLoss: 633.0629\n",
      "Training Epoch: 36 [13350/36045]\tLoss: 613.5054\n",
      "Training Epoch: 36 [13400/36045]\tLoss: 616.8105\n",
      "Training Epoch: 36 [13450/36045]\tLoss: 613.8546\n",
      "Training Epoch: 36 [13500/36045]\tLoss: 633.3940\n",
      "Training Epoch: 36 [13550/36045]\tLoss: 770.6348\n",
      "Training Epoch: 36 [13600/36045]\tLoss: 803.3989\n",
      "Training Epoch: 36 [13650/36045]\tLoss: 884.4863\n",
      "Training Epoch: 36 [13700/36045]\tLoss: 780.3198\n",
      "Training Epoch: 36 [13750/36045]\tLoss: 620.1385\n",
      "Training Epoch: 36 [13800/36045]\tLoss: 591.7805\n",
      "Training Epoch: 36 [13850/36045]\tLoss: 574.4730\n",
      "Training Epoch: 36 [13900/36045]\tLoss: 581.7148\n",
      "Training Epoch: 36 [13950/36045]\tLoss: 627.0104\n",
      "Training Epoch: 36 [14000/36045]\tLoss: 660.2778\n",
      "Training Epoch: 36 [14050/36045]\tLoss: 635.0209\n",
      "Training Epoch: 36 [14100/36045]\tLoss: 630.2897\n",
      "Training Epoch: 36 [14150/36045]\tLoss: 618.2233\n",
      "Training Epoch: 36 [14200/36045]\tLoss: 659.2739\n",
      "Training Epoch: 36 [14250/36045]\tLoss: 724.0464\n",
      "Training Epoch: 36 [14300/36045]\tLoss: 727.4313\n",
      "Training Epoch: 36 [14350/36045]\tLoss: 695.6351\n",
      "Training Epoch: 36 [14400/36045]\tLoss: 681.4305\n",
      "Training Epoch: 36 [14450/36045]\tLoss: 717.3469\n",
      "Training Epoch: 36 [14500/36045]\tLoss: 648.3490\n",
      "Training Epoch: 36 [14550/36045]\tLoss: 677.4500\n",
      "Training Epoch: 36 [14600/36045]\tLoss: 663.7327\n",
      "Training Epoch: 36 [14650/36045]\tLoss: 663.4499\n",
      "Training Epoch: 36 [14700/36045]\tLoss: 628.3066\n",
      "Training Epoch: 36 [14750/36045]\tLoss: 540.1820\n",
      "Training Epoch: 36 [14800/36045]\tLoss: 530.3403\n",
      "Training Epoch: 36 [14850/36045]\tLoss: 537.3764\n",
      "Training Epoch: 36 [14900/36045]\tLoss: 530.9013\n",
      "Training Epoch: 36 [14950/36045]\tLoss: 538.8275\n",
      "Training Epoch: 36 [15000/36045]\tLoss: 552.1383\n",
      "Training Epoch: 36 [15050/36045]\tLoss: 549.2701\n",
      "Training Epoch: 36 [15100/36045]\tLoss: 533.3505\n",
      "Training Epoch: 36 [15150/36045]\tLoss: 528.3657\n",
      "Training Epoch: 36 [15200/36045]\tLoss: 489.3065\n",
      "Training Epoch: 36 [15250/36045]\tLoss: 511.5308\n",
      "Training Epoch: 36 [15300/36045]\tLoss: 496.8213\n",
      "Training Epoch: 36 [15350/36045]\tLoss: 508.5680\n",
      "Training Epoch: 36 [15400/36045]\tLoss: 491.9193\n",
      "Training Epoch: 36 [15450/36045]\tLoss: 477.2125\n",
      "Training Epoch: 36 [15500/36045]\tLoss: 490.9308\n",
      "Training Epoch: 36 [15550/36045]\tLoss: 487.0920\n",
      "Training Epoch: 36 [15600/36045]\tLoss: 555.3395\n",
      "Training Epoch: 36 [15650/36045]\tLoss: 572.7425\n",
      "Training Epoch: 36 [15700/36045]\tLoss: 564.5887\n",
      "Training Epoch: 36 [15750/36045]\tLoss: 556.0641\n",
      "Training Epoch: 36 [15800/36045]\tLoss: 529.6973\n",
      "Training Epoch: 36 [15850/36045]\tLoss: 544.4485\n",
      "Training Epoch: 36 [15900/36045]\tLoss: 553.6716\n",
      "Training Epoch: 36 [15950/36045]\tLoss: 573.4828\n",
      "Training Epoch: 36 [16000/36045]\tLoss: 545.3809\n",
      "Training Epoch: 36 [16050/36045]\tLoss: 514.7867\n",
      "Training Epoch: 36 [16100/36045]\tLoss: 477.4659\n",
      "Training Epoch: 36 [16150/36045]\tLoss: 465.3831\n",
      "Training Epoch: 36 [16200/36045]\tLoss: 564.5357\n",
      "Training Epoch: 36 [16250/36045]\tLoss: 592.5683\n",
      "Training Epoch: 36 [16300/36045]\tLoss: 646.9263\n",
      "Training Epoch: 36 [16350/36045]\tLoss: 666.6643\n",
      "Training Epoch: 36 [16400/36045]\tLoss: 638.4052\n",
      "Training Epoch: 36 [16450/36045]\tLoss: 620.4718\n",
      "Training Epoch: 36 [16500/36045]\tLoss: 620.3927\n",
      "Training Epoch: 36 [16550/36045]\tLoss: 585.4003\n",
      "Training Epoch: 36 [16600/36045]\tLoss: 608.3438\n",
      "Training Epoch: 36 [16650/36045]\tLoss: 625.1687\n",
      "Training Epoch: 36 [16700/36045]\tLoss: 604.3945\n",
      "Training Epoch: 36 [16750/36045]\tLoss: 597.0009\n",
      "Training Epoch: 36 [16800/36045]\tLoss: 606.5139\n",
      "Training Epoch: 36 [16850/36045]\tLoss: 577.8358\n",
      "Training Epoch: 36 [16900/36045]\tLoss: 587.7278\n",
      "Training Epoch: 36 [16950/36045]\tLoss: 611.0335\n",
      "Training Epoch: 36 [17000/36045]\tLoss: 594.8599\n",
      "Training Epoch: 36 [17050/36045]\tLoss: 619.9111\n",
      "Training Epoch: 36 [17100/36045]\tLoss: 616.0568\n",
      "Training Epoch: 36 [17150/36045]\tLoss: 535.2616\n",
      "Training Epoch: 36 [17200/36045]\tLoss: 497.1259\n",
      "Training Epoch: 36 [17250/36045]\tLoss: 520.4438\n",
      "Training Epoch: 36 [17300/36045]\tLoss: 550.7302\n",
      "Training Epoch: 36 [17350/36045]\tLoss: 530.4100\n",
      "Training Epoch: 36 [17400/36045]\tLoss: 549.8530\n",
      "Training Epoch: 36 [17450/36045]\tLoss: 568.8370\n",
      "Training Epoch: 36 [17500/36045]\tLoss: 557.0712\n",
      "Training Epoch: 36 [17550/36045]\tLoss: 555.8635\n",
      "Training Epoch: 36 [17600/36045]\tLoss: 549.9262\n",
      "Training Epoch: 36 [17650/36045]\tLoss: 565.9550\n",
      "Training Epoch: 36 [17700/36045]\tLoss: 545.1155\n",
      "Training Epoch: 36 [17750/36045]\tLoss: 561.4755\n",
      "Training Epoch: 36 [17800/36045]\tLoss: 552.3873\n",
      "Training Epoch: 36 [17850/36045]\tLoss: 566.3318\n",
      "Training Epoch: 36 [17900/36045]\tLoss: 594.6267\n",
      "Training Epoch: 36 [17950/36045]\tLoss: 606.6777\n",
      "Training Epoch: 36 [18000/36045]\tLoss: 597.1285\n",
      "Training Epoch: 36 [18050/36045]\tLoss: 657.4476\n",
      "Training Epoch: 36 [18100/36045]\tLoss: 659.0004\n",
      "Training Epoch: 36 [18150/36045]\tLoss: 670.3599\n",
      "Training Epoch: 36 [18200/36045]\tLoss: 652.3868\n",
      "Training Epoch: 36 [18250/36045]\tLoss: 673.2448\n",
      "Training Epoch: 36 [18300/36045]\tLoss: 626.6865\n",
      "Training Epoch: 36 [18350/36045]\tLoss: 698.8211\n",
      "Training Epoch: 36 [18400/36045]\tLoss: 672.0180\n",
      "Training Epoch: 36 [18450/36045]\tLoss: 651.7852\n",
      "Training Epoch: 36 [18500/36045]\tLoss: 650.7642\n",
      "Training Epoch: 36 [18550/36045]\tLoss: 638.0416\n",
      "Training Epoch: 36 [18600/36045]\tLoss: 627.5154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 36 [18650/36045]\tLoss: 673.6569\n",
      "Training Epoch: 36 [18700/36045]\tLoss: 708.6592\n",
      "Training Epoch: 36 [18750/36045]\tLoss: 695.8479\n",
      "Training Epoch: 36 [18800/36045]\tLoss: 719.0400\n",
      "Training Epoch: 36 [18850/36045]\tLoss: 663.7745\n",
      "Training Epoch: 36 [18900/36045]\tLoss: 710.0813\n",
      "Training Epoch: 36 [18950/36045]\tLoss: 652.2094\n",
      "Training Epoch: 36 [19000/36045]\tLoss: 539.8942\n",
      "Training Epoch: 36 [19050/36045]\tLoss: 523.8781\n",
      "Training Epoch: 36 [19100/36045]\tLoss: 532.1131\n",
      "Training Epoch: 36 [19150/36045]\tLoss: 522.1312\n",
      "Training Epoch: 36 [19200/36045]\tLoss: 552.0955\n",
      "Training Epoch: 36 [19250/36045]\tLoss: 567.0300\n",
      "Training Epoch: 36 [19300/36045]\tLoss: 576.7930\n",
      "Training Epoch: 36 [19350/36045]\tLoss: 560.3746\n",
      "Training Epoch: 36 [19400/36045]\tLoss: 581.3328\n",
      "Training Epoch: 36 [19450/36045]\tLoss: 572.4969\n",
      "Training Epoch: 36 [19500/36045]\tLoss: 573.9920\n",
      "Training Epoch: 36 [19550/36045]\tLoss: 572.4020\n",
      "Training Epoch: 36 [19600/36045]\tLoss: 613.2553\n",
      "Training Epoch: 36 [19650/36045]\tLoss: 816.0053\n",
      "Training Epoch: 36 [19700/36045]\tLoss: 774.7235\n",
      "Training Epoch: 36 [19750/36045]\tLoss: 778.4761\n",
      "Training Epoch: 36 [19800/36045]\tLoss: 778.5134\n",
      "Training Epoch: 36 [19850/36045]\tLoss: 512.5000\n",
      "Training Epoch: 36 [19900/36045]\tLoss: 491.1746\n",
      "Training Epoch: 36 [19950/36045]\tLoss: 494.8213\n",
      "Training Epoch: 36 [20000/36045]\tLoss: 494.1834\n",
      "Training Epoch: 36 [20050/36045]\tLoss: 553.3587\n",
      "Training Epoch: 36 [20100/36045]\tLoss: 559.2943\n",
      "Training Epoch: 36 [20150/36045]\tLoss: 560.8177\n",
      "Training Epoch: 36 [20200/36045]\tLoss: 560.5569\n",
      "Training Epoch: 36 [20250/36045]\tLoss: 597.3156\n",
      "Training Epoch: 36 [20300/36045]\tLoss: 633.2880\n",
      "Training Epoch: 36 [20350/36045]\tLoss: 651.9139\n",
      "Training Epoch: 36 [20400/36045]\tLoss: 668.5663\n",
      "Training Epoch: 36 [20450/36045]\tLoss: 638.8014\n",
      "Training Epoch: 36 [20500/36045]\tLoss: 623.0365\n",
      "Training Epoch: 36 [20550/36045]\tLoss: 547.7939\n",
      "Training Epoch: 36 [20600/36045]\tLoss: 558.0467\n",
      "Training Epoch: 36 [20650/36045]\tLoss: 555.3060\n",
      "Training Epoch: 36 [20700/36045]\tLoss: 543.3396\n",
      "Training Epoch: 36 [20750/36045]\tLoss: 585.5031\n",
      "Training Epoch: 36 [20800/36045]\tLoss: 636.1297\n",
      "Training Epoch: 36 [20850/36045]\tLoss: 622.8454\n",
      "Training Epoch: 36 [20900/36045]\tLoss: 666.8303\n",
      "Training Epoch: 36 [20950/36045]\tLoss: 628.7017\n",
      "Training Epoch: 36 [21000/36045]\tLoss: 591.8713\n",
      "Training Epoch: 36 [21050/36045]\tLoss: 506.4465\n",
      "Training Epoch: 36 [21100/36045]\tLoss: 510.7842\n",
      "Training Epoch: 36 [21150/36045]\tLoss: 546.7175\n",
      "Training Epoch: 36 [21200/36045]\tLoss: 545.9296\n",
      "Training Epoch: 36 [21250/36045]\tLoss: 522.2819\n",
      "Training Epoch: 36 [21300/36045]\tLoss: 609.8988\n",
      "Training Epoch: 36 [21350/36045]\tLoss: 601.8351\n",
      "Training Epoch: 36 [21400/36045]\tLoss: 605.3857\n",
      "Training Epoch: 36 [21450/36045]\tLoss: 611.5655\n",
      "Training Epoch: 36 [21500/36045]\tLoss: 613.7512\n",
      "Training Epoch: 36 [21550/36045]\tLoss: 708.3953\n",
      "Training Epoch: 36 [21600/36045]\tLoss: 707.1777\n",
      "Training Epoch: 36 [21650/36045]\tLoss: 719.6016\n",
      "Training Epoch: 36 [21700/36045]\tLoss: 722.3395\n",
      "Training Epoch: 36 [21750/36045]\tLoss: 693.6794\n",
      "Training Epoch: 36 [21800/36045]\tLoss: 510.8596\n",
      "Training Epoch: 36 [21850/36045]\tLoss: 493.8898\n",
      "Training Epoch: 36 [21900/36045]\tLoss: 503.4825\n",
      "Training Epoch: 36 [21950/36045]\tLoss: 504.0594\n",
      "Training Epoch: 36 [22000/36045]\tLoss: 507.4132\n",
      "Training Epoch: 36 [22050/36045]\tLoss: 528.5652\n",
      "Training Epoch: 36 [22100/36045]\tLoss: 521.1868\n",
      "Training Epoch: 36 [22150/36045]\tLoss: 507.0237\n",
      "Training Epoch: 36 [22200/36045]\tLoss: 523.2128\n",
      "Training Epoch: 36 [22250/36045]\tLoss: 528.1712\n",
      "Training Epoch: 36 [22300/36045]\tLoss: 580.3267\n",
      "Training Epoch: 36 [22350/36045]\tLoss: 606.1810\n",
      "Training Epoch: 36 [22400/36045]\tLoss: 620.6166\n",
      "Training Epoch: 36 [22450/36045]\tLoss: 608.0121\n",
      "Training Epoch: 36 [22500/36045]\tLoss: 590.4879\n",
      "Training Epoch: 36 [22550/36045]\tLoss: 625.9119\n",
      "Training Epoch: 36 [22600/36045]\tLoss: 677.3438\n",
      "Training Epoch: 36 [22650/36045]\tLoss: 711.5469\n",
      "Training Epoch: 36 [22700/36045]\tLoss: 733.5703\n",
      "Training Epoch: 36 [22750/36045]\tLoss: 752.9449\n",
      "Training Epoch: 36 [22800/36045]\tLoss: 782.6937\n",
      "Training Epoch: 36 [22850/36045]\tLoss: 650.7444\n",
      "Training Epoch: 36 [22900/36045]\tLoss: 655.6398\n",
      "Training Epoch: 36 [22950/36045]\tLoss: 635.1493\n",
      "Training Epoch: 36 [23000/36045]\tLoss: 632.0258\n",
      "Training Epoch: 36 [23050/36045]\tLoss: 561.4201\n",
      "Training Epoch: 36 [23100/36045]\tLoss: 577.1705\n",
      "Training Epoch: 36 [23150/36045]\tLoss: 565.4786\n",
      "Training Epoch: 36 [23200/36045]\tLoss: 535.5010\n",
      "Training Epoch: 36 [23250/36045]\tLoss: 538.5382\n",
      "Training Epoch: 36 [23300/36045]\tLoss: 534.7130\n",
      "Training Epoch: 36 [23350/36045]\tLoss: 555.4862\n",
      "Training Epoch: 36 [23400/36045]\tLoss: 602.1351\n",
      "Training Epoch: 36 [23450/36045]\tLoss: 595.5246\n",
      "Training Epoch: 36 [23500/36045]\tLoss: 574.0085\n",
      "Training Epoch: 36 [23550/36045]\tLoss: 615.2544\n",
      "Training Epoch: 36 [23600/36045]\tLoss: 696.7737\n",
      "Training Epoch: 36 [23650/36045]\tLoss: 709.1480\n",
      "Training Epoch: 36 [23700/36045]\tLoss: 717.2674\n",
      "Training Epoch: 36 [23750/36045]\tLoss: 693.1902\n",
      "Training Epoch: 36 [23800/36045]\tLoss: 554.8405\n",
      "Training Epoch: 36 [23850/36045]\tLoss: 580.9272\n",
      "Training Epoch: 36 [23900/36045]\tLoss: 570.5716\n",
      "Training Epoch: 36 [23950/36045]\tLoss: 553.6851\n",
      "Training Epoch: 36 [24000/36045]\tLoss: 530.5690\n",
      "Training Epoch: 36 [24050/36045]\tLoss: 490.0950\n",
      "Training Epoch: 36 [24100/36045]\tLoss: 515.4725\n",
      "Training Epoch: 36 [24150/36045]\tLoss: 508.1655\n",
      "Training Epoch: 36 [24200/36045]\tLoss: 505.0775\n",
      "Training Epoch: 36 [24250/36045]\tLoss: 490.4266\n",
      "Training Epoch: 36 [24300/36045]\tLoss: 529.9148\n",
      "Training Epoch: 36 [24350/36045]\tLoss: 542.6760\n",
      "Training Epoch: 36 [24400/36045]\tLoss: 557.8596\n",
      "Training Epoch: 36 [24450/36045]\tLoss: 531.6682\n",
      "Training Epoch: 36 [24500/36045]\tLoss: 560.8539\n",
      "Training Epoch: 36 [24550/36045]\tLoss: 649.1194\n",
      "Training Epoch: 36 [24600/36045]\tLoss: 640.8382\n",
      "Training Epoch: 36 [24650/36045]\tLoss: 613.8525\n",
      "Training Epoch: 36 [24700/36045]\tLoss: 623.6884\n",
      "Training Epoch: 36 [24750/36045]\tLoss: 576.2946\n",
      "Training Epoch: 36 [24800/36045]\tLoss: 474.1487\n",
      "Training Epoch: 36 [24850/36045]\tLoss: 493.1748\n",
      "Training Epoch: 36 [24900/36045]\tLoss: 490.5240\n",
      "Training Epoch: 36 [24950/36045]\tLoss: 493.0985\n",
      "Training Epoch: 36 [25000/36045]\tLoss: 473.9844\n",
      "Training Epoch: 36 [25050/36045]\tLoss: 452.9010\n",
      "Training Epoch: 36 [25100/36045]\tLoss: 405.7913\n",
      "Training Epoch: 36 [25150/36045]\tLoss: 375.8072\n",
      "Training Epoch: 36 [25200/36045]\tLoss: 370.8792\n",
      "Training Epoch: 36 [25250/36045]\tLoss: 397.5143\n",
      "Training Epoch: 36 [25300/36045]\tLoss: 522.1585\n",
      "Training Epoch: 36 [25350/36045]\tLoss: 518.5872\n",
      "Training Epoch: 36 [25400/36045]\tLoss: 484.0792\n",
      "Training Epoch: 36 [25450/36045]\tLoss: 486.4106\n",
      "Training Epoch: 36 [25500/36045]\tLoss: 528.5278\n",
      "Training Epoch: 36 [25550/36045]\tLoss: 618.1700\n",
      "Training Epoch: 36 [25600/36045]\tLoss: 622.3582\n",
      "Training Epoch: 36 [25650/36045]\tLoss: 600.5212\n",
      "Training Epoch: 36 [25700/36045]\tLoss: 609.7611\n",
      "Training Epoch: 36 [25750/36045]\tLoss: 588.5023\n",
      "Training Epoch: 36 [25800/36045]\tLoss: 371.5334\n",
      "Training Epoch: 36 [25850/36045]\tLoss: 380.3964\n",
      "Training Epoch: 36 [25900/36045]\tLoss: 361.4535\n",
      "Training Epoch: 36 [25950/36045]\tLoss: 370.3225\n",
      "Training Epoch: 36 [26000/36045]\tLoss: 454.6180\n",
      "Training Epoch: 36 [26050/36045]\tLoss: 619.0767\n",
      "Training Epoch: 36 [26100/36045]\tLoss: 646.1611\n",
      "Training Epoch: 36 [26150/36045]\tLoss: 646.1880\n",
      "Training Epoch: 36 [26200/36045]\tLoss: 619.1860\n",
      "Training Epoch: 36 [26250/36045]\tLoss: 648.9594\n",
      "Training Epoch: 36 [26300/36045]\tLoss: 591.2606\n",
      "Training Epoch: 36 [26350/36045]\tLoss: 602.3480\n",
      "Training Epoch: 36 [26400/36045]\tLoss: 578.9078\n",
      "Training Epoch: 36 [26450/36045]\tLoss: 508.7192\n",
      "Training Epoch: 36 [26500/36045]\tLoss: 602.9953\n",
      "Training Epoch: 36 [26550/36045]\tLoss: 603.2224\n",
      "Training Epoch: 36 [26600/36045]\tLoss: 599.7589\n",
      "Training Epoch: 36 [26650/36045]\tLoss: 615.5898\n",
      "Training Epoch: 36 [26700/36045]\tLoss: 594.9142\n",
      "Training Epoch: 36 [26750/36045]\tLoss: 557.2931\n",
      "Training Epoch: 36 [26800/36045]\tLoss: 410.9518\n",
      "Training Epoch: 36 [26850/36045]\tLoss: 340.6558\n",
      "Training Epoch: 36 [26900/36045]\tLoss: 343.2677\n",
      "Training Epoch: 36 [26950/36045]\tLoss: 377.1566\n",
      "Training Epoch: 36 [27000/36045]\tLoss: 617.7585\n",
      "Training Epoch: 36 [27050/36045]\tLoss: 645.0200\n",
      "Training Epoch: 36 [27100/36045]\tLoss: 625.0816\n",
      "Training Epoch: 36 [27150/36045]\tLoss: 665.0064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 36 [27200/36045]\tLoss: 484.4285\n",
      "Training Epoch: 36 [27250/36045]\tLoss: 475.6905\n",
      "Training Epoch: 36 [27300/36045]\tLoss: 463.5641\n",
      "Training Epoch: 36 [27350/36045]\tLoss: 461.6191\n",
      "Training Epoch: 36 [27400/36045]\tLoss: 460.8829\n",
      "Training Epoch: 36 [27450/36045]\tLoss: 582.6042\n",
      "Training Epoch: 36 [27500/36045]\tLoss: 624.2746\n",
      "Training Epoch: 36 [27550/36045]\tLoss: 617.5170\n",
      "Training Epoch: 36 [27600/36045]\tLoss: 629.1355\n",
      "Training Epoch: 36 [27650/36045]\tLoss: 621.0441\n",
      "Training Epoch: 36 [27700/36045]\tLoss: 649.3375\n",
      "Training Epoch: 36 [27750/36045]\tLoss: 661.3550\n",
      "Training Epoch: 36 [27800/36045]\tLoss: 648.2995\n",
      "Training Epoch: 36 [27850/36045]\tLoss: 638.3098\n",
      "Training Epoch: 36 [27900/36045]\tLoss: 578.5411\n",
      "Training Epoch: 36 [27950/36045]\tLoss: 482.6304\n",
      "Training Epoch: 36 [28000/36045]\tLoss: 459.2876\n",
      "Training Epoch: 36 [28050/36045]\tLoss: 469.1019\n",
      "Training Epoch: 36 [28100/36045]\tLoss: 460.7254\n",
      "Training Epoch: 36 [28150/36045]\tLoss: 482.0175\n",
      "Training Epoch: 36 [28200/36045]\tLoss: 489.7924\n",
      "Training Epoch: 36 [28250/36045]\tLoss: 482.9508\n",
      "Training Epoch: 36 [28300/36045]\tLoss: 458.4455\n",
      "Training Epoch: 36 [28350/36045]\tLoss: 454.3792\n",
      "Training Epoch: 36 [28400/36045]\tLoss: 783.6445\n",
      "Training Epoch: 36 [28450/36045]\tLoss: 718.2850\n",
      "Training Epoch: 36 [28500/36045]\tLoss: 620.6749\n",
      "Training Epoch: 36 [28550/36045]\tLoss: 570.4067\n",
      "Training Epoch: 36 [28600/36045]\tLoss: 598.5568\n",
      "Training Epoch: 36 [28650/36045]\tLoss: 658.6485\n",
      "Training Epoch: 36 [28700/36045]\tLoss: 652.6527\n",
      "Training Epoch: 36 [28750/36045]\tLoss: 639.2029\n",
      "Training Epoch: 36 [28800/36045]\tLoss: 648.4494\n",
      "Training Epoch: 36 [28850/36045]\tLoss: 562.7880\n",
      "Training Epoch: 36 [28900/36045]\tLoss: 457.3640\n",
      "Training Epoch: 36 [28950/36045]\tLoss: 456.4021\n",
      "Training Epoch: 36 [29000/36045]\tLoss: 452.9933\n",
      "Training Epoch: 36 [29050/36045]\tLoss: 459.8228\n",
      "Training Epoch: 36 [29100/36045]\tLoss: 478.2544\n",
      "Training Epoch: 36 [29150/36045]\tLoss: 467.8289\n",
      "Training Epoch: 36 [29200/36045]\tLoss: 453.6241\n",
      "Training Epoch: 36 [29250/36045]\tLoss: 443.4072\n",
      "Training Epoch: 36 [29300/36045]\tLoss: 501.0417\n",
      "Training Epoch: 36 [29350/36045]\tLoss: 589.6560\n",
      "Training Epoch: 36 [29400/36045]\tLoss: 607.1403\n",
      "Training Epoch: 36 [29450/36045]\tLoss: 624.6104\n",
      "Training Epoch: 36 [29500/36045]\tLoss: 639.4747\n",
      "Training Epoch: 36 [29550/36045]\tLoss: 608.6165\n",
      "Training Epoch: 36 [29600/36045]\tLoss: 512.8882\n",
      "Training Epoch: 36 [29650/36045]\tLoss: 494.7221\n",
      "Training Epoch: 36 [29700/36045]\tLoss: 442.8757\n",
      "Training Epoch: 36 [29750/36045]\tLoss: 441.5045\n",
      "Training Epoch: 36 [29800/36045]\tLoss: 488.6736\n",
      "Training Epoch: 36 [29850/36045]\tLoss: 565.2303\n",
      "Training Epoch: 36 [29900/36045]\tLoss: 561.4102\n",
      "Training Epoch: 36 [29950/36045]\tLoss: 583.1704\n",
      "Training Epoch: 36 [30000/36045]\tLoss: 557.4231\n",
      "Training Epoch: 36 [30050/36045]\tLoss: 564.0520\n",
      "Training Epoch: 36 [30100/36045]\tLoss: 688.2573\n",
      "Training Epoch: 36 [30150/36045]\tLoss: 671.3638\n",
      "Training Epoch: 36 [30200/36045]\tLoss: 633.2999\n",
      "Training Epoch: 36 [30250/36045]\tLoss: 681.7570\n",
      "Training Epoch: 36 [30300/36045]\tLoss: 666.3292\n",
      "Training Epoch: 36 [30350/36045]\tLoss: 512.3771\n",
      "Training Epoch: 36 [30400/36045]\tLoss: 496.9961\n",
      "Training Epoch: 36 [30450/36045]\tLoss: 499.1124\n",
      "Training Epoch: 36 [30500/36045]\tLoss: 465.9824\n",
      "Training Epoch: 36 [30550/36045]\tLoss: 432.0420\n",
      "Training Epoch: 36 [30600/36045]\tLoss: 423.5125\n",
      "Training Epoch: 36 [30650/36045]\tLoss: 413.5306\n",
      "Training Epoch: 36 [30700/36045]\tLoss: 431.3736\n",
      "Training Epoch: 36 [30750/36045]\tLoss: 418.3009\n",
      "Training Epoch: 36 [30800/36045]\tLoss: 444.6631\n",
      "Training Epoch: 36 [30850/36045]\tLoss: 436.3368\n",
      "Training Epoch: 36 [30900/36045]\tLoss: 448.5995\n",
      "Training Epoch: 36 [30950/36045]\tLoss: 471.3093\n",
      "Training Epoch: 36 [31000/36045]\tLoss: 463.4254\n",
      "Training Epoch: 36 [31050/36045]\tLoss: 387.1574\n",
      "Training Epoch: 36 [31100/36045]\tLoss: 377.7341\n",
      "Training Epoch: 36 [31150/36045]\tLoss: 385.2139\n",
      "Training Epoch: 36 [31200/36045]\tLoss: 478.8509\n",
      "Training Epoch: 36 [31250/36045]\tLoss: 621.3116\n",
      "Training Epoch: 36 [31300/36045]\tLoss: 592.1866\n",
      "Training Epoch: 36 [31350/36045]\tLoss: 608.0747\n",
      "Training Epoch: 36 [31400/36045]\tLoss: 587.5422\n",
      "Training Epoch: 36 [31450/36045]\tLoss: 604.0101\n",
      "Training Epoch: 36 [31500/36045]\tLoss: 616.5161\n",
      "Training Epoch: 36 [31550/36045]\tLoss: 623.8287\n",
      "Training Epoch: 36 [31600/36045]\tLoss: 586.4509\n",
      "Training Epoch: 36 [31650/36045]\tLoss: 627.4565\n",
      "Training Epoch: 36 [31700/36045]\tLoss: 454.0817\n",
      "Training Epoch: 36 [31750/36045]\tLoss: 375.4377\n",
      "Training Epoch: 36 [31800/36045]\tLoss: 358.3098\n",
      "Training Epoch: 36 [31850/36045]\tLoss: 366.5759\n",
      "Training Epoch: 36 [31900/36045]\tLoss: 577.6184\n",
      "Training Epoch: 36 [31950/36045]\tLoss: 747.1835\n",
      "Training Epoch: 36 [32000/36045]\tLoss: 856.0056\n",
      "Training Epoch: 36 [32050/36045]\tLoss: 811.2770\n",
      "Training Epoch: 36 [32100/36045]\tLoss: 801.8934\n",
      "Training Epoch: 36 [32150/36045]\tLoss: 618.7961\n",
      "Training Epoch: 36 [32200/36045]\tLoss: 621.6146\n",
      "Training Epoch: 36 [32250/36045]\tLoss: 632.0457\n",
      "Training Epoch: 36 [32300/36045]\tLoss: 613.9066\n",
      "Training Epoch: 36 [32350/36045]\tLoss: 609.6286\n",
      "Training Epoch: 36 [32400/36045]\tLoss: 572.0201\n",
      "Training Epoch: 36 [32450/36045]\tLoss: 471.0341\n",
      "Training Epoch: 36 [32500/36045]\tLoss: 452.6561\n",
      "Training Epoch: 36 [32550/36045]\tLoss: 454.8850\n",
      "Training Epoch: 36 [32600/36045]\tLoss: 451.8233\n",
      "Training Epoch: 36 [32650/36045]\tLoss: 583.1348\n",
      "Training Epoch: 36 [32700/36045]\tLoss: 636.5807\n",
      "Training Epoch: 36 [32750/36045]\tLoss: 606.4410\n",
      "Training Epoch: 36 [32800/36045]\tLoss: 621.9150\n",
      "Training Epoch: 36 [32850/36045]\tLoss: 574.1465\n",
      "Training Epoch: 36 [32900/36045]\tLoss: 459.8510\n",
      "Training Epoch: 36 [32950/36045]\tLoss: 481.5645\n",
      "Training Epoch: 36 [33000/36045]\tLoss: 480.6373\n",
      "Training Epoch: 36 [33050/36045]\tLoss: 456.8242\n",
      "Training Epoch: 36 [33100/36045]\tLoss: 519.1315\n",
      "Training Epoch: 36 [33150/36045]\tLoss: 705.5994\n",
      "Training Epoch: 36 [33200/36045]\tLoss: 687.0936\n",
      "Training Epoch: 36 [33250/36045]\tLoss: 708.0842\n",
      "Training Epoch: 36 [33300/36045]\tLoss: 754.3049\n",
      "Training Epoch: 36 [33350/36045]\tLoss: 578.0753\n",
      "Training Epoch: 36 [33400/36045]\tLoss: 422.4171\n",
      "Training Epoch: 36 [33450/36045]\tLoss: 417.9201\n",
      "Training Epoch: 36 [33500/36045]\tLoss: 430.2383\n",
      "Training Epoch: 36 [33550/36045]\tLoss: 445.8860\n",
      "Training Epoch: 36 [33600/36045]\tLoss: 447.7102\n",
      "Training Epoch: 36 [33650/36045]\tLoss: 597.6552\n",
      "Training Epoch: 36 [33700/36045]\tLoss: 578.1082\n",
      "Training Epoch: 36 [33750/36045]\tLoss: 598.6105\n",
      "Training Epoch: 36 [33800/36045]\tLoss: 595.1843\n",
      "Training Epoch: 36 [33850/36045]\tLoss: 597.3372\n",
      "Training Epoch: 36 [33900/36045]\tLoss: 610.5508\n",
      "Training Epoch: 36 [33950/36045]\tLoss: 620.7095\n",
      "Training Epoch: 36 [34000/36045]\tLoss: 607.1927\n",
      "Training Epoch: 36 [34050/36045]\tLoss: 611.1646\n",
      "Training Epoch: 36 [34100/36045]\tLoss: 588.8525\n",
      "Training Epoch: 36 [34150/36045]\tLoss: 546.2258\n",
      "Training Epoch: 36 [34200/36045]\tLoss: 517.3506\n",
      "Training Epoch: 36 [34250/36045]\tLoss: 531.6168\n",
      "Training Epoch: 36 [34300/36045]\tLoss: 454.0981\n",
      "Training Epoch: 36 [34350/36045]\tLoss: 478.2770\n",
      "Training Epoch: 36 [34400/36045]\tLoss: 470.5271\n",
      "Training Epoch: 36 [34450/36045]\tLoss: 442.7330\n",
      "Training Epoch: 36 [34500/36045]\tLoss: 472.4033\n",
      "Training Epoch: 36 [34550/36045]\tLoss: 463.6718\n",
      "Training Epoch: 36 [34600/36045]\tLoss: 469.2865\n",
      "Training Epoch: 36 [34650/36045]\tLoss: 575.7636\n",
      "Training Epoch: 36 [34700/36045]\tLoss: 610.2794\n",
      "Training Epoch: 36 [34750/36045]\tLoss: 541.0328\n",
      "Training Epoch: 36 [34800/36045]\tLoss: 620.5474\n",
      "Training Epoch: 36 [34850/36045]\tLoss: 628.1163\n",
      "Training Epoch: 36 [34900/36045]\tLoss: 682.3082\n",
      "Training Epoch: 36 [34950/36045]\tLoss: 667.6599\n",
      "Training Epoch: 36 [35000/36045]\tLoss: 668.5330\n",
      "Training Epoch: 36 [35050/36045]\tLoss: 655.2670\n",
      "Training Epoch: 36 [35100/36045]\tLoss: 561.2745\n",
      "Training Epoch: 36 [35150/36045]\tLoss: 553.9886\n",
      "Training Epoch: 36 [35200/36045]\tLoss: 467.1817\n",
      "Training Epoch: 36 [35250/36045]\tLoss: 513.0027\n",
      "Training Epoch: 36 [35300/36045]\tLoss: 528.1849\n",
      "Training Epoch: 36 [35350/36045]\tLoss: 593.8336\n",
      "Training Epoch: 36 [35400/36045]\tLoss: 625.9590\n",
      "Training Epoch: 36 [35450/36045]\tLoss: 596.8616\n",
      "Training Epoch: 36 [35500/36045]\tLoss: 577.9044\n",
      "Training Epoch: 36 [35550/36045]\tLoss: 563.5635\n",
      "Training Epoch: 36 [35600/36045]\tLoss: 612.9615\n",
      "Training Epoch: 36 [35650/36045]\tLoss: 686.0977\n",
      "Training Epoch: 36 [35700/36045]\tLoss: 613.4272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 36 [35750/36045]\tLoss: 670.8360\n",
      "Training Epoch: 36 [35800/36045]\tLoss: 677.2554\n",
      "Training Epoch: 36 [35850/36045]\tLoss: 651.2380\n",
      "Training Epoch: 36 [35900/36045]\tLoss: 675.1400\n",
      "Training Epoch: 36 [35950/36045]\tLoss: 672.4886\n",
      "Training Epoch: 36 [36000/36045]\tLoss: 665.1708\n",
      "Training Epoch: 36 [36045/36045]\tLoss: 649.6437\n",
      "Training Epoch: 36 [4004/4004]\tLoss: 603.6062\n",
      "Training Epoch: 37 [50/36045]\tLoss: 602.5256\n",
      "Training Epoch: 37 [100/36045]\tLoss: 577.8083\n",
      "Training Epoch: 37 [150/36045]\tLoss: 576.1379\n",
      "Training Epoch: 37 [200/36045]\tLoss: 562.5909\n",
      "Training Epoch: 37 [250/36045]\tLoss: 674.0831\n",
      "Training Epoch: 37 [300/36045]\tLoss: 738.5114\n",
      "Training Epoch: 37 [350/36045]\tLoss: 704.8670\n",
      "Training Epoch: 37 [400/36045]\tLoss: 699.9376\n",
      "Training Epoch: 37 [450/36045]\tLoss: 680.5901\n",
      "Training Epoch: 37 [500/36045]\tLoss: 631.3252\n",
      "Training Epoch: 37 [550/36045]\tLoss: 634.2866\n",
      "Training Epoch: 37 [600/36045]\tLoss: 618.3055\n",
      "Training Epoch: 37 [650/36045]\tLoss: 640.4869\n",
      "Training Epoch: 37 [700/36045]\tLoss: 626.6644\n",
      "Training Epoch: 37 [750/36045]\tLoss: 605.1895\n",
      "Training Epoch: 37 [800/36045]\tLoss: 617.5491\n",
      "Training Epoch: 37 [850/36045]\tLoss: 599.4143\n",
      "Training Epoch: 37 [900/36045]\tLoss: 572.9986\n",
      "Training Epoch: 37 [950/36045]\tLoss: 542.6229\n",
      "Training Epoch: 37 [1000/36045]\tLoss: 525.3763\n",
      "Training Epoch: 37 [1050/36045]\tLoss: 527.4411\n",
      "Training Epoch: 37 [1100/36045]\tLoss: 513.0106\n",
      "Training Epoch: 37 [1150/36045]\tLoss: 521.7940\n",
      "Training Epoch: 37 [1200/36045]\tLoss: 551.8010\n",
      "Training Epoch: 37 [1250/36045]\tLoss: 631.8287\n",
      "Training Epoch: 37 [1300/36045]\tLoss: 638.9118\n",
      "Training Epoch: 37 [1350/36045]\tLoss: 640.5357\n",
      "Training Epoch: 37 [1400/36045]\tLoss: 665.2354\n",
      "Training Epoch: 37 [1450/36045]\tLoss: 643.3251\n",
      "Training Epoch: 37 [1500/36045]\tLoss: 588.6746\n",
      "Training Epoch: 37 [1550/36045]\tLoss: 603.9130\n",
      "Training Epoch: 37 [1600/36045]\tLoss: 614.5575\n",
      "Training Epoch: 37 [1650/36045]\tLoss: 601.7427\n",
      "Training Epoch: 37 [1700/36045]\tLoss: 613.8906\n",
      "Training Epoch: 37 [1750/36045]\tLoss: 655.1614\n",
      "Training Epoch: 37 [1800/36045]\tLoss: 636.9832\n",
      "Training Epoch: 37 [1850/36045]\tLoss: 653.4050\n",
      "Training Epoch: 37 [1900/36045]\tLoss: 611.6439\n",
      "Training Epoch: 37 [1950/36045]\tLoss: 622.1714\n",
      "Training Epoch: 37 [2000/36045]\tLoss: 561.4844\n",
      "Training Epoch: 37 [2050/36045]\tLoss: 563.6735\n",
      "Training Epoch: 37 [2100/36045]\tLoss: 593.7610\n",
      "Training Epoch: 37 [2150/36045]\tLoss: 574.2031\n",
      "Training Epoch: 37 [2200/36045]\tLoss: 534.8198\n",
      "Training Epoch: 37 [2250/36045]\tLoss: 505.0361\n",
      "Training Epoch: 37 [2300/36045]\tLoss: 529.4885\n",
      "Training Epoch: 37 [2350/36045]\tLoss: 505.8764\n",
      "Training Epoch: 37 [2400/36045]\tLoss: 513.8074\n",
      "Training Epoch: 37 [2450/36045]\tLoss: 656.9669\n",
      "Training Epoch: 37 [2500/36045]\tLoss: 690.3439\n",
      "Training Epoch: 37 [2550/36045]\tLoss: 688.0184\n",
      "Training Epoch: 37 [2600/36045]\tLoss: 696.5225\n",
      "Training Epoch: 37 [2650/36045]\tLoss: 821.1619\n",
      "Training Epoch: 37 [2700/36045]\tLoss: 908.5767\n",
      "Training Epoch: 37 [2750/36045]\tLoss: 979.9247\n",
      "Training Epoch: 37 [2800/36045]\tLoss: 989.7327\n",
      "Training Epoch: 37 [2850/36045]\tLoss: 757.9377\n",
      "Training Epoch: 37 [2900/36045]\tLoss: 720.3058\n",
      "Training Epoch: 37 [2950/36045]\tLoss: 695.8015\n",
      "Training Epoch: 37 [3000/36045]\tLoss: 689.8243\n",
      "Training Epoch: 37 [3050/36045]\tLoss: 720.7892\n",
      "Training Epoch: 37 [3100/36045]\tLoss: 659.4672\n",
      "Training Epoch: 37 [3150/36045]\tLoss: 508.3464\n",
      "Training Epoch: 37 [3200/36045]\tLoss: 526.4255\n",
      "Training Epoch: 37 [3250/36045]\tLoss: 495.8904\n",
      "Training Epoch: 37 [3300/36045]\tLoss: 469.2383\n",
      "Training Epoch: 37 [3350/36045]\tLoss: 495.5659\n",
      "Training Epoch: 37 [3400/36045]\tLoss: 519.8388\n",
      "Training Epoch: 37 [3450/36045]\tLoss: 558.1877\n",
      "Training Epoch: 37 [3500/36045]\tLoss: 545.3387\n",
      "Training Epoch: 37 [3550/36045]\tLoss: 521.7581\n",
      "Training Epoch: 37 [3600/36045]\tLoss: 559.8281\n",
      "Training Epoch: 37 [3650/36045]\tLoss: 647.3785\n",
      "Training Epoch: 37 [3700/36045]\tLoss: 655.0374\n",
      "Training Epoch: 37 [3750/36045]\tLoss: 624.2313\n",
      "Training Epoch: 37 [3800/36045]\tLoss: 620.2208\n",
      "Training Epoch: 37 [3850/36045]\tLoss: 620.3803\n",
      "Training Epoch: 37 [3900/36045]\tLoss: 625.2386\n",
      "Training Epoch: 37 [3950/36045]\tLoss: 603.3912\n",
      "Training Epoch: 37 [4000/36045]\tLoss: 608.6155\n",
      "Training Epoch: 37 [4050/36045]\tLoss: 558.6697\n",
      "Training Epoch: 37 [4100/36045]\tLoss: 544.6640\n",
      "Training Epoch: 37 [4150/36045]\tLoss: 559.5166\n",
      "Training Epoch: 37 [4200/36045]\tLoss: 554.3555\n",
      "Training Epoch: 37 [4250/36045]\tLoss: 556.7954\n",
      "Training Epoch: 37 [4300/36045]\tLoss: 573.7527\n",
      "Training Epoch: 37 [4350/36045]\tLoss: 556.8982\n",
      "Training Epoch: 37 [4400/36045]\tLoss: 532.6646\n",
      "Training Epoch: 37 [4450/36045]\tLoss: 583.9653\n",
      "Training Epoch: 37 [4500/36045]\tLoss: 627.1823\n",
      "Training Epoch: 37 [4550/36045]\tLoss: 630.9848\n",
      "Training Epoch: 37 [4600/36045]\tLoss: 653.4100\n",
      "Training Epoch: 37 [4650/36045]\tLoss: 642.9057\n",
      "Training Epoch: 37 [4700/36045]\tLoss: 592.9736\n",
      "Training Epoch: 37 [4750/36045]\tLoss: 575.2423\n",
      "Training Epoch: 37 [4800/36045]\tLoss: 600.3025\n",
      "Training Epoch: 37 [4850/36045]\tLoss: 586.9329\n",
      "Training Epoch: 37 [4900/36045]\tLoss: 571.1893\n",
      "Training Epoch: 37 [4950/36045]\tLoss: 586.7543\n",
      "Training Epoch: 37 [5000/36045]\tLoss: 616.4313\n",
      "Training Epoch: 37 [5050/36045]\tLoss: 597.2236\n",
      "Training Epoch: 37 [5100/36045]\tLoss: 607.6736\n",
      "Training Epoch: 37 [5150/36045]\tLoss: 592.0622\n",
      "Training Epoch: 37 [5200/36045]\tLoss: 590.0011\n",
      "Training Epoch: 37 [5250/36045]\tLoss: 583.5347\n",
      "Training Epoch: 37 [5300/36045]\tLoss: 583.8075\n",
      "Training Epoch: 37 [5350/36045]\tLoss: 605.8137\n",
      "Training Epoch: 37 [5400/36045]\tLoss: 583.7366\n",
      "Training Epoch: 37 [5450/36045]\tLoss: 553.4167\n",
      "Training Epoch: 37 [5500/36045]\tLoss: 581.7739\n",
      "Training Epoch: 37 [5550/36045]\tLoss: 569.9536\n",
      "Training Epoch: 37 [5600/36045]\tLoss: 650.3242\n",
      "Training Epoch: 37 [5650/36045]\tLoss: 615.2603\n",
      "Training Epoch: 37 [5700/36045]\tLoss: 577.1977\n",
      "Training Epoch: 37 [5750/36045]\tLoss: 561.4313\n",
      "Training Epoch: 37 [5800/36045]\tLoss: 592.2689\n",
      "Training Epoch: 37 [5850/36045]\tLoss: 580.7560\n",
      "Training Epoch: 37 [5900/36045]\tLoss: 667.6953\n",
      "Training Epoch: 37 [5950/36045]\tLoss: 684.5474\n",
      "Training Epoch: 37 [6000/36045]\tLoss: 670.1930\n",
      "Training Epoch: 37 [6050/36045]\tLoss: 647.9181\n",
      "Training Epoch: 37 [6100/36045]\tLoss: 652.1469\n",
      "Training Epoch: 37 [6150/36045]\tLoss: 641.0606\n",
      "Training Epoch: 37 [6200/36045]\tLoss: 644.5267\n",
      "Training Epoch: 37 [6250/36045]\tLoss: 666.0401\n",
      "Training Epoch: 37 [6300/36045]\tLoss: 677.7227\n",
      "Training Epoch: 37 [6350/36045]\tLoss: 723.8745\n",
      "Training Epoch: 37 [6400/36045]\tLoss: 598.4212\n",
      "Training Epoch: 37 [6450/36045]\tLoss: 551.3240\n",
      "Training Epoch: 37 [6500/36045]\tLoss: 561.3948\n",
      "Training Epoch: 37 [6550/36045]\tLoss: 578.7337\n",
      "Training Epoch: 37 [6600/36045]\tLoss: 577.2031\n",
      "Training Epoch: 37 [6650/36045]\tLoss: 651.3292\n",
      "Training Epoch: 37 [6700/36045]\tLoss: 681.2919\n",
      "Training Epoch: 37 [6750/36045]\tLoss: 658.0768\n",
      "Training Epoch: 37 [6800/36045]\tLoss: 660.9867\n",
      "Training Epoch: 37 [6850/36045]\tLoss: 648.9861\n",
      "Training Epoch: 37 [6900/36045]\tLoss: 578.2054\n",
      "Training Epoch: 37 [6950/36045]\tLoss: 545.1199\n",
      "Training Epoch: 37 [7000/36045]\tLoss: 579.8831\n",
      "Training Epoch: 37 [7050/36045]\tLoss: 592.6022\n",
      "Training Epoch: 37 [7100/36045]\tLoss: 592.0484\n",
      "Training Epoch: 37 [7150/36045]\tLoss: 602.4478\n",
      "Training Epoch: 37 [7200/36045]\tLoss: 605.2290\n",
      "Training Epoch: 37 [7250/36045]\tLoss: 603.2421\n",
      "Training Epoch: 37 [7300/36045]\tLoss: 589.7696\n",
      "Training Epoch: 37 [7350/36045]\tLoss: 586.9716\n",
      "Training Epoch: 37 [7400/36045]\tLoss: 534.1133\n",
      "Training Epoch: 37 [7450/36045]\tLoss: 537.3605\n",
      "Training Epoch: 37 [7500/36045]\tLoss: 532.6633\n",
      "Training Epoch: 37 [7550/36045]\tLoss: 510.2315\n",
      "Training Epoch: 37 [7600/36045]\tLoss: 565.8768\n",
      "Training Epoch: 37 [7650/36045]\tLoss: 605.3551\n",
      "Training Epoch: 37 [7700/36045]\tLoss: 576.2161\n",
      "Training Epoch: 37 [7750/36045]\tLoss: 590.7453\n",
      "Training Epoch: 37 [7800/36045]\tLoss: 579.9270\n",
      "Training Epoch: 37 [7850/36045]\tLoss: 561.3300\n",
      "Training Epoch: 37 [7900/36045]\tLoss: 592.0582\n",
      "Training Epoch: 37 [7950/36045]\tLoss: 589.5110\n",
      "Training Epoch: 37 [8000/36045]\tLoss: 607.6292\n",
      "Training Epoch: 37 [8050/36045]\tLoss: 572.7100\n",
      "Training Epoch: 37 [8100/36045]\tLoss: 597.9795\n",
      "Training Epoch: 37 [8150/36045]\tLoss: 677.5878\n",
      "Training Epoch: 37 [8200/36045]\tLoss: 664.6813\n",
      "Training Epoch: 37 [8250/36045]\tLoss: 632.5539\n",
      "Training Epoch: 37 [8300/36045]\tLoss: 690.6041\n",
      "Training Epoch: 37 [8350/36045]\tLoss: 633.6017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 37 [8400/36045]\tLoss: 567.2993\n",
      "Training Epoch: 37 [8450/36045]\tLoss: 531.0837\n",
      "Training Epoch: 37 [8500/36045]\tLoss: 564.7896\n",
      "Training Epoch: 37 [8550/36045]\tLoss: 557.5953\n",
      "Training Epoch: 37 [8600/36045]\tLoss: 551.7234\n",
      "Training Epoch: 37 [8650/36045]\tLoss: 586.9296\n",
      "Training Epoch: 37 [8700/36045]\tLoss: 620.7895\n",
      "Training Epoch: 37 [8750/36045]\tLoss: 609.7461\n",
      "Training Epoch: 37 [8800/36045]\tLoss: 615.5253\n",
      "Training Epoch: 37 [8850/36045]\tLoss: 608.9875\n",
      "Training Epoch: 37 [8900/36045]\tLoss: 549.7697\n",
      "Training Epoch: 37 [8950/36045]\tLoss: 561.1607\n",
      "Training Epoch: 37 [9000/36045]\tLoss: 576.8734\n",
      "Training Epoch: 37 [9050/36045]\tLoss: 578.3577\n",
      "Training Epoch: 37 [9100/36045]\tLoss: 595.4356\n",
      "Training Epoch: 37 [9150/36045]\tLoss: 440.5634\n",
      "Training Epoch: 37 [9200/36045]\tLoss: 329.5611\n",
      "Training Epoch: 37 [9250/36045]\tLoss: 357.5183\n",
      "Training Epoch: 37 [9300/36045]\tLoss: 367.6167\n",
      "Training Epoch: 37 [9350/36045]\tLoss: 339.1255\n",
      "Training Epoch: 37 [9400/36045]\tLoss: 664.5339\n",
      "Training Epoch: 37 [9450/36045]\tLoss: 705.8112\n",
      "Training Epoch: 37 [9500/36045]\tLoss: 693.0674\n",
      "Training Epoch: 37 [9550/36045]\tLoss: 733.1976\n",
      "Training Epoch: 37 [9600/36045]\tLoss: 545.5479\n",
      "Training Epoch: 37 [9650/36045]\tLoss: 550.4500\n",
      "Training Epoch: 37 [9700/36045]\tLoss: 535.9220\n",
      "Training Epoch: 37 [9750/36045]\tLoss: 534.6779\n",
      "Training Epoch: 37 [9800/36045]\tLoss: 699.2488\n",
      "Training Epoch: 37 [9850/36045]\tLoss: 738.6309\n",
      "Training Epoch: 37 [9900/36045]\tLoss: 749.3647\n",
      "Training Epoch: 37 [9950/36045]\tLoss: 730.1691\n",
      "Training Epoch: 37 [10000/36045]\tLoss: 675.1458\n",
      "Training Epoch: 37 [10050/36045]\tLoss: 553.8067\n",
      "Training Epoch: 37 [10100/36045]\tLoss: 561.4574\n",
      "Training Epoch: 37 [10150/36045]\tLoss: 570.2590\n",
      "Training Epoch: 37 [10200/36045]\tLoss: 559.1672\n",
      "Training Epoch: 37 [10250/36045]\tLoss: 670.4778\n",
      "Training Epoch: 37 [10300/36045]\tLoss: 651.4042\n",
      "Training Epoch: 37 [10350/36045]\tLoss: 686.1440\n",
      "Training Epoch: 37 [10400/36045]\tLoss: 676.5106\n",
      "Training Epoch: 37 [10450/36045]\tLoss: 633.8329\n",
      "Training Epoch: 37 [10500/36045]\tLoss: 529.5811\n",
      "Training Epoch: 37 [10550/36045]\tLoss: 524.3833\n",
      "Training Epoch: 37 [10600/36045]\tLoss: 546.7296\n",
      "Training Epoch: 37 [10650/36045]\tLoss: 552.8630\n",
      "Training Epoch: 37 [10700/36045]\tLoss: 634.6508\n",
      "Training Epoch: 37 [10750/36045]\tLoss: 694.5192\n",
      "Training Epoch: 37 [10800/36045]\tLoss: 639.6315\n",
      "Training Epoch: 37 [10850/36045]\tLoss: 677.9652\n",
      "Training Epoch: 37 [10900/36045]\tLoss: 705.7207\n",
      "Training Epoch: 37 [10950/36045]\tLoss: 520.1249\n",
      "Training Epoch: 37 [11000/36045]\tLoss: 513.7454\n",
      "Training Epoch: 37 [11050/36045]\tLoss: 550.7337\n",
      "Training Epoch: 37 [11100/36045]\tLoss: 561.2802\n",
      "Training Epoch: 37 [11150/36045]\tLoss: 608.7481\n",
      "Training Epoch: 37 [11200/36045]\tLoss: 636.7935\n",
      "Training Epoch: 37 [11250/36045]\tLoss: 648.4962\n",
      "Training Epoch: 37 [11300/36045]\tLoss: 628.6608\n",
      "Training Epoch: 37 [11350/36045]\tLoss: 626.1920\n",
      "Training Epoch: 37 [11400/36045]\tLoss: 588.9969\n",
      "Training Epoch: 37 [11450/36045]\tLoss: 557.6247\n",
      "Training Epoch: 37 [11500/36045]\tLoss: 555.0000\n",
      "Training Epoch: 37 [11550/36045]\tLoss: 565.3303\n",
      "Training Epoch: 37 [11600/36045]\tLoss: 625.9672\n",
      "Training Epoch: 37 [11650/36045]\tLoss: 677.7300\n",
      "Training Epoch: 37 [11700/36045]\tLoss: 676.6993\n",
      "Training Epoch: 37 [11750/36045]\tLoss: 695.1619\n",
      "Training Epoch: 37 [11800/36045]\tLoss: 737.8152\n",
      "Training Epoch: 37 [11850/36045]\tLoss: 795.8874\n",
      "Training Epoch: 37 [11900/36045]\tLoss: 1009.2497\n",
      "Training Epoch: 37 [11950/36045]\tLoss: 1012.0220\n",
      "Training Epoch: 37 [12000/36045]\tLoss: 1024.1260\n",
      "Training Epoch: 37 [12050/36045]\tLoss: 983.2712\n",
      "Training Epoch: 37 [12100/36045]\tLoss: 633.1017\n",
      "Training Epoch: 37 [12150/36045]\tLoss: 479.2263\n",
      "Training Epoch: 37 [12200/36045]\tLoss: 473.9414\n",
      "Training Epoch: 37 [12250/36045]\tLoss: 482.7184\n",
      "Training Epoch: 37 [12300/36045]\tLoss: 620.2195\n",
      "Training Epoch: 37 [12350/36045]\tLoss: 676.2375\n",
      "Training Epoch: 37 [12400/36045]\tLoss: 683.6917\n",
      "Training Epoch: 37 [12450/36045]\tLoss: 672.3950\n",
      "Training Epoch: 37 [12500/36045]\tLoss: 699.7814\n",
      "Training Epoch: 37 [12550/36045]\tLoss: 669.4046\n",
      "Training Epoch: 37 [12600/36045]\tLoss: 613.7775\n",
      "Training Epoch: 37 [12650/36045]\tLoss: 612.2492\n",
      "Training Epoch: 37 [12700/36045]\tLoss: 633.6326\n",
      "Training Epoch: 37 [12750/36045]\tLoss: 632.3730\n",
      "Training Epoch: 37 [12800/36045]\tLoss: 617.4096\n",
      "Training Epoch: 37 [12850/36045]\tLoss: 646.6036\n",
      "Training Epoch: 37 [12900/36045]\tLoss: 620.0378\n",
      "Training Epoch: 37 [12950/36045]\tLoss: 606.3409\n",
      "Training Epoch: 37 [13000/36045]\tLoss: 639.2676\n",
      "Training Epoch: 37 [13050/36045]\tLoss: 578.8700\n",
      "Training Epoch: 37 [13100/36045]\tLoss: 595.4423\n",
      "Training Epoch: 37 [13150/36045]\tLoss: 587.1188\n",
      "Training Epoch: 37 [13200/36045]\tLoss: 569.3759\n",
      "Training Epoch: 37 [13250/36045]\tLoss: 592.0194\n",
      "Training Epoch: 37 [13300/36045]\tLoss: 629.7555\n",
      "Training Epoch: 37 [13350/36045]\tLoss: 610.2762\n",
      "Training Epoch: 37 [13400/36045]\tLoss: 613.5495\n",
      "Training Epoch: 37 [13450/36045]\tLoss: 610.6530\n",
      "Training Epoch: 37 [13500/36045]\tLoss: 630.0689\n",
      "Training Epoch: 37 [13550/36045]\tLoss: 767.4042\n",
      "Training Epoch: 37 [13600/36045]\tLoss: 800.1515\n",
      "Training Epoch: 37 [13650/36045]\tLoss: 881.3129\n",
      "Training Epoch: 37 [13700/36045]\tLoss: 777.3374\n",
      "Training Epoch: 37 [13750/36045]\tLoss: 616.9235\n",
      "Training Epoch: 37 [13800/36045]\tLoss: 588.4651\n",
      "Training Epoch: 37 [13850/36045]\tLoss: 571.1298\n",
      "Training Epoch: 37 [13900/36045]\tLoss: 578.3657\n",
      "Training Epoch: 37 [13950/36045]\tLoss: 623.6389\n",
      "Training Epoch: 37 [14000/36045]\tLoss: 656.8462\n",
      "Training Epoch: 37 [14050/36045]\tLoss: 631.6562\n",
      "Training Epoch: 37 [14100/36045]\tLoss: 626.8873\n",
      "Training Epoch: 37 [14150/36045]\tLoss: 614.8295\n",
      "Training Epoch: 37 [14200/36045]\tLoss: 655.7843\n",
      "Training Epoch: 37 [14250/36045]\tLoss: 720.3058\n",
      "Training Epoch: 37 [14300/36045]\tLoss: 723.6863\n",
      "Training Epoch: 37 [14350/36045]\tLoss: 691.9822\n",
      "Training Epoch: 37 [14400/36045]\tLoss: 677.8075\n",
      "Training Epoch: 37 [14450/36045]\tLoss: 713.6258\n",
      "Training Epoch: 37 [14500/36045]\tLoss: 644.6986\n",
      "Training Epoch: 37 [14550/36045]\tLoss: 673.6675\n",
      "Training Epoch: 37 [14600/36045]\tLoss: 660.0100\n",
      "Training Epoch: 37 [14650/36045]\tLoss: 659.6806\n",
      "Training Epoch: 37 [14700/36045]\tLoss: 624.8350\n",
      "Training Epoch: 37 [14750/36045]\tLoss: 537.2607\n",
      "Training Epoch: 37 [14800/36045]\tLoss: 527.4214\n",
      "Training Epoch: 37 [14850/36045]\tLoss: 534.4529\n",
      "Training Epoch: 37 [14900/36045]\tLoss: 527.9764\n",
      "Training Epoch: 37 [14950/36045]\tLoss: 535.9123\n",
      "Training Epoch: 37 [15000/36045]\tLoss: 549.1122\n",
      "Training Epoch: 37 [15050/36045]\tLoss: 546.1876\n",
      "Training Epoch: 37 [15100/36045]\tLoss: 530.2675\n",
      "Training Epoch: 37 [15150/36045]\tLoss: 525.3929\n",
      "Training Epoch: 37 [15200/36045]\tLoss: 486.5814\n",
      "Training Epoch: 37 [15250/36045]\tLoss: 508.7011\n",
      "Training Epoch: 37 [15300/36045]\tLoss: 494.0513\n",
      "Training Epoch: 37 [15350/36045]\tLoss: 505.7582\n",
      "Training Epoch: 37 [15400/36045]\tLoss: 489.0252\n",
      "Training Epoch: 37 [15450/36045]\tLoss: 474.2846\n",
      "Training Epoch: 37 [15500/36045]\tLoss: 487.8927\n",
      "Training Epoch: 37 [15550/36045]\tLoss: 484.1136\n",
      "Training Epoch: 37 [15600/36045]\tLoss: 552.1929\n",
      "Training Epoch: 37 [15650/36045]\tLoss: 569.5043\n",
      "Training Epoch: 37 [15700/36045]\tLoss: 561.4277\n",
      "Training Epoch: 37 [15750/36045]\tLoss: 552.8691\n",
      "Training Epoch: 37 [15800/36045]\tLoss: 527.0858\n",
      "Training Epoch: 37 [15850/36045]\tLoss: 541.9022\n",
      "Training Epoch: 37 [15900/36045]\tLoss: 551.0923\n",
      "Training Epoch: 37 [15950/36045]\tLoss: 570.9074\n",
      "Training Epoch: 37 [16000/36045]\tLoss: 542.6583\n",
      "Training Epoch: 37 [16050/36045]\tLoss: 512.0798\n",
      "Training Epoch: 37 [16100/36045]\tLoss: 474.9935\n",
      "Training Epoch: 37 [16150/36045]\tLoss: 462.9293\n",
      "Training Epoch: 37 [16200/36045]\tLoss: 561.6810\n",
      "Training Epoch: 37 [16250/36045]\tLoss: 589.6276\n",
      "Training Epoch: 37 [16300/36045]\tLoss: 643.7115\n",
      "Training Epoch: 37 [16350/36045]\tLoss: 663.5270\n",
      "Training Epoch: 37 [16400/36045]\tLoss: 635.2609\n",
      "Training Epoch: 37 [16450/36045]\tLoss: 617.3776\n",
      "Training Epoch: 37 [16500/36045]\tLoss: 617.2999\n",
      "Training Epoch: 37 [16550/36045]\tLoss: 582.3755\n",
      "Training Epoch: 37 [16600/36045]\tLoss: 605.1628\n",
      "Training Epoch: 37 [16650/36045]\tLoss: 621.8470\n",
      "Training Epoch: 37 [16700/36045]\tLoss: 601.2418\n",
      "Training Epoch: 37 [16750/36045]\tLoss: 593.8706\n",
      "Training Epoch: 37 [16800/36045]\tLoss: 603.2556\n",
      "Training Epoch: 37 [16850/36045]\tLoss: 574.7621\n",
      "Training Epoch: 37 [16900/36045]\tLoss: 584.6204\n",
      "Training Epoch: 37 [16950/36045]\tLoss: 607.8292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 37 [17000/36045]\tLoss: 591.7572\n",
      "Training Epoch: 37 [17050/36045]\tLoss: 616.6133\n",
      "Training Epoch: 37 [17100/36045]\tLoss: 612.7200\n",
      "Training Epoch: 37 [17150/36045]\tLoss: 532.2860\n",
      "Training Epoch: 37 [17200/36045]\tLoss: 494.2486\n",
      "Training Epoch: 37 [17250/36045]\tLoss: 517.4644\n",
      "Training Epoch: 37 [17300/36045]\tLoss: 547.5979\n",
      "Training Epoch: 37 [17350/36045]\tLoss: 527.5081\n",
      "Training Epoch: 37 [17400/36045]\tLoss: 546.9622\n",
      "Training Epoch: 37 [17450/36045]\tLoss: 565.8552\n",
      "Training Epoch: 37 [17500/36045]\tLoss: 554.1110\n",
      "Training Epoch: 37 [17550/36045]\tLoss: 552.8581\n",
      "Training Epoch: 37 [17600/36045]\tLoss: 547.0045\n",
      "Training Epoch: 37 [17650/36045]\tLoss: 562.9349\n",
      "Training Epoch: 37 [17700/36045]\tLoss: 542.1413\n",
      "Training Epoch: 37 [17750/36045]\tLoss: 558.4220\n",
      "Training Epoch: 37 [17800/36045]\tLoss: 549.3704\n",
      "Training Epoch: 37 [17850/36045]\tLoss: 563.6843\n",
      "Training Epoch: 37 [17900/36045]\tLoss: 591.9197\n",
      "Training Epoch: 37 [17950/36045]\tLoss: 604.0156\n",
      "Training Epoch: 37 [18000/36045]\tLoss: 594.4935\n",
      "Training Epoch: 37 [18050/36045]\tLoss: 654.2034\n",
      "Training Epoch: 37 [18100/36045]\tLoss: 655.7104\n",
      "Training Epoch: 37 [18150/36045]\tLoss: 667.0545\n",
      "Training Epoch: 37 [18200/36045]\tLoss: 649.1178\n",
      "Training Epoch: 37 [18250/36045]\tLoss: 669.9647\n",
      "Training Epoch: 37 [18300/36045]\tLoss: 623.7718\n",
      "Training Epoch: 37 [18350/36045]\tLoss: 696.0767\n",
      "Training Epoch: 37 [18400/36045]\tLoss: 669.2234\n",
      "Training Epoch: 37 [18450/36045]\tLoss: 648.9841\n",
      "Training Epoch: 37 [18500/36045]\tLoss: 647.9496\n",
      "Training Epoch: 37 [18550/36045]\tLoss: 635.2491\n",
      "Training Epoch: 37 [18600/36045]\tLoss: 624.7665\n",
      "Training Epoch: 37 [18650/36045]\tLoss: 670.7966\n",
      "Training Epoch: 37 [18700/36045]\tLoss: 705.6529\n",
      "Training Epoch: 37 [18750/36045]\tLoss: 692.9275\n",
      "Training Epoch: 37 [18800/36045]\tLoss: 716.0093\n",
      "Training Epoch: 37 [18850/36045]\tLoss: 660.8375\n",
      "Training Epoch: 37 [18900/36045]\tLoss: 706.9028\n",
      "Training Epoch: 37 [18950/36045]\tLoss: 649.1198\n",
      "Training Epoch: 37 [19000/36045]\tLoss: 536.7285\n",
      "Training Epoch: 37 [19050/36045]\tLoss: 520.7936\n",
      "Training Epoch: 37 [19100/36045]\tLoss: 529.0178\n",
      "Training Epoch: 37 [19150/36045]\tLoss: 519.0670\n",
      "Training Epoch: 37 [19200/36045]\tLoss: 549.1802\n",
      "Training Epoch: 37 [19250/36045]\tLoss: 564.1722\n",
      "Training Epoch: 37 [19300/36045]\tLoss: 573.8301\n",
      "Training Epoch: 37 [19350/36045]\tLoss: 557.4565\n",
      "Training Epoch: 37 [19400/36045]\tLoss: 578.2933\n",
      "Training Epoch: 37 [19450/36045]\tLoss: 569.4858\n",
      "Training Epoch: 37 [19500/36045]\tLoss: 570.9796\n",
      "Training Epoch: 37 [19550/36045]\tLoss: 569.3323\n",
      "Training Epoch: 37 [19600/36045]\tLoss: 610.1849\n",
      "Training Epoch: 37 [19650/36045]\tLoss: 812.5242\n",
      "Training Epoch: 37 [19700/36045]\tLoss: 771.2252\n",
      "Training Epoch: 37 [19750/36045]\tLoss: 775.0622\n",
      "Training Epoch: 37 [19800/36045]\tLoss: 775.1674\n",
      "Training Epoch: 37 [19850/36045]\tLoss: 509.7580\n",
      "Training Epoch: 37 [19900/36045]\tLoss: 488.5127\n",
      "Training Epoch: 37 [19950/36045]\tLoss: 492.1637\n",
      "Training Epoch: 37 [20000/36045]\tLoss: 491.6132\n",
      "Training Epoch: 37 [20050/36045]\tLoss: 550.4451\n",
      "Training Epoch: 37 [20100/36045]\tLoss: 556.3139\n",
      "Training Epoch: 37 [20150/36045]\tLoss: 557.7783\n",
      "Training Epoch: 37 [20200/36045]\tLoss: 557.4972\n",
      "Training Epoch: 37 [20250/36045]\tLoss: 594.1363\n",
      "Training Epoch: 37 [20300/36045]\tLoss: 630.1170\n",
      "Training Epoch: 37 [20350/36045]\tLoss: 648.6794\n",
      "Training Epoch: 37 [20400/36045]\tLoss: 665.3093\n",
      "Training Epoch: 37 [20450/36045]\tLoss: 635.4810\n",
      "Training Epoch: 37 [20500/36045]\tLoss: 619.7766\n",
      "Training Epoch: 37 [20550/36045]\tLoss: 544.8698\n",
      "Training Epoch: 37 [20600/36045]\tLoss: 555.0430\n",
      "Training Epoch: 37 [20650/36045]\tLoss: 552.3420\n",
      "Training Epoch: 37 [20700/36045]\tLoss: 540.3883\n",
      "Training Epoch: 37 [20750/36045]\tLoss: 582.3894\n",
      "Training Epoch: 37 [20800/36045]\tLoss: 632.7275\n",
      "Training Epoch: 37 [20850/36045]\tLoss: 619.4507\n",
      "Training Epoch: 37 [20900/36045]\tLoss: 663.2975\n",
      "Training Epoch: 37 [20950/36045]\tLoss: 625.3337\n",
      "Training Epoch: 37 [21000/36045]\tLoss: 588.6489\n",
      "Training Epoch: 37 [21050/36045]\tLoss: 503.6404\n",
      "Training Epoch: 37 [21100/36045]\tLoss: 508.0670\n",
      "Training Epoch: 37 [21150/36045]\tLoss: 543.8160\n",
      "Training Epoch: 37 [21200/36045]\tLoss: 543.0251\n",
      "Training Epoch: 37 [21250/36045]\tLoss: 519.4818\n",
      "Training Epoch: 37 [21300/36045]\tLoss: 606.6138\n",
      "Training Epoch: 37 [21350/36045]\tLoss: 598.5122\n",
      "Training Epoch: 37 [21400/36045]\tLoss: 602.0880\n",
      "Training Epoch: 37 [21450/36045]\tLoss: 608.2729\n",
      "Training Epoch: 37 [21500/36045]\tLoss: 610.3843\n",
      "Training Epoch: 37 [21550/36045]\tLoss: 704.9578\n",
      "Training Epoch: 37 [21600/36045]\tLoss: 703.6720\n",
      "Training Epoch: 37 [21650/36045]\tLoss: 716.0638\n",
      "Training Epoch: 37 [21700/36045]\tLoss: 718.9060\n",
      "Training Epoch: 37 [21750/36045]\tLoss: 690.3476\n",
      "Training Epoch: 37 [21800/36045]\tLoss: 508.2115\n",
      "Training Epoch: 37 [21850/36045]\tLoss: 491.2414\n",
      "Training Epoch: 37 [21900/36045]\tLoss: 500.7494\n",
      "Training Epoch: 37 [21950/36045]\tLoss: 501.4463\n",
      "Training Epoch: 37 [22000/36045]\tLoss: 504.7784\n",
      "Training Epoch: 37 [22050/36045]\tLoss: 525.7145\n",
      "Training Epoch: 37 [22100/36045]\tLoss: 518.3326\n",
      "Training Epoch: 37 [22150/36045]\tLoss: 504.2558\n",
      "Training Epoch: 37 [22200/36045]\tLoss: 520.3450\n",
      "Training Epoch: 37 [22250/36045]\tLoss: 525.3089\n",
      "Training Epoch: 37 [22300/36045]\tLoss: 577.4120\n",
      "Training Epoch: 37 [22350/36045]\tLoss: 603.2310\n",
      "Training Epoch: 37 [22400/36045]\tLoss: 617.5895\n",
      "Training Epoch: 37 [22450/36045]\tLoss: 604.9451\n",
      "Training Epoch: 37 [22500/36045]\tLoss: 587.5059\n",
      "Training Epoch: 37 [22550/36045]\tLoss: 622.8537\n",
      "Training Epoch: 37 [22600/36045]\tLoss: 673.8696\n",
      "Training Epoch: 37 [22650/36045]\tLoss: 707.9090\n",
      "Training Epoch: 37 [22700/36045]\tLoss: 729.8109\n",
      "Training Epoch: 37 [22750/36045]\tLoss: 749.1678\n",
      "Training Epoch: 37 [22800/36045]\tLoss: 778.7540\n",
      "Training Epoch: 37 [22850/36045]\tLoss: 647.3880\n",
      "Training Epoch: 37 [22900/36045]\tLoss: 652.2991\n",
      "Training Epoch: 37 [22950/36045]\tLoss: 631.8196\n",
      "Training Epoch: 37 [23000/36045]\tLoss: 628.6055\n",
      "Training Epoch: 37 [23050/36045]\tLoss: 558.3604\n",
      "Training Epoch: 37 [23100/36045]\tLoss: 574.0743\n",
      "Training Epoch: 37 [23150/36045]\tLoss: 562.4100\n",
      "Training Epoch: 37 [23200/36045]\tLoss: 532.5616\n",
      "Training Epoch: 37 [23250/36045]\tLoss: 535.5934\n",
      "Training Epoch: 37 [23300/36045]\tLoss: 531.7275\n",
      "Training Epoch: 37 [23350/36045]\tLoss: 552.4330\n",
      "Training Epoch: 37 [23400/36045]\tLoss: 598.8770\n",
      "Training Epoch: 37 [23450/36045]\tLoss: 592.3216\n",
      "Training Epoch: 37 [23500/36045]\tLoss: 570.9000\n",
      "Training Epoch: 37 [23550/36045]\tLoss: 611.8801\n",
      "Training Epoch: 37 [23600/36045]\tLoss: 693.1870\n",
      "Training Epoch: 37 [23650/36045]\tLoss: 705.4400\n",
      "Training Epoch: 37 [23700/36045]\tLoss: 713.5223\n",
      "Training Epoch: 37 [23750/36045]\tLoss: 689.5386\n",
      "Training Epoch: 37 [23800/36045]\tLoss: 552.0716\n",
      "Training Epoch: 37 [23850/36045]\tLoss: 578.1061\n",
      "Training Epoch: 37 [23900/36045]\tLoss: 567.7735\n",
      "Training Epoch: 37 [23950/36045]\tLoss: 550.9092\n",
      "Training Epoch: 37 [24000/36045]\tLoss: 527.8452\n",
      "Training Epoch: 37 [24050/36045]\tLoss: 487.5371\n",
      "Training Epoch: 37 [24100/36045]\tLoss: 512.7684\n",
      "Training Epoch: 37 [24150/36045]\tLoss: 505.3633\n",
      "Training Epoch: 37 [24200/36045]\tLoss: 502.3777\n",
      "Training Epoch: 37 [24250/36045]\tLoss: 487.8384\n",
      "Training Epoch: 37 [24300/36045]\tLoss: 527.1404\n",
      "Training Epoch: 37 [24350/36045]\tLoss: 539.8018\n",
      "Training Epoch: 37 [24400/36045]\tLoss: 554.9053\n",
      "Training Epoch: 37 [24450/36045]\tLoss: 528.8297\n",
      "Training Epoch: 37 [24500/36045]\tLoss: 557.9296\n",
      "Training Epoch: 37 [24550/36045]\tLoss: 645.9847\n",
      "Training Epoch: 37 [24600/36045]\tLoss: 637.6588\n",
      "Training Epoch: 37 [24650/36045]\tLoss: 610.6983\n",
      "Training Epoch: 37 [24700/36045]\tLoss: 620.5127\n",
      "Training Epoch: 37 [24750/36045]\tLoss: 573.3872\n",
      "Training Epoch: 37 [24800/36045]\tLoss: 471.4627\n",
      "Training Epoch: 37 [24850/36045]\tLoss: 490.4029\n",
      "Training Epoch: 37 [24900/36045]\tLoss: 487.7793\n",
      "Training Epoch: 37 [24950/36045]\tLoss: 490.3379\n",
      "Training Epoch: 37 [25000/36045]\tLoss: 471.3314\n",
      "Training Epoch: 37 [25050/36045]\tLoss: 450.4067\n",
      "Training Epoch: 37 [25100/36045]\tLoss: 403.5500\n",
      "Training Epoch: 37 [25150/36045]\tLoss: 373.7250\n",
      "Training Epoch: 37 [25200/36045]\tLoss: 368.7844\n",
      "Training Epoch: 37 [25250/36045]\tLoss: 395.2758\n",
      "Training Epoch: 37 [25300/36045]\tLoss: 519.2457\n",
      "Training Epoch: 37 [25350/36045]\tLoss: 515.6266\n",
      "Training Epoch: 37 [25400/36045]\tLoss: 481.4172\n",
      "Training Epoch: 37 [25450/36045]\tLoss: 483.7112\n",
      "Training Epoch: 37 [25500/36045]\tLoss: 525.5805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 37 [25550/36045]\tLoss: 614.8141\n",
      "Training Epoch: 37 [25600/36045]\tLoss: 618.9585\n",
      "Training Epoch: 37 [25650/36045]\tLoss: 597.2482\n",
      "Training Epoch: 37 [25700/36045]\tLoss: 606.5245\n",
      "Training Epoch: 37 [25750/36045]\tLoss: 585.5048\n",
      "Training Epoch: 37 [25800/36045]\tLoss: 369.6737\n",
      "Training Epoch: 37 [25850/36045]\tLoss: 378.3726\n",
      "Training Epoch: 37 [25900/36045]\tLoss: 359.4659\n",
      "Training Epoch: 37 [25950/36045]\tLoss: 368.3887\n",
      "Training Epoch: 37 [26000/36045]\tLoss: 452.3830\n",
      "Training Epoch: 37 [26050/36045]\tLoss: 616.0290\n",
      "Training Epoch: 37 [26100/36045]\tLoss: 642.9788\n",
      "Training Epoch: 37 [26150/36045]\tLoss: 642.9093\n",
      "Training Epoch: 37 [26200/36045]\tLoss: 615.9213\n",
      "Training Epoch: 37 [26250/36045]\tLoss: 645.6891\n",
      "Training Epoch: 37 [26300/36045]\tLoss: 588.9902\n",
      "Training Epoch: 37 [26350/36045]\tLoss: 600.0956\n",
      "Training Epoch: 37 [26400/36045]\tLoss: 576.4983\n",
      "Training Epoch: 37 [26450/36045]\tLoss: 506.3156\n",
      "Training Epoch: 37 [26500/36045]\tLoss: 599.9631\n",
      "Training Epoch: 37 [26550/36045]\tLoss: 600.1094\n",
      "Training Epoch: 37 [26600/36045]\tLoss: 596.6891\n",
      "Training Epoch: 37 [26650/36045]\tLoss: 612.4203\n",
      "Training Epoch: 37 [26700/36045]\tLoss: 591.6830\n",
      "Training Epoch: 37 [26750/36045]\tLoss: 554.3047\n",
      "Training Epoch: 37 [26800/36045]\tLoss: 408.8534\n",
      "Training Epoch: 37 [26850/36045]\tLoss: 338.8882\n",
      "Training Epoch: 37 [26900/36045]\tLoss: 341.4269\n",
      "Training Epoch: 37 [26950/36045]\tLoss: 375.0795\n",
      "Training Epoch: 37 [27000/36045]\tLoss: 614.8560\n",
      "Training Epoch: 37 [27050/36045]\tLoss: 641.9149\n",
      "Training Epoch: 37 [27100/36045]\tLoss: 622.1254\n",
      "Training Epoch: 37 [27150/36045]\tLoss: 661.9316\n",
      "Training Epoch: 37 [27200/36045]\tLoss: 481.8726\n",
      "Training Epoch: 37 [27250/36045]\tLoss: 473.0200\n",
      "Training Epoch: 37 [27300/36045]\tLoss: 460.9947\n",
      "Training Epoch: 37 [27350/36045]\tLoss: 458.9526\n",
      "Training Epoch: 37 [27400/36045]\tLoss: 458.2413\n",
      "Training Epoch: 37 [27450/36045]\tLoss: 579.3736\n",
      "Training Epoch: 37 [27500/36045]\tLoss: 620.8193\n",
      "Training Epoch: 37 [27550/36045]\tLoss: 614.1010\n",
      "Training Epoch: 37 [27600/36045]\tLoss: 625.6962\n",
      "Training Epoch: 37 [27650/36045]\tLoss: 617.6230\n",
      "Training Epoch: 37 [27700/36045]\tLoss: 645.7652\n",
      "Training Epoch: 37 [27750/36045]\tLoss: 657.7642\n",
      "Training Epoch: 37 [27800/36045]\tLoss: 644.7661\n",
      "Training Epoch: 37 [27850/36045]\tLoss: 634.8489\n",
      "Training Epoch: 37 [27900/36045]\tLoss: 575.6708\n",
      "Training Epoch: 37 [27950/36045]\tLoss: 480.4352\n",
      "Training Epoch: 37 [28000/36045]\tLoss: 457.1316\n",
      "Training Epoch: 37 [28050/36045]\tLoss: 466.8786\n",
      "Training Epoch: 37 [28100/36045]\tLoss: 458.5030\n",
      "Training Epoch: 37 [28150/36045]\tLoss: 479.4402\n",
      "Training Epoch: 37 [28200/36045]\tLoss: 487.3453\n",
      "Training Epoch: 37 [28250/36045]\tLoss: 480.4672\n",
      "Training Epoch: 37 [28300/36045]\tLoss: 456.0887\n",
      "Training Epoch: 37 [28350/36045]\tLoss: 451.9867\n",
      "Training Epoch: 37 [28400/36045]\tLoss: 780.9387\n",
      "Training Epoch: 37 [28450/36045]\tLoss: 715.9694\n",
      "Training Epoch: 37 [28500/36045]\tLoss: 618.6896\n",
      "Training Epoch: 37 [28550/36045]\tLoss: 568.6420\n",
      "Training Epoch: 37 [28600/36045]\tLoss: 596.2358\n",
      "Training Epoch: 37 [28650/36045]\tLoss: 655.3176\n",
      "Training Epoch: 37 [28700/36045]\tLoss: 649.2912\n",
      "Training Epoch: 37 [28750/36045]\tLoss: 635.9061\n",
      "Training Epoch: 37 [28800/36045]\tLoss: 645.2626\n",
      "Training Epoch: 37 [28850/36045]\tLoss: 560.0947\n",
      "Training Epoch: 37 [28900/36045]\tLoss: 455.3050\n",
      "Training Epoch: 37 [28950/36045]\tLoss: 454.3549\n",
      "Training Epoch: 37 [29000/36045]\tLoss: 450.8404\n",
      "Training Epoch: 37 [29050/36045]\tLoss: 457.6830\n",
      "Training Epoch: 37 [29100/36045]\tLoss: 476.0432\n",
      "Training Epoch: 37 [29150/36045]\tLoss: 465.7663\n",
      "Training Epoch: 37 [29200/36045]\tLoss: 451.5767\n",
      "Training Epoch: 37 [29250/36045]\tLoss: 441.3864\n",
      "Training Epoch: 37 [29300/36045]\tLoss: 498.3887\n",
      "Training Epoch: 37 [29350/36045]\tLoss: 586.3016\n",
      "Training Epoch: 37 [29400/36045]\tLoss: 603.7888\n",
      "Training Epoch: 37 [29450/36045]\tLoss: 621.1306\n",
      "Training Epoch: 37 [29500/36045]\tLoss: 636.0222\n",
      "Training Epoch: 37 [29550/36045]\tLoss: 605.3034\n",
      "Training Epoch: 37 [29600/36045]\tLoss: 509.9380\n",
      "Training Epoch: 37 [29650/36045]\tLoss: 491.7011\n",
      "Training Epoch: 37 [29700/36045]\tLoss: 440.3536\n",
      "Training Epoch: 37 [29750/36045]\tLoss: 438.9492\n",
      "Training Epoch: 37 [29800/36045]\tLoss: 486.1508\n",
      "Training Epoch: 37 [29850/36045]\tLoss: 562.8274\n",
      "Training Epoch: 37 [29900/36045]\tLoss: 558.9078\n",
      "Training Epoch: 37 [29950/36045]\tLoss: 580.6364\n",
      "Training Epoch: 37 [30000/36045]\tLoss: 554.8240\n",
      "Training Epoch: 37 [30050/36045]\tLoss: 561.5509\n",
      "Training Epoch: 37 [30100/36045]\tLoss: 685.2055\n",
      "Training Epoch: 37 [30150/36045]\tLoss: 668.2108\n",
      "Training Epoch: 37 [30200/36045]\tLoss: 630.2803\n",
      "Training Epoch: 37 [30250/36045]\tLoss: 678.7040\n",
      "Training Epoch: 37 [30300/36045]\tLoss: 663.2233\n",
      "Training Epoch: 37 [30350/36045]\tLoss: 509.4327\n",
      "Training Epoch: 37 [30400/36045]\tLoss: 494.0668\n",
      "Training Epoch: 37 [30450/36045]\tLoss: 496.2383\n",
      "Training Epoch: 37 [30500/36045]\tLoss: 463.2447\n",
      "Training Epoch: 37 [30550/36045]\tLoss: 429.5346\n",
      "Training Epoch: 37 [30600/36045]\tLoss: 421.1956\n",
      "Training Epoch: 37 [30650/36045]\tLoss: 411.2377\n",
      "Training Epoch: 37 [30700/36045]\tLoss: 429.0745\n",
      "Training Epoch: 37 [30750/36045]\tLoss: 416.0197\n",
      "Training Epoch: 37 [30800/36045]\tLoss: 442.3116\n",
      "Training Epoch: 37 [30850/36045]\tLoss: 433.9794\n",
      "Training Epoch: 37 [30900/36045]\tLoss: 446.1767\n",
      "Training Epoch: 37 [30950/36045]\tLoss: 468.7840\n",
      "Training Epoch: 37 [31000/36045]\tLoss: 460.9626\n",
      "Training Epoch: 37 [31050/36045]\tLoss: 385.0711\n",
      "Training Epoch: 37 [31100/36045]\tLoss: 375.6441\n",
      "Training Epoch: 37 [31150/36045]\tLoss: 383.1632\n",
      "Training Epoch: 37 [31200/36045]\tLoss: 476.1913\n",
      "Training Epoch: 37 [31250/36045]\tLoss: 617.8687\n",
      "Training Epoch: 37 [31300/36045]\tLoss: 588.8310\n",
      "Training Epoch: 37 [31350/36045]\tLoss: 604.7078\n",
      "Training Epoch: 37 [31400/36045]\tLoss: 584.1986\n",
      "Training Epoch: 37 [31450/36045]\tLoss: 600.7057\n",
      "Training Epoch: 37 [31500/36045]\tLoss: 613.2378\n",
      "Training Epoch: 37 [31550/36045]\tLoss: 620.4774\n",
      "Training Epoch: 37 [31600/36045]\tLoss: 583.3000\n",
      "Training Epoch: 37 [31650/36045]\tLoss: 624.1726\n",
      "Training Epoch: 37 [31700/36045]\tLoss: 451.5985\n",
      "Training Epoch: 37 [31750/36045]\tLoss: 373.3414\n",
      "Training Epoch: 37 [31800/36045]\tLoss: 356.3421\n",
      "Training Epoch: 37 [31850/36045]\tLoss: 364.5017\n",
      "Training Epoch: 37 [31900/36045]\tLoss: 574.7619\n",
      "Training Epoch: 37 [31950/36045]\tLoss: 743.7175\n",
      "Training Epoch: 37 [32000/36045]\tLoss: 852.3314\n",
      "Training Epoch: 37 [32050/36045]\tLoss: 807.6971\n",
      "Training Epoch: 37 [32100/36045]\tLoss: 798.3801\n",
      "Training Epoch: 37 [32150/36045]\tLoss: 615.5287\n",
      "Training Epoch: 37 [32200/36045]\tLoss: 618.2216\n",
      "Training Epoch: 37 [32250/36045]\tLoss: 628.6177\n",
      "Training Epoch: 37 [32300/36045]\tLoss: 610.5210\n",
      "Training Epoch: 37 [32350/36045]\tLoss: 606.2860\n",
      "Training Epoch: 37 [32400/36045]\tLoss: 568.8810\n",
      "Training Epoch: 37 [32450/36045]\tLoss: 468.4243\n",
      "Training Epoch: 37 [32500/36045]\tLoss: 450.1269\n",
      "Training Epoch: 37 [32550/36045]\tLoss: 452.3426\n",
      "Training Epoch: 37 [32600/36045]\tLoss: 449.3133\n",
      "Training Epoch: 37 [32650/36045]\tLoss: 580.3219\n",
      "Training Epoch: 37 [32700/36045]\tLoss: 633.5748\n",
      "Training Epoch: 37 [32750/36045]\tLoss: 603.5276\n",
      "Training Epoch: 37 [32800/36045]\tLoss: 618.9138\n",
      "Training Epoch: 37 [32850/36045]\tLoss: 571.3676\n",
      "Training Epoch: 37 [32900/36045]\tLoss: 457.4612\n",
      "Training Epoch: 37 [32950/36045]\tLoss: 479.0963\n",
      "Training Epoch: 37 [33000/36045]\tLoss: 478.0971\n",
      "Training Epoch: 37 [33050/36045]\tLoss: 454.4728\n",
      "Training Epoch: 37 [33100/36045]\tLoss: 516.4617\n",
      "Training Epoch: 37 [33150/36045]\tLoss: 702.1259\n",
      "Training Epoch: 37 [33200/36045]\tLoss: 683.6931\n",
      "Training Epoch: 37 [33250/36045]\tLoss: 704.5815\n",
      "Training Epoch: 37 [33300/36045]\tLoss: 750.6216\n",
      "Training Epoch: 37 [33350/36045]\tLoss: 575.1586\n",
      "Training Epoch: 37 [33400/36045]\tLoss: 420.0283\n",
      "Training Epoch: 37 [33450/36045]\tLoss: 415.5952\n",
      "Training Epoch: 37 [33500/36045]\tLoss: 427.8559\n",
      "Training Epoch: 37 [33550/36045]\tLoss: 443.3781\n",
      "Training Epoch: 37 [33600/36045]\tLoss: 445.1949\n",
      "Training Epoch: 37 [33650/36045]\tLoss: 594.3714\n",
      "Training Epoch: 38 [8000/36045]\tLoss: 604.5374\n",
      "Training Epoch: 38 [8050/36045]\tLoss: 569.7088\n",
      "Training Epoch: 38 [8100/36045]\tLoss: 594.9429\n",
      "Training Epoch: 38 [8150/36045]\tLoss: 674.2400\n",
      "Training Epoch: 38 [8200/36045]\tLoss: 661.3805\n",
      "Training Epoch: 38 [8250/36045]\tLoss: 629.3292\n",
      "Training Epoch: 38 [8300/36045]\tLoss: 687.1906\n",
      "Training Epoch: 38 [8350/36045]\tLoss: 630.3912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 38 [8400/36045]\tLoss: 564.3455\n",
      "Training Epoch: 38 [8450/36045]\tLoss: 528.2840\n",
      "Training Epoch: 38 [8500/36045]\tLoss: 561.8758\n",
      "Training Epoch: 38 [8550/36045]\tLoss: 554.7646\n",
      "Training Epoch: 38 [8600/36045]\tLoss: 548.9343\n",
      "Training Epoch: 38 [8650/36045]\tLoss: 583.7648\n",
      "Training Epoch: 38 [8700/36045]\tLoss: 617.4573\n",
      "Training Epoch: 38 [8750/36045]\tLoss: 606.4929\n",
      "Training Epoch: 38 [8800/36045]\tLoss: 612.2732\n",
      "Training Epoch: 38 [8850/36045]\tLoss: 605.7526\n",
      "Training Epoch: 38 [8900/36045]\tLoss: 546.8583\n",
      "Training Epoch: 38 [8950/36045]\tLoss: 558.1412\n",
      "Training Epoch: 38 [9000/36045]\tLoss: 573.8730\n",
      "Training Epoch: 38 [9050/36045]\tLoss: 575.3896\n",
      "Training Epoch: 38 [9100/36045]\tLoss: 592.3979\n",
      "Training Epoch: 38 [9150/36045]\tLoss: 438.3478\n",
      "Training Epoch: 38 [9200/36045]\tLoss: 327.8332\n",
      "Training Epoch: 38 [9250/36045]\tLoss: 355.6632\n",
      "Training Epoch: 38 [9300/36045]\tLoss: 365.6955\n",
      "Training Epoch: 38 [9350/36045]\tLoss: 337.3786\n",
      "Training Epoch: 38 [9400/36045]\tLoss: 661.0889\n",
      "Training Epoch: 38 [9450/36045]\tLoss: 702.1379\n",
      "Training Epoch: 38 [9500/36045]\tLoss: 689.4346\n",
      "Training Epoch: 38 [9550/36045]\tLoss: 729.3602\n",
      "Training Epoch: 38 [9600/36045]\tLoss: 542.8043\n",
      "Training Epoch: 38 [9650/36045]\tLoss: 547.7492\n",
      "Training Epoch: 38 [9700/36045]\tLoss: 533.2251\n",
      "Training Epoch: 38 [9750/36045]\tLoss: 531.9407\n",
      "Training Epoch: 38 [9800/36045]\tLoss: 695.7166\n",
      "Training Epoch: 38 [9850/36045]\tLoss: 734.9009\n",
      "Training Epoch: 38 [9900/36045]\tLoss: 745.4307\n",
      "Training Epoch: 38 [9950/36045]\tLoss: 726.3321\n",
      "Training Epoch: 38 [10000/36045]\tLoss: 671.6663\n",
      "Training Epoch: 38 [10050/36045]\tLoss: 550.7715\n",
      "Training Epoch: 38 [10100/36045]\tLoss: 558.4476\n",
      "Training Epoch: 38 [10150/36045]\tLoss: 567.1786\n",
      "Training Epoch: 38 [10200/36045]\tLoss: 556.0845\n",
      "Training Epoch: 38 [10250/36045]\tLoss: 666.9895\n",
      "Training Epoch: 38 [10300/36045]\tLoss: 648.0535\n",
      "Training Epoch: 38 [10350/36045]\tLoss: 682.6415\n",
      "Training Epoch: 38 [10400/36045]\tLoss: 672.9832\n",
      "Training Epoch: 38 [10450/36045]\tLoss: 630.5257\n",
      "Training Epoch: 38 [10500/36045]\tLoss: 526.7422\n",
      "Training Epoch: 38 [10550/36045]\tLoss: 521.5230\n",
      "Training Epoch: 38 [10600/36045]\tLoss: 543.8214\n",
      "Training Epoch: 38 [10650/36045]\tLoss: 549.9333\n",
      "Training Epoch: 38 [10700/36045]\tLoss: 631.4120\n",
      "Training Epoch: 38 [10750/36045]\tLoss: 691.1452\n",
      "Training Epoch: 38 [10800/36045]\tLoss: 636.3750\n",
      "Training Epoch: 38 [10850/36045]\tLoss: 674.5744\n",
      "Training Epoch: 38 [10900/36045]\tLoss: 702.2095\n",
      "Training Epoch: 38 [10950/36045]\tLoss: 517.3896\n",
      "Training Epoch: 38 [11000/36045]\tLoss: 511.0152\n",
      "Training Epoch: 38 [11050/36045]\tLoss: 547.8591\n",
      "Training Epoch: 38 [11100/36045]\tLoss: 558.3419\n",
      "Training Epoch: 38 [11150/36045]\tLoss: 605.5953\n",
      "Training Epoch: 38 [11200/36045]\tLoss: 633.6643\n",
      "Training Epoch: 38 [11250/36045]\tLoss: 645.3223\n",
      "Training Epoch: 38 [11300/36045]\tLoss: 625.5112\n",
      "Training Epoch: 38 [11350/36045]\tLoss: 623.0701\n",
      "Training Epoch: 38 [11400/36045]\tLoss: 585.9893\n",
      "Training Epoch: 38 [11450/36045]\tLoss: 554.7187\n",
      "Training Epoch: 38 [11500/36045]\tLoss: 552.0915\n",
      "Training Epoch: 38 [11550/36045]\tLoss: 562.3027\n",
      "Training Epoch: 38 [11600/36045]\tLoss: 622.8325\n",
      "Training Epoch: 38 [11650/36045]\tLoss: 674.5499\n",
      "Training Epoch: 38 [11700/36045]\tLoss: 673.5198\n",
      "Training Epoch: 38 [11750/36045]\tLoss: 691.9084\n",
      "Training Epoch: 38 [11800/36045]\tLoss: 734.5352\n",
      "Training Epoch: 38 [11850/36045]\tLoss: 792.6616\n",
      "Training Epoch: 38 [11900/36045]\tLoss: 1005.8108\n",
      "Training Epoch: 38 [11950/36045]\tLoss: 1008.6432\n",
      "Training Epoch: 38 [12000/36045]\tLoss: 1020.6360\n",
      "Training Epoch: 38 [12050/36045]\tLoss: 979.8499\n",
      "Training Epoch: 38 [12100/36045]\tLoss: 630.2799\n",
      "Training Epoch: 38 [12150/36045]\tLoss: 476.6607\n",
      "Training Epoch: 38 [12200/36045]\tLoss: 471.3863\n",
      "Training Epoch: 38 [12250/36045]\tLoss: 480.1427\n",
      "Training Epoch: 38 [12300/36045]\tLoss: 617.2716\n",
      "Training Epoch: 38 [12350/36045]\tLoss: 673.1619\n",
      "Training Epoch: 38 [12400/36045]\tLoss: 680.5878\n",
      "Training Epoch: 38 [12450/36045]\tLoss: 669.3196\n",
      "Training Epoch: 38 [12500/36045]\tLoss: 696.6278\n",
      "Training Epoch: 38 [12550/36045]\tLoss: 666.3107\n",
      "Training Epoch: 38 [12600/36045]\tLoss: 610.7128\n",
      "Training Epoch: 38 [12650/36045]\tLoss: 609.1449\n",
      "Training Epoch: 38 [12700/36045]\tLoss: 630.5118\n",
      "Training Epoch: 38 [12750/36045]\tLoss: 629.2169\n",
      "Training Epoch: 38 [12800/36045]\tLoss: 614.4106\n",
      "Training Epoch: 38 [12850/36045]\tLoss: 643.6006\n",
      "Training Epoch: 38 [12900/36045]\tLoss: 617.1323\n",
      "Training Epoch: 38 [12950/36045]\tLoss: 603.3469\n",
      "Training Epoch: 38 [13000/36045]\tLoss: 636.3215\n",
      "Training Epoch: 38 [13050/36045]\tLoss: 576.0867\n",
      "Training Epoch: 38 [13100/36045]\tLoss: 592.4791\n",
      "Training Epoch: 38 [13150/36045]\tLoss: 584.1483\n",
      "Training Epoch: 38 [13200/36045]\tLoss: 566.5333\n",
      "Training Epoch: 38 [13250/36045]\tLoss: 589.0247\n",
      "Training Epoch: 38 [13300/36045]\tLoss: 626.6270\n",
      "Training Epoch: 38 [13350/36045]\tLoss: 607.2228\n",
      "Training Epoch: 38 [13400/36045]\tLoss: 610.4648\n",
      "Training Epoch: 38 [13450/36045]\tLoss: 607.6251\n",
      "Training Epoch: 38 [13500/36045]\tLoss: 626.9225\n",
      "Training Epoch: 38 [13550/36045]\tLoss: 764.3461\n",
      "Training Epoch: 38 [13600/36045]\tLoss: 797.0778\n",
      "Training Epoch: 38 [13650/36045]\tLoss: 878.3106\n",
      "Training Epoch: 38 [13700/36045]\tLoss: 774.5186\n",
      "Training Epoch: 38 [13750/36045]\tLoss: 613.8822\n",
      "Training Epoch: 38 [13800/36045]\tLoss: 585.3217\n",
      "Training Epoch: 38 [13850/36045]\tLoss: 567.9655\n",
      "Training Epoch: 38 [13900/36045]\tLoss: 575.1978\n",
      "Training Epoch: 38 [13950/36045]\tLoss: 620.4521\n",
      "Training Epoch: 38 [14000/36045]\tLoss: 653.5997\n",
      "Training Epoch: 38 [14050/36045]\tLoss: 628.4681\n",
      "Training Epoch: 38 [14100/36045]\tLoss: 623.6628\n",
      "Training Epoch: 38 [14150/36045]\tLoss: 611.6132\n",
      "Training Epoch: 38 [14200/36045]\tLoss: 652.4831\n",
      "Training Epoch: 38 [14250/36045]\tLoss: 716.7674\n",
      "Training Epoch: 38 [14300/36045]\tLoss: 720.1377\n",
      "Training Epoch: 38 [14350/36045]\tLoss: 688.5222\n",
      "Training Epoch: 38 [14400/36045]\tLoss: 674.3741\n",
      "Training Epoch: 38 [14450/36045]\tLoss: 710.1053\n",
      "Training Epoch: 38 [14500/36045]\tLoss: 641.2523\n",
      "Training Epoch: 38 [14550/36045]\tLoss: 670.0887\n",
      "Training Epoch: 38 [14600/36045]\tLoss: 656.4845\n",
      "Training Epoch: 38 [14650/36045]\tLoss: 656.1118\n",
      "Training Epoch: 38 [14700/36045]\tLoss: 621.5516\n",
      "Training Epoch: 38 [14750/36045]\tLoss: 534.5001\n",
      "Training Epoch: 38 [14800/36045]\tLoss: 524.6585\n",
      "Training Epoch: 38 [14850/36045]\tLoss: 531.6880\n",
      "Training Epoch: 38 [14900/36045]\tLoss: 525.2071\n",
      "Training Epoch: 38 [14950/36045]\tLoss: 533.1542\n",
      "Training Epoch: 38 [15000/36045]\tLoss: 546.2529\n",
      "Training Epoch: 38 [15050/36045]\tLoss: 543.2723\n",
      "Training Epoch: 38 [15100/36045]\tLoss: 527.3513\n",
      "Training Epoch: 38 [15150/36045]\tLoss: 522.5769\n",
      "Training Epoch: 38 [15200/36045]\tLoss: 483.9983\n",
      "Training Epoch: 38 [15250/36045]\tLoss: 506.0234\n",
      "Training Epoch: 38 [15300/36045]\tLoss: 491.4314\n",
      "Training Epoch: 38 [15350/36045]\tLoss: 503.1057\n",
      "Training Epoch: 38 [15400/36045]\tLoss: 486.2853\n",
      "Training Epoch: 38 [15450/36045]\tLoss: 471.5143\n",
      "Training Epoch: 38 [15500/36045]\tLoss: 485.0186\n",
      "Training Epoch: 38 [15550/36045]\tLoss: 481.3007\n",
      "Training Epoch: 38 [15600/36045]\tLoss: 549.2203\n",
      "Training Epoch: 38 [15650/36045]\tLoss: 566.4417\n",
      "Training Epoch: 38 [15700/36045]\tLoss: 558.4317\n",
      "Training Epoch: 38 [15750/36045]\tLoss: 549.8436\n",
      "Training Epoch: 38 [15800/36045]\tLoss: 524.6151\n",
      "Training Epoch: 38 [15850/36045]\tLoss: 539.4992\n",
      "Training Epoch: 38 [15900/36045]\tLoss: 548.6558\n",
      "Training Epoch: 38 [15950/36045]\tLoss: 568.4724\n",
      "Training Epoch: 38 [16000/36045]\tLoss: 540.0836\n",
      "Training Epoch: 38 [16050/36045]\tLoss: 509.5192\n",
      "Training Epoch: 38 [16100/36045]\tLoss: 472.6548\n",
      "Training Epoch: 38 [16150/36045]\tLoss: 460.6093\n",
      "Training Epoch: 38 [16200/36045]\tLoss: 558.9822\n",
      "Training Epoch: 38 [16250/36045]\tLoss: 586.8480\n",
      "Training Epoch: 38 [16300/36045]\tLoss: 640.6701\n",
      "Training Epoch: 38 [16350/36045]\tLoss: 660.5574\n",
      "Training Epoch: 38 [16400/36045]\tLoss: 632.2895\n",
      "Training Epoch: 38 [16450/36045]\tLoss: 614.4553\n",
      "Training Epoch: 38 [16500/36045]\tLoss: 614.3761\n",
      "Training Epoch: 38 [16550/36045]\tLoss: 579.5164\n",
      "Training Epoch: 38 [16600/36045]\tLoss: 602.1522\n",
      "Training Epoch: 38 [16650/36045]\tLoss: 618.7070\n",
      "Training Epoch: 38 [16700/36045]\tLoss: 598.2621\n",
      "Training Epoch: 38 [16750/36045]\tLoss: 590.9091\n",
      "Training Epoch: 38 [16800/36045]\tLoss: 600.1735\n",
      "Training Epoch: 38 [16850/36045]\tLoss: 571.8538\n",
      "Training Epoch: 38 [16900/36045]\tLoss: 581.6821\n",
      "Training Epoch: 38 [16950/36045]\tLoss: 604.8055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 38 [17000/36045]\tLoss: 588.8255\n",
      "Training Epoch: 38 [17050/36045]\tLoss: 613.4981\n",
      "Training Epoch: 38 [17100/36045]\tLoss: 609.5682\n",
      "Training Epoch: 38 [17150/36045]\tLoss: 529.4736\n",
      "Training Epoch: 38 [17200/36045]\tLoss: 491.5253\n",
      "Training Epoch: 38 [17250/36045]\tLoss: 514.6450\n",
      "Training Epoch: 38 [17300/36045]\tLoss: 544.6350\n",
      "Training Epoch: 38 [17350/36045]\tLoss: 524.7618\n",
      "Training Epoch: 38 [17400/36045]\tLoss: 544.2288\n",
      "Training Epoch: 38 [17450/36045]\tLoss: 563.0342\n",
      "Training Epoch: 38 [17500/36045]\tLoss: 551.3105\n",
      "Training Epoch: 38 [17550/36045]\tLoss: 550.0209\n",
      "Training Epoch: 38 [17600/36045]\tLoss: 544.2393\n",
      "Training Epoch: 38 [17650/36045]\tLoss: 560.0761\n",
      "Training Epoch: 38 [17700/36045]\tLoss: 539.3255\n",
      "Training Epoch: 38 [17750/36045]\tLoss: 555.5266\n",
      "Training Epoch: 38 [17800/36045]\tLoss: 546.5170\n",
      "Training Epoch: 38 [17850/36045]\tLoss: 561.1790\n",
      "Training Epoch: 38 [17900/36045]\tLoss: 589.3572\n",
      "Training Epoch: 38 [17950/36045]\tLoss: 601.4935\n",
      "Training Epoch: 38 [18000/36045]\tLoss: 591.9943\n",
      "Training Epoch: 38 [18050/36045]\tLoss: 651.1395\n",
      "Training Epoch: 38 [18100/36045]\tLoss: 652.5990\n",
      "Training Epoch: 38 [18150/36045]\tLoss: 663.9230\n",
      "Training Epoch: 38 [18200/36045]\tLoss: 646.0325\n",
      "Training Epoch: 38 [18250/36045]\tLoss: 666.8562\n",
      "Training Epoch: 38 [18300/36045]\tLoss: 621.0187\n",
      "Training Epoch: 38 [18350/36045]\tLoss: 693.4824\n",
      "Training Epoch: 38 [18400/36045]\tLoss: 666.5743\n",
      "Training Epoch: 38 [18450/36045]\tLoss: 646.3405\n",
      "Training Epoch: 38 [18500/36045]\tLoss: 645.2809\n",
      "Training Epoch: 38 [18550/36045]\tLoss: 632.6115\n",
      "Training Epoch: 38 [18600/36045]\tLoss: 622.1717\n",
      "Training Epoch: 38 [18650/36045]\tLoss: 668.0856\n",
      "Training Epoch: 38 [18700/36045]\tLoss: 702.8181\n",
      "Training Epoch: 38 [18750/36045]\tLoss: 690.1581\n",
      "Training Epoch: 38 [18800/36045]\tLoss: 713.1437\n",
      "Training Epoch: 38 [18850/36045]\tLoss: 658.0598\n",
      "Training Epoch: 38 [18900/36045]\tLoss: 703.8905\n",
      "Training Epoch: 38 [18950/36045]\tLoss: 646.2043\n",
      "Training Epoch: 38 [19000/36045]\tLoss: 533.7211\n",
      "Training Epoch: 38 [19050/36045]\tLoss: 517.8697\n",
      "Training Epoch: 38 [19100/36045]\tLoss: 526.0888\n",
      "Training Epoch: 38 [19150/36045]\tLoss: 516.1536\n",
      "Training Epoch: 38 [19200/36045]\tLoss: 546.4284\n",
      "Training Epoch: 38 [19250/36045]\tLoss: 561.4622\n",
      "Training Epoch: 38 [19300/36045]\tLoss: 571.0284\n",
      "Training Epoch: 38 [19350/36045]\tLoss: 554.7019\n",
      "Training Epoch: 38 [19400/36045]\tLoss: 575.4106\n",
      "Training Epoch: 38 [19450/36045]\tLoss: 566.6453\n",
      "Training Epoch: 38 [19500/36045]\tLoss: 568.1226\n",
      "Training Epoch: 38 [19550/36045]\tLoss: 566.4214\n",
      "Training Epoch: 38 [19600/36045]\tLoss: 607.2809\n",
      "Training Epoch: 38 [19650/36045]\tLoss: 809.2288\n",
      "Training Epoch: 38 [19700/36045]\tLoss: 767.9274\n",
      "Training Epoch: 38 [19750/36045]\tLoss: 771.8350\n",
      "Training Epoch: 38 [19800/36045]\tLoss: 771.9972\n",
      "Training Epoch: 38 [19850/36045]\tLoss: 507.1611\n",
      "Training Epoch: 38 [19900/36045]\tLoss: 485.9860\n",
      "Training Epoch: 38 [19950/36045]\tLoss: 489.6513\n",
      "Training Epoch: 38 [20000/36045]\tLoss: 489.1837\n",
      "Training Epoch: 38 [20050/36045]\tLoss: 547.6868\n",
      "Training Epoch: 38 [20100/36045]\tLoss: 553.4910\n",
      "Training Epoch: 38 [20150/36045]\tLoss: 554.8932\n",
      "Training Epoch: 38 [20200/36045]\tLoss: 554.5997\n",
      "Training Epoch: 38 [20250/36045]\tLoss: 591.1306\n",
      "Training Epoch: 38 [20300/36045]\tLoss: 627.1235\n",
      "Training Epoch: 38 [20350/36045]\tLoss: 645.6234\n",
      "Training Epoch: 38 [20400/36045]\tLoss: 662.2239\n",
      "Training Epoch: 38 [20450/36045]\tLoss: 632.3354\n",
      "Training Epoch: 38 [20500/36045]\tLoss: 616.6933\n",
      "Training Epoch: 38 [20550/36045]\tLoss: 542.1016\n",
      "Training Epoch: 38 [20600/36045]\tLoss: 552.2012\n",
      "Training Epoch: 38 [20650/36045]\tLoss: 549.5342\n",
      "Training Epoch: 38 [20700/36045]\tLoss: 537.5886\n",
      "Training Epoch: 38 [20750/36045]\tLoss: 579.4352\n",
      "Training Epoch: 38 [20800/36045]\tLoss: 629.5034\n",
      "Training Epoch: 38 [20850/36045]\tLoss: 616.2388\n",
      "Training Epoch: 38 [20900/36045]\tLoss: 659.9525\n",
      "Training Epoch: 38 [20950/36045]\tLoss: 622.1398\n",
      "Training Epoch: 38 [21000/36045]\tLoss: 585.5989\n",
      "Training Epoch: 38 [21050/36045]\tLoss: 500.9862\n",
      "Training Epoch: 38 [21100/36045]\tLoss: 505.4999\n",
      "Training Epoch: 38 [21150/36045]\tLoss: 541.0760\n",
      "Training Epoch: 38 [21200/36045]\tLoss: 540.2787\n",
      "Training Epoch: 38 [21250/36045]\tLoss: 516.8318\n",
      "Training Epoch: 38 [21300/36045]\tLoss: 603.4993\n",
      "Training Epoch: 38 [21350/36045]\tLoss: 595.3633\n",
      "Training Epoch: 38 [21400/36045]\tLoss: 598.9652\n",
      "Training Epoch: 38 [21450/36045]\tLoss: 605.1584\n",
      "Training Epoch: 38 [21500/36045]\tLoss: 607.1960\n",
      "Training Epoch: 38 [21550/36045]\tLoss: 701.7007\n",
      "Training Epoch: 38 [21600/36045]\tLoss: 700.3530\n",
      "Training Epoch: 38 [21650/36045]\tLoss: 712.7160\n",
      "Training Epoch: 38 [21700/36045]\tLoss: 715.6596\n",
      "Training Epoch: 38 [21750/36045]\tLoss: 687.1984\n",
      "Training Epoch: 38 [21800/36045]\tLoss: 505.7029\n",
      "Training Epoch: 38 [21850/36045]\tLoss: 488.7282\n",
      "Training Epoch: 38 [21900/36045]\tLoss: 498.1637\n",
      "Training Epoch: 38 [21950/36045]\tLoss: 498.9747\n",
      "Training Epoch: 38 [22000/36045]\tLoss: 502.2912\n",
      "Training Epoch: 38 [22050/36045]\tLoss: 523.0201\n",
      "Training Epoch: 38 [22100/36045]\tLoss: 515.6291\n",
      "Training Epoch: 38 [22150/36045]\tLoss: 501.6322\n",
      "Training Epoch: 38 [22200/36045]\tLoss: 517.6315\n",
      "Training Epoch: 38 [22250/36045]\tLoss: 522.6013\n",
      "Training Epoch: 38 [22300/36045]\tLoss: 574.6592\n",
      "Training Epoch: 38 [22350/36045]\tLoss: 600.4394\n",
      "Training Epoch: 38 [22400/36045]\tLoss: 614.7231\n",
      "Training Epoch: 38 [22450/36045]\tLoss: 602.0416\n",
      "Training Epoch: 38 [22500/36045]\tLoss: 584.6868\n",
      "Training Epoch: 38 [22550/36045]\tLoss: 619.9662\n",
      "Training Epoch: 38 [22600/36045]\tLoss: 670.5876\n",
      "Training Epoch: 38 [22650/36045]\tLoss: 704.4653\n",
      "Training Epoch: 38 [22700/36045]\tLoss: 726.2490\n",
      "Training Epoch: 38 [22750/36045]\tLoss: 745.6002\n",
      "Training Epoch: 38 [22800/36045]\tLoss: 775.0309\n",
      "Training Epoch: 38 [22850/36045]\tLoss: 644.2085\n",
      "Training Epoch: 38 [22900/36045]\tLoss: 649.1284\n",
      "Training Epoch: 38 [22950/36045]\tLoss: 628.6541\n",
      "Training Epoch: 38 [23000/36045]\tLoss: 625.3604\n",
      "Training Epoch: 38 [23050/36045]\tLoss: 555.4644\n",
      "Training Epoch: 38 [23100/36045]\tLoss: 571.1483\n",
      "Training Epoch: 38 [23150/36045]\tLoss: 559.5086\n",
      "Training Epoch: 38 [23200/36045]\tLoss: 529.7762\n",
      "Training Epoch: 38 [23250/36045]\tLoss: 532.8019\n",
      "Training Epoch: 38 [23300/36045]\tLoss: 528.8981\n",
      "Training Epoch: 38 [23350/36045]\tLoss: 549.5408\n",
      "Training Epoch: 38 [23400/36045]\tLoss: 595.7940\n",
      "Training Epoch: 38 [23450/36045]\tLoss: 589.2877\n",
      "Training Epoch: 38 [23500/36045]\tLoss: 567.9521\n",
      "Training Epoch: 38 [23550/36045]\tLoss: 608.6828\n",
      "Training Epoch: 38 [23600/36045]\tLoss: 689.7938\n",
      "Training Epoch: 38 [23650/36045]\tLoss: 701.9282\n",
      "Training Epoch: 38 [23700/36045]\tLoss: 709.9822\n",
      "Training Epoch: 38 [23750/36045]\tLoss: 686.0791\n",
      "Training Epoch: 38 [23800/36045]\tLoss: 549.4504\n",
      "Training Epoch: 38 [23850/36045]\tLoss: 575.4319\n",
      "Training Epoch: 38 [23900/36045]\tLoss: 565.1255\n",
      "Training Epoch: 38 [23950/36045]\tLoss: 548.2854\n",
      "Training Epoch: 38 [24000/36045]\tLoss: 525.2669\n",
      "Training Epoch: 38 [24050/36045]\tLoss: 485.1129\n",
      "Training Epoch: 38 [24100/36045]\tLoss: 510.2043\n",
      "Training Epoch: 38 [24150/36045]\tLoss: 502.7085\n",
      "Training Epoch: 38 [24200/36045]\tLoss: 499.8243\n",
      "Training Epoch: 38 [24250/36045]\tLoss: 485.3922\n",
      "Training Epoch: 38 [24300/36045]\tLoss: 524.5086\n",
      "Training Epoch: 38 [24350/36045]\tLoss: 537.0736\n",
      "Training Epoch: 38 [24400/36045]\tLoss: 552.1048\n",
      "Training Epoch: 38 [24450/36045]\tLoss: 526.1421\n",
      "Training Epoch: 38 [24500/36045]\tLoss: 555.1653\n",
      "Training Epoch: 38 [24550/36045]\tLoss: 643.0194\n",
      "Training Epoch: 38 [24600/36045]\tLoss: 634.6447\n",
      "Training Epoch: 38 [24650/36045]\tLoss: 607.7081\n",
      "Training Epoch: 38 [24700/36045]\tLoss: 617.5060\n",
      "Training Epoch: 38 [24750/36045]\tLoss: 570.6417\n",
      "Training Epoch: 38 [24800/36045]\tLoss: 468.9243\n",
      "Training Epoch: 38 [24850/36045]\tLoss: 487.7756\n",
      "Training Epoch: 38 [24900/36045]\tLoss: 485.1739\n",
      "Training Epoch: 38 [24950/36045]\tLoss: 487.7137\n",
      "Training Epoch: 38 [25000/36045]\tLoss: 468.8164\n",
      "Training Epoch: 38 [25050/36045]\tLoss: 448.0437\n",
      "Training Epoch: 38 [25100/36045]\tLoss: 401.4312\n",
      "Training Epoch: 38 [25150/36045]\tLoss: 371.7531\n",
      "Training Epoch: 38 [25200/36045]\tLoss: 366.7952\n",
      "Training Epoch: 38 [25250/36045]\tLoss: 393.1531\n",
      "Training Epoch: 38 [25300/36045]\tLoss: 516.4838\n",
      "Training Epoch: 38 [25350/36045]\tLoss: 512.8272\n",
      "Training Epoch: 38 [25400/36045]\tLoss: 478.9019\n",
      "Training Epoch: 38 [25450/36045]\tLoss: 481.1563\n",
      "Training Epoch: 38 [25500/36045]\tLoss: 522.7869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 38 [25550/36045]\tLoss: 611.6118\n",
      "Training Epoch: 38 [25600/36045]\tLoss: 615.7232\n",
      "Training Epoch: 38 [25650/36045]\tLoss: 594.1415\n",
      "Training Epoch: 38 [25700/36045]\tLoss: 603.4558\n",
      "Training Epoch: 38 [25750/36045]\tLoss: 582.6631\n",
      "Training Epoch: 38 [25800/36045]\tLoss: 367.8927\n",
      "Training Epoch: 38 [25850/36045]\tLoss: 376.4297\n",
      "Training Epoch: 38 [25900/36045]\tLoss: 357.5730\n",
      "Training Epoch: 38 [25950/36045]\tLoss: 366.5602\n",
      "Training Epoch: 38 [26000/36045]\tLoss: 450.2801\n",
      "Training Epoch: 38 [26050/36045]\tLoss: 613.1442\n",
      "Training Epoch: 38 [26100/36045]\tLoss: 639.9463\n",
      "Training Epoch: 38 [26150/36045]\tLoss: 639.7773\n",
      "Training Epoch: 38 [26200/36045]\tLoss: 612.8180\n",
      "Training Epoch: 38 [26250/36045]\tLoss: 642.5992\n",
      "Training Epoch: 38 [26300/36045]\tLoss: 586.8593\n",
      "Training Epoch: 38 [26350/36045]\tLoss: 597.9667\n",
      "Training Epoch: 38 [26400/36045]\tLoss: 574.2043\n",
      "Training Epoch: 38 [26450/36045]\tLoss: 504.0310\n",
      "Training Epoch: 38 [26500/36045]\tLoss: 597.1103\n",
      "Training Epoch: 38 [26550/36045]\tLoss: 597.1863\n",
      "Training Epoch: 38 [26600/36045]\tLoss: 593.7924\n",
      "Training Epoch: 38 [26650/36045]\tLoss: 609.4139\n",
      "Training Epoch: 38 [26700/36045]\tLoss: 588.6135\n",
      "Training Epoch: 38 [26750/36045]\tLoss: 551.4794\n",
      "Training Epoch: 38 [26800/36045]\tLoss: 406.8803\n",
      "Training Epoch: 38 [26850/36045]\tLoss: 337.2241\n",
      "Training Epoch: 38 [26900/36045]\tLoss: 339.6862\n",
      "Training Epoch: 38 [26950/36045]\tLoss: 373.1068\n",
      "Training Epoch: 38 [27000/36045]\tLoss: 612.1059\n",
      "Training Epoch: 38 [27050/36045]\tLoss: 638.9863\n",
      "Training Epoch: 38 [27100/36045]\tLoss: 619.3392\n",
      "Training Epoch: 38 [27150/36045]\tLoss: 659.0244\n",
      "Training Epoch: 38 [27200/36045]\tLoss: 479.4529\n",
      "Training Epoch: 38 [27250/36045]\tLoss: 470.4894\n",
      "Training Epoch: 38 [27300/36045]\tLoss: 458.5603\n",
      "Training Epoch: 38 [27350/36045]\tLoss: 456.4243\n",
      "Training Epoch: 38 [27400/36045]\tLoss: 455.7386\n",
      "Training Epoch: 38 [27450/36045]\tLoss: 576.3193\n",
      "Training Epoch: 38 [27500/36045]\tLoss: 617.5520\n",
      "Training Epoch: 38 [27550/36045]\tLoss: 610.8638\n",
      "Training Epoch: 38 [27600/36045]\tLoss: 622.4377\n",
      "Training Epoch: 38 [27650/36045]\tLoss: 614.3805\n",
      "Training Epoch: 38 [27700/36045]\tLoss: 642.3771\n",
      "Training Epoch: 38 [27750/36045]\tLoss: 654.3588\n",
      "Training Epoch: 38 [27800/36045]\tLoss: 641.4174\n",
      "Training Epoch: 38 [27850/36045]\tLoss: 631.5673\n",
      "Training Epoch: 38 [27900/36045]\tLoss: 572.9501\n",
      "Training Epoch: 38 [27950/36045]\tLoss: 478.3593\n",
      "Training Epoch: 38 [28000/36045]\tLoss: 455.0919\n",
      "Training Epoch: 38 [28050/36045]\tLoss: 464.7756\n",
      "Training Epoch: 38 [28100/36045]\tLoss: 456.4041\n",
      "Training Epoch: 38 [28150/36045]\tLoss: 477.0094\n",
      "Training Epoch: 38 [28200/36045]\tLoss: 485.0339\n",
      "Training Epoch: 38 [28250/36045]\tLoss: 478.1157\n",
      "Training Epoch: 38 [28300/36045]\tLoss: 453.8542\n",
      "Training Epoch: 38 [28350/36045]\tLoss: 449.7225\n",
      "Training Epoch: 38 [28400/36045]\tLoss: 778.3779\n",
      "Training Epoch: 38 [28450/36045]\tLoss: 713.7843\n",
      "Training Epoch: 38 [28500/36045]\tLoss: 616.8174\n",
      "Training Epoch: 38 [28550/36045]\tLoss: 566.9720\n",
      "Training Epoch: 38 [28600/36045]\tLoss: 594.0270\n",
      "Training Epoch: 38 [28650/36045]\tLoss: 652.1549\n",
      "Training Epoch: 38 [28700/36045]\tLoss: 646.1073\n",
      "Training Epoch: 38 [28750/36045]\tLoss: 632.7947\n",
      "Training Epoch: 38 [28800/36045]\tLoss: 642.2535\n",
      "Training Epoch: 38 [28850/36045]\tLoss: 557.5430\n",
      "Training Epoch: 38 [28900/36045]\tLoss: 453.3435\n",
      "Training Epoch: 38 [28950/36045]\tLoss: 452.4067\n",
      "Training Epoch: 38 [29000/36045]\tLoss: 448.8015\n",
      "Training Epoch: 38 [29050/36045]\tLoss: 455.6632\n",
      "Training Epoch: 38 [29100/36045]\tLoss: 473.9583\n",
      "Training Epoch: 38 [29150/36045]\tLoss: 463.8150\n",
      "Training Epoch: 38 [29200/36045]\tLoss: 449.6331\n",
      "Training Epoch: 38 [29250/36045]\tLoss: 439.4647\n",
      "Training Epoch: 38 [29300/36045]\tLoss: 495.8681\n",
      "Training Epoch: 38 [29350/36045]\tLoss: 583.1240\n",
      "Training Epoch: 38 [29400/36045]\tLoss: 600.6187\n",
      "Training Epoch: 38 [29450/36045]\tLoss: 617.8362\n",
      "Training Epoch: 38 [29500/36045]\tLoss: 632.7465\n",
      "Training Epoch: 38 [29550/36045]\tLoss: 602.1479\n",
      "Training Epoch: 38 [29600/36045]\tLoss: 507.1313\n",
      "Training Epoch: 38 [29650/36045]\tLoss: 488.8373\n",
      "Training Epoch: 38 [29700/36045]\tLoss: 437.9721\n",
      "Training Epoch: 38 [29750/36045]\tLoss: 436.5406\n",
      "Training Epoch: 38 [29800/36045]\tLoss: 483.7577\n",
      "Training Epoch: 38 [29850/36045]\tLoss: 560.5426\n",
      "Training Epoch: 38 [29900/36045]\tLoss: 556.5289\n",
      "Training Epoch: 38 [29950/36045]\tLoss: 578.2363\n",
      "Training Epoch: 38 [30000/36045]\tLoss: 552.3720\n",
      "Training Epoch: 38 [30050/36045]\tLoss: 559.1917\n",
      "Training Epoch: 38 [30100/36045]\tLoss: 682.3182\n",
      "Training Epoch: 38 [30150/36045]\tLoss: 665.2182\n",
      "Training Epoch: 38 [30200/36045]\tLoss: 627.4131\n",
      "Training Epoch: 38 [30250/36045]\tLoss: 675.8162\n",
      "Training Epoch: 38 [30300/36045]\tLoss: 660.2898\n",
      "Training Epoch: 38 [30350/36045]\tLoss: 506.6445\n",
      "Training Epoch: 38 [30400/36045]\tLoss: 491.2894\n",
      "Training Epoch: 38 [30450/36045]\tLoss: 493.5042\n",
      "Training Epoch: 38 [30500/36045]\tLoss: 460.6444\n",
      "Training Epoch: 38 [30550/36045]\tLoss: 427.1578\n",
      "Training Epoch: 38 [30600/36045]\tLoss: 419.0027\n",
      "Training Epoch: 38 [30650/36045]\tLoss: 409.0671\n",
      "Training Epoch: 38 [30700/36045]\tLoss: 426.8927\n",
      "Training Epoch: 38 [30750/36045]\tLoss: 413.8513\n",
      "Training Epoch: 38 [30800/36045]\tLoss: 440.0807\n",
      "Training Epoch: 38 [30850/36045]\tLoss: 431.7503\n",
      "Training Epoch: 38 [30900/36045]\tLoss: 443.8859\n",
      "Training Epoch: 38 [30950/36045]\tLoss: 466.3979\n",
      "Training Epoch: 38 [31000/36045]\tLoss: 458.6344\n",
      "Training Epoch: 38 [31050/36045]\tLoss: 383.0893\n",
      "Training Epoch: 38 [31100/36045]\tLoss: 373.6648\n",
      "Training Epoch: 38 [31150/36045]\tLoss: 381.2201\n",
      "Training Epoch: 38 [31200/36045]\tLoss: 473.6740\n",
      "Training Epoch: 38 [31250/36045]\tLoss: 614.6176\n",
      "Training Epoch: 38 [31300/36045]\tLoss: 585.6510\n",
      "Training Epoch: 38 [31350/36045]\tLoss: 601.5223\n",
      "Training Epoch: 38 [31400/36045]\tLoss: 581.0342\n",
      "Training Epoch: 38 [31450/36045]\tLoss: 597.5747\n",
      "Training Epoch: 38 [31500/36045]\tLoss: 610.1326\n",
      "Training Epoch: 38 [31550/36045]\tLoss: 617.2951\n",
      "Training Epoch: 38 [31600/36045]\tLoss: 580.3127\n",
      "Training Epoch: 38 [31650/36045]\tLoss: 621.0532\n",
      "Training Epoch: 38 [31700/36045]\tLoss: 449.2491\n",
      "Training Epoch: 38 [31750/36045]\tLoss: 371.3661\n",
      "Training Epoch: 38 [31800/36045]\tLoss: 354.4769\n",
      "Training Epoch: 38 [31850/36045]\tLoss: 362.5391\n",
      "Training Epoch: 38 [31900/36045]\tLoss: 572.0630\n",
      "Training Epoch: 38 [31950/36045]\tLoss: 740.4386\n",
      "Training Epoch: 38 [32000/36045]\tLoss: 848.8637\n",
      "Training Epoch: 38 [32050/36045]\tLoss: 804.3088\n",
      "Training Epoch: 38 [32100/36045]\tLoss: 795.0620\n",
      "Training Epoch: 38 [32150/36045]\tLoss: 612.4354\n",
      "Training Epoch: 38 [32200/36045]\tLoss: 615.0001\n",
      "Training Epoch: 38 [32250/36045]\tLoss: 625.3698\n",
      "Training Epoch: 38 [32300/36045]\tLoss: 607.3101\n",
      "Training Epoch: 38 [32350/36045]\tLoss: 603.1190\n",
      "Training Epoch: 38 [32400/36045]\tLoss: 565.9095\n",
      "Training Epoch: 38 [32450/36045]\tLoss: 465.9543\n",
      "Training Epoch: 38 [32500/36045]\tLoss: 447.7373\n",
      "Training Epoch: 38 [32550/36045]\tLoss: 449.9397\n",
      "Training Epoch: 38 [32600/36045]\tLoss: 446.9381\n",
      "Training Epoch: 38 [32650/36045]\tLoss: 577.6593\n",
      "Training Epoch: 38 [32700/36045]\tLoss: 630.7288\n",
      "Training Epoch: 38 [32750/36045]\tLoss: 600.7690\n",
      "Training Epoch: 38 [32800/36045]\tLoss: 616.0703\n",
      "Training Epoch: 38 [32850/36045]\tLoss: 568.7342\n",
      "Training Epoch: 38 [32900/36045]\tLoss: 455.1984\n",
      "Training Epoch: 38 [32950/36045]\tLoss: 476.7542\n",
      "Training Epoch: 38 [33000/36045]\tLoss: 475.6878\n",
      "Training Epoch: 38 [33050/36045]\tLoss: 452.2482\n",
      "Training Epoch: 38 [33100/36045]\tLoss: 513.9326\n",
      "Training Epoch: 38 [33150/36045]\tLoss: 698.8415\n",
      "Training Epoch: 38 [33200/36045]\tLoss: 680.4783\n",
      "Training Epoch: 38 [33250/36045]\tLoss: 701.2659\n",
      "Training Epoch: 38 [33300/36045]\tLoss: 747.1388\n",
      "Training Epoch: 38 [33350/36045]\tLoss: 572.3973\n",
      "Training Epoch: 38 [33400/36045]\tLoss: 417.7668\n",
      "Training Epoch: 38 [33450/36045]\tLoss: 413.3982\n",
      "Training Epoch: 38 [33500/36045]\tLoss: 425.6022\n",
      "Training Epoch: 38 [33550/36045]\tLoss: 441.0008\n",
      "Training Epoch: 38 [33600/36045]\tLoss: 442.8072\n",
      "Training Epoch: 38 [33650/36045]\tLoss: 591.2558\n",
      "Training Epoch: 38 [33700/36045]\tLoss: 571.9108\n",
      "Training Epoch: 38 [33750/36045]\tLoss: 592.2258\n",
      "Training Epoch: 38 [33800/36045]\tLoss: 589.0396\n",
      "Training Epoch: 38 [33850/36045]\tLoss: 591.1016\n",
      "Training Epoch: 38 [33900/36045]\tLoss: 604.4327\n",
      "Training Epoch: 38 [33950/36045]\tLoss: 614.4031\n",
      "Training Epoch: 38 [34000/36045]\tLoss: 600.8831\n",
      "Training Epoch: 38 [34050/36045]\tLoss: 604.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 38 [34100/36045]\tLoss: 582.8873\n",
      "Training Epoch: 38 [34150/36045]\tLoss: 540.4689\n",
      "Training Epoch: 38 [34200/36045]\tLoss: 511.9156\n",
      "Training Epoch: 38 [34250/36045]\tLoss: 526.0919\n",
      "Training Epoch: 38 [34300/36045]\tLoss: 449.2242\n",
      "Training Epoch: 38 [34350/36045]\tLoss: 473.1540\n",
      "Training Epoch: 38 [34400/36045]\tLoss: 465.6292\n",
      "Training Epoch: 38 [34450/36045]\tLoss: 438.2418\n",
      "Training Epoch: 38 [34500/36045]\tLoss: 467.5687\n",
      "Training Epoch: 38 [34550/36045]\tLoss: 458.9314\n",
      "Training Epoch: 38 [34600/36045]\tLoss: 464.9280\n",
      "Training Epoch: 38 [34650/36045]\tLoss: 571.1475\n",
      "Training Epoch: 38 [34700/36045]\tLoss: 605.5167\n",
      "Training Epoch: 38 [34750/36045]\tLoss: 536.7566\n",
      "Training Epoch: 38 [34800/36045]\tLoss: 615.9130\n",
      "Training Epoch: 38 [34850/36045]\tLoss: 623.3392\n",
      "Training Epoch: 38 [34900/36045]\tLoss: 675.6299\n",
      "Training Epoch: 38 [34950/36045]\tLoss: 660.7648\n",
      "Training Epoch: 38 [35000/36045]\tLoss: 661.7142\n",
      "Training Epoch: 38 [35050/36045]\tLoss: 648.6364\n",
      "Training Epoch: 38 [35100/36045]\tLoss: 556.9899\n",
      "Training Epoch: 38 [35150/36045]\tLoss: 549.5471\n",
      "Training Epoch: 38 [35200/36045]\tLoss: 462.9728\n",
      "Training Epoch: 38 [35250/36045]\tLoss: 508.4575\n",
      "Training Epoch: 38 [35300/36045]\tLoss: 523.9146\n",
      "Training Epoch: 38 [35350/36045]\tLoss: 588.1151\n",
      "Training Epoch: 38 [35400/36045]\tLoss: 619.6151\n",
      "Training Epoch: 38 [35450/36045]\tLoss: 590.6103\n",
      "Training Epoch: 38 [35500/36045]\tLoss: 571.6187\n",
      "Training Epoch: 38 [35550/36045]\tLoss: 557.4731\n",
      "Training Epoch: 38 [35600/36045]\tLoss: 607.2535\n",
      "Training Epoch: 38 [35650/36045]\tLoss: 680.2473\n",
      "Training Epoch: 38 [35700/36045]\tLoss: 607.4337\n",
      "Training Epoch: 38 [35750/36045]\tLoss: 664.7075\n",
      "Training Epoch: 38 [35800/36045]\tLoss: 671.1444\n",
      "Training Epoch: 38 [35850/36045]\tLoss: 645.2122\n",
      "Training Epoch: 38 [35900/36045]\tLoss: 668.7097\n",
      "Training Epoch: 38 [35950/36045]\tLoss: 665.9147\n",
      "Training Epoch: 38 [36000/36045]\tLoss: 658.6656\n",
      "Training Epoch: 38 [36045/36045]\tLoss: 643.3453\n",
      "Training Epoch: 38 [4004/4004]\tLoss: 597.5759\n",
      "Training Epoch: 39 [50/36045]\tLoss: 596.3837\n",
      "Training Epoch: 39 [100/36045]\tLoss: 571.8951\n",
      "Training Epoch: 39 [150/36045]\tLoss: 570.1735\n",
      "Training Epoch: 39 [200/36045]\tLoss: 556.5367\n",
      "Training Epoch: 39 [250/36045]\tLoss: 667.4419\n",
      "Training Epoch: 39 [300/36045]\tLoss: 732.0080\n",
      "Training Epoch: 39 [350/36045]\tLoss: 698.5366\n",
      "Training Epoch: 39 [400/36045]\tLoss: 693.4816\n",
      "Training Epoch: 39 [450/36045]\tLoss: 674.2222\n",
      "Training Epoch: 39 [500/36045]\tLoss: 625.0048\n",
      "Training Epoch: 39 [550/36045]\tLoss: 627.8265\n",
      "Training Epoch: 39 [600/36045]\tLoss: 612.3810\n",
      "Training Epoch: 39 [650/36045]\tLoss: 634.3433\n",
      "Training Epoch: 39 [700/36045]\tLoss: 620.3510\n",
      "Training Epoch: 39 [750/36045]\tLoss: 598.3324\n",
      "Training Epoch: 39 [800/36045]\tLoss: 610.3806\n",
      "Training Epoch: 39 [850/36045]\tLoss: 592.5251\n",
      "Training Epoch: 39 [900/36045]\tLoss: 566.7220\n",
      "Training Epoch: 39 [950/36045]\tLoss: 536.5968\n",
      "Training Epoch: 39 [1000/36045]\tLoss: 519.7689\n",
      "Training Epoch: 39 [1050/36045]\tLoss: 521.7660\n",
      "Training Epoch: 39 [1100/36045]\tLoss: 507.4561\n",
      "Training Epoch: 39 [1150/36045]\tLoss: 516.3231\n",
      "Training Epoch: 39 [1200/36045]\tLoss: 546.2337\n",
      "Training Epoch: 39 [1250/36045]\tLoss: 625.4523\n",
      "Training Epoch: 39 [1300/36045]\tLoss: 632.5409\n",
      "Training Epoch: 39 [1350/36045]\tLoss: 634.0021\n",
      "Training Epoch: 39 [1400/36045]\tLoss: 658.3551\n",
      "Training Epoch: 39 [1450/36045]\tLoss: 636.7405\n",
      "Training Epoch: 39 [1500/36045]\tLoss: 582.4380\n",
      "Training Epoch: 39 [1550/36045]\tLoss: 597.5743\n",
      "Training Epoch: 39 [1600/36045]\tLoss: 608.0870\n",
      "Training Epoch: 39 [1650/36045]\tLoss: 595.2631\n",
      "Training Epoch: 39 [1700/36045]\tLoss: 607.3614\n",
      "Training Epoch: 39 [1750/36045]\tLoss: 648.6627\n",
      "Training Epoch: 39 [1800/36045]\tLoss: 630.7468\n",
      "Training Epoch: 39 [1850/36045]\tLoss: 647.0622\n",
      "Training Epoch: 39 [1900/36045]\tLoss: 605.5760\n",
      "Training Epoch: 39 [1950/36045]\tLoss: 615.9037\n",
      "Training Epoch: 39 [2000/36045]\tLoss: 555.4852\n",
      "Training Epoch: 39 [2050/36045]\tLoss: 557.7291\n",
      "Training Epoch: 39 [2100/36045]\tLoss: 587.6312\n",
      "Training Epoch: 39 [2150/36045]\tLoss: 568.2982\n",
      "Training Epoch: 39 [2200/36045]\tLoss: 529.4514\n",
      "Training Epoch: 39 [2250/36045]\tLoss: 499.9532\n",
      "Training Epoch: 39 [2300/36045]\tLoss: 524.0933\n",
      "Training Epoch: 39 [2350/36045]\tLoss: 500.7925\n",
      "Training Epoch: 39 [2400/36045]\tLoss: 508.6058\n",
      "Training Epoch: 39 [2450/36045]\tLoss: 650.7587\n",
      "Training Epoch: 39 [2500/36045]\tLoss: 683.7249\n",
      "Training Epoch: 39 [2550/36045]\tLoss: 681.4185\n",
      "Training Epoch: 39 [2600/36045]\tLoss: 689.8900\n",
      "Training Epoch: 39 [2650/36045]\tLoss: 814.3788\n",
      "Training Epoch: 39 [2700/36045]\tLoss: 901.9299\n",
      "Training Epoch: 39 [2750/36045]\tLoss: 973.1796\n",
      "Training Epoch: 39 [2800/36045]\tLoss: 982.9141\n",
      "Training Epoch: 39 [2850/36045]\tLoss: 750.8816\n",
      "Training Epoch: 39 [2900/36045]\tLoss: 712.7729\n",
      "Training Epoch: 39 [2950/36045]\tLoss: 688.7633\n",
      "Training Epoch: 39 [3000/36045]\tLoss: 682.7534\n",
      "Training Epoch: 39 [3050/36045]\tLoss: 713.7446\n",
      "Training Epoch: 39 [3100/36045]\tLoss: 652.8607\n",
      "Training Epoch: 39 [3150/36045]\tLoss: 502.9273\n",
      "Training Epoch: 39 [3200/36045]\tLoss: 520.7114\n",
      "Training Epoch: 39 [3250/36045]\tLoss: 490.6534\n",
      "Training Epoch: 39 [3300/36045]\tLoss: 464.4006\n",
      "Training Epoch: 39 [3350/36045]\tLoss: 490.5910\n",
      "Training Epoch: 39 [3400/36045]\tLoss: 514.4703\n",
      "Training Epoch: 39 [3450/36045]\tLoss: 552.2574\n",
      "Training Epoch: 39 [3500/36045]\tLoss: 539.3687\n",
      "Training Epoch: 39 [3550/36045]\tLoss: 515.9141\n",
      "Training Epoch: 39 [3600/36045]\tLoss: 553.7913\n",
      "Training Epoch: 39 [3650/36045]\tLoss: 640.5421\n",
      "Training Epoch: 39 [3700/36045]\tLoss: 648.3032\n",
      "Training Epoch: 39 [3750/36045]\tLoss: 617.4787\n",
      "Training Epoch: 39 [3800/36045]\tLoss: 613.6570\n",
      "Training Epoch: 39 [3850/36045]\tLoss: 614.1530\n",
      "Training Epoch: 39 [3900/36045]\tLoss: 619.0002\n",
      "Training Epoch: 39 [3950/36045]\tLoss: 597.4339\n",
      "Training Epoch: 39 [4000/36045]\tLoss: 602.5131\n",
      "Training Epoch: 39 [4050/36045]\tLoss: 552.8792\n",
      "Training Epoch: 39 [4100/36045]\tLoss: 539.0063\n",
      "Training Epoch: 39 [4150/36045]\tLoss: 553.6232\n",
      "Training Epoch: 39 [4200/36045]\tLoss: 548.5450\n",
      "Training Epoch: 39 [4250/36045]\tLoss: 551.0126\n",
      "Training Epoch: 39 [4300/36045]\tLoss: 567.5790\n",
      "Training Epoch: 39 [4350/36045]\tLoss: 550.8254\n",
      "Training Epoch: 39 [4400/36045]\tLoss: 526.7858\n",
      "Training Epoch: 39 [4450/36045]\tLoss: 577.8655\n",
      "Training Epoch: 39 [4500/36045]\tLoss: 621.0552\n",
      "Training Epoch: 39 [4550/36045]\tLoss: 624.6080\n",
      "Training Epoch: 39 [4600/36045]\tLoss: 646.8663\n",
      "Training Epoch: 39 [4650/36045]\tLoss: 636.4012\n",
      "Training Epoch: 39 [4700/36045]\tLoss: 586.8807\n",
      "Training Epoch: 39 [4750/36045]\tLoss: 569.1519\n",
      "Training Epoch: 39 [4800/36045]\tLoss: 594.0333\n",
      "Training Epoch: 39 [4850/36045]\tLoss: 580.6266\n",
      "Training Epoch: 39 [4900/36045]\tLoss: 565.0530\n",
      "Training Epoch: 39 [4950/36045]\tLoss: 580.4858\n",
      "Training Epoch: 39 [5000/36045]\tLoss: 610.0905\n",
      "Training Epoch: 39 [5050/36045]\tLoss: 591.1290\n",
      "Training Epoch: 39 [5100/36045]\tLoss: 601.5322\n",
      "Training Epoch: 39 [5150/36045]\tLoss: 585.8869\n",
      "Training Epoch: 39 [5200/36045]\tLoss: 583.7848\n",
      "Training Epoch: 39 [5250/36045]\tLoss: 577.3480\n",
      "Training Epoch: 39 [5300/36045]\tLoss: 577.5964\n",
      "Training Epoch: 39 [5350/36045]\tLoss: 599.4098\n",
      "Training Epoch: 39 [5400/36045]\tLoss: 577.6201\n",
      "Training Epoch: 39 [5450/36045]\tLoss: 547.5256\n",
      "Training Epoch: 39 [5500/36045]\tLoss: 575.7681\n",
      "Training Epoch: 39 [5550/36045]\tLoss: 563.9722\n",
      "Training Epoch: 39 [5600/36045]\tLoss: 643.9147\n",
      "Training Epoch: 39 [5650/36045]\tLoss: 609.0927\n",
      "Training Epoch: 39 [5700/36045]\tLoss: 571.3529\n",
      "Training Epoch: 39 [5750/36045]\tLoss: 555.5093\n",
      "Training Epoch: 39 [5800/36045]\tLoss: 586.0206\n",
      "Training Epoch: 39 [5850/36045]\tLoss: 574.8398\n",
      "Training Epoch: 39 [5900/36045]\tLoss: 660.7889\n",
      "Training Epoch: 39 [5950/36045]\tLoss: 677.5358\n",
      "Training Epoch: 39 [6000/36045]\tLoss: 663.2925\n",
      "Training Epoch: 39 [6050/36045]\tLoss: 641.1517\n",
      "Training Epoch: 39 [6100/36045]\tLoss: 645.3513\n",
      "Training Epoch: 39 [6150/36045]\tLoss: 634.7736\n",
      "Training Epoch: 39 [6200/36045]\tLoss: 638.5455\n",
      "Training Epoch: 39 [6250/36045]\tLoss: 660.0741\n",
      "Training Epoch: 39 [6300/36045]\tLoss: 671.7363\n",
      "Training Epoch: 39 [6350/36045]\tLoss: 717.6186\n",
      "Training Epoch: 39 [6400/36045]\tLoss: 592.6829\n",
      "Training Epoch: 39 [6450/36045]\tLoss: 545.7310\n",
      "Training Epoch: 39 [6500/36045]\tLoss: 555.6962\n",
      "Training Epoch: 39 [6550/36045]\tLoss: 573.0695\n",
      "Training Epoch: 39 [6600/36045]\tLoss: 571.4108\n",
      "Training Epoch: 39 [6650/36045]\tLoss: 644.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 39 [6700/36045]\tLoss: 674.5053\n",
      "Training Epoch: 39 [6750/36045]\tLoss: 651.4827\n",
      "Training Epoch: 39 [6800/36045]\tLoss: 654.4037\n",
      "Training Epoch: 39 [6850/36045]\tLoss: 642.4339\n",
      "Training Epoch: 39 [6900/36045]\tLoss: 572.3792\n",
      "Training Epoch: 39 [6950/36045]\tLoss: 539.6334\n",
      "Training Epoch: 39 [7000/36045]\tLoss: 573.9985\n",
      "Training Epoch: 39 [7050/36045]\tLoss: 586.5591\n",
      "Training Epoch: 39 [7100/36045]\tLoss: 585.9800\n",
      "Training Epoch: 39 [7150/36045]\tLoss: 596.2994\n",
      "Training Epoch: 39 [7200/36045]\tLoss: 598.9104\n",
      "Training Epoch: 39 [7250/36045]\tLoss: 596.9893\n",
      "Training Epoch: 39 [7300/36045]\tLoss: 583.4560\n",
      "Training Epoch: 39 [7350/36045]\tLoss: 580.8227\n",
      "Training Epoch: 39 [7400/36045]\tLoss: 528.8238\n",
      "Training Epoch: 39 [7450/36045]\tLoss: 532.0627\n",
      "Training Epoch: 39 [7500/36045]\tLoss: 527.4329\n",
      "Training Epoch: 39 [7550/36045]\tLoss: 505.1999\n",
      "Training Epoch: 39 [7600/36045]\tLoss: 560.1683\n",
      "Training Epoch: 39 [7650/36045]\tLoss: 599.0870\n",
      "Training Epoch: 39 [7700/36045]\tLoss: 570.1956\n",
      "Training Epoch: 39 [7750/36045]\tLoss: 584.7015\n",
      "Training Epoch: 39 [7800/36045]\tLoss: 574.0203\n",
      "Training Epoch: 39 [7850/36045]\tLoss: 555.5959\n",
      "Training Epoch: 39 [7900/36045]\tLoss: 586.0049\n",
      "Training Epoch: 39 [7950/36045]\tLoss: 583.4703\n",
      "Training Epoch: 39 [8000/36045]\tLoss: 601.6047\n",
      "Training Epoch: 39 [8050/36045]\tLoss: 566.8619\n",
      "Training Epoch: 39 [8100/36045]\tLoss: 592.0665\n",
      "Training Epoch: 39 [8150/36045]\tLoss: 671.0677\n",
      "Training Epoch: 39 [8200/36045]\tLoss: 658.2561\n",
      "Training Epoch: 39 [8250/36045]\tLoss: 626.2786\n",
      "Training Epoch: 39 [8300/36045]\tLoss: 683.9617\n",
      "Training Epoch: 39 [8350/36045]\tLoss: 627.3545\n",
      "Training Epoch: 39 [8400/36045]\tLoss: 561.5463\n",
      "Training Epoch: 39 [8450/36045]\tLoss: 525.6367\n",
      "Training Epoch: 39 [8500/36045]\tLoss: 559.1168\n",
      "Training Epoch: 39 [8550/36045]\tLoss: 552.0880\n",
      "Training Epoch: 39 [8600/36045]\tLoss: 546.2971\n",
      "Training Epoch: 39 [8650/36045]\tLoss: 580.7725\n",
      "Training Epoch: 39 [8700/36045]\tLoss: 614.3011\n",
      "Training Epoch: 39 [8750/36045]\tLoss: 603.4106\n",
      "Training Epoch: 39 [8800/36045]\tLoss: 609.1921\n",
      "Training Epoch: 39 [8850/36045]\tLoss: 602.6859\n",
      "Training Epoch: 39 [8900/36045]\tLoss: 544.0992\n",
      "Training Epoch: 39 [8950/36045]\tLoss: 555.2788\n",
      "Training Epoch: 39 [9000/36045]\tLoss: 571.0317\n",
      "Training Epoch: 39 [9050/36045]\tLoss: 572.5745\n",
      "Training Epoch: 39 [9100/36045]\tLoss: 589.5197\n",
      "Training Epoch: 39 [9150/36045]\tLoss: 436.2468\n",
      "Training Epoch: 39 [9200/36045]\tLoss: 326.1954\n",
      "Training Epoch: 39 [9250/36045]\tLoss: 353.9062\n",
      "Training Epoch: 39 [9300/36045]\tLoss: 363.8772\n",
      "Training Epoch: 39 [9350/36045]\tLoss: 335.7245\n",
      "Training Epoch: 39 [9400/36045]\tLoss: 657.8165\n",
      "Training Epoch: 39 [9450/36045]\tLoss: 698.6483\n",
      "Training Epoch: 39 [9500/36045]\tLoss: 685.9880\n",
      "Training Epoch: 39 [9550/36045]\tLoss: 725.7196\n",
      "Training Epoch: 39 [9600/36045]\tLoss: 540.2048\n",
      "Training Epoch: 39 [9650/36045]\tLoss: 545.1859\n",
      "Training Epoch: 39 [9700/36045]\tLoss: 530.6655\n",
      "Training Epoch: 39 [9750/36045]\tLoss: 529.3499\n",
      "Training Epoch: 39 [9800/36045]\tLoss: 692.3679\n",
      "Training Epoch: 39 [9850/36045]\tLoss: 731.3593\n",
      "Training Epoch: 39 [9900/36045]\tLoss: 741.6942\n",
      "Training Epoch: 39 [9950/36045]\tLoss: 722.6872\n",
      "Training Epoch: 39 [10000/36045]\tLoss: 668.3684\n",
      "Training Epoch: 39 [10050/36045]\tLoss: 547.9030\n",
      "Training Epoch: 39 [10100/36045]\tLoss: 555.5982\n",
      "Training Epoch: 39 [10150/36045]\tLoss: 564.2566\n",
      "Training Epoch: 39 [10200/36045]\tLoss: 553.1604\n",
      "Training Epoch: 39 [10250/36045]\tLoss: 663.6902\n",
      "Training Epoch: 39 [10300/36045]\tLoss: 644.8878\n",
      "Training Epoch: 39 [10350/36045]\tLoss: 679.3269\n",
      "Training Epoch: 39 [10400/36045]\tLoss: 669.6401\n",
      "Training Epoch: 39 [10450/36045]\tLoss: 627.3887\n",
      "Training Epoch: 39 [10500/36045]\tLoss: 524.0608\n",
      "Training Epoch: 39 [10550/36045]\tLoss: 518.8199\n",
      "Training Epoch: 39 [10600/36045]\tLoss: 541.0723\n",
      "Training Epoch: 39 [10650/36045]\tLoss: 547.1586\n",
      "Training Epoch: 39 [10700/36045]\tLoss: 628.3403\n",
      "Training Epoch: 39 [10750/36045]\tLoss: 687.9472\n",
      "Training Epoch: 39 [10800/36045]\tLoss: 633.2885\n",
      "Training Epoch: 39 [10850/36045]\tLoss: 671.3615\n",
      "Training Epoch: 39 [10900/36045]\tLoss: 698.8804\n",
      "Training Epoch: 39 [10950/36045]\tLoss: 514.7939\n",
      "Training Epoch: 39 [11000/36045]\tLoss: 508.4275\n",
      "Training Epoch: 39 [11050/36045]\tLoss: 545.1326\n",
      "Training Epoch: 39 [11100/36045]\tLoss: 555.5568\n",
      "Training Epoch: 39 [11150/36045]\tLoss: 602.6075\n",
      "Training Epoch: 39 [11200/36045]\tLoss: 630.7030\n",
      "Training Epoch: 39 [11250/36045]\tLoss: 642.3163\n",
      "Training Epoch: 39 [11300/36045]\tLoss: 622.5298\n",
      "Training Epoch: 39 [11350/36045]\tLoss: 620.1144\n",
      "Training Epoch: 39 [11400/36045]\tLoss: 583.1432\n",
      "Training Epoch: 39 [11450/36045]\tLoss: 551.9672\n",
      "Training Epoch: 39 [11500/36045]\tLoss: 549.3386\n",
      "Training Epoch: 39 [11550/36045]\tLoss: 559.4357\n",
      "Training Epoch: 39 [11600/36045]\tLoss: 619.8679\n",
      "Training Epoch: 39 [11650/36045]\tLoss: 671.5426\n",
      "Training Epoch: 39 [11700/36045]\tLoss: 670.5105\n",
      "Training Epoch: 39 [11750/36045]\tLoss: 688.8289\n",
      "Training Epoch: 39 [11800/36045]\tLoss: 731.4280\n",
      "Training Epoch: 39 [11850/36045]\tLoss: 789.6052\n",
      "Training Epoch: 39 [11900/36045]\tLoss: 1002.5565\n",
      "Training Epoch: 39 [11950/36045]\tLoss: 1005.4449\n",
      "Training Epoch: 39 [12000/36045]\tLoss: 1017.3315\n",
      "Training Epoch: 39 [12050/36045]\tLoss: 976.6082\n",
      "Training Epoch: 39 [12100/36045]\tLoss: 627.6077\n",
      "Training Epoch: 39 [12150/36045]\tLoss: 474.2301\n",
      "Training Epoch: 39 [12200/36045]\tLoss: 468.9687\n",
      "Training Epoch: 39 [12250/36045]\tLoss: 477.7051\n",
      "Training Epoch: 39 [12300/36045]\tLoss: 614.4792\n",
      "Training Epoch: 39 [12350/36045]\tLoss: 670.2519\n",
      "Training Epoch: 39 [12400/36045]\tLoss: 677.6473\n",
      "Training Epoch: 39 [12450/36045]\tLoss: 666.4053\n",
      "Training Epoch: 39 [12500/36045]\tLoss: 693.6435\n",
      "Training Epoch: 39 [12550/36045]\tLoss: 663.3809\n",
      "Training Epoch: 39 [12600/36045]\tLoss: 607.8058\n",
      "Training Epoch: 39 [12650/36045]\tLoss: 606.1997\n",
      "Training Epoch: 39 [12700/36045]\tLoss: 627.5483\n",
      "Training Epoch: 39 [12750/36045]\tLoss: 626.2238\n",
      "Training Epoch: 39 [12800/36045]\tLoss: 611.5688\n",
      "Training Epoch: 39 [12850/36045]\tLoss: 640.7583\n",
      "Training Epoch: 39 [12900/36045]\tLoss: 614.3807\n",
      "Training Epoch: 39 [12950/36045]\tLoss: 600.5108\n",
      "Training Epoch: 39 [13000/36045]\tLoss: 633.5320\n",
      "Training Epoch: 39 [13050/36045]\tLoss: 573.4519\n",
      "Training Epoch: 39 [13100/36045]\tLoss: 589.6728\n",
      "Training Epoch: 39 [13150/36045]\tLoss: 581.3342\n",
      "Training Epoch: 39 [13200/36045]\tLoss: 563.8387\n",
      "Training Epoch: 39 [13250/36045]\tLoss: 586.1891\n",
      "Training Epoch: 39 [13300/36045]\tLoss: 623.6630\n",
      "Training Epoch: 39 [13350/36045]\tLoss: 604.3308\n",
      "Training Epoch: 39 [13400/36045]\tLoss: 607.5440\n",
      "Training Epoch: 39 [13450/36045]\tLoss: 604.7570\n",
      "Training Epoch: 39 [13500/36045]\tLoss: 623.9401\n",
      "Training Epoch: 39 [13550/36045]\tLoss: 761.4478\n",
      "Training Epoch: 39 [13600/36045]\tLoss: 794.1638\n",
      "Training Epoch: 39 [13650/36045]\tLoss: 875.4670\n",
      "Training Epoch: 39 [13700/36045]\tLoss: 771.8504\n",
      "Training Epoch: 39 [13750/36045]\tLoss: 610.9986\n",
      "Training Epoch: 39 [13800/36045]\tLoss: 582.3359\n",
      "Training Epoch: 39 [13850/36045]\tLoss: 564.9664\n",
      "Training Epoch: 39 [13900/36045]\tLoss: 572.1974\n",
      "Training Epoch: 39 [13950/36045]\tLoss: 617.4364\n",
      "Training Epoch: 39 [14000/36045]\tLoss: 650.5240\n",
      "Training Epoch: 39 [14050/36045]\tLoss: 625.4419\n",
      "Training Epoch: 39 [14100/36045]\tLoss: 620.6025\n",
      "Training Epoch: 39 [14150/36045]\tLoss: 608.5620\n",
      "Training Epoch: 39 [14200/36045]\tLoss: 649.3571\n",
      "Training Epoch: 39 [14250/36045]\tLoss: 713.4135\n",
      "Training Epoch: 39 [14300/36045]\tLoss: 716.7702\n",
      "Training Epoch: 39 [14350/36045]\tLoss: 685.2388\n",
      "Training Epoch: 39 [14400/36045]\tLoss: 671.1150\n",
      "Training Epoch: 39 [14450/36045]\tLoss: 706.7719\n",
      "Training Epoch: 39 [14500/36045]\tLoss: 637.9944\n",
      "Training Epoch: 39 [14550/36045]\tLoss: 666.6996\n",
      "Training Epoch: 39 [14600/36045]\tLoss: 653.1387\n",
      "Training Epoch: 39 [14650/36045]\tLoss: 652.7291\n",
      "Training Epoch: 39 [14700/36045]\tLoss: 618.4426\n",
      "Training Epoch: 39 [14750/36045]\tLoss: 531.8873\n",
      "Training Epoch: 39 [14800/36045]\tLoss: 522.0411\n",
      "Training Epoch: 39 [14850/36045]\tLoss: 529.0682\n",
      "Training Epoch: 39 [14900/36045]\tLoss: 522.5804\n",
      "Training Epoch: 39 [14950/36045]\tLoss: 530.5415\n",
      "Training Epoch: 39 [15000/36045]\tLoss: 543.5484\n",
      "Training Epoch: 39 [15050/36045]\tLoss: 540.5132\n",
      "Training Epoch: 39 [15100/36045]\tLoss: 524.5886\n",
      "Training Epoch: 39 [15150/36045]\tLoss: 519.9045\n",
      "Training Epoch: 39 [15200/36045]\tLoss: 481.5462\n",
      "Training Epoch: 39 [15250/36045]\tLoss: 503.4868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 39 [15300/36045]\tLoss: 488.9507\n",
      "Training Epoch: 39 [15350/36045]\tLoss: 500.5990\n",
      "Training Epoch: 39 [15400/36045]\tLoss: 483.6861\n",
      "Training Epoch: 39 [15450/36045]\tLoss: 468.8886\n",
      "Training Epoch: 39 [15500/36045]\tLoss: 482.2963\n",
      "Training Epoch: 39 [15550/36045]\tLoss: 478.6428\n",
      "Training Epoch: 39 [15600/36045]\tLoss: 546.4088\n",
      "Training Epoch: 39 [15650/36045]\tLoss: 563.5405\n",
      "Training Epoch: 39 [15700/36045]\tLoss: 555.5863\n",
      "Training Epoch: 39 [15750/36045]\tLoss: 546.9747\n",
      "Training Epoch: 39 [15800/36045]\tLoss: 522.2762\n",
      "Training Epoch: 39 [15850/36045]\tLoss: 537.2298\n",
      "Training Epoch: 39 [15900/36045]\tLoss: 546.3519\n",
      "Training Epoch: 39 [15950/36045]\tLoss: 566.1670\n",
      "Training Epoch: 39 [16000/36045]\tLoss: 537.6448\n",
      "Training Epoch: 39 [16050/36045]\tLoss: 507.0929\n",
      "Training Epoch: 39 [16100/36045]\tLoss: 470.4403\n",
      "Training Epoch: 39 [16150/36045]\tLoss: 458.4144\n",
      "Training Epoch: 39 [16200/36045]\tLoss: 556.4287\n",
      "Training Epoch: 39 [16250/36045]\tLoss: 584.2171\n",
      "Training Epoch: 39 [16300/36045]\tLoss: 637.7888\n",
      "Training Epoch: 39 [16350/36045]\tLoss: 657.7432\n",
      "Training Epoch: 39 [16400/36045]\tLoss: 629.4784\n",
      "Training Epoch: 39 [16450/36045]\tLoss: 611.6916\n",
      "Training Epoch: 39 [16500/36045]\tLoss: 611.6088\n",
      "Training Epoch: 39 [16550/36045]\tLoss: 576.8097\n",
      "Training Epoch: 39 [16600/36045]\tLoss: 599.2988\n",
      "Training Epoch: 39 [16650/36045]\tLoss: 615.7356\n",
      "Training Epoch: 39 [16700/36045]\tLoss: 595.4406\n",
      "Training Epoch: 39 [16750/36045]\tLoss: 588.1044\n",
      "Training Epoch: 39 [16800/36045]\tLoss: 597.2540\n",
      "Training Epoch: 39 [16850/36045]\tLoss: 569.0983\n",
      "Training Epoch: 39 [16900/36045]\tLoss: 578.9006\n",
      "Training Epoch: 39 [16950/36045]\tLoss: 601.9470\n",
      "Training Epoch: 39 [17000/36045]\tLoss: 586.0520\n",
      "Training Epoch: 39 [17050/36045]\tLoss: 610.5521\n",
      "Training Epoch: 39 [17100/36045]\tLoss: 606.5865\n",
      "Training Epoch: 39 [17150/36045]\tLoss: 526.8132\n",
      "Training Epoch: 39 [17200/36045]\tLoss: 488.9448\n",
      "Training Epoch: 39 [17250/36045]\tLoss: 511.9734\n",
      "Training Epoch: 39 [17300/36045]\tLoss: 541.8277\n",
      "Training Epoch: 39 [17350/36045]\tLoss: 522.1591\n",
      "Training Epoch: 39 [17400/36045]\tLoss: 541.6401\n",
      "Training Epoch: 39 [17450/36045]\tLoss: 560.3606\n",
      "Training Epoch: 39 [17500/36045]\tLoss: 548.6586\n",
      "Training Epoch: 39 [17550/36045]\tLoss: 547.3389\n",
      "Training Epoch: 39 [17600/36045]\tLoss: 541.6180\n",
      "Training Epoch: 39 [17650/36045]\tLoss: 557.3659\n",
      "Training Epoch: 39 [17700/36045]\tLoss: 536.6545\n",
      "Training Epoch: 39 [17750/36045]\tLoss: 552.7780\n",
      "Training Epoch: 39 [17800/36045]\tLoss: 543.8149\n",
      "Training Epoch: 39 [17850/36045]\tLoss: 558.8043\n",
      "Training Epoch: 39 [17900/36045]\tLoss: 586.9294\n",
      "Training Epoch: 39 [17950/36045]\tLoss: 599.0988\n",
      "Training Epoch: 39 [18000/36045]\tLoss: 589.6236\n",
      "Training Epoch: 39 [18050/36045]\tLoss: 648.2445\n",
      "Training Epoch: 39 [18100/36045]\tLoss: 649.6506\n",
      "Training Epoch: 39 [18150/36045]\tLoss: 660.9552\n",
      "Training Epoch: 39 [18200/36045]\tLoss: 643.1151\n",
      "Training Epoch: 39 [18250/36045]\tLoss: 663.9063\n",
      "Training Epoch: 39 [18300/36045]\tLoss: 618.4195\n",
      "Training Epoch: 39 [18350/36045]\tLoss: 691.0258\n",
      "Training Epoch: 39 [18400/36045]\tLoss: 664.0651\n",
      "Training Epoch: 39 [18450/36045]\tLoss: 643.8402\n",
      "Training Epoch: 39 [18500/36045]\tLoss: 642.7493\n",
      "Training Epoch: 39 [18550/36045]\tLoss: 630.1208\n",
      "Training Epoch: 39 [18600/36045]\tLoss: 619.7180\n",
      "Training Epoch: 39 [18650/36045]\tLoss: 665.5195\n",
      "Training Epoch: 39 [18700/36045]\tLoss: 700.1404\n",
      "Training Epoch: 39 [18750/36045]\tLoss: 687.5300\n",
      "Training Epoch: 39 [18800/36045]\tLoss: 710.4365\n",
      "Training Epoch: 39 [18850/36045]\tLoss: 655.4254\n",
      "Training Epoch: 39 [18900/36045]\tLoss: 701.0384\n",
      "Training Epoch: 39 [18950/36045]\tLoss: 643.4466\n",
      "Training Epoch: 39 [19000/36045]\tLoss: 530.8612\n",
      "Training Epoch: 39 [19050/36045]\tLoss: 515.1002\n",
      "Training Epoch: 39 [19100/36045]\tLoss: 523.3095\n",
      "Training Epoch: 39 [19150/36045]\tLoss: 513.3860\n",
      "Training Epoch: 39 [19200/36045]\tLoss: 543.8245\n",
      "Training Epoch: 39 [19250/36045]\tLoss: 558.8856\n",
      "Training Epoch: 39 [19300/36045]\tLoss: 568.3797\n",
      "Training Epoch: 39 [19350/36045]\tLoss: 552.0939\n",
      "Training Epoch: 39 [19400/36045]\tLoss: 572.6777\n",
      "Training Epoch: 39 [19450/36045]\tLoss: 563.9621\n",
      "Training Epoch: 39 [19500/36045]\tLoss: 565.4045\n",
      "Training Epoch: 39 [19550/36045]\tLoss: 563.6635\n",
      "Training Epoch: 39 [19600/36045]\tLoss: 604.5255\n",
      "Training Epoch: 39 [19650/36045]\tLoss: 806.1061\n",
      "Training Epoch: 39 [19700/36045]\tLoss: 764.8134\n",
      "Training Epoch: 39 [19750/36045]\tLoss: 768.7758\n",
      "Training Epoch: 39 [19800/36045]\tLoss: 768.9926\n",
      "Training Epoch: 39 [19850/36045]\tLoss: 504.6938\n",
      "Training Epoch: 39 [19900/36045]\tLoss: 483.5836\n",
      "Training Epoch: 39 [19950/36045]\tLoss: 487.2748\n",
      "Training Epoch: 39 [20000/36045]\tLoss: 486.8817\n",
      "Training Epoch: 39 [20050/36045]\tLoss: 545.0725\n",
      "Training Epoch: 39 [20100/36045]\tLoss: 550.8112\n",
      "Training Epoch: 39 [20150/36045]\tLoss: 552.1496\n",
      "Training Epoch: 39 [20200/36045]\tLoss: 551.8516\n",
      "Training Epoch: 39 [20250/36045]\tLoss: 588.2839\n",
      "Training Epoch: 39 [20300/36045]\tLoss: 624.2939\n",
      "Training Epoch: 39 [20350/36045]\tLoss: 642.7302\n",
      "Training Epoch: 39 [20400/36045]\tLoss: 659.2960\n",
      "Training Epoch: 39 [20450/36045]\tLoss: 629.3517\n",
      "Training Epoch: 39 [20500/36045]\tLoss: 613.7726\n",
      "Training Epoch: 39 [20550/36045]\tLoss: 539.4773\n",
      "Training Epoch: 39 [20600/36045]\tLoss: 549.5091\n",
      "Training Epoch: 39 [20650/36045]\tLoss: 546.8686\n",
      "Training Epoch: 39 [20700/36045]\tLoss: 534.9283\n",
      "Training Epoch: 39 [20750/36045]\tLoss: 576.6285\n",
      "Training Epoch: 39 [20800/36045]\tLoss: 626.4446\n",
      "Training Epoch: 39 [20850/36045]\tLoss: 613.1970\n",
      "Training Epoch: 39 [20900/36045]\tLoss: 656.7789\n",
      "Training Epoch: 39 [20950/36045]\tLoss: 619.1049\n",
      "Training Epoch: 39 [21000/36045]\tLoss: 582.7081\n",
      "Training Epoch: 39 [21050/36045]\tLoss: 498.4719\n",
      "Training Epoch: 39 [21100/36045]\tLoss: 503.0725\n",
      "Training Epoch: 39 [21150/36045]\tLoss: 538.4847\n",
      "Training Epoch: 39 [21200/36045]\tLoss: 537.6770\n",
      "Training Epoch: 39 [21250/36045]\tLoss: 514.3201\n",
      "Training Epoch: 39 [21300/36045]\tLoss: 600.5411\n",
      "Training Epoch: 39 [21350/36045]\tLoss: 592.3765\n",
      "Training Epoch: 39 [21400/36045]\tLoss: 596.0059\n",
      "Training Epoch: 39 [21450/36045]\tLoss: 602.2087\n",
      "Training Epoch: 39 [21500/36045]\tLoss: 604.1731\n",
      "Training Epoch: 39 [21550/36045]\tLoss: 698.6103\n",
      "Training Epoch: 39 [21600/36045]\tLoss: 697.2050\n",
      "Training Epoch: 39 [21650/36045]\tLoss: 709.5457\n",
      "Training Epoch: 39 [21700/36045]\tLoss: 712.5831\n",
      "Training Epoch: 39 [21750/36045]\tLoss: 684.2175\n",
      "Training Epoch: 39 [21800/36045]\tLoss: 503.3228\n",
      "Training Epoch: 39 [21850/36045]\tLoss: 486.3385\n",
      "Training Epoch: 39 [21900/36045]\tLoss: 495.7138\n",
      "Training Epoch: 39 [21950/36045]\tLoss: 496.6324\n",
      "Training Epoch: 39 [22000/36045]\tLoss: 499.9400\n",
      "Training Epoch: 39 [22050/36045]\tLoss: 520.4703\n",
      "Training Epoch: 39 [22100/36045]\tLoss: 513.0630\n",
      "Training Epoch: 39 [22150/36045]\tLoss: 499.1428\n",
      "Training Epoch: 39 [22200/36045]\tLoss: 515.0611\n",
      "Training Epoch: 39 [22250/36045]\tLoss: 520.0377\n",
      "Training Epoch: 39 [22300/36045]\tLoss: 572.0544\n",
      "Training Epoch: 39 [22350/36045]\tLoss: 597.7937\n",
      "Training Epoch: 39 [22400/36045]\tLoss: 612.0010\n",
      "Training Epoch: 39 [22450/36045]\tLoss: 599.2877\n",
      "Training Epoch: 39 [22500/36045]\tLoss: 582.0197\n",
      "Training Epoch: 39 [22550/36045]\tLoss: 617.2324\n",
      "Training Epoch: 39 [22600/36045]\tLoss: 667.4805\n",
      "Training Epoch: 39 [22650/36045]\tLoss: 701.1977\n",
      "Training Epoch: 39 [22700/36045]\tLoss: 722.8677\n",
      "Training Epoch: 39 [22750/36045]\tLoss: 742.2253\n",
      "Training Epoch: 39 [22800/36045]\tLoss: 771.5046\n",
      "Training Epoch: 39 [22850/36045]\tLoss: 641.1940\n",
      "Training Epoch: 39 [22900/36045]\tLoss: 646.1117\n",
      "Training Epoch: 39 [22950/36045]\tLoss: 625.6385\n",
      "Training Epoch: 39 [23000/36045]\tLoss: 622.2772\n",
      "Training Epoch: 39 [23050/36045]\tLoss: 552.7184\n",
      "Training Epoch: 39 [23100/36045]\tLoss: 568.3807\n",
      "Training Epoch: 39 [23150/36045]\tLoss: 556.7584\n",
      "Training Epoch: 39 [23200/36045]\tLoss: 527.1332\n",
      "Training Epoch: 39 [23250/36045]\tLoss: 530.1516\n",
      "Training Epoch: 39 [23300/36045]\tLoss: 526.2118\n",
      "Training Epoch: 39 [23350/36045]\tLoss: 546.7984\n",
      "Training Epoch: 39 [23400/36045]\tLoss: 592.8727\n",
      "Training Epoch: 39 [23450/36045]\tLoss: 586.4088\n",
      "Training Epoch: 39 [23500/36045]\tLoss: 565.1522\n",
      "Training Epoch: 39 [23550/36045]\tLoss: 605.6481\n",
      "Training Epoch: 39 [23600/36045]\tLoss: 686.5790\n",
      "Training Epoch: 39 [23650/36045]\tLoss: 698.5977\n",
      "Training Epoch: 39 [23700/36045]\tLoss: 706.6315\n",
      "Training Epoch: 39 [23750/36045]\tLoss: 682.7950\n",
      "Training Epoch: 39 [23800/36045]\tLoss: 546.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 39 [23850/36045]\tLoss: 572.8942\n",
      "Training Epoch: 39 [23900/36045]\tLoss: 562.6178\n",
      "Training Epoch: 39 [23950/36045]\tLoss: 545.8022\n",
      "Training Epoch: 39 [24000/36045]\tLoss: 522.8228\n",
      "Training Epoch: 39 [24050/36045]\tLoss: 482.8124\n",
      "Training Epoch: 39 [24100/36045]\tLoss: 507.7694\n",
      "Training Epoch: 39 [24150/36045]\tLoss: 500.1912\n",
      "Training Epoch: 39 [24200/36045]\tLoss: 497.4076\n",
      "Training Epoch: 39 [24250/36045]\tLoss: 483.0765\n",
      "Training Epoch: 39 [24300/36045]\tLoss: 522.0065\n",
      "Training Epoch: 39 [24350/36045]\tLoss: 534.4781\n",
      "Training Epoch: 39 [24400/36045]\tLoss: 549.4443\n",
      "Training Epoch: 39 [24450/36045]\tLoss: 523.5960\n",
      "Training Epoch: 39 [24500/36045]\tLoss: 552.5479\n",
      "Training Epoch: 39 [24550/36045]\tLoss: 640.2103\n",
      "Training Epoch: 39 [24600/36045]\tLoss: 631.7815\n",
      "Training Epoch: 39 [24650/36045]\tLoss: 604.8691\n",
      "Training Epoch: 39 [24700/36045]\tLoss: 614.6569\n",
      "Training Epoch: 39 [24750/36045]\tLoss: 568.0475\n",
      "Training Epoch: 39 [24800/36045]\tLoss: 466.5230\n",
      "Training Epoch: 39 [24850/36045]\tLoss: 485.2828\n",
      "Training Epoch: 39 [24900/36045]\tLoss: 482.6960\n",
      "Training Epoch: 39 [24950/36045]\tLoss: 485.2160\n",
      "Training Epoch: 39 [25000/36045]\tLoss: 466.4305\n",
      "Training Epoch: 39 [25050/36045]\tLoss: 445.8034\n",
      "Training Epoch: 39 [25100/36045]\tLoss: 399.4261\n",
      "Training Epoch: 39 [25150/36045]\tLoss: 369.8815\n",
      "Training Epoch: 39 [25200/36045]\tLoss: 364.9040\n",
      "Training Epoch: 39 [25250/36045]\tLoss: 391.1375\n",
      "Training Epoch: 39 [25300/36045]\tLoss: 513.8616\n",
      "Training Epoch: 39 [25350/36045]\tLoss: 510.1784\n",
      "Training Epoch: 39 [25400/36045]\tLoss: 476.5206\n",
      "Training Epoch: 39 [25450/36045]\tLoss: 478.7339\n",
      "Training Epoch: 39 [25500/36045]\tLoss: 520.1342\n",
      "Training Epoch: 39 [25550/36045]\tLoss: 608.5489\n",
      "Training Epoch: 39 [25600/36045]\tLoss: 612.6400\n",
      "Training Epoch: 39 [25650/36045]\tLoss: 591.1889\n",
      "Training Epoch: 39 [25700/36045]\tLoss: 600.5429\n",
      "Training Epoch: 39 [25750/36045]\tLoss: 579.9634\n",
      "Training Epoch: 39 [25800/36045]\tLoss: 366.1798\n",
      "Training Epoch: 39 [25850/36045]\tLoss: 374.5617\n",
      "Training Epoch: 39 [25900/36045]\tLoss: 355.7677\n",
      "Training Epoch: 39 [25950/36045]\tLoss: 364.8315\n",
      "Training Epoch: 39 [26000/36045]\tLoss: 448.2995\n",
      "Training Epoch: 39 [26050/36045]\tLoss: 610.4058\n",
      "Training Epoch: 39 [26100/36045]\tLoss: 637.0490\n",
      "Training Epoch: 39 [26150/36045]\tLoss: 636.7808\n",
      "Training Epoch: 39 [26200/36045]\tLoss: 609.8661\n",
      "Training Epoch: 39 [26250/36045]\tLoss: 639.6795\n",
      "Training Epoch: 39 [26300/36045]\tLoss: 584.8559\n",
      "Training Epoch: 39 [26350/36045]\tLoss: 595.9498\n",
      "Training Epoch: 39 [26400/36045]\tLoss: 572.0121\n",
      "Training Epoch: 39 [26450/36045]\tLoss: 501.8566\n",
      "Training Epoch: 39 [26500/36045]\tLoss: 594.4272\n",
      "Training Epoch: 39 [26550/36045]\tLoss: 594.4394\n",
      "Training Epoch: 39 [26600/36045]\tLoss: 591.0547\n",
      "Training Epoch: 39 [26650/36045]\tLoss: 606.5527\n",
      "Training Epoch: 39 [26700/36045]\tLoss: 585.6931\n",
      "Training Epoch: 39 [26750/36045]\tLoss: 548.8093\n",
      "Training Epoch: 39 [26800/36045]\tLoss: 405.0261\n",
      "Training Epoch: 39 [26850/36045]\tLoss: 335.6573\n",
      "Training Epoch: 39 [26900/36045]\tLoss: 338.0373\n",
      "Training Epoch: 39 [26950/36045]\tLoss: 371.2277\n",
      "Training Epoch: 39 [27000/36045]\tLoss: 609.4977\n",
      "Training Epoch: 39 [27050/36045]\tLoss: 636.2236\n",
      "Training Epoch: 39 [27100/36045]\tLoss: 616.7101\n",
      "Training Epoch: 39 [27150/36045]\tLoss: 656.2728\n",
      "Training Epoch: 39 [27200/36045]\tLoss: 477.1581\n",
      "Training Epoch: 39 [27250/36045]\tLoss: 468.0878\n",
      "Training Epoch: 39 [27300/36045]\tLoss: 456.2525\n",
      "Training Epoch: 39 [27350/36045]\tLoss: 454.0264\n",
      "Training Epoch: 39 [27400/36045]\tLoss: 453.3679\n",
      "Training Epoch: 39 [27450/36045]\tLoss: 573.4291\n",
      "Training Epoch: 39 [27500/36045]\tLoss: 614.4575\n",
      "Training Epoch: 39 [27550/36045]\tLoss: 607.7923\n",
      "Training Epoch: 39 [27600/36045]\tLoss: 619.3466\n",
      "Training Epoch: 39 [27650/36045]\tLoss: 611.3044\n",
      "Training Epoch: 39 [27700/36045]\tLoss: 639.1595\n",
      "Training Epoch: 39 [27750/36045]\tLoss: 651.1257\n",
      "Training Epoch: 39 [27800/36045]\tLoss: 638.2410\n",
      "Training Epoch: 39 [27850/36045]\tLoss: 628.4498\n",
      "Training Epoch: 39 [27900/36045]\tLoss: 570.3688\n",
      "Training Epoch: 39 [27950/36045]\tLoss: 476.3956\n",
      "Training Epoch: 39 [28000/36045]\tLoss: 453.1595\n",
      "Training Epoch: 39 [28050/36045]\tLoss: 462.7852\n",
      "Training Epoch: 39 [28100/36045]\tLoss: 454.4201\n",
      "Training Epoch: 39 [28150/36045]\tLoss: 474.7147\n",
      "Training Epoch: 39 [28200/36045]\tLoss: 482.8476\n",
      "Training Epoch: 39 [28250/36045]\tLoss: 475.8856\n",
      "Training Epoch: 39 [28300/36045]\tLoss: 451.7351\n",
      "Training Epoch: 39 [28350/36045]\tLoss: 447.5784\n",
      "Training Epoch: 39 [28400/36045]\tLoss: 775.9565\n",
      "Training Epoch: 39 [28450/36045]\tLoss: 711.7236\n",
      "Training Epoch: 39 [28500/36045]\tLoss: 615.0498\n",
      "Training Epoch: 39 [28550/36045]\tLoss: 565.3893\n",
      "Training Epoch: 39 [28600/36045]\tLoss: 591.9250\n",
      "Training Epoch: 39 [28650/36045]\tLoss: 649.1536\n",
      "Training Epoch: 39 [28700/36045]\tLoss: 643.0926\n",
      "Training Epoch: 39 [28750/36045]\tLoss: 629.8550\n",
      "Training Epoch: 39 [28800/36045]\tLoss: 639.4073\n",
      "Training Epoch: 39 [28850/36045]\tLoss: 555.1230\n",
      "Training Epoch: 39 [28900/36045]\tLoss: 451.4752\n",
      "Training Epoch: 39 [28950/36045]\tLoss: 450.5551\n",
      "Training Epoch: 39 [29000/36045]\tLoss: 446.8710\n",
      "Training Epoch: 39 [29050/36045]\tLoss: 453.7559\n",
      "Training Epoch: 39 [29100/36045]\tLoss: 471.9893\n",
      "Training Epoch: 39 [29150/36045]\tLoss: 461.9662\n",
      "Training Epoch: 39 [29200/36045]\tLoss: 447.7860\n",
      "Training Epoch: 39 [29250/36045]\tLoss: 437.6369\n",
      "Training Epoch: 39 [29300/36045]\tLoss: 493.4738\n",
      "Training Epoch: 39 [29350/36045]\tLoss: 580.1124\n",
      "Training Epoch: 39 [29400/36045]\tLoss: 597.6151\n",
      "Training Epoch: 39 [29450/36045]\tLoss: 614.7128\n",
      "Training Epoch: 39 [29500/36045]\tLoss: 629.6328\n",
      "Training Epoch: 39 [29550/36045]\tLoss: 599.1402\n",
      "Training Epoch: 39 [29600/36045]\tLoss: 504.4609\n",
      "Training Epoch: 39 [29650/36045]\tLoss: 486.1209\n",
      "Training Epoch: 39 [29700/36045]\tLoss: 435.7228\n",
      "Training Epoch: 39 [29750/36045]\tLoss: 434.2652\n",
      "Training Epoch: 39 [29800/36045]\tLoss: 481.4833\n",
      "Training Epoch: 39 [29850/36045]\tLoss: 558.3693\n",
      "Training Epoch: 39 [29900/36045]\tLoss: 554.2666\n",
      "Training Epoch: 39 [29950/36045]\tLoss: 575.9631\n",
      "Training Epoch: 39 [30000/36045]\tLoss: 550.0566\n",
      "Training Epoch: 39 [30050/36045]\tLoss: 556.9636\n",
      "Training Epoch: 39 [30100/36045]\tLoss: 679.5812\n",
      "Training Epoch: 39 [30150/36045]\tLoss: 662.3739\n",
      "Training Epoch: 39 [30200/36045]\tLoss: 624.6902\n",
      "Training Epoch: 39 [30250/36045]\tLoss: 673.0826\n",
      "Training Epoch: 39 [30300/36045]\tLoss: 657.5165\n",
      "Training Epoch: 39 [30350/36045]\tLoss: 503.9990\n",
      "Training Epoch: 39 [30400/36045]\tLoss: 488.6504\n",
      "Training Epoch: 39 [30450/36045]\tLoss: 490.9001\n",
      "Training Epoch: 39 [30500/36045]\tLoss: 458.1718\n",
      "Training Epoch: 39 [30550/36045]\tLoss: 424.9050\n",
      "Training Epoch: 39 [30600/36045]\tLoss: 416.9254\n",
      "Training Epoch: 39 [30650/36045]\tLoss: 407.0087\n",
      "Training Epoch: 39 [30700/36045]\tLoss: 424.8196\n",
      "Training Epoch: 39 [30750/36045]\tLoss: 411.7874\n",
      "Training Epoch: 39 [30800/36045]\tLoss: 437.9626\n",
      "Training Epoch: 39 [30850/36045]\tLoss: 429.6412\n",
      "Training Epoch: 39 [30900/36045]\tLoss: 441.7166\n",
      "Training Epoch: 39 [30950/36045]\tLoss: 464.1408\n",
      "Training Epoch: 39 [31000/36045]\tLoss: 456.4283\n",
      "Training Epoch: 39 [31050/36045]\tLoss: 381.2053\n",
      "Training Epoch: 39 [31100/36045]\tLoss: 371.7907\n",
      "Training Epoch: 39 [31150/36045]\tLoss: 379.3756\n",
      "Training Epoch: 39 [31200/36045]\tLoss: 471.2909\n",
      "Training Epoch: 39 [31250/36045]\tLoss: 611.5426\n",
      "Training Epoch: 39 [31300/36045]\tLoss: 582.6320\n",
      "Training Epoch: 39 [31350/36045]\tLoss: 598.5073\n",
      "Training Epoch: 39 [31400/36045]\tLoss: 578.0340\n",
      "Training Epoch: 39 [31450/36045]\tLoss: 594.6078\n",
      "Training Epoch: 39 [31500/36045]\tLoss: 607.1884\n",
      "Training Epoch: 39 [31550/36045]\tLoss: 614.2698\n",
      "Training Epoch: 39 [31600/36045]\tLoss: 577.4780\n",
      "Training Epoch: 39 [31650/36045]\tLoss: 618.0839\n",
      "Training Epoch: 39 [31700/36045]\tLoss: 447.0271\n",
      "Training Epoch: 39 [31750/36045]\tLoss: 369.5007\n",
      "Training Epoch: 39 [31800/36045]\tLoss: 352.7067\n",
      "Training Epoch: 39 [31850/36045]\tLoss: 360.6823\n",
      "Training Epoch: 39 [31900/36045]\tLoss: 569.5076\n",
      "Training Epoch: 39 [31950/36045]\tLoss: 737.3347\n",
      "Training Epoch: 39 [32000/36045]\tLoss: 845.5853\n",
      "Training Epoch: 39 [32050/36045]\tLoss: 801.0957\n",
      "Training Epoch: 39 [32100/36045]\tLoss: 791.9266\n",
      "Training Epoch: 39 [32150/36045]\tLoss: 609.5002\n",
      "Training Epoch: 39 [32200/36045]\tLoss: 611.9391\n",
      "Training Epoch: 39 [32250/36045]\tLoss: 622.2888\n",
      "Training Epoch: 39 [32300/36045]\tLoss: 604.2604\n",
      "Training Epoch: 39 [32350/36045]\tLoss: 600.1166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 39 [32400/36045]\tLoss: 563.0909\n",
      "Training Epoch: 39 [32450/36045]\tLoss: 463.6154\n",
      "Training Epoch: 39 [32500/36045]\tLoss: 445.4786\n",
      "Training Epoch: 39 [32550/36045]\tLoss: 447.6654\n",
      "Training Epoch: 39 [32600/36045]\tLoss: 444.6897\n",
      "Training Epoch: 39 [32650/36045]\tLoss: 575.1368\n",
      "Training Epoch: 39 [32700/36045]\tLoss: 628.0314\n",
      "Training Epoch: 39 [32750/36045]\tLoss: 598.1564\n",
      "Training Epoch: 39 [32800/36045]\tLoss: 613.3737\n",
      "Training Epoch: 39 [32850/36045]\tLoss: 566.2376\n",
      "Training Epoch: 39 [32900/36045]\tLoss: 453.0535\n",
      "Training Epoch: 39 [32950/36045]\tLoss: 474.5297\n",
      "Training Epoch: 39 [33000/36045]\tLoss: 473.4013\n",
      "Training Epoch: 39 [33050/36045]\tLoss: 450.1408\n",
      "Training Epoch: 39 [33100/36045]\tLoss: 511.5349\n",
      "Training Epoch: 39 [33150/36045]\tLoss: 695.7316\n",
      "Training Epoch: 39 [33200/36045]\tLoss: 677.4341\n",
      "Training Epoch: 39 [33250/36045]\tLoss: 698.1221\n",
      "Training Epoch: 39 [33300/36045]\tLoss: 743.8408\n",
      "Training Epoch: 39 [33350/36045]\tLoss: 569.7798\n",
      "Training Epoch: 39 [33400/36045]\tLoss: 415.6262\n",
      "Training Epoch: 39 [33450/36045]\tLoss: 411.3214\n",
      "Training Epoch: 39 [33500/36045]\tLoss: 423.4680\n",
      "Training Epoch: 39 [33550/36045]\tLoss: 438.7452\n",
      "Training Epoch: 39 [33600/36045]\tLoss: 440.5403\n",
      "Training Epoch: 39 [33650/36045]\tLoss: 588.3005\n",
      "Training Epoch: 39 [33700/36045]\tLoss: 569.0515\n",
      "Training Epoch: 39 [33750/36045]\tLoss: 589.2816\n",
      "Training Epoch: 39 [33800/36045]\tLoss: 586.2047\n",
      "Training Epoch: 39 [33850/36045]\tLoss: 588.2181\n",
      "Training Epoch: 39 [33900/36045]\tLoss: 601.5954\n",
      "Training Epoch: 39 [33950/36045]\tLoss: 611.4805\n",
      "Training Epoch: 39 [34000/36045]\tLoss: 597.9714\n",
      "Training Epoch: 39 [34050/36045]\tLoss: 601.8600\n",
      "Training Epoch: 39 [34100/36045]\tLoss: 580.1328\n",
      "Training Epoch: 39 [34150/36045]\tLoss: 537.8081\n",
      "Training Epoch: 39 [34200/36045]\tLoss: 509.4009\n",
      "Training Epoch: 39 [34250/36045]\tLoss: 523.5298\n",
      "Training Epoch: 39 [34300/36045]\tLoss: 446.9707\n",
      "Training Epoch: 39 [34350/36045]\tLoss: 470.7888\n",
      "Training Epoch: 39 [34400/36045]\tLoss: 463.3732\n",
      "Training Epoch: 39 [34450/36045]\tLoss: 436.1696\n",
      "Training Epoch: 39 [34500/36045]\tLoss: 465.3334\n",
      "Training Epoch: 39 [34550/36045]\tLoss: 456.7422\n",
      "Training Epoch: 39 [34600/36045]\tLoss: 462.9137\n",
      "Training Epoch: 39 [34650/36045]\tLoss: 569.0230\n",
      "Training Epoch: 39 [34700/36045]\tLoss: 603.3277\n",
      "Training Epoch: 39 [34750/36045]\tLoss: 534.7911\n",
      "Training Epoch: 39 [34800/36045]\tLoss: 613.7771\n",
      "Training Epoch: 39 [34850/36045]\tLoss: 621.1340\n",
      "Training Epoch: 39 [34900/36045]\tLoss: 672.5368\n",
      "Training Epoch: 39 [34950/36045]\tLoss: 657.5812\n",
      "Training Epoch: 39 [35000/36045]\tLoss: 658.5730\n",
      "Training Epoch: 39 [35050/36045]\tLoss: 645.5791\n",
      "Training Epoch: 39 [35100/36045]\tLoss: 555.0052\n",
      "Training Epoch: 39 [35150/36045]\tLoss: 547.4818\n",
      "Training Epoch: 39 [35200/36045]\tLoss: 461.0228\n",
      "Training Epoch: 39 [35250/36045]\tLoss: 506.3659\n",
      "Training Epoch: 39 [35300/36045]\tLoss: 521.9614\n",
      "Training Epoch: 39 [35350/36045]\tLoss: 585.4769\n",
      "Training Epoch: 39 [35400/36045]\tLoss: 616.6644\n",
      "Training Epoch: 39 [35450/36045]\tLoss: 587.7039\n",
      "Training Epoch: 39 [35500/36045]\tLoss: 568.7129\n",
      "Training Epoch: 39 [35550/36045]\tLoss: 554.6711\n",
      "Training Epoch: 39 [35600/36045]\tLoss: 604.6397\n",
      "Training Epoch: 39 [35650/36045]\tLoss: 677.5477\n",
      "Training Epoch: 39 [35700/36045]\tLoss: 604.6541\n",
      "Training Epoch: 39 [35750/36045]\tLoss: 661.8790\n",
      "Training Epoch: 39 [35800/36045]\tLoss: 668.3389\n",
      "Training Epoch: 39 [35850/36045]\tLoss: 642.4548\n",
      "Training Epoch: 39 [35900/36045]\tLoss: 665.7383\n",
      "Training Epoch: 39 [35950/36045]\tLoss: 662.8614\n",
      "Training Epoch: 39 [36000/36045]\tLoss: 655.6460\n",
      "Training Epoch: 39 [36045/36045]\tLoss: 640.4360\n",
      "Training Epoch: 39 [4004/4004]\tLoss: 594.8148\n",
      "Training Epoch: 40 [50/36045]\tLoss: 593.5700\n",
      "Training Epoch: 40 [100/36045]\tLoss: 569.1699\n",
      "Training Epoch: 40 [150/36045]\tLoss: 567.4080\n",
      "Training Epoch: 40 [200/36045]\tLoss: 553.7280\n",
      "Training Epoch: 40 [250/36045]\tLoss: 664.3709\n",
      "Training Epoch: 40 [300/36045]\tLoss: 729.0192\n",
      "Training Epoch: 40 [350/36045]\tLoss: 695.6298\n",
      "Training Epoch: 40 [400/36045]\tLoss: 690.5076\n",
      "Training Epoch: 40 [450/36045]\tLoss: 671.2735\n",
      "Training Epoch: 40 [500/36045]\tLoss: 622.0674\n",
      "Training Epoch: 40 [550/36045]\tLoss: 624.8461\n",
      "Training Epoch: 40 [600/36045]\tLoss: 609.6592\n",
      "Training Epoch: 40 [650/36045]\tLoss: 631.5195\n",
      "Training Epoch: 40 [700/36045]\tLoss: 617.4316\n",
      "Training Epoch: 40 [750/36045]\tLoss: 595.1342\n",
      "Training Epoch: 40 [800/36045]\tLoss: 607.0469\n",
      "Training Epoch: 40 [850/36045]\tLoss: 589.3453\n",
      "Training Epoch: 40 [900/36045]\tLoss: 563.8354\n",
      "Training Epoch: 40 [950/36045]\tLoss: 533.8115\n",
      "Training Epoch: 40 [1000/36045]\tLoss: 517.1572\n",
      "Training Epoch: 40 [1050/36045]\tLoss: 519.1213\n",
      "Training Epoch: 40 [1100/36045]\tLoss: 504.8813\n",
      "Training Epoch: 40 [1150/36045]\tLoss: 513.8073\n",
      "Training Epoch: 40 [1200/36045]\tLoss: 543.6829\n",
      "Training Epoch: 40 [1250/36045]\tLoss: 622.5020\n",
      "Training Epoch: 40 [1300/36045]\tLoss: 629.5790\n",
      "Training Epoch: 40 [1350/36045]\tLoss: 630.9631\n",
      "Training Epoch: 40 [1400/36045]\tLoss: 655.1668\n",
      "Training Epoch: 40 [1450/36045]\tLoss: 633.7070\n",
      "Training Epoch: 40 [1500/36045]\tLoss: 579.5693\n",
      "Training Epoch: 40 [1550/36045]\tLoss: 594.6456\n",
      "Training Epoch: 40 [1600/36045]\tLoss: 605.0746\n",
      "Training Epoch: 40 [1650/36045]\tLoss: 592.2506\n",
      "Training Epoch: 40 [1700/36045]\tLoss: 604.3402\n",
      "Training Epoch: 40 [1750/36045]\tLoss: 645.6666\n",
      "Training Epoch: 40 [1800/36045]\tLoss: 627.8842\n",
      "Training Epoch: 40 [1850/36045]\tLoss: 644.1390\n",
      "Training Epoch: 40 [1900/36045]\tLoss: 602.7597\n",
      "Training Epoch: 40 [1950/36045]\tLoss: 612.9878\n",
      "Training Epoch: 40 [2000/36045]\tLoss: 552.6881\n",
      "Training Epoch: 40 [2050/36045]\tLoss: 554.9865\n",
      "Training Epoch: 40 [2100/36045]\tLoss: 584.8091\n",
      "Training Epoch: 40 [2150/36045]\tLoss: 565.5653\n",
      "Training Epoch: 40 [2200/36045]\tLoss: 526.9493\n",
      "Training Epoch: 40 [2250/36045]\tLoss: 497.5898\n",
      "Training Epoch: 40 [2300/36045]\tLoss: 521.6041\n",
      "Training Epoch: 40 [2350/36045]\tLoss: 498.4557\n",
      "Training Epoch: 40 [2400/36045]\tLoss: 506.2241\n",
      "Training Epoch: 40 [2450/36045]\tLoss: 647.8975\n",
      "Training Epoch: 40 [2500/36045]\tLoss: 680.6562\n",
      "Training Epoch: 40 [2550/36045]\tLoss: 678.3596\n",
      "Training Epoch: 40 [2600/36045]\tLoss: 686.8306\n",
      "Training Epoch: 40 [2650/36045]\tLoss: 811.2579\n",
      "Training Epoch: 40 [2700/36045]\tLoss: 898.8766\n",
      "Training Epoch: 40 [2750/36045]\tLoss: 970.0691\n",
      "Training Epoch: 40 [2800/36045]\tLoss: 979.7586\n",
      "Training Epoch: 40 [2850/36045]\tLoss: 747.5901\n",
      "Training Epoch: 40 [2900/36045]\tLoss: 709.2812\n",
      "Training Epoch: 40 [2950/36045]\tLoss: 685.5184\n",
      "Training Epoch: 40 [3000/36045]\tLoss: 679.4968\n",
      "Training Epoch: 40 [3050/36045]\tLoss: 710.4852\n",
      "Training Epoch: 40 [3100/36045]\tLoss: 649.7781\n",
      "Training Epoch: 40 [3150/36045]\tLoss: 500.3858\n",
      "Training Epoch: 40 [3200/36045]\tLoss: 518.0503\n",
      "Training Epoch: 40 [3250/36045]\tLoss: 488.2383\n",
      "Training Epoch: 40 [3300/36045]\tLoss: 462.1826\n",
      "Training Epoch: 40 [3350/36045]\tLoss: 488.2849\n",
      "Training Epoch: 40 [3400/36045]\tLoss: 511.9726\n",
      "Training Epoch: 40 [3450/36045]\tLoss: 549.4886\n",
      "Training Epoch: 40 [3500/36045]\tLoss: 536.5974\n",
      "Training Epoch: 40 [3550/36045]\tLoss: 513.2234\n",
      "Training Epoch: 40 [3600/36045]\tLoss: 551.0226\n",
      "Training Epoch: 40 [3650/36045]\tLoss: 637.3899\n",
      "Training Epoch: 40 [3700/36045]\tLoss: 645.1695\n",
      "Training Epoch: 40 [3750/36045]\tLoss: 614.3358\n",
      "Training Epoch: 40 [3800/36045]\tLoss: 610.6139\n",
      "Training Epoch: 40 [3850/36045]\tLoss: 611.3034\n",
      "Training Epoch: 40 [3900/36045]\tLoss: 616.1366\n",
      "Training Epoch: 40 [3950/36045]\tLoss: 594.6918\n",
      "Training Epoch: 40 [4000/36045]\tLoss: 599.6976\n",
      "Training Epoch: 40 [4050/36045]\tLoss: 550.1973\n",
      "Training Epoch: 40 [4100/36045]\tLoss: 536.4017\n",
      "Training Epoch: 40 [4150/36045]\tLoss: 550.9161\n",
      "Training Epoch: 40 [4200/36045]\tLoss: 545.8736\n",
      "Training Epoch: 40 [4250/36045]\tLoss: 548.3374\n",
      "Training Epoch: 40 [4300/36045]\tLoss: 564.6963\n",
      "Training Epoch: 40 [4350/36045]\tLoss: 548.0005\n",
      "Training Epoch: 40 [4400/36045]\tLoss: 524.0674\n",
      "Training Epoch: 40 [4450/36045]\tLoss: 575.0509\n",
      "Training Epoch: 40 [4500/36045]\tLoss: 618.2230\n",
      "Training Epoch: 40 [4550/36045]\tLoss: 621.6475\n",
      "Training Epoch: 40 [4600/36045]\tLoss: 643.8214\n",
      "Training Epoch: 40 [4650/36045]\tLoss: 633.3902\n",
      "Training Epoch: 40 [4700/36045]\tLoss: 584.0764\n",
      "Training Epoch: 40 [4750/36045]\tLoss: 566.3429\n",
      "Training Epoch: 40 [4800/36045]\tLoss: 591.1262\n",
      "Training Epoch: 40 [4850/36045]\tLoss: 577.6945\n",
      "Training Epoch: 40 [4900/36045]\tLoss: 562.2039\n",
      "Training Epoch: 40 [4950/36045]\tLoss: 577.5970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 40 [5000/36045]\tLoss: 607.1810\n",
      "Training Epoch: 40 [5050/36045]\tLoss: 588.3249\n",
      "Training Epoch: 40 [5100/36045]\tLoss: 598.6902\n",
      "Training Epoch: 40 [5150/36045]\tLoss: 583.0289\n",
      "Training Epoch: 40 [5200/36045]\tLoss: 580.9145\n",
      "Training Epoch: 40 [5250/36045]\tLoss: 574.4999\n",
      "Training Epoch: 40 [5300/36045]\tLoss: 574.7385\n",
      "Training Epoch: 40 [5350/36045]\tLoss: 596.4506\n",
      "Training Epoch: 40 [5400/36045]\tLoss: 574.7852\n",
      "Training Epoch: 40 [5450/36045]\tLoss: 544.7890\n",
      "Training Epoch: 40 [5500/36045]\tLoss: 572.9932\n",
      "Training Epoch: 40 [5550/36045]\tLoss: 561.2106\n",
      "Training Epoch: 40 [5600/36045]\tLoss: 640.9548\n",
      "Training Epoch: 40 [5650/36045]\tLoss: 606.2392\n",
      "Training Epoch: 40 [5700/36045]\tLoss: 568.6497\n",
      "Training Epoch: 40 [5750/36045]\tLoss: 552.7748\n",
      "Training Epoch: 40 [5800/36045]\tLoss: 583.1440\n",
      "Training Epoch: 40 [5850/36045]\tLoss: 572.1177\n",
      "Training Epoch: 40 [5900/36045]\tLoss: 657.6015\n",
      "Training Epoch: 40 [5950/36045]\tLoss: 674.2972\n",
      "Training Epoch: 40 [6000/36045]\tLoss: 660.1007\n",
      "Training Epoch: 40 [6050/36045]\tLoss: 638.0203\n",
      "Training Epoch: 40 [6100/36045]\tLoss: 642.2183\n",
      "Training Epoch: 40 [6150/36045]\tLoss: 631.8746\n",
      "Training Epoch: 40 [6200/36045]\tLoss: 635.7938\n",
      "Training Epoch: 40 [6250/36045]\tLoss: 657.3274\n",
      "Training Epoch: 40 [6300/36045]\tLoss: 668.9761\n",
      "Training Epoch: 40 [6350/36045]\tLoss: 714.7355\n",
      "Training Epoch: 40 [6400/36045]\tLoss: 590.0343\n",
      "Training Epoch: 40 [6450/36045]\tLoss: 543.1470\n",
      "Training Epoch: 40 [6500/36045]\tLoss: 553.0664\n",
      "Training Epoch: 40 [6550/36045]\tLoss: 570.4545\n",
      "Training Epoch: 40 [6600/36045]\tLoss: 568.7327\n",
      "Training Epoch: 40 [6650/36045]\tLoss: 641.9106\n",
      "Training Epoch: 40 [6700/36045]\tLoss: 671.3773\n",
      "Training Epoch: 40 [6750/36045]\tLoss: 648.4423\n",
      "Training Epoch: 40 [6800/36045]\tLoss: 651.3725\n",
      "Training Epoch: 40 [6850/36045]\tLoss: 639.4188\n",
      "Training Epoch: 40 [6900/36045]\tLoss: 569.6907\n",
      "Training Epoch: 40 [6950/36045]\tLoss: 537.1029\n",
      "Training Epoch: 40 [7000/36045]\tLoss: 571.2847\n",
      "Training Epoch: 40 [7050/36045]\tLoss: 583.7741\n",
      "Training Epoch: 40 [7100/36045]\tLoss: 583.1758\n",
      "Training Epoch: 40 [7150/36045]\tLoss: 593.4529\n",
      "Training Epoch: 40 [7200/36045]\tLoss: 595.9814\n",
      "Training Epoch: 40 [7250/36045]\tLoss: 594.0934\n",
      "Training Epoch: 40 [7300/36045]\tLoss: 580.5347\n",
      "Training Epoch: 40 [7350/36045]\tLoss: 577.9783\n",
      "Training Epoch: 40 [7400/36045]\tLoss: 526.3761\n",
      "Training Epoch: 40 [7450/36045]\tLoss: 529.6078\n",
      "Training Epoch: 40 [7500/36045]\tLoss: 525.0132\n",
      "Training Epoch: 40 [7550/36045]\tLoss: 502.8742\n",
      "Training Epoch: 40 [7600/36045]\tLoss: 557.5296\n",
      "Training Epoch: 40 [7650/36045]\tLoss: 596.1937\n",
      "Training Epoch: 40 [7700/36045]\tLoss: 567.4197\n",
      "Training Epoch: 40 [7750/36045]\tLoss: 581.9137\n",
      "Training Epoch: 40 [7800/36045]\tLoss: 571.2880\n",
      "Training Epoch: 40 [7850/36045]\tLoss: 552.9459\n",
      "Training Epoch: 40 [7900/36045]\tLoss: 583.2029\n",
      "Training Epoch: 40 [7950/36045]\tLoss: 580.6776\n",
      "Training Epoch: 40 [8000/36045]\tLoss: 598.8186\n",
      "Training Epoch: 40 [8050/36045]\tLoss: 564.1583\n",
      "Training Epoch: 40 [8100/36045]\tLoss: 589.3370\n",
      "Training Epoch: 40 [8150/36045]\tLoss: 668.0580\n",
      "Training Epoch: 40 [8200/36045]\tLoss: 655.2964\n",
      "Training Epoch: 40 [8250/36045]\tLoss: 623.3867\n",
      "Training Epoch: 40 [8300/36045]\tLoss: 680.9038\n",
      "Training Epoch: 40 [8350/36045]\tLoss: 624.4767\n",
      "Training Epoch: 40 [8400/36045]\tLoss: 558.8910\n",
      "Training Epoch: 40 [8450/36045]\tLoss: 523.1293\n",
      "Training Epoch: 40 [8500/36045]\tLoss: 556.4994\n",
      "Training Epoch: 40 [8550/36045]\tLoss: 549.5552\n",
      "Training Epoch: 40 [8600/36045]\tLoss: 543.7979\n",
      "Training Epoch: 40 [8650/36045]\tLoss: 577.9400\n",
      "Training Epoch: 40 [8700/36045]\tLoss: 611.3067\n",
      "Training Epoch: 40 [8750/36045]\tLoss: 600.4875\n",
      "Training Epoch: 40 [8800/36045]\tLoss: 606.2677\n",
      "Training Epoch: 40 [8850/36045]\tLoss: 599.7759\n",
      "Training Epoch: 40 [8900/36045]\tLoss: 541.4813\n",
      "Training Epoch: 40 [8950/36045]\tLoss: 552.5630\n",
      "Training Epoch: 40 [9000/36045]\tLoss: 568.3390\n",
      "Training Epoch: 40 [9050/36045]\tLoss: 569.9000\n",
      "Training Epoch: 40 [9100/36045]\tLoss: 586.7896\n",
      "Training Epoch: 40 [9150/36045]\tLoss: 434.2510\n",
      "Training Epoch: 40 [9200/36045]\tLoss: 324.6412\n",
      "Training Epoch: 40 [9250/36045]\tLoss: 352.2397\n",
      "Training Epoch: 40 [9300/36045]\tLoss: 362.1526\n",
      "Training Epoch: 40 [9350/36045]\tLoss: 334.1562\n",
      "Training Epoch: 40 [9400/36045]\tLoss: 654.7037\n",
      "Training Epoch: 40 [9450/36045]\tLoss: 695.3282\n",
      "Training Epoch: 40 [9500/36045]\tLoss: 682.7136\n",
      "Training Epoch: 40 [9550/36045]\tLoss: 722.2595\n",
      "Training Epoch: 40 [9600/36045]\tLoss: 537.7379\n",
      "Training Epoch: 40 [9650/36045]\tLoss: 542.7491\n",
      "Training Epoch: 40 [9700/36045]\tLoss: 528.2338\n",
      "Training Epoch: 40 [9750/36045]\tLoss: 526.8947\n",
      "Training Epoch: 40 [9800/36045]\tLoss: 689.1873\n",
      "Training Epoch: 40 [9850/36045]\tLoss: 727.9904\n",
      "Training Epoch: 40 [9900/36045]\tLoss: 738.1384\n",
      "Training Epoch: 40 [9950/36045]\tLoss: 719.2198\n",
      "Training Epoch: 40 [10000/36045]\tLoss: 665.2389\n",
      "Training Epoch: 40 [10050/36045]\tLoss: 545.1880\n",
      "Training Epoch: 40 [10100/36045]\tLoss: 552.8959\n",
      "Training Epoch: 40 [10150/36045]\tLoss: 561.4808\n",
      "Training Epoch: 40 [10200/36045]\tLoss: 550.3827\n",
      "Training Epoch: 40 [10250/36045]\tLoss: 660.5670\n",
      "Training Epoch: 40 [10300/36045]\tLoss: 641.8918\n",
      "Training Epoch: 40 [10350/36045]\tLoss: 676.1843\n",
      "Training Epoch: 40 [10400/36045]\tLoss: 666.4665\n",
      "Training Epoch: 40 [10450/36045]\tLoss: 624.4103\n",
      "Training Epoch: 40 [10500/36045]\tLoss: 521.5262\n",
      "Training Epoch: 40 [10550/36045]\tLoss: 516.2643\n",
      "Training Epoch: 40 [10600/36045]\tLoss: 538.4702\n",
      "Training Epoch: 40 [10650/36045]\tLoss: 544.5278\n",
      "Training Epoch: 40 [10700/36045]\tLoss: 625.4248\n",
      "Training Epoch: 40 [10750/36045]\tLoss: 684.9135\n",
      "Training Epoch: 40 [10800/36045]\tLoss: 630.3602\n",
      "Training Epoch: 40 [10850/36045]\tLoss: 668.3141\n",
      "Training Epoch: 40 [10900/36045]\tLoss: 695.7205\n",
      "Training Epoch: 40 [10950/36045]\tLoss: 512.3286\n",
      "Training Epoch: 40 [11000/36045]\tLoss: 505.9734\n",
      "Training Epoch: 40 [11050/36045]\tLoss: 542.5435\n",
      "Training Epoch: 40 [11100/36045]\tLoss: 552.9140\n",
      "Training Epoch: 40 [11150/36045]\tLoss: 599.7703\n",
      "Training Epoch: 40 [11200/36045]\tLoss: 627.8976\n",
      "Training Epoch: 40 [11250/36045]\tLoss: 639.4664\n",
      "Training Epoch: 40 [11300/36045]\tLoss: 619.7036\n",
      "Training Epoch: 40 [11350/36045]\tLoss: 617.3129\n",
      "Training Epoch: 40 [11400/36045]\tLoss: 580.4468\n",
      "Training Epoch: 40 [11450/36045]\tLoss: 549.3585\n",
      "Training Epoch: 40 [11500/36045]\tLoss: 546.7296\n",
      "Training Epoch: 40 [11550/36045]\tLoss: 556.7179\n",
      "Training Epoch: 40 [11600/36045]\tLoss: 617.0623\n",
      "Training Epoch: 40 [11650/36045]\tLoss: 668.6960\n",
      "Training Epoch: 40 [11700/36045]\tLoss: 667.6598\n",
      "Training Epoch: 40 [11750/36045]\tLoss: 685.9102\n",
      "Training Epoch: 40 [11800/36045]\tLoss: 728.4816\n",
      "Training Epoch: 40 [11850/36045]\tLoss: 786.7083\n",
      "Training Epoch: 40 [11900/36045]\tLoss: 999.4742\n",
      "Training Epoch: 40 [11950/36045]\tLoss: 1002.4155\n",
      "Training Epoch: 40 [12000/36045]\tLoss: 1014.1981\n",
      "Training Epoch: 40 [12050/36045]\tLoss: 973.5328\n",
      "Training Epoch: 40 [12100/36045]\tLoss: 625.0740\n",
      "Training Epoch: 40 [12150/36045]\tLoss: 471.9239\n",
      "Training Epoch: 40 [12200/36045]\tLoss: 466.6786\n",
      "Training Epoch: 40 [12250/36045]\tLoss: 475.3942\n",
      "Training Epoch: 40 [12300/36045]\tLoss: 611.8318\n",
      "Training Epoch: 40 [12350/36045]\tLoss: 667.4951\n",
      "Training Epoch: 40 [12400/36045]\tLoss: 674.8571\n",
      "Training Epoch: 40 [12450/36045]\tLoss: 663.6413\n",
      "Training Epoch: 40 [12500/36045]\tLoss: 690.8151\n",
      "Training Epoch: 40 [12550/36045]\tLoss: 660.6025\n",
      "Training Epoch: 40 [12600/36045]\tLoss: 605.0447\n",
      "Training Epoch: 40 [12650/36045]\tLoss: 603.4012\n",
      "Training Epoch: 40 [12700/36045]\tLoss: 624.7308\n",
      "Training Epoch: 40 [12750/36045]\tLoss: 623.3811\n",
      "Training Epoch: 40 [12800/36045]\tLoss: 608.8729\n",
      "Training Epoch: 40 [12850/36045]\tLoss: 638.0636\n",
      "Training Epoch: 40 [12900/36045]\tLoss: 611.7709\n",
      "Training Epoch: 40 [12950/36045]\tLoss: 597.8218\n",
      "Training Epoch: 40 [13000/36045]\tLoss: 630.8869\n",
      "Training Epoch: 40 [13050/36045]\tLoss: 570.9529\n",
      "Training Epoch: 40 [13100/36045]\tLoss: 587.0111\n",
      "Training Epoch: 40 [13150/36045]\tLoss: 578.6635\n",
      "Training Epoch: 40 [13200/36045]\tLoss: 561.2805\n",
      "Training Epoch: 40 [13250/36045]\tLoss: 583.5005\n",
      "Training Epoch: 40 [13300/36045]\tLoss: 620.8513\n",
      "Training Epoch: 40 [13350/36045]\tLoss: 601.5885\n",
      "Training Epoch: 40 [13400/36045]\tLoss: 604.7745\n",
      "Training Epoch: 40 [13450/36045]\tLoss: 602.0362\n",
      "Training Epoch: 40 [13500/36045]\tLoss: 621.1096\n",
      "Training Epoch: 40 [13550/36045]\tLoss: 758.6979\n",
      "Training Epoch: 40 [13600/36045]\tLoss: 791.3976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 40 [13650/36045]\tLoss: 872.7714\n",
      "Training Epoch: 40 [13700/36045]\tLoss: 769.3211\n",
      "Training Epoch: 40 [13750/36045]\tLoss: 608.2582\n",
      "Training Epoch: 40 [13800/36045]\tLoss: 579.4966\n",
      "Training Epoch: 40 [13850/36045]\tLoss: 562.1208\n",
      "Training Epoch: 40 [13900/36045]\tLoss: 569.3530\n",
      "Training Epoch: 40 [13950/36045]\tLoss: 614.5787\n",
      "Training Epoch: 40 [14000/36045]\tLoss: 647.6045\n",
      "Training Epoch: 40 [14050/36045]\tLoss: 622.5654\n",
      "Training Epoch: 40 [14100/36045]\tLoss: 617.6941\n",
      "Training Epoch: 40 [14150/36045]\tLoss: 605.6652\n",
      "Training Epoch: 40 [14200/36045]\tLoss: 646.3939\n",
      "Training Epoch: 40 [14250/36045]\tLoss: 710.2300\n",
      "Training Epoch: 40 [14300/36045]\tLoss: 713.5698\n",
      "Training Epoch: 40 [14350/36045]\tLoss: 682.1181\n",
      "Training Epoch: 40 [14400/36045]\tLoss: 668.0179\n",
      "Training Epoch: 40 [14450/36045]\tLoss: 703.6115\n",
      "Training Epoch: 40 [14500/36045]\tLoss: 634.9095\n",
      "Training Epoch: 40 [14550/36045]\tLoss: 663.4857\n",
      "Training Epoch: 40 [14600/36045]\tLoss: 649.9575\n",
      "Training Epoch: 40 [14650/36045]\tLoss: 649.5193\n",
      "Training Epoch: 40 [14700/36045]\tLoss: 615.4965\n",
      "Training Epoch: 40 [14750/36045]\tLoss: 529.4108\n",
      "Training Epoch: 40 [14800/36045]\tLoss: 519.5588\n",
      "Training Epoch: 40 [14850/36045]\tLoss: 526.5812\n",
      "Training Epoch: 40 [14900/36045]\tLoss: 520.0837\n",
      "Training Epoch: 40 [14950/36045]\tLoss: 528.0640\n",
      "Training Epoch: 40 [15000/36045]\tLoss: 540.9872\n",
      "Training Epoch: 40 [15050/36045]\tLoss: 537.8989\n",
      "Training Epoch: 40 [15100/36045]\tLoss: 521.9659\n",
      "Training Epoch: 40 [15150/36045]\tLoss: 517.3637\n",
      "Training Epoch: 40 [15200/36045]\tLoss: 479.2154\n",
      "Training Epoch: 40 [15250/36045]\tLoss: 501.0816\n",
      "Training Epoch: 40 [15300/36045]\tLoss: 486.5999\n",
      "Training Epoch: 40 [15350/36045]\tLoss: 498.2263\n",
      "Training Epoch: 40 [15400/36045]\tLoss: 481.2142\n",
      "Training Epoch: 40 [15450/36045]\tLoss: 466.3953\n",
      "Training Epoch: 40 [15500/36045]\tLoss: 479.7148\n",
      "Training Epoch: 40 [15550/36045]\tLoss: 476.1289\n",
      "Training Epoch: 40 [15600/36045]\tLoss: 543.7443\n",
      "Training Epoch: 40 [15650/36045]\tLoss: 560.7866\n",
      "Training Epoch: 40 [15700/36045]\tLoss: 552.8797\n",
      "Training Epoch: 40 [15750/36045]\tLoss: 544.2505\n",
      "Training Epoch: 40 [15800/36045]\tLoss: 520.0598\n",
      "Training Epoch: 40 [15850/36045]\tLoss: 535.0835\n",
      "Training Epoch: 40 [15900/36045]\tLoss: 544.1693\n",
      "Training Epoch: 40 [15950/36045]\tLoss: 563.9806\n",
      "Training Epoch: 40 [16000/36045]\tLoss: 535.3312\n",
      "Training Epoch: 40 [16050/36045]\tLoss: 504.7921\n",
      "Training Epoch: 40 [16100/36045]\tLoss: 468.3413\n",
      "Training Epoch: 40 [16150/36045]\tLoss: 456.3357\n",
      "Training Epoch: 40 [16200/36045]\tLoss: 554.0089\n",
      "Training Epoch: 40 [16250/36045]\tLoss: 581.7233\n",
      "Training Epoch: 40 [16300/36045]\tLoss: 635.0546\n",
      "Training Epoch: 40 [16350/36045]\tLoss: 655.0722\n",
      "Training Epoch: 40 [16400/36045]\tLoss: 626.8160\n",
      "Training Epoch: 40 [16450/36045]\tLoss: 609.0734\n",
      "Training Epoch: 40 [16500/36045]\tLoss: 608.9861\n",
      "Training Epoch: 40 [16550/36045]\tLoss: 574.2427\n",
      "Training Epoch: 40 [16600/36045]\tLoss: 596.5901\n",
      "Training Epoch: 40 [16650/36045]\tLoss: 612.9203\n",
      "Training Epoch: 40 [16700/36045]\tLoss: 592.7650\n",
      "Training Epoch: 40 [16750/36045]\tLoss: 585.4450\n",
      "Training Epoch: 40 [16800/36045]\tLoss: 594.4843\n",
      "Training Epoch: 40 [16850/36045]\tLoss: 566.4835\n",
      "Training Epoch: 40 [16900/36045]\tLoss: 576.2651\n",
      "Training Epoch: 40 [16950/36045]\tLoss: 599.2407\n",
      "Training Epoch: 40 [17000/36045]\tLoss: 583.4254\n",
      "Training Epoch: 40 [17050/36045]\tLoss: 607.7617\n",
      "Training Epoch: 40 [17100/36045]\tLoss: 603.7610\n",
      "Training Epoch: 40 [17150/36045]\tLoss: 524.2930\n",
      "Training Epoch: 40 [17200/36045]\tLoss: 486.4963\n",
      "Training Epoch: 40 [17250/36045]\tLoss: 509.4390\n",
      "Training Epoch: 40 [17300/36045]\tLoss: 539.1642\n",
      "Training Epoch: 40 [17350/36045]\tLoss: 519.6902\n",
      "Training Epoch: 40 [17400/36045]\tLoss: 539.1857\n",
      "Training Epoch: 40 [17450/36045]\tLoss: 557.8226\n",
      "Training Epoch: 40 [17500/36045]\tLoss: 546.1454\n",
      "Training Epoch: 40 [17550/36045]\tLoss: 544.7982\n",
      "Training Epoch: 40 [17600/36045]\tLoss: 539.1299\n",
      "Training Epoch: 40 [17650/36045]\tLoss: 554.7939\n",
      "Training Epoch: 40 [17700/36045]\tLoss: 534.1160\n",
      "Training Epoch: 40 [17750/36045]\tLoss: 550.1671\n",
      "Training Epoch: 40 [17800/36045]\tLoss: 541.2523\n",
      "Training Epoch: 40 [17850/36045]\tLoss: 556.5513\n",
      "Training Epoch: 40 [17900/36045]\tLoss: 584.6263\n",
      "Training Epoch: 40 [17950/36045]\tLoss: 596.8209\n",
      "Training Epoch: 40 [18000/36045]\tLoss: 587.3747\n",
      "Training Epoch: 40 [18050/36045]\tLoss: 645.5062\n",
      "Training Epoch: 40 [18100/36045]\tLoss: 646.8523\n",
      "Training Epoch: 40 [18150/36045]\tLoss: 658.1417\n",
      "Training Epoch: 40 [18200/36045]\tLoss: 640.3500\n",
      "Training Epoch: 40 [18250/36045]\tLoss: 661.1057\n",
      "Training Epoch: 40 [18300/36045]\tLoss: 615.9633\n",
      "Training Epoch: 40 [18350/36045]\tLoss: 688.6960\n",
      "Training Epoch: 40 [18400/36045]\tLoss: 661.6880\n",
      "Training Epoch: 40 [18450/36045]\tLoss: 641.4701\n",
      "Training Epoch: 40 [18500/36045]\tLoss: 640.3483\n",
      "Training Epoch: 40 [18550/36045]\tLoss: 627.7669\n",
      "Training Epoch: 40 [18600/36045]\tLoss: 617.3937\n",
      "Training Epoch: 40 [18650/36045]\tLoss: 663.0929\n",
      "Training Epoch: 40 [18700/36045]\tLoss: 697.6047\n",
      "Training Epoch: 40 [18750/36045]\tLoss: 685.0358\n",
      "Training Epoch: 40 [18800/36045]\tLoss: 707.8782\n",
      "Training Epoch: 40 [18850/36045]\tLoss: 652.9236\n",
      "Training Epoch: 40 [18900/36045]\tLoss: 698.3412\n",
      "Training Epoch: 40 [18950/36045]\tLoss: 640.8311\n",
      "Training Epoch: 40 [19000/36045]\tLoss: 528.1413\n",
      "Training Epoch: 40 [19050/36045]\tLoss: 512.4765\n",
      "Training Epoch: 40 [19100/36045]\tLoss: 520.6662\n",
      "Training Epoch: 40 [19150/36045]\tLoss: 510.7608\n",
      "Training Epoch: 40 [19200/36045]\tLoss: 541.3525\n",
      "Training Epoch: 40 [19250/36045]\tLoss: 556.4337\n",
      "Training Epoch: 40 [19300/36045]\tLoss: 565.8725\n",
      "Training Epoch: 40 [19350/36045]\tLoss: 549.6188\n",
      "Training Epoch: 40 [19400/36045]\tLoss: 570.0895\n",
      "Training Epoch: 40 [19450/36045]\tLoss: 561.4213\n",
      "Training Epoch: 40 [19500/36045]\tLoss: 562.8168\n",
      "Training Epoch: 40 [19550/36045]\tLoss: 561.0499\n",
      "Training Epoch: 40 [19600/36045]\tLoss: 601.9047\n",
      "Training Epoch: 40 [19650/36045]\tLoss: 803.1473\n",
      "Training Epoch: 40 [19700/36045]\tLoss: 761.8653\n",
      "Training Epoch: 40 [19750/36045]\tLoss: 765.8704\n",
      "Training Epoch: 40 [19800/36045]\tLoss: 766.1418\n",
      "Training Epoch: 40 [19850/36045]\tLoss: 502.3434\n",
      "Training Epoch: 40 [19900/36045]\tLoss: 481.2984\n",
      "Training Epoch: 40 [19950/36045]\tLoss: 485.0214\n",
      "Training Epoch: 40 [20000/36045]\tLoss: 484.6984\n",
      "Training Epoch: 40 [20050/36045]\tLoss: 542.5906\n",
      "Training Epoch: 40 [20100/36045]\tLoss: 548.2620\n",
      "Training Epoch: 40 [20150/36045]\tLoss: 549.5375\n",
      "Training Epoch: 40 [20200/36045]\tLoss: 549.2393\n",
      "Training Epoch: 40 [20250/36045]\tLoss: 585.5848\n",
      "Training Epoch: 40 [20300/36045]\tLoss: 621.6154\n",
      "Training Epoch: 40 [20350/36045]\tLoss: 639.9872\n",
      "Training Epoch: 40 [20400/36045]\tLoss: 656.5141\n",
      "Training Epoch: 40 [20450/36045]\tLoss: 626.5175\n",
      "Training Epoch: 40 [20500/36045]\tLoss: 611.0045\n",
      "Training Epoch: 40 [20550/36045]\tLoss: 536.9852\n",
      "Training Epoch: 40 [20600/36045]\tLoss: 546.9551\n",
      "Training Epoch: 40 [20650/36045]\tLoss: 544.3326\n",
      "Training Epoch: 40 [20700/36045]\tLoss: 532.3956\n",
      "Training Epoch: 40 [20750/36045]\tLoss: 573.9572\n",
      "Training Epoch: 40 [20800/36045]\tLoss: 623.5377\n",
      "Training Epoch: 40 [20850/36045]\tLoss: 610.3126\n",
      "Training Epoch: 40 [20900/36045]\tLoss: 653.7642\n",
      "Training Epoch: 40 [20950/36045]\tLoss: 616.2151\n",
      "Training Epoch: 40 [21000/36045]\tLoss: 579.9636\n",
      "Training Epoch: 40 [21050/36045]\tLoss: 496.0872\n",
      "Training Epoch: 40 [21100/36045]\tLoss: 500.7748\n",
      "Training Epoch: 40 [21150/36045]\tLoss: 536.0304\n",
      "Training Epoch: 40 [21200/36045]\tLoss: 535.2087\n",
      "Training Epoch: 40 [21250/36045]\tLoss: 511.9352\n",
      "Training Epoch: 40 [21300/36045]\tLoss: 597.7277\n",
      "Training Epoch: 40 [21350/36045]\tLoss: 589.5403\n",
      "Training Epoch: 40 [21400/36045]\tLoss: 593.1987\n",
      "Training Epoch: 40 [21450/36045]\tLoss: 599.4100\n",
      "Training Epoch: 40 [21500/36045]\tLoss: 601.3029\n",
      "Training Epoch: 40 [21550/36045]\tLoss: 695.6750\n",
      "Training Epoch: 40 [21600/36045]\tLoss: 694.2167\n",
      "Training Epoch: 40 [21650/36045]\tLoss: 706.5424\n",
      "Training Epoch: 40 [21700/36045]\tLoss: 709.6630\n",
      "Training Epoch: 40 [21750/36045]\tLoss: 681.3926\n",
      "Training Epoch: 40 [21800/36045]\tLoss: 501.0592\n",
      "Training Epoch: 40 [21850/36045]\tLoss: 484.0630\n",
      "Training Epoch: 40 [21900/36045]\tLoss: 493.3866\n",
      "Training Epoch: 40 [21950/36045]\tLoss: 494.4104\n",
      "Training Epoch: 40 [22000/36045]\tLoss: 497.7136\n",
      "Training Epoch: 40 [22050/36045]\tLoss: 518.0536\n",
      "Training Epoch: 40 [22100/36045]\tLoss: 510.6242\n",
      "Training Epoch: 40 [22150/36045]\tLoss: 496.7770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 40 [22200/36045]\tLoss: 512.6238\n",
      "Training Epoch: 40 [22250/36045]\tLoss: 517.6083\n",
      "Training Epoch: 40 [22300/36045]\tLoss: 569.5858\n",
      "Training Epoch: 40 [22350/36045]\tLoss: 595.2839\n",
      "Training Epoch: 40 [22400/36045]\tLoss: 609.4091\n",
      "Training Epoch: 40 [22450/36045]\tLoss: 596.6738\n",
      "Training Epoch: 40 [22500/36045]\tLoss: 579.4935\n",
      "Training Epoch: 40 [22550/36045]\tLoss: 614.6400\n",
      "Training Epoch: 40 [22600/36045]\tLoss: 664.5358\n",
      "Training Epoch: 40 [22650/36045]\tLoss: 698.0903\n",
      "Training Epoch: 40 [22700/36045]\tLoss: 719.6552\n",
      "Training Epoch: 40 [22750/36045]\tLoss: 739.0290\n",
      "Training Epoch: 40 [22800/36045]\tLoss: 768.1602\n",
      "Training Epoch: 40 [22850/36045]\tLoss: 638.3311\n",
      "Training Epoch: 40 [22900/36045]\tLoss: 643.2339\n",
      "Training Epoch: 40 [22950/36045]\tLoss: 622.7623\n",
      "Training Epoch: 40 [23000/36045]\tLoss: 619.3431\n",
      "Training Epoch: 40 [23050/36045]\tLoss: 550.1126\n",
      "Training Epoch: 40 [23100/36045]\tLoss: 565.7606\n",
      "Training Epoch: 40 [23150/36045]\tLoss: 554.1451\n",
      "Training Epoch: 40 [23200/36045]\tLoss: 524.6213\n",
      "Training Epoch: 40 [23250/36045]\tLoss: 527.6307\n",
      "Training Epoch: 40 [23300/36045]\tLoss: 523.6586\n",
      "Training Epoch: 40 [23350/36045]\tLoss: 544.1968\n",
      "Training Epoch: 40 [23400/36045]\tLoss: 590.0989\n",
      "Training Epoch: 40 [23450/36045]\tLoss: 583.6724\n",
      "Training Epoch: 40 [23500/36045]\tLoss: 562.4871\n",
      "Training Epoch: 40 [23550/36045]\tLoss: 602.7632\n",
      "Training Epoch: 40 [23600/36045]\tLoss: 683.5300\n",
      "Training Epoch: 40 [23650/36045]\tLoss: 695.4343\n",
      "Training Epoch: 40 [23700/36045]\tLoss: 703.4550\n",
      "Training Epoch: 40 [23750/36045]\tLoss: 679.6711\n",
      "Training Epoch: 40 [23800/36045]\tLoss: 544.6082\n",
      "Training Epoch: 40 [23850/36045]\tLoss: 570.4829\n",
      "Training Epoch: 40 [23900/36045]\tLoss: 560.2407\n",
      "Training Epoch: 40 [23950/36045]\tLoss: 543.4474\n",
      "Training Epoch: 40 [24000/36045]\tLoss: 520.5021\n",
      "Training Epoch: 40 [24050/36045]\tLoss: 480.6235\n",
      "Training Epoch: 40 [24100/36045]\tLoss: 505.4535\n",
      "Training Epoch: 40 [24150/36045]\tLoss: 497.8026\n",
      "Training Epoch: 40 [24200/36045]\tLoss: 495.1173\n",
      "Training Epoch: 40 [24250/36045]\tLoss: 480.8807\n",
      "Training Epoch: 40 [24300/36045]\tLoss: 519.6234\n",
      "Training Epoch: 40 [24350/36045]\tLoss: 532.0036\n",
      "Training Epoch: 40 [24400/36045]\tLoss: 546.9139\n",
      "Training Epoch: 40 [24450/36045]\tLoss: 521.1824\n",
      "Training Epoch: 40 [24500/36045]\tLoss: 550.0654\n",
      "Training Epoch: 40 [24550/36045]\tLoss: 637.5434\n",
      "Training Epoch: 40 [24600/36045]\tLoss: 629.0547\n",
      "Training Epoch: 40 [24650/36045]\tLoss: 602.1691\n",
      "Training Epoch: 40 [24700/36045]\tLoss: 611.9560\n",
      "Training Epoch: 40 [24750/36045]\tLoss: 565.5944\n",
      "Training Epoch: 40 [24800/36045]\tLoss: 464.2488\n",
      "Training Epoch: 40 [24850/36045]\tLoss: 482.9118\n",
      "Training Epoch: 40 [24900/36045]\tLoss: 480.3347\n",
      "Training Epoch: 40 [24950/36045]\tLoss: 482.8361\n",
      "Training Epoch: 40 [25000/36045]\tLoss: 464.1644\n",
      "Training Epoch: 40 [25050/36045]\tLoss: 443.6784\n",
      "Training Epoch: 40 [25100/36045]\tLoss: 397.5247\n",
      "Training Epoch: 40 [25150/36045]\tLoss: 368.1003\n",
      "Training Epoch: 40 [25200/36045]\tLoss: 363.1015\n",
      "Training Epoch: 40 [25250/36045]\tLoss: 389.2210\n",
      "Training Epoch: 40 [25300/36045]\tLoss: 511.3698\n",
      "Training Epoch: 40 [25350/36045]\tLoss: 507.6687\n",
      "Training Epoch: 40 [25400/36045]\tLoss: 474.2608\n",
      "Training Epoch: 40 [25450/36045]\tLoss: 476.4325\n",
      "Training Epoch: 40 [25500/36045]\tLoss: 517.6096\n",
      "Training Epoch: 40 [25550/36045]\tLoss: 605.6151\n",
      "Training Epoch: 40 [25600/36045]\tLoss: 609.6996\n",
      "Training Epoch: 40 [25650/36045]\tLoss: 588.3807\n",
      "Training Epoch: 40 [25700/36045]\tLoss: 597.7761\n",
      "Training Epoch: 40 [25750/36045]\tLoss: 577.3912\n",
      "Training Epoch: 40 [25800/36045]\tLoss: 364.5272\n",
      "Training Epoch: 40 [25850/36045]\tLoss: 372.7644\n",
      "Training Epoch: 40 [25900/36045]\tLoss: 354.0451\n",
      "Training Epoch: 40 [25950/36045]\tLoss: 363.1978\n",
      "Training Epoch: 40 [26000/36045]\tLoss: 446.4320\n",
      "Training Epoch: 40 [26050/36045]\tLoss: 607.7992\n",
      "Training Epoch: 40 [26100/36045]\tLoss: 634.2749\n",
      "Training Epoch: 40 [26150/36045]\tLoss: 633.9105\n",
      "Training Epoch: 40 [26200/36045]\tLoss: 607.0580\n",
      "Training Epoch: 40 [26250/36045]\tLoss: 636.9196\n",
      "Training Epoch: 40 [26300/36045]\tLoss: 582.9695\n",
      "Training Epoch: 40 [26350/36045]\tLoss: 594.0319\n",
      "Training Epoch: 40 [26400/36045]\tLoss: 569.9098\n",
      "Training Epoch: 40 [26450/36045]\tLoss: 499.7874\n",
      "Training Epoch: 40 [26500/36045]\tLoss: 591.9049\n",
      "Training Epoch: 40 [26550/36045]\tLoss: 591.8563\n",
      "Training Epoch: 40 [26600/36045]\tLoss: 588.4617\n",
      "Training Epoch: 40 [26650/36045]\tLoss: 603.8215\n",
      "Training Epoch: 40 [26700/36045]\tLoss: 582.9121\n",
      "Training Epoch: 40 [26750/36045]\tLoss: 546.2864\n",
      "Training Epoch: 40 [26800/36045]\tLoss: 403.2853\n",
      "Training Epoch: 40 [26850/36045]\tLoss: 334.1805\n",
      "Training Epoch: 40 [26900/36045]\tLoss: 336.4698\n",
      "Training Epoch: 40 [26950/36045]\tLoss: 369.4323\n",
      "Training Epoch: 40 [27000/36045]\tLoss: 607.0225\n",
      "Training Epoch: 40 [27050/36045]\tLoss: 633.6167\n",
      "Training Epoch: 40 [27100/36045]\tLoss: 614.2280\n",
      "Training Epoch: 40 [27150/36045]\tLoss: 653.6643\n",
      "Training Epoch: 40 [27200/36045]\tLoss: 474.9769\n",
      "Training Epoch: 40 [27250/36045]\tLoss: 465.8039\n",
      "Training Epoch: 40 [27300/36045]\tLoss: 454.0633\n",
      "Training Epoch: 40 [27350/36045]\tLoss: 451.7511\n",
      "Training Epoch: 40 [27400/36045]\tLoss: 451.1220\n",
      "Training Epoch: 40 [27450/36045]\tLoss: 570.6909\n",
      "Training Epoch: 40 [27500/36045]\tLoss: 611.5242\n",
      "Training Epoch: 40 [27550/36045]\tLoss: 604.8750\n",
      "Training Epoch: 40 [27600/36045]\tLoss: 616.4110\n",
      "Training Epoch: 40 [27650/36045]\tLoss: 608.3839\n",
      "Training Epoch: 40 [27700/36045]\tLoss: 636.1016\n",
      "Training Epoch: 40 [27750/36045]\tLoss: 648.0536\n",
      "Training Epoch: 40 [27800/36045]\tLoss: 635.2245\n",
      "Training Epoch: 40 [27850/36045]\tLoss: 625.4836\n",
      "Training Epoch: 40 [27900/36045]\tLoss: 567.9175\n",
      "Training Epoch: 40 [27950/36045]\tLoss: 474.5360\n",
      "Training Epoch: 40 [28000/36045]\tLoss: 451.3270\n",
      "Training Epoch: 40 [28050/36045]\tLoss: 460.9001\n",
      "Training Epoch: 40 [28100/36045]\tLoss: 452.5432\n",
      "Training Epoch: 40 [28150/36045]\tLoss: 472.5470\n",
      "Training Epoch: 40 [28200/36045]\tLoss: 480.7757\n",
      "Training Epoch: 40 [28250/36045]\tLoss: 473.7682\n",
      "Training Epoch: 40 [28300/36045]\tLoss: 449.7237\n",
      "Training Epoch: 40 [28350/36045]\tLoss: 445.5468\n",
      "Training Epoch: 40 [28400/36045]\tLoss: 773.6672\n",
      "Training Epoch: 40 [28450/36045]\tLoss: 709.7813\n",
      "Training Epoch: 40 [28500/36045]\tLoss: 613.3784\n",
      "Training Epoch: 40 [28550/36045]\tLoss: 563.8880\n",
      "Training Epoch: 40 [28600/36045]\tLoss: 589.9274\n",
      "Training Epoch: 40 [28650/36045]\tLoss: 646.3091\n",
      "Training Epoch: 40 [28700/36045]\tLoss: 640.2390\n",
      "Training Epoch: 40 [28750/36045]\tLoss: 627.0743\n",
      "Training Epoch: 40 [28800/36045]\tLoss: 636.7112\n",
      "Training Epoch: 40 [28850/36045]\tLoss: 552.8276\n",
      "Training Epoch: 40 [28900/36045]\tLoss: 449.6974\n",
      "Training Epoch: 40 [28950/36045]\tLoss: 448.7975\n",
      "Training Epoch: 40 [29000/36045]\tLoss: 445.0434\n",
      "Training Epoch: 40 [29050/36045]\tLoss: 451.9535\n",
      "Training Epoch: 40 [29100/36045]\tLoss: 470.1268\n",
      "Training Epoch: 40 [29150/36045]\tLoss: 460.2124\n",
      "Training Epoch: 40 [29200/36045]\tLoss: 446.0305\n",
      "Training Epoch: 40 [29250/36045]\tLoss: 435.8991\n",
      "Training Epoch: 40 [29300/36045]\tLoss: 491.2001\n",
      "Training Epoch: 40 [29350/36045]\tLoss: 577.2556\n",
      "Training Epoch: 40 [29400/36045]\tLoss: 594.7653\n",
      "Training Epoch: 40 [29450/36045]\tLoss: 611.7482\n",
      "Training Epoch: 40 [29500/36045]\tLoss: 626.6697\n",
      "Training Epoch: 40 [29550/36045]\tLoss: 596.2725\n",
      "Training Epoch: 40 [29600/36045]\tLoss: 501.9204\n",
      "Training Epoch: 40 [29650/36045]\tLoss: 483.5443\n",
      "Training Epoch: 40 [29700/36045]\tLoss: 433.5963\n",
      "Training Epoch: 40 [29750/36045]\tLoss: 432.1120\n",
      "Training Epoch: 40 [29800/36045]\tLoss: 479.3193\n",
      "Training Epoch: 40 [29850/36045]\tLoss: 556.3010\n",
      "Training Epoch: 40 [29900/36045]\tLoss: 552.1145\n",
      "Training Epoch: 40 [29950/36045]\tLoss: 573.8087\n",
      "Training Epoch: 40 [30000/36045]\tLoss: 547.8674\n",
      "Training Epoch: 40 [30050/36045]\tLoss: 554.8553\n",
      "Training Epoch: 40 [30100/36045]\tLoss: 676.9826\n",
      "Training Epoch: 40 [30150/36045]\tLoss: 659.6676\n",
      "Training Epoch: 40 [30200/36045]\tLoss: 622.1030\n",
      "Training Epoch: 40 [30250/36045]\tLoss: 670.4928\n",
      "Training Epoch: 40 [30300/36045]\tLoss: 654.8923\n",
      "Training Epoch: 40 [30350/36045]\tLoss: 501.4823\n",
      "Training Epoch: 40 [30400/36045]\tLoss: 486.1378\n",
      "Training Epoch: 40 [30450/36045]\tLoss: 488.4152\n",
      "Training Epoch: 40 [30500/36045]\tLoss: 455.8188\n",
      "Training Epoch: 40 [30550/36045]\tLoss: 422.7681\n",
      "Training Epoch: 40 [30600/36045]\tLoss: 414.9538\n",
      "Training Epoch: 40 [30650/36045]\tLoss: 405.0541\n",
      "Training Epoch: 40 [30700/36045]\tLoss: 422.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 40 [30750/36045]\tLoss: 409.8194\n",
      "Training Epoch: 40 [30800/36045]\tLoss: 435.9504\n",
      "Training Epoch: 40 [30850/36045]\tLoss: 427.6438\n",
      "Training Epoch: 40 [30900/36045]\tLoss: 439.6594\n",
      "Training Epoch: 40 [30950/36045]\tLoss: 462.0035\n",
      "Training Epoch: 40 [31000/36045]\tLoss: 454.3328\n",
      "Training Epoch: 40 [31050/36045]\tLoss: 379.4133\n",
      "Training Epoch: 40 [31100/36045]\tLoss: 370.0145\n",
      "Training Epoch: 40 [31150/36045]\tLoss: 377.6224\n",
      "Training Epoch: 40 [31200/36045]\tLoss: 469.0353\n",
      "Training Epoch: 40 [31250/36045]\tLoss: 608.6283\n",
      "Training Epoch: 40 [31300/36045]\tLoss: 579.7628\n",
      "Training Epoch: 40 [31350/36045]\tLoss: 595.6501\n",
      "Training Epoch: 40 [31400/36045]\tLoss: 575.1850\n",
      "Training Epoch: 40 [31450/36045]\tLoss: 591.7958\n",
      "Training Epoch: 40 [31500/36045]\tLoss: 604.3904\n",
      "Training Epoch: 40 [31550/36045]\tLoss: 611.3928\n",
      "Training Epoch: 40 [31600/36045]\tLoss: 574.7836\n",
      "Training Epoch: 40 [31650/36045]\tLoss: 615.2551\n",
      "Training Epoch: 40 [31700/36045]\tLoss: 444.9243\n",
      "Training Epoch: 40 [31750/36045]\tLoss: 367.7347\n",
      "Training Epoch: 40 [31800/36045]\tLoss: 351.0255\n",
      "Training Epoch: 40 [31850/36045]\tLoss: 358.9239\n",
      "Training Epoch: 40 [31900/36045]\tLoss: 567.0845\n",
      "Training Epoch: 40 [31950/36045]\tLoss: 734.3978\n",
      "Training Epoch: 40 [32000/36045]\tLoss: 842.4803\n",
      "Training Epoch: 40 [32050/36045]\tLoss: 798.0468\n",
      "Training Epoch: 40 [32100/36045]\tLoss: 788.9622\n",
      "Training Epoch: 40 [32150/36045]\tLoss: 606.7097\n",
      "Training Epoch: 40 [32200/36045]\tLoss: 609.0307\n",
      "Training Epoch: 40 [32250/36045]\tLoss: 619.3602\n",
      "Training Epoch: 40 [32300/36045]\tLoss: 601.3619\n",
      "Training Epoch: 40 [32350/36045]\tLoss: 597.2660\n",
      "Training Epoch: 40 [32400/36045]\tLoss: 560.4119\n",
      "Training Epoch: 40 [32450/36045]\tLoss: 461.3990\n",
      "Training Epoch: 40 [32500/36045]\tLoss: 443.3398\n",
      "Training Epoch: 40 [32550/36045]\tLoss: 445.5101\n",
      "Training Epoch: 40 [32600/36045]\tLoss: 442.5582\n",
      "Training Epoch: 40 [32650/36045]\tLoss: 572.7438\n",
      "Training Epoch: 40 [32700/36045]\tLoss: 625.4719\n",
      "Training Epoch: 40 [32750/36045]\tLoss: 595.6792\n",
      "Training Epoch: 40 [32800/36045]\tLoss: 610.8134\n",
      "Training Epoch: 40 [32850/36045]\tLoss: 563.8677\n",
      "Training Epoch: 40 [32900/36045]\tLoss: 451.0164\n",
      "Training Epoch: 40 [32950/36045]\tLoss: 472.4144\n",
      "Training Epoch: 40 [33000/36045]\tLoss: 471.2300\n",
      "Training Epoch: 40 [33050/36045]\tLoss: 448.1410\n",
      "Training Epoch: 40 [33100/36045]\tLoss: 509.2592\n",
      "Training Epoch: 40 [33150/36045]\tLoss: 692.7826\n",
      "Training Epoch: 40 [33200/36045]\tLoss: 674.5468\n",
      "Training Epoch: 40 [33250/36045]\tLoss: 695.1367\n",
      "Training Epoch: 40 [33300/36045]\tLoss: 740.7130\n",
      "Training Epoch: 40 [33350/36045]\tLoss: 567.2964\n",
      "Training Epoch: 40 [33400/36045]\tLoss: 413.5991\n",
      "Training Epoch: 40 [33450/36045]\tLoss: 409.3557\n",
      "Training Epoch: 40 [33500/36045]\tLoss: 421.4441\n",
      "Training Epoch: 40 [33550/36045]\tLoss: 436.6017\n",
      "Training Epoch: 40 [33600/36045]\tLoss: 438.3869\n",
      "Training Epoch: 40 [33650/36045]\tLoss: 585.4962\n",
      "Training Epoch: 40 [33700/36045]\tLoss: 566.3380\n",
      "Training Epoch: 40 [33750/36045]\tLoss: 586.4888\n",
      "Training Epoch: 40 [33800/36045]\tLoss: 583.5108\n",
      "Training Epoch: 40 [33850/36045]\tLoss: 585.4763\n",
      "Training Epoch: 40 [33900/36045]\tLoss: 598.8936\n",
      "Training Epoch: 40 [33950/36045]\tLoss: 608.6993\n",
      "Training Epoch: 40 [34000/36045]\tLoss: 595.2086\n",
      "Training Epoch: 40 [34050/36045]\tLoss: 599.0728\n",
      "Training Epoch: 40 [34100/36045]\tLoss: 577.5149\n",
      "Training Epoch: 40 [34150/36045]\tLoss: 535.2773\n",
      "Training Epoch: 40 [34200/36045]\tLoss: 507.0090\n",
      "Training Epoch: 40 [34250/36045]\tLoss: 521.0888\n",
      "Training Epoch: 40 [34300/36045]\tLoss: 444.8286\n",
      "Training Epoch: 40 [34350/36045]\tLoss: 468.5417\n",
      "Training Epoch: 40 [34400/36045]\tLoss: 461.2332\n",
      "Training Epoch: 40 [34450/36045]\tLoss: 434.1998\n",
      "Training Epoch: 40 [34500/36045]\tLoss: 463.2087\n",
      "Training Epoch: 40 [34550/36045]\tLoss: 454.6616\n",
      "Training Epoch: 40 [34600/36045]\tLoss: 460.9999\n",
      "Training Epoch: 40 [34650/36045]\tLoss: 567.0110\n",
      "Training Epoch: 40 [34700/36045]\tLoss: 601.2547\n",
      "Training Epoch: 40 [34750/36045]\tLoss: 532.9298\n",
      "Training Epoch: 40 [34800/36045]\tLoss: 611.7507\n",
      "Training Epoch: 40 [34850/36045]\tLoss: 619.0392\n",
      "Training Epoch: 40 [34900/36045]\tLoss: 669.5956\n",
      "Training Epoch: 40 [34950/36045]\tLoss: 654.5591\n",
      "Training Epoch: 40 [35000/36045]\tLoss: 655.5919\n",
      "Training Epoch: 40 [35050/36045]\tLoss: 642.6767\n",
      "Training Epoch: 40 [35100/36045]\tLoss: 553.1149\n",
      "Training Epoch: 40 [35150/36045]\tLoss: 545.5118\n",
      "Training Epoch: 40 [35200/36045]\tLoss: 459.1683\n",
      "Training Epoch: 40 [35250/36045]\tLoss: 504.3858\n",
      "Training Epoch: 40 [35300/36045]\tLoss: 520.1156\n",
      "Training Epoch: 40 [35350/36045]\tLoss: 582.9709\n",
      "Training Epoch: 40 [35400/36045]\tLoss: 613.8463\n",
      "Training Epoch: 40 [35450/36045]\tLoss: 584.9315\n",
      "Training Epoch: 40 [35500/36045]\tLoss: 565.9519\n",
      "Training Epoch: 40 [35550/36045]\tLoss: 552.0163\n",
      "Training Epoch: 40 [35600/36045]\tLoss: 602.1681\n",
      "Training Epoch: 40 [35650/36045]\tLoss: 674.9799\n",
      "Training Epoch: 40 [35700/36045]\tLoss: 602.0056\n",
      "Training Epoch: 40 [35750/36045]\tLoss: 659.1940\n",
      "Training Epoch: 40 [35800/36045]\tLoss: 665.6866\n",
      "Training Epoch: 40 [35850/36045]\tLoss: 639.8487\n",
      "Training Epoch: 40 [35900/36045]\tLoss: 662.9101\n",
      "Training Epoch: 40 [35950/36045]\tLoss: 659.9447\n",
      "Training Epoch: 40 [36000/36045]\tLoss: 652.7664\n",
      "Training Epoch: 40 [36045/36045]\tLoss: 637.6757\n",
      "Training Epoch: 40 [4004/4004]\tLoss: 592.2069\n",
      "Training Epoch: 41 [50/36045]\tLoss: 590.9101\n",
      "Training Epoch: 41 [100/36045]\tLoss: 566.5804\n",
      "Training Epoch: 41 [150/36045]\tLoss: 564.7691\n",
      "Training Epoch: 41 [200/36045]\tLoss: 551.0486\n",
      "Training Epoch: 41 [250/36045]\tLoss: 661.4513\n",
      "Training Epoch: 41 [300/36045]\tLoss: 726.1899\n",
      "Training Epoch: 41 [350/36045]\tLoss: 692.8774\n",
      "Training Epoch: 41 [400/36045]\tLoss: 687.6820\n",
      "Training Epoch: 41 [450/36045]\tLoss: 668.4608\n",
      "Training Epoch: 41 [500/36045]\tLoss: 619.2620\n",
      "Training Epoch: 41 [550/36045]\tLoss: 622.0180\n",
      "Training Epoch: 41 [600/36045]\tLoss: 607.0833\n",
      "Training Epoch: 41 [650/36045]\tLoss: 628.8420\n",
      "Training Epoch: 41 [700/36045]\tLoss: 614.6485\n",
      "Training Epoch: 41 [750/36045]\tLoss: 592.0697\n",
      "Training Epoch: 41 [800/36045]\tLoss: 603.8665\n",
      "Training Epoch: 41 [850/36045]\tLoss: 586.3290\n",
      "Training Epoch: 41 [900/36045]\tLoss: 561.1014\n",
      "Training Epoch: 41 [950/36045]\tLoss: 531.1586\n",
      "Training Epoch: 41 [1000/36045]\tLoss: 514.6562\n",
      "Training Epoch: 41 [1050/36045]\tLoss: 516.5923\n",
      "Training Epoch: 41 [1100/36045]\tLoss: 502.4312\n",
      "Training Epoch: 41 [1150/36045]\tLoss: 511.4273\n",
      "Training Epoch: 41 [1200/36045]\tLoss: 541.2714\n",
      "Training Epoch: 41 [1250/36045]\tLoss: 619.6913\n",
      "Training Epoch: 41 [1300/36045]\tLoss: 626.7488\n",
      "Training Epoch: 41 [1350/36045]\tLoss: 628.0627\n",
      "Training Epoch: 41 [1400/36045]\tLoss: 652.1362\n",
      "Training Epoch: 41 [1450/36045]\tLoss: 630.8315\n",
      "Training Epoch: 41 [1500/36045]\tLoss: 576.8508\n",
      "Training Epoch: 41 [1550/36045]\tLoss: 591.8581\n",
      "Training Epoch: 41 [1600/36045]\tLoss: 602.1936\n",
      "Training Epoch: 41 [1650/36045]\tLoss: 589.3771\n",
      "Training Epoch: 41 [1700/36045]\tLoss: 601.4702\n",
      "Training Epoch: 41 [1750/36045]\tLoss: 642.8263\n",
      "Training Epoch: 41 [1800/36045]\tLoss: 625.1758\n",
      "Training Epoch: 41 [1850/36045]\tLoss: 641.3600\n",
      "Training Epoch: 41 [1900/36045]\tLoss: 600.0740\n",
      "Training Epoch: 41 [1950/36045]\tLoss: 610.2029\n",
      "Training Epoch: 41 [2000/36045]\tLoss: 550.0187\n",
      "Training Epoch: 41 [2050/36045]\tLoss: 552.3848\n",
      "Training Epoch: 41 [2100/36045]\tLoss: 582.1313\n",
      "Training Epoch: 41 [2150/36045]\tLoss: 562.9614\n",
      "Training Epoch: 41 [2200/36045]\tLoss: 524.5527\n",
      "Training Epoch: 41 [2250/36045]\tLoss: 495.3361\n",
      "Training Epoch: 41 [2300/36045]\tLoss: 519.2441\n",
      "Training Epoch: 41 [2350/36045]\tLoss: 496.2447\n",
      "Training Epoch: 41 [2400/36045]\tLoss: 503.9740\n",
      "Training Epoch: 41 [2450/36045]\tLoss: 645.1758\n",
      "Training Epoch: 41 [2500/36045]\tLoss: 677.7297\n",
      "Training Epoch: 41 [2550/36045]\tLoss: 675.4467\n",
      "Training Epoch: 41 [2600/36045]\tLoss: 683.9291\n",
      "Training Epoch: 41 [2650/36045]\tLoss: 808.3043\n",
      "Training Epoch: 41 [2700/36045]\tLoss: 895.9838\n",
      "Training Epoch: 41 [2750/36045]\tLoss: 967.1161\n",
      "Training Epoch: 41 [2800/36045]\tLoss: 976.7528\n",
      "Training Epoch: 41 [2850/36045]\tLoss: 744.4468\n",
      "Training Epoch: 41 [2900/36045]\tLoss: 705.9606\n",
      "Training Epoch: 41 [2950/36045]\tLoss: 682.4387\n",
      "Training Epoch: 41 [3000/36045]\tLoss: 676.4077\n",
      "Training Epoch: 41 [3050/36045]\tLoss: 707.3782\n",
      "Training Epoch: 41 [3100/36045]\tLoss: 646.8291\n",
      "Training Epoch: 41 [3150/36045]\tLoss: 497.9521\n",
      "Training Epoch: 41 [3200/36045]\tLoss: 515.5124\n",
      "Training Epoch: 41 [3250/36045]\tLoss: 485.9519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 41 [3300/36045]\tLoss: 460.0804\n",
      "Training Epoch: 41 [3350/36045]\tLoss: 486.0835\n",
      "Training Epoch: 41 [3400/36045]\tLoss: 509.5839\n",
      "Training Epoch: 41 [3450/36045]\tLoss: 546.8389\n",
      "Training Epoch: 41 [3500/36045]\tLoss: 533.9602\n",
      "Training Epoch: 41 [3550/36045]\tLoss: 510.6738\n",
      "Training Epoch: 41 [3600/36045]\tLoss: 548.4024\n",
      "Training Epoch: 41 [3650/36045]\tLoss: 634.3906\n",
      "Training Epoch: 41 [3700/36045]\tLoss: 642.1731\n",
      "Training Epoch: 41 [3750/36045]\tLoss: 611.3328\n",
      "Training Epoch: 41 [3800/36045]\tLoss: 607.7199\n",
      "Training Epoch: 41 [3850/36045]\tLoss: 608.6111\n",
      "Training Epoch: 41 [3900/36045]\tLoss: 613.4238\n",
      "Training Epoch: 41 [3950/36045]\tLoss: 592.0890\n",
      "Training Epoch: 41 [4000/36045]\tLoss: 597.0183\n",
      "Training Epoch: 41 [4050/36045]\tLoss: 547.6448\n",
      "Training Epoch: 41 [4100/36045]\tLoss: 533.9341\n",
      "Training Epoch: 41 [4150/36045]\tLoss: 548.3538\n",
      "Training Epoch: 41 [4200/36045]\tLoss: 543.3406\n",
      "Training Epoch: 41 [4250/36045]\tLoss: 545.7877\n",
      "Training Epoch: 41 [4300/36045]\tLoss: 561.9372\n",
      "Training Epoch: 41 [4350/36045]\tLoss: 545.3077\n",
      "Training Epoch: 41 [4400/36045]\tLoss: 521.4848\n",
      "Training Epoch: 41 [4450/36045]\tLoss: 572.3781\n",
      "Training Epoch: 41 [4500/36045]\tLoss: 615.5252\n",
      "Training Epoch: 41 [4550/36045]\tLoss: 618.8209\n",
      "Training Epoch: 41 [4600/36045]\tLoss: 640.9136\n",
      "Training Epoch: 41 [4650/36045]\tLoss: 630.5284\n",
      "Training Epoch: 41 [4700/36045]\tLoss: 581.4197\n",
      "Training Epoch: 41 [4750/36045]\tLoss: 563.6716\n",
      "Training Epoch: 41 [4800/36045]\tLoss: 588.3511\n",
      "Training Epoch: 41 [4850/36045]\tLoss: 574.8934\n",
      "Training Epoch: 41 [4900/36045]\tLoss: 559.4889\n",
      "Training Epoch: 41 [4950/36045]\tLoss: 574.8585\n",
      "Training Epoch: 41 [5000/36045]\tLoss: 604.4271\n",
      "Training Epoch: 41 [5050/36045]\tLoss: 585.6629\n",
      "Training Epoch: 41 [5100/36045]\tLoss: 595.9825\n",
      "Training Epoch: 41 [5150/36045]\tLoss: 580.3085\n",
      "Training Epoch: 41 [5200/36045]\tLoss: 578.1902\n",
      "Training Epoch: 41 [5250/36045]\tLoss: 571.8009\n",
      "Training Epoch: 41 [5300/36045]\tLoss: 572.0278\n",
      "Training Epoch: 41 [5350/36045]\tLoss: 593.6329\n",
      "Training Epoch: 41 [5400/36045]\tLoss: 572.0818\n",
      "Training Epoch: 41 [5450/36045]\tLoss: 542.1796\n",
      "Training Epoch: 41 [5500/36045]\tLoss: 570.3576\n",
      "Training Epoch: 41 [5550/36045]\tLoss: 558.5886\n",
      "Training Epoch: 41 [5600/36045]\tLoss: 638.1381\n",
      "Training Epoch: 41 [5650/36045]\tLoss: 603.5189\n",
      "Training Epoch: 41 [5700/36045]\tLoss: 566.0768\n",
      "Training Epoch: 41 [5750/36045]\tLoss: 550.1791\n",
      "Training Epoch: 41 [5800/36045]\tLoss: 580.4185\n",
      "Training Epoch: 41 [5850/36045]\tLoss: 569.5371\n",
      "Training Epoch: 41 [5900/36045]\tLoss: 654.5732\n",
      "Training Epoch: 41 [5950/36045]\tLoss: 671.2178\n",
      "Training Epoch: 41 [6000/36045]\tLoss: 657.0646\n",
      "Training Epoch: 41 [6050/36045]\tLoss: 635.0418\n",
      "Training Epoch: 41 [6100/36045]\tLoss: 639.2452\n",
      "Training Epoch: 41 [6150/36045]\tLoss: 629.1242\n",
      "Training Epoch: 41 [6200/36045]\tLoss: 633.1867\n",
      "Training Epoch: 41 [6250/36045]\tLoss: 654.7230\n",
      "Training Epoch: 41 [6300/36045]\tLoss: 666.3566\n",
      "Training Epoch: 41 [6350/36045]\tLoss: 712.0002\n",
      "Training Epoch: 41 [6400/36045]\tLoss: 587.5192\n",
      "Training Epoch: 41 [6450/36045]\tLoss: 540.6929\n",
      "Training Epoch: 41 [6500/36045]\tLoss: 550.5681\n",
      "Training Epoch: 41 [6550/36045]\tLoss: 567.9691\n",
      "Training Epoch: 41 [6600/36045]\tLoss: 566.1852\n",
      "Training Epoch: 41 [6650/36045]\tLoss: 639.0902\n",
      "Training Epoch: 41 [6700/36045]\tLoss: 668.4081\n",
      "Training Epoch: 41 [6750/36045]\tLoss: 645.5573\n",
      "Training Epoch: 41 [6800/36045]\tLoss: 648.4985\n",
      "Training Epoch: 41 [6850/36045]\tLoss: 636.5592\n",
      "Training Epoch: 41 [6900/36045]\tLoss: 567.1356\n",
      "Training Epoch: 41 [6950/36045]\tLoss: 534.6995\n",
      "Training Epoch: 41 [7000/36045]\tLoss: 568.7083\n",
      "Training Epoch: 41 [7050/36045]\tLoss: 581.1328\n",
      "Training Epoch: 41 [7100/36045]\tLoss: 580.5088\n",
      "Training Epoch: 41 [7150/36045]\tLoss: 590.7413\n",
      "Training Epoch: 41 [7200/36045]\tLoss: 593.1906\n",
      "Training Epoch: 41 [7250/36045]\tLoss: 591.3364\n",
      "Training Epoch: 41 [7300/36045]\tLoss: 577.7562\n",
      "Training Epoch: 41 [7350/36045]\tLoss: 575.2717\n",
      "Training Epoch: 41 [7400/36045]\tLoss: 524.0459\n",
      "Training Epoch: 41 [7450/36045]\tLoss: 527.2701\n",
      "Training Epoch: 41 [7500/36045]\tLoss: 522.7114\n",
      "Training Epoch: 41 [7550/36045]\tLoss: 500.6636\n",
      "Training Epoch: 41 [7600/36045]\tLoss: 555.0215\n",
      "Training Epoch: 41 [7650/36045]\tLoss: 593.4451\n",
      "Training Epoch: 41 [7700/36045]\tLoss: 564.7858\n",
      "Training Epoch: 41 [7750/36045]\tLoss: 579.2650\n",
      "Training Epoch: 41 [7800/36045]\tLoss: 568.6889\n",
      "Training Epoch: 41 [7850/36045]\tLoss: 550.4269\n",
      "Training Epoch: 41 [7900/36045]\tLoss: 580.5358\n",
      "Training Epoch: 41 [7950/36045]\tLoss: 578.0232\n",
      "Training Epoch: 41 [8000/36045]\tLoss: 596.1674\n",
      "Training Epoch: 41 [8050/36045]\tLoss: 561.5880\n",
      "Training Epoch: 41 [8100/36045]\tLoss: 586.7418\n",
      "Training Epoch: 41 [8150/36045]\tLoss: 665.1992\n",
      "Training Epoch: 41 [8200/36045]\tLoss: 652.4890\n",
      "Training Epoch: 41 [8250/36045]\tLoss: 620.6412\n",
      "Training Epoch: 41 [8300/36045]\tLoss: 678.0044\n",
      "Training Epoch: 41 [8350/36045]\tLoss: 621.7442\n",
      "Training Epoch: 41 [8400/36045]\tLoss: 556.3698\n",
      "Training Epoch: 41 [8450/36045]\tLoss: 520.7497\n",
      "Training Epoch: 41 [8500/36045]\tLoss: 554.0139\n",
      "Training Epoch: 41 [8550/36045]\tLoss: 547.1566\n",
      "Training Epoch: 41 [8600/36045]\tLoss: 541.4253\n",
      "Training Epoch: 41 [8650/36045]\tLoss: 575.2527\n",
      "Training Epoch: 41 [8700/36045]\tLoss: 608.4608\n",
      "Training Epoch: 41 [8750/36045]\tLoss: 597.7109\n",
      "Training Epoch: 41 [8800/36045]\tLoss: 603.4866\n",
      "Training Epoch: 41 [8850/36045]\tLoss: 597.0117\n",
      "Training Epoch: 41 [8900/36045]\tLoss: 538.9916\n",
      "Training Epoch: 41 [8950/36045]\tLoss: 549.9828\n",
      "Training Epoch: 41 [9000/36045]\tLoss: 565.7813\n",
      "Training Epoch: 41 [9050/36045]\tLoss: 567.3550\n",
      "Training Epoch: 41 [9100/36045]\tLoss: 584.1959\n",
      "Training Epoch: 41 [9150/36045]\tLoss: 432.3513\n",
      "Training Epoch: 41 [9200/36045]\tLoss: 323.1647\n",
      "Training Epoch: 41 [9250/36045]\tLoss: 350.6570\n",
      "Training Epoch: 41 [9300/36045]\tLoss: 360.5135\n",
      "Training Epoch: 41 [9350/36045]\tLoss: 332.6671\n",
      "Training Epoch: 41 [9400/36045]\tLoss: 651.7389\n",
      "Training Epoch: 41 [9450/36045]\tLoss: 692.1659\n",
      "Training Epoch: 41 [9500/36045]\tLoss: 679.5991\n",
      "Training Epoch: 41 [9550/36045]\tLoss: 718.9664\n",
      "Training Epoch: 41 [9600/36045]\tLoss: 535.3929\n",
      "Training Epoch: 41 [9650/36045]\tLoss: 540.4288\n",
      "Training Epoch: 41 [9700/36045]\tLoss: 525.9218\n",
      "Training Epoch: 41 [9750/36045]\tLoss: 524.5654\n",
      "Training Epoch: 41 [9800/36045]\tLoss: 686.1606\n",
      "Training Epoch: 41 [9850/36045]\tLoss: 724.7809\n",
      "Training Epoch: 41 [9900/36045]\tLoss: 734.7473\n",
      "Training Epoch: 41 [9950/36045]\tLoss: 715.9181\n",
      "Training Epoch: 41 [10000/36045]\tLoss: 662.2646\n",
      "Training Epoch: 41 [10050/36045]\tLoss: 542.6149\n",
      "Training Epoch: 41 [10100/36045]\tLoss: 550.3290\n",
      "Training Epoch: 41 [10150/36045]\tLoss: 558.8386\n",
      "Training Epoch: 41 [10200/36045]\tLoss: 547.7417\n",
      "Training Epoch: 41 [10250/36045]\tLoss: 657.6086\n",
      "Training Epoch: 41 [10300/36045]\tLoss: 639.0519\n",
      "Training Epoch: 41 [10350/36045]\tLoss: 673.2015\n",
      "Training Epoch: 41 [10400/36045]\tLoss: 663.4477\n",
      "Training Epoch: 41 [10450/36045]\tLoss: 621.5806\n",
      "Training Epoch: 41 [10500/36045]\tLoss: 519.1271\n",
      "Training Epoch: 41 [10550/36045]\tLoss: 513.8456\n",
      "Training Epoch: 41 [10600/36045]\tLoss: 536.0028\n",
      "Training Epoch: 41 [10650/36045]\tLoss: 542.0286\n",
      "Training Epoch: 41 [10700/36045]\tLoss: 622.6548\n",
      "Training Epoch: 41 [10750/36045]\tLoss: 682.0320\n",
      "Training Epoch: 41 [10800/36045]\tLoss: 627.5791\n",
      "Training Epoch: 41 [10850/36045]\tLoss: 665.4190\n",
      "Training Epoch: 41 [10900/36045]\tLoss: 692.7158\n",
      "Training Epoch: 41 [10950/36045]\tLoss: 509.9849\n",
      "Training Epoch: 41 [11000/36045]\tLoss: 503.6434\n",
      "Training Epoch: 41 [11050/36045]\tLoss: 540.0828\n",
      "Training Epoch: 41 [11100/36045]\tLoss: 550.4031\n",
      "Training Epoch: 41 [11150/36045]\tLoss: 597.0728\n",
      "Training Epoch: 41 [11200/36045]\tLoss: 625.2366\n",
      "Training Epoch: 41 [11250/36045]\tLoss: 636.7612\n",
      "Training Epoch: 41 [11300/36045]\tLoss: 617.0209\n",
      "Training Epoch: 41 [11350/36045]\tLoss: 614.6550\n",
      "Training Epoch: 41 [11400/36045]\tLoss: 577.8881\n",
      "Training Epoch: 41 [11450/36045]\tLoss: 546.8812\n",
      "Training Epoch: 41 [11500/36045]\tLoss: 544.2531\n",
      "Training Epoch: 41 [11550/36045]\tLoss: 554.1386\n",
      "Training Epoch: 41 [11600/36045]\tLoss: 614.4048\n",
      "Training Epoch: 41 [11650/36045]\tLoss: 665.9963\n",
      "Training Epoch: 41 [11700/36045]\tLoss: 664.9548\n",
      "Training Epoch: 41 [11750/36045]\tLoss: 683.1396\n",
      "Training Epoch: 41 [11800/36045]\tLoss: 725.6835\n",
      "Training Epoch: 41 [11850/36045]\tLoss: 783.9593\n",
      "Training Epoch: 41 [11900/36045]\tLoss: 996.5521\n",
      "Training Epoch: 41 [11950/36045]\tLoss: 999.5424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 41 [12000/36045]\tLoss: 1011.2220\n",
      "Training Epoch: 41 [12050/36045]\tLoss: 970.6122\n",
      "Training Epoch: 41 [12100/36045]\tLoss: 622.6675\n",
      "Training Epoch: 41 [12150/36045]\tLoss: 469.7332\n",
      "Training Epoch: 41 [12200/36045]\tLoss: 464.5063\n",
      "Training Epoch: 41 [12250/36045]\tLoss: 473.1997\n",
      "Training Epoch: 41 [12300/36045]\tLoss: 609.3196\n",
      "Training Epoch: 41 [12350/36045]\tLoss: 664.8788\n",
      "Training Epoch: 41 [12400/36045]\tLoss: 672.2065\n",
      "Training Epoch: 41 [12450/36045]\tLoss: 661.0170\n",
      "Training Epoch: 41 [12500/36045]\tLoss: 688.1313\n",
      "Training Epoch: 41 [12550/36045]\tLoss: 657.9637\n",
      "Training Epoch: 41 [12600/36045]\tLoss: 602.4171\n",
      "Training Epoch: 41 [12650/36045]\tLoss: 600.7383\n",
      "Training Epoch: 41 [12700/36045]\tLoss: 622.0493\n",
      "Training Epoch: 41 [12750/36045]\tLoss: 620.6779\n",
      "Training Epoch: 41 [12800/36045]\tLoss: 606.3126\n",
      "Training Epoch: 41 [12850/36045]\tLoss: 635.5047\n",
      "Training Epoch: 41 [12900/36045]\tLoss: 609.2911\n",
      "Training Epoch: 41 [12950/36045]\tLoss: 595.2690\n",
      "Training Epoch: 41 [13000/36045]\tLoss: 628.3760\n",
      "Training Epoch: 41 [13050/36045]\tLoss: 568.5782\n",
      "Training Epoch: 41 [13100/36045]\tLoss: 584.4821\n",
      "Training Epoch: 41 [13150/36045]\tLoss: 576.1246\n",
      "Training Epoch: 41 [13200/36045]\tLoss: 558.8480\n",
      "Training Epoch: 41 [13250/36045]\tLoss: 580.9481\n",
      "Training Epoch: 41 [13300/36045]\tLoss: 618.1807\n",
      "Training Epoch: 41 [13350/36045]\tLoss: 598.9839\n",
      "Training Epoch: 41 [13400/36045]\tLoss: 602.1451\n",
      "Training Epoch: 41 [13450/36045]\tLoss: 599.4509\n",
      "Training Epoch: 41 [13500/36045]\tLoss: 618.4203\n",
      "Training Epoch: 41 [13550/36045]\tLoss: 756.0851\n",
      "Training Epoch: 41 [13600/36045]\tLoss: 788.7672\n",
      "Training Epoch: 41 [13650/36045]\tLoss: 870.2131\n",
      "Training Epoch: 41 [13700/36045]\tLoss: 766.9199\n",
      "Training Epoch: 41 [13750/36045]\tLoss: 605.6476\n",
      "Training Epoch: 41 [13800/36045]\tLoss: 576.7919\n",
      "Training Epoch: 41 [13850/36045]\tLoss: 559.4167\n",
      "Training Epoch: 41 [13900/36045]\tLoss: 566.6526\n",
      "Training Epoch: 41 [13950/36045]\tLoss: 611.8663\n",
      "Training Epoch: 41 [14000/36045]\tLoss: 644.8291\n",
      "Training Epoch: 41 [14050/36045]\tLoss: 619.8260\n",
      "Training Epoch: 41 [14100/36045]\tLoss: 614.9255\n",
      "Training Epoch: 41 [14150/36045]\tLoss: 602.9120\n",
      "Training Epoch: 41 [14200/36045]\tLoss: 643.5814\n",
      "Training Epoch: 41 [14250/36045]\tLoss: 707.2032\n",
      "Training Epoch: 41 [14300/36045]\tLoss: 710.5219\n",
      "Training Epoch: 41 [14350/36045]\tLoss: 679.1462\n",
      "Training Epoch: 41 [14400/36045]\tLoss: 665.0715\n",
      "Training Epoch: 41 [14450/36045]\tLoss: 700.6113\n",
      "Training Epoch: 41 [14500/36045]\tLoss: 631.9835\n",
      "Training Epoch: 41 [14550/36045]\tLoss: 660.4323\n",
      "Training Epoch: 41 [14600/36045]\tLoss: 646.9261\n",
      "Training Epoch: 41 [14650/36045]\tLoss: 646.4708\n",
      "Training Epoch: 41 [14700/36045]\tLoss: 612.7009\n",
      "Training Epoch: 41 [14750/36045]\tLoss: 527.0601\n",
      "Training Epoch: 41 [14800/36045]\tLoss: 517.2000\n",
      "Training Epoch: 41 [14850/36045]\tLoss: 524.2153\n",
      "Training Epoch: 41 [14900/36045]\tLoss: 517.7075\n",
      "Training Epoch: 41 [14950/36045]\tLoss: 525.7120\n",
      "Training Epoch: 41 [15000/36045]\tLoss: 538.5591\n",
      "Training Epoch: 41 [15050/36045]\tLoss: 535.4180\n",
      "Training Epoch: 41 [15100/36045]\tLoss: 519.4722\n",
      "Training Epoch: 41 [15150/36045]\tLoss: 514.9433\n",
      "Training Epoch: 41 [15200/36045]\tLoss: 476.9975\n",
      "Training Epoch: 41 [15250/36045]\tLoss: 498.7990\n",
      "Training Epoch: 41 [15300/36045]\tLoss: 484.3693\n",
      "Training Epoch: 41 [15350/36045]\tLoss: 495.9760\n",
      "Training Epoch: 41 [15400/36045]\tLoss: 478.8573\n",
      "Training Epoch: 41 [15450/36045]\tLoss: 464.0240\n",
      "Training Epoch: 41 [15500/36045]\tLoss: 477.2641\n",
      "Training Epoch: 41 [15550/36045]\tLoss: 473.7485\n",
      "Training Epoch: 41 [15600/36045]\tLoss: 541.2152\n",
      "Training Epoch: 41 [15650/36045]\tLoss: 558.1670\n",
      "Training Epoch: 41 [15700/36045]\tLoss: 550.3012\n",
      "Training Epoch: 41 [15750/36045]\tLoss: 541.6605\n",
      "Training Epoch: 41 [15800/36045]\tLoss: 517.9579\n",
      "Training Epoch: 41 [15850/36045]\tLoss: 533.0511\n",
      "Training Epoch: 41 [15900/36045]\tLoss: 542.0986\n",
      "Training Epoch: 41 [15950/36045]\tLoss: 561.9035\n",
      "Training Epoch: 41 [16000/36045]\tLoss: 533.1329\n",
      "Training Epoch: 41 [16050/36045]\tLoss: 502.6074\n",
      "Training Epoch: 41 [16100/36045]\tLoss: 466.3501\n",
      "Training Epoch: 41 [16150/36045]\tLoss: 454.3645\n",
      "Training Epoch: 41 [16200/36045]\tLoss: 551.7116\n",
      "Training Epoch: 41 [16250/36045]\tLoss: 579.3547\n",
      "Training Epoch: 41 [16300/36045]\tLoss: 632.4564\n",
      "Training Epoch: 41 [16350/36045]\tLoss: 652.5332\n",
      "Training Epoch: 41 [16400/36045]\tLoss: 624.2913\n",
      "Training Epoch: 41 [16450/36045]\tLoss: 606.5894\n",
      "Training Epoch: 41 [16500/36045]\tLoss: 606.4967\n",
      "Training Epoch: 41 [16550/36045]\tLoss: 571.8042\n",
      "Training Epoch: 41 [16600/36045]\tLoss: 594.0153\n",
      "Training Epoch: 41 [16650/36045]\tLoss: 610.2501\n",
      "Training Epoch: 41 [16700/36045]\tLoss: 590.2242\n",
      "Training Epoch: 41 [16750/36045]\tLoss: 582.9195\n",
      "Training Epoch: 41 [16800/36045]\tLoss: 591.8530\n",
      "Training Epoch: 41 [16850/36045]\tLoss: 563.9992\n",
      "Training Epoch: 41 [16900/36045]\tLoss: 573.7656\n",
      "Training Epoch: 41 [16950/36045]\tLoss: 596.6752\n",
      "Training Epoch: 41 [17000/36045]\tLoss: 580.9351\n",
      "Training Epoch: 41 [17050/36045]\tLoss: 605.1149\n",
      "Training Epoch: 41 [17100/36045]\tLoss: 601.0809\n",
      "Training Epoch: 41 [17150/36045]\tLoss: 521.9019\n",
      "Training Epoch: 41 [17200/36045]\tLoss: 484.1696\n",
      "Training Epoch: 41 [17250/36045]\tLoss: 507.0321\n",
      "Training Epoch: 41 [17300/36045]\tLoss: 536.6334\n",
      "Training Epoch: 41 [17350/36045]\tLoss: 517.3453\n",
      "Training Epoch: 41 [17400/36045]\tLoss: 536.8546\n",
      "Training Epoch: 41 [17450/36045]\tLoss: 555.4103\n",
      "Training Epoch: 41 [17500/36045]\tLoss: 543.7606\n",
      "Training Epoch: 41 [17550/36045]\tLoss: 542.3879\n",
      "Training Epoch: 41 [17600/36045]\tLoss: 536.7651\n",
      "Training Epoch: 41 [17650/36045]\tLoss: 552.3489\n",
      "Training Epoch: 41 [17700/36045]\tLoss: 531.7003\n",
      "Training Epoch: 41 [17750/36045]\tLoss: 547.6854\n",
      "Training Epoch: 41 [17800/36045]\tLoss: 538.8191\n",
      "Training Epoch: 41 [17850/36045]\tLoss: 554.4100\n",
      "Training Epoch: 41 [17900/36045]\tLoss: 582.4375\n",
      "Training Epoch: 41 [17950/36045]\tLoss: 594.6497\n",
      "Training Epoch: 41 [18000/36045]\tLoss: 585.2394\n",
      "Training Epoch: 41 [18050/36045]\tLoss: 642.9119\n",
      "Training Epoch: 41 [18100/36045]\tLoss: 644.1927\n",
      "Training Epoch: 41 [18150/36045]\tLoss: 655.4705\n",
      "Training Epoch: 41 [18200/36045]\tLoss: 637.7236\n",
      "Training Epoch: 41 [18250/36045]\tLoss: 658.4447\n",
      "Training Epoch: 41 [18300/36045]\tLoss: 613.6382\n",
      "Training Epoch: 41 [18350/36045]\tLoss: 686.4850\n",
      "Training Epoch: 41 [18400/36045]\tLoss: 659.4336\n",
      "Training Epoch: 41 [18450/36045]\tLoss: 639.2188\n",
      "Training Epoch: 41 [18500/36045]\tLoss: 638.0709\n",
      "Training Epoch: 41 [18550/36045]\tLoss: 625.5381\n",
      "Training Epoch: 41 [18600/36045]\tLoss: 615.1895\n",
      "Training Epoch: 41 [18650/36045]\tLoss: 660.7971\n",
      "Training Epoch: 41 [18700/36045]\tLoss: 695.1973\n",
      "Training Epoch: 41 [18750/36045]\tLoss: 682.6680\n",
      "Training Epoch: 41 [18800/36045]\tLoss: 705.4568\n",
      "Training Epoch: 41 [18850/36045]\tLoss: 650.5465\n",
      "Training Epoch: 41 [18900/36045]\tLoss: 695.7879\n",
      "Training Epoch: 41 [18950/36045]\tLoss: 638.3445\n",
      "Training Epoch: 41 [19000/36045]\tLoss: 525.5540\n",
      "Training Epoch: 41 [19050/36045]\tLoss: 509.9874\n",
      "Training Epoch: 41 [19100/36045]\tLoss: 518.1503\n",
      "Training Epoch: 41 [19150/36045]\tLoss: 508.2708\n",
      "Training Epoch: 41 [19200/36045]\tLoss: 538.9985\n",
      "Training Epoch: 41 [19250/36045]\tLoss: 554.0997\n",
      "Training Epoch: 41 [19300/36045]\tLoss: 563.4943\n",
      "Training Epoch: 41 [19350/36045]\tLoss: 547.2673\n",
      "Training Epoch: 41 [19400/36045]\tLoss: 567.6373\n",
      "Training Epoch: 41 [19450/36045]\tLoss: 559.0087\n",
      "Training Epoch: 41 [19500/36045]\tLoss: 560.3529\n",
      "Training Epoch: 41 [19550/36045]\tLoss: 558.5683\n",
      "Training Epoch: 41 [19600/36045]\tLoss: 599.4095\n",
      "Training Epoch: 41 [19650/36045]\tLoss: 800.3416\n",
      "Training Epoch: 41 [19700/36045]\tLoss: 759.0677\n",
      "Training Epoch: 41 [19750/36045]\tLoss: 763.1076\n",
      "Training Epoch: 41 [19800/36045]\tLoss: 763.4304\n",
      "Training Epoch: 41 [19850/36045]\tLoss: 500.0991\n",
      "Training Epoch: 41 [19900/36045]\tLoss: 479.1215\n",
      "Training Epoch: 41 [19950/36045]\tLoss: 482.8797\n",
      "Training Epoch: 41 [20000/36045]\tLoss: 482.6249\n",
      "Training Epoch: 41 [20050/36045]\tLoss: 540.2285\n",
      "Training Epoch: 41 [20100/36045]\tLoss: 545.8320\n",
      "Training Epoch: 41 [20150/36045]\tLoss: 547.0464\n",
      "Training Epoch: 41 [20200/36045]\tLoss: 546.7518\n",
      "Training Epoch: 41 [20250/36045]\tLoss: 583.0240\n",
      "Training Epoch: 41 [20300/36045]\tLoss: 619.0753\n",
      "Training Epoch: 41 [20350/36045]\tLoss: 637.3831\n",
      "Training Epoch: 41 [20400/36045]\tLoss: 653.8655\n",
      "Training Epoch: 41 [20450/36045]\tLoss: 623.8198\n",
      "Training Epoch: 41 [20500/36045]\tLoss: 608.3789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 41 [20550/36045]\tLoss: 534.6152\n",
      "Training Epoch: 41 [20600/36045]\tLoss: 544.5285\n",
      "Training Epoch: 41 [20650/36045]\tLoss: 541.9144\n",
      "Training Epoch: 41 [20700/36045]\tLoss: 529.9791\n",
      "Training Epoch: 41 [20750/36045]\tLoss: 571.4100\n",
      "Training Epoch: 41 [20800/36045]\tLoss: 620.7728\n",
      "Training Epoch: 41 [20850/36045]\tLoss: 607.5735\n",
      "Training Epoch: 41 [20900/36045]\tLoss: 650.8954\n",
      "Training Epoch: 41 [20950/36045]\tLoss: 613.4594\n",
      "Training Epoch: 41 [21000/36045]\tLoss: 577.3526\n",
      "Training Epoch: 41 [21050/36045]\tLoss: 493.8230\n",
      "Training Epoch: 41 [21100/36045]\tLoss: 498.5978\n",
      "Training Epoch: 41 [21150/36045]\tLoss: 533.7019\n",
      "Training Epoch: 41 [21200/36045]\tLoss: 532.8636\n",
      "Training Epoch: 41 [21250/36045]\tLoss: 509.6672\n",
      "Training Epoch: 41 [21300/36045]\tLoss: 595.0472\n",
      "Training Epoch: 41 [21350/36045]\tLoss: 586.8444\n",
      "Training Epoch: 41 [21400/36045]\tLoss: 590.5323\n",
      "Training Epoch: 41 [21450/36045]\tLoss: 596.7512\n",
      "Training Epoch: 41 [21500/36045]\tLoss: 598.5734\n",
      "Training Epoch: 41 [21550/36045]\tLoss: 692.8826\n",
      "Training Epoch: 41 [21600/36045]\tLoss: 691.3764\n",
      "Training Epoch: 41 [21650/36045]\tLoss: 703.6942\n",
      "Training Epoch: 41 [21700/36045]\tLoss: 706.8893\n",
      "Training Epoch: 41 [21750/36045]\tLoss: 678.7099\n",
      "Training Epoch: 41 [21800/36045]\tLoss: 498.9022\n",
      "Training Epoch: 41 [21850/36045]\tLoss: 481.8930\n",
      "Training Epoch: 41 [21900/36045]\tLoss: 491.1722\n",
      "Training Epoch: 41 [21950/36045]\tLoss: 492.3013\n",
      "Training Epoch: 41 [22000/36045]\tLoss: 495.6009\n",
      "Training Epoch: 41 [22050/36045]\tLoss: 515.7595\n",
      "Training Epoch: 41 [22100/36045]\tLoss: 508.3035\n",
      "Training Epoch: 41 [22150/36045]\tLoss: 494.5253\n",
      "Training Epoch: 41 [22200/36045]\tLoss: 510.3104\n",
      "Training Epoch: 41 [22250/36045]\tLoss: 515.3032\n",
      "Training Epoch: 41 [22300/36045]\tLoss: 567.2429\n",
      "Training Epoch: 41 [22350/36045]\tLoss: 592.9001\n",
      "Training Epoch: 41 [22400/36045]\tLoss: 606.9369\n",
      "Training Epoch: 41 [22450/36045]\tLoss: 594.1923\n",
      "Training Epoch: 41 [22500/36045]\tLoss: 577.0978\n",
      "Training Epoch: 41 [22550/36045]\tLoss: 612.1796\n",
      "Training Epoch: 41 [22600/36045]\tLoss: 661.7396\n",
      "Training Epoch: 41 [22650/36045]\tLoss: 695.1301\n",
      "Training Epoch: 41 [22700/36045]\tLoss: 716.6017\n",
      "Training Epoch: 41 [22750/36045]\tLoss: 735.9984\n",
      "Training Epoch: 41 [22800/36045]\tLoss: 764.9870\n",
      "Training Epoch: 41 [22850/36045]\tLoss: 635.6064\n",
      "Training Epoch: 41 [22900/36045]\tLoss: 640.4833\n",
      "Training Epoch: 41 [22950/36045]\tLoss: 620.0158\n",
      "Training Epoch: 41 [23000/36045]\tLoss: 616.5474\n",
      "Training Epoch: 41 [23050/36045]\tLoss: 547.6390\n",
      "Training Epoch: 41 [23100/36045]\tLoss: 563.2769\n",
      "Training Epoch: 41 [23150/36045]\tLoss: 551.6580\n",
      "Training Epoch: 41 [23200/36045]\tLoss: 522.2297\n",
      "Training Epoch: 41 [23250/36045]\tLoss: 525.2281\n",
      "Training Epoch: 41 [23300/36045]\tLoss: 521.2310\n",
      "Training Epoch: 41 [23350/36045]\tLoss: 541.7253\n",
      "Training Epoch: 41 [23400/36045]\tLoss: 587.4612\n",
      "Training Epoch: 41 [23450/36045]\tLoss: 581.0665\n",
      "Training Epoch: 41 [23500/36045]\tLoss: 559.9445\n",
      "Training Epoch: 41 [23550/36045]\tLoss: 600.0175\n",
      "Training Epoch: 41 [23600/36045]\tLoss: 680.6345\n",
      "Training Epoch: 41 [23650/36045]\tLoss: 692.4265\n",
      "Training Epoch: 41 [23700/36045]\tLoss: 700.4366\n",
      "Training Epoch: 41 [23750/36045]\tLoss: 676.6926\n",
      "Training Epoch: 41 [23800/36045]\tLoss: 542.3645\n",
      "Training Epoch: 41 [23850/36045]\tLoss: 568.1898\n",
      "Training Epoch: 41 [23900/36045]\tLoss: 557.9836\n",
      "Training Epoch: 41 [23950/36045]\tLoss: 541.2087\n",
      "Training Epoch: 41 [24000/36045]\tLoss: 518.2950\n",
      "Training Epoch: 41 [24050/36045]\tLoss: 478.5348\n",
      "Training Epoch: 41 [24100/36045]\tLoss: 503.2478\n",
      "Training Epoch: 41 [24150/36045]\tLoss: 495.5332\n",
      "Training Epoch: 41 [24200/36045]\tLoss: 492.9437\n",
      "Training Epoch: 41 [24250/36045]\tLoss: 478.7944\n",
      "Training Epoch: 41 [24300/36045]\tLoss: 517.3492\n",
      "Training Epoch: 41 [24350/36045]\tLoss: 529.6400\n",
      "Training Epoch: 41 [24400/36045]\tLoss: 544.5057\n",
      "Training Epoch: 41 [24450/36045]\tLoss: 518.8928\n",
      "Training Epoch: 41 [24500/36045]\tLoss: 547.7080\n",
      "Training Epoch: 41 [24550/36045]\tLoss: 635.0059\n",
      "Training Epoch: 41 [24600/36045]\tLoss: 626.4520\n",
      "Training Epoch: 41 [24650/36045]\tLoss: 599.5977\n",
      "Training Epoch: 41 [24700/36045]\tLoss: 609.3937\n",
      "Training Epoch: 41 [24750/36045]\tLoss: 563.2719\n",
      "Training Epoch: 41 [24800/36045]\tLoss: 462.0920\n",
      "Training Epoch: 41 [24850/36045]\tLoss: 480.6507\n",
      "Training Epoch: 41 [24900/36045]\tLoss: 478.0800\n",
      "Training Epoch: 41 [24950/36045]\tLoss: 480.5656\n",
      "Training Epoch: 41 [25000/36045]\tLoss: 462.0110\n",
      "Training Epoch: 41 [25050/36045]\tLoss: 441.6606\n",
      "Training Epoch: 41 [25100/36045]\tLoss: 395.7168\n",
      "Training Epoch: 41 [25150/36045]\tLoss: 366.4005\n",
      "Training Epoch: 41 [25200/36045]\tLoss: 361.3805\n",
      "Training Epoch: 41 [25250/36045]\tLoss: 387.3970\n",
      "Training Epoch: 41 [25300/36045]\tLoss: 509.0018\n",
      "Training Epoch: 41 [25350/36045]\tLoss: 505.2885\n",
      "Training Epoch: 41 [25400/36045]\tLoss: 472.1129\n",
      "Training Epoch: 41 [25450/36045]\tLoss: 474.2419\n",
      "Training Epoch: 41 [25500/36045]\tLoss: 515.2021\n",
      "Training Epoch: 41 [25550/36045]\tLoss: 602.8024\n",
      "Training Epoch: 41 [25600/36045]\tLoss: 606.8939\n",
      "Training Epoch: 41 [25650/36045]\tLoss: 585.7083\n",
      "Training Epoch: 41 [25700/36045]\tLoss: 595.1451\n",
      "Training Epoch: 41 [25750/36045]\tLoss: 574.9330\n",
      "Training Epoch: 41 [25800/36045]\tLoss: 362.9287\n",
      "Training Epoch: 41 [25850/36045]\tLoss: 371.0338\n",
      "Training Epoch: 41 [25900/36045]\tLoss: 352.4014\n",
      "Training Epoch: 41 [25950/36045]\tLoss: 361.6545\n",
      "Training Epoch: 41 [26000/36045]\tLoss: 444.6672\n",
      "Training Epoch: 41 [26050/36045]\tLoss: 605.3107\n",
      "Training Epoch: 41 [26100/36045]\tLoss: 631.6115\n",
      "Training Epoch: 41 [26150/36045]\tLoss: 631.1572\n",
      "Training Epoch: 41 [26200/36045]\tLoss: 604.3869\n",
      "Training Epoch: 41 [26250/36045]\tLoss: 634.3099\n",
      "Training Epoch: 41 [26300/36045]\tLoss: 581.1893\n",
      "Training Epoch: 41 [26350/36045]\tLoss: 592.1982\n",
      "Training Epoch: 41 [26400/36045]\tLoss: 567.8860\n",
      "Training Epoch: 41 [26450/36045]\tLoss: 497.8187\n",
      "Training Epoch: 41 [26500/36045]\tLoss: 589.5347\n",
      "Training Epoch: 41 [26550/36045]\tLoss: 589.4255\n",
      "Training Epoch: 41 [26600/36045]\tLoss: 585.9972\n",
      "Training Epoch: 41 [26650/36045]\tLoss: 601.2052\n",
      "Training Epoch: 41 [26700/36045]\tLoss: 580.2623\n",
      "Training Epoch: 41 [26750/36045]\tLoss: 543.9039\n",
      "Training Epoch: 41 [26800/36045]\tLoss: 401.6518\n",
      "Training Epoch: 41 [26850/36045]\tLoss: 332.7840\n",
      "Training Epoch: 41 [26900/36045]\tLoss: 334.9718\n",
      "Training Epoch: 41 [26950/36045]\tLoss: 367.7101\n",
      "Training Epoch: 41 [27000/36045]\tLoss: 604.6716\n",
      "Training Epoch: 41 [27050/36045]\tLoss: 631.1570\n",
      "Training Epoch: 41 [27100/36045]\tLoss: 611.8838\n",
      "Training Epoch: 41 [27150/36045]\tLoss: 651.1852\n",
      "Training Epoch: 41 [27200/36045]\tLoss: 472.8975\n",
      "Training Epoch: 41 [27250/36045]\tLoss: 463.6284\n",
      "Training Epoch: 41 [27300/36045]\tLoss: 451.9860\n",
      "Training Epoch: 41 [27350/36045]\tLoss: 449.5914\n",
      "Training Epoch: 41 [27400/36045]\tLoss: 448.9906\n",
      "Training Epoch: 41 [27450/36045]\tLoss: 568.0934\n",
      "Training Epoch: 41 [27500/36045]\tLoss: 608.7415\n",
      "Training Epoch: 41 [27550/36045]\tLoss: 602.1027\n",
      "Training Epoch: 41 [27600/36045]\tLoss: 613.6211\n",
      "Training Epoch: 41 [27650/36045]\tLoss: 605.6091\n",
      "Training Epoch: 41 [27700/36045]\tLoss: 633.1932\n",
      "Training Epoch: 41 [27750/36045]\tLoss: 645.1318\n",
      "Training Epoch: 41 [27800/36045]\tLoss: 632.3540\n",
      "Training Epoch: 41 [27850/36045]\tLoss: 622.6559\n",
      "Training Epoch: 41 [27900/36045]\tLoss: 565.5868\n",
      "Training Epoch: 41 [27950/36045]\tLoss: 472.7729\n",
      "Training Epoch: 41 [28000/36045]\tLoss: 449.5863\n",
      "Training Epoch: 41 [28050/36045]\tLoss: 459.1130\n",
      "Training Epoch: 41 [28100/36045]\tLoss: 450.7657\n",
      "Training Epoch: 41 [28150/36045]\tLoss: 470.4970\n",
      "Training Epoch: 41 [28200/36045]\tLoss: 478.8085\n",
      "Training Epoch: 41 [28250/36045]\tLoss: 471.7555\n",
      "Training Epoch: 41 [28300/36045]\tLoss: 447.8128\n",
      "Training Epoch: 41 [28350/36045]\tLoss: 443.6197\n",
      "Training Epoch: 41 [28400/36045]\tLoss: 771.5040\n",
      "Training Epoch: 41 [28450/36045]\tLoss: 707.9506\n",
      "Training Epoch: 41 [28500/36045]\tLoss: 611.7951\n",
      "Training Epoch: 41 [28550/36045]\tLoss: 562.4619\n",
      "Training Epoch: 41 [28600/36045]\tLoss: 588.0316\n",
      "Training Epoch: 41 [28650/36045]\tLoss: 643.6168\n",
      "Training Epoch: 41 [28700/36045]\tLoss: 637.5377\n",
      "Training Epoch: 41 [28750/36045]\tLoss: 624.4402\n",
      "Training Epoch: 41 [28800/36045]\tLoss: 634.1536\n",
      "Training Epoch: 41 [28850/36045]\tLoss: 550.6508\n",
      "Training Epoch: 41 [28900/36045]\tLoss: 448.0089\n",
      "Training Epoch: 41 [28950/36045]\tLoss: 447.1310\n",
      "Training Epoch: 41 [29000/36045]\tLoss: 443.3130\n",
      "Training Epoch: 41 [29050/36045]\tLoss: 450.2479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 41 [29100/36045]\tLoss: 468.3617\n",
      "Training Epoch: 41 [29150/36045]\tLoss: 458.5472\n",
      "Training Epoch: 41 [29200/36045]\tLoss: 444.3622\n",
      "Training Epoch: 41 [29250/36045]\tLoss: 434.2467\n",
      "Training Epoch: 41 [29300/36045]\tLoss: 489.0404\n",
      "Training Epoch: 41 [29350/36045]\tLoss: 574.5422\n",
      "Training Epoch: 41 [29400/36045]\tLoss: 592.0576\n",
      "Training Epoch: 41 [29450/36045]\tLoss: 608.9307\n",
      "Training Epoch: 41 [29500/36045]\tLoss: 623.8472\n",
      "Training Epoch: 41 [29550/36045]\tLoss: 593.5375\n",
      "Training Epoch: 41 [29600/36045]\tLoss: 499.5016\n",
      "Training Epoch: 41 [29650/36045]\tLoss: 481.0972\n",
      "Training Epoch: 41 [29700/36045]\tLoss: 431.5831\n",
      "Training Epoch: 41 [29750/36045]\tLoss: 430.0707\n",
      "Training Epoch: 41 [29800/36045]\tLoss: 477.2582\n",
      "Training Epoch: 41 [29850/36045]\tLoss: 554.3298\n",
      "Training Epoch: 41 [29900/36045]\tLoss: 550.0655\n",
      "Training Epoch: 41 [29950/36045]\tLoss: 571.7647\n",
      "Training Epoch: 41 [30000/36045]\tLoss: 545.7944\n",
      "Training Epoch: 41 [30050/36045]\tLoss: 552.8573\n",
      "Training Epoch: 41 [30100/36045]\tLoss: 674.5112\n",
      "Training Epoch: 41 [30150/36045]\tLoss: 657.0892\n",
      "Training Epoch: 41 [30200/36045]\tLoss: 619.6429\n",
      "Training Epoch: 41 [30250/36045]\tLoss: 668.0375\n",
      "Training Epoch: 41 [30300/36045]\tLoss: 652.4062\n",
      "Training Epoch: 41 [30350/36045]\tLoss: 499.0836\n",
      "Training Epoch: 41 [30400/36045]\tLoss: 483.7421\n",
      "Training Epoch: 41 [30450/36045]\tLoss: 486.0407\n",
      "Training Epoch: 41 [30500/36045]\tLoss: 453.5787\n",
      "Training Epoch: 41 [30550/36045]\tLoss: 420.7388\n",
      "Training Epoch: 41 [30600/36045]\tLoss: 413.0792\n",
      "Training Epoch: 41 [30650/36045]\tLoss: 403.1948\n",
      "Training Epoch: 41 [30700/36045]\tLoss: 420.9645\n",
      "Training Epoch: 41 [30750/36045]\tLoss: 407.9400\n",
      "Training Epoch: 41 [30800/36045]\tLoss: 434.0386\n",
      "Training Epoch: 41 [30850/36045]\tLoss: 425.7495\n",
      "Training Epoch: 41 [30900/36045]\tLoss: 437.7067\n",
      "Training Epoch: 41 [30950/36045]\tLoss: 459.9763\n",
      "Training Epoch: 41 [31000/36045]\tLoss: 452.3382\n",
      "Training Epoch: 41 [31050/36045]\tLoss: 377.7086\n",
      "Training Epoch: 41 [31100/36045]\tLoss: 368.3282\n",
      "Training Epoch: 41 [31150/36045]\tLoss: 375.9547\n",
      "Training Epoch: 41 [31200/36045]\tLoss: 466.8982\n",
      "Training Epoch: 41 [31250/36045]\tLoss: 605.8605\n",
      "Training Epoch: 41 [31300/36045]\tLoss: 577.0339\n",
      "Training Epoch: 41 [31350/36045]\tLoss: 592.9385\n",
      "Training Epoch: 41 [31400/36045]\tLoss: 572.4783\n",
      "Training Epoch: 41 [31450/36045]\tLoss: 589.1284\n",
      "Training Epoch: 41 [31500/36045]\tLoss: 601.7271\n",
      "Training Epoch: 41 [31550/36045]\tLoss: 608.6549\n",
      "Training Epoch: 41 [31600/36045]\tLoss: 572.2167\n",
      "Training Epoch: 41 [31650/36045]\tLoss: 612.5594\n",
      "Training Epoch: 41 [31700/36045]\tLoss: 442.9330\n",
      "Training Epoch: 41 [31750/36045]\tLoss: 366.0591\n",
      "Training Epoch: 41 [31800/36045]\tLoss: 349.4269\n",
      "Training Epoch: 41 [31850/36045]\tLoss: 357.2551\n",
      "Training Epoch: 41 [31900/36045]\tLoss: 564.7855\n",
      "Training Epoch: 41 [31950/36045]\tLoss: 731.6186\n",
      "Training Epoch: 41 [32000/36045]\tLoss: 839.5344\n",
      "Training Epoch: 41 [32050/36045]\tLoss: 795.1537\n",
      "Training Epoch: 41 [32100/36045]\tLoss: 786.1553\n",
      "Training Epoch: 41 [32150/36045]\tLoss: 604.0529\n",
      "Training Epoch: 41 [32200/36045]\tLoss: 606.2648\n",
      "Training Epoch: 41 [32250/36045]\tLoss: 616.5717\n",
      "Training Epoch: 41 [32300/36045]\tLoss: 598.6048\n",
      "Training Epoch: 41 [32350/36045]\tLoss: 594.5537\n",
      "Training Epoch: 41 [32400/36045]\tLoss: 557.8623\n",
      "Training Epoch: 41 [32450/36045]\tLoss: 459.2965\n",
      "Training Epoch: 41 [32500/36045]\tLoss: 441.3108\n",
      "Training Epoch: 41 [32550/36045]\tLoss: 443.4645\n",
      "Training Epoch: 41 [32600/36045]\tLoss: 440.5347\n",
      "Training Epoch: 41 [32650/36045]\tLoss: 570.4705\n",
      "Training Epoch: 41 [32700/36045]\tLoss: 623.0389\n",
      "Training Epoch: 41 [32750/36045]\tLoss: 593.3259\n",
      "Training Epoch: 41 [32800/36045]\tLoss: 608.3791\n",
      "Training Epoch: 41 [32850/36045]\tLoss: 561.6149\n",
      "Training Epoch: 41 [32900/36045]\tLoss: 449.0786\n",
      "Training Epoch: 41 [32950/36045]\tLoss: 470.4007\n",
      "Training Epoch: 41 [33000/36045]\tLoss: 469.1650\n",
      "Training Epoch: 41 [33050/36045]\tLoss: 446.2390\n",
      "Training Epoch: 41 [33100/36045]\tLoss: 507.0958\n",
      "Training Epoch: 41 [33150/36045]\tLoss: 689.9818\n",
      "Training Epoch: 41 [33200/36045]\tLoss: 671.8030\n",
      "Training Epoch: 41 [33250/36045]\tLoss: 692.2985\n",
      "Training Epoch: 41 [33300/36045]\tLoss: 737.7412\n",
      "Training Epoch: 41 [33350/36045]\tLoss: 564.9373\n",
      "Training Epoch: 41 [33400/36045]\tLoss: 411.6768\n",
      "Training Epoch: 41 [33450/36045]\tLoss: 407.4922\n",
      "Training Epoch: 41 [33500/36045]\tLoss: 419.5222\n",
      "Training Epoch: 41 [33550/36045]\tLoss: 434.5613\n",
      "Training Epoch: 41 [33600/36045]\tLoss: 436.3396\n",
      "Training Epoch: 41 [33650/36045]\tLoss: 582.8325\n",
      "Training Epoch: 41 [33700/36045]\tLoss: 563.7608\n",
      "Training Epoch: 41 [33750/36045]\tLoss: 583.8359\n",
      "Training Epoch: 41 [33800/36045]\tLoss: 580.9479\n",
      "Training Epoch: 41 [33850/36045]\tLoss: 582.8658\n",
      "Training Epoch: 41 [33900/36045]\tLoss: 596.3184\n",
      "Training Epoch: 41 [33950/36045]\tLoss: 606.0506\n",
      "Training Epoch: 41 [34000/36045]\tLoss: 592.5836\n",
      "Training Epoch: 41 [34050/36045]\tLoss: 596.4244\n",
      "Training Epoch: 41 [34100/36045]\tLoss: 575.0220\n",
      "Training Epoch: 41 [34150/36045]\tLoss: 532.8669\n",
      "Training Epoch: 41 [34200/36045]\tLoss: 504.7306\n",
      "Training Epoch: 41 [34250/36045]\tLoss: 518.7614\n",
      "Training Epoch: 41 [34300/36045]\tLoss: 442.7904\n",
      "Training Epoch: 41 [34350/36045]\tLoss: 466.4058\n",
      "Training Epoch: 41 [34400/36045]\tLoss: 459.1996\n",
      "Training Epoch: 41 [34450/36045]\tLoss: 432.3253\n",
      "Training Epoch: 41 [34500/36045]\tLoss: 461.1862\n",
      "Training Epoch: 41 [34550/36045]\tLoss: 452.6819\n",
      "Training Epoch: 41 [34600/36045]\tLoss: 459.1799\n",
      "Training Epoch: 41 [34650/36045]\tLoss: 565.1031\n",
      "Training Epoch: 41 [34700/36045]\tLoss: 599.2878\n",
      "Training Epoch: 41 [34750/36045]\tLoss: 531.1639\n",
      "Training Epoch: 41 [34800/36045]\tLoss: 609.8251\n",
      "Training Epoch: 41 [34850/36045]\tLoss: 617.0472\n",
      "Training Epoch: 41 [34900/36045]\tLoss: 666.7966\n",
      "Training Epoch: 41 [34950/36045]\tLoss: 651.6879\n",
      "Training Epoch: 41 [35000/36045]\tLoss: 652.7595\n",
      "Training Epoch: 41 [35050/36045]\tLoss: 639.9176\n",
      "Training Epoch: 41 [35100/36045]\tLoss: 551.3135\n",
      "Training Epoch: 41 [35150/36045]\tLoss: 543.6307\n",
      "Training Epoch: 41 [35200/36045]\tLoss: 457.4041\n",
      "Training Epoch: 41 [35250/36045]\tLoss: 502.5096\n",
      "Training Epoch: 41 [35300/36045]\tLoss: 518.3693\n",
      "Training Epoch: 41 [35350/36045]\tLoss: 580.5864\n",
      "Training Epoch: 41 [35400/36045]\tLoss: 611.1514\n",
      "Training Epoch: 41 [35450/36045]\tLoss: 582.2837\n",
      "Training Epoch: 41 [35500/36045]\tLoss: 563.3260\n",
      "Training Epoch: 41 [35550/36045]\tLoss: 549.4985\n",
      "Training Epoch: 41 [35600/36045]\tLoss: 599.8260\n",
      "Training Epoch: 41 [35650/36045]\tLoss: 672.5321\n",
      "Training Epoch: 41 [35700/36045]\tLoss: 599.4787\n",
      "Training Epoch: 41 [35750/36045]\tLoss: 656.6430\n",
      "Training Epoch: 41 [35800/36045]\tLoss: 663.1779\n",
      "Training Epoch: 41 [35850/36045]\tLoss: 637.3810\n",
      "Training Epoch: 41 [35900/36045]\tLoss: 660.2117\n",
      "Training Epoch: 41 [35950/36045]\tLoss: 657.1526\n",
      "Training Epoch: 41 [36000/36045]\tLoss: 650.0175\n",
      "Training Epoch: 41 [36045/36045]\tLoss: 635.0559\n",
      "Training Epoch: 41 [4004/4004]\tLoss: 589.7406\n",
      "Training Epoch: 42 [50/36045]\tLoss: 588.3922\n",
      "Training Epoch: 42 [100/36045]\tLoss: 564.1139\n",
      "Training Epoch: 42 [150/36045]\tLoss: 562.2443\n",
      "Training Epoch: 42 [200/36045]\tLoss: 548.4883\n",
      "Training Epoch: 42 [250/36045]\tLoss: 658.6741\n",
      "Training Epoch: 42 [300/36045]\tLoss: 723.5089\n",
      "Training Epoch: 42 [350/36045]\tLoss: 690.2664\n",
      "Training Epoch: 42 [400/36045]\tLoss: 684.9889\n",
      "Training Epoch: 42 [450/36045]\tLoss: 665.7710\n",
      "Training Epoch: 42 [500/36045]\tLoss: 616.5797\n",
      "Training Epoch: 42 [550/36045]\tLoss: 619.3336\n",
      "Training Epoch: 42 [600/36045]\tLoss: 604.6420\n",
      "Training Epoch: 42 [650/36045]\tLoss: 626.2965\n",
      "Training Epoch: 42 [700/36045]\tLoss: 611.9875\n",
      "Training Epoch: 42 [750/36045]\tLoss: 589.1274\n",
      "Training Epoch: 42 [800/36045]\tLoss: 600.8294\n",
      "Training Epoch: 42 [850/36045]\tLoss: 583.4661\n",
      "Training Epoch: 42 [900/36045]\tLoss: 558.5064\n",
      "Training Epoch: 42 [950/36045]\tLoss: 528.6238\n",
      "Training Epoch: 42 [1000/36045]\tLoss: 512.2542\n",
      "Training Epoch: 42 [1050/36045]\tLoss: 514.1704\n",
      "Training Epoch: 42 [1100/36045]\tLoss: 500.0992\n",
      "Training Epoch: 42 [1150/36045]\tLoss: 509.1744\n",
      "Training Epoch: 42 [1200/36045]\tLoss: 538.9860\n",
      "Training Epoch: 42 [1250/36045]\tLoss: 617.0065\n",
      "Training Epoch: 42 [1300/36045]\tLoss: 624.0376\n",
      "Training Epoch: 42 [1350/36045]\tLoss: 625.2924\n",
      "Training Epoch: 42 [1400/36045]\tLoss: 649.2546\n",
      "Training Epoch: 42 [1450/36045]\tLoss: 628.1036\n",
      "Training Epoch: 42 [1500/36045]\tLoss: 574.2689\n",
      "Training Epoch: 42 [1550/36045]\tLoss: 589.1967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 42 [1600/36045]\tLoss: 599.4330\n",
      "Training Epoch: 42 [1650/36045]\tLoss: 586.6344\n",
      "Training Epoch: 42 [1700/36045]\tLoss: 598.7428\n",
      "Training Epoch: 42 [1750/36045]\tLoss: 640.1318\n",
      "Training Epoch: 42 [1800/36045]\tLoss: 622.6072\n",
      "Training Epoch: 42 [1850/36045]\tLoss: 638.7106\n",
      "Training Epoch: 42 [1900/36045]\tLoss: 597.5079\n",
      "Training Epoch: 42 [1950/36045]\tLoss: 607.5408\n",
      "Training Epoch: 42 [2000/36045]\tLoss: 547.4732\n",
      "Training Epoch: 42 [2050/36045]\tLoss: 549.9151\n",
      "Training Epoch: 42 [2100/36045]\tLoss: 579.5848\n",
      "Training Epoch: 42 [2150/36045]\tLoss: 560.4719\n",
      "Training Epoch: 42 [2200/36045]\tLoss: 522.2526\n",
      "Training Epoch: 42 [2250/36045]\tLoss: 493.1867\n",
      "Training Epoch: 42 [2300/36045]\tLoss: 517.0065\n",
      "Training Epoch: 42 [2350/36045]\tLoss: 494.1521\n",
      "Training Epoch: 42 [2400/36045]\tLoss: 501.8432\n",
      "Training Epoch: 42 [2450/36045]\tLoss: 642.5793\n",
      "Training Epoch: 42 [2500/36045]\tLoss: 674.9351\n",
      "Training Epoch: 42 [2550/36045]\tLoss: 672.6709\n",
      "Training Epoch: 42 [2600/36045]\tLoss: 681.1773\n",
      "Training Epoch: 42 [2650/36045]\tLoss: 805.5059\n",
      "Training Epoch: 42 [2700/36045]\tLoss: 893.2388\n",
      "Training Epoch: 42 [2750/36045]\tLoss: 964.3079\n",
      "Training Epoch: 42 [2800/36045]\tLoss: 973.8847\n",
      "Training Epoch: 42 [2850/36045]\tLoss: 741.4468\n",
      "Training Epoch: 42 [2900/36045]\tLoss: 702.8032\n",
      "Training Epoch: 42 [2950/36045]\tLoss: 679.5126\n",
      "Training Epoch: 42 [3000/36045]\tLoss: 673.4708\n",
      "Training Epoch: 42 [3050/36045]\tLoss: 704.4096\n",
      "Training Epoch: 42 [3100/36045]\tLoss: 644.0057\n",
      "Training Epoch: 42 [3150/36045]\tLoss: 495.6239\n",
      "Training Epoch: 42 [3200/36045]\tLoss: 513.0951\n",
      "Training Epoch: 42 [3250/36045]\tLoss: 483.7866\n",
      "Training Epoch: 42 [3300/36045]\tLoss: 458.0812\n",
      "Training Epoch: 42 [3350/36045]\tLoss: 483.9768\n",
      "Training Epoch: 42 [3400/36045]\tLoss: 507.2954\n",
      "Training Epoch: 42 [3450/36045]\tLoss: 544.3032\n",
      "Training Epoch: 42 [3500/36045]\tLoss: 531.4508\n",
      "Training Epoch: 42 [3550/36045]\tLoss: 508.2557\n",
      "Training Epoch: 42 [3600/36045]\tLoss: 545.9175\n",
      "Training Epoch: 42 [3650/36045]\tLoss: 631.5278\n",
      "Training Epoch: 42 [3700/36045]\tLoss: 639.3036\n",
      "Training Epoch: 42 [3750/36045]\tLoss: 608.4611\n",
      "Training Epoch: 42 [3800/36045]\tLoss: 604.9680\n",
      "Training Epoch: 42 [3850/36045]\tLoss: 606.0615\n",
      "Training Epoch: 42 [3900/36045]\tLoss: 610.8484\n",
      "Training Epoch: 42 [3950/36045]\tLoss: 589.6118\n",
      "Training Epoch: 42 [4000/36045]\tLoss: 594.4629\n",
      "Training Epoch: 42 [4050/36045]\tLoss: 545.2153\n",
      "Training Epoch: 42 [4100/36045]\tLoss: 531.5939\n",
      "Training Epoch: 42 [4150/36045]\tLoss: 545.9253\n",
      "Training Epoch: 42 [4200/36045]\tLoss: 540.9343\n",
      "Training Epoch: 42 [4250/36045]\tLoss: 543.3515\n",
      "Training Epoch: 42 [4300/36045]\tLoss: 559.2935\n",
      "Training Epoch: 42 [4350/36045]\tLoss: 542.7413\n",
      "Training Epoch: 42 [4400/36045]\tLoss: 519.0291\n",
      "Training Epoch: 42 [4450/36045]\tLoss: 569.8342\n",
      "Training Epoch: 42 [4500/36045]\tLoss: 612.9473\n",
      "Training Epoch: 42 [4550/36045]\tLoss: 616.1179\n",
      "Training Epoch: 42 [4600/36045]\tLoss: 638.1351\n",
      "Training Epoch: 42 [4650/36045]\tLoss: 627.8078\n",
      "Training Epoch: 42 [4700/36045]\tLoss: 578.8980\n",
      "Training Epoch: 42 [4750/36045]\tLoss: 561.1237\n",
      "Training Epoch: 42 [4800/36045]\tLoss: 585.6949\n",
      "Training Epoch: 42 [4850/36045]\tLoss: 572.2150\n",
      "Training Epoch: 42 [4900/36045]\tLoss: 556.9005\n",
      "Training Epoch: 42 [4950/36045]\tLoss: 572.2610\n",
      "Training Epoch: 42 [5000/36045]\tLoss: 601.8138\n",
      "Training Epoch: 42 [5050/36045]\tLoss: 583.1283\n",
      "Training Epoch: 42 [5100/36045]\tLoss: 593.3970\n",
      "Training Epoch: 42 [5150/36045]\tLoss: 577.7180\n",
      "Training Epoch: 42 [5200/36045]\tLoss: 575.6038\n",
      "Training Epoch: 42 [5250/36045]\tLoss: 569.2398\n",
      "Training Epoch: 42 [5300/36045]\tLoss: 569.4501\n",
      "Training Epoch: 42 [5350/36045]\tLoss: 590.9435\n",
      "Training Epoch: 42 [5400/36045]\tLoss: 569.5004\n",
      "Training Epoch: 42 [5450/36045]\tLoss: 539.6904\n",
      "Training Epoch: 42 [5500/36045]\tLoss: 567.8522\n",
      "Training Epoch: 42 [5550/36045]\tLoss: 556.0963\n",
      "Training Epoch: 42 [5600/36045]\tLoss: 635.4497\n",
      "Training Epoch: 42 [5650/36045]\tLoss: 600.9191\n",
      "Training Epoch: 42 [5700/36045]\tLoss: 563.6250\n",
      "Training Epoch: 42 [5750/36045]\tLoss: 547.7133\n",
      "Training Epoch: 42 [5800/36045]\tLoss: 577.8334\n",
      "Training Epoch: 42 [5850/36045]\tLoss: 567.0859\n",
      "Training Epoch: 42 [5900/36045]\tLoss: 651.6918\n",
      "Training Epoch: 42 [5950/36045]\tLoss: 668.2851\n",
      "Training Epoch: 42 [6000/36045]\tLoss: 654.1742\n",
      "Training Epoch: 42 [6050/36045]\tLoss: 632.2066\n",
      "Training Epoch: 42 [6100/36045]\tLoss: 636.4207\n",
      "Training Epoch: 42 [6150/36045]\tLoss: 626.5122\n",
      "Training Epoch: 42 [6200/36045]\tLoss: 630.7147\n",
      "Training Epoch: 42 [6250/36045]\tLoss: 652.2506\n",
      "Training Epoch: 42 [6300/36045]\tLoss: 663.8674\n",
      "Training Epoch: 42 [6350/36045]\tLoss: 709.4033\n",
      "Training Epoch: 42 [6400/36045]\tLoss: 585.1292\n",
      "Training Epoch: 42 [6450/36045]\tLoss: 538.3602\n",
      "Training Epoch: 42 [6500/36045]\tLoss: 548.1920\n",
      "Training Epoch: 42 [6550/36045]\tLoss: 565.6037\n",
      "Training Epoch: 42 [6600/36045]\tLoss: 563.7587\n",
      "Training Epoch: 42 [6650/36045]\tLoss: 636.4098\n",
      "Training Epoch: 42 [6700/36045]\tLoss: 665.5869\n",
      "Training Epoch: 42 [6750/36045]\tLoss: 642.8167\n",
      "Training Epoch: 42 [6800/36045]\tLoss: 645.7693\n",
      "Training Epoch: 42 [6850/36045]\tLoss: 633.8427\n",
      "Training Epoch: 42 [6900/36045]\tLoss: 564.7043\n",
      "Training Epoch: 42 [6950/36045]\tLoss: 532.4132\n",
      "Training Epoch: 42 [7000/36045]\tLoss: 566.2597\n",
      "Training Epoch: 42 [7050/36045]\tLoss: 578.6249\n",
      "Training Epoch: 42 [7100/36045]\tLoss: 577.9683\n",
      "Training Epoch: 42 [7150/36045]\tLoss: 588.1539\n",
      "Training Epoch: 42 [7200/36045]\tLoss: 590.5279\n",
      "Training Epoch: 42 [7250/36045]\tLoss: 588.7086\n",
      "Training Epoch: 42 [7300/36045]\tLoss: 575.1104\n",
      "Training Epoch: 42 [7350/36045]\tLoss: 572.6913\n",
      "Training Epoch: 42 [7400/36045]\tLoss: 521.8230\n",
      "Training Epoch: 42 [7450/36045]\tLoss: 525.0416\n",
      "Training Epoch: 42 [7500/36045]\tLoss: 520.5198\n",
      "Training Epoch: 42 [7550/36045]\tLoss: 498.5595\n",
      "Training Epoch: 42 [7600/36045]\tLoss: 552.6352\n",
      "Training Epoch: 42 [7650/36045]\tLoss: 590.8307\n",
      "Training Epoch: 42 [7700/36045]\tLoss: 562.2840\n",
      "Training Epoch: 42 [7750/36045]\tLoss: 576.7448\n",
      "Training Epoch: 42 [7800/36045]\tLoss: 566.2153\n",
      "Training Epoch: 42 [7850/36045]\tLoss: 548.0292\n",
      "Training Epoch: 42 [7900/36045]\tLoss: 577.9942\n",
      "Training Epoch: 42 [7950/36045]\tLoss: 575.4971\n",
      "Training Epoch: 42 [8000/36045]\tLoss: 593.6409\n",
      "Training Epoch: 42 [8050/36045]\tLoss: 559.1409\n",
      "Training Epoch: 42 [8100/36045]\tLoss: 584.2701\n",
      "Training Epoch: 42 [8150/36045]\tLoss: 662.4825\n",
      "Training Epoch: 42 [8200/36045]\tLoss: 649.8226\n",
      "Training Epoch: 42 [8250/36045]\tLoss: 618.0322\n",
      "Training Epoch: 42 [8300/36045]\tLoss: 675.2512\n",
      "Training Epoch: 42 [8350/36045]\tLoss: 619.1462\n",
      "Training Epoch: 42 [8400/36045]\tLoss: 553.9732\n",
      "Training Epoch: 42 [8450/36045]\tLoss: 518.4869\n",
      "Training Epoch: 42 [8500/36045]\tLoss: 551.6516\n",
      "Training Epoch: 42 [8550/36045]\tLoss: 544.8812\n",
      "Training Epoch: 42 [8600/36045]\tLoss: 539.1713\n",
      "Training Epoch: 42 [8650/36045]\tLoss: 572.6953\n",
      "Training Epoch: 42 [8700/36045]\tLoss: 605.7510\n",
      "Training Epoch: 42 [8750/36045]\tLoss: 595.0684\n",
      "Training Epoch: 42 [8800/36045]\tLoss: 600.8362\n",
      "Training Epoch: 42 [8850/36045]\tLoss: 594.3809\n",
      "Training Epoch: 42 [8900/36045]\tLoss: 536.6189\n",
      "Training Epoch: 42 [8950/36045]\tLoss: 547.5265\n",
      "Training Epoch: 42 [9000/36045]\tLoss: 563.3466\n",
      "Training Epoch: 42 [9050/36045]\tLoss: 564.9294\n",
      "Training Epoch: 42 [9100/36045]\tLoss: 581.7277\n",
      "Training Epoch: 42 [9150/36045]\tLoss: 430.5399\n",
      "Training Epoch: 42 [9200/36045]\tLoss: 321.7604\n",
      "Training Epoch: 42 [9250/36045]\tLoss: 349.1510\n",
      "Training Epoch: 42 [9300/36045]\tLoss: 358.9535\n",
      "Training Epoch: 42 [9350/36045]\tLoss: 331.2509\n",
      "Training Epoch: 42 [9400/36045]\tLoss: 648.9120\n",
      "Training Epoch: 42 [9450/36045]\tLoss: 689.1522\n",
      "Training Epoch: 42 [9500/36045]\tLoss: 676.6329\n",
      "Training Epoch: 42 [9550/36045]\tLoss: 715.8288\n",
      "Training Epoch: 42 [9600/36045]\tLoss: 533.1607\n",
      "Training Epoch: 42 [9650/36045]\tLoss: 538.2179\n",
      "Training Epoch: 42 [9700/36045]\tLoss: 523.7224\n",
      "Training Epoch: 42 [9750/36045]\tLoss: 522.3528\n",
      "Training Epoch: 42 [9800/36045]\tLoss: 683.2780\n",
      "Training Epoch: 42 [9850/36045]\tLoss: 721.7183\n",
      "Training Epoch: 42 [9900/36045]\tLoss: 731.5097\n",
      "Training Epoch: 42 [9950/36045]\tLoss: 712.7713\n",
      "Training Epoch: 42 [10000/36045]\tLoss: 659.4330\n",
      "Training Epoch: 42 [10050/36045]\tLoss: 540.1725\n",
      "Training Epoch: 42 [10100/36045]\tLoss: 547.8875\n",
      "Training Epoch: 42 [10150/36045]\tLoss: 556.3201\n",
      "Training Epoch: 42 [10200/36045]\tLoss: 545.2281\n",
      "Training Epoch: 42 [10250/36045]\tLoss: 654.8041\n",
      "Training Epoch: 42 [10300/36045]\tLoss: 636.3568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 42 [10350/36045]\tLoss: 670.3670\n",
      "Training Epoch: 42 [10400/36045]\tLoss: 660.5728\n",
      "Training Epoch: 42 [10450/36045]\tLoss: 618.8899\n",
      "Training Epoch: 42 [10500/36045]\tLoss: 516.8542\n",
      "Training Epoch: 42 [10550/36045]\tLoss: 511.5530\n",
      "Training Epoch: 42 [10600/36045]\tLoss: 533.6586\n",
      "Training Epoch: 42 [10650/36045]\tLoss: 539.6512\n",
      "Training Epoch: 42 [10700/36045]\tLoss: 620.0201\n",
      "Training Epoch: 42 [10750/36045]\tLoss: 679.2925\n",
      "Training Epoch: 42 [10800/36045]\tLoss: 624.9339\n",
      "Training Epoch: 42 [10850/36045]\tLoss: 662.6641\n",
      "Training Epoch: 42 [10900/36045]\tLoss: 689.8544\n",
      "Training Epoch: 42 [10950/36045]\tLoss: 507.7538\n",
      "Training Epoch: 42 [11000/36045]\tLoss: 501.4281\n",
      "Training Epoch: 42 [11050/36045]\tLoss: 537.7421\n",
      "Training Epoch: 42 [11100/36045]\tLoss: 548.0143\n",
      "Training Epoch: 42 [11150/36045]\tLoss: 594.5042\n",
      "Training Epoch: 42 [11200/36045]\tLoss: 622.7095\n",
      "Training Epoch: 42 [11250/36045]\tLoss: 634.1896\n",
      "Training Epoch: 42 [11300/36045]\tLoss: 614.4716\n",
      "Training Epoch: 42 [11350/36045]\tLoss: 612.1299\n",
      "Training Epoch: 42 [11400/36045]\tLoss: 575.4560\n",
      "Training Epoch: 42 [11450/36045]\tLoss: 544.5235\n",
      "Training Epoch: 42 [11500/36045]\tLoss: 541.8981\n",
      "Training Epoch: 42 [11550/36045]\tLoss: 551.6882\n",
      "Training Epoch: 42 [11600/36045]\tLoss: 611.8837\n",
      "Training Epoch: 42 [11650/36045]\tLoss: 663.4319\n",
      "Training Epoch: 42 [11700/36045]\tLoss: 662.3842\n",
      "Training Epoch: 42 [11750/36045]\tLoss: 680.5052\n",
      "Training Epoch: 42 [11800/36045]\tLoss: 723.0236\n",
      "Training Epoch: 42 [11850/36045]\tLoss: 781.3479\n",
      "Training Epoch: 42 [11900/36045]\tLoss: 993.7790\n",
      "Training Epoch: 42 [11950/36045]\tLoss: 996.8141\n",
      "Training Epoch: 42 [12000/36045]\tLoss: 1008.3918\n",
      "Training Epoch: 42 [12050/36045]\tLoss: 967.8357\n",
      "Training Epoch: 42 [12100/36045]\tLoss: 620.3787\n",
      "Training Epoch: 42 [12150/36045]\tLoss: 467.6494\n",
      "Training Epoch: 42 [12200/36045]\tLoss: 462.4419\n",
      "Training Epoch: 42 [12250/36045]\tLoss: 471.1125\n",
      "Training Epoch: 42 [12300/36045]\tLoss: 606.9326\n",
      "Training Epoch: 42 [12350/36045]\tLoss: 662.3916\n",
      "Training Epoch: 42 [12400/36045]\tLoss: 669.6865\n",
      "Training Epoch: 42 [12450/36045]\tLoss: 658.5224\n",
      "Training Epoch: 42 [12500/36045]\tLoss: 685.5810\n",
      "Training Epoch: 42 [12550/36045]\tLoss: 655.4534\n",
      "Training Epoch: 42 [12600/36045]\tLoss: 599.9127\n",
      "Training Epoch: 42 [12650/36045]\tLoss: 598.2014\n",
      "Training Epoch: 42 [12700/36045]\tLoss: 619.4949\n",
      "Training Epoch: 42 [12750/36045]\tLoss: 618.1033\n",
      "Training Epoch: 42 [12800/36045]\tLoss: 603.8781\n",
      "Training Epoch: 42 [12850/36045]\tLoss: 633.0709\n",
      "Training Epoch: 42 [12900/36045]\tLoss: 606.9314\n",
      "Training Epoch: 42 [12950/36045]\tLoss: 592.8428\n",
      "Training Epoch: 42 [13000/36045]\tLoss: 625.9897\n",
      "Training Epoch: 42 [13050/36045]\tLoss: 566.3179\n",
      "Training Epoch: 42 [13100/36045]\tLoss: 582.0745\n",
      "Training Epoch: 42 [13150/36045]\tLoss: 573.7074\n",
      "Training Epoch: 42 [13200/36045]\tLoss: 556.5320\n",
      "Training Epoch: 42 [13250/36045]\tLoss: 578.5220\n",
      "Training Epoch: 42 [13300/36045]\tLoss: 615.6410\n",
      "Training Epoch: 42 [13350/36045]\tLoss: 596.5061\n",
      "Training Epoch: 42 [13400/36045]\tLoss: 599.6452\n",
      "Training Epoch: 42 [13450/36045]\tLoss: 596.9923\n",
      "Training Epoch: 42 [13500/36045]\tLoss: 615.8621\n",
      "Training Epoch: 42 [13550/36045]\tLoss: 753.6003\n",
      "Training Epoch: 42 [13600/36045]\tLoss: 786.2637\n",
      "Training Epoch: 42 [13650/36045]\tLoss: 867.7828\n",
      "Training Epoch: 42 [13700/36045]\tLoss: 764.6365\n",
      "Training Epoch: 42 [13750/36045]\tLoss: 603.1548\n",
      "Training Epoch: 42 [13800/36045]\tLoss: 574.2111\n",
      "Training Epoch: 42 [13850/36045]\tLoss: 556.8445\n",
      "Training Epoch: 42 [13900/36045]\tLoss: 564.0856\n",
      "Training Epoch: 42 [13950/36045]\tLoss: 609.2883\n",
      "Training Epoch: 42 [14000/36045]\tLoss: 642.1863\n",
      "Training Epoch: 42 [14050/36045]\tLoss: 617.2124\n",
      "Training Epoch: 42 [14100/36045]\tLoss: 612.2864\n",
      "Training Epoch: 42 [14150/36045]\tLoss: 600.2927\n",
      "Training Epoch: 42 [14200/36045]\tLoss: 640.9080\n",
      "Training Epoch: 42 [14250/36045]\tLoss: 704.3201\n",
      "Training Epoch: 42 [14300/36045]\tLoss: 707.6139\n",
      "Training Epoch: 42 [14350/36045]\tLoss: 676.3120\n",
      "Training Epoch: 42 [14400/36045]\tLoss: 662.2655\n",
      "Training Epoch: 42 [14450/36045]\tLoss: 697.7599\n",
      "Training Epoch: 42 [14500/36045]\tLoss: 629.2031\n",
      "Training Epoch: 42 [14550/36045]\tLoss: 657.5258\n",
      "Training Epoch: 42 [14600/36045]\tLoss: 644.0327\n",
      "Training Epoch: 42 [14650/36045]\tLoss: 643.5726\n",
      "Training Epoch: 42 [14700/36045]\tLoss: 610.0447\n",
      "Training Epoch: 42 [14750/36045]\tLoss: 524.8254\n",
      "Training Epoch: 42 [14800/36045]\tLoss: 514.9543\n",
      "Training Epoch: 42 [14850/36045]\tLoss: 521.9606\n",
      "Training Epoch: 42 [14900/36045]\tLoss: 515.4431\n",
      "Training Epoch: 42 [14950/36045]\tLoss: 523.4774\n",
      "Training Epoch: 42 [15000/36045]\tLoss: 536.2544\n",
      "Training Epoch: 42 [15050/36045]\tLoss: 533.0594\n",
      "Training Epoch: 42 [15100/36045]\tLoss: 517.0967\n",
      "Training Epoch: 42 [15150/36045]\tLoss: 512.6337\n",
      "Training Epoch: 42 [15200/36045]\tLoss: 474.8854\n",
      "Training Epoch: 42 [15250/36045]\tLoss: 496.6299\n",
      "Training Epoch: 42 [15300/36045]\tLoss: 482.2500\n",
      "Training Epoch: 42 [15350/36045]\tLoss: 493.8372\n",
      "Training Epoch: 42 [15400/36045]\tLoss: 476.6046\n",
      "Training Epoch: 42 [15450/36045]\tLoss: 461.7643\n",
      "Training Epoch: 42 [15500/36045]\tLoss: 474.9345\n",
      "Training Epoch: 42 [15550/36045]\tLoss: 471.4910\n",
      "Training Epoch: 42 [15600/36045]\tLoss: 538.8096\n",
      "Training Epoch: 42 [15650/36045]\tLoss: 555.6693\n",
      "Training Epoch: 42 [15700/36045]\tLoss: 547.8404\n",
      "Training Epoch: 42 [15750/36045]\tLoss: 539.1962\n",
      "Training Epoch: 42 [15800/36045]\tLoss: 515.9625\n",
      "Training Epoch: 42 [15850/36045]\tLoss: 531.1229\n",
      "Training Epoch: 42 [15900/36045]\tLoss: 540.1306\n",
      "Training Epoch: 42 [15950/36045]\tLoss: 559.9260\n",
      "Training Epoch: 42 [16000/36045]\tLoss: 531.0404\n",
      "Training Epoch: 42 [16050/36045]\tLoss: 500.5303\n",
      "Training Epoch: 42 [16100/36045]\tLoss: 464.4586\n",
      "Training Epoch: 42 [16150/36045]\tLoss: 452.4928\n",
      "Training Epoch: 42 [16200/36045]\tLoss: 549.5269\n",
      "Training Epoch: 42 [16250/36045]\tLoss: 577.1015\n",
      "Training Epoch: 42 [16300/36045]\tLoss: 629.9833\n",
      "Training Epoch: 42 [16350/36045]\tLoss: 650.1177\n",
      "Training Epoch: 42 [16400/36045]\tLoss: 621.8940\n",
      "Training Epoch: 42 [16450/36045]\tLoss: 604.2294\n",
      "Training Epoch: 42 [16500/36045]\tLoss: 604.1299\n",
      "Training Epoch: 42 [16550/36045]\tLoss: 569.4831\n",
      "Training Epoch: 42 [16600/36045]\tLoss: 591.5658\n",
      "Training Epoch: 42 [16650/36045]\tLoss: 607.7138\n",
      "Training Epoch: 42 [16700/36045]\tLoss: 587.8089\n",
      "Training Epoch: 42 [16750/36045]\tLoss: 580.5186\n",
      "Training Epoch: 42 [16800/36045]\tLoss: 589.3490\n",
      "Training Epoch: 42 [16850/36045]\tLoss: 561.6357\n",
      "Training Epoch: 42 [16900/36045]\tLoss: 571.3920\n",
      "Training Epoch: 42 [16950/36045]\tLoss: 594.2396\n",
      "Training Epoch: 42 [17000/36045]\tLoss: 578.5710\n",
      "Training Epoch: 42 [17050/36045]\tLoss: 602.6006\n",
      "Training Epoch: 42 [17100/36045]\tLoss: 598.5363\n",
      "Training Epoch: 42 [17150/36045]\tLoss: 519.6304\n",
      "Training Epoch: 42 [17200/36045]\tLoss: 481.9552\n",
      "Training Epoch: 42 [17250/36045]\tLoss: 504.7432\n",
      "Training Epoch: 42 [17300/36045]\tLoss: 534.2255\n",
      "Training Epoch: 42 [17350/36045]\tLoss: 515.1154\n",
      "Training Epoch: 42 [17400/36045]\tLoss: 534.6365\n",
      "Training Epoch: 42 [17450/36045]\tLoss: 553.1154\n",
      "Training Epoch: 42 [17500/36045]\tLoss: 541.4948\n",
      "Training Epoch: 42 [17550/36045]\tLoss: 540.0978\n",
      "Training Epoch: 42 [17600/36045]\tLoss: 534.5132\n",
      "Training Epoch: 42 [17650/36045]\tLoss: 550.0190\n",
      "Training Epoch: 42 [17700/36045]\tLoss: 529.3981\n",
      "Training Epoch: 42 [17750/36045]\tLoss: 545.3235\n",
      "Training Epoch: 42 [17800/36045]\tLoss: 536.5048\n",
      "Training Epoch: 42 [17850/36045]\tLoss: 552.3713\n",
      "Training Epoch: 42 [17900/36045]\tLoss: 580.3515\n",
      "Training Epoch: 42 [17950/36045]\tLoss: 592.5772\n",
      "Training Epoch: 42 [18000/36045]\tLoss: 583.2095\n",
      "Training Epoch: 42 [18050/36045]\tLoss: 640.4488\n",
      "Training Epoch: 42 [18100/36045]\tLoss: 641.6620\n",
      "Training Epoch: 42 [18150/36045]\tLoss: 652.9288\n",
      "Training Epoch: 42 [18200/36045]\tLoss: 635.2238\n",
      "Training Epoch: 42 [18250/36045]\tLoss: 655.9147\n",
      "Training Epoch: 42 [18300/36045]\tLoss: 611.4339\n",
      "Training Epoch: 42 [18350/36045]\tLoss: 684.3839\n",
      "Training Epoch: 42 [18400/36045]\tLoss: 657.2915\n",
      "Training Epoch: 42 [18450/36045]\tLoss: 637.0765\n",
      "Training Epoch: 42 [18500/36045]\tLoss: 635.9089\n",
      "Training Epoch: 42 [18550/36045]\tLoss: 623.4246\n",
      "Training Epoch: 42 [18600/36045]\tLoss: 613.0974\n",
      "Training Epoch: 42 [18650/36045]\tLoss: 658.6220\n",
      "Training Epoch: 42 [18700/36045]\tLoss: 692.9068\n",
      "Training Epoch: 42 [18750/36045]\tLoss: 680.4182\n",
      "Training Epoch: 42 [18800/36045]\tLoss: 703.1601\n",
      "Training Epoch: 42 [18850/36045]\tLoss: 648.2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 42 [18900/36045]\tLoss: 693.3669\n",
      "Training Epoch: 42 [18950/36045]\tLoss: 635.9747\n",
      "Training Epoch: 42 [19000/36045]\tLoss: 523.0912\n",
      "Training Epoch: 42 [19050/36045]\tLoss: 507.6209\n",
      "Training Epoch: 42 [19100/36045]\tLoss: 515.7545\n",
      "Training Epoch: 42 [19150/36045]\tLoss: 505.9049\n",
      "Training Epoch: 42 [19200/36045]\tLoss: 536.7525\n",
      "Training Epoch: 42 [19250/36045]\tLoss: 551.8760\n",
      "Training Epoch: 42 [19300/36045]\tLoss: 561.2336\n",
      "Training Epoch: 42 [19350/36045]\tLoss: 545.0319\n",
      "Training Epoch: 42 [19400/36045]\tLoss: 565.3101\n",
      "Training Epoch: 42 [19450/36045]\tLoss: 556.7130\n",
      "Training Epoch: 42 [19500/36045]\tLoss: 558.0051\n",
      "Training Epoch: 42 [19550/36045]\tLoss: 556.2079\n",
      "Training Epoch: 42 [19600/36045]\tLoss: 597.0328\n",
      "Training Epoch: 42 [19650/36045]\tLoss: 797.6776\n",
      "Training Epoch: 42 [19700/36045]\tLoss: 756.4089\n",
      "Training Epoch: 42 [19750/36045]\tLoss: 760.4765\n",
      "Training Epoch: 42 [19800/36045]\tLoss: 760.8464\n",
      "Training Epoch: 42 [19850/36045]\tLoss: 497.9525\n",
      "Training Epoch: 42 [19900/36045]\tLoss: 477.0455\n",
      "Training Epoch: 42 [19950/36045]\tLoss: 480.8413\n",
      "Training Epoch: 42 [20000/36045]\tLoss: 480.6526\n",
      "Training Epoch: 42 [20050/36045]\tLoss: 537.9749\n",
      "Training Epoch: 42 [20100/36045]\tLoss: 543.5112\n",
      "Training Epoch: 42 [20150/36045]\tLoss: 544.6681\n",
      "Training Epoch: 42 [20200/36045]\tLoss: 544.3811\n",
      "Training Epoch: 42 [20250/36045]\tLoss: 580.5920\n",
      "Training Epoch: 42 [20300/36045]\tLoss: 616.6622\n",
      "Training Epoch: 42 [20350/36045]\tLoss: 634.9067\n",
      "Training Epoch: 42 [20400/36045]\tLoss: 651.3396\n",
      "Training Epoch: 42 [20450/36045]\tLoss: 621.2488\n",
      "Training Epoch: 42 [20500/36045]\tLoss: 605.8857\n",
      "Training Epoch: 42 [20550/36045]\tLoss: 532.3593\n",
      "Training Epoch: 42 [20600/36045]\tLoss: 542.2195\n",
      "Training Epoch: 42 [20650/36045]\tLoss: 539.6024\n",
      "Training Epoch: 42 [20700/36045]\tLoss: 527.6697\n",
      "Training Epoch: 42 [20750/36045]\tLoss: 568.9780\n",
      "Training Epoch: 42 [20800/36045]\tLoss: 618.1398\n",
      "Training Epoch: 42 [20850/36045]\tLoss: 604.9680\n",
      "Training Epoch: 42 [20900/36045]\tLoss: 648.1608\n",
      "Training Epoch: 42 [20950/36045]\tLoss: 610.8275\n",
      "Training Epoch: 42 [21000/36045]\tLoss: 574.8646\n",
      "Training Epoch: 42 [21050/36045]\tLoss: 491.6715\n",
      "Training Epoch: 42 [21100/36045]\tLoss: 496.5330\n",
      "Training Epoch: 42 [21150/36045]\tLoss: 531.4896\n",
      "Training Epoch: 42 [21200/36045]\tLoss: 530.6319\n",
      "Training Epoch: 42 [21250/36045]\tLoss: 507.5064\n",
      "Training Epoch: 42 [21300/36045]\tLoss: 592.4902\n",
      "Training Epoch: 42 [21350/36045]\tLoss: 584.2788\n",
      "Training Epoch: 42 [21400/36045]\tLoss: 587.9963\n",
      "Training Epoch: 42 [21450/36045]\tLoss: 594.2204\n",
      "Training Epoch: 42 [21500/36045]\tLoss: 595.9728\n",
      "Training Epoch: 42 [21550/36045]\tLoss: 690.2219\n",
      "Training Epoch: 42 [21600/36045]\tLoss: 688.6747\n",
      "Training Epoch: 42 [21650/36045]\tLoss: 700.9895\n",
      "Training Epoch: 42 [21700/36045]\tLoss: 704.2525\n",
      "Training Epoch: 42 [21750/36045]\tLoss: 676.1568\n",
      "Training Epoch: 42 [21800/36045]\tLoss: 496.8442\n",
      "Training Epoch: 42 [21850/36045]\tLoss: 479.8213\n",
      "Training Epoch: 42 [21900/36045]\tLoss: 489.0644\n",
      "Training Epoch: 42 [21950/36045]\tLoss: 490.2993\n",
      "Training Epoch: 42 [22000/36045]\tLoss: 493.5927\n",
      "Training Epoch: 42 [22050/36045]\tLoss: 513.5770\n",
      "Training Epoch: 42 [22100/36045]\tLoss: 506.0913\n",
      "Training Epoch: 42 [22150/36045]\tLoss: 492.3792\n",
      "Training Epoch: 42 [22200/36045]\tLoss: 508.1131\n",
      "Training Epoch: 42 [22250/36045]\tLoss: 513.1139\n",
      "Training Epoch: 42 [22300/36045]\tLoss: 565.0169\n",
      "Training Epoch: 42 [22350/36045]\tLoss: 590.6302\n",
      "Training Epoch: 42 [22400/36045]\tLoss: 604.5763\n",
      "Training Epoch: 42 [22450/36045]\tLoss: 591.8342\n",
      "Training Epoch: 42 [22500/36045]\tLoss: 574.8238\n",
      "Training Epoch: 42 [22550/36045]\tLoss: 609.8419\n",
      "Training Epoch: 42 [22600/36045]\tLoss: 659.0770\n",
      "Training Epoch: 42 [22650/36045]\tLoss: 692.3054\n",
      "Training Epoch: 42 [22700/36045]\tLoss: 713.6964\n",
      "Training Epoch: 42 [22750/36045]\tLoss: 733.1224\n",
      "Training Epoch: 42 [22800/36045]\tLoss: 761.9733\n",
      "Training Epoch: 42 [22850/36045]\tLoss: 633.0075\n",
      "Training Epoch: 42 [22900/36045]\tLoss: 637.8497\n",
      "Training Epoch: 42 [22950/36045]\tLoss: 617.3889\n",
      "Training Epoch: 42 [23000/36045]\tLoss: 613.8813\n",
      "Training Epoch: 42 [23050/36045]\tLoss: 545.2898\n",
      "Training Epoch: 42 [23100/36045]\tLoss: 560.9182\n",
      "Training Epoch: 42 [23150/36045]\tLoss: 549.2874\n",
      "Training Epoch: 42 [23200/36045]\tLoss: 519.9477\n",
      "Training Epoch: 42 [23250/36045]\tLoss: 522.9349\n",
      "Training Epoch: 42 [23300/36045]\tLoss: 518.9219\n",
      "Training Epoch: 42 [23350/36045]\tLoss: 539.3742\n",
      "Training Epoch: 42 [23400/36045]\tLoss: 584.9490\n",
      "Training Epoch: 42 [23450/36045]\tLoss: 578.5785\n",
      "Training Epoch: 42 [23500/36045]\tLoss: 557.5145\n",
      "Training Epoch: 42 [23550/36045]\tLoss: 597.4009\n",
      "Training Epoch: 42 [23600/36045]\tLoss: 677.8810\n",
      "Training Epoch: 42 [23650/36045]\tLoss: 689.5620\n",
      "Training Epoch: 42 [23700/36045]\tLoss: 697.5605\n",
      "Training Epoch: 42 [23750/36045]\tLoss: 673.8481\n",
      "Training Epoch: 42 [23800/36045]\tLoss: 540.2260\n",
      "Training Epoch: 42 [23850/36045]\tLoss: 566.0070\n",
      "Training Epoch: 42 [23900/36045]\tLoss: 555.8376\n",
      "Training Epoch: 42 [23950/36045]\tLoss: 539.0756\n",
      "Training Epoch: 42 [24000/36045]\tLoss: 516.1913\n",
      "Training Epoch: 42 [24050/36045]\tLoss: 476.5373\n",
      "Training Epoch: 42 [24100/36045]\tLoss: 501.1451\n",
      "Training Epoch: 42 [24150/36045]\tLoss: 493.3741\n",
      "Training Epoch: 42 [24200/36045]\tLoss: 490.8779\n",
      "Training Epoch: 42 [24250/36045]\tLoss: 476.8068\n",
      "Training Epoch: 42 [24300/36045]\tLoss: 515.1741\n",
      "Training Epoch: 42 [24350/36045]\tLoss: 527.3788\n",
      "Training Epoch: 42 [24400/36045]\tLoss: 542.2125\n",
      "Training Epoch: 42 [24450/36045]\tLoss: 516.7185\n",
      "Training Epoch: 42 [24500/36045]\tLoss: 545.4661\n",
      "Training Epoch: 42 [24550/36045]\tLoss: 632.5860\n",
      "Training Epoch: 42 [24600/36045]\tLoss: 623.9623\n",
      "Training Epoch: 42 [24650/36045]\tLoss: 597.1478\n",
      "Training Epoch: 42 [24700/36045]\tLoss: 606.9615\n",
      "Training Epoch: 42 [24750/36045]\tLoss: 561.0718\n",
      "Training Epoch: 42 [24800/36045]\tLoss: 460.0421\n",
      "Training Epoch: 42 [24850/36045]\tLoss: 478.4894\n",
      "Training Epoch: 42 [24900/36045]\tLoss: 475.9236\n",
      "Training Epoch: 42 [24950/36045]\tLoss: 478.3978\n",
      "Training Epoch: 42 [25000/36045]\tLoss: 459.9641\n",
      "Training Epoch: 42 [25050/36045]\tLoss: 439.7419\n",
      "Training Epoch: 42 [25100/36045]\tLoss: 393.9930\n",
      "Training Epoch: 42 [25150/36045]\tLoss: 364.7747\n",
      "Training Epoch: 42 [25200/36045]\tLoss: 359.7349\n",
      "Training Epoch: 42 [25250/36045]\tLoss: 385.6597\n",
      "Training Epoch: 42 [25300/36045]\tLoss: 506.7500\n",
      "Training Epoch: 42 [25350/36045]\tLoss: 503.0273\n",
      "Training Epoch: 42 [25400/36045]\tLoss: 470.0673\n",
      "Training Epoch: 42 [25450/36045]\tLoss: 472.1510\n",
      "Training Epoch: 42 [25500/36045]\tLoss: 512.9028\n",
      "Training Epoch: 42 [25550/36045]\tLoss: 600.1019\n",
      "Training Epoch: 42 [25600/36045]\tLoss: 604.2148\n",
      "Training Epoch: 42 [25650/36045]\tLoss: 583.1631\n",
      "Training Epoch: 42 [25700/36045]\tLoss: 592.6389\n",
      "Training Epoch: 42 [25750/36045]\tLoss: 572.5759\n",
      "Training Epoch: 42 [25800/36045]\tLoss: 361.3780\n",
      "Training Epoch: 42 [25850/36045]\tLoss: 369.3650\n",
      "Training Epoch: 42 [25900/36045]\tLoss: 350.8330\n",
      "Training Epoch: 42 [25950/36045]\tLoss: 360.1969\n",
      "Training Epoch: 42 [26000/36045]\tLoss: 442.9944\n",
      "Training Epoch: 42 [26050/36045]\tLoss: 602.9254\n",
      "Training Epoch: 42 [26100/36045]\tLoss: 629.0458\n",
      "Training Epoch: 42 [26150/36045]\tLoss: 628.5125\n",
      "Training Epoch: 42 [26200/36045]\tLoss: 601.8470\n",
      "Training Epoch: 42 [26250/36045]\tLoss: 631.8397\n",
      "Training Epoch: 42 [26300/36045]\tLoss: 579.5027\n",
      "Training Epoch: 42 [26350/36045]\tLoss: 590.4339\n",
      "Training Epoch: 42 [26400/36045]\tLoss: 565.9329\n",
      "Training Epoch: 42 [26450/36045]\tLoss: 495.9467\n",
      "Training Epoch: 42 [26500/36045]\tLoss: 587.3088\n",
      "Training Epoch: 42 [26550/36045]\tLoss: 587.1337\n",
      "Training Epoch: 42 [26600/36045]\tLoss: 583.6439\n",
      "Training Epoch: 42 [26650/36045]\tLoss: 598.6921\n",
      "Training Epoch: 42 [26700/36045]\tLoss: 577.7368\n",
      "Training Epoch: 42 [26750/36045]\tLoss: 541.6564\n",
      "Training Epoch: 42 [26800/36045]\tLoss: 400.1180\n",
      "Training Epoch: 42 [26850/36045]\tLoss: 331.4574\n",
      "Training Epoch: 42 [26900/36045]\tLoss: 333.5318\n",
      "Training Epoch: 42 [26950/36045]\tLoss: 366.0530\n",
      "Training Epoch: 42 [27000/36045]\tLoss: 602.4400\n",
      "Training Epoch: 42 [27050/36045]\tLoss: 628.8381\n",
      "Training Epoch: 42 [27100/36045]\tLoss: 609.6663\n",
      "Training Epoch: 42 [27150/36045]\tLoss: 648.8221\n",
      "Training Epoch: 42 [27200/36045]\tLoss: 470.9102\n",
      "Training Epoch: 42 [27250/36045]\tLoss: 461.5546\n",
      "Training Epoch: 42 [27300/36045]\tLoss: 450.0155\n",
      "Training Epoch: 42 [27350/36045]\tLoss: 447.5415\n",
      "Training Epoch: 42 [27400/36045]\tLoss: 446.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 42 [27450/36045]\tLoss: 565.6276\n",
      "Training Epoch: 42 [27500/36045]\tLoss: 606.0994\n",
      "Training Epoch: 42 [27550/36045]\tLoss: 599.4655\n",
      "Training Epoch: 42 [27600/36045]\tLoss: 610.9677\n",
      "Training Epoch: 42 [27650/36045]\tLoss: 602.9698\n",
      "Training Epoch: 42 [27700/36045]\tLoss: 630.4262\n",
      "Training Epoch: 42 [27750/36045]\tLoss: 642.3495\n",
      "Training Epoch: 42 [27800/36045]\tLoss: 629.6192\n",
      "Training Epoch: 42 [27850/36045]\tLoss: 619.9557\n",
      "Training Epoch: 42 [27900/36045]\tLoss: 563.3690\n",
      "Training Epoch: 42 [27950/36045]\tLoss: 471.0991\n",
      "Training Epoch: 42 [28000/36045]\tLoss: 447.9314\n",
      "Training Epoch: 42 [28050/36045]\tLoss: 457.4172\n",
      "Training Epoch: 42 [28100/36045]\tLoss: 449.0808\n",
      "Training Epoch: 42 [28150/36045]\tLoss: 468.5557\n",
      "Training Epoch: 42 [28200/36045]\tLoss: 476.9374\n",
      "Training Epoch: 42 [28250/36045]\tLoss: 469.8398\n",
      "Training Epoch: 42 [28300/36045]\tLoss: 445.9956\n",
      "Training Epoch: 42 [28350/36045]\tLoss: 441.7902\n",
      "Training Epoch: 42 [28400/36045]\tLoss: 769.4603\n",
      "Training Epoch: 42 [28450/36045]\tLoss: 706.2223\n",
      "Training Epoch: 42 [28500/36045]\tLoss: 610.2917\n",
      "Training Epoch: 42 [28550/36045]\tLoss: 561.1072\n",
      "Training Epoch: 42 [28600/36045]\tLoss: 586.2359\n",
      "Training Epoch: 42 [28650/36045]\tLoss: 641.0714\n",
      "Training Epoch: 42 [28700/36045]\tLoss: 634.9796\n",
      "Training Epoch: 42 [28750/36045]\tLoss: 621.9406\n",
      "Training Epoch: 42 [28800/36045]\tLoss: 631.7245\n",
      "Training Epoch: 42 [28850/36045]\tLoss: 548.5876\n",
      "Training Epoch: 42 [28900/36045]\tLoss: 446.4077\n",
      "Training Epoch: 42 [28950/36045]\tLoss: 445.5518\n",
      "Training Epoch: 42 [29000/36045]\tLoss: 441.6733\n",
      "Training Epoch: 42 [29050/36045]\tLoss: 448.6315\n",
      "Training Epoch: 42 [29100/36045]\tLoss: 466.6859\n",
      "Training Epoch: 42 [29150/36045]\tLoss: 456.9652\n",
      "Training Epoch: 42 [29200/36045]\tLoss: 442.7766\n",
      "Training Epoch: 42 [29250/36045]\tLoss: 432.6758\n",
      "Training Epoch: 42 [29300/36045]\tLoss: 486.9873\n",
      "Training Epoch: 42 [29350/36045]\tLoss: 571.9608\n",
      "Training Epoch: 42 [29400/36045]\tLoss: 589.4808\n",
      "Training Epoch: 42 [29450/36045]\tLoss: 606.2502\n",
      "Training Epoch: 42 [29500/36045]\tLoss: 621.1562\n",
      "Training Epoch: 42 [29550/36045]\tLoss: 590.9274\n",
      "Training Epoch: 42 [29600/36045]\tLoss: 497.1961\n",
      "Training Epoch: 42 [29650/36045]\tLoss: 478.7708\n",
      "Training Epoch: 42 [29700/36045]\tLoss: 429.6732\n",
      "Training Epoch: 42 [29750/36045]\tLoss: 428.1326\n",
      "Training Epoch: 42 [29800/36045]\tLoss: 475.2929\n",
      "Training Epoch: 42 [29850/36045]\tLoss: 552.4501\n",
      "Training Epoch: 42 [29900/36045]\tLoss: 548.1130\n",
      "Training Epoch: 42 [29950/36045]\tLoss: 569.8240\n",
      "Training Epoch: 42 [30000/36045]\tLoss: 543.8289\n",
      "Training Epoch: 42 [30050/36045]\tLoss: 550.9600\n",
      "Training Epoch: 42 [30100/36045]\tLoss: 672.1567\n",
      "Training Epoch: 42 [30150/36045]\tLoss: 654.6302\n",
      "Training Epoch: 42 [30200/36045]\tLoss: 617.3021\n",
      "Training Epoch: 42 [30250/36045]\tLoss: 665.7076\n",
      "Training Epoch: 42 [30300/36045]\tLoss: 650.0477\n",
      "Training Epoch: 42 [30350/36045]\tLoss: 496.7930\n",
      "Training Epoch: 42 [30400/36045]\tLoss: 481.4542\n",
      "Training Epoch: 42 [30450/36045]\tLoss: 483.7682\n",
      "Training Epoch: 42 [30500/36045]\tLoss: 451.4444\n",
      "Training Epoch: 42 [30550/36045]\tLoss: 418.8093\n",
      "Training Epoch: 42 [30600/36045]\tLoss: 411.2941\n",
      "Training Epoch: 42 [30650/36045]\tLoss: 401.4231\n",
      "Training Epoch: 42 [30700/36045]\tLoss: 419.1663\n",
      "Training Epoch: 42 [30750/36045]\tLoss: 406.1433\n",
      "Training Epoch: 42 [30800/36045]\tLoss: 432.2210\n",
      "Training Epoch: 42 [30850/36045]\tLoss: 423.9510\n",
      "Training Epoch: 42 [30900/36045]\tLoss: 435.8516\n",
      "Training Epoch: 42 [30950/36045]\tLoss: 458.0494\n",
      "Training Epoch: 42 [31000/36045]\tLoss: 450.4376\n",
      "Training Epoch: 42 [31050/36045]\tLoss: 376.0857\n",
      "Training Epoch: 42 [31100/36045]\tLoss: 366.7250\n",
      "Training Epoch: 42 [31150/36045]\tLoss: 374.3681\n",
      "Training Epoch: 42 [31200/36045]\tLoss: 464.8706\n",
      "Training Epoch: 42 [31250/36045]\tLoss: 603.2278\n",
      "Training Epoch: 42 [31300/36045]\tLoss: 574.4354\n",
      "Training Epoch: 42 [31350/36045]\tLoss: 590.3610\n",
      "Training Epoch: 42 [31400/36045]\tLoss: 569.9064\n",
      "Training Epoch: 42 [31450/36045]\tLoss: 586.5934\n",
      "Training Epoch: 42 [31500/36045]\tLoss: 599.1866\n",
      "Training Epoch: 42 [31550/36045]\tLoss: 606.0453\n",
      "Training Epoch: 42 [31600/36045]\tLoss: 569.7672\n",
      "Training Epoch: 42 [31650/36045]\tLoss: 609.9894\n",
      "Training Epoch: 42 [31700/36045]\tLoss: 441.0439\n",
      "Training Epoch: 42 [31750/36045]\tLoss: 364.4655\n",
      "Training Epoch: 42 [31800/36045]\tLoss: 347.9045\n",
      "Training Epoch: 42 [31850/36045]\tLoss: 355.6683\n",
      "Training Epoch: 42 [31900/36045]\tLoss: 562.6028\n",
      "Training Epoch: 42 [31950/36045]\tLoss: 728.9857\n",
      "Training Epoch: 42 [32000/36045]\tLoss: 836.7351\n",
      "Training Epoch: 42 [32050/36045]\tLoss: 792.4076\n",
      "Training Epoch: 42 [32100/36045]\tLoss: 783.4942\n",
      "Training Epoch: 42 [32150/36045]\tLoss: 601.5211\n",
      "Training Epoch: 42 [32200/36045]\tLoss: 603.6309\n",
      "Training Epoch: 42 [32250/36045]\tLoss: 613.9122\n",
      "Training Epoch: 42 [32300/36045]\tLoss: 595.9788\n",
      "Training Epoch: 42 [32350/36045]\tLoss: 591.9678\n",
      "Training Epoch: 42 [32400/36045]\tLoss: 555.4329\n",
      "Training Epoch: 42 [32450/36045]\tLoss: 457.2990\n",
      "Training Epoch: 42 [32500/36045]\tLoss: 439.3826\n",
      "Training Epoch: 42 [32550/36045]\tLoss: 441.5201\n",
      "Training Epoch: 42 [32600/36045]\tLoss: 438.6102\n",
      "Training Epoch: 42 [32650/36045]\tLoss: 568.3081\n",
      "Training Epoch: 42 [32700/36045]\tLoss: 620.7225\n",
      "Training Epoch: 42 [32750/36045]\tLoss: 591.0860\n",
      "Training Epoch: 42 [32800/36045]\tLoss: 606.0610\n",
      "Training Epoch: 42 [32850/36045]\tLoss: 559.4702\n",
      "Training Epoch: 42 [32900/36045]\tLoss: 447.2324\n",
      "Training Epoch: 42 [32950/36045]\tLoss: 468.4827\n",
      "Training Epoch: 42 [33000/36045]\tLoss: 467.1985\n",
      "Training Epoch: 42 [33050/36045]\tLoss: 444.4269\n",
      "Training Epoch: 42 [33100/36045]\tLoss: 505.0363\n",
      "Training Epoch: 42 [33150/36045]\tLoss: 687.3188\n",
      "Training Epoch: 42 [33200/36045]\tLoss: 669.1925\n",
      "Training Epoch: 42 [33250/36045]\tLoss: 689.5967\n",
      "Training Epoch: 42 [33300/36045]\tLoss: 734.9146\n",
      "Training Epoch: 42 [33350/36045]\tLoss: 562.6933\n",
      "Training Epoch: 42 [33400/36045]\tLoss: 409.8517\n",
      "Training Epoch: 42 [33450/36045]\tLoss: 405.7232\n",
      "Training Epoch: 42 [33500/36045]\tLoss: 417.6937\n",
      "Training Epoch: 42 [33550/36045]\tLoss: 432.6165\n",
      "Training Epoch: 42 [33600/36045]\tLoss: 434.3908\n",
      "Training Epoch: 42 [33650/36045]\tLoss: 580.2985\n",
      "Training Epoch: 42 [33700/36045]\tLoss: 561.3098\n",
      "Training Epoch: 42 [33750/36045]\tLoss: 581.3105\n",
      "Training Epoch: 42 [33800/36045]\tLoss: 578.5051\n",
      "Training Epoch: 42 [33850/36045]\tLoss: 580.3769\n",
      "Training Epoch: 42 [33900/36045]\tLoss: 593.8597\n",
      "Training Epoch: 42 [33950/36045]\tLoss: 603.5257\n",
      "Training Epoch: 42 [34000/36045]\tLoss: 590.0872\n",
      "Training Epoch: 42 [34050/36045]\tLoss: 593.9034\n",
      "Training Epoch: 42 [34100/36045]\tLoss: 572.6438\n",
      "Training Epoch: 42 [34150/36045]\tLoss: 530.5671\n",
      "Training Epoch: 42 [34200/36045]\tLoss: 502.5571\n",
      "Training Epoch: 42 [34250/36045]\tLoss: 516.5385\n",
      "Training Epoch: 42 [34300/36045]\tLoss: 440.8488\n",
      "Training Epoch: 42 [34350/36045]\tLoss: 464.3713\n",
      "Training Epoch: 42 [34400/36045]\tLoss: 457.2639\n",
      "Training Epoch: 42 [34450/36045]\tLoss: 430.5382\n",
      "Training Epoch: 42 [34500/36045]\tLoss: 459.2581\n",
      "Training Epoch: 42 [34550/36045]\tLoss: 450.7960\n",
      "Training Epoch: 42 [34600/36045]\tLoss: 457.4465\n",
      "Training Epoch: 42 [34650/36045]\tLoss: 563.2915\n",
      "Training Epoch: 42 [34700/36045]\tLoss: 597.4190\n",
      "Training Epoch: 42 [34750/36045]\tLoss: 529.4859\n",
      "Training Epoch: 42 [34800/36045]\tLoss: 607.9930\n",
      "Training Epoch: 42 [34850/36045]\tLoss: 615.1511\n",
      "Training Epoch: 42 [34900/36045]\tLoss: 664.1304\n",
      "Training Epoch: 42 [34950/36045]\tLoss: 648.9563\n",
      "Training Epoch: 42 [35000/36045]\tLoss: 650.0642\n",
      "Training Epoch: 42 [35050/36045]\tLoss: 637.2912\n",
      "Training Epoch: 42 [35100/36045]\tLoss: 549.5938\n",
      "Training Epoch: 42 [35150/36045]\tLoss: 541.8325\n",
      "Training Epoch: 42 [35200/36045]\tLoss: 455.7244\n",
      "Training Epoch: 42 [35250/36045]\tLoss: 500.7294\n",
      "Training Epoch: 42 [35300/36045]\tLoss: 516.7144\n",
      "Training Epoch: 42 [35350/36045]\tLoss: 578.3124\n",
      "Training Epoch: 42 [35400/36045]\tLoss: 608.5696\n",
      "Training Epoch: 42 [35450/36045]\tLoss: 579.7521\n",
      "Training Epoch: 42 [35500/36045]\tLoss: 560.8270\n",
      "Training Epoch: 42 [35550/36045]\tLoss: 547.1078\n",
      "Training Epoch: 42 [35600/36045]\tLoss: 597.6017\n",
      "Training Epoch: 42 [35650/36045]\tLoss: 670.1932\n",
      "Training Epoch: 42 [35700/36045]\tLoss: 597.0645\n",
      "Training Epoch: 42 [35750/36045]\tLoss: 654.2175\n",
      "Training Epoch: 42 [35800/36045]\tLoss: 660.8043\n",
      "Training Epoch: 42 [35850/36045]\tLoss: 635.0396\n",
      "Training Epoch: 42 [35900/36045]\tLoss: 657.6303\n",
      "Training Epoch: 42 [35950/36045]\tLoss: 654.4737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 42 [36000/36045]\tLoss: 647.3918\n",
      "Training Epoch: 42 [36045/36045]\tLoss: 632.5687\n",
      "Training Epoch: 42 [4004/4004]\tLoss: 587.4053\n",
      "Training Epoch: 43 [50/36045]\tLoss: 586.0037\n",
      "Training Epoch: 43 [100/36045]\tLoss: 561.7574\n",
      "Training Epoch: 43 [150/36045]\tLoss: 559.8212\n",
      "Training Epoch: 43 [200/36045]\tLoss: 546.0383\n",
      "Training Epoch: 43 [250/36045]\tLoss: 656.0306\n",
      "Training Epoch: 43 [300/36045]\tLoss: 720.9664\n",
      "Training Epoch: 43 [350/36045]\tLoss: 687.7854\n",
      "Training Epoch: 43 [400/36045]\tLoss: 682.4139\n",
      "Training Epoch: 43 [450/36045]\tLoss: 663.1920\n",
      "Training Epoch: 43 [500/36045]\tLoss: 614.0121\n",
      "Training Epoch: 43 [550/36045]\tLoss: 616.7839\n",
      "Training Epoch: 43 [600/36045]\tLoss: 602.3251\n",
      "Training Epoch: 43 [650/36045]\tLoss: 623.8693\n",
      "Training Epoch: 43 [700/36045]\tLoss: 609.4344\n",
      "Training Epoch: 43 [750/36045]\tLoss: 586.2968\n",
      "Training Epoch: 43 [800/36045]\tLoss: 597.9277\n",
      "Training Epoch: 43 [850/36045]\tLoss: 580.7473\n",
      "Training Epoch: 43 [900/36045]\tLoss: 556.0369\n",
      "Training Epoch: 43 [950/36045]\tLoss: 526.1936\n",
      "Training Epoch: 43 [1000/36045]\tLoss: 509.9417\n",
      "Training Epoch: 43 [1050/36045]\tLoss: 511.8479\n",
      "Training Epoch: 43 [1100/36045]\tLoss: 497.8795\n",
      "Training Epoch: 43 [1150/36045]\tLoss: 507.0401\n",
      "Training Epoch: 43 [1200/36045]\tLoss: 536.8137\n",
      "Training Epoch: 43 [1250/36045]\tLoss: 614.4343\n",
      "Training Epoch: 43 [1300/36045]\tLoss: 621.4346\n",
      "Training Epoch: 43 [1350/36045]\tLoss: 622.6451\n",
      "Training Epoch: 43 [1400/36045]\tLoss: 646.5139\n",
      "Training Epoch: 43 [1450/36045]\tLoss: 625.5136\n",
      "Training Epoch: 43 [1500/36045]\tLoss: 571.8114\n",
      "Training Epoch: 43 [1550/36045]\tLoss: 586.6479\n",
      "Training Epoch: 43 [1600/36045]\tLoss: 596.7838\n",
      "Training Epoch: 43 [1650/36045]\tLoss: 584.0158\n",
      "Training Epoch: 43 [1700/36045]\tLoss: 596.1500\n",
      "Training Epoch: 43 [1750/36045]\tLoss: 637.5748\n",
      "Training Epoch: 43 [1800/36045]\tLoss: 620.1660\n",
      "Training Epoch: 43 [1850/36045]\tLoss: 636.1780\n",
      "Training Epoch: 43 [1900/36045]\tLoss: 595.0521\n",
      "Training Epoch: 43 [1950/36045]\tLoss: 604.9962\n",
      "Training Epoch: 43 [2000/36045]\tLoss: 545.0468\n",
      "Training Epoch: 43 [2050/36045]\tLoss: 547.5687\n",
      "Training Epoch: 43 [2100/36045]\tLoss: 577.1578\n",
      "Training Epoch: 43 [2150/36045]\tLoss: 558.0861\n",
      "Training Epoch: 43 [2200/36045]\tLoss: 520.0424\n",
      "Training Epoch: 43 [2250/36045]\tLoss: 491.1368\n",
      "Training Epoch: 43 [2300/36045]\tLoss: 514.8842\n",
      "Training Epoch: 43 [2350/36045]\tLoss: 492.1692\n",
      "Training Epoch: 43 [2400/36045]\tLoss: 499.8197\n",
      "Training Epoch: 43 [2450/36045]\tLoss: 640.0964\n",
      "Training Epoch: 43 [2500/36045]\tLoss: 672.2629\n",
      "Training Epoch: 43 [2550/36045]\tLoss: 670.0252\n",
      "Training Epoch: 43 [2600/36045]\tLoss: 678.5660\n",
      "Training Epoch: 43 [2650/36045]\tLoss: 802.8511\n",
      "Training Epoch: 43 [2700/36045]\tLoss: 890.6292\n",
      "Training Epoch: 43 [2750/36045]\tLoss: 961.6325\n",
      "Training Epoch: 43 [2800/36045]\tLoss: 971.1454\n",
      "Training Epoch: 43 [2850/36045]\tLoss: 738.5844\n",
      "Training Epoch: 43 [2900/36045]\tLoss: 699.7999\n",
      "Training Epoch: 43 [2950/36045]\tLoss: 676.7296\n",
      "Training Epoch: 43 [3000/36045]\tLoss: 670.6718\n",
      "Training Epoch: 43 [3050/36045]\tLoss: 701.5682\n",
      "Training Epoch: 43 [3100/36045]\tLoss: 641.2998\n",
      "Training Epoch: 43 [3150/36045]\tLoss: 493.3979\n",
      "Training Epoch: 43 [3200/36045]\tLoss: 510.7941\n",
      "Training Epoch: 43 [3250/36045]\tLoss: 481.7330\n",
      "Training Epoch: 43 [3300/36045]\tLoss: 456.1750\n",
      "Training Epoch: 43 [3350/36045]\tLoss: 481.9570\n",
      "Training Epoch: 43 [3400/36045]\tLoss: 505.1015\n",
      "Training Epoch: 43 [3450/36045]\tLoss: 541.8770\n",
      "Training Epoch: 43 [3500/36045]\tLoss: 529.0624\n",
      "Training Epoch: 43 [3550/36045]\tLoss: 505.9599\n",
      "Training Epoch: 43 [3600/36045]\tLoss: 543.5550\n",
      "Training Epoch: 43 [3650/36045]\tLoss: 628.7890\n",
      "Training Epoch: 43 [3700/36045]\tLoss: 636.5532\n",
      "Training Epoch: 43 [3750/36045]\tLoss: 605.7132\n",
      "Training Epoch: 43 [3800/36045]\tLoss: 602.3506\n",
      "Training Epoch: 43 [3850/36045]\tLoss: 603.6415\n",
      "Training Epoch: 43 [3900/36045]\tLoss: 608.3973\n",
      "Training Epoch: 43 [3950/36045]\tLoss: 587.2480\n",
      "Training Epoch: 43 [4000/36045]\tLoss: 592.0227\n",
      "Training Epoch: 43 [4050/36045]\tLoss: 542.9023\n",
      "Training Epoch: 43 [4100/36045]\tLoss: 529.3722\n",
      "Training Epoch: 43 [4150/36045]\tLoss: 543.6196\n",
      "Training Epoch: 43 [4200/36045]\tLoss: 538.6426\n",
      "Training Epoch: 43 [4250/36045]\tLoss: 541.0193\n",
      "Training Epoch: 43 [4300/36045]\tLoss: 556.7595\n",
      "Training Epoch: 43 [4350/36045]\tLoss: 540.2943\n",
      "Training Epoch: 43 [4400/36045]\tLoss: 516.6916\n",
      "Training Epoch: 43 [4450/36045]\tLoss: 567.4075\n",
      "Training Epoch: 43 [4500/36045]\tLoss: 610.4785\n",
      "Training Epoch: 43 [4550/36045]\tLoss: 613.5299\n",
      "Training Epoch: 43 [4600/36045]\tLoss: 635.4791\n",
      "Training Epoch: 43 [4650/36045]\tLoss: 625.2200\n",
      "Training Epoch: 43 [4700/36045]\tLoss: 576.4995\n",
      "Training Epoch: 43 [4750/36045]\tLoss: 558.6871\n",
      "Training Epoch: 43 [4800/36045]\tLoss: 583.1474\n",
      "Training Epoch: 43 [4850/36045]\tLoss: 569.6520\n",
      "Training Epoch: 43 [4900/36045]\tLoss: 554.4318\n",
      "Training Epoch: 43 [4950/36045]\tLoss: 569.7943\n",
      "Training Epoch: 43 [5000/36045]\tLoss: 599.3270\n",
      "Training Epoch: 43 [5050/36045]\tLoss: 580.7087\n",
      "Training Epoch: 43 [5100/36045]\tLoss: 590.9250\n",
      "Training Epoch: 43 [5150/36045]\tLoss: 575.2502\n",
      "Training Epoch: 43 [5200/36045]\tLoss: 573.1469\n",
      "Training Epoch: 43 [5250/36045]\tLoss: 566.8052\n",
      "Training Epoch: 43 [5300/36045]\tLoss: 566.9925\n",
      "Training Epoch: 43 [5350/36045]\tLoss: 588.3705\n",
      "Training Epoch: 43 [5400/36045]\tLoss: 567.0331\n",
      "Training Epoch: 43 [5450/36045]\tLoss: 537.3150\n",
      "Training Epoch: 43 [5500/36045]\tLoss: 565.4681\n",
      "Training Epoch: 43 [5550/36045]\tLoss: 553.7235\n",
      "Training Epoch: 43 [5600/36045]\tLoss: 632.8766\n",
      "Training Epoch: 43 [5650/36045]\tLoss: 598.4304\n",
      "Training Epoch: 43 [5700/36045]\tLoss: 561.2865\n",
      "Training Epoch: 43 [5750/36045]\tLoss: 545.3691\n",
      "Training Epoch: 43 [5800/36045]\tLoss: 575.3784\n",
      "Training Epoch: 43 [5850/36045]\tLoss: 564.7520\n",
      "Training Epoch: 43 [5900/36045]\tLoss: 648.9469\n",
      "Training Epoch: 43 [5950/36045]\tLoss: 665.4896\n",
      "Training Epoch: 43 [6000/36045]\tLoss: 651.4207\n",
      "Training Epoch: 43 [6050/36045]\tLoss: 629.5061\n",
      "Training Epoch: 43 [6100/36045]\tLoss: 633.7328\n",
      "Training Epoch: 43 [6150/36045]\tLoss: 624.0289\n",
      "Training Epoch: 43 [6200/36045]\tLoss: 628.3681\n",
      "Training Epoch: 43 [6250/36045]\tLoss: 649.9011\n",
      "Training Epoch: 43 [6300/36045]\tLoss: 661.5009\n",
      "Training Epoch: 43 [6350/36045]\tLoss: 706.9360\n",
      "Training Epoch: 43 [6400/36045]\tLoss: 582.8557\n",
      "Training Epoch: 43 [6450/36045]\tLoss: 536.1415\n",
      "Training Epoch: 43 [6500/36045]\tLoss: 545.9292\n",
      "Training Epoch: 43 [6550/36045]\tLoss: 563.3493\n",
      "Training Epoch: 43 [6600/36045]\tLoss: 561.4452\n",
      "Training Epoch: 43 [6650/36045]\tLoss: 633.8599\n",
      "Training Epoch: 43 [6700/36045]\tLoss: 662.9022\n",
      "Training Epoch: 43 [6750/36045]\tLoss: 640.2105\n",
      "Training Epoch: 43 [6800/36045]\tLoss: 643.1739\n",
      "Training Epoch: 43 [6850/36045]\tLoss: 631.2582\n",
      "Training Epoch: 43 [6900/36045]\tLoss: 562.3877\n",
      "Training Epoch: 43 [6950/36045]\tLoss: 530.2355\n",
      "Training Epoch: 43 [7000/36045]\tLoss: 563.9302\n",
      "Training Epoch: 43 [7050/36045]\tLoss: 576.2416\n",
      "Training Epoch: 43 [7100/36045]\tLoss: 575.5446\n",
      "Training Epoch: 43 [7150/36045]\tLoss: 585.6818\n",
      "Training Epoch: 43 [7200/36045]\tLoss: 587.9849\n",
      "Training Epoch: 43 [7250/36045]\tLoss: 586.2012\n",
      "Training Epoch: 43 [7300/36045]\tLoss: 572.5883\n",
      "Training Epoch: 43 [7350/36045]\tLoss: 570.2276\n",
      "Training Epoch: 43 [7400/36045]\tLoss: 519.6998\n",
      "Training Epoch: 43 [7450/36045]\tLoss: 522.9151\n",
      "Training Epoch: 43 [7500/36045]\tLoss: 518.4307\n",
      "Training Epoch: 43 [7550/36045]\tLoss: 496.5542\n",
      "Training Epoch: 43 [7600/36045]\tLoss: 550.3618\n",
      "Training Epoch: 43 [7650/36045]\tLoss: 588.3415\n",
      "Training Epoch: 43 [7700/36045]\tLoss: 559.9043\n",
      "Training Epoch: 43 [7750/36045]\tLoss: 574.3441\n",
      "Training Epoch: 43 [7800/36045]\tLoss: 563.8583\n",
      "Training Epoch: 43 [7850/36045]\tLoss: 545.7437\n",
      "Training Epoch: 43 [7900/36045]\tLoss: 575.5692\n",
      "Training Epoch: 43 [7950/36045]\tLoss: 573.0900\n",
      "Training Epoch: 43 [8000/36045]\tLoss: 591.2306\n",
      "Training Epoch: 43 [8050/36045]\tLoss: 556.8090\n",
      "Training Epoch: 43 [8100/36045]\tLoss: 581.9122\n",
      "Training Epoch: 43 [8150/36045]\tLoss: 659.8983\n",
      "Training Epoch: 43 [8200/36045]\tLoss: 647.2868\n",
      "Training Epoch: 43 [8250/36045]\tLoss: 615.5503\n",
      "Training Epoch: 43 [8300/36045]\tLoss: 672.6328\n",
      "Training Epoch: 43 [8350/36045]\tLoss: 616.6739\n",
      "Training Epoch: 43 [8400/36045]\tLoss: 551.6922\n",
      "Training Epoch: 43 [8450/36045]\tLoss: 516.3318\n",
      "Training Epoch: 43 [8500/36045]\tLoss: 549.4033\n",
      "Training Epoch: 43 [8550/36045]\tLoss: 542.7200\n",
      "Training Epoch: 43 [8600/36045]\tLoss: 537.0278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 43 [8650/36045]\tLoss: 570.2562\n",
      "Training Epoch: 43 [8700/36045]\tLoss: 603.1670\n",
      "Training Epoch: 43 [8750/36045]\tLoss: 592.5486\n",
      "Training Epoch: 43 [8800/36045]\tLoss: 598.3066\n",
      "Training Epoch: 43 [8850/36045]\tLoss: 591.8720\n",
      "Training Epoch: 43 [8900/36045]\tLoss: 534.3531\n",
      "Training Epoch: 43 [8950/36045]\tLoss: 545.1846\n",
      "Training Epoch: 43 [9000/36045]\tLoss: 561.0243\n",
      "Training Epoch: 43 [9050/36045]\tLoss: 562.6137\n",
      "Training Epoch: 43 [9100/36045]\tLoss: 579.3745\n",
      "Training Epoch: 43 [9150/36045]\tLoss: 428.8106\n",
      "Training Epoch: 43 [9200/36045]\tLoss: 320.4225\n",
      "Training Epoch: 43 [9250/36045]\tLoss: 347.7150\n",
      "Training Epoch: 43 [9300/36045]\tLoss: 357.4667\n",
      "Training Epoch: 43 [9350/36045]\tLoss: 329.9019\n",
      "Training Epoch: 43 [9400/36045]\tLoss: 646.2139\n",
      "Training Epoch: 43 [9450/36045]\tLoss: 686.2778\n",
      "Training Epoch: 43 [9500/36045]\tLoss: 673.8040\n",
      "Training Epoch: 43 [9550/36045]\tLoss: 712.8354\n",
      "Training Epoch: 43 [9600/36045]\tLoss: 531.0344\n",
      "Training Epoch: 43 [9650/36045]\tLoss: 536.1094\n",
      "Training Epoch: 43 [9700/36045]\tLoss: 521.6279\n",
      "Training Epoch: 43 [9750/36045]\tLoss: 520.2484\n",
      "Training Epoch: 43 [9800/36045]\tLoss: 680.5294\n",
      "Training Epoch: 43 [9850/36045]\tLoss: 718.7926\n",
      "Training Epoch: 43 [9900/36045]\tLoss: 728.4169\n",
      "Training Epoch: 43 [9950/36045]\tLoss: 709.7694\n",
      "Training Epoch: 43 [10000/36045]\tLoss: 656.7341\n",
      "Training Epoch: 43 [10050/36045]\tLoss: 537.8514\n",
      "Training Epoch: 43 [10100/36045]\tLoss: 545.5610\n",
      "Training Epoch: 43 [10150/36045]\tLoss: 553.9164\n",
      "Training Epoch: 43 [10200/36045]\tLoss: 542.8339\n",
      "Training Epoch: 43 [10250/36045]\tLoss: 652.1420\n",
      "Training Epoch: 43 [10300/36045]\tLoss: 633.7955\n",
      "Training Epoch: 43 [10350/36045]\tLoss: 667.6680\n",
      "Training Epoch: 43 [10400/36045]\tLoss: 657.8318\n",
      "Training Epoch: 43 [10450/36045]\tLoss: 616.3290\n",
      "Training Epoch: 43 [10500/36045]\tLoss: 514.6987\n",
      "Training Epoch: 43 [10550/36045]\tLoss: 509.3775\n",
      "Training Epoch: 43 [10600/36045]\tLoss: 531.4274\n",
      "Training Epoch: 43 [10650/36045]\tLoss: 537.3869\n",
      "Training Epoch: 43 [10700/36045]\tLoss: 617.5117\n",
      "Training Epoch: 43 [10750/36045]\tLoss: 676.6857\n",
      "Training Epoch: 43 [10800/36045]\tLoss: 622.4143\n",
      "Training Epoch: 43 [10850/36045]\tLoss: 660.0380\n",
      "Training Epoch: 43 [10900/36045]\tLoss: 687.1254\n",
      "Training Epoch: 43 [10950/36045]\tLoss: 505.6277\n",
      "Training Epoch: 43 [11000/36045]\tLoss: 499.3203\n",
      "Training Epoch: 43 [11050/36045]\tLoss: 535.5128\n",
      "Training Epoch: 43 [11100/36045]\tLoss: 545.7385\n",
      "Training Epoch: 43 [11150/36045]\tLoss: 592.0550\n",
      "Training Epoch: 43 [11200/36045]\tLoss: 620.3065\n",
      "Training Epoch: 43 [11250/36045]\tLoss: 631.7421\n",
      "Training Epoch: 43 [11300/36045]\tLoss: 612.0457\n",
      "Training Epoch: 43 [11350/36045]\tLoss: 609.7278\n",
      "Training Epoch: 43 [11400/36045]\tLoss: 573.1402\n",
      "Training Epoch: 43 [11450/36045]\tLoss: 542.2748\n",
      "Training Epoch: 43 [11500/36045]\tLoss: 539.6548\n",
      "Training Epoch: 43 [11550/36045]\tLoss: 549.3583\n",
      "Training Epoch: 43 [11600/36045]\tLoss: 609.4890\n",
      "Training Epoch: 43 [11650/36045]\tLoss: 660.9915\n",
      "Training Epoch: 43 [11700/36045]\tLoss: 659.9361\n",
      "Training Epoch: 43 [11750/36045]\tLoss: 677.9960\n",
      "Training Epoch: 43 [11800/36045]\tLoss: 720.4924\n",
      "Training Epoch: 43 [11850/36045]\tLoss: 778.8643\n",
      "Training Epoch: 43 [11900/36045]\tLoss: 991.1437\n",
      "Training Epoch: 43 [11950/36045]\tLoss: 994.2189\n",
      "Training Epoch: 43 [12000/36045]\tLoss: 1005.6971\n",
      "Training Epoch: 43 [12050/36045]\tLoss: 965.1934\n",
      "Training Epoch: 43 [12100/36045]\tLoss: 618.2003\n",
      "Training Epoch: 43 [12150/36045]\tLoss: 465.6658\n",
      "Training Epoch: 43 [12200/36045]\tLoss: 460.4767\n",
      "Training Epoch: 43 [12250/36045]\tLoss: 469.1243\n",
      "Training Epoch: 43 [12300/36045]\tLoss: 604.6616\n",
      "Training Epoch: 43 [12350/36045]\tLoss: 660.0236\n",
      "Training Epoch: 43 [12400/36045]\tLoss: 667.2870\n",
      "Training Epoch: 43 [12450/36045]\tLoss: 656.1484\n",
      "Training Epoch: 43 [12500/36045]\tLoss: 683.1539\n",
      "Training Epoch: 43 [12550/36045]\tLoss: 653.0616\n",
      "Training Epoch: 43 [12600/36045]\tLoss: 597.5221\n",
      "Training Epoch: 43 [12650/36045]\tLoss: 595.7806\n",
      "Training Epoch: 43 [12700/36045]\tLoss: 617.0583\n",
      "Training Epoch: 43 [12750/36045]\tLoss: 615.6482\n",
      "Training Epoch: 43 [12800/36045]\tLoss: 601.5598\n",
      "Training Epoch: 43 [12850/36045]\tLoss: 630.7524\n",
      "Training Epoch: 43 [12900/36045]\tLoss: 604.6833\n",
      "Training Epoch: 43 [12950/36045]\tLoss: 590.5337\n",
      "Training Epoch: 43 [13000/36045]\tLoss: 623.7192\n",
      "Training Epoch: 43 [13050/36045]\tLoss: 564.1620\n",
      "Training Epoch: 43 [13100/36045]\tLoss: 579.7781\n",
      "Training Epoch: 43 [13150/36045]\tLoss: 571.4025\n",
      "Training Epoch: 43 [13200/36045]\tLoss: 554.3243\n",
      "Training Epoch: 43 [13250/36045]\tLoss: 576.2125\n",
      "Training Epoch: 43 [13300/36045]\tLoss: 613.2222\n",
      "Training Epoch: 43 [13350/36045]\tLoss: 594.1447\n",
      "Training Epoch: 43 [13400/36045]\tLoss: 597.2643\n",
      "Training Epoch: 43 [13450/36045]\tLoss: 594.6509\n",
      "Training Epoch: 43 [13500/36045]\tLoss: 613.4259\n",
      "Training Epoch: 43 [13550/36045]\tLoss: 751.2345\n",
      "Training Epoch: 43 [13600/36045]\tLoss: 783.8802\n",
      "Training Epoch: 43 [13650/36045]\tLoss: 865.4716\n",
      "Training Epoch: 43 [13700/36045]\tLoss: 762.4614\n",
      "Training Epoch: 43 [13750/36045]\tLoss: 600.7708\n",
      "Training Epoch: 43 [13800/36045]\tLoss: 571.7461\n",
      "Training Epoch: 43 [13850/36045]\tLoss: 554.3951\n",
      "Training Epoch: 43 [13900/36045]\tLoss: 561.6422\n",
      "Training Epoch: 43 [13950/36045]\tLoss: 606.8339\n",
      "Training Epoch: 43 [14000/36045]\tLoss: 639.6656\n",
      "Training Epoch: 43 [14050/36045]\tLoss: 614.7153\n",
      "Training Epoch: 43 [14100/36045]\tLoss: 609.7684\n",
      "Training Epoch: 43 [14150/36045]\tLoss: 597.7980\n",
      "Training Epoch: 43 [14200/36045]\tLoss: 638.3633\n",
      "Training Epoch: 43 [14250/36045]\tLoss: 701.5693\n",
      "Training Epoch: 43 [14300/36045]\tLoss: 704.8344\n",
      "Training Epoch: 43 [14350/36045]\tLoss: 673.6054\n",
      "Training Epoch: 43 [14400/36045]\tLoss: 659.5909\n",
      "Training Epoch: 43 [14450/36045]\tLoss: 695.0470\n",
      "Training Epoch: 43 [14500/36045]\tLoss: 626.5564\n",
      "Training Epoch: 43 [14550/36045]\tLoss: 654.7531\n",
      "Training Epoch: 43 [14600/36045]\tLoss: 641.2670\n",
      "Training Epoch: 43 [14650/36045]\tLoss: 640.8153\n",
      "Training Epoch: 43 [14700/36045]\tLoss: 607.5189\n",
      "Training Epoch: 43 [14750/36045]\tLoss: 522.6982\n",
      "Training Epoch: 43 [14800/36045]\tLoss: 512.8120\n",
      "Training Epoch: 43 [14850/36045]\tLoss: 519.8073\n",
      "Training Epoch: 43 [14900/36045]\tLoss: 513.2836\n",
      "Training Epoch: 43 [14950/36045]\tLoss: 521.3524\n",
      "Training Epoch: 43 [15000/36045]\tLoss: 534.0635\n",
      "Training Epoch: 43 [15050/36045]\tLoss: 530.8126\n",
      "Training Epoch: 43 [15100/36045]\tLoss: 514.8300\n",
      "Training Epoch: 43 [15150/36045]\tLoss: 510.4271\n",
      "Training Epoch: 43 [15200/36045]\tLoss: 472.8719\n",
      "Training Epoch: 43 [15250/36045]\tLoss: 494.5665\n",
      "Training Epoch: 43 [15300/36045]\tLoss: 480.2328\n",
      "Training Epoch: 43 [15350/36045]\tLoss: 491.7999\n",
      "Training Epoch: 43 [15400/36045]\tLoss: 474.4464\n",
      "Training Epoch: 43 [15450/36045]\tLoss: 459.6087\n",
      "Training Epoch: 43 [15500/36045]\tLoss: 472.7188\n",
      "Training Epoch: 43 [15550/36045]\tLoss: 469.3470\n",
      "Training Epoch: 43 [15600/36045]\tLoss: 536.5169\n",
      "Training Epoch: 43 [15650/36045]\tLoss: 553.2827\n",
      "Training Epoch: 43 [15700/36045]\tLoss: 545.4886\n",
      "Training Epoch: 43 [15750/36045]\tLoss: 536.8496\n",
      "Training Epoch: 43 [15800/36045]\tLoss: 514.0662\n",
      "Training Epoch: 43 [15850/36045]\tLoss: 529.2911\n",
      "Training Epoch: 43 [15900/36045]\tLoss: 538.2565\n",
      "Training Epoch: 43 [15950/36045]\tLoss: 558.0399\n",
      "Training Epoch: 43 [16000/36045]\tLoss: 529.0460\n",
      "Training Epoch: 43 [16050/36045]\tLoss: 498.5529\n",
      "Training Epoch: 43 [16100/36045]\tLoss: 462.6592\n",
      "Training Epoch: 43 [16150/36045]\tLoss: 450.7128\n",
      "Training Epoch: 43 [16200/36045]\tLoss: 547.4458\n",
      "Training Epoch: 43 [16250/36045]\tLoss: 574.9542\n",
      "Training Epoch: 43 [16300/36045]\tLoss: 627.6266\n",
      "Training Epoch: 43 [16350/36045]\tLoss: 647.8172\n",
      "Training Epoch: 43 [16400/36045]\tLoss: 619.6146\n",
      "Training Epoch: 43 [16450/36045]\tLoss: 601.9835\n",
      "Training Epoch: 43 [16500/36045]\tLoss: 601.8755\n",
      "Training Epoch: 43 [16550/36045]\tLoss: 567.2700\n",
      "Training Epoch: 43 [16600/36045]\tLoss: 589.2331\n",
      "Training Epoch: 43 [16650/36045]\tLoss: 605.3018\n",
      "Training Epoch: 43 [16700/36045]\tLoss: 585.5090\n",
      "Training Epoch: 43 [16750/36045]\tLoss: 578.2319\n",
      "Training Epoch: 43 [16800/36045]\tLoss: 586.9625\n",
      "Training Epoch: 43 [16850/36045]\tLoss: 559.3845\n",
      "Training Epoch: 43 [16900/36045]\tLoss: 569.1347\n",
      "Training Epoch: 43 [16950/36045]\tLoss: 591.9241\n",
      "Training Epoch: 43 [17000/36045]\tLoss: 576.3237\n",
      "Training Epoch: 43 [17050/36045]\tLoss: 600.2092\n",
      "Training Epoch: 43 [17100/36045]\tLoss: 596.1166\n",
      "Training Epoch: 43 [17150/36045]\tLoss: 517.4698\n",
      "Training Epoch: 43 [17200/36045]\tLoss: 479.8454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 43 [17250/36045]\tLoss: 502.5635\n",
      "Training Epoch: 43 [17300/36045]\tLoss: 531.9321\n",
      "Training Epoch: 43 [17350/36045]\tLoss: 512.9910\n",
      "Training Epoch: 43 [17400/36045]\tLoss: 532.5229\n",
      "Training Epoch: 43 [17450/36045]\tLoss: 550.9295\n",
      "Training Epoch: 43 [17500/36045]\tLoss: 539.3384\n",
      "Training Epoch: 43 [17550/36045]\tLoss: 537.9179\n",
      "Training Epoch: 43 [17600/36045]\tLoss: 532.3652\n",
      "Training Epoch: 43 [17650/36045]\tLoss: 547.7946\n",
      "Training Epoch: 43 [17700/36045]\tLoss: 527.2012\n",
      "Training Epoch: 43 [17750/36045]\tLoss: 543.0735\n",
      "Training Epoch: 43 [17800/36045]\tLoss: 534.3004\n",
      "Training Epoch: 43 [17850/36045]\tLoss: 550.4257\n",
      "Training Epoch: 43 [17900/36045]\tLoss: 578.3581\n",
      "Training Epoch: 43 [17950/36045]\tLoss: 590.5955\n",
      "Training Epoch: 43 [18000/36045]\tLoss: 581.2780\n",
      "Training Epoch: 43 [18050/36045]\tLoss: 638.1065\n",
      "Training Epoch: 43 [18100/36045]\tLoss: 639.2493\n",
      "Training Epoch: 43 [18150/36045]\tLoss: 650.5033\n",
      "Training Epoch: 43 [18200/36045]\tLoss: 632.8403\n",
      "Training Epoch: 43 [18250/36045]\tLoss: 653.5061\n",
      "Training Epoch: 43 [18300/36045]\tLoss: 609.3398\n",
      "Training Epoch: 43 [18350/36045]\tLoss: 682.3836\n",
      "Training Epoch: 43 [18400/36045]\tLoss: 655.2501\n",
      "Training Epoch: 43 [18450/36045]\tLoss: 635.0349\n",
      "Training Epoch: 43 [18500/36045]\tLoss: 633.8547\n",
      "Training Epoch: 43 [18550/36045]\tLoss: 621.4169\n",
      "Training Epoch: 43 [18600/36045]\tLoss: 611.1090\n",
      "Training Epoch: 43 [18650/36045]\tLoss: 656.5557\n",
      "Training Epoch: 43 [18700/36045]\tLoss: 690.7226\n",
      "Training Epoch: 43 [18750/36045]\tLoss: 678.2771\n",
      "Training Epoch: 43 [18800/36045]\tLoss: 700.9781\n",
      "Training Epoch: 43 [18850/36045]\tLoss: 646.1373\n",
      "Training Epoch: 43 [18900/36045]\tLoss: 691.0662\n",
      "Training Epoch: 43 [18950/36045]\tLoss: 633.7123\n",
      "Training Epoch: 43 [19000/36045]\tLoss: 520.7438\n",
      "Training Epoch: 43 [19050/36045]\tLoss: 505.3676\n",
      "Training Epoch: 43 [19100/36045]\tLoss: 513.4709\n",
      "Training Epoch: 43 [19150/36045]\tLoss: 503.6519\n",
      "Training Epoch: 43 [19200/36045]\tLoss: 534.6058\n",
      "Training Epoch: 43 [19250/36045]\tLoss: 549.7549\n",
      "Training Epoch: 43 [19300/36045]\tLoss: 559.0821\n",
      "Training Epoch: 43 [19350/36045]\tLoss: 542.9052\n",
      "Training Epoch: 43 [19400/36045]\tLoss: 563.0975\n",
      "Training Epoch: 43 [19450/36045]\tLoss: 554.5248\n",
      "Training Epoch: 43 [19500/36045]\tLoss: 555.7651\n",
      "Training Epoch: 43 [19550/36045]\tLoss: 553.9585\n",
      "Training Epoch: 43 [19600/36045]\tLoss: 594.7682\n",
      "Training Epoch: 43 [19650/36045]\tLoss: 795.1453\n",
      "Training Epoch: 43 [19700/36045]\tLoss: 753.8781\n",
      "Training Epoch: 43 [19750/36045]\tLoss: 757.9666\n",
      "Training Epoch: 43 [19800/36045]\tLoss: 758.3802\n",
      "Training Epoch: 43 [19850/36045]\tLoss: 495.8967\n",
      "Training Epoch: 43 [19900/36045]\tLoss: 475.0627\n",
      "Training Epoch: 43 [19950/36045]\tLoss: 478.8978\n",
      "Training Epoch: 43 [20000/36045]\tLoss: 478.7737\n",
      "Training Epoch: 43 [20050/36045]\tLoss: 535.8212\n",
      "Training Epoch: 43 [20100/36045]\tLoss: 541.2911\n",
      "Training Epoch: 43 [20150/36045]\tLoss: 542.3936\n",
      "Training Epoch: 43 [20200/36045]\tLoss: 542.1205\n",
      "Training Epoch: 43 [20250/36045]\tLoss: 578.2799\n",
      "Training Epoch: 43 [20300/36045]\tLoss: 614.3666\n",
      "Training Epoch: 43 [20350/36045]\tLoss: 632.5472\n",
      "Training Epoch: 43 [20400/36045]\tLoss: 648.9262\n",
      "Training Epoch: 43 [20450/36045]\tLoss: 618.7949\n",
      "Training Epoch: 43 [20500/36045]\tLoss: 603.5161\n",
      "Training Epoch: 43 [20550/36045]\tLoss: 530.2102\n",
      "Training Epoch: 43 [20600/36045]\tLoss: 540.0186\n",
      "Training Epoch: 43 [20650/36045]\tLoss: 537.3889\n",
      "Training Epoch: 43 [20700/36045]\tLoss: 525.4581\n",
      "Training Epoch: 43 [20750/36045]\tLoss: 566.6530\n",
      "Training Epoch: 43 [20800/36045]\tLoss: 615.6302\n",
      "Training Epoch: 43 [20850/36045]\tLoss: 602.4855\n",
      "Training Epoch: 43 [20900/36045]\tLoss: 645.5494\n",
      "Training Epoch: 43 [20950/36045]\tLoss: 608.3098\n",
      "Training Epoch: 43 [21000/36045]\tLoss: 572.4907\n",
      "Training Epoch: 43 [21050/36045]\tLoss: 489.6252\n",
      "Training Epoch: 43 [21100/36045]\tLoss: 494.5717\n",
      "Training Epoch: 43 [21150/36045]\tLoss: 529.3845\n",
      "Training Epoch: 43 [21200/36045]\tLoss: 528.5039\n",
      "Training Epoch: 43 [21250/36045]\tLoss: 505.4451\n",
      "Training Epoch: 43 [21300/36045]\tLoss: 590.0490\n",
      "Training Epoch: 43 [21350/36045]\tLoss: 581.8347\n",
      "Training Epoch: 43 [21400/36045]\tLoss: 585.5809\n",
      "Training Epoch: 43 [21450/36045]\tLoss: 591.8069\n",
      "Training Epoch: 43 [21500/36045]\tLoss: 593.4899\n",
      "Training Epoch: 43 [21550/36045]\tLoss: 687.6832\n",
      "Training Epoch: 43 [21600/36045]\tLoss: 686.1011\n",
      "Training Epoch: 43 [21650/36045]\tLoss: 698.4176\n",
      "Training Epoch: 43 [21700/36045]\tLoss: 701.7430\n",
      "Training Epoch: 43 [21750/36045]\tLoss: 673.7218\n",
      "Training Epoch: 43 [21800/36045]\tLoss: 494.8771\n",
      "Training Epoch: 43 [21850/36045]\tLoss: 477.8411\n",
      "Training Epoch: 43 [21900/36045]\tLoss: 487.0577\n",
      "Training Epoch: 43 [21950/36045]\tLoss: 488.3961\n",
      "Training Epoch: 43 [22000/36045]\tLoss: 491.6813\n",
      "Training Epoch: 43 [22050/36045]\tLoss: 511.4956\n",
      "Training Epoch: 43 [22100/36045]\tLoss: 503.9798\n",
      "Training Epoch: 43 [22150/36045]\tLoss: 490.3314\n",
      "Training Epoch: 43 [22200/36045]\tLoss: 506.0235\n",
      "Training Epoch: 43 [22250/36045]\tLoss: 511.0309\n",
      "Training Epoch: 43 [22300/36045]\tLoss: 562.8976\n",
      "Training Epoch: 43 [22350/36045]\tLoss: 588.4643\n",
      "Training Epoch: 43 [22400/36045]\tLoss: 602.3211\n",
      "Training Epoch: 43 [22450/36045]\tLoss: 589.5917\n",
      "Training Epoch: 43 [22500/36045]\tLoss: 572.6638\n",
      "Training Epoch: 43 [22550/36045]\tLoss: 607.6166\n",
      "Training Epoch: 43 [22600/36045]\tLoss: 656.5356\n",
      "Training Epoch: 43 [22650/36045]\tLoss: 689.6063\n",
      "Training Epoch: 43 [22700/36045]\tLoss: 710.9292\n",
      "Training Epoch: 43 [22750/36045]\tLoss: 730.3913\n",
      "Training Epoch: 43 [22800/36045]\tLoss: 759.1069\n",
      "Training Epoch: 43 [22850/36045]\tLoss: 630.5217\n",
      "Training Epoch: 43 [22900/36045]\tLoss: 635.3233\n",
      "Training Epoch: 43 [22950/36045]\tLoss: 614.8724\n",
      "Training Epoch: 43 [23000/36045]\tLoss: 611.3381\n",
      "Training Epoch: 43 [23050/36045]\tLoss: 543.0577\n",
      "Training Epoch: 43 [23100/36045]\tLoss: 558.6738\n",
      "Training Epoch: 43 [23150/36045]\tLoss: 547.0234\n",
      "Training Epoch: 43 [23200/36045]\tLoss: 517.7659\n",
      "Training Epoch: 43 [23250/36045]\tLoss: 520.7448\n",
      "Training Epoch: 43 [23300/36045]\tLoss: 516.7252\n",
      "Training Epoch: 43 [23350/36045]\tLoss: 537.1353\n",
      "Training Epoch: 43 [23400/36045]\tLoss: 582.5515\n",
      "Training Epoch: 43 [23450/36045]\tLoss: 576.1982\n",
      "Training Epoch: 43 [23500/36045]\tLoss: 555.1890\n",
      "Training Epoch: 43 [23550/36045]\tLoss: 594.9064\n",
      "Training Epoch: 43 [23600/36045]\tLoss: 675.2604\n",
      "Training Epoch: 43 [23650/36045]\tLoss: 686.8300\n",
      "Training Epoch: 43 [23700/36045]\tLoss: 694.8152\n",
      "Training Epoch: 43 [23750/36045]\tLoss: 671.1271\n",
      "Training Epoch: 43 [23800/36045]\tLoss: 538.1849\n",
      "Training Epoch: 43 [23850/36045]\tLoss: 563.9279\n",
      "Training Epoch: 43 [23900/36045]\tLoss: 553.7925\n",
      "Training Epoch: 43 [23950/36045]\tLoss: 537.0384\n",
      "Training Epoch: 43 [24000/36045]\tLoss: 514.1822\n",
      "Training Epoch: 43 [24050/36045]\tLoss: 474.6249\n",
      "Training Epoch: 43 [24100/36045]\tLoss: 499.1393\n",
      "Training Epoch: 43 [24150/36045]\tLoss: 491.3184\n",
      "Training Epoch: 43 [24200/36045]\tLoss: 488.9107\n",
      "Training Epoch: 43 [24250/36045]\tLoss: 474.9091\n",
      "Training Epoch: 43 [24300/36045]\tLoss: 513.0889\n",
      "Training Epoch: 43 [24350/36045]\tLoss: 525.2142\n",
      "Training Epoch: 43 [24400/36045]\tLoss: 540.0281\n",
      "Training Epoch: 43 [24450/36045]\tLoss: 514.6511\n",
      "Training Epoch: 43 [24500/36045]\tLoss: 543.3304\n",
      "Training Epoch: 43 [24550/36045]\tLoss: 630.2703\n",
      "Training Epoch: 43 [24600/36045]\tLoss: 621.5769\n",
      "Training Epoch: 43 [24650/36045]\tLoss: 594.8107\n",
      "Training Epoch: 43 [24700/36045]\tLoss: 604.6509\n",
      "Training Epoch: 43 [24750/36045]\tLoss: 558.9831\n",
      "Training Epoch: 43 [24800/36045]\tLoss: 458.0887\n",
      "Training Epoch: 43 [24850/36045]\tLoss: 476.4185\n",
      "Training Epoch: 43 [24900/36045]\tLoss: 473.8578\n",
      "Training Epoch: 43 [24950/36045]\tLoss: 476.3267\n",
      "Training Epoch: 43 [25000/36045]\tLoss: 458.0168\n",
      "Training Epoch: 43 [25050/36045]\tLoss: 437.9136\n",
      "Training Epoch: 43 [25100/36045]\tLoss: 392.3434\n",
      "Training Epoch: 43 [25150/36045]\tLoss: 363.2158\n",
      "Training Epoch: 43 [25200/36045]\tLoss: 358.1592\n",
      "Training Epoch: 43 [25250/36045]\tLoss: 384.0041\n",
      "Training Epoch: 43 [25300/36045]\tLoss: 504.6070\n",
      "Training Epoch: 43 [25350/36045]\tLoss: 500.8762\n",
      "Training Epoch: 43 [25400/36045]\tLoss: 468.1148\n",
      "Training Epoch: 43 [25450/36045]\tLoss: 470.1512\n",
      "Training Epoch: 43 [25500/36045]\tLoss: 510.7041\n",
      "Training Epoch: 43 [25550/36045]\tLoss: 597.5087\n",
      "Training Epoch: 43 [25600/36045]\tLoss: 601.6568\n",
      "Training Epoch: 43 [25650/36045]\tLoss: 580.7374\n",
      "Training Epoch: 43 [25700/36045]\tLoss: 590.2463\n",
      "Training Epoch: 43 [25750/36045]\tLoss: 570.3103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 43 [25800/36045]\tLoss: 359.8716\n",
      "Training Epoch: 43 [25850/36045]\tLoss: 367.7573\n",
      "Training Epoch: 43 [25900/36045]\tLoss: 349.3389\n",
      "Training Epoch: 43 [25950/36045]\tLoss: 358.8198\n",
      "Training Epoch: 43 [26000/36045]\tLoss: 441.4049\n",
      "Training Epoch: 43 [26050/36045]\tLoss: 600.6311\n",
      "Training Epoch: 43 [26100/36045]\tLoss: 626.5693\n",
      "Training Epoch: 43 [26150/36045]\tLoss: 625.9706\n",
      "Training Epoch: 43 [26200/36045]\tLoss: 599.4325\n",
      "Training Epoch: 43 [26250/36045]\tLoss: 629.4994\n",
      "Training Epoch: 43 [26300/36045]\tLoss: 577.8968\n",
      "Training Epoch: 43 [26350/36045]\tLoss: 588.7260\n",
      "Training Epoch: 43 [26400/36045]\tLoss: 564.0444\n",
      "Training Epoch: 43 [26450/36045]\tLoss: 494.1697\n",
      "Training Epoch: 43 [26500/36045]\tLoss: 585.2192\n",
      "Training Epoch: 43 [26550/36045]\tLoss: 584.9661\n",
      "Training Epoch: 43 [26600/36045]\tLoss: 581.3858\n",
      "Training Epoch: 43 [26650/36045]\tLoss: 596.2714\n",
      "Training Epoch: 43 [26700/36045]\tLoss: 575.3296\n",
      "Training Epoch: 43 [26750/36045]\tLoss: 539.5403\n",
      "Training Epoch: 43 [26800/36045]\tLoss: 398.6743\n",
      "Training Epoch: 43 [26850/36045]\tLoss: 330.1883\n",
      "Training Epoch: 43 [26900/36045]\tLoss: 332.1402\n",
      "Training Epoch: 43 [26950/36045]\tLoss: 364.4559\n",
      "Training Epoch: 43 [27000/36045]\tLoss: 600.3235\n",
      "Training Epoch: 43 [27050/36045]\tLoss: 626.6523\n",
      "Training Epoch: 43 [27100/36045]\tLoss: 607.5632\n",
      "Training Epoch: 43 [27150/36045]\tLoss: 646.5607\n",
      "Training Epoch: 43 [27200/36045]\tLoss: 469.0063\n",
      "Training Epoch: 43 [27250/36045]\tLoss: 459.5779\n",
      "Training Epoch: 43 [27300/36045]\tLoss: 448.1483\n",
      "Training Epoch: 43 [27350/36045]\tLoss: 445.5942\n",
      "Training Epoch: 43 [27400/36045]\tLoss: 445.0303\n",
      "Training Epoch: 43 [27450/36045]\tLoss: 563.2848\n",
      "Training Epoch: 43 [27500/36045]\tLoss: 603.5906\n",
      "Training Epoch: 43 [27550/36045]\tLoss: 596.9565\n",
      "Training Epoch: 43 [27600/36045]\tLoss: 608.4421\n",
      "Training Epoch: 43 [27650/36045]\tLoss: 600.4574\n",
      "Training Epoch: 43 [27700/36045]\tLoss: 627.7913\n",
      "Training Epoch: 43 [27750/36045]\tLoss: 639.6968\n",
      "Training Epoch: 43 [27800/36045]\tLoss: 627.0096\n",
      "Training Epoch: 43 [27850/36045]\tLoss: 617.3749\n",
      "Training Epoch: 43 [27900/36045]\tLoss: 561.2565\n",
      "Training Epoch: 43 [27950/36045]\tLoss: 469.5078\n",
      "Training Epoch: 43 [28000/36045]\tLoss: 446.3561\n",
      "Training Epoch: 43 [28050/36045]\tLoss: 455.8069\n",
      "Training Epoch: 43 [28100/36045]\tLoss: 447.4824\n",
      "Training Epoch: 43 [28150/36045]\tLoss: 466.7137\n",
      "Training Epoch: 43 [28200/36045]\tLoss: 475.1545\n",
      "Training Epoch: 43 [28250/36045]\tLoss: 468.0135\n",
      "Training Epoch: 43 [28300/36045]\tLoss: 444.2656\n",
      "Training Epoch: 43 [28350/36045]\tLoss: 440.0515\n",
      "Training Epoch: 43 [28400/36045]\tLoss: 767.5292\n",
      "Training Epoch: 43 [28450/36045]\tLoss: 704.5861\n",
      "Training Epoch: 43 [28500/36045]\tLoss: 608.8601\n",
      "Training Epoch: 43 [28550/36045]\tLoss: 559.8199\n",
      "Training Epoch: 43 [28600/36045]\tLoss: 584.5380\n",
      "Training Epoch: 43 [28650/36045]\tLoss: 638.6669\n",
      "Training Epoch: 43 [28700/36045]\tLoss: 632.5540\n",
      "Training Epoch: 43 [28750/36045]\tLoss: 619.5643\n",
      "Training Epoch: 43 [28800/36045]\tLoss: 629.4153\n",
      "Training Epoch: 43 [28850/36045]\tLoss: 546.6335\n",
      "Training Epoch: 43 [28900/36045]\tLoss: 444.8917\n",
      "Training Epoch: 43 [28950/36045]\tLoss: 444.0546\n",
      "Training Epoch: 43 [29000/36045]\tLoss: 440.1171\n",
      "Training Epoch: 43 [29050/36045]\tLoss: 447.0968\n",
      "Training Epoch: 43 [29100/36045]\tLoss: 465.0925\n",
      "Training Epoch: 43 [29150/36045]\tLoss: 455.4612\n",
      "Training Epoch: 43 [29200/36045]\tLoss: 441.2687\n",
      "Training Epoch: 43 [29250/36045]\tLoss: 431.1809\n",
      "Training Epoch: 43 [29300/36045]\tLoss: 485.0319\n",
      "Training Epoch: 43 [29350/36045]\tLoss: 569.5000\n",
      "Training Epoch: 43 [29400/36045]\tLoss: 587.0242\n",
      "Training Epoch: 43 [29450/36045]\tLoss: 603.6974\n",
      "Training Epoch: 43 [29500/36045]\tLoss: 618.5892\n",
      "Training Epoch: 43 [29550/36045]\tLoss: 588.4344\n",
      "Training Epoch: 43 [29600/36045]\tLoss: 494.9957\n",
      "Training Epoch: 43 [29650/36045]\tLoss: 476.5561\n",
      "Training Epoch: 43 [29700/36045]\tLoss: 427.8582\n",
      "Training Epoch: 43 [29750/36045]\tLoss: 426.2900\n",
      "Training Epoch: 43 [29800/36045]\tLoss: 473.4163\n",
      "Training Epoch: 43 [29850/36045]\tLoss: 550.6550\n",
      "Training Epoch: 43 [29900/36045]\tLoss: 546.2509\n",
      "Training Epoch: 43 [29950/36045]\tLoss: 567.9794\n",
      "Training Epoch: 43 [30000/36045]\tLoss: 541.9626\n",
      "Training Epoch: 43 [30050/36045]\tLoss: 549.1553\n",
      "Training Epoch: 43 [30100/36045]\tLoss: 669.9088\n",
      "Training Epoch: 43 [30150/36045]\tLoss: 652.2823\n",
      "Training Epoch: 43 [30200/36045]\tLoss: 615.0723\n",
      "Training Epoch: 43 [30250/36045]\tLoss: 663.4944\n",
      "Training Epoch: 43 [30300/36045]\tLoss: 647.8059\n",
      "Training Epoch: 43 [30350/36045]\tLoss: 494.6015\n",
      "Training Epoch: 43 [30400/36045]\tLoss: 479.2661\n",
      "Training Epoch: 43 [30450/36045]\tLoss: 481.5914\n",
      "Training Epoch: 43 [30500/36045]\tLoss: 449.4093\n",
      "Training Epoch: 43 [30550/36045]\tLoss: 416.9721\n",
      "Training Epoch: 43 [30600/36045]\tLoss: 409.5910\n",
      "Training Epoch: 43 [30650/36045]\tLoss: 399.7314\n",
      "Training Epoch: 43 [30700/36045]\tLoss: 417.4452\n",
      "Training Epoch: 43 [30750/36045]\tLoss: 404.4236\n",
      "Training Epoch: 43 [30800/36045]\tLoss: 430.4911\n",
      "Training Epoch: 43 [30850/36045]\tLoss: 422.2407\n",
      "Training Epoch: 43 [30900/36045]\tLoss: 434.0867\n",
      "Training Epoch: 43 [30950/36045]\tLoss: 456.2146\n",
      "Training Epoch: 43 [31000/36045]\tLoss: 448.6237\n",
      "Training Epoch: 43 [31050/36045]\tLoss: 374.5382\n",
      "Training Epoch: 43 [31100/36045]\tLoss: 365.1990\n",
      "Training Epoch: 43 [31150/36045]\tLoss: 372.8576\n",
      "Training Epoch: 43 [31200/36045]\tLoss: 462.9424\n",
      "Training Epoch: 43 [31250/36045]\tLoss: 600.7201\n",
      "Training Epoch: 43 [31300/36045]\tLoss: 571.9572\n",
      "Training Epoch: 43 [31350/36045]\tLoss: 587.9073\n",
      "Training Epoch: 43 [31400/36045]\tLoss: 567.4608\n",
      "Training Epoch: 43 [31450/36045]\tLoss: 584.1797\n",
      "Training Epoch: 43 [31500/36045]\tLoss: 596.7592\n",
      "Training Epoch: 43 [31550/36045]\tLoss: 603.5531\n",
      "Training Epoch: 43 [31600/36045]\tLoss: 567.4257\n",
      "Training Epoch: 43 [31650/36045]\tLoss: 607.5364\n",
      "Training Epoch: 43 [31700/36045]\tLoss: 439.2479\n",
      "Training Epoch: 43 [31750/36045]\tLoss: 362.9467\n",
      "Training Epoch: 43 [31800/36045]\tLoss: 346.4510\n",
      "Training Epoch: 43 [31850/36045]\tLoss: 354.1568\n",
      "Training Epoch: 43 [31900/36045]\tLoss: 560.5282\n",
      "Training Epoch: 43 [31950/36045]\tLoss: 726.4868\n",
      "Training Epoch: 43 [32000/36045]\tLoss: 834.0712\n",
      "Training Epoch: 43 [32050/36045]\tLoss: 789.7969\n",
      "Training Epoch: 43 [32100/36045]\tLoss: 780.9676\n",
      "Training Epoch: 43 [32150/36045]\tLoss: 599.1045\n",
      "Training Epoch: 43 [32200/36045]\tLoss: 601.1180\n",
      "Training Epoch: 43 [32250/36045]\tLoss: 611.3723\n",
      "Training Epoch: 43 [32300/36045]\tLoss: 593.4730\n",
      "Training Epoch: 43 [32350/36045]\tLoss: 589.4974\n",
      "Training Epoch: 43 [32400/36045]\tLoss: 553.1142\n",
      "Training Epoch: 43 [32450/36045]\tLoss: 455.3987\n",
      "Training Epoch: 43 [32500/36045]\tLoss: 437.5472\n",
      "Training Epoch: 43 [32550/36045]\tLoss: 439.6689\n",
      "Training Epoch: 43 [32600/36045]\tLoss: 436.7770\n",
      "Training Epoch: 43 [32650/36045]\tLoss: 566.2489\n",
      "Training Epoch: 43 [32700/36045]\tLoss: 618.5137\n",
      "Training Epoch: 43 [32750/36045]\tLoss: 588.9504\n",
      "Training Epoch: 43 [32800/36045]\tLoss: 603.8501\n",
      "Training Epoch: 43 [32850/36045]\tLoss: 557.4254\n",
      "Training Epoch: 43 [32900/36045]\tLoss: 445.4714\n",
      "Training Epoch: 43 [32950/36045]\tLoss: 466.6535\n",
      "Training Epoch: 43 [33000/36045]\tLoss: 465.3226\n",
      "Training Epoch: 43 [33050/36045]\tLoss: 442.6983\n",
      "Training Epoch: 43 [33100/36045]\tLoss: 503.0730\n",
      "Training Epoch: 43 [33150/36045]\tLoss: 684.7827\n",
      "Training Epoch: 43 [33200/36045]\tLoss: 666.7051\n",
      "Training Epoch: 43 [33250/36045]\tLoss: 687.0208\n",
      "Training Epoch: 43 [33300/36045]\tLoss: 732.2217\n",
      "Training Epoch: 43 [33350/36045]\tLoss: 560.5566\n",
      "Training Epoch: 43 [33400/36045]\tLoss: 408.1164\n",
      "Training Epoch: 43 [33450/36045]\tLoss: 404.0407\n",
      "Training Epoch: 43 [33500/36045]\tLoss: 415.9508\n",
      "Training Epoch: 43 [33550/36045]\tLoss: 430.7600\n",
      "Training Epoch: 43 [33600/36045]\tLoss: 432.5337\n",
      "Training Epoch: 43 [33650/36045]\tLoss: 577.8856\n",
      "Training Epoch: 43 [33700/36045]\tLoss: 558.9771\n",
      "Training Epoch: 43 [33750/36045]\tLoss: 578.9027\n",
      "Training Epoch: 43 [33800/36045]\tLoss: 576.1740\n",
      "Training Epoch: 43 [33850/36045]\tLoss: 578.0010\n",
      "Training Epoch: 43 [33900/36045]\tLoss: 591.5108\n",
      "Training Epoch: 43 [33950/36045]\tLoss: 601.1163\n",
      "Training Epoch: 43 [34000/36045]\tLoss: 587.7099\n",
      "Training Epoch: 43 [34050/36045]\tLoss: 591.5009\n",
      "Training Epoch: 43 [34100/36045]\tLoss: 570.3718\n",
      "Training Epoch: 43 [34150/36045]\tLoss: 528.3709\n",
      "Training Epoch: 43 [34200/36045]\tLoss: 500.4812\n",
      "Training Epoch: 43 [34250/36045]\tLoss: 514.4145\n",
      "Training Epoch: 43 [34300/36045]\tLoss: 438.9970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 43 [34350/36045]\tLoss: 462.4314\n",
      "Training Epoch: 43 [34400/36045]\tLoss: 455.4194\n",
      "Training Epoch: 43 [34450/36045]\tLoss: 428.8317\n",
      "Training Epoch: 43 [34500/36045]\tLoss: 457.4182\n",
      "Training Epoch: 43 [34550/36045]\tLoss: 448.9973\n",
      "Training Epoch: 43 [34600/36045]\tLoss: 455.7943\n",
      "Training Epoch: 43 [34650/36045]\tLoss: 561.5690\n",
      "Training Epoch: 43 [34700/36045]\tLoss: 595.6404\n",
      "Training Epoch: 43 [34750/36045]\tLoss: 527.8887\n",
      "Training Epoch: 43 [34800/36045]\tLoss: 606.2472\n",
      "Training Epoch: 43 [34850/36045]\tLoss: 613.3444\n",
      "Training Epoch: 43 [34900/36045]\tLoss: 661.5889\n",
      "Training Epoch: 43 [34950/36045]\tLoss: 646.3545\n",
      "Training Epoch: 43 [35000/36045]\tLoss: 647.4960\n",
      "Training Epoch: 43 [35050/36045]\tLoss: 634.7879\n",
      "Training Epoch: 43 [35100/36045]\tLoss: 547.9501\n",
      "Training Epoch: 43 [35150/36045]\tLoss: 540.1124\n",
      "Training Epoch: 43 [35200/36045]\tLoss: 454.1228\n",
      "Training Epoch: 43 [35250/36045]\tLoss: 499.0383\n",
      "Training Epoch: 43 [35300/36045]\tLoss: 515.1439\n",
      "Training Epoch: 43 [35350/36045]\tLoss: 576.1407\n",
      "Training Epoch: 43 [35400/36045]\tLoss: 606.0930\n",
      "Training Epoch: 43 [35450/36045]\tLoss: 577.3292\n",
      "Training Epoch: 43 [35500/36045]\tLoss: 558.4462\n",
      "Training Epoch: 43 [35550/36045]\tLoss: 544.8355\n",
      "Training Epoch: 43 [35600/36045]\tLoss: 595.4835\n",
      "Training Epoch: 43 [35650/36045]\tLoss: 667.9531\n",
      "Training Epoch: 43 [35700/36045]\tLoss: 594.7548\n",
      "Training Epoch: 43 [35750/36045]\tLoss: 651.9104\n",
      "Training Epoch: 43 [35800/36045]\tLoss: 658.5565\n",
      "Training Epoch: 43 [35850/36045]\tLoss: 632.8136\n",
      "Training Epoch: 43 [35900/36045]\tLoss: 655.1541\n",
      "Training Epoch: 43 [35950/36045]\tLoss: 651.8984\n",
      "Training Epoch: 43 [36000/36045]\tLoss: 644.8831\n",
      "Training Epoch: 43 [36045/36045]\tLoss: 630.2070\n",
      "Training Epoch: 43 [4004/4004]\tLoss: 585.1918\n",
      "Training Epoch: 44 [50/36045]\tLoss: 583.7346\n",
      "Training Epoch: 44 [100/36045]\tLoss: 559.5005\n",
      "Training Epoch: 44 [150/36045]\tLoss: 557.4905\n",
      "Training Epoch: 44 [200/36045]\tLoss: 543.6925\n",
      "Training Epoch: 44 [250/36045]\tLoss: 653.5137\n",
      "Training Epoch: 44 [300/36045]\tLoss: 718.5537\n",
      "Training Epoch: 44 [350/36045]\tLoss: 685.4229\n",
      "Training Epoch: 44 [400/36045]\tLoss: 679.9454\n",
      "Training Epoch: 44 [450/36045]\tLoss: 660.7161\n",
      "Training Epoch: 44 [500/36045]\tLoss: 611.5533\n",
      "Training Epoch: 44 [550/36045]\tLoss: 614.3619\n",
      "Training Epoch: 44 [600/36045]\tLoss: 600.1234\n",
      "Training Epoch: 44 [650/36045]\tLoss: 621.5478\n",
      "Training Epoch: 44 [700/36045]\tLoss: 606.9792\n",
      "Training Epoch: 44 [750/36045]\tLoss: 583.5712\n",
      "Training Epoch: 44 [800/36045]\tLoss: 595.1555\n",
      "Training Epoch: 44 [850/36045]\tLoss: 578.1643\n",
      "Training Epoch: 44 [900/36045]\tLoss: 553.6801\n",
      "Training Epoch: 44 [950/36045]\tLoss: 523.8567\n",
      "Training Epoch: 44 [1000/36045]\tLoss: 507.7116\n",
      "Training Epoch: 44 [1050/36045]\tLoss: 509.6208\n",
      "Training Epoch: 44 [1100/36045]\tLoss: 495.7679\n",
      "Training Epoch: 44 [1150/36045]\tLoss: 505.0161\n",
      "Training Epoch: 44 [1200/36045]\tLoss: 534.7421\n",
      "Training Epoch: 44 [1250/36045]\tLoss: 611.9626\n",
      "Training Epoch: 44 [1300/36045]\tLoss: 618.9316\n",
      "Training Epoch: 44 [1350/36045]\tLoss: 620.1154\n",
      "Training Epoch: 44 [1400/36045]\tLoss: 643.9059\n",
      "Training Epoch: 44 [1450/36045]\tLoss: 623.0513\n",
      "Training Epoch: 44 [1500/36045]\tLoss: 569.4655\n",
      "Training Epoch: 44 [1550/36045]\tLoss: 584.2007\n",
      "Training Epoch: 44 [1600/36045]\tLoss: 594.2391\n",
      "Training Epoch: 44 [1650/36045]\tLoss: 581.5160\n",
      "Training Epoch: 44 [1700/36045]\tLoss: 593.6843\n",
      "Training Epoch: 44 [1750/36045]\tLoss: 635.1458\n",
      "Training Epoch: 44 [1800/36045]\tLoss: 617.8394\n",
      "Training Epoch: 44 [1850/36045]\tLoss: 633.7513\n",
      "Training Epoch: 44 [1900/36045]\tLoss: 592.6992\n",
      "Training Epoch: 44 [1950/36045]\tLoss: 602.5647\n",
      "Training Epoch: 44 [2000/36045]\tLoss: 542.7355\n",
      "Training Epoch: 44 [2050/36045]\tLoss: 545.3369\n",
      "Training Epoch: 44 [2100/36045]\tLoss: 574.8382\n",
      "Training Epoch: 44 [2150/36045]\tLoss: 555.7942\n",
      "Training Epoch: 44 [2200/36045]\tLoss: 517.9178\n",
      "Training Epoch: 44 [2250/36045]\tLoss: 489.1826\n",
      "Training Epoch: 44 [2300/36045]\tLoss: 512.8710\n",
      "Training Epoch: 44 [2350/36045]\tLoss: 490.2870\n",
      "Training Epoch: 44 [2400/36045]\tLoss: 497.8922\n",
      "Training Epoch: 44 [2450/36045]\tLoss: 637.7165\n",
      "Training Epoch: 44 [2500/36045]\tLoss: 669.7066\n",
      "Training Epoch: 44 [2550/36045]\tLoss: 667.5029\n",
      "Training Epoch: 44 [2600/36045]\tLoss: 676.0862\n",
      "Training Epoch: 44 [2650/36045]\tLoss: 800.3270\n",
      "Training Epoch: 44 [2700/36045]\tLoss: 888.1443\n",
      "Training Epoch: 44 [2750/36045]\tLoss: 959.0794\n",
      "Training Epoch: 44 [2800/36045]\tLoss: 968.5284\n",
      "Training Epoch: 44 [2850/36045]\tLoss: 735.8546\n",
      "Training Epoch: 44 [2900/36045]\tLoss: 696.9435\n",
      "Training Epoch: 44 [2950/36045]\tLoss: 674.0790\n",
      "Training Epoch: 44 [3000/36045]\tLoss: 667.9988\n",
      "Training Epoch: 44 [3050/36045]\tLoss: 698.8444\n",
      "Training Epoch: 44 [3100/36045]\tLoss: 638.7073\n",
      "Training Epoch: 44 [3150/36045]\tLoss: 491.2726\n",
      "Training Epoch: 44 [3200/36045]\tLoss: 508.6055\n",
      "Training Epoch: 44 [3250/36045]\tLoss: 479.7818\n",
      "Training Epoch: 44 [3300/36045]\tLoss: 454.3536\n",
      "Training Epoch: 44 [3350/36045]\tLoss: 480.0179\n",
      "Training Epoch: 44 [3400/36045]\tLoss: 502.9977\n",
      "Training Epoch: 44 [3450/36045]\tLoss: 539.5574\n",
      "Training Epoch: 44 [3500/36045]\tLoss: 526.7892\n",
      "Training Epoch: 44 [3550/36045]\tLoss: 503.7777\n",
      "Training Epoch: 44 [3600/36045]\tLoss: 541.3033\n",
      "Training Epoch: 44 [3650/36045]\tLoss: 626.1644\n",
      "Training Epoch: 44 [3700/36045]\tLoss: 633.9155\n",
      "Training Epoch: 44 [3750/36045]\tLoss: 603.0844\n",
      "Training Epoch: 44 [3800/36045]\tLoss: 599.8597\n",
      "Training Epoch: 44 [3850/36045]\tLoss: 601.3399\n",
      "Training Epoch: 44 [3900/36045]\tLoss: 606.0596\n",
      "Training Epoch: 44 [3950/36045]\tLoss: 584.9887\n",
      "Training Epoch: 44 [4000/36045]\tLoss: 589.6920\n",
      "Training Epoch: 44 [4050/36045]\tLoss: 540.7000\n",
      "Training Epoch: 44 [4100/36045]\tLoss: 527.2602\n",
      "Training Epoch: 44 [4150/36045]\tLoss: 541.4269\n",
      "Training Epoch: 44 [4200/36045]\tLoss: 536.4554\n",
      "Training Epoch: 44 [4250/36045]\tLoss: 538.7843\n",
      "Training Epoch: 44 [4300/36045]\tLoss: 554.3295\n",
      "Training Epoch: 44 [4350/36045]\tLoss: 537.9596\n",
      "Training Epoch: 44 [4400/36045]\tLoss: 514.4633\n",
      "Training Epoch: 44 [4450/36045]\tLoss: 565.0864\n",
      "Training Epoch: 44 [4500/36045]\tLoss: 608.1082\n",
      "Training Epoch: 44 [4550/36045]\tLoss: 611.0501\n",
      "Training Epoch: 44 [4600/36045]\tLoss: 632.9400\n",
      "Training Epoch: 44 [4650/36045]\tLoss: 622.7572\n",
      "Training Epoch: 44 [4700/36045]\tLoss: 574.2125\n",
      "Training Epoch: 44 [4750/36045]\tLoss: 556.3511\n",
      "Training Epoch: 44 [4800/36045]\tLoss: 580.7002\n",
      "Training Epoch: 44 [4850/36045]\tLoss: 567.1987\n",
      "Training Epoch: 44 [4900/36045]\tLoss: 552.0768\n",
      "Training Epoch: 44 [4950/36045]\tLoss: 567.4485\n",
      "Training Epoch: 44 [5000/36045]\tLoss: 596.9532\n",
      "Training Epoch: 44 [5050/36045]\tLoss: 578.3945\n",
      "Training Epoch: 44 [5100/36045]\tLoss: 588.5601\n",
      "Training Epoch: 44 [5150/36045]\tLoss: 572.8995\n",
      "Training Epoch: 44 [5200/36045]\tLoss: 570.8110\n",
      "Training Epoch: 44 [5250/36045]\tLoss: 564.4866\n",
      "Training Epoch: 44 [5300/36045]\tLoss: 564.6437\n",
      "Training Epoch: 44 [5350/36045]\tLoss: 585.9048\n",
      "Training Epoch: 44 [5400/36045]\tLoss: 564.6738\n",
      "Training Epoch: 44 [5450/36045]\tLoss: 535.0486\n",
      "Training Epoch: 44 [5500/36045]\tLoss: 563.1971\n",
      "Training Epoch: 44 [5550/36045]\tLoss: 551.4602\n",
      "Training Epoch: 44 [5600/36045]\tLoss: 630.4072\n",
      "Training Epoch: 44 [5650/36045]\tLoss: 596.0440\n",
      "Training Epoch: 44 [5700/36045]\tLoss: 559.0560\n",
      "Training Epoch: 44 [5750/36045]\tLoss: 543.1395\n",
      "Training Epoch: 44 [5800/36045]\tLoss: 573.0427\n",
      "Training Epoch: 44 [5850/36045]\tLoss: 562.5246\n",
      "Training Epoch: 44 [5900/36045]\tLoss: 646.3278\n",
      "Training Epoch: 44 [5950/36045]\tLoss: 662.8221\n",
      "Training Epoch: 44 [6000/36045]\tLoss: 648.7959\n",
      "Training Epoch: 44 [6050/36045]\tLoss: 626.9312\n",
      "Training Epoch: 44 [6100/36045]\tLoss: 631.1700\n",
      "Training Epoch: 44 [6150/36045]\tLoss: 621.6656\n",
      "Training Epoch: 44 [6200/36045]\tLoss: 626.1379\n",
      "Training Epoch: 44 [6250/36045]\tLoss: 647.6656\n",
      "Training Epoch: 44 [6300/36045]\tLoss: 659.2488\n",
      "Training Epoch: 44 [6350/36045]\tLoss: 704.5895\n",
      "Training Epoch: 44 [6400/36045]\tLoss: 580.6910\n",
      "Training Epoch: 44 [6450/36045]\tLoss: 534.0297\n",
      "Training Epoch: 44 [6500/36045]\tLoss: 543.7719\n",
      "Training Epoch: 44 [6550/36045]\tLoss: 561.1982\n",
      "Training Epoch: 44 [6600/36045]\tLoss: 559.2371\n",
      "Training Epoch: 44 [6650/36045]\tLoss: 631.4305\n",
      "Training Epoch: 44 [6700/36045]\tLoss: 660.3445\n",
      "Training Epoch: 44 [6750/36045]\tLoss: 637.7295\n",
      "Training Epoch: 44 [6800/36045]\tLoss: 640.7019\n",
      "Training Epoch: 44 [6850/36045]\tLoss: 628.7955\n",
      "Training Epoch: 44 [6900/36045]\tLoss: 560.1777\n",
      "Training Epoch: 44 [6950/36045]\tLoss: 528.1589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 44 [7000/36045]\tLoss: 561.7111\n",
      "Training Epoch: 44 [7050/36045]\tLoss: 573.9735\n",
      "Training Epoch: 44 [7100/36045]\tLoss: 573.2292\n",
      "Training Epoch: 44 [7150/36045]\tLoss: 583.3171\n",
      "Training Epoch: 44 [7200/36045]\tLoss: 585.5542\n",
      "Training Epoch: 44 [7250/36045]\tLoss: 583.8065\n",
      "Training Epoch: 44 [7300/36045]\tLoss: 570.1809\n",
      "Training Epoch: 44 [7350/36045]\tLoss: 567.8718\n",
      "Training Epoch: 44 [7400/36045]\tLoss: 517.6685\n",
      "Training Epoch: 44 [7450/36045]\tLoss: 520.8843\n",
      "Training Epoch: 44 [7500/36045]\tLoss: 516.4372\n",
      "Training Epoch: 44 [7550/36045]\tLoss: 494.6404\n",
      "Training Epoch: 44 [7600/36045]\tLoss: 548.1933\n",
      "Training Epoch: 44 [7650/36045]\tLoss: 585.9680\n",
      "Training Epoch: 44 [7700/36045]\tLoss: 557.6378\n",
      "Training Epoch: 44 [7750/36045]\tLoss: 572.0541\n",
      "Training Epoch: 44 [7800/36045]\tLoss: 561.6105\n",
      "Training Epoch: 44 [7850/36045]\tLoss: 543.5626\n",
      "Training Epoch: 44 [7900/36045]\tLoss: 573.2537\n",
      "Training Epoch: 44 [7950/36045]\tLoss: 570.7930\n",
      "Training Epoch: 44 [8000/36045]\tLoss: 588.9285\n",
      "Training Epoch: 44 [8050/36045]\tLoss: 554.5842\n",
      "Training Epoch: 44 [8100/36045]\tLoss: 579.6605\n",
      "Training Epoch: 44 [8150/36045]\tLoss: 657.4373\n",
      "Training Epoch: 44 [8200/36045]\tLoss: 644.8716\n",
      "Training Epoch: 44 [8250/36045]\tLoss: 613.1863\n",
      "Training Epoch: 44 [8300/36045]\tLoss: 670.1387\n",
      "Training Epoch: 44 [8350/36045]\tLoss: 614.3185\n",
      "Training Epoch: 44 [8400/36045]\tLoss: 549.5181\n",
      "Training Epoch: 44 [8450/36045]\tLoss: 514.2772\n",
      "Training Epoch: 44 [8500/36045]\tLoss: 547.2610\n",
      "Training Epoch: 44 [8550/36045]\tLoss: 540.6635\n",
      "Training Epoch: 44 [8600/36045]\tLoss: 534.9872\n",
      "Training Epoch: 44 [8650/36045]\tLoss: 567.9264\n",
      "Training Epoch: 44 [8700/36045]\tLoss: 600.7000\n",
      "Training Epoch: 44 [8750/36045]\tLoss: 590.1426\n",
      "Training Epoch: 44 [8800/36045]\tLoss: 595.8902\n",
      "Training Epoch: 44 [8850/36045]\tLoss: 589.4756\n",
      "Training Epoch: 44 [8900/36045]\tLoss: 532.1874\n",
      "Training Epoch: 44 [8950/36045]\tLoss: 542.9487\n",
      "Training Epoch: 44 [9000/36045]\tLoss: 558.8058\n",
      "Training Epoch: 44 [9050/36045]\tLoss: 560.4009\n",
      "Training Epoch: 44 [9100/36045]\tLoss: 577.1274\n",
      "Training Epoch: 44 [9150/36045]\tLoss: 427.1584\n",
      "Training Epoch: 44 [9200/36045]\tLoss: 319.1468\n",
      "Training Epoch: 44 [9250/36045]\tLoss: 346.3444\n",
      "Training Epoch: 44 [9300/36045]\tLoss: 356.0487\n",
      "Training Epoch: 44 [9350/36045]\tLoss: 328.6154\n",
      "Training Epoch: 44 [9400/36045]\tLoss: 643.6360\n",
      "Training Epoch: 44 [9450/36045]\tLoss: 683.5321\n",
      "Training Epoch: 44 [9500/36045]\tLoss: 671.1021\n",
      "Training Epoch: 44 [9550/36045]\tLoss: 709.9761\n",
      "Training Epoch: 44 [9600/36045]\tLoss: 529.0063\n",
      "Training Epoch: 44 [9650/36045]\tLoss: 534.0967\n",
      "Training Epoch: 44 [9700/36045]\tLoss: 519.6305\n",
      "Training Epoch: 44 [9750/36045]\tLoss: 518.2443\n",
      "Training Epoch: 44 [9800/36045]\tLoss: 677.9056\n",
      "Training Epoch: 44 [9850/36045]\tLoss: 715.9933\n",
      "Training Epoch: 44 [9900/36045]\tLoss: 725.4590\n",
      "Training Epoch: 44 [9950/36045]\tLoss: 706.9012\n",
      "Training Epoch: 44 [10000/36045]\tLoss: 654.1588\n",
      "Training Epoch: 44 [10050/36045]\tLoss: 535.6436\n",
      "Training Epoch: 44 [10100/36045]\tLoss: 543.3409\n",
      "Training Epoch: 44 [10150/36045]\tLoss: 551.6188\n",
      "Training Epoch: 44 [10200/36045]\tLoss: 540.5507\n",
      "Training Epoch: 44 [10250/36045]\tLoss: 649.6122\n",
      "Training Epoch: 44 [10300/36045]\tLoss: 631.3583\n",
      "Training Epoch: 44 [10350/36045]\tLoss: 665.0930\n",
      "Training Epoch: 44 [10400/36045]\tLoss: 655.2153\n",
      "Training Epoch: 44 [10450/36045]\tLoss: 613.8890\n",
      "Training Epoch: 44 [10500/36045]\tLoss: 512.6530\n",
      "Training Epoch: 44 [10550/36045]\tLoss: 507.3096\n",
      "Training Epoch: 44 [10600/36045]\tLoss: 529.3000\n",
      "Training Epoch: 44 [10650/36045]\tLoss: 535.2267\n",
      "Training Epoch: 44 [10700/36045]\tLoss: 615.1213\n",
      "Training Epoch: 44 [10750/36045]\tLoss: 674.2033\n",
      "Training Epoch: 44 [10800/36045]\tLoss: 620.0109\n",
      "Training Epoch: 44 [10850/36045]\tLoss: 657.5309\n",
      "Training Epoch: 44 [10900/36045]\tLoss: 684.5187\n",
      "Training Epoch: 44 [10950/36045]\tLoss: 503.6005\n",
      "Training Epoch: 44 [11000/36045]\tLoss: 497.3126\n",
      "Training Epoch: 44 [11050/36045]\tLoss: 533.3876\n",
      "Training Epoch: 44 [11100/36045]\tLoss: 543.5677\n",
      "Training Epoch: 44 [11150/36045]\tLoss: 589.7169\n",
      "Training Epoch: 44 [11200/36045]\tLoss: 618.0189\n",
      "Training Epoch: 44 [11250/36045]\tLoss: 629.4099\n",
      "Training Epoch: 44 [11300/36045]\tLoss: 609.7352\n",
      "Training Epoch: 44 [11350/36045]\tLoss: 607.4397\n",
      "Training Epoch: 44 [11400/36045]\tLoss: 570.9307\n",
      "Training Epoch: 44 [11450/36045]\tLoss: 540.1258\n",
      "Training Epoch: 44 [11500/36045]\tLoss: 537.5148\n",
      "Training Epoch: 44 [11550/36045]\tLoss: 547.1404\n",
      "Training Epoch: 44 [11600/36045]\tLoss: 607.2108\n",
      "Training Epoch: 44 [11650/36045]\tLoss: 658.6638\n",
      "Training Epoch: 44 [11700/36045]\tLoss: 657.6002\n",
      "Training Epoch: 44 [11750/36045]\tLoss: 675.6022\n",
      "Training Epoch: 44 [11800/36045]\tLoss: 718.0816\n",
      "Training Epoch: 44 [11850/36045]\tLoss: 776.4998\n",
      "Training Epoch: 44 [11900/36045]\tLoss: 988.6356\n",
      "Training Epoch: 44 [11950/36045]\tLoss: 991.7463\n",
      "Training Epoch: 44 [12000/36045]\tLoss: 1003.1281\n",
      "Training Epoch: 44 [12050/36045]\tLoss: 962.6760\n",
      "Training Epoch: 44 [12100/36045]\tLoss: 616.1251\n",
      "Training Epoch: 44 [12150/36045]\tLoss: 463.7750\n",
      "Training Epoch: 44 [12200/36045]\tLoss: 458.6019\n",
      "Training Epoch: 44 [12250/36045]\tLoss: 467.2271\n",
      "Training Epoch: 44 [12300/36045]\tLoss: 602.4980\n",
      "Training Epoch: 44 [12350/36045]\tLoss: 657.7657\n",
      "Training Epoch: 44 [12400/36045]\tLoss: 664.9995\n",
      "Training Epoch: 44 [12450/36045]\tLoss: 653.8851\n",
      "Training Epoch: 44 [12500/36045]\tLoss: 680.8406\n",
      "Training Epoch: 44 [12550/36045]\tLoss: 650.7784\n",
      "Training Epoch: 44 [12600/36045]\tLoss: 595.2364\n",
      "Training Epoch: 44 [12650/36045]\tLoss: 593.4683\n",
      "Training Epoch: 44 [12700/36045]\tLoss: 614.7314\n",
      "Training Epoch: 44 [12750/36045]\tLoss: 613.3036\n",
      "Training Epoch: 44 [12800/36045]\tLoss: 599.3489\n",
      "Training Epoch: 44 [12850/36045]\tLoss: 628.5405\n",
      "Training Epoch: 44 [12900/36045]\tLoss: 602.5388\n",
      "Training Epoch: 44 [12950/36045]\tLoss: 588.3334\n",
      "Training Epoch: 44 [13000/36045]\tLoss: 621.5555\n",
      "Training Epoch: 44 [13050/36045]\tLoss: 562.1017\n",
      "Training Epoch: 44 [13100/36045]\tLoss: 577.5833\n",
      "Training Epoch: 44 [13150/36045]\tLoss: 569.2013\n",
      "Training Epoch: 44 [13200/36045]\tLoss: 552.2175\n",
      "Training Epoch: 44 [13250/36045]\tLoss: 574.0108\n",
      "Training Epoch: 44 [13300/36045]\tLoss: 610.9141\n",
      "Training Epoch: 44 [13350/36045]\tLoss: 591.8908\n",
      "Training Epoch: 44 [13400/36045]\tLoss: 594.9936\n",
      "Training Epoch: 44 [13450/36045]\tLoss: 592.4180\n",
      "Training Epoch: 44 [13500/36045]\tLoss: 611.1028\n",
      "Training Epoch: 44 [13550/36045]\tLoss: 748.9799\n",
      "Training Epoch: 44 [13600/36045]\tLoss: 781.6095\n",
      "Training Epoch: 44 [13650/36045]\tLoss: 863.2716\n",
      "Training Epoch: 44 [13700/36045]\tLoss: 760.3875\n",
      "Training Epoch: 44 [13750/36045]\tLoss: 598.4879\n",
      "Training Epoch: 44 [13800/36045]\tLoss: 569.3896\n",
      "Training Epoch: 44 [13850/36045]\tLoss: 552.0609\n",
      "Training Epoch: 44 [13900/36045]\tLoss: 559.3132\n",
      "Training Epoch: 44 [13950/36045]\tLoss: 604.4925\n",
      "Training Epoch: 44 [14000/36045]\tLoss: 637.2570\n",
      "Training Epoch: 44 [14050/36045]\tLoss: 612.3267\n",
      "Training Epoch: 44 [14100/36045]\tLoss: 607.3635\n",
      "Training Epoch: 44 [14150/36045]\tLoss: 595.4198\n",
      "Training Epoch: 44 [14200/36045]\tLoss: 635.9378\n",
      "Training Epoch: 44 [14250/36045]\tLoss: 698.9396\n",
      "Training Epoch: 44 [14300/36045]\tLoss: 702.1735\n",
      "Training Epoch: 44 [14350/36045]\tLoss: 671.0176\n",
      "Training Epoch: 44 [14400/36045]\tLoss: 657.0396\n",
      "Training Epoch: 44 [14450/36045]\tLoss: 692.4626\n",
      "Training Epoch: 44 [14500/36045]\tLoss: 624.0322\n",
      "Training Epoch: 44 [14550/36045]\tLoss: 652.1027\n",
      "Training Epoch: 44 [14600/36045]\tLoss: 638.6211\n",
      "Training Epoch: 44 [14650/36045]\tLoss: 638.1896\n",
      "Training Epoch: 44 [14700/36045]\tLoss: 605.1145\n",
      "Training Epoch: 44 [14750/36045]\tLoss: 520.6693\n",
      "Training Epoch: 44 [14800/36045]\tLoss: 510.7631\n",
      "Training Epoch: 44 [14850/36045]\tLoss: 517.7479\n",
      "Training Epoch: 44 [14900/36045]\tLoss: 511.2227\n",
      "Training Epoch: 44 [14950/36045]\tLoss: 519.3300\n",
      "Training Epoch: 44 [15000/36045]\tLoss: 531.9781\n",
      "Training Epoch: 44 [15050/36045]\tLoss: 528.6666\n",
      "Training Epoch: 44 [15100/36045]\tLoss: 512.6632\n",
      "Training Epoch: 44 [15150/36045]\tLoss: 508.3170\n",
      "Training Epoch: 44 [15200/36045]\tLoss: 470.9517\n",
      "Training Epoch: 44 [15250/36045]\tLoss: 492.6015\n",
      "Training Epoch: 44 [15300/36045]\tLoss: 478.3094\n",
      "Training Epoch: 44 [15350/36045]\tLoss: 489.8546\n",
      "Training Epoch: 44 [15400/36045]\tLoss: 472.3753\n",
      "Training Epoch: 44 [15450/36045]\tLoss: 457.5504\n",
      "Training Epoch: 44 [15500/36045]\tLoss: 470.6086\n",
      "Training Epoch: 44 [15550/36045]\tLoss: 467.3058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 44 [15600/36045]\tLoss: 534.3264\n",
      "Training Epoch: 44 [15650/36045]\tLoss: 550.9983\n",
      "Training Epoch: 44 [15700/36045]\tLoss: 543.2390\n",
      "Training Epoch: 44 [15750/36045]\tLoss: 534.6126\n",
      "Training Epoch: 44 [15800/36045]\tLoss: 512.2623\n",
      "Training Epoch: 44 [15850/36045]\tLoss: 527.5473\n",
      "Training Epoch: 44 [15900/36045]\tLoss: 536.4680\n",
      "Training Epoch: 44 [15950/36045]\tLoss: 556.2382\n",
      "Training Epoch: 44 [16000/36045]\tLoss: 527.1420\n",
      "Training Epoch: 44 [16050/36045]\tLoss: 496.6687\n",
      "Training Epoch: 44 [16100/36045]\tLoss: 460.9451\n",
      "Training Epoch: 44 [16150/36045]\tLoss: 449.0176\n",
      "Training Epoch: 44 [16200/36045]\tLoss: 545.4598\n",
      "Training Epoch: 44 [16250/36045]\tLoss: 572.9037\n",
      "Training Epoch: 44 [16300/36045]\tLoss: 625.3773\n",
      "Training Epoch: 44 [16350/36045]\tLoss: 645.6241\n",
      "Training Epoch: 44 [16400/36045]\tLoss: 617.4439\n",
      "Training Epoch: 44 [16450/36045]\tLoss: 599.8427\n",
      "Training Epoch: 44 [16500/36045]\tLoss: 599.7239\n",
      "Training Epoch: 44 [16550/36045]\tLoss: 565.1574\n",
      "Training Epoch: 44 [16600/36045]\tLoss: 587.0093\n",
      "Training Epoch: 44 [16650/36045]\tLoss: 603.0052\n",
      "Training Epoch: 44 [16700/36045]\tLoss: 583.3160\n",
      "Training Epoch: 44 [16750/36045]\tLoss: 576.0499\n",
      "Training Epoch: 44 [16800/36045]\tLoss: 584.6849\n",
      "Training Epoch: 44 [16850/36045]\tLoss: 557.2369\n",
      "Training Epoch: 44 [16900/36045]\tLoss: 566.9842\n",
      "Training Epoch: 44 [16950/36045]\tLoss: 589.7194\n",
      "Training Epoch: 44 [17000/36045]\tLoss: 574.1838\n",
      "Training Epoch: 44 [17050/36045]\tLoss: 597.9307\n",
      "Training Epoch: 44 [17100/36045]\tLoss: 593.8137\n",
      "Training Epoch: 44 [17150/36045]\tLoss: 515.4108\n",
      "Training Epoch: 44 [17200/36045]\tLoss: 477.8326\n",
      "Training Epoch: 44 [17250/36045]\tLoss: 500.4846\n",
      "Training Epoch: 44 [17300/36045]\tLoss: 529.7441\n",
      "Training Epoch: 44 [17350/36045]\tLoss: 510.9639\n",
      "Training Epoch: 44 [17400/36045]\tLoss: 530.5064\n",
      "Training Epoch: 44 [17450/36045]\tLoss: 548.8448\n",
      "Training Epoch: 44 [17500/36045]\tLoss: 537.2834\n",
      "Training Epoch: 44 [17550/36045]\tLoss: 535.8397\n",
      "Training Epoch: 44 [17600/36045]\tLoss: 530.3128\n",
      "Training Epoch: 44 [17650/36045]\tLoss: 545.6672\n",
      "Training Epoch: 44 [17700/36045]\tLoss: 525.1028\n",
      "Training Epoch: 44 [17750/36045]\tLoss: 540.9262\n",
      "Training Epoch: 44 [17800/36045]\tLoss: 532.1968\n",
      "Training Epoch: 44 [17850/36045]\tLoss: 548.5633\n",
      "Training Epoch: 44 [17900/36045]\tLoss: 576.4487\n",
      "Training Epoch: 44 [17950/36045]\tLoss: 588.6980\n",
      "Training Epoch: 44 [18000/36045]\tLoss: 579.4376\n",
      "Training Epoch: 44 [18050/36045]\tLoss: 635.8746\n",
      "Training Epoch: 44 [18100/36045]\tLoss: 636.9441\n",
      "Training Epoch: 44 [18150/36045]\tLoss: 648.1835\n",
      "Training Epoch: 44 [18200/36045]\tLoss: 630.5645\n",
      "Training Epoch: 44 [18250/36045]\tLoss: 651.2114\n",
      "Training Epoch: 44 [18300/36045]\tLoss: 607.3474\n",
      "Training Epoch: 44 [18350/36045]\tLoss: 680.4752\n",
      "Training Epoch: 44 [18400/36045]\tLoss: 653.2997\n",
      "Training Epoch: 44 [18450/36045]\tLoss: 633.0867\n",
      "Training Epoch: 44 [18500/36045]\tLoss: 631.9003\n",
      "Training Epoch: 44 [18550/36045]\tLoss: 619.5065\n",
      "Training Epoch: 44 [18600/36045]\tLoss: 609.2161\n",
      "Training Epoch: 44 [18650/36045]\tLoss: 654.5870\n",
      "Training Epoch: 44 [18700/36045]\tLoss: 688.6367\n",
      "Training Epoch: 44 [18750/36045]\tLoss: 676.2369\n",
      "Training Epoch: 44 [18800/36045]\tLoss: 698.9023\n",
      "Training Epoch: 44 [18850/36045]\tLoss: 644.0889\n",
      "Training Epoch: 44 [18900/36045]\tLoss: 688.8743\n",
      "Training Epoch: 44 [18950/36045]\tLoss: 631.5485\n",
      "Training Epoch: 44 [19000/36045]\tLoss: 518.5032\n",
      "Training Epoch: 44 [19050/36045]\tLoss: 503.2185\n",
      "Training Epoch: 44 [19100/36045]\tLoss: 511.2917\n",
      "Training Epoch: 44 [19150/36045]\tLoss: 501.5015\n",
      "Training Epoch: 44 [19200/36045]\tLoss: 532.5497\n",
      "Training Epoch: 44 [19250/36045]\tLoss: 547.7290\n",
      "Training Epoch: 44 [19300/36045]\tLoss: 557.0319\n",
      "Training Epoch: 44 [19350/36045]\tLoss: 540.8799\n",
      "Training Epoch: 44 [19400/36045]\tLoss: 560.9886\n",
      "Training Epoch: 44 [19450/36045]\tLoss: 552.4349\n",
      "Training Epoch: 44 [19500/36045]\tLoss: 553.6250\n",
      "Training Epoch: 44 [19550/36045]\tLoss: 551.8124\n",
      "Training Epoch: 44 [19600/36045]\tLoss: 592.6093\n",
      "Training Epoch: 44 [19650/36045]\tLoss: 792.7348\n",
      "Training Epoch: 44 [19700/36045]\tLoss: 751.4653\n",
      "Training Epoch: 44 [19750/36045]\tLoss: 755.5684\n",
      "Training Epoch: 44 [19800/36045]\tLoss: 756.0236\n",
      "Training Epoch: 44 [19850/36045]\tLoss: 493.9258\n",
      "Training Epoch: 44 [19900/36045]\tLoss: 473.1680\n",
      "Training Epoch: 44 [19950/36045]\tLoss: 477.0431\n",
      "Training Epoch: 44 [20000/36045]\tLoss: 476.9799\n",
      "Training Epoch: 44 [20050/36045]\tLoss: 533.7593\n",
      "Training Epoch: 44 [20100/36045]\tLoss: 539.1637\n",
      "Training Epoch: 44 [20150/36045]\tLoss: 540.2169\n",
      "Training Epoch: 44 [20200/36045]\tLoss: 539.9633\n",
      "Training Epoch: 44 [20250/36045]\tLoss: 576.0792\n",
      "Training Epoch: 44 [20300/36045]\tLoss: 612.1802\n",
      "Training Epoch: 44 [20350/36045]\tLoss: 630.2943\n",
      "Training Epoch: 44 [20400/36045]\tLoss: 646.6169\n",
      "Training Epoch: 44 [20450/36045]\tLoss: 616.4501\n",
      "Training Epoch: 44 [20500/36045]\tLoss: 601.2620\n",
      "Training Epoch: 44 [20550/36045]\tLoss: 528.1614\n",
      "Training Epoch: 44 [20600/36045]\tLoss: 537.9164\n",
      "Training Epoch: 44 [20650/36045]\tLoss: 535.2654\n",
      "Training Epoch: 44 [20700/36045]\tLoss: 523.3380\n",
      "Training Epoch: 44 [20750/36045]\tLoss: 564.4291\n",
      "Training Epoch: 44 [20800/36045]\tLoss: 613.2369\n",
      "Training Epoch: 44 [20850/36045]\tLoss: 600.1161\n",
      "Training Epoch: 44 [20900/36045]\tLoss: 643.0511\n",
      "Training Epoch: 44 [20950/36045]\tLoss: 605.8974\n",
      "Training Epoch: 44 [21000/36045]\tLoss: 570.2235\n",
      "Training Epoch: 44 [21050/36045]\tLoss: 487.6784\n",
      "Training Epoch: 44 [21100/36045]\tLoss: 492.7068\n",
      "Training Epoch: 44 [21150/36045]\tLoss: 527.3777\n",
      "Training Epoch: 44 [21200/36045]\tLoss: 526.4702\n",
      "Training Epoch: 44 [21250/36045]\tLoss: 503.4764\n",
      "Training Epoch: 44 [21300/36045]\tLoss: 587.7166\n",
      "Training Epoch: 44 [21350/36045]\tLoss: 579.5038\n",
      "Training Epoch: 44 [21400/36045]\tLoss: 583.2762\n",
      "Training Epoch: 44 [21450/36045]\tLoss: 589.4998\n",
      "Training Epoch: 44 [21500/36045]\tLoss: 591.1160\n",
      "Training Epoch: 44 [21550/36045]\tLoss: 685.2579\n",
      "Training Epoch: 44 [21600/36045]\tLoss: 683.6484\n",
      "Training Epoch: 44 [21650/36045]\tLoss: 695.9690\n",
      "Training Epoch: 44 [21700/36045]\tLoss: 699.3514\n",
      "Training Epoch: 44 [21750/36045]\tLoss: 671.3947\n",
      "Training Epoch: 44 [21800/36045]\tLoss: 492.9942\n",
      "Training Epoch: 44 [21850/36045]\tLoss: 475.9483\n",
      "Training Epoch: 44 [21900/36045]\tLoss: 485.1469\n",
      "Training Epoch: 44 [21950/36045]\tLoss: 486.5851\n",
      "Training Epoch: 44 [22000/36045]\tLoss: 489.8597\n",
      "Training Epoch: 44 [22050/36045]\tLoss: 509.5069\n",
      "Training Epoch: 44 [22100/36045]\tLoss: 501.9618\n",
      "Training Epoch: 44 [22150/36045]\tLoss: 488.3775\n",
      "Training Epoch: 44 [22200/36045]\tLoss: 504.0349\n",
      "Training Epoch: 44 [22250/36045]\tLoss: 509.0469\n",
      "Training Epoch: 44 [22300/36045]\tLoss: 560.8760\n",
      "Training Epoch: 44 [22350/36045]\tLoss: 586.3926\n",
      "Training Epoch: 44 [22400/36045]\tLoss: 600.1646\n",
      "Training Epoch: 44 [22450/36045]\tLoss: 587.4575\n",
      "Training Epoch: 44 [22500/36045]\tLoss: 570.6088\n",
      "Training Epoch: 44 [22550/36045]\tLoss: 605.4930\n",
      "Training Epoch: 44 [22600/36045]\tLoss: 654.1033\n",
      "Training Epoch: 44 [22650/36045]\tLoss: 687.0233\n",
      "Training Epoch: 44 [22700/36045]\tLoss: 708.2926\n",
      "Training Epoch: 44 [22750/36045]\tLoss: 727.7960\n",
      "Training Epoch: 44 [22800/36045]\tLoss: 756.3759\n",
      "Training Epoch: 44 [22850/36045]\tLoss: 628.1375\n",
      "Training Epoch: 44 [22900/36045]\tLoss: 632.8954\n",
      "Training Epoch: 44 [22950/36045]\tLoss: 612.4603\n",
      "Training Epoch: 44 [23000/36045]\tLoss: 608.9113\n",
      "Training Epoch: 44 [23050/36045]\tLoss: 540.9351\n",
      "Training Epoch: 44 [23100/36045]\tLoss: 556.5335\n",
      "Training Epoch: 44 [23150/36045]\tLoss: 544.8577\n",
      "Training Epoch: 44 [23200/36045]\tLoss: 515.6775\n",
      "Training Epoch: 44 [23250/36045]\tLoss: 518.6535\n",
      "Training Epoch: 44 [23300/36045]\tLoss: 514.6343\n",
      "Training Epoch: 44 [23350/36045]\tLoss: 534.9998\n",
      "Training Epoch: 44 [23400/36045]\tLoss: 580.2583\n",
      "Training Epoch: 44 [23450/36045]\tLoss: 573.9170\n",
      "Training Epoch: 44 [23500/36045]\tLoss: 552.9620\n",
      "Training Epoch: 44 [23550/36045]\tLoss: 592.5269\n",
      "Training Epoch: 44 [23600/36045]\tLoss: 672.7639\n",
      "Training Epoch: 44 [23650/36045]\tLoss: 684.2186\n",
      "Training Epoch: 44 [23700/36045]\tLoss: 692.1891\n",
      "Training Epoch: 44 [23750/36045]\tLoss: 668.5216\n",
      "Training Epoch: 44 [23800/36045]\tLoss: 536.2365\n",
      "Training Epoch: 44 [23850/36045]\tLoss: 561.9456\n",
      "Training Epoch: 44 [23900/36045]\tLoss: 551.8398\n",
      "Training Epoch: 44 [23950/36045]\tLoss: 535.0883\n",
      "Training Epoch: 44 [24000/36045]\tLoss: 512.2611\n",
      "Training Epoch: 44 [24050/36045]\tLoss: 472.7928\n",
      "Training Epoch: 44 [24100/36045]\tLoss: 497.2253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 44 [24150/36045]\tLoss: 489.3587\n",
      "Training Epoch: 44 [24200/36045]\tLoss: 487.0330\n",
      "Training Epoch: 44 [24250/36045]\tLoss: 473.0921\n",
      "Training Epoch: 44 [24300/36045]\tLoss: 511.0865\n",
      "Training Epoch: 44 [24350/36045]\tLoss: 523.1408\n",
      "Training Epoch: 44 [24400/36045]\tLoss: 537.9469\n",
      "Training Epoch: 44 [24450/36045]\tLoss: 512.6817\n",
      "Training Epoch: 44 [24500/36045]\tLoss: 541.2902\n",
      "Training Epoch: 44 [24550/36045]\tLoss: 628.0491\n",
      "Training Epoch: 44 [24600/36045]\tLoss: 619.2886\n",
      "Training Epoch: 44 [24650/36045]\tLoss: 592.5820\n",
      "Training Epoch: 44 [24700/36045]\tLoss: 602.4547\n",
      "Training Epoch: 44 [24750/36045]\tLoss: 556.9960\n",
      "Training Epoch: 44 [24800/36045]\tLoss: 456.2220\n",
      "Training Epoch: 44 [24850/36045]\tLoss: 474.4299\n",
      "Training Epoch: 44 [24900/36045]\tLoss: 471.8777\n",
      "Training Epoch: 44 [24950/36045]\tLoss: 474.3480\n",
      "Training Epoch: 44 [25000/36045]\tLoss: 456.1624\n",
      "Training Epoch: 44 [25050/36045]\tLoss: 436.1674\n",
      "Training Epoch: 44 [25100/36045]\tLoss: 390.7597\n",
      "Training Epoch: 44 [25150/36045]\tLoss: 361.7175\n",
      "Training Epoch: 44 [25200/36045]\tLoss: 356.6498\n",
      "Training Epoch: 44 [25250/36045]\tLoss: 382.4257\n",
      "Training Epoch: 44 [25300/36045]\tLoss: 502.5667\n",
      "Training Epoch: 44 [25350/36045]\tLoss: 498.8268\n",
      "Training Epoch: 44 [25400/36045]\tLoss: 466.2466\n",
      "Training Epoch: 44 [25450/36045]\tLoss: 468.2345\n",
      "Training Epoch: 44 [25500/36045]\tLoss: 508.6009\n",
      "Training Epoch: 44 [25550/36045]\tLoss: 595.0190\n",
      "Training Epoch: 44 [25600/36045]\tLoss: 599.2145\n",
      "Training Epoch: 44 [25650/36045]\tLoss: 578.4232\n",
      "Training Epoch: 44 [25700/36045]\tLoss: 587.9566\n",
      "Training Epoch: 44 [25750/36045]\tLoss: 568.1267\n",
      "Training Epoch: 44 [25800/36045]\tLoss: 358.4069\n",
      "Training Epoch: 44 [25850/36045]\tLoss: 366.2096\n",
      "Training Epoch: 44 [25900/36045]\tLoss: 347.9178\n",
      "Training Epoch: 44 [25950/36045]\tLoss: 357.5178\n",
      "Training Epoch: 44 [26000/36045]\tLoss: 439.8888\n",
      "Training Epoch: 44 [26050/36045]\tLoss: 598.4152\n",
      "Training Epoch: 44 [26100/36045]\tLoss: 624.1740\n",
      "Training Epoch: 44 [26150/36045]\tLoss: 623.5281\n",
      "Training Epoch: 44 [26200/36045]\tLoss: 597.1382\n",
      "Training Epoch: 44 [26250/36045]\tLoss: 627.2791\n",
      "Training Epoch: 44 [26300/36045]\tLoss: 576.3564\n",
      "Training Epoch: 44 [26350/36045]\tLoss: 587.0621\n",
      "Training Epoch: 44 [26400/36045]\tLoss: 562.2167\n",
      "Training Epoch: 44 [26450/36045]\tLoss: 492.4866\n",
      "Training Epoch: 44 [26500/36045]\tLoss: 583.2565\n",
      "Training Epoch: 44 [26550/36045]\tLoss: 582.9050\n",
      "Training Epoch: 44 [26600/36045]\tLoss: 579.2068\n",
      "Training Epoch: 44 [26650/36045]\tLoss: 593.9354\n",
      "Training Epoch: 44 [26700/36045]\tLoss: 573.0389\n",
      "Training Epoch: 44 [26750/36045]\tLoss: 537.5497\n",
      "Training Epoch: 44 [26800/36045]\tLoss: 397.3094\n",
      "Training Epoch: 44 [26850/36045]\tLoss: 328.9621\n",
      "Training Epoch: 44 [26900/36045]\tLoss: 330.7872\n",
      "Training Epoch: 44 [26950/36045]\tLoss: 362.9159\n",
      "Training Epoch: 44 [27000/36045]\tLoss: 598.3213\n",
      "Training Epoch: 44 [27050/36045]\tLoss: 624.5941\n",
      "Training Epoch: 44 [27100/36045]\tLoss: 605.5605\n",
      "Training Epoch: 44 [27150/36045]\tLoss: 644.3884\n",
      "Training Epoch: 44 [27200/36045]\tLoss: 467.1797\n",
      "Training Epoch: 44 [27250/36045]\tLoss: 457.6965\n",
      "Training Epoch: 44 [27300/36045]\tLoss: 446.3809\n",
      "Training Epoch: 44 [27350/36045]\tLoss: 443.7415\n",
      "Training Epoch: 44 [27400/36045]\tLoss: 443.1798\n",
      "Training Epoch: 44 [27450/36045]\tLoss: 561.0572\n",
      "Training Epoch: 44 [27500/36045]\tLoss: 601.2087\n",
      "Training Epoch: 44 [27550/36045]\tLoss: 594.5692\n",
      "Training Epoch: 44 [27600/36045]\tLoss: 606.0373\n",
      "Training Epoch: 44 [27650/36045]\tLoss: 598.0625\n",
      "Training Epoch: 44 [27700/36045]\tLoss: 625.2811\n",
      "Training Epoch: 44 [27750/36045]\tLoss: 637.1650\n",
      "Training Epoch: 44 [27800/36045]\tLoss: 624.5178\n",
      "Training Epoch: 44 [27850/36045]\tLoss: 614.9070\n",
      "Training Epoch: 44 [27900/36045]\tLoss: 559.2424\n",
      "Training Epoch: 44 [27950/36045]\tLoss: 467.9922\n",
      "Training Epoch: 44 [28000/36045]\tLoss: 444.8545\n",
      "Training Epoch: 44 [28050/36045]\tLoss: 454.2766\n",
      "Training Epoch: 44 [28100/36045]\tLoss: 445.9642\n",
      "Training Epoch: 44 [28150/36045]\tLoss: 464.9627\n",
      "Training Epoch: 44 [28200/36045]\tLoss: 473.4520\n",
      "Training Epoch: 44 [28250/36045]\tLoss: 466.2690\n",
      "Training Epoch: 44 [28300/36045]\tLoss: 442.6164\n",
      "Training Epoch: 44 [28350/36045]\tLoss: 438.3972\n",
      "Training Epoch: 44 [28400/36045]\tLoss: 765.7020\n",
      "Training Epoch: 44 [28450/36045]\tLoss: 703.0312\n",
      "Training Epoch: 44 [28500/36045]\tLoss: 607.4931\n",
      "Training Epoch: 44 [28550/36045]\tLoss: 558.5966\n",
      "Training Epoch: 44 [28600/36045]\tLoss: 582.9362\n",
      "Training Epoch: 44 [28650/36045]\tLoss: 636.3958\n",
      "Training Epoch: 44 [28700/36045]\tLoss: 630.2496\n",
      "Training Epoch: 44 [28750/36045]\tLoss: 617.2997\n",
      "Training Epoch: 44 [28800/36045]\tLoss: 627.2191\n",
      "Training Epoch: 44 [28850/36045]\tLoss: 544.7857\n",
      "Training Epoch: 44 [28900/36045]\tLoss: 443.4583\n",
      "Training Epoch: 44 [28950/36045]\tLoss: 442.6344\n",
      "Training Epoch: 44 [29000/36045]\tLoss: 438.6367\n",
      "Training Epoch: 44 [29050/36045]\tLoss: 445.6370\n",
      "Training Epoch: 44 [29100/36045]\tLoss: 463.5758\n",
      "Training Epoch: 44 [29150/36045]\tLoss: 454.0319\n",
      "Training Epoch: 44 [29200/36045]\tLoss: 439.8350\n",
      "Training Epoch: 44 [29250/36045]\tLoss: 429.7570\n",
      "Training Epoch: 44 [29300/36045]\tLoss: 483.1653\n",
      "Training Epoch: 44 [29350/36045]\tLoss: 567.1488\n",
      "Training Epoch: 44 [29400/36045]\tLoss: 584.6787\n",
      "Training Epoch: 44 [29450/36045]\tLoss: 601.2644\n",
      "Training Epoch: 44 [29500/36045]\tLoss: 616.1381\n",
      "Training Epoch: 44 [29550/36045]\tLoss: 586.0488\n",
      "Training Epoch: 44 [29600/36045]\tLoss: 492.8919\n",
      "Training Epoch: 44 [29650/36045]\tLoss: 474.4436\n",
      "Training Epoch: 44 [29700/36045]\tLoss: 426.1313\n",
      "Training Epoch: 44 [29750/36045]\tLoss: 424.5367\n",
      "Training Epoch: 44 [29800/36045]\tLoss: 471.6216\n",
      "Training Epoch: 44 [29850/36045]\tLoss: 548.9381\n",
      "Training Epoch: 44 [29900/36045]\tLoss: 544.4724\n",
      "Training Epoch: 44 [29950/36045]\tLoss: 566.2245\n",
      "Training Epoch: 44 [30000/36045]\tLoss: 540.1888\n",
      "Training Epoch: 44 [30050/36045]\tLoss: 547.4360\n",
      "Training Epoch: 44 [30100/36045]\tLoss: 667.7587\n",
      "Training Epoch: 44 [30150/36045]\tLoss: 650.0369\n",
      "Training Epoch: 44 [30200/36045]\tLoss: 612.9462\n",
      "Training Epoch: 44 [30250/36045]\tLoss: 661.3899\n",
      "Training Epoch: 44 [30300/36045]\tLoss: 645.6715\n",
      "Training Epoch: 44 [30350/36045]\tLoss: 492.5007\n",
      "Training Epoch: 44 [30400/36045]\tLoss: 477.1694\n",
      "Training Epoch: 44 [30450/36045]\tLoss: 479.5047\n",
      "Training Epoch: 44 [30500/36045]\tLoss: 447.4666\n",
      "Training Epoch: 44 [30550/36045]\tLoss: 415.2201\n",
      "Training Epoch: 44 [30600/36045]\tLoss: 407.9630\n",
      "Training Epoch: 44 [30650/36045]\tLoss: 398.1122\n",
      "Training Epoch: 44 [30700/36045]\tLoss: 415.7947\n",
      "Training Epoch: 44 [30750/36045]\tLoss: 402.7761\n",
      "Training Epoch: 44 [30800/36045]\tLoss: 428.8431\n",
      "Training Epoch: 44 [30850/36045]\tLoss: 420.6117\n",
      "Training Epoch: 44 [30900/36045]\tLoss: 432.4047\n",
      "Training Epoch: 44 [30950/36045]\tLoss: 454.4631\n",
      "Training Epoch: 44 [31000/36045]\tLoss: 446.8902\n",
      "Training Epoch: 44 [31050/36045]\tLoss: 373.0609\n",
      "Training Epoch: 44 [31100/36045]\tLoss: 363.7444\n",
      "Training Epoch: 44 [31150/36045]\tLoss: 371.4176\n",
      "Training Epoch: 44 [31200/36045]\tLoss: 461.1053\n",
      "Training Epoch: 44 [31250/36045]\tLoss: 598.3266\n",
      "Training Epoch: 44 [31300/36045]\tLoss: 569.5899\n",
      "Training Epoch: 44 [31350/36045]\tLoss: 585.5696\n",
      "Training Epoch: 44 [31400/36045]\tLoss: 565.1328\n",
      "Training Epoch: 44 [31450/36045]\tLoss: 581.8773\n",
      "Training Epoch: 44 [31500/36045]\tLoss: 594.4353\n",
      "Training Epoch: 44 [31550/36045]\tLoss: 601.1680\n",
      "Training Epoch: 44 [31600/36045]\tLoss: 565.1841\n",
      "Training Epoch: 44 [31650/36045]\tLoss: 605.1927\n",
      "Training Epoch: 44 [31700/36045]\tLoss: 437.5362\n",
      "Training Epoch: 44 [31750/36045]\tLoss: 361.4951\n",
      "Training Epoch: 44 [31800/36045]\tLoss: 345.0599\n",
      "Training Epoch: 44 [31850/36045]\tLoss: 352.7151\n",
      "Training Epoch: 44 [31900/36045]\tLoss: 558.5549\n",
      "Training Epoch: 44 [31950/36045]\tLoss: 724.1107\n",
      "Training Epoch: 44 [32000/36045]\tLoss: 831.5316\n",
      "Training Epoch: 44 [32050/36045]\tLoss: 787.3115\n",
      "Training Epoch: 44 [32100/36045]\tLoss: 778.5652\n",
      "Training Epoch: 44 [32150/36045]\tLoss: 596.7937\n",
      "Training Epoch: 44 [32200/36045]\tLoss: 598.7169\n",
      "Training Epoch: 44 [32250/36045]\tLoss: 608.9434\n",
      "Training Epoch: 44 [32300/36045]\tLoss: 591.0776\n",
      "Training Epoch: 44 [32350/36045]\tLoss: 587.1337\n",
      "Training Epoch: 44 [32400/36045]\tLoss: 550.8986\n",
      "Training Epoch: 44 [32450/36045]\tLoss: 453.5885\n",
      "Training Epoch: 44 [32500/36045]\tLoss: 435.7979\n",
      "Training Epoch: 44 [32550/36045]\tLoss: 437.9037\n",
      "Training Epoch: 44 [32600/36045]\tLoss: 435.0287\n",
      "Training Epoch: 44 [32650/36045]\tLoss: 564.2855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 44 [32700/36045]\tLoss: 616.4050\n",
      "Training Epoch: 44 [32750/36045]\tLoss: 586.9114\n",
      "Training Epoch: 44 [32800/36045]\tLoss: 601.7386\n",
      "Training Epoch: 44 [32850/36045]\tLoss: 555.4734\n",
      "Training Epoch: 44 [32900/36045]\tLoss: 443.7899\n",
      "Training Epoch: 44 [32950/36045]\tLoss: 464.9066\n",
      "Training Epoch: 44 [33000/36045]\tLoss: 463.5305\n",
      "Training Epoch: 44 [33050/36045]\tLoss: 441.0474\n",
      "Training Epoch: 44 [33100/36045]\tLoss: 501.1988\n",
      "Training Epoch: 44 [33150/36045]\tLoss: 682.3640\n",
      "Training Epoch: 44 [33200/36045]\tLoss: 664.3318\n",
      "Training Epoch: 44 [33250/36045]\tLoss: 684.5616\n",
      "Training Epoch: 44 [33300/36045]\tLoss: 729.6527\n",
      "Training Epoch: 44 [33350/36045]\tLoss: 558.5197\n",
      "Training Epoch: 44 [33400/36045]\tLoss: 406.4644\n",
      "Training Epoch: 44 [33450/36045]\tLoss: 402.4384\n",
      "Training Epoch: 44 [33500/36045]\tLoss: 414.2868\n",
      "Training Epoch: 44 [33550/36045]\tLoss: 428.9856\n",
      "Training Epoch: 44 [33600/36045]\tLoss: 430.7615\n",
      "Training Epoch: 44 [33650/36045]\tLoss: 575.5854\n",
      "Training Epoch: 44 [33700/36045]\tLoss: 556.7548\n",
      "Training Epoch: 44 [33750/36045]\tLoss: 576.6035\n",
      "Training Epoch: 44 [33800/36045]\tLoss: 573.9469\n",
      "Training Epoch: 44 [33850/36045]\tLoss: 575.7302\n",
      "Training Epoch: 44 [33900/36045]\tLoss: 589.2646\n",
      "Training Epoch: 44 [33950/36045]\tLoss: 598.8151\n",
      "Training Epoch: 44 [34000/36045]\tLoss: 585.4429\n",
      "Training Epoch: 44 [34050/36045]\tLoss: 589.2081\n",
      "Training Epoch: 44 [34100/36045]\tLoss: 568.1980\n",
      "Training Epoch: 44 [34150/36045]\tLoss: 526.2705\n",
      "Training Epoch: 44 [34200/36045]\tLoss: 498.4955\n",
      "Training Epoch: 44 [34250/36045]\tLoss: 512.3824\n",
      "Training Epoch: 44 [34300/36045]\tLoss: 437.2277\n",
      "Training Epoch: 44 [34350/36045]\tLoss: 460.5793\n",
      "Training Epoch: 44 [34400/36045]\tLoss: 453.6584\n",
      "Training Epoch: 44 [34450/36045]\tLoss: 427.2006\n",
      "Training Epoch: 44 [34500/36045]\tLoss: 455.6591\n",
      "Training Epoch: 44 [34550/36045]\tLoss: 447.2800\n",
      "Training Epoch: 44 [34600/36045]\tLoss: 454.2184\n",
      "Training Epoch: 44 [34650/36045]\tLoss: 559.9290\n",
      "Training Epoch: 44 [34700/36045]\tLoss: 593.9446\n",
      "Training Epoch: 44 [34750/36045]\tLoss: 526.3657\n",
      "Training Epoch: 44 [34800/36045]\tLoss: 604.5817\n",
      "Training Epoch: 44 [34850/36045]\tLoss: 611.6207\n",
      "Training Epoch: 44 [34900/36045]\tLoss: 659.1638\n",
      "Training Epoch: 44 [34950/36045]\tLoss: 643.8735\n",
      "Training Epoch: 44 [35000/36045]\tLoss: 645.0460\n",
      "Training Epoch: 44 [35050/36045]\tLoss: 632.3985\n",
      "Training Epoch: 44 [35100/36045]\tLoss: 546.3772\n",
      "Training Epoch: 44 [35150/36045]\tLoss: 538.4650\n",
      "Training Epoch: 44 [35200/36045]\tLoss: 452.5942\n",
      "Training Epoch: 44 [35250/36045]\tLoss: 497.4299\n",
      "Training Epoch: 44 [35300/36045]\tLoss: 513.6503\n",
      "Training Epoch: 44 [35350/36045]\tLoss: 574.0629\n",
      "Training Epoch: 44 [35400/36045]\tLoss: 603.7142\n",
      "Training Epoch: 44 [35450/36045]\tLoss: 575.0081\n",
      "Training Epoch: 44 [35500/36045]\tLoss: 556.1761\n",
      "Training Epoch: 44 [35550/36045]\tLoss: 542.6729\n",
      "Training Epoch: 44 [35600/36045]\tLoss: 593.4619\n",
      "Training Epoch: 44 [35650/36045]\tLoss: 665.8033\n",
      "Training Epoch: 44 [35700/36045]\tLoss: 592.5425\n",
      "Training Epoch: 44 [35750/36045]\tLoss: 649.7156\n",
      "Training Epoch: 44 [35800/36045]\tLoss: 656.4257\n",
      "Training Epoch: 44 [35850/36045]\tLoss: 630.6920\n",
      "Training Epoch: 44 [35900/36045]\tLoss: 652.7717\n",
      "Training Epoch: 44 [35950/36045]\tLoss: 649.4189\n",
      "Training Epoch: 44 [36000/36045]\tLoss: 642.4852\n",
      "Training Epoch: 44 [36045/36045]\tLoss: 627.9633\n",
      "Training Epoch: 44 [4004/4004]\tLoss: 583.0904\n",
      "Training Epoch: 45 [50/36045]\tLoss: 581.5745\n",
      "Training Epoch: 45 [100/36045]\tLoss: 557.3315\n",
      "Training Epoch: 45 [150/36045]\tLoss: 555.2429\n",
      "Training Epoch: 45 [200/36045]\tLoss: 541.4453\n",
      "Training Epoch: 45 [250/36045]\tLoss: 651.1168\n",
      "Training Epoch: 45 [300/36045]\tLoss: 716.2618\n",
      "Training Epoch: 45 [350/36045]\tLoss: 683.1665\n",
      "Training Epoch: 45 [400/36045]\tLoss: 677.5710\n",
      "Training Epoch: 45 [450/36045]\tLoss: 658.3346\n",
      "Training Epoch: 45 [500/36045]\tLoss: 609.1976\n",
      "Training Epoch: 45 [550/36045]\tLoss: 612.0594\n",
      "Training Epoch: 45 [600/36045]\tLoss: 598.0262\n",
      "Training Epoch: 45 [650/36045]\tLoss: 619.3188\n",
      "Training Epoch: 45 [700/36045]\tLoss: 604.6111\n",
      "Training Epoch: 45 [750/36045]\tLoss: 580.9454\n",
      "Training Epoch: 45 [800/36045]\tLoss: 592.5070\n",
      "Training Epoch: 45 [850/36045]\tLoss: 575.7078\n",
      "Training Epoch: 45 [900/36045]\tLoss: 551.4230\n",
      "Training Epoch: 45 [950/36045]\tLoss: 521.6016\n",
      "Training Epoch: 45 [1000/36045]\tLoss: 505.5576\n",
      "Training Epoch: 45 [1050/36045]\tLoss: 507.4854\n",
      "Training Epoch: 45 [1100/36045]\tLoss: 493.7596\n",
      "Training Epoch: 45 [1150/36045]\tLoss: 503.0935\n",
      "Training Epoch: 45 [1200/36045]\tLoss: 532.7585\n",
      "Training Epoch: 45 [1250/36045]\tLoss: 609.5804\n",
      "Training Epoch: 45 [1300/36045]\tLoss: 616.5229\n",
      "Training Epoch: 45 [1350/36045]\tLoss: 617.6982\n",
      "Training Epoch: 45 [1400/36045]\tLoss: 641.4230\n",
      "Training Epoch: 45 [1450/36045]\tLoss: 620.7061\n",
      "Training Epoch: 45 [1500/36045]\tLoss: 567.2195\n",
      "Training Epoch: 45 [1550/36045]\tLoss: 581.8453\n",
      "Training Epoch: 45 [1600/36045]\tLoss: 591.7945\n",
      "Training Epoch: 45 [1650/36045]\tLoss: 579.1300\n",
      "Training Epoch: 45 [1700/36045]\tLoss: 591.3387\n",
      "Training Epoch: 45 [1750/36045]\tLoss: 632.8356\n",
      "Training Epoch: 45 [1800/36045]\tLoss: 615.6158\n",
      "Training Epoch: 45 [1850/36045]\tLoss: 631.4217\n",
      "Training Epoch: 45 [1900/36045]\tLoss: 590.4448\n",
      "Training Epoch: 45 [1950/36045]\tLoss: 600.2433\n",
      "Training Epoch: 45 [2000/36045]\tLoss: 540.5356\n",
      "Training Epoch: 45 [2050/36045]\tLoss: 543.2108\n",
      "Training Epoch: 45 [2100/36045]\tLoss: 572.6155\n",
      "Training Epoch: 45 [2150/36045]\tLoss: 553.5889\n",
      "Training Epoch: 45 [2200/36045]\tLoss: 515.8766\n",
      "Training Epoch: 45 [2250/36045]\tLoss: 487.3214\n",
      "Training Epoch: 45 [2300/36045]\tLoss: 510.9601\n",
      "Training Epoch: 45 [2350/36045]\tLoss: 488.4965\n",
      "Training Epoch: 45 [2400/36045]\tLoss: 496.0514\n",
      "Training Epoch: 45 [2450/36045]\tLoss: 635.4325\n",
      "Training Epoch: 45 [2500/36045]\tLoss: 667.2610\n",
      "Training Epoch: 45 [2550/36045]\tLoss: 665.0985\n",
      "Training Epoch: 45 [2600/36045]\tLoss: 673.7279\n",
      "Training Epoch: 45 [2650/36045]\tLoss: 797.9235\n",
      "Training Epoch: 45 [2700/36045]\tLoss: 885.7740\n",
      "Training Epoch: 45 [2750/36045]\tLoss: 956.6406\n",
      "Training Epoch: 45 [2800/36045]\tLoss: 966.0280\n",
      "Training Epoch: 45 [2850/36045]\tLoss: 733.2518\n",
      "Training Epoch: 45 [2900/36045]\tLoss: 694.2271\n",
      "Training Epoch: 45 [2950/36045]\tLoss: 671.5514\n",
      "Training Epoch: 45 [3000/36045]\tLoss: 665.4415\n",
      "Training Epoch: 45 [3050/36045]\tLoss: 696.2311\n",
      "Training Epoch: 45 [3100/36045]\tLoss: 636.2239\n",
      "Training Epoch: 45 [3150/36045]\tLoss: 489.2467\n",
      "Training Epoch: 45 [3200/36045]\tLoss: 506.5237\n",
      "Training Epoch: 45 [3250/36045]\tLoss: 477.9243\n",
      "Training Epoch: 45 [3300/36045]\tLoss: 452.6100\n",
      "Training Epoch: 45 [3350/36045]\tLoss: 478.1553\n",
      "Training Epoch: 45 [3400/36045]\tLoss: 500.9818\n",
      "Training Epoch: 45 [3450/36045]\tLoss: 537.3423\n",
      "Training Epoch: 45 [3500/36045]\tLoss: 524.6258\n",
      "Training Epoch: 45 [3550/36045]\tLoss: 501.7005\n",
      "Training Epoch: 45 [3600/36045]\tLoss: 539.1525\n",
      "Training Epoch: 45 [3650/36045]\tLoss: 623.6462\n",
      "Training Epoch: 45 [3700/36045]\tLoss: 631.3860\n",
      "Training Epoch: 45 [3750/36045]\tLoss: 600.5704\n",
      "Training Epoch: 45 [3800/36045]\tLoss: 597.4873\n",
      "Training Epoch: 45 [3850/36045]\tLoss: 599.1464\n",
      "Training Epoch: 45 [3900/36045]\tLoss: 603.8251\n",
      "Training Epoch: 45 [3950/36045]\tLoss: 582.8271\n",
      "Training Epoch: 45 [4000/36045]\tLoss: 587.4647\n",
      "Training Epoch: 45 [4050/36045]\tLoss: 538.6021\n",
      "Training Epoch: 45 [4100/36045]\tLoss: 525.2500\n",
      "Training Epoch: 45 [4150/36045]\tLoss: 539.3372\n",
      "Training Epoch: 45 [4200/36045]\tLoss: 534.3643\n",
      "Training Epoch: 45 [4250/36045]\tLoss: 536.6400\n",
      "Training Epoch: 45 [4300/36045]\tLoss: 552.0000\n",
      "Training Epoch: 45 [4350/36045]\tLoss: 535.7304\n",
      "Training Epoch: 45 [4400/36045]\tLoss: 512.3347\n",
      "Training Epoch: 45 [4450/36045]\tLoss: 562.8612\n",
      "Training Epoch: 45 [4500/36045]\tLoss: 605.8289\n",
      "Training Epoch: 45 [4550/36045]\tLoss: 608.6730\n",
      "Training Epoch: 45 [4600/36045]\tLoss: 630.5122\n",
      "Training Epoch: 45 [4650/36045]\tLoss: 620.4101\n",
      "Training Epoch: 45 [4700/36045]\tLoss: 572.0265\n",
      "Training Epoch: 45 [4750/36045]\tLoss: 554.1066\n",
      "Training Epoch: 45 [4800/36045]\tLoss: 578.3472\n",
      "Training Epoch: 45 [4850/36045]\tLoss: 564.8507\n",
      "Training Epoch: 45 [4900/36045]\tLoss: 549.8300\n",
      "Training Epoch: 45 [4950/36045]\tLoss: 565.2130\n",
      "Training Epoch: 45 [5000/36045]\tLoss: 594.6810\n",
      "Training Epoch: 45 [5050/36045]\tLoss: 576.1766\n",
      "Training Epoch: 45 [5100/36045]\tLoss: 586.2965\n",
      "Training Epoch: 45 [5150/36045]\tLoss: 570.6606\n",
      "Training Epoch: 45 [5200/36045]\tLoss: 568.5878\n",
      "Training Epoch: 45 [5250/36045]\tLoss: 562.2729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 45 [5300/36045]\tLoss: 562.3940\n",
      "Training Epoch: 45 [5350/36045]\tLoss: 583.5397\n",
      "Training Epoch: 45 [5400/36045]\tLoss: 562.4176\n",
      "Training Epoch: 45 [5450/36045]\tLoss: 532.8857\n",
      "Training Epoch: 45 [5500/36045]\tLoss: 561.0300\n",
      "Training Epoch: 45 [5550/36045]\tLoss: 549.2965\n",
      "Training Epoch: 45 [5600/36045]\tLoss: 628.0331\n",
      "Training Epoch: 45 [5650/36045]\tLoss: 593.7541\n",
      "Training Epoch: 45 [5700/36045]\tLoss: 556.9293\n",
      "Training Epoch: 45 [5750/36045]\tLoss: 541.0177\n",
      "Training Epoch: 45 [5800/36045]\tLoss: 570.8158\n",
      "Training Epoch: 45 [5850/36045]\tLoss: 560.3938\n",
      "Training Epoch: 45 [5900/36045]\tLoss: 643.8260\n",
      "Training Epoch: 45 [5950/36045]\tLoss: 660.2747\n",
      "Training Epoch: 45 [6000/36045]\tLoss: 646.2919\n",
      "Training Epoch: 45 [6050/36045]\tLoss: 624.4731\n",
      "Training Epoch: 45 [6100/36045]\tLoss: 628.7230\n",
      "Training Epoch: 45 [6150/36045]\tLoss: 619.4141\n",
      "Training Epoch: 45 [6200/36045]\tLoss: 624.0164\n",
      "Training Epoch: 45 [6250/36045]\tLoss: 645.5363\n",
      "Training Epoch: 45 [6300/36045]\tLoss: 657.1039\n",
      "Training Epoch: 45 [6350/36045]\tLoss: 702.3564\n",
      "Training Epoch: 45 [6400/36045]\tLoss: 578.6282\n",
      "Training Epoch: 45 [6450/36045]\tLoss: 532.0175\n",
      "Training Epoch: 45 [6500/36045]\tLoss: 541.7128\n",
      "Training Epoch: 45 [6550/36045]\tLoss: 559.1430\n",
      "Training Epoch: 45 [6600/36045]\tLoss: 557.1277\n",
      "Training Epoch: 45 [6650/36045]\tLoss: 629.1146\n",
      "Training Epoch: 45 [6700/36045]\tLoss: 657.9055\n",
      "Training Epoch: 45 [6750/36045]\tLoss: 635.3652\n",
      "Training Epoch: 45 [6800/36045]\tLoss: 638.3444\n",
      "Training Epoch: 45 [6850/36045]\tLoss: 626.4454\n",
      "Training Epoch: 45 [6900/36045]\tLoss: 558.0675\n",
      "Training Epoch: 45 [6950/36045]\tLoss: 526.1763\n",
      "Training Epoch: 45 [7000/36045]\tLoss: 559.5947\n",
      "Training Epoch: 45 [7050/36045]\tLoss: 571.8118\n",
      "Training Epoch: 45 [7100/36045]\tLoss: 571.0145\n",
      "Training Epoch: 45 [7150/36045]\tLoss: 581.0540\n",
      "Training Epoch: 45 [7200/36045]\tLoss: 583.2289\n",
      "Training Epoch: 45 [7250/36045]\tLoss: 581.5176\n",
      "Training Epoch: 45 [7300/36045]\tLoss: 567.8799\n",
      "Training Epoch: 45 [7350/36045]\tLoss: 565.6160\n",
      "Training Epoch: 45 [7400/36045]\tLoss: 515.7236\n",
      "Training Epoch: 45 [7450/36045]\tLoss: 518.9430\n",
      "Training Epoch: 45 [7500/36045]\tLoss: 514.5335\n",
      "Training Epoch: 45 [7550/36045]\tLoss: 492.8117\n",
      "Training Epoch: 45 [7600/36045]\tLoss: 546.1216\n",
      "Training Epoch: 45 [7650/36045]\tLoss: 583.7020\n",
      "Training Epoch: 45 [7700/36045]\tLoss: 555.4755\n",
      "Training Epoch: 45 [7750/36045]\tLoss: 569.8679\n",
      "Training Epoch: 45 [7800/36045]\tLoss: 559.4638\n",
      "Training Epoch: 45 [7850/36045]\tLoss: 541.4791\n",
      "Training Epoch: 45 [7900/36045]\tLoss: 571.0393\n",
      "Training Epoch: 45 [7950/36045]\tLoss: 568.5980\n",
      "Training Epoch: 45 [8000/36045]\tLoss: 586.7270\n",
      "Training Epoch: 45 [8050/36045]\tLoss: 552.4582\n",
      "Training Epoch: 45 [8100/36045]\tLoss: 577.5082\n",
      "Training Epoch: 45 [8150/36045]\tLoss: 655.0903\n",
      "Training Epoch: 45 [8200/36045]\tLoss: 642.5681\n",
      "Training Epoch: 45 [8250/36045]\tLoss: 610.9321\n",
      "Training Epoch: 45 [8300/36045]\tLoss: 667.7606\n",
      "Training Epoch: 45 [8350/36045]\tLoss: 612.0717\n",
      "Training Epoch: 45 [8400/36045]\tLoss: 547.4433\n",
      "Training Epoch: 45 [8450/36045]\tLoss: 512.3163\n",
      "Training Epoch: 45 [8500/36045]\tLoss: 545.2172\n",
      "Training Epoch: 45 [8550/36045]\tLoss: 538.7038\n",
      "Training Epoch: 45 [8600/36045]\tLoss: 533.0414\n",
      "Training Epoch: 45 [8650/36045]\tLoss: 565.6981\n",
      "Training Epoch: 45 [8700/36045]\tLoss: 598.3418\n",
      "Training Epoch: 45 [8750/36045]\tLoss: 587.8416\n",
      "Training Epoch: 45 [8800/36045]\tLoss: 593.5789\n",
      "Training Epoch: 45 [8850/36045]\tLoss: 587.1837\n",
      "Training Epoch: 45 [8900/36045]\tLoss: 530.1149\n",
      "Training Epoch: 45 [8950/36045]\tLoss: 540.8110\n",
      "Training Epoch: 45 [9000/36045]\tLoss: 556.6838\n",
      "Training Epoch: 45 [9050/36045]\tLoss: 558.2834\n",
      "Training Epoch: 45 [9100/36045]\tLoss: 574.9781\n",
      "Training Epoch: 45 [9150/36045]\tLoss: 425.5779\n",
      "Training Epoch: 45 [9200/36045]\tLoss: 317.9284\n",
      "Training Epoch: 45 [9250/36045]\tLoss: 345.0340\n",
      "Training Epoch: 45 [9300/36045]\tLoss: 354.6947\n",
      "Training Epoch: 45 [9350/36045]\tLoss: 327.3863\n",
      "Training Epoch: 45 [9400/36045]\tLoss: 641.1700\n",
      "Training Epoch: 45 [9450/36045]\tLoss: 680.9062\n",
      "Training Epoch: 45 [9500/36045]\tLoss: 668.5185\n",
      "Training Epoch: 45 [9550/36045]\tLoss: 707.2424\n",
      "Training Epoch: 45 [9600/36045]\tLoss: 527.0704\n",
      "Training Epoch: 45 [9650/36045]\tLoss: 532.1738\n",
      "Training Epoch: 45 [9700/36045]\tLoss: 517.7238\n",
      "Training Epoch: 45 [9750/36045]\tLoss: 516.3329\n",
      "Training Epoch: 45 [9800/36045]\tLoss: 675.3978\n",
      "Training Epoch: 45 [9850/36045]\tLoss: 713.3123\n",
      "Training Epoch: 45 [9900/36045]\tLoss: 722.6276\n",
      "Training Epoch: 45 [9950/36045]\tLoss: 704.1583\n",
      "Training Epoch: 45 [10000/36045]\tLoss: 651.6990\n",
      "Training Epoch: 45 [10050/36045]\tLoss: 533.5401\n",
      "Training Epoch: 45 [10100/36045]\tLoss: 541.2197\n",
      "Training Epoch: 45 [10150/36045]\tLoss: 549.4203\n",
      "Training Epoch: 45 [10200/36045]\tLoss: 538.3718\n",
      "Training Epoch: 45 [10250/36045]\tLoss: 647.2051\n",
      "Training Epoch: 45 [10300/36045]\tLoss: 629.0352\n",
      "Training Epoch: 45 [10350/36045]\tLoss: 662.6322\n",
      "Training Epoch: 45 [10400/36045]\tLoss: 652.7139\n",
      "Training Epoch: 45 [10450/36045]\tLoss: 611.5620\n",
      "Training Epoch: 45 [10500/36045]\tLoss: 510.7099\n",
      "Training Epoch: 45 [10550/36045]\tLoss: 505.3408\n",
      "Training Epoch: 45 [10600/36045]\tLoss: 527.2679\n",
      "Training Epoch: 45 [10650/36045]\tLoss: 533.1622\n",
      "Training Epoch: 45 [10700/36045]\tLoss: 612.8422\n",
      "Training Epoch: 45 [10750/36045]\tLoss: 671.8370\n",
      "Training Epoch: 45 [10800/36045]\tLoss: 617.7156\n",
      "Training Epoch: 45 [10850/36045]\tLoss: 655.1334\n",
      "Training Epoch: 45 [10900/36045]\tLoss: 682.0258\n",
      "Training Epoch: 45 [10950/36045]\tLoss: 501.6664\n",
      "Training Epoch: 45 [11000/36045]\tLoss: 495.3988\n",
      "Training Epoch: 45 [11050/36045]\tLoss: 531.3588\n",
      "Training Epoch: 45 [11100/36045]\tLoss: 541.4941\n",
      "Training Epoch: 45 [11150/36045]\tLoss: 587.4818\n",
      "Training Epoch: 45 [11200/36045]\tLoss: 615.8380\n",
      "Training Epoch: 45 [11250/36045]\tLoss: 627.1846\n",
      "Training Epoch: 45 [11300/36045]\tLoss: 607.5317\n",
      "Training Epoch: 45 [11350/36045]\tLoss: 605.2565\n",
      "Training Epoch: 45 [11400/36045]\tLoss: 568.8183\n",
      "Training Epoch: 45 [11450/36045]\tLoss: 538.0680\n",
      "Training Epoch: 45 [11500/36045]\tLoss: 535.4706\n",
      "Training Epoch: 45 [11550/36045]\tLoss: 545.0267\n",
      "Training Epoch: 45 [11600/36045]\tLoss: 605.0397\n",
      "Training Epoch: 45 [11650/36045]\tLoss: 656.4395\n",
      "Training Epoch: 45 [11700/36045]\tLoss: 655.3663\n",
      "Training Epoch: 45 [11750/36045]\tLoss: 673.3165\n",
      "Training Epoch: 45 [11800/36045]\tLoss: 715.7835\n",
      "Training Epoch: 45 [11850/36045]\tLoss: 774.2457\n",
      "Training Epoch: 45 [11900/36045]\tLoss: 986.2450\n",
      "Training Epoch: 45 [11950/36045]\tLoss: 989.3864\n",
      "Training Epoch: 45 [12000/36045]\tLoss: 1000.6750\n",
      "Training Epoch: 45 [12050/36045]\tLoss: 960.2757\n",
      "Training Epoch: 45 [12100/36045]\tLoss: 614.1461\n",
      "Training Epoch: 45 [12150/36045]\tLoss: 461.9705\n",
      "Training Epoch: 45 [12200/36045]\tLoss: 456.8105\n",
      "Training Epoch: 45 [12250/36045]\tLoss: 465.4136\n",
      "Training Epoch: 45 [12300/36045]\tLoss: 600.4343\n",
      "Training Epoch: 45 [12350/36045]\tLoss: 655.6105\n",
      "Training Epoch: 45 [12400/36045]\tLoss: 662.8165\n",
      "Training Epoch: 45 [12450/36045]\tLoss: 651.7244\n",
      "Training Epoch: 45 [12500/36045]\tLoss: 678.6326\n",
      "Training Epoch: 45 [12550/36045]\tLoss: 648.5955\n",
      "Training Epoch: 45 [12600/36045]\tLoss: 593.0482\n",
      "Training Epoch: 45 [12650/36045]\tLoss: 591.2562\n",
      "Training Epoch: 45 [12700/36045]\tLoss: 612.5059\n",
      "Training Epoch: 45 [12750/36045]\tLoss: 611.0605\n",
      "Training Epoch: 45 [12800/36045]\tLoss: 597.2360\n",
      "Training Epoch: 45 [12850/36045]\tLoss: 626.4267\n",
      "Training Epoch: 45 [12900/36045]\tLoss: 600.4904\n",
      "Training Epoch: 45 [12950/36045]\tLoss: 586.2334\n",
      "Training Epoch: 45 [13000/36045]\tLoss: 619.4908\n",
      "Training Epoch: 45 [13050/36045]\tLoss: 560.1291\n",
      "Training Epoch: 45 [13100/36045]\tLoss: 575.4824\n",
      "Training Epoch: 45 [13150/36045]\tLoss: 567.0964\n",
      "Training Epoch: 45 [13200/36045]\tLoss: 550.2044\n",
      "Training Epoch: 45 [13250/36045]\tLoss: 571.9085\n",
      "Training Epoch: 45 [13300/36045]\tLoss: 608.7078\n",
      "Training Epoch: 45 [13350/36045]\tLoss: 589.7363\n",
      "Training Epoch: 45 [13400/36045]\tLoss: 592.8246\n",
      "Training Epoch: 45 [13450/36045]\tLoss: 590.2865\n",
      "Training Epoch: 45 [13500/36045]\tLoss: 608.8851\n",
      "Training Epoch: 45 [13550/36045]\tLoss: 746.8298\n",
      "Training Epoch: 45 [13600/36045]\tLoss: 779.4457\n",
      "Training Epoch: 45 [13650/36045]\tLoss: 861.1746\n",
      "Training Epoch: 45 [13700/36045]\tLoss: 758.4078\n",
      "Training Epoch: 45 [13750/36045]\tLoss: 596.3008\n",
      "Training Epoch: 45 [13800/36045]\tLoss: 567.1367\n",
      "Training Epoch: 45 [13850/36045]\tLoss: 549.8347\n",
      "Training Epoch: 45 [13900/36045]\tLoss: 557.0911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 45 [13950/36045]\tLoss: 602.2557\n",
      "Training Epoch: 45 [14000/36045]\tLoss: 634.9532\n",
      "Training Epoch: 45 [14050/36045]\tLoss: 610.0399\n",
      "Training Epoch: 45 [14100/36045]\tLoss: 605.0654\n",
      "Training Epoch: 45 [14150/36045]\tLoss: 593.1506\n",
      "Training Epoch: 45 [14200/36045]\tLoss: 633.6220\n",
      "Training Epoch: 45 [14250/36045]\tLoss: 696.4220\n",
      "Training Epoch: 45 [14300/36045]\tLoss: 699.6228\n",
      "Training Epoch: 45 [14350/36045]\tLoss: 668.5413\n",
      "Training Epoch: 45 [14400/36045]\tLoss: 654.6030\n",
      "Training Epoch: 45 [14450/36045]\tLoss: 689.9966\n",
      "Training Epoch: 45 [14500/36045]\tLoss: 621.6199\n",
      "Training Epoch: 45 [14550/36045]\tLoss: 649.5652\n",
      "Training Epoch: 45 [14600/36045]\tLoss: 636.0873\n",
      "Training Epoch: 45 [14650/36045]\tLoss: 635.6868\n",
      "Training Epoch: 45 [14700/36045]\tLoss: 602.8223\n",
      "Training Epoch: 45 [14750/36045]\tLoss: 518.7308\n",
      "Training Epoch: 45 [14800/36045]\tLoss: 508.8001\n",
      "Training Epoch: 45 [14850/36045]\tLoss: 515.7759\n",
      "Training Epoch: 45 [14900/36045]\tLoss: 509.2552\n",
      "Training Epoch: 45 [14950/36045]\tLoss: 517.4037\n",
      "Training Epoch: 45 [15000/36045]\tLoss: 529.9892\n",
      "Training Epoch: 45 [15050/36045]\tLoss: 526.6128\n",
      "Training Epoch: 45 [15100/36045]\tLoss: 510.5891\n",
      "Training Epoch: 45 [15150/36045]\tLoss: 506.2985\n",
      "Training Epoch: 45 [15200/36045]\tLoss: 469.1195\n",
      "Training Epoch: 45 [15250/36045]\tLoss: 490.7276\n",
      "Training Epoch: 45 [15300/36045]\tLoss: 476.4718\n",
      "Training Epoch: 45 [15350/36045]\tLoss: 487.9939\n",
      "Training Epoch: 45 [15400/36045]\tLoss: 470.3844\n",
      "Training Epoch: 45 [15450/36045]\tLoss: 455.5839\n",
      "Training Epoch: 45 [15500/36045]\tLoss: 468.5966\n",
      "Training Epoch: 45 [15550/36045]\tLoss: 465.3576\n",
      "Training Epoch: 45 [15600/36045]\tLoss: 532.2284\n",
      "Training Epoch: 45 [15650/36045]\tLoss: 548.8079\n",
      "Training Epoch: 45 [15700/36045]\tLoss: 541.0852\n",
      "Training Epoch: 45 [15750/36045]\tLoss: 532.4785\n",
      "Training Epoch: 45 [15800/36045]\tLoss: 510.5439\n",
      "Training Epoch: 45 [15850/36045]\tLoss: 525.8843\n",
      "Training Epoch: 45 [15900/36045]\tLoss: 534.7574\n",
      "Training Epoch: 45 [15950/36045]\tLoss: 554.5151\n",
      "Training Epoch: 45 [16000/36045]\tLoss: 525.3234\n",
      "Training Epoch: 45 [16050/36045]\tLoss: 494.8717\n",
      "Training Epoch: 45 [16100/36045]\tLoss: 459.3100\n",
      "Training Epoch: 45 [16150/36045]\tLoss: 447.4001\n",
      "Training Epoch: 45 [16200/36045]\tLoss: 543.5616\n",
      "Training Epoch: 45 [16250/36045]\tLoss: 570.9425\n",
      "Training Epoch: 45 [16300/36045]\tLoss: 623.2288\n",
      "Training Epoch: 45 [16350/36045]\tLoss: 643.5312\n",
      "Training Epoch: 45 [16400/36045]\tLoss: 615.3737\n",
      "Training Epoch: 45 [16450/36045]\tLoss: 597.7976\n",
      "Training Epoch: 45 [16500/36045]\tLoss: 597.6663\n",
      "Training Epoch: 45 [16550/36045]\tLoss: 563.1378\n",
      "Training Epoch: 45 [16600/36045]\tLoss: 584.8868\n",
      "Training Epoch: 45 [16650/36045]\tLoss: 600.8151\n",
      "Training Epoch: 45 [16700/36045]\tLoss: 581.2213\n",
      "Training Epoch: 45 [16750/36045]\tLoss: 573.9635\n",
      "Training Epoch: 45 [16800/36045]\tLoss: 582.5084\n",
      "Training Epoch: 45 [16850/36045]\tLoss: 555.1858\n",
      "Training Epoch: 45 [16900/36045]\tLoss: 564.9330\n",
      "Training Epoch: 45 [16950/36045]\tLoss: 587.6166\n",
      "Training Epoch: 45 [17000/36045]\tLoss: 572.1428\n",
      "Training Epoch: 45 [17050/36045]\tLoss: 595.7566\n",
      "Training Epoch: 45 [17100/36045]\tLoss: 591.6191\n",
      "Training Epoch: 45 [17150/36045]\tLoss: 513.4464\n",
      "Training Epoch: 45 [17200/36045]\tLoss: 475.9097\n",
      "Training Epoch: 45 [17250/36045]\tLoss: 498.4990\n",
      "Training Epoch: 45 [17300/36045]\tLoss: 527.6544\n",
      "Training Epoch: 45 [17350/36045]\tLoss: 509.0272\n",
      "Training Epoch: 45 [17400/36045]\tLoss: 528.5804\n",
      "Training Epoch: 45 [17450/36045]\tLoss: 546.8541\n",
      "Training Epoch: 45 [17500/36045]\tLoss: 535.3223\n",
      "Training Epoch: 45 [17550/36045]\tLoss: 533.8549\n",
      "Training Epoch: 45 [17600/36045]\tLoss: 528.3479\n",
      "Training Epoch: 45 [17650/36045]\tLoss: 543.6301\n",
      "Training Epoch: 45 [17700/36045]\tLoss: 523.0960\n",
      "Training Epoch: 45 [17750/36045]\tLoss: 538.8739\n",
      "Training Epoch: 45 [17800/36045]\tLoss: 530.1859\n",
      "Training Epoch: 45 [17850/36045]\tLoss: 546.7767\n",
      "Training Epoch: 45 [17900/36045]\tLoss: 574.6164\n",
      "Training Epoch: 45 [17950/36045]\tLoss: 586.8796\n",
      "Training Epoch: 45 [18000/36045]\tLoss: 577.6815\n",
      "Training Epoch: 45 [18050/36045]\tLoss: 633.7436\n",
      "Training Epoch: 45 [18100/36045]\tLoss: 634.7368\n",
      "Training Epoch: 45 [18150/36045]\tLoss: 645.9602\n",
      "Training Epoch: 45 [18200/36045]\tLoss: 628.3888\n",
      "Training Epoch: 45 [18250/36045]\tLoss: 649.0220\n",
      "Training Epoch: 45 [18300/36045]\tLoss: 605.4474\n",
      "Training Epoch: 45 [18350/36045]\tLoss: 678.6501\n",
      "Training Epoch: 45 [18400/36045]\tLoss: 651.4321\n",
      "Training Epoch: 45 [18450/36045]\tLoss: 631.2256\n",
      "Training Epoch: 45 [18500/36045]\tLoss: 630.0386\n",
      "Training Epoch: 45 [18550/36045]\tLoss: 617.6859\n",
      "Training Epoch: 45 [18600/36045]\tLoss: 607.4103\n",
      "Training Epoch: 45 [18650/36045]\tLoss: 652.7072\n",
      "Training Epoch: 45 [18700/36045]\tLoss: 686.6420\n",
      "Training Epoch: 45 [18750/36045]\tLoss: 674.2908\n",
      "Training Epoch: 45 [18800/36045]\tLoss: 696.9251\n",
      "Training Epoch: 45 [18850/36045]\tLoss: 642.1345\n",
      "Training Epoch: 45 [18900/36045]\tLoss: 686.7809\n",
      "Training Epoch: 45 [18950/36045]\tLoss: 629.4758\n",
      "Training Epoch: 45 [19000/36045]\tLoss: 516.3622\n",
      "Training Epoch: 45 [19050/36045]\tLoss: 501.1670\n",
      "Training Epoch: 45 [19100/36045]\tLoss: 509.2086\n",
      "Training Epoch: 45 [19150/36045]\tLoss: 499.4439\n",
      "Training Epoch: 45 [19200/36045]\tLoss: 530.5773\n",
      "Training Epoch: 45 [19250/36045]\tLoss: 545.7925\n",
      "Training Epoch: 45 [19300/36045]\tLoss: 555.0767\n",
      "Training Epoch: 45 [19350/36045]\tLoss: 538.9493\n",
      "Training Epoch: 45 [19400/36045]\tLoss: 558.9746\n",
      "Training Epoch: 45 [19450/36045]\tLoss: 550.4350\n",
      "Training Epoch: 45 [19500/36045]\tLoss: 551.5787\n",
      "Training Epoch: 45 [19550/36045]\tLoss: 549.7633\n",
      "Training Epoch: 45 [19600/36045]\tLoss: 590.5499\n",
      "Training Epoch: 45 [19650/36045]\tLoss: 790.4389\n",
      "Training Epoch: 45 [19700/36045]\tLoss: 749.1614\n",
      "Training Epoch: 45 [19750/36045]\tLoss: 753.2743\n",
      "Training Epoch: 45 [19800/36045]\tLoss: 753.7710\n",
      "Training Epoch: 45 [19850/36045]\tLoss: 492.0380\n",
      "Training Epoch: 45 [19900/36045]\tLoss: 471.3582\n",
      "Training Epoch: 45 [19950/36045]\tLoss: 475.2714\n",
      "Training Epoch: 45 [20000/36045]\tLoss: 475.2644\n",
      "Training Epoch: 45 [20050/36045]\tLoss: 531.7833\n",
      "Training Epoch: 45 [20100/36045]\tLoss: 537.1241\n",
      "Training Epoch: 45 [20150/36045]\tLoss: 538.1342\n",
      "Training Epoch: 45 [20200/36045]\tLoss: 537.9045\n",
      "Training Epoch: 45 [20250/36045]\tLoss: 573.9823\n",
      "Training Epoch: 45 [20300/36045]\tLoss: 610.0947\n",
      "Training Epoch: 45 [20350/36045]\tLoss: 628.1397\n",
      "Training Epoch: 45 [20400/36045]\tLoss: 644.4063\n",
      "Training Epoch: 45 [20450/36045]\tLoss: 614.2092\n",
      "Training Epoch: 45 [20500/36045]\tLoss: 599.1169\n",
      "Training Epoch: 45 [20550/36045]\tLoss: 526.2061\n",
      "Training Epoch: 45 [20600/36045]\tLoss: 535.9047\n",
      "Training Epoch: 45 [20650/36045]\tLoss: 533.2254\n",
      "Training Epoch: 45 [20700/36045]\tLoss: 521.3044\n",
      "Training Epoch: 45 [20750/36045]\tLoss: 562.3015\n",
      "Training Epoch: 45 [20800/36045]\tLoss: 610.9514\n",
      "Training Epoch: 45 [20850/36045]\tLoss: 597.8502\n",
      "Training Epoch: 45 [20900/36045]\tLoss: 640.6567\n",
      "Training Epoch: 45 [20950/36045]\tLoss: 603.5832\n",
      "Training Epoch: 45 [21000/36045]\tLoss: 568.0577\n",
      "Training Epoch: 45 [21050/36045]\tLoss: 485.8250\n",
      "Training Epoch: 45 [21100/36045]\tLoss: 490.9301\n",
      "Training Epoch: 45 [21150/36045]\tLoss: 525.4606\n",
      "Training Epoch: 45 [21200/36045]\tLoss: 524.5242\n",
      "Training Epoch: 45 [21250/36045]\tLoss: 501.5941\n",
      "Training Epoch: 45 [21300/36045]\tLoss: 585.4872\n",
      "Training Epoch: 45 [21350/36045]\tLoss: 577.2795\n",
      "Training Epoch: 45 [21400/36045]\tLoss: 581.0736\n",
      "Training Epoch: 45 [21450/36045]\tLoss: 587.2896\n",
      "Training Epoch: 45 [21500/36045]\tLoss: 588.8422\n",
      "Training Epoch: 45 [21550/36045]\tLoss: 682.9397\n",
      "Training Epoch: 45 [21600/36045]\tLoss: 681.3093\n",
      "Training Epoch: 45 [21650/36045]\tLoss: 693.6351\n",
      "Training Epoch: 45 [21700/36045]\tLoss: 697.0677\n",
      "Training Epoch: 45 [21750/36045]\tLoss: 669.1663\n",
      "Training Epoch: 45 [21800/36045]\tLoss: 491.1902\n",
      "Training Epoch: 45 [21850/36045]\tLoss: 474.1392\n",
      "Training Epoch: 45 [21900/36045]\tLoss: 483.3274\n",
      "Training Epoch: 45 [21950/36045]\tLoss: 484.8594\n",
      "Training Epoch: 45 [22000/36045]\tLoss: 488.1205\n",
      "Training Epoch: 45 [22050/36045]\tLoss: 507.6020\n",
      "Training Epoch: 45 [22100/36045]\tLoss: 500.0309\n",
      "Training Epoch: 45 [22150/36045]\tLoss: 486.5122\n",
      "Training Epoch: 45 [22200/36045]\tLoss: 502.1408\n",
      "Training Epoch: 45 [22250/36045]\tLoss: 507.1541\n",
      "Training Epoch: 45 [22300/36045]\tLoss: 558.9436\n",
      "Training Epoch: 45 [22350/36045]\tLoss: 584.4078\n",
      "Training Epoch: 45 [22400/36045]\tLoss: 598.1017\n",
      "Training Epoch: 45 [22450/36045]\tLoss: 585.4256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 45 [22500/36045]\tLoss: 568.6514\n",
      "Training Epoch: 45 [22550/36045]\tLoss: 603.4620\n",
      "Training Epoch: 45 [22600/36045]\tLoss: 651.7697\n",
      "Training Epoch: 45 [22650/36045]\tLoss: 684.5501\n",
      "Training Epoch: 45 [22700/36045]\tLoss: 705.7785\n",
      "Training Epoch: 45 [22750/36045]\tLoss: 725.3271\n",
      "Training Epoch: 45 [22800/36045]\tLoss: 753.7682\n",
      "Training Epoch: 45 [22850/36045]\tLoss: 625.8445\n",
      "Training Epoch: 45 [22900/36045]\tLoss: 630.5594\n",
      "Training Epoch: 45 [22950/36045]\tLoss: 610.1473\n",
      "Training Epoch: 45 [23000/36045]\tLoss: 606.5949\n",
      "Training Epoch: 45 [23050/36045]\tLoss: 538.9134\n",
      "Training Epoch: 45 [23100/36045]\tLoss: 554.4875\n",
      "Training Epoch: 45 [23150/36045]\tLoss: 542.7820\n",
      "Training Epoch: 45 [23200/36045]\tLoss: 513.6769\n",
      "Training Epoch: 45 [23250/36045]\tLoss: 516.6575\n",
      "Training Epoch: 45 [23300/36045]\tLoss: 512.6437\n",
      "Training Epoch: 45 [23350/36045]\tLoss: 532.9594\n",
      "Training Epoch: 45 [23400/36045]\tLoss: 578.0596\n",
      "Training Epoch: 45 [23450/36045]\tLoss: 571.7272\n",
      "Training Epoch: 45 [23500/36045]\tLoss: 550.8278\n",
      "Training Epoch: 45 [23550/36045]\tLoss: 590.2555\n",
      "Training Epoch: 45 [23600/36045]\tLoss: 670.3814\n",
      "Training Epoch: 45 [23650/36045]\tLoss: 681.7162\n",
      "Training Epoch: 45 [23700/36045]\tLoss: 689.6725\n",
      "Training Epoch: 45 [23750/36045]\tLoss: 666.0255\n",
      "Training Epoch: 45 [23800/36045]\tLoss: 534.3758\n",
      "Training Epoch: 45 [23850/36045]\tLoss: 560.0527\n",
      "Training Epoch: 45 [23900/36045]\tLoss: 549.9703\n",
      "Training Epoch: 45 [23950/36045]\tLoss: 533.2189\n",
      "Training Epoch: 45 [24000/36045]\tLoss: 510.4221\n",
      "Training Epoch: 45 [24050/36045]\tLoss: 471.0385\n",
      "Training Epoch: 45 [24100/36045]\tLoss: 495.3989\n",
      "Training Epoch: 45 [24150/36045]\tLoss: 487.4890\n",
      "Training Epoch: 45 [24200/36045]\tLoss: 485.2369\n",
      "Training Epoch: 45 [24250/36045]\tLoss: 471.3499\n",
      "Training Epoch: 45 [24300/36045]\tLoss: 509.1626\n",
      "Training Epoch: 45 [24350/36045]\tLoss: 521.1566\n",
      "Training Epoch: 45 [24400/36045]\tLoss: 535.9637\n",
      "Training Epoch: 45 [24450/36045]\tLoss: 510.8021\n",
      "Training Epoch: 45 [24500/36045]\tLoss: 539.3362\n",
      "Training Epoch: 45 [24550/36045]\tLoss: 625.9139\n",
      "Training Epoch: 45 [24600/36045]\tLoss: 617.0931\n",
      "Training Epoch: 45 [24650/36045]\tLoss: 590.4567\n",
      "Training Epoch: 45 [24700/36045]\tLoss: 600.3644\n",
      "Training Epoch: 45 [24750/36045]\tLoss: 555.1008\n",
      "Training Epoch: 45 [24800/36045]\tLoss: 454.4337\n",
      "Training Epoch: 45 [24850/36045]\tLoss: 472.5176\n",
      "Training Epoch: 45 [24900/36045]\tLoss: 469.9796\n",
      "Training Epoch: 45 [24950/36045]\tLoss: 472.4578\n",
      "Training Epoch: 45 [25000/36045]\tLoss: 454.3950\n",
      "Training Epoch: 45 [25050/36045]\tLoss: 434.4957\n",
      "Training Epoch: 45 [25100/36045]\tLoss: 389.2357\n",
      "Training Epoch: 45 [25150/36045]\tLoss: 360.2767\n",
      "Training Epoch: 45 [25200/36045]\tLoss: 355.2053\n",
      "Training Epoch: 45 [25250/36045]\tLoss: 380.9206\n",
      "Training Epoch: 45 [25300/36045]\tLoss: 500.6227\n",
      "Training Epoch: 45 [25350/36045]\tLoss: 496.8712\n",
      "Training Epoch: 45 [25400/36045]\tLoss: 464.4549\n",
      "Training Epoch: 45 [25450/36045]\tLoss: 466.3958\n",
      "Training Epoch: 45 [25500/36045]\tLoss: 506.5893\n",
      "Training Epoch: 45 [25550/36045]\tLoss: 592.6316\n",
      "Training Epoch: 45 [25600/36045]\tLoss: 596.8839\n",
      "Training Epoch: 45 [25650/36045]\tLoss: 576.2123\n",
      "Training Epoch: 45 [25700/36045]\tLoss: 585.7606\n",
      "Training Epoch: 45 [25750/36045]\tLoss: 566.0195\n",
      "Training Epoch: 45 [25800/36045]\tLoss: 356.9861\n",
      "Training Epoch: 45 [25850/36045]\tLoss: 364.7259\n",
      "Training Epoch: 45 [25900/36045]\tLoss: 346.5697\n",
      "Training Epoch: 45 [25950/36045]\tLoss: 356.2852\n",
      "Training Epoch: 45 [26000/36045]\tLoss: 438.4378\n",
      "Training Epoch: 45 [26050/36045]\tLoss: 596.2695\n",
      "Training Epoch: 45 [26100/36045]\tLoss: 621.8568\n",
      "Training Epoch: 45 [26150/36045]\tLoss: 621.1836\n",
      "Training Epoch: 45 [26200/36045]\tLoss: 594.9582\n",
      "Training Epoch: 45 [26250/36045]\tLoss: 625.1686\n",
      "Training Epoch: 45 [26300/36045]\tLoss: 574.8686\n",
      "Training Epoch: 45 [26350/36045]\tLoss: 585.4358\n",
      "Training Epoch: 45 [26400/36045]\tLoss: 560.4501\n",
      "Training Epoch: 45 [26450/36045]\tLoss: 490.8956\n",
      "Training Epoch: 45 [26500/36045]\tLoss: 581.4107\n",
      "Training Epoch: 45 [26550/36045]\tLoss: 580.9343\n",
      "Training Epoch: 45 [26600/36045]\tLoss: 577.0947\n",
      "Training Epoch: 45 [26650/36045]\tLoss: 591.6812\n",
      "Training Epoch: 45 [26700/36045]\tLoss: 570.8644\n",
      "Training Epoch: 45 [26750/36045]\tLoss: 535.6785\n",
      "Training Epoch: 45 [26800/36045]\tLoss: 396.0106\n",
      "Training Epoch: 45 [26850/36045]\tLoss: 327.7681\n",
      "Training Epoch: 45 [26900/36045]\tLoss: 329.4685\n",
      "Training Epoch: 45 [26950/36045]\tLoss: 361.4349\n",
      "Training Epoch: 45 [27000/36045]\tLoss: 596.4341\n",
      "Training Epoch: 45 [27050/36045]\tLoss: 622.6550\n",
      "Training Epoch: 45 [27100/36045]\tLoss: 603.6442\n",
      "Training Epoch: 45 [27150/36045]\tLoss: 642.2949\n",
      "Training Epoch: 45 [27200/36045]\tLoss: 465.4288\n",
      "Training Epoch: 45 [27250/36045]\tLoss: 455.9111\n",
      "Training Epoch: 45 [27300/36045]\tLoss: 444.7094\n",
      "Training Epoch: 45 [27350/36045]\tLoss: 441.9746\n",
      "Training Epoch: 45 [27400/36045]\tLoss: 441.4028\n",
      "Training Epoch: 45 [27450/36045]\tLoss: 558.9393\n",
      "Training Epoch: 45 [27500/36045]\tLoss: 598.9492\n",
      "Training Epoch: 45 [27550/36045]\tLoss: 592.2991\n",
      "Training Epoch: 45 [27600/36045]\tLoss: 603.7454\n",
      "Training Epoch: 45 [27650/36045]\tLoss: 595.7765\n",
      "Training Epoch: 45 [27700/36045]\tLoss: 622.8885\n",
      "Training Epoch: 45 [27750/36045]\tLoss: 634.7477\n",
      "Training Epoch: 45 [27800/36045]\tLoss: 622.1379\n",
      "Training Epoch: 45 [27850/36045]\tLoss: 612.5456\n",
      "Training Epoch: 45 [27900/36045]\tLoss: 557.3190\n",
      "Training Epoch: 45 [27950/36045]\tLoss: 466.5461\n",
      "Training Epoch: 45 [28000/36045]\tLoss: 443.4224\n",
      "Training Epoch: 45 [28050/36045]\tLoss: 452.8220\n",
      "Training Epoch: 45 [28100/36045]\tLoss: 444.5210\n",
      "Training Epoch: 45 [28150/36045]\tLoss: 463.2936\n",
      "Training Epoch: 45 [28200/36045]\tLoss: 471.8224\n",
      "Training Epoch: 45 [28250/36045]\tLoss: 464.6008\n",
      "Training Epoch: 45 [28300/36045]\tLoss: 441.0431\n",
      "Training Epoch: 45 [28350/36045]\tLoss: 436.8223\n",
      "Training Epoch: 45 [28400/36045]\tLoss: 763.9672\n",
      "Training Epoch: 45 [28450/36045]\tLoss: 701.5444\n",
      "Training Epoch: 45 [28500/36045]\tLoss: 606.1838\n",
      "Training Epoch: 45 [28550/36045]\tLoss: 557.4348\n",
      "Training Epoch: 45 [28600/36045]\tLoss: 581.4268\n",
      "Training Epoch: 45 [28650/36045]\tLoss: 634.2483\n",
      "Training Epoch: 45 [28700/36045]\tLoss: 628.0538\n",
      "Training Epoch: 45 [28750/36045]\tLoss: 615.1359\n",
      "Training Epoch: 45 [28800/36045]\tLoss: 625.1311\n",
      "Training Epoch: 45 [28850/36045]\tLoss: 543.0400\n",
      "Training Epoch: 45 [28900/36045]\tLoss: 442.1038\n",
      "Training Epoch: 45 [28950/36045]\tLoss: 441.2838\n",
      "Training Epoch: 45 [29000/36045]\tLoss: 437.2244\n",
      "Training Epoch: 45 [29050/36045]\tLoss: 444.2466\n",
      "Training Epoch: 45 [29100/36045]\tLoss: 462.1320\n",
      "Training Epoch: 45 [29150/36045]\tLoss: 452.6736\n",
      "Training Epoch: 45 [29200/36045]\tLoss: 438.4703\n",
      "Training Epoch: 45 [29250/36045]\tLoss: 428.3973\n",
      "Training Epoch: 45 [29300/36045]\tLoss: 481.3791\n",
      "Training Epoch: 45 [29350/36045]\tLoss: 564.8978\n",
      "Training Epoch: 45 [29400/36045]\tLoss: 582.4368\n",
      "Training Epoch: 45 [29450/36045]\tLoss: 598.9429\n",
      "Training Epoch: 45 [29500/36045]\tLoss: 613.7942\n",
      "Training Epoch: 45 [29550/36045]\tLoss: 583.7623\n",
      "Training Epoch: 45 [29600/36045]\tLoss: 490.8764\n",
      "Training Epoch: 45 [29650/36045]\tLoss: 472.4253\n",
      "Training Epoch: 45 [29700/36045]\tLoss: 424.4877\n",
      "Training Epoch: 45 [29750/36045]\tLoss: 422.8660\n",
      "Training Epoch: 45 [29800/36045]\tLoss: 469.9021\n",
      "Training Epoch: 45 [29850/36045]\tLoss: 547.2934\n",
      "Training Epoch: 45 [29900/36045]\tLoss: 542.7720\n",
      "Training Epoch: 45 [29950/36045]\tLoss: 564.5540\n",
      "Training Epoch: 45 [30000/36045]\tLoss: 538.5008\n",
      "Training Epoch: 45 [30050/36045]\tLoss: 545.7949\n",
      "Training Epoch: 45 [30100/36045]\tLoss: 665.6978\n",
      "Training Epoch: 45 [30150/36045]\tLoss: 647.8870\n",
      "Training Epoch: 45 [30200/36045]\tLoss: 610.9177\n",
      "Training Epoch: 45 [30250/36045]\tLoss: 659.3857\n",
      "Training Epoch: 45 [30300/36045]\tLoss: 643.6349\n",
      "Training Epoch: 45 [30350/36045]\tLoss: 490.4820\n",
      "Training Epoch: 45 [30400/36045]\tLoss: 475.1573\n",
      "Training Epoch: 45 [30450/36045]\tLoss: 477.5031\n",
      "Training Epoch: 45 [30500/36045]\tLoss: 445.6117\n",
      "Training Epoch: 45 [30550/36045]\tLoss: 413.5467\n",
      "Training Epoch: 45 [30600/36045]\tLoss: 406.4039\n",
      "Training Epoch: 45 [30650/36045]\tLoss: 396.5596\n",
      "Training Epoch: 45 [30700/36045]\tLoss: 414.2100\n",
      "Training Epoch: 45 [30750/36045]\tLoss: 401.1967\n",
      "Training Epoch: 45 [30800/36045]\tLoss: 427.2715\n",
      "Training Epoch: 45 [30850/36045]\tLoss: 419.0582\n",
      "Training Epoch: 45 [30900/36045]\tLoss: 430.7998\n",
      "Training Epoch: 45 [30950/36045]\tLoss: 452.7883\n",
      "Training Epoch: 45 [31000/36045]\tLoss: 445.2327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 45 [31050/36045]\tLoss: 371.6494\n",
      "Training Epoch: 45 [31100/36045]\tLoss: 362.3574\n",
      "Training Epoch: 45 [31150/36045]\tLoss: 370.0435\n",
      "Training Epoch: 45 [31200/36045]\tLoss: 459.3513\n",
      "Training Epoch: 45 [31250/36045]\tLoss: 596.0379\n",
      "Training Epoch: 45 [31300/36045]\tLoss: 567.3251\n",
      "Training Epoch: 45 [31350/36045]\tLoss: 583.3394\n",
      "Training Epoch: 45 [31400/36045]\tLoss: 562.9138\n",
      "Training Epoch: 45 [31450/36045]\tLoss: 579.6768\n",
      "Training Epoch: 45 [31500/36045]\tLoss: 592.2057\n",
      "Training Epoch: 45 [31550/36045]\tLoss: 598.8821\n",
      "Training Epoch: 45 [31600/36045]\tLoss: 563.0369\n",
      "Training Epoch: 45 [31650/36045]\tLoss: 602.9514\n",
      "Training Epoch: 45 [31700/36045]\tLoss: 435.9008\n",
      "Training Epoch: 45 [31750/36045]\tLoss: 360.1034\n",
      "Training Epoch: 45 [31800/36045]\tLoss: 343.7263\n",
      "Training Epoch: 45 [31850/36045]\tLoss: 351.3391\n",
      "Training Epoch: 45 [31900/36045]\tLoss: 556.6760\n",
      "Training Epoch: 45 [31950/36045]\tLoss: 721.8469\n",
      "Training Epoch: 45 [32000/36045]\tLoss: 829.1057\n",
      "Training Epoch: 45 [32050/36045]\tLoss: 784.9407\n",
      "Training Epoch: 45 [32100/36045]\tLoss: 776.2778\n",
      "Training Epoch: 45 [32150/36045]\tLoss: 594.5807\n",
      "Training Epoch: 45 [32200/36045]\tLoss: 596.4196\n",
      "Training Epoch: 45 [32250/36045]\tLoss: 606.6163\n",
      "Training Epoch: 45 [32300/36045]\tLoss: 588.7824\n",
      "Training Epoch: 45 [32350/36045]\tLoss: 584.8684\n",
      "Training Epoch: 45 [32400/36045]\tLoss: 548.7786\n",
      "Training Epoch: 45 [32450/36045]\tLoss: 451.8615\n",
      "Training Epoch: 45 [32500/36045]\tLoss: 434.1282\n",
      "Training Epoch: 45 [32550/36045]\tLoss: 436.2180\n",
      "Training Epoch: 45 [32600/36045]\tLoss: 433.3584\n",
      "Training Epoch: 45 [32650/36045]\tLoss: 562.4117\n",
      "Training Epoch: 45 [32700/36045]\tLoss: 614.3894\n",
      "Training Epoch: 45 [32750/36045]\tLoss: 584.9626\n",
      "Training Epoch: 45 [32800/36045]\tLoss: 599.7196\n",
      "Training Epoch: 45 [32850/36045]\tLoss: 553.6082\n",
      "Training Epoch: 45 [32900/36045]\tLoss: 442.1832\n",
      "Training Epoch: 45 [32950/36045]\tLoss: 463.2361\n",
      "Training Epoch: 45 [33000/36045]\tLoss: 461.8160\n",
      "Training Epoch: 45 [33050/36045]\tLoss: 439.4696\n",
      "Training Epoch: 45 [33100/36045]\tLoss: 499.4083\n",
      "Training Epoch: 45 [33150/36045]\tLoss: 680.0543\n",
      "Training Epoch: 45 [33200/36045]\tLoss: 662.0645\n",
      "Training Epoch: 45 [33250/36045]\tLoss: 682.2109\n",
      "Training Epoch: 45 [33300/36045]\tLoss: 727.1992\n",
      "Training Epoch: 45 [33350/36045]\tLoss: 556.5753\n",
      "Training Epoch: 45 [33400/36045]\tLoss: 404.8902\n",
      "Training Epoch: 45 [33450/36045]\tLoss: 400.9103\n",
      "Training Epoch: 45 [33500/36045]\tLoss: 412.6959\n",
      "Training Epoch: 45 [33550/36045]\tLoss: 427.2876\n",
      "Training Epoch: 45 [33600/36045]\tLoss: 429.0681\n",
      "Training Epoch: 45 [33650/36045]\tLoss: 573.3909\n",
      "Training Epoch: 45 [33700/36045]\tLoss: 554.6354\n",
      "Training Epoch: 45 [33750/36045]\tLoss: 574.4049\n",
      "Training Epoch: 45 [33800/36045]\tLoss: 571.8159\n",
      "Training Epoch: 45 [33850/36045]\tLoss: 573.5567\n",
      "Training Epoch: 45 [33900/36045]\tLoss: 587.1147\n",
      "Training Epoch: 45 [33950/36045]\tLoss: 596.6149\n",
      "Training Epoch: 45 [34000/36045]\tLoss: 583.2781\n",
      "Training Epoch: 45 [34050/36045]\tLoss: 587.0173\n",
      "Training Epoch: 45 [34100/36045]\tLoss: 566.1153\n",
      "Training Epoch: 45 [34150/36045]\tLoss: 524.2587\n",
      "Training Epoch: 45 [34200/36045]\tLoss: 496.5945\n",
      "Training Epoch: 45 [34250/36045]\tLoss: 510.4365\n",
      "Training Epoch: 45 [34300/36045]\tLoss: 435.5359\n",
      "Training Epoch: 45 [34350/36045]\tLoss: 458.8083\n",
      "Training Epoch: 45 [34400/36045]\tLoss: 451.9750\n",
      "Training Epoch: 45 [34450/36045]\tLoss: 425.6396\n",
      "Training Epoch: 45 [34500/36045]\tLoss: 453.9759\n",
      "Training Epoch: 45 [34550/36045]\tLoss: 445.6385\n",
      "Training Epoch: 45 [34600/36045]\tLoss: 452.7131\n",
      "Training Epoch: 45 [34650/36045]\tLoss: 558.3657\n",
      "Training Epoch: 45 [34700/36045]\tLoss: 592.3257\n",
      "Training Epoch: 45 [34750/36045]\tLoss: 524.9113\n",
      "Training Epoch: 45 [34800/36045]\tLoss: 602.9910\n",
      "Training Epoch: 45 [34850/36045]\tLoss: 609.9748\n",
      "Training Epoch: 45 [34900/36045]\tLoss: 656.8472\n",
      "Training Epoch: 45 [34950/36045]\tLoss: 641.5046\n",
      "Training Epoch: 45 [35000/36045]\tLoss: 642.7059\n",
      "Training Epoch: 45 [35050/36045]\tLoss: 630.1151\n",
      "Training Epoch: 45 [35100/36045]\tLoss: 544.8713\n",
      "Training Epoch: 45 [35150/36045]\tLoss: 536.8864\n",
      "Training Epoch: 45 [35200/36045]\tLoss: 451.1340\n",
      "Training Epoch: 45 [35250/36045]\tLoss: 495.8982\n",
      "Training Epoch: 45 [35300/36045]\tLoss: 512.2285\n",
      "Training Epoch: 45 [35350/36045]\tLoss: 572.0728\n",
      "Training Epoch: 45 [35400/36045]\tLoss: 601.4278\n",
      "Training Epoch: 45 [35450/36045]\tLoss: 572.7830\n",
      "Training Epoch: 45 [35500/36045]\tLoss: 554.0106\n",
      "Training Epoch: 45 [35550/36045]\tLoss: 540.6124\n",
      "Training Epoch: 45 [35600/36045]\tLoss: 591.5284\n",
      "Training Epoch: 45 [35650/36045]\tLoss: 663.7360\n",
      "Training Epoch: 45 [35700/36045]\tLoss: 590.4221\n",
      "Training Epoch: 45 [35750/36045]\tLoss: 647.6272\n",
      "Training Epoch: 45 [35800/36045]\tLoss: 654.4045\n",
      "Training Epoch: 45 [35850/36045]\tLoss: 628.6648\n",
      "Training Epoch: 45 [35900/36045]\tLoss: 650.4739\n",
      "Training Epoch: 45 [35950/36045]\tLoss: 647.0286\n",
      "Training Epoch: 45 [36000/36045]\tLoss: 640.1940\n",
      "Training Epoch: 45 [36045/36045]\tLoss: 625.8312\n",
      "Training Epoch: 45 [4004/4004]\tLoss: 581.0913\n",
      "Training Epoch: 46 [50/36045]\tLoss: 579.5137\n",
      "Training Epoch: 46 [100/36045]\tLoss: 555.2405\n",
      "Training Epoch: 46 [150/36045]\tLoss: 553.0718\n",
      "Training Epoch: 46 [200/36045]\tLoss: 539.2926\n",
      "Training Epoch: 46 [250/36045]\tLoss: 648.8338\n",
      "Training Epoch: 46 [300/36045]\tLoss: 714.0820\n",
      "Training Epoch: 46 [350/36045]\tLoss: 681.0057\n",
      "Training Epoch: 46 [400/36045]\tLoss: 675.2822\n",
      "Training Epoch: 46 [450/36045]\tLoss: 656.0424\n",
      "Training Epoch: 46 [500/36045]\tLoss: 606.9415\n",
      "Training Epoch: 46 [550/36045]\tLoss: 609.8696\n",
      "Training Epoch: 46 [600/36045]\tLoss: 596.0237\n",
      "Training Epoch: 46 [650/36045]\tLoss: 617.1715\n",
      "Training Epoch: 46 [700/36045]\tLoss: 602.3239\n",
      "Training Epoch: 46 [750/36045]\tLoss: 578.4177\n",
      "Training Epoch: 46 [800/36045]\tLoss: 589.9789\n",
      "Training Epoch: 46 [850/36045]\tLoss: 573.3685\n",
      "Training Epoch: 46 [900/36045]\tLoss: 549.2542\n",
      "Training Epoch: 46 [950/36045]\tLoss: 519.4205\n",
      "Training Epoch: 46 [1000/36045]\tLoss: 503.4774\n",
      "Training Epoch: 46 [1050/36045]\tLoss: 505.4414\n",
      "Training Epoch: 46 [1100/36045]\tLoss: 491.8501\n",
      "Training Epoch: 46 [1150/36045]\tLoss: 501.2630\n",
      "Training Epoch: 46 [1200/36045]\tLoss: 530.8527\n",
      "Training Epoch: 46 [1250/36045]\tLoss: 607.2807\n",
      "Training Epoch: 46 [1300/36045]\tLoss: 614.2047\n",
      "Training Epoch: 46 [1350/36045]\tLoss: 615.3900\n",
      "Training Epoch: 46 [1400/36045]\tLoss: 639.0578\n",
      "Training Epoch: 46 [1450/36045]\tLoss: 618.4674\n",
      "Training Epoch: 46 [1500/36045]\tLoss: 565.0634\n",
      "Training Epoch: 46 [1550/36045]\tLoss: 579.5753\n",
      "Training Epoch: 46 [1600/36045]\tLoss: 589.4472\n",
      "Training Epoch: 46 [1650/36045]\tLoss: 576.8530\n",
      "Training Epoch: 46 [1700/36045]\tLoss: 589.1049\n",
      "Training Epoch: 46 [1750/36045]\tLoss: 630.6354\n",
      "Training Epoch: 46 [1800/36045]\tLoss: 613.4856\n",
      "Training Epoch: 46 [1850/36045]\tLoss: 629.1823\n",
      "Training Epoch: 46 [1900/36045]\tLoss: 588.2852\n",
      "Training Epoch: 46 [1950/36045]\tLoss: 598.0292\n",
      "Training Epoch: 46 [2000/36045]\tLoss: 538.4434\n",
      "Training Epoch: 46 [2050/36045]\tLoss: 541.1808\n",
      "Training Epoch: 46 [2100/36045]\tLoss: 570.4808\n",
      "Training Epoch: 46 [2150/36045]\tLoss: 551.4655\n",
      "Training Epoch: 46 [2200/36045]\tLoss: 513.9185\n",
      "Training Epoch: 46 [2250/36045]\tLoss: 485.5504\n",
      "Training Epoch: 46 [2300/36045]\tLoss: 509.1440\n",
      "Training Epoch: 46 [2350/36045]\tLoss: 486.7889\n",
      "Training Epoch: 46 [2400/36045]\tLoss: 494.2893\n",
      "Training Epoch: 46 [2450/36045]\tLoss: 633.2385\n",
      "Training Epoch: 46 [2500/36045]\tLoss: 664.9218\n",
      "Training Epoch: 46 [2550/36045]\tLoss: 662.8057\n",
      "Training Epoch: 46 [2600/36045]\tLoss: 671.4816\n",
      "Training Epoch: 46 [2650/36045]\tLoss: 795.6296\n",
      "Training Epoch: 46 [2700/36045]\tLoss: 883.5103\n",
      "Training Epoch: 46 [2750/36045]\tLoss: 954.3093\n",
      "Training Epoch: 46 [2800/36045]\tLoss: 963.6395\n",
      "Training Epoch: 46 [2850/36045]\tLoss: 730.7714\n",
      "Training Epoch: 46 [2900/36045]\tLoss: 691.6427\n",
      "Training Epoch: 46 [2950/36045]\tLoss: 669.1371\n",
      "Training Epoch: 46 [3000/36045]\tLoss: 662.9911\n",
      "Training Epoch: 46 [3050/36045]\tLoss: 693.7233\n",
      "Training Epoch: 46 [3100/36045]\tLoss: 633.8464\n",
      "Training Epoch: 46 [3150/36045]\tLoss: 487.3187\n",
      "Training Epoch: 46 [3200/36045]\tLoss: 504.5426\n",
      "Training Epoch: 46 [3250/36045]\tLoss: 476.1527\n",
      "Training Epoch: 46 [3300/36045]\tLoss: 450.9376\n",
      "Training Epoch: 46 [3350/36045]\tLoss: 476.3658\n",
      "Training Epoch: 46 [3400/36045]\tLoss: 499.0526\n",
      "Training Epoch: 46 [3450/36045]\tLoss: 535.2288\n",
      "Training Epoch: 46 [3500/36045]\tLoss: 522.5657\n",
      "Training Epoch: 46 [3550/36045]\tLoss: 499.7200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 46 [3600/36045]\tLoss: 537.0944\n",
      "Training Epoch: 46 [3650/36045]\tLoss: 621.2289\n",
      "Training Epoch: 46 [3700/36045]\tLoss: 628.9612\n",
      "Training Epoch: 46 [3750/36045]\tLoss: 598.1665\n",
      "Training Epoch: 46 [3800/36045]\tLoss: 595.2252\n",
      "Training Epoch: 46 [3850/36045]\tLoss: 597.0527\n",
      "Training Epoch: 46 [3900/36045]\tLoss: 601.6866\n",
      "Training Epoch: 46 [3950/36045]\tLoss: 580.7578\n",
      "Training Epoch: 46 [4000/36045]\tLoss: 585.3364\n",
      "Training Epoch: 46 [4050/36045]\tLoss: 536.6030\n",
      "Training Epoch: 46 [4100/36045]\tLoss: 523.3336\n",
      "Training Epoch: 46 [4150/36045]\tLoss: 537.3419\n",
      "Training Epoch: 46 [4200/36045]\tLoss: 532.3627\n",
      "Training Epoch: 46 [4250/36045]\tLoss: 534.5824\n",
      "Training Epoch: 46 [4300/36045]\tLoss: 549.7670\n",
      "Training Epoch: 46 [4350/36045]\tLoss: 533.6000\n",
      "Training Epoch: 46 [4400/36045]\tLoss: 510.2970\n",
      "Training Epoch: 46 [4450/36045]\tLoss: 560.7236\n",
      "Training Epoch: 46 [4500/36045]\tLoss: 603.6359\n",
      "Training Epoch: 46 [4550/36045]\tLoss: 606.3943\n",
      "Training Epoch: 46 [4600/36045]\tLoss: 628.1906\n",
      "Training Epoch: 46 [4650/36045]\tLoss: 618.1702\n",
      "Training Epoch: 46 [4700/36045]\tLoss: 569.9315\n",
      "Training Epoch: 46 [4750/36045]\tLoss: 551.9462\n",
      "Training Epoch: 46 [4800/36045]\tLoss: 576.0833\n",
      "Training Epoch: 46 [4850/36045]\tLoss: 562.6034\n",
      "Training Epoch: 46 [4900/36045]\tLoss: 547.6854\n",
      "Training Epoch: 46 [4950/36045]\tLoss: 563.0769\n",
      "Training Epoch: 46 [5000/36045]\tLoss: 592.4995\n",
      "Training Epoch: 46 [5050/36045]\tLoss: 574.0485\n",
      "Training Epoch: 46 [5100/36045]\tLoss: 584.1308\n",
      "Training Epoch: 46 [5150/36045]\tLoss: 568.5276\n",
      "Training Epoch: 46 [5200/36045]\tLoss: 566.4689\n",
      "Training Epoch: 46 [5250/36045]\tLoss: 560.1541\n",
      "Training Epoch: 46 [5300/36045]\tLoss: 560.2349\n",
      "Training Epoch: 46 [5350/36045]\tLoss: 581.2701\n",
      "Training Epoch: 46 [5400/36045]\tLoss: 560.2609\n",
      "Training Epoch: 46 [5450/36045]\tLoss: 530.8204\n",
      "Training Epoch: 46 [5500/36045]\tLoss: 558.9572\n",
      "Training Epoch: 46 [5550/36045]\tLoss: 547.2240\n",
      "Training Epoch: 46 [5600/36045]\tLoss: 625.7476\n",
      "Training Epoch: 46 [5650/36045]\tLoss: 591.5569\n",
      "Training Epoch: 46 [5700/36045]\tLoss: 554.9011\n",
      "Training Epoch: 46 [5750/36045]\tLoss: 538.9958\n",
      "Training Epoch: 46 [5800/36045]\tLoss: 568.6876\n",
      "Training Epoch: 46 [5850/36045]\tLoss: 558.3514\n",
      "Training Epoch: 46 [5900/36045]\tLoss: 641.4338\n",
      "Training Epoch: 46 [5950/36045]\tLoss: 657.8402\n",
      "Training Epoch: 46 [6000/36045]\tLoss: 643.9004\n",
      "Training Epoch: 46 [6050/36045]\tLoss: 622.1237\n",
      "Training Epoch: 46 [6100/36045]\tLoss: 626.3826\n",
      "Training Epoch: 46 [6150/36045]\tLoss: 617.2662\n",
      "Training Epoch: 46 [6200/36045]\tLoss: 621.9964\n",
      "Training Epoch: 46 [6250/36045]\tLoss: 643.5057\n",
      "Training Epoch: 46 [6300/36045]\tLoss: 655.0590\n",
      "Training Epoch: 46 [6350/36045]\tLoss: 700.2291\n",
      "Training Epoch: 46 [6400/36045]\tLoss: 576.6611\n",
      "Training Epoch: 46 [6450/36045]\tLoss: 530.0983\n",
      "Training Epoch: 46 [6500/36045]\tLoss: 539.7453\n",
      "Training Epoch: 46 [6550/36045]\tLoss: 557.1769\n",
      "Training Epoch: 46 [6600/36045]\tLoss: 555.1110\n",
      "Training Epoch: 46 [6650/36045]\tLoss: 626.9044\n",
      "Training Epoch: 46 [6700/36045]\tLoss: 655.5766\n",
      "Training Epoch: 46 [6750/36045]\tLoss: 633.1095\n",
      "Training Epoch: 46 [6800/36045]\tLoss: 636.0926\n",
      "Training Epoch: 46 [6850/36045]\tLoss: 624.2003\n",
      "Training Epoch: 46 [6900/36045]\tLoss: 556.0510\n",
      "Training Epoch: 46 [6950/36045]\tLoss: 524.2818\n",
      "Training Epoch: 46 [7000/36045]\tLoss: 557.5742\n",
      "Training Epoch: 46 [7050/36045]\tLoss: 569.7487\n",
      "Training Epoch: 46 [7100/36045]\tLoss: 568.8938\n",
      "Training Epoch: 46 [7150/36045]\tLoss: 578.8860\n",
      "Training Epoch: 46 [7200/36045]\tLoss: 581.0020\n",
      "Training Epoch: 46 [7250/36045]\tLoss: 579.3269\n",
      "Training Epoch: 46 [7300/36045]\tLoss: 565.6774\n",
      "Training Epoch: 46 [7350/36045]\tLoss: 563.4537\n",
      "Training Epoch: 46 [7400/36045]\tLoss: 513.8596\n",
      "Training Epoch: 46 [7450/36045]\tLoss: 517.0858\n",
      "Training Epoch: 46 [7500/36045]\tLoss: 512.7130\n",
      "Training Epoch: 46 [7550/36045]\tLoss: 491.0623\n",
      "Training Epoch: 46 [7600/36045]\tLoss: 544.1403\n",
      "Training Epoch: 46 [7650/36045]\tLoss: 581.5359\n",
      "Training Epoch: 46 [7700/36045]\tLoss: 553.4105\n",
      "Training Epoch: 46 [7750/36045]\tLoss: 567.7787\n",
      "Training Epoch: 46 [7800/36045]\tLoss: 557.4118\n",
      "Training Epoch: 46 [7850/36045]\tLoss: 539.4861\n",
      "Training Epoch: 46 [7900/36045]\tLoss: 568.9200\n",
      "Training Epoch: 46 [7950/36045]\tLoss: 566.4975\n",
      "Training Epoch: 46 [8000/36045]\tLoss: 584.6191\n",
      "Training Epoch: 46 [8050/36045]\tLoss: 550.4249\n",
      "Training Epoch: 46 [8100/36045]\tLoss: 575.4490\n",
      "Training Epoch: 46 [8150/36045]\tLoss: 652.8489\n",
      "Training Epoch: 46 [8200/36045]\tLoss: 640.3679\n",
      "Training Epoch: 46 [8250/36045]\tLoss: 608.7797\n",
      "Training Epoch: 46 [8300/36045]\tLoss: 665.4904\n",
      "Training Epoch: 46 [8350/36045]\tLoss: 609.9260\n",
      "Training Epoch: 46 [8400/36045]\tLoss: 545.4610\n",
      "Training Epoch: 46 [8450/36045]\tLoss: 510.4427\n",
      "Training Epoch: 46 [8500/36045]\tLoss: 543.2650\n",
      "Training Epoch: 46 [8550/36045]\tLoss: 536.8339\n",
      "Training Epoch: 46 [8600/36045]\tLoss: 531.1840\n",
      "Training Epoch: 46 [8650/36045]\tLoss: 563.5643\n",
      "Training Epoch: 46 [8700/36045]\tLoss: 596.0849\n",
      "Training Epoch: 46 [8750/36045]\tLoss: 585.6388\n",
      "Training Epoch: 46 [8800/36045]\tLoss: 591.3653\n",
      "Training Epoch: 46 [8850/36045]\tLoss: 584.9883\n",
      "Training Epoch: 46 [8900/36045]\tLoss: 528.1295\n",
      "Training Epoch: 46 [8950/36045]\tLoss: 538.7647\n",
      "Training Epoch: 46 [9000/36045]\tLoss: 554.6515\n",
      "Training Epoch: 46 [9050/36045]\tLoss: 556.2540\n",
      "Training Epoch: 46 [9100/36045]\tLoss: 572.9207\n",
      "Training Epoch: 46 [9150/36045]\tLoss: 424.0642\n",
      "Training Epoch: 46 [9200/36045]\tLoss: 316.7632\n",
      "Training Epoch: 46 [9250/36045]\tLoss: 343.7793\n",
      "Training Epoch: 46 [9300/36045]\tLoss: 353.4004\n",
      "Training Epoch: 46 [9350/36045]\tLoss: 326.2103\n",
      "Training Epoch: 46 [9400/36045]\tLoss: 638.8086\n",
      "Training Epoch: 46 [9450/36045]\tLoss: 678.3922\n",
      "Training Epoch: 46 [9500/36045]\tLoss: 666.0447\n",
      "Training Epoch: 46 [9550/36045]\tLoss: 704.6257\n",
      "Training Epoch: 46 [9600/36045]\tLoss: 525.2213\n",
      "Training Epoch: 46 [9650/36045]\tLoss: 530.3350\n",
      "Training Epoch: 46 [9700/36045]\tLoss: 515.9015\n",
      "Training Epoch: 46 [9750/36045]\tLoss: 514.5085\n",
      "Training Epoch: 46 [9800/36045]\tLoss: 672.9977\n",
      "Training Epoch: 46 [9850/36045]\tLoss: 710.7418\n",
      "Training Epoch: 46 [9900/36045]\tLoss: 719.9142\n",
      "Training Epoch: 46 [9950/36045]\tLoss: 701.5325\n",
      "Training Epoch: 46 [10000/36045]\tLoss: 649.3475\n",
      "Training Epoch: 46 [10050/36045]\tLoss: 531.5339\n",
      "Training Epoch: 46 [10100/36045]\tLoss: 539.1903\n",
      "Training Epoch: 46 [10150/36045]\tLoss: 547.3142\n",
      "Training Epoch: 46 [10200/36045]\tLoss: 536.2909\n",
      "Training Epoch: 46 [10250/36045]\tLoss: 644.9130\n",
      "Training Epoch: 46 [10300/36045]\tLoss: 626.8174\n",
      "Training Epoch: 46 [10350/36045]\tLoss: 660.2772\n",
      "Training Epoch: 46 [10400/36045]\tLoss: 650.3203\n",
      "Training Epoch: 46 [10450/36045]\tLoss: 609.3413\n",
      "Training Epoch: 46 [10500/36045]\tLoss: 508.8629\n",
      "Training Epoch: 46 [10550/36045]\tLoss: 503.4637\n",
      "Training Epoch: 46 [10600/36045]\tLoss: 525.3232\n",
      "Training Epoch: 46 [10650/36045]\tLoss: 531.1879\n",
      "Training Epoch: 46 [10700/36045]\tLoss: 610.6672\n",
      "Training Epoch: 46 [10750/36045]\tLoss: 669.5797\n",
      "Training Epoch: 46 [10800/36045]\tLoss: 615.5201\n",
      "Training Epoch: 46 [10850/36045]\tLoss: 652.8366\n",
      "Training Epoch: 46 [10900/36045]\tLoss: 679.6389\n",
      "Training Epoch: 46 [10950/36045]\tLoss: 499.8197\n",
      "Training Epoch: 46 [11000/36045]\tLoss: 493.5736\n",
      "Training Epoch: 46 [11050/36045]\tLoss: 529.4194\n",
      "Training Epoch: 46 [11100/36045]\tLoss: 539.5110\n",
      "Training Epoch: 46 [11150/36045]\tLoss: 585.3426\n",
      "Training Epoch: 46 [11200/36045]\tLoss: 613.7570\n",
      "Training Epoch: 46 [11250/36045]\tLoss: 625.0599\n",
      "Training Epoch: 46 [11300/36045]\tLoss: 605.4281\n",
      "Training Epoch: 46 [11350/36045]\tLoss: 603.1707\n",
      "Training Epoch: 46 [11400/36045]\tLoss: 566.7954\n",
      "Training Epoch: 46 [11450/36045]\tLoss: 536.0948\n",
      "Training Epoch: 46 [11500/36045]\tLoss: 533.5165\n",
      "Training Epoch: 46 [11550/36045]\tLoss: 543.0099\n",
      "Training Epoch: 46 [11600/36045]\tLoss: 602.9670\n",
      "Training Epoch: 46 [11650/36045]\tLoss: 654.3085\n",
      "Training Epoch: 46 [11700/36045]\tLoss: 653.2263\n",
      "Training Epoch: 46 [11750/36045]\tLoss: 671.1321\n",
      "Training Epoch: 46 [11800/36045]\tLoss: 713.5907\n",
      "Training Epoch: 46 [11850/36045]\tLoss: 772.0934\n",
      "Training Epoch: 46 [11900/36045]\tLoss: 983.9621\n",
      "Training Epoch: 46 [11950/36045]\tLoss: 987.1302\n",
      "Training Epoch: 46 [12000/36045]\tLoss: 998.3307\n",
      "Training Epoch: 46 [12050/36045]\tLoss: 957.9856\n",
      "Training Epoch: 46 [12100/36045]\tLoss: 612.2568\n",
      "Training Epoch: 46 [12150/36045]\tLoss: 460.2462\n",
      "Training Epoch: 46 [12200/36045]\tLoss: 455.0960\n",
      "Training Epoch: 46 [12250/36045]\tLoss: 463.6783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 46 [12300/36045]\tLoss: 598.4636\n",
      "Training Epoch: 46 [12350/36045]\tLoss: 653.5501\n",
      "Training Epoch: 46 [12400/36045]\tLoss: 660.7300\n",
      "Training Epoch: 46 [12450/36045]\tLoss: 649.6589\n",
      "Training Epoch: 46 [12500/36045]\tLoss: 676.5222\n",
      "Training Epoch: 46 [12550/36045]\tLoss: 646.5059\n",
      "Training Epoch: 46 [12600/36045]\tLoss: 590.9514\n",
      "Training Epoch: 46 [12650/36045]\tLoss: 589.1377\n",
      "Training Epoch: 46 [12700/36045]\tLoss: 610.3750\n",
      "Training Epoch: 46 [12750/36045]\tLoss: 608.9119\n",
      "Training Epoch: 46 [12800/36045]\tLoss: 595.2143\n",
      "Training Epoch: 46 [12850/36045]\tLoss: 624.4042\n",
      "Training Epoch: 46 [12900/36045]\tLoss: 598.5319\n",
      "Training Epoch: 46 [12950/36045]\tLoss: 584.2272\n",
      "Training Epoch: 46 [13000/36045]\tLoss: 617.5176\n",
      "Training Epoch: 46 [13050/36045]\tLoss: 558.2375\n",
      "Training Epoch: 46 [13100/36045]\tLoss: 573.4689\n",
      "Training Epoch: 46 [13150/36045]\tLoss: 565.0822\n",
      "Training Epoch: 46 [13200/36045]\tLoss: 548.2785\n",
      "Training Epoch: 46 [13250/36045]\tLoss: 569.8984\n",
      "Training Epoch: 46 [13300/36045]\tLoss: 606.5954\n",
      "Training Epoch: 46 [13350/36045]\tLoss: 587.6743\n",
      "Training Epoch: 46 [13400/36045]\tLoss: 590.7504\n",
      "Training Epoch: 46 [13450/36045]\tLoss: 588.2495\n",
      "Training Epoch: 46 [13500/36045]\tLoss: 606.7651\n",
      "Training Epoch: 46 [13550/36045]\tLoss: 744.7789\n",
      "Training Epoch: 46 [13600/36045]\tLoss: 777.3818\n",
      "Training Epoch: 46 [13650/36045]\tLoss: 859.1733\n",
      "Training Epoch: 46 [13700/36045]\tLoss: 756.5159\n",
      "Training Epoch: 46 [13750/36045]\tLoss: 594.2063\n",
      "Training Epoch: 46 [13800/36045]\tLoss: 564.9835\n",
      "Training Epoch: 46 [13850/36045]\tLoss: 547.7099\n",
      "Training Epoch: 46 [13900/36045]\tLoss: 554.9684\n",
      "Training Epoch: 46 [13950/36045]\tLoss: 600.1161\n",
      "Training Epoch: 46 [14000/36045]\tLoss: 632.7476\n",
      "Training Epoch: 46 [14050/36045]\tLoss: 607.8495\n",
      "Training Epoch: 46 [14100/36045]\tLoss: 602.8679\n",
      "Training Epoch: 46 [14150/36045]\tLoss: 590.9827\n",
      "Training Epoch: 46 [14200/36045]\tLoss: 631.4078\n",
      "Training Epoch: 46 [14250/36045]\tLoss: 694.0091\n",
      "Training Epoch: 46 [14300/36045]\tLoss: 697.1767\n",
      "Training Epoch: 46 [14350/36045]\tLoss: 666.1701\n",
      "Training Epoch: 46 [14400/36045]\tLoss: 652.2743\n",
      "Training Epoch: 46 [14450/36045]\tLoss: 687.6412\n",
      "Training Epoch: 46 [14500/36045]\tLoss: 619.3110\n",
      "Training Epoch: 46 [14550/36045]\tLoss: 647.1341\n",
      "Training Epoch: 46 [14600/36045]\tLoss: 633.6598\n",
      "Training Epoch: 46 [14650/36045]\tLoss: 633.2986\n",
      "Training Epoch: 46 [14700/36045]\tLoss: 600.6342\n",
      "Training Epoch: 46 [14750/36045]\tLoss: 516.8755\n",
      "Training Epoch: 46 [14800/36045]\tLoss: 506.9164\n",
      "Training Epoch: 46 [14850/36045]\tLoss: 513.8869\n",
      "Training Epoch: 46 [14900/36045]\tLoss: 507.3762\n",
      "Training Epoch: 46 [14950/36045]\tLoss: 515.5672\n",
      "Training Epoch: 46 [15000/36045]\tLoss: 528.0890\n",
      "Training Epoch: 46 [15050/36045]\tLoss: 524.6433\n",
      "Training Epoch: 46 [15100/36045]\tLoss: 508.6023\n",
      "Training Epoch: 46 [15150/36045]\tLoss: 504.3671\n",
      "Training Epoch: 46 [15200/36045]\tLoss: 467.3706\n",
      "Training Epoch: 46 [15250/36045]\tLoss: 488.9382\n",
      "Training Epoch: 46 [15300/36045]\tLoss: 474.7117\n",
      "Training Epoch: 46 [15350/36045]\tLoss: 486.2114\n",
      "Training Epoch: 46 [15400/36045]\tLoss: 468.4697\n",
      "Training Epoch: 46 [15450/36045]\tLoss: 453.7040\n",
      "Training Epoch: 46 [15500/36045]\tLoss: 466.6762\n",
      "Training Epoch: 46 [15550/36045]\tLoss: 463.4939\n",
      "Training Epoch: 46 [15600/36045]\tLoss: 530.2148\n",
      "Training Epoch: 46 [15650/36045]\tLoss: 546.7056\n",
      "Training Epoch: 46 [15700/36045]\tLoss: 539.0227\n",
      "Training Epoch: 46 [15750/36045]\tLoss: 530.4409\n",
      "Training Epoch: 46 [15800/36045]\tLoss: 508.9048\n",
      "Training Epoch: 46 [15850/36045]\tLoss: 524.2945\n",
      "Training Epoch: 46 [15900/36045]\tLoss: 533.1191\n",
      "Training Epoch: 46 [15950/36045]\tLoss: 552.8656\n",
      "Training Epoch: 46 [16000/36045]\tLoss: 523.5849\n",
      "Training Epoch: 46 [16050/36045]\tLoss: 493.1560\n",
      "Training Epoch: 46 [16100/36045]\tLoss: 457.7478\n",
      "Training Epoch: 46 [16150/36045]\tLoss: 445.8537\n",
      "Training Epoch: 46 [16200/36045]\tLoss: 541.7449\n",
      "Training Epoch: 46 [16250/36045]\tLoss: 569.0648\n",
      "Training Epoch: 46 [16300/36045]\tLoss: 621.1739\n",
      "Training Epoch: 46 [16350/36045]\tLoss: 641.5320\n",
      "Training Epoch: 46 [16400/36045]\tLoss: 613.3955\n",
      "Training Epoch: 46 [16450/36045]\tLoss: 595.8405\n",
      "Training Epoch: 46 [16500/36045]\tLoss: 595.6959\n",
      "Training Epoch: 46 [16550/36045]\tLoss: 561.2054\n",
      "Training Epoch: 46 [16600/36045]\tLoss: 582.8597\n",
      "Training Epoch: 46 [16650/36045]\tLoss: 598.7234\n",
      "Training Epoch: 46 [16700/36045]\tLoss: 579.2172\n",
      "Training Epoch: 46 [16750/36045]\tLoss: 571.9669\n",
      "Training Epoch: 46 [16800/36045]\tLoss: 580.4269\n",
      "Training Epoch: 46 [16850/36045]\tLoss: 553.2253\n",
      "Training Epoch: 46 [16900/36045]\tLoss: 562.9739\n",
      "Training Epoch: 46 [16950/36045]\tLoss: 585.6073\n",
      "Training Epoch: 46 [17000/36045]\tLoss: 570.1928\n",
      "Training Epoch: 46 [17050/36045]\tLoss: 593.6794\n",
      "Training Epoch: 46 [17100/36045]\tLoss: 589.5252\n",
      "Training Epoch: 46 [17150/36045]\tLoss: 511.5703\n",
      "Training Epoch: 46 [17200/36045]\tLoss: 474.0704\n",
      "Training Epoch: 46 [17250/36045]\tLoss: 496.5998\n",
      "Training Epoch: 46 [17300/36045]\tLoss: 525.6563\n",
      "Training Epoch: 46 [17350/36045]\tLoss: 507.1745\n",
      "Training Epoch: 46 [17400/36045]\tLoss: 526.7391\n",
      "Training Epoch: 46 [17450/36045]\tLoss: 544.9526\n",
      "Training Epoch: 46 [17500/36045]\tLoss: 533.4474\n",
      "Training Epoch: 46 [17550/36045]\tLoss: 531.9566\n",
      "Training Epoch: 46 [17600/36045]\tLoss: 526.4650\n",
      "Training Epoch: 46 [17650/36045]\tLoss: 541.6782\n",
      "Training Epoch: 46 [17700/36045]\tLoss: 521.1761\n",
      "Training Epoch: 46 [17750/36045]\tLoss: 536.9102\n",
      "Training Epoch: 46 [17800/36045]\tLoss: 528.2607\n",
      "Training Epoch: 46 [17850/36045]\tLoss: 545.0598\n",
      "Training Epoch: 46 [17900/36045]\tLoss: 572.8553\n",
      "Training Epoch: 46 [17950/36045]\tLoss: 585.1364\n",
      "Training Epoch: 46 [18000/36045]\tLoss: 576.0039\n",
      "Training Epoch: 46 [18050/36045]\tLoss: 631.7042\n",
      "Training Epoch: 46 [18100/36045]\tLoss: 632.6187\n",
      "Training Epoch: 46 [18150/36045]\tLoss: 643.8260\n",
      "Training Epoch: 46 [18200/36045]\tLoss: 626.3077\n",
      "Training Epoch: 46 [18250/36045]\tLoss: 646.9314\n",
      "Training Epoch: 46 [18300/36045]\tLoss: 603.6317\n",
      "Training Epoch: 46 [18350/36045]\tLoss: 676.8995\n",
      "Training Epoch: 46 [18400/36045]\tLoss: 649.6402\n",
      "Training Epoch: 46 [18450/36045]\tLoss: 629.4460\n",
      "Training Epoch: 46 [18500/36045]\tLoss: 628.2635\n",
      "Training Epoch: 46 [18550/36045]\tLoss: 615.9474\n",
      "Training Epoch: 46 [18600/36045]\tLoss: 605.6843\n",
      "Training Epoch: 46 [18650/36045]\tLoss: 650.9079\n",
      "Training Epoch: 46 [18700/36045]\tLoss: 684.7331\n",
      "Training Epoch: 46 [18750/36045]\tLoss: 672.4327\n",
      "Training Epoch: 46 [18800/36045]\tLoss: 695.0391\n",
      "Training Epoch: 46 [18850/36045]\tLoss: 640.2654\n",
      "Training Epoch: 46 [18900/36045]\tLoss: 684.7777\n",
      "Training Epoch: 46 [18950/36045]\tLoss: 627.4883\n",
      "Training Epoch: 46 [19000/36045]\tLoss: 514.3146\n",
      "Training Epoch: 46 [19050/36045]\tLoss: 499.2059\n",
      "Training Epoch: 46 [19100/36045]\tLoss: 507.2140\n",
      "Training Epoch: 46 [19150/36045]\tLoss: 497.4709\n",
      "Training Epoch: 46 [19200/36045]\tLoss: 528.6822\n",
      "Training Epoch: 46 [19250/36045]\tLoss: 543.9399\n",
      "Training Epoch: 46 [19300/36045]\tLoss: 553.2100\n",
      "Training Epoch: 46 [19350/36045]\tLoss: 537.1052\n",
      "Training Epoch: 46 [19400/36045]\tLoss: 557.0462\n",
      "Training Epoch: 46 [19450/36045]\tLoss: 548.5177\n",
      "Training Epoch: 46 [19500/36045]\tLoss: 549.6206\n",
      "Training Epoch: 46 [19550/36045]\tLoss: 547.8062\n",
      "Training Epoch: 46 [19600/36045]\tLoss: 588.5834\n",
      "Training Epoch: 46 [19650/36045]\tLoss: 788.2498\n",
      "Training Epoch: 46 [19700/36045]\tLoss: 746.9580\n",
      "Training Epoch: 46 [19750/36045]\tLoss: 751.0774\n",
      "Training Epoch: 46 [19800/36045]\tLoss: 751.6163\n",
      "Training Epoch: 46 [19850/36045]\tLoss: 490.2311\n",
      "Training Epoch: 46 [19900/36045]\tLoss: 469.6300\n",
      "Training Epoch: 46 [19950/36045]\tLoss: 473.5776\n",
      "Training Epoch: 46 [20000/36045]\tLoss: 473.6214\n",
      "Training Epoch: 46 [20050/36045]\tLoss: 529.8865\n",
      "Training Epoch: 46 [20100/36045]\tLoss: 535.1686\n",
      "Training Epoch: 46 [20150/36045]\tLoss: 536.1418\n",
      "Training Epoch: 46 [20200/36045]\tLoss: 535.9390\n",
      "Training Epoch: 46 [20250/36045]\tLoss: 571.9824\n",
      "Training Epoch: 46 [20300/36045]\tLoss: 608.1020\n",
      "Training Epoch: 46 [20350/36045]\tLoss: 626.0767\n",
      "Training Epoch: 46 [20400/36045]\tLoss: 642.2885\n",
      "Training Epoch: 46 [20450/36045]\tLoss: 612.0680\n",
      "Training Epoch: 46 [20500/36045]\tLoss: 597.0743\n",
      "Training Epoch: 46 [20550/36045]\tLoss: 524.3369\n",
      "Training Epoch: 46 [20600/36045]\tLoss: 533.9756\n",
      "Training Epoch: 46 [20650/36045]\tLoss: 531.2625\n",
      "Training Epoch: 46 [20700/36045]\tLoss: 519.3530\n",
      "Training Epoch: 46 [20750/36045]\tLoss: 560.2656\n",
      "Training Epoch: 46 [20800/36045]\tLoss: 608.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 46 [20850/36045]\tLoss: 595.6776\n",
      "Training Epoch: 46 [20900/36045]\tLoss: 638.3578\n",
      "Training Epoch: 46 [20950/36045]\tLoss: 601.3610\n",
      "Training Epoch: 46 [21000/36045]\tLoss: 565.9874\n",
      "Training Epoch: 46 [21050/36045]\tLoss: 484.0592\n",
      "Training Epoch: 46 [21100/36045]\tLoss: 489.2335\n",
      "Training Epoch: 46 [21150/36045]\tLoss: 523.6255\n",
      "Training Epoch: 46 [21200/36045]\tLoss: 522.6594\n",
      "Training Epoch: 46 [21250/36045]\tLoss: 499.7937\n",
      "Training Epoch: 46 [21300/36045]\tLoss: 583.3556\n",
      "Training Epoch: 46 [21350/36045]\tLoss: 575.1545\n",
      "Training Epoch: 46 [21400/36045]\tLoss: 578.9641\n",
      "Training Epoch: 46 [21450/36045]\tLoss: 585.1677\n",
      "Training Epoch: 46 [21500/36045]\tLoss: 586.6622\n",
      "Training Epoch: 46 [21550/36045]\tLoss: 680.7231\n",
      "Training Epoch: 46 [21600/36045]\tLoss: 679.0776\n",
      "Training Epoch: 46 [21650/36045]\tLoss: 691.4076\n",
      "Training Epoch: 46 [21700/36045]\tLoss: 694.8832\n",
      "Training Epoch: 46 [21750/36045]\tLoss: 667.0295\n",
      "Training Epoch: 46 [21800/36045]\tLoss: 489.4609\n",
      "Training Epoch: 46 [21850/36045]\tLoss: 472.4102\n",
      "Training Epoch: 46 [21900/36045]\tLoss: 481.5942\n",
      "Training Epoch: 46 [21950/36045]\tLoss: 483.2120\n",
      "Training Epoch: 46 [22000/36045]\tLoss: 486.4570\n",
      "Training Epoch: 46 [22050/36045]\tLoss: 505.7737\n",
      "Training Epoch: 46 [22100/36045]\tLoss: 498.1826\n",
      "Training Epoch: 46 [22150/36045]\tLoss: 484.7311\n",
      "Training Epoch: 46 [22200/36045]\tLoss: 500.3351\n",
      "Training Epoch: 46 [22250/36045]\tLoss: 505.3449\n",
      "Training Epoch: 46 [22300/36045]\tLoss: 557.0921\n",
      "Training Epoch: 46 [22350/36045]\tLoss: 582.5043\n",
      "Training Epoch: 46 [22400/36045]\tLoss: 596.1283\n",
      "Training Epoch: 46 [22450/36045]\tLoss: 583.4892\n",
      "Training Epoch: 46 [22500/36045]\tLoss: 566.7830\n",
      "Training Epoch: 46 [22550/36045]\tLoss: 601.5143\n",
      "Training Epoch: 46 [22600/36045]\tLoss: 649.5278\n",
      "Training Epoch: 46 [22650/36045]\tLoss: 682.1814\n",
      "Training Epoch: 46 [22700/36045]\tLoss: 703.3810\n",
      "Training Epoch: 46 [22750/36045]\tLoss: 722.9744\n",
      "Training Epoch: 46 [22800/36045]\tLoss: 751.2714\n",
      "Training Epoch: 46 [22850/36045]\tLoss: 623.6339\n",
      "Training Epoch: 46 [22900/36045]\tLoss: 628.3103\n",
      "Training Epoch: 46 [22950/36045]\tLoss: 607.9307\n",
      "Training Epoch: 46 [23000/36045]\tLoss: 604.3828\n",
      "Training Epoch: 46 [23050/36045]\tLoss: 536.9841\n",
      "Training Epoch: 46 [23100/36045]\tLoss: 552.5265\n",
      "Training Epoch: 46 [23150/36045]\tLoss: 540.7897\n",
      "Training Epoch: 46 [23200/36045]\tLoss: 511.7607\n",
      "Training Epoch: 46 [23250/36045]\tLoss: 514.7536\n",
      "Training Epoch: 46 [23300/36045]\tLoss: 510.7469\n",
      "Training Epoch: 46 [23350/36045]\tLoss: 531.0054\n",
      "Training Epoch: 46 [23400/36045]\tLoss: 575.9474\n",
      "Training Epoch: 46 [23450/36045]\tLoss: 569.6238\n",
      "Training Epoch: 46 [23500/36045]\tLoss: 548.7831\n",
      "Training Epoch: 46 [23550/36045]\tLoss: 588.0867\n",
      "Training Epoch: 46 [23600/36045]\tLoss: 668.1027\n",
      "Training Epoch: 46 [23650/36045]\tLoss: 679.3130\n",
      "Training Epoch: 46 [23700/36045]\tLoss: 687.2576\n",
      "Training Epoch: 46 [23750/36045]\tLoss: 663.6332\n",
      "Training Epoch: 46 [23800/36045]\tLoss: 532.5980\n",
      "Training Epoch: 46 [23850/36045]\tLoss: 558.2418\n",
      "Training Epoch: 46 [23900/36045]\tLoss: 548.1761\n",
      "Training Epoch: 46 [23950/36045]\tLoss: 531.4235\n",
      "Training Epoch: 46 [24000/36045]\tLoss: 508.6603\n",
      "Training Epoch: 46 [24050/36045]\tLoss: 469.3597\n",
      "Training Epoch: 46 [24100/36045]\tLoss: 493.6566\n",
      "Training Epoch: 46 [24150/36045]\tLoss: 485.7026\n",
      "Training Epoch: 46 [24200/36045]\tLoss: 483.5151\n",
      "Training Epoch: 46 [24250/36045]\tLoss: 469.6770\n",
      "Training Epoch: 46 [24300/36045]\tLoss: 507.3148\n",
      "Training Epoch: 46 [24350/36045]\tLoss: 519.2596\n",
      "Training Epoch: 46 [24400/36045]\tLoss: 534.0731\n",
      "Training Epoch: 46 [24450/36045]\tLoss: 509.0041\n",
      "Training Epoch: 46 [24500/36045]\tLoss: 537.4600\n",
      "Training Epoch: 46 [24550/36045]\tLoss: 623.8594\n",
      "Training Epoch: 46 [24600/36045]\tLoss: 614.9887\n",
      "Training Epoch: 46 [24650/36045]\tLoss: 588.4311\n",
      "Training Epoch: 46 [24700/36045]\tLoss: 598.3722\n",
      "Training Epoch: 46 [24750/36045]\tLoss: 553.2873\n",
      "Training Epoch: 46 [24800/36045]\tLoss: 452.7171\n",
      "Training Epoch: 46 [24850/36045]\tLoss: 470.6783\n",
      "Training Epoch: 46 [24900/36045]\tLoss: 468.1605\n",
      "Training Epoch: 46 [24950/36045]\tLoss: 470.6522\n",
      "Training Epoch: 46 [25000/36045]\tLoss: 452.7067\n",
      "Training Epoch: 46 [25050/36045]\tLoss: 432.8908\n",
      "Training Epoch: 46 [25100/36045]\tLoss: 387.7660\n",
      "Training Epoch: 46 [25150/36045]\tLoss: 358.8915\n",
      "Training Epoch: 46 [25200/36045]\tLoss: 353.8239\n",
      "Training Epoch: 46 [25250/36045]\tLoss: 379.4850\n",
      "Training Epoch: 46 [25300/36045]\tLoss: 498.7697\n",
      "Training Epoch: 46 [25350/36045]\tLoss: 495.0013\n",
      "Training Epoch: 46 [25400/36045]\tLoss: 462.7349\n",
      "Training Epoch: 46 [25450/36045]\tLoss: 464.6322\n",
      "Training Epoch: 46 [25500/36045]\tLoss: 504.6665\n",
      "Training Epoch: 46 [25550/36045]\tLoss: 590.3463\n",
      "Training Epoch: 46 [25600/36045]\tLoss: 594.6601\n",
      "Training Epoch: 46 [25650/36045]\tLoss: 574.0970\n",
      "Training Epoch: 46 [25700/36045]\tLoss: 583.6508\n",
      "Training Epoch: 46 [25750/36045]\tLoss: 563.9854\n",
      "Training Epoch: 46 [25800/36045]\tLoss: 355.6137\n",
      "Training Epoch: 46 [25850/36045]\tLoss: 363.3090\n",
      "Training Epoch: 46 [25900/36045]\tLoss: 345.2930\n",
      "Training Epoch: 46 [25950/36045]\tLoss: 355.1154\n",
      "Training Epoch: 46 [26000/36045]\tLoss: 437.0434\n",
      "Training Epoch: 46 [26050/36045]\tLoss: 594.1883\n",
      "Training Epoch: 46 [26100/36045]\tLoss: 619.6167\n",
      "Training Epoch: 46 [26150/36045]\tLoss: 618.9375\n",
      "Training Epoch: 46 [26200/36045]\tLoss: 592.8873\n",
      "Training Epoch: 46 [26250/36045]\tLoss: 623.1572\n",
      "Training Epoch: 46 [26300/36045]\tLoss: 573.4230\n",
      "Training Epoch: 46 [26350/36045]\tLoss: 583.8437\n",
      "Training Epoch: 46 [26400/36045]\tLoss: 558.7468\n",
      "Training Epoch: 46 [26450/36045]\tLoss: 489.3953\n",
      "Training Epoch: 46 [26500/36045]\tLoss: 579.6667\n",
      "Training Epoch: 46 [26550/36045]\tLoss: 579.0364\n",
      "Training Epoch: 46 [26600/36045]\tLoss: 575.0407\n",
      "Training Epoch: 46 [26650/36045]\tLoss: 589.5086\n",
      "Training Epoch: 46 [26700/36045]\tLoss: 568.8052\n",
      "Training Epoch: 46 [26750/36045]\tLoss: 533.9167\n",
      "Training Epoch: 46 [26800/36045]\tLoss: 394.7617\n",
      "Training Epoch: 46 [26850/36045]\tLoss: 326.5944\n",
      "Training Epoch: 46 [26900/36045]\tLoss: 328.1821\n",
      "Training Epoch: 46 [26950/36045]\tLoss: 360.0155\n",
      "Training Epoch: 46 [27000/36045]\tLoss: 594.6605\n",
      "Training Epoch: 46 [27050/36045]\tLoss: 620.8220\n",
      "Training Epoch: 46 [27100/36045]\tLoss: 601.7979\n",
      "Training Epoch: 46 [27150/36045]\tLoss: 640.2729\n",
      "Training Epoch: 46 [27200/36045]\tLoss: 463.7550\n",
      "Training Epoch: 46 [27250/36045]\tLoss: 454.2214\n",
      "Training Epoch: 46 [27300/36045]\tLoss: 443.1261\n",
      "Training Epoch: 46 [27350/36045]\tLoss: 440.2815\n",
      "Training Epoch: 46 [27400/36045]\tLoss: 439.6902\n",
      "Training Epoch: 46 [27450/36045]\tLoss: 556.9264\n",
      "Training Epoch: 46 [27500/36045]\tLoss: 596.8074\n",
      "Training Epoch: 46 [27550/36045]\tLoss: 590.1398\n",
      "Training Epoch: 46 [27600/36045]\tLoss: 601.5582\n",
      "Training Epoch: 46 [27650/36045]\tLoss: 593.5900\n",
      "Training Epoch: 46 [27700/36045]\tLoss: 620.6068\n",
      "Training Epoch: 46 [27750/36045]\tLoss: 632.4390\n",
      "Training Epoch: 46 [27800/36045]\tLoss: 619.8644\n",
      "Training Epoch: 46 [27850/36045]\tLoss: 610.2847\n",
      "Training Epoch: 46 [27900/36045]\tLoss: 555.4777\n",
      "Training Epoch: 46 [27950/36045]\tLoss: 465.1633\n",
      "Training Epoch: 46 [28000/36045]\tLoss: 442.0556\n",
      "Training Epoch: 46 [28050/36045]\tLoss: 451.4391\n",
      "Training Epoch: 46 [28100/36045]\tLoss: 443.1483\n",
      "Training Epoch: 46 [28150/36045]\tLoss: 461.6967\n",
      "Training Epoch: 46 [28200/36045]\tLoss: 470.2585\n",
      "Training Epoch: 46 [28250/36045]\tLoss: 463.0034\n",
      "Training Epoch: 46 [28300/36045]\tLoss: 439.5417\n",
      "Training Epoch: 46 [28350/36045]\tLoss: 435.3214\n",
      "Training Epoch: 46 [28400/36045]\tLoss: 762.3114\n",
      "Training Epoch: 46 [28450/36045]\tLoss: 700.1133\n",
      "Training Epoch: 46 [28500/36045]\tLoss: 604.9271\n",
      "Training Epoch: 46 [28550/36045]\tLoss: 556.3325\n",
      "Training Epoch: 46 [28600/36045]\tLoss: 580.0038\n",
      "Training Epoch: 46 [28650/36045]\tLoss: 632.2106\n",
      "Training Epoch: 46 [28700/36045]\tLoss: 625.9530\n",
      "Training Epoch: 46 [28750/36045]\tLoss: 613.0647\n",
      "Training Epoch: 46 [28800/36045]\tLoss: 623.1466\n",
      "Training Epoch: 46 [28850/36045]\tLoss: 541.3915\n",
      "Training Epoch: 46 [28900/36045]\tLoss: 440.8220\n",
      "Training Epoch: 46 [28950/36045]\tLoss: 439.9944\n",
      "Training Epoch: 46 [29000/36045]\tLoss: 435.8726\n",
      "Training Epoch: 46 [29050/36045]\tLoss: 442.9212\n",
      "Training Epoch: 46 [29100/36045]\tLoss: 460.7579\n",
      "Training Epoch: 46 [29150/36045]\tLoss: 451.3825\n",
      "Training Epoch: 46 [29200/36045]\tLoss: 437.1680\n",
      "Training Epoch: 46 [29250/36045]\tLoss: 427.0948\n",
      "Training Epoch: 46 [29300/36045]\tLoss: 479.6644\n",
      "Training Epoch: 46 [29350/36045]\tLoss: 562.7391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 46 [29400/36045]\tLoss: 580.2926\n",
      "Training Epoch: 46 [29450/36045]\tLoss: 596.7253\n",
      "Training Epoch: 46 [29500/36045]\tLoss: 611.5477\n",
      "Training Epoch: 46 [29550/36045]\tLoss: 581.5656\n",
      "Training Epoch: 46 [29600/36045]\tLoss: 488.9424\n",
      "Training Epoch: 46 [29650/36045]\tLoss: 470.4954\n",
      "Training Epoch: 46 [29700/36045]\tLoss: 422.9215\n",
      "Training Epoch: 46 [29750/36045]\tLoss: 421.2707\n",
      "Training Epoch: 46 [29800/36045]\tLoss: 468.2500\n",
      "Training Epoch: 46 [29850/36045]\tLoss: 545.7155\n",
      "Training Epoch: 46 [29900/36045]\tLoss: 541.1449\n",
      "Training Epoch: 46 [29950/36045]\tLoss: 562.9631\n",
      "Training Epoch: 46 [30000/36045]\tLoss: 536.8923\n",
      "Training Epoch: 46 [30050/36045]\tLoss: 544.2245\n",
      "Training Epoch: 46 [30100/36045]\tLoss: 663.7183\n",
      "Training Epoch: 46 [30150/36045]\tLoss: 645.8266\n",
      "Training Epoch: 46 [30200/36045]\tLoss: 608.9817\n",
      "Training Epoch: 46 [30250/36045]\tLoss: 657.4752\n",
      "Training Epoch: 46 [30300/36045]\tLoss: 641.6876\n",
      "Training Epoch: 46 [30350/36045]\tLoss: 488.5367\n",
      "Training Epoch: 46 [30400/36045]\tLoss: 473.2232\n",
      "Training Epoch: 46 [30450/36045]\tLoss: 475.5816\n",
      "Training Epoch: 46 [30500/36045]\tLoss: 443.8388\n",
      "Training Epoch: 46 [30550/36045]\tLoss: 411.9451\n",
      "Training Epoch: 46 [30600/36045]\tLoss: 404.9073\n",
      "Training Epoch: 46 [30650/36045]\tLoss: 395.0677\n",
      "Training Epoch: 46 [30700/36045]\tLoss: 412.6871\n",
      "Training Epoch: 46 [30750/36045]\tLoss: 399.6813\n",
      "Training Epoch: 46 [30800/36045]\tLoss: 425.7718\n",
      "Training Epoch: 46 [30850/36045]\tLoss: 417.5747\n",
      "Training Epoch: 46 [30900/36045]\tLoss: 429.2656\n",
      "Training Epoch: 46 [30950/36045]\tLoss: 451.1838\n",
      "Training Epoch: 46 [31000/36045]\tLoss: 443.6464\n",
      "Training Epoch: 46 [31050/36045]\tLoss: 370.2997\n",
      "Training Epoch: 46 [31100/36045]\tLoss: 361.0332\n",
      "Training Epoch: 46 [31150/36045]\tLoss: 368.7298\n",
      "Training Epoch: 46 [31200/36045]\tLoss: 457.6734\n",
      "Training Epoch: 46 [31250/36045]\tLoss: 593.8464\n",
      "Training Epoch: 46 [31300/36045]\tLoss: 565.1559\n",
      "Training Epoch: 46 [31350/36045]\tLoss: 581.2096\n",
      "Training Epoch: 46 [31400/36045]\tLoss: 560.7964\n",
      "Training Epoch: 46 [31450/36045]\tLoss: 577.5688\n",
      "Training Epoch: 46 [31500/36045]\tLoss: 590.0627\n",
      "Training Epoch: 46 [31550/36045]\tLoss: 596.6884\n",
      "Training Epoch: 46 [31600/36045]\tLoss: 560.9786\n",
      "Training Epoch: 46 [31650/36045]\tLoss: 600.8047\n",
      "Training Epoch: 46 [31700/36045]\tLoss: 434.3337\n",
      "Training Epoch: 46 [31750/36045]\tLoss: 358.7654\n",
      "Training Epoch: 46 [31800/36045]\tLoss: 342.4450\n",
      "Training Epoch: 46 [31850/36045]\tLoss: 350.0250\n",
      "Training Epoch: 46 [31900/36045]\tLoss: 554.8844\n",
      "Training Epoch: 46 [31950/36045]\tLoss: 719.6860\n",
      "Training Epoch: 46 [32000/36045]\tLoss: 826.7844\n",
      "Training Epoch: 46 [32050/36045]\tLoss: 782.6768\n",
      "Training Epoch: 46 [32100/36045]\tLoss: 774.0976\n",
      "Training Epoch: 46 [32150/36045]\tLoss: 592.4589\n",
      "Training Epoch: 46 [32200/36045]\tLoss: 594.2188\n",
      "Training Epoch: 46 [32250/36045]\tLoss: 604.3835\n",
      "Training Epoch: 46 [32300/36045]\tLoss: 586.5798\n",
      "Training Epoch: 46 [32350/36045]\tLoss: 582.6939\n",
      "Training Epoch: 46 [32400/36045]\tLoss: 546.7476\n",
      "Training Epoch: 46 [32450/36045]\tLoss: 450.2112\n",
      "Training Epoch: 46 [32500/36045]\tLoss: 432.5320\n",
      "Training Epoch: 46 [32550/36045]\tLoss: 434.6057\n",
      "Training Epoch: 46 [32600/36045]\tLoss: 431.7597\n",
      "Training Epoch: 46 [32650/36045]\tLoss: 560.6216\n",
      "Training Epoch: 46 [32700/36045]\tLoss: 612.4602\n",
      "Training Epoch: 46 [32750/36045]\tLoss: 583.0975\n",
      "Training Epoch: 46 [32800/36045]\tLoss: 597.7861\n",
      "Training Epoch: 46 [32850/36045]\tLoss: 551.8246\n",
      "Training Epoch: 46 [32900/36045]\tLoss: 440.6465\n",
      "Training Epoch: 46 [32950/36045]\tLoss: 461.6372\n",
      "Training Epoch: 46 [33000/36045]\tLoss: 460.1745\n",
      "Training Epoch: 46 [33050/36045]\tLoss: 437.9596\n",
      "Training Epoch: 46 [33100/36045]\tLoss: 497.6956\n",
      "Training Epoch: 46 [33150/36045]\tLoss: 677.8452\n",
      "Training Epoch: 46 [33200/36045]\tLoss: 659.8956\n",
      "Training Epoch: 46 [33250/36045]\tLoss: 679.9607\n",
      "Training Epoch: 46 [33300/36045]\tLoss: 724.8535\n",
      "Training Epoch: 46 [33350/36045]\tLoss: 554.7167\n",
      "Training Epoch: 46 [33400/36045]\tLoss: 403.3883\n",
      "Training Epoch: 46 [33450/36045]\tLoss: 399.4507\n",
      "Training Epoch: 46 [33500/36045]\tLoss: 411.1729\n",
      "Training Epoch: 46 [33550/36045]\tLoss: 425.6611\n",
      "Training Epoch: 46 [33600/36045]\tLoss: 427.4481\n",
      "Training Epoch: 46 [33650/36045]\tLoss: 571.2955\n",
      "Training Epoch: 46 [33700/36045]\tLoss: 552.6113\n",
      "Training Epoch: 46 [33750/36045]\tLoss: 572.3004\n",
      "Training Epoch: 46 [33800/36045]\tLoss: 569.7740\n",
      "Training Epoch: 46 [33850/36045]\tLoss: 571.4752\n",
      "Training Epoch: 46 [33900/36045]\tLoss: 585.0564\n",
      "Training Epoch: 46 [33950/36045]\tLoss: 594.5099\n",
      "Training Epoch: 46 [34000/36045]\tLoss: 581.2092\n",
      "Training Epoch: 46 [34050/36045]\tLoss: 584.9209\n",
      "Training Epoch: 46 [34100/36045]\tLoss: 564.1182\n",
      "Training Epoch: 46 [34150/36045]\tLoss: 522.3297\n",
      "Training Epoch: 46 [34200/36045]\tLoss: 494.7727\n",
      "Training Epoch: 46 [34250/36045]\tLoss: 508.5715\n",
      "Training Epoch: 46 [34300/36045]\tLoss: 433.9164\n",
      "Training Epoch: 46 [34350/36045]\tLoss: 457.1128\n",
      "Training Epoch: 46 [34400/36045]\tLoss: 450.3636\n",
      "Training Epoch: 46 [34450/36045]\tLoss: 424.1443\n",
      "Training Epoch: 46 [34500/36045]\tLoss: 452.3643\n",
      "Training Epoch: 46 [34550/36045]\tLoss: 444.0675\n",
      "Training Epoch: 46 [34600/36045]\tLoss: 451.2744\n",
      "Training Epoch: 46 [34650/36045]\tLoss: 556.8738\n",
      "Training Epoch: 46 [34700/36045]\tLoss: 590.7791\n",
      "Training Epoch: 46 [34750/36045]\tLoss: 523.5211\n",
      "Training Epoch: 46 [34800/36045]\tLoss: 601.4702\n",
      "Training Epoch: 46 [34850/36045]\tLoss: 608.4019\n",
      "Training Epoch: 46 [34900/36045]\tLoss: 654.6337\n",
      "Training Epoch: 46 [34950/36045]\tLoss: 639.2405\n",
      "Training Epoch: 46 [35000/36045]\tLoss: 640.4684\n",
      "Training Epoch: 46 [35050/36045]\tLoss: 627.9310\n",
      "Training Epoch: 46 [35100/36045]\tLoss: 543.4299\n",
      "Training Epoch: 46 [35150/36045]\tLoss: 535.3734\n",
      "Training Epoch: 46 [35200/36045]\tLoss: 449.7383\n",
      "Training Epoch: 46 [35250/36045]\tLoss: 494.4379\n",
      "Training Epoch: 46 [35300/36045]\tLoss: 510.8731\n",
      "Training Epoch: 46 [35350/36045]\tLoss: 570.1642\n",
      "Training Epoch: 46 [35400/36045]\tLoss: 599.2292\n",
      "Training Epoch: 46 [35450/36045]\tLoss: 570.6490\n",
      "Training Epoch: 46 [35500/36045]\tLoss: 551.9437\n",
      "Training Epoch: 46 [35550/36045]\tLoss: 538.6467\n",
      "Training Epoch: 46 [35600/36045]\tLoss: 589.6752\n",
      "Training Epoch: 46 [35650/36045]\tLoss: 661.7454\n",
      "Training Epoch: 46 [35700/36045]\tLoss: 588.3886\n",
      "Training Epoch: 46 [35750/36045]\tLoss: 645.6401\n",
      "Training Epoch: 46 [35800/36045]\tLoss: 652.4841\n",
      "Training Epoch: 46 [35850/36045]\tLoss: 626.7226\n",
      "Training Epoch: 46 [35900/36045]\tLoss: 648.2526\n",
      "Training Epoch: 46 [35950/36045]\tLoss: 644.7231\n",
      "Training Epoch: 46 [36000/36045]\tLoss: 638.0042\n",
      "Training Epoch: 46 [36045/36045]\tLoss: 623.8033\n",
      "Training Epoch: 46 [4004/4004]\tLoss: 579.1848\n",
      "Training Epoch: 47 [50/36045]\tLoss: 577.5428\n",
      "Training Epoch: 47 [100/36045]\tLoss: 553.2196\n",
      "Training Epoch: 47 [150/36045]\tLoss: 550.9734\n",
      "Training Epoch: 47 [200/36045]\tLoss: 537.2310\n",
      "Training Epoch: 47 [250/36045]\tLoss: 646.6592\n",
      "Training Epoch: 47 [300/36045]\tLoss: 712.0049\n",
      "Training Epoch: 47 [350/36045]\tLoss: 678.9306\n",
      "Training Epoch: 47 [400/36045]\tLoss: 673.0724\n",
      "Training Epoch: 47 [450/36045]\tLoss: 653.8367\n",
      "Training Epoch: 47 [500/36045]\tLoss: 604.7828\n",
      "Training Epoch: 47 [550/36045]\tLoss: 607.7851\n",
      "Training Epoch: 47 [600/36045]\tLoss: 594.1056\n",
      "Training Epoch: 47 [650/36045]\tLoss: 615.0969\n",
      "Training Epoch: 47 [700/36045]\tLoss: 600.1139\n",
      "Training Epoch: 47 [750/36045]\tLoss: 575.9868\n",
      "Training Epoch: 47 [800/36045]\tLoss: 587.5671\n",
      "Training Epoch: 47 [850/36045]\tLoss: 571.1367\n",
      "Training Epoch: 47 [900/36045]\tLoss: 547.1636\n",
      "Training Epoch: 47 [950/36045]\tLoss: 517.3076\n",
      "Training Epoch: 47 [1000/36045]\tLoss: 501.4704\n",
      "Training Epoch: 47 [1050/36045]\tLoss: 503.4882\n",
      "Training Epoch: 47 [1100/36045]\tLoss: 490.0344\n",
      "Training Epoch: 47 [1150/36045]\tLoss: 499.5156\n",
      "Training Epoch: 47 [1200/36045]\tLoss: 529.0150\n",
      "Training Epoch: 47 [1250/36045]\tLoss: 605.0584\n",
      "Training Epoch: 47 [1300/36045]\tLoss: 611.9761\n",
      "Training Epoch: 47 [1350/36045]\tLoss: 613.1871\n",
      "Training Epoch: 47 [1400/36045]\tLoss: 636.8024\n",
      "Training Epoch: 47 [1450/36045]\tLoss: 616.3251\n",
      "Training Epoch: 47 [1500/36045]\tLoss: 562.9888\n",
      "Training Epoch: 47 [1550/36045]\tLoss: 577.3865\n",
      "Training Epoch: 47 [1600/36045]\tLoss: 587.1956\n",
      "Training Epoch: 47 [1650/36045]\tLoss: 574.6808\n",
      "Training Epoch: 47 [1700/36045]\tLoss: 586.9745\n",
      "Training Epoch: 47 [1750/36045]\tLoss: 628.5366\n",
      "Training Epoch: 47 [1800/36045]\tLoss: 611.4413\n",
      "Training Epoch: 47 [1850/36045]\tLoss: 627.0300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 47 [1900/36045]\tLoss: 586.2188\n",
      "Training Epoch: 47 [1950/36045]\tLoss: 595.9194\n",
      "Training Epoch: 47 [2000/36045]\tLoss: 536.4529\n",
      "Training Epoch: 47 [2050/36045]\tLoss: 539.2393\n",
      "Training Epoch: 47 [2100/36045]\tLoss: 568.4262\n",
      "Training Epoch: 47 [2150/36045]\tLoss: 549.4214\n",
      "Training Epoch: 47 [2200/36045]\tLoss: 512.0430\n",
      "Training Epoch: 47 [2250/36045]\tLoss: 483.8658\n",
      "Training Epoch: 47 [2300/36045]\tLoss: 507.4154\n",
      "Training Epoch: 47 [2350/36045]\tLoss: 485.1558\n",
      "Training Epoch: 47 [2400/36045]\tLoss: 492.5993\n",
      "Training Epoch: 47 [2450/36045]\tLoss: 631.1313\n",
      "Training Epoch: 47 [2500/36045]\tLoss: 662.6849\n",
      "Training Epoch: 47 [2550/36045]\tLoss: 660.6185\n",
      "Training Epoch: 47 [2600/36045]\tLoss: 669.3383\n",
      "Training Epoch: 47 [2650/36045]\tLoss: 793.4368\n",
      "Training Epoch: 47 [2700/36045]\tLoss: 881.3464\n",
      "Training Epoch: 47 [2750/36045]\tLoss: 952.0796\n",
      "Training Epoch: 47 [2800/36045]\tLoss: 961.3588\n",
      "Training Epoch: 47 [2850/36045]\tLoss: 728.4082\n",
      "Training Epoch: 47 [2900/36045]\tLoss: 689.1826\n",
      "Training Epoch: 47 [2950/36045]\tLoss: 666.8275\n",
      "Training Epoch: 47 [3000/36045]\tLoss: 660.6407\n",
      "Training Epoch: 47 [3050/36045]\tLoss: 691.3177\n",
      "Training Epoch: 47 [3100/36045]\tLoss: 631.5717\n",
      "Training Epoch: 47 [3150/36045]\tLoss: 485.4855\n",
      "Training Epoch: 47 [3200/36045]\tLoss: 502.6552\n",
      "Training Epoch: 47 [3250/36045]\tLoss: 474.4595\n",
      "Training Epoch: 47 [3300/36045]\tLoss: 449.3327\n",
      "Training Epoch: 47 [3350/36045]\tLoss: 474.6485\n",
      "Training Epoch: 47 [3400/36045]\tLoss: 497.2090\n",
      "Training Epoch: 47 [3450/36045]\tLoss: 533.2132\n",
      "Training Epoch: 47 [3500/36045]\tLoss: 520.6022\n",
      "Training Epoch: 47 [3550/36045]\tLoss: 497.8282\n",
      "Training Epoch: 47 [3600/36045]\tLoss: 535.1226\n",
      "Training Epoch: 47 [3650/36045]\tLoss: 618.9075\n",
      "Training Epoch: 47 [3700/36045]\tLoss: 626.6367\n",
      "Training Epoch: 47 [3750/36045]\tLoss: 595.8669\n",
      "Training Epoch: 47 [3800/36045]\tLoss: 593.0654\n",
      "Training Epoch: 47 [3850/36045]\tLoss: 595.0512\n",
      "Training Epoch: 47 [3900/36045]\tLoss: 599.6379\n",
      "Training Epoch: 47 [3950/36045]\tLoss: 578.7760\n",
      "Training Epoch: 47 [4000/36045]\tLoss: 583.3022\n",
      "Training Epoch: 47 [4050/36045]\tLoss: 534.6961\n",
      "Training Epoch: 47 [4100/36045]\tLoss: 521.5046\n",
      "Training Epoch: 47 [4150/36045]\tLoss: 535.4339\n",
      "Training Epoch: 47 [4200/36045]\tLoss: 530.4446\n",
      "Training Epoch: 47 [4250/36045]\tLoss: 532.6080\n",
      "Training Epoch: 47 [4300/36045]\tLoss: 547.6271\n",
      "Training Epoch: 47 [4350/36045]\tLoss: 531.5624\n",
      "Training Epoch: 47 [4400/36045]\tLoss: 508.3442\n",
      "Training Epoch: 47 [4450/36045]\tLoss: 558.6681\n",
      "Training Epoch: 47 [4500/36045]\tLoss: 601.5259\n",
      "Training Epoch: 47 [4550/36045]\tLoss: 604.2107\n",
      "Training Epoch: 47 [4600/36045]\tLoss: 625.9697\n",
      "Training Epoch: 47 [4650/36045]\tLoss: 616.0289\n",
      "Training Epoch: 47 [4700/36045]\tLoss: 567.9206\n",
      "Training Epoch: 47 [4750/36045]\tLoss: 549.8654\n",
      "Training Epoch: 47 [4800/36045]\tLoss: 573.9069\n",
      "Training Epoch: 47 [4850/36045]\tLoss: 560.4532\n",
      "Training Epoch: 47 [4900/36045]\tLoss: 545.6368\n",
      "Training Epoch: 47 [4950/36045]\tLoss: 561.0316\n",
      "Training Epoch: 47 [5000/36045]\tLoss: 590.4012\n",
      "Training Epoch: 47 [5050/36045]\tLoss: 572.0056\n",
      "Training Epoch: 47 [5100/36045]\tLoss: 582.0593\n",
      "Training Epoch: 47 [5150/36045]\tLoss: 566.4943\n",
      "Training Epoch: 47 [5200/36045]\tLoss: 564.4457\n",
      "Training Epoch: 47 [5250/36045]\tLoss: 558.1217\n",
      "Training Epoch: 47 [5300/36045]\tLoss: 558.1603\n",
      "Training Epoch: 47 [5350/36045]\tLoss: 579.0936\n",
      "Training Epoch: 47 [5400/36045]\tLoss: 558.1993\n",
      "Training Epoch: 47 [5450/36045]\tLoss: 528.8474\n",
      "Training Epoch: 47 [5500/36045]\tLoss: 556.9711\n",
      "Training Epoch: 47 [5550/36045]\tLoss: 545.2359\n",
      "Training Epoch: 47 [5600/36045]\tLoss: 623.5466\n",
      "Training Epoch: 47 [5650/36045]\tLoss: 589.4501\n",
      "Training Epoch: 47 [5700/36045]\tLoss: 552.9664\n",
      "Training Epoch: 47 [5750/36045]\tLoss: 537.0658\n",
      "Training Epoch: 47 [5800/36045]\tLoss: 566.6490\n",
      "Training Epoch: 47 [5850/36045]\tLoss: 556.3912\n",
      "Training Epoch: 47 [5900/36045]\tLoss: 639.1448\n",
      "Training Epoch: 47 [5950/36045]\tLoss: 655.5120\n",
      "Training Epoch: 47 [6000/36045]\tLoss: 641.6138\n",
      "Training Epoch: 47 [6050/36045]\tLoss: 619.8748\n",
      "Training Epoch: 47 [6100/36045]\tLoss: 624.1415\n",
      "Training Epoch: 47 [6150/36045]\tLoss: 615.2151\n",
      "Training Epoch: 47 [6200/36045]\tLoss: 620.0703\n",
      "Training Epoch: 47 [6250/36045]\tLoss: 641.5662\n",
      "Training Epoch: 47 [6300/36045]\tLoss: 653.1063\n",
      "Training Epoch: 47 [6350/36045]\tLoss: 698.2003\n",
      "Training Epoch: 47 [6400/36045]\tLoss: 574.7834\n",
      "Training Epoch: 47 [6450/36045]\tLoss: 528.2653\n",
      "Training Epoch: 47 [6500/36045]\tLoss: 537.8624\n",
      "Training Epoch: 47 [6550/36045]\tLoss: 555.2936\n",
      "Training Epoch: 47 [6600/36045]\tLoss: 553.1814\n",
      "Training Epoch: 47 [6650/36045]\tLoss: 624.7927\n",
      "Training Epoch: 47 [6700/36045]\tLoss: 653.3508\n",
      "Training Epoch: 47 [6750/36045]\tLoss: 630.9547\n",
      "Training Epoch: 47 [6800/36045]\tLoss: 633.9388\n",
      "Training Epoch: 47 [6850/36045]\tLoss: 622.0529\n",
      "Training Epoch: 47 [6900/36045]\tLoss: 554.1224\n",
      "Training Epoch: 47 [6950/36045]\tLoss: 522.4692\n",
      "Training Epoch: 47 [7000/36045]\tLoss: 555.6424\n",
      "Training Epoch: 47 [7050/36045]\tLoss: 567.7769\n",
      "Training Epoch: 47 [7100/36045]\tLoss: 566.8615\n",
      "Training Epoch: 47 [7150/36045]\tLoss: 576.8082\n",
      "Training Epoch: 47 [7200/36045]\tLoss: 578.8677\n",
      "Training Epoch: 47 [7250/36045]\tLoss: 577.2283\n",
      "Training Epoch: 47 [7300/36045]\tLoss: 563.5666\n",
      "Training Epoch: 47 [7350/36045]\tLoss: 561.3790\n",
      "Training Epoch: 47 [7400/36045]\tLoss: 512.0720\n",
      "Training Epoch: 47 [7450/36045]\tLoss: 515.3080\n",
      "Training Epoch: 47 [7500/36045]\tLoss: 510.9702\n",
      "Training Epoch: 47 [7550/36045]\tLoss: 489.3866\n",
      "Training Epoch: 47 [7600/36045]\tLoss: 542.2429\n",
      "Training Epoch: 47 [7650/36045]\tLoss: 579.4627\n",
      "Training Epoch: 47 [7700/36045]\tLoss: 551.4357\n",
      "Training Epoch: 47 [7750/36045]\tLoss: 565.7792\n",
      "Training Epoch: 47 [7800/36045]\tLoss: 555.4477\n",
      "Training Epoch: 47 [7850/36045]\tLoss: 537.5772\n",
      "Training Epoch: 47 [7900/36045]\tLoss: 566.8887\n",
      "Training Epoch: 47 [7950/36045]\tLoss: 564.4856\n",
      "Training Epoch: 47 [8000/36045]\tLoss: 582.5990\n",
      "Training Epoch: 47 [8050/36045]\tLoss: 548.4779\n",
      "Training Epoch: 47 [8100/36045]\tLoss: 573.4761\n",
      "Training Epoch: 47 [8150/36045]\tLoss: 650.7052\n",
      "Training Epoch: 47 [8200/36045]\tLoss: 638.2642\n",
      "Training Epoch: 47 [8250/36045]\tLoss: 606.7213\n",
      "Training Epoch: 47 [8300/36045]\tLoss: 663.3205\n",
      "Training Epoch: 47 [8350/36045]\tLoss: 607.8744\n",
      "Training Epoch: 47 [8400/36045]\tLoss: 543.5639\n",
      "Training Epoch: 47 [8450/36045]\tLoss: 508.6506\n",
      "Training Epoch: 47 [8500/36045]\tLoss: 541.3977\n",
      "Training Epoch: 47 [8550/36045]\tLoss: 535.0475\n",
      "Training Epoch: 47 [8600/36045]\tLoss: 529.4087\n",
      "Training Epoch: 47 [8650/36045]\tLoss: 561.5196\n",
      "Training Epoch: 47 [8700/36045]\tLoss: 593.9225\n",
      "Training Epoch: 47 [8750/36045]\tLoss: 583.5278\n",
      "Training Epoch: 47 [8800/36045]\tLoss: 589.2432\n",
      "Training Epoch: 47 [8850/36045]\tLoss: 582.8840\n",
      "Training Epoch: 47 [8900/36045]\tLoss: 526.2252\n",
      "Training Epoch: 47 [8950/36045]\tLoss: 536.8037\n",
      "Training Epoch: 47 [9000/36045]\tLoss: 552.7031\n",
      "Training Epoch: 47 [9050/36045]\tLoss: 554.3075\n",
      "Training Epoch: 47 [9100/36045]\tLoss: 570.9487\n",
      "Training Epoch: 47 [9150/36045]\tLoss: 422.6130\n",
      "Training Epoch: 47 [9200/36045]\tLoss: 315.6477\n",
      "Training Epoch: 47 [9250/36045]\tLoss: 342.5770\n",
      "Training Epoch: 47 [9300/36045]\tLoss: 352.1615\n",
      "Training Epoch: 47 [9350/36045]\tLoss: 325.0840\n",
      "Training Epoch: 47 [9400/36045]\tLoss: 636.5446\n",
      "Training Epoch: 47 [9450/36045]\tLoss: 675.9816\n",
      "Training Epoch: 47 [9500/36045]\tLoss: 663.6733\n",
      "Training Epoch: 47 [9550/36045]\tLoss: 702.1180\n",
      "Training Epoch: 47 [9600/36045]\tLoss: 523.4516\n",
      "Training Epoch: 47 [9650/36045]\tLoss: 528.5743\n",
      "Training Epoch: 47 [9700/36045]\tLoss: 514.1578\n",
      "Training Epoch: 47 [9750/36045]\tLoss: 512.7646\n",
      "Training Epoch: 47 [9800/36045]\tLoss: 670.6980\n",
      "Training Epoch: 47 [9850/36045]\tLoss: 708.2731\n",
      "Training Epoch: 47 [9900/36045]\tLoss: 717.3107\n",
      "Training Epoch: 47 [9950/36045]\tLoss: 699.0157\n",
      "Training Epoch: 47 [10000/36045]\tLoss: 647.0968\n",
      "Training Epoch: 47 [10050/36045]\tLoss: 529.6176\n",
      "Training Epoch: 47 [10100/36045]\tLoss: 537.2452\n",
      "Training Epoch: 47 [10150/36045]\tLoss: 545.2942\n",
      "Training Epoch: 47 [10200/36045]\tLoss: 534.3024\n",
      "Training Epoch: 47 [10250/36045]\tLoss: 642.7264\n",
      "Training Epoch: 47 [10300/36045]\tLoss: 624.6957\n",
      "Training Epoch: 47 [10350/36045]\tLoss: 658.0186\n",
      "Training Epoch: 47 [10400/36045]\tLoss: 648.0275\n",
      "Training Epoch: 47 [10450/36045]\tLoss: 607.2208\n",
      "Training Epoch: 47 [10500/36045]\tLoss: 507.1049\n",
      "Training Epoch: 47 [10550/36045]\tLoss: 501.6694\n",
      "Training Epoch: 47 [10600/36045]\tLoss: 523.4587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 47 [10650/36045]\tLoss: 529.2975\n",
      "Training Epoch: 47 [10700/36045]\tLoss: 608.5909\n",
      "Training Epoch: 47 [10750/36045]\tLoss: 667.4238\n",
      "Training Epoch: 47 [10800/36045]\tLoss: 613.4160\n",
      "Training Epoch: 47 [10850/36045]\tLoss: 650.6327\n",
      "Training Epoch: 47 [10900/36045]\tLoss: 677.3514\n",
      "Training Epoch: 47 [10950/36045]\tLoss: 498.0551\n",
      "Training Epoch: 47 [11000/36045]\tLoss: 491.8312\n",
      "Training Epoch: 47 [11050/36045]\tLoss: 527.5626\n",
      "Training Epoch: 47 [11100/36045]\tLoss: 537.6111\n",
      "Training Epoch: 47 [11150/36045]\tLoss: 583.2930\n",
      "Training Epoch: 47 [11200/36045]\tLoss: 611.7692\n",
      "Training Epoch: 47 [11250/36045]\tLoss: 623.0294\n",
      "Training Epoch: 47 [11300/36045]\tLoss: 603.4176\n",
      "Training Epoch: 47 [11350/36045]\tLoss: 601.1749\n",
      "Training Epoch: 47 [11400/36045]\tLoss: 564.8557\n",
      "Training Epoch: 47 [11450/36045]\tLoss: 534.2011\n",
      "Training Epoch: 47 [11500/36045]\tLoss: 531.6477\n",
      "Training Epoch: 47 [11550/36045]\tLoss: 541.0839\n",
      "Training Epoch: 47 [11600/36045]\tLoss: 600.9852\n",
      "Training Epoch: 47 [11650/36045]\tLoss: 652.2630\n",
      "Training Epoch: 47 [11700/36045]\tLoss: 651.1738\n",
      "Training Epoch: 47 [11750/36045]\tLoss: 669.0432\n",
      "Training Epoch: 47 [11800/36045]\tLoss: 711.4966\n",
      "Training Epoch: 47 [11850/36045]\tLoss: 770.0352\n",
      "Training Epoch: 47 [11900/36045]\tLoss: 981.7787\n",
      "Training Epoch: 47 [11950/36045]\tLoss: 984.9705\n",
      "Training Epoch: 47 [12000/36045]\tLoss: 996.0882\n",
      "Training Epoch: 47 [12050/36045]\tLoss: 955.7990\n",
      "Training Epoch: 47 [12100/36045]\tLoss: 610.4518\n",
      "Training Epoch: 47 [12150/36045]\tLoss: 458.5960\n",
      "Training Epoch: 47 [12200/36045]\tLoss: 453.4527\n",
      "Training Epoch: 47 [12250/36045]\tLoss: 462.0166\n",
      "Training Epoch: 47 [12300/36045]\tLoss: 596.5795\n",
      "Training Epoch: 47 [12350/36045]\tLoss: 651.5780\n",
      "Training Epoch: 47 [12400/36045]\tLoss: 658.7328\n",
      "Training Epoch: 47 [12450/36045]\tLoss: 647.6814\n",
      "Training Epoch: 47 [12500/36045]\tLoss: 674.5031\n",
      "Training Epoch: 47 [12550/36045]\tLoss: 644.5033\n",
      "Training Epoch: 47 [12600/36045]\tLoss: 588.9401\n",
      "Training Epoch: 47 [12650/36045]\tLoss: 587.1068\n",
      "Training Epoch: 47 [12700/36045]\tLoss: 608.3317\n",
      "Training Epoch: 47 [12750/36045]\tLoss: 606.8508\n",
      "Training Epoch: 47 [12800/36045]\tLoss: 593.2770\n",
      "Training Epoch: 47 [12850/36045]\tLoss: 622.4669\n",
      "Training Epoch: 47 [12900/36045]\tLoss: 596.6586\n",
      "Training Epoch: 47 [12950/36045]\tLoss: 582.3084\n",
      "Training Epoch: 47 [13000/36045]\tLoss: 615.6293\n",
      "Training Epoch: 47 [13050/36045]\tLoss: 556.4208\n",
      "Training Epoch: 47 [13100/36045]\tLoss: 571.5374\n",
      "Training Epoch: 47 [13150/36045]\tLoss: 563.1525\n",
      "Training Epoch: 47 [13200/36045]\tLoss: 546.4338\n",
      "Training Epoch: 47 [13250/36045]\tLoss: 567.9738\n",
      "Training Epoch: 47 [13300/36045]\tLoss: 604.5695\n",
      "Training Epoch: 47 [13350/36045]\tLoss: 585.6986\n",
      "Training Epoch: 47 [13400/36045]\tLoss: 588.7650\n",
      "Training Epoch: 47 [13450/36045]\tLoss: 586.2998\n",
      "Training Epoch: 47 [13500/36045]\tLoss: 604.7363\n",
      "Training Epoch: 47 [13550/36045]\tLoss: 742.8212\n",
      "Training Epoch: 47 [13600/36045]\tLoss: 775.4114\n",
      "Training Epoch: 47 [13650/36045]\tLoss: 857.2618\n",
      "Training Epoch: 47 [13700/36045]\tLoss: 754.7070\n",
      "Training Epoch: 47 [13750/36045]\tLoss: 592.2009\n",
      "Training Epoch: 47 [13800/36045]\tLoss: 562.9248\n",
      "Training Epoch: 47 [13850/36045]\tLoss: 545.6800\n",
      "Training Epoch: 47 [13900/36045]\tLoss: 552.9383\n",
      "Training Epoch: 47 [13950/36045]\tLoss: 598.0677\n",
      "Training Epoch: 47 [14000/36045]\tLoss: 630.6339\n",
      "Training Epoch: 47 [14050/36045]\tLoss: 605.7505\n",
      "Training Epoch: 47 [14100/36045]\tLoss: 600.7654\n",
      "Training Epoch: 47 [14150/36045]\tLoss: 588.9095\n",
      "Training Epoch: 47 [14200/36045]\tLoss: 629.2877\n",
      "Training Epoch: 47 [14250/36045]\tLoss: 691.6942\n",
      "Training Epoch: 47 [14300/36045]\tLoss: 694.8292\n",
      "Training Epoch: 47 [14350/36045]\tLoss: 663.8981\n",
      "Training Epoch: 47 [14400/36045]\tLoss: 650.0463\n",
      "Training Epoch: 47 [14450/36045]\tLoss: 685.3878\n",
      "Training Epoch: 47 [14500/36045]\tLoss: 617.0981\n",
      "Training Epoch: 47 [14550/36045]\tLoss: 644.8021\n",
      "Training Epoch: 47 [14600/36045]\tLoss: 631.3332\n",
      "Training Epoch: 47 [14650/36045]\tLoss: 631.0178\n",
      "Training Epoch: 47 [14700/36045]\tLoss: 598.5428\n",
      "Training Epoch: 47 [14750/36045]\tLoss: 515.0976\n",
      "Training Epoch: 47 [14800/36045]\tLoss: 505.1075\n",
      "Training Epoch: 47 [14850/36045]\tLoss: 512.0772\n",
      "Training Epoch: 47 [14900/36045]\tLoss: 505.5815\n",
      "Training Epoch: 47 [14950/36045]\tLoss: 513.8135\n",
      "Training Epoch: 47 [15000/36045]\tLoss: 526.2708\n",
      "Training Epoch: 47 [15050/36045]\tLoss: 522.7524\n",
      "Training Epoch: 47 [15100/36045]\tLoss: 506.6973\n",
      "Training Epoch: 47 [15150/36045]\tLoss: 502.5190\n",
      "Training Epoch: 47 [15200/36045]\tLoss: 465.6999\n",
      "Training Epoch: 47 [15250/36045]\tLoss: 487.2271\n",
      "Training Epoch: 47 [15300/36045]\tLoss: 473.0235\n",
      "Training Epoch: 47 [15350/36045]\tLoss: 484.5016\n",
      "Training Epoch: 47 [15400/36045]\tLoss: 466.6283\n",
      "Training Epoch: 47 [15450/36045]\tLoss: 451.9066\n",
      "Training Epoch: 47 [15500/36045]\tLoss: 464.8412\n",
      "Training Epoch: 47 [15550/36045]\tLoss: 461.7068\n",
      "Training Epoch: 47 [15600/36045]\tLoss: 528.2794\n",
      "Training Epoch: 47 [15650/36045]\tLoss: 544.6865\n",
      "Training Epoch: 47 [15700/36045]\tLoss: 537.0473\n",
      "Training Epoch: 47 [15750/36045]\tLoss: 528.4933\n",
      "Training Epoch: 47 [15800/36045]\tLoss: 507.3394\n",
      "Training Epoch: 47 [15850/36045]\tLoss: 522.7717\n",
      "Training Epoch: 47 [15900/36045]\tLoss: 531.5478\n",
      "Training Epoch: 47 [15950/36045]\tLoss: 551.2855\n",
      "Training Epoch: 47 [16000/36045]\tLoss: 521.9222\n",
      "Training Epoch: 47 [16050/36045]\tLoss: 491.5164\n",
      "Training Epoch: 47 [16100/36045]\tLoss: 456.2533\n",
      "Training Epoch: 47 [16150/36045]\tLoss: 444.3737\n",
      "Training Epoch: 47 [16200/36045]\tLoss: 540.0048\n",
      "Training Epoch: 47 [16250/36045]\tLoss: 567.2661\n",
      "Training Epoch: 47 [16300/36045]\tLoss: 619.2071\n",
      "Training Epoch: 47 [16350/36045]\tLoss: 639.6209\n",
      "Training Epoch: 47 [16400/36045]\tLoss: 611.5017\n",
      "Training Epoch: 47 [16450/36045]\tLoss: 593.9653\n",
      "Training Epoch: 47 [16500/36045]\tLoss: 593.8069\n",
      "Training Epoch: 47 [16550/36045]\tLoss: 559.3550\n",
      "Training Epoch: 47 [16600/36045]\tLoss: 580.9218\n",
      "Training Epoch: 47 [16650/36045]\tLoss: 596.7229\n",
      "Training Epoch: 47 [16700/36045]\tLoss: 577.2974\n",
      "Training Epoch: 47 [16750/36045]\tLoss: 570.0540\n",
      "Training Epoch: 47 [16800/36045]\tLoss: 578.4343\n",
      "Training Epoch: 47 [16850/36045]\tLoss: 551.3495\n",
      "Training Epoch: 47 [16900/36045]\tLoss: 561.1001\n",
      "Training Epoch: 47 [16950/36045]\tLoss: 583.6840\n",
      "Training Epoch: 47 [17000/36045]\tLoss: 568.3281\n",
      "Training Epoch: 47 [17050/36045]\tLoss: 591.6926\n",
      "Training Epoch: 47 [17100/36045]\tLoss: 587.5248\n",
      "Training Epoch: 47 [17150/36045]\tLoss: 509.7760\n",
      "Training Epoch: 47 [17200/36045]\tLoss: 472.3084\n",
      "Training Epoch: 47 [17250/36045]\tLoss: 494.7804\n",
      "Training Epoch: 47 [17300/36045]\tLoss: 523.7441\n",
      "Training Epoch: 47 [17350/36045]\tLoss: 505.4009\n",
      "Training Epoch: 47 [17400/36045]\tLoss: 524.9775\n",
      "Training Epoch: 47 [17450/36045]\tLoss: 543.1347\n",
      "Training Epoch: 47 [17500/36045]\tLoss: 531.6529\n",
      "Training Epoch: 47 [17550/36045]\tLoss: 530.1390\n",
      "Training Epoch: 47 [17600/36045]\tLoss: 524.6589\n",
      "Training Epoch: 47 [17650/36045]\tLoss: 539.8073\n",
      "Training Epoch: 47 [17700/36045]\tLoss: 519.3375\n",
      "Training Epoch: 47 [17750/36045]\tLoss: 535.0279\n",
      "Training Epoch: 47 [17800/36045]\tLoss: 526.4153\n",
      "Training Epoch: 47 [17850/36045]\tLoss: 543.4086\n",
      "Training Epoch: 47 [17900/36045]\tLoss: 571.1628\n",
      "Training Epoch: 47 [17950/36045]\tLoss: 583.4655\n",
      "Training Epoch: 47 [18000/36045]\tLoss: 574.3994\n",
      "Training Epoch: 47 [18050/36045]\tLoss: 629.7487\n",
      "Training Epoch: 47 [18100/36045]\tLoss: 630.5833\n",
      "Training Epoch: 47 [18150/36045]\tLoss: 641.7757\n",
      "Training Epoch: 47 [18200/36045]\tLoss: 624.3157\n",
      "Training Epoch: 47 [18250/36045]\tLoss: 644.9328\n",
      "Training Epoch: 47 [18300/36045]\tLoss: 601.8931\n",
      "Training Epoch: 47 [18350/36045]\tLoss: 675.2169\n",
      "Training Epoch: 47 [18400/36045]\tLoss: 647.9190\n",
      "Training Epoch: 47 [18450/36045]\tLoss: 627.7436\n",
      "Training Epoch: 47 [18500/36045]\tLoss: 626.5687\n",
      "Training Epoch: 47 [18550/36045]\tLoss: 614.2847\n",
      "Training Epoch: 47 [18600/36045]\tLoss: 604.0314\n",
      "Training Epoch: 47 [18650/36045]\tLoss: 649.1843\n",
      "Training Epoch: 47 [18700/36045]\tLoss: 682.9056\n",
      "Training Epoch: 47 [18750/36045]\tLoss: 670.6582\n",
      "Training Epoch: 47 [18800/36045]\tLoss: 693.2374\n",
      "Training Epoch: 47 [18850/36045]\tLoss: 638.4750\n",
      "Training Epoch: 47 [18900/36045]\tLoss: 682.8577\n",
      "Training Epoch: 47 [18950/36045]\tLoss: 625.5812\n",
      "Training Epoch: 47 [19000/36045]\tLoss: 512.3545\n",
      "Training Epoch: 47 [19050/36045]\tLoss: 497.3290\n",
      "Training Epoch: 47 [19100/36045]\tLoss: 505.3008\n",
      "Training Epoch: 47 [19150/36045]\tLoss: 495.5761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 47 [19200/36045]\tLoss: 526.8600\n",
      "Training Epoch: 47 [19250/36045]\tLoss: 542.1664\n",
      "Training Epoch: 47 [19300/36045]\tLoss: 551.4257\n",
      "Training Epoch: 47 [19350/36045]\tLoss: 535.3400\n",
      "Training Epoch: 47 [19400/36045]\tLoss: 555.1965\n",
      "Training Epoch: 47 [19450/36045]\tLoss: 546.6782\n",
      "Training Epoch: 47 [19500/36045]\tLoss: 547.7460\n",
      "Training Epoch: 47 [19550/36045]\tLoss: 545.9358\n",
      "Training Epoch: 47 [19600/36045]\tLoss: 586.7032\n",
      "Training Epoch: 47 [19650/36045]\tLoss: 786.1602\n",
      "Training Epoch: 47 [19700/36045]\tLoss: 744.8483\n",
      "Training Epoch: 47 [19750/36045]\tLoss: 748.9724\n",
      "Training Epoch: 47 [19800/36045]\tLoss: 749.5551\n",
      "Training Epoch: 47 [19850/36045]\tLoss: 488.5039\n",
      "Training Epoch: 47 [19900/36045]\tLoss: 467.9795\n",
      "Training Epoch: 47 [19950/36045]\tLoss: 471.9562\n",
      "Training Epoch: 47 [20000/36045]\tLoss: 472.0456\n",
      "Training Epoch: 47 [20050/36045]\tLoss: 528.0647\n",
      "Training Epoch: 47 [20100/36045]\tLoss: 533.2941\n",
      "Training Epoch: 47 [20150/36045]\tLoss: 534.2368\n",
      "Training Epoch: 47 [20200/36045]\tLoss: 534.0625\n",
      "Training Epoch: 47 [20250/36045]\tLoss: 570.0730\n",
      "Training Epoch: 47 [20300/36045]\tLoss: 606.1954\n",
      "Training Epoch: 47 [20350/36045]\tLoss: 624.0986\n",
      "Training Epoch: 47 [20400/36045]\tLoss: 640.2594\n",
      "Training Epoch: 47 [20450/36045]\tLoss: 610.0225\n",
      "Training Epoch: 47 [20500/36045]\tLoss: 595.1276\n",
      "Training Epoch: 47 [20550/36045]\tLoss: 522.5473\n",
      "Training Epoch: 47 [20600/36045]\tLoss: 532.1227\n",
      "Training Epoch: 47 [20650/36045]\tLoss: 529.3734\n",
      "Training Epoch: 47 [20700/36045]\tLoss: 517.4815\n",
      "Training Epoch: 47 [20750/36045]\tLoss: 558.3175\n",
      "Training Epoch: 47 [20800/36045]\tLoss: 606.6741\n",
      "Training Epoch: 47 [20850/36045]\tLoss: 593.5911\n",
      "Training Epoch: 47 [20900/36045]\tLoss: 636.1481\n",
      "Training Epoch: 47 [20950/36045]\tLoss: 599.2266\n",
      "Training Epoch: 47 [21000/36045]\tLoss: 564.0081\n",
      "Training Epoch: 47 [21050/36045]\tLoss: 482.3752\n",
      "Training Epoch: 47 [21100/36045]\tLoss: 487.6096\n",
      "Training Epoch: 47 [21150/36045]\tLoss: 521.8650\n",
      "Training Epoch: 47 [21200/36045]\tLoss: 520.8702\n",
      "Training Epoch: 47 [21250/36045]\tLoss: 498.0709\n",
      "Training Epoch: 47 [21300/36045]\tLoss: 581.3171\n",
      "Training Epoch: 47 [21350/36045]\tLoss: 573.1227\n",
      "Training Epoch: 47 [21400/36045]\tLoss: 576.9397\n",
      "Training Epoch: 47 [21450/36045]\tLoss: 583.1280\n",
      "Training Epoch: 47 [21500/36045]\tLoss: 584.5711\n",
      "Training Epoch: 47 [21550/36045]\tLoss: 678.6049\n",
      "Training Epoch: 47 [21600/36045]\tLoss: 676.9477\n",
      "Training Epoch: 47 [21650/36045]\tLoss: 689.2797\n",
      "Training Epoch: 47 [21700/36045]\tLoss: 692.7908\n",
      "Training Epoch: 47 [21750/36045]\tLoss: 664.9792\n",
      "Training Epoch: 47 [21800/36045]\tLoss: 487.8048\n",
      "Training Epoch: 47 [21850/36045]\tLoss: 470.7604\n",
      "Training Epoch: 47 [21900/36045]\tLoss: 479.9428\n",
      "Training Epoch: 47 [21950/36045]\tLoss: 481.6375\n",
      "Training Epoch: 47 [22000/36045]\tLoss: 484.8636\n",
      "Training Epoch: 47 [22050/36045]\tLoss: 504.0187\n",
      "Training Epoch: 47 [22100/36045]\tLoss: 496.4141\n",
      "Training Epoch: 47 [22150/36045]\tLoss: 483.0309\n",
      "Training Epoch: 47 [22200/36045]\tLoss: 498.6123\n",
      "Training Epoch: 47 [22250/36045]\tLoss: 503.6130\n",
      "Training Epoch: 47 [22300/36045]\tLoss: 555.3162\n",
      "Training Epoch: 47 [22350/36045]\tLoss: 580.6782\n",
      "Training Epoch: 47 [22400/36045]\tLoss: 594.2410\n",
      "Training Epoch: 47 [22450/36045]\tLoss: 581.6425\n",
      "Training Epoch: 47 [22500/36045]\tLoss: 564.9957\n",
      "Training Epoch: 47 [22550/36045]\tLoss: 599.6422\n",
      "Training Epoch: 47 [22600/36045]\tLoss: 647.3732\n",
      "Training Epoch: 47 [22650/36045]\tLoss: 679.9137\n",
      "Training Epoch: 47 [22700/36045]\tLoss: 701.0936\n",
      "Training Epoch: 47 [22750/36045]\tLoss: 720.7277\n",
      "Training Epoch: 47 [22800/36045]\tLoss: 748.8747\n",
      "Training Epoch: 47 [22850/36045]\tLoss: 621.5010\n",
      "Training Epoch: 47 [22900/36045]\tLoss: 626.1453\n",
      "Training Epoch: 47 [22950/36045]\tLoss: 605.8064\n",
      "Training Epoch: 47 [23000/36045]\tLoss: 602.2672\n",
      "Training Epoch: 47 [23050/36045]\tLoss: 535.1387\n",
      "Training Epoch: 47 [23100/36045]\tLoss: 550.6427\n",
      "Training Epoch: 47 [23150/36045]\tLoss: 538.8763\n",
      "Training Epoch: 47 [23200/36045]\tLoss: 509.9267\n",
      "Training Epoch: 47 [23250/36045]\tLoss: 512.9382\n",
      "Training Epoch: 47 [23300/36045]\tLoss: 508.9370\n",
      "Training Epoch: 47 [23350/36045]\tLoss: 529.1304\n",
      "Training Epoch: 47 [23400/36045]\tLoss: 573.9157\n",
      "Training Epoch: 47 [23450/36045]\tLoss: 567.6027\n",
      "Training Epoch: 47 [23500/36045]\tLoss: 546.8253\n",
      "Training Epoch: 47 [23550/36045]\tLoss: 586.0132\n",
      "Training Epoch: 47 [23600/36045]\tLoss: 665.9177\n",
      "Training Epoch: 47 [23650/36045]\tLoss: 677.0001\n",
      "Training Epoch: 47 [23700/36045]\tLoss: 684.9393\n",
      "Training Epoch: 47 [23750/36045]\tLoss: 661.3410\n",
      "Training Epoch: 47 [23800/36045]\tLoss: 530.8974\n",
      "Training Epoch: 47 [23850/36045]\tLoss: 556.5046\n",
      "Training Epoch: 47 [23900/36045]\tLoss: 546.4496\n",
      "Training Epoch: 47 [23950/36045]\tLoss: 529.6972\n",
      "Training Epoch: 47 [24000/36045]\tLoss: 506.9721\n",
      "Training Epoch: 47 [24050/36045]\tLoss: 467.7544\n",
      "Training Epoch: 47 [24100/36045]\tLoss: 491.9947\n",
      "Training Epoch: 47 [24150/36045]\tLoss: 483.9934\n",
      "Training Epoch: 47 [24200/36045]\tLoss: 481.8617\n",
      "Training Epoch: 47 [24250/36045]\tLoss: 468.0699\n",
      "Training Epoch: 47 [24300/36045]\tLoss: 505.5423\n",
      "Training Epoch: 47 [24350/36045]\tLoss: 517.4484\n",
      "Training Epoch: 47 [24400/36045]\tLoss: 532.2690\n",
      "Training Epoch: 47 [24450/36045]\tLoss: 507.2798\n",
      "Training Epoch: 47 [24500/36045]\tLoss: 535.6555\n",
      "Training Epoch: 47 [24550/36045]\tLoss: 621.8832\n",
      "Training Epoch: 47 [24600/36045]\tLoss: 612.9739\n",
      "Training Epoch: 47 [24650/36045]\tLoss: 586.5010\n",
      "Training Epoch: 47 [24700/36045]\tLoss: 596.4695\n",
      "Training Epoch: 47 [24750/36045]\tLoss: 551.5474\n",
      "Training Epoch: 47 [24800/36045]\tLoss: 451.0667\n",
      "Training Epoch: 47 [24850/36045]\tLoss: 468.9094\n",
      "Training Epoch: 47 [24900/36045]\tLoss: 466.4181\n",
      "Training Epoch: 47 [24950/36045]\tLoss: 468.9267\n",
      "Training Epoch: 47 [25000/36045]\tLoss: 451.0913\n",
      "Training Epoch: 47 [25050/36045]\tLoss: 431.3472\n",
      "Training Epoch: 47 [25100/36045]\tLoss: 386.3477\n",
      "Training Epoch: 47 [25150/36045]\tLoss: 357.5605\n",
      "Training Epoch: 47 [25200/36045]\tLoss: 352.5040\n",
      "Training Epoch: 47 [25250/36045]\tLoss: 378.1143\n",
      "Training Epoch: 47 [25300/36045]\tLoss: 497.0013\n",
      "Training Epoch: 47 [25350/36045]\tLoss: 493.2104\n",
      "Training Epoch: 47 [25400/36045]\tLoss: 461.0822\n",
      "Training Epoch: 47 [25450/36045]\tLoss: 462.9417\n",
      "Training Epoch: 47 [25500/36045]\tLoss: 502.8290\n",
      "Training Epoch: 47 [25550/36045]\tLoss: 588.1633\n",
      "Training Epoch: 47 [25600/36045]\tLoss: 592.5378\n",
      "Training Epoch: 47 [25650/36045]\tLoss: 572.0696\n",
      "Training Epoch: 47 [25700/36045]\tLoss: 581.6210\n",
      "Training Epoch: 47 [25750/36045]\tLoss: 562.0239\n",
      "Training Epoch: 47 [25800/36045]\tLoss: 354.2957\n",
      "Training Epoch: 47 [25850/36045]\tLoss: 361.9627\n",
      "Training Epoch: 47 [25900/36045]\tLoss: 344.0856\n",
      "Training Epoch: 47 [25950/36045]\tLoss: 354.0023\n",
      "Training Epoch: 47 [26000/36045]\tLoss: 435.7003\n",
      "Training Epoch: 47 [26050/36045]\tLoss: 592.1713\n",
      "Training Epoch: 47 [26100/36045]\tLoss: 617.4571\n",
      "Training Epoch: 47 [26150/36045]\tLoss: 616.7903\n",
      "Training Epoch: 47 [26200/36045]\tLoss: 590.9185\n",
      "Training Epoch: 47 [26250/36045]\tLoss: 621.2344\n",
      "Training Epoch: 47 [26300/36045]\tLoss: 572.0140\n",
      "Training Epoch: 47 [26350/36045]\tLoss: 582.2883\n",
      "Training Epoch: 47 [26400/36045]\tLoss: 557.1117\n",
      "Training Epoch: 47 [26450/36045]\tLoss: 487.9820\n",
      "Training Epoch: 47 [26500/36045]\tLoss: 578.0091\n",
      "Training Epoch: 47 [26550/36045]\tLoss: 577.1976\n",
      "Training Epoch: 47 [26600/36045]\tLoss: 573.0419\n",
      "Training Epoch: 47 [26650/36045]\tLoss: 587.4208\n",
      "Training Epoch: 47 [26700/36045]\tLoss: 566.8592\n",
      "Training Epoch: 47 [26750/36045]\tLoss: 532.2515\n",
      "Training Epoch: 47 [26800/36045]\tLoss: 393.5485\n",
      "Training Epoch: 47 [26850/36045]\tLoss: 325.4354\n",
      "Training Epoch: 47 [26900/36045]\tLoss: 326.9310\n",
      "Training Epoch: 47 [26950/36045]\tLoss: 358.6623\n",
      "Training Epoch: 47 [27000/36045]\tLoss: 592.9954\n",
      "Training Epoch: 47 [27050/36045]\tLoss: 619.0801\n",
      "Training Epoch: 47 [27100/36045]\tLoss: 600.0091\n",
      "Training Epoch: 47 [27150/36045]\tLoss: 638.3203\n",
      "Training Epoch: 47 [27200/36045]\tLoss: 462.1604\n",
      "Training Epoch: 47 [27250/36045]\tLoss: 452.6245\n",
      "Training Epoch: 47 [27300/36045]\tLoss: 441.6201\n",
      "Training Epoch: 47 [27350/36045]\tLoss: 438.6507\n",
      "Training Epoch: 47 [27400/36045]\tLoss: 438.0354\n",
      "Training Epoch: 47 [27450/36045]\tLoss: 555.0143\n",
      "Training Epoch: 47 [27500/36045]\tLoss: 594.7773\n",
      "Training Epoch: 47 [27550/36045]\tLoss: 588.0833\n",
      "Training Epoch: 47 [27600/36045]\tLoss: 599.4666\n",
      "Training Epoch: 47 [27650/36045]\tLoss: 591.4939\n",
      "Training Epoch: 47 [27700/36045]\tLoss: 618.4304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 47 [27750/36045]\tLoss: 630.2338\n",
      "Training Epoch: 47 [27800/36045]\tLoss: 617.6910\n",
      "Training Epoch: 47 [27850/36045]\tLoss: 608.1172\n",
      "Training Epoch: 47 [27900/36045]\tLoss: 553.7095\n",
      "Training Epoch: 47 [27950/36045]\tLoss: 463.8387\n",
      "Training Epoch: 47 [28000/36045]\tLoss: 440.7507\n",
      "Training Epoch: 47 [28050/36045]\tLoss: 450.1232\n",
      "Training Epoch: 47 [28100/36045]\tLoss: 441.8388\n",
      "Training Epoch: 47 [28150/36045]\tLoss: 460.1624\n",
      "Training Epoch: 47 [28200/36045]\tLoss: 468.7538\n",
      "Training Epoch: 47 [28250/36045]\tLoss: 461.4738\n",
      "Training Epoch: 47 [28300/36045]\tLoss: 438.1085\n",
      "Training Epoch: 47 [28350/36045]\tLoss: 433.8878\n",
      "Training Epoch: 47 [28400/36045]\tLoss: 760.7197\n",
      "Training Epoch: 47 [28450/36045]\tLoss: 698.7274\n",
      "Training Epoch: 47 [28500/36045]\tLoss: 603.7187\n",
      "Training Epoch: 47 [28550/36045]\tLoss: 555.2866\n",
      "Training Epoch: 47 [28600/36045]\tLoss: 578.6567\n",
      "Training Epoch: 47 [28650/36045]\tLoss: 630.2668\n",
      "Training Epoch: 47 [28700/36045]\tLoss: 623.9341\n",
      "Training Epoch: 47 [28750/36045]\tLoss: 611.0790\n",
      "Training Epoch: 47 [28800/36045]\tLoss: 621.2610\n",
      "Training Epoch: 47 [28850/36045]\tLoss: 539.8320\n",
      "Training Epoch: 47 [28900/36045]\tLoss: 439.6032\n",
      "Training Epoch: 47 [28950/36045]\tLoss: 438.7573\n",
      "Training Epoch: 47 [29000/36045]\tLoss: 434.5755\n",
      "Training Epoch: 47 [29050/36045]\tLoss: 441.6580\n",
      "Training Epoch: 47 [29100/36045]\tLoss: 459.4512\n",
      "Training Epoch: 47 [29150/36045]\tLoss: 450.1538\n",
      "Training Epoch: 47 [29200/36045]\tLoss: 435.9204\n",
      "Training Epoch: 47 [29250/36045]\tLoss: 425.8421\n",
      "Training Epoch: 47 [29300/36045]\tLoss: 478.0156\n",
      "Training Epoch: 47 [29350/36045]\tLoss: 560.6671\n",
      "Training Epoch: 47 [29400/36045]\tLoss: 578.2404\n",
      "Training Epoch: 47 [29450/36045]\tLoss: 594.6028\n",
      "Training Epoch: 47 [29500/36045]\tLoss: 609.3882\n",
      "Training Epoch: 47 [29550/36045]\tLoss: 579.4503\n",
      "Training Epoch: 47 [29600/36045]\tLoss: 487.0860\n",
      "Training Epoch: 47 [29650/36045]\tLoss: 468.6507\n",
      "Training Epoch: 47 [29700/36045]\tLoss: 421.4279\n",
      "Training Epoch: 47 [29750/36045]\tLoss: 419.7438\n",
      "Training Epoch: 47 [29800/36045]\tLoss: 466.6579\n",
      "Training Epoch: 47 [29850/36045]\tLoss: 544.2014\n",
      "Training Epoch: 47 [29900/36045]\tLoss: 539.5887\n",
      "Training Epoch: 47 [29950/36045]\tLoss: 561.4485\n",
      "Training Epoch: 47 [30000/36045]\tLoss: 535.3572\n",
      "Training Epoch: 47 [30050/36045]\tLoss: 542.7178\n",
      "Training Epoch: 47 [30100/36045]\tLoss: 661.8142\n",
      "Training Epoch: 47 [30150/36045]\tLoss: 643.8520\n",
      "Training Epoch: 47 [30200/36045]\tLoss: 607.1328\n",
      "Training Epoch: 47 [30250/36045]\tLoss: 655.6511\n",
      "Training Epoch: 47 [30300/36045]\tLoss: 639.8204\n",
      "Training Epoch: 47 [30350/36045]\tLoss: 486.6590\n",
      "Training Epoch: 47 [30400/36045]\tLoss: 471.3627\n",
      "Training Epoch: 47 [30450/36045]\tLoss: 473.7380\n",
      "Training Epoch: 47 [30500/36045]\tLoss: 442.1434\n",
      "Training Epoch: 47 [30550/36045]\tLoss: 410.4086\n",
      "Training Epoch: 47 [30600/36045]\tLoss: 403.4682\n",
      "Training Epoch: 47 [30650/36045]\tLoss: 393.6325\n",
      "Training Epoch: 47 [30700/36045]\tLoss: 411.2234\n",
      "Training Epoch: 47 [30750/36045]\tLoss: 398.2286\n",
      "Training Epoch: 47 [30800/36045]\tLoss: 424.3396\n",
      "Training Epoch: 47 [30850/36045]\tLoss: 416.1556\n",
      "Training Epoch: 47 [30900/36045]\tLoss: 427.7960\n",
      "Training Epoch: 47 [30950/36045]\tLoss: 449.6455\n",
      "Training Epoch: 47 [31000/36045]\tLoss: 442.1280\n",
      "Training Epoch: 47 [31050/36045]\tLoss: 369.0094\n",
      "Training Epoch: 47 [31100/36045]\tLoss: 359.7682\n",
      "Training Epoch: 47 [31150/36045]\tLoss: 367.4722\n",
      "Training Epoch: 47 [31200/36045]\tLoss: 456.0653\n",
      "Training Epoch: 47 [31250/36045]\tLoss: 591.7454\n",
      "Training Epoch: 47 [31300/36045]\tLoss: 563.0776\n",
      "Training Epoch: 47 [31350/36045]\tLoss: 579.1730\n",
      "Training Epoch: 47 [31400/36045]\tLoss: 558.7720\n",
      "Training Epoch: 47 [31450/36045]\tLoss: 575.5457\n",
      "Training Epoch: 47 [31500/36045]\tLoss: 588.0005\n",
      "Training Epoch: 47 [31550/36045]\tLoss: 594.5825\n",
      "Training Epoch: 47 [31600/36045]\tLoss: 559.0052\n",
      "Training Epoch: 47 [31650/36045]\tLoss: 598.7460\n",
      "Training Epoch: 47 [31700/36045]\tLoss: 432.8270\n",
      "Training Epoch: 47 [31750/36045]\tLoss: 357.4755\n",
      "Training Epoch: 47 [31800/36045]\tLoss: 341.2138\n",
      "Training Epoch: 47 [31850/36045]\tLoss: 348.7697\n",
      "Training Epoch: 47 [31900/36045]\tLoss: 553.1732\n",
      "Training Epoch: 47 [31950/36045]\tLoss: 717.6184\n",
      "Training Epoch: 47 [32000/36045]\tLoss: 824.5592\n",
      "Training Epoch: 47 [32050/36045]\tLoss: 780.5125\n",
      "Training Epoch: 47 [32100/36045]\tLoss: 772.0172\n",
      "Training Epoch: 47 [32150/36045]\tLoss: 590.4223\n",
      "Training Epoch: 47 [32200/36045]\tLoss: 592.1074\n",
      "Training Epoch: 47 [32250/36045]\tLoss: 602.2389\n",
      "Training Epoch: 47 [32300/36045]\tLoss: 584.4626\n",
      "Training Epoch: 47 [32350/36045]\tLoss: 580.6047\n",
      "Training Epoch: 47 [32400/36045]\tLoss: 544.8002\n",
      "Training Epoch: 47 [32450/36045]\tLoss: 448.6321\n",
      "Training Epoch: 47 [32500/36045]\tLoss: 431.0041\n",
      "Training Epoch: 47 [32550/36045]\tLoss: 433.0611\n",
      "Training Epoch: 47 [32600/36045]\tLoss: 430.2280\n",
      "Training Epoch: 47 [32650/36045]\tLoss: 558.9094\n",
      "Training Epoch: 47 [32700/36045]\tLoss: 610.6114\n",
      "Training Epoch: 47 [32750/36045]\tLoss: 581.3097\n",
      "Training Epoch: 47 [32800/36045]\tLoss: 595.9324\n",
      "Training Epoch: 47 [32850/36045]\tLoss: 550.1165\n",
      "Training Epoch: 47 [32900/36045]\tLoss: 439.1746\n",
      "Training Epoch: 47 [32950/36045]\tLoss: 460.1049\n",
      "Training Epoch: 47 [33000/36045]\tLoss: 458.6011\n",
      "Training Epoch: 47 [33050/36045]\tLoss: 436.5137\n",
      "Training Epoch: 47 [33100/36045]\tLoss: 496.0560\n",
      "Training Epoch: 47 [33150/36045]\tLoss: 675.7296\n",
      "Training Epoch: 47 [33200/36045]\tLoss: 657.8180\n",
      "Training Epoch: 47 [33250/36045]\tLoss: 677.8043\n",
      "Training Epoch: 47 [33300/36045]\tLoss: 722.6082\n",
      "Training Epoch: 47 [33350/36045]\tLoss: 552.9380\n",
      "Training Epoch: 47 [33400/36045]\tLoss: 401.9534\n",
      "Training Epoch: 47 [33450/36045]\tLoss: 398.0541\n",
      "Training Epoch: 47 [33500/36045]\tLoss: 409.7130\n",
      "Training Epoch: 47 [33550/36045]\tLoss: 424.1015\n",
      "Training Epoch: 47 [33600/36045]\tLoss: 425.8966\n",
      "Training Epoch: 47 [33650/36045]\tLoss: 569.2925\n",
      "Training Epoch: 47 [33700/36045]\tLoss: 550.6766\n",
      "Training Epoch: 47 [33750/36045]\tLoss: 570.2834\n",
      "Training Epoch: 47 [33800/36045]\tLoss: 567.8148\n",
      "Training Epoch: 47 [33850/36045]\tLoss: 569.4792\n",
      "Training Epoch: 47 [33900/36045]\tLoss: 583.0843\n",
      "Training Epoch: 47 [33950/36045]\tLoss: 592.4944\n",
      "Training Epoch: 47 [34000/36045]\tLoss: 579.2299\n",
      "Training Epoch: 47 [34050/36045]\tLoss: 582.9127\n",
      "Training Epoch: 47 [34100/36045]\tLoss: 562.2008\n",
      "Training Epoch: 47 [34150/36045]\tLoss: 520.4784\n",
      "Training Epoch: 47 [34200/36045]\tLoss: 493.0246\n",
      "Training Epoch: 47 [34250/36045]\tLoss: 506.7824\n",
      "Training Epoch: 47 [34300/36045]\tLoss: 432.3641\n",
      "Training Epoch: 47 [34350/36045]\tLoss: 455.4879\n",
      "Training Epoch: 47 [34400/36045]\tLoss: 448.8199\n",
      "Training Epoch: 47 [34450/36045]\tLoss: 422.7104\n",
      "Training Epoch: 47 [34500/36045]\tLoss: 450.8198\n",
      "Training Epoch: 47 [34550/36045]\tLoss: 442.5627\n",
      "Training Epoch: 47 [34600/36045]\tLoss: 449.8981\n",
      "Training Epoch: 47 [34650/36045]\tLoss: 555.4483\n",
      "Training Epoch: 47 [34700/36045]\tLoss: 589.2997\n",
      "Training Epoch: 47 [34750/36045]\tLoss: 522.1907\n",
      "Training Epoch: 47 [34800/36045]\tLoss: 600.0148\n",
      "Training Epoch: 47 [34850/36045]\tLoss: 606.8979\n",
      "Training Epoch: 47 [34900/36045]\tLoss: 652.5170\n",
      "Training Epoch: 47 [34950/36045]\tLoss: 637.0745\n",
      "Training Epoch: 47 [35000/36045]\tLoss: 638.3267\n",
      "Training Epoch: 47 [35050/36045]\tLoss: 625.8400\n",
      "Training Epoch: 47 [35100/36045]\tLoss: 542.0486\n",
      "Training Epoch: 47 [35150/36045]\tLoss: 533.9225\n",
      "Training Epoch: 47 [35200/36045]\tLoss: 448.4026\n",
      "Training Epoch: 47 [35250/36045]\tLoss: 493.0438\n",
      "Training Epoch: 47 [35300/36045]\tLoss: 509.5792\n",
      "Training Epoch: 47 [35350/36045]\tLoss: 568.3320\n",
      "Training Epoch: 47 [35400/36045]\tLoss: 597.1133\n",
      "Training Epoch: 47 [35450/36045]\tLoss: 568.6011\n",
      "Training Epoch: 47 [35500/36045]\tLoss: 549.9692\n",
      "Training Epoch: 47 [35550/36045]\tLoss: 536.7693\n",
      "Training Epoch: 47 [35600/36045]\tLoss: 587.8956\n",
      "Training Epoch: 47 [35650/36045]\tLoss: 659.8260\n",
      "Training Epoch: 47 [35700/36045]\tLoss: 586.4387\n",
      "Training Epoch: 47 [35750/36045]\tLoss: 643.7489\n",
      "Training Epoch: 47 [35800/36045]\tLoss: 650.6567\n",
      "Training Epoch: 47 [35850/36045]\tLoss: 624.8566\n",
      "Training Epoch: 47 [35900/36045]\tLoss: 646.1024\n",
      "Training Epoch: 47 [35950/36045]\tLoss: 642.4992\n",
      "Training Epoch: 47 [36000/36045]\tLoss: 635.9119\n",
      "Training Epoch: 47 [36045/36045]\tLoss: 621.8719\n",
      "Training Epoch: 47 [4004/4004]\tLoss: 577.3604\n",
      "Training Epoch: 48 [50/36045]\tLoss: 575.6509\n",
      "Training Epoch: 48 [100/36045]\tLoss: 551.2615\n",
      "Training Epoch: 48 [150/36045]\tLoss: 548.9445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 48 [200/36045]\tLoss: 535.2578\n",
      "Training Epoch: 48 [250/36045]\tLoss: 644.5883\n",
      "Training Epoch: 48 [300/36045]\tLoss: 710.0214\n",
      "Training Epoch: 48 [350/36045]\tLoss: 676.9325\n",
      "Training Epoch: 48 [400/36045]\tLoss: 670.9366\n",
      "Training Epoch: 48 [450/36045]\tLoss: 651.7158\n",
      "Training Epoch: 48 [500/36045]\tLoss: 602.7199\n",
      "Training Epoch: 48 [550/36045]\tLoss: 605.7990\n",
      "Training Epoch: 48 [600/36045]\tLoss: 592.2620\n",
      "Training Epoch: 48 [650/36045]\tLoss: 613.0887\n",
      "Training Epoch: 48 [700/36045]\tLoss: 597.9792\n",
      "Training Epoch: 48 [750/36045]\tLoss: 573.6536\n",
      "Training Epoch: 48 [800/36045]\tLoss: 585.2674\n",
      "Training Epoch: 48 [850/36045]\tLoss: 569.0022\n",
      "Training Epoch: 48 [900/36045]\tLoss: 545.1419\n",
      "Training Epoch: 48 [950/36045]\tLoss: 515.2598\n",
      "Training Epoch: 48 [1000/36045]\tLoss: 499.5390\n",
      "Training Epoch: 48 [1050/36045]\tLoss: 501.6263\n",
      "Training Epoch: 48 [1100/36045]\tLoss: 488.3067\n",
      "Training Epoch: 48 [1150/36045]\tLoss: 497.8415\n",
      "Training Epoch: 48 [1200/36045]\tLoss: 527.2382\n",
      "Training Epoch: 48 [1250/36045]\tLoss: 602.9118\n",
      "Training Epoch: 48 [1300/36045]\tLoss: 609.8366\n",
      "Training Epoch: 48 [1350/36045]\tLoss: 611.0854\n",
      "Training Epoch: 48 [1400/36045]\tLoss: 634.6469\n",
      "Training Epoch: 48 [1450/36045]\tLoss: 614.2697\n",
      "Training Epoch: 48 [1500/36045]\tLoss: 560.9892\n",
      "Training Epoch: 48 [1550/36045]\tLoss: 575.2772\n",
      "Training Epoch: 48 [1600/36045]\tLoss: 585.0379\n",
      "Training Epoch: 48 [1650/36045]\tLoss: 572.6075\n",
      "Training Epoch: 48 [1700/36045]\tLoss: 584.9389\n",
      "Training Epoch: 48 [1750/36045]\tLoss: 626.5308\n",
      "Training Epoch: 48 [1800/36045]\tLoss: 609.4774\n",
      "Training Epoch: 48 [1850/36045]\tLoss: 624.9638\n",
      "Training Epoch: 48 [1900/36045]\tLoss: 584.2450\n",
      "Training Epoch: 48 [1950/36045]\tLoss: 593.9107\n",
      "Training Epoch: 48 [2000/36045]\tLoss: 534.5585\n",
      "Training Epoch: 48 [2050/36045]\tLoss: 537.3783\n",
      "Training Epoch: 48 [2100/36045]\tLoss: 566.4473\n",
      "Training Epoch: 48 [2150/36045]\tLoss: 547.4554\n",
      "Training Epoch: 48 [2200/36045]\tLoss: 510.2504\n",
      "Training Epoch: 48 [2250/36045]\tLoss: 482.2632\n",
      "Training Epoch: 48 [2300/36045]\tLoss: 505.7669\n",
      "Training Epoch: 48 [2350/36045]\tLoss: 483.5912\n",
      "Training Epoch: 48 [2400/36045]\tLoss: 490.9773\n",
      "Training Epoch: 48 [2450/36045]\tLoss: 629.1091\n",
      "Training Epoch: 48 [2500/36045]\tLoss: 660.5466\n",
      "Training Epoch: 48 [2550/36045]\tLoss: 658.5295\n",
      "Training Epoch: 48 [2600/36045]\tLoss: 667.2884\n",
      "Training Epoch: 48 [2650/36045]\tLoss: 791.3372\n",
      "Training Epoch: 48 [2700/36045]\tLoss: 879.2767\n",
      "Training Epoch: 48 [2750/36045]\tLoss: 949.9474\n",
      "Training Epoch: 48 [2800/36045]\tLoss: 959.1812\n",
      "Training Epoch: 48 [2850/36045]\tLoss: 726.1559\n",
      "Training Epoch: 48 [2900/36045]\tLoss: 686.8377\n",
      "Training Epoch: 48 [2950/36045]\tLoss: 664.6147\n",
      "Training Epoch: 48 [3000/36045]\tLoss: 658.3851\n",
      "Training Epoch: 48 [3050/36045]\tLoss: 689.0112\n",
      "Training Epoch: 48 [3100/36045]\tLoss: 629.3965\n",
      "Training Epoch: 48 [3150/36045]\tLoss: 483.7420\n",
      "Training Epoch: 48 [3200/36045]\tLoss: 500.8542\n",
      "Training Epoch: 48 [3250/36045]\tLoss: 472.8383\n",
      "Training Epoch: 48 [3300/36045]\tLoss: 447.7921\n",
      "Training Epoch: 48 [3350/36045]\tLoss: 473.0017\n",
      "Training Epoch: 48 [3400/36045]\tLoss: 495.4482\n",
      "Training Epoch: 48 [3450/36045]\tLoss: 531.2896\n",
      "Training Epoch: 48 [3500/36045]\tLoss: 518.7278\n",
      "Training Epoch: 48 [3550/36045]\tLoss: 496.0183\n",
      "Training Epoch: 48 [3600/36045]\tLoss: 533.2314\n",
      "Training Epoch: 48 [3650/36045]\tLoss: 616.6779\n",
      "Training Epoch: 48 [3700/36045]\tLoss: 624.4078\n",
      "Training Epoch: 48 [3750/36045]\tLoss: 593.6651\n",
      "Training Epoch: 48 [3800/36045]\tLoss: 590.9996\n",
      "Training Epoch: 48 [3850/36045]\tLoss: 593.1359\n",
      "Training Epoch: 48 [3900/36045]\tLoss: 597.6745\n",
      "Training Epoch: 48 [3950/36045]\tLoss: 576.8773\n",
      "Training Epoch: 48 [4000/36045]\tLoss: 581.3565\n",
      "Training Epoch: 48 [4050/36045]\tLoss: 532.8757\n",
      "Training Epoch: 48 [4100/36045]\tLoss: 519.7563\n",
      "Training Epoch: 48 [4150/36045]\tLoss: 533.6069\n",
      "Training Epoch: 48 [4200/36045]\tLoss: 528.6056\n",
      "Training Epoch: 48 [4250/36045]\tLoss: 530.7132\n",
      "Training Epoch: 48 [4300/36045]\tLoss: 545.5760\n",
      "Training Epoch: 48 [4350/36045]\tLoss: 529.6113\n",
      "Training Epoch: 48 [4400/36045]\tLoss: 506.4703\n",
      "Training Epoch: 48 [4450/36045]\tLoss: 556.6896\n",
      "Training Epoch: 48 [4500/36045]\tLoss: 599.4958\n",
      "Training Epoch: 48 [4550/36045]\tLoss: 602.1188\n",
      "Training Epoch: 48 [4600/36045]\tLoss: 623.8441\n",
      "Training Epoch: 48 [4650/36045]\tLoss: 613.9785\n",
      "Training Epoch: 48 [4700/36045]\tLoss: 565.9867\n",
      "Training Epoch: 48 [4750/36045]\tLoss: 547.8600\n",
      "Training Epoch: 48 [4800/36045]\tLoss: 571.8148\n",
      "Training Epoch: 48 [4850/36045]\tLoss: 558.3952\n",
      "Training Epoch: 48 [4900/36045]\tLoss: 543.6774\n",
      "Training Epoch: 48 [4950/36045]\tLoss: 559.0684\n",
      "Training Epoch: 48 [5000/36045]\tLoss: 588.3801\n",
      "Training Epoch: 48 [5050/36045]\tLoss: 570.0437\n",
      "Training Epoch: 48 [5100/36045]\tLoss: 580.0776\n",
      "Training Epoch: 48 [5150/36045]\tLoss: 564.5534\n",
      "Training Epoch: 48 [5200/36045]\tLoss: 562.5093\n",
      "Training Epoch: 48 [5250/36045]\tLoss: 556.1684\n",
      "Training Epoch: 48 [5300/36045]\tLoss: 556.1665\n",
      "Training Epoch: 48 [5350/36045]\tLoss: 577.0069\n",
      "Training Epoch: 48 [5400/36045]\tLoss: 556.2289\n",
      "Training Epoch: 48 [5450/36045]\tLoss: 526.9600\n",
      "Training Epoch: 48 [5500/36045]\tLoss: 555.0641\n",
      "Training Epoch: 48 [5550/36045]\tLoss: 543.3262\n",
      "Training Epoch: 48 [5600/36045]\tLoss: 621.4266\n",
      "Training Epoch: 48 [5650/36045]\tLoss: 587.4307\n",
      "Training Epoch: 48 [5700/36045]\tLoss: 551.1206\n",
      "Training Epoch: 48 [5750/36045]\tLoss: 535.2202\n",
      "Training Epoch: 48 [5800/36045]\tLoss: 564.6924\n",
      "Training Epoch: 48 [5850/36045]\tLoss: 554.5087\n",
      "Training Epoch: 48 [5900/36045]\tLoss: 636.9532\n",
      "Training Epoch: 48 [5950/36045]\tLoss: 653.2833\n",
      "Training Epoch: 48 [6000/36045]\tLoss: 639.4241\n",
      "Training Epoch: 48 [6050/36045]\tLoss: 617.7191\n",
      "Training Epoch: 48 [6100/36045]\tLoss: 621.9922\n",
      "Training Epoch: 48 [6150/36045]\tLoss: 613.2543\n",
      "Training Epoch: 48 [6200/36045]\tLoss: 618.2303\n",
      "Training Epoch: 48 [6250/36045]\tLoss: 639.7103\n",
      "Training Epoch: 48 [6300/36045]\tLoss: 651.2394\n",
      "Training Epoch: 48 [6350/36045]\tLoss: 696.2634\n",
      "Training Epoch: 48 [6400/36045]\tLoss: 572.9887\n",
      "Training Epoch: 48 [6450/36045]\tLoss: 526.5120\n",
      "Training Epoch: 48 [6500/36045]\tLoss: 536.0580\n",
      "Training Epoch: 48 [6550/36045]\tLoss: 553.4869\n",
      "Training Epoch: 48 [6600/36045]\tLoss: 551.3331\n",
      "Training Epoch: 48 [6650/36045]\tLoss: 622.7736\n",
      "Training Epoch: 48 [6700/36045]\tLoss: 651.2213\n",
      "Training Epoch: 48 [6750/36045]\tLoss: 628.8932\n",
      "Training Epoch: 48 [6800/36045]\tLoss: 631.8749\n",
      "Training Epoch: 48 [6850/36045]\tLoss: 619.9963\n",
      "Training Epoch: 48 [6900/36045]\tLoss: 552.2762\n",
      "Training Epoch: 48 [6950/36045]\tLoss: 520.7336\n",
      "Training Epoch: 48 [7000/36045]\tLoss: 553.7936\n",
      "Training Epoch: 48 [7050/36045]\tLoss: 565.8896\n",
      "Training Epoch: 48 [7100/36045]\tLoss: 564.9116\n",
      "Training Epoch: 48 [7150/36045]\tLoss: 574.8151\n",
      "Training Epoch: 48 [7200/36045]\tLoss: 576.8196\n",
      "Training Epoch: 48 [7250/36045]\tLoss: 575.2142\n",
      "Training Epoch: 48 [7300/36045]\tLoss: 561.5411\n",
      "Training Epoch: 48 [7350/36045]\tLoss: 559.3865\n",
      "Training Epoch: 48 [7400/36045]\tLoss: 510.3563\n",
      "Training Epoch: 48 [7450/36045]\tLoss: 513.6048\n",
      "Training Epoch: 48 [7500/36045]\tLoss: 509.3002\n",
      "Training Epoch: 48 [7550/36045]\tLoss: 487.7801\n",
      "Training Epoch: 48 [7600/36045]\tLoss: 540.4240\n",
      "Training Epoch: 48 [7650/36045]\tLoss: 577.4764\n",
      "Training Epoch: 48 [7700/36045]\tLoss: 549.5454\n",
      "Training Epoch: 48 [7750/36045]\tLoss: 563.8641\n",
      "Training Epoch: 48 [7800/36045]\tLoss: 553.5655\n",
      "Training Epoch: 48 [7850/36045]\tLoss: 535.7466\n",
      "Training Epoch: 48 [7900/36045]\tLoss: 564.9399\n",
      "Training Epoch: 48 [7950/36045]\tLoss: 562.5558\n",
      "Training Epoch: 48 [8000/36045]\tLoss: 580.6613\n",
      "Training Epoch: 48 [8050/36045]\tLoss: 546.6110\n",
      "Training Epoch: 48 [8100/36045]\tLoss: 571.5838\n",
      "Training Epoch: 48 [8150/36045]\tLoss: 648.6526\n",
      "Training Epoch: 48 [8200/36045]\tLoss: 636.2501\n",
      "Training Epoch: 48 [8250/36045]\tLoss: 604.7510\n",
      "Training Epoch: 48 [8300/36045]\tLoss: 661.2438\n",
      "Training Epoch: 48 [8350/36045]\tLoss: 605.9103\n",
      "Training Epoch: 48 [8400/36045]\tLoss: 541.7468\n",
      "Training Epoch: 48 [8450/36045]\tLoss: 506.9349\n",
      "Training Epoch: 48 [8500/36045]\tLoss: 539.6102\n",
      "Training Epoch: 48 [8550/36045]\tLoss: 533.3392\n",
      "Training Epoch: 48 [8600/36045]\tLoss: 527.7103\n",
      "Training Epoch: 48 [8650/36045]\tLoss: 559.5584\n",
      "Training Epoch: 48 [8700/36045]\tLoss: 591.8491\n",
      "Training Epoch: 48 [8750/36045]\tLoss: 581.5025\n",
      "Training Epoch: 48 [8800/36045]\tLoss: 587.2066\n",
      "Training Epoch: 48 [8850/36045]\tLoss: 580.8641\n",
      "Training Epoch: 48 [8900/36045]\tLoss: 524.3967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 48 [8950/36045]\tLoss: 534.9227\n",
      "Training Epoch: 48 [9000/36045]\tLoss: 550.8337\n",
      "Training Epoch: 48 [9050/36045]\tLoss: 552.4384\n",
      "Training Epoch: 48 [9100/36045]\tLoss: 569.0571\n",
      "Training Epoch: 48 [9150/36045]\tLoss: 421.2202\n",
      "Training Epoch: 48 [9200/36045]\tLoss: 314.5785\n",
      "Training Epoch: 48 [9250/36045]\tLoss: 341.4241\n",
      "Training Epoch: 48 [9300/36045]\tLoss: 350.9743\n",
      "Training Epoch: 48 [9350/36045]\tLoss: 324.0043\n",
      "Training Epoch: 48 [9400/36045]\tLoss: 634.3707\n",
      "Training Epoch: 48 [9450/36045]\tLoss: 673.6674\n",
      "Training Epoch: 48 [9500/36045]\tLoss: 661.3975\n",
      "Training Epoch: 48 [9550/36045]\tLoss: 699.7120\n",
      "Training Epoch: 48 [9600/36045]\tLoss: 521.7549\n",
      "Training Epoch: 48 [9650/36045]\tLoss: 526.8860\n",
      "Training Epoch: 48 [9700/36045]\tLoss: 512.4872\n",
      "Training Epoch: 48 [9750/36045]\tLoss: 511.0957\n",
      "Training Epoch: 48 [9800/36045]\tLoss: 668.4909\n",
      "Training Epoch: 48 [9850/36045]\tLoss: 705.8985\n",
      "Training Epoch: 48 [9900/36045]\tLoss: 714.8101\n",
      "Training Epoch: 48 [9950/36045]\tLoss: 696.6013\n",
      "Training Epoch: 48 [10000/36045]\tLoss: 644.9395\n",
      "Training Epoch: 48 [10050/36045]\tLoss: 527.7838\n",
      "Training Epoch: 48 [10100/36045]\tLoss: 535.3780\n",
      "Training Epoch: 48 [10150/36045]\tLoss: 543.3546\n",
      "Training Epoch: 48 [10200/36045]\tLoss: 532.4009\n",
      "Training Epoch: 48 [10250/36045]\tLoss: 640.6370\n",
      "Training Epoch: 48 [10300/36045]\tLoss: 622.6615\n",
      "Training Epoch: 48 [10350/36045]\tLoss: 655.8484\n",
      "Training Epoch: 48 [10400/36045]\tLoss: 645.8297\n",
      "Training Epoch: 48 [10450/36045]\tLoss: 605.1938\n",
      "Training Epoch: 48 [10500/36045]\tLoss: 505.4287\n",
      "Training Epoch: 48 [10550/36045]\tLoss: 499.9505\n",
      "Training Epoch: 48 [10600/36045]\tLoss: 521.6681\n",
      "Training Epoch: 48 [10650/36045]\tLoss: 527.4864\n",
      "Training Epoch: 48 [10700/36045]\tLoss: 606.6072\n",
      "Training Epoch: 48 [10750/36045]\tLoss: 665.3626\n",
      "Training Epoch: 48 [10800/36045]\tLoss: 611.3962\n",
      "Training Epoch: 48 [10850/36045]\tLoss: 648.5145\n",
      "Training Epoch: 48 [10900/36045]\tLoss: 675.1574\n",
      "Training Epoch: 48 [10950/36045]\tLoss: 496.3680\n",
      "Training Epoch: 48 [11000/36045]\tLoss: 490.1649\n",
      "Training Epoch: 48 [11050/36045]\tLoss: 525.7816\n",
      "Training Epoch: 48 [11100/36045]\tLoss: 535.7877\n",
      "Training Epoch: 48 [11150/36045]\tLoss: 581.3272\n",
      "Training Epoch: 48 [11200/36045]\tLoss: 609.8688\n",
      "Training Epoch: 48 [11250/36045]\tLoss: 621.0870\n",
      "Training Epoch: 48 [11300/36045]\tLoss: 601.4938\n",
      "Training Epoch: 48 [11350/36045]\tLoss: 599.2624\n",
      "Training Epoch: 48 [11400/36045]\tLoss: 562.9932\n",
      "Training Epoch: 48 [11450/36045]\tLoss: 532.3829\n",
      "Training Epoch: 48 [11500/36045]\tLoss: 529.8589\n",
      "Training Epoch: 48 [11550/36045]\tLoss: 539.2421\n",
      "Training Epoch: 48 [11600/36045]\tLoss: 599.0866\n",
      "Training Epoch: 48 [11650/36045]\tLoss: 650.2962\n",
      "Training Epoch: 48 [11700/36045]\tLoss: 649.2042\n",
      "Training Epoch: 48 [11750/36045]\tLoss: 667.0448\n",
      "Training Epoch: 48 [11800/36045]\tLoss: 709.4944\n",
      "Training Epoch: 48 [11850/36045]\tLoss: 768.0629\n",
      "Training Epoch: 48 [11900/36045]\tLoss: 979.6872\n",
      "Training Epoch: 48 [11950/36045]\tLoss: 982.9008\n",
      "Training Epoch: 48 [12000/36045]\tLoss: 993.9414\n",
      "Training Epoch: 48 [12050/36045]\tLoss: 953.7096\n",
      "Training Epoch: 48 [12100/36045]\tLoss: 608.7251\n",
      "Training Epoch: 48 [12150/36045]\tLoss: 457.0138\n",
      "Training Epoch: 48 [12200/36045]\tLoss: 451.8755\n",
      "Training Epoch: 48 [12250/36045]\tLoss: 460.4239\n",
      "Training Epoch: 48 [12300/36045]\tLoss: 594.7756\n",
      "Training Epoch: 48 [12350/36045]\tLoss: 649.6882\n",
      "Training Epoch: 48 [12400/36045]\tLoss: 656.8187\n",
      "Training Epoch: 48 [12450/36045]\tLoss: 645.7859\n",
      "Training Epoch: 48 [12500/36045]\tLoss: 672.5690\n",
      "Training Epoch: 48 [12550/36045]\tLoss: 642.5828\n",
      "Training Epoch: 48 [12600/36045]\tLoss: 587.0098\n",
      "Training Epoch: 48 [12650/36045]\tLoss: 585.1575\n",
      "Training Epoch: 48 [12700/36045]\tLoss: 606.3698\n",
      "Training Epoch: 48 [12750/36045]\tLoss: 604.8715\n",
      "Training Epoch: 48 [12800/36045]\tLoss: 591.4192\n",
      "Training Epoch: 48 [12850/36045]\tLoss: 620.6100\n",
      "Training Epoch: 48 [12900/36045]\tLoss: 594.8653\n",
      "Training Epoch: 48 [12950/36045]\tLoss: 580.4714\n",
      "Training Epoch: 48 [13000/36045]\tLoss: 613.8201\n",
      "Training Epoch: 48 [13050/36045]\tLoss: 554.6747\n",
      "Training Epoch: 48 [13100/36045]\tLoss: 569.6829\n",
      "Training Epoch: 48 [13150/36045]\tLoss: 561.3021\n",
      "Training Epoch: 48 [13200/36045]\tLoss: 544.6640\n",
      "Training Epoch: 48 [13250/36045]\tLoss: 566.1281\n",
      "Training Epoch: 48 [13300/36045]\tLoss: 602.6245\n",
      "Training Epoch: 48 [13350/36045]\tLoss: 583.8041\n",
      "Training Epoch: 48 [13400/36045]\tLoss: 586.8630\n",
      "Training Epoch: 48 [13450/36045]\tLoss: 584.4316\n",
      "Training Epoch: 48 [13500/36045]\tLoss: 602.7926\n",
      "Training Epoch: 48 [13550/36045]\tLoss: 740.9504\n",
      "Training Epoch: 48 [13600/36045]\tLoss: 773.5280\n",
      "Training Epoch: 48 [13650/36045]\tLoss: 855.4336\n",
      "Training Epoch: 48 [13700/36045]\tLoss: 752.9761\n",
      "Training Epoch: 48 [13750/36045]\tLoss: 590.2809\n",
      "Training Epoch: 48 [13800/36045]\tLoss: 560.9561\n",
      "Training Epoch: 48 [13850/36045]\tLoss: 543.7391\n",
      "Training Epoch: 48 [13900/36045]\tLoss: 550.9940\n",
      "Training Epoch: 48 [13950/36045]\tLoss: 596.1052\n",
      "Training Epoch: 48 [14000/36045]\tLoss: 628.6071\n",
      "Training Epoch: 48 [14050/36045]\tLoss: 603.7380\n",
      "Training Epoch: 48 [14100/36045]\tLoss: 598.7527\n",
      "Training Epoch: 48 [14150/36045]\tLoss: 586.9249\n",
      "Training Epoch: 48 [14200/36045]\tLoss: 627.2550\n",
      "Training Epoch: 48 [14250/36045]\tLoss: 689.4712\n",
      "Training Epoch: 48 [14300/36045]\tLoss: 692.5746\n",
      "Training Epoch: 48 [14350/36045]\tLoss: 661.7199\n",
      "Training Epoch: 48 [14400/36045]\tLoss: 647.9122\n",
      "Training Epoch: 48 [14450/36045]\tLoss: 683.2287\n",
      "Training Epoch: 48 [14500/36045]\tLoss: 614.9747\n",
      "Training Epoch: 48 [14550/36045]\tLoss: 642.5640\n",
      "Training Epoch: 48 [14600/36045]\tLoss: 629.1025\n",
      "Training Epoch: 48 [14650/36045]\tLoss: 628.8370\n",
      "Training Epoch: 48 [14700/36045]\tLoss: 596.5401\n",
      "Training Epoch: 48 [14750/36045]\tLoss: 513.3911\n",
      "Training Epoch: 48 [14800/36045]\tLoss: 503.3690\n",
      "Training Epoch: 48 [14850/36045]\tLoss: 510.3433\n",
      "Training Epoch: 48 [14900/36045]\tLoss: 503.8666\n",
      "Training Epoch: 48 [14950/36045]\tLoss: 512.1369\n",
      "Training Epoch: 48 [15000/36045]\tLoss: 524.5273\n",
      "Training Epoch: 48 [15050/36045]\tLoss: 520.9346\n",
      "Training Epoch: 48 [15100/36045]\tLoss: 504.8699\n",
      "Training Epoch: 48 [15150/36045]\tLoss: 500.7497\n",
      "Training Epoch: 48 [15200/36045]\tLoss: 464.1031\n",
      "Training Epoch: 48 [15250/36045]\tLoss: 485.5877\n",
      "Training Epoch: 48 [15300/36045]\tLoss: 471.4016\n",
      "Training Epoch: 48 [15350/36045]\tLoss: 482.8598\n",
      "Training Epoch: 48 [15400/36045]\tLoss: 464.8568\n",
      "Training Epoch: 48 [15450/36045]\tLoss: 450.1875\n",
      "Training Epoch: 48 [15500/36045]\tLoss: 463.0853\n",
      "Training Epoch: 48 [15550/36045]\tLoss: 459.9891\n",
      "Training Epoch: 48 [15600/36045]\tLoss: 526.4168\n",
      "Training Epoch: 48 [15650/36045]\tLoss: 542.7463\n",
      "Training Epoch: 48 [15700/36045]\tLoss: 535.1539\n",
      "Training Epoch: 48 [15750/36045]\tLoss: 526.6297\n",
      "Training Epoch: 48 [15800/36045]\tLoss: 505.8419\n",
      "Training Epoch: 48 [15850/36045]\tLoss: 521.3114\n",
      "Training Epoch: 48 [15900/36045]\tLoss: 530.0398\n",
      "Training Epoch: 48 [15950/36045]\tLoss: 549.7720\n",
      "Training Epoch: 48 [16000/36045]\tLoss: 520.3315\n",
      "Training Epoch: 48 [16050/36045]\tLoss: 489.9474\n",
      "Training Epoch: 48 [16100/36045]\tLoss: 454.8215\n",
      "Training Epoch: 48 [16150/36045]\tLoss: 442.9557\n",
      "Training Epoch: 48 [16200/36045]\tLoss: 538.3372\n",
      "Training Epoch: 48 [16250/36045]\tLoss: 565.5424\n",
      "Training Epoch: 48 [16300/36045]\tLoss: 617.3236\n",
      "Training Epoch: 48 [16350/36045]\tLoss: 637.7923\n",
      "Training Epoch: 48 [16400/36045]\tLoss: 609.6868\n",
      "Training Epoch: 48 [16450/36045]\tLoss: 592.1662\n",
      "Training Epoch: 48 [16500/36045]\tLoss: 591.9947\n",
      "Training Epoch: 48 [16550/36045]\tLoss: 557.5825\n",
      "Training Epoch: 48 [16600/36045]\tLoss: 579.0671\n",
      "Training Epoch: 48 [16650/36045]\tLoss: 594.8072\n",
      "Training Epoch: 48 [16700/36045]\tLoss: 575.4560\n",
      "Training Epoch: 48 [16750/36045]\tLoss: 568.2200\n",
      "Training Epoch: 48 [16800/36045]\tLoss: 576.5261\n",
      "Training Epoch: 48 [16850/36045]\tLoss: 549.5537\n",
      "Training Epoch: 48 [16900/36045]\tLoss: 559.3050\n",
      "Training Epoch: 48 [16950/36045]\tLoss: 581.8406\n",
      "Training Epoch: 48 [17000/36045]\tLoss: 566.5428\n",
      "Training Epoch: 48 [17050/36045]\tLoss: 589.7910\n",
      "Training Epoch: 48 [17100/36045]\tLoss: 585.6113\n",
      "Training Epoch: 48 [17150/36045]\tLoss: 508.0576\n",
      "Training Epoch: 48 [17200/36045]\tLoss: 470.6188\n",
      "Training Epoch: 48 [17250/36045]\tLoss: 493.0356\n",
      "Training Epoch: 48 [17300/36045]\tLoss: 521.9125\n",
      "Training Epoch: 48 [17350/36045]\tLoss: 503.7015\n",
      "Training Epoch: 48 [17400/36045]\tLoss: 523.2914\n",
      "Training Epoch: 48 [17450/36045]\tLoss: 541.3953\n",
      "Training Epoch: 48 [17500/36045]\tLoss: 529.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 48 [17550/36045]\tLoss: 528.3968\n",
      "Training Epoch: 48 [17600/36045]\tLoss: 522.9266\n",
      "Training Epoch: 48 [17650/36045]\tLoss: 538.0140\n",
      "Training Epoch: 48 [17700/36045]\tLoss: 517.5754\n",
      "Training Epoch: 48 [17750/36045]\tLoss: 533.2214\n",
      "Training Epoch: 48 [17800/36045]\tLoss: 524.6440\n",
      "Training Epoch: 48 [17850/36045]\tLoss: 541.8196\n",
      "Training Epoch: 48 [17900/36045]\tLoss: 569.5375\n",
      "Training Epoch: 48 [17950/36045]\tLoss: 581.8641\n",
      "Training Epoch: 48 [18000/36045]\tLoss: 572.8631\n",
      "Training Epoch: 48 [18050/36045]\tLoss: 627.8698\n",
      "Training Epoch: 48 [18100/36045]\tLoss: 628.6255\n",
      "Training Epoch: 48 [18150/36045]\tLoss: 639.8059\n",
      "Training Epoch: 48 [18200/36045]\tLoss: 622.4080\n",
      "Training Epoch: 48 [18250/36045]\tLoss: 643.0200\n",
      "Training Epoch: 48 [18300/36045]\tLoss: 600.2252\n",
      "Training Epoch: 48 [18350/36045]\tLoss: 673.5981\n",
      "Training Epoch: 48 [18400/36045]\tLoss: 646.2655\n",
      "Training Epoch: 48 [18450/36045]\tLoss: 626.1143\n",
      "Training Epoch: 48 [18500/36045]\tLoss: 624.9481\n",
      "Training Epoch: 48 [18550/36045]\tLoss: 612.6918\n",
      "Training Epoch: 48 [18600/36045]\tLoss: 602.4459\n",
      "Training Epoch: 48 [18650/36045]\tLoss: 647.5318\n",
      "Training Epoch: 48 [18700/36045]\tLoss: 681.1553\n",
      "Training Epoch: 48 [18750/36045]\tLoss: 668.9616\n",
      "Training Epoch: 48 [18800/36045]\tLoss: 691.5137\n",
      "Training Epoch: 48 [18850/36045]\tLoss: 636.7575\n",
      "Training Epoch: 48 [18900/36045]\tLoss: 681.0156\n",
      "Training Epoch: 48 [18950/36045]\tLoss: 623.7507\n",
      "Training Epoch: 48 [19000/36045]\tLoss: 510.4765\n",
      "Training Epoch: 48 [19050/36045]\tLoss: 495.5301\n",
      "Training Epoch: 48 [19100/36045]\tLoss: 503.4625\n",
      "Training Epoch: 48 [19150/36045]\tLoss: 493.7537\n",
      "Training Epoch: 48 [19200/36045]\tLoss: 525.1070\n",
      "Training Epoch: 48 [19250/36045]\tLoss: 540.4677\n",
      "Training Epoch: 48 [19300/36045]\tLoss: 549.7175\n",
      "Training Epoch: 48 [19350/36045]\tLoss: 533.6468\n",
      "Training Epoch: 48 [19400/36045]\tLoss: 553.4196\n",
      "Training Epoch: 48 [19450/36045]\tLoss: 544.9122\n",
      "Training Epoch: 48 [19500/36045]\tLoss: 545.9506\n",
      "Training Epoch: 48 [19550/36045]\tLoss: 544.1470\n",
      "Training Epoch: 48 [19600/36045]\tLoss: 584.9032\n",
      "Training Epoch: 48 [19650/36045]\tLoss: 784.1630\n",
      "Training Epoch: 48 [19700/36045]\tLoss: 742.8254\n",
      "Training Epoch: 48 [19750/36045]\tLoss: 746.9543\n",
      "Training Epoch: 48 [19800/36045]\tLoss: 747.5829\n",
      "Training Epoch: 48 [19850/36045]\tLoss: 486.8533\n",
      "Training Epoch: 48 [19900/36045]\tLoss: 466.4030\n",
      "Training Epoch: 48 [19950/36045]\tLoss: 470.4026\n",
      "Training Epoch: 48 [20000/36045]\tLoss: 470.5325\n",
      "Training Epoch: 48 [20050/36045]\tLoss: 526.3146\n",
      "Training Epoch: 48 [20100/36045]\tLoss: 531.4980\n",
      "Training Epoch: 48 [20150/36045]\tLoss: 532.4155\n",
      "Training Epoch: 48 [20200/36045]\tLoss: 532.2695\n",
      "Training Epoch: 48 [20250/36045]\tLoss: 568.2475\n",
      "Training Epoch: 48 [20300/36045]\tLoss: 604.3688\n",
      "Training Epoch: 48 [20350/36045]\tLoss: 622.2014\n",
      "Training Epoch: 48 [20400/36045]\tLoss: 638.3158\n",
      "Training Epoch: 48 [20450/36045]\tLoss: 608.0683\n",
      "Training Epoch: 48 [20500/36045]\tLoss: 593.2713\n",
      "Training Epoch: 48 [20550/36045]\tLoss: 520.8311\n",
      "Training Epoch: 48 [20600/36045]\tLoss: 530.3405\n",
      "Training Epoch: 48 [20650/36045]\tLoss: 527.5546\n",
      "Training Epoch: 48 [20700/36045]\tLoss: 515.6865\n",
      "Training Epoch: 48 [20750/36045]\tLoss: 556.4524\n",
      "Training Epoch: 48 [20800/36045]\tLoss: 604.6680\n",
      "Training Epoch: 48 [20850/36045]\tLoss: 591.5832\n",
      "Training Epoch: 48 [20900/36045]\tLoss: 634.0220\n",
      "Training Epoch: 48 [20950/36045]\tLoss: 597.1759\n",
      "Training Epoch: 48 [21000/36045]\tLoss: 562.1151\n",
      "Training Epoch: 48 [21050/36045]\tLoss: 480.7669\n",
      "Training Epoch: 48 [21100/36045]\tLoss: 486.0520\n",
      "Training Epoch: 48 [21150/36045]\tLoss: 520.1737\n",
      "Training Epoch: 48 [21200/36045]\tLoss: 519.1533\n",
      "Training Epoch: 48 [21250/36045]\tLoss: 496.4224\n",
      "Training Epoch: 48 [21300/36045]\tLoss: 579.3661\n",
      "Training Epoch: 48 [21350/36045]\tLoss: 571.1772\n",
      "Training Epoch: 48 [21400/36045]\tLoss: 574.9937\n",
      "Training Epoch: 48 [21450/36045]\tLoss: 581.1652\n",
      "Training Epoch: 48 [21500/36045]\tLoss: 582.5645\n",
      "Training Epoch: 48 [21550/36045]\tLoss: 676.5807\n",
      "Training Epoch: 48 [21600/36045]\tLoss: 674.9142\n",
      "Training Epoch: 48 [21650/36045]\tLoss: 687.2441\n",
      "Training Epoch: 48 [21700/36045]\tLoss: 690.7844\n",
      "Training Epoch: 48 [21750/36045]\tLoss: 663.0120\n",
      "Training Epoch: 48 [21800/36045]\tLoss: 486.2200\n",
      "Training Epoch: 48 [21850/36045]\tLoss: 469.1866\n",
      "Training Epoch: 48 [21900/36045]\tLoss: 478.3678\n",
      "Training Epoch: 48 [21950/36045]\tLoss: 480.1293\n",
      "Training Epoch: 48 [22000/36045]\tLoss: 483.3354\n",
      "Training Epoch: 48 [22050/36045]\tLoss: 502.3327\n",
      "Training Epoch: 48 [22100/36045]\tLoss: 494.7216\n",
      "Training Epoch: 48 [22150/36045]\tLoss: 481.4075\n",
      "Training Epoch: 48 [22200/36045]\tLoss: 496.9659\n",
      "Training Epoch: 48 [22250/36045]\tLoss: 501.9521\n",
      "Training Epoch: 48 [22300/36045]\tLoss: 553.6107\n",
      "Training Epoch: 48 [22350/36045]\tLoss: 578.9279\n",
      "Training Epoch: 48 [22400/36045]\tLoss: 592.4368\n",
      "Training Epoch: 48 [22450/36045]\tLoss: 579.8799\n",
      "Training Epoch: 48 [22500/36045]\tLoss: 563.2831\n",
      "Training Epoch: 48 [22550/36045]\tLoss: 597.8412\n",
      "Training Epoch: 48 [22600/36045]\tLoss: 645.3027\n",
      "Training Epoch: 48 [22650/36045]\tLoss: 677.7435\n",
      "Training Epoch: 48 [22700/36045]\tLoss: 698.9101\n",
      "Training Epoch: 48 [22750/36045]\tLoss: 718.5780\n",
      "Training Epoch: 48 [22800/36045]\tLoss: 746.5706\n",
      "Training Epoch: 48 [22850/36045]\tLoss: 619.4410\n",
      "Training Epoch: 48 [22900/36045]\tLoss: 624.0624\n",
      "Training Epoch: 48 [22950/36045]\tLoss: 603.7709\n",
      "Training Epoch: 48 [23000/36045]\tLoss: 600.2405\n",
      "Training Epoch: 48 [23050/36045]\tLoss: 533.3695\n",
      "Training Epoch: 48 [23100/36045]\tLoss: 548.8293\n",
      "Training Epoch: 48 [23150/36045]\tLoss: 537.0385\n",
      "Training Epoch: 48 [23200/36045]\tLoss: 508.1721\n",
      "Training Epoch: 48 [23250/36045]\tLoss: 511.2067\n",
      "Training Epoch: 48 [23300/36045]\tLoss: 507.2060\n",
      "Training Epoch: 48 [23350/36045]\tLoss: 527.3268\n",
      "Training Epoch: 48 [23400/36045]\tLoss: 571.9594\n",
      "Training Epoch: 48 [23450/36045]\tLoss: 565.6610\n",
      "Training Epoch: 48 [23500/36045]\tLoss: 544.9501\n",
      "Training Epoch: 48 [23550/36045]\tLoss: 584.0275\n",
      "Training Epoch: 48 [23600/36045]\tLoss: 663.8171\n",
      "Training Epoch: 48 [23650/36045]\tLoss: 674.7710\n",
      "Training Epoch: 48 [23700/36045]\tLoss: 682.7125\n",
      "Training Epoch: 48 [23750/36045]\tLoss: 659.1444\n",
      "Training Epoch: 48 [23800/36045]\tLoss: 529.2680\n",
      "Training Epoch: 48 [23850/36045]\tLoss: 554.8342\n",
      "Training Epoch: 48 [23900/36045]\tLoss: 544.7856\n",
      "Training Epoch: 48 [23950/36045]\tLoss: 528.0374\n",
      "Training Epoch: 48 [24000/36045]\tLoss: 505.3543\n",
      "Training Epoch: 48 [24050/36045]\tLoss: 466.2207\n",
      "Training Epoch: 48 [24100/36045]\tLoss: 490.4086\n",
      "Training Epoch: 48 [24150/36045]\tLoss: 482.3561\n",
      "Training Epoch: 48 [24200/36045]\tLoss: 480.2724\n",
      "Training Epoch: 48 [24250/36045]\tLoss: 466.5269\n",
      "Training Epoch: 48 [24300/36045]\tLoss: 503.8454\n",
      "Training Epoch: 48 [24350/36045]\tLoss: 515.7206\n",
      "Training Epoch: 48 [24400/36045]\tLoss: 530.5456\n",
      "Training Epoch: 48 [24450/36045]\tLoss: 505.6232\n",
      "Training Epoch: 48 [24500/36045]\tLoss: 533.9193\n",
      "Training Epoch: 48 [24550/36045]\tLoss: 619.9848\n",
      "Training Epoch: 48 [24600/36045]\tLoss: 611.0478\n",
      "Training Epoch: 48 [24650/36045]\tLoss: 584.6611\n",
      "Training Epoch: 48 [24700/36045]\tLoss: 594.6488\n",
      "Training Epoch: 48 [24750/36045]\tLoss: 549.8751\n",
      "Training Epoch: 48 [24800/36045]\tLoss: 449.4788\n",
      "Training Epoch: 48 [24850/36045]\tLoss: 467.2097\n",
      "Training Epoch: 48 [24900/36045]\tLoss: 464.7505\n",
      "Training Epoch: 48 [24950/36045]\tLoss: 467.2758\n",
      "Training Epoch: 48 [25000/36045]\tLoss: 449.5414\n",
      "Training Epoch: 48 [25050/36045]\tLoss: 429.8603\n",
      "Training Epoch: 48 [25100/36045]\tLoss: 384.9801\n",
      "Training Epoch: 48 [25150/36045]\tLoss: 356.2837\n",
      "Training Epoch: 48 [25200/36045]\tLoss: 351.2432\n",
      "Training Epoch: 48 [25250/36045]\tLoss: 376.8037\n",
      "Training Epoch: 48 [25300/36045]\tLoss: 495.3117\n",
      "Training Epoch: 48 [25350/36045]\tLoss: 491.4928\n",
      "Training Epoch: 48 [25400/36045]\tLoss: 459.4943\n",
      "Training Epoch: 48 [25450/36045]\tLoss: 461.3218\n",
      "Training Epoch: 48 [25500/36045]\tLoss: 501.0736\n",
      "Training Epoch: 48 [25550/36045]\tLoss: 586.0804\n",
      "Training Epoch: 48 [25600/36045]\tLoss: 590.5106\n",
      "Training Epoch: 48 [25650/36045]\tLoss: 570.1240\n",
      "Training Epoch: 48 [25700/36045]\tLoss: 579.6668\n",
      "Training Epoch: 48 [25750/36045]\tLoss: 560.1347\n",
      "Training Epoch: 48 [25800/36045]\tLoss: 353.0364\n",
      "Training Epoch: 48 [25850/36045]\tLoss: 360.6864\n",
      "Training Epoch: 48 [25900/36045]\tLoss: 342.9437\n",
      "Training Epoch: 48 [25950/36045]\tLoss: 352.9395\n",
      "Training Epoch: 48 [26000/36045]\tLoss: 434.4048\n",
      "Training Epoch: 48 [26050/36045]\tLoss: 590.2183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 48 [26100/36045]\tLoss: 615.3799\n",
      "Training Epoch: 48 [26150/36045]\tLoss: 614.7402\n",
      "Training Epoch: 48 [26200/36045]\tLoss: 589.0445\n",
      "Training Epoch: 48 [26250/36045]\tLoss: 619.3907\n",
      "Training Epoch: 48 [26300/36045]\tLoss: 570.6386\n",
      "Training Epoch: 48 [26350/36045]\tLoss: 580.7744\n",
      "Training Epoch: 48 [26400/36045]\tLoss: 555.5477\n",
      "Training Epoch: 48 [26450/36045]\tLoss: 486.6489\n",
      "Training Epoch: 48 [26500/36045]\tLoss: 576.4244\n",
      "Training Epoch: 48 [26550/36045]\tLoss: 575.4104\n",
      "Training Epoch: 48 [26600/36045]\tLoss: 571.0999\n",
      "Training Epoch: 48 [26650/36045]\tLoss: 585.4218\n",
      "Training Epoch: 48 [26700/36045]\tLoss: 565.0205\n",
      "Training Epoch: 48 [26750/36045]\tLoss: 530.6682\n",
      "Training Epoch: 48 [26800/36045]\tLoss: 392.3607\n",
      "Training Epoch: 48 [26850/36045]\tLoss: 324.2907\n",
      "Training Epoch: 48 [26900/36045]\tLoss: 325.7202\n",
      "Training Epoch: 48 [26950/36045]\tLoss: 357.3784\n",
      "Training Epoch: 48 [27000/36045]\tLoss: 591.4298\n",
      "Training Epoch: 48 [27050/36045]\tLoss: 617.4131\n",
      "Training Epoch: 48 [27100/36045]\tLoss: 598.2681\n",
      "Training Epoch: 48 [27150/36045]\tLoss: 636.4380\n",
      "Training Epoch: 48 [27200/36045]\tLoss: 460.6469\n",
      "Training Epoch: 48 [27250/36045]\tLoss: 451.1140\n",
      "Training Epoch: 48 [27300/36045]\tLoss: 440.1786\n",
      "Training Epoch: 48 [27350/36045]\tLoss: 437.0725\n",
      "Training Epoch: 48 [27400/36045]\tLoss: 436.4358\n",
      "Training Epoch: 48 [27450/36045]\tLoss: 553.1989\n",
      "Training Epoch: 48 [27500/36045]\tLoss: 592.8509\n",
      "Training Epoch: 48 [27550/36045]\tLoss: 586.1192\n",
      "Training Epoch: 48 [27600/36045]\tLoss: 597.4608\n",
      "Training Epoch: 48 [27650/36045]\tLoss: 589.4817\n",
      "Training Epoch: 48 [27700/36045]\tLoss: 616.3538\n",
      "Training Epoch: 48 [27750/36045]\tLoss: 628.1271\n",
      "Training Epoch: 48 [27800/36045]\tLoss: 615.6097\n",
      "Training Epoch: 48 [27850/36045]\tLoss: 606.0354\n",
      "Training Epoch: 48 [27900/36045]\tLoss: 552.0079\n",
      "Training Epoch: 48 [27950/36045]\tLoss: 462.5690\n",
      "Training Epoch: 48 [28000/36045]\tLoss: 439.5056\n",
      "Training Epoch: 48 [28050/36045]\tLoss: 448.8693\n",
      "Training Epoch: 48 [28100/36045]\tLoss: 440.5857\n",
      "Training Epoch: 48 [28150/36045]\tLoss: 458.6827\n",
      "Training Epoch: 48 [28200/36045]\tLoss: 467.3046\n",
      "Training Epoch: 48 [28250/36045]\tLoss: 460.0090\n",
      "Training Epoch: 48 [28300/36045]\tLoss: 436.7399\n",
      "Training Epoch: 48 [28350/36045]\tLoss: 432.5149\n",
      "Training Epoch: 48 [28400/36045]\tLoss: 759.1801\n",
      "Training Epoch: 48 [28450/36045]\tLoss: 697.3802\n",
      "Training Epoch: 48 [28500/36045]\tLoss: 602.5589\n",
      "Training Epoch: 48 [28550/36045]\tLoss: 554.2939\n",
      "Training Epoch: 48 [28600/36045]\tLoss: 577.3741\n",
      "Training Epoch: 48 [28650/36045]\tLoss: 628.4005\n",
      "Training Epoch: 48 [28700/36045]\tLoss: 621.9876\n",
      "Training Epoch: 48 [28750/36045]\tLoss: 609.1746\n",
      "Training Epoch: 48 [28800/36045]\tLoss: 619.4698\n",
      "Training Epoch: 48 [28850/36045]\tLoss: 538.3512\n",
      "Training Epoch: 48 [28900/36045]\tLoss: 438.4378\n",
      "Training Epoch: 48 [28950/36045]\tLoss: 437.5650\n",
      "Training Epoch: 48 [29000/36045]\tLoss: 433.3299\n",
      "Training Epoch: 48 [29050/36045]\tLoss: 440.4550\n",
      "Training Epoch: 48 [29100/36045]\tLoss: 458.2094\n",
      "Training Epoch: 48 [29150/36045]\tLoss: 448.9825\n",
      "Training Epoch: 48 [29200/36045]\tLoss: 434.7212\n",
      "Training Epoch: 48 [29250/36045]\tLoss: 424.6346\n",
      "Training Epoch: 48 [29300/36045]\tLoss: 476.4293\n",
      "Training Epoch: 48 [29350/36045]\tLoss: 558.6788\n",
      "Training Epoch: 48 [29400/36045]\tLoss: 576.2760\n",
      "Training Epoch: 48 [29450/36045]\tLoss: 592.5674\n",
      "Training Epoch: 48 [29500/36045]\tLoss: 607.3068\n",
      "Training Epoch: 48 [29550/36045]\tLoss: 577.4110\n",
      "Training Epoch: 48 [29600/36045]\tLoss: 485.3045\n",
      "Training Epoch: 48 [29650/36045]\tLoss: 466.8887\n",
      "Training Epoch: 48 [29700/36045]\tLoss: 420.0010\n",
      "Training Epoch: 48 [29750/36045]\tLoss: 418.2770\n",
      "Training Epoch: 48 [29800/36045]\tLoss: 465.1196\n",
      "Training Epoch: 48 [29850/36045]\tLoss: 542.7479\n",
      "Training Epoch: 48 [29900/36045]\tLoss: 538.1016\n",
      "Training Epoch: 48 [29950/36045]\tLoss: 560.0057\n",
      "Training Epoch: 48 [30000/36045]\tLoss: 533.8891\n",
      "Training Epoch: 48 [30050/36045]\tLoss: 541.2686\n",
      "Training Epoch: 48 [30100/36045]\tLoss: 659.9816\n",
      "Training Epoch: 48 [30150/36045]\tLoss: 641.9604\n",
      "Training Epoch: 48 [30200/36045]\tLoss: 605.3666\n",
      "Training Epoch: 48 [30250/36045]\tLoss: 653.9063\n",
      "Training Epoch: 48 [30300/36045]\tLoss: 638.0270\n",
      "Training Epoch: 48 [30350/36045]\tLoss: 484.8428\n",
      "Training Epoch: 48 [30400/36045]\tLoss: 469.5727\n",
      "Training Epoch: 48 [30450/36045]\tLoss: 471.9693\n",
      "Training Epoch: 48 [30500/36045]\tLoss: 440.5193\n",
      "Training Epoch: 48 [30550/36045]\tLoss: 408.9311\n",
      "Training Epoch: 48 [30600/36045]\tLoss: 402.0805\n",
      "Training Epoch: 48 [30650/36045]\tLoss: 392.2510\n",
      "Training Epoch: 48 [30700/36045]\tLoss: 409.8162\n",
      "Training Epoch: 48 [30750/36045]\tLoss: 396.8353\n",
      "Training Epoch: 48 [30800/36045]\tLoss: 422.9696\n",
      "Training Epoch: 48 [30850/36045]\tLoss: 414.7945\n",
      "Training Epoch: 48 [30900/36045]\tLoss: 426.3865\n",
      "Training Epoch: 48 [30950/36045]\tLoss: 448.1692\n",
      "Training Epoch: 48 [31000/36045]\tLoss: 440.6745\n",
      "Training Epoch: 48 [31050/36045]\tLoss: 367.7743\n",
      "Training Epoch: 48 [31100/36045]\tLoss: 358.5574\n",
      "Training Epoch: 48 [31150/36045]\tLoss: 366.2664\n",
      "Training Epoch: 48 [31200/36045]\tLoss: 454.5219\n",
      "Training Epoch: 48 [31250/36045]\tLoss: 589.7297\n",
      "Training Epoch: 48 [31300/36045]\tLoss: 561.0853\n",
      "Training Epoch: 48 [31350/36045]\tLoss: 577.2235\n",
      "Training Epoch: 48 [31400/36045]\tLoss: 556.8337\n",
      "Training Epoch: 48 [31450/36045]\tLoss: 573.6009\n",
      "Training Epoch: 48 [31500/36045]\tLoss: 586.0146\n",
      "Training Epoch: 48 [31550/36045]\tLoss: 592.5604\n",
      "Training Epoch: 48 [31600/36045]\tLoss: 557.1119\n",
      "Training Epoch: 48 [31650/36045]\tLoss: 596.7679\n",
      "Training Epoch: 48 [31700/36045]\tLoss: 431.3748\n",
      "Training Epoch: 48 [31750/36045]\tLoss: 356.2303\n",
      "Training Epoch: 48 [31800/36045]\tLoss: 340.0307\n",
      "Training Epoch: 48 [31850/36045]\tLoss: 347.5693\n",
      "Training Epoch: 48 [31900/36045]\tLoss: 551.5361\n",
      "Training Epoch: 48 [31950/36045]\tLoss: 715.6361\n",
      "Training Epoch: 48 [32000/36045]\tLoss: 822.4236\n",
      "Training Epoch: 48 [32050/36045]\tLoss: 778.4419\n",
      "Training Epoch: 48 [32100/36045]\tLoss: 770.0305\n",
      "Training Epoch: 48 [32150/36045]\tLoss: 588.4659\n",
      "Training Epoch: 48 [32200/36045]\tLoss: 590.0784\n",
      "Training Epoch: 48 [32250/36045]\tLoss: 600.1758\n",
      "Training Epoch: 48 [32300/36045]\tLoss: 582.4252\n",
      "Training Epoch: 48 [32350/36045]\tLoss: 578.5955\n",
      "Training Epoch: 48 [32400/36045]\tLoss: 542.9309\n",
      "Training Epoch: 48 [32450/36045]\tLoss: 447.1186\n",
      "Training Epoch: 48 [32500/36045]\tLoss: 429.5392\n",
      "Training Epoch: 48 [32550/36045]\tLoss: 431.5797\n",
      "Training Epoch: 48 [32600/36045]\tLoss: 428.7594\n",
      "Training Epoch: 48 [32650/36045]\tLoss: 557.2703\n",
      "Training Epoch: 48 [32700/36045]\tLoss: 608.8376\n",
      "Training Epoch: 48 [32750/36045]\tLoss: 579.5939\n",
      "Training Epoch: 48 [32800/36045]\tLoss: 594.1537\n",
      "Training Epoch: 48 [32850/36045]\tLoss: 548.4795\n",
      "Training Epoch: 48 [32900/36045]\tLoss: 437.7630\n",
      "Training Epoch: 48 [32950/36045]\tLoss: 458.6335\n",
      "Training Epoch: 48 [33000/36045]\tLoss: 457.0914\n",
      "Training Epoch: 48 [33050/36045]\tLoss: 435.1275\n",
      "Training Epoch: 48 [33100/36045]\tLoss: 494.4849\n",
      "Training Epoch: 48 [33150/36045]\tLoss: 673.7014\n",
      "Training Epoch: 48 [33200/36045]\tLoss: 655.8251\n",
      "Training Epoch: 48 [33250/36045]\tLoss: 675.7356\n",
      "Training Epoch: 48 [33300/36045]\tLoss: 720.4568\n",
      "Training Epoch: 48 [33350/36045]\tLoss: 551.2336\n",
      "Training Epoch: 48 [33400/36045]\tLoss: 400.5805\n",
      "Training Epoch: 48 [33450/36045]\tLoss: 396.7155\n",
      "Training Epoch: 48 [33500/36045]\tLoss: 408.3116\n",
      "Training Epoch: 48 [33550/36045]\tLoss: 422.6043\n",
      "Training Epoch: 48 [33600/36045]\tLoss: 424.4087\n",
      "Training Epoch: 48 [33650/36045]\tLoss: 567.3768\n",
      "Training Epoch: 48 [33700/36045]\tLoss: 548.8260\n",
      "Training Epoch: 48 [33750/36045]\tLoss: 568.3481\n",
      "Training Epoch: 48 [33800/36045]\tLoss: 565.9338\n",
      "Training Epoch: 48 [33850/36045]\tLoss: 567.5630\n",
      "Training Epoch: 48 [33900/36045]\tLoss: 581.1943\n",
      "Training Epoch: 48 [33950/36045]\tLoss: 590.5627\n",
      "Training Epoch: 48 [34000/36045]\tLoss: 577.3349\n",
      "Training Epoch: 48 [34050/36045]\tLoss: 580.9860\n",
      "Training Epoch: 48 [34100/36045]\tLoss: 560.3579\n",
      "Training Epoch: 48 [34150/36045]\tLoss: 518.7001\n",
      "Training Epoch: 48 [34200/36045]\tLoss: 491.3453\n",
      "Training Epoch: 48 [34250/36045]\tLoss: 505.0647\n",
      "Training Epoch: 48 [34300/36045]\tLoss: 430.8745\n",
      "Training Epoch: 48 [34350/36045]\tLoss: 453.9293\n",
      "Training Epoch: 48 [34400/36045]\tLoss: 447.3389\n",
      "Training Epoch: 48 [34450/36045]\tLoss: 421.3347\n",
      "Training Epoch: 48 [34500/36045]\tLoss: 449.3379\n",
      "Training Epoch: 48 [34550/36045]\tLoss: 441.1205\n",
      "Training Epoch: 48 [34600/36045]\tLoss: 448.5802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 48 [34650/36045]\tLoss: 554.0850\n",
      "Training Epoch: 48 [34700/36045]\tLoss: 587.8838\n",
      "Training Epoch: 48 [34750/36045]\tLoss: 520.9160\n",
      "Training Epoch: 48 [34800/36045]\tLoss: 598.6215\n",
      "Training Epoch: 48 [34850/36045]\tLoss: 605.4591\n",
      "Training Epoch: 48 [34900/36045]\tLoss: 650.4926\n",
      "Training Epoch: 48 [34950/36045]\tLoss: 635.0010\n",
      "Training Epoch: 48 [35000/36045]\tLoss: 636.2748\n",
      "Training Epoch: 48 [35050/36045]\tLoss: 623.8366\n",
      "Training Epoch: 48 [35100/36045]\tLoss: 540.7244\n",
      "Training Epoch: 48 [35150/36045]\tLoss: 532.5318\n",
      "Training Epoch: 48 [35200/36045]\tLoss: 447.1236\n",
      "Training Epoch: 48 [35250/36045]\tLoss: 491.7119\n",
      "Training Epoch: 48 [35300/36045]\tLoss: 508.3427\n",
      "Training Epoch: 48 [35350/36045]\tLoss: 566.5703\n",
      "Training Epoch: 48 [35400/36045]\tLoss: 595.0757\n",
      "Training Epoch: 48 [35450/36045]\tLoss: 566.6351\n",
      "Training Epoch: 48 [35500/36045]\tLoss: 548.0814\n",
      "Training Epoch: 48 [35550/36045]\tLoss: 534.9714\n",
      "Training Epoch: 48 [35600/36045]\tLoss: 586.1818\n",
      "Training Epoch: 48 [35650/36045]\tLoss: 657.9736\n",
      "Training Epoch: 48 [35700/36045]\tLoss: 584.5690\n",
      "Training Epoch: 48 [35750/36045]\tLoss: 641.9483\n",
      "Training Epoch: 48 [35800/36045]\tLoss: 648.9136\n",
      "Training Epoch: 48 [35850/36045]\tLoss: 623.0583\n",
      "Training Epoch: 48 [35900/36045]\tLoss: 644.0183\n",
      "Training Epoch: 48 [35950/36045]\tLoss: 640.3547\n",
      "Training Epoch: 48 [36000/36045]\tLoss: 633.9128\n",
      "Training Epoch: 48 [36045/36045]\tLoss: 620.0295\n",
      "Training Epoch: 48 [4004/4004]\tLoss: 575.6079\n",
      "Training Epoch: 49 [50/36045]\tLoss: 573.8282\n",
      "Training Epoch: 49 [100/36045]\tLoss: 549.3600\n",
      "Training Epoch: 49 [150/36045]\tLoss: 546.9824\n",
      "Training Epoch: 49 [200/36045]\tLoss: 533.3708\n",
      "Training Epoch: 49 [250/36045]\tLoss: 642.6143\n",
      "Training Epoch: 49 [300/36045]\tLoss: 708.1222\n",
      "Training Epoch: 49 [350/36045]\tLoss: 675.0039\n",
      "Training Epoch: 49 [400/36045]\tLoss: 668.8724\n",
      "Training Epoch: 49 [450/36045]\tLoss: 649.6794\n",
      "Training Epoch: 49 [500/36045]\tLoss: 600.7514\n",
      "Training Epoch: 49 [550/36045]\tLoss: 603.9040\n",
      "Training Epoch: 49 [600/36045]\tLoss: 590.4852\n",
      "Training Epoch: 49 [650/36045]\tLoss: 611.1418\n",
      "Training Epoch: 49 [700/36045]\tLoss: 595.9200\n",
      "Training Epoch: 49 [750/36045]\tLoss: 571.4184\n",
      "Training Epoch: 49 [800/36045]\tLoss: 583.0733\n",
      "Training Epoch: 49 [850/36045]\tLoss: 566.9549\n",
      "Training Epoch: 49 [900/36045]\tLoss: 543.1818\n",
      "Training Epoch: 49 [950/36045]\tLoss: 513.2761\n",
      "Training Epoch: 49 [1000/36045]\tLoss: 497.6848\n",
      "Training Epoch: 49 [1050/36045]\tLoss: 499.8541\n",
      "Training Epoch: 49 [1100/36045]\tLoss: 486.6598\n",
      "Training Epoch: 49 [1150/36045]\tLoss: 496.2322\n",
      "Training Epoch: 49 [1200/36045]\tLoss: 525.5172\n",
      "Training Epoch: 49 [1250/36045]\tLoss: 600.8406\n",
      "Training Epoch: 49 [1300/36045]\tLoss: 607.7866\n",
      "Training Epoch: 49 [1350/36045]\tLoss: 609.0801\n",
      "Training Epoch: 49 [1400/36045]\tLoss: 632.5822\n",
      "Training Epoch: 49 [1450/36045]\tLoss: 612.2927\n",
      "Training Epoch: 49 [1500/36045]\tLoss: 559.0607\n",
      "Training Epoch: 49 [1550/36045]\tLoss: 573.2474\n",
      "Training Epoch: 49 [1600/36045]\tLoss: 582.9720\n",
      "Training Epoch: 49 [1650/36045]\tLoss: 570.6255\n",
      "Training Epoch: 49 [1700/36045]\tLoss: 582.9889\n",
      "Training Epoch: 49 [1750/36045]\tLoss: 624.6118\n",
      "Training Epoch: 49 [1800/36045]\tLoss: 607.5907\n",
      "Training Epoch: 49 [1850/36045]\tLoss: 622.9827\n",
      "Training Epoch: 49 [1900/36045]\tLoss: 582.3616\n",
      "Training Epoch: 49 [1950/36045]\tLoss: 591.9976\n",
      "Training Epoch: 49 [2000/36045]\tLoss: 532.7525\n",
      "Training Epoch: 49 [2050/36045]\tLoss: 535.5908\n",
      "Training Epoch: 49 [2100/36045]\tLoss: 564.5404\n",
      "Training Epoch: 49 [2150/36045]\tLoss: 545.5660\n",
      "Training Epoch: 49 [2200/36045]\tLoss: 508.5393\n",
      "Training Epoch: 49 [2250/36045]\tLoss: 480.7373\n",
      "Training Epoch: 49 [2300/36045]\tLoss: 504.1909\n",
      "Training Epoch: 49 [2350/36045]\tLoss: 482.0897\n",
      "Training Epoch: 49 [2400/36045]\tLoss: 489.4206\n",
      "Training Epoch: 49 [2450/36045]\tLoss: 627.1695\n",
      "Training Epoch: 49 [2500/36045]\tLoss: 658.5019\n",
      "Training Epoch: 49 [2550/36045]\tLoss: 656.5316\n",
      "Training Epoch: 49 [2600/36045]\tLoss: 665.3240\n",
      "Training Epoch: 49 [2650/36045]\tLoss: 789.3248\n",
      "Training Epoch: 49 [2700/36045]\tLoss: 877.2967\n",
      "Training Epoch: 49 [2750/36045]\tLoss: 947.9074\n",
      "Training Epoch: 49 [2800/36045]\tLoss: 957.1010\n",
      "Training Epoch: 49 [2850/36045]\tLoss: 724.0079\n",
      "Training Epoch: 49 [2900/36045]\tLoss: 684.5987\n",
      "Training Epoch: 49 [2950/36045]\tLoss: 662.4921\n",
      "Training Epoch: 49 [3000/36045]\tLoss: 656.2191\n",
      "Training Epoch: 49 [3050/36045]\tLoss: 686.8004\n",
      "Training Epoch: 49 [3100/36045]\tLoss: 627.3159\n",
      "Training Epoch: 49 [3150/36045]\tLoss: 482.0820\n",
      "Training Epoch: 49 [3200/36045]\tLoss: 499.1317\n",
      "Training Epoch: 49 [3250/36045]\tLoss: 471.2831\n",
      "Training Epoch: 49 [3300/36045]\tLoss: 446.3129\n",
      "Training Epoch: 49 [3350/36045]\tLoss: 471.4225\n",
      "Training Epoch: 49 [3400/36045]\tLoss: 493.7660\n",
      "Training Epoch: 49 [3450/36045]\tLoss: 529.4512\n",
      "Training Epoch: 49 [3500/36045]\tLoss: 516.9343\n",
      "Training Epoch: 49 [3550/36045]\tLoss: 494.2841\n",
      "Training Epoch: 49 [3600/36045]\tLoss: 531.4161\n",
      "Training Epoch: 49 [3650/36045]\tLoss: 614.5355\n",
      "Training Epoch: 49 [3700/36045]\tLoss: 622.2692\n",
      "Training Epoch: 49 [3750/36045]\tLoss: 591.5545\n",
      "Training Epoch: 49 [3800/36045]\tLoss: 589.0208\n",
      "Training Epoch: 49 [3850/36045]\tLoss: 591.3018\n",
      "Training Epoch: 49 [3900/36045]\tLoss: 595.7916\n",
      "Training Epoch: 49 [3950/36045]\tLoss: 575.0574\n",
      "Training Epoch: 49 [4000/36045]\tLoss: 579.4944\n",
      "Training Epoch: 49 [4050/36045]\tLoss: 531.1357\n",
      "Training Epoch: 49 [4100/36045]\tLoss: 518.0829\n",
      "Training Epoch: 49 [4150/36045]\tLoss: 531.8556\n",
      "Training Epoch: 49 [4200/36045]\tLoss: 526.8405\n",
      "Training Epoch: 49 [4250/36045]\tLoss: 528.8938\n",
      "Training Epoch: 49 [4300/36045]\tLoss: 543.6091\n",
      "Training Epoch: 49 [4350/36045]\tLoss: 527.7404\n",
      "Training Epoch: 49 [4400/36045]\tLoss: 504.6691\n",
      "Training Epoch: 49 [4450/36045]\tLoss: 554.7835\n",
      "Training Epoch: 49 [4500/36045]\tLoss: 597.5421\n",
      "Training Epoch: 49 [4550/36045]\tLoss: 600.1144\n",
      "Training Epoch: 49 [4600/36045]\tLoss: 621.8077\n",
      "Training Epoch: 49 [4650/36045]\tLoss: 612.0125\n",
      "Training Epoch: 49 [4700/36045]\tLoss: 564.1238\n",
      "Training Epoch: 49 [4750/36045]\tLoss: 545.9267\n",
      "Training Epoch: 49 [4800/36045]\tLoss: 569.8036\n",
      "Training Epoch: 49 [4850/36045]\tLoss: 556.4252\n",
      "Training Epoch: 49 [4900/36045]\tLoss: 541.7996\n",
      "Training Epoch: 49 [4950/36045]\tLoss: 557.1807\n",
      "Training Epoch: 49 [5000/36045]\tLoss: 586.4319\n",
      "Training Epoch: 49 [5050/36045]\tLoss: 568.1602\n",
      "Training Epoch: 49 [5100/36045]\tLoss: 578.1818\n",
      "Training Epoch: 49 [5150/36045]\tLoss: 562.6985\n",
      "Training Epoch: 49 [5200/36045]\tLoss: 560.6525\n",
      "Training Epoch: 49 [5250/36045]\tLoss: 554.2893\n",
      "Training Epoch: 49 [5300/36045]\tLoss: 554.2498\n",
      "Training Epoch: 49 [5350/36045]\tLoss: 575.0068\n",
      "Training Epoch: 49 [5400/36045]\tLoss: 554.3439\n",
      "Training Epoch: 49 [5450/36045]\tLoss: 525.1511\n",
      "Training Epoch: 49 [5500/36045]\tLoss: 553.2296\n",
      "Training Epoch: 49 [5550/36045]\tLoss: 541.4902\n",
      "Training Epoch: 49 [5600/36045]\tLoss: 619.3850\n",
      "Training Epoch: 49 [5650/36045]\tLoss: 585.4954\n",
      "Training Epoch: 49 [5700/36045]\tLoss: 549.3579\n",
      "Training Epoch: 49 [5750/36045]\tLoss: 533.4520\n",
      "Training Epoch: 49 [5800/36045]\tLoss: 562.8123\n",
      "Training Epoch: 49 [5850/36045]\tLoss: 552.7006\n",
      "Training Epoch: 49 [5900/36045]\tLoss: 634.8526\n",
      "Training Epoch: 49 [5950/36045]\tLoss: 651.1467\n",
      "Training Epoch: 49 [6000/36045]\tLoss: 637.3232\n",
      "Training Epoch: 49 [6050/36045]\tLoss: 615.6497\n",
      "Training Epoch: 49 [6100/36045]\tLoss: 619.9301\n",
      "Training Epoch: 49 [6150/36045]\tLoss: 611.3780\n",
      "Training Epoch: 49 [6200/36045]\tLoss: 616.4696\n",
      "Training Epoch: 49 [6250/36045]\tLoss: 637.9314\n",
      "Training Epoch: 49 [6300/36045]\tLoss: 649.4517\n",
      "Training Epoch: 49 [6350/36045]\tLoss: 694.4119\n",
      "Training Epoch: 49 [6400/36045]\tLoss: 571.2720\n",
      "Training Epoch: 49 [6450/36045]\tLoss: 524.8316\n",
      "Training Epoch: 49 [6500/36045]\tLoss: 534.3258\n",
      "Training Epoch: 49 [6550/36045]\tLoss: 551.7517\n",
      "Training Epoch: 49 [6600/36045]\tLoss: 549.5615\n",
      "Training Epoch: 49 [6650/36045]\tLoss: 620.8413\n",
      "Training Epoch: 49 [6700/36045]\tLoss: 649.1819\n",
      "Training Epoch: 49 [6750/36045]\tLoss: 626.9184\n",
      "Training Epoch: 49 [6800/36045]\tLoss: 629.8958\n",
      "Training Epoch: 49 [6850/36045]\tLoss: 618.0258\n",
      "Training Epoch: 49 [6900/36045]\tLoss: 550.5075\n",
      "Training Epoch: 49 [6950/36045]\tLoss: 519.0698\n",
      "Training Epoch: 49 [7000/36045]\tLoss: 552.0221\n",
      "Training Epoch: 49 [7050/36045]\tLoss: 564.0805\n",
      "Training Epoch: 49 [7100/36045]\tLoss: 563.0394\n",
      "Training Epoch: 49 [7150/36045]\tLoss: 572.9009\n",
      "Training Epoch: 49 [7200/36045]\tLoss: 574.8511\n",
      "Training Epoch: 49 [7250/36045]\tLoss: 573.2786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 49 [7300/36045]\tLoss: 559.5955\n",
      "Training Epoch: 49 [7350/36045]\tLoss: 557.4708\n",
      "Training Epoch: 49 [7400/36045]\tLoss: 508.7082\n",
      "Training Epoch: 49 [7450/36045]\tLoss: 511.9710\n",
      "Training Epoch: 49 [7500/36045]\tLoss: 507.6980\n",
      "Training Epoch: 49 [7550/36045]\tLoss: 486.2383\n",
      "Training Epoch: 49 [7600/36045]\tLoss: 538.6777\n",
      "Training Epoch: 49 [7650/36045]\tLoss: 575.5713\n",
      "Training Epoch: 49 [7700/36045]\tLoss: 547.7343\n",
      "Training Epoch: 49 [7750/36045]\tLoss: 562.0279\n",
      "Training Epoch: 49 [7800/36045]\tLoss: 551.7596\n",
      "Training Epoch: 49 [7850/36045]\tLoss: 533.9888\n",
      "Training Epoch: 49 [7900/36045]\tLoss: 563.0672\n",
      "Training Epoch: 49 [7950/36045]\tLoss: 560.7032\n",
      "Training Epoch: 49 [8000/36045]\tLoss: 578.7997\n",
      "Training Epoch: 49 [8050/36045]\tLoss: 544.8184\n",
      "Training Epoch: 49 [8100/36045]\tLoss: 569.7663\n",
      "Training Epoch: 49 [8150/36045]\tLoss: 646.6851\n",
      "Training Epoch: 49 [8200/36045]\tLoss: 634.3196\n",
      "Training Epoch: 49 [8250/36045]\tLoss: 602.8623\n",
      "Training Epoch: 49 [8300/36045]\tLoss: 659.2538\n",
      "Training Epoch: 49 [8350/36045]\tLoss: 604.0280\n",
      "Training Epoch: 49 [8400/36045]\tLoss: 540.0046\n",
      "Training Epoch: 49 [8450/36045]\tLoss: 505.2910\n",
      "Training Epoch: 49 [8500/36045]\tLoss: 537.8975\n",
      "Training Epoch: 49 [8550/36045]\tLoss: 531.7036\n",
      "Training Epoch: 49 [8600/36045]\tLoss: 526.0837\n",
      "Training Epoch: 49 [8650/36045]\tLoss: 557.6765\n",
      "Training Epoch: 49 [8700/36045]\tLoss: 589.8587\n",
      "Training Epoch: 49 [8750/36045]\tLoss: 579.5567\n",
      "Training Epoch: 49 [8800/36045]\tLoss: 585.2499\n",
      "Training Epoch: 49 [8850/36045]\tLoss: 578.9241\n",
      "Training Epoch: 49 [8900/36045]\tLoss: 522.6397\n",
      "Training Epoch: 49 [8950/36045]\tLoss: 533.1169\n",
      "Training Epoch: 49 [9000/36045]\tLoss: 549.0372\n",
      "Training Epoch: 49 [9050/36045]\tLoss: 550.6418\n",
      "Training Epoch: 49 [9100/36045]\tLoss: 567.2407\n",
      "Training Epoch: 49 [9150/36045]\tLoss: 419.8824\n",
      "Training Epoch: 49 [9200/36045]\tLoss: 313.5531\n",
      "Training Epoch: 49 [9250/36045]\tLoss: 340.3170\n",
      "Training Epoch: 49 [9300/36045]\tLoss: 349.8353\n",
      "Training Epoch: 49 [9350/36045]\tLoss: 322.9682\n",
      "Training Epoch: 49 [9400/36045]\tLoss: 632.2813\n",
      "Training Epoch: 49 [9450/36045]\tLoss: 671.4440\n",
      "Training Epoch: 49 [9500/36045]\tLoss: 659.2111\n",
      "Training Epoch: 49 [9550/36045]\tLoss: 697.4006\n",
      "Training Epoch: 49 [9600/36045]\tLoss: 520.1262\n",
      "Training Epoch: 49 [9650/36045]\tLoss: 525.2654\n",
      "Training Epoch: 49 [9700/36045]\tLoss: 510.8855\n",
      "Training Epoch: 49 [9750/36045]\tLoss: 509.4964\n",
      "Training Epoch: 49 [9800/36045]\tLoss: 666.3691\n",
      "Training Epoch: 49 [9850/36045]\tLoss: 703.6119\n",
      "Training Epoch: 49 [9900/36045]\tLoss: 712.4055\n",
      "Training Epoch: 49 [9950/36045]\tLoss: 694.2825\n",
      "Training Epoch: 49 [10000/36045]\tLoss: 642.8689\n",
      "Training Epoch: 49 [10050/36045]\tLoss: 526.0254\n",
      "Training Epoch: 49 [10100/36045]\tLoss: 533.5826\n",
      "Training Epoch: 49 [10150/36045]\tLoss: 541.4907\n",
      "Training Epoch: 49 [10200/36045]\tLoss: 530.5810\n",
      "Training Epoch: 49 [10250/36045]\tLoss: 638.6370\n",
      "Training Epoch: 49 [10300/36045]\tLoss: 620.7074\n",
      "Training Epoch: 49 [10350/36045]\tLoss: 653.7610\n",
      "Training Epoch: 49 [10400/36045]\tLoss: 643.7213\n",
      "Training Epoch: 49 [10450/36045]\tLoss: 603.2551\n",
      "Training Epoch: 49 [10500/36045]\tLoss: 503.8271\n",
      "Training Epoch: 49 [10550/36045]\tLoss: 498.2998\n",
      "Training Epoch: 49 [10600/36045]\tLoss: 519.9462\n",
      "Training Epoch: 49 [10650/36045]\tLoss: 525.7510\n",
      "Training Epoch: 49 [10700/36045]\tLoss: 604.7103\n",
      "Training Epoch: 49 [10750/36045]\tLoss: 663.3884\n",
      "Training Epoch: 49 [10800/36045]\tLoss: 609.4531\n",
      "Training Epoch: 49 [10850/36045]\tLoss: 646.4760\n",
      "Training Epoch: 49 [10900/36045]\tLoss: 673.0516\n",
      "Training Epoch: 49 [10950/36045]\tLoss: 494.7533\n",
      "Training Epoch: 49 [11000/36045]\tLoss: 488.5682\n",
      "Training Epoch: 49 [11050/36045]\tLoss: 524.0700\n",
      "Training Epoch: 49 [11100/36045]\tLoss: 534.0344\n",
      "Training Epoch: 49 [11150/36045]\tLoss: 579.4395\n",
      "Training Epoch: 49 [11200/36045]\tLoss: 608.0500\n",
      "Training Epoch: 49 [11250/36045]\tLoss: 619.2267\n",
      "Training Epoch: 49 [11300/36045]\tLoss: 599.6511\n",
      "Training Epoch: 49 [11350/36045]\tLoss: 597.4272\n",
      "Training Epoch: 49 [11400/36045]\tLoss: 561.2029\n",
      "Training Epoch: 49 [11450/36045]\tLoss: 530.6366\n",
      "Training Epoch: 49 [11500/36045]\tLoss: 528.1459\n",
      "Training Epoch: 49 [11550/36045]\tLoss: 537.4785\n",
      "Training Epoch: 49 [11600/36045]\tLoss: 597.2639\n",
      "Training Epoch: 49 [11650/36045]\tLoss: 648.4026\n",
      "Training Epoch: 49 [11700/36045]\tLoss: 647.3122\n",
      "Training Epoch: 49 [11750/36045]\tLoss: 665.1317\n",
      "Training Epoch: 49 [11800/36045]\tLoss: 707.5772\n",
      "Training Epoch: 49 [11850/36045]\tLoss: 766.1691\n",
      "Training Epoch: 49 [11900/36045]\tLoss: 977.6802\n",
      "Training Epoch: 49 [11950/36045]\tLoss: 980.9160\n",
      "Training Epoch: 49 [12000/36045]\tLoss: 991.8853\n",
      "Training Epoch: 49 [12050/36045]\tLoss: 951.7114\n",
      "Training Epoch: 49 [12100/36045]\tLoss: 607.0709\n",
      "Training Epoch: 49 [12150/36045]\tLoss: 455.4951\n",
      "Training Epoch: 49 [12200/36045]\tLoss: 450.3610\n",
      "Training Epoch: 49 [12250/36045]\tLoss: 458.8971\n",
      "Training Epoch: 49 [12300/36045]\tLoss: 593.0464\n",
      "Training Epoch: 49 [12350/36045]\tLoss: 647.8749\n",
      "Training Epoch: 49 [12400/36045]\tLoss: 654.9813\n",
      "Training Epoch: 49 [12450/36045]\tLoss: 643.9673\n",
      "Training Epoch: 49 [12500/36045]\tLoss: 670.7150\n",
      "Training Epoch: 49 [12550/36045]\tLoss: 640.7394\n",
      "Training Epoch: 49 [12600/36045]\tLoss: 585.1555\n",
      "Training Epoch: 49 [12650/36045]\tLoss: 583.2847\n",
      "Training Epoch: 49 [12700/36045]\tLoss: 604.4839\n",
      "Training Epoch: 49 [12750/36045]\tLoss: 602.9684\n",
      "Training Epoch: 49 [12800/36045]\tLoss: 589.6361\n",
      "Training Epoch: 49 [12850/36045]\tLoss: 618.8292\n",
      "Training Epoch: 49 [12900/36045]\tLoss: 593.1473\n",
      "Training Epoch: 49 [12950/36045]\tLoss: 578.7105\n",
      "Training Epoch: 49 [13000/36045]\tLoss: 612.0843\n",
      "Training Epoch: 49 [13050/36045]\tLoss: 552.9948\n",
      "Training Epoch: 49 [13100/36045]\tLoss: 567.9014\n",
      "Training Epoch: 49 [13150/36045]\tLoss: 559.5260\n",
      "Training Epoch: 49 [13200/36045]\tLoss: 542.9637\n",
      "Training Epoch: 49 [13250/36045]\tLoss: 564.3561\n",
      "Training Epoch: 49 [13300/36045]\tLoss: 600.7552\n",
      "Training Epoch: 49 [13350/36045]\tLoss: 581.9862\n",
      "Training Epoch: 49 [13400/36045]\tLoss: 585.0397\n",
      "Training Epoch: 49 [13450/36045]\tLoss: 582.6388\n",
      "Training Epoch: 49 [13500/36045]\tLoss: 600.9278\n",
      "Training Epoch: 49 [13550/36045]\tLoss: 739.1611\n",
      "Training Epoch: 49 [13600/36045]\tLoss: 771.7253\n",
      "Training Epoch: 49 [13650/36045]\tLoss: 853.6828\n",
      "Training Epoch: 49 [13700/36045]\tLoss: 751.3182\n",
      "Training Epoch: 49 [13750/36045]\tLoss: 588.4423\n",
      "Training Epoch: 49 [13800/36045]\tLoss: 559.0720\n",
      "Training Epoch: 49 [13850/36045]\tLoss: 541.8815\n",
      "Training Epoch: 49 [13900/36045]\tLoss: 549.1307\n",
      "Training Epoch: 49 [13950/36045]\tLoss: 594.2227\n",
      "Training Epoch: 49 [14000/36045]\tLoss: 626.6616\n",
      "Training Epoch: 49 [14050/36045]\tLoss: 601.8063\n",
      "Training Epoch: 49 [14100/36045]\tLoss: 596.8240\n",
      "Training Epoch: 49 [14150/36045]\tLoss: 585.0227\n",
      "Training Epoch: 49 [14200/36045]\tLoss: 625.3043\n",
      "Training Epoch: 49 [14250/36045]\tLoss: 687.3348\n",
      "Training Epoch: 49 [14300/36045]\tLoss: 690.4082\n",
      "Training Epoch: 49 [14350/36045]\tLoss: 659.6298\n",
      "Training Epoch: 49 [14400/36045]\tLoss: 645.8657\n",
      "Training Epoch: 49 [14450/36045]\tLoss: 681.1573\n",
      "Training Epoch: 49 [14500/36045]\tLoss: 612.9354\n",
      "Training Epoch: 49 [14550/36045]\tLoss: 640.4144\n",
      "Training Epoch: 49 [14600/36045]\tLoss: 626.9623\n",
      "Training Epoch: 49 [14650/36045]\tLoss: 626.7492\n",
      "Training Epoch: 49 [14700/36045]\tLoss: 594.6201\n",
      "Training Epoch: 49 [14750/36045]\tLoss: 511.7511\n",
      "Training Epoch: 49 [14800/36045]\tLoss: 501.6979\n",
      "Training Epoch: 49 [14850/36045]\tLoss: 508.6818\n",
      "Training Epoch: 49 [14900/36045]\tLoss: 502.2267\n",
      "Training Epoch: 49 [14950/36045]\tLoss: 510.5317\n",
      "Training Epoch: 49 [15000/36045]\tLoss: 522.8536\n",
      "Training Epoch: 49 [15050/36045]\tLoss: 519.1861\n",
      "Training Epoch: 49 [15100/36045]\tLoss: 503.1164\n",
      "Training Epoch: 49 [15150/36045]\tLoss: 499.0558\n",
      "Training Epoch: 49 [15200/36045]\tLoss: 462.5754\n",
      "Training Epoch: 49 [15250/36045]\tLoss: 484.0148\n",
      "Training Epoch: 49 [15300/36045]\tLoss: 469.8414\n",
      "Training Epoch: 49 [15350/36045]\tLoss: 481.2825\n",
      "Training Epoch: 49 [15400/36045]\tLoss: 463.1537\n",
      "Training Epoch: 49 [15450/36045]\tLoss: 448.5426\n",
      "Training Epoch: 49 [15500/36045]\tLoss: 461.4030\n",
      "Training Epoch: 49 [15550/36045]\tLoss: 458.3362\n",
      "Training Epoch: 49 [15600/36045]\tLoss: 524.6235\n",
      "Training Epoch: 49 [15650/36045]\tLoss: 540.8821\n",
      "Training Epoch: 49 [15700/36045]\tLoss: 533.3382\n",
      "Training Epoch: 49 [15750/36045]\tLoss: 524.8442\n",
      "Training Epoch: 49 [15800/36045]\tLoss: 504.4070\n",
      "Training Epoch: 49 [15850/36045]\tLoss: 519.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 49 [15900/36045]\tLoss: 528.5926\n",
      "Training Epoch: 49 [15950/36045]\tLoss: 548.3219\n",
      "Training Epoch: 49 [16000/36045]\tLoss: 518.8090\n",
      "Training Epoch: 49 [16050/36045]\tLoss: 488.4444\n",
      "Training Epoch: 49 [16100/36045]\tLoss: 453.4485\n",
      "Training Epoch: 49 [16150/36045]\tLoss: 441.5963\n",
      "Training Epoch: 49 [16200/36045]\tLoss: 536.7381\n",
      "Training Epoch: 49 [16250/36045]\tLoss: 563.8903\n",
      "Training Epoch: 49 [16300/36045]\tLoss: 615.5186\n",
      "Training Epoch: 49 [16350/36045]\tLoss: 636.0410\n",
      "Training Epoch: 49 [16400/36045]\tLoss: 607.9452\n",
      "Training Epoch: 49 [16450/36045]\tLoss: 590.4391\n",
      "Training Epoch: 49 [16500/36045]\tLoss: 590.2561\n",
      "Training Epoch: 49 [16550/36045]\tLoss: 555.8831\n",
      "Training Epoch: 49 [16600/36045]\tLoss: 577.2902\n",
      "Training Epoch: 49 [16650/36045]\tLoss: 592.9703\n",
      "Training Epoch: 49 [16700/36045]\tLoss: 573.6881\n",
      "Training Epoch: 49 [16750/36045]\tLoss: 566.4615\n",
      "Training Epoch: 49 [16800/36045]\tLoss: 574.6972\n",
      "Training Epoch: 49 [16850/36045]\tLoss: 547.8326\n",
      "Training Epoch: 49 [16900/36045]\tLoss: 557.5832\n",
      "Training Epoch: 49 [16950/36045]\tLoss: 580.0719\n",
      "Training Epoch: 49 [17000/36045]\tLoss: 564.8326\n",
      "Training Epoch: 49 [17050/36045]\tLoss: 587.9697\n",
      "Training Epoch: 49 [17100/36045]\tLoss: 583.7791\n",
      "Training Epoch: 49 [17150/36045]\tLoss: 506.4097\n",
      "Training Epoch: 49 [17200/36045]\tLoss: 468.9967\n",
      "Training Epoch: 49 [17250/36045]\tLoss: 491.3606\n",
      "Training Epoch: 49 [17300/36045]\tLoss: 520.1562\n",
      "Training Epoch: 49 [17350/36045]\tLoss: 502.0714\n",
      "Training Epoch: 49 [17400/36045]\tLoss: 521.6764\n",
      "Training Epoch: 49 [17450/36045]\tLoss: 539.7291\n",
      "Training Epoch: 49 [17500/36045]\tLoss: 528.2840\n",
      "Training Epoch: 49 [17550/36045]\tLoss: 526.7256\n",
      "Training Epoch: 49 [17600/36045]\tLoss: 521.2640\n",
      "Training Epoch: 49 [17650/36045]\tLoss: 536.2942\n",
      "Training Epoch: 49 [17700/36045]\tLoss: 515.8851\n",
      "Training Epoch: 49 [17750/36045]\tLoss: 531.4858\n",
      "Training Epoch: 49 [17800/36045]\tLoss: 522.9420\n",
      "Training Epoch: 49 [17850/36045]\tLoss: 540.2908\n",
      "Training Epoch: 49 [17900/36045]\tLoss: 567.9772\n",
      "Training Epoch: 49 [17950/36045]\tLoss: 580.3292\n",
      "Training Epoch: 49 [18000/36045]\tLoss: 571.3903\n",
      "Training Epoch: 49 [18050/36045]\tLoss: 626.0619\n",
      "Training Epoch: 49 [18100/36045]\tLoss: 626.7415\n",
      "Training Epoch: 49 [18150/36045]\tLoss: 637.9134\n",
      "Training Epoch: 49 [18200/36045]\tLoss: 620.5798\n",
      "Training Epoch: 49 [18250/36045]\tLoss: 641.1864\n",
      "Training Epoch: 49 [18300/36045]\tLoss: 598.6218\n",
      "Training Epoch: 49 [18350/36045]\tLoss: 672.0402\n",
      "Training Epoch: 49 [18400/36045]\tLoss: 644.6771\n",
      "Training Epoch: 49 [18450/36045]\tLoss: 624.5546\n",
      "Training Epoch: 49 [18500/36045]\tLoss: 623.3969\n",
      "Training Epoch: 49 [18550/36045]\tLoss: 611.1631\n",
      "Training Epoch: 49 [18600/36045]\tLoss: 600.9234\n",
      "Training Epoch: 49 [18650/36045]\tLoss: 645.9469\n",
      "Training Epoch: 49 [18700/36045]\tLoss: 679.4786\n",
      "Training Epoch: 49 [18750/36045]\tLoss: 667.3378\n",
      "Training Epoch: 49 [18800/36045]\tLoss: 689.8618\n",
      "Training Epoch: 49 [18850/36045]\tLoss: 635.1078\n",
      "Training Epoch: 49 [18900/36045]\tLoss: 679.2471\n",
      "Training Epoch: 49 [18950/36045]\tLoss: 621.9933\n",
      "Training Epoch: 49 [19000/36045]\tLoss: 508.6750\n",
      "Training Epoch: 49 [19050/36045]\tLoss: 493.8033\n",
      "Training Epoch: 49 [19100/36045]\tLoss: 501.6934\n",
      "Training Epoch: 49 [19150/36045]\tLoss: 491.9991\n",
      "Training Epoch: 49 [19200/36045]\tLoss: 523.4200\n",
      "Training Epoch: 49 [19250/36045]\tLoss: 538.8396\n",
      "Training Epoch: 49 [19300/36045]\tLoss: 548.0796\n",
      "Training Epoch: 49 [19350/36045]\tLoss: 532.0194\n",
      "Training Epoch: 49 [19400/36045]\tLoss: 551.7102\n",
      "Training Epoch: 49 [19450/36045]\tLoss: 543.2165\n",
      "Training Epoch: 49 [19500/36045]\tLoss: 544.2296\n",
      "Training Epoch: 49 [19550/36045]\tLoss: 542.4344\n",
      "Training Epoch: 49 [19600/36045]\tLoss: 583.1768\n",
      "Training Epoch: 49 [19650/36045]\tLoss: 782.2516\n",
      "Training Epoch: 49 [19700/36045]\tLoss: 740.8840\n",
      "Training Epoch: 49 [19750/36045]\tLoss: 745.0182\n",
      "Training Epoch: 49 [19800/36045]\tLoss: 745.6949\n",
      "Training Epoch: 49 [19850/36045]\tLoss: 485.2768\n",
      "Training Epoch: 49 [19900/36045]\tLoss: 464.8959\n",
      "Training Epoch: 49 [19950/36045]\tLoss: 468.9120\n",
      "Training Epoch: 49 [20000/36045]\tLoss: 469.0791\n",
      "Training Epoch: 49 [20050/36045]\tLoss: 524.6332\n",
      "Training Epoch: 49 [20100/36045]\tLoss: 529.7772\n",
      "Training Epoch: 49 [20150/36045]\tLoss: 530.6741\n",
      "Training Epoch: 49 [20200/36045]\tLoss: 530.5544\n",
      "Training Epoch: 49 [20250/36045]\tLoss: 566.4998\n",
      "Training Epoch: 49 [20300/36045]\tLoss: 602.6165\n",
      "Training Epoch: 49 [20350/36045]\tLoss: 620.3812\n",
      "Training Epoch: 49 [20400/36045]\tLoss: 636.4547\n",
      "Training Epoch: 49 [20450/36045]\tLoss: 606.2013\n",
      "Training Epoch: 49 [20500/36045]\tLoss: 591.4992\n",
      "Training Epoch: 49 [20550/36045]\tLoss: 519.1828\n",
      "Training Epoch: 49 [20600/36045]\tLoss: 528.6251\n",
      "Training Epoch: 49 [20650/36045]\tLoss: 525.8036\n",
      "Training Epoch: 49 [20700/36045]\tLoss: 513.9653\n",
      "Training Epoch: 49 [20750/36045]\tLoss: 554.6655\n",
      "Training Epoch: 49 [20800/36045]\tLoss: 602.7423\n",
      "Training Epoch: 49 [20850/36045]\tLoss: 589.6491\n",
      "Training Epoch: 49 [20900/36045]\tLoss: 631.9753\n",
      "Training Epoch: 49 [20950/36045]\tLoss: 595.2060\n",
      "Training Epoch: 49 [21000/36045]\tLoss: 560.3040\n",
      "Training Epoch: 49 [21050/36045]\tLoss: 479.2289\n",
      "Training Epoch: 49 [21100/36045]\tLoss: 484.5548\n",
      "Training Epoch: 49 [21150/36045]\tLoss: 518.5471\n",
      "Training Epoch: 49 [21200/36045]\tLoss: 517.5056\n",
      "Training Epoch: 49 [21250/36045]\tLoss: 494.8448\n",
      "Training Epoch: 49 [21300/36045]\tLoss: 577.4968\n",
      "Training Epoch: 49 [21350/36045]\tLoss: 569.3109\n",
      "Training Epoch: 49 [21400/36045]\tLoss: 573.1202\n",
      "Training Epoch: 49 [21450/36045]\tLoss: 579.2756\n",
      "Training Epoch: 49 [21500/36045]\tLoss: 580.6380\n",
      "Training Epoch: 49 [21550/36045]\tLoss: 674.6459\n",
      "Training Epoch: 49 [21600/36045]\tLoss: 672.9709\n",
      "Training Epoch: 49 [21650/36045]\tLoss: 685.2938\n",
      "Training Epoch: 49 [21700/36045]\tLoss: 688.8587\n",
      "Training Epoch: 49 [21750/36045]\tLoss: 661.1241\n",
      "Training Epoch: 49 [21800/36045]\tLoss: 484.7046\n",
      "Training Epoch: 49 [21850/36045]\tLoss: 467.6849\n",
      "Training Epoch: 49 [21900/36045]\tLoss: 476.8640\n",
      "Training Epoch: 49 [21950/36045]\tLoss: 478.6826\n",
      "Training Epoch: 49 [22000/36045]\tLoss: 481.8682\n",
      "Training Epoch: 49 [22050/36045]\tLoss: 500.7139\n",
      "Training Epoch: 49 [22100/36045]\tLoss: 493.1007\n",
      "Training Epoch: 49 [22150/36045]\tLoss: 479.8558\n",
      "Training Epoch: 49 [22200/36045]\tLoss: 495.3902\n",
      "Training Epoch: 49 [22250/36045]\tLoss: 500.3573\n",
      "Training Epoch: 49 [22300/36045]\tLoss: 551.9723\n",
      "Training Epoch: 49 [22350/36045]\tLoss: 577.2508\n",
      "Training Epoch: 49 [22400/36045]\tLoss: 590.7117\n",
      "Training Epoch: 49 [22450/36045]\tLoss: 578.1956\n",
      "Training Epoch: 49 [22500/36045]\tLoss: 561.6395\n",
      "Training Epoch: 49 [22550/36045]\tLoss: 596.1074\n",
      "Training Epoch: 49 [22600/36045]\tLoss: 643.3141\n",
      "Training Epoch: 49 [22650/36045]\tLoss: 675.6668\n",
      "Training Epoch: 49 [22700/36045]\tLoss: 696.8229\n",
      "Training Epoch: 49 [22750/36045]\tLoss: 716.5163\n",
      "Training Epoch: 49 [22800/36045]\tLoss: 744.3521\n",
      "Training Epoch: 49 [22850/36045]\tLoss: 617.4511\n",
      "Training Epoch: 49 [22900/36045]\tLoss: 622.0592\n",
      "Training Epoch: 49 [22950/36045]\tLoss: 601.8195\n",
      "Training Epoch: 49 [23000/36045]\tLoss: 598.2945\n",
      "Training Epoch: 49 [23050/36045]\tLoss: 531.6694\n",
      "Training Epoch: 49 [23100/36045]\tLoss: 547.0819\n",
      "Training Epoch: 49 [23150/36045]\tLoss: 535.2735\n",
      "Training Epoch: 49 [23200/36045]\tLoss: 506.4938\n",
      "Training Epoch: 49 [23250/36045]\tLoss: 509.5538\n",
      "Training Epoch: 49 [23300/36045]\tLoss: 505.5464\n",
      "Training Epoch: 49 [23350/36045]\tLoss: 525.5889\n",
      "Training Epoch: 49 [23400/36045]\tLoss: 570.0750\n",
      "Training Epoch: 49 [23450/36045]\tLoss: 563.7957\n",
      "Training Epoch: 49 [23500/36045]\tLoss: 543.1524\n",
      "Training Epoch: 49 [23550/36045]\tLoss: 582.1219\n",
      "Training Epoch: 49 [23600/36045]\tLoss: 661.7928\n",
      "Training Epoch: 49 [23650/36045]\tLoss: 672.6214\n",
      "Training Epoch: 49 [23700/36045]\tLoss: 680.5734\n",
      "Training Epoch: 49 [23750/36045]\tLoss: 657.0380\n",
      "Training Epoch: 49 [23800/36045]\tLoss: 527.7036\n",
      "Training Epoch: 49 [23850/36045]\tLoss: 553.2241\n",
      "Training Epoch: 49 [23900/36045]\tLoss: 543.1801\n",
      "Training Epoch: 49 [23950/36045]\tLoss: 526.4413\n",
      "Training Epoch: 49 [24000/36045]\tLoss: 503.8029\n",
      "Training Epoch: 49 [24050/36045]\tLoss: 464.7549\n",
      "Training Epoch: 49 [24100/36045]\tLoss: 488.8930\n",
      "Training Epoch: 49 [24150/36045]\tLoss: 480.7855\n",
      "Training Epoch: 49 [24200/36045]\tLoss: 478.7436\n",
      "Training Epoch: 49 [24250/36045]\tLoss: 465.0464\n",
      "Training Epoch: 49 [24300/36045]\tLoss: 502.2229\n",
      "Training Epoch: 49 [24350/36045]\tLoss: 514.0714\n",
      "Training Epoch: 49 [24400/36045]\tLoss: 528.8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 49 [24450/36045]\tLoss: 504.0293\n",
      "Training Epoch: 49 [24500/36045]\tLoss: 532.2484\n",
      "Training Epoch: 49 [24550/36045]\tLoss: 618.1632\n",
      "Training Epoch: 49 [24600/36045]\tLoss: 609.2076\n",
      "Training Epoch: 49 [24650/36045]\tLoss: 582.9051\n",
      "Training Epoch: 49 [24700/36045]\tLoss: 592.9031\n",
      "Training Epoch: 49 [24750/36045]\tLoss: 548.2663\n",
      "Training Epoch: 49 [24800/36045]\tLoss: 447.9511\n",
      "Training Epoch: 49 [24850/36045]\tLoss: 465.5767\n",
      "Training Epoch: 49 [24900/36045]\tLoss: 463.1539\n",
      "Training Epoch: 49 [24950/36045]\tLoss: 465.6941\n",
      "Training Epoch: 49 [25000/36045]\tLoss: 448.0521\n",
      "Training Epoch: 49 [25050/36045]\tLoss: 428.4273\n",
      "Training Epoch: 49 [25100/36045]\tLoss: 383.6628\n",
      "Training Epoch: 49 [25150/36045]\tLoss: 355.0596\n",
      "Training Epoch: 49 [25200/36045]\tLoss: 350.0381\n",
      "Training Epoch: 49 [25250/36045]\tLoss: 375.5483\n",
      "Training Epoch: 49 [25300/36045]\tLoss: 493.6949\n",
      "Training Epoch: 49 [25350/36045]\tLoss: 489.8438\n",
      "Training Epoch: 49 [25400/36045]\tLoss: 457.9693\n",
      "Training Epoch: 49 [25450/36045]\tLoss: 459.7701\n",
      "Training Epoch: 49 [25500/36045]\tLoss: 499.3954\n",
      "Training Epoch: 49 [25550/36045]\tLoss: 584.0921\n",
      "Training Epoch: 49 [25600/36045]\tLoss: 588.5706\n",
      "Training Epoch: 49 [25650/36045]\tLoss: 568.2537\n",
      "Training Epoch: 49 [25700/36045]\tLoss: 577.7836\n",
      "Training Epoch: 49 [25750/36045]\tLoss: 558.3173\n",
      "Training Epoch: 49 [25800/36045]\tLoss: 351.8363\n",
      "Training Epoch: 49 [25850/36045]\tLoss: 359.4777\n",
      "Training Epoch: 49 [25900/36045]\tLoss: 341.8621\n",
      "Training Epoch: 49 [25950/36045]\tLoss: 351.9226\n",
      "Training Epoch: 49 [26000/36045]\tLoss: 433.1540\n",
      "Training Epoch: 49 [26050/36045]\tLoss: 588.3306\n",
      "Training Epoch: 49 [26100/36045]\tLoss: 613.3855\n",
      "Training Epoch: 49 [26150/36045]\tLoss: 612.7830\n",
      "Training Epoch: 49 [26200/36045]\tLoss: 587.2563\n",
      "Training Epoch: 49 [26250/36045]\tLoss: 617.6176\n",
      "Training Epoch: 49 [26300/36045]\tLoss: 569.2966\n",
      "Training Epoch: 49 [26350/36045]\tLoss: 579.3066\n",
      "Training Epoch: 49 [26400/36045]\tLoss: 554.0557\n",
      "Training Epoch: 49 [26450/36045]\tLoss: 485.3891\n",
      "Training Epoch: 49 [26500/36045]\tLoss: 574.9005\n",
      "Training Epoch: 49 [26550/36045]\tLoss: 573.6691\n",
      "Training Epoch: 49 [26600/36045]\tLoss: 569.2162\n",
      "Training Epoch: 49 [26650/36045]\tLoss: 583.5126\n",
      "Training Epoch: 49 [26700/36045]\tLoss: 563.2812\n",
      "Training Epoch: 49 [26750/36045]\tLoss: 529.1538\n",
      "Training Epoch: 49 [26800/36045]\tLoss: 391.1914\n",
      "Training Epoch: 49 [26850/36045]\tLoss: 323.1628\n",
      "Training Epoch: 49 [26900/36045]\tLoss: 324.5549\n",
      "Training Epoch: 49 [26950/36045]\tLoss: 356.1639\n",
      "Training Epoch: 49 [27000/36045]\tLoss: 589.9519\n",
      "Training Epoch: 49 [27050/36045]\tLoss: 615.8066\n",
      "Training Epoch: 49 [27100/36045]\tLoss: 596.5698\n",
      "Training Epoch: 49 [27150/36045]\tLoss: 634.6281\n",
      "Training Epoch: 49 [27200/36045]\tLoss: 459.2145\n",
      "Training Epoch: 49 [27250/36045]\tLoss: 449.6808\n",
      "Training Epoch: 49 [27300/36045]\tLoss: 438.7886\n",
      "Training Epoch: 49 [27350/36045]\tLoss: 435.5395\n",
      "Training Epoch: 49 [27400/36045]\tLoss: 434.8911\n",
      "Training Epoch: 49 [27450/36045]\tLoss: 551.4745\n",
      "Training Epoch: 49 [27500/36045]\tLoss: 591.0170\n",
      "Training Epoch: 49 [27550/36045]\tLoss: 584.2358\n",
      "Training Epoch: 49 [27600/36045]\tLoss: 595.5314\n",
      "Training Epoch: 49 [27650/36045]\tLoss: 587.5472\n",
      "Training Epoch: 49 [27700/36045]\tLoss: 614.3713\n",
      "Training Epoch: 49 [27750/36045]\tLoss: 626.1117\n",
      "Training Epoch: 49 [27800/36045]\tLoss: 613.6112\n",
      "Training Epoch: 49 [27850/36045]\tLoss: 604.0316\n",
      "Training Epoch: 49 [27900/36045]\tLoss: 550.3676\n",
      "Training Epoch: 49 [27950/36045]\tLoss: 461.3524\n",
      "Training Epoch: 49 [28000/36045]\tLoss: 438.3168\n",
      "Training Epoch: 49 [28050/36045]\tLoss: 447.6714\n",
      "Training Epoch: 49 [28100/36045]\tLoss: 439.3813\n",
      "Training Epoch: 49 [28150/36045]\tLoss: 457.2518\n",
      "Training Epoch: 49 [28200/36045]\tLoss: 465.9094\n",
      "Training Epoch: 49 [28250/36045]\tLoss: 458.6071\n",
      "Training Epoch: 49 [28300/36045]\tLoss: 435.4311\n",
      "Training Epoch: 49 [28350/36045]\tLoss: 431.1956\n",
      "Training Epoch: 49 [28400/36045]\tLoss: 757.6823\n",
      "Training Epoch: 49 [28450/36045]\tLoss: 696.0705\n",
      "Training Epoch: 49 [28500/36045]\tLoss: 601.4474\n",
      "Training Epoch: 49 [28550/36045]\tLoss: 553.3494\n",
      "Training Epoch: 49 [28600/36045]\tLoss: 576.1422\n",
      "Training Epoch: 49 [28650/36045]\tLoss: 626.5977\n",
      "Training Epoch: 49 [28700/36045]\tLoss: 620.1069\n",
      "Training Epoch: 49 [28750/36045]\tLoss: 607.3493\n",
      "Training Epoch: 49 [28800/36045]\tLoss: 617.7656\n",
      "Training Epoch: 49 [28850/36045]\tLoss: 536.9380\n",
      "Training Epoch: 49 [28900/36045]\tLoss: 437.3154\n",
      "Training Epoch: 49 [28950/36045]\tLoss: 436.4120\n",
      "Training Epoch: 49 [29000/36045]\tLoss: 432.1342\n",
      "Training Epoch: 49 [29050/36045]\tLoss: 439.3114\n",
      "Training Epoch: 49 [29100/36045]\tLoss: 457.0288\n",
      "Training Epoch: 49 [29150/36045]\tLoss: 447.8618\n",
      "Training Epoch: 49 [29200/36045]\tLoss: 433.5635\n",
      "Training Epoch: 49 [29250/36045]\tLoss: 423.4697\n",
      "Training Epoch: 49 [29300/36045]\tLoss: 474.9035\n",
      "Training Epoch: 49 [29350/36045]\tLoss: 556.7719\n",
      "Training Epoch: 49 [29400/36045]\tLoss: 574.3945\n",
      "Training Epoch: 49 [29450/36045]\tLoss: 590.6094\n",
      "Training Epoch: 49 [29500/36045]\tLoss: 605.2952\n",
      "Training Epoch: 49 [29550/36045]\tLoss: 575.4427\n",
      "Training Epoch: 49 [29600/36045]\tLoss: 483.5958\n",
      "Training Epoch: 49 [29650/36045]\tLoss: 465.2067\n",
      "Training Epoch: 49 [29700/36045]\tLoss: 418.6351\n",
      "Training Epoch: 49 [29750/36045]\tLoss: 416.8633\n",
      "Training Epoch: 49 [29800/36045]\tLoss: 463.6308\n",
      "Training Epoch: 49 [29850/36045]\tLoss: 541.3537\n",
      "Training Epoch: 49 [29900/36045]\tLoss: 536.6815\n",
      "Training Epoch: 49 [29950/36045]\tLoss: 558.6294\n",
      "Training Epoch: 49 [30000/36045]\tLoss: 532.4823\n",
      "Training Epoch: 49 [30050/36045]\tLoss: 539.8720\n",
      "Training Epoch: 49 [30100/36045]\tLoss: 658.2180\n",
      "Training Epoch: 49 [30150/36045]\tLoss: 640.1500\n",
      "Training Epoch: 49 [30200/36045]\tLoss: 603.6782\n",
      "Training Epoch: 49 [30250/36045]\tLoss: 652.2344\n",
      "Training Epoch: 49 [30300/36045]\tLoss: 636.3009\n",
      "Training Epoch: 49 [30350/36045]\tLoss: 483.0857\n",
      "Training Epoch: 49 [30400/36045]\tLoss: 467.8509\n",
      "Training Epoch: 49 [30450/36045]\tLoss: 470.2723\n",
      "Training Epoch: 49 [30500/36045]\tLoss: 438.9605\n",
      "Training Epoch: 49 [30550/36045]\tLoss: 407.5056\n",
      "Training Epoch: 49 [30600/36045]\tLoss: 400.7409\n",
      "Training Epoch: 49 [30650/36045]\tLoss: 390.9203\n",
      "Training Epoch: 49 [30700/36045]\tLoss: 408.4641\n",
      "Training Epoch: 49 [30750/36045]\tLoss: 395.4990\n",
      "Training Epoch: 49 [30800/36045]\tLoss: 421.6565\n",
      "Training Epoch: 49 [30850/36045]\tLoss: 413.4867\n",
      "Training Epoch: 49 [30900/36045]\tLoss: 425.0328\n",
      "Training Epoch: 49 [30950/36045]\tLoss: 446.7520\n",
      "Training Epoch: 49 [31000/36045]\tLoss: 439.2823\n",
      "Training Epoch: 49 [31050/36045]\tLoss: 366.5905\n",
      "Training Epoch: 49 [31100/36045]\tLoss: 357.3967\n",
      "Training Epoch: 49 [31150/36045]\tLoss: 365.1082\n",
      "Training Epoch: 49 [31200/36045]\tLoss: 453.0394\n",
      "Training Epoch: 49 [31250/36045]\tLoss: 587.7959\n",
      "Training Epoch: 49 [31300/36045]\tLoss: 559.1748\n",
      "Training Epoch: 49 [31350/36045]\tLoss: 575.3539\n",
      "Training Epoch: 49 [31400/36045]\tLoss: 554.9749\n",
      "Training Epoch: 49 [31450/36045]\tLoss: 571.7295\n",
      "Training Epoch: 49 [31500/36045]\tLoss: 584.1011\n",
      "Training Epoch: 49 [31550/36045]\tLoss: 590.6187\n",
      "Training Epoch: 49 [31600/36045]\tLoss: 555.2939\n",
      "Training Epoch: 49 [31650/36045]\tLoss: 594.8642\n",
      "Training Epoch: 49 [31700/36045]\tLoss: 429.9714\n",
      "Training Epoch: 49 [31750/36045]\tLoss: 355.0279\n",
      "Training Epoch: 49 [31800/36045]\tLoss: 338.8946\n",
      "Training Epoch: 49 [31850/36045]\tLoss: 346.4200\n",
      "Training Epoch: 49 [31900/36045]\tLoss: 549.9666\n",
      "Training Epoch: 49 [31950/36045]\tLoss: 713.7318\n",
      "Training Epoch: 49 [32000/36045]\tLoss: 820.3726\n",
      "Training Epoch: 49 [32050/36045]\tLoss: 776.4598\n",
      "Training Epoch: 49 [32100/36045]\tLoss: 768.1315\n",
      "Training Epoch: 49 [32150/36045]\tLoss: 586.5848\n",
      "Training Epoch: 49 [32200/36045]\tLoss: 588.1268\n",
      "Training Epoch: 49 [32250/36045]\tLoss: 598.1885\n",
      "Training Epoch: 49 [32300/36045]\tLoss: 580.4633\n",
      "Training Epoch: 49 [32350/36045]\tLoss: 576.6622\n",
      "Training Epoch: 49 [32400/36045]\tLoss: 541.1350\n",
      "Training Epoch: 49 [32450/36045]\tLoss: 445.6658\n",
      "Training Epoch: 49 [32500/36045]\tLoss: 428.1335\n",
      "Training Epoch: 49 [32550/36045]\tLoss: 430.1577\n",
      "Training Epoch: 49 [32600/36045]\tLoss: 427.3505\n",
      "Training Epoch: 49 [32650/36045]\tLoss: 555.6990\n",
      "Training Epoch: 49 [32700/36045]\tLoss: 607.1338\n",
      "Training Epoch: 49 [32750/36045]\tLoss: 577.9457\n",
      "Training Epoch: 49 [32800/36045]\tLoss: 592.4457\n",
      "Training Epoch: 49 [32850/36045]\tLoss: 546.9089\n",
      "Training Epoch: 49 [32900/36045]\tLoss: 436.4067\n",
      "Training Epoch: 49 [32950/36045]\tLoss: 457.2190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 49 [33000/36045]\tLoss: 455.6411\n",
      "Training Epoch: 49 [33050/36045]\tLoss: 433.7973\n",
      "Training Epoch: 49 [33100/36045]\tLoss: 492.9783\n",
      "Training Epoch: 49 [33150/36045]\tLoss: 671.7542\n",
      "Training Epoch: 49 [33200/36045]\tLoss: 653.9109\n",
      "Training Epoch: 49 [33250/36045]\tLoss: 673.7489\n",
      "Training Epoch: 49 [33300/36045]\tLoss: 718.3932\n",
      "Training Epoch: 49 [33350/36045]\tLoss: 549.5985\n",
      "Training Epoch: 49 [33400/36045]\tLoss: 399.2651\n",
      "Training Epoch: 49 [33450/36045]\tLoss: 395.4304\n",
      "Training Epoch: 49 [33500/36045]\tLoss: 406.9649\n",
      "Training Epoch: 49 [33550/36045]\tLoss: 421.1655\n",
      "Training Epoch: 49 [33600/36045]\tLoss: 422.9798\n",
      "Training Epoch: 49 [33650/36045]\tLoss: 565.5434\n",
      "Training Epoch: 49 [33700/36045]\tLoss: 547.0538\n",
      "Training Epoch: 49 [33750/36045]\tLoss: 566.4899\n",
      "Training Epoch: 49 [33800/36045]\tLoss: 564.1255\n",
      "Training Epoch: 49 [33850/36045]\tLoss: 565.7229\n",
      "Training Epoch: 49 [33900/36045]\tLoss: 579.3808\n",
      "Training Epoch: 49 [33950/36045]\tLoss: 588.7095\n",
      "Training Epoch: 49 [34000/36045]\tLoss: 575.5179\n",
      "Training Epoch: 49 [34050/36045]\tLoss: 579.1364\n",
      "Training Epoch: 49 [34100/36045]\tLoss: 558.5852\n",
      "Training Epoch: 49 [34150/36045]\tLoss: 516.9900\n",
      "Training Epoch: 49 [34200/36045]\tLoss: 489.7310\n",
      "Training Epoch: 49 [34250/36045]\tLoss: 503.4136\n",
      "Training Epoch: 49 [34300/36045]\tLoss: 429.4439\n",
      "Training Epoch: 49 [34350/36045]\tLoss: 452.4322\n",
      "Training Epoch: 49 [34400/36045]\tLoss: 445.9171\n",
      "Training Epoch: 49 [34450/36045]\tLoss: 420.0130\n",
      "Training Epoch: 49 [34500/36045]\tLoss: 447.9144\n",
      "Training Epoch: 49 [34550/36045]\tLoss: 439.7365\n",
      "Training Epoch: 49 [34600/36045]\tLoss: 447.3174\n",
      "Training Epoch: 49 [34650/36045]\tLoss: 552.7801\n",
      "Training Epoch: 49 [34700/36045]\tLoss: 586.5272\n",
      "Training Epoch: 49 [34750/36045]\tLoss: 519.6939\n",
      "Training Epoch: 49 [34800/36045]\tLoss: 597.2867\n",
      "Training Epoch: 49 [34850/36045]\tLoss: 604.0817\n",
      "Training Epoch: 49 [34900/36045]\tLoss: 648.5538\n",
      "Training Epoch: 49 [34950/36045]\tLoss: 633.0129\n",
      "Training Epoch: 49 [35000/36045]\tLoss: 634.3061\n",
      "Training Epoch: 49 [35050/36045]\tLoss: 621.9153\n",
      "Training Epoch: 49 [35100/36045]\tLoss: 539.4532\n",
      "Training Epoch: 49 [35150/36045]\tLoss: 531.1965\n",
      "Training Epoch: 49 [35200/36045]\tLoss: 445.8976\n",
      "Training Epoch: 49 [35250/36045]\tLoss: 490.4378\n",
      "Training Epoch: 49 [35300/36045]\tLoss: 507.1590\n",
      "Training Epoch: 49 [35350/36045]\tLoss: 564.8738\n",
      "Training Epoch: 49 [35400/36045]\tLoss: 593.1112\n",
      "Training Epoch: 49 [35450/36045]\tLoss: 564.7469\n",
      "Training Epoch: 49 [35500/36045]\tLoss: 546.2740\n",
      "Training Epoch: 49 [35550/36045]\tLoss: 533.2474\n",
      "Training Epoch: 49 [35600/36045]\tLoss: 584.5283\n",
      "Training Epoch: 49 [35650/36045]\tLoss: 656.1841\n",
      "Training Epoch: 49 [35700/36045]\tLoss: 582.7766\n",
      "Training Epoch: 49 [35750/36045]\tLoss: 640.2325\n",
      "Training Epoch: 49 [35800/36045]\tLoss: 647.2477\n",
      "Training Epoch: 49 [35850/36045]\tLoss: 621.3214\n",
      "Training Epoch: 49 [35900/36045]\tLoss: 641.9962\n",
      "Training Epoch: 49 [35950/36045]\tLoss: 638.2867\n",
      "Training Epoch: 49 [36000/36045]\tLoss: 632.0021\n",
      "Training Epoch: 49 [36045/36045]\tLoss: 618.2687\n",
      "Training Epoch: 49 [4004/4004]\tLoss: 573.9190\n",
      "Training Epoch: 50 [50/36045]\tLoss: 572.0654\n",
      "Training Epoch: 50 [100/36045]\tLoss: 547.5107\n",
      "Training Epoch: 50 [150/36045]\tLoss: 545.0858\n",
      "Training Epoch: 50 [200/36045]\tLoss: 531.5667\n",
      "Training Epoch: 50 [250/36045]\tLoss: 640.7308\n",
      "Training Epoch: 50 [300/36045]\tLoss: 706.2997\n",
      "Training Epoch: 50 [350/36045]\tLoss: 673.1398\n",
      "Training Epoch: 50 [400/36045]\tLoss: 666.8777\n",
      "Training Epoch: 50 [450/36045]\tLoss: 647.7266\n",
      "Training Epoch: 50 [500/36045]\tLoss: 598.8733\n",
      "Training Epoch: 50 [550/36045]\tLoss: 602.0913\n",
      "Training Epoch: 50 [600/36045]\tLoss: 588.7684\n",
      "Training Epoch: 50 [650/36045]\tLoss: 609.2534\n",
      "Training Epoch: 50 [700/36045]\tLoss: 593.9359\n",
      "Training Epoch: 50 [750/36045]\tLoss: 569.2798\n",
      "Training Epoch: 50 [800/36045]\tLoss: 580.9774\n",
      "Training Epoch: 50 [850/36045]\tLoss: 564.9856\n",
      "Training Epoch: 50 [900/36045]\tLoss: 541.2785\n",
      "Training Epoch: 50 [950/36045]\tLoss: 511.3570\n",
      "Training Epoch: 50 [1000/36045]\tLoss: 495.9078\n",
      "Training Epoch: 50 [1050/36045]\tLoss: 498.1673\n",
      "Training Epoch: 50 [1100/36045]\tLoss: 485.0860\n",
      "Training Epoch: 50 [1150/36045]\tLoss: 494.6802\n",
      "Training Epoch: 50 [1200/36045]\tLoss: 523.8495\n",
      "Training Epoch: 50 [1250/36045]\tLoss: 598.8442\n",
      "Training Epoch: 50 [1300/36045]\tLoss: 605.8243\n",
      "Training Epoch: 50 [1350/36045]\tLoss: 607.1638\n",
      "Training Epoch: 50 [1400/36045]\tLoss: 630.5988\n",
      "Training Epoch: 50 [1450/36045]\tLoss: 610.3877\n",
      "Training Epoch: 50 [1500/36045]\tLoss: 557.2009\n",
      "Training Epoch: 50 [1550/36045]\tLoss: 571.2960\n",
      "Training Epoch: 50 [1600/36045]\tLoss: 580.9944\n",
      "Training Epoch: 50 [1650/36045]\tLoss: 568.7270\n",
      "Training Epoch: 50 [1700/36045]\tLoss: 581.1169\n",
      "Training Epoch: 50 [1750/36045]\tLoss: 622.7737\n",
      "Training Epoch: 50 [1800/36045]\tLoss: 605.7792\n",
      "Training Epoch: 50 [1850/36045]\tLoss: 621.0859\n",
      "Training Epoch: 50 [1900/36045]\tLoss: 580.5654\n",
      "Training Epoch: 50 [1950/36045]\tLoss: 590.1736\n",
      "Training Epoch: 50 [2000/36045]\tLoss: 531.0278\n",
      "Training Epoch: 50 [2050/36045]\tLoss: 533.8715\n",
      "Training Epoch: 50 [2100/36045]\tLoss: 562.7035\n",
      "Training Epoch: 50 [2150/36045]\tLoss: 543.7520\n",
      "Training Epoch: 50 [2200/36045]\tLoss: 506.9057\n",
      "Training Epoch: 50 [2250/36045]\tLoss: 479.2823\n",
      "Training Epoch: 50 [2300/36045]\tLoss: 502.6809\n",
      "Training Epoch: 50 [2350/36045]\tLoss: 480.6476\n",
      "Training Epoch: 50 [2400/36045]\tLoss: 487.9272\n",
      "Training Epoch: 50 [2450/36045]\tLoss: 625.3102\n",
      "Training Epoch: 50 [2500/36045]\tLoss: 656.5453\n",
      "Training Epoch: 50 [2550/36045]\tLoss: 654.6172\n",
      "Training Epoch: 50 [2600/36045]\tLoss: 663.4395\n",
      "Training Epoch: 50 [2650/36045]\tLoss: 787.3950\n",
      "Training Epoch: 50 [2700/36045]\tLoss: 875.4008\n",
      "Training Epoch: 50 [2750/36045]\tLoss: 945.9538\n",
      "Training Epoch: 50 [2800/36045]\tLoss: 955.1122\n",
      "Training Epoch: 50 [2850/36045]\tLoss: 721.9561\n",
      "Training Epoch: 50 [2900/36045]\tLoss: 682.4554\n",
      "Training Epoch: 50 [2950/36045]\tLoss: 660.4531\n",
      "Training Epoch: 50 [3000/36045]\tLoss: 654.1376\n",
      "Training Epoch: 50 [3050/36045]\tLoss: 684.6802\n",
      "Training Epoch: 50 [3100/36045]\tLoss: 625.3239\n",
      "Training Epoch: 50 [3150/36045]\tLoss: 480.4969\n",
      "Training Epoch: 50 [3200/36045]\tLoss: 497.4799\n",
      "Training Epoch: 50 [3250/36045]\tLoss: 469.7891\n",
      "Training Epoch: 50 [3300/36045]\tLoss: 444.8922\n",
      "Training Epoch: 50 [3350/36045]\tLoss: 469.9076\n",
      "Training Epoch: 50 [3400/36045]\tLoss: 492.1570\n",
      "Training Epoch: 50 [3450/36045]\tLoss: 527.6912\n",
      "Training Epoch: 50 [3500/36045]\tLoss: 515.2151\n",
      "Training Epoch: 50 [3550/36045]\tLoss: 492.6205\n",
      "Training Epoch: 50 [3600/36045]\tLoss: 529.6723\n",
      "Training Epoch: 50 [3650/36045]\tLoss: 612.4752\n",
      "Training Epoch: 50 [3700/36045]\tLoss: 620.2145\n",
      "Training Epoch: 50 [3750/36045]\tLoss: 589.5279\n",
      "Training Epoch: 50 [3800/36045]\tLoss: 587.1232\n",
      "Training Epoch: 50 [3850/36045]\tLoss: 589.5446\n",
      "Training Epoch: 50 [3900/36045]\tLoss: 593.9846\n",
      "Training Epoch: 50 [3950/36045]\tLoss: 573.3114\n",
      "Training Epoch: 50 [4000/36045]\tLoss: 577.7106\n",
      "Training Epoch: 50 [4050/36045]\tLoss: 529.4709\n",
      "Training Epoch: 50 [4100/36045]\tLoss: 516.4790\n",
      "Training Epoch: 50 [4150/36045]\tLoss: 530.1748\n",
      "Training Epoch: 50 [4200/36045]\tLoss: 525.1456\n",
      "Training Epoch: 50 [4250/36045]\tLoss: 527.1453\n",
      "Training Epoch: 50 [4300/36045]\tLoss: 541.7205\n",
      "Training Epoch: 50 [4350/36045]\tLoss: 525.9437\n",
      "Training Epoch: 50 [4400/36045]\tLoss: 502.9356\n",
      "Training Epoch: 50 [4450/36045]\tLoss: 552.9464\n",
      "Training Epoch: 50 [4500/36045]\tLoss: 595.6616\n",
      "Training Epoch: 50 [4550/36045]\tLoss: 598.1927\n",
      "Training Epoch: 50 [4600/36045]\tLoss: 619.8550\n",
      "Training Epoch: 50 [4650/36045]\tLoss: 610.1245\n",
      "Training Epoch: 50 [4700/36045]\tLoss: 562.3279\n",
      "Training Epoch: 50 [4750/36045]\tLoss: 544.0624\n",
      "Training Epoch: 50 [4800/36045]\tLoss: 567.8702\n",
      "Training Epoch: 50 [4850/36045]\tLoss: 554.5377\n",
      "Training Epoch: 50 [4900/36045]\tLoss: 539.9975\n",
      "Training Epoch: 50 [4950/36045]\tLoss: 555.3621\n",
      "Training Epoch: 50 [5000/36045]\tLoss: 584.5534\n",
      "Training Epoch: 50 [5050/36045]\tLoss: 566.3517\n",
      "Training Epoch: 50 [5100/36045]\tLoss: 576.3676\n",
      "Training Epoch: 50 [5150/36045]\tLoss: 560.9229\n",
      "Training Epoch: 50 [5200/36045]\tLoss: 558.8691\n",
      "Training Epoch: 50 [5250/36045]\tLoss: 552.4798\n",
      "Training Epoch: 50 [5300/36045]\tLoss: 552.4070\n",
      "Training Epoch: 50 [5350/36045]\tLoss: 573.0899\n",
      "Training Epoch: 50 [5400/36045]\tLoss: 552.5386\n",
      "Training Epoch: 50 [5450/36045]\tLoss: 523.4147\n",
      "Training Epoch: 50 [5500/36045]\tLoss: 551.4621\n",
      "Training Epoch: 50 [5550/36045]\tLoss: 539.7239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 50 [5600/36045]\tLoss: 617.4199\n",
      "Training Epoch: 50 [5650/36045]\tLoss: 583.6403\n",
      "Training Epoch: 50 [5700/36045]\tLoss: 547.6718\n",
      "Training Epoch: 50 [5750/36045]\tLoss: 531.7545\n",
      "Training Epoch: 50 [5800/36045]\tLoss: 561.0036\n",
      "Training Epoch: 50 [5850/36045]\tLoss: 550.9639\n",
      "Training Epoch: 50 [5900/36045]\tLoss: 632.8368\n",
      "Training Epoch: 50 [5950/36045]\tLoss: 649.0954\n",
      "Training Epoch: 50 [6000/36045]\tLoss: 635.3046\n",
      "Training Epoch: 50 [6050/36045]\tLoss: 613.6613\n",
      "Training Epoch: 50 [6100/36045]\tLoss: 617.9500\n",
      "Training Epoch: 50 [6150/36045]\tLoss: 609.5801\n",
      "Training Epoch: 50 [6200/36045]\tLoss: 614.7814\n",
      "Training Epoch: 50 [6250/36045]\tLoss: 636.2232\n",
      "Training Epoch: 50 [6300/36045]\tLoss: 647.7380\n",
      "Training Epoch: 50 [6350/36045]\tLoss: 692.6407\n",
      "Training Epoch: 50 [6400/36045]\tLoss: 569.6277\n",
      "Training Epoch: 50 [6450/36045]\tLoss: 523.2182\n",
      "Training Epoch: 50 [6500/36045]\tLoss: 532.6608\n",
      "Training Epoch: 50 [6550/36045]\tLoss: 550.0837\n",
      "Training Epoch: 50 [6600/36045]\tLoss: 547.8618\n",
      "Training Epoch: 50 [6650/36045]\tLoss: 618.9902\n",
      "Training Epoch: 50 [6700/36045]\tLoss: 647.2266\n",
      "Training Epoch: 50 [6750/36045]\tLoss: 625.0240\n",
      "Training Epoch: 50 [6800/36045]\tLoss: 627.9957\n",
      "Training Epoch: 50 [6850/36045]\tLoss: 616.1351\n",
      "Training Epoch: 50 [6900/36045]\tLoss: 548.8116\n",
      "Training Epoch: 50 [6950/36045]\tLoss: 517.4735\n",
      "Training Epoch: 50 [7000/36045]\tLoss: 550.3228\n",
      "Training Epoch: 50 [7050/36045]\tLoss: 562.3440\n",
      "Training Epoch: 50 [7100/36045]\tLoss: 561.2396\n",
      "Training Epoch: 50 [7150/36045]\tLoss: 571.0607\n",
      "Training Epoch: 50 [7200/36045]\tLoss: 572.9570\n",
      "Training Epoch: 50 [7250/36045]\tLoss: 571.4160\n",
      "Training Epoch: 50 [7300/36045]\tLoss: 557.7247\n",
      "Training Epoch: 50 [7350/36045]\tLoss: 555.6274\n",
      "Training Epoch: 50 [7400/36045]\tLoss: 507.1237\n",
      "Training Epoch: 50 [7450/36045]\tLoss: 510.4029\n",
      "Training Epoch: 50 [7500/36045]\tLoss: 506.1588\n",
      "Training Epoch: 50 [7550/36045]\tLoss: 484.7577\n",
      "Training Epoch: 50 [7600/36045]\tLoss: 536.9998\n",
      "Training Epoch: 50 [7650/36045]\tLoss: 573.7429\n",
      "Training Epoch: 50 [7700/36045]\tLoss: 545.9977\n",
      "Training Epoch: 50 [7750/36045]\tLoss: 560.2651\n",
      "Training Epoch: 50 [7800/36045]\tLoss: 550.0245\n",
      "Training Epoch: 50 [7850/36045]\tLoss: 532.2991\n",
      "Training Epoch: 50 [7900/36045]\tLoss: 561.2666\n",
      "Training Epoch: 50 [7950/36045]\tLoss: 558.9227\n",
      "Training Epoch: 50 [8000/36045]\tLoss: 577.0098\n",
      "Training Epoch: 50 [8050/36045]\tLoss: 543.0945\n",
      "Training Epoch: 50 [8100/36045]\tLoss: 568.0192\n",
      "Training Epoch: 50 [8150/36045]\tLoss: 644.7977\n",
      "Training Epoch: 50 [8200/36045]\tLoss: 632.4671\n",
      "Training Epoch: 50 [8250/36045]\tLoss: 601.0507\n",
      "Training Epoch: 50 [8300/36045]\tLoss: 657.3447\n",
      "Training Epoch: 50 [8350/36045]\tLoss: 602.2224\n",
      "Training Epoch: 50 [8400/36045]\tLoss: 538.3326\n",
      "Training Epoch: 50 [8450/36045]\tLoss: 503.7147\n",
      "Training Epoch: 50 [8500/36045]\tLoss: 536.2552\n",
      "Training Epoch: 50 [8550/36045]\tLoss: 530.1360\n",
      "Training Epoch: 50 [8600/36045]\tLoss: 524.5250\n",
      "Training Epoch: 50 [8650/36045]\tLoss: 555.8687\n",
      "Training Epoch: 50 [8700/36045]\tLoss: 587.9462\n",
      "Training Epoch: 50 [8750/36045]\tLoss: 577.6852\n",
      "Training Epoch: 50 [8800/36045]\tLoss: 583.3677\n",
      "Training Epoch: 50 [8850/36045]\tLoss: 577.0585\n",
      "Training Epoch: 50 [8900/36045]\tLoss: 520.9497\n",
      "Training Epoch: 50 [8950/36045]\tLoss: 531.3815\n",
      "Training Epoch: 50 [9000/36045]\tLoss: 547.3095\n",
      "Training Epoch: 50 [9050/36045]\tLoss: 548.9133\n",
      "Training Epoch: 50 [9100/36045]\tLoss: 565.4952\n",
      "Training Epoch: 50 [9150/36045]\tLoss: 418.5962\n",
      "Training Epoch: 50 [9200/36045]\tLoss: 312.5688\n",
      "Training Epoch: 50 [9250/36045]\tLoss: 339.2531\n",
      "Training Epoch: 50 [9300/36045]\tLoss: 348.7417\n",
      "Training Epoch: 50 [9350/36045]\tLoss: 321.9730\n",
      "Training Epoch: 50 [9400/36045]\tLoss: 630.2715\n",
      "Training Epoch: 50 [9450/36045]\tLoss: 669.3055\n",
      "Training Epoch: 50 [9500/36045]\tLoss: 657.1085\n",
      "Training Epoch: 50 [9550/36045]\tLoss: 695.1781\n",
      "Training Epoch: 50 [9600/36045]\tLoss: 518.5609\n",
      "Training Epoch: 50 [9650/36045]\tLoss: 523.7083\n",
      "Training Epoch: 50 [9700/36045]\tLoss: 509.3479\n",
      "Training Epoch: 50 [9750/36045]\tLoss: 507.9622\n",
      "Training Epoch: 50 [9800/36045]\tLoss: 664.3264\n",
      "Training Epoch: 50 [9850/36045]\tLoss: 701.4071\n",
      "Training Epoch: 50 [9900/36045]\tLoss: 710.0908\n",
      "Training Epoch: 50 [9950/36045]\tLoss: 692.0530\n",
      "Training Epoch: 50 [10000/36045]\tLoss: 640.8790\n",
      "Training Epoch: 50 [10050/36045]\tLoss: 524.3368\n",
      "Training Epoch: 50 [10100/36045]\tLoss: 531.8549\n",
      "Training Epoch: 50 [10150/36045]\tLoss: 539.6982\n",
      "Training Epoch: 50 [10200/36045]\tLoss: 528.8383\n",
      "Training Epoch: 50 [10250/36045]\tLoss: 636.7191\n",
      "Training Epoch: 50 [10300/36045]\tLoss: 618.8271\n",
      "Training Epoch: 50 [10350/36045]\tLoss: 651.7508\n",
      "Training Epoch: 50 [10400/36045]\tLoss: 641.6976\n",
      "Training Epoch: 50 [10450/36045]\tLoss: 601.3989\n",
      "Training Epoch: 50 [10500/36045]\tLoss: 502.2939\n",
      "Training Epoch: 50 [10550/36045]\tLoss: 496.7122\n",
      "Training Epoch: 50 [10600/36045]\tLoss: 518.2897\n",
      "Training Epoch: 50 [10650/36045]\tLoss: 524.0870\n",
      "Training Epoch: 50 [10700/36045]\tLoss: 602.8946\n",
      "Training Epoch: 50 [10750/36045]\tLoss: 661.4943\n",
      "Training Epoch: 50 [10800/36045]\tLoss: 607.5808\n",
      "Training Epoch: 50 [10850/36045]\tLoss: 644.5126\n",
      "Training Epoch: 50 [10900/36045]\tLoss: 671.0294\n",
      "Training Epoch: 50 [10950/36045]\tLoss: 493.2054\n",
      "Training Epoch: 50 [11000/36045]\tLoss: 487.0355\n",
      "Training Epoch: 50 [11050/36045]\tLoss: 522.4219\n",
      "Training Epoch: 50 [11100/36045]\tLoss: 532.3469\n",
      "Training Epoch: 50 [11150/36045]\tLoss: 577.6248\n",
      "Training Epoch: 50 [11200/36045]\tLoss: 606.3084\n",
      "Training Epoch: 50 [11250/36045]\tLoss: 617.4432\n",
      "Training Epoch: 50 [11300/36045]\tLoss: 597.8835\n",
      "Training Epoch: 50 [11350/36045]\tLoss: 595.6641\n",
      "Training Epoch: 50 [11400/36045]\tLoss: 559.4813\n",
      "Training Epoch: 50 [11450/36045]\tLoss: 528.9592\n",
      "Training Epoch: 50 [11500/36045]\tLoss: 526.5051\n",
      "Training Epoch: 50 [11550/36045]\tLoss: 535.7871\n",
      "Training Epoch: 50 [11600/36045]\tLoss: 595.5110\n",
      "Training Epoch: 50 [11650/36045]\tLoss: 646.5782\n",
      "Training Epoch: 50 [11700/36045]\tLoss: 645.4948\n",
      "Training Epoch: 50 [11750/36045]\tLoss: 663.2992\n",
      "Training Epoch: 50 [11800/36045]\tLoss: 705.7393\n",
      "Training Epoch: 50 [11850/36045]\tLoss: 764.3483\n",
      "Training Epoch: 50 [11900/36045]\tLoss: 975.7529\n",
      "Training Epoch: 50 [11950/36045]\tLoss: 979.0121\n",
      "Training Epoch: 50 [12000/36045]\tLoss: 989.9158\n",
      "Training Epoch: 50 [12050/36045]\tLoss: 949.7978\n",
      "Training Epoch: 50 [12100/36045]\tLoss: 605.4845\n",
      "Training Epoch: 50 [12150/36045]\tLoss: 454.0361\n",
      "Training Epoch: 50 [12200/36045]\tLoss: 448.9060\n",
      "Training Epoch: 50 [12250/36045]\tLoss: 457.4325\n",
      "Training Epoch: 50 [12300/36045]\tLoss: 591.3870\n",
      "Training Epoch: 50 [12350/36045]\tLoss: 646.1328\n",
      "Training Epoch: 50 [12400/36045]\tLoss: 653.2155\n",
      "Training Epoch: 50 [12450/36045]\tLoss: 642.2211\n",
      "Training Epoch: 50 [12500/36045]\tLoss: 668.9354\n",
      "Training Epoch: 50 [12550/36045]\tLoss: 638.9690\n",
      "Training Epoch: 50 [12600/36045]\tLoss: 583.3733\n",
      "Training Epoch: 50 [12650/36045]\tLoss: 581.4831\n",
      "Training Epoch: 50 [12700/36045]\tLoss: 602.6693\n",
      "Training Epoch: 50 [12750/36045]\tLoss: 601.1376\n",
      "Training Epoch: 50 [12800/36045]\tLoss: 587.9229\n",
      "Training Epoch: 50 [12850/36045]\tLoss: 617.1208\n",
      "Training Epoch: 50 [12900/36045]\tLoss: 591.5002\n",
      "Training Epoch: 50 [12950/36045]\tLoss: 577.0206\n",
      "Training Epoch: 50 [13000/36045]\tLoss: 610.4182\n",
      "Training Epoch: 50 [13050/36045]\tLoss: 551.3783\n",
      "Training Epoch: 50 [13100/36045]\tLoss: 566.1891\n",
      "Training Epoch: 50 [13150/36045]\tLoss: 557.8201\n",
      "Training Epoch: 50 [13200/36045]\tLoss: 541.3286\n",
      "Training Epoch: 50 [13250/36045]\tLoss: 562.6526\n",
      "Training Epoch: 50 [13300/36045]\tLoss: 598.9579\n",
      "Training Epoch: 50 [13350/36045]\tLoss: 580.2412\n",
      "Training Epoch: 50 [13400/36045]\tLoss: 583.2899\n",
      "Training Epoch: 50 [13450/36045]\tLoss: 580.9170\n",
      "Training Epoch: 50 [13500/36045]\tLoss: 599.1367\n",
      "Training Epoch: 50 [13550/36045]\tLoss: 737.4475\n",
      "Training Epoch: 50 [13600/36045]\tLoss: 769.9979\n",
      "Training Epoch: 50 [13650/36045]\tLoss: 852.0043\n",
      "Training Epoch: 50 [13700/36045]\tLoss: 749.7289\n",
      "Training Epoch: 50 [13750/36045]\tLoss: 586.6805\n",
      "Training Epoch: 50 [13800/36045]\tLoss: 557.2671\n",
      "Training Epoch: 50 [13850/36045]\tLoss: 540.1012\n",
      "Training Epoch: 50 [13900/36045]\tLoss: 547.3438\n",
      "Training Epoch: 50 [13950/36045]\tLoss: 592.4156\n",
      "Training Epoch: 50 [14000/36045]\tLoss: 624.7929\n",
      "Training Epoch: 50 [14050/36045]\tLoss: 599.9512\n",
      "Training Epoch: 50 [14100/36045]\tLoss: 594.9734\n",
      "Training Epoch: 50 [14150/36045]\tLoss: 583.1976\n",
      "Training Epoch: 50 [14200/36045]\tLoss: 623.4304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 50 [14250/36045]\tLoss: 685.2802\n",
      "Training Epoch: 50 [14300/36045]\tLoss: 688.3249\n",
      "Training Epoch: 50 [14350/36045]\tLoss: 657.6219\n",
      "Training Epoch: 50 [14400/36045]\tLoss: 643.9016\n",
      "Training Epoch: 50 [14450/36045]\tLoss: 679.1678\n",
      "Training Epoch: 50 [14500/36045]\tLoss: 610.9735\n",
      "Training Epoch: 50 [14550/36045]\tLoss: 638.3476\n",
      "Training Epoch: 50 [14600/36045]\tLoss: 624.9074\n",
      "Training Epoch: 50 [14650/36045]\tLoss: 624.7477\n",
      "Training Epoch: 50 [14700/36045]\tLoss: 592.7769\n",
      "Training Epoch: 50 [14750/36045]\tLoss: 510.1729\n",
      "Training Epoch: 50 [14800/36045]\tLoss: 500.0896\n",
      "Training Epoch: 50 [14850/36045]\tLoss: 507.0886\n",
      "Training Epoch: 50 [14900/36045]\tLoss: 500.6571\n",
      "Training Epoch: 50 [14950/36045]\tLoss: 508.9927\n",
      "Training Epoch: 50 [15000/36045]\tLoss: 521.2442\n",
      "Training Epoch: 50 [15050/36045]\tLoss: 517.5031\n",
      "Training Epoch: 50 [15100/36045]\tLoss: 501.4331\n",
      "Training Epoch: 50 [15150/36045]\tLoss: 497.4334\n",
      "Training Epoch: 50 [15200/36045]\tLoss: 461.1125\n",
      "Training Epoch: 50 [15250/36045]\tLoss: 482.5040\n",
      "Training Epoch: 50 [15300/36045]\tLoss: 468.3394\n",
      "Training Epoch: 50 [15350/36045]\tLoss: 479.7672\n",
      "Training Epoch: 50 [15400/36045]\tLoss: 461.5174\n",
      "Training Epoch: 50 [15450/36045]\tLoss: 446.9686\n",
      "Training Epoch: 50 [15500/36045]\tLoss: 459.7886\n",
      "Training Epoch: 50 [15550/36045]\tLoss: 456.7432\n",
      "Training Epoch: 50 [15600/36045]\tLoss: 522.8958\n",
      "Training Epoch: 50 [15650/36045]\tLoss: 539.0911\n",
      "Training Epoch: 50 [15700/36045]\tLoss: 531.5959\n",
      "Training Epoch: 50 [15750/36045]\tLoss: 523.1306\n",
      "Training Epoch: 50 [15800/36045]\tLoss: 503.0298\n",
      "Training Epoch: 50 [15850/36045]\tLoss: 518.5610\n",
      "Training Epoch: 50 [15900/36045]\tLoss: 527.2024\n",
      "Training Epoch: 50 [15950/36045]\tLoss: 546.9319\n",
      "Training Epoch: 50 [16000/36045]\tLoss: 517.3500\n",
      "Training Epoch: 50 [16050/36045]\tLoss: 487.0025\n",
      "Training Epoch: 50 [16100/36045]\tLoss: 452.1304\n",
      "Training Epoch: 50 [16150/36045]\tLoss: 440.2921\n",
      "Training Epoch: 50 [16200/36045]\tLoss: 535.2036\n",
      "Training Epoch: 50 [16250/36045]\tLoss: 562.3061\n",
      "Training Epoch: 50 [16300/36045]\tLoss: 613.7867\n",
      "Training Epoch: 50 [16350/36045]\tLoss: 634.3622\n",
      "Training Epoch: 50 [16400/36045]\tLoss: 606.2724\n",
      "Training Epoch: 50 [16450/36045]\tLoss: 588.7801\n",
      "Training Epoch: 50 [16500/36045]\tLoss: 588.5870\n",
      "Training Epoch: 50 [16550/36045]\tLoss: 554.2527\n",
      "Training Epoch: 50 [16600/36045]\tLoss: 575.5861\n",
      "Training Epoch: 50 [16650/36045]\tLoss: 591.2067\n",
      "Training Epoch: 50 [16700/36045]\tLoss: 571.9896\n",
      "Training Epoch: 50 [16750/36045]\tLoss: 564.7736\n",
      "Training Epoch: 50 [16800/36045]\tLoss: 572.9427\n",
      "Training Epoch: 50 [16850/36045]\tLoss: 546.1807\n",
      "Training Epoch: 50 [16900/36045]\tLoss: 555.9293\n",
      "Training Epoch: 50 [16950/36045]\tLoss: 578.3740\n",
      "Training Epoch: 50 [17000/36045]\tLoss: 563.1936\n",
      "Training Epoch: 50 [17050/36045]\tLoss: 586.2239\n",
      "Training Epoch: 50 [17100/36045]\tLoss: 582.0227\n",
      "Training Epoch: 50 [17150/36045]\tLoss: 504.8279\n",
      "Training Epoch: 50 [17200/36045]\tLoss: 467.4380\n",
      "Training Epoch: 50 [17250/36045]\tLoss: 489.7510\n",
      "Training Epoch: 50 [17300/36045]\tLoss: 518.4700\n",
      "Training Epoch: 50 [17350/36045]\tLoss: 500.5064\n",
      "Training Epoch: 50 [17400/36045]\tLoss: 520.1287\n",
      "Training Epoch: 50 [17450/36045]\tLoss: 538.1314\n",
      "Training Epoch: 50 [17500/36045]\tLoss: 526.7006\n",
      "Training Epoch: 50 [17550/36045]\tLoss: 525.1216\n",
      "Training Epoch: 50 [17600/36045]\tLoss: 519.6681\n",
      "Training Epoch: 50 [17650/36045]\tLoss: 534.6441\n",
      "Training Epoch: 50 [17700/36045]\tLoss: 514.2620\n",
      "Training Epoch: 50 [17750/36045]\tLoss: 529.8167\n",
      "Training Epoch: 50 [17800/36045]\tLoss: 521.3054\n",
      "Training Epoch: 50 [17850/36045]\tLoss: 538.8210\n",
      "Training Epoch: 50 [17900/36045]\tLoss: 566.4786\n",
      "Training Epoch: 50 [17950/36045]\tLoss: 578.8575\n",
      "Training Epoch: 50 [18000/36045]\tLoss: 569.9769\n",
      "Training Epoch: 50 [18050/36045]\tLoss: 624.3220\n",
      "Training Epoch: 50 [18100/36045]\tLoss: 624.9285\n",
      "Training Epoch: 50 [18150/36045]\tLoss: 636.0944\n",
      "Training Epoch: 50 [18200/36045]\tLoss: 618.8261\n",
      "Training Epoch: 50 [18250/36045]\tLoss: 639.4267\n",
      "Training Epoch: 50 [18300/36045]\tLoss: 597.0796\n",
      "Training Epoch: 50 [18350/36045]\tLoss: 670.5401\n",
      "Training Epoch: 50 [18400/36045]\tLoss: 643.1514\n",
      "Training Epoch: 50 [18450/36045]\tLoss: 623.0602\n",
      "Training Epoch: 50 [18500/36045]\tLoss: 621.9094\n",
      "Training Epoch: 50 [18550/36045]\tLoss: 609.6944\n",
      "Training Epoch: 50 [18600/36045]\tLoss: 599.4604\n",
      "Training Epoch: 50 [18650/36045]\tLoss: 644.4261\n",
      "Training Epoch: 50 [18700/36045]\tLoss: 677.8716\n",
      "Training Epoch: 50 [18750/36045]\tLoss: 665.7818\n",
      "Training Epoch: 50 [18800/36045]\tLoss: 688.2763\n",
      "Training Epoch: 50 [18850/36045]\tLoss: 633.5218\n",
      "Training Epoch: 50 [18900/36045]\tLoss: 677.5486\n",
      "Training Epoch: 50 [18950/36045]\tLoss: 620.3062\n",
      "Training Epoch: 50 [19000/36045]\tLoss: 506.9446\n",
      "Training Epoch: 50 [19050/36045]\tLoss: 492.1438\n",
      "Training Epoch: 50 [19100/36045]\tLoss: 499.9894\n",
      "Training Epoch: 50 [19150/36045]\tLoss: 490.3094\n",
      "Training Epoch: 50 [19200/36045]\tLoss: 521.7961\n",
      "Training Epoch: 50 [19250/36045]\tLoss: 537.2773\n",
      "Training Epoch: 50 [19300/36045]\tLoss: 546.5068\n",
      "Training Epoch: 50 [19350/36045]\tLoss: 530.4530\n",
      "Training Epoch: 50 [19400/36045]\tLoss: 550.0643\n",
      "Training Epoch: 50 [19450/36045]\tLoss: 541.5872\n",
      "Training Epoch: 50 [19500/36045]\tLoss: 542.5790\n",
      "Training Epoch: 50 [19550/36045]\tLoss: 540.7932\n",
      "Training Epoch: 50 [19600/36045]\tLoss: 581.5191\n",
      "Training Epoch: 50 [19650/36045]\tLoss: 780.4189\n",
      "Training Epoch: 50 [19700/36045]\tLoss: 739.0189\n",
      "Training Epoch: 50 [19750/36045]\tLoss: 743.1600\n",
      "Training Epoch: 50 [19800/36045]\tLoss: 743.8861\n",
      "Training Epoch: 50 [19850/36045]\tLoss: 483.7696\n",
      "Training Epoch: 50 [19900/36045]\tLoss: 463.4523\n",
      "Training Epoch: 50 [19950/36045]\tLoss: 467.4799\n",
      "Training Epoch: 50 [20000/36045]\tLoss: 467.6816\n",
      "Training Epoch: 50 [20050/36045]\tLoss: 523.0173\n",
      "Training Epoch: 50 [20100/36045]\tLoss: 528.1277\n",
      "Training Epoch: 50 [20150/36045]\tLoss: 529.0073\n",
      "Training Epoch: 50 [20200/36045]\tLoss: 528.9116\n",
      "Training Epoch: 50 [20250/36045]\tLoss: 564.8245\n",
      "Training Epoch: 50 [20300/36045]\tLoss: 600.9343\n",
      "Training Epoch: 50 [20350/36045]\tLoss: 618.6344\n",
      "Training Epoch: 50 [20400/36045]\tLoss: 634.6719\n",
      "Training Epoch: 50 [20450/36045]\tLoss: 604.4167\n",
      "Training Epoch: 50 [20500/36045]\tLoss: 589.8053\n",
      "Training Epoch: 50 [20550/36045]\tLoss: 517.5975\n",
      "Training Epoch: 50 [20600/36045]\tLoss: 526.9724\n",
      "Training Epoch: 50 [20650/36045]\tLoss: 524.1176\n",
      "Training Epoch: 50 [20700/36045]\tLoss: 512.3145\n",
      "Training Epoch: 50 [20750/36045]\tLoss: 552.9521\n",
      "Training Epoch: 50 [20800/36045]\tLoss: 600.8906\n",
      "Training Epoch: 50 [20850/36045]\tLoss: 587.7837\n",
      "Training Epoch: 50 [20900/36045]\tLoss: 630.0043\n",
      "Training Epoch: 50 [20950/36045]\tLoss: 593.3134\n",
      "Training Epoch: 50 [21000/36045]\tLoss: 558.5695\n",
      "Training Epoch: 50 [21050/36045]\tLoss: 477.7558\n",
      "Training Epoch: 50 [21100/36045]\tLoss: 483.1132\n",
      "Training Epoch: 50 [21150/36045]\tLoss: 516.9819\n",
      "Training Epoch: 50 [21200/36045]\tLoss: 515.9237\n",
      "Training Epoch: 50 [21250/36045]\tLoss: 493.3341\n",
      "Training Epoch: 50 [21300/36045]\tLoss: 575.7031\n",
      "Training Epoch: 50 [21350/36045]\tLoss: 567.5180\n",
      "Training Epoch: 50 [21400/36045]\tLoss: 571.3146\n",
      "Training Epoch: 50 [21450/36045]\tLoss: 577.4559\n",
      "Training Epoch: 50 [21500/36045]\tLoss: 578.7884\n",
      "Training Epoch: 50 [21550/36045]\tLoss: 672.7952\n",
      "Training Epoch: 50 [21600/36045]\tLoss: 671.1113\n",
      "Training Epoch: 50 [21650/36045]\tLoss: 683.4227\n",
      "Training Epoch: 50 [21700/36045]\tLoss: 687.0087\n",
      "Training Epoch: 50 [21750/36045]\tLoss: 659.3121\n",
      "Training Epoch: 50 [21800/36045]\tLoss: 483.2545\n",
      "Training Epoch: 50 [21850/36045]\tLoss: 466.2516\n",
      "Training Epoch: 50 [21900/36045]\tLoss: 475.4256\n",
      "Training Epoch: 50 [21950/36045]\tLoss: 477.2923\n",
      "Training Epoch: 50 [22000/36045]\tLoss: 480.4585\n",
      "Training Epoch: 50 [22050/36045]\tLoss: 499.1589\n",
      "Training Epoch: 50 [22100/36045]\tLoss: 491.5480\n",
      "Training Epoch: 50 [22150/36045]\tLoss: 478.3708\n",
      "Training Epoch: 50 [22200/36045]\tLoss: 493.8795\n",
      "Training Epoch: 50 [22250/36045]\tLoss: 498.8242\n",
      "Training Epoch: 50 [22300/36045]\tLoss: 550.3979\n",
      "Training Epoch: 50 [22350/36045]\tLoss: 575.6441\n",
      "Training Epoch: 50 [22400/36045]\tLoss: 589.0615\n",
      "Training Epoch: 50 [22450/36045]\tLoss: 576.5827\n",
      "Training Epoch: 50 [22500/36045]\tLoss: 560.0598\n",
      "Training Epoch: 50 [22550/36045]\tLoss: 594.4378\n",
      "Training Epoch: 50 [22600/36045]\tLoss: 641.4058\n",
      "Training Epoch: 50 [22650/36045]\tLoss: 673.6793\n",
      "Training Epoch: 50 [22700/36045]\tLoss: 694.8252\n",
      "Training Epoch: 50 [22750/36045]\tLoss: 714.5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 50 [22800/36045]\tLoss: 742.2148\n",
      "Training Epoch: 50 [22850/36045]\tLoss: 615.5298\n",
      "Training Epoch: 50 [22900/36045]\tLoss: 620.1335\n",
      "Training Epoch: 50 [22950/36045]\tLoss: 599.9468\n",
      "Training Epoch: 50 [23000/36045]\tLoss: 596.4225\n",
      "Training Epoch: 50 [23050/36045]\tLoss: 530.0325\n",
      "Training Epoch: 50 [23100/36045]\tLoss: 545.3977\n",
      "Training Epoch: 50 [23150/36045]\tLoss: 533.5789\n",
      "Training Epoch: 50 [23200/36045]\tLoss: 504.8880\n",
      "Training Epoch: 50 [23250/36045]\tLoss: 507.9739\n",
      "Training Epoch: 50 [23300/36045]\tLoss: 503.9520\n",
      "Training Epoch: 50 [23350/36045]\tLoss: 523.9124\n",
      "Training Epoch: 50 [23400/36045]\tLoss: 568.2605\n",
      "Training Epoch: 50 [23450/36045]\tLoss: 562.0035\n",
      "Training Epoch: 50 [23500/36045]\tLoss: 541.4271\n",
      "Training Epoch: 50 [23550/36045]\tLoss: 580.2898\n",
      "Training Epoch: 50 [23600/36045]\tLoss: 659.8395\n",
      "Training Epoch: 50 [23650/36045]\tLoss: 670.5480\n",
      "Training Epoch: 50 [23700/36045]\tLoss: 678.5181\n",
      "Training Epoch: 50 [23750/36045]\tLoss: 655.0157\n",
      "Training Epoch: 50 [23800/36045]\tLoss: 526.1984\n",
      "Training Epoch: 50 [23850/36045]\tLoss: 551.6691\n",
      "Training Epoch: 50 [23900/36045]\tLoss: 541.6309\n",
      "Training Epoch: 50 [23950/36045]\tLoss: 524.9069\n",
      "Training Epoch: 50 [24000/36045]\tLoss: 502.3138\n",
      "Training Epoch: 50 [24050/36045]\tLoss: 463.3534\n",
      "Training Epoch: 50 [24100/36045]\tLoss: 487.4427\n",
      "Training Epoch: 50 [24150/36045]\tLoss: 479.2774\n",
      "Training Epoch: 50 [24200/36045]\tLoss: 477.2739\n",
      "Training Epoch: 50 [24250/36045]\tLoss: 463.6272\n",
      "Training Epoch: 50 [24300/36045]\tLoss: 500.6723\n",
      "Training Epoch: 50 [24350/36045]\tLoss: 512.4964\n",
      "Training Epoch: 50 [24400/36045]\tLoss: 527.3141\n",
      "Training Epoch: 50 [24450/36045]\tLoss: 502.4951\n",
      "Training Epoch: 50 [24500/36045]\tLoss: 530.6409\n",
      "Training Epoch: 50 [24550/36045]\tLoss: 616.4176\n",
      "Training Epoch: 50 [24600/36045]\tLoss: 607.4501\n",
      "Training Epoch: 50 [24650/36045]\tLoss: 581.2267\n",
      "Training Epoch: 50 [24700/36045]\tLoss: 591.2270\n",
      "Training Epoch: 50 [24750/36045]\tLoss: 546.7183\n",
      "Training Epoch: 50 [24800/36045]\tLoss: 446.4813\n",
      "Training Epoch: 50 [24850/36045]\tLoss: 464.0080\n",
      "Training Epoch: 50 [24900/36045]\tLoss: 461.6239\n",
      "Training Epoch: 50 [24950/36045]\tLoss: 464.1758\n",
      "Training Epoch: 50 [25000/36045]\tLoss: 446.6187\n",
      "Training Epoch: 50 [25050/36045]\tLoss: 427.0457\n",
      "Training Epoch: 50 [25100/36045]\tLoss: 382.3950\n",
      "Training Epoch: 50 [25150/36045]\tLoss: 353.8864\n",
      "Training Epoch: 50 [25200/36045]\tLoss: 348.8848\n",
      "Training Epoch: 50 [25250/36045]\tLoss: 374.3428\n",
      "Training Epoch: 50 [25300/36045]\tLoss: 492.1456\n",
      "Training Epoch: 50 [25350/36045]\tLoss: 488.2599\n",
      "Training Epoch: 50 [25400/36045]\tLoss: 456.5053\n",
      "Training Epoch: 50 [25450/36045]\tLoss: 458.2833\n",
      "Training Epoch: 50 [25500/36045]\tLoss: 497.7893\n",
      "Training Epoch: 50 [25550/36045]\tLoss: 582.1919\n",
      "Training Epoch: 50 [25600/36045]\tLoss: 586.7108\n",
      "Training Epoch: 50 [25650/36045]\tLoss: 566.4536\n",
      "Training Epoch: 50 [25700/36045]\tLoss: 575.9687\n",
      "Training Epoch: 50 [25750/36045]\tLoss: 556.5694\n",
      "Training Epoch: 50 [25800/36045]\tLoss: 350.6934\n",
      "Training Epoch: 50 [25850/36045]\tLoss: 358.3308\n",
      "Training Epoch: 50 [25900/36045]\tLoss: 340.8346\n",
      "Training Epoch: 50 [25950/36045]\tLoss: 350.9473\n",
      "Training Epoch: 50 [26000/36045]\tLoss: 431.9465\n",
      "Training Epoch: 50 [26050/36045]\tLoss: 586.5072\n",
      "Training Epoch: 50 [26100/36045]\tLoss: 611.4714\n",
      "Training Epoch: 50 [26150/36045]\tLoss: 610.9133\n",
      "Training Epoch: 50 [26200/36045]\tLoss: 585.5468\n",
      "Training Epoch: 50 [26250/36045]\tLoss: 615.9099\n",
      "Training Epoch: 50 [26300/36045]\tLoss: 567.9890\n",
      "Training Epoch: 50 [26350/36045]\tLoss: 577.8878\n",
      "Training Epoch: 50 [26400/36045]\tLoss: 552.6339\n",
      "Training Epoch: 50 [26450/36045]\tLoss: 484.1961\n",
      "Training Epoch: 50 [26500/36045]\tLoss: 573.4287\n",
      "Training Epoch: 50 [26550/36045]\tLoss: 571.9714\n",
      "Training Epoch: 50 [26600/36045]\tLoss: 567.3936\n",
      "Training Epoch: 50 [26650/36045]\tLoss: 581.6923\n",
      "Training Epoch: 50 [26700/36045]\tLoss: 561.6328\n",
      "Training Epoch: 50 [26750/36045]\tLoss: 527.6981\n",
      "Training Epoch: 50 [26800/36045]\tLoss: 390.0385\n",
      "Training Epoch: 50 [26850/36045]\tLoss: 322.0555\n",
      "Training Epoch: 50 [26900/36045]\tLoss: 323.4384\n",
      "Training Epoch: 50 [26950/36045]\tLoss: 355.0164\n",
      "Training Epoch: 50 [27000/36045]\tLoss: 588.5497\n",
      "Training Epoch: 50 [27050/36045]\tLoss: 614.2499\n",
      "Training Epoch: 50 [27100/36045]\tLoss: 594.9133\n",
      "Training Epoch: 50 [27150/36045]\tLoss: 632.8924\n",
      "Training Epoch: 50 [27200/36045]\tLoss: 457.8599\n",
      "Training Epoch: 50 [27250/36045]\tLoss: 448.3153\n",
      "Training Epoch: 50 [27300/36045]\tLoss: 437.4400\n",
      "Training Epoch: 50 [27350/36045]\tLoss: 434.0475\n",
      "Training Epoch: 50 [27400/36045]\tLoss: 433.4016\n",
      "Training Epoch: 50 [27450/36045]\tLoss: 549.8359\n",
      "Training Epoch: 50 [27500/36045]\tLoss: 589.2664\n",
      "Training Epoch: 50 [27550/36045]\tLoss: 582.4235\n",
      "Training Epoch: 50 [27600/36045]\tLoss: 593.6716\n",
      "Training Epoch: 50 [27650/36045]\tLoss: 585.6880\n",
      "Training Epoch: 50 [27700/36045]\tLoss: 612.4775\n",
      "Training Epoch: 50 [27750/36045]\tLoss: 624.1804\n",
      "Training Epoch: 50 [27800/36045]\tLoss: 611.6871\n",
      "Training Epoch: 50 [27850/36045]\tLoss: 602.0989\n",
      "Training Epoch: 50 [27900/36045]\tLoss: 548.7856\n",
      "Training Epoch: 50 [27950/36045]\tLoss: 460.1874\n",
      "Training Epoch: 50 [28000/36045]\tLoss: 437.1809\n",
      "Training Epoch: 50 [28050/36045]\tLoss: 446.5232\n",
      "Training Epoch: 50 [28100/36045]\tLoss: 438.2195\n",
      "Training Epoch: 50 [28150/36045]\tLoss: 455.8661\n",
      "Training Epoch: 50 [28200/36045]\tLoss: 464.5672\n",
      "Training Epoch: 50 [28250/36045]\tLoss: 457.2653\n",
      "Training Epoch: 50 [28300/36045]\tLoss: 434.1770\n",
      "Training Epoch: 50 [28350/36045]\tLoss: 429.9237\n",
      "Training Epoch: 50 [28400/36045]\tLoss: 756.2217\n",
      "Training Epoch: 50 [28450/36045]\tLoss: 694.7987\n",
      "Training Epoch: 50 [28500/36045]\tLoss: 600.3843\n",
      "Training Epoch: 50 [28550/36045]\tLoss: 552.4483\n",
      "Training Epoch: 50 [28600/36045]\tLoss: 574.9509\n",
      "Training Epoch: 50 [28650/36045]\tLoss: 624.8483\n",
      "Training Epoch: 50 [28700/36045]\tLoss: 618.2891\n",
      "Training Epoch: 50 [28750/36045]\tLoss: 605.6005\n",
      "Training Epoch: 50 [28800/36045]\tLoss: 616.1411\n",
      "Training Epoch: 50 [28850/36045]\tLoss: 535.5821\n",
      "Training Epoch: 50 [28900/36045]\tLoss: 436.2278\n",
      "Training Epoch: 50 [28950/36045]\tLoss: 435.2957\n",
      "Training Epoch: 50 [29000/36045]\tLoss: 430.9878\n",
      "Training Epoch: 50 [29050/36045]\tLoss: 438.2251\n",
      "Training Epoch: 50 [29100/36045]\tLoss: 455.9048\n",
      "Training Epoch: 50 [29150/36045]\tLoss: 446.7853\n",
      "Training Epoch: 50 [29200/36045]\tLoss: 432.4434\n",
      "Training Epoch: 50 [29250/36045]\tLoss: 422.3462\n",
      "Training Epoch: 50 [29300/36045]\tLoss: 473.4379\n",
      "Training Epoch: 50 [29350/36045]\tLoss: 554.9444\n",
      "Training Epoch: 50 [29400/36045]\tLoss: 572.5898\n",
      "Training Epoch: 50 [29450/36045]\tLoss: 588.7205\n",
      "Training Epoch: 50 [29500/36045]\tLoss: 603.3487\n",
      "Training Epoch: 50 [29550/36045]\tLoss: 573.5430\n",
      "Training Epoch: 50 [29600/36045]\tLoss: 481.9582\n",
      "Training Epoch: 50 [29650/36045]\tLoss: 463.6011\n",
      "Training Epoch: 50 [29700/36045]\tLoss: 417.3253\n",
      "Training Epoch: 50 [29750/36045]\tLoss: 415.4979\n",
      "Training Epoch: 50 [29800/36045]\tLoss: 462.1895\n",
      "Training Epoch: 50 [29850/36045]\tLoss: 540.0177\n",
      "Training Epoch: 50 [29900/36045]\tLoss: 535.3260\n",
      "Training Epoch: 50 [29950/36045]\tLoss: 557.3148\n",
      "Training Epoch: 50 [30000/36045]\tLoss: 531.1312\n",
      "Training Epoch: 50 [30050/36045]\tLoss: 538.5250\n",
      "Training Epoch: 50 [30100/36045]\tLoss: 656.5215\n",
      "Training Epoch: 50 [30150/36045]\tLoss: 638.4177\n",
      "Training Epoch: 50 [30200/36045]\tLoss: 602.0617\n",
      "Training Epoch: 50 [30250/36045]\tLoss: 650.6296\n",
      "Training Epoch: 50 [30300/36045]\tLoss: 634.6368\n",
      "Training Epoch: 50 [30350/36045]\tLoss: 481.3873\n",
      "Training Epoch: 50 [30400/36045]\tLoss: 466.1956\n",
      "Training Epoch: 50 [30450/36045]\tLoss: 468.6438\n",
      "Training Epoch: 50 [30500/36045]\tLoss: 437.4610\n",
      "Training Epoch: 50 [30550/36045]\tLoss: 406.1273\n",
      "Training Epoch: 50 [30600/36045]\tLoss: 399.4470\n",
      "Training Epoch: 50 [30650/36045]\tLoss: 389.6392\n",
      "Training Epoch: 50 [30700/36045]\tLoss: 407.1654\n",
      "Training Epoch: 50 [30750/36045]\tLoss: 394.2159\n",
      "Training Epoch: 50 [30800/36045]\tLoss: 420.3954\n",
      "Training Epoch: 50 [30850/36045]\tLoss: 412.2281\n",
      "Training Epoch: 50 [30900/36045]\tLoss: 423.7320\n",
      "Training Epoch: 50 [30950/36045]\tLoss: 445.3923\n",
      "Training Epoch: 50 [31000/36045]\tLoss: 437.9480\n",
      "Training Epoch: 50 [31050/36045]\tLoss: 365.4540\n",
      "Training Epoch: 50 [31100/36045]\tLoss: 356.2821\n",
      "Training Epoch: 50 [31150/36045]\tLoss: 363.9948\n",
      "Training Epoch: 50 [31200/36045]\tLoss: 451.6149\n",
      "Training Epoch: 50 [31250/36045]\tLoss: 585.9395\n",
      "Training Epoch: 50 [31300/36045]\tLoss: 557.3422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 50 [31350/36045]\tLoss: 573.5581\n",
      "Training Epoch: 50 [31400/36045]\tLoss: 553.1901\n",
      "Training Epoch: 50 [31450/36045]\tLoss: 569.9277\n",
      "Training Epoch: 50 [31500/36045]\tLoss: 582.2579\n",
      "Training Epoch: 50 [31550/36045]\tLoss: 588.7527\n",
      "Training Epoch: 50 [31600/36045]\tLoss: 553.5458\n",
      "Training Epoch: 50 [31650/36045]\tLoss: 593.0289\n",
      "Training Epoch: 50 [31700/36045]\tLoss: 428.6142\n",
      "Training Epoch: 50 [31750/36045]\tLoss: 353.8681\n",
      "Training Epoch: 50 [31800/36045]\tLoss: 337.8045\n",
      "Training Epoch: 50 [31850/36045]\tLoss: 345.3174\n",
      "Training Epoch: 50 [31900/36045]\tLoss: 548.4590\n",
      "Training Epoch: 50 [31950/36045]\tLoss: 711.9014\n",
      "Training Epoch: 50 [32000/36045]\tLoss: 818.4033\n",
      "Training Epoch: 50 [32050/36045]\tLoss: 774.5621\n",
      "Training Epoch: 50 [32100/36045]\tLoss: 766.3145\n",
      "Training Epoch: 50 [32150/36045]\tLoss: 584.7742\n",
      "Training Epoch: 50 [32200/36045]\tLoss: 586.2473\n",
      "Training Epoch: 50 [32250/36045]\tLoss: 596.2730\n",
      "Training Epoch: 50 [32300/36045]\tLoss: 578.5734\n",
      "Training Epoch: 50 [32350/36045]\tLoss: 574.8013\n",
      "Training Epoch: 50 [32400/36045]\tLoss: 539.4081\n",
      "Training Epoch: 50 [32450/36045]\tLoss: 444.2699\n",
      "Training Epoch: 50 [32500/36045]\tLoss: 426.7832\n",
      "Training Epoch: 50 [32550/36045]\tLoss: 428.7922\n",
      "Training Epoch: 50 [32600/36045]\tLoss: 425.9982\n",
      "Training Epoch: 50 [32650/36045]\tLoss: 554.1907\n",
      "Training Epoch: 50 [32700/36045]\tLoss: 605.4953\n",
      "Training Epoch: 50 [32750/36045]\tLoss: 576.3608\n",
      "Training Epoch: 50 [32800/36045]\tLoss: 590.8041\n",
      "Training Epoch: 50 [32850/36045]\tLoss: 545.4002\n",
      "Training Epoch: 50 [32900/36045]\tLoss: 435.1017\n",
      "Training Epoch: 50 [32950/36045]\tLoss: 455.8575\n",
      "Training Epoch: 50 [33000/36045]\tLoss: 454.2467\n",
      "Training Epoch: 50 [33050/36045]\tLoss: 432.5201\n",
      "Training Epoch: 50 [33100/36045]\tLoss: 491.5321\n",
      "Training Epoch: 50 [33150/36045]\tLoss: 669.8819\n",
      "Training Epoch: 50 [33200/36045]\tLoss: 652.0703\n",
      "Training Epoch: 50 [33250/36045]\tLoss: 671.8394\n",
      "Training Epoch: 50 [33300/36045]\tLoss: 716.4124\n",
      "Training Epoch: 50 [33350/36045]\tLoss: 548.0280\n",
      "Training Epoch: 50 [33400/36045]\tLoss: 398.0034\n",
      "Training Epoch: 50 [33450/36045]\tLoss: 394.1952\n",
      "Training Epoch: 50 [33500/36045]\tLoss: 405.6700\n",
      "Training Epoch: 50 [33550/36045]\tLoss: 419.7820\n",
      "Training Epoch: 50 [33600/36045]\tLoss: 421.6066\n",
      "Training Epoch: 50 [33650/36045]\tLoss: 563.7870\n",
      "Training Epoch: 50 [33700/36045]\tLoss: 545.3550\n",
      "Training Epoch: 50 [33750/36045]\tLoss: 564.7031\n",
      "Training Epoch: 50 [33800/36045]\tLoss: 562.3851\n",
      "Training Epoch: 50 [33850/36045]\tLoss: 563.9536\n",
      "Training Epoch: 50 [33900/36045]\tLoss: 577.6388\n",
      "Training Epoch: 50 [33950/36045]\tLoss: 586.9296\n",
      "Training Epoch: 50 [34000/36045]\tLoss: 573.7737\n",
      "Training Epoch: 50 [34050/36045]\tLoss: 577.3581\n",
      "Training Epoch: 50 [34100/36045]\tLoss: 556.8786\n",
      "Training Epoch: 50 [34150/36045]\tLoss: 515.3436\n",
      "Training Epoch: 50 [34200/36045]\tLoss: 488.1775\n",
      "Training Epoch: 50 [34250/36045]\tLoss: 501.8241\n",
      "Training Epoch: 50 [34300/36045]\tLoss: 428.0683\n",
      "Training Epoch: 50 [34350/36045]\tLoss: 450.9924\n",
      "Training Epoch: 50 [34400/36045]\tLoss: 444.5507\n",
      "Training Epoch: 50 [34450/36045]\tLoss: 418.7426\n",
      "Training Epoch: 50 [34500/36045]\tLoss: 446.5457\n",
      "Training Epoch: 50 [34550/36045]\tLoss: 438.4075\n",
      "Training Epoch: 50 [34600/36045]\tLoss: 446.1056\n",
      "Training Epoch: 50 [34650/36045]\tLoss: 551.5302\n",
      "Training Epoch: 50 [34700/36045]\tLoss: 585.2261\n",
      "Training Epoch: 50 [34750/36045]\tLoss: 518.5215\n",
      "Training Epoch: 50 [34800/36045]\tLoss: 596.0079\n",
      "Training Epoch: 50 [34850/36045]\tLoss: 602.7620\n",
      "Training Epoch: 50 [34900/36045]\tLoss: 646.6942\n",
      "Training Epoch: 50 [34950/36045]\tLoss: 631.1042\n",
      "Training Epoch: 50 [35000/36045]\tLoss: 632.4154\n",
      "Training Epoch: 50 [35050/36045]\tLoss: 620.0713\n",
      "Training Epoch: 50 [35100/36045]\tLoss: 538.2318\n",
      "Training Epoch: 50 [35150/36045]\tLoss: 529.9133\n",
      "Training Epoch: 50 [35200/36045]\tLoss: 444.7218\n",
      "Training Epoch: 50 [35250/36045]\tLoss: 489.2179\n",
      "Training Epoch: 50 [35300/36045]\tLoss: 506.0248\n",
      "Training Epoch: 50 [35350/36045]\tLoss: 563.2374\n",
      "Training Epoch: 50 [35400/36045]\tLoss: 591.2148\n",
      "Training Epoch: 50 [35450/36045]\tLoss: 562.9312\n",
      "Training Epoch: 50 [35500/36045]\tLoss: 544.5422\n",
      "Training Epoch: 50 [35550/36045]\tLoss: 531.5910\n",
      "Training Epoch: 50 [35600/36045]\tLoss: 582.9301\n",
      "Training Epoch: 50 [35650/36045]\tLoss: 654.4542\n",
      "Training Epoch: 50 [35700/36045]\tLoss: 581.0578\n",
      "Training Epoch: 50 [35750/36045]\tLoss: 638.5963\n",
      "Training Epoch: 50 [35800/36045]\tLoss: 645.6515\n",
      "Training Epoch: 50 [35850/36045]\tLoss: 619.6405\n",
      "Training Epoch: 50 [35900/36045]\tLoss: 640.0336\n",
      "Training Epoch: 50 [35950/36045]\tLoss: 636.2938\n",
      "Training Epoch: 50 [36000/36045]\tLoss: 630.1746\n",
      "Training Epoch: 50 [36045/36045]\tLoss: 616.5823\n",
      "Training Epoch: 50 [4004/4004]\tLoss: 572.2852\n",
      "Training Epoch: 51 [50/36045]\tLoss: 570.3541\n",
      "Training Epoch: 51 [100/36045]\tLoss: 545.7098\n",
      "Training Epoch: 51 [150/36045]\tLoss: 543.2535\n",
      "Training Epoch: 51 [200/36045]\tLoss: 529.8410\n",
      "Training Epoch: 51 [250/36045]\tLoss: 638.9310\n",
      "Training Epoch: 51 [300/36045]\tLoss: 704.5455\n",
      "Training Epoch: 51 [350/36045]\tLoss: 671.3352\n",
      "Training Epoch: 51 [400/36045]\tLoss: 664.9513\n",
      "Training Epoch: 51 [450/36045]\tLoss: 645.8560\n",
      "Training Epoch: 51 [500/36045]\tLoss: 597.0817\n",
      "Training Epoch: 51 [550/36045]\tLoss: 600.3537\n",
      "Training Epoch: 51 [600/36045]\tLoss: 587.1059\n",
      "Training Epoch: 51 [650/36045]\tLoss: 607.4218\n",
      "Training Epoch: 51 [700/36045]\tLoss: 592.0268\n",
      "Training Epoch: 51 [750/36045]\tLoss: 567.2350\n",
      "Training Epoch: 51 [800/36045]\tLoss: 578.9725\n",
      "Training Epoch: 51 [850/36045]\tLoss: 563.0864\n",
      "Training Epoch: 51 [900/36045]\tLoss: 539.4290\n",
      "Training Epoch: 51 [950/36045]\tLoss: 509.5031\n",
      "Training Epoch: 51 [1000/36045]\tLoss: 494.2072\n",
      "Training Epoch: 51 [1050/36045]\tLoss: 496.5609\n",
      "Training Epoch: 51 [1100/36045]\tLoss: 483.5782\n",
      "Training Epoch: 51 [1150/36045]\tLoss: 493.1796\n",
      "Training Epoch: 51 [1200/36045]\tLoss: 522.2345\n",
      "Training Epoch: 51 [1250/36045]\tLoss: 596.9225\n",
      "Training Epoch: 51 [1300/36045]\tLoss: 603.9468\n",
      "Training Epoch: 51 [1350/36045]\tLoss: 605.3292\n",
      "Training Epoch: 51 [1400/36045]\tLoss: 628.6887\n",
      "Training Epoch: 51 [1450/36045]\tLoss: 608.5498\n",
      "Training Epoch: 51 [1500/36045]\tLoss: 555.4080\n",
      "Training Epoch: 51 [1550/36045]\tLoss: 569.4221\n",
      "Training Epoch: 51 [1600/36045]\tLoss: 579.1002\n",
      "Training Epoch: 51 [1650/36045]\tLoss: 566.9039\n",
      "Training Epoch: 51 [1700/36045]\tLoss: 579.3157\n",
      "Training Epoch: 51 [1750/36045]\tLoss: 621.0110\n",
      "Training Epoch: 51 [1800/36045]\tLoss: 604.0399\n",
      "Training Epoch: 51 [1850/36045]\tLoss: 619.2704\n",
      "Training Epoch: 51 [1900/36045]\tLoss: 578.8507\n",
      "Training Epoch: 51 [1950/36045]\tLoss: 588.4316\n",
      "Training Epoch: 51 [2000/36045]\tLoss: 529.3763\n",
      "Training Epoch: 51 [2050/36045]\tLoss: 532.2161\n",
      "Training Epoch: 51 [2100/36045]\tLoss: 560.9343\n",
      "Training Epoch: 51 [2150/36045]\tLoss: 542.0104\n",
      "Training Epoch: 51 [2200/36045]\tLoss: 505.3451\n",
      "Training Epoch: 51 [2250/36045]\tLoss: 477.8920\n",
      "Training Epoch: 51 [2300/36045]\tLoss: 501.2319\n",
      "Training Epoch: 51 [2350/36045]\tLoss: 479.2618\n",
      "Training Epoch: 51 [2400/36045]\tLoss: 486.4947\n",
      "Training Epoch: 51 [2450/36045]\tLoss: 623.5272\n",
      "Training Epoch: 51 [2500/36045]\tLoss: 654.6703\n",
      "Training Epoch: 51 [2550/36045]\tLoss: 652.7798\n",
      "Training Epoch: 51 [2600/36045]\tLoss: 661.6290\n",
      "Training Epoch: 51 [2650/36045]\tLoss: 785.5435\n",
      "Training Epoch: 51 [2700/36045]\tLoss: 873.5842\n",
      "Training Epoch: 51 [2750/36045]\tLoss: 944.0806\n",
      "Training Epoch: 51 [2800/36045]\tLoss: 953.2085\n",
      "Training Epoch: 51 [2850/36045]\tLoss: 719.9918\n",
      "Training Epoch: 51 [2900/36045]\tLoss: 680.3980\n",
      "Training Epoch: 51 [2950/36045]\tLoss: 658.4917\n",
      "Training Epoch: 51 [3000/36045]\tLoss: 652.1357\n",
      "Training Epoch: 51 [3050/36045]\tLoss: 682.6459\n",
      "Training Epoch: 51 [3100/36045]\tLoss: 623.4136\n",
      "Training Epoch: 51 [3150/36045]\tLoss: 478.9782\n",
      "Training Epoch: 51 [3200/36045]\tLoss: 495.8920\n",
      "Training Epoch: 51 [3250/36045]\tLoss: 468.3521\n",
      "Training Epoch: 51 [3300/36045]\tLoss: 443.5276\n",
      "Training Epoch: 51 [3350/36045]\tLoss: 468.4534\n",
      "Training Epoch: 51 [3400/36045]\tLoss: 490.6152\n",
      "Training Epoch: 51 [3450/36045]\tLoss: 526.0024\n",
      "Training Epoch: 51 [3500/36045]\tLoss: 513.5640\n",
      "Training Epoch: 51 [3550/36045]\tLoss: 491.0232\n",
      "Training Epoch: 51 [3600/36045]\tLoss: 527.9958\n",
      "Training Epoch: 51 [3650/36045]\tLoss: 610.4915\n",
      "Training Epoch: 51 [3700/36045]\tLoss: 618.2372\n",
      "Training Epoch: 51 [3750/36045]\tLoss: 587.5793\n",
      "Training Epoch: 51 [3800/36045]\tLoss: 585.3009\n",
      "Training Epoch: 51 [3850/36045]\tLoss: 587.8591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 51 [3900/36045]\tLoss: 592.2485\n",
      "Training Epoch: 51 [3950/36045]\tLoss: 571.6347\n",
      "Training Epoch: 51 [4000/36045]\tLoss: 575.9997\n",
      "Training Epoch: 51 [4050/36045]\tLoss: 527.8762\n",
      "Training Epoch: 51 [4100/36045]\tLoss: 514.9402\n",
      "Training Epoch: 51 [4150/36045]\tLoss: 528.5601\n",
      "Training Epoch: 51 [4200/36045]\tLoss: 523.5159\n",
      "Training Epoch: 51 [4250/36045]\tLoss: 525.4632\n",
      "Training Epoch: 51 [4300/36045]\tLoss: 539.9056\n",
      "Training Epoch: 51 [4350/36045]\tLoss: 524.2161\n",
      "Training Epoch: 51 [4400/36045]\tLoss: 501.2653\n",
      "Training Epoch: 51 [4450/36045]\tLoss: 551.1741\n",
      "Training Epoch: 51 [4500/36045]\tLoss: 593.8505\n",
      "Training Epoch: 51 [4550/36045]\tLoss: 596.3488\n",
      "Training Epoch: 51 [4600/36045]\tLoss: 617.9800\n",
      "Training Epoch: 51 [4650/36045]\tLoss: 608.3088\n",
      "Training Epoch: 51 [4700/36045]\tLoss: 560.5942\n",
      "Training Epoch: 51 [4750/36045]\tLoss: 542.2637\n",
      "Training Epoch: 51 [4800/36045]\tLoss: 566.0109\n",
      "Training Epoch: 51 [4850/36045]\tLoss: 552.7279\n",
      "Training Epoch: 51 [4900/36045]\tLoss: 538.2655\n",
      "Training Epoch: 51 [4950/36045]\tLoss: 553.6079\n",
      "Training Epoch: 51 [5000/36045]\tLoss: 582.7415\n",
      "Training Epoch: 51 [5050/36045]\tLoss: 564.6155\n",
      "Training Epoch: 51 [5100/36045]\tLoss: 574.6301\n",
      "Training Epoch: 51 [5150/36045]\tLoss: 559.2205\n",
      "Training Epoch: 51 [5200/36045]\tLoss: 557.1534\n",
      "Training Epoch: 51 [5250/36045]\tLoss: 550.7354\n",
      "Training Epoch: 51 [5300/36045]\tLoss: 550.6352\n",
      "Training Epoch: 51 [5350/36045]\tLoss: 571.2515\n",
      "Training Epoch: 51 [5400/36045]\tLoss: 550.8071\n",
      "Training Epoch: 51 [5450/36045]\tLoss: 521.7450\n",
      "Training Epoch: 51 [5500/36045]\tLoss: 549.7570\n",
      "Training Epoch: 51 [5550/36045]\tLoss: 538.0238\n",
      "Training Epoch: 51 [5600/36045]\tLoss: 615.5275\n",
      "Training Epoch: 51 [5650/36045]\tLoss: 581.8603\n",
      "Training Epoch: 51 [5700/36045]\tLoss: 546.0568\n",
      "Training Epoch: 51 [5750/36045]\tLoss: 530.1229\n",
      "Training Epoch: 51 [5800/36045]\tLoss: 559.2628\n",
      "Training Epoch: 51 [5850/36045]\tLoss: 549.2954\n",
      "Training Epoch: 51 [5900/36045]\tLoss: 630.8997\n",
      "Training Epoch: 51 [5950/36045]\tLoss: 647.1226\n",
      "Training Epoch: 51 [6000/36045]\tLoss: 633.3623\n",
      "Training Epoch: 51 [6050/36045]\tLoss: 611.7487\n",
      "Training Epoch: 51 [6100/36045]\tLoss: 616.0477\n",
      "Training Epoch: 51 [6150/36045]\tLoss: 607.8552\n",
      "Training Epoch: 51 [6200/36045]\tLoss: 613.1595\n",
      "Training Epoch: 51 [6250/36045]\tLoss: 634.5804\n",
      "Training Epoch: 51 [6300/36045]\tLoss: 646.0937\n",
      "Training Epoch: 51 [6350/36045]\tLoss: 690.9445\n",
      "Training Epoch: 51 [6400/36045]\tLoss: 568.0504\n",
      "Training Epoch: 51 [6450/36045]\tLoss: 521.6660\n",
      "Training Epoch: 51 [6500/36045]\tLoss: 531.0577\n",
      "Training Epoch: 51 [6550/36045]\tLoss: 548.4791\n",
      "Training Epoch: 51 [6600/36045]\tLoss: 546.2296\n",
      "Training Epoch: 51 [6650/36045]\tLoss: 617.2142\n",
      "Training Epoch: 51 [6700/36045]\tLoss: 645.3494\n",
      "Training Epoch: 51 [6750/36045]\tLoss: 623.2043\n",
      "Training Epoch: 51 [6800/36045]\tLoss: 626.1700\n",
      "Training Epoch: 51 [6850/36045]\tLoss: 614.3201\n",
      "Training Epoch: 51 [6900/36045]\tLoss: 547.1843\n",
      "Training Epoch: 51 [6950/36045]\tLoss: 515.9402\n",
      "Training Epoch: 51 [7000/36045]\tLoss: 548.6907\n",
      "Training Epoch: 51 [7050/36045]\tLoss: 560.6757\n",
      "Training Epoch: 51 [7100/36045]\tLoss: 559.5079\n",
      "Training Epoch: 51 [7150/36045]\tLoss: 569.2889\n",
      "Training Epoch: 51 [7200/36045]\tLoss: 571.1318\n",
      "Training Epoch: 51 [7250/36045]\tLoss: 569.6215\n",
      "Training Epoch: 51 [7300/36045]\tLoss: 555.9243\n",
      "Training Epoch: 51 [7350/36045]\tLoss: 553.8522\n",
      "Training Epoch: 51 [7400/36045]\tLoss: 505.5989\n",
      "Training Epoch: 51 [7450/36045]\tLoss: 508.8955\n",
      "Training Epoch: 51 [7500/36045]\tLoss: 504.6786\n",
      "Training Epoch: 51 [7550/36045]\tLoss: 483.3339\n",
      "Training Epoch: 51 [7600/36045]\tLoss: 535.3861\n",
      "Training Epoch: 51 [7650/36045]\tLoss: 571.9863\n",
      "Training Epoch: 51 [7700/36045]\tLoss: 544.3309\n",
      "Training Epoch: 51 [7750/36045]\tLoss: 558.5706\n",
      "Training Epoch: 51 [7800/36045]\tLoss: 548.3556\n",
      "Training Epoch: 51 [7850/36045]\tLoss: 530.6735\n",
      "Training Epoch: 51 [7900/36045]\tLoss: 559.5335\n",
      "Training Epoch: 51 [7950/36045]\tLoss: 557.2099\n",
      "Training Epoch: 51 [8000/36045]\tLoss: 575.2864\n",
      "Training Epoch: 51 [8050/36045]\tLoss: 541.4352\n",
      "Training Epoch: 51 [8100/36045]\tLoss: 566.3378\n",
      "Training Epoch: 51 [8150/36045]\tLoss: 642.9855\n",
      "Training Epoch: 51 [8200/36045]\tLoss: 630.6884\n",
      "Training Epoch: 51 [8250/36045]\tLoss: 599.3106\n",
      "Training Epoch: 51 [8300/36045]\tLoss: 655.5111\n",
      "Training Epoch: 51 [8350/36045]\tLoss: 600.4884\n",
      "Training Epoch: 51 [8400/36045]\tLoss: 536.7264\n",
      "Training Epoch: 51 [8450/36045]\tLoss: 502.2018\n",
      "Training Epoch: 51 [8500/36045]\tLoss: 534.6783\n",
      "Training Epoch: 51 [8550/36045]\tLoss: 528.6324\n",
      "Training Epoch: 51 [8600/36045]\tLoss: 523.0293\n",
      "Training Epoch: 51 [8650/36045]\tLoss: 554.1309\n",
      "Training Epoch: 51 [8700/36045]\tLoss: 586.1064\n",
      "Training Epoch: 51 [8750/36045]\tLoss: 575.8834\n",
      "Training Epoch: 51 [8800/36045]\tLoss: 581.5560\n",
      "Training Epoch: 51 [8850/36045]\tLoss: 575.2629\n",
      "Training Epoch: 51 [8900/36045]\tLoss: 519.3234\n",
      "Training Epoch: 51 [8950/36045]\tLoss: 529.7117\n",
      "Training Epoch: 51 [9000/36045]\tLoss: 545.6459\n",
      "Training Epoch: 51 [9050/36045]\tLoss: 547.2488\n",
      "Training Epoch: 51 [9100/36045]\tLoss: 563.8159\n",
      "Training Epoch: 51 [9150/36045]\tLoss: 417.3585\n",
      "Training Epoch: 51 [9200/36045]\tLoss: 311.6230\n",
      "Training Epoch: 51 [9250/36045]\tLoss: 338.2296\n",
      "Training Epoch: 51 [9300/36045]\tLoss: 347.6906\n",
      "Training Epoch: 51 [9350/36045]\tLoss: 321.0165\n",
      "Training Epoch: 51 [9400/36045]\tLoss: 628.3363\n",
      "Training Epoch: 51 [9450/36045]\tLoss: 667.2461\n",
      "Training Epoch: 51 [9500/36045]\tLoss: 655.0844\n",
      "Training Epoch: 51 [9550/36045]\tLoss: 693.0386\n",
      "Training Epoch: 51 [9600/36045]\tLoss: 517.0552\n",
      "Training Epoch: 51 [9650/36045]\tLoss: 522.2113\n",
      "Training Epoch: 51 [9700/36045]\tLoss: 507.8709\n",
      "Training Epoch: 51 [9750/36045]\tLoss: 506.4891\n",
      "Training Epoch: 51 [9800/36045]\tLoss: 662.3578\n",
      "Training Epoch: 51 [9850/36045]\tLoss: 699.2792\n",
      "Training Epoch: 51 [9900/36045]\tLoss: 707.8614\n",
      "Training Epoch: 51 [9950/36045]\tLoss: 689.9068\n",
      "Training Epoch: 51 [10000/36045]\tLoss: 638.9637\n",
      "Training Epoch: 51 [10050/36045]\tLoss: 522.7129\n",
      "Training Epoch: 51 [10100/36045]\tLoss: 530.1906\n",
      "Training Epoch: 51 [10150/36045]\tLoss: 537.9735\n",
      "Training Epoch: 51 [10200/36045]\tLoss: 527.1676\n",
      "Training Epoch: 51 [10250/36045]\tLoss: 634.8771\n",
      "Training Epoch: 51 [10300/36045]\tLoss: 617.0141\n",
      "Training Epoch: 51 [10350/36045]\tLoss: 649.8132\n",
      "Training Epoch: 51 [10400/36045]\tLoss: 639.7536\n",
      "Training Epoch: 51 [10450/36045]\tLoss: 599.6201\n",
      "Training Epoch: 51 [10500/36045]\tLoss: 500.8228\n",
      "Training Epoch: 51 [10550/36045]\tLoss: 495.1824\n",
      "Training Epoch: 51 [10600/36045]\tLoss: 516.6946\n",
      "Training Epoch: 51 [10650/36045]\tLoss: 522.4911\n",
      "Training Epoch: 51 [10700/36045]\tLoss: 601.1542\n",
      "Training Epoch: 51 [10750/36045]\tLoss: 659.6730\n",
      "Training Epoch: 51 [10800/36045]\tLoss: 605.7738\n",
      "Training Epoch: 51 [10850/36045]\tLoss: 642.6194\n",
      "Training Epoch: 51 [10900/36045]\tLoss: 669.0857\n",
      "Training Epoch: 51 [10950/36045]\tLoss: 491.7199\n",
      "Training Epoch: 51 [11000/36045]\tLoss: 485.5613\n",
      "Training Epoch: 51 [11050/36045]\tLoss: 520.8325\n",
      "Training Epoch: 51 [11100/36045]\tLoss: 530.7205\n",
      "Training Epoch: 51 [11150/36045]\tLoss: 575.8795\n",
      "Training Epoch: 51 [11200/36045]\tLoss: 604.6387\n",
      "Training Epoch: 51 [11250/36045]\tLoss: 615.7305\n",
      "Training Epoch: 51 [11300/36045]\tLoss: 596.1849\n",
      "Training Epoch: 51 [11350/36045]\tLoss: 593.9677\n",
      "Training Epoch: 51 [11400/36045]\tLoss: 557.8248\n",
      "Training Epoch: 51 [11450/36045]\tLoss: 527.3480\n",
      "Training Epoch: 51 [11500/36045]\tLoss: 524.9316\n",
      "Training Epoch: 51 [11550/36045]\tLoss: 534.1627\n",
      "Training Epoch: 51 [11600/36045]\tLoss: 593.8226\n",
      "Training Epoch: 51 [11650/36045]\tLoss: 644.8192\n",
      "Training Epoch: 51 [11700/36045]\tLoss: 643.7483\n",
      "Training Epoch: 51 [11750/36045]\tLoss: 661.5431\n",
      "Training Epoch: 51 [11800/36045]\tLoss: 703.9745\n",
      "Training Epoch: 51 [11850/36045]\tLoss: 762.5947\n",
      "Training Epoch: 51 [11900/36045]\tLoss: 973.9005\n",
      "Training Epoch: 51 [11950/36045]\tLoss: 977.1846\n",
      "Training Epoch: 51 [12000/36045]\tLoss: 988.0277\n",
      "Training Epoch: 51 [12050/36045]\tLoss: 947.9632\n",
      "Training Epoch: 51 [12100/36045]\tLoss: 603.9611\n",
      "Training Epoch: 51 [12150/36045]\tLoss: 452.6333\n",
      "Training Epoch: 51 [12200/36045]\tLoss: 447.5073\n",
      "Training Epoch: 51 [12250/36045]\tLoss: 456.0270\n",
      "Training Epoch: 51 [12300/36045]\tLoss: 589.7920\n",
      "Training Epoch: 51 [12350/36045]\tLoss: 644.4571\n",
      "Training Epoch: 51 [12400/36045]\tLoss: 651.5172\n",
      "Training Epoch: 51 [12450/36045]\tLoss: 640.5433\n",
      "Training Epoch: 51 [12500/36045]\tLoss: 667.2258\n",
      "Training Epoch: 51 [12550/36045]\tLoss: 637.2672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 51 [12600/36045]\tLoss: 581.6586\n",
      "Training Epoch: 51 [12650/36045]\tLoss: 579.7493\n",
      "Training Epoch: 51 [12700/36045]\tLoss: 600.9220\n",
      "Training Epoch: 51 [12750/36045]\tLoss: 599.3755\n",
      "Training Epoch: 51 [12800/36045]\tLoss: 586.2758\n",
      "Training Epoch: 51 [12850/36045]\tLoss: 615.4813\n",
      "Training Epoch: 51 [12900/36045]\tLoss: 589.9200\n",
      "Training Epoch: 51 [12950/36045]\tLoss: 575.3975\n",
      "Training Epoch: 51 [13000/36045]\tLoss: 608.8175\n",
      "Training Epoch: 51 [13050/36045]\tLoss: 549.8221\n",
      "Training Epoch: 51 [13100/36045]\tLoss: 564.5422\n",
      "Training Epoch: 51 [13150/36045]\tLoss: 556.1801\n",
      "Training Epoch: 51 [13200/36045]\tLoss: 539.7542\n",
      "Training Epoch: 51 [13250/36045]\tLoss: 561.0142\n",
      "Training Epoch: 51 [13300/36045]\tLoss: 597.2285\n",
      "Training Epoch: 51 [13350/36045]\tLoss: 578.5654\n",
      "Training Epoch: 51 [13400/36045]\tLoss: 581.6093\n",
      "Training Epoch: 51 [13450/36045]\tLoss: 579.2615\n",
      "Training Epoch: 51 [13500/36045]\tLoss: 597.4150\n",
      "Training Epoch: 51 [13550/36045]\tLoss: 735.8044\n",
      "Training Epoch: 51 [13600/36045]\tLoss: 768.3405\n",
      "Training Epoch: 51 [13650/36045]\tLoss: 850.3934\n",
      "Training Epoch: 51 [13700/36045]\tLoss: 748.2043\n",
      "Training Epoch: 51 [13750/36045]\tLoss: 584.9897\n",
      "Training Epoch: 51 [13800/36045]\tLoss: 555.5342\n",
      "Training Epoch: 51 [13850/36045]\tLoss: 538.3928\n",
      "Training Epoch: 51 [13900/36045]\tLoss: 545.6274\n",
      "Training Epoch: 51 [13950/36045]\tLoss: 590.6794\n",
      "Training Epoch: 51 [14000/36045]\tLoss: 622.9961\n",
      "Training Epoch: 51 [14050/36045]\tLoss: 598.1669\n",
      "Training Epoch: 51 [14100/36045]\tLoss: 593.1954\n",
      "Training Epoch: 51 [14150/36045]\tLoss: 581.4438\n",
      "Training Epoch: 51 [14200/36045]\tLoss: 621.6282\n",
      "Training Epoch: 51 [14250/36045]\tLoss: 683.3023\n",
      "Training Epoch: 51 [14300/36045]\tLoss: 686.3182\n",
      "Training Epoch: 51 [14350/36045]\tLoss: 655.6915\n",
      "Training Epoch: 51 [14400/36045]\tLoss: 642.0150\n",
      "Training Epoch: 51 [14450/36045]\tLoss: 677.2552\n",
      "Training Epoch: 51 [14500/36045]\tLoss: 609.0845\n",
      "Training Epoch: 51 [14550/36045]\tLoss: 636.3588\n",
      "Training Epoch: 51 [14600/36045]\tLoss: 622.9325\n",
      "Training Epoch: 51 [14650/36045]\tLoss: 622.8271\n",
      "Training Epoch: 51 [14700/36045]\tLoss: 591.0046\n",
      "Training Epoch: 51 [14750/36045]\tLoss: 508.6524\n",
      "Training Epoch: 51 [14800/36045]\tLoss: 498.5414\n",
      "Training Epoch: 51 [14850/36045]\tLoss: 505.5600\n",
      "Training Epoch: 51 [14900/36045]\tLoss: 499.1532\n",
      "Training Epoch: 51 [14950/36045]\tLoss: 507.5151\n",
      "Training Epoch: 51 [15000/36045]\tLoss: 519.6951\n",
      "Training Epoch: 51 [15050/36045]\tLoss: 515.8821\n",
      "Training Epoch: 51 [15100/36045]\tLoss: 499.8160\n",
      "Training Epoch: 51 [15150/36045]\tLoss: 495.8783\n",
      "Training Epoch: 51 [15200/36045]\tLoss: 459.7098\n",
      "Training Epoch: 51 [15250/36045]\tLoss: 481.0512\n",
      "Training Epoch: 51 [15300/36045]\tLoss: 466.8924\n",
      "Training Epoch: 51 [15350/36045]\tLoss: 478.3108\n",
      "Training Epoch: 51 [15400/36045]\tLoss: 459.9456\n",
      "Training Epoch: 51 [15450/36045]\tLoss: 445.4614\n",
      "Training Epoch: 51 [15500/36045]\tLoss: 458.2376\n",
      "Training Epoch: 51 [15550/36045]\tLoss: 455.2066\n",
      "Training Epoch: 51 [15600/36045]\tLoss: 521.2305\n",
      "Training Epoch: 51 [15650/36045]\tLoss: 537.3690\n",
      "Training Epoch: 51 [15700/36045]\tLoss: 529.9227\n",
      "Training Epoch: 51 [15750/36045]\tLoss: 521.4835\n",
      "Training Epoch: 51 [15800/36045]\tLoss: 501.7061\n",
      "Training Epoch: 51 [15850/36045]\tLoss: 517.2642\n",
      "Training Epoch: 51 [15900/36045]\tLoss: 525.8667\n",
      "Training Epoch: 51 [15950/36045]\tLoss: 545.5986\n",
      "Training Epoch: 51 [16000/36045]\tLoss: 515.9503\n",
      "Training Epoch: 51 [16050/36045]\tLoss: 485.6170\n",
      "Training Epoch: 51 [16100/36045]\tLoss: 450.8641\n",
      "Training Epoch: 51 [16150/36045]\tLoss: 439.0400\n",
      "Training Epoch: 51 [16200/36045]\tLoss: 533.7302\n",
      "Training Epoch: 51 [16250/36045]\tLoss: 560.7855\n",
      "Training Epoch: 51 [16300/36045]\tLoss: 612.1237\n",
      "Training Epoch: 51 [16350/36045]\tLoss: 632.7512\n",
      "Training Epoch: 51 [16400/36045]\tLoss: 604.6639\n",
      "Training Epoch: 51 [16450/36045]\tLoss: 587.1856\n",
      "Training Epoch: 51 [16500/36045]\tLoss: 586.9834\n",
      "Training Epoch: 51 [16550/36045]\tLoss: 552.6870\n",
      "Training Epoch: 51 [16600/36045]\tLoss: 573.9496\n",
      "Training Epoch: 51 [16650/36045]\tLoss: 589.5123\n",
      "Training Epoch: 51 [16700/36045]\tLoss: 570.3571\n",
      "Training Epoch: 51 [16750/36045]\tLoss: 563.1525\n",
      "Training Epoch: 51 [16800/36045]\tLoss: 571.2583\n",
      "Training Epoch: 51 [16850/36045]\tLoss: 544.5946\n",
      "Training Epoch: 51 [16900/36045]\tLoss: 554.3391\n",
      "Training Epoch: 51 [16950/36045]\tLoss: 576.7428\n",
      "Training Epoch: 51 [17000/36045]\tLoss: 561.6213\n",
      "Training Epoch: 51 [17050/36045]\tLoss: 584.5488\n",
      "Training Epoch: 51 [17100/36045]\tLoss: 580.3364\n",
      "Training Epoch: 51 [17150/36045]\tLoss: 503.3076\n",
      "Training Epoch: 51 [17200/36045]\tLoss: 465.9391\n",
      "Training Epoch: 51 [17250/36045]\tLoss: 488.2028\n",
      "Training Epoch: 51 [17300/36045]\tLoss: 516.8496\n",
      "Training Epoch: 51 [17350/36045]\tLoss: 499.0028\n",
      "Training Epoch: 51 [17400/36045]\tLoss: 518.6443\n",
      "Training Epoch: 51 [17450/36045]\tLoss: 536.5982\n",
      "Training Epoch: 51 [17500/36045]\tLoss: 525.1791\n",
      "Training Epoch: 51 [17550/36045]\tLoss: 523.5815\n",
      "Training Epoch: 51 [17600/36045]\tLoss: 518.1356\n",
      "Training Epoch: 51 [17650/36045]\tLoss: 533.0602\n",
      "Training Epoch: 51 [17700/36045]\tLoss: 512.7018\n",
      "Training Epoch: 51 [17750/36045]\tLoss: 528.2101\n",
      "Training Epoch: 51 [17800/36045]\tLoss: 519.7313\n",
      "Training Epoch: 51 [17850/36045]\tLoss: 537.4083\n",
      "Training Epoch: 51 [17900/36045]\tLoss: 565.0399\n",
      "Training Epoch: 51 [17950/36045]\tLoss: 577.4456\n",
      "Training Epoch: 51 [18000/36045]\tLoss: 568.6196\n",
      "Training Epoch: 51 [18050/36045]\tLoss: 622.6467\n",
      "Training Epoch: 51 [18100/36045]\tLoss: 623.1832\n",
      "Training Epoch: 51 [18150/36045]\tLoss: 634.3455\n",
      "Training Epoch: 51 [18200/36045]\tLoss: 617.1428\n",
      "Training Epoch: 51 [18250/36045]\tLoss: 637.7357\n",
      "Training Epoch: 51 [18300/36045]\tLoss: 595.5953\n",
      "Training Epoch: 51 [18350/36045]\tLoss: 669.0964\n",
      "Training Epoch: 51 [18400/36045]\tLoss: 641.6857\n",
      "Training Epoch: 51 [18450/36045]\tLoss: 621.6277\n",
      "Training Epoch: 51 [18500/36045]\tLoss: 620.4815\n",
      "Training Epoch: 51 [18550/36045]\tLoss: 608.2815\n",
      "Training Epoch: 51 [18600/36045]\tLoss: 598.0545\n",
      "Training Epoch: 51 [18650/36045]\tLoss: 642.9668\n",
      "Training Epoch: 51 [18700/36045]\tLoss: 676.3304\n",
      "Training Epoch: 51 [18750/36045]\tLoss: 664.2893\n",
      "Training Epoch: 51 [18800/36045]\tLoss: 686.7530\n",
      "Training Epoch: 51 [18850/36045]\tLoss: 631.9960\n",
      "Training Epoch: 51 [18900/36045]\tLoss: 675.9167\n",
      "Training Epoch: 51 [18950/36045]\tLoss: 618.6852\n",
      "Training Epoch: 51 [19000/36045]\tLoss: 505.2804\n",
      "Training Epoch: 51 [19050/36045]\tLoss: 490.5462\n",
      "Training Epoch: 51 [19100/36045]\tLoss: 498.3466\n",
      "Training Epoch: 51 [19150/36045]\tLoss: 488.6812\n",
      "Training Epoch: 51 [19200/36045]\tLoss: 520.2329\n",
      "Training Epoch: 51 [19250/36045]\tLoss: 535.7766\n",
      "Training Epoch: 51 [19300/36045]\tLoss: 544.9943\n",
      "Training Epoch: 51 [19350/36045]\tLoss: 528.9438\n",
      "Training Epoch: 51 [19400/36045]\tLoss: 548.4792\n",
      "Training Epoch: 51 [19450/36045]\tLoss: 540.0208\n",
      "Training Epoch: 51 [19500/36045]\tLoss: 540.9943\n",
      "Training Epoch: 51 [19550/36045]\tLoss: 539.2180\n",
      "Training Epoch: 51 [19600/36045]\tLoss: 579.9250\n",
      "Training Epoch: 51 [19650/36045]\tLoss: 778.6600\n",
      "Training Epoch: 51 [19700/36045]\tLoss: 737.2257\n",
      "Training Epoch: 51 [19750/36045]\tLoss: 741.3758\n",
      "Training Epoch: 51 [19800/36045]\tLoss: 742.1512\n",
      "Training Epoch: 51 [19850/36045]\tLoss: 482.3265\n",
      "Training Epoch: 51 [19900/36045]\tLoss: 462.0676\n",
      "Training Epoch: 51 [19950/36045]\tLoss: 466.1025\n",
      "Training Epoch: 51 [20000/36045]\tLoss: 466.3371\n",
      "Training Epoch: 51 [20050/36045]\tLoss: 521.4639\n",
      "Training Epoch: 51 [20100/36045]\tLoss: 526.5453\n",
      "Training Epoch: 51 [20150/36045]\tLoss: 527.4099\n",
      "Training Epoch: 51 [20200/36045]\tLoss: 527.3356\n",
      "Training Epoch: 51 [20250/36045]\tLoss: 563.2166\n",
      "Training Epoch: 51 [20300/36045]\tLoss: 599.3182\n",
      "Training Epoch: 51 [20350/36045]\tLoss: 616.9575\n",
      "Training Epoch: 51 [20400/36045]\tLoss: 632.9637\n",
      "Training Epoch: 51 [20450/36045]\tLoss: 602.7090\n",
      "Training Epoch: 51 [20500/36045]\tLoss: 588.1845\n",
      "Training Epoch: 51 [20550/36045]\tLoss: 516.0710\n",
      "Training Epoch: 51 [20600/36045]\tLoss: 525.3793\n",
      "Training Epoch: 51 [20650/36045]\tLoss: 522.4938\n",
      "Training Epoch: 51 [20700/36045]\tLoss: 510.7305\n",
      "Training Epoch: 51 [20750/36045]\tLoss: 551.3072\n",
      "Training Epoch: 51 [20800/36045]\tLoss: 599.1077\n",
      "Training Epoch: 51 [20850/36045]\tLoss: 585.9830\n",
      "Training Epoch: 51 [20900/36045]\tLoss: 628.1051\n",
      "Training Epoch: 51 [20950/36045]\tLoss: 591.4948\n",
      "Training Epoch: 51 [21000/36045]\tLoss: 556.9061\n",
      "Training Epoch: 51 [21050/36045]\tLoss: 476.3425\n",
      "Training Epoch: 51 [21100/36045]\tLoss: 481.7237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 51 [21150/36045]\tLoss: 515.4752\n",
      "Training Epoch: 51 [21200/36045]\tLoss: 514.4052\n",
      "Training Epoch: 51 [21250/36045]\tLoss: 491.8866\n",
      "Training Epoch: 51 [21300/36045]\tLoss: 573.9794\n",
      "Training Epoch: 51 [21350/36045]\tLoss: 565.7933\n",
      "Training Epoch: 51 [21400/36045]\tLoss: 569.5728\n",
      "Training Epoch: 51 [21450/36045]\tLoss: 575.7029\n",
      "Training Epoch: 51 [21500/36045]\tLoss: 577.0111\n",
      "Training Epoch: 51 [21550/36045]\tLoss: 671.0237\n",
      "Training Epoch: 51 [21600/36045]\tLoss: 669.3292\n",
      "Training Epoch: 51 [21650/36045]\tLoss: 681.6258\n",
      "Training Epoch: 51 [21700/36045]\tLoss: 685.2303\n",
      "Training Epoch: 51 [21750/36045]\tLoss: 657.5723\n",
      "Training Epoch: 51 [21800/36045]\tLoss: 481.8662\n",
      "Training Epoch: 51 [21850/36045]\tLoss: 464.8820\n",
      "Training Epoch: 51 [21900/36045]\tLoss: 474.0473\n",
      "Training Epoch: 51 [21950/36045]\tLoss: 475.9550\n",
      "Training Epoch: 51 [22000/36045]\tLoss: 479.1029\n",
      "Training Epoch: 51 [22050/36045]\tLoss: 497.6655\n",
      "Training Epoch: 51 [22100/36045]\tLoss: 490.0591\n",
      "Training Epoch: 51 [22150/36045]\tLoss: 476.9475\n",
      "Training Epoch: 51 [22200/36045]\tLoss: 492.4294\n",
      "Training Epoch: 51 [22250/36045]\tLoss: 497.3490\n",
      "Training Epoch: 51 [22300/36045]\tLoss: 548.8848\n",
      "Training Epoch: 51 [22350/36045]\tLoss: 574.1042\n",
      "Training Epoch: 51 [22400/36045]\tLoss: 587.4818\n",
      "Training Epoch: 51 [22450/36045]\tLoss: 575.0365\n",
      "Training Epoch: 51 [22500/36045]\tLoss: 558.5400\n",
      "Training Epoch: 51 [22550/36045]\tLoss: 592.8301\n",
      "Training Epoch: 51 [22600/36045]\tLoss: 639.5751\n",
      "Training Epoch: 51 [22650/36045]\tLoss: 671.7749\n",
      "Training Epoch: 51 [22700/36045]\tLoss: 692.9095\n",
      "Training Epoch: 51 [22750/36045]\tLoss: 712.6289\n",
      "Training Epoch: 51 [22800/36045]\tLoss: 740.1545\n",
      "Training Epoch: 51 [22850/36045]\tLoss: 613.6748\n",
      "Training Epoch: 51 [22900/36045]\tLoss: 618.2814\n",
      "Training Epoch: 51 [22950/36045]\tLoss: 598.1468\n",
      "Training Epoch: 51 [23000/36045]\tLoss: 594.6183\n",
      "Training Epoch: 51 [23050/36045]\tLoss: 528.4543\n",
      "Training Epoch: 51 [23100/36045]\tLoss: 543.7743\n",
      "Training Epoch: 51 [23150/36045]\tLoss: 531.9514\n",
      "Training Epoch: 51 [23200/36045]\tLoss: 503.3497\n",
      "Training Epoch: 51 [23250/36045]\tLoss: 506.4612\n",
      "Training Epoch: 51 [23300/36045]\tLoss: 502.4177\n",
      "Training Epoch: 51 [23350/36045]\tLoss: 522.2941\n",
      "Training Epoch: 51 [23400/36045]\tLoss: 566.5131\n",
      "Training Epoch: 51 [23450/36045]\tLoss: 560.2805\n",
      "Training Epoch: 51 [23500/36045]\tLoss: 539.7689\n",
      "Training Epoch: 51 [23550/36045]\tLoss: 578.5251\n",
      "Training Epoch: 51 [23600/36045]\tLoss: 657.9534\n",
      "Training Epoch: 51 [23650/36045]\tLoss: 668.5480\n",
      "Training Epoch: 51 [23700/36045]\tLoss: 676.5424\n",
      "Training Epoch: 51 [23750/36045]\tLoss: 653.0714\n",
      "Training Epoch: 51 [23800/36045]\tLoss: 524.7474\n",
      "Training Epoch: 51 [23850/36045]\tLoss: 550.1658\n",
      "Training Epoch: 51 [23900/36045]\tLoss: 540.1363\n",
      "Training Epoch: 51 [23950/36045]\tLoss: 523.4318\n",
      "Training Epoch: 51 [24000/36045]\tLoss: 500.8832\n",
      "Training Epoch: 51 [24050/36045]\tLoss: 462.0117\n",
      "Training Epoch: 51 [24100/36045]\tLoss: 486.0529\n",
      "Training Epoch: 51 [24150/36045]\tLoss: 477.8282\n",
      "Training Epoch: 51 [24200/36045]\tLoss: 475.8611\n",
      "Training Epoch: 51 [24250/36045]\tLoss: 462.2668\n",
      "Training Epoch: 51 [24300/36045]\tLoss: 499.1909\n",
      "Training Epoch: 51 [24350/36045]\tLoss: 510.9901\n",
      "Training Epoch: 51 [24400/36045]\tLoss: 525.7952\n",
      "Training Epoch: 51 [24450/36045]\tLoss: 501.0178\n",
      "Training Epoch: 51 [24500/36045]\tLoss: 529.0956\n",
      "Training Epoch: 51 [24550/36045]\tLoss: 614.7451\n",
      "Training Epoch: 51 [24600/36045]\tLoss: 605.7706\n",
      "Training Epoch: 51 [24650/36045]\tLoss: 579.6200\n",
      "Training Epoch: 51 [24700/36045]\tLoss: 589.6157\n",
      "Training Epoch: 51 [24750/36045]\tLoss: 545.2290\n",
      "Training Epoch: 51 [24800/36045]\tLoss: 445.0663\n",
      "Training Epoch: 51 [24850/36045]\tLoss: 462.5005\n",
      "Training Epoch: 51 [24900/36045]\tLoss: 460.1559\n",
      "Training Epoch: 51 [24950/36045]\tLoss: 462.7159\n",
      "Training Epoch: 51 [25000/36045]\tLoss: 445.2375\n",
      "Training Epoch: 51 [25050/36045]\tLoss: 425.7138\n",
      "Training Epoch: 51 [25100/36045]\tLoss: 381.1754\n",
      "Training Epoch: 51 [25150/36045]\tLoss: 352.7610\n",
      "Training Epoch: 51 [25200/36045]\tLoss: 347.7790\n",
      "Training Epoch: 51 [25250/36045]\tLoss: 373.1836\n",
      "Training Epoch: 51 [25300/36045]\tLoss: 490.6583\n",
      "Training Epoch: 51 [25350/36045]\tLoss: 486.7379\n",
      "Training Epoch: 51 [25400/36045]\tLoss: 455.0994\n",
      "Training Epoch: 51 [25450/36045]\tLoss: 456.8575\n",
      "Training Epoch: 51 [25500/36045]\tLoss: 496.2504\n",
      "Training Epoch: 51 [25550/36045]\tLoss: 580.3727\n",
      "Training Epoch: 51 [25600/36045]\tLoss: 584.9248\n",
      "Training Epoch: 51 [25650/36045]\tLoss: 564.7190\n",
      "Training Epoch: 51 [25700/36045]\tLoss: 574.2184\n",
      "Training Epoch: 51 [25750/36045]\tLoss: 554.8877\n",
      "Training Epoch: 51 [25800/36045]\tLoss: 349.6036\n",
      "Training Epoch: 51 [25850/36045]\tLoss: 357.2399\n",
      "Training Epoch: 51 [25900/36045]\tLoss: 339.8561\n",
      "Training Epoch: 51 [25950/36045]\tLoss: 350.0105\n",
      "Training Epoch: 51 [26000/36045]\tLoss: 430.7808\n",
      "Training Epoch: 51 [26050/36045]\tLoss: 584.7470\n",
      "Training Epoch: 51 [26100/36045]\tLoss: 609.6342\n",
      "Training Epoch: 51 [26150/36045]\tLoss: 609.1247\n",
      "Training Epoch: 51 [26200/36045]\tLoss: 583.9089\n",
      "Training Epoch: 51 [26250/36045]\tLoss: 614.2623\n",
      "Training Epoch: 51 [26300/36045]\tLoss: 566.7164\n",
      "Training Epoch: 51 [26350/36045]\tLoss: 576.5180\n",
      "Training Epoch: 51 [26400/36045]\tLoss: 551.2787\n",
      "Training Epoch: 51 [26450/36045]\tLoss: 483.0624\n",
      "Training Epoch: 51 [26500/36045]\tLoss: 572.0023\n",
      "Training Epoch: 51 [26550/36045]\tLoss: 570.3160\n",
      "Training Epoch: 51 [26600/36045]\tLoss: 565.6322\n",
      "Training Epoch: 51 [26650/36045]\tLoss: 579.9580\n",
      "Training Epoch: 51 [26700/36045]\tLoss: 560.0657\n",
      "Training Epoch: 51 [26750/36045]\tLoss: 526.2922\n",
      "Training Epoch: 51 [26800/36045]\tLoss: 388.9014\n",
      "Training Epoch: 51 [26850/36045]\tLoss: 320.9733\n",
      "Training Epoch: 51 [26900/36045]\tLoss: 322.3719\n",
      "Training Epoch: 51 [26950/36045]\tLoss: 353.9318\n",
      "Training Epoch: 51 [27000/36045]\tLoss: 587.2123\n",
      "Training Epoch: 51 [27050/36045]\tLoss: 612.7352\n",
      "Training Epoch: 51 [27100/36045]\tLoss: 593.2993\n",
      "Training Epoch: 51 [27150/36045]\tLoss: 631.2315\n",
      "Training Epoch: 51 [27200/36045]\tLoss: 456.5783\n",
      "Training Epoch: 51 [27250/36045]\tLoss: 447.0084\n",
      "Training Epoch: 51 [27300/36045]\tLoss: 436.1254\n",
      "Training Epoch: 51 [27350/36045]\tLoss: 432.5945\n",
      "Training Epoch: 51 [27400/36045]\tLoss: 431.9682\n",
      "Training Epoch: 51 [27450/36045]\tLoss: 548.2769\n",
      "Training Epoch: 51 [27500/36045]\tLoss: 587.5887\n",
      "Training Epoch: 51 [27550/36045]\tLoss: 580.6738\n",
      "Training Epoch: 51 [27600/36045]\tLoss: 591.8765\n",
      "Training Epoch: 51 [27650/36045]\tLoss: 583.9013\n",
      "Training Epoch: 51 [27700/36045]\tLoss: 610.6680\n",
      "Training Epoch: 51 [27750/36045]\tLoss: 622.3254\n",
      "Training Epoch: 51 [27800/36045]\tLoss: 609.8293\n",
      "Training Epoch: 51 [27850/36045]\tLoss: 600.2319\n",
      "Training Epoch: 51 [27900/36045]\tLoss: 547.2607\n",
      "Training Epoch: 51 [27950/36045]\tLoss: 459.0727\n",
      "Training Epoch: 51 [28000/36045]\tLoss: 436.0939\n",
      "Training Epoch: 51 [28050/36045]\tLoss: 445.4189\n",
      "Training Epoch: 51 [28100/36045]\tLoss: 437.0960\n",
      "Training Epoch: 51 [28150/36045]\tLoss: 454.5245\n",
      "Training Epoch: 51 [28200/36045]\tLoss: 463.2773\n",
      "Training Epoch: 51 [28250/36045]\tLoss: 455.9803\n",
      "Training Epoch: 51 [28300/36045]\tLoss: 432.9722\n",
      "Training Epoch: 51 [28350/36045]\tLoss: 428.6941\n",
      "Training Epoch: 51 [28400/36045]\tLoss: 754.7972\n",
      "Training Epoch: 51 [28450/36045]\tLoss: 693.5683\n",
      "Training Epoch: 51 [28500/36045]\tLoss: 599.3691\n",
      "Training Epoch: 51 [28550/36045]\tLoss: 551.5853\n",
      "Training Epoch: 51 [28600/36045]\tLoss: 573.7920\n",
      "Training Epoch: 51 [28650/36045]\tLoss: 623.1473\n",
      "Training Epoch: 51 [28700/36045]\tLoss: 616.5335\n",
      "Training Epoch: 51 [28750/36045]\tLoss: 603.9261\n",
      "Training Epoch: 51 [28800/36045]\tLoss: 614.5887\n",
      "Training Epoch: 51 [28850/36045]\tLoss: 534.2751\n",
      "Training Epoch: 51 [28900/36045]\tLoss: 435.1701\n",
      "Training Epoch: 51 [28950/36045]\tLoss: 434.2153\n",
      "Training Epoch: 51 [29000/36045]\tLoss: 429.8909\n",
      "Training Epoch: 51 [29050/36045]\tLoss: 437.1932\n",
      "Training Epoch: 51 [29100/36045]\tLoss: 454.8318\n",
      "Training Epoch: 51 [29150/36045]\tLoss: 445.7480\n",
      "Training Epoch: 51 [29200/36045]\tLoss: 431.3578\n",
      "Training Epoch: 51 [29250/36045]\tLoss: 421.2644\n",
      "Training Epoch: 51 [29300/36045]\tLoss: 472.0321\n",
      "Training Epoch: 51 [29350/36045]\tLoss: 553.1932\n",
      "Training Epoch: 51 [29400/36045]\tLoss: 570.8560\n",
      "Training Epoch: 51 [29450/36045]\tLoss: 586.8947\n",
      "Training Epoch: 51 [29500/36045]\tLoss: 601.4636\n",
      "Training Epoch: 51 [29550/36045]\tLoss: 571.7108\n",
      "Training Epoch: 51 [29600/36045]\tLoss: 480.3895\n",
      "Training Epoch: 51 [29650/36045]\tLoss: 462.0676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 51 [29700/36045]\tLoss: 416.0654\n",
      "Training Epoch: 51 [29750/36045]\tLoss: 414.1762\n",
      "Training Epoch: 51 [29800/36045]\tLoss: 460.7945\n",
      "Training Epoch: 51 [29850/36045]\tLoss: 538.7386\n",
      "Training Epoch: 51 [29900/36045]\tLoss: 534.0318\n",
      "Training Epoch: 51 [29950/36045]\tLoss: 556.0562\n",
      "Training Epoch: 51 [30000/36045]\tLoss: 529.8306\n",
      "Training Epoch: 51 [30050/36045]\tLoss: 537.2250\n",
      "Training Epoch: 51 [30100/36045]\tLoss: 654.8904\n",
      "Training Epoch: 51 [30150/36045]\tLoss: 636.7601\n",
      "Training Epoch: 51 [30200/36045]\tLoss: 600.5110\n",
      "Training Epoch: 51 [30250/36045]\tLoss: 649.0861\n",
      "Training Epoch: 51 [30300/36045]\tLoss: 633.0320\n",
      "Training Epoch: 51 [30350/36045]\tLoss: 479.7457\n",
      "Training Epoch: 51 [30400/36045]\tLoss: 464.6039\n",
      "Training Epoch: 51 [30450/36045]\tLoss: 467.0789\n",
      "Training Epoch: 51 [30500/36045]\tLoss: 436.0144\n",
      "Training Epoch: 51 [30550/36045]\tLoss: 404.7924\n",
      "Training Epoch: 51 [30600/36045]\tLoss: 398.1963\n",
      "Training Epoch: 51 [30650/36045]\tLoss: 388.4057\n",
      "Training Epoch: 51 [30700/36045]\tLoss: 405.9172\n",
      "Training Epoch: 51 [30750/36045]\tLoss: 392.9822\n",
      "Training Epoch: 51 [30800/36045]\tLoss: 419.1818\n",
      "Training Epoch: 51 [30850/36045]\tLoss: 411.0154\n",
      "Training Epoch: 51 [30900/36045]\tLoss: 422.4811\n",
      "Training Epoch: 51 [30950/36045]\tLoss: 444.0875\n",
      "Training Epoch: 51 [31000/36045]\tLoss: 436.6675\n",
      "Training Epoch: 51 [31050/36045]\tLoss: 364.3612\n",
      "Training Epoch: 51 [31100/36045]\tLoss: 355.2101\n",
      "Training Epoch: 51 [31150/36045]\tLoss: 362.9234\n",
      "Training Epoch: 51 [31200/36045]\tLoss: 450.2460\n",
      "Training Epoch: 51 [31250/36045]\tLoss: 584.1569\n",
      "Training Epoch: 51 [31300/36045]\tLoss: 555.5826\n",
      "Training Epoch: 51 [31350/36045]\tLoss: 571.8305\n",
      "Training Epoch: 51 [31400/36045]\tLoss: 551.4747\n",
      "Training Epoch: 51 [31450/36045]\tLoss: 568.1923\n",
      "Training Epoch: 51 [31500/36045]\tLoss: 580.4818\n",
      "Training Epoch: 51 [31550/36045]\tLoss: 586.9585\n",
      "Training Epoch: 51 [31600/36045]\tLoss: 551.8627\n",
      "Training Epoch: 51 [31650/36045]\tLoss: 591.2576\n",
      "Training Epoch: 51 [31700/36045]\tLoss: 427.3014\n",
      "Training Epoch: 51 [31750/36045]\tLoss: 352.7505\n",
      "Training Epoch: 51 [31800/36045]\tLoss: 336.7582\n",
      "Training Epoch: 51 [31850/36045]\tLoss: 344.2576\n",
      "Training Epoch: 51 [31900/36045]\tLoss: 547.0081\n",
      "Training Epoch: 51 [31950/36045]\tLoss: 710.1413\n",
      "Training Epoch: 51 [32000/36045]\tLoss: 816.5119\n",
      "Training Epoch: 51 [32050/36045]\tLoss: 772.7431\n",
      "Training Epoch: 51 [32100/36045]\tLoss: 764.5734\n",
      "Training Epoch: 51 [32150/36045]\tLoss: 583.0295\n",
      "Training Epoch: 51 [32200/36045]\tLoss: 584.4355\n",
      "Training Epoch: 51 [32250/36045]\tLoss: 594.4257\n",
      "Training Epoch: 51 [32300/36045]\tLoss: 576.7522\n",
      "Training Epoch: 51 [32350/36045]\tLoss: 573.0088\n",
      "Training Epoch: 51 [32400/36045]\tLoss: 537.7462\n",
      "Training Epoch: 51 [32450/36045]\tLoss: 442.9276\n",
      "Training Epoch: 51 [32500/36045]\tLoss: 425.4856\n",
      "Training Epoch: 51 [32550/36045]\tLoss: 427.4802\n",
      "Training Epoch: 51 [32600/36045]\tLoss: 424.6996\n",
      "Training Epoch: 51 [32650/36045]\tLoss: 552.7408\n",
      "Training Epoch: 51 [32700/36045]\tLoss: 603.9180\n",
      "Training Epoch: 51 [32750/36045]\tLoss: 574.8356\n",
      "Training Epoch: 51 [32800/36045]\tLoss: 589.2253\n",
      "Training Epoch: 51 [32850/36045]\tLoss: 543.9489\n",
      "Training Epoch: 51 [32900/36045]\tLoss: 433.8440\n",
      "Training Epoch: 51 [32950/36045]\tLoss: 454.5451\n",
      "Training Epoch: 51 [33000/36045]\tLoss: 452.9045\n",
      "Training Epoch: 51 [33050/36045]\tLoss: 431.2923\n",
      "Training Epoch: 51 [33100/36045]\tLoss: 490.1417\n",
      "Training Epoch: 51 [33150/36045]\tLoss: 668.0795\n",
      "Training Epoch: 51 [33200/36045]\tLoss: 650.2984\n",
      "Training Epoch: 51 [33250/36045]\tLoss: 670.0021\n",
      "Training Epoch: 51 [33300/36045]\tLoss: 714.5092\n",
      "Training Epoch: 51 [33350/36045]\tLoss: 546.5178\n",
      "Training Epoch: 51 [33400/36045]\tLoss: 396.7913\n",
      "Training Epoch: 51 [33450/36045]\tLoss: 393.0066\n",
      "Training Epoch: 51 [33500/36045]\tLoss: 404.4235\n",
      "Training Epoch: 51 [33550/36045]\tLoss: 418.4502\n",
      "Training Epoch: 51 [33600/36045]\tLoss: 420.2855\n",
      "Training Epoch: 51 [33650/36045]\tLoss: 562.1025\n",
      "Training Epoch: 51 [33700/36045]\tLoss: 543.7256\n",
      "Training Epoch: 51 [33750/36045]\tLoss: 562.9836\n",
      "Training Epoch: 51 [33800/36045]\tLoss: 560.7098\n",
      "Training Epoch: 51 [33850/36045]\tLoss: 562.2516\n",
      "Training Epoch: 51 [33900/36045]\tLoss: 575.9646\n",
      "Training Epoch: 51 [33950/36045]\tLoss: 585.2191\n",
      "Training Epoch: 51 [34000/36045]\tLoss: 572.0974\n",
      "Training Epoch: 51 [34050/36045]\tLoss: 575.6472\n",
      "Training Epoch: 51 [34100/36045]\tLoss: 555.2342\n",
      "Training Epoch: 51 [34150/36045]\tLoss: 513.7576\n",
      "Training Epoch: 51 [34200/36045]\tLoss: 486.6810\n",
      "Training Epoch: 51 [34250/36045]\tLoss: 500.2932\n",
      "Training Epoch: 51 [34300/36045]\tLoss: 426.7439\n",
      "Training Epoch: 51 [34350/36045]\tLoss: 449.6060\n",
      "Training Epoch: 51 [34400/36045]\tLoss: 443.2360\n",
      "Training Epoch: 51 [34450/36045]\tLoss: 417.5199\n",
      "Training Epoch: 51 [34500/36045]\tLoss: 445.2288\n",
      "Training Epoch: 51 [34550/36045]\tLoss: 437.1295\n",
      "Training Epoch: 51 [34600/36045]\tLoss: 444.9417\n",
      "Training Epoch: 51 [34650/36045]\tLoss: 550.3313\n",
      "Training Epoch: 51 [34700/36045]\tLoss: 583.9768\n",
      "Training Epoch: 51 [34750/36045]\tLoss: 517.3959\n",
      "Training Epoch: 51 [34800/36045]\tLoss: 594.7810\n",
      "Training Epoch: 51 [34850/36045]\tLoss: 601.4965\n",
      "Training Epoch: 51 [34900/36045]\tLoss: 644.9075\n",
      "Training Epoch: 51 [34950/36045]\tLoss: 629.2694\n",
      "Training Epoch: 51 [35000/36045]\tLoss: 630.5977\n",
      "Training Epoch: 51 [35050/36045]\tLoss: 618.2993\n",
      "Training Epoch: 51 [35100/36045]\tLoss: 537.0560\n",
      "Training Epoch: 51 [35150/36045]\tLoss: 528.6781\n",
      "Training Epoch: 51 [35200/36045]\tLoss: 443.5927\n",
      "Training Epoch: 51 [35250/36045]\tLoss: 488.0485\n",
      "Training Epoch: 51 [35300/36045]\tLoss: 504.9361\n",
      "Training Epoch: 51 [35350/36045]\tLoss: 561.6564\n",
      "Training Epoch: 51 [35400/36045]\tLoss: 589.3820\n",
      "Training Epoch: 51 [35450/36045]\tLoss: 561.1848\n",
      "Training Epoch: 51 [35500/36045]\tLoss: 542.8806\n",
      "Training Epoch: 51 [35550/36045]\tLoss: 529.9968\n",
      "Training Epoch: 51 [35600/36045]\tLoss: 581.3820\n",
      "Training Epoch: 51 [35650/36045]\tLoss: 652.7806\n",
      "Training Epoch: 51 [35700/36045]\tLoss: 579.4099\n",
      "Training Epoch: 51 [35750/36045]\tLoss: 637.0341\n",
      "Training Epoch: 51 [35800/36045]\tLoss: 644.1190\n",
      "Training Epoch: 51 [35850/36045]\tLoss: 618.0102\n",
      "Training Epoch: 51 [35900/36045]\tLoss: 638.1274\n",
      "Training Epoch: 51 [35950/36045]\tLoss: 634.3734\n",
      "Training Epoch: 51 [36000/36045]\tLoss: 628.4257\n",
      "Training Epoch: 51 [36045/36045]\tLoss: 614.9640\n",
      "Training Epoch: 51 [4004/4004]\tLoss: 570.6996\n",
      "Training Epoch: 52 [50/36045]\tLoss: 568.6874\n",
      "Training Epoch: 52 [100/36045]\tLoss: 543.9533\n",
      "Training Epoch: 52 [150/36045]\tLoss: 541.4842\n",
      "Training Epoch: 52 [200/36045]\tLoss: 528.1898\n",
      "Training Epoch: 52 [250/36045]\tLoss: 637.2076\n",
      "Training Epoch: 52 [300/36045]\tLoss: 702.8531\n",
      "Training Epoch: 52 [350/36045]\tLoss: 669.5864\n",
      "Training Epoch: 52 [400/36045]\tLoss: 663.0918\n",
      "Training Epoch: 52 [450/36045]\tLoss: 644.0653\n",
      "Training Epoch: 52 [500/36045]\tLoss: 595.3715\n",
      "Training Epoch: 52 [550/36045]\tLoss: 598.6841\n",
      "Training Epoch: 52 [600/36045]\tLoss: 585.4930\n",
      "Training Epoch: 52 [650/36045]\tLoss: 605.6461\n",
      "Training Epoch: 52 [700/36045]\tLoss: 590.1919\n",
      "Training Epoch: 52 [750/36045]\tLoss: 565.2794\n",
      "Training Epoch: 52 [800/36045]\tLoss: 577.0504\n",
      "Training Epoch: 52 [850/36045]\tLoss: 561.2504\n",
      "Training Epoch: 52 [900/36045]\tLoss: 537.6311\n",
      "Training Epoch: 52 [950/36045]\tLoss: 507.7151\n",
      "Training Epoch: 52 [1000/36045]\tLoss: 492.5811\n",
      "Training Epoch: 52 [1050/36045]\tLoss: 495.0291\n",
      "Training Epoch: 52 [1100/36045]\tLoss: 482.1288\n",
      "Training Epoch: 52 [1150/36045]\tLoss: 491.7263\n",
      "Training Epoch: 52 [1200/36045]\tLoss: 520.6713\n",
      "Training Epoch: 52 [1250/36045]\tLoss: 595.0746\n",
      "Training Epoch: 52 [1300/36045]\tLoss: 602.1494\n",
      "Training Epoch: 52 [1350/36045]\tLoss: 603.5682\n",
      "Training Epoch: 52 [1400/36045]\tLoss: 626.8450\n",
      "Training Epoch: 52 [1450/36045]\tLoss: 606.7755\n",
      "Training Epoch: 52 [1500/36045]\tLoss: 553.6799\n",
      "Training Epoch: 52 [1550/36045]\tLoss: 567.6229\n",
      "Training Epoch: 52 [1600/36045]\tLoss: 577.2840\n",
      "Training Epoch: 52 [1650/36045]\tLoss: 565.1494\n",
      "Training Epoch: 52 [1700/36045]\tLoss: 577.5798\n",
      "Training Epoch: 52 [1750/36045]\tLoss: 619.3192\n",
      "Training Epoch: 52 [1800/36045]\tLoss: 602.3701\n",
      "Training Epoch: 52 [1850/36045]\tLoss: 617.5320\n",
      "Training Epoch: 52 [1900/36045]\tLoss: 577.2121\n",
      "Training Epoch: 52 [1950/36045]\tLoss: 586.7645\n",
      "Training Epoch: 52 [2000/36045]\tLoss: 527.7920\n",
      "Training Epoch: 52 [2050/36045]\tLoss: 530.6210\n",
      "Training Epoch: 52 [2100/36045]\tLoss: 559.2300\n",
      "Training Epoch: 52 [2150/36045]\tLoss: 540.3369\n",
      "Training Epoch: 52 [2200/36045]\tLoss: 503.8524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 52 [2250/36045]\tLoss: 476.5611\n",
      "Training Epoch: 52 [2300/36045]\tLoss: 499.8396\n",
      "Training Epoch: 52 [2350/36045]\tLoss: 477.9295\n",
      "Training Epoch: 52 [2400/36045]\tLoss: 485.1205\n",
      "Training Epoch: 52 [2450/36045]\tLoss: 621.8163\n",
      "Training Epoch: 52 [2500/36045]\tLoss: 652.8709\n",
      "Training Epoch: 52 [2550/36045]\tLoss: 651.0138\n",
      "Training Epoch: 52 [2600/36045]\tLoss: 659.8886\n",
      "Training Epoch: 52 [2650/36045]\tLoss: 783.7657\n",
      "Training Epoch: 52 [2700/36045]\tLoss: 871.8410\n",
      "Training Epoch: 52 [2750/36045]\tLoss: 942.2822\n",
      "Training Epoch: 52 [2800/36045]\tLoss: 951.3830\n",
      "Training Epoch: 52 [2850/36045]\tLoss: 718.1073\n",
      "Training Epoch: 52 [2900/36045]\tLoss: 678.4183\n",
      "Training Epoch: 52 [2950/36045]\tLoss: 656.6024\n",
      "Training Epoch: 52 [3000/36045]\tLoss: 650.2089\n",
      "Training Epoch: 52 [3050/36045]\tLoss: 680.6914\n",
      "Training Epoch: 52 [3100/36045]\tLoss: 621.5785\n",
      "Training Epoch: 52 [3150/36045]\tLoss: 477.5189\n",
      "Training Epoch: 52 [3200/36045]\tLoss: 494.3625\n",
      "Training Epoch: 52 [3250/36045]\tLoss: 466.9685\n",
      "Training Epoch: 52 [3300/36045]\tLoss: 442.2159\n",
      "Training Epoch: 52 [3350/36045]\tLoss: 467.0553\n",
      "Training Epoch: 52 [3400/36045]\tLoss: 489.1345\n",
      "Training Epoch: 52 [3450/36045]\tLoss: 524.3781\n",
      "Training Epoch: 52 [3500/36045]\tLoss: 511.9758\n",
      "Training Epoch: 52 [3550/36045]\tLoss: 489.4880\n",
      "Training Epoch: 52 [3600/36045]\tLoss: 526.3822\n",
      "Training Epoch: 52 [3650/36045]\tLoss: 608.5792\n",
      "Training Epoch: 52 [3700/36045]\tLoss: 616.3312\n",
      "Training Epoch: 52 [3750/36045]\tLoss: 585.7028\n",
      "Training Epoch: 52 [3800/36045]\tLoss: 583.5496\n",
      "Training Epoch: 52 [3850/36045]\tLoss: 586.2406\n",
      "Training Epoch: 52 [3900/36045]\tLoss: 590.5781\n",
      "Training Epoch: 52 [3950/36045]\tLoss: 570.0217\n",
      "Training Epoch: 52 [4000/36045]\tLoss: 574.3568\n",
      "Training Epoch: 52 [4050/36045]\tLoss: 526.3472\n",
      "Training Epoch: 52 [4100/36045]\tLoss: 513.4625\n",
      "Training Epoch: 52 [4150/36045]\tLoss: 527.0062\n",
      "Training Epoch: 52 [4200/36045]\tLoss: 521.9468\n",
      "Training Epoch: 52 [4250/36045]\tLoss: 523.8434\n",
      "Training Epoch: 52 [4300/36045]\tLoss: 538.1600\n",
      "Training Epoch: 52 [4350/36045]\tLoss: 522.5527\n",
      "Training Epoch: 52 [4400/36045]\tLoss: 499.6535\n",
      "Training Epoch: 52 [4450/36045]\tLoss: 549.4620\n",
      "Training Epoch: 52 [4500/36045]\tLoss: 592.1051\n",
      "Training Epoch: 52 [4550/36045]\tLoss: 594.5782\n",
      "Training Epoch: 52 [4600/36045]\tLoss: 616.1780\n",
      "Training Epoch: 52 [4650/36045]\tLoss: 606.5601\n",
      "Training Epoch: 52 [4700/36045]\tLoss: 558.9177\n",
      "Training Epoch: 52 [4750/36045]\tLoss: 540.5271\n",
      "Training Epoch: 52 [4800/36045]\tLoss: 564.2227\n",
      "Training Epoch: 52 [4850/36045]\tLoss: 550.9903\n",
      "Training Epoch: 52 [4900/36045]\tLoss: 536.5981\n",
      "Training Epoch: 52 [4950/36045]\tLoss: 551.9131\n",
      "Training Epoch: 52 [5000/36045]\tLoss: 580.9933\n",
      "Training Epoch: 52 [5050/36045]\tLoss: 562.9481\n",
      "Training Epoch: 52 [5100/36045]\tLoss: 572.9646\n",
      "Training Epoch: 52 [5150/36045]\tLoss: 557.5859\n",
      "Training Epoch: 52 [5200/36045]\tLoss: 555.5002\n",
      "Training Epoch: 52 [5250/36045]\tLoss: 549.0528\n",
      "Training Epoch: 52 [5300/36045]\tLoss: 548.9314\n",
      "Training Epoch: 52 [5350/36045]\tLoss: 569.4872\n",
      "Training Epoch: 52 [5400/36045]\tLoss: 549.1439\n",
      "Training Epoch: 52 [5450/36045]\tLoss: 520.1369\n",
      "Training Epoch: 52 [5500/36045]\tLoss: 548.1110\n",
      "Training Epoch: 52 [5550/36045]\tLoss: 536.3865\n",
      "Training Epoch: 52 [5600/36045]\tLoss: 613.7051\n",
      "Training Epoch: 52 [5650/36045]\tLoss: 580.1513\n",
      "Training Epoch: 52 [5700/36045]\tLoss: 544.5073\n",
      "Training Epoch: 52 [5750/36045]\tLoss: 528.5522\n",
      "Training Epoch: 52 [5800/36045]\tLoss: 557.5865\n",
      "Training Epoch: 52 [5850/36045]\tLoss: 547.6924\n",
      "Training Epoch: 52 [5900/36045]\tLoss: 629.0355\n",
      "Training Epoch: 52 [5950/36045]\tLoss: 645.2226\n",
      "Training Epoch: 52 [6000/36045]\tLoss: 631.4905\n",
      "Training Epoch: 52 [6050/36045]\tLoss: 609.9075\n",
      "Training Epoch: 52 [6100/36045]\tLoss: 614.2188\n",
      "Training Epoch: 52 [6150/36045]\tLoss: 606.1976\n",
      "Training Epoch: 52 [6200/36045]\tLoss: 611.5989\n",
      "Training Epoch: 52 [6250/36045]\tLoss: 632.9986\n",
      "Training Epoch: 52 [6300/36045]\tLoss: 644.5151\n",
      "Training Epoch: 52 [6350/36045]\tLoss: 689.3190\n",
      "Training Epoch: 52 [6400/36045]\tLoss: 566.5353\n",
      "Training Epoch: 52 [6450/36045]\tLoss: 520.1703\n",
      "Training Epoch: 52 [6500/36045]\tLoss: 529.5125\n",
      "Training Epoch: 52 [6550/36045]\tLoss: 546.9348\n",
      "Training Epoch: 52 [6600/36045]\tLoss: 544.6604\n",
      "Training Epoch: 52 [6650/36045]\tLoss: 615.5086\n",
      "Training Epoch: 52 [6700/36045]\tLoss: 643.5454\n",
      "Training Epoch: 52 [6750/36045]\tLoss: 621.4540\n",
      "Training Epoch: 52 [6800/36045]\tLoss: 624.4139\n",
      "Training Epoch: 52 [6850/36045]\tLoss: 612.5759\n",
      "Training Epoch: 52 [6900/36045]\tLoss: 545.6210\n",
      "Training Epoch: 52 [6950/36045]\tLoss: 514.4658\n",
      "Training Epoch: 52 [7000/36045]\tLoss: 547.1213\n",
      "Training Epoch: 52 [7050/36045]\tLoss: 559.0710\n",
      "Training Epoch: 52 [7100/36045]\tLoss: 557.8401\n",
      "Training Epoch: 52 [7150/36045]\tLoss: 567.5810\n",
      "Training Epoch: 52 [7200/36045]\tLoss: 569.3707\n",
      "Training Epoch: 52 [7250/36045]\tLoss: 567.8908\n",
      "Training Epoch: 52 [7300/36045]\tLoss: 554.1896\n",
      "Training Epoch: 52 [7350/36045]\tLoss: 552.1407\n",
      "Training Epoch: 52 [7400/36045]\tLoss: 504.1304\n",
      "Training Epoch: 52 [7450/36045]\tLoss: 507.4451\n",
      "Training Epoch: 52 [7500/36045]\tLoss: 503.2539\n",
      "Training Epoch: 52 [7550/36045]\tLoss: 481.9637\n",
      "Training Epoch: 52 [7600/36045]\tLoss: 533.8329\n",
      "Training Epoch: 52 [7650/36045]\tLoss: 570.2975\n",
      "Training Epoch: 52 [7700/36045]\tLoss: 542.7296\n",
      "Training Epoch: 52 [7750/36045]\tLoss: 556.9396\n",
      "Training Epoch: 52 [7800/36045]\tLoss: 546.7484\n",
      "Training Epoch: 52 [7850/36045]\tLoss: 529.1081\n",
      "Training Epoch: 52 [7900/36045]\tLoss: 557.8639\n",
      "Training Epoch: 52 [7950/36045]\tLoss: 555.5606\n",
      "Training Epoch: 52 [8000/36045]\tLoss: 573.6253\n",
      "Training Epoch: 52 [8050/36045]\tLoss: 539.8355\n",
      "Training Epoch: 52 [8100/36045]\tLoss: 564.7183\n",
      "Training Epoch: 52 [8150/36045]\tLoss: 641.2441\n",
      "Training Epoch: 52 [8200/36045]\tLoss: 628.9788\n",
      "Training Epoch: 52 [8250/36045]\tLoss: 597.6375\n",
      "Training Epoch: 52 [8300/36045]\tLoss: 653.7481\n",
      "Training Epoch: 52 [8350/36045]\tLoss: 598.8220\n",
      "Training Epoch: 52 [8400/36045]\tLoss: 535.1823\n",
      "Training Epoch: 52 [8450/36045]\tLoss: 500.7484\n",
      "Training Epoch: 52 [8500/36045]\tLoss: 533.1622\n",
      "Training Epoch: 52 [8550/36045]\tLoss: 527.1888\n",
      "Training Epoch: 52 [8600/36045]\tLoss: 521.5936\n",
      "Training Epoch: 52 [8650/36045]\tLoss: 552.4582\n",
      "Training Epoch: 52 [8700/36045]\tLoss: 584.3346\n",
      "Training Epoch: 52 [8750/36045]\tLoss: 574.1461\n",
      "Training Epoch: 52 [8800/36045]\tLoss: 579.8101\n",
      "Training Epoch: 52 [8850/36045]\tLoss: 573.5328\n",
      "Training Epoch: 52 [8900/36045]\tLoss: 517.7563\n",
      "Training Epoch: 52 [8950/36045]\tLoss: 528.1033\n",
      "Training Epoch: 52 [9000/36045]\tLoss: 544.0424\n",
      "Training Epoch: 52 [9050/36045]\tLoss: 545.6445\n",
      "Training Epoch: 52 [9100/36045]\tLoss: 562.1987\n",
      "Training Epoch: 52 [9150/36045]\tLoss: 416.1664\n",
      "Training Epoch: 52 [9200/36045]\tLoss: 310.7132\n",
      "Training Epoch: 52 [9250/36045]\tLoss: 337.2439\n",
      "Training Epoch: 52 [9300/36045]\tLoss: 346.6795\n",
      "Training Epoch: 52 [9350/36045]\tLoss: 320.0963\n",
      "Training Epoch: 52 [9400/36045]\tLoss: 626.4714\n",
      "Training Epoch: 52 [9450/36045]\tLoss: 665.2614\n",
      "Training Epoch: 52 [9500/36045]\tLoss: 653.1334\n",
      "Training Epoch: 52 [9550/36045]\tLoss: 690.9772\n",
      "Training Epoch: 52 [9600/36045]\tLoss: 515.6058\n",
      "Training Epoch: 52 [9650/36045]\tLoss: 520.7710\n",
      "Training Epoch: 52 [9700/36045]\tLoss: 506.4503\n",
      "Training Epoch: 52 [9750/36045]\tLoss: 505.0731\n",
      "Training Epoch: 52 [9800/36045]\tLoss: 660.4590\n",
      "Training Epoch: 52 [9850/36045]\tLoss: 697.2238\n",
      "Training Epoch: 52 [9900/36045]\tLoss: 705.7121\n",
      "Training Epoch: 52 [9950/36045]\tLoss: 687.8387\n",
      "Training Epoch: 52 [10000/36045]\tLoss: 637.1180\n",
      "Training Epoch: 52 [10050/36045]\tLoss: 521.1496\n",
      "Training Epoch: 52 [10100/36045]\tLoss: 528.5854\n",
      "Training Epoch: 52 [10150/36045]\tLoss: 536.3120\n",
      "Training Epoch: 52 [10200/36045]\tLoss: 525.5646\n",
      "Training Epoch: 52 [10250/36045]\tLoss: 633.1058\n",
      "Training Epoch: 52 [10300/36045]\tLoss: 615.2646\n",
      "Training Epoch: 52 [10350/36045]\tLoss: 647.9443\n",
      "Training Epoch: 52 [10400/36045]\tLoss: 637.8849\n",
      "Training Epoch: 52 [10450/36045]\tLoss: 597.9133\n",
      "Training Epoch: 52 [10500/36045]\tLoss: 499.4091\n",
      "Training Epoch: 52 [10550/36045]\tLoss: 493.7064\n",
      "Training Epoch: 52 [10600/36045]\tLoss: 515.1581\n",
      "Training Epoch: 52 [10650/36045]\tLoss: 520.9599\n",
      "Training Epoch: 52 [10700/36045]\tLoss: 599.4843\n",
      "Training Epoch: 52 [10750/36045]\tLoss: 657.9197\n",
      "Training Epoch: 52 [10800/36045]\tLoss: 604.0270\n",
      "Training Epoch: 52 [10850/36045]\tLoss: 640.7929\n",
      "Training Epoch: 52 [10900/36045]\tLoss: 667.2163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 52 [10950/36045]\tLoss: 490.2926\n",
      "Training Epoch: 52 [11000/36045]\tLoss: 484.1413\n",
      "Training Epoch: 52 [11050/36045]\tLoss: 519.2975\n",
      "Training Epoch: 52 [11100/36045]\tLoss: 529.1519\n",
      "Training Epoch: 52 [11150/36045]\tLoss: 574.1995\n",
      "Training Epoch: 52 [11200/36045]\tLoss: 603.0364\n",
      "Training Epoch: 52 [11250/36045]\tLoss: 614.0839\n",
      "Training Epoch: 52 [11300/36045]\tLoss: 594.5507\n",
      "Training Epoch: 52 [11350/36045]\tLoss: 592.3343\n",
      "Training Epoch: 52 [11400/36045]\tLoss: 556.2299\n",
      "Training Epoch: 52 [11450/36045]\tLoss: 525.7998\n",
      "Training Epoch: 52 [11500/36045]\tLoss: 523.4216\n",
      "Training Epoch: 52 [11550/36045]\tLoss: 532.6005\n",
      "Training Epoch: 52 [11600/36045]\tLoss: 592.1937\n",
      "Training Epoch: 52 [11650/36045]\tLoss: 643.1219\n",
      "Training Epoch: 52 [11700/36045]\tLoss: 642.0696\n",
      "Training Epoch: 52 [11750/36045]\tLoss: 659.8582\n",
      "Training Epoch: 52 [11800/36045]\tLoss: 702.2770\n",
      "Training Epoch: 52 [11850/36045]\tLoss: 760.9034\n",
      "Training Epoch: 52 [11900/36045]\tLoss: 972.1187\n",
      "Training Epoch: 52 [11950/36045]\tLoss: 975.4297\n",
      "Training Epoch: 52 [12000/36045]\tLoss: 986.2166\n",
      "Training Epoch: 52 [12050/36045]\tLoss: 946.2017\n",
      "Training Epoch: 52 [12100/36045]\tLoss: 602.4957\n",
      "Training Epoch: 52 [12150/36045]\tLoss: 451.2831\n",
      "Training Epoch: 52 [12200/36045]\tLoss: 446.1623\n",
      "Training Epoch: 52 [12250/36045]\tLoss: 454.6767\n",
      "Training Epoch: 52 [12300/36045]\tLoss: 588.2566\n",
      "Training Epoch: 52 [12350/36045]\tLoss: 642.8426\n",
      "Training Epoch: 52 [12400/36045]\tLoss: 649.8817\n",
      "Training Epoch: 52 [12450/36045]\tLoss: 638.9295\n",
      "Training Epoch: 52 [12500/36045]\tLoss: 665.5813\n",
      "Training Epoch: 52 [12550/36045]\tLoss: 635.6295\n",
      "Training Epoch: 52 [12600/36045]\tLoss: 580.0076\n",
      "Training Epoch: 52 [12650/36045]\tLoss: 578.0788\n",
      "Training Epoch: 52 [12700/36045]\tLoss: 599.2379\n",
      "Training Epoch: 52 [12750/36045]\tLoss: 597.6780\n",
      "Training Epoch: 52 [12800/36045]\tLoss: 584.6909\n",
      "Training Epoch: 52 [12850/36045]\tLoss: 613.9064\n",
      "Training Epoch: 52 [12900/36045]\tLoss: 588.4022\n",
      "Training Epoch: 52 [12950/36045]\tLoss: 573.8370\n",
      "Training Epoch: 52 [13000/36045]\tLoss: 607.2786\n",
      "Training Epoch: 52 [13050/36045]\tLoss: 548.3232\n",
      "Training Epoch: 52 [13100/36045]\tLoss: 562.9572\n",
      "Training Epoch: 52 [13150/36045]\tLoss: 554.6019\n",
      "Training Epoch: 52 [13200/36045]\tLoss: 538.2371\n",
      "Training Epoch: 52 [13250/36045]\tLoss: 559.4363\n",
      "Training Epoch: 52 [13300/36045]\tLoss: 595.5627\n",
      "Training Epoch: 52 [13350/36045]\tLoss: 576.9545\n",
      "Training Epoch: 52 [13400/36045]\tLoss: 579.9932\n",
      "Training Epoch: 52 [13450/36045]\tLoss: 577.6673\n",
      "Training Epoch: 52 [13500/36045]\tLoss: 595.7584\n",
      "Training Epoch: 52 [13550/36045]\tLoss: 734.2271\n",
      "Training Epoch: 52 [13600/36045]\tLoss: 766.7472\n",
      "Training Epoch: 52 [13650/36045]\tLoss: 848.8456\n",
      "Training Epoch: 52 [13700/36045]\tLoss: 746.7403\n",
      "Training Epoch: 52 [13750/36045]\tLoss: 583.3645\n",
      "Training Epoch: 52 [13800/36045]\tLoss: 553.8677\n",
      "Training Epoch: 52 [13850/36045]\tLoss: 536.7512\n",
      "Training Epoch: 52 [13900/36045]\tLoss: 543.9777\n",
      "Training Epoch: 52 [13950/36045]\tLoss: 589.0097\n",
      "Training Epoch: 52 [14000/36045]\tLoss: 621.2659\n",
      "Training Epoch: 52 [14050/36045]\tLoss: 596.4487\n",
      "Training Epoch: 52 [14100/36045]\tLoss: 591.4847\n",
      "Training Epoch: 52 [14150/36045]\tLoss: 579.7568\n",
      "Training Epoch: 52 [14200/36045]\tLoss: 619.8932\n",
      "Training Epoch: 52 [14250/36045]\tLoss: 681.3959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8df9f718b9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_pcoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0meval_pcoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/predify/utils/training.py\u001b[0m in \u001b[0;36mtrain_pcoders\u001b[0;34m(net, optimizer, loss_function, epoch, train_loader, device, writer)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_pcoders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/predify/networks/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/es3773/hallucnn/src/models/networks_2022.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, _input)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mlayer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mspeech_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mgenre_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenre_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspeech_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/es3773/hallucnn/src/models/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, _input)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [left, right, top, bot]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/es3773/hallucnn/src/models/pbranchednetwork_all.py\u001b[0m in \u001b[0;36mfw_hook3\u001b[0;34m(m, m_in, m_out)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCoderN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfw_hook3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffm3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfbm3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merm3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech_branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_hook3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ff, fb, target, build_graph, ffm, fbm, erm)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mff\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mffm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0merm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror_scale\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_sqrt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_C_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# print(self.C_sqrt * self.C_sqrt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec27ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
