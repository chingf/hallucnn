{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'\n",
    "datafile = f'{engram_dir}seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 21 10:22:09 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1E:00.0 Off |                  N/A |\r\n",
      "| 27%   25C    P8    19W / 250W |      3MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_AllSeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(3.9285714285714284, 3.9411764705882355), mode=bilinear)\n",
       "      (1): ConvTranspose2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.0, 2.0), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder4): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder5): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CleanSoundsDataset(datafile)\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/36450]\tLoss: 149598.8281\n",
      "Training Epoch: 1 [100/36450]\tLoss: 107713.1719\n",
      "Training Epoch: 1 [150/36450]\tLoss: 87476.9062\n",
      "Training Epoch: 1 [200/36450]\tLoss: 81291.9531\n",
      "Training Epoch: 1 [250/36450]\tLoss: 73679.8906\n",
      "Training Epoch: 1 [300/36450]\tLoss: 75263.9219\n",
      "Training Epoch: 1 [350/36450]\tLoss: 80098.4453\n",
      "Training Epoch: 1 [400/36450]\tLoss: 85437.3594\n",
      "Training Epoch: 1 [450/36450]\tLoss: 86389.7188\n",
      "Training Epoch: 1 [500/36450]\tLoss: 75190.0469\n",
      "Training Epoch: 1 [550/36450]\tLoss: 73881.2344\n",
      "Training Epoch: 1 [600/36450]\tLoss: 70281.7422\n",
      "Training Epoch: 1 [650/36450]\tLoss: 60905.4570\n",
      "Training Epoch: 1 [700/36450]\tLoss: 57642.3086\n",
      "Training Epoch: 1 [750/36450]\tLoss: 57320.9062\n",
      "Training Epoch: 1 [800/36450]\tLoss: 51446.9609\n",
      "Training Epoch: 1 [850/36450]\tLoss: 55782.9570\n",
      "Training Epoch: 1 [900/36450]\tLoss: 55659.3281\n",
      "Training Epoch: 1 [950/36450]\tLoss: 50686.7461\n",
      "Training Epoch: 1 [1000/36450]\tLoss: 58447.4766\n",
      "Training Epoch: 1 [1050/36450]\tLoss: 53968.1836\n",
      "Training Epoch: 1 [1100/36450]\tLoss: 45750.0859\n",
      "Training Epoch: 1 [1150/36450]\tLoss: 48387.7148\n",
      "Training Epoch: 1 [1200/36450]\tLoss: 53299.0781\n",
      "Training Epoch: 1 [1250/36450]\tLoss: 42724.4922\n",
      "Training Epoch: 1 [1300/36450]\tLoss: 47718.8672\n",
      "Training Epoch: 1 [1350/36450]\tLoss: 45460.1133\n",
      "Training Epoch: 1 [1400/36450]\tLoss: 41906.0312\n",
      "Training Epoch: 1 [1450/36450]\tLoss: 42773.9883\n",
      "Training Epoch: 1 [1500/36450]\tLoss: 42199.4180\n",
      "Training Epoch: 1 [1550/36450]\tLoss: 41085.6523\n",
      "Training Epoch: 1 [1600/36450]\tLoss: 45231.6445\n",
      "Training Epoch: 1 [1650/36450]\tLoss: 39682.7617\n",
      "Training Epoch: 1 [1700/36450]\tLoss: 40589.3047\n",
      "Training Epoch: 1 [1750/36450]\tLoss: 36541.1289\n",
      "Training Epoch: 1 [1800/36450]\tLoss: 37521.2109\n",
      "Training Epoch: 1 [1850/36450]\tLoss: 33791.0039\n",
      "Training Epoch: 1 [1900/36450]\tLoss: 37577.2656\n",
      "Training Epoch: 1 [1950/36450]\tLoss: 37876.5078\n",
      "Training Epoch: 1 [2000/36450]\tLoss: 33006.4648\n",
      "Training Epoch: 1 [2050/36450]\tLoss: 38683.9102\n",
      "Training Epoch: 1 [2100/36450]\tLoss: 34562.5391\n",
      "Training Epoch: 1 [2150/36450]\tLoss: 39708.9297\n",
      "Training Epoch: 1 [2200/36450]\tLoss: 32411.1973\n",
      "Training Epoch: 1 [2250/36450]\tLoss: 33007.6289\n",
      "Training Epoch: 1 [2300/36450]\tLoss: 31011.5176\n",
      "Training Epoch: 1 [2350/36450]\tLoss: 31209.7695\n",
      "Training Epoch: 1 [2400/36450]\tLoss: 31515.0938\n",
      "Training Epoch: 1 [2450/36450]\tLoss: 30731.2656\n",
      "Training Epoch: 1 [2500/36450]\tLoss: 32383.5293\n",
      "Training Epoch: 1 [2550/36450]\tLoss: 28692.9922\n",
      "Training Epoch: 1 [2600/36450]\tLoss: 29503.1465\n",
      "Training Epoch: 1 [2650/36450]\tLoss: 30621.0293\n",
      "Training Epoch: 1 [2700/36450]\tLoss: 26562.9121\n",
      "Training Epoch: 1 [2750/36450]\tLoss: 28975.2402\n",
      "Training Epoch: 1 [2800/36450]\tLoss: 27098.0410\n",
      "Training Epoch: 1 [2850/36450]\tLoss: 26886.2656\n",
      "Training Epoch: 1 [2900/36450]\tLoss: 25756.7754\n",
      "Training Epoch: 1 [2950/36450]\tLoss: 25970.1504\n",
      "Training Epoch: 1 [3000/36450]\tLoss: 26253.8164\n",
      "Training Epoch: 1 [3050/36450]\tLoss: 26830.6328\n",
      "Training Epoch: 1 [3100/36450]\tLoss: 24898.7949\n",
      "Training Epoch: 1 [3150/36450]\tLoss: 24372.7305\n",
      "Training Epoch: 1 [3200/36450]\tLoss: 23566.8359\n",
      "Training Epoch: 1 [3250/36450]\tLoss: 24477.7832\n",
      "Training Epoch: 1 [3300/36450]\tLoss: 28350.0176\n",
      "Training Epoch: 1 [3350/36450]\tLoss: 24731.3828\n",
      "Training Epoch: 1 [3400/36450]\tLoss: 24625.0527\n",
      "Training Epoch: 1 [3450/36450]\tLoss: 23704.5117\n",
      "Training Epoch: 1 [3500/36450]\tLoss: 23163.2734\n",
      "Training Epoch: 1 [3550/36450]\tLoss: 21848.1621\n",
      "Training Epoch: 1 [3600/36450]\tLoss: 23230.7832\n",
      "Training Epoch: 1 [3650/36450]\tLoss: 23332.8750\n",
      "Training Epoch: 1 [3700/36450]\tLoss: 23911.5957\n",
      "Training Epoch: 1 [3750/36450]\tLoss: 23521.9434\n",
      "Training Epoch: 1 [3800/36450]\tLoss: 22337.6289\n",
      "Training Epoch: 1 [3850/36450]\tLoss: 23466.9707\n",
      "Training Epoch: 1 [3900/36450]\tLoss: 21252.4023\n",
      "Training Epoch: 1 [3950/36450]\tLoss: 21283.6660\n",
      "Training Epoch: 1 [4000/36450]\tLoss: 20974.1543\n",
      "Training Epoch: 1 [4050/36450]\tLoss: 21995.9688\n",
      "Training Epoch: 1 [4100/36450]\tLoss: 19766.7383\n",
      "Training Epoch: 1 [4150/36450]\tLoss: 18616.6602\n",
      "Training Epoch: 1 [4200/36450]\tLoss: 21893.8906\n",
      "Training Epoch: 1 [4250/36450]\tLoss: 22483.4453\n",
      "Training Epoch: 1 [4300/36450]\tLoss: 19224.8457\n",
      "Training Epoch: 1 [4350/36450]\tLoss: 19220.3477\n",
      "Training Epoch: 1 [4400/36450]\tLoss: 19082.0059\n",
      "Training Epoch: 1 [4450/36450]\tLoss: 19946.7402\n",
      "Training Epoch: 1 [4500/36450]\tLoss: 19913.9727\n",
      "Training Epoch: 1 [4550/36450]\tLoss: 18642.9336\n",
      "Training Epoch: 1 [4600/36450]\tLoss: 19198.3730\n",
      "Training Epoch: 1 [4650/36450]\tLoss: 18692.8750\n",
      "Training Epoch: 1 [4700/36450]\tLoss: 18035.5879\n",
      "Training Epoch: 1 [4750/36450]\tLoss: 18025.9492\n",
      "Training Epoch: 1 [4800/36450]\tLoss: 17188.1172\n",
      "Training Epoch: 1 [4850/36450]\tLoss: 18079.8848\n",
      "Training Epoch: 1 [4900/36450]\tLoss: 18314.2305\n",
      "Training Epoch: 1 [4950/36450]\tLoss: 16473.4707\n",
      "Training Epoch: 1 [5000/36450]\tLoss: 17275.4961\n",
      "Training Epoch: 1 [5050/36450]\tLoss: 16100.3701\n",
      "Training Epoch: 1 [5100/36450]\tLoss: 15701.9336\n",
      "Training Epoch: 1 [5150/36450]\tLoss: 18079.5840\n",
      "Training Epoch: 1 [5200/36450]\tLoss: 16652.0742\n",
      "Training Epoch: 1 [5250/36450]\tLoss: 16665.0938\n",
      "Training Epoch: 1 [5300/36450]\tLoss: 19375.4043\n",
      "Training Epoch: 1 [5350/36450]\tLoss: 17498.8945\n",
      "Training Epoch: 1 [5400/36450]\tLoss: 17478.1191\n",
      "Training Epoch: 1 [5450/36450]\tLoss: 15507.6289\n",
      "Training Epoch: 1 [5500/36450]\tLoss: 17234.9512\n",
      "Training Epoch: 1 [5550/36450]\tLoss: 15955.9902\n",
      "Training Epoch: 1 [5600/36450]\tLoss: 16725.8770\n",
      "Training Epoch: 1 [5650/36450]\tLoss: 15152.0898\n",
      "Training Epoch: 1 [5700/36450]\tLoss: 15113.9219\n",
      "Training Epoch: 1 [5750/36450]\tLoss: 15891.9355\n",
      "Training Epoch: 1 [5800/36450]\tLoss: 15138.0195\n",
      "Training Epoch: 1 [5850/36450]\tLoss: 14574.6445\n",
      "Training Epoch: 1 [5900/36450]\tLoss: 15330.3867\n",
      "Training Epoch: 1 [5950/36450]\tLoss: 14276.1924\n",
      "Training Epoch: 1 [6000/36450]\tLoss: 14801.2109\n",
      "Training Epoch: 1 [6050/36450]\tLoss: 14589.7461\n",
      "Training Epoch: 1 [6100/36450]\tLoss: 15086.4023\n",
      "Training Epoch: 1 [6150/36450]\tLoss: 15206.6533\n",
      "Training Epoch: 1 [6200/36450]\tLoss: 13945.8457\n",
      "Training Epoch: 1 [6250/36450]\tLoss: 15324.1162\n",
      "Training Epoch: 1 [6300/36450]\tLoss: 14510.7197\n",
      "Training Epoch: 1 [6350/36450]\tLoss: 13577.8848\n",
      "Training Epoch: 1 [6400/36450]\tLoss: 13967.8262\n",
      "Training Epoch: 1 [6450/36450]\tLoss: 14804.0918\n",
      "Training Epoch: 1 [6500/36450]\tLoss: 15534.2979\n",
      "Training Epoch: 1 [6550/36450]\tLoss: 14928.1318\n",
      "Training Epoch: 1 [6600/36450]\tLoss: 14000.3506\n",
      "Training Epoch: 1 [6650/36450]\tLoss: 13178.0371\n",
      "Training Epoch: 1 [6700/36450]\tLoss: 13679.3076\n",
      "Training Epoch: 1 [6750/36450]\tLoss: 13737.2988\n",
      "Training Epoch: 1 [6800/36450]\tLoss: 13458.3760\n",
      "Training Epoch: 1 [6850/36450]\tLoss: 14537.8115\n",
      "Training Epoch: 1 [6900/36450]\tLoss: 13828.1807\n",
      "Training Epoch: 1 [6950/36450]\tLoss: 13824.2705\n",
      "Training Epoch: 1 [7000/36450]\tLoss: 13394.0957\n",
      "Training Epoch: 1 [7050/36450]\tLoss: 13924.3379\n",
      "Training Epoch: 1 [7100/36450]\tLoss: 13908.7236\n",
      "Training Epoch: 1 [7150/36450]\tLoss: 12550.2832\n",
      "Training Epoch: 1 [7200/36450]\tLoss: 12906.1631\n",
      "Training Epoch: 1 [7250/36450]\tLoss: 12989.4922\n",
      "Training Epoch: 1 [7300/36450]\tLoss: 12586.9561\n",
      "Training Epoch: 1 [7350/36450]\tLoss: 12860.3652\n",
      "Training Epoch: 1 [7400/36450]\tLoss: 12712.0527\n",
      "Training Epoch: 1 [7450/36450]\tLoss: 12735.7070\n",
      "Training Epoch: 1 [7500/36450]\tLoss: 12220.5889\n",
      "Training Epoch: 1 [7550/36450]\tLoss: 12545.3076\n",
      "Training Epoch: 1 [7600/36450]\tLoss: 12022.9648\n",
      "Training Epoch: 1 [7650/36450]\tLoss: 13615.1621\n",
      "Training Epoch: 1 [7700/36450]\tLoss: 13128.5537\n",
      "Training Epoch: 1 [7750/36450]\tLoss: 12274.0166\n",
      "Training Epoch: 1 [7800/36450]\tLoss: 12248.0625\n",
      "Training Epoch: 1 [7850/36450]\tLoss: 13249.0078\n",
      "Training Epoch: 1 [7900/36450]\tLoss: 12797.5117\n",
      "Training Epoch: 1 [7950/36450]\tLoss: 11478.7578\n",
      "Training Epoch: 1 [8000/36450]\tLoss: 12854.3320\n",
      "Training Epoch: 1 [8050/36450]\tLoss: 13635.0391\n",
      "Training Epoch: 1 [8100/36450]\tLoss: 12232.7490\n",
      "Training Epoch: 1 [8150/36450]\tLoss: 11036.1475\n",
      "Training Epoch: 1 [8200/36450]\tLoss: 12127.4883\n",
      "Training Epoch: 1 [8250/36450]\tLoss: 11724.6895\n",
      "Training Epoch: 1 [8300/36450]\tLoss: 11787.3135\n",
      "Training Epoch: 1 [8350/36450]\tLoss: 11584.5156\n",
      "Training Epoch: 1 [8400/36450]\tLoss: 12614.4922\n",
      "Training Epoch: 1 [8450/36450]\tLoss: 10978.0254\n",
      "Training Epoch: 1 [8500/36450]\tLoss: 11816.8594\n",
      "Training Epoch: 1 [8550/36450]\tLoss: 12029.3018\n",
      "Training Epoch: 1 [8600/36450]\tLoss: 11553.8877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8650/36450]\tLoss: 11932.1279\n",
      "Training Epoch: 1 [8700/36450]\tLoss: 10801.0205\n",
      "Training Epoch: 1 [8750/36450]\tLoss: 10855.8145\n",
      "Training Epoch: 1 [8800/36450]\tLoss: 10741.6475\n",
      "Training Epoch: 1 [8850/36450]\tLoss: 11473.0264\n",
      "Training Epoch: 1 [8900/36450]\tLoss: 11239.7998\n",
      "Training Epoch: 1 [8950/36450]\tLoss: 10894.8906\n",
      "Training Epoch: 1 [9000/36450]\tLoss: 11096.7578\n",
      "Training Epoch: 1 [9050/36450]\tLoss: 10791.1553\n",
      "Training Epoch: 1 [9100/36450]\tLoss: 10535.8066\n",
      "Training Epoch: 1 [9150/36450]\tLoss: 11112.8135\n",
      "Training Epoch: 1 [9200/36450]\tLoss: 11236.3740\n",
      "Training Epoch: 1 [9250/36450]\tLoss: 10811.6787\n",
      "Training Epoch: 1 [9300/36450]\tLoss: 10074.3574\n",
      "Training Epoch: 1 [9350/36450]\tLoss: 10551.3340\n",
      "Training Epoch: 1 [9400/36450]\tLoss: 10886.8379\n",
      "Training Epoch: 1 [9450/36450]\tLoss: 10808.0762\n",
      "Training Epoch: 1 [9500/36450]\tLoss: 10736.1748\n",
      "Training Epoch: 1 [9550/36450]\tLoss: 10720.2217\n",
      "Training Epoch: 1 [9600/36450]\tLoss: 9848.7900\n",
      "Training Epoch: 1 [9650/36450]\tLoss: 10792.8096\n",
      "Training Epoch: 1 [9700/36450]\tLoss: 9535.9180\n",
      "Training Epoch: 1 [9750/36450]\tLoss: 10763.8418\n",
      "Training Epoch: 1 [9800/36450]\tLoss: 9559.4141\n",
      "Training Epoch: 1 [9850/36450]\tLoss: 10008.0576\n",
      "Training Epoch: 1 [9900/36450]\tLoss: 9972.5156\n",
      "Training Epoch: 1 [9950/36450]\tLoss: 10213.0420\n",
      "Training Epoch: 1 [10000/36450]\tLoss: 9872.1660\n",
      "Training Epoch: 1 [10050/36450]\tLoss: 9892.4326\n",
      "Training Epoch: 1 [10100/36450]\tLoss: 10048.1826\n",
      "Training Epoch: 1 [10150/36450]\tLoss: 9254.3369\n",
      "Training Epoch: 1 [10200/36450]\tLoss: 9456.0508\n",
      "Training Epoch: 1 [10250/36450]\tLoss: 10128.1787\n",
      "Training Epoch: 1 [10300/36450]\tLoss: 10493.8350\n",
      "Training Epoch: 1 [10350/36450]\tLoss: 10079.0820\n",
      "Training Epoch: 1 [10400/36450]\tLoss: 9574.2568\n",
      "Training Epoch: 1 [10450/36450]\tLoss: 9154.2793\n",
      "Training Epoch: 1 [10500/36450]\tLoss: 9513.4238\n",
      "Training Epoch: 1 [10550/36450]\tLoss: 9657.3594\n",
      "Training Epoch: 1 [10600/36450]\tLoss: 9518.4854\n",
      "Training Epoch: 1 [10650/36450]\tLoss: 9556.2520\n",
      "Training Epoch: 1 [10700/36450]\tLoss: 9246.5986\n",
      "Training Epoch: 1 [10750/36450]\tLoss: 9527.7305\n",
      "Training Epoch: 1 [10800/36450]\tLoss: 9193.4404\n",
      "Training Epoch: 1 [10850/36450]\tLoss: 8909.7588\n",
      "Training Epoch: 1 [10900/36450]\tLoss: 9583.5635\n",
      "Training Epoch: 1 [10950/36450]\tLoss: 8638.8652\n",
      "Training Epoch: 1 [11000/36450]\tLoss: 9423.7100\n",
      "Training Epoch: 1 [11050/36450]\tLoss: 8586.9199\n",
      "Training Epoch: 1 [11100/36450]\tLoss: 8831.9951\n",
      "Training Epoch: 1 [11150/36450]\tLoss: 9094.5088\n",
      "Training Epoch: 1 [11200/36450]\tLoss: 8313.8760\n",
      "Training Epoch: 1 [11250/36450]\tLoss: 8755.9990\n",
      "Training Epoch: 1 [11300/36450]\tLoss: 8934.7451\n",
      "Training Epoch: 1 [11350/36450]\tLoss: 8263.3301\n",
      "Training Epoch: 1 [11400/36450]\tLoss: 9018.1084\n",
      "Training Epoch: 1 [11450/36450]\tLoss: 9071.6621\n",
      "Training Epoch: 1 [11500/36450]\tLoss: 8353.8711\n",
      "Training Epoch: 1 [11550/36450]\tLoss: 8752.1973\n",
      "Training Epoch: 1 [11600/36450]\tLoss: 8896.7695\n",
      "Training Epoch: 1 [11650/36450]\tLoss: 9151.9912\n",
      "Training Epoch: 1 [11700/36450]\tLoss: 8727.5820\n",
      "Training Epoch: 1 [11750/36450]\tLoss: 9211.7246\n",
      "Training Epoch: 1 [11800/36450]\tLoss: 8846.3271\n",
      "Training Epoch: 1 [11850/36450]\tLoss: 8563.0107\n",
      "Training Epoch: 1 [11900/36450]\tLoss: 7789.4834\n",
      "Training Epoch: 1 [11950/36450]\tLoss: 8102.4829\n",
      "Training Epoch: 1 [12000/36450]\tLoss: 8517.8047\n",
      "Training Epoch: 1 [12050/36450]\tLoss: 8345.0283\n",
      "Training Epoch: 1 [12100/36450]\tLoss: 8168.3105\n",
      "Training Epoch: 1 [12150/36450]\tLoss: 8242.8506\n",
      "Training Epoch: 1 [12200/36450]\tLoss: 8160.8179\n",
      "Training Epoch: 1 [12250/36450]\tLoss: 8214.0791\n",
      "Training Epoch: 1 [12300/36450]\tLoss: 8129.5664\n",
      "Training Epoch: 1 [12350/36450]\tLoss: 8493.4180\n",
      "Training Epoch: 1 [12400/36450]\tLoss: 8402.3848\n",
      "Training Epoch: 1 [12450/36450]\tLoss: 8130.1763\n",
      "Training Epoch: 1 [12500/36450]\tLoss: 8167.0825\n",
      "Training Epoch: 1 [12550/36450]\tLoss: 8045.5684\n",
      "Training Epoch: 1 [12600/36450]\tLoss: 8157.2793\n",
      "Training Epoch: 1 [12650/36450]\tLoss: 7935.3374\n",
      "Training Epoch: 1 [12700/36450]\tLoss: 7606.2197\n",
      "Training Epoch: 1 [12750/36450]\tLoss: 7746.2578\n",
      "Training Epoch: 1 [12800/36450]\tLoss: 7475.4048\n",
      "Training Epoch: 1 [12850/36450]\tLoss: 7613.0615\n",
      "Training Epoch: 1 [12900/36450]\tLoss: 7601.3462\n",
      "Training Epoch: 1 [12950/36450]\tLoss: 7439.2705\n",
      "Training Epoch: 1 [13000/36450]\tLoss: 7941.0923\n",
      "Training Epoch: 1 [13050/36450]\tLoss: 7455.8452\n",
      "Training Epoch: 1 [13100/36450]\tLoss: 7530.7471\n",
      "Training Epoch: 1 [13150/36450]\tLoss: 7575.6543\n",
      "Training Epoch: 1 [13200/36450]\tLoss: 8066.2500\n",
      "Training Epoch: 1 [13250/36450]\tLoss: 7775.7056\n",
      "Training Epoch: 1 [13300/36450]\tLoss: 7418.1064\n",
      "Training Epoch: 1 [13350/36450]\tLoss: 7348.0186\n",
      "Training Epoch: 1 [13400/36450]\tLoss: 7225.3252\n",
      "Training Epoch: 1 [13450/36450]\tLoss: 7465.2095\n",
      "Training Epoch: 1 [13500/36450]\tLoss: 6760.1909\n",
      "Training Epoch: 1 [13550/36450]\tLoss: 7608.7065\n",
      "Training Epoch: 1 [13600/36450]\tLoss: 7083.6841\n",
      "Training Epoch: 1 [13650/36450]\tLoss: 7905.7715\n",
      "Training Epoch: 1 [13700/36450]\tLoss: 7069.2764\n",
      "Training Epoch: 1 [13750/36450]\tLoss: 7254.5596\n",
      "Training Epoch: 1 [13800/36450]\tLoss: 7125.2554\n",
      "Training Epoch: 1 [13850/36450]\tLoss: 7415.2744\n",
      "Training Epoch: 1 [13900/36450]\tLoss: 7178.4468\n",
      "Training Epoch: 1 [13950/36450]\tLoss: 7038.4673\n",
      "Training Epoch: 1 [14000/36450]\tLoss: 7460.5259\n",
      "Training Epoch: 1 [14050/36450]\tLoss: 6878.9478\n",
      "Training Epoch: 1 [14100/36450]\tLoss: 6477.4355\n",
      "Training Epoch: 1 [14150/36450]\tLoss: 7060.9492\n",
      "Training Epoch: 1 [14200/36450]\tLoss: 6839.4175\n",
      "Training Epoch: 1 [14250/36450]\tLoss: 6826.2212\n",
      "Training Epoch: 1 [14300/36450]\tLoss: 6372.0942\n",
      "Training Epoch: 1 [14350/36450]\tLoss: 6837.6050\n",
      "Training Epoch: 1 [14400/36450]\tLoss: 6857.4893\n",
      "Training Epoch: 1 [14450/36450]\tLoss: 6887.5615\n",
      "Training Epoch: 1 [14500/36450]\tLoss: 6677.8589\n",
      "Training Epoch: 1 [14550/36450]\tLoss: 7100.6973\n",
      "Training Epoch: 1 [14600/36450]\tLoss: 6794.2856\n",
      "Training Epoch: 1 [14650/36450]\tLoss: 6785.5215\n",
      "Training Epoch: 1 [14700/36450]\tLoss: 6694.9067\n",
      "Training Epoch: 1 [14750/36450]\tLoss: 6489.0894\n",
      "Training Epoch: 1 [14800/36450]\tLoss: 6801.2866\n",
      "Training Epoch: 1 [14850/36450]\tLoss: 6840.1221\n",
      "Training Epoch: 1 [14900/36450]\tLoss: 6411.7466\n",
      "Training Epoch: 1 [14950/36450]\tLoss: 6815.7202\n",
      "Training Epoch: 1 [15000/36450]\tLoss: 6566.4795\n",
      "Training Epoch: 1 [15050/36450]\tLoss: 6181.7383\n",
      "Training Epoch: 1 [15100/36450]\tLoss: 6367.1997\n",
      "Training Epoch: 1 [15150/36450]\tLoss: 6650.6963\n",
      "Training Epoch: 1 [15200/36450]\tLoss: 6250.8198\n",
      "Training Epoch: 1 [15250/36450]\tLoss: 6735.0132\n",
      "Training Epoch: 1 [15300/36450]\tLoss: 6066.3130\n",
      "Training Epoch: 1 [15350/36450]\tLoss: 6331.1968\n",
      "Training Epoch: 1 [15400/36450]\tLoss: 6313.3340\n",
      "Training Epoch: 1 [15450/36450]\tLoss: 6250.7559\n",
      "Training Epoch: 1 [15500/36450]\tLoss: 6512.7002\n",
      "Training Epoch: 1 [15550/36450]\tLoss: 6558.5913\n",
      "Training Epoch: 1 [15600/36450]\tLoss: 6292.8828\n",
      "Training Epoch: 1 [15650/36450]\tLoss: 6129.8765\n",
      "Training Epoch: 1 [15700/36450]\tLoss: 6005.8867\n",
      "Training Epoch: 1 [15750/36450]\tLoss: 6644.6411\n",
      "Training Epoch: 1 [15800/36450]\tLoss: 6025.4136\n",
      "Training Epoch: 1 [15850/36450]\tLoss: 6375.8018\n",
      "Training Epoch: 1 [15900/36450]\tLoss: 6120.1958\n",
      "Training Epoch: 1 [15950/36450]\tLoss: 5835.2036\n",
      "Training Epoch: 1 [16000/36450]\tLoss: 6025.7383\n",
      "Training Epoch: 1 [16050/36450]\tLoss: 5832.5142\n",
      "Training Epoch: 1 [16100/36450]\tLoss: 6177.7036\n",
      "Training Epoch: 1 [16150/36450]\tLoss: 6166.0410\n",
      "Training Epoch: 1 [16200/36450]\tLoss: 5872.2627\n",
      "Training Epoch: 1 [16250/36450]\tLoss: 5910.0073\n",
      "Training Epoch: 1 [16300/36450]\tLoss: 5924.5195\n",
      "Training Epoch: 1 [16350/36450]\tLoss: 5572.4707\n",
      "Training Epoch: 1 [16400/36450]\tLoss: 5784.2983\n",
      "Training Epoch: 1 [16450/36450]\tLoss: 5896.4009\n",
      "Training Epoch: 1 [16500/36450]\tLoss: 5775.2051\n",
      "Training Epoch: 1 [16550/36450]\tLoss: 5469.8594\n",
      "Training Epoch: 1 [16600/36450]\tLoss: 5488.1357\n",
      "Training Epoch: 1 [16650/36450]\tLoss: 5881.0371\n",
      "Training Epoch: 1 [16700/36450]\tLoss: 5421.6396\n",
      "Training Epoch: 1 [16750/36450]\tLoss: 5603.6416\n",
      "Training Epoch: 1 [16800/36450]\tLoss: 5497.4688\n",
      "Training Epoch: 1 [16850/36450]\tLoss: 5651.7441\n",
      "Training Epoch: 1 [16900/36450]\tLoss: 5587.8955\n",
      "Training Epoch: 1 [16950/36450]\tLoss: 5603.2334\n",
      "Training Epoch: 1 [17000/36450]\tLoss: 5392.5767\n",
      "Training Epoch: 1 [17050/36450]\tLoss: 5519.1782\n",
      "Training Epoch: 1 [17100/36450]\tLoss: 5332.0483\n",
      "Training Epoch: 1 [17150/36450]\tLoss: 5241.1782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [17200/36450]\tLoss: 5748.0859\n",
      "Training Epoch: 1 [17250/36450]\tLoss: 5358.8462\n",
      "Training Epoch: 1 [17300/36450]\tLoss: 5534.0889\n",
      "Training Epoch: 1 [17350/36450]\tLoss: 5459.5054\n",
      "Training Epoch: 1 [17400/36450]\tLoss: 5335.9292\n",
      "Training Epoch: 1 [17450/36450]\tLoss: 5243.0303\n",
      "Training Epoch: 1 [17500/36450]\tLoss: 5255.7012\n",
      "Training Epoch: 1 [17550/36450]\tLoss: 5145.5942\n",
      "Training Epoch: 1 [17600/36450]\tLoss: 5257.8628\n",
      "Training Epoch: 1 [17650/36450]\tLoss: 5171.7632\n",
      "Training Epoch: 1 [17700/36450]\tLoss: 5337.7661\n",
      "Training Epoch: 1 [17750/36450]\tLoss: 5059.0781\n",
      "Training Epoch: 1 [17800/36450]\tLoss: 5040.4346\n",
      "Training Epoch: 1 [17850/36450]\tLoss: 5161.0820\n",
      "Training Epoch: 1 [17900/36450]\tLoss: 5166.5469\n",
      "Training Epoch: 1 [17950/36450]\tLoss: 5367.4844\n",
      "Training Epoch: 1 [18000/36450]\tLoss: 5393.7773\n",
      "Training Epoch: 1 [18050/36450]\tLoss: 5246.8804\n",
      "Training Epoch: 1 [18100/36450]\tLoss: 5282.9321\n",
      "Training Epoch: 1 [18150/36450]\tLoss: 5168.7271\n",
      "Training Epoch: 1 [18200/36450]\tLoss: 4677.8232\n",
      "Training Epoch: 1 [18250/36450]\tLoss: 5255.3765\n",
      "Training Epoch: 1 [18300/36450]\tLoss: 5110.5396\n",
      "Training Epoch: 1 [18350/36450]\tLoss: 4808.2773\n",
      "Training Epoch: 1 [18400/36450]\tLoss: 5074.7275\n",
      "Training Epoch: 1 [18450/36450]\tLoss: 5220.7729\n",
      "Training Epoch: 1 [18500/36450]\tLoss: 4630.8511\n",
      "Training Epoch: 1 [18550/36450]\tLoss: 4716.3169\n",
      "Training Epoch: 1 [18600/36450]\tLoss: 4652.9351\n",
      "Training Epoch: 1 [18650/36450]\tLoss: 5038.6387\n",
      "Training Epoch: 1 [18700/36450]\tLoss: 4921.9517\n",
      "Training Epoch: 1 [18750/36450]\tLoss: 5201.9839\n",
      "Training Epoch: 1 [18800/36450]\tLoss: 4783.2300\n",
      "Training Epoch: 1 [18850/36450]\tLoss: 4890.1729\n",
      "Training Epoch: 1 [18900/36450]\tLoss: 4887.6279\n",
      "Training Epoch: 1 [18950/36450]\tLoss: 4613.0386\n",
      "Training Epoch: 1 [19000/36450]\tLoss: 4835.1958\n",
      "Training Epoch: 1 [19050/36450]\tLoss: 4613.2036\n",
      "Training Epoch: 1 [19100/36450]\tLoss: 4725.4434\n",
      "Training Epoch: 1 [19150/36450]\tLoss: 4762.0747\n",
      "Training Epoch: 1 [19200/36450]\tLoss: 4705.9878\n",
      "Training Epoch: 1 [19250/36450]\tLoss: 4613.6553\n",
      "Training Epoch: 1 [19300/36450]\tLoss: 4779.5483\n",
      "Training Epoch: 1 [19350/36450]\tLoss: 4751.2026\n",
      "Training Epoch: 1 [19400/36450]\tLoss: 4772.9697\n",
      "Training Epoch: 1 [19450/36450]\tLoss: 4581.4175\n",
      "Training Epoch: 1 [19500/36450]\tLoss: 4598.6641\n",
      "Training Epoch: 1 [19550/36450]\tLoss: 4554.2983\n",
      "Training Epoch: 1 [19600/36450]\tLoss: 5010.8262\n",
      "Training Epoch: 1 [19650/36450]\tLoss: 4682.3628\n",
      "Training Epoch: 1 [19700/36450]\tLoss: 4804.3506\n",
      "Training Epoch: 1 [19750/36450]\tLoss: 4540.9463\n",
      "Training Epoch: 1 [19800/36450]\tLoss: 4577.9277\n",
      "Training Epoch: 1 [19850/36450]\tLoss: 4660.0571\n",
      "Training Epoch: 1 [19900/36450]\tLoss: 4473.5059\n",
      "Training Epoch: 1 [19950/36450]\tLoss: 4821.2876\n",
      "Training Epoch: 1 [20000/36450]\tLoss: 4461.5986\n",
      "Training Epoch: 1 [20050/36450]\tLoss: 4514.7148\n",
      "Training Epoch: 1 [20100/36450]\tLoss: 4382.5073\n",
      "Training Epoch: 1 [20150/36450]\tLoss: 4588.6958\n",
      "Training Epoch: 1 [20200/36450]\tLoss: 4312.4233\n",
      "Training Epoch: 1 [20250/36450]\tLoss: 4621.1089\n",
      "Training Epoch: 1 [20300/36450]\tLoss: 4311.4761\n",
      "Training Epoch: 1 [20350/36450]\tLoss: 4224.5815\n",
      "Training Epoch: 1 [20400/36450]\tLoss: 4486.4658\n",
      "Training Epoch: 1 [20450/36450]\tLoss: 4643.8672\n",
      "Training Epoch: 1 [20500/36450]\tLoss: 4340.1045\n",
      "Training Epoch: 1 [20550/36450]\tLoss: 4314.5737\n",
      "Training Epoch: 1 [20600/36450]\tLoss: 4219.2378\n",
      "Training Epoch: 1 [20650/36450]\tLoss: 4356.3540\n",
      "Training Epoch: 1 [20700/36450]\tLoss: 4386.7524\n",
      "Training Epoch: 1 [20750/36450]\tLoss: 4212.2065\n",
      "Training Epoch: 1 [20800/36450]\tLoss: 4273.4062\n",
      "Training Epoch: 1 [20850/36450]\tLoss: 4287.6938\n",
      "Training Epoch: 1 [20900/36450]\tLoss: 4343.1558\n",
      "Training Epoch: 1 [20950/36450]\tLoss: 4284.5972\n",
      "Training Epoch: 1 [21000/36450]\tLoss: 4136.7935\n",
      "Training Epoch: 1 [21050/36450]\tLoss: 4054.9763\n",
      "Training Epoch: 1 [21100/36450]\tLoss: 4068.0869\n",
      "Training Epoch: 1 [21150/36450]\tLoss: 4132.5229\n",
      "Training Epoch: 1 [21200/36450]\tLoss: 4103.9531\n",
      "Training Epoch: 1 [21250/36450]\tLoss: 3993.6213\n",
      "Training Epoch: 1 [21300/36450]\tLoss: 4096.8042\n",
      "Training Epoch: 1 [21350/36450]\tLoss: 3944.0020\n",
      "Training Epoch: 1 [21400/36450]\tLoss: 4052.3730\n",
      "Training Epoch: 1 [21450/36450]\tLoss: 4235.9800\n",
      "Training Epoch: 1 [21500/36450]\tLoss: 4290.5586\n",
      "Training Epoch: 1 [21550/36450]\tLoss: 3947.3086\n",
      "Training Epoch: 1 [21600/36450]\tLoss: 4008.9976\n",
      "Training Epoch: 1 [21650/36450]\tLoss: 4233.5327\n",
      "Training Epoch: 1 [21700/36450]\tLoss: 3999.9834\n",
      "Training Epoch: 1 [21750/36450]\tLoss: 4009.4060\n",
      "Training Epoch: 1 [21800/36450]\tLoss: 3861.2649\n",
      "Training Epoch: 1 [21850/36450]\tLoss: 4008.5605\n",
      "Training Epoch: 1 [21900/36450]\tLoss: 3890.3640\n",
      "Training Epoch: 1 [21950/36450]\tLoss: 4064.0447\n",
      "Training Epoch: 1 [22000/36450]\tLoss: 4042.5471\n",
      "Training Epoch: 1 [22050/36450]\tLoss: 4149.2988\n",
      "Training Epoch: 1 [22100/36450]\tLoss: 3628.6379\n",
      "Training Epoch: 1 [22150/36450]\tLoss: 3891.6487\n",
      "Training Epoch: 1 [22200/36450]\tLoss: 3895.7292\n",
      "Training Epoch: 1 [22250/36450]\tLoss: 3923.9780\n",
      "Training Epoch: 1 [22300/36450]\tLoss: 3727.4539\n",
      "Training Epoch: 1 [22350/36450]\tLoss: 3703.7065\n",
      "Training Epoch: 1 [22400/36450]\tLoss: 3851.4768\n",
      "Training Epoch: 1 [22450/36450]\tLoss: 3893.3547\n",
      "Training Epoch: 1 [22500/36450]\tLoss: 3812.7737\n",
      "Training Epoch: 1 [22550/36450]\tLoss: 3664.9019\n",
      "Training Epoch: 1 [22600/36450]\tLoss: 3877.3757\n",
      "Training Epoch: 1 [22650/36450]\tLoss: 3571.8633\n",
      "Training Epoch: 1 [22700/36450]\tLoss: 3865.9675\n",
      "Training Epoch: 1 [22750/36450]\tLoss: 3698.5532\n",
      "Training Epoch: 1 [22800/36450]\tLoss: 3641.8352\n",
      "Training Epoch: 1 [22850/36450]\tLoss: 3621.6716\n",
      "Training Epoch: 1 [22900/36450]\tLoss: 3678.6560\n",
      "Training Epoch: 1 [22950/36450]\tLoss: 3733.8582\n",
      "Training Epoch: 1 [23000/36450]\tLoss: 3567.3835\n",
      "Training Epoch: 1 [23050/36450]\tLoss: 3807.5569\n",
      "Training Epoch: 1 [23100/36450]\tLoss: 3566.9968\n",
      "Training Epoch: 1 [23150/36450]\tLoss: 3571.6594\n",
      "Training Epoch: 1 [23200/36450]\tLoss: 3668.1660\n",
      "Training Epoch: 1 [23250/36450]\tLoss: 3533.6965\n",
      "Training Epoch: 1 [23300/36450]\tLoss: 3701.1936\n",
      "Training Epoch: 1 [23350/36450]\tLoss: 3624.6724\n",
      "Training Epoch: 1 [23400/36450]\tLoss: 3805.9048\n",
      "Training Epoch: 1 [23450/36450]\tLoss: 3563.5234\n",
      "Training Epoch: 1 [23500/36450]\tLoss: 3540.6753\n",
      "Training Epoch: 1 [23550/36450]\tLoss: 3626.2971\n",
      "Training Epoch: 1 [23600/36450]\tLoss: 3721.1147\n",
      "Training Epoch: 1 [23650/36450]\tLoss: 3442.6240\n",
      "Training Epoch: 1 [23700/36450]\tLoss: 3652.6035\n",
      "Training Epoch: 1 [23750/36450]\tLoss: 3556.6482\n",
      "Training Epoch: 1 [23800/36450]\tLoss: 3734.8987\n",
      "Training Epoch: 1 [23850/36450]\tLoss: 3407.5959\n",
      "Training Epoch: 1 [23900/36450]\tLoss: 3491.0566\n",
      "Training Epoch: 1 [23950/36450]\tLoss: 3599.7175\n",
      "Training Epoch: 1 [24000/36450]\tLoss: 3589.0510\n",
      "Training Epoch: 1 [24050/36450]\tLoss: 3603.1694\n",
      "Training Epoch: 1 [24100/36450]\tLoss: 3457.3088\n",
      "Training Epoch: 1 [24150/36450]\tLoss: 3574.2939\n",
      "Training Epoch: 1 [24200/36450]\tLoss: 3354.9409\n",
      "Training Epoch: 1 [24250/36450]\tLoss: 3462.3828\n",
      "Training Epoch: 1 [24300/36450]\tLoss: 3401.9485\n",
      "Training Epoch: 1 [24350/36450]\tLoss: 3595.2249\n",
      "Training Epoch: 1 [24400/36450]\tLoss: 3503.0229\n",
      "Training Epoch: 1 [24450/36450]\tLoss: 3415.0471\n",
      "Training Epoch: 1 [24500/36450]\tLoss: 3487.9839\n",
      "Training Epoch: 1 [24550/36450]\tLoss: 3380.6548\n",
      "Training Epoch: 1 [24600/36450]\tLoss: 3311.5818\n",
      "Training Epoch: 1 [24650/36450]\tLoss: 3187.2415\n",
      "Training Epoch: 1 [24700/36450]\tLoss: 3412.0315\n",
      "Training Epoch: 1 [24750/36450]\tLoss: 3299.7520\n",
      "Training Epoch: 1 [24800/36450]\tLoss: 3453.6326\n",
      "Training Epoch: 1 [24850/36450]\tLoss: 3390.1863\n",
      "Training Epoch: 1 [24900/36450]\tLoss: 3402.0520\n",
      "Training Epoch: 1 [24950/36450]\tLoss: 3400.8071\n",
      "Training Epoch: 1 [25000/36450]\tLoss: 3378.1416\n",
      "Training Epoch: 1 [25050/36450]\tLoss: 3273.8171\n",
      "Training Epoch: 1 [25100/36450]\tLoss: 3331.6128\n",
      "Training Epoch: 1 [25150/36450]\tLoss: 3161.3445\n",
      "Training Epoch: 1 [25200/36450]\tLoss: 3351.6250\n",
      "Training Epoch: 1 [25250/36450]\tLoss: 3211.9189\n",
      "Training Epoch: 1 [25300/36450]\tLoss: 3251.5842\n",
      "Training Epoch: 1 [25350/36450]\tLoss: 3211.2026\n",
      "Training Epoch: 1 [25400/36450]\tLoss: 3081.5979\n",
      "Training Epoch: 1 [25450/36450]\tLoss: 3183.4539\n",
      "Training Epoch: 1 [25500/36450]\tLoss: 3117.9929\n",
      "Training Epoch: 1 [25550/36450]\tLoss: 3103.1694\n",
      "Training Epoch: 1 [25600/36450]\tLoss: 3162.5085\n",
      "Training Epoch: 1 [25650/36450]\tLoss: 3142.3171\n",
      "Training Epoch: 1 [25700/36450]\tLoss: 3203.0718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [25750/36450]\tLoss: 3284.0571\n",
      "Training Epoch: 1 [25800/36450]\tLoss: 3009.7075\n",
      "Training Epoch: 1 [25850/36450]\tLoss: 3194.6143\n",
      "Training Epoch: 1 [25900/36450]\tLoss: 3158.1770\n",
      "Training Epoch: 1 [25950/36450]\tLoss: 2981.0803\n",
      "Training Epoch: 1 [26000/36450]\tLoss: 3154.5928\n",
      "Training Epoch: 1 [26050/36450]\tLoss: 3021.7097\n",
      "Training Epoch: 1 [26100/36450]\tLoss: 3011.2358\n",
      "Training Epoch: 1 [26150/36450]\tLoss: 3077.8240\n",
      "Training Epoch: 1 [26200/36450]\tLoss: 2976.0491\n",
      "Training Epoch: 1 [26250/36450]\tLoss: 3087.9351\n",
      "Training Epoch: 1 [26300/36450]\tLoss: 3112.7573\n",
      "Training Epoch: 1 [26350/36450]\tLoss: 3031.2236\n",
      "Training Epoch: 1 [26400/36450]\tLoss: 3024.5813\n",
      "Training Epoch: 1 [26450/36450]\tLoss: 3105.3594\n",
      "Training Epoch: 1 [26500/36450]\tLoss: 3128.8323\n",
      "Training Epoch: 1 [26550/36450]\tLoss: 3145.7976\n",
      "Training Epoch: 1 [26600/36450]\tLoss: 3042.5300\n",
      "Training Epoch: 1 [26650/36450]\tLoss: 3071.1973\n",
      "Training Epoch: 1 [26700/36450]\tLoss: 3044.6948\n",
      "Training Epoch: 1 [26750/36450]\tLoss: 2965.6038\n",
      "Training Epoch: 1 [26800/36450]\tLoss: 2847.8687\n",
      "Training Epoch: 1 [26850/36450]\tLoss: 3103.0723\n",
      "Training Epoch: 1 [26900/36450]\tLoss: 2942.4741\n",
      "Training Epoch: 1 [26950/36450]\tLoss: 3118.0225\n",
      "Training Epoch: 1 [27000/36450]\tLoss: 3041.8552\n",
      "Training Epoch: 1 [27050/36450]\tLoss: 2940.6111\n",
      "Training Epoch: 1 [27100/36450]\tLoss: 3074.5322\n",
      "Training Epoch: 1 [27150/36450]\tLoss: 2999.6453\n",
      "Training Epoch: 1 [27200/36450]\tLoss: 2947.2483\n",
      "Training Epoch: 1 [27250/36450]\tLoss: 2942.1328\n",
      "Training Epoch: 1 [27300/36450]\tLoss: 3001.8811\n",
      "Training Epoch: 1 [27350/36450]\tLoss: 2892.0449\n",
      "Training Epoch: 1 [27400/36450]\tLoss: 2790.3708\n",
      "Training Epoch: 1 [27450/36450]\tLoss: 3090.2805\n",
      "Training Epoch: 1 [27500/36450]\tLoss: 3115.0308\n",
      "Training Epoch: 1 [27550/36450]\tLoss: 2829.5356\n",
      "Training Epoch: 1 [27600/36450]\tLoss: 2717.8518\n",
      "Training Epoch: 1 [27650/36450]\tLoss: 2721.4612\n",
      "Training Epoch: 1 [27700/36450]\tLoss: 2961.4377\n",
      "Training Epoch: 1 [27750/36450]\tLoss: 2754.4421\n",
      "Training Epoch: 1 [27800/36450]\tLoss: 2940.1768\n",
      "Training Epoch: 1 [27850/36450]\tLoss: 2938.6824\n",
      "Training Epoch: 1 [27900/36450]\tLoss: 2794.2446\n",
      "Training Epoch: 1 [27950/36450]\tLoss: 2758.7039\n",
      "Training Epoch: 1 [28000/36450]\tLoss: 2841.1418\n",
      "Training Epoch: 1 [28050/36450]\tLoss: 2855.3533\n",
      "Training Epoch: 1 [28100/36450]\tLoss: 2780.0989\n",
      "Training Epoch: 1 [28150/36450]\tLoss: 2733.4255\n",
      "Training Epoch: 1 [28200/36450]\tLoss: 2901.9048\n",
      "Training Epoch: 1 [28250/36450]\tLoss: 2706.3408\n",
      "Training Epoch: 1 [28300/36450]\tLoss: 2826.7007\n",
      "Training Epoch: 1 [28350/36450]\tLoss: 2781.0857\n",
      "Training Epoch: 1 [28400/36450]\tLoss: 2745.4302\n",
      "Training Epoch: 1 [28450/36450]\tLoss: 2817.5098\n",
      "Training Epoch: 1 [28500/36450]\tLoss: 2797.0916\n",
      "Training Epoch: 1 [28550/36450]\tLoss: 2638.7100\n",
      "Training Epoch: 1 [28600/36450]\tLoss: 2735.0603\n",
      "Training Epoch: 1 [28650/36450]\tLoss: 2754.8352\n",
      "Training Epoch: 1 [28700/36450]\tLoss: 2731.0090\n",
      "Training Epoch: 1 [28750/36450]\tLoss: 2828.6111\n",
      "Training Epoch: 1 [28800/36450]\tLoss: 2600.9600\n",
      "Training Epoch: 1 [28850/36450]\tLoss: 2646.1782\n",
      "Training Epoch: 1 [28900/36450]\tLoss: 2777.4644\n",
      "Training Epoch: 1 [28950/36450]\tLoss: 2766.7837\n",
      "Training Epoch: 1 [29000/36450]\tLoss: 2695.4727\n",
      "Training Epoch: 1 [29050/36450]\tLoss: 2629.8020\n",
      "Training Epoch: 1 [29100/36450]\tLoss: 2725.0557\n",
      "Training Epoch: 1 [29150/36450]\tLoss: 2713.1399\n",
      "Training Epoch: 1 [29200/36450]\tLoss: 2653.9236\n",
      "Training Epoch: 1 [29250/36450]\tLoss: 2637.5173\n",
      "Training Epoch: 1 [29300/36450]\tLoss: 2702.6431\n",
      "Training Epoch: 1 [29350/36450]\tLoss: 2489.4106\n",
      "Training Epoch: 1 [29400/36450]\tLoss: 2703.9573\n",
      "Training Epoch: 1 [29450/36450]\tLoss: 2786.0322\n",
      "Training Epoch: 1 [29500/36450]\tLoss: 2630.8613\n",
      "Training Epoch: 1 [29550/36450]\tLoss: 2602.6133\n",
      "Training Epoch: 1 [29600/36450]\tLoss: 2587.5740\n",
      "Training Epoch: 1 [29650/36450]\tLoss: 2605.2563\n",
      "Training Epoch: 1 [29700/36450]\tLoss: 2516.0720\n",
      "Training Epoch: 1 [29750/36450]\tLoss: 2633.8201\n",
      "Training Epoch: 1 [29800/36450]\tLoss: 2648.2639\n",
      "Training Epoch: 1 [29850/36450]\tLoss: 2513.4475\n",
      "Training Epoch: 1 [29900/36450]\tLoss: 2557.7312\n",
      "Training Epoch: 1 [29950/36450]\tLoss: 2426.9590\n",
      "Training Epoch: 1 [30000/36450]\tLoss: 2708.2354\n",
      "Training Epoch: 1 [30050/36450]\tLoss: 2576.5867\n",
      "Training Epoch: 1 [30100/36450]\tLoss: 2478.4417\n",
      "Training Epoch: 1 [30150/36450]\tLoss: 2433.3760\n",
      "Training Epoch: 1 [30200/36450]\tLoss: 2572.7830\n",
      "Training Epoch: 1 [30250/36450]\tLoss: 2517.5725\n",
      "Training Epoch: 1 [30300/36450]\tLoss: 2524.3242\n",
      "Training Epoch: 1 [30350/36450]\tLoss: 2514.1182\n",
      "Training Epoch: 1 [30400/36450]\tLoss: 2508.1484\n",
      "Training Epoch: 1 [30450/36450]\tLoss: 2572.8909\n",
      "Training Epoch: 1 [30500/36450]\tLoss: 2569.5239\n",
      "Training Epoch: 1 [30550/36450]\tLoss: 2479.9451\n",
      "Training Epoch: 1 [30600/36450]\tLoss: 2425.7214\n",
      "Training Epoch: 1 [30650/36450]\tLoss: 2404.2588\n",
      "Training Epoch: 1 [30700/36450]\tLoss: 2482.4275\n",
      "Training Epoch: 1 [30750/36450]\tLoss: 2472.2297\n",
      "Training Epoch: 1 [30800/36450]\tLoss: 2432.2646\n",
      "Training Epoch: 1 [30850/36450]\tLoss: 2547.2532\n",
      "Training Epoch: 1 [30900/36450]\tLoss: 2467.3682\n",
      "Training Epoch: 1 [30950/36450]\tLoss: 2521.1804\n",
      "Training Epoch: 1 [31000/36450]\tLoss: 2472.6609\n",
      "Training Epoch: 1 [31050/36450]\tLoss: 2458.2246\n",
      "Training Epoch: 1 [31100/36450]\tLoss: 2444.9719\n",
      "Training Epoch: 1 [31150/36450]\tLoss: 2398.4685\n",
      "Training Epoch: 1 [31200/36450]\tLoss: 2333.0815\n",
      "Training Epoch: 1 [31250/36450]\tLoss: 2465.6707\n",
      "Training Epoch: 1 [31300/36450]\tLoss: 2307.0891\n",
      "Training Epoch: 1 [31350/36450]\tLoss: 2435.5215\n",
      "Training Epoch: 1 [31400/36450]\tLoss: 2392.8977\n",
      "Training Epoch: 1 [31450/36450]\tLoss: 2388.5266\n",
      "Training Epoch: 1 [31500/36450]\tLoss: 2484.1768\n",
      "Training Epoch: 1 [31550/36450]\tLoss: 2344.9241\n",
      "Training Epoch: 1 [31600/36450]\tLoss: 2325.8174\n",
      "Training Epoch: 1 [31650/36450]\tLoss: 2285.2388\n",
      "Training Epoch: 1 [31700/36450]\tLoss: 2469.8208\n",
      "Training Epoch: 1 [31750/36450]\tLoss: 2480.6255\n",
      "Training Epoch: 1 [31800/36450]\tLoss: 2408.8762\n",
      "Training Epoch: 1 [31850/36450]\tLoss: 2338.2961\n",
      "Training Epoch: 1 [31900/36450]\tLoss: 2359.9197\n",
      "Training Epoch: 1 [31950/36450]\tLoss: 2339.6482\n",
      "Training Epoch: 1 [32000/36450]\tLoss: 2236.1121\n",
      "Training Epoch: 1 [32050/36450]\tLoss: 2354.5730\n",
      "Training Epoch: 1 [32100/36450]\tLoss: 2286.1426\n",
      "Training Epoch: 1 [32150/36450]\tLoss: 2265.2932\n",
      "Training Epoch: 1 [32200/36450]\tLoss: 2323.7815\n",
      "Training Epoch: 1 [32250/36450]\tLoss: 2325.9199\n",
      "Training Epoch: 1 [32300/36450]\tLoss: 2368.2168\n",
      "Training Epoch: 1 [32350/36450]\tLoss: 2262.3752\n",
      "Training Epoch: 1 [32400/36450]\tLoss: 2319.7815\n",
      "Training Epoch: 1 [32450/36450]\tLoss: 2324.8457\n",
      "Training Epoch: 1 [32500/36450]\tLoss: 2273.9451\n",
      "Training Epoch: 1 [32550/36450]\tLoss: 2158.3196\n",
      "Training Epoch: 1 [32600/36450]\tLoss: 2261.7063\n",
      "Training Epoch: 1 [32650/36450]\tLoss: 2295.3098\n",
      "Training Epoch: 1 [32700/36450]\tLoss: 2216.1155\n",
      "Training Epoch: 1 [32750/36450]\tLoss: 2389.1992\n",
      "Training Epoch: 1 [32800/36450]\tLoss: 2367.5852\n",
      "Training Epoch: 1 [32850/36450]\tLoss: 2377.7534\n",
      "Training Epoch: 1 [32900/36450]\tLoss: 2366.8203\n",
      "Training Epoch: 1 [32950/36450]\tLoss: 2199.6433\n",
      "Training Epoch: 1 [33000/36450]\tLoss: 2248.4480\n",
      "Training Epoch: 1 [33050/36450]\tLoss: 2221.3572\n",
      "Training Epoch: 1 [33100/36450]\tLoss: 2108.3157\n",
      "Training Epoch: 1 [33150/36450]\tLoss: 2181.5525\n",
      "Training Epoch: 1 [33200/36450]\tLoss: 2211.4404\n",
      "Training Epoch: 1 [33250/36450]\tLoss: 2191.0264\n",
      "Training Epoch: 1 [33300/36450]\tLoss: 2262.8892\n",
      "Training Epoch: 1 [33350/36450]\tLoss: 2160.5347\n",
      "Training Epoch: 1 [33400/36450]\tLoss: 2313.9839\n",
      "Training Epoch: 1 [33450/36450]\tLoss: 2178.0308\n",
      "Training Epoch: 1 [33500/36450]\tLoss: 2258.8018\n",
      "Training Epoch: 1 [33550/36450]\tLoss: 2173.1855\n",
      "Training Epoch: 1 [33600/36450]\tLoss: 2228.0068\n",
      "Training Epoch: 1 [33650/36450]\tLoss: 2179.9036\n",
      "Training Epoch: 1 [33700/36450]\tLoss: 2148.9211\n",
      "Training Epoch: 1 [33750/36450]\tLoss: 2205.9983\n",
      "Training Epoch: 1 [33800/36450]\tLoss: 2348.2483\n",
      "Training Epoch: 1 [33850/36450]\tLoss: 2124.4600\n",
      "Training Epoch: 1 [33900/36450]\tLoss: 2168.1199\n",
      "Training Epoch: 1 [33950/36450]\tLoss: 2161.5320\n",
      "Training Epoch: 1 [34000/36450]\tLoss: 2249.7502\n",
      "Training Epoch: 1 [34050/36450]\tLoss: 2246.3027\n",
      "Training Epoch: 1 [34100/36450]\tLoss: 2213.3501\n",
      "Training Epoch: 1 [34150/36450]\tLoss: 2268.0811\n",
      "Training Epoch: 1 [34200/36450]\tLoss: 2223.3005\n",
      "Training Epoch: 1 [34250/36450]\tLoss: 2112.5598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [34300/36450]\tLoss: 2125.5950\n",
      "Training Epoch: 1 [34350/36450]\tLoss: 2076.0833\n",
      "Training Epoch: 1 [34400/36450]\tLoss: 2105.8152\n",
      "Training Epoch: 1 [34450/36450]\tLoss: 2150.0603\n",
      "Training Epoch: 1 [34500/36450]\tLoss: 2068.0688\n",
      "Training Epoch: 1 [34550/36450]\tLoss: 2145.7173\n",
      "Training Epoch: 1 [34600/36450]\tLoss: 2215.0986\n",
      "Training Epoch: 1 [34650/36450]\tLoss: 2130.9065\n",
      "Training Epoch: 1 [34700/36450]\tLoss: 2108.9155\n",
      "Training Epoch: 1 [34750/36450]\tLoss: 2186.0696\n",
      "Training Epoch: 1 [34800/36450]\tLoss: 2073.4788\n",
      "Training Epoch: 1 [34850/36450]\tLoss: 2132.7566\n",
      "Training Epoch: 1 [34900/36450]\tLoss: 2099.9705\n",
      "Training Epoch: 1 [34950/36450]\tLoss: 2102.7559\n",
      "Training Epoch: 1 [35000/36450]\tLoss: 2028.6316\n",
      "Training Epoch: 1 [35050/36450]\tLoss: 2102.0388\n",
      "Training Epoch: 1 [35100/36450]\tLoss: 2115.2134\n",
      "Training Epoch: 1 [35150/36450]\tLoss: 2044.1351\n",
      "Training Epoch: 1 [35200/36450]\tLoss: 2011.6365\n",
      "Training Epoch: 1 [35250/36450]\tLoss: 2092.9375\n",
      "Training Epoch: 1 [35300/36450]\tLoss: 2060.3816\n",
      "Training Epoch: 1 [35350/36450]\tLoss: 2034.5629\n",
      "Training Epoch: 1 [35400/36450]\tLoss: 1974.2363\n",
      "Training Epoch: 1 [35450/36450]\tLoss: 2086.4719\n",
      "Training Epoch: 1 [35500/36450]\tLoss: 2084.0273\n",
      "Training Epoch: 1 [35550/36450]\tLoss: 2077.4880\n",
      "Training Epoch: 1 [35600/36450]\tLoss: 2036.1149\n",
      "Training Epoch: 1 [35650/36450]\tLoss: 2037.5601\n",
      "Training Epoch: 1 [35700/36450]\tLoss: 1962.3812\n",
      "Training Epoch: 1 [35750/36450]\tLoss: 2104.1531\n",
      "Training Epoch: 1 [35800/36450]\tLoss: 1964.5948\n",
      "Training Epoch: 1 [35850/36450]\tLoss: 2070.0193\n",
      "Training Epoch: 1 [35900/36450]\tLoss: 2072.9321\n",
      "Training Epoch: 1 [35950/36450]\tLoss: 1968.1162\n",
      "Training Epoch: 1 [36000/36450]\tLoss: 2094.8828\n",
      "Training Epoch: 1 [36050/36450]\tLoss: 2046.7397\n",
      "Training Epoch: 1 [36100/36450]\tLoss: 2068.6536\n",
      "Training Epoch: 1 [36150/36450]\tLoss: 2132.0225\n",
      "Training Epoch: 1 [36200/36450]\tLoss: 2054.2922\n",
      "Training Epoch: 1 [36250/36450]\tLoss: 1965.9521\n",
      "Training Epoch: 1 [36300/36450]\tLoss: 1955.0507\n",
      "Training Epoch: 1 [36350/36450]\tLoss: 1897.1368\n",
      "Training Epoch: 1 [36400/36450]\tLoss: 2086.3621\n",
      "Training Epoch: 1 [36450/36450]\tLoss: 2014.9265\n",
      "Training Epoch: 1 [4050/4050]\tLoss: 1031.3748\n",
      "Training Epoch: 2 [50/36450]\tLoss: 1962.1936\n",
      "Training Epoch: 2 [100/36450]\tLoss: 1974.2557\n",
      "Training Epoch: 2 [150/36450]\tLoss: 2081.1167\n",
      "Training Epoch: 2 [200/36450]\tLoss: 2013.5870\n",
      "Training Epoch: 2 [250/36450]\tLoss: 2032.2958\n",
      "Training Epoch: 2 [300/36450]\tLoss: 2057.3499\n",
      "Training Epoch: 2 [350/36450]\tLoss: 1894.8364\n",
      "Training Epoch: 2 [400/36450]\tLoss: 1889.7549\n",
      "Training Epoch: 2 [450/36450]\tLoss: 1951.6180\n",
      "Training Epoch: 2 [500/36450]\tLoss: 1960.9014\n",
      "Training Epoch: 2 [550/36450]\tLoss: 2044.8212\n",
      "Training Epoch: 2 [600/36450]\tLoss: 2012.4689\n",
      "Training Epoch: 2 [650/36450]\tLoss: 1977.4728\n",
      "Training Epoch: 2 [700/36450]\tLoss: 1893.8906\n",
      "Training Epoch: 2 [750/36450]\tLoss: 1960.3824\n",
      "Training Epoch: 2 [800/36450]\tLoss: 2014.0085\n",
      "Training Epoch: 2 [850/36450]\tLoss: 1928.6387\n",
      "Training Epoch: 2 [900/36450]\tLoss: 1866.3107\n",
      "Training Epoch: 2 [950/36450]\tLoss: 1969.6667\n",
      "Training Epoch: 2 [1000/36450]\tLoss: 1995.9885\n",
      "Training Epoch: 2 [1050/36450]\tLoss: 1883.4957\n",
      "Training Epoch: 2 [1100/36450]\tLoss: 1847.5563\n",
      "Training Epoch: 2 [1150/36450]\tLoss: 1865.9062\n",
      "Training Epoch: 2 [1200/36450]\tLoss: 1977.7252\n",
      "Training Epoch: 2 [1250/36450]\tLoss: 1918.4208\n",
      "Training Epoch: 2 [1300/36450]\tLoss: 1897.4137\n",
      "Training Epoch: 2 [1350/36450]\tLoss: 1966.4749\n",
      "Training Epoch: 2 [1400/36450]\tLoss: 1983.7645\n",
      "Training Epoch: 2 [1450/36450]\tLoss: 1902.7485\n",
      "Training Epoch: 2 [1500/36450]\tLoss: 1941.8256\n",
      "Training Epoch: 2 [1550/36450]\tLoss: 2006.6547\n",
      "Training Epoch: 2 [1600/36450]\tLoss: 1875.1029\n",
      "Training Epoch: 2 [1650/36450]\tLoss: 1973.6792\n",
      "Training Epoch: 2 [1700/36450]\tLoss: 1904.0829\n",
      "Training Epoch: 2 [1750/36450]\tLoss: 1802.6136\n",
      "Training Epoch: 2 [1800/36450]\tLoss: 1936.3486\n",
      "Training Epoch: 2 [1850/36450]\tLoss: 1894.0763\n",
      "Training Epoch: 2 [1900/36450]\tLoss: 1904.5698\n",
      "Training Epoch: 2 [1950/36450]\tLoss: 1931.0798\n",
      "Training Epoch: 2 [2000/36450]\tLoss: 1939.1921\n",
      "Training Epoch: 2 [2050/36450]\tLoss: 1898.1327\n",
      "Training Epoch: 2 [2100/36450]\tLoss: 1871.3079\n",
      "Training Epoch: 2 [2150/36450]\tLoss: 1918.4580\n",
      "Training Epoch: 2 [2200/36450]\tLoss: 1867.6315\n",
      "Training Epoch: 2 [2250/36450]\tLoss: 1871.5986\n",
      "Training Epoch: 2 [2300/36450]\tLoss: 1788.3947\n",
      "Training Epoch: 2 [2350/36450]\tLoss: 1890.7505\n",
      "Training Epoch: 2 [2400/36450]\tLoss: 1883.8870\n",
      "Training Epoch: 2 [2450/36450]\tLoss: 1894.5529\n",
      "Training Epoch: 2 [2500/36450]\tLoss: 1853.6812\n",
      "Training Epoch: 2 [2550/36450]\tLoss: 1973.2321\n",
      "Training Epoch: 2 [2600/36450]\tLoss: 1754.9771\n",
      "Training Epoch: 2 [2650/36450]\tLoss: 1827.0897\n",
      "Training Epoch: 2 [2700/36450]\tLoss: 1870.8170\n",
      "Training Epoch: 2 [2750/36450]\tLoss: 1767.4370\n",
      "Training Epoch: 2 [2800/36450]\tLoss: 1890.8889\n",
      "Training Epoch: 2 [2850/36450]\tLoss: 1833.0107\n",
      "Training Epoch: 2 [2900/36450]\tLoss: 1801.1121\n",
      "Training Epoch: 2 [2950/36450]\tLoss: 1814.3936\n",
      "Training Epoch: 2 [3000/36450]\tLoss: 1795.8837\n",
      "Training Epoch: 2 [3050/36450]\tLoss: 1768.5723\n",
      "Training Epoch: 2 [3100/36450]\tLoss: 1872.6218\n",
      "Training Epoch: 2 [3150/36450]\tLoss: 1817.3357\n",
      "Training Epoch: 2 [3200/36450]\tLoss: 1782.6891\n",
      "Training Epoch: 2 [3250/36450]\tLoss: 1855.4348\n",
      "Training Epoch: 2 [3300/36450]\tLoss: 1825.7831\n",
      "Training Epoch: 2 [3350/36450]\tLoss: 1900.7803\n",
      "Training Epoch: 2 [3400/36450]\tLoss: 1773.5811\n",
      "Training Epoch: 2 [3450/36450]\tLoss: 1828.1329\n",
      "Training Epoch: 2 [3500/36450]\tLoss: 1890.2303\n",
      "Training Epoch: 2 [3550/36450]\tLoss: 1757.0349\n",
      "Training Epoch: 2 [3600/36450]\tLoss: 1915.3682\n",
      "Training Epoch: 2 [3650/36450]\tLoss: 1815.1492\n",
      "Training Epoch: 2 [3700/36450]\tLoss: 1852.2427\n",
      "Training Epoch: 2 [3750/36450]\tLoss: 1726.2170\n",
      "Training Epoch: 2 [3800/36450]\tLoss: 1897.4807\n",
      "Training Epoch: 2 [3850/36450]\tLoss: 1884.9967\n",
      "Training Epoch: 2 [3900/36450]\tLoss: 1693.4395\n",
      "Training Epoch: 2 [3950/36450]\tLoss: 1811.8715\n",
      "Training Epoch: 2 [4000/36450]\tLoss: 1689.4301\n",
      "Training Epoch: 2 [4050/36450]\tLoss: 1804.3734\n",
      "Training Epoch: 2 [4100/36450]\tLoss: 1756.1361\n",
      "Training Epoch: 2 [4150/36450]\tLoss: 1793.2993\n",
      "Training Epoch: 2 [4200/36450]\tLoss: 1855.6998\n",
      "Training Epoch: 2 [4250/36450]\tLoss: 1786.0444\n",
      "Training Epoch: 2 [4300/36450]\tLoss: 1768.6807\n",
      "Training Epoch: 2 [4350/36450]\tLoss: 1774.7363\n",
      "Training Epoch: 2 [4400/36450]\tLoss: 1762.9598\n",
      "Training Epoch: 2 [4450/36450]\tLoss: 1718.2538\n",
      "Training Epoch: 2 [4500/36450]\tLoss: 1749.3650\n",
      "Training Epoch: 2 [4550/36450]\tLoss: 1846.4565\n",
      "Training Epoch: 2 [4600/36450]\tLoss: 1683.8229\n",
      "Training Epoch: 2 [4650/36450]\tLoss: 1672.4685\n",
      "Training Epoch: 2 [4700/36450]\tLoss: 1685.8693\n",
      "Training Epoch: 2 [4750/36450]\tLoss: 1705.6144\n",
      "Training Epoch: 2 [4800/36450]\tLoss: 1797.4009\n",
      "Training Epoch: 2 [4850/36450]\tLoss: 1760.8939\n",
      "Training Epoch: 2 [4900/36450]\tLoss: 1758.1775\n",
      "Training Epoch: 2 [4950/36450]\tLoss: 1713.4260\n",
      "Training Epoch: 2 [5000/36450]\tLoss: 1705.9160\n",
      "Training Epoch: 2 [5050/36450]\tLoss: 1731.5624\n",
      "Training Epoch: 2 [5100/36450]\tLoss: 1689.1819\n",
      "Training Epoch: 2 [5150/36450]\tLoss: 1725.9778\n",
      "Training Epoch: 2 [5200/36450]\tLoss: 1712.4493\n",
      "Training Epoch: 2 [5250/36450]\tLoss: 1752.1636\n",
      "Training Epoch: 2 [5300/36450]\tLoss: 1696.1632\n",
      "Training Epoch: 2 [5350/36450]\tLoss: 1782.1173\n",
      "Training Epoch: 2 [5400/36450]\tLoss: 1712.8765\n",
      "Training Epoch: 2 [5450/36450]\tLoss: 1753.4495\n",
      "Training Epoch: 2 [5500/36450]\tLoss: 1585.5221\n",
      "Training Epoch: 2 [5550/36450]\tLoss: 1682.4325\n",
      "Training Epoch: 2 [5600/36450]\tLoss: 1670.8444\n",
      "Training Epoch: 2 [5650/36450]\tLoss: 1680.2106\n",
      "Training Epoch: 2 [5700/36450]\tLoss: 1603.6328\n",
      "Training Epoch: 2 [5750/36450]\tLoss: 1732.2864\n",
      "Training Epoch: 2 [5800/36450]\tLoss: 1647.3793\n",
      "Training Epoch: 2 [5850/36450]\tLoss: 1766.5415\n",
      "Training Epoch: 2 [5900/36450]\tLoss: 1688.3374\n",
      "Training Epoch: 2 [5950/36450]\tLoss: 1700.3771\n",
      "Training Epoch: 2 [6000/36450]\tLoss: 1732.8235\n",
      "Training Epoch: 2 [6050/36450]\tLoss: 1704.1263\n",
      "Training Epoch: 2 [6100/36450]\tLoss: 1781.5239\n",
      "Training Epoch: 2 [6150/36450]\tLoss: 1619.9746\n",
      "Training Epoch: 2 [6200/36450]\tLoss: 1650.9071\n",
      "Training Epoch: 2 [6250/36450]\tLoss: 1627.7225\n",
      "Training Epoch: 2 [6300/36450]\tLoss: 1576.5144\n",
      "Training Epoch: 2 [6350/36450]\tLoss: 1677.0000\n",
      "Training Epoch: 2 [6400/36450]\tLoss: 1618.5916\n",
      "Training Epoch: 2 [6450/36450]\tLoss: 1658.3049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [6500/36450]\tLoss: 1597.9249\n",
      "Training Epoch: 2 [6550/36450]\tLoss: 1665.3217\n",
      "Training Epoch: 2 [6600/36450]\tLoss: 1735.9983\n",
      "Training Epoch: 2 [6650/36450]\tLoss: 1664.2919\n",
      "Training Epoch: 2 [6700/36450]\tLoss: 1746.3964\n",
      "Training Epoch: 2 [6750/36450]\tLoss: 1705.8981\n",
      "Training Epoch: 2 [6800/36450]\tLoss: 1703.5448\n",
      "Training Epoch: 2 [6850/36450]\tLoss: 1604.8625\n",
      "Training Epoch: 2 [6900/36450]\tLoss: 1630.3771\n",
      "Training Epoch: 2 [6950/36450]\tLoss: 1610.5336\n",
      "Training Epoch: 2 [7000/36450]\tLoss: 1651.8560\n",
      "Training Epoch: 2 [7050/36450]\tLoss: 1650.0710\n",
      "Training Epoch: 2 [7100/36450]\tLoss: 1626.7983\n",
      "Training Epoch: 2 [7150/36450]\tLoss: 1674.4364\n",
      "Training Epoch: 2 [7200/36450]\tLoss: 1637.6743\n",
      "Training Epoch: 2 [7250/36450]\tLoss: 1693.5756\n",
      "Training Epoch: 2 [7300/36450]\tLoss: 1621.4989\n",
      "Training Epoch: 2 [7350/36450]\tLoss: 1556.2334\n",
      "Training Epoch: 2 [7400/36450]\tLoss: 1668.1127\n",
      "Training Epoch: 2 [7450/36450]\tLoss: 1674.9774\n",
      "Training Epoch: 2 [7500/36450]\tLoss: 1669.0870\n",
      "Training Epoch: 2 [7550/36450]\tLoss: 1647.4866\n",
      "Training Epoch: 2 [7600/36450]\tLoss: 1660.4796\n",
      "Training Epoch: 2 [7650/36450]\tLoss: 1604.3495\n",
      "Training Epoch: 2 [7700/36450]\tLoss: 1653.8807\n",
      "Training Epoch: 2 [7750/36450]\tLoss: 1662.3441\n",
      "Training Epoch: 2 [7800/36450]\tLoss: 1591.6619\n",
      "Training Epoch: 2 [7850/36450]\tLoss: 1568.5873\n",
      "Training Epoch: 2 [7900/36450]\tLoss: 1658.1195\n",
      "Training Epoch: 2 [7950/36450]\tLoss: 1612.5476\n",
      "Training Epoch: 2 [8000/36450]\tLoss: 1660.1155\n",
      "Training Epoch: 2 [8050/36450]\tLoss: 1569.7805\n",
      "Training Epoch: 2 [8100/36450]\tLoss: 1683.0842\n",
      "Training Epoch: 2 [8150/36450]\tLoss: 1577.4387\n",
      "Training Epoch: 2 [8200/36450]\tLoss: 1540.1560\n",
      "Training Epoch: 2 [8250/36450]\tLoss: 1567.6904\n",
      "Training Epoch: 2 [8300/36450]\tLoss: 1512.3479\n",
      "Training Epoch: 2 [8350/36450]\tLoss: 1628.7847\n",
      "Training Epoch: 2 [8400/36450]\tLoss: 1537.5994\n",
      "Training Epoch: 2 [8450/36450]\tLoss: 1508.9059\n",
      "Training Epoch: 2 [8500/36450]\tLoss: 1576.5690\n",
      "Training Epoch: 2 [8550/36450]\tLoss: 1582.7552\n",
      "Training Epoch: 2 [8600/36450]\tLoss: 1543.8766\n",
      "Training Epoch: 2 [8650/36450]\tLoss: 1592.5529\n",
      "Training Epoch: 2 [8700/36450]\tLoss: 1565.4358\n",
      "Training Epoch: 2 [8750/36450]\tLoss: 1558.6552\n",
      "Training Epoch: 2 [8800/36450]\tLoss: 1608.5559\n",
      "Training Epoch: 2 [8850/36450]\tLoss: 1651.0575\n",
      "Training Epoch: 2 [8900/36450]\tLoss: 1640.1991\n",
      "Training Epoch: 2 [8950/36450]\tLoss: 1509.9750\n",
      "Training Epoch: 2 [9000/36450]\tLoss: 1556.2242\n",
      "Training Epoch: 2 [9050/36450]\tLoss: 1606.7402\n",
      "Training Epoch: 2 [9100/36450]\tLoss: 1510.9998\n",
      "Training Epoch: 2 [9150/36450]\tLoss: 1631.4188\n",
      "Training Epoch: 2 [9200/36450]\tLoss: 1492.5315\n",
      "Training Epoch: 2 [9250/36450]\tLoss: 1550.2097\n",
      "Training Epoch: 2 [9300/36450]\tLoss: 1517.8370\n",
      "Training Epoch: 2 [9350/36450]\tLoss: 1622.1508\n",
      "Training Epoch: 2 [9400/36450]\tLoss: 1561.6168\n",
      "Training Epoch: 2 [9450/36450]\tLoss: 1518.4645\n",
      "Training Epoch: 2 [9500/36450]\tLoss: 1511.2230\n",
      "Training Epoch: 2 [9550/36450]\tLoss: 1559.0671\n",
      "Training Epoch: 2 [9600/36450]\tLoss: 1526.6162\n",
      "Training Epoch: 2 [9650/36450]\tLoss: 1463.8906\n",
      "Training Epoch: 2 [9700/36450]\tLoss: 1504.2122\n",
      "Training Epoch: 2 [9750/36450]\tLoss: 1486.6023\n",
      "Training Epoch: 2 [9800/36450]\tLoss: 1620.9081\n",
      "Training Epoch: 2 [9850/36450]\tLoss: 1573.9557\n",
      "Training Epoch: 2 [9900/36450]\tLoss: 1540.0811\n",
      "Training Epoch: 2 [9950/36450]\tLoss: 1614.5074\n",
      "Training Epoch: 2 [10000/36450]\tLoss: 1615.8340\n",
      "Training Epoch: 2 [10050/36450]\tLoss: 1473.8138\n",
      "Training Epoch: 2 [10100/36450]\tLoss: 1536.1818\n",
      "Training Epoch: 2 [10150/36450]\tLoss: 1604.1362\n",
      "Training Epoch: 2 [10200/36450]\tLoss: 1481.3718\n",
      "Training Epoch: 2 [10250/36450]\tLoss: 1426.8069\n",
      "Training Epoch: 2 [10300/36450]\tLoss: 1467.9891\n",
      "Training Epoch: 2 [10350/36450]\tLoss: 1506.8462\n",
      "Training Epoch: 2 [10400/36450]\tLoss: 1610.6240\n",
      "Training Epoch: 2 [10450/36450]\tLoss: 1588.8707\n",
      "Training Epoch: 2 [10500/36450]\tLoss: 1563.1642\n",
      "Training Epoch: 2 [10550/36450]\tLoss: 1535.4208\n",
      "Training Epoch: 2 [10600/36450]\tLoss: 1616.4313\n",
      "Training Epoch: 2 [10650/36450]\tLoss: 1461.3486\n",
      "Training Epoch: 2 [10700/36450]\tLoss: 1443.7301\n",
      "Training Epoch: 2 [10750/36450]\tLoss: 1538.9750\n",
      "Training Epoch: 2 [10800/36450]\tLoss: 1518.9302\n",
      "Training Epoch: 2 [10850/36450]\tLoss: 1512.1381\n",
      "Training Epoch: 2 [10900/36450]\tLoss: 1535.5334\n",
      "Training Epoch: 2 [10950/36450]\tLoss: 1496.1235\n",
      "Training Epoch: 2 [11000/36450]\tLoss: 1526.6995\n",
      "Training Epoch: 2 [11050/36450]\tLoss: 1451.7714\n",
      "Training Epoch: 2 [11100/36450]\tLoss: 1416.6168\n",
      "Training Epoch: 2 [11150/36450]\tLoss: 1475.6907\n",
      "Training Epoch: 2 [11200/36450]\tLoss: 1485.0734\n",
      "Training Epoch: 2 [11250/36450]\tLoss: 1494.2562\n",
      "Training Epoch: 2 [11300/36450]\tLoss: 1463.2014\n",
      "Training Epoch: 2 [11350/36450]\tLoss: 1436.3242\n",
      "Training Epoch: 2 [11400/36450]\tLoss: 1548.0526\n",
      "Training Epoch: 2 [11450/36450]\tLoss: 1541.1818\n",
      "Training Epoch: 2 [11500/36450]\tLoss: 1520.1881\n",
      "Training Epoch: 2 [11550/36450]\tLoss: 1476.0656\n",
      "Training Epoch: 2 [11600/36450]\tLoss: 1473.8262\n",
      "Training Epoch: 2 [11650/36450]\tLoss: 1499.8561\n",
      "Training Epoch: 2 [11700/36450]\tLoss: 1460.5764\n",
      "Training Epoch: 2 [11750/36450]\tLoss: 1402.9165\n",
      "Training Epoch: 2 [11800/36450]\tLoss: 1497.6666\n",
      "Training Epoch: 2 [11850/36450]\tLoss: 1445.9636\n",
      "Training Epoch: 2 [11900/36450]\tLoss: 1390.0787\n",
      "Training Epoch: 2 [11950/36450]\tLoss: 1492.5614\n",
      "Training Epoch: 2 [12000/36450]\tLoss: 1445.6302\n",
      "Training Epoch: 2 [12050/36450]\tLoss: 1498.7198\n",
      "Training Epoch: 2 [12100/36450]\tLoss: 1421.6171\n",
      "Training Epoch: 2 [12150/36450]\tLoss: 1397.2856\n",
      "Training Epoch: 2 [12200/36450]\tLoss: 1472.5081\n",
      "Training Epoch: 2 [12250/36450]\tLoss: 1518.6179\n",
      "Training Epoch: 2 [12300/36450]\tLoss: 1450.5471\n",
      "Training Epoch: 2 [12350/36450]\tLoss: 1463.9458\n",
      "Training Epoch: 2 [12400/36450]\tLoss: 1463.2748\n",
      "Training Epoch: 2 [12450/36450]\tLoss: 1500.8727\n",
      "Training Epoch: 2 [12500/36450]\tLoss: 1495.2823\n",
      "Training Epoch: 2 [12550/36450]\tLoss: 1421.4928\n",
      "Training Epoch: 2 [12600/36450]\tLoss: 1479.1976\n",
      "Training Epoch: 2 [12650/36450]\tLoss: 1424.6779\n",
      "Training Epoch: 2 [12700/36450]\tLoss: 1471.8076\n",
      "Training Epoch: 2 [12750/36450]\tLoss: 1435.7794\n",
      "Training Epoch: 2 [12800/36450]\tLoss: 1470.3173\n",
      "Training Epoch: 2 [12850/36450]\tLoss: 1473.8728\n",
      "Training Epoch: 2 [12900/36450]\tLoss: 1497.6226\n",
      "Training Epoch: 2 [12950/36450]\tLoss: 1501.5454\n",
      "Training Epoch: 2 [13000/36450]\tLoss: 1401.2106\n",
      "Training Epoch: 2 [13050/36450]\tLoss: 1407.4224\n",
      "Training Epoch: 2 [13100/36450]\tLoss: 1418.6896\n",
      "Training Epoch: 2 [13150/36450]\tLoss: 1443.9281\n",
      "Training Epoch: 2 [13200/36450]\tLoss: 1508.9276\n",
      "Training Epoch: 2 [13250/36450]\tLoss: 1409.7094\n",
      "Training Epoch: 2 [13300/36450]\tLoss: 1396.0753\n",
      "Training Epoch: 2 [13350/36450]\tLoss: 1433.2802\n",
      "Training Epoch: 2 [13400/36450]\tLoss: 1454.0281\n",
      "Training Epoch: 2 [13450/36450]\tLoss: 1484.4352\n",
      "Training Epoch: 2 [13500/36450]\tLoss: 1502.8077\n",
      "Training Epoch: 2 [13550/36450]\tLoss: 1394.0013\n",
      "Training Epoch: 2 [13600/36450]\tLoss: 1460.9792\n",
      "Training Epoch: 2 [13650/36450]\tLoss: 1424.5852\n",
      "Training Epoch: 2 [13700/36450]\tLoss: 1528.4412\n",
      "Training Epoch: 2 [13750/36450]\tLoss: 1416.6603\n",
      "Training Epoch: 2 [13800/36450]\tLoss: 1367.6785\n",
      "Training Epoch: 2 [13850/36450]\tLoss: 1505.8600\n",
      "Training Epoch: 2 [13900/36450]\tLoss: 1479.2427\n",
      "Training Epoch: 2 [13950/36450]\tLoss: 1375.1357\n",
      "Training Epoch: 2 [14000/36450]\tLoss: 1464.5940\n",
      "Training Epoch: 2 [14050/36450]\tLoss: 1408.1973\n",
      "Training Epoch: 2 [14100/36450]\tLoss: 1428.4908\n",
      "Training Epoch: 2 [14150/36450]\tLoss: 1438.2644\n",
      "Training Epoch: 2 [14200/36450]\tLoss: 1510.4160\n",
      "Training Epoch: 2 [14250/36450]\tLoss: 1415.6632\n",
      "Training Epoch: 2 [14300/36450]\tLoss: 1419.2400\n",
      "Training Epoch: 2 [14350/36450]\tLoss: 1452.0474\n",
      "Training Epoch: 2 [14400/36450]\tLoss: 1473.0331\n",
      "Training Epoch: 2 [14450/36450]\tLoss: 1369.5114\n",
      "Training Epoch: 2 [14500/36450]\tLoss: 1392.2892\n",
      "Training Epoch: 2 [14550/36450]\tLoss: 1466.2386\n",
      "Training Epoch: 2 [14600/36450]\tLoss: 1460.4331\n",
      "Training Epoch: 2 [14650/36450]\tLoss: 1493.4830\n",
      "Training Epoch: 2 [14700/36450]\tLoss: 1465.0785\n",
      "Training Epoch: 2 [14750/36450]\tLoss: 1503.1250\n",
      "Training Epoch: 2 [14800/36450]\tLoss: 1428.3341\n",
      "Training Epoch: 2 [14850/36450]\tLoss: 1331.5930\n",
      "Training Epoch: 2 [14900/36450]\tLoss: 1386.6750\n",
      "Training Epoch: 2 [14950/36450]\tLoss: 1359.9534\n",
      "Training Epoch: 2 [15000/36450]\tLoss: 1371.1913\n",
      "Training Epoch: 2 [15050/36450]\tLoss: 1378.4895\n",
      "Training Epoch: 2 [15100/36450]\tLoss: 1388.3756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [15150/36450]\tLoss: 1393.1909\n",
      "Training Epoch: 2 [15200/36450]\tLoss: 1428.9894\n",
      "Training Epoch: 2 [15250/36450]\tLoss: 1335.8689\n",
      "Training Epoch: 2 [15300/36450]\tLoss: 1392.4762\n",
      "Training Epoch: 2 [15350/36450]\tLoss: 1313.0768\n",
      "Training Epoch: 2 [15400/36450]\tLoss: 1454.6415\n",
      "Training Epoch: 2 [15450/36450]\tLoss: 1332.2573\n",
      "Training Epoch: 2 [15500/36450]\tLoss: 1414.9877\n",
      "Training Epoch: 2 [15550/36450]\tLoss: 1342.6670\n",
      "Training Epoch: 2 [15600/36450]\tLoss: 1371.2899\n",
      "Training Epoch: 2 [15650/36450]\tLoss: 1394.1117\n",
      "Training Epoch: 2 [15700/36450]\tLoss: 1368.7781\n",
      "Training Epoch: 2 [15750/36450]\tLoss: 1354.8057\n",
      "Training Epoch: 2 [15800/36450]\tLoss: 1397.1899\n",
      "Training Epoch: 2 [15850/36450]\tLoss: 1325.7126\n",
      "Training Epoch: 2 [15900/36450]\tLoss: 1462.7761\n",
      "Training Epoch: 2 [15950/36450]\tLoss: 1354.9541\n",
      "Training Epoch: 2 [16000/36450]\tLoss: 1361.9679\n",
      "Training Epoch: 2 [16050/36450]\tLoss: 1409.2094\n",
      "Training Epoch: 2 [16100/36450]\tLoss: 1406.9423\n",
      "Training Epoch: 2 [16150/36450]\tLoss: 1390.7472\n",
      "Training Epoch: 2 [16200/36450]\tLoss: 1376.2161\n",
      "Training Epoch: 2 [16250/36450]\tLoss: 1358.7859\n",
      "Training Epoch: 2 [16300/36450]\tLoss: 1327.6572\n",
      "Training Epoch: 2 [16350/36450]\tLoss: 1337.2395\n",
      "Training Epoch: 2 [16400/36450]\tLoss: 1421.3275\n",
      "Training Epoch: 2 [16450/36450]\tLoss: 1416.4092\n",
      "Training Epoch: 2 [16500/36450]\tLoss: 1321.1538\n",
      "Training Epoch: 2 [16550/36450]\tLoss: 1382.3835\n",
      "Training Epoch: 2 [16600/36450]\tLoss: 1392.8979\n",
      "Training Epoch: 2 [16650/36450]\tLoss: 1402.8794\n",
      "Training Epoch: 2 [16700/36450]\tLoss: 1399.5211\n",
      "Training Epoch: 2 [16750/36450]\tLoss: 1385.8229\n",
      "Training Epoch: 2 [16800/36450]\tLoss: 1351.2963\n",
      "Training Epoch: 2 [16850/36450]\tLoss: 1394.4399\n",
      "Training Epoch: 2 [16900/36450]\tLoss: 1382.5198\n",
      "Training Epoch: 2 [16950/36450]\tLoss: 1292.0239\n",
      "Training Epoch: 2 [17000/36450]\tLoss: 1371.8511\n",
      "Training Epoch: 2 [17050/36450]\tLoss: 1316.9078\n",
      "Training Epoch: 2 [17100/36450]\tLoss: 1337.1426\n",
      "Training Epoch: 2 [17150/36450]\tLoss: 1432.4407\n",
      "Training Epoch: 2 [17200/36450]\tLoss: 1402.6929\n",
      "Training Epoch: 2 [17250/36450]\tLoss: 1365.5487\n",
      "Training Epoch: 2 [17300/36450]\tLoss: 1348.2157\n",
      "Training Epoch: 2 [17350/36450]\tLoss: 1367.6896\n",
      "Training Epoch: 2 [17400/36450]\tLoss: 1380.9751\n",
      "Training Epoch: 2 [17450/36450]\tLoss: 1275.6708\n",
      "Training Epoch: 2 [17500/36450]\tLoss: 1306.4559\n",
      "Training Epoch: 2 [17550/36450]\tLoss: 1369.6837\n",
      "Training Epoch: 2 [17600/36450]\tLoss: 1262.9874\n",
      "Training Epoch: 2 [17650/36450]\tLoss: 1373.7030\n",
      "Training Epoch: 2 [17700/36450]\tLoss: 1388.2802\n",
      "Training Epoch: 2 [17750/36450]\tLoss: 1373.1565\n",
      "Training Epoch: 2 [17800/36450]\tLoss: 1390.4025\n",
      "Training Epoch: 2 [17850/36450]\tLoss: 1374.2921\n",
      "Training Epoch: 2 [17900/36450]\tLoss: 1368.7322\n",
      "Training Epoch: 2 [17950/36450]\tLoss: 1335.3297\n",
      "Training Epoch: 2 [18000/36450]\tLoss: 1338.0730\n",
      "Training Epoch: 2 [18050/36450]\tLoss: 1325.5978\n",
      "Training Epoch: 2 [18100/36450]\tLoss: 1298.0261\n",
      "Training Epoch: 2 [18150/36450]\tLoss: 1394.2294\n",
      "Training Epoch: 2 [18200/36450]\tLoss: 1344.5809\n",
      "Training Epoch: 2 [18250/36450]\tLoss: 1353.5385\n",
      "Training Epoch: 2 [18300/36450]\tLoss: 1326.7701\n",
      "Training Epoch: 2 [18350/36450]\tLoss: 1338.4623\n",
      "Training Epoch: 2 [18400/36450]\tLoss: 1361.6776\n",
      "Training Epoch: 2 [18450/36450]\tLoss: 1332.9751\n",
      "Training Epoch: 2 [18500/36450]\tLoss: 1334.9714\n",
      "Training Epoch: 2 [18550/36450]\tLoss: 1296.6162\n",
      "Training Epoch: 2 [18600/36450]\tLoss: 1294.8395\n",
      "Training Epoch: 2 [18650/36450]\tLoss: 1380.6715\n",
      "Training Epoch: 2 [18700/36450]\tLoss: 1311.7708\n",
      "Training Epoch: 2 [18750/36450]\tLoss: 1288.8700\n",
      "Training Epoch: 2 [18800/36450]\tLoss: 1302.6614\n",
      "Training Epoch: 2 [18850/36450]\tLoss: 1309.9982\n",
      "Training Epoch: 2 [18900/36450]\tLoss: 1372.9229\n",
      "Training Epoch: 2 [18950/36450]\tLoss: 1341.1449\n",
      "Training Epoch: 2 [19000/36450]\tLoss: 1298.8156\n",
      "Training Epoch: 2 [19050/36450]\tLoss: 1389.1711\n",
      "Training Epoch: 2 [19100/36450]\tLoss: 1265.5664\n",
      "Training Epoch: 2 [19150/36450]\tLoss: 1276.9146\n",
      "Training Epoch: 2 [19200/36450]\tLoss: 1374.4048\n",
      "Training Epoch: 2 [19250/36450]\tLoss: 1329.5249\n",
      "Training Epoch: 2 [19300/36450]\tLoss: 1334.0835\n",
      "Training Epoch: 2 [19350/36450]\tLoss: 1291.4730\n",
      "Training Epoch: 2 [19400/36450]\tLoss: 1338.4575\n",
      "Training Epoch: 2 [19450/36450]\tLoss: 1276.3954\n",
      "Training Epoch: 2 [19500/36450]\tLoss: 1263.1704\n",
      "Training Epoch: 2 [19550/36450]\tLoss: 1303.5260\n",
      "Training Epoch: 2 [19600/36450]\tLoss: 1244.0946\n",
      "Training Epoch: 2 [19650/36450]\tLoss: 1280.9438\n",
      "Training Epoch: 2 [19700/36450]\tLoss: 1304.4081\n",
      "Training Epoch: 2 [19750/36450]\tLoss: 1350.5206\n",
      "Training Epoch: 2 [19800/36450]\tLoss: 1329.3861\n",
      "Training Epoch: 2 [19850/36450]\tLoss: 1397.0372\n",
      "Training Epoch: 2 [19900/36450]\tLoss: 1332.6044\n",
      "Training Epoch: 2 [19950/36450]\tLoss: 1283.9912\n",
      "Training Epoch: 2 [20000/36450]\tLoss: 1276.9014\n",
      "Training Epoch: 2 [20050/36450]\tLoss: 1370.5483\n",
      "Training Epoch: 2 [20100/36450]\tLoss: 1318.1425\n",
      "Training Epoch: 2 [20150/36450]\tLoss: 1321.3401\n",
      "Training Epoch: 2 [20200/36450]\tLoss: 1266.4943\n",
      "Training Epoch: 2 [20250/36450]\tLoss: 1363.9933\n",
      "Training Epoch: 2 [20300/36450]\tLoss: 1281.9520\n",
      "Training Epoch: 2 [20350/36450]\tLoss: 1247.3680\n",
      "Training Epoch: 2 [20400/36450]\tLoss: 1262.1237\n",
      "Training Epoch: 2 [20450/36450]\tLoss: 1283.7815\n",
      "Training Epoch: 2 [20500/36450]\tLoss: 1317.3873\n",
      "Training Epoch: 2 [20550/36450]\tLoss: 1304.9048\n",
      "Training Epoch: 2 [20600/36450]\tLoss: 1274.5582\n",
      "Training Epoch: 2 [20650/36450]\tLoss: 1330.0468\n",
      "Training Epoch: 2 [20700/36450]\tLoss: 1316.9669\n",
      "Training Epoch: 2 [20750/36450]\tLoss: 1340.0050\n",
      "Training Epoch: 2 [20800/36450]\tLoss: 1298.3291\n",
      "Training Epoch: 2 [20850/36450]\tLoss: 1319.2937\n",
      "Training Epoch: 2 [20900/36450]\tLoss: 1275.3783\n",
      "Training Epoch: 2 [20950/36450]\tLoss: 1397.6434\n",
      "Training Epoch: 2 [21000/36450]\tLoss: 1363.8995\n",
      "Training Epoch: 2 [21050/36450]\tLoss: 1292.6129\n",
      "Training Epoch: 2 [21100/36450]\tLoss: 1285.3093\n",
      "Training Epoch: 2 [21150/36450]\tLoss: 1305.5597\n",
      "Training Epoch: 2 [21200/36450]\tLoss: 1378.0555\n",
      "Training Epoch: 2 [21250/36450]\tLoss: 1316.0209\n",
      "Training Epoch: 2 [21300/36450]\tLoss: 1293.9298\n",
      "Training Epoch: 2 [21350/36450]\tLoss: 1309.2566\n",
      "Training Epoch: 2 [21400/36450]\tLoss: 1292.1063\n",
      "Training Epoch: 2 [21450/36450]\tLoss: 1325.5464\n",
      "Training Epoch: 2 [21500/36450]\tLoss: 1295.2107\n",
      "Training Epoch: 2 [21550/36450]\tLoss: 1372.2477\n",
      "Training Epoch: 2 [21600/36450]\tLoss: 1336.9912\n",
      "Training Epoch: 2 [21650/36450]\tLoss: 1307.2008\n",
      "Training Epoch: 2 [21700/36450]\tLoss: 1335.6831\n",
      "Training Epoch: 2 [21750/36450]\tLoss: 1327.5714\n",
      "Training Epoch: 2 [21800/36450]\tLoss: 1286.5983\n",
      "Training Epoch: 2 [21850/36450]\tLoss: 1317.0707\n",
      "Training Epoch: 2 [21900/36450]\tLoss: 1276.3539\n",
      "Training Epoch: 2 [21950/36450]\tLoss: 1342.1467\n",
      "Training Epoch: 2 [22000/36450]\tLoss: 1288.2158\n",
      "Training Epoch: 2 [22050/36450]\tLoss: 1236.1974\n",
      "Training Epoch: 2 [22100/36450]\tLoss: 1275.2905\n",
      "Training Epoch: 2 [22150/36450]\tLoss: 1240.3911\n",
      "Training Epoch: 2 [22200/36450]\tLoss: 1341.3696\n",
      "Training Epoch: 2 [22250/36450]\tLoss: 1228.9524\n",
      "Training Epoch: 2 [22300/36450]\tLoss: 1265.8428\n",
      "Training Epoch: 2 [22350/36450]\tLoss: 1259.0660\n",
      "Training Epoch: 2 [22400/36450]\tLoss: 1329.6173\n",
      "Training Epoch: 2 [22450/36450]\tLoss: 1308.0879\n",
      "Training Epoch: 2 [22500/36450]\tLoss: 1228.9855\n",
      "Training Epoch: 2 [22550/36450]\tLoss: 1274.7375\n",
      "Training Epoch: 2 [22600/36450]\tLoss: 1256.9808\n",
      "Training Epoch: 2 [22650/36450]\tLoss: 1224.3862\n",
      "Training Epoch: 2 [22700/36450]\tLoss: 1279.0693\n",
      "Training Epoch: 2 [22750/36450]\tLoss: 1276.0615\n",
      "Training Epoch: 2 [22800/36450]\tLoss: 1313.4951\n",
      "Training Epoch: 2 [22850/36450]\tLoss: 1305.5166\n",
      "Training Epoch: 2 [22900/36450]\tLoss: 1242.5681\n",
      "Training Epoch: 2 [22950/36450]\tLoss: 1282.3251\n",
      "Training Epoch: 2 [23000/36450]\tLoss: 1272.8300\n",
      "Training Epoch: 2 [23050/36450]\tLoss: 1270.5515\n",
      "Training Epoch: 2 [23100/36450]\tLoss: 1181.2983\n",
      "Training Epoch: 2 [23150/36450]\tLoss: 1218.2229\n",
      "Training Epoch: 2 [23200/36450]\tLoss: 1250.2587\n",
      "Training Epoch: 2 [23250/36450]\tLoss: 1260.2250\n",
      "Training Epoch: 2 [23300/36450]\tLoss: 1211.9180\n",
      "Training Epoch: 2 [23350/36450]\tLoss: 1284.4984\n",
      "Training Epoch: 2 [23400/36450]\tLoss: 1287.4474\n",
      "Training Epoch: 2 [23450/36450]\tLoss: 1171.5981\n",
      "Training Epoch: 2 [23500/36450]\tLoss: 1304.3173\n",
      "Training Epoch: 2 [23550/36450]\tLoss: 1232.3473\n",
      "Training Epoch: 2 [23600/36450]\tLoss: 1192.5292\n",
      "Training Epoch: 2 [23650/36450]\tLoss: 1225.7355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [23700/36450]\tLoss: 1257.2169\n",
      "Training Epoch: 2 [23750/36450]\tLoss: 1213.5249\n",
      "Training Epoch: 2 [23800/36450]\tLoss: 1316.4655\n",
      "Training Epoch: 2 [23850/36450]\tLoss: 1312.1852\n",
      "Training Epoch: 2 [23900/36450]\tLoss: 1225.7828\n",
      "Training Epoch: 2 [23950/36450]\tLoss: 1212.2867\n",
      "Training Epoch: 2 [24000/36450]\tLoss: 1209.4746\n",
      "Training Epoch: 2 [24050/36450]\tLoss: 1229.1759\n",
      "Training Epoch: 2 [24100/36450]\tLoss: 1303.2100\n",
      "Training Epoch: 2 [24150/36450]\tLoss: 1191.3411\n",
      "Training Epoch: 2 [24200/36450]\tLoss: 1231.3727\n",
      "Training Epoch: 2 [24250/36450]\tLoss: 1331.5757\n",
      "Training Epoch: 2 [24300/36450]\tLoss: 1269.0126\n",
      "Training Epoch: 2 [24350/36450]\tLoss: 1170.3221\n",
      "Training Epoch: 2 [24400/36450]\tLoss: 1206.7223\n",
      "Training Epoch: 2 [24450/36450]\tLoss: 1181.8696\n",
      "Training Epoch: 2 [24500/36450]\tLoss: 1315.1615\n",
      "Training Epoch: 2 [24550/36450]\tLoss: 1302.0549\n",
      "Training Epoch: 2 [24600/36450]\tLoss: 1302.5245\n",
      "Training Epoch: 2 [24650/36450]\tLoss: 1217.7139\n",
      "Training Epoch: 2 [24700/36450]\tLoss: 1221.1414\n",
      "Training Epoch: 2 [24750/36450]\tLoss: 1277.1174\n",
      "Training Epoch: 2 [24800/36450]\tLoss: 1195.2855\n",
      "Training Epoch: 2 [24850/36450]\tLoss: 1228.7051\n",
      "Training Epoch: 2 [24900/36450]\tLoss: 1244.2649\n",
      "Training Epoch: 2 [24950/36450]\tLoss: 1290.2964\n",
      "Training Epoch: 2 [25000/36450]\tLoss: 1195.4204\n",
      "Training Epoch: 2 [25050/36450]\tLoss: 1255.3431\n",
      "Training Epoch: 2 [25100/36450]\tLoss: 1319.6962\n",
      "Training Epoch: 2 [25150/36450]\tLoss: 1249.4731\n",
      "Training Epoch: 2 [25200/36450]\tLoss: 1174.7682\n",
      "Training Epoch: 2 [25250/36450]\tLoss: 1220.2047\n",
      "Training Epoch: 2 [25300/36450]\tLoss: 1238.3153\n",
      "Training Epoch: 2 [25350/36450]\tLoss: 1201.5399\n",
      "Training Epoch: 2 [25400/36450]\tLoss: 1234.5098\n",
      "Training Epoch: 2 [25450/36450]\tLoss: 1234.4319\n",
      "Training Epoch: 2 [25500/36450]\tLoss: 1188.1439\n",
      "Training Epoch: 2 [25550/36450]\tLoss: 1251.1387\n",
      "Training Epoch: 2 [25600/36450]\tLoss: 1192.7375\n",
      "Training Epoch: 2 [25650/36450]\tLoss: 1239.7521\n",
      "Training Epoch: 2 [25700/36450]\tLoss: 1177.1610\n",
      "Training Epoch: 2 [25750/36450]\tLoss: 1243.5999\n",
      "Training Epoch: 2 [25800/36450]\tLoss: 1217.0796\n",
      "Training Epoch: 2 [25850/36450]\tLoss: 1244.8699\n",
      "Training Epoch: 2 [25900/36450]\tLoss: 1191.4181\n",
      "Training Epoch: 2 [25950/36450]\tLoss: 1268.1880\n",
      "Training Epoch: 2 [26000/36450]\tLoss: 1249.3199\n",
      "Training Epoch: 2 [26050/36450]\tLoss: 1249.1697\n",
      "Training Epoch: 2 [26100/36450]\tLoss: 1281.9633\n",
      "Training Epoch: 2 [26150/36450]\tLoss: 1153.7196\n",
      "Training Epoch: 2 [26200/36450]\tLoss: 1262.2924\n",
      "Training Epoch: 2 [26250/36450]\tLoss: 1202.5638\n",
      "Training Epoch: 2 [26300/36450]\tLoss: 1230.0612\n",
      "Training Epoch: 2 [26350/36450]\tLoss: 1237.6111\n",
      "Training Epoch: 2 [26400/36450]\tLoss: 1213.0702\n",
      "Training Epoch: 2 [26450/36450]\tLoss: 1187.4675\n",
      "Training Epoch: 2 [26500/36450]\tLoss: 1249.5201\n",
      "Training Epoch: 2 [26550/36450]\tLoss: 1233.4105\n",
      "Training Epoch: 2 [26600/36450]\tLoss: 1230.9364\n",
      "Training Epoch: 2 [26650/36450]\tLoss: 1226.7623\n",
      "Training Epoch: 2 [26700/36450]\tLoss: 1267.7336\n",
      "Training Epoch: 2 [26750/36450]\tLoss: 1234.4487\n",
      "Training Epoch: 2 [26800/36450]\tLoss: 1299.9216\n",
      "Training Epoch: 2 [26850/36450]\tLoss: 1314.6738\n",
      "Training Epoch: 2 [26900/36450]\tLoss: 1319.5363\n",
      "Training Epoch: 2 [26950/36450]\tLoss: 1318.6187\n",
      "Training Epoch: 2 [27000/36450]\tLoss: 1234.5948\n",
      "Training Epoch: 2 [27050/36450]\tLoss: 1192.3518\n",
      "Training Epoch: 2 [27100/36450]\tLoss: 1189.3590\n",
      "Training Epoch: 2 [27150/36450]\tLoss: 1221.1311\n",
      "Training Epoch: 2 [27200/36450]\tLoss: 1235.1982\n",
      "Training Epoch: 2 [27250/36450]\tLoss: 1196.2239\n",
      "Training Epoch: 2 [27300/36450]\tLoss: 1222.6077\n",
      "Training Epoch: 2 [27350/36450]\tLoss: 1279.2870\n",
      "Training Epoch: 2 [27400/36450]\tLoss: 1251.4065\n",
      "Training Epoch: 2 [27450/36450]\tLoss: 1159.7296\n",
      "Training Epoch: 2 [27500/36450]\tLoss: 1233.2841\n",
      "Training Epoch: 2 [27550/36450]\tLoss: 1248.7166\n",
      "Training Epoch: 2 [27600/36450]\tLoss: 1267.3682\n",
      "Training Epoch: 2 [27650/36450]\tLoss: 1181.6367\n",
      "Training Epoch: 2 [27700/36450]\tLoss: 1207.3717\n",
      "Training Epoch: 2 [27750/36450]\tLoss: 1226.1211\n",
      "Training Epoch: 2 [27800/36450]\tLoss: 1241.5642\n",
      "Training Epoch: 2 [27850/36450]\tLoss: 1246.0532\n",
      "Training Epoch: 2 [27900/36450]\tLoss: 1195.2758\n",
      "Training Epoch: 2 [27950/36450]\tLoss: 1237.2139\n",
      "Training Epoch: 2 [28000/36450]\tLoss: 1230.0765\n",
      "Training Epoch: 2 [28050/36450]\tLoss: 1150.0626\n",
      "Training Epoch: 2 [28100/36450]\tLoss: 1239.8623\n",
      "Training Epoch: 2 [28150/36450]\tLoss: 1185.2919\n",
      "Training Epoch: 2 [28200/36450]\tLoss: 1212.2787\n",
      "Training Epoch: 2 [28250/36450]\tLoss: 1200.1254\n",
      "Training Epoch: 2 [28300/36450]\tLoss: 1232.4095\n",
      "Training Epoch: 2 [28350/36450]\tLoss: 1164.4856\n",
      "Training Epoch: 2 [28400/36450]\tLoss: 1214.1943\n",
      "Training Epoch: 2 [28450/36450]\tLoss: 1189.5231\n",
      "Training Epoch: 2 [28500/36450]\tLoss: 1210.2125\n",
      "Training Epoch: 2 [28550/36450]\tLoss: 1160.3773\n",
      "Training Epoch: 2 [28600/36450]\tLoss: 1151.3071\n",
      "Training Epoch: 2 [28650/36450]\tLoss: 1158.7706\n",
      "Training Epoch: 2 [28700/36450]\tLoss: 1191.5242\n",
      "Training Epoch: 2 [28750/36450]\tLoss: 1154.3760\n",
      "Training Epoch: 2 [28800/36450]\tLoss: 1252.7462\n",
      "Training Epoch: 2 [28850/36450]\tLoss: 1246.6011\n",
      "Training Epoch: 2 [28900/36450]\tLoss: 1171.4204\n",
      "Training Epoch: 2 [28950/36450]\tLoss: 1163.8494\n",
      "Training Epoch: 2 [29000/36450]\tLoss: 1214.6932\n",
      "Training Epoch: 2 [29050/36450]\tLoss: 1186.3826\n",
      "Training Epoch: 2 [29100/36450]\tLoss: 1231.2174\n",
      "Training Epoch: 2 [29150/36450]\tLoss: 1193.9080\n",
      "Training Epoch: 2 [29200/36450]\tLoss: 1207.8539\n",
      "Training Epoch: 2 [29250/36450]\tLoss: 1275.8105\n",
      "Training Epoch: 2 [29300/36450]\tLoss: 1137.1183\n",
      "Training Epoch: 2 [29350/36450]\tLoss: 1171.7202\n",
      "Training Epoch: 2 [29400/36450]\tLoss: 1165.0757\n",
      "Training Epoch: 2 [29450/36450]\tLoss: 1176.8646\n",
      "Training Epoch: 2 [29500/36450]\tLoss: 1193.1438\n",
      "Training Epoch: 2 [29550/36450]\tLoss: 1177.3245\n",
      "Training Epoch: 2 [29600/36450]\tLoss: 1203.5957\n",
      "Training Epoch: 2 [29650/36450]\tLoss: 1174.2043\n",
      "Training Epoch: 2 [29700/36450]\tLoss: 1208.4523\n",
      "Training Epoch: 2 [29750/36450]\tLoss: 1200.8351\n",
      "Training Epoch: 2 [29800/36450]\tLoss: 1175.3732\n",
      "Training Epoch: 2 [29850/36450]\tLoss: 1158.5382\n",
      "Training Epoch: 2 [29900/36450]\tLoss: 1204.4021\n",
      "Training Epoch: 2 [29950/36450]\tLoss: 1127.1246\n",
      "Training Epoch: 2 [30000/36450]\tLoss: 1143.6066\n",
      "Training Epoch: 2 [30050/36450]\tLoss: 1152.2435\n",
      "Training Epoch: 2 [30100/36450]\tLoss: 1166.2657\n",
      "Training Epoch: 2 [30150/36450]\tLoss: 1223.5938\n",
      "Training Epoch: 2 [30200/36450]\tLoss: 1165.6696\n",
      "Training Epoch: 2 [30250/36450]\tLoss: 1231.0885\n",
      "Training Epoch: 2 [30300/36450]\tLoss: 1185.3022\n",
      "Training Epoch: 2 [30350/36450]\tLoss: 1195.4650\n",
      "Training Epoch: 2 [30400/36450]\tLoss: 1183.1348\n",
      "Training Epoch: 2 [30450/36450]\tLoss: 1132.0177\n",
      "Training Epoch: 2 [30500/36450]\tLoss: 1145.6506\n",
      "Training Epoch: 2 [30550/36450]\tLoss: 1186.8873\n",
      "Training Epoch: 2 [30600/36450]\tLoss: 1229.3926\n",
      "Training Epoch: 2 [30650/36450]\tLoss: 1155.5809\n",
      "Training Epoch: 2 [30700/36450]\tLoss: 1178.4750\n",
      "Training Epoch: 2 [30750/36450]\tLoss: 1254.8920\n",
      "Training Epoch: 2 [30800/36450]\tLoss: 1181.4326\n",
      "Training Epoch: 2 [30850/36450]\tLoss: 1171.3110\n",
      "Training Epoch: 2 [30900/36450]\tLoss: 1176.7704\n",
      "Training Epoch: 2 [30950/36450]\tLoss: 1294.9108\n",
      "Training Epoch: 2 [31000/36450]\tLoss: 1213.3740\n",
      "Training Epoch: 2 [31050/36450]\tLoss: 1122.5410\n",
      "Training Epoch: 2 [31100/36450]\tLoss: 1188.7307\n",
      "Training Epoch: 2 [31150/36450]\tLoss: 1166.5961\n",
      "Training Epoch: 2 [31200/36450]\tLoss: 1185.7712\n",
      "Training Epoch: 2 [31250/36450]\tLoss: 1190.0696\n",
      "Training Epoch: 2 [31300/36450]\tLoss: 1135.4199\n",
      "Training Epoch: 2 [31350/36450]\tLoss: 1206.0695\n",
      "Training Epoch: 2 [31400/36450]\tLoss: 1123.7213\n",
      "Training Epoch: 2 [31450/36450]\tLoss: 1174.3778\n",
      "Training Epoch: 2 [31500/36450]\tLoss: 1134.7660\n",
      "Training Epoch: 2 [31550/36450]\tLoss: 1157.4504\n",
      "Training Epoch: 2 [31600/36450]\tLoss: 1180.2412\n",
      "Training Epoch: 2 [31650/36450]\tLoss: 1182.4670\n",
      "Training Epoch: 2 [31700/36450]\tLoss: 1131.0854\n",
      "Training Epoch: 2 [31750/36450]\tLoss: 1238.9211\n",
      "Training Epoch: 2 [31800/36450]\tLoss: 1196.1656\n",
      "Training Epoch: 2 [31850/36450]\tLoss: 1178.1816\n",
      "Training Epoch: 2 [31900/36450]\tLoss: 1153.6609\n",
      "Training Epoch: 2 [31950/36450]\tLoss: 1153.8341\n",
      "Training Epoch: 2 [32000/36450]\tLoss: 1153.7689\n",
      "Training Epoch: 2 [32050/36450]\tLoss: 1108.2318\n",
      "Training Epoch: 2 [32100/36450]\tLoss: 1190.2314\n",
      "Training Epoch: 2 [32150/36450]\tLoss: 1142.8003\n",
      "Training Epoch: 2 [32200/36450]\tLoss: 1167.8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [32250/36450]\tLoss: 1132.9158\n",
      "Training Epoch: 2 [32300/36450]\tLoss: 1156.5994\n",
      "Training Epoch: 2 [32350/36450]\tLoss: 1103.4532\n",
      "Training Epoch: 2 [32400/36450]\tLoss: 1157.2891\n",
      "Training Epoch: 2 [32450/36450]\tLoss: 1154.4247\n",
      "Training Epoch: 2 [32500/36450]\tLoss: 1132.7827\n",
      "Training Epoch: 2 [32550/36450]\tLoss: 1133.5341\n",
      "Training Epoch: 2 [32600/36450]\tLoss: 1166.1472\n",
      "Training Epoch: 2 [32650/36450]\tLoss: 1127.7705\n",
      "Training Epoch: 2 [32700/36450]\tLoss: 1152.1415\n",
      "Training Epoch: 2 [32750/36450]\tLoss: 1235.1061\n",
      "Training Epoch: 2 [32800/36450]\tLoss: 1171.6472\n",
      "Training Epoch: 2 [32850/36450]\tLoss: 1088.8889\n",
      "Training Epoch: 2 [32900/36450]\tLoss: 1093.9426\n",
      "Training Epoch: 2 [32950/36450]\tLoss: 1129.4504\n",
      "Training Epoch: 2 [33000/36450]\tLoss: 1160.2360\n",
      "Training Epoch: 2 [33050/36450]\tLoss: 1138.5181\n",
      "Training Epoch: 2 [33100/36450]\tLoss: 1142.9564\n",
      "Training Epoch: 2 [33150/36450]\tLoss: 1143.4716\n",
      "Training Epoch: 2 [33200/36450]\tLoss: 1173.6261\n",
      "Training Epoch: 2 [33250/36450]\tLoss: 1207.7922\n",
      "Training Epoch: 2 [33300/36450]\tLoss: 1188.6726\n",
      "Training Epoch: 2 [33350/36450]\tLoss: 1123.6938\n",
      "Training Epoch: 2 [33400/36450]\tLoss: 1137.0966\n",
      "Training Epoch: 2 [33450/36450]\tLoss: 1182.6367\n",
      "Training Epoch: 2 [33500/36450]\tLoss: 1146.6034\n",
      "Training Epoch: 2 [33550/36450]\tLoss: 1179.5386\n",
      "Training Epoch: 2 [33600/36450]\tLoss: 1225.2598\n",
      "Training Epoch: 2 [33650/36450]\tLoss: 1088.1902\n",
      "Training Epoch: 2 [33700/36450]\tLoss: 1082.9259\n",
      "Training Epoch: 2 [33750/36450]\tLoss: 1134.9100\n",
      "Training Epoch: 2 [33800/36450]\tLoss: 1116.1528\n",
      "Training Epoch: 2 [33850/36450]\tLoss: 1123.6254\n",
      "Training Epoch: 2 [33900/36450]\tLoss: 1159.1392\n",
      "Training Epoch: 2 [33950/36450]\tLoss: 1150.8267\n",
      "Training Epoch: 2 [34000/36450]\tLoss: 1190.8381\n",
      "Training Epoch: 2 [34050/36450]\tLoss: 1155.2426\n",
      "Training Epoch: 2 [34100/36450]\tLoss: 1176.8029\n",
      "Training Epoch: 2 [34150/36450]\tLoss: 1158.2233\n",
      "Training Epoch: 2 [34200/36450]\tLoss: 1164.1982\n",
      "Training Epoch: 2 [34250/36450]\tLoss: 1100.3708\n",
      "Training Epoch: 2 [34300/36450]\tLoss: 1027.0006\n",
      "Training Epoch: 2 [34350/36450]\tLoss: 1095.3455\n",
      "Training Epoch: 2 [34400/36450]\tLoss: 1140.3047\n",
      "Training Epoch: 2 [34450/36450]\tLoss: 1144.2162\n",
      "Training Epoch: 2 [34500/36450]\tLoss: 1188.7146\n",
      "Training Epoch: 2 [34550/36450]\tLoss: 1144.2385\n",
      "Training Epoch: 2 [34600/36450]\tLoss: 1121.7363\n",
      "Training Epoch: 2 [34650/36450]\tLoss: 1172.1959\n",
      "Training Epoch: 2 [34700/36450]\tLoss: 1121.1063\n",
      "Training Epoch: 2 [34750/36450]\tLoss: 1206.2782\n",
      "Training Epoch: 2 [34800/36450]\tLoss: 1169.6079\n",
      "Training Epoch: 2 [34850/36450]\tLoss: 1165.5568\n",
      "Training Epoch: 2 [34900/36450]\tLoss: 1184.7056\n",
      "Training Epoch: 2 [34950/36450]\tLoss: 1113.1615\n",
      "Training Epoch: 2 [35000/36450]\tLoss: 1125.6692\n",
      "Training Epoch: 2 [35050/36450]\tLoss: 1129.9208\n",
      "Training Epoch: 2 [35100/36450]\tLoss: 1165.0356\n",
      "Training Epoch: 2 [35150/36450]\tLoss: 1108.9487\n",
      "Training Epoch: 2 [35200/36450]\tLoss: 1113.2731\n",
      "Training Epoch: 2 [35250/36450]\tLoss: 1100.5880\n",
      "Training Epoch: 2 [35300/36450]\tLoss: 1138.3890\n",
      "Training Epoch: 2 [35350/36450]\tLoss: 1150.6863\n",
      "Training Epoch: 2 [35400/36450]\tLoss: 1091.1608\n",
      "Training Epoch: 2 [35450/36450]\tLoss: 1185.5118\n",
      "Training Epoch: 2 [35500/36450]\tLoss: 1112.9087\n",
      "Training Epoch: 2 [35550/36450]\tLoss: 1144.3412\n",
      "Training Epoch: 2 [35600/36450]\tLoss: 1190.9600\n",
      "Training Epoch: 2 [35650/36450]\tLoss: 1068.1577\n",
      "Training Epoch: 2 [35700/36450]\tLoss: 1119.0485\n",
      "Training Epoch: 2 [35750/36450]\tLoss: 1182.1604\n",
      "Training Epoch: 2 [35800/36450]\tLoss: 1140.3020\n",
      "Training Epoch: 2 [35850/36450]\tLoss: 1126.6927\n",
      "Training Epoch: 2 [35900/36450]\tLoss: 1119.7614\n",
      "Training Epoch: 2 [35950/36450]\tLoss: 1091.6055\n",
      "Training Epoch: 2 [36000/36450]\tLoss: 1164.2731\n",
      "Training Epoch: 2 [36050/36450]\tLoss: 1165.3643\n",
      "Training Epoch: 2 [36100/36450]\tLoss: 1159.0122\n",
      "Training Epoch: 2 [36150/36450]\tLoss: 1113.3737\n",
      "Training Epoch: 2 [36200/36450]\tLoss: 1092.1536\n",
      "Training Epoch: 2 [36250/36450]\tLoss: 1107.2755\n",
      "Training Epoch: 2 [36300/36450]\tLoss: 1111.3575\n",
      "Training Epoch: 2 [36350/36450]\tLoss: 1176.7976\n",
      "Training Epoch: 2 [36400/36450]\tLoss: 1119.2115\n",
      "Training Epoch: 2 [36450/36450]\tLoss: 1174.0060\n",
      "Training Epoch: 2 [4050/4050]\tLoss: 570.2226\n",
      "Training Epoch: 3 [50/36450]\tLoss: 1104.4429\n",
      "Training Epoch: 3 [100/36450]\tLoss: 1130.6783\n",
      "Training Epoch: 3 [150/36450]\tLoss: 1106.5565\n",
      "Training Epoch: 3 [200/36450]\tLoss: 1128.7659\n",
      "Training Epoch: 3 [250/36450]\tLoss: 1096.2649\n",
      "Training Epoch: 3 [300/36450]\tLoss: 1053.1034\n",
      "Training Epoch: 3 [350/36450]\tLoss: 1130.1543\n",
      "Training Epoch: 3 [400/36450]\tLoss: 1126.0752\n",
      "Training Epoch: 3 [450/36450]\tLoss: 1108.6426\n",
      "Training Epoch: 3 [500/36450]\tLoss: 1074.5922\n",
      "Training Epoch: 3 [550/36450]\tLoss: 1107.2799\n",
      "Training Epoch: 3 [600/36450]\tLoss: 1057.6359\n",
      "Training Epoch: 3 [650/36450]\tLoss: 1151.3799\n",
      "Training Epoch: 3 [700/36450]\tLoss: 1159.2924\n",
      "Training Epoch: 3 [750/36450]\tLoss: 1136.0145\n",
      "Training Epoch: 3 [800/36450]\tLoss: 1133.7559\n",
      "Training Epoch: 3 [850/36450]\tLoss: 1177.0112\n",
      "Training Epoch: 3 [900/36450]\tLoss: 1153.6550\n",
      "Training Epoch: 3 [950/36450]\tLoss: 1129.6755\n",
      "Training Epoch: 3 [1000/36450]\tLoss: 1090.4810\n",
      "Training Epoch: 3 [1050/36450]\tLoss: 1099.0454\n",
      "Training Epoch: 3 [1100/36450]\tLoss: 1093.6891\n",
      "Training Epoch: 3 [1150/36450]\tLoss: 1183.4341\n",
      "Training Epoch: 3 [1200/36450]\tLoss: 1152.7037\n",
      "Training Epoch: 3 [1250/36450]\tLoss: 1103.5261\n",
      "Training Epoch: 3 [1300/36450]\tLoss: 1190.3977\n",
      "Training Epoch: 3 [1350/36450]\tLoss: 1110.8130\n",
      "Training Epoch: 3 [1400/36450]\tLoss: 1154.1866\n",
      "Training Epoch: 3 [1450/36450]\tLoss: 1100.2233\n",
      "Training Epoch: 3 [1500/36450]\tLoss: 1121.5045\n",
      "Training Epoch: 3 [1550/36450]\tLoss: 1112.9531\n",
      "Training Epoch: 3 [1600/36450]\tLoss: 1091.1532\n",
      "Training Epoch: 3 [1650/36450]\tLoss: 1084.8822\n",
      "Training Epoch: 3 [1700/36450]\tLoss: 1128.4370\n",
      "Training Epoch: 3 [1750/36450]\tLoss: 1121.6873\n",
      "Training Epoch: 3 [1800/36450]\tLoss: 1068.7659\n",
      "Training Epoch: 3 [1850/36450]\tLoss: 1110.6021\n",
      "Training Epoch: 3 [1900/36450]\tLoss: 1164.3917\n",
      "Training Epoch: 3 [1950/36450]\tLoss: 1176.5852\n",
      "Training Epoch: 3 [2000/36450]\tLoss: 1135.5242\n",
      "Training Epoch: 3 [2050/36450]\tLoss: 1120.1658\n",
      "Training Epoch: 3 [2100/36450]\tLoss: 1140.4769\n",
      "Training Epoch: 3 [2150/36450]\tLoss: 1103.9412\n",
      "Training Epoch: 3 [2200/36450]\tLoss: 1129.4935\n",
      "Training Epoch: 3 [2250/36450]\tLoss: 1139.8429\n",
      "Training Epoch: 3 [2300/36450]\tLoss: 1081.2032\n",
      "Training Epoch: 3 [2350/36450]\tLoss: 1056.7426\n",
      "Training Epoch: 3 [2400/36450]\tLoss: 1118.9871\n",
      "Training Epoch: 3 [2450/36450]\tLoss: 1107.2904\n",
      "Training Epoch: 3 [2500/36450]\tLoss: 1130.4559\n",
      "Training Epoch: 3 [2550/36450]\tLoss: 1103.6914\n",
      "Training Epoch: 3 [2600/36450]\tLoss: 1097.5065\n",
      "Training Epoch: 3 [2650/36450]\tLoss: 1120.2805\n",
      "Training Epoch: 3 [2700/36450]\tLoss: 1135.0305\n",
      "Training Epoch: 3 [2750/36450]\tLoss: 1104.4973\n",
      "Training Epoch: 3 [2800/36450]\tLoss: 1108.4337\n",
      "Training Epoch: 3 [2850/36450]\tLoss: 1167.5990\n",
      "Training Epoch: 3 [2900/36450]\tLoss: 1092.5192\n",
      "Training Epoch: 3 [2950/36450]\tLoss: 1134.5236\n",
      "Training Epoch: 3 [3000/36450]\tLoss: 1137.4440\n",
      "Training Epoch: 3 [3050/36450]\tLoss: 1057.1742\n",
      "Training Epoch: 3 [3100/36450]\tLoss: 1125.8262\n",
      "Training Epoch: 3 [3150/36450]\tLoss: 1101.9535\n",
      "Training Epoch: 3 [3200/36450]\tLoss: 1088.3781\n",
      "Training Epoch: 3 [3250/36450]\tLoss: 1186.6660\n",
      "Training Epoch: 3 [3300/36450]\tLoss: 1100.5878\n",
      "Training Epoch: 3 [3350/36450]\tLoss: 1105.9031\n",
      "Training Epoch: 3 [3400/36450]\tLoss: 1055.8540\n",
      "Training Epoch: 3 [3450/36450]\tLoss: 1137.4176\n",
      "Training Epoch: 3 [3500/36450]\tLoss: 1088.3922\n",
      "Training Epoch: 3 [3550/36450]\tLoss: 1148.0312\n",
      "Training Epoch: 3 [3600/36450]\tLoss: 1080.8888\n",
      "Training Epoch: 3 [3650/36450]\tLoss: 1149.8618\n",
      "Training Epoch: 3 [3700/36450]\tLoss: 1094.9723\n",
      "Training Epoch: 3 [3750/36450]\tLoss: 1071.7108\n",
      "Training Epoch: 3 [3800/36450]\tLoss: 1078.9189\n",
      "Training Epoch: 3 [3850/36450]\tLoss: 1095.4958\n",
      "Training Epoch: 3 [3900/36450]\tLoss: 1085.3591\n",
      "Training Epoch: 3 [3950/36450]\tLoss: 1123.1423\n",
      "Training Epoch: 3 [4000/36450]\tLoss: 1190.6589\n",
      "Training Epoch: 3 [4050/36450]\tLoss: 1149.3567\n",
      "Training Epoch: 3 [4100/36450]\tLoss: 1124.7797\n",
      "Training Epoch: 3 [4150/36450]\tLoss: 1156.2762\n",
      "Training Epoch: 3 [4200/36450]\tLoss: 1137.9917\n",
      "Training Epoch: 3 [4250/36450]\tLoss: 1057.0127\n",
      "Training Epoch: 3 [4300/36450]\tLoss: 1113.1200\n",
      "Training Epoch: 3 [4350/36450]\tLoss: 1124.5703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [4400/36450]\tLoss: 1112.8920\n",
      "Training Epoch: 3 [4450/36450]\tLoss: 1120.7987\n",
      "Training Epoch: 3 [4500/36450]\tLoss: 1161.2001\n",
      "Training Epoch: 3 [4550/36450]\tLoss: 1132.5570\n",
      "Training Epoch: 3 [4600/36450]\tLoss: 1082.5035\n",
      "Training Epoch: 3 [4650/36450]\tLoss: 1166.0188\n",
      "Training Epoch: 3 [4700/36450]\tLoss: 1141.8883\n",
      "Training Epoch: 3 [4750/36450]\tLoss: 1061.9114\n",
      "Training Epoch: 3 [4800/36450]\tLoss: 1121.1738\n",
      "Training Epoch: 3 [4850/36450]\tLoss: 1114.3457\n",
      "Training Epoch: 3 [4900/36450]\tLoss: 1143.7006\n",
      "Training Epoch: 3 [4950/36450]\tLoss: 1081.7047\n",
      "Training Epoch: 3 [5000/36450]\tLoss: 1020.8688\n",
      "Training Epoch: 3 [5050/36450]\tLoss: 1139.9055\n",
      "Training Epoch: 3 [5100/36450]\tLoss: 1123.9026\n",
      "Training Epoch: 3 [5150/36450]\tLoss: 1126.2203\n",
      "Training Epoch: 3 [5200/36450]\tLoss: 1103.1881\n",
      "Training Epoch: 3 [5250/36450]\tLoss: 1125.5472\n",
      "Training Epoch: 3 [5300/36450]\tLoss: 1073.5350\n",
      "Training Epoch: 3 [5350/36450]\tLoss: 1111.5262\n",
      "Training Epoch: 3 [5400/36450]\tLoss: 1113.4706\n",
      "Training Epoch: 3 [5450/36450]\tLoss: 1106.4751\n",
      "Training Epoch: 3 [5500/36450]\tLoss: 1084.8177\n",
      "Training Epoch: 3 [5550/36450]\tLoss: 1088.2439\n",
      "Training Epoch: 3 [5600/36450]\tLoss: 1105.1980\n",
      "Training Epoch: 3 [5650/36450]\tLoss: 1071.1344\n",
      "Training Epoch: 3 [5700/36450]\tLoss: 1070.6956\n",
      "Training Epoch: 3 [5750/36450]\tLoss: 1063.1012\n",
      "Training Epoch: 3 [5800/36450]\tLoss: 1144.2915\n",
      "Training Epoch: 3 [5850/36450]\tLoss: 1025.8311\n",
      "Training Epoch: 3 [5900/36450]\tLoss: 1078.3485\n",
      "Training Epoch: 3 [5950/36450]\tLoss: 1095.6010\n",
      "Training Epoch: 3 [6000/36450]\tLoss: 1103.1791\n",
      "Training Epoch: 3 [6050/36450]\tLoss: 1126.2126\n",
      "Training Epoch: 3 [6100/36450]\tLoss: 1080.5597\n",
      "Training Epoch: 3 [6150/36450]\tLoss: 1065.0024\n",
      "Training Epoch: 3 [6200/36450]\tLoss: 1060.9901\n",
      "Training Epoch: 3 [6250/36450]\tLoss: 1109.9769\n",
      "Training Epoch: 3 [6300/36450]\tLoss: 1009.6912\n",
      "Training Epoch: 3 [6350/36450]\tLoss: 1055.4327\n",
      "Training Epoch: 3 [6400/36450]\tLoss: 1134.1870\n",
      "Training Epoch: 3 [6450/36450]\tLoss: 1136.3751\n",
      "Training Epoch: 3 [6500/36450]\tLoss: 1116.8455\n",
      "Training Epoch: 3 [6550/36450]\tLoss: 1044.5930\n",
      "Training Epoch: 3 [6600/36450]\tLoss: 1089.5981\n",
      "Training Epoch: 3 [6650/36450]\tLoss: 996.0615\n",
      "Training Epoch: 3 [6700/36450]\tLoss: 1055.5018\n",
      "Training Epoch: 3 [6750/36450]\tLoss: 1022.4520\n",
      "Training Epoch: 3 [6800/36450]\tLoss: 1059.1909\n",
      "Training Epoch: 3 [6850/36450]\tLoss: 1076.2339\n",
      "Training Epoch: 3 [6900/36450]\tLoss: 1080.9816\n",
      "Training Epoch: 3 [6950/36450]\tLoss: 1105.6643\n",
      "Training Epoch: 3 [7000/36450]\tLoss: 1072.7383\n",
      "Training Epoch: 3 [7050/36450]\tLoss: 1080.4196\n",
      "Training Epoch: 3 [7100/36450]\tLoss: 1073.6655\n",
      "Training Epoch: 3 [7150/36450]\tLoss: 1136.6213\n",
      "Training Epoch: 3 [7200/36450]\tLoss: 1173.5367\n",
      "Training Epoch: 3 [7250/36450]\tLoss: 1075.0663\n",
      "Training Epoch: 3 [7300/36450]\tLoss: 1070.6956\n",
      "Training Epoch: 3 [7350/36450]\tLoss: 1119.5261\n",
      "Training Epoch: 3 [7400/36450]\tLoss: 1060.4296\n",
      "Training Epoch: 3 [7450/36450]\tLoss: 1105.2246\n",
      "Training Epoch: 3 [7500/36450]\tLoss: 1102.5392\n",
      "Training Epoch: 3 [7550/36450]\tLoss: 1099.4039\n",
      "Training Epoch: 3 [7600/36450]\tLoss: 1101.1797\n",
      "Training Epoch: 3 [7650/36450]\tLoss: 1141.9207\n",
      "Training Epoch: 3 [7700/36450]\tLoss: 960.1686\n",
      "Training Epoch: 3 [7750/36450]\tLoss: 1056.5417\n",
      "Training Epoch: 3 [7800/36450]\tLoss: 1099.3706\n",
      "Training Epoch: 3 [7850/36450]\tLoss: 1129.7710\n",
      "Training Epoch: 3 [7900/36450]\tLoss: 1094.5465\n",
      "Training Epoch: 3 [7950/36450]\tLoss: 1056.3708\n",
      "Training Epoch: 3 [8000/36450]\tLoss: 1118.4707\n",
      "Training Epoch: 3 [8050/36450]\tLoss: 1024.2673\n",
      "Training Epoch: 3 [8100/36450]\tLoss: 1067.8881\n",
      "Training Epoch: 3 [8150/36450]\tLoss: 1081.8817\n",
      "Training Epoch: 3 [8200/36450]\tLoss: 1060.2035\n",
      "Training Epoch: 3 [8250/36450]\tLoss: 1087.3375\n",
      "Training Epoch: 3 [8300/36450]\tLoss: 1087.5110\n",
      "Training Epoch: 3 [8350/36450]\tLoss: 1000.9775\n",
      "Training Epoch: 3 [8400/36450]\tLoss: 1066.7229\n",
      "Training Epoch: 3 [8450/36450]\tLoss: 1076.2582\n",
      "Training Epoch: 3 [8500/36450]\tLoss: 1009.3980\n",
      "Training Epoch: 3 [8550/36450]\tLoss: 1095.7615\n",
      "Training Epoch: 3 [8600/36450]\tLoss: 1102.5430\n",
      "Training Epoch: 3 [8650/36450]\tLoss: 1083.7260\n",
      "Training Epoch: 3 [8700/36450]\tLoss: 1103.0885\n",
      "Training Epoch: 3 [8750/36450]\tLoss: 1067.4128\n",
      "Training Epoch: 3 [8800/36450]\tLoss: 1096.6525\n",
      "Training Epoch: 3 [8850/36450]\tLoss: 1082.1920\n",
      "Training Epoch: 3 [8900/36450]\tLoss: 1092.5394\n",
      "Training Epoch: 3 [8950/36450]\tLoss: 1091.4503\n",
      "Training Epoch: 3 [9000/36450]\tLoss: 1059.6636\n",
      "Training Epoch: 3 [9050/36450]\tLoss: 1130.7406\n",
      "Training Epoch: 3 [9100/36450]\tLoss: 1069.6526\n",
      "Training Epoch: 3 [9150/36450]\tLoss: 1122.6342\n",
      "Training Epoch: 3 [9200/36450]\tLoss: 1045.1826\n",
      "Training Epoch: 3 [9250/36450]\tLoss: 1052.4830\n",
      "Training Epoch: 3 [9300/36450]\tLoss: 1040.7965\n",
      "Training Epoch: 3 [9350/36450]\tLoss: 1078.0522\n",
      "Training Epoch: 3 [9400/36450]\tLoss: 1072.4476\n",
      "Training Epoch: 3 [9450/36450]\tLoss: 1042.6481\n",
      "Training Epoch: 3 [9500/36450]\tLoss: 1053.2887\n",
      "Training Epoch: 3 [9550/36450]\tLoss: 1031.1876\n",
      "Training Epoch: 3 [9600/36450]\tLoss: 1001.5904\n",
      "Training Epoch: 3 [9650/36450]\tLoss: 1067.1066\n",
      "Training Epoch: 3 [9700/36450]\tLoss: 1071.0205\n",
      "Training Epoch: 3 [9750/36450]\tLoss: 1011.7784\n",
      "Training Epoch: 3 [9800/36450]\tLoss: 1031.3652\n",
      "Training Epoch: 3 [9850/36450]\tLoss: 1072.2256\n",
      "Training Epoch: 3 [9900/36450]\tLoss: 1076.7021\n",
      "Training Epoch: 3 [9950/36450]\tLoss: 1070.4303\n",
      "Training Epoch: 3 [10000/36450]\tLoss: 1075.3624\n",
      "Training Epoch: 3 [10050/36450]\tLoss: 1015.2704\n",
      "Training Epoch: 3 [10100/36450]\tLoss: 1072.8384\n",
      "Training Epoch: 3 [10150/36450]\tLoss: 1069.9934\n",
      "Training Epoch: 3 [10200/36450]\tLoss: 1082.5769\n",
      "Training Epoch: 3 [10250/36450]\tLoss: 1065.6877\n",
      "Training Epoch: 3 [10300/36450]\tLoss: 1051.3848\n",
      "Training Epoch: 3 [10350/36450]\tLoss: 1028.4451\n",
      "Training Epoch: 3 [10400/36450]\tLoss: 1080.8195\n",
      "Training Epoch: 3 [10450/36450]\tLoss: 1007.7814\n",
      "Training Epoch: 3 [10500/36450]\tLoss: 1067.2134\n",
      "Training Epoch: 3 [10550/36450]\tLoss: 1065.8505\n",
      "Training Epoch: 3 [10600/36450]\tLoss: 1009.7204\n",
      "Training Epoch: 3 [10650/36450]\tLoss: 1058.2203\n",
      "Training Epoch: 3 [10700/36450]\tLoss: 1015.6181\n",
      "Training Epoch: 3 [10750/36450]\tLoss: 1035.0896\n",
      "Training Epoch: 3 [10800/36450]\tLoss: 1041.0555\n",
      "Training Epoch: 3 [10850/36450]\tLoss: 1015.1189\n",
      "Training Epoch: 3 [10900/36450]\tLoss: 1095.7910\n",
      "Training Epoch: 3 [10950/36450]\tLoss: 1063.3921\n",
      "Training Epoch: 3 [11000/36450]\tLoss: 1070.1489\n",
      "Training Epoch: 3 [11050/36450]\tLoss: 1071.8442\n",
      "Training Epoch: 3 [11100/36450]\tLoss: 1057.4266\n",
      "Training Epoch: 3 [11150/36450]\tLoss: 1020.0013\n",
      "Training Epoch: 3 [11200/36450]\tLoss: 1028.8185\n",
      "Training Epoch: 3 [11250/36450]\tLoss: 1088.7618\n",
      "Training Epoch: 3 [11300/36450]\tLoss: 1059.1315\n",
      "Training Epoch: 3 [11350/36450]\tLoss: 1082.0378\n",
      "Training Epoch: 3 [11400/36450]\tLoss: 1020.8394\n",
      "Training Epoch: 3 [11450/36450]\tLoss: 1002.5447\n",
      "Training Epoch: 3 [11500/36450]\tLoss: 1010.8997\n",
      "Training Epoch: 3 [11550/36450]\tLoss: 1051.9982\n",
      "Training Epoch: 3 [11600/36450]\tLoss: 1038.9021\n",
      "Training Epoch: 3 [11650/36450]\tLoss: 1105.4745\n",
      "Training Epoch: 3 [11700/36450]\tLoss: 1027.6453\n",
      "Training Epoch: 3 [11750/36450]\tLoss: 1080.5331\n",
      "Training Epoch: 3 [11800/36450]\tLoss: 1034.3728\n",
      "Training Epoch: 3 [11850/36450]\tLoss: 1054.1986\n",
      "Training Epoch: 3 [11900/36450]\tLoss: 991.8860\n",
      "Training Epoch: 3 [11950/36450]\tLoss: 1074.6921\n",
      "Training Epoch: 3 [12000/36450]\tLoss: 1118.3130\n",
      "Training Epoch: 3 [12050/36450]\tLoss: 1026.0802\n",
      "Training Epoch: 3 [12100/36450]\tLoss: 1018.4918\n",
      "Training Epoch: 3 [12150/36450]\tLoss: 1074.0919\n",
      "Training Epoch: 3 [12200/36450]\tLoss: 1022.3478\n",
      "Training Epoch: 3 [12250/36450]\tLoss: 1006.3009\n",
      "Training Epoch: 3 [12300/36450]\tLoss: 1014.5402\n",
      "Training Epoch: 3 [12350/36450]\tLoss: 1062.6935\n",
      "Training Epoch: 3 [12400/36450]\tLoss: 1070.7927\n",
      "Training Epoch: 3 [12450/36450]\tLoss: 996.8234\n",
      "Training Epoch: 3 [12500/36450]\tLoss: 1058.0292\n",
      "Training Epoch: 3 [12550/36450]\tLoss: 1012.7403\n",
      "Training Epoch: 3 [12600/36450]\tLoss: 1004.5844\n",
      "Training Epoch: 3 [12650/36450]\tLoss: 1034.6783\n",
      "Training Epoch: 3 [12700/36450]\tLoss: 1068.4911\n",
      "Training Epoch: 3 [12750/36450]\tLoss: 1011.4124\n",
      "Training Epoch: 3 [12800/36450]\tLoss: 1061.4216\n",
      "Training Epoch: 3 [12850/36450]\tLoss: 1032.8036\n",
      "Training Epoch: 3 [12900/36450]\tLoss: 1052.7955\n",
      "Training Epoch: 3 [12950/36450]\tLoss: 1004.1960\n",
      "Training Epoch: 3 [13000/36450]\tLoss: 1018.9099\n",
      "Training Epoch: 3 [13050/36450]\tLoss: 1001.5682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [13100/36450]\tLoss: 1060.8987\n",
      "Training Epoch: 3 [13150/36450]\tLoss: 1010.3797\n",
      "Training Epoch: 3 [13200/36450]\tLoss: 1050.8442\n",
      "Training Epoch: 3 [13250/36450]\tLoss: 1088.5399\n",
      "Training Epoch: 3 [13300/36450]\tLoss: 1020.3296\n",
      "Training Epoch: 3 [13350/36450]\tLoss: 1033.0627\n",
      "Training Epoch: 3 [13400/36450]\tLoss: 1010.8541\n",
      "Training Epoch: 3 [13450/36450]\tLoss: 1022.9772\n",
      "Training Epoch: 3 [13500/36450]\tLoss: 1041.8212\n",
      "Training Epoch: 3 [13550/36450]\tLoss: 1008.1019\n",
      "Training Epoch: 3 [13600/36450]\tLoss: 949.2411\n",
      "Training Epoch: 3 [13650/36450]\tLoss: 930.9558\n",
      "Training Epoch: 3 [13700/36450]\tLoss: 1030.3065\n",
      "Training Epoch: 3 [13750/36450]\tLoss: 1022.2286\n",
      "Training Epoch: 3 [13800/36450]\tLoss: 1002.2078\n",
      "Training Epoch: 3 [13850/36450]\tLoss: 1007.4413\n",
      "Training Epoch: 3 [13900/36450]\tLoss: 1014.0511\n",
      "Training Epoch: 3 [13950/36450]\tLoss: 1068.5135\n",
      "Training Epoch: 3 [14000/36450]\tLoss: 1049.5806\n",
      "Training Epoch: 3 [14050/36450]\tLoss: 981.1080\n",
      "Training Epoch: 3 [14100/36450]\tLoss: 1081.5292\n",
      "Training Epoch: 3 [14150/36450]\tLoss: 1039.5199\n",
      "Training Epoch: 3 [14200/36450]\tLoss: 1056.8888\n",
      "Training Epoch: 3 [14250/36450]\tLoss: 1008.9711\n",
      "Training Epoch: 3 [14300/36450]\tLoss: 1052.3345\n",
      "Training Epoch: 3 [14350/36450]\tLoss: 996.4537\n",
      "Training Epoch: 3 [14400/36450]\tLoss: 1024.7903\n",
      "Training Epoch: 3 [14450/36450]\tLoss: 1098.3020\n",
      "Training Epoch: 3 [14500/36450]\tLoss: 1043.8805\n",
      "Training Epoch: 3 [14550/36450]\tLoss: 1031.6357\n",
      "Training Epoch: 3 [14600/36450]\tLoss: 1008.0889\n",
      "Training Epoch: 3 [14650/36450]\tLoss: 1011.8740\n",
      "Training Epoch: 3 [14700/36450]\tLoss: 1049.9937\n",
      "Training Epoch: 3 [14750/36450]\tLoss: 1111.8718\n",
      "Training Epoch: 3 [14800/36450]\tLoss: 1013.1780\n",
      "Training Epoch: 3 [14850/36450]\tLoss: 1026.6970\n",
      "Training Epoch: 3 [14900/36450]\tLoss: 989.0845\n",
      "Training Epoch: 3 [14950/36450]\tLoss: 1099.6279\n",
      "Training Epoch: 3 [15000/36450]\tLoss: 1048.0364\n",
      "Training Epoch: 3 [15050/36450]\tLoss: 1063.3055\n",
      "Training Epoch: 3 [15100/36450]\tLoss: 1075.7368\n",
      "Training Epoch: 3 [15150/36450]\tLoss: 1000.7123\n",
      "Training Epoch: 3 [15200/36450]\tLoss: 1009.8864\n",
      "Training Epoch: 3 [15250/36450]\tLoss: 1021.0564\n",
      "Training Epoch: 3 [15300/36450]\tLoss: 1110.3081\n",
      "Training Epoch: 3 [15350/36450]\tLoss: 1064.2861\n",
      "Training Epoch: 3 [15400/36450]\tLoss: 966.6343\n",
      "Training Epoch: 3 [15450/36450]\tLoss: 1062.8256\n",
      "Training Epoch: 3 [15500/36450]\tLoss: 1026.7504\n",
      "Training Epoch: 3 [15550/36450]\tLoss: 1044.9316\n",
      "Training Epoch: 3 [15600/36450]\tLoss: 1051.6569\n",
      "Training Epoch: 3 [15650/36450]\tLoss: 1040.2356\n",
      "Training Epoch: 3 [15700/36450]\tLoss: 1023.0904\n",
      "Training Epoch: 3 [15750/36450]\tLoss: 1026.9725\n",
      "Training Epoch: 3 [15800/36450]\tLoss: 993.6811\n",
      "Training Epoch: 3 [15850/36450]\tLoss: 1079.6970\n",
      "Training Epoch: 3 [15900/36450]\tLoss: 1045.0912\n",
      "Training Epoch: 3 [15950/36450]\tLoss: 1050.9036\n",
      "Training Epoch: 3 [16000/36450]\tLoss: 1082.6143\n",
      "Training Epoch: 3 [16050/36450]\tLoss: 1024.5939\n",
      "Training Epoch: 3 [16100/36450]\tLoss: 1023.0535\n",
      "Training Epoch: 3 [16150/36450]\tLoss: 1082.4590\n",
      "Training Epoch: 3 [16200/36450]\tLoss: 1035.2200\n",
      "Training Epoch: 3 [16250/36450]\tLoss: 1032.8185\n",
      "Training Epoch: 3 [16300/36450]\tLoss: 942.8312\n",
      "Training Epoch: 3 [16350/36450]\tLoss: 1003.2844\n",
      "Training Epoch: 3 [16400/36450]\tLoss: 1096.2106\n",
      "Training Epoch: 3 [16450/36450]\tLoss: 978.8633\n",
      "Training Epoch: 3 [16500/36450]\tLoss: 1030.6302\n",
      "Training Epoch: 3 [16550/36450]\tLoss: 1023.0830\n",
      "Training Epoch: 3 [16600/36450]\tLoss: 1028.2573\n",
      "Training Epoch: 3 [16650/36450]\tLoss: 1042.0094\n",
      "Training Epoch: 3 [16700/36450]\tLoss: 1036.0497\n",
      "Training Epoch: 3 [16750/36450]\tLoss: 1024.7400\n",
      "Training Epoch: 3 [16800/36450]\tLoss: 1033.7794\n",
      "Training Epoch: 3 [16850/36450]\tLoss: 1063.9158\n",
      "Training Epoch: 3 [16900/36450]\tLoss: 996.0477\n",
      "Training Epoch: 3 [16950/36450]\tLoss: 1008.1853\n",
      "Training Epoch: 3 [17000/36450]\tLoss: 1065.9130\n",
      "Training Epoch: 3 [17050/36450]\tLoss: 1006.4031\n",
      "Training Epoch: 3 [17100/36450]\tLoss: 1064.7026\n",
      "Training Epoch: 3 [17150/36450]\tLoss: 1052.7833\n",
      "Training Epoch: 3 [17200/36450]\tLoss: 1059.9933\n",
      "Training Epoch: 3 [17250/36450]\tLoss: 933.3768\n",
      "Training Epoch: 3 [17300/36450]\tLoss: 1046.6230\n",
      "Training Epoch: 3 [17350/36450]\tLoss: 1000.9246\n",
      "Training Epoch: 3 [17400/36450]\tLoss: 1062.3588\n",
      "Training Epoch: 3 [17450/36450]\tLoss: 1033.2803\n",
      "Training Epoch: 3 [17500/36450]\tLoss: 996.8785\n",
      "Training Epoch: 3 [17550/36450]\tLoss: 1049.2526\n",
      "Training Epoch: 3 [17600/36450]\tLoss: 1003.3588\n",
      "Training Epoch: 3 [17650/36450]\tLoss: 1027.9927\n",
      "Training Epoch: 3 [17700/36450]\tLoss: 1006.5645\n",
      "Training Epoch: 3 [17750/36450]\tLoss: 1012.4274\n",
      "Training Epoch: 3 [17800/36450]\tLoss: 977.3834\n",
      "Training Epoch: 3 [17850/36450]\tLoss: 976.1555\n",
      "Training Epoch: 3 [17900/36450]\tLoss: 1064.7096\n",
      "Training Epoch: 3 [17950/36450]\tLoss: 991.9666\n",
      "Training Epoch: 3 [18000/36450]\tLoss: 1011.5407\n",
      "Training Epoch: 3 [18050/36450]\tLoss: 1068.5184\n",
      "Training Epoch: 3 [18100/36450]\tLoss: 1031.5159\n",
      "Training Epoch: 3 [18150/36450]\tLoss: 986.7297\n",
      "Training Epoch: 3 [18200/36450]\tLoss: 1023.6094\n",
      "Training Epoch: 3 [18250/36450]\tLoss: 989.9092\n",
      "Training Epoch: 3 [18300/36450]\tLoss: 1035.4010\n",
      "Training Epoch: 3 [18350/36450]\tLoss: 1048.3710\n",
      "Training Epoch: 3 [18400/36450]\tLoss: 1038.5841\n",
      "Training Epoch: 3 [18450/36450]\tLoss: 1069.1819\n",
      "Training Epoch: 3 [18500/36450]\tLoss: 1053.2775\n",
      "Training Epoch: 3 [18550/36450]\tLoss: 1034.6780\n",
      "Training Epoch: 3 [18600/36450]\tLoss: 1020.5737\n",
      "Training Epoch: 3 [18650/36450]\tLoss: 969.8704\n",
      "Training Epoch: 3 [18700/36450]\tLoss: 1030.5797\n",
      "Training Epoch: 3 [18750/36450]\tLoss: 969.8770\n",
      "Training Epoch: 3 [18800/36450]\tLoss: 998.8855\n",
      "Training Epoch: 3 [18850/36450]\tLoss: 985.8815\n",
      "Training Epoch: 3 [18900/36450]\tLoss: 1027.3147\n",
      "Training Epoch: 3 [18950/36450]\tLoss: 1001.6294\n",
      "Training Epoch: 3 [19000/36450]\tLoss: 1024.0369\n",
      "Training Epoch: 3 [19050/36450]\tLoss: 1004.0888\n",
      "Training Epoch: 3 [19100/36450]\tLoss: 1049.5594\n",
      "Training Epoch: 3 [19150/36450]\tLoss: 1000.0578\n",
      "Training Epoch: 3 [19200/36450]\tLoss: 1015.5912\n",
      "Training Epoch: 3 [19250/36450]\tLoss: 1074.3690\n",
      "Training Epoch: 3 [19300/36450]\tLoss: 1056.5170\n",
      "Training Epoch: 3 [19350/36450]\tLoss: 1033.3812\n",
      "Training Epoch: 3 [19400/36450]\tLoss: 984.8232\n",
      "Training Epoch: 3 [19450/36450]\tLoss: 965.5096\n",
      "Training Epoch: 3 [19500/36450]\tLoss: 1020.8263\n",
      "Training Epoch: 3 [19550/36450]\tLoss: 1053.2858\n",
      "Training Epoch: 3 [19600/36450]\tLoss: 967.1811\n",
      "Training Epoch: 3 [19650/36450]\tLoss: 1069.5651\n",
      "Training Epoch: 3 [19700/36450]\tLoss: 1013.7449\n",
      "Training Epoch: 3 [19750/36450]\tLoss: 941.3696\n",
      "Training Epoch: 3 [19800/36450]\tLoss: 1026.5056\n",
      "Training Epoch: 3 [19850/36450]\tLoss: 965.1566\n",
      "Training Epoch: 3 [19900/36450]\tLoss: 993.3271\n",
      "Training Epoch: 3 [19950/36450]\tLoss: 1013.7913\n",
      "Training Epoch: 3 [20000/36450]\tLoss: 1056.6440\n",
      "Training Epoch: 3 [20050/36450]\tLoss: 1060.1111\n",
      "Training Epoch: 3 [20100/36450]\tLoss: 968.2384\n",
      "Training Epoch: 3 [20150/36450]\tLoss: 1015.3957\n",
      "Training Epoch: 3 [20200/36450]\tLoss: 976.5849\n",
      "Training Epoch: 3 [20250/36450]\tLoss: 971.8066\n",
      "Training Epoch: 3 [20300/36450]\tLoss: 971.5957\n",
      "Training Epoch: 3 [20350/36450]\tLoss: 973.3310\n",
      "Training Epoch: 3 [20400/36450]\tLoss: 995.3053\n",
      "Training Epoch: 3 [20450/36450]\tLoss: 984.6833\n",
      "Training Epoch: 3 [20500/36450]\tLoss: 978.0932\n",
      "Training Epoch: 3 [20550/36450]\tLoss: 950.5622\n",
      "Training Epoch: 3 [20600/36450]\tLoss: 1034.1094\n",
      "Training Epoch: 3 [20650/36450]\tLoss: 1022.3668\n",
      "Training Epoch: 3 [20700/36450]\tLoss: 1014.7038\n",
      "Training Epoch: 3 [20750/36450]\tLoss: 1041.7784\n",
      "Training Epoch: 3 [20800/36450]\tLoss: 1008.9059\n",
      "Training Epoch: 3 [20850/36450]\tLoss: 1055.9192\n",
      "Training Epoch: 3 [20900/36450]\tLoss: 1029.8219\n",
      "Training Epoch: 3 [20950/36450]\tLoss: 1016.0475\n",
      "Training Epoch: 3 [21000/36450]\tLoss: 965.2077\n",
      "Training Epoch: 3 [21050/36450]\tLoss: 927.5657\n",
      "Training Epoch: 3 [21100/36450]\tLoss: 1024.1888\n",
      "Training Epoch: 3 [21150/36450]\tLoss: 995.2977\n",
      "Training Epoch: 3 [21200/36450]\tLoss: 1007.8137\n",
      "Training Epoch: 3 [21250/36450]\tLoss: 1003.5748\n",
      "Training Epoch: 3 [21300/36450]\tLoss: 1060.7852\n",
      "Training Epoch: 3 [21350/36450]\tLoss: 977.5057\n",
      "Training Epoch: 3 [21400/36450]\tLoss: 950.5095\n",
      "Training Epoch: 3 [21450/36450]\tLoss: 961.2126\n",
      "Training Epoch: 3 [21500/36450]\tLoss: 1009.6132\n",
      "Training Epoch: 3 [21550/36450]\tLoss: 1047.0884\n",
      "Training Epoch: 3 [21600/36450]\tLoss: 997.5746\n",
      "Training Epoch: 3 [21650/36450]\tLoss: 1041.9298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [21700/36450]\tLoss: 983.4371\n",
      "Training Epoch: 3 [21750/36450]\tLoss: 1003.0295\n",
      "Training Epoch: 3 [21800/36450]\tLoss: 1067.7140\n",
      "Training Epoch: 3 [21850/36450]\tLoss: 988.7714\n",
      "Training Epoch: 3 [21900/36450]\tLoss: 1059.1475\n",
      "Training Epoch: 3 [21950/36450]\tLoss: 966.1971\n",
      "Training Epoch: 3 [22000/36450]\tLoss: 1039.2274\n",
      "Training Epoch: 3 [22050/36450]\tLoss: 1043.4088\n",
      "Training Epoch: 3 [22100/36450]\tLoss: 968.3313\n",
      "Training Epoch: 3 [22150/36450]\tLoss: 926.3275\n",
      "Training Epoch: 3 [22200/36450]\tLoss: 1053.5553\n",
      "Training Epoch: 3 [22250/36450]\tLoss: 1048.7852\n",
      "Training Epoch: 3 [22300/36450]\tLoss: 984.3907\n",
      "Training Epoch: 3 [22350/36450]\tLoss: 1035.3480\n",
      "Training Epoch: 3 [22400/36450]\tLoss: 1035.2100\n",
      "Training Epoch: 3 [22450/36450]\tLoss: 1035.0986\n",
      "Training Epoch: 3 [22500/36450]\tLoss: 1011.1850\n",
      "Training Epoch: 3 [22550/36450]\tLoss: 979.4106\n",
      "Training Epoch: 3 [22600/36450]\tLoss: 956.9644\n",
      "Training Epoch: 3 [22650/36450]\tLoss: 982.7067\n",
      "Training Epoch: 3 [22700/36450]\tLoss: 1045.5043\n",
      "Training Epoch: 3 [22750/36450]\tLoss: 994.5435\n",
      "Training Epoch: 3 [22800/36450]\tLoss: 976.1519\n",
      "Training Epoch: 3 [22850/36450]\tLoss: 1023.4467\n",
      "Training Epoch: 3 [22900/36450]\tLoss: 993.7905\n",
      "Training Epoch: 3 [22950/36450]\tLoss: 1050.7062\n",
      "Training Epoch: 3 [23000/36450]\tLoss: 1003.8292\n",
      "Training Epoch: 3 [23050/36450]\tLoss: 1028.0612\n",
      "Training Epoch: 3 [23100/36450]\tLoss: 964.6536\n",
      "Training Epoch: 3 [23150/36450]\tLoss: 954.6602\n",
      "Training Epoch: 3 [23200/36450]\tLoss: 1021.9613\n",
      "Training Epoch: 3 [23250/36450]\tLoss: 977.0960\n",
      "Training Epoch: 3 [23300/36450]\tLoss: 985.1165\n",
      "Training Epoch: 3 [23350/36450]\tLoss: 934.3589\n",
      "Training Epoch: 3 [23400/36450]\tLoss: 961.9230\n",
      "Training Epoch: 3 [23450/36450]\tLoss: 991.2336\n",
      "Training Epoch: 3 [23500/36450]\tLoss: 988.5589\n",
      "Training Epoch: 3 [23550/36450]\tLoss: 983.0001\n",
      "Training Epoch: 3 [23600/36450]\tLoss: 979.6836\n",
      "Training Epoch: 3 [23650/36450]\tLoss: 947.8575\n",
      "Training Epoch: 3 [23700/36450]\tLoss: 983.2690\n",
      "Training Epoch: 3 [23750/36450]\tLoss: 998.6419\n",
      "Training Epoch: 3 [23800/36450]\tLoss: 991.2054\n",
      "Training Epoch: 3 [23850/36450]\tLoss: 985.3828\n",
      "Training Epoch: 3 [23900/36450]\tLoss: 981.3083\n",
      "Training Epoch: 3 [23950/36450]\tLoss: 969.2762\n",
      "Training Epoch: 3 [24000/36450]\tLoss: 1007.4320\n",
      "Training Epoch: 3 [24050/36450]\tLoss: 916.6972\n",
      "Training Epoch: 3 [24100/36450]\tLoss: 1020.8411\n",
      "Training Epoch: 3 [24150/36450]\tLoss: 996.5848\n",
      "Training Epoch: 3 [24200/36450]\tLoss: 971.6857\n",
      "Training Epoch: 3 [24250/36450]\tLoss: 1014.2350\n",
      "Training Epoch: 3 [24300/36450]\tLoss: 1038.1569\n",
      "Training Epoch: 3 [24350/36450]\tLoss: 1030.9415\n",
      "Training Epoch: 3 [24400/36450]\tLoss: 965.3231\n",
      "Training Epoch: 3 [24450/36450]\tLoss: 990.9023\n",
      "Training Epoch: 3 [24500/36450]\tLoss: 958.6175\n",
      "Training Epoch: 3 [24550/36450]\tLoss: 971.9803\n",
      "Training Epoch: 3 [24600/36450]\tLoss: 1026.9357\n",
      "Training Epoch: 3 [24650/36450]\tLoss: 1005.9514\n",
      "Training Epoch: 3 [24700/36450]\tLoss: 1011.6111\n",
      "Training Epoch: 3 [24750/36450]\tLoss: 1012.4703\n",
      "Training Epoch: 3 [24800/36450]\tLoss: 1020.2791\n",
      "Training Epoch: 3 [24850/36450]\tLoss: 1037.0350\n",
      "Training Epoch: 3 [24900/36450]\tLoss: 958.2495\n",
      "Training Epoch: 3 [24950/36450]\tLoss: 959.8029\n",
      "Training Epoch: 3 [25000/36450]\tLoss: 959.9087\n",
      "Training Epoch: 3 [25050/36450]\tLoss: 1029.9126\n",
      "Training Epoch: 3 [25100/36450]\tLoss: 955.7381\n",
      "Training Epoch: 3 [25150/36450]\tLoss: 1013.4792\n",
      "Training Epoch: 3 [25200/36450]\tLoss: 1015.1323\n",
      "Training Epoch: 3 [25250/36450]\tLoss: 934.0159\n",
      "Training Epoch: 3 [25300/36450]\tLoss: 1025.9077\n",
      "Training Epoch: 3 [25350/36450]\tLoss: 997.7920\n",
      "Training Epoch: 3 [25400/36450]\tLoss: 991.6154\n",
      "Training Epoch: 3 [25450/36450]\tLoss: 983.3005\n",
      "Training Epoch: 3 [25500/36450]\tLoss: 950.6119\n",
      "Training Epoch: 3 [25550/36450]\tLoss: 989.2034\n",
      "Training Epoch: 3 [25600/36450]\tLoss: 990.6323\n",
      "Training Epoch: 3 [25650/36450]\tLoss: 961.5410\n",
      "Training Epoch: 3 [25700/36450]\tLoss: 1012.1565\n",
      "Training Epoch: 3 [25750/36450]\tLoss: 961.3214\n",
      "Training Epoch: 3 [25800/36450]\tLoss: 966.3628\n",
      "Training Epoch: 3 [25850/36450]\tLoss: 987.4758\n",
      "Training Epoch: 3 [25900/36450]\tLoss: 1016.9741\n",
      "Training Epoch: 3 [25950/36450]\tLoss: 998.1304\n",
      "Training Epoch: 3 [26000/36450]\tLoss: 936.4833\n",
      "Training Epoch: 3 [26050/36450]\tLoss: 966.5173\n",
      "Training Epoch: 3 [26100/36450]\tLoss: 957.4661\n",
      "Training Epoch: 3 [26150/36450]\tLoss: 951.9207\n",
      "Training Epoch: 3 [26200/36450]\tLoss: 1017.1456\n",
      "Training Epoch: 3 [26250/36450]\tLoss: 986.9271\n",
      "Training Epoch: 3 [26300/36450]\tLoss: 958.5427\n",
      "Training Epoch: 3 [26350/36450]\tLoss: 952.4201\n",
      "Training Epoch: 3 [26400/36450]\tLoss: 966.5421\n",
      "Training Epoch: 3 [26450/36450]\tLoss: 1005.0379\n",
      "Training Epoch: 3 [26500/36450]\tLoss: 1016.1009\n",
      "Training Epoch: 3 [26550/36450]\tLoss: 1030.9095\n",
      "Training Epoch: 3 [26600/36450]\tLoss: 990.9106\n",
      "Training Epoch: 3 [26650/36450]\tLoss: 1000.3979\n",
      "Training Epoch: 3 [26700/36450]\tLoss: 959.1359\n",
      "Training Epoch: 3 [26750/36450]\tLoss: 954.6879\n",
      "Training Epoch: 3 [26800/36450]\tLoss: 997.5930\n",
      "Training Epoch: 3 [26850/36450]\tLoss: 991.4098\n",
      "Training Epoch: 3 [26900/36450]\tLoss: 1032.0797\n",
      "Training Epoch: 3 [26950/36450]\tLoss: 953.5892\n",
      "Training Epoch: 3 [27000/36450]\tLoss: 938.8990\n",
      "Training Epoch: 3 [27050/36450]\tLoss: 961.0884\n",
      "Training Epoch: 3 [27100/36450]\tLoss: 968.6698\n",
      "Training Epoch: 3 [27150/36450]\tLoss: 995.2792\n",
      "Training Epoch: 3 [27200/36450]\tLoss: 986.9399\n",
      "Training Epoch: 3 [27250/36450]\tLoss: 959.7593\n",
      "Training Epoch: 3 [27300/36450]\tLoss: 1050.1403\n",
      "Training Epoch: 3 [27350/36450]\tLoss: 976.3357\n",
      "Training Epoch: 3 [27400/36450]\tLoss: 996.7990\n",
      "Training Epoch: 3 [27450/36450]\tLoss: 1058.7845\n",
      "Training Epoch: 3 [27500/36450]\tLoss: 1043.6637\n",
      "Training Epoch: 3 [27550/36450]\tLoss: 994.5145\n",
      "Training Epoch: 3 [27600/36450]\tLoss: 988.7939\n",
      "Training Epoch: 3 [27650/36450]\tLoss: 1006.6558\n",
      "Training Epoch: 3 [27700/36450]\tLoss: 931.6331\n",
      "Training Epoch: 3 [27750/36450]\tLoss: 1083.5804\n",
      "Training Epoch: 3 [27800/36450]\tLoss: 1000.9667\n",
      "Training Epoch: 3 [27850/36450]\tLoss: 971.0172\n",
      "Training Epoch: 3 [27900/36450]\tLoss: 977.5193\n",
      "Training Epoch: 3 [27950/36450]\tLoss: 958.7784\n",
      "Training Epoch: 3 [28000/36450]\tLoss: 999.5859\n",
      "Training Epoch: 3 [28050/36450]\tLoss: 988.6194\n",
      "Training Epoch: 3 [28100/36450]\tLoss: 987.3765\n",
      "Training Epoch: 3 [28150/36450]\tLoss: 984.7351\n",
      "Training Epoch: 3 [28200/36450]\tLoss: 1027.0575\n",
      "Training Epoch: 3 [28250/36450]\tLoss: 984.9018\n",
      "Training Epoch: 3 [28300/36450]\tLoss: 979.8896\n",
      "Training Epoch: 3 [28350/36450]\tLoss: 1012.8522\n",
      "Training Epoch: 3 [28400/36450]\tLoss: 982.4764\n",
      "Training Epoch: 3 [28450/36450]\tLoss: 967.1955\n",
      "Training Epoch: 3 [28500/36450]\tLoss: 986.7901\n",
      "Training Epoch: 3 [28550/36450]\tLoss: 1006.5128\n",
      "Training Epoch: 3 [28600/36450]\tLoss: 982.0352\n",
      "Training Epoch: 3 [28650/36450]\tLoss: 973.4061\n",
      "Training Epoch: 3 [28700/36450]\tLoss: 999.1949\n",
      "Training Epoch: 3 [28750/36450]\tLoss: 991.1150\n",
      "Training Epoch: 3 [28800/36450]\tLoss: 964.6229\n",
      "Training Epoch: 3 [28850/36450]\tLoss: 957.7109\n",
      "Training Epoch: 3 [28900/36450]\tLoss: 1025.9023\n",
      "Training Epoch: 3 [28950/36450]\tLoss: 998.5264\n",
      "Training Epoch: 3 [29000/36450]\tLoss: 967.6884\n",
      "Training Epoch: 3 [29050/36450]\tLoss: 955.3132\n",
      "Training Epoch: 3 [29100/36450]\tLoss: 975.7056\n",
      "Training Epoch: 3 [29150/36450]\tLoss: 962.8407\n",
      "Training Epoch: 3 [29200/36450]\tLoss: 977.6407\n",
      "Training Epoch: 3 [29250/36450]\tLoss: 907.7803\n",
      "Training Epoch: 3 [29300/36450]\tLoss: 930.3168\n",
      "Training Epoch: 3 [29350/36450]\tLoss: 961.3566\n",
      "Training Epoch: 3 [29400/36450]\tLoss: 1010.6425\n",
      "Training Epoch: 3 [29450/36450]\tLoss: 969.3783\n",
      "Training Epoch: 3 [29500/36450]\tLoss: 976.3234\n",
      "Training Epoch: 3 [29550/36450]\tLoss: 1043.0095\n",
      "Training Epoch: 3 [29600/36450]\tLoss: 916.3301\n",
      "Training Epoch: 3 [29650/36450]\tLoss: 956.2542\n",
      "Training Epoch: 3 [29700/36450]\tLoss: 954.1165\n",
      "Training Epoch: 3 [29750/36450]\tLoss: 954.4377\n",
      "Training Epoch: 3 [29800/36450]\tLoss: 990.2186\n",
      "Training Epoch: 3 [29850/36450]\tLoss: 964.5687\n",
      "Training Epoch: 3 [29900/36450]\tLoss: 970.3263\n",
      "Training Epoch: 3 [29950/36450]\tLoss: 966.7040\n",
      "Training Epoch: 3 [30000/36450]\tLoss: 968.4581\n",
      "Training Epoch: 3 [30050/36450]\tLoss: 987.4658\n",
      "Training Epoch: 3 [30100/36450]\tLoss: 968.4461\n",
      "Training Epoch: 3 [30150/36450]\tLoss: 945.2182\n",
      "Training Epoch: 3 [30200/36450]\tLoss: 991.6454\n",
      "Training Epoch: 3 [30250/36450]\tLoss: 902.4433\n",
      "Training Epoch: 3 [30300/36450]\tLoss: 963.2655\n",
      "Training Epoch: 3 [30350/36450]\tLoss: 983.7844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [30400/36450]\tLoss: 991.0266\n",
      "Training Epoch: 3 [30450/36450]\tLoss: 961.5561\n",
      "Training Epoch: 3 [30500/36450]\tLoss: 947.1082\n",
      "Training Epoch: 3 [30550/36450]\tLoss: 937.6625\n",
      "Training Epoch: 3 [30600/36450]\tLoss: 948.1291\n",
      "Training Epoch: 3 [30650/36450]\tLoss: 1009.5859\n",
      "Training Epoch: 3 [30700/36450]\tLoss: 952.4336\n",
      "Training Epoch: 3 [30750/36450]\tLoss: 1033.8210\n",
      "Training Epoch: 3 [30800/36450]\tLoss: 958.4945\n",
      "Training Epoch: 3 [30850/36450]\tLoss: 932.3564\n",
      "Training Epoch: 3 [30900/36450]\tLoss: 989.6155\n",
      "Training Epoch: 3 [30950/36450]\tLoss: 966.1700\n",
      "Training Epoch: 3 [31000/36450]\tLoss: 983.9501\n",
      "Training Epoch: 3 [31050/36450]\tLoss: 968.0428\n",
      "Training Epoch: 3 [31100/36450]\tLoss: 908.0108\n",
      "Training Epoch: 3 [31150/36450]\tLoss: 906.9970\n",
      "Training Epoch: 3 [31200/36450]\tLoss: 971.3617\n",
      "Training Epoch: 3 [31250/36450]\tLoss: 917.6325\n",
      "Training Epoch: 3 [31300/36450]\tLoss: 936.6488\n",
      "Training Epoch: 3 [31350/36450]\tLoss: 947.1780\n",
      "Training Epoch: 3 [31400/36450]\tLoss: 932.3738\n",
      "Training Epoch: 3 [31450/36450]\tLoss: 931.7958\n",
      "Training Epoch: 3 [31500/36450]\tLoss: 912.5748\n",
      "Training Epoch: 3 [31550/36450]\tLoss: 976.6105\n",
      "Training Epoch: 3 [31600/36450]\tLoss: 907.0191\n",
      "Training Epoch: 3 [31650/36450]\tLoss: 951.0274\n",
      "Training Epoch: 3 [31700/36450]\tLoss: 1008.5966\n",
      "Training Epoch: 3 [31750/36450]\tLoss: 976.0963\n",
      "Training Epoch: 3 [31800/36450]\tLoss: 967.8438\n",
      "Training Epoch: 3 [31850/36450]\tLoss: 939.8698\n",
      "Training Epoch: 3 [31900/36450]\tLoss: 949.3835\n",
      "Training Epoch: 3 [31950/36450]\tLoss: 1030.3751\n",
      "Training Epoch: 3 [32000/36450]\tLoss: 963.4654\n",
      "Training Epoch: 3 [32050/36450]\tLoss: 980.7193\n",
      "Training Epoch: 3 [32100/36450]\tLoss: 953.4255\n",
      "Training Epoch: 3 [32150/36450]\tLoss: 963.2268\n",
      "Training Epoch: 3 [32200/36450]\tLoss: 898.2280\n",
      "Training Epoch: 3 [32250/36450]\tLoss: 905.0920\n",
      "Training Epoch: 3 [32300/36450]\tLoss: 920.7296\n",
      "Training Epoch: 3 [32350/36450]\tLoss: 945.3820\n",
      "Training Epoch: 3 [32400/36450]\tLoss: 1001.0725\n",
      "Training Epoch: 3 [32450/36450]\tLoss: 960.7599\n",
      "Training Epoch: 3 [32500/36450]\tLoss: 960.2719\n",
      "Training Epoch: 3 [32550/36450]\tLoss: 981.7512\n",
      "Training Epoch: 3 [32600/36450]\tLoss: 963.5223\n",
      "Training Epoch: 3 [32650/36450]\tLoss: 983.2115\n",
      "Training Epoch: 3 [32700/36450]\tLoss: 893.0284\n",
      "Training Epoch: 3 [32750/36450]\tLoss: 930.9214\n",
      "Training Epoch: 3 [32800/36450]\tLoss: 966.3894\n",
      "Training Epoch: 3 [32850/36450]\tLoss: 928.2318\n",
      "Training Epoch: 3 [32900/36450]\tLoss: 980.1275\n",
      "Training Epoch: 3 [32950/36450]\tLoss: 994.2811\n",
      "Training Epoch: 3 [33000/36450]\tLoss: 980.2943\n",
      "Training Epoch: 3 [33050/36450]\tLoss: 991.9069\n",
      "Training Epoch: 3 [33100/36450]\tLoss: 898.4766\n",
      "Training Epoch: 3 [33150/36450]\tLoss: 906.2535\n",
      "Training Epoch: 3 [33200/36450]\tLoss: 938.8148\n",
      "Training Epoch: 3 [33250/36450]\tLoss: 961.6573\n",
      "Training Epoch: 3 [33300/36450]\tLoss: 942.6436\n",
      "Training Epoch: 3 [33350/36450]\tLoss: 1004.9018\n",
      "Training Epoch: 3 [33400/36450]\tLoss: 975.0868\n",
      "Training Epoch: 3 [33450/36450]\tLoss: 944.5052\n",
      "Training Epoch: 3 [33500/36450]\tLoss: 934.3176\n",
      "Training Epoch: 3 [33550/36450]\tLoss: 977.0804\n",
      "Training Epoch: 3 [33600/36450]\tLoss: 901.4046\n",
      "Training Epoch: 3 [33650/36450]\tLoss: 906.8191\n",
      "Training Epoch: 3 [33700/36450]\tLoss: 921.8687\n",
      "Training Epoch: 3 [33750/36450]\tLoss: 928.2787\n",
      "Training Epoch: 3 [33800/36450]\tLoss: 1021.3627\n",
      "Training Epoch: 3 [33850/36450]\tLoss: 970.1351\n",
      "Training Epoch: 3 [33900/36450]\tLoss: 942.7693\n",
      "Training Epoch: 3 [33950/36450]\tLoss: 948.7958\n",
      "Training Epoch: 3 [34000/36450]\tLoss: 964.9970\n",
      "Training Epoch: 3 [34050/36450]\tLoss: 930.9743\n",
      "Training Epoch: 3 [34100/36450]\tLoss: 898.3633\n",
      "Training Epoch: 3 [34150/36450]\tLoss: 957.9940\n",
      "Training Epoch: 3 [34200/36450]\tLoss: 960.9425\n",
      "Training Epoch: 3 [34250/36450]\tLoss: 950.0634\n",
      "Training Epoch: 3 [34300/36450]\tLoss: 909.8043\n",
      "Training Epoch: 3 [34350/36450]\tLoss: 924.2195\n",
      "Training Epoch: 3 [34400/36450]\tLoss: 944.6343\n",
      "Training Epoch: 3 [34450/36450]\tLoss: 982.2333\n",
      "Training Epoch: 3 [34500/36450]\tLoss: 963.1472\n",
      "Training Epoch: 3 [34550/36450]\tLoss: 946.6621\n",
      "Training Epoch: 3 [34600/36450]\tLoss: 953.9960\n",
      "Training Epoch: 3 [34650/36450]\tLoss: 984.7350\n",
      "Training Epoch: 3 [34700/36450]\tLoss: 977.3470\n",
      "Training Epoch: 3 [34750/36450]\tLoss: 917.4579\n",
      "Training Epoch: 3 [34800/36450]\tLoss: 953.5565\n",
      "Training Epoch: 3 [34850/36450]\tLoss: 963.4789\n",
      "Training Epoch: 3 [34900/36450]\tLoss: 975.0037\n",
      "Training Epoch: 3 [34950/36450]\tLoss: 964.1340\n",
      "Training Epoch: 3 [35000/36450]\tLoss: 910.6531\n",
      "Training Epoch: 3 [35050/36450]\tLoss: 937.5686\n",
      "Training Epoch: 3 [35100/36450]\tLoss: 872.1974\n",
      "Training Epoch: 3 [35150/36450]\tLoss: 911.7621\n",
      "Training Epoch: 3 [35200/36450]\tLoss: 951.1879\n",
      "Training Epoch: 3 [35250/36450]\tLoss: 944.9690\n",
      "Training Epoch: 3 [35300/36450]\tLoss: 961.3915\n",
      "Training Epoch: 3 [35350/36450]\tLoss: 935.4568\n",
      "Training Epoch: 3 [35400/36450]\tLoss: 942.2081\n",
      "Training Epoch: 3 [35450/36450]\tLoss: 1019.7935\n",
      "Training Epoch: 3 [35500/36450]\tLoss: 955.7272\n",
      "Training Epoch: 3 [35550/36450]\tLoss: 935.5460\n",
      "Training Epoch: 3 [35600/36450]\tLoss: 943.9005\n",
      "Training Epoch: 3 [35650/36450]\tLoss: 940.7726\n",
      "Training Epoch: 3 [35700/36450]\tLoss: 1003.2256\n",
      "Training Epoch: 3 [35750/36450]\tLoss: 950.4169\n",
      "Training Epoch: 3 [35800/36450]\tLoss: 941.5486\n",
      "Training Epoch: 3 [35850/36450]\tLoss: 1008.6707\n",
      "Training Epoch: 3 [35900/36450]\tLoss: 978.7015\n",
      "Training Epoch: 3 [35950/36450]\tLoss: 932.8698\n",
      "Training Epoch: 3 [36000/36450]\tLoss: 930.9899\n",
      "Training Epoch: 3 [36050/36450]\tLoss: 927.8212\n",
      "Training Epoch: 3 [36100/36450]\tLoss: 941.2750\n",
      "Training Epoch: 3 [36150/36450]\tLoss: 964.2271\n",
      "Training Epoch: 3 [36200/36450]\tLoss: 929.6509\n",
      "Training Epoch: 3 [36250/36450]\tLoss: 967.2701\n",
      "Training Epoch: 3 [36300/36450]\tLoss: 940.6025\n",
      "Training Epoch: 3 [36350/36450]\tLoss: 1008.5709\n",
      "Training Epoch: 3 [36400/36450]\tLoss: 952.4083\n",
      "Training Epoch: 3 [36450/36450]\tLoss: 979.4698\n",
      "Training Epoch: 3 [4050/4050]\tLoss: 476.1810\n",
      "Training Epoch: 4 [50/36450]\tLoss: 936.7408\n",
      "Training Epoch: 4 [100/36450]\tLoss: 953.3812\n",
      "Training Epoch: 4 [150/36450]\tLoss: 920.2557\n",
      "Training Epoch: 4 [200/36450]\tLoss: 931.1814\n",
      "Training Epoch: 4 [250/36450]\tLoss: 941.6725\n",
      "Training Epoch: 4 [300/36450]\tLoss: 898.4014\n",
      "Training Epoch: 4 [350/36450]\tLoss: 931.2888\n",
      "Training Epoch: 4 [400/36450]\tLoss: 928.0019\n",
      "Training Epoch: 4 [450/36450]\tLoss: 939.3474\n",
      "Training Epoch: 4 [500/36450]\tLoss: 920.7327\n",
      "Training Epoch: 4 [550/36450]\tLoss: 979.0718\n",
      "Training Epoch: 4 [600/36450]\tLoss: 942.4344\n",
      "Training Epoch: 4 [650/36450]\tLoss: 917.2026\n",
      "Training Epoch: 4 [700/36450]\tLoss: 888.7338\n",
      "Training Epoch: 4 [750/36450]\tLoss: 946.1036\n",
      "Training Epoch: 4 [800/36450]\tLoss: 909.0621\n",
      "Training Epoch: 4 [850/36450]\tLoss: 963.0792\n",
      "Training Epoch: 4 [900/36450]\tLoss: 906.9932\n",
      "Training Epoch: 4 [950/36450]\tLoss: 920.9738\n",
      "Training Epoch: 4 [1000/36450]\tLoss: 946.0366\n",
      "Training Epoch: 4 [1050/36450]\tLoss: 934.7797\n",
      "Training Epoch: 4 [1100/36450]\tLoss: 945.5251\n",
      "Training Epoch: 4 [1150/36450]\tLoss: 947.3930\n",
      "Training Epoch: 4 [1200/36450]\tLoss: 936.2988\n",
      "Training Epoch: 4 [1250/36450]\tLoss: 933.7837\n",
      "Training Epoch: 4 [1300/36450]\tLoss: 926.6770\n",
      "Training Epoch: 4 [1350/36450]\tLoss: 899.3770\n",
      "Training Epoch: 4 [1400/36450]\tLoss: 931.8945\n",
      "Training Epoch: 4 [1450/36450]\tLoss: 948.8438\n",
      "Training Epoch: 4 [1500/36450]\tLoss: 944.1702\n",
      "Training Epoch: 4 [1550/36450]\tLoss: 934.3768\n",
      "Training Epoch: 4 [1600/36450]\tLoss: 972.2460\n",
      "Training Epoch: 4 [1650/36450]\tLoss: 913.7560\n",
      "Training Epoch: 4 [1700/36450]\tLoss: 956.1550\n",
      "Training Epoch: 4 [1750/36450]\tLoss: 971.6776\n",
      "Training Epoch: 4 [1800/36450]\tLoss: 951.7822\n",
      "Training Epoch: 4 [1850/36450]\tLoss: 894.4154\n",
      "Training Epoch: 4 [1900/36450]\tLoss: 937.9033\n",
      "Training Epoch: 4 [1950/36450]\tLoss: 864.0989\n",
      "Training Epoch: 4 [2000/36450]\tLoss: 909.6828\n",
      "Training Epoch: 4 [2050/36450]\tLoss: 970.1823\n",
      "Training Epoch: 4 [2100/36450]\tLoss: 896.9966\n",
      "Training Epoch: 4 [2150/36450]\tLoss: 924.6163\n",
      "Training Epoch: 4 [2200/36450]\tLoss: 929.5920\n",
      "Training Epoch: 4 [2250/36450]\tLoss: 940.3600\n",
      "Training Epoch: 4 [2300/36450]\tLoss: 909.3965\n",
      "Training Epoch: 4 [2350/36450]\tLoss: 967.8698\n",
      "Training Epoch: 4 [2400/36450]\tLoss: 865.9551\n",
      "Training Epoch: 4 [2450/36450]\tLoss: 931.0588\n",
      "Training Epoch: 4 [2500/36450]\tLoss: 915.5656\n",
      "Training Epoch: 4 [2550/36450]\tLoss: 901.7830\n",
      "Training Epoch: 4 [2600/36450]\tLoss: 975.0253\n",
      "Training Epoch: 4 [2650/36450]\tLoss: 956.0640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [2700/36450]\tLoss: 975.0549\n",
      "Training Epoch: 4 [2750/36450]\tLoss: 908.6481\n",
      "Training Epoch: 4 [2800/36450]\tLoss: 926.2916\n",
      "Training Epoch: 4 [2850/36450]\tLoss: 936.0355\n",
      "Training Epoch: 4 [2900/36450]\tLoss: 947.9439\n",
      "Training Epoch: 4 [2950/36450]\tLoss: 978.7862\n",
      "Training Epoch: 4 [3000/36450]\tLoss: 905.4772\n",
      "Training Epoch: 4 [3050/36450]\tLoss: 991.1775\n",
      "Training Epoch: 4 [3100/36450]\tLoss: 860.3416\n",
      "Training Epoch: 4 [3150/36450]\tLoss: 942.6316\n",
      "Training Epoch: 4 [3200/36450]\tLoss: 982.6373\n",
      "Training Epoch: 4 [3250/36450]\tLoss: 941.7480\n",
      "Training Epoch: 4 [3300/36450]\tLoss: 949.5589\n",
      "Training Epoch: 4 [3350/36450]\tLoss: 895.3493\n",
      "Training Epoch: 4 [3400/36450]\tLoss: 940.0512\n",
      "Training Epoch: 4 [3450/36450]\tLoss: 934.0036\n",
      "Training Epoch: 4 [3500/36450]\tLoss: 909.0422\n",
      "Training Epoch: 4 [3550/36450]\tLoss: 983.5269\n",
      "Training Epoch: 4 [3600/36450]\tLoss: 901.3528\n",
      "Training Epoch: 4 [3650/36450]\tLoss: 908.5601\n",
      "Training Epoch: 4 [3700/36450]\tLoss: 877.6961\n",
      "Training Epoch: 4 [3750/36450]\tLoss: 919.7930\n",
      "Training Epoch: 4 [3800/36450]\tLoss: 912.3574\n",
      "Training Epoch: 4 [3850/36450]\tLoss: 908.1434\n",
      "Training Epoch: 4 [3900/36450]\tLoss: 904.0070\n",
      "Training Epoch: 4 [3950/36450]\tLoss: 907.2400\n",
      "Training Epoch: 4 [4000/36450]\tLoss: 928.9033\n",
      "Training Epoch: 4 [4050/36450]\tLoss: 954.9644\n",
      "Training Epoch: 4 [4100/36450]\tLoss: 917.2417\n",
      "Training Epoch: 4 [4150/36450]\tLoss: 925.0653\n",
      "Training Epoch: 4 [4200/36450]\tLoss: 933.4235\n",
      "Training Epoch: 4 [4250/36450]\tLoss: 962.8270\n",
      "Training Epoch: 4 [4300/36450]\tLoss: 992.3723\n",
      "Training Epoch: 4 [4350/36450]\tLoss: 875.1257\n",
      "Training Epoch: 4 [4400/36450]\tLoss: 922.6342\n",
      "Training Epoch: 4 [4450/36450]\tLoss: 916.5233\n",
      "Training Epoch: 4 [4500/36450]\tLoss: 916.3475\n",
      "Training Epoch: 4 [4550/36450]\tLoss: 903.6508\n",
      "Training Epoch: 4 [4600/36450]\tLoss: 960.3238\n",
      "Training Epoch: 4 [4650/36450]\tLoss: 1000.8878\n",
      "Training Epoch: 4 [4700/36450]\tLoss: 907.2872\n",
      "Training Epoch: 4 [4750/36450]\tLoss: 970.2294\n",
      "Training Epoch: 4 [4800/36450]\tLoss: 956.2263\n",
      "Training Epoch: 4 [4850/36450]\tLoss: 896.3498\n",
      "Training Epoch: 4 [4900/36450]\tLoss: 957.1745\n",
      "Training Epoch: 4 [4950/36450]\tLoss: 943.3193\n",
      "Training Epoch: 4 [5000/36450]\tLoss: 929.7405\n",
      "Training Epoch: 4 [5050/36450]\tLoss: 913.9500\n",
      "Training Epoch: 4 [5100/36450]\tLoss: 919.2403\n",
      "Training Epoch: 4 [5150/36450]\tLoss: 934.2178\n",
      "Training Epoch: 4 [5200/36450]\tLoss: 935.7477\n",
      "Training Epoch: 4 [5250/36450]\tLoss: 876.4899\n",
      "Training Epoch: 4 [5300/36450]\tLoss: 880.1289\n",
      "Training Epoch: 4 [5350/36450]\tLoss: 885.2833\n",
      "Training Epoch: 4 [5400/36450]\tLoss: 924.8659\n",
      "Training Epoch: 4 [5450/36450]\tLoss: 899.3157\n",
      "Training Epoch: 4 [5500/36450]\tLoss: 902.8862\n",
      "Training Epoch: 4 [5550/36450]\tLoss: 943.8218\n",
      "Training Epoch: 4 [5600/36450]\tLoss: 898.4444\n",
      "Training Epoch: 4 [5650/36450]\tLoss: 881.9435\n",
      "Training Epoch: 4 [5700/36450]\tLoss: 936.3228\n",
      "Training Epoch: 4 [5750/36450]\tLoss: 884.7852\n",
      "Training Epoch: 4 [5800/36450]\tLoss: 922.3994\n",
      "Training Epoch: 4 [5850/36450]\tLoss: 916.0804\n",
      "Training Epoch: 4 [5900/36450]\tLoss: 877.2861\n",
      "Training Epoch: 4 [5950/36450]\tLoss: 930.7205\n",
      "Training Epoch: 4 [6000/36450]\tLoss: 877.1618\n",
      "Training Epoch: 4 [6050/36450]\tLoss: 940.5375\n",
      "Training Epoch: 4 [6100/36450]\tLoss: 946.8047\n",
      "Training Epoch: 4 [6150/36450]\tLoss: 938.2073\n",
      "Training Epoch: 4 [6200/36450]\tLoss: 920.8190\n",
      "Training Epoch: 4 [6250/36450]\tLoss: 941.2451\n",
      "Training Epoch: 4 [6300/36450]\tLoss: 900.2785\n",
      "Training Epoch: 4 [6350/36450]\tLoss: 917.3798\n",
      "Training Epoch: 4 [6400/36450]\tLoss: 947.5464\n",
      "Training Epoch: 4 [6450/36450]\tLoss: 896.5582\n",
      "Training Epoch: 4 [6500/36450]\tLoss: 963.2404\n",
      "Training Epoch: 4 [6550/36450]\tLoss: 857.8618\n",
      "Training Epoch: 4 [6600/36450]\tLoss: 916.0374\n",
      "Training Epoch: 4 [6650/36450]\tLoss: 927.7836\n",
      "Training Epoch: 4 [6700/36450]\tLoss: 910.2870\n",
      "Training Epoch: 4 [6750/36450]\tLoss: 869.4364\n",
      "Training Epoch: 4 [6800/36450]\tLoss: 990.3485\n",
      "Training Epoch: 4 [6850/36450]\tLoss: 909.8867\n",
      "Training Epoch: 4 [6900/36450]\tLoss: 980.1729\n",
      "Training Epoch: 4 [6950/36450]\tLoss: 875.3858\n",
      "Training Epoch: 4 [7000/36450]\tLoss: 977.6141\n",
      "Training Epoch: 4 [7050/36450]\tLoss: 882.4080\n",
      "Training Epoch: 4 [7100/36450]\tLoss: 925.1664\n",
      "Training Epoch: 4 [7150/36450]\tLoss: 948.4667\n",
      "Training Epoch: 4 [7200/36450]\tLoss: 919.4354\n",
      "Training Epoch: 4 [7250/36450]\tLoss: 929.3580\n",
      "Training Epoch: 4 [7300/36450]\tLoss: 833.9752\n",
      "Training Epoch: 4 [7350/36450]\tLoss: 949.7706\n",
      "Training Epoch: 4 [7400/36450]\tLoss: 960.2302\n",
      "Training Epoch: 4 [7450/36450]\tLoss: 901.9894\n",
      "Training Epoch: 4 [7500/36450]\tLoss: 877.4412\n",
      "Training Epoch: 4 [7550/36450]\tLoss: 875.1738\n",
      "Training Epoch: 4 [7600/36450]\tLoss: 936.0826\n",
      "Training Epoch: 4 [7650/36450]\tLoss: 920.5649\n",
      "Training Epoch: 4 [7700/36450]\tLoss: 938.2855\n",
      "Training Epoch: 4 [7750/36450]\tLoss: 908.1962\n",
      "Training Epoch: 4 [7800/36450]\tLoss: 919.0198\n",
      "Training Epoch: 4 [7850/36450]\tLoss: 897.1658\n",
      "Training Epoch: 4 [7900/36450]\tLoss: 900.0480\n",
      "Training Epoch: 4 [7950/36450]\tLoss: 950.7302\n",
      "Training Epoch: 4 [8000/36450]\tLoss: 930.8890\n",
      "Training Epoch: 4 [8050/36450]\tLoss: 884.6688\n",
      "Training Epoch: 4 [8100/36450]\tLoss: 947.4376\n",
      "Training Epoch: 4 [8150/36450]\tLoss: 911.2110\n",
      "Training Epoch: 4 [8200/36450]\tLoss: 954.7837\n",
      "Training Epoch: 4 [8250/36450]\tLoss: 961.5096\n",
      "Training Epoch: 4 [8300/36450]\tLoss: 900.8190\n",
      "Training Epoch: 4 [8350/36450]\tLoss: 915.6824\n",
      "Training Epoch: 4 [8400/36450]\tLoss: 916.7872\n",
      "Training Epoch: 4 [8450/36450]\tLoss: 910.1118\n",
      "Training Epoch: 4 [8500/36450]\tLoss: 937.9585\n",
      "Training Epoch: 4 [8550/36450]\tLoss: 942.2968\n",
      "Training Epoch: 4 [8600/36450]\tLoss: 841.4694\n",
      "Training Epoch: 4 [8650/36450]\tLoss: 972.0891\n",
      "Training Epoch: 4 [8700/36450]\tLoss: 906.5134\n",
      "Training Epoch: 4 [8750/36450]\tLoss: 930.1361\n",
      "Training Epoch: 4 [8800/36450]\tLoss: 926.9644\n",
      "Training Epoch: 4 [8850/36450]\tLoss: 915.1082\n",
      "Training Epoch: 4 [8900/36450]\tLoss: 893.8251\n",
      "Training Epoch: 4 [8950/36450]\tLoss: 885.3494\n",
      "Training Epoch: 4 [9000/36450]\tLoss: 900.6874\n",
      "Training Epoch: 4 [9050/36450]\tLoss: 907.6542\n",
      "Training Epoch: 4 [9100/36450]\tLoss: 858.5427\n",
      "Training Epoch: 4 [9150/36450]\tLoss: 887.1487\n",
      "Training Epoch: 4 [9200/36450]\tLoss: 913.6069\n",
      "Training Epoch: 4 [9250/36450]\tLoss: 944.5140\n",
      "Training Epoch: 4 [9300/36450]\tLoss: 935.6461\n",
      "Training Epoch: 4 [9350/36450]\tLoss: 943.4337\n",
      "Training Epoch: 4 [9400/36450]\tLoss: 863.4728\n",
      "Training Epoch: 4 [9450/36450]\tLoss: 907.8708\n",
      "Training Epoch: 4 [9500/36450]\tLoss: 949.2961\n",
      "Training Epoch: 4 [9550/36450]\tLoss: 907.3981\n",
      "Training Epoch: 4 [9600/36450]\tLoss: 909.5079\n",
      "Training Epoch: 4 [9650/36450]\tLoss: 919.6542\n",
      "Training Epoch: 4 [9700/36450]\tLoss: 868.9899\n",
      "Training Epoch: 4 [9750/36450]\tLoss: 908.3832\n",
      "Training Epoch: 4 [9800/36450]\tLoss: 912.0249\n",
      "Training Epoch: 4 [9850/36450]\tLoss: 907.1411\n",
      "Training Epoch: 4 [9900/36450]\tLoss: 907.8157\n",
      "Training Epoch: 4 [9950/36450]\tLoss: 924.8878\n",
      "Training Epoch: 4 [10000/36450]\tLoss: 891.0673\n",
      "Training Epoch: 4 [10050/36450]\tLoss: 911.6885\n",
      "Training Epoch: 4 [10100/36450]\tLoss: 856.4806\n",
      "Training Epoch: 4 [10150/36450]\tLoss: 882.5856\n",
      "Training Epoch: 4 [10200/36450]\tLoss: 886.2441\n",
      "Training Epoch: 4 [10250/36450]\tLoss: 850.3189\n",
      "Training Epoch: 4 [10300/36450]\tLoss: 919.1195\n",
      "Training Epoch: 4 [10350/36450]\tLoss: 882.1451\n",
      "Training Epoch: 4 [10400/36450]\tLoss: 1024.7745\n",
      "Training Epoch: 4 [10450/36450]\tLoss: 929.7281\n",
      "Training Epoch: 4 [10500/36450]\tLoss: 941.2173\n",
      "Training Epoch: 4 [10550/36450]\tLoss: 892.0272\n",
      "Training Epoch: 4 [10600/36450]\tLoss: 942.9905\n",
      "Training Epoch: 4 [10650/36450]\tLoss: 931.3351\n",
      "Training Epoch: 4 [10700/36450]\tLoss: 915.8405\n",
      "Training Epoch: 4 [10750/36450]\tLoss: 921.8345\n",
      "Training Epoch: 4 [10800/36450]\tLoss: 898.2559\n",
      "Training Epoch: 4 [10850/36450]\tLoss: 898.2679\n",
      "Training Epoch: 4 [10900/36450]\tLoss: 922.0179\n",
      "Training Epoch: 4 [10950/36450]\tLoss: 908.9414\n",
      "Training Epoch: 4 [11000/36450]\tLoss: 924.7855\n",
      "Training Epoch: 4 [11050/36450]\tLoss: 920.4946\n",
      "Training Epoch: 4 [11100/36450]\tLoss: 879.4927\n",
      "Training Epoch: 4 [11150/36450]\tLoss: 892.5552\n",
      "Training Epoch: 4 [11200/36450]\tLoss: 955.8187\n",
      "Training Epoch: 4 [11250/36450]\tLoss: 921.9183\n",
      "Training Epoch: 4 [11300/36450]\tLoss: 892.8157\n",
      "Training Epoch: 4 [11350/36450]\tLoss: 862.1979\n",
      "Training Epoch: 4 [11400/36450]\tLoss: 900.9297\n",
      "Training Epoch: 4 [11450/36450]\tLoss: 894.4861\n",
      "Training Epoch: 4 [11500/36450]\tLoss: 907.5829\n",
      "Training Epoch: 4 [11550/36450]\tLoss: 915.2169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [11600/36450]\tLoss: 908.1287\n",
      "Training Epoch: 4 [11650/36450]\tLoss: 891.2799\n",
      "Training Epoch: 4 [11700/36450]\tLoss: 938.6931\n",
      "Training Epoch: 4 [11750/36450]\tLoss: 935.9667\n",
      "Training Epoch: 4 [11800/36450]\tLoss: 912.4154\n",
      "Training Epoch: 4 [11850/36450]\tLoss: 937.0679\n",
      "Training Epoch: 4 [11900/36450]\tLoss: 932.7117\n",
      "Training Epoch: 4 [11950/36450]\tLoss: 856.7655\n",
      "Training Epoch: 4 [12000/36450]\tLoss: 849.5220\n",
      "Training Epoch: 4 [12050/36450]\tLoss: 923.2878\n",
      "Training Epoch: 4 [12100/36450]\tLoss: 982.9518\n",
      "Training Epoch: 4 [12150/36450]\tLoss: 923.2321\n",
      "Training Epoch: 4 [12200/36450]\tLoss: 836.3748\n",
      "Training Epoch: 4 [12250/36450]\tLoss: 898.2771\n",
      "Training Epoch: 4 [12300/36450]\tLoss: 845.2947\n",
      "Training Epoch: 4 [12350/36450]\tLoss: 901.4810\n",
      "Training Epoch: 4 [12400/36450]\tLoss: 925.4638\n",
      "Training Epoch: 4 [12450/36450]\tLoss: 879.5345\n",
      "Training Epoch: 4 [12500/36450]\tLoss: 890.2755\n",
      "Training Epoch: 4 [12550/36450]\tLoss: 910.8599\n",
      "Training Epoch: 4 [12600/36450]\tLoss: 846.3652\n",
      "Training Epoch: 4 [12650/36450]\tLoss: 950.1925\n",
      "Training Epoch: 4 [12700/36450]\tLoss: 879.2834\n",
      "Training Epoch: 4 [12750/36450]\tLoss: 878.1735\n",
      "Training Epoch: 4 [12800/36450]\tLoss: 912.7013\n",
      "Training Epoch: 4 [12850/36450]\tLoss: 909.2115\n",
      "Training Epoch: 4 [12900/36450]\tLoss: 873.1555\n",
      "Training Epoch: 4 [12950/36450]\tLoss: 947.8070\n",
      "Training Epoch: 4 [13000/36450]\tLoss: 932.7083\n",
      "Training Epoch: 4 [13050/36450]\tLoss: 891.0477\n",
      "Training Epoch: 4 [13100/36450]\tLoss: 963.4990\n",
      "Training Epoch: 4 [13150/36450]\tLoss: 903.9012\n",
      "Training Epoch: 4 [13200/36450]\tLoss: 894.1193\n",
      "Training Epoch: 4 [13250/36450]\tLoss: 879.4026\n",
      "Training Epoch: 4 [13300/36450]\tLoss: 838.9218\n",
      "Training Epoch: 4 [13350/36450]\tLoss: 888.4592\n",
      "Training Epoch: 4 [13400/36450]\tLoss: 953.2994\n",
      "Training Epoch: 4 [13450/36450]\tLoss: 905.9217\n",
      "Training Epoch: 4 [13500/36450]\tLoss: 923.5581\n",
      "Training Epoch: 4 [13550/36450]\tLoss: 907.8312\n",
      "Training Epoch: 4 [13600/36450]\tLoss: 943.9199\n",
      "Training Epoch: 4 [13650/36450]\tLoss: 896.8115\n",
      "Training Epoch: 4 [13700/36450]\tLoss: 896.7230\n",
      "Training Epoch: 4 [13750/36450]\tLoss: 866.1083\n",
      "Training Epoch: 4 [13800/36450]\tLoss: 930.6020\n",
      "Training Epoch: 4 [13850/36450]\tLoss: 892.7798\n",
      "Training Epoch: 4 [13900/36450]\tLoss: 914.7101\n",
      "Training Epoch: 4 [13950/36450]\tLoss: 897.1165\n",
      "Training Epoch: 4 [14000/36450]\tLoss: 882.9274\n",
      "Training Epoch: 4 [14050/36450]\tLoss: 970.0894\n",
      "Training Epoch: 4 [14100/36450]\tLoss: 829.3997\n",
      "Training Epoch: 4 [14150/36450]\tLoss: 871.9895\n",
      "Training Epoch: 4 [14200/36450]\tLoss: 890.6662\n",
      "Training Epoch: 4 [14250/36450]\tLoss: 941.2277\n",
      "Training Epoch: 4 [14300/36450]\tLoss: 853.0339\n",
      "Training Epoch: 4 [14350/36450]\tLoss: 875.8483\n",
      "Training Epoch: 4 [14400/36450]\tLoss: 926.1635\n",
      "Training Epoch: 4 [14450/36450]\tLoss: 875.6007\n",
      "Training Epoch: 4 [14500/36450]\tLoss: 899.0801\n",
      "Training Epoch: 4 [14550/36450]\tLoss: 928.6296\n",
      "Training Epoch: 4 [14600/36450]\tLoss: 917.4827\n",
      "Training Epoch: 4 [14650/36450]\tLoss: 892.0413\n",
      "Training Epoch: 4 [14700/36450]\tLoss: 930.8172\n",
      "Training Epoch: 4 [14750/36450]\tLoss: 899.3311\n",
      "Training Epoch: 4 [14800/36450]\tLoss: 899.3763\n",
      "Training Epoch: 4 [14850/36450]\tLoss: 865.3800\n",
      "Training Epoch: 4 [14900/36450]\tLoss: 879.2168\n",
      "Training Epoch: 4 [14950/36450]\tLoss: 936.0200\n",
      "Training Epoch: 4 [15000/36450]\tLoss: 876.1921\n",
      "Training Epoch: 4 [15050/36450]\tLoss: 911.8207\n",
      "Training Epoch: 4 [15100/36450]\tLoss: 865.6021\n",
      "Training Epoch: 4 [15150/36450]\tLoss: 947.0853\n",
      "Training Epoch: 4 [15200/36450]\tLoss: 910.5329\n",
      "Training Epoch: 4 [15250/36450]\tLoss: 900.6476\n",
      "Training Epoch: 4 [15300/36450]\tLoss: 888.1878\n",
      "Training Epoch: 4 [15350/36450]\tLoss: 899.4122\n",
      "Training Epoch: 4 [15400/36450]\tLoss: 879.9090\n",
      "Training Epoch: 4 [15450/36450]\tLoss: 902.6245\n",
      "Training Epoch: 4 [15500/36450]\tLoss: 882.9083\n",
      "Training Epoch: 4 [15550/36450]\tLoss: 888.5393\n",
      "Training Epoch: 4 [15600/36450]\tLoss: 845.7233\n",
      "Training Epoch: 4 [15650/36450]\tLoss: 877.6207\n",
      "Training Epoch: 4 [15700/36450]\tLoss: 854.7219\n",
      "Training Epoch: 4 [15750/36450]\tLoss: 912.7139\n",
      "Training Epoch: 4 [15800/36450]\tLoss: 897.8259\n",
      "Training Epoch: 4 [15850/36450]\tLoss: 903.7325\n",
      "Training Epoch: 4 [15900/36450]\tLoss: 920.7273\n",
      "Training Epoch: 4 [15950/36450]\tLoss: 851.8537\n",
      "Training Epoch: 4 [16000/36450]\tLoss: 879.9916\n",
      "Training Epoch: 4 [16050/36450]\tLoss: 934.5085\n",
      "Training Epoch: 4 [16100/36450]\tLoss: 862.7672\n",
      "Training Epoch: 4 [16150/36450]\tLoss: 893.0629\n",
      "Training Epoch: 4 [16200/36450]\tLoss: 922.0840\n",
      "Training Epoch: 4 [16250/36450]\tLoss: 903.2261\n",
      "Training Epoch: 4 [16300/36450]\tLoss: 857.5554\n",
      "Training Epoch: 4 [16350/36450]\tLoss: 906.0825\n",
      "Training Epoch: 4 [16400/36450]\tLoss: 937.6230\n",
      "Training Epoch: 4 [16450/36450]\tLoss: 872.7658\n",
      "Training Epoch: 4 [16500/36450]\tLoss: 884.2975\n",
      "Training Epoch: 4 [16550/36450]\tLoss: 924.9039\n",
      "Training Epoch: 4 [16600/36450]\tLoss: 904.2386\n",
      "Training Epoch: 4 [16650/36450]\tLoss: 896.5506\n",
      "Training Epoch: 4 [16700/36450]\tLoss: 834.0754\n",
      "Training Epoch: 4 [16750/36450]\tLoss: 873.2336\n",
      "Training Epoch: 4 [16800/36450]\tLoss: 839.8862\n",
      "Training Epoch: 4 [16850/36450]\tLoss: 906.9781\n",
      "Training Epoch: 4 [16900/36450]\tLoss: 854.8903\n",
      "Training Epoch: 4 [16950/36450]\tLoss: 912.5263\n",
      "Training Epoch: 4 [17000/36450]\tLoss: 885.5013\n",
      "Training Epoch: 4 [17050/36450]\tLoss: 893.6411\n",
      "Training Epoch: 4 [17100/36450]\tLoss: 905.9698\n",
      "Training Epoch: 4 [17150/36450]\tLoss: 922.0776\n",
      "Training Epoch: 4 [17200/36450]\tLoss: 939.9990\n",
      "Training Epoch: 4 [17250/36450]\tLoss: 867.6201\n",
      "Training Epoch: 4 [17300/36450]\tLoss: 948.7198\n",
      "Training Epoch: 4 [17350/36450]\tLoss: 938.9157\n",
      "Training Epoch: 4 [17400/36450]\tLoss: 947.1355\n",
      "Training Epoch: 4 [17450/36450]\tLoss: 936.5991\n",
      "Training Epoch: 4 [17500/36450]\tLoss: 903.5255\n",
      "Training Epoch: 4 [17550/36450]\tLoss: 886.1146\n",
      "Training Epoch: 4 [17600/36450]\tLoss: 933.0388\n",
      "Training Epoch: 4 [17650/36450]\tLoss: 906.1102\n",
      "Training Epoch: 4 [17700/36450]\tLoss: 920.1047\n",
      "Training Epoch: 4 [17750/36450]\tLoss: 809.3988\n",
      "Training Epoch: 4 [17800/36450]\tLoss: 977.4839\n",
      "Training Epoch: 4 [17850/36450]\tLoss: 927.2731\n",
      "Training Epoch: 4 [17900/36450]\tLoss: 936.7780\n",
      "Training Epoch: 4 [17950/36450]\tLoss: 944.9086\n",
      "Training Epoch: 4 [18000/36450]\tLoss: 889.7842\n",
      "Training Epoch: 4 [18050/36450]\tLoss: 897.1444\n",
      "Training Epoch: 4 [18100/36450]\tLoss: 916.7205\n",
      "Training Epoch: 4 [18150/36450]\tLoss: 886.6057\n",
      "Training Epoch: 4 [18200/36450]\tLoss: 881.9933\n",
      "Training Epoch: 4 [18250/36450]\tLoss: 900.7802\n",
      "Training Epoch: 4 [18300/36450]\tLoss: 911.9507\n",
      "Training Epoch: 4 [18350/36450]\tLoss: 891.9624\n",
      "Training Epoch: 4 [18400/36450]\tLoss: 808.3992\n",
      "Training Epoch: 4 [18450/36450]\tLoss: 839.1000\n",
      "Training Epoch: 4 [18500/36450]\tLoss: 877.0090\n",
      "Training Epoch: 4 [18550/36450]\tLoss: 857.5370\n",
      "Training Epoch: 4 [18600/36450]\tLoss: 830.4072\n",
      "Training Epoch: 4 [18650/36450]\tLoss: 865.7550\n",
      "Training Epoch: 4 [18700/36450]\tLoss: 895.7579\n",
      "Training Epoch: 4 [18750/36450]\tLoss: 897.2231\n",
      "Training Epoch: 4 [18800/36450]\tLoss: 908.9136\n",
      "Training Epoch: 4 [18850/36450]\tLoss: 905.9711\n",
      "Training Epoch: 4 [18900/36450]\tLoss: 904.3372\n",
      "Training Epoch: 4 [18950/36450]\tLoss: 886.1424\n",
      "Training Epoch: 4 [19000/36450]\tLoss: 903.3301\n",
      "Training Epoch: 4 [19050/36450]\tLoss: 856.0619\n",
      "Training Epoch: 4 [19100/36450]\tLoss: 872.1248\n",
      "Training Epoch: 4 [19150/36450]\tLoss: 849.1492\n",
      "Training Epoch: 4 [19200/36450]\tLoss: 846.3036\n",
      "Training Epoch: 4 [19250/36450]\tLoss: 874.7532\n",
      "Training Epoch: 4 [19300/36450]\tLoss: 861.4658\n",
      "Training Epoch: 4 [19350/36450]\tLoss: 927.4764\n",
      "Training Epoch: 4 [19400/36450]\tLoss: 899.6076\n",
      "Training Epoch: 4 [19450/36450]\tLoss: 810.5488\n",
      "Training Epoch: 4 [19500/36450]\tLoss: 868.6312\n",
      "Training Epoch: 4 [19550/36450]\tLoss: 864.8595\n",
      "Training Epoch: 4 [19600/36450]\tLoss: 830.7224\n",
      "Training Epoch: 4 [19650/36450]\tLoss: 926.6349\n",
      "Training Epoch: 4 [19700/36450]\tLoss: 881.2573\n",
      "Training Epoch: 4 [19750/36450]\tLoss: 908.9669\n",
      "Training Epoch: 4 [19800/36450]\tLoss: 903.6561\n",
      "Training Epoch: 4 [19850/36450]\tLoss: 868.1127\n",
      "Training Epoch: 4 [19900/36450]\tLoss: 928.8594\n",
      "Training Epoch: 4 [19950/36450]\tLoss: 890.8104\n",
      "Training Epoch: 4 [20000/36450]\tLoss: 856.5962\n",
      "Training Epoch: 4 [20050/36450]\tLoss: 834.7240\n",
      "Training Epoch: 4 [20100/36450]\tLoss: 882.9861\n",
      "Training Epoch: 4 [20150/36450]\tLoss: 876.9620\n",
      "Training Epoch: 4 [20200/36450]\tLoss: 851.0998\n",
      "Training Epoch: 4 [20250/36450]\tLoss: 888.8657\n",
      "Training Epoch: 4 [20300/36450]\tLoss: 860.6891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [20350/36450]\tLoss: 883.8531\n",
      "Training Epoch: 4 [20400/36450]\tLoss: 857.5735\n",
      "Training Epoch: 4 [20450/36450]\tLoss: 821.5190\n",
      "Training Epoch: 4 [20500/36450]\tLoss: 849.2292\n",
      "Training Epoch: 4 [20550/36450]\tLoss: 849.2222\n",
      "Training Epoch: 4 [20600/36450]\tLoss: 881.9183\n",
      "Training Epoch: 4 [20650/36450]\tLoss: 830.5797\n",
      "Training Epoch: 4 [20700/36450]\tLoss: 899.9572\n",
      "Training Epoch: 4 [20750/36450]\tLoss: 860.3671\n",
      "Training Epoch: 4 [20800/36450]\tLoss: 925.8259\n",
      "Training Epoch: 4 [20850/36450]\tLoss: 812.1940\n",
      "Training Epoch: 4 [20900/36450]\tLoss: 879.0308\n",
      "Training Epoch: 4 [20950/36450]\tLoss: 852.2434\n",
      "Training Epoch: 4 [21000/36450]\tLoss: 874.9078\n",
      "Training Epoch: 4 [21050/36450]\tLoss: 920.7352\n",
      "Training Epoch: 4 [21100/36450]\tLoss: 852.3268\n",
      "Training Epoch: 4 [21150/36450]\tLoss: 853.1077\n",
      "Training Epoch: 4 [21200/36450]\tLoss: 859.2295\n",
      "Training Epoch: 4 [21250/36450]\tLoss: 872.6696\n",
      "Training Epoch: 4 [21300/36450]\tLoss: 856.5421\n",
      "Training Epoch: 4 [21350/36450]\tLoss: 869.9094\n",
      "Training Epoch: 4 [21400/36450]\tLoss: 885.2112\n",
      "Training Epoch: 4 [21450/36450]\tLoss: 899.7563\n",
      "Training Epoch: 4 [21500/36450]\tLoss: 861.7760\n",
      "Training Epoch: 4 [21550/36450]\tLoss: 883.9400\n",
      "Training Epoch: 4 [21600/36450]\tLoss: 888.4344\n",
      "Training Epoch: 4 [21650/36450]\tLoss: 870.5750\n",
      "Training Epoch: 4 [21700/36450]\tLoss: 920.8195\n",
      "Training Epoch: 4 [21750/36450]\tLoss: 827.9647\n",
      "Training Epoch: 4 [21800/36450]\tLoss: 875.8227\n",
      "Training Epoch: 4 [21850/36450]\tLoss: 953.2361\n",
      "Training Epoch: 4 [21900/36450]\tLoss: 900.3948\n",
      "Training Epoch: 4 [21950/36450]\tLoss: 850.7695\n",
      "Training Epoch: 4 [22000/36450]\tLoss: 842.0574\n",
      "Training Epoch: 4 [22050/36450]\tLoss: 894.0306\n",
      "Training Epoch: 4 [22100/36450]\tLoss: 857.5011\n",
      "Training Epoch: 4 [22150/36450]\tLoss: 876.1664\n",
      "Training Epoch: 4 [22200/36450]\tLoss: 838.6165\n",
      "Training Epoch: 4 [22250/36450]\tLoss: 852.6640\n",
      "Training Epoch: 4 [22300/36450]\tLoss: 878.2219\n",
      "Training Epoch: 4 [22350/36450]\tLoss: 907.4030\n",
      "Training Epoch: 4 [22400/36450]\tLoss: 861.6279\n",
      "Training Epoch: 4 [22450/36450]\tLoss: 834.5333\n",
      "Training Epoch: 4 [22500/36450]\tLoss: 877.9898\n",
      "Training Epoch: 4 [22550/36450]\tLoss: 867.9980\n",
      "Training Epoch: 4 [22600/36450]\tLoss: 863.6897\n",
      "Training Epoch: 4 [22650/36450]\tLoss: 857.8289\n",
      "Training Epoch: 4 [22700/36450]\tLoss: 848.6170\n",
      "Training Epoch: 4 [22750/36450]\tLoss: 834.1860\n",
      "Training Epoch: 4 [22800/36450]\tLoss: 848.9967\n",
      "Training Epoch: 4 [22850/36450]\tLoss: 896.6258\n",
      "Training Epoch: 4 [22900/36450]\tLoss: 884.5892\n",
      "Training Epoch: 4 [22950/36450]\tLoss: 882.4951\n",
      "Training Epoch: 4 [23000/36450]\tLoss: 877.4883\n",
      "Training Epoch: 4 [23050/36450]\tLoss: 846.6689\n",
      "Training Epoch: 4 [23100/36450]\tLoss: 908.1412\n",
      "Training Epoch: 4 [23150/36450]\tLoss: 920.3982\n",
      "Training Epoch: 4 [23200/36450]\tLoss: 867.7617\n",
      "Training Epoch: 4 [23250/36450]\tLoss: 885.5211\n",
      "Training Epoch: 4 [23300/36450]\tLoss: 889.7721\n",
      "Training Epoch: 4 [23350/36450]\tLoss: 851.5106\n",
      "Training Epoch: 4 [23400/36450]\tLoss: 831.1288\n",
      "Training Epoch: 4 [23450/36450]\tLoss: 893.8470\n",
      "Training Epoch: 4 [23500/36450]\tLoss: 924.1893\n",
      "Training Epoch: 4 [23550/36450]\tLoss: 845.6492\n",
      "Training Epoch: 4 [23600/36450]\tLoss: 895.9048\n",
      "Training Epoch: 4 [23650/36450]\tLoss: 874.6822\n",
      "Training Epoch: 4 [23700/36450]\tLoss: 887.1761\n",
      "Training Epoch: 4 [23750/36450]\tLoss: 895.7132\n",
      "Training Epoch: 4 [23800/36450]\tLoss: 874.3314\n",
      "Training Epoch: 4 [23850/36450]\tLoss: 825.3950\n",
      "Training Epoch: 4 [23900/36450]\tLoss: 911.9283\n",
      "Training Epoch: 4 [23950/36450]\tLoss: 847.4763\n",
      "Training Epoch: 4 [24000/36450]\tLoss: 881.9282\n",
      "Training Epoch: 4 [24050/36450]\tLoss: 858.7288\n",
      "Training Epoch: 4 [24100/36450]\tLoss: 824.1813\n",
      "Training Epoch: 4 [24150/36450]\tLoss: 841.8587\n",
      "Training Epoch: 4 [24200/36450]\tLoss: 860.7189\n",
      "Training Epoch: 4 [24250/36450]\tLoss: 922.6616\n",
      "Training Epoch: 4 [24300/36450]\tLoss: 890.5724\n",
      "Training Epoch: 4 [24350/36450]\tLoss: 893.9696\n",
      "Training Epoch: 4 [24400/36450]\tLoss: 857.4470\n",
      "Training Epoch: 4 [24450/36450]\tLoss: 882.0076\n",
      "Training Epoch: 4 [24500/36450]\tLoss: 843.3897\n",
      "Training Epoch: 4 [24550/36450]\tLoss: 864.4481\n",
      "Training Epoch: 4 [24600/36450]\tLoss: 822.2040\n",
      "Training Epoch: 4 [24650/36450]\tLoss: 904.0475\n",
      "Training Epoch: 4 [24700/36450]\tLoss: 865.2642\n",
      "Training Epoch: 4 [24750/36450]\tLoss: 827.4229\n",
      "Training Epoch: 4 [24800/36450]\tLoss: 897.9555\n",
      "Training Epoch: 4 [24850/36450]\tLoss: 889.8057\n",
      "Training Epoch: 4 [24900/36450]\tLoss: 832.0605\n",
      "Training Epoch: 4 [24950/36450]\tLoss: 836.7975\n",
      "Training Epoch: 4 [25000/36450]\tLoss: 853.7629\n",
      "Training Epoch: 4 [25050/36450]\tLoss: 862.4781\n",
      "Training Epoch: 4 [25100/36450]\tLoss: 861.6133\n",
      "Training Epoch: 4 [25150/36450]\tLoss: 967.4543\n",
      "Training Epoch: 4 [25200/36450]\tLoss: 903.5527\n",
      "Training Epoch: 4 [25250/36450]\tLoss: 855.2974\n",
      "Training Epoch: 4 [25300/36450]\tLoss: 828.4980\n",
      "Training Epoch: 4 [25350/36450]\tLoss: 815.4545\n",
      "Training Epoch: 4 [25400/36450]\tLoss: 915.4940\n",
      "Training Epoch: 4 [25450/36450]\tLoss: 898.5967\n",
      "Training Epoch: 4 [25500/36450]\tLoss: 883.6131\n",
      "Training Epoch: 4 [25550/36450]\tLoss: 886.7666\n",
      "Training Epoch: 4 [25600/36450]\tLoss: 900.6153\n",
      "Training Epoch: 4 [25650/36450]\tLoss: 866.9211\n",
      "Training Epoch: 4 [25700/36450]\tLoss: 853.7158\n",
      "Training Epoch: 4 [25750/36450]\tLoss: 854.0889\n",
      "Training Epoch: 4 [25800/36450]\tLoss: 851.1001\n",
      "Training Epoch: 4 [25850/36450]\tLoss: 909.9349\n",
      "Training Epoch: 4 [25900/36450]\tLoss: 875.7021\n",
      "Training Epoch: 4 [25950/36450]\tLoss: 907.0515\n",
      "Training Epoch: 4 [26000/36450]\tLoss: 939.5090\n",
      "Training Epoch: 4 [26050/36450]\tLoss: 885.3376\n",
      "Training Epoch: 4 [26100/36450]\tLoss: 821.8273\n",
      "Training Epoch: 4 [26150/36450]\tLoss: 892.3721\n",
      "Training Epoch: 4 [26200/36450]\tLoss: 820.3074\n",
      "Training Epoch: 4 [26250/36450]\tLoss: 934.2250\n",
      "Training Epoch: 4 [26300/36450]\tLoss: 866.4382\n",
      "Training Epoch: 4 [26350/36450]\tLoss: 843.2636\n",
      "Training Epoch: 4 [26400/36450]\tLoss: 882.2092\n",
      "Training Epoch: 4 [26450/36450]\tLoss: 825.6048\n",
      "Training Epoch: 4 [26500/36450]\tLoss: 874.8546\n",
      "Training Epoch: 4 [26550/36450]\tLoss: 897.6854\n",
      "Training Epoch: 4 [26600/36450]\tLoss: 867.7677\n",
      "Training Epoch: 4 [26650/36450]\tLoss: 799.2905\n",
      "Training Epoch: 4 [26700/36450]\tLoss: 850.3365\n",
      "Training Epoch: 4 [26750/36450]\tLoss: 893.3036\n",
      "Training Epoch: 4 [26800/36450]\tLoss: 879.8524\n",
      "Training Epoch: 4 [26850/36450]\tLoss: 862.9915\n",
      "Training Epoch: 4 [26900/36450]\tLoss: 839.2031\n",
      "Training Epoch: 4 [26950/36450]\tLoss: 809.9391\n",
      "Training Epoch: 4 [27000/36450]\tLoss: 836.3342\n",
      "Training Epoch: 4 [27050/36450]\tLoss: 866.7356\n",
      "Training Epoch: 4 [27100/36450]\tLoss: 898.8896\n",
      "Training Epoch: 4 [27150/36450]\tLoss: 882.8386\n",
      "Training Epoch: 4 [27200/36450]\tLoss: 808.8598\n",
      "Training Epoch: 4 [27250/36450]\tLoss: 851.0085\n",
      "Training Epoch: 4 [27300/36450]\tLoss: 844.4877\n",
      "Training Epoch: 4 [27350/36450]\tLoss: 864.7094\n",
      "Training Epoch: 4 [27400/36450]\tLoss: 855.0296\n",
      "Training Epoch: 4 [27450/36450]\tLoss: 840.9027\n",
      "Training Epoch: 4 [27500/36450]\tLoss: 853.5710\n",
      "Training Epoch: 4 [27550/36450]\tLoss: 882.4030\n",
      "Training Epoch: 4 [27600/36450]\tLoss: 838.4371\n",
      "Training Epoch: 4 [27650/36450]\tLoss: 853.3325\n",
      "Training Epoch: 4 [27700/36450]\tLoss: 875.0088\n",
      "Training Epoch: 4 [27750/36450]\tLoss: 840.0925\n",
      "Training Epoch: 4 [27800/36450]\tLoss: 861.3868\n",
      "Training Epoch: 4 [27850/36450]\tLoss: 832.2865\n",
      "Training Epoch: 4 [27900/36450]\tLoss: 915.8993\n",
      "Training Epoch: 4 [27950/36450]\tLoss: 874.1526\n",
      "Training Epoch: 4 [28000/36450]\tLoss: 824.5732\n",
      "Training Epoch: 4 [28050/36450]\tLoss: 861.9324\n",
      "Training Epoch: 4 [28100/36450]\tLoss: 838.1855\n",
      "Training Epoch: 4 [28150/36450]\tLoss: 881.1472\n",
      "Training Epoch: 4 [28200/36450]\tLoss: 829.3823\n",
      "Training Epoch: 4 [28250/36450]\tLoss: 844.7001\n",
      "Training Epoch: 4 [28300/36450]\tLoss: 850.8047\n",
      "Training Epoch: 4 [28350/36450]\tLoss: 846.8326\n",
      "Training Epoch: 4 [28400/36450]\tLoss: 875.6409\n",
      "Training Epoch: 4 [28450/36450]\tLoss: 829.6069\n",
      "Training Epoch: 4 [28500/36450]\tLoss: 834.5573\n",
      "Training Epoch: 4 [28550/36450]\tLoss: 840.7111\n",
      "Training Epoch: 4 [28600/36450]\tLoss: 828.2485\n",
      "Training Epoch: 4 [28650/36450]\tLoss: 877.4882\n",
      "Training Epoch: 4 [28700/36450]\tLoss: 869.3351\n",
      "Training Epoch: 4 [28750/36450]\tLoss: 908.3497\n",
      "Training Epoch: 4 [28800/36450]\tLoss: 865.8042\n",
      "Training Epoch: 4 [28850/36450]\tLoss: 864.4515\n",
      "Training Epoch: 4 [28900/36450]\tLoss: 886.1190\n",
      "Training Epoch: 4 [28950/36450]\tLoss: 830.0399\n",
      "Training Epoch: 4 [29000/36450]\tLoss: 885.5947\n",
      "Training Epoch: 4 [29050/36450]\tLoss: 842.6102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [29100/36450]\tLoss: 840.5598\n",
      "Training Epoch: 4 [29150/36450]\tLoss: 827.3538\n",
      "Training Epoch: 4 [29200/36450]\tLoss: 888.6871\n",
      "Training Epoch: 4 [29250/36450]\tLoss: 805.3449\n",
      "Training Epoch: 4 [29300/36450]\tLoss: 795.7004\n",
      "Training Epoch: 4 [29350/36450]\tLoss: 811.0533\n",
      "Training Epoch: 4 [29400/36450]\tLoss: 831.7763\n",
      "Training Epoch: 4 [29450/36450]\tLoss: 881.8502\n",
      "Training Epoch: 4 [29500/36450]\tLoss: 891.0656\n",
      "Training Epoch: 4 [29550/36450]\tLoss: 857.2163\n",
      "Training Epoch: 4 [29600/36450]\tLoss: 857.6685\n",
      "Training Epoch: 4 [29650/36450]\tLoss: 840.4195\n",
      "Training Epoch: 4 [29700/36450]\tLoss: 837.0510\n",
      "Training Epoch: 4 [29750/36450]\tLoss: 873.3104\n",
      "Training Epoch: 4 [29800/36450]\tLoss: 896.5113\n",
      "Training Epoch: 4 [29850/36450]\tLoss: 816.3536\n",
      "Training Epoch: 4 [29900/36450]\tLoss: 850.5354\n",
      "Training Epoch: 4 [29950/36450]\tLoss: 900.1077\n",
      "Training Epoch: 4 [30000/36450]\tLoss: 826.2822\n",
      "Training Epoch: 4 [30050/36450]\tLoss: 827.0373\n",
      "Training Epoch: 4 [30100/36450]\tLoss: 808.2095\n",
      "Training Epoch: 4 [30150/36450]\tLoss: 844.5383\n",
      "Training Epoch: 4 [30200/36450]\tLoss: 847.4553\n",
      "Training Epoch: 4 [30250/36450]\tLoss: 820.0034\n",
      "Training Epoch: 4 [30300/36450]\tLoss: 849.0101\n",
      "Training Epoch: 4 [30350/36450]\tLoss: 832.2289\n",
      "Training Epoch: 4 [30400/36450]\tLoss: 858.4493\n",
      "Training Epoch: 4 [30450/36450]\tLoss: 822.3309\n",
      "Training Epoch: 4 [30500/36450]\tLoss: 843.6727\n",
      "Training Epoch: 4 [30550/36450]\tLoss: 850.3756\n",
      "Training Epoch: 4 [30600/36450]\tLoss: 824.8787\n",
      "Training Epoch: 4 [30650/36450]\tLoss: 867.9586\n",
      "Training Epoch: 4 [30700/36450]\tLoss: 914.6792\n",
      "Training Epoch: 4 [30750/36450]\tLoss: 868.4658\n",
      "Training Epoch: 4 [30800/36450]\tLoss: 870.0616\n",
      "Training Epoch: 4 [30850/36450]\tLoss: 820.7869\n",
      "Training Epoch: 4 [30900/36450]\tLoss: 826.3115\n",
      "Training Epoch: 4 [30950/36450]\tLoss: 861.1382\n",
      "Training Epoch: 4 [31000/36450]\tLoss: 859.5944\n",
      "Training Epoch: 4 [31050/36450]\tLoss: 901.7070\n",
      "Training Epoch: 4 [31100/36450]\tLoss: 828.8685\n",
      "Training Epoch: 4 [31150/36450]\tLoss: 864.8945\n",
      "Training Epoch: 4 [31200/36450]\tLoss: 837.2269\n",
      "Training Epoch: 4 [31250/36450]\tLoss: 896.1522\n",
      "Training Epoch: 4 [31300/36450]\tLoss: 870.5911\n",
      "Training Epoch: 4 [31350/36450]\tLoss: 787.8116\n",
      "Training Epoch: 4 [31400/36450]\tLoss: 883.1727\n",
      "Training Epoch: 4 [31450/36450]\tLoss: 842.3544\n",
      "Training Epoch: 4 [31500/36450]\tLoss: 892.0971\n",
      "Training Epoch: 4 [31550/36450]\tLoss: 816.1599\n",
      "Training Epoch: 4 [31600/36450]\tLoss: 831.5037\n",
      "Training Epoch: 4 [31650/36450]\tLoss: 827.1321\n",
      "Training Epoch: 4 [31700/36450]\tLoss: 856.4249\n",
      "Training Epoch: 4 [31750/36450]\tLoss: 876.2545\n",
      "Training Epoch: 4 [31800/36450]\tLoss: 861.6947\n",
      "Training Epoch: 4 [31850/36450]\tLoss: 911.7955\n",
      "Training Epoch: 4 [31900/36450]\tLoss: 863.9606\n",
      "Training Epoch: 4 [31950/36450]\tLoss: 849.5634\n",
      "Training Epoch: 4 [32000/36450]\tLoss: 858.9972\n",
      "Training Epoch: 4 [32050/36450]\tLoss: 835.1846\n",
      "Training Epoch: 4 [32100/36450]\tLoss: 809.2318\n",
      "Training Epoch: 4 [32150/36450]\tLoss: 883.6214\n",
      "Training Epoch: 4 [32200/36450]\tLoss: 861.2051\n",
      "Training Epoch: 4 [32250/36450]\tLoss: 852.1550\n",
      "Training Epoch: 4 [32300/36450]\tLoss: 839.2848\n",
      "Training Epoch: 4 [32350/36450]\tLoss: 785.2894\n",
      "Training Epoch: 4 [32400/36450]\tLoss: 836.9157\n",
      "Training Epoch: 4 [32450/36450]\tLoss: 815.8550\n",
      "Training Epoch: 4 [32500/36450]\tLoss: 890.6324\n",
      "Training Epoch: 4 [32550/36450]\tLoss: 856.8915\n",
      "Training Epoch: 4 [32600/36450]\tLoss: 892.6614\n",
      "Training Epoch: 4 [32650/36450]\tLoss: 894.6126\n",
      "Training Epoch: 4 [32700/36450]\tLoss: 862.6730\n",
      "Training Epoch: 4 [32750/36450]\tLoss: 862.9914\n",
      "Training Epoch: 4 [32800/36450]\tLoss: 872.3986\n",
      "Training Epoch: 4 [32850/36450]\tLoss: 840.4012\n",
      "Training Epoch: 4 [32900/36450]\tLoss: 869.1541\n",
      "Training Epoch: 4 [32950/36450]\tLoss: 811.2306\n",
      "Training Epoch: 4 [33000/36450]\tLoss: 898.7012\n",
      "Training Epoch: 4 [33050/36450]\tLoss: 865.5186\n",
      "Training Epoch: 4 [33100/36450]\tLoss: 854.5538\n",
      "Training Epoch: 4 [33150/36450]\tLoss: 858.9086\n",
      "Training Epoch: 4 [33200/36450]\tLoss: 865.6912\n",
      "Training Epoch: 4 [33250/36450]\tLoss: 827.2819\n",
      "Training Epoch: 4 [33300/36450]\tLoss: 833.6512\n",
      "Training Epoch: 4 [33350/36450]\tLoss: 828.0258\n",
      "Training Epoch: 4 [33400/36450]\tLoss: 848.3861\n",
      "Training Epoch: 4 [33450/36450]\tLoss: 847.6564\n",
      "Training Epoch: 4 [33500/36450]\tLoss: 875.7347\n",
      "Training Epoch: 4 [33550/36450]\tLoss: 857.7286\n",
      "Training Epoch: 4 [33600/36450]\tLoss: 830.6887\n",
      "Training Epoch: 4 [33650/36450]\tLoss: 870.6025\n",
      "Training Epoch: 4 [33700/36450]\tLoss: 868.3225\n",
      "Training Epoch: 4 [33750/36450]\tLoss: 867.4008\n",
      "Training Epoch: 4 [33800/36450]\tLoss: 841.3737\n",
      "Training Epoch: 4 [33850/36450]\tLoss: 896.0435\n",
      "Training Epoch: 4 [33900/36450]\tLoss: 864.1488\n",
      "Training Epoch: 4 [33950/36450]\tLoss: 832.3781\n",
      "Training Epoch: 4 [34000/36450]\tLoss: 849.1870\n",
      "Training Epoch: 4 [34050/36450]\tLoss: 809.7810\n",
      "Training Epoch: 4 [34100/36450]\tLoss: 861.0229\n",
      "Training Epoch: 4 [34150/36450]\tLoss: 843.5436\n",
      "Training Epoch: 4 [34200/36450]\tLoss: 826.1846\n",
      "Training Epoch: 4 [34250/36450]\tLoss: 797.3174\n",
      "Training Epoch: 4 [34300/36450]\tLoss: 852.4427\n",
      "Training Epoch: 4 [34350/36450]\tLoss: 849.3130\n",
      "Training Epoch: 4 [34400/36450]\tLoss: 868.2042\n",
      "Training Epoch: 4 [34450/36450]\tLoss: 812.7045\n",
      "Training Epoch: 4 [34500/36450]\tLoss: 807.4290\n",
      "Training Epoch: 4 [34550/36450]\tLoss: 856.3220\n",
      "Training Epoch: 4 [34600/36450]\tLoss: 857.0528\n",
      "Training Epoch: 4 [34650/36450]\tLoss: 803.5973\n",
      "Training Epoch: 4 [34700/36450]\tLoss: 822.5370\n",
      "Training Epoch: 4 [34750/36450]\tLoss: 850.7047\n",
      "Training Epoch: 4 [34800/36450]\tLoss: 892.3480\n",
      "Training Epoch: 4 [34850/36450]\tLoss: 788.7935\n",
      "Training Epoch: 4 [34900/36450]\tLoss: 839.0054\n",
      "Training Epoch: 4 [34950/36450]\tLoss: 849.9278\n",
      "Training Epoch: 4 [35000/36450]\tLoss: 812.3509\n",
      "Training Epoch: 4 [35050/36450]\tLoss: 813.0973\n",
      "Training Epoch: 4 [35100/36450]\tLoss: 852.3784\n",
      "Training Epoch: 4 [35150/36450]\tLoss: 855.1655\n",
      "Training Epoch: 4 [35200/36450]\tLoss: 813.1460\n",
      "Training Epoch: 4 [35250/36450]\tLoss: 864.4255\n",
      "Training Epoch: 4 [35300/36450]\tLoss: 834.2865\n",
      "Training Epoch: 4 [35350/36450]\tLoss: 808.6693\n",
      "Training Epoch: 4 [35400/36450]\tLoss: 839.1242\n",
      "Training Epoch: 4 [35450/36450]\tLoss: 842.0310\n",
      "Training Epoch: 4 [35500/36450]\tLoss: 834.0279\n",
      "Training Epoch: 4 [35550/36450]\tLoss: 855.1771\n",
      "Training Epoch: 4 [35600/36450]\tLoss: 816.1852\n",
      "Training Epoch: 4 [35650/36450]\tLoss: 849.8088\n",
      "Training Epoch: 4 [35700/36450]\tLoss: 855.4216\n",
      "Training Epoch: 4 [35750/36450]\tLoss: 890.6467\n",
      "Training Epoch: 4 [35800/36450]\tLoss: 791.3876\n",
      "Training Epoch: 4 [35850/36450]\tLoss: 828.3867\n",
      "Training Epoch: 4 [35900/36450]\tLoss: 812.2059\n",
      "Training Epoch: 4 [35950/36450]\tLoss: 776.1088\n",
      "Training Epoch: 4 [36000/36450]\tLoss: 859.4596\n",
      "Training Epoch: 4 [36050/36450]\tLoss: 850.7213\n",
      "Training Epoch: 4 [36100/36450]\tLoss: 797.3340\n",
      "Training Epoch: 4 [36150/36450]\tLoss: 800.4152\n",
      "Training Epoch: 4 [36200/36450]\tLoss: 847.0525\n",
      "Training Epoch: 4 [36250/36450]\tLoss: 813.5371\n",
      "Training Epoch: 4 [36300/36450]\tLoss: 860.2718\n",
      "Training Epoch: 4 [36350/36450]\tLoss: 869.2444\n",
      "Training Epoch: 4 [36400/36450]\tLoss: 835.2177\n",
      "Training Epoch: 4 [36450/36450]\tLoss: 819.1204\n",
      "Training Epoch: 4 [4050/4050]\tLoss: 422.1797\n",
      "Training Epoch: 5 [50/36450]\tLoss: 827.2512\n",
      "Training Epoch: 5 [100/36450]\tLoss: 813.9589\n",
      "Training Epoch: 5 [150/36450]\tLoss: 829.4841\n",
      "Training Epoch: 5 [200/36450]\tLoss: 783.0484\n",
      "Training Epoch: 5 [250/36450]\tLoss: 790.5296\n",
      "Training Epoch: 5 [300/36450]\tLoss: 775.1959\n",
      "Training Epoch: 5 [350/36450]\tLoss: 853.8065\n",
      "Training Epoch: 5 [400/36450]\tLoss: 839.1173\n",
      "Training Epoch: 5 [450/36450]\tLoss: 875.8224\n",
      "Training Epoch: 5 [500/36450]\tLoss: 871.0496\n",
      "Training Epoch: 5 [550/36450]\tLoss: 818.6761\n",
      "Training Epoch: 5 [600/36450]\tLoss: 850.9221\n",
      "Training Epoch: 5 [650/36450]\tLoss: 808.0872\n",
      "Training Epoch: 5 [700/36450]\tLoss: 799.0031\n",
      "Training Epoch: 5 [750/36450]\tLoss: 857.0381\n",
      "Training Epoch: 5 [800/36450]\tLoss: 819.4133\n",
      "Training Epoch: 5 [850/36450]\tLoss: 808.5624\n",
      "Training Epoch: 5 [900/36450]\tLoss: 811.6348\n",
      "Training Epoch: 5 [950/36450]\tLoss: 867.3441\n",
      "Training Epoch: 5 [1000/36450]\tLoss: 795.8389\n",
      "Training Epoch: 5 [1050/36450]\tLoss: 828.5051\n",
      "Training Epoch: 5 [1100/36450]\tLoss: 880.5790\n",
      "Training Epoch: 5 [1150/36450]\tLoss: 816.9387\n",
      "Training Epoch: 5 [1200/36450]\tLoss: 865.8950\n",
      "Training Epoch: 5 [1250/36450]\tLoss: 846.4489\n",
      "Training Epoch: 5 [1300/36450]\tLoss: 837.6371\n",
      "Training Epoch: 5 [1350/36450]\tLoss: 839.6866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [1400/36450]\tLoss: 838.5307\n",
      "Training Epoch: 5 [1450/36450]\tLoss: 841.1852\n",
      "Training Epoch: 5 [1500/36450]\tLoss: 859.0039\n",
      "Training Epoch: 5 [1550/36450]\tLoss: 802.9420\n",
      "Training Epoch: 5 [1600/36450]\tLoss: 820.1034\n",
      "Training Epoch: 5 [1650/36450]\tLoss: 849.2193\n",
      "Training Epoch: 5 [1700/36450]\tLoss: 855.2856\n",
      "Training Epoch: 5 [1750/36450]\tLoss: 855.2838\n",
      "Training Epoch: 5 [1800/36450]\tLoss: 810.1898\n",
      "Training Epoch: 5 [1850/36450]\tLoss: 884.0608\n",
      "Training Epoch: 5 [1900/36450]\tLoss: 809.3413\n",
      "Training Epoch: 5 [1950/36450]\tLoss: 851.8527\n",
      "Training Epoch: 5 [2000/36450]\tLoss: 864.0317\n",
      "Training Epoch: 5 [2050/36450]\tLoss: 857.1548\n",
      "Training Epoch: 5 [2100/36450]\tLoss: 792.6335\n",
      "Training Epoch: 5 [2150/36450]\tLoss: 852.9805\n",
      "Training Epoch: 5 [2200/36450]\tLoss: 835.4473\n",
      "Training Epoch: 5 [2250/36450]\tLoss: 842.0538\n",
      "Training Epoch: 5 [2300/36450]\tLoss: 835.1205\n",
      "Training Epoch: 5 [2350/36450]\tLoss: 853.7921\n",
      "Training Epoch: 5 [2400/36450]\tLoss: 819.5461\n",
      "Training Epoch: 5 [2450/36450]\tLoss: 809.3911\n",
      "Training Epoch: 5 [2500/36450]\tLoss: 815.4174\n",
      "Training Epoch: 5 [2550/36450]\tLoss: 826.2507\n",
      "Training Epoch: 5 [2600/36450]\tLoss: 820.7694\n",
      "Training Epoch: 5 [2650/36450]\tLoss: 866.1273\n",
      "Training Epoch: 5 [2700/36450]\tLoss: 793.1720\n",
      "Training Epoch: 5 [2750/36450]\tLoss: 837.7172\n",
      "Training Epoch: 5 [2800/36450]\tLoss: 815.3859\n",
      "Training Epoch: 5 [2850/36450]\tLoss: 796.2519\n",
      "Training Epoch: 5 [2900/36450]\tLoss: 790.1844\n",
      "Training Epoch: 5 [2950/36450]\tLoss: 820.6964\n",
      "Training Epoch: 5 [3000/36450]\tLoss: 823.0062\n",
      "Training Epoch: 5 [3050/36450]\tLoss: 855.9543\n",
      "Training Epoch: 5 [3100/36450]\tLoss: 870.2820\n",
      "Training Epoch: 5 [3150/36450]\tLoss: 830.8530\n",
      "Training Epoch: 5 [3200/36450]\tLoss: 850.9939\n",
      "Training Epoch: 5 [3250/36450]\tLoss: 798.3277\n",
      "Training Epoch: 5 [3300/36450]\tLoss: 833.4647\n",
      "Training Epoch: 5 [3350/36450]\tLoss: 774.2573\n",
      "Training Epoch: 5 [3400/36450]\tLoss: 818.7821\n",
      "Training Epoch: 5 [3450/36450]\tLoss: 821.9196\n",
      "Training Epoch: 5 [3500/36450]\tLoss: 807.4282\n",
      "Training Epoch: 5 [3550/36450]\tLoss: 820.6967\n",
      "Training Epoch: 5 [3600/36450]\tLoss: 869.3931\n",
      "Training Epoch: 5 [3650/36450]\tLoss: 835.0418\n",
      "Training Epoch: 5 [3700/36450]\tLoss: 783.3810\n",
      "Training Epoch: 5 [3750/36450]\tLoss: 797.6158\n",
      "Training Epoch: 5 [3800/36450]\tLoss: 794.7577\n",
      "Training Epoch: 5 [3850/36450]\tLoss: 836.0143\n",
      "Training Epoch: 5 [3900/36450]\tLoss: 779.6824\n",
      "Training Epoch: 5 [3950/36450]\tLoss: 840.1252\n",
      "Training Epoch: 5 [4000/36450]\tLoss: 828.1324\n",
      "Training Epoch: 5 [4050/36450]\tLoss: 854.2013\n",
      "Training Epoch: 5 [4100/36450]\tLoss: 816.1353\n",
      "Training Epoch: 5 [4150/36450]\tLoss: 780.9729\n",
      "Training Epoch: 5 [4200/36450]\tLoss: 837.9790\n",
      "Training Epoch: 5 [4250/36450]\tLoss: 842.0327\n",
      "Training Epoch: 5 [4300/36450]\tLoss: 821.3372\n",
      "Training Epoch: 5 [4350/36450]\tLoss: 828.7147\n",
      "Training Epoch: 5 [4400/36450]\tLoss: 778.6555\n",
      "Training Epoch: 5 [4450/36450]\tLoss: 838.4013\n",
      "Training Epoch: 5 [4500/36450]\tLoss: 802.0137\n",
      "Training Epoch: 5 [4550/36450]\tLoss: 819.7855\n",
      "Training Epoch: 5 [4600/36450]\tLoss: 813.7221\n",
      "Training Epoch: 5 [4650/36450]\tLoss: 817.0830\n",
      "Training Epoch: 5 [4700/36450]\tLoss: 827.5322\n",
      "Training Epoch: 5 [4750/36450]\tLoss: 802.5640\n",
      "Training Epoch: 5 [4800/36450]\tLoss: 816.7415\n",
      "Training Epoch: 5 [4850/36450]\tLoss: 792.9413\n",
      "Training Epoch: 5 [4900/36450]\tLoss: 821.8899\n",
      "Training Epoch: 5 [4950/36450]\tLoss: 863.7250\n",
      "Training Epoch: 5 [5000/36450]\tLoss: 824.2437\n",
      "Training Epoch: 5 [5050/36450]\tLoss: 785.8984\n",
      "Training Epoch: 5 [5100/36450]\tLoss: 831.4937\n",
      "Training Epoch: 5 [5150/36450]\tLoss: 815.0305\n",
      "Training Epoch: 5 [5200/36450]\tLoss: 828.2850\n",
      "Training Epoch: 5 [5250/36450]\tLoss: 774.4044\n",
      "Training Epoch: 5 [5300/36450]\tLoss: 859.6608\n",
      "Training Epoch: 5 [5350/36450]\tLoss: 812.4478\n",
      "Training Epoch: 5 [5400/36450]\tLoss: 757.1827\n",
      "Training Epoch: 5 [5450/36450]\tLoss: 827.8027\n",
      "Training Epoch: 5 [5500/36450]\tLoss: 844.6471\n",
      "Training Epoch: 5 [5550/36450]\tLoss: 856.3431\n",
      "Training Epoch: 5 [5600/36450]\tLoss: 843.6793\n",
      "Training Epoch: 5 [5650/36450]\tLoss: 832.6465\n",
      "Training Epoch: 5 [5700/36450]\tLoss: 835.9255\n",
      "Training Epoch: 5 [5750/36450]\tLoss: 850.9276\n",
      "Training Epoch: 5 [5800/36450]\tLoss: 828.7029\n",
      "Training Epoch: 5 [5850/36450]\tLoss: 815.5657\n",
      "Training Epoch: 5 [5900/36450]\tLoss: 768.2474\n",
      "Training Epoch: 5 [5950/36450]\tLoss: 778.2139\n",
      "Training Epoch: 5 [6000/36450]\tLoss: 820.6987\n",
      "Training Epoch: 5 [6050/36450]\tLoss: 870.6592\n",
      "Training Epoch: 5 [6100/36450]\tLoss: 856.0382\n",
      "Training Epoch: 5 [6150/36450]\tLoss: 789.2372\n",
      "Training Epoch: 5 [6200/36450]\tLoss: 838.7538\n",
      "Training Epoch: 5 [6250/36450]\tLoss: 821.3806\n",
      "Training Epoch: 5 [6300/36450]\tLoss: 758.5342\n",
      "Training Epoch: 5 [6350/36450]\tLoss: 844.4160\n",
      "Training Epoch: 5 [6400/36450]\tLoss: 789.1067\n",
      "Training Epoch: 5 [6450/36450]\tLoss: 820.3794\n",
      "Training Epoch: 5 [6500/36450]\tLoss: 804.9538\n",
      "Training Epoch: 5 [6550/36450]\tLoss: 821.0852\n",
      "Training Epoch: 5 [6600/36450]\tLoss: 810.0865\n",
      "Training Epoch: 5 [6650/36450]\tLoss: 849.5197\n",
      "Training Epoch: 5 [6700/36450]\tLoss: 785.9042\n",
      "Training Epoch: 5 [6750/36450]\tLoss: 816.6734\n",
      "Training Epoch: 5 [6800/36450]\tLoss: 801.6345\n",
      "Training Epoch: 5 [6850/36450]\tLoss: 806.0552\n",
      "Training Epoch: 5 [6900/36450]\tLoss: 818.2704\n",
      "Training Epoch: 5 [6950/36450]\tLoss: 872.5231\n",
      "Training Epoch: 5 [7000/36450]\tLoss: 836.4257\n",
      "Training Epoch: 5 [7050/36450]\tLoss: 789.1680\n",
      "Training Epoch: 5 [7100/36450]\tLoss: 821.6800\n",
      "Training Epoch: 5 [7150/36450]\tLoss: 768.6306\n",
      "Training Epoch: 5 [7200/36450]\tLoss: 798.6324\n",
      "Training Epoch: 5 [7250/36450]\tLoss: 809.6824\n",
      "Training Epoch: 5 [7300/36450]\tLoss: 781.7251\n",
      "Training Epoch: 5 [7350/36450]\tLoss: 843.4341\n",
      "Training Epoch: 5 [7400/36450]\tLoss: 838.7326\n",
      "Training Epoch: 5 [7450/36450]\tLoss: 811.1657\n",
      "Training Epoch: 5 [7500/36450]\tLoss: 820.8380\n",
      "Training Epoch: 5 [7550/36450]\tLoss: 811.0990\n",
      "Training Epoch: 5 [7600/36450]\tLoss: 824.4716\n",
      "Training Epoch: 5 [7650/36450]\tLoss: 845.3057\n",
      "Training Epoch: 5 [7700/36450]\tLoss: 832.8740\n",
      "Training Epoch: 5 [7750/36450]\tLoss: 868.8628\n",
      "Training Epoch: 5 [7800/36450]\tLoss: 860.6923\n",
      "Training Epoch: 5 [7850/36450]\tLoss: 820.7286\n",
      "Training Epoch: 5 [7900/36450]\tLoss: 805.5007\n",
      "Training Epoch: 5 [7950/36450]\tLoss: 835.9583\n",
      "Training Epoch: 5 [8000/36450]\tLoss: 808.6044\n",
      "Training Epoch: 5 [8050/36450]\tLoss: 782.7209\n",
      "Training Epoch: 5 [8100/36450]\tLoss: 825.7476\n",
      "Training Epoch: 5 [8150/36450]\tLoss: 814.9550\n",
      "Training Epoch: 5 [8200/36450]\tLoss: 836.7336\n",
      "Training Epoch: 5 [8250/36450]\tLoss: 805.6651\n",
      "Training Epoch: 5 [8300/36450]\tLoss: 803.3630\n",
      "Training Epoch: 5 [8350/36450]\tLoss: 831.6295\n",
      "Training Epoch: 5 [8400/36450]\tLoss: 822.8236\n",
      "Training Epoch: 5 [8450/36450]\tLoss: 811.3157\n",
      "Training Epoch: 5 [8500/36450]\tLoss: 793.8174\n",
      "Training Epoch: 5 [8550/36450]\tLoss: 793.7390\n",
      "Training Epoch: 5 [8600/36450]\tLoss: 804.9671\n",
      "Training Epoch: 5 [8650/36450]\tLoss: 781.1277\n",
      "Training Epoch: 5 [8700/36450]\tLoss: 826.0917\n",
      "Training Epoch: 5 [8750/36450]\tLoss: 786.6736\n",
      "Training Epoch: 5 [8800/36450]\tLoss: 798.4288\n",
      "Training Epoch: 5 [8850/36450]\tLoss: 812.4891\n",
      "Training Epoch: 5 [8900/36450]\tLoss: 820.7263\n",
      "Training Epoch: 5 [8950/36450]\tLoss: 832.4677\n",
      "Training Epoch: 5 [9000/36450]\tLoss: 820.6251\n",
      "Training Epoch: 5 [9050/36450]\tLoss: 879.9841\n",
      "Training Epoch: 5 [9100/36450]\tLoss: 770.8984\n",
      "Training Epoch: 5 [9150/36450]\tLoss: 823.4317\n",
      "Training Epoch: 5 [9200/36450]\tLoss: 809.5102\n",
      "Training Epoch: 5 [9250/36450]\tLoss: 821.8478\n",
      "Training Epoch: 5 [9300/36450]\tLoss: 824.9881\n",
      "Training Epoch: 5 [9350/36450]\tLoss: 810.5082\n",
      "Training Epoch: 5 [9400/36450]\tLoss: 789.1391\n",
      "Training Epoch: 5 [9450/36450]\tLoss: 816.5889\n",
      "Training Epoch: 5 [9500/36450]\tLoss: 821.7516\n",
      "Training Epoch: 5 [9550/36450]\tLoss: 793.9071\n",
      "Training Epoch: 5 [9600/36450]\tLoss: 832.9646\n",
      "Training Epoch: 5 [9650/36450]\tLoss: 776.2631\n",
      "Training Epoch: 5 [9700/36450]\tLoss: 822.0073\n",
      "Training Epoch: 5 [9750/36450]\tLoss: 841.0777\n",
      "Training Epoch: 5 [9800/36450]\tLoss: 869.0698\n",
      "Training Epoch: 5 [9850/36450]\tLoss: 833.7509\n",
      "Training Epoch: 5 [9900/36450]\tLoss: 830.3406\n",
      "Training Epoch: 5 [9950/36450]\tLoss: 782.7666\n",
      "Training Epoch: 5 [10000/36450]\tLoss: 836.8196\n",
      "Training Epoch: 5 [10050/36450]\tLoss: 821.2234\n",
      "Training Epoch: 5 [10100/36450]\tLoss: 833.7366\n",
      "Training Epoch: 5 [10150/36450]\tLoss: 831.2025\n",
      "Training Epoch: 5 [10200/36450]\tLoss: 806.5066\n",
      "Training Epoch: 5 [10250/36450]\tLoss: 780.2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [10300/36450]\tLoss: 821.7768\n",
      "Training Epoch: 5 [10350/36450]\tLoss: 859.8941\n",
      "Training Epoch: 5 [10400/36450]\tLoss: 769.5270\n",
      "Training Epoch: 5 [10450/36450]\tLoss: 844.3520\n",
      "Training Epoch: 5 [10500/36450]\tLoss: 828.7509\n",
      "Training Epoch: 5 [10550/36450]\tLoss: 830.3550\n",
      "Training Epoch: 5 [10600/36450]\tLoss: 784.8530\n",
      "Training Epoch: 5 [10650/36450]\tLoss: 830.3626\n",
      "Training Epoch: 5 [10700/36450]\tLoss: 797.7712\n",
      "Training Epoch: 5 [10750/36450]\tLoss: 846.9479\n",
      "Training Epoch: 5 [10800/36450]\tLoss: 831.1483\n",
      "Training Epoch: 5 [10850/36450]\tLoss: 776.9865\n",
      "Training Epoch: 5 [10900/36450]\tLoss: 819.3730\n",
      "Training Epoch: 5 [10950/36450]\tLoss: 771.8386\n",
      "Training Epoch: 5 [11000/36450]\tLoss: 789.3517\n",
      "Training Epoch: 5 [11050/36450]\tLoss: 841.0445\n",
      "Training Epoch: 5 [11100/36450]\tLoss: 776.0239\n",
      "Training Epoch: 5 [11150/36450]\tLoss: 773.3857\n",
      "Training Epoch: 5 [11200/36450]\tLoss: 793.7288\n",
      "Training Epoch: 5 [11250/36450]\tLoss: 825.7404\n",
      "Training Epoch: 5 [11300/36450]\tLoss: 799.4137\n",
      "Training Epoch: 5 [11350/36450]\tLoss: 826.0554\n",
      "Training Epoch: 5 [11400/36450]\tLoss: 839.5610\n",
      "Training Epoch: 5 [11450/36450]\tLoss: 799.1378\n",
      "Training Epoch: 5 [11500/36450]\tLoss: 839.4575\n",
      "Training Epoch: 5 [11550/36450]\tLoss: 788.5334\n",
      "Training Epoch: 5 [11600/36450]\tLoss: 834.3388\n",
      "Training Epoch: 5 [11650/36450]\tLoss: 779.6656\n",
      "Training Epoch: 5 [11700/36450]\tLoss: 825.4088\n",
      "Training Epoch: 5 [11750/36450]\tLoss: 824.4345\n",
      "Training Epoch: 5 [11800/36450]\tLoss: 844.3295\n",
      "Training Epoch: 5 [11850/36450]\tLoss: 773.6566\n",
      "Training Epoch: 5 [11900/36450]\tLoss: 825.1461\n",
      "Training Epoch: 5 [11950/36450]\tLoss: 846.6548\n",
      "Training Epoch: 5 [12000/36450]\tLoss: 823.1893\n",
      "Training Epoch: 5 [12050/36450]\tLoss: 801.2908\n",
      "Training Epoch: 5 [12100/36450]\tLoss: 788.7667\n",
      "Training Epoch: 5 [12150/36450]\tLoss: 793.4083\n",
      "Training Epoch: 5 [12200/36450]\tLoss: 786.3516\n",
      "Training Epoch: 5 [12250/36450]\tLoss: 781.9786\n",
      "Training Epoch: 5 [12300/36450]\tLoss: 804.3167\n",
      "Training Epoch: 5 [12350/36450]\tLoss: 793.7623\n",
      "Training Epoch: 5 [12400/36450]\tLoss: 786.2456\n",
      "Training Epoch: 5 [12450/36450]\tLoss: 804.0114\n",
      "Training Epoch: 5 [12500/36450]\tLoss: 819.8137\n",
      "Training Epoch: 5 [12550/36450]\tLoss: 786.0090\n",
      "Training Epoch: 5 [12600/36450]\tLoss: 768.6438\n",
      "Training Epoch: 5 [12650/36450]\tLoss: 772.4665\n",
      "Training Epoch: 5 [12700/36450]\tLoss: 773.4231\n",
      "Training Epoch: 5 [12750/36450]\tLoss: 774.7797\n",
      "Training Epoch: 5 [12800/36450]\tLoss: 836.6070\n",
      "Training Epoch: 5 [12850/36450]\tLoss: 819.1768\n",
      "Training Epoch: 5 [12900/36450]\tLoss: 799.8910\n",
      "Training Epoch: 5 [12950/36450]\tLoss: 800.8365\n",
      "Training Epoch: 5 [13000/36450]\tLoss: 815.5911\n",
      "Training Epoch: 5 [13050/36450]\tLoss: 812.8932\n",
      "Training Epoch: 5 [13100/36450]\tLoss: 822.7583\n",
      "Training Epoch: 5 [13150/36450]\tLoss: 775.8592\n",
      "Training Epoch: 5 [13200/36450]\tLoss: 832.8027\n",
      "Training Epoch: 5 [13250/36450]\tLoss: 815.3682\n",
      "Training Epoch: 5 [13300/36450]\tLoss: 797.9250\n",
      "Training Epoch: 5 [13350/36450]\tLoss: 777.3942\n",
      "Training Epoch: 5 [13400/36450]\tLoss: 760.4756\n",
      "Training Epoch: 5 [13450/36450]\tLoss: 772.3337\n",
      "Training Epoch: 5 [13500/36450]\tLoss: 777.0312\n",
      "Training Epoch: 5 [13550/36450]\tLoss: 823.1959\n",
      "Training Epoch: 5 [13600/36450]\tLoss: 792.6725\n",
      "Training Epoch: 5 [13650/36450]\tLoss: 791.4023\n",
      "Training Epoch: 5 [13700/36450]\tLoss: 875.0375\n",
      "Training Epoch: 5 [13750/36450]\tLoss: 795.2737\n",
      "Training Epoch: 5 [13800/36450]\tLoss: 753.4552\n",
      "Training Epoch: 5 [13850/36450]\tLoss: 804.2121\n",
      "Training Epoch: 5 [13900/36450]\tLoss: 815.9874\n",
      "Training Epoch: 5 [13950/36450]\tLoss: 810.2262\n",
      "Training Epoch: 5 [14000/36450]\tLoss: 800.4664\n",
      "Training Epoch: 5 [14050/36450]\tLoss: 839.5440\n",
      "Training Epoch: 5 [14100/36450]\tLoss: 797.8003\n",
      "Training Epoch: 5 [14150/36450]\tLoss: 808.6765\n",
      "Training Epoch: 5 [14200/36450]\tLoss: 774.8022\n",
      "Training Epoch: 5 [14250/36450]\tLoss: 826.8765\n",
      "Training Epoch: 5 [14300/36450]\tLoss: 809.0876\n",
      "Training Epoch: 5 [14350/36450]\tLoss: 816.8088\n",
      "Training Epoch: 5 [14400/36450]\tLoss: 768.3193\n",
      "Training Epoch: 5 [14450/36450]\tLoss: 815.8803\n",
      "Training Epoch: 5 [14500/36450]\tLoss: 833.4457\n",
      "Training Epoch: 5 [14550/36450]\tLoss: 834.4618\n",
      "Training Epoch: 5 [14600/36450]\tLoss: 832.8068\n",
      "Training Epoch: 5 [14650/36450]\tLoss: 846.3727\n",
      "Training Epoch: 5 [14700/36450]\tLoss: 767.4595\n",
      "Training Epoch: 5 [14750/36450]\tLoss: 804.0336\n",
      "Training Epoch: 5 [14800/36450]\tLoss: 847.6827\n",
      "Training Epoch: 5 [14850/36450]\tLoss: 797.5014\n",
      "Training Epoch: 5 [14900/36450]\tLoss: 820.5610\n",
      "Training Epoch: 5 [14950/36450]\tLoss: 776.3152\n",
      "Training Epoch: 5 [15000/36450]\tLoss: 797.1061\n",
      "Training Epoch: 5 [15050/36450]\tLoss: 820.6121\n",
      "Training Epoch: 5 [15100/36450]\tLoss: 829.5648\n",
      "Training Epoch: 5 [15150/36450]\tLoss: 851.9115\n",
      "Training Epoch: 5 [15200/36450]\tLoss: 807.6620\n",
      "Training Epoch: 5 [15250/36450]\tLoss: 802.1955\n",
      "Training Epoch: 5 [15300/36450]\tLoss: 815.7617\n",
      "Training Epoch: 5 [15350/36450]\tLoss: 787.3143\n",
      "Training Epoch: 5 [15400/36450]\tLoss: 822.5441\n",
      "Training Epoch: 5 [15450/36450]\tLoss: 750.6268\n",
      "Training Epoch: 5 [15500/36450]\tLoss: 808.0060\n",
      "Training Epoch: 5 [15550/36450]\tLoss: 808.5059\n",
      "Training Epoch: 5 [15600/36450]\tLoss: 804.2775\n",
      "Training Epoch: 5 [15650/36450]\tLoss: 801.5819\n",
      "Training Epoch: 5 [15700/36450]\tLoss: 795.6613\n",
      "Training Epoch: 5 [15750/36450]\tLoss: 787.9151\n",
      "Training Epoch: 5 [15800/36450]\tLoss: 805.2960\n",
      "Training Epoch: 5 [15850/36450]\tLoss: 791.3260\n",
      "Training Epoch: 5 [15900/36450]\tLoss: 840.2003\n",
      "Training Epoch: 5 [15950/36450]\tLoss: 819.8052\n",
      "Training Epoch: 5 [16000/36450]\tLoss: 749.1182\n",
      "Training Epoch: 5 [16050/36450]\tLoss: 771.3755\n",
      "Training Epoch: 5 [16100/36450]\tLoss: 762.6915\n",
      "Training Epoch: 5 [16150/36450]\tLoss: 848.3151\n",
      "Training Epoch: 5 [16200/36450]\tLoss: 809.1771\n",
      "Training Epoch: 5 [16250/36450]\tLoss: 787.3729\n",
      "Training Epoch: 5 [16300/36450]\tLoss: 841.6443\n",
      "Training Epoch: 5 [16350/36450]\tLoss: 796.6796\n",
      "Training Epoch: 5 [16400/36450]\tLoss: 831.8858\n",
      "Training Epoch: 5 [16450/36450]\tLoss: 866.0468\n",
      "Training Epoch: 5 [16500/36450]\tLoss: 780.0593\n",
      "Training Epoch: 5 [16550/36450]\tLoss: 762.8975\n",
      "Training Epoch: 5 [16600/36450]\tLoss: 792.8432\n",
      "Training Epoch: 5 [16650/36450]\tLoss: 772.4706\n",
      "Training Epoch: 5 [16700/36450]\tLoss: 772.9544\n",
      "Training Epoch: 5 [16750/36450]\tLoss: 806.0201\n",
      "Training Epoch: 5 [16800/36450]\tLoss: 774.5507\n",
      "Training Epoch: 5 [16850/36450]\tLoss: 798.3878\n",
      "Training Epoch: 5 [16900/36450]\tLoss: 852.4080\n",
      "Training Epoch: 5 [16950/36450]\tLoss: 784.3214\n",
      "Training Epoch: 5 [17000/36450]\tLoss: 855.6508\n",
      "Training Epoch: 5 [17050/36450]\tLoss: 774.0135\n",
      "Training Epoch: 5 [17100/36450]\tLoss: 772.5669\n",
      "Training Epoch: 5 [17150/36450]\tLoss: 787.7050\n",
      "Training Epoch: 5 [17200/36450]\tLoss: 780.7340\n",
      "Training Epoch: 5 [17250/36450]\tLoss: 828.1493\n",
      "Training Epoch: 5 [17300/36450]\tLoss: 806.3989\n",
      "Training Epoch: 5 [17350/36450]\tLoss: 820.1641\n",
      "Training Epoch: 5 [17400/36450]\tLoss: 752.3521\n",
      "Training Epoch: 5 [17450/36450]\tLoss: 801.7067\n",
      "Training Epoch: 5 [17500/36450]\tLoss: 784.3068\n",
      "Training Epoch: 5 [17550/36450]\tLoss: 797.6823\n",
      "Training Epoch: 5 [17600/36450]\tLoss: 777.2910\n",
      "Training Epoch: 5 [17650/36450]\tLoss: 846.3356\n",
      "Training Epoch: 5 [17700/36450]\tLoss: 733.4774\n",
      "Training Epoch: 5 [17750/36450]\tLoss: 822.2491\n",
      "Training Epoch: 5 [17800/36450]\tLoss: 789.7943\n",
      "Training Epoch: 5 [17850/36450]\tLoss: 791.2421\n",
      "Training Epoch: 5 [17900/36450]\tLoss: 807.3307\n",
      "Training Epoch: 5 [17950/36450]\tLoss: 829.1149\n",
      "Training Epoch: 5 [18000/36450]\tLoss: 760.6545\n",
      "Training Epoch: 5 [18050/36450]\tLoss: 757.9769\n",
      "Training Epoch: 5 [18100/36450]\tLoss: 812.1221\n",
      "Training Epoch: 5 [18150/36450]\tLoss: 808.8436\n",
      "Training Epoch: 5 [18200/36450]\tLoss: 777.8442\n",
      "Training Epoch: 5 [18250/36450]\tLoss: 791.2861\n",
      "Training Epoch: 5 [18300/36450]\tLoss: 758.1133\n",
      "Training Epoch: 5 [18350/36450]\tLoss: 795.9050\n",
      "Training Epoch: 5 [18400/36450]\tLoss: 799.5167\n",
      "Training Epoch: 5 [18450/36450]\tLoss: 797.1791\n",
      "Training Epoch: 5 [18500/36450]\tLoss: 791.6148\n",
      "Training Epoch: 5 [18550/36450]\tLoss: 803.7885\n",
      "Training Epoch: 5 [18600/36450]\tLoss: 778.2382\n",
      "Training Epoch: 5 [18650/36450]\tLoss: 781.5121\n",
      "Training Epoch: 5 [18700/36450]\tLoss: 777.3784\n",
      "Training Epoch: 5 [18750/36450]\tLoss: 837.3314\n",
      "Training Epoch: 5 [18800/36450]\tLoss: 794.0194\n",
      "Training Epoch: 5 [18850/36450]\tLoss: 750.0148\n",
      "Training Epoch: 5 [18900/36450]\tLoss: 779.9525\n",
      "Training Epoch: 5 [18950/36450]\tLoss: 768.0579\n",
      "Training Epoch: 5 [19000/36450]\tLoss: 792.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [19050/36450]\tLoss: 801.5441\n",
      "Training Epoch: 5 [19100/36450]\tLoss: 797.5522\n",
      "Training Epoch: 5 [19150/36450]\tLoss: 797.0162\n",
      "Training Epoch: 5 [19200/36450]\tLoss: 759.4407\n",
      "Training Epoch: 5 [19250/36450]\tLoss: 784.6899\n",
      "Training Epoch: 5 [19300/36450]\tLoss: 818.1829\n",
      "Training Epoch: 5 [19350/36450]\tLoss: 794.0321\n",
      "Training Epoch: 5 [19400/36450]\tLoss: 819.9943\n",
      "Training Epoch: 5 [19450/36450]\tLoss: 780.8524\n",
      "Training Epoch: 5 [19500/36450]\tLoss: 810.4919\n",
      "Training Epoch: 5 [19550/36450]\tLoss: 750.3526\n",
      "Training Epoch: 5 [19600/36450]\tLoss: 807.3706\n",
      "Training Epoch: 5 [19650/36450]\tLoss: 799.5518\n",
      "Training Epoch: 5 [19700/36450]\tLoss: 817.8765\n",
      "Training Epoch: 5 [19750/36450]\tLoss: 775.9433\n",
      "Training Epoch: 5 [19800/36450]\tLoss: 774.1768\n",
      "Training Epoch: 5 [19850/36450]\tLoss: 783.5479\n",
      "Training Epoch: 5 [19900/36450]\tLoss: 822.7756\n",
      "Training Epoch: 5 [19950/36450]\tLoss: 801.8224\n",
      "Training Epoch: 5 [20000/36450]\tLoss: 813.4387\n",
      "Training Epoch: 5 [20050/36450]\tLoss: 806.4115\n",
      "Training Epoch: 5 [20100/36450]\tLoss: 792.2859\n",
      "Training Epoch: 5 [20150/36450]\tLoss: 791.8911\n",
      "Training Epoch: 5 [20200/36450]\tLoss: 754.2783\n",
      "Training Epoch: 5 [20250/36450]\tLoss: 759.5488\n",
      "Training Epoch: 5 [20300/36450]\tLoss: 779.5022\n",
      "Training Epoch: 5 [20350/36450]\tLoss: 779.0494\n",
      "Training Epoch: 5 [20400/36450]\tLoss: 813.5063\n",
      "Training Epoch: 5 [20450/36450]\tLoss: 781.8431\n",
      "Training Epoch: 5 [20500/36450]\tLoss: 759.3011\n",
      "Training Epoch: 5 [20550/36450]\tLoss: 798.0045\n",
      "Training Epoch: 5 [20600/36450]\tLoss: 750.5324\n",
      "Training Epoch: 5 [20650/36450]\tLoss: 741.1122\n",
      "Training Epoch: 5 [20700/36450]\tLoss: 789.1204\n",
      "Training Epoch: 5 [20750/36450]\tLoss: 807.1566\n",
      "Training Epoch: 5 [20800/36450]\tLoss: 844.8264\n",
      "Training Epoch: 5 [20850/36450]\tLoss: 771.3749\n",
      "Training Epoch: 5 [20900/36450]\tLoss: 831.8440\n",
      "Training Epoch: 5 [20950/36450]\tLoss: 770.0909\n",
      "Training Epoch: 5 [21000/36450]\tLoss: 784.0471\n",
      "Training Epoch: 5 [21050/36450]\tLoss: 828.1882\n",
      "Training Epoch: 5 [21100/36450]\tLoss: 789.5416\n",
      "Training Epoch: 5 [21150/36450]\tLoss: 774.0419\n",
      "Training Epoch: 5 [21200/36450]\tLoss: 797.9742\n",
      "Training Epoch: 5 [21250/36450]\tLoss: 774.4468\n",
      "Training Epoch: 5 [21300/36450]\tLoss: 777.6494\n",
      "Training Epoch: 5 [21350/36450]\tLoss: 765.7756\n",
      "Training Epoch: 5 [21400/36450]\tLoss: 820.3159\n",
      "Training Epoch: 5 [21450/36450]\tLoss: 806.6581\n",
      "Training Epoch: 5 [21500/36450]\tLoss: 797.2233\n",
      "Training Epoch: 5 [21550/36450]\tLoss: 775.6343\n",
      "Training Epoch: 5 [21600/36450]\tLoss: 775.0201\n",
      "Training Epoch: 5 [21650/36450]\tLoss: 801.9152\n",
      "Training Epoch: 5 [21700/36450]\tLoss: 792.8729\n",
      "Training Epoch: 5 [21750/36450]\tLoss: 805.9683\n",
      "Training Epoch: 5 [21800/36450]\tLoss: 781.5536\n",
      "Training Epoch: 5 [21850/36450]\tLoss: 802.2955\n",
      "Training Epoch: 5 [21900/36450]\tLoss: 802.4239\n",
      "Training Epoch: 5 [21950/36450]\tLoss: 791.5837\n",
      "Training Epoch: 5 [22000/36450]\tLoss: 749.2477\n",
      "Training Epoch: 5 [22050/36450]\tLoss: 768.3683\n",
      "Training Epoch: 5 [22100/36450]\tLoss: 745.0911\n",
      "Training Epoch: 5 [22150/36450]\tLoss: 737.1754\n",
      "Training Epoch: 5 [22200/36450]\tLoss: 769.2137\n",
      "Training Epoch: 5 [22250/36450]\tLoss: 784.9406\n",
      "Training Epoch: 5 [22300/36450]\tLoss: 828.6647\n",
      "Training Epoch: 5 [22350/36450]\tLoss: 801.1414\n",
      "Training Epoch: 5 [22400/36450]\tLoss: 792.7688\n",
      "Training Epoch: 5 [22450/36450]\tLoss: 776.0151\n",
      "Training Epoch: 5 [22500/36450]\tLoss: 755.1804\n",
      "Training Epoch: 5 [22550/36450]\tLoss: 788.2243\n",
      "Training Epoch: 5 [22600/36450]\tLoss: 777.9900\n",
      "Training Epoch: 5 [22650/36450]\tLoss: 734.9921\n",
      "Training Epoch: 5 [22700/36450]\tLoss: 760.6082\n",
      "Training Epoch: 5 [22750/36450]\tLoss: 800.6137\n",
      "Training Epoch: 5 [22800/36450]\tLoss: 791.7255\n",
      "Training Epoch: 5 [22850/36450]\tLoss: 809.3615\n",
      "Training Epoch: 5 [22900/36450]\tLoss: 783.5624\n",
      "Training Epoch: 5 [22950/36450]\tLoss: 801.8564\n",
      "Training Epoch: 5 [23000/36450]\tLoss: 748.4949\n",
      "Training Epoch: 5 [23050/36450]\tLoss: 820.2956\n",
      "Training Epoch: 5 [23100/36450]\tLoss: 773.0829\n",
      "Training Epoch: 5 [23150/36450]\tLoss: 747.0350\n",
      "Training Epoch: 5 [23200/36450]\tLoss: 738.0354\n",
      "Training Epoch: 5 [23250/36450]\tLoss: 811.5950\n",
      "Training Epoch: 5 [23300/36450]\tLoss: 804.3514\n",
      "Training Epoch: 5 [23350/36450]\tLoss: 758.2333\n",
      "Training Epoch: 5 [23400/36450]\tLoss: 731.3015\n",
      "Training Epoch: 5 [23450/36450]\tLoss: 823.5343\n",
      "Training Epoch: 5 [23500/36450]\tLoss: 804.6252\n",
      "Training Epoch: 5 [23550/36450]\tLoss: 799.5997\n",
      "Training Epoch: 5 [23600/36450]\tLoss: 815.9635\n",
      "Training Epoch: 5 [23650/36450]\tLoss: 782.5669\n",
      "Training Epoch: 5 [23700/36450]\tLoss: 779.7001\n",
      "Training Epoch: 5 [23750/36450]\tLoss: 747.1259\n",
      "Training Epoch: 5 [23800/36450]\tLoss: 777.0745\n",
      "Training Epoch: 5 [23850/36450]\tLoss: 767.2922\n",
      "Training Epoch: 5 [23900/36450]\tLoss: 778.1160\n",
      "Training Epoch: 5 [23950/36450]\tLoss: 737.0722\n",
      "Training Epoch: 5 [24000/36450]\tLoss: 793.5726\n",
      "Training Epoch: 5 [24050/36450]\tLoss: 812.0775\n",
      "Training Epoch: 5 [24100/36450]\tLoss: 785.2719\n",
      "Training Epoch: 5 [24150/36450]\tLoss: 750.6893\n",
      "Training Epoch: 5 [24200/36450]\tLoss: 799.7173\n",
      "Training Epoch: 5 [24250/36450]\tLoss: 729.2493\n",
      "Training Epoch: 5 [24300/36450]\tLoss: 794.0262\n",
      "Training Epoch: 5 [24350/36450]\tLoss: 756.4803\n",
      "Training Epoch: 5 [24400/36450]\tLoss: 756.4650\n",
      "Training Epoch: 5 [24450/36450]\tLoss: 773.4503\n",
      "Training Epoch: 5 [24500/36450]\tLoss: 765.7955\n",
      "Training Epoch: 5 [24550/36450]\tLoss: 815.1327\n",
      "Training Epoch: 5 [24600/36450]\tLoss: 826.9145\n",
      "Training Epoch: 5 [24650/36450]\tLoss: 766.5419\n",
      "Training Epoch: 5 [24700/36450]\tLoss: 759.6838\n",
      "Training Epoch: 5 [24750/36450]\tLoss: 743.5529\n",
      "Training Epoch: 5 [24800/36450]\tLoss: 783.3331\n",
      "Training Epoch: 5 [24850/36450]\tLoss: 785.8037\n",
      "Training Epoch: 5 [24900/36450]\tLoss: 773.4955\n",
      "Training Epoch: 5 [24950/36450]\tLoss: 781.2447\n",
      "Training Epoch: 5 [25000/36450]\tLoss: 808.3997\n",
      "Training Epoch: 5 [25050/36450]\tLoss: 823.1017\n",
      "Training Epoch: 5 [25100/36450]\tLoss: 780.3875\n",
      "Training Epoch: 5 [25150/36450]\tLoss: 774.2758\n",
      "Training Epoch: 5 [25200/36450]\tLoss: 795.1521\n",
      "Training Epoch: 5 [25250/36450]\tLoss: 803.6171\n",
      "Training Epoch: 5 [25300/36450]\tLoss: 786.5733\n",
      "Training Epoch: 5 [25350/36450]\tLoss: 835.1404\n",
      "Training Epoch: 5 [25400/36450]\tLoss: 740.4554\n",
      "Training Epoch: 5 [25450/36450]\tLoss: 790.2451\n",
      "Training Epoch: 5 [25500/36450]\tLoss: 749.9977\n",
      "Training Epoch: 5 [25550/36450]\tLoss: 786.1381\n",
      "Training Epoch: 5 [25600/36450]\tLoss: 789.7560\n",
      "Training Epoch: 5 [25650/36450]\tLoss: 791.2765\n",
      "Training Epoch: 5 [25700/36450]\tLoss: 790.5796\n",
      "Training Epoch: 5 [25750/36450]\tLoss: 793.4234\n",
      "Training Epoch: 5 [25800/36450]\tLoss: 787.0636\n",
      "Training Epoch: 5 [25850/36450]\tLoss: 804.2939\n",
      "Training Epoch: 5 [25900/36450]\tLoss: 782.3758\n",
      "Training Epoch: 5 [25950/36450]\tLoss: 787.8770\n",
      "Training Epoch: 5 [26000/36450]\tLoss: 787.2229\n",
      "Training Epoch: 5 [26050/36450]\tLoss: 785.5677\n",
      "Training Epoch: 5 [26100/36450]\tLoss: 818.9976\n",
      "Training Epoch: 5 [26150/36450]\tLoss: 821.6453\n",
      "Training Epoch: 5 [26200/36450]\tLoss: 793.0525\n",
      "Training Epoch: 5 [26250/36450]\tLoss: 750.5131\n",
      "Training Epoch: 5 [26300/36450]\tLoss: 717.3505\n",
      "Training Epoch: 5 [26350/36450]\tLoss: 724.9904\n",
      "Training Epoch: 5 [26400/36450]\tLoss: 763.2755\n",
      "Training Epoch: 5 [26450/36450]\tLoss: 804.9680\n",
      "Training Epoch: 5 [26500/36450]\tLoss: 757.5649\n",
      "Training Epoch: 5 [26550/36450]\tLoss: 824.2015\n",
      "Training Epoch: 5 [26600/36450]\tLoss: 754.8671\n",
      "Training Epoch: 5 [26650/36450]\tLoss: 785.3220\n",
      "Training Epoch: 5 [26700/36450]\tLoss: 791.9564\n",
      "Training Epoch: 5 [26750/36450]\tLoss: 750.8098\n",
      "Training Epoch: 5 [26800/36450]\tLoss: 734.8420\n",
      "Training Epoch: 5 [26850/36450]\tLoss: 789.4448\n",
      "Training Epoch: 5 [26900/36450]\tLoss: 760.5479\n",
      "Training Epoch: 5 [26950/36450]\tLoss: 774.5286\n",
      "Training Epoch: 5 [27000/36450]\tLoss: 776.1487\n",
      "Training Epoch: 5 [27050/36450]\tLoss: 755.8890\n",
      "Training Epoch: 5 [27100/36450]\tLoss: 749.6951\n",
      "Training Epoch: 5 [27150/36450]\tLoss: 780.0861\n",
      "Training Epoch: 5 [27200/36450]\tLoss: 818.0297\n",
      "Training Epoch: 5 [27250/36450]\tLoss: 787.9246\n",
      "Training Epoch: 5 [27300/36450]\tLoss: 765.6183\n",
      "Training Epoch: 5 [27350/36450]\tLoss: 731.7802\n",
      "Training Epoch: 5 [27400/36450]\tLoss: 767.2148\n",
      "Training Epoch: 5 [27450/36450]\tLoss: 804.8483\n",
      "Training Epoch: 5 [27500/36450]\tLoss: 765.3028\n",
      "Training Epoch: 5 [27550/36450]\tLoss: 729.6516\n",
      "Training Epoch: 5 [27600/36450]\tLoss: 716.1846\n",
      "Training Epoch: 5 [27650/36450]\tLoss: 798.5946\n",
      "Training Epoch: 5 [27700/36450]\tLoss: 752.3901\n",
      "Training Epoch: 5 [27750/36450]\tLoss: 757.6492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [27800/36450]\tLoss: 746.8433\n",
      "Training Epoch: 5 [27850/36450]\tLoss: 830.4620\n",
      "Training Epoch: 5 [27900/36450]\tLoss: 747.0371\n",
      "Training Epoch: 5 [27950/36450]\tLoss: 773.0267\n",
      "Training Epoch: 5 [28000/36450]\tLoss: 817.6741\n",
      "Training Epoch: 5 [28050/36450]\tLoss: 817.6792\n",
      "Training Epoch: 5 [28100/36450]\tLoss: 777.1012\n",
      "Training Epoch: 5 [28150/36450]\tLoss: 772.5740\n",
      "Training Epoch: 5 [28200/36450]\tLoss: 789.8718\n",
      "Training Epoch: 5 [28250/36450]\tLoss: 778.6571\n",
      "Training Epoch: 5 [28300/36450]\tLoss: 789.0587\n",
      "Training Epoch: 5 [28350/36450]\tLoss: 734.7875\n",
      "Training Epoch: 5 [28400/36450]\tLoss: 809.5748\n",
      "Training Epoch: 5 [28450/36450]\tLoss: 756.0505\n",
      "Training Epoch: 5 [28500/36450]\tLoss: 754.3138\n",
      "Training Epoch: 5 [28550/36450]\tLoss: 774.8309\n",
      "Training Epoch: 5 [28600/36450]\tLoss: 769.8242\n",
      "Training Epoch: 5 [28650/36450]\tLoss: 761.6843\n",
      "Training Epoch: 5 [28700/36450]\tLoss: 788.7787\n",
      "Training Epoch: 5 [28750/36450]\tLoss: 756.4099\n",
      "Training Epoch: 5 [28800/36450]\tLoss: 762.4213\n",
      "Training Epoch: 5 [28850/36450]\tLoss: 823.9357\n",
      "Training Epoch: 5 [28900/36450]\tLoss: 786.3452\n",
      "Training Epoch: 5 [28950/36450]\tLoss: 741.2902\n",
      "Training Epoch: 5 [29000/36450]\tLoss: 727.2688\n",
      "Training Epoch: 5 [29050/36450]\tLoss: 752.2687\n",
      "Training Epoch: 5 [29100/36450]\tLoss: 785.0829\n",
      "Training Epoch: 5 [29150/36450]\tLoss: 777.2964\n",
      "Training Epoch: 5 [29200/36450]\tLoss: 782.3372\n",
      "Training Epoch: 5 [29250/36450]\tLoss: 778.8859\n",
      "Training Epoch: 5 [29300/36450]\tLoss: 747.0193\n",
      "Training Epoch: 5 [29350/36450]\tLoss: 744.7482\n",
      "Training Epoch: 5 [29400/36450]\tLoss: 762.1654\n",
      "Training Epoch: 5 [29450/36450]\tLoss: 722.0308\n",
      "Training Epoch: 5 [29500/36450]\tLoss: 772.7570\n",
      "Training Epoch: 5 [29550/36450]\tLoss: 788.1995\n",
      "Training Epoch: 5 [29600/36450]\tLoss: 788.9708\n",
      "Training Epoch: 5 [29650/36450]\tLoss: 757.0380\n",
      "Training Epoch: 5 [29700/36450]\tLoss: 787.8167\n",
      "Training Epoch: 5 [29750/36450]\tLoss: 758.0942\n",
      "Training Epoch: 5 [29800/36450]\tLoss: 798.5995\n",
      "Training Epoch: 5 [29850/36450]\tLoss: 769.2316\n",
      "Training Epoch: 5 [29900/36450]\tLoss: 795.9372\n",
      "Training Epoch: 5 [29950/36450]\tLoss: 793.2583\n",
      "Training Epoch: 5 [30000/36450]\tLoss: 791.4086\n",
      "Training Epoch: 5 [30050/36450]\tLoss: 779.3873\n",
      "Training Epoch: 5 [30100/36450]\tLoss: 793.7810\n",
      "Training Epoch: 5 [30150/36450]\tLoss: 785.3799\n",
      "Training Epoch: 5 [30200/36450]\tLoss: 831.1816\n",
      "Training Epoch: 5 [30250/36450]\tLoss: 770.5663\n",
      "Training Epoch: 5 [30300/36450]\tLoss: 771.7545\n",
      "Training Epoch: 5 [30350/36450]\tLoss: 755.6350\n",
      "Training Epoch: 5 [30400/36450]\tLoss: 795.4045\n",
      "Training Epoch: 5 [30450/36450]\tLoss: 771.9301\n",
      "Training Epoch: 5 [30500/36450]\tLoss: 799.5029\n",
      "Training Epoch: 5 [30550/36450]\tLoss: 722.7891\n",
      "Training Epoch: 5 [30600/36450]\tLoss: 794.0325\n",
      "Training Epoch: 5 [30650/36450]\tLoss: 794.8978\n",
      "Training Epoch: 5 [30700/36450]\tLoss: 783.0574\n",
      "Training Epoch: 5 [30750/36450]\tLoss: 757.3232\n",
      "Training Epoch: 5 [30800/36450]\tLoss: 790.0667\n",
      "Training Epoch: 5 [30850/36450]\tLoss: 752.6648\n",
      "Training Epoch: 5 [30900/36450]\tLoss: 796.2924\n",
      "Training Epoch: 5 [30950/36450]\tLoss: 797.5602\n",
      "Training Epoch: 5 [31000/36450]\tLoss: 762.1458\n",
      "Training Epoch: 5 [31050/36450]\tLoss: 733.7141\n",
      "Training Epoch: 5 [31100/36450]\tLoss: 764.1706\n",
      "Training Epoch: 5 [31150/36450]\tLoss: 781.0577\n",
      "Training Epoch: 5 [31200/36450]\tLoss: 743.1263\n",
      "Training Epoch: 5 [31250/36450]\tLoss: 742.3569\n",
      "Training Epoch: 5 [31300/36450]\tLoss: 721.3750\n",
      "Training Epoch: 5 [31350/36450]\tLoss: 774.9564\n",
      "Training Epoch: 5 [31400/36450]\tLoss: 789.2510\n",
      "Training Epoch: 5 [31450/36450]\tLoss: 802.3127\n",
      "Training Epoch: 5 [31500/36450]\tLoss: 734.4218\n",
      "Training Epoch: 5 [31550/36450]\tLoss: 784.0522\n",
      "Training Epoch: 5 [31600/36450]\tLoss: 754.2615\n",
      "Training Epoch: 5 [31650/36450]\tLoss: 769.5344\n",
      "Training Epoch: 5 [31700/36450]\tLoss: 749.0812\n",
      "Training Epoch: 5 [31750/36450]\tLoss: 750.1912\n",
      "Training Epoch: 5 [31800/36450]\tLoss: 719.0663\n",
      "Training Epoch: 5 [31850/36450]\tLoss: 756.1859\n",
      "Training Epoch: 5 [31900/36450]\tLoss: 754.9654\n",
      "Training Epoch: 5 [31950/36450]\tLoss: 762.0572\n",
      "Training Epoch: 5 [32000/36450]\tLoss: 752.8634\n",
      "Training Epoch: 5 [32050/36450]\tLoss: 742.7445\n",
      "Training Epoch: 5 [32100/36450]\tLoss: 783.6223\n",
      "Training Epoch: 5 [32150/36450]\tLoss: 741.8574\n",
      "Training Epoch: 5 [32200/36450]\tLoss: 783.9724\n",
      "Training Epoch: 5 [32250/36450]\tLoss: 775.2426\n",
      "Training Epoch: 5 [32300/36450]\tLoss: 750.2229\n",
      "Training Epoch: 5 [32350/36450]\tLoss: 766.1779\n",
      "Training Epoch: 5 [32400/36450]\tLoss: 764.8764\n",
      "Training Epoch: 5 [32450/36450]\tLoss: 814.0193\n",
      "Training Epoch: 5 [32500/36450]\tLoss: 784.9839\n",
      "Training Epoch: 5 [32550/36450]\tLoss: 784.1671\n",
      "Training Epoch: 5 [32600/36450]\tLoss: 731.9755\n",
      "Training Epoch: 5 [32650/36450]\tLoss: 762.9618\n",
      "Training Epoch: 5 [32700/36450]\tLoss: 794.7430\n",
      "Training Epoch: 5 [32750/36450]\tLoss: 762.0824\n",
      "Training Epoch: 5 [32800/36450]\tLoss: 811.6805\n",
      "Training Epoch: 5 [32850/36450]\tLoss: 775.8929\n",
      "Training Epoch: 5 [32900/36450]\tLoss: 769.6992\n",
      "Training Epoch: 5 [32950/36450]\tLoss: 763.5765\n",
      "Training Epoch: 5 [33000/36450]\tLoss: 795.2336\n",
      "Training Epoch: 5 [33050/36450]\tLoss: 751.5121\n",
      "Training Epoch: 5 [33100/36450]\tLoss: 789.0233\n",
      "Training Epoch: 5 [33150/36450]\tLoss: 802.8974\n",
      "Training Epoch: 5 [33200/36450]\tLoss: 788.3353\n",
      "Training Epoch: 5 [33250/36450]\tLoss: 770.2151\n",
      "Training Epoch: 5 [33300/36450]\tLoss: 793.6412\n",
      "Training Epoch: 5 [33350/36450]\tLoss: 739.2243\n",
      "Training Epoch: 5 [33400/36450]\tLoss: 765.2885\n",
      "Training Epoch: 5 [33450/36450]\tLoss: 730.3723\n",
      "Training Epoch: 5 [33500/36450]\tLoss: 759.3752\n",
      "Training Epoch: 5 [33550/36450]\tLoss: 727.9045\n",
      "Training Epoch: 5 [33600/36450]\tLoss: 768.2215\n",
      "Training Epoch: 5 [33650/36450]\tLoss: 789.9997\n",
      "Training Epoch: 5 [33700/36450]\tLoss: 775.1011\n",
      "Training Epoch: 5 [33750/36450]\tLoss: 725.0148\n",
      "Training Epoch: 5 [33800/36450]\tLoss: 747.2167\n",
      "Training Epoch: 5 [33850/36450]\tLoss: 775.1635\n",
      "Training Epoch: 5 [33900/36450]\tLoss: 776.5854\n",
      "Training Epoch: 5 [33950/36450]\tLoss: 781.4128\n",
      "Training Epoch: 5 [34000/36450]\tLoss: 774.6951\n",
      "Training Epoch: 5 [34050/36450]\tLoss: 742.5731\n",
      "Training Epoch: 5 [34100/36450]\tLoss: 737.2401\n",
      "Training Epoch: 5 [34150/36450]\tLoss: 806.1942\n",
      "Training Epoch: 5 [34200/36450]\tLoss: 711.1271\n",
      "Training Epoch: 5 [34250/36450]\tLoss: 759.4750\n",
      "Training Epoch: 5 [34300/36450]\tLoss: 766.3655\n",
      "Training Epoch: 5 [34350/36450]\tLoss: 739.8151\n",
      "Training Epoch: 5 [34400/36450]\tLoss: 798.8384\n",
      "Training Epoch: 5 [34450/36450]\tLoss: 815.7812\n",
      "Training Epoch: 5 [34500/36450]\tLoss: 778.3973\n",
      "Training Epoch: 5 [34550/36450]\tLoss: 750.3417\n",
      "Training Epoch: 5 [34600/36450]\tLoss: 753.2901\n",
      "Training Epoch: 5 [34650/36450]\tLoss: 749.0557\n",
      "Training Epoch: 5 [34700/36450]\tLoss: 736.3167\n",
      "Training Epoch: 5 [34750/36450]\tLoss: 750.9098\n",
      "Training Epoch: 5 [34800/36450]\tLoss: 786.1455\n",
      "Training Epoch: 5 [34850/36450]\tLoss: 710.8972\n",
      "Training Epoch: 5 [34900/36450]\tLoss: 759.7649\n",
      "Training Epoch: 5 [34950/36450]\tLoss: 731.2400\n",
      "Training Epoch: 5 [35000/36450]\tLoss: 773.4659\n",
      "Training Epoch: 5 [35050/36450]\tLoss: 772.1275\n",
      "Training Epoch: 5 [35100/36450]\tLoss: 760.3563\n",
      "Training Epoch: 5 [35150/36450]\tLoss: 736.9468\n",
      "Training Epoch: 5 [35200/36450]\tLoss: 776.1761\n",
      "Training Epoch: 5 [35250/36450]\tLoss: 834.6880\n",
      "Training Epoch: 5 [35300/36450]\tLoss: 756.9735\n",
      "Training Epoch: 5 [35350/36450]\tLoss: 753.9760\n",
      "Training Epoch: 5 [35400/36450]\tLoss: 754.1386\n",
      "Training Epoch: 5 [35450/36450]\tLoss: 760.9034\n",
      "Training Epoch: 5 [35500/36450]\tLoss: 769.8275\n",
      "Training Epoch: 5 [35550/36450]\tLoss: 824.4850\n",
      "Training Epoch: 5 [35600/36450]\tLoss: 737.3622\n",
      "Training Epoch: 5 [35650/36450]\tLoss: 765.6782\n",
      "Training Epoch: 5 [35700/36450]\tLoss: 764.1314\n",
      "Training Epoch: 5 [35750/36450]\tLoss: 791.0652\n",
      "Training Epoch: 5 [35800/36450]\tLoss: 798.5969\n",
      "Training Epoch: 5 [35850/36450]\tLoss: 690.7933\n",
      "Training Epoch: 5 [35900/36450]\tLoss: 763.9910\n",
      "Training Epoch: 5 [35950/36450]\tLoss: 759.6888\n",
      "Training Epoch: 5 [36000/36450]\tLoss: 785.3357\n",
      "Training Epoch: 5 [36050/36450]\tLoss: 740.1017\n",
      "Training Epoch: 5 [36100/36450]\tLoss: 780.1448\n",
      "Training Epoch: 5 [36150/36450]\tLoss: 771.5529\n",
      "Training Epoch: 5 [36200/36450]\tLoss: 779.0148\n",
      "Training Epoch: 5 [36250/36450]\tLoss: 745.2938\n",
      "Training Epoch: 5 [36300/36450]\tLoss: 802.8124\n",
      "Training Epoch: 5 [36350/36450]\tLoss: 717.5479\n",
      "Training Epoch: 5 [36400/36450]\tLoss: 788.0874\n",
      "Training Epoch: 5 [36450/36450]\tLoss: 740.7031\n",
      "Training Epoch: 5 [4050/4050]\tLoss: 383.9514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [50/36450]\tLoss: 705.2271\n",
      "Training Epoch: 6 [100/36450]\tLoss: 760.2651\n",
      "Training Epoch: 6 [150/36450]\tLoss: 738.5405\n",
      "Training Epoch: 6 [200/36450]\tLoss: 725.2838\n",
      "Training Epoch: 6 [250/36450]\tLoss: 731.9017\n",
      "Training Epoch: 6 [300/36450]\tLoss: 768.3511\n",
      "Training Epoch: 6 [350/36450]\tLoss: 767.5308\n",
      "Training Epoch: 6 [400/36450]\tLoss: 705.4930\n",
      "Training Epoch: 6 [450/36450]\tLoss: 707.8049\n",
      "Training Epoch: 6 [500/36450]\tLoss: 781.7894\n",
      "Training Epoch: 6 [550/36450]\tLoss: 798.7029\n",
      "Training Epoch: 6 [600/36450]\tLoss: 761.7758\n",
      "Training Epoch: 6 [650/36450]\tLoss: 721.4208\n",
      "Training Epoch: 6 [700/36450]\tLoss: 695.1960\n",
      "Training Epoch: 6 [750/36450]\tLoss: 725.7185\n",
      "Training Epoch: 6 [800/36450]\tLoss: 780.5771\n",
      "Training Epoch: 6 [850/36450]\tLoss: 762.0247\n",
      "Training Epoch: 6 [900/36450]\tLoss: 729.5123\n",
      "Training Epoch: 6 [950/36450]\tLoss: 786.6465\n",
      "Training Epoch: 6 [1000/36450]\tLoss: 810.9543\n",
      "Training Epoch: 6 [1050/36450]\tLoss: 761.6205\n",
      "Training Epoch: 6 [1100/36450]\tLoss: 805.4717\n",
      "Training Epoch: 6 [1150/36450]\tLoss: 776.3550\n",
      "Training Epoch: 6 [1200/36450]\tLoss: 745.1936\n",
      "Training Epoch: 6 [1250/36450]\tLoss: 780.8472\n",
      "Training Epoch: 6 [1300/36450]\tLoss: 825.9388\n",
      "Training Epoch: 6 [1350/36450]\tLoss: 852.0258\n",
      "Training Epoch: 6 [1400/36450]\tLoss: 857.2571\n",
      "Training Epoch: 6 [1450/36450]\tLoss: 842.1136\n",
      "Training Epoch: 6 [1500/36450]\tLoss: 780.7569\n",
      "Training Epoch: 6 [1550/36450]\tLoss: 813.7922\n",
      "Training Epoch: 6 [1600/36450]\tLoss: 760.5805\n",
      "Training Epoch: 6 [1650/36450]\tLoss: 756.3730\n",
      "Training Epoch: 6 [1700/36450]\tLoss: 780.7233\n",
      "Training Epoch: 6 [1750/36450]\tLoss: 757.8094\n",
      "Training Epoch: 6 [1800/36450]\tLoss: 756.3882\n",
      "Training Epoch: 6 [1850/36450]\tLoss: 704.9230\n",
      "Training Epoch: 6 [1900/36450]\tLoss: 784.5350\n",
      "Training Epoch: 6 [1950/36450]\tLoss: 832.7586\n",
      "Training Epoch: 6 [2000/36450]\tLoss: 776.7599\n",
      "Training Epoch: 6 [2050/36450]\tLoss: 788.7070\n",
      "Training Epoch: 6 [2100/36450]\tLoss: 739.5659\n",
      "Training Epoch: 6 [2150/36450]\tLoss: 747.3394\n",
      "Training Epoch: 6 [2200/36450]\tLoss: 701.6732\n",
      "Training Epoch: 6 [2250/36450]\tLoss: 806.3364\n",
      "Training Epoch: 6 [2300/36450]\tLoss: 769.5894\n",
      "Training Epoch: 6 [2350/36450]\tLoss: 750.0088\n",
      "Training Epoch: 6 [2400/36450]\tLoss: 764.5374\n",
      "Training Epoch: 6 [2450/36450]\tLoss: 703.7689\n",
      "Training Epoch: 6 [2500/36450]\tLoss: 744.8381\n",
      "Training Epoch: 6 [2550/36450]\tLoss: 793.3126\n",
      "Training Epoch: 6 [2600/36450]\tLoss: 720.4907\n",
      "Training Epoch: 6 [2650/36450]\tLoss: 804.7656\n",
      "Training Epoch: 6 [2700/36450]\tLoss: 748.4911\n",
      "Training Epoch: 6 [2750/36450]\tLoss: 760.0164\n",
      "Training Epoch: 6 [2800/36450]\tLoss: 783.0829\n",
      "Training Epoch: 6 [2850/36450]\tLoss: 765.4095\n",
      "Training Epoch: 6 [2900/36450]\tLoss: 790.1212\n",
      "Training Epoch: 6 [2950/36450]\tLoss: 771.0990\n",
      "Training Epoch: 6 [3000/36450]\tLoss: 721.3387\n",
      "Training Epoch: 6 [3050/36450]\tLoss: 748.9673\n",
      "Training Epoch: 6 [3100/36450]\tLoss: 767.6682\n",
      "Training Epoch: 6 [3150/36450]\tLoss: 758.1995\n",
      "Training Epoch: 6 [3200/36450]\tLoss: 717.3171\n",
      "Training Epoch: 6 [3250/36450]\tLoss: 740.5649\n",
      "Training Epoch: 6 [3300/36450]\tLoss: 717.9214\n",
      "Training Epoch: 6 [3350/36450]\tLoss: 746.7361\n",
      "Training Epoch: 6 [3400/36450]\tLoss: 750.6089\n",
      "Training Epoch: 6 [3450/36450]\tLoss: 729.6002\n",
      "Training Epoch: 6 [3500/36450]\tLoss: 727.7726\n",
      "Training Epoch: 6 [3550/36450]\tLoss: 759.9421\n",
      "Training Epoch: 6 [3600/36450]\tLoss: 732.1778\n",
      "Training Epoch: 6 [3650/36450]\tLoss: 774.2558\n",
      "Training Epoch: 6 [3700/36450]\tLoss: 759.9444\n",
      "Training Epoch: 6 [3750/36450]\tLoss: 742.1451\n",
      "Training Epoch: 6 [3800/36450]\tLoss: 818.9660\n",
      "Training Epoch: 6 [3850/36450]\tLoss: 738.9501\n",
      "Training Epoch: 6 [3900/36450]\tLoss: 776.9852\n",
      "Training Epoch: 6 [3950/36450]\tLoss: 727.8600\n",
      "Training Epoch: 6 [4000/36450]\tLoss: 760.6476\n",
      "Training Epoch: 6 [4050/36450]\tLoss: 783.3637\n",
      "Training Epoch: 6 [4100/36450]\tLoss: 734.5607\n",
      "Training Epoch: 6 [4150/36450]\tLoss: 744.0237\n",
      "Training Epoch: 6 [4200/36450]\tLoss: 711.2860\n",
      "Training Epoch: 6 [4250/36450]\tLoss: 765.8040\n",
      "Training Epoch: 6 [4300/36450]\tLoss: 774.6862\n",
      "Training Epoch: 6 [4350/36450]\tLoss: 736.7305\n",
      "Training Epoch: 6 [4400/36450]\tLoss: 738.8093\n",
      "Training Epoch: 6 [4450/36450]\tLoss: 748.2451\n",
      "Training Epoch: 6 [4500/36450]\tLoss: 738.5388\n",
      "Training Epoch: 6 [4550/36450]\tLoss: 749.0571\n",
      "Training Epoch: 6 [4600/36450]\tLoss: 781.9371\n",
      "Training Epoch: 6 [4650/36450]\tLoss: 787.1763\n",
      "Training Epoch: 6 [4700/36450]\tLoss: 737.5859\n",
      "Training Epoch: 6 [4750/36450]\tLoss: 756.7806\n",
      "Training Epoch: 6 [4800/36450]\tLoss: 788.5433\n",
      "Training Epoch: 6 [4850/36450]\tLoss: 694.1918\n",
      "Training Epoch: 6 [4900/36450]\tLoss: 692.1180\n",
      "Training Epoch: 6 [4950/36450]\tLoss: 737.9250\n",
      "Training Epoch: 6 [5000/36450]\tLoss: 754.4117\n",
      "Training Epoch: 6 [5050/36450]\tLoss: 708.9462\n",
      "Training Epoch: 6 [5100/36450]\tLoss: 738.3228\n",
      "Training Epoch: 6 [5150/36450]\tLoss: 756.4688\n",
      "Training Epoch: 6 [5200/36450]\tLoss: 749.1150\n",
      "Training Epoch: 6 [5250/36450]\tLoss: 700.1309\n",
      "Training Epoch: 6 [5300/36450]\tLoss: 705.4427\n",
      "Training Epoch: 6 [5350/36450]\tLoss: 746.6612\n",
      "Training Epoch: 6 [5400/36450]\tLoss: 752.2403\n",
      "Training Epoch: 6 [5450/36450]\tLoss: 709.7011\n",
      "Training Epoch: 6 [5500/36450]\tLoss: 733.1653\n",
      "Training Epoch: 6 [5550/36450]\tLoss: 697.1456\n",
      "Training Epoch: 6 [5600/36450]\tLoss: 724.2083\n",
      "Training Epoch: 6 [5650/36450]\tLoss: 761.2485\n",
      "Training Epoch: 6 [5700/36450]\tLoss: 743.6390\n",
      "Training Epoch: 6 [5750/36450]\tLoss: 770.0001\n",
      "Training Epoch: 6 [5800/36450]\tLoss: 758.3153\n",
      "Training Epoch: 6 [5850/36450]\tLoss: 778.3920\n",
      "Training Epoch: 6 [5900/36450]\tLoss: 778.2202\n",
      "Training Epoch: 6 [5950/36450]\tLoss: 772.2579\n",
      "Training Epoch: 6 [6000/36450]\tLoss: 732.1682\n",
      "Training Epoch: 6 [6050/36450]\tLoss: 709.9693\n",
      "Training Epoch: 6 [6100/36450]\tLoss: 762.4502\n",
      "Training Epoch: 6 [6150/36450]\tLoss: 705.5838\n",
      "Training Epoch: 6 [6200/36450]\tLoss: 726.8895\n",
      "Training Epoch: 6 [6250/36450]\tLoss: 729.0552\n",
      "Training Epoch: 6 [6300/36450]\tLoss: 728.9070\n",
      "Training Epoch: 6 [6350/36450]\tLoss: 743.8903\n",
      "Training Epoch: 6 [6400/36450]\tLoss: 774.6038\n",
      "Training Epoch: 6 [6450/36450]\tLoss: 730.7987\n",
      "Training Epoch: 6 [6500/36450]\tLoss: 693.3090\n",
      "Training Epoch: 6 [6550/36450]\tLoss: 724.0582\n",
      "Training Epoch: 6 [6600/36450]\tLoss: 737.6457\n",
      "Training Epoch: 6 [6650/36450]\tLoss: 711.9298\n",
      "Training Epoch: 6 [6700/36450]\tLoss: 691.2820\n",
      "Training Epoch: 6 [6750/36450]\tLoss: 763.5319\n",
      "Training Epoch: 6 [6800/36450]\tLoss: 767.4286\n",
      "Training Epoch: 6 [6850/36450]\tLoss: 784.7510\n",
      "Training Epoch: 6 [6900/36450]\tLoss: 730.3630\n",
      "Training Epoch: 6 [6950/36450]\tLoss: 750.1705\n",
      "Training Epoch: 6 [7000/36450]\tLoss: 779.1430\n",
      "Training Epoch: 6 [7050/36450]\tLoss: 743.0980\n",
      "Training Epoch: 6 [7100/36450]\tLoss: 753.0110\n",
      "Training Epoch: 6 [7150/36450]\tLoss: 725.5359\n",
      "Training Epoch: 6 [7200/36450]\tLoss: 714.0400\n",
      "Training Epoch: 6 [7250/36450]\tLoss: 760.0579\n",
      "Training Epoch: 6 [7300/36450]\tLoss: 779.2120\n",
      "Training Epoch: 6 [7350/36450]\tLoss: 710.9927\n",
      "Training Epoch: 6 [7400/36450]\tLoss: 751.9785\n",
      "Training Epoch: 6 [7450/36450]\tLoss: 726.6298\n",
      "Training Epoch: 6 [7500/36450]\tLoss: 781.5463\n",
      "Training Epoch: 6 [7550/36450]\tLoss: 752.8044\n",
      "Training Epoch: 6 [7600/36450]\tLoss: 741.7019\n",
      "Training Epoch: 6 [7650/36450]\tLoss: 758.9634\n",
      "Training Epoch: 6 [7700/36450]\tLoss: 711.4435\n",
      "Training Epoch: 6 [7750/36450]\tLoss: 740.2601\n",
      "Training Epoch: 6 [7800/36450]\tLoss: 741.1790\n",
      "Training Epoch: 6 [7850/36450]\tLoss: 747.5446\n",
      "Training Epoch: 6 [7900/36450]\tLoss: 704.0018\n",
      "Training Epoch: 6 [7950/36450]\tLoss: 776.3911\n",
      "Training Epoch: 6 [8000/36450]\tLoss: 753.8209\n",
      "Training Epoch: 6 [8050/36450]\tLoss: 761.8189\n",
      "Training Epoch: 6 [8100/36450]\tLoss: 814.1840\n",
      "Training Epoch: 6 [8150/36450]\tLoss: 806.6806\n",
      "Training Epoch: 6 [8200/36450]\tLoss: 766.5300\n",
      "Training Epoch: 6 [8250/36450]\tLoss: 729.3729\n",
      "Training Epoch: 6 [8300/36450]\tLoss: 760.4140\n",
      "Training Epoch: 6 [8350/36450]\tLoss: 770.2564\n",
      "Training Epoch: 6 [8400/36450]\tLoss: 726.3857\n",
      "Training Epoch: 6 [8450/36450]\tLoss: 759.5293\n",
      "Training Epoch: 6 [8500/36450]\tLoss: 763.2314\n",
      "Training Epoch: 6 [8550/36450]\tLoss: 738.9330\n",
      "Training Epoch: 6 [8600/36450]\tLoss: 735.5478\n",
      "Training Epoch: 6 [8650/36450]\tLoss: 700.4748\n",
      "Training Epoch: 6 [8700/36450]\tLoss: 718.8477\n",
      "Training Epoch: 6 [8750/36450]\tLoss: 787.2457\n",
      "Training Epoch: 6 [8800/36450]\tLoss: 787.7652\n",
      "Training Epoch: 6 [8850/36450]\tLoss: 742.7913\n",
      "Training Epoch: 6 [8900/36450]\tLoss: 786.9142\n",
      "Training Epoch: 6 [8950/36450]\tLoss: 759.0507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [9000/36450]\tLoss: 743.6975\n",
      "Training Epoch: 6 [9050/36450]\tLoss: 723.8616\n",
      "Training Epoch: 6 [9100/36450]\tLoss: 701.6395\n",
      "Training Epoch: 6 [9150/36450]\tLoss: 766.6081\n",
      "Training Epoch: 6 [9200/36450]\tLoss: 769.1667\n",
      "Training Epoch: 6 [9250/36450]\tLoss: 750.0242\n",
      "Training Epoch: 6 [9300/36450]\tLoss: 728.6595\n",
      "Training Epoch: 6 [9350/36450]\tLoss: 734.1966\n",
      "Training Epoch: 6 [9400/36450]\tLoss: 762.9716\n",
      "Training Epoch: 6 [9450/36450]\tLoss: 724.2634\n",
      "Training Epoch: 6 [9500/36450]\tLoss: 725.2449\n",
      "Training Epoch: 6 [9550/36450]\tLoss: 736.8705\n",
      "Training Epoch: 6 [9600/36450]\tLoss: 740.4271\n",
      "Training Epoch: 6 [9650/36450]\tLoss: 715.1819\n",
      "Training Epoch: 6 [9700/36450]\tLoss: 742.0148\n",
      "Training Epoch: 6 [9750/36450]\tLoss: 757.1240\n",
      "Training Epoch: 6 [9800/36450]\tLoss: 736.4615\n",
      "Training Epoch: 6 [9850/36450]\tLoss: 768.2653\n",
      "Training Epoch: 6 [9900/36450]\tLoss: 781.0110\n",
      "Training Epoch: 6 [9950/36450]\tLoss: 775.1614\n",
      "Training Epoch: 6 [10000/36450]\tLoss: 769.5615\n",
      "Training Epoch: 6 [10050/36450]\tLoss: 752.3320\n",
      "Training Epoch: 6 [10100/36450]\tLoss: 731.5135\n",
      "Training Epoch: 6 [10150/36450]\tLoss: 745.4870\n",
      "Training Epoch: 6 [10200/36450]\tLoss: 676.1060\n",
      "Training Epoch: 6 [10250/36450]\tLoss: 758.9982\n",
      "Training Epoch: 6 [10300/36450]\tLoss: 725.4205\n",
      "Training Epoch: 6 [10350/36450]\tLoss: 693.3394\n",
      "Training Epoch: 6 [10400/36450]\tLoss: 748.2150\n",
      "Training Epoch: 6 [10450/36450]\tLoss: 767.9508\n",
      "Training Epoch: 6 [10500/36450]\tLoss: 741.7158\n",
      "Training Epoch: 6 [10550/36450]\tLoss: 743.4453\n",
      "Training Epoch: 6 [10600/36450]\tLoss: 723.4836\n",
      "Training Epoch: 6 [10650/36450]\tLoss: 733.4746\n",
      "Training Epoch: 6 [10700/36450]\tLoss: 740.8533\n",
      "Training Epoch: 6 [10750/36450]\tLoss: 729.2570\n",
      "Training Epoch: 6 [10800/36450]\tLoss: 704.4650\n",
      "Training Epoch: 6 [10850/36450]\tLoss: 770.7947\n",
      "Training Epoch: 6 [10900/36450]\tLoss: 702.1268\n",
      "Training Epoch: 6 [10950/36450]\tLoss: 703.2244\n",
      "Training Epoch: 6 [11000/36450]\tLoss: 722.9708\n",
      "Training Epoch: 6 [11050/36450]\tLoss: 690.7584\n",
      "Training Epoch: 6 [11100/36450]\tLoss: 712.3019\n",
      "Training Epoch: 6 [11150/36450]\tLoss: 789.5875\n",
      "Training Epoch: 6 [11200/36450]\tLoss: 713.8813\n",
      "Training Epoch: 6 [11250/36450]\tLoss: 725.4995\n",
      "Training Epoch: 6 [11300/36450]\tLoss: 741.8423\n",
      "Training Epoch: 6 [11350/36450]\tLoss: 787.6679\n",
      "Training Epoch: 6 [11400/36450]\tLoss: 757.8661\n",
      "Training Epoch: 6 [11450/36450]\tLoss: 748.6509\n",
      "Training Epoch: 6 [11500/36450]\tLoss: 725.7028\n",
      "Training Epoch: 6 [11550/36450]\tLoss: 727.3281\n",
      "Training Epoch: 6 [11600/36450]\tLoss: 710.0029\n",
      "Training Epoch: 6 [11650/36450]\tLoss: 730.0612\n",
      "Training Epoch: 6 [11700/36450]\tLoss: 718.0082\n",
      "Training Epoch: 6 [11750/36450]\tLoss: 716.7471\n",
      "Training Epoch: 6 [11800/36450]\tLoss: 732.5358\n",
      "Training Epoch: 6 [11850/36450]\tLoss: 730.6439\n",
      "Training Epoch: 6 [11900/36450]\tLoss: 774.7900\n",
      "Training Epoch: 6 [11950/36450]\tLoss: 754.9255\n",
      "Training Epoch: 6 [12000/36450]\tLoss: 720.3077\n",
      "Training Epoch: 6 [12050/36450]\tLoss: 690.1952\n",
      "Training Epoch: 6 [12100/36450]\tLoss: 724.1139\n",
      "Training Epoch: 6 [12150/36450]\tLoss: 689.6700\n",
      "Training Epoch: 6 [12200/36450]\tLoss: 739.7478\n",
      "Training Epoch: 6 [12250/36450]\tLoss: 734.3024\n",
      "Training Epoch: 6 [12300/36450]\tLoss: 709.7227\n",
      "Training Epoch: 6 [12350/36450]\tLoss: 709.2889\n",
      "Training Epoch: 6 [12400/36450]\tLoss: 767.6475\n",
      "Training Epoch: 6 [12450/36450]\tLoss: 716.7304\n",
      "Training Epoch: 6 [12500/36450]\tLoss: 702.5515\n",
      "Training Epoch: 6 [12550/36450]\tLoss: 713.7096\n",
      "Training Epoch: 6 [12600/36450]\tLoss: 744.2538\n",
      "Training Epoch: 6 [12650/36450]\tLoss: 717.8647\n",
      "Training Epoch: 6 [12700/36450]\tLoss: 737.4708\n",
      "Training Epoch: 6 [12750/36450]\tLoss: 762.3952\n",
      "Training Epoch: 6 [12800/36450]\tLoss: 757.0944\n",
      "Training Epoch: 6 [12850/36450]\tLoss: 756.0214\n",
      "Training Epoch: 6 [12900/36450]\tLoss: 733.1693\n",
      "Training Epoch: 6 [12950/36450]\tLoss: 736.0817\n",
      "Training Epoch: 6 [13000/36450]\tLoss: 764.8887\n",
      "Training Epoch: 6 [13050/36450]\tLoss: 760.7499\n",
      "Training Epoch: 6 [13100/36450]\tLoss: 720.9987\n",
      "Training Epoch: 6 [13150/36450]\tLoss: 732.5018\n",
      "Training Epoch: 6 [13200/36450]\tLoss: 756.3499\n",
      "Training Epoch: 6 [13250/36450]\tLoss: 763.3109\n",
      "Training Epoch: 6 [13300/36450]\tLoss: 701.0723\n",
      "Training Epoch: 6 [13350/36450]\tLoss: 757.0131\n",
      "Training Epoch: 6 [13400/36450]\tLoss: 731.2281\n",
      "Training Epoch: 6 [13450/36450]\tLoss: 716.1086\n",
      "Training Epoch: 6 [13500/36450]\tLoss: 782.7206\n",
      "Training Epoch: 6 [13550/36450]\tLoss: 765.8799\n",
      "Training Epoch: 6 [13600/36450]\tLoss: 735.6016\n",
      "Training Epoch: 6 [13650/36450]\tLoss: 747.2731\n",
      "Training Epoch: 6 [13700/36450]\tLoss: 725.0026\n",
      "Training Epoch: 6 [13750/36450]\tLoss: 720.2369\n",
      "Training Epoch: 6 [13800/36450]\tLoss: 689.2048\n",
      "Training Epoch: 6 [13850/36450]\tLoss: 716.6417\n",
      "Training Epoch: 6 [13900/36450]\tLoss: 738.6926\n",
      "Training Epoch: 6 [13950/36450]\tLoss: 755.6924\n",
      "Training Epoch: 6 [14000/36450]\tLoss: 731.9271\n",
      "Training Epoch: 6 [14050/36450]\tLoss: 692.8868\n",
      "Training Epoch: 6 [14100/36450]\tLoss: 731.0260\n",
      "Training Epoch: 6 [14150/36450]\tLoss: 718.0929\n",
      "Training Epoch: 6 [14200/36450]\tLoss: 765.6944\n",
      "Training Epoch: 6 [14250/36450]\tLoss: 715.7245\n",
      "Training Epoch: 6 [14300/36450]\tLoss: 766.7972\n",
      "Training Epoch: 6 [14350/36450]\tLoss: 743.9073\n",
      "Training Epoch: 6 [14400/36450]\tLoss: 718.2838\n",
      "Training Epoch: 6 [14450/36450]\tLoss: 749.7585\n",
      "Training Epoch: 6 [14500/36450]\tLoss: 748.0088\n",
      "Training Epoch: 6 [14550/36450]\tLoss: 729.3045\n",
      "Training Epoch: 6 [14600/36450]\tLoss: 727.9715\n",
      "Training Epoch: 6 [14650/36450]\tLoss: 744.0931\n",
      "Training Epoch: 6 [14700/36450]\tLoss: 771.0497\n",
      "Training Epoch: 6 [14750/36450]\tLoss: 750.3641\n",
      "Training Epoch: 6 [14800/36450]\tLoss: 745.9497\n",
      "Training Epoch: 6 [14850/36450]\tLoss: 740.2829\n",
      "Training Epoch: 6 [14900/36450]\tLoss: 714.3129\n",
      "Training Epoch: 6 [14950/36450]\tLoss: 766.5754\n",
      "Training Epoch: 6 [15000/36450]\tLoss: 783.6584\n",
      "Training Epoch: 6 [15050/36450]\tLoss: 788.4233\n",
      "Training Epoch: 6 [15100/36450]\tLoss: 728.7352\n",
      "Training Epoch: 6 [15150/36450]\tLoss: 745.7654\n",
      "Training Epoch: 6 [15200/36450]\tLoss: 744.7588\n",
      "Training Epoch: 6 [15250/36450]\tLoss: 786.6836\n",
      "Training Epoch: 6 [15300/36450]\tLoss: 725.7429\n",
      "Training Epoch: 6 [15350/36450]\tLoss: 726.3319\n",
      "Training Epoch: 6 [15400/36450]\tLoss: 803.2381\n",
      "Training Epoch: 6 [15450/36450]\tLoss: 727.8678\n",
      "Training Epoch: 6 [15500/36450]\tLoss: 765.1572\n",
      "Training Epoch: 6 [15550/36450]\tLoss: 776.3550\n",
      "Training Epoch: 6 [15600/36450]\tLoss: 776.8033\n",
      "Training Epoch: 6 [15650/36450]\tLoss: 736.9627\n",
      "Training Epoch: 6 [15700/36450]\tLoss: 726.3748\n",
      "Training Epoch: 6 [15750/36450]\tLoss: 750.0005\n",
      "Training Epoch: 6 [15800/36450]\tLoss: 768.4427\n",
      "Training Epoch: 6 [15850/36450]\tLoss: 756.0383\n",
      "Training Epoch: 6 [15900/36450]\tLoss: 739.4366\n",
      "Training Epoch: 6 [15950/36450]\tLoss: 716.8092\n",
      "Training Epoch: 6 [16000/36450]\tLoss: 795.5588\n",
      "Training Epoch: 6 [16050/36450]\tLoss: 703.2241\n",
      "Training Epoch: 6 [16100/36450]\tLoss: 728.8336\n",
      "Training Epoch: 6 [16150/36450]\tLoss: 746.9509\n",
      "Training Epoch: 6 [16200/36450]\tLoss: 739.2635\n",
      "Training Epoch: 6 [16250/36450]\tLoss: 734.0327\n",
      "Training Epoch: 6 [16300/36450]\tLoss: 714.9603\n",
      "Training Epoch: 6 [16350/36450]\tLoss: 718.2328\n",
      "Training Epoch: 6 [16400/36450]\tLoss: 742.5069\n",
      "Training Epoch: 6 [16450/36450]\tLoss: 745.5286\n",
      "Training Epoch: 6 [16500/36450]\tLoss: 762.8149\n",
      "Training Epoch: 6 [16550/36450]\tLoss: 729.8273\n",
      "Training Epoch: 6 [16600/36450]\tLoss: 725.5203\n",
      "Training Epoch: 6 [16650/36450]\tLoss: 745.1146\n",
      "Training Epoch: 6 [16700/36450]\tLoss: 825.8422\n",
      "Training Epoch: 6 [16750/36450]\tLoss: 763.3220\n",
      "Training Epoch: 6 [16800/36450]\tLoss: 741.9689\n",
      "Training Epoch: 6 [16850/36450]\tLoss: 746.4383\n",
      "Training Epoch: 6 [16900/36450]\tLoss: 740.3583\n",
      "Training Epoch: 6 [16950/36450]\tLoss: 736.4525\n",
      "Training Epoch: 6 [17000/36450]\tLoss: 693.1824\n",
      "Training Epoch: 6 [17050/36450]\tLoss: 718.4113\n",
      "Training Epoch: 6 [17100/36450]\tLoss: 741.1042\n",
      "Training Epoch: 6 [17150/36450]\tLoss: 698.7650\n",
      "Training Epoch: 6 [17200/36450]\tLoss: 742.6440\n",
      "Training Epoch: 6 [17250/36450]\tLoss: 755.7642\n",
      "Training Epoch: 6 [17300/36450]\tLoss: 733.9078\n",
      "Training Epoch: 6 [17350/36450]\tLoss: 731.5000\n",
      "Training Epoch: 6 [17400/36450]\tLoss: 717.2498\n",
      "Training Epoch: 6 [17450/36450]\tLoss: 712.1949\n",
      "Training Epoch: 6 [17500/36450]\tLoss: 707.7709\n",
      "Training Epoch: 6 [17550/36450]\tLoss: 697.1364\n",
      "Training Epoch: 6 [17600/36450]\tLoss: 726.1492\n",
      "Training Epoch: 6 [17650/36450]\tLoss: 746.8577\n",
      "Training Epoch: 6 [17700/36450]\tLoss: 690.6920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [17750/36450]\tLoss: 731.1053\n",
      "Training Epoch: 6 [17800/36450]\tLoss: 781.8077\n",
      "Training Epoch: 6 [17850/36450]\tLoss: 715.3229\n",
      "Training Epoch: 6 [17900/36450]\tLoss: 712.2123\n",
      "Training Epoch: 6 [17950/36450]\tLoss: 713.6407\n",
      "Training Epoch: 6 [18000/36450]\tLoss: 712.2296\n",
      "Training Epoch: 6 [18050/36450]\tLoss: 738.3885\n",
      "Training Epoch: 6 [18100/36450]\tLoss: 734.7495\n",
      "Training Epoch: 6 [18150/36450]\tLoss: 737.3384\n",
      "Training Epoch: 6 [18200/36450]\tLoss: 793.1715\n",
      "Training Epoch: 6 [18250/36450]\tLoss: 702.5928\n",
      "Training Epoch: 6 [18300/36450]\tLoss: 747.0024\n",
      "Training Epoch: 6 [18350/36450]\tLoss: 718.8438\n",
      "Training Epoch: 6 [18400/36450]\tLoss: 681.2515\n",
      "Training Epoch: 6 [18450/36450]\tLoss: 724.7960\n",
      "Training Epoch: 6 [18500/36450]\tLoss: 715.1910\n",
      "Training Epoch: 6 [18550/36450]\tLoss: 729.3116\n",
      "Training Epoch: 6 [18600/36450]\tLoss: 734.6887\n",
      "Training Epoch: 6 [18650/36450]\tLoss: 702.1795\n",
      "Training Epoch: 6 [18700/36450]\tLoss: 745.7673\n",
      "Training Epoch: 6 [18750/36450]\tLoss: 740.8658\n",
      "Training Epoch: 6 [18800/36450]\tLoss: 702.2314\n",
      "Training Epoch: 6 [18850/36450]\tLoss: 688.7466\n",
      "Training Epoch: 6 [18900/36450]\tLoss: 731.5712\n",
      "Training Epoch: 6 [18950/36450]\tLoss: 728.9404\n",
      "Training Epoch: 6 [19000/36450]\tLoss: 709.6705\n",
      "Training Epoch: 6 [19050/36450]\tLoss: 697.1570\n",
      "Training Epoch: 6 [19100/36450]\tLoss: 692.4003\n",
      "Training Epoch: 6 [19150/36450]\tLoss: 708.0853\n",
      "Training Epoch: 6 [19200/36450]\tLoss: 680.3015\n",
      "Training Epoch: 6 [19250/36450]\tLoss: 731.8619\n",
      "Training Epoch: 6 [19300/36450]\tLoss: 734.1738\n",
      "Training Epoch: 6 [19350/36450]\tLoss: 715.8555\n",
      "Training Epoch: 6 [19400/36450]\tLoss: 711.9905\n",
      "Training Epoch: 6 [19450/36450]\tLoss: 722.7269\n",
      "Training Epoch: 6 [19500/36450]\tLoss: 670.6921\n",
      "Training Epoch: 6 [19550/36450]\tLoss: 719.8876\n",
      "Training Epoch: 6 [19600/36450]\tLoss: 727.6205\n",
      "Training Epoch: 6 [19650/36450]\tLoss: 691.4847\n",
      "Training Epoch: 6 [19700/36450]\tLoss: 740.7158\n",
      "Training Epoch: 6 [19750/36450]\tLoss: 727.3037\n",
      "Training Epoch: 6 [19800/36450]\tLoss: 743.0835\n",
      "Training Epoch: 6 [19850/36450]\tLoss: 716.8323\n",
      "Training Epoch: 6 [19900/36450]\tLoss: 707.6798\n",
      "Training Epoch: 6 [19950/36450]\tLoss: 656.7433\n",
      "Training Epoch: 6 [20000/36450]\tLoss: 697.0551\n",
      "Training Epoch: 6 [20050/36450]\tLoss: 761.7103\n",
      "Training Epoch: 6 [20100/36450]\tLoss: 706.0199\n",
      "Training Epoch: 6 [20150/36450]\tLoss: 715.3496\n",
      "Training Epoch: 6 [20200/36450]\tLoss: 697.1073\n",
      "Training Epoch: 6 [20250/36450]\tLoss: 694.2304\n",
      "Training Epoch: 6 [20300/36450]\tLoss: 793.7897\n",
      "Training Epoch: 6 [20350/36450]\tLoss: 703.8845\n",
      "Training Epoch: 6 [20400/36450]\tLoss: 701.0623\n",
      "Training Epoch: 6 [20450/36450]\tLoss: 721.1066\n",
      "Training Epoch: 6 [20500/36450]\tLoss: 731.4778\n",
      "Training Epoch: 6 [20550/36450]\tLoss: 669.8914\n",
      "Training Epoch: 6 [20600/36450]\tLoss: 722.0186\n",
      "Training Epoch: 6 [20650/36450]\tLoss: 715.2714\n",
      "Training Epoch: 6 [20700/36450]\tLoss: 728.8688\n",
      "Training Epoch: 6 [20750/36450]\tLoss: 736.6755\n",
      "Training Epoch: 6 [20800/36450]\tLoss: 663.7018\n",
      "Training Epoch: 6 [20850/36450]\tLoss: 696.0973\n",
      "Training Epoch: 6 [20900/36450]\tLoss: 685.4992\n",
      "Training Epoch: 6 [20950/36450]\tLoss: 755.8132\n",
      "Training Epoch: 6 [21000/36450]\tLoss: 687.4928\n",
      "Training Epoch: 6 [21050/36450]\tLoss: 679.8845\n",
      "Training Epoch: 6 [21100/36450]\tLoss: 770.0369\n",
      "Training Epoch: 6 [21150/36450]\tLoss: 760.1945\n",
      "Training Epoch: 6 [21200/36450]\tLoss: 733.8804\n",
      "Training Epoch: 6 [21250/36450]\tLoss: 725.6811\n",
      "Training Epoch: 6 [21300/36450]\tLoss: 703.2082\n",
      "Training Epoch: 6 [21350/36450]\tLoss: 686.7975\n",
      "Training Epoch: 6 [21400/36450]\tLoss: 691.5901\n",
      "Training Epoch: 6 [21450/36450]\tLoss: 689.7621\n",
      "Training Epoch: 6 [21500/36450]\tLoss: 748.1490\n",
      "Training Epoch: 6 [21550/36450]\tLoss: 687.9214\n",
      "Training Epoch: 6 [21600/36450]\tLoss: 760.8147\n",
      "Training Epoch: 6 [21650/36450]\tLoss: 715.7700\n",
      "Training Epoch: 6 [21700/36450]\tLoss: 719.3802\n",
      "Training Epoch: 6 [21750/36450]\tLoss: 723.6852\n",
      "Training Epoch: 6 [21800/36450]\tLoss: 691.2203\n",
      "Training Epoch: 6 [21850/36450]\tLoss: 712.1465\n",
      "Training Epoch: 6 [21900/36450]\tLoss: 727.7204\n",
      "Training Epoch: 6 [21950/36450]\tLoss: 710.8535\n",
      "Training Epoch: 6 [22000/36450]\tLoss: 690.2214\n",
      "Training Epoch: 6 [22050/36450]\tLoss: 717.7618\n",
      "Training Epoch: 6 [22100/36450]\tLoss: 741.6667\n",
      "Training Epoch: 6 [22150/36450]\tLoss: 715.5018\n",
      "Training Epoch: 6 [22200/36450]\tLoss: 729.1956\n",
      "Training Epoch: 6 [22250/36450]\tLoss: 703.9990\n",
      "Training Epoch: 6 [22300/36450]\tLoss: 707.6511\n",
      "Training Epoch: 6 [22350/36450]\tLoss: 745.8362\n",
      "Training Epoch: 6 [22400/36450]\tLoss: 736.7482\n",
      "Training Epoch: 6 [22450/36450]\tLoss: 713.2379\n",
      "Training Epoch: 6 [22500/36450]\tLoss: 698.1496\n",
      "Training Epoch: 6 [22550/36450]\tLoss: 648.8418\n",
      "Training Epoch: 6 [22600/36450]\tLoss: 703.3658\n",
      "Training Epoch: 6 [22650/36450]\tLoss: 755.8797\n",
      "Training Epoch: 6 [22700/36450]\tLoss: 746.5495\n",
      "Training Epoch: 6 [22750/36450]\tLoss: 717.9926\n",
      "Training Epoch: 6 [22800/36450]\tLoss: 728.7494\n",
      "Training Epoch: 6 [22850/36450]\tLoss: 767.1223\n",
      "Training Epoch: 6 [22900/36450]\tLoss: 735.7473\n",
      "Training Epoch: 6 [22950/36450]\tLoss: 716.8892\n",
      "Training Epoch: 6 [23000/36450]\tLoss: 696.9337\n",
      "Training Epoch: 6 [23050/36450]\tLoss: 749.3964\n",
      "Training Epoch: 6 [23100/36450]\tLoss: 742.0607\n",
      "Training Epoch: 6 [23150/36450]\tLoss: 722.1891\n",
      "Training Epoch: 6 [23200/36450]\tLoss: 727.2690\n",
      "Training Epoch: 6 [23250/36450]\tLoss: 720.0347\n",
      "Training Epoch: 6 [23300/36450]\tLoss: 704.0777\n",
      "Training Epoch: 6 [23350/36450]\tLoss: 746.7701\n",
      "Training Epoch: 6 [23400/36450]\tLoss: 738.1317\n",
      "Training Epoch: 6 [23450/36450]\tLoss: 733.5554\n",
      "Training Epoch: 6 [23500/36450]\tLoss: 755.9081\n",
      "Training Epoch: 6 [23550/36450]\tLoss: 709.4067\n",
      "Training Epoch: 6 [23600/36450]\tLoss: 718.8610\n",
      "Training Epoch: 6 [23650/36450]\tLoss: 702.3633\n",
      "Training Epoch: 6 [23700/36450]\tLoss: 718.2258\n",
      "Training Epoch: 6 [23750/36450]\tLoss: 725.9418\n",
      "Training Epoch: 6 [23800/36450]\tLoss: 745.1687\n",
      "Training Epoch: 6 [23850/36450]\tLoss: 717.7123\n",
      "Training Epoch: 6 [23900/36450]\tLoss: 713.9569\n",
      "Training Epoch: 6 [23950/36450]\tLoss: 691.7975\n",
      "Training Epoch: 6 [24000/36450]\tLoss: 673.2404\n",
      "Training Epoch: 6 [24050/36450]\tLoss: 745.8690\n",
      "Training Epoch: 6 [24100/36450]\tLoss: 707.0029\n",
      "Training Epoch: 6 [24150/36450]\tLoss: 694.4926\n",
      "Training Epoch: 6 [24200/36450]\tLoss: 687.9528\n",
      "Training Epoch: 6 [24250/36450]\tLoss: 678.6795\n",
      "Training Epoch: 6 [24300/36450]\tLoss: 704.1036\n",
      "Training Epoch: 6 [24350/36450]\tLoss: 681.0335\n",
      "Training Epoch: 6 [24400/36450]\tLoss: 732.5466\n",
      "Training Epoch: 6 [24450/36450]\tLoss: 754.0068\n",
      "Training Epoch: 6 [24500/36450]\tLoss: 776.3731\n",
      "Training Epoch: 6 [24550/36450]\tLoss: 698.7694\n",
      "Training Epoch: 6 [24600/36450]\tLoss: 672.2869\n",
      "Training Epoch: 6 [24650/36450]\tLoss: 745.8797\n",
      "Training Epoch: 6 [24700/36450]\tLoss: 720.6587\n",
      "Training Epoch: 6 [24750/36450]\tLoss: 740.6353\n",
      "Training Epoch: 6 [24800/36450]\tLoss: 686.7827\n",
      "Training Epoch: 6 [24850/36450]\tLoss: 746.9880\n",
      "Training Epoch: 6 [24900/36450]\tLoss: 740.8851\n",
      "Training Epoch: 6 [24950/36450]\tLoss: 744.7441\n",
      "Training Epoch: 6 [25000/36450]\tLoss: 691.6069\n",
      "Training Epoch: 6 [25050/36450]\tLoss: 706.6346\n",
      "Training Epoch: 6 [25100/36450]\tLoss: 762.4631\n",
      "Training Epoch: 6 [25150/36450]\tLoss: 707.9603\n",
      "Training Epoch: 6 [25200/36450]\tLoss: 705.3229\n",
      "Training Epoch: 6 [25250/36450]\tLoss: 671.6627\n",
      "Training Epoch: 6 [25300/36450]\tLoss: 771.5158\n",
      "Training Epoch: 6 [25350/36450]\tLoss: 723.9638\n",
      "Training Epoch: 6 [25400/36450]\tLoss: 754.9416\n",
      "Training Epoch: 6 [25450/36450]\tLoss: 706.7820\n",
      "Training Epoch: 6 [25500/36450]\tLoss: 671.2709\n",
      "Training Epoch: 6 [25550/36450]\tLoss: 743.4742\n",
      "Training Epoch: 6 [25600/36450]\tLoss: 731.9498\n",
      "Training Epoch: 6 [25650/36450]\tLoss: 729.5793\n",
      "Training Epoch: 6 [25700/36450]\tLoss: 700.0443\n",
      "Training Epoch: 6 [25750/36450]\tLoss: 696.1736\n",
      "Training Epoch: 6 [25800/36450]\tLoss: 731.8336\n",
      "Training Epoch: 6 [25850/36450]\tLoss: 725.8021\n",
      "Training Epoch: 6 [25900/36450]\tLoss: 680.7805\n",
      "Training Epoch: 6 [25950/36450]\tLoss: 721.3297\n",
      "Training Epoch: 6 [26000/36450]\tLoss: 708.7586\n",
      "Training Epoch: 6 [26050/36450]\tLoss: 682.2582\n",
      "Training Epoch: 6 [26100/36450]\tLoss: 725.3303\n",
      "Training Epoch: 6 [26150/36450]\tLoss: 635.0624\n",
      "Training Epoch: 6 [26200/36450]\tLoss: 769.1077\n",
      "Training Epoch: 6 [26250/36450]\tLoss: 715.1033\n",
      "Training Epoch: 6 [26300/36450]\tLoss: 740.2643\n",
      "Training Epoch: 6 [26350/36450]\tLoss: 750.6871\n",
      "Training Epoch: 6 [26400/36450]\tLoss: 683.5732\n",
      "Training Epoch: 6 [26450/36450]\tLoss: 694.7042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [26500/36450]\tLoss: 694.0262\n",
      "Training Epoch: 6 [26550/36450]\tLoss: 734.0175\n",
      "Training Epoch: 6 [26600/36450]\tLoss: 677.6873\n",
      "Training Epoch: 6 [26650/36450]\tLoss: 675.4062\n",
      "Training Epoch: 6 [26700/36450]\tLoss: 689.1013\n",
      "Training Epoch: 6 [26750/36450]\tLoss: 707.5534\n",
      "Training Epoch: 6 [26800/36450]\tLoss: 671.9949\n",
      "Training Epoch: 6 [26850/36450]\tLoss: 682.3271\n",
      "Training Epoch: 6 [26900/36450]\tLoss: 747.0909\n",
      "Training Epoch: 6 [26950/36450]\tLoss: 697.7865\n",
      "Training Epoch: 6 [27000/36450]\tLoss: 770.9818\n",
      "Training Epoch: 6 [27050/36450]\tLoss: 732.2065\n",
      "Training Epoch: 6 [27100/36450]\tLoss: 743.7681\n",
      "Training Epoch: 6 [27150/36450]\tLoss: 713.4536\n",
      "Training Epoch: 6 [27200/36450]\tLoss: 745.5297\n",
      "Training Epoch: 6 [27250/36450]\tLoss: 689.4144\n",
      "Training Epoch: 6 [27300/36450]\tLoss: 715.2271\n",
      "Training Epoch: 6 [27350/36450]\tLoss: 699.5521\n",
      "Training Epoch: 6 [27400/36450]\tLoss: 693.2511\n",
      "Training Epoch: 6 [27450/36450]\tLoss: 701.1791\n",
      "Training Epoch: 6 [27500/36450]\tLoss: 742.6522\n",
      "Training Epoch: 6 [27550/36450]\tLoss: 722.5538\n",
      "Training Epoch: 6 [27600/36450]\tLoss: 713.8902\n",
      "Training Epoch: 6 [27650/36450]\tLoss: 742.6091\n",
      "Training Epoch: 6 [27700/36450]\tLoss: 737.6459\n",
      "Training Epoch: 6 [27750/36450]\tLoss: 717.4368\n",
      "Training Epoch: 6 [27800/36450]\tLoss: 724.1920\n",
      "Training Epoch: 6 [27850/36450]\tLoss: 720.9955\n",
      "Training Epoch: 6 [27900/36450]\tLoss: 727.2092\n",
      "Training Epoch: 6 [27950/36450]\tLoss: 739.8331\n",
      "Training Epoch: 6 [28000/36450]\tLoss: 693.1013\n",
      "Training Epoch: 6 [28050/36450]\tLoss: 662.6602\n",
      "Training Epoch: 6 [28100/36450]\tLoss: 710.1538\n",
      "Training Epoch: 6 [28150/36450]\tLoss: 703.8451\n",
      "Training Epoch: 6 [28200/36450]\tLoss: 703.5933\n",
      "Training Epoch: 6 [28250/36450]\tLoss: 752.6684\n",
      "Training Epoch: 6 [28300/36450]\tLoss: 708.1522\n",
      "Training Epoch: 6 [28350/36450]\tLoss: 737.0756\n",
      "Training Epoch: 6 [28400/36450]\tLoss: 779.3826\n",
      "Training Epoch: 6 [28450/36450]\tLoss: 751.8928\n",
      "Training Epoch: 6 [28500/36450]\tLoss: 722.3863\n",
      "Training Epoch: 6 [28550/36450]\tLoss: 718.7291\n",
      "Training Epoch: 6 [28600/36450]\tLoss: 721.4098\n",
      "Training Epoch: 6 [28650/36450]\tLoss: 738.5806\n",
      "Training Epoch: 6 [28700/36450]\tLoss: 715.0478\n",
      "Training Epoch: 6 [28750/36450]\tLoss: 691.0947\n",
      "Training Epoch: 6 [28800/36450]\tLoss: 745.3351\n",
      "Training Epoch: 6 [28850/36450]\tLoss: 690.0004\n",
      "Training Epoch: 6 [28900/36450]\tLoss: 736.2323\n",
      "Training Epoch: 6 [28950/36450]\tLoss: 685.8823\n",
      "Training Epoch: 6 [29000/36450]\tLoss: 741.8955\n",
      "Training Epoch: 6 [29050/36450]\tLoss: 692.8186\n",
      "Training Epoch: 6 [29100/36450]\tLoss: 692.1074\n",
      "Training Epoch: 6 [29150/36450]\tLoss: 716.8123\n",
      "Training Epoch: 6 [29200/36450]\tLoss: 731.0095\n",
      "Training Epoch: 6 [29250/36450]\tLoss: 720.0726\n",
      "Training Epoch: 6 [29300/36450]\tLoss: 734.8333\n",
      "Training Epoch: 6 [29350/36450]\tLoss: 693.7142\n",
      "Training Epoch: 6 [29400/36450]\tLoss: 684.7743\n",
      "Training Epoch: 6 [29450/36450]\tLoss: 691.7885\n",
      "Training Epoch: 6 [29500/36450]\tLoss: 698.8509\n",
      "Training Epoch: 6 [29550/36450]\tLoss: 710.0856\n",
      "Training Epoch: 6 [29600/36450]\tLoss: 767.3401\n",
      "Training Epoch: 6 [29650/36450]\tLoss: 717.0067\n",
      "Training Epoch: 6 [29700/36450]\tLoss: 709.0972\n",
      "Training Epoch: 6 [29750/36450]\tLoss: 701.2702\n",
      "Training Epoch: 6 [29800/36450]\tLoss: 724.5062\n",
      "Training Epoch: 6 [29850/36450]\tLoss: 765.2430\n",
      "Training Epoch: 6 [29900/36450]\tLoss: 690.1002\n",
      "Training Epoch: 6 [29950/36450]\tLoss: 725.2336\n",
      "Training Epoch: 6 [30000/36450]\tLoss: 736.1240\n",
      "Training Epoch: 6 [30050/36450]\tLoss: 738.2343\n",
      "Training Epoch: 6 [30100/36450]\tLoss: 741.9109\n",
      "Training Epoch: 6 [30150/36450]\tLoss: 749.5131\n",
      "Training Epoch: 6 [30200/36450]\tLoss: 648.8004\n",
      "Training Epoch: 6 [30250/36450]\tLoss: 742.8582\n",
      "Training Epoch: 6 [30300/36450]\tLoss: 694.2358\n",
      "Training Epoch: 6 [30350/36450]\tLoss: 767.0043\n",
      "Training Epoch: 6 [30400/36450]\tLoss: 680.4736\n",
      "Training Epoch: 6 [30450/36450]\tLoss: 723.2773\n",
      "Training Epoch: 6 [30500/36450]\tLoss: 709.9417\n",
      "Training Epoch: 6 [30550/36450]\tLoss: 723.9318\n",
      "Training Epoch: 6 [30600/36450]\tLoss: 724.8337\n",
      "Training Epoch: 6 [30650/36450]\tLoss: 675.6346\n",
      "Training Epoch: 6 [30700/36450]\tLoss: 756.8583\n",
      "Training Epoch: 6 [30750/36450]\tLoss: 726.3320\n",
      "Training Epoch: 6 [30800/36450]\tLoss: 738.0487\n",
      "Training Epoch: 6 [30850/36450]\tLoss: 750.5123\n",
      "Training Epoch: 6 [30900/36450]\tLoss: 707.8188\n",
      "Training Epoch: 6 [30950/36450]\tLoss: 707.9182\n",
      "Training Epoch: 6 [31000/36450]\tLoss: 756.1531\n",
      "Training Epoch: 6 [31050/36450]\tLoss: 758.1865\n",
      "Training Epoch: 6 [31100/36450]\tLoss: 737.8312\n",
      "Training Epoch: 6 [31150/36450]\tLoss: 699.1401\n",
      "Training Epoch: 6 [31200/36450]\tLoss: 732.3438\n",
      "Training Epoch: 6 [31250/36450]\tLoss: 759.3257\n",
      "Training Epoch: 6 [31300/36450]\tLoss: 698.0857\n",
      "Training Epoch: 6 [31350/36450]\tLoss: 748.0388\n",
      "Training Epoch: 6 [31400/36450]\tLoss: 715.9200\n",
      "Training Epoch: 6 [31450/36450]\tLoss: 737.5508\n",
      "Training Epoch: 6 [31500/36450]\tLoss: 715.0560\n",
      "Training Epoch: 6 [31550/36450]\tLoss: 706.4413\n",
      "Training Epoch: 6 [31600/36450]\tLoss: 650.7554\n",
      "Training Epoch: 6 [31650/36450]\tLoss: 676.1815\n",
      "Training Epoch: 6 [31700/36450]\tLoss: 680.3474\n",
      "Training Epoch: 6 [31750/36450]\tLoss: 699.4103\n",
      "Training Epoch: 6 [31800/36450]\tLoss: 693.8692\n",
      "Training Epoch: 6 [31850/36450]\tLoss: 707.6522\n",
      "Training Epoch: 6 [31900/36450]\tLoss: 694.2737\n",
      "Training Epoch: 6 [31950/36450]\tLoss: 700.7586\n",
      "Training Epoch: 6 [32000/36450]\tLoss: 651.0450\n",
      "Training Epoch: 6 [32050/36450]\tLoss: 659.4204\n",
      "Training Epoch: 6 [32100/36450]\tLoss: 683.4249\n",
      "Training Epoch: 6 [32150/36450]\tLoss: 687.8743\n",
      "Training Epoch: 6 [32200/36450]\tLoss: 748.8000\n",
      "Training Epoch: 6 [32250/36450]\tLoss: 738.1716\n",
      "Training Epoch: 6 [32300/36450]\tLoss: 743.1485\n",
      "Training Epoch: 6 [32350/36450]\tLoss: 713.5575\n",
      "Training Epoch: 6 [32400/36450]\tLoss: 677.8381\n",
      "Training Epoch: 6 [32450/36450]\tLoss: 707.6774\n",
      "Training Epoch: 6 [32500/36450]\tLoss: 709.2311\n",
      "Training Epoch: 6 [32550/36450]\tLoss: 682.7965\n",
      "Training Epoch: 6 [32600/36450]\tLoss: 751.6678\n",
      "Training Epoch: 6 [32650/36450]\tLoss: 698.9829\n",
      "Training Epoch: 6 [32700/36450]\tLoss: 725.5927\n",
      "Training Epoch: 6 [32750/36450]\tLoss: 713.5779\n",
      "Training Epoch: 6 [32800/36450]\tLoss: 700.8274\n",
      "Training Epoch: 6 [32850/36450]\tLoss: 697.1245\n",
      "Training Epoch: 6 [32900/36450]\tLoss: 723.2715\n",
      "Training Epoch: 6 [32950/36450]\tLoss: 705.7042\n",
      "Training Epoch: 6 [33000/36450]\tLoss: 697.5649\n",
      "Training Epoch: 6 [33050/36450]\tLoss: 670.0308\n",
      "Training Epoch: 6 [33100/36450]\tLoss: 657.1339\n",
      "Training Epoch: 6 [33150/36450]\tLoss: 712.7333\n",
      "Training Epoch: 6 [33200/36450]\tLoss: 696.5914\n",
      "Training Epoch: 6 [33250/36450]\tLoss: 720.0844\n",
      "Training Epoch: 6 [33300/36450]\tLoss: 735.5440\n",
      "Training Epoch: 6 [33350/36450]\tLoss: 642.9198\n",
      "Training Epoch: 6 [33400/36450]\tLoss: 674.8265\n",
      "Training Epoch: 6 [33450/36450]\tLoss: 695.2097\n",
      "Training Epoch: 6 [33500/36450]\tLoss: 698.3392\n",
      "Training Epoch: 6 [33550/36450]\tLoss: 651.1735\n",
      "Training Epoch: 6 [33600/36450]\tLoss: 711.1901\n",
      "Training Epoch: 6 [33650/36450]\tLoss: 712.4068\n",
      "Training Epoch: 6 [33700/36450]\tLoss: 692.9604\n",
      "Training Epoch: 6 [33750/36450]\tLoss: 693.0840\n",
      "Training Epoch: 6 [33800/36450]\tLoss: 671.7169\n",
      "Training Epoch: 6 [33850/36450]\tLoss: 658.7575\n",
      "Training Epoch: 6 [33900/36450]\tLoss: 727.5078\n",
      "Training Epoch: 6 [33950/36450]\tLoss: 680.1428\n",
      "Training Epoch: 6 [34000/36450]\tLoss: 737.6066\n",
      "Training Epoch: 6 [34050/36450]\tLoss: 685.2316\n",
      "Training Epoch: 6 [34100/36450]\tLoss: 704.8177\n",
      "Training Epoch: 6 [34150/36450]\tLoss: 642.7254\n",
      "Training Epoch: 6 [34200/36450]\tLoss: 672.9735\n",
      "Training Epoch: 6 [34250/36450]\tLoss: 675.4955\n",
      "Training Epoch: 6 [34300/36450]\tLoss: 726.4622\n",
      "Training Epoch: 6 [34350/36450]\tLoss: 687.8334\n",
      "Training Epoch: 6 [34400/36450]\tLoss: 714.4833\n",
      "Training Epoch: 6 [34450/36450]\tLoss: 720.6683\n",
      "Training Epoch: 6 [34500/36450]\tLoss: 682.4987\n",
      "Training Epoch: 6 [34550/36450]\tLoss: 685.6495\n",
      "Training Epoch: 6 [34600/36450]\tLoss: 702.9742\n",
      "Training Epoch: 6 [34650/36450]\tLoss: 688.6011\n",
      "Training Epoch: 6 [34700/36450]\tLoss: 678.9297\n",
      "Training Epoch: 6 [34750/36450]\tLoss: 741.1712\n",
      "Training Epoch: 6 [34800/36450]\tLoss: 688.7516\n",
      "Training Epoch: 6 [34850/36450]\tLoss: 707.5536\n",
      "Training Epoch: 6 [34900/36450]\tLoss: 745.2475\n",
      "Training Epoch: 6 [34950/36450]\tLoss: 707.0941\n",
      "Training Epoch: 6 [35000/36450]\tLoss: 649.6354\n",
      "Training Epoch: 6 [35050/36450]\tLoss: 676.9877\n",
      "Training Epoch: 6 [35100/36450]\tLoss: 735.1860\n",
      "Training Epoch: 6 [35150/36450]\tLoss: 693.6417\n",
      "Training Epoch: 6 [35200/36450]\tLoss: 682.5197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [35250/36450]\tLoss: 737.8884\n",
      "Training Epoch: 6 [35300/36450]\tLoss: 693.6992\n",
      "Training Epoch: 6 [35350/36450]\tLoss: 722.0598\n",
      "Training Epoch: 6 [35400/36450]\tLoss: 718.3801\n",
      "Training Epoch: 6 [35450/36450]\tLoss: 690.4388\n",
      "Training Epoch: 6 [35500/36450]\tLoss: 677.7908\n",
      "Training Epoch: 6 [35550/36450]\tLoss: 713.2581\n",
      "Training Epoch: 6 [35600/36450]\tLoss: 741.5840\n",
      "Training Epoch: 6 [35650/36450]\tLoss: 726.9681\n",
      "Training Epoch: 6 [35700/36450]\tLoss: 690.9758\n",
      "Training Epoch: 6 [35750/36450]\tLoss: 666.8156\n",
      "Training Epoch: 6 [35800/36450]\tLoss: 769.7911\n",
      "Training Epoch: 6 [35850/36450]\tLoss: 719.1168\n",
      "Training Epoch: 6 [35900/36450]\tLoss: 725.4639\n",
      "Training Epoch: 6 [35950/36450]\tLoss: 727.8710\n",
      "Training Epoch: 6 [36000/36450]\tLoss: 760.7203\n",
      "Training Epoch: 6 [36050/36450]\tLoss: 746.0558\n",
      "Training Epoch: 6 [36100/36450]\tLoss: 739.1428\n",
      "Training Epoch: 6 [36150/36450]\tLoss: 699.1534\n",
      "Training Epoch: 6 [36200/36450]\tLoss: 699.1627\n",
      "Training Epoch: 6 [36250/36450]\tLoss: 655.0244\n",
      "Training Epoch: 6 [36300/36450]\tLoss: 714.3575\n",
      "Training Epoch: 6 [36350/36450]\tLoss: 676.8149\n",
      "Training Epoch: 6 [36400/36450]\tLoss: 721.9946\n",
      "Training Epoch: 6 [36450/36450]\tLoss: 711.4597\n",
      "Training Epoch: 6 [4050/4050]\tLoss: 359.9916\n",
      "Training Epoch: 7 [50/36450]\tLoss: 679.2437\n",
      "Training Epoch: 7 [100/36450]\tLoss: 749.5817\n",
      "Training Epoch: 7 [150/36450]\tLoss: 642.5077\n",
      "Training Epoch: 7 [200/36450]\tLoss: 701.2106\n",
      "Training Epoch: 7 [250/36450]\tLoss: 687.5866\n",
      "Training Epoch: 7 [300/36450]\tLoss: 721.1055\n",
      "Training Epoch: 7 [350/36450]\tLoss: 720.7905\n",
      "Training Epoch: 7 [400/36450]\tLoss: 721.5246\n",
      "Training Epoch: 7 [450/36450]\tLoss: 702.3336\n",
      "Training Epoch: 7 [500/36450]\tLoss: 717.8855\n",
      "Training Epoch: 7 [550/36450]\tLoss: 677.6697\n",
      "Training Epoch: 7 [600/36450]\tLoss: 701.1597\n",
      "Training Epoch: 7 [650/36450]\tLoss: 679.2646\n",
      "Training Epoch: 7 [700/36450]\tLoss: 668.1218\n",
      "Training Epoch: 7 [750/36450]\tLoss: 720.9938\n",
      "Training Epoch: 7 [800/36450]\tLoss: 742.1400\n",
      "Training Epoch: 7 [850/36450]\tLoss: 730.7777\n",
      "Training Epoch: 7 [900/36450]\tLoss: 737.9299\n",
      "Training Epoch: 7 [950/36450]\tLoss: 715.4012\n",
      "Training Epoch: 7 [1000/36450]\tLoss: 740.5984\n",
      "Training Epoch: 7 [1050/36450]\tLoss: 694.6482\n",
      "Training Epoch: 7 [1100/36450]\tLoss: 683.5280\n",
      "Training Epoch: 7 [1150/36450]\tLoss: 693.7198\n",
      "Training Epoch: 7 [1200/36450]\tLoss: 677.6111\n",
      "Training Epoch: 7 [1250/36450]\tLoss: 718.4671\n",
      "Training Epoch: 7 [1300/36450]\tLoss: 696.4462\n",
      "Training Epoch: 7 [1350/36450]\tLoss: 712.3371\n",
      "Training Epoch: 7 [1400/36450]\tLoss: 675.2881\n",
      "Training Epoch: 7 [1450/36450]\tLoss: 745.4134\n",
      "Training Epoch: 7 [1500/36450]\tLoss: 714.5058\n",
      "Training Epoch: 7 [1550/36450]\tLoss: 694.2013\n",
      "Training Epoch: 7 [1600/36450]\tLoss: 694.8323\n",
      "Training Epoch: 7 [1650/36450]\tLoss: 690.1039\n",
      "Training Epoch: 7 [1700/36450]\tLoss: 724.7429\n",
      "Training Epoch: 7 [1750/36450]\tLoss: 709.9820\n",
      "Training Epoch: 7 [1800/36450]\tLoss: 672.6184\n",
      "Training Epoch: 7 [1850/36450]\tLoss: 740.7497\n",
      "Training Epoch: 7 [1900/36450]\tLoss: 682.6803\n",
      "Training Epoch: 7 [1950/36450]\tLoss: 698.4348\n",
      "Training Epoch: 7 [2000/36450]\tLoss: 696.1669\n",
      "Training Epoch: 7 [2050/36450]\tLoss: 668.1238\n",
      "Training Epoch: 7 [2100/36450]\tLoss: 687.1583\n",
      "Training Epoch: 7 [2150/36450]\tLoss: 727.3928\n",
      "Training Epoch: 7 [2200/36450]\tLoss: 694.2529\n",
      "Training Epoch: 7 [2250/36450]\tLoss: 721.6648\n",
      "Training Epoch: 7 [2300/36450]\tLoss: 710.3781\n",
      "Training Epoch: 7 [2350/36450]\tLoss: 710.8229\n",
      "Training Epoch: 7 [2400/36450]\tLoss: 687.0080\n",
      "Training Epoch: 7 [2450/36450]\tLoss: 718.5988\n",
      "Training Epoch: 7 [2500/36450]\tLoss: 745.4584\n",
      "Training Epoch: 7 [2550/36450]\tLoss: 698.1710\n",
      "Training Epoch: 7 [2600/36450]\tLoss: 700.5206\n",
      "Training Epoch: 7 [2650/36450]\tLoss: 691.1103\n",
      "Training Epoch: 7 [2700/36450]\tLoss: 709.7966\n",
      "Training Epoch: 7 [2750/36450]\tLoss: 702.4282\n",
      "Training Epoch: 7 [2800/36450]\tLoss: 712.4774\n",
      "Training Epoch: 7 [2850/36450]\tLoss: 700.8780\n",
      "Training Epoch: 7 [2900/36450]\tLoss: 693.1225\n",
      "Training Epoch: 7 [2950/36450]\tLoss: 712.4200\n",
      "Training Epoch: 7 [3000/36450]\tLoss: 666.0097\n",
      "Training Epoch: 7 [3050/36450]\tLoss: 663.2557\n",
      "Training Epoch: 7 [3100/36450]\tLoss: 708.0219\n",
      "Training Epoch: 7 [3150/36450]\tLoss: 689.1859\n",
      "Training Epoch: 7 [3200/36450]\tLoss: 731.0281\n",
      "Training Epoch: 7 [3250/36450]\tLoss: 643.9841\n",
      "Training Epoch: 7 [3300/36450]\tLoss: 670.6072\n",
      "Training Epoch: 7 [3350/36450]\tLoss: 704.6525\n",
      "Training Epoch: 7 [3400/36450]\tLoss: 742.0937\n",
      "Training Epoch: 7 [3450/36450]\tLoss: 703.6552\n",
      "Training Epoch: 7 [3500/36450]\tLoss: 698.5890\n",
      "Training Epoch: 7 [3550/36450]\tLoss: 707.4184\n",
      "Training Epoch: 7 [3600/36450]\tLoss: 737.1074\n",
      "Training Epoch: 7 [3650/36450]\tLoss: 688.3344\n",
      "Training Epoch: 7 [3700/36450]\tLoss: 680.1556\n",
      "Training Epoch: 7 [3750/36450]\tLoss: 707.8265\n",
      "Training Epoch: 7 [3800/36450]\tLoss: 711.0246\n",
      "Training Epoch: 7 [3850/36450]\tLoss: 701.1155\n",
      "Training Epoch: 7 [3900/36450]\tLoss: 714.3213\n",
      "Training Epoch: 7 [3950/36450]\tLoss: 694.0512\n",
      "Training Epoch: 7 [4000/36450]\tLoss: 711.5163\n",
      "Training Epoch: 7 [4050/36450]\tLoss: 647.4770\n",
      "Training Epoch: 7 [4100/36450]\tLoss: 662.7496\n",
      "Training Epoch: 7 [4150/36450]\tLoss: 667.4573\n",
      "Training Epoch: 7 [4200/36450]\tLoss: 694.2199\n",
      "Training Epoch: 7 [4250/36450]\tLoss: 688.5841\n",
      "Training Epoch: 7 [4300/36450]\tLoss: 694.9821\n",
      "Training Epoch: 7 [4350/36450]\tLoss: 629.2421\n",
      "Training Epoch: 7 [4400/36450]\tLoss: 678.7472\n",
      "Training Epoch: 7 [4450/36450]\tLoss: 684.7757\n",
      "Training Epoch: 7 [4500/36450]\tLoss: 688.1844\n",
      "Training Epoch: 7 [4550/36450]\tLoss: 666.8612\n",
      "Training Epoch: 7 [4600/36450]\tLoss: 688.4252\n",
      "Training Epoch: 7 [4650/36450]\tLoss: 656.5413\n",
      "Training Epoch: 7 [4700/36450]\tLoss: 639.5860\n",
      "Training Epoch: 7 [4750/36450]\tLoss: 659.8701\n",
      "Training Epoch: 7 [4800/36450]\tLoss: 733.9045\n",
      "Training Epoch: 7 [4850/36450]\tLoss: 716.3039\n",
      "Training Epoch: 7 [4900/36450]\tLoss: 680.0977\n",
      "Training Epoch: 7 [4950/36450]\tLoss: 698.5591\n",
      "Training Epoch: 7 [5000/36450]\tLoss: 684.6643\n",
      "Training Epoch: 7 [5050/36450]\tLoss: 657.9542\n",
      "Training Epoch: 7 [5100/36450]\tLoss: 649.4608\n",
      "Training Epoch: 7 [5150/36450]\tLoss: 720.2864\n",
      "Training Epoch: 7 [5200/36450]\tLoss: 663.4014\n",
      "Training Epoch: 7 [5250/36450]\tLoss: 646.8330\n",
      "Training Epoch: 7 [5300/36450]\tLoss: 715.1392\n",
      "Training Epoch: 7 [5350/36450]\tLoss: 672.6286\n",
      "Training Epoch: 7 [5400/36450]\tLoss: 712.0817\n",
      "Training Epoch: 7 [5450/36450]\tLoss: 672.3172\n",
      "Training Epoch: 7 [5500/36450]\tLoss: 704.5162\n",
      "Training Epoch: 7 [5550/36450]\tLoss: 734.9420\n",
      "Training Epoch: 7 [5600/36450]\tLoss: 691.2799\n",
      "Training Epoch: 7 [5650/36450]\tLoss: 734.5400\n",
      "Training Epoch: 7 [5700/36450]\tLoss: 672.3525\n",
      "Training Epoch: 7 [5750/36450]\tLoss: 677.0445\n",
      "Training Epoch: 7 [5800/36450]\tLoss: 681.0588\n",
      "Training Epoch: 7 [5850/36450]\tLoss: 674.9581\n",
      "Training Epoch: 7 [5900/36450]\tLoss: 733.2950\n",
      "Training Epoch: 7 [5950/36450]\tLoss: 680.4563\n",
      "Training Epoch: 7 [6000/36450]\tLoss: 701.3808\n",
      "Training Epoch: 7 [6050/36450]\tLoss: 674.2620\n",
      "Training Epoch: 7 [6100/36450]\tLoss: 668.0349\n",
      "Training Epoch: 7 [6150/36450]\tLoss: 710.1635\n",
      "Training Epoch: 7 [6200/36450]\tLoss: 685.7858\n",
      "Training Epoch: 7 [6250/36450]\tLoss: 683.3273\n",
      "Training Epoch: 7 [6300/36450]\tLoss: 699.3606\n",
      "Training Epoch: 7 [6350/36450]\tLoss: 704.5397\n",
      "Training Epoch: 7 [6400/36450]\tLoss: 747.7980\n",
      "Training Epoch: 7 [6450/36450]\tLoss: 678.7523\n",
      "Training Epoch: 7 [6500/36450]\tLoss: 709.3528\n",
      "Training Epoch: 7 [6550/36450]\tLoss: 728.3227\n",
      "Training Epoch: 7 [6600/36450]\tLoss: 668.6480\n",
      "Training Epoch: 7 [6650/36450]\tLoss: 695.4245\n",
      "Training Epoch: 7 [6700/36450]\tLoss: 719.7654\n",
      "Training Epoch: 7 [6750/36450]\tLoss: 682.8215\n",
      "Training Epoch: 7 [6800/36450]\tLoss: 705.4077\n",
      "Training Epoch: 7 [6850/36450]\tLoss: 666.8433\n",
      "Training Epoch: 7 [6900/36450]\tLoss: 690.3375\n",
      "Training Epoch: 7 [6950/36450]\tLoss: 727.8439\n",
      "Training Epoch: 7 [7000/36450]\tLoss: 677.1849\n",
      "Training Epoch: 7 [7050/36450]\tLoss: 681.9795\n",
      "Training Epoch: 7 [7100/36450]\tLoss: 712.8555\n",
      "Training Epoch: 7 [7150/36450]\tLoss: 708.4860\n",
      "Training Epoch: 7 [7200/36450]\tLoss: 690.6265\n",
      "Training Epoch: 7 [7250/36450]\tLoss: 676.3293\n",
      "Training Epoch: 7 [7300/36450]\tLoss: 654.0448\n",
      "Training Epoch: 7 [7350/36450]\tLoss: 732.0485\n",
      "Training Epoch: 7 [7400/36450]\tLoss: 677.0858\n",
      "Training Epoch: 7 [7450/36450]\tLoss: 658.4152\n",
      "Training Epoch: 7 [7500/36450]\tLoss: 711.5283\n",
      "Training Epoch: 7 [7550/36450]\tLoss: 644.5494\n",
      "Training Epoch: 7 [7600/36450]\tLoss: 701.0022\n",
      "Training Epoch: 7 [7650/36450]\tLoss: 678.1048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [7700/36450]\tLoss: 692.1424\n",
      "Training Epoch: 7 [7750/36450]\tLoss: 690.3773\n",
      "Training Epoch: 7 [7800/36450]\tLoss: 670.0766\n",
      "Training Epoch: 7 [7850/36450]\tLoss: 656.1794\n",
      "Training Epoch: 7 [7900/36450]\tLoss: 688.5885\n",
      "Training Epoch: 7 [7950/36450]\tLoss: 643.4805\n",
      "Training Epoch: 7 [8000/36450]\tLoss: 671.5026\n",
      "Training Epoch: 7 [8050/36450]\tLoss: 692.5434\n",
      "Training Epoch: 7 [8100/36450]\tLoss: 691.9362\n",
      "Training Epoch: 7 [8150/36450]\tLoss: 658.5405\n",
      "Training Epoch: 7 [8200/36450]\tLoss: 679.4465\n",
      "Training Epoch: 7 [8250/36450]\tLoss: 691.8839\n",
      "Training Epoch: 7 [8300/36450]\tLoss: 685.4948\n",
      "Training Epoch: 7 [8350/36450]\tLoss: 702.5501\n",
      "Training Epoch: 7 [8400/36450]\tLoss: 653.9606\n",
      "Training Epoch: 7 [8450/36450]\tLoss: 665.7365\n",
      "Training Epoch: 7 [8500/36450]\tLoss: 706.7094\n",
      "Training Epoch: 7 [8550/36450]\tLoss: 686.1866\n",
      "Training Epoch: 7 [8600/36450]\tLoss: 658.9782\n",
      "Training Epoch: 7 [8650/36450]\tLoss: 671.9318\n",
      "Training Epoch: 7 [8700/36450]\tLoss: 664.2812\n",
      "Training Epoch: 7 [8750/36450]\tLoss: 670.7454\n",
      "Training Epoch: 7 [8800/36450]\tLoss: 659.3112\n",
      "Training Epoch: 7 [8850/36450]\tLoss: 679.1040\n",
      "Training Epoch: 7 [8900/36450]\tLoss: 651.2694\n",
      "Training Epoch: 7 [8950/36450]\tLoss: 714.4738\n",
      "Training Epoch: 7 [9000/36450]\tLoss: 654.1597\n",
      "Training Epoch: 7 [9050/36450]\tLoss: 648.2253\n",
      "Training Epoch: 7 [9100/36450]\tLoss: 691.0223\n",
      "Training Epoch: 7 [9150/36450]\tLoss: 684.9810\n",
      "Training Epoch: 7 [9200/36450]\tLoss: 652.8386\n",
      "Training Epoch: 7 [9250/36450]\tLoss: 704.0232\n",
      "Training Epoch: 7 [9300/36450]\tLoss: 670.4566\n",
      "Training Epoch: 7 [9350/36450]\tLoss: 726.3743\n",
      "Training Epoch: 7 [9400/36450]\tLoss: 726.8621\n",
      "Training Epoch: 7 [9450/36450]\tLoss: 661.2258\n",
      "Training Epoch: 7 [9500/36450]\tLoss: 703.5125\n",
      "Training Epoch: 7 [9550/36450]\tLoss: 724.1061\n",
      "Training Epoch: 7 [9600/36450]\tLoss: 674.6884\n",
      "Training Epoch: 7 [9650/36450]\tLoss: 667.2404\n",
      "Training Epoch: 7 [9700/36450]\tLoss: 693.1426\n",
      "Training Epoch: 7 [9750/36450]\tLoss: 721.2639\n",
      "Training Epoch: 7 [9800/36450]\tLoss: 671.8730\n",
      "Training Epoch: 7 [9850/36450]\tLoss: 703.0218\n",
      "Training Epoch: 7 [9900/36450]\tLoss: 660.3238\n",
      "Training Epoch: 7 [9950/36450]\tLoss: 715.2010\n",
      "Training Epoch: 7 [10000/36450]\tLoss: 735.3670\n",
      "Training Epoch: 7 [10050/36450]\tLoss: 680.9772\n",
      "Training Epoch: 7 [10100/36450]\tLoss: 675.9409\n",
      "Training Epoch: 7 [10150/36450]\tLoss: 695.5500\n",
      "Training Epoch: 7 [10200/36450]\tLoss: 682.3588\n",
      "Training Epoch: 7 [10250/36450]\tLoss: 726.4608\n",
      "Training Epoch: 7 [10300/36450]\tLoss: 668.2928\n",
      "Training Epoch: 7 [10350/36450]\tLoss: 664.3290\n",
      "Training Epoch: 7 [10400/36450]\tLoss: 645.8492\n",
      "Training Epoch: 7 [10450/36450]\tLoss: 674.1110\n",
      "Training Epoch: 7 [10500/36450]\tLoss: 661.0790\n",
      "Training Epoch: 7 [10550/36450]\tLoss: 646.5253\n",
      "Training Epoch: 7 [10600/36450]\tLoss: 673.2817\n",
      "Training Epoch: 7 [10650/36450]\tLoss: 691.3709\n",
      "Training Epoch: 7 [10700/36450]\tLoss: 672.6940\n",
      "Training Epoch: 7 [10750/36450]\tLoss: 672.6049\n",
      "Training Epoch: 7 [10800/36450]\tLoss: 658.2964\n",
      "Training Epoch: 7 [10850/36450]\tLoss: 685.6273\n",
      "Training Epoch: 7 [10900/36450]\tLoss: 666.0107\n",
      "Training Epoch: 7 [10950/36450]\tLoss: 632.3419\n",
      "Training Epoch: 7 [11000/36450]\tLoss: 644.9337\n",
      "Training Epoch: 7 [11050/36450]\tLoss: 698.4038\n",
      "Training Epoch: 7 [11100/36450]\tLoss: 644.9199\n",
      "Training Epoch: 7 [11150/36450]\tLoss: 681.5609\n",
      "Training Epoch: 7 [11200/36450]\tLoss: 674.5315\n",
      "Training Epoch: 7 [11250/36450]\tLoss: 669.3159\n",
      "Training Epoch: 7 [11300/36450]\tLoss: 674.2532\n",
      "Training Epoch: 7 [11350/36450]\tLoss: 699.1437\n",
      "Training Epoch: 7 [11400/36450]\tLoss: 731.8135\n",
      "Training Epoch: 7 [11450/36450]\tLoss: 652.4149\n",
      "Training Epoch: 7 [11500/36450]\tLoss: 704.5765\n",
      "Training Epoch: 7 [11550/36450]\tLoss: 716.9459\n",
      "Training Epoch: 7 [11600/36450]\tLoss: 723.1970\n",
      "Training Epoch: 7 [11650/36450]\tLoss: 671.7106\n",
      "Training Epoch: 7 [11700/36450]\tLoss: 710.6089\n",
      "Training Epoch: 7 [11750/36450]\tLoss: 727.6979\n",
      "Training Epoch: 7 [11800/36450]\tLoss: 742.6033\n",
      "Training Epoch: 7 [11850/36450]\tLoss: 715.0079\n",
      "Training Epoch: 7 [11900/36450]\tLoss: 705.6309\n",
      "Training Epoch: 7 [11950/36450]\tLoss: 688.0283\n",
      "Training Epoch: 7 [12000/36450]\tLoss: 709.8022\n",
      "Training Epoch: 7 [12050/36450]\tLoss: 710.4219\n",
      "Training Epoch: 7 [12100/36450]\tLoss: 673.5840\n",
      "Training Epoch: 7 [12150/36450]\tLoss: 676.1942\n",
      "Training Epoch: 7 [12200/36450]\tLoss: 719.9435\n",
      "Training Epoch: 7 [12250/36450]\tLoss: 713.1487\n",
      "Training Epoch: 7 [12300/36450]\tLoss: 672.8411\n",
      "Training Epoch: 7 [12350/36450]\tLoss: 689.6156\n",
      "Training Epoch: 7 [12400/36450]\tLoss: 678.7471\n",
      "Training Epoch: 7 [12450/36450]\tLoss: 669.0690\n",
      "Training Epoch: 7 [12500/36450]\tLoss: 643.8540\n",
      "Training Epoch: 7 [12550/36450]\tLoss: 706.4758\n",
      "Training Epoch: 7 [12600/36450]\tLoss: 704.1855\n",
      "Training Epoch: 7 [12650/36450]\tLoss: 674.1711\n",
      "Training Epoch: 7 [12700/36450]\tLoss: 701.8972\n",
      "Training Epoch: 7 [12750/36450]\tLoss: 717.3683\n",
      "Training Epoch: 7 [12800/36450]\tLoss: 707.6105\n",
      "Training Epoch: 7 [12850/36450]\tLoss: 666.7421\n",
      "Training Epoch: 7 [12900/36450]\tLoss: 678.9095\n",
      "Training Epoch: 7 [12950/36450]\tLoss: 656.5742\n",
      "Training Epoch: 7 [13000/36450]\tLoss: 684.3184\n",
      "Training Epoch: 7 [13050/36450]\tLoss: 640.7295\n",
      "Training Epoch: 7 [13100/36450]\tLoss: 677.7065\n",
      "Training Epoch: 7 [13150/36450]\tLoss: 698.2594\n",
      "Training Epoch: 7 [13200/36450]\tLoss: 683.5909\n",
      "Training Epoch: 7 [13250/36450]\tLoss: 660.8422\n",
      "Training Epoch: 7 [13300/36450]\tLoss: 641.2458\n",
      "Training Epoch: 7 [13350/36450]\tLoss: 680.5900\n",
      "Training Epoch: 7 [13400/36450]\tLoss: 718.0458\n",
      "Training Epoch: 7 [13450/36450]\tLoss: 676.8165\n",
      "Training Epoch: 7 [13500/36450]\tLoss: 707.8502\n",
      "Training Epoch: 7 [13550/36450]\tLoss: 689.8787\n",
      "Training Epoch: 7 [13600/36450]\tLoss: 706.2651\n",
      "Training Epoch: 7 [13650/36450]\tLoss: 659.1176\n",
      "Training Epoch: 7 [13700/36450]\tLoss: 712.9267\n",
      "Training Epoch: 7 [13750/36450]\tLoss: 645.8298\n",
      "Training Epoch: 7 [13800/36450]\tLoss: 731.7887\n",
      "Training Epoch: 7 [13850/36450]\tLoss: 667.0448\n",
      "Training Epoch: 7 [13900/36450]\tLoss: 663.0326\n",
      "Training Epoch: 7 [13950/36450]\tLoss: 688.7247\n",
      "Training Epoch: 7 [14000/36450]\tLoss: 654.7620\n",
      "Training Epoch: 7 [14050/36450]\tLoss: 708.0592\n",
      "Training Epoch: 7 [14100/36450]\tLoss: 648.7513\n",
      "Training Epoch: 7 [14150/36450]\tLoss: 694.5200\n",
      "Training Epoch: 7 [14200/36450]\tLoss: 724.6433\n",
      "Training Epoch: 7 [14250/36450]\tLoss: 675.1946\n",
      "Training Epoch: 7 [14300/36450]\tLoss: 688.0565\n",
      "Training Epoch: 7 [14350/36450]\tLoss: 675.4775\n",
      "Training Epoch: 7 [14400/36450]\tLoss: 662.7202\n",
      "Training Epoch: 7 [14450/36450]\tLoss: 694.8704\n",
      "Training Epoch: 7 [14500/36450]\tLoss: 636.0282\n",
      "Training Epoch: 7 [14550/36450]\tLoss: 653.9199\n",
      "Training Epoch: 7 [14600/36450]\tLoss: 699.8197\n",
      "Training Epoch: 7 [14650/36450]\tLoss: 718.7250\n",
      "Training Epoch: 7 [14700/36450]\tLoss: 686.4381\n",
      "Training Epoch: 7 [14750/36450]\tLoss: 637.5645\n",
      "Training Epoch: 7 [14800/36450]\tLoss: 688.3287\n",
      "Training Epoch: 7 [14850/36450]\tLoss: 708.8609\n",
      "Training Epoch: 7 [14900/36450]\tLoss: 726.0125\n",
      "Training Epoch: 7 [14950/36450]\tLoss: 715.1630\n",
      "Training Epoch: 7 [15000/36450]\tLoss: 695.1539\n",
      "Training Epoch: 7 [15050/36450]\tLoss: 719.8318\n",
      "Training Epoch: 7 [15100/36450]\tLoss: 712.4209\n",
      "Training Epoch: 7 [15150/36450]\tLoss: 687.8514\n",
      "Training Epoch: 7 [15200/36450]\tLoss: 655.5140\n",
      "Training Epoch: 7 [15250/36450]\tLoss: 666.1519\n",
      "Training Epoch: 7 [15300/36450]\tLoss: 706.9818\n",
      "Training Epoch: 7 [15350/36450]\tLoss: 671.6384\n",
      "Training Epoch: 7 [15400/36450]\tLoss: 738.8810\n",
      "Training Epoch: 7 [15450/36450]\tLoss: 657.0165\n",
      "Training Epoch: 7 [15500/36450]\tLoss: 685.8736\n",
      "Training Epoch: 7 [15550/36450]\tLoss: 669.6321\n",
      "Training Epoch: 7 [15600/36450]\tLoss: 733.7518\n",
      "Training Epoch: 7 [15650/36450]\tLoss: 689.4916\n",
      "Training Epoch: 7 [15700/36450]\tLoss: 696.7549\n",
      "Training Epoch: 7 [15750/36450]\tLoss: 639.9899\n",
      "Training Epoch: 7 [15800/36450]\tLoss: 681.0872\n",
      "Training Epoch: 7 [15850/36450]\tLoss: 654.1211\n",
      "Training Epoch: 7 [15900/36450]\tLoss: 714.5123\n",
      "Training Epoch: 7 [15950/36450]\tLoss: 714.2328\n",
      "Training Epoch: 7 [16000/36450]\tLoss: 681.1434\n",
      "Training Epoch: 7 [16050/36450]\tLoss: 709.2784\n",
      "Training Epoch: 7 [16100/36450]\tLoss: 659.6976\n",
      "Training Epoch: 7 [16150/36450]\tLoss: 682.5106\n",
      "Training Epoch: 7 [16200/36450]\tLoss: 701.5281\n",
      "Training Epoch: 7 [16250/36450]\tLoss: 712.1813\n",
      "Training Epoch: 7 [16300/36450]\tLoss: 662.8070\n",
      "Training Epoch: 7 [16350/36450]\tLoss: 739.8447\n",
      "Training Epoch: 7 [16400/36450]\tLoss: 681.3697\n",
      "Training Epoch: 7 [16450/36450]\tLoss: 675.2523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [16500/36450]\tLoss: 709.5861\n",
      "Training Epoch: 7 [16550/36450]\tLoss: 740.8683\n",
      "Training Epoch: 7 [16600/36450]\tLoss: 708.5875\n",
      "Training Epoch: 7 [16650/36450]\tLoss: 709.3259\n",
      "Training Epoch: 7 [16700/36450]\tLoss: 668.3699\n",
      "Training Epoch: 7 [16750/36450]\tLoss: 640.2225\n",
      "Training Epoch: 7 [16800/36450]\tLoss: 668.7303\n",
      "Training Epoch: 7 [16850/36450]\tLoss: 669.5087\n",
      "Training Epoch: 7 [16900/36450]\tLoss: 696.8338\n",
      "Training Epoch: 7 [16950/36450]\tLoss: 697.3103\n",
      "Training Epoch: 7 [17000/36450]\tLoss: 678.8743\n",
      "Training Epoch: 7 [17050/36450]\tLoss: 689.6160\n",
      "Training Epoch: 7 [17100/36450]\tLoss: 660.8667\n",
      "Training Epoch: 7 [17150/36450]\tLoss: 643.2123\n",
      "Training Epoch: 7 [17200/36450]\tLoss: 645.1850\n",
      "Training Epoch: 7 [17250/36450]\tLoss: 691.5093\n",
      "Training Epoch: 7 [17300/36450]\tLoss: 690.0662\n",
      "Training Epoch: 7 [17350/36450]\tLoss: 699.4420\n",
      "Training Epoch: 7 [17400/36450]\tLoss: 683.9592\n",
      "Training Epoch: 7 [17450/36450]\tLoss: 691.5831\n",
      "Training Epoch: 7 [17500/36450]\tLoss: 633.2816\n",
      "Training Epoch: 7 [17550/36450]\tLoss: 712.9255\n",
      "Training Epoch: 7 [17600/36450]\tLoss: 695.2566\n",
      "Training Epoch: 7 [17650/36450]\tLoss: 658.4144\n",
      "Training Epoch: 7 [17700/36450]\tLoss: 708.1593\n",
      "Training Epoch: 7 [17750/36450]\tLoss: 688.0334\n",
      "Training Epoch: 7 [17800/36450]\tLoss: 698.7706\n",
      "Training Epoch: 7 [17850/36450]\tLoss: 693.7052\n",
      "Training Epoch: 7 [17900/36450]\tLoss: 668.3762\n",
      "Training Epoch: 7 [17950/36450]\tLoss: 675.9609\n",
      "Training Epoch: 7 [18000/36450]\tLoss: 652.9019\n",
      "Training Epoch: 7 [18050/36450]\tLoss: 668.4670\n",
      "Training Epoch: 7 [18100/36450]\tLoss: 665.7306\n",
      "Training Epoch: 7 [18150/36450]\tLoss: 671.0588\n",
      "Training Epoch: 7 [18200/36450]\tLoss: 703.8315\n",
      "Training Epoch: 7 [18250/36450]\tLoss: 685.8762\n",
      "Training Epoch: 7 [18300/36450]\tLoss: 674.2839\n",
      "Training Epoch: 7 [18350/36450]\tLoss: 684.7368\n",
      "Training Epoch: 7 [18400/36450]\tLoss: 692.8412\n",
      "Training Epoch: 7 [18450/36450]\tLoss: 684.0754\n",
      "Training Epoch: 7 [18500/36450]\tLoss: 639.2660\n",
      "Training Epoch: 7 [18550/36450]\tLoss: 668.4519\n",
      "Training Epoch: 7 [18600/36450]\tLoss: 671.3250\n",
      "Training Epoch: 7 [18650/36450]\tLoss: 672.6589\n",
      "Training Epoch: 7 [18700/36450]\tLoss: 664.5367\n",
      "Training Epoch: 7 [18750/36450]\tLoss: 699.3054\n",
      "Training Epoch: 7 [18800/36450]\tLoss: 676.1609\n",
      "Training Epoch: 7 [18850/36450]\tLoss: 683.4938\n",
      "Training Epoch: 7 [18900/36450]\tLoss: 665.6677\n",
      "Training Epoch: 7 [18950/36450]\tLoss: 704.7083\n",
      "Training Epoch: 7 [19000/36450]\tLoss: 708.6671\n",
      "Training Epoch: 7 [19050/36450]\tLoss: 692.4354\n",
      "Training Epoch: 7 [19100/36450]\tLoss: 684.1696\n",
      "Training Epoch: 7 [19150/36450]\tLoss: 685.4914\n",
      "Training Epoch: 7 [19200/36450]\tLoss: 650.4116\n",
      "Training Epoch: 7 [19250/36450]\tLoss: 678.3519\n",
      "Training Epoch: 7 [19300/36450]\tLoss: 662.2166\n",
      "Training Epoch: 7 [19350/36450]\tLoss: 701.6258\n",
      "Training Epoch: 7 [19400/36450]\tLoss: 664.3231\n",
      "Training Epoch: 7 [19450/36450]\tLoss: 680.6176\n",
      "Training Epoch: 7 [19500/36450]\tLoss: 617.4963\n",
      "Training Epoch: 7 [19550/36450]\tLoss: 679.7035\n",
      "Training Epoch: 7 [19600/36450]\tLoss: 690.8945\n",
      "Training Epoch: 7 [19650/36450]\tLoss: 663.8152\n",
      "Training Epoch: 7 [19700/36450]\tLoss: 660.6831\n",
      "Training Epoch: 7 [19750/36450]\tLoss: 666.2952\n",
      "Training Epoch: 7 [19800/36450]\tLoss: 673.0352\n",
      "Training Epoch: 7 [19850/36450]\tLoss: 656.1218\n",
      "Training Epoch: 7 [19900/36450]\tLoss: 685.2135\n",
      "Training Epoch: 7 [19950/36450]\tLoss: 691.6403\n",
      "Training Epoch: 7 [20000/36450]\tLoss: 711.6633\n",
      "Training Epoch: 7 [20050/36450]\tLoss: 647.7199\n",
      "Training Epoch: 7 [20100/36450]\tLoss: 683.7352\n",
      "Training Epoch: 7 [20150/36450]\tLoss: 667.0963\n",
      "Training Epoch: 7 [20200/36450]\tLoss: 678.5037\n",
      "Training Epoch: 7 [20250/36450]\tLoss: 691.7946\n",
      "Training Epoch: 7 [20300/36450]\tLoss: 687.4728\n",
      "Training Epoch: 7 [20350/36450]\tLoss: 681.1182\n",
      "Training Epoch: 7 [20400/36450]\tLoss: 723.4802\n",
      "Training Epoch: 7 [20450/36450]\tLoss: 700.4086\n",
      "Training Epoch: 7 [20500/36450]\tLoss: 675.4659\n",
      "Training Epoch: 7 [20550/36450]\tLoss: 691.3176\n",
      "Training Epoch: 7 [20600/36450]\tLoss: 668.9180\n",
      "Training Epoch: 7 [20650/36450]\tLoss: 659.3214\n",
      "Training Epoch: 7 [20700/36450]\tLoss: 671.2616\n",
      "Training Epoch: 7 [20750/36450]\tLoss: 707.2878\n",
      "Training Epoch: 7 [20800/36450]\tLoss: 667.8915\n",
      "Training Epoch: 7 [20850/36450]\tLoss: 708.7061\n",
      "Training Epoch: 7 [20900/36450]\tLoss: 704.4604\n",
      "Training Epoch: 7 [20950/36450]\tLoss: 700.3998\n",
      "Training Epoch: 7 [21000/36450]\tLoss: 661.5951\n",
      "Training Epoch: 7 [21050/36450]\tLoss: 659.2895\n",
      "Training Epoch: 7 [21100/36450]\tLoss: 666.5128\n",
      "Training Epoch: 7 [21150/36450]\tLoss: 689.8063\n",
      "Training Epoch: 7 [21200/36450]\tLoss: 628.2128\n",
      "Training Epoch: 7 [21250/36450]\tLoss: 678.7576\n",
      "Training Epoch: 7 [21300/36450]\tLoss: 663.5555\n",
      "Training Epoch: 7 [21350/36450]\tLoss: 650.6613\n",
      "Training Epoch: 7 [21400/36450]\tLoss: 695.5624\n",
      "Training Epoch: 7 [21450/36450]\tLoss: 668.7499\n",
      "Training Epoch: 7 [21500/36450]\tLoss: 675.4402\n",
      "Training Epoch: 7 [21550/36450]\tLoss: 632.7939\n",
      "Training Epoch: 7 [21600/36450]\tLoss: 677.4971\n",
      "Training Epoch: 7 [21650/36450]\tLoss: 678.2599\n",
      "Training Epoch: 7 [21700/36450]\tLoss: 700.6795\n",
      "Training Epoch: 7 [21750/36450]\tLoss: 694.9046\n",
      "Training Epoch: 7 [21800/36450]\tLoss: 693.0612\n",
      "Training Epoch: 7 [21850/36450]\tLoss: 698.6545\n",
      "Training Epoch: 7 [21900/36450]\tLoss: 704.3229\n",
      "Training Epoch: 7 [21950/36450]\tLoss: 697.2646\n",
      "Training Epoch: 7 [22000/36450]\tLoss: 682.8344\n",
      "Training Epoch: 7 [22050/36450]\tLoss: 754.1622\n",
      "Training Epoch: 7 [22100/36450]\tLoss: 771.9617\n",
      "Training Epoch: 7 [22150/36450]\tLoss: 767.7864\n",
      "Training Epoch: 7 [22200/36450]\tLoss: 759.4100\n",
      "Training Epoch: 7 [22250/36450]\tLoss: 733.9233\n",
      "Training Epoch: 7 [22300/36450]\tLoss: 660.5831\n",
      "Training Epoch: 7 [22350/36450]\tLoss: 654.3922\n",
      "Training Epoch: 7 [22400/36450]\tLoss: 696.0201\n",
      "Training Epoch: 7 [22450/36450]\tLoss: 681.1586\n",
      "Training Epoch: 7 [22500/36450]\tLoss: 700.4227\n",
      "Training Epoch: 7 [22550/36450]\tLoss: 694.5650\n",
      "Training Epoch: 7 [22600/36450]\tLoss: 654.3556\n",
      "Training Epoch: 7 [22650/36450]\tLoss: 630.4986\n",
      "Training Epoch: 7 [22700/36450]\tLoss: 652.2611\n",
      "Training Epoch: 7 [22750/36450]\tLoss: 668.8615\n",
      "Training Epoch: 7 [22800/36450]\tLoss: 694.4329\n",
      "Training Epoch: 7 [22850/36450]\tLoss: 694.4002\n",
      "Training Epoch: 7 [22900/36450]\tLoss: 693.0588\n",
      "Training Epoch: 7 [22950/36450]\tLoss: 686.4564\n",
      "Training Epoch: 7 [23000/36450]\tLoss: 673.0913\n",
      "Training Epoch: 7 [23050/36450]\tLoss: 667.4493\n",
      "Training Epoch: 7 [23100/36450]\tLoss: 687.3530\n",
      "Training Epoch: 7 [23150/36450]\tLoss: 692.3417\n",
      "Training Epoch: 7 [23200/36450]\tLoss: 663.2529\n",
      "Training Epoch: 7 [23250/36450]\tLoss: 657.4476\n",
      "Training Epoch: 7 [23300/36450]\tLoss: 653.0276\n",
      "Training Epoch: 7 [23350/36450]\tLoss: 670.8017\n",
      "Training Epoch: 7 [23400/36450]\tLoss: 655.0296\n",
      "Training Epoch: 7 [23450/36450]\tLoss: 684.0787\n",
      "Training Epoch: 7 [23500/36450]\tLoss: 671.0082\n",
      "Training Epoch: 7 [23550/36450]\tLoss: 646.8107\n",
      "Training Epoch: 7 [23600/36450]\tLoss: 625.1856\n",
      "Training Epoch: 7 [23650/36450]\tLoss: 681.8024\n",
      "Training Epoch: 7 [23700/36450]\tLoss: 670.6831\n",
      "Training Epoch: 7 [23750/36450]\tLoss: 693.7693\n",
      "Training Epoch: 7 [23800/36450]\tLoss: 705.6456\n",
      "Training Epoch: 7 [23850/36450]\tLoss: 686.1996\n",
      "Training Epoch: 7 [23900/36450]\tLoss: 680.3649\n",
      "Training Epoch: 7 [23950/36450]\tLoss: 658.4926\n",
      "Training Epoch: 7 [24000/36450]\tLoss: 634.8174\n",
      "Training Epoch: 7 [24050/36450]\tLoss: 685.5300\n",
      "Training Epoch: 7 [24100/36450]\tLoss: 712.8931\n",
      "Training Epoch: 7 [24150/36450]\tLoss: 635.7859\n",
      "Training Epoch: 7 [24200/36450]\tLoss: 632.9562\n",
      "Training Epoch: 7 [24250/36450]\tLoss: 702.2925\n",
      "Training Epoch: 7 [24300/36450]\tLoss: 666.9558\n",
      "Training Epoch: 7 [24350/36450]\tLoss: 693.5013\n",
      "Training Epoch: 7 [24400/36450]\tLoss: 677.2347\n",
      "Training Epoch: 7 [24450/36450]\tLoss: 632.2946\n",
      "Training Epoch: 7 [24500/36450]\tLoss: 668.5871\n",
      "Training Epoch: 7 [24550/36450]\tLoss: 670.0705\n",
      "Training Epoch: 7 [24600/36450]\tLoss: 667.9568\n",
      "Training Epoch: 7 [24650/36450]\tLoss: 653.9937\n",
      "Training Epoch: 7 [24700/36450]\tLoss: 672.9180\n",
      "Training Epoch: 7 [24750/36450]\tLoss: 685.7408\n",
      "Training Epoch: 7 [24800/36450]\tLoss: 670.9698\n",
      "Training Epoch: 7 [24850/36450]\tLoss: 643.4194\n",
      "Training Epoch: 7 [24900/36450]\tLoss: 686.5894\n",
      "Training Epoch: 7 [24950/36450]\tLoss: 659.0002\n",
      "Training Epoch: 7 [25000/36450]\tLoss: 655.9633\n",
      "Training Epoch: 7 [25050/36450]\tLoss: 649.5342\n",
      "Training Epoch: 7 [25100/36450]\tLoss: 678.7964\n",
      "Training Epoch: 7 [25150/36450]\tLoss: 684.0956\n",
      "Training Epoch: 7 [25200/36450]\tLoss: 648.4607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [25250/36450]\tLoss: 643.6523\n",
      "Training Epoch: 7 [25300/36450]\tLoss: 669.7994\n",
      "Training Epoch: 7 [25350/36450]\tLoss: 680.2232\n",
      "Training Epoch: 7 [25400/36450]\tLoss: 687.1454\n",
      "Training Epoch: 7 [25450/36450]\tLoss: 676.8478\n",
      "Training Epoch: 7 [25500/36450]\tLoss: 669.0777\n",
      "Training Epoch: 7 [25550/36450]\tLoss: 660.6697\n",
      "Training Epoch: 7 [25600/36450]\tLoss: 679.1916\n",
      "Training Epoch: 7 [25650/36450]\tLoss: 670.1972\n",
      "Training Epoch: 7 [25700/36450]\tLoss: 709.0822\n",
      "Training Epoch: 7 [25750/36450]\tLoss: 661.9389\n",
      "Training Epoch: 7 [25800/36450]\tLoss: 704.4816\n",
      "Training Epoch: 7 [25850/36450]\tLoss: 651.7487\n",
      "Training Epoch: 7 [25900/36450]\tLoss: 568.1454\n",
      "Training Epoch: 7 [25950/36450]\tLoss: 644.3671\n",
      "Training Epoch: 7 [26000/36450]\tLoss: 670.2205\n",
      "Training Epoch: 7 [26050/36450]\tLoss: 703.4719\n",
      "Training Epoch: 7 [26100/36450]\tLoss: 666.0981\n",
      "Training Epoch: 7 [26150/36450]\tLoss: 664.6152\n",
      "Training Epoch: 7 [26200/36450]\tLoss: 664.6125\n",
      "Training Epoch: 7 [26250/36450]\tLoss: 651.4819\n",
      "Training Epoch: 7 [26300/36450]\tLoss: 714.0901\n",
      "Training Epoch: 7 [26350/36450]\tLoss: 651.2936\n",
      "Training Epoch: 7 [26400/36450]\tLoss: 682.6503\n",
      "Training Epoch: 7 [26450/36450]\tLoss: 676.8177\n",
      "Training Epoch: 7 [26500/36450]\tLoss: 699.4810\n",
      "Training Epoch: 7 [26550/36450]\tLoss: 658.5140\n",
      "Training Epoch: 7 [26600/36450]\tLoss: 692.8424\n",
      "Training Epoch: 7 [26650/36450]\tLoss: 688.3171\n",
      "Training Epoch: 7 [26700/36450]\tLoss: 671.3409\n",
      "Training Epoch: 7 [26750/36450]\tLoss: 659.3088\n",
      "Training Epoch: 7 [26800/36450]\tLoss: 647.4536\n",
      "Training Epoch: 7 [26850/36450]\tLoss: 651.5933\n",
      "Training Epoch: 7 [26900/36450]\tLoss: 677.0008\n",
      "Training Epoch: 7 [26950/36450]\tLoss: 640.8806\n",
      "Training Epoch: 7 [27000/36450]\tLoss: 691.5279\n",
      "Training Epoch: 7 [27050/36450]\tLoss: 626.2094\n",
      "Training Epoch: 7 [27100/36450]\tLoss: 689.6158\n",
      "Training Epoch: 7 [27150/36450]\tLoss: 628.4489\n",
      "Training Epoch: 7 [27200/36450]\tLoss: 693.0251\n",
      "Training Epoch: 7 [27250/36450]\tLoss: 664.2455\n",
      "Training Epoch: 7 [27300/36450]\tLoss: 666.4719\n",
      "Training Epoch: 7 [27350/36450]\tLoss: 667.6509\n",
      "Training Epoch: 7 [27400/36450]\tLoss: 639.1975\n",
      "Training Epoch: 7 [27450/36450]\tLoss: 636.9927\n",
      "Training Epoch: 7 [27500/36450]\tLoss: 637.2314\n",
      "Training Epoch: 7 [27550/36450]\tLoss: 648.3785\n",
      "Training Epoch: 7 [27600/36450]\tLoss: 671.6343\n",
      "Training Epoch: 7 [27650/36450]\tLoss: 644.8472\n",
      "Training Epoch: 7 [27700/36450]\tLoss: 660.0910\n",
      "Training Epoch: 7 [27750/36450]\tLoss: 603.6698\n",
      "Training Epoch: 7 [27800/36450]\tLoss: 630.8207\n",
      "Training Epoch: 7 [27850/36450]\tLoss: 657.2675\n",
      "Training Epoch: 7 [27900/36450]\tLoss: 670.9123\n",
      "Training Epoch: 7 [27950/36450]\tLoss: 704.0426\n",
      "Training Epoch: 7 [28000/36450]\tLoss: 700.5151\n",
      "Training Epoch: 7 [28050/36450]\tLoss: 683.5796\n",
      "Training Epoch: 7 [28100/36450]\tLoss: 650.4620\n",
      "Training Epoch: 7 [28150/36450]\tLoss: 668.7247\n",
      "Training Epoch: 7 [28200/36450]\tLoss: 684.8308\n",
      "Training Epoch: 7 [28250/36450]\tLoss: 684.1837\n",
      "Training Epoch: 7 [28300/36450]\tLoss: 682.4663\n",
      "Training Epoch: 7 [28350/36450]\tLoss: 644.0858\n",
      "Training Epoch: 7 [28400/36450]\tLoss: 708.9521\n",
      "Training Epoch: 7 [28450/36450]\tLoss: 630.0500\n",
      "Training Epoch: 7 [28500/36450]\tLoss: 678.8558\n",
      "Training Epoch: 7 [28550/36450]\tLoss: 648.3135\n",
      "Training Epoch: 7 [28600/36450]\tLoss: 697.1315\n",
      "Training Epoch: 7 [28650/36450]\tLoss: 638.5350\n",
      "Training Epoch: 7 [28700/36450]\tLoss: 635.0779\n",
      "Training Epoch: 7 [28750/36450]\tLoss: 682.9250\n",
      "Training Epoch: 7 [28800/36450]\tLoss: 682.8573\n",
      "Training Epoch: 7 [28850/36450]\tLoss: 684.1317\n",
      "Training Epoch: 7 [28900/36450]\tLoss: 678.5518\n",
      "Training Epoch: 7 [28950/36450]\tLoss: 637.2335\n",
      "Training Epoch: 7 [29000/36450]\tLoss: 658.6409\n",
      "Training Epoch: 7 [29050/36450]\tLoss: 659.8660\n",
      "Training Epoch: 7 [29100/36450]\tLoss: 665.5220\n",
      "Training Epoch: 7 [29150/36450]\tLoss: 692.4752\n",
      "Training Epoch: 7 [29200/36450]\tLoss: 666.4285\n",
      "Training Epoch: 7 [29250/36450]\tLoss: 697.3688\n",
      "Training Epoch: 7 [29300/36450]\tLoss: 656.8134\n",
      "Training Epoch: 7 [29350/36450]\tLoss: 627.8159\n",
      "Training Epoch: 7 [29400/36450]\tLoss: 669.6202\n",
      "Training Epoch: 7 [29450/36450]\tLoss: 666.8974\n",
      "Training Epoch: 7 [29500/36450]\tLoss: 700.7016\n",
      "Training Epoch: 7 [29550/36450]\tLoss: 688.9231\n",
      "Training Epoch: 7 [29600/36450]\tLoss: 643.7476\n",
      "Training Epoch: 7 [29650/36450]\tLoss: 659.4072\n",
      "Training Epoch: 7 [29700/36450]\tLoss: 712.7756\n",
      "Training Epoch: 7 [29750/36450]\tLoss: 674.0743\n",
      "Training Epoch: 7 [29800/36450]\tLoss: 665.2206\n",
      "Training Epoch: 7 [29850/36450]\tLoss: 668.5423\n",
      "Training Epoch: 7 [29900/36450]\tLoss: 684.0110\n",
      "Training Epoch: 7 [29950/36450]\tLoss: 659.0164\n",
      "Training Epoch: 7 [30000/36450]\tLoss: 635.5148\n",
      "Training Epoch: 7 [30050/36450]\tLoss: 643.5417\n",
      "Training Epoch: 7 [30100/36450]\tLoss: 639.6409\n",
      "Training Epoch: 7 [30150/36450]\tLoss: 677.0555\n",
      "Training Epoch: 7 [30200/36450]\tLoss: 643.0488\n",
      "Training Epoch: 7 [30250/36450]\tLoss: 646.9620\n",
      "Training Epoch: 7 [30300/36450]\tLoss: 659.9001\n",
      "Training Epoch: 7 [30350/36450]\tLoss: 626.7623\n",
      "Training Epoch: 7 [30400/36450]\tLoss: 658.8315\n",
      "Training Epoch: 7 [30450/36450]\tLoss: 643.2759\n",
      "Training Epoch: 7 [30500/36450]\tLoss: 640.9327\n",
      "Training Epoch: 7 [30550/36450]\tLoss: 683.0689\n",
      "Training Epoch: 7 [30600/36450]\tLoss: 653.6065\n",
      "Training Epoch: 7 [30650/36450]\tLoss: 650.8499\n",
      "Training Epoch: 7 [30700/36450]\tLoss: 699.3558\n",
      "Training Epoch: 7 [30750/36450]\tLoss: 662.2218\n",
      "Training Epoch: 7 [30800/36450]\tLoss: 666.0134\n",
      "Training Epoch: 7 [30850/36450]\tLoss: 638.6926\n",
      "Training Epoch: 7 [30900/36450]\tLoss: 638.3383\n",
      "Training Epoch: 7 [30950/36450]\tLoss: 663.8920\n",
      "Training Epoch: 7 [31000/36450]\tLoss: 654.3762\n",
      "Training Epoch: 7 [31050/36450]\tLoss: 669.5354\n",
      "Training Epoch: 7 [31100/36450]\tLoss: 685.7272\n",
      "Training Epoch: 7 [31150/36450]\tLoss: 676.2916\n",
      "Training Epoch: 7 [31200/36450]\tLoss: 659.3010\n",
      "Training Epoch: 7 [31250/36450]\tLoss: 652.1472\n",
      "Training Epoch: 7 [31300/36450]\tLoss: 666.7972\n",
      "Training Epoch: 7 [31350/36450]\tLoss: 646.9935\n",
      "Training Epoch: 7 [31400/36450]\tLoss: 670.5450\n",
      "Training Epoch: 7 [31450/36450]\tLoss: 673.1157\n",
      "Training Epoch: 7 [31500/36450]\tLoss: 658.8680\n",
      "Training Epoch: 7 [31550/36450]\tLoss: 686.9249\n",
      "Training Epoch: 7 [31600/36450]\tLoss: 707.8726\n",
      "Training Epoch: 7 [31650/36450]\tLoss: 660.5735\n",
      "Training Epoch: 7 [31700/36450]\tLoss: 682.8525\n",
      "Training Epoch: 7 [31750/36450]\tLoss: 658.2548\n",
      "Training Epoch: 7 [31800/36450]\tLoss: 641.8273\n",
      "Training Epoch: 7 [31850/36450]\tLoss: 687.6586\n",
      "Training Epoch: 7 [31900/36450]\tLoss: 675.0737\n",
      "Training Epoch: 7 [31950/36450]\tLoss: 594.3677\n",
      "Training Epoch: 7 [32000/36450]\tLoss: 634.6705\n",
      "Training Epoch: 7 [32050/36450]\tLoss: 657.0467\n",
      "Training Epoch: 7 [32100/36450]\tLoss: 653.1291\n",
      "Training Epoch: 7 [32150/36450]\tLoss: 656.4592\n",
      "Training Epoch: 7 [32200/36450]\tLoss: 643.5245\n",
      "Training Epoch: 7 [32250/36450]\tLoss: 647.0931\n",
      "Training Epoch: 7 [32300/36450]\tLoss: 663.4807\n",
      "Training Epoch: 7 [32350/36450]\tLoss: 682.2926\n",
      "Training Epoch: 7 [32400/36450]\tLoss: 649.3124\n",
      "Training Epoch: 7 [32450/36450]\tLoss: 664.2756\n",
      "Training Epoch: 7 [32500/36450]\tLoss: 671.0153\n",
      "Training Epoch: 7 [32550/36450]\tLoss: 652.0629\n",
      "Training Epoch: 7 [32600/36450]\tLoss: 679.5007\n",
      "Training Epoch: 7 [32650/36450]\tLoss: 650.8746\n",
      "Training Epoch: 7 [32700/36450]\tLoss: 656.6371\n",
      "Training Epoch: 7 [32750/36450]\tLoss: 662.2148\n",
      "Training Epoch: 7 [32800/36450]\tLoss: 683.5275\n",
      "Training Epoch: 7 [32850/36450]\tLoss: 684.7759\n",
      "Training Epoch: 7 [32900/36450]\tLoss: 688.4281\n",
      "Training Epoch: 7 [32950/36450]\tLoss: 642.7347\n",
      "Training Epoch: 7 [33000/36450]\tLoss: 612.3853\n",
      "Training Epoch: 7 [33050/36450]\tLoss: 656.3845\n",
      "Training Epoch: 7 [33100/36450]\tLoss: 660.9237\n",
      "Training Epoch: 7 [33150/36450]\tLoss: 641.2253\n",
      "Training Epoch: 7 [33200/36450]\tLoss: 668.1454\n",
      "Training Epoch: 7 [33250/36450]\tLoss: 666.4301\n",
      "Training Epoch: 7 [33300/36450]\tLoss: 656.2432\n",
      "Training Epoch: 7 [33350/36450]\tLoss: 644.3270\n",
      "Training Epoch: 7 [33400/36450]\tLoss: 623.0241\n",
      "Training Epoch: 7 [33450/36450]\tLoss: 614.8899\n",
      "Training Epoch: 7 [33500/36450]\tLoss: 659.3452\n",
      "Training Epoch: 7 [33550/36450]\tLoss: 640.8415\n",
      "Training Epoch: 7 [33600/36450]\tLoss: 676.7286\n",
      "Training Epoch: 7 [33650/36450]\tLoss: 696.0685\n",
      "Training Epoch: 7 [33700/36450]\tLoss: 645.9855\n",
      "Training Epoch: 7 [33750/36450]\tLoss: 607.9241\n",
      "Training Epoch: 7 [33800/36450]\tLoss: 663.7470\n",
      "Training Epoch: 7 [33850/36450]\tLoss: 663.5288\n",
      "Training Epoch: 7 [33900/36450]\tLoss: 647.4675\n",
      "Training Epoch: 7 [33950/36450]\tLoss: 667.2820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [34000/36450]\tLoss: 659.2401\n",
      "Training Epoch: 7 [34050/36450]\tLoss: 650.1227\n",
      "Training Epoch: 7 [34100/36450]\tLoss: 665.9087\n",
      "Training Epoch: 7 [34150/36450]\tLoss: 643.9720\n",
      "Training Epoch: 7 [34200/36450]\tLoss: 687.0218\n",
      "Training Epoch: 7 [34250/36450]\tLoss: 659.4979\n",
      "Training Epoch: 7 [34300/36450]\tLoss: 646.2723\n",
      "Training Epoch: 7 [34350/36450]\tLoss: 661.5220\n",
      "Training Epoch: 7 [34400/36450]\tLoss: 621.2823\n",
      "Training Epoch: 7 [34450/36450]\tLoss: 638.9180\n",
      "Training Epoch: 7 [34500/36450]\tLoss: 643.6754\n",
      "Training Epoch: 7 [34550/36450]\tLoss: 662.7922\n",
      "Training Epoch: 7 [34600/36450]\tLoss: 670.9434\n",
      "Training Epoch: 7 [34650/36450]\tLoss: 722.9515\n",
      "Training Epoch: 7 [34700/36450]\tLoss: 643.2748\n",
      "Training Epoch: 7 [34750/36450]\tLoss: 647.5660\n",
      "Training Epoch: 7 [34800/36450]\tLoss: 685.7836\n",
      "Training Epoch: 7 [34850/36450]\tLoss: 668.2917\n",
      "Training Epoch: 7 [34900/36450]\tLoss: 652.0529\n",
      "Training Epoch: 7 [34950/36450]\tLoss: 629.3939\n",
      "Training Epoch: 7 [35000/36450]\tLoss: 656.5729\n",
      "Training Epoch: 7 [35050/36450]\tLoss: 689.9373\n",
      "Training Epoch: 7 [35100/36450]\tLoss: 662.1514\n",
      "Training Epoch: 7 [35150/36450]\tLoss: 631.3280\n",
      "Training Epoch: 7 [35200/36450]\tLoss: 590.0215\n",
      "Training Epoch: 7 [35250/36450]\tLoss: 637.6224\n",
      "Training Epoch: 7 [35300/36450]\tLoss: 626.1259\n",
      "Training Epoch: 7 [35350/36450]\tLoss: 689.1027\n",
      "Training Epoch: 7 [35400/36450]\tLoss: 639.8320\n",
      "Training Epoch: 7 [35450/36450]\tLoss: 672.8695\n",
      "Training Epoch: 7 [35500/36450]\tLoss: 626.3869\n",
      "Training Epoch: 7 [35550/36450]\tLoss: 646.9335\n",
      "Training Epoch: 7 [35600/36450]\tLoss: 700.6177\n",
      "Training Epoch: 7 [35650/36450]\tLoss: 692.6497\n",
      "Training Epoch: 7 [35700/36450]\tLoss: 683.6267\n",
      "Training Epoch: 7 [35750/36450]\tLoss: 676.5369\n",
      "Training Epoch: 7 [35800/36450]\tLoss: 662.4706\n",
      "Training Epoch: 7 [35850/36450]\tLoss: 720.3069\n",
      "Training Epoch: 7 [35900/36450]\tLoss: 719.9122\n",
      "Training Epoch: 7 [35950/36450]\tLoss: 710.9241\n",
      "Training Epoch: 7 [36000/36450]\tLoss: 716.5914\n",
      "Training Epoch: 7 [36050/36450]\tLoss: 686.1584\n",
      "Training Epoch: 7 [36100/36450]\tLoss: 700.6725\n",
      "Training Epoch: 7 [36150/36450]\tLoss: 622.1941\n",
      "Training Epoch: 7 [36200/36450]\tLoss: 672.1063\n",
      "Training Epoch: 7 [36250/36450]\tLoss: 697.3344\n",
      "Training Epoch: 7 [36300/36450]\tLoss: 695.3370\n",
      "Training Epoch: 7 [36350/36450]\tLoss: 683.5587\n",
      "Training Epoch: 7 [36400/36450]\tLoss: 677.6561\n",
      "Training Epoch: 7 [36450/36450]\tLoss: 718.6090\n",
      "Training Epoch: 7 [4050/4050]\tLoss: 336.7496\n",
      "Training Epoch: 8 [50/36450]\tLoss: 685.5812\n",
      "Training Epoch: 8 [100/36450]\tLoss: 637.3867\n",
      "Training Epoch: 8 [150/36450]\tLoss: 644.6412\n",
      "Training Epoch: 8 [200/36450]\tLoss: 631.9866\n",
      "Training Epoch: 8 [250/36450]\tLoss: 650.9268\n",
      "Training Epoch: 8 [300/36450]\tLoss: 674.0457\n",
      "Training Epoch: 8 [350/36450]\tLoss: 650.8058\n",
      "Training Epoch: 8 [400/36450]\tLoss: 647.1133\n",
      "Training Epoch: 8 [450/36450]\tLoss: 656.3484\n",
      "Training Epoch: 8 [500/36450]\tLoss: 624.6231\n",
      "Training Epoch: 8 [550/36450]\tLoss: 662.0047\n",
      "Training Epoch: 8 [600/36450]\tLoss: 673.1785\n",
      "Training Epoch: 8 [650/36450]\tLoss: 674.9244\n",
      "Training Epoch: 8 [700/36450]\tLoss: 629.2656\n",
      "Training Epoch: 8 [750/36450]\tLoss: 703.5030\n",
      "Training Epoch: 8 [800/36450]\tLoss: 684.3958\n",
      "Training Epoch: 8 [850/36450]\tLoss: 691.9843\n",
      "Training Epoch: 8 [900/36450]\tLoss: 670.5098\n",
      "Training Epoch: 8 [950/36450]\tLoss: 654.4654\n",
      "Training Epoch: 8 [1000/36450]\tLoss: 657.0985\n",
      "Training Epoch: 8 [1050/36450]\tLoss: 634.0106\n",
      "Training Epoch: 8 [1100/36450]\tLoss: 649.5844\n",
      "Training Epoch: 8 [1150/36450]\tLoss: 615.3533\n",
      "Training Epoch: 8 [1200/36450]\tLoss: 655.4425\n",
      "Training Epoch: 8 [1250/36450]\tLoss: 682.2722\n",
      "Training Epoch: 8 [1300/36450]\tLoss: 608.1689\n",
      "Training Epoch: 8 [1350/36450]\tLoss: 637.6177\n",
      "Training Epoch: 8 [1400/36450]\tLoss: 658.3054\n",
      "Training Epoch: 8 [1450/36450]\tLoss: 624.8862\n",
      "Training Epoch: 8 [1500/36450]\tLoss: 627.6061\n",
      "Training Epoch: 8 [1550/36450]\tLoss: 670.3351\n",
      "Training Epoch: 8 [1600/36450]\tLoss: 603.1539\n",
      "Training Epoch: 8 [1650/36450]\tLoss: 673.8292\n",
      "Training Epoch: 8 [1700/36450]\tLoss: 645.7107\n",
      "Training Epoch: 8 [1750/36450]\tLoss: 613.2516\n",
      "Training Epoch: 8 [1800/36450]\tLoss: 671.0433\n",
      "Training Epoch: 8 [1850/36450]\tLoss: 618.0356\n",
      "Training Epoch: 8 [1900/36450]\tLoss: 632.8624\n",
      "Training Epoch: 8 [1950/36450]\tLoss: 637.2030\n",
      "Training Epoch: 8 [2000/36450]\tLoss: 635.4131\n",
      "Training Epoch: 8 [2050/36450]\tLoss: 635.8160\n",
      "Training Epoch: 8 [2100/36450]\tLoss: 650.5403\n",
      "Training Epoch: 8 [2150/36450]\tLoss: 638.0046\n",
      "Training Epoch: 8 [2200/36450]\tLoss: 662.4753\n",
      "Training Epoch: 8 [2250/36450]\tLoss: 672.6771\n",
      "Training Epoch: 8 [2300/36450]\tLoss: 671.7128\n",
      "Training Epoch: 8 [2350/36450]\tLoss: 665.5134\n",
      "Training Epoch: 8 [2400/36450]\tLoss: 640.7356\n",
      "Training Epoch: 8 [2450/36450]\tLoss: 601.8773\n",
      "Training Epoch: 8 [2500/36450]\tLoss: 638.6756\n",
      "Training Epoch: 8 [2550/36450]\tLoss: 678.2197\n",
      "Training Epoch: 8 [2600/36450]\tLoss: 630.1859\n",
      "Training Epoch: 8 [2650/36450]\tLoss: 662.2304\n",
      "Training Epoch: 8 [2700/36450]\tLoss: 663.3501\n",
      "Training Epoch: 8 [2750/36450]\tLoss: 660.8044\n",
      "Training Epoch: 8 [2800/36450]\tLoss: 663.6291\n",
      "Training Epoch: 8 [2850/36450]\tLoss: 649.5684\n",
      "Training Epoch: 8 [2900/36450]\tLoss: 670.6871\n",
      "Training Epoch: 8 [2950/36450]\tLoss: 646.9244\n",
      "Training Epoch: 8 [3000/36450]\tLoss: 629.3475\n",
      "Training Epoch: 8 [3050/36450]\tLoss: 658.3209\n",
      "Training Epoch: 8 [3100/36450]\tLoss: 647.1577\n",
      "Training Epoch: 8 [3150/36450]\tLoss: 624.8766\n",
      "Training Epoch: 8 [3200/36450]\tLoss: 643.6607\n",
      "Training Epoch: 8 [3250/36450]\tLoss: 672.7067\n",
      "Training Epoch: 8 [3300/36450]\tLoss: 607.6429\n",
      "Training Epoch: 8 [3350/36450]\tLoss: 655.4741\n",
      "Training Epoch: 8 [3400/36450]\tLoss: 624.8705\n",
      "Training Epoch: 8 [3450/36450]\tLoss: 685.1666\n",
      "Training Epoch: 8 [3500/36450]\tLoss: 681.7436\n",
      "Training Epoch: 8 [3550/36450]\tLoss: 671.2065\n",
      "Training Epoch: 8 [3600/36450]\tLoss: 653.2073\n",
      "Training Epoch: 8 [3650/36450]\tLoss: 656.1051\n",
      "Training Epoch: 8 [3700/36450]\tLoss: 651.8613\n",
      "Training Epoch: 8 [3750/36450]\tLoss: 637.9917\n",
      "Training Epoch: 8 [3800/36450]\tLoss: 677.4808\n",
      "Training Epoch: 8 [3850/36450]\tLoss: 637.2844\n",
      "Training Epoch: 8 [3900/36450]\tLoss: 650.5821\n",
      "Training Epoch: 8 [3950/36450]\tLoss: 648.6266\n",
      "Training Epoch: 8 [4000/36450]\tLoss: 622.4083\n",
      "Training Epoch: 8 [4050/36450]\tLoss: 634.8829\n",
      "Training Epoch: 8 [4100/36450]\tLoss: 613.0027\n",
      "Training Epoch: 8 [4150/36450]\tLoss: 626.2999\n",
      "Training Epoch: 8 [4200/36450]\tLoss: 700.9865\n",
      "Training Epoch: 8 [4250/36450]\tLoss: 625.4488\n",
      "Training Epoch: 8 [4300/36450]\tLoss: 655.5895\n",
      "Training Epoch: 8 [4350/36450]\tLoss: 649.8980\n",
      "Training Epoch: 8 [4400/36450]\tLoss: 642.1623\n",
      "Training Epoch: 8 [4450/36450]\tLoss: 646.1083\n",
      "Training Epoch: 8 [4500/36450]\tLoss: 650.1697\n",
      "Training Epoch: 8 [4550/36450]\tLoss: 685.8043\n",
      "Training Epoch: 8 [4600/36450]\tLoss: 633.1274\n",
      "Training Epoch: 8 [4650/36450]\tLoss: 685.1422\n",
      "Training Epoch: 8 [4700/36450]\tLoss: 632.8575\n",
      "Training Epoch: 8 [4750/36450]\tLoss: 633.5358\n",
      "Training Epoch: 8 [4800/36450]\tLoss: 611.2374\n",
      "Training Epoch: 8 [4850/36450]\tLoss: 647.5700\n",
      "Training Epoch: 8 [4900/36450]\tLoss: 650.9436\n",
      "Training Epoch: 8 [4950/36450]\tLoss: 661.5254\n",
      "Training Epoch: 8 [5000/36450]\tLoss: 694.8380\n",
      "Training Epoch: 8 [5050/36450]\tLoss: 635.2831\n",
      "Training Epoch: 8 [5100/36450]\tLoss: 644.5874\n",
      "Training Epoch: 8 [5150/36450]\tLoss: 625.4172\n",
      "Training Epoch: 8 [5200/36450]\tLoss: 616.5278\n",
      "Training Epoch: 8 [5250/36450]\tLoss: 674.8325\n",
      "Training Epoch: 8 [5300/36450]\tLoss: 660.4593\n",
      "Training Epoch: 8 [5350/36450]\tLoss: 639.8962\n",
      "Training Epoch: 8 [5400/36450]\tLoss: 657.1705\n",
      "Training Epoch: 8 [5450/36450]\tLoss: 624.7517\n",
      "Training Epoch: 8 [5500/36450]\tLoss: 643.8405\n",
      "Training Epoch: 8 [5550/36450]\tLoss: 654.0153\n",
      "Training Epoch: 8 [5600/36450]\tLoss: 701.4300\n",
      "Training Epoch: 8 [5650/36450]\tLoss: 670.5989\n",
      "Training Epoch: 8 [5700/36450]\tLoss: 619.3892\n",
      "Training Epoch: 8 [5750/36450]\tLoss: 655.8810\n",
      "Training Epoch: 8 [5800/36450]\tLoss: 627.4124\n",
      "Training Epoch: 8 [5850/36450]\tLoss: 659.8494\n",
      "Training Epoch: 8 [5900/36450]\tLoss: 646.4274\n",
      "Training Epoch: 8 [5950/36450]\tLoss: 683.6375\n",
      "Training Epoch: 8 [6000/36450]\tLoss: 655.2737\n",
      "Training Epoch: 8 [6050/36450]\tLoss: 644.6736\n",
      "Training Epoch: 8 [6100/36450]\tLoss: 649.0332\n",
      "Training Epoch: 8 [6150/36450]\tLoss: 672.5644\n",
      "Training Epoch: 8 [6200/36450]\tLoss: 629.5677\n",
      "Training Epoch: 8 [6250/36450]\tLoss: 676.2428\n",
      "Training Epoch: 8 [6300/36450]\tLoss: 636.3849\n",
      "Training Epoch: 8 [6350/36450]\tLoss: 621.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [6400/36450]\tLoss: 646.8341\n",
      "Training Epoch: 8 [6450/36450]\tLoss: 641.8521\n",
      "Training Epoch: 8 [6500/36450]\tLoss: 634.5468\n",
      "Training Epoch: 8 [6550/36450]\tLoss: 648.6284\n",
      "Training Epoch: 8 [6600/36450]\tLoss: 628.8752\n",
      "Training Epoch: 8 [6650/36450]\tLoss: 650.8449\n",
      "Training Epoch: 8 [6700/36450]\tLoss: 654.8428\n",
      "Training Epoch: 8 [6750/36450]\tLoss: 618.3705\n",
      "Training Epoch: 8 [6800/36450]\tLoss: 671.7051\n",
      "Training Epoch: 8 [6850/36450]\tLoss: 633.1572\n",
      "Training Epoch: 8 [6900/36450]\tLoss: 647.5024\n",
      "Training Epoch: 8 [6950/36450]\tLoss: 656.8884\n",
      "Training Epoch: 8 [7000/36450]\tLoss: 626.9464\n",
      "Training Epoch: 8 [7050/36450]\tLoss: 617.1961\n",
      "Training Epoch: 8 [7100/36450]\tLoss: 671.5761\n",
      "Training Epoch: 8 [7150/36450]\tLoss: 640.6870\n",
      "Training Epoch: 8 [7200/36450]\tLoss: 685.8517\n",
      "Training Epoch: 8 [7250/36450]\tLoss: 651.5242\n",
      "Training Epoch: 8 [7300/36450]\tLoss: 657.6652\n",
      "Training Epoch: 8 [7350/36450]\tLoss: 642.1324\n",
      "Training Epoch: 8 [7400/36450]\tLoss: 634.8239\n",
      "Training Epoch: 8 [7450/36450]\tLoss: 642.3921\n",
      "Training Epoch: 8 [7500/36450]\tLoss: 653.5887\n",
      "Training Epoch: 8 [7550/36450]\tLoss: 674.1899\n",
      "Training Epoch: 8 [7600/36450]\tLoss: 666.3319\n",
      "Training Epoch: 8 [7650/36450]\tLoss: 658.4092\n",
      "Training Epoch: 8 [7700/36450]\tLoss: 635.5808\n",
      "Training Epoch: 8 [7750/36450]\tLoss: 641.5355\n",
      "Training Epoch: 8 [7800/36450]\tLoss: 662.0693\n",
      "Training Epoch: 8 [7850/36450]\tLoss: 682.7278\n",
      "Training Epoch: 8 [7900/36450]\tLoss: 686.3549\n",
      "Training Epoch: 8 [7950/36450]\tLoss: 703.9880\n",
      "Training Epoch: 8 [8000/36450]\tLoss: 762.7872\n",
      "Training Epoch: 8 [8050/36450]\tLoss: 690.3208\n",
      "Training Epoch: 8 [8100/36450]\tLoss: 722.6771\n",
      "Training Epoch: 8 [8150/36450]\tLoss: 758.5328\n",
      "Training Epoch: 8 [8200/36450]\tLoss: 688.7316\n",
      "Training Epoch: 8 [8250/36450]\tLoss: 722.1749\n",
      "Training Epoch: 8 [8300/36450]\tLoss: 693.9722\n",
      "Training Epoch: 8 [8350/36450]\tLoss: 685.1425\n",
      "Training Epoch: 8 [8400/36450]\tLoss: 641.1302\n",
      "Training Epoch: 8 [8450/36450]\tLoss: 663.6618\n",
      "Training Epoch: 8 [8500/36450]\tLoss: 682.6069\n",
      "Training Epoch: 8 [8550/36450]\tLoss: 682.3984\n",
      "Training Epoch: 8 [8600/36450]\tLoss: 643.8797\n",
      "Training Epoch: 8 [8650/36450]\tLoss: 627.7945\n",
      "Training Epoch: 8 [8700/36450]\tLoss: 642.3076\n",
      "Training Epoch: 8 [8750/36450]\tLoss: 645.4041\n",
      "Training Epoch: 8 [8800/36450]\tLoss: 682.1252\n",
      "Training Epoch: 8 [8850/36450]\tLoss: 648.1086\n",
      "Training Epoch: 8 [8900/36450]\tLoss: 669.9689\n",
      "Training Epoch: 8 [8950/36450]\tLoss: 707.8964\n",
      "Training Epoch: 8 [9000/36450]\tLoss: 651.8231\n",
      "Training Epoch: 8 [9050/36450]\tLoss: 675.7413\n",
      "Training Epoch: 8 [9100/36450]\tLoss: 619.0278\n",
      "Training Epoch: 8 [9150/36450]\tLoss: 651.5662\n",
      "Training Epoch: 8 [9200/36450]\tLoss: 632.2781\n",
      "Training Epoch: 8 [9250/36450]\tLoss: 638.4451\n",
      "Training Epoch: 8 [9300/36450]\tLoss: 675.7278\n",
      "Training Epoch: 8 [9350/36450]\tLoss: 633.9819\n",
      "Training Epoch: 8 [9400/36450]\tLoss: 629.8597\n",
      "Training Epoch: 8 [9450/36450]\tLoss: 645.0687\n",
      "Training Epoch: 8 [9500/36450]\tLoss: 654.4305\n",
      "Training Epoch: 8 [9550/36450]\tLoss: 635.1443\n",
      "Training Epoch: 8 [9600/36450]\tLoss: 651.0444\n",
      "Training Epoch: 8 [9650/36450]\tLoss: 621.6533\n",
      "Training Epoch: 8 [9700/36450]\tLoss: 618.0203\n",
      "Training Epoch: 8 [9750/36450]\tLoss: 623.9926\n",
      "Training Epoch: 8 [9800/36450]\tLoss: 643.4080\n",
      "Training Epoch: 8 [9850/36450]\tLoss: 670.4333\n",
      "Training Epoch: 8 [9900/36450]\tLoss: 685.2762\n",
      "Training Epoch: 8 [9950/36450]\tLoss: 671.4035\n",
      "Training Epoch: 8 [10000/36450]\tLoss: 635.8005\n",
      "Training Epoch: 8 [10050/36450]\tLoss: 606.6446\n",
      "Training Epoch: 8 [10100/36450]\tLoss: 632.8848\n",
      "Training Epoch: 8 [10150/36450]\tLoss: 666.3158\n",
      "Training Epoch: 8 [10200/36450]\tLoss: 637.8079\n",
      "Training Epoch: 8 [10250/36450]\tLoss: 682.9046\n",
      "Training Epoch: 8 [10300/36450]\tLoss: 612.8140\n",
      "Training Epoch: 8 [10350/36450]\tLoss: 621.5461\n",
      "Training Epoch: 8 [10400/36450]\tLoss: 641.3306\n",
      "Training Epoch: 8 [10450/36450]\tLoss: 661.6380\n",
      "Training Epoch: 8 [10500/36450]\tLoss: 648.7496\n",
      "Training Epoch: 8 [10550/36450]\tLoss: 657.1496\n",
      "Training Epoch: 8 [10600/36450]\tLoss: 664.0445\n",
      "Training Epoch: 8 [10650/36450]\tLoss: 661.1256\n",
      "Training Epoch: 8 [10700/36450]\tLoss: 610.7917\n",
      "Training Epoch: 8 [10750/36450]\tLoss: 640.6427\n",
      "Training Epoch: 8 [10800/36450]\tLoss: 658.6407\n",
      "Training Epoch: 8 [10850/36450]\tLoss: 593.8753\n",
      "Training Epoch: 8 [10900/36450]\tLoss: 629.5377\n",
      "Training Epoch: 8 [10950/36450]\tLoss: 640.4810\n",
      "Training Epoch: 8 [11000/36450]\tLoss: 643.9446\n",
      "Training Epoch: 8 [11050/36450]\tLoss: 646.3361\n",
      "Training Epoch: 8 [11100/36450]\tLoss: 647.2371\n",
      "Training Epoch: 8 [11150/36450]\tLoss: 610.3176\n",
      "Training Epoch: 8 [11200/36450]\tLoss: 651.6150\n",
      "Training Epoch: 8 [11250/36450]\tLoss: 584.7744\n",
      "Training Epoch: 8 [11300/36450]\tLoss: 622.8964\n",
      "Training Epoch: 8 [11350/36450]\tLoss: 626.3669\n",
      "Training Epoch: 8 [11400/36450]\tLoss: 657.9592\n",
      "Training Epoch: 8 [11450/36450]\tLoss: 678.9243\n",
      "Training Epoch: 8 [11500/36450]\tLoss: 603.7739\n",
      "Training Epoch: 8 [11550/36450]\tLoss: 646.8527\n",
      "Training Epoch: 8 [11600/36450]\tLoss: 624.0168\n",
      "Training Epoch: 8 [11650/36450]\tLoss: 667.4038\n",
      "Training Epoch: 8 [11700/36450]\tLoss: 637.8990\n",
      "Training Epoch: 8 [11750/36450]\tLoss: 653.7565\n",
      "Training Epoch: 8 [11800/36450]\tLoss: 658.4838\n",
      "Training Epoch: 8 [11850/36450]\tLoss: 656.6746\n",
      "Training Epoch: 8 [11900/36450]\tLoss: 634.1912\n",
      "Training Epoch: 8 [11950/36450]\tLoss: 645.6924\n",
      "Training Epoch: 8 [12000/36450]\tLoss: 597.7545\n",
      "Training Epoch: 8 [12050/36450]\tLoss: 664.1969\n",
      "Training Epoch: 8 [12100/36450]\tLoss: 644.3367\n",
      "Training Epoch: 8 [12150/36450]\tLoss: 582.6877\n",
      "Training Epoch: 8 [12200/36450]\tLoss: 664.4562\n",
      "Training Epoch: 8 [12250/36450]\tLoss: 657.7130\n",
      "Training Epoch: 8 [12300/36450]\tLoss: 642.3992\n",
      "Training Epoch: 8 [12350/36450]\tLoss: 612.9191\n",
      "Training Epoch: 8 [12400/36450]\tLoss: 630.5242\n",
      "Training Epoch: 8 [12450/36450]\tLoss: 674.7620\n",
      "Training Epoch: 8 [12500/36450]\tLoss: 641.1551\n",
      "Training Epoch: 8 [12550/36450]\tLoss: 607.2916\n",
      "Training Epoch: 8 [12600/36450]\tLoss: 634.9470\n",
      "Training Epoch: 8 [12650/36450]\tLoss: 622.6126\n",
      "Training Epoch: 8 [12700/36450]\tLoss: 668.0934\n",
      "Training Epoch: 8 [12750/36450]\tLoss: 639.0425\n",
      "Training Epoch: 8 [12800/36450]\tLoss: 589.4390\n",
      "Training Epoch: 8 [12850/36450]\tLoss: 637.1871\n",
      "Training Epoch: 8 [12900/36450]\tLoss: 616.5113\n",
      "Training Epoch: 8 [12950/36450]\tLoss: 632.7883\n",
      "Training Epoch: 8 [13000/36450]\tLoss: 623.7379\n",
      "Training Epoch: 8 [13050/36450]\tLoss: 641.3229\n",
      "Training Epoch: 8 [13100/36450]\tLoss: 607.2902\n",
      "Training Epoch: 8 [13150/36450]\tLoss: 632.5952\n",
      "Training Epoch: 8 [13200/36450]\tLoss: 630.5446\n",
      "Training Epoch: 8 [13250/36450]\tLoss: 606.4855\n",
      "Training Epoch: 8 [13300/36450]\tLoss: 662.5250\n",
      "Training Epoch: 8 [13350/36450]\tLoss: 654.4762\n",
      "Training Epoch: 8 [13400/36450]\tLoss: 618.8468\n",
      "Training Epoch: 8 [13450/36450]\tLoss: 644.3721\n",
      "Training Epoch: 8 [13500/36450]\tLoss: 632.0430\n",
      "Training Epoch: 8 [13550/36450]\tLoss: 606.8036\n",
      "Training Epoch: 8 [13600/36450]\tLoss: 631.9980\n",
      "Training Epoch: 8 [13650/36450]\tLoss: 646.0247\n",
      "Training Epoch: 8 [13700/36450]\tLoss: 624.4795\n",
      "Training Epoch: 8 [13750/36450]\tLoss: 646.0859\n",
      "Training Epoch: 8 [13800/36450]\tLoss: 658.3631\n",
      "Training Epoch: 8 [13850/36450]\tLoss: 631.5557\n",
      "Training Epoch: 8 [13900/36450]\tLoss: 659.1924\n",
      "Training Epoch: 8 [13950/36450]\tLoss: 603.0198\n",
      "Training Epoch: 8 [14000/36450]\tLoss: 650.2781\n",
      "Training Epoch: 8 [14050/36450]\tLoss: 647.6058\n",
      "Training Epoch: 8 [14100/36450]\tLoss: 652.4995\n",
      "Training Epoch: 8 [14150/36450]\tLoss: 702.1296\n",
      "Training Epoch: 8 [14200/36450]\tLoss: 660.3531\n",
      "Training Epoch: 8 [14250/36450]\tLoss: 611.2764\n",
      "Training Epoch: 8 [14300/36450]\tLoss: 617.2845\n",
      "Training Epoch: 8 [14350/36450]\tLoss: 641.4208\n",
      "Training Epoch: 8 [14400/36450]\tLoss: 652.9404\n",
      "Training Epoch: 8 [14450/36450]\tLoss: 667.7960\n",
      "Training Epoch: 8 [14500/36450]\tLoss: 627.8193\n",
      "Training Epoch: 8 [14550/36450]\tLoss: 647.9740\n",
      "Training Epoch: 8 [14600/36450]\tLoss: 652.5702\n",
      "Training Epoch: 8 [14650/36450]\tLoss: 627.8149\n",
      "Training Epoch: 8 [14700/36450]\tLoss: 619.5768\n",
      "Training Epoch: 8 [14750/36450]\tLoss: 643.9626\n",
      "Training Epoch: 8 [14800/36450]\tLoss: 639.3947\n",
      "Training Epoch: 8 [14850/36450]\tLoss: 629.4235\n",
      "Training Epoch: 8 [14900/36450]\tLoss: 652.6252\n",
      "Training Epoch: 8 [14950/36450]\tLoss: 638.5148\n",
      "Training Epoch: 8 [15000/36450]\tLoss: 633.2673\n",
      "Training Epoch: 8 [15050/36450]\tLoss: 633.9471\n",
      "Training Epoch: 8 [15100/36450]\tLoss: 616.3071\n",
      "Training Epoch: 8 [15150/36450]\tLoss: 646.9948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [15200/36450]\tLoss: 674.9243\n",
      "Training Epoch: 8 [15250/36450]\tLoss: 619.2406\n",
      "Training Epoch: 8 [15300/36450]\tLoss: 676.8963\n",
      "Training Epoch: 8 [15350/36450]\tLoss: 660.2762\n",
      "Training Epoch: 8 [15400/36450]\tLoss: 650.3138\n",
      "Training Epoch: 8 [15450/36450]\tLoss: 648.4493\n",
      "Training Epoch: 8 [15500/36450]\tLoss: 622.2208\n",
      "Training Epoch: 8 [15550/36450]\tLoss: 626.1272\n",
      "Training Epoch: 8 [15600/36450]\tLoss: 586.9991\n",
      "Training Epoch: 8 [15650/36450]\tLoss: 636.9252\n",
      "Training Epoch: 8 [15700/36450]\tLoss: 634.2014\n",
      "Training Epoch: 8 [15750/36450]\tLoss: 642.5918\n",
      "Training Epoch: 8 [15800/36450]\tLoss: 629.4043\n",
      "Training Epoch: 8 [15850/36450]\tLoss: 610.1857\n",
      "Training Epoch: 8 [15900/36450]\tLoss: 614.6600\n",
      "Training Epoch: 8 [15950/36450]\tLoss: 656.5928\n",
      "Training Epoch: 8 [16000/36450]\tLoss: 658.8321\n",
      "Training Epoch: 8 [16050/36450]\tLoss: 643.0621\n",
      "Training Epoch: 8 [16100/36450]\tLoss: 635.7125\n",
      "Training Epoch: 8 [16150/36450]\tLoss: 659.7694\n",
      "Training Epoch: 8 [16200/36450]\tLoss: 615.3902\n",
      "Training Epoch: 8 [16250/36450]\tLoss: 656.3438\n",
      "Training Epoch: 8 [16300/36450]\tLoss: 692.8972\n",
      "Training Epoch: 8 [16350/36450]\tLoss: 626.0228\n",
      "Training Epoch: 8 [16400/36450]\tLoss: 604.5323\n",
      "Training Epoch: 8 [16450/36450]\tLoss: 610.3402\n",
      "Training Epoch: 8 [16500/36450]\tLoss: 637.2614\n",
      "Training Epoch: 8 [16550/36450]\tLoss: 632.2413\n",
      "Training Epoch: 8 [16600/36450]\tLoss: 608.6823\n",
      "Training Epoch: 8 [16650/36450]\tLoss: 611.9023\n",
      "Training Epoch: 8 [16700/36450]\tLoss: 637.6036\n",
      "Training Epoch: 8 [16750/36450]\tLoss: 629.4078\n",
      "Training Epoch: 8 [16800/36450]\tLoss: 631.0262\n",
      "Training Epoch: 8 [16850/36450]\tLoss: 608.0977\n",
      "Training Epoch: 8 [16900/36450]\tLoss: 605.4848\n",
      "Training Epoch: 8 [16950/36450]\tLoss: 656.7450\n",
      "Training Epoch: 8 [17000/36450]\tLoss: 650.1399\n",
      "Training Epoch: 8 [17050/36450]\tLoss: 629.1506\n",
      "Training Epoch: 8 [17100/36450]\tLoss: 619.3876\n",
      "Training Epoch: 8 [17150/36450]\tLoss: 652.5911\n",
      "Training Epoch: 8 [17200/36450]\tLoss: 657.5001\n",
      "Training Epoch: 8 [17250/36450]\tLoss: 659.5532\n",
      "Training Epoch: 8 [17300/36450]\tLoss: 651.8348\n",
      "Training Epoch: 8 [17350/36450]\tLoss: 634.3334\n",
      "Training Epoch: 8 [17400/36450]\tLoss: 652.3278\n",
      "Training Epoch: 8 [17450/36450]\tLoss: 655.8390\n",
      "Training Epoch: 8 [17500/36450]\tLoss: 664.0969\n",
      "Training Epoch: 8 [17550/36450]\tLoss: 613.8337\n",
      "Training Epoch: 8 [17600/36450]\tLoss: 658.2855\n",
      "Training Epoch: 8 [17650/36450]\tLoss: 666.1235\n",
      "Training Epoch: 8 [17700/36450]\tLoss: 611.3969\n",
      "Training Epoch: 8 [17750/36450]\tLoss: 628.2398\n",
      "Training Epoch: 8 [17800/36450]\tLoss: 626.2589\n",
      "Training Epoch: 8 [17850/36450]\tLoss: 621.6638\n",
      "Training Epoch: 8 [17900/36450]\tLoss: 602.9807\n",
      "Training Epoch: 8 [17950/36450]\tLoss: 642.0111\n",
      "Training Epoch: 8 [18000/36450]\tLoss: 655.4621\n",
      "Training Epoch: 8 [18050/36450]\tLoss: 664.3754\n",
      "Training Epoch: 8 [18100/36450]\tLoss: 629.1478\n",
      "Training Epoch: 8 [18150/36450]\tLoss: 636.4786\n",
      "Training Epoch: 8 [18200/36450]\tLoss: 635.1482\n",
      "Training Epoch: 8 [18250/36450]\tLoss: 656.4189\n",
      "Training Epoch: 8 [18300/36450]\tLoss: 626.9918\n",
      "Training Epoch: 8 [18350/36450]\tLoss: 660.4202\n",
      "Training Epoch: 8 [18400/36450]\tLoss: 616.4128\n",
      "Training Epoch: 8 [18450/36450]\tLoss: 628.5658\n",
      "Training Epoch: 8 [18500/36450]\tLoss: 598.2339\n",
      "Training Epoch: 8 [18550/36450]\tLoss: 649.8099\n",
      "Training Epoch: 8 [18600/36450]\tLoss: 638.2925\n",
      "Training Epoch: 8 [18650/36450]\tLoss: 634.3444\n",
      "Training Epoch: 8 [18700/36450]\tLoss: 635.4642\n",
      "Training Epoch: 8 [18750/36450]\tLoss: 658.1190\n",
      "Training Epoch: 8 [18800/36450]\tLoss: 655.7013\n",
      "Training Epoch: 8 [18850/36450]\tLoss: 657.2081\n",
      "Training Epoch: 8 [18900/36450]\tLoss: 659.9447\n",
      "Training Epoch: 8 [18950/36450]\tLoss: 627.4622\n",
      "Training Epoch: 8 [19000/36450]\tLoss: 681.3832\n",
      "Training Epoch: 8 [19050/36450]\tLoss: 623.4086\n",
      "Training Epoch: 8 [19100/36450]\tLoss: 639.9467\n",
      "Training Epoch: 8 [19150/36450]\tLoss: 668.7336\n",
      "Training Epoch: 8 [19200/36450]\tLoss: 624.9467\n",
      "Training Epoch: 8 [19250/36450]\tLoss: 591.5855\n",
      "Training Epoch: 8 [19300/36450]\tLoss: 682.8885\n",
      "Training Epoch: 8 [19350/36450]\tLoss: 657.5309\n",
      "Training Epoch: 8 [19400/36450]\tLoss: 643.4617\n",
      "Training Epoch: 8 [19450/36450]\tLoss: 698.8856\n",
      "Training Epoch: 8 [19500/36450]\tLoss: 646.6332\n",
      "Training Epoch: 8 [19550/36450]\tLoss: 610.7544\n",
      "Training Epoch: 8 [19600/36450]\tLoss: 645.8265\n",
      "Training Epoch: 8 [19650/36450]\tLoss: 617.4421\n",
      "Training Epoch: 8 [19700/36450]\tLoss: 652.1841\n",
      "Training Epoch: 8 [19750/36450]\tLoss: 650.0428\n",
      "Training Epoch: 8 [19800/36450]\tLoss: 639.2930\n",
      "Training Epoch: 8 [19850/36450]\tLoss: 645.4052\n",
      "Training Epoch: 8 [19900/36450]\tLoss: 631.2652\n",
      "Training Epoch: 8 [19950/36450]\tLoss: 676.5936\n",
      "Training Epoch: 8 [20000/36450]\tLoss: 662.0278\n",
      "Training Epoch: 8 [20050/36450]\tLoss: 640.8156\n",
      "Training Epoch: 8 [20100/36450]\tLoss: 666.6423\n",
      "Training Epoch: 8 [20150/36450]\tLoss: 650.0803\n",
      "Training Epoch: 8 [20200/36450]\tLoss: 651.1299\n",
      "Training Epoch: 8 [20250/36450]\tLoss: 667.7181\n",
      "Training Epoch: 8 [20300/36450]\tLoss: 694.9554\n",
      "Training Epoch: 8 [20350/36450]\tLoss: 657.3469\n",
      "Training Epoch: 8 [20400/36450]\tLoss: 736.8334\n",
      "Training Epoch: 8 [20450/36450]\tLoss: 659.0618\n",
      "Training Epoch: 8 [20500/36450]\tLoss: 684.3303\n",
      "Training Epoch: 8 [20550/36450]\tLoss: 639.7201\n",
      "Training Epoch: 8 [20600/36450]\tLoss: 649.2350\n",
      "Training Epoch: 8 [20650/36450]\tLoss: 610.8579\n",
      "Training Epoch: 8 [20700/36450]\tLoss: 655.9284\n",
      "Training Epoch: 8 [20750/36450]\tLoss: 652.1200\n",
      "Training Epoch: 8 [20800/36450]\tLoss: 666.8289\n",
      "Training Epoch: 8 [20850/36450]\tLoss: 608.5110\n",
      "Training Epoch: 8 [20900/36450]\tLoss: 642.3094\n",
      "Training Epoch: 8 [20950/36450]\tLoss: 659.7943\n",
      "Training Epoch: 8 [21000/36450]\tLoss: 633.4075\n",
      "Training Epoch: 8 [21050/36450]\tLoss: 616.9634\n",
      "Training Epoch: 8 [21100/36450]\tLoss: 625.4515\n",
      "Training Epoch: 8 [21150/36450]\tLoss: 592.8651\n",
      "Training Epoch: 8 [21200/36450]\tLoss: 632.3407\n",
      "Training Epoch: 8 [21250/36450]\tLoss: 677.6942\n",
      "Training Epoch: 8 [21300/36450]\tLoss: 629.3975\n",
      "Training Epoch: 8 [21350/36450]\tLoss: 654.5757\n",
      "Training Epoch: 8 [21400/36450]\tLoss: 655.5227\n",
      "Training Epoch: 8 [21450/36450]\tLoss: 603.9634\n",
      "Training Epoch: 8 [21500/36450]\tLoss: 647.1712\n",
      "Training Epoch: 8 [21550/36450]\tLoss: 629.8212\n",
      "Training Epoch: 8 [21600/36450]\tLoss: 625.3281\n",
      "Training Epoch: 8 [21650/36450]\tLoss: 655.9722\n",
      "Training Epoch: 8 [21700/36450]\tLoss: 641.7253\n",
      "Training Epoch: 8 [21750/36450]\tLoss: 610.8614\n",
      "Training Epoch: 8 [21800/36450]\tLoss: 623.9113\n",
      "Training Epoch: 8 [21850/36450]\tLoss: 634.2072\n",
      "Training Epoch: 8 [21900/36450]\tLoss: 645.3305\n",
      "Training Epoch: 8 [21950/36450]\tLoss: 612.6335\n",
      "Training Epoch: 8 [22000/36450]\tLoss: 607.5488\n",
      "Training Epoch: 8 [22050/36450]\tLoss: 655.5511\n",
      "Training Epoch: 8 [22100/36450]\tLoss: 614.4706\n",
      "Training Epoch: 8 [22150/36450]\tLoss: 622.5225\n",
      "Training Epoch: 8 [22200/36450]\tLoss: 604.7607\n",
      "Training Epoch: 8 [22250/36450]\tLoss: 620.6541\n",
      "Training Epoch: 8 [22300/36450]\tLoss: 587.0233\n",
      "Training Epoch: 8 [22350/36450]\tLoss: 599.0903\n",
      "Training Epoch: 8 [22400/36450]\tLoss: 649.8943\n",
      "Training Epoch: 8 [22450/36450]\tLoss: 574.0129\n",
      "Training Epoch: 8 [22500/36450]\tLoss: 618.5219\n",
      "Training Epoch: 8 [22550/36450]\tLoss: 616.1262\n",
      "Training Epoch: 8 [22600/36450]\tLoss: 613.0916\n",
      "Training Epoch: 8 [22650/36450]\tLoss: 633.2461\n",
      "Training Epoch: 8 [22700/36450]\tLoss: 651.6718\n",
      "Training Epoch: 8 [22750/36450]\tLoss: 653.6945\n",
      "Training Epoch: 8 [22800/36450]\tLoss: 631.5823\n",
      "Training Epoch: 8 [22850/36450]\tLoss: 603.8130\n",
      "Training Epoch: 8 [22900/36450]\tLoss: 630.9650\n",
      "Training Epoch: 8 [22950/36450]\tLoss: 636.1788\n",
      "Training Epoch: 8 [23000/36450]\tLoss: 574.4191\n",
      "Training Epoch: 8 [23050/36450]\tLoss: 576.8644\n",
      "Training Epoch: 8 [23100/36450]\tLoss: 650.9374\n",
      "Training Epoch: 8 [23150/36450]\tLoss: 620.1951\n",
      "Training Epoch: 8 [23200/36450]\tLoss: 653.8015\n",
      "Training Epoch: 8 [23250/36450]\tLoss: 659.5209\n",
      "Training Epoch: 8 [23300/36450]\tLoss: 644.2789\n",
      "Training Epoch: 8 [23350/36450]\tLoss: 648.8173\n",
      "Training Epoch: 8 [23400/36450]\tLoss: 656.2246\n",
      "Training Epoch: 8 [23450/36450]\tLoss: 651.4097\n",
      "Training Epoch: 8 [23500/36450]\tLoss: 615.7570\n",
      "Training Epoch: 8 [23550/36450]\tLoss: 592.7526\n",
      "Training Epoch: 8 [23600/36450]\tLoss: 555.5771\n",
      "Training Epoch: 8 [23650/36450]\tLoss: 641.7487\n",
      "Training Epoch: 8 [23700/36450]\tLoss: 635.0914\n",
      "Training Epoch: 8 [23750/36450]\tLoss: 609.8800\n",
      "Training Epoch: 8 [23800/36450]\tLoss: 568.1642\n",
      "Training Epoch: 8 [23850/36450]\tLoss: 619.8500\n",
      "Training Epoch: 8 [23900/36450]\tLoss: 672.2280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [23950/36450]\tLoss: 602.1426\n",
      "Training Epoch: 8 [24000/36450]\tLoss: 637.6880\n",
      "Training Epoch: 8 [24050/36450]\tLoss: 618.6369\n",
      "Training Epoch: 8 [24100/36450]\tLoss: 632.7070\n",
      "Training Epoch: 8 [24150/36450]\tLoss: 630.7476\n",
      "Training Epoch: 8 [24200/36450]\tLoss: 635.7402\n",
      "Training Epoch: 8 [24250/36450]\tLoss: 666.6317\n",
      "Training Epoch: 8 [24300/36450]\tLoss: 624.7716\n",
      "Training Epoch: 8 [24350/36450]\tLoss: 618.6053\n",
      "Training Epoch: 8 [24400/36450]\tLoss: 655.8547\n",
      "Training Epoch: 8 [24450/36450]\tLoss: 614.8309\n",
      "Training Epoch: 8 [24500/36450]\tLoss: 594.5507\n",
      "Training Epoch: 8 [24550/36450]\tLoss: 603.4728\n",
      "Training Epoch: 8 [24600/36450]\tLoss: 644.3295\n",
      "Training Epoch: 8 [24650/36450]\tLoss: 636.0729\n",
      "Training Epoch: 8 [24700/36450]\tLoss: 648.2377\n",
      "Training Epoch: 8 [24750/36450]\tLoss: 609.0845\n",
      "Training Epoch: 8 [24800/36450]\tLoss: 658.2948\n",
      "Training Epoch: 8 [24850/36450]\tLoss: 618.6129\n",
      "Training Epoch: 8 [24900/36450]\tLoss: 623.4286\n",
      "Training Epoch: 8 [24950/36450]\tLoss: 615.0081\n",
      "Training Epoch: 8 [25000/36450]\tLoss: 592.5999\n",
      "Training Epoch: 8 [25050/36450]\tLoss: 631.4198\n",
      "Training Epoch: 8 [25100/36450]\tLoss: 625.3254\n",
      "Training Epoch: 8 [25150/36450]\tLoss: 591.6953\n",
      "Training Epoch: 8 [25200/36450]\tLoss: 622.1371\n",
      "Training Epoch: 8 [25250/36450]\tLoss: 622.7886\n",
      "Training Epoch: 8 [25300/36450]\tLoss: 615.9003\n",
      "Training Epoch: 8 [25350/36450]\tLoss: 639.3308\n",
      "Training Epoch: 8 [25400/36450]\tLoss: 639.8140\n",
      "Training Epoch: 8 [25450/36450]\tLoss: 660.2128\n",
      "Training Epoch: 8 [25500/36450]\tLoss: 616.7386\n",
      "Training Epoch: 8 [25550/36450]\tLoss: 628.3348\n",
      "Training Epoch: 8 [25600/36450]\tLoss: 644.3275\n",
      "Training Epoch: 8 [25650/36450]\tLoss: 672.0975\n",
      "Training Epoch: 8 [25700/36450]\tLoss: 652.2023\n",
      "Training Epoch: 8 [25750/36450]\tLoss: 622.7666\n",
      "Training Epoch: 8 [25800/36450]\tLoss: 644.7734\n",
      "Training Epoch: 8 [25850/36450]\tLoss: 620.0113\n",
      "Training Epoch: 8 [25900/36450]\tLoss: 623.0912\n",
      "Training Epoch: 8 [25950/36450]\tLoss: 610.0436\n",
      "Training Epoch: 8 [26000/36450]\tLoss: 620.8466\n",
      "Training Epoch: 8 [26050/36450]\tLoss: 624.6644\n",
      "Training Epoch: 8 [26100/36450]\tLoss: 658.2826\n",
      "Training Epoch: 8 [26150/36450]\tLoss: 631.4835\n",
      "Training Epoch: 8 [26200/36450]\tLoss: 612.1683\n",
      "Training Epoch: 8 [26250/36450]\tLoss: 580.4063\n",
      "Training Epoch: 8 [26300/36450]\tLoss: 614.3502\n",
      "Training Epoch: 8 [26350/36450]\tLoss: 641.6588\n",
      "Training Epoch: 8 [26400/36450]\tLoss: 625.2701\n",
      "Training Epoch: 8 [26450/36450]\tLoss: 623.2830\n",
      "Training Epoch: 8 [26500/36450]\tLoss: 654.1107\n",
      "Training Epoch: 8 [26550/36450]\tLoss: 633.1218\n",
      "Training Epoch: 8 [26600/36450]\tLoss: 657.4230\n",
      "Training Epoch: 8 [26650/36450]\tLoss: 661.2318\n",
      "Training Epoch: 8 [26700/36450]\tLoss: 666.7206\n",
      "Training Epoch: 8 [26750/36450]\tLoss: 681.4738\n",
      "Training Epoch: 8 [26800/36450]\tLoss: 705.4678\n",
      "Training Epoch: 8 [26850/36450]\tLoss: 733.4024\n",
      "Training Epoch: 8 [26900/36450]\tLoss: 729.1213\n",
      "Training Epoch: 8 [26950/36450]\tLoss: 724.6964\n",
      "Training Epoch: 8 [27000/36450]\tLoss: 705.8757\n",
      "Training Epoch: 8 [27050/36450]\tLoss: 660.9118\n",
      "Training Epoch: 8 [27100/36450]\tLoss: 646.6552\n",
      "Training Epoch: 8 [27150/36450]\tLoss: 644.6080\n",
      "Training Epoch: 8 [27200/36450]\tLoss: 639.4117\n",
      "Training Epoch: 8 [27250/36450]\tLoss: 632.0103\n",
      "Training Epoch: 8 [27300/36450]\tLoss: 677.6160\n",
      "Training Epoch: 8 [27350/36450]\tLoss: 640.7031\n",
      "Training Epoch: 8 [27400/36450]\tLoss: 621.9583\n",
      "Training Epoch: 8 [27450/36450]\tLoss: 626.4590\n",
      "Training Epoch: 8 [27500/36450]\tLoss: 625.8622\n",
      "Training Epoch: 8 [27550/36450]\tLoss: 626.5991\n",
      "Training Epoch: 8 [27600/36450]\tLoss: 634.2895\n",
      "Training Epoch: 8 [27650/36450]\tLoss: 654.7917\n",
      "Training Epoch: 8 [27700/36450]\tLoss: 582.0141\n",
      "Training Epoch: 8 [27750/36450]\tLoss: 648.9796\n",
      "Training Epoch: 8 [27800/36450]\tLoss: 629.9314\n",
      "Training Epoch: 8 [27850/36450]\tLoss: 641.7473\n",
      "Training Epoch: 8 [27900/36450]\tLoss: 650.6596\n",
      "Training Epoch: 8 [27950/36450]\tLoss: 622.5585\n",
      "Training Epoch: 8 [28000/36450]\tLoss: 627.6199\n",
      "Training Epoch: 8 [28050/36450]\tLoss: 621.6046\n",
      "Training Epoch: 8 [28100/36450]\tLoss: 611.3927\n",
      "Training Epoch: 8 [28150/36450]\tLoss: 604.8715\n",
      "Training Epoch: 8 [28200/36450]\tLoss: 618.6092\n",
      "Training Epoch: 8 [28250/36450]\tLoss: 636.7266\n",
      "Training Epoch: 8 [28300/36450]\tLoss: 630.1193\n",
      "Training Epoch: 8 [28350/36450]\tLoss: 651.2733\n",
      "Training Epoch: 8 [28400/36450]\tLoss: 601.0558\n",
      "Training Epoch: 8 [28450/36450]\tLoss: 630.9886\n",
      "Training Epoch: 8 [28500/36450]\tLoss: 619.2574\n",
      "Training Epoch: 8 [28550/36450]\tLoss: 611.9799\n",
      "Training Epoch: 8 [28600/36450]\tLoss: 600.9473\n",
      "Training Epoch: 8 [28650/36450]\tLoss: 609.6512\n",
      "Training Epoch: 8 [28700/36450]\tLoss: 597.3356\n",
      "Training Epoch: 8 [28750/36450]\tLoss: 646.6984\n",
      "Training Epoch: 8 [28800/36450]\tLoss: 631.7418\n",
      "Training Epoch: 8 [28850/36450]\tLoss: 613.8574\n",
      "Training Epoch: 8 [28900/36450]\tLoss: 622.1098\n",
      "Training Epoch: 8 [28950/36450]\tLoss: 689.7205\n",
      "Training Epoch: 8 [29000/36450]\tLoss: 582.9608\n",
      "Training Epoch: 8 [29050/36450]\tLoss: 631.2687\n",
      "Training Epoch: 8 [29100/36450]\tLoss: 607.5591\n",
      "Training Epoch: 8 [29150/36450]\tLoss: 597.8826\n",
      "Training Epoch: 8 [29200/36450]\tLoss: 622.2392\n",
      "Training Epoch: 8 [29250/36450]\tLoss: 674.6581\n",
      "Training Epoch: 8 [29300/36450]\tLoss: 616.5654\n",
      "Training Epoch: 8 [29350/36450]\tLoss: 630.6827\n",
      "Training Epoch: 8 [29400/36450]\tLoss: 612.5170\n",
      "Training Epoch: 8 [29450/36450]\tLoss: 584.0757\n",
      "Training Epoch: 8 [29500/36450]\tLoss: 646.7944\n",
      "Training Epoch: 8 [29550/36450]\tLoss: 622.3577\n",
      "Training Epoch: 8 [29600/36450]\tLoss: 647.8309\n",
      "Training Epoch: 8 [29650/36450]\tLoss: 623.6162\n",
      "Training Epoch: 8 [29700/36450]\tLoss: 580.9890\n",
      "Training Epoch: 8 [29750/36450]\tLoss: 615.6293\n",
      "Training Epoch: 8 [29800/36450]\tLoss: 610.7734\n",
      "Training Epoch: 8 [29850/36450]\tLoss: 675.4883\n",
      "Training Epoch: 8 [29900/36450]\tLoss: 632.7566\n",
      "Training Epoch: 8 [29950/36450]\tLoss: 633.7941\n",
      "Training Epoch: 8 [30000/36450]\tLoss: 576.7823\n",
      "Training Epoch: 8 [30050/36450]\tLoss: 615.7030\n",
      "Training Epoch: 8 [30100/36450]\tLoss: 617.6328\n",
      "Training Epoch: 8 [30150/36450]\tLoss: 665.7102\n",
      "Training Epoch: 8 [30200/36450]\tLoss: 643.2109\n",
      "Training Epoch: 8 [30250/36450]\tLoss: 629.2440\n",
      "Training Epoch: 8 [30300/36450]\tLoss: 657.5022\n",
      "Training Epoch: 8 [30350/36450]\tLoss: 654.9827\n",
      "Training Epoch: 8 [30400/36450]\tLoss: 645.6544\n",
      "Training Epoch: 8 [30450/36450]\tLoss: 603.5877\n",
      "Training Epoch: 8 [30500/36450]\tLoss: 626.9462\n",
      "Training Epoch: 8 [30550/36450]\tLoss: 654.7824\n",
      "Training Epoch: 8 [30600/36450]\tLoss: 638.4463\n",
      "Training Epoch: 8 [30650/36450]\tLoss: 639.3268\n",
      "Training Epoch: 8 [30700/36450]\tLoss: 589.6733\n",
      "Training Epoch: 8 [30750/36450]\tLoss: 644.2501\n",
      "Training Epoch: 8 [30800/36450]\tLoss: 615.1154\n",
      "Training Epoch: 8 [30850/36450]\tLoss: 614.5668\n",
      "Training Epoch: 8 [30900/36450]\tLoss: 627.1320\n",
      "Training Epoch: 8 [30950/36450]\tLoss: 614.2858\n",
      "Training Epoch: 8 [31000/36450]\tLoss: 635.7787\n",
      "Training Epoch: 8 [31050/36450]\tLoss: 578.6266\n",
      "Training Epoch: 8 [31100/36450]\tLoss: 625.8064\n",
      "Training Epoch: 8 [31150/36450]\tLoss: 631.1034\n",
      "Training Epoch: 8 [31200/36450]\tLoss: 596.6176\n",
      "Training Epoch: 8 [31250/36450]\tLoss: 628.1898\n",
      "Training Epoch: 8 [31300/36450]\tLoss: 619.2905\n",
      "Training Epoch: 8 [31350/36450]\tLoss: 629.7433\n",
      "Training Epoch: 8 [31400/36450]\tLoss: 619.1727\n",
      "Training Epoch: 8 [31450/36450]\tLoss: 657.4809\n",
      "Training Epoch: 8 [31500/36450]\tLoss: 633.0144\n",
      "Training Epoch: 8 [31550/36450]\tLoss: 631.7222\n",
      "Training Epoch: 8 [31600/36450]\tLoss: 647.4231\n",
      "Training Epoch: 8 [31650/36450]\tLoss: 630.5698\n",
      "Training Epoch: 8 [31700/36450]\tLoss: 630.9716\n",
      "Training Epoch: 8 [31750/36450]\tLoss: 660.7016\n",
      "Training Epoch: 8 [31800/36450]\tLoss: 616.3923\n",
      "Training Epoch: 8 [31850/36450]\tLoss: 609.4692\n",
      "Training Epoch: 8 [31900/36450]\tLoss: 622.2515\n",
      "Training Epoch: 8 [31950/36450]\tLoss: 638.5202\n",
      "Training Epoch: 8 [32000/36450]\tLoss: 640.8967\n",
      "Training Epoch: 8 [32050/36450]\tLoss: 650.6209\n",
      "Training Epoch: 8 [32100/36450]\tLoss: 620.2433\n",
      "Training Epoch: 8 [32150/36450]\tLoss: 596.5579\n",
      "Training Epoch: 8 [32200/36450]\tLoss: 625.7988\n",
      "Training Epoch: 8 [32250/36450]\tLoss: 590.6014\n",
      "Training Epoch: 8 [32300/36450]\tLoss: 592.4565\n",
      "Training Epoch: 8 [32350/36450]\tLoss: 640.1280\n",
      "Training Epoch: 8 [32400/36450]\tLoss: 663.0991\n",
      "Training Epoch: 8 [32450/36450]\tLoss: 595.2909\n",
      "Training Epoch: 8 [32500/36450]\tLoss: 608.3424\n",
      "Training Epoch: 8 [32550/36450]\tLoss: 652.5729\n",
      "Training Epoch: 8 [32600/36450]\tLoss: 638.1793\n",
      "Training Epoch: 8 [32650/36450]\tLoss: 596.5746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [32700/36450]\tLoss: 667.4973\n",
      "Training Epoch: 8 [32750/36450]\tLoss: 602.6561\n",
      "Training Epoch: 8 [32800/36450]\tLoss: 628.6467\n",
      "Training Epoch: 8 [32850/36450]\tLoss: 638.1434\n",
      "Training Epoch: 8 [32900/36450]\tLoss: 651.8848\n",
      "Training Epoch: 8 [32950/36450]\tLoss: 611.7108\n",
      "Training Epoch: 8 [33000/36450]\tLoss: 619.2792\n",
      "Training Epoch: 8 [33050/36450]\tLoss: 588.6632\n",
      "Training Epoch: 8 [33100/36450]\tLoss: 599.4505\n",
      "Training Epoch: 8 [33150/36450]\tLoss: 642.0662\n",
      "Training Epoch: 8 [33200/36450]\tLoss: 652.8362\n",
      "Training Epoch: 8 [33250/36450]\tLoss: 655.8661\n",
      "Training Epoch: 8 [33300/36450]\tLoss: 641.6378\n",
      "Training Epoch: 8 [33350/36450]\tLoss: 622.3761\n",
      "Training Epoch: 8 [33400/36450]\tLoss: 634.2690\n",
      "Training Epoch: 8 [33450/36450]\tLoss: 610.8992\n",
      "Training Epoch: 8 [33500/36450]\tLoss: 623.6946\n",
      "Training Epoch: 8 [33550/36450]\tLoss: 628.9247\n",
      "Training Epoch: 8 [33600/36450]\tLoss: 623.8774\n",
      "Training Epoch: 8 [33650/36450]\tLoss: 612.2061\n",
      "Training Epoch: 8 [33700/36450]\tLoss: 628.0420\n",
      "Training Epoch: 8 [33750/36450]\tLoss: 665.2194\n",
      "Training Epoch: 8 [33800/36450]\tLoss: 636.1769\n",
      "Training Epoch: 8 [33850/36450]\tLoss: 602.2076\n",
      "Training Epoch: 8 [33900/36450]\tLoss: 649.1874\n",
      "Training Epoch: 8 [33950/36450]\tLoss: 664.0687\n",
      "Training Epoch: 8 [34000/36450]\tLoss: 614.8232\n",
      "Training Epoch: 8 [34050/36450]\tLoss: 659.6487\n",
      "Training Epoch: 8 [34100/36450]\tLoss: 642.8405\n",
      "Training Epoch: 8 [34150/36450]\tLoss: 646.9282\n",
      "Training Epoch: 8 [34200/36450]\tLoss: 651.2121\n",
      "Training Epoch: 8 [34250/36450]\tLoss: 641.2380\n",
      "Training Epoch: 8 [34300/36450]\tLoss: 599.1312\n",
      "Training Epoch: 8 [34350/36450]\tLoss: 679.9940\n",
      "Training Epoch: 8 [34400/36450]\tLoss: 648.6540\n",
      "Training Epoch: 8 [34450/36450]\tLoss: 650.4815\n",
      "Training Epoch: 8 [34500/36450]\tLoss: 640.9495\n",
      "Training Epoch: 8 [34550/36450]\tLoss: 624.0256\n",
      "Training Epoch: 8 [34600/36450]\tLoss: 686.1145\n",
      "Training Epoch: 8 [34650/36450]\tLoss: 669.3104\n",
      "Training Epoch: 8 [34700/36450]\tLoss: 645.9786\n",
      "Training Epoch: 8 [34750/36450]\tLoss: 609.2025\n",
      "Training Epoch: 8 [34800/36450]\tLoss: 644.8070\n",
      "Training Epoch: 8 [34850/36450]\tLoss: 637.0433\n",
      "Training Epoch: 8 [34900/36450]\tLoss: 618.1840\n",
      "Training Epoch: 8 [34950/36450]\tLoss: 623.4003\n",
      "Training Epoch: 8 [35000/36450]\tLoss: 605.3786\n",
      "Training Epoch: 8 [35050/36450]\tLoss: 649.7188\n",
      "Training Epoch: 8 [35100/36450]\tLoss: 678.0976\n",
      "Training Epoch: 8 [35150/36450]\tLoss: 634.7794\n",
      "Training Epoch: 8 [35200/36450]\tLoss: 648.7186\n",
      "Training Epoch: 8 [35250/36450]\tLoss: 642.8859\n",
      "Training Epoch: 8 [35300/36450]\tLoss: 624.9171\n",
      "Training Epoch: 8 [35350/36450]\tLoss: 677.9178\n",
      "Training Epoch: 8 [35400/36450]\tLoss: 652.0146\n",
      "Training Epoch: 8 [35450/36450]\tLoss: 618.3491\n",
      "Training Epoch: 8 [35500/36450]\tLoss: 638.8228\n",
      "Training Epoch: 8 [35550/36450]\tLoss: 630.2910\n",
      "Training Epoch: 8 [35600/36450]\tLoss: 633.4308\n",
      "Training Epoch: 8 [35650/36450]\tLoss: 589.4492\n",
      "Training Epoch: 8 [35700/36450]\tLoss: 615.8208\n",
      "Training Epoch: 8 [35750/36450]\tLoss: 637.6671\n",
      "Training Epoch: 8 [35800/36450]\tLoss: 602.0356\n",
      "Training Epoch: 8 [35850/36450]\tLoss: 633.3803\n",
      "Training Epoch: 8 [35900/36450]\tLoss: 633.0664\n",
      "Training Epoch: 8 [35950/36450]\tLoss: 643.5433\n",
      "Training Epoch: 8 [36000/36450]\tLoss: 595.6130\n",
      "Training Epoch: 8 [36050/36450]\tLoss: 604.3115\n",
      "Training Epoch: 8 [36100/36450]\tLoss: 627.3235\n",
      "Training Epoch: 8 [36150/36450]\tLoss: 647.7480\n",
      "Training Epoch: 8 [36200/36450]\tLoss: 604.0588\n",
      "Training Epoch: 8 [36250/36450]\tLoss: 632.0667\n",
      "Training Epoch: 8 [36300/36450]\tLoss: 605.8602\n",
      "Training Epoch: 8 [36350/36450]\tLoss: 620.4908\n",
      "Training Epoch: 8 [36400/36450]\tLoss: 657.7730\n",
      "Training Epoch: 8 [36450/36450]\tLoss: 621.9985\n",
      "Training Epoch: 8 [4050/4050]\tLoss: 316.1421\n",
      "Training Epoch: 9 [50/36450]\tLoss: 626.5319\n",
      "Training Epoch: 9 [100/36450]\tLoss: 626.9442\n",
      "Training Epoch: 9 [150/36450]\tLoss: 641.7137\n",
      "Training Epoch: 9 [200/36450]\tLoss: 605.5467\n",
      "Training Epoch: 9 [250/36450]\tLoss: 598.2646\n",
      "Training Epoch: 9 [300/36450]\tLoss: 585.1047\n",
      "Training Epoch: 9 [350/36450]\tLoss: 636.3985\n",
      "Training Epoch: 9 [400/36450]\tLoss: 622.4506\n",
      "Training Epoch: 9 [450/36450]\tLoss: 595.3553\n",
      "Training Epoch: 9 [500/36450]\tLoss: 651.4835\n",
      "Training Epoch: 9 [550/36450]\tLoss: 617.1641\n",
      "Training Epoch: 9 [600/36450]\tLoss: 617.9900\n",
      "Training Epoch: 9 [650/36450]\tLoss: 604.7197\n",
      "Training Epoch: 9 [700/36450]\tLoss: 609.2424\n",
      "Training Epoch: 9 [750/36450]\tLoss: 607.4960\n",
      "Training Epoch: 9 [800/36450]\tLoss: 663.7266\n",
      "Training Epoch: 9 [850/36450]\tLoss: 613.1671\n",
      "Training Epoch: 9 [900/36450]\tLoss: 637.2439\n",
      "Training Epoch: 9 [950/36450]\tLoss: 656.7472\n",
      "Training Epoch: 9 [1000/36450]\tLoss: 647.6217\n",
      "Training Epoch: 9 [1050/36450]\tLoss: 646.7025\n",
      "Training Epoch: 9 [1100/36450]\tLoss: 612.1662\n",
      "Training Epoch: 9 [1150/36450]\tLoss: 643.4457\n",
      "Training Epoch: 9 [1200/36450]\tLoss: 610.3098\n",
      "Training Epoch: 9 [1250/36450]\tLoss: 589.2938\n",
      "Training Epoch: 9 [1300/36450]\tLoss: 641.4888\n",
      "Training Epoch: 9 [1350/36450]\tLoss: 604.6767\n",
      "Training Epoch: 9 [1400/36450]\tLoss: 594.6665\n",
      "Training Epoch: 9 [1450/36450]\tLoss: 616.7513\n",
      "Training Epoch: 9 [1500/36450]\tLoss: 617.4576\n",
      "Training Epoch: 9 [1550/36450]\tLoss: 631.3150\n",
      "Training Epoch: 9 [1600/36450]\tLoss: 643.1732\n",
      "Training Epoch: 9 [1650/36450]\tLoss: 655.2063\n",
      "Training Epoch: 9 [1700/36450]\tLoss: 622.9247\n",
      "Training Epoch: 9 [1750/36450]\tLoss: 651.8187\n",
      "Training Epoch: 9 [1800/36450]\tLoss: 644.3508\n",
      "Training Epoch: 9 [1850/36450]\tLoss: 632.8735\n",
      "Training Epoch: 9 [1900/36450]\tLoss: 636.8892\n",
      "Training Epoch: 9 [1950/36450]\tLoss: 636.2922\n",
      "Training Epoch: 9 [2000/36450]\tLoss: 627.7520\n",
      "Training Epoch: 9 [2050/36450]\tLoss: 624.6171\n",
      "Training Epoch: 9 [2100/36450]\tLoss: 595.0850\n",
      "Training Epoch: 9 [2150/36450]\tLoss: 591.9899\n",
      "Training Epoch: 9 [2200/36450]\tLoss: 652.7109\n",
      "Training Epoch: 9 [2250/36450]\tLoss: 628.9073\n",
      "Training Epoch: 9 [2300/36450]\tLoss: 625.1055\n",
      "Training Epoch: 9 [2350/36450]\tLoss: 642.3755\n",
      "Training Epoch: 9 [2400/36450]\tLoss: 616.7556\n",
      "Training Epoch: 9 [2450/36450]\tLoss: 636.2542\n",
      "Training Epoch: 9 [2500/36450]\tLoss: 669.5878\n",
      "Training Epoch: 9 [2550/36450]\tLoss: 665.0458\n",
      "Training Epoch: 9 [2600/36450]\tLoss: 646.0066\n",
      "Training Epoch: 9 [2650/36450]\tLoss: 679.8119\n",
      "Training Epoch: 9 [2700/36450]\tLoss: 684.1823\n",
      "Training Epoch: 9 [2750/36450]\tLoss: 689.1266\n",
      "Training Epoch: 9 [2800/36450]\tLoss: 618.1038\n",
      "Training Epoch: 9 [2850/36450]\tLoss: 618.9692\n",
      "Training Epoch: 9 [2900/36450]\tLoss: 622.0831\n",
      "Training Epoch: 9 [2950/36450]\tLoss: 660.5754\n",
      "Training Epoch: 9 [3000/36450]\tLoss: 625.2706\n",
      "Training Epoch: 9 [3050/36450]\tLoss: 652.4283\n",
      "Training Epoch: 9 [3100/36450]\tLoss: 657.9293\n",
      "Training Epoch: 9 [3150/36450]\tLoss: 617.8038\n",
      "Training Epoch: 9 [3200/36450]\tLoss: 618.9577\n",
      "Training Epoch: 9 [3250/36450]\tLoss: 597.2629\n",
      "Training Epoch: 9 [3300/36450]\tLoss: 649.1424\n",
      "Training Epoch: 9 [3350/36450]\tLoss: 640.9323\n",
      "Training Epoch: 9 [3400/36450]\tLoss: 660.7289\n",
      "Training Epoch: 9 [3450/36450]\tLoss: 654.0145\n",
      "Training Epoch: 9 [3500/36450]\tLoss: 618.5771\n",
      "Training Epoch: 9 [3550/36450]\tLoss: 600.2887\n",
      "Training Epoch: 9 [3600/36450]\tLoss: 600.9546\n",
      "Training Epoch: 9 [3650/36450]\tLoss: 597.7700\n",
      "Training Epoch: 9 [3700/36450]\tLoss: 604.4353\n",
      "Training Epoch: 9 [3750/36450]\tLoss: 657.4025\n",
      "Training Epoch: 9 [3800/36450]\tLoss: 628.5122\n",
      "Training Epoch: 9 [3850/36450]\tLoss: 611.2278\n",
      "Training Epoch: 9 [3900/36450]\tLoss: 570.6352\n",
      "Training Epoch: 9 [3950/36450]\tLoss: 617.4731\n",
      "Training Epoch: 9 [4000/36450]\tLoss: 627.6889\n",
      "Training Epoch: 9 [4050/36450]\tLoss: 624.5858\n",
      "Training Epoch: 9 [4100/36450]\tLoss: 627.9052\n",
      "Training Epoch: 9 [4150/36450]\tLoss: 584.0052\n",
      "Training Epoch: 9 [4200/36450]\tLoss: 604.5998\n",
      "Training Epoch: 9 [4250/36450]\tLoss: 597.3629\n",
      "Training Epoch: 9 [4300/36450]\tLoss: 611.4032\n",
      "Training Epoch: 9 [4350/36450]\tLoss: 614.2147\n",
      "Training Epoch: 9 [4400/36450]\tLoss: 610.8497\n",
      "Training Epoch: 9 [4450/36450]\tLoss: 642.7311\n",
      "Training Epoch: 9 [4500/36450]\tLoss: 610.4986\n",
      "Training Epoch: 9 [4550/36450]\tLoss: 587.8214\n",
      "Training Epoch: 9 [4600/36450]\tLoss: 598.8115\n",
      "Training Epoch: 9 [4650/36450]\tLoss: 591.9185\n",
      "Training Epoch: 9 [4700/36450]\tLoss: 598.5259\n",
      "Training Epoch: 9 [4750/36450]\tLoss: 634.0592\n",
      "Training Epoch: 9 [4800/36450]\tLoss: 631.2300\n",
      "Training Epoch: 9 [4850/36450]\tLoss: 597.7986\n",
      "Training Epoch: 9 [4900/36450]\tLoss: 600.2037\n",
      "Training Epoch: 9 [4950/36450]\tLoss: 602.1752\n",
      "Training Epoch: 9 [5000/36450]\tLoss: 613.5795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [5050/36450]\tLoss: 602.7504\n",
      "Training Epoch: 9 [5100/36450]\tLoss: 621.9355\n",
      "Training Epoch: 9 [5150/36450]\tLoss: 635.1321\n",
      "Training Epoch: 9 [5200/36450]\tLoss: 605.9084\n",
      "Training Epoch: 9 [5250/36450]\tLoss: 633.7461\n",
      "Training Epoch: 9 [5300/36450]\tLoss: 597.0706\n",
      "Training Epoch: 9 [5350/36450]\tLoss: 646.5705\n",
      "Training Epoch: 9 [5400/36450]\tLoss: 678.3741\n",
      "Training Epoch: 9 [5450/36450]\tLoss: 605.9465\n",
      "Training Epoch: 9 [5500/36450]\tLoss: 639.1290\n",
      "Training Epoch: 9 [5550/36450]\tLoss: 614.3690\n",
      "Training Epoch: 9 [5600/36450]\tLoss: 605.2825\n",
      "Training Epoch: 9 [5650/36450]\tLoss: 656.8265\n",
      "Training Epoch: 9 [5700/36450]\tLoss: 656.7686\n",
      "Training Epoch: 9 [5750/36450]\tLoss: 644.6262\n",
      "Training Epoch: 9 [5800/36450]\tLoss: 593.3770\n",
      "Training Epoch: 9 [5850/36450]\tLoss: 583.6389\n",
      "Training Epoch: 9 [5900/36450]\tLoss: 646.9218\n",
      "Training Epoch: 9 [5950/36450]\tLoss: 620.7004\n",
      "Training Epoch: 9 [6000/36450]\tLoss: 583.0010\n",
      "Training Epoch: 9 [6050/36450]\tLoss: 603.2339\n",
      "Training Epoch: 9 [6100/36450]\tLoss: 565.8397\n",
      "Training Epoch: 9 [6150/36450]\tLoss: 589.5707\n",
      "Training Epoch: 9 [6200/36450]\tLoss: 606.6764\n",
      "Training Epoch: 9 [6250/36450]\tLoss: 611.9554\n",
      "Training Epoch: 9 [6300/36450]\tLoss: 598.8987\n",
      "Training Epoch: 9 [6350/36450]\tLoss: 633.2000\n",
      "Training Epoch: 9 [6400/36450]\tLoss: 595.7667\n",
      "Training Epoch: 9 [6450/36450]\tLoss: 597.4558\n",
      "Training Epoch: 9 [6500/36450]\tLoss: 603.8851\n",
      "Training Epoch: 9 [6550/36450]\tLoss: 608.9107\n",
      "Training Epoch: 9 [6600/36450]\tLoss: 583.1176\n",
      "Training Epoch: 9 [6650/36450]\tLoss: 591.0615\n",
      "Training Epoch: 9 [6700/36450]\tLoss: 622.2023\n",
      "Training Epoch: 9 [6750/36450]\tLoss: 602.6823\n",
      "Training Epoch: 9 [6800/36450]\tLoss: 626.8395\n",
      "Training Epoch: 9 [6850/36450]\tLoss: 630.0612\n",
      "Training Epoch: 9 [6900/36450]\tLoss: 582.7643\n",
      "Training Epoch: 9 [6950/36450]\tLoss: 608.2855\n",
      "Training Epoch: 9 [7000/36450]\tLoss: 614.7402\n",
      "Training Epoch: 9 [7050/36450]\tLoss: 636.3397\n",
      "Training Epoch: 9 [7100/36450]\tLoss: 618.9886\n",
      "Training Epoch: 9 [7150/36450]\tLoss: 654.5355\n",
      "Training Epoch: 9 [7200/36450]\tLoss: 614.3235\n",
      "Training Epoch: 9 [7250/36450]\tLoss: 625.9916\n",
      "Training Epoch: 9 [7300/36450]\tLoss: 641.3541\n",
      "Training Epoch: 9 [7350/36450]\tLoss: 621.3146\n",
      "Training Epoch: 9 [7400/36450]\tLoss: 589.9118\n",
      "Training Epoch: 9 [7450/36450]\tLoss: 603.7422\n",
      "Training Epoch: 9 [7500/36450]\tLoss: 612.5038\n",
      "Training Epoch: 9 [7550/36450]\tLoss: 618.3039\n",
      "Training Epoch: 9 [7600/36450]\tLoss: 651.7637\n",
      "Training Epoch: 9 [7650/36450]\tLoss: 586.9523\n",
      "Training Epoch: 9 [7700/36450]\tLoss: 622.5802\n",
      "Training Epoch: 9 [7750/36450]\tLoss: 629.9573\n",
      "Training Epoch: 9 [7800/36450]\tLoss: 665.6749\n",
      "Training Epoch: 9 [7850/36450]\tLoss: 638.6954\n",
      "Training Epoch: 9 [7900/36450]\tLoss: 602.2297\n",
      "Training Epoch: 9 [7950/36450]\tLoss: 603.5884\n",
      "Training Epoch: 9 [8000/36450]\tLoss: 613.4256\n",
      "Training Epoch: 9 [8050/36450]\tLoss: 617.3604\n",
      "Training Epoch: 9 [8100/36450]\tLoss: 583.6907\n",
      "Training Epoch: 9 [8150/36450]\tLoss: 590.0156\n",
      "Training Epoch: 9 [8200/36450]\tLoss: 611.1356\n",
      "Training Epoch: 9 [8250/36450]\tLoss: 595.7369\n",
      "Training Epoch: 9 [8300/36450]\tLoss: 588.0394\n",
      "Training Epoch: 9 [8350/36450]\tLoss: 633.8347\n",
      "Training Epoch: 9 [8400/36450]\tLoss: 611.5637\n",
      "Training Epoch: 9 [8450/36450]\tLoss: 604.5823\n",
      "Training Epoch: 9 [8500/36450]\tLoss: 606.7435\n",
      "Training Epoch: 9 [8550/36450]\tLoss: 589.1328\n",
      "Training Epoch: 9 [8600/36450]\tLoss: 625.4609\n",
      "Training Epoch: 9 [8650/36450]\tLoss: 655.4736\n",
      "Training Epoch: 9 [8700/36450]\tLoss: 610.3384\n",
      "Training Epoch: 9 [8750/36450]\tLoss: 612.9807\n",
      "Training Epoch: 9 [8800/36450]\tLoss: 643.3157\n",
      "Training Epoch: 9 [8850/36450]\tLoss: 606.0721\n",
      "Training Epoch: 9 [8900/36450]\tLoss: 617.3505\n",
      "Training Epoch: 9 [8950/36450]\tLoss: 635.6440\n",
      "Training Epoch: 9 [9000/36450]\tLoss: 652.0903\n",
      "Training Epoch: 9 [9050/36450]\tLoss: 609.1192\n",
      "Training Epoch: 9 [9100/36450]\tLoss: 616.5844\n",
      "Training Epoch: 9 [9150/36450]\tLoss: 638.0212\n",
      "Training Epoch: 9 [9200/36450]\tLoss: 601.9473\n",
      "Training Epoch: 9 [9250/36450]\tLoss: 587.2620\n",
      "Training Epoch: 9 [9300/36450]\tLoss: 623.6350\n",
      "Training Epoch: 9 [9350/36450]\tLoss: 578.8405\n",
      "Training Epoch: 9 [9400/36450]\tLoss: 619.2523\n",
      "Training Epoch: 9 [9450/36450]\tLoss: 599.4352\n",
      "Training Epoch: 9 [9500/36450]\tLoss: 611.1927\n",
      "Training Epoch: 9 [9550/36450]\tLoss: 602.5173\n",
      "Training Epoch: 9 [9600/36450]\tLoss: 623.1137\n",
      "Training Epoch: 9 [9650/36450]\tLoss: 577.8494\n",
      "Training Epoch: 9 [9700/36450]\tLoss: 629.5021\n",
      "Training Epoch: 9 [9750/36450]\tLoss: 626.9379\n",
      "Training Epoch: 9 [9800/36450]\tLoss: 592.3690\n",
      "Training Epoch: 9 [9850/36450]\tLoss: 611.4363\n",
      "Training Epoch: 9 [9900/36450]\tLoss: 621.5084\n",
      "Training Epoch: 9 [9950/36450]\tLoss: 603.7672\n",
      "Training Epoch: 9 [10000/36450]\tLoss: 611.5543\n",
      "Training Epoch: 9 [10050/36450]\tLoss: 598.4379\n",
      "Training Epoch: 9 [10100/36450]\tLoss: 607.1849\n",
      "Training Epoch: 9 [10150/36450]\tLoss: 577.2673\n",
      "Training Epoch: 9 [10200/36450]\tLoss: 635.8624\n",
      "Training Epoch: 9 [10250/36450]\tLoss: 581.7496\n",
      "Training Epoch: 9 [10300/36450]\tLoss: 605.7099\n",
      "Training Epoch: 9 [10350/36450]\tLoss: 609.1301\n",
      "Training Epoch: 9 [10400/36450]\tLoss: 634.7636\n",
      "Training Epoch: 9 [10450/36450]\tLoss: 605.5316\n",
      "Training Epoch: 9 [10500/36450]\tLoss: 649.5072\n",
      "Training Epoch: 9 [10550/36450]\tLoss: 611.2469\n",
      "Training Epoch: 9 [10600/36450]\tLoss: 644.6104\n",
      "Training Epoch: 9 [10650/36450]\tLoss: 649.3287\n",
      "Training Epoch: 9 [10700/36450]\tLoss: 696.7568\n",
      "Training Epoch: 9 [10750/36450]\tLoss: 627.2997\n",
      "Training Epoch: 9 [10800/36450]\tLoss: 656.5950\n",
      "Training Epoch: 9 [10850/36450]\tLoss: 631.8854\n",
      "Training Epoch: 9 [10900/36450]\tLoss: 627.4872\n",
      "Training Epoch: 9 [10950/36450]\tLoss: 608.3892\n",
      "Training Epoch: 9 [11000/36450]\tLoss: 642.9590\n",
      "Training Epoch: 9 [11050/36450]\tLoss: 578.8708\n",
      "Training Epoch: 9 [11100/36450]\tLoss: 634.2943\n",
      "Training Epoch: 9 [11150/36450]\tLoss: 649.9384\n",
      "Training Epoch: 9 [11200/36450]\tLoss: 546.4578\n",
      "Training Epoch: 9 [11250/36450]\tLoss: 647.9262\n",
      "Training Epoch: 9 [11300/36450]\tLoss: 605.2651\n",
      "Training Epoch: 9 [11350/36450]\tLoss: 607.1571\n",
      "Training Epoch: 9 [11400/36450]\tLoss: 640.4745\n",
      "Training Epoch: 9 [11450/36450]\tLoss: 632.8121\n",
      "Training Epoch: 9 [11500/36450]\tLoss: 604.4633\n",
      "Training Epoch: 9 [11550/36450]\tLoss: 641.0034\n",
      "Training Epoch: 9 [11600/36450]\tLoss: 615.9786\n",
      "Training Epoch: 9 [11650/36450]\tLoss: 593.9810\n",
      "Training Epoch: 9 [11700/36450]\tLoss: 621.0405\n",
      "Training Epoch: 9 [11750/36450]\tLoss: 622.1962\n",
      "Training Epoch: 9 [11800/36450]\tLoss: 617.2395\n",
      "Training Epoch: 9 [11850/36450]\tLoss: 616.7494\n",
      "Training Epoch: 9 [11900/36450]\tLoss: 611.4702\n",
      "Training Epoch: 9 [11950/36450]\tLoss: 606.9332\n",
      "Training Epoch: 9 [12000/36450]\tLoss: 574.0237\n",
      "Training Epoch: 9 [12050/36450]\tLoss: 652.5207\n",
      "Training Epoch: 9 [12100/36450]\tLoss: 635.5904\n",
      "Training Epoch: 9 [12150/36450]\tLoss: 604.5631\n",
      "Training Epoch: 9 [12200/36450]\tLoss: 607.6935\n",
      "Training Epoch: 9 [12250/36450]\tLoss: 598.3674\n",
      "Training Epoch: 9 [12300/36450]\tLoss: 587.2487\n",
      "Training Epoch: 9 [12350/36450]\tLoss: 651.1310\n",
      "Training Epoch: 9 [12400/36450]\tLoss: 583.6239\n",
      "Training Epoch: 9 [12450/36450]\tLoss: 635.2182\n",
      "Training Epoch: 9 [12500/36450]\tLoss: 616.3742\n",
      "Training Epoch: 9 [12550/36450]\tLoss: 603.2299\n",
      "Training Epoch: 9 [12600/36450]\tLoss: 569.7148\n",
      "Training Epoch: 9 [12650/36450]\tLoss: 590.1614\n",
      "Training Epoch: 9 [12700/36450]\tLoss: 649.6517\n",
      "Training Epoch: 9 [12750/36450]\tLoss: 616.1161\n",
      "Training Epoch: 9 [12800/36450]\tLoss: 631.6491\n",
      "Training Epoch: 9 [12850/36450]\tLoss: 611.0396\n",
      "Training Epoch: 9 [12900/36450]\tLoss: 611.9984\n",
      "Training Epoch: 9 [12950/36450]\tLoss: 636.5627\n",
      "Training Epoch: 9 [13000/36450]\tLoss: 606.9059\n",
      "Training Epoch: 9 [13050/36450]\tLoss: 619.0321\n",
      "Training Epoch: 9 [13100/36450]\tLoss: 594.5883\n",
      "Training Epoch: 9 [13150/36450]\tLoss: 626.0719\n",
      "Training Epoch: 9 [13200/36450]\tLoss: 612.6696\n",
      "Training Epoch: 9 [13250/36450]\tLoss: 639.4736\n",
      "Training Epoch: 9 [13300/36450]\tLoss: 612.4639\n",
      "Training Epoch: 9 [13350/36450]\tLoss: 586.8232\n",
      "Training Epoch: 9 [13400/36450]\tLoss: 605.7789\n",
      "Training Epoch: 9 [13450/36450]\tLoss: 614.8342\n",
      "Training Epoch: 9 [13500/36450]\tLoss: 584.5254\n",
      "Training Epoch: 9 [13550/36450]\tLoss: 584.3484\n",
      "Training Epoch: 9 [13600/36450]\tLoss: 610.4389\n",
      "Training Epoch: 9 [13650/36450]\tLoss: 620.7596\n",
      "Training Epoch: 9 [13700/36450]\tLoss: 617.2507\n",
      "Training Epoch: 9 [13750/36450]\tLoss: 592.7101\n",
      "Training Epoch: 9 [13800/36450]\tLoss: 616.8839\n",
      "Training Epoch: 9 [13850/36450]\tLoss: 606.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [13900/36450]\tLoss: 592.6782\n",
      "Training Epoch: 9 [13950/36450]\tLoss: 585.6204\n",
      "Training Epoch: 9 [14000/36450]\tLoss: 598.1049\n",
      "Training Epoch: 9 [14050/36450]\tLoss: 572.7321\n",
      "Training Epoch: 9 [14100/36450]\tLoss: 645.0430\n",
      "Training Epoch: 9 [14150/36450]\tLoss: 625.0130\n",
      "Training Epoch: 9 [14200/36450]\tLoss: 614.3068\n",
      "Training Epoch: 9 [14250/36450]\tLoss: 625.9851\n",
      "Training Epoch: 9 [14300/36450]\tLoss: 625.2435\n",
      "Training Epoch: 9 [14350/36450]\tLoss: 587.8175\n",
      "Training Epoch: 9 [14400/36450]\tLoss: 639.7161\n",
      "Training Epoch: 9 [14450/36450]\tLoss: 624.6553\n",
      "Training Epoch: 9 [14500/36450]\tLoss: 602.5562\n",
      "Training Epoch: 9 [14550/36450]\tLoss: 578.7129\n",
      "Training Epoch: 9 [14600/36450]\tLoss: 677.2583\n",
      "Training Epoch: 9 [14650/36450]\tLoss: 597.1944\n",
      "Training Epoch: 9 [14700/36450]\tLoss: 623.3354\n",
      "Training Epoch: 9 [14750/36450]\tLoss: 609.3254\n",
      "Training Epoch: 9 [14800/36450]\tLoss: 588.6165\n",
      "Training Epoch: 9 [14850/36450]\tLoss: 609.5281\n",
      "Training Epoch: 9 [14900/36450]\tLoss: 638.3482\n",
      "Training Epoch: 9 [14950/36450]\tLoss: 605.2936\n",
      "Training Epoch: 9 [15000/36450]\tLoss: 633.0365\n",
      "Training Epoch: 9 [15050/36450]\tLoss: 636.3347\n",
      "Training Epoch: 9 [15100/36450]\tLoss: 622.3614\n",
      "Training Epoch: 9 [15150/36450]\tLoss: 660.6562\n",
      "Training Epoch: 9 [15200/36450]\tLoss: 601.7158\n",
      "Training Epoch: 9 [15250/36450]\tLoss: 590.8369\n",
      "Training Epoch: 9 [15300/36450]\tLoss: 592.7777\n",
      "Training Epoch: 9 [15350/36450]\tLoss: 613.8920\n",
      "Training Epoch: 9 [15400/36450]\tLoss: 629.5651\n",
      "Training Epoch: 9 [15450/36450]\tLoss: 637.5064\n",
      "Training Epoch: 9 [15500/36450]\tLoss: 647.9969\n",
      "Training Epoch: 9 [15550/36450]\tLoss: 613.0516\n",
      "Training Epoch: 9 [15600/36450]\tLoss: 673.2832\n",
      "Training Epoch: 9 [15650/36450]\tLoss: 683.7504\n",
      "Training Epoch: 9 [15700/36450]\tLoss: 693.4644\n",
      "Training Epoch: 9 [15750/36450]\tLoss: 752.7669\n",
      "Training Epoch: 9 [15800/36450]\tLoss: 781.0690\n",
      "Training Epoch: 9 [15850/36450]\tLoss: 768.6610\n",
      "Training Epoch: 9 [15900/36450]\tLoss: 731.7632\n",
      "Training Epoch: 9 [15950/36450]\tLoss: 636.6443\n",
      "Training Epoch: 9 [16000/36450]\tLoss: 619.0256\n",
      "Training Epoch: 9 [16050/36450]\tLoss: 625.2068\n",
      "Training Epoch: 9 [16100/36450]\tLoss: 652.0829\n",
      "Training Epoch: 9 [16150/36450]\tLoss: 658.0561\n",
      "Training Epoch: 9 [16200/36450]\tLoss: 650.8416\n",
      "Training Epoch: 9 [16250/36450]\tLoss: 589.6478\n",
      "Training Epoch: 9 [16300/36450]\tLoss: 600.8035\n",
      "Training Epoch: 9 [16350/36450]\tLoss: 622.6204\n",
      "Training Epoch: 9 [16400/36450]\tLoss: 602.2137\n",
      "Training Epoch: 9 [16450/36450]\tLoss: 652.4235\n",
      "Training Epoch: 9 [16500/36450]\tLoss: 628.6120\n",
      "Training Epoch: 9 [16550/36450]\tLoss: 607.2039\n",
      "Training Epoch: 9 [16600/36450]\tLoss: 600.2300\n",
      "Training Epoch: 9 [16650/36450]\tLoss: 640.8270\n",
      "Training Epoch: 9 [16700/36450]\tLoss: 595.5032\n",
      "Training Epoch: 9 [16750/36450]\tLoss: 576.7292\n",
      "Training Epoch: 9 [16800/36450]\tLoss: 606.3065\n",
      "Training Epoch: 9 [16850/36450]\tLoss: 617.3057\n",
      "Training Epoch: 9 [16900/36450]\tLoss: 615.7315\n",
      "Training Epoch: 9 [16950/36450]\tLoss: 604.3125\n",
      "Training Epoch: 9 [17000/36450]\tLoss: 608.7856\n",
      "Training Epoch: 9 [17050/36450]\tLoss: 678.6571\n",
      "Training Epoch: 9 [17100/36450]\tLoss: 607.2998\n",
      "Training Epoch: 9 [17150/36450]\tLoss: 650.1815\n",
      "Training Epoch: 9 [17200/36450]\tLoss: 609.8922\n",
      "Training Epoch: 9 [17250/36450]\tLoss: 647.4620\n",
      "Training Epoch: 9 [17300/36450]\tLoss: 625.7449\n",
      "Training Epoch: 9 [17350/36450]\tLoss: 590.1611\n",
      "Training Epoch: 9 [17400/36450]\tLoss: 548.1567\n",
      "Training Epoch: 9 [17450/36450]\tLoss: 625.6407\n",
      "Training Epoch: 9 [17500/36450]\tLoss: 607.5552\n",
      "Training Epoch: 9 [17550/36450]\tLoss: 616.5654\n",
      "Training Epoch: 9 [17600/36450]\tLoss: 589.9667\n",
      "Training Epoch: 9 [17650/36450]\tLoss: 611.0908\n",
      "Training Epoch: 9 [17700/36450]\tLoss: 589.1361\n",
      "Training Epoch: 9 [17750/36450]\tLoss: 603.7050\n",
      "Training Epoch: 9 [17800/36450]\tLoss: 590.6448\n",
      "Training Epoch: 9 [17850/36450]\tLoss: 579.5718\n",
      "Training Epoch: 9 [17900/36450]\tLoss: 626.3971\n",
      "Training Epoch: 9 [17950/36450]\tLoss: 637.4325\n",
      "Training Epoch: 9 [18000/36450]\tLoss: 608.8045\n",
      "Training Epoch: 9 [18050/36450]\tLoss: 585.4376\n",
      "Training Epoch: 9 [18100/36450]\tLoss: 605.0322\n",
      "Training Epoch: 9 [18150/36450]\tLoss: 626.8521\n",
      "Training Epoch: 9 [18200/36450]\tLoss: 620.7781\n",
      "Training Epoch: 9 [18250/36450]\tLoss: 603.3479\n",
      "Training Epoch: 9 [18300/36450]\tLoss: 625.1208\n",
      "Training Epoch: 9 [18350/36450]\tLoss: 612.1688\n",
      "Training Epoch: 9 [18400/36450]\tLoss: 624.0539\n",
      "Training Epoch: 9 [18450/36450]\tLoss: 627.4730\n",
      "Training Epoch: 9 [18500/36450]\tLoss: 574.0529\n",
      "Training Epoch: 9 [18550/36450]\tLoss: 620.7620\n",
      "Training Epoch: 9 [18600/36450]\tLoss: 553.0902\n",
      "Training Epoch: 9 [18650/36450]\tLoss: 623.7376\n",
      "Training Epoch: 9 [18700/36450]\tLoss: 613.3137\n",
      "Training Epoch: 9 [18750/36450]\tLoss: 633.5420\n",
      "Training Epoch: 9 [18800/36450]\tLoss: 611.4280\n",
      "Training Epoch: 9 [18850/36450]\tLoss: 615.6738\n",
      "Training Epoch: 9 [18900/36450]\tLoss: 616.1786\n",
      "Training Epoch: 9 [18950/36450]\tLoss: 601.5031\n",
      "Training Epoch: 9 [19000/36450]\tLoss: 607.0736\n",
      "Training Epoch: 9 [19050/36450]\tLoss: 602.3719\n",
      "Training Epoch: 9 [19100/36450]\tLoss: 585.1685\n",
      "Training Epoch: 9 [19150/36450]\tLoss: 617.7545\n",
      "Training Epoch: 9 [19200/36450]\tLoss: 621.1785\n",
      "Training Epoch: 9 [19250/36450]\tLoss: 554.4507\n",
      "Training Epoch: 9 [19300/36450]\tLoss: 621.2814\n",
      "Training Epoch: 9 [19350/36450]\tLoss: 597.7983\n",
      "Training Epoch: 9 [19400/36450]\tLoss: 583.1096\n",
      "Training Epoch: 9 [19450/36450]\tLoss: 641.9969\n",
      "Training Epoch: 9 [19500/36450]\tLoss: 580.7761\n",
      "Training Epoch: 9 [19550/36450]\tLoss: 602.6389\n",
      "Training Epoch: 9 [19600/36450]\tLoss: 595.4181\n",
      "Training Epoch: 9 [19650/36450]\tLoss: 633.2703\n",
      "Training Epoch: 9 [19700/36450]\tLoss: 609.6656\n",
      "Training Epoch: 9 [19750/36450]\tLoss: 592.6095\n",
      "Training Epoch: 9 [19800/36450]\tLoss: 607.8205\n",
      "Training Epoch: 9 [19850/36450]\tLoss: 569.3868\n",
      "Training Epoch: 9 [19900/36450]\tLoss: 613.9515\n",
      "Training Epoch: 9 [19950/36450]\tLoss: 628.6272\n",
      "Training Epoch: 9 [20000/36450]\tLoss: 576.4323\n",
      "Training Epoch: 9 [20050/36450]\tLoss: 609.2455\n",
      "Training Epoch: 9 [20100/36450]\tLoss: 616.7260\n",
      "Training Epoch: 9 [20150/36450]\tLoss: 567.4879\n",
      "Training Epoch: 9 [20200/36450]\tLoss: 614.2561\n",
      "Training Epoch: 9 [20250/36450]\tLoss: 602.4637\n",
      "Training Epoch: 9 [20300/36450]\tLoss: 589.1192\n",
      "Training Epoch: 9 [20350/36450]\tLoss: 635.9015\n",
      "Training Epoch: 9 [20400/36450]\tLoss: 580.3400\n",
      "Training Epoch: 9 [20450/36450]\tLoss: 632.4901\n",
      "Training Epoch: 9 [20500/36450]\tLoss: 608.3857\n",
      "Training Epoch: 9 [20550/36450]\tLoss: 613.2105\n",
      "Training Epoch: 9 [20600/36450]\tLoss: 626.1720\n",
      "Training Epoch: 9 [20650/36450]\tLoss: 628.7950\n",
      "Training Epoch: 9 [20700/36450]\tLoss: 601.1664\n",
      "Training Epoch: 9 [20750/36450]\tLoss: 603.1955\n",
      "Training Epoch: 9 [20800/36450]\tLoss: 637.5083\n",
      "Training Epoch: 9 [20850/36450]\tLoss: 578.9575\n",
      "Training Epoch: 9 [20900/36450]\tLoss: 619.9298\n",
      "Training Epoch: 9 [20950/36450]\tLoss: 630.9826\n",
      "Training Epoch: 9 [21000/36450]\tLoss: 587.4778\n",
      "Training Epoch: 9 [21050/36450]\tLoss: 617.4872\n",
      "Training Epoch: 9 [21100/36450]\tLoss: 550.0483\n",
      "Training Epoch: 9 [21150/36450]\tLoss: 610.6602\n",
      "Training Epoch: 9 [21200/36450]\tLoss: 637.1603\n",
      "Training Epoch: 9 [21250/36450]\tLoss: 610.8521\n",
      "Training Epoch: 9 [21300/36450]\tLoss: 617.3682\n",
      "Training Epoch: 9 [21350/36450]\tLoss: 589.4836\n",
      "Training Epoch: 9 [21400/36450]\tLoss: 602.3994\n",
      "Training Epoch: 9 [21450/36450]\tLoss: 597.1360\n",
      "Training Epoch: 9 [21500/36450]\tLoss: 601.8282\n",
      "Training Epoch: 9 [21550/36450]\tLoss: 607.2209\n",
      "Training Epoch: 9 [21600/36450]\tLoss: 566.1696\n",
      "Training Epoch: 9 [21650/36450]\tLoss: 586.3929\n",
      "Training Epoch: 9 [21700/36450]\tLoss: 572.8519\n",
      "Training Epoch: 9 [21750/36450]\tLoss: 610.4991\n",
      "Training Epoch: 9 [21800/36450]\tLoss: 584.8707\n",
      "Training Epoch: 9 [21850/36450]\tLoss: 612.6731\n",
      "Training Epoch: 9 [21900/36450]\tLoss: 606.5683\n",
      "Training Epoch: 9 [21950/36450]\tLoss: 653.7931\n",
      "Training Epoch: 9 [22000/36450]\tLoss: 603.6991\n",
      "Training Epoch: 9 [22050/36450]\tLoss: 576.0010\n",
      "Training Epoch: 9 [22100/36450]\tLoss: 577.5082\n",
      "Training Epoch: 9 [22150/36450]\tLoss: 584.3046\n",
      "Training Epoch: 9 [22200/36450]\tLoss: 590.3313\n",
      "Training Epoch: 9 [22250/36450]\tLoss: 574.5762\n",
      "Training Epoch: 9 [22300/36450]\tLoss: 606.2071\n",
      "Training Epoch: 9 [22350/36450]\tLoss: 613.8035\n",
      "Training Epoch: 9 [22400/36450]\tLoss: 595.0106\n",
      "Training Epoch: 9 [22450/36450]\tLoss: 615.5784\n",
      "Training Epoch: 9 [22500/36450]\tLoss: 630.2625\n",
      "Training Epoch: 9 [22550/36450]\tLoss: 572.4818\n",
      "Training Epoch: 9 [22600/36450]\tLoss: 584.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [22650/36450]\tLoss: 644.8007\n",
      "Training Epoch: 9 [22700/36450]\tLoss: 581.2634\n",
      "Training Epoch: 9 [22750/36450]\tLoss: 634.7593\n",
      "Training Epoch: 9 [22800/36450]\tLoss: 581.5778\n",
      "Training Epoch: 9 [22850/36450]\tLoss: 597.0049\n",
      "Training Epoch: 9 [22900/36450]\tLoss: 611.7433\n",
      "Training Epoch: 9 [22950/36450]\tLoss: 602.5179\n",
      "Training Epoch: 9 [23000/36450]\tLoss: 596.9655\n",
      "Training Epoch: 9 [23050/36450]\tLoss: 595.1240\n",
      "Training Epoch: 9 [23100/36450]\tLoss: 618.9124\n",
      "Training Epoch: 9 [23150/36450]\tLoss: 608.8954\n",
      "Training Epoch: 9 [23200/36450]\tLoss: 620.4734\n",
      "Training Epoch: 9 [23250/36450]\tLoss: 612.5702\n",
      "Training Epoch: 9 [23300/36450]\tLoss: 604.5951\n",
      "Training Epoch: 9 [23350/36450]\tLoss: 604.8292\n",
      "Training Epoch: 9 [23400/36450]\tLoss: 622.1160\n",
      "Training Epoch: 9 [23450/36450]\tLoss: 588.7429\n",
      "Training Epoch: 9 [23500/36450]\tLoss: 600.4619\n",
      "Training Epoch: 9 [23550/36450]\tLoss: 616.8013\n",
      "Training Epoch: 9 [23600/36450]\tLoss: 612.8530\n",
      "Training Epoch: 9 [23650/36450]\tLoss: 612.5749\n",
      "Training Epoch: 9 [23700/36450]\tLoss: 624.6729\n",
      "Training Epoch: 9 [23750/36450]\tLoss: 599.5952\n",
      "Training Epoch: 9 [23800/36450]\tLoss: 609.7225\n",
      "Training Epoch: 9 [23850/36450]\tLoss: 604.6039\n",
      "Training Epoch: 9 [23900/36450]\tLoss: 603.5359\n",
      "Training Epoch: 9 [23950/36450]\tLoss: 606.2917\n",
      "Training Epoch: 9 [24000/36450]\tLoss: 618.8602\n",
      "Training Epoch: 9 [24050/36450]\tLoss: 614.4199\n",
      "Training Epoch: 9 [24100/36450]\tLoss: 615.0055\n",
      "Training Epoch: 9 [24150/36450]\tLoss: 633.4666\n",
      "Training Epoch: 9 [24200/36450]\tLoss: 612.1089\n",
      "Training Epoch: 9 [24250/36450]\tLoss: 629.0165\n",
      "Training Epoch: 9 [24300/36450]\tLoss: 588.6005\n",
      "Training Epoch: 9 [24350/36450]\tLoss: 601.3038\n",
      "Training Epoch: 9 [24400/36450]\tLoss: 578.9960\n",
      "Training Epoch: 9 [24450/36450]\tLoss: 583.2409\n",
      "Training Epoch: 9 [24500/36450]\tLoss: 575.4316\n",
      "Training Epoch: 9 [24550/36450]\tLoss: 580.3183\n",
      "Training Epoch: 9 [24600/36450]\tLoss: 596.5782\n",
      "Training Epoch: 9 [24650/36450]\tLoss: 582.4192\n",
      "Training Epoch: 9 [24700/36450]\tLoss: 612.8320\n",
      "Training Epoch: 9 [24750/36450]\tLoss: 629.6567\n",
      "Training Epoch: 9 [24800/36450]\tLoss: 617.3528\n",
      "Training Epoch: 9 [24850/36450]\tLoss: 625.7630\n",
      "Training Epoch: 9 [24900/36450]\tLoss: 588.9165\n",
      "Training Epoch: 9 [24950/36450]\tLoss: 609.4536\n",
      "Training Epoch: 9 [25000/36450]\tLoss: 612.6655\n",
      "Training Epoch: 9 [25050/36450]\tLoss: 597.1377\n",
      "Training Epoch: 9 [25100/36450]\tLoss: 596.8705\n",
      "Training Epoch: 9 [25150/36450]\tLoss: 620.8319\n",
      "Training Epoch: 9 [25200/36450]\tLoss: 569.6497\n",
      "Training Epoch: 9 [25250/36450]\tLoss: 572.2690\n",
      "Training Epoch: 9 [25300/36450]\tLoss: 583.4542\n",
      "Training Epoch: 9 [25350/36450]\tLoss: 553.9081\n",
      "Training Epoch: 9 [25400/36450]\tLoss: 611.0247\n",
      "Training Epoch: 9 [25450/36450]\tLoss: 578.1041\n",
      "Training Epoch: 9 [25500/36450]\tLoss: 584.9933\n",
      "Training Epoch: 9 [25550/36450]\tLoss: 636.0058\n",
      "Training Epoch: 9 [25600/36450]\tLoss: 594.7275\n",
      "Training Epoch: 9 [25650/36450]\tLoss: 595.1557\n",
      "Training Epoch: 9 [25700/36450]\tLoss: 590.4681\n",
      "Training Epoch: 9 [25750/36450]\tLoss: 593.4602\n",
      "Training Epoch: 9 [25800/36450]\tLoss: 557.3456\n",
      "Training Epoch: 9 [25850/36450]\tLoss: 587.5173\n",
      "Training Epoch: 9 [25900/36450]\tLoss: 614.1395\n",
      "Training Epoch: 9 [25950/36450]\tLoss: 581.5251\n",
      "Training Epoch: 9 [26000/36450]\tLoss: 604.8204\n",
      "Training Epoch: 9 [26050/36450]\tLoss: 588.9112\n",
      "Training Epoch: 9 [26100/36450]\tLoss: 609.3141\n",
      "Training Epoch: 9 [26150/36450]\tLoss: 604.4871\n",
      "Training Epoch: 9 [26200/36450]\tLoss: 627.8299\n",
      "Training Epoch: 9 [26250/36450]\tLoss: 599.6997\n",
      "Training Epoch: 9 [26300/36450]\tLoss: 574.8548\n",
      "Training Epoch: 9 [26350/36450]\tLoss: 599.0635\n",
      "Training Epoch: 9 [26400/36450]\tLoss: 585.4583\n",
      "Training Epoch: 9 [26450/36450]\tLoss: 596.0272\n",
      "Training Epoch: 9 [26500/36450]\tLoss: 593.4282\n",
      "Training Epoch: 9 [26550/36450]\tLoss: 574.0201\n",
      "Training Epoch: 9 [26600/36450]\tLoss: 567.8533\n",
      "Training Epoch: 9 [26650/36450]\tLoss: 630.3619\n",
      "Training Epoch: 9 [26700/36450]\tLoss: 576.2333\n",
      "Training Epoch: 9 [26750/36450]\tLoss: 609.7821\n",
      "Training Epoch: 9 [26800/36450]\tLoss: 581.7895\n",
      "Training Epoch: 9 [26850/36450]\tLoss: 652.0978\n",
      "Training Epoch: 9 [26900/36450]\tLoss: 607.5701\n",
      "Training Epoch: 9 [26950/36450]\tLoss: 593.2005\n",
      "Training Epoch: 9 [27000/36450]\tLoss: 612.1933\n",
      "Training Epoch: 9 [27050/36450]\tLoss: 615.6880\n",
      "Training Epoch: 9 [27100/36450]\tLoss: 606.8729\n",
      "Training Epoch: 9 [27150/36450]\tLoss: 600.7626\n",
      "Training Epoch: 9 [27200/36450]\tLoss: 630.1990\n",
      "Training Epoch: 9 [27250/36450]\tLoss: 580.8185\n",
      "Training Epoch: 9 [27300/36450]\tLoss: 601.5142\n",
      "Training Epoch: 9 [27350/36450]\tLoss: 599.7336\n",
      "Training Epoch: 9 [27400/36450]\tLoss: 587.7234\n",
      "Training Epoch: 9 [27450/36450]\tLoss: 597.8651\n",
      "Training Epoch: 9 [27500/36450]\tLoss: 586.9384\n",
      "Training Epoch: 9 [27550/36450]\tLoss: 628.0347\n",
      "Training Epoch: 9 [27600/36450]\tLoss: 598.9438\n",
      "Training Epoch: 9 [27650/36450]\tLoss: 567.9575\n",
      "Training Epoch: 9 [27700/36450]\tLoss: 599.0316\n",
      "Training Epoch: 9 [27750/36450]\tLoss: 607.6101\n",
      "Training Epoch: 9 [27800/36450]\tLoss: 585.4217\n",
      "Training Epoch: 9 [27850/36450]\tLoss: 589.0829\n",
      "Training Epoch: 9 [27900/36450]\tLoss: 611.8024\n",
      "Training Epoch: 9 [27950/36450]\tLoss: 608.9790\n",
      "Training Epoch: 9 [28000/36450]\tLoss: 597.3959\n",
      "Training Epoch: 9 [28050/36450]\tLoss: 608.5924\n",
      "Training Epoch: 9 [28100/36450]\tLoss: 620.0267\n",
      "Training Epoch: 9 [28150/36450]\tLoss: 605.3494\n",
      "Training Epoch: 9 [28200/36450]\tLoss: 569.4009\n",
      "Training Epoch: 9 [28250/36450]\tLoss: 605.8299\n",
      "Training Epoch: 9 [28300/36450]\tLoss: 622.1854\n",
      "Training Epoch: 9 [28350/36450]\tLoss: 601.0458\n",
      "Training Epoch: 9 [28400/36450]\tLoss: 576.2966\n",
      "Training Epoch: 9 [28450/36450]\tLoss: 612.3142\n",
      "Training Epoch: 9 [28500/36450]\tLoss: 603.6361\n",
      "Training Epoch: 9 [28550/36450]\tLoss: 617.3819\n",
      "Training Epoch: 9 [28600/36450]\tLoss: 598.4357\n",
      "Training Epoch: 9 [28650/36450]\tLoss: 594.0366\n",
      "Training Epoch: 9 [28700/36450]\tLoss: 585.6653\n",
      "Training Epoch: 9 [28750/36450]\tLoss: 602.9855\n",
      "Training Epoch: 9 [28800/36450]\tLoss: 592.3499\n",
      "Training Epoch: 9 [28850/36450]\tLoss: 592.4213\n",
      "Training Epoch: 9 [28900/36450]\tLoss: 573.6853\n",
      "Training Epoch: 9 [28950/36450]\tLoss: 596.7583\n",
      "Training Epoch: 9 [29000/36450]\tLoss: 581.3382\n",
      "Training Epoch: 9 [29050/36450]\tLoss: 596.1878\n",
      "Training Epoch: 9 [29100/36450]\tLoss: 639.5793\n",
      "Training Epoch: 9 [29150/36450]\tLoss: 610.1769\n",
      "Training Epoch: 9 [29200/36450]\tLoss: 597.0543\n",
      "Training Epoch: 9 [29250/36450]\tLoss: 614.9548\n",
      "Training Epoch: 9 [29300/36450]\tLoss: 610.1401\n",
      "Training Epoch: 9 [29350/36450]\tLoss: 616.5766\n",
      "Training Epoch: 9 [29400/36450]\tLoss: 590.4040\n",
      "Training Epoch: 9 [29450/36450]\tLoss: 581.5073\n",
      "Training Epoch: 9 [29500/36450]\tLoss: 566.2933\n",
      "Training Epoch: 9 [29550/36450]\tLoss: 632.5162\n",
      "Training Epoch: 9 [29600/36450]\tLoss: 578.5267\n",
      "Training Epoch: 9 [29650/36450]\tLoss: 570.8711\n",
      "Training Epoch: 9 [29700/36450]\tLoss: 600.9714\n",
      "Training Epoch: 9 [29750/36450]\tLoss: 589.6744\n",
      "Training Epoch: 9 [29800/36450]\tLoss: 581.0666\n",
      "Training Epoch: 9 [29850/36450]\tLoss: 619.4358\n",
      "Training Epoch: 9 [29900/36450]\tLoss: 600.1088\n",
      "Training Epoch: 9 [29950/36450]\tLoss: 580.2628\n",
      "Training Epoch: 9 [30000/36450]\tLoss: 600.2364\n",
      "Training Epoch: 9 [30050/36450]\tLoss: 629.8243\n",
      "Training Epoch: 9 [30100/36450]\tLoss: 582.0164\n",
      "Training Epoch: 9 [30150/36450]\tLoss: 642.3711\n",
      "Training Epoch: 9 [30200/36450]\tLoss: 632.9857\n",
      "Training Epoch: 9 [30250/36450]\tLoss: 612.7551\n",
      "Training Epoch: 9 [30300/36450]\tLoss: 616.0211\n",
      "Training Epoch: 9 [30350/36450]\tLoss: 631.7385\n",
      "Training Epoch: 9 [30400/36450]\tLoss: 624.4909\n",
      "Training Epoch: 9 [30450/36450]\tLoss: 591.2347\n",
      "Training Epoch: 9 [30500/36450]\tLoss: 603.0173\n",
      "Training Epoch: 9 [30550/36450]\tLoss: 610.3289\n",
      "Training Epoch: 9 [30600/36450]\tLoss: 595.1071\n",
      "Training Epoch: 9 [30650/36450]\tLoss: 615.6007\n",
      "Training Epoch: 9 [30700/36450]\tLoss: 592.8131\n",
      "Training Epoch: 9 [30750/36450]\tLoss: 584.7903\n",
      "Training Epoch: 9 [30800/36450]\tLoss: 616.5232\n",
      "Training Epoch: 9 [30850/36450]\tLoss: 623.2656\n",
      "Training Epoch: 9 [30900/36450]\tLoss: 610.9948\n",
      "Training Epoch: 9 [30950/36450]\tLoss: 662.4543\n",
      "Training Epoch: 9 [31000/36450]\tLoss: 650.2523\n",
      "Training Epoch: 9 [31050/36450]\tLoss: 606.9778\n",
      "Training Epoch: 9 [31100/36450]\tLoss: 641.3630\n",
      "Training Epoch: 9 [31150/36450]\tLoss: 582.8112\n",
      "Training Epoch: 9 [31200/36450]\tLoss: 626.5043\n",
      "Training Epoch: 9 [31250/36450]\tLoss: 614.3039\n",
      "Training Epoch: 9 [31300/36450]\tLoss: 631.8394\n",
      "Training Epoch: 9 [31350/36450]\tLoss: 586.9738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [31400/36450]\tLoss: 639.4973\n",
      "Training Epoch: 9 [31450/36450]\tLoss: 608.6707\n",
      "Training Epoch: 9 [31500/36450]\tLoss: 617.1381\n",
      "Training Epoch: 9 [31550/36450]\tLoss: 600.1783\n",
      "Training Epoch: 9 [31600/36450]\tLoss: 582.4333\n",
      "Training Epoch: 9 [31650/36450]\tLoss: 591.9879\n",
      "Training Epoch: 9 [31700/36450]\tLoss: 579.5723\n",
      "Training Epoch: 9 [31750/36450]\tLoss: 601.9918\n",
      "Training Epoch: 9 [31800/36450]\tLoss: 609.9033\n",
      "Training Epoch: 9 [31850/36450]\tLoss: 588.9542\n",
      "Training Epoch: 9 [31900/36450]\tLoss: 559.7318\n",
      "Training Epoch: 9 [31950/36450]\tLoss: 621.9785\n",
      "Training Epoch: 9 [32000/36450]\tLoss: 618.0235\n",
      "Training Epoch: 9 [32050/36450]\tLoss: 584.7830\n",
      "Training Epoch: 9 [32100/36450]\tLoss: 606.1693\n",
      "Training Epoch: 9 [32150/36450]\tLoss: 591.2398\n",
      "Training Epoch: 9 [32200/36450]\tLoss: 586.5372\n",
      "Training Epoch: 9 [32250/36450]\tLoss: 560.1668\n",
      "Training Epoch: 9 [32300/36450]\tLoss: 606.5380\n",
      "Training Epoch: 9 [32350/36450]\tLoss: 597.0433\n",
      "Training Epoch: 9 [32400/36450]\tLoss: 603.6350\n",
      "Training Epoch: 9 [32450/36450]\tLoss: 577.0485\n",
      "Training Epoch: 9 [32500/36450]\tLoss: 574.7098\n",
      "Training Epoch: 9 [32550/36450]\tLoss: 586.5379\n",
      "Training Epoch: 9 [32600/36450]\tLoss: 573.5053\n",
      "Training Epoch: 9 [32650/36450]\tLoss: 577.3172\n",
      "Training Epoch: 9 [32700/36450]\tLoss: 581.5215\n",
      "Training Epoch: 9 [32750/36450]\tLoss: 614.7987\n",
      "Training Epoch: 9 [32800/36450]\tLoss: 595.7769\n",
      "Training Epoch: 9 [32850/36450]\tLoss: 612.9063\n",
      "Training Epoch: 9 [32900/36450]\tLoss: 600.6852\n",
      "Training Epoch: 9 [32950/36450]\tLoss: 592.5768\n",
      "Training Epoch: 9 [33000/36450]\tLoss: 575.2319\n",
      "Training Epoch: 9 [33050/36450]\tLoss: 618.7102\n",
      "Training Epoch: 9 [33100/36450]\tLoss: 630.5060\n",
      "Training Epoch: 9 [33150/36450]\tLoss: 625.0085\n",
      "Training Epoch: 9 [33200/36450]\tLoss: 588.6788\n",
      "Training Epoch: 9 [33250/36450]\tLoss: 643.6591\n",
      "Training Epoch: 9 [33300/36450]\tLoss: 555.5497\n",
      "Training Epoch: 9 [33350/36450]\tLoss: 560.8944\n",
      "Training Epoch: 9 [33400/36450]\tLoss: 589.7471\n",
      "Training Epoch: 9 [33450/36450]\tLoss: 562.7753\n",
      "Training Epoch: 9 [33500/36450]\tLoss: 589.6777\n",
      "Training Epoch: 9 [33550/36450]\tLoss: 573.3318\n",
      "Training Epoch: 9 [33600/36450]\tLoss: 575.1255\n",
      "Training Epoch: 9 [33650/36450]\tLoss: 560.8682\n",
      "Training Epoch: 9 [33700/36450]\tLoss: 605.2307\n",
      "Training Epoch: 9 [33750/36450]\tLoss: 603.7305\n",
      "Training Epoch: 9 [33800/36450]\tLoss: 628.7851\n",
      "Training Epoch: 9 [33850/36450]\tLoss: 594.7993\n",
      "Training Epoch: 9 [33900/36450]\tLoss: 604.5298\n",
      "Training Epoch: 9 [33950/36450]\tLoss: 588.7630\n",
      "Training Epoch: 9 [34000/36450]\tLoss: 538.9073\n",
      "Training Epoch: 9 [34050/36450]\tLoss: 580.2271\n",
      "Training Epoch: 9 [34100/36450]\tLoss: 607.1199\n",
      "Training Epoch: 9 [34150/36450]\tLoss: 579.0553\n",
      "Training Epoch: 9 [34200/36450]\tLoss: 585.3826\n",
      "Training Epoch: 9 [34250/36450]\tLoss: 592.5381\n",
      "Training Epoch: 9 [34300/36450]\tLoss: 606.8563\n",
      "Training Epoch: 9 [34350/36450]\tLoss: 591.7004\n",
      "Training Epoch: 9 [34400/36450]\tLoss: 598.0393\n",
      "Training Epoch: 9 [34450/36450]\tLoss: 602.3248\n",
      "Training Epoch: 9 [34500/36450]\tLoss: 611.0252\n",
      "Training Epoch: 9 [34550/36450]\tLoss: 630.1263\n",
      "Training Epoch: 9 [34600/36450]\tLoss: 600.4153\n",
      "Training Epoch: 9 [34650/36450]\tLoss: 545.1511\n",
      "Training Epoch: 9 [34700/36450]\tLoss: 628.0240\n",
      "Training Epoch: 9 [34750/36450]\tLoss: 589.6927\n",
      "Training Epoch: 9 [34800/36450]\tLoss: 577.6996\n",
      "Training Epoch: 9 [34850/36450]\tLoss: 588.6784\n",
      "Training Epoch: 9 [34900/36450]\tLoss: 642.2358\n",
      "Training Epoch: 9 [34950/36450]\tLoss: 565.0738\n",
      "Training Epoch: 9 [35000/36450]\tLoss: 609.7270\n",
      "Training Epoch: 9 [35050/36450]\tLoss: 618.3713\n",
      "Training Epoch: 9 [35100/36450]\tLoss: 568.7988\n",
      "Training Epoch: 9 [35150/36450]\tLoss: 615.5711\n",
      "Training Epoch: 9 [35200/36450]\tLoss: 584.5089\n",
      "Training Epoch: 9 [35250/36450]\tLoss: 594.1932\n",
      "Training Epoch: 9 [35300/36450]\tLoss: 590.4157\n",
      "Training Epoch: 9 [35350/36450]\tLoss: 564.7809\n",
      "Training Epoch: 9 [35400/36450]\tLoss: 529.4653\n",
      "Training Epoch: 9 [35450/36450]\tLoss: 628.5271\n",
      "Training Epoch: 9 [35500/36450]\tLoss: 578.0239\n",
      "Training Epoch: 9 [35550/36450]\tLoss: 580.1202\n",
      "Training Epoch: 9 [35600/36450]\tLoss: 612.9225\n",
      "Training Epoch: 9 [35650/36450]\tLoss: 612.7412\n",
      "Training Epoch: 9 [35700/36450]\tLoss: 572.5911\n",
      "Training Epoch: 9 [35750/36450]\tLoss: 584.7923\n",
      "Training Epoch: 9 [35800/36450]\tLoss: 596.8901\n",
      "Training Epoch: 9 [35850/36450]\tLoss: 601.4704\n",
      "Training Epoch: 9 [35900/36450]\tLoss: 608.6256\n",
      "Training Epoch: 9 [35950/36450]\tLoss: 600.2169\n",
      "Training Epoch: 9 [36000/36450]\tLoss: 569.7223\n",
      "Training Epoch: 9 [36050/36450]\tLoss: 552.8552\n",
      "Training Epoch: 9 [36100/36450]\tLoss: 579.6410\n",
      "Training Epoch: 9 [36150/36450]\tLoss: 629.0262\n",
      "Training Epoch: 9 [36200/36450]\tLoss: 607.9852\n",
      "Training Epoch: 9 [36250/36450]\tLoss: 635.3406\n",
      "Training Epoch: 9 [36300/36450]\tLoss: 583.5886\n",
      "Training Epoch: 9 [36350/36450]\tLoss: 578.8616\n",
      "Training Epoch: 9 [36400/36450]\tLoss: 592.8914\n",
      "Training Epoch: 9 [36450/36450]\tLoss: 567.1896\n",
      "Training Epoch: 9 [4050/4050]\tLoss: 299.1040\n",
      "Training Epoch: 10 [50/36450]\tLoss: 559.8959\n",
      "Training Epoch: 10 [100/36450]\tLoss: 589.9950\n",
      "Training Epoch: 10 [150/36450]\tLoss: 612.5657\n",
      "Training Epoch: 10 [200/36450]\tLoss: 595.2145\n",
      "Training Epoch: 10 [250/36450]\tLoss: 573.4730\n",
      "Training Epoch: 10 [300/36450]\tLoss: 572.5417\n",
      "Training Epoch: 10 [350/36450]\tLoss: 600.0963\n",
      "Training Epoch: 10 [400/36450]\tLoss: 577.4559\n",
      "Training Epoch: 10 [450/36450]\tLoss: 558.8916\n",
      "Training Epoch: 10 [500/36450]\tLoss: 619.9409\n",
      "Training Epoch: 10 [550/36450]\tLoss: 612.1369\n",
      "Training Epoch: 10 [600/36450]\tLoss: 623.6213\n",
      "Training Epoch: 10 [650/36450]\tLoss: 562.8883\n",
      "Training Epoch: 10 [700/36450]\tLoss: 606.1681\n",
      "Training Epoch: 10 [750/36450]\tLoss: 574.4613\n",
      "Training Epoch: 10 [800/36450]\tLoss: 608.9981\n",
      "Training Epoch: 10 [850/36450]\tLoss: 570.5032\n",
      "Training Epoch: 10 [900/36450]\tLoss: 593.2525\n",
      "Training Epoch: 10 [950/36450]\tLoss: 572.1596\n",
      "Training Epoch: 10 [1000/36450]\tLoss: 634.5982\n",
      "Training Epoch: 10 [1050/36450]\tLoss: 634.5335\n",
      "Training Epoch: 10 [1100/36450]\tLoss: 611.9244\n",
      "Training Epoch: 10 [1150/36450]\tLoss: 614.8210\n",
      "Training Epoch: 10 [1200/36450]\tLoss: 567.6960\n",
      "Training Epoch: 10 [1250/36450]\tLoss: 613.0143\n",
      "Training Epoch: 10 [1300/36450]\tLoss: 601.2619\n",
      "Training Epoch: 10 [1350/36450]\tLoss: 584.5488\n",
      "Training Epoch: 10 [1400/36450]\tLoss: 611.1965\n",
      "Training Epoch: 10 [1450/36450]\tLoss: 581.1740\n",
      "Training Epoch: 10 [1500/36450]\tLoss: 603.4975\n",
      "Training Epoch: 10 [1550/36450]\tLoss: 606.0296\n",
      "Training Epoch: 10 [1600/36450]\tLoss: 598.2988\n",
      "Training Epoch: 10 [1650/36450]\tLoss: 601.1913\n",
      "Training Epoch: 10 [1700/36450]\tLoss: 603.4224\n",
      "Training Epoch: 10 [1750/36450]\tLoss: 595.3481\n",
      "Training Epoch: 10 [1800/36450]\tLoss: 619.9012\n",
      "Training Epoch: 10 [1850/36450]\tLoss: 572.7899\n",
      "Training Epoch: 10 [1900/36450]\tLoss: 623.5792\n",
      "Training Epoch: 10 [1950/36450]\tLoss: 625.5338\n",
      "Training Epoch: 10 [2000/36450]\tLoss: 609.9893\n",
      "Training Epoch: 10 [2050/36450]\tLoss: 571.0493\n",
      "Training Epoch: 10 [2100/36450]\tLoss: 593.3157\n",
      "Training Epoch: 10 [2150/36450]\tLoss: 592.4788\n",
      "Training Epoch: 10 [2200/36450]\tLoss: 571.8733\n",
      "Training Epoch: 10 [2250/36450]\tLoss: 610.4682\n",
      "Training Epoch: 10 [2300/36450]\tLoss: 576.7289\n",
      "Training Epoch: 10 [2350/36450]\tLoss: 599.8364\n",
      "Training Epoch: 10 [2400/36450]\tLoss: 571.1191\n",
      "Training Epoch: 10 [2450/36450]\tLoss: 585.1754\n",
      "Training Epoch: 10 [2500/36450]\tLoss: 583.0865\n",
      "Training Epoch: 10 [2550/36450]\tLoss: 619.7816\n",
      "Training Epoch: 10 [2600/36450]\tLoss: 602.6635\n",
      "Training Epoch: 10 [2650/36450]\tLoss: 577.3688\n",
      "Training Epoch: 10 [2700/36450]\tLoss: 596.1986\n",
      "Training Epoch: 10 [2750/36450]\tLoss: 547.4599\n",
      "Training Epoch: 10 [2800/36450]\tLoss: 542.0227\n",
      "Training Epoch: 10 [2850/36450]\tLoss: 552.7505\n",
      "Training Epoch: 10 [2900/36450]\tLoss: 589.8008\n",
      "Training Epoch: 10 [2950/36450]\tLoss: 587.3704\n",
      "Training Epoch: 10 [3000/36450]\tLoss: 598.2220\n",
      "Training Epoch: 10 [3050/36450]\tLoss: 607.3554\n",
      "Training Epoch: 10 [3100/36450]\tLoss: 598.8177\n",
      "Training Epoch: 10 [3150/36450]\tLoss: 613.8124\n",
      "Training Epoch: 10 [3200/36450]\tLoss: 617.5414\n",
      "Training Epoch: 10 [3250/36450]\tLoss: 581.4237\n",
      "Training Epoch: 10 [3300/36450]\tLoss: 594.6836\n",
      "Training Epoch: 10 [3350/36450]\tLoss: 626.5425\n",
      "Training Epoch: 10 [3400/36450]\tLoss: 595.1151\n",
      "Training Epoch: 10 [3450/36450]\tLoss: 537.2473\n",
      "Training Epoch: 10 [3500/36450]\tLoss: 578.1770\n",
      "Training Epoch: 10 [3550/36450]\tLoss: 612.5574\n",
      "Training Epoch: 10 [3600/36450]\tLoss: 596.1138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [3650/36450]\tLoss: 617.1625\n",
      "Training Epoch: 10 [3700/36450]\tLoss: 610.2055\n",
      "Training Epoch: 10 [3750/36450]\tLoss: 590.2380\n",
      "Training Epoch: 10 [3800/36450]\tLoss: 615.9122\n",
      "Training Epoch: 10 [3850/36450]\tLoss: 598.1546\n",
      "Training Epoch: 10 [3900/36450]\tLoss: 602.4269\n",
      "Training Epoch: 10 [3950/36450]\tLoss: 556.0463\n",
      "Training Epoch: 10 [4000/36450]\tLoss: 612.4788\n",
      "Training Epoch: 10 [4050/36450]\tLoss: 594.1685\n",
      "Training Epoch: 10 [4100/36450]\tLoss: 598.0540\n",
      "Training Epoch: 10 [4150/36450]\tLoss: 595.8457\n",
      "Training Epoch: 10 [4200/36450]\tLoss: 554.8917\n",
      "Training Epoch: 10 [4250/36450]\tLoss: 577.6760\n",
      "Training Epoch: 10 [4300/36450]\tLoss: 611.3153\n",
      "Training Epoch: 10 [4350/36450]\tLoss: 625.4724\n",
      "Training Epoch: 10 [4400/36450]\tLoss: 558.1666\n",
      "Training Epoch: 10 [4450/36450]\tLoss: 585.9501\n",
      "Training Epoch: 10 [4500/36450]\tLoss: 613.2350\n",
      "Training Epoch: 10 [4550/36450]\tLoss: 619.7313\n",
      "Training Epoch: 10 [4600/36450]\tLoss: 615.3024\n",
      "Training Epoch: 10 [4650/36450]\tLoss: 535.3554\n",
      "Training Epoch: 10 [4700/36450]\tLoss: 598.7801\n",
      "Training Epoch: 10 [4750/36450]\tLoss: 619.8120\n",
      "Training Epoch: 10 [4800/36450]\tLoss: 572.8668\n",
      "Training Epoch: 10 [4850/36450]\tLoss: 581.4594\n",
      "Training Epoch: 10 [4900/36450]\tLoss: 586.7250\n",
      "Training Epoch: 10 [4950/36450]\tLoss: 585.6147\n",
      "Training Epoch: 10 [5000/36450]\tLoss: 607.1724\n",
      "Training Epoch: 10 [5050/36450]\tLoss: 608.9321\n",
      "Training Epoch: 10 [5100/36450]\tLoss: 580.3757\n",
      "Training Epoch: 10 [5150/36450]\tLoss: 602.9184\n",
      "Training Epoch: 10 [5200/36450]\tLoss: 573.5110\n",
      "Training Epoch: 10 [5250/36450]\tLoss: 568.3299\n",
      "Training Epoch: 10 [5300/36450]\tLoss: 573.2342\n",
      "Training Epoch: 10 [5350/36450]\tLoss: 615.0245\n",
      "Training Epoch: 10 [5400/36450]\tLoss: 585.7898\n",
      "Training Epoch: 10 [5450/36450]\tLoss: 607.4792\n",
      "Training Epoch: 10 [5500/36450]\tLoss: 593.5142\n",
      "Training Epoch: 10 [5550/36450]\tLoss: 589.3375\n",
      "Training Epoch: 10 [5600/36450]\tLoss: 607.4944\n",
      "Training Epoch: 10 [5650/36450]\tLoss: 587.7759\n",
      "Training Epoch: 10 [5700/36450]\tLoss: 542.2961\n",
      "Training Epoch: 10 [5750/36450]\tLoss: 570.6193\n",
      "Training Epoch: 10 [5800/36450]\tLoss: 599.8023\n",
      "Training Epoch: 10 [5850/36450]\tLoss: 612.8475\n",
      "Training Epoch: 10 [5900/36450]\tLoss: 576.0692\n",
      "Training Epoch: 10 [5950/36450]\tLoss: 568.0436\n",
      "Training Epoch: 10 [6000/36450]\tLoss: 625.2842\n",
      "Training Epoch: 10 [6050/36450]\tLoss: 598.0729\n",
      "Training Epoch: 10 [6100/36450]\tLoss: 607.2045\n",
      "Training Epoch: 10 [6150/36450]\tLoss: 574.8118\n",
      "Training Epoch: 10 [6200/36450]\tLoss: 604.8314\n",
      "Training Epoch: 10 [6250/36450]\tLoss: 595.4108\n",
      "Training Epoch: 10 [6300/36450]\tLoss: 600.6964\n",
      "Training Epoch: 10 [6350/36450]\tLoss: 589.1705\n",
      "Training Epoch: 10 [6400/36450]\tLoss: 571.2585\n",
      "Training Epoch: 10 [6450/36450]\tLoss: 607.0563\n",
      "Training Epoch: 10 [6500/36450]\tLoss: 601.2364\n",
      "Training Epoch: 10 [6550/36450]\tLoss: 596.1215\n",
      "Training Epoch: 10 [6600/36450]\tLoss: 596.9530\n",
      "Training Epoch: 10 [6650/36450]\tLoss: 576.8182\n",
      "Training Epoch: 10 [6700/36450]\tLoss: 577.3079\n",
      "Training Epoch: 10 [6750/36450]\tLoss: 584.2588\n",
      "Training Epoch: 10 [6800/36450]\tLoss: 575.0760\n",
      "Training Epoch: 10 [6850/36450]\tLoss: 603.3005\n",
      "Training Epoch: 10 [6900/36450]\tLoss: 587.6993\n",
      "Training Epoch: 10 [6950/36450]\tLoss: 601.4915\n",
      "Training Epoch: 10 [7000/36450]\tLoss: 633.6281\n",
      "Training Epoch: 10 [7050/36450]\tLoss: 583.1948\n",
      "Training Epoch: 10 [7100/36450]\tLoss: 651.1213\n",
      "Training Epoch: 10 [7150/36450]\tLoss: 572.6368\n",
      "Training Epoch: 10 [7200/36450]\tLoss: 577.9640\n",
      "Training Epoch: 10 [7250/36450]\tLoss: 636.4593\n",
      "Training Epoch: 10 [7300/36450]\tLoss: 572.2077\n",
      "Training Epoch: 10 [7350/36450]\tLoss: 562.6024\n",
      "Training Epoch: 10 [7400/36450]\tLoss: 599.9186\n",
      "Training Epoch: 10 [7450/36450]\tLoss: 604.9462\n",
      "Training Epoch: 10 [7500/36450]\tLoss: 562.6393\n",
      "Training Epoch: 10 [7550/36450]\tLoss: 594.4978\n",
      "Training Epoch: 10 [7600/36450]\tLoss: 596.7861\n",
      "Training Epoch: 10 [7650/36450]\tLoss: 561.8201\n",
      "Training Epoch: 10 [7700/36450]\tLoss: 592.8203\n",
      "Training Epoch: 10 [7750/36450]\tLoss: 622.0879\n",
      "Training Epoch: 10 [7800/36450]\tLoss: 605.2676\n",
      "Training Epoch: 10 [7850/36450]\tLoss: 602.1537\n",
      "Training Epoch: 10 [7900/36450]\tLoss: 573.9775\n",
      "Training Epoch: 10 [7950/36450]\tLoss: 592.9592\n",
      "Training Epoch: 10 [8000/36450]\tLoss: 567.9340\n",
      "Training Epoch: 10 [8050/36450]\tLoss: 573.2491\n",
      "Training Epoch: 10 [8100/36450]\tLoss: 629.1880\n",
      "Training Epoch: 10 [8150/36450]\tLoss: 588.6681\n",
      "Training Epoch: 10 [8200/36450]\tLoss: 619.3475\n",
      "Training Epoch: 10 [8250/36450]\tLoss: 614.5193\n",
      "Training Epoch: 10 [8300/36450]\tLoss: 594.4980\n",
      "Training Epoch: 10 [8350/36450]\tLoss: 605.1286\n",
      "Training Epoch: 10 [8400/36450]\tLoss: 600.9271\n",
      "Training Epoch: 10 [8450/36450]\tLoss: 585.5596\n",
      "Training Epoch: 10 [8500/36450]\tLoss: 616.8213\n",
      "Training Epoch: 10 [8550/36450]\tLoss: 625.4686\n",
      "Training Epoch: 10 [8600/36450]\tLoss: 566.1965\n",
      "Training Epoch: 10 [8650/36450]\tLoss: 573.0461\n",
      "Training Epoch: 10 [8700/36450]\tLoss: 600.4366\n",
      "Training Epoch: 10 [8750/36450]\tLoss: 632.8921\n",
      "Training Epoch: 10 [8800/36450]\tLoss: 609.0005\n",
      "Training Epoch: 10 [8850/36450]\tLoss: 577.7734\n",
      "Training Epoch: 10 [8900/36450]\tLoss: 560.7986\n",
      "Training Epoch: 10 [8950/36450]\tLoss: 585.9015\n",
      "Training Epoch: 10 [9000/36450]\tLoss: 586.9251\n",
      "Training Epoch: 10 [9050/36450]\tLoss: 562.9240\n",
      "Training Epoch: 10 [9100/36450]\tLoss: 549.7646\n",
      "Training Epoch: 10 [9150/36450]\tLoss: 558.6425\n",
      "Training Epoch: 10 [9200/36450]\tLoss: 602.2515\n",
      "Training Epoch: 10 [9250/36450]\tLoss: 627.8271\n",
      "Training Epoch: 10 [9300/36450]\tLoss: 575.3602\n",
      "Training Epoch: 10 [9350/36450]\tLoss: 592.9397\n",
      "Training Epoch: 10 [9400/36450]\tLoss: 567.1624\n",
      "Training Epoch: 10 [9450/36450]\tLoss: 571.8057\n",
      "Training Epoch: 10 [9500/36450]\tLoss: 550.2274\n",
      "Training Epoch: 10 [9550/36450]\tLoss: 596.7539\n",
      "Training Epoch: 10 [9600/36450]\tLoss: 581.6670\n",
      "Training Epoch: 10 [9650/36450]\tLoss: 557.8990\n",
      "Training Epoch: 10 [9700/36450]\tLoss: 588.9056\n",
      "Training Epoch: 10 [9750/36450]\tLoss: 593.7888\n",
      "Training Epoch: 10 [9800/36450]\tLoss: 583.3523\n",
      "Training Epoch: 10 [9850/36450]\tLoss: 554.4369\n",
      "Training Epoch: 10 [9900/36450]\tLoss: 580.6562\n",
      "Training Epoch: 10 [9950/36450]\tLoss: 575.4623\n",
      "Training Epoch: 10 [10000/36450]\tLoss: 595.2366\n",
      "Training Epoch: 10 [10050/36450]\tLoss: 602.7055\n",
      "Training Epoch: 10 [10100/36450]\tLoss: 524.5508\n",
      "Training Epoch: 10 [10150/36450]\tLoss: 574.6346\n",
      "Training Epoch: 10 [10200/36450]\tLoss: 571.4814\n",
      "Training Epoch: 10 [10250/36450]\tLoss: 582.9158\n",
      "Training Epoch: 10 [10300/36450]\tLoss: 603.8257\n",
      "Training Epoch: 10 [10350/36450]\tLoss: 560.1661\n",
      "Training Epoch: 10 [10400/36450]\tLoss: 557.1645\n",
      "Training Epoch: 10 [10450/36450]\tLoss: 582.7713\n",
      "Training Epoch: 10 [10500/36450]\tLoss: 590.0677\n",
      "Training Epoch: 10 [10550/36450]\tLoss: 595.4944\n",
      "Training Epoch: 10 [10600/36450]\tLoss: 590.2197\n",
      "Training Epoch: 10 [10650/36450]\tLoss: 590.7026\n",
      "Training Epoch: 10 [10700/36450]\tLoss: 561.1223\n",
      "Training Epoch: 10 [10750/36450]\tLoss: 575.4542\n",
      "Training Epoch: 10 [10800/36450]\tLoss: 581.7662\n",
      "Training Epoch: 10 [10850/36450]\tLoss: 561.8578\n",
      "Training Epoch: 10 [10900/36450]\tLoss: 593.9791\n",
      "Training Epoch: 10 [10950/36450]\tLoss: 609.1328\n",
      "Training Epoch: 10 [11000/36450]\tLoss: 586.5068\n",
      "Training Epoch: 10 [11050/36450]\tLoss: 561.5787\n",
      "Training Epoch: 10 [11100/36450]\tLoss: 591.3079\n",
      "Training Epoch: 10 [11150/36450]\tLoss: 567.2045\n",
      "Training Epoch: 10 [11200/36450]\tLoss: 575.6430\n",
      "Training Epoch: 10 [11250/36450]\tLoss: 602.0984\n",
      "Training Epoch: 10 [11300/36450]\tLoss: 624.8751\n",
      "Training Epoch: 10 [11350/36450]\tLoss: 518.6646\n",
      "Training Epoch: 10 [11400/36450]\tLoss: 603.9099\n",
      "Training Epoch: 10 [11450/36450]\tLoss: 582.3113\n",
      "Training Epoch: 10 [11500/36450]\tLoss: 586.1068\n",
      "Training Epoch: 10 [11550/36450]\tLoss: 551.9099\n",
      "Training Epoch: 10 [11600/36450]\tLoss: 624.3995\n",
      "Training Epoch: 10 [11650/36450]\tLoss: 599.0963\n",
      "Training Epoch: 10 [11700/36450]\tLoss: 599.7542\n",
      "Training Epoch: 10 [11750/36450]\tLoss: 600.7017\n",
      "Training Epoch: 10 [11800/36450]\tLoss: 667.0845\n",
      "Training Epoch: 10 [11850/36450]\tLoss: 630.2619\n",
      "Training Epoch: 10 [11900/36450]\tLoss: 679.0925\n",
      "Training Epoch: 10 [11950/36450]\tLoss: 635.9666\n",
      "Training Epoch: 10 [12000/36450]\tLoss: 637.9237\n",
      "Training Epoch: 10 [12050/36450]\tLoss: 643.3048\n",
      "Training Epoch: 10 [12100/36450]\tLoss: 618.1700\n",
      "Training Epoch: 10 [12150/36450]\tLoss: 646.6604\n",
      "Training Epoch: 10 [12200/36450]\tLoss: 580.6091\n",
      "Training Epoch: 10 [12250/36450]\tLoss: 574.4515\n",
      "Training Epoch: 10 [12300/36450]\tLoss: 601.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [12350/36450]\tLoss: 599.9427\n",
      "Training Epoch: 10 [12400/36450]\tLoss: 584.4594\n",
      "Training Epoch: 10 [12450/36450]\tLoss: 602.7104\n",
      "Training Epoch: 10 [12500/36450]\tLoss: 574.5065\n",
      "Training Epoch: 10 [12550/36450]\tLoss: 566.8443\n",
      "Training Epoch: 10 [12600/36450]\tLoss: 538.5977\n",
      "Training Epoch: 10 [12650/36450]\tLoss: 609.5157\n",
      "Training Epoch: 10 [12700/36450]\tLoss: 622.2631\n",
      "Training Epoch: 10 [12750/36450]\tLoss: 617.4878\n",
      "Training Epoch: 10 [12800/36450]\tLoss: 619.6633\n",
      "Training Epoch: 10 [12850/36450]\tLoss: 563.3723\n",
      "Training Epoch: 10 [12900/36450]\tLoss: 576.1938\n",
      "Training Epoch: 10 [12950/36450]\tLoss: 592.6653\n",
      "Training Epoch: 10 [13000/36450]\tLoss: 609.3051\n",
      "Training Epoch: 10 [13050/36450]\tLoss: 584.3898\n",
      "Training Epoch: 10 [13100/36450]\tLoss: 633.0217\n",
      "Training Epoch: 10 [13150/36450]\tLoss: 585.6653\n",
      "Training Epoch: 10 [13200/36450]\tLoss: 576.9540\n",
      "Training Epoch: 10 [13250/36450]\tLoss: 553.3013\n",
      "Training Epoch: 10 [13300/36450]\tLoss: 532.9524\n",
      "Training Epoch: 10 [13350/36450]\tLoss: 563.5422\n",
      "Training Epoch: 10 [13400/36450]\tLoss: 588.4163\n",
      "Training Epoch: 10 [13450/36450]\tLoss: 566.6016\n",
      "Training Epoch: 10 [13500/36450]\tLoss: 586.3531\n",
      "Training Epoch: 10 [13550/36450]\tLoss: 597.3063\n",
      "Training Epoch: 10 [13600/36450]\tLoss: 569.9837\n",
      "Training Epoch: 10 [13650/36450]\tLoss: 583.9022\n",
      "Training Epoch: 10 [13700/36450]\tLoss: 557.4494\n",
      "Training Epoch: 10 [13750/36450]\tLoss: 578.2162\n",
      "Training Epoch: 10 [13800/36450]\tLoss: 619.0916\n",
      "Training Epoch: 10 [13850/36450]\tLoss: 606.9258\n",
      "Training Epoch: 10 [13900/36450]\tLoss: 617.7476\n",
      "Training Epoch: 10 [13950/36450]\tLoss: 592.4911\n",
      "Training Epoch: 10 [14000/36450]\tLoss: 587.4369\n",
      "Training Epoch: 10 [14050/36450]\tLoss: 606.2650\n",
      "Training Epoch: 10 [14100/36450]\tLoss: 586.2722\n",
      "Training Epoch: 10 [14150/36450]\tLoss: 633.1544\n",
      "Training Epoch: 10 [14200/36450]\tLoss: 568.1450\n",
      "Training Epoch: 10 [14250/36450]\tLoss: 577.3947\n",
      "Training Epoch: 10 [14300/36450]\tLoss: 560.2518\n",
      "Training Epoch: 10 [14350/36450]\tLoss: 565.1273\n",
      "Training Epoch: 10 [14400/36450]\tLoss: 569.3902\n",
      "Training Epoch: 10 [14450/36450]\tLoss: 586.3557\n",
      "Training Epoch: 10 [14500/36450]\tLoss: 577.0706\n",
      "Training Epoch: 10 [14550/36450]\tLoss: 617.2090\n",
      "Training Epoch: 10 [14600/36450]\tLoss: 599.6808\n",
      "Training Epoch: 10 [14650/36450]\tLoss: 575.2339\n",
      "Training Epoch: 10 [14700/36450]\tLoss: 634.2748\n",
      "Training Epoch: 10 [14750/36450]\tLoss: 603.0743\n",
      "Training Epoch: 10 [14800/36450]\tLoss: 591.4123\n",
      "Training Epoch: 10 [14850/36450]\tLoss: 602.7147\n",
      "Training Epoch: 10 [14900/36450]\tLoss: 581.2745\n",
      "Training Epoch: 10 [14950/36450]\tLoss: 598.5824\n",
      "Training Epoch: 10 [15000/36450]\tLoss: 595.8648\n",
      "Training Epoch: 10 [15050/36450]\tLoss: 594.8972\n",
      "Training Epoch: 10 [15100/36450]\tLoss: 625.1483\n",
      "Training Epoch: 10 [15150/36450]\tLoss: 582.0605\n",
      "Training Epoch: 10 [15200/36450]\tLoss: 599.1099\n",
      "Training Epoch: 10 [15250/36450]\tLoss: 547.4473\n",
      "Training Epoch: 10 [15300/36450]\tLoss: 601.5836\n",
      "Training Epoch: 10 [15350/36450]\tLoss: 553.7589\n",
      "Training Epoch: 10 [15400/36450]\tLoss: 571.6520\n",
      "Training Epoch: 10 [15450/36450]\tLoss: 583.5073\n",
      "Training Epoch: 10 [15500/36450]\tLoss: 586.5106\n",
      "Training Epoch: 10 [15550/36450]\tLoss: 569.6840\n",
      "Training Epoch: 10 [15600/36450]\tLoss: 584.0146\n",
      "Training Epoch: 10 [15650/36450]\tLoss: 560.3431\n",
      "Training Epoch: 10 [15700/36450]\tLoss: 594.4332\n",
      "Training Epoch: 10 [15750/36450]\tLoss: 588.6682\n",
      "Training Epoch: 10 [15800/36450]\tLoss: 574.8756\n",
      "Training Epoch: 10 [15850/36450]\tLoss: 575.1440\n",
      "Training Epoch: 10 [15900/36450]\tLoss: 565.2188\n",
      "Training Epoch: 10 [15950/36450]\tLoss: 530.4558\n",
      "Training Epoch: 10 [16000/36450]\tLoss: 578.8394\n",
      "Training Epoch: 10 [16050/36450]\tLoss: 548.4843\n",
      "Training Epoch: 10 [16100/36450]\tLoss: 563.5663\n",
      "Training Epoch: 10 [16150/36450]\tLoss: 566.6781\n",
      "Training Epoch: 10 [16200/36450]\tLoss: 568.1788\n",
      "Training Epoch: 10 [16250/36450]\tLoss: 620.6196\n",
      "Training Epoch: 10 [16300/36450]\tLoss: 569.2523\n",
      "Training Epoch: 10 [16350/36450]\tLoss: 552.6848\n",
      "Training Epoch: 10 [16400/36450]\tLoss: 578.2949\n",
      "Training Epoch: 10 [16450/36450]\tLoss: 597.1561\n",
      "Training Epoch: 10 [16500/36450]\tLoss: 563.6213\n",
      "Training Epoch: 10 [16550/36450]\tLoss: 561.2656\n",
      "Training Epoch: 10 [16600/36450]\tLoss: 602.7183\n",
      "Training Epoch: 10 [16650/36450]\tLoss: 584.4377\n",
      "Training Epoch: 10 [16700/36450]\tLoss: 603.1971\n",
      "Training Epoch: 10 [16750/36450]\tLoss: 603.5599\n",
      "Training Epoch: 10 [16800/36450]\tLoss: 591.2914\n",
      "Training Epoch: 10 [16850/36450]\tLoss: 566.0229\n",
      "Training Epoch: 10 [16900/36450]\tLoss: 562.8039\n",
      "Training Epoch: 10 [16950/36450]\tLoss: 588.9124\n",
      "Training Epoch: 10 [17000/36450]\tLoss: 584.4175\n",
      "Training Epoch: 10 [17050/36450]\tLoss: 567.7873\n",
      "Training Epoch: 10 [17100/36450]\tLoss: 570.6766\n",
      "Training Epoch: 10 [17150/36450]\tLoss: 554.6046\n",
      "Training Epoch: 10 [17200/36450]\tLoss: 627.6164\n",
      "Training Epoch: 10 [17250/36450]\tLoss: 588.1406\n",
      "Training Epoch: 10 [17300/36450]\tLoss: 596.9943\n",
      "Training Epoch: 10 [17350/36450]\tLoss: 582.1116\n",
      "Training Epoch: 10 [17400/36450]\tLoss: 582.7452\n",
      "Training Epoch: 10 [17450/36450]\tLoss: 598.3465\n",
      "Training Epoch: 10 [17500/36450]\tLoss: 564.4244\n",
      "Training Epoch: 10 [17550/36450]\tLoss: 577.5953\n",
      "Training Epoch: 10 [17600/36450]\tLoss: 628.3925\n",
      "Training Epoch: 10 [17650/36450]\tLoss: 577.1605\n",
      "Training Epoch: 10 [17700/36450]\tLoss: 601.2230\n",
      "Training Epoch: 10 [17750/36450]\tLoss: 598.8173\n",
      "Training Epoch: 10 [17800/36450]\tLoss: 559.5234\n",
      "Training Epoch: 10 [17850/36450]\tLoss: 593.6593\n",
      "Training Epoch: 10 [17900/36450]\tLoss: 585.0727\n",
      "Training Epoch: 10 [17950/36450]\tLoss: 601.1058\n",
      "Training Epoch: 10 [18000/36450]\tLoss: 587.2784\n",
      "Training Epoch: 10 [18050/36450]\tLoss: 570.2624\n",
      "Training Epoch: 10 [18100/36450]\tLoss: 626.3868\n",
      "Training Epoch: 10 [18150/36450]\tLoss: 599.7303\n",
      "Training Epoch: 10 [18200/36450]\tLoss: 585.8883\n",
      "Training Epoch: 10 [18250/36450]\tLoss: 564.5618\n",
      "Training Epoch: 10 [18300/36450]\tLoss: 597.3741\n",
      "Training Epoch: 10 [18350/36450]\tLoss: 567.5799\n",
      "Training Epoch: 10 [18400/36450]\tLoss: 587.6135\n",
      "Training Epoch: 10 [18450/36450]\tLoss: 550.3965\n",
      "Training Epoch: 10 [18500/36450]\tLoss: 571.2386\n",
      "Training Epoch: 10 [18550/36450]\tLoss: 576.9199\n",
      "Training Epoch: 10 [18600/36450]\tLoss: 568.3066\n",
      "Training Epoch: 10 [18650/36450]\tLoss: 614.6441\n",
      "Training Epoch: 10 [18700/36450]\tLoss: 581.5706\n",
      "Training Epoch: 10 [18750/36450]\tLoss: 600.6577\n",
      "Training Epoch: 10 [18800/36450]\tLoss: 588.9106\n",
      "Training Epoch: 10 [18850/36450]\tLoss: 562.3727\n",
      "Training Epoch: 10 [18900/36450]\tLoss: 590.0579\n",
      "Training Epoch: 10 [18950/36450]\tLoss: 608.2685\n",
      "Training Epoch: 10 [19000/36450]\tLoss: 561.0737\n",
      "Training Epoch: 10 [19050/36450]\tLoss: 567.5207\n",
      "Training Epoch: 10 [19100/36450]\tLoss: 596.4355\n",
      "Training Epoch: 10 [19150/36450]\tLoss: 606.4297\n",
      "Training Epoch: 10 [19200/36450]\tLoss: 594.3127\n",
      "Training Epoch: 10 [19250/36450]\tLoss: 584.7201\n",
      "Training Epoch: 10 [19300/36450]\tLoss: 612.0385\n",
      "Training Epoch: 10 [19350/36450]\tLoss: 601.2256\n",
      "Training Epoch: 10 [19400/36450]\tLoss: 576.0822\n",
      "Training Epoch: 10 [19450/36450]\tLoss: 539.9131\n",
      "Training Epoch: 10 [19500/36450]\tLoss: 602.3250\n",
      "Training Epoch: 10 [19550/36450]\tLoss: 584.0701\n",
      "Training Epoch: 10 [19600/36450]\tLoss: 581.3669\n",
      "Training Epoch: 10 [19650/36450]\tLoss: 624.7347\n",
      "Training Epoch: 10 [19700/36450]\tLoss: 608.8815\n",
      "Training Epoch: 10 [19750/36450]\tLoss: 624.2847\n",
      "Training Epoch: 10 [19800/36450]\tLoss: 583.6365\n",
      "Training Epoch: 10 [19850/36450]\tLoss: 578.4764\n",
      "Training Epoch: 10 [19900/36450]\tLoss: 613.8401\n",
      "Training Epoch: 10 [19950/36450]\tLoss: 604.4194\n",
      "Training Epoch: 10 [20000/36450]\tLoss: 587.1716\n",
      "Training Epoch: 10 [20050/36450]\tLoss: 600.6729\n",
      "Training Epoch: 10 [20100/36450]\tLoss: 569.1506\n",
      "Training Epoch: 10 [20150/36450]\tLoss: 601.2448\n",
      "Training Epoch: 10 [20200/36450]\tLoss: 556.2549\n",
      "Training Epoch: 10 [20250/36450]\tLoss: 638.7861\n",
      "Training Epoch: 10 [20300/36450]\tLoss: 614.6022\n",
      "Training Epoch: 10 [20350/36450]\tLoss: 565.9686\n",
      "Training Epoch: 10 [20400/36450]\tLoss: 601.0358\n",
      "Training Epoch: 10 [20450/36450]\tLoss: 628.3543\n",
      "Training Epoch: 10 [20500/36450]\tLoss: 645.8206\n",
      "Training Epoch: 10 [20550/36450]\tLoss: 626.1673\n",
      "Training Epoch: 10 [20600/36450]\tLoss: 705.0544\n",
      "Training Epoch: 10 [20650/36450]\tLoss: 640.1655\n",
      "Training Epoch: 10 [20700/36450]\tLoss: 596.0574\n",
      "Training Epoch: 10 [20750/36450]\tLoss: 584.2634\n",
      "Training Epoch: 10 [20800/36450]\tLoss: 612.3026\n",
      "Training Epoch: 10 [20850/36450]\tLoss: 624.6792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [20900/36450]\tLoss: 620.5398\n",
      "Training Epoch: 10 [20950/36450]\tLoss: 587.4661\n",
      "Training Epoch: 10 [21000/36450]\tLoss: 610.1547\n",
      "Training Epoch: 10 [21050/36450]\tLoss: 544.0576\n",
      "Training Epoch: 10 [21100/36450]\tLoss: 571.5156\n",
      "Training Epoch: 10 [21150/36450]\tLoss: 604.8774\n",
      "Training Epoch: 10 [21200/36450]\tLoss: 584.9787\n",
      "Training Epoch: 10 [21250/36450]\tLoss: 593.3235\n",
      "Training Epoch: 10 [21300/36450]\tLoss: 591.8394\n",
      "Training Epoch: 10 [21350/36450]\tLoss: 620.5481\n",
      "Training Epoch: 10 [21400/36450]\tLoss: 570.7223\n",
      "Training Epoch: 10 [21450/36450]\tLoss: 591.4203\n",
      "Training Epoch: 10 [21500/36450]\tLoss: 567.5828\n",
      "Training Epoch: 10 [21550/36450]\tLoss: 579.6038\n",
      "Training Epoch: 10 [21600/36450]\tLoss: 571.9127\n",
      "Training Epoch: 10 [21650/36450]\tLoss: 593.4885\n",
      "Training Epoch: 10 [21700/36450]\tLoss: 568.7745\n",
      "Training Epoch: 10 [21750/36450]\tLoss: 588.3595\n",
      "Training Epoch: 10 [21800/36450]\tLoss: 611.0596\n",
      "Training Epoch: 10 [21850/36450]\tLoss: 601.8157\n",
      "Training Epoch: 10 [21900/36450]\tLoss: 587.6637\n",
      "Training Epoch: 10 [21950/36450]\tLoss: 572.4316\n",
      "Training Epoch: 10 [22000/36450]\tLoss: 561.6994\n",
      "Training Epoch: 10 [22050/36450]\tLoss: 557.3032\n",
      "Training Epoch: 10 [22100/36450]\tLoss: 557.9670\n",
      "Training Epoch: 10 [22150/36450]\tLoss: 567.3029\n",
      "Training Epoch: 10 [22200/36450]\tLoss: 595.1583\n",
      "Training Epoch: 10 [22250/36450]\tLoss: 617.7872\n",
      "Training Epoch: 10 [22300/36450]\tLoss: 604.1542\n",
      "Training Epoch: 10 [22350/36450]\tLoss: 596.5820\n",
      "Training Epoch: 10 [22400/36450]\tLoss: 624.9775\n",
      "Training Epoch: 10 [22450/36450]\tLoss: 603.2605\n",
      "Training Epoch: 10 [22500/36450]\tLoss: 551.7333\n",
      "Training Epoch: 10 [22550/36450]\tLoss: 603.5054\n",
      "Training Epoch: 10 [22600/36450]\tLoss: 597.1620\n",
      "Training Epoch: 10 [22650/36450]\tLoss: 588.6798\n",
      "Training Epoch: 10 [22700/36450]\tLoss: 571.4845\n",
      "Training Epoch: 10 [22750/36450]\tLoss: 575.9727\n",
      "Training Epoch: 10 [22800/36450]\tLoss: 533.8521\n",
      "Training Epoch: 10 [22850/36450]\tLoss: 597.9363\n",
      "Training Epoch: 10 [22900/36450]\tLoss: 571.1505\n",
      "Training Epoch: 10 [22950/36450]\tLoss: 593.2925\n",
      "Training Epoch: 10 [23000/36450]\tLoss: 531.5454\n",
      "Training Epoch: 10 [23050/36450]\tLoss: 607.8574\n",
      "Training Epoch: 10 [23100/36450]\tLoss: 605.4026\n",
      "Training Epoch: 10 [23150/36450]\tLoss: 562.4927\n",
      "Training Epoch: 10 [23200/36450]\tLoss: 580.5963\n",
      "Training Epoch: 10 [23250/36450]\tLoss: 582.1106\n",
      "Training Epoch: 10 [23300/36450]\tLoss: 595.0729\n",
      "Training Epoch: 10 [23350/36450]\tLoss: 567.0378\n",
      "Training Epoch: 10 [23400/36450]\tLoss: 552.4648\n",
      "Training Epoch: 10 [23450/36450]\tLoss: 567.2124\n",
      "Training Epoch: 10 [23500/36450]\tLoss: 577.8539\n",
      "Training Epoch: 10 [23550/36450]\tLoss: 564.3655\n",
      "Training Epoch: 10 [23600/36450]\tLoss: 585.1262\n",
      "Training Epoch: 10 [23650/36450]\tLoss: 588.9266\n",
      "Training Epoch: 10 [23700/36450]\tLoss: 585.5222\n",
      "Training Epoch: 10 [23750/36450]\tLoss: 586.0444\n",
      "Training Epoch: 10 [23800/36450]\tLoss: 594.9301\n",
      "Training Epoch: 10 [23850/36450]\tLoss: 586.7355\n",
      "Training Epoch: 10 [23900/36450]\tLoss: 580.7405\n",
      "Training Epoch: 10 [23950/36450]\tLoss: 578.0704\n",
      "Training Epoch: 10 [24000/36450]\tLoss: 564.6779\n",
      "Training Epoch: 10 [24050/36450]\tLoss: 565.8388\n",
      "Training Epoch: 10 [24100/36450]\tLoss: 574.0292\n",
      "Training Epoch: 10 [24150/36450]\tLoss: 570.8163\n",
      "Training Epoch: 10 [24200/36450]\tLoss: 599.6400\n",
      "Training Epoch: 10 [24250/36450]\tLoss: 558.2788\n",
      "Training Epoch: 10 [24300/36450]\tLoss: 591.9429\n",
      "Training Epoch: 10 [24350/36450]\tLoss: 580.2544\n",
      "Training Epoch: 10 [24400/36450]\tLoss: 562.5227\n",
      "Training Epoch: 10 [24450/36450]\tLoss: 576.6086\n",
      "Training Epoch: 10 [24500/36450]\tLoss: 553.7919\n",
      "Training Epoch: 10 [24550/36450]\tLoss: 570.9744\n",
      "Training Epoch: 10 [24600/36450]\tLoss: 599.0303\n",
      "Training Epoch: 10 [24650/36450]\tLoss: 603.8398\n",
      "Training Epoch: 10 [24700/36450]\tLoss: 585.4339\n",
      "Training Epoch: 10 [24750/36450]\tLoss: 568.5009\n",
      "Training Epoch: 10 [24800/36450]\tLoss: 549.0200\n",
      "Training Epoch: 10 [24850/36450]\tLoss: 549.2408\n",
      "Training Epoch: 10 [24900/36450]\tLoss: 600.3453\n",
      "Training Epoch: 10 [24950/36450]\tLoss: 633.7557\n",
      "Training Epoch: 10 [25000/36450]\tLoss: 572.1784\n",
      "Training Epoch: 10 [25050/36450]\tLoss: 589.7768\n",
      "Training Epoch: 10 [25100/36450]\tLoss: 607.3882\n",
      "Training Epoch: 10 [25150/36450]\tLoss: 586.6068\n",
      "Training Epoch: 10 [25200/36450]\tLoss: 610.7996\n",
      "Training Epoch: 10 [25250/36450]\tLoss: 599.8113\n",
      "Training Epoch: 10 [25300/36450]\tLoss: 567.5502\n",
      "Training Epoch: 10 [25350/36450]\tLoss: 588.5697\n",
      "Training Epoch: 10 [25400/36450]\tLoss: 551.6055\n",
      "Training Epoch: 10 [25450/36450]\tLoss: 603.6424\n",
      "Training Epoch: 10 [25500/36450]\tLoss: 548.3017\n",
      "Training Epoch: 10 [25550/36450]\tLoss: 546.7804\n",
      "Training Epoch: 10 [25600/36450]\tLoss: 627.3058\n",
      "Training Epoch: 10 [25650/36450]\tLoss: 579.7300\n",
      "Training Epoch: 10 [25700/36450]\tLoss: 544.6683\n",
      "Training Epoch: 10 [25750/36450]\tLoss: 575.4845\n",
      "Training Epoch: 10 [25800/36450]\tLoss: 589.8245\n",
      "Training Epoch: 10 [25850/36450]\tLoss: 565.1381\n",
      "Training Epoch: 10 [25900/36450]\tLoss: 555.7123\n",
      "Training Epoch: 10 [25950/36450]\tLoss: 599.6515\n",
      "Training Epoch: 10 [26000/36450]\tLoss: 560.3077\n",
      "Training Epoch: 10 [26050/36450]\tLoss: 625.6221\n",
      "Training Epoch: 10 [26100/36450]\tLoss: 557.6416\n",
      "Training Epoch: 10 [26150/36450]\tLoss: 586.2440\n",
      "Training Epoch: 10 [26200/36450]\tLoss: 592.9668\n",
      "Training Epoch: 10 [26250/36450]\tLoss: 603.9160\n",
      "Training Epoch: 10 [26300/36450]\tLoss: 539.2379\n",
      "Training Epoch: 10 [26350/36450]\tLoss: 533.4012\n",
      "Training Epoch: 10 [26400/36450]\tLoss: 555.5783\n",
      "Training Epoch: 10 [26450/36450]\tLoss: 588.5939\n",
      "Training Epoch: 10 [26500/36450]\tLoss: 585.1224\n",
      "Training Epoch: 10 [26550/36450]\tLoss: 550.9804\n",
      "Training Epoch: 10 [26600/36450]\tLoss: 562.7791\n",
      "Training Epoch: 10 [26650/36450]\tLoss: 549.7733\n",
      "Training Epoch: 10 [26700/36450]\tLoss: 562.4755\n",
      "Training Epoch: 10 [26750/36450]\tLoss: 576.7569\n",
      "Training Epoch: 10 [26800/36450]\tLoss: 563.0970\n",
      "Training Epoch: 10 [26850/36450]\tLoss: 597.3721\n",
      "Training Epoch: 10 [26900/36450]\tLoss: 571.0181\n",
      "Training Epoch: 10 [26950/36450]\tLoss: 555.0266\n",
      "Training Epoch: 10 [27000/36450]\tLoss: 563.2795\n",
      "Training Epoch: 10 [27050/36450]\tLoss: 557.5182\n",
      "Training Epoch: 10 [27100/36450]\tLoss: 556.1028\n",
      "Training Epoch: 10 [27150/36450]\tLoss: 597.9846\n",
      "Training Epoch: 10 [27200/36450]\tLoss: 582.6714\n",
      "Training Epoch: 10 [27250/36450]\tLoss: 587.9206\n",
      "Training Epoch: 10 [27300/36450]\tLoss: 561.5172\n",
      "Training Epoch: 10 [27350/36450]\tLoss: 543.0812\n",
      "Training Epoch: 10 [27400/36450]\tLoss: 575.1417\n",
      "Training Epoch: 10 [27450/36450]\tLoss: 547.8262\n",
      "Training Epoch: 10 [27500/36450]\tLoss: 602.4766\n",
      "Training Epoch: 10 [27550/36450]\tLoss: 575.1627\n",
      "Training Epoch: 10 [27600/36450]\tLoss: 534.2810\n",
      "Training Epoch: 10 [27650/36450]\tLoss: 567.7341\n",
      "Training Epoch: 10 [27700/36450]\tLoss: 564.5355\n",
      "Training Epoch: 10 [27750/36450]\tLoss: 585.8680\n",
      "Training Epoch: 10 [27800/36450]\tLoss: 579.7874\n",
      "Training Epoch: 10 [27850/36450]\tLoss: 572.0724\n",
      "Training Epoch: 10 [27900/36450]\tLoss: 607.9981\n",
      "Training Epoch: 10 [27950/36450]\tLoss: 575.2735\n",
      "Training Epoch: 10 [28000/36450]\tLoss: 590.3505\n",
      "Training Epoch: 10 [28050/36450]\tLoss: 562.5901\n",
      "Training Epoch: 10 [28100/36450]\tLoss: 567.4528\n",
      "Training Epoch: 10 [28150/36450]\tLoss: 579.4368\n",
      "Training Epoch: 10 [28200/36450]\tLoss: 597.7759\n",
      "Training Epoch: 10 [28250/36450]\tLoss: 589.8399\n",
      "Training Epoch: 10 [28300/36450]\tLoss: 564.2505\n",
      "Training Epoch: 10 [28350/36450]\tLoss: 549.6743\n",
      "Training Epoch: 10 [28400/36450]\tLoss: 593.7534\n",
      "Training Epoch: 10 [28450/36450]\tLoss: 621.9243\n",
      "Training Epoch: 10 [28500/36450]\tLoss: 566.1601\n",
      "Training Epoch: 10 [28550/36450]\tLoss: 606.5947\n",
      "Training Epoch: 10 [28600/36450]\tLoss: 573.3757\n",
      "Training Epoch: 10 [28650/36450]\tLoss: 593.3279\n",
      "Training Epoch: 10 [28700/36450]\tLoss: 556.6592\n",
      "Training Epoch: 10 [28750/36450]\tLoss: 592.5424\n",
      "Training Epoch: 10 [28800/36450]\tLoss: 597.5071\n",
      "Training Epoch: 10 [28850/36450]\tLoss: 569.2436\n",
      "Training Epoch: 10 [28900/36450]\tLoss: 552.0989\n",
      "Training Epoch: 10 [28950/36450]\tLoss: 555.5496\n",
      "Training Epoch: 10 [29000/36450]\tLoss: 530.5113\n",
      "Training Epoch: 10 [29050/36450]\tLoss: 540.5973\n",
      "Training Epoch: 10 [29100/36450]\tLoss: 584.6591\n",
      "Training Epoch: 10 [29150/36450]\tLoss: 569.8257\n",
      "Training Epoch: 10 [29200/36450]\tLoss: 608.3842\n",
      "Training Epoch: 10 [29250/36450]\tLoss: 569.7689\n",
      "Training Epoch: 10 [29300/36450]\tLoss: 556.9691\n",
      "Training Epoch: 10 [29350/36450]\tLoss: 604.0730\n",
      "Training Epoch: 10 [29400/36450]\tLoss: 622.8492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [29450/36450]\tLoss: 591.0049\n",
      "Training Epoch: 10 [29500/36450]\tLoss: 585.8245\n",
      "Training Epoch: 10 [29550/36450]\tLoss: 585.1747\n",
      "Training Epoch: 10 [29600/36450]\tLoss: 661.9368\n",
      "Training Epoch: 10 [29650/36450]\tLoss: 654.2695\n",
      "Training Epoch: 10 [29700/36450]\tLoss: 674.1936\n",
      "Training Epoch: 10 [29750/36450]\tLoss: 613.7621\n",
      "Training Epoch: 10 [29800/36450]\tLoss: 575.9322\n",
      "Training Epoch: 10 [29850/36450]\tLoss: 585.6010\n",
      "Training Epoch: 10 [29900/36450]\tLoss: 592.8359\n",
      "Training Epoch: 10 [29950/36450]\tLoss: 619.0782\n",
      "Training Epoch: 10 [30000/36450]\tLoss: 660.9979\n",
      "Training Epoch: 10 [30050/36450]\tLoss: 680.7935\n",
      "Training Epoch: 10 [30100/36450]\tLoss: 655.2979\n",
      "Training Epoch: 10 [30150/36450]\tLoss: 633.6112\n",
      "Training Epoch: 10 [30200/36450]\tLoss: 568.8125\n",
      "Training Epoch: 10 [30250/36450]\tLoss: 591.7889\n",
      "Training Epoch: 10 [30300/36450]\tLoss: 568.6191\n",
      "Training Epoch: 10 [30350/36450]\tLoss: 622.0027\n",
      "Training Epoch: 10 [30400/36450]\tLoss: 650.5656\n",
      "Training Epoch: 10 [30450/36450]\tLoss: 617.0738\n",
      "Training Epoch: 10 [30500/36450]\tLoss: 608.5392\n",
      "Training Epoch: 10 [30550/36450]\tLoss: 576.4452\n",
      "Training Epoch: 10 [30600/36450]\tLoss: 587.8100\n",
      "Training Epoch: 10 [30650/36450]\tLoss: 638.9703\n",
      "Training Epoch: 10 [30700/36450]\tLoss: 661.1246\n",
      "Training Epoch: 10 [30750/36450]\tLoss: 621.1264\n",
      "Training Epoch: 10 [30800/36450]\tLoss: 593.2532\n",
      "Training Epoch: 10 [30850/36450]\tLoss: 578.6171\n",
      "Training Epoch: 10 [30900/36450]\tLoss: 562.4591\n",
      "Training Epoch: 10 [30950/36450]\tLoss: 587.5409\n",
      "Training Epoch: 10 [31000/36450]\tLoss: 574.0727\n",
      "Training Epoch: 10 [31050/36450]\tLoss: 589.8859\n",
      "Training Epoch: 10 [31100/36450]\tLoss: 582.1622\n",
      "Training Epoch: 10 [31150/36450]\tLoss: 613.1188\n",
      "Training Epoch: 10 [31200/36450]\tLoss: 565.7593\n",
      "Training Epoch: 10 [31250/36450]\tLoss: 571.8522\n",
      "Training Epoch: 10 [31300/36450]\tLoss: 614.9692\n",
      "Training Epoch: 10 [31350/36450]\tLoss: 572.3712\n",
      "Training Epoch: 10 [31400/36450]\tLoss: 586.1436\n",
      "Training Epoch: 10 [31450/36450]\tLoss: 603.0461\n",
      "Training Epoch: 10 [31500/36450]\tLoss: 578.4870\n",
      "Training Epoch: 10 [31550/36450]\tLoss: 568.2145\n",
      "Training Epoch: 10 [31600/36450]\tLoss: 560.1406\n",
      "Training Epoch: 10 [31650/36450]\tLoss: 574.4819\n",
      "Training Epoch: 10 [31700/36450]\tLoss: 620.5351\n",
      "Training Epoch: 10 [31750/36450]\tLoss: 594.1586\n",
      "Training Epoch: 10 [31800/36450]\tLoss: 590.0905\n",
      "Training Epoch: 10 [31850/36450]\tLoss: 591.8256\n",
      "Training Epoch: 10 [31900/36450]\tLoss: 588.4576\n",
      "Training Epoch: 10 [31950/36450]\tLoss: 632.1448\n",
      "Training Epoch: 10 [32000/36450]\tLoss: 605.7678\n",
      "Training Epoch: 10 [32050/36450]\tLoss: 574.5056\n",
      "Training Epoch: 10 [32100/36450]\tLoss: 591.6598\n",
      "Training Epoch: 10 [32150/36450]\tLoss: 592.4178\n",
      "Training Epoch: 10 [32200/36450]\tLoss: 547.5611\n",
      "Training Epoch: 10 [32250/36450]\tLoss: 558.6305\n",
      "Training Epoch: 10 [32300/36450]\tLoss: 579.9429\n",
      "Training Epoch: 10 [32350/36450]\tLoss: 547.0076\n",
      "Training Epoch: 10 [32400/36450]\tLoss: 571.7242\n",
      "Training Epoch: 10 [32450/36450]\tLoss: 598.7011\n",
      "Training Epoch: 10 [32500/36450]\tLoss: 597.4311\n",
      "Training Epoch: 10 [32550/36450]\tLoss: 610.0407\n",
      "Training Epoch: 10 [32600/36450]\tLoss: 551.0339\n",
      "Training Epoch: 10 [32650/36450]\tLoss: 537.9473\n",
      "Training Epoch: 10 [32700/36450]\tLoss: 572.8770\n",
      "Training Epoch: 10 [32750/36450]\tLoss: 548.7666\n",
      "Training Epoch: 10 [32800/36450]\tLoss: 558.3742\n",
      "Training Epoch: 10 [32850/36450]\tLoss: 531.9725\n",
      "Training Epoch: 10 [32900/36450]\tLoss: 570.3632\n",
      "Training Epoch: 10 [32950/36450]\tLoss: 562.2944\n",
      "Training Epoch: 10 [33000/36450]\tLoss: 572.2784\n",
      "Training Epoch: 10 [33050/36450]\tLoss: 569.6686\n",
      "Training Epoch: 10 [33100/36450]\tLoss: 564.0771\n",
      "Training Epoch: 10 [33150/36450]\tLoss: 595.5124\n",
      "Training Epoch: 10 [33200/36450]\tLoss: 635.9088\n",
      "Training Epoch: 10 [33250/36450]\tLoss: 578.4764\n",
      "Training Epoch: 10 [33300/36450]\tLoss: 563.6201\n",
      "Training Epoch: 10 [33350/36450]\tLoss: 594.8666\n",
      "Training Epoch: 10 [33400/36450]\tLoss: 584.6065\n",
      "Training Epoch: 10 [33450/36450]\tLoss: 578.5634\n",
      "Training Epoch: 10 [33500/36450]\tLoss: 592.3614\n",
      "Training Epoch: 10 [33550/36450]\tLoss: 563.7206\n",
      "Training Epoch: 10 [33600/36450]\tLoss: 602.9879\n",
      "Training Epoch: 10 [33650/36450]\tLoss: 585.6649\n",
      "Training Epoch: 10 [33700/36450]\tLoss: 571.6959\n",
      "Training Epoch: 10 [33750/36450]\tLoss: 540.1006\n",
      "Training Epoch: 10 [33800/36450]\tLoss: 542.5065\n",
      "Training Epoch: 10 [33850/36450]\tLoss: 577.0116\n",
      "Training Epoch: 10 [33900/36450]\tLoss: 557.6625\n",
      "Training Epoch: 10 [33950/36450]\tLoss: 588.2230\n",
      "Training Epoch: 10 [34000/36450]\tLoss: 598.2552\n",
      "Training Epoch: 10 [34050/36450]\tLoss: 566.2778\n",
      "Training Epoch: 10 [34100/36450]\tLoss: 608.5409\n",
      "Training Epoch: 10 [34150/36450]\tLoss: 573.7217\n",
      "Training Epoch: 10 [34200/36450]\tLoss: 612.7559\n",
      "Training Epoch: 10 [34250/36450]\tLoss: 581.6293\n",
      "Training Epoch: 10 [34300/36450]\tLoss: 577.8217\n",
      "Training Epoch: 10 [34350/36450]\tLoss: 578.3749\n",
      "Training Epoch: 10 [34400/36450]\tLoss: 554.8384\n",
      "Training Epoch: 10 [34450/36450]\tLoss: 584.9383\n",
      "Training Epoch: 10 [34500/36450]\tLoss: 553.5659\n",
      "Training Epoch: 10 [34550/36450]\tLoss: 586.6553\n",
      "Training Epoch: 10 [34600/36450]\tLoss: 544.7866\n",
      "Training Epoch: 10 [34650/36450]\tLoss: 610.3240\n",
      "Training Epoch: 10 [34700/36450]\tLoss: 569.2138\n",
      "Training Epoch: 10 [34750/36450]\tLoss: 599.2112\n",
      "Training Epoch: 10 [34800/36450]\tLoss: 573.3915\n",
      "Training Epoch: 10 [34850/36450]\tLoss: 559.2490\n",
      "Training Epoch: 10 [34900/36450]\tLoss: 553.8032\n",
      "Training Epoch: 10 [34950/36450]\tLoss: 594.5543\n",
      "Training Epoch: 10 [35000/36450]\tLoss: 576.2753\n",
      "Training Epoch: 10 [35050/36450]\tLoss: 596.5017\n",
      "Training Epoch: 10 [35100/36450]\tLoss: 546.3444\n",
      "Training Epoch: 10 [35150/36450]\tLoss: 557.2307\n",
      "Training Epoch: 10 [35200/36450]\tLoss: 587.4413\n",
      "Training Epoch: 10 [35250/36450]\tLoss: 541.3354\n",
      "Training Epoch: 10 [35300/36450]\tLoss: 608.6452\n",
      "Training Epoch: 10 [35350/36450]\tLoss: 537.9374\n",
      "Training Epoch: 10 [35400/36450]\tLoss: 557.0923\n",
      "Training Epoch: 10 [35450/36450]\tLoss: 572.7950\n",
      "Training Epoch: 10 [35500/36450]\tLoss: 571.3731\n",
      "Training Epoch: 10 [35550/36450]\tLoss: 580.3533\n",
      "Training Epoch: 10 [35600/36450]\tLoss: 595.3181\n",
      "Training Epoch: 10 [35650/36450]\tLoss: 561.3763\n",
      "Training Epoch: 10 [35700/36450]\tLoss: 590.2631\n",
      "Training Epoch: 10 [35750/36450]\tLoss: 583.1950\n",
      "Training Epoch: 10 [35800/36450]\tLoss: 539.8983\n",
      "Training Epoch: 10 [35850/36450]\tLoss: 520.8359\n",
      "Training Epoch: 10 [35900/36450]\tLoss: 581.4581\n",
      "Training Epoch: 10 [35950/36450]\tLoss: 603.5345\n",
      "Training Epoch: 10 [36000/36450]\tLoss: 596.3953\n",
      "Training Epoch: 10 [36050/36450]\tLoss: 574.8170\n",
      "Training Epoch: 10 [36100/36450]\tLoss: 586.4312\n",
      "Training Epoch: 10 [36150/36450]\tLoss: 602.3591\n",
      "Training Epoch: 10 [36200/36450]\tLoss: 589.8681\n",
      "Training Epoch: 10 [36250/36450]\tLoss: 550.8227\n",
      "Training Epoch: 10 [36300/36450]\tLoss: 574.2684\n",
      "Training Epoch: 10 [36350/36450]\tLoss: 605.6116\n",
      "Training Epoch: 10 [36400/36450]\tLoss: 623.0946\n",
      "Training Epoch: 10 [36450/36450]\tLoss: 553.9831\n",
      "Training Epoch: 10 [4050/4050]\tLoss: 288.3567\n",
      "Training Epoch: 11 [50/36450]\tLoss: 556.9763\n",
      "Training Epoch: 11 [100/36450]\tLoss: 620.0214\n",
      "Training Epoch: 11 [150/36450]\tLoss: 564.5829\n",
      "Training Epoch: 11 [200/36450]\tLoss: 591.0818\n",
      "Training Epoch: 11 [250/36450]\tLoss: 552.5955\n",
      "Training Epoch: 11 [300/36450]\tLoss: 556.4224\n",
      "Training Epoch: 11 [350/36450]\tLoss: 591.4277\n",
      "Training Epoch: 11 [400/36450]\tLoss: 588.3491\n",
      "Training Epoch: 11 [450/36450]\tLoss: 576.2322\n",
      "Training Epoch: 11 [500/36450]\tLoss: 594.3645\n",
      "Training Epoch: 11 [550/36450]\tLoss: 569.1088\n",
      "Training Epoch: 11 [600/36450]\tLoss: 563.7883\n",
      "Training Epoch: 11 [650/36450]\tLoss: 546.5627\n",
      "Training Epoch: 11 [700/36450]\tLoss: 555.0486\n",
      "Training Epoch: 11 [750/36450]\tLoss: 563.0099\n",
      "Training Epoch: 11 [800/36450]\tLoss: 586.8273\n",
      "Training Epoch: 11 [850/36450]\tLoss: 552.5789\n",
      "Training Epoch: 11 [900/36450]\tLoss: 562.1997\n",
      "Training Epoch: 11 [950/36450]\tLoss: 580.0638\n",
      "Training Epoch: 11 [1000/36450]\tLoss: 577.5254\n",
      "Training Epoch: 11 [1050/36450]\tLoss: 568.6694\n",
      "Training Epoch: 11 [1100/36450]\tLoss: 563.7490\n",
      "Training Epoch: 11 [1150/36450]\tLoss: 590.7719\n",
      "Training Epoch: 11 [1200/36450]\tLoss: 548.6713\n",
      "Training Epoch: 11 [1250/36450]\tLoss: 558.9405\n",
      "Training Epoch: 11 [1300/36450]\tLoss: 554.7119\n",
      "Training Epoch: 11 [1350/36450]\tLoss: 577.4148\n",
      "Training Epoch: 11 [1400/36450]\tLoss: 597.1204\n",
      "Training Epoch: 11 [1450/36450]\tLoss: 566.8748\n",
      "Training Epoch: 11 [1500/36450]\tLoss: 575.2945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [1550/36450]\tLoss: 561.3729\n",
      "Training Epoch: 11 [1600/36450]\tLoss: 590.7722\n",
      "Training Epoch: 11 [1650/36450]\tLoss: 549.8449\n",
      "Training Epoch: 11 [1700/36450]\tLoss: 570.4419\n",
      "Training Epoch: 11 [1750/36450]\tLoss: 602.9990\n",
      "Training Epoch: 11 [1800/36450]\tLoss: 608.0258\n",
      "Training Epoch: 11 [1850/36450]\tLoss: 553.8932\n",
      "Training Epoch: 11 [1900/36450]\tLoss: 547.0009\n",
      "Training Epoch: 11 [1950/36450]\tLoss: 549.8967\n",
      "Training Epoch: 11 [2000/36450]\tLoss: 562.3001\n",
      "Training Epoch: 11 [2050/36450]\tLoss: 569.4816\n",
      "Training Epoch: 11 [2100/36450]\tLoss: 550.4672\n",
      "Training Epoch: 11 [2150/36450]\tLoss: 540.0154\n",
      "Training Epoch: 11 [2200/36450]\tLoss: 544.8051\n",
      "Training Epoch: 11 [2250/36450]\tLoss: 621.5670\n",
      "Training Epoch: 11 [2300/36450]\tLoss: 577.1465\n",
      "Training Epoch: 11 [2350/36450]\tLoss: 592.7311\n",
      "Training Epoch: 11 [2400/36450]\tLoss: 588.4641\n",
      "Training Epoch: 11 [2450/36450]\tLoss: 553.0773\n",
      "Training Epoch: 11 [2500/36450]\tLoss: 544.5626\n",
      "Training Epoch: 11 [2550/36450]\tLoss: 567.1573\n",
      "Training Epoch: 11 [2600/36450]\tLoss: 609.1066\n",
      "Training Epoch: 11 [2650/36450]\tLoss: 529.5107\n",
      "Training Epoch: 11 [2700/36450]\tLoss: 589.3183\n",
      "Training Epoch: 11 [2750/36450]\tLoss: 580.0878\n",
      "Training Epoch: 11 [2800/36450]\tLoss: 570.5915\n",
      "Training Epoch: 11 [2850/36450]\tLoss: 573.4143\n",
      "Training Epoch: 11 [2900/36450]\tLoss: 584.4159\n",
      "Training Epoch: 11 [2950/36450]\tLoss: 541.1441\n",
      "Training Epoch: 11 [3000/36450]\tLoss: 556.8027\n",
      "Training Epoch: 11 [3050/36450]\tLoss: 540.9453\n",
      "Training Epoch: 11 [3100/36450]\tLoss: 560.3944\n",
      "Training Epoch: 11 [3150/36450]\tLoss: 584.2153\n",
      "Training Epoch: 11 [3200/36450]\tLoss: 545.3710\n",
      "Training Epoch: 11 [3250/36450]\tLoss: 584.9495\n",
      "Training Epoch: 11 [3300/36450]\tLoss: 568.9385\n",
      "Training Epoch: 11 [3350/36450]\tLoss: 565.8126\n",
      "Training Epoch: 11 [3400/36450]\tLoss: 585.6594\n",
      "Training Epoch: 11 [3450/36450]\tLoss: 548.8785\n",
      "Training Epoch: 11 [3500/36450]\tLoss: 591.5360\n",
      "Training Epoch: 11 [3550/36450]\tLoss: 580.8403\n",
      "Training Epoch: 11 [3600/36450]\tLoss: 585.7569\n",
      "Training Epoch: 11 [3650/36450]\tLoss: 562.0148\n",
      "Training Epoch: 11 [3700/36450]\tLoss: 529.3094\n",
      "Training Epoch: 11 [3750/36450]\tLoss: 550.8926\n",
      "Training Epoch: 11 [3800/36450]\tLoss: 583.7287\n",
      "Training Epoch: 11 [3850/36450]\tLoss: 562.0782\n",
      "Training Epoch: 11 [3900/36450]\tLoss: 572.3506\n",
      "Training Epoch: 11 [3950/36450]\tLoss: 603.5529\n",
      "Training Epoch: 11 [4000/36450]\tLoss: 548.5952\n",
      "Training Epoch: 11 [4050/36450]\tLoss: 574.9442\n",
      "Training Epoch: 11 [4100/36450]\tLoss: 580.8374\n",
      "Training Epoch: 11 [4150/36450]\tLoss: 543.9732\n",
      "Training Epoch: 11 [4200/36450]\tLoss: 588.9402\n",
      "Training Epoch: 11 [4250/36450]\tLoss: 550.1953\n",
      "Training Epoch: 11 [4300/36450]\tLoss: 570.3265\n",
      "Training Epoch: 11 [4350/36450]\tLoss: 559.5981\n",
      "Training Epoch: 11 [4400/36450]\tLoss: 507.9373\n",
      "Training Epoch: 11 [4450/36450]\tLoss: 596.3347\n",
      "Training Epoch: 11 [4500/36450]\tLoss: 569.9556\n",
      "Training Epoch: 11 [4550/36450]\tLoss: 593.0585\n",
      "Training Epoch: 11 [4600/36450]\tLoss: 590.0474\n",
      "Training Epoch: 11 [4650/36450]\tLoss: 559.5701\n",
      "Training Epoch: 11 [4700/36450]\tLoss: 550.2006\n",
      "Training Epoch: 11 [4750/36450]\tLoss: 563.7891\n",
      "Training Epoch: 11 [4800/36450]\tLoss: 527.0212\n",
      "Training Epoch: 11 [4850/36450]\tLoss: 562.8185\n",
      "Training Epoch: 11 [4900/36450]\tLoss: 548.7093\n",
      "Training Epoch: 11 [4950/36450]\tLoss: 615.5831\n",
      "Training Epoch: 11 [5000/36450]\tLoss: 553.6355\n",
      "Training Epoch: 11 [5050/36450]\tLoss: 565.2938\n",
      "Training Epoch: 11 [5100/36450]\tLoss: 592.7961\n",
      "Training Epoch: 11 [5150/36450]\tLoss: 570.9823\n",
      "Training Epoch: 11 [5200/36450]\tLoss: 567.7250\n",
      "Training Epoch: 11 [5250/36450]\tLoss: 570.7793\n",
      "Training Epoch: 11 [5300/36450]\tLoss: 575.5702\n",
      "Training Epoch: 11 [5350/36450]\tLoss: 561.8660\n",
      "Training Epoch: 11 [5400/36450]\tLoss: 569.3382\n",
      "Training Epoch: 11 [5450/36450]\tLoss: 555.6430\n",
      "Training Epoch: 11 [5500/36450]\tLoss: 576.7684\n",
      "Training Epoch: 11 [5550/36450]\tLoss: 559.9112\n",
      "Training Epoch: 11 [5600/36450]\tLoss: 613.3042\n",
      "Training Epoch: 11 [5650/36450]\tLoss: 553.7088\n",
      "Training Epoch: 11 [5700/36450]\tLoss: 561.5424\n",
      "Training Epoch: 11 [5750/36450]\tLoss: 578.7833\n",
      "Training Epoch: 11 [5800/36450]\tLoss: 553.4686\n",
      "Training Epoch: 11 [5850/36450]\tLoss: 605.7164\n",
      "Training Epoch: 11 [5900/36450]\tLoss: 526.2261\n",
      "Training Epoch: 11 [5950/36450]\tLoss: 564.6380\n",
      "Training Epoch: 11 [6000/36450]\tLoss: 567.7381\n",
      "Training Epoch: 11 [6050/36450]\tLoss: 599.0407\n",
      "Training Epoch: 11 [6100/36450]\tLoss: 547.7396\n",
      "Training Epoch: 11 [6150/36450]\tLoss: 570.6339\n",
      "Training Epoch: 11 [6200/36450]\tLoss: 556.6879\n",
      "Training Epoch: 11 [6250/36450]\tLoss: 602.1611\n",
      "Training Epoch: 11 [6300/36450]\tLoss: 588.8653\n",
      "Training Epoch: 11 [6350/36450]\tLoss: 566.3010\n",
      "Training Epoch: 11 [6400/36450]\tLoss: 590.2809\n",
      "Training Epoch: 11 [6450/36450]\tLoss: 583.9861\n",
      "Training Epoch: 11 [6500/36450]\tLoss: 584.7424\n",
      "Training Epoch: 11 [6550/36450]\tLoss: 634.5843\n",
      "Training Epoch: 11 [6600/36450]\tLoss: 608.3788\n",
      "Training Epoch: 11 [6650/36450]\tLoss: 625.7314\n",
      "Training Epoch: 11 [6700/36450]\tLoss: 599.8370\n",
      "Training Epoch: 11 [6750/36450]\tLoss: 629.3272\n",
      "Training Epoch: 11 [6800/36450]\tLoss: 563.2953\n",
      "Training Epoch: 11 [6850/36450]\tLoss: 580.8433\n",
      "Training Epoch: 11 [6900/36450]\tLoss: 558.5574\n",
      "Training Epoch: 11 [6950/36450]\tLoss: 518.9601\n",
      "Training Epoch: 11 [7000/36450]\tLoss: 574.2827\n",
      "Training Epoch: 11 [7050/36450]\tLoss: 598.4621\n",
      "Training Epoch: 11 [7100/36450]\tLoss: 595.3730\n",
      "Training Epoch: 11 [7150/36450]\tLoss: 606.5848\n",
      "Training Epoch: 11 [7200/36450]\tLoss: 605.2078\n",
      "Training Epoch: 11 [7250/36450]\tLoss: 618.3532\n",
      "Training Epoch: 11 [7300/36450]\tLoss: 541.5156\n",
      "Training Epoch: 11 [7350/36450]\tLoss: 570.3158\n",
      "Training Epoch: 11 [7400/36450]\tLoss: 620.1138\n",
      "Training Epoch: 11 [7450/36450]\tLoss: 591.2162\n",
      "Training Epoch: 11 [7500/36450]\tLoss: 584.6046\n",
      "Training Epoch: 11 [7550/36450]\tLoss: 612.1650\n",
      "Training Epoch: 11 [7600/36450]\tLoss: 555.8838\n",
      "Training Epoch: 11 [7650/36450]\tLoss: 577.5764\n",
      "Training Epoch: 11 [7700/36450]\tLoss: 575.8774\n",
      "Training Epoch: 11 [7750/36450]\tLoss: 610.6885\n",
      "Training Epoch: 11 [7800/36450]\tLoss: 576.4274\n",
      "Training Epoch: 11 [7850/36450]\tLoss: 591.8896\n",
      "Training Epoch: 11 [7900/36450]\tLoss: 567.7342\n",
      "Training Epoch: 11 [7950/36450]\tLoss: 577.4509\n",
      "Training Epoch: 11 [8000/36450]\tLoss: 551.2125\n",
      "Training Epoch: 11 [8050/36450]\tLoss: 591.8088\n",
      "Training Epoch: 11 [8100/36450]\tLoss: 571.5110\n",
      "Training Epoch: 11 [8150/36450]\tLoss: 585.8221\n",
      "Training Epoch: 11 [8200/36450]\tLoss: 574.7308\n",
      "Training Epoch: 11 [8250/36450]\tLoss: 573.1524\n",
      "Training Epoch: 11 [8300/36450]\tLoss: 564.0916\n",
      "Training Epoch: 11 [8350/36450]\tLoss: 552.8777\n",
      "Training Epoch: 11 [8400/36450]\tLoss: 580.7001\n",
      "Training Epoch: 11 [8450/36450]\tLoss: 538.1110\n",
      "Training Epoch: 11 [8500/36450]\tLoss: 567.5208\n",
      "Training Epoch: 11 [8550/36450]\tLoss: 565.8895\n",
      "Training Epoch: 11 [8600/36450]\tLoss: 592.4043\n",
      "Training Epoch: 11 [8650/36450]\tLoss: 600.8134\n",
      "Training Epoch: 11 [8700/36450]\tLoss: 545.3453\n",
      "Training Epoch: 11 [8750/36450]\tLoss: 556.1899\n",
      "Training Epoch: 11 [8800/36450]\tLoss: 564.4144\n",
      "Training Epoch: 11 [8850/36450]\tLoss: 587.5914\n",
      "Training Epoch: 11 [8900/36450]\tLoss: 589.2861\n",
      "Training Epoch: 11 [8950/36450]\tLoss: 564.9911\n",
      "Training Epoch: 11 [9000/36450]\tLoss: 561.8788\n",
      "Training Epoch: 11 [9050/36450]\tLoss: 561.8315\n",
      "Training Epoch: 11 [9100/36450]\tLoss: 547.9722\n",
      "Training Epoch: 11 [9150/36450]\tLoss: 561.5067\n",
      "Training Epoch: 11 [9200/36450]\tLoss: 562.9920\n",
      "Training Epoch: 11 [9250/36450]\tLoss: 594.6678\n",
      "Training Epoch: 11 [9300/36450]\tLoss: 550.2022\n",
      "Training Epoch: 11 [9350/36450]\tLoss: 573.7642\n",
      "Training Epoch: 11 [9400/36450]\tLoss: 571.1642\n",
      "Training Epoch: 11 [9450/36450]\tLoss: 585.9134\n",
      "Training Epoch: 11 [9500/36450]\tLoss: 552.1887\n",
      "Training Epoch: 11 [9550/36450]\tLoss: 593.1364\n",
      "Training Epoch: 11 [9600/36450]\tLoss: 533.9108\n",
      "Training Epoch: 11 [9650/36450]\tLoss: 568.2134\n",
      "Training Epoch: 11 [9700/36450]\tLoss: 575.1434\n",
      "Training Epoch: 11 [9750/36450]\tLoss: 541.3323\n",
      "Training Epoch: 11 [9800/36450]\tLoss: 582.4386\n",
      "Training Epoch: 11 [9850/36450]\tLoss: 582.4951\n",
      "Training Epoch: 11 [9900/36450]\tLoss: 553.8369\n",
      "Training Epoch: 11 [9950/36450]\tLoss: 564.7202\n",
      "Training Epoch: 11 [10000/36450]\tLoss: 598.9036\n",
      "Training Epoch: 11 [10050/36450]\tLoss: 542.9176\n",
      "Training Epoch: 11 [10100/36450]\tLoss: 573.2936\n",
      "Training Epoch: 11 [10150/36450]\tLoss: 571.1342\n",
      "Training Epoch: 11 [10200/36450]\tLoss: 547.4277\n",
      "Training Epoch: 11 [10250/36450]\tLoss: 572.3578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [10300/36450]\tLoss: 595.2765\n",
      "Training Epoch: 11 [10350/36450]\tLoss: 578.9129\n",
      "Training Epoch: 11 [10400/36450]\tLoss: 569.3868\n",
      "Training Epoch: 11 [10450/36450]\tLoss: 565.9609\n",
      "Training Epoch: 11 [10500/36450]\tLoss: 588.5435\n",
      "Training Epoch: 11 [10550/36450]\tLoss: 537.6497\n",
      "Training Epoch: 11 [10600/36450]\tLoss: 572.5538\n",
      "Training Epoch: 11 [10650/36450]\tLoss: 561.8027\n",
      "Training Epoch: 11 [10700/36450]\tLoss: 577.9853\n",
      "Training Epoch: 11 [10750/36450]\tLoss: 535.1990\n",
      "Training Epoch: 11 [10800/36450]\tLoss: 562.6391\n",
      "Training Epoch: 11 [10850/36450]\tLoss: 545.5885\n",
      "Training Epoch: 11 [10900/36450]\tLoss: 561.5842\n",
      "Training Epoch: 11 [10950/36450]\tLoss: 592.1043\n",
      "Training Epoch: 11 [11000/36450]\tLoss: 572.4380\n",
      "Training Epoch: 11 [11050/36450]\tLoss: 557.7715\n",
      "Training Epoch: 11 [11100/36450]\tLoss: 582.4594\n",
      "Training Epoch: 11 [11150/36450]\tLoss: 591.5012\n",
      "Training Epoch: 11 [11200/36450]\tLoss: 538.4022\n",
      "Training Epoch: 11 [11250/36450]\tLoss: 590.6434\n",
      "Training Epoch: 11 [11300/36450]\tLoss: 568.1507\n",
      "Training Epoch: 11 [11350/36450]\tLoss: 571.4738\n",
      "Training Epoch: 11 [11400/36450]\tLoss: 560.0471\n",
      "Training Epoch: 11 [11450/36450]\tLoss: 550.4362\n",
      "Training Epoch: 11 [11500/36450]\tLoss: 577.1535\n",
      "Training Epoch: 11 [11550/36450]\tLoss: 574.4886\n",
      "Training Epoch: 11 [11600/36450]\tLoss: 536.2691\n",
      "Training Epoch: 11 [11650/36450]\tLoss: 562.7217\n",
      "Training Epoch: 11 [11700/36450]\tLoss: 562.5264\n",
      "Training Epoch: 11 [11750/36450]\tLoss: 602.6609\n",
      "Training Epoch: 11 [11800/36450]\tLoss: 543.5889\n",
      "Training Epoch: 11 [11850/36450]\tLoss: 590.1834\n",
      "Training Epoch: 11 [11900/36450]\tLoss: 527.2687\n",
      "Training Epoch: 11 [11950/36450]\tLoss: 569.7882\n",
      "Training Epoch: 11 [12000/36450]\tLoss: 554.7989\n",
      "Training Epoch: 11 [12050/36450]\tLoss: 581.0997\n",
      "Training Epoch: 11 [12100/36450]\tLoss: 532.8419\n",
      "Training Epoch: 11 [12150/36450]\tLoss: 601.3106\n",
      "Training Epoch: 11 [12200/36450]\tLoss: 563.7916\n",
      "Training Epoch: 11 [12250/36450]\tLoss: 574.6592\n",
      "Training Epoch: 11 [12300/36450]\tLoss: 568.7230\n",
      "Training Epoch: 11 [12350/36450]\tLoss: 537.2385\n",
      "Training Epoch: 11 [12400/36450]\tLoss: 567.7802\n",
      "Training Epoch: 11 [12450/36450]\tLoss: 538.2712\n",
      "Training Epoch: 11 [12500/36450]\tLoss: 558.3814\n",
      "Training Epoch: 11 [12550/36450]\tLoss: 554.3327\n",
      "Training Epoch: 11 [12600/36450]\tLoss: 556.7949\n",
      "Training Epoch: 11 [12650/36450]\tLoss: 562.3475\n",
      "Training Epoch: 11 [12700/36450]\tLoss: 580.6825\n",
      "Training Epoch: 11 [12750/36450]\tLoss: 565.3430\n",
      "Training Epoch: 11 [12800/36450]\tLoss: 541.4038\n",
      "Training Epoch: 11 [12850/36450]\tLoss: 582.5959\n",
      "Training Epoch: 11 [12900/36450]\tLoss: 588.1396\n",
      "Training Epoch: 11 [12950/36450]\tLoss: 544.7242\n",
      "Training Epoch: 11 [13000/36450]\tLoss: 601.3708\n",
      "Training Epoch: 11 [13050/36450]\tLoss: 591.0936\n",
      "Training Epoch: 11 [13100/36450]\tLoss: 564.7164\n",
      "Training Epoch: 11 [13150/36450]\tLoss: 518.3063\n",
      "Training Epoch: 11 [13200/36450]\tLoss: 575.0630\n",
      "Training Epoch: 11 [13250/36450]\tLoss: 614.7433\n",
      "Training Epoch: 11 [13300/36450]\tLoss: 572.9950\n",
      "Training Epoch: 11 [13350/36450]\tLoss: 574.4579\n",
      "Training Epoch: 11 [13400/36450]\tLoss: 591.6215\n",
      "Training Epoch: 11 [13450/36450]\tLoss: 548.9046\n",
      "Training Epoch: 11 [13500/36450]\tLoss: 567.4872\n",
      "Training Epoch: 11 [13550/36450]\tLoss: 573.6269\n",
      "Training Epoch: 11 [13600/36450]\tLoss: 580.6996\n",
      "Training Epoch: 11 [13650/36450]\tLoss: 541.3096\n",
      "Training Epoch: 11 [13700/36450]\tLoss: 560.8698\n",
      "Training Epoch: 11 [13750/36450]\tLoss: 560.8980\n",
      "Training Epoch: 11 [13800/36450]\tLoss: 560.0797\n",
      "Training Epoch: 11 [13850/36450]\tLoss: 565.7825\n",
      "Training Epoch: 11 [13900/36450]\tLoss: 548.9461\n",
      "Training Epoch: 11 [13950/36450]\tLoss: 544.4186\n",
      "Training Epoch: 11 [14000/36450]\tLoss: 556.6180\n",
      "Training Epoch: 11 [14050/36450]\tLoss: 531.2490\n",
      "Training Epoch: 11 [14100/36450]\tLoss: 590.9523\n",
      "Training Epoch: 11 [14150/36450]\tLoss: 584.5080\n",
      "Training Epoch: 11 [14200/36450]\tLoss: 534.3563\n",
      "Training Epoch: 11 [14250/36450]\tLoss: 571.5872\n",
      "Training Epoch: 11 [14300/36450]\tLoss: 584.4003\n",
      "Training Epoch: 11 [14350/36450]\tLoss: 584.9127\n",
      "Training Epoch: 11 [14400/36450]\tLoss: 552.1130\n",
      "Training Epoch: 11 [14450/36450]\tLoss: 540.7368\n",
      "Training Epoch: 11 [14500/36450]\tLoss: 569.7457\n",
      "Training Epoch: 11 [14550/36450]\tLoss: 536.7499\n",
      "Training Epoch: 11 [14600/36450]\tLoss: 582.9168\n",
      "Training Epoch: 11 [14650/36450]\tLoss: 595.7974\n",
      "Training Epoch: 11 [14700/36450]\tLoss: 569.6386\n",
      "Training Epoch: 11 [14750/36450]\tLoss: 582.3569\n",
      "Training Epoch: 11 [14800/36450]\tLoss: 568.6801\n",
      "Training Epoch: 11 [14850/36450]\tLoss: 595.4670\n",
      "Training Epoch: 11 [14900/36450]\tLoss: 582.3619\n",
      "Training Epoch: 11 [14950/36450]\tLoss: 545.9932\n",
      "Training Epoch: 11 [15000/36450]\tLoss: 547.1920\n",
      "Training Epoch: 11 [15050/36450]\tLoss: 567.9695\n",
      "Training Epoch: 11 [15100/36450]\tLoss: 592.4688\n",
      "Training Epoch: 11 [15150/36450]\tLoss: 624.0191\n",
      "Training Epoch: 11 [15200/36450]\tLoss: 656.1007\n",
      "Training Epoch: 11 [15250/36450]\tLoss: 667.9845\n",
      "Training Epoch: 11 [15300/36450]\tLoss: 639.3282\n",
      "Training Epoch: 11 [15350/36450]\tLoss: 636.9977\n",
      "Training Epoch: 11 [15400/36450]\tLoss: 580.4503\n",
      "Training Epoch: 11 [15450/36450]\tLoss: 576.9216\n",
      "Training Epoch: 11 [15500/36450]\tLoss: 573.2485\n",
      "Training Epoch: 11 [15550/36450]\tLoss: 584.1458\n",
      "Training Epoch: 11 [15600/36450]\tLoss: 599.1580\n",
      "Training Epoch: 11 [15650/36450]\tLoss: 634.1454\n",
      "Training Epoch: 11 [15700/36450]\tLoss: 602.6308\n",
      "Training Epoch: 11 [15750/36450]\tLoss: 612.6080\n",
      "Training Epoch: 11 [15800/36450]\tLoss: 568.4041\n",
      "Training Epoch: 11 [15850/36450]\tLoss: 569.9941\n",
      "Training Epoch: 11 [15900/36450]\tLoss: 574.5114\n",
      "Training Epoch: 11 [15950/36450]\tLoss: 591.7327\n",
      "Training Epoch: 11 [16000/36450]\tLoss: 561.9512\n",
      "Training Epoch: 11 [16050/36450]\tLoss: 583.5964\n",
      "Training Epoch: 11 [16100/36450]\tLoss: 589.7905\n",
      "Training Epoch: 11 [16150/36450]\tLoss: 571.7759\n",
      "Training Epoch: 11 [16200/36450]\tLoss: 595.5627\n",
      "Training Epoch: 11 [16250/36450]\tLoss: 589.7416\n",
      "Training Epoch: 11 [16300/36450]\tLoss: 558.5134\n",
      "Training Epoch: 11 [16350/36450]\tLoss: 546.7538\n",
      "Training Epoch: 11 [16400/36450]\tLoss: 585.5159\n",
      "Training Epoch: 11 [16450/36450]\tLoss: 565.7159\n",
      "Training Epoch: 11 [16500/36450]\tLoss: 564.4865\n",
      "Training Epoch: 11 [16550/36450]\tLoss: 573.4775\n",
      "Training Epoch: 11 [16600/36450]\tLoss: 550.3415\n",
      "Training Epoch: 11 [16650/36450]\tLoss: 559.0652\n",
      "Training Epoch: 11 [16700/36450]\tLoss: 567.7156\n",
      "Training Epoch: 11 [16750/36450]\tLoss: 576.6299\n",
      "Training Epoch: 11 [16800/36450]\tLoss: 592.0406\n",
      "Training Epoch: 11 [16850/36450]\tLoss: 574.2617\n",
      "Training Epoch: 11 [16900/36450]\tLoss: 571.1932\n",
      "Training Epoch: 11 [16950/36450]\tLoss: 573.5388\n",
      "Training Epoch: 11 [17000/36450]\tLoss: 593.5168\n",
      "Training Epoch: 11 [17050/36450]\tLoss: 572.7382\n",
      "Training Epoch: 11 [17100/36450]\tLoss: 541.5075\n",
      "Training Epoch: 11 [17150/36450]\tLoss: 617.7492\n",
      "Training Epoch: 11 [17200/36450]\tLoss: 581.2641\n",
      "Training Epoch: 11 [17250/36450]\tLoss: 561.9431\n",
      "Training Epoch: 11 [17300/36450]\tLoss: 576.4606\n",
      "Training Epoch: 11 [17350/36450]\tLoss: 593.3267\n",
      "Training Epoch: 11 [17400/36450]\tLoss: 540.8558\n",
      "Training Epoch: 11 [17450/36450]\tLoss: 551.2112\n",
      "Training Epoch: 11 [17500/36450]\tLoss: 543.3529\n",
      "Training Epoch: 11 [17550/36450]\tLoss: 560.7798\n",
      "Training Epoch: 11 [17600/36450]\tLoss: 548.3607\n",
      "Training Epoch: 11 [17650/36450]\tLoss: 584.0496\n",
      "Training Epoch: 11 [17700/36450]\tLoss: 589.1805\n",
      "Training Epoch: 11 [17750/36450]\tLoss: 570.0137\n",
      "Training Epoch: 11 [17800/36450]\tLoss: 552.5953\n",
      "Training Epoch: 11 [17850/36450]\tLoss: 580.0631\n",
      "Training Epoch: 11 [17900/36450]\tLoss: 547.4835\n",
      "Training Epoch: 11 [17950/36450]\tLoss: 561.7325\n",
      "Training Epoch: 11 [18000/36450]\tLoss: 535.3708\n",
      "Training Epoch: 11 [18050/36450]\tLoss: 530.4684\n",
      "Training Epoch: 11 [18100/36450]\tLoss: 540.3619\n",
      "Training Epoch: 11 [18150/36450]\tLoss: 581.6647\n",
      "Training Epoch: 11 [18200/36450]\tLoss: 582.5427\n",
      "Training Epoch: 11 [18250/36450]\tLoss: 565.7265\n",
      "Training Epoch: 11 [18300/36450]\tLoss: 562.6282\n",
      "Training Epoch: 11 [18350/36450]\tLoss: 533.0478\n",
      "Training Epoch: 11 [18400/36450]\tLoss: 547.4222\n",
      "Training Epoch: 11 [18450/36450]\tLoss: 555.8010\n",
      "Training Epoch: 11 [18500/36450]\tLoss: 579.5863\n",
      "Training Epoch: 11 [18550/36450]\tLoss: 533.6464\n",
      "Training Epoch: 11 [18600/36450]\tLoss: 557.2255\n",
      "Training Epoch: 11 [18650/36450]\tLoss: 584.4412\n",
      "Training Epoch: 11 [18700/36450]\tLoss: 563.6523\n",
      "Training Epoch: 11 [18750/36450]\tLoss: 575.1796\n",
      "Training Epoch: 11 [18800/36450]\tLoss: 533.5355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [18850/36450]\tLoss: 551.0563\n",
      "Training Epoch: 11 [18900/36450]\tLoss: 573.6948\n",
      "Training Epoch: 11 [18950/36450]\tLoss: 556.8003\n",
      "Training Epoch: 11 [19000/36450]\tLoss: 558.7866\n",
      "Training Epoch: 11 [19050/36450]\tLoss: 538.5281\n",
      "Training Epoch: 11 [19100/36450]\tLoss: 562.6932\n",
      "Training Epoch: 11 [19150/36450]\tLoss: 579.5687\n",
      "Training Epoch: 11 [19200/36450]\tLoss: 575.1104\n",
      "Training Epoch: 11 [19250/36450]\tLoss: 555.2442\n",
      "Training Epoch: 11 [19300/36450]\tLoss: 553.4845\n",
      "Training Epoch: 11 [19350/36450]\tLoss: 590.1317\n",
      "Training Epoch: 11 [19400/36450]\tLoss: 551.6068\n",
      "Training Epoch: 11 [19450/36450]\tLoss: 587.8848\n",
      "Training Epoch: 11 [19500/36450]\tLoss: 558.1232\n",
      "Training Epoch: 11 [19550/36450]\tLoss: 579.4606\n",
      "Training Epoch: 11 [19600/36450]\tLoss: 612.2231\n",
      "Training Epoch: 11 [19650/36450]\tLoss: 639.1791\n",
      "Training Epoch: 11 [19700/36450]\tLoss: 609.1375\n",
      "Training Epoch: 11 [19750/36450]\tLoss: 577.2282\n",
      "Training Epoch: 11 [19800/36450]\tLoss: 549.4592\n",
      "Training Epoch: 11 [19850/36450]\tLoss: 569.9789\n",
      "Training Epoch: 11 [19900/36450]\tLoss: 613.5120\n",
      "Training Epoch: 11 [19950/36450]\tLoss: 598.7457\n",
      "Training Epoch: 11 [20000/36450]\tLoss: 552.5242\n",
      "Training Epoch: 11 [20050/36450]\tLoss: 551.4551\n",
      "Training Epoch: 11 [20100/36450]\tLoss: 588.8865\n",
      "Training Epoch: 11 [20150/36450]\tLoss: 579.5583\n",
      "Training Epoch: 11 [20200/36450]\tLoss: 572.6180\n",
      "Training Epoch: 11 [20250/36450]\tLoss: 567.0750\n",
      "Training Epoch: 11 [20300/36450]\tLoss: 563.0128\n",
      "Training Epoch: 11 [20350/36450]\tLoss: 570.1919\n",
      "Training Epoch: 11 [20400/36450]\tLoss: 559.0511\n",
      "Training Epoch: 11 [20450/36450]\tLoss: 567.9581\n",
      "Training Epoch: 11 [20500/36450]\tLoss: 580.8694\n",
      "Training Epoch: 11 [20550/36450]\tLoss: 593.5214\n",
      "Training Epoch: 11 [20600/36450]\tLoss: 583.8359\n",
      "Training Epoch: 11 [20650/36450]\tLoss: 595.8238\n",
      "Training Epoch: 11 [20700/36450]\tLoss: 557.5571\n",
      "Training Epoch: 11 [20750/36450]\tLoss: 518.4122\n",
      "Training Epoch: 11 [20800/36450]\tLoss: 565.6953\n",
      "Training Epoch: 11 [20850/36450]\tLoss: 527.2368\n",
      "Training Epoch: 11 [20900/36450]\tLoss: 566.0457\n",
      "Training Epoch: 11 [20950/36450]\tLoss: 557.2212\n",
      "Training Epoch: 11 [21000/36450]\tLoss: 578.4677\n",
      "Training Epoch: 11 [21050/36450]\tLoss: 564.2164\n",
      "Training Epoch: 11 [21100/36450]\tLoss: 561.7812\n",
      "Training Epoch: 11 [21150/36450]\tLoss: 595.6707\n",
      "Training Epoch: 11 [21200/36450]\tLoss: 519.2004\n",
      "Training Epoch: 11 [21250/36450]\tLoss: 553.5324\n",
      "Training Epoch: 11 [21300/36450]\tLoss: 572.6210\n",
      "Training Epoch: 11 [21350/36450]\tLoss: 596.1037\n",
      "Training Epoch: 11 [21400/36450]\tLoss: 602.8613\n",
      "Training Epoch: 11 [21450/36450]\tLoss: 558.4894\n",
      "Training Epoch: 11 [21500/36450]\tLoss: 549.5798\n",
      "Training Epoch: 11 [21550/36450]\tLoss: 578.1752\n",
      "Training Epoch: 11 [21600/36450]\tLoss: 549.7362\n",
      "Training Epoch: 11 [21650/36450]\tLoss: 511.6127\n",
      "Training Epoch: 11 [21700/36450]\tLoss: 541.4355\n",
      "Training Epoch: 11 [21750/36450]\tLoss: 541.1012\n",
      "Training Epoch: 11 [21800/36450]\tLoss: 585.0886\n",
      "Training Epoch: 11 [21850/36450]\tLoss: 568.6410\n",
      "Training Epoch: 11 [21900/36450]\tLoss: 549.0388\n",
      "Training Epoch: 11 [21950/36450]\tLoss: 555.8897\n",
      "Training Epoch: 11 [22000/36450]\tLoss: 551.8047\n",
      "Training Epoch: 11 [22050/36450]\tLoss: 570.2185\n",
      "Training Epoch: 11 [22100/36450]\tLoss: 597.5969\n",
      "Training Epoch: 11 [22150/36450]\tLoss: 586.7518\n",
      "Training Epoch: 11 [22200/36450]\tLoss: 570.8591\n",
      "Training Epoch: 11 [22250/36450]\tLoss: 581.5589\n",
      "Training Epoch: 11 [22300/36450]\tLoss: 576.4232\n",
      "Training Epoch: 11 [22350/36450]\tLoss: 553.7542\n",
      "Training Epoch: 11 [22400/36450]\tLoss: 530.6802\n",
      "Training Epoch: 11 [22450/36450]\tLoss: 573.4377\n",
      "Training Epoch: 11 [22500/36450]\tLoss: 567.5956\n",
      "Training Epoch: 11 [22550/36450]\tLoss: 568.8174\n",
      "Training Epoch: 11 [22600/36450]\tLoss: 570.6517\n",
      "Training Epoch: 11 [22650/36450]\tLoss: 558.1580\n",
      "Training Epoch: 11 [22700/36450]\tLoss: 597.4940\n",
      "Training Epoch: 11 [22750/36450]\tLoss: 532.7035\n",
      "Training Epoch: 11 [22800/36450]\tLoss: 571.3540\n",
      "Training Epoch: 11 [22850/36450]\tLoss: 555.2410\n",
      "Training Epoch: 11 [22900/36450]\tLoss: 598.5348\n",
      "Training Epoch: 11 [22950/36450]\tLoss: 553.0920\n",
      "Training Epoch: 11 [23000/36450]\tLoss: 524.7423\n",
      "Training Epoch: 11 [23050/36450]\tLoss: 561.0292\n",
      "Training Epoch: 11 [23100/36450]\tLoss: 548.2238\n",
      "Training Epoch: 11 [23150/36450]\tLoss: 569.7895\n",
      "Training Epoch: 11 [23200/36450]\tLoss: 547.0181\n",
      "Training Epoch: 11 [23250/36450]\tLoss: 576.2694\n",
      "Training Epoch: 11 [23300/36450]\tLoss: 549.4863\n",
      "Training Epoch: 11 [23350/36450]\tLoss: 553.8912\n",
      "Training Epoch: 11 [23400/36450]\tLoss: 578.7419\n",
      "Training Epoch: 11 [23450/36450]\tLoss: 543.8086\n",
      "Training Epoch: 11 [23500/36450]\tLoss: 546.6912\n",
      "Training Epoch: 11 [23550/36450]\tLoss: 551.8976\n",
      "Training Epoch: 11 [23600/36450]\tLoss: 555.1254\n",
      "Training Epoch: 11 [23650/36450]\tLoss: 567.7933\n",
      "Training Epoch: 11 [23700/36450]\tLoss: 580.1332\n",
      "Training Epoch: 11 [23750/36450]\tLoss: 537.7780\n",
      "Training Epoch: 11 [23800/36450]\tLoss: 553.3170\n",
      "Training Epoch: 11 [23850/36450]\tLoss: 559.5296\n",
      "Training Epoch: 11 [23900/36450]\tLoss: 561.4423\n",
      "Training Epoch: 11 [23950/36450]\tLoss: 554.0781\n",
      "Training Epoch: 11 [24000/36450]\tLoss: 543.1598\n",
      "Training Epoch: 11 [24050/36450]\tLoss: 560.3746\n",
      "Training Epoch: 11 [24100/36450]\tLoss: 558.8875\n",
      "Training Epoch: 11 [24150/36450]\tLoss: 576.3119\n",
      "Training Epoch: 11 [24200/36450]\tLoss: 553.2333\n",
      "Training Epoch: 11 [24250/36450]\tLoss: 558.5187\n",
      "Training Epoch: 11 [24300/36450]\tLoss: 604.2364\n",
      "Training Epoch: 11 [24350/36450]\tLoss: 567.6263\n",
      "Training Epoch: 11 [24400/36450]\tLoss: 604.8943\n",
      "Training Epoch: 11 [24450/36450]\tLoss: 567.8685\n",
      "Training Epoch: 11 [24500/36450]\tLoss: 605.6311\n",
      "Training Epoch: 11 [24550/36450]\tLoss: 573.6754\n",
      "Training Epoch: 11 [24600/36450]\tLoss: 538.9732\n",
      "Training Epoch: 11 [24650/36450]\tLoss: 591.6242\n",
      "Training Epoch: 11 [24700/36450]\tLoss: 549.6750\n",
      "Training Epoch: 11 [24750/36450]\tLoss: 555.7697\n",
      "Training Epoch: 11 [24800/36450]\tLoss: 537.6961\n",
      "Training Epoch: 11 [24850/36450]\tLoss: 571.9661\n",
      "Training Epoch: 11 [24900/36450]\tLoss: 576.3045\n",
      "Training Epoch: 11 [24950/36450]\tLoss: 566.1284\n",
      "Training Epoch: 11 [25000/36450]\tLoss: 559.5056\n",
      "Training Epoch: 11 [25050/36450]\tLoss: 565.5010\n",
      "Training Epoch: 11 [25100/36450]\tLoss: 538.7602\n",
      "Training Epoch: 11 [25150/36450]\tLoss: 540.1524\n",
      "Training Epoch: 11 [25200/36450]\tLoss: 556.6909\n",
      "Training Epoch: 11 [25250/36450]\tLoss: 591.0927\n",
      "Training Epoch: 11 [25300/36450]\tLoss: 566.3691\n",
      "Training Epoch: 11 [25350/36450]\tLoss: 553.7789\n",
      "Training Epoch: 11 [25400/36450]\tLoss: 552.7935\n",
      "Training Epoch: 11 [25450/36450]\tLoss: 568.4392\n",
      "Training Epoch: 11 [25500/36450]\tLoss: 586.7284\n",
      "Training Epoch: 11 [25550/36450]\tLoss: 562.7280\n",
      "Training Epoch: 11 [25600/36450]\tLoss: 545.7698\n",
      "Training Epoch: 11 [25650/36450]\tLoss: 575.1990\n",
      "Training Epoch: 11 [25700/36450]\tLoss: 575.6954\n",
      "Training Epoch: 11 [25750/36450]\tLoss: 524.5797\n",
      "Training Epoch: 11 [25800/36450]\tLoss: 572.2973\n",
      "Training Epoch: 11 [25850/36450]\tLoss: 559.7847\n",
      "Training Epoch: 11 [25900/36450]\tLoss: 582.8091\n",
      "Training Epoch: 11 [25950/36450]\tLoss: 587.7065\n",
      "Training Epoch: 11 [26000/36450]\tLoss: 570.0112\n",
      "Training Epoch: 11 [26050/36450]\tLoss: 563.1041\n",
      "Training Epoch: 11 [26100/36450]\tLoss: 560.9012\n",
      "Training Epoch: 11 [26150/36450]\tLoss: 533.2078\n",
      "Training Epoch: 11 [26200/36450]\tLoss: 586.2258\n",
      "Training Epoch: 11 [26250/36450]\tLoss: 568.4263\n",
      "Training Epoch: 11 [26300/36450]\tLoss: 573.3191\n",
      "Training Epoch: 11 [26350/36450]\tLoss: 583.6870\n",
      "Training Epoch: 11 [26400/36450]\tLoss: 597.3398\n",
      "Training Epoch: 11 [26450/36450]\tLoss: 576.6311\n",
      "Training Epoch: 11 [26500/36450]\tLoss: 565.4285\n",
      "Training Epoch: 11 [26550/36450]\tLoss: 604.5226\n",
      "Training Epoch: 11 [26600/36450]\tLoss: 614.0719\n",
      "Training Epoch: 11 [26650/36450]\tLoss: 580.3286\n",
      "Training Epoch: 11 [26700/36450]\tLoss: 563.8320\n",
      "Training Epoch: 11 [26750/36450]\tLoss: 572.1305\n",
      "Training Epoch: 11 [26800/36450]\tLoss: 583.6200\n",
      "Training Epoch: 11 [26850/36450]\tLoss: 545.6599\n",
      "Training Epoch: 11 [26900/36450]\tLoss: 565.6788\n",
      "Training Epoch: 11 [26950/36450]\tLoss: 557.4899\n",
      "Training Epoch: 11 [27000/36450]\tLoss: 590.6741\n",
      "Training Epoch: 11 [27050/36450]\tLoss: 543.7712\n",
      "Training Epoch: 11 [27100/36450]\tLoss: 577.9155\n",
      "Training Epoch: 11 [27150/36450]\tLoss: 547.2787\n",
      "Training Epoch: 11 [27200/36450]\tLoss: 586.6468\n",
      "Training Epoch: 11 [27250/36450]\tLoss: 550.1364\n",
      "Training Epoch: 11 [27300/36450]\tLoss: 528.4923\n",
      "Training Epoch: 11 [27350/36450]\tLoss: 561.7443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [27400/36450]\tLoss: 591.4555\n",
      "Training Epoch: 11 [27450/36450]\tLoss: 553.4089\n",
      "Training Epoch: 11 [27500/36450]\tLoss: 524.6523\n",
      "Training Epoch: 11 [27550/36450]\tLoss: 544.1720\n",
      "Training Epoch: 11 [27600/36450]\tLoss: 561.9709\n",
      "Training Epoch: 11 [27650/36450]\tLoss: 546.9634\n",
      "Training Epoch: 11 [27700/36450]\tLoss: 588.1821\n",
      "Training Epoch: 11 [27750/36450]\tLoss: 547.4918\n",
      "Training Epoch: 11 [27800/36450]\tLoss: 558.6031\n",
      "Training Epoch: 11 [27850/36450]\tLoss: 520.6452\n",
      "Training Epoch: 11 [27900/36450]\tLoss: 573.6685\n",
      "Training Epoch: 11 [27950/36450]\tLoss: 593.3866\n",
      "Training Epoch: 11 [28000/36450]\tLoss: 575.2418\n",
      "Training Epoch: 11 [28050/36450]\tLoss: 567.3958\n",
      "Training Epoch: 11 [28100/36450]\tLoss: 529.4731\n",
      "Training Epoch: 11 [28150/36450]\tLoss: 584.7517\n",
      "Training Epoch: 11 [28200/36450]\tLoss: 539.7315\n",
      "Training Epoch: 11 [28250/36450]\tLoss: 539.9187\n",
      "Training Epoch: 11 [28300/36450]\tLoss: 520.0214\n",
      "Training Epoch: 11 [28350/36450]\tLoss: 584.5162\n",
      "Training Epoch: 11 [28400/36450]\tLoss: 547.4479\n",
      "Training Epoch: 11 [28450/36450]\tLoss: 532.0389\n",
      "Training Epoch: 11 [28500/36450]\tLoss: 570.0762\n",
      "Training Epoch: 11 [28550/36450]\tLoss: 542.1160\n",
      "Training Epoch: 11 [28600/36450]\tLoss: 555.8414\n",
      "Training Epoch: 11 [28650/36450]\tLoss: 554.9725\n",
      "Training Epoch: 11 [28700/36450]\tLoss: 569.0607\n",
      "Training Epoch: 11 [28750/36450]\tLoss: 582.4671\n",
      "Training Epoch: 11 [28800/36450]\tLoss: 572.6711\n",
      "Training Epoch: 11 [28850/36450]\tLoss: 547.4386\n",
      "Training Epoch: 11 [28900/36450]\tLoss: 549.5263\n",
      "Training Epoch: 11 [28950/36450]\tLoss: 540.9628\n",
      "Training Epoch: 11 [29000/36450]\tLoss: 556.0756\n",
      "Training Epoch: 11 [29050/36450]\tLoss: 569.1743\n",
      "Training Epoch: 11 [29100/36450]\tLoss: 560.3967\n",
      "Training Epoch: 11 [29150/36450]\tLoss: 583.3034\n",
      "Training Epoch: 11 [29200/36450]\tLoss: 552.1329\n",
      "Training Epoch: 11 [29250/36450]\tLoss: 549.5952\n",
      "Training Epoch: 11 [29300/36450]\tLoss: 566.9103\n",
      "Training Epoch: 11 [29350/36450]\tLoss: 542.6183\n",
      "Training Epoch: 11 [29400/36450]\tLoss: 533.7073\n",
      "Training Epoch: 11 [29450/36450]\tLoss: 560.4231\n",
      "Training Epoch: 11 [29500/36450]\tLoss: 552.9246\n",
      "Training Epoch: 11 [29550/36450]\tLoss: 578.7383\n",
      "Training Epoch: 11 [29600/36450]\tLoss: 562.5465\n",
      "Training Epoch: 11 [29650/36450]\tLoss: 545.1860\n",
      "Training Epoch: 11 [29700/36450]\tLoss: 568.3326\n",
      "Training Epoch: 11 [29750/36450]\tLoss: 539.9457\n",
      "Training Epoch: 11 [29800/36450]\tLoss: 587.2017\n",
      "Training Epoch: 11 [29850/36450]\tLoss: 592.1265\n",
      "Training Epoch: 11 [29900/36450]\tLoss: 571.0940\n",
      "Training Epoch: 11 [29950/36450]\tLoss: 607.0594\n",
      "Training Epoch: 11 [30000/36450]\tLoss: 564.7647\n",
      "Training Epoch: 11 [30050/36450]\tLoss: 584.9572\n",
      "Training Epoch: 11 [30100/36450]\tLoss: 584.4407\n",
      "Training Epoch: 11 [30150/36450]\tLoss: 548.0826\n",
      "Training Epoch: 11 [30200/36450]\tLoss: 524.5452\n",
      "Training Epoch: 11 [30250/36450]\tLoss: 557.2688\n",
      "Training Epoch: 11 [30300/36450]\tLoss: 572.9672\n",
      "Training Epoch: 11 [30350/36450]\tLoss: 549.1331\n",
      "Training Epoch: 11 [30400/36450]\tLoss: 563.7111\n",
      "Training Epoch: 11 [30450/36450]\tLoss: 587.7589\n",
      "Training Epoch: 11 [30500/36450]\tLoss: 585.8378\n",
      "Training Epoch: 11 [30550/36450]\tLoss: 563.8563\n",
      "Training Epoch: 11 [30600/36450]\tLoss: 573.6677\n",
      "Training Epoch: 11 [30650/36450]\tLoss: 557.8344\n",
      "Training Epoch: 11 [30700/36450]\tLoss: 519.9460\n",
      "Training Epoch: 11 [30750/36450]\tLoss: 585.9775\n",
      "Training Epoch: 11 [30800/36450]\tLoss: 570.3595\n",
      "Training Epoch: 11 [30850/36450]\tLoss: 537.9725\n",
      "Training Epoch: 11 [30900/36450]\tLoss: 554.6255\n",
      "Training Epoch: 11 [30950/36450]\tLoss: 554.3372\n",
      "Training Epoch: 11 [31000/36450]\tLoss: 549.8198\n",
      "Training Epoch: 11 [31050/36450]\tLoss: 569.9995\n",
      "Training Epoch: 11 [31100/36450]\tLoss: 541.6413\n",
      "Training Epoch: 11 [31150/36450]\tLoss: 614.0156\n",
      "Training Epoch: 11 [31200/36450]\tLoss: 561.1231\n",
      "Training Epoch: 11 [31250/36450]\tLoss: 593.5405\n",
      "Training Epoch: 11 [31300/36450]\tLoss: 600.1118\n",
      "Training Epoch: 11 [31350/36450]\tLoss: 587.1674\n",
      "Training Epoch: 11 [31400/36450]\tLoss: 552.6656\n",
      "Training Epoch: 11 [31450/36450]\tLoss: 546.7513\n",
      "Training Epoch: 11 [31500/36450]\tLoss: 547.6188\n",
      "Training Epoch: 11 [31550/36450]\tLoss: 574.9082\n",
      "Training Epoch: 11 [31600/36450]\tLoss: 548.6840\n",
      "Training Epoch: 11 [31650/36450]\tLoss: 536.5784\n",
      "Training Epoch: 11 [31700/36450]\tLoss: 544.1600\n",
      "Training Epoch: 11 [31750/36450]\tLoss: 564.3464\n",
      "Training Epoch: 11 [31800/36450]\tLoss: 554.6411\n",
      "Training Epoch: 11 [31850/36450]\tLoss: 545.1254\n",
      "Training Epoch: 11 [31900/36450]\tLoss: 535.8827\n",
      "Training Epoch: 11 [31950/36450]\tLoss: 548.3668\n",
      "Training Epoch: 11 [32000/36450]\tLoss: 541.5313\n",
      "Training Epoch: 11 [32050/36450]\tLoss: 563.2244\n",
      "Training Epoch: 11 [32100/36450]\tLoss: 545.7407\n",
      "Training Epoch: 11 [32150/36450]\tLoss: 566.5109\n",
      "Training Epoch: 11 [32200/36450]\tLoss: 563.0184\n",
      "Training Epoch: 11 [32250/36450]\tLoss: 566.4579\n",
      "Training Epoch: 11 [32300/36450]\tLoss: 557.6335\n",
      "Training Epoch: 11 [32350/36450]\tLoss: 550.1354\n",
      "Training Epoch: 11 [32400/36450]\tLoss: 548.5773\n",
      "Training Epoch: 11 [32450/36450]\tLoss: 538.0623\n",
      "Training Epoch: 11 [32500/36450]\tLoss: 531.2134\n",
      "Training Epoch: 11 [32550/36450]\tLoss: 524.2523\n",
      "Training Epoch: 11 [32600/36450]\tLoss: 567.6656\n",
      "Training Epoch: 11 [32650/36450]\tLoss: 552.1407\n",
      "Training Epoch: 11 [32700/36450]\tLoss: 560.4075\n",
      "Training Epoch: 11 [32750/36450]\tLoss: 547.7626\n",
      "Training Epoch: 11 [32800/36450]\tLoss: 554.1831\n",
      "Training Epoch: 11 [32850/36450]\tLoss: 561.7505\n",
      "Training Epoch: 11 [32900/36450]\tLoss: 532.0013\n",
      "Training Epoch: 11 [32950/36450]\tLoss: 568.5852\n",
      "Training Epoch: 11 [33000/36450]\tLoss: 545.3907\n",
      "Training Epoch: 11 [33050/36450]\tLoss: 535.2031\n",
      "Training Epoch: 11 [33100/36450]\tLoss: 608.5303\n",
      "Training Epoch: 11 [33150/36450]\tLoss: 527.1015\n",
      "Training Epoch: 11 [33200/36450]\tLoss: 547.7544\n",
      "Training Epoch: 11 [33250/36450]\tLoss: 569.5027\n",
      "Training Epoch: 11 [33300/36450]\tLoss: 546.9904\n",
      "Training Epoch: 11 [33350/36450]\tLoss: 553.6289\n",
      "Training Epoch: 11 [33400/36450]\tLoss: 571.4319\n",
      "Training Epoch: 11 [33450/36450]\tLoss: 540.1318\n",
      "Training Epoch: 11 [33500/36450]\tLoss: 527.9062\n",
      "Training Epoch: 11 [33550/36450]\tLoss: 563.3253\n",
      "Training Epoch: 11 [33600/36450]\tLoss: 554.0208\n",
      "Training Epoch: 11 [33650/36450]\tLoss: 568.6550\n",
      "Training Epoch: 11 [33700/36450]\tLoss: 536.4249\n",
      "Training Epoch: 11 [33750/36450]\tLoss: 544.8233\n",
      "Training Epoch: 11 [33800/36450]\tLoss: 580.5171\n",
      "Training Epoch: 11 [33850/36450]\tLoss: 538.5434\n",
      "Training Epoch: 11 [33900/36450]\tLoss: 549.2484\n",
      "Training Epoch: 11 [33950/36450]\tLoss: 553.2449\n",
      "Training Epoch: 11 [34000/36450]\tLoss: 558.3489\n",
      "Training Epoch: 11 [34050/36450]\tLoss: 551.2328\n",
      "Training Epoch: 11 [34100/36450]\tLoss: 570.6510\n",
      "Training Epoch: 11 [34150/36450]\tLoss: 542.3165\n",
      "Training Epoch: 11 [34200/36450]\tLoss: 522.8642\n",
      "Training Epoch: 11 [34250/36450]\tLoss: 531.3352\n",
      "Training Epoch: 11 [34300/36450]\tLoss: 572.6141\n",
      "Training Epoch: 11 [34350/36450]\tLoss: 579.9867\n",
      "Training Epoch: 11 [34400/36450]\tLoss: 556.6061\n",
      "Training Epoch: 11 [34450/36450]\tLoss: 524.6577\n",
      "Training Epoch: 11 [34500/36450]\tLoss: 551.7068\n",
      "Training Epoch: 11 [34550/36450]\tLoss: 546.2227\n",
      "Training Epoch: 11 [34600/36450]\tLoss: 560.6642\n",
      "Training Epoch: 11 [34650/36450]\tLoss: 536.4193\n",
      "Training Epoch: 11 [34700/36450]\tLoss: 555.0428\n",
      "Training Epoch: 11 [34750/36450]\tLoss: 545.0180\n",
      "Training Epoch: 11 [34800/36450]\tLoss: 571.1523\n",
      "Training Epoch: 11 [34850/36450]\tLoss: 569.8173\n",
      "Training Epoch: 11 [34900/36450]\tLoss: 558.1801\n",
      "Training Epoch: 11 [34950/36450]\tLoss: 593.0326\n",
      "Training Epoch: 11 [35000/36450]\tLoss: 556.6973\n",
      "Training Epoch: 11 [35050/36450]\tLoss: 566.4764\n",
      "Training Epoch: 11 [35100/36450]\tLoss: 575.8430\n",
      "Training Epoch: 11 [35150/36450]\tLoss: 592.4271\n",
      "Training Epoch: 11 [35200/36450]\tLoss: 532.0172\n",
      "Training Epoch: 11 [35250/36450]\tLoss: 564.3621\n",
      "Training Epoch: 11 [35300/36450]\tLoss: 580.0748\n",
      "Training Epoch: 11 [35350/36450]\tLoss: 576.3436\n",
      "Training Epoch: 11 [35400/36450]\tLoss: 535.7081\n",
      "Training Epoch: 11 [35450/36450]\tLoss: 570.4792\n",
      "Training Epoch: 11 [35500/36450]\tLoss: 540.6676\n",
      "Training Epoch: 11 [35550/36450]\tLoss: 571.6988\n",
      "Training Epoch: 11 [35600/36450]\tLoss: 605.1265\n",
      "Training Epoch: 11 [35650/36450]\tLoss: 524.8718\n",
      "Training Epoch: 11 [35700/36450]\tLoss: 542.8333\n",
      "Training Epoch: 11 [35750/36450]\tLoss: 559.5920\n",
      "Training Epoch: 11 [35800/36450]\tLoss: 524.5568\n",
      "Training Epoch: 11 [35850/36450]\tLoss: 536.4655\n",
      "Training Epoch: 11 [35900/36450]\tLoss: 535.3518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [35950/36450]\tLoss: 534.6341\n",
      "Training Epoch: 11 [36000/36450]\tLoss: 535.1839\n",
      "Training Epoch: 11 [36050/36450]\tLoss: 567.2296\n",
      "Training Epoch: 11 [36100/36450]\tLoss: 548.0731\n",
      "Training Epoch: 11 [36150/36450]\tLoss: 569.3275\n",
      "Training Epoch: 11 [36200/36450]\tLoss: 549.7743\n",
      "Training Epoch: 11 [36250/36450]\tLoss: 528.2149\n",
      "Training Epoch: 11 [36300/36450]\tLoss: 583.5512\n",
      "Training Epoch: 11 [36350/36450]\tLoss: 561.6310\n",
      "Training Epoch: 11 [36400/36450]\tLoss: 559.5468\n",
      "Training Epoch: 11 [36450/36450]\tLoss: 548.3883\n",
      "Training Epoch: 11 [4050/4050]\tLoss: 284.1712\n",
      "Training Epoch: 12 [50/36450]\tLoss: 573.7465\n",
      "Training Epoch: 12 [100/36450]\tLoss: 538.9789\n",
      "Training Epoch: 12 [150/36450]\tLoss: 573.4327\n",
      "Training Epoch: 12 [200/36450]\tLoss: 624.4623\n",
      "Training Epoch: 12 [250/36450]\tLoss: 562.6165\n",
      "Training Epoch: 12 [300/36450]\tLoss: 576.2040\n",
      "Training Epoch: 12 [350/36450]\tLoss: 566.4932\n",
      "Training Epoch: 12 [400/36450]\tLoss: 563.5930\n",
      "Training Epoch: 12 [450/36450]\tLoss: 598.9634\n",
      "Training Epoch: 12 [500/36450]\tLoss: 570.6196\n",
      "Training Epoch: 12 [550/36450]\tLoss: 563.2825\n",
      "Training Epoch: 12 [600/36450]\tLoss: 539.5202\n",
      "Training Epoch: 12 [650/36450]\tLoss: 551.2200\n",
      "Training Epoch: 12 [700/36450]\tLoss: 563.3068\n",
      "Training Epoch: 12 [750/36450]\tLoss: 549.9368\n",
      "Training Epoch: 12 [800/36450]\tLoss: 565.4355\n",
      "Training Epoch: 12 [850/36450]\tLoss: 526.9138\n",
      "Training Epoch: 12 [900/36450]\tLoss: 584.3700\n",
      "Training Epoch: 12 [950/36450]\tLoss: 543.5308\n",
      "Training Epoch: 12 [1000/36450]\tLoss: 538.8018\n",
      "Training Epoch: 12 [1050/36450]\tLoss: 575.0143\n",
      "Training Epoch: 12 [1100/36450]\tLoss: 579.3048\n",
      "Training Epoch: 12 [1150/36450]\tLoss: 546.2511\n",
      "Training Epoch: 12 [1200/36450]\tLoss: 592.3284\n",
      "Training Epoch: 12 [1250/36450]\tLoss: 559.1576\n",
      "Training Epoch: 12 [1300/36450]\tLoss: 548.1605\n",
      "Training Epoch: 12 [1350/36450]\tLoss: 540.9147\n",
      "Training Epoch: 12 [1400/36450]\tLoss: 567.2562\n",
      "Training Epoch: 12 [1450/36450]\tLoss: 573.7495\n",
      "Training Epoch: 12 [1500/36450]\tLoss: 595.7006\n",
      "Training Epoch: 12 [1550/36450]\tLoss: 553.5249\n",
      "Training Epoch: 12 [1600/36450]\tLoss: 565.6642\n",
      "Training Epoch: 12 [1650/36450]\tLoss: 572.3597\n",
      "Training Epoch: 12 [1700/36450]\tLoss: 550.3341\n",
      "Training Epoch: 12 [1750/36450]\tLoss: 563.6869\n",
      "Training Epoch: 12 [1800/36450]\tLoss: 515.7990\n",
      "Training Epoch: 12 [1850/36450]\tLoss: 603.1633\n",
      "Training Epoch: 12 [1900/36450]\tLoss: 564.6630\n",
      "Training Epoch: 12 [1950/36450]\tLoss: 572.2494\n",
      "Training Epoch: 12 [2000/36450]\tLoss: 581.2371\n",
      "Training Epoch: 12 [2050/36450]\tLoss: 563.8610\n",
      "Training Epoch: 12 [2100/36450]\tLoss: 579.3500\n",
      "Training Epoch: 12 [2150/36450]\tLoss: 544.0839\n",
      "Training Epoch: 12 [2200/36450]\tLoss: 544.1053\n",
      "Training Epoch: 12 [2250/36450]\tLoss: 580.8727\n",
      "Training Epoch: 12 [2300/36450]\tLoss: 557.7366\n",
      "Training Epoch: 12 [2350/36450]\tLoss: 586.8775\n",
      "Training Epoch: 12 [2400/36450]\tLoss: 604.2072\n",
      "Training Epoch: 12 [2450/36450]\tLoss: 558.6872\n",
      "Training Epoch: 12 [2500/36450]\tLoss: 561.8346\n",
      "Training Epoch: 12 [2550/36450]\tLoss: 576.6235\n",
      "Training Epoch: 12 [2600/36450]\tLoss: 569.5476\n",
      "Training Epoch: 12 [2650/36450]\tLoss: 578.3026\n",
      "Training Epoch: 12 [2700/36450]\tLoss: 547.1758\n",
      "Training Epoch: 12 [2750/36450]\tLoss: 538.6810\n",
      "Training Epoch: 12 [2800/36450]\tLoss: 557.6437\n",
      "Training Epoch: 12 [2850/36450]\tLoss: 544.8514\n",
      "Training Epoch: 12 [2900/36450]\tLoss: 546.6468\n",
      "Training Epoch: 12 [2950/36450]\tLoss: 584.3985\n",
      "Training Epoch: 12 [3000/36450]\tLoss: 541.7316\n",
      "Training Epoch: 12 [3050/36450]\tLoss: 554.7366\n",
      "Training Epoch: 12 [3100/36450]\tLoss: 533.0613\n",
      "Training Epoch: 12 [3150/36450]\tLoss: 569.1898\n",
      "Training Epoch: 12 [3200/36450]\tLoss: 530.4888\n",
      "Training Epoch: 12 [3250/36450]\tLoss: 563.2170\n",
      "Training Epoch: 12 [3300/36450]\tLoss: 569.6359\n",
      "Training Epoch: 12 [3350/36450]\tLoss: 534.1119\n",
      "Training Epoch: 12 [3400/36450]\tLoss: 555.9874\n",
      "Training Epoch: 12 [3450/36450]\tLoss: 559.9299\n",
      "Training Epoch: 12 [3500/36450]\tLoss: 578.4843\n",
      "Training Epoch: 12 [3550/36450]\tLoss: 562.1641\n",
      "Training Epoch: 12 [3600/36450]\tLoss: 571.3455\n",
      "Training Epoch: 12 [3650/36450]\tLoss: 575.9564\n",
      "Training Epoch: 12 [3700/36450]\tLoss: 530.1512\n",
      "Training Epoch: 12 [3750/36450]\tLoss: 551.2927\n",
      "Training Epoch: 12 [3800/36450]\tLoss: 529.5963\n",
      "Training Epoch: 12 [3850/36450]\tLoss: 558.5541\n",
      "Training Epoch: 12 [3900/36450]\tLoss: 537.9883\n",
      "Training Epoch: 12 [3950/36450]\tLoss: 596.3113\n",
      "Training Epoch: 12 [4000/36450]\tLoss: 537.5544\n",
      "Training Epoch: 12 [4050/36450]\tLoss: 555.3931\n",
      "Training Epoch: 12 [4100/36450]\tLoss: 555.7294\n",
      "Training Epoch: 12 [4150/36450]\tLoss: 550.5370\n",
      "Training Epoch: 12 [4200/36450]\tLoss: 518.1654\n",
      "Training Epoch: 12 [4250/36450]\tLoss: 566.3409\n",
      "Training Epoch: 12 [4300/36450]\tLoss: 539.6454\n",
      "Training Epoch: 12 [4350/36450]\tLoss: 539.2313\n",
      "Training Epoch: 12 [4400/36450]\tLoss: 584.0309\n",
      "Training Epoch: 12 [4450/36450]\tLoss: 550.8923\n",
      "Training Epoch: 12 [4500/36450]\tLoss: 556.2338\n",
      "Training Epoch: 12 [4550/36450]\tLoss: 549.4348\n",
      "Training Epoch: 12 [4600/36450]\tLoss: 571.5701\n",
      "Training Epoch: 12 [4650/36450]\tLoss: 553.5043\n",
      "Training Epoch: 12 [4700/36450]\tLoss: 535.7908\n",
      "Training Epoch: 12 [4750/36450]\tLoss: 557.3319\n",
      "Training Epoch: 12 [4800/36450]\tLoss: 553.8646\n",
      "Training Epoch: 12 [4850/36450]\tLoss: 566.6921\n",
      "Training Epoch: 12 [4900/36450]\tLoss: 564.3611\n",
      "Training Epoch: 12 [4950/36450]\tLoss: 506.5176\n",
      "Training Epoch: 12 [5000/36450]\tLoss: 582.6594\n",
      "Training Epoch: 12 [5050/36450]\tLoss: 550.0008\n",
      "Training Epoch: 12 [5100/36450]\tLoss: 572.1744\n",
      "Training Epoch: 12 [5150/36450]\tLoss: 582.3953\n",
      "Training Epoch: 12 [5200/36450]\tLoss: 550.0861\n",
      "Training Epoch: 12 [5250/36450]\tLoss: 540.6113\n",
      "Training Epoch: 12 [5300/36450]\tLoss: 562.4636\n",
      "Training Epoch: 12 [5350/36450]\tLoss: 578.2978\n",
      "Training Epoch: 12 [5400/36450]\tLoss: 547.0713\n",
      "Training Epoch: 12 [5450/36450]\tLoss: 544.0280\n",
      "Training Epoch: 12 [5500/36450]\tLoss: 564.6445\n",
      "Training Epoch: 12 [5550/36450]\tLoss: 555.3679\n",
      "Training Epoch: 12 [5600/36450]\tLoss: 539.7842\n",
      "Training Epoch: 12 [5650/36450]\tLoss: 541.7563\n",
      "Training Epoch: 12 [5700/36450]\tLoss: 551.4721\n",
      "Training Epoch: 12 [5750/36450]\tLoss: 544.3527\n",
      "Training Epoch: 12 [5800/36450]\tLoss: 577.6626\n",
      "Training Epoch: 12 [5850/36450]\tLoss: 593.2383\n",
      "Training Epoch: 12 [5900/36450]\tLoss: 571.4999\n",
      "Training Epoch: 12 [5950/36450]\tLoss: 543.3713\n",
      "Training Epoch: 12 [6000/36450]\tLoss: 533.1444\n",
      "Training Epoch: 12 [6050/36450]\tLoss: 535.3420\n",
      "Training Epoch: 12 [6100/36450]\tLoss: 563.7001\n",
      "Training Epoch: 12 [6150/36450]\tLoss: 536.5098\n",
      "Training Epoch: 12 [6200/36450]\tLoss: 551.2983\n",
      "Training Epoch: 12 [6250/36450]\tLoss: 550.6410\n",
      "Training Epoch: 12 [6300/36450]\tLoss: 552.5096\n",
      "Training Epoch: 12 [6350/36450]\tLoss: 536.5420\n",
      "Training Epoch: 12 [6400/36450]\tLoss: 579.6688\n",
      "Training Epoch: 12 [6450/36450]\tLoss: 527.8055\n",
      "Training Epoch: 12 [6500/36450]\tLoss: 551.4043\n",
      "Training Epoch: 12 [6550/36450]\tLoss: 514.9163\n",
      "Training Epoch: 12 [6600/36450]\tLoss: 553.8989\n",
      "Training Epoch: 12 [6650/36450]\tLoss: 565.7753\n",
      "Training Epoch: 12 [6700/36450]\tLoss: 531.4481\n",
      "Training Epoch: 12 [6750/36450]\tLoss: 571.0727\n",
      "Training Epoch: 12 [6800/36450]\tLoss: 520.4868\n",
      "Training Epoch: 12 [6850/36450]\tLoss: 543.5649\n",
      "Training Epoch: 12 [6900/36450]\tLoss: 554.2953\n",
      "Training Epoch: 12 [6950/36450]\tLoss: 507.3553\n",
      "Training Epoch: 12 [7000/36450]\tLoss: 554.0699\n",
      "Training Epoch: 12 [7050/36450]\tLoss: 575.3214\n",
      "Training Epoch: 12 [7100/36450]\tLoss: 555.3068\n",
      "Training Epoch: 12 [7150/36450]\tLoss: 527.8286\n",
      "Training Epoch: 12 [7200/36450]\tLoss: 548.1389\n",
      "Training Epoch: 12 [7250/36450]\tLoss: 587.0117\n",
      "Training Epoch: 12 [7300/36450]\tLoss: 577.4805\n",
      "Training Epoch: 12 [7350/36450]\tLoss: 551.3456\n",
      "Training Epoch: 12 [7400/36450]\tLoss: 568.9701\n",
      "Training Epoch: 12 [7450/36450]\tLoss: 554.1465\n",
      "Training Epoch: 12 [7500/36450]\tLoss: 521.5892\n",
      "Training Epoch: 12 [7550/36450]\tLoss: 553.6803\n",
      "Training Epoch: 12 [7600/36450]\tLoss: 580.8292\n",
      "Training Epoch: 12 [7650/36450]\tLoss: 553.2511\n",
      "Training Epoch: 12 [7700/36450]\tLoss: 586.6483\n",
      "Training Epoch: 12 [7750/36450]\tLoss: 547.5479\n",
      "Training Epoch: 12 [7800/36450]\tLoss: 544.5762\n",
      "Training Epoch: 12 [7850/36450]\tLoss: 565.3361\n",
      "Training Epoch: 12 [7900/36450]\tLoss: 589.8092\n",
      "Training Epoch: 12 [7950/36450]\tLoss: 559.0954\n",
      "Training Epoch: 12 [8000/36450]\tLoss: 560.0379\n",
      "Training Epoch: 12 [8050/36450]\tLoss: 568.9662\n",
      "Training Epoch: 12 [8100/36450]\tLoss: 561.0505\n",
      "Training Epoch: 12 [8150/36450]\tLoss: 570.4722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [8200/36450]\tLoss: 586.3419\n",
      "Training Epoch: 12 [8250/36450]\tLoss: 604.8162\n",
      "Training Epoch: 12 [8300/36450]\tLoss: 568.5670\n",
      "Training Epoch: 12 [8350/36450]\tLoss: 577.8073\n",
      "Training Epoch: 12 [8400/36450]\tLoss: 599.9024\n",
      "Training Epoch: 12 [8450/36450]\tLoss: 584.1649\n",
      "Training Epoch: 12 [8500/36450]\tLoss: 560.0085\n",
      "Training Epoch: 12 [8550/36450]\tLoss: 574.6479\n",
      "Training Epoch: 12 [8600/36450]\tLoss: 603.4785\n",
      "Training Epoch: 12 [8650/36450]\tLoss: 551.4304\n",
      "Training Epoch: 12 [8700/36450]\tLoss: 591.4266\n",
      "Training Epoch: 12 [8750/36450]\tLoss: 570.9769\n",
      "Training Epoch: 12 [8800/36450]\tLoss: 550.7765\n",
      "Training Epoch: 12 [8850/36450]\tLoss: 568.6838\n",
      "Training Epoch: 12 [8900/36450]\tLoss: 541.7507\n",
      "Training Epoch: 12 [8950/36450]\tLoss: 577.8961\n",
      "Training Epoch: 12 [9000/36450]\tLoss: 549.7168\n",
      "Training Epoch: 12 [9050/36450]\tLoss: 514.6253\n",
      "Training Epoch: 12 [9100/36450]\tLoss: 540.0948\n",
      "Training Epoch: 12 [9150/36450]\tLoss: 541.9991\n",
      "Training Epoch: 12 [9200/36450]\tLoss: 505.7650\n",
      "Training Epoch: 12 [9250/36450]\tLoss: 583.0572\n",
      "Training Epoch: 12 [9300/36450]\tLoss: 532.5580\n",
      "Training Epoch: 12 [9350/36450]\tLoss: 493.4831\n",
      "Training Epoch: 12 [9400/36450]\tLoss: 525.5414\n",
      "Training Epoch: 12 [9450/36450]\tLoss: 574.0962\n",
      "Training Epoch: 12 [9500/36450]\tLoss: 520.5424\n",
      "Training Epoch: 12 [9550/36450]\tLoss: 544.4262\n",
      "Training Epoch: 12 [9600/36450]\tLoss: 550.2947\n",
      "Training Epoch: 12 [9650/36450]\tLoss: 535.9449\n",
      "Training Epoch: 12 [9700/36450]\tLoss: 544.3460\n",
      "Training Epoch: 12 [9750/36450]\tLoss: 565.9904\n",
      "Training Epoch: 12 [9800/36450]\tLoss: 556.3776\n",
      "Training Epoch: 12 [9850/36450]\tLoss: 548.0020\n",
      "Training Epoch: 12 [9900/36450]\tLoss: 535.4808\n",
      "Training Epoch: 12 [9950/36450]\tLoss: 541.8378\n",
      "Training Epoch: 12 [10000/36450]\tLoss: 533.6791\n",
      "Training Epoch: 12 [10050/36450]\tLoss: 556.8325\n",
      "Training Epoch: 12 [10100/36450]\tLoss: 522.0848\n",
      "Training Epoch: 12 [10150/36450]\tLoss: 528.1423\n",
      "Training Epoch: 12 [10200/36450]\tLoss: 544.9983\n",
      "Training Epoch: 12 [10250/36450]\tLoss: 565.2272\n",
      "Training Epoch: 12 [10300/36450]\tLoss: 549.2163\n",
      "Training Epoch: 12 [10350/36450]\tLoss: 524.6592\n",
      "Training Epoch: 12 [10400/36450]\tLoss: 541.7205\n",
      "Training Epoch: 12 [10450/36450]\tLoss: 604.8121\n",
      "Training Epoch: 12 [10500/36450]\tLoss: 540.7965\n",
      "Training Epoch: 12 [10550/36450]\tLoss: 543.3145\n",
      "Training Epoch: 12 [10600/36450]\tLoss: 537.0133\n",
      "Training Epoch: 12 [10650/36450]\tLoss: 541.4282\n",
      "Training Epoch: 12 [10700/36450]\tLoss: 539.0740\n",
      "Training Epoch: 12 [10750/36450]\tLoss: 499.9880\n",
      "Training Epoch: 12 [10800/36450]\tLoss: 563.5750\n",
      "Training Epoch: 12 [10850/36450]\tLoss: 555.9380\n",
      "Training Epoch: 12 [10900/36450]\tLoss: 532.1293\n",
      "Training Epoch: 12 [10950/36450]\tLoss: 574.2103\n",
      "Training Epoch: 12 [11000/36450]\tLoss: 538.1769\n",
      "Training Epoch: 12 [11050/36450]\tLoss: 581.1653\n",
      "Training Epoch: 12 [11100/36450]\tLoss: 558.8254\n",
      "Training Epoch: 12 [11150/36450]\tLoss: 517.3409\n",
      "Training Epoch: 12 [11200/36450]\tLoss: 560.8879\n",
      "Training Epoch: 12 [11250/36450]\tLoss: 534.0737\n",
      "Training Epoch: 12 [11300/36450]\tLoss: 565.7007\n",
      "Training Epoch: 12 [11350/36450]\tLoss: 575.8597\n",
      "Training Epoch: 12 [11400/36450]\tLoss: 569.6162\n",
      "Training Epoch: 12 [11450/36450]\tLoss: 611.4850\n",
      "Training Epoch: 12 [11500/36450]\tLoss: 566.6999\n",
      "Training Epoch: 12 [11550/36450]\tLoss: 558.0447\n",
      "Training Epoch: 12 [11600/36450]\tLoss: 565.7535\n",
      "Training Epoch: 12 [11650/36450]\tLoss: 591.0620\n",
      "Training Epoch: 12 [11700/36450]\tLoss: 551.3323\n",
      "Training Epoch: 12 [11750/36450]\tLoss: 556.9126\n",
      "Training Epoch: 12 [11800/36450]\tLoss: 537.6359\n",
      "Training Epoch: 12 [11850/36450]\tLoss: 573.6334\n",
      "Training Epoch: 12 [11900/36450]\tLoss: 569.1042\n",
      "Training Epoch: 12 [11950/36450]\tLoss: 553.8187\n",
      "Training Epoch: 12 [12000/36450]\tLoss: 571.6395\n",
      "Training Epoch: 12 [12050/36450]\tLoss: 588.6161\n",
      "Training Epoch: 12 [12100/36450]\tLoss: 556.4409\n",
      "Training Epoch: 12 [12150/36450]\tLoss: 535.9736\n",
      "Training Epoch: 12 [12200/36450]\tLoss: 548.6233\n",
      "Training Epoch: 12 [12250/36450]\tLoss: 558.5501\n",
      "Training Epoch: 12 [12300/36450]\tLoss: 533.3643\n",
      "Training Epoch: 12 [12350/36450]\tLoss: 588.5775\n",
      "Training Epoch: 12 [12400/36450]\tLoss: 496.7160\n",
      "Training Epoch: 12 [12450/36450]\tLoss: 545.6468\n",
      "Training Epoch: 12 [12500/36450]\tLoss: 540.5517\n",
      "Training Epoch: 12 [12550/36450]\tLoss: 561.7253\n",
      "Training Epoch: 12 [12600/36450]\tLoss: 542.8005\n",
      "Training Epoch: 12 [12650/36450]\tLoss: 540.0789\n",
      "Training Epoch: 12 [12700/36450]\tLoss: 558.4370\n",
      "Training Epoch: 12 [12750/36450]\tLoss: 531.6908\n",
      "Training Epoch: 12 [12800/36450]\tLoss: 557.6931\n",
      "Training Epoch: 12 [12850/36450]\tLoss: 587.0218\n",
      "Training Epoch: 12 [12900/36450]\tLoss: 554.9667\n",
      "Training Epoch: 12 [12950/36450]\tLoss: 539.2573\n",
      "Training Epoch: 12 [13000/36450]\tLoss: 579.1420\n",
      "Training Epoch: 12 [13050/36450]\tLoss: 525.1637\n",
      "Training Epoch: 12 [13100/36450]\tLoss: 537.6476\n",
      "Training Epoch: 12 [13150/36450]\tLoss: 569.8047\n",
      "Training Epoch: 12 [13200/36450]\tLoss: 597.8530\n",
      "Training Epoch: 12 [13250/36450]\tLoss: 559.1877\n",
      "Training Epoch: 12 [13300/36450]\tLoss: 566.3444\n",
      "Training Epoch: 12 [13350/36450]\tLoss: 511.4836\n",
      "Training Epoch: 12 [13400/36450]\tLoss: 554.6940\n",
      "Training Epoch: 12 [13450/36450]\tLoss: 550.2433\n",
      "Training Epoch: 12 [13500/36450]\tLoss: 523.5211\n",
      "Training Epoch: 12 [13550/36450]\tLoss: 553.6076\n",
      "Training Epoch: 12 [13600/36450]\tLoss: 569.0446\n",
      "Training Epoch: 12 [13650/36450]\tLoss: 570.6356\n",
      "Training Epoch: 12 [13700/36450]\tLoss: 550.5450\n",
      "Training Epoch: 12 [13750/36450]\tLoss: 562.9028\n",
      "Training Epoch: 12 [13800/36450]\tLoss: 546.3573\n",
      "Training Epoch: 12 [13850/36450]\tLoss: 559.7886\n",
      "Training Epoch: 12 [13900/36450]\tLoss: 581.6420\n",
      "Training Epoch: 12 [13950/36450]\tLoss: 544.9567\n",
      "Training Epoch: 12 [14000/36450]\tLoss: 521.8660\n",
      "Training Epoch: 12 [14050/36450]\tLoss: 569.6660\n",
      "Training Epoch: 12 [14100/36450]\tLoss: 573.5746\n",
      "Training Epoch: 12 [14150/36450]\tLoss: 547.6052\n",
      "Training Epoch: 12 [14200/36450]\tLoss: 542.2729\n",
      "Training Epoch: 12 [14250/36450]\tLoss: 584.3489\n",
      "Training Epoch: 12 [14300/36450]\tLoss: 604.8140\n",
      "Training Epoch: 12 [14350/36450]\tLoss: 562.7215\n",
      "Training Epoch: 12 [14400/36450]\tLoss: 561.7911\n",
      "Training Epoch: 12 [14450/36450]\tLoss: 545.0638\n",
      "Training Epoch: 12 [14500/36450]\tLoss: 524.2613\n",
      "Training Epoch: 12 [14550/36450]\tLoss: 528.9004\n",
      "Training Epoch: 12 [14600/36450]\tLoss: 522.7789\n",
      "Training Epoch: 12 [14650/36450]\tLoss: 551.7217\n",
      "Training Epoch: 12 [14700/36450]\tLoss: 530.7875\n",
      "Training Epoch: 12 [14750/36450]\tLoss: 568.9191\n",
      "Training Epoch: 12 [14800/36450]\tLoss: 549.6624\n",
      "Training Epoch: 12 [14850/36450]\tLoss: 536.9051\n",
      "Training Epoch: 12 [14900/36450]\tLoss: 545.7568\n",
      "Training Epoch: 12 [14950/36450]\tLoss: 547.9841\n",
      "Training Epoch: 12 [15000/36450]\tLoss: 558.9310\n",
      "Training Epoch: 12 [15050/36450]\tLoss: 530.9553\n",
      "Training Epoch: 12 [15100/36450]\tLoss: 552.5533\n",
      "Training Epoch: 12 [15150/36450]\tLoss: 512.1167\n",
      "Training Epoch: 12 [15200/36450]\tLoss: 495.5407\n",
      "Training Epoch: 12 [15250/36450]\tLoss: 538.2880\n",
      "Training Epoch: 12 [15300/36450]\tLoss: 526.5053\n",
      "Training Epoch: 12 [15350/36450]\tLoss: 546.4219\n",
      "Training Epoch: 12 [15400/36450]\tLoss: 543.3472\n",
      "Training Epoch: 12 [15450/36450]\tLoss: 515.0412\n",
      "Training Epoch: 12 [15500/36450]\tLoss: 566.8113\n",
      "Training Epoch: 12 [15550/36450]\tLoss: 553.1055\n",
      "Training Epoch: 12 [15600/36450]\tLoss: 564.2726\n",
      "Training Epoch: 12 [15650/36450]\tLoss: 507.7273\n",
      "Training Epoch: 12 [15700/36450]\tLoss: 536.1913\n",
      "Training Epoch: 12 [15750/36450]\tLoss: 556.9739\n",
      "Training Epoch: 12 [15800/36450]\tLoss: 542.7465\n",
      "Training Epoch: 12 [15850/36450]\tLoss: 600.6277\n",
      "Training Epoch: 12 [15900/36450]\tLoss: 570.0877\n",
      "Training Epoch: 12 [15950/36450]\tLoss: 523.7616\n",
      "Training Epoch: 12 [16000/36450]\tLoss: 550.7362\n",
      "Training Epoch: 12 [16050/36450]\tLoss: 548.0161\n",
      "Training Epoch: 12 [16100/36450]\tLoss: 551.7726\n",
      "Training Epoch: 12 [16150/36450]\tLoss: 545.4112\n",
      "Training Epoch: 12 [16200/36450]\tLoss: 522.1058\n",
      "Training Epoch: 12 [16250/36450]\tLoss: 529.7836\n",
      "Training Epoch: 12 [16300/36450]\tLoss: 565.9686\n",
      "Training Epoch: 12 [16350/36450]\tLoss: 582.1235\n",
      "Training Epoch: 12 [16400/36450]\tLoss: 559.7218\n",
      "Training Epoch: 12 [16450/36450]\tLoss: 550.3672\n",
      "Training Epoch: 12 [16500/36450]\tLoss: 567.7196\n",
      "Training Epoch: 12 [16550/36450]\tLoss: 569.8001\n",
      "Training Epoch: 12 [16600/36450]\tLoss: 581.1667\n",
      "Training Epoch: 12 [16650/36450]\tLoss: 561.7890\n",
      "Training Epoch: 12 [16700/36450]\tLoss: 569.5844\n",
      "Training Epoch: 12 [16750/36450]\tLoss: 565.2571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [16800/36450]\tLoss: 549.5708\n",
      "Training Epoch: 12 [16850/36450]\tLoss: 584.6746\n",
      "Training Epoch: 12 [16900/36450]\tLoss: 551.3769\n",
      "Training Epoch: 12 [16950/36450]\tLoss: 563.4191\n",
      "Training Epoch: 12 [17000/36450]\tLoss: 539.9541\n",
      "Training Epoch: 12 [17050/36450]\tLoss: 577.4536\n",
      "Training Epoch: 12 [17100/36450]\tLoss: 518.0466\n",
      "Training Epoch: 12 [17150/36450]\tLoss: 553.0953\n",
      "Training Epoch: 12 [17200/36450]\tLoss: 543.2966\n",
      "Training Epoch: 12 [17250/36450]\tLoss: 549.0359\n",
      "Training Epoch: 12 [17300/36450]\tLoss: 528.8141\n",
      "Training Epoch: 12 [17350/36450]\tLoss: 572.0081\n",
      "Training Epoch: 12 [17400/36450]\tLoss: 565.2213\n",
      "Training Epoch: 12 [17450/36450]\tLoss: 536.3640\n",
      "Training Epoch: 12 [17500/36450]\tLoss: 556.7264\n",
      "Training Epoch: 12 [17550/36450]\tLoss: 534.0519\n",
      "Training Epoch: 12 [17600/36450]\tLoss: 547.7195\n",
      "Training Epoch: 12 [17650/36450]\tLoss: 557.7001\n",
      "Training Epoch: 12 [17700/36450]\tLoss: 538.2609\n",
      "Training Epoch: 12 [17750/36450]\tLoss: 538.6544\n",
      "Training Epoch: 12 [17800/36450]\tLoss: 531.9426\n",
      "Training Epoch: 12 [17850/36450]\tLoss: 544.9149\n",
      "Training Epoch: 12 [17900/36450]\tLoss: 548.8563\n",
      "Training Epoch: 12 [17950/36450]\tLoss: 581.5035\n",
      "Training Epoch: 12 [18000/36450]\tLoss: 542.8778\n",
      "Training Epoch: 12 [18050/36450]\tLoss: 528.2774\n",
      "Training Epoch: 12 [18100/36450]\tLoss: 546.1509\n",
      "Training Epoch: 12 [18150/36450]\tLoss: 549.2545\n",
      "Training Epoch: 12 [18200/36450]\tLoss: 580.0059\n",
      "Training Epoch: 12 [18250/36450]\tLoss: 542.0944\n",
      "Training Epoch: 12 [18300/36450]\tLoss: 540.0726\n",
      "Training Epoch: 12 [18350/36450]\tLoss: 547.5590\n",
      "Training Epoch: 12 [18400/36450]\tLoss: 547.7488\n",
      "Training Epoch: 12 [18450/36450]\tLoss: 536.7888\n",
      "Training Epoch: 12 [18500/36450]\tLoss: 551.6387\n",
      "Training Epoch: 12 [18550/36450]\tLoss: 549.2805\n",
      "Training Epoch: 12 [18600/36450]\tLoss: 584.4841\n",
      "Training Epoch: 12 [18650/36450]\tLoss: 538.8484\n",
      "Training Epoch: 12 [18700/36450]\tLoss: 514.5180\n",
      "Training Epoch: 12 [18750/36450]\tLoss: 570.4157\n",
      "Training Epoch: 12 [18800/36450]\tLoss: 583.8693\n",
      "Training Epoch: 12 [18850/36450]\tLoss: 610.1758\n",
      "Training Epoch: 12 [18900/36450]\tLoss: 563.0500\n",
      "Training Epoch: 12 [18950/36450]\tLoss: 556.7278\n",
      "Training Epoch: 12 [19000/36450]\tLoss: 561.4644\n",
      "Training Epoch: 12 [19050/36450]\tLoss: 545.4077\n",
      "Training Epoch: 12 [19100/36450]\tLoss: 600.2778\n",
      "Training Epoch: 12 [19150/36450]\tLoss: 526.9598\n",
      "Training Epoch: 12 [19200/36450]\tLoss: 563.4477\n",
      "Training Epoch: 12 [19250/36450]\tLoss: 557.0149\n",
      "Training Epoch: 12 [19300/36450]\tLoss: 567.4330\n",
      "Training Epoch: 12 [19350/36450]\tLoss: 574.5675\n",
      "Training Epoch: 12 [19400/36450]\tLoss: 510.6766\n",
      "Training Epoch: 12 [19450/36450]\tLoss: 565.6100\n",
      "Training Epoch: 12 [19500/36450]\tLoss: 518.2032\n",
      "Training Epoch: 12 [19550/36450]\tLoss: 541.2454\n",
      "Training Epoch: 12 [19600/36450]\tLoss: 554.4651\n",
      "Training Epoch: 12 [19650/36450]\tLoss: 564.5469\n",
      "Training Epoch: 12 [19700/36450]\tLoss: 519.6226\n",
      "Training Epoch: 12 [19750/36450]\tLoss: 563.1197\n",
      "Training Epoch: 12 [19800/36450]\tLoss: 571.7414\n",
      "Training Epoch: 12 [19850/36450]\tLoss: 519.1139\n",
      "Training Epoch: 12 [19900/36450]\tLoss: 516.4163\n",
      "Training Epoch: 12 [19950/36450]\tLoss: 580.7856\n",
      "Training Epoch: 12 [20000/36450]\tLoss: 533.7537\n",
      "Training Epoch: 12 [20050/36450]\tLoss: 575.2494\n",
      "Training Epoch: 12 [20100/36450]\tLoss: 563.7092\n",
      "Training Epoch: 12 [20150/36450]\tLoss: 531.8558\n",
      "Training Epoch: 12 [20200/36450]\tLoss: 550.4252\n",
      "Training Epoch: 12 [20250/36450]\tLoss: 604.9871\n",
      "Training Epoch: 12 [20300/36450]\tLoss: 582.7546\n",
      "Training Epoch: 12 [20350/36450]\tLoss: 545.1022\n",
      "Training Epoch: 12 [20400/36450]\tLoss: 537.5600\n",
      "Training Epoch: 12 [20450/36450]\tLoss: 547.8344\n",
      "Training Epoch: 12 [20500/36450]\tLoss: 545.9322\n",
      "Training Epoch: 12 [20550/36450]\tLoss: 511.3307\n",
      "Training Epoch: 12 [20600/36450]\tLoss: 561.4621\n",
      "Training Epoch: 12 [20650/36450]\tLoss: 519.8973\n",
      "Training Epoch: 12 [20700/36450]\tLoss: 546.9002\n",
      "Training Epoch: 12 [20750/36450]\tLoss: 542.4654\n",
      "Training Epoch: 12 [20800/36450]\tLoss: 554.6678\n",
      "Training Epoch: 12 [20850/36450]\tLoss: 558.1085\n",
      "Training Epoch: 12 [20900/36450]\tLoss: 545.1929\n",
      "Training Epoch: 12 [20950/36450]\tLoss: 539.8295\n",
      "Training Epoch: 12 [21000/36450]\tLoss: 574.7490\n",
      "Training Epoch: 12 [21050/36450]\tLoss: 544.3597\n",
      "Training Epoch: 12 [21100/36450]\tLoss: 560.3098\n",
      "Training Epoch: 12 [21150/36450]\tLoss: 585.5579\n",
      "Training Epoch: 12 [21200/36450]\tLoss: 554.5459\n",
      "Training Epoch: 12 [21250/36450]\tLoss: 572.7172\n",
      "Training Epoch: 12 [21300/36450]\tLoss: 571.8624\n",
      "Training Epoch: 12 [21350/36450]\tLoss: 607.2242\n",
      "Training Epoch: 12 [21400/36450]\tLoss: 581.9635\n",
      "Training Epoch: 12 [21450/36450]\tLoss: 565.9236\n",
      "Training Epoch: 12 [21500/36450]\tLoss: 586.2245\n",
      "Training Epoch: 12 [21550/36450]\tLoss: 608.3574\n",
      "Training Epoch: 12 [21600/36450]\tLoss: 632.2844\n",
      "Training Epoch: 12 [21650/36450]\tLoss: 639.4130\n",
      "Training Epoch: 12 [21700/36450]\tLoss: 653.5980\n",
      "Training Epoch: 12 [21750/36450]\tLoss: 603.1345\n",
      "Training Epoch: 12 [21800/36450]\tLoss: 595.3063\n",
      "Training Epoch: 12 [21850/36450]\tLoss: 549.1642\n",
      "Training Epoch: 12 [21900/36450]\tLoss: 543.2788\n",
      "Training Epoch: 12 [21950/36450]\tLoss: 580.8226\n",
      "Training Epoch: 12 [22000/36450]\tLoss: 573.6589\n",
      "Training Epoch: 12 [22050/36450]\tLoss: 556.5540\n",
      "Training Epoch: 12 [22100/36450]\tLoss: 571.7627\n",
      "Training Epoch: 12 [22150/36450]\tLoss: 515.7766\n",
      "Training Epoch: 12 [22200/36450]\tLoss: 538.4117\n",
      "Training Epoch: 12 [22250/36450]\tLoss: 559.2749\n",
      "Training Epoch: 12 [22300/36450]\tLoss: 570.3447\n",
      "Training Epoch: 12 [22350/36450]\tLoss: 546.0399\n",
      "Training Epoch: 12 [22400/36450]\tLoss: 501.6519\n",
      "Training Epoch: 12 [22450/36450]\tLoss: 535.8773\n",
      "Training Epoch: 12 [22500/36450]\tLoss: 534.4626\n",
      "Training Epoch: 12 [22550/36450]\tLoss: 579.2573\n",
      "Training Epoch: 12 [22600/36450]\tLoss: 610.7841\n",
      "Training Epoch: 12 [22650/36450]\tLoss: 566.5336\n",
      "Training Epoch: 12 [22700/36450]\tLoss: 606.7769\n",
      "Training Epoch: 12 [22750/36450]\tLoss: 537.9326\n",
      "Training Epoch: 12 [22800/36450]\tLoss: 553.9092\n",
      "Training Epoch: 12 [22850/36450]\tLoss: 559.0720\n",
      "Training Epoch: 12 [22900/36450]\tLoss: 564.5398\n",
      "Training Epoch: 12 [22950/36450]\tLoss: 577.8281\n",
      "Training Epoch: 12 [23000/36450]\tLoss: 508.3812\n",
      "Training Epoch: 12 [23050/36450]\tLoss: 527.2940\n",
      "Training Epoch: 12 [23100/36450]\tLoss: 548.0331\n",
      "Training Epoch: 12 [23150/36450]\tLoss: 582.4706\n",
      "Training Epoch: 12 [23200/36450]\tLoss: 571.7125\n",
      "Training Epoch: 12 [23250/36450]\tLoss: 563.8889\n",
      "Training Epoch: 12 [23300/36450]\tLoss: 553.4640\n",
      "Training Epoch: 12 [23350/36450]\tLoss: 562.5190\n",
      "Training Epoch: 12 [23400/36450]\tLoss: 543.4156\n",
      "Training Epoch: 12 [23450/36450]\tLoss: 544.8481\n",
      "Training Epoch: 12 [23500/36450]\tLoss: 532.8313\n",
      "Training Epoch: 12 [23550/36450]\tLoss: 537.2126\n",
      "Training Epoch: 12 [23600/36450]\tLoss: 529.3030\n",
      "Training Epoch: 12 [23650/36450]\tLoss: 542.1257\n",
      "Training Epoch: 12 [23700/36450]\tLoss: 530.9203\n",
      "Training Epoch: 12 [23750/36450]\tLoss: 564.6730\n",
      "Training Epoch: 12 [23800/36450]\tLoss: 522.4908\n",
      "Training Epoch: 12 [23850/36450]\tLoss: 552.2594\n",
      "Training Epoch: 12 [23900/36450]\tLoss: 540.3276\n",
      "Training Epoch: 12 [23950/36450]\tLoss: 516.3027\n",
      "Training Epoch: 12 [24000/36450]\tLoss: 550.9280\n",
      "Training Epoch: 12 [24050/36450]\tLoss: 550.0432\n",
      "Training Epoch: 12 [24100/36450]\tLoss: 535.7090\n",
      "Training Epoch: 12 [24150/36450]\tLoss: 516.6733\n",
      "Training Epoch: 12 [24200/36450]\tLoss: 528.3223\n",
      "Training Epoch: 12 [24250/36450]\tLoss: 539.4478\n",
      "Training Epoch: 12 [24300/36450]\tLoss: 511.1267\n",
      "Training Epoch: 12 [24350/36450]\tLoss: 530.3572\n",
      "Training Epoch: 12 [24400/36450]\tLoss: 550.2982\n",
      "Training Epoch: 12 [24450/36450]\tLoss: 574.6472\n",
      "Training Epoch: 12 [24500/36450]\tLoss: 518.1532\n",
      "Training Epoch: 12 [24550/36450]\tLoss: 542.7004\n",
      "Training Epoch: 12 [24600/36450]\tLoss: 554.0536\n",
      "Training Epoch: 12 [24650/36450]\tLoss: 554.0919\n",
      "Training Epoch: 12 [24700/36450]\tLoss: 530.9236\n",
      "Training Epoch: 12 [24750/36450]\tLoss: 523.7789\n",
      "Training Epoch: 12 [24800/36450]\tLoss: 560.4890\n",
      "Training Epoch: 12 [24850/36450]\tLoss: 558.7775\n",
      "Training Epoch: 12 [24900/36450]\tLoss: 512.8784\n",
      "Training Epoch: 12 [24950/36450]\tLoss: 552.9085\n",
      "Training Epoch: 12 [25000/36450]\tLoss: 546.6796\n",
      "Training Epoch: 12 [25050/36450]\tLoss: 550.0956\n",
      "Training Epoch: 12 [25100/36450]\tLoss: 557.3516\n",
      "Training Epoch: 12 [25150/36450]\tLoss: 519.4147\n",
      "Training Epoch: 12 [25200/36450]\tLoss: 544.0365\n",
      "Training Epoch: 12 [25250/36450]\tLoss: 557.4659\n",
      "Training Epoch: 12 [25300/36450]\tLoss: 515.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [25350/36450]\tLoss: 526.5941\n",
      "Training Epoch: 12 [25400/36450]\tLoss: 541.6749\n",
      "Training Epoch: 12 [25450/36450]\tLoss: 523.9579\n",
      "Training Epoch: 12 [25500/36450]\tLoss: 544.8921\n",
      "Training Epoch: 12 [25550/36450]\tLoss: 538.8153\n",
      "Training Epoch: 12 [25600/36450]\tLoss: 556.4689\n",
      "Training Epoch: 12 [25650/36450]\tLoss: 565.7776\n",
      "Training Epoch: 12 [25700/36450]\tLoss: 525.2098\n",
      "Training Epoch: 12 [25750/36450]\tLoss: 569.7488\n",
      "Training Epoch: 12 [25800/36450]\tLoss: 578.6637\n",
      "Training Epoch: 12 [25850/36450]\tLoss: 531.1176\n",
      "Training Epoch: 12 [25900/36450]\tLoss: 540.0798\n",
      "Training Epoch: 12 [25950/36450]\tLoss: 544.5610\n",
      "Training Epoch: 12 [26000/36450]\tLoss: 534.6608\n",
      "Training Epoch: 12 [26050/36450]\tLoss: 548.3168\n",
      "Training Epoch: 12 [26100/36450]\tLoss: 565.0306\n",
      "Training Epoch: 12 [26150/36450]\tLoss: 521.3290\n",
      "Training Epoch: 12 [26200/36450]\tLoss: 547.6451\n",
      "Training Epoch: 12 [26250/36450]\tLoss: 515.3757\n",
      "Training Epoch: 12 [26300/36450]\tLoss: 552.3784\n",
      "Training Epoch: 12 [26350/36450]\tLoss: 572.5015\n",
      "Training Epoch: 12 [26400/36450]\tLoss: 538.9628\n",
      "Training Epoch: 12 [26450/36450]\tLoss: 539.8411\n",
      "Training Epoch: 12 [26500/36450]\tLoss: 556.8885\n",
      "Training Epoch: 12 [26550/36450]\tLoss: 556.5965\n",
      "Training Epoch: 12 [26600/36450]\tLoss: 523.5524\n",
      "Training Epoch: 12 [26650/36450]\tLoss: 540.5602\n",
      "Training Epoch: 12 [26700/36450]\tLoss: 532.8948\n",
      "Training Epoch: 12 [26750/36450]\tLoss: 583.2187\n",
      "Training Epoch: 12 [26800/36450]\tLoss: 535.4766\n",
      "Training Epoch: 12 [26850/36450]\tLoss: 518.5375\n",
      "Training Epoch: 12 [26900/36450]\tLoss: 549.5502\n",
      "Training Epoch: 12 [26950/36450]\tLoss: 551.7466\n",
      "Training Epoch: 12 [27000/36450]\tLoss: 534.3549\n",
      "Training Epoch: 12 [27050/36450]\tLoss: 536.7484\n",
      "Training Epoch: 12 [27100/36450]\tLoss: 554.7908\n",
      "Training Epoch: 12 [27150/36450]\tLoss: 546.0598\n",
      "Training Epoch: 12 [27200/36450]\tLoss: 544.3895\n",
      "Training Epoch: 12 [27250/36450]\tLoss: 525.9431\n",
      "Training Epoch: 12 [27300/36450]\tLoss: 584.3164\n",
      "Training Epoch: 12 [27350/36450]\tLoss: 558.4171\n",
      "Training Epoch: 12 [27400/36450]\tLoss: 524.1158\n",
      "Training Epoch: 12 [27450/36450]\tLoss: 540.1292\n",
      "Training Epoch: 12 [27500/36450]\tLoss: 537.7584\n",
      "Training Epoch: 12 [27550/36450]\tLoss: 536.9403\n",
      "Training Epoch: 12 [27600/36450]\tLoss: 521.2863\n",
      "Training Epoch: 12 [27650/36450]\tLoss: 536.7236\n",
      "Training Epoch: 12 [27700/36450]\tLoss: 538.0115\n",
      "Training Epoch: 12 [27750/36450]\tLoss: 518.6955\n",
      "Training Epoch: 12 [27800/36450]\tLoss: 521.6215\n",
      "Training Epoch: 12 [27850/36450]\tLoss: 587.6398\n",
      "Training Epoch: 12 [27900/36450]\tLoss: 524.3895\n",
      "Training Epoch: 12 [27950/36450]\tLoss: 518.5270\n",
      "Training Epoch: 12 [28000/36450]\tLoss: 558.0663\n",
      "Training Epoch: 12 [28050/36450]\tLoss: 550.8409\n",
      "Training Epoch: 12 [28100/36450]\tLoss: 504.7967\n",
      "Training Epoch: 12 [28150/36450]\tLoss: 524.8965\n",
      "Training Epoch: 12 [28200/36450]\tLoss: 526.4461\n",
      "Training Epoch: 12 [28250/36450]\tLoss: 526.1923\n",
      "Training Epoch: 12 [28300/36450]\tLoss: 572.6007\n",
      "Training Epoch: 12 [28350/36450]\tLoss: 568.1343\n",
      "Training Epoch: 12 [28400/36450]\tLoss: 549.3690\n",
      "Training Epoch: 12 [28450/36450]\tLoss: 559.5610\n",
      "Training Epoch: 12 [28500/36450]\tLoss: 512.1549\n",
      "Training Epoch: 12 [28550/36450]\tLoss: 533.6576\n",
      "Training Epoch: 12 [28600/36450]\tLoss: 539.0567\n",
      "Training Epoch: 12 [28650/36450]\tLoss: 550.0353\n",
      "Training Epoch: 12 [28700/36450]\tLoss: 551.7121\n",
      "Training Epoch: 12 [28750/36450]\tLoss: 533.6437\n",
      "Training Epoch: 12 [28800/36450]\tLoss: 560.3561\n",
      "Training Epoch: 12 [28850/36450]\tLoss: 540.8771\n",
      "Training Epoch: 12 [28900/36450]\tLoss: 513.3899\n",
      "Training Epoch: 12 [28950/36450]\tLoss: 516.1110\n",
      "Training Epoch: 12 [29000/36450]\tLoss: 499.9005\n",
      "Training Epoch: 12 [29050/36450]\tLoss: 580.5446\n",
      "Training Epoch: 12 [29100/36450]\tLoss: 542.1484\n",
      "Training Epoch: 12 [29150/36450]\tLoss: 543.2155\n",
      "Training Epoch: 12 [29200/36450]\tLoss: 525.2579\n",
      "Training Epoch: 12 [29250/36450]\tLoss: 534.8197\n",
      "Training Epoch: 12 [29300/36450]\tLoss: 529.6266\n",
      "Training Epoch: 12 [29350/36450]\tLoss: 502.4957\n",
      "Training Epoch: 12 [29400/36450]\tLoss: 539.4006\n",
      "Training Epoch: 12 [29450/36450]\tLoss: 517.9156\n",
      "Training Epoch: 12 [29500/36450]\tLoss: 546.6644\n",
      "Training Epoch: 12 [29550/36450]\tLoss: 551.5977\n",
      "Training Epoch: 12 [29600/36450]\tLoss: 533.8934\n",
      "Training Epoch: 12 [29650/36450]\tLoss: 537.4191\n",
      "Training Epoch: 12 [29700/36450]\tLoss: 573.1992\n",
      "Training Epoch: 12 [29750/36450]\tLoss: 560.5726\n",
      "Training Epoch: 12 [29800/36450]\tLoss: 566.7164\n",
      "Training Epoch: 12 [29850/36450]\tLoss: 566.7888\n",
      "Training Epoch: 12 [29900/36450]\tLoss: 572.5582\n",
      "Training Epoch: 12 [29950/36450]\tLoss: 552.0821\n",
      "Training Epoch: 12 [30000/36450]\tLoss: 532.6354\n",
      "Training Epoch: 12 [30050/36450]\tLoss: 529.8430\n",
      "Training Epoch: 12 [30100/36450]\tLoss: 581.6898\n",
      "Training Epoch: 12 [30150/36450]\tLoss: 547.0507\n",
      "Training Epoch: 12 [30200/36450]\tLoss: 541.9399\n",
      "Training Epoch: 12 [30250/36450]\tLoss: 557.6874\n",
      "Training Epoch: 12 [30300/36450]\tLoss: 555.8806\n",
      "Training Epoch: 12 [30350/36450]\tLoss: 549.2000\n",
      "Training Epoch: 12 [30400/36450]\tLoss: 556.5746\n",
      "Training Epoch: 12 [30450/36450]\tLoss: 551.1605\n",
      "Training Epoch: 12 [30500/36450]\tLoss: 547.1281\n",
      "Training Epoch: 12 [30550/36450]\tLoss: 532.9995\n",
      "Training Epoch: 12 [30600/36450]\tLoss: 541.7591\n",
      "Training Epoch: 12 [30650/36450]\tLoss: 527.5456\n",
      "Training Epoch: 12 [30700/36450]\tLoss: 536.4773\n",
      "Training Epoch: 12 [30750/36450]\tLoss: 542.6398\n",
      "Training Epoch: 12 [30800/36450]\tLoss: 503.4536\n",
      "Training Epoch: 12 [30850/36450]\tLoss: 561.9383\n",
      "Training Epoch: 12 [30900/36450]\tLoss: 531.1826\n",
      "Training Epoch: 12 [30950/36450]\tLoss: 563.0417\n",
      "Training Epoch: 12 [31000/36450]\tLoss: 548.4048\n",
      "Training Epoch: 12 [31050/36450]\tLoss: 558.2097\n",
      "Training Epoch: 12 [31100/36450]\tLoss: 533.6930\n",
      "Training Epoch: 12 [31150/36450]\tLoss: 551.7214\n",
      "Training Epoch: 12 [31200/36450]\tLoss: 528.8500\n",
      "Training Epoch: 12 [31250/36450]\tLoss: 545.2302\n",
      "Training Epoch: 12 [31300/36450]\tLoss: 527.9098\n",
      "Training Epoch: 12 [31350/36450]\tLoss: 544.9052\n",
      "Training Epoch: 12 [31400/36450]\tLoss: 537.1036\n",
      "Training Epoch: 12 [31450/36450]\tLoss: 541.3824\n",
      "Training Epoch: 12 [31500/36450]\tLoss: 572.6857\n",
      "Training Epoch: 12 [31550/36450]\tLoss: 577.4166\n",
      "Training Epoch: 12 [31600/36450]\tLoss: 505.5750\n",
      "Training Epoch: 12 [31650/36450]\tLoss: 558.0996\n",
      "Training Epoch: 12 [31700/36450]\tLoss: 538.7105\n",
      "Training Epoch: 12 [31750/36450]\tLoss: 527.3491\n",
      "Training Epoch: 12 [31800/36450]\tLoss: 543.2190\n",
      "Training Epoch: 12 [31850/36450]\tLoss: 518.5051\n",
      "Training Epoch: 12 [31900/36450]\tLoss: 553.7105\n",
      "Training Epoch: 12 [31950/36450]\tLoss: 550.4416\n",
      "Training Epoch: 12 [32000/36450]\tLoss: 538.4562\n",
      "Training Epoch: 12 [32050/36450]\tLoss: 573.0104\n",
      "Training Epoch: 12 [32100/36450]\tLoss: 533.7276\n",
      "Training Epoch: 12 [32150/36450]\tLoss: 552.1141\n",
      "Training Epoch: 12 [32200/36450]\tLoss: 533.1876\n",
      "Training Epoch: 12 [32250/36450]\tLoss: 526.5768\n",
      "Training Epoch: 12 [32300/36450]\tLoss: 580.1234\n",
      "Training Epoch: 12 [32350/36450]\tLoss: 540.1221\n",
      "Training Epoch: 12 [32400/36450]\tLoss: 543.2329\n",
      "Training Epoch: 12 [32450/36450]\tLoss: 552.7695\n",
      "Training Epoch: 12 [32500/36450]\tLoss: 544.6580\n",
      "Training Epoch: 12 [32550/36450]\tLoss: 561.2050\n",
      "Training Epoch: 12 [32600/36450]\tLoss: 568.3482\n",
      "Training Epoch: 12 [32650/36450]\tLoss: 531.3712\n",
      "Training Epoch: 12 [32700/36450]\tLoss: 505.2884\n",
      "Training Epoch: 12 [32750/36450]\tLoss: 579.4330\n",
      "Training Epoch: 12 [32800/36450]\tLoss: 562.9402\n",
      "Training Epoch: 12 [32850/36450]\tLoss: 554.8265\n",
      "Training Epoch: 12 [32900/36450]\tLoss: 520.0940\n",
      "Training Epoch: 12 [32950/36450]\tLoss: 576.5745\n",
      "Training Epoch: 12 [33000/36450]\tLoss: 538.1492\n",
      "Training Epoch: 12 [33050/36450]\tLoss: 563.4388\n",
      "Training Epoch: 12 [33100/36450]\tLoss: 558.4968\n",
      "Training Epoch: 12 [33150/36450]\tLoss: 535.0734\n",
      "Training Epoch: 12 [33200/36450]\tLoss: 547.6014\n",
      "Training Epoch: 12 [33250/36450]\tLoss: 540.2214\n",
      "Training Epoch: 12 [33300/36450]\tLoss: 531.5732\n",
      "Training Epoch: 12 [33350/36450]\tLoss: 551.4475\n",
      "Training Epoch: 12 [33400/36450]\tLoss: 500.5339\n",
      "Training Epoch: 12 [33450/36450]\tLoss: 531.7754\n",
      "Training Epoch: 12 [33500/36450]\tLoss: 527.9200\n",
      "Training Epoch: 12 [33550/36450]\tLoss: 556.1816\n",
      "Training Epoch: 12 [33600/36450]\tLoss: 560.8580\n",
      "Training Epoch: 12 [33650/36450]\tLoss: 547.3295\n",
      "Training Epoch: 12 [33700/36450]\tLoss: 517.3431\n",
      "Training Epoch: 12 [33750/36450]\tLoss: 544.7769\n",
      "Training Epoch: 12 [33800/36450]\tLoss: 539.6823\n",
      "Training Epoch: 12 [33850/36450]\tLoss: 547.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [33900/36450]\tLoss: 532.6976\n",
      "Training Epoch: 12 [33950/36450]\tLoss: 572.5788\n",
      "Training Epoch: 12 [34000/36450]\tLoss: 560.7581\n",
      "Training Epoch: 12 [34050/36450]\tLoss: 566.7362\n",
      "Training Epoch: 12 [34100/36450]\tLoss: 555.7944\n",
      "Training Epoch: 12 [34150/36450]\tLoss: 565.2999\n",
      "Training Epoch: 12 [34200/36450]\tLoss: 525.1810\n",
      "Training Epoch: 12 [34250/36450]\tLoss: 536.6177\n",
      "Training Epoch: 12 [34300/36450]\tLoss: 562.3969\n",
      "Training Epoch: 12 [34350/36450]\tLoss: 576.7739\n",
      "Training Epoch: 12 [34400/36450]\tLoss: 551.6500\n",
      "Training Epoch: 12 [34450/36450]\tLoss: 534.5088\n",
      "Training Epoch: 12 [34500/36450]\tLoss: 523.6918\n",
      "Training Epoch: 12 [34550/36450]\tLoss: 547.6979\n",
      "Training Epoch: 12 [34600/36450]\tLoss: 552.9000\n",
      "Training Epoch: 12 [34650/36450]\tLoss: 554.2781\n",
      "Training Epoch: 12 [34700/36450]\tLoss: 557.2832\n",
      "Training Epoch: 12 [34750/36450]\tLoss: 484.5010\n",
      "Training Epoch: 12 [34800/36450]\tLoss: 543.2641\n",
      "Training Epoch: 12 [34850/36450]\tLoss: 589.8314\n",
      "Training Epoch: 12 [34900/36450]\tLoss: 512.4439\n",
      "Training Epoch: 12 [34950/36450]\tLoss: 539.6910\n",
      "Training Epoch: 12 [35000/36450]\tLoss: 525.7634\n",
      "Training Epoch: 12 [35050/36450]\tLoss: 539.6761\n",
      "Training Epoch: 12 [35100/36450]\tLoss: 523.0087\n",
      "Training Epoch: 12 [35150/36450]\tLoss: 541.0009\n",
      "Training Epoch: 12 [35200/36450]\tLoss: 527.4326\n",
      "Training Epoch: 12 [35250/36450]\tLoss: 540.0271\n",
      "Training Epoch: 12 [35300/36450]\tLoss: 553.0225\n",
      "Training Epoch: 12 [35350/36450]\tLoss: 548.6553\n",
      "Training Epoch: 12 [35400/36450]\tLoss: 537.8943\n",
      "Training Epoch: 12 [35450/36450]\tLoss: 536.0225\n",
      "Training Epoch: 12 [35500/36450]\tLoss: 557.4127\n",
      "Training Epoch: 12 [35550/36450]\tLoss: 532.6910\n",
      "Training Epoch: 12 [35600/36450]\tLoss: 589.0345\n",
      "Training Epoch: 12 [35650/36450]\tLoss: 551.9652\n",
      "Training Epoch: 12 [35700/36450]\tLoss: 569.4485\n",
      "Training Epoch: 12 [35750/36450]\tLoss: 524.6106\n",
      "Training Epoch: 12 [35800/36450]\tLoss: 573.0684\n",
      "Training Epoch: 12 [35850/36450]\tLoss: 539.5847\n",
      "Training Epoch: 12 [35900/36450]\tLoss: 518.1666\n",
      "Training Epoch: 12 [35950/36450]\tLoss: 550.6486\n",
      "Training Epoch: 12 [36000/36450]\tLoss: 540.4388\n",
      "Training Epoch: 12 [36050/36450]\tLoss: 573.0305\n",
      "Training Epoch: 12 [36100/36450]\tLoss: 563.7239\n",
      "Training Epoch: 12 [36150/36450]\tLoss: 517.7859\n",
      "Training Epoch: 12 [36200/36450]\tLoss: 532.9027\n",
      "Training Epoch: 12 [36250/36450]\tLoss: 549.9509\n",
      "Training Epoch: 12 [36300/36450]\tLoss: 557.9941\n",
      "Training Epoch: 12 [36350/36450]\tLoss: 546.3353\n",
      "Training Epoch: 12 [36400/36450]\tLoss: 594.8067\n",
      "Training Epoch: 12 [36450/36450]\tLoss: 517.2753\n",
      "Training Epoch: 12 [4050/4050]\tLoss: 278.3014\n",
      "Training Epoch: 13 [50/36450]\tLoss: 553.0550\n",
      "Training Epoch: 13 [100/36450]\tLoss: 566.5305\n",
      "Training Epoch: 13 [150/36450]\tLoss: 550.7241\n",
      "Training Epoch: 13 [200/36450]\tLoss: 590.9779\n",
      "Training Epoch: 13 [250/36450]\tLoss: 579.2164\n",
      "Training Epoch: 13 [300/36450]\tLoss: 555.2083\n",
      "Training Epoch: 13 [350/36450]\tLoss: 542.7015\n",
      "Training Epoch: 13 [400/36450]\tLoss: 556.5300\n",
      "Training Epoch: 13 [450/36450]\tLoss: 519.2726\n",
      "Training Epoch: 13 [500/36450]\tLoss: 496.0931\n",
      "Training Epoch: 13 [550/36450]\tLoss: 540.4058\n",
      "Training Epoch: 13 [600/36450]\tLoss: 543.6968\n",
      "Training Epoch: 13 [650/36450]\tLoss: 580.5039\n",
      "Training Epoch: 13 [700/36450]\tLoss: 568.7175\n",
      "Training Epoch: 13 [750/36450]\tLoss: 549.0612\n",
      "Training Epoch: 13 [800/36450]\tLoss: 582.8867\n",
      "Training Epoch: 13 [850/36450]\tLoss: 602.2786\n",
      "Training Epoch: 13 [900/36450]\tLoss: 547.6976\n",
      "Training Epoch: 13 [950/36450]\tLoss: 535.9974\n",
      "Training Epoch: 13 [1000/36450]\tLoss: 517.7293\n",
      "Training Epoch: 13 [1050/36450]\tLoss: 543.1066\n",
      "Training Epoch: 13 [1100/36450]\tLoss: 532.7135\n",
      "Training Epoch: 13 [1150/36450]\tLoss: 528.4230\n",
      "Training Epoch: 13 [1200/36450]\tLoss: 568.9325\n",
      "Training Epoch: 13 [1250/36450]\tLoss: 535.1002\n",
      "Training Epoch: 13 [1300/36450]\tLoss: 561.7074\n",
      "Training Epoch: 13 [1350/36450]\tLoss: 533.3016\n",
      "Training Epoch: 13 [1400/36450]\tLoss: 529.8345\n",
      "Training Epoch: 13 [1450/36450]\tLoss: 543.6978\n",
      "Training Epoch: 13 [1500/36450]\tLoss: 556.5203\n",
      "Training Epoch: 13 [1550/36450]\tLoss: 505.8052\n",
      "Training Epoch: 13 [1600/36450]\tLoss: 562.8547\n",
      "Training Epoch: 13 [1650/36450]\tLoss: 559.8123\n",
      "Training Epoch: 13 [1700/36450]\tLoss: 513.6201\n",
      "Training Epoch: 13 [1750/36450]\tLoss: 527.1277\n",
      "Training Epoch: 13 [1800/36450]\tLoss: 551.1459\n",
      "Training Epoch: 13 [1850/36450]\tLoss: 510.7202\n",
      "Training Epoch: 13 [1900/36450]\tLoss: 537.6248\n",
      "Training Epoch: 13 [1950/36450]\tLoss: 513.5694\n",
      "Training Epoch: 13 [2000/36450]\tLoss: 560.7088\n",
      "Training Epoch: 13 [2050/36450]\tLoss: 570.9810\n",
      "Training Epoch: 13 [2100/36450]\tLoss: 579.3258\n",
      "Training Epoch: 13 [2150/36450]\tLoss: 533.8748\n",
      "Training Epoch: 13 [2200/36450]\tLoss: 522.2518\n",
      "Training Epoch: 13 [2250/36450]\tLoss: 479.1094\n",
      "Training Epoch: 13 [2300/36450]\tLoss: 503.3384\n",
      "Training Epoch: 13 [2350/36450]\tLoss: 502.2986\n",
      "Training Epoch: 13 [2400/36450]\tLoss: 506.8286\n",
      "Training Epoch: 13 [2450/36450]\tLoss: 525.4079\n",
      "Training Epoch: 13 [2500/36450]\tLoss: 542.7003\n",
      "Training Epoch: 13 [2550/36450]\tLoss: 552.5057\n",
      "Training Epoch: 13 [2600/36450]\tLoss: 537.3572\n",
      "Training Epoch: 13 [2650/36450]\tLoss: 531.3989\n",
      "Training Epoch: 13 [2700/36450]\tLoss: 534.2942\n",
      "Training Epoch: 13 [2750/36450]\tLoss: 540.3954\n",
      "Training Epoch: 13 [2800/36450]\tLoss: 522.6588\n",
      "Training Epoch: 13 [2850/36450]\tLoss: 553.8766\n",
      "Training Epoch: 13 [2900/36450]\tLoss: 523.5934\n",
      "Training Epoch: 13 [2950/36450]\tLoss: 539.4263\n",
      "Training Epoch: 13 [3000/36450]\tLoss: 540.1462\n",
      "Training Epoch: 13 [3050/36450]\tLoss: 535.1831\n",
      "Training Epoch: 13 [3100/36450]\tLoss: 552.9482\n",
      "Training Epoch: 13 [3150/36450]\tLoss: 539.7289\n",
      "Training Epoch: 13 [3200/36450]\tLoss: 558.9193\n",
      "Training Epoch: 13 [3250/36450]\tLoss: 542.8171\n",
      "Training Epoch: 13 [3300/36450]\tLoss: 541.0009\n",
      "Training Epoch: 13 [3350/36450]\tLoss: 542.0157\n",
      "Training Epoch: 13 [3400/36450]\tLoss: 565.6336\n",
      "Training Epoch: 13 [3450/36450]\tLoss: 548.5191\n",
      "Training Epoch: 13 [3500/36450]\tLoss: 548.8578\n",
      "Training Epoch: 13 [3550/36450]\tLoss: 528.6925\n",
      "Training Epoch: 13 [3600/36450]\tLoss: 529.8245\n",
      "Training Epoch: 13 [3650/36450]\tLoss: 543.4344\n",
      "Training Epoch: 13 [3700/36450]\tLoss: 547.9532\n",
      "Training Epoch: 13 [3750/36450]\tLoss: 541.8712\n",
      "Training Epoch: 13 [3800/36450]\tLoss: 524.1271\n",
      "Training Epoch: 13 [3850/36450]\tLoss: 521.2422\n",
      "Training Epoch: 13 [3900/36450]\tLoss: 561.0675\n",
      "Training Epoch: 13 [3950/36450]\tLoss: 526.7444\n",
      "Training Epoch: 13 [4000/36450]\tLoss: 536.3619\n",
      "Training Epoch: 13 [4050/36450]\tLoss: 558.5875\n",
      "Training Epoch: 13 [4100/36450]\tLoss: 559.6512\n",
      "Training Epoch: 13 [4150/36450]\tLoss: 543.0911\n",
      "Training Epoch: 13 [4200/36450]\tLoss: 569.1875\n",
      "Training Epoch: 13 [4250/36450]\tLoss: 572.3970\n",
      "Training Epoch: 13 [4300/36450]\tLoss: 529.1145\n",
      "Training Epoch: 13 [4350/36450]\tLoss: 549.5883\n",
      "Training Epoch: 13 [4400/36450]\tLoss: 533.0504\n",
      "Training Epoch: 13 [4450/36450]\tLoss: 561.1486\n",
      "Training Epoch: 13 [4500/36450]\tLoss: 541.3294\n",
      "Training Epoch: 13 [4550/36450]\tLoss: 532.5566\n",
      "Training Epoch: 13 [4600/36450]\tLoss: 548.6516\n",
      "Training Epoch: 13 [4650/36450]\tLoss: 526.8595\n",
      "Training Epoch: 13 [4700/36450]\tLoss: 573.0150\n",
      "Training Epoch: 13 [4750/36450]\tLoss: 559.1516\n",
      "Training Epoch: 13 [4800/36450]\tLoss: 558.2374\n",
      "Training Epoch: 13 [4850/36450]\tLoss: 491.3789\n",
      "Training Epoch: 13 [4900/36450]\tLoss: 509.3259\n",
      "Training Epoch: 13 [4950/36450]\tLoss: 538.6727\n",
      "Training Epoch: 13 [5000/36450]\tLoss: 537.6539\n",
      "Training Epoch: 13 [5050/36450]\tLoss: 539.4930\n",
      "Training Epoch: 13 [5100/36450]\tLoss: 555.8066\n",
      "Training Epoch: 13 [5150/36450]\tLoss: 511.4442\n",
      "Training Epoch: 13 [5200/36450]\tLoss: 539.5525\n",
      "Training Epoch: 13 [5250/36450]\tLoss: 545.9973\n",
      "Training Epoch: 13 [5300/36450]\tLoss: 524.1541\n",
      "Training Epoch: 13 [5350/36450]\tLoss: 523.4409\n",
      "Training Epoch: 13 [5400/36450]\tLoss: 511.9773\n",
      "Training Epoch: 13 [5450/36450]\tLoss: 527.5379\n",
      "Training Epoch: 13 [5500/36450]\tLoss: 517.7358\n",
      "Training Epoch: 13 [5550/36450]\tLoss: 547.1528\n",
      "Training Epoch: 13 [5600/36450]\tLoss: 550.2828\n",
      "Training Epoch: 13 [5650/36450]\tLoss: 526.6163\n",
      "Training Epoch: 13 [5700/36450]\tLoss: 512.1535\n",
      "Training Epoch: 13 [5750/36450]\tLoss: 508.1080\n",
      "Training Epoch: 13 [5800/36450]\tLoss: 502.5118\n",
      "Training Epoch: 13 [5850/36450]\tLoss: 550.1709\n",
      "Training Epoch: 13 [5900/36450]\tLoss: 542.9816\n",
      "Training Epoch: 13 [5950/36450]\tLoss: 563.2998\n",
      "Training Epoch: 13 [6000/36450]\tLoss: 566.8993\n",
      "Training Epoch: 13 [6050/36450]\tLoss: 553.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [6100/36450]\tLoss: 505.7943\n",
      "Training Epoch: 13 [6150/36450]\tLoss: 529.5997\n",
      "Training Epoch: 13 [6200/36450]\tLoss: 545.5197\n",
      "Training Epoch: 13 [6250/36450]\tLoss: 512.6729\n",
      "Training Epoch: 13 [6300/36450]\tLoss: 551.3635\n",
      "Training Epoch: 13 [6350/36450]\tLoss: 527.7922\n",
      "Training Epoch: 13 [6400/36450]\tLoss: 530.6905\n",
      "Training Epoch: 13 [6450/36450]\tLoss: 543.3550\n",
      "Training Epoch: 13 [6500/36450]\tLoss: 528.0487\n",
      "Training Epoch: 13 [6550/36450]\tLoss: 524.4874\n",
      "Training Epoch: 13 [6600/36450]\tLoss: 521.4365\n",
      "Training Epoch: 13 [6650/36450]\tLoss: 519.2733\n",
      "Training Epoch: 13 [6700/36450]\tLoss: 532.1632\n",
      "Training Epoch: 13 [6750/36450]\tLoss: 517.3052\n",
      "Training Epoch: 13 [6800/36450]\tLoss: 526.8879\n",
      "Training Epoch: 13 [6850/36450]\tLoss: 534.1907\n",
      "Training Epoch: 13 [6900/36450]\tLoss: 517.4088\n",
      "Training Epoch: 13 [6950/36450]\tLoss: 576.3492\n",
      "Training Epoch: 13 [7000/36450]\tLoss: 570.1439\n",
      "Training Epoch: 13 [7050/36450]\tLoss: 579.7040\n",
      "Training Epoch: 13 [7100/36450]\tLoss: 523.4584\n",
      "Training Epoch: 13 [7150/36450]\tLoss: 559.6911\n",
      "Training Epoch: 13 [7200/36450]\tLoss: 528.0088\n",
      "Training Epoch: 13 [7250/36450]\tLoss: 509.8446\n",
      "Training Epoch: 13 [7300/36450]\tLoss: 509.5235\n",
      "Training Epoch: 13 [7350/36450]\tLoss: 552.7197\n",
      "Training Epoch: 13 [7400/36450]\tLoss: 522.3024\n",
      "Training Epoch: 13 [7450/36450]\tLoss: 518.2941\n",
      "Training Epoch: 13 [7500/36450]\tLoss: 563.6202\n",
      "Training Epoch: 13 [7550/36450]\tLoss: 541.0457\n",
      "Training Epoch: 13 [7600/36450]\tLoss: 551.9578\n",
      "Training Epoch: 13 [7650/36450]\tLoss: 558.3854\n",
      "Training Epoch: 13 [7700/36450]\tLoss: 551.8634\n",
      "Training Epoch: 13 [7750/36450]\tLoss: 519.1768\n",
      "Training Epoch: 13 [7800/36450]\tLoss: 533.7594\n",
      "Training Epoch: 13 [7850/36450]\tLoss: 545.5303\n",
      "Training Epoch: 13 [7900/36450]\tLoss: 535.5121\n",
      "Training Epoch: 13 [7950/36450]\tLoss: 559.7963\n",
      "Training Epoch: 13 [8000/36450]\tLoss: 516.6502\n",
      "Training Epoch: 13 [8050/36450]\tLoss: 556.6976\n",
      "Training Epoch: 13 [8100/36450]\tLoss: 530.9171\n",
      "Training Epoch: 13 [8150/36450]\tLoss: 523.7429\n",
      "Training Epoch: 13 [8200/36450]\tLoss: 529.2476\n",
      "Training Epoch: 13 [8250/36450]\tLoss: 580.2824\n",
      "Training Epoch: 13 [8300/36450]\tLoss: 550.0632\n",
      "Training Epoch: 13 [8350/36450]\tLoss: 539.4831\n",
      "Training Epoch: 13 [8400/36450]\tLoss: 571.3477\n",
      "Training Epoch: 13 [8450/36450]\tLoss: 544.3322\n",
      "Training Epoch: 13 [8500/36450]\tLoss: 553.6992\n",
      "Training Epoch: 13 [8550/36450]\tLoss: 528.5579\n",
      "Training Epoch: 13 [8600/36450]\tLoss: 531.2692\n",
      "Training Epoch: 13 [8650/36450]\tLoss: 550.3759\n",
      "Training Epoch: 13 [8700/36450]\tLoss: 554.1119\n",
      "Training Epoch: 13 [8750/36450]\tLoss: 559.1271\n",
      "Training Epoch: 13 [8800/36450]\tLoss: 540.5666\n",
      "Training Epoch: 13 [8850/36450]\tLoss: 563.0527\n",
      "Training Epoch: 13 [8900/36450]\tLoss: 561.4598\n",
      "Training Epoch: 13 [8950/36450]\tLoss: 544.0182\n",
      "Training Epoch: 13 [9000/36450]\tLoss: 575.1266\n",
      "Training Epoch: 13 [9050/36450]\tLoss: 558.9745\n",
      "Training Epoch: 13 [9100/36450]\tLoss: 552.1974\n",
      "Training Epoch: 13 [9150/36450]\tLoss: 580.0747\n",
      "Training Epoch: 13 [9200/36450]\tLoss: 597.1076\n",
      "Training Epoch: 13 [9250/36450]\tLoss: 566.1589\n",
      "Training Epoch: 13 [9300/36450]\tLoss: 582.6217\n",
      "Training Epoch: 13 [9350/36450]\tLoss: 577.7634\n",
      "Training Epoch: 13 [9400/36450]\tLoss: 557.9332\n",
      "Training Epoch: 13 [9450/36450]\tLoss: 524.1525\n",
      "Training Epoch: 13 [9500/36450]\tLoss: 552.2047\n",
      "Training Epoch: 13 [9550/36450]\tLoss: 533.1379\n",
      "Training Epoch: 13 [9600/36450]\tLoss: 540.2360\n",
      "Training Epoch: 13 [9650/36450]\tLoss: 534.7891\n",
      "Training Epoch: 13 [9700/36450]\tLoss: 571.1030\n",
      "Training Epoch: 13 [9750/36450]\tLoss: 526.5128\n",
      "Training Epoch: 13 [9800/36450]\tLoss: 589.7716\n",
      "Training Epoch: 13 [9850/36450]\tLoss: 600.6183\n",
      "Training Epoch: 13 [9900/36450]\tLoss: 566.6022\n",
      "Training Epoch: 13 [9950/36450]\tLoss: 561.2530\n",
      "Training Epoch: 13 [10000/36450]\tLoss: 563.1768\n",
      "Training Epoch: 13 [10050/36450]\tLoss: 562.4348\n",
      "Training Epoch: 13 [10100/36450]\tLoss: 550.4946\n",
      "Training Epoch: 13 [10150/36450]\tLoss: 569.5285\n",
      "Training Epoch: 13 [10200/36450]\tLoss: 535.6260\n",
      "Training Epoch: 13 [10250/36450]\tLoss: 588.5605\n",
      "Training Epoch: 13 [10300/36450]\tLoss: 547.1851\n",
      "Training Epoch: 13 [10350/36450]\tLoss: 569.8095\n",
      "Training Epoch: 13 [10400/36450]\tLoss: 621.9578\n",
      "Training Epoch: 13 [10450/36450]\tLoss: 508.5198\n",
      "Training Epoch: 13 [10500/36450]\tLoss: 546.3673\n",
      "Training Epoch: 13 [10550/36450]\tLoss: 523.6419\n",
      "Training Epoch: 13 [10600/36450]\tLoss: 583.0161\n",
      "Training Epoch: 13 [10650/36450]\tLoss: 566.0576\n",
      "Training Epoch: 13 [10700/36450]\tLoss: 577.9948\n",
      "Training Epoch: 13 [10750/36450]\tLoss: 558.2781\n",
      "Training Epoch: 13 [10800/36450]\tLoss: 570.6719\n",
      "Training Epoch: 13 [10850/36450]\tLoss: 516.5892\n",
      "Training Epoch: 13 [10900/36450]\tLoss: 538.2830\n",
      "Training Epoch: 13 [10950/36450]\tLoss: 536.9474\n",
      "Training Epoch: 13 [11000/36450]\tLoss: 535.8065\n",
      "Training Epoch: 13 [11050/36450]\tLoss: 571.6370\n",
      "Training Epoch: 13 [11100/36450]\tLoss: 529.3115\n",
      "Training Epoch: 13 [11150/36450]\tLoss: 533.0833\n",
      "Training Epoch: 13 [11200/36450]\tLoss: 557.1562\n",
      "Training Epoch: 13 [11250/36450]\tLoss: 572.7751\n",
      "Training Epoch: 13 [11300/36450]\tLoss: 510.8193\n",
      "Training Epoch: 13 [11350/36450]\tLoss: 569.1070\n",
      "Training Epoch: 13 [11400/36450]\tLoss: 539.2718\n",
      "Training Epoch: 13 [11450/36450]\tLoss: 553.9247\n",
      "Training Epoch: 13 [11500/36450]\tLoss: 555.2125\n",
      "Training Epoch: 13 [11550/36450]\tLoss: 566.7010\n",
      "Training Epoch: 13 [11600/36450]\tLoss: 556.4952\n",
      "Training Epoch: 13 [11650/36450]\tLoss: 529.8912\n",
      "Training Epoch: 13 [11700/36450]\tLoss: 535.9625\n",
      "Training Epoch: 13 [11750/36450]\tLoss: 553.8498\n",
      "Training Epoch: 13 [11800/36450]\tLoss: 554.4047\n",
      "Training Epoch: 13 [11850/36450]\tLoss: 554.9174\n",
      "Training Epoch: 13 [11900/36450]\tLoss: 547.4904\n",
      "Training Epoch: 13 [11950/36450]\tLoss: 531.6443\n",
      "Training Epoch: 13 [12000/36450]\tLoss: 533.2117\n",
      "Training Epoch: 13 [12050/36450]\tLoss: 511.5919\n",
      "Training Epoch: 13 [12100/36450]\tLoss: 497.1465\n",
      "Training Epoch: 13 [12150/36450]\tLoss: 563.9944\n",
      "Training Epoch: 13 [12200/36450]\tLoss: 542.4993\n",
      "Training Epoch: 13 [12250/36450]\tLoss: 527.6385\n",
      "Training Epoch: 13 [12300/36450]\tLoss: 559.7928\n",
      "Training Epoch: 13 [12350/36450]\tLoss: 547.9934\n",
      "Training Epoch: 13 [12400/36450]\tLoss: 509.0493\n",
      "Training Epoch: 13 [12450/36450]\tLoss: 520.7858\n",
      "Training Epoch: 13 [12500/36450]\tLoss: 527.8430\n",
      "Training Epoch: 13 [12550/36450]\tLoss: 527.0054\n",
      "Training Epoch: 13 [12600/36450]\tLoss: 522.3104\n",
      "Training Epoch: 13 [12650/36450]\tLoss: 551.2604\n",
      "Training Epoch: 13 [12700/36450]\tLoss: 544.1207\n",
      "Training Epoch: 13 [12750/36450]\tLoss: 537.9453\n",
      "Training Epoch: 13 [12800/36450]\tLoss: 607.5993\n",
      "Training Epoch: 13 [12850/36450]\tLoss: 526.9923\n",
      "Training Epoch: 13 [12900/36450]\tLoss: 549.1694\n",
      "Training Epoch: 13 [12950/36450]\tLoss: 546.7852\n",
      "Training Epoch: 13 [13000/36450]\tLoss: 543.0450\n",
      "Training Epoch: 13 [13050/36450]\tLoss: 557.5181\n",
      "Training Epoch: 13 [13100/36450]\tLoss: 525.4941\n",
      "Training Epoch: 13 [13150/36450]\tLoss: 536.1607\n",
      "Training Epoch: 13 [13200/36450]\tLoss: 522.5240\n",
      "Training Epoch: 13 [13250/36450]\tLoss: 521.8013\n",
      "Training Epoch: 13 [13300/36450]\tLoss: 524.6620\n",
      "Training Epoch: 13 [13350/36450]\tLoss: 511.7632\n",
      "Training Epoch: 13 [13400/36450]\tLoss: 545.3841\n",
      "Training Epoch: 13 [13450/36450]\tLoss: 538.3083\n",
      "Training Epoch: 13 [13500/36450]\tLoss: 533.1822\n",
      "Training Epoch: 13 [13550/36450]\tLoss: 551.3842\n",
      "Training Epoch: 13 [13600/36450]\tLoss: 553.3863\n",
      "Training Epoch: 13 [13650/36450]\tLoss: 537.8495\n",
      "Training Epoch: 13 [13700/36450]\tLoss: 534.2780\n",
      "Training Epoch: 13 [13750/36450]\tLoss: 522.0587\n",
      "Training Epoch: 13 [13800/36450]\tLoss: 526.5424\n",
      "Training Epoch: 13 [13850/36450]\tLoss: 536.8428\n",
      "Training Epoch: 13 [13900/36450]\tLoss: 507.7100\n",
      "Training Epoch: 13 [13950/36450]\tLoss: 540.3271\n",
      "Training Epoch: 13 [14000/36450]\tLoss: 563.2209\n",
      "Training Epoch: 13 [14050/36450]\tLoss: 539.7578\n",
      "Training Epoch: 13 [14100/36450]\tLoss: 589.0030\n",
      "Training Epoch: 13 [14150/36450]\tLoss: 530.9815\n",
      "Training Epoch: 13 [14200/36450]\tLoss: 491.6237\n",
      "Training Epoch: 13 [14250/36450]\tLoss: 503.8789\n",
      "Training Epoch: 13 [14300/36450]\tLoss: 535.8499\n",
      "Training Epoch: 13 [14350/36450]\tLoss: 516.8506\n",
      "Training Epoch: 13 [14400/36450]\tLoss: 535.6802\n",
      "Training Epoch: 13 [14450/36450]\tLoss: 556.9854\n",
      "Training Epoch: 13 [14500/36450]\tLoss: 554.5370\n",
      "Training Epoch: 13 [14550/36450]\tLoss: 523.1511\n",
      "Training Epoch: 13 [14600/36450]\tLoss: 493.2524\n",
      "Training Epoch: 13 [14650/36450]\tLoss: 576.6912\n",
      "Training Epoch: 13 [14700/36450]\tLoss: 558.3665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [14750/36450]\tLoss: 542.9332\n",
      "Training Epoch: 13 [14800/36450]\tLoss: 562.7545\n",
      "Training Epoch: 13 [14850/36450]\tLoss: 555.2416\n",
      "Training Epoch: 13 [14900/36450]\tLoss: 543.5574\n",
      "Training Epoch: 13 [14950/36450]\tLoss: 560.6466\n",
      "Training Epoch: 13 [15000/36450]\tLoss: 535.7732\n",
      "Training Epoch: 13 [15050/36450]\tLoss: 531.3435\n",
      "Training Epoch: 13 [15100/36450]\tLoss: 544.0854\n",
      "Training Epoch: 13 [15150/36450]\tLoss: 517.4309\n",
      "Training Epoch: 13 [15200/36450]\tLoss: 520.2783\n",
      "Training Epoch: 13 [15250/36450]\tLoss: 519.6772\n",
      "Training Epoch: 13 [15300/36450]\tLoss: 525.5208\n",
      "Training Epoch: 13 [15350/36450]\tLoss: 519.3079\n",
      "Training Epoch: 13 [15400/36450]\tLoss: 512.6681\n",
      "Training Epoch: 13 [15450/36450]\tLoss: 558.7917\n",
      "Training Epoch: 13 [15500/36450]\tLoss: 555.9862\n",
      "Training Epoch: 13 [15550/36450]\tLoss: 511.7223\n",
      "Training Epoch: 13 [15600/36450]\tLoss: 548.6553\n",
      "Training Epoch: 13 [15650/36450]\tLoss: 542.6417\n",
      "Training Epoch: 13 [15700/36450]\tLoss: 558.4577\n",
      "Training Epoch: 13 [15750/36450]\tLoss: 509.5292\n",
      "Training Epoch: 13 [15800/36450]\tLoss: 562.0715\n",
      "Training Epoch: 13 [15850/36450]\tLoss: 498.4524\n",
      "Training Epoch: 13 [15900/36450]\tLoss: 546.7297\n",
      "Training Epoch: 13 [15950/36450]\tLoss: 529.6052\n",
      "Training Epoch: 13 [16000/36450]\tLoss: 549.7092\n",
      "Training Epoch: 13 [16050/36450]\tLoss: 543.5299\n",
      "Training Epoch: 13 [16100/36450]\tLoss: 572.4690\n",
      "Training Epoch: 13 [16150/36450]\tLoss: 543.6537\n",
      "Training Epoch: 13 [16200/36450]\tLoss: 538.3067\n",
      "Training Epoch: 13 [16250/36450]\tLoss: 528.2498\n",
      "Training Epoch: 13 [16300/36450]\tLoss: 526.9710\n",
      "Training Epoch: 13 [16350/36450]\tLoss: 546.2964\n",
      "Training Epoch: 13 [16400/36450]\tLoss: 502.7811\n",
      "Training Epoch: 13 [16450/36450]\tLoss: 538.0229\n",
      "Training Epoch: 13 [16500/36450]\tLoss: 551.6514\n",
      "Training Epoch: 13 [16550/36450]\tLoss: 577.0579\n",
      "Training Epoch: 13 [16600/36450]\tLoss: 513.7478\n",
      "Training Epoch: 13 [16650/36450]\tLoss: 583.2338\n",
      "Training Epoch: 13 [16700/36450]\tLoss: 551.3911\n",
      "Training Epoch: 13 [16750/36450]\tLoss: 537.1233\n",
      "Training Epoch: 13 [16800/36450]\tLoss: 521.1759\n",
      "Training Epoch: 13 [16850/36450]\tLoss: 548.2623\n",
      "Training Epoch: 13 [16900/36450]\tLoss: 561.2900\n",
      "Training Epoch: 13 [16950/36450]\tLoss: 525.5646\n",
      "Training Epoch: 13 [17000/36450]\tLoss: 573.7438\n",
      "Training Epoch: 13 [17050/36450]\tLoss: 545.5773\n",
      "Training Epoch: 13 [17100/36450]\tLoss: 537.9046\n",
      "Training Epoch: 13 [17150/36450]\tLoss: 547.9219\n",
      "Training Epoch: 13 [17200/36450]\tLoss: 531.3796\n",
      "Training Epoch: 13 [17250/36450]\tLoss: 529.4305\n",
      "Training Epoch: 13 [17300/36450]\tLoss: 539.7720\n",
      "Training Epoch: 13 [17350/36450]\tLoss: 533.7459\n",
      "Training Epoch: 13 [17400/36450]\tLoss: 528.7666\n",
      "Training Epoch: 13 [17450/36450]\tLoss: 488.7684\n",
      "Training Epoch: 13 [17500/36450]\tLoss: 519.9474\n",
      "Training Epoch: 13 [17550/36450]\tLoss: 554.9997\n",
      "Training Epoch: 13 [17600/36450]\tLoss: 513.4830\n",
      "Training Epoch: 13 [17650/36450]\tLoss: 519.5765\n",
      "Training Epoch: 13 [17700/36450]\tLoss: 521.8141\n",
      "Training Epoch: 13 [17750/36450]\tLoss: 531.8546\n",
      "Training Epoch: 13 [17800/36450]\tLoss: 547.8870\n",
      "Training Epoch: 13 [17850/36450]\tLoss: 511.5116\n",
      "Training Epoch: 13 [17900/36450]\tLoss: 535.7051\n",
      "Training Epoch: 13 [17950/36450]\tLoss: 561.9062\n",
      "Training Epoch: 13 [18000/36450]\tLoss: 538.7398\n",
      "Training Epoch: 13 [18050/36450]\tLoss: 560.0151\n",
      "Training Epoch: 13 [18100/36450]\tLoss: 528.7583\n",
      "Training Epoch: 13 [18150/36450]\tLoss: 543.2882\n",
      "Training Epoch: 13 [18200/36450]\tLoss: 493.2360\n",
      "Training Epoch: 13 [18250/36450]\tLoss: 515.6143\n",
      "Training Epoch: 13 [18300/36450]\tLoss: 512.5327\n",
      "Training Epoch: 13 [18350/36450]\tLoss: 541.2524\n",
      "Training Epoch: 13 [18400/36450]\tLoss: 546.2362\n",
      "Training Epoch: 13 [18450/36450]\tLoss: 520.8138\n",
      "Training Epoch: 13 [18500/36450]\tLoss: 549.8467\n",
      "Training Epoch: 13 [18550/36450]\tLoss: 552.5120\n",
      "Training Epoch: 13 [18600/36450]\tLoss: 525.5676\n",
      "Training Epoch: 13 [18650/36450]\tLoss: 555.1023\n",
      "Training Epoch: 13 [18700/36450]\tLoss: 516.7944\n",
      "Training Epoch: 13 [18750/36450]\tLoss: 542.9046\n",
      "Training Epoch: 13 [18800/36450]\tLoss: 554.5146\n",
      "Training Epoch: 13 [18850/36450]\tLoss: 572.9255\n",
      "Training Epoch: 13 [18900/36450]\tLoss: 526.9293\n",
      "Training Epoch: 13 [18950/36450]\tLoss: 552.3866\n",
      "Training Epoch: 13 [19000/36450]\tLoss: 572.1899\n",
      "Training Epoch: 13 [19050/36450]\tLoss: 498.2581\n",
      "Training Epoch: 13 [19100/36450]\tLoss: 551.3240\n",
      "Training Epoch: 13 [19150/36450]\tLoss: 514.4672\n",
      "Training Epoch: 13 [19200/36450]\tLoss: 531.0665\n",
      "Training Epoch: 13 [19250/36450]\tLoss: 528.4292\n",
      "Training Epoch: 13 [19300/36450]\tLoss: 505.9826\n",
      "Training Epoch: 13 [19350/36450]\tLoss: 553.4708\n",
      "Training Epoch: 13 [19400/36450]\tLoss: 513.4319\n",
      "Training Epoch: 13 [19450/36450]\tLoss: 547.8054\n",
      "Training Epoch: 13 [19500/36450]\tLoss: 535.1010\n",
      "Training Epoch: 13 [19550/36450]\tLoss: 513.9338\n",
      "Training Epoch: 13 [19600/36450]\tLoss: 531.2897\n",
      "Training Epoch: 13 [19650/36450]\tLoss: 536.0609\n",
      "Training Epoch: 13 [19700/36450]\tLoss: 539.8513\n",
      "Training Epoch: 13 [19750/36450]\tLoss: 515.2783\n",
      "Training Epoch: 13 [19800/36450]\tLoss: 550.0519\n",
      "Training Epoch: 13 [19850/36450]\tLoss: 557.6478\n",
      "Training Epoch: 13 [19900/36450]\tLoss: 529.0004\n",
      "Training Epoch: 13 [19950/36450]\tLoss: 530.8021\n",
      "Training Epoch: 13 [20000/36450]\tLoss: 532.4468\n",
      "Training Epoch: 13 [20050/36450]\tLoss: 545.1597\n",
      "Training Epoch: 13 [20100/36450]\tLoss: 525.5701\n",
      "Training Epoch: 13 [20150/36450]\tLoss: 535.3344\n",
      "Training Epoch: 13 [20200/36450]\tLoss: 527.4897\n",
      "Training Epoch: 13 [20250/36450]\tLoss: 549.3533\n",
      "Training Epoch: 13 [20300/36450]\tLoss: 515.9230\n",
      "Training Epoch: 13 [20350/36450]\tLoss: 523.3120\n",
      "Training Epoch: 13 [20400/36450]\tLoss: 563.6461\n",
      "Training Epoch: 13 [20450/36450]\tLoss: 560.1317\n",
      "Training Epoch: 13 [20500/36450]\tLoss: 563.2615\n",
      "Training Epoch: 13 [20550/36450]\tLoss: 510.5597\n",
      "Training Epoch: 13 [20600/36450]\tLoss: 540.2507\n",
      "Training Epoch: 13 [20650/36450]\tLoss: 562.3416\n",
      "Training Epoch: 13 [20700/36450]\tLoss: 552.8844\n",
      "Training Epoch: 13 [20750/36450]\tLoss: 560.3533\n",
      "Training Epoch: 13 [20800/36450]\tLoss: 573.6787\n",
      "Training Epoch: 13 [20850/36450]\tLoss: 595.8580\n",
      "Training Epoch: 13 [20900/36450]\tLoss: 581.5445\n",
      "Training Epoch: 13 [20950/36450]\tLoss: 546.2139\n",
      "Training Epoch: 13 [21000/36450]\tLoss: 521.1822\n",
      "Training Epoch: 13 [21050/36450]\tLoss: 534.4777\n",
      "Training Epoch: 13 [21100/36450]\tLoss: 535.5481\n",
      "Training Epoch: 13 [21150/36450]\tLoss: 544.4369\n",
      "Training Epoch: 13 [21200/36450]\tLoss: 533.8411\n",
      "Training Epoch: 13 [21250/36450]\tLoss: 553.3872\n",
      "Training Epoch: 13 [21300/36450]\tLoss: 560.2460\n",
      "Training Epoch: 13 [21350/36450]\tLoss: 520.0715\n",
      "Training Epoch: 13 [21400/36450]\tLoss: 517.6633\n",
      "Training Epoch: 13 [21450/36450]\tLoss: 576.8467\n",
      "Training Epoch: 13 [21500/36450]\tLoss: 536.4649\n",
      "Training Epoch: 13 [21550/36450]\tLoss: 531.9054\n",
      "Training Epoch: 13 [21600/36450]\tLoss: 544.8152\n",
      "Training Epoch: 13 [21650/36450]\tLoss: 567.7813\n",
      "Training Epoch: 13 [21700/36450]\tLoss: 515.2972\n",
      "Training Epoch: 13 [21750/36450]\tLoss: 526.5443\n",
      "Training Epoch: 13 [21800/36450]\tLoss: 531.2258\n",
      "Training Epoch: 13 [21850/36450]\tLoss: 564.8221\n",
      "Training Epoch: 13 [21900/36450]\tLoss: 522.3900\n",
      "Training Epoch: 13 [21950/36450]\tLoss: 524.9197\n",
      "Training Epoch: 13 [22000/36450]\tLoss: 531.3568\n",
      "Training Epoch: 13 [22050/36450]\tLoss: 525.4243\n",
      "Training Epoch: 13 [22100/36450]\tLoss: 550.4478\n",
      "Training Epoch: 13 [22150/36450]\tLoss: 562.1401\n",
      "Training Epoch: 13 [22200/36450]\tLoss: 569.6180\n",
      "Training Epoch: 13 [22250/36450]\tLoss: 539.2562\n",
      "Training Epoch: 13 [22300/36450]\tLoss: 558.1937\n",
      "Training Epoch: 13 [22350/36450]\tLoss: 544.8340\n",
      "Training Epoch: 13 [22400/36450]\tLoss: 523.3735\n",
      "Training Epoch: 13 [22450/36450]\tLoss: 565.6865\n",
      "Training Epoch: 13 [22500/36450]\tLoss: 512.6641\n",
      "Training Epoch: 13 [22550/36450]\tLoss: 529.2291\n",
      "Training Epoch: 13 [22600/36450]\tLoss: 500.7611\n",
      "Training Epoch: 13 [22650/36450]\tLoss: 553.9467\n",
      "Training Epoch: 13 [22700/36450]\tLoss: 556.4899\n",
      "Training Epoch: 13 [22750/36450]\tLoss: 503.9026\n",
      "Training Epoch: 13 [22800/36450]\tLoss: 541.9454\n",
      "Training Epoch: 13 [22850/36450]\tLoss: 527.0074\n",
      "Training Epoch: 13 [22900/36450]\tLoss: 544.8090\n",
      "Training Epoch: 13 [22950/36450]\tLoss: 543.6461\n",
      "Training Epoch: 13 [23000/36450]\tLoss: 528.9352\n",
      "Training Epoch: 13 [23050/36450]\tLoss: 512.5483\n",
      "Training Epoch: 13 [23100/36450]\tLoss: 511.6958\n",
      "Training Epoch: 13 [23150/36450]\tLoss: 545.5860\n",
      "Training Epoch: 13 [23200/36450]\tLoss: 518.1667\n",
      "Training Epoch: 13 [23250/36450]\tLoss: 528.5032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [23300/36450]\tLoss: 551.6696\n",
      "Training Epoch: 13 [23350/36450]\tLoss: 510.5426\n",
      "Training Epoch: 13 [23400/36450]\tLoss: 531.0359\n",
      "Training Epoch: 13 [23450/36450]\tLoss: 519.7121\n",
      "Training Epoch: 13 [23500/36450]\tLoss: 487.3940\n",
      "Training Epoch: 13 [23550/36450]\tLoss: 543.6980\n",
      "Training Epoch: 13 [23600/36450]\tLoss: 525.3829\n",
      "Training Epoch: 13 [23650/36450]\tLoss: 512.5378\n",
      "Training Epoch: 13 [23700/36450]\tLoss: 520.5662\n",
      "Training Epoch: 13 [23750/36450]\tLoss: 547.2311\n",
      "Training Epoch: 13 [23800/36450]\tLoss: 550.3719\n",
      "Training Epoch: 13 [23850/36450]\tLoss: 517.7648\n",
      "Training Epoch: 13 [23900/36450]\tLoss: 534.5720\n",
      "Training Epoch: 13 [23950/36450]\tLoss: 553.5376\n",
      "Training Epoch: 13 [24000/36450]\tLoss: 529.4943\n",
      "Training Epoch: 13 [24050/36450]\tLoss: 501.5429\n",
      "Training Epoch: 13 [24100/36450]\tLoss: 536.9490\n",
      "Training Epoch: 13 [24150/36450]\tLoss: 515.9868\n",
      "Training Epoch: 13 [24200/36450]\tLoss: 528.5154\n",
      "Training Epoch: 13 [24250/36450]\tLoss: 519.0088\n",
      "Training Epoch: 13 [24300/36450]\tLoss: 502.6270\n",
      "Training Epoch: 13 [24350/36450]\tLoss: 534.7093\n",
      "Training Epoch: 13 [24400/36450]\tLoss: 537.7874\n",
      "Training Epoch: 13 [24450/36450]\tLoss: 545.0871\n",
      "Training Epoch: 13 [24500/36450]\tLoss: 517.1337\n",
      "Training Epoch: 13 [24550/36450]\tLoss: 533.1620\n",
      "Training Epoch: 13 [24600/36450]\tLoss: 510.1023\n",
      "Training Epoch: 13 [24650/36450]\tLoss: 522.4729\n",
      "Training Epoch: 13 [24700/36450]\tLoss: 501.3013\n",
      "Training Epoch: 13 [24750/36450]\tLoss: 520.5209\n",
      "Training Epoch: 13 [24800/36450]\tLoss: 527.9409\n",
      "Training Epoch: 13 [24850/36450]\tLoss: 560.4643\n",
      "Training Epoch: 13 [24900/36450]\tLoss: 577.4105\n",
      "Training Epoch: 13 [24950/36450]\tLoss: 542.9404\n",
      "Training Epoch: 13 [25000/36450]\tLoss: 550.0633\n",
      "Training Epoch: 13 [25050/36450]\tLoss: 532.3827\n",
      "Training Epoch: 13 [25100/36450]\tLoss: 527.7798\n",
      "Training Epoch: 13 [25150/36450]\tLoss: 534.9736\n",
      "Training Epoch: 13 [25200/36450]\tLoss: 559.0815\n",
      "Training Epoch: 13 [25250/36450]\tLoss: 566.6981\n",
      "Training Epoch: 13 [25300/36450]\tLoss: 526.3608\n",
      "Training Epoch: 13 [25350/36450]\tLoss: 525.9567\n",
      "Training Epoch: 13 [25400/36450]\tLoss: 592.9777\n",
      "Training Epoch: 13 [25450/36450]\tLoss: 568.0505\n",
      "Training Epoch: 13 [25500/36450]\tLoss: 587.2579\n",
      "Training Epoch: 13 [25550/36450]\tLoss: 544.6251\n",
      "Training Epoch: 13 [25600/36450]\tLoss: 569.4987\n",
      "Training Epoch: 13 [25650/36450]\tLoss: 536.7245\n",
      "Training Epoch: 13 [25700/36450]\tLoss: 542.1364\n",
      "Training Epoch: 13 [25750/36450]\tLoss: 543.7355\n",
      "Training Epoch: 13 [25800/36450]\tLoss: 585.6529\n",
      "Training Epoch: 13 [25850/36450]\tLoss: 569.6636\n",
      "Training Epoch: 13 [25900/36450]\tLoss: 528.0161\n",
      "Training Epoch: 13 [25950/36450]\tLoss: 586.1265\n",
      "Training Epoch: 13 [26000/36450]\tLoss: 514.7235\n",
      "Training Epoch: 13 [26050/36450]\tLoss: 485.1817\n",
      "Training Epoch: 13 [26100/36450]\tLoss: 543.1061\n",
      "Training Epoch: 13 [26150/36450]\tLoss: 533.0953\n",
      "Training Epoch: 13 [26200/36450]\tLoss: 550.6797\n",
      "Training Epoch: 13 [26250/36450]\tLoss: 544.6093\n",
      "Training Epoch: 13 [26300/36450]\tLoss: 558.9711\n",
      "Training Epoch: 13 [26350/36450]\tLoss: 539.0069\n",
      "Training Epoch: 13 [26400/36450]\tLoss: 538.0642\n",
      "Training Epoch: 13 [26450/36450]\tLoss: 549.0054\n",
      "Training Epoch: 13 [26500/36450]\tLoss: 547.9425\n",
      "Training Epoch: 13 [26550/36450]\tLoss: 542.6307\n",
      "Training Epoch: 13 [26600/36450]\tLoss: 531.7880\n",
      "Training Epoch: 13 [26650/36450]\tLoss: 542.3228\n",
      "Training Epoch: 13 [26700/36450]\tLoss: 539.1796\n",
      "Training Epoch: 13 [26750/36450]\tLoss: 510.6331\n",
      "Training Epoch: 13 [26800/36450]\tLoss: 561.9935\n",
      "Training Epoch: 13 [26850/36450]\tLoss: 542.8632\n",
      "Training Epoch: 13 [26900/36450]\tLoss: 545.6088\n",
      "Training Epoch: 13 [26950/36450]\tLoss: 529.2867\n",
      "Training Epoch: 13 [27000/36450]\tLoss: 579.8262\n",
      "Training Epoch: 13 [27050/36450]\tLoss: 527.1464\n",
      "Training Epoch: 13 [27100/36450]\tLoss: 544.3035\n",
      "Training Epoch: 13 [27150/36450]\tLoss: 556.8109\n",
      "Training Epoch: 13 [27200/36450]\tLoss: 573.7036\n",
      "Training Epoch: 13 [27250/36450]\tLoss: 559.2975\n",
      "Training Epoch: 13 [27300/36450]\tLoss: 544.2953\n",
      "Training Epoch: 13 [27350/36450]\tLoss: 534.1710\n",
      "Training Epoch: 13 [27400/36450]\tLoss: 540.6060\n",
      "Training Epoch: 13 [27450/36450]\tLoss: 500.6241\n",
      "Training Epoch: 13 [27500/36450]\tLoss: 546.5483\n",
      "Training Epoch: 13 [27550/36450]\tLoss: 541.5034\n",
      "Training Epoch: 13 [27600/36450]\tLoss: 564.8989\n",
      "Training Epoch: 13 [27650/36450]\tLoss: 542.2576\n",
      "Training Epoch: 13 [27700/36450]\tLoss: 568.9887\n",
      "Training Epoch: 13 [27750/36450]\tLoss: 527.1807\n",
      "Training Epoch: 13 [27800/36450]\tLoss: 557.5181\n",
      "Training Epoch: 13 [27850/36450]\tLoss: 559.0298\n",
      "Training Epoch: 13 [27900/36450]\tLoss: 507.0590\n",
      "Training Epoch: 13 [27950/36450]\tLoss: 527.9742\n",
      "Training Epoch: 13 [28000/36450]\tLoss: 552.3128\n",
      "Training Epoch: 13 [28050/36450]\tLoss: 551.6048\n",
      "Training Epoch: 13 [28100/36450]\tLoss: 551.2290\n",
      "Training Epoch: 13 [28150/36450]\tLoss: 546.8907\n",
      "Training Epoch: 13 [28200/36450]\tLoss: 573.8337\n",
      "Training Epoch: 13 [28250/36450]\tLoss: 496.1848\n",
      "Training Epoch: 13 [28300/36450]\tLoss: 505.5881\n",
      "Training Epoch: 13 [28350/36450]\tLoss: 529.9891\n",
      "Training Epoch: 13 [28400/36450]\tLoss: 521.8613\n",
      "Training Epoch: 13 [28450/36450]\tLoss: 515.2545\n",
      "Training Epoch: 13 [28500/36450]\tLoss: 557.5679\n",
      "Training Epoch: 13 [28550/36450]\tLoss: 532.5753\n",
      "Training Epoch: 13 [28600/36450]\tLoss: 557.9780\n",
      "Training Epoch: 13 [28650/36450]\tLoss: 559.3892\n",
      "Training Epoch: 13 [28700/36450]\tLoss: 536.0380\n",
      "Training Epoch: 13 [28750/36450]\tLoss: 519.7822\n",
      "Training Epoch: 13 [28800/36450]\tLoss: 530.8196\n",
      "Training Epoch: 13 [28850/36450]\tLoss: 498.5849\n",
      "Training Epoch: 13 [28900/36450]\tLoss: 510.0879\n",
      "Training Epoch: 13 [28950/36450]\tLoss: 532.4620\n",
      "Training Epoch: 13 [29000/36450]\tLoss: 516.4394\n",
      "Training Epoch: 13 [29050/36450]\tLoss: 521.9003\n",
      "Training Epoch: 13 [29100/36450]\tLoss: 532.7538\n",
      "Training Epoch: 13 [29150/36450]\tLoss: 540.6255\n",
      "Training Epoch: 13 [29200/36450]\tLoss: 522.8898\n",
      "Training Epoch: 13 [29250/36450]\tLoss: 562.4080\n",
      "Training Epoch: 13 [29300/36450]\tLoss: 493.5634\n",
      "Training Epoch: 13 [29350/36450]\tLoss: 513.2138\n",
      "Training Epoch: 13 [29400/36450]\tLoss: 512.5233\n",
      "Training Epoch: 13 [29450/36450]\tLoss: 569.1714\n",
      "Training Epoch: 13 [29500/36450]\tLoss: 524.2995\n",
      "Training Epoch: 13 [29550/36450]\tLoss: 534.7520\n",
      "Training Epoch: 13 [29600/36450]\tLoss: 539.9936\n",
      "Training Epoch: 13 [29650/36450]\tLoss: 550.5702\n",
      "Training Epoch: 13 [29700/36450]\tLoss: 528.7590\n",
      "Training Epoch: 13 [29750/36450]\tLoss: 531.4755\n",
      "Training Epoch: 13 [29800/36450]\tLoss: 574.5159\n",
      "Training Epoch: 13 [29850/36450]\tLoss: 518.4455\n",
      "Training Epoch: 13 [29900/36450]\tLoss: 541.2077\n",
      "Training Epoch: 13 [29950/36450]\tLoss: 517.3420\n",
      "Training Epoch: 13 [30000/36450]\tLoss: 546.2731\n",
      "Training Epoch: 13 [30050/36450]\tLoss: 521.7482\n",
      "Training Epoch: 13 [30100/36450]\tLoss: 531.5842\n",
      "Training Epoch: 13 [30150/36450]\tLoss: 561.9758\n",
      "Training Epoch: 13 [30200/36450]\tLoss: 535.2223\n",
      "Training Epoch: 13 [30250/36450]\tLoss: 553.7134\n",
      "Training Epoch: 13 [30300/36450]\tLoss: 547.0089\n",
      "Training Epoch: 13 [30350/36450]\tLoss: 538.8596\n",
      "Training Epoch: 13 [30400/36450]\tLoss: 497.0346\n",
      "Training Epoch: 13 [30450/36450]\tLoss: 563.0459\n",
      "Training Epoch: 13 [30500/36450]\tLoss: 536.7540\n",
      "Training Epoch: 13 [30550/36450]\tLoss: 506.2808\n",
      "Training Epoch: 13 [30600/36450]\tLoss: 513.5012\n",
      "Training Epoch: 13 [30650/36450]\tLoss: 525.6102\n",
      "Training Epoch: 13 [30700/36450]\tLoss: 542.2357\n",
      "Training Epoch: 13 [30750/36450]\tLoss: 511.9968\n",
      "Training Epoch: 13 [30800/36450]\tLoss: 568.3655\n",
      "Training Epoch: 13 [30850/36450]\tLoss: 526.9319\n",
      "Training Epoch: 13 [30900/36450]\tLoss: 553.6529\n",
      "Training Epoch: 13 [30950/36450]\tLoss: 512.0917\n",
      "Training Epoch: 13 [31000/36450]\tLoss: 528.5278\n",
      "Training Epoch: 13 [31050/36450]\tLoss: 511.5532\n",
      "Training Epoch: 13 [31100/36450]\tLoss: 539.7916\n",
      "Training Epoch: 13 [31150/36450]\tLoss: 520.9240\n",
      "Training Epoch: 13 [31200/36450]\tLoss: 507.9452\n",
      "Training Epoch: 13 [31250/36450]\tLoss: 522.8639\n",
      "Training Epoch: 13 [31300/36450]\tLoss: 540.7324\n",
      "Training Epoch: 13 [31350/36450]\tLoss: 540.4318\n",
      "Training Epoch: 13 [31400/36450]\tLoss: 565.7787\n",
      "Training Epoch: 13 [31450/36450]\tLoss: 553.1000\n",
      "Training Epoch: 13 [31500/36450]\tLoss: 532.0076\n",
      "Training Epoch: 13 [31550/36450]\tLoss: 543.0380\n",
      "Training Epoch: 13 [31600/36450]\tLoss: 525.3090\n",
      "Training Epoch: 13 [31650/36450]\tLoss: 541.3222\n",
      "Training Epoch: 13 [31700/36450]\tLoss: 536.3625\n",
      "Training Epoch: 13 [31750/36450]\tLoss: 522.6500\n",
      "Training Epoch: 13 [31800/36450]\tLoss: 564.8657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [31850/36450]\tLoss: 565.7576\n",
      "Training Epoch: 13 [31900/36450]\tLoss: 552.9286\n",
      "Training Epoch: 13 [31950/36450]\tLoss: 532.3809\n",
      "Training Epoch: 13 [32000/36450]\tLoss: 559.5941\n",
      "Training Epoch: 13 [32050/36450]\tLoss: 526.6724\n",
      "Training Epoch: 13 [32100/36450]\tLoss: 510.7787\n",
      "Training Epoch: 13 [32150/36450]\tLoss: 557.8089\n",
      "Training Epoch: 13 [32200/36450]\tLoss: 529.0422\n",
      "Training Epoch: 13 [32250/36450]\tLoss: 535.5134\n",
      "Training Epoch: 13 [32300/36450]\tLoss: 534.7142\n",
      "Training Epoch: 13 [32350/36450]\tLoss: 542.9965\n",
      "Training Epoch: 13 [32400/36450]\tLoss: 559.3411\n",
      "Training Epoch: 13 [32450/36450]\tLoss: 522.9713\n",
      "Training Epoch: 13 [32500/36450]\tLoss: 529.4819\n",
      "Training Epoch: 13 [32550/36450]\tLoss: 516.6203\n",
      "Training Epoch: 13 [32600/36450]\tLoss: 525.7020\n",
      "Training Epoch: 13 [32650/36450]\tLoss: 513.0356\n",
      "Training Epoch: 13 [32700/36450]\tLoss: 528.6451\n",
      "Training Epoch: 13 [32750/36450]\tLoss: 531.8948\n",
      "Training Epoch: 13 [32800/36450]\tLoss: 549.4093\n",
      "Training Epoch: 13 [32850/36450]\tLoss: 506.0033\n",
      "Training Epoch: 13 [32900/36450]\tLoss: 544.4715\n",
      "Training Epoch: 13 [32950/36450]\tLoss: 554.2566\n",
      "Training Epoch: 13 [33000/36450]\tLoss: 538.9451\n",
      "Training Epoch: 13 [33050/36450]\tLoss: 538.1632\n",
      "Training Epoch: 13 [33100/36450]\tLoss: 518.6068\n",
      "Training Epoch: 13 [33150/36450]\tLoss: 552.4851\n",
      "Training Epoch: 13 [33200/36450]\tLoss: 533.5577\n",
      "Training Epoch: 13 [33250/36450]\tLoss: 551.2400\n",
      "Training Epoch: 13 [33300/36450]\tLoss: 515.8621\n",
      "Training Epoch: 13 [33350/36450]\tLoss: 542.8878\n",
      "Training Epoch: 13 [33400/36450]\tLoss: 551.4044\n",
      "Training Epoch: 13 [33450/36450]\tLoss: 517.5892\n",
      "Training Epoch: 13 [33500/36450]\tLoss: 512.0864\n",
      "Training Epoch: 13 [33550/36450]\tLoss: 508.3303\n",
      "Training Epoch: 13 [33600/36450]\tLoss: 556.6385\n",
      "Training Epoch: 13 [33650/36450]\tLoss: 556.4888\n",
      "Training Epoch: 13 [33700/36450]\tLoss: 516.3486\n",
      "Training Epoch: 13 [33750/36450]\tLoss: 570.4039\n",
      "Training Epoch: 13 [33800/36450]\tLoss: 512.8848\n",
      "Training Epoch: 13 [33850/36450]\tLoss: 522.9227\n",
      "Training Epoch: 13 [33900/36450]\tLoss: 562.9686\n",
      "Training Epoch: 13 [33950/36450]\tLoss: 508.2047\n",
      "Training Epoch: 13 [34000/36450]\tLoss: 563.7314\n",
      "Training Epoch: 13 [34050/36450]\tLoss: 525.2794\n",
      "Training Epoch: 13 [34100/36450]\tLoss: 604.8674\n",
      "Training Epoch: 13 [34150/36450]\tLoss: 532.6719\n",
      "Training Epoch: 13 [34200/36450]\tLoss: 551.2351\n",
      "Training Epoch: 13 [34250/36450]\tLoss: 523.6249\n",
      "Training Epoch: 13 [34300/36450]\tLoss: 496.6876\n",
      "Training Epoch: 13 [34350/36450]\tLoss: 540.2336\n",
      "Training Epoch: 13 [34400/36450]\tLoss: 541.0776\n",
      "Training Epoch: 13 [34450/36450]\tLoss: 529.1073\n",
      "Training Epoch: 13 [34500/36450]\tLoss: 509.6186\n",
      "Training Epoch: 13 [34550/36450]\tLoss: 540.2805\n",
      "Training Epoch: 13 [34600/36450]\tLoss: 559.0549\n",
      "Training Epoch: 13 [34650/36450]\tLoss: 519.2614\n",
      "Training Epoch: 13 [34700/36450]\tLoss: 534.6168\n",
      "Training Epoch: 13 [34750/36450]\tLoss: 519.6345\n",
      "Training Epoch: 13 [34800/36450]\tLoss: 558.5674\n",
      "Training Epoch: 13 [34850/36450]\tLoss: 515.7894\n",
      "Training Epoch: 13 [34900/36450]\tLoss: 549.6241\n",
      "Training Epoch: 13 [34950/36450]\tLoss: 526.4502\n",
      "Training Epoch: 13 [35000/36450]\tLoss: 514.7593\n",
      "Training Epoch: 13 [35050/36450]\tLoss: 554.4709\n",
      "Training Epoch: 13 [35100/36450]\tLoss: 540.0910\n",
      "Training Epoch: 13 [35150/36450]\tLoss: 557.5270\n",
      "Training Epoch: 13 [35200/36450]\tLoss: 528.1524\n",
      "Training Epoch: 13 [35250/36450]\tLoss: 556.8976\n",
      "Training Epoch: 13 [35300/36450]\tLoss: 542.5157\n",
      "Training Epoch: 13 [35350/36450]\tLoss: 540.8492\n",
      "Training Epoch: 13 [35400/36450]\tLoss: 541.7118\n",
      "Training Epoch: 13 [35450/36450]\tLoss: 529.9321\n",
      "Training Epoch: 13 [35500/36450]\tLoss: 515.0443\n",
      "Training Epoch: 13 [35550/36450]\tLoss: 568.5323\n",
      "Training Epoch: 13 [35600/36450]\tLoss: 560.9026\n",
      "Training Epoch: 13 [35650/36450]\tLoss: 581.9927\n",
      "Training Epoch: 13 [35700/36450]\tLoss: 541.5901\n",
      "Training Epoch: 13 [35750/36450]\tLoss: 554.0409\n",
      "Training Epoch: 13 [35800/36450]\tLoss: 559.1907\n",
      "Training Epoch: 13 [35850/36450]\tLoss: 550.1924\n",
      "Training Epoch: 13 [35900/36450]\tLoss: 519.4442\n",
      "Training Epoch: 13 [35950/36450]\tLoss: 495.5767\n",
      "Training Epoch: 13 [36000/36450]\tLoss: 543.1944\n",
      "Training Epoch: 13 [36050/36450]\tLoss: 531.9343\n",
      "Training Epoch: 13 [36100/36450]\tLoss: 521.9158\n",
      "Training Epoch: 13 [36150/36450]\tLoss: 545.1637\n",
      "Training Epoch: 13 [36200/36450]\tLoss: 522.8178\n",
      "Training Epoch: 13 [36250/36450]\tLoss: 521.6635\n",
      "Training Epoch: 13 [36300/36450]\tLoss: 526.7797\n",
      "Training Epoch: 13 [36350/36450]\tLoss: 578.2969\n",
      "Training Epoch: 13 [36400/36450]\tLoss: 526.6189\n",
      "Training Epoch: 13 [36450/36450]\tLoss: 518.1428\n",
      "Training Epoch: 13 [4050/4050]\tLoss: 266.5693\n",
      "Training Epoch: 14 [50/36450]\tLoss: 510.0427\n",
      "Training Epoch: 14 [100/36450]\tLoss: 537.0907\n",
      "Training Epoch: 14 [150/36450]\tLoss: 508.2893\n",
      "Training Epoch: 14 [200/36450]\tLoss: 554.9787\n",
      "Training Epoch: 14 [250/36450]\tLoss: 493.6426\n",
      "Training Epoch: 14 [300/36450]\tLoss: 551.1362\n",
      "Training Epoch: 14 [350/36450]\tLoss: 520.8943\n",
      "Training Epoch: 14 [400/36450]\tLoss: 523.0383\n",
      "Training Epoch: 14 [450/36450]\tLoss: 522.4500\n",
      "Training Epoch: 14 [500/36450]\tLoss: 505.4415\n",
      "Training Epoch: 14 [550/36450]\tLoss: 510.0867\n",
      "Training Epoch: 14 [600/36450]\tLoss: 514.1547\n",
      "Training Epoch: 14 [650/36450]\tLoss: 535.0133\n",
      "Training Epoch: 14 [700/36450]\tLoss: 521.5892\n",
      "Training Epoch: 14 [750/36450]\tLoss: 521.3188\n",
      "Training Epoch: 14 [800/36450]\tLoss: 519.0330\n",
      "Training Epoch: 14 [850/36450]\tLoss: 509.7556\n",
      "Training Epoch: 14 [900/36450]\tLoss: 518.0603\n",
      "Training Epoch: 14 [950/36450]\tLoss: 528.4429\n",
      "Training Epoch: 14 [1000/36450]\tLoss: 492.9743\n",
      "Training Epoch: 14 [1050/36450]\tLoss: 521.2729\n",
      "Training Epoch: 14 [1100/36450]\tLoss: 578.2800\n",
      "Training Epoch: 14 [1150/36450]\tLoss: 531.6130\n",
      "Training Epoch: 14 [1200/36450]\tLoss: 568.4206\n",
      "Training Epoch: 14 [1250/36450]\tLoss: 529.1041\n",
      "Training Epoch: 14 [1300/36450]\tLoss: 511.9483\n",
      "Training Epoch: 14 [1350/36450]\tLoss: 519.1473\n",
      "Training Epoch: 14 [1400/36450]\tLoss: 514.5364\n",
      "Training Epoch: 14 [1450/36450]\tLoss: 513.1112\n",
      "Training Epoch: 14 [1500/36450]\tLoss: 518.0667\n",
      "Training Epoch: 14 [1550/36450]\tLoss: 537.1775\n",
      "Training Epoch: 14 [1600/36450]\tLoss: 531.9276\n",
      "Training Epoch: 14 [1650/36450]\tLoss: 540.3820\n",
      "Training Epoch: 14 [1700/36450]\tLoss: 513.2400\n",
      "Training Epoch: 14 [1750/36450]\tLoss: 516.0807\n",
      "Training Epoch: 14 [1800/36450]\tLoss: 535.1019\n",
      "Training Epoch: 14 [1850/36450]\tLoss: 508.9692\n",
      "Training Epoch: 14 [1900/36450]\tLoss: 526.4158\n",
      "Training Epoch: 14 [1950/36450]\tLoss: 530.0452\n",
      "Training Epoch: 14 [2000/36450]\tLoss: 494.5583\n",
      "Training Epoch: 14 [2050/36450]\tLoss: 523.0121\n",
      "Training Epoch: 14 [2100/36450]\tLoss: 548.8188\n",
      "Training Epoch: 14 [2150/36450]\tLoss: 529.8330\n",
      "Training Epoch: 14 [2200/36450]\tLoss: 486.8747\n",
      "Training Epoch: 14 [2250/36450]\tLoss: 529.3854\n",
      "Training Epoch: 14 [2300/36450]\tLoss: 493.8082\n",
      "Training Epoch: 14 [2350/36450]\tLoss: 524.9436\n",
      "Training Epoch: 14 [2400/36450]\tLoss: 545.7037\n",
      "Training Epoch: 14 [2450/36450]\tLoss: 516.5602\n",
      "Training Epoch: 14 [2500/36450]\tLoss: 573.2123\n",
      "Training Epoch: 14 [2550/36450]\tLoss: 551.9075\n",
      "Training Epoch: 14 [2600/36450]\tLoss: 530.8137\n",
      "Training Epoch: 14 [2650/36450]\tLoss: 509.5116\n",
      "Training Epoch: 14 [2700/36450]\tLoss: 527.8779\n",
      "Training Epoch: 14 [2750/36450]\tLoss: 525.8935\n",
      "Training Epoch: 14 [2800/36450]\tLoss: 513.8900\n",
      "Training Epoch: 14 [2850/36450]\tLoss: 564.6426\n",
      "Training Epoch: 14 [2900/36450]\tLoss: 522.5143\n",
      "Training Epoch: 14 [2950/36450]\tLoss: 537.2639\n",
      "Training Epoch: 14 [3000/36450]\tLoss: 531.3951\n",
      "Training Epoch: 14 [3050/36450]\tLoss: 551.3616\n",
      "Training Epoch: 14 [3100/36450]\tLoss: 528.5860\n",
      "Training Epoch: 14 [3150/36450]\tLoss: 505.7162\n",
      "Training Epoch: 14 [3200/36450]\tLoss: 522.7521\n",
      "Training Epoch: 14 [3250/36450]\tLoss: 525.5347\n",
      "Training Epoch: 14 [3300/36450]\tLoss: 504.1413\n",
      "Training Epoch: 14 [3350/36450]\tLoss: 510.1782\n",
      "Training Epoch: 14 [3400/36450]\tLoss: 517.4841\n",
      "Training Epoch: 14 [3450/36450]\tLoss: 530.7377\n",
      "Training Epoch: 14 [3500/36450]\tLoss: 543.6049\n",
      "Training Epoch: 14 [3550/36450]\tLoss: 543.7794\n",
      "Training Epoch: 14 [3600/36450]\tLoss: 538.4882\n",
      "Training Epoch: 14 [3650/36450]\tLoss: 552.8630\n",
      "Training Epoch: 14 [3700/36450]\tLoss: 557.4222\n",
      "Training Epoch: 14 [3750/36450]\tLoss: 514.0964\n",
      "Training Epoch: 14 [3800/36450]\tLoss: 528.5048\n",
      "Training Epoch: 14 [3850/36450]\tLoss: 577.9202\n",
      "Training Epoch: 14 [3900/36450]\tLoss: 544.8015\n",
      "Training Epoch: 14 [3950/36450]\tLoss: 556.8191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [4000/36450]\tLoss: 491.6497\n",
      "Training Epoch: 14 [4050/36450]\tLoss: 546.4357\n",
      "Training Epoch: 14 [4100/36450]\tLoss: 510.6245\n",
      "Training Epoch: 14 [4150/36450]\tLoss: 528.2559\n",
      "Training Epoch: 14 [4200/36450]\tLoss: 523.6031\n",
      "Training Epoch: 14 [4250/36450]\tLoss: 536.9022\n",
      "Training Epoch: 14 [4300/36450]\tLoss: 519.9491\n",
      "Training Epoch: 14 [4350/36450]\tLoss: 504.5720\n",
      "Training Epoch: 14 [4400/36450]\tLoss: 568.9218\n",
      "Training Epoch: 14 [4450/36450]\tLoss: 535.7684\n",
      "Training Epoch: 14 [4500/36450]\tLoss: 533.7114\n",
      "Training Epoch: 14 [4550/36450]\tLoss: 532.6750\n",
      "Training Epoch: 14 [4600/36450]\tLoss: 510.7953\n",
      "Training Epoch: 14 [4650/36450]\tLoss: 552.7019\n",
      "Training Epoch: 14 [4700/36450]\tLoss: 550.2596\n",
      "Training Epoch: 14 [4750/36450]\tLoss: 575.9070\n",
      "Training Epoch: 14 [4800/36450]\tLoss: 515.3604\n",
      "Training Epoch: 14 [4850/36450]\tLoss: 512.3612\n",
      "Training Epoch: 14 [4900/36450]\tLoss: 509.5762\n",
      "Training Epoch: 14 [4950/36450]\tLoss: 533.8665\n",
      "Training Epoch: 14 [5000/36450]\tLoss: 528.7455\n",
      "Training Epoch: 14 [5050/36450]\tLoss: 535.5266\n",
      "Training Epoch: 14 [5100/36450]\tLoss: 554.3712\n",
      "Training Epoch: 14 [5150/36450]\tLoss: 551.6485\n",
      "Training Epoch: 14 [5200/36450]\tLoss: 531.8447\n",
      "Training Epoch: 14 [5250/36450]\tLoss: 535.7722\n",
      "Training Epoch: 14 [5300/36450]\tLoss: 528.4576\n",
      "Training Epoch: 14 [5350/36450]\tLoss: 546.7277\n",
      "Training Epoch: 14 [5400/36450]\tLoss: 519.2086\n",
      "Training Epoch: 14 [5450/36450]\tLoss: 527.8492\n",
      "Training Epoch: 14 [5500/36450]\tLoss: 527.6953\n",
      "Training Epoch: 14 [5550/36450]\tLoss: 544.8538\n",
      "Training Epoch: 14 [5600/36450]\tLoss: 496.9418\n",
      "Training Epoch: 14 [5650/36450]\tLoss: 526.2480\n",
      "Training Epoch: 14 [5700/36450]\tLoss: 518.5338\n",
      "Training Epoch: 14 [5750/36450]\tLoss: 526.0411\n",
      "Training Epoch: 14 [5800/36450]\tLoss: 508.1547\n",
      "Training Epoch: 14 [5850/36450]\tLoss: 530.3640\n",
      "Training Epoch: 14 [5900/36450]\tLoss: 543.6756\n",
      "Training Epoch: 14 [5950/36450]\tLoss: 540.2917\n",
      "Training Epoch: 14 [6000/36450]\tLoss: 559.3922\n",
      "Training Epoch: 14 [6050/36450]\tLoss: 561.3254\n",
      "Training Epoch: 14 [6100/36450]\tLoss: 545.8637\n",
      "Training Epoch: 14 [6150/36450]\tLoss: 533.6692\n",
      "Training Epoch: 14 [6200/36450]\tLoss: 577.3639\n",
      "Training Epoch: 14 [6250/36450]\tLoss: 547.3710\n",
      "Training Epoch: 14 [6300/36450]\tLoss: 528.2044\n",
      "Training Epoch: 14 [6350/36450]\tLoss: 519.1437\n",
      "Training Epoch: 14 [6400/36450]\tLoss: 529.7945\n",
      "Training Epoch: 14 [6450/36450]\tLoss: 563.0687\n",
      "Training Epoch: 14 [6500/36450]\tLoss: 520.5220\n",
      "Training Epoch: 14 [6550/36450]\tLoss: 531.8630\n",
      "Training Epoch: 14 [6600/36450]\tLoss: 528.7899\n",
      "Training Epoch: 14 [6650/36450]\tLoss: 528.3541\n",
      "Training Epoch: 14 [6700/36450]\tLoss: 537.2642\n",
      "Training Epoch: 14 [6750/36450]\tLoss: 543.2620\n",
      "Training Epoch: 14 [6800/36450]\tLoss: 538.2935\n",
      "Training Epoch: 14 [6850/36450]\tLoss: 521.9641\n",
      "Training Epoch: 14 [6900/36450]\tLoss: 586.7917\n",
      "Training Epoch: 14 [6950/36450]\tLoss: 484.9467\n",
      "Training Epoch: 14 [7000/36450]\tLoss: 494.9704\n",
      "Training Epoch: 14 [7050/36450]\tLoss: 537.1027\n",
      "Training Epoch: 14 [7100/36450]\tLoss: 549.1425\n",
      "Training Epoch: 14 [7150/36450]\tLoss: 565.2816\n",
      "Training Epoch: 14 [7200/36450]\tLoss: 539.4949\n",
      "Training Epoch: 14 [7250/36450]\tLoss: 498.9621\n",
      "Training Epoch: 14 [7300/36450]\tLoss: 545.9211\n",
      "Training Epoch: 14 [7350/36450]\tLoss: 508.2820\n",
      "Training Epoch: 14 [7400/36450]\tLoss: 524.8928\n",
      "Training Epoch: 14 [7450/36450]\tLoss: 526.7448\n",
      "Training Epoch: 14 [7500/36450]\tLoss: 525.2469\n",
      "Training Epoch: 14 [7550/36450]\tLoss: 528.2874\n",
      "Training Epoch: 14 [7600/36450]\tLoss: 538.3807\n",
      "Training Epoch: 14 [7650/36450]\tLoss: 556.5807\n",
      "Training Epoch: 14 [7700/36450]\tLoss: 529.5029\n",
      "Training Epoch: 14 [7750/36450]\tLoss: 490.5823\n",
      "Training Epoch: 14 [7800/36450]\tLoss: 549.4211\n",
      "Training Epoch: 14 [7850/36450]\tLoss: 553.4902\n",
      "Training Epoch: 14 [7900/36450]\tLoss: 531.8326\n",
      "Training Epoch: 14 [7950/36450]\tLoss: 578.1599\n",
      "Training Epoch: 14 [8000/36450]\tLoss: 521.8902\n",
      "Training Epoch: 14 [8050/36450]\tLoss: 522.1859\n",
      "Training Epoch: 14 [8100/36450]\tLoss: 522.3078\n",
      "Training Epoch: 14 [8150/36450]\tLoss: 523.1780\n",
      "Training Epoch: 14 [8200/36450]\tLoss: 547.7778\n",
      "Training Epoch: 14 [8250/36450]\tLoss: 501.4452\n",
      "Training Epoch: 14 [8300/36450]\tLoss: 542.3543\n",
      "Training Epoch: 14 [8350/36450]\tLoss: 508.4326\n",
      "Training Epoch: 14 [8400/36450]\tLoss: 531.3765\n",
      "Training Epoch: 14 [8450/36450]\tLoss: 532.0268\n",
      "Training Epoch: 14 [8500/36450]\tLoss: 547.2017\n",
      "Training Epoch: 14 [8550/36450]\tLoss: 509.2968\n",
      "Training Epoch: 14 [8600/36450]\tLoss: 528.3295\n",
      "Training Epoch: 14 [8650/36450]\tLoss: 493.7294\n",
      "Training Epoch: 14 [8700/36450]\tLoss: 543.9913\n",
      "Training Epoch: 14 [8750/36450]\tLoss: 521.8341\n",
      "Training Epoch: 14 [8800/36450]\tLoss: 503.2130\n",
      "Training Epoch: 14 [8850/36450]\tLoss: 539.5469\n",
      "Training Epoch: 14 [8900/36450]\tLoss: 512.8267\n",
      "Training Epoch: 14 [8950/36450]\tLoss: 497.7149\n",
      "Training Epoch: 14 [9000/36450]\tLoss: 512.4045\n",
      "Training Epoch: 14 [9050/36450]\tLoss: 533.9452\n",
      "Training Epoch: 14 [9100/36450]\tLoss: 520.8904\n",
      "Training Epoch: 14 [9150/36450]\tLoss: 517.6486\n",
      "Training Epoch: 14 [9200/36450]\tLoss: 517.8227\n",
      "Training Epoch: 14 [9250/36450]\tLoss: 476.4589\n",
      "Training Epoch: 14 [9300/36450]\tLoss: 518.1131\n",
      "Training Epoch: 14 [9350/36450]\tLoss: 521.8765\n",
      "Training Epoch: 14 [9400/36450]\tLoss: 541.3982\n",
      "Training Epoch: 14 [9450/36450]\tLoss: 557.8968\n",
      "Training Epoch: 14 [9500/36450]\tLoss: 534.4623\n",
      "Training Epoch: 14 [9550/36450]\tLoss: 509.0370\n",
      "Training Epoch: 14 [9600/36450]\tLoss: 488.4362\n",
      "Training Epoch: 14 [9650/36450]\tLoss: 504.3790\n",
      "Training Epoch: 14 [9700/36450]\tLoss: 536.2410\n",
      "Training Epoch: 14 [9750/36450]\tLoss: 544.3618\n",
      "Training Epoch: 14 [9800/36450]\tLoss: 518.6085\n",
      "Training Epoch: 14 [9850/36450]\tLoss: 516.9523\n",
      "Training Epoch: 14 [9900/36450]\tLoss: 541.2216\n",
      "Training Epoch: 14 [9950/36450]\tLoss: 538.9888\n",
      "Training Epoch: 14 [10000/36450]\tLoss: 534.3427\n",
      "Training Epoch: 14 [10050/36450]\tLoss: 493.7477\n",
      "Training Epoch: 14 [10100/36450]\tLoss: 532.2999\n",
      "Training Epoch: 14 [10150/36450]\tLoss: 531.3088\n",
      "Training Epoch: 14 [10200/36450]\tLoss: 516.4138\n",
      "Training Epoch: 14 [10250/36450]\tLoss: 552.2104\n",
      "Training Epoch: 14 [10300/36450]\tLoss: 539.3776\n",
      "Training Epoch: 14 [10350/36450]\tLoss: 547.2018\n",
      "Training Epoch: 14 [10400/36450]\tLoss: 538.2925\n",
      "Training Epoch: 14 [10450/36450]\tLoss: 586.9189\n",
      "Training Epoch: 14 [10500/36450]\tLoss: 523.9352\n",
      "Training Epoch: 14 [10550/36450]\tLoss: 536.9583\n",
      "Training Epoch: 14 [10600/36450]\tLoss: 505.1413\n",
      "Training Epoch: 14 [10650/36450]\tLoss: 510.9177\n",
      "Training Epoch: 14 [10700/36450]\tLoss: 541.0218\n",
      "Training Epoch: 14 [10750/36450]\tLoss: 509.2163\n",
      "Training Epoch: 14 [10800/36450]\tLoss: 526.3868\n",
      "Training Epoch: 14 [10850/36450]\tLoss: 529.8984\n",
      "Training Epoch: 14 [10900/36450]\tLoss: 504.3757\n",
      "Training Epoch: 14 [10950/36450]\tLoss: 537.1667\n",
      "Training Epoch: 14 [11000/36450]\tLoss: 521.9169\n",
      "Training Epoch: 14 [11050/36450]\tLoss: 519.0364\n",
      "Training Epoch: 14 [11100/36450]\tLoss: 534.6815\n",
      "Training Epoch: 14 [11150/36450]\tLoss: 517.0916\n",
      "Training Epoch: 14 [11200/36450]\tLoss: 542.1801\n",
      "Training Epoch: 14 [11250/36450]\tLoss: 524.1526\n",
      "Training Epoch: 14 [11300/36450]\tLoss: 542.3965\n",
      "Training Epoch: 14 [11350/36450]\tLoss: 526.7581\n",
      "Training Epoch: 14 [11400/36450]\tLoss: 491.2933\n",
      "Training Epoch: 14 [11450/36450]\tLoss: 514.4948\n",
      "Training Epoch: 14 [11500/36450]\tLoss: 520.8975\n",
      "Training Epoch: 14 [11550/36450]\tLoss: 532.9017\n",
      "Training Epoch: 14 [11600/36450]\tLoss: 569.0917\n",
      "Training Epoch: 14 [11650/36450]\tLoss: 508.8065\n",
      "Training Epoch: 14 [11700/36450]\tLoss: 538.1582\n",
      "Training Epoch: 14 [11750/36450]\tLoss: 529.2321\n",
      "Training Epoch: 14 [11800/36450]\tLoss: 542.5092\n",
      "Training Epoch: 14 [11850/36450]\tLoss: 502.0366\n",
      "Training Epoch: 14 [11900/36450]\tLoss: 495.7018\n",
      "Training Epoch: 14 [11950/36450]\tLoss: 504.7430\n",
      "Training Epoch: 14 [12000/36450]\tLoss: 569.4518\n",
      "Training Epoch: 14 [12050/36450]\tLoss: 544.4689\n",
      "Training Epoch: 14 [12100/36450]\tLoss: 525.4251\n",
      "Training Epoch: 14 [12150/36450]\tLoss: 524.5663\n",
      "Training Epoch: 14 [12200/36450]\tLoss: 532.1359\n",
      "Training Epoch: 14 [12250/36450]\tLoss: 517.4403\n",
      "Training Epoch: 14 [12300/36450]\tLoss: 533.2641\n",
      "Training Epoch: 14 [12350/36450]\tLoss: 566.5479\n",
      "Training Epoch: 14 [12400/36450]\tLoss: 549.3884\n",
      "Training Epoch: 14 [12450/36450]\tLoss: 513.7103\n",
      "Training Epoch: 14 [12500/36450]\tLoss: 540.3542\n",
      "Training Epoch: 14 [12550/36450]\tLoss: 558.5552\n",
      "Training Epoch: 14 [12600/36450]\tLoss: 549.0676\n",
      "Training Epoch: 14 [12650/36450]\tLoss: 500.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [12700/36450]\tLoss: 558.0181\n",
      "Training Epoch: 14 [12750/36450]\tLoss: 532.5330\n",
      "Training Epoch: 14 [12800/36450]\tLoss: 521.8709\n",
      "Training Epoch: 14 [12850/36450]\tLoss: 530.4819\n",
      "Training Epoch: 14 [12900/36450]\tLoss: 560.3316\n",
      "Training Epoch: 14 [12950/36450]\tLoss: 534.3804\n",
      "Training Epoch: 14 [13000/36450]\tLoss: 533.2413\n",
      "Training Epoch: 14 [13050/36450]\tLoss: 563.6924\n",
      "Training Epoch: 14 [13100/36450]\tLoss: 552.2767\n",
      "Training Epoch: 14 [13150/36450]\tLoss: 526.3922\n",
      "Training Epoch: 14 [13200/36450]\tLoss: 554.8089\n",
      "Training Epoch: 14 [13250/36450]\tLoss: 563.0927\n",
      "Training Epoch: 14 [13300/36450]\tLoss: 559.7541\n",
      "Training Epoch: 14 [13350/36450]\tLoss: 550.4127\n",
      "Training Epoch: 14 [13400/36450]\tLoss: 530.2018\n",
      "Training Epoch: 14 [13450/36450]\tLoss: 531.3476\n",
      "Training Epoch: 14 [13500/36450]\tLoss: 524.0162\n",
      "Training Epoch: 14 [13550/36450]\tLoss: 514.4355\n",
      "Training Epoch: 14 [13600/36450]\tLoss: 541.4443\n",
      "Training Epoch: 14 [13650/36450]\tLoss: 524.5143\n",
      "Training Epoch: 14 [13700/36450]\tLoss: 514.6176\n",
      "Training Epoch: 14 [13750/36450]\tLoss: 559.3365\n",
      "Training Epoch: 14 [13800/36450]\tLoss: 507.6919\n",
      "Training Epoch: 14 [13850/36450]\tLoss: 538.9077\n",
      "Training Epoch: 14 [13900/36450]\tLoss: 530.8242\n",
      "Training Epoch: 14 [13950/36450]\tLoss: 522.6349\n",
      "Training Epoch: 14 [14000/36450]\tLoss: 507.6943\n",
      "Training Epoch: 14 [14050/36450]\tLoss: 517.8456\n",
      "Training Epoch: 14 [14100/36450]\tLoss: 546.5972\n",
      "Training Epoch: 14 [14150/36450]\tLoss: 482.3422\n",
      "Training Epoch: 14 [14200/36450]\tLoss: 510.0980\n",
      "Training Epoch: 14 [14250/36450]\tLoss: 526.5413\n",
      "Training Epoch: 14 [14300/36450]\tLoss: 513.4998\n",
      "Training Epoch: 14 [14350/36450]\tLoss: 530.3796\n",
      "Training Epoch: 14 [14400/36450]\tLoss: 537.9663\n",
      "Training Epoch: 14 [14450/36450]\tLoss: 532.1506\n",
      "Training Epoch: 14 [14500/36450]\tLoss: 515.8027\n",
      "Training Epoch: 14 [14550/36450]\tLoss: 541.9070\n",
      "Training Epoch: 14 [14600/36450]\tLoss: 532.2446\n",
      "Training Epoch: 14 [14650/36450]\tLoss: 506.3840\n",
      "Training Epoch: 14 [14700/36450]\tLoss: 484.7523\n",
      "Training Epoch: 14 [14750/36450]\tLoss: 499.0233\n",
      "Training Epoch: 14 [14800/36450]\tLoss: 535.6654\n",
      "Training Epoch: 14 [14850/36450]\tLoss: 528.0692\n",
      "Training Epoch: 14 [14900/36450]\tLoss: 521.7109\n",
      "Training Epoch: 14 [14950/36450]\tLoss: 516.1105\n",
      "Training Epoch: 14 [15000/36450]\tLoss: 548.1609\n",
      "Training Epoch: 14 [15050/36450]\tLoss: 555.5413\n",
      "Training Epoch: 14 [15100/36450]\tLoss: 515.4266\n",
      "Training Epoch: 14 [15150/36450]\tLoss: 548.3452\n",
      "Training Epoch: 14 [15200/36450]\tLoss: 538.1520\n",
      "Training Epoch: 14 [15250/36450]\tLoss: 534.3818\n",
      "Training Epoch: 14 [15300/36450]\tLoss: 501.5504\n",
      "Training Epoch: 14 [15350/36450]\tLoss: 547.3351\n",
      "Training Epoch: 14 [15400/36450]\tLoss: 531.0232\n",
      "Training Epoch: 14 [15450/36450]\tLoss: 538.0240\n",
      "Training Epoch: 14 [15500/36450]\tLoss: 548.7950\n",
      "Training Epoch: 14 [15550/36450]\tLoss: 508.1125\n",
      "Training Epoch: 14 [15600/36450]\tLoss: 538.8638\n",
      "Training Epoch: 14 [15650/36450]\tLoss: 496.2825\n",
      "Training Epoch: 14 [15700/36450]\tLoss: 518.5347\n",
      "Training Epoch: 14 [15750/36450]\tLoss: 538.3267\n",
      "Training Epoch: 14 [15800/36450]\tLoss: 536.2449\n",
      "Training Epoch: 14 [15850/36450]\tLoss: 549.9127\n",
      "Training Epoch: 14 [15900/36450]\tLoss: 526.2429\n",
      "Training Epoch: 14 [15950/36450]\tLoss: 551.6791\n",
      "Training Epoch: 14 [16000/36450]\tLoss: 521.1943\n",
      "Training Epoch: 14 [16050/36450]\tLoss: 539.3983\n",
      "Training Epoch: 14 [16100/36450]\tLoss: 542.7227\n",
      "Training Epoch: 14 [16150/36450]\tLoss: 511.8895\n",
      "Training Epoch: 14 [16200/36450]\tLoss: 557.1335\n",
      "Training Epoch: 14 [16250/36450]\tLoss: 492.2594\n",
      "Training Epoch: 14 [16300/36450]\tLoss: 567.2399\n",
      "Training Epoch: 14 [16350/36450]\tLoss: 510.4708\n",
      "Training Epoch: 14 [16400/36450]\tLoss: 503.5644\n",
      "Training Epoch: 14 [16450/36450]\tLoss: 533.8975\n",
      "Training Epoch: 14 [16500/36450]\tLoss: 526.7918\n",
      "Training Epoch: 14 [16550/36450]\tLoss: 497.6948\n",
      "Training Epoch: 14 [16600/36450]\tLoss: 525.4398\n",
      "Training Epoch: 14 [16650/36450]\tLoss: 519.7193\n",
      "Training Epoch: 14 [16700/36450]\tLoss: 501.6004\n",
      "Training Epoch: 14 [16750/36450]\tLoss: 555.4603\n",
      "Training Epoch: 14 [16800/36450]\tLoss: 549.5923\n",
      "Training Epoch: 14 [16850/36450]\tLoss: 513.1415\n",
      "Training Epoch: 14 [16900/36450]\tLoss: 520.8882\n",
      "Training Epoch: 14 [16950/36450]\tLoss: 519.0043\n",
      "Training Epoch: 14 [17000/36450]\tLoss: 527.7535\n",
      "Training Epoch: 14 [17050/36450]\tLoss: 528.1863\n",
      "Training Epoch: 14 [17100/36450]\tLoss: 500.3777\n",
      "Training Epoch: 14 [17150/36450]\tLoss: 471.4085\n",
      "Training Epoch: 14 [17200/36450]\tLoss: 532.9226\n",
      "Training Epoch: 14 [17250/36450]\tLoss: 508.3805\n",
      "Training Epoch: 14 [17300/36450]\tLoss: 523.7895\n",
      "Training Epoch: 14 [17350/36450]\tLoss: 536.7244\n",
      "Training Epoch: 14 [17400/36450]\tLoss: 524.2007\n",
      "Training Epoch: 14 [17450/36450]\tLoss: 504.7185\n",
      "Training Epoch: 14 [17500/36450]\tLoss: 541.0154\n",
      "Training Epoch: 14 [17550/36450]\tLoss: 529.7394\n",
      "Training Epoch: 14 [17600/36450]\tLoss: 537.3302\n",
      "Training Epoch: 14 [17650/36450]\tLoss: 520.9838\n",
      "Training Epoch: 14 [17700/36450]\tLoss: 507.2208\n",
      "Training Epoch: 14 [17750/36450]\tLoss: 530.6573\n",
      "Training Epoch: 14 [17800/36450]\tLoss: 507.9782\n",
      "Training Epoch: 14 [17850/36450]\tLoss: 512.9458\n",
      "Training Epoch: 14 [17900/36450]\tLoss: 548.8918\n",
      "Training Epoch: 14 [17950/36450]\tLoss: 518.1115\n",
      "Training Epoch: 14 [18000/36450]\tLoss: 520.0496\n",
      "Training Epoch: 14 [18050/36450]\tLoss: 563.5085\n",
      "Training Epoch: 14 [18100/36450]\tLoss: 525.8328\n",
      "Training Epoch: 14 [18150/36450]\tLoss: 487.7928\n",
      "Training Epoch: 14 [18200/36450]\tLoss: 526.8683\n",
      "Training Epoch: 14 [18250/36450]\tLoss: 518.7827\n",
      "Training Epoch: 14 [18300/36450]\tLoss: 528.4347\n",
      "Training Epoch: 14 [18350/36450]\tLoss: 510.4201\n",
      "Training Epoch: 14 [18400/36450]\tLoss: 517.5926\n",
      "Training Epoch: 14 [18450/36450]\tLoss: 513.7529\n",
      "Training Epoch: 14 [18500/36450]\tLoss: 531.7158\n",
      "Training Epoch: 14 [18550/36450]\tLoss: 511.0858\n",
      "Training Epoch: 14 [18600/36450]\tLoss: 539.7198\n",
      "Training Epoch: 14 [18650/36450]\tLoss: 555.0539\n",
      "Training Epoch: 14 [18700/36450]\tLoss: 541.9680\n",
      "Training Epoch: 14 [18750/36450]\tLoss: 512.3879\n",
      "Training Epoch: 14 [18800/36450]\tLoss: 525.0209\n",
      "Training Epoch: 14 [18850/36450]\tLoss: 523.7767\n",
      "Training Epoch: 14 [18900/36450]\tLoss: 536.3217\n",
      "Training Epoch: 14 [18950/36450]\tLoss: 557.1874\n",
      "Training Epoch: 14 [19000/36450]\tLoss: 545.9077\n",
      "Training Epoch: 14 [19050/36450]\tLoss: 552.4568\n",
      "Training Epoch: 14 [19100/36450]\tLoss: 548.3298\n",
      "Training Epoch: 14 [19150/36450]\tLoss: 537.0903\n",
      "Training Epoch: 14 [19200/36450]\tLoss: 567.2744\n",
      "Training Epoch: 14 [19250/36450]\tLoss: 573.0259\n",
      "Training Epoch: 14 [19300/36450]\tLoss: 588.3618\n",
      "Training Epoch: 14 [19350/36450]\tLoss: 629.5069\n",
      "Training Epoch: 14 [19400/36450]\tLoss: 678.3381\n",
      "Training Epoch: 14 [19450/36450]\tLoss: 712.1357\n",
      "Training Epoch: 14 [19500/36450]\tLoss: 667.5226\n",
      "Training Epoch: 14 [19550/36450]\tLoss: 623.8397\n",
      "Training Epoch: 14 [19600/36450]\tLoss: 550.2877\n",
      "Training Epoch: 14 [19650/36450]\tLoss: 509.9570\n",
      "Training Epoch: 14 [19700/36450]\tLoss: 558.9882\n",
      "Training Epoch: 14 [19750/36450]\tLoss: 579.7869\n",
      "Training Epoch: 14 [19800/36450]\tLoss: 593.5458\n",
      "Training Epoch: 14 [19850/36450]\tLoss: 555.3402\n",
      "Training Epoch: 14 [19900/36450]\tLoss: 520.5281\n",
      "Training Epoch: 14 [19950/36450]\tLoss: 540.5160\n",
      "Training Epoch: 14 [20000/36450]\tLoss: 592.5021\n",
      "Training Epoch: 14 [20050/36450]\tLoss: 612.2598\n",
      "Training Epoch: 14 [20100/36450]\tLoss: 572.8158\n",
      "Training Epoch: 14 [20150/36450]\tLoss: 529.1675\n",
      "Training Epoch: 14 [20200/36450]\tLoss: 526.7006\n",
      "Training Epoch: 14 [20250/36450]\tLoss: 547.7580\n",
      "Training Epoch: 14 [20300/36450]\tLoss: 590.4904\n",
      "Training Epoch: 14 [20350/36450]\tLoss: 544.4376\n",
      "Training Epoch: 14 [20400/36450]\tLoss: 525.5777\n",
      "Training Epoch: 14 [20450/36450]\tLoss: 521.0757\n",
      "Training Epoch: 14 [20500/36450]\tLoss: 541.3589\n",
      "Training Epoch: 14 [20550/36450]\tLoss: 578.1575\n",
      "Training Epoch: 14 [20600/36450]\tLoss: 532.9710\n",
      "Training Epoch: 14 [20650/36450]\tLoss: 515.8385\n",
      "Training Epoch: 14 [20700/36450]\tLoss: 556.8070\n",
      "Training Epoch: 14 [20750/36450]\tLoss: 544.8596\n",
      "Training Epoch: 14 [20800/36450]\tLoss: 526.2548\n",
      "Training Epoch: 14 [20850/36450]\tLoss: 559.2724\n",
      "Training Epoch: 14 [20900/36450]\tLoss: 553.1471\n",
      "Training Epoch: 14 [20950/36450]\tLoss: 537.7296\n",
      "Training Epoch: 14 [21000/36450]\tLoss: 565.6921\n",
      "Training Epoch: 14 [21050/36450]\tLoss: 531.0731\n",
      "Training Epoch: 14 [21100/36450]\tLoss: 502.2033\n",
      "Training Epoch: 14 [21150/36450]\tLoss: 530.7090\n",
      "Training Epoch: 14 [21200/36450]\tLoss: 512.1262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [21250/36450]\tLoss: 495.0263\n",
      "Training Epoch: 14 [21300/36450]\tLoss: 494.8512\n",
      "Training Epoch: 14 [21350/36450]\tLoss: 523.9227\n",
      "Training Epoch: 14 [21400/36450]\tLoss: 521.4186\n",
      "Training Epoch: 14 [21450/36450]\tLoss: 527.5905\n",
      "Training Epoch: 14 [21500/36450]\tLoss: 551.6862\n",
      "Training Epoch: 14 [21550/36450]\tLoss: 523.1962\n",
      "Training Epoch: 14 [21600/36450]\tLoss: 541.3366\n",
      "Training Epoch: 14 [21650/36450]\tLoss: 533.1211\n",
      "Training Epoch: 14 [21700/36450]\tLoss: 498.3185\n",
      "Training Epoch: 14 [21750/36450]\tLoss: 516.3453\n",
      "Training Epoch: 14 [21800/36450]\tLoss: 513.6586\n",
      "Training Epoch: 14 [21850/36450]\tLoss: 519.4789\n",
      "Training Epoch: 14 [21900/36450]\tLoss: 542.7304\n",
      "Training Epoch: 14 [21950/36450]\tLoss: 525.2590\n",
      "Training Epoch: 14 [22000/36450]\tLoss: 510.0602\n",
      "Training Epoch: 14 [22050/36450]\tLoss: 503.0957\n",
      "Training Epoch: 14 [22100/36450]\tLoss: 503.4937\n",
      "Training Epoch: 14 [22150/36450]\tLoss: 545.0825\n",
      "Training Epoch: 14 [22200/36450]\tLoss: 541.8566\n",
      "Training Epoch: 14 [22250/36450]\tLoss: 513.3647\n",
      "Training Epoch: 14 [22300/36450]\tLoss: 502.3606\n",
      "Training Epoch: 14 [22350/36450]\tLoss: 493.5457\n",
      "Training Epoch: 14 [22400/36450]\tLoss: 521.4001\n",
      "Training Epoch: 14 [22450/36450]\tLoss: 528.6132\n",
      "Training Epoch: 14 [22500/36450]\tLoss: 514.5122\n",
      "Training Epoch: 14 [22550/36450]\tLoss: 544.9628\n",
      "Training Epoch: 14 [22600/36450]\tLoss: 505.3882\n",
      "Training Epoch: 14 [22650/36450]\tLoss: 543.4341\n",
      "Training Epoch: 14 [22700/36450]\tLoss: 521.1513\n",
      "Training Epoch: 14 [22750/36450]\tLoss: 543.6642\n",
      "Training Epoch: 14 [22800/36450]\tLoss: 516.2983\n",
      "Training Epoch: 14 [22850/36450]\tLoss: 472.4595\n",
      "Training Epoch: 14 [22900/36450]\tLoss: 513.3592\n",
      "Training Epoch: 14 [22950/36450]\tLoss: 513.9372\n",
      "Training Epoch: 14 [23000/36450]\tLoss: 540.6420\n",
      "Training Epoch: 14 [23050/36450]\tLoss: 527.0234\n",
      "Training Epoch: 14 [23100/36450]\tLoss: 528.8080\n",
      "Training Epoch: 14 [23150/36450]\tLoss: 530.6038\n",
      "Training Epoch: 14 [23200/36450]\tLoss: 555.0147\n",
      "Training Epoch: 14 [23250/36450]\tLoss: 516.4560\n",
      "Training Epoch: 14 [23300/36450]\tLoss: 490.5579\n",
      "Training Epoch: 14 [23350/36450]\tLoss: 514.5811\n",
      "Training Epoch: 14 [23400/36450]\tLoss: 509.2061\n",
      "Training Epoch: 14 [23450/36450]\tLoss: 507.2139\n",
      "Training Epoch: 14 [23500/36450]\tLoss: 537.1281\n",
      "Training Epoch: 14 [23550/36450]\tLoss: 528.2936\n",
      "Training Epoch: 14 [23600/36450]\tLoss: 525.8751\n",
      "Training Epoch: 14 [23650/36450]\tLoss: 502.7706\n",
      "Training Epoch: 14 [23700/36450]\tLoss: 539.4287\n",
      "Training Epoch: 14 [23750/36450]\tLoss: 526.5304\n",
      "Training Epoch: 14 [23800/36450]\tLoss: 550.5883\n",
      "Training Epoch: 14 [23850/36450]\tLoss: 532.7201\n",
      "Training Epoch: 14 [23900/36450]\tLoss: 537.5586\n",
      "Training Epoch: 14 [23950/36450]\tLoss: 509.9626\n",
      "Training Epoch: 14 [24000/36450]\tLoss: 539.7720\n",
      "Training Epoch: 14 [24050/36450]\tLoss: 570.2872\n",
      "Training Epoch: 14 [24100/36450]\tLoss: 519.9742\n",
      "Training Epoch: 14 [24150/36450]\tLoss: 545.6129\n",
      "Training Epoch: 14 [24200/36450]\tLoss: 519.9866\n",
      "Training Epoch: 14 [24250/36450]\tLoss: 537.4877\n",
      "Training Epoch: 14 [24300/36450]\tLoss: 523.8429\n",
      "Training Epoch: 14 [24350/36450]\tLoss: 503.4902\n",
      "Training Epoch: 14 [24400/36450]\tLoss: 525.1067\n",
      "Training Epoch: 14 [24450/36450]\tLoss: 521.7142\n",
      "Training Epoch: 14 [24500/36450]\tLoss: 554.2693\n",
      "Training Epoch: 14 [24550/36450]\tLoss: 532.4204\n",
      "Training Epoch: 14 [24600/36450]\tLoss: 489.7608\n",
      "Training Epoch: 14 [24650/36450]\tLoss: 525.4150\n",
      "Training Epoch: 14 [24700/36450]\tLoss: 475.6061\n",
      "Training Epoch: 14 [24750/36450]\tLoss: 519.4854\n",
      "Training Epoch: 14 [24800/36450]\tLoss: 520.8533\n",
      "Training Epoch: 14 [24850/36450]\tLoss: 500.3352\n",
      "Training Epoch: 14 [24900/36450]\tLoss: 520.8200\n",
      "Training Epoch: 14 [24950/36450]\tLoss: 516.9577\n",
      "Training Epoch: 14 [25000/36450]\tLoss: 511.6312\n",
      "Training Epoch: 14 [25050/36450]\tLoss: 502.7099\n",
      "Training Epoch: 14 [25100/36450]\tLoss: 534.8922\n",
      "Training Epoch: 14 [25150/36450]\tLoss: 494.2540\n",
      "Training Epoch: 14 [25200/36450]\tLoss: 521.4811\n",
      "Training Epoch: 14 [25250/36450]\tLoss: 514.7396\n",
      "Training Epoch: 14 [25300/36450]\tLoss: 542.4416\n",
      "Training Epoch: 14 [25350/36450]\tLoss: 536.7850\n",
      "Training Epoch: 14 [25400/36450]\tLoss: 499.9418\n",
      "Training Epoch: 14 [25450/36450]\tLoss: 509.3697\n",
      "Training Epoch: 14 [25500/36450]\tLoss: 534.7081\n",
      "Training Epoch: 14 [25550/36450]\tLoss: 537.6976\n",
      "Training Epoch: 14 [25600/36450]\tLoss: 517.4565\n",
      "Training Epoch: 14 [25650/36450]\tLoss: 524.7311\n",
      "Training Epoch: 14 [25700/36450]\tLoss: 586.3408\n",
      "Training Epoch: 14 [25750/36450]\tLoss: 522.4798\n",
      "Training Epoch: 14 [25800/36450]\tLoss: 533.2597\n",
      "Training Epoch: 14 [25850/36450]\tLoss: 510.8771\n",
      "Training Epoch: 14 [25900/36450]\tLoss: 513.1093\n",
      "Training Epoch: 14 [25950/36450]\tLoss: 509.7115\n",
      "Training Epoch: 14 [26000/36450]\tLoss: 527.2249\n",
      "Training Epoch: 14 [26050/36450]\tLoss: 509.3284\n",
      "Training Epoch: 14 [26100/36450]\tLoss: 555.8908\n",
      "Training Epoch: 14 [26150/36450]\tLoss: 535.7435\n",
      "Training Epoch: 14 [26200/36450]\tLoss: 537.8312\n",
      "Training Epoch: 14 [26250/36450]\tLoss: 510.1346\n",
      "Training Epoch: 14 [26300/36450]\tLoss: 524.2203\n",
      "Training Epoch: 14 [26350/36450]\tLoss: 524.6210\n",
      "Training Epoch: 14 [26400/36450]\tLoss: 511.0746\n",
      "Training Epoch: 14 [26450/36450]\tLoss: 525.5908\n",
      "Training Epoch: 14 [26500/36450]\tLoss: 533.9894\n",
      "Training Epoch: 14 [26550/36450]\tLoss: 543.8674\n",
      "Training Epoch: 14 [26600/36450]\tLoss: 518.0685\n",
      "Training Epoch: 14 [26650/36450]\tLoss: 514.5063\n",
      "Training Epoch: 14 [26700/36450]\tLoss: 555.0453\n",
      "Training Epoch: 14 [26750/36450]\tLoss: 527.3975\n",
      "Training Epoch: 14 [26800/36450]\tLoss: 550.4700\n",
      "Training Epoch: 14 [26850/36450]\tLoss: 524.0867\n",
      "Training Epoch: 14 [26900/36450]\tLoss: 522.3632\n",
      "Training Epoch: 14 [26950/36450]\tLoss: 512.3834\n",
      "Training Epoch: 14 [27000/36450]\tLoss: 508.2304\n",
      "Training Epoch: 14 [27050/36450]\tLoss: 536.2964\n",
      "Training Epoch: 14 [27100/36450]\tLoss: 487.4456\n",
      "Training Epoch: 14 [27150/36450]\tLoss: 544.8940\n",
      "Training Epoch: 14 [27200/36450]\tLoss: 516.9932\n",
      "Training Epoch: 14 [27250/36450]\tLoss: 516.0421\n",
      "Training Epoch: 14 [27300/36450]\tLoss: 489.7151\n",
      "Training Epoch: 14 [27350/36450]\tLoss: 497.2946\n",
      "Training Epoch: 14 [27400/36450]\tLoss: 506.2642\n",
      "Training Epoch: 14 [27450/36450]\tLoss: 513.9615\n",
      "Training Epoch: 14 [27500/36450]\tLoss: 474.1255\n",
      "Training Epoch: 14 [27550/36450]\tLoss: 519.6610\n",
      "Training Epoch: 14 [27600/36450]\tLoss: 495.8040\n",
      "Training Epoch: 14 [27650/36450]\tLoss: 520.6392\n",
      "Training Epoch: 14 [27700/36450]\tLoss: 525.0428\n",
      "Training Epoch: 14 [27750/36450]\tLoss: 512.3469\n",
      "Training Epoch: 14 [27800/36450]\tLoss: 498.1908\n",
      "Training Epoch: 14 [27850/36450]\tLoss: 496.6406\n",
      "Training Epoch: 14 [27900/36450]\tLoss: 495.7827\n",
      "Training Epoch: 14 [27950/36450]\tLoss: 501.5874\n",
      "Training Epoch: 14 [28000/36450]\tLoss: 536.7610\n",
      "Training Epoch: 14 [28050/36450]\tLoss: 541.0156\n",
      "Training Epoch: 14 [28100/36450]\tLoss: 514.2885\n",
      "Training Epoch: 14 [28150/36450]\tLoss: 515.8190\n",
      "Training Epoch: 14 [28200/36450]\tLoss: 535.5182\n",
      "Training Epoch: 14 [28250/36450]\tLoss: 527.8974\n",
      "Training Epoch: 14 [28300/36450]\tLoss: 529.0383\n",
      "Training Epoch: 14 [28350/36450]\tLoss: 498.8563\n",
      "Training Epoch: 14 [28400/36450]\tLoss: 515.6252\n",
      "Training Epoch: 14 [28450/36450]\tLoss: 530.7735\n",
      "Training Epoch: 14 [28500/36450]\tLoss: 503.8863\n",
      "Training Epoch: 14 [28550/36450]\tLoss: 519.6850\n",
      "Training Epoch: 14 [28600/36450]\tLoss: 512.7136\n",
      "Training Epoch: 14 [28650/36450]\tLoss: 510.6551\n",
      "Training Epoch: 14 [28700/36450]\tLoss: 523.7382\n",
      "Training Epoch: 14 [28750/36450]\tLoss: 519.5807\n",
      "Training Epoch: 14 [28800/36450]\tLoss: 502.6293\n",
      "Training Epoch: 14 [28850/36450]\tLoss: 511.9895\n",
      "Training Epoch: 14 [28900/36450]\tLoss: 546.7003\n",
      "Training Epoch: 14 [28950/36450]\tLoss: 528.5776\n",
      "Training Epoch: 14 [29000/36450]\tLoss: 532.7894\n",
      "Training Epoch: 14 [29050/36450]\tLoss: 542.4673\n",
      "Training Epoch: 14 [29100/36450]\tLoss: 541.6451\n",
      "Training Epoch: 14 [29150/36450]\tLoss: 511.9913\n",
      "Training Epoch: 14 [29200/36450]\tLoss: 534.4872\n",
      "Training Epoch: 14 [29250/36450]\tLoss: 536.1320\n",
      "Training Epoch: 14 [29300/36450]\tLoss: 507.5555\n",
      "Training Epoch: 14 [29350/36450]\tLoss: 513.6025\n",
      "Training Epoch: 14 [29400/36450]\tLoss: 470.6858\n",
      "Training Epoch: 14 [29450/36450]\tLoss: 477.7789\n",
      "Training Epoch: 14 [29500/36450]\tLoss: 539.1551\n",
      "Training Epoch: 14 [29550/36450]\tLoss: 515.6298\n",
      "Training Epoch: 14 [29600/36450]\tLoss: 529.1448\n",
      "Training Epoch: 14 [29650/36450]\tLoss: 551.2075\n",
      "Training Epoch: 14 [29700/36450]\tLoss: 514.8817\n",
      "Training Epoch: 14 [29750/36450]\tLoss: 506.8940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [29800/36450]\tLoss: 526.5464\n",
      "Training Epoch: 14 [29850/36450]\tLoss: 514.5331\n",
      "Training Epoch: 14 [29900/36450]\tLoss: 545.5709\n",
      "Training Epoch: 14 [29950/36450]\tLoss: 522.2387\n",
      "Training Epoch: 14 [30000/36450]\tLoss: 513.5959\n",
      "Training Epoch: 14 [30050/36450]\tLoss: 506.2319\n",
      "Training Epoch: 14 [30100/36450]\tLoss: 528.1896\n",
      "Training Epoch: 14 [30150/36450]\tLoss: 493.3074\n",
      "Training Epoch: 14 [30200/36450]\tLoss: 510.0483\n",
      "Training Epoch: 14 [30250/36450]\tLoss: 527.2563\n",
      "Training Epoch: 14 [30300/36450]\tLoss: 523.9983\n",
      "Training Epoch: 14 [30350/36450]\tLoss: 487.7541\n",
      "Training Epoch: 14 [30400/36450]\tLoss: 525.5886\n",
      "Training Epoch: 14 [30450/36450]\tLoss: 530.8082\n",
      "Training Epoch: 14 [30500/36450]\tLoss: 530.6387\n",
      "Training Epoch: 14 [30550/36450]\tLoss: 533.9100\n",
      "Training Epoch: 14 [30600/36450]\tLoss: 519.8748\n",
      "Training Epoch: 14 [30650/36450]\tLoss: 526.9877\n",
      "Training Epoch: 14 [30700/36450]\tLoss: 529.8820\n",
      "Training Epoch: 14 [30750/36450]\tLoss: 534.6246\n",
      "Training Epoch: 14 [30800/36450]\tLoss: 520.6996\n",
      "Training Epoch: 14 [30850/36450]\tLoss: 510.1579\n",
      "Training Epoch: 14 [30900/36450]\tLoss: 536.5571\n",
      "Training Epoch: 14 [30950/36450]\tLoss: 503.7575\n",
      "Training Epoch: 14 [31000/36450]\tLoss: 535.7393\n",
      "Training Epoch: 14 [31050/36450]\tLoss: 494.7472\n",
      "Training Epoch: 14 [31100/36450]\tLoss: 509.5318\n",
      "Training Epoch: 14 [31150/36450]\tLoss: 500.0133\n",
      "Training Epoch: 14 [31200/36450]\tLoss: 533.3837\n",
      "Training Epoch: 14 [31250/36450]\tLoss: 521.6659\n",
      "Training Epoch: 14 [31300/36450]\tLoss: 538.4969\n",
      "Training Epoch: 14 [31350/36450]\tLoss: 511.7024\n",
      "Training Epoch: 14 [31400/36450]\tLoss: 532.3204\n",
      "Training Epoch: 14 [31450/36450]\tLoss: 495.6821\n",
      "Training Epoch: 14 [31500/36450]\tLoss: 546.5625\n",
      "Training Epoch: 14 [31550/36450]\tLoss: 532.9076\n",
      "Training Epoch: 14 [31600/36450]\tLoss: 502.8723\n",
      "Training Epoch: 14 [31650/36450]\tLoss: 535.0706\n",
      "Training Epoch: 14 [31700/36450]\tLoss: 532.2071\n",
      "Training Epoch: 14 [31750/36450]\tLoss: 527.7277\n",
      "Training Epoch: 14 [31800/36450]\tLoss: 541.6255\n",
      "Training Epoch: 14 [31850/36450]\tLoss: 520.6274\n",
      "Training Epoch: 14 [31900/36450]\tLoss: 536.9949\n",
      "Training Epoch: 14 [31950/36450]\tLoss: 543.2860\n",
      "Training Epoch: 14 [32000/36450]\tLoss: 492.0738\n",
      "Training Epoch: 14 [32050/36450]\tLoss: 497.1300\n",
      "Training Epoch: 14 [32100/36450]\tLoss: 538.7548\n",
      "Training Epoch: 14 [32150/36450]\tLoss: 529.0502\n",
      "Training Epoch: 14 [32200/36450]\tLoss: 500.0773\n",
      "Training Epoch: 14 [32250/36450]\tLoss: 510.1809\n",
      "Training Epoch: 14 [32300/36450]\tLoss: 511.1458\n",
      "Training Epoch: 14 [32350/36450]\tLoss: 534.7007\n",
      "Training Epoch: 14 [32400/36450]\tLoss: 530.4394\n",
      "Training Epoch: 14 [32450/36450]\tLoss: 506.1310\n",
      "Training Epoch: 14 [32500/36450]\tLoss: 486.4487\n",
      "Training Epoch: 14 [32550/36450]\tLoss: 540.3387\n",
      "Training Epoch: 14 [32600/36450]\tLoss: 523.4409\n",
      "Training Epoch: 14 [32650/36450]\tLoss: 537.9803\n",
      "Training Epoch: 14 [32700/36450]\tLoss: 519.6695\n",
      "Training Epoch: 14 [32750/36450]\tLoss: 554.7745\n",
      "Training Epoch: 14 [32800/36450]\tLoss: 538.5626\n",
      "Training Epoch: 14 [32850/36450]\tLoss: 564.1257\n",
      "Training Epoch: 14 [32900/36450]\tLoss: 531.3160\n",
      "Training Epoch: 14 [32950/36450]\tLoss: 496.1718\n",
      "Training Epoch: 14 [33000/36450]\tLoss: 527.9699\n",
      "Training Epoch: 14 [33050/36450]\tLoss: 522.4039\n",
      "Training Epoch: 14 [33100/36450]\tLoss: 570.7732\n",
      "Training Epoch: 14 [33150/36450]\tLoss: 503.1790\n",
      "Training Epoch: 14 [33200/36450]\tLoss: 501.7399\n",
      "Training Epoch: 14 [33250/36450]\tLoss: 496.3479\n",
      "Training Epoch: 14 [33300/36450]\tLoss: 550.1003\n",
      "Training Epoch: 14 [33350/36450]\tLoss: 506.2122\n",
      "Training Epoch: 14 [33400/36450]\tLoss: 505.6288\n",
      "Training Epoch: 14 [33450/36450]\tLoss: 493.3378\n",
      "Training Epoch: 14 [33500/36450]\tLoss: 488.0864\n",
      "Training Epoch: 14 [33550/36450]\tLoss: 538.2867\n",
      "Training Epoch: 14 [33600/36450]\tLoss: 549.3685\n",
      "Training Epoch: 14 [33650/36450]\tLoss: 484.4692\n",
      "Training Epoch: 14 [33700/36450]\tLoss: 502.4502\n",
      "Training Epoch: 14 [33750/36450]\tLoss: 527.8616\n",
      "Training Epoch: 14 [33800/36450]\tLoss: 530.2995\n",
      "Training Epoch: 14 [33850/36450]\tLoss: 538.4800\n",
      "Training Epoch: 14 [33900/36450]\tLoss: 515.1697\n",
      "Training Epoch: 14 [33950/36450]\tLoss: 543.1748\n",
      "Training Epoch: 14 [34000/36450]\tLoss: 499.7844\n",
      "Training Epoch: 14 [34050/36450]\tLoss: 522.7252\n",
      "Training Epoch: 14 [34100/36450]\tLoss: 539.2330\n",
      "Training Epoch: 14 [34150/36450]\tLoss: 536.7581\n",
      "Training Epoch: 14 [34200/36450]\tLoss: 545.3268\n",
      "Training Epoch: 14 [34250/36450]\tLoss: 519.1314\n",
      "Training Epoch: 14 [34300/36450]\tLoss: 558.7488\n",
      "Training Epoch: 14 [34350/36450]\tLoss: 514.6127\n",
      "Training Epoch: 14 [34400/36450]\tLoss: 543.4060\n",
      "Training Epoch: 14 [34450/36450]\tLoss: 580.4566\n",
      "Training Epoch: 14 [34500/36450]\tLoss: 514.9235\n",
      "Training Epoch: 14 [34550/36450]\tLoss: 505.8177\n",
      "Training Epoch: 14 [34600/36450]\tLoss: 517.7968\n",
      "Training Epoch: 14 [34650/36450]\tLoss: 518.5497\n",
      "Training Epoch: 14 [34700/36450]\tLoss: 527.9454\n",
      "Training Epoch: 14 [34750/36450]\tLoss: 511.7160\n",
      "Training Epoch: 14 [34800/36450]\tLoss: 540.3845\n",
      "Training Epoch: 14 [34850/36450]\tLoss: 535.9214\n",
      "Training Epoch: 14 [34900/36450]\tLoss: 520.5413\n",
      "Training Epoch: 14 [34950/36450]\tLoss: 537.5009\n",
      "Training Epoch: 14 [35000/36450]\tLoss: 505.8371\n",
      "Training Epoch: 14 [35050/36450]\tLoss: 525.6090\n",
      "Training Epoch: 14 [35100/36450]\tLoss: 526.1906\n",
      "Training Epoch: 14 [35150/36450]\tLoss: 513.4257\n",
      "Training Epoch: 14 [35200/36450]\tLoss: 553.5016\n",
      "Training Epoch: 14 [35250/36450]\tLoss: 548.3775\n",
      "Training Epoch: 14 [35300/36450]\tLoss: 518.9548\n",
      "Training Epoch: 14 [35350/36450]\tLoss: 540.9656\n",
      "Training Epoch: 14 [35400/36450]\tLoss: 539.7182\n",
      "Training Epoch: 14 [35450/36450]\tLoss: 515.5632\n",
      "Training Epoch: 14 [35500/36450]\tLoss: 539.1913\n",
      "Training Epoch: 14 [35550/36450]\tLoss: 520.2325\n",
      "Training Epoch: 14 [35600/36450]\tLoss: 520.1641\n",
      "Training Epoch: 14 [35650/36450]\tLoss: 540.6512\n",
      "Training Epoch: 14 [35700/36450]\tLoss: 530.5549\n",
      "Training Epoch: 14 [35750/36450]\tLoss: 531.9639\n",
      "Training Epoch: 14 [35800/36450]\tLoss: 548.1345\n",
      "Training Epoch: 14 [35850/36450]\tLoss: 569.1550\n",
      "Training Epoch: 14 [35900/36450]\tLoss: 572.5480\n",
      "Training Epoch: 14 [35950/36450]\tLoss: 595.5706\n",
      "Training Epoch: 14 [36000/36450]\tLoss: 568.5964\n",
      "Training Epoch: 14 [36050/36450]\tLoss: 593.4470\n",
      "Training Epoch: 14 [36100/36450]\tLoss: 594.5605\n",
      "Training Epoch: 14 [36150/36450]\tLoss: 576.7531\n",
      "Training Epoch: 14 [36200/36450]\tLoss: 582.6252\n",
      "Training Epoch: 14 [36250/36450]\tLoss: 514.1409\n",
      "Training Epoch: 14 [36300/36450]\tLoss: 535.5118\n",
      "Training Epoch: 14 [36350/36450]\tLoss: 521.8743\n",
      "Training Epoch: 14 [36400/36450]\tLoss: 568.0624\n",
      "Training Epoch: 14 [36450/36450]\tLoss: 573.8428\n",
      "Training Epoch: 14 [4050/4050]\tLoss: 296.7482\n",
      "Training Epoch: 15 [50/36450]\tLoss: 582.7365\n",
      "Training Epoch: 15 [100/36450]\tLoss: 597.2808\n",
      "Training Epoch: 15 [150/36450]\tLoss: 537.5975\n",
      "Training Epoch: 15 [200/36450]\tLoss: 525.2234\n",
      "Training Epoch: 15 [250/36450]\tLoss: 495.4777\n",
      "Training Epoch: 15 [300/36450]\tLoss: 536.3911\n",
      "Training Epoch: 15 [350/36450]\tLoss: 518.0677\n",
      "Training Epoch: 15 [400/36450]\tLoss: 512.2169\n",
      "Training Epoch: 15 [450/36450]\tLoss: 515.3576\n",
      "Training Epoch: 15 [500/36450]\tLoss: 530.2077\n",
      "Training Epoch: 15 [550/36450]\tLoss: 559.9775\n",
      "Training Epoch: 15 [600/36450]\tLoss: 507.1189\n",
      "Training Epoch: 15 [650/36450]\tLoss: 517.2231\n",
      "Training Epoch: 15 [700/36450]\tLoss: 504.9754\n",
      "Training Epoch: 15 [750/36450]\tLoss: 496.6087\n",
      "Training Epoch: 15 [800/36450]\tLoss: 534.4058\n",
      "Training Epoch: 15 [850/36450]\tLoss: 523.3854\n",
      "Training Epoch: 15 [900/36450]\tLoss: 530.6607\n",
      "Training Epoch: 15 [950/36450]\tLoss: 531.2649\n",
      "Training Epoch: 15 [1000/36450]\tLoss: 518.9359\n",
      "Training Epoch: 15 [1050/36450]\tLoss: 531.4177\n",
      "Training Epoch: 15 [1100/36450]\tLoss: 500.0079\n",
      "Training Epoch: 15 [1150/36450]\tLoss: 534.8906\n",
      "Training Epoch: 15 [1200/36450]\tLoss: 561.7826\n",
      "Training Epoch: 15 [1250/36450]\tLoss: 530.7841\n",
      "Training Epoch: 15 [1300/36450]\tLoss: 527.0630\n",
      "Training Epoch: 15 [1350/36450]\tLoss: 563.8386\n",
      "Training Epoch: 15 [1400/36450]\tLoss: 530.8749\n",
      "Training Epoch: 15 [1450/36450]\tLoss: 510.2138\n",
      "Training Epoch: 15 [1500/36450]\tLoss: 523.7438\n",
      "Training Epoch: 15 [1550/36450]\tLoss: 520.6699\n",
      "Training Epoch: 15 [1600/36450]\tLoss: 521.9075\n",
      "Training Epoch: 15 [1650/36450]\tLoss: 517.6309\n",
      "Training Epoch: 15 [1700/36450]\tLoss: 542.4185\n",
      "Training Epoch: 15 [1750/36450]\tLoss: 493.1214\n",
      "Training Epoch: 15 [1800/36450]\tLoss: 498.0758\n",
      "Training Epoch: 15 [1850/36450]\tLoss: 536.0588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [1900/36450]\tLoss: 544.6415\n",
      "Training Epoch: 15 [1950/36450]\tLoss: 527.1916\n",
      "Training Epoch: 15 [2000/36450]\tLoss: 532.0054\n",
      "Training Epoch: 15 [2050/36450]\tLoss: 534.8937\n",
      "Training Epoch: 15 [2100/36450]\tLoss: 522.8707\n",
      "Training Epoch: 15 [2150/36450]\tLoss: 515.6116\n",
      "Training Epoch: 15 [2200/36450]\tLoss: 522.7152\n",
      "Training Epoch: 15 [2250/36450]\tLoss: 530.4076\n",
      "Training Epoch: 15 [2300/36450]\tLoss: 529.3771\n",
      "Training Epoch: 15 [2350/36450]\tLoss: 529.6478\n",
      "Training Epoch: 15 [2400/36450]\tLoss: 536.4586\n",
      "Training Epoch: 15 [2450/36450]\tLoss: 523.4957\n",
      "Training Epoch: 15 [2500/36450]\tLoss: 512.5555\n",
      "Training Epoch: 15 [2550/36450]\tLoss: 540.6524\n",
      "Training Epoch: 15 [2600/36450]\tLoss: 509.8990\n",
      "Training Epoch: 15 [2650/36450]\tLoss: 503.1714\n",
      "Training Epoch: 15 [2700/36450]\tLoss: 499.5642\n",
      "Training Epoch: 15 [2750/36450]\tLoss: 514.2999\n",
      "Training Epoch: 15 [2800/36450]\tLoss: 526.4655\n",
      "Training Epoch: 15 [2850/36450]\tLoss: 516.6002\n",
      "Training Epoch: 15 [2900/36450]\tLoss: 508.5955\n",
      "Training Epoch: 15 [2950/36450]\tLoss: 558.4854\n",
      "Training Epoch: 15 [3000/36450]\tLoss: 521.1180\n",
      "Training Epoch: 15 [3050/36450]\tLoss: 543.2651\n",
      "Training Epoch: 15 [3100/36450]\tLoss: 515.1002\n",
      "Training Epoch: 15 [3150/36450]\tLoss: 521.7015\n",
      "Training Epoch: 15 [3200/36450]\tLoss: 534.8686\n",
      "Training Epoch: 15 [3250/36450]\tLoss: 508.1260\n",
      "Training Epoch: 15 [3300/36450]\tLoss: 518.6617\n",
      "Training Epoch: 15 [3350/36450]\tLoss: 507.4695\n",
      "Training Epoch: 15 [3400/36450]\tLoss: 540.1284\n",
      "Training Epoch: 15 [3450/36450]\tLoss: 526.0800\n",
      "Training Epoch: 15 [3500/36450]\tLoss: 494.2788\n",
      "Training Epoch: 15 [3550/36450]\tLoss: 516.5401\n",
      "Training Epoch: 15 [3600/36450]\tLoss: 496.1206\n",
      "Training Epoch: 15 [3650/36450]\tLoss: 519.8087\n",
      "Training Epoch: 15 [3700/36450]\tLoss: 505.6432\n",
      "Training Epoch: 15 [3750/36450]\tLoss: 527.2399\n",
      "Training Epoch: 15 [3800/36450]\tLoss: 549.7799\n",
      "Training Epoch: 15 [3850/36450]\tLoss: 511.5106\n",
      "Training Epoch: 15 [3900/36450]\tLoss: 493.1571\n",
      "Training Epoch: 15 [3950/36450]\tLoss: 532.7000\n",
      "Training Epoch: 15 [4000/36450]\tLoss: 529.8842\n",
      "Training Epoch: 15 [4050/36450]\tLoss: 516.6037\n",
      "Training Epoch: 15 [4100/36450]\tLoss: 527.5578\n",
      "Training Epoch: 15 [4150/36450]\tLoss: 535.4828\n",
      "Training Epoch: 15 [4200/36450]\tLoss: 503.5430\n",
      "Training Epoch: 15 [4250/36450]\tLoss: 561.3947\n",
      "Training Epoch: 15 [4300/36450]\tLoss: 561.2756\n",
      "Training Epoch: 15 [4350/36450]\tLoss: 500.3391\n",
      "Training Epoch: 15 [4400/36450]\tLoss: 514.0997\n",
      "Training Epoch: 15 [4450/36450]\tLoss: 522.1157\n",
      "Training Epoch: 15 [4500/36450]\tLoss: 507.1060\n",
      "Training Epoch: 15 [4550/36450]\tLoss: 517.1353\n",
      "Training Epoch: 15 [4600/36450]\tLoss: 500.4381\n",
      "Training Epoch: 15 [4650/36450]\tLoss: 519.2617\n",
      "Training Epoch: 15 [4700/36450]\tLoss: 494.3174\n",
      "Training Epoch: 15 [4750/36450]\tLoss: 500.8618\n",
      "Training Epoch: 15 [4800/36450]\tLoss: 514.0699\n",
      "Training Epoch: 15 [4850/36450]\tLoss: 545.0967\n",
      "Training Epoch: 15 [4900/36450]\tLoss: 534.4478\n",
      "Training Epoch: 15 [4950/36450]\tLoss: 526.6542\n",
      "Training Epoch: 15 [5000/36450]\tLoss: 521.9075\n",
      "Training Epoch: 15 [5050/36450]\tLoss: 498.1894\n",
      "Training Epoch: 15 [5100/36450]\tLoss: 521.5727\n",
      "Training Epoch: 15 [5150/36450]\tLoss: 529.8675\n",
      "Training Epoch: 15 [5200/36450]\tLoss: 529.7405\n",
      "Training Epoch: 15 [5250/36450]\tLoss: 509.6967\n",
      "Training Epoch: 15 [5300/36450]\tLoss: 508.8704\n",
      "Training Epoch: 15 [5350/36450]\tLoss: 512.0583\n",
      "Training Epoch: 15 [5400/36450]\tLoss: 526.2844\n",
      "Training Epoch: 15 [5450/36450]\tLoss: 562.0558\n",
      "Training Epoch: 15 [5500/36450]\tLoss: 516.6265\n",
      "Training Epoch: 15 [5550/36450]\tLoss: 520.4692\n",
      "Training Epoch: 15 [5600/36450]\tLoss: 519.3602\n",
      "Training Epoch: 15 [5650/36450]\tLoss: 499.7600\n",
      "Training Epoch: 15 [5700/36450]\tLoss: 543.6859\n",
      "Training Epoch: 15 [5750/36450]\tLoss: 514.7676\n",
      "Training Epoch: 15 [5800/36450]\tLoss: 490.8696\n",
      "Training Epoch: 15 [5850/36450]\tLoss: 515.6470\n",
      "Training Epoch: 15 [5900/36450]\tLoss: 515.0118\n",
      "Training Epoch: 15 [5950/36450]\tLoss: 501.9360\n",
      "Training Epoch: 15 [6000/36450]\tLoss: 508.6992\n",
      "Training Epoch: 15 [6050/36450]\tLoss: 534.2008\n",
      "Training Epoch: 15 [6100/36450]\tLoss: 524.3864\n",
      "Training Epoch: 15 [6150/36450]\tLoss: 506.3873\n",
      "Training Epoch: 15 [6200/36450]\tLoss: 535.5588\n",
      "Training Epoch: 15 [6250/36450]\tLoss: 531.2133\n",
      "Training Epoch: 15 [6300/36450]\tLoss: 512.2726\n",
      "Training Epoch: 15 [6350/36450]\tLoss: 510.8469\n",
      "Training Epoch: 15 [6400/36450]\tLoss: 533.2517\n",
      "Training Epoch: 15 [6450/36450]\tLoss: 503.6747\n",
      "Training Epoch: 15 [6500/36450]\tLoss: 523.6376\n",
      "Training Epoch: 15 [6550/36450]\tLoss: 512.5903\n",
      "Training Epoch: 15 [6600/36450]\tLoss: 525.1221\n",
      "Training Epoch: 15 [6650/36450]\tLoss: 504.4529\n",
      "Training Epoch: 15 [6700/36450]\tLoss: 482.4592\n",
      "Training Epoch: 15 [6750/36450]\tLoss: 501.5525\n",
      "Training Epoch: 15 [6800/36450]\tLoss: 531.1388\n",
      "Training Epoch: 15 [6850/36450]\tLoss: 527.4385\n",
      "Training Epoch: 15 [6900/36450]\tLoss: 512.5965\n",
      "Training Epoch: 15 [6950/36450]\tLoss: 481.7985\n",
      "Training Epoch: 15 [7000/36450]\tLoss: 571.7314\n",
      "Training Epoch: 15 [7050/36450]\tLoss: 492.3947\n",
      "Training Epoch: 15 [7100/36450]\tLoss: 528.7936\n",
      "Training Epoch: 15 [7150/36450]\tLoss: 507.5524\n",
      "Training Epoch: 15 [7200/36450]\tLoss: 511.9548\n",
      "Training Epoch: 15 [7250/36450]\tLoss: 514.8878\n",
      "Training Epoch: 15 [7300/36450]\tLoss: 561.0074\n",
      "Training Epoch: 15 [7350/36450]\tLoss: 483.0727\n",
      "Training Epoch: 15 [7400/36450]\tLoss: 518.7559\n",
      "Training Epoch: 15 [7450/36450]\tLoss: 525.4365\n",
      "Training Epoch: 15 [7500/36450]\tLoss: 521.9210\n",
      "Training Epoch: 15 [7550/36450]\tLoss: 530.2511\n",
      "Training Epoch: 15 [7600/36450]\tLoss: 499.8795\n",
      "Training Epoch: 15 [7650/36450]\tLoss: 519.5903\n",
      "Training Epoch: 15 [7700/36450]\tLoss: 513.3384\n",
      "Training Epoch: 15 [7750/36450]\tLoss: 533.3039\n",
      "Training Epoch: 15 [7800/36450]\tLoss: 514.4894\n",
      "Training Epoch: 15 [7850/36450]\tLoss: 515.7321\n",
      "Training Epoch: 15 [7900/36450]\tLoss: 500.7594\n",
      "Training Epoch: 15 [7950/36450]\tLoss: 485.0641\n",
      "Training Epoch: 15 [8000/36450]\tLoss: 506.6129\n",
      "Training Epoch: 15 [8050/36450]\tLoss: 494.8717\n",
      "Training Epoch: 15 [8100/36450]\tLoss: 517.9262\n",
      "Training Epoch: 15 [8150/36450]\tLoss: 507.6435\n",
      "Training Epoch: 15 [8200/36450]\tLoss: 527.9493\n",
      "Training Epoch: 15 [8250/36450]\tLoss: 520.9119\n",
      "Training Epoch: 15 [8300/36450]\tLoss: 530.5856\n",
      "Training Epoch: 15 [8350/36450]\tLoss: 530.3715\n",
      "Training Epoch: 15 [8400/36450]\tLoss: 539.0474\n",
      "Training Epoch: 15 [8450/36450]\tLoss: 526.7239\n",
      "Training Epoch: 15 [8500/36450]\tLoss: 517.6896\n",
      "Training Epoch: 15 [8550/36450]\tLoss: 522.1481\n",
      "Training Epoch: 15 [8600/36450]\tLoss: 511.0167\n",
      "Training Epoch: 15 [8650/36450]\tLoss: 525.6744\n",
      "Training Epoch: 15 [8700/36450]\tLoss: 496.4457\n",
      "Training Epoch: 15 [8750/36450]\tLoss: 547.4274\n",
      "Training Epoch: 15 [8800/36450]\tLoss: 511.5313\n",
      "Training Epoch: 15 [8850/36450]\tLoss: 515.9982\n",
      "Training Epoch: 15 [8900/36450]\tLoss: 559.3067\n",
      "Training Epoch: 15 [8950/36450]\tLoss: 543.2531\n",
      "Training Epoch: 15 [9000/36450]\tLoss: 523.0667\n",
      "Training Epoch: 15 [9050/36450]\tLoss: 526.2028\n",
      "Training Epoch: 15 [9100/36450]\tLoss: 514.7831\n",
      "Training Epoch: 15 [9150/36450]\tLoss: 545.8568\n",
      "Training Epoch: 15 [9200/36450]\tLoss: 561.1310\n",
      "Training Epoch: 15 [9250/36450]\tLoss: 543.9886\n",
      "Training Epoch: 15 [9300/36450]\tLoss: 545.0326\n",
      "Training Epoch: 15 [9350/36450]\tLoss: 521.4902\n",
      "Training Epoch: 15 [9400/36450]\tLoss: 528.4081\n",
      "Training Epoch: 15 [9450/36450]\tLoss: 490.4905\n",
      "Training Epoch: 15 [9500/36450]\tLoss: 544.6885\n",
      "Training Epoch: 15 [9550/36450]\tLoss: 506.8598\n",
      "Training Epoch: 15 [9600/36450]\tLoss: 537.0172\n",
      "Training Epoch: 15 [9650/36450]\tLoss: 541.7736\n",
      "Training Epoch: 15 [9700/36450]\tLoss: 539.9849\n",
      "Training Epoch: 15 [9750/36450]\tLoss: 532.2499\n",
      "Training Epoch: 15 [9800/36450]\tLoss: 533.5480\n",
      "Training Epoch: 15 [9850/36450]\tLoss: 526.4079\n",
      "Training Epoch: 15 [9900/36450]\tLoss: 499.8118\n",
      "Training Epoch: 15 [9950/36450]\tLoss: 531.4002\n",
      "Training Epoch: 15 [10000/36450]\tLoss: 499.8737\n",
      "Training Epoch: 15 [10050/36450]\tLoss: 499.9708\n",
      "Training Epoch: 15 [10100/36450]\tLoss: 523.7892\n",
      "Training Epoch: 15 [10150/36450]\tLoss: 517.3144\n",
      "Training Epoch: 15 [10200/36450]\tLoss: 524.9290\n",
      "Training Epoch: 15 [10250/36450]\tLoss: 539.0157\n",
      "Training Epoch: 15 [10300/36450]\tLoss: 522.5782\n",
      "Training Epoch: 15 [10350/36450]\tLoss: 518.1475\n",
      "Training Epoch: 15 [10400/36450]\tLoss: 510.1573\n",
      "Training Epoch: 15 [10450/36450]\tLoss: 543.2963\n",
      "Training Epoch: 15 [10500/36450]\tLoss: 542.0327\n",
      "Training Epoch: 15 [10550/36450]\tLoss: 529.8665\n",
      "Training Epoch: 15 [10600/36450]\tLoss: 501.0688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [10650/36450]\tLoss: 558.2551\n",
      "Training Epoch: 15 [10700/36450]\tLoss: 541.4683\n",
      "Training Epoch: 15 [10750/36450]\tLoss: 530.8570\n",
      "Training Epoch: 15 [10800/36450]\tLoss: 498.0888\n",
      "Training Epoch: 15 [10850/36450]\tLoss: 489.9976\n",
      "Training Epoch: 15 [10900/36450]\tLoss: 502.9249\n",
      "Training Epoch: 15 [10950/36450]\tLoss: 512.6898\n",
      "Training Epoch: 15 [11000/36450]\tLoss: 477.0042\n",
      "Training Epoch: 15 [11050/36450]\tLoss: 545.9484\n",
      "Training Epoch: 15 [11100/36450]\tLoss: 523.1071\n",
      "Training Epoch: 15 [11150/36450]\tLoss: 528.1368\n",
      "Training Epoch: 15 [11200/36450]\tLoss: 563.7061\n",
      "Training Epoch: 15 [11250/36450]\tLoss: 517.0018\n",
      "Training Epoch: 15 [11300/36450]\tLoss: 524.8206\n",
      "Training Epoch: 15 [11350/36450]\tLoss: 520.4758\n",
      "Training Epoch: 15 [11400/36450]\tLoss: 507.8651\n",
      "Training Epoch: 15 [11450/36450]\tLoss: 498.7880\n",
      "Training Epoch: 15 [11500/36450]\tLoss: 504.3908\n",
      "Training Epoch: 15 [11550/36450]\tLoss: 506.0240\n",
      "Training Epoch: 15 [11600/36450]\tLoss: 489.2426\n",
      "Training Epoch: 15 [11650/36450]\tLoss: 506.2888\n",
      "Training Epoch: 15 [11700/36450]\tLoss: 519.7619\n",
      "Training Epoch: 15 [11750/36450]\tLoss: 491.1902\n",
      "Training Epoch: 15 [11800/36450]\tLoss: 517.8680\n",
      "Training Epoch: 15 [11850/36450]\tLoss: 493.6148\n",
      "Training Epoch: 15 [11900/36450]\tLoss: 505.9141\n",
      "Training Epoch: 15 [11950/36450]\tLoss: 528.1950\n",
      "Training Epoch: 15 [12000/36450]\tLoss: 499.5039\n",
      "Training Epoch: 15 [12050/36450]\tLoss: 509.1086\n",
      "Training Epoch: 15 [12100/36450]\tLoss: 522.4120\n",
      "Training Epoch: 15 [12150/36450]\tLoss: 528.5002\n",
      "Training Epoch: 15 [12200/36450]\tLoss: 513.6853\n",
      "Training Epoch: 15 [12250/36450]\tLoss: 500.2322\n",
      "Training Epoch: 15 [12300/36450]\tLoss: 497.4906\n",
      "Training Epoch: 15 [12350/36450]\tLoss: 534.5403\n",
      "Training Epoch: 15 [12400/36450]\tLoss: 515.6680\n",
      "Training Epoch: 15 [12450/36450]\tLoss: 510.4165\n",
      "Training Epoch: 15 [12500/36450]\tLoss: 523.2836\n",
      "Training Epoch: 15 [12550/36450]\tLoss: 501.7168\n",
      "Training Epoch: 15 [12600/36450]\tLoss: 520.7449\n",
      "Training Epoch: 15 [12650/36450]\tLoss: 489.5723\n",
      "Training Epoch: 15 [12700/36450]\tLoss: 528.6251\n",
      "Training Epoch: 15 [12750/36450]\tLoss: 539.6987\n",
      "Training Epoch: 15 [12800/36450]\tLoss: 515.5083\n",
      "Training Epoch: 15 [12850/36450]\tLoss: 532.2798\n",
      "Training Epoch: 15 [12900/36450]\tLoss: 523.3747\n",
      "Training Epoch: 15 [12950/36450]\tLoss: 510.1032\n",
      "Training Epoch: 15 [13000/36450]\tLoss: 541.6842\n",
      "Training Epoch: 15 [13050/36450]\tLoss: 538.7523\n",
      "Training Epoch: 15 [13100/36450]\tLoss: 505.3138\n",
      "Training Epoch: 15 [13150/36450]\tLoss: 535.7118\n",
      "Training Epoch: 15 [13200/36450]\tLoss: 519.3657\n",
      "Training Epoch: 15 [13250/36450]\tLoss: 544.9800\n",
      "Training Epoch: 15 [13300/36450]\tLoss: 553.9459\n",
      "Training Epoch: 15 [13350/36450]\tLoss: 563.2504\n",
      "Training Epoch: 15 [13400/36450]\tLoss: 563.3115\n",
      "Training Epoch: 15 [13450/36450]\tLoss: 560.8585\n",
      "Training Epoch: 15 [13500/36450]\tLoss: 558.7261\n",
      "Training Epoch: 15 [13550/36450]\tLoss: 531.5605\n",
      "Training Epoch: 15 [13600/36450]\tLoss: 507.5667\n",
      "Training Epoch: 15 [13650/36450]\tLoss: 523.4102\n",
      "Training Epoch: 15 [13700/36450]\tLoss: 549.7391\n",
      "Training Epoch: 15 [13750/36450]\tLoss: 516.0207\n",
      "Training Epoch: 15 [13800/36450]\tLoss: 536.6931\n",
      "Training Epoch: 15 [13850/36450]\tLoss: 507.6427\n",
      "Training Epoch: 15 [13900/36450]\tLoss: 530.1342\n",
      "Training Epoch: 15 [13950/36450]\tLoss: 501.2630\n",
      "Training Epoch: 15 [14000/36450]\tLoss: 469.0396\n",
      "Training Epoch: 15 [14050/36450]\tLoss: 502.3451\n",
      "Training Epoch: 15 [14100/36450]\tLoss: 504.9253\n",
      "Training Epoch: 15 [14150/36450]\tLoss: 497.3751\n",
      "Training Epoch: 15 [14200/36450]\tLoss: 502.1617\n",
      "Training Epoch: 15 [14250/36450]\tLoss: 539.6384\n",
      "Training Epoch: 15 [14300/36450]\tLoss: 534.7006\n",
      "Training Epoch: 15 [14350/36450]\tLoss: 529.0472\n",
      "Training Epoch: 15 [14400/36450]\tLoss: 558.3527\n",
      "Training Epoch: 15 [14450/36450]\tLoss: 535.4155\n",
      "Training Epoch: 15 [14500/36450]\tLoss: 504.1876\n",
      "Training Epoch: 15 [14550/36450]\tLoss: 512.9344\n",
      "Training Epoch: 15 [14600/36450]\tLoss: 530.0787\n",
      "Training Epoch: 15 [14650/36450]\tLoss: 526.6789\n",
      "Training Epoch: 15 [14700/36450]\tLoss: 503.9688\n",
      "Training Epoch: 15 [14750/36450]\tLoss: 540.3948\n",
      "Training Epoch: 15 [14800/36450]\tLoss: 515.1331\n",
      "Training Epoch: 15 [14850/36450]\tLoss: 568.2090\n",
      "Training Epoch: 15 [14900/36450]\tLoss: 524.4855\n",
      "Training Epoch: 15 [14950/36450]\tLoss: 521.8749\n",
      "Training Epoch: 15 [15000/36450]\tLoss: 517.0591\n",
      "Training Epoch: 15 [15050/36450]\tLoss: 530.5903\n",
      "Training Epoch: 15 [15100/36450]\tLoss: 495.8828\n",
      "Training Epoch: 15 [15150/36450]\tLoss: 507.9729\n",
      "Training Epoch: 15 [15200/36450]\tLoss: 522.4143\n",
      "Training Epoch: 15 [15250/36450]\tLoss: 511.4669\n",
      "Training Epoch: 15 [15300/36450]\tLoss: 504.3707\n",
      "Training Epoch: 15 [15350/36450]\tLoss: 537.4346\n",
      "Training Epoch: 15 [15400/36450]\tLoss: 502.9063\n",
      "Training Epoch: 15 [15450/36450]\tLoss: 500.2286\n",
      "Training Epoch: 15 [15500/36450]\tLoss: 514.6791\n",
      "Training Epoch: 15 [15550/36450]\tLoss: 522.1945\n",
      "Training Epoch: 15 [15600/36450]\tLoss: 500.9311\n",
      "Training Epoch: 15 [15650/36450]\tLoss: 523.6768\n",
      "Training Epoch: 15 [15700/36450]\tLoss: 521.3964\n",
      "Training Epoch: 15 [15750/36450]\tLoss: 550.4786\n",
      "Training Epoch: 15 [15800/36450]\tLoss: 528.6673\n",
      "Training Epoch: 15 [15850/36450]\tLoss: 482.5373\n",
      "Training Epoch: 15 [15900/36450]\tLoss: 503.4615\n",
      "Training Epoch: 15 [15950/36450]\tLoss: 502.5430\n",
      "Training Epoch: 15 [16000/36450]\tLoss: 532.6860\n",
      "Training Epoch: 15 [16050/36450]\tLoss: 503.7764\n",
      "Training Epoch: 15 [16100/36450]\tLoss: 529.1447\n",
      "Training Epoch: 15 [16150/36450]\tLoss: 516.8117\n",
      "Training Epoch: 15 [16200/36450]\tLoss: 534.9311\n",
      "Training Epoch: 15 [16250/36450]\tLoss: 509.4922\n",
      "Training Epoch: 15 [16300/36450]\tLoss: 517.0090\n",
      "Training Epoch: 15 [16350/36450]\tLoss: 513.4485\n",
      "Training Epoch: 15 [16400/36450]\tLoss: 534.9911\n",
      "Training Epoch: 15 [16450/36450]\tLoss: 476.9457\n",
      "Training Epoch: 15 [16500/36450]\tLoss: 509.8227\n",
      "Training Epoch: 15 [16550/36450]\tLoss: 526.1257\n",
      "Training Epoch: 15 [16600/36450]\tLoss: 520.0464\n",
      "Training Epoch: 15 [16650/36450]\tLoss: 520.4540\n",
      "Training Epoch: 15 [16700/36450]\tLoss: 514.3082\n",
      "Training Epoch: 15 [16750/36450]\tLoss: 500.1978\n",
      "Training Epoch: 15 [16800/36450]\tLoss: 537.4086\n",
      "Training Epoch: 15 [16850/36450]\tLoss: 503.7335\n",
      "Training Epoch: 15 [16900/36450]\tLoss: 539.2973\n",
      "Training Epoch: 15 [16950/36450]\tLoss: 520.9548\n",
      "Training Epoch: 15 [17000/36450]\tLoss: 552.6858\n",
      "Training Epoch: 15 [17050/36450]\tLoss: 484.9933\n",
      "Training Epoch: 15 [17100/36450]\tLoss: 521.3998\n",
      "Training Epoch: 15 [17150/36450]\tLoss: 510.2945\n",
      "Training Epoch: 15 [17200/36450]\tLoss: 529.4459\n",
      "Training Epoch: 15 [17250/36450]\tLoss: 542.3408\n",
      "Training Epoch: 15 [17300/36450]\tLoss: 556.1624\n",
      "Training Epoch: 15 [17350/36450]\tLoss: 521.4404\n",
      "Training Epoch: 15 [17400/36450]\tLoss: 520.7560\n",
      "Training Epoch: 15 [17450/36450]\tLoss: 529.8815\n",
      "Training Epoch: 15 [17500/36450]\tLoss: 509.4538\n",
      "Training Epoch: 15 [17550/36450]\tLoss: 470.1842\n",
      "Training Epoch: 15 [17600/36450]\tLoss: 514.9835\n",
      "Training Epoch: 15 [17650/36450]\tLoss: 533.8631\n",
      "Training Epoch: 15 [17700/36450]\tLoss: 528.3191\n",
      "Training Epoch: 15 [17750/36450]\tLoss: 500.7790\n",
      "Training Epoch: 15 [17800/36450]\tLoss: 524.5066\n",
      "Training Epoch: 15 [17850/36450]\tLoss: 480.9449\n",
      "Training Epoch: 15 [17900/36450]\tLoss: 529.3555\n",
      "Training Epoch: 15 [17950/36450]\tLoss: 518.9655\n",
      "Training Epoch: 15 [18000/36450]\tLoss: 503.1971\n",
      "Training Epoch: 15 [18050/36450]\tLoss: 516.0205\n",
      "Training Epoch: 15 [18100/36450]\tLoss: 500.8309\n",
      "Training Epoch: 15 [18150/36450]\tLoss: 503.0511\n",
      "Training Epoch: 15 [18200/36450]\tLoss: 513.6237\n",
      "Training Epoch: 15 [18250/36450]\tLoss: 511.1472\n",
      "Training Epoch: 15 [18300/36450]\tLoss: 510.9265\n",
      "Training Epoch: 15 [18350/36450]\tLoss: 505.8033\n",
      "Training Epoch: 15 [18400/36450]\tLoss: 524.4044\n",
      "Training Epoch: 15 [18450/36450]\tLoss: 546.2419\n",
      "Training Epoch: 15 [18500/36450]\tLoss: 518.0400\n",
      "Training Epoch: 15 [18550/36450]\tLoss: 490.5252\n",
      "Training Epoch: 15 [18600/36450]\tLoss: 501.6059\n",
      "Training Epoch: 15 [18650/36450]\tLoss: 513.2318\n",
      "Training Epoch: 15 [18700/36450]\tLoss: 506.8033\n",
      "Training Epoch: 15 [18750/36450]\tLoss: 507.5747\n",
      "Training Epoch: 15 [18800/36450]\tLoss: 526.8228\n",
      "Training Epoch: 15 [18850/36450]\tLoss: 510.4533\n",
      "Training Epoch: 15 [18900/36450]\tLoss: 535.3456\n",
      "Training Epoch: 15 [18950/36450]\tLoss: 519.6084\n",
      "Training Epoch: 15 [19000/36450]\tLoss: 489.1682\n",
      "Training Epoch: 15 [19050/36450]\tLoss: 530.7985\n",
      "Training Epoch: 15 [19100/36450]\tLoss: 546.8500\n",
      "Training Epoch: 15 [19150/36450]\tLoss: 568.5944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [19200/36450]\tLoss: 504.9469\n",
      "Training Epoch: 15 [19250/36450]\tLoss: 517.1284\n",
      "Training Epoch: 15 [19300/36450]\tLoss: 532.3961\n",
      "Training Epoch: 15 [19350/36450]\tLoss: 537.8504\n",
      "Training Epoch: 15 [19400/36450]\tLoss: 500.2934\n",
      "Training Epoch: 15 [19450/36450]\tLoss: 555.9274\n",
      "Training Epoch: 15 [19500/36450]\tLoss: 535.0466\n",
      "Training Epoch: 15 [19550/36450]\tLoss: 544.8633\n",
      "Training Epoch: 15 [19600/36450]\tLoss: 529.9719\n",
      "Training Epoch: 15 [19650/36450]\tLoss: 503.0082\n",
      "Training Epoch: 15 [19700/36450]\tLoss: 499.2679\n",
      "Training Epoch: 15 [19750/36450]\tLoss: 491.2501\n",
      "Training Epoch: 15 [19800/36450]\tLoss: 512.8564\n",
      "Training Epoch: 15 [19850/36450]\tLoss: 485.9058\n",
      "Training Epoch: 15 [19900/36450]\tLoss: 512.0284\n",
      "Training Epoch: 15 [19950/36450]\tLoss: 538.1850\n",
      "Training Epoch: 15 [20000/36450]\tLoss: 542.7960\n",
      "Training Epoch: 15 [20050/36450]\tLoss: 525.0449\n",
      "Training Epoch: 15 [20100/36450]\tLoss: 509.5566\n",
      "Training Epoch: 15 [20150/36450]\tLoss: 527.4309\n",
      "Training Epoch: 15 [20200/36450]\tLoss: 546.8727\n",
      "Training Epoch: 15 [20250/36450]\tLoss: 568.4411\n",
      "Training Epoch: 15 [20300/36450]\tLoss: 544.7679\n",
      "Training Epoch: 15 [20350/36450]\tLoss: 504.4633\n",
      "Training Epoch: 15 [20400/36450]\tLoss: 519.9763\n",
      "Training Epoch: 15 [20450/36450]\tLoss: 539.6274\n",
      "Training Epoch: 15 [20500/36450]\tLoss: 514.3716\n",
      "Training Epoch: 15 [20550/36450]\tLoss: 503.0007\n",
      "Training Epoch: 15 [20600/36450]\tLoss: 560.5096\n",
      "Training Epoch: 15 [20650/36450]\tLoss: 486.9884\n",
      "Training Epoch: 15 [20700/36450]\tLoss: 530.3118\n",
      "Training Epoch: 15 [20750/36450]\tLoss: 506.7426\n",
      "Training Epoch: 15 [20800/36450]\tLoss: 523.7175\n",
      "Training Epoch: 15 [20850/36450]\tLoss: 509.6193\n",
      "Training Epoch: 15 [20900/36450]\tLoss: 500.8211\n",
      "Training Epoch: 15 [20950/36450]\tLoss: 536.4758\n",
      "Training Epoch: 15 [21000/36450]\tLoss: 524.4750\n",
      "Training Epoch: 15 [21050/36450]\tLoss: 496.1785\n",
      "Training Epoch: 15 [21100/36450]\tLoss: 493.8829\n",
      "Training Epoch: 15 [21150/36450]\tLoss: 510.1666\n",
      "Training Epoch: 15 [21200/36450]\tLoss: 510.0452\n",
      "Training Epoch: 15 [21250/36450]\tLoss: 522.2429\n",
      "Training Epoch: 15 [21300/36450]\tLoss: 477.4461\n",
      "Training Epoch: 15 [21350/36450]\tLoss: 509.2905\n",
      "Training Epoch: 15 [21400/36450]\tLoss: 508.7840\n",
      "Training Epoch: 15 [21450/36450]\tLoss: 520.6446\n",
      "Training Epoch: 15 [21500/36450]\tLoss: 532.0410\n",
      "Training Epoch: 15 [21550/36450]\tLoss: 511.4977\n",
      "Training Epoch: 15 [21600/36450]\tLoss: 524.4695\n",
      "Training Epoch: 15 [21650/36450]\tLoss: 484.2114\n",
      "Training Epoch: 15 [21700/36450]\tLoss: 507.5124\n",
      "Training Epoch: 15 [21750/36450]\tLoss: 510.7795\n",
      "Training Epoch: 15 [21800/36450]\tLoss: 505.1772\n",
      "Training Epoch: 15 [21850/36450]\tLoss: 541.8525\n",
      "Training Epoch: 15 [21900/36450]\tLoss: 522.5320\n",
      "Training Epoch: 15 [21950/36450]\tLoss: 509.6480\n",
      "Training Epoch: 15 [22000/36450]\tLoss: 514.7669\n",
      "Training Epoch: 15 [22050/36450]\tLoss: 516.6052\n",
      "Training Epoch: 15 [22100/36450]\tLoss: 513.8644\n",
      "Training Epoch: 15 [22150/36450]\tLoss: 536.2112\n",
      "Training Epoch: 15 [22200/36450]\tLoss: 478.2597\n",
      "Training Epoch: 15 [22250/36450]\tLoss: 511.0431\n",
      "Training Epoch: 15 [22300/36450]\tLoss: 528.7775\n",
      "Training Epoch: 15 [22350/36450]\tLoss: 488.7780\n",
      "Training Epoch: 15 [22400/36450]\tLoss: 530.6753\n",
      "Training Epoch: 15 [22450/36450]\tLoss: 538.2313\n",
      "Training Epoch: 15 [22500/36450]\tLoss: 521.7679\n",
      "Training Epoch: 15 [22550/36450]\tLoss: 538.6520\n",
      "Training Epoch: 15 [22600/36450]\tLoss: 526.2821\n",
      "Training Epoch: 15 [22650/36450]\tLoss: 527.2326\n",
      "Training Epoch: 15 [22700/36450]\tLoss: 492.2234\n",
      "Training Epoch: 15 [22750/36450]\tLoss: 553.6710\n",
      "Training Epoch: 15 [22800/36450]\tLoss: 495.8696\n",
      "Training Epoch: 15 [22850/36450]\tLoss: 493.0668\n",
      "Training Epoch: 15 [22900/36450]\tLoss: 518.9331\n",
      "Training Epoch: 15 [22950/36450]\tLoss: 539.6082\n",
      "Training Epoch: 15 [23000/36450]\tLoss: 519.0000\n",
      "Training Epoch: 15 [23050/36450]\tLoss: 548.8887\n",
      "Training Epoch: 15 [23100/36450]\tLoss: 561.6204\n",
      "Training Epoch: 15 [23150/36450]\tLoss: 552.8168\n",
      "Training Epoch: 15 [23200/36450]\tLoss: 576.5730\n",
      "Training Epoch: 15 [23250/36450]\tLoss: 531.5742\n",
      "Training Epoch: 15 [23300/36450]\tLoss: 538.4247\n",
      "Training Epoch: 15 [23350/36450]\tLoss: 525.3050\n",
      "Training Epoch: 15 [23400/36450]\tLoss: 530.3276\n",
      "Training Epoch: 15 [23450/36450]\tLoss: 506.7604\n",
      "Training Epoch: 15 [23500/36450]\tLoss: 514.6695\n",
      "Training Epoch: 15 [23550/36450]\tLoss: 511.9361\n",
      "Training Epoch: 15 [23600/36450]\tLoss: 519.1586\n",
      "Training Epoch: 15 [23650/36450]\tLoss: 546.6716\n",
      "Training Epoch: 15 [23700/36450]\tLoss: 550.1260\n",
      "Training Epoch: 15 [23750/36450]\tLoss: 498.4622\n",
      "Training Epoch: 15 [23800/36450]\tLoss: 501.7987\n",
      "Training Epoch: 15 [23850/36450]\tLoss: 515.5649\n",
      "Training Epoch: 15 [23900/36450]\tLoss: 550.6555\n",
      "Training Epoch: 15 [23950/36450]\tLoss: 538.2083\n",
      "Training Epoch: 15 [24000/36450]\tLoss: 475.6367\n",
      "Training Epoch: 15 [24050/36450]\tLoss: 534.4272\n",
      "Training Epoch: 15 [24100/36450]\tLoss: 525.1721\n",
      "Training Epoch: 15 [24150/36450]\tLoss: 509.1142\n",
      "Training Epoch: 15 [24200/36450]\tLoss: 507.4890\n",
      "Training Epoch: 15 [24250/36450]\tLoss: 502.8941\n",
      "Training Epoch: 15 [24300/36450]\tLoss: 517.1207\n",
      "Training Epoch: 15 [24350/36450]\tLoss: 526.9050\n",
      "Training Epoch: 15 [24400/36450]\tLoss: 533.6870\n",
      "Training Epoch: 15 [24450/36450]\tLoss: 540.8771\n",
      "Training Epoch: 15 [24500/36450]\tLoss: 498.4110\n",
      "Training Epoch: 15 [24550/36450]\tLoss: 489.5139\n",
      "Training Epoch: 15 [24600/36450]\tLoss: 484.2762\n",
      "Training Epoch: 15 [24650/36450]\tLoss: 514.6119\n",
      "Training Epoch: 15 [24700/36450]\tLoss: 534.6885\n",
      "Training Epoch: 15 [24750/36450]\tLoss: 500.5926\n",
      "Training Epoch: 15 [24800/36450]\tLoss: 555.9066\n",
      "Training Epoch: 15 [24850/36450]\tLoss: 526.0367\n",
      "Training Epoch: 15 [24900/36450]\tLoss: 542.2883\n",
      "Training Epoch: 15 [24950/36450]\tLoss: 508.1549\n",
      "Training Epoch: 15 [25000/36450]\tLoss: 518.1455\n",
      "Training Epoch: 15 [25050/36450]\tLoss: 500.7566\n",
      "Training Epoch: 15 [25100/36450]\tLoss: 498.1019\n",
      "Training Epoch: 15 [25150/36450]\tLoss: 502.8102\n",
      "Training Epoch: 15 [25200/36450]\tLoss: 529.1091\n",
      "Training Epoch: 15 [25250/36450]\tLoss: 536.7682\n",
      "Training Epoch: 15 [25300/36450]\tLoss: 566.7339\n",
      "Training Epoch: 15 [25350/36450]\tLoss: 508.8468\n",
      "Training Epoch: 15 [25400/36450]\tLoss: 510.7228\n",
      "Training Epoch: 15 [25450/36450]\tLoss: 505.4406\n",
      "Training Epoch: 15 [25500/36450]\tLoss: 510.2140\n",
      "Training Epoch: 15 [25550/36450]\tLoss: 530.9766\n",
      "Training Epoch: 15 [25600/36450]\tLoss: 542.7538\n",
      "Training Epoch: 15 [25650/36450]\tLoss: 529.5647\n",
      "Training Epoch: 15 [25700/36450]\tLoss: 483.1263\n",
      "Training Epoch: 15 [25750/36450]\tLoss: 501.6357\n",
      "Training Epoch: 15 [25800/36450]\tLoss: 518.4480\n",
      "Training Epoch: 15 [25850/36450]\tLoss: 520.8870\n",
      "Training Epoch: 15 [25900/36450]\tLoss: 520.6802\n",
      "Training Epoch: 15 [25950/36450]\tLoss: 532.8140\n",
      "Training Epoch: 15 [26000/36450]\tLoss: 531.7528\n",
      "Training Epoch: 15 [26050/36450]\tLoss: 542.9942\n",
      "Training Epoch: 15 [26100/36450]\tLoss: 513.5032\n",
      "Training Epoch: 15 [26150/36450]\tLoss: 542.2672\n",
      "Training Epoch: 15 [26200/36450]\tLoss: 546.7631\n",
      "Training Epoch: 15 [26250/36450]\tLoss: 493.4693\n",
      "Training Epoch: 15 [26300/36450]\tLoss: 523.9221\n",
      "Training Epoch: 15 [26350/36450]\tLoss: 484.3664\n",
      "Training Epoch: 15 [26400/36450]\tLoss: 519.7106\n",
      "Training Epoch: 15 [26450/36450]\tLoss: 517.9907\n",
      "Training Epoch: 15 [26500/36450]\tLoss: 534.0925\n",
      "Training Epoch: 15 [26550/36450]\tLoss: 536.8489\n",
      "Training Epoch: 15 [26600/36450]\tLoss: 502.3029\n",
      "Training Epoch: 15 [26650/36450]\tLoss: 548.0298\n",
      "Training Epoch: 15 [26700/36450]\tLoss: 540.2682\n",
      "Training Epoch: 15 [26750/36450]\tLoss: 509.0394\n",
      "Training Epoch: 15 [26800/36450]\tLoss: 494.1418\n",
      "Training Epoch: 15 [26850/36450]\tLoss: 492.6150\n",
      "Training Epoch: 15 [26900/36450]\tLoss: 489.1880\n",
      "Training Epoch: 15 [26950/36450]\tLoss: 517.1710\n",
      "Training Epoch: 15 [27000/36450]\tLoss: 509.6172\n",
      "Training Epoch: 15 [27050/36450]\tLoss: 520.4084\n",
      "Training Epoch: 15 [27100/36450]\tLoss: 471.4144\n",
      "Training Epoch: 15 [27150/36450]\tLoss: 527.1419\n",
      "Training Epoch: 15 [27200/36450]\tLoss: 511.9027\n",
      "Training Epoch: 15 [27250/36450]\tLoss: 513.9374\n",
      "Training Epoch: 15 [27300/36450]\tLoss: 483.0276\n",
      "Training Epoch: 15 [27350/36450]\tLoss: 496.1295\n",
      "Training Epoch: 15 [27400/36450]\tLoss: 521.4748\n",
      "Training Epoch: 15 [27450/36450]\tLoss: 505.6552\n",
      "Training Epoch: 15 [27500/36450]\tLoss: 492.0504\n",
      "Training Epoch: 15 [27550/36450]\tLoss: 512.4435\n",
      "Training Epoch: 15 [27600/36450]\tLoss: 533.8823\n",
      "Training Epoch: 15 [27650/36450]\tLoss: 528.5637\n",
      "Training Epoch: 15 [27700/36450]\tLoss: 522.4884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [27750/36450]\tLoss: 491.1405\n",
      "Training Epoch: 15 [27800/36450]\tLoss: 546.0074\n",
      "Training Epoch: 15 [27850/36450]\tLoss: 497.6373\n",
      "Training Epoch: 15 [27900/36450]\tLoss: 549.5098\n",
      "Training Epoch: 15 [27950/36450]\tLoss: 544.8947\n",
      "Training Epoch: 15 [28000/36450]\tLoss: 486.6467\n",
      "Training Epoch: 15 [28050/36450]\tLoss: 499.1483\n",
      "Training Epoch: 15 [28100/36450]\tLoss: 521.0836\n",
      "Training Epoch: 15 [28150/36450]\tLoss: 494.8466\n",
      "Training Epoch: 15 [28200/36450]\tLoss: 517.0701\n",
      "Training Epoch: 15 [28250/36450]\tLoss: 512.9943\n",
      "Training Epoch: 15 [28300/36450]\tLoss: 553.2777\n",
      "Training Epoch: 15 [28350/36450]\tLoss: 490.8740\n",
      "Training Epoch: 15 [28400/36450]\tLoss: 482.9634\n",
      "Training Epoch: 15 [28450/36450]\tLoss: 488.8038\n",
      "Training Epoch: 15 [28500/36450]\tLoss: 497.7598\n",
      "Training Epoch: 15 [28550/36450]\tLoss: 513.3168\n",
      "Training Epoch: 15 [28600/36450]\tLoss: 512.2213\n",
      "Training Epoch: 15 [28650/36450]\tLoss: 479.0322\n",
      "Training Epoch: 15 [28700/36450]\tLoss: 522.8415\n",
      "Training Epoch: 15 [28750/36450]\tLoss: 531.0460\n",
      "Training Epoch: 15 [28800/36450]\tLoss: 516.2206\n",
      "Training Epoch: 15 [28850/36450]\tLoss: 486.1140\n",
      "Training Epoch: 15 [28900/36450]\tLoss: 501.4370\n",
      "Training Epoch: 15 [28950/36450]\tLoss: 493.2892\n",
      "Training Epoch: 15 [29000/36450]\tLoss: 496.7851\n",
      "Training Epoch: 15 [29050/36450]\tLoss: 507.2576\n",
      "Training Epoch: 15 [29100/36450]\tLoss: 545.7489\n",
      "Training Epoch: 15 [29150/36450]\tLoss: 531.0992\n",
      "Training Epoch: 15 [29200/36450]\tLoss: 495.1607\n",
      "Training Epoch: 15 [29250/36450]\tLoss: 501.8468\n",
      "Training Epoch: 15 [29300/36450]\tLoss: 537.6536\n",
      "Training Epoch: 15 [29350/36450]\tLoss: 497.5784\n",
      "Training Epoch: 15 [29400/36450]\tLoss: 529.1132\n",
      "Training Epoch: 15 [29450/36450]\tLoss: 508.1511\n",
      "Training Epoch: 15 [29500/36450]\tLoss: 523.3835\n",
      "Training Epoch: 15 [29550/36450]\tLoss: 515.8484\n",
      "Training Epoch: 15 [29600/36450]\tLoss: 503.8853\n",
      "Training Epoch: 15 [29650/36450]\tLoss: 495.6305\n",
      "Training Epoch: 15 [29700/36450]\tLoss: 541.5329\n",
      "Training Epoch: 15 [29750/36450]\tLoss: 523.7168\n",
      "Training Epoch: 15 [29800/36450]\tLoss: 505.1003\n",
      "Training Epoch: 15 [29850/36450]\tLoss: 523.2377\n",
      "Training Epoch: 15 [29900/36450]\tLoss: 553.1406\n",
      "Training Epoch: 15 [29950/36450]\tLoss: 509.3563\n",
      "Training Epoch: 15 [30000/36450]\tLoss: 484.6272\n",
      "Training Epoch: 15 [30050/36450]\tLoss: 484.4787\n",
      "Training Epoch: 15 [30100/36450]\tLoss: 495.4970\n",
      "Training Epoch: 15 [30150/36450]\tLoss: 498.7371\n",
      "Training Epoch: 15 [30200/36450]\tLoss: 539.6411\n",
      "Training Epoch: 15 [30250/36450]\tLoss: 546.7941\n",
      "Training Epoch: 15 [30300/36450]\tLoss: 520.8906\n",
      "Training Epoch: 15 [30350/36450]\tLoss: 504.5027\n",
      "Training Epoch: 15 [30400/36450]\tLoss: 480.6055\n",
      "Training Epoch: 15 [30450/36450]\tLoss: 528.7791\n",
      "Training Epoch: 15 [30500/36450]\tLoss: 521.0147\n",
      "Training Epoch: 15 [30550/36450]\tLoss: 568.5179\n",
      "Training Epoch: 15 [30600/36450]\tLoss: 510.6889\n",
      "Training Epoch: 15 [30650/36450]\tLoss: 525.4502\n",
      "Training Epoch: 15 [30700/36450]\tLoss: 534.4421\n",
      "Training Epoch: 15 [30750/36450]\tLoss: 515.1733\n",
      "Training Epoch: 15 [30800/36450]\tLoss: 533.4376\n",
      "Training Epoch: 15 [30850/36450]\tLoss: 528.2025\n",
      "Training Epoch: 15 [30900/36450]\tLoss: 531.7770\n",
      "Training Epoch: 15 [30950/36450]\tLoss: 549.9797\n",
      "Training Epoch: 15 [31000/36450]\tLoss: 547.4398\n",
      "Training Epoch: 15 [31050/36450]\tLoss: 589.6842\n",
      "Training Epoch: 15 [31100/36450]\tLoss: 576.1683\n",
      "Training Epoch: 15 [31150/36450]\tLoss: 578.0320\n",
      "Training Epoch: 15 [31200/36450]\tLoss: 554.9261\n",
      "Training Epoch: 15 [31250/36450]\tLoss: 525.1565\n",
      "Training Epoch: 15 [31300/36450]\tLoss: 543.6443\n",
      "Training Epoch: 15 [31350/36450]\tLoss: 521.6578\n",
      "Training Epoch: 15 [31400/36450]\tLoss: 510.7724\n",
      "Training Epoch: 15 [31450/36450]\tLoss: 533.2232\n",
      "Training Epoch: 15 [31500/36450]\tLoss: 503.2421\n",
      "Training Epoch: 15 [31550/36450]\tLoss: 505.2493\n",
      "Training Epoch: 15 [31600/36450]\tLoss: 522.3036\n",
      "Training Epoch: 15 [31650/36450]\tLoss: 487.9209\n",
      "Training Epoch: 15 [31700/36450]\tLoss: 523.3992\n",
      "Training Epoch: 15 [31750/36450]\tLoss: 517.5386\n",
      "Training Epoch: 15 [31800/36450]\tLoss: 493.8060\n",
      "Training Epoch: 15 [31850/36450]\tLoss: 563.8279\n",
      "Training Epoch: 15 [31900/36450]\tLoss: 552.0222\n",
      "Training Epoch: 15 [31950/36450]\tLoss: 569.9954\n",
      "Training Epoch: 15 [32000/36450]\tLoss: 558.8314\n",
      "Training Epoch: 15 [32050/36450]\tLoss: 555.9030\n",
      "Training Epoch: 15 [32100/36450]\tLoss: 525.0834\n",
      "Training Epoch: 15 [32150/36450]\tLoss: 497.2221\n",
      "Training Epoch: 15 [32200/36450]\tLoss: 514.2863\n",
      "Training Epoch: 15 [32250/36450]\tLoss: 524.6926\n",
      "Training Epoch: 15 [32300/36450]\tLoss: 542.8051\n",
      "Training Epoch: 15 [32350/36450]\tLoss: 557.3710\n",
      "Training Epoch: 15 [32400/36450]\tLoss: 558.9694\n",
      "Training Epoch: 15 [32450/36450]\tLoss: 494.1840\n",
      "Training Epoch: 15 [32500/36450]\tLoss: 519.3093\n",
      "Training Epoch: 15 [32550/36450]\tLoss: 508.9045\n",
      "Training Epoch: 15 [32600/36450]\tLoss: 511.1340\n",
      "Training Epoch: 15 [32650/36450]\tLoss: 549.5930\n",
      "Training Epoch: 15 [32700/36450]\tLoss: 522.3460\n",
      "Training Epoch: 15 [32750/36450]\tLoss: 496.7394\n",
      "Training Epoch: 15 [32800/36450]\tLoss: 494.1409\n",
      "Training Epoch: 15 [32850/36450]\tLoss: 497.0616\n",
      "Training Epoch: 15 [32900/36450]\tLoss: 525.6341\n",
      "Training Epoch: 15 [32950/36450]\tLoss: 470.4910\n",
      "Training Epoch: 15 [33000/36450]\tLoss: 551.3360\n",
      "Training Epoch: 15 [33050/36450]\tLoss: 497.4113\n",
      "Training Epoch: 15 [33100/36450]\tLoss: 511.3144\n",
      "Training Epoch: 15 [33150/36450]\tLoss: 527.7899\n",
      "Training Epoch: 15 [33200/36450]\tLoss: 487.8034\n",
      "Training Epoch: 15 [33250/36450]\tLoss: 506.9197\n",
      "Training Epoch: 15 [33300/36450]\tLoss: 527.3154\n",
      "Training Epoch: 15 [33350/36450]\tLoss: 533.9199\n",
      "Training Epoch: 15 [33400/36450]\tLoss: 541.3304\n",
      "Training Epoch: 15 [33450/36450]\tLoss: 527.6811\n",
      "Training Epoch: 15 [33500/36450]\tLoss: 519.9500\n",
      "Training Epoch: 15 [33550/36450]\tLoss: 482.6351\n",
      "Training Epoch: 15 [33600/36450]\tLoss: 530.8785\n",
      "Training Epoch: 15 [33650/36450]\tLoss: 544.9951\n",
      "Training Epoch: 15 [33700/36450]\tLoss: 515.9794\n",
      "Training Epoch: 15 [33750/36450]\tLoss: 569.2059\n",
      "Training Epoch: 15 [33800/36450]\tLoss: 519.2872\n",
      "Training Epoch: 15 [33850/36450]\tLoss: 503.9246\n",
      "Training Epoch: 15 [33900/36450]\tLoss: 475.3854\n",
      "Training Epoch: 15 [33950/36450]\tLoss: 519.7565\n",
      "Training Epoch: 15 [34000/36450]\tLoss: 508.8140\n",
      "Training Epoch: 15 [34050/36450]\tLoss: 509.3885\n",
      "Training Epoch: 15 [34100/36450]\tLoss: 551.8868\n",
      "Training Epoch: 15 [34150/36450]\tLoss: 477.9321\n",
      "Training Epoch: 15 [34200/36450]\tLoss: 496.5392\n",
      "Training Epoch: 15 [34250/36450]\tLoss: 485.6098\n",
      "Training Epoch: 15 [34300/36450]\tLoss: 533.6616\n",
      "Training Epoch: 15 [34350/36450]\tLoss: 490.5023\n",
      "Training Epoch: 15 [34400/36450]\tLoss: 510.0430\n",
      "Training Epoch: 15 [34450/36450]\tLoss: 531.8764\n",
      "Training Epoch: 15 [34500/36450]\tLoss: 526.2510\n",
      "Training Epoch: 15 [34550/36450]\tLoss: 496.2297\n",
      "Training Epoch: 15 [34600/36450]\tLoss: 497.6175\n",
      "Training Epoch: 15 [34650/36450]\tLoss: 501.7278\n",
      "Training Epoch: 15 [34700/36450]\tLoss: 523.4531\n",
      "Training Epoch: 15 [34750/36450]\tLoss: 504.5976\n",
      "Training Epoch: 15 [34800/36450]\tLoss: 514.4005\n",
      "Training Epoch: 15 [34850/36450]\tLoss: 529.5408\n",
      "Training Epoch: 15 [34900/36450]\tLoss: 518.5053\n",
      "Training Epoch: 15 [34950/36450]\tLoss: 543.1725\n",
      "Training Epoch: 15 [35000/36450]\tLoss: 533.0030\n",
      "Training Epoch: 15 [35050/36450]\tLoss: 515.7918\n",
      "Training Epoch: 15 [35100/36450]\tLoss: 538.3825\n",
      "Training Epoch: 15 [35150/36450]\tLoss: 479.7550\n",
      "Training Epoch: 15 [35200/36450]\tLoss: 510.2189\n",
      "Training Epoch: 15 [35250/36450]\tLoss: 515.6671\n",
      "Training Epoch: 15 [35300/36450]\tLoss: 505.4413\n",
      "Training Epoch: 15 [35350/36450]\tLoss: 535.6395\n",
      "Training Epoch: 15 [35400/36450]\tLoss: 483.6658\n",
      "Training Epoch: 15 [35450/36450]\tLoss: 508.7188\n",
      "Training Epoch: 15 [35500/36450]\tLoss: 499.5027\n",
      "Training Epoch: 15 [35550/36450]\tLoss: 504.2908\n",
      "Training Epoch: 15 [35600/36450]\tLoss: 513.9084\n",
      "Training Epoch: 15 [35650/36450]\tLoss: 512.8239\n",
      "Training Epoch: 15 [35700/36450]\tLoss: 549.2927\n",
      "Training Epoch: 15 [35750/36450]\tLoss: 491.4332\n",
      "Training Epoch: 15 [35800/36450]\tLoss: 497.3156\n",
      "Training Epoch: 15 [35850/36450]\tLoss: 539.2556\n",
      "Training Epoch: 15 [35900/36450]\tLoss: 494.3726\n",
      "Training Epoch: 15 [35950/36450]\tLoss: 529.2125\n",
      "Training Epoch: 15 [36000/36450]\tLoss: 530.1088\n",
      "Training Epoch: 15 [36050/36450]\tLoss: 508.2822\n",
      "Training Epoch: 15 [36100/36450]\tLoss: 531.7271\n",
      "Training Epoch: 15 [36150/36450]\tLoss: 512.0153\n",
      "Training Epoch: 15 [36200/36450]\tLoss: 496.3469\n",
      "Training Epoch: 15 [36250/36450]\tLoss: 518.8335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [36300/36450]\tLoss: 524.0716\n",
      "Training Epoch: 15 [36350/36450]\tLoss: 494.6224\n",
      "Training Epoch: 15 [36400/36450]\tLoss: 491.7627\n",
      "Training Epoch: 15 [36450/36450]\tLoss: 521.5502\n",
      "Training Epoch: 15 [4050/4050]\tLoss: 257.1697\n",
      "Training Epoch: 16 [50/36450]\tLoss: 482.9452\n",
      "Training Epoch: 16 [100/36450]\tLoss: 483.8170\n",
      "Training Epoch: 16 [150/36450]\tLoss: 475.8567\n",
      "Training Epoch: 16 [200/36450]\tLoss: 505.7670\n",
      "Training Epoch: 16 [250/36450]\tLoss: 494.5388\n",
      "Training Epoch: 16 [300/36450]\tLoss: 476.8837\n",
      "Training Epoch: 16 [350/36450]\tLoss: 539.1021\n",
      "Training Epoch: 16 [400/36450]\tLoss: 552.6962\n",
      "Training Epoch: 16 [450/36450]\tLoss: 508.4419\n",
      "Training Epoch: 16 [500/36450]\tLoss: 510.2644\n",
      "Training Epoch: 16 [550/36450]\tLoss: 539.7245\n",
      "Training Epoch: 16 [600/36450]\tLoss: 553.8048\n",
      "Training Epoch: 16 [650/36450]\tLoss: 527.9016\n",
      "Training Epoch: 16 [700/36450]\tLoss: 533.2665\n",
      "Training Epoch: 16 [750/36450]\tLoss: 491.8537\n",
      "Training Epoch: 16 [800/36450]\tLoss: 479.2883\n",
      "Training Epoch: 16 [850/36450]\tLoss: 466.3629\n",
      "Training Epoch: 16 [900/36450]\tLoss: 562.8033\n",
      "Training Epoch: 16 [950/36450]\tLoss: 503.2146\n",
      "Training Epoch: 16 [1000/36450]\tLoss: 493.3365\n",
      "Training Epoch: 16 [1050/36450]\tLoss: 481.4967\n",
      "Training Epoch: 16 [1100/36450]\tLoss: 517.2419\n",
      "Training Epoch: 16 [1150/36450]\tLoss: 537.3422\n",
      "Training Epoch: 16 [1200/36450]\tLoss: 505.4913\n",
      "Training Epoch: 16 [1250/36450]\tLoss: 501.7947\n",
      "Training Epoch: 16 [1300/36450]\tLoss: 481.5554\n",
      "Training Epoch: 16 [1350/36450]\tLoss: 503.1196\n",
      "Training Epoch: 16 [1400/36450]\tLoss: 510.4630\n",
      "Training Epoch: 16 [1450/36450]\tLoss: 527.4172\n",
      "Training Epoch: 16 [1500/36450]\tLoss: 537.9352\n",
      "Training Epoch: 16 [1550/36450]\tLoss: 488.7656\n",
      "Training Epoch: 16 [1600/36450]\tLoss: 524.1903\n",
      "Training Epoch: 16 [1650/36450]\tLoss: 503.2698\n",
      "Training Epoch: 16 [1700/36450]\tLoss: 507.5558\n",
      "Training Epoch: 16 [1750/36450]\tLoss: 546.3975\n",
      "Training Epoch: 16 [1800/36450]\tLoss: 524.4648\n",
      "Training Epoch: 16 [1850/36450]\tLoss: 507.4248\n",
      "Training Epoch: 16 [1900/36450]\tLoss: 503.0535\n",
      "Training Epoch: 16 [1950/36450]\tLoss: 504.3806\n",
      "Training Epoch: 16 [2000/36450]\tLoss: 555.8768\n",
      "Training Epoch: 16 [2050/36450]\tLoss: 503.3312\n",
      "Training Epoch: 16 [2100/36450]\tLoss: 521.3228\n",
      "Training Epoch: 16 [2150/36450]\tLoss: 497.6275\n",
      "Training Epoch: 16 [2200/36450]\tLoss: 490.3218\n",
      "Training Epoch: 16 [2250/36450]\tLoss: 528.8604\n",
      "Training Epoch: 16 [2300/36450]\tLoss: 502.7789\n",
      "Training Epoch: 16 [2350/36450]\tLoss: 505.7060\n",
      "Training Epoch: 16 [2400/36450]\tLoss: 526.2485\n",
      "Training Epoch: 16 [2450/36450]\tLoss: 496.1684\n",
      "Training Epoch: 16 [2500/36450]\tLoss: 514.1325\n",
      "Training Epoch: 16 [2550/36450]\tLoss: 505.1745\n",
      "Training Epoch: 16 [2600/36450]\tLoss: 504.7322\n",
      "Training Epoch: 16 [2650/36450]\tLoss: 514.8876\n",
      "Training Epoch: 16 [2700/36450]\tLoss: 496.9615\n",
      "Training Epoch: 16 [2750/36450]\tLoss: 501.9668\n",
      "Training Epoch: 16 [2800/36450]\tLoss: 508.5563\n",
      "Training Epoch: 16 [2850/36450]\tLoss: 521.5365\n",
      "Training Epoch: 16 [2900/36450]\tLoss: 495.5565\n",
      "Training Epoch: 16 [2950/36450]\tLoss: 479.0934\n",
      "Training Epoch: 16 [3000/36450]\tLoss: 512.7614\n",
      "Training Epoch: 16 [3050/36450]\tLoss: 509.1158\n",
      "Training Epoch: 16 [3100/36450]\tLoss: 522.9346\n",
      "Training Epoch: 16 [3150/36450]\tLoss: 548.8226\n",
      "Training Epoch: 16 [3200/36450]\tLoss: 542.4540\n",
      "Training Epoch: 16 [3250/36450]\tLoss: 528.7726\n",
      "Training Epoch: 16 [3300/36450]\tLoss: 537.9501\n",
      "Training Epoch: 16 [3350/36450]\tLoss: 488.5498\n",
      "Training Epoch: 16 [3400/36450]\tLoss: 499.0445\n",
      "Training Epoch: 16 [3450/36450]\tLoss: 495.7884\n",
      "Training Epoch: 16 [3500/36450]\tLoss: 490.7116\n",
      "Training Epoch: 16 [3550/36450]\tLoss: 520.7733\n",
      "Training Epoch: 16 [3600/36450]\tLoss: 525.0087\n",
      "Training Epoch: 16 [3650/36450]\tLoss: 461.6927\n",
      "Training Epoch: 16 [3700/36450]\tLoss: 508.9786\n",
      "Training Epoch: 16 [3750/36450]\tLoss: 511.8786\n",
      "Training Epoch: 16 [3800/36450]\tLoss: 517.8167\n",
      "Training Epoch: 16 [3850/36450]\tLoss: 524.5560\n",
      "Training Epoch: 16 [3900/36450]\tLoss: 520.6701\n",
      "Training Epoch: 16 [3950/36450]\tLoss: 550.8747\n",
      "Training Epoch: 16 [4000/36450]\tLoss: 487.0935\n",
      "Training Epoch: 16 [4050/36450]\tLoss: 537.0110\n",
      "Training Epoch: 16 [4100/36450]\tLoss: 509.2887\n",
      "Training Epoch: 16 [4150/36450]\tLoss: 510.0539\n",
      "Training Epoch: 16 [4200/36450]\tLoss: 526.1514\n",
      "Training Epoch: 16 [4250/36450]\tLoss: 519.5443\n",
      "Training Epoch: 16 [4300/36450]\tLoss: 531.7182\n",
      "Training Epoch: 16 [4350/36450]\tLoss: 498.9866\n",
      "Training Epoch: 16 [4400/36450]\tLoss: 486.8269\n",
      "Training Epoch: 16 [4450/36450]\tLoss: 540.0146\n",
      "Training Epoch: 16 [4500/36450]\tLoss: 545.2912\n",
      "Training Epoch: 16 [4550/36450]\tLoss: 520.2730\n",
      "Training Epoch: 16 [4600/36450]\tLoss: 522.3314\n",
      "Training Epoch: 16 [4650/36450]\tLoss: 504.6485\n",
      "Training Epoch: 16 [4700/36450]\tLoss: 500.7477\n",
      "Training Epoch: 16 [4750/36450]\tLoss: 515.0758\n",
      "Training Epoch: 16 [4800/36450]\tLoss: 550.1860\n",
      "Training Epoch: 16 [4850/36450]\tLoss: 538.4500\n",
      "Training Epoch: 16 [4900/36450]\tLoss: 568.0804\n",
      "Training Epoch: 16 [4950/36450]\tLoss: 560.9594\n",
      "Training Epoch: 16 [5000/36450]\tLoss: 584.3478\n",
      "Training Epoch: 16 [5050/36450]\tLoss: 595.5772\n",
      "Training Epoch: 16 [5100/36450]\tLoss: 575.9191\n",
      "Training Epoch: 16 [5150/36450]\tLoss: 594.7524\n",
      "Training Epoch: 16 [5200/36450]\tLoss: 525.4244\n",
      "Training Epoch: 16 [5250/36450]\tLoss: 519.0853\n",
      "Training Epoch: 16 [5300/36450]\tLoss: 530.7807\n",
      "Training Epoch: 16 [5350/36450]\tLoss: 514.9608\n",
      "Training Epoch: 16 [5400/36450]\tLoss: 560.2464\n",
      "Training Epoch: 16 [5450/36450]\tLoss: 573.1815\n",
      "Training Epoch: 16 [5500/36450]\tLoss: 545.7239\n",
      "Training Epoch: 16 [5550/36450]\tLoss: 524.0098\n",
      "Training Epoch: 16 [5600/36450]\tLoss: 501.3585\n",
      "Training Epoch: 16 [5650/36450]\tLoss: 525.8060\n",
      "Training Epoch: 16 [5700/36450]\tLoss: 512.3871\n",
      "Training Epoch: 16 [5750/36450]\tLoss: 538.6008\n",
      "Training Epoch: 16 [5800/36450]\tLoss: 516.1277\n",
      "Training Epoch: 16 [5850/36450]\tLoss: 511.7933\n",
      "Training Epoch: 16 [5900/36450]\tLoss: 518.6022\n",
      "Training Epoch: 16 [5950/36450]\tLoss: 482.8220\n",
      "Training Epoch: 16 [6000/36450]\tLoss: 508.7754\n",
      "Training Epoch: 16 [6050/36450]\tLoss: 509.5872\n",
      "Training Epoch: 16 [6100/36450]\tLoss: 527.6977\n",
      "Training Epoch: 16 [6150/36450]\tLoss: 519.3758\n",
      "Training Epoch: 16 [6200/36450]\tLoss: 556.7415\n",
      "Training Epoch: 16 [6250/36450]\tLoss: 501.1747\n",
      "Training Epoch: 16 [6300/36450]\tLoss: 517.1176\n",
      "Training Epoch: 16 [6350/36450]\tLoss: 535.2263\n",
      "Training Epoch: 16 [6400/36450]\tLoss: 520.8342\n",
      "Training Epoch: 16 [6450/36450]\tLoss: 519.2328\n",
      "Training Epoch: 16 [6500/36450]\tLoss: 529.7239\n",
      "Training Epoch: 16 [6550/36450]\tLoss: 507.3139\n",
      "Training Epoch: 16 [6600/36450]\tLoss: 540.7502\n",
      "Training Epoch: 16 [6650/36450]\tLoss: 509.9321\n",
      "Training Epoch: 16 [6700/36450]\tLoss: 519.8452\n",
      "Training Epoch: 16 [6750/36450]\tLoss: 504.5215\n",
      "Training Epoch: 16 [6800/36450]\tLoss: 535.1997\n",
      "Training Epoch: 16 [6850/36450]\tLoss: 488.0727\n",
      "Training Epoch: 16 [6900/36450]\tLoss: 522.9853\n",
      "Training Epoch: 16 [6950/36450]\tLoss: 519.1833\n",
      "Training Epoch: 16 [7000/36450]\tLoss: 513.3586\n",
      "Training Epoch: 16 [7050/36450]\tLoss: 536.6936\n",
      "Training Epoch: 16 [7100/36450]\tLoss: 491.0726\n",
      "Training Epoch: 16 [7150/36450]\tLoss: 493.9588\n",
      "Training Epoch: 16 [7200/36450]\tLoss: 508.9728\n",
      "Training Epoch: 16 [7250/36450]\tLoss: 501.4901\n",
      "Training Epoch: 16 [7300/36450]\tLoss: 513.7314\n",
      "Training Epoch: 16 [7350/36450]\tLoss: 521.2834\n",
      "Training Epoch: 16 [7400/36450]\tLoss: 515.4494\n",
      "Training Epoch: 16 [7450/36450]\tLoss: 527.5477\n",
      "Training Epoch: 16 [7500/36450]\tLoss: 509.9455\n",
      "Training Epoch: 16 [7550/36450]\tLoss: 528.0845\n",
      "Training Epoch: 16 [7600/36450]\tLoss: 509.3060\n",
      "Training Epoch: 16 [7650/36450]\tLoss: 511.6437\n",
      "Training Epoch: 16 [7700/36450]\tLoss: 507.4208\n",
      "Training Epoch: 16 [7750/36450]\tLoss: 506.0664\n",
      "Training Epoch: 16 [7800/36450]\tLoss: 521.9016\n",
      "Training Epoch: 16 [7850/36450]\tLoss: 490.9366\n",
      "Training Epoch: 16 [7900/36450]\tLoss: 518.9786\n",
      "Training Epoch: 16 [7950/36450]\tLoss: 500.9995\n",
      "Training Epoch: 16 [8000/36450]\tLoss: 504.0266\n",
      "Training Epoch: 16 [8050/36450]\tLoss: 491.4208\n",
      "Training Epoch: 16 [8100/36450]\tLoss: 503.6013\n",
      "Training Epoch: 16 [8150/36450]\tLoss: 494.5312\n",
      "Training Epoch: 16 [8200/36450]\tLoss: 507.3909\n",
      "Training Epoch: 16 [8250/36450]\tLoss: 491.6309\n",
      "Training Epoch: 16 [8300/36450]\tLoss: 508.0723\n",
      "Training Epoch: 16 [8350/36450]\tLoss: 541.9713\n",
      "Training Epoch: 16 [8400/36450]\tLoss: 487.6101\n",
      "Training Epoch: 16 [8450/36450]\tLoss: 514.2803\n",
      "Training Epoch: 16 [8500/36450]\tLoss: 493.7632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [8550/36450]\tLoss: 495.8939\n",
      "Training Epoch: 16 [8600/36450]\tLoss: 514.3594\n",
      "Training Epoch: 16 [8650/36450]\tLoss: 511.2447\n",
      "Training Epoch: 16 [8700/36450]\tLoss: 523.5269\n",
      "Training Epoch: 16 [8750/36450]\tLoss: 508.1243\n",
      "Training Epoch: 16 [8800/36450]\tLoss: 538.2363\n",
      "Training Epoch: 16 [8850/36450]\tLoss: 517.1938\n",
      "Training Epoch: 16 [8900/36450]\tLoss: 520.3161\n",
      "Training Epoch: 16 [8950/36450]\tLoss: 517.8348\n",
      "Training Epoch: 16 [9000/36450]\tLoss: 494.0570\n",
      "Training Epoch: 16 [9050/36450]\tLoss: 524.0150\n",
      "Training Epoch: 16 [9100/36450]\tLoss: 506.4094\n",
      "Training Epoch: 16 [9150/36450]\tLoss: 498.1039\n",
      "Training Epoch: 16 [9200/36450]\tLoss: 493.6737\n",
      "Training Epoch: 16 [9250/36450]\tLoss: 496.7005\n",
      "Training Epoch: 16 [9300/36450]\tLoss: 479.3063\n",
      "Training Epoch: 16 [9350/36450]\tLoss: 483.0860\n",
      "Training Epoch: 16 [9400/36450]\tLoss: 516.7697\n",
      "Training Epoch: 16 [9450/36450]\tLoss: 511.3620\n",
      "Training Epoch: 16 [9500/36450]\tLoss: 537.6333\n",
      "Training Epoch: 16 [9550/36450]\tLoss: 482.9499\n",
      "Training Epoch: 16 [9600/36450]\tLoss: 508.4159\n",
      "Training Epoch: 16 [9650/36450]\tLoss: 540.9965\n",
      "Training Epoch: 16 [9700/36450]\tLoss: 517.0472\n",
      "Training Epoch: 16 [9750/36450]\tLoss: 529.1959\n",
      "Training Epoch: 16 [9800/36450]\tLoss: 530.9357\n",
      "Training Epoch: 16 [9850/36450]\tLoss: 518.9051\n",
      "Training Epoch: 16 [9900/36450]\tLoss: 528.1182\n",
      "Training Epoch: 16 [9950/36450]\tLoss: 507.5820\n",
      "Training Epoch: 16 [10000/36450]\tLoss: 524.0754\n",
      "Training Epoch: 16 [10050/36450]\tLoss: 520.4523\n",
      "Training Epoch: 16 [10100/36450]\tLoss: 469.6268\n",
      "Training Epoch: 16 [10150/36450]\tLoss: 502.1827\n",
      "Training Epoch: 16 [10200/36450]\tLoss: 510.4293\n",
      "Training Epoch: 16 [10250/36450]\tLoss: 488.6901\n",
      "Training Epoch: 16 [10300/36450]\tLoss: 494.1970\n",
      "Training Epoch: 16 [10350/36450]\tLoss: 497.2025\n",
      "Training Epoch: 16 [10400/36450]\tLoss: 504.9247\n",
      "Training Epoch: 16 [10450/36450]\tLoss: 512.1660\n",
      "Training Epoch: 16 [10500/36450]\tLoss: 521.4672\n",
      "Training Epoch: 16 [10550/36450]\tLoss: 514.4630\n",
      "Training Epoch: 16 [10600/36450]\tLoss: 502.7327\n",
      "Training Epoch: 16 [10650/36450]\tLoss: 493.7972\n",
      "Training Epoch: 16 [10700/36450]\tLoss: 521.3353\n",
      "Training Epoch: 16 [10750/36450]\tLoss: 494.6114\n",
      "Training Epoch: 16 [10800/36450]\tLoss: 515.2396\n",
      "Training Epoch: 16 [10850/36450]\tLoss: 521.8734\n",
      "Training Epoch: 16 [10900/36450]\tLoss: 479.3658\n",
      "Training Epoch: 16 [10950/36450]\tLoss: 532.1366\n",
      "Training Epoch: 16 [11000/36450]\tLoss: 521.3497\n",
      "Training Epoch: 16 [11050/36450]\tLoss: 537.6469\n",
      "Training Epoch: 16 [11100/36450]\tLoss: 498.4845\n",
      "Training Epoch: 16 [11150/36450]\tLoss: 508.6405\n",
      "Training Epoch: 16 [11200/36450]\tLoss: 503.0834\n",
      "Training Epoch: 16 [11250/36450]\tLoss: 488.3161\n",
      "Training Epoch: 16 [11300/36450]\tLoss: 514.7025\n",
      "Training Epoch: 16 [11350/36450]\tLoss: 495.3514\n",
      "Training Epoch: 16 [11400/36450]\tLoss: 468.9108\n",
      "Training Epoch: 16 [11450/36450]\tLoss: 497.9148\n",
      "Training Epoch: 16 [11500/36450]\tLoss: 500.2623\n",
      "Training Epoch: 16 [11550/36450]\tLoss: 520.4905\n",
      "Training Epoch: 16 [11600/36450]\tLoss: 524.9534\n",
      "Training Epoch: 16 [11650/36450]\tLoss: 541.0747\n",
      "Training Epoch: 16 [11700/36450]\tLoss: 511.8301\n",
      "Training Epoch: 16 [11750/36450]\tLoss: 535.8828\n",
      "Training Epoch: 16 [11800/36450]\tLoss: 510.4819\n",
      "Training Epoch: 16 [11850/36450]\tLoss: 473.2552\n",
      "Training Epoch: 16 [11900/36450]\tLoss: 500.8592\n",
      "Training Epoch: 16 [11950/36450]\tLoss: 520.5515\n",
      "Training Epoch: 16 [12000/36450]\tLoss: 479.7326\n",
      "Training Epoch: 16 [12050/36450]\tLoss: 505.0693\n",
      "Training Epoch: 16 [12100/36450]\tLoss: 509.3159\n",
      "Training Epoch: 16 [12150/36450]\tLoss: 518.2034\n",
      "Training Epoch: 16 [12200/36450]\tLoss: 506.7173\n",
      "Training Epoch: 16 [12250/36450]\tLoss: 518.9413\n",
      "Training Epoch: 16 [12300/36450]\tLoss: 537.1581\n",
      "Training Epoch: 16 [12350/36450]\tLoss: 496.8453\n",
      "Training Epoch: 16 [12400/36450]\tLoss: 528.4512\n",
      "Training Epoch: 16 [12450/36450]\tLoss: 472.6791\n",
      "Training Epoch: 16 [12500/36450]\tLoss: 532.7719\n",
      "Training Epoch: 16 [12550/36450]\tLoss: 570.8545\n",
      "Training Epoch: 16 [12600/36450]\tLoss: 527.8417\n",
      "Training Epoch: 16 [12650/36450]\tLoss: 527.7386\n",
      "Training Epoch: 16 [12700/36450]\tLoss: 529.9620\n",
      "Training Epoch: 16 [12750/36450]\tLoss: 512.2152\n",
      "Training Epoch: 16 [12800/36450]\tLoss: 535.1448\n",
      "Training Epoch: 16 [12850/36450]\tLoss: 477.3968\n",
      "Training Epoch: 16 [12900/36450]\tLoss: 498.3659\n",
      "Training Epoch: 16 [12950/36450]\tLoss: 506.2074\n",
      "Training Epoch: 16 [13000/36450]\tLoss: 534.2599\n",
      "Training Epoch: 16 [13050/36450]\tLoss: 514.0782\n",
      "Training Epoch: 16 [13100/36450]\tLoss: 578.1569\n",
      "Training Epoch: 16 [13150/36450]\tLoss: 471.5472\n",
      "Training Epoch: 16 [13200/36450]\tLoss: 499.3165\n",
      "Training Epoch: 16 [13250/36450]\tLoss: 501.8293\n",
      "Training Epoch: 16 [13300/36450]\tLoss: 516.8264\n",
      "Training Epoch: 16 [13350/36450]\tLoss: 490.5945\n",
      "Training Epoch: 16 [13400/36450]\tLoss: 526.2930\n",
      "Training Epoch: 16 [13450/36450]\tLoss: 521.2604\n",
      "Training Epoch: 16 [13500/36450]\tLoss: 514.2262\n",
      "Training Epoch: 16 [13550/36450]\tLoss: 506.9026\n",
      "Training Epoch: 16 [13600/36450]\tLoss: 506.8723\n",
      "Training Epoch: 16 [13650/36450]\tLoss: 499.5409\n",
      "Training Epoch: 16 [13700/36450]\tLoss: 496.9529\n",
      "Training Epoch: 16 [13750/36450]\tLoss: 505.6754\n",
      "Training Epoch: 16 [13800/36450]\tLoss: 530.0507\n",
      "Training Epoch: 16 [13850/36450]\tLoss: 488.6457\n",
      "Training Epoch: 16 [13900/36450]\tLoss: 492.0549\n",
      "Training Epoch: 16 [13950/36450]\tLoss: 498.0702\n",
      "Training Epoch: 16 [14000/36450]\tLoss: 488.4409\n",
      "Training Epoch: 16 [14050/36450]\tLoss: 486.1901\n",
      "Training Epoch: 16 [14100/36450]\tLoss: 490.2380\n",
      "Training Epoch: 16 [14150/36450]\tLoss: 523.2086\n",
      "Training Epoch: 16 [14200/36450]\tLoss: 509.9538\n",
      "Training Epoch: 16 [14250/36450]\tLoss: 517.4852\n",
      "Training Epoch: 16 [14300/36450]\tLoss: 511.3150\n",
      "Training Epoch: 16 [14350/36450]\tLoss: 501.8860\n",
      "Training Epoch: 16 [14400/36450]\tLoss: 547.3205\n",
      "Training Epoch: 16 [14450/36450]\tLoss: 521.5006\n",
      "Training Epoch: 16 [14500/36450]\tLoss: 513.1237\n",
      "Training Epoch: 16 [14550/36450]\tLoss: 473.0945\n",
      "Training Epoch: 16 [14600/36450]\tLoss: 547.9359\n",
      "Training Epoch: 16 [14650/36450]\tLoss: 509.6189\n",
      "Training Epoch: 16 [14700/36450]\tLoss: 503.7173\n",
      "Training Epoch: 16 [14750/36450]\tLoss: 508.9344\n",
      "Training Epoch: 16 [14800/36450]\tLoss: 491.9441\n",
      "Training Epoch: 16 [14850/36450]\tLoss: 514.3734\n",
      "Training Epoch: 16 [14900/36450]\tLoss: 493.2682\n",
      "Training Epoch: 16 [14950/36450]\tLoss: 545.7435\n",
      "Training Epoch: 16 [15000/36450]\tLoss: 496.0517\n",
      "Training Epoch: 16 [15050/36450]\tLoss: 509.1347\n",
      "Training Epoch: 16 [15100/36450]\tLoss: 529.5319\n",
      "Training Epoch: 16 [15150/36450]\tLoss: 549.0671\n",
      "Training Epoch: 16 [15200/36450]\tLoss: 505.5600\n",
      "Training Epoch: 16 [15250/36450]\tLoss: 559.2153\n",
      "Training Epoch: 16 [15300/36450]\tLoss: 548.9956\n",
      "Training Epoch: 16 [15350/36450]\tLoss: 564.6998\n",
      "Training Epoch: 16 [15400/36450]\tLoss: 559.9675\n",
      "Training Epoch: 16 [15450/36450]\tLoss: 556.3060\n",
      "Training Epoch: 16 [15500/36450]\tLoss: 562.2220\n",
      "Training Epoch: 16 [15550/36450]\tLoss: 516.9339\n",
      "Training Epoch: 16 [15600/36450]\tLoss: 532.9048\n",
      "Training Epoch: 16 [15650/36450]\tLoss: 513.3951\n",
      "Training Epoch: 16 [15700/36450]\tLoss: 527.3859\n",
      "Training Epoch: 16 [15750/36450]\tLoss: 512.5840\n",
      "Training Epoch: 16 [15800/36450]\tLoss: 572.8203\n",
      "Training Epoch: 16 [15850/36450]\tLoss: 541.7900\n",
      "Training Epoch: 16 [15900/36450]\tLoss: 556.9044\n",
      "Training Epoch: 16 [15950/36450]\tLoss: 543.3357\n",
      "Training Epoch: 16 [16000/36450]\tLoss: 550.1807\n",
      "Training Epoch: 16 [16050/36450]\tLoss: 494.9286\n",
      "Training Epoch: 16 [16100/36450]\tLoss: 485.2856\n",
      "Training Epoch: 16 [16150/36450]\tLoss: 533.3789\n",
      "Training Epoch: 16 [16200/36450]\tLoss: 511.2150\n",
      "Training Epoch: 16 [16250/36450]\tLoss: 554.7919\n",
      "Training Epoch: 16 [16300/36450]\tLoss: 547.1313\n",
      "Training Epoch: 16 [16350/36450]\tLoss: 560.5583\n",
      "Training Epoch: 16 [16400/36450]\tLoss: 513.4109\n",
      "Training Epoch: 16 [16450/36450]\tLoss: 514.1164\n",
      "Training Epoch: 16 [16500/36450]\tLoss: 516.7982\n",
      "Training Epoch: 16 [16550/36450]\tLoss: 488.4301\n",
      "Training Epoch: 16 [16600/36450]\tLoss: 525.9070\n",
      "Training Epoch: 16 [16650/36450]\tLoss: 541.1149\n",
      "Training Epoch: 16 [16700/36450]\tLoss: 514.1826\n",
      "Training Epoch: 16 [16750/36450]\tLoss: 486.5247\n",
      "Training Epoch: 16 [16800/36450]\tLoss: 519.7541\n",
      "Training Epoch: 16 [16850/36450]\tLoss: 523.0706\n",
      "Training Epoch: 16 [16900/36450]\tLoss: 534.2251\n",
      "Training Epoch: 16 [16950/36450]\tLoss: 514.9259\n",
      "Training Epoch: 16 [17000/36450]\tLoss: 533.1123\n",
      "Training Epoch: 16 [17050/36450]\tLoss: 520.7900\n",
      "Training Epoch: 16 [17100/36450]\tLoss: 526.1025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [17150/36450]\tLoss: 545.4904\n",
      "Training Epoch: 16 [17200/36450]\tLoss: 496.4690\n",
      "Training Epoch: 16 [17250/36450]\tLoss: 495.6130\n",
      "Training Epoch: 16 [17300/36450]\tLoss: 513.1958\n",
      "Training Epoch: 16 [17350/36450]\tLoss: 500.9290\n",
      "Training Epoch: 16 [17400/36450]\tLoss: 524.5541\n",
      "Training Epoch: 16 [17450/36450]\tLoss: 495.7085\n",
      "Training Epoch: 16 [17500/36450]\tLoss: 475.3011\n",
      "Training Epoch: 16 [17550/36450]\tLoss: 487.2753\n",
      "Training Epoch: 16 [17600/36450]\tLoss: 543.7449\n",
      "Training Epoch: 16 [17650/36450]\tLoss: 505.4053\n",
      "Training Epoch: 16 [17700/36450]\tLoss: 490.3364\n",
      "Training Epoch: 16 [17750/36450]\tLoss: 479.4472\n",
      "Training Epoch: 16 [17800/36450]\tLoss: 516.9644\n",
      "Training Epoch: 16 [17850/36450]\tLoss: 500.8931\n",
      "Training Epoch: 16 [17900/36450]\tLoss: 489.5876\n",
      "Training Epoch: 16 [17950/36450]\tLoss: 525.6985\n",
      "Training Epoch: 16 [18000/36450]\tLoss: 485.1676\n",
      "Training Epoch: 16 [18050/36450]\tLoss: 528.0180\n",
      "Training Epoch: 16 [18100/36450]\tLoss: 523.9355\n",
      "Training Epoch: 16 [18150/36450]\tLoss: 507.3903\n",
      "Training Epoch: 16 [18200/36450]\tLoss: 547.6993\n",
      "Training Epoch: 16 [18250/36450]\tLoss: 520.8921\n",
      "Training Epoch: 16 [18300/36450]\tLoss: 518.0175\n",
      "Training Epoch: 16 [18350/36450]\tLoss: 479.8891\n",
      "Training Epoch: 16 [18400/36450]\tLoss: 455.2022\n",
      "Training Epoch: 16 [18450/36450]\tLoss: 517.7330\n",
      "Training Epoch: 16 [18500/36450]\tLoss: 492.3412\n",
      "Training Epoch: 16 [18550/36450]\tLoss: 507.3130\n",
      "Training Epoch: 16 [18600/36450]\tLoss: 497.0008\n",
      "Training Epoch: 16 [18650/36450]\tLoss: 512.1384\n",
      "Training Epoch: 16 [18700/36450]\tLoss: 551.4167\n",
      "Training Epoch: 16 [18750/36450]\tLoss: 507.2178\n",
      "Training Epoch: 16 [18800/36450]\tLoss: 520.5126\n",
      "Training Epoch: 16 [18850/36450]\tLoss: 487.8018\n",
      "Training Epoch: 16 [18900/36450]\tLoss: 526.2916\n",
      "Training Epoch: 16 [18950/36450]\tLoss: 490.3874\n",
      "Training Epoch: 16 [19000/36450]\tLoss: 509.6250\n",
      "Training Epoch: 16 [19050/36450]\tLoss: 467.6783\n",
      "Training Epoch: 16 [19100/36450]\tLoss: 519.8550\n",
      "Training Epoch: 16 [19150/36450]\tLoss: 545.0104\n",
      "Training Epoch: 16 [19200/36450]\tLoss: 508.5747\n",
      "Training Epoch: 16 [19250/36450]\tLoss: 481.3843\n",
      "Training Epoch: 16 [19300/36450]\tLoss: 538.4938\n",
      "Training Epoch: 16 [19350/36450]\tLoss: 525.5990\n",
      "Training Epoch: 16 [19400/36450]\tLoss: 461.4562\n",
      "Training Epoch: 16 [19450/36450]\tLoss: 509.8791\n",
      "Training Epoch: 16 [19500/36450]\tLoss: 504.6753\n",
      "Training Epoch: 16 [19550/36450]\tLoss: 533.9744\n",
      "Training Epoch: 16 [19600/36450]\tLoss: 510.6362\n",
      "Training Epoch: 16 [19650/36450]\tLoss: 530.4893\n",
      "Training Epoch: 16 [19700/36450]\tLoss: 511.0366\n",
      "Training Epoch: 16 [19750/36450]\tLoss: 525.3834\n",
      "Training Epoch: 16 [19800/36450]\tLoss: 504.4627\n",
      "Training Epoch: 16 [19850/36450]\tLoss: 495.9226\n",
      "Training Epoch: 16 [19900/36450]\tLoss: 533.3265\n",
      "Training Epoch: 16 [19950/36450]\tLoss: 481.7791\n",
      "Training Epoch: 16 [20000/36450]\tLoss: 526.4759\n",
      "Training Epoch: 16 [20050/36450]\tLoss: 514.6505\n",
      "Training Epoch: 16 [20100/36450]\tLoss: 533.1962\n",
      "Training Epoch: 16 [20150/36450]\tLoss: 519.6924\n",
      "Training Epoch: 16 [20200/36450]\tLoss: 527.7471\n",
      "Training Epoch: 16 [20250/36450]\tLoss: 480.1516\n",
      "Training Epoch: 16 [20300/36450]\tLoss: 473.0932\n",
      "Training Epoch: 16 [20350/36450]\tLoss: 500.4489\n",
      "Training Epoch: 16 [20400/36450]\tLoss: 484.6882\n",
      "Training Epoch: 16 [20450/36450]\tLoss: 499.5918\n",
      "Training Epoch: 16 [20500/36450]\tLoss: 517.0067\n",
      "Training Epoch: 16 [20550/36450]\tLoss: 539.1353\n",
      "Training Epoch: 16 [20600/36450]\tLoss: 486.7523\n",
      "Training Epoch: 16 [20650/36450]\tLoss: 516.4131\n",
      "Training Epoch: 16 [20700/36450]\tLoss: 543.8400\n",
      "Training Epoch: 16 [20750/36450]\tLoss: 504.8224\n",
      "Training Epoch: 16 [20800/36450]\tLoss: 511.9577\n",
      "Training Epoch: 16 [20850/36450]\tLoss: 496.0786\n",
      "Training Epoch: 16 [20900/36450]\tLoss: 529.8414\n",
      "Training Epoch: 16 [20950/36450]\tLoss: 495.7238\n",
      "Training Epoch: 16 [21000/36450]\tLoss: 489.8127\n",
      "Training Epoch: 16 [21050/36450]\tLoss: 509.0963\n",
      "Training Epoch: 16 [21100/36450]\tLoss: 510.4525\n",
      "Training Epoch: 16 [21150/36450]\tLoss: 493.3287\n",
      "Training Epoch: 16 [21200/36450]\tLoss: 483.4178\n",
      "Training Epoch: 16 [21250/36450]\tLoss: 504.5003\n",
      "Training Epoch: 16 [21300/36450]\tLoss: 520.1627\n",
      "Training Epoch: 16 [21350/36450]\tLoss: 512.1882\n",
      "Training Epoch: 16 [21400/36450]\tLoss: 472.5216\n",
      "Training Epoch: 16 [21450/36450]\tLoss: 505.4731\n",
      "Training Epoch: 16 [21500/36450]\tLoss: 476.5409\n",
      "Training Epoch: 16 [21550/36450]\tLoss: 483.2238\n",
      "Training Epoch: 16 [21600/36450]\tLoss: 524.9867\n",
      "Training Epoch: 16 [21650/36450]\tLoss: 521.2627\n",
      "Training Epoch: 16 [21700/36450]\tLoss: 477.9010\n",
      "Training Epoch: 16 [21750/36450]\tLoss: 526.6770\n",
      "Training Epoch: 16 [21800/36450]\tLoss: 547.6132\n",
      "Training Epoch: 16 [21850/36450]\tLoss: 507.5856\n",
      "Training Epoch: 16 [21900/36450]\tLoss: 486.7416\n",
      "Training Epoch: 16 [21950/36450]\tLoss: 512.2039\n",
      "Training Epoch: 16 [22000/36450]\tLoss: 505.5899\n",
      "Training Epoch: 16 [22050/36450]\tLoss: 483.4118\n",
      "Training Epoch: 16 [22100/36450]\tLoss: 510.5809\n",
      "Training Epoch: 16 [22150/36450]\tLoss: 508.0409\n",
      "Training Epoch: 16 [22200/36450]\tLoss: 489.8144\n",
      "Training Epoch: 16 [22250/36450]\tLoss: 487.1336\n",
      "Training Epoch: 16 [22300/36450]\tLoss: 498.6878\n",
      "Training Epoch: 16 [22350/36450]\tLoss: 511.9621\n",
      "Training Epoch: 16 [22400/36450]\tLoss: 485.8738\n",
      "Training Epoch: 16 [22450/36450]\tLoss: 506.5802\n",
      "Training Epoch: 16 [22500/36450]\tLoss: 491.7082\n",
      "Training Epoch: 16 [22550/36450]\tLoss: 514.4985\n",
      "Training Epoch: 16 [22600/36450]\tLoss: 506.5233\n",
      "Training Epoch: 16 [22650/36450]\tLoss: 498.8823\n",
      "Training Epoch: 16 [22700/36450]\tLoss: 523.4875\n",
      "Training Epoch: 16 [22750/36450]\tLoss: 526.4097\n",
      "Training Epoch: 16 [22800/36450]\tLoss: 503.5208\n",
      "Training Epoch: 16 [22850/36450]\tLoss: 524.1332\n",
      "Training Epoch: 16 [22900/36450]\tLoss: 510.3757\n",
      "Training Epoch: 16 [22950/36450]\tLoss: 520.3611\n",
      "Training Epoch: 16 [23000/36450]\tLoss: 529.5972\n",
      "Training Epoch: 16 [23050/36450]\tLoss: 466.1365\n",
      "Training Epoch: 16 [23100/36450]\tLoss: 526.0493\n",
      "Training Epoch: 16 [23150/36450]\tLoss: 464.7945\n",
      "Training Epoch: 16 [23200/36450]\tLoss: 472.6557\n",
      "Training Epoch: 16 [23250/36450]\tLoss: 516.1511\n",
      "Training Epoch: 16 [23300/36450]\tLoss: 502.9998\n",
      "Training Epoch: 16 [23350/36450]\tLoss: 527.2996\n",
      "Training Epoch: 16 [23400/36450]\tLoss: 509.0511\n",
      "Training Epoch: 16 [23450/36450]\tLoss: 525.9980\n",
      "Training Epoch: 16 [23500/36450]\tLoss: 502.1842\n",
      "Training Epoch: 16 [23550/36450]\tLoss: 489.4305\n",
      "Training Epoch: 16 [23600/36450]\tLoss: 530.3591\n",
      "Training Epoch: 16 [23650/36450]\tLoss: 532.8555\n",
      "Training Epoch: 16 [23700/36450]\tLoss: 501.5305\n",
      "Training Epoch: 16 [23750/36450]\tLoss: 543.6101\n",
      "Training Epoch: 16 [23800/36450]\tLoss: 555.3799\n",
      "Training Epoch: 16 [23850/36450]\tLoss: 541.0428\n",
      "Training Epoch: 16 [23900/36450]\tLoss: 504.5906\n",
      "Training Epoch: 16 [23950/36450]\tLoss: 535.4725\n",
      "Training Epoch: 16 [24000/36450]\tLoss: 552.5615\n",
      "Training Epoch: 16 [24050/36450]\tLoss: 526.0081\n",
      "Training Epoch: 16 [24100/36450]\tLoss: 517.8860\n",
      "Training Epoch: 16 [24150/36450]\tLoss: 497.9809\n",
      "Training Epoch: 16 [24200/36450]\tLoss: 525.0703\n",
      "Training Epoch: 16 [24250/36450]\tLoss: 517.5864\n",
      "Training Epoch: 16 [24300/36450]\tLoss: 539.7650\n",
      "Training Epoch: 16 [24350/36450]\tLoss: 511.4735\n",
      "Training Epoch: 16 [24400/36450]\tLoss: 510.1713\n",
      "Training Epoch: 16 [24450/36450]\tLoss: 524.6114\n",
      "Training Epoch: 16 [24500/36450]\tLoss: 526.4917\n",
      "Training Epoch: 16 [24550/36450]\tLoss: 489.7289\n",
      "Training Epoch: 16 [24600/36450]\tLoss: 475.6483\n",
      "Training Epoch: 16 [24650/36450]\tLoss: 500.3189\n",
      "Training Epoch: 16 [24700/36450]\tLoss: 522.8362\n",
      "Training Epoch: 16 [24750/36450]\tLoss: 513.6137\n",
      "Training Epoch: 16 [24800/36450]\tLoss: 527.6185\n",
      "Training Epoch: 16 [24850/36450]\tLoss: 476.3882\n",
      "Training Epoch: 16 [24900/36450]\tLoss: 470.6029\n",
      "Training Epoch: 16 [24950/36450]\tLoss: 487.9871\n",
      "Training Epoch: 16 [25000/36450]\tLoss: 496.7989\n",
      "Training Epoch: 16 [25050/36450]\tLoss: 510.0791\n",
      "Training Epoch: 16 [25100/36450]\tLoss: 509.9601\n",
      "Training Epoch: 16 [25150/36450]\tLoss: 516.2048\n",
      "Training Epoch: 16 [25200/36450]\tLoss: 521.2900\n",
      "Training Epoch: 16 [25250/36450]\tLoss: 514.7424\n",
      "Training Epoch: 16 [25300/36450]\tLoss: 506.6953\n",
      "Training Epoch: 16 [25350/36450]\tLoss: 510.7746\n",
      "Training Epoch: 16 [25400/36450]\tLoss: 548.1270\n",
      "Training Epoch: 16 [25450/36450]\tLoss: 505.8956\n",
      "Training Epoch: 16 [25500/36450]\tLoss: 490.9571\n",
      "Training Epoch: 16 [25550/36450]\tLoss: 513.5289\n",
      "Training Epoch: 16 [25600/36450]\tLoss: 489.9925\n",
      "Training Epoch: 16 [25650/36450]\tLoss: 483.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [25700/36450]\tLoss: 506.6885\n",
      "Training Epoch: 16 [25750/36450]\tLoss: 501.5140\n",
      "Training Epoch: 16 [25800/36450]\tLoss: 505.7288\n",
      "Training Epoch: 16 [25850/36450]\tLoss: 526.6934\n",
      "Training Epoch: 16 [25900/36450]\tLoss: 536.4338\n",
      "Training Epoch: 16 [25950/36450]\tLoss: 526.8137\n",
      "Training Epoch: 16 [26000/36450]\tLoss: 499.6725\n",
      "Training Epoch: 16 [26050/36450]\tLoss: 527.3746\n",
      "Training Epoch: 16 [26100/36450]\tLoss: 509.9762\n",
      "Training Epoch: 16 [26150/36450]\tLoss: 493.7218\n",
      "Training Epoch: 16 [26200/36450]\tLoss: 503.9474\n",
      "Training Epoch: 16 [26250/36450]\tLoss: 503.6888\n",
      "Training Epoch: 16 [26300/36450]\tLoss: 529.8992\n",
      "Training Epoch: 16 [26350/36450]\tLoss: 517.8983\n",
      "Training Epoch: 16 [26400/36450]\tLoss: 481.9121\n",
      "Training Epoch: 16 [26450/36450]\tLoss: 500.9496\n",
      "Training Epoch: 16 [26500/36450]\tLoss: 506.4877\n",
      "Training Epoch: 16 [26550/36450]\tLoss: 500.4674\n",
      "Training Epoch: 16 [26600/36450]\tLoss: 507.4227\n",
      "Training Epoch: 16 [26650/36450]\tLoss: 520.8538\n",
      "Training Epoch: 16 [26700/36450]\tLoss: 529.9271\n",
      "Training Epoch: 16 [26750/36450]\tLoss: 513.2812\n",
      "Training Epoch: 16 [26800/36450]\tLoss: 526.6372\n",
      "Training Epoch: 16 [26850/36450]\tLoss: 491.5737\n",
      "Training Epoch: 16 [26900/36450]\tLoss: 499.8030\n",
      "Training Epoch: 16 [26950/36450]\tLoss: 477.6288\n",
      "Training Epoch: 16 [27000/36450]\tLoss: 556.7758\n",
      "Training Epoch: 16 [27050/36450]\tLoss: 536.4869\n",
      "Training Epoch: 16 [27100/36450]\tLoss: 489.3376\n",
      "Training Epoch: 16 [27150/36450]\tLoss: 539.0876\n",
      "Training Epoch: 16 [27200/36450]\tLoss: 532.5911\n",
      "Training Epoch: 16 [27250/36450]\tLoss: 548.1599\n",
      "Training Epoch: 16 [27300/36450]\tLoss: 525.0848\n",
      "Training Epoch: 16 [27350/36450]\tLoss: 509.4835\n",
      "Training Epoch: 16 [27400/36450]\tLoss: 501.1210\n",
      "Training Epoch: 16 [27450/36450]\tLoss: 483.1730\n",
      "Training Epoch: 16 [27500/36450]\tLoss: 503.9947\n",
      "Training Epoch: 16 [27550/36450]\tLoss: 510.2981\n",
      "Training Epoch: 16 [27600/36450]\tLoss: 485.8464\n",
      "Training Epoch: 16 [27650/36450]\tLoss: 492.3380\n",
      "Training Epoch: 16 [27700/36450]\tLoss: 502.3978\n",
      "Training Epoch: 16 [27750/36450]\tLoss: 517.4850\n",
      "Training Epoch: 16 [27800/36450]\tLoss: 497.4577\n",
      "Training Epoch: 16 [27850/36450]\tLoss: 530.2444\n",
      "Training Epoch: 16 [27900/36450]\tLoss: 516.8427\n",
      "Training Epoch: 16 [27950/36450]\tLoss: 489.7174\n",
      "Training Epoch: 16 [28000/36450]\tLoss: 507.2839\n",
      "Training Epoch: 16 [28050/36450]\tLoss: 500.1435\n",
      "Training Epoch: 16 [28100/36450]\tLoss: 508.5111\n",
      "Training Epoch: 16 [28150/36450]\tLoss: 491.8600\n",
      "Training Epoch: 16 [28200/36450]\tLoss: 495.8478\n",
      "Training Epoch: 16 [28250/36450]\tLoss: 545.8630\n",
      "Training Epoch: 16 [28300/36450]\tLoss: 497.7480\n",
      "Training Epoch: 16 [28350/36450]\tLoss: 485.6821\n",
      "Training Epoch: 16 [28400/36450]\tLoss: 491.3627\n",
      "Training Epoch: 16 [28450/36450]\tLoss: 494.7506\n",
      "Training Epoch: 16 [28500/36450]\tLoss: 520.5419\n",
      "Training Epoch: 16 [28550/36450]\tLoss: 506.9120\n",
      "Training Epoch: 16 [28600/36450]\tLoss: 507.5044\n",
      "Training Epoch: 16 [28650/36450]\tLoss: 491.5227\n",
      "Training Epoch: 16 [28700/36450]\tLoss: 505.6957\n",
      "Training Epoch: 16 [28750/36450]\tLoss: 494.0110\n",
      "Training Epoch: 16 [28800/36450]\tLoss: 528.4805\n",
      "Training Epoch: 16 [28850/36450]\tLoss: 499.9322\n",
      "Training Epoch: 16 [28900/36450]\tLoss: 517.3331\n",
      "Training Epoch: 16 [28950/36450]\tLoss: 509.7935\n",
      "Training Epoch: 16 [29000/36450]\tLoss: 507.3315\n",
      "Training Epoch: 16 [29050/36450]\tLoss: 483.4266\n",
      "Training Epoch: 16 [29100/36450]\tLoss: 537.6727\n",
      "Training Epoch: 16 [29150/36450]\tLoss: 520.1826\n",
      "Training Epoch: 16 [29200/36450]\tLoss: 517.6225\n",
      "Training Epoch: 16 [29250/36450]\tLoss: 496.1553\n",
      "Training Epoch: 16 [29300/36450]\tLoss: 496.7205\n",
      "Training Epoch: 16 [29350/36450]\tLoss: 528.6696\n",
      "Training Epoch: 16 [29400/36450]\tLoss: 510.7970\n",
      "Training Epoch: 16 [29450/36450]\tLoss: 509.1917\n",
      "Training Epoch: 16 [29500/36450]\tLoss: 503.8498\n",
      "Training Epoch: 16 [29550/36450]\tLoss: 521.8151\n",
      "Training Epoch: 16 [29600/36450]\tLoss: 523.8061\n",
      "Training Epoch: 16 [29650/36450]\tLoss: 519.5920\n",
      "Training Epoch: 16 [29700/36450]\tLoss: 504.5695\n",
      "Training Epoch: 16 [29750/36450]\tLoss: 497.0854\n",
      "Training Epoch: 16 [29800/36450]\tLoss: 526.4805\n",
      "Training Epoch: 16 [29850/36450]\tLoss: 517.6879\n",
      "Training Epoch: 16 [29900/36450]\tLoss: 497.5413\n",
      "Training Epoch: 16 [29950/36450]\tLoss: 513.8234\n",
      "Training Epoch: 16 [30000/36450]\tLoss: 589.3934\n",
      "Training Epoch: 16 [30050/36450]\tLoss: 560.1495\n",
      "Training Epoch: 16 [30100/36450]\tLoss: 582.5014\n",
      "Training Epoch: 16 [30150/36450]\tLoss: 565.4280\n",
      "Training Epoch: 16 [30200/36450]\tLoss: 597.7683\n",
      "Training Epoch: 16 [30250/36450]\tLoss: 571.5716\n",
      "Training Epoch: 16 [30300/36450]\tLoss: 527.1292\n",
      "Training Epoch: 16 [30350/36450]\tLoss: 528.3651\n",
      "Training Epoch: 16 [30400/36450]\tLoss: 504.8463\n",
      "Training Epoch: 16 [30450/36450]\tLoss: 487.6790\n",
      "Training Epoch: 16 [30500/36450]\tLoss: 521.3763\n",
      "Training Epoch: 16 [30550/36450]\tLoss: 528.5082\n",
      "Training Epoch: 16 [30600/36450]\tLoss: 521.8392\n",
      "Training Epoch: 16 [30650/36450]\tLoss: 491.2657\n",
      "Training Epoch: 16 [30700/36450]\tLoss: 502.5999\n",
      "Training Epoch: 16 [30750/36450]\tLoss: 500.5054\n",
      "Training Epoch: 16 [30800/36450]\tLoss: 514.6353\n",
      "Training Epoch: 16 [30850/36450]\tLoss: 507.9920\n",
      "Training Epoch: 16 [30900/36450]\tLoss: 531.7410\n",
      "Training Epoch: 16 [30950/36450]\tLoss: 542.8686\n",
      "Training Epoch: 16 [31000/36450]\tLoss: 516.4103\n",
      "Training Epoch: 16 [31050/36450]\tLoss: 493.8047\n",
      "Training Epoch: 16 [31100/36450]\tLoss: 507.2756\n",
      "Training Epoch: 16 [31150/36450]\tLoss: 528.1084\n",
      "Training Epoch: 16 [31200/36450]\tLoss: 554.4840\n",
      "Training Epoch: 16 [31250/36450]\tLoss: 529.4011\n",
      "Training Epoch: 16 [31300/36450]\tLoss: 517.9669\n",
      "Training Epoch: 16 [31350/36450]\tLoss: 488.0172\n",
      "Training Epoch: 16 [31400/36450]\tLoss: 515.8860\n",
      "Training Epoch: 16 [31450/36450]\tLoss: 507.7727\n",
      "Training Epoch: 16 [31500/36450]\tLoss: 478.0807\n",
      "Training Epoch: 16 [31550/36450]\tLoss: 520.7355\n",
      "Training Epoch: 16 [31600/36450]\tLoss: 492.4290\n",
      "Training Epoch: 16 [31650/36450]\tLoss: 497.6087\n",
      "Training Epoch: 16 [31700/36450]\tLoss: 479.5317\n",
      "Training Epoch: 16 [31750/36450]\tLoss: 507.5609\n",
      "Training Epoch: 16 [31800/36450]\tLoss: 524.5717\n",
      "Training Epoch: 16 [31850/36450]\tLoss: 537.4688\n",
      "Training Epoch: 16 [31900/36450]\tLoss: 507.8084\n",
      "Training Epoch: 16 [31950/36450]\tLoss: 489.7007\n",
      "Training Epoch: 16 [32000/36450]\tLoss: 524.4866\n",
      "Training Epoch: 16 [32050/36450]\tLoss: 498.0513\n",
      "Training Epoch: 16 [32100/36450]\tLoss: 494.6911\n",
      "Training Epoch: 16 [32150/36450]\tLoss: 482.7329\n",
      "Training Epoch: 16 [32200/36450]\tLoss: 522.2595\n",
      "Training Epoch: 16 [32250/36450]\tLoss: 498.8723\n",
      "Training Epoch: 16 [32300/36450]\tLoss: 483.1299\n",
      "Training Epoch: 16 [32350/36450]\tLoss: 498.3829\n",
      "Training Epoch: 16 [32400/36450]\tLoss: 520.4730\n",
      "Training Epoch: 16 [32450/36450]\tLoss: 491.9244\n",
      "Training Epoch: 16 [32500/36450]\tLoss: 542.8325\n",
      "Training Epoch: 16 [32550/36450]\tLoss: 513.2545\n",
      "Training Epoch: 16 [32600/36450]\tLoss: 473.9290\n",
      "Training Epoch: 16 [32650/36450]\tLoss: 495.5673\n",
      "Training Epoch: 16 [32700/36450]\tLoss: 493.5473\n",
      "Training Epoch: 16 [32750/36450]\tLoss: 499.2442\n",
      "Training Epoch: 16 [32800/36450]\tLoss: 507.0283\n",
      "Training Epoch: 16 [32850/36450]\tLoss: 481.6585\n",
      "Training Epoch: 16 [32900/36450]\tLoss: 541.3385\n",
      "Training Epoch: 16 [32950/36450]\tLoss: 525.5027\n",
      "Training Epoch: 16 [33000/36450]\tLoss: 490.2054\n",
      "Training Epoch: 16 [33050/36450]\tLoss: 502.3696\n",
      "Training Epoch: 16 [33100/36450]\tLoss: 541.0895\n",
      "Training Epoch: 16 [33150/36450]\tLoss: 504.3964\n",
      "Training Epoch: 16 [33200/36450]\tLoss: 482.8765\n",
      "Training Epoch: 16 [33250/36450]\tLoss: 472.2448\n",
      "Training Epoch: 16 [33300/36450]\tLoss: 500.3019\n",
      "Training Epoch: 16 [33350/36450]\tLoss: 522.0282\n",
      "Training Epoch: 16 [33400/36450]\tLoss: 485.4794\n",
      "Training Epoch: 16 [33450/36450]\tLoss: 499.1230\n",
      "Training Epoch: 16 [33500/36450]\tLoss: 510.6422\n",
      "Training Epoch: 16 [33550/36450]\tLoss: 533.1929\n",
      "Training Epoch: 16 [33600/36450]\tLoss: 520.0815\n",
      "Training Epoch: 16 [33650/36450]\tLoss: 507.1653\n",
      "Training Epoch: 16 [33700/36450]\tLoss: 521.4058\n",
      "Training Epoch: 16 [33750/36450]\tLoss: 499.6238\n",
      "Training Epoch: 16 [33800/36450]\tLoss: 514.5552\n",
      "Training Epoch: 16 [33850/36450]\tLoss: 511.7746\n",
      "Training Epoch: 16 [33900/36450]\tLoss: 509.7837\n",
      "Training Epoch: 16 [33950/36450]\tLoss: 518.3585\n",
      "Training Epoch: 16 [34000/36450]\tLoss: 510.1804\n",
      "Training Epoch: 16 [34050/36450]\tLoss: 489.9978\n",
      "Training Epoch: 16 [34100/36450]\tLoss: 516.1665\n",
      "Training Epoch: 16 [34150/36450]\tLoss: 485.6480\n",
      "Training Epoch: 16 [34200/36450]\tLoss: 520.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16 [34250/36450]\tLoss: 499.0049\n",
      "Training Epoch: 16 [34300/36450]\tLoss: 504.7963\n",
      "Training Epoch: 16 [34350/36450]\tLoss: 500.9188\n",
      "Training Epoch: 16 [34400/36450]\tLoss: 509.8456\n",
      "Training Epoch: 16 [34450/36450]\tLoss: 513.2646\n",
      "Training Epoch: 16 [34500/36450]\tLoss: 502.1778\n",
      "Training Epoch: 16 [34550/36450]\tLoss: 516.1174\n",
      "Training Epoch: 16 [34600/36450]\tLoss: 495.5815\n",
      "Training Epoch: 16 [34650/36450]\tLoss: 505.5283\n",
      "Training Epoch: 16 [34700/36450]\tLoss: 541.0978\n",
      "Training Epoch: 16 [34750/36450]\tLoss: 469.8401\n",
      "Training Epoch: 16 [34800/36450]\tLoss: 470.9565\n",
      "Training Epoch: 16 [34850/36450]\tLoss: 502.7770\n",
      "Training Epoch: 16 [34900/36450]\tLoss: 504.5632\n",
      "Training Epoch: 16 [34950/36450]\tLoss: 508.2692\n",
      "Training Epoch: 16 [35000/36450]\tLoss: 495.0319\n",
      "Training Epoch: 16 [35050/36450]\tLoss: 490.6536\n",
      "Training Epoch: 16 [35100/36450]\tLoss: 492.4161\n",
      "Training Epoch: 16 [35150/36450]\tLoss: 493.5693\n",
      "Training Epoch: 16 [35200/36450]\tLoss: 487.9156\n",
      "Training Epoch: 16 [35250/36450]\tLoss: 529.5115\n",
      "Training Epoch: 16 [35300/36450]\tLoss: 493.1605\n",
      "Training Epoch: 16 [35350/36450]\tLoss: 486.6446\n",
      "Training Epoch: 16 [35400/36450]\tLoss: 521.0354\n",
      "Training Epoch: 16 [35450/36450]\tLoss: 470.3529\n",
      "Training Epoch: 16 [35500/36450]\tLoss: 474.0652\n",
      "Training Epoch: 16 [35550/36450]\tLoss: 505.8952\n",
      "Training Epoch: 16 [35600/36450]\tLoss: 475.6341\n",
      "Training Epoch: 16 [35650/36450]\tLoss: 499.1971\n",
      "Training Epoch: 16 [35700/36450]\tLoss: 524.2105\n",
      "Training Epoch: 16 [35750/36450]\tLoss: 515.5439\n",
      "Training Epoch: 16 [35800/36450]\tLoss: 515.7696\n",
      "Training Epoch: 16 [35850/36450]\tLoss: 484.9744\n",
      "Training Epoch: 16 [35900/36450]\tLoss: 526.8310\n",
      "Training Epoch: 16 [35950/36450]\tLoss: 485.8178\n",
      "Training Epoch: 16 [36000/36450]\tLoss: 487.8294\n",
      "Training Epoch: 16 [36050/36450]\tLoss: 519.5175\n",
      "Training Epoch: 16 [36100/36450]\tLoss: 513.3644\n",
      "Training Epoch: 16 [36150/36450]\tLoss: 498.3438\n",
      "Training Epoch: 16 [36200/36450]\tLoss: 504.7393\n",
      "Training Epoch: 16 [36250/36450]\tLoss: 519.9211\n",
      "Training Epoch: 16 [36300/36450]\tLoss: 516.5306\n",
      "Training Epoch: 16 [36350/36450]\tLoss: 465.4372\n",
      "Training Epoch: 16 [36400/36450]\tLoss: 524.9750\n",
      "Training Epoch: 16 [36450/36450]\tLoss: 514.7706\n",
      "Training Epoch: 16 [4050/4050]\tLoss: 253.0874\n",
      "Training Epoch: 17 [50/36450]\tLoss: 502.5761\n",
      "Training Epoch: 17 [100/36450]\tLoss: 494.2453\n",
      "Training Epoch: 17 [150/36450]\tLoss: 500.5343\n",
      "Training Epoch: 17 [200/36450]\tLoss: 512.4652\n",
      "Training Epoch: 17 [250/36450]\tLoss: 483.8629\n",
      "Training Epoch: 17 [300/36450]\tLoss: 513.1108\n",
      "Training Epoch: 17 [350/36450]\tLoss: 506.2269\n",
      "Training Epoch: 17 [400/36450]\tLoss: 514.9694\n",
      "Training Epoch: 17 [450/36450]\tLoss: 497.2107\n",
      "Training Epoch: 17 [500/36450]\tLoss: 528.8663\n",
      "Training Epoch: 17 [550/36450]\tLoss: 493.0493\n",
      "Training Epoch: 17 [600/36450]\tLoss: 505.6425\n",
      "Training Epoch: 17 [650/36450]\tLoss: 479.6863\n",
      "Training Epoch: 17 [700/36450]\tLoss: 540.5940\n",
      "Training Epoch: 17 [750/36450]\tLoss: 551.6169\n",
      "Training Epoch: 17 [800/36450]\tLoss: 572.3049\n",
      "Training Epoch: 17 [850/36450]\tLoss: 608.3782\n",
      "Training Epoch: 17 [900/36450]\tLoss: 613.8334\n",
      "Training Epoch: 17 [950/36450]\tLoss: 598.7236\n",
      "Training Epoch: 17 [1000/36450]\tLoss: 600.7124\n",
      "Training Epoch: 17 [1050/36450]\tLoss: 519.4077\n",
      "Training Epoch: 17 [1100/36450]\tLoss: 511.1593\n",
      "Training Epoch: 17 [1150/36450]\tLoss: 544.0683\n",
      "Training Epoch: 17 [1200/36450]\tLoss: 528.7083\n",
      "Training Epoch: 17 [1250/36450]\tLoss: 512.2444\n",
      "Training Epoch: 17 [1300/36450]\tLoss: 535.6037\n",
      "Training Epoch: 17 [1350/36450]\tLoss: 521.9304\n",
      "Training Epoch: 17 [1400/36450]\tLoss: 519.3317\n",
      "Training Epoch: 17 [1450/36450]\tLoss: 527.8497\n",
      "Training Epoch: 17 [1500/36450]\tLoss: 518.7378\n",
      "Training Epoch: 17 [1550/36450]\tLoss: 514.8738\n",
      "Training Epoch: 17 [1600/36450]\tLoss: 524.1568\n",
      "Training Epoch: 17 [1650/36450]\tLoss: 488.3364\n",
      "Training Epoch: 17 [1700/36450]\tLoss: 505.5730\n",
      "Training Epoch: 17 [1750/36450]\tLoss: 517.7692\n",
      "Training Epoch: 17 [1800/36450]\tLoss: 524.5256\n",
      "Training Epoch: 17 [1850/36450]\tLoss: 499.2104\n",
      "Training Epoch: 17 [1900/36450]\tLoss: 528.8518\n",
      "Training Epoch: 17 [1950/36450]\tLoss: 518.6639\n",
      "Training Epoch: 17 [2000/36450]\tLoss: 505.5935\n",
      "Training Epoch: 17 [2050/36450]\tLoss: 506.9414\n",
      "Training Epoch: 17 [2100/36450]\tLoss: 515.7125\n",
      "Training Epoch: 17 [2150/36450]\tLoss: 518.5793\n",
      "Training Epoch: 17 [2200/36450]\tLoss: 490.8965\n",
      "Training Epoch: 17 [2250/36450]\tLoss: 521.3441\n",
      "Training Epoch: 17 [2300/36450]\tLoss: 544.9111\n",
      "Training Epoch: 17 [2350/36450]\tLoss: 512.6015\n",
      "Training Epoch: 17 [2400/36450]\tLoss: 531.4788\n",
      "Training Epoch: 17 [2450/36450]\tLoss: 521.4116\n",
      "Training Epoch: 17 [2500/36450]\tLoss: 506.9260\n",
      "Training Epoch: 17 [2550/36450]\tLoss: 506.8643\n",
      "Training Epoch: 17 [2600/36450]\tLoss: 528.4618\n",
      "Training Epoch: 17 [2650/36450]\tLoss: 554.5390\n",
      "Training Epoch: 17 [2700/36450]\tLoss: 543.0207\n",
      "Training Epoch: 17 [2750/36450]\tLoss: 495.2365\n",
      "Training Epoch: 17 [2800/36450]\tLoss: 528.6361\n",
      "Training Epoch: 17 [2850/36450]\tLoss: 514.1085\n",
      "Training Epoch: 17 [2900/36450]\tLoss: 492.4283\n",
      "Training Epoch: 17 [2950/36450]\tLoss: 485.7599\n",
      "Training Epoch: 17 [3000/36450]\tLoss: 521.9882\n",
      "Training Epoch: 17 [3050/36450]\tLoss: 535.7208\n",
      "Training Epoch: 17 [3100/36450]\tLoss: 524.1809\n",
      "Training Epoch: 17 [3150/36450]\tLoss: 504.3190\n",
      "Training Epoch: 17 [3200/36450]\tLoss: 497.8571\n",
      "Training Epoch: 17 [3250/36450]\tLoss: 501.3136\n",
      "Training Epoch: 17 [3300/36450]\tLoss: 519.4193\n",
      "Training Epoch: 17 [3350/36450]\tLoss: 535.0517\n",
      "Training Epoch: 17 [3400/36450]\tLoss: 478.6370\n",
      "Training Epoch: 17 [3450/36450]\tLoss: 514.5396\n",
      "Training Epoch: 17 [3500/36450]\tLoss: 538.3552\n",
      "Training Epoch: 17 [3550/36450]\tLoss: 545.9387\n",
      "Training Epoch: 17 [3600/36450]\tLoss: 498.1409\n",
      "Training Epoch: 17 [3650/36450]\tLoss: 501.0672\n",
      "Training Epoch: 17 [3700/36450]\tLoss: 494.0593\n",
      "Training Epoch: 17 [3750/36450]\tLoss: 536.1118\n",
      "Training Epoch: 17 [3800/36450]\tLoss: 518.2637\n",
      "Training Epoch: 17 [3850/36450]\tLoss: 478.0813\n",
      "Training Epoch: 17 [3900/36450]\tLoss: 506.3262\n",
      "Training Epoch: 17 [3950/36450]\tLoss: 506.3971\n",
      "Training Epoch: 17 [4000/36450]\tLoss: 489.8495\n",
      "Training Epoch: 17 [4050/36450]\tLoss: 502.5523\n",
      "Training Epoch: 17 [4100/36450]\tLoss: 507.6111\n",
      "Training Epoch: 17 [4150/36450]\tLoss: 555.5355\n",
      "Training Epoch: 17 [4200/36450]\tLoss: 498.0340\n",
      "Training Epoch: 17 [4250/36450]\tLoss: 512.7869\n",
      "Training Epoch: 17 [4300/36450]\tLoss: 527.8573\n",
      "Training Epoch: 17 [4350/36450]\tLoss: 504.1099\n",
      "Training Epoch: 17 [4400/36450]\tLoss: 543.3689\n",
      "Training Epoch: 17 [4450/36450]\tLoss: 528.7402\n",
      "Training Epoch: 17 [4500/36450]\tLoss: 526.8714\n",
      "Training Epoch: 17 [4550/36450]\tLoss: 510.4272\n",
      "Training Epoch: 17 [4600/36450]\tLoss: 493.5471\n",
      "Training Epoch: 17 [4650/36450]\tLoss: 535.4974\n",
      "Training Epoch: 17 [4700/36450]\tLoss: 490.7861\n",
      "Training Epoch: 17 [4750/36450]\tLoss: 504.0991\n",
      "Training Epoch: 17 [4800/36450]\tLoss: 481.7471\n",
      "Training Epoch: 17 [4850/36450]\tLoss: 493.0765\n",
      "Training Epoch: 17 [4900/36450]\tLoss: 494.9785\n",
      "Training Epoch: 17 [4950/36450]\tLoss: 506.3539\n",
      "Training Epoch: 17 [5000/36450]\tLoss: 503.7905\n",
      "Training Epoch: 17 [5050/36450]\tLoss: 476.0391\n",
      "Training Epoch: 17 [5100/36450]\tLoss: 509.9165\n",
      "Training Epoch: 17 [5150/36450]\tLoss: 523.2878\n",
      "Training Epoch: 17 [5200/36450]\tLoss: 483.0625\n",
      "Training Epoch: 17 [5250/36450]\tLoss: 551.4450\n",
      "Training Epoch: 17 [5300/36450]\tLoss: 492.4499\n",
      "Training Epoch: 17 [5350/36450]\tLoss: 508.0574\n",
      "Training Epoch: 17 [5400/36450]\tLoss: 514.1165\n",
      "Training Epoch: 17 [5450/36450]\tLoss: 456.9121\n",
      "Training Epoch: 17 [5500/36450]\tLoss: 472.5967\n",
      "Training Epoch: 17 [5550/36450]\tLoss: 462.0165\n",
      "Training Epoch: 17 [5600/36450]\tLoss: 503.6387\n",
      "Training Epoch: 17 [5650/36450]\tLoss: 507.7925\n",
      "Training Epoch: 17 [5700/36450]\tLoss: 519.9020\n",
      "Training Epoch: 17 [5750/36450]\tLoss: 510.0450\n",
      "Training Epoch: 17 [5800/36450]\tLoss: 513.8936\n",
      "Training Epoch: 17 [5850/36450]\tLoss: 503.8143\n",
      "Training Epoch: 17 [5900/36450]\tLoss: 532.5923\n",
      "Training Epoch: 17 [5950/36450]\tLoss: 495.8038\n",
      "Training Epoch: 17 [6000/36450]\tLoss: 492.6850\n",
      "Training Epoch: 17 [6050/36450]\tLoss: 519.9471\n",
      "Training Epoch: 17 [6100/36450]\tLoss: 502.9377\n",
      "Training Epoch: 17 [6150/36450]\tLoss: 494.7659\n",
      "Training Epoch: 17 [6200/36450]\tLoss: 492.1444\n",
      "Training Epoch: 17 [6250/36450]\tLoss: 498.0587\n",
      "Training Epoch: 17 [6300/36450]\tLoss: 494.6205\n",
      "Training Epoch: 17 [6350/36450]\tLoss: 488.9397\n",
      "Training Epoch: 17 [6400/36450]\tLoss: 557.4910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [6450/36450]\tLoss: 491.9888\n",
      "Training Epoch: 17 [6500/36450]\tLoss: 493.2108\n",
      "Training Epoch: 17 [6550/36450]\tLoss: 508.3447\n",
      "Training Epoch: 17 [6600/36450]\tLoss: 512.0901\n",
      "Training Epoch: 17 [6650/36450]\tLoss: 503.3741\n",
      "Training Epoch: 17 [6700/36450]\tLoss: 468.2555\n",
      "Training Epoch: 17 [6750/36450]\tLoss: 525.3190\n",
      "Training Epoch: 17 [6800/36450]\tLoss: 508.0732\n",
      "Training Epoch: 17 [6850/36450]\tLoss: 515.1906\n",
      "Training Epoch: 17 [6900/36450]\tLoss: 523.1926\n",
      "Training Epoch: 17 [6950/36450]\tLoss: 473.8248\n",
      "Training Epoch: 17 [7000/36450]\tLoss: 500.5641\n",
      "Training Epoch: 17 [7050/36450]\tLoss: 523.0273\n",
      "Training Epoch: 17 [7100/36450]\tLoss: 520.9445\n",
      "Training Epoch: 17 [7150/36450]\tLoss: 509.1063\n",
      "Training Epoch: 17 [7200/36450]\tLoss: 502.1491\n",
      "Training Epoch: 17 [7250/36450]\tLoss: 470.6766\n",
      "Training Epoch: 17 [7300/36450]\tLoss: 535.2425\n",
      "Training Epoch: 17 [7350/36450]\tLoss: 510.2047\n",
      "Training Epoch: 17 [7400/36450]\tLoss: 505.0021\n",
      "Training Epoch: 17 [7450/36450]\tLoss: 513.2301\n",
      "Training Epoch: 17 [7500/36450]\tLoss: 497.6337\n",
      "Training Epoch: 17 [7550/36450]\tLoss: 479.4623\n",
      "Training Epoch: 17 [7600/36450]\tLoss: 525.5096\n",
      "Training Epoch: 17 [7650/36450]\tLoss: 499.1307\n",
      "Training Epoch: 17 [7700/36450]\tLoss: 470.6687\n",
      "Training Epoch: 17 [7750/36450]\tLoss: 517.8776\n",
      "Training Epoch: 17 [7800/36450]\tLoss: 484.3019\n",
      "Training Epoch: 17 [7850/36450]\tLoss: 502.6754\n",
      "Training Epoch: 17 [7900/36450]\tLoss: 503.4286\n",
      "Training Epoch: 17 [7950/36450]\tLoss: 489.6367\n",
      "Training Epoch: 17 [8000/36450]\tLoss: 507.7567\n",
      "Training Epoch: 17 [8050/36450]\tLoss: 524.1550\n",
      "Training Epoch: 17 [8100/36450]\tLoss: 497.0978\n",
      "Training Epoch: 17 [8150/36450]\tLoss: 496.0686\n",
      "Training Epoch: 17 [8200/36450]\tLoss: 491.7642\n",
      "Training Epoch: 17 [8250/36450]\tLoss: 540.1239\n",
      "Training Epoch: 17 [8300/36450]\tLoss: 506.7895\n",
      "Training Epoch: 17 [8350/36450]\tLoss: 544.7043\n",
      "Training Epoch: 17 [8400/36450]\tLoss: 507.5438\n",
      "Training Epoch: 17 [8450/36450]\tLoss: 518.1797\n",
      "Training Epoch: 17 [8500/36450]\tLoss: 508.9603\n",
      "Training Epoch: 17 [8550/36450]\tLoss: 516.6230\n",
      "Training Epoch: 17 [8600/36450]\tLoss: 521.7061\n",
      "Training Epoch: 17 [8650/36450]\tLoss: 502.4919\n",
      "Training Epoch: 17 [8700/36450]\tLoss: 477.1801\n",
      "Training Epoch: 17 [8750/36450]\tLoss: 522.5773\n",
      "Training Epoch: 17 [8800/36450]\tLoss: 491.4431\n",
      "Training Epoch: 17 [8850/36450]\tLoss: 518.1020\n",
      "Training Epoch: 17 [8900/36450]\tLoss: 535.5791\n",
      "Training Epoch: 17 [8950/36450]\tLoss: 522.8141\n",
      "Training Epoch: 17 [9000/36450]\tLoss: 504.3063\n",
      "Training Epoch: 17 [9050/36450]\tLoss: 500.7528\n",
      "Training Epoch: 17 [9100/36450]\tLoss: 488.0326\n",
      "Training Epoch: 17 [9150/36450]\tLoss: 494.2976\n",
      "Training Epoch: 17 [9200/36450]\tLoss: 511.0837\n",
      "Training Epoch: 17 [9250/36450]\tLoss: 500.3376\n",
      "Training Epoch: 17 [9300/36450]\tLoss: 530.5547\n",
      "Training Epoch: 17 [9350/36450]\tLoss: 522.0829\n",
      "Training Epoch: 17 [9400/36450]\tLoss: 529.7249\n",
      "Training Epoch: 17 [9450/36450]\tLoss: 510.7566\n",
      "Training Epoch: 17 [9500/36450]\tLoss: 483.4358\n",
      "Training Epoch: 17 [9550/36450]\tLoss: 482.3857\n",
      "Training Epoch: 17 [9600/36450]\tLoss: 518.3422\n",
      "Training Epoch: 17 [9650/36450]\tLoss: 481.0233\n",
      "Training Epoch: 17 [9700/36450]\tLoss: 505.0208\n",
      "Training Epoch: 17 [9750/36450]\tLoss: 513.1597\n",
      "Training Epoch: 17 [9800/36450]\tLoss: 503.2544\n",
      "Training Epoch: 17 [9850/36450]\tLoss: 476.2665\n",
      "Training Epoch: 17 [9900/36450]\tLoss: 541.3090\n",
      "Training Epoch: 17 [9950/36450]\tLoss: 518.2716\n",
      "Training Epoch: 17 [10000/36450]\tLoss: 498.6386\n",
      "Training Epoch: 17 [10050/36450]\tLoss: 540.1212\n",
      "Training Epoch: 17 [10100/36450]\tLoss: 520.8228\n",
      "Training Epoch: 17 [10150/36450]\tLoss: 501.5066\n",
      "Training Epoch: 17 [10200/36450]\tLoss: 491.0693\n",
      "Training Epoch: 17 [10250/36450]\tLoss: 508.7947\n",
      "Training Epoch: 17 [10300/36450]\tLoss: 543.9196\n",
      "Training Epoch: 17 [10350/36450]\tLoss: 494.4272\n",
      "Training Epoch: 17 [10400/36450]\tLoss: 505.8900\n",
      "Training Epoch: 17 [10450/36450]\tLoss: 531.0055\n",
      "Training Epoch: 17 [10500/36450]\tLoss: 501.4039\n",
      "Training Epoch: 17 [10550/36450]\tLoss: 475.9908\n",
      "Training Epoch: 17 [10600/36450]\tLoss: 523.4565\n",
      "Training Epoch: 17 [10650/36450]\tLoss: 527.6818\n",
      "Training Epoch: 17 [10700/36450]\tLoss: 491.2439\n",
      "Training Epoch: 17 [10750/36450]\tLoss: 537.9736\n",
      "Training Epoch: 17 [10800/36450]\tLoss: 532.3678\n",
      "Training Epoch: 17 [10850/36450]\tLoss: 536.8719\n",
      "Training Epoch: 17 [10900/36450]\tLoss: 536.5221\n",
      "Training Epoch: 17 [10950/36450]\tLoss: 538.2236\n",
      "Training Epoch: 17 [11000/36450]\tLoss: 529.1609\n",
      "Training Epoch: 17 [11050/36450]\tLoss: 502.9667\n",
      "Training Epoch: 17 [11100/36450]\tLoss: 517.0977\n",
      "Training Epoch: 17 [11150/36450]\tLoss: 520.6381\n",
      "Training Epoch: 17 [11200/36450]\tLoss: 518.6993\n",
      "Training Epoch: 17 [11250/36450]\tLoss: 521.7264\n",
      "Training Epoch: 17 [11300/36450]\tLoss: 475.8064\n",
      "Training Epoch: 17 [11350/36450]\tLoss: 503.0637\n",
      "Training Epoch: 17 [11400/36450]\tLoss: 529.8430\n",
      "Training Epoch: 17 [11450/36450]\tLoss: 511.5601\n",
      "Training Epoch: 17 [11500/36450]\tLoss: 525.7211\n",
      "Training Epoch: 17 [11550/36450]\tLoss: 479.0113\n",
      "Training Epoch: 17 [11600/36450]\tLoss: 507.7643\n",
      "Training Epoch: 17 [11650/36450]\tLoss: 482.3098\n",
      "Training Epoch: 17 [11700/36450]\tLoss: 480.8882\n",
      "Training Epoch: 17 [11750/36450]\tLoss: 520.3966\n",
      "Training Epoch: 17 [11800/36450]\tLoss: 468.6899\n",
      "Training Epoch: 17 [11850/36450]\tLoss: 470.2519\n",
      "Training Epoch: 17 [11900/36450]\tLoss: 476.9844\n",
      "Training Epoch: 17 [11950/36450]\tLoss: 518.5515\n",
      "Training Epoch: 17 [12000/36450]\tLoss: 521.7906\n",
      "Training Epoch: 17 [12050/36450]\tLoss: 498.8483\n",
      "Training Epoch: 17 [12100/36450]\tLoss: 510.6965\n",
      "Training Epoch: 17 [12150/36450]\tLoss: 503.4147\n",
      "Training Epoch: 17 [12200/36450]\tLoss: 504.8618\n",
      "Training Epoch: 17 [12250/36450]\tLoss: 471.9855\n",
      "Training Epoch: 17 [12300/36450]\tLoss: 496.4680\n",
      "Training Epoch: 17 [12350/36450]\tLoss: 495.4951\n",
      "Training Epoch: 17 [12400/36450]\tLoss: 535.8922\n",
      "Training Epoch: 17 [12450/36450]\tLoss: 503.1304\n",
      "Training Epoch: 17 [12500/36450]\tLoss: 525.0291\n",
      "Training Epoch: 17 [12550/36450]\tLoss: 499.7242\n",
      "Training Epoch: 17 [12600/36450]\tLoss: 505.7903\n",
      "Training Epoch: 17 [12650/36450]\tLoss: 523.3944\n",
      "Training Epoch: 17 [12700/36450]\tLoss: 558.4553\n",
      "Training Epoch: 17 [12750/36450]\tLoss: 505.2120\n",
      "Training Epoch: 17 [12800/36450]\tLoss: 540.1288\n",
      "Training Epoch: 17 [12850/36450]\tLoss: 523.4271\n",
      "Training Epoch: 17 [12900/36450]\tLoss: 488.7778\n",
      "Training Epoch: 17 [12950/36450]\tLoss: 508.4624\n",
      "Training Epoch: 17 [13000/36450]\tLoss: 522.4434\n",
      "Training Epoch: 17 [13050/36450]\tLoss: 488.1135\n",
      "Training Epoch: 17 [13100/36450]\tLoss: 509.9727\n",
      "Training Epoch: 17 [13150/36450]\tLoss: 517.1619\n",
      "Training Epoch: 17 [13200/36450]\tLoss: 518.3823\n",
      "Training Epoch: 17 [13250/36450]\tLoss: 499.1493\n",
      "Training Epoch: 17 [13300/36450]\tLoss: 517.9058\n",
      "Training Epoch: 17 [13350/36450]\tLoss: 497.4292\n",
      "Training Epoch: 17 [13400/36450]\tLoss: 499.0691\n",
      "Training Epoch: 17 [13450/36450]\tLoss: 505.8071\n",
      "Training Epoch: 17 [13500/36450]\tLoss: 500.8530\n",
      "Training Epoch: 17 [13550/36450]\tLoss: 503.4655\n",
      "Training Epoch: 17 [13600/36450]\tLoss: 505.1664\n",
      "Training Epoch: 17 [13650/36450]\tLoss: 477.6943\n",
      "Training Epoch: 17 [13700/36450]\tLoss: 521.0902\n",
      "Training Epoch: 17 [13750/36450]\tLoss: 510.5723\n",
      "Training Epoch: 17 [13800/36450]\tLoss: 492.8575\n",
      "Training Epoch: 17 [13850/36450]\tLoss: 491.1181\n",
      "Training Epoch: 17 [13900/36450]\tLoss: 453.1089\n",
      "Training Epoch: 17 [13950/36450]\tLoss: 525.4192\n",
      "Training Epoch: 17 [14000/36450]\tLoss: 523.5341\n",
      "Training Epoch: 17 [14050/36450]\tLoss: 507.6676\n",
      "Training Epoch: 17 [14100/36450]\tLoss: 575.3578\n",
      "Training Epoch: 17 [14150/36450]\tLoss: 500.1693\n",
      "Training Epoch: 17 [14200/36450]\tLoss: 513.0217\n",
      "Training Epoch: 17 [14250/36450]\tLoss: 470.7501\n",
      "Training Epoch: 17 [14300/36450]\tLoss: 541.9388\n",
      "Training Epoch: 17 [14350/36450]\tLoss: 486.8625\n",
      "Training Epoch: 17 [14400/36450]\tLoss: 526.9505\n",
      "Training Epoch: 17 [14450/36450]\tLoss: 525.2537\n",
      "Training Epoch: 17 [14500/36450]\tLoss: 482.9566\n",
      "Training Epoch: 17 [14550/36450]\tLoss: 462.1965\n",
      "Training Epoch: 17 [14600/36450]\tLoss: 491.8772\n",
      "Training Epoch: 17 [14650/36450]\tLoss: 480.0149\n",
      "Training Epoch: 17 [14700/36450]\tLoss: 532.2113\n",
      "Training Epoch: 17 [14750/36450]\tLoss: 475.2391\n",
      "Training Epoch: 17 [14800/36450]\tLoss: 481.5846\n",
      "Training Epoch: 17 [14850/36450]\tLoss: 488.7632\n",
      "Training Epoch: 17 [14900/36450]\tLoss: 513.1215\n",
      "Training Epoch: 17 [14950/36450]\tLoss: 512.0706\n",
      "Training Epoch: 17 [15000/36450]\tLoss: 486.3748\n",
      "Training Epoch: 17 [15050/36450]\tLoss: 490.5297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [15100/36450]\tLoss: 498.6065\n",
      "Training Epoch: 17 [15150/36450]\tLoss: 517.1184\n",
      "Training Epoch: 17 [15200/36450]\tLoss: 516.0490\n",
      "Training Epoch: 17 [15250/36450]\tLoss: 497.0059\n",
      "Training Epoch: 17 [15300/36450]\tLoss: 479.9456\n",
      "Training Epoch: 17 [15350/36450]\tLoss: 492.1636\n",
      "Training Epoch: 17 [15400/36450]\tLoss: 501.1231\n",
      "Training Epoch: 17 [15450/36450]\tLoss: 507.5851\n",
      "Training Epoch: 17 [15500/36450]\tLoss: 520.0555\n",
      "Training Epoch: 17 [15550/36450]\tLoss: 516.9743\n",
      "Training Epoch: 17 [15600/36450]\tLoss: 513.9637\n",
      "Training Epoch: 17 [15650/36450]\tLoss: 492.2304\n",
      "Training Epoch: 17 [15700/36450]\tLoss: 484.6956\n",
      "Training Epoch: 17 [15750/36450]\tLoss: 504.0095\n",
      "Training Epoch: 17 [15800/36450]\tLoss: 504.1731\n",
      "Training Epoch: 17 [15850/36450]\tLoss: 524.7293\n",
      "Training Epoch: 17 [15900/36450]\tLoss: 502.7988\n",
      "Training Epoch: 17 [15950/36450]\tLoss: 532.1641\n",
      "Training Epoch: 17 [16000/36450]\tLoss: 521.7309\n",
      "Training Epoch: 17 [16050/36450]\tLoss: 511.9203\n",
      "Training Epoch: 17 [16100/36450]\tLoss: 501.8042\n",
      "Training Epoch: 17 [16150/36450]\tLoss: 494.7070\n",
      "Training Epoch: 17 [16200/36450]\tLoss: 528.4615\n",
      "Training Epoch: 17 [16250/36450]\tLoss: 504.8543\n",
      "Training Epoch: 17 [16300/36450]\tLoss: 508.2568\n",
      "Training Epoch: 17 [16350/36450]\tLoss: 544.1292\n",
      "Training Epoch: 17 [16400/36450]\tLoss: 516.9716\n",
      "Training Epoch: 17 [16450/36450]\tLoss: 505.7838\n",
      "Training Epoch: 17 [16500/36450]\tLoss: 517.0312\n",
      "Training Epoch: 17 [16550/36450]\tLoss: 499.1364\n",
      "Training Epoch: 17 [16600/36450]\tLoss: 515.8961\n",
      "Training Epoch: 17 [16650/36450]\tLoss: 572.4121\n",
      "Training Epoch: 17 [16700/36450]\tLoss: 547.5126\n",
      "Training Epoch: 17 [16750/36450]\tLoss: 534.6228\n",
      "Training Epoch: 17 [16800/36450]\tLoss: 553.4509\n",
      "Training Epoch: 17 [16850/36450]\tLoss: 540.5278\n",
      "Training Epoch: 17 [16900/36450]\tLoss: 498.8828\n",
      "Training Epoch: 17 [16950/36450]\tLoss: 489.5607\n",
      "Training Epoch: 17 [17000/36450]\tLoss: 489.0591\n",
      "Training Epoch: 17 [17050/36450]\tLoss: 495.8126\n",
      "Training Epoch: 17 [17100/36450]\tLoss: 528.4714\n",
      "Training Epoch: 17 [17150/36450]\tLoss: 498.0255\n",
      "Training Epoch: 17 [17200/36450]\tLoss: 511.1768\n",
      "Training Epoch: 17 [17250/36450]\tLoss: 510.5819\n",
      "Training Epoch: 17 [17300/36450]\tLoss: 519.9761\n",
      "Training Epoch: 17 [17350/36450]\tLoss: 506.6144\n",
      "Training Epoch: 17 [17400/36450]\tLoss: 531.0621\n",
      "Training Epoch: 17 [17450/36450]\tLoss: 500.8021\n",
      "Training Epoch: 17 [17500/36450]\tLoss: 508.5929\n",
      "Training Epoch: 17 [17550/36450]\tLoss: 522.7026\n",
      "Training Epoch: 17 [17600/36450]\tLoss: 513.8675\n",
      "Training Epoch: 17 [17650/36450]\tLoss: 510.6620\n",
      "Training Epoch: 17 [17700/36450]\tLoss: 500.6193\n",
      "Training Epoch: 17 [17750/36450]\tLoss: 474.3727\n",
      "Training Epoch: 17 [17800/36450]\tLoss: 501.1884\n",
      "Training Epoch: 17 [17850/36450]\tLoss: 516.8705\n",
      "Training Epoch: 17 [17900/36450]\tLoss: 510.0917\n",
      "Training Epoch: 17 [17950/36450]\tLoss: 514.9119\n",
      "Training Epoch: 17 [18000/36450]\tLoss: 501.1516\n",
      "Training Epoch: 17 [18050/36450]\tLoss: 505.6852\n",
      "Training Epoch: 17 [18100/36450]\tLoss: 497.6536\n",
      "Training Epoch: 17 [18150/36450]\tLoss: 517.2515\n",
      "Training Epoch: 17 [18200/36450]\tLoss: 512.6957\n",
      "Training Epoch: 17 [18250/36450]\tLoss: 506.1602\n",
      "Training Epoch: 17 [18300/36450]\tLoss: 522.5195\n",
      "Training Epoch: 17 [18350/36450]\tLoss: 473.5347\n",
      "Training Epoch: 17 [18400/36450]\tLoss: 526.5820\n",
      "Training Epoch: 17 [18450/36450]\tLoss: 484.7260\n",
      "Training Epoch: 17 [18500/36450]\tLoss: 518.4861\n",
      "Training Epoch: 17 [18550/36450]\tLoss: 494.3674\n",
      "Training Epoch: 17 [18600/36450]\tLoss: 510.7955\n",
      "Training Epoch: 17 [18650/36450]\tLoss: 511.7604\n",
      "Training Epoch: 17 [18700/36450]\tLoss: 529.8990\n",
      "Training Epoch: 17 [18750/36450]\tLoss: 513.1196\n",
      "Training Epoch: 17 [18800/36450]\tLoss: 496.8694\n",
      "Training Epoch: 17 [18850/36450]\tLoss: 486.3703\n",
      "Training Epoch: 17 [18900/36450]\tLoss: 521.2401\n",
      "Training Epoch: 17 [18950/36450]\tLoss: 498.1012\n",
      "Training Epoch: 17 [19000/36450]\tLoss: 523.4880\n",
      "Training Epoch: 17 [19050/36450]\tLoss: 507.7592\n",
      "Training Epoch: 17 [19100/36450]\tLoss: 494.4294\n",
      "Training Epoch: 17 [19150/36450]\tLoss: 520.4963\n",
      "Training Epoch: 17 [19200/36450]\tLoss: 523.6756\n",
      "Training Epoch: 17 [19250/36450]\tLoss: 545.5323\n",
      "Training Epoch: 17 [19300/36450]\tLoss: 510.4834\n",
      "Training Epoch: 17 [19350/36450]\tLoss: 525.4980\n",
      "Training Epoch: 17 [19400/36450]\tLoss: 518.0473\n",
      "Training Epoch: 17 [19450/36450]\tLoss: 484.2742\n",
      "Training Epoch: 17 [19500/36450]\tLoss: 498.5486\n",
      "Training Epoch: 17 [19550/36450]\tLoss: 529.9556\n",
      "Training Epoch: 17 [19600/36450]\tLoss: 523.7071\n",
      "Training Epoch: 17 [19650/36450]\tLoss: 514.9182\n",
      "Training Epoch: 17 [19700/36450]\tLoss: 528.9003\n",
      "Training Epoch: 17 [19750/36450]\tLoss: 503.7589\n",
      "Training Epoch: 17 [19800/36450]\tLoss: 511.6567\n",
      "Training Epoch: 17 [19850/36450]\tLoss: 509.3524\n",
      "Training Epoch: 17 [19900/36450]\tLoss: 523.9678\n",
      "Training Epoch: 17 [19950/36450]\tLoss: 510.7457\n",
      "Training Epoch: 17 [20000/36450]\tLoss: 490.8090\n",
      "Training Epoch: 17 [20050/36450]\tLoss: 500.5500\n",
      "Training Epoch: 17 [20100/36450]\tLoss: 540.5923\n",
      "Training Epoch: 17 [20150/36450]\tLoss: 523.6519\n",
      "Training Epoch: 17 [20200/36450]\tLoss: 505.3365\n",
      "Training Epoch: 17 [20250/36450]\tLoss: 500.8757\n",
      "Training Epoch: 17 [20300/36450]\tLoss: 478.0674\n",
      "Training Epoch: 17 [20350/36450]\tLoss: 513.8619\n",
      "Training Epoch: 17 [20400/36450]\tLoss: 547.9745\n",
      "Training Epoch: 17 [20450/36450]\tLoss: 517.6967\n",
      "Training Epoch: 17 [20500/36450]\tLoss: 520.3706\n",
      "Training Epoch: 17 [20550/36450]\tLoss: 519.7708\n",
      "Training Epoch: 17 [20600/36450]\tLoss: 511.6167\n",
      "Training Epoch: 17 [20650/36450]\tLoss: 474.7897\n",
      "Training Epoch: 17 [20700/36450]\tLoss: 470.1438\n",
      "Training Epoch: 17 [20750/36450]\tLoss: 513.7458\n",
      "Training Epoch: 17 [20800/36450]\tLoss: 520.5332\n",
      "Training Epoch: 17 [20850/36450]\tLoss: 508.9274\n",
      "Training Epoch: 17 [20900/36450]\tLoss: 475.9846\n",
      "Training Epoch: 17 [20950/36450]\tLoss: 498.9563\n",
      "Training Epoch: 17 [21000/36450]\tLoss: 501.0746\n",
      "Training Epoch: 17 [21050/36450]\tLoss: 534.5974\n",
      "Training Epoch: 17 [21100/36450]\tLoss: 483.0035\n",
      "Training Epoch: 17 [21150/36450]\tLoss: 486.8756\n",
      "Training Epoch: 17 [21200/36450]\tLoss: 488.0134\n",
      "Training Epoch: 17 [21250/36450]\tLoss: 492.3802\n",
      "Training Epoch: 17 [21300/36450]\tLoss: 500.1561\n",
      "Training Epoch: 17 [21350/36450]\tLoss: 494.5131\n",
      "Training Epoch: 17 [21400/36450]\tLoss: 484.8199\n",
      "Training Epoch: 17 [21450/36450]\tLoss: 469.4786\n",
      "Training Epoch: 17 [21500/36450]\tLoss: 490.5947\n",
      "Training Epoch: 17 [21550/36450]\tLoss: 504.3047\n",
      "Training Epoch: 17 [21600/36450]\tLoss: 488.1808\n",
      "Training Epoch: 17 [21650/36450]\tLoss: 511.5464\n",
      "Training Epoch: 17 [21700/36450]\tLoss: 524.8275\n",
      "Training Epoch: 17 [21750/36450]\tLoss: 522.2504\n",
      "Training Epoch: 17 [21800/36450]\tLoss: 489.7677\n",
      "Training Epoch: 17 [21850/36450]\tLoss: 504.0473\n",
      "Training Epoch: 17 [21900/36450]\tLoss: 481.7224\n",
      "Training Epoch: 17 [21950/36450]\tLoss: 491.6118\n",
      "Training Epoch: 17 [22000/36450]\tLoss: 524.3632\n",
      "Training Epoch: 17 [22050/36450]\tLoss: 502.6443\n",
      "Training Epoch: 17 [22100/36450]\tLoss: 492.9666\n",
      "Training Epoch: 17 [22150/36450]\tLoss: 484.4477\n",
      "Training Epoch: 17 [22200/36450]\tLoss: 482.7926\n",
      "Training Epoch: 17 [22250/36450]\tLoss: 481.8346\n",
      "Training Epoch: 17 [22300/36450]\tLoss: 528.6221\n",
      "Training Epoch: 17 [22350/36450]\tLoss: 491.8126\n",
      "Training Epoch: 17 [22400/36450]\tLoss: 480.5748\n",
      "Training Epoch: 17 [22450/36450]\tLoss: 485.7629\n",
      "Training Epoch: 17 [22500/36450]\tLoss: 512.2986\n",
      "Training Epoch: 17 [22550/36450]\tLoss: 490.7828\n",
      "Training Epoch: 17 [22600/36450]\tLoss: 523.9185\n",
      "Training Epoch: 17 [22650/36450]\tLoss: 496.5349\n",
      "Training Epoch: 17 [22700/36450]\tLoss: 495.0963\n",
      "Training Epoch: 17 [22750/36450]\tLoss: 501.7335\n",
      "Training Epoch: 17 [22800/36450]\tLoss: 480.9641\n",
      "Training Epoch: 17 [22850/36450]\tLoss: 506.9916\n",
      "Training Epoch: 17 [22900/36450]\tLoss: 504.8951\n",
      "Training Epoch: 17 [22950/36450]\tLoss: 483.2486\n",
      "Training Epoch: 17 [23000/36450]\tLoss: 494.2243\n",
      "Training Epoch: 17 [23050/36450]\tLoss: 492.3365\n",
      "Training Epoch: 17 [23100/36450]\tLoss: 501.3937\n",
      "Training Epoch: 17 [23150/36450]\tLoss: 519.7084\n",
      "Training Epoch: 17 [23200/36450]\tLoss: 498.3282\n",
      "Training Epoch: 17 [23250/36450]\tLoss: 481.4973\n",
      "Training Epoch: 17 [23300/36450]\tLoss: 487.0965\n",
      "Training Epoch: 17 [23350/36450]\tLoss: 524.4332\n",
      "Training Epoch: 17 [23400/36450]\tLoss: 528.8568\n",
      "Training Epoch: 17 [23450/36450]\tLoss: 466.3546\n",
      "Training Epoch: 17 [23500/36450]\tLoss: 516.3913\n",
      "Training Epoch: 17 [23550/36450]\tLoss: 515.8948\n",
      "Training Epoch: 17 [23600/36450]\tLoss: 507.2238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [23650/36450]\tLoss: 486.0906\n",
      "Training Epoch: 17 [23700/36450]\tLoss: 480.5877\n",
      "Training Epoch: 17 [23750/36450]\tLoss: 520.2130\n",
      "Training Epoch: 17 [23800/36450]\tLoss: 498.2844\n",
      "Training Epoch: 17 [23850/36450]\tLoss: 471.8350\n",
      "Training Epoch: 17 [23900/36450]\tLoss: 470.7182\n",
      "Training Epoch: 17 [23950/36450]\tLoss: 523.6517\n",
      "Training Epoch: 17 [24000/36450]\tLoss: 447.2021\n",
      "Training Epoch: 17 [24050/36450]\tLoss: 493.9652\n",
      "Training Epoch: 17 [24100/36450]\tLoss: 480.9850\n",
      "Training Epoch: 17 [24150/36450]\tLoss: 518.8683\n",
      "Training Epoch: 17 [24200/36450]\tLoss: 504.3671\n",
      "Training Epoch: 17 [24250/36450]\tLoss: 523.0468\n",
      "Training Epoch: 17 [24300/36450]\tLoss: 501.1814\n",
      "Training Epoch: 17 [24350/36450]\tLoss: 530.1229\n",
      "Training Epoch: 17 [24400/36450]\tLoss: 500.2584\n",
      "Training Epoch: 17 [24450/36450]\tLoss: 525.6851\n",
      "Training Epoch: 17 [24500/36450]\tLoss: 489.3568\n",
      "Training Epoch: 17 [24550/36450]\tLoss: 482.9054\n",
      "Training Epoch: 17 [24600/36450]\tLoss: 510.8995\n",
      "Training Epoch: 17 [24650/36450]\tLoss: 465.0812\n",
      "Training Epoch: 17 [24700/36450]\tLoss: 514.0045\n",
      "Training Epoch: 17 [24750/36450]\tLoss: 505.8972\n",
      "Training Epoch: 17 [24800/36450]\tLoss: 512.7292\n",
      "Training Epoch: 17 [24850/36450]\tLoss: 475.4766\n",
      "Training Epoch: 17 [24900/36450]\tLoss: 492.1100\n",
      "Training Epoch: 17 [24950/36450]\tLoss: 483.2640\n",
      "Training Epoch: 17 [25000/36450]\tLoss: 516.9086\n",
      "Training Epoch: 17 [25050/36450]\tLoss: 498.8070\n",
      "Training Epoch: 17 [25100/36450]\tLoss: 504.3629\n",
      "Training Epoch: 17 [25150/36450]\tLoss: 478.4587\n",
      "Training Epoch: 17 [25200/36450]\tLoss: 516.7941\n",
      "Training Epoch: 17 [25250/36450]\tLoss: 520.3738\n",
      "Training Epoch: 17 [25300/36450]\tLoss: 511.6392\n",
      "Training Epoch: 17 [25350/36450]\tLoss: 463.1755\n",
      "Training Epoch: 17 [25400/36450]\tLoss: 492.0189\n",
      "Training Epoch: 17 [25450/36450]\tLoss: 489.1441\n",
      "Training Epoch: 17 [25500/36450]\tLoss: 506.3081\n",
      "Training Epoch: 17 [25550/36450]\tLoss: 498.2021\n",
      "Training Epoch: 17 [25600/36450]\tLoss: 524.3494\n",
      "Training Epoch: 17 [25650/36450]\tLoss: 485.3944\n",
      "Training Epoch: 17 [25700/36450]\tLoss: 475.3857\n",
      "Training Epoch: 17 [25750/36450]\tLoss: 484.1568\n",
      "Training Epoch: 17 [25800/36450]\tLoss: 504.2764\n",
      "Training Epoch: 17 [25850/36450]\tLoss: 486.1093\n",
      "Training Epoch: 17 [25900/36450]\tLoss: 474.6413\n",
      "Training Epoch: 17 [25950/36450]\tLoss: 492.2979\n",
      "Training Epoch: 17 [26000/36450]\tLoss: 504.1887\n",
      "Training Epoch: 17 [26050/36450]\tLoss: 534.3450\n",
      "Training Epoch: 17 [26100/36450]\tLoss: 495.8705\n",
      "Training Epoch: 17 [26150/36450]\tLoss: 509.9222\n",
      "Training Epoch: 17 [26200/36450]\tLoss: 490.5122\n",
      "Training Epoch: 17 [26250/36450]\tLoss: 519.3978\n",
      "Training Epoch: 17 [26300/36450]\tLoss: 498.9472\n",
      "Training Epoch: 17 [26350/36450]\tLoss: 476.2667\n",
      "Training Epoch: 17 [26400/36450]\tLoss: 502.4911\n",
      "Training Epoch: 17 [26450/36450]\tLoss: 488.0084\n",
      "Training Epoch: 17 [26500/36450]\tLoss: 495.7534\n",
      "Training Epoch: 17 [26550/36450]\tLoss: 481.1462\n",
      "Training Epoch: 17 [26600/36450]\tLoss: 483.9672\n",
      "Training Epoch: 17 [26650/36450]\tLoss: 470.6965\n",
      "Training Epoch: 17 [26700/36450]\tLoss: 516.6967\n",
      "Training Epoch: 17 [26750/36450]\tLoss: 521.6619\n",
      "Training Epoch: 17 [26800/36450]\tLoss: 502.2692\n",
      "Training Epoch: 17 [26850/36450]\tLoss: 505.0102\n",
      "Training Epoch: 17 [26900/36450]\tLoss: 494.7186\n",
      "Training Epoch: 17 [26950/36450]\tLoss: 507.0069\n",
      "Training Epoch: 17 [27000/36450]\tLoss: 503.8999\n",
      "Training Epoch: 17 [27050/36450]\tLoss: 531.9312\n",
      "Training Epoch: 17 [27100/36450]\tLoss: 489.4536\n",
      "Training Epoch: 17 [27150/36450]\tLoss: 509.3541\n",
      "Training Epoch: 17 [27200/36450]\tLoss: 555.8668\n",
      "Training Epoch: 17 [27250/36450]\tLoss: 514.1045\n",
      "Training Epoch: 17 [27300/36450]\tLoss: 534.3484\n",
      "Training Epoch: 17 [27350/36450]\tLoss: 550.2935\n",
      "Training Epoch: 17 [27400/36450]\tLoss: 532.9224\n",
      "Training Epoch: 17 [27450/36450]\tLoss: 560.7819\n",
      "Training Epoch: 17 [27500/36450]\tLoss: 541.7578\n",
      "Training Epoch: 17 [27550/36450]\tLoss: 534.9385\n",
      "Training Epoch: 17 [27600/36450]\tLoss: 529.3815\n",
      "Training Epoch: 17 [27650/36450]\tLoss: 514.5375\n",
      "Training Epoch: 17 [27700/36450]\tLoss: 505.1558\n",
      "Training Epoch: 17 [27750/36450]\tLoss: 510.5132\n",
      "Training Epoch: 17 [27800/36450]\tLoss: 530.8636\n",
      "Training Epoch: 17 [27850/36450]\tLoss: 494.6376\n",
      "Training Epoch: 17 [27900/36450]\tLoss: 489.2866\n",
      "Training Epoch: 17 [27950/36450]\tLoss: 487.4910\n",
      "Training Epoch: 17 [28000/36450]\tLoss: 502.6082\n",
      "Training Epoch: 17 [28050/36450]\tLoss: 490.8150\n",
      "Training Epoch: 17 [28100/36450]\tLoss: 514.3917\n",
      "Training Epoch: 17 [28150/36450]\tLoss: 539.7535\n",
      "Training Epoch: 17 [28200/36450]\tLoss: 505.0220\n",
      "Training Epoch: 17 [28250/36450]\tLoss: 466.2078\n",
      "Training Epoch: 17 [28300/36450]\tLoss: 508.3199\n",
      "Training Epoch: 17 [28350/36450]\tLoss: 539.9293\n",
      "Training Epoch: 17 [28400/36450]\tLoss: 502.7743\n",
      "Training Epoch: 17 [28450/36450]\tLoss: 504.9529\n",
      "Training Epoch: 17 [28500/36450]\tLoss: 512.6300\n",
      "Training Epoch: 17 [28550/36450]\tLoss: 472.9760\n",
      "Training Epoch: 17 [28600/36450]\tLoss: 490.1910\n",
      "Training Epoch: 17 [28650/36450]\tLoss: 529.2041\n",
      "Training Epoch: 17 [28700/36450]\tLoss: 490.2706\n",
      "Training Epoch: 17 [28750/36450]\tLoss: 459.9730\n",
      "Training Epoch: 17 [28800/36450]\tLoss: 501.1033\n",
      "Training Epoch: 17 [28850/36450]\tLoss: 507.0535\n",
      "Training Epoch: 17 [28900/36450]\tLoss: 513.3541\n",
      "Training Epoch: 17 [28950/36450]\tLoss: 509.8695\n",
      "Training Epoch: 17 [29000/36450]\tLoss: 526.1652\n",
      "Training Epoch: 17 [29050/36450]\tLoss: 525.6378\n",
      "Training Epoch: 17 [29100/36450]\tLoss: 510.5959\n",
      "Training Epoch: 17 [29150/36450]\tLoss: 479.9335\n",
      "Training Epoch: 17 [29200/36450]\tLoss: 512.8255\n",
      "Training Epoch: 17 [29250/36450]\tLoss: 486.9204\n",
      "Training Epoch: 17 [29300/36450]\tLoss: 556.9286\n",
      "Training Epoch: 17 [29350/36450]\tLoss: 514.3289\n",
      "Training Epoch: 17 [29400/36450]\tLoss: 499.8482\n",
      "Training Epoch: 17 [29450/36450]\tLoss: 504.5344\n",
      "Training Epoch: 17 [29500/36450]\tLoss: 524.0482\n",
      "Training Epoch: 17 [29550/36450]\tLoss: 489.7053\n",
      "Training Epoch: 17 [29600/36450]\tLoss: 475.4670\n",
      "Training Epoch: 17 [29650/36450]\tLoss: 469.7194\n",
      "Training Epoch: 17 [29700/36450]\tLoss: 477.4956\n",
      "Training Epoch: 17 [29750/36450]\tLoss: 507.3188\n",
      "Training Epoch: 17 [29800/36450]\tLoss: 492.2731\n",
      "Training Epoch: 17 [29850/36450]\tLoss: 509.9632\n",
      "Training Epoch: 17 [29900/36450]\tLoss: 484.2449\n",
      "Training Epoch: 17 [29950/36450]\tLoss: 477.6207\n",
      "Training Epoch: 17 [30000/36450]\tLoss: 494.0453\n",
      "Training Epoch: 17 [30050/36450]\tLoss: 497.0096\n",
      "Training Epoch: 17 [30100/36450]\tLoss: 475.8445\n",
      "Training Epoch: 17 [30150/36450]\tLoss: 486.3070\n",
      "Training Epoch: 17 [30200/36450]\tLoss: 454.8410\n",
      "Training Epoch: 17 [30250/36450]\tLoss: 493.2444\n",
      "Training Epoch: 17 [30300/36450]\tLoss: 506.3204\n",
      "Training Epoch: 17 [30350/36450]\tLoss: 498.1120\n",
      "Training Epoch: 17 [30400/36450]\tLoss: 491.0848\n",
      "Training Epoch: 17 [30450/36450]\tLoss: 482.3240\n",
      "Training Epoch: 17 [30500/36450]\tLoss: 489.6389\n",
      "Training Epoch: 17 [30550/36450]\tLoss: 513.2422\n",
      "Training Epoch: 17 [30600/36450]\tLoss: 532.5859\n",
      "Training Epoch: 17 [30650/36450]\tLoss: 432.0999\n",
      "Training Epoch: 17 [30700/36450]\tLoss: 528.7963\n",
      "Training Epoch: 17 [30750/36450]\tLoss: 458.9306\n",
      "Training Epoch: 17 [30800/36450]\tLoss: 475.0351\n",
      "Training Epoch: 17 [30850/36450]\tLoss: 495.5495\n",
      "Training Epoch: 17 [30900/36450]\tLoss: 503.4788\n",
      "Training Epoch: 17 [30950/36450]\tLoss: 480.2654\n",
      "Training Epoch: 17 [31000/36450]\tLoss: 479.5171\n",
      "Training Epoch: 17 [31050/36450]\tLoss: 503.1039\n",
      "Training Epoch: 17 [31100/36450]\tLoss: 482.4033\n",
      "Training Epoch: 17 [31150/36450]\tLoss: 485.2034\n",
      "Training Epoch: 17 [31200/36450]\tLoss: 504.0332\n",
      "Training Epoch: 17 [31250/36450]\tLoss: 508.9843\n",
      "Training Epoch: 17 [31300/36450]\tLoss: 479.0959\n",
      "Training Epoch: 17 [31350/36450]\tLoss: 500.2164\n",
      "Training Epoch: 17 [31400/36450]\tLoss: 486.7875\n",
      "Training Epoch: 17 [31450/36450]\tLoss: 490.4902\n",
      "Training Epoch: 17 [31500/36450]\tLoss: 504.0667\n",
      "Training Epoch: 17 [31550/36450]\tLoss: 533.8575\n",
      "Training Epoch: 17 [31600/36450]\tLoss: 508.8008\n",
      "Training Epoch: 17 [31650/36450]\tLoss: 508.7954\n",
      "Training Epoch: 17 [31700/36450]\tLoss: 515.9274\n",
      "Training Epoch: 17 [31750/36450]\tLoss: 477.2785\n",
      "Training Epoch: 17 [31800/36450]\tLoss: 511.8600\n",
      "Training Epoch: 17 [31850/36450]\tLoss: 498.6859\n",
      "Training Epoch: 17 [31900/36450]\tLoss: 514.7385\n",
      "Training Epoch: 17 [31950/36450]\tLoss: 493.9170\n",
      "Training Epoch: 17 [32000/36450]\tLoss: 505.4964\n",
      "Training Epoch: 17 [32050/36450]\tLoss: 490.5992\n",
      "Training Epoch: 17 [32100/36450]\tLoss: 492.3720\n",
      "Training Epoch: 17 [32150/36450]\tLoss: 531.1361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17 [32200/36450]\tLoss: 512.2725\n",
      "Training Epoch: 17 [32250/36450]\tLoss: 520.6943\n",
      "Training Epoch: 17 [32300/36450]\tLoss: 490.5098\n",
      "Training Epoch: 17 [32350/36450]\tLoss: 524.1516\n",
      "Training Epoch: 17 [32400/36450]\tLoss: 519.2769\n",
      "Training Epoch: 17 [32450/36450]\tLoss: 491.0845\n",
      "Training Epoch: 17 [32500/36450]\tLoss: 577.6346\n",
      "Training Epoch: 17 [32550/36450]\tLoss: 511.4006\n",
      "Training Epoch: 17 [32600/36450]\tLoss: 518.7645\n",
      "Training Epoch: 17 [32650/36450]\tLoss: 561.4418\n",
      "Training Epoch: 17 [32700/36450]\tLoss: 537.8473\n",
      "Training Epoch: 17 [32750/36450]\tLoss: 539.3918\n",
      "Training Epoch: 17 [32800/36450]\tLoss: 517.5081\n",
      "Training Epoch: 17 [32850/36450]\tLoss: 525.1277\n",
      "Training Epoch: 17 [32900/36450]\tLoss: 506.3789\n",
      "Training Epoch: 17 [32950/36450]\tLoss: 493.0305\n",
      "Training Epoch: 17 [33000/36450]\tLoss: 484.2634\n",
      "Training Epoch: 17 [33050/36450]\tLoss: 505.2480\n",
      "Training Epoch: 17 [33100/36450]\tLoss: 483.5373\n",
      "Training Epoch: 17 [33150/36450]\tLoss: 502.7089\n",
      "Training Epoch: 17 [33200/36450]\tLoss: 519.9493\n",
      "Training Epoch: 17 [33250/36450]\tLoss: 507.7476\n",
      "Training Epoch: 17 [33300/36450]\tLoss: 512.8234\n",
      "Training Epoch: 17 [33350/36450]\tLoss: 462.0653\n",
      "Training Epoch: 17 [33400/36450]\tLoss: 458.3077\n",
      "Training Epoch: 17 [33450/36450]\tLoss: 512.4854\n",
      "Training Epoch: 17 [33500/36450]\tLoss: 500.9714\n",
      "Training Epoch: 17 [33550/36450]\tLoss: 524.3875\n",
      "Training Epoch: 17 [33600/36450]\tLoss: 519.6784\n",
      "Training Epoch: 17 [33650/36450]\tLoss: 533.5476\n",
      "Training Epoch: 17 [33700/36450]\tLoss: 497.0294\n",
      "Training Epoch: 17 [33750/36450]\tLoss: 517.3155\n",
      "Training Epoch: 17 [33800/36450]\tLoss: 468.1421\n",
      "Training Epoch: 17 [33850/36450]\tLoss: 508.5124\n",
      "Training Epoch: 17 [33900/36450]\tLoss: 486.3309\n",
      "Training Epoch: 17 [33950/36450]\tLoss: 507.6341\n",
      "Training Epoch: 17 [34000/36450]\tLoss: 489.9402\n",
      "Training Epoch: 17 [34050/36450]\tLoss: 481.0915\n",
      "Training Epoch: 17 [34100/36450]\tLoss: 497.7432\n",
      "Training Epoch: 17 [34150/36450]\tLoss: 491.7724\n",
      "Training Epoch: 17 [34200/36450]\tLoss: 496.5789\n",
      "Training Epoch: 17 [34250/36450]\tLoss: 490.9625\n",
      "Training Epoch: 17 [34300/36450]\tLoss: 496.0540\n",
      "Training Epoch: 17 [34350/36450]\tLoss: 495.2207\n",
      "Training Epoch: 17 [34400/36450]\tLoss: 490.9308\n",
      "Training Epoch: 17 [34450/36450]\tLoss: 497.7825\n",
      "Training Epoch: 17 [34500/36450]\tLoss: 514.0864\n",
      "Training Epoch: 17 [34550/36450]\tLoss: 547.1170\n",
      "Training Epoch: 17 [34600/36450]\tLoss: 500.2481\n",
      "Training Epoch: 17 [34650/36450]\tLoss: 522.2804\n",
      "Training Epoch: 17 [34700/36450]\tLoss: 506.8584\n",
      "Training Epoch: 17 [34750/36450]\tLoss: 496.0190\n",
      "Training Epoch: 17 [34800/36450]\tLoss: 493.8510\n",
      "Training Epoch: 17 [34850/36450]\tLoss: 479.3560\n",
      "Training Epoch: 17 [34900/36450]\tLoss: 500.7426\n",
      "Training Epoch: 17 [34950/36450]\tLoss: 471.4796\n",
      "Training Epoch: 17 [35000/36450]\tLoss: 463.7950\n",
      "Training Epoch: 17 [35050/36450]\tLoss: 498.6678\n",
      "Training Epoch: 17 [35100/36450]\tLoss: 476.1806\n",
      "Training Epoch: 17 [35150/36450]\tLoss: 490.4905\n",
      "Training Epoch: 17 [35200/36450]\tLoss: 481.5359\n",
      "Training Epoch: 17 [35250/36450]\tLoss: 472.1496\n",
      "Training Epoch: 17 [35300/36450]\tLoss: 525.1167\n",
      "Training Epoch: 17 [35350/36450]\tLoss: 515.4150\n",
      "Training Epoch: 17 [35400/36450]\tLoss: 515.7061\n",
      "Training Epoch: 17 [35450/36450]\tLoss: 511.1747\n",
      "Training Epoch: 17 [35500/36450]\tLoss: 491.1207\n",
      "Training Epoch: 17 [35550/36450]\tLoss: 489.6609\n",
      "Training Epoch: 17 [35600/36450]\tLoss: 510.5896\n",
      "Training Epoch: 17 [35650/36450]\tLoss: 507.9851\n",
      "Training Epoch: 17 [35700/36450]\tLoss: 502.1120\n",
      "Training Epoch: 17 [35750/36450]\tLoss: 529.4100\n",
      "Training Epoch: 17 [35800/36450]\tLoss: 506.8987\n",
      "Training Epoch: 17 [35850/36450]\tLoss: 484.8486\n",
      "Training Epoch: 17 [35900/36450]\tLoss: 519.7226\n",
      "Training Epoch: 17 [35950/36450]\tLoss: 506.0359\n",
      "Training Epoch: 17 [36000/36450]\tLoss: 466.4971\n",
      "Training Epoch: 17 [36050/36450]\tLoss: 485.7597\n",
      "Training Epoch: 17 [36100/36450]\tLoss: 501.8398\n",
      "Training Epoch: 17 [36150/36450]\tLoss: 503.3268\n",
      "Training Epoch: 17 [36200/36450]\tLoss: 481.2757\n",
      "Training Epoch: 17 [36250/36450]\tLoss: 499.8324\n",
      "Training Epoch: 17 [36300/36450]\tLoss: 509.3699\n",
      "Training Epoch: 17 [36350/36450]\tLoss: 480.9344\n",
      "Training Epoch: 17 [36400/36450]\tLoss: 495.8107\n",
      "Training Epoch: 17 [36450/36450]\tLoss: 485.1481\n",
      "Training Epoch: 17 [4050/4050]\tLoss: 253.2912\n",
      "Training Epoch: 18 [50/36450]\tLoss: 522.7945\n",
      "Training Epoch: 18 [100/36450]\tLoss: 464.5484\n",
      "Training Epoch: 18 [150/36450]\tLoss: 476.6627\n",
      "Training Epoch: 18 [200/36450]\tLoss: 477.1799\n",
      "Training Epoch: 18 [250/36450]\tLoss: 486.6267\n",
      "Training Epoch: 18 [300/36450]\tLoss: 514.8829\n",
      "Training Epoch: 18 [350/36450]\tLoss: 501.6999\n",
      "Training Epoch: 18 [400/36450]\tLoss: 484.8435\n",
      "Training Epoch: 18 [450/36450]\tLoss: 474.1532\n",
      "Training Epoch: 18 [500/36450]\tLoss: 519.9656\n",
      "Training Epoch: 18 [550/36450]\tLoss: 518.9995\n",
      "Training Epoch: 18 [600/36450]\tLoss: 501.0995\n",
      "Training Epoch: 18 [650/36450]\tLoss: 533.7986\n",
      "Training Epoch: 18 [700/36450]\tLoss: 476.0702\n",
      "Training Epoch: 18 [750/36450]\tLoss: 471.6133\n",
      "Training Epoch: 18 [800/36450]\tLoss: 481.2496\n",
      "Training Epoch: 18 [850/36450]\tLoss: 497.4458\n",
      "Training Epoch: 18 [900/36450]\tLoss: 468.5279\n",
      "Training Epoch: 18 [950/36450]\tLoss: 523.9422\n",
      "Training Epoch: 18 [1000/36450]\tLoss: 512.4714\n",
      "Training Epoch: 18 [1050/36450]\tLoss: 497.9442\n",
      "Training Epoch: 18 [1100/36450]\tLoss: 478.2155\n",
      "Training Epoch: 18 [1150/36450]\tLoss: 479.3869\n",
      "Training Epoch: 18 [1200/36450]\tLoss: 473.8377\n",
      "Training Epoch: 18 [1250/36450]\tLoss: 500.9615\n",
      "Training Epoch: 18 [1300/36450]\tLoss: 509.3882\n",
      "Training Epoch: 18 [1350/36450]\tLoss: 511.3226\n",
      "Training Epoch: 18 [1400/36450]\tLoss: 550.6588\n",
      "Training Epoch: 18 [1450/36450]\tLoss: 470.3841\n",
      "Training Epoch: 18 [1500/36450]\tLoss: 521.1613\n",
      "Training Epoch: 18 [1550/36450]\tLoss: 526.3902\n",
      "Training Epoch: 18 [1600/36450]\tLoss: 496.0431\n",
      "Training Epoch: 18 [1650/36450]\tLoss: 476.5025\n",
      "Training Epoch: 18 [1700/36450]\tLoss: 506.8691\n",
      "Training Epoch: 18 [1750/36450]\tLoss: 507.6645\n",
      "Training Epoch: 18 [1800/36450]\tLoss: 503.3195\n",
      "Training Epoch: 18 [1850/36450]\tLoss: 502.9230\n",
      "Training Epoch: 18 [1900/36450]\tLoss: 496.9514\n",
      "Training Epoch: 18 [1950/36450]\tLoss: 485.6525\n",
      "Training Epoch: 18 [2000/36450]\tLoss: 529.2589\n",
      "Training Epoch: 18 [2050/36450]\tLoss: 513.0725\n",
      "Training Epoch: 18 [2100/36450]\tLoss: 485.7234\n",
      "Training Epoch: 18 [2150/36450]\tLoss: 508.8177\n",
      "Training Epoch: 18 [2200/36450]\tLoss: 487.7200\n",
      "Training Epoch: 18 [2250/36450]\tLoss: 549.3195\n",
      "Training Epoch: 18 [2300/36450]\tLoss: 499.9509\n",
      "Training Epoch: 18 [2350/36450]\tLoss: 489.2694\n",
      "Training Epoch: 18 [2400/36450]\tLoss: 511.8419\n",
      "Training Epoch: 18 [2450/36450]\tLoss: 532.2405\n",
      "Training Epoch: 18 [2500/36450]\tLoss: 475.3971\n",
      "Training Epoch: 18 [2550/36450]\tLoss: 460.7842\n",
      "Training Epoch: 18 [2600/36450]\tLoss: 487.2720\n",
      "Training Epoch: 18 [2650/36450]\tLoss: 477.6928\n",
      "Training Epoch: 18 [2700/36450]\tLoss: 474.3106\n",
      "Training Epoch: 18 [2750/36450]\tLoss: 494.2629\n",
      "Training Epoch: 18 [2800/36450]\tLoss: 478.7179\n",
      "Training Epoch: 18 [2850/36450]\tLoss: 467.6425\n",
      "Training Epoch: 18 [2900/36450]\tLoss: 470.2928\n",
      "Training Epoch: 18 [2950/36450]\tLoss: 494.2439\n",
      "Training Epoch: 18 [3000/36450]\tLoss: 483.6505\n",
      "Training Epoch: 18 [3050/36450]\tLoss: 491.3649\n",
      "Training Epoch: 18 [3100/36450]\tLoss: 495.5976\n",
      "Training Epoch: 18 [3150/36450]\tLoss: 480.2685\n",
      "Training Epoch: 18 [3200/36450]\tLoss: 516.7272\n",
      "Training Epoch: 18 [3250/36450]\tLoss: 507.4235\n",
      "Training Epoch: 18 [3300/36450]\tLoss: 512.0254\n",
      "Training Epoch: 18 [3350/36450]\tLoss: 515.3203\n",
      "Training Epoch: 18 [3400/36450]\tLoss: 517.2889\n",
      "Training Epoch: 18 [3450/36450]\tLoss: 527.7194\n",
      "Training Epoch: 18 [3500/36450]\tLoss: 529.4222\n",
      "Training Epoch: 18 [3550/36450]\tLoss: 515.1798\n",
      "Training Epoch: 18 [3600/36450]\tLoss: 531.0260\n",
      "Training Epoch: 18 [3650/36450]\tLoss: 494.6743\n",
      "Training Epoch: 18 [3700/36450]\tLoss: 494.2229\n",
      "Training Epoch: 18 [3750/36450]\tLoss: 516.6530\n",
      "Training Epoch: 18 [3800/36450]\tLoss: 493.1934\n",
      "Training Epoch: 18 [3850/36450]\tLoss: 509.7733\n",
      "Training Epoch: 18 [3900/36450]\tLoss: 514.4271\n",
      "Training Epoch: 18 [3950/36450]\tLoss: 464.0671\n",
      "Training Epoch: 18 [4000/36450]\tLoss: 493.6334\n",
      "Training Epoch: 18 [4050/36450]\tLoss: 497.6960\n",
      "Training Epoch: 18 [4100/36450]\tLoss: 494.0101\n",
      "Training Epoch: 18 [4150/36450]\tLoss: 517.6600\n",
      "Training Epoch: 18 [4200/36450]\tLoss: 527.0321\n",
      "Training Epoch: 18 [4250/36450]\tLoss: 512.0922\n",
      "Training Epoch: 18 [4300/36450]\tLoss: 509.5016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [4350/36450]\tLoss: 514.0752\n",
      "Training Epoch: 18 [4400/36450]\tLoss: 527.7245\n",
      "Training Epoch: 18 [4450/36450]\tLoss: 528.0886\n",
      "Training Epoch: 18 [4500/36450]\tLoss: 506.2036\n",
      "Training Epoch: 18 [4550/36450]\tLoss: 520.4133\n",
      "Training Epoch: 18 [4600/36450]\tLoss: 520.5473\n",
      "Training Epoch: 18 [4650/36450]\tLoss: 501.0227\n",
      "Training Epoch: 18 [4700/36450]\tLoss: 499.5633\n",
      "Training Epoch: 18 [4750/36450]\tLoss: 499.9669\n",
      "Training Epoch: 18 [4800/36450]\tLoss: 505.1364\n",
      "Training Epoch: 18 [4850/36450]\tLoss: 482.6946\n",
      "Training Epoch: 18 [4900/36450]\tLoss: 512.3708\n",
      "Training Epoch: 18 [4950/36450]\tLoss: 494.5989\n",
      "Training Epoch: 18 [5000/36450]\tLoss: 505.8137\n",
      "Training Epoch: 18 [5050/36450]\tLoss: 489.2839\n",
      "Training Epoch: 18 [5100/36450]\tLoss: 541.4110\n",
      "Training Epoch: 18 [5150/36450]\tLoss: 472.4477\n",
      "Training Epoch: 18 [5200/36450]\tLoss: 507.1529\n",
      "Training Epoch: 18 [5250/36450]\tLoss: 494.5620\n",
      "Training Epoch: 18 [5300/36450]\tLoss: 517.1542\n",
      "Training Epoch: 18 [5350/36450]\tLoss: 493.3574\n",
      "Training Epoch: 18 [5400/36450]\tLoss: 484.6925\n",
      "Training Epoch: 18 [5450/36450]\tLoss: 499.7892\n",
      "Training Epoch: 18 [5500/36450]\tLoss: 504.8029\n",
      "Training Epoch: 18 [5550/36450]\tLoss: 520.6433\n",
      "Training Epoch: 18 [5600/36450]\tLoss: 513.4210\n",
      "Training Epoch: 18 [5650/36450]\tLoss: 479.7589\n",
      "Training Epoch: 18 [5700/36450]\tLoss: 457.7993\n",
      "Training Epoch: 18 [5750/36450]\tLoss: 484.9614\n",
      "Training Epoch: 18 [5800/36450]\tLoss: 480.6942\n",
      "Training Epoch: 18 [5850/36450]\tLoss: 515.8199\n",
      "Training Epoch: 18 [5900/36450]\tLoss: 460.6679\n",
      "Training Epoch: 18 [5950/36450]\tLoss: 501.7355\n",
      "Training Epoch: 18 [6000/36450]\tLoss: 503.0110\n",
      "Training Epoch: 18 [6050/36450]\tLoss: 476.5062\n",
      "Training Epoch: 18 [6100/36450]\tLoss: 499.5482\n",
      "Training Epoch: 18 [6150/36450]\tLoss: 524.8855\n",
      "Training Epoch: 18 [6200/36450]\tLoss: 501.3306\n",
      "Training Epoch: 18 [6250/36450]\tLoss: 520.9869\n",
      "Training Epoch: 18 [6300/36450]\tLoss: 456.3390\n",
      "Training Epoch: 18 [6350/36450]\tLoss: 522.6937\n",
      "Training Epoch: 18 [6400/36450]\tLoss: 486.1671\n",
      "Training Epoch: 18 [6450/36450]\tLoss: 469.7090\n",
      "Training Epoch: 18 [6500/36450]\tLoss: 512.6116\n",
      "Training Epoch: 18 [6550/36450]\tLoss: 506.6606\n",
      "Training Epoch: 18 [6600/36450]\tLoss: 487.4997\n",
      "Training Epoch: 18 [6650/36450]\tLoss: 502.8629\n",
      "Training Epoch: 18 [6700/36450]\tLoss: 503.6864\n",
      "Training Epoch: 18 [6750/36450]\tLoss: 489.1326\n",
      "Training Epoch: 18 [6800/36450]\tLoss: 498.9842\n",
      "Training Epoch: 18 [6850/36450]\tLoss: 502.2680\n",
      "Training Epoch: 18 [6900/36450]\tLoss: 489.7905\n",
      "Training Epoch: 18 [6950/36450]\tLoss: 492.5852\n",
      "Training Epoch: 18 [7000/36450]\tLoss: 473.9918\n",
      "Training Epoch: 18 [7050/36450]\tLoss: 489.9579\n",
      "Training Epoch: 18 [7100/36450]\tLoss: 505.9149\n",
      "Training Epoch: 18 [7150/36450]\tLoss: 473.1063\n",
      "Training Epoch: 18 [7200/36450]\tLoss: 507.1760\n",
      "Training Epoch: 18 [7250/36450]\tLoss: 492.0447\n",
      "Training Epoch: 18 [7300/36450]\tLoss: 480.8139\n",
      "Training Epoch: 18 [7350/36450]\tLoss: 489.5233\n",
      "Training Epoch: 18 [7400/36450]\tLoss: 515.7835\n",
      "Training Epoch: 18 [7450/36450]\tLoss: 495.3187\n",
      "Training Epoch: 18 [7500/36450]\tLoss: 498.0620\n",
      "Training Epoch: 18 [7550/36450]\tLoss: 516.4054\n",
      "Training Epoch: 18 [7600/36450]\tLoss: 495.5780\n",
      "Training Epoch: 18 [7650/36450]\tLoss: 495.3083\n",
      "Training Epoch: 18 [7700/36450]\tLoss: 509.4443\n",
      "Training Epoch: 18 [7750/36450]\tLoss: 510.2108\n",
      "Training Epoch: 18 [7800/36450]\tLoss: 502.2622\n",
      "Training Epoch: 18 [7850/36450]\tLoss: 473.3227\n",
      "Training Epoch: 18 [7900/36450]\tLoss: 494.7847\n",
      "Training Epoch: 18 [7950/36450]\tLoss: 496.4722\n",
      "Training Epoch: 18 [8000/36450]\tLoss: 507.6490\n",
      "Training Epoch: 18 [8050/36450]\tLoss: 504.4959\n",
      "Training Epoch: 18 [8100/36450]\tLoss: 487.8383\n",
      "Training Epoch: 18 [8150/36450]\tLoss: 512.3040\n",
      "Training Epoch: 18 [8200/36450]\tLoss: 504.2744\n",
      "Training Epoch: 18 [8250/36450]\tLoss: 524.1426\n",
      "Training Epoch: 18 [8300/36450]\tLoss: 514.7951\n",
      "Training Epoch: 18 [8350/36450]\tLoss: 497.9903\n",
      "Training Epoch: 18 [8400/36450]\tLoss: 504.9458\n",
      "Training Epoch: 18 [8450/36450]\tLoss: 473.8338\n",
      "Training Epoch: 18 [8500/36450]\tLoss: 516.1691\n",
      "Training Epoch: 18 [8550/36450]\tLoss: 517.9128\n",
      "Training Epoch: 18 [8600/36450]\tLoss: 492.8422\n",
      "Training Epoch: 18 [8650/36450]\tLoss: 500.8580\n",
      "Training Epoch: 18 [8700/36450]\tLoss: 517.3484\n",
      "Training Epoch: 18 [8750/36450]\tLoss: 481.8685\n",
      "Training Epoch: 18 [8800/36450]\tLoss: 523.5068\n",
      "Training Epoch: 18 [8850/36450]\tLoss: 505.2801\n",
      "Training Epoch: 18 [8900/36450]\tLoss: 480.1372\n",
      "Training Epoch: 18 [8950/36450]\tLoss: 513.6379\n",
      "Training Epoch: 18 [9000/36450]\tLoss: 503.8961\n",
      "Training Epoch: 18 [9050/36450]\tLoss: 518.1417\n",
      "Training Epoch: 18 [9100/36450]\tLoss: 487.3716\n",
      "Training Epoch: 18 [9150/36450]\tLoss: 508.1386\n",
      "Training Epoch: 18 [9200/36450]\tLoss: 493.8083\n",
      "Training Epoch: 18 [9250/36450]\tLoss: 504.7328\n",
      "Training Epoch: 18 [9300/36450]\tLoss: 522.7892\n",
      "Training Epoch: 18 [9350/36450]\tLoss: 511.4431\n",
      "Training Epoch: 18 [9400/36450]\tLoss: 497.9965\n",
      "Training Epoch: 18 [9450/36450]\tLoss: 519.8260\n",
      "Training Epoch: 18 [9500/36450]\tLoss: 518.6198\n",
      "Training Epoch: 18 [9550/36450]\tLoss: 543.3691\n",
      "Training Epoch: 18 [9600/36450]\tLoss: 526.7847\n",
      "Training Epoch: 18 [9650/36450]\tLoss: 521.0818\n",
      "Training Epoch: 18 [9700/36450]\tLoss: 484.0303\n",
      "Training Epoch: 18 [9750/36450]\tLoss: 522.5330\n",
      "Training Epoch: 18 [9800/36450]\tLoss: 508.4478\n",
      "Training Epoch: 18 [9850/36450]\tLoss: 535.2997\n",
      "Training Epoch: 18 [9900/36450]\tLoss: 477.5224\n",
      "Training Epoch: 18 [9950/36450]\tLoss: 498.1685\n",
      "Training Epoch: 18 [10000/36450]\tLoss: 572.4045\n",
      "Training Epoch: 18 [10050/36450]\tLoss: 513.3055\n",
      "Training Epoch: 18 [10100/36450]\tLoss: 506.0543\n",
      "Training Epoch: 18 [10150/36450]\tLoss: 523.2062\n",
      "Training Epoch: 18 [10200/36450]\tLoss: 519.9799\n",
      "Training Epoch: 18 [10250/36450]\tLoss: 507.4003\n",
      "Training Epoch: 18 [10300/36450]\tLoss: 498.5882\n",
      "Training Epoch: 18 [10350/36450]\tLoss: 462.3866\n",
      "Training Epoch: 18 [10400/36450]\tLoss: 467.4786\n",
      "Training Epoch: 18 [10450/36450]\tLoss: 488.2940\n",
      "Training Epoch: 18 [10500/36450]\tLoss: 511.6102\n",
      "Training Epoch: 18 [10550/36450]\tLoss: 518.3763\n",
      "Training Epoch: 18 [10600/36450]\tLoss: 480.2168\n",
      "Training Epoch: 18 [10650/36450]\tLoss: 501.1406\n",
      "Training Epoch: 18 [10700/36450]\tLoss: 538.8218\n",
      "Training Epoch: 18 [10750/36450]\tLoss: 487.6392\n",
      "Training Epoch: 18 [10800/36450]\tLoss: 497.0101\n",
      "Training Epoch: 18 [10850/36450]\tLoss: 535.7609\n",
      "Training Epoch: 18 [10900/36450]\tLoss: 510.2151\n",
      "Training Epoch: 18 [10950/36450]\tLoss: 491.8401\n",
      "Training Epoch: 18 [11000/36450]\tLoss: 487.8889\n",
      "Training Epoch: 18 [11050/36450]\tLoss: 499.4412\n",
      "Training Epoch: 18 [11100/36450]\tLoss: 504.3777\n",
      "Training Epoch: 18 [11150/36450]\tLoss: 523.6920\n",
      "Training Epoch: 18 [11200/36450]\tLoss: 501.3412\n",
      "Training Epoch: 18 [11250/36450]\tLoss: 514.5002\n",
      "Training Epoch: 18 [11300/36450]\tLoss: 501.1261\n",
      "Training Epoch: 18 [11350/36450]\tLoss: 485.4099\n",
      "Training Epoch: 18 [11400/36450]\tLoss: 497.7023\n",
      "Training Epoch: 18 [11450/36450]\tLoss: 531.1760\n",
      "Training Epoch: 18 [11500/36450]\tLoss: 533.3495\n",
      "Training Epoch: 18 [11550/36450]\tLoss: 544.8391\n",
      "Training Epoch: 18 [11600/36450]\tLoss: 525.2162\n",
      "Training Epoch: 18 [11650/36450]\tLoss: 519.2736\n",
      "Training Epoch: 18 [11700/36450]\tLoss: 511.1077\n",
      "Training Epoch: 18 [11750/36450]\tLoss: 523.8754\n",
      "Training Epoch: 18 [11800/36450]\tLoss: 491.4175\n",
      "Training Epoch: 18 [11850/36450]\tLoss: 459.2835\n",
      "Training Epoch: 18 [11900/36450]\tLoss: 502.0325\n",
      "Training Epoch: 18 [11950/36450]\tLoss: 514.0062\n",
      "Training Epoch: 18 [12000/36450]\tLoss: 512.4318\n",
      "Training Epoch: 18 [12050/36450]\tLoss: 491.0469\n",
      "Training Epoch: 18 [12100/36450]\tLoss: 522.3510\n",
      "Training Epoch: 18 [12150/36450]\tLoss: 497.5791\n",
      "Training Epoch: 18 [12200/36450]\tLoss: 508.7476\n",
      "Training Epoch: 18 [12250/36450]\tLoss: 489.8888\n",
      "Training Epoch: 18 [12300/36450]\tLoss: 523.1998\n",
      "Training Epoch: 18 [12350/36450]\tLoss: 477.8699\n",
      "Training Epoch: 18 [12400/36450]\tLoss: 496.4962\n",
      "Training Epoch: 18 [12450/36450]\tLoss: 467.2897\n",
      "Training Epoch: 18 [12500/36450]\tLoss: 487.3137\n",
      "Training Epoch: 18 [12550/36450]\tLoss: 464.2134\n",
      "Training Epoch: 18 [12600/36450]\tLoss: 503.7668\n",
      "Training Epoch: 18 [12650/36450]\tLoss: 509.4174\n",
      "Training Epoch: 18 [12700/36450]\tLoss: 484.5340\n",
      "Training Epoch: 18 [12750/36450]\tLoss: 530.4576\n",
      "Training Epoch: 18 [12800/36450]\tLoss: 471.1776\n",
      "Training Epoch: 18 [12850/36450]\tLoss: 477.3043\n",
      "Training Epoch: 18 [12900/36450]\tLoss: 496.6528\n",
      "Training Epoch: 18 [12950/36450]\tLoss: 470.1386\n",
      "Training Epoch: 18 [13000/36450]\tLoss: 475.5685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [13050/36450]\tLoss: 512.1226\n",
      "Training Epoch: 18 [13100/36450]\tLoss: 496.1643\n",
      "Training Epoch: 18 [13150/36450]\tLoss: 504.2034\n",
      "Training Epoch: 18 [13200/36450]\tLoss: 533.8223\n",
      "Training Epoch: 18 [13250/36450]\tLoss: 505.5760\n",
      "Training Epoch: 18 [13300/36450]\tLoss: 494.1448\n",
      "Training Epoch: 18 [13350/36450]\tLoss: 501.3144\n",
      "Training Epoch: 18 [13400/36450]\tLoss: 494.4683\n",
      "Training Epoch: 18 [13450/36450]\tLoss: 519.5241\n",
      "Training Epoch: 18 [13500/36450]\tLoss: 499.2430\n",
      "Training Epoch: 18 [13550/36450]\tLoss: 487.6826\n",
      "Training Epoch: 18 [13600/36450]\tLoss: 519.8177\n",
      "Training Epoch: 18 [13650/36450]\tLoss: 533.3890\n",
      "Training Epoch: 18 [13700/36450]\tLoss: 510.5882\n",
      "Training Epoch: 18 [13750/36450]\tLoss: 521.5488\n",
      "Training Epoch: 18 [13800/36450]\tLoss: 514.0828\n",
      "Training Epoch: 18 [13850/36450]\tLoss: 505.9979\n",
      "Training Epoch: 18 [13900/36450]\tLoss: 517.5496\n",
      "Training Epoch: 18 [13950/36450]\tLoss: 499.6981\n",
      "Training Epoch: 18 [14000/36450]\tLoss: 520.9756\n",
      "Training Epoch: 18 [14050/36450]\tLoss: 493.6092\n",
      "Training Epoch: 18 [14100/36450]\tLoss: 475.3748\n",
      "Training Epoch: 18 [14150/36450]\tLoss: 474.8931\n",
      "Training Epoch: 18 [14200/36450]\tLoss: 471.1286\n",
      "Training Epoch: 18 [14250/36450]\tLoss: 507.6585\n",
      "Training Epoch: 18 [14300/36450]\tLoss: 495.2240\n",
      "Training Epoch: 18 [14350/36450]\tLoss: 499.3119\n",
      "Training Epoch: 18 [14400/36450]\tLoss: 515.9976\n",
      "Training Epoch: 18 [14450/36450]\tLoss: 552.0152\n",
      "Training Epoch: 18 [14500/36450]\tLoss: 489.7165\n",
      "Training Epoch: 18 [14550/36450]\tLoss: 488.2830\n",
      "Training Epoch: 18 [14600/36450]\tLoss: 482.5846\n",
      "Training Epoch: 18 [14650/36450]\tLoss: 491.4906\n",
      "Training Epoch: 18 [14700/36450]\tLoss: 498.4893\n",
      "Training Epoch: 18 [14750/36450]\tLoss: 473.1227\n",
      "Training Epoch: 18 [14800/36450]\tLoss: 504.1104\n",
      "Training Epoch: 18 [14850/36450]\tLoss: 478.8651\n",
      "Training Epoch: 18 [14900/36450]\tLoss: 508.6089\n",
      "Training Epoch: 18 [14950/36450]\tLoss: 463.9804\n",
      "Training Epoch: 18 [15000/36450]\tLoss: 489.4039\n",
      "Training Epoch: 18 [15050/36450]\tLoss: 472.5312\n",
      "Training Epoch: 18 [15100/36450]\tLoss: 506.1988\n",
      "Training Epoch: 18 [15150/36450]\tLoss: 491.4154\n",
      "Training Epoch: 18 [15200/36450]\tLoss: 499.2968\n",
      "Training Epoch: 18 [15250/36450]\tLoss: 500.1218\n",
      "Training Epoch: 18 [15300/36450]\tLoss: 526.6417\n",
      "Training Epoch: 18 [15350/36450]\tLoss: 526.0413\n",
      "Training Epoch: 18 [15400/36450]\tLoss: 529.2092\n",
      "Training Epoch: 18 [15450/36450]\tLoss: 480.5337\n",
      "Training Epoch: 18 [15500/36450]\tLoss: 513.2358\n",
      "Training Epoch: 18 [15550/36450]\tLoss: 506.5757\n",
      "Training Epoch: 18 [15600/36450]\tLoss: 518.1577\n",
      "Training Epoch: 18 [15650/36450]\tLoss: 486.8573\n",
      "Training Epoch: 18 [15700/36450]\tLoss: 512.0577\n",
      "Training Epoch: 18 [15750/36450]\tLoss: 472.7202\n",
      "Training Epoch: 18 [15800/36450]\tLoss: 488.0196\n",
      "Training Epoch: 18 [15850/36450]\tLoss: 504.7657\n",
      "Training Epoch: 18 [15900/36450]\tLoss: 525.4221\n",
      "Training Epoch: 18 [15950/36450]\tLoss: 487.3201\n",
      "Training Epoch: 18 [16000/36450]\tLoss: 492.1140\n",
      "Training Epoch: 18 [16050/36450]\tLoss: 506.7105\n",
      "Training Epoch: 18 [16100/36450]\tLoss: 498.2664\n",
      "Training Epoch: 18 [16150/36450]\tLoss: 502.7011\n",
      "Training Epoch: 18 [16200/36450]\tLoss: 520.1896\n",
      "Training Epoch: 18 [16250/36450]\tLoss: 509.9420\n",
      "Training Epoch: 18 [16300/36450]\tLoss: 503.3803\n",
      "Training Epoch: 18 [16350/36450]\tLoss: 501.4644\n",
      "Training Epoch: 18 [16400/36450]\tLoss: 537.0796\n",
      "Training Epoch: 18 [16450/36450]\tLoss: 514.3688\n",
      "Training Epoch: 18 [16500/36450]\tLoss: 512.8479\n",
      "Training Epoch: 18 [16550/36450]\tLoss: 510.5854\n",
      "Training Epoch: 18 [16600/36450]\tLoss: 521.3472\n",
      "Training Epoch: 18 [16650/36450]\tLoss: 528.2444\n",
      "Training Epoch: 18 [16700/36450]\tLoss: 457.4399\n",
      "Training Epoch: 18 [16750/36450]\tLoss: 470.2614\n",
      "Training Epoch: 18 [16800/36450]\tLoss: 499.2628\n",
      "Training Epoch: 18 [16850/36450]\tLoss: 479.1836\n",
      "Training Epoch: 18 [16900/36450]\tLoss: 468.3983\n",
      "Training Epoch: 18 [16950/36450]\tLoss: 507.3276\n",
      "Training Epoch: 18 [17000/36450]\tLoss: 488.3275\n",
      "Training Epoch: 18 [17050/36450]\tLoss: 489.0193\n",
      "Training Epoch: 18 [17100/36450]\tLoss: 508.1101\n",
      "Training Epoch: 18 [17150/36450]\tLoss: 497.3058\n",
      "Training Epoch: 18 [17200/36450]\tLoss: 489.7789\n",
      "Training Epoch: 18 [17250/36450]\tLoss: 495.3246\n",
      "Training Epoch: 18 [17300/36450]\tLoss: 464.9334\n",
      "Training Epoch: 18 [17350/36450]\tLoss: 484.7567\n",
      "Training Epoch: 18 [17400/36450]\tLoss: 520.7020\n",
      "Training Epoch: 18 [17450/36450]\tLoss: 497.3440\n",
      "Training Epoch: 18 [17500/36450]\tLoss: 476.8997\n",
      "Training Epoch: 18 [17550/36450]\tLoss: 514.6755\n",
      "Training Epoch: 18 [17600/36450]\tLoss: 473.5923\n",
      "Training Epoch: 18 [17650/36450]\tLoss: 476.9933\n",
      "Training Epoch: 18 [17700/36450]\tLoss: 463.0585\n",
      "Training Epoch: 18 [17750/36450]\tLoss: 490.3318\n",
      "Training Epoch: 18 [17800/36450]\tLoss: 487.1932\n",
      "Training Epoch: 18 [17850/36450]\tLoss: 512.0314\n",
      "Training Epoch: 18 [17900/36450]\tLoss: 492.0925\n",
      "Training Epoch: 18 [17950/36450]\tLoss: 514.2350\n",
      "Training Epoch: 18 [18000/36450]\tLoss: 506.0500\n",
      "Training Epoch: 18 [18050/36450]\tLoss: 528.5750\n",
      "Training Epoch: 18 [18100/36450]\tLoss: 491.9017\n",
      "Training Epoch: 18 [18150/36450]\tLoss: 486.7388\n",
      "Training Epoch: 18 [18200/36450]\tLoss: 527.3285\n",
      "Training Epoch: 18 [18250/36450]\tLoss: 522.6139\n",
      "Training Epoch: 18 [18300/36450]\tLoss: 528.6952\n",
      "Training Epoch: 18 [18350/36450]\tLoss: 492.3965\n",
      "Training Epoch: 18 [18400/36450]\tLoss: 505.2756\n",
      "Training Epoch: 18 [18450/36450]\tLoss: 501.7095\n",
      "Training Epoch: 18 [18500/36450]\tLoss: 524.4036\n",
      "Training Epoch: 18 [18550/36450]\tLoss: 506.9979\n",
      "Training Epoch: 18 [18600/36450]\tLoss: 519.1019\n",
      "Training Epoch: 18 [18650/36450]\tLoss: 504.0523\n",
      "Training Epoch: 18 [18700/36450]\tLoss: 493.9329\n",
      "Training Epoch: 18 [18750/36450]\tLoss: 497.0552\n",
      "Training Epoch: 18 [18800/36450]\tLoss: 551.3467\n",
      "Training Epoch: 18 [18850/36450]\tLoss: 510.3691\n",
      "Training Epoch: 18 [18900/36450]\tLoss: 544.8457\n",
      "Training Epoch: 18 [18950/36450]\tLoss: 517.6335\n",
      "Training Epoch: 18 [19000/36450]\tLoss: 533.5557\n",
      "Training Epoch: 18 [19050/36450]\tLoss: 530.7006\n",
      "Training Epoch: 18 [19100/36450]\tLoss: 521.8068\n",
      "Training Epoch: 18 [19150/36450]\tLoss: 537.8774\n",
      "Training Epoch: 18 [19200/36450]\tLoss: 478.8203\n",
      "Training Epoch: 18 [19250/36450]\tLoss: 504.4150\n",
      "Training Epoch: 18 [19300/36450]\tLoss: 490.8248\n",
      "Training Epoch: 18 [19350/36450]\tLoss: 475.8629\n",
      "Training Epoch: 18 [19400/36450]\tLoss: 503.4453\n",
      "Training Epoch: 18 [19450/36450]\tLoss: 551.1989\n",
      "Training Epoch: 18 [19500/36450]\tLoss: 540.8137\n",
      "Training Epoch: 18 [19550/36450]\tLoss: 548.0974\n",
      "Training Epoch: 18 [19600/36450]\tLoss: 535.4429\n",
      "Training Epoch: 18 [19650/36450]\tLoss: 514.7009\n",
      "Training Epoch: 18 [19700/36450]\tLoss: 518.9517\n",
      "Training Epoch: 18 [19750/36450]\tLoss: 515.4673\n",
      "Training Epoch: 18 [19800/36450]\tLoss: 494.6189\n",
      "Training Epoch: 18 [19850/36450]\tLoss: 506.3292\n",
      "Training Epoch: 18 [19900/36450]\tLoss: 513.2827\n",
      "Training Epoch: 18 [19950/36450]\tLoss: 519.8302\n",
      "Training Epoch: 18 [20000/36450]\tLoss: 484.1417\n",
      "Training Epoch: 18 [20050/36450]\tLoss: 518.8929\n",
      "Training Epoch: 18 [20100/36450]\tLoss: 476.0567\n",
      "Training Epoch: 18 [20150/36450]\tLoss: 474.5970\n",
      "Training Epoch: 18 [20200/36450]\tLoss: 514.9405\n",
      "Training Epoch: 18 [20250/36450]\tLoss: 509.5016\n",
      "Training Epoch: 18 [20300/36450]\tLoss: 507.3597\n",
      "Training Epoch: 18 [20350/36450]\tLoss: 488.8360\n",
      "Training Epoch: 18 [20400/36450]\tLoss: 483.0253\n",
      "Training Epoch: 18 [20450/36450]\tLoss: 501.5093\n",
      "Training Epoch: 18 [20500/36450]\tLoss: 492.6423\n",
      "Training Epoch: 18 [20550/36450]\tLoss: 510.7047\n",
      "Training Epoch: 18 [20600/36450]\tLoss: 516.5706\n",
      "Training Epoch: 18 [20650/36450]\tLoss: 489.1600\n",
      "Training Epoch: 18 [20700/36450]\tLoss: 511.5042\n",
      "Training Epoch: 18 [20750/36450]\tLoss: 472.4276\n",
      "Training Epoch: 18 [20800/36450]\tLoss: 536.2473\n",
      "Training Epoch: 18 [20850/36450]\tLoss: 495.6318\n",
      "Training Epoch: 18 [20900/36450]\tLoss: 518.9621\n",
      "Training Epoch: 18 [20950/36450]\tLoss: 504.8097\n",
      "Training Epoch: 18 [21000/36450]\tLoss: 479.1308\n",
      "Training Epoch: 18 [21050/36450]\tLoss: 514.7978\n",
      "Training Epoch: 18 [21100/36450]\tLoss: 521.5555\n",
      "Training Epoch: 18 [21150/36450]\tLoss: 483.6389\n",
      "Training Epoch: 18 [21200/36450]\tLoss: 501.0474\n",
      "Training Epoch: 18 [21250/36450]\tLoss: 484.4482\n",
      "Training Epoch: 18 [21300/36450]\tLoss: 472.4454\n",
      "Training Epoch: 18 [21350/36450]\tLoss: 473.6757\n",
      "Training Epoch: 18 [21400/36450]\tLoss: 476.6244\n",
      "Training Epoch: 18 [21450/36450]\tLoss: 503.8696\n",
      "Training Epoch: 18 [21500/36450]\tLoss: 518.2616\n",
      "Training Epoch: 18 [21550/36450]\tLoss: 535.2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [21600/36450]\tLoss: 520.6364\n",
      "Training Epoch: 18 [21650/36450]\tLoss: 513.8151\n",
      "Training Epoch: 18 [21700/36450]\tLoss: 491.2439\n",
      "Training Epoch: 18 [21750/36450]\tLoss: 473.3801\n",
      "Training Epoch: 18 [21800/36450]\tLoss: 487.8795\n",
      "Training Epoch: 18 [21850/36450]\tLoss: 520.3171\n",
      "Training Epoch: 18 [21900/36450]\tLoss: 481.8872\n",
      "Training Epoch: 18 [21950/36450]\tLoss: 492.3342\n",
      "Training Epoch: 18 [22000/36450]\tLoss: 502.6955\n",
      "Training Epoch: 18 [22050/36450]\tLoss: 505.7941\n",
      "Training Epoch: 18 [22100/36450]\tLoss: 504.8688\n",
      "Training Epoch: 18 [22150/36450]\tLoss: 490.7375\n",
      "Training Epoch: 18 [22200/36450]\tLoss: 507.8543\n",
      "Training Epoch: 18 [22250/36450]\tLoss: 486.3782\n",
      "Training Epoch: 18 [22300/36450]\tLoss: 475.1465\n",
      "Training Epoch: 18 [22350/36450]\tLoss: 481.7007\n",
      "Training Epoch: 18 [22400/36450]\tLoss: 494.2363\n",
      "Training Epoch: 18 [22450/36450]\tLoss: 490.0075\n",
      "Training Epoch: 18 [22500/36450]\tLoss: 483.9249\n",
      "Training Epoch: 18 [22550/36450]\tLoss: 520.6625\n",
      "Training Epoch: 18 [22600/36450]\tLoss: 475.0205\n",
      "Training Epoch: 18 [22650/36450]\tLoss: 493.6093\n",
      "Training Epoch: 18 [22700/36450]\tLoss: 474.3096\n",
      "Training Epoch: 18 [22750/36450]\tLoss: 487.0993\n",
      "Training Epoch: 18 [22800/36450]\tLoss: 491.3869\n",
      "Training Epoch: 18 [22850/36450]\tLoss: 506.7508\n",
      "Training Epoch: 18 [22900/36450]\tLoss: 460.3036\n",
      "Training Epoch: 18 [22950/36450]\tLoss: 485.6246\n",
      "Training Epoch: 18 [23000/36450]\tLoss: 501.9815\n",
      "Training Epoch: 18 [23050/36450]\tLoss: 464.2954\n",
      "Training Epoch: 18 [23100/36450]\tLoss: 526.0795\n",
      "Training Epoch: 18 [23150/36450]\tLoss: 492.1022\n",
      "Training Epoch: 18 [23200/36450]\tLoss: 481.6508\n",
      "Training Epoch: 18 [23250/36450]\tLoss: 494.4098\n",
      "Training Epoch: 18 [23300/36450]\tLoss: 499.3729\n",
      "Training Epoch: 18 [23350/36450]\tLoss: 481.9958\n",
      "Training Epoch: 18 [23400/36450]\tLoss: 506.8733\n",
      "Training Epoch: 18 [23450/36450]\tLoss: 482.9673\n",
      "Training Epoch: 18 [23500/36450]\tLoss: 500.8936\n",
      "Training Epoch: 18 [23550/36450]\tLoss: 511.8242\n",
      "Training Epoch: 18 [23600/36450]\tLoss: 503.3416\n",
      "Training Epoch: 18 [23650/36450]\tLoss: 503.9739\n",
      "Training Epoch: 18 [23700/36450]\tLoss: 485.5133\n",
      "Training Epoch: 18 [23750/36450]\tLoss: 491.1260\n",
      "Training Epoch: 18 [23800/36450]\tLoss: 488.6470\n",
      "Training Epoch: 18 [23850/36450]\tLoss: 500.5733\n",
      "Training Epoch: 18 [23900/36450]\tLoss: 492.2401\n",
      "Training Epoch: 18 [23950/36450]\tLoss: 524.7161\n",
      "Training Epoch: 18 [24000/36450]\tLoss: 493.4674\n",
      "Training Epoch: 18 [24050/36450]\tLoss: 511.8007\n",
      "Training Epoch: 18 [24100/36450]\tLoss: 480.2762\n",
      "Training Epoch: 18 [24150/36450]\tLoss: 466.2212\n",
      "Training Epoch: 18 [24200/36450]\tLoss: 500.5730\n",
      "Training Epoch: 18 [24250/36450]\tLoss: 499.3953\n",
      "Training Epoch: 18 [24300/36450]\tLoss: 507.0439\n",
      "Training Epoch: 18 [24350/36450]\tLoss: 507.6573\n",
      "Training Epoch: 18 [24400/36450]\tLoss: 512.0312\n",
      "Training Epoch: 18 [24450/36450]\tLoss: 482.1951\n",
      "Training Epoch: 18 [24500/36450]\tLoss: 490.9975\n",
      "Training Epoch: 18 [24550/36450]\tLoss: 540.4538\n",
      "Training Epoch: 18 [24600/36450]\tLoss: 465.4012\n",
      "Training Epoch: 18 [24650/36450]\tLoss: 510.3149\n",
      "Training Epoch: 18 [24700/36450]\tLoss: 500.1097\n",
      "Training Epoch: 18 [24750/36450]\tLoss: 500.4844\n",
      "Training Epoch: 18 [24800/36450]\tLoss: 476.8707\n",
      "Training Epoch: 18 [24850/36450]\tLoss: 530.0520\n",
      "Training Epoch: 18 [24900/36450]\tLoss: 507.8416\n",
      "Training Epoch: 18 [24950/36450]\tLoss: 476.8061\n",
      "Training Epoch: 18 [25000/36450]\tLoss: 475.1891\n",
      "Training Epoch: 18 [25050/36450]\tLoss: 492.0941\n",
      "Training Epoch: 18 [25100/36450]\tLoss: 504.6078\n",
      "Training Epoch: 18 [25150/36450]\tLoss: 490.3768\n",
      "Training Epoch: 18 [25200/36450]\tLoss: 483.2411\n",
      "Training Epoch: 18 [25250/36450]\tLoss: 507.1227\n",
      "Training Epoch: 18 [25300/36450]\tLoss: 484.6566\n",
      "Training Epoch: 18 [25350/36450]\tLoss: 499.4664\n",
      "Training Epoch: 18 [25400/36450]\tLoss: 527.9621\n",
      "Training Epoch: 18 [25450/36450]\tLoss: 461.8624\n",
      "Training Epoch: 18 [25500/36450]\tLoss: 528.5419\n",
      "Training Epoch: 18 [25550/36450]\tLoss: 470.4366\n",
      "Training Epoch: 18 [25600/36450]\tLoss: 516.3407\n",
      "Training Epoch: 18 [25650/36450]\tLoss: 475.4091\n",
      "Training Epoch: 18 [25700/36450]\tLoss: 511.1875\n",
      "Training Epoch: 18 [25750/36450]\tLoss: 517.9330\n",
      "Training Epoch: 18 [25800/36450]\tLoss: 443.8954\n",
      "Training Epoch: 18 [25850/36450]\tLoss: 476.2451\n",
      "Training Epoch: 18 [25900/36450]\tLoss: 496.1227\n",
      "Training Epoch: 18 [25950/36450]\tLoss: 477.0041\n",
      "Training Epoch: 18 [26000/36450]\tLoss: 480.8642\n",
      "Training Epoch: 18 [26050/36450]\tLoss: 493.5831\n",
      "Training Epoch: 18 [26100/36450]\tLoss: 514.4730\n",
      "Training Epoch: 18 [26150/36450]\tLoss: 491.0921\n",
      "Training Epoch: 18 [26200/36450]\tLoss: 464.7300\n",
      "Training Epoch: 18 [26250/36450]\tLoss: 491.5267\n",
      "Training Epoch: 18 [26300/36450]\tLoss: 506.9795\n",
      "Training Epoch: 18 [26350/36450]\tLoss: 500.3444\n",
      "Training Epoch: 18 [26400/36450]\tLoss: 508.4572\n",
      "Training Epoch: 18 [26450/36450]\tLoss: 509.7723\n",
      "Training Epoch: 18 [26500/36450]\tLoss: 527.2848\n",
      "Training Epoch: 18 [26550/36450]\tLoss: 488.8363\n",
      "Training Epoch: 18 [26600/36450]\tLoss: 463.4727\n",
      "Training Epoch: 18 [26650/36450]\tLoss: 494.9884\n",
      "Training Epoch: 18 [26700/36450]\tLoss: 508.7874\n",
      "Training Epoch: 18 [26750/36450]\tLoss: 537.9792\n",
      "Training Epoch: 18 [26800/36450]\tLoss: 528.0491\n",
      "Training Epoch: 18 [26850/36450]\tLoss: 552.4233\n",
      "Training Epoch: 18 [26900/36450]\tLoss: 480.9991\n",
      "Training Epoch: 18 [26950/36450]\tLoss: 484.3495\n",
      "Training Epoch: 18 [27000/36450]\tLoss: 530.2727\n",
      "Training Epoch: 18 [27050/36450]\tLoss: 528.5835\n",
      "Training Epoch: 18 [27100/36450]\tLoss: 507.0161\n",
      "Training Epoch: 18 [27150/36450]\tLoss: 515.3997\n",
      "Training Epoch: 18 [27200/36450]\tLoss: 510.1500\n",
      "Training Epoch: 18 [27250/36450]\tLoss: 510.1130\n",
      "Training Epoch: 18 [27300/36450]\tLoss: 517.6757\n",
      "Training Epoch: 18 [27350/36450]\tLoss: 539.1097\n",
      "Training Epoch: 18 [27400/36450]\tLoss: 549.8115\n",
      "Training Epoch: 18 [27450/36450]\tLoss: 548.6094\n",
      "Training Epoch: 18 [27500/36450]\tLoss: 560.6622\n",
      "Training Epoch: 18 [27550/36450]\tLoss: 597.7573\n",
      "Training Epoch: 18 [27600/36450]\tLoss: 573.8791\n",
      "Training Epoch: 18 [27650/36450]\tLoss: 543.6360\n",
      "Training Epoch: 18 [27700/36450]\tLoss: 477.5915\n",
      "Training Epoch: 18 [27750/36450]\tLoss: 477.3721\n",
      "Training Epoch: 18 [27800/36450]\tLoss: 520.5909\n",
      "Training Epoch: 18 [27850/36450]\tLoss: 526.1270\n",
      "Training Epoch: 18 [27900/36450]\tLoss: 493.9536\n",
      "Training Epoch: 18 [27950/36450]\tLoss: 472.8842\n",
      "Training Epoch: 18 [28000/36450]\tLoss: 488.2166\n",
      "Training Epoch: 18 [28050/36450]\tLoss: 496.4343\n",
      "Training Epoch: 18 [28100/36450]\tLoss: 502.3436\n",
      "Training Epoch: 18 [28150/36450]\tLoss: 495.8966\n",
      "Training Epoch: 18 [28200/36450]\tLoss: 535.4268\n",
      "Training Epoch: 18 [28250/36450]\tLoss: 512.9222\n",
      "Training Epoch: 18 [28300/36450]\tLoss: 520.1128\n",
      "Training Epoch: 18 [28350/36450]\tLoss: 509.4789\n",
      "Training Epoch: 18 [28400/36450]\tLoss: 482.8618\n",
      "Training Epoch: 18 [28450/36450]\tLoss: 535.4260\n",
      "Training Epoch: 18 [28500/36450]\tLoss: 510.3280\n",
      "Training Epoch: 18 [28550/36450]\tLoss: 505.7266\n",
      "Training Epoch: 18 [28600/36450]\tLoss: 475.5249\n",
      "Training Epoch: 18 [28650/36450]\tLoss: 489.3365\n",
      "Training Epoch: 18 [28700/36450]\tLoss: 536.7330\n",
      "Training Epoch: 18 [28750/36450]\tLoss: 492.9328\n",
      "Training Epoch: 18 [28800/36450]\tLoss: 477.4608\n",
      "Training Epoch: 18 [28850/36450]\tLoss: 482.5702\n",
      "Training Epoch: 18 [28900/36450]\tLoss: 480.7019\n",
      "Training Epoch: 18 [28950/36450]\tLoss: 489.4207\n",
      "Training Epoch: 18 [29000/36450]\tLoss: 501.4409\n",
      "Training Epoch: 18 [29050/36450]\tLoss: 475.7271\n",
      "Training Epoch: 18 [29100/36450]\tLoss: 474.9621\n",
      "Training Epoch: 18 [29150/36450]\tLoss: 479.4379\n",
      "Training Epoch: 18 [29200/36450]\tLoss: 500.6217\n",
      "Training Epoch: 18 [29250/36450]\tLoss: 499.8813\n",
      "Training Epoch: 18 [29300/36450]\tLoss: 505.6141\n",
      "Training Epoch: 18 [29350/36450]\tLoss: 500.5587\n",
      "Training Epoch: 18 [29400/36450]\tLoss: 510.4133\n",
      "Training Epoch: 18 [29450/36450]\tLoss: 492.8061\n",
      "Training Epoch: 18 [29500/36450]\tLoss: 511.1833\n",
      "Training Epoch: 18 [29550/36450]\tLoss: 465.6422\n",
      "Training Epoch: 18 [29600/36450]\tLoss: 477.9994\n",
      "Training Epoch: 18 [29650/36450]\tLoss: 482.4304\n",
      "Training Epoch: 18 [29700/36450]\tLoss: 485.5330\n",
      "Training Epoch: 18 [29750/36450]\tLoss: 519.5839\n",
      "Training Epoch: 18 [29800/36450]\tLoss: 502.0311\n",
      "Training Epoch: 18 [29850/36450]\tLoss: 500.8364\n",
      "Training Epoch: 18 [29900/36450]\tLoss: 518.4398\n",
      "Training Epoch: 18 [29950/36450]\tLoss: 479.0421\n",
      "Training Epoch: 18 [30000/36450]\tLoss: 481.7863\n",
      "Training Epoch: 18 [30050/36450]\tLoss: 486.8764\n",
      "Training Epoch: 18 [30100/36450]\tLoss: 516.7781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18 [30150/36450]\tLoss: 495.0615\n",
      "Training Epoch: 18 [30200/36450]\tLoss: 539.5411\n",
      "Training Epoch: 18 [30250/36450]\tLoss: 481.0726\n",
      "Training Epoch: 18 [30300/36450]\tLoss: 505.5510\n",
      "Training Epoch: 18 [30350/36450]\tLoss: 522.9572\n",
      "Training Epoch: 18 [30400/36450]\tLoss: 487.7372\n",
      "Training Epoch: 18 [30450/36450]\tLoss: 504.6219\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
