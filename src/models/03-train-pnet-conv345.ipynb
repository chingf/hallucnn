{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/es3773/.conda/envs/hcnn\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - torchaudio\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.07.19 |       h06a4308_0         131 KB\n",
      "    cudatoolkit-11.3.1         |       h2bc3f7f_2       815.6 MB\n",
      "    dataclasses-0.8            |     pyh4f3eec9_6          22 KB\n",
      "    intel-openmp-2022.0.1      |    h06a4308_3633         8.5 MB\n",
      "    libuv-1.40.0               |       h7b6447c_0         933 KB\n",
      "    mkl-2020.2                 |              256       213.9 MB\n",
      "    mkl-service-2.3.0          |   py36he8ac12f_0          56 KB\n",
      "    mkl_fft-1.3.0              |   py36h54f3939_0         185 KB\n",
      "    mkl_random-1.1.1           |   py36h0573a6f_0         382 KB\n",
      "    numpy-1.19.2               |   py36h54aff64_0          21 KB\n",
      "    numpy-base-1.19.2          |   py36hfa32c7d_0         5.2 MB\n",
      "    olefile-0.46               |           py36_0          48 KB\n",
      "    openjpeg-2.4.0             |       h3ad879b_0         505 KB\n",
      "    openssl-1.1.1q             |       h7f8727e_0         3.8 MB\n",
      "    pillow-8.3.1               |   py36h2c7a002_0         695 KB\n",
      "    pytorch-1.10.2             |py3.6_cuda11.3_cudnn8.2.0_0        1.21 GB  pytorch\n",
      "    six-1.16.0                 |     pyhd3eb1b0_1          19 KB\n",
      "    torchaudio-0.10.2          |       py36_cu113         4.5 MB  pytorch\n",
      "    torchvision-0.11.3         |       py36_cu113        30.4 MB  pytorch\n",
      "    typing_extensions-4.1.1    |     pyh06a4308_0          29 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        2.27 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
      "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2\n",
      "  dataclasses        pkgs/main/noarch::dataclasses-0.8-pyh4f3eec9_6\n",
      "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0\n",
      "  freetype           pkgs/main/linux-64::freetype-2.11.0-h70c0345_0\n",
      "  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3\n",
      "  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0\n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2022.0.1-h06a4308_3633\n",
      "  jpeg               pkgs/main/linux-64::jpeg-9e-h7f8727e_0\n",
      "  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0\n",
      "  lcms2              pkgs/main/linux-64::lcms2-2.12-h3be6417_0\n",
      "  libiconv           pkgs/main/linux-64::libiconv-1.16-h7f8727e_2\n",
      "  libidn2            pkgs/main/linux-64::libidn2-2.3.2-h7f8727e_0\n",
      "  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0\n",
      "  libtasn1           pkgs/main/linux-64::libtasn1-4.16.0-h27cfd23_0\n",
      "  libtiff            pkgs/main/linux-64::libtiff-4.2.0-h2818925_1\n",
      "  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0\n",
      "  libuv              pkgs/main/linux-64::libuv-1.40.0-h7b6447c_0\n",
      "  libwebp-base       pkgs/main/linux-64::libwebp-base-1.2.2-h7f8727e_0\n",
      "  lz4-c              pkgs/main/linux-64::lz4-c-1.9.3-h295c915_1\n",
      "  mkl                pkgs/main/linux-64::mkl-2020.2-256\n",
      "  mkl-service        pkgs/main/linux-64::mkl-service-2.3.0-py36he8ac12f_0\n",
      "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.0-py36h54f3939_0\n",
      "  mkl_random         pkgs/main/linux-64::mkl_random-1.1.1-py36h0573a6f_0\n",
      "  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1\n",
      "  numpy              pkgs/main/linux-64::numpy-1.19.2-py36h54aff64_0\n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.19.2-py36hfa32c7d_0\n",
      "  olefile            pkgs/main/linux-64::olefile-0.46-py36_0\n",
      "  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0\n",
      "  openjpeg           pkgs/main/linux-64::openjpeg-2.4.0-h3ad879b_0\n",
      "  pillow             pkgs/main/linux-64::pillow-8.3.1-py36h2c7a002_0\n",
      "  pytorch            pytorch/linux-64::pytorch-1.10.2-py3.6_cuda11.3_cudnn8.2.0_0\n",
      "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda\n",
      "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1\n",
      "  torchaudio         pytorch/linux-64::torchaudio-0.10.2-py36_cu113\n",
      "  torchvision        pytorch/linux-64::torchvision-0.11.3-py36_cu113\n",
      "  typing_extensions  pkgs/main/noarch::typing_extensions-4.1.1-pyh06a4308_0\n",
      "  zstd               pkgs/main/linux-64::zstd-1.5.2-ha4553b6_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2022.4.26-h06a4308_0 --> 2022.07.19-h06a4308_0\n",
      "  openssl                                 1.1.1o-h7f8727e_0 --> 1.1.1q-h7f8727e_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8114b7e23868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset, TrainCleanSoundsDataset, PsychophysicsCleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Network to train\n",
    "TODO: This should be converted to a script that accepts arguments for which network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv345 import PBranchedNetwork_Conv345SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv345SeparateHP\n",
    "pnet_name = 'conv345'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 20\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'\n",
    "train_datafile = f'{engram_dir}seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  7 13:36:35 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:B2:00.0 Off |                  N/A |\r\n",
      "|  0%   28C    P8    19W / 250W |      3MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_Conv345SeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(23.428571428571427, 23.529411764705884), mode=bilinear)\n",
       "      (1): ConvTranspose2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder2): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pcoder3): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): ConvTranspose2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up TrainSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainCleanSoundsDataset(train_datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up PsychophysicsSoundsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = h5py.File(f\"{engram_dir}PsychophysicsWord2017W_not_resampled.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metadata = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META_key.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPsychophysics2017WCleanCochleagrams():\n",
    "    \n",
    "    cochleagrams_clean = []\n",
    "   \n",
    "    cochleagrams = []\n",
    "    for batch_ii in range(0,15300,100):\n",
    "        hdf5_path = '/mnt/smb/locker/issa-locker/users/Erica/cgrams_for_noise_robustness_analysis/PsychophysicsWord2017W_clean/batch_'+str(batch_ii)+'_to_'+str(batch_ii+100)+'.hdf5'\n",
    "        with h5py.File(hdf5_path, 'r') as f_in:\n",
    "            cochleagrams += list(f_in['data'])\n",
    "\n",
    "    return cochleagrams\n",
    "clean_in = getPsychophysics2017WCleanCochleagrams()\n",
    "clean_in = np.array(clean_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for word in f_metadata['word']:\n",
    "    idx = np.argwhere(f_key == word)\n",
    "    if len(idx) == 0:\n",
    "        labels.append(-1)\n",
    "    else:\n",
    "        labels.append(idx.item())\n",
    "labels = np.array(labels)\n",
    "labels += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dset = []\n",
    "for _orig_dset in f_metadata['orig_dset']:\n",
    "    _orig_dset = str(_orig_dset, 'utf-8')\n",
    "    _orig_dset = 'WSJ' if 'WSJ' in _orig_dset else 'Timit'\n",
    "    orig_dset.append(_orig_dset)\n",
    "orig_dset = np.array(orig_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psycho_dataset = PsychophysicsCleanSoundsDataset(\n",
    "    clean_in, labels, orig_dset, exclude_timit=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del orig_dset\n",
    "del clean_in\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55188, 1, 164, 400])\n"
     ]
    }
   ],
   "source": [
    "full_dataset = CleanSoundsDataset(train_dataset, psycho_dataset)\n",
    "del train_dataset\n",
    "del psycho_dataset\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [20/49669]\tLoss: 11970782.0000\n",
      "Training Epoch: 1 [40/49669]\tLoss: 7230903.0000\n",
      "Training Epoch: 1 [60/49669]\tLoss: 4840701.5000\n",
      "Training Epoch: 1 [80/49669]\tLoss: 2625204.5000\n",
      "Training Epoch: 1 [100/49669]\tLoss: 2356654.5000\n",
      "Training Epoch: 1 [120/49669]\tLoss: 2885562.7500\n",
      "Training Epoch: 1 [140/49669]\tLoss: 2969443.2500\n",
      "Training Epoch: 1 [160/49669]\tLoss: 3177833.2500\n",
      "Training Epoch: 1 [180/49669]\tLoss: 3565537.2500\n",
      "Training Epoch: 1 [200/49669]\tLoss: 3896911.5000\n",
      "Training Epoch: 1 [220/49669]\tLoss: 4000930.0000\n",
      "Training Epoch: 1 [240/49669]\tLoss: 2949415.7500\n",
      "Training Epoch: 1 [260/49669]\tLoss: 3125396.7500\n",
      "Training Epoch: 1 [280/49669]\tLoss: 2520996.7500\n",
      "Training Epoch: 1 [300/49669]\tLoss: 2221031.2500\n",
      "Training Epoch: 1 [320/49669]\tLoss: 1895894.5000\n",
      "Training Epoch: 1 [340/49669]\tLoss: 1716454.0000\n",
      "Training Epoch: 1 [360/49669]\tLoss: 1640568.8750\n",
      "Training Epoch: 1 [380/49669]\tLoss: 2086494.2500\n",
      "Training Epoch: 1 [400/49669]\tLoss: 1835595.6250\n",
      "Training Epoch: 1 [420/49669]\tLoss: 1930826.6250\n",
      "Training Epoch: 1 [440/49669]\tLoss: 2589886.7500\n",
      "Training Epoch: 1 [460/49669]\tLoss: 1583946.2500\n",
      "Training Epoch: 1 [480/49669]\tLoss: 1630839.6250\n",
      "Training Epoch: 1 [500/49669]\tLoss: 1185366.2500\n",
      "Training Epoch: 1 [520/49669]\tLoss: 1436366.6250\n",
      "Training Epoch: 1 [540/49669]\tLoss: 1308432.2500\n",
      "Training Epoch: 1 [560/49669]\tLoss: 1283133.7500\n",
      "Training Epoch: 1 [580/49669]\tLoss: 1167462.3750\n",
      "Training Epoch: 1 [600/49669]\tLoss: 1194173.1250\n",
      "Training Epoch: 1 [620/49669]\tLoss: 1045355.3125\n",
      "Training Epoch: 1 [640/49669]\tLoss: 1065414.7500\n",
      "Training Epoch: 1 [660/49669]\tLoss: 1246306.7500\n",
      "Training Epoch: 1 [680/49669]\tLoss: 1172008.8750\n",
      "Training Epoch: 1 [700/49669]\tLoss: 1250345.3750\n",
      "Training Epoch: 1 [720/49669]\tLoss: 1005587.1875\n",
      "Training Epoch: 1 [740/49669]\tLoss: 937041.6875\n",
      "Training Epoch: 1 [760/49669]\tLoss: 962023.3125\n",
      "Training Epoch: 1 [780/49669]\tLoss: 930599.3125\n",
      "Training Epoch: 1 [800/49669]\tLoss: 797759.2500\n",
      "Training Epoch: 1 [820/49669]\tLoss: 735459.5625\n",
      "Training Epoch: 1 [840/49669]\tLoss: 839778.6250\n",
      "Training Epoch: 1 [860/49669]\tLoss: 762764.6875\n",
      "Training Epoch: 1 [880/49669]\tLoss: 944996.7500\n",
      "Training Epoch: 1 [900/49669]\tLoss: 753452.1250\n",
      "Training Epoch: 1 [920/49669]\tLoss: 789355.5000\n",
      "Training Epoch: 1 [940/49669]\tLoss: 819963.8750\n",
      "Training Epoch: 1 [960/49669]\tLoss: 649466.8125\n",
      "Training Epoch: 1 [980/49669]\tLoss: 704384.3750\n",
      "Training Epoch: 1 [1000/49669]\tLoss: 665593.3750\n",
      "Training Epoch: 1 [1020/49669]\tLoss: 640950.3750\n",
      "Training Epoch: 1 [1040/49669]\tLoss: 709855.0000\n",
      "Training Epoch: 1 [1060/49669]\tLoss: 587856.1875\n",
      "Training Epoch: 1 [1080/49669]\tLoss: 775691.8125\n",
      "Training Epoch: 1 [1100/49669]\tLoss: 593647.1250\n",
      "Training Epoch: 1 [1120/49669]\tLoss: 588863.0000\n",
      "Training Epoch: 1 [1140/49669]\tLoss: 664169.5000\n",
      "Training Epoch: 1 [1160/49669]\tLoss: 744868.8750\n",
      "Training Epoch: 1 [1180/49669]\tLoss: 557492.6250\n",
      "Training Epoch: 1 [1200/49669]\tLoss: 627290.5000\n",
      "Training Epoch: 1 [1220/49669]\tLoss: 627461.3750\n",
      "Training Epoch: 1 [1240/49669]\tLoss: 737882.5625\n",
      "Training Epoch: 1 [1260/49669]\tLoss: 505412.7500\n",
      "Training Epoch: 1 [1280/49669]\tLoss: 563513.1250\n",
      "Training Epoch: 1 [1300/49669]\tLoss: 526017.1250\n",
      "Training Epoch: 1 [1320/49669]\tLoss: 592494.7500\n",
      "Training Epoch: 1 [1340/49669]\tLoss: 488564.6250\n",
      "Training Epoch: 1 [1360/49669]\tLoss: 585890.8750\n",
      "Training Epoch: 1 [1380/49669]\tLoss: 481164.1562\n",
      "Training Epoch: 1 [1400/49669]\tLoss: 540305.3125\n",
      "Training Epoch: 1 [1420/49669]\tLoss: 676261.6250\n",
      "Training Epoch: 1 [1440/49669]\tLoss: 469359.5625\n",
      "Training Epoch: 1 [1460/49669]\tLoss: 442485.5938\n",
      "Training Epoch: 1 [1480/49669]\tLoss: 459150.5312\n",
      "Training Epoch: 1 [1500/49669]\tLoss: 534485.6250\n",
      "Training Epoch: 1 [1520/49669]\tLoss: 491754.8438\n",
      "Training Epoch: 1 [1540/49669]\tLoss: 477650.0000\n",
      "Training Epoch: 1 [1560/49669]\tLoss: 523243.8438\n",
      "Training Epoch: 1 [1580/49669]\tLoss: 612111.8750\n",
      "Training Epoch: 1 [1600/49669]\tLoss: 415412.0938\n",
      "Training Epoch: 1 [1620/49669]\tLoss: 396044.4375\n",
      "Training Epoch: 1 [1640/49669]\tLoss: 375402.0625\n",
      "Training Epoch: 1 [1660/49669]\tLoss: 425058.3125\n",
      "Training Epoch: 1 [1680/49669]\tLoss: 419723.1875\n",
      "Training Epoch: 1 [1700/49669]\tLoss: 404263.0625\n",
      "Training Epoch: 1 [1720/49669]\tLoss: 308573.0000\n",
      "Training Epoch: 1 [1740/49669]\tLoss: 573321.1875\n",
      "Training Epoch: 1 [1760/49669]\tLoss: 348406.3125\n",
      "Training Epoch: 1 [1780/49669]\tLoss: 389350.0312\n",
      "Training Epoch: 1 [1800/49669]\tLoss: 390702.1250\n",
      "Training Epoch: 1 [1820/49669]\tLoss: 427607.3125\n",
      "Training Epoch: 1 [1840/49669]\tLoss: 381868.9062\n",
      "Training Epoch: 1 [1860/49669]\tLoss: 395897.0625\n",
      "Training Epoch: 1 [1880/49669]\tLoss: 330671.2188\n",
      "Training Epoch: 1 [1900/49669]\tLoss: 383842.5312\n",
      "Training Epoch: 1 [1920/49669]\tLoss: 522967.4375\n",
      "Training Epoch: 1 [1940/49669]\tLoss: 356354.5000\n",
      "Training Epoch: 1 [1960/49669]\tLoss: 426512.6562\n",
      "Training Epoch: 1 [1980/49669]\tLoss: 393270.4375\n",
      "Training Epoch: 1 [2000/49669]\tLoss: 405632.6250\n",
      "Training Epoch: 1 [2020/49669]\tLoss: 477562.4375\n",
      "Training Epoch: 1 [2040/49669]\tLoss: 413808.4062\n",
      "Training Epoch: 1 [2060/49669]\tLoss: 326643.4688\n",
      "Training Epoch: 1 [2080/49669]\tLoss: 463526.7188\n",
      "Training Epoch: 1 [2100/49669]\tLoss: 305606.3438\n",
      "Training Epoch: 1 [2120/49669]\tLoss: 313217.9688\n",
      "Training Epoch: 1 [2140/49669]\tLoss: 563513.1250\n",
      "Training Epoch: 1 [2160/49669]\tLoss: 341951.4375\n",
      "Training Epoch: 1 [2180/49669]\tLoss: 494115.1875\n",
      "Training Epoch: 1 [2200/49669]\tLoss: 396498.5000\n",
      "Training Epoch: 1 [2220/49669]\tLoss: 363569.0000\n",
      "Training Epoch: 1 [2240/49669]\tLoss: 411270.5000\n",
      "Training Epoch: 1 [2260/49669]\tLoss: 457377.5625\n",
      "Training Epoch: 1 [2280/49669]\tLoss: 507524.6250\n",
      "Training Epoch: 1 [2300/49669]\tLoss: 344486.9688\n",
      "Training Epoch: 1 [2320/49669]\tLoss: 386151.2500\n",
      "Training Epoch: 1 [2340/49669]\tLoss: 343381.9375\n",
      "Training Epoch: 1 [2360/49669]\tLoss: 300653.5312\n",
      "Training Epoch: 1 [2380/49669]\tLoss: 347791.1250\n",
      "Training Epoch: 1 [2400/49669]\tLoss: 358303.5938\n",
      "Training Epoch: 1 [2420/49669]\tLoss: 276060.8750\n",
      "Training Epoch: 1 [2440/49669]\tLoss: 384558.5625\n",
      "Training Epoch: 1 [2460/49669]\tLoss: 350794.4688\n",
      "Training Epoch: 1 [2480/49669]\tLoss: 508586.3750\n",
      "Training Epoch: 1 [2500/49669]\tLoss: 297369.0938\n",
      "Training Epoch: 1 [2520/49669]\tLoss: 320091.3125\n",
      "Training Epoch: 1 [2540/49669]\tLoss: 475221.7500\n",
      "Training Epoch: 1 [2560/49669]\tLoss: 315777.1250\n",
      "Training Epoch: 1 [2580/49669]\tLoss: 357724.9375\n",
      "Training Epoch: 1 [2600/49669]\tLoss: 429636.0625\n",
      "Training Epoch: 1 [2620/49669]\tLoss: 309524.5625\n",
      "Training Epoch: 1 [2640/49669]\tLoss: 331386.6875\n",
      "Training Epoch: 1 [2660/49669]\tLoss: 402861.6875\n",
      "Training Epoch: 1 [2680/49669]\tLoss: 268448.3750\n",
      "Training Epoch: 1 [2700/49669]\tLoss: 387796.5938\n",
      "Training Epoch: 1 [2720/49669]\tLoss: 327860.2500\n",
      "Training Epoch: 1 [2740/49669]\tLoss: 370907.5625\n",
      "Training Epoch: 1 [2760/49669]\tLoss: 373700.7812\n",
      "Training Epoch: 1 [2780/49669]\tLoss: 279022.0000\n",
      "Training Epoch: 1 [2800/49669]\tLoss: 300882.6250\n",
      "Training Epoch: 1 [2820/49669]\tLoss: 382746.2812\n",
      "Training Epoch: 1 [2840/49669]\tLoss: 316129.7188\n",
      "Training Epoch: 1 [2860/49669]\tLoss: 356700.9688\n",
      "Training Epoch: 1 [2880/49669]\tLoss: 383932.5312\n",
      "Training Epoch: 1 [2900/49669]\tLoss: 237610.1719\n",
      "Training Epoch: 1 [2920/49669]\tLoss: 310998.0000\n",
      "Training Epoch: 1 [2940/49669]\tLoss: 292127.5312\n",
      "Training Epoch: 1 [2960/49669]\tLoss: 309841.7812\n",
      "Training Epoch: 1 [2980/49669]\tLoss: 403285.0312\n",
      "Training Epoch: 1 [3000/49669]\tLoss: 268983.2812\n",
      "Training Epoch: 1 [3020/49669]\tLoss: 311861.9062\n",
      "Training Epoch: 1 [3040/49669]\tLoss: 331472.3125\n",
      "Training Epoch: 1 [3060/49669]\tLoss: 386365.2500\n",
      "Training Epoch: 1 [3080/49669]\tLoss: 296471.2812\n",
      "Training Epoch: 1 [3100/49669]\tLoss: 313133.4688\n",
      "Training Epoch: 1 [3120/49669]\tLoss: 373531.4688\n",
      "Training Epoch: 1 [3140/49669]\tLoss: 269461.6562\n",
      "Training Epoch: 1 [3160/49669]\tLoss: 345116.5625\n",
      "Training Epoch: 1 [3180/49669]\tLoss: 272235.2500\n",
      "Training Epoch: 1 [3200/49669]\tLoss: 267683.2812\n",
      "Training Epoch: 1 [3220/49669]\tLoss: 232447.9219\n",
      "Training Epoch: 1 [3240/49669]\tLoss: 374294.9375\n",
      "Training Epoch: 1 [3260/49669]\tLoss: 270993.9375\n",
      "Training Epoch: 1 [3280/49669]\tLoss: 435333.0625\n",
      "Training Epoch: 1 [3300/49669]\tLoss: 239393.6562\n",
      "Training Epoch: 1 [3320/49669]\tLoss: 296906.4375\n",
      "Training Epoch: 1 [3340/49669]\tLoss: 363602.3438\n",
      "Training Epoch: 1 [3360/49669]\tLoss: 314851.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [3380/49669]\tLoss: 312975.1250\n",
      "Training Epoch: 1 [3400/49669]\tLoss: 258586.9531\n",
      "Training Epoch: 1 [3420/49669]\tLoss: 286245.7188\n",
      "Training Epoch: 1 [3440/49669]\tLoss: 238968.1719\n",
      "Training Epoch: 1 [3460/49669]\tLoss: 343129.0000\n",
      "Training Epoch: 1 [3480/49669]\tLoss: 304746.6875\n",
      "Training Epoch: 1 [3500/49669]\tLoss: 254322.7031\n",
      "Training Epoch: 1 [3520/49669]\tLoss: 261568.3125\n",
      "Training Epoch: 1 [3540/49669]\tLoss: 316277.0312\n",
      "Training Epoch: 1 [3560/49669]\tLoss: 236765.6875\n",
      "Training Epoch: 1 [3580/49669]\tLoss: 230135.2188\n",
      "Training Epoch: 1 [3600/49669]\tLoss: 327814.6875\n",
      "Training Epoch: 1 [3620/49669]\tLoss: 294868.1875\n",
      "Training Epoch: 1 [3640/49669]\tLoss: 277500.7812\n",
      "Training Epoch: 1 [3660/49669]\tLoss: 258375.6875\n",
      "Training Epoch: 1 [3680/49669]\tLoss: 301420.0938\n",
      "Training Epoch: 1 [3700/49669]\tLoss: 299485.3125\n",
      "Training Epoch: 1 [3720/49669]\tLoss: 314791.6562\n",
      "Training Epoch: 1 [3740/49669]\tLoss: 294830.8125\n",
      "Training Epoch: 1 [3760/49669]\tLoss: 310814.1250\n",
      "Training Epoch: 1 [3780/49669]\tLoss: 267627.6875\n",
      "Training Epoch: 1 [3800/49669]\tLoss: 228441.2500\n",
      "Training Epoch: 1 [3820/49669]\tLoss: 265485.8750\n",
      "Training Epoch: 1 [3840/49669]\tLoss: 278089.8125\n",
      "Training Epoch: 1 [3860/49669]\tLoss: 196402.6094\n",
      "Training Epoch: 1 [3880/49669]\tLoss: 236835.8438\n",
      "Training Epoch: 1 [3900/49669]\tLoss: 360079.3125\n",
      "Training Epoch: 1 [3920/49669]\tLoss: 272140.1562\n",
      "Training Epoch: 1 [3940/49669]\tLoss: 255846.3281\n",
      "Training Epoch: 1 [3960/49669]\tLoss: 258147.7969\n",
      "Training Epoch: 1 [3980/49669]\tLoss: 222080.0469\n",
      "Training Epoch: 1 [4000/49669]\tLoss: 212294.3594\n",
      "Training Epoch: 1 [4020/49669]\tLoss: 250289.3594\n",
      "Training Epoch: 1 [4040/49669]\tLoss: 328865.2188\n",
      "Training Epoch: 1 [4060/49669]\tLoss: 315845.1250\n",
      "Training Epoch: 1 [4080/49669]\tLoss: 214020.5625\n",
      "Training Epoch: 1 [4100/49669]\tLoss: 243690.7031\n",
      "Training Epoch: 1 [4120/49669]\tLoss: 280046.5000\n",
      "Training Epoch: 1 [4140/49669]\tLoss: 245539.2812\n",
      "Training Epoch: 1 [4160/49669]\tLoss: 238196.0000\n",
      "Training Epoch: 1 [4180/49669]\tLoss: 181231.4219\n",
      "Training Epoch: 1 [4200/49669]\tLoss: 289784.5625\n",
      "Training Epoch: 1 [4220/49669]\tLoss: 282543.0625\n",
      "Training Epoch: 1 [4240/49669]\tLoss: 286739.5625\n",
      "Training Epoch: 1 [4260/49669]\tLoss: 209433.2656\n",
      "Training Epoch: 1 [4280/49669]\tLoss: 190539.3750\n",
      "Training Epoch: 1 [4300/49669]\tLoss: 242435.8594\n",
      "Training Epoch: 1 [4320/49669]\tLoss: 252981.2812\n",
      "Training Epoch: 1 [4340/49669]\tLoss: 235113.3125\n",
      "Training Epoch: 1 [4360/49669]\tLoss: 373371.5938\n",
      "Training Epoch: 1 [4380/49669]\tLoss: 325836.8438\n",
      "Training Epoch: 1 [4400/49669]\tLoss: 180960.7344\n",
      "Training Epoch: 1 [4420/49669]\tLoss: 298381.9688\n",
      "Training Epoch: 1 [4440/49669]\tLoss: 297761.5312\n",
      "Training Epoch: 1 [4460/49669]\tLoss: 306547.9062\n",
      "Training Epoch: 1 [4480/49669]\tLoss: 234573.6875\n",
      "Training Epoch: 1 [4500/49669]\tLoss: 241123.7188\n",
      "Training Epoch: 1 [4520/49669]\tLoss: 251281.7812\n",
      "Training Epoch: 1 [4540/49669]\tLoss: 299464.5312\n",
      "Training Epoch: 1 [4560/49669]\tLoss: 253755.6094\n",
      "Training Epoch: 1 [4580/49669]\tLoss: 173720.8125\n",
      "Training Epoch: 1 [4600/49669]\tLoss: 262396.9688\n",
      "Training Epoch: 1 [4620/49669]\tLoss: 248264.1875\n",
      "Training Epoch: 1 [4640/49669]\tLoss: 178895.5156\n",
      "Training Epoch: 1 [4660/49669]\tLoss: 235901.5156\n",
      "Training Epoch: 1 [4680/49669]\tLoss: 222400.0781\n",
      "Training Epoch: 1 [4700/49669]\tLoss: 216800.1406\n",
      "Training Epoch: 1 [4720/49669]\tLoss: 237485.6406\n",
      "Training Epoch: 1 [4740/49669]\tLoss: 269580.8750\n",
      "Training Epoch: 1 [4760/49669]\tLoss: 332786.6250\n",
      "Training Epoch: 1 [4780/49669]\tLoss: 210358.0156\n",
      "Training Epoch: 1 [4800/49669]\tLoss: 222974.2031\n",
      "Training Epoch: 1 [4820/49669]\tLoss: 267406.6250\n",
      "Training Epoch: 1 [4840/49669]\tLoss: 210526.7031\n",
      "Training Epoch: 1 [4860/49669]\tLoss: 248686.5469\n",
      "Training Epoch: 1 [4880/49669]\tLoss: 258949.7031\n",
      "Training Epoch: 1 [4900/49669]\tLoss: 269387.0000\n",
      "Training Epoch: 1 [4920/49669]\tLoss: 236748.7656\n",
      "Training Epoch: 1 [4940/49669]\tLoss: 218305.2344\n",
      "Training Epoch: 1 [4960/49669]\tLoss: 244123.9219\n",
      "Training Epoch: 1 [4980/49669]\tLoss: 267847.3750\n",
      "Training Epoch: 1 [5000/49669]\tLoss: 233028.9062\n",
      "Training Epoch: 1 [5020/49669]\tLoss: 238927.5312\n",
      "Training Epoch: 1 [5040/49669]\tLoss: 238905.2188\n",
      "Training Epoch: 1 [5060/49669]\tLoss: 174265.6250\n",
      "Training Epoch: 1 [5080/49669]\tLoss: 186429.9844\n",
      "Training Epoch: 1 [5100/49669]\tLoss: 324514.5938\n",
      "Training Epoch: 1 [5120/49669]\tLoss: 253416.0312\n",
      "Training Epoch: 1 [5140/49669]\tLoss: 197193.2344\n",
      "Training Epoch: 1 [5160/49669]\tLoss: 151737.6094\n",
      "Training Epoch: 1 [5180/49669]\tLoss: 209827.7031\n",
      "Training Epoch: 1 [5200/49669]\tLoss: 189909.9219\n",
      "Training Epoch: 1 [5220/49669]\tLoss: 284434.4375\n",
      "Training Epoch: 1 [5240/49669]\tLoss: 207452.5938\n",
      "Training Epoch: 1 [5260/49669]\tLoss: 268287.5000\n",
      "Training Epoch: 1 [5280/49669]\tLoss: 201131.2656\n",
      "Training Epoch: 1 [5300/49669]\tLoss: 198692.1562\n",
      "Training Epoch: 1 [5320/49669]\tLoss: 244838.2656\n",
      "Training Epoch: 1 [5340/49669]\tLoss: 221724.6406\n",
      "Training Epoch: 1 [5360/49669]\tLoss: 177311.4844\n",
      "Training Epoch: 1 [5380/49669]\tLoss: 188815.0000\n",
      "Training Epoch: 1 [5400/49669]\tLoss: 245306.4688\n",
      "Training Epoch: 1 [5420/49669]\tLoss: 216502.5000\n",
      "Training Epoch: 1 [5440/49669]\tLoss: 233522.1719\n",
      "Training Epoch: 1 [5460/49669]\tLoss: 212527.8438\n",
      "Training Epoch: 1 [5480/49669]\tLoss: 200068.1562\n",
      "Training Epoch: 1 [5500/49669]\tLoss: 240857.0469\n",
      "Training Epoch: 1 [5520/49669]\tLoss: 227977.1250\n",
      "Training Epoch: 1 [5540/49669]\tLoss: 197859.7500\n",
      "Training Epoch: 1 [5560/49669]\tLoss: 227730.9062\n",
      "Training Epoch: 1 [5580/49669]\tLoss: 243365.8906\n",
      "Training Epoch: 1 [5600/49669]\tLoss: 167157.2344\n",
      "Training Epoch: 1 [5620/49669]\tLoss: 207855.0156\n",
      "Training Epoch: 1 [5640/49669]\tLoss: 197220.2500\n",
      "Training Epoch: 1 [5660/49669]\tLoss: 182095.2969\n",
      "Training Epoch: 1 [5680/49669]\tLoss: 215602.1562\n",
      "Training Epoch: 1 [5700/49669]\tLoss: 228655.5625\n",
      "Training Epoch: 1 [5720/49669]\tLoss: 148534.0625\n",
      "Training Epoch: 1 [5740/49669]\tLoss: 210240.9844\n",
      "Training Epoch: 1 [5760/49669]\tLoss: 195557.0156\n",
      "Training Epoch: 1 [5780/49669]\tLoss: 232326.2031\n",
      "Training Epoch: 1 [5800/49669]\tLoss: 190680.9375\n",
      "Training Epoch: 1 [5820/49669]\tLoss: 181654.8438\n",
      "Training Epoch: 1 [5840/49669]\tLoss: 212229.1875\n",
      "Training Epoch: 1 [5860/49669]\tLoss: 200221.9062\n",
      "Training Epoch: 1 [5880/49669]\tLoss: 248717.4062\n",
      "Training Epoch: 1 [5900/49669]\tLoss: 190493.7812\n",
      "Training Epoch: 1 [5920/49669]\tLoss: 209723.0000\n",
      "Training Epoch: 1 [5940/49669]\tLoss: 187303.6875\n",
      "Training Epoch: 1 [5960/49669]\tLoss: 190651.6562\n",
      "Training Epoch: 1 [5980/49669]\tLoss: 234980.2344\n",
      "Training Epoch: 1 [6000/49669]\tLoss: 187229.6406\n",
      "Training Epoch: 1 [6020/49669]\tLoss: 167791.5781\n",
      "Training Epoch: 1 [6040/49669]\tLoss: 240795.5781\n",
      "Training Epoch: 1 [6060/49669]\tLoss: 229457.4844\n",
      "Training Epoch: 1 [6080/49669]\tLoss: 188594.8594\n",
      "Training Epoch: 1 [6100/49669]\tLoss: 146154.7031\n",
      "Training Epoch: 1 [6120/49669]\tLoss: 200568.9531\n",
      "Training Epoch: 1 [6140/49669]\tLoss: 179042.0156\n",
      "Training Epoch: 1 [6160/49669]\tLoss: 188315.2188\n",
      "Training Epoch: 1 [6180/49669]\tLoss: 185050.4062\n",
      "Training Epoch: 1 [6200/49669]\tLoss: 196986.4688\n",
      "Training Epoch: 1 [6220/49669]\tLoss: 131898.4219\n",
      "Training Epoch: 1 [6240/49669]\tLoss: 200458.1875\n",
      "Training Epoch: 1 [6260/49669]\tLoss: 191781.1875\n",
      "Training Epoch: 1 [6280/49669]\tLoss: 211221.8750\n",
      "Training Epoch: 1 [6300/49669]\tLoss: 208849.8750\n",
      "Training Epoch: 1 [6320/49669]\tLoss: 211680.5156\n",
      "Training Epoch: 1 [6340/49669]\tLoss: 148694.4844\n",
      "Training Epoch: 1 [6360/49669]\tLoss: 184213.6719\n",
      "Training Epoch: 1 [6380/49669]\tLoss: 178255.7969\n",
      "Training Epoch: 1 [6400/49669]\tLoss: 219791.0781\n",
      "Training Epoch: 1 [6420/49669]\tLoss: 195586.1562\n",
      "Training Epoch: 1 [6440/49669]\tLoss: 144314.0469\n",
      "Training Epoch: 1 [6460/49669]\tLoss: 182727.5312\n",
      "Training Epoch: 1 [6480/49669]\tLoss: 202714.9531\n",
      "Training Epoch: 1 [6500/49669]\tLoss: 188380.8750\n",
      "Training Epoch: 1 [6520/49669]\tLoss: 204652.6406\n",
      "Training Epoch: 1 [6540/49669]\tLoss: 180021.2656\n",
      "Training Epoch: 1 [6560/49669]\tLoss: 256446.5156\n",
      "Training Epoch: 1 [6580/49669]\tLoss: 231525.5781\n",
      "Training Epoch: 1 [6600/49669]\tLoss: 184305.5781\n",
      "Training Epoch: 1 [6620/49669]\tLoss: 151152.8594\n",
      "Training Epoch: 1 [6640/49669]\tLoss: 238022.9531\n",
      "Training Epoch: 1 [6660/49669]\tLoss: 189306.4844\n",
      "Training Epoch: 1 [6680/49669]\tLoss: 166932.9688\n",
      "Training Epoch: 1 [6700/49669]\tLoss: 191755.5156\n",
      "Training Epoch: 1 [6720/49669]\tLoss: 208712.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [6740/49669]\tLoss: 167596.2188\n",
      "Training Epoch: 1 [6760/49669]\tLoss: 245383.0000\n",
      "Training Epoch: 1 [6780/49669]\tLoss: 150397.2812\n",
      "Training Epoch: 1 [6800/49669]\tLoss: 208541.5469\n",
      "Training Epoch: 1 [6820/49669]\tLoss: 172541.8438\n",
      "Training Epoch: 1 [6840/49669]\tLoss: 189706.3438\n",
      "Training Epoch: 1 [6860/49669]\tLoss: 200592.7344\n",
      "Training Epoch: 1 [6880/49669]\tLoss: 217584.1719\n",
      "Training Epoch: 1 [6900/49669]\tLoss: 186651.1250\n",
      "Training Epoch: 1 [6920/49669]\tLoss: 223817.9062\n",
      "Training Epoch: 1 [6940/49669]\tLoss: 229878.6875\n",
      "Training Epoch: 1 [6960/49669]\tLoss: 224099.9688\n",
      "Training Epoch: 1 [6980/49669]\tLoss: 177144.4531\n",
      "Training Epoch: 1 [7000/49669]\tLoss: 212799.0625\n",
      "Training Epoch: 1 [7020/49669]\tLoss: 150062.4844\n",
      "Training Epoch: 1 [7040/49669]\tLoss: 182453.2969\n",
      "Training Epoch: 1 [7060/49669]\tLoss: 194234.5938\n",
      "Training Epoch: 1 [7080/49669]\tLoss: 198950.7656\n",
      "Training Epoch: 1 [7100/49669]\tLoss: 162530.5625\n",
      "Training Epoch: 1 [7120/49669]\tLoss: 154839.9219\n",
      "Training Epoch: 1 [7140/49669]\tLoss: 173861.1719\n",
      "Training Epoch: 1 [7160/49669]\tLoss: 159729.3906\n",
      "Training Epoch: 1 [7180/49669]\tLoss: 167936.9844\n",
      "Training Epoch: 1 [7200/49669]\tLoss: 176839.5938\n",
      "Training Epoch: 1 [7220/49669]\tLoss: 192396.3906\n",
      "Training Epoch: 1 [7240/49669]\tLoss: 185912.1094\n",
      "Training Epoch: 1 [7260/49669]\tLoss: 165158.3750\n",
      "Training Epoch: 1 [7280/49669]\tLoss: 171403.4062\n",
      "Training Epoch: 1 [7300/49669]\tLoss: 166449.5625\n",
      "Training Epoch: 1 [7320/49669]\tLoss: 242207.0781\n",
      "Training Epoch: 1 [7340/49669]\tLoss: 184070.7031\n",
      "Training Epoch: 1 [7360/49669]\tLoss: 166004.2656\n",
      "Training Epoch: 1 [7380/49669]\tLoss: 181486.4688\n",
      "Training Epoch: 1 [7400/49669]\tLoss: 139147.2188\n",
      "Training Epoch: 1 [7420/49669]\tLoss: 167902.6094\n",
      "Training Epoch: 1 [7440/49669]\tLoss: 184924.9375\n",
      "Training Epoch: 1 [7460/49669]\tLoss: 198383.6406\n",
      "Training Epoch: 1 [7480/49669]\tLoss: 199352.6875\n",
      "Training Epoch: 1 [7500/49669]\tLoss: 210245.6406\n",
      "Training Epoch: 1 [7520/49669]\tLoss: 201136.9375\n",
      "Training Epoch: 1 [7540/49669]\tLoss: 147914.7656\n",
      "Training Epoch: 1 [7560/49669]\tLoss: 201392.8906\n",
      "Training Epoch: 1 [7580/49669]\tLoss: 154645.2656\n",
      "Training Epoch: 1 [7600/49669]\tLoss: 184683.6719\n",
      "Training Epoch: 1 [7620/49669]\tLoss: 151778.5156\n",
      "Training Epoch: 1 [7640/49669]\tLoss: 207226.3594\n",
      "Training Epoch: 1 [7660/49669]\tLoss: 174733.7188\n",
      "Training Epoch: 1 [7680/49669]\tLoss: 142667.9688\n",
      "Training Epoch: 1 [7700/49669]\tLoss: 227146.0469\n",
      "Training Epoch: 1 [7720/49669]\tLoss: 146158.6250\n",
      "Training Epoch: 1 [7740/49669]\tLoss: 197734.7812\n",
      "Training Epoch: 1 [7760/49669]\tLoss: 188477.3594\n",
      "Training Epoch: 1 [7780/49669]\tLoss: 201225.5625\n",
      "Training Epoch: 1 [7800/49669]\tLoss: 174777.4062\n",
      "Training Epoch: 1 [7820/49669]\tLoss: 151833.2031\n",
      "Training Epoch: 1 [7840/49669]\tLoss: 153459.1406\n",
      "Training Epoch: 1 [7860/49669]\tLoss: 185257.4531\n",
      "Training Epoch: 1 [7880/49669]\tLoss: 180160.2969\n",
      "Training Epoch: 1 [7900/49669]\tLoss: 139517.5156\n",
      "Training Epoch: 1 [7920/49669]\tLoss: 156716.3125\n",
      "Training Epoch: 1 [7940/49669]\tLoss: 186916.2188\n",
      "Training Epoch: 1 [7960/49669]\tLoss: 159167.6562\n",
      "Training Epoch: 1 [7980/49669]\tLoss: 157869.5469\n",
      "Training Epoch: 1 [8000/49669]\tLoss: 182870.3594\n",
      "Training Epoch: 1 [8020/49669]\tLoss: 222489.5000\n",
      "Training Epoch: 1 [8040/49669]\tLoss: 150528.6094\n",
      "Training Epoch: 1 [8060/49669]\tLoss: 164362.1406\n",
      "Training Epoch: 1 [8080/49669]\tLoss: 202222.3750\n",
      "Training Epoch: 1 [8100/49669]\tLoss: 171307.2344\n",
      "Training Epoch: 1 [8120/49669]\tLoss: 170722.6406\n",
      "Training Epoch: 1 [8140/49669]\tLoss: 189878.3438\n",
      "Training Epoch: 1 [8160/49669]\tLoss: 173872.3906\n",
      "Training Epoch: 1 [8180/49669]\tLoss: 179643.6562\n",
      "Training Epoch: 1 [8200/49669]\tLoss: 125509.9766\n",
      "Training Epoch: 1 [8220/49669]\tLoss: 191712.7031\n",
      "Training Epoch: 1 [8240/49669]\tLoss: 153963.7031\n",
      "Training Epoch: 1 [8260/49669]\tLoss: 148238.1719\n",
      "Training Epoch: 1 [8280/49669]\tLoss: 167440.1094\n",
      "Training Epoch: 1 [8300/49669]\tLoss: 162500.1719\n",
      "Training Epoch: 1 [8320/49669]\tLoss: 198245.4219\n",
      "Training Epoch: 1 [8340/49669]\tLoss: 155500.9219\n",
      "Training Epoch: 1 [8360/49669]\tLoss: 197035.3125\n",
      "Training Epoch: 1 [8380/49669]\tLoss: 159772.7656\n",
      "Training Epoch: 1 [8400/49669]\tLoss: 163903.9062\n",
      "Training Epoch: 1 [8420/49669]\tLoss: 167898.4531\n",
      "Training Epoch: 1 [8440/49669]\tLoss: 188456.5938\n",
      "Training Epoch: 1 [8460/49669]\tLoss: 192096.8438\n",
      "Training Epoch: 1 [8480/49669]\tLoss: 166446.4375\n",
      "Training Epoch: 1 [8500/49669]\tLoss: 192616.1875\n",
      "Training Epoch: 1 [8520/49669]\tLoss: 183961.3906\n",
      "Training Epoch: 1 [8540/49669]\tLoss: 200358.4844\n",
      "Training Epoch: 1 [8560/49669]\tLoss: 149623.6562\n",
      "Training Epoch: 1 [8580/49669]\tLoss: 155485.0625\n",
      "Training Epoch: 1 [8600/49669]\tLoss: 219829.8906\n",
      "Training Epoch: 1 [8620/49669]\tLoss: 206509.1562\n",
      "Training Epoch: 1 [8640/49669]\tLoss: 155071.7188\n",
      "Training Epoch: 1 [8660/49669]\tLoss: 157459.0312\n",
      "Training Epoch: 1 [8680/49669]\tLoss: 175986.0625\n",
      "Training Epoch: 1 [8700/49669]\tLoss: 150972.4531\n",
      "Training Epoch: 1 [8720/49669]\tLoss: 157098.3750\n",
      "Training Epoch: 1 [8740/49669]\tLoss: 179598.0000\n",
      "Training Epoch: 1 [8760/49669]\tLoss: 183175.1562\n",
      "Training Epoch: 1 [8780/49669]\tLoss: 142627.0781\n",
      "Training Epoch: 1 [8800/49669]\tLoss: 180299.6875\n",
      "Training Epoch: 1 [8820/49669]\tLoss: 169586.4062\n",
      "Training Epoch: 1 [8840/49669]\tLoss: 156175.6875\n",
      "Training Epoch: 1 [8860/49669]\tLoss: 140703.6250\n",
      "Training Epoch: 1 [8880/49669]\tLoss: 164955.1562\n",
      "Training Epoch: 1 [8900/49669]\tLoss: 198262.1875\n",
      "Training Epoch: 1 [8920/49669]\tLoss: 178708.4062\n",
      "Training Epoch: 1 [8940/49669]\tLoss: 153376.2500\n",
      "Training Epoch: 1 [8960/49669]\tLoss: 160855.8594\n",
      "Training Epoch: 1 [8980/49669]\tLoss: 192872.6250\n",
      "Training Epoch: 1 [9000/49669]\tLoss: 212497.3594\n",
      "Training Epoch: 1 [9020/49669]\tLoss: 193270.0625\n",
      "Training Epoch: 1 [9040/49669]\tLoss: 172634.2969\n",
      "Training Epoch: 1 [9060/49669]\tLoss: 202364.1719\n",
      "Training Epoch: 1 [9080/49669]\tLoss: 155263.3281\n",
      "Training Epoch: 1 [9100/49669]\tLoss: 186673.6562\n",
      "Training Epoch: 1 [9120/49669]\tLoss: 192112.3750\n",
      "Training Epoch: 1 [9140/49669]\tLoss: 173805.5938\n",
      "Training Epoch: 1 [9160/49669]\tLoss: 177482.4531\n",
      "Training Epoch: 1 [9180/49669]\tLoss: 177090.8594\n",
      "Training Epoch: 1 [9200/49669]\tLoss: 142506.4375\n",
      "Training Epoch: 1 [9220/49669]\tLoss: 184239.8281\n",
      "Training Epoch: 1 [9240/49669]\tLoss: 183694.5938\n",
      "Training Epoch: 1 [9260/49669]\tLoss: 159764.8125\n",
      "Training Epoch: 1 [9280/49669]\tLoss: 174246.5312\n",
      "Training Epoch: 1 [9300/49669]\tLoss: 140447.8594\n",
      "Training Epoch: 1 [9320/49669]\tLoss: 182350.2969\n",
      "Training Epoch: 1 [9340/49669]\tLoss: 124535.4844\n",
      "Training Epoch: 1 [9360/49669]\tLoss: 184507.5469\n",
      "Training Epoch: 1 [9380/49669]\tLoss: 130144.2344\n",
      "Training Epoch: 1 [9400/49669]\tLoss: 183901.6562\n",
      "Training Epoch: 1 [9420/49669]\tLoss: 157904.7812\n",
      "Training Epoch: 1 [9440/49669]\tLoss: 204205.8594\n",
      "Training Epoch: 1 [9460/49669]\tLoss: 146553.3594\n",
      "Training Epoch: 1 [9480/49669]\tLoss: 148359.4375\n",
      "Training Epoch: 1 [9500/49669]\tLoss: 137536.8594\n",
      "Training Epoch: 1 [9520/49669]\tLoss: 138513.6406\n",
      "Training Epoch: 1 [9540/49669]\tLoss: 165443.3750\n",
      "Training Epoch: 1 [9560/49669]\tLoss: 147938.0625\n",
      "Training Epoch: 1 [9580/49669]\tLoss: 191269.5781\n",
      "Training Epoch: 1 [9600/49669]\tLoss: 139257.6406\n",
      "Training Epoch: 1 [9620/49669]\tLoss: 161930.3125\n",
      "Training Epoch: 1 [9640/49669]\tLoss: 149323.7969\n",
      "Training Epoch: 1 [9660/49669]\tLoss: 152188.7812\n",
      "Training Epoch: 1 [9680/49669]\tLoss: 181670.8750\n",
      "Training Epoch: 1 [9700/49669]\tLoss: 146652.1719\n",
      "Training Epoch: 1 [9720/49669]\tLoss: 157910.6250\n",
      "Training Epoch: 1 [9740/49669]\tLoss: 137297.7344\n",
      "Training Epoch: 1 [9760/49669]\tLoss: 175367.9062\n",
      "Training Epoch: 1 [9780/49669]\tLoss: 154466.2344\n",
      "Training Epoch: 1 [9800/49669]\tLoss: 180958.1875\n",
      "Training Epoch: 1 [9820/49669]\tLoss: 146893.9688\n",
      "Training Epoch: 1 [9840/49669]\tLoss: 163758.4062\n",
      "Training Epoch: 1 [9860/49669]\tLoss: 167325.4531\n",
      "Training Epoch: 1 [9880/49669]\tLoss: 159269.9375\n",
      "Training Epoch: 1 [9900/49669]\tLoss: 181409.3750\n",
      "Training Epoch: 1 [9920/49669]\tLoss: 140252.2188\n",
      "Training Epoch: 1 [9940/49669]\tLoss: 195119.5625\n",
      "Training Epoch: 1 [9960/49669]\tLoss: 196049.2969\n",
      "Training Epoch: 1 [9980/49669]\tLoss: 164003.3125\n",
      "Training Epoch: 1 [10000/49669]\tLoss: 115987.3438\n",
      "Training Epoch: 1 [10020/49669]\tLoss: 173054.8438\n",
      "Training Epoch: 1 [10040/49669]\tLoss: 198470.9219\n",
      "Training Epoch: 1 [10060/49669]\tLoss: 137772.4219\n",
      "Training Epoch: 1 [10080/49669]\tLoss: 141911.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [10100/49669]\tLoss: 175074.0781\n",
      "Training Epoch: 1 [10120/49669]\tLoss: 156310.4844\n",
      "Training Epoch: 1 [10140/49669]\tLoss: 182808.0000\n",
      "Training Epoch: 1 [10160/49669]\tLoss: 134538.4688\n",
      "Training Epoch: 1 [10180/49669]\tLoss: 149353.0781\n",
      "Training Epoch: 1 [10200/49669]\tLoss: 152016.5781\n",
      "Training Epoch: 1 [10220/49669]\tLoss: 181797.4844\n",
      "Training Epoch: 1 [10240/49669]\tLoss: 147133.5625\n",
      "Training Epoch: 1 [10260/49669]\tLoss: 168790.7031\n",
      "Training Epoch: 1 [10280/49669]\tLoss: 160441.6250\n",
      "Training Epoch: 1 [10300/49669]\tLoss: 141556.6875\n",
      "Training Epoch: 1 [10320/49669]\tLoss: 140596.4375\n",
      "Training Epoch: 1 [10340/49669]\tLoss: 163964.7812\n",
      "Training Epoch: 1 [10360/49669]\tLoss: 158340.6094\n",
      "Training Epoch: 1 [10380/49669]\tLoss: 147280.7500\n",
      "Training Epoch: 1 [10400/49669]\tLoss: 122706.9609\n",
      "Training Epoch: 1 [10420/49669]\tLoss: 155335.7969\n",
      "Training Epoch: 1 [10440/49669]\tLoss: 154708.5625\n",
      "Training Epoch: 1 [10460/49669]\tLoss: 125826.9531\n",
      "Training Epoch: 1 [10480/49669]\tLoss: 175668.8125\n",
      "Training Epoch: 1 [10500/49669]\tLoss: 145157.6719\n",
      "Training Epoch: 1 [10520/49669]\tLoss: 145172.9375\n",
      "Training Epoch: 1 [10540/49669]\tLoss: 165982.2500\n",
      "Training Epoch: 1 [10560/49669]\tLoss: 178975.1406\n",
      "Training Epoch: 1 [10580/49669]\tLoss: 183740.8594\n",
      "Training Epoch: 1 [10600/49669]\tLoss: 138253.6094\n",
      "Training Epoch: 1 [10620/49669]\tLoss: 140595.2188\n",
      "Training Epoch: 1 [10640/49669]\tLoss: 126544.4922\n",
      "Training Epoch: 1 [10660/49669]\tLoss: 148435.7344\n",
      "Training Epoch: 1 [10680/49669]\tLoss: 182721.2500\n",
      "Training Epoch: 1 [10700/49669]\tLoss: 136089.4531\n",
      "Training Epoch: 1 [10720/49669]\tLoss: 141901.0938\n",
      "Training Epoch: 1 [10740/49669]\tLoss: 132540.4219\n",
      "Training Epoch: 1 [10760/49669]\tLoss: 138863.9531\n",
      "Training Epoch: 1 [10780/49669]\tLoss: 152657.8594\n",
      "Training Epoch: 1 [10800/49669]\tLoss: 171885.2031\n",
      "Training Epoch: 1 [10820/49669]\tLoss: 147695.2031\n",
      "Training Epoch: 1 [10840/49669]\tLoss: 179567.6250\n",
      "Training Epoch: 1 [10860/49669]\tLoss: 165475.2812\n",
      "Training Epoch: 1 [10880/49669]\tLoss: 167391.2344\n",
      "Training Epoch: 1 [10900/49669]\tLoss: 215584.7031\n",
      "Training Epoch: 1 [10920/49669]\tLoss: 119721.2578\n",
      "Training Epoch: 1 [10940/49669]\tLoss: 127609.9375\n",
      "Training Epoch: 1 [10960/49669]\tLoss: 151516.6875\n",
      "Training Epoch: 1 [10980/49669]\tLoss: 122084.1875\n",
      "Training Epoch: 1 [11000/49669]\tLoss: 155673.7031\n",
      "Training Epoch: 1 [11020/49669]\tLoss: 161265.9219\n",
      "Training Epoch: 1 [11040/49669]\tLoss: 136003.8750\n",
      "Training Epoch: 1 [11060/49669]\tLoss: 141122.6875\n",
      "Training Epoch: 1 [11080/49669]\tLoss: 155340.2656\n",
      "Training Epoch: 1 [11100/49669]\tLoss: 153527.4844\n",
      "Training Epoch: 1 [11120/49669]\tLoss: 145593.1250\n",
      "Training Epoch: 1 [11140/49669]\tLoss: 141172.0625\n",
      "Training Epoch: 1 [11160/49669]\tLoss: 140770.3594\n",
      "Training Epoch: 1 [11180/49669]\tLoss: 143869.6250\n",
      "Training Epoch: 1 [11200/49669]\tLoss: 121444.6641\n",
      "Training Epoch: 1 [11220/49669]\tLoss: 129536.8047\n",
      "Training Epoch: 1 [11240/49669]\tLoss: 146195.2500\n",
      "Training Epoch: 1 [11260/49669]\tLoss: 118554.8750\n",
      "Training Epoch: 1 [11280/49669]\tLoss: 134297.6719\n",
      "Training Epoch: 1 [11300/49669]\tLoss: 139780.2344\n",
      "Training Epoch: 1 [11320/49669]\tLoss: 160367.4219\n",
      "Training Epoch: 1 [11340/49669]\tLoss: 156424.3125\n",
      "Training Epoch: 1 [11360/49669]\tLoss: 161059.0625\n",
      "Training Epoch: 1 [11380/49669]\tLoss: 145509.0781\n",
      "Training Epoch: 1 [11400/49669]\tLoss: 129676.2188\n",
      "Training Epoch: 1 [11420/49669]\tLoss: 224997.7656\n",
      "Training Epoch: 1 [11440/49669]\tLoss: 115742.3906\n",
      "Training Epoch: 1 [11460/49669]\tLoss: 134195.5000\n",
      "Training Epoch: 1 [11480/49669]\tLoss: 143820.5469\n",
      "Training Epoch: 1 [11500/49669]\tLoss: 101110.9141\n",
      "Training Epoch: 1 [11520/49669]\tLoss: 124519.0625\n",
      "Training Epoch: 1 [11540/49669]\tLoss: 176013.0469\n",
      "Training Epoch: 1 [11560/49669]\tLoss: 154623.0312\n",
      "Training Epoch: 1 [11580/49669]\tLoss: 148204.1250\n",
      "Training Epoch: 1 [11600/49669]\tLoss: 140148.2500\n",
      "Training Epoch: 1 [11620/49669]\tLoss: 147486.5625\n",
      "Training Epoch: 1 [11640/49669]\tLoss: 163862.0938\n",
      "Training Epoch: 1 [11660/49669]\tLoss: 144815.5781\n",
      "Training Epoch: 1 [11680/49669]\tLoss: 139150.8438\n",
      "Training Epoch: 1 [11700/49669]\tLoss: 160369.8750\n",
      "Training Epoch: 1 [11720/49669]\tLoss: 137323.2969\n",
      "Training Epoch: 1 [11740/49669]\tLoss: 147597.8281\n",
      "Training Epoch: 1 [11760/49669]\tLoss: 124277.1641\n",
      "Training Epoch: 1 [11780/49669]\tLoss: 128292.2656\n",
      "Training Epoch: 1 [11800/49669]\tLoss: 127953.2031\n",
      "Training Epoch: 1 [11820/49669]\tLoss: 154492.3281\n",
      "Training Epoch: 1 [11840/49669]\tLoss: 143528.5000\n",
      "Training Epoch: 1 [11860/49669]\tLoss: 130204.4062\n",
      "Training Epoch: 1 [11880/49669]\tLoss: 152320.5312\n",
      "Training Epoch: 1 [11900/49669]\tLoss: 141572.1875\n",
      "Training Epoch: 1 [11920/49669]\tLoss: 133440.7812\n",
      "Training Epoch: 1 [11940/49669]\tLoss: 158622.8906\n",
      "Training Epoch: 1 [11960/49669]\tLoss: 136392.6719\n",
      "Training Epoch: 1 [11980/49669]\tLoss: 121335.3438\n",
      "Training Epoch: 1 [12000/49669]\tLoss: 156205.4688\n",
      "Training Epoch: 1 [12020/49669]\tLoss: 153521.5469\n",
      "Training Epoch: 1 [12040/49669]\tLoss: 159912.4062\n",
      "Training Epoch: 1 [12060/49669]\tLoss: 192057.4062\n",
      "Training Epoch: 1 [12080/49669]\tLoss: 121218.2422\n",
      "Training Epoch: 1 [12100/49669]\tLoss: 159871.0312\n",
      "Training Epoch: 1 [12120/49669]\tLoss: 149154.1875\n",
      "Training Epoch: 1 [12140/49669]\tLoss: 175832.5469\n",
      "Training Epoch: 1 [12160/49669]\tLoss: 148473.8750\n",
      "Training Epoch: 1 [12180/49669]\tLoss: 120196.4375\n",
      "Training Epoch: 1 [12200/49669]\tLoss: 125016.1953\n",
      "Training Epoch: 1 [12220/49669]\tLoss: 132817.5625\n",
      "Training Epoch: 1 [12240/49669]\tLoss: 153738.2656\n",
      "Training Epoch: 1 [12260/49669]\tLoss: 164285.4531\n",
      "Training Epoch: 1 [12280/49669]\tLoss: 140791.3281\n",
      "Training Epoch: 1 [12300/49669]\tLoss: 152720.5000\n",
      "Training Epoch: 1 [12320/49669]\tLoss: 170600.9688\n",
      "Training Epoch: 1 [12340/49669]\tLoss: 153669.4531\n",
      "Training Epoch: 1 [12360/49669]\tLoss: 165981.2344\n",
      "Training Epoch: 1 [12380/49669]\tLoss: 116177.6094\n",
      "Training Epoch: 1 [12400/49669]\tLoss: 170747.0156\n",
      "Training Epoch: 1 [12420/49669]\tLoss: 124300.8203\n",
      "Training Epoch: 1 [12440/49669]\tLoss: 163550.8594\n",
      "Training Epoch: 1 [12460/49669]\tLoss: 116233.6797\n",
      "Training Epoch: 1 [12480/49669]\tLoss: 122284.6641\n",
      "Training Epoch: 1 [12500/49669]\tLoss: 138250.7344\n",
      "Training Epoch: 1 [12520/49669]\tLoss: 132314.7031\n",
      "Training Epoch: 1 [12540/49669]\tLoss: 133635.3906\n",
      "Training Epoch: 1 [12560/49669]\tLoss: 105424.4766\n",
      "Training Epoch: 1 [12580/49669]\tLoss: 166741.4688\n",
      "Training Epoch: 1 [12600/49669]\tLoss: 164433.9219\n",
      "Training Epoch: 1 [12620/49669]\tLoss: 147981.3594\n",
      "Training Epoch: 1 [12640/49669]\tLoss: 142979.0938\n",
      "Training Epoch: 1 [12660/49669]\tLoss: 144101.1250\n",
      "Training Epoch: 1 [12680/49669]\tLoss: 117811.2188\n",
      "Training Epoch: 1 [12700/49669]\tLoss: 121911.8672\n",
      "Training Epoch: 1 [12720/49669]\tLoss: 107157.3516\n",
      "Training Epoch: 1 [12740/49669]\tLoss: 162462.5938\n",
      "Training Epoch: 1 [12760/49669]\tLoss: 143818.7344\n",
      "Training Epoch: 1 [12780/49669]\tLoss: 125139.0938\n",
      "Training Epoch: 1 [12800/49669]\tLoss: 159319.7031\n",
      "Training Epoch: 1 [12820/49669]\tLoss: 121099.1250\n",
      "Training Epoch: 1 [12840/49669]\tLoss: 125946.1719\n",
      "Training Epoch: 1 [12860/49669]\tLoss: 143414.4844\n",
      "Training Epoch: 1 [12880/49669]\tLoss: 138520.0781\n",
      "Training Epoch: 1 [12900/49669]\tLoss: 127452.9531\n",
      "Training Epoch: 1 [12920/49669]\tLoss: 90492.0547\n",
      "Training Epoch: 1 [12940/49669]\tLoss: 127901.3438\n",
      "Training Epoch: 1 [12960/49669]\tLoss: 123085.1641\n",
      "Training Epoch: 1 [12980/49669]\tLoss: 140832.6562\n",
      "Training Epoch: 1 [13000/49669]\tLoss: 151125.8750\n",
      "Training Epoch: 1 [13020/49669]\tLoss: 116353.4453\n",
      "Training Epoch: 1 [13040/49669]\tLoss: 116415.3203\n",
      "Training Epoch: 1 [13060/49669]\tLoss: 137819.4531\n",
      "Training Epoch: 1 [13080/49669]\tLoss: 142376.8750\n",
      "Training Epoch: 1 [13100/49669]\tLoss: 124436.6484\n",
      "Training Epoch: 1 [13120/49669]\tLoss: 123609.8828\n",
      "Training Epoch: 1 [13140/49669]\tLoss: 111270.1406\n",
      "Training Epoch: 1 [13160/49669]\tLoss: 131326.5469\n",
      "Training Epoch: 1 [13180/49669]\tLoss: 136910.0000\n",
      "Training Epoch: 1 [13200/49669]\tLoss: 153443.4688\n",
      "Training Epoch: 1 [13220/49669]\tLoss: 316919.6562\n",
      "Training Epoch: 1 [13240/49669]\tLoss: 113324.3594\n",
      "Training Epoch: 1 [13260/49669]\tLoss: 143483.2656\n",
      "Training Epoch: 1 [13280/49669]\tLoss: 126427.5078\n",
      "Training Epoch: 1 [13300/49669]\tLoss: 157858.7031\n",
      "Training Epoch: 1 [13320/49669]\tLoss: 109101.6328\n",
      "Training Epoch: 1 [13340/49669]\tLoss: 127412.4766\n",
      "Training Epoch: 1 [13360/49669]\tLoss: 112659.5469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [13380/49669]\tLoss: 149992.4531\n",
      "Training Epoch: 1 [13400/49669]\tLoss: 97703.1562\n",
      "Training Epoch: 1 [13420/49669]\tLoss: 114134.9375\n",
      "Training Epoch: 1 [13440/49669]\tLoss: 131066.7109\n",
      "Training Epoch: 1 [13460/49669]\tLoss: 119386.1406\n",
      "Training Epoch: 1 [13480/49669]\tLoss: 107441.2109\n",
      "Training Epoch: 1 [13500/49669]\tLoss: 136072.8281\n",
      "Training Epoch: 1 [13520/49669]\tLoss: 131080.5625\n",
      "Training Epoch: 1 [13540/49669]\tLoss: 116819.7969\n",
      "Training Epoch: 1 [13560/49669]\tLoss: 112061.2266\n",
      "Training Epoch: 1 [13580/49669]\tLoss: 150301.6719\n",
      "Training Epoch: 1 [13600/49669]\tLoss: 147166.5625\n",
      "Training Epoch: 1 [13620/49669]\tLoss: 114532.7109\n",
      "Training Epoch: 1 [13640/49669]\tLoss: 120292.6953\n",
      "Training Epoch: 1 [13660/49669]\tLoss: 166324.4219\n",
      "Training Epoch: 1 [13680/49669]\tLoss: 151871.1250\n",
      "Training Epoch: 1 [13700/49669]\tLoss: 117041.1875\n",
      "Training Epoch: 1 [13720/49669]\tLoss: 122156.6641\n",
      "Training Epoch: 1 [13740/49669]\tLoss: 107490.9375\n",
      "Training Epoch: 1 [13760/49669]\tLoss: 110527.2500\n",
      "Training Epoch: 1 [13780/49669]\tLoss: 186386.1875\n",
      "Training Epoch: 1 [13800/49669]\tLoss: 145932.7344\n",
      "Training Epoch: 1 [13820/49669]\tLoss: 147532.9531\n",
      "Training Epoch: 1 [13840/49669]\tLoss: 105413.4531\n",
      "Training Epoch: 1 [13860/49669]\tLoss: 126145.0625\n",
      "Training Epoch: 1 [13880/49669]\tLoss: 143696.6719\n",
      "Training Epoch: 1 [13900/49669]\tLoss: 176458.1875\n",
      "Training Epoch: 1 [13920/49669]\tLoss: 136907.8906\n",
      "Training Epoch: 1 [13940/49669]\tLoss: 161403.6719\n",
      "Training Epoch: 1 [13960/49669]\tLoss: 133439.7656\n",
      "Training Epoch: 1 [13980/49669]\tLoss: 158809.6250\n",
      "Training Epoch: 1 [14000/49669]\tLoss: 139098.9531\n",
      "Training Epoch: 1 [14020/49669]\tLoss: 153827.7031\n",
      "Training Epoch: 1 [14040/49669]\tLoss: 125751.7344\n",
      "Training Epoch: 1 [14060/49669]\tLoss: 120172.7969\n",
      "Training Epoch: 1 [14080/49669]\tLoss: 118000.9375\n",
      "Training Epoch: 1 [14100/49669]\tLoss: 117787.2031\n",
      "Training Epoch: 1 [14120/49669]\tLoss: 131622.5469\n",
      "Training Epoch: 1 [14140/49669]\tLoss: 112051.8672\n",
      "Training Epoch: 1 [14160/49669]\tLoss: 136444.7344\n",
      "Training Epoch: 1 [14180/49669]\tLoss: 154602.2188\n",
      "Training Epoch: 1 [14200/49669]\tLoss: 113302.3281\n",
      "Training Epoch: 1 [14220/49669]\tLoss: 154037.9375\n",
      "Training Epoch: 1 [14240/49669]\tLoss: 160970.7812\n",
      "Training Epoch: 1 [14260/49669]\tLoss: 121856.2422\n",
      "Training Epoch: 1 [14280/49669]\tLoss: 130350.3750\n",
      "Training Epoch: 1 [14300/49669]\tLoss: 102879.9844\n",
      "Training Epoch: 1 [14320/49669]\tLoss: 149437.0781\n",
      "Training Epoch: 1 [14340/49669]\tLoss: 120903.7656\n",
      "Training Epoch: 1 [14360/49669]\tLoss: 150653.2188\n",
      "Training Epoch: 1 [14380/49669]\tLoss: 123183.9766\n",
      "Training Epoch: 1 [14400/49669]\tLoss: 90288.4688\n",
      "Training Epoch: 1 [14420/49669]\tLoss: 109318.3906\n",
      "Training Epoch: 1 [14440/49669]\tLoss: 117878.0312\n",
      "Training Epoch: 1 [14460/49669]\tLoss: 128958.9922\n",
      "Training Epoch: 1 [14480/49669]\tLoss: 122999.6484\n",
      "Training Epoch: 1 [14500/49669]\tLoss: 166616.6094\n",
      "Training Epoch: 1 [14520/49669]\tLoss: 131017.3828\n",
      "Training Epoch: 1 [14540/49669]\tLoss: 147484.0312\n",
      "Training Epoch: 1 [14560/49669]\tLoss: 143464.1875\n",
      "Training Epoch: 1 [14580/49669]\tLoss: 113180.4453\n",
      "Training Epoch: 1 [14600/49669]\tLoss: 118044.6016\n",
      "Training Epoch: 1 [14620/49669]\tLoss: 121501.5703\n",
      "Training Epoch: 1 [14640/49669]\tLoss: 121680.6719\n",
      "Training Epoch: 1 [14660/49669]\tLoss: 169669.1094\n",
      "Training Epoch: 1 [14680/49669]\tLoss: 155074.0938\n",
      "Training Epoch: 1 [14700/49669]\tLoss: 129261.5781\n",
      "Training Epoch: 1 [14720/49669]\tLoss: 136759.4688\n",
      "Training Epoch: 1 [14740/49669]\tLoss: 137140.1094\n",
      "Training Epoch: 1 [14760/49669]\tLoss: 155206.3750\n",
      "Training Epoch: 1 [14780/49669]\tLoss: 128755.5000\n",
      "Training Epoch: 1 [14800/49669]\tLoss: 139010.2031\n",
      "Training Epoch: 1 [14820/49669]\tLoss: 123805.5781\n",
      "Training Epoch: 1 [14840/49669]\tLoss: 115097.7891\n",
      "Training Epoch: 1 [14860/49669]\tLoss: 98164.2969\n",
      "Training Epoch: 1 [14880/49669]\tLoss: 129467.1250\n",
      "Training Epoch: 1 [14900/49669]\tLoss: 123456.2031\n",
      "Training Epoch: 1 [14920/49669]\tLoss: 156439.2188\n",
      "Training Epoch: 1 [14940/49669]\tLoss: 116783.0859\n",
      "Training Epoch: 1 [14960/49669]\tLoss: 116014.9609\n",
      "Training Epoch: 1 [14980/49669]\tLoss: 128726.9062\n",
      "Training Epoch: 1 [15000/49669]\tLoss: 141462.0938\n",
      "Training Epoch: 1 [15020/49669]\tLoss: 122812.3125\n",
      "Training Epoch: 1 [15040/49669]\tLoss: 155526.0312\n",
      "Training Epoch: 1 [15060/49669]\tLoss: 135287.4688\n",
      "Training Epoch: 1 [15080/49669]\tLoss: 134098.9219\n",
      "Training Epoch: 1 [15100/49669]\tLoss: 127379.5859\n",
      "Training Epoch: 1 [15120/49669]\tLoss: 136516.2031\n",
      "Training Epoch: 1 [15140/49669]\tLoss: 110574.3359\n",
      "Training Epoch: 1 [15160/49669]\tLoss: 108453.7344\n",
      "Training Epoch: 1 [15180/49669]\tLoss: 120567.5312\n",
      "Training Epoch: 1 [15200/49669]\tLoss: 122441.0547\n",
      "Training Epoch: 1 [15220/49669]\tLoss: 122453.7188\n",
      "Training Epoch: 1 [15240/49669]\tLoss: 140734.3906\n",
      "Training Epoch: 1 [15260/49669]\tLoss: 111960.0234\n",
      "Training Epoch: 1 [15280/49669]\tLoss: 139554.1094\n",
      "Training Epoch: 1 [15300/49669]\tLoss: 117842.4688\n",
      "Training Epoch: 1 [15320/49669]\tLoss: 150042.6094\n",
      "Training Epoch: 1 [15340/49669]\tLoss: 130625.2188\n",
      "Training Epoch: 1 [15360/49669]\tLoss: 111478.5703\n",
      "Training Epoch: 1 [15380/49669]\tLoss: 105578.7812\n",
      "Training Epoch: 1 [15400/49669]\tLoss: 103944.7188\n",
      "Training Epoch: 1 [15420/49669]\tLoss: 145399.5625\n",
      "Training Epoch: 1 [15440/49669]\tLoss: 129855.1328\n",
      "Training Epoch: 1 [15460/49669]\tLoss: 115838.2344\n",
      "Training Epoch: 1 [15480/49669]\tLoss: 122869.4922\n",
      "Training Epoch: 1 [15500/49669]\tLoss: 111094.5938\n",
      "Training Epoch: 1 [15520/49669]\tLoss: 138445.2812\n",
      "Training Epoch: 1 [15540/49669]\tLoss: 100677.9844\n",
      "Training Epoch: 1 [15560/49669]\tLoss: 103132.0703\n",
      "Training Epoch: 1 [15580/49669]\tLoss: 148831.1875\n",
      "Training Epoch: 1 [15600/49669]\tLoss: 108664.3984\n",
      "Training Epoch: 1 [15620/49669]\tLoss: 145644.5938\n",
      "Training Epoch: 1 [15640/49669]\tLoss: 102723.4688\n",
      "Training Epoch: 1 [15660/49669]\tLoss: 120128.1875\n",
      "Training Epoch: 1 [15680/49669]\tLoss: 121604.0000\n",
      "Training Epoch: 1 [15700/49669]\tLoss: 145588.7344\n",
      "Training Epoch: 1 [15720/49669]\tLoss: 130156.5234\n",
      "Training Epoch: 1 [15740/49669]\tLoss: 87487.2266\n",
      "Training Epoch: 1 [15760/49669]\tLoss: 109818.6016\n",
      "Training Epoch: 1 [15780/49669]\tLoss: 140557.9375\n",
      "Training Epoch: 1 [15800/49669]\tLoss: 112136.3047\n",
      "Training Epoch: 1 [15820/49669]\tLoss: 149714.5312\n",
      "Training Epoch: 1 [15840/49669]\tLoss: 113537.4531\n",
      "Training Epoch: 1 [15860/49669]\tLoss: 122347.2031\n",
      "Training Epoch: 1 [15880/49669]\tLoss: 129262.9297\n",
      "Training Epoch: 1 [15900/49669]\tLoss: 127168.9141\n",
      "Training Epoch: 1 [15920/49669]\tLoss: 106440.5703\n",
      "Training Epoch: 1 [15940/49669]\tLoss: 127142.8047\n",
      "Training Epoch: 1 [15960/49669]\tLoss: 133701.9844\n",
      "Training Epoch: 1 [15980/49669]\tLoss: 110384.6406\n",
      "Training Epoch: 1 [16000/49669]\tLoss: 104923.5703\n",
      "Training Epoch: 1 [16020/49669]\tLoss: 116381.6875\n",
      "Training Epoch: 1 [16040/49669]\tLoss: 142857.1562\n",
      "Training Epoch: 1 [16060/49669]\tLoss: 118005.4219\n",
      "Training Epoch: 1 [16080/49669]\tLoss: 103728.2500\n",
      "Training Epoch: 1 [16100/49669]\tLoss: 126756.0625\n",
      "Training Epoch: 1 [16120/49669]\tLoss: 110769.8750\n",
      "Training Epoch: 1 [16140/49669]\tLoss: 101753.2344\n",
      "Training Epoch: 1 [16160/49669]\tLoss: 107139.6719\n",
      "Training Epoch: 1 [16180/49669]\tLoss: 115211.8125\n",
      "Training Epoch: 1 [16200/49669]\tLoss: 141925.1719\n",
      "Training Epoch: 1 [16220/49669]\tLoss: 126173.1562\n",
      "Training Epoch: 1 [16240/49669]\tLoss: 153402.0000\n",
      "Training Epoch: 1 [16260/49669]\tLoss: 116422.9531\n",
      "Training Epoch: 1 [16280/49669]\tLoss: 123851.3828\n",
      "Training Epoch: 1 [16300/49669]\tLoss: 105490.3203\n",
      "Training Epoch: 1 [16320/49669]\tLoss: 98523.3984\n",
      "Training Epoch: 1 [16340/49669]\tLoss: 112138.3906\n",
      "Training Epoch: 1 [16360/49669]\tLoss: 105053.1641\n",
      "Training Epoch: 1 [16380/49669]\tLoss: 132133.1094\n",
      "Training Epoch: 1 [16400/49669]\tLoss: 105065.0547\n",
      "Training Epoch: 1 [16420/49669]\tLoss: 119457.1719\n",
      "Training Epoch: 1 [16440/49669]\tLoss: 127698.6562\n",
      "Training Epoch: 1 [16460/49669]\tLoss: 129574.1875\n",
      "Training Epoch: 1 [16480/49669]\tLoss: 135372.8438\n",
      "Training Epoch: 1 [16500/49669]\tLoss: 110447.0078\n",
      "Training Epoch: 1 [16520/49669]\tLoss: 111763.9453\n",
      "Training Epoch: 1 [16540/49669]\tLoss: 114081.6953\n",
      "Training Epoch: 1 [16560/49669]\tLoss: 99215.0781\n",
      "Training Epoch: 1 [16580/49669]\tLoss: 114250.4844\n",
      "Training Epoch: 1 [16600/49669]\tLoss: 131328.9375\n",
      "Training Epoch: 1 [16620/49669]\tLoss: 107909.4766\n",
      "Training Epoch: 1 [16640/49669]\tLoss: 121496.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [16660/49669]\tLoss: 120942.6094\n",
      "Training Epoch: 1 [16680/49669]\tLoss: 140149.1250\n",
      "Training Epoch: 1 [16700/49669]\tLoss: 102148.0938\n",
      "Training Epoch: 1 [16720/49669]\tLoss: 146171.7031\n",
      "Training Epoch: 1 [16740/49669]\tLoss: 89302.2812\n",
      "Training Epoch: 1 [16760/49669]\tLoss: 89186.6484\n",
      "Training Epoch: 1 [16780/49669]\tLoss: 102461.0625\n",
      "Training Epoch: 1 [16800/49669]\tLoss: 115820.5469\n",
      "Training Epoch: 1 [16820/49669]\tLoss: 119853.8516\n",
      "Training Epoch: 1 [16840/49669]\tLoss: 125652.0625\n",
      "Training Epoch: 1 [16860/49669]\tLoss: 134667.9375\n",
      "Training Epoch: 1 [16880/49669]\tLoss: 121042.9219\n",
      "Training Epoch: 1 [16900/49669]\tLoss: 126917.1484\n",
      "Training Epoch: 1 [16920/49669]\tLoss: 102256.4609\n",
      "Training Epoch: 1 [16940/49669]\tLoss: 104851.3594\n",
      "Training Epoch: 1 [16960/49669]\tLoss: 126958.1250\n",
      "Training Epoch: 1 [16980/49669]\tLoss: 91069.5078\n",
      "Training Epoch: 1 [17000/49669]\tLoss: 91667.8750\n",
      "Training Epoch: 1 [17020/49669]\tLoss: 121136.5312\n",
      "Training Epoch: 1 [17040/49669]\tLoss: 121148.1250\n",
      "Training Epoch: 1 [17060/49669]\tLoss: 113058.5391\n",
      "Training Epoch: 1 [17080/49669]\tLoss: 103999.5938\n",
      "Training Epoch: 1 [17100/49669]\tLoss: 132514.2969\n",
      "Training Epoch: 1 [17120/49669]\tLoss: 131106.6094\n",
      "Training Epoch: 1 [17140/49669]\tLoss: 91398.6250\n",
      "Training Epoch: 1 [17160/49669]\tLoss: 138190.3125\n",
      "Training Epoch: 1 [17180/49669]\tLoss: 124894.3906\n",
      "Training Epoch: 1 [17200/49669]\tLoss: 99496.5547\n",
      "Training Epoch: 1 [17220/49669]\tLoss: 98417.9453\n",
      "Training Epoch: 1 [17240/49669]\tLoss: 102589.7734\n",
      "Training Epoch: 1 [17260/49669]\tLoss: 109769.3516\n",
      "Training Epoch: 1 [17280/49669]\tLoss: 99955.3750\n",
      "Training Epoch: 1 [17300/49669]\tLoss: 103289.4609\n",
      "Training Epoch: 1 [17320/49669]\tLoss: 107129.5469\n",
      "Training Epoch: 1 [17340/49669]\tLoss: 123876.9609\n",
      "Training Epoch: 1 [17360/49669]\tLoss: 117970.8125\n",
      "Training Epoch: 1 [17380/49669]\tLoss: 96221.7188\n",
      "Training Epoch: 1 [17400/49669]\tLoss: 100072.0859\n",
      "Training Epoch: 1 [17420/49669]\tLoss: 108134.6484\n",
      "Training Epoch: 1 [17440/49669]\tLoss: 106037.9922\n",
      "Training Epoch: 1 [17460/49669]\tLoss: 123333.0547\n",
      "Training Epoch: 1 [17480/49669]\tLoss: 136035.4531\n",
      "Training Epoch: 1 [17500/49669]\tLoss: 145798.7812\n",
      "Training Epoch: 1 [17520/49669]\tLoss: 111007.2031\n",
      "Training Epoch: 1 [17540/49669]\tLoss: 104312.7500\n",
      "Training Epoch: 1 [17560/49669]\tLoss: 101255.5391\n",
      "Training Epoch: 1 [17580/49669]\tLoss: 121525.3438\n",
      "Training Epoch: 1 [17600/49669]\tLoss: 118940.1484\n",
      "Training Epoch: 1 [17620/49669]\tLoss: 133492.9219\n",
      "Training Epoch: 1 [17640/49669]\tLoss: 127577.3672\n",
      "Training Epoch: 1 [17660/49669]\tLoss: 95955.9141\n",
      "Training Epoch: 1 [17680/49669]\tLoss: 107506.4062\n",
      "Training Epoch: 1 [17700/49669]\tLoss: 108817.6484\n",
      "Training Epoch: 1 [17720/49669]\tLoss: 104877.0312\n",
      "Training Epoch: 1 [17740/49669]\tLoss: 94334.5625\n",
      "Training Epoch: 1 [17760/49669]\tLoss: 122291.8516\n",
      "Training Epoch: 1 [17780/49669]\tLoss: 94159.8203\n",
      "Training Epoch: 1 [17800/49669]\tLoss: 108065.6484\n",
      "Training Epoch: 1 [17820/49669]\tLoss: 134482.9531\n",
      "Training Epoch: 1 [17840/49669]\tLoss: 126028.1875\n",
      "Training Epoch: 1 [17860/49669]\tLoss: 121069.0234\n",
      "Training Epoch: 1 [17880/49669]\tLoss: 101425.8906\n",
      "Training Epoch: 1 [17900/49669]\tLoss: 123943.1172\n",
      "Training Epoch: 1 [17920/49669]\tLoss: 114099.7109\n",
      "Training Epoch: 1 [17940/49669]\tLoss: 106738.6562\n",
      "Training Epoch: 1 [17960/49669]\tLoss: 92448.0938\n",
      "Training Epoch: 1 [17980/49669]\tLoss: 121946.6250\n",
      "Training Epoch: 1 [18000/49669]\tLoss: 111326.0859\n",
      "Training Epoch: 1 [18020/49669]\tLoss: 108319.9922\n",
      "Training Epoch: 1 [18040/49669]\tLoss: 127531.8359\n",
      "Training Epoch: 1 [18060/49669]\tLoss: 88203.6094\n",
      "Training Epoch: 1 [18080/49669]\tLoss: 124226.7031\n",
      "Training Epoch: 1 [18100/49669]\tLoss: 114550.9062\n",
      "Training Epoch: 1 [18120/49669]\tLoss: 107762.1406\n",
      "Training Epoch: 1 [18140/49669]\tLoss: 102327.1719\n",
      "Training Epoch: 1 [18160/49669]\tLoss: 93098.0234\n",
      "Training Epoch: 1 [18180/49669]\tLoss: 108080.3203\n",
      "Training Epoch: 1 [18200/49669]\tLoss: 85390.9453\n",
      "Training Epoch: 1 [18220/49669]\tLoss: 124522.3047\n",
      "Training Epoch: 1 [18240/49669]\tLoss: 99524.3438\n",
      "Training Epoch: 1 [18260/49669]\tLoss: 98534.8984\n",
      "Training Epoch: 1 [18280/49669]\tLoss: 102649.4531\n",
      "Training Epoch: 1 [18300/49669]\tLoss: 127022.4766\n",
      "Training Epoch: 1 [18320/49669]\tLoss: 124631.9453\n",
      "Training Epoch: 1 [18340/49669]\tLoss: 104498.8203\n",
      "Training Epoch: 1 [18360/49669]\tLoss: 97106.8047\n",
      "Training Epoch: 1 [18380/49669]\tLoss: 104024.7188\n",
      "Training Epoch: 1 [18400/49669]\tLoss: 109079.9766\n",
      "Training Epoch: 1 [18420/49669]\tLoss: 109420.3359\n",
      "Training Epoch: 1 [18440/49669]\tLoss: 95692.4531\n",
      "Training Epoch: 1 [18460/49669]\tLoss: 98216.5391\n",
      "Training Epoch: 1 [18480/49669]\tLoss: 131097.4219\n",
      "Training Epoch: 1 [18500/49669]\tLoss: 97528.1875\n",
      "Training Epoch: 1 [18520/49669]\tLoss: 96457.2266\n",
      "Training Epoch: 1 [18540/49669]\tLoss: 104599.5234\n",
      "Training Epoch: 1 [18560/49669]\tLoss: 93929.3125\n",
      "Training Epoch: 1 [18580/49669]\tLoss: 110592.5312\n",
      "Training Epoch: 1 [18600/49669]\tLoss: 108754.5938\n",
      "Training Epoch: 1 [18620/49669]\tLoss: 107027.0547\n",
      "Training Epoch: 1 [18640/49669]\tLoss: 125742.8359\n",
      "Training Epoch: 1 [18660/49669]\tLoss: 92335.2891\n",
      "Training Epoch: 1 [18680/49669]\tLoss: 102678.6484\n",
      "Training Epoch: 1 [18700/49669]\tLoss: 94068.2109\n",
      "Training Epoch: 1 [18720/49669]\tLoss: 113455.1719\n",
      "Training Epoch: 1 [18740/49669]\tLoss: 117962.1719\n",
      "Training Epoch: 1 [18760/49669]\tLoss: 121878.4922\n",
      "Training Epoch: 1 [18780/49669]\tLoss: 117702.9609\n",
      "Training Epoch: 1 [18800/49669]\tLoss: 108312.1484\n",
      "Training Epoch: 1 [18820/49669]\tLoss: 108959.0391\n",
      "Training Epoch: 1 [18840/49669]\tLoss: 104657.3281\n",
      "Training Epoch: 1 [18860/49669]\tLoss: 128067.2500\n",
      "Training Epoch: 1 [18880/49669]\tLoss: 123511.8594\n",
      "Training Epoch: 1 [18900/49669]\tLoss: 100315.8750\n",
      "Training Epoch: 1 [18920/49669]\tLoss: 98790.5000\n",
      "Training Epoch: 1 [18940/49669]\tLoss: 109958.1016\n",
      "Training Epoch: 1 [18960/49669]\tLoss: 106190.5078\n",
      "Training Epoch: 1 [18980/49669]\tLoss: 89293.7578\n",
      "Training Epoch: 1 [19000/49669]\tLoss: 117909.6016\n",
      "Training Epoch: 1 [19020/49669]\tLoss: 111492.6562\n",
      "Training Epoch: 1 [19040/49669]\tLoss: 120928.0703\n",
      "Training Epoch: 1 [19060/49669]\tLoss: 85282.6172\n",
      "Training Epoch: 1 [19080/49669]\tLoss: 111286.3516\n",
      "Training Epoch: 1 [19100/49669]\tLoss: 130858.0156\n",
      "Training Epoch: 1 [19120/49669]\tLoss: 114958.4062\n",
      "Training Epoch: 1 [19140/49669]\tLoss: 111397.6250\n",
      "Training Epoch: 1 [19160/49669]\tLoss: 124228.4688\n",
      "Training Epoch: 1 [19180/49669]\tLoss: 104579.7891\n",
      "Training Epoch: 1 [19200/49669]\tLoss: 119704.2031\n",
      "Training Epoch: 1 [19220/49669]\tLoss: 92890.9062\n",
      "Training Epoch: 1 [19240/49669]\tLoss: 94463.8047\n",
      "Training Epoch: 1 [19260/49669]\tLoss: 105260.0547\n",
      "Training Epoch: 1 [19280/49669]\tLoss: 102842.1797\n",
      "Training Epoch: 1 [19300/49669]\tLoss: 111307.7656\n",
      "Training Epoch: 1 [19320/49669]\tLoss: 104460.0391\n",
      "Training Epoch: 1 [19340/49669]\tLoss: 102172.3125\n",
      "Training Epoch: 1 [19360/49669]\tLoss: 92061.5703\n",
      "Training Epoch: 1 [19380/49669]\tLoss: 104990.7969\n",
      "Training Epoch: 1 [19400/49669]\tLoss: 81819.8125\n",
      "Training Epoch: 1 [19420/49669]\tLoss: 85125.6016\n",
      "Training Epoch: 1 [19440/49669]\tLoss: 122682.5156\n",
      "Training Epoch: 1 [19460/49669]\tLoss: 102724.6484\n",
      "Training Epoch: 1 [19480/49669]\tLoss: 92374.1641\n",
      "Training Epoch: 1 [19500/49669]\tLoss: 103755.2969\n",
      "Training Epoch: 1 [19520/49669]\tLoss: 86235.3984\n",
      "Training Epoch: 1 [19540/49669]\tLoss: 86835.8594\n",
      "Training Epoch: 1 [19560/49669]\tLoss: 140409.9688\n",
      "Training Epoch: 1 [19580/49669]\tLoss: 112403.7266\n",
      "Training Epoch: 1 [19600/49669]\tLoss: 112606.6562\n",
      "Training Epoch: 1 [19620/49669]\tLoss: 91749.7656\n",
      "Training Epoch: 1 [19640/49669]\tLoss: 115151.8281\n",
      "Training Epoch: 1 [19660/49669]\tLoss: 92613.8359\n",
      "Training Epoch: 1 [19680/49669]\tLoss: 111295.4219\n",
      "Training Epoch: 1 [19700/49669]\tLoss: 94450.1953\n",
      "Training Epoch: 1 [19720/49669]\tLoss: 82108.3125\n",
      "Training Epoch: 1 [19740/49669]\tLoss: 89834.3516\n",
      "Training Epoch: 1 [19760/49669]\tLoss: 116027.6641\n",
      "Training Epoch: 1 [19780/49669]\tLoss: 93371.5547\n",
      "Training Epoch: 1 [19800/49669]\tLoss: 84033.0391\n",
      "Training Epoch: 1 [19820/49669]\tLoss: 101329.0859\n",
      "Training Epoch: 1 [19840/49669]\tLoss: 111344.0469\n",
      "Training Epoch: 1 [19860/49669]\tLoss: 88848.1641\n",
      "Training Epoch: 1 [19880/49669]\tLoss: 81763.5547\n",
      "Training Epoch: 1 [19900/49669]\tLoss: 102189.8984\n",
      "Training Epoch: 1 [19920/49669]\tLoss: 92214.3828\n",
      "Training Epoch: 1 [19940/49669]\tLoss: 101672.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [19960/49669]\tLoss: 99847.3047\n",
      "Training Epoch: 1 [19980/49669]\tLoss: 103202.6562\n",
      "Training Epoch: 1 [20000/49669]\tLoss: 102383.8281\n",
      "Training Epoch: 1 [20020/49669]\tLoss: 106926.7344\n",
      "Training Epoch: 1 [20040/49669]\tLoss: 96664.1484\n",
      "Training Epoch: 1 [20060/49669]\tLoss: 93106.5078\n",
      "Training Epoch: 1 [20080/49669]\tLoss: 103361.2891\n",
      "Training Epoch: 1 [20100/49669]\tLoss: 109879.0234\n",
      "Training Epoch: 1 [20120/49669]\tLoss: 95254.5000\n",
      "Training Epoch: 1 [20140/49669]\tLoss: 102763.4766\n",
      "Training Epoch: 1 [20160/49669]\tLoss: 127185.2812\n",
      "Training Epoch: 1 [20180/49669]\tLoss: 114441.8125\n",
      "Training Epoch: 1 [20200/49669]\tLoss: 104554.4844\n",
      "Training Epoch: 1 [20220/49669]\tLoss: 105009.3281\n",
      "Training Epoch: 1 [20240/49669]\tLoss: 86087.8594\n",
      "Training Epoch: 1 [20260/49669]\tLoss: 109607.1875\n",
      "Training Epoch: 1 [20280/49669]\tLoss: 104009.0547\n",
      "Training Epoch: 1 [20300/49669]\tLoss: 89379.0078\n",
      "Training Epoch: 1 [20320/49669]\tLoss: 101469.2109\n",
      "Training Epoch: 1 [20340/49669]\tLoss: 95155.5156\n",
      "Training Epoch: 1 [20360/49669]\tLoss: 91546.3438\n",
      "Training Epoch: 1 [20380/49669]\tLoss: 132884.1875\n",
      "Training Epoch: 1 [20400/49669]\tLoss: 96621.0000\n",
      "Training Epoch: 1 [21180/49669]\tLoss: 84895.9609\n",
      "Training Epoch: 1 [21200/49669]\tLoss: 86385.7812\n",
      "Training Epoch: 1 [21220/49669]\tLoss: 83245.6250\n",
      "Training Epoch: 1 [21240/49669]\tLoss: 94150.2266\n",
      "Training Epoch: 1 [21260/49669]\tLoss: 90872.5859\n",
      "Training Epoch: 1 [21280/49669]\tLoss: 72522.3125\n",
      "Training Epoch: 1 [21300/49669]\tLoss: 95279.4844\n",
      "Training Epoch: 1 [21320/49669]\tLoss: 100668.7578\n",
      "Training Epoch: 1 [21340/49669]\tLoss: 95100.6016\n",
      "Training Epoch: 1 [21360/49669]\tLoss: 106173.1875\n",
      "Training Epoch: 1 [21380/49669]\tLoss: 95416.3281\n",
      "Training Epoch: 1 [21400/49669]\tLoss: 119559.5234\n",
      "Training Epoch: 1 [21420/49669]\tLoss: 87899.7891\n",
      "Training Epoch: 1 [21440/49669]\tLoss: 78280.6406\n",
      "Training Epoch: 1 [21460/49669]\tLoss: 103053.3516\n",
      "Training Epoch: 1 [21480/49669]\tLoss: 68419.1016\n",
      "Training Epoch: 1 [21500/49669]\tLoss: 92920.4375\n",
      "Training Epoch: 1 [21520/49669]\tLoss: 93092.4453\n",
      "Training Epoch: 1 [21540/49669]\tLoss: 104465.6562\n",
      "Training Epoch: 1 [21560/49669]\tLoss: 105760.6328\n",
      "Training Epoch: 1 [21580/49669]\tLoss: 92417.0781\n",
      "Training Epoch: 1 [21600/49669]\tLoss: 96494.6484\n",
      "Training Epoch: 1 [21620/49669]\tLoss: 94662.6172\n",
      "Training Epoch: 1 [21640/49669]\tLoss: 115671.2109\n",
      "Training Epoch: 1 [21660/49669]\tLoss: 114434.7031\n",
      "Training Epoch: 1 [21680/49669]\tLoss: 85402.7422\n",
      "Training Epoch: 1 [21700/49669]\tLoss: 88652.1406\n",
      "Training Epoch: 1 [21720/49669]\tLoss: 94046.7031\n",
      "Training Epoch: 1 [21740/49669]\tLoss: 101489.9062\n",
      "Training Epoch: 1 [21760/49669]\tLoss: 88311.7969\n",
      "Training Epoch: 1 [21780/49669]\tLoss: 98751.2734\n",
      "Training Epoch: 1 [21800/49669]\tLoss: 119854.1797\n",
      "Training Epoch: 1 [21820/49669]\tLoss: 111666.2500\n",
      "Training Epoch: 1 [21840/49669]\tLoss: 95759.6016\n",
      "Training Epoch: 1 [21860/49669]\tLoss: 91527.0078\n",
      "Training Epoch: 1 [21880/49669]\tLoss: 105690.2656\n",
      "Training Epoch: 1 [21900/49669]\tLoss: 76296.6328\n",
      "Training Epoch: 1 [21920/49669]\tLoss: 97756.2500\n",
      "Training Epoch: 1 [21940/49669]\tLoss: 73215.7500\n",
      "Training Epoch: 1 [21960/49669]\tLoss: 88910.5078\n",
      "Training Epoch: 1 [21980/49669]\tLoss: 96308.7891\n",
      "Training Epoch: 1 [22000/49669]\tLoss: 104805.6094\n",
      "Training Epoch: 1 [22020/49669]\tLoss: 102872.0859\n",
      "Training Epoch: 1 [22040/49669]\tLoss: 93096.0859\n",
      "Training Epoch: 1 [22060/49669]\tLoss: 79305.5781\n",
      "Training Epoch: 1 [22080/49669]\tLoss: 87349.5547\n",
      "Training Epoch: 1 [22100/49669]\tLoss: 105483.1953\n",
      "Training Epoch: 1 [22120/49669]\tLoss: 85584.7422\n",
      "Training Epoch: 1 [22140/49669]\tLoss: 88706.2578\n",
      "Training Epoch: 1 [22160/49669]\tLoss: 80482.9609\n",
      "Training Epoch: 1 [22180/49669]\tLoss: 94744.9609\n",
      "Training Epoch: 1 [22200/49669]\tLoss: 80205.7031\n",
      "Training Epoch: 1 [22220/49669]\tLoss: 86561.6875\n",
      "Training Epoch: 1 [22240/49669]\tLoss: 90321.2891\n",
      "Training Epoch: 1 [22260/49669]\tLoss: 82826.4688\n",
      "Training Epoch: 1 [22280/49669]\tLoss: 93240.7188\n",
      "Training Epoch: 1 [22300/49669]\tLoss: 88576.8438\n",
      "Training Epoch: 1 [22320/49669]\tLoss: 124376.2344\n",
      "Training Epoch: 1 [22340/49669]\tLoss: 81906.7422\n",
      "Training Epoch: 1 [22360/49669]\tLoss: 82497.7109\n",
      "Training Epoch: 1 [22380/49669]\tLoss: 106226.5859\n",
      "Training Epoch: 1 [22400/49669]\tLoss: 100192.9609\n",
      "Training Epoch: 1 [22420/49669]\tLoss: 80019.1562\n",
      "Training Epoch: 1 [22440/49669]\tLoss: 97697.7266\n",
      "Training Epoch: 1 [22460/49669]\tLoss: 73977.1953\n",
      "Training Epoch: 1 [22480/49669]\tLoss: 99864.3203\n",
      "Training Epoch: 1 [22500/49669]\tLoss: 96611.4297\n",
      "Training Epoch: 1 [22520/49669]\tLoss: 91299.2734\n",
      "Training Epoch: 1 [22540/49669]\tLoss: 90846.9453\n",
      "Training Epoch: 1 [22560/49669]\tLoss: 83019.8984\n",
      "Training Epoch: 1 [22580/49669]\tLoss: 73377.3438\n",
      "Training Epoch: 1 [22600/49669]\tLoss: 73262.1250\n",
      "Training Epoch: 1 [22620/49669]\tLoss: 91650.5469\n",
      "Training Epoch: 1 [22640/49669]\tLoss: 88278.6094\n",
      "Training Epoch: 1 [22660/49669]\tLoss: 83076.7266\n",
      "Training Epoch: 1 [22680/49669]\tLoss: 95987.0469\n",
      "Training Epoch: 1 [22700/49669]\tLoss: 97090.7109\n",
      "Training Epoch: 1 [22720/49669]\tLoss: 87732.8047\n",
      "Training Epoch: 1 [22740/49669]\tLoss: 83792.4141\n",
      "Training Epoch: 1 [22760/49669]\tLoss: 85911.5781\n",
      "Training Epoch: 1 [22780/49669]\tLoss: 83710.0859\n",
      "Training Epoch: 1 [22800/49669]\tLoss: 94021.6719\n",
      "Training Epoch: 1 [22820/49669]\tLoss: 76243.0234\n",
      "Training Epoch: 1 [22840/49669]\tLoss: 73446.8594\n",
      "Training Epoch: 1 [22860/49669]\tLoss: 86652.2266\n",
      "Training Epoch: 1 [22880/49669]\tLoss: 77096.5859\n",
      "Training Epoch: 1 [22900/49669]\tLoss: 81750.4375\n",
      "Training Epoch: 1 [22920/49669]\tLoss: 91571.8672\n",
      "Training Epoch: 1 [22940/49669]\tLoss: 86496.8516\n",
      "Training Epoch: 1 [22960/49669]\tLoss: 71083.9219\n",
      "Training Epoch: 1 [22980/49669]\tLoss: 103372.6250\n",
      "Training Epoch: 1 [23000/49669]\tLoss: 90122.1719\n",
      "Training Epoch: 1 [23020/49669]\tLoss: 92040.1562\n",
      "Training Epoch: 1 [23040/49669]\tLoss: 86092.9766\n",
      "Training Epoch: 1 [23060/49669]\tLoss: 72099.6328\n",
      "Training Epoch: 1 [23080/49669]\tLoss: 90898.7344\n",
      "Training Epoch: 1 [23100/49669]\tLoss: 80300.5938\n",
      "Training Epoch: 1 [23120/49669]\tLoss: 92949.4375\n",
      "Training Epoch: 1 [23140/49669]\tLoss: 96913.7891\n",
      "Training Epoch: 1 [23160/49669]\tLoss: 73863.6953\n",
      "Training Epoch: 1 [23180/49669]\tLoss: 95654.3984\n",
      "Training Epoch: 1 [23200/49669]\tLoss: 81818.5391\n",
      "Training Epoch: 1 [23220/49669]\tLoss: 90084.8672\n",
      "Training Epoch: 1 [23240/49669]\tLoss: 85995.9922\n",
      "Training Epoch: 1 [23260/49669]\tLoss: 66471.8203\n",
      "Training Epoch: 1 [23280/49669]\tLoss: 84874.7812\n",
      "Training Epoch: 1 [23300/49669]\tLoss: 89189.2188\n",
      "Training Epoch: 1 [23320/49669]\tLoss: 88738.7578\n",
      "Training Epoch: 1 [23340/49669]\tLoss: 100056.2891\n",
      "Training Epoch: 1 [23360/49669]\tLoss: 89445.0938\n",
      "Training Epoch: 1 [23380/49669]\tLoss: 80157.4688\n",
      "Training Epoch: 1 [23400/49669]\tLoss: 82606.5938\n",
      "Training Epoch: 1 [23420/49669]\tLoss: 81237.1250\n",
      "Training Epoch: 1 [23440/49669]\tLoss: 101541.7266\n",
      "Training Epoch: 1 [23460/49669]\tLoss: 82098.2812\n",
      "Training Epoch: 1 [23480/49669]\tLoss: 97313.0859\n",
      "Training Epoch: 1 [23500/49669]\tLoss: 83739.8594\n",
      "Training Epoch: 1 [23520/49669]\tLoss: 73780.9844\n",
      "Training Epoch: 1 [23540/49669]\tLoss: 72940.9688\n",
      "Training Epoch: 1 [23560/49669]\tLoss: 88923.4922\n",
      "Training Epoch: 1 [23580/49669]\tLoss: 75780.4531\n",
      "Training Epoch: 1 [23600/49669]\tLoss: 84791.4688\n",
      "Training Epoch: 1 [23620/49669]\tLoss: 84279.4453\n",
      "Training Epoch: 1 [23640/49669]\tLoss: 94044.8828\n",
      "Training Epoch: 1 [23660/49669]\tLoss: 88700.3203\n",
      "Training Epoch: 1 [23680/49669]\tLoss: 90984.7812\n",
      "Training Epoch: 1 [23700/49669]\tLoss: 79778.1094\n",
      "Training Epoch: 1 [23720/49669]\tLoss: 67266.7656\n",
      "Training Epoch: 1 [23740/49669]\tLoss: 106541.2578\n",
      "Training Epoch: 1 [23760/49669]\tLoss: 80795.0859\n",
      "Training Epoch: 1 [23780/49669]\tLoss: 85714.2031\n",
      "Training Epoch: 1 [23800/49669]\tLoss: 72470.5859\n",
      "Training Epoch: 1 [23820/49669]\tLoss: 76082.9062\n",
      "Training Epoch: 1 [23840/49669]\tLoss: 73496.9375\n",
      "Training Epoch: 1 [23860/49669]\tLoss: 90406.7656\n",
      "Training Epoch: 1 [23880/49669]\tLoss: 87772.5469\n",
      "Training Epoch: 1 [23900/49669]\tLoss: 79658.8125\n",
      "Training Epoch: 1 [23920/49669]\tLoss: 78153.3828\n",
      "Training Epoch: 1 [23940/49669]\tLoss: 92243.6250\n",
      "Training Epoch: 1 [23960/49669]\tLoss: 84788.0938\n",
      "Training Epoch: 1 [23980/49669]\tLoss: 87431.1094\n",
      "Training Epoch: 1 [24000/49669]\tLoss: 95139.0234\n",
      "Training Epoch: 1 [24020/49669]\tLoss: 80136.7812\n",
      "Training Epoch: 1 [24040/49669]\tLoss: 65563.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [24060/49669]\tLoss: 85074.7188\n",
      "Training Epoch: 1 [24080/49669]\tLoss: 85654.8047\n",
      "Training Epoch: 1 [24100/49669]\tLoss: 81834.6875\n",
      "Training Epoch: 1 [24120/49669]\tLoss: 77144.0391\n",
      "Training Epoch: 1 [24140/49669]\tLoss: 91857.5469\n",
      "Training Epoch: 1 [24160/49669]\tLoss: 71831.4453\n",
      "Training Epoch: 1 [24180/49669]\tLoss: 93902.4375\n",
      "Training Epoch: 1 [24200/49669]\tLoss: 68542.0625\n",
      "Training Epoch: 1 [24220/49669]\tLoss: 80583.6875\n",
      "Training Epoch: 1 [24240/49669]\tLoss: 102719.1172\n",
      "Training Epoch: 1 [24260/49669]\tLoss: 105607.0547\n",
      "Training Epoch: 1 [24280/49669]\tLoss: 94711.6875\n",
      "Training Epoch: 1 [24300/49669]\tLoss: 91599.3203\n",
      "Training Epoch: 1 [24320/49669]\tLoss: 78212.8906\n",
      "Training Epoch: 1 [24340/49669]\tLoss: 71707.4297\n",
      "Training Epoch: 1 [24360/49669]\tLoss: 79789.7109\n",
      "Training Epoch: 1 [24380/49669]\tLoss: 72088.2500\n",
      "Training Epoch: 1 [24400/49669]\tLoss: 79530.7188\n",
      "Training Epoch: 1 [24420/49669]\tLoss: 89332.9609\n",
      "Training Epoch: 1 [24440/49669]\tLoss: 72771.5000\n",
      "Training Epoch: 1 [24460/49669]\tLoss: 85088.2969\n",
      "Training Epoch: 1 [24480/49669]\tLoss: 84791.8672\n",
      "Training Epoch: 1 [24500/49669]\tLoss: 93080.0703\n",
      "Training Epoch: 1 [24520/49669]\tLoss: 82325.3438\n",
      "Training Epoch: 1 [24540/49669]\tLoss: 88961.6719\n",
      "Training Epoch: 1 [24560/49669]\tLoss: 69538.7109\n",
      "Training Epoch: 1 [24580/49669]\tLoss: 83643.8828\n",
      "Training Epoch: 1 [24600/49669]\tLoss: 91129.2969\n",
      "Training Epoch: 1 [24620/49669]\tLoss: 74438.3906\n",
      "Training Epoch: 1 [24640/49669]\tLoss: 71602.9922\n",
      "Training Epoch: 1 [24660/49669]\tLoss: 98320.5391\n",
      "Training Epoch: 1 [24680/49669]\tLoss: 79038.6250\n",
      "Training Epoch: 1 [24700/49669]\tLoss: 81116.1562\n",
      "Training Epoch: 1 [24720/49669]\tLoss: 78078.4219\n",
      "Training Epoch: 1 [24740/49669]\tLoss: 65260.4492\n",
      "Training Epoch: 1 [24760/49669]\tLoss: 80254.7422\n",
      "Training Epoch: 1 [24780/49669]\tLoss: 88493.3828\n",
      "Training Epoch: 1 [24800/49669]\tLoss: 74110.7578\n",
      "Training Epoch: 1 [24820/49669]\tLoss: 86894.8906\n",
      "Training Epoch: 1 [24840/49669]\tLoss: 75566.0469\n",
      "Training Epoch: 1 [24860/49669]\tLoss: 69343.0547\n",
      "Training Epoch: 1 [24880/49669]\tLoss: 72867.3672\n",
      "Training Epoch: 1 [24900/49669]\tLoss: 81110.1875\n",
      "Training Epoch: 1 [24920/49669]\tLoss: 89553.2344\n",
      "Training Epoch: 1 [24940/49669]\tLoss: 81957.0312\n",
      "Training Epoch: 1 [24960/49669]\tLoss: 84377.5234\n",
      "Training Epoch: 1 [24980/49669]\tLoss: 84895.2188\n",
      "Training Epoch: 1 [25000/49669]\tLoss: 91908.8594\n",
      "Training Epoch: 1 [25020/49669]\tLoss: 104848.0312\n",
      "Training Epoch: 1 [25040/49669]\tLoss: 61869.8711\n",
      "Training Epoch: 1 [25060/49669]\tLoss: 72449.2500\n",
      "Training Epoch: 1 [25080/49669]\tLoss: 81791.0859\n",
      "Training Epoch: 1 [25100/49669]\tLoss: 81091.7109\n",
      "Training Epoch: 1 [25120/49669]\tLoss: 100551.1016\n",
      "Training Epoch: 1 [25140/49669]\tLoss: 87355.5703\n",
      "Training Epoch: 1 [25160/49669]\tLoss: 77829.2734\n",
      "Training Epoch: 1 [25180/49669]\tLoss: 83406.8828\n",
      "Training Epoch: 1 [25200/49669]\tLoss: 74168.2734\n",
      "Training Epoch: 1 [25220/49669]\tLoss: 84595.9375\n",
      "Training Epoch: 1 [25240/49669]\tLoss: 71461.6484\n",
      "Training Epoch: 1 [25260/49669]\tLoss: 83808.7578\n",
      "Training Epoch: 1 [25280/49669]\tLoss: 107315.3438\n",
      "Training Epoch: 1 [25300/49669]\tLoss: 87623.4609\n",
      "Training Epoch: 1 [25320/49669]\tLoss: 85052.4766\n",
      "Training Epoch: 1 [25340/49669]\tLoss: 61823.7773\n",
      "Training Epoch: 1 [25360/49669]\tLoss: 81422.5625\n",
      "Training Epoch: 1 [25380/49669]\tLoss: 72912.2266\n",
      "Training Epoch: 1 [25400/49669]\tLoss: 85101.7266\n",
      "Training Epoch: 1 [25420/49669]\tLoss: 76374.7969\n",
      "Training Epoch: 1 [25440/49669]\tLoss: 75127.3672\n",
      "Training Epoch: 1 [25460/49669]\tLoss: 78326.5547\n",
      "Training Epoch: 1 [25480/49669]\tLoss: 73361.2422\n",
      "Training Epoch: 1 [25500/49669]\tLoss: 95130.6719\n",
      "Training Epoch: 1 [25520/49669]\tLoss: 80581.5391\n",
      "Training Epoch: 1 [25540/49669]\tLoss: 71527.4141\n",
      "Training Epoch: 1 [25560/49669]\tLoss: 69889.3203\n",
      "Training Epoch: 1 [25580/49669]\tLoss: 81481.3047\n",
      "Training Epoch: 1 [25600/49669]\tLoss: 79420.4688\n",
      "Training Epoch: 1 [25620/49669]\tLoss: 72723.5312\n",
      "Training Epoch: 1 [25640/49669]\tLoss: 80220.1250\n",
      "Training Epoch: 1 [25660/49669]\tLoss: 103739.5391\n",
      "Training Epoch: 1 [25680/49669]\tLoss: 80538.4922\n",
      "Training Epoch: 1 [25700/49669]\tLoss: 73773.8203\n",
      "Training Epoch: 1 [25720/49669]\tLoss: 53605.6367\n",
      "Training Epoch: 1 [25740/49669]\tLoss: 78987.9062\n",
      "Training Epoch: 1 [25760/49669]\tLoss: 75638.3516\n",
      "Training Epoch: 1 [25780/49669]\tLoss: 79216.6875\n",
      "Training Epoch: 1 [25800/49669]\tLoss: 102319.8594\n",
      "Training Epoch: 1 [25820/49669]\tLoss: 79771.0938\n",
      "Training Epoch: 1 [25840/49669]\tLoss: 73081.6562\n",
      "Training Epoch: 1 [25860/49669]\tLoss: 63803.1992\n",
      "Training Epoch: 1 [25880/49669]\tLoss: 59230.4453\n",
      "Training Epoch: 1 [25900/49669]\tLoss: 57797.8359\n",
      "Training Epoch: 1 [25920/49669]\tLoss: 73449.3516\n",
      "Training Epoch: 1 [25940/49669]\tLoss: 75922.5391\n",
      "Training Epoch: 1 [25960/49669]\tLoss: 58929.9180\n",
      "Training Epoch: 1 [25980/49669]\tLoss: 66590.7266\n",
      "Training Epoch: 1 [26000/49669]\tLoss: 62502.1367\n",
      "Training Epoch: 1 [26020/49669]\tLoss: 73453.3750\n",
      "Training Epoch: 1 [26040/49669]\tLoss: 73424.1250\n",
      "Training Epoch: 1 [26060/49669]\tLoss: 75095.1797\n",
      "Training Epoch: 1 [26080/49669]\tLoss: 77415.1328\n",
      "Training Epoch: 1 [26100/49669]\tLoss: 60892.9648\n",
      "Training Epoch: 1 [26120/49669]\tLoss: 74221.2500\n",
      "Training Epoch: 1 [26140/49669]\tLoss: 54544.4062\n",
      "Training Epoch: 1 [26160/49669]\tLoss: 80227.0312\n",
      "Training Epoch: 1 [26180/49669]\tLoss: 70231.7734\n",
      "Training Epoch: 1 [26200/49669]\tLoss: 83201.4766\n",
      "Training Epoch: 1 [26220/49669]\tLoss: 59731.1562\n",
      "Training Epoch: 1 [26240/49669]\tLoss: 88348.0547\n",
      "Training Epoch: 1 [26260/49669]\tLoss: 69539.7734\n",
      "Training Epoch: 1 [26280/49669]\tLoss: 69690.2891\n",
      "Training Epoch: 1 [26300/49669]\tLoss: 83130.1797\n",
      "Training Epoch: 1 [26320/49669]\tLoss: 80070.3359\n",
      "Training Epoch: 1 [26340/49669]\tLoss: 66994.2891\n",
      "Training Epoch: 1 [26360/49669]\tLoss: 70035.7344\n",
      "Training Epoch: 1 [26380/49669]\tLoss: 90769.3516\n",
      "Training Epoch: 1 [26400/49669]\tLoss: 67463.6562\n",
      "Training Epoch: 1 [26420/49669]\tLoss: 70880.3828\n",
      "Training Epoch: 1 [26440/49669]\tLoss: 77877.3125\n",
      "Training Epoch: 1 [26460/49669]\tLoss: 73594.1250\n",
      "Training Epoch: 1 [26480/49669]\tLoss: 60951.9062\n",
      "Training Epoch: 1 [26500/49669]\tLoss: 75753.9844\n",
      "Training Epoch: 1 [26520/49669]\tLoss: 76644.1484\n",
      "Training Epoch: 1 [26540/49669]\tLoss: 76962.5156\n",
      "Training Epoch: 1 [26560/49669]\tLoss: 75022.9062\n",
      "Training Epoch: 1 [26580/49669]\tLoss: 72417.2578\n",
      "Training Epoch: 1 [26600/49669]\tLoss: 66584.0703\n",
      "Training Epoch: 1 [26620/49669]\tLoss: 66926.7188\n",
      "Training Epoch: 1 [26640/49669]\tLoss: 62989.2852\n",
      "Training Epoch: 1 [26660/49669]\tLoss: 67817.8047\n",
      "Training Epoch: 1 [26680/49669]\tLoss: 73987.4688\n",
      "Training Epoch: 1 [26700/49669]\tLoss: 66846.7031\n",
      "Training Epoch: 1 [26720/49669]\tLoss: 78417.7812\n",
      "Training Epoch: 1 [26740/49669]\tLoss: 78220.1172\n",
      "Training Epoch: 1 [26760/49669]\tLoss: 79612.2891\n",
      "Training Epoch: 1 [26780/49669]\tLoss: 57785.9492\n",
      "Training Epoch: 1 [26800/49669]\tLoss: 62953.1055\n",
      "Training Epoch: 1 [26820/49669]\tLoss: 79913.8438\n",
      "Training Epoch: 1 [26840/49669]\tLoss: 81831.0938\n",
      "Training Epoch: 1 [26860/49669]\tLoss: 70443.9531\n",
      "Training Epoch: 1 [26880/49669]\tLoss: 80286.5625\n",
      "Training Epoch: 1 [26900/49669]\tLoss: 77050.6406\n",
      "Training Epoch: 1 [26920/49669]\tLoss: 77040.3906\n",
      "Training Epoch: 1 [26940/49669]\tLoss: 79318.7344\n",
      "Training Epoch: 1 [26960/49669]\tLoss: 66535.7969\n",
      "Training Epoch: 1 [26980/49669]\tLoss: 75463.6328\n",
      "Training Epoch: 1 [27000/49669]\tLoss: 65976.1719\n",
      "Training Epoch: 1 [27020/49669]\tLoss: 75705.6797\n",
      "Training Epoch: 1 [27040/49669]\tLoss: 78350.9766\n",
      "Training Epoch: 1 [27060/49669]\tLoss: 81906.2344\n",
      "Training Epoch: 1 [27080/49669]\tLoss: 66964.8125\n",
      "Training Epoch: 1 [27100/49669]\tLoss: 64910.1328\n",
      "Training Epoch: 1 [27120/49669]\tLoss: 80131.2578\n",
      "Training Epoch: 1 [27140/49669]\tLoss: 68508.4922\n",
      "Training Epoch: 1 [27160/49669]\tLoss: 83253.4297\n",
      "Training Epoch: 1 [27180/49669]\tLoss: 64464.9453\n",
      "Training Epoch: 1 [27200/49669]\tLoss: 78210.5312\n",
      "Training Epoch: 1 [27220/49669]\tLoss: 84264.2969\n",
      "Training Epoch: 1 [27240/49669]\tLoss: 68749.9219\n",
      "Training Epoch: 1 [27260/49669]\tLoss: 73509.0312\n",
      "Training Epoch: 1 [27280/49669]\tLoss: 73309.7266\n",
      "Training Epoch: 1 [27300/49669]\tLoss: 83812.6250\n",
      "Training Epoch: 1 [27320/49669]\tLoss: 80515.7891\n",
      "Training Epoch: 1 [27340/49669]\tLoss: 68157.5156\n",
      "Training Epoch: 1 [27360/49669]\tLoss: 74751.1484\n",
      "Training Epoch: 1 [27380/49669]\tLoss: 66889.1328\n",
      "Training Epoch: 1 [27400/49669]\tLoss: 58400.1484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [27420/49669]\tLoss: 79911.3594\n",
      "Training Epoch: 1 [27440/49669]\tLoss: 64587.9297\n",
      "Training Epoch: 1 [27460/49669]\tLoss: 73492.2500\n",
      "Training Epoch: 1 [27480/49669]\tLoss: 76090.1562\n",
      "Training Epoch: 1 [27500/49669]\tLoss: 75304.7422\n",
      "Training Epoch: 1 [27520/49669]\tLoss: 84013.7734\n",
      "Training Epoch: 1 [27540/49669]\tLoss: 61828.3750\n",
      "Training Epoch: 1 [27560/49669]\tLoss: 77697.7812\n",
      "Training Epoch: 1 [27580/49669]\tLoss: 95396.7109\n",
      "Training Epoch: 1 [27600/49669]\tLoss: 73642.2422\n",
      "Training Epoch: 1 [27620/49669]\tLoss: 75268.7812\n",
      "Training Epoch: 1 [27640/49669]\tLoss: 72321.4531\n",
      "Training Epoch: 1 [27660/49669]\tLoss: 82729.9688\n",
      "Training Epoch: 1 [27680/49669]\tLoss: 54698.1602\n",
      "Training Epoch: 1 [27700/49669]\tLoss: 65126.6133\n",
      "Training Epoch: 1 [27720/49669]\tLoss: 64876.4297\n",
      "Training Epoch: 1 [27740/49669]\tLoss: 71580.1406\n",
      "Training Epoch: 1 [27760/49669]\tLoss: 80186.0625\n",
      "Training Epoch: 1 [27780/49669]\tLoss: 66983.2266\n",
      "Training Epoch: 1 [27800/49669]\tLoss: 78105.7109\n",
      "Training Epoch: 1 [27820/49669]\tLoss: 82280.3828\n",
      "Training Epoch: 1 [27840/49669]\tLoss: 70167.0938\n",
      "Training Epoch: 1 [27860/49669]\tLoss: 80443.8516\n",
      "Training Epoch: 1 [27880/49669]\tLoss: 68360.1953\n",
      "Training Epoch: 1 [27900/49669]\tLoss: 72970.3516\n",
      "Training Epoch: 1 [27920/49669]\tLoss: 87691.3750\n",
      "Training Epoch: 1 [27940/49669]\tLoss: 63066.2266\n",
      "Training Epoch: 1 [27960/49669]\tLoss: 80852.2812\n",
      "Training Epoch: 1 [27980/49669]\tLoss: 69883.1484\n",
      "Training Epoch: 1 [28000/49669]\tLoss: 78040.3359\n",
      "Training Epoch: 1 [28020/49669]\tLoss: 63139.2148\n",
      "Training Epoch: 1 [28040/49669]\tLoss: 66019.2734\n",
      "Training Epoch: 1 [28060/49669]\tLoss: 66087.6484\n",
      "Training Epoch: 1 [28080/49669]\tLoss: 58803.8359\n",
      "Training Epoch: 1 [28100/49669]\tLoss: 51319.0156\n",
      "Training Epoch: 1 [28120/49669]\tLoss: 59772.1133\n",
      "Training Epoch: 1 [28140/49669]\tLoss: 51887.6328\n",
      "Training Epoch: 1 [28160/49669]\tLoss: 68783.2656\n",
      "Training Epoch: 1 [28180/49669]\tLoss: 59541.1484\n",
      "Training Epoch: 1 [28200/49669]\tLoss: 56317.4492\n",
      "Training Epoch: 1 [28220/49669]\tLoss: 53527.9570\n",
      "Training Epoch: 1 [28240/49669]\tLoss: 62448.7969\n",
      "Training Epoch: 1 [28260/49669]\tLoss: 57028.8672\n",
      "Training Epoch: 1 [28280/49669]\tLoss: 71091.0859\n",
      "Training Epoch: 1 [28300/49669]\tLoss: 82537.8516\n",
      "Training Epoch: 1 [28320/49669]\tLoss: 59413.7461\n",
      "Training Epoch: 1 [28340/49669]\tLoss: 66522.4609\n",
      "Training Epoch: 1 [28360/49669]\tLoss: 67703.5547\n",
      "Training Epoch: 1 [28380/49669]\tLoss: 62608.5664\n",
      "Training Epoch: 1 [28400/49669]\tLoss: 68604.9844\n",
      "Training Epoch: 1 [28420/49669]\tLoss: 60572.9102\n",
      "Training Epoch: 1 [28440/49669]\tLoss: 63181.4453\n",
      "Training Epoch: 1 [28460/49669]\tLoss: 72176.2500\n",
      "Training Epoch: 1 [28480/49669]\tLoss: 64550.0195\n",
      "Training Epoch: 1 [28500/49669]\tLoss: 63890.0859\n",
      "Training Epoch: 1 [28520/49669]\tLoss: 57847.8633\n",
      "Training Epoch: 1 [28540/49669]\tLoss: 53162.9180\n",
      "Training Epoch: 1 [28560/49669]\tLoss: 63914.5625\n",
      "Training Epoch: 1 [28580/49669]\tLoss: 70161.4297\n",
      "Training Epoch: 1 [28600/49669]\tLoss: 80977.1328\n",
      "Training Epoch: 1 [28620/49669]\tLoss: 64764.2422\n",
      "Training Epoch: 1 [28640/49669]\tLoss: 69625.0312\n",
      "Training Epoch: 1 [28660/49669]\tLoss: 65766.6406\n",
      "Training Epoch: 1 [28680/49669]\tLoss: 68510.3594\n",
      "Training Epoch: 1 [28700/49669]\tLoss: 61542.3008\n",
      "Training Epoch: 1 [28720/49669]\tLoss: 61419.1016\n",
      "Training Epoch: 1 [28740/49669]\tLoss: 71534.2812\n",
      "Training Epoch: 1 [28760/49669]\tLoss: 70020.3516\n",
      "Training Epoch: 1 [28780/49669]\tLoss: 66196.9141\n",
      "Training Epoch: 1 [28800/49669]\tLoss: 62422.0781\n",
      "Training Epoch: 1 [28820/49669]\tLoss: 62783.1602\n",
      "Training Epoch: 1 [28840/49669]\tLoss: 70576.3984\n",
      "Training Epoch: 1 [28860/49669]\tLoss: 61811.0391\n",
      "Training Epoch: 1 [28880/49669]\tLoss: 68140.1562\n",
      "Training Epoch: 1 [28900/49669]\tLoss: 58333.3438\n",
      "Training Epoch: 1 [28920/49669]\tLoss: 65612.8438\n",
      "Training Epoch: 1 [28940/49669]\tLoss: 64307.9609\n",
      "Training Epoch: 1 [28960/49669]\tLoss: 65972.1953\n",
      "Training Epoch: 1 [28980/49669]\tLoss: 75686.7344\n",
      "Training Epoch: 1 [29000/49669]\tLoss: 64376.8789\n",
      "Training Epoch: 1 [29020/49669]\tLoss: 60044.0430\n",
      "Training Epoch: 1 [29040/49669]\tLoss: 71066.6328\n",
      "Training Epoch: 1 [29060/49669]\tLoss: 58966.5469\n",
      "Training Epoch: 1 [29080/49669]\tLoss: 60917.6094\n",
      "Training Epoch: 1 [29100/49669]\tLoss: 58935.0117\n",
      "Training Epoch: 1 [29120/49669]\tLoss: 75371.4062\n",
      "Training Epoch: 1 [29140/49669]\tLoss: 65777.4766\n",
      "Training Epoch: 1 [29160/49669]\tLoss: 43247.5820\n",
      "Training Epoch: 1 [29180/49669]\tLoss: 68600.5234\n",
      "Training Epoch: 1 [29200/49669]\tLoss: 58091.0625\n",
      "Training Epoch: 1 [29220/49669]\tLoss: 58179.5469\n",
      "Training Epoch: 1 [29240/49669]\tLoss: 59582.3125\n",
      "Training Epoch: 1 [29260/49669]\tLoss: 61757.0781\n",
      "Training Epoch: 1 [29280/49669]\tLoss: 60676.8242\n",
      "Training Epoch: 1 [29300/49669]\tLoss: 58232.9375\n",
      "Training Epoch: 1 [29320/49669]\tLoss: 80331.1719\n",
      "Training Epoch: 1 [29340/49669]\tLoss: 85929.2344\n",
      "Training Epoch: 1 [29360/49669]\tLoss: 55922.4180\n",
      "Training Epoch: 1 [29380/49669]\tLoss: 71897.3125\n",
      "Training Epoch: 1 [29400/49669]\tLoss: 81881.8750\n",
      "Training Epoch: 1 [29420/49669]\tLoss: 78965.3125\n",
      "Training Epoch: 1 [29440/49669]\tLoss: 67073.0547\n",
      "Training Epoch: 1 [29460/49669]\tLoss: 68076.2969\n",
      "Training Epoch: 1 [29480/49669]\tLoss: 65341.2266\n",
      "Training Epoch: 1 [29500/49669]\tLoss: 70695.6562\n",
      "Training Epoch: 1 [29520/49669]\tLoss: 58525.9180\n",
      "Training Epoch: 1 [29540/49669]\tLoss: 69331.2188\n",
      "Training Epoch: 1 [29560/49669]\tLoss: 68801.9531\n",
      "Training Epoch: 1 [29580/49669]\tLoss: 66963.2266\n",
      "Training Epoch: 1 [29600/49669]\tLoss: 56601.7227\n",
      "Training Epoch: 1 [29620/49669]\tLoss: 65364.9766\n",
      "Training Epoch: 1 [29640/49669]\tLoss: 75091.8047\n",
      "Training Epoch: 1 [29660/49669]\tLoss: 62322.1719\n",
      "Training Epoch: 1 [29680/49669]\tLoss: 83106.5000\n",
      "Training Epoch: 1 [29700/49669]\tLoss: 79769.5625\n",
      "Training Epoch: 1 [29720/49669]\tLoss: 49032.5352\n",
      "Training Epoch: 1 [29740/49669]\tLoss: 53744.2461\n",
      "Training Epoch: 1 [29760/49669]\tLoss: 65708.0547\n",
      "Training Epoch: 1 [29780/49669]\tLoss: 58683.7969\n",
      "Training Epoch: 1 [29800/49669]\tLoss: 63582.8398\n",
      "Training Epoch: 1 [29820/49669]\tLoss: 67103.0703\n",
      "Training Epoch: 1 [29840/49669]\tLoss: 65481.6094\n",
      "Training Epoch: 1 [29860/49669]\tLoss: 60714.1992\n",
      "Training Epoch: 1 [29880/49669]\tLoss: 52138.0156\n",
      "Training Epoch: 1 [29900/49669]\tLoss: 52135.4648\n",
      "Training Epoch: 1 [29920/49669]\tLoss: 88103.1953\n",
      "Training Epoch: 1 [29940/49669]\tLoss: 55686.7656\n",
      "Training Epoch: 1 [29960/49669]\tLoss: 56676.9688\n",
      "Training Epoch: 1 [29980/49669]\tLoss: 70144.3750\n",
      "Training Epoch: 1 [30000/49669]\tLoss: 55309.8555\n",
      "Training Epoch: 1 [30020/49669]\tLoss: 58209.6406\n",
      "Training Epoch: 1 [30040/49669]\tLoss: 66893.0078\n",
      "Training Epoch: 1 [30060/49669]\tLoss: 61677.1055\n",
      "Training Epoch: 1 [30080/49669]\tLoss: 66305.1406\n",
      "Training Epoch: 1 [30100/49669]\tLoss: 78139.0078\n",
      "Training Epoch: 1 [30120/49669]\tLoss: 68042.2969\n",
      "Training Epoch: 1 [30140/49669]\tLoss: 54605.3047\n",
      "Training Epoch: 1 [30160/49669]\tLoss: 69831.8984\n",
      "Training Epoch: 1 [30180/49669]\tLoss: 63947.7344\n",
      "Training Epoch: 1 [30200/49669]\tLoss: 58944.7812\n",
      "Training Epoch: 1 [30220/49669]\tLoss: 49149.7891\n",
      "Training Epoch: 1 [30240/49669]\tLoss: 53934.1719\n",
      "Training Epoch: 1 [30260/49669]\tLoss: 53565.5859\n",
      "Training Epoch: 1 [30280/49669]\tLoss: 53907.4375\n",
      "Training Epoch: 1 [30300/49669]\tLoss: 61706.4570\n",
      "Training Epoch: 1 [30320/49669]\tLoss: 52826.9883\n",
      "Training Epoch: 1 [30340/49669]\tLoss: 52714.2617\n",
      "Training Epoch: 1 [30360/49669]\tLoss: 53468.5000\n",
      "Training Epoch: 1 [30380/49669]\tLoss: 63857.8477\n",
      "Training Epoch: 1 [30400/49669]\tLoss: 53912.5586\n",
      "Training Epoch: 1 [30420/49669]\tLoss: 63530.3945\n",
      "Training Epoch: 1 [30440/49669]\tLoss: 49959.2930\n",
      "Training Epoch: 1 [30460/49669]\tLoss: 56957.5430\n",
      "Training Epoch: 1 [30480/49669]\tLoss: 63312.8516\n",
      "Training Epoch: 1 [30500/49669]\tLoss: 55493.6406\n",
      "Training Epoch: 1 [30520/49669]\tLoss: 56194.8906\n",
      "Training Epoch: 1 [30540/49669]\tLoss: 59155.1172\n",
      "Training Epoch: 1 [30560/49669]\tLoss: 71714.9766\n",
      "Training Epoch: 1 [30580/49669]\tLoss: 51708.3164\n",
      "Training Epoch: 1 [30600/49669]\tLoss: 67853.4297\n",
      "Training Epoch: 1 [30620/49669]\tLoss: 52322.3555\n",
      "Training Epoch: 1 [30640/49669]\tLoss: 67347.2500\n",
      "Training Epoch: 1 [30660/49669]\tLoss: 46040.1094\n",
      "Training Epoch: 1 [30680/49669]\tLoss: 53017.1641\n",
      "Training Epoch: 1 [30700/49669]\tLoss: 60133.6797\n",
      "Training Epoch: 1 [30720/49669]\tLoss: 61770.2070\n",
      "Training Epoch: 1 [30740/49669]\tLoss: 53097.2500\n",
      "Training Epoch: 1 [30760/49669]\tLoss: 56852.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [30780/49669]\tLoss: 64974.4609\n",
      "Training Epoch: 1 [30800/49669]\tLoss: 66161.5078\n",
      "Training Epoch: 1 [30820/49669]\tLoss: 58244.7109\n",
      "Training Epoch: 1 [30840/49669]\tLoss: 49767.3438\n",
      "Training Epoch: 1 [30860/49669]\tLoss: 64968.9219\n",
      "Training Epoch: 1 [30880/49669]\tLoss: 58373.1094\n",
      "Training Epoch: 1 [30900/49669]\tLoss: 69978.1172\n",
      "Training Epoch: 1 [30920/49669]\tLoss: 56529.6719\n",
      "Training Epoch: 1 [30940/49669]\tLoss: 58770.6797\n",
      "Training Epoch: 1 [30960/49669]\tLoss: 54265.2422\n",
      "Training Epoch: 1 [30980/49669]\tLoss: 68734.9922\n",
      "Training Epoch: 1 [31000/49669]\tLoss: 49437.0195\n",
      "Training Epoch: 1 [31020/49669]\tLoss: 59525.8047\n",
      "Training Epoch: 1 [31040/49669]\tLoss: 57828.0938\n",
      "Training Epoch: 1 [31060/49669]\tLoss: 54448.2070\n",
      "Training Epoch: 1 [31080/49669]\tLoss: 46997.2500\n",
      "Training Epoch: 1 [31100/49669]\tLoss: 55377.4375\n",
      "Training Epoch: 1 [31120/49669]\tLoss: 61723.4688\n",
      "Training Epoch: 1 [31140/49669]\tLoss: 57587.6133\n",
      "Training Epoch: 1 [31160/49669]\tLoss: 45098.0859\n",
      "Training Epoch: 1 [31180/49669]\tLoss: 54763.3281\n",
      "Training Epoch: 1 [31200/49669]\tLoss: 50996.1992\n",
      "Training Epoch: 1 [31220/49669]\tLoss: 64187.8555\n",
      "Training Epoch: 1 [31240/49669]\tLoss: 68805.0391\n",
      "Training Epoch: 1 [31260/49669]\tLoss: 66862.3047\n",
      "Training Epoch: 1 [31280/49669]\tLoss: 54640.1055\n",
      "Training Epoch: 1 [31300/49669]\tLoss: 55596.2852\n",
      "Training Epoch: 1 [31320/49669]\tLoss: 62884.9805\n",
      "Training Epoch: 1 [31340/49669]\tLoss: 45561.1758\n",
      "Training Epoch: 1 [31360/49669]\tLoss: 54634.8203\n",
      "Training Epoch: 1 [31380/49669]\tLoss: 59590.5625\n",
      "Training Epoch: 1 [31400/49669]\tLoss: 67827.6406\n",
      "Training Epoch: 1 [31420/49669]\tLoss: 55722.2617\n",
      "Training Epoch: 1 [31440/49669]\tLoss: 55615.6016\n",
      "Training Epoch: 1 [31460/49669]\tLoss: 51467.3867\n",
      "Training Epoch: 1 [31480/49669]\tLoss: 52742.6055\n",
      "Training Epoch: 1 [31500/49669]\tLoss: 57272.1797\n",
      "Training Epoch: 1 [31520/49669]\tLoss: 55551.2656\n",
      "Training Epoch: 1 [31540/49669]\tLoss: 73604.4531\n",
      "Training Epoch: 1 [31560/49669]\tLoss: 53694.1016\n",
      "Training Epoch: 1 [31580/49669]\tLoss: 55224.7266\n",
      "Training Epoch: 1 [31600/49669]\tLoss: 56586.3789\n",
      "Training Epoch: 1 [31620/49669]\tLoss: 60899.6523\n",
      "Training Epoch: 1 [31640/49669]\tLoss: 59545.2812\n",
      "Training Epoch: 1 [31660/49669]\tLoss: 67726.1406\n",
      "Training Epoch: 1 [31680/49669]\tLoss: 48092.6172\n",
      "Training Epoch: 1 [31700/49669]\tLoss: 54205.8555\n",
      "Training Epoch: 1 [31720/49669]\tLoss: 57290.3633\n",
      "Training Epoch: 1 [31740/49669]\tLoss: 67525.8359\n",
      "Training Epoch: 1 [31760/49669]\tLoss: 52684.4336\n",
      "Training Epoch: 1 [31780/49669]\tLoss: 53752.8555\n",
      "Training Epoch: 1 [31800/49669]\tLoss: 63229.7422\n",
      "Training Epoch: 1 [31820/49669]\tLoss: 53855.9219\n",
      "Training Epoch: 1 [31840/49669]\tLoss: 54058.3398\n",
      "Training Epoch: 1 [31860/49669]\tLoss: 41343.4453\n",
      "Training Epoch: 1 [31880/49669]\tLoss: 58303.5430\n",
      "Training Epoch: 1 [31900/49669]\tLoss: 48274.3203\n",
      "Training Epoch: 1 [31920/49669]\tLoss: 54589.3125\n",
      "Training Epoch: 1 [31940/49669]\tLoss: 52366.6445\n",
      "Training Epoch: 1 [31960/49669]\tLoss: 52137.7422\n",
      "Training Epoch: 1 [31980/49669]\tLoss: 44217.7031\n",
      "Training Epoch: 1 [32000/49669]\tLoss: 57951.0977\n",
      "Training Epoch: 1 [32020/49669]\tLoss: 50778.6836\n",
      "Training Epoch: 1 [32040/49669]\tLoss: 60357.7031\n",
      "Training Epoch: 1 [32060/49669]\tLoss: 49030.4961\n",
      "Training Epoch: 1 [32080/49669]\tLoss: 59971.5898\n",
      "Training Epoch: 1 [32100/49669]\tLoss: 59166.2266\n",
      "Training Epoch: 1 [32120/49669]\tLoss: 55845.9531\n",
      "Training Epoch: 1 [32140/49669]\tLoss: 64366.0547\n",
      "Training Epoch: 1 [32160/49669]\tLoss: 63865.9766\n",
      "Training Epoch: 1 [32180/49669]\tLoss: 46516.9297\n",
      "Training Epoch: 1 [32200/49669]\tLoss: 42628.3672\n",
      "Training Epoch: 1 [32220/49669]\tLoss: 59847.1211\n",
      "Training Epoch: 1 [32240/49669]\tLoss: 50633.2578\n",
      "Training Epoch: 1 [32260/49669]\tLoss: 63934.8906\n",
      "Training Epoch: 1 [32280/49669]\tLoss: 55163.0859\n",
      "Training Epoch: 1 [32300/49669]\tLoss: 52739.1211\n",
      "Training Epoch: 1 [32320/49669]\tLoss: 41179.7305\n",
      "Training Epoch: 1 [32340/49669]\tLoss: 45212.8047\n",
      "Training Epoch: 1 [32360/49669]\tLoss: 56082.5039\n",
      "Training Epoch: 1 [32380/49669]\tLoss: 55810.5625\n",
      "Training Epoch: 1 [32400/49669]\tLoss: 46361.2539\n",
      "Training Epoch: 1 [32420/49669]\tLoss: 46534.3125\n",
      "Training Epoch: 1 [32440/49669]\tLoss: 53330.3438\n",
      "Training Epoch: 1 [32460/49669]\tLoss: 46391.1367\n",
      "Training Epoch: 1 [32480/49669]\tLoss: 50439.9062\n",
      "Training Epoch: 1 [32500/49669]\tLoss: 45596.4883\n",
      "Training Epoch: 1 [32520/49669]\tLoss: 45779.7422\n",
      "Training Epoch: 1 [32540/49669]\tLoss: 61771.4102\n",
      "Training Epoch: 1 [32560/49669]\tLoss: 46526.5273\n",
      "Training Epoch: 1 [32580/49669]\tLoss: 52872.0898\n",
      "Training Epoch: 1 [32600/49669]\tLoss: 62158.1953\n",
      "Training Epoch: 1 [32620/49669]\tLoss: 50798.1719\n",
      "Training Epoch: 1 [32640/49669]\tLoss: 47354.0000\n",
      "Training Epoch: 1 [32660/49669]\tLoss: 59349.2461\n",
      "Training Epoch: 1 [32680/49669]\tLoss: 53522.0117\n",
      "Training Epoch: 1 [32700/49669]\tLoss: 56554.6875\n",
      "Training Epoch: 1 [32720/49669]\tLoss: 46644.2734\n",
      "Training Epoch: 1 [32740/49669]\tLoss: 45973.1172\n",
      "Training Epoch: 1 [32760/49669]\tLoss: 46763.4336\n",
      "Training Epoch: 1 [32780/49669]\tLoss: 50241.1680\n",
      "Training Epoch: 1 [32800/49669]\tLoss: 50893.6680\n",
      "Training Epoch: 1 [32820/49669]\tLoss: 45926.9023\n",
      "Training Epoch: 1 [32840/49669]\tLoss: 41165.9453\n",
      "Training Epoch: 1 [32860/49669]\tLoss: 62858.2461\n",
      "Training Epoch: 1 [32880/49669]\tLoss: 57487.1289\n",
      "Training Epoch: 1 [32900/49669]\tLoss: 46909.3750\n",
      "Training Epoch: 1 [32920/49669]\tLoss: 52012.0156\n",
      "Training Epoch: 1 [32940/49669]\tLoss: 48464.2539\n",
      "Training Epoch: 1 [32960/49669]\tLoss: 51118.1602\n",
      "Training Epoch: 1 [32980/49669]\tLoss: 51996.5938\n",
      "Training Epoch: 1 [33000/49669]\tLoss: 50600.0898\n",
      "Training Epoch: 1 [33020/49669]\tLoss: 48683.4883\n",
      "Training Epoch: 1 [33040/49669]\tLoss: 55385.9062\n",
      "Training Epoch: 1 [33060/49669]\tLoss: 54301.9219\n",
      "Training Epoch: 1 [33080/49669]\tLoss: 47979.6406\n",
      "Training Epoch: 1 [33100/49669]\tLoss: 63005.7773\n",
      "Training Epoch: 1 [33120/49669]\tLoss: 58704.5312\n",
      "Training Epoch: 1 [33140/49669]\tLoss: 52382.1094\n",
      "Training Epoch: 1 [33160/49669]\tLoss: 60329.6836\n",
      "Training Epoch: 1 [33180/49669]\tLoss: 50201.9727\n",
      "Training Epoch: 1 [33200/49669]\tLoss: 42243.8867\n",
      "Training Epoch: 1 [33220/49669]\tLoss: 49103.1641\n",
      "Training Epoch: 1 [33240/49669]\tLoss: 48475.5703\n",
      "Training Epoch: 1 [33260/49669]\tLoss: 43560.9648\n",
      "Training Epoch: 1 [33280/49669]\tLoss: 46758.8398\n",
      "Training Epoch: 1 [33300/49669]\tLoss: 47839.3438\n",
      "Training Epoch: 1 [33320/49669]\tLoss: 48596.5898\n",
      "Training Epoch: 1 [33340/49669]\tLoss: 43068.6328\n",
      "Training Epoch: 1 [33360/49669]\tLoss: 58464.0586\n",
      "Training Epoch: 1 [33380/49669]\tLoss: 57834.5703\n",
      "Training Epoch: 1 [33400/49669]\tLoss: 45868.4766\n",
      "Training Epoch: 1 [33420/49669]\tLoss: 48681.1016\n",
      "Training Epoch: 1 [33440/49669]\tLoss: 47449.5703\n",
      "Training Epoch: 1 [33460/49669]\tLoss: 53616.6250\n",
      "Training Epoch: 1 [33480/49669]\tLoss: 48617.1172\n",
      "Training Epoch: 1 [33500/49669]\tLoss: 44324.2773\n",
      "Training Epoch: 1 [33520/49669]\tLoss: 56015.5977\n",
      "Training Epoch: 1 [33540/49669]\tLoss: 51238.1328\n",
      "Training Epoch: 1 [33560/49669]\tLoss: 51792.5938\n",
      "Training Epoch: 1 [33580/49669]\tLoss: 42758.6602\n",
      "Training Epoch: 1 [33600/49669]\tLoss: 57165.0234\n",
      "Training Epoch: 1 [33620/49669]\tLoss: 49417.7461\n",
      "Training Epoch: 1 [33640/49669]\tLoss: 42656.3906\n",
      "Training Epoch: 1 [33660/49669]\tLoss: 53232.4102\n",
      "Training Epoch: 1 [33680/49669]\tLoss: 43119.7148\n",
      "Training Epoch: 1 [33700/49669]\tLoss: 49206.8672\n",
      "Training Epoch: 1 [33720/49669]\tLoss: 49773.1562\n",
      "Training Epoch: 1 [33740/49669]\tLoss: 49571.1328\n",
      "Training Epoch: 1 [33760/49669]\tLoss: 47437.4258\n",
      "Training Epoch: 1 [33780/49669]\tLoss: 47110.8281\n",
      "Training Epoch: 1 [33800/49669]\tLoss: 50696.0703\n",
      "Training Epoch: 1 [33820/49669]\tLoss: 53116.4766\n",
      "Training Epoch: 1 [33840/49669]\tLoss: 59884.9414\n",
      "Training Epoch: 1 [33860/49669]\tLoss: 49639.9570\n",
      "Training Epoch: 1 [33880/49669]\tLoss: 38071.4531\n",
      "Training Epoch: 1 [33900/49669]\tLoss: 43854.6250\n",
      "Training Epoch: 1 [33920/49669]\tLoss: 49317.5391\n",
      "Training Epoch: 1 [33940/49669]\tLoss: 48607.8594\n",
      "Training Epoch: 1 [33960/49669]\tLoss: 43440.8633\n",
      "Training Epoch: 1 [33980/49669]\tLoss: 42365.6641\n",
      "Training Epoch: 1 [34000/49669]\tLoss: 50046.7188\n",
      "Training Epoch: 1 [34020/49669]\tLoss: 50521.7383\n",
      "Training Epoch: 1 [34040/49669]\tLoss: 47974.4805\n",
      "Training Epoch: 1 [34060/49669]\tLoss: 55434.3281\n",
      "Training Epoch: 1 [34080/49669]\tLoss: 40163.9922\n",
      "Training Epoch: 1 [34100/49669]\tLoss: 46636.2422\n",
      "Training Epoch: 1 [34120/49669]\tLoss: 46393.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [34140/49669]\tLoss: 53837.3516\n",
      "Training Epoch: 1 [34160/49669]\tLoss: 45706.6055\n",
      "Training Epoch: 1 [34180/49669]\tLoss: 44048.0312\n",
      "Training Epoch: 1 [34200/49669]\tLoss: 40275.5469\n",
      "Training Epoch: 1 [34220/49669]\tLoss: 50089.7305\n",
      "Training Epoch: 1 [34240/49669]\tLoss: 50275.9414\n",
      "Training Epoch: 1 [34260/49669]\tLoss: 45585.2188\n",
      "Training Epoch: 1 [34280/49669]\tLoss: 47826.4727\n",
      "Training Epoch: 1 [34300/49669]\tLoss: 55940.0430\n",
      "Training Epoch: 1 [34320/49669]\tLoss: 44451.9219\n",
      "Training Epoch: 1 [34340/49669]\tLoss: 53936.9766\n",
      "Training Epoch: 1 [34360/49669]\tLoss: 51915.7852\n",
      "Training Epoch: 1 [34380/49669]\tLoss: 61228.2227\n",
      "Training Epoch: 1 [34400/49669]\tLoss: 66691.6875\n",
      "Training Epoch: 1 [34420/49669]\tLoss: 51380.9141\n",
      "Training Epoch: 1 [34440/49669]\tLoss: 45971.9570\n",
      "Training Epoch: 1 [34460/49669]\tLoss: 40996.8828\n",
      "Training Epoch: 1 [34480/49669]\tLoss: 46890.3516\n",
      "Training Epoch: 1 [34500/49669]\tLoss: 43481.3789\n",
      "Training Epoch: 1 [34520/49669]\tLoss: 41284.2578\n",
      "Training Epoch: 1 [34540/49669]\tLoss: 47653.5664\n",
      "Training Epoch: 1 [34560/49669]\tLoss: 39389.9375\n",
      "Training Epoch: 1 [34580/49669]\tLoss: 46568.2266\n",
      "Training Epoch: 1 [34600/49669]\tLoss: 45307.3828\n",
      "Training Epoch: 1 [34620/49669]\tLoss: 36578.8555\n",
      "Training Epoch: 1 [34640/49669]\tLoss: 45920.5039\n",
      "Training Epoch: 1 [34660/49669]\tLoss: 52171.0859\n",
      "Training Epoch: 1 [34680/49669]\tLoss: 44958.6602\n",
      "Training Epoch: 1 [34700/49669]\tLoss: 42901.4531\n",
      "Training Epoch: 1 [34720/49669]\tLoss: 48494.8047\n",
      "Training Epoch: 1 [34740/49669]\tLoss: 34944.6367\n",
      "Training Epoch: 1 [34760/49669]\tLoss: 41063.3984\n",
      "Training Epoch: 1 [34780/49669]\tLoss: 40549.5391\n",
      "Training Epoch: 1 [34800/49669]\tLoss: 43022.4531\n",
      "Training Epoch: 1 [34820/49669]\tLoss: 57437.3477\n",
      "Training Epoch: 1 [34840/49669]\tLoss: 35280.9219\n",
      "Training Epoch: 1 [34860/49669]\tLoss: 37420.1953\n",
      "Training Epoch: 1 [34880/49669]\tLoss: 43643.6523\n",
      "Training Epoch: 1 [34900/49669]\tLoss: 40888.1055\n",
      "Training Epoch: 1 [34920/49669]\tLoss: 48740.6055\n",
      "Training Epoch: 1 [34940/49669]\tLoss: 46054.4727\n",
      "Training Epoch: 1 [34960/49669]\tLoss: 47415.5898\n",
      "Training Epoch: 1 [34980/49669]\tLoss: 56489.4844\n",
      "Training Epoch: 1 [35000/49669]\tLoss: 62051.4648\n",
      "Training Epoch: 1 [35020/49669]\tLoss: 52512.4805\n",
      "Training Epoch: 1 [35040/49669]\tLoss: 42754.2344\n",
      "Training Epoch: 1 [35060/49669]\tLoss: 43969.5977\n",
      "Training Epoch: 1 [35080/49669]\tLoss: 43496.1523\n",
      "Training Epoch: 1 [35100/49669]\tLoss: 50134.7852\n",
      "Training Epoch: 1 [35120/49669]\tLoss: 40917.1602\n",
      "Training Epoch: 1 [35140/49669]\tLoss: 40532.2773\n",
      "Training Epoch: 1 [35160/49669]\tLoss: 42987.3867\n",
      "Training Epoch: 1 [35180/49669]\tLoss: 44898.1016\n",
      "Training Epoch: 1 [35200/49669]\tLoss: 39947.7773\n",
      "Training Epoch: 1 [35220/49669]\tLoss: 42679.7852\n",
      "Training Epoch: 1 [35240/49669]\tLoss: 47400.9297\n",
      "Training Epoch: 1 [35260/49669]\tLoss: 40893.4414\n",
      "Training Epoch: 1 [35280/49669]\tLoss: 35379.2461\n",
      "Training Epoch: 1 [35300/49669]\tLoss: 42452.3086\n",
      "Training Epoch: 1 [35320/49669]\tLoss: 49321.6133\n",
      "Training Epoch: 1 [35340/49669]\tLoss: 35851.7578\n",
      "Training Epoch: 1 [35360/49669]\tLoss: 51766.2617\n",
      "Training Epoch: 1 [35380/49669]\tLoss: 44051.8984\n",
      "Training Epoch: 1 [35400/49669]\tLoss: 42241.3047\n",
      "Training Epoch: 1 [35420/49669]\tLoss: 44555.1250\n",
      "Training Epoch: 1 [35440/49669]\tLoss: 42377.3164\n",
      "Training Epoch: 1 [35460/49669]\tLoss: 46405.2695\n",
      "Training Epoch: 1 [35480/49669]\tLoss: 39169.8828\n",
      "Training Epoch: 1 [35500/49669]\tLoss: 41091.2969\n",
      "Training Epoch: 1 [35520/49669]\tLoss: 40540.1016\n",
      "Training Epoch: 1 [35540/49669]\tLoss: 41747.4570\n",
      "Training Epoch: 1 [35560/49669]\tLoss: 42343.1914\n",
      "Training Epoch: 1 [35580/49669]\tLoss: 49793.0742\n",
      "Training Epoch: 1 [35600/49669]\tLoss: 47004.9648\n",
      "Training Epoch: 1 [35620/49669]\tLoss: 44589.4102\n",
      "Training Epoch: 1 [35640/49669]\tLoss: 37619.5156\n",
      "Training Epoch: 1 [35660/49669]\tLoss: 48752.2305\n",
      "Training Epoch: 1 [35680/49669]\tLoss: 45838.9180\n",
      "Training Epoch: 1 [35700/49669]\tLoss: 42751.3320\n",
      "Training Epoch: 1 [35720/49669]\tLoss: 41505.1016\n",
      "Training Epoch: 1 [35740/49669]\tLoss: 36502.8398\n",
      "Training Epoch: 1 [35760/49669]\tLoss: 46898.5430\n",
      "Training Epoch: 1 [35780/49669]\tLoss: 41973.2344\n",
      "Training Epoch: 1 [35800/49669]\tLoss: 39119.7109\n",
      "Training Epoch: 1 [35820/49669]\tLoss: 46068.2539\n",
      "Training Epoch: 1 [35840/49669]\tLoss: 43471.2969\n",
      "Training Epoch: 1 [35860/49669]\tLoss: 36221.5586\n",
      "Training Epoch: 1 [35880/49669]\tLoss: 39657.9375\n",
      "Training Epoch: 1 [35900/49669]\tLoss: 46624.0430\n",
      "Training Epoch: 1 [35920/49669]\tLoss: 48067.0781\n",
      "Training Epoch: 1 [35940/49669]\tLoss: 38020.4336\n",
      "Training Epoch: 1 [35960/49669]\tLoss: 48610.5547\n",
      "Training Epoch: 1 [35980/49669]\tLoss: 42596.7773\n",
      "Training Epoch: 1 [36000/49669]\tLoss: 44809.8750\n",
      "Training Epoch: 1 [36020/49669]\tLoss: 41994.2812\n",
      "Training Epoch: 1 [36040/49669]\tLoss: 40230.6250\n",
      "Training Epoch: 1 [36060/49669]\tLoss: 46776.1953\n",
      "Training Epoch: 1 [36080/49669]\tLoss: 40229.3945\n",
      "Training Epoch: 1 [36100/49669]\tLoss: 39985.0781\n",
      "Training Epoch: 1 [36120/49669]\tLoss: 50782.3594\n",
      "Training Epoch: 1 [36140/49669]\tLoss: 43993.3477\n",
      "Training Epoch: 1 [36160/49669]\tLoss: 47265.2188\n",
      "Training Epoch: 1 [36180/49669]\tLoss: 50028.8828\n",
      "Training Epoch: 1 [36200/49669]\tLoss: 48589.6289\n",
      "Training Epoch: 1 [36220/49669]\tLoss: 35181.8555\n",
      "Training Epoch: 1 [36240/49669]\tLoss: 48985.2891\n",
      "Training Epoch: 1 [36260/49669]\tLoss: 44674.8477\n",
      "Training Epoch: 1 [36280/49669]\tLoss: 45062.9531\n",
      "Training Epoch: 1 [36300/49669]\tLoss: 43812.3398\n",
      "Training Epoch: 1 [36320/49669]\tLoss: 43956.5000\n",
      "Training Epoch: 1 [36340/49669]\tLoss: 43216.4336\n",
      "Training Epoch: 1 [36360/49669]\tLoss: 43757.0664\n",
      "Training Epoch: 1 [36380/49669]\tLoss: 34686.1641\n",
      "Training Epoch: 1 [36400/49669]\tLoss: 41052.1602\n",
      "Training Epoch: 1 [36420/49669]\tLoss: 41493.3711\n",
      "Training Epoch: 1 [36440/49669]\tLoss: 40323.2305\n",
      "Training Epoch: 1 [36460/49669]\tLoss: 48462.8828\n",
      "Training Epoch: 1 [36480/49669]\tLoss: 38515.9180\n",
      "Training Epoch: 1 [36500/49669]\tLoss: 44838.9883\n",
      "Training Epoch: 1 [36520/49669]\tLoss: 43729.0117\n",
      "Training Epoch: 1 [36540/49669]\tLoss: 55003.1836\n",
      "Training Epoch: 1 [36560/49669]\tLoss: 44743.7461\n",
      "Training Epoch: 1 [36580/49669]\tLoss: 38443.0703\n",
      "Training Epoch: 1 [36600/49669]\tLoss: 43600.9297\n",
      "Training Epoch: 1 [36620/49669]\tLoss: 37866.3984\n",
      "Training Epoch: 1 [36640/49669]\tLoss: 41590.9102\n",
      "Training Epoch: 1 [36660/49669]\tLoss: 34830.9648\n",
      "Training Epoch: 1 [36680/49669]\tLoss: 39917.4023\n",
      "Training Epoch: 1 [36700/49669]\tLoss: 36982.0156\n",
      "Training Epoch: 1 [36720/49669]\tLoss: 40635.9336\n",
      "Training Epoch: 1 [36740/49669]\tLoss: 43640.8359\n",
      "Training Epoch: 1 [36760/49669]\tLoss: 38865.8086\n",
      "Training Epoch: 1 [36780/49669]\tLoss: 36514.6641\n",
      "Training Epoch: 1 [36800/49669]\tLoss: 34188.9141\n",
      "Training Epoch: 1 [36820/49669]\tLoss: 39299.9258\n",
      "Training Epoch: 1 [36840/49669]\tLoss: 47787.5352\n",
      "Training Epoch: 1 [36860/49669]\tLoss: 43538.3828\n",
      "Training Epoch: 1 [36880/49669]\tLoss: 41491.2266\n",
      "Training Epoch: 1 [36900/49669]\tLoss: 40032.0703\n",
      "Training Epoch: 1 [36920/49669]\tLoss: 40515.8594\n",
      "Training Epoch: 1 [36940/49669]\tLoss: 43214.1875\n",
      "Training Epoch: 1 [36960/49669]\tLoss: 38014.2461\n",
      "Training Epoch: 1 [36980/49669]\tLoss: 37409.9141\n",
      "Training Epoch: 1 [37000/49669]\tLoss: 39987.9297\n",
      "Training Epoch: 1 [37020/49669]\tLoss: 37675.8164\n",
      "Training Epoch: 1 [37040/49669]\tLoss: 45696.1211\n",
      "Training Epoch: 1 [37060/49669]\tLoss: 38198.4688\n",
      "Training Epoch: 1 [37080/49669]\tLoss: 44089.9961\n",
      "Training Epoch: 1 [37100/49669]\tLoss: 37178.1250\n",
      "Training Epoch: 1 [37120/49669]\tLoss: 40577.0508\n",
      "Training Epoch: 1 [37140/49669]\tLoss: 35898.3203\n",
      "Training Epoch: 1 [37160/49669]\tLoss: 37448.6992\n",
      "Training Epoch: 1 [37180/49669]\tLoss: 43595.0859\n",
      "Training Epoch: 1 [37200/49669]\tLoss: 39334.0781\n",
      "Training Epoch: 1 [37220/49669]\tLoss: 39477.4570\n",
      "Training Epoch: 1 [37240/49669]\tLoss: 38022.8047\n",
      "Training Epoch: 1 [37260/49669]\tLoss: 37972.5820\n",
      "Training Epoch: 1 [37280/49669]\tLoss: 45605.5938\n",
      "Training Epoch: 1 [37300/49669]\tLoss: 41366.7773\n",
      "Training Epoch: 1 [37320/49669]\tLoss: 42108.4570\n",
      "Training Epoch: 1 [37340/49669]\tLoss: 39508.8164\n",
      "Training Epoch: 1 [37360/49669]\tLoss: 35475.7930\n",
      "Training Epoch: 1 [37380/49669]\tLoss: 36605.3438\n",
      "Training Epoch: 1 [37400/49669]\tLoss: 46367.6289\n",
      "Training Epoch: 1 [37420/49669]\tLoss: 38526.5859\n",
      "Training Epoch: 1 [37440/49669]\tLoss: 34959.7539\n",
      "Training Epoch: 1 [37460/49669]\tLoss: 45019.4727\n",
      "Training Epoch: 1 [37480/49669]\tLoss: 49055.6836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [37500/49669]\tLoss: 42021.0117\n",
      "Training Epoch: 1 [37520/49669]\tLoss: 36676.3633\n",
      "Training Epoch: 1 [37540/49669]\tLoss: 37896.4883\n",
      "Training Epoch: 1 [37560/49669]\tLoss: 41947.6719\n",
      "Training Epoch: 1 [37580/49669]\tLoss: 34137.9023\n",
      "Training Epoch: 1 [37600/49669]\tLoss: 43566.3828\n",
      "Training Epoch: 1 [37620/49669]\tLoss: 36171.7461\n",
      "Training Epoch: 1 [37640/49669]\tLoss: 41922.9609\n",
      "Training Epoch: 1 [37660/49669]\tLoss: 40472.6016\n",
      "Training Epoch: 1 [37680/49669]\tLoss: 34780.2969\n",
      "Training Epoch: 1 [37700/49669]\tLoss: 39867.5742\n",
      "Training Epoch: 1 [37720/49669]\tLoss: 40759.4375\n",
      "Training Epoch: 1 [37740/49669]\tLoss: 32399.3711\n",
      "Training Epoch: 1 [37760/49669]\tLoss: 39385.4414\n",
      "Training Epoch: 1 [37780/49669]\tLoss: 45140.8281\n",
      "Training Epoch: 1 [37800/49669]\tLoss: 37555.5469\n",
      "Training Epoch: 1 [37820/49669]\tLoss: 39635.8828\n",
      "Training Epoch: 1 [37840/49669]\tLoss: 38513.0625\n",
      "Training Epoch: 1 [37860/49669]\tLoss: 34460.2578\n",
      "Training Epoch: 1 [37880/49669]\tLoss: 33060.7070\n",
      "Training Epoch: 1 [37900/49669]\tLoss: 37713.4844\n",
      "Training Epoch: 1 [37920/49669]\tLoss: 43227.7070\n",
      "Training Epoch: 1 [37940/49669]\tLoss: 37826.7461\n",
      "Training Epoch: 1 [37960/49669]\tLoss: 34793.1484\n",
      "Training Epoch: 1 [37980/49669]\tLoss: 32884.2031\n",
      "Training Epoch: 1 [38000/49669]\tLoss: 35573.4648\n",
      "Training Epoch: 1 [38020/49669]\tLoss: 37095.1172\n",
      "Training Epoch: 1 [38040/49669]\tLoss: 44875.3555\n",
      "Training Epoch: 1 [38060/49669]\tLoss: 45276.8242\n",
      "Training Epoch: 1 [38080/49669]\tLoss: 34667.6055\n",
      "Training Epoch: 1 [38100/49669]\tLoss: 37473.5117\n",
      "Training Epoch: 1 [38120/49669]\tLoss: 31433.1641\n",
      "Training Epoch: 1 [38140/49669]\tLoss: 36446.6914\n",
      "Training Epoch: 1 [38160/49669]\tLoss: 42676.5000\n",
      "Training Epoch: 1 [38180/49669]\tLoss: 43956.4570\n",
      "Training Epoch: 1 [38200/49669]\tLoss: 34240.0938\n",
      "Training Epoch: 1 [38220/49669]\tLoss: 39531.3242\n",
      "Training Epoch: 1 [38240/49669]\tLoss: 39300.8789\n",
      "Training Epoch: 1 [38260/49669]\tLoss: 32910.9062\n",
      "Training Epoch: 1 [38280/49669]\tLoss: 33256.2578\n",
      "Training Epoch: 1 [38300/49669]\tLoss: 31261.6914\n",
      "Training Epoch: 1 [38320/49669]\tLoss: 38745.2188\n",
      "Training Epoch: 1 [38340/49669]\tLoss: 52266.7578\n",
      "Training Epoch: 1 [38360/49669]\tLoss: 33941.2695\n",
      "Training Epoch: 1 [38380/49669]\tLoss: 30474.4609\n",
      "Training Epoch: 1 [38400/49669]\tLoss: 35971.8828\n",
      "Training Epoch: 1 [38420/49669]\tLoss: 40356.7227\n",
      "Training Epoch: 1 [38440/49669]\tLoss: 36897.9141\n",
      "Training Epoch: 1 [38460/49669]\tLoss: 35762.7383\n",
      "Training Epoch: 1 [38480/49669]\tLoss: 27624.3945\n",
      "Training Epoch: 1 [38500/49669]\tLoss: 47584.9102\n",
      "Training Epoch: 1 [38520/49669]\tLoss: 32436.4766\n",
      "Training Epoch: 1 [38540/49669]\tLoss: 32345.5859\n",
      "Training Epoch: 1 [38560/49669]\tLoss: 39697.9688\n",
      "Training Epoch: 1 [38580/49669]\tLoss: 33677.3906\n",
      "Training Epoch: 1 [38600/49669]\tLoss: 39019.1367\n",
      "Training Epoch: 1 [38620/49669]\tLoss: 42093.7109\n",
      "Training Epoch: 1 [38640/49669]\tLoss: 32581.0703\n",
      "Training Epoch: 1 [38660/49669]\tLoss: 30518.4141\n",
      "Training Epoch: 1 [38680/49669]\tLoss: 43593.4102\n",
      "Training Epoch: 1 [38700/49669]\tLoss: 38251.7773\n",
      "Training Epoch: 1 [38720/49669]\tLoss: 35770.0391\n",
      "Training Epoch: 1 [38740/49669]\tLoss: 42303.8672\n",
      "Training Epoch: 1 [38760/49669]\tLoss: 32402.9082\n",
      "Training Epoch: 1 [38780/49669]\tLoss: 33078.7227\n",
      "Training Epoch: 1 [38800/49669]\tLoss: 47408.8125\n",
      "Training Epoch: 1 [38820/49669]\tLoss: 39967.4023\n",
      "Training Epoch: 1 [38840/49669]\tLoss: 35223.7305\n",
      "Training Epoch: 1 [38860/49669]\tLoss: 44138.8438\n",
      "Training Epoch: 1 [38880/49669]\tLoss: 34866.1680\n",
      "Training Epoch: 1 [38900/49669]\tLoss: 31611.0156\n",
      "Training Epoch: 1 [38920/49669]\tLoss: 33738.6641\n",
      "Training Epoch: 1 [38940/49669]\tLoss: 35423.8516\n",
      "Training Epoch: 1 [38960/49669]\tLoss: 34640.8359\n",
      "Training Epoch: 1 [38980/49669]\tLoss: 38017.8398\n",
      "Training Epoch: 1 [39000/49669]\tLoss: 35814.5977\n",
      "Training Epoch: 1 [39020/49669]\tLoss: 34294.4062\n",
      "Training Epoch: 1 [39040/49669]\tLoss: 38912.1562\n",
      "Training Epoch: 1 [39060/49669]\tLoss: 40047.8008\n",
      "Training Epoch: 1 [39080/49669]\tLoss: 45810.6758\n",
      "Training Epoch: 1 [39100/49669]\tLoss: 33921.2148\n",
      "Training Epoch: 1 [39120/49669]\tLoss: 44648.9492\n",
      "Training Epoch: 1 [39140/49669]\tLoss: 25637.5449\n",
      "Training Epoch: 1 [39160/49669]\tLoss: 35009.6211\n",
      "Training Epoch: 1 [39180/49669]\tLoss: 34969.9062\n",
      "Training Epoch: 1 [39200/49669]\tLoss: 34916.3633\n",
      "Training Epoch: 1 [39220/49669]\tLoss: 44001.1172\n",
      "Training Epoch: 1 [39240/49669]\tLoss: 33310.1172\n",
      "Training Epoch: 1 [39260/49669]\tLoss: 32077.3125\n",
      "Training Epoch: 1 [39280/49669]\tLoss: 38105.8906\n",
      "Training Epoch: 1 [39300/49669]\tLoss: 30531.1934\n",
      "Training Epoch: 1 [39320/49669]\tLoss: 37645.8203\n",
      "Training Epoch: 1 [39340/49669]\tLoss: 39411.0898\n",
      "Training Epoch: 1 [39360/49669]\tLoss: 37997.1211\n",
      "Training Epoch: 1 [39380/49669]\tLoss: 35830.5586\n",
      "Training Epoch: 1 [39400/49669]\tLoss: 38346.6406\n",
      "Training Epoch: 1 [39420/49669]\tLoss: 42871.2227\n",
      "Training Epoch: 1 [39440/49669]\tLoss: 39969.0391\n",
      "Training Epoch: 1 [39460/49669]\tLoss: 36505.2422\n",
      "Training Epoch: 1 [39480/49669]\tLoss: 41979.3477\n",
      "Training Epoch: 1 [39500/49669]\tLoss: 43399.4961\n",
      "Training Epoch: 1 [39520/49669]\tLoss: 37409.3008\n",
      "Training Epoch: 1 [39540/49669]\tLoss: 38192.1602\n",
      "Training Epoch: 1 [39560/49669]\tLoss: 33989.9766\n",
      "Training Epoch: 1 [39580/49669]\tLoss: 47765.3828\n",
      "Training Epoch: 1 [39600/49669]\tLoss: 34097.6836\n",
      "Training Epoch: 1 [39620/49669]\tLoss: 35843.4336\n",
      "Training Epoch: 1 [39640/49669]\tLoss: 35355.7656\n",
      "Training Epoch: 1 [39660/49669]\tLoss: 34940.8789\n",
      "Training Epoch: 1 [39680/49669]\tLoss: 31496.2520\n",
      "Training Epoch: 1 [39700/49669]\tLoss: 32861.2461\n",
      "Training Epoch: 1 [39720/49669]\tLoss: 27266.9277\n",
      "Training Epoch: 1 [39740/49669]\tLoss: 28122.2148\n",
      "Training Epoch: 1 [39760/49669]\tLoss: 40025.8672\n",
      "Training Epoch: 1 [39780/49669]\tLoss: 35776.8672\n",
      "Training Epoch: 1 [39800/49669]\tLoss: 41840.7969\n",
      "Training Epoch: 1 [39820/49669]\tLoss: 38405.9727\n",
      "Training Epoch: 1 [39840/49669]\tLoss: 35444.4336\n",
      "Training Epoch: 1 [39860/49669]\tLoss: 30350.5859\n",
      "Training Epoch: 1 [39880/49669]\tLoss: 32567.4492\n",
      "Training Epoch: 1 [39900/49669]\tLoss: 45563.5977\n",
      "Training Epoch: 1 [39920/49669]\tLoss: 30789.8145\n",
      "Training Epoch: 1 [39940/49669]\tLoss: 34545.0039\n",
      "Training Epoch: 1 [39960/49669]\tLoss: 40967.9766\n",
      "Training Epoch: 1 [39980/49669]\tLoss: 32132.3574\n",
      "Training Epoch: 1 [40000/49669]\tLoss: 34736.3633\n",
      "Training Epoch: 1 [40020/49669]\tLoss: 34232.3086\n",
      "Training Epoch: 1 [40040/49669]\tLoss: 36592.3320\n",
      "Training Epoch: 1 [40060/49669]\tLoss: 33148.1602\n",
      "Training Epoch: 1 [40080/49669]\tLoss: 38751.6211\n",
      "Training Epoch: 1 [40100/49669]\tLoss: 33592.7773\n",
      "Training Epoch: 1 [40120/49669]\tLoss: 46331.5391\n",
      "Training Epoch: 1 [40140/49669]\tLoss: 35780.2734\n",
      "Training Epoch: 1 [40160/49669]\tLoss: 31920.6504\n",
      "Training Epoch: 1 [40180/49669]\tLoss: 32681.7832\n",
      "Training Epoch: 1 [40200/49669]\tLoss: 32228.5723\n",
      "Training Epoch: 1 [40220/49669]\tLoss: 31308.3457\n",
      "Training Epoch: 1 [40240/49669]\tLoss: 36851.4414\n",
      "Training Epoch: 1 [40260/49669]\tLoss: 33208.8164\n",
      "Training Epoch: 1 [40280/49669]\tLoss: 29969.6035\n",
      "Training Epoch: 1 [40300/49669]\tLoss: 38214.9727\n",
      "Training Epoch: 1 [40320/49669]\tLoss: 35206.9492\n",
      "Training Epoch: 1 [40340/49669]\tLoss: 34126.9922\n",
      "Training Epoch: 1 [40360/49669]\tLoss: 32889.1562\n",
      "Training Epoch: 1 [40380/49669]\tLoss: 34458.6484\n",
      "Training Epoch: 1 [40400/49669]\tLoss: 30145.7891\n",
      "Training Epoch: 1 [40420/49669]\tLoss: 27243.8730\n",
      "Training Epoch: 1 [40440/49669]\tLoss: 38718.3438\n",
      "Training Epoch: 1 [40460/49669]\tLoss: 32351.4824\n",
      "Training Epoch: 1 [40480/49669]\tLoss: 28660.7773\n",
      "Training Epoch: 1 [40500/49669]\tLoss: 31571.0840\n",
      "Training Epoch: 1 [40520/49669]\tLoss: 26635.2148\n",
      "Training Epoch: 1 [40540/49669]\tLoss: 33503.6172\n",
      "Training Epoch: 1 [40560/49669]\tLoss: 33290.0508\n",
      "Training Epoch: 1 [40580/49669]\tLoss: 27715.0703\n",
      "Training Epoch: 1 [40600/49669]\tLoss: 35081.2344\n",
      "Training Epoch: 1 [40620/49669]\tLoss: 35971.6445\n",
      "Training Epoch: 1 [40640/49669]\tLoss: 30339.4629\n",
      "Training Epoch: 1 [40660/49669]\tLoss: 32731.2227\n",
      "Training Epoch: 1 [40680/49669]\tLoss: 33322.6602\n",
      "Training Epoch: 1 [40700/49669]\tLoss: 27252.7188\n",
      "Training Epoch: 1 [40720/49669]\tLoss: 26392.3418\n",
      "Training Epoch: 1 [40740/49669]\tLoss: 36270.0664\n",
      "Training Epoch: 1 [40760/49669]\tLoss: 30226.3164\n",
      "Training Epoch: 1 [40780/49669]\tLoss: 31620.0391\n",
      "Training Epoch: 1 [40800/49669]\tLoss: 29636.5156\n",
      "Training Epoch: 1 [40820/49669]\tLoss: 26759.0859\n",
      "Training Epoch: 1 [40840/49669]\tLoss: 31086.7148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [40860/49669]\tLoss: 40975.8398\n",
      "Training Epoch: 1 [40880/49669]\tLoss: 37852.1836\n",
      "Training Epoch: 1 [40900/49669]\tLoss: 31541.6191\n",
      "Training Epoch: 1 [40920/49669]\tLoss: 37819.7031\n",
      "Training Epoch: 1 [40940/49669]\tLoss: 29311.7598\n",
      "Training Epoch: 1 [40960/49669]\tLoss: 28479.9277\n",
      "Training Epoch: 1 [40980/49669]\tLoss: 25829.8203\n",
      "Training Epoch: 1 [41000/49669]\tLoss: 29986.1523\n",
      "Training Epoch: 1 [41020/49669]\tLoss: 32430.1621\n",
      "Training Epoch: 1 [41040/49669]\tLoss: 30372.7578\n",
      "Training Epoch: 1 [41060/49669]\tLoss: 36157.1602\n",
      "Training Epoch: 1 [41080/49669]\tLoss: 28813.8750\n",
      "Training Epoch: 1 [41100/49669]\tLoss: 31610.6113\n",
      "Training Epoch: 1 [41120/49669]\tLoss: 31236.1953\n",
      "Training Epoch: 1 [41140/49669]\tLoss: 30332.6289\n",
      "Training Epoch: 1 [41160/49669]\tLoss: 27737.5625\n",
      "Training Epoch: 1 [41180/49669]\tLoss: 31610.2676\n",
      "Training Epoch: 1 [41200/49669]\tLoss: 30872.6113\n",
      "Training Epoch: 1 [41220/49669]\tLoss: 38827.1094\n",
      "Training Epoch: 1 [41240/49669]\tLoss: 28643.5059\n",
      "Training Epoch: 1 [41260/49669]\tLoss: 31211.5273\n",
      "Training Epoch: 1 [41280/49669]\tLoss: 33911.3594\n",
      "Training Epoch: 1 [41300/49669]\tLoss: 33831.2109\n",
      "Training Epoch: 1 [41320/49669]\tLoss: 33857.6758\n",
      "Training Epoch: 1 [41340/49669]\tLoss: 30621.2910\n",
      "Training Epoch: 1 [41360/49669]\tLoss: 30275.2910\n",
      "Training Epoch: 1 [41380/49669]\tLoss: 33089.4688\n",
      "Training Epoch: 1 [41400/49669]\tLoss: 26736.5254\n",
      "Training Epoch: 1 [41420/49669]\tLoss: 33755.8047\n",
      "Training Epoch: 1 [41440/49669]\tLoss: 29561.3008\n",
      "Training Epoch: 1 [41460/49669]\tLoss: 30233.7969\n",
      "Training Epoch: 1 [41480/49669]\tLoss: 30715.4551\n",
      "Training Epoch: 1 [41500/49669]\tLoss: 29361.3828\n",
      "Training Epoch: 1 [41520/49669]\tLoss: 31483.6758\n",
      "Training Epoch: 1 [41540/49669]\tLoss: 31317.7324\n",
      "Training Epoch: 1 [41560/49669]\tLoss: 34921.6641\n",
      "Training Epoch: 1 [41580/49669]\tLoss: 27547.0801\n",
      "Training Epoch: 1 [41600/49669]\tLoss: 34635.4844\n",
      "Training Epoch: 1 [41620/49669]\tLoss: 35273.3516\n",
      "Training Epoch: 1 [41640/49669]\tLoss: 28559.6758\n",
      "Training Epoch: 1 [41660/49669]\tLoss: 24311.8594\n",
      "Training Epoch: 1 [41680/49669]\tLoss: 33728.0547\n",
      "Training Epoch: 1 [41700/49669]\tLoss: 29459.3672\n",
      "Training Epoch: 1 [41720/49669]\tLoss: 26221.9297\n",
      "Training Epoch: 1 [41740/49669]\tLoss: 26728.9746\n",
      "Training Epoch: 1 [41760/49669]\tLoss: 28279.1523\n",
      "Training Epoch: 1 [41780/49669]\tLoss: 30685.5586\n",
      "Training Epoch: 1 [41800/49669]\tLoss: 35544.7344\n",
      "Training Epoch: 1 [41820/49669]\tLoss: 25113.5117\n",
      "Training Epoch: 1 [41840/49669]\tLoss: 29704.0918\n",
      "Training Epoch: 1 [41860/49669]\tLoss: 28393.5586\n",
      "Training Epoch: 1 [41880/49669]\tLoss: 30348.5391\n",
      "Training Epoch: 1 [41900/49669]\tLoss: 31089.3711\n",
      "Training Epoch: 1 [41920/49669]\tLoss: 25413.0723\n",
      "Training Epoch: 1 [41940/49669]\tLoss: 33465.8125\n",
      "Training Epoch: 1 [41960/49669]\tLoss: 30268.4043\n",
      "Training Epoch: 1 [41980/49669]\tLoss: 32761.3184\n",
      "Training Epoch: 1 [42000/49669]\tLoss: 33973.3359\n",
      "Training Epoch: 1 [42020/49669]\tLoss: 30917.7051\n",
      "Training Epoch: 1 [42040/49669]\tLoss: 32346.8262\n",
      "Training Epoch: 1 [42060/49669]\tLoss: 26046.0020\n",
      "Training Epoch: 1 [42080/49669]\tLoss: 28967.5859\n",
      "Training Epoch: 1 [42100/49669]\tLoss: 29945.3281\n",
      "Training Epoch: 1 [42120/49669]\tLoss: 29437.3086\n",
      "Training Epoch: 1 [42140/49669]\tLoss: 25687.8164\n",
      "Training Epoch: 1 [42160/49669]\tLoss: 31501.3633\n",
      "Training Epoch: 1 [42180/49669]\tLoss: 22783.2754\n",
      "Training Epoch: 1 [42200/49669]\tLoss: 27292.3555\n",
      "Training Epoch: 1 [42220/49669]\tLoss: 32171.8398\n",
      "Training Epoch: 1 [42240/49669]\tLoss: 32177.1250\n",
      "Training Epoch: 1 [42260/49669]\tLoss: 26330.1602\n",
      "Training Epoch: 1 [42280/49669]\tLoss: 28977.6465\n",
      "Training Epoch: 1 [42300/49669]\tLoss: 27321.4336\n",
      "Training Epoch: 1 [42320/49669]\tLoss: 22968.3301\n",
      "Training Epoch: 1 [42340/49669]\tLoss: 30378.8867\n",
      "Training Epoch: 1 [42360/49669]\tLoss: 29871.3535\n",
      "Training Epoch: 1 [42380/49669]\tLoss: 29214.2012\n",
      "Training Epoch: 1 [42400/49669]\tLoss: 34329.9375\n",
      "Training Epoch: 1 [42420/49669]\tLoss: 30380.8984\n",
      "Training Epoch: 1 [42440/49669]\tLoss: 28599.4902\n",
      "Training Epoch: 1 [42460/49669]\tLoss: 27981.3945\n",
      "Training Epoch: 1 [42480/49669]\tLoss: 30524.6191\n",
      "Training Epoch: 1 [42500/49669]\tLoss: 28039.9453\n",
      "Training Epoch: 1 [42520/49669]\tLoss: 27303.7969\n",
      "Training Epoch: 1 [42540/49669]\tLoss: 26033.9492\n",
      "Training Epoch: 1 [42560/49669]\tLoss: 25472.1758\n",
      "Training Epoch: 1 [42580/49669]\tLoss: 28140.1582\n",
      "Training Epoch: 1 [42600/49669]\tLoss: 34732.5703\n",
      "Training Epoch: 1 [42620/49669]\tLoss: 20719.9004\n",
      "Training Epoch: 1 [42640/49669]\tLoss: 28648.6152\n",
      "Training Epoch: 1 [42660/49669]\tLoss: 30731.3320\n",
      "Training Epoch: 1 [42680/49669]\tLoss: 28739.4531\n",
      "Training Epoch: 1 [42700/49669]\tLoss: 24060.9062\n",
      "Training Epoch: 1 [42720/49669]\tLoss: 28362.7188\n",
      "Training Epoch: 1 [42740/49669]\tLoss: 30990.7441\n",
      "Training Epoch: 1 [42760/49669]\tLoss: 29700.5137\n",
      "Training Epoch: 1 [42780/49669]\tLoss: 22161.9570\n",
      "Training Epoch: 1 [42800/49669]\tLoss: 27606.4023\n",
      "Training Epoch: 1 [42820/49669]\tLoss: 28242.0039\n",
      "Training Epoch: 1 [42840/49669]\tLoss: 29864.5898\n",
      "Training Epoch: 1 [42860/49669]\tLoss: 24821.9316\n",
      "Training Epoch: 1 [42880/49669]\tLoss: 30393.2832\n",
      "Training Epoch: 1 [42900/49669]\tLoss: 24858.7539\n",
      "Training Epoch: 1 [42920/49669]\tLoss: 29589.2148\n",
      "Training Epoch: 1 [42940/49669]\tLoss: 25563.2383\n",
      "Training Epoch: 1 [42960/49669]\tLoss: 25523.4668\n",
      "Training Epoch: 1 [42980/49669]\tLoss: 32150.6270\n",
      "Training Epoch: 1 [43000/49669]\tLoss: 25016.5605\n",
      "Training Epoch: 1 [43020/49669]\tLoss: 23897.1484\n",
      "Training Epoch: 1 [43040/49669]\tLoss: 24573.4551\n",
      "Training Epoch: 1 [43060/49669]\tLoss: 26482.0684\n",
      "Training Epoch: 1 [43080/49669]\tLoss: 26153.5371\n",
      "Training Epoch: 1 [43100/49669]\tLoss: 24814.3945\n",
      "Training Epoch: 1 [43120/49669]\tLoss: 25636.2871\n",
      "Training Epoch: 1 [43140/49669]\tLoss: 25017.4570\n",
      "Training Epoch: 1 [43160/49669]\tLoss: 27457.5703\n",
      "Training Epoch: 1 [43180/49669]\tLoss: 27865.3594\n",
      "Training Epoch: 1 [43200/49669]\tLoss: 24507.1191\n",
      "Training Epoch: 1 [43220/49669]\tLoss: 28673.9219\n",
      "Training Epoch: 1 [43240/49669]\tLoss: 21565.8750\n",
      "Training Epoch: 1 [43260/49669]\tLoss: 28924.9980\n",
      "Training Epoch: 1 [43280/49669]\tLoss: 26702.3281\n",
      "Training Epoch: 1 [43300/49669]\tLoss: 26976.3496\n",
      "Training Epoch: 1 [43320/49669]\tLoss: 25979.0117\n",
      "Training Epoch: 1 [43340/49669]\tLoss: 26101.7793\n",
      "Training Epoch: 1 [43360/49669]\tLoss: 26737.5977\n",
      "Training Epoch: 1 [43380/49669]\tLoss: 28521.1426\n",
      "Training Epoch: 1 [43400/49669]\tLoss: 23054.1758\n",
      "Training Epoch: 1 [43420/49669]\tLoss: 27804.0723\n",
      "Training Epoch: 1 [43440/49669]\tLoss: 31846.4785\n",
      "Training Epoch: 1 [43460/49669]\tLoss: 33781.9961\n",
      "Training Epoch: 1 [43480/49669]\tLoss: 25474.6934\n",
      "Training Epoch: 1 [43500/49669]\tLoss: 28435.7949\n",
      "Training Epoch: 1 [43520/49669]\tLoss: 28615.7598\n",
      "Training Epoch: 1 [43540/49669]\tLoss: 26327.3340\n",
      "Training Epoch: 1 [43560/49669]\tLoss: 25206.7500\n",
      "Training Epoch: 1 [43580/49669]\tLoss: 36246.8906\n",
      "Training Epoch: 1 [43600/49669]\tLoss: 31672.9199\n",
      "Training Epoch: 1 [43620/49669]\tLoss: 28628.1387\n",
      "Training Epoch: 1 [43640/49669]\tLoss: 26631.2480\n",
      "Training Epoch: 1 [43660/49669]\tLoss: 27551.6445\n",
      "Training Epoch: 1 [43680/49669]\tLoss: 30415.5605\n",
      "Training Epoch: 1 [43700/49669]\tLoss: 29635.6953\n",
      "Training Epoch: 1 [43720/49669]\tLoss: 32287.8125\n",
      "Training Epoch: 1 [43740/49669]\tLoss: 28514.5645\n",
      "Training Epoch: 1 [43760/49669]\tLoss: 24383.4199\n",
      "Training Epoch: 1 [43780/49669]\tLoss: 29283.8242\n",
      "Training Epoch: 1 [43800/49669]\tLoss: 25501.1055\n",
      "Training Epoch: 1 [43820/49669]\tLoss: 28915.2246\n",
      "Training Epoch: 1 [43840/49669]\tLoss: 30385.1680\n",
      "Training Epoch: 1 [43860/49669]\tLoss: 25834.8848\n",
      "Training Epoch: 1 [43880/49669]\tLoss: 32361.9668\n",
      "Training Epoch: 1 [43900/49669]\tLoss: 21442.0625\n",
      "Training Epoch: 1 [43920/49669]\tLoss: 22358.7637\n",
      "Training Epoch: 1 [43940/49669]\tLoss: 26205.3848\n",
      "Training Epoch: 1 [43960/49669]\tLoss: 22240.2207\n",
      "Training Epoch: 1 [43980/49669]\tLoss: 26837.9277\n",
      "Training Epoch: 1 [44000/49669]\tLoss: 25127.1680\n",
      "Training Epoch: 1 [44020/49669]\tLoss: 31470.4258\n",
      "Training Epoch: 1 [44040/49669]\tLoss: 30347.7305\n",
      "Training Epoch: 1 [44060/49669]\tLoss: 26681.0156\n",
      "Training Epoch: 1 [44080/49669]\tLoss: 21266.2383\n",
      "Training Epoch: 1 [44100/49669]\tLoss: 29758.4121\n",
      "Training Epoch: 1 [44120/49669]\tLoss: 27228.4902\n",
      "Training Epoch: 1 [44140/49669]\tLoss: 25153.9141\n",
      "Training Epoch: 1 [44160/49669]\tLoss: 29825.1367\n",
      "Training Epoch: 1 [44180/49669]\tLoss: 26723.3105\n",
      "Training Epoch: 1 [44200/49669]\tLoss: 28353.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [44220/49669]\tLoss: 30132.9941\n",
      "Training Epoch: 1 [44240/49669]\tLoss: 28202.2246\n",
      "Training Epoch: 1 [44260/49669]\tLoss: 22955.0898\n",
      "Training Epoch: 1 [44280/49669]\tLoss: 26772.6816\n",
      "Training Epoch: 1 [44300/49669]\tLoss: 23701.7715\n",
      "Training Epoch: 1 [44320/49669]\tLoss: 20777.6621\n",
      "Training Epoch: 1 [44340/49669]\tLoss: 25337.4609\n",
      "Training Epoch: 1 [44360/49669]\tLoss: 24482.9941\n",
      "Training Epoch: 1 [44380/49669]\tLoss: 24455.0859\n",
      "Training Epoch: 1 [44400/49669]\tLoss: 28958.4180\n",
      "Training Epoch: 1 [44420/49669]\tLoss: 29868.4355\n",
      "Training Epoch: 1 [44440/49669]\tLoss: 23122.9688\n",
      "Training Epoch: 1 [44460/49669]\tLoss: 26813.3398\n",
      "Training Epoch: 1 [44480/49669]\tLoss: 28005.2871\n",
      "Training Epoch: 1 [44500/49669]\tLoss: 26359.3379\n",
      "Training Epoch: 1 [44520/49669]\tLoss: 24080.2168\n",
      "Training Epoch: 1 [44540/49669]\tLoss: 25568.2949\n",
      "Training Epoch: 1 [44560/49669]\tLoss: 26744.5508\n",
      "Training Epoch: 1 [44580/49669]\tLoss: 23655.6797\n",
      "Training Epoch: 1 [44600/49669]\tLoss: 27069.6309\n",
      "Training Epoch: 1 [44620/49669]\tLoss: 25242.8105\n",
      "Training Epoch: 1 [44640/49669]\tLoss: 26260.3340\n",
      "Training Epoch: 1 [44660/49669]\tLoss: 22213.5762\n",
      "Training Epoch: 1 [44680/49669]\tLoss: 27859.8281\n",
      "Training Epoch: 1 [44700/49669]\tLoss: 20472.2910\n",
      "Training Epoch: 1 [44720/49669]\tLoss: 25070.4355\n",
      "Training Epoch: 1 [44740/49669]\tLoss: 21459.2246\n",
      "Training Epoch: 1 [44760/49669]\tLoss: 24651.7227\n",
      "Training Epoch: 1 [44780/49669]\tLoss: 19349.4590\n",
      "Training Epoch: 1 [44800/49669]\tLoss: 24738.7402\n",
      "Training Epoch: 1 [44820/49669]\tLoss: 30904.9336\n",
      "Training Epoch: 1 [44840/49669]\tLoss: 26099.0410\n",
      "Training Epoch: 1 [44860/49669]\tLoss: 23028.1309\n",
      "Training Epoch: 1 [44880/49669]\tLoss: 26004.5137\n",
      "Training Epoch: 1 [44900/49669]\tLoss: 20201.8457\n",
      "Training Epoch: 1 [44920/49669]\tLoss: 22700.3496\n",
      "Training Epoch: 1 [44940/49669]\tLoss: 22846.8965\n",
      "Training Epoch: 1 [44960/49669]\tLoss: 28280.3965\n",
      "Training Epoch: 1 [44980/49669]\tLoss: 22355.9844\n",
      "Training Epoch: 1 [45000/49669]\tLoss: 27846.3477\n",
      "Training Epoch: 1 [45020/49669]\tLoss: 29089.4297\n",
      "Training Epoch: 1 [45040/49669]\tLoss: 23520.4473\n",
      "Training Epoch: 1 [45060/49669]\tLoss: 24992.2793\n",
      "Training Epoch: 1 [45080/49669]\tLoss: 22032.6074\n",
      "Training Epoch: 1 [45100/49669]\tLoss: 24624.5547\n",
      "Training Epoch: 1 [45120/49669]\tLoss: 23484.1523\n",
      "Training Epoch: 1 [45140/49669]\tLoss: 23522.5215\n",
      "Training Epoch: 1 [45160/49669]\tLoss: 24346.0684\n",
      "Training Epoch: 1 [45180/49669]\tLoss: 27153.2578\n",
      "Training Epoch: 1 [45200/49669]\tLoss: 21606.2578\n",
      "Training Epoch: 1 [45220/49669]\tLoss: 26750.4551\n",
      "Training Epoch: 1 [45240/49669]\tLoss: 24282.2324\n",
      "Training Epoch: 1 [45260/49669]\tLoss: 21787.8223\n",
      "Training Epoch: 1 [45280/49669]\tLoss: 21849.3008\n",
      "Training Epoch: 1 [45300/49669]\tLoss: 27398.8691\n",
      "Training Epoch: 1 [45320/49669]\tLoss: 24447.6152\n",
      "Training Epoch: 1 [45340/49669]\tLoss: 19114.6797\n",
      "Training Epoch: 1 [45360/49669]\tLoss: 22011.5742\n",
      "Training Epoch: 1 [45380/49669]\tLoss: 24945.2129\n",
      "Training Epoch: 1 [45400/49669]\tLoss: 22701.2656\n",
      "Training Epoch: 1 [45420/49669]\tLoss: 24259.7422\n",
      "Training Epoch: 1 [45440/49669]\tLoss: 18484.8223\n",
      "Training Epoch: 1 [45460/49669]\tLoss: 21889.6934\n",
      "Training Epoch: 1 [45480/49669]\tLoss: 21786.7793\n",
      "Training Epoch: 1 [45500/49669]\tLoss: 25012.3828\n",
      "Training Epoch: 1 [45520/49669]\tLoss: 21466.7852\n",
      "Training Epoch: 1 [45540/49669]\tLoss: 22129.0762\n",
      "Training Epoch: 1 [45560/49669]\tLoss: 25403.8359\n",
      "Training Epoch: 1 [45580/49669]\tLoss: 21791.0977\n",
      "Training Epoch: 1 [45600/49669]\tLoss: 23723.8281\n",
      "Training Epoch: 1 [45620/49669]\tLoss: 21080.8652\n",
      "Training Epoch: 1 [45640/49669]\tLoss: 18594.9785\n",
      "Training Epoch: 1 [45660/49669]\tLoss: 23587.0391\n",
      "Training Epoch: 1 [45680/49669]\tLoss: 25205.9297\n",
      "Training Epoch: 1 [45700/49669]\tLoss: 22586.4453\n",
      "Training Epoch: 1 [45720/49669]\tLoss: 26721.4121\n",
      "Training Epoch: 1 [45740/49669]\tLoss: 23777.9297\n",
      "Training Epoch: 1 [45760/49669]\tLoss: 27355.7148\n",
      "Training Epoch: 1 [45780/49669]\tLoss: 28550.8770\n",
      "Training Epoch: 1 [45800/49669]\tLoss: 20971.6211\n",
      "Training Epoch: 1 [45820/49669]\tLoss: 22399.9043\n",
      "Training Epoch: 1 [45840/49669]\tLoss: 28472.2852\n",
      "Training Epoch: 1 [45860/49669]\tLoss: 23513.8457\n",
      "Training Epoch: 1 [45880/49669]\tLoss: 22801.8809\n",
      "Training Epoch: 1 [45900/49669]\tLoss: 24443.3633\n",
      "Training Epoch: 1 [45920/49669]\tLoss: 26090.6191\n",
      "Training Epoch: 1 [45940/49669]\tLoss: 20926.7109\n",
      "Training Epoch: 1 [45960/49669]\tLoss: 25137.8086\n",
      "Training Epoch: 1 [45980/49669]\tLoss: 27619.3281\n",
      "Training Epoch: 1 [46000/49669]\tLoss: 22135.6230\n",
      "Training Epoch: 1 [46020/49669]\tLoss: 22818.8828\n",
      "Training Epoch: 1 [46040/49669]\tLoss: 18609.3438\n",
      "Training Epoch: 1 [46060/49669]\tLoss: 22486.7539\n",
      "Training Epoch: 1 [46080/49669]\tLoss: 21354.4023\n",
      "Training Epoch: 1 [46100/49669]\tLoss: 26010.5469\n",
      "Training Epoch: 1 [46120/49669]\tLoss: 29953.8223\n",
      "Training Epoch: 1 [46140/49669]\tLoss: 26127.9531\n",
      "Training Epoch: 1 [46160/49669]\tLoss: 22051.0723\n",
      "Training Epoch: 1 [46180/49669]\tLoss: 20805.5312\n",
      "Training Epoch: 1 [46200/49669]\tLoss: 20563.7363\n",
      "Training Epoch: 1 [46220/49669]\tLoss: 23805.7129\n",
      "Training Epoch: 1 [46240/49669]\tLoss: 21511.3086\n",
      "Training Epoch: 1 [46260/49669]\tLoss: 21868.6934\n",
      "Training Epoch: 1 [46280/49669]\tLoss: 21212.6602\n",
      "Training Epoch: 1 [46300/49669]\tLoss: 20637.5527\n",
      "Training Epoch: 1 [46320/49669]\tLoss: 22636.5938\n",
      "Training Epoch: 1 [46340/49669]\tLoss: 20800.4297\n",
      "Training Epoch: 1 [46360/49669]\tLoss: 23997.0566\n",
      "Training Epoch: 1 [46380/49669]\tLoss: 26073.6465\n",
      "Training Epoch: 1 [46400/49669]\tLoss: 20914.8457\n",
      "Training Epoch: 1 [46420/49669]\tLoss: 22495.4785\n",
      "Training Epoch: 1 [46440/49669]\tLoss: 22201.2305\n",
      "Training Epoch: 1 [46460/49669]\tLoss: 23858.7598\n",
      "Training Epoch: 1 [46480/49669]\tLoss: 20850.6152\n",
      "Training Epoch: 1 [46500/49669]\tLoss: 26971.7578\n",
      "Training Epoch: 1 [46520/49669]\tLoss: 18667.0586\n",
      "Training Epoch: 1 [46540/49669]\tLoss: 16978.1934\n",
      "Training Epoch: 1 [46560/49669]\tLoss: 22533.0098\n",
      "Training Epoch: 1 [46580/49669]\tLoss: 21181.9434\n",
      "Training Epoch: 1 [46600/49669]\tLoss: 22962.7148\n",
      "Training Epoch: 1 [46620/49669]\tLoss: 20593.3516\n",
      "Training Epoch: 1 [46640/49669]\tLoss: 24434.3281\n",
      "Training Epoch: 1 [46660/49669]\tLoss: 22660.1250\n",
      "Training Epoch: 1 [46680/49669]\tLoss: 23230.8906\n",
      "Training Epoch: 1 [46700/49669]\tLoss: 23776.8965\n",
      "Training Epoch: 1 [46720/49669]\tLoss: 23401.1230\n",
      "Training Epoch: 1 [46740/49669]\tLoss: 27319.8984\n",
      "Training Epoch: 1 [46760/49669]\tLoss: 20488.0938\n",
      "Training Epoch: 1 [46780/49669]\tLoss: 22061.3027\n",
      "Training Epoch: 1 [46800/49669]\tLoss: 19768.1660\n",
      "Training Epoch: 1 [46820/49669]\tLoss: 22402.9512\n",
      "Training Epoch: 1 [46840/49669]\tLoss: 24408.8730\n",
      "Training Epoch: 1 [46860/49669]\tLoss: 23302.2070\n",
      "Training Epoch: 1 [46880/49669]\tLoss: 23782.4316\n",
      "Training Epoch: 1 [46900/49669]\tLoss: 23026.3691\n",
      "Training Epoch: 1 [46920/49669]\tLoss: 21332.1016\n",
      "Training Epoch: 1 [46940/49669]\tLoss: 20629.9727\n",
      "Training Epoch: 1 [46960/49669]\tLoss: 24701.2676\n",
      "Training Epoch: 1 [46980/49669]\tLoss: 26966.1191\n",
      "Training Epoch: 1 [47000/49669]\tLoss: 22006.1016\n",
      "Training Epoch: 1 [47020/49669]\tLoss: 20308.3379\n",
      "Training Epoch: 1 [47040/49669]\tLoss: 24596.5762\n",
      "Training Epoch: 1 [47060/49669]\tLoss: 21124.0117\n",
      "Training Epoch: 1 [47080/49669]\tLoss: 21122.3438\n",
      "Training Epoch: 1 [47100/49669]\tLoss: 23164.9082\n",
      "Training Epoch: 1 [47120/49669]\tLoss: 20187.9551\n",
      "Training Epoch: 1 [47140/49669]\tLoss: 19674.1523\n",
      "Training Epoch: 1 [47160/49669]\tLoss: 23160.4316\n",
      "Training Epoch: 1 [47180/49669]\tLoss: 23562.1328\n",
      "Training Epoch: 1 [47200/49669]\tLoss: 21956.0391\n",
      "Training Epoch: 1 [47220/49669]\tLoss: 18861.8145\n",
      "Training Epoch: 1 [47240/49669]\tLoss: 21566.9922\n",
      "Training Epoch: 1 [47260/49669]\tLoss: 16953.7676\n",
      "Training Epoch: 1 [47280/49669]\tLoss: 15703.1729\n",
      "Training Epoch: 1 [47300/49669]\tLoss: 22912.2324\n",
      "Training Epoch: 1 [47320/49669]\tLoss: 22427.4199\n",
      "Training Epoch: 1 [47340/49669]\tLoss: 24988.0840\n",
      "Training Epoch: 1 [47360/49669]\tLoss: 21106.6230\n",
      "Training Epoch: 1 [47380/49669]\tLoss: 20175.2520\n",
      "Training Epoch: 1 [47400/49669]\tLoss: 23306.8477\n",
      "Training Epoch: 1 [47420/49669]\tLoss: 22748.4219\n",
      "Training Epoch: 1 [47440/49669]\tLoss: 23617.5684\n",
      "Training Epoch: 1 [47460/49669]\tLoss: 21816.4551\n",
      "Training Epoch: 1 [47480/49669]\tLoss: 19228.0527\n",
      "Training Epoch: 1 [47500/49669]\tLoss: 25418.9062\n",
      "Training Epoch: 1 [47520/49669]\tLoss: 22559.0312\n",
      "Training Epoch: 1 [47540/49669]\tLoss: 21759.6094\n",
      "Training Epoch: 1 [47560/49669]\tLoss: 23806.9746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [47580/49669]\tLoss: 21943.0781\n",
      "Training Epoch: 1 [47600/49669]\tLoss: 23346.9551\n",
      "Training Epoch: 1 [47620/49669]\tLoss: 20860.8086\n",
      "Training Epoch: 1 [47640/49669]\tLoss: 22088.9434\n",
      "Training Epoch: 1 [47660/49669]\tLoss: 23350.0254\n",
      "Training Epoch: 1 [47680/49669]\tLoss: 21006.8359\n",
      "Training Epoch: 1 [47700/49669]\tLoss: 17983.5137\n",
      "Training Epoch: 1 [47720/49669]\tLoss: 15360.0088\n",
      "Training Epoch: 1 [47740/49669]\tLoss: 23830.2031\n",
      "Training Epoch: 1 [47760/49669]\tLoss: 22162.8516\n",
      "Training Epoch: 1 [47780/49669]\tLoss: 19984.1309\n",
      "Training Epoch: 1 [47800/49669]\tLoss: 20049.2988\n",
      "Training Epoch: 1 [47820/49669]\tLoss: 21938.6738\n",
      "Training Epoch: 1 [47840/49669]\tLoss: 16652.5508\n",
      "Training Epoch: 1 [47860/49669]\tLoss: 18950.8516\n",
      "Training Epoch: 1 [47880/49669]\tLoss: 19953.6309\n",
      "Training Epoch: 1 [47900/49669]\tLoss: 16490.3086\n",
      "Training Epoch: 1 [47920/49669]\tLoss: 20459.2695\n",
      "Training Epoch: 1 [47940/49669]\tLoss: 20426.7480\n",
      "Training Epoch: 1 [47960/49669]\tLoss: 20297.5234\n",
      "Training Epoch: 1 [47980/49669]\tLoss: 21515.3730\n",
      "Training Epoch: 1 [48000/49669]\tLoss: 21445.9746\n",
      "Training Epoch: 1 [48020/49669]\tLoss: 22199.9277\n",
      "Training Epoch: 1 [48040/49669]\tLoss: 22423.2266\n",
      "Training Epoch: 1 [48060/49669]\tLoss: 21401.2930\n",
      "Training Epoch: 1 [48080/49669]\tLoss: 22152.1113\n",
      "Training Epoch: 1 [48100/49669]\tLoss: 20400.7480\n",
      "Training Epoch: 1 [48120/49669]\tLoss: 21642.9473\n",
      "Training Epoch: 1 [48140/49669]\tLoss: 18253.2617\n",
      "Training Epoch: 1 [48160/49669]\tLoss: 22597.0723\n",
      "Training Epoch: 1 [48180/49669]\tLoss: 25173.0391\n",
      "Training Epoch: 1 [48200/49669]\tLoss: 25133.3340\n",
      "Training Epoch: 1 [48220/49669]\tLoss: 20870.6641\n",
      "Training Epoch: 1 [48240/49669]\tLoss: 19317.5645\n",
      "Training Epoch: 1 [48260/49669]\tLoss: 19846.5879\n",
      "Training Epoch: 1 [48280/49669]\tLoss: 23451.9453\n",
      "Training Epoch: 1 [48300/49669]\tLoss: 22653.7461\n",
      "Training Epoch: 1 [48320/49669]\tLoss: 19827.8691\n",
      "Training Epoch: 1 [48340/49669]\tLoss: 18094.2246\n",
      "Training Epoch: 1 [48360/49669]\tLoss: 20360.4062\n",
      "Training Epoch: 1 [48380/49669]\tLoss: 20616.5742\n",
      "Training Epoch: 1 [48400/49669]\tLoss: 19666.2363\n",
      "Training Epoch: 1 [48420/49669]\tLoss: 20794.9551\n",
      "Training Epoch: 1 [48440/49669]\tLoss: 19655.5566\n",
      "Training Epoch: 1 [48460/49669]\tLoss: 20440.9707\n",
      "Training Epoch: 1 [48480/49669]\tLoss: 22185.0957\n",
      "Training Epoch: 1 [48500/49669]\tLoss: 24232.4453\n",
      "Training Epoch: 1 [48520/49669]\tLoss: 18889.5977\n",
      "Training Epoch: 1 [48540/49669]\tLoss: 21357.7910\n",
      "Training Epoch: 1 [48560/49669]\tLoss: 22855.8184\n",
      "Training Epoch: 1 [48580/49669]\tLoss: 20236.2227\n",
      "Training Epoch: 1 [48600/49669]\tLoss: 21605.2109\n",
      "Training Epoch: 1 [48620/49669]\tLoss: 22911.7168\n",
      "Training Epoch: 1 [48640/49669]\tLoss: 20596.7539\n",
      "Training Epoch: 1 [48660/49669]\tLoss: 16840.7734\n",
      "Training Epoch: 1 [48680/49669]\tLoss: 21444.7051\n",
      "Training Epoch: 1 [48700/49669]\tLoss: 20734.4941\n",
      "Training Epoch: 1 [48720/49669]\tLoss: 20431.4785\n",
      "Training Epoch: 1 [48740/49669]\tLoss: 22649.5488\n",
      "Training Epoch: 1 [48760/49669]\tLoss: 22952.0020\n",
      "Training Epoch: 1 [48780/49669]\tLoss: 20882.5273\n",
      "Training Epoch: 1 [48800/49669]\tLoss: 20853.8828\n",
      "Training Epoch: 1 [48820/49669]\tLoss: 20333.4805\n",
      "Training Epoch: 1 [48840/49669]\tLoss: 21205.2871\n",
      "Training Epoch: 1 [48860/49669]\tLoss: 18339.0527\n",
      "Training Epoch: 1 [48880/49669]\tLoss: 19343.9844\n",
      "Training Epoch: 1 [48900/49669]\tLoss: 22883.0273\n",
      "Training Epoch: 1 [48920/49669]\tLoss: 19883.3340\n",
      "Training Epoch: 1 [48940/49669]\tLoss: 21399.9707\n",
      "Training Epoch: 1 [48960/49669]\tLoss: 24421.6055\n",
      "Training Epoch: 1 [48980/49669]\tLoss: 21158.3418\n",
      "Training Epoch: 1 [49000/49669]\tLoss: 20502.5938\n",
      "Training Epoch: 1 [49020/49669]\tLoss: 19706.9492\n",
      "Training Epoch: 1 [49040/49669]\tLoss: 19691.9941\n",
      "Training Epoch: 1 [49060/49669]\tLoss: 18921.9414\n",
      "Training Epoch: 1 [49080/49669]\tLoss: 21990.5000\n",
      "Training Epoch: 1 [49100/49669]\tLoss: 19653.4766\n",
      "Training Epoch: 1 [49120/49669]\tLoss: 21409.7500\n",
      "Training Epoch: 1 [49140/49669]\tLoss: 19865.7480\n",
      "Training Epoch: 1 [49160/49669]\tLoss: 21610.6738\n",
      "Training Epoch: 1 [49180/49669]\tLoss: 17334.5566\n",
      "Training Epoch: 1 [49200/49669]\tLoss: 19146.4219\n",
      "Training Epoch: 1 [49220/49669]\tLoss: 18724.8457\n",
      "Training Epoch: 1 [49240/49669]\tLoss: 18801.1016\n",
      "Training Epoch: 1 [49260/49669]\tLoss: 26095.5781\n",
      "Training Epoch: 1 [49280/49669]\tLoss: 18758.2402\n",
      "Training Epoch: 1 [49300/49669]\tLoss: 21165.3203\n",
      "Training Epoch: 1 [49320/49669]\tLoss: 22614.9629\n",
      "Training Epoch: 1 [49340/49669]\tLoss: 23728.8516\n",
      "Training Epoch: 1 [49360/49669]\tLoss: 20611.6602\n",
      "Training Epoch: 1 [49380/49669]\tLoss: 19032.6074\n",
      "Training Epoch: 1 [49400/49669]\tLoss: 26093.7734\n",
      "Training Epoch: 1 [49420/49669]\tLoss: 18947.9414\n",
      "Training Epoch: 1 [49440/49669]\tLoss: 18539.9297\n",
      "Training Epoch: 1 [49460/49669]\tLoss: 17920.3418\n",
      "Training Epoch: 1 [49480/49669]\tLoss: 22011.2949\n",
      "Training Epoch: 1 [49500/49669]\tLoss: 22633.9473\n",
      "Training Epoch: 1 [49520/49669]\tLoss: 16897.1504\n",
      "Training Epoch: 1 [49540/49669]\tLoss: 20128.4707\n",
      "Training Epoch: 1 [49560/49669]\tLoss: 21231.6152\n",
      "Training Epoch: 1 [49580/49669]\tLoss: 22483.8770\n",
      "Training Epoch: 1 [49600/49669]\tLoss: 21988.7480\n",
      "Training Epoch: 1 [49620/49669]\tLoss: 21167.0352\n",
      "Training Epoch: 1 [49640/49669]\tLoss: 22495.4922\n",
      "Training Epoch: 1 [49660/49669]\tLoss: 18519.2930\n",
      "Training Epoch: 1 [49669/49669]\tLoss: 20293.6445\n",
      "Training Epoch: 1 [5519/5519]\tLoss: 19353.8161\n",
      "Training Epoch: 2 [20/49669]\tLoss: 17290.9258\n",
      "Training Epoch: 2 [40/49669]\tLoss: 15642.3564\n",
      "Training Epoch: 2 [60/49669]\tLoss: 18923.6680\n",
      "Training Epoch: 2 [80/49669]\tLoss: 18813.4160\n",
      "Training Epoch: 2 [100/49669]\tLoss: 24736.0918\n",
      "Training Epoch: 2 [120/49669]\tLoss: 23086.5059\n",
      "Training Epoch: 2 [140/49669]\tLoss: 22071.6836\n",
      "Training Epoch: 2 [160/49669]\tLoss: 15705.2520\n",
      "Training Epoch: 2 [180/49669]\tLoss: 19549.5410\n",
      "Training Epoch: 2 [200/49669]\tLoss: 17833.6543\n",
      "Training Epoch: 2 [220/49669]\tLoss: 19535.4570\n",
      "Training Epoch: 2 [240/49669]\tLoss: 22837.7852\n",
      "Training Epoch: 2 [260/49669]\tLoss: 16220.0977\n",
      "Training Epoch: 2 [280/49669]\tLoss: 22966.4512\n",
      "Training Epoch: 2 [300/49669]\tLoss: 19304.1895\n",
      "Training Epoch: 2 [320/49669]\tLoss: 18167.2773\n",
      "Training Epoch: 2 [340/49669]\tLoss: 18769.5469\n",
      "Training Epoch: 2 [360/49669]\tLoss: 19178.9160\n",
      "Training Epoch: 2 [380/49669]\tLoss: 22064.3965\n",
      "Training Epoch: 2 [400/49669]\tLoss: 19398.1855\n",
      "Training Epoch: 2 [420/49669]\tLoss: 21019.1738\n",
      "Training Epoch: 2 [440/49669]\tLoss: 21194.7227\n",
      "Training Epoch: 2 [460/49669]\tLoss: 20083.9609\n",
      "Training Epoch: 2 [480/49669]\tLoss: 16674.0059\n",
      "Training Epoch: 2 [500/49669]\tLoss: 18196.0039\n",
      "Training Epoch: 2 [520/49669]\tLoss: 18211.0195\n",
      "Training Epoch: 2 [540/49669]\tLoss: 19859.0430\n",
      "Training Epoch: 2 [560/49669]\tLoss: 15779.9814\n",
      "Training Epoch: 2 [580/49669]\tLoss: 21254.0957\n",
      "Training Epoch: 2 [600/49669]\tLoss: 19727.1855\n",
      "Training Epoch: 2 [620/49669]\tLoss: 20487.4805\n",
      "Training Epoch: 2 [640/49669]\tLoss: 19153.5117\n",
      "Training Epoch: 2 [660/49669]\tLoss: 16120.9414\n",
      "Training Epoch: 2 [680/49669]\tLoss: 18810.0410\n",
      "Training Epoch: 2 [700/49669]\tLoss: 14880.6729\n",
      "Training Epoch: 2 [720/49669]\tLoss: 15449.3340\n",
      "Training Epoch: 2 [740/49669]\tLoss: 22345.8086\n",
      "Training Epoch: 2 [760/49669]\tLoss: 19957.2266\n",
      "Training Epoch: 2 [780/49669]\tLoss: 15889.4111\n",
      "Training Epoch: 2 [800/49669]\tLoss: 22254.9199\n",
      "Training Epoch: 2 [820/49669]\tLoss: 17512.6113\n",
      "Training Epoch: 2 [840/49669]\tLoss: 17173.7363\n",
      "Training Epoch: 2 [860/49669]\tLoss: 19112.1484\n",
      "Training Epoch: 2 [880/49669]\tLoss: 14339.4746\n",
      "Training Epoch: 2 [900/49669]\tLoss: 18920.0234\n",
      "Training Epoch: 2 [920/49669]\tLoss: 13637.3975\n",
      "Training Epoch: 2 [940/49669]\tLoss: 19672.5859\n",
      "Training Epoch: 2 [960/49669]\tLoss: 18501.4199\n",
      "Training Epoch: 2 [980/49669]\tLoss: 17190.3945\n",
      "Training Epoch: 2 [1000/49669]\tLoss: 20968.2988\n",
      "Training Epoch: 2 [1020/49669]\tLoss: 19947.8027\n",
      "Training Epoch: 2 [1040/49669]\tLoss: 24227.2324\n",
      "Training Epoch: 2 [1060/49669]\tLoss: 20298.5098\n",
      "Training Epoch: 2 [1080/49669]\tLoss: 17210.5098\n",
      "Training Epoch: 2 [1100/49669]\tLoss: 17582.4629\n",
      "Training Epoch: 2 [1120/49669]\tLoss: 18039.4609\n",
      "Training Epoch: 2 [1140/49669]\tLoss: 19009.2988\n",
      "Training Epoch: 2 [1160/49669]\tLoss: 22175.0762\n",
      "Training Epoch: 2 [1180/49669]\tLoss: 14021.8730\n",
      "Training Epoch: 2 [1200/49669]\tLoss: 17259.7090\n",
      "Training Epoch: 2 [1220/49669]\tLoss: 15949.5781\n",
      "Training Epoch: 2 [1240/49669]\tLoss: 17660.9941\n",
      "Training Epoch: 2 [1260/49669]\tLoss: 20098.3730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [1280/49669]\tLoss: 16738.6309\n",
      "Training Epoch: 2 [1300/49669]\tLoss: 21261.3066\n",
      "Training Epoch: 2 [1320/49669]\tLoss: 18888.5664\n",
      "Training Epoch: 2 [1340/49669]\tLoss: 16957.8770\n",
      "Training Epoch: 2 [1360/49669]\tLoss: 16451.9590\n",
      "Training Epoch: 2 [1380/49669]\tLoss: 19086.0449\n",
      "Training Epoch: 2 [1400/49669]\tLoss: 17690.6016\n",
      "Training Epoch: 2 [1420/49669]\tLoss: 16156.9102\n",
      "Training Epoch: 2 [1440/49669]\tLoss: 15991.3330\n",
      "Training Epoch: 2 [1460/49669]\tLoss: 15697.9150\n",
      "Training Epoch: 2 [1480/49669]\tLoss: 19509.2598\n",
      "Training Epoch: 2 [1500/49669]\tLoss: 18777.4316\n",
      "Training Epoch: 2 [1520/49669]\tLoss: 15819.5215\n",
      "Training Epoch: 2 [1540/49669]\tLoss: 18882.9043\n",
      "Training Epoch: 2 [1560/49669]\tLoss: 19240.7910\n",
      "Training Epoch: 2 [1580/49669]\tLoss: 19437.1387\n",
      "Training Epoch: 2 [1600/49669]\tLoss: 18893.3047\n",
      "Training Epoch: 2 [1620/49669]\tLoss: 18204.5449\n",
      "Training Epoch: 2 [1640/49669]\tLoss: 17359.8125\n",
      "Training Epoch: 2 [1660/49669]\tLoss: 19217.5234\n",
      "Training Epoch: 2 [1680/49669]\tLoss: 13949.5820\n",
      "Training Epoch: 2 [1700/49669]\tLoss: 14833.4150\n",
      "Training Epoch: 2 [1720/49669]\tLoss: 16745.3105\n",
      "Training Epoch: 2 [1740/49669]\tLoss: 17153.8730\n",
      "Training Epoch: 2 [1760/49669]\tLoss: 16317.2266\n",
      "Training Epoch: 2 [1780/49669]\tLoss: 16869.5703\n",
      "Training Epoch: 2 [1800/49669]\tLoss: 18181.3926\n",
      "Training Epoch: 2 [1820/49669]\tLoss: 19353.5117\n",
      "Training Epoch: 2 [1840/49669]\tLoss: 17055.8555\n",
      "Training Epoch: 2 [1860/49669]\tLoss: 19800.0566\n",
      "Training Epoch: 2 [1880/49669]\tLoss: 17446.3008\n",
      "Training Epoch: 2 [1900/49669]\tLoss: 15979.1855\n",
      "Training Epoch: 2 [1920/49669]\tLoss: 16293.4844\n",
      "Training Epoch: 2 [1940/49669]\tLoss: 19715.6152\n",
      "Training Epoch: 2 [1960/49669]\tLoss: 19023.5547\n",
      "Training Epoch: 2 [1980/49669]\tLoss: 17001.8770\n",
      "Training Epoch: 2 [2000/49669]\tLoss: 15648.6475\n",
      "Training Epoch: 2 [2020/49669]\tLoss: 19655.3730\n",
      "Training Epoch: 2 [2040/49669]\tLoss: 17353.2988\n",
      "Training Epoch: 2 [2060/49669]\tLoss: 17853.1875\n",
      "Training Epoch: 2 [2080/49669]\tLoss: 17860.5781\n",
      "Training Epoch: 2 [2100/49669]\tLoss: 19895.8340\n",
      "Training Epoch: 2 [2120/49669]\tLoss: 20788.1191\n",
      "Training Epoch: 2 [2140/49669]\tLoss: 15985.2119\n",
      "Training Epoch: 2 [2160/49669]\tLoss: 17135.1367\n",
      "Training Epoch: 2 [2180/49669]\tLoss: 19469.0977\n",
      "Training Epoch: 2 [2200/49669]\tLoss: 14952.3711\n",
      "Training Epoch: 2 [2220/49669]\tLoss: 14953.0117\n",
      "Training Epoch: 2 [2240/49669]\tLoss: 13906.0410\n",
      "Training Epoch: 2 [2260/49669]\tLoss: 16286.3115\n",
      "Training Epoch: 2 [2280/49669]\tLoss: 17746.9277\n",
      "Training Epoch: 2 [2300/49669]\tLoss: 19257.1484\n",
      "Training Epoch: 2 [2320/49669]\tLoss: 14609.0576\n",
      "Training Epoch: 2 [2340/49669]\tLoss: 15975.7861\n",
      "Training Epoch: 2 [2360/49669]\tLoss: 16370.3535\n",
      "Training Epoch: 2 [2380/49669]\tLoss: 18884.9570\n",
      "Training Epoch: 2 [2400/49669]\tLoss: 21209.1484\n",
      "Training Epoch: 2 [2420/49669]\tLoss: 19239.0156\n",
      "Training Epoch: 2 [2440/49669]\tLoss: 16431.3184\n",
      "Training Epoch: 2 [2460/49669]\tLoss: 16415.3848\n",
      "Training Epoch: 2 [2480/49669]\tLoss: 18213.3613\n",
      "Training Epoch: 2 [2500/49669]\tLoss: 17739.4141\n",
      "Training Epoch: 2 [2520/49669]\tLoss: 18411.6699\n",
      "Training Epoch: 2 [2540/49669]\tLoss: 19787.0762\n",
      "Training Epoch: 2 [2560/49669]\tLoss: 16488.4160\n",
      "Training Epoch: 2 [2580/49669]\tLoss: 17371.2051\n",
      "Training Epoch: 2 [2600/49669]\tLoss: 20881.9648\n",
      "Training Epoch: 2 [2620/49669]\tLoss: 18962.9160\n",
      "Training Epoch: 2 [2640/49669]\tLoss: 19077.7188\n",
      "Training Epoch: 2 [2660/49669]\tLoss: 15261.0107\n",
      "Training Epoch: 2 [2680/49669]\tLoss: 15068.3008\n",
      "Training Epoch: 2 [2700/49669]\tLoss: 16967.1992\n",
      "Training Epoch: 2 [2720/49669]\tLoss: 15130.9307\n",
      "Training Epoch: 2 [2740/49669]\tLoss: 16464.2520\n",
      "Training Epoch: 2 [2760/49669]\tLoss: 22300.5859\n",
      "Training Epoch: 2 [2780/49669]\tLoss: 18010.8672\n",
      "Training Epoch: 2 [2800/49669]\tLoss: 19173.2793\n",
      "Training Epoch: 2 [2820/49669]\tLoss: 20254.2715\n",
      "Training Epoch: 2 [2840/49669]\tLoss: 15652.2871\n",
      "Training Epoch: 2 [2860/49669]\tLoss: 17928.7754\n",
      "Training Epoch: 2 [2880/49669]\tLoss: 15231.9463\n",
      "Training Epoch: 2 [2900/49669]\tLoss: 13112.7979\n",
      "Training Epoch: 2 [2920/49669]\tLoss: 16366.8672\n",
      "Training Epoch: 2 [2940/49669]\tLoss: 16885.9844\n",
      "Training Epoch: 2 [2960/49669]\tLoss: 17342.8633\n",
      "Training Epoch: 2 [2980/49669]\tLoss: 20591.0879\n",
      "Training Epoch: 2 [3000/49669]\tLoss: 19236.2617\n",
      "Training Epoch: 2 [3020/49669]\tLoss: 17254.8438\n",
      "Training Epoch: 2 [3040/49669]\tLoss: 15197.2725\n",
      "Training Epoch: 2 [3060/49669]\tLoss: 15989.9766\n",
      "Training Epoch: 2 [3080/49669]\tLoss: 17801.2598\n",
      "Training Epoch: 2 [3100/49669]\tLoss: 17549.1504\n",
      "Training Epoch: 2 [3120/49669]\tLoss: 18229.4004\n",
      "Training Epoch: 2 [3140/49669]\tLoss: 16284.0430\n",
      "Training Epoch: 2 [3160/49669]\tLoss: 18542.1367\n",
      "Training Epoch: 2 [3180/49669]\tLoss: 15770.8018\n",
      "Training Epoch: 2 [3200/49669]\tLoss: 17009.5586\n",
      "Training Epoch: 2 [3220/49669]\tLoss: 15286.4844\n",
      "Training Epoch: 2 [3240/49669]\tLoss: 18168.2480\n",
      "Training Epoch: 2 [3260/49669]\tLoss: 16072.7695\n",
      "Training Epoch: 2 [3280/49669]\tLoss: 12972.5654\n",
      "Training Epoch: 2 [3300/49669]\tLoss: 17699.4590\n",
      "Training Epoch: 2 [3320/49669]\tLoss: 14939.8496\n",
      "Training Epoch: 2 [3340/49669]\tLoss: 18389.2559\n",
      "Training Epoch: 2 [3360/49669]\tLoss: 17267.8848\n",
      "Training Epoch: 2 [3380/49669]\tLoss: 14098.2422\n",
      "Training Epoch: 2 [3400/49669]\tLoss: 19966.7656\n",
      "Training Epoch: 2 [3420/49669]\tLoss: 18164.5078\n",
      "Training Epoch: 2 [3440/49669]\tLoss: 14502.7363\n",
      "Training Epoch: 2 [3460/49669]\tLoss: 12799.5078\n",
      "Training Epoch: 2 [3480/49669]\tLoss: 15737.4824\n",
      "Training Epoch: 2 [3500/49669]\tLoss: 15253.6338\n",
      "Training Epoch: 2 [3520/49669]\tLoss: 18786.6191\n",
      "Training Epoch: 2 [3540/49669]\tLoss: 14882.5439\n",
      "Training Epoch: 2 [3560/49669]\tLoss: 17120.3809\n",
      "Training Epoch: 2 [3580/49669]\tLoss: 17583.0527\n",
      "Training Epoch: 2 [3600/49669]\tLoss: 16605.3340\n",
      "Training Epoch: 2 [3620/49669]\tLoss: 17886.4219\n",
      "Training Epoch: 2 [3640/49669]\tLoss: 18069.8027\n",
      "Training Epoch: 2 [3660/49669]\tLoss: 15790.2832\n",
      "Training Epoch: 2 [3680/49669]\tLoss: 17419.3809\n",
      "Training Epoch: 2 [3700/49669]\tLoss: 14011.4805\n",
      "Training Epoch: 2 [3720/49669]\tLoss: 20347.8340\n",
      "Training Epoch: 2 [3740/49669]\tLoss: 14840.9531\n",
      "Training Epoch: 2 [3760/49669]\tLoss: 18365.8535\n",
      "Training Epoch: 2 [3780/49669]\tLoss: 13977.8262\n",
      "Training Epoch: 2 [3800/49669]\tLoss: 14820.9043\n",
      "Training Epoch: 2 [3820/49669]\tLoss: 16668.2793\n",
      "Training Epoch: 2 [3840/49669]\tLoss: 15217.2812\n",
      "Training Epoch: 2 [3860/49669]\tLoss: 17041.1172\n",
      "Training Epoch: 2 [3880/49669]\tLoss: 16024.1191\n",
      "Training Epoch: 2 [3900/49669]\tLoss: 15965.9395\n",
      "Training Epoch: 2 [3920/49669]\tLoss: 13642.5664\n",
      "Training Epoch: 2 [3940/49669]\tLoss: 14874.0527\n",
      "Training Epoch: 2 [3960/49669]\tLoss: 18041.7012\n",
      "Training Epoch: 2 [3980/49669]\tLoss: 14389.2217\n",
      "Training Epoch: 2 [4000/49669]\tLoss: 15535.7041\n",
      "Training Epoch: 2 [4020/49669]\tLoss: 14696.2168\n",
      "Training Epoch: 2 [4040/49669]\tLoss: 15947.2656\n",
      "Training Epoch: 2 [4060/49669]\tLoss: 12974.4072\n",
      "Training Epoch: 2 [4080/49669]\tLoss: 15948.4111\n",
      "Training Epoch: 2 [4100/49669]\tLoss: 12148.1934\n",
      "Training Epoch: 2 [4120/49669]\tLoss: 16639.3027\n",
      "Training Epoch: 2 [4140/49669]\tLoss: 16858.1426\n",
      "Training Epoch: 2 [4160/49669]\tLoss: 16925.4824\n",
      "Training Epoch: 2 [4180/49669]\tLoss: 14674.7148\n",
      "Training Epoch: 2 [4200/49669]\tLoss: 12893.0469\n",
      "Training Epoch: 2 [4220/49669]\tLoss: 16912.7383\n",
      "Training Epoch: 2 [4240/49669]\tLoss: 17305.6348\n",
      "Training Epoch: 2 [4260/49669]\tLoss: 15403.8721\n",
      "Training Epoch: 2 [4280/49669]\tLoss: 18262.7930\n",
      "Training Epoch: 2 [4300/49669]\tLoss: 14279.8477\n",
      "Training Epoch: 2 [4320/49669]\tLoss: 15780.6104\n",
      "Training Epoch: 2 [4340/49669]\tLoss: 15547.3564\n",
      "Training Epoch: 2 [4360/49669]\tLoss: 17041.0137\n",
      "Training Epoch: 2 [4380/49669]\tLoss: 16429.3516\n",
      "Training Epoch: 2 [4400/49669]\tLoss: 15265.4229\n",
      "Training Epoch: 2 [4420/49669]\tLoss: 16521.0566\n",
      "Training Epoch: 2 [4440/49669]\tLoss: 14133.5439\n",
      "Training Epoch: 2 [4460/49669]\tLoss: 14821.2842\n",
      "Training Epoch: 2 [4480/49669]\tLoss: 16630.5312\n",
      "Training Epoch: 2 [4500/49669]\tLoss: 19850.0391\n",
      "Training Epoch: 2 [4520/49669]\tLoss: 14788.3271\n",
      "Training Epoch: 2 [4540/49669]\tLoss: 17929.4609\n",
      "Training Epoch: 2 [4560/49669]\tLoss: 16209.8271\n",
      "Training Epoch: 2 [4580/49669]\tLoss: 16830.3164\n",
      "Training Epoch: 2 [4600/49669]\tLoss: 18545.3516\n",
      "Training Epoch: 2 [4620/49669]\tLoss: 15788.3350\n",
      "Training Epoch: 2 [4640/49669]\tLoss: 13802.0732\n",
      "Training Epoch: 2 [4660/49669]\tLoss: 17383.2871\n",
      "Training Epoch: 2 [4680/49669]\tLoss: 16642.6523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [4700/49669]\tLoss: 14226.1826\n",
      "Training Epoch: 2 [4720/49669]\tLoss: 13871.5840\n",
      "Training Epoch: 2 [4740/49669]\tLoss: 14847.4189\n",
      "Training Epoch: 2 [4760/49669]\tLoss: 14935.9365\n",
      "Training Epoch: 2 [4780/49669]\tLoss: 15274.3574\n",
      "Training Epoch: 2 [4800/49669]\tLoss: 16307.5869\n",
      "Training Epoch: 2 [4820/49669]\tLoss: 13535.1182\n",
      "Training Epoch: 2 [4840/49669]\tLoss: 13258.0273\n",
      "Training Epoch: 2 [4860/49669]\tLoss: 16722.4023\n",
      "Training Epoch: 2 [4880/49669]\tLoss: 16087.0352\n",
      "Training Epoch: 2 [4900/49669]\tLoss: 14624.4648\n",
      "Training Epoch: 2 [4920/49669]\tLoss: 15294.5986\n",
      "Training Epoch: 2 [4940/49669]\tLoss: 16428.2832\n",
      "Training Epoch: 2 [4960/49669]\tLoss: 15052.9912\n",
      "Training Epoch: 2 [4980/49669]\tLoss: 15456.6113\n",
      "Training Epoch: 2 [5000/49669]\tLoss: 18283.9707\n",
      "Training Epoch: 2 [5020/49669]\tLoss: 13327.1582\n",
      "Training Epoch: 2 [5040/49669]\tLoss: 14685.5059\n",
      "Training Epoch: 2 [5060/49669]\tLoss: 13097.8311\n",
      "Training Epoch: 2 [5080/49669]\tLoss: 16347.3174\n",
      "Training Epoch: 2 [5100/49669]\tLoss: 15269.4033\n",
      "Training Epoch: 2 [5120/49669]\tLoss: 14790.6123\n",
      "Training Epoch: 2 [5140/49669]\tLoss: 13919.3564\n",
      "Training Epoch: 2 [5160/49669]\tLoss: 14635.2832\n",
      "Training Epoch: 2 [5180/49669]\tLoss: 13845.9082\n",
      "Training Epoch: 2 [5200/49669]\tLoss: 15412.3496\n",
      "Training Epoch: 2 [5220/49669]\tLoss: 13373.7500\n",
      "Training Epoch: 2 [5240/49669]\tLoss: 16163.1182\n",
      "Training Epoch: 2 [5260/49669]\tLoss: 14867.9375\n",
      "Training Epoch: 2 [5280/49669]\tLoss: 13289.2441\n",
      "Training Epoch: 2 [5300/49669]\tLoss: 13713.8330\n",
      "Training Epoch: 2 [5320/49669]\tLoss: 12723.0078\n",
      "Training Epoch: 2 [5340/49669]\tLoss: 16627.5645\n",
      "Training Epoch: 2 [5360/49669]\tLoss: 15330.9434\n",
      "Training Epoch: 2 [5380/49669]\tLoss: 12649.7500\n",
      "Training Epoch: 2 [5400/49669]\tLoss: 16027.2441\n",
      "Training Epoch: 2 [5420/49669]\tLoss: 13754.8955\n",
      "Training Epoch: 2 [5440/49669]\tLoss: 16881.8262\n",
      "Training Epoch: 2 [5460/49669]\tLoss: 12055.4629\n",
      "Training Epoch: 2 [5480/49669]\tLoss: 14964.8535\n",
      "Training Epoch: 2 [5500/49669]\tLoss: 11153.4199\n",
      "Training Epoch: 2 [5520/49669]\tLoss: 16397.5781\n",
      "Training Epoch: 2 [5540/49669]\tLoss: 15327.9922\n",
      "Training Epoch: 2 [5560/49669]\tLoss: 15110.9531\n",
      "Training Epoch: 2 [5580/49669]\tLoss: 15131.2988\n",
      "Training Epoch: 2 [5600/49669]\tLoss: 14848.5156\n",
      "Training Epoch: 2 [5620/49669]\tLoss: 19099.9316\n",
      "Training Epoch: 2 [5640/49669]\tLoss: 13213.1201\n",
      "Training Epoch: 2 [5660/49669]\tLoss: 16025.2012\n",
      "Training Epoch: 2 [5680/49669]\tLoss: 17732.7676\n",
      "Training Epoch: 2 [5700/49669]\tLoss: 16068.1924\n",
      "Training Epoch: 2 [5720/49669]\tLoss: 14064.5889\n",
      "Training Epoch: 2 [5740/49669]\tLoss: 14649.2002\n",
      "Training Epoch: 2 [5760/49669]\tLoss: 16582.6875\n",
      "Training Epoch: 2 [5780/49669]\tLoss: 16432.2246\n",
      "Training Epoch: 2 [5800/49669]\tLoss: 14312.7949\n",
      "Training Epoch: 2 [5820/49669]\tLoss: 13731.6055\n",
      "Training Epoch: 2 [5840/49669]\tLoss: 12973.6621\n",
      "Training Epoch: 2 [5860/49669]\tLoss: 15587.3350\n",
      "Training Epoch: 2 [5880/49669]\tLoss: 17088.8320\n",
      "Training Epoch: 2 [5900/49669]\tLoss: 13425.2188\n",
      "Training Epoch: 2 [5920/49669]\tLoss: 12817.4395\n",
      "Training Epoch: 2 [5940/49669]\tLoss: 14577.0225\n",
      "Training Epoch: 2 [5960/49669]\tLoss: 15710.3477\n",
      "Training Epoch: 2 [5980/49669]\tLoss: 17797.8184\n",
      "Training Epoch: 2 [6000/49669]\tLoss: 14120.5957\n",
      "Training Epoch: 2 [6020/49669]\tLoss: 13844.2598\n",
      "Training Epoch: 2 [6040/49669]\tLoss: 15671.3105\n",
      "Training Epoch: 2 [6060/49669]\tLoss: 19920.5020\n",
      "Training Epoch: 2 [6080/49669]\tLoss: 13420.4531\n",
      "Training Epoch: 2 [6100/49669]\tLoss: 12432.7793\n",
      "Training Epoch: 2 [6120/49669]\tLoss: 15639.5000\n",
      "Training Epoch: 2 [6140/49669]\tLoss: 14913.3994\n",
      "Training Epoch: 2 [6160/49669]\tLoss: 16214.4531\n",
      "Training Epoch: 2 [6180/49669]\tLoss: 15784.8564\n",
      "Training Epoch: 2 [6200/49669]\tLoss: 14281.7588\n",
      "Training Epoch: 2 [6220/49669]\tLoss: 15379.9121\n",
      "Training Epoch: 2 [6240/49669]\tLoss: 15991.8340\n",
      "Training Epoch: 2 [6260/49669]\tLoss: 13777.7930\n",
      "Training Epoch: 2 [6280/49669]\tLoss: 13579.6201\n",
      "Training Epoch: 2 [6300/49669]\tLoss: 13665.7988\n",
      "Training Epoch: 2 [6320/49669]\tLoss: 11380.8281\n",
      "Training Epoch: 2 [6340/49669]\tLoss: 15620.1318\n",
      "Training Epoch: 2 [6360/49669]\tLoss: 14941.8906\n",
      "Training Epoch: 2 [6380/49669]\tLoss: 14134.3408\n",
      "Training Epoch: 2 [6400/49669]\tLoss: 11425.6943\n",
      "Training Epoch: 2 [6420/49669]\tLoss: 19221.0547\n",
      "Training Epoch: 2 [6440/49669]\tLoss: 14717.2627\n",
      "Training Epoch: 2 [6460/49669]\tLoss: 15911.7949\n",
      "Training Epoch: 2 [6480/49669]\tLoss: 12451.7275\n",
      "Training Epoch: 2 [6500/49669]\tLoss: 18298.3125\n",
      "Training Epoch: 2 [6520/49669]\tLoss: 15205.1455\n",
      "Training Epoch: 2 [6540/49669]\tLoss: 10433.5654\n",
      "Training Epoch: 2 [6560/49669]\tLoss: 11509.2305\n",
      "Training Epoch: 2 [6580/49669]\tLoss: 13873.8340\n",
      "Training Epoch: 2 [6600/49669]\tLoss: 14053.9766\n",
      "Training Epoch: 2 [6620/49669]\tLoss: 13936.0625\n",
      "Training Epoch: 2 [6640/49669]\tLoss: 13128.8291\n",
      "Training Epoch: 2 [6660/49669]\tLoss: 15147.2002\n",
      "Training Epoch: 2 [6680/49669]\tLoss: 14417.6025\n",
      "Training Epoch: 2 [6700/49669]\tLoss: 14956.8691\n",
      "Training Epoch: 2 [6720/49669]\tLoss: 11253.4131\n",
      "Training Epoch: 2 [6740/49669]\tLoss: 12775.8672\n",
      "Training Epoch: 2 [6760/49669]\tLoss: 12714.6035\n",
      "Training Epoch: 2 [6780/49669]\tLoss: 15094.8027\n",
      "Training Epoch: 2 [6800/49669]\tLoss: 15166.8125\n",
      "Training Epoch: 2 [6820/49669]\tLoss: 14313.3936\n",
      "Training Epoch: 2 [6840/49669]\tLoss: 14372.2588\n",
      "Training Epoch: 2 [6860/49669]\tLoss: 12729.1123\n",
      "Training Epoch: 2 [6880/49669]\tLoss: 13233.1016\n",
      "Training Epoch: 2 [6900/49669]\tLoss: 11676.4551\n",
      "Training Epoch: 2 [6920/49669]\tLoss: 13883.7236\n",
      "Training Epoch: 2 [6940/49669]\tLoss: 12726.2363\n",
      "Training Epoch: 2 [6960/49669]\tLoss: 13130.4521\n",
      "Training Epoch: 2 [6980/49669]\tLoss: 10783.5527\n",
      "Training Epoch: 2 [7000/49669]\tLoss: 12476.9238\n",
      "Training Epoch: 2 [7020/49669]\tLoss: 13982.0254\n",
      "Training Epoch: 2 [7040/49669]\tLoss: 13606.8232\n",
      "Training Epoch: 2 [7060/49669]\tLoss: 16056.0342\n",
      "Training Epoch: 2 [7080/49669]\tLoss: 14042.3457\n",
      "Training Epoch: 2 [7100/49669]\tLoss: 14555.6387\n",
      "Training Epoch: 2 [7120/49669]\tLoss: 13213.8047\n",
      "Training Epoch: 2 [7140/49669]\tLoss: 12751.4492\n",
      "Training Epoch: 2 [7160/49669]\tLoss: 13046.8291\n",
      "Training Epoch: 2 [7180/49669]\tLoss: 16512.1797\n",
      "Training Epoch: 2 [7200/49669]\tLoss: 13766.1016\n",
      "Training Epoch: 2 [7220/49669]\tLoss: 11639.2021\n",
      "Training Epoch: 2 [7240/49669]\tLoss: 14048.7021\n",
      "Training Epoch: 2 [7260/49669]\tLoss: 13676.5918\n",
      "Training Epoch: 2 [7280/49669]\tLoss: 13861.0762\n",
      "Training Epoch: 2 [7300/49669]\tLoss: 11690.0684\n",
      "Training Epoch: 2 [7320/49669]\tLoss: 15832.9893\n",
      "Training Epoch: 2 [7340/49669]\tLoss: 14366.9580\n",
      "Training Epoch: 2 [7360/49669]\tLoss: 13543.1689\n",
      "Training Epoch: 2 [7380/49669]\tLoss: 12124.3076\n",
      "Training Epoch: 2 [7400/49669]\tLoss: 12816.5547\n",
      "Training Epoch: 2 [7420/49669]\tLoss: 17983.6328\n",
      "Training Epoch: 2 [7440/49669]\tLoss: 11382.6943\n",
      "Training Epoch: 2 [7460/49669]\tLoss: 13075.7217\n",
      "Training Epoch: 2 [7480/49669]\tLoss: 12186.9922\n",
      "Training Epoch: 2 [7500/49669]\tLoss: 11576.4600\n",
      "Training Epoch: 2 [7520/49669]\tLoss: 13684.5020\n",
      "Training Epoch: 2 [7540/49669]\tLoss: 12079.1113\n",
      "Training Epoch: 2 [7560/49669]\tLoss: 13047.8193\n",
      "Training Epoch: 2 [7580/49669]\tLoss: 13827.2920\n",
      "Training Epoch: 2 [7600/49669]\tLoss: 13432.4336\n",
      "Training Epoch: 2 [7620/49669]\tLoss: 11650.4502\n",
      "Training Epoch: 2 [7640/49669]\tLoss: 16881.5137\n",
      "Training Epoch: 2 [7660/49669]\tLoss: 13084.9775\n",
      "Training Epoch: 2 [7680/49669]\tLoss: 12391.2881\n",
      "Training Epoch: 2 [7700/49669]\tLoss: 14854.8154\n",
      "Training Epoch: 2 [7720/49669]\tLoss: 15998.5889\n",
      "Training Epoch: 2 [7740/49669]\tLoss: 13543.2188\n",
      "Training Epoch: 2 [7760/49669]\tLoss: 13995.0156\n",
      "Training Epoch: 2 [7780/49669]\tLoss: 12618.1670\n",
      "Training Epoch: 2 [7800/49669]\tLoss: 13338.3301\n",
      "Training Epoch: 2 [7820/49669]\tLoss: 13525.2646\n",
      "Training Epoch: 2 [7840/49669]\tLoss: 12362.3730\n",
      "Training Epoch: 2 [7860/49669]\tLoss: 12609.5488\n",
      "Training Epoch: 2 [7880/49669]\tLoss: 13050.5273\n",
      "Training Epoch: 2 [7900/49669]\tLoss: 15745.9424\n",
      "Training Epoch: 2 [7920/49669]\tLoss: 12844.2656\n",
      "Training Epoch: 2 [7940/49669]\tLoss: 12146.6240\n",
      "Training Epoch: 2 [7960/49669]\tLoss: 11084.2588\n",
      "Training Epoch: 2 [7980/49669]\tLoss: 12602.0186\n",
      "Training Epoch: 2 [8000/49669]\tLoss: 14216.9287\n",
      "Training Epoch: 2 [8020/49669]\tLoss: 11234.9395\n",
      "Training Epoch: 2 [8040/49669]\tLoss: 11560.3770\n",
      "Training Epoch: 2 [8060/49669]\tLoss: 11398.6162\n",
      "Training Epoch: 2 [8080/49669]\tLoss: 13731.4248\n",
      "Training Epoch: 2 [8100/49669]\tLoss: 15106.2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [8120/49669]\tLoss: 13946.2119\n",
      "Training Epoch: 2 [8140/49669]\tLoss: 13075.7705\n",
      "Training Epoch: 2 [8160/49669]\tLoss: 11168.3232\n",
      "Training Epoch: 2 [8180/49669]\tLoss: 11679.5625\n",
      "Training Epoch: 2 [8200/49669]\tLoss: 13882.0762\n",
      "Training Epoch: 2 [8220/49669]\tLoss: 11752.8799\n",
      "Training Epoch: 2 [8240/49669]\tLoss: 14180.4971\n",
      "Training Epoch: 2 [8260/49669]\tLoss: 15244.7441\n",
      "Training Epoch: 2 [8280/49669]\tLoss: 13354.1045\n",
      "Training Epoch: 2 [8300/49669]\tLoss: 12543.8301\n",
      "Training Epoch: 2 [8320/49669]\tLoss: 12618.2412\n",
      "Training Epoch: 2 [8340/49669]\tLoss: 15537.2695\n",
      "Training Epoch: 2 [8360/49669]\tLoss: 12898.4160\n",
      "Training Epoch: 2 [8380/49669]\tLoss: 13486.1211\n",
      "Training Epoch: 2 [8400/49669]\tLoss: 13699.4141\n",
      "Training Epoch: 2 [8420/49669]\tLoss: 14066.6689\n",
      "Training Epoch: 2 [8440/49669]\tLoss: 14671.7256\n",
      "Training Epoch: 2 [8460/49669]\tLoss: 13470.9512\n",
      "Training Epoch: 2 [8480/49669]\tLoss: 12906.1621\n",
      "Training Epoch: 2 [8500/49669]\tLoss: 14019.1982\n",
      "Training Epoch: 2 [8520/49669]\tLoss: 12227.8438\n",
      "Training Epoch: 2 [8540/49669]\tLoss: 11716.5410\n",
      "Training Epoch: 2 [8560/49669]\tLoss: 13641.8184\n",
      "Training Epoch: 2 [8580/49669]\tLoss: 13953.2744\n",
      "Training Epoch: 2 [8600/49669]\tLoss: 11225.7705\n",
      "Training Epoch: 2 [8620/49669]\tLoss: 14393.1152\n",
      "Training Epoch: 2 [8640/49669]\tLoss: 10520.9971\n",
      "Training Epoch: 2 [8660/49669]\tLoss: 15239.8105\n",
      "Training Epoch: 2 [8680/49669]\tLoss: 14070.0205\n",
      "Training Epoch: 2 [8700/49669]\tLoss: 11714.6074\n",
      "Training Epoch: 2 [8720/49669]\tLoss: 14476.5771\n",
      "Training Epoch: 2 [8740/49669]\tLoss: 11346.2627\n",
      "Training Epoch: 2 [8760/49669]\tLoss: 12294.9912\n",
      "Training Epoch: 2 [8780/49669]\tLoss: 12490.6621\n",
      "Training Epoch: 2 [8800/49669]\tLoss: 12867.9668\n",
      "Training Epoch: 2 [8820/49669]\tLoss: 11873.4395\n",
      "Training Epoch: 2 [8840/49669]\tLoss: 14133.8916\n",
      "Training Epoch: 2 [8860/49669]\tLoss: 11091.8896\n",
      "Training Epoch: 2 [8880/49669]\tLoss: 12754.7041\n",
      "Training Epoch: 2 [8900/49669]\tLoss: 13664.8584\n",
      "Training Epoch: 2 [8920/49669]\tLoss: 15606.1689\n",
      "Training Epoch: 2 [8940/49669]\tLoss: 13873.8760\n",
      "Training Epoch: 2 [8960/49669]\tLoss: 13345.3154\n",
      "Training Epoch: 2 [8980/49669]\tLoss: 10137.8906\n",
      "Training Epoch: 2 [9000/49669]\tLoss: 12303.2891\n",
      "Training Epoch: 2 [9020/49669]\tLoss: 13308.0801\n",
      "Training Epoch: 2 [9040/49669]\tLoss: 12361.8721\n",
      "Training Epoch: 2 [9060/49669]\tLoss: 10384.7393\n",
      "Training Epoch: 2 [9080/49669]\tLoss: 11130.3945\n",
      "Training Epoch: 2 [9100/49669]\tLoss: 12875.9492\n",
      "Training Epoch: 2 [9120/49669]\tLoss: 12570.9834\n",
      "Training Epoch: 2 [9140/49669]\tLoss: 11112.2559\n",
      "Training Epoch: 2 [9160/49669]\tLoss: 13829.5684\n",
      "Training Epoch: 2 [9180/49669]\tLoss: 13027.5391\n",
      "Training Epoch: 2 [9200/49669]\tLoss: 12566.6016\n",
      "Training Epoch: 2 [9220/49669]\tLoss: 12332.0898\n",
      "Training Epoch: 2 [9240/49669]\tLoss: 12974.4561\n",
      "Training Epoch: 2 [9260/49669]\tLoss: 11186.0889\n",
      "Training Epoch: 2 [9280/49669]\tLoss: 11506.4834\n",
      "Training Epoch: 2 [9300/49669]\tLoss: 14240.7744\n",
      "Training Epoch: 2 [9320/49669]\tLoss: 11249.7832\n",
      "Training Epoch: 2 [9340/49669]\tLoss: 12032.2363\n",
      "Training Epoch: 2 [9360/49669]\tLoss: 12680.7041\n",
      "Training Epoch: 2 [9380/49669]\tLoss: 11609.9258\n",
      "Training Epoch: 2 [9400/49669]\tLoss: 11824.3613\n",
      "Training Epoch: 2 [9420/49669]\tLoss: 11303.3262\n",
      "Training Epoch: 2 [9440/49669]\tLoss: 11767.3408\n",
      "Training Epoch: 2 [9460/49669]\tLoss: 13553.6133\n",
      "Training Epoch: 2 [9480/49669]\tLoss: 12424.4248\n",
      "Training Epoch: 2 [9500/49669]\tLoss: 12100.8975\n",
      "Training Epoch: 2 [9520/49669]\tLoss: 14039.1543\n",
      "Training Epoch: 2 [9540/49669]\tLoss: 10948.6445\n",
      "Training Epoch: 2 [9560/49669]\tLoss: 10572.3711\n",
      "Training Epoch: 2 [9580/49669]\tLoss: 15095.2119\n",
      "Training Epoch: 2 [9600/49669]\tLoss: 11082.6748\n",
      "Training Epoch: 2 [9620/49669]\tLoss: 12242.8584\n",
      "Training Epoch: 2 [9640/49669]\tLoss: 12617.8984\n",
      "Training Epoch: 2 [9660/49669]\tLoss: 12679.4697\n",
      "Training Epoch: 2 [9680/49669]\tLoss: 12245.7598\n",
      "Training Epoch: 2 [9700/49669]\tLoss: 11258.4336\n",
      "Training Epoch: 2 [9720/49669]\tLoss: 11541.1992\n",
      "Training Epoch: 2 [9740/49669]\tLoss: 11845.5615\n",
      "Training Epoch: 2 [9760/49669]\tLoss: 12111.9902\n",
      "Training Epoch: 2 [9780/49669]\tLoss: 12725.2979\n",
      "Training Epoch: 2 [9800/49669]\tLoss: 11656.1152\n",
      "Training Epoch: 2 [9820/49669]\tLoss: 15155.0596\n",
      "Training Epoch: 2 [9840/49669]\tLoss: 16307.1445\n",
      "Training Epoch: 2 [9860/49669]\tLoss: 12462.5176\n",
      "Training Epoch: 2 [9880/49669]\tLoss: 11913.5967\n",
      "Training Epoch: 2 [9900/49669]\tLoss: 13878.3174\n",
      "Training Epoch: 2 [9920/49669]\tLoss: 13846.6357\n",
      "Training Epoch: 2 [9940/49669]\tLoss: 12122.0547\n",
      "Training Epoch: 2 [9960/49669]\tLoss: 11088.9375\n",
      "Training Epoch: 2 [9980/49669]\tLoss: 12830.4688\n",
      "Training Epoch: 2 [10000/49669]\tLoss: 11020.1758\n",
      "Training Epoch: 2 [10020/49669]\tLoss: 11552.1113\n",
      "Training Epoch: 2 [10040/49669]\tLoss: 12580.8926\n",
      "Training Epoch: 2 [10060/49669]\tLoss: 10436.6455\n",
      "Training Epoch: 2 [10080/49669]\tLoss: 11916.3555\n",
      "Training Epoch: 2 [10100/49669]\tLoss: 13432.3203\n",
      "Training Epoch: 2 [10120/49669]\tLoss: 12955.5850\n",
      "Training Epoch: 2 [10140/49669]\tLoss: 11341.1768\n",
      "Training Epoch: 2 [10160/49669]\tLoss: 11933.8438\n",
      "Training Epoch: 2 [10180/49669]\tLoss: 12984.7676\n",
      "Training Epoch: 2 [10200/49669]\tLoss: 10547.5723\n",
      "Training Epoch: 2 [10220/49669]\tLoss: 13135.0322\n",
      "Training Epoch: 2 [10240/49669]\tLoss: 11259.5293\n",
      "Training Epoch: 2 [10260/49669]\tLoss: 12380.0605\n",
      "Training Epoch: 2 [10280/49669]\tLoss: 12594.2803\n",
      "Training Epoch: 2 [10300/49669]\tLoss: 11570.3984\n",
      "Training Epoch: 2 [10320/49669]\tLoss: 10204.7197\n",
      "Training Epoch: 2 [10340/49669]\tLoss: 11133.1494\n",
      "Training Epoch: 2 [10360/49669]\tLoss: 11497.8096\n",
      "Training Epoch: 2 [10380/49669]\tLoss: 11733.7568\n",
      "Training Epoch: 2 [10400/49669]\tLoss: 12694.3672\n",
      "Training Epoch: 2 [10420/49669]\tLoss: 10741.5723\n",
      "Training Epoch: 2 [10440/49669]\tLoss: 12396.5840\n",
      "Training Epoch: 2 [10460/49669]\tLoss: 12654.6523\n",
      "Training Epoch: 2 [10480/49669]\tLoss: 13572.9199\n",
      "Training Epoch: 2 [10500/49669]\tLoss: 10230.2930\n",
      "Training Epoch: 2 [10520/49669]\tLoss: 14752.4092\n",
      "Training Epoch: 2 [10540/49669]\tLoss: 9014.7236\n",
      "Training Epoch: 2 [10560/49669]\tLoss: 13411.2324\n",
      "Training Epoch: 2 [10580/49669]\tLoss: 12238.0420\n",
      "Training Epoch: 2 [10600/49669]\tLoss: 9759.2744\n",
      "Training Epoch: 2 [10620/49669]\tLoss: 11767.9639\n",
      "Training Epoch: 2 [10640/49669]\tLoss: 12461.4883\n",
      "Training Epoch: 2 [10660/49669]\tLoss: 11010.6729\n",
      "Training Epoch: 2 [10680/49669]\tLoss: 10995.6455\n",
      "Training Epoch: 2 [10700/49669]\tLoss: 13012.3906\n",
      "Training Epoch: 2 [10720/49669]\tLoss: 10484.1348\n",
      "Training Epoch: 2 [10740/49669]\tLoss: 11628.3047\n",
      "Training Epoch: 2 [10760/49669]\tLoss: 10481.3252\n",
      "Training Epoch: 2 [10780/49669]\tLoss: 10747.7002\n",
      "Training Epoch: 2 [10800/49669]\tLoss: 11927.0557\n",
      "Training Epoch: 2 [10820/49669]\tLoss: 11358.1064\n",
      "Training Epoch: 2 [10840/49669]\tLoss: 12190.8496\n",
      "Training Epoch: 2 [10860/49669]\tLoss: 10766.8320\n",
      "Training Epoch: 2 [10880/49669]\tLoss: 13302.2549\n",
      "Training Epoch: 2 [10900/49669]\tLoss: 10311.8760\n",
      "Training Epoch: 2 [10920/49669]\tLoss: 11743.7441\n",
      "Training Epoch: 2 [10940/49669]\tLoss: 10246.5488\n",
      "Training Epoch: 2 [10960/49669]\tLoss: 11277.8643\n",
      "Training Epoch: 2 [10980/49669]\tLoss: 9871.0137\n",
      "Training Epoch: 2 [11000/49669]\tLoss: 9316.1816\n",
      "Training Epoch: 2 [11020/49669]\tLoss: 8838.0283\n",
      "Training Epoch: 2 [11040/49669]\tLoss: 11615.9023\n",
      "Training Epoch: 2 [11060/49669]\tLoss: 11219.9492\n",
      "Training Epoch: 2 [11080/49669]\tLoss: 11852.5322\n",
      "Training Epoch: 2 [11100/49669]\tLoss: 11100.8955\n",
      "Training Epoch: 2 [11120/49669]\tLoss: 10617.8262\n",
      "Training Epoch: 2 [11140/49669]\tLoss: 11447.9121\n",
      "Training Epoch: 2 [11160/49669]\tLoss: 9952.9346\n",
      "Training Epoch: 2 [11180/49669]\tLoss: 11163.3555\n",
      "Training Epoch: 2 [11200/49669]\tLoss: 12204.6826\n",
      "Training Epoch: 2 [11220/49669]\tLoss: 10953.3018\n",
      "Training Epoch: 2 [11240/49669]\tLoss: 9359.6328\n",
      "Training Epoch: 2 [11260/49669]\tLoss: 9817.0801\n",
      "Training Epoch: 2 [11280/49669]\tLoss: 12125.2412\n",
      "Training Epoch: 2 [11300/49669]\tLoss: 10140.2607\n",
      "Training Epoch: 2 [11320/49669]\tLoss: 12192.4062\n",
      "Training Epoch: 2 [11340/49669]\tLoss: 11272.6992\n",
      "Training Epoch: 2 [11360/49669]\tLoss: 10961.4570\n",
      "Training Epoch: 2 [11380/49669]\tLoss: 11558.2021\n",
      "Training Epoch: 2 [11400/49669]\tLoss: 13098.6416\n",
      "Training Epoch: 2 [11420/49669]\tLoss: 9322.7666\n",
      "Training Epoch: 2 [11440/49669]\tLoss: 12956.4297\n",
      "Training Epoch: 2 [11460/49669]\tLoss: 10404.4248\n",
      "Training Epoch: 2 [11480/49669]\tLoss: 11285.3271\n",
      "Training Epoch: 2 [11500/49669]\tLoss: 11289.1904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [11520/49669]\tLoss: 12434.8457\n",
      "Training Epoch: 2 [11540/49669]\tLoss: 12507.6143\n",
      "Training Epoch: 2 [11560/49669]\tLoss: 11585.3369\n",
      "Training Epoch: 2 [11580/49669]\tLoss: 10951.1143\n",
      "Training Epoch: 2 [11600/49669]\tLoss: 9616.6426\n",
      "Training Epoch: 2 [11620/49669]\tLoss: 11114.6582\n",
      "Training Epoch: 2 [11640/49669]\tLoss: 11280.7783\n",
      "Training Epoch: 2 [11660/49669]\tLoss: 10721.0859\n",
      "Training Epoch: 2 [11680/49669]\tLoss: 12355.8906\n",
      "Training Epoch: 2 [11700/49669]\tLoss: 11559.5088\n",
      "Training Epoch: 2 [11720/49669]\tLoss: 10641.1533\n",
      "Training Epoch: 2 [11740/49669]\tLoss: 10918.6367\n",
      "Training Epoch: 2 [11760/49669]\tLoss: 9132.7607\n",
      "Training Epoch: 2 [11780/49669]\tLoss: 11234.6299\n",
      "Training Epoch: 2 [11800/49669]\tLoss: 10915.9121\n",
      "Training Epoch: 2 [11820/49669]\tLoss: 9664.9893\n",
      "Training Epoch: 2 [11840/49669]\tLoss: 10941.4219\n",
      "Training Epoch: 2 [11860/49669]\tLoss: 12096.6416\n",
      "Training Epoch: 2 [11880/49669]\tLoss: 11396.3770\n",
      "Training Epoch: 2 [11900/49669]\tLoss: 11421.4238\n",
      "Training Epoch: 2 [11920/49669]\tLoss: 11143.1816\n",
      "Training Epoch: 2 [11940/49669]\tLoss: 12004.4600\n",
      "Training Epoch: 2 [11960/49669]\tLoss: 9739.5244\n",
      "Training Epoch: 2 [11980/49669]\tLoss: 12928.4482\n",
      "Training Epoch: 2 [12000/49669]\tLoss: 10482.0352\n",
      "Training Epoch: 2 [12020/49669]\tLoss: 10665.0547\n",
      "Training Epoch: 2 [12040/49669]\tLoss: 10488.3154\n",
      "Training Epoch: 2 [12060/49669]\tLoss: 9585.2920\n",
      "Training Epoch: 2 [12080/49669]\tLoss: 12332.2373\n",
      "Training Epoch: 2 [12100/49669]\tLoss: 10526.7051\n",
      "Training Epoch: 2 [12120/49669]\tLoss: 10862.4111\n",
      "Training Epoch: 2 [12140/49669]\tLoss: 12810.4980\n",
      "Training Epoch: 2 [12160/49669]\tLoss: 8854.5791\n",
      "Training Epoch: 2 [12180/49669]\tLoss: 10875.1943\n",
      "Training Epoch: 2 [12200/49669]\tLoss: 11369.2715\n",
      "Training Epoch: 2 [12220/49669]\tLoss: 12124.0732\n",
      "Training Epoch: 2 [12240/49669]\tLoss: 10459.3184\n",
      "Training Epoch: 2 [12260/49669]\tLoss: 10015.5371\n",
      "Training Epoch: 2 [12280/49669]\tLoss: 11966.0400\n",
      "Training Epoch: 2 [12300/49669]\tLoss: 11565.7734\n",
      "Training Epoch: 2 [12320/49669]\tLoss: 10387.9805\n",
      "Training Epoch: 2 [12340/49669]\tLoss: 10280.7432\n",
      "Training Epoch: 2 [12360/49669]\tLoss: 11173.1650\n",
      "Training Epoch: 2 [12380/49669]\tLoss: 10507.3164\n",
      "Training Epoch: 2 [12400/49669]\tLoss: 9943.3926\n",
      "Training Epoch: 2 [12420/49669]\tLoss: 10915.3262\n",
      "Training Epoch: 2 [12440/49669]\tLoss: 11004.1729\n",
      "Training Epoch: 2 [12460/49669]\tLoss: 10212.0146\n",
      "Training Epoch: 2 [12480/49669]\tLoss: 11677.9180\n",
      "Training Epoch: 2 [12500/49669]\tLoss: 10997.8262\n",
      "Training Epoch: 2 [12520/49669]\tLoss: 11537.0723\n",
      "Training Epoch: 2 [12540/49669]\tLoss: 10827.0820\n",
      "Training Epoch: 2 [12560/49669]\tLoss: 12256.1543\n",
      "Training Epoch: 2 [12580/49669]\tLoss: 9281.8955\n",
      "Training Epoch: 2 [12600/49669]\tLoss: 11256.8477\n",
      "Training Epoch: 2 [12620/49669]\tLoss: 10008.0557\n",
      "Training Epoch: 2 [12640/49669]\tLoss: 11414.2227\n",
      "Training Epoch: 2 [12660/49669]\tLoss: 11920.8340\n",
      "Training Epoch: 2 [12680/49669]\tLoss: 9872.3027\n",
      "Training Epoch: 2 [12700/49669]\tLoss: 8455.8008\n",
      "Training Epoch: 2 [12720/49669]\tLoss: 10003.1416\n",
      "Training Epoch: 2 [12740/49669]\tLoss: 11666.1543\n",
      "Training Epoch: 2 [12760/49669]\tLoss: 9953.0879\n",
      "Training Epoch: 2 [12780/49669]\tLoss: 10409.8164\n",
      "Training Epoch: 2 [12800/49669]\tLoss: 10412.1699\n",
      "Training Epoch: 2 [12820/49669]\tLoss: 11006.0020\n",
      "Training Epoch: 2 [12840/49669]\tLoss: 9190.2773\n",
      "Training Epoch: 2 [12860/49669]\tLoss: 11864.4951\n",
      "Training Epoch: 2 [12880/49669]\tLoss: 12497.1260\n",
      "Training Epoch: 2 [12900/49669]\tLoss: 10647.3906\n",
      "Training Epoch: 2 [12920/49669]\tLoss: 9556.5000\n",
      "Training Epoch: 2 [12940/49669]\tLoss: 9773.5166\n",
      "Training Epoch: 2 [12960/49669]\tLoss: 10861.7637\n",
      "Training Epoch: 2 [12980/49669]\tLoss: 11895.5557\n",
      "Training Epoch: 2 [13000/49669]\tLoss: 9321.9922\n",
      "Training Epoch: 2 [13020/49669]\tLoss: 10285.6455\n",
      "Training Epoch: 2 [13040/49669]\tLoss: 10818.6982\n",
      "Training Epoch: 2 [13060/49669]\tLoss: 10938.5713\n",
      "Training Epoch: 2 [13080/49669]\tLoss: 8335.8818\n",
      "Training Epoch: 2 [13100/49669]\tLoss: 9681.2666\n",
      "Training Epoch: 2 [13120/49669]\tLoss: 11672.9951\n",
      "Training Epoch: 2 [13140/49669]\tLoss: 12072.2803\n",
      "Training Epoch: 2 [13160/49669]\tLoss: 10481.4297\n",
      "Training Epoch: 2 [13180/49669]\tLoss: 9662.1885\n",
      "Training Epoch: 2 [13200/49669]\tLoss: 11475.2783\n",
      "Training Epoch: 2 [13220/49669]\tLoss: 11071.3457\n",
      "Training Epoch: 2 [13240/49669]\tLoss: 7865.5596\n",
      "Training Epoch: 2 [13260/49669]\tLoss: 7807.0127\n",
      "Training Epoch: 2 [13280/49669]\tLoss: 11210.5498\n",
      "Training Epoch: 2 [13300/49669]\tLoss: 12245.2412\n",
      "Training Epoch: 2 [13320/49669]\tLoss: 8904.7969\n",
      "Training Epoch: 2 [13340/49669]\tLoss: 8852.1631\n",
      "Training Epoch: 2 [13360/49669]\tLoss: 9262.3730\n",
      "Training Epoch: 2 [13380/49669]\tLoss: 11059.6250\n",
      "Training Epoch: 2 [13400/49669]\tLoss: 11040.2666\n",
      "Training Epoch: 2 [13420/49669]\tLoss: 11134.3506\n",
      "Training Epoch: 2 [13440/49669]\tLoss: 10821.8125\n",
      "Training Epoch: 2 [13460/49669]\tLoss: 9680.9912\n",
      "Training Epoch: 2 [13480/49669]\tLoss: 10027.1523\n",
      "Training Epoch: 2 [13500/49669]\tLoss: 10560.1621\n",
      "Training Epoch: 2 [13520/49669]\tLoss: 10029.8271\n",
      "Training Epoch: 2 [13540/49669]\tLoss: 9483.4961\n",
      "Training Epoch: 2 [13560/49669]\tLoss: 10602.9678\n",
      "Training Epoch: 2 [13580/49669]\tLoss: 9376.4521\n",
      "Training Epoch: 2 [13600/49669]\tLoss: 9214.6309\n",
      "Training Epoch: 2 [13620/49669]\tLoss: 8619.9951\n",
      "Training Epoch: 2 [13640/49669]\tLoss: 11320.7773\n",
      "Training Epoch: 2 [13660/49669]\tLoss: 9892.3076\n",
      "Training Epoch: 2 [13680/49669]\tLoss: 10592.9268\n",
      "Training Epoch: 2 [13700/49669]\tLoss: 9802.3379\n",
      "Training Epoch: 2 [13720/49669]\tLoss: 11703.7598\n",
      "Training Epoch: 2 [13740/49669]\tLoss: 9439.8711\n",
      "Training Epoch: 2 [13760/49669]\tLoss: 11466.3857\n",
      "Training Epoch: 2 [13780/49669]\tLoss: 7349.0181\n",
      "Training Epoch: 2 [13800/49669]\tLoss: 8984.3877\n",
      "Training Epoch: 2 [13820/49669]\tLoss: 9187.2959\n",
      "Training Epoch: 2 [13840/49669]\tLoss: 9625.4658\n",
      "Training Epoch: 2 [13860/49669]\tLoss: 8926.3936\n",
      "Training Epoch: 2 [13880/49669]\tLoss: 10472.6523\n",
      "Training Epoch: 2 [13900/49669]\tLoss: 10638.0068\n",
      "Training Epoch: 2 [13920/49669]\tLoss: 9548.0762\n",
      "Training Epoch: 2 [13940/49669]\tLoss: 11690.3926\n",
      "Training Epoch: 2 [13960/49669]\tLoss: 8378.6416\n",
      "Training Epoch: 2 [13980/49669]\tLoss: 10312.1318\n",
      "Training Epoch: 2 [14000/49669]\tLoss: 8484.8379\n",
      "Training Epoch: 2 [14020/49669]\tLoss: 8864.5234\n",
      "Training Epoch: 2 [14040/49669]\tLoss: 11308.6777\n",
      "Training Epoch: 2 [14060/49669]\tLoss: 10157.9844\n",
      "Training Epoch: 2 [14080/49669]\tLoss: 10857.8184\n",
      "Training Epoch: 2 [14100/49669]\tLoss: 8737.5557\n",
      "Training Epoch: 2 [14120/49669]\tLoss: 10853.4277\n",
      "Training Epoch: 2 [14140/49669]\tLoss: 8568.4561\n",
      "Training Epoch: 2 [14160/49669]\tLoss: 10751.4404\n",
      "Training Epoch: 2 [14180/49669]\tLoss: 11098.7246\n",
      "Training Epoch: 2 [14200/49669]\tLoss: 8565.2344\n",
      "Training Epoch: 2 [14220/49669]\tLoss: 10299.5381\n",
      "Training Epoch: 2 [14240/49669]\tLoss: 10309.8496\n",
      "Training Epoch: 2 [14260/49669]\tLoss: 10643.9619\n",
      "Training Epoch: 2 [14280/49669]\tLoss: 9153.8838\n",
      "Training Epoch: 2 [14300/49669]\tLoss: 11085.7402\n",
      "Training Epoch: 2 [14320/49669]\tLoss: 11201.6855\n",
      "Training Epoch: 2 [14340/49669]\tLoss: 10593.0225\n",
      "Training Epoch: 2 [14360/49669]\tLoss: 10484.2402\n",
      "Training Epoch: 2 [14380/49669]\tLoss: 11330.2832\n",
      "Training Epoch: 2 [14400/49669]\tLoss: 9926.3867\n",
      "Training Epoch: 2 [14420/49669]\tLoss: 9302.3076\n",
      "Training Epoch: 2 [14440/49669]\tLoss: 11725.4580\n",
      "Training Epoch: 2 [14460/49669]\tLoss: 9009.5693\n",
      "Training Epoch: 2 [14480/49669]\tLoss: 11119.4883\n",
      "Training Epoch: 2 [14500/49669]\tLoss: 8181.5889\n",
      "Training Epoch: 2 [14520/49669]\tLoss: 9843.7158\n",
      "Training Epoch: 2 [14540/49669]\tLoss: 9048.5684\n",
      "Training Epoch: 2 [14560/49669]\tLoss: 9713.4150\n",
      "Training Epoch: 2 [14580/49669]\tLoss: 10312.2812\n",
      "Training Epoch: 2 [14600/49669]\tLoss: 8951.6680\n",
      "Training Epoch: 2 [14620/49669]\tLoss: 8079.9419\n",
      "Training Epoch: 2 [14640/49669]\tLoss: 10597.9102\n",
      "Training Epoch: 2 [14660/49669]\tLoss: 10662.0928\n",
      "Training Epoch: 2 [14680/49669]\tLoss: 8350.4160\n",
      "Training Epoch: 2 [14700/49669]\tLoss: 9361.4941\n",
      "Training Epoch: 2 [14720/49669]\tLoss: 9517.0391\n",
      "Training Epoch: 2 [14740/49669]\tLoss: 11104.1680\n",
      "Training Epoch: 2 [14760/49669]\tLoss: 10014.2188\n",
      "Training Epoch: 2 [14780/49669]\tLoss: 9555.6943\n",
      "Training Epoch: 2 [14800/49669]\tLoss: 9424.8145\n",
      "Training Epoch: 2 [14820/49669]\tLoss: 9196.7070\n",
      "Training Epoch: 2 [14840/49669]\tLoss: 10233.3750\n",
      "Training Epoch: 2 [14860/49669]\tLoss: 8503.2686\n",
      "Training Epoch: 2 [14880/49669]\tLoss: 9166.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [14900/49669]\tLoss: 10349.2197\n",
      "Training Epoch: 2 [14920/49669]\tLoss: 8884.0752\n",
      "Training Epoch: 2 [14940/49669]\tLoss: 11504.4834\n",
      "Training Epoch: 2 [14960/49669]\tLoss: 11477.8018\n",
      "Training Epoch: 2 [14980/49669]\tLoss: 10288.1602\n",
      "Training Epoch: 2 [15000/49669]\tLoss: 10128.4668\n",
      "Training Epoch: 2 [15020/49669]\tLoss: 9547.8975\n",
      "Training Epoch: 2 [15040/49669]\tLoss: 11189.3643\n",
      "Training Epoch: 2 [15060/49669]\tLoss: 9800.6855\n",
      "Training Epoch: 2 [15080/49669]\tLoss: 8857.4668\n",
      "Training Epoch: 2 [15100/49669]\tLoss: 10582.4727\n",
      "Training Epoch: 2 [15120/49669]\tLoss: 8047.5688\n",
      "Training Epoch: 2 [15140/49669]\tLoss: 10128.5576\n",
      "Training Epoch: 2 [15160/49669]\tLoss: 8477.6504\n",
      "Training Epoch: 2 [15180/49669]\tLoss: 10315.9961\n",
      "Training Epoch: 2 [15200/49669]\tLoss: 9386.1699\n",
      "Training Epoch: 2 [15220/49669]\tLoss: 8305.8623\n",
      "Training Epoch: 2 [15240/49669]\tLoss: 9183.3252\n",
      "Training Epoch: 2 [15260/49669]\tLoss: 10415.7988\n",
      "Training Epoch: 2 [15280/49669]\tLoss: 9478.9619\n",
      "Training Epoch: 2 [15300/49669]\tLoss: 9141.8154\n",
      "Training Epoch: 2 [15320/49669]\tLoss: 8656.4629\n",
      "Training Epoch: 2 [15340/49669]\tLoss: 9219.2178\n",
      "Training Epoch: 2 [15360/49669]\tLoss: 10320.6191\n",
      "Training Epoch: 2 [15380/49669]\tLoss: 9400.9590\n",
      "Training Epoch: 2 [15400/49669]\tLoss: 10017.0957\n",
      "Training Epoch: 2 [15420/49669]\tLoss: 8916.1221\n",
      "Training Epoch: 2 [15440/49669]\tLoss: 9974.7354\n",
      "Training Epoch: 2 [15460/49669]\tLoss: 9338.4561\n",
      "Training Epoch: 2 [15480/49669]\tLoss: 9634.1406\n",
      "Training Epoch: 2 [15500/49669]\tLoss: 9090.6328\n",
      "Training Epoch: 2 [15520/49669]\tLoss: 9584.6191\n",
      "Training Epoch: 2 [15540/49669]\tLoss: 8919.4531\n",
      "Training Epoch: 2 [15560/49669]\tLoss: 9926.0469\n",
      "Training Epoch: 2 [15580/49669]\tLoss: 10024.1572\n",
      "Training Epoch: 2 [15600/49669]\tLoss: 10116.5088\n",
      "Training Epoch: 2 [15620/49669]\tLoss: 8951.6025\n",
      "Training Epoch: 2 [15640/49669]\tLoss: 8225.6982\n",
      "Training Epoch: 2 [15660/49669]\tLoss: 7851.7490\n",
      "Training Epoch: 2 [15680/49669]\tLoss: 10178.3584\n",
      "Training Epoch: 2 [15700/49669]\tLoss: 10988.0908\n",
      "Training Epoch: 2 [15720/49669]\tLoss: 8994.8955\n",
      "Training Epoch: 2 [15740/49669]\tLoss: 8500.9414\n",
      "Training Epoch: 2 [15760/49669]\tLoss: 8731.4678\n",
      "Training Epoch: 2 [15780/49669]\tLoss: 7797.9531\n",
      "Training Epoch: 2 [15800/49669]\tLoss: 8098.4463\n",
      "Training Epoch: 2 [15820/49669]\tLoss: 9819.8867\n",
      "Training Epoch: 2 [15840/49669]\tLoss: 9109.1631\n",
      "Training Epoch: 2 [15860/49669]\tLoss: 9828.6904\n",
      "Training Epoch: 2 [15880/49669]\tLoss: 8959.0703\n",
      "Training Epoch: 2 [15900/49669]\tLoss: 8829.9102\n",
      "Training Epoch: 2 [15920/49669]\tLoss: 8918.7637\n",
      "Training Epoch: 2 [15940/49669]\tLoss: 10427.0205\n",
      "Training Epoch: 2 [15960/49669]\tLoss: 9029.9746\n",
      "Training Epoch: 2 [15980/49669]\tLoss: 10755.6602\n",
      "Training Epoch: 2 [16000/49669]\tLoss: 10221.6807\n",
      "Training Epoch: 2 [16020/49669]\tLoss: 7983.8682\n",
      "Training Epoch: 2 [16040/49669]\tLoss: 8617.6738\n",
      "Training Epoch: 2 [16060/49669]\tLoss: 12032.9102\n",
      "Training Epoch: 2 [16080/49669]\tLoss: 10313.6504\n",
      "Training Epoch: 2 [16100/49669]\tLoss: 9986.7891\n",
      "Training Epoch: 2 [16120/49669]\tLoss: 10057.5352\n",
      "Training Epoch: 2 [16140/49669]\tLoss: 10677.8730\n",
      "Training Epoch: 2 [16160/49669]\tLoss: 8683.7930\n",
      "Training Epoch: 2 [16180/49669]\tLoss: 8743.4268\n",
      "Training Epoch: 2 [16200/49669]\tLoss: 9767.5928\n",
      "Training Epoch: 2 [16220/49669]\tLoss: 8750.6201\n",
      "Training Epoch: 2 [16240/49669]\tLoss: 9784.2861\n",
      "Training Epoch: 2 [16260/49669]\tLoss: 9978.9053\n",
      "Training Epoch: 2 [16280/49669]\tLoss: 9477.7246\n",
      "Training Epoch: 2 [16300/49669]\tLoss: 9103.8564\n",
      "Training Epoch: 2 [16320/49669]\tLoss: 9420.2939\n",
      "Training Epoch: 2 [16340/49669]\tLoss: 7282.8906\n",
      "Training Epoch: 2 [16360/49669]\tLoss: 9862.1641\n",
      "Training Epoch: 2 [16380/49669]\tLoss: 8802.8477\n",
      "Training Epoch: 2 [16400/49669]\tLoss: 9156.4922\n",
      "Training Epoch: 2 [16420/49669]\tLoss: 9056.4199\n",
      "Training Epoch: 2 [16440/49669]\tLoss: 8673.9277\n",
      "Training Epoch: 2 [16460/49669]\tLoss: 8775.5146\n",
      "Training Epoch: 2 [16480/49669]\tLoss: 9612.6270\n",
      "Training Epoch: 2 [16500/49669]\tLoss: 9447.9580\n",
      "Training Epoch: 2 [16520/49669]\tLoss: 8942.9180\n",
      "Training Epoch: 2 [16540/49669]\tLoss: 7995.9341\n",
      "Training Epoch: 2 [16560/49669]\tLoss: 8756.9844\n",
      "Training Epoch: 2 [16580/49669]\tLoss: 8678.3857\n",
      "Training Epoch: 2 [16600/49669]\tLoss: 11044.5078\n",
      "Training Epoch: 2 [16620/49669]\tLoss: 9692.1729\n",
      "Training Epoch: 2 [16640/49669]\tLoss: 9505.3887\n",
      "Training Epoch: 2 [16660/49669]\tLoss: 8376.4102\n",
      "Training Epoch: 2 [16680/49669]\tLoss: 9283.7920\n",
      "Training Epoch: 2 [16700/49669]\tLoss: 8696.4287\n",
      "Training Epoch: 2 [16720/49669]\tLoss: 8776.8682\n",
      "Training Epoch: 2 [16740/49669]\tLoss: 9732.0176\n",
      "Training Epoch: 2 [16760/49669]\tLoss: 9568.2656\n",
      "Training Epoch: 2 [16780/49669]\tLoss: 8925.4365\n",
      "Training Epoch: 2 [16800/49669]\tLoss: 11100.3145\n",
      "Training Epoch: 2 [16820/49669]\tLoss: 7546.9150\n",
      "Training Epoch: 2 [16840/49669]\tLoss: 7350.4482\n",
      "Training Epoch: 2 [16860/49669]\tLoss: 8331.4424\n",
      "Training Epoch: 2 [16880/49669]\tLoss: 9459.8438\n",
      "Training Epoch: 2 [16900/49669]\tLoss: 9234.6602\n",
      "Training Epoch: 2 [16920/49669]\tLoss: 10331.7178\n",
      "Training Epoch: 2 [16940/49669]\tLoss: 9317.7383\n",
      "Training Epoch: 2 [16960/49669]\tLoss: 7504.2207\n",
      "Training Epoch: 2 [16980/49669]\tLoss: 8757.4082\n",
      "Training Epoch: 2 [17000/49669]\tLoss: 9948.7539\n",
      "Training Epoch: 2 [17020/49669]\tLoss: 10451.5020\n",
      "Training Epoch: 2 [17040/49669]\tLoss: 9508.5596\n",
      "Training Epoch: 2 [17060/49669]\tLoss: 9206.0029\n",
      "Training Epoch: 2 [17080/49669]\tLoss: 8564.8623\n",
      "Training Epoch: 2 [17100/49669]\tLoss: 10079.0010\n",
      "Training Epoch: 2 [17120/49669]\tLoss: 8292.5840\n",
      "Training Epoch: 2 [17140/49669]\tLoss: 7760.5830\n",
      "Training Epoch: 2 [17160/49669]\tLoss: 8821.8916\n",
      "Training Epoch: 2 [17180/49669]\tLoss: 10071.6113\n",
      "Training Epoch: 2 [17200/49669]\tLoss: 9264.4121\n",
      "Training Epoch: 2 [17220/49669]\tLoss: 8826.5488\n",
      "Training Epoch: 2 [17240/49669]\tLoss: 9271.3027\n",
      "Training Epoch: 2 [17260/49669]\tLoss: 10013.2354\n",
      "Training Epoch: 2 [17280/49669]\tLoss: 9285.1777\n",
      "Training Epoch: 2 [17300/49669]\tLoss: 9203.4707\n",
      "Training Epoch: 2 [17320/49669]\tLoss: 8214.7109\n",
      "Training Epoch: 2 [17340/49669]\tLoss: 7697.7085\n",
      "Training Epoch: 2 [17360/49669]\tLoss: 10620.3984\n",
      "Training Epoch: 2 [17380/49669]\tLoss: 10836.8604\n",
      "Training Epoch: 2 [17400/49669]\tLoss: 7665.6782\n",
      "Training Epoch: 2 [17420/49669]\tLoss: 8104.0688\n",
      "Training Epoch: 2 [17440/49669]\tLoss: 7568.8687\n",
      "Training Epoch: 2 [17460/49669]\tLoss: 9120.3330\n",
      "Training Epoch: 2 [17480/49669]\tLoss: 8528.2080\n",
      "Training Epoch: 2 [17500/49669]\tLoss: 7923.5635\n",
      "Training Epoch: 2 [17520/49669]\tLoss: 9560.2178\n",
      "Training Epoch: 2 [17540/49669]\tLoss: 11401.5020\n",
      "Training Epoch: 2 [17560/49669]\tLoss: 9362.0811\n",
      "Training Epoch: 2 [17580/49669]\tLoss: 7720.6519\n",
      "Training Epoch: 2 [17600/49669]\tLoss: 10767.2695\n",
      "Training Epoch: 2 [17620/49669]\tLoss: 8518.5400\n",
      "Training Epoch: 2 [17640/49669]\tLoss: 8755.1875\n",
      "Training Epoch: 2 [17660/49669]\tLoss: 8336.2324\n",
      "Training Epoch: 2 [17680/49669]\tLoss: 8510.5244\n",
      "Training Epoch: 2 [17700/49669]\tLoss: 8229.5098\n",
      "Training Epoch: 2 [17720/49669]\tLoss: 9154.8203\n",
      "Training Epoch: 2 [17740/49669]\tLoss: 9282.8105\n",
      "Training Epoch: 2 [17760/49669]\tLoss: 8168.8267\n",
      "Training Epoch: 2 [17780/49669]\tLoss: 9885.4629\n",
      "Training Epoch: 2 [17800/49669]\tLoss: 8818.1777\n",
      "Training Epoch: 2 [17820/49669]\tLoss: 8168.7515\n",
      "Training Epoch: 2 [17840/49669]\tLoss: 8452.0410\n",
      "Training Epoch: 2 [17860/49669]\tLoss: 8744.0957\n",
      "Training Epoch: 2 [17880/49669]\tLoss: 8036.2593\n",
      "Training Epoch: 2 [17900/49669]\tLoss: 8589.8389\n",
      "Training Epoch: 2 [17920/49669]\tLoss: 8455.4512\n",
      "Training Epoch: 2 [17940/49669]\tLoss: 8817.9355\n",
      "Training Epoch: 2 [17960/49669]\tLoss: 8365.3975\n",
      "Training Epoch: 2 [17980/49669]\tLoss: 7392.7808\n",
      "Training Epoch: 2 [18000/49669]\tLoss: 8707.8574\n",
      "Training Epoch: 2 [18020/49669]\tLoss: 6680.4526\n",
      "Training Epoch: 2 [18040/49669]\tLoss: 9915.3447\n",
      "Training Epoch: 2 [18060/49669]\tLoss: 8060.3589\n",
      "Training Epoch: 2 [18080/49669]\tLoss: 10332.6904\n",
      "Training Epoch: 2 [18100/49669]\tLoss: 10669.9033\n",
      "Training Epoch: 2 [18120/49669]\tLoss: 7932.1826\n",
      "Training Epoch: 2 [18140/49669]\tLoss: 6772.1909\n",
      "Training Epoch: 2 [18160/49669]\tLoss: 8276.6855\n",
      "Training Epoch: 2 [18180/49669]\tLoss: 10092.9951\n",
      "Training Epoch: 2 [18200/49669]\tLoss: 7746.7334\n",
      "Training Epoch: 2 [18220/49669]\tLoss: 8595.6855\n",
      "Training Epoch: 2 [18240/49669]\tLoss: 8261.1416\n",
      "Training Epoch: 2 [18260/49669]\tLoss: 8453.2090\n",
      "Training Epoch: 2 [18280/49669]\tLoss: 8731.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [18300/49669]\tLoss: 7679.7998\n",
      "Training Epoch: 2 [18320/49669]\tLoss: 7125.0947\n",
      "Training Epoch: 2 [18340/49669]\tLoss: 8698.2842\n",
      "Training Epoch: 2 [18360/49669]\tLoss: 9509.0977\n",
      "Training Epoch: 2 [18380/49669]\tLoss: 7164.4824\n",
      "Training Epoch: 2 [18400/49669]\tLoss: 9688.9834\n",
      "Training Epoch: 2 [18420/49669]\tLoss: 8386.7158\n",
      "Training Epoch: 2 [18440/49669]\tLoss: 8925.8174\n",
      "Training Epoch: 2 [18460/49669]\tLoss: 7837.3076\n",
      "Training Epoch: 2 [18480/49669]\tLoss: 7964.4326\n",
      "Training Epoch: 2 [18500/49669]\tLoss: 7273.6724\n",
      "Training Epoch: 2 [18520/49669]\tLoss: 8234.9727\n",
      "Training Epoch: 2 [18540/49669]\tLoss: 7407.4521\n",
      "Training Epoch: 2 [18560/49669]\tLoss: 9222.9102\n",
      "Training Epoch: 2 [18580/49669]\tLoss: 9206.5000\n",
      "Training Epoch: 2 [18600/49669]\tLoss: 8516.1660\n",
      "Training Epoch: 2 [18620/49669]\tLoss: 8705.6230\n",
      "Training Epoch: 2 [18640/49669]\tLoss: 8726.6016\n",
      "Training Epoch: 2 [18660/49669]\tLoss: 9007.0244\n",
      "Training Epoch: 2 [18680/49669]\tLoss: 7700.4209\n",
      "Training Epoch: 2 [18700/49669]\tLoss: 10004.9932\n",
      "Training Epoch: 2 [18720/49669]\tLoss: 7358.3901\n",
      "Training Epoch: 2 [18740/49669]\tLoss: 8344.4824\n",
      "Training Epoch: 2 [18760/49669]\tLoss: 9778.0957\n",
      "Training Epoch: 2 [18780/49669]\tLoss: 8477.6387\n",
      "Training Epoch: 2 [18800/49669]\tLoss: 7781.5474\n",
      "Training Epoch: 2 [18820/49669]\tLoss: 7761.3462\n",
      "Training Epoch: 2 [18840/49669]\tLoss: 8222.2305\n",
      "Training Epoch: 2 [18860/49669]\tLoss: 9026.5586\n",
      "Training Epoch: 2 [18880/49669]\tLoss: 8667.3164\n",
      "Training Epoch: 2 [18900/49669]\tLoss: 9140.9434\n",
      "Training Epoch: 2 [18920/49669]\tLoss: 7156.1880\n",
      "Training Epoch: 2 [18940/49669]\tLoss: 9408.1143\n",
      "Training Epoch: 2 [18960/49669]\tLoss: 8436.4551\n",
      "Training Epoch: 2 [18980/49669]\tLoss: 7954.8975\n",
      "Training Epoch: 2 [19000/49669]\tLoss: 9124.0420\n",
      "Training Epoch: 2 [19020/49669]\tLoss: 10032.9141\n",
      "Training Epoch: 2 [19040/49669]\tLoss: 9500.2188\n",
      "Training Epoch: 2 [19060/49669]\tLoss: 7663.0391\n",
      "Training Epoch: 2 [19080/49669]\tLoss: 8428.9053\n",
      "Training Epoch: 2 [19100/49669]\tLoss: 9808.2725\n",
      "Training Epoch: 2 [19120/49669]\tLoss: 8771.7061\n",
      "Training Epoch: 2 [19140/49669]\tLoss: 8004.9106\n",
      "Training Epoch: 2 [19160/49669]\tLoss: 9894.5996\n",
      "Training Epoch: 2 [19180/49669]\tLoss: 7903.9077\n",
      "Training Epoch: 2 [19200/49669]\tLoss: 8757.1230\n",
      "Training Epoch: 2 [19220/49669]\tLoss: 8025.2793\n",
      "Training Epoch: 2 [19240/49669]\tLoss: 7867.6602\n",
      "Training Epoch: 2 [19260/49669]\tLoss: 8337.8760\n",
      "Training Epoch: 2 [19280/49669]\tLoss: 7561.0415\n",
      "Training Epoch: 2 [19300/49669]\tLoss: 8466.4023\n",
      "Training Epoch: 2 [19320/49669]\tLoss: 8026.3809\n",
      "Training Epoch: 2 [19340/49669]\tLoss: 7320.5288\n",
      "Training Epoch: 2 [19360/49669]\tLoss: 7374.6011\n",
      "Training Epoch: 2 [19380/49669]\tLoss: 7536.0444\n",
      "Training Epoch: 2 [19400/49669]\tLoss: 8667.1289\n",
      "Training Epoch: 2 [19420/49669]\tLoss: 7224.9648\n",
      "Training Epoch: 2 [19440/49669]\tLoss: 7134.5234\n",
      "Training Epoch: 2 [19460/49669]\tLoss: 7535.1255\n",
      "Training Epoch: 2 [19480/49669]\tLoss: 8790.9287\n",
      "Training Epoch: 2 [19500/49669]\tLoss: 7809.8081\n",
      "Training Epoch: 2 [19520/49669]\tLoss: 7650.1250\n",
      "Training Epoch: 2 [19540/49669]\tLoss: 8688.8506\n",
      "Training Epoch: 2 [19560/49669]\tLoss: 8688.8154\n",
      "Training Epoch: 2 [19580/49669]\tLoss: 9257.1865\n",
      "Training Epoch: 2 [19600/49669]\tLoss: 7995.7207\n",
      "Training Epoch: 2 [19620/49669]\tLoss: 8788.0088\n",
      "Training Epoch: 2 [19640/49669]\tLoss: 6917.8018\n",
      "Training Epoch: 2 [19660/49669]\tLoss: 7859.9565\n",
      "Training Epoch: 2 [19680/49669]\tLoss: 7292.6548\n",
      "Training Epoch: 2 [19700/49669]\tLoss: 9298.8887\n",
      "Training Epoch: 2 [19720/49669]\tLoss: 8390.9590\n",
      "Training Epoch: 2 [19740/49669]\tLoss: 8225.2070\n",
      "Training Epoch: 2 [19760/49669]\tLoss: 8314.9824\n",
      "Training Epoch: 2 [19780/49669]\tLoss: 8735.5645\n",
      "Training Epoch: 2 [19800/49669]\tLoss: 7476.8110\n",
      "Training Epoch: 2 [19820/49669]\tLoss: 7592.0933\n",
      "Training Epoch: 2 [19840/49669]\tLoss: 8036.2026\n",
      "Training Epoch: 2 [19860/49669]\tLoss: 8555.9287\n",
      "Training Epoch: 2 [19880/49669]\tLoss: 8781.9463\n",
      "Training Epoch: 2 [19900/49669]\tLoss: 6616.5986\n",
      "Training Epoch: 2 [19920/49669]\tLoss: 8244.0625\n",
      "Training Epoch: 2 [19940/49669]\tLoss: 8761.1260\n",
      "Training Epoch: 2 [19960/49669]\tLoss: 7152.7749\n",
      "Training Epoch: 2 [19980/49669]\tLoss: 7842.6001\n",
      "Training Epoch: 2 [20000/49669]\tLoss: 7852.2163\n",
      "Training Epoch: 2 [20020/49669]\tLoss: 8352.3896\n",
      "Training Epoch: 2 [20040/49669]\tLoss: 8970.7930\n",
      "Training Epoch: 2 [20060/49669]\tLoss: 7186.8662\n",
      "Training Epoch: 2 [20080/49669]\tLoss: 8560.9951\n",
      "Training Epoch: 2 [20100/49669]\tLoss: 8199.8516\n",
      "Training Epoch: 2 [20120/49669]\tLoss: 8359.2764\n",
      "Training Epoch: 2 [20140/49669]\tLoss: 7957.7476\n",
      "Training Epoch: 2 [20160/49669]\tLoss: 15723.3857\n",
      "Training Epoch: 2 [20180/49669]\tLoss: 8574.7656\n",
      "Training Epoch: 2 [20200/49669]\tLoss: 9026.8008\n",
      "Training Epoch: 2 [20220/49669]\tLoss: 8356.9082\n",
      "Training Epoch: 2 [20240/49669]\tLoss: 8531.8018\n",
      "Training Epoch: 2 [20260/49669]\tLoss: 7650.1938\n",
      "Training Epoch: 2 [20280/49669]\tLoss: 8188.3359\n",
      "Training Epoch: 2 [20300/49669]\tLoss: 8720.1182\n",
      "Training Epoch: 2 [20320/49669]\tLoss: 6012.0068\n",
      "Training Epoch: 2 [20340/49669]\tLoss: 9361.2822\n",
      "Training Epoch: 2 [20360/49669]\tLoss: 7675.3398\n",
      "Training Epoch: 2 [20380/49669]\tLoss: 8301.6846\n",
      "Training Epoch: 2 [20400/49669]\tLoss: 7421.4229\n",
      "Training Epoch: 2 [20420/49669]\tLoss: 7165.7656\n",
      "Training Epoch: 2 [20440/49669]\tLoss: 7907.3481\n",
      "Training Epoch: 2 [20460/49669]\tLoss: 8714.9736\n",
      "Training Epoch: 2 [20480/49669]\tLoss: 8296.7070\n",
      "Training Epoch: 2 [20500/49669]\tLoss: 9508.5820\n",
      "Training Epoch: 2 [20520/49669]\tLoss: 8063.4175\n",
      "Training Epoch: 2 [20540/49669]\tLoss: 9364.5654\n",
      "Training Epoch: 2 [20560/49669]\tLoss: 8907.4834\n",
      "Training Epoch: 2 [20580/49669]\tLoss: 9310.7021\n",
      "Training Epoch: 2 [20600/49669]\tLoss: 7618.6802\n",
      "Training Epoch: 2 [20620/49669]\tLoss: 8623.1973\n",
      "Training Epoch: 2 [20640/49669]\tLoss: 6942.7671\n",
      "Training Epoch: 2 [20660/49669]\tLoss: 7929.7314\n",
      "Training Epoch: 2 [20680/49669]\tLoss: 7104.4473\n",
      "Training Epoch: 2 [20700/49669]\tLoss: 9056.9766\n",
      "Training Epoch: 2 [20720/49669]\tLoss: 7997.6328\n",
      "Training Epoch: 2 [20740/49669]\tLoss: 7961.1880\n",
      "Training Epoch: 2 [20760/49669]\tLoss: 7201.3857\n",
      "Training Epoch: 2 [20780/49669]\tLoss: 7598.6133\n",
      "Training Epoch: 2 [20800/49669]\tLoss: 8742.5293\n",
      "Training Epoch: 2 [20820/49669]\tLoss: 9330.4648\n",
      "Training Epoch: 2 [20840/49669]\tLoss: 8810.2910\n",
      "Training Epoch: 2 [20860/49669]\tLoss: 8721.3096\n",
      "Training Epoch: 2 [20880/49669]\tLoss: 7516.8071\n",
      "Training Epoch: 2 [20900/49669]\tLoss: 8624.3613\n",
      "Training Epoch: 2 [20920/49669]\tLoss: 7143.7949\n",
      "Training Epoch: 2 [20940/49669]\tLoss: 7374.3418\n",
      "Training Epoch: 2 [20960/49669]\tLoss: 8158.3960\n",
      "Training Epoch: 2 [20980/49669]\tLoss: 7409.5317\n",
      "Training Epoch: 2 [21000/49669]\tLoss: 7917.3979\n",
      "Training Epoch: 2 [21020/49669]\tLoss: 7547.7373\n",
      "Training Epoch: 2 [21040/49669]\tLoss: 6434.1616\n",
      "Training Epoch: 2 [21060/49669]\tLoss: 7311.6572\n",
      "Training Epoch: 2 [21080/49669]\tLoss: 8069.8394\n",
      "Training Epoch: 2 [21100/49669]\tLoss: 7711.7256\n",
      "Training Epoch: 2 [21120/49669]\tLoss: 7351.7158\n",
      "Training Epoch: 2 [21140/49669]\tLoss: 7600.4155\n",
      "Training Epoch: 2 [21160/49669]\tLoss: 8011.4248\n",
      "Training Epoch: 2 [21180/49669]\tLoss: 6540.9688\n",
      "Training Epoch: 2 [21200/49669]\tLoss: 5759.8135\n",
      "Training Epoch: 2 [21220/49669]\tLoss: 7098.2534\n",
      "Training Epoch: 2 [21240/49669]\tLoss: 6845.4224\n",
      "Training Epoch: 2 [21260/49669]\tLoss: 8638.5391\n",
      "Training Epoch: 2 [21280/49669]\tLoss: 7784.2852\n",
      "Training Epoch: 2 [21300/49669]\tLoss: 7473.6572\n",
      "Training Epoch: 2 [21320/49669]\tLoss: 7351.3062\n",
      "Training Epoch: 2 [21340/49669]\tLoss: 7316.7925\n",
      "Training Epoch: 2 [21360/49669]\tLoss: 8787.9424\n",
      "Training Epoch: 2 [21380/49669]\tLoss: 7741.9336\n",
      "Training Epoch: 2 [21400/49669]\tLoss: 8011.2769\n",
      "Training Epoch: 2 [21420/49669]\tLoss: 7810.8691\n",
      "Training Epoch: 2 [21440/49669]\tLoss: 6619.3906\n",
      "Training Epoch: 2 [21460/49669]\tLoss: 7757.1641\n",
      "Training Epoch: 2 [21480/49669]\tLoss: 7644.8745\n",
      "Training Epoch: 2 [21500/49669]\tLoss: 6139.3330\n",
      "Training Epoch: 2 [21520/49669]\tLoss: 7014.4624\n",
      "Training Epoch: 2 [21540/49669]\tLoss: 6441.4116\n",
      "Training Epoch: 2 [21560/49669]\tLoss: 7812.2681\n",
      "Training Epoch: 2 [21580/49669]\tLoss: 8000.2021\n",
      "Training Epoch: 2 [21600/49669]\tLoss: 7702.9180\n",
      "Training Epoch: 2 [21620/49669]\tLoss: 8009.3018\n",
      "Training Epoch: 2 [21640/49669]\tLoss: 7252.9995\n",
      "Training Epoch: 2 [21660/49669]\tLoss: 6855.1006\n",
      "Training Epoch: 2 [21680/49669]\tLoss: 6836.5439\n",
      "Training Epoch: 2 [21700/49669]\tLoss: 6922.7202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [21720/49669]\tLoss: 7620.5376\n",
      "Training Epoch: 2 [21740/49669]\tLoss: 6992.9409\n",
      "Training Epoch: 2 [21760/49669]\tLoss: 6918.0693\n",
      "Training Epoch: 2 [21780/49669]\tLoss: 6522.3086\n",
      "Training Epoch: 2 [21800/49669]\tLoss: 8993.2471\n",
      "Training Epoch: 2 [21820/49669]\tLoss: 7021.4600\n",
      "Training Epoch: 2 [21840/49669]\tLoss: 8445.8584\n",
      "Training Epoch: 2 [21860/49669]\tLoss: 7389.1777\n",
      "Training Epoch: 2 [21880/49669]\tLoss: 7849.5171\n",
      "Training Epoch: 2 [21900/49669]\tLoss: 6697.6904\n",
      "Training Epoch: 2 [21920/49669]\tLoss: 7602.3027\n",
      "Training Epoch: 2 [21940/49669]\tLoss: 8597.1914\n",
      "Training Epoch: 2 [21960/49669]\tLoss: 7356.4478\n",
      "Training Epoch: 2 [21980/49669]\tLoss: 6846.8394\n",
      "Training Epoch: 2 [22000/49669]\tLoss: 8303.7100\n",
      "Training Epoch: 2 [22020/49669]\tLoss: 7610.7388\n",
      "Training Epoch: 2 [22040/49669]\tLoss: 7440.6694\n",
      "Training Epoch: 2 [22060/49669]\tLoss: 8130.7905\n",
      "Training Epoch: 2 [22080/49669]\tLoss: 7774.0220\n",
      "Training Epoch: 2 [22100/49669]\tLoss: 7366.1333\n",
      "Training Epoch: 2 [22120/49669]\tLoss: 6378.6602\n",
      "Training Epoch: 2 [22140/49669]\tLoss: 6781.6641\n",
      "Training Epoch: 2 [22160/49669]\tLoss: 7965.0015\n",
      "Training Epoch: 2 [22180/49669]\tLoss: 7092.1509\n",
      "Training Epoch: 2 [22200/49669]\tLoss: 7599.2910\n",
      "Training Epoch: 2 [22220/49669]\tLoss: 6797.9077\n",
      "Training Epoch: 2 [22240/49669]\tLoss: 6891.1704\n",
      "Training Epoch: 2 [22260/49669]\tLoss: 7422.2207\n",
      "Training Epoch: 2 [22280/49669]\tLoss: 6468.3203\n",
      "Training Epoch: 2 [22300/49669]\tLoss: 7371.8545\n",
      "Training Epoch: 2 [22320/49669]\tLoss: 7081.8545\n",
      "Training Epoch: 2 [22340/49669]\tLoss: 6927.7598\n",
      "Training Epoch: 2 [22360/49669]\tLoss: 7330.6055\n",
      "Training Epoch: 2 [22380/49669]\tLoss: 7697.2930\n",
      "Training Epoch: 2 [22400/49669]\tLoss: 6721.3164\n",
      "Training Epoch: 2 [22420/49669]\tLoss: 6710.9263\n",
      "Training Epoch: 2 [22440/49669]\tLoss: 7011.8330\n",
      "Training Epoch: 2 [22460/49669]\tLoss: 7941.7437\n",
      "Training Epoch: 2 [22480/49669]\tLoss: 7507.2603\n",
      "Training Epoch: 2 [22500/49669]\tLoss: 5872.1133\n",
      "Training Epoch: 2 [22520/49669]\tLoss: 8276.9980\n",
      "Training Epoch: 2 [22540/49669]\tLoss: 8113.5098\n",
      "Training Epoch: 2 [22560/49669]\tLoss: 7693.9121\n",
      "Training Epoch: 2 [22580/49669]\tLoss: 9419.9912\n",
      "Training Epoch: 2 [22600/49669]\tLoss: 7665.7739\n",
      "Training Epoch: 2 [22620/49669]\tLoss: 6650.9692\n",
      "Training Epoch: 2 [22640/49669]\tLoss: 7139.1338\n",
      "Training Epoch: 2 [22660/49669]\tLoss: 7030.4150\n",
      "Training Epoch: 2 [22680/49669]\tLoss: 6796.0591\n",
      "Training Epoch: 2 [22700/49669]\tLoss: 6936.3877\n",
      "Training Epoch: 2 [22720/49669]\tLoss: 6109.1968\n",
      "Training Epoch: 2 [22740/49669]\tLoss: 7588.5747\n",
      "Training Epoch: 2 [22760/49669]\tLoss: 7243.1074\n",
      "Training Epoch: 2 [22780/49669]\tLoss: 7227.0591\n",
      "Training Epoch: 2 [22800/49669]\tLoss: 6695.3540\n",
      "Training Epoch: 2 [22820/49669]\tLoss: 8196.3311\n",
      "Training Epoch: 2 [22840/49669]\tLoss: 7479.6567\n",
      "Training Epoch: 2 [22860/49669]\tLoss: 6663.7339\n",
      "Training Epoch: 2 [22880/49669]\tLoss: 7586.7065\n",
      "Training Epoch: 2 [22900/49669]\tLoss: 8194.4619\n",
      "Training Epoch: 2 [22920/49669]\tLoss: 6122.6177\n",
      "Training Epoch: 2 [22940/49669]\tLoss: 7602.1270\n",
      "Training Epoch: 2 [22960/49669]\tLoss: 6562.0693\n",
      "Training Epoch: 2 [22980/49669]\tLoss: 6433.3691\n",
      "Training Epoch: 2 [23000/49669]\tLoss: 7188.9009\n",
      "Training Epoch: 2 [23020/49669]\tLoss: 7135.8921\n",
      "Training Epoch: 2 [23040/49669]\tLoss: 7776.9194\n",
      "Training Epoch: 2 [23060/49669]\tLoss: 7554.7515\n",
      "Training Epoch: 2 [23080/49669]\tLoss: 7445.3296\n",
      "Training Epoch: 2 [23100/49669]\tLoss: 7144.7212\n",
      "Training Epoch: 2 [23120/49669]\tLoss: 7781.7075\n",
      "Training Epoch: 2 [23140/49669]\tLoss: 6316.8149\n",
      "Training Epoch: 2 [23160/49669]\tLoss: 7244.7231\n",
      "Training Epoch: 2 [23180/49669]\tLoss: 6273.6660\n",
      "Training Epoch: 2 [23200/49669]\tLoss: 7410.6831\n",
      "Training Epoch: 2 [23220/49669]\tLoss: 6915.9175\n",
      "Training Epoch: 2 [23240/49669]\tLoss: 6763.9795\n",
      "Training Epoch: 2 [23260/49669]\tLoss: 7999.9268\n",
      "Training Epoch: 2 [23280/49669]\tLoss: 7815.6055\n",
      "Training Epoch: 2 [23300/49669]\tLoss: 7439.1025\n",
      "Training Epoch: 2 [23320/49669]\tLoss: 7633.8345\n",
      "Training Epoch: 2 [23340/49669]\tLoss: 7084.7046\n",
      "Training Epoch: 2 [23360/49669]\tLoss: 6598.8188\n",
      "Training Epoch: 2 [23380/49669]\tLoss: 7303.1191\n",
      "Training Epoch: 2 [23400/49669]\tLoss: 6954.0361\n",
      "Training Epoch: 2 [23420/49669]\tLoss: 6901.2285\n",
      "Training Epoch: 2 [23440/49669]\tLoss: 8776.4355\n",
      "Training Epoch: 2 [23460/49669]\tLoss: 6356.8984\n",
      "Training Epoch: 2 [23480/49669]\tLoss: 7005.3364\n",
      "Training Epoch: 2 [23500/49669]\tLoss: 6462.1006\n",
      "Training Epoch: 2 [23520/49669]\tLoss: 7159.4878\n",
      "Training Epoch: 2 [23540/49669]\tLoss: 7751.9639\n",
      "Training Epoch: 2 [23560/49669]\tLoss: 5571.8213\n",
      "Training Epoch: 2 [23580/49669]\tLoss: 7113.0830\n",
      "Training Epoch: 2 [23600/49669]\tLoss: 7305.8491\n",
      "Training Epoch: 2 [23620/49669]\tLoss: 6563.1880\n",
      "Training Epoch: 2 [23640/49669]\tLoss: 6719.6216\n",
      "Training Epoch: 2 [23660/49669]\tLoss: 7144.4155\n",
      "Training Epoch: 2 [23680/49669]\tLoss: 7035.3462\n",
      "Training Epoch: 2 [23700/49669]\tLoss: 7696.9780\n",
      "Training Epoch: 2 [23720/49669]\tLoss: 6718.5708\n",
      "Training Epoch: 2 [23740/49669]\tLoss: 7308.0986\n",
      "Training Epoch: 2 [23760/49669]\tLoss: 7869.5967\n",
      "Training Epoch: 2 [23780/49669]\tLoss: 7435.9150\n",
      "Training Epoch: 2 [23800/49669]\tLoss: 7185.8340\n",
      "Training Epoch: 2 [23820/49669]\tLoss: 7641.7842\n",
      "Training Epoch: 2 [23840/49669]\tLoss: 6330.9150\n",
      "Training Epoch: 2 [23860/49669]\tLoss: 7656.9663\n",
      "Training Epoch: 2 [23880/49669]\tLoss: 6474.9683\n",
      "Training Epoch: 2 [23900/49669]\tLoss: 6749.2227\n",
      "Training Epoch: 2 [23920/49669]\tLoss: 6082.2188\n",
      "Training Epoch: 2 [23940/49669]\tLoss: 5318.6587\n",
      "Training Epoch: 2 [23960/49669]\tLoss: 7192.3037\n",
      "Training Epoch: 2 [23980/49669]\tLoss: 6539.2192\n",
      "Training Epoch: 2 [24000/49669]\tLoss: 8454.1553\n",
      "Training Epoch: 2 [24020/49669]\tLoss: 6728.8882\n",
      "Training Epoch: 2 [24040/49669]\tLoss: 6515.0322\n",
      "Training Epoch: 2 [24060/49669]\tLoss: 6414.3813\n",
      "Training Epoch: 2 [24080/49669]\tLoss: 6592.3760\n",
      "Training Epoch: 2 [24100/49669]\tLoss: 6926.0332\n",
      "Training Epoch: 2 [24120/49669]\tLoss: 5735.8442\n",
      "Training Epoch: 2 [24140/49669]\tLoss: 6724.1558\n",
      "Training Epoch: 2 [24160/49669]\tLoss: 6833.4688\n",
      "Training Epoch: 2 [24180/49669]\tLoss: 6587.3398\n",
      "Training Epoch: 2 [24200/49669]\tLoss: 7250.8169\n",
      "Training Epoch: 2 [24220/49669]\tLoss: 6776.4585\n",
      "Training Epoch: 2 [24240/49669]\tLoss: 7089.1567\n",
      "Training Epoch: 2 [24260/49669]\tLoss: 6900.4136\n",
      "Training Epoch: 2 [24280/49669]\tLoss: 6988.9307\n",
      "Training Epoch: 2 [24300/49669]\tLoss: 6920.8013\n",
      "Training Epoch: 2 [24320/49669]\tLoss: 7926.9707\n",
      "Training Epoch: 2 [24340/49669]\tLoss: 7006.4277\n",
      "Training Epoch: 2 [24360/49669]\tLoss: 6943.8271\n",
      "Training Epoch: 2 [24380/49669]\tLoss: 7260.4731\n",
      "Training Epoch: 2 [24400/49669]\tLoss: 7231.1738\n",
      "Training Epoch: 2 [24420/49669]\tLoss: 7053.3506\n",
      "Training Epoch: 2 [24440/49669]\tLoss: 6637.9204\n",
      "Training Epoch: 2 [24460/49669]\tLoss: 6567.3306\n",
      "Training Epoch: 2 [24480/49669]\tLoss: 6480.7856\n",
      "Training Epoch: 2 [24500/49669]\tLoss: 7687.6831\n",
      "Training Epoch: 2 [24520/49669]\tLoss: 5786.2085\n",
      "Training Epoch: 2 [24540/49669]\tLoss: 6536.6519\n",
      "Training Epoch: 2 [24560/49669]\tLoss: 8168.2407\n",
      "Training Epoch: 2 [24580/49669]\tLoss: 7279.2832\n",
      "Training Epoch: 2 [24600/49669]\tLoss: 5729.2305\n",
      "Training Epoch: 2 [24620/49669]\tLoss: 8184.3804\n",
      "Training Epoch: 2 [24640/49669]\tLoss: 6959.2983\n",
      "Training Epoch: 2 [24660/49669]\tLoss: 7546.4395\n",
      "Training Epoch: 2 [24680/49669]\tLoss: 6529.6724\n",
      "Training Epoch: 2 [24700/49669]\tLoss: 7123.4556\n",
      "Training Epoch: 2 [24720/49669]\tLoss: 6855.8955\n",
      "Training Epoch: 2 [24740/49669]\tLoss: 6490.7915\n",
      "Training Epoch: 2 [24760/49669]\tLoss: 5909.9346\n",
      "Training Epoch: 2 [24780/49669]\tLoss: 7371.1489\n",
      "Training Epoch: 2 [24800/49669]\tLoss: 6587.4434\n",
      "Training Epoch: 2 [24820/49669]\tLoss: 6861.2085\n",
      "Training Epoch: 2 [24840/49669]\tLoss: 6514.1021\n",
      "Training Epoch: 2 [24860/49669]\tLoss: 6441.5449\n",
      "Training Epoch: 2 [24880/49669]\tLoss: 6139.3569\n",
      "Training Epoch: 2 [24900/49669]\tLoss: 6661.5615\n",
      "Training Epoch: 2 [24920/49669]\tLoss: 7153.3242\n",
      "Training Epoch: 2 [24940/49669]\tLoss: 6847.4517\n",
      "Training Epoch: 2 [24960/49669]\tLoss: 6365.0298\n",
      "Training Epoch: 2 [24980/49669]\tLoss: 7380.8525\n",
      "Training Epoch: 2 [25000/49669]\tLoss: 6900.7251\n",
      "Training Epoch: 2 [25020/49669]\tLoss: 6576.7783\n",
      "Training Epoch: 2 [25040/49669]\tLoss: 5999.9092\n",
      "Training Epoch: 2 [25060/49669]\tLoss: 6252.4688\n",
      "Training Epoch: 2 [25080/49669]\tLoss: 6273.9780\n",
      "Training Epoch: 2 [25100/49669]\tLoss: 6259.8315\n",
      "Training Epoch: 2 [25120/49669]\tLoss: 6965.3735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [25140/49669]\tLoss: 7589.2871\n",
      "Training Epoch: 2 [25160/49669]\tLoss: 6365.4805\n",
      "Training Epoch: 2 [25180/49669]\tLoss: 5852.2715\n",
      "Training Epoch: 2 [25200/49669]\tLoss: 7248.3901\n",
      "Training Epoch: 2 [25220/49669]\tLoss: 6465.3618\n",
      "Training Epoch: 2 [25240/49669]\tLoss: 6308.0679\n",
      "Training Epoch: 2 [25260/49669]\tLoss: 6451.0864\n",
      "Training Epoch: 2 [25280/49669]\tLoss: 7909.1221\n",
      "Training Epoch: 2 [25300/49669]\tLoss: 6543.2549\n",
      "Training Epoch: 2 [25320/49669]\tLoss: 7109.1514\n",
      "Training Epoch: 2 [25340/49669]\tLoss: 6383.8096\n",
      "Training Epoch: 2 [25360/49669]\tLoss: 7478.4126\n",
      "Training Epoch: 2 [25380/49669]\tLoss: 5923.0146\n",
      "Training Epoch: 2 [25400/49669]\tLoss: 6448.1411\n",
      "Training Epoch: 2 [25420/49669]\tLoss: 6552.3154\n",
      "Training Epoch: 2 [25440/49669]\tLoss: 6111.2417\n",
      "Training Epoch: 2 [25460/49669]\tLoss: 6715.3970\n",
      "Training Epoch: 2 [25480/49669]\tLoss: 6515.2305\n",
      "Training Epoch: 2 [25500/49669]\tLoss: 8048.4263\n",
      "Training Epoch: 2 [25520/49669]\tLoss: 7659.9575\n",
      "Training Epoch: 2 [25540/49669]\tLoss: 6334.4473\n",
      "Training Epoch: 2 [25560/49669]\tLoss: 6444.6138\n",
      "Training Epoch: 2 [25580/49669]\tLoss: 5862.7920\n",
      "Training Epoch: 2 [25600/49669]\tLoss: 6679.9219\n",
      "Training Epoch: 2 [25620/49669]\tLoss: 6083.6221\n",
      "Training Epoch: 2 [25640/49669]\tLoss: 6376.4912\n",
      "Training Epoch: 2 [25660/49669]\tLoss: 6668.0581\n",
      "Training Epoch: 2 [25680/49669]\tLoss: 7345.1685\n",
      "Training Epoch: 2 [25700/49669]\tLoss: 6079.1006\n",
      "Training Epoch: 2 [25720/49669]\tLoss: 6582.3252\n",
      "Training Epoch: 2 [25740/49669]\tLoss: 6571.2051\n",
      "Training Epoch: 2 [25760/49669]\tLoss: 5063.9751\n",
      "Training Epoch: 2 [25780/49669]\tLoss: 5927.9902\n",
      "Training Epoch: 2 [25800/49669]\tLoss: 6811.4126\n",
      "Training Epoch: 2 [25820/49669]\tLoss: 7308.4380\n",
      "Training Epoch: 2 [25840/49669]\tLoss: 7738.8628\n",
      "Training Epoch: 2 [25860/49669]\tLoss: 7060.6987\n",
      "Training Epoch: 2 [25880/49669]\tLoss: 6692.5132\n",
      "Training Epoch: 2 [25900/49669]\tLoss: 6540.0225\n",
      "Training Epoch: 2 [25920/49669]\tLoss: 6593.9272\n",
      "Training Epoch: 2 [25940/49669]\tLoss: 7190.9341\n",
      "Training Epoch: 2 [25960/49669]\tLoss: 5825.2861\n",
      "Training Epoch: 2 [25980/49669]\tLoss: 7504.7378\n",
      "Training Epoch: 2 [26000/49669]\tLoss: 7048.8774\n",
      "Training Epoch: 2 [26020/49669]\tLoss: 7967.7144\n",
      "Training Epoch: 2 [26040/49669]\tLoss: 6690.8403\n",
      "Training Epoch: 2 [26060/49669]\tLoss: 6648.9727\n",
      "Training Epoch: 2 [26080/49669]\tLoss: 5966.4053\n",
      "Training Epoch: 2 [26100/49669]\tLoss: 6448.0771\n",
      "Training Epoch: 2 [26120/49669]\tLoss: 7107.6787\n",
      "Training Epoch: 2 [26140/49669]\tLoss: 7265.6509\n",
      "Training Epoch: 2 [26160/49669]\tLoss: 5576.6255\n",
      "Training Epoch: 2 [26180/49669]\tLoss: 6500.6528\n",
      "Training Epoch: 2 [26200/49669]\tLoss: 5911.1450\n",
      "Training Epoch: 2 [26220/49669]\tLoss: 5826.1514\n",
      "Training Epoch: 2 [26240/49669]\tLoss: 6009.2827\n",
      "Training Epoch: 2 [26260/49669]\tLoss: 6250.2358\n",
      "Training Epoch: 2 [26280/49669]\tLoss: 6564.2573\n",
      "Training Epoch: 2 [26300/49669]\tLoss: 6805.9067\n",
      "Training Epoch: 2 [26320/49669]\tLoss: 5516.1318\n",
      "Training Epoch: 2 [26340/49669]\tLoss: 7011.9902\n",
      "Training Epoch: 2 [26360/49669]\tLoss: 6148.2954\n",
      "Training Epoch: 2 [26380/49669]\tLoss: 7438.5312\n",
      "Training Epoch: 2 [26400/49669]\tLoss: 6426.0903\n",
      "Training Epoch: 2 [26420/49669]\tLoss: 6065.4863\n",
      "Training Epoch: 2 [26440/49669]\tLoss: 6395.4570\n",
      "Training Epoch: 2 [26460/49669]\tLoss: 6285.2402\n",
      "Training Epoch: 2 [26480/49669]\tLoss: 6751.2915\n",
      "Training Epoch: 2 [26500/49669]\tLoss: 6582.4438\n",
      "Training Epoch: 2 [26520/49669]\tLoss: 5843.7446\n",
      "Training Epoch: 2 [26540/49669]\tLoss: 6446.2129\n",
      "Training Epoch: 2 [26560/49669]\tLoss: 7448.1187\n",
      "Training Epoch: 2 [26580/49669]\tLoss: 6629.1670\n",
      "Training Epoch: 2 [26600/49669]\tLoss: 6734.7778\n",
      "Training Epoch: 2 [26620/49669]\tLoss: 6439.4102\n",
      "Training Epoch: 2 [26640/49669]\tLoss: 7016.5640\n",
      "Training Epoch: 2 [26660/49669]\tLoss: 6406.7007\n",
      "Training Epoch: 2 [26680/49669]\tLoss: 5654.3882\n",
      "Training Epoch: 2 [26700/49669]\tLoss: 6173.7729\n",
      "Training Epoch: 2 [26720/49669]\tLoss: 5539.4326\n",
      "Training Epoch: 2 [26740/49669]\tLoss: 6426.8467\n",
      "Training Epoch: 2 [26760/49669]\tLoss: 6392.1143\n",
      "Training Epoch: 2 [26780/49669]\tLoss: 6741.7451\n",
      "Training Epoch: 2 [26800/49669]\tLoss: 5576.0723\n",
      "Training Epoch: 2 [26820/49669]\tLoss: 6526.2734\n",
      "Training Epoch: 2 [26840/49669]\tLoss: 6515.0298\n",
      "Training Epoch: 2 [26860/49669]\tLoss: 6855.1255\n",
      "Training Epoch: 2 [26880/49669]\tLoss: 6247.2085\n",
      "Training Epoch: 2 [26900/49669]\tLoss: 5965.1846\n",
      "Training Epoch: 2 [26920/49669]\tLoss: 6498.8496\n",
      "Training Epoch: 2 [26940/49669]\tLoss: 6526.9214\n",
      "Training Epoch: 2 [26960/49669]\tLoss: 6107.9292\n",
      "Training Epoch: 2 [26980/49669]\tLoss: 7573.7852\n",
      "Training Epoch: 2 [27000/49669]\tLoss: 6748.2563\n",
      "Training Epoch: 2 [27020/49669]\tLoss: 6513.5713\n",
      "Training Epoch: 2 [27040/49669]\tLoss: 6709.1338\n",
      "Training Epoch: 2 [27060/49669]\tLoss: 7015.0537\n",
      "Training Epoch: 2 [27080/49669]\tLoss: 5337.2822\n",
      "Training Epoch: 2 [27100/49669]\tLoss: 6662.2593\n",
      "Training Epoch: 2 [27120/49669]\tLoss: 6599.4346\n",
      "Training Epoch: 2 [27140/49669]\tLoss: 6058.2529\n",
      "Training Epoch: 2 [27160/49669]\tLoss: 7174.9604\n",
      "Training Epoch: 2 [27180/49669]\tLoss: 6401.4595\n",
      "Training Epoch: 2 [27200/49669]\tLoss: 7794.5991\n",
      "Training Epoch: 2 [27220/49669]\tLoss: 6439.1650\n",
      "Training Epoch: 2 [27240/49669]\tLoss: 5434.9287\n",
      "Training Epoch: 2 [27260/49669]\tLoss: 5848.1187\n",
      "Training Epoch: 2 [27280/49669]\tLoss: 6296.2002\n",
      "Training Epoch: 2 [27300/49669]\tLoss: 6290.5303\n",
      "Training Epoch: 2 [27320/49669]\tLoss: 5921.8062\n",
      "Training Epoch: 2 [27340/49669]\tLoss: 5756.8447\n",
      "Training Epoch: 2 [27360/49669]\tLoss: 6754.6172\n",
      "Training Epoch: 2 [27380/49669]\tLoss: 6701.0708\n",
      "Training Epoch: 2 [27400/49669]\tLoss: 6012.1562\n",
      "Training Epoch: 2 [27420/49669]\tLoss: 6839.1489\n",
      "Training Epoch: 2 [27440/49669]\tLoss: 5879.3760\n",
      "Training Epoch: 2 [27460/49669]\tLoss: 5557.5044\n",
      "Training Epoch: 2 [27480/49669]\tLoss: 7141.7412\n",
      "Training Epoch: 2 [27500/49669]\tLoss: 6918.1460\n",
      "Training Epoch: 2 [27520/49669]\tLoss: 6725.7656\n",
      "Training Epoch: 2 [27540/49669]\tLoss: 6091.2886\n",
      "Training Epoch: 2 [27560/49669]\tLoss: 6197.1729\n",
      "Training Epoch: 2 [27580/49669]\tLoss: 6138.4941\n",
      "Training Epoch: 2 [27600/49669]\tLoss: 6565.1235\n",
      "Training Epoch: 2 [27620/49669]\tLoss: 6753.3975\n",
      "Training Epoch: 2 [27640/49669]\tLoss: 7005.0171\n",
      "Training Epoch: 2 [27660/49669]\tLoss: 6267.5098\n",
      "Training Epoch: 2 [27680/49669]\tLoss: 5882.5449\n",
      "Training Epoch: 2 [27700/49669]\tLoss: 7111.9272\n",
      "Training Epoch: 2 [27720/49669]\tLoss: 6056.6489\n",
      "Training Epoch: 2 [27740/49669]\tLoss: 6478.3359\n",
      "Training Epoch: 2 [27760/49669]\tLoss: 6177.4277\n",
      "Training Epoch: 2 [27780/49669]\tLoss: 5771.0342\n",
      "Training Epoch: 2 [27800/49669]\tLoss: 6630.5176\n",
      "Training Epoch: 2 [27820/49669]\tLoss: 6689.5737\n",
      "Training Epoch: 2 [27840/49669]\tLoss: 5336.8818\n",
      "Training Epoch: 2 [27860/49669]\tLoss: 5931.1172\n",
      "Training Epoch: 2 [27880/49669]\tLoss: 5631.3994\n",
      "Training Epoch: 2 [27900/49669]\tLoss: 6723.7617\n",
      "Training Epoch: 2 [27920/49669]\tLoss: 6523.2456\n",
      "Training Epoch: 2 [27940/49669]\tLoss: 6266.9419\n",
      "Training Epoch: 2 [27960/49669]\tLoss: 6303.3291\n",
      "Training Epoch: 2 [27980/49669]\tLoss: 6338.9043\n",
      "Training Epoch: 2 [28000/49669]\tLoss: 6211.7207\n",
      "Training Epoch: 2 [28020/49669]\tLoss: 7135.7700\n",
      "Training Epoch: 2 [28040/49669]\tLoss: 6608.3218\n",
      "Training Epoch: 2 [28060/49669]\tLoss: 6196.1235\n",
      "Training Epoch: 2 [28080/49669]\tLoss: 6249.0571\n",
      "Training Epoch: 2 [28100/49669]\tLoss: 6953.8213\n",
      "Training Epoch: 2 [28120/49669]\tLoss: 7170.7578\n",
      "Training Epoch: 2 [28140/49669]\tLoss: 6924.3638\n",
      "Training Epoch: 2 [28160/49669]\tLoss: 5118.9331\n",
      "Training Epoch: 2 [28180/49669]\tLoss: 5854.7881\n",
      "Training Epoch: 2 [28200/49669]\tLoss: 7028.3545\n",
      "Training Epoch: 2 [28220/49669]\tLoss: 6499.1133\n",
      "Training Epoch: 2 [28240/49669]\tLoss: 5384.0679\n",
      "Training Epoch: 2 [28260/49669]\tLoss: 6026.4116\n",
      "Training Epoch: 2 [28280/49669]\tLoss: 6599.1133\n",
      "Training Epoch: 2 [28300/49669]\tLoss: 6161.5337\n",
      "Training Epoch: 2 [28320/49669]\tLoss: 6276.2993\n",
      "Training Epoch: 2 [28340/49669]\tLoss: 5953.7046\n",
      "Training Epoch: 2 [28360/49669]\tLoss: 6334.4966\n",
      "Training Epoch: 2 [28380/49669]\tLoss: 7274.5942\n",
      "Training Epoch: 2 [28400/49669]\tLoss: 6045.7202\n",
      "Training Epoch: 2 [28420/49669]\tLoss: 5719.9180\n",
      "Training Epoch: 2 [28440/49669]\tLoss: 5308.2363\n",
      "Training Epoch: 2 [28460/49669]\tLoss: 6973.2251\n",
      "Training Epoch: 2 [28480/49669]\tLoss: 6047.0820\n",
      "Training Epoch: 2 [28500/49669]\tLoss: 5812.1772\n",
      "Training Epoch: 2 [28520/49669]\tLoss: 5775.0068\n",
      "Training Epoch: 2 [28540/49669]\tLoss: 6713.4707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [28560/49669]\tLoss: 5700.7373\n",
      "Training Epoch: 2 [28580/49669]\tLoss: 6575.1406\n",
      "Training Epoch: 2 [28600/49669]\tLoss: 6128.2300\n",
      "Training Epoch: 2 [28620/49669]\tLoss: 6443.2559\n",
      "Training Epoch: 2 [28640/49669]\tLoss: 5933.9146\n",
      "Training Epoch: 2 [28660/49669]\tLoss: 6748.6167\n",
      "Training Epoch: 2 [28680/49669]\tLoss: 5988.9785\n",
      "Training Epoch: 2 [28700/49669]\tLoss: 6305.5918\n",
      "Training Epoch: 2 [28720/49669]\tLoss: 7039.7354\n",
      "Training Epoch: 2 [28740/49669]\tLoss: 6072.0327\n",
      "Training Epoch: 2 [28760/49669]\tLoss: 6069.3950\n",
      "Training Epoch: 2 [28780/49669]\tLoss: 7347.0044\n",
      "Training Epoch: 2 [28800/49669]\tLoss: 6093.5605\n",
      "Training Epoch: 2 [28820/49669]\tLoss: 5485.9834\n",
      "Training Epoch: 2 [28840/49669]\tLoss: 5948.6533\n",
      "Training Epoch: 2 [28860/49669]\tLoss: 5931.2778\n",
      "Training Epoch: 2 [28880/49669]\tLoss: 6423.7017\n",
      "Training Epoch: 2 [28900/49669]\tLoss: 6838.9453\n",
      "Training Epoch: 2 [28920/49669]\tLoss: 5947.5063\n",
      "Training Epoch: 2 [28940/49669]\tLoss: 6734.1328\n",
      "Training Epoch: 2 [28960/49669]\tLoss: 6134.9746\n",
      "Training Epoch: 2 [28980/49669]\tLoss: 7499.4209\n",
      "Training Epoch: 2 [29000/49669]\tLoss: 6721.9639\n",
      "Training Epoch: 2 [29020/49669]\tLoss: 6568.7114\n",
      "Training Epoch: 2 [29040/49669]\tLoss: 7007.6323\n",
      "Training Epoch: 2 [29060/49669]\tLoss: 6303.8062\n",
      "Training Epoch: 2 [29080/49669]\tLoss: 5278.5493\n",
      "Training Epoch: 2 [29100/49669]\tLoss: 6716.0752\n",
      "Training Epoch: 2 [29120/49669]\tLoss: 6076.8623\n",
      "Training Epoch: 2 [29140/49669]\tLoss: 6273.7388\n",
      "Training Epoch: 2 [29160/49669]\tLoss: 6218.2812\n",
      "Training Epoch: 2 [29180/49669]\tLoss: 6474.2119\n",
      "Training Epoch: 2 [29200/49669]\tLoss: 7147.7310\n",
      "Training Epoch: 2 [29220/49669]\tLoss: 5273.1533\n",
      "Training Epoch: 2 [29240/49669]\tLoss: 6230.2925\n",
      "Training Epoch: 2 [29260/49669]\tLoss: 7346.2529\n",
      "Training Epoch: 2 [29280/49669]\tLoss: 6543.5493\n",
      "Training Epoch: 2 [29300/49669]\tLoss: 6433.7446\n",
      "Training Epoch: 2 [29320/49669]\tLoss: 6786.5303\n",
      "Training Epoch: 2 [29340/49669]\tLoss: 6861.6406\n",
      "Training Epoch: 2 [29360/49669]\tLoss: 6367.4385\n",
      "Training Epoch: 2 [29380/49669]\tLoss: 5369.5576\n",
      "Training Epoch: 2 [29400/49669]\tLoss: 6803.2939\n",
      "Training Epoch: 2 [29420/49669]\tLoss: 5563.1299\n",
      "Training Epoch: 2 [29440/49669]\tLoss: 6400.3545\n",
      "Training Epoch: 2 [29460/49669]\tLoss: 6150.7124\n",
      "Training Epoch: 2 [29480/49669]\tLoss: 6415.8511\n",
      "Training Epoch: 2 [29500/49669]\tLoss: 5837.3994\n",
      "Training Epoch: 2 [29520/49669]\tLoss: 5331.5552\n",
      "Training Epoch: 2 [29540/49669]\tLoss: 5568.7124\n",
      "Training Epoch: 2 [29560/49669]\tLoss: 5633.5688\n",
      "Training Epoch: 2 [29580/49669]\tLoss: 6192.9668\n",
      "Training Epoch: 2 [29600/49669]\tLoss: 6386.9463\n",
      "Training Epoch: 2 [29620/49669]\tLoss: 5973.2178\n",
      "Training Epoch: 2 [29640/49669]\tLoss: 6677.5161\n",
      "Training Epoch: 2 [29660/49669]\tLoss: 6420.7905\n",
      "Training Epoch: 2 [29680/49669]\tLoss: 6174.2241\n",
      "Training Epoch: 2 [29700/49669]\tLoss: 6202.3398\n",
      "Training Epoch: 2 [29720/49669]\tLoss: 6178.7319\n",
      "Training Epoch: 2 [29740/49669]\tLoss: 5164.9424\n",
      "Training Epoch: 2 [29760/49669]\tLoss: 6795.4478\n",
      "Training Epoch: 2 [29780/49669]\tLoss: 6633.5303\n",
      "Training Epoch: 2 [29800/49669]\tLoss: 5639.3262\n",
      "Training Epoch: 2 [29820/49669]\tLoss: 5723.6841\n",
      "Training Epoch: 2 [29840/49669]\tLoss: 5828.9556\n",
      "Training Epoch: 2 [29860/49669]\tLoss: 6811.1216\n",
      "Training Epoch: 2 [29880/49669]\tLoss: 5073.0947\n",
      "Training Epoch: 2 [29900/49669]\tLoss: 5935.4897\n",
      "Training Epoch: 2 [29920/49669]\tLoss: 6043.8545\n",
      "Training Epoch: 2 [29940/49669]\tLoss: 5662.8018\n",
      "Training Epoch: 2 [29960/49669]\tLoss: 4730.9395\n",
      "Training Epoch: 2 [29980/49669]\tLoss: 5330.4399\n",
      "Training Epoch: 2 [30000/49669]\tLoss: 6434.1855\n",
      "Training Epoch: 2 [30020/49669]\tLoss: 5413.1182\n",
      "Training Epoch: 2 [30040/49669]\tLoss: 4135.6777\n",
      "Training Epoch: 2 [30060/49669]\tLoss: 5500.4028\n",
      "Training Epoch: 2 [30080/49669]\tLoss: 5931.2876\n",
      "Training Epoch: 2 [30100/49669]\tLoss: 6156.3369\n",
      "Training Epoch: 2 [30120/49669]\tLoss: 5684.1460\n",
      "Training Epoch: 2 [30140/49669]\tLoss: 5537.0352\n",
      "Training Epoch: 2 [30160/49669]\tLoss: 5821.3462\n",
      "Training Epoch: 2 [30180/49669]\tLoss: 5013.2749\n",
      "Training Epoch: 2 [30200/49669]\tLoss: 5502.5977\n",
      "Training Epoch: 2 [30220/49669]\tLoss: 5858.6914\n",
      "Training Epoch: 2 [30240/49669]\tLoss: 5986.9160\n",
      "Training Epoch: 2 [30260/49669]\tLoss: 6125.3364\n",
      "Training Epoch: 2 [30280/49669]\tLoss: 6635.3306\n",
      "Training Epoch: 2 [30300/49669]\tLoss: 5594.1084\n",
      "Training Epoch: 2 [30320/49669]\tLoss: 5162.2261\n",
      "Training Epoch: 2 [30340/49669]\tLoss: 5676.9722\n",
      "Training Epoch: 2 [30360/49669]\tLoss: 6230.2397\n",
      "Training Epoch: 2 [30380/49669]\tLoss: 5556.9609\n",
      "Training Epoch: 2 [30400/49669]\tLoss: 5320.6914\n",
      "Training Epoch: 2 [30420/49669]\tLoss: 5475.7734\n",
      "Training Epoch: 2 [30440/49669]\tLoss: 5073.6689\n",
      "Training Epoch: 2 [30460/49669]\tLoss: 5919.2886\n",
      "Training Epoch: 2 [30480/49669]\tLoss: 6424.4038\n",
      "Training Epoch: 2 [30500/49669]\tLoss: 5162.1821\n",
      "Training Epoch: 2 [30520/49669]\tLoss: 5292.7368\n",
      "Training Epoch: 2 [30540/49669]\tLoss: 5625.5015\n",
      "Training Epoch: 2 [30560/49669]\tLoss: 6222.8545\n",
      "Training Epoch: 2 [30580/49669]\tLoss: 6551.3174\n",
      "Training Epoch: 2 [30600/49669]\tLoss: 5740.2100\n",
      "Training Epoch: 2 [30620/49669]\tLoss: 5467.9961\n",
      "Training Epoch: 2 [30640/49669]\tLoss: 5036.3242\n",
      "Training Epoch: 2 [30660/49669]\tLoss: 5229.1470\n",
      "Training Epoch: 2 [30680/49669]\tLoss: 5880.5317\n",
      "Training Epoch: 2 [30700/49669]\tLoss: 5136.7788\n",
      "Training Epoch: 2 [30720/49669]\tLoss: 5255.4487\n",
      "Training Epoch: 2 [30740/49669]\tLoss: 5239.6504\n",
      "Training Epoch: 2 [30760/49669]\tLoss: 5597.3550\n",
      "Training Epoch: 2 [30780/49669]\tLoss: 5755.5796\n",
      "Training Epoch: 2 [30800/49669]\tLoss: 6300.7949\n",
      "Training Epoch: 2 [30820/49669]\tLoss: 5433.7432\n",
      "Training Epoch: 2 [30840/49669]\tLoss: 5766.4448\n",
      "Training Epoch: 2 [30860/49669]\tLoss: 5561.9082\n",
      "Training Epoch: 2 [30880/49669]\tLoss: 5568.2700\n",
      "Training Epoch: 2 [30900/49669]\tLoss: 5683.6255\n",
      "Training Epoch: 2 [30920/49669]\tLoss: 4755.8379\n",
      "Training Epoch: 2 [30940/49669]\tLoss: 5306.0962\n",
      "Training Epoch: 2 [30960/49669]\tLoss: 5870.7124\n",
      "Training Epoch: 2 [30980/49669]\tLoss: 5633.7446\n",
      "Training Epoch: 2 [31000/49669]\tLoss: 5620.2798\n",
      "Training Epoch: 2 [31020/49669]\tLoss: 5637.5181\n",
      "Training Epoch: 2 [31040/49669]\tLoss: 6015.5664\n",
      "Training Epoch: 2 [31060/49669]\tLoss: 5492.0361\n",
      "Training Epoch: 2 [31080/49669]\tLoss: 5321.2046\n",
      "Training Epoch: 2 [31100/49669]\tLoss: 6124.9326\n",
      "Training Epoch: 2 [31120/49669]\tLoss: 5604.5723\n",
      "Training Epoch: 2 [31140/49669]\tLoss: 5915.0352\n",
      "Training Epoch: 2 [31160/49669]\tLoss: 5287.5908\n",
      "Training Epoch: 2 [31180/49669]\tLoss: 6007.6494\n",
      "Training Epoch: 2 [31200/49669]\tLoss: 5648.1855\n",
      "Training Epoch: 2 [31220/49669]\tLoss: 4917.8154\n",
      "Training Epoch: 2 [31240/49669]\tLoss: 6041.8140\n",
      "Training Epoch: 2 [31260/49669]\tLoss: 5149.7568\n",
      "Training Epoch: 2 [31280/49669]\tLoss: 5142.2520\n",
      "Training Epoch: 2 [31300/49669]\tLoss: 5643.0391\n",
      "Training Epoch: 2 [31320/49669]\tLoss: 6293.1553\n",
      "Training Epoch: 2 [31340/49669]\tLoss: 5395.0088\n",
      "Training Epoch: 2 [31360/49669]\tLoss: 5628.9194\n",
      "Training Epoch: 2 [31380/49669]\tLoss: 5071.1470\n",
      "Training Epoch: 2 [31400/49669]\tLoss: 5368.6289\n",
      "Training Epoch: 2 [31420/49669]\tLoss: 5782.1973\n",
      "Training Epoch: 2 [31440/49669]\tLoss: 5449.4033\n",
      "Training Epoch: 2 [31460/49669]\tLoss: 5449.6245\n",
      "Training Epoch: 2 [31480/49669]\tLoss: 6096.3823\n",
      "Training Epoch: 2 [31500/49669]\tLoss: 5319.2192\n",
      "Training Epoch: 2 [31520/49669]\tLoss: 5328.0581\n",
      "Training Epoch: 2 [31540/49669]\tLoss: 5839.1602\n",
      "Training Epoch: 2 [31560/49669]\tLoss: 6075.1118\n",
      "Training Epoch: 2 [31580/49669]\tLoss: 5271.3823\n",
      "Training Epoch: 2 [31600/49669]\tLoss: 4954.5474\n",
      "Training Epoch: 2 [31620/49669]\tLoss: 5736.6104\n",
      "Training Epoch: 2 [31640/49669]\tLoss: 5700.6294\n",
      "Training Epoch: 2 [31660/49669]\tLoss: 5768.9150\n",
      "Training Epoch: 2 [31680/49669]\tLoss: 4996.7578\n",
      "Training Epoch: 2 [31700/49669]\tLoss: 6386.7915\n",
      "Training Epoch: 2 [31720/49669]\tLoss: 5331.9302\n",
      "Training Epoch: 2 [31740/49669]\tLoss: 5102.6045\n",
      "Training Epoch: 2 [31760/49669]\tLoss: 4767.0601\n",
      "Training Epoch: 2 [31780/49669]\tLoss: 6021.7568\n",
      "Training Epoch: 2 [31800/49669]\tLoss: 5707.8306\n",
      "Training Epoch: 2 [31820/49669]\tLoss: 5487.2896\n",
      "Training Epoch: 2 [31840/49669]\tLoss: 6420.9814\n",
      "Training Epoch: 2 [31860/49669]\tLoss: 4974.5264\n",
      "Training Epoch: 2 [31880/49669]\tLoss: 6273.4521\n",
      "Training Epoch: 2 [31900/49669]\tLoss: 5847.8647\n",
      "Training Epoch: 2 [31920/49669]\tLoss: 6526.0493\n",
      "Training Epoch: 2 [31940/49669]\tLoss: 5434.3135\n",
      "Training Epoch: 2 [31960/49669]\tLoss: 5205.0522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [31980/49669]\tLoss: 5861.3823\n",
      "Training Epoch: 2 [32000/49669]\tLoss: 5719.8960\n",
      "Training Epoch: 2 [32020/49669]\tLoss: 5124.1289\n",
      "Training Epoch: 2 [32040/49669]\tLoss: 6175.3438\n",
      "Training Epoch: 2 [32060/49669]\tLoss: 5569.2769\n",
      "Training Epoch: 2 [32080/49669]\tLoss: 5476.1646\n",
      "Training Epoch: 2 [32100/49669]\tLoss: 6288.1650\n",
      "Training Epoch: 2 [32120/49669]\tLoss: 5361.2168\n",
      "Training Epoch: 2 [32140/49669]\tLoss: 4881.2676\n",
      "Training Epoch: 2 [32160/49669]\tLoss: 5319.8726\n",
      "Training Epoch: 2 [32180/49669]\tLoss: 4810.3086\n",
      "Training Epoch: 2 [32200/49669]\tLoss: 6006.7144\n",
      "Training Epoch: 2 [32220/49669]\tLoss: 5555.7002\n",
      "Training Epoch: 2 [32240/49669]\tLoss: 5553.1860\n",
      "Training Epoch: 2 [32260/49669]\tLoss: 4982.4082\n",
      "Training Epoch: 2 [32280/49669]\tLoss: 5433.2749\n",
      "Training Epoch: 2 [32300/49669]\tLoss: 5801.3447\n",
      "Training Epoch: 2 [32320/49669]\tLoss: 4643.5112\n",
      "Training Epoch: 2 [32340/49669]\tLoss: 5573.7686\n",
      "Training Epoch: 2 [32360/49669]\tLoss: 5751.0142\n",
      "Training Epoch: 2 [32380/49669]\tLoss: 5472.0913\n",
      "Training Epoch: 2 [32400/49669]\tLoss: 5024.7412\n",
      "Training Epoch: 2 [32420/49669]\tLoss: 5038.7603\n",
      "Training Epoch: 2 [32440/49669]\tLoss: 5166.6724\n",
      "Training Epoch: 2 [32460/49669]\tLoss: 5382.6172\n",
      "Training Epoch: 2 [32480/49669]\tLoss: 5230.9331\n",
      "Training Epoch: 2 [32500/49669]\tLoss: 5343.0903\n",
      "Training Epoch: 2 [32520/49669]\tLoss: 4755.8247\n",
      "Training Epoch: 2 [32540/49669]\tLoss: 5289.7798\n",
      "Training Epoch: 2 [32560/49669]\tLoss: 5961.0928\n",
      "Training Epoch: 2 [32580/49669]\tLoss: 5271.8711\n",
      "Training Epoch: 2 [32600/49669]\tLoss: 5370.4575\n",
      "Training Epoch: 2 [32620/49669]\tLoss: 4749.1904\n",
      "Training Epoch: 2 [32640/49669]\tLoss: 5206.5454\n",
      "Training Epoch: 2 [32660/49669]\tLoss: 5795.1074\n",
      "Training Epoch: 2 [32680/49669]\tLoss: 5271.2920\n",
      "Training Epoch: 2 [32700/49669]\tLoss: 5930.3193\n",
      "Training Epoch: 2 [32720/49669]\tLoss: 5968.4624\n",
      "Training Epoch: 2 [32740/49669]\tLoss: 5180.2275\n",
      "Training Epoch: 2 [32760/49669]\tLoss: 5827.7349\n",
      "Training Epoch: 2 [32780/49669]\tLoss: 5212.2705\n",
      "Training Epoch: 2 [32800/49669]\tLoss: 5457.2388\n",
      "Training Epoch: 2 [32820/49669]\tLoss: 5493.0098\n",
      "Training Epoch: 2 [32840/49669]\tLoss: 4754.0088\n",
      "Training Epoch: 2 [32860/49669]\tLoss: 6102.8550\n",
      "Training Epoch: 2 [32880/49669]\tLoss: 6405.3496\n",
      "Training Epoch: 2 [32900/49669]\tLoss: 5647.4741\n",
      "Training Epoch: 2 [32920/49669]\tLoss: 5816.4790\n",
      "Training Epoch: 2 [32940/49669]\tLoss: 5832.5229\n",
      "Training Epoch: 2 [32960/49669]\tLoss: 5561.1812\n",
      "Training Epoch: 2 [32980/49669]\tLoss: 5365.4873\n",
      "Training Epoch: 2 [33000/49669]\tLoss: 4738.4795\n",
      "Training Epoch: 2 [33020/49669]\tLoss: 5932.8076\n",
      "Training Epoch: 2 [33040/49669]\tLoss: 5854.7310\n",
      "Training Epoch: 2 [33060/49669]\tLoss: 5671.0859\n",
      "Training Epoch: 2 [33080/49669]\tLoss: 6069.1992\n",
      "Training Epoch: 2 [33100/49669]\tLoss: 5590.9126\n",
      "Training Epoch: 2 [33120/49669]\tLoss: 4877.3452\n",
      "Training Epoch: 2 [33140/49669]\tLoss: 5788.0352\n",
      "Training Epoch: 2 [33160/49669]\tLoss: 5770.0791\n",
      "Training Epoch: 2 [33180/49669]\tLoss: 4932.4287\n",
      "Training Epoch: 2 [33200/49669]\tLoss: 5944.6636\n",
      "Training Epoch: 2 [33220/49669]\tLoss: 5425.6553\n",
      "Training Epoch: 2 [33240/49669]\tLoss: 5818.4219\n",
      "Training Epoch: 2 [33260/49669]\tLoss: 5225.3721\n",
      "Training Epoch: 2 [33280/49669]\tLoss: 5419.3789\n",
      "Training Epoch: 2 [33300/49669]\tLoss: 5473.7163\n",
      "Training Epoch: 2 [33320/49669]\tLoss: 5522.2979\n",
      "Training Epoch: 2 [33340/49669]\tLoss: 5133.5381\n",
      "Training Epoch: 2 [33360/49669]\tLoss: 4540.7002\n",
      "Training Epoch: 2 [33380/49669]\tLoss: 5457.7251\n",
      "Training Epoch: 2 [33400/49669]\tLoss: 5491.8511\n",
      "Training Epoch: 2 [33420/49669]\tLoss: 5093.4067\n",
      "Training Epoch: 2 [33440/49669]\tLoss: 5781.6709\n",
      "Training Epoch: 2 [33460/49669]\tLoss: 5157.3984\n",
      "Training Epoch: 2 [33480/49669]\tLoss: 5062.1904\n",
      "Training Epoch: 2 [33500/49669]\tLoss: 5449.0864\n",
      "Training Epoch: 2 [33520/49669]\tLoss: 5344.2935\n",
      "Training Epoch: 2 [33540/49669]\tLoss: 5682.1621\n",
      "Training Epoch: 2 [33560/49669]\tLoss: 5295.3960\n",
      "Training Epoch: 2 [33580/49669]\tLoss: 5390.7778\n",
      "Training Epoch: 2 [33600/49669]\tLoss: 5532.3408\n",
      "Training Epoch: 2 [33620/49669]\tLoss: 5042.4961\n",
      "Training Epoch: 2 [33640/49669]\tLoss: 5405.3013\n",
      "Training Epoch: 2 [33660/49669]\tLoss: 5592.2441\n",
      "Training Epoch: 2 [33680/49669]\tLoss: 5574.7397\n",
      "Training Epoch: 2 [33700/49669]\tLoss: 5365.4565\n",
      "Training Epoch: 2 [33720/49669]\tLoss: 5315.8970\n",
      "Training Epoch: 2 [33740/49669]\tLoss: 5429.2729\n",
      "Training Epoch: 2 [33760/49669]\tLoss: 5994.1089\n",
      "Training Epoch: 2 [33780/49669]\tLoss: 5749.3032\n",
      "Training Epoch: 2 [33800/49669]\tLoss: 3963.8779\n",
      "Training Epoch: 2 [33820/49669]\tLoss: 5241.2954\n",
      "Training Epoch: 2 [33840/49669]\tLoss: 5489.9087\n",
      "Training Epoch: 2 [33860/49669]\tLoss: 5225.6206\n",
      "Training Epoch: 2 [33880/49669]\tLoss: 4973.3916\n",
      "Training Epoch: 2 [33900/49669]\tLoss: 5896.2822\n",
      "Training Epoch: 2 [33920/49669]\tLoss: 5843.0835\n",
      "Training Epoch: 2 [33940/49669]\tLoss: 5318.5850\n",
      "Training Epoch: 2 [33960/49669]\tLoss: 5277.9326\n",
      "Training Epoch: 2 [33980/49669]\tLoss: 5680.2324\n",
      "Training Epoch: 2 [34000/49669]\tLoss: 4755.5605\n",
      "Training Epoch: 2 [34020/49669]\tLoss: 4951.9985\n",
      "Training Epoch: 2 [34040/49669]\tLoss: 6250.1504\n",
      "Training Epoch: 2 [34060/49669]\tLoss: 4858.8950\n",
      "Training Epoch: 2 [34080/49669]\tLoss: 4434.5405\n",
      "Training Epoch: 2 [34100/49669]\tLoss: 5681.2393\n",
      "Training Epoch: 2 [34120/49669]\tLoss: 5218.8828\n",
      "Training Epoch: 2 [34140/49669]\tLoss: 4912.6997\n",
      "Training Epoch: 2 [34160/49669]\tLoss: 4969.2930\n",
      "Training Epoch: 2 [34180/49669]\tLoss: 5337.8989\n",
      "Training Epoch: 2 [34200/49669]\tLoss: 4756.9961\n",
      "Training Epoch: 2 [34220/49669]\tLoss: 5329.6514\n",
      "Training Epoch: 2 [34240/49669]\tLoss: 5800.7588\n",
      "Training Epoch: 2 [34260/49669]\tLoss: 5481.3076\n",
      "Training Epoch: 2 [34280/49669]\tLoss: 4356.2188\n",
      "Training Epoch: 2 [34300/49669]\tLoss: 5967.2388\n",
      "Training Epoch: 2 [34320/49669]\tLoss: 5594.7251\n",
      "Training Epoch: 2 [34340/49669]\tLoss: 5691.2256\n",
      "Training Epoch: 2 [34360/49669]\tLoss: 5494.1880\n",
      "Training Epoch: 2 [34380/49669]\tLoss: 4501.4404\n",
      "Training Epoch: 2 [34400/49669]\tLoss: 4822.1421\n",
      "Training Epoch: 2 [34420/49669]\tLoss: 5639.1953\n",
      "Training Epoch: 2 [34440/49669]\tLoss: 5115.1099\n",
      "Training Epoch: 2 [34460/49669]\tLoss: 5236.1914\n",
      "Training Epoch: 2 [34480/49669]\tLoss: 5310.6235\n",
      "Training Epoch: 2 [34500/49669]\tLoss: 5119.5562\n",
      "Training Epoch: 2 [34520/49669]\tLoss: 5229.0117\n",
      "Training Epoch: 2 [34540/49669]\tLoss: 4523.7495\n",
      "Training Epoch: 2 [34560/49669]\tLoss: 6118.0625\n",
      "Training Epoch: 2 [34580/49669]\tLoss: 5295.2476\n",
      "Training Epoch: 2 [34600/49669]\tLoss: 5575.2827\n",
      "Training Epoch: 2 [34620/49669]\tLoss: 5065.7432\n",
      "Training Epoch: 2 [34640/49669]\tLoss: 5796.2002\n",
      "Training Epoch: 2 [34660/49669]\tLoss: 4197.9243\n",
      "Training Epoch: 2 [34680/49669]\tLoss: 5578.4658\n",
      "Training Epoch: 2 [34700/49669]\tLoss: 4910.4541\n",
      "Training Epoch: 2 [34720/49669]\tLoss: 5655.7163\n",
      "Training Epoch: 2 [34740/49669]\tLoss: 4290.4619\n",
      "Training Epoch: 2 [34760/49669]\tLoss: 4659.4541\n",
      "Training Epoch: 2 [34780/49669]\tLoss: 4774.4795\n",
      "Training Epoch: 2 [34800/49669]\tLoss: 5980.1162\n",
      "Training Epoch: 2 [34820/49669]\tLoss: 5521.8057\n",
      "Training Epoch: 2 [34840/49669]\tLoss: 5430.3301\n",
      "Training Epoch: 2 [34860/49669]\tLoss: 4895.6611\n",
      "Training Epoch: 2 [34880/49669]\tLoss: 5243.4971\n",
      "Training Epoch: 2 [34900/49669]\tLoss: 5174.7349\n",
      "Training Epoch: 2 [34920/49669]\tLoss: 5017.3486\n",
      "Training Epoch: 2 [34940/49669]\tLoss: 5560.5425\n",
      "Training Epoch: 2 [34960/49669]\tLoss: 5284.9204\n",
      "Training Epoch: 2 [34980/49669]\tLoss: 5189.5718\n",
      "Training Epoch: 2 [35000/49669]\tLoss: 5171.9214\n",
      "Training Epoch: 2 [35020/49669]\tLoss: 4542.3564\n",
      "Training Epoch: 2 [35040/49669]\tLoss: 4611.6387\n",
      "Training Epoch: 2 [35060/49669]\tLoss: 5334.7891\n",
      "Training Epoch: 2 [35080/49669]\tLoss: 4862.7051\n",
      "Training Epoch: 2 [35100/49669]\tLoss: 5244.9399\n",
      "Training Epoch: 2 [35120/49669]\tLoss: 4392.7725\n",
      "Training Epoch: 2 [35140/49669]\tLoss: 5661.8789\n",
      "Training Epoch: 2 [35160/49669]\tLoss: 4344.3579\n",
      "Training Epoch: 2 [35180/49669]\tLoss: 5313.5229\n",
      "Training Epoch: 2 [35200/49669]\tLoss: 5134.4756\n",
      "Training Epoch: 2 [35220/49669]\tLoss: 4939.7700\n",
      "Training Epoch: 2 [35240/49669]\tLoss: 5272.2896\n",
      "Training Epoch: 2 [35260/49669]\tLoss: 5400.9258\n",
      "Training Epoch: 2 [35280/49669]\tLoss: 4947.1426\n",
      "Training Epoch: 2 [35300/49669]\tLoss: 5420.9775\n",
      "Training Epoch: 2 [35320/49669]\tLoss: 5198.6235\n",
      "Training Epoch: 2 [35340/49669]\tLoss: 4957.4546\n",
      "Training Epoch: 2 [35360/49669]\tLoss: 5514.2412\n",
      "Training Epoch: 2 [35380/49669]\tLoss: 5499.5176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [35400/49669]\tLoss: 4852.8579\n",
      "Training Epoch: 2 [35420/49669]\tLoss: 4400.9204\n",
      "Training Epoch: 2 [35440/49669]\tLoss: 4960.1753\n",
      "Training Epoch: 2 [35460/49669]\tLoss: 5215.3413\n",
      "Training Epoch: 2 [35480/49669]\tLoss: 4585.7959\n",
      "Training Epoch: 2 [35500/49669]\tLoss: 5695.8218\n",
      "Training Epoch: 2 [35520/49669]\tLoss: 4039.8669\n",
      "Training Epoch: 2 [35540/49669]\tLoss: 3989.8967\n",
      "Training Epoch: 2 [35560/49669]\tLoss: 4763.1333\n",
      "Training Epoch: 2 [35580/49669]\tLoss: 5866.4775\n",
      "Training Epoch: 2 [35600/49669]\tLoss: 5720.5859\n",
      "Training Epoch: 2 [35620/49669]\tLoss: 4713.3662\n",
      "Training Epoch: 2 [35640/49669]\tLoss: 5822.3823\n",
      "Training Epoch: 2 [35660/49669]\tLoss: 5442.6309\n",
      "Training Epoch: 2 [35680/49669]\tLoss: 4845.4199\n",
      "Training Epoch: 2 [35700/49669]\tLoss: 5404.4829\n",
      "Training Epoch: 2 [35720/49669]\tLoss: 4933.4785\n",
      "Training Epoch: 2 [35740/49669]\tLoss: 5308.7925\n",
      "Training Epoch: 2 [35760/49669]\tLoss: 4694.6245\n",
      "Training Epoch: 2 [35780/49669]\tLoss: 4940.2007\n",
      "Training Epoch: 2 [35800/49669]\tLoss: 4766.9062\n",
      "Training Epoch: 2 [35820/49669]\tLoss: 4767.3267\n",
      "Training Epoch: 2 [35840/49669]\tLoss: 5094.2534\n",
      "Training Epoch: 2 [35860/49669]\tLoss: 4423.0708\n",
      "Training Epoch: 2 [35880/49669]\tLoss: 5400.3389\n",
      "Training Epoch: 2 [35900/49669]\tLoss: 4862.3804\n",
      "Training Epoch: 2 [35920/49669]\tLoss: 5153.0005\n",
      "Training Epoch: 2 [35940/49669]\tLoss: 5311.8872\n",
      "Training Epoch: 2 [35960/49669]\tLoss: 4837.6812\n",
      "Training Epoch: 2 [35980/49669]\tLoss: 4708.3247\n",
      "Training Epoch: 2 [36000/49669]\tLoss: 5180.9917\n",
      "Training Epoch: 2 [36020/49669]\tLoss: 5718.3652\n",
      "Training Epoch: 2 [36040/49669]\tLoss: 4937.8965\n",
      "Training Epoch: 2 [36060/49669]\tLoss: 5018.1655\n",
      "Training Epoch: 2 [36080/49669]\tLoss: 5255.8643\n",
      "Training Epoch: 2 [36100/49669]\tLoss: 4730.3228\n",
      "Training Epoch: 2 [36120/49669]\tLoss: 5062.7832\n",
      "Training Epoch: 2 [36140/49669]\tLoss: 5231.1157\n",
      "Training Epoch: 2 [36160/49669]\tLoss: 4449.5894\n",
      "Training Epoch: 2 [36180/49669]\tLoss: 4880.1460\n",
      "Training Epoch: 2 [36200/49669]\tLoss: 5074.0381\n",
      "Training Epoch: 2 [36220/49669]\tLoss: 5010.1128\n",
      "Training Epoch: 2 [36240/49669]\tLoss: 4761.2627\n",
      "Training Epoch: 2 [36260/49669]\tLoss: 4889.4790\n",
      "Training Epoch: 2 [36280/49669]\tLoss: 6091.9951\n",
      "Training Epoch: 2 [36300/49669]\tLoss: 4600.9966\n",
      "Training Epoch: 2 [36320/49669]\tLoss: 4684.7754\n",
      "Training Epoch: 2 [36340/49669]\tLoss: 5312.9355\n",
      "Training Epoch: 2 [36360/49669]\tLoss: 4617.0068\n",
      "Training Epoch: 2 [36380/49669]\tLoss: 4968.7495\n",
      "Training Epoch: 2 [36400/49669]\tLoss: 4015.7205\n",
      "Training Epoch: 2 [36420/49669]\tLoss: 4515.4502\n",
      "Training Epoch: 2 [36440/49669]\tLoss: 4573.7334\n",
      "Training Epoch: 2 [36460/49669]\tLoss: 5256.8428\n",
      "Training Epoch: 2 [36480/49669]\tLoss: 5188.4390\n",
      "Training Epoch: 2 [36500/49669]\tLoss: 4864.5527\n",
      "Training Epoch: 2 [36520/49669]\tLoss: 5029.0693\n",
      "Training Epoch: 2 [36540/49669]\tLoss: 4060.1628\n",
      "Training Epoch: 2 [36560/49669]\tLoss: 5202.8252\n",
      "Training Epoch: 2 [36580/49669]\tLoss: 4829.8735\n",
      "Training Epoch: 2 [36600/49669]\tLoss: 5114.8560\n",
      "Training Epoch: 2 [36620/49669]\tLoss: 3964.2429\n",
      "Training Epoch: 2 [36640/49669]\tLoss: 4742.8848\n",
      "Training Epoch: 2 [36660/49669]\tLoss: 5242.2705\n",
      "Training Epoch: 2 [36680/49669]\tLoss: 6047.5767\n",
      "Training Epoch: 2 [36700/49669]\tLoss: 5125.9995\n",
      "Training Epoch: 2 [36720/49669]\tLoss: 5186.8125\n",
      "Training Epoch: 2 [36740/49669]\tLoss: 5276.3560\n",
      "Training Epoch: 2 [36760/49669]\tLoss: 4293.5234\n",
      "Training Epoch: 2 [36780/49669]\tLoss: 4935.5981\n",
      "Training Epoch: 2 [36800/49669]\tLoss: 4758.4976\n",
      "Training Epoch: 2 [36820/49669]\tLoss: 3932.2617\n",
      "Training Epoch: 2 [36840/49669]\tLoss: 5287.1318\n",
      "Training Epoch: 2 [36860/49669]\tLoss: 3951.8745\n",
      "Training Epoch: 2 [36880/49669]\tLoss: 4517.1455\n",
      "Training Epoch: 2 [36900/49669]\tLoss: 5323.2114\n",
      "Training Epoch: 2 [36920/49669]\tLoss: 5291.3398\n",
      "Training Epoch: 2 [36940/49669]\tLoss: 4321.3491\n",
      "Training Epoch: 2 [36960/49669]\tLoss: 4924.5732\n",
      "Training Epoch: 2 [36980/49669]\tLoss: 4815.3726\n",
      "Training Epoch: 2 [37000/49669]\tLoss: 4793.3110\n",
      "Training Epoch: 2 [37020/49669]\tLoss: 4679.9468\n",
      "Training Epoch: 2 [37040/49669]\tLoss: 4809.0977\n",
      "Training Epoch: 2 [37060/49669]\tLoss: 4527.7158\n",
      "Training Epoch: 2 [37080/49669]\tLoss: 5121.5728\n",
      "Training Epoch: 2 [37100/49669]\tLoss: 4811.9668\n",
      "Training Epoch: 2 [37120/49669]\tLoss: 4537.5981\n",
      "Training Epoch: 2 [37140/49669]\tLoss: 4728.5293\n",
      "Training Epoch: 2 [37160/49669]\tLoss: 4851.8789\n",
      "Training Epoch: 2 [37180/49669]\tLoss: 4723.6821\n",
      "Training Epoch: 2 [37200/49669]\tLoss: 4966.2236\n",
      "Training Epoch: 2 [37220/49669]\tLoss: 4463.9805\n",
      "Training Epoch: 2 [37240/49669]\tLoss: 5038.5420\n",
      "Training Epoch: 2 [37260/49669]\tLoss: 5081.1323\n",
      "Training Epoch: 2 [37280/49669]\tLoss: 5344.5801\n",
      "Training Epoch: 2 [37300/49669]\tLoss: 5225.4688\n",
      "Training Epoch: 2 [37320/49669]\tLoss: 4817.4165\n",
      "Training Epoch: 2 [37340/49669]\tLoss: 4806.2158\n",
      "Training Epoch: 2 [37360/49669]\tLoss: 4614.6104\n",
      "Training Epoch: 2 [37380/49669]\tLoss: 4977.1450\n",
      "Training Epoch: 2 [37400/49669]\tLoss: 5070.2241\n",
      "Training Epoch: 2 [37420/49669]\tLoss: 5118.4590\n",
      "Training Epoch: 2 [37440/49669]\tLoss: 4371.2964\n",
      "Training Epoch: 2 [37460/49669]\tLoss: 5381.5112\n",
      "Training Epoch: 2 [37480/49669]\tLoss: 4785.5776\n",
      "Training Epoch: 2 [37500/49669]\tLoss: 4878.5396\n",
      "Training Epoch: 2 [37520/49669]\tLoss: 4906.4429\n",
      "Training Epoch: 2 [37540/49669]\tLoss: 4689.9253\n",
      "Training Epoch: 2 [37560/49669]\tLoss: 4734.0884\n",
      "Training Epoch: 2 [37580/49669]\tLoss: 5196.0068\n",
      "Training Epoch: 2 [37600/49669]\tLoss: 5230.1753\n",
      "Training Epoch: 2 [37620/49669]\tLoss: 5066.6455\n",
      "Training Epoch: 2 [37640/49669]\tLoss: 4719.3740\n",
      "Training Epoch: 2 [37660/49669]\tLoss: 4166.7241\n",
      "Training Epoch: 2 [37680/49669]\tLoss: 4808.6206\n",
      "Training Epoch: 2 [37700/49669]\tLoss: 5655.8809\n",
      "Training Epoch: 2 [37720/49669]\tLoss: 4540.7354\n",
      "Training Epoch: 2 [37740/49669]\tLoss: 4793.7324\n",
      "Training Epoch: 2 [37760/49669]\tLoss: 4536.3813\n",
      "Training Epoch: 2 [37780/49669]\tLoss: 4984.6284\n",
      "Training Epoch: 2 [37800/49669]\tLoss: 4732.9492\n",
      "Training Epoch: 2 [37820/49669]\tLoss: 4638.2227\n",
      "Training Epoch: 2 [37840/49669]\tLoss: 5104.2080\n",
      "Training Epoch: 2 [37860/49669]\tLoss: 4708.5957\n",
      "Training Epoch: 2 [37880/49669]\tLoss: 4876.1055\n",
      "Training Epoch: 2 [37900/49669]\tLoss: 5181.1426\n",
      "Training Epoch: 2 [37920/49669]\tLoss: 4643.7197\n",
      "Training Epoch: 2 [37940/49669]\tLoss: 4196.0273\n",
      "Training Epoch: 2 [37960/49669]\tLoss: 4088.8958\n",
      "Training Epoch: 2 [37980/49669]\tLoss: 4567.4062\n",
      "Training Epoch: 2 [38000/49669]\tLoss: 5471.7646\n",
      "Training Epoch: 2 [38020/49669]\tLoss: 4423.3828\n",
      "Training Epoch: 2 [38040/49669]\tLoss: 4656.5962\n",
      "Training Epoch: 2 [38060/49669]\tLoss: 5115.0464\n",
      "Training Epoch: 2 [38080/49669]\tLoss: 4515.3584\n",
      "Training Epoch: 2 [38100/49669]\tLoss: 4247.1021\n",
      "Training Epoch: 2 [38120/49669]\tLoss: 4690.5796\n",
      "Training Epoch: 2 [38140/49669]\tLoss: 4763.7896\n",
      "Training Epoch: 2 [38160/49669]\tLoss: 4851.1270\n",
      "Training Epoch: 2 [38180/49669]\tLoss: 5026.8335\n",
      "Training Epoch: 2 [38200/49669]\tLoss: 4683.5693\n",
      "Training Epoch: 2 [38220/49669]\tLoss: 4787.3853\n",
      "Training Epoch: 2 [38240/49669]\tLoss: 4390.8018\n",
      "Training Epoch: 2 [38260/49669]\tLoss: 4835.6021\n",
      "Training Epoch: 2 [38280/49669]\tLoss: 4530.9829\n",
      "Training Epoch: 2 [38300/49669]\tLoss: 4479.3877\n",
      "Training Epoch: 2 [38320/49669]\tLoss: 4965.4834\n",
      "Training Epoch: 2 [38340/49669]\tLoss: 5166.2573\n",
      "Training Epoch: 2 [38360/49669]\tLoss: 4394.0166\n",
      "Training Epoch: 2 [38380/49669]\tLoss: 4994.5640\n",
      "Training Epoch: 2 [38400/49669]\tLoss: 3953.2649\n",
      "Training Epoch: 2 [38420/49669]\tLoss: 5325.2646\n",
      "Training Epoch: 2 [38440/49669]\tLoss: 5114.6455\n",
      "Training Epoch: 2 [38460/49669]\tLoss: 4682.2451\n",
      "Training Epoch: 2 [38480/49669]\tLoss: 4640.5933\n",
      "Training Epoch: 2 [38500/49669]\tLoss: 4633.1001\n",
      "Training Epoch: 2 [38520/49669]\tLoss: 4641.7085\n",
      "Training Epoch: 2 [38540/49669]\tLoss: 4953.3467\n",
      "Training Epoch: 2 [38560/49669]\tLoss: 5268.4282\n",
      "Training Epoch: 2 [38580/49669]\tLoss: 5055.9453\n",
      "Training Epoch: 2 [38600/49669]\tLoss: 4552.5225\n",
      "Training Epoch: 2 [38620/49669]\tLoss: 4757.4448\n",
      "Training Epoch: 2 [38640/49669]\tLoss: 4972.0381\n",
      "Training Epoch: 2 [38660/49669]\tLoss: 4873.3242\n",
      "Training Epoch: 2 [38680/49669]\tLoss: 4345.4873\n",
      "Training Epoch: 2 [38700/49669]\tLoss: 5435.4995\n",
      "Training Epoch: 2 [38720/49669]\tLoss: 4123.9800\n",
      "Training Epoch: 2 [38740/49669]\tLoss: 3835.2979\n",
      "Training Epoch: 2 [38760/49669]\tLoss: 4723.1006\n",
      "Training Epoch: 2 [38780/49669]\tLoss: 4052.7192\n",
      "Training Epoch: 2 [38800/49669]\tLoss: 4806.3994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [38820/49669]\tLoss: 4694.7544\n",
      "Training Epoch: 2 [38840/49669]\tLoss: 4962.8120\n",
      "Training Epoch: 2 [38860/49669]\tLoss: 6355.8550\n",
      "Training Epoch: 2 [38880/49669]\tLoss: 4971.7324\n",
      "Training Epoch: 2 [38900/49669]\tLoss: 4945.2285\n",
      "Training Epoch: 2 [38920/49669]\tLoss: 4080.8838\n",
      "Training Epoch: 2 [38940/49669]\tLoss: 4581.2847\n",
      "Training Epoch: 2 [38960/49669]\tLoss: 4206.3125\n",
      "Training Epoch: 2 [38980/49669]\tLoss: 5464.7524\n",
      "Training Epoch: 2 [39000/49669]\tLoss: 4889.7578\n",
      "Training Epoch: 2 [39020/49669]\tLoss: 4908.2090\n",
      "Training Epoch: 2 [39040/49669]\tLoss: 4234.7251\n",
      "Training Epoch: 2 [39060/49669]\tLoss: 5159.1221\n",
      "Training Epoch: 2 [39080/49669]\tLoss: 5227.1846\n",
      "Training Epoch: 2 [39100/49669]\tLoss: 4471.3999\n",
      "Training Epoch: 2 [39120/49669]\tLoss: 5290.0342\n",
      "Training Epoch: 2 [39140/49669]\tLoss: 4232.5596\n",
      "Training Epoch: 2 [39160/49669]\tLoss: 4690.3901\n",
      "Training Epoch: 2 [39180/49669]\tLoss: 4515.2642\n",
      "Training Epoch: 2 [39200/49669]\tLoss: 4673.6709\n",
      "Training Epoch: 2 [39220/49669]\tLoss: 4780.5962\n",
      "Training Epoch: 2 [39240/49669]\tLoss: 4788.1655\n",
      "Training Epoch: 2 [39260/49669]\tLoss: 4061.4114\n",
      "Training Epoch: 2 [39280/49669]\tLoss: 5163.0010\n",
      "Training Epoch: 2 [39300/49669]\tLoss: 4221.4102\n",
      "Training Epoch: 2 [39320/49669]\tLoss: 4557.5474\n",
      "Training Epoch: 2 [39340/49669]\tLoss: 5049.0376\n",
      "Training Epoch: 2 [39360/49669]\tLoss: 4803.3721\n",
      "Training Epoch: 2 [39380/49669]\tLoss: 5025.3208\n",
      "Training Epoch: 2 [39400/49669]\tLoss: 5237.2617\n",
      "Training Epoch: 2 [39420/49669]\tLoss: 4815.3530\n",
      "Training Epoch: 2 [39440/49669]\tLoss: 4844.9517\n",
      "Training Epoch: 2 [39460/49669]\tLoss: 3779.5012\n",
      "Training Epoch: 2 [39480/49669]\tLoss: 4809.9072\n",
      "Training Epoch: 2 [39500/49669]\tLoss: 4080.5081\n",
      "Training Epoch: 2 [39520/49669]\tLoss: 4594.0146\n",
      "Training Epoch: 2 [39540/49669]\tLoss: 4868.0518\n",
      "Training Epoch: 2 [39560/49669]\tLoss: 4527.6030\n",
      "Training Epoch: 2 [39580/49669]\tLoss: 4712.7124\n",
      "Training Epoch: 2 [39600/49669]\tLoss: 5110.1758\n",
      "Training Epoch: 2 [39620/49669]\tLoss: 4404.2690\n",
      "Training Epoch: 2 [39640/49669]\tLoss: 4862.8013\n",
      "Training Epoch: 2 [39660/49669]\tLoss: 4749.2812\n",
      "Training Epoch: 2 [39680/49669]\tLoss: 4857.2437\n",
      "Training Epoch: 2 [39700/49669]\tLoss: 4984.2114\n",
      "Training Epoch: 2 [39720/49669]\tLoss: 5004.2104\n",
      "Training Epoch: 2 [39740/49669]\tLoss: 4533.5464\n",
      "Training Epoch: 2 [39760/49669]\tLoss: 4986.8740\n",
      "Training Epoch: 2 [39780/49669]\tLoss: 4258.2778\n",
      "Training Epoch: 2 [39800/49669]\tLoss: 4576.6904\n",
      "Training Epoch: 2 [39820/49669]\tLoss: 4811.4536\n",
      "Training Epoch: 2 [39840/49669]\tLoss: 4650.8115\n",
      "Training Epoch: 2 [39860/49669]\tLoss: 4281.8940\n",
      "Training Epoch: 2 [39880/49669]\tLoss: 4801.0747\n",
      "Training Epoch: 2 [39900/49669]\tLoss: 4620.5342\n",
      "Training Epoch: 2 [39920/49669]\tLoss: 4171.3286\n",
      "Training Epoch: 2 [39940/49669]\tLoss: 5069.0098\n",
      "Training Epoch: 2 [39960/49669]\tLoss: 4311.0864\n",
      "Training Epoch: 2 [39980/49669]\tLoss: 4640.3130\n",
      "Training Epoch: 2 [40000/49669]\tLoss: 4564.1069\n",
      "Training Epoch: 2 [40020/49669]\tLoss: 4501.3989\n",
      "Training Epoch: 2 [40040/49669]\tLoss: 4548.5610\n",
      "Training Epoch: 2 [40060/49669]\tLoss: 4284.5771\n",
      "Training Epoch: 2 [40080/49669]\tLoss: 4125.7788\n",
      "Training Epoch: 2 [40100/49669]\tLoss: 3973.4116\n",
      "Training Epoch: 2 [40120/49669]\tLoss: 4274.3560\n",
      "Training Epoch: 2 [40140/49669]\tLoss: 4587.7500\n",
      "Training Epoch: 2 [40160/49669]\tLoss: 4908.7729\n",
      "Training Epoch: 2 [40180/49669]\tLoss: 4327.7285\n",
      "Training Epoch: 2 [40200/49669]\tLoss: 4441.7705\n",
      "Training Epoch: 2 [40220/49669]\tLoss: 4620.2017\n",
      "Training Epoch: 2 [40240/49669]\tLoss: 4284.6216\n",
      "Training Epoch: 2 [40260/49669]\tLoss: 4417.3032\n",
      "Training Epoch: 2 [40280/49669]\tLoss: 5121.1460\n",
      "Training Epoch: 2 [40300/49669]\tLoss: 4217.2397\n",
      "Training Epoch: 2 [40320/49669]\tLoss: 4608.0894\n",
      "Training Epoch: 2 [40340/49669]\tLoss: 4664.8545\n",
      "Training Epoch: 2 [40360/49669]\tLoss: 4493.6816\n",
      "Training Epoch: 2 [40380/49669]\tLoss: 4508.7563\n",
      "Training Epoch: 2 [40400/49669]\tLoss: 4180.9907\n",
      "Training Epoch: 2 [40420/49669]\tLoss: 4016.2122\n",
      "Training Epoch: 2 [40440/49669]\tLoss: 4737.2383\n",
      "Training Epoch: 2 [40460/49669]\tLoss: 4343.0078\n",
      "Training Epoch: 2 [40480/49669]\tLoss: 4262.8755\n",
      "Training Epoch: 2 [40500/49669]\tLoss: 4600.0054\n",
      "Training Epoch: 2 [40520/49669]\tLoss: 4659.5508\n",
      "Training Epoch: 2 [40540/49669]\tLoss: 4523.1729\n",
      "Training Epoch: 2 [40560/49669]\tLoss: 4702.2949\n",
      "Training Epoch: 2 [40580/49669]\tLoss: 3997.1895\n",
      "Training Epoch: 2 [40600/49669]\tLoss: 4382.5020\n",
      "Training Epoch: 2 [40620/49669]\tLoss: 4568.5806\n",
      "Training Epoch: 2 [40640/49669]\tLoss: 4421.0513\n",
      "Training Epoch: 2 [40660/49669]\tLoss: 5040.0571\n",
      "Training Epoch: 2 [40680/49669]\tLoss: 4452.6436\n",
      "Training Epoch: 2 [40700/49669]\tLoss: 4942.8467\n",
      "Training Epoch: 2 [40720/49669]\tLoss: 5261.4653\n",
      "Training Epoch: 2 [40740/49669]\tLoss: 4855.0708\n",
      "Training Epoch: 2 [40760/49669]\tLoss: 4346.5239\n",
      "Training Epoch: 2 [40780/49669]\tLoss: 5220.3145\n",
      "Training Epoch: 2 [40800/49669]\tLoss: 4484.0547\n",
      "Training Epoch: 2 [40820/49669]\tLoss: 4727.9097\n",
      "Training Epoch: 2 [40840/49669]\tLoss: 4408.7852\n",
      "Training Epoch: 2 [40860/49669]\tLoss: 4567.1616\n",
      "Training Epoch: 2 [40880/49669]\tLoss: 4016.6890\n",
      "Training Epoch: 2 [40900/49669]\tLoss: 4397.7632\n",
      "Training Epoch: 2 [40920/49669]\tLoss: 4796.4580\n",
      "Training Epoch: 2 [40940/49669]\tLoss: 3717.0681\n",
      "Training Epoch: 2 [40960/49669]\tLoss: 5035.9316\n",
      "Training Epoch: 2 [40980/49669]\tLoss: 4339.3286\n",
      "Training Epoch: 2 [41000/49669]\tLoss: 4520.4233\n",
      "Training Epoch: 2 [41020/49669]\tLoss: 5023.3354\n",
      "Training Epoch: 2 [41040/49669]\tLoss: 4620.4062\n",
      "Training Epoch: 2 [41060/49669]\tLoss: 4655.4868\n",
      "Training Epoch: 2 [41080/49669]\tLoss: 4698.6621\n",
      "Training Epoch: 2 [41100/49669]\tLoss: 4855.9897\n",
      "Training Epoch: 2 [41120/49669]\tLoss: 4885.1533\n",
      "Training Epoch: 2 [41140/49669]\tLoss: 4777.8828\n",
      "Training Epoch: 2 [41160/49669]\tLoss: 4626.2163\n",
      "Training Epoch: 2 [41180/49669]\tLoss: 4208.4663\n",
      "Training Epoch: 2 [41200/49669]\tLoss: 3979.7527\n",
      "Training Epoch: 2 [41220/49669]\tLoss: 4573.5132\n",
      "Training Epoch: 2 [41240/49669]\tLoss: 4606.6201\n",
      "Training Epoch: 2 [41260/49669]\tLoss: 4097.9321\n",
      "Training Epoch: 2 [41280/49669]\tLoss: 4568.7354\n",
      "Training Epoch: 2 [41300/49669]\tLoss: 4289.1377\n",
      "Training Epoch: 2 [41320/49669]\tLoss: 4539.7456\n",
      "Training Epoch: 2 [41340/49669]\tLoss: 4979.7725\n",
      "Training Epoch: 2 [41360/49669]\tLoss: 4784.2803\n",
      "Training Epoch: 2 [41380/49669]\tLoss: 4587.8125\n",
      "Training Epoch: 2 [41400/49669]\tLoss: 4242.8779\n",
      "Training Epoch: 2 [41420/49669]\tLoss: 4219.7490\n",
      "Training Epoch: 2 [41440/49669]\tLoss: 4719.4336\n",
      "Training Epoch: 2 [41460/49669]\tLoss: 4291.7749\n",
      "Training Epoch: 2 [41480/49669]\tLoss: 3916.1519\n",
      "Training Epoch: 2 [41500/49669]\tLoss: 4870.1880\n",
      "Training Epoch: 2 [41520/49669]\tLoss: 4392.9155\n",
      "Training Epoch: 2 [41540/49669]\tLoss: 4407.1348\n",
      "Training Epoch: 2 [41560/49669]\tLoss: 5196.2510\n",
      "Training Epoch: 2 [41580/49669]\tLoss: 4499.6313\n",
      "Training Epoch: 2 [41600/49669]\tLoss: 5086.0220\n",
      "Training Epoch: 2 [41620/49669]\tLoss: 4661.7119\n",
      "Training Epoch: 2 [41640/49669]\tLoss: 3946.4319\n",
      "Training Epoch: 2 [41660/49669]\tLoss: 4332.3960\n",
      "Training Epoch: 2 [41680/49669]\tLoss: 4214.0518\n",
      "Training Epoch: 2 [41700/49669]\tLoss: 4323.3491\n",
      "Training Epoch: 2 [41720/49669]\tLoss: 4595.9795\n",
      "Training Epoch: 2 [41740/49669]\tLoss: 4139.6714\n",
      "Training Epoch: 2 [41760/49669]\tLoss: 4669.2227\n",
      "Training Epoch: 2 [41780/49669]\tLoss: 4427.3882\n",
      "Training Epoch: 2 [41800/49669]\tLoss: 4349.3242\n",
      "Training Epoch: 2 [41820/49669]\tLoss: 4988.0518\n",
      "Training Epoch: 2 [41840/49669]\tLoss: 4652.7651\n",
      "Training Epoch: 2 [41860/49669]\tLoss: 4325.9180\n",
      "Training Epoch: 2 [41880/49669]\tLoss: 5132.6685\n",
      "Training Epoch: 2 [41900/49669]\tLoss: 4281.6040\n",
      "Training Epoch: 2 [41920/49669]\tLoss: 4628.9683\n",
      "Training Epoch: 2 [41940/49669]\tLoss: 4282.6875\n",
      "Training Epoch: 2 [41960/49669]\tLoss: 4008.8313\n",
      "Training Epoch: 2 [41980/49669]\tLoss: 4475.2705\n",
      "Training Epoch: 2 [42000/49669]\tLoss: 4778.0176\n",
      "Training Epoch: 2 [42020/49669]\tLoss: 4073.9028\n",
      "Training Epoch: 2 [42040/49669]\tLoss: 4472.1641\n",
      "Training Epoch: 2 [42060/49669]\tLoss: 4951.2847\n",
      "Training Epoch: 2 [42080/49669]\tLoss: 4391.7002\n",
      "Training Epoch: 2 [42100/49669]\tLoss: 4033.2625\n",
      "Training Epoch: 2 [42120/49669]\tLoss: 4573.1548\n",
      "Training Epoch: 2 [42140/49669]\tLoss: 4451.1885\n",
      "Training Epoch: 2 [42160/49669]\tLoss: 3928.8384\n",
      "Training Epoch: 2 [42180/49669]\tLoss: 4759.2656\n",
      "Training Epoch: 2 [42200/49669]\tLoss: 4609.4141\n",
      "Training Epoch: 2 [42220/49669]\tLoss: 4860.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [42240/49669]\tLoss: 3484.4534\n",
      "Training Epoch: 2 [42260/49669]\tLoss: 4596.6206\n",
      "Training Epoch: 2 [42280/49669]\tLoss: 4382.8291\n",
      "Training Epoch: 2 [42300/49669]\tLoss: 4626.7012\n",
      "Training Epoch: 2 [42320/49669]\tLoss: 4822.3003\n",
      "Training Epoch: 2 [42340/49669]\tLoss: 4897.7769\n",
      "Training Epoch: 2 [42360/49669]\tLoss: 3871.3296\n",
      "Training Epoch: 2 [42380/49669]\tLoss: 4107.5410\n",
      "Training Epoch: 2 [42400/49669]\tLoss: 3932.7307\n",
      "Training Epoch: 2 [42420/49669]\tLoss: 3763.4814\n",
      "Training Epoch: 2 [42440/49669]\tLoss: 3929.0989\n",
      "Training Epoch: 2 [42460/49669]\tLoss: 4868.8306\n",
      "Training Epoch: 2 [42480/49669]\tLoss: 4349.8423\n",
      "Training Epoch: 2 [42500/49669]\tLoss: 3969.4050\n",
      "Training Epoch: 2 [42520/49669]\tLoss: 3995.7092\n",
      "Training Epoch: 2 [42540/49669]\tLoss: 4315.0078\n",
      "Training Epoch: 2 [42560/49669]\tLoss: 4374.5225\n",
      "Training Epoch: 2 [42580/49669]\tLoss: 4389.5122\n",
      "Training Epoch: 2 [42600/49669]\tLoss: 4191.4214\n",
      "Training Epoch: 2 [42620/49669]\tLoss: 4368.5083\n",
      "Training Epoch: 2 [42640/49669]\tLoss: 4601.8945\n",
      "Training Epoch: 2 [42660/49669]\tLoss: 4393.8506\n",
      "Training Epoch: 2 [42680/49669]\tLoss: 4472.6030\n",
      "Training Epoch: 2 [42700/49669]\tLoss: 4707.9404\n",
      "Training Epoch: 2 [42720/49669]\tLoss: 4486.4038\n",
      "Training Epoch: 2 [42740/49669]\tLoss: 4477.4170\n",
      "Training Epoch: 2 [42760/49669]\tLoss: 4220.8887\n",
      "Training Epoch: 2 [42780/49669]\tLoss: 4037.9092\n",
      "Training Epoch: 2 [42800/49669]\tLoss: 4766.0044\n",
      "Training Epoch: 2 [42820/49669]\tLoss: 4733.9258\n",
      "Training Epoch: 2 [42840/49669]\tLoss: 4425.6338\n",
      "Training Epoch: 2 [42860/49669]\tLoss: 4249.8462\n",
      "Training Epoch: 2 [42880/49669]\tLoss: 4243.4761\n",
      "Training Epoch: 2 [42900/49669]\tLoss: 4674.6484\n",
      "Training Epoch: 2 [42920/49669]\tLoss: 3506.5786\n",
      "Training Epoch: 2 [42940/49669]\tLoss: 4625.1309\n",
      "Training Epoch: 2 [42960/49669]\tLoss: 3952.3054\n",
      "Training Epoch: 2 [42980/49669]\tLoss: 3701.2432\n",
      "Training Epoch: 2 [43000/49669]\tLoss: 4134.5952\n",
      "Training Epoch: 2 [43020/49669]\tLoss: 4255.2393\n",
      "Training Epoch: 2 [43040/49669]\tLoss: 3863.8674\n",
      "Training Epoch: 2 [43060/49669]\tLoss: 4333.6113\n",
      "Training Epoch: 2 [43080/49669]\tLoss: 3652.8972\n",
      "Training Epoch: 2 [43100/49669]\tLoss: 4491.6143\n",
      "Training Epoch: 2 [43120/49669]\tLoss: 4384.9878\n",
      "Training Epoch: 2 [43140/49669]\tLoss: 3911.1648\n",
      "Training Epoch: 2 [43160/49669]\tLoss: 4729.8477\n",
      "Training Epoch: 2 [43180/49669]\tLoss: 4075.8357\n",
      "Training Epoch: 2 [43200/49669]\tLoss: 3761.7168\n",
      "Training Epoch: 2 [43220/49669]\tLoss: 3761.8662\n",
      "Training Epoch: 2 [43240/49669]\tLoss: 4318.2275\n",
      "Training Epoch: 2 [43260/49669]\tLoss: 3949.8828\n",
      "Training Epoch: 2 [43280/49669]\tLoss: 4499.7075\n",
      "Training Epoch: 2 [43300/49669]\tLoss: 4503.3364\n",
      "Training Epoch: 2 [43320/49669]\tLoss: 4378.5308\n",
      "Training Epoch: 2 [43340/49669]\tLoss: 4458.4062\n",
      "Training Epoch: 2 [43360/49669]\tLoss: 4215.4585\n",
      "Training Epoch: 2 [43380/49669]\tLoss: 4434.3237\n",
      "Training Epoch: 2 [43400/49669]\tLoss: 4271.6187\n",
      "Training Epoch: 2 [43420/49669]\tLoss: 4250.9263\n",
      "Training Epoch: 2 [43440/49669]\tLoss: 3936.1536\n",
      "Training Epoch: 2 [43460/49669]\tLoss: 4393.5210\n",
      "Training Epoch: 2 [43480/49669]\tLoss: 3975.8420\n",
      "Training Epoch: 2 [43500/49669]\tLoss: 4072.4524\n",
      "Training Epoch: 2 [43520/49669]\tLoss: 3806.0420\n",
      "Training Epoch: 2 [43540/49669]\tLoss: 4642.5610\n",
      "Training Epoch: 2 [43560/49669]\tLoss: 4432.1392\n",
      "Training Epoch: 2 [43580/49669]\tLoss: 4212.7031\n",
      "Training Epoch: 2 [43600/49669]\tLoss: 4433.4873\n",
      "Training Epoch: 2 [43620/49669]\tLoss: 4155.5449\n",
      "Training Epoch: 2 [43640/49669]\tLoss: 4358.0298\n",
      "Training Epoch: 2 [43660/49669]\tLoss: 4119.1260\n",
      "Training Epoch: 2 [43680/49669]\tLoss: 3957.1731\n",
      "Training Epoch: 2 [43700/49669]\tLoss: 4597.8237\n",
      "Training Epoch: 2 [43720/49669]\tLoss: 4348.5254\n",
      "Training Epoch: 2 [43740/49669]\tLoss: 4597.5996\n",
      "Training Epoch: 2 [43760/49669]\tLoss: 4281.9888\n",
      "Training Epoch: 2 [43780/49669]\tLoss: 4512.3662\n",
      "Training Epoch: 2 [43800/49669]\tLoss: 3627.0491\n",
      "Training Epoch: 2 [43820/49669]\tLoss: 4253.3550\n",
      "Training Epoch: 2 [43840/49669]\tLoss: 4430.1265\n",
      "Training Epoch: 2 [43860/49669]\tLoss: 4128.0376\n",
      "Training Epoch: 2 [43880/49669]\tLoss: 3404.4595\n",
      "Training Epoch: 2 [43900/49669]\tLoss: 4280.9229\n",
      "Training Epoch: 2 [43920/49669]\tLoss: 4338.0610\n",
      "Training Epoch: 2 [43940/49669]\tLoss: 4616.5132\n",
      "Training Epoch: 2 [43960/49669]\tLoss: 3790.5496\n",
      "Training Epoch: 2 [43980/49669]\tLoss: 3784.8706\n",
      "Training Epoch: 2 [44000/49669]\tLoss: 3784.7334\n",
      "Training Epoch: 2 [44020/49669]\tLoss: 3892.0029\n",
      "Training Epoch: 2 [44040/49669]\tLoss: 4368.4575\n",
      "Training Epoch: 2 [44060/49669]\tLoss: 3976.8599\n",
      "Training Epoch: 2 [44080/49669]\tLoss: 4408.7393\n",
      "Training Epoch: 2 [44100/49669]\tLoss: 4372.2529\n",
      "Training Epoch: 2 [44120/49669]\tLoss: 3956.6123\n",
      "Training Epoch: 2 [44140/49669]\tLoss: 4161.7056\n",
      "Training Epoch: 2 [44160/49669]\tLoss: 3954.1577\n",
      "Training Epoch: 2 [44180/49669]\tLoss: 4454.9731\n",
      "Training Epoch: 2 [44200/49669]\tLoss: 4111.0234\n",
      "Training Epoch: 2 [44220/49669]\tLoss: 4121.9951\n",
      "Training Epoch: 2 [44240/49669]\tLoss: 4308.7314\n",
      "Training Epoch: 2 [44260/49669]\tLoss: 4622.6084\n",
      "Training Epoch: 2 [44280/49669]\tLoss: 4539.9556\n",
      "Training Epoch: 2 [44300/49669]\tLoss: 3822.0132\n",
      "Training Epoch: 2 [44320/49669]\tLoss: 4726.4863\n",
      "Training Epoch: 2 [44340/49669]\tLoss: 4002.6741\n",
      "Training Epoch: 2 [44360/49669]\tLoss: 3730.4006\n",
      "Training Epoch: 2 [44380/49669]\tLoss: 4228.9727\n",
      "Training Epoch: 2 [44400/49669]\tLoss: 4068.1084\n",
      "Training Epoch: 2 [44420/49669]\tLoss: 4088.1914\n",
      "Training Epoch: 2 [44440/49669]\tLoss: 4192.2529\n",
      "Training Epoch: 2 [44460/49669]\tLoss: 4219.6704\n",
      "Training Epoch: 2 [44480/49669]\tLoss: 4436.9248\n",
      "Training Epoch: 2 [44500/49669]\tLoss: 4038.7859\n",
      "Training Epoch: 2 [44520/49669]\tLoss: 4103.2607\n",
      "Training Epoch: 2 [44540/49669]\tLoss: 4216.5283\n",
      "Training Epoch: 2 [44560/49669]\tLoss: 4293.8027\n",
      "Training Epoch: 2 [44580/49669]\tLoss: 3731.8291\n",
      "Training Epoch: 2 [44600/49669]\tLoss: 4492.3428\n",
      "Training Epoch: 2 [44620/49669]\tLoss: 3840.1917\n",
      "Training Epoch: 2 [44640/49669]\tLoss: 3880.2432\n",
      "Training Epoch: 2 [44660/49669]\tLoss: 4073.6047\n",
      "Training Epoch: 2 [44680/49669]\tLoss: 4206.8052\n",
      "Training Epoch: 2 [44700/49669]\tLoss: 4652.1245\n",
      "Training Epoch: 2 [44720/49669]\tLoss: 3286.8223\n",
      "Training Epoch: 2 [44740/49669]\tLoss: 3918.1819\n",
      "Training Epoch: 2 [44760/49669]\tLoss: 3970.2883\n",
      "Training Epoch: 2 [44780/49669]\tLoss: 4360.4141\n",
      "Training Epoch: 2 [44800/49669]\tLoss: 4270.9629\n",
      "Training Epoch: 2 [44820/49669]\tLoss: 4288.9893\n",
      "Training Epoch: 2 [44840/49669]\tLoss: 4057.6548\n",
      "Training Epoch: 2 [44860/49669]\tLoss: 3760.7751\n",
      "Training Epoch: 2 [44880/49669]\tLoss: 4432.0029\n",
      "Training Epoch: 2 [44900/49669]\tLoss: 4557.3335\n",
      "Training Epoch: 2 [44920/49669]\tLoss: 4055.2925\n",
      "Training Epoch: 2 [44940/49669]\tLoss: 4037.7690\n",
      "Training Epoch: 2 [44960/49669]\tLoss: 4266.2163\n",
      "Training Epoch: 2 [44980/49669]\tLoss: 4137.7134\n",
      "Training Epoch: 2 [45000/49669]\tLoss: 3457.2571\n",
      "Training Epoch: 2 [45020/49669]\tLoss: 4170.1685\n",
      "Training Epoch: 2 [45040/49669]\tLoss: 4489.7651\n",
      "Training Epoch: 2 [45060/49669]\tLoss: 4512.7090\n",
      "Training Epoch: 2 [45080/49669]\tLoss: 4348.2202\n",
      "Training Epoch: 2 [45100/49669]\tLoss: 3998.3228\n",
      "Training Epoch: 2 [45120/49669]\tLoss: 4068.1279\n",
      "Training Epoch: 2 [45140/49669]\tLoss: 3670.1946\n",
      "Training Epoch: 2 [45160/49669]\tLoss: 4256.5073\n",
      "Training Epoch: 2 [45180/49669]\tLoss: 4126.5986\n",
      "Training Epoch: 2 [45200/49669]\tLoss: 4105.2227\n",
      "Training Epoch: 2 [45220/49669]\tLoss: 4369.1421\n",
      "Training Epoch: 2 [45240/49669]\tLoss: 4007.6775\n",
      "Training Epoch: 2 [45260/49669]\tLoss: 4314.7529\n",
      "Training Epoch: 2 [45280/49669]\tLoss: 4443.3516\n",
      "Training Epoch: 2 [45300/49669]\tLoss: 4068.6199\n",
      "Training Epoch: 2 [45320/49669]\tLoss: 4182.2373\n",
      "Training Epoch: 2 [45340/49669]\tLoss: 4381.1611\n",
      "Training Epoch: 2 [45360/49669]\tLoss: 4487.7271\n",
      "Training Epoch: 2 [45380/49669]\tLoss: 4107.9795\n",
      "Training Epoch: 2 [45400/49669]\tLoss: 5252.1851\n",
      "Training Epoch: 2 [45420/49669]\tLoss: 4028.9097\n",
      "Training Epoch: 2 [45440/49669]\tLoss: 4496.4507\n",
      "Training Epoch: 2 [45460/49669]\tLoss: 4659.6724\n",
      "Training Epoch: 2 [45480/49669]\tLoss: 4788.7974\n",
      "Training Epoch: 2 [45500/49669]\tLoss: 4457.5312\n",
      "Training Epoch: 2 [45520/49669]\tLoss: 4017.4983\n",
      "Training Epoch: 2 [45540/49669]\tLoss: 4522.0518\n",
      "Training Epoch: 2 [45560/49669]\tLoss: 3919.2561\n",
      "Training Epoch: 2 [45580/49669]\tLoss: 3805.3884\n",
      "Training Epoch: 2 [45600/49669]\tLoss: 3935.1313\n",
      "Training Epoch: 2 [45620/49669]\tLoss: 4213.5020\n",
      "Training Epoch: 2 [45640/49669]\tLoss: 4214.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [45660/49669]\tLoss: 3958.8232\n",
      "Training Epoch: 2 [45680/49669]\tLoss: 4207.1909\n",
      "Training Epoch: 2 [45700/49669]\tLoss: 3597.6643\n",
      "Training Epoch: 2 [45720/49669]\tLoss: 3732.3037\n",
      "Training Epoch: 2 [45740/49669]\tLoss: 3923.7314\n",
      "Training Epoch: 2 [45760/49669]\tLoss: 4464.5571\n",
      "Training Epoch: 2 [45780/49669]\tLoss: 3504.6694\n",
      "Training Epoch: 2 [45800/49669]\tLoss: 4305.3027\n",
      "Training Epoch: 2 [45820/49669]\tLoss: 4907.8042\n",
      "Training Epoch: 2 [45840/49669]\tLoss: 3596.1567\n",
      "Training Epoch: 2 [45860/49669]\tLoss: 4158.1685\n",
      "Training Epoch: 2 [45880/49669]\tLoss: 4171.6763\n",
      "Training Epoch: 2 [45900/49669]\tLoss: 3888.1177\n",
      "Training Epoch: 2 [45920/49669]\tLoss: 4283.7969\n",
      "Training Epoch: 2 [45940/49669]\tLoss: 3796.4995\n",
      "Training Epoch: 2 [45960/49669]\tLoss: 4002.1646\n",
      "Training Epoch: 2 [45980/49669]\tLoss: 4276.4204\n",
      "Training Epoch: 2 [46000/49669]\tLoss: 4577.7427\n",
      "Training Epoch: 2 [46020/49669]\tLoss: 3994.4551\n",
      "Training Epoch: 2 [46040/49669]\tLoss: 4163.1543\n",
      "Training Epoch: 2 [46060/49669]\tLoss: 4100.1836\n",
      "Training Epoch: 2 [46080/49669]\tLoss: 3655.2302\n",
      "Training Epoch: 2 [46100/49669]\tLoss: 3579.3132\n",
      "Training Epoch: 2 [46120/49669]\tLoss: 3663.1160\n",
      "Training Epoch: 2 [46140/49669]\tLoss: 3749.9348\n",
      "Training Epoch: 2 [46160/49669]\tLoss: 4070.8445\n",
      "Training Epoch: 2 [46180/49669]\tLoss: 3996.4424\n",
      "Training Epoch: 2 [46200/49669]\tLoss: 4070.2734\n",
      "Training Epoch: 2 [46220/49669]\tLoss: 3504.8098\n",
      "Training Epoch: 2 [46240/49669]\tLoss: 4448.7798\n",
      "Training Epoch: 2 [46260/49669]\tLoss: 4041.6614\n",
      "Training Epoch: 2 [46280/49669]\tLoss: 3760.9546\n",
      "Training Epoch: 2 [46300/49669]\tLoss: 4485.9932\n",
      "Training Epoch: 2 [46320/49669]\tLoss: 3945.3057\n",
      "Training Epoch: 2 [46340/49669]\tLoss: 4030.2507\n",
      "Training Epoch: 2 [46360/49669]\tLoss: 3726.2053\n",
      "Training Epoch: 2 [46380/49669]\tLoss: 4340.7769\n",
      "Training Epoch: 2 [46400/49669]\tLoss: 4256.8071\n",
      "Training Epoch: 2 [46420/49669]\tLoss: 4342.3545\n",
      "Training Epoch: 2 [46440/49669]\tLoss: 4421.4731\n",
      "Training Epoch: 2 [46460/49669]\tLoss: 4224.2974\n",
      "Training Epoch: 2 [46480/49669]\tLoss: 4284.3140\n",
      "Training Epoch: 2 [46500/49669]\tLoss: 3786.5896\n",
      "Training Epoch: 2 [46520/49669]\tLoss: 4044.0657\n",
      "Training Epoch: 2 [46540/49669]\tLoss: 3876.2329\n",
      "Training Epoch: 2 [46560/49669]\tLoss: 3768.7698\n",
      "Training Epoch: 2 [46580/49669]\tLoss: 3926.6006\n",
      "Training Epoch: 2 [46600/49669]\tLoss: 4061.0134\n",
      "Training Epoch: 2 [46620/49669]\tLoss: 3942.3621\n",
      "Training Epoch: 2 [46640/49669]\tLoss: 4845.8970\n",
      "Training Epoch: 2 [46660/49669]\tLoss: 4038.6685\n",
      "Training Epoch: 2 [46680/49669]\tLoss: 3600.5740\n",
      "Training Epoch: 2 [46700/49669]\tLoss: 4175.9160\n",
      "Training Epoch: 2 [46720/49669]\tLoss: 3897.2881\n",
      "Training Epoch: 2 [46740/49669]\tLoss: 3979.9260\n",
      "Training Epoch: 2 [46760/49669]\tLoss: 4133.9531\n",
      "Training Epoch: 2 [46780/49669]\tLoss: 4004.5530\n",
      "Training Epoch: 2 [46800/49669]\tLoss: 3801.0071\n",
      "Training Epoch: 2 [46820/49669]\tLoss: 3604.9878\n",
      "Training Epoch: 2 [46840/49669]\tLoss: 4070.6714\n",
      "Training Epoch: 2 [46860/49669]\tLoss: 3647.1382\n",
      "Training Epoch: 2 [46880/49669]\tLoss: 4296.7520\n",
      "Training Epoch: 2 [46900/49669]\tLoss: 3671.6216\n",
      "Training Epoch: 2 [46920/49669]\tLoss: 4529.1123\n",
      "Training Epoch: 2 [46940/49669]\tLoss: 3654.6931\n",
      "Training Epoch: 2 [46960/49669]\tLoss: 3939.9937\n",
      "Training Epoch: 2 [46980/49669]\tLoss: 3551.0918\n",
      "Training Epoch: 2 [47000/49669]\tLoss: 4555.2812\n",
      "Training Epoch: 2 [47020/49669]\tLoss: 3599.3203\n",
      "Training Epoch: 2 [47040/49669]\tLoss: 3901.7585\n",
      "Training Epoch: 2 [47060/49669]\tLoss: 4038.9709\n",
      "Training Epoch: 2 [47080/49669]\tLoss: 3752.8367\n",
      "Training Epoch: 2 [47100/49669]\tLoss: 3811.3357\n",
      "Training Epoch: 2 [47120/49669]\tLoss: 3604.4648\n",
      "Training Epoch: 2 [47140/49669]\tLoss: 4127.4297\n",
      "Training Epoch: 2 [47160/49669]\tLoss: 4142.4248\n",
      "Training Epoch: 2 [47180/49669]\tLoss: 4329.2041\n",
      "Training Epoch: 2 [47200/49669]\tLoss: 3671.5403\n",
      "Training Epoch: 2 [47220/49669]\tLoss: 3551.1262\n",
      "Training Epoch: 2 [47240/49669]\tLoss: 4139.0576\n",
      "Training Epoch: 2 [47260/49669]\tLoss: 4172.4302\n",
      "Training Epoch: 2 [47280/49669]\tLoss: 3835.6677\n",
      "Training Epoch: 2 [47300/49669]\tLoss: 4559.6562\n",
      "Training Epoch: 2 [47320/49669]\tLoss: 3798.5222\n",
      "Training Epoch: 2 [47340/49669]\tLoss: 4177.1455\n",
      "Training Epoch: 2 [47360/49669]\tLoss: 4106.5269\n",
      "Training Epoch: 2 [47380/49669]\tLoss: 3816.8677\n",
      "Training Epoch: 2 [47400/49669]\tLoss: 3815.1111\n",
      "Training Epoch: 2 [47420/49669]\tLoss: 4053.3840\n",
      "Training Epoch: 2 [47440/49669]\tLoss: 4057.6943\n",
      "Training Epoch: 2 [47460/49669]\tLoss: 3796.2734\n",
      "Training Epoch: 2 [47480/49669]\tLoss: 3831.6448\n",
      "Training Epoch: 2 [47500/49669]\tLoss: 3997.7510\n",
      "Training Epoch: 2 [47520/49669]\tLoss: 4141.2295\n",
      "Training Epoch: 2 [47540/49669]\tLoss: 4203.1987\n",
      "Training Epoch: 2 [47560/49669]\tLoss: 3834.4924\n",
      "Training Epoch: 2 [47580/49669]\tLoss: 4552.6240\n",
      "Training Epoch: 2 [47600/49669]\tLoss: 3509.2471\n",
      "Training Epoch: 2 [47620/49669]\tLoss: 3717.9683\n",
      "Training Epoch: 2 [47640/49669]\tLoss: 4071.5757\n",
      "Training Epoch: 2 [47660/49669]\tLoss: 3786.9832\n",
      "Training Epoch: 2 [47680/49669]\tLoss: 3635.4956\n",
      "Training Epoch: 2 [47700/49669]\tLoss: 3807.9414\n",
      "Training Epoch: 2 [47720/49669]\tLoss: 4236.8325\n",
      "Training Epoch: 2 [47740/49669]\tLoss: 4002.2651\n",
      "Training Epoch: 2 [47760/49669]\tLoss: 4295.6899\n",
      "Training Epoch: 2 [47780/49669]\tLoss: 3883.6804\n",
      "Training Epoch: 2 [47800/49669]\tLoss: 3877.7949\n",
      "Training Epoch: 2 [47820/49669]\tLoss: 4132.3047\n",
      "Training Epoch: 2 [47840/49669]\tLoss: 3346.5630\n",
      "Training Epoch: 2 [47860/49669]\tLoss: 3880.3101\n",
      "Training Epoch: 2 [47880/49669]\tLoss: 4116.3984\n",
      "Training Epoch: 2 [47900/49669]\tLoss: 3531.3989\n",
      "Training Epoch: 2 [47920/49669]\tLoss: 3408.7754\n",
      "Training Epoch: 2 [47940/49669]\tLoss: 3691.6331\n",
      "Training Epoch: 2 [47960/49669]\tLoss: 4405.2964\n",
      "Training Epoch: 2 [47980/49669]\tLoss: 3396.5818\n",
      "Training Epoch: 2 [48000/49669]\tLoss: 4293.5249\n",
      "Training Epoch: 2 [48020/49669]\tLoss: 4325.6826\n",
      "Training Epoch: 2 [48040/49669]\tLoss: 3844.6389\n",
      "Training Epoch: 2 [48060/49669]\tLoss: 4023.7083\n",
      "Training Epoch: 2 [48080/49669]\tLoss: 3993.3262\n",
      "Training Epoch: 2 [48100/49669]\tLoss: 3895.4619\n",
      "Training Epoch: 2 [48120/49669]\tLoss: 3824.8040\n",
      "Training Epoch: 2 [48140/49669]\tLoss: 3965.8362\n",
      "Training Epoch: 2 [48160/49669]\tLoss: 4099.3057\n",
      "Training Epoch: 2 [48180/49669]\tLoss: 3459.7310\n",
      "Training Epoch: 2 [48200/49669]\tLoss: 4307.4497\n",
      "Training Epoch: 2 [48220/49669]\tLoss: 3861.7134\n",
      "Training Epoch: 2 [48240/49669]\tLoss: 3600.8167\n",
      "Training Epoch: 2 [48260/49669]\tLoss: 3657.2668\n",
      "Training Epoch: 2 [48280/49669]\tLoss: 4309.0459\n",
      "Training Epoch: 2 [48300/49669]\tLoss: 3999.9905\n",
      "Training Epoch: 2 [48320/49669]\tLoss: 3932.6462\n",
      "Training Epoch: 2 [48340/49669]\tLoss: 4011.6448\n",
      "Training Epoch: 2 [48360/49669]\tLoss: 3668.7283\n",
      "Training Epoch: 2 [48380/49669]\tLoss: 3423.8938\n",
      "Training Epoch: 2 [48400/49669]\tLoss: 3698.9102\n",
      "Training Epoch: 2 [48420/49669]\tLoss: 3881.4287\n",
      "Training Epoch: 2 [48440/49669]\tLoss: 3351.2378\n",
      "Training Epoch: 2 [48460/49669]\tLoss: 3811.7200\n",
      "Training Epoch: 2 [48480/49669]\tLoss: 3944.0657\n",
      "Training Epoch: 2 [48500/49669]\tLoss: 3404.7537\n",
      "Training Epoch: 2 [48520/49669]\tLoss: 4186.8994\n",
      "Training Epoch: 2 [48540/49669]\tLoss: 4566.8359\n",
      "Training Epoch: 2 [48560/49669]\tLoss: 4110.0010\n",
      "Training Epoch: 2 [48580/49669]\tLoss: 4071.7571\n",
      "Training Epoch: 2 [48600/49669]\tLoss: 3762.5454\n",
      "Training Epoch: 2 [48620/49669]\tLoss: 4341.5298\n",
      "Training Epoch: 2 [48640/49669]\tLoss: 3260.0979\n",
      "Training Epoch: 2 [48660/49669]\tLoss: 3505.0164\n",
      "Training Epoch: 2 [48680/49669]\tLoss: 3963.0378\n",
      "Training Epoch: 2 [48700/49669]\tLoss: 3021.9600\n",
      "Training Epoch: 2 [48720/49669]\tLoss: 4064.4868\n",
      "Training Epoch: 2 [48740/49669]\tLoss: 4175.6973\n",
      "Training Epoch: 2 [48760/49669]\tLoss: 3660.3213\n",
      "Training Epoch: 2 [48780/49669]\tLoss: 3682.9417\n",
      "Training Epoch: 2 [48800/49669]\tLoss: 3622.0186\n",
      "Training Epoch: 2 [48820/49669]\tLoss: 4178.2583\n",
      "Training Epoch: 2 [48840/49669]\tLoss: 3683.8884\n",
      "Training Epoch: 2 [48860/49669]\tLoss: 3833.6848\n",
      "Training Epoch: 2 [48880/49669]\tLoss: 3749.8184\n",
      "Training Epoch: 2 [48900/49669]\tLoss: 3835.5449\n",
      "Training Epoch: 2 [48920/49669]\tLoss: 4069.9263\n",
      "Training Epoch: 2 [48940/49669]\tLoss: 3902.2939\n",
      "Training Epoch: 2 [48960/49669]\tLoss: 3668.6497\n",
      "Training Epoch: 2 [48980/49669]\tLoss: 3855.2551\n",
      "Training Epoch: 2 [49000/49669]\tLoss: 3748.8184\n",
      "Training Epoch: 2 [49020/49669]\tLoss: 3264.4380\n",
      "Training Epoch: 2 [49040/49669]\tLoss: 4119.2144\n",
      "Training Epoch: 2 [49060/49669]\tLoss: 4116.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [49080/49669]\tLoss: 4070.1042\n",
      "Training Epoch: 2 [49100/49669]\tLoss: 4020.2742\n",
      "Training Epoch: 2 [49120/49669]\tLoss: 3708.2471\n",
      "Training Epoch: 2 [49140/49669]\tLoss: 4142.2549\n",
      "Training Epoch: 2 [49160/49669]\tLoss: 4147.6431\n",
      "Training Epoch: 2 [49180/49669]\tLoss: 4012.3076\n",
      "Training Epoch: 2 [49200/49669]\tLoss: 4612.1440\n",
      "Training Epoch: 2 [49220/49669]\tLoss: 3738.9907\n",
      "Training Epoch: 2 [49240/49669]\tLoss: 3743.4116\n",
      "Training Epoch: 2 [49260/49669]\tLoss: 4089.8088\n",
      "Training Epoch: 2 [49280/49669]\tLoss: 3791.3655\n",
      "Training Epoch: 2 [49300/49669]\tLoss: 3434.9346\n",
      "Training Epoch: 2 [49320/49669]\tLoss: 4001.5730\n",
      "Training Epoch: 2 [49340/49669]\tLoss: 3796.5833\n",
      "Training Epoch: 2 [49360/49669]\tLoss: 3828.2722\n",
      "Training Epoch: 2 [49380/49669]\tLoss: 4184.5723\n",
      "Training Epoch: 2 [49400/49669]\tLoss: 3814.2021\n",
      "Training Epoch: 2 [49420/49669]\tLoss: 3724.6111\n",
      "Training Epoch: 2 [49440/49669]\tLoss: 3873.1631\n",
      "Training Epoch: 2 [49460/49669]\tLoss: 3429.9543\n",
      "Training Epoch: 2 [49480/49669]\tLoss: 3786.2859\n",
      "Training Epoch: 2 [49500/49669]\tLoss: 4024.1624\n",
      "Training Epoch: 2 [49520/49669]\tLoss: 3814.0342\n",
      "Training Epoch: 2 [49540/49669]\tLoss: 4067.4651\n",
      "Training Epoch: 2 [49560/49669]\tLoss: 3641.5747\n",
      "Training Epoch: 2 [49580/49669]\tLoss: 4207.6577\n",
      "Training Epoch: 2 [49600/49669]\tLoss: 4385.1685\n",
      "Training Epoch: 2 [49620/49669]\tLoss: 3887.5713\n",
      "Training Epoch: 2 [49640/49669]\tLoss: 3445.4976\n",
      "Training Epoch: 2 [49660/49669]\tLoss: 3548.8674\n",
      "Training Epoch: 2 [49669/49669]\tLoss: 4086.8679\n",
      "Training Epoch: 2 [5519/5519]\tLoss: 3801.9000\n",
      "Training Epoch: 3 [20/49669]\tLoss: 3964.8005\n",
      "Training Epoch: 3 [40/49669]\tLoss: 3913.6545\n",
      "Training Epoch: 3 [60/49669]\tLoss: 3908.2290\n",
      "Training Epoch: 3 [80/49669]\tLoss: 3934.0063\n",
      "Training Epoch: 3 [100/49669]\tLoss: 3861.4629\n",
      "Training Epoch: 3 [120/49669]\tLoss: 3668.7554\n",
      "Training Epoch: 3 [140/49669]\tLoss: 3834.8809\n",
      "Training Epoch: 3 [160/49669]\tLoss: 4085.2224\n",
      "Training Epoch: 3 [180/49669]\tLoss: 3258.9617\n",
      "Training Epoch: 3 [200/49669]\tLoss: 4214.2925\n",
      "Training Epoch: 3 [220/49669]\tLoss: 4032.3992\n",
      "Training Epoch: 3 [240/49669]\tLoss: 3958.9395\n",
      "Training Epoch: 3 [260/49669]\tLoss: 3818.9331\n",
      "Training Epoch: 3 [280/49669]\tLoss: 4670.3423\n",
      "Training Epoch: 3 [300/49669]\tLoss: 3832.0203\n",
      "Training Epoch: 3 [320/49669]\tLoss: 3947.8977\n",
      "Training Epoch: 3 [340/49669]\tLoss: 4036.8452\n",
      "Training Epoch: 3 [360/49669]\tLoss: 3712.6895\n",
      "Training Epoch: 3 [380/49669]\tLoss: 4038.4773\n",
      "Training Epoch: 3 [400/49669]\tLoss: 3710.4531\n",
      "Training Epoch: 3 [420/49669]\tLoss: 3759.1885\n",
      "Training Epoch: 3 [440/49669]\tLoss: 3696.6960\n",
      "Training Epoch: 3 [460/49669]\tLoss: 4163.4927\n",
      "Training Epoch: 3 [480/49669]\tLoss: 4299.9692\n",
      "Training Epoch: 3 [500/49669]\tLoss: 3817.5347\n",
      "Training Epoch: 3 [520/49669]\tLoss: 3962.0081\n",
      "Training Epoch: 3 [540/49669]\tLoss: 3634.4211\n",
      "Training Epoch: 3 [560/49669]\tLoss: 3973.1082\n",
      "Training Epoch: 3 [580/49669]\tLoss: 3652.4170\n",
      "Training Epoch: 3 [600/49669]\tLoss: 3591.0405\n",
      "Training Epoch: 3 [620/49669]\tLoss: 3637.6265\n",
      "Training Epoch: 3 [640/49669]\tLoss: 4057.9382\n",
      "Training Epoch: 3 [660/49669]\tLoss: 4709.7637\n",
      "Training Epoch: 3 [680/49669]\tLoss: 3808.3848\n",
      "Training Epoch: 3 [700/49669]\tLoss: 3264.3420\n",
      "Training Epoch: 3 [720/49669]\tLoss: 3623.6174\n",
      "Training Epoch: 3 [740/49669]\tLoss: 3877.3250\n",
      "Training Epoch: 3 [760/49669]\tLoss: 3495.9993\n",
      "Training Epoch: 3 [780/49669]\tLoss: 3537.0212\n",
      "Training Epoch: 3 [800/49669]\tLoss: 3652.3877\n",
      "Training Epoch: 3 [820/49669]\tLoss: 3360.6814\n",
      "Training Epoch: 3 [840/49669]\tLoss: 3781.0417\n",
      "Training Epoch: 3 [860/49669]\tLoss: 3715.2014\n",
      "Training Epoch: 3 [880/49669]\tLoss: 4050.1069\n",
      "Training Epoch: 3 [900/49669]\tLoss: 4091.6982\n",
      "Training Epoch: 3 [920/49669]\tLoss: 3661.3237\n",
      "Training Epoch: 3 [940/49669]\tLoss: 3566.9829\n",
      "Training Epoch: 3 [960/49669]\tLoss: 3981.3247\n",
      "Training Epoch: 3 [980/49669]\tLoss: 3643.8027\n",
      "Training Epoch: 3 [1000/49669]\tLoss: 3630.0916\n",
      "Training Epoch: 3 [1020/49669]\tLoss: 3562.5635\n",
      "Training Epoch: 3 [1040/49669]\tLoss: 3793.6523\n",
      "Training Epoch: 3 [1060/49669]\tLoss: 3896.0308\n",
      "Training Epoch: 3 [1080/49669]\tLoss: 3948.4597\n",
      "Training Epoch: 3 [1100/49669]\tLoss: 3713.2341\n",
      "Training Epoch: 3 [1120/49669]\tLoss: 3903.9673\n",
      "Training Epoch: 3 [1140/49669]\tLoss: 3694.9133\n",
      "Training Epoch: 3 [1160/49669]\tLoss: 3459.3164\n",
      "Training Epoch: 3 [1180/49669]\tLoss: 3936.3582\n",
      "Training Epoch: 3 [1200/49669]\tLoss: 3799.5337\n",
      "Training Epoch: 3 [1220/49669]\tLoss: 2899.9946\n",
      "Training Epoch: 3 [1240/49669]\tLoss: 3940.3674\n",
      "Training Epoch: 3 [1260/49669]\tLoss: 3624.5518\n",
      "Training Epoch: 3 [1280/49669]\tLoss: 3471.0503\n",
      "Training Epoch: 3 [1300/49669]\tLoss: 4228.5249\n",
      "Training Epoch: 3 [1320/49669]\tLoss: 4291.8193\n",
      "Training Epoch: 3 [1340/49669]\tLoss: 4178.4663\n",
      "Training Epoch: 3 [1360/49669]\tLoss: 3858.8015\n",
      "Training Epoch: 3 [1380/49669]\tLoss: 4079.4482\n",
      "Training Epoch: 3 [1400/49669]\tLoss: 3591.0232\n",
      "Training Epoch: 3 [1420/49669]\tLoss: 3797.4302\n",
      "Training Epoch: 3 [1440/49669]\tLoss: 3927.1553\n",
      "Training Epoch: 3 [1460/49669]\tLoss: 2853.6506\n",
      "Training Epoch: 3 [1480/49669]\tLoss: 3884.0654\n",
      "Training Epoch: 3 [1500/49669]\tLoss: 3653.9404\n",
      "Training Epoch: 3 [1520/49669]\tLoss: 4209.3530\n",
      "Training Epoch: 3 [1540/49669]\tLoss: 3740.3687\n",
      "Training Epoch: 3 [1560/49669]\tLoss: 3983.6172\n",
      "Training Epoch: 3 [1580/49669]\tLoss: 3539.4595\n",
      "Training Epoch: 3 [1600/49669]\tLoss: 3609.4573\n",
      "Training Epoch: 3 [1620/49669]\tLoss: 3984.8027\n",
      "Training Epoch: 3 [1640/49669]\tLoss: 3900.7666\n",
      "Training Epoch: 3 [1660/49669]\tLoss: 3497.1294\n",
      "Training Epoch: 3 [1680/49669]\tLoss: 3665.9514\n",
      "Training Epoch: 3 [1700/49669]\tLoss: 3541.2708\n",
      "Training Epoch: 3 [1720/49669]\tLoss: 3672.0442\n",
      "Training Epoch: 3 [1740/49669]\tLoss: 3864.9587\n",
      "Training Epoch: 3 [1760/49669]\tLoss: 3482.5615\n",
      "Training Epoch: 3 [1780/49669]\tLoss: 4216.3516\n",
      "Training Epoch: 3 [1800/49669]\tLoss: 3746.9666\n",
      "Training Epoch: 3 [1820/49669]\tLoss: 3625.3442\n",
      "Training Epoch: 3 [1840/49669]\tLoss: 3912.7905\n",
      "Training Epoch: 3 [1860/49669]\tLoss: 3172.3865\n",
      "Training Epoch: 3 [1880/49669]\tLoss: 3584.9548\n",
      "Training Epoch: 3 [1900/49669]\tLoss: 4043.7434\n",
      "Training Epoch: 3 [1920/49669]\tLoss: 3563.2852\n",
      "Training Epoch: 3 [1940/49669]\tLoss: 4029.8574\n",
      "Training Epoch: 3 [1960/49669]\tLoss: 3639.0623\n",
      "Training Epoch: 3 [1980/49669]\tLoss: 3857.5811\n",
      "Training Epoch: 3 [2000/49669]\tLoss: 3710.7380\n",
      "Training Epoch: 3 [2020/49669]\tLoss: 3161.0837\n",
      "Training Epoch: 3 [2040/49669]\tLoss: 3587.4536\n",
      "Training Epoch: 3 [2060/49669]\tLoss: 3512.8877\n",
      "Training Epoch: 3 [2080/49669]\tLoss: 4071.1565\n",
      "Training Epoch: 3 [2100/49669]\tLoss: 3902.2932\n",
      "Training Epoch: 3 [2120/49669]\tLoss: 4120.9424\n",
      "Training Epoch: 3 [2140/49669]\tLoss: 3268.5364\n",
      "Training Epoch: 3 [2160/49669]\tLoss: 3887.3896\n",
      "Training Epoch: 3 [2180/49669]\tLoss: 3252.4277\n",
      "Training Epoch: 3 [2200/49669]\tLoss: 3848.1245\n",
      "Training Epoch: 3 [2220/49669]\tLoss: 3880.9592\n",
      "Training Epoch: 3 [2240/49669]\tLoss: 3909.1316\n",
      "Training Epoch: 3 [2260/49669]\tLoss: 3605.7324\n",
      "Training Epoch: 3 [2280/49669]\tLoss: 3427.3958\n",
      "Training Epoch: 3 [2300/49669]\tLoss: 3185.1028\n",
      "Training Epoch: 3 [2320/49669]\tLoss: 3378.8525\n",
      "Training Epoch: 3 [2340/49669]\tLoss: 3630.0605\n",
      "Training Epoch: 3 [2360/49669]\tLoss: 3738.1868\n",
      "Training Epoch: 3 [2380/49669]\tLoss: 3976.6416\n",
      "Training Epoch: 3 [2400/49669]\tLoss: 3674.8267\n",
      "Training Epoch: 3 [2420/49669]\tLoss: 3557.8240\n",
      "Training Epoch: 3 [2440/49669]\tLoss: 3562.3574\n",
      "Training Epoch: 3 [2460/49669]\tLoss: 3308.2275\n",
      "Training Epoch: 3 [2480/49669]\tLoss: 3561.0962\n",
      "Training Epoch: 3 [2500/49669]\tLoss: 4096.0015\n",
      "Training Epoch: 3 [2520/49669]\tLoss: 3902.8708\n",
      "Training Epoch: 3 [2540/49669]\tLoss: 3640.2334\n",
      "Training Epoch: 3 [2560/49669]\tLoss: 3877.4163\n",
      "Training Epoch: 3 [2580/49669]\tLoss: 3806.3132\n",
      "Training Epoch: 3 [2600/49669]\tLoss: 3943.0759\n",
      "Training Epoch: 3 [2620/49669]\tLoss: 3763.2485\n",
      "Training Epoch: 3 [2640/49669]\tLoss: 3620.1448\n",
      "Training Epoch: 3 [2660/49669]\tLoss: 3857.5640\n",
      "Training Epoch: 3 [2680/49669]\tLoss: 3949.8215\n",
      "Training Epoch: 3 [2700/49669]\tLoss: 3377.7224\n",
      "Training Epoch: 3 [2720/49669]\tLoss: 3813.0449\n",
      "Training Epoch: 3 [2740/49669]\tLoss: 3691.1694\n",
      "Training Epoch: 3 [2760/49669]\tLoss: 3640.2788\n",
      "Training Epoch: 3 [2780/49669]\tLoss: 4150.7441\n",
      "Training Epoch: 3 [2800/49669]\tLoss: 4266.2495\n",
      "Training Epoch: 3 [2820/49669]\tLoss: 3720.5481\n",
      "Training Epoch: 3 [2840/49669]\tLoss: 3438.5000\n",
      "Training Epoch: 3 [2860/49669]\tLoss: 3839.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [2880/49669]\tLoss: 3900.1912\n",
      "Training Epoch: 3 [2900/49669]\tLoss: 3793.6262\n",
      "Training Epoch: 3 [2920/49669]\tLoss: 3735.8538\n",
      "Training Epoch: 3 [2940/49669]\tLoss: 4124.7798\n",
      "Training Epoch: 3 [2960/49669]\tLoss: 3753.7388\n",
      "Training Epoch: 3 [2980/49669]\tLoss: 3945.0693\n",
      "Training Epoch: 3 [3000/49669]\tLoss: 3416.4087\n",
      "Training Epoch: 3 [3020/49669]\tLoss: 4047.5469\n",
      "Training Epoch: 3 [3040/49669]\tLoss: 3665.6184\n",
      "Training Epoch: 3 [3060/49669]\tLoss: 3647.3586\n",
      "Training Epoch: 3 [3080/49669]\tLoss: 3185.1401\n",
      "Training Epoch: 3 [3100/49669]\tLoss: 3929.0515\n",
      "Training Epoch: 3 [3120/49669]\tLoss: 3570.4058\n",
      "Training Epoch: 3 [3140/49669]\tLoss: 3373.6306\n",
      "Training Epoch: 3 [3160/49669]\tLoss: 3740.0125\n",
      "Training Epoch: 3 [3180/49669]\tLoss: 3974.0862\n",
      "Training Epoch: 3 [3200/49669]\tLoss: 3611.3401\n",
      "Training Epoch: 3 [3220/49669]\tLoss: 3522.2607\n",
      "Training Epoch: 3 [3240/49669]\tLoss: 3928.0095\n",
      "Training Epoch: 3 [3260/49669]\tLoss: 3310.1260\n",
      "Training Epoch: 3 [3280/49669]\tLoss: 3271.0103\n",
      "Training Epoch: 3 [3300/49669]\tLoss: 3367.9836\n",
      "Training Epoch: 3 [3320/49669]\tLoss: 3780.9985\n",
      "Training Epoch: 3 [3340/49669]\tLoss: 3842.7083\n",
      "Training Epoch: 3 [3360/49669]\tLoss: 3651.0144\n",
      "Training Epoch: 3 [3380/49669]\tLoss: 3575.5940\n",
      "Training Epoch: 3 [3400/49669]\tLoss: 3679.8967\n",
      "Training Epoch: 3 [3420/49669]\tLoss: 3634.9014\n",
      "Training Epoch: 3 [3440/49669]\tLoss: 3407.8372\n",
      "Training Epoch: 3 [3460/49669]\tLoss: 3974.9685\n",
      "Training Epoch: 3 [3480/49669]\tLoss: 3417.3621\n",
      "Training Epoch: 3 [3500/49669]\tLoss: 3725.8938\n",
      "Training Epoch: 3 [3520/49669]\tLoss: 3182.3848\n",
      "Training Epoch: 3 [3540/49669]\tLoss: 3546.0959\n",
      "Training Epoch: 3 [3560/49669]\tLoss: 3413.0283\n",
      "Training Epoch: 3 [3580/49669]\tLoss: 4044.6472\n",
      "Training Epoch: 3 [3600/49669]\tLoss: 3610.6299\n",
      "Training Epoch: 3 [3620/49669]\tLoss: 3465.3145\n",
      "Training Epoch: 3 [3640/49669]\tLoss: 3733.5012\n",
      "Training Epoch: 3 [3660/49669]\tLoss: 3392.9478\n",
      "Training Epoch: 3 [3680/49669]\tLoss: 3589.7698\n",
      "Training Epoch: 3 [3700/49669]\tLoss: 3425.1855\n",
      "Training Epoch: 3 [3720/49669]\tLoss: 4057.9436\n",
      "Training Epoch: 3 [3740/49669]\tLoss: 3749.7053\n",
      "Training Epoch: 3 [3760/49669]\tLoss: 3299.7114\n",
      "Training Epoch: 3 [3780/49669]\tLoss: 3566.3088\n",
      "Training Epoch: 3 [3800/49669]\tLoss: 3705.7124\n",
      "Training Epoch: 3 [3820/49669]\tLoss: 3451.9231\n",
      "Training Epoch: 3 [3840/49669]\tLoss: 3467.7488\n",
      "Training Epoch: 3 [3860/49669]\tLoss: 3736.1196\n",
      "Training Epoch: 3 [3880/49669]\tLoss: 3877.6685\n",
      "Training Epoch: 3 [3900/49669]\tLoss: 3488.0239\n",
      "Training Epoch: 3 [3920/49669]\tLoss: 3819.3560\n",
      "Training Epoch: 3 [3940/49669]\tLoss: 3777.4163\n",
      "Training Epoch: 3 [3960/49669]\tLoss: 3590.6675\n",
      "Training Epoch: 3 [3980/49669]\tLoss: 4172.3696\n",
      "Training Epoch: 3 [4000/49669]\tLoss: 3005.9080\n",
      "Training Epoch: 3 [4020/49669]\tLoss: 4016.1057\n",
      "Training Epoch: 3 [4040/49669]\tLoss: 3974.9490\n",
      "Training Epoch: 3 [4060/49669]\tLoss: 4064.0786\n",
      "Training Epoch: 3 [4080/49669]\tLoss: 3609.3328\n",
      "Training Epoch: 3 [4100/49669]\tLoss: 3897.8215\n",
      "Training Epoch: 3 [4120/49669]\tLoss: 3104.0083\n",
      "Training Epoch: 3 [4140/49669]\tLoss: 3494.9883\n",
      "Training Epoch: 3 [4160/49669]\tLoss: 3592.8760\n",
      "Training Epoch: 3 [4180/49669]\tLoss: 3715.7185\n",
      "Training Epoch: 3 [4200/49669]\tLoss: 4195.6128\n",
      "Training Epoch: 3 [4220/49669]\tLoss: 3309.1912\n",
      "Training Epoch: 3 [4240/49669]\tLoss: 3631.9272\n",
      "Training Epoch: 3 [4260/49669]\tLoss: 3870.7788\n",
      "Training Epoch: 3 [4280/49669]\tLoss: 4056.7881\n",
      "Training Epoch: 3 [4300/49669]\tLoss: 3064.8079\n",
      "Training Epoch: 3 [4320/49669]\tLoss: 3657.1089\n",
      "Training Epoch: 3 [4340/49669]\tLoss: 3090.7539\n",
      "Training Epoch: 3 [4360/49669]\tLoss: 3100.1277\n",
      "Training Epoch: 3 [4380/49669]\tLoss: 3390.8726\n",
      "Training Epoch: 3 [4400/49669]\tLoss: 3540.8525\n",
      "Training Epoch: 3 [4420/49669]\tLoss: 3729.0320\n",
      "Training Epoch: 3 [4440/49669]\tLoss: 3489.6833\n",
      "Training Epoch: 3 [4460/49669]\tLoss: 3815.9893\n",
      "Training Epoch: 3 [4480/49669]\tLoss: 3338.5325\n",
      "Training Epoch: 3 [4500/49669]\tLoss: 4220.8418\n",
      "Training Epoch: 3 [4520/49669]\tLoss: 3857.8262\n",
      "Training Epoch: 3 [4540/49669]\tLoss: 3739.9495\n",
      "Training Epoch: 3 [4560/49669]\tLoss: 3302.2690\n",
      "Training Epoch: 3 [4580/49669]\tLoss: 3718.0896\n",
      "Training Epoch: 3 [4600/49669]\tLoss: 3383.1968\n",
      "Training Epoch: 3 [4620/49669]\tLoss: 3814.4114\n",
      "Training Epoch: 3 [4640/49669]\tLoss: 3926.6699\n",
      "Training Epoch: 3 [4660/49669]\tLoss: 3421.8188\n",
      "Training Epoch: 3 [4680/49669]\tLoss: 3556.2542\n",
      "Training Epoch: 3 [4700/49669]\tLoss: 3748.6587\n",
      "Training Epoch: 3 [4720/49669]\tLoss: 3917.2437\n",
      "Training Epoch: 3 [4740/49669]\tLoss: 3630.4194\n",
      "Training Epoch: 3 [4760/49669]\tLoss: 3313.5645\n",
      "Training Epoch: 3 [4780/49669]\tLoss: 3407.9702\n",
      "Training Epoch: 3 [4800/49669]\tLoss: 3788.1145\n",
      "Training Epoch: 3 [4820/49669]\tLoss: 3710.1831\n",
      "Training Epoch: 3 [4840/49669]\tLoss: 3549.3982\n",
      "Training Epoch: 3 [4860/49669]\tLoss: 3396.5928\n",
      "Training Epoch: 3 [4880/49669]\tLoss: 3777.1155\n",
      "Training Epoch: 3 [4900/49669]\tLoss: 3424.1431\n",
      "Training Epoch: 3 [4920/49669]\tLoss: 3345.0647\n",
      "Training Epoch: 3 [4940/49669]\tLoss: 3176.8699\n",
      "Training Epoch: 3 [4960/49669]\tLoss: 3416.8611\n",
      "Training Epoch: 3 [4980/49669]\tLoss: 3081.8857\n",
      "Training Epoch: 3 [5000/49669]\tLoss: 3431.1819\n",
      "Training Epoch: 3 [5780/49669]\tLoss: 3448.8901\n",
      "Training Epoch: 3 [5800/49669]\tLoss: 3852.4993\n",
      "Training Epoch: 3 [5820/49669]\tLoss: 3580.7546\n",
      "Training Epoch: 3 [5840/49669]\tLoss: 3262.4868\n",
      "Training Epoch: 3 [5860/49669]\tLoss: 3508.8267\n",
      "Training Epoch: 3 [5880/49669]\tLoss: 2973.3962\n",
      "Training Epoch: 3 [5900/49669]\tLoss: 3004.1318\n",
      "Training Epoch: 3 [5920/49669]\tLoss: 3315.0464\n",
      "Training Epoch: 3 [5940/49669]\tLoss: 3081.4976\n",
      "Training Epoch: 3 [5960/49669]\tLoss: 3182.8677\n",
      "Training Epoch: 3 [5980/49669]\tLoss: 3499.6807\n",
      "Training Epoch: 3 [6000/49669]\tLoss: 3578.2705\n",
      "Training Epoch: 3 [6020/49669]\tLoss: 3236.9556\n",
      "Training Epoch: 3 [6040/49669]\tLoss: 3762.5232\n",
      "Training Epoch: 3 [6060/49669]\tLoss: 3598.7461\n",
      "Training Epoch: 3 [6080/49669]\tLoss: 3363.9500\n",
      "Training Epoch: 3 [6100/49669]\tLoss: 3638.1309\n",
      "Training Epoch: 3 [6120/49669]\tLoss: 3645.7666\n",
      "Training Epoch: 3 [6140/49669]\tLoss: 3831.9412\n",
      "Training Epoch: 3 [6160/49669]\tLoss: 3566.0828\n",
      "Training Epoch: 3 [6180/49669]\tLoss: 3462.9980\n",
      "Training Epoch: 3 [6200/49669]\tLoss: 3537.8467\n",
      "Training Epoch: 3 [6220/49669]\tLoss: 3567.0637\n",
      "Training Epoch: 3 [6240/49669]\tLoss: 3175.7717\n",
      "Training Epoch: 3 [6260/49669]\tLoss: 3521.9680\n",
      "Training Epoch: 3 [6280/49669]\tLoss: 3607.9866\n",
      "Training Epoch: 3 [6300/49669]\tLoss: 3667.5237\n",
      "Training Epoch: 3 [6320/49669]\tLoss: 3776.3357\n",
      "Training Epoch: 3 [6340/49669]\tLoss: 3108.9612\n",
      "Training Epoch: 3 [6360/49669]\tLoss: 3245.4270\n",
      "Training Epoch: 3 [6380/49669]\tLoss: 3702.1689\n",
      "Training Epoch: 3 [6400/49669]\tLoss: 3370.8452\n",
      "Training Epoch: 3 [6420/49669]\tLoss: 3365.5503\n",
      "Training Epoch: 3 [6440/49669]\tLoss: 3432.3945\n",
      "Training Epoch: 3 [6460/49669]\tLoss: 3235.5603\n",
      "Training Epoch: 3 [6480/49669]\tLoss: 3551.1221\n",
      "Training Epoch: 3 [6500/49669]\tLoss: 3157.8923\n",
      "Training Epoch: 3 [6520/49669]\tLoss: 3493.9524\n",
      "Training Epoch: 3 [6540/49669]\tLoss: 3461.6753\n",
      "Training Epoch: 3 [6560/49669]\tLoss: 3557.2852\n",
      "Training Epoch: 3 [6580/49669]\tLoss: 3332.1541\n",
      "Training Epoch: 3 [6600/49669]\tLoss: 3728.2820\n",
      "Training Epoch: 3 [6620/49669]\tLoss: 3129.8611\n",
      "Training Epoch: 3 [6640/49669]\tLoss: 3176.1509\n",
      "Training Epoch: 3 [6660/49669]\tLoss: 3830.3547\n",
      "Training Epoch: 3 [6680/49669]\tLoss: 3695.3491\n",
      "Training Epoch: 3 [6700/49669]\tLoss: 3867.8303\n",
      "Training Epoch: 3 [6720/49669]\tLoss: 3292.6980\n",
      "Training Epoch: 3 [6740/49669]\tLoss: 3559.9980\n",
      "Training Epoch: 3 [6760/49669]\tLoss: 3988.3528\n",
      "Training Epoch: 3 [6780/49669]\tLoss: 3151.1255\n",
      "Training Epoch: 3 [6800/49669]\tLoss: 3542.8894\n",
      "Training Epoch: 3 [6820/49669]\tLoss: 3524.5632\n",
      "Training Epoch: 3 [6840/49669]\tLoss: 2912.7551\n",
      "Training Epoch: 3 [6860/49669]\tLoss: 3318.9646\n",
      "Training Epoch: 3 [6880/49669]\tLoss: 3635.3828\n",
      "Training Epoch: 3 [6900/49669]\tLoss: 2986.9011\n",
      "Training Epoch: 3 [6920/49669]\tLoss: 3896.7146\n",
      "Training Epoch: 3 [6940/49669]\tLoss: 3397.4204\n",
      "Training Epoch: 3 [6960/49669]\tLoss: 3413.5833\n",
      "Training Epoch: 3 [6980/49669]\tLoss: 3760.9534\n",
      "Training Epoch: 3 [7000/49669]\tLoss: 3290.3442\n",
      "Training Epoch: 3 [7020/49669]\tLoss: 3212.3279\n",
      "Training Epoch: 3 [7040/49669]\tLoss: 3317.5586\n",
      "Training Epoch: 3 [7060/49669]\tLoss: 3712.4470\n",
      "Training Epoch: 3 [7080/49669]\tLoss: 3581.3147\n",
      "Training Epoch: 3 [7100/49669]\tLoss: 3408.1985\n",
      "Training Epoch: 3 [7120/49669]\tLoss: 3375.7478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [7140/49669]\tLoss: 3614.5808\n",
      "Training Epoch: 3 [7160/49669]\tLoss: 3083.1951\n",
      "Training Epoch: 3 [7180/49669]\tLoss: 4081.6572\n",
      "Training Epoch: 3 [7200/49669]\tLoss: 3589.8159\n",
      "Training Epoch: 3 [7220/49669]\tLoss: 3743.8223\n",
      "Training Epoch: 3 [7240/49669]\tLoss: 3426.1477\n",
      "Training Epoch: 3 [7260/49669]\tLoss: 3554.8330\n",
      "Training Epoch: 3 [7280/49669]\tLoss: 3233.0955\n",
      "Training Epoch: 3 [7300/49669]\tLoss: 3698.5178\n",
      "Training Epoch: 3 [7320/49669]\tLoss: 3459.5078\n",
      "Training Epoch: 3 [7340/49669]\tLoss: 3781.1069\n",
      "Training Epoch: 3 [7360/49669]\tLoss: 3694.3115\n",
      "Training Epoch: 3 [7380/49669]\tLoss: 3236.7817\n",
      "Training Epoch: 3 [7400/49669]\tLoss: 2916.0303\n",
      "Training Epoch: 3 [7420/49669]\tLoss: 3543.2249\n",
      "Training Epoch: 3 [7440/49669]\tLoss: 3461.3347\n",
      "Training Epoch: 3 [7460/49669]\tLoss: 3409.0752\n",
      "Training Epoch: 3 [7480/49669]\tLoss: 3188.5081\n",
      "Training Epoch: 3 [7500/49669]\tLoss: 3639.3838\n",
      "Training Epoch: 3 [7520/49669]\tLoss: 3143.4604\n",
      "Training Epoch: 3 [7540/49669]\tLoss: 3218.7903\n",
      "Training Epoch: 3 [7560/49669]\tLoss: 3501.2314\n",
      "Training Epoch: 3 [7580/49669]\tLoss: 2841.5281\n",
      "Training Epoch: 3 [7600/49669]\tLoss: 3967.0969\n",
      "Training Epoch: 3 [7620/49669]\tLoss: 3177.3728\n",
      "Training Epoch: 3 [7640/49669]\tLoss: 3023.7651\n",
      "Training Epoch: 3 [7660/49669]\tLoss: 3463.0869\n",
      "Training Epoch: 3 [7680/49669]\tLoss: 3523.1348\n",
      "Training Epoch: 3 [7700/49669]\tLoss: 3724.6687\n",
      "Training Epoch: 3 [7720/49669]\tLoss: 3701.1084\n",
      "Training Epoch: 3 [7740/49669]\tLoss: 3279.6218\n",
      "Training Epoch: 3 [7760/49669]\tLoss: 3798.5461\n",
      "Training Epoch: 3 [7780/49669]\tLoss: 3714.5210\n",
      "Training Epoch: 3 [7800/49669]\tLoss: 3445.4966\n",
      "Training Epoch: 3 [7820/49669]\tLoss: 3742.7705\n",
      "Training Epoch: 3 [7840/49669]\tLoss: 3119.6909\n",
      "Training Epoch: 3 [7860/49669]\tLoss: 3143.8726\n",
      "Training Epoch: 3 [7880/49669]\tLoss: 3491.9043\n",
      "Training Epoch: 3 [7900/49669]\tLoss: 3049.9922\n",
      "Training Epoch: 3 [7920/49669]\tLoss: 3636.0164\n",
      "Training Epoch: 3 [7940/49669]\tLoss: 3363.6428\n",
      "Training Epoch: 3 [7960/49669]\tLoss: 3507.5093\n",
      "Training Epoch: 3 [7980/49669]\tLoss: 2844.7510\n",
      "Training Epoch: 3 [8000/49669]\tLoss: 3483.0906\n",
      "Training Epoch: 3 [8020/49669]\tLoss: 3705.6423\n",
      "Training Epoch: 3 [8040/49669]\tLoss: 3221.2598\n",
      "Training Epoch: 3 [8060/49669]\tLoss: 3640.7512\n",
      "Training Epoch: 3 [8080/49669]\tLoss: 3220.8738\n",
      "Training Epoch: 3 [8100/49669]\tLoss: 3500.5642\n",
      "Training Epoch: 3 [8120/49669]\tLoss: 3303.6484\n",
      "Training Epoch: 3 [8140/49669]\tLoss: 3103.6309\n",
      "Training Epoch: 3 [8160/49669]\tLoss: 3666.8579\n",
      "Training Epoch: 3 [8180/49669]\tLoss: 3617.3713\n",
      "Training Epoch: 3 [8200/49669]\tLoss: 3871.9578\n",
      "Training Epoch: 3 [8220/49669]\tLoss: 3305.4536\n",
      "Training Epoch: 3 [8240/49669]\tLoss: 3583.6953\n",
      "Training Epoch: 3 [8260/49669]\tLoss: 3113.6777\n",
      "Training Epoch: 3 [8280/49669]\tLoss: 3577.2402\n",
      "Training Epoch: 3 [8300/49669]\tLoss: 3658.7432\n",
      "Training Epoch: 3 [8320/49669]\tLoss: 3609.8474\n",
      "Training Epoch: 3 [8340/49669]\tLoss: 3470.6404\n",
      "Training Epoch: 3 [8360/49669]\tLoss: 3443.2529\n",
      "Training Epoch: 3 [8380/49669]\tLoss: 3287.3430\n",
      "Training Epoch: 3 [8400/49669]\tLoss: 3473.6355\n",
      "Training Epoch: 3 [8420/49669]\tLoss: 3435.6089\n",
      "Training Epoch: 3 [8440/49669]\tLoss: 3454.0127\n",
      "Training Epoch: 3 [8460/49669]\tLoss: 3752.5234\n",
      "Training Epoch: 3 [8480/49669]\tLoss: 3661.1362\n",
      "Training Epoch: 3 [8500/49669]\tLoss: 3567.1313\n",
      "Training Epoch: 3 [8520/49669]\tLoss: 3973.3625\n",
      "Training Epoch: 3 [8540/49669]\tLoss: 2925.5925\n",
      "Training Epoch: 3 [8560/49669]\tLoss: 3316.1719\n",
      "Training Epoch: 3 [8580/49669]\tLoss: 3246.2542\n",
      "Training Epoch: 3 [8600/49669]\tLoss: 3189.8958\n",
      "Training Epoch: 3 [8620/49669]\tLoss: 3679.0928\n",
      "Training Epoch: 3 [8640/49669]\tLoss: 3511.2092\n",
      "Training Epoch: 3 [8660/49669]\tLoss: 3573.5762\n",
      "Training Epoch: 3 [8680/49669]\tLoss: 3445.7419\n",
      "Training Epoch: 3 [8700/49669]\tLoss: 3230.9470\n",
      "Training Epoch: 3 [8720/49669]\tLoss: 3795.7246\n",
      "Training Epoch: 3 [8740/49669]\tLoss: 3484.7234\n",
      "Training Epoch: 3 [8760/49669]\tLoss: 3562.5649\n",
      "Training Epoch: 3 [8780/49669]\tLoss: 3210.0537\n",
      "Training Epoch: 3 [8800/49669]\tLoss: 2895.0100\n",
      "Training Epoch: 3 [8820/49669]\tLoss: 3493.4915\n",
      "Training Epoch: 3 [8840/49669]\tLoss: 3416.4702\n",
      "Training Epoch: 3 [8860/49669]\tLoss: 3055.1819\n",
      "Training Epoch: 3 [8880/49669]\tLoss: 3686.9250\n",
      "Training Epoch: 3 [8900/49669]\tLoss: 3806.4204\n",
      "Training Epoch: 3 [8920/49669]\tLoss: 3643.1543\n",
      "Training Epoch: 3 [8940/49669]\tLoss: 3109.8738\n",
      "Training Epoch: 3 [8960/49669]\tLoss: 3115.9722\n",
      "Training Epoch: 3 [8980/49669]\tLoss: 3073.0066\n",
      "Training Epoch: 3 [9000/49669]\tLoss: 3078.6785\n",
      "Training Epoch: 3 [9020/49669]\tLoss: 3619.3833\n",
      "Training Epoch: 3 [9040/49669]\tLoss: 3462.7205\n",
      "Training Epoch: 3 [9060/49669]\tLoss: 3462.6262\n",
      "Training Epoch: 3 [9080/49669]\tLoss: 3279.5483\n",
      "Training Epoch: 3 [9100/49669]\tLoss: 3487.1584\n",
      "Training Epoch: 3 [9120/49669]\tLoss: 3168.3250\n",
      "Training Epoch: 3 [9140/49669]\tLoss: 3564.1685\n",
      "Training Epoch: 3 [9160/49669]\tLoss: 3298.4797\n",
      "Training Epoch: 3 [9180/49669]\tLoss: 3394.8867\n",
      "Training Epoch: 3 [9200/49669]\tLoss: 3280.4712\n",
      "Training Epoch: 3 [9220/49669]\tLoss: 3284.2732\n",
      "Training Epoch: 3 [9240/49669]\tLoss: 3318.8003\n",
      "Training Epoch: 3 [9260/49669]\tLoss: 3016.5017\n",
      "Training Epoch: 3 [9280/49669]\tLoss: 3685.8386\n",
      "Training Epoch: 3 [9300/49669]\tLoss: 3294.2107\n",
      "Training Epoch: 3 [9320/49669]\tLoss: 3606.2400\n",
      "Training Epoch: 3 [9340/49669]\tLoss: 3157.3970\n",
      "Training Epoch: 3 [9360/49669]\tLoss: 3078.9324\n",
      "Training Epoch: 3 [9380/49669]\tLoss: 3445.6372\n",
      "Training Epoch: 3 [9400/49669]\tLoss: 3463.2500\n",
      "Training Epoch: 3 [9420/49669]\tLoss: 3467.8435\n",
      "Training Epoch: 3 [9440/49669]\tLoss: 3104.2207\n",
      "Training Epoch: 3 [9460/49669]\tLoss: 3601.3330\n",
      "Training Epoch: 3 [9480/49669]\tLoss: 2861.8821\n",
      "Training Epoch: 3 [9500/49669]\tLoss: 3278.6240\n",
      "Training Epoch: 3 [9520/49669]\tLoss: 3097.9587\n",
      "Training Epoch: 3 [9540/49669]\tLoss: 3279.8245\n",
      "Training Epoch: 3 [9560/49669]\tLoss: 3172.7825\n",
      "Training Epoch: 3 [9580/49669]\tLoss: 3395.7852\n",
      "Training Epoch: 3 [9600/49669]\tLoss: 3221.8945\n",
      "Training Epoch: 3 [9620/49669]\tLoss: 3422.7607\n",
      "Training Epoch: 3 [9640/49669]\tLoss: 3164.5005\n",
      "Training Epoch: 3 [9660/49669]\tLoss: 3252.2253\n",
      "Training Epoch: 3 [9680/49669]\tLoss: 3280.5771\n",
      "Training Epoch: 3 [9700/49669]\tLoss: 3947.0920\n",
      "Training Epoch: 3 [9720/49669]\tLoss: 3696.1155\n",
      "Training Epoch: 3 [9740/49669]\tLoss: 3374.3679\n",
      "Training Epoch: 3 [9760/49669]\tLoss: 3421.8608\n",
      "Training Epoch: 3 [9780/49669]\tLoss: 2655.0955\n",
      "Training Epoch: 3 [9800/49669]\tLoss: 3844.7241\n",
      "Training Epoch: 3 [9820/49669]\tLoss: 3528.6995\n",
      "Training Epoch: 3 [9840/49669]\tLoss: 3336.9766\n",
      "Training Epoch: 3 [9860/49669]\tLoss: 3055.7964\n",
      "Training Epoch: 3 [9880/49669]\tLoss: 3996.0610\n",
      "Training Epoch: 3 [9900/49669]\tLoss: 3652.0479\n",
      "Training Epoch: 3 [9920/49669]\tLoss: 3370.5596\n",
      "Training Epoch: 3 [9940/49669]\tLoss: 3632.0593\n",
      "Training Epoch: 3 [9960/49669]\tLoss: 3285.5154\n",
      "Training Epoch: 3 [9980/49669]\tLoss: 3633.4954\n",
      "Training Epoch: 3 [10000/49669]\tLoss: 3169.2959\n",
      "Training Epoch: 3 [10020/49669]\tLoss: 3272.2979\n",
      "Training Epoch: 3 [10040/49669]\tLoss: 3398.0088\n",
      "Training Epoch: 3 [10060/49669]\tLoss: 3415.7166\n",
      "Training Epoch: 3 [10080/49669]\tLoss: 3441.7834\n",
      "Training Epoch: 3 [10100/49669]\tLoss: 3202.8508\n",
      "Training Epoch: 3 [10120/49669]\tLoss: 3673.4529\n",
      "Training Epoch: 3 [10140/49669]\tLoss: 3458.3455\n",
      "Training Epoch: 3 [10160/49669]\tLoss: 3726.5684\n",
      "Training Epoch: 3 [10180/49669]\tLoss: 2892.3560\n",
      "Training Epoch: 3 [10200/49669]\tLoss: 2818.8672\n",
      "Training Epoch: 3 [10220/49669]\tLoss: 3291.8801\n",
      "Training Epoch: 3 [10240/49669]\tLoss: 3356.9724\n",
      "Training Epoch: 3 [10260/49669]\tLoss: 3756.9592\n",
      "Training Epoch: 3 [10280/49669]\tLoss: 4019.0789\n",
      "Training Epoch: 3 [10300/49669]\tLoss: 3209.6487\n",
      "Training Epoch: 3 [10320/49669]\tLoss: 3548.4517\n",
      "Training Epoch: 3 [10340/49669]\tLoss: 3569.7354\n",
      "Training Epoch: 3 [10360/49669]\tLoss: 3267.1826\n",
      "Training Epoch: 3 [10380/49669]\tLoss: 3092.8838\n",
      "Training Epoch: 3 [10400/49669]\tLoss: 3358.9392\n",
      "Training Epoch: 3 [10420/49669]\tLoss: 3516.4590\n",
      "Training Epoch: 3 [10440/49669]\tLoss: 3044.5056\n",
      "Training Epoch: 3 [10460/49669]\tLoss: 3670.9985\n",
      "Training Epoch: 3 [10480/49669]\tLoss: 3528.9392\n",
      "Training Epoch: 3 [10500/49669]\tLoss: 3071.0466\n",
      "Training Epoch: 3 [10520/49669]\tLoss: 3192.8904\n",
      "Training Epoch: 3 [10540/49669]\tLoss: 2959.2366\n",
      "Training Epoch: 3 [10560/49669]\tLoss: 3173.7361\n",
      "Training Epoch: 3 [10580/49669]\tLoss: 3527.6926\n",
      "Training Epoch: 3 [10600/49669]\tLoss: 3521.8406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [10620/49669]\tLoss: 3018.2510\n",
      "Training Epoch: 3 [10640/49669]\tLoss: 3335.0779\n",
      "Training Epoch: 3 [10660/49669]\tLoss: 3177.5425\n",
      "Training Epoch: 3 [10680/49669]\tLoss: 3446.9080\n",
      "Training Epoch: 3 [10700/49669]\tLoss: 3777.8105\n",
      "Training Epoch: 3 [10720/49669]\tLoss: 3291.1077\n",
      "Training Epoch: 3 [10740/49669]\tLoss: 3107.0344\n",
      "Training Epoch: 3 [10760/49669]\tLoss: 3481.3230\n",
      "Training Epoch: 3 [10780/49669]\tLoss: 3663.0403\n",
      "Training Epoch: 3 [10800/49669]\tLoss: 3700.6699\n",
      "Training Epoch: 3 [10820/49669]\tLoss: 3311.7063\n",
      "Training Epoch: 3 [10840/49669]\tLoss: 3522.1885\n",
      "Training Epoch: 3 [10860/49669]\tLoss: 3324.5405\n",
      "Training Epoch: 3 [10880/49669]\tLoss: 3200.8154\n",
      "Training Epoch: 3 [10900/49669]\tLoss: 3124.8091\n",
      "Training Epoch: 3 [10920/49669]\tLoss: 3275.4766\n",
      "Training Epoch: 3 [10940/49669]\tLoss: 3530.8716\n",
      "Training Epoch: 3 [10960/49669]\tLoss: 3359.3762\n",
      "Training Epoch: 3 [10980/49669]\tLoss: 3538.9744\n",
      "Training Epoch: 3 [11000/49669]\tLoss: 3301.4546\n",
      "Training Epoch: 3 [11020/49669]\tLoss: 3050.9241\n",
      "Training Epoch: 3 [11040/49669]\tLoss: 3537.8462\n",
      "Training Epoch: 3 [11060/49669]\tLoss: 2901.8845\n",
      "Training Epoch: 3 [11080/49669]\tLoss: 3215.4678\n",
      "Training Epoch: 3 [11100/49669]\tLoss: 3297.9614\n",
      "Training Epoch: 3 [11120/49669]\tLoss: 3526.1018\n",
      "Training Epoch: 3 [11140/49669]\tLoss: 3603.2424\n",
      "Training Epoch: 3 [11160/49669]\tLoss: 3247.0896\n",
      "Training Epoch: 3 [11180/49669]\tLoss: 3242.8494\n",
      "Training Epoch: 3 [11200/49669]\tLoss: 2961.8518\n",
      "Training Epoch: 3 [11220/49669]\tLoss: 3620.3730\n",
      "Training Epoch: 3 [11240/49669]\tLoss: 3750.1272\n",
      "Training Epoch: 3 [11260/49669]\tLoss: 3247.8887\n",
      "Training Epoch: 3 [11280/49669]\tLoss: 3331.0676\n",
      "Training Epoch: 3 [11300/49669]\tLoss: 3629.4163\n",
      "Training Epoch: 3 [11320/49669]\tLoss: 3238.3787\n",
      "Training Epoch: 3 [11340/49669]\tLoss: 3105.9197\n",
      "Training Epoch: 3 [11360/49669]\tLoss: 3491.7568\n",
      "Training Epoch: 3 [11380/49669]\tLoss: 3650.5447\n",
      "Training Epoch: 3 [11400/49669]\tLoss: 3007.2742\n",
      "Training Epoch: 3 [11420/49669]\tLoss: 3243.2666\n",
      "Training Epoch: 3 [11440/49669]\tLoss: 2962.6580\n",
      "Training Epoch: 3 [11460/49669]\tLoss: 3224.5461\n",
      "Training Epoch: 3 [11480/49669]\tLoss: 3143.5210\n",
      "Training Epoch: 3 [11500/49669]\tLoss: 3165.0603\n",
      "Training Epoch: 3 [11520/49669]\tLoss: 3731.7549\n",
      "Training Epoch: 3 [11540/49669]\tLoss: 3324.7642\n",
      "Training Epoch: 3 [11560/49669]\tLoss: 3130.8313\n",
      "Training Epoch: 3 [11580/49669]\tLoss: 3151.6943\n",
      "Training Epoch: 3 [11600/49669]\tLoss: 3673.7173\n",
      "Training Epoch: 3 [11620/49669]\tLoss: 2978.3113\n",
      "Training Epoch: 3 [11640/49669]\tLoss: 3721.2021\n",
      "Training Epoch: 3 [11660/49669]\tLoss: 3604.4902\n",
      "Training Epoch: 3 [11680/49669]\tLoss: 3660.2805\n",
      "Training Epoch: 3 [11700/49669]\tLoss: 2716.4331\n",
      "Training Epoch: 3 [11720/49669]\tLoss: 3324.5347\n",
      "Training Epoch: 3 [11740/49669]\tLoss: 3347.2578\n",
      "Training Epoch: 3 [11760/49669]\tLoss: 3057.6147\n",
      "Training Epoch: 3 [11780/49669]\tLoss: 3221.5918\n",
      "Training Epoch: 3 [11800/49669]\tLoss: 3222.1848\n",
      "Training Epoch: 3 [11820/49669]\tLoss: 3237.3801\n",
      "Training Epoch: 3 [11840/49669]\tLoss: 3282.9805\n",
      "Training Epoch: 3 [11860/49669]\tLoss: 3448.5022\n",
      "Training Epoch: 3 [11880/49669]\tLoss: 3089.3276\n",
      "Training Epoch: 3 [11900/49669]\tLoss: 3221.8232\n",
      "Training Epoch: 3 [11920/49669]\tLoss: 3119.2441\n",
      "Training Epoch: 3 [11940/49669]\tLoss: 3221.8501\n",
      "Training Epoch: 3 [11960/49669]\tLoss: 2964.0203\n",
      "Training Epoch: 3 [11980/49669]\tLoss: 3696.5789\n",
      "Training Epoch: 3 [12000/49669]\tLoss: 3129.3677\n",
      "Training Epoch: 3 [12020/49669]\tLoss: 3590.5605\n",
      "Training Epoch: 3 [12040/49669]\tLoss: 3003.2849\n",
      "Training Epoch: 3 [12060/49669]\tLoss: 3210.5786\n",
      "Training Epoch: 3 [12080/49669]\tLoss: 3217.5042\n",
      "Training Epoch: 3 [12100/49669]\tLoss: 2973.4888\n",
      "Training Epoch: 3 [12120/49669]\tLoss: 3666.8479\n",
      "Training Epoch: 3 [12140/49669]\tLoss: 3127.2271\n",
      "Training Epoch: 3 [12160/49669]\tLoss: 3335.0718\n",
      "Training Epoch: 3 [12180/49669]\tLoss: 3405.6006\n",
      "Training Epoch: 3 [12200/49669]\tLoss: 2881.3914\n",
      "Training Epoch: 3 [12220/49669]\tLoss: 3382.6733\n",
      "Training Epoch: 3 [12240/49669]\tLoss: 3333.4509\n",
      "Training Epoch: 3 [12260/49669]\tLoss: 2925.6052\n",
      "Training Epoch: 3 [12280/49669]\tLoss: 3218.1130\n",
      "Training Epoch: 3 [12300/49669]\tLoss: 3365.2864\n",
      "Training Epoch: 3 [12320/49669]\tLoss: 3682.9412\n",
      "Training Epoch: 3 [12340/49669]\tLoss: 3599.8906\n",
      "Training Epoch: 3 [12360/49669]\tLoss: 3326.1018\n",
      "Training Epoch: 3 [12380/49669]\tLoss: 3307.2175\n",
      "Training Epoch: 3 [12400/49669]\tLoss: 3603.4426\n",
      "Training Epoch: 3 [12420/49669]\tLoss: 3082.9424\n",
      "Training Epoch: 3 [12440/49669]\tLoss: 3245.5996\n",
      "Training Epoch: 3 [12460/49669]\tLoss: 3387.2124\n",
      "Training Epoch: 3 [12480/49669]\tLoss: 3613.4937\n",
      "Training Epoch: 3 [12500/49669]\tLoss: 3502.6990\n",
      "Training Epoch: 3 [12520/49669]\tLoss: 3068.4988\n",
      "Training Epoch: 3 [12540/49669]\tLoss: 3482.9978\n",
      "Training Epoch: 3 [12560/49669]\tLoss: 3333.2114\n",
      "Training Epoch: 3 [12580/49669]\tLoss: 3476.0254\n",
      "Training Epoch: 3 [12600/49669]\tLoss: 3673.4221\n",
      "Training Epoch: 3 [12620/49669]\tLoss: 3338.3435\n",
      "Training Epoch: 3 [12640/49669]\tLoss: 3356.0686\n",
      "Training Epoch: 3 [12660/49669]\tLoss: 3251.0107\n",
      "Training Epoch: 3 [12680/49669]\tLoss: 3359.3936\n",
      "Training Epoch: 3 [12700/49669]\tLoss: 3203.1255\n",
      "Training Epoch: 3 [12720/49669]\tLoss: 3589.7866\n",
      "Training Epoch: 3 [12740/49669]\tLoss: 3421.6677\n",
      "Training Epoch: 3 [12760/49669]\tLoss: 3128.5974\n",
      "Training Epoch: 3 [12780/49669]\tLoss: 3087.7217\n",
      "Training Epoch: 3 [12800/49669]\tLoss: 3405.6248\n",
      "Training Epoch: 3 [12820/49669]\tLoss: 2887.9712\n",
      "Training Epoch: 3 [12840/49669]\tLoss: 3389.9534\n",
      "Training Epoch: 3 [12860/49669]\tLoss: 3433.4006\n",
      "Training Epoch: 3 [12880/49669]\tLoss: 3257.0144\n",
      "Training Epoch: 3 [12900/49669]\tLoss: 3530.6348\n",
      "Training Epoch: 3 [12920/49669]\tLoss: 3013.8884\n",
      "Training Epoch: 3 [12940/49669]\tLoss: 2724.8354\n",
      "Training Epoch: 3 [12960/49669]\tLoss: 3135.2156\n",
      "Training Epoch: 3 [12980/49669]\tLoss: 3364.6436\n",
      "Training Epoch: 3 [13000/49669]\tLoss: 3276.0032\n",
      "Training Epoch: 3 [13020/49669]\tLoss: 2830.2227\n",
      "Training Epoch: 3 [13040/49669]\tLoss: 3001.1826\n",
      "Training Epoch: 3 [13060/49669]\tLoss: 2760.4629\n",
      "Training Epoch: 3 [13080/49669]\tLoss: 3009.1274\n",
      "Training Epoch: 3 [13100/49669]\tLoss: 3451.5862\n",
      "Training Epoch: 3 [13120/49669]\tLoss: 3630.7422\n",
      "Training Epoch: 3 [13140/49669]\tLoss: 2957.8638\n",
      "Training Epoch: 3 [13160/49669]\tLoss: 2965.8718\n",
      "Training Epoch: 3 [13180/49669]\tLoss: 2775.4219\n",
      "Training Epoch: 3 [13200/49669]\tLoss: 3482.0142\n",
      "Training Epoch: 3 [13220/49669]\tLoss: 3045.9204\n",
      "Training Epoch: 3 [13240/49669]\tLoss: 3167.8269\n",
      "Training Epoch: 3 [13260/49669]\tLoss: 3468.0476\n",
      "Training Epoch: 3 [13280/49669]\tLoss: 3232.8206\n",
      "Training Epoch: 3 [13300/49669]\tLoss: 2912.4146\n",
      "Training Epoch: 3 [13320/49669]\tLoss: 3020.0691\n",
      "Training Epoch: 3 [13340/49669]\tLoss: 2950.2900\n",
      "Training Epoch: 3 [13360/49669]\tLoss: 2793.7576\n",
      "Training Epoch: 3 [13380/49669]\tLoss: 2997.2886\n",
      "Training Epoch: 3 [13400/49669]\tLoss: 2778.6440\n",
      "Training Epoch: 3 [13420/49669]\tLoss: 2642.2983\n",
      "Training Epoch: 3 [13440/49669]\tLoss: 2977.0190\n",
      "Training Epoch: 3 [13460/49669]\tLoss: 3332.1086\n",
      "Training Epoch: 3 [13480/49669]\tLoss: 2758.6580\n",
      "Training Epoch: 3 [13500/49669]\tLoss: 2746.5391\n",
      "Training Epoch: 3 [13520/49669]\tLoss: 3227.1833\n",
      "Training Epoch: 3 [13540/49669]\tLoss: 3395.1372\n",
      "Training Epoch: 3 [13560/49669]\tLoss: 2924.0771\n",
      "Training Epoch: 3 [13580/49669]\tLoss: 3748.1079\n",
      "Training Epoch: 3 [13600/49669]\tLoss: 3273.2285\n",
      "Training Epoch: 3 [13620/49669]\tLoss: 3373.1033\n",
      "Training Epoch: 3 [13640/49669]\tLoss: 3350.6614\n",
      "Training Epoch: 3 [13660/49669]\tLoss: 3102.7495\n",
      "Training Epoch: 3 [13680/49669]\tLoss: 3121.5947\n",
      "Training Epoch: 3 [13700/49669]\tLoss: 3480.0085\n",
      "Training Epoch: 3 [13720/49669]\tLoss: 2907.8511\n",
      "Training Epoch: 3 [13740/49669]\tLoss: 2837.8152\n",
      "Training Epoch: 3 [13760/49669]\tLoss: 3295.0996\n",
      "Training Epoch: 3 [13780/49669]\tLoss: 3444.9692\n",
      "Training Epoch: 3 [13800/49669]\tLoss: 3261.9189\n",
      "Training Epoch: 3 [13820/49669]\tLoss: 3365.7822\n",
      "Training Epoch: 3 [13840/49669]\tLoss: 3331.1875\n",
      "Training Epoch: 3 [13860/49669]\tLoss: 2877.7395\n",
      "Training Epoch: 3 [13880/49669]\tLoss: 3018.8386\n",
      "Training Epoch: 3 [13900/49669]\tLoss: 3006.0071\n",
      "Training Epoch: 3 [13920/49669]\tLoss: 3187.8123\n",
      "Training Epoch: 3 [13940/49669]\tLoss: 3191.1943\n",
      "Training Epoch: 3 [13960/49669]\tLoss: 3137.2107\n",
      "Training Epoch: 3 [13980/49669]\tLoss: 3048.7795\n",
      "Training Epoch: 3 [14000/49669]\tLoss: 2948.9082\n",
      "Training Epoch: 3 [14020/49669]\tLoss: 2954.3943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [14040/49669]\tLoss: 2937.1230\n",
      "Training Epoch: 3 [14060/49669]\tLoss: 3255.7642\n",
      "Training Epoch: 3 [14080/49669]\tLoss: 3175.8518\n",
      "Training Epoch: 3 [14100/49669]\tLoss: 3165.3889\n",
      "Training Epoch: 3 [14120/49669]\tLoss: 3185.6196\n",
      "Training Epoch: 3 [14140/49669]\tLoss: 3319.6196\n",
      "Training Epoch: 3 [14160/49669]\tLoss: 3013.7913\n",
      "Training Epoch: 3 [14180/49669]\tLoss: 3303.5554\n",
      "Training Epoch: 3 [14200/49669]\tLoss: 2614.5027\n",
      "Training Epoch: 3 [14220/49669]\tLoss: 2914.3320\n",
      "Training Epoch: 3 [14240/49669]\tLoss: 3088.2327\n",
      "Training Epoch: 3 [14260/49669]\tLoss: 2789.7551\n",
      "Training Epoch: 3 [14280/49669]\tLoss: 2806.5208\n",
      "Training Epoch: 3 [14300/49669]\tLoss: 3178.4648\n",
      "Training Epoch: 3 [14320/49669]\tLoss: 3242.3193\n",
      "Training Epoch: 3 [14340/49669]\tLoss: 3142.1521\n",
      "Training Epoch: 3 [14360/49669]\tLoss: 3185.3318\n",
      "Training Epoch: 3 [14380/49669]\tLoss: 3419.8887\n",
      "Training Epoch: 3 [14400/49669]\tLoss: 3334.6133\n",
      "Training Epoch: 3 [14420/49669]\tLoss: 3241.4045\n",
      "Training Epoch: 3 [14440/49669]\tLoss: 3092.7737\n",
      "Training Epoch: 3 [14460/49669]\tLoss: 3205.5840\n",
      "Training Epoch: 3 [14480/49669]\tLoss: 2815.4607\n",
      "Training Epoch: 3 [14500/49669]\tLoss: 3327.8240\n",
      "Training Epoch: 3 [14520/49669]\tLoss: 3187.0901\n",
      "Training Epoch: 3 [14540/49669]\tLoss: 3386.0312\n",
      "Training Epoch: 3 [14560/49669]\tLoss: 3060.3081\n",
      "Training Epoch: 3 [14580/49669]\tLoss: 3212.1050\n",
      "Training Epoch: 3 [14600/49669]\tLoss: 3519.4727\n",
      "Training Epoch: 3 [14620/49669]\tLoss: 3544.8887\n",
      "Training Epoch: 3 [14640/49669]\tLoss: 3179.4617\n",
      "Training Epoch: 3 [14660/49669]\tLoss: 3405.4663\n",
      "Training Epoch: 3 [14680/49669]\tLoss: 3517.9348\n",
      "Training Epoch: 3 [14700/49669]\tLoss: 3022.0024\n",
      "Training Epoch: 3 [14720/49669]\tLoss: 2944.9473\n",
      "Training Epoch: 3 [14740/49669]\tLoss: 3274.5894\n",
      "Training Epoch: 3 [14760/49669]\tLoss: 3206.0312\n",
      "Training Epoch: 3 [14780/49669]\tLoss: 3364.1667\n",
      "Training Epoch: 3 [14800/49669]\tLoss: 3267.6372\n",
      "Training Epoch: 3 [14820/49669]\tLoss: 3331.1636\n",
      "Training Epoch: 3 [14840/49669]\tLoss: 3035.6157\n",
      "Training Epoch: 3 [14860/49669]\tLoss: 3669.5427\n",
      "Training Epoch: 3 [14880/49669]\tLoss: 2861.8718\n",
      "Training Epoch: 3 [14900/49669]\tLoss: 2958.8647\n",
      "Training Epoch: 3 [14920/49669]\tLoss: 3163.5085\n",
      "Training Epoch: 3 [14940/49669]\tLoss: 3231.1423\n",
      "Training Epoch: 3 [14960/49669]\tLoss: 3011.9492\n",
      "Training Epoch: 3 [14980/49669]\tLoss: 3371.6255\n",
      "Training Epoch: 3 [15000/49669]\tLoss: 2978.7197\n",
      "Training Epoch: 3 [15020/49669]\tLoss: 2686.1245\n",
      "Training Epoch: 3 [15040/49669]\tLoss: 3158.6143\n",
      "Training Epoch: 3 [15060/49669]\tLoss: 3266.3154\n",
      "Training Epoch: 3 [15080/49669]\tLoss: 3319.6064\n",
      "Training Epoch: 3 [15100/49669]\tLoss: 3299.0430\n",
      "Training Epoch: 3 [15120/49669]\tLoss: 3373.4062\n",
      "Training Epoch: 3 [15140/49669]\tLoss: 2902.1519\n",
      "Training Epoch: 3 [15160/49669]\tLoss: 3698.2737\n",
      "Training Epoch: 3 [15180/49669]\tLoss: 3382.7773\n",
      "Training Epoch: 3 [15200/49669]\tLoss: 2892.4480\n",
      "Training Epoch: 3 [15220/49669]\tLoss: 3042.5557\n",
      "Training Epoch: 3 [15240/49669]\tLoss: 3231.7617\n",
      "Training Epoch: 3 [15260/49669]\tLoss: 3235.2876\n",
      "Training Epoch: 3 [15280/49669]\tLoss: 3447.6758\n",
      "Training Epoch: 3 [15300/49669]\tLoss: 3166.2004\n",
      "Training Epoch: 3 [15320/49669]\tLoss: 2958.6711\n",
      "Training Epoch: 3 [15340/49669]\tLoss: 3680.6067\n",
      "Training Epoch: 3 [15360/49669]\tLoss: 3196.3845\n",
      "Training Epoch: 3 [15380/49669]\tLoss: 2963.7737\n",
      "Training Epoch: 3 [15400/49669]\tLoss: 2862.4521\n",
      "Training Epoch: 3 [15420/49669]\tLoss: 2610.0105\n",
      "Training Epoch: 3 [15440/49669]\tLoss: 3091.0691\n",
      "Training Epoch: 3 [15460/49669]\tLoss: 3590.7546\n",
      "Training Epoch: 3 [15480/49669]\tLoss: 3018.5164\n",
      "Training Epoch: 3 [15500/49669]\tLoss: 2815.3511\n",
      "Training Epoch: 3 [15520/49669]\tLoss: 2944.5469\n",
      "Training Epoch: 3 [15540/49669]\tLoss: 3295.5815\n",
      "Training Epoch: 3 [15560/49669]\tLoss: 3386.6472\n",
      "Training Epoch: 3 [15580/49669]\tLoss: 3162.1726\n",
      "Training Epoch: 3 [15600/49669]\tLoss: 2674.5476\n",
      "Training Epoch: 3 [15620/49669]\tLoss: 3087.7939\n",
      "Training Epoch: 3 [15640/49669]\tLoss: 2568.2507\n",
      "Training Epoch: 3 [15660/49669]\tLoss: 3162.1052\n",
      "Training Epoch: 3 [15680/49669]\tLoss: 3539.6235\n",
      "Training Epoch: 3 [15700/49669]\tLoss: 3074.4937\n",
      "Training Epoch: 3 [15720/49669]\tLoss: 3202.8362\n",
      "Training Epoch: 3 [15740/49669]\tLoss: 3194.6121\n",
      "Training Epoch: 3 [15760/49669]\tLoss: 3303.5349\n",
      "Training Epoch: 3 [15780/49669]\tLoss: 3189.6792\n",
      "Training Epoch: 3 [15800/49669]\tLoss: 3514.7119\n",
      "Training Epoch: 3 [15820/49669]\tLoss: 2829.2480\n",
      "Training Epoch: 3 [15840/49669]\tLoss: 3341.8579\n",
      "Training Epoch: 3 [15860/49669]\tLoss: 3222.6301\n",
      "Training Epoch: 3 [15880/49669]\tLoss: 3035.0710\n",
      "Training Epoch: 3 [15900/49669]\tLoss: 3204.9189\n",
      "Training Epoch: 3 [15920/49669]\tLoss: 3325.6130\n",
      "Training Epoch: 3 [15940/49669]\tLoss: 3126.1223\n",
      "Training Epoch: 3 [15960/49669]\tLoss: 3299.4778\n",
      "Training Epoch: 3 [15980/49669]\tLoss: 3001.2546\n",
      "Training Epoch: 3 [16000/49669]\tLoss: 3273.5767\n",
      "Training Epoch: 3 [16020/49669]\tLoss: 2944.6411\n",
      "Training Epoch: 3 [16040/49669]\tLoss: 2659.5947\n",
      "Training Epoch: 3 [16060/49669]\tLoss: 3065.3586\n",
      "Training Epoch: 3 [16080/49669]\tLoss: 2956.0823\n",
      "Training Epoch: 3 [16100/49669]\tLoss: 3116.2949\n",
      "Training Epoch: 3 [16120/49669]\tLoss: 2756.8474\n",
      "Training Epoch: 3 [16140/49669]\tLoss: 3144.7566\n",
      "Training Epoch: 3 [16160/49669]\tLoss: 3117.2026\n",
      "Training Epoch: 3 [16180/49669]\tLoss: 3041.0017\n",
      "Training Epoch: 3 [16200/49669]\tLoss: 3061.9175\n",
      "Training Epoch: 3 [16220/49669]\tLoss: 3080.3401\n",
      "Training Epoch: 3 [16240/49669]\tLoss: 3304.9126\n",
      "Training Epoch: 3 [16260/49669]\tLoss: 3012.6152\n",
      "Training Epoch: 3 [16280/49669]\tLoss: 3135.0037\n",
      "Training Epoch: 3 [16300/49669]\tLoss: 3377.2947\n",
      "Training Epoch: 3 [16320/49669]\tLoss: 2975.0950\n",
      "Training Epoch: 3 [16340/49669]\tLoss: 3230.9050\n",
      "Training Epoch: 3 [16360/49669]\tLoss: 3199.2283\n",
      "Training Epoch: 3 [16380/49669]\tLoss: 2892.6970\n",
      "Training Epoch: 3 [16400/49669]\tLoss: 3178.4719\n",
      "Training Epoch: 3 [16420/49669]\tLoss: 3216.8069\n",
      "Training Epoch: 3 [16440/49669]\tLoss: 3292.2732\n",
      "Training Epoch: 3 [16460/49669]\tLoss: 2814.5549\n",
      "Training Epoch: 3 [16480/49669]\tLoss: 3203.7427\n",
      "Training Epoch: 3 [16500/49669]\tLoss: 3129.2966\n",
      "Training Epoch: 3 [16520/49669]\tLoss: 3103.8772\n",
      "Training Epoch: 3 [16540/49669]\tLoss: 2921.9390\n",
      "Training Epoch: 3 [16560/49669]\tLoss: 3177.1660\n",
      "Training Epoch: 3 [16580/49669]\tLoss: 2976.0034\n",
      "Training Epoch: 3 [16600/49669]\tLoss: 3251.2512\n",
      "Training Epoch: 3 [16620/49669]\tLoss: 3137.4360\n",
      "Training Epoch: 3 [16640/49669]\tLoss: 3167.6899\n",
      "Training Epoch: 3 [16660/49669]\tLoss: 3159.1824\n",
      "Training Epoch: 3 [16680/49669]\tLoss: 3106.4688\n",
      "Training Epoch: 3 [16700/49669]\tLoss: 2380.7156\n",
      "Training Epoch: 3 [16720/49669]\tLoss: 3055.0378\n",
      "Training Epoch: 3 [16740/49669]\tLoss: 2515.6294\n",
      "Training Epoch: 3 [16760/49669]\tLoss: 2817.6975\n",
      "Training Epoch: 3 [16780/49669]\tLoss: 2840.6184\n",
      "Training Epoch: 3 [16800/49669]\tLoss: 3018.3762\n",
      "Training Epoch: 3 [16820/49669]\tLoss: 2948.5471\n",
      "Training Epoch: 3 [16840/49669]\tLoss: 3146.9268\n",
      "Training Epoch: 3 [16860/49669]\tLoss: 2553.2603\n",
      "Training Epoch: 3 [16880/49669]\tLoss: 3201.4534\n",
      "Training Epoch: 3 [16900/49669]\tLoss: 3137.8545\n",
      "Training Epoch: 3 [16920/49669]\tLoss: 2878.2676\n",
      "Training Epoch: 3 [16940/49669]\tLoss: 3541.6477\n",
      "Training Epoch: 3 [16960/49669]\tLoss: 3358.3328\n",
      "Training Epoch: 3 [16980/49669]\tLoss: 3074.6890\n",
      "Training Epoch: 3 [17000/49669]\tLoss: 2937.0051\n",
      "Training Epoch: 3 [17020/49669]\tLoss: 3263.5505\n",
      "Training Epoch: 3 [17040/49669]\tLoss: 2998.2278\n",
      "Training Epoch: 3 [17060/49669]\tLoss: 2907.8528\n",
      "Training Epoch: 3 [17080/49669]\tLoss: 3224.5251\n",
      "Training Epoch: 3 [17100/49669]\tLoss: 3214.9116\n",
      "Training Epoch: 3 [17120/49669]\tLoss: 3606.9890\n",
      "Training Epoch: 3 [17140/49669]\tLoss: 3020.1594\n",
      "Training Epoch: 3 [17160/49669]\tLoss: 3111.7080\n",
      "Training Epoch: 3 [17180/49669]\tLoss: 3234.4756\n",
      "Training Epoch: 3 [17200/49669]\tLoss: 2855.6553\n",
      "Training Epoch: 3 [17220/49669]\tLoss: 3312.2883\n",
      "Training Epoch: 3 [17240/49669]\tLoss: 3200.6025\n",
      "Training Epoch: 3 [17260/49669]\tLoss: 3170.0037\n",
      "Training Epoch: 3 [17280/49669]\tLoss: 2895.7522\n",
      "Training Epoch: 3 [17300/49669]\tLoss: 3624.1584\n",
      "Training Epoch: 3 [17320/49669]\tLoss: 3125.7068\n",
      "Training Epoch: 3 [17340/49669]\tLoss: 2983.6187\n",
      "Training Epoch: 3 [17360/49669]\tLoss: 3033.4739\n",
      "Training Epoch: 3 [17380/49669]\tLoss: 3195.4395\n",
      "Training Epoch: 3 [17400/49669]\tLoss: 3463.9507\n",
      "Training Epoch: 3 [17420/49669]\tLoss: 2806.0938\n",
      "Training Epoch: 3 [17440/49669]\tLoss: 2768.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [17460/49669]\tLoss: 3316.4395\n",
      "Training Epoch: 3 [17480/49669]\tLoss: 3030.9070\n",
      "Training Epoch: 3 [17500/49669]\tLoss: 3242.4214\n",
      "Training Epoch: 3 [17520/49669]\tLoss: 3130.7266\n",
      "Training Epoch: 3 [17540/49669]\tLoss: 2724.6262\n",
      "Training Epoch: 3 [17560/49669]\tLoss: 3275.3203\n",
      "Training Epoch: 3 [17580/49669]\tLoss: 2881.0532\n",
      "Training Epoch: 3 [17600/49669]\tLoss: 3151.9124\n",
      "Training Epoch: 3 [17620/49669]\tLoss: 3176.5051\n",
      "Training Epoch: 3 [17640/49669]\tLoss: 3324.2141\n",
      "Training Epoch: 3 [17660/49669]\tLoss: 3019.0605\n",
      "Training Epoch: 3 [17680/49669]\tLoss: 2702.8179\n",
      "Training Epoch: 3 [17700/49669]\tLoss: 3384.3997\n",
      "Training Epoch: 3 [17720/49669]\tLoss: 3275.0427\n",
      "Training Epoch: 3 [17740/49669]\tLoss: 3444.7917\n",
      "Training Epoch: 3 [17760/49669]\tLoss: 2990.3391\n",
      "Training Epoch: 3 [17780/49669]\tLoss: 3134.4951\n",
      "Training Epoch: 3 [17800/49669]\tLoss: 3098.0078\n",
      "Training Epoch: 3 [17820/49669]\tLoss: 3091.8582\n",
      "Training Epoch: 3 [17840/49669]\tLoss: 3276.1404\n",
      "Training Epoch: 3 [17860/49669]\tLoss: 2780.1255\n",
      "Training Epoch: 3 [17880/49669]\tLoss: 3077.9226\n",
      "Training Epoch: 3 [17900/49669]\tLoss: 3182.9321\n",
      "Training Epoch: 3 [17920/49669]\tLoss: 3236.7166\n",
      "Training Epoch: 3 [17940/49669]\tLoss: 3521.0391\n",
      "Training Epoch: 3 [17960/49669]\tLoss: 3079.7905\n",
      "Training Epoch: 3 [17980/49669]\tLoss: 2848.2200\n",
      "Training Epoch: 3 [18000/49669]\tLoss: 3218.5720\n",
      "Training Epoch: 3 [18020/49669]\tLoss: 2696.4136\n",
      "Training Epoch: 3 [18040/49669]\tLoss: 2919.6333\n",
      "Training Epoch: 3 [18060/49669]\tLoss: 2935.1475\n",
      "Training Epoch: 3 [18080/49669]\tLoss: 3133.1895\n",
      "Training Epoch: 3 [18100/49669]\tLoss: 2969.6228\n",
      "Training Epoch: 3 [18120/49669]\tLoss: 3546.5146\n",
      "Training Epoch: 3 [18140/49669]\tLoss: 2814.5344\n",
      "Training Epoch: 3 [18160/49669]\tLoss: 3175.0625\n",
      "Training Epoch: 3 [18180/49669]\tLoss: 3543.5132\n",
      "Training Epoch: 3 [18200/49669]\tLoss: 2920.8904\n",
      "Training Epoch: 3 [18220/49669]\tLoss: 3119.2126\n",
      "Training Epoch: 3 [18240/49669]\tLoss: 2933.2200\n",
      "Training Epoch: 3 [18260/49669]\tLoss: 3416.9287\n",
      "Training Epoch: 3 [18280/49669]\tLoss: 3113.2253\n",
      "Training Epoch: 3 [18300/49669]\tLoss: 3082.6196\n",
      "Training Epoch: 3 [18320/49669]\tLoss: 3158.7322\n",
      "Training Epoch: 3 [18340/49669]\tLoss: 3156.9294\n",
      "Training Epoch: 3 [18360/49669]\tLoss: 3146.7332\n",
      "Training Epoch: 3 [18380/49669]\tLoss: 3060.6602\n",
      "Training Epoch: 3 [18400/49669]\tLoss: 3098.3494\n",
      "Training Epoch: 3 [18420/49669]\tLoss: 3161.9363\n",
      "Training Epoch: 3 [18440/49669]\tLoss: 3226.3025\n",
      "Training Epoch: 3 [18460/49669]\tLoss: 2937.4963\n",
      "Training Epoch: 3 [18480/49669]\tLoss: 3315.1133\n",
      "Training Epoch: 3 [18500/49669]\tLoss: 2993.5835\n",
      "Training Epoch: 3 [18520/49669]\tLoss: 3053.1089\n",
      "Training Epoch: 3 [18540/49669]\tLoss: 3037.9031\n",
      "Training Epoch: 3 [18560/49669]\tLoss: 2720.9922\n",
      "Training Epoch: 3 [18580/49669]\tLoss: 3462.6851\n",
      "Training Epoch: 3 [18600/49669]\tLoss: 2926.4136\n",
      "Training Epoch: 3 [18620/49669]\tLoss: 2884.1323\n",
      "Training Epoch: 3 [18640/49669]\tLoss: 3004.8447\n",
      "Training Epoch: 3 [18660/49669]\tLoss: 3073.5120\n",
      "Training Epoch: 3 [18680/49669]\tLoss: 2662.3489\n",
      "Training Epoch: 3 [18700/49669]\tLoss: 2893.5408\n",
      "Training Epoch: 3 [18720/49669]\tLoss: 3343.1174\n",
      "Training Epoch: 3 [18740/49669]\tLoss: 3075.6721\n",
      "Training Epoch: 3 [18760/49669]\tLoss: 3024.4177\n",
      "Training Epoch: 3 [18780/49669]\tLoss: 3177.1589\n",
      "Training Epoch: 3 [18800/49669]\tLoss: 3196.4907\n",
      "Training Epoch: 3 [18820/49669]\tLoss: 2818.9580\n",
      "Training Epoch: 3 [18840/49669]\tLoss: 3380.4568\n",
      "Training Epoch: 3 [18860/49669]\tLoss: 2689.5637\n",
      "Training Epoch: 3 [18880/49669]\tLoss: 3097.8210\n",
      "Training Epoch: 3 [18900/49669]\tLoss: 3100.2798\n",
      "Training Epoch: 3 [18920/49669]\tLoss: 2962.6089\n",
      "Training Epoch: 3 [18940/49669]\tLoss: 3345.0427\n",
      "Training Epoch: 3 [18960/49669]\tLoss: 2788.0557\n",
      "Training Epoch: 3 [18980/49669]\tLoss: 2854.7136\n",
      "Training Epoch: 3 [19000/49669]\tLoss: 3158.8823\n",
      "Training Epoch: 3 [19020/49669]\tLoss: 3014.4141\n",
      "Training Epoch: 3 [19040/49669]\tLoss: 3357.5654\n",
      "Training Epoch: 3 [19060/49669]\tLoss: 2993.5796\n",
      "Training Epoch: 3 [19080/49669]\tLoss: 3414.9038\n",
      "Training Epoch: 3 [19100/49669]\tLoss: 3007.9719\n",
      "Training Epoch: 3 [19120/49669]\tLoss: 2928.4275\n",
      "Training Epoch: 3 [19140/49669]\tLoss: 3105.6819\n",
      "Training Epoch: 3 [19160/49669]\tLoss: 2793.9202\n",
      "Training Epoch: 3 [19180/49669]\tLoss: 3030.8044\n",
      "Training Epoch: 3 [19200/49669]\tLoss: 2852.0022\n",
      "Training Epoch: 3 [19220/49669]\tLoss: 2819.9783\n",
      "Training Epoch: 3 [19240/49669]\tLoss: 3121.2698\n",
      "Training Epoch: 3 [19260/49669]\tLoss: 2766.9941\n",
      "Training Epoch: 3 [19280/49669]\tLoss: 2924.5344\n",
      "Training Epoch: 3 [19300/49669]\tLoss: 3295.6680\n",
      "Training Epoch: 3 [19320/49669]\tLoss: 3079.8411\n",
      "Training Epoch: 3 [19340/49669]\tLoss: 3048.6165\n",
      "Training Epoch: 3 [19360/49669]\tLoss: 2785.2966\n",
      "Training Epoch: 3 [19380/49669]\tLoss: 2948.5813\n",
      "Training Epoch: 3 [19400/49669]\tLoss: 2954.0388\n",
      "Training Epoch: 3 [19420/49669]\tLoss: 2598.6025\n",
      "Training Epoch: 3 [19440/49669]\tLoss: 2952.4155\n",
      "Training Epoch: 3 [19460/49669]\tLoss: 2980.4785\n",
      "Training Epoch: 3 [19480/49669]\tLoss: 2979.0503\n",
      "Training Epoch: 3 [19500/49669]\tLoss: 3401.7017\n",
      "Training Epoch: 3 [19520/49669]\tLoss: 3168.4226\n",
      "Training Epoch: 3 [19540/49669]\tLoss: 3265.2639\n",
      "Training Epoch: 3 [19560/49669]\tLoss: 3238.4749\n",
      "Training Epoch: 3 [19580/49669]\tLoss: 2989.0745\n",
      "Training Epoch: 3 [19600/49669]\tLoss: 2905.0137\n",
      "Training Epoch: 3 [19620/49669]\tLoss: 2359.2100\n",
      "Training Epoch: 3 [19640/49669]\tLoss: 3189.2173\n",
      "Training Epoch: 3 [19660/49669]\tLoss: 3009.5774\n",
      "Training Epoch: 3 [19680/49669]\tLoss: 3191.6279\n",
      "Training Epoch: 3 [19700/49669]\tLoss: 2752.0696\n",
      "Training Epoch: 3 [19720/49669]\tLoss: 3391.1941\n",
      "Training Epoch: 3 [19740/49669]\tLoss: 2647.5247\n",
      "Training Epoch: 3 [19760/49669]\tLoss: 3186.4226\n",
      "Training Epoch: 3 [19780/49669]\tLoss: 2792.1758\n",
      "Training Epoch: 3 [19800/49669]\tLoss: 2947.3179\n",
      "Training Epoch: 3 [19820/49669]\tLoss: 3140.3572\n",
      "Training Epoch: 3 [19840/49669]\tLoss: 3095.7969\n",
      "Training Epoch: 3 [19860/49669]\tLoss: 3126.4492\n",
      "Training Epoch: 3 [19880/49669]\tLoss: 3038.0066\n",
      "Training Epoch: 3 [19900/49669]\tLoss: 2740.4319\n",
      "Training Epoch: 3 [19920/49669]\tLoss: 2934.6941\n",
      "Training Epoch: 3 [19940/49669]\tLoss: 3031.8257\n",
      "Training Epoch: 3 [19960/49669]\tLoss: 3362.7058\n",
      "Training Epoch: 3 [19980/49669]\tLoss: 2580.3005\n",
      "Training Epoch: 3 [20000/49669]\tLoss: 3297.1731\n",
      "Training Epoch: 3 [20020/49669]\tLoss: 3214.0818\n",
      "Training Epoch: 3 [20040/49669]\tLoss: 3013.3728\n",
      "Training Epoch: 3 [20060/49669]\tLoss: 2675.1362\n",
      "Training Epoch: 3 [20080/49669]\tLoss: 2508.0789\n",
      "Training Epoch: 3 [20100/49669]\tLoss: 3136.2534\n",
      "Training Epoch: 3 [20120/49669]\tLoss: 2785.5579\n",
      "Training Epoch: 3 [20140/49669]\tLoss: 3088.0647\n",
      "Training Epoch: 3 [20160/49669]\tLoss: 3390.2356\n",
      "Training Epoch: 3 [20180/49669]\tLoss: 3505.2480\n",
      "Training Epoch: 3 [20200/49669]\tLoss: 2933.1956\n",
      "Training Epoch: 3 [20220/49669]\tLoss: 2951.9146\n",
      "Training Epoch: 3 [20240/49669]\tLoss: 2991.2056\n",
      "Training Epoch: 3 [20260/49669]\tLoss: 3083.5764\n",
      "Training Epoch: 3 [20280/49669]\tLoss: 3076.9922\n",
      "Training Epoch: 3 [20300/49669]\tLoss: 2546.3840\n",
      "Training Epoch: 3 [20320/49669]\tLoss: 2939.7695\n",
      "Training Epoch: 3 [20340/49669]\tLoss: 2815.8682\n",
      "Training Epoch: 3 [20360/49669]\tLoss: 2680.8254\n",
      "Training Epoch: 3 [20380/49669]\tLoss: 3027.8374\n",
      "Training Epoch: 3 [20400/49669]\tLoss: 2902.3071\n",
      "Training Epoch: 3 [20420/49669]\tLoss: 2801.6775\n",
      "Training Epoch: 3 [20440/49669]\tLoss: 2857.7849\n",
      "Training Epoch: 3 [20460/49669]\tLoss: 3168.8855\n",
      "Training Epoch: 3 [20480/49669]\tLoss: 3067.1665\n",
      "Training Epoch: 3 [20500/49669]\tLoss: 2870.0815\n",
      "Training Epoch: 3 [20520/49669]\tLoss: 3147.3892\n",
      "Training Epoch: 3 [20540/49669]\tLoss: 3436.2202\n",
      "Training Epoch: 3 [20560/49669]\tLoss: 2788.8518\n",
      "Training Epoch: 3 [20580/49669]\tLoss: 3088.7507\n",
      "Training Epoch: 3 [20600/49669]\tLoss: 2867.7129\n",
      "Training Epoch: 3 [20620/49669]\tLoss: 3048.4114\n",
      "Training Epoch: 3 [20640/49669]\tLoss: 2886.9805\n",
      "Training Epoch: 3 [20660/49669]\tLoss: 3057.2930\n",
      "Training Epoch: 3 [20680/49669]\tLoss: 2872.2366\n",
      "Training Epoch: 3 [20700/49669]\tLoss: 2991.9490\n",
      "Training Epoch: 3 [20720/49669]\tLoss: 2707.8320\n",
      "Training Epoch: 3 [20740/49669]\tLoss: 2932.3555\n",
      "Training Epoch: 3 [20760/49669]\tLoss: 2980.8005\n",
      "Training Epoch: 3 [20780/49669]\tLoss: 3402.1699\n",
      "Training Epoch: 3 [20800/49669]\tLoss: 3442.3862\n",
      "Training Epoch: 3 [20820/49669]\tLoss: 3239.8057\n",
      "Training Epoch: 3 [20840/49669]\tLoss: 2823.9055\n",
      "Training Epoch: 3 [20860/49669]\tLoss: 3041.0662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [20880/49669]\tLoss: 3050.3782\n",
      "Training Epoch: 3 [20900/49669]\tLoss: 3164.9199\n",
      "Training Epoch: 3 [20920/49669]\tLoss: 3068.7830\n",
      "Training Epoch: 3 [20940/49669]\tLoss: 3269.1030\n",
      "Training Epoch: 3 [20960/49669]\tLoss: 3237.2947\n",
      "Training Epoch: 3 [20980/49669]\tLoss: 2840.9949\n",
      "Training Epoch: 3 [21000/49669]\tLoss: 3351.4880\n",
      "Training Epoch: 3 [21020/49669]\tLoss: 2973.4294\n",
      "Training Epoch: 3 [21040/49669]\tLoss: 2963.8540\n",
      "Training Epoch: 3 [21060/49669]\tLoss: 3263.8328\n",
      "Training Epoch: 3 [21080/49669]\tLoss: 2853.0378\n",
      "Training Epoch: 3 [21100/49669]\tLoss: 3276.1099\n",
      "Training Epoch: 3 [21120/49669]\tLoss: 3116.7505\n",
      "Training Epoch: 3 [21140/49669]\tLoss: 2938.4492\n",
      "Training Epoch: 3 [21160/49669]\tLoss: 3149.2646\n",
      "Training Epoch: 3 [21180/49669]\tLoss: 2780.0381\n",
      "Training Epoch: 3 [21200/49669]\tLoss: 2791.6804\n",
      "Training Epoch: 3 [21220/49669]\tLoss: 2944.4585\n",
      "Training Epoch: 3 [21240/49669]\tLoss: 3157.7603\n",
      "Training Epoch: 3 [21260/49669]\tLoss: 3179.3188\n",
      "Training Epoch: 3 [21280/49669]\tLoss: 2975.3259\n",
      "Training Epoch: 3 [21300/49669]\tLoss: 2743.2302\n",
      "Training Epoch: 3 [21320/49669]\tLoss: 2889.7383\n",
      "Training Epoch: 3 [21340/49669]\tLoss: 3028.4714\n",
      "Training Epoch: 3 [21360/49669]\tLoss: 3092.3914\n",
      "Training Epoch: 3 [21380/49669]\tLoss: 2829.1118\n",
      "Training Epoch: 3 [21400/49669]\tLoss: 3030.3892\n",
      "Training Epoch: 3 [21420/49669]\tLoss: 2632.6802\n",
      "Training Epoch: 3 [21440/49669]\tLoss: 2992.7903\n",
      "Training Epoch: 3 [21460/49669]\tLoss: 2927.4077\n",
      "Training Epoch: 3 [21480/49669]\tLoss: 3148.8477\n",
      "Training Epoch: 3 [21500/49669]\tLoss: 2822.9783\n",
      "Training Epoch: 3 [21520/49669]\tLoss: 3135.0334\n",
      "Training Epoch: 3 [21540/49669]\tLoss: 3105.0903\n",
      "Training Epoch: 3 [21560/49669]\tLoss: 2987.8616\n",
      "Training Epoch: 3 [21580/49669]\tLoss: 3128.3796\n",
      "Training Epoch: 3 [21600/49669]\tLoss: 2907.1895\n",
      "Training Epoch: 3 [21620/49669]\tLoss: 2847.5500\n",
      "Training Epoch: 3 [21640/49669]\tLoss: 3072.7593\n",
      "Training Epoch: 3 [21660/49669]\tLoss: 3022.3267\n",
      "Training Epoch: 3 [21680/49669]\tLoss: 2975.7241\n",
      "Training Epoch: 3 [21700/49669]\tLoss: 3059.4548\n",
      "Training Epoch: 3 [21720/49669]\tLoss: 2694.3218\n",
      "Training Epoch: 3 [21740/49669]\tLoss: 2811.3701\n",
      "Training Epoch: 3 [21760/49669]\tLoss: 2631.5579\n",
      "Training Epoch: 3 [21780/49669]\tLoss: 3018.9094\n",
      "Training Epoch: 3 [21800/49669]\tLoss: 2819.6777\n",
      "Training Epoch: 3 [21820/49669]\tLoss: 3066.4001\n",
      "Training Epoch: 3 [21840/49669]\tLoss: 3015.7566\n",
      "Training Epoch: 3 [21860/49669]\tLoss: 2810.2324\n",
      "Training Epoch: 3 [21880/49669]\tLoss: 2902.0364\n",
      "Training Epoch: 3 [21900/49669]\tLoss: 3124.0364\n",
      "Training Epoch: 3 [21920/49669]\tLoss: 3109.2717\n",
      "Training Epoch: 3 [21940/49669]\tLoss: 3200.1814\n",
      "Training Epoch: 3 [21960/49669]\tLoss: 2966.7358\n",
      "Training Epoch: 3 [21980/49669]\tLoss: 2968.9382\n",
      "Training Epoch: 3 [22000/49669]\tLoss: 2912.4224\n",
      "Training Epoch: 3 [22020/49669]\tLoss: 3277.8677\n",
      "Training Epoch: 3 [22040/49669]\tLoss: 2834.5835\n",
      "Training Epoch: 3 [22060/49669]\tLoss: 3047.1709\n",
      "Training Epoch: 3 [22080/49669]\tLoss: 3340.3323\n",
      "Training Epoch: 3 [22100/49669]\tLoss: 2935.3992\n",
      "Training Epoch: 3 [22120/49669]\tLoss: 3016.1155\n",
      "Training Epoch: 3 [22140/49669]\tLoss: 3061.0137\n",
      "Training Epoch: 3 [22160/49669]\tLoss: 3311.3562\n",
      "Training Epoch: 3 [22180/49669]\tLoss: 2642.4160\n",
      "Training Epoch: 3 [22200/49669]\tLoss: 3158.0388\n",
      "Training Epoch: 3 [22220/49669]\tLoss: 2689.2625\n",
      "Training Epoch: 3 [22240/49669]\tLoss: 2593.4458\n",
      "Training Epoch: 3 [22260/49669]\tLoss: 3147.9895\n",
      "Training Epoch: 3 [22280/49669]\tLoss: 2959.8538\n",
      "Training Epoch: 3 [22300/49669]\tLoss: 2873.8665\n",
      "Training Epoch: 3 [22320/49669]\tLoss: 2853.2205\n",
      "Training Epoch: 3 [22340/49669]\tLoss: 3349.1799\n",
      "Training Epoch: 3 [22360/49669]\tLoss: 2925.4941\n",
      "Training Epoch: 3 [22380/49669]\tLoss: 3099.7778\n",
      "Training Epoch: 3 [22400/49669]\tLoss: 2934.1079\n",
      "Training Epoch: 3 [22420/49669]\tLoss: 3081.1846\n",
      "Training Epoch: 3 [22440/49669]\tLoss: 3074.8833\n",
      "Training Epoch: 3 [22460/49669]\tLoss: 3199.1748\n",
      "Training Epoch: 3 [22480/49669]\tLoss: 3051.9517\n",
      "Training Epoch: 3 [22500/49669]\tLoss: 3174.3704\n",
      "Training Epoch: 3 [22520/49669]\tLoss: 2925.9927\n",
      "Training Epoch: 3 [22540/49669]\tLoss: 2595.9800\n",
      "Training Epoch: 3 [22560/49669]\tLoss: 3016.2429\n",
      "Training Epoch: 3 [22580/49669]\tLoss: 2733.6646\n",
      "Training Epoch: 3 [22600/49669]\tLoss: 2685.5269\n",
      "Training Epoch: 3 [22620/49669]\tLoss: 3429.7253\n",
      "Training Epoch: 3 [22640/49669]\tLoss: 2966.9453\n",
      "Training Epoch: 3 [22660/49669]\tLoss: 3476.6367\n",
      "Training Epoch: 3 [22680/49669]\tLoss: 2719.5437\n",
      "Training Epoch: 3 [22700/49669]\tLoss: 3050.7747\n",
      "Training Epoch: 3 [22720/49669]\tLoss: 3103.6780\n",
      "Training Epoch: 3 [22740/49669]\tLoss: 3035.6892\n",
      "Training Epoch: 3 [22760/49669]\tLoss: 2633.6799\n",
      "Training Epoch: 3 [22780/49669]\tLoss: 3304.2935\n",
      "Training Epoch: 3 [22800/49669]\tLoss: 2963.1116\n",
      "Training Epoch: 3 [22820/49669]\tLoss: 3213.2393\n",
      "Training Epoch: 3 [22840/49669]\tLoss: 2879.7183\n",
      "Training Epoch: 3 [22860/49669]\tLoss: 3063.6560\n",
      "Training Epoch: 3 [22880/49669]\tLoss: 3033.1223\n",
      "Training Epoch: 3 [22900/49669]\tLoss: 2838.1790\n",
      "Training Epoch: 3 [22920/49669]\tLoss: 3088.6768\n",
      "Training Epoch: 3 [22940/49669]\tLoss: 3018.9443\n",
      "Training Epoch: 3 [22960/49669]\tLoss: 3100.9458\n",
      "Training Epoch: 3 [22980/49669]\tLoss: 3152.2002\n",
      "Training Epoch: 3 [23000/49669]\tLoss: 3085.7258\n",
      "Training Epoch: 3 [23020/49669]\tLoss: 2890.2141\n",
      "Training Epoch: 3 [23040/49669]\tLoss: 3204.8770\n",
      "Training Epoch: 3 [23060/49669]\tLoss: 3107.2668\n",
      "Training Epoch: 3 [23080/49669]\tLoss: 2853.6677\n",
      "Training Epoch: 3 [23100/49669]\tLoss: 3414.3608\n",
      "Training Epoch: 3 [23120/49669]\tLoss: 2967.6006\n",
      "Training Epoch: 3 [23140/49669]\tLoss: 2795.7461\n",
      "Training Epoch: 3 [23160/49669]\tLoss: 3020.4482\n",
      "Training Epoch: 3 [23180/49669]\tLoss: 2921.4861\n",
      "Training Epoch: 3 [23200/49669]\tLoss: 3374.2314\n",
      "Training Epoch: 3 [23220/49669]\tLoss: 2901.2505\n",
      "Training Epoch: 3 [23240/49669]\tLoss: 3132.9209\n",
      "Training Epoch: 3 [23260/49669]\tLoss: 3233.9016\n",
      "Training Epoch: 3 [23280/49669]\tLoss: 3062.2256\n",
      "Training Epoch: 3 [23300/49669]\tLoss: 2972.5798\n",
      "Training Epoch: 3 [23320/49669]\tLoss: 3282.3550\n",
      "Training Epoch: 3 [23340/49669]\tLoss: 2552.5745\n",
      "Training Epoch: 3 [23360/49669]\tLoss: 2930.1511\n",
      "Training Epoch: 3 [23380/49669]\tLoss: 3042.0171\n",
      "Training Epoch: 3 [23400/49669]\tLoss: 3170.1755\n",
      "Training Epoch: 3 [23420/49669]\tLoss: 3069.9434\n",
      "Training Epoch: 3 [23440/49669]\tLoss: 2952.5479\n",
      "Training Epoch: 3 [23460/49669]\tLoss: 3004.1753\n",
      "Training Epoch: 3 [23480/49669]\tLoss: 3023.1299\n",
      "Training Epoch: 3 [23500/49669]\tLoss: 2969.5552\n",
      "Training Epoch: 3 [23520/49669]\tLoss: 2551.9680\n",
      "Training Epoch: 3 [23540/49669]\tLoss: 3021.4824\n",
      "Training Epoch: 3 [23560/49669]\tLoss: 2707.1382\n",
      "Training Epoch: 3 [23580/49669]\tLoss: 2822.9014\n",
      "Training Epoch: 3 [23600/49669]\tLoss: 2993.7144\n",
      "Training Epoch: 3 [23620/49669]\tLoss: 2897.0154\n",
      "Training Epoch: 3 [23640/49669]\tLoss: 2730.0925\n",
      "Training Epoch: 3 [23660/49669]\tLoss: 3019.0437\n",
      "Training Epoch: 3 [23680/49669]\tLoss: 2644.9832\n",
      "Training Epoch: 3 [23700/49669]\tLoss: 2832.8076\n",
      "Training Epoch: 3 [23720/49669]\tLoss: 2934.6440\n",
      "Training Epoch: 3 [23740/49669]\tLoss: 2877.4473\n",
      "Training Epoch: 3 [23760/49669]\tLoss: 2766.3862\n",
      "Training Epoch: 3 [23780/49669]\tLoss: 2987.2271\n",
      "Training Epoch: 3 [23800/49669]\tLoss: 2984.4314\n",
      "Training Epoch: 3 [23820/49669]\tLoss: 2926.2720\n",
      "Training Epoch: 3 [23840/49669]\tLoss: 2958.5862\n",
      "Training Epoch: 3 [23860/49669]\tLoss: 2483.1086\n",
      "Training Epoch: 3 [23880/49669]\tLoss: 2919.4653\n",
      "Training Epoch: 3 [23900/49669]\tLoss: 3013.4290\n",
      "Training Epoch: 3 [23920/49669]\tLoss: 2898.2991\n",
      "Training Epoch: 3 [23940/49669]\tLoss: 2544.5959\n",
      "Training Epoch: 3 [23960/49669]\tLoss: 2735.8657\n",
      "Training Epoch: 3 [23980/49669]\tLoss: 3150.7991\n",
      "Training Epoch: 3 [24000/49669]\tLoss: 3125.5850\n",
      "Training Epoch: 3 [24020/49669]\tLoss: 3045.3398\n",
      "Training Epoch: 3 [24040/49669]\tLoss: 2964.2217\n",
      "Training Epoch: 3 [24060/49669]\tLoss: 2765.4829\n",
      "Training Epoch: 3 [24080/49669]\tLoss: 2786.8457\n",
      "Training Epoch: 3 [24100/49669]\tLoss: 3211.8525\n",
      "Training Epoch: 3 [24120/49669]\tLoss: 2570.9934\n",
      "Training Epoch: 3 [24140/49669]\tLoss: 2493.0432\n",
      "Training Epoch: 3 [24160/49669]\tLoss: 3181.9988\n",
      "Training Epoch: 3 [24180/49669]\tLoss: 2636.7639\n",
      "Training Epoch: 3 [24200/49669]\tLoss: 3325.2043\n",
      "Training Epoch: 3 [24220/49669]\tLoss: 3442.2739\n",
      "Training Epoch: 3 [24240/49669]\tLoss: 3298.3501\n",
      "Training Epoch: 3 [24260/49669]\tLoss: 2937.0103\n",
      "Training Epoch: 3 [24280/49669]\tLoss: 2722.2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [24300/49669]\tLoss: 2539.8838\n",
      "Training Epoch: 3 [24320/49669]\tLoss: 3053.2568\n",
      "Training Epoch: 3 [24340/49669]\tLoss: 2852.5024\n",
      "Training Epoch: 3 [24360/49669]\tLoss: 3122.7498\n",
      "Training Epoch: 3 [24380/49669]\tLoss: 2930.9675\n",
      "Training Epoch: 3 [24400/49669]\tLoss: 3072.8479\n",
      "Training Epoch: 3 [24420/49669]\tLoss: 2758.0503\n",
      "Training Epoch: 3 [24440/49669]\tLoss: 3063.6926\n",
      "Training Epoch: 3 [24460/49669]\tLoss: 2615.3208\n",
      "Training Epoch: 3 [24480/49669]\tLoss: 3262.6458\n",
      "Training Epoch: 3 [24500/49669]\tLoss: 2917.1396\n",
      "Training Epoch: 3 [24520/49669]\tLoss: 2554.4861\n",
      "Training Epoch: 3 [24540/49669]\tLoss: 2866.4351\n",
      "Training Epoch: 3 [24560/49669]\tLoss: 2920.3828\n",
      "Training Epoch: 3 [24580/49669]\tLoss: 3219.8325\n",
      "Training Epoch: 3 [24600/49669]\tLoss: 3114.1833\n",
      "Training Epoch: 3 [24620/49669]\tLoss: 2967.2332\n",
      "Training Epoch: 3 [24640/49669]\tLoss: 2915.8259\n",
      "Training Epoch: 3 [24660/49669]\tLoss: 3169.2705\n",
      "Training Epoch: 3 [24680/49669]\tLoss: 3043.3777\n",
      "Training Epoch: 3 [24700/49669]\tLoss: 2548.6062\n",
      "Training Epoch: 3 [24720/49669]\tLoss: 2717.1704\n",
      "Training Epoch: 3 [24740/49669]\tLoss: 2774.8945\n",
      "Training Epoch: 3 [24760/49669]\tLoss: 2720.6794\n",
      "Training Epoch: 3 [24780/49669]\tLoss: 3241.1741\n",
      "Training Epoch: 3 [24800/49669]\tLoss: 3046.4866\n",
      "Training Epoch: 3 [24820/49669]\tLoss: 3110.1736\n",
      "Training Epoch: 3 [24840/49669]\tLoss: 3254.5789\n",
      "Training Epoch: 3 [24860/49669]\tLoss: 3007.8638\n",
      "Training Epoch: 3 [24880/49669]\tLoss: 3201.6196\n",
      "Training Epoch: 3 [24900/49669]\tLoss: 2927.2527\n",
      "Training Epoch: 3 [24920/49669]\tLoss: 3613.7080\n",
      "Training Epoch: 3 [24940/49669]\tLoss: 3121.4871\n",
      "Training Epoch: 3 [24960/49669]\tLoss: 3022.3159\n",
      "Training Epoch: 3 [24980/49669]\tLoss: 2952.6143\n",
      "Training Epoch: 3 [25000/49669]\tLoss: 3114.4641\n",
      "Training Epoch: 3 [25020/49669]\tLoss: 2960.4229\n",
      "Training Epoch: 3 [25040/49669]\tLoss: 3136.8870\n",
      "Training Epoch: 3 [25060/49669]\tLoss: 3026.5361\n",
      "Training Epoch: 3 [25080/49669]\tLoss: 3024.6877\n",
      "Training Epoch: 3 [25100/49669]\tLoss: 3100.2241\n",
      "Training Epoch: 3 [25120/49669]\tLoss: 2684.3633\n",
      "Training Epoch: 3 [25140/49669]\tLoss: 2721.6196\n",
      "Training Epoch: 3 [25160/49669]\tLoss: 2756.1360\n",
      "Training Epoch: 3 [25180/49669]\tLoss: 2990.5989\n",
      "Training Epoch: 3 [25200/49669]\tLoss: 2836.9014\n",
      "Training Epoch: 3 [25220/49669]\tLoss: 2739.9636\n",
      "Training Epoch: 3 [25240/49669]\tLoss: 2630.6895\n",
      "Training Epoch: 3 [25260/49669]\tLoss: 2616.2725\n",
      "Training Epoch: 3 [25280/49669]\tLoss: 2941.6951\n",
      "Training Epoch: 3 [25300/49669]\tLoss: 3211.5977\n",
      "Training Epoch: 3 [25320/49669]\tLoss: 2736.8682\n",
      "Training Epoch: 3 [25340/49669]\tLoss: 3107.2908\n",
      "Training Epoch: 3 [25360/49669]\tLoss: 3048.8215\n",
      "Training Epoch: 3 [25380/49669]\tLoss: 2927.9397\n",
      "Training Epoch: 3 [25400/49669]\tLoss: 2736.0835\n",
      "Training Epoch: 3 [25420/49669]\tLoss: 2997.6753\n",
      "Training Epoch: 3 [25440/49669]\tLoss: 3415.7292\n",
      "Training Epoch: 3 [25460/49669]\tLoss: 2670.9941\n",
      "Training Epoch: 3 [25480/49669]\tLoss: 3036.7954\n",
      "Training Epoch: 3 [25500/49669]\tLoss: 3239.0005\n",
      "Training Epoch: 3 [25520/49669]\tLoss: 3132.5713\n",
      "Training Epoch: 3 [25540/49669]\tLoss: 3049.4236\n",
      "Training Epoch: 3 [25560/49669]\tLoss: 2720.2637\n",
      "Training Epoch: 3 [25580/49669]\tLoss: 2868.1304\n",
      "Training Epoch: 3 [25600/49669]\tLoss: 2518.0286\n",
      "Training Epoch: 3 [25620/49669]\tLoss: 2493.4568\n",
      "Training Epoch: 3 [25640/49669]\tLoss: 2818.0203\n",
      "Training Epoch: 3 [25660/49669]\tLoss: 2557.4360\n",
      "Training Epoch: 3 [25680/49669]\tLoss: 2817.1741\n",
      "Training Epoch: 3 [25700/49669]\tLoss: 2969.9006\n",
      "Training Epoch: 3 [25720/49669]\tLoss: 3039.3147\n",
      "Training Epoch: 3 [25740/49669]\tLoss: 2910.6470\n",
      "Training Epoch: 3 [25760/49669]\tLoss: 2948.9485\n",
      "Training Epoch: 3 [25780/49669]\tLoss: 2870.9980\n",
      "Training Epoch: 3 [25800/49669]\tLoss: 2881.0767\n",
      "Training Epoch: 3 [25820/49669]\tLoss: 2901.3508\n",
      "Training Epoch: 3 [25840/49669]\tLoss: 3103.6248\n",
      "Training Epoch: 3 [25860/49669]\tLoss: 2794.7192\n",
      "Training Epoch: 3 [25880/49669]\tLoss: 2621.5891\n",
      "Training Epoch: 3 [25900/49669]\tLoss: 2671.6143\n",
      "Training Epoch: 3 [25920/49669]\tLoss: 2843.8599\n",
      "Training Epoch: 3 [25940/49669]\tLoss: 2649.6768\n",
      "Training Epoch: 3 [25960/49669]\tLoss: 2853.3071\n",
      "Training Epoch: 3 [25980/49669]\tLoss: 2419.9875\n",
      "Training Epoch: 3 [26000/49669]\tLoss: 2719.3020\n",
      "Training Epoch: 3 [26020/49669]\tLoss: 3248.3608\n",
      "Training Epoch: 3 [26040/49669]\tLoss: 3335.8477\n",
      "Training Epoch: 3 [26060/49669]\tLoss: 2643.1882\n",
      "Training Epoch: 3 [26080/49669]\tLoss: 2932.8694\n",
      "Training Epoch: 3 [26100/49669]\tLoss: 3223.5203\n",
      "Training Epoch: 3 [26120/49669]\tLoss: 2990.7617\n",
      "Training Epoch: 3 [26140/49669]\tLoss: 2914.2585\n",
      "Training Epoch: 3 [26160/49669]\tLoss: 2926.8726\n",
      "Training Epoch: 3 [26180/49669]\tLoss: 2482.9675\n",
      "Training Epoch: 3 [26200/49669]\tLoss: 2731.7285\n",
      "Training Epoch: 3 [26220/49669]\tLoss: 2841.1995\n",
      "Training Epoch: 3 [26240/49669]\tLoss: 2816.2292\n",
      "Training Epoch: 3 [26260/49669]\tLoss: 3224.1038\n",
      "Training Epoch: 3 [26280/49669]\tLoss: 2555.5205\n",
      "Training Epoch: 3 [26300/49669]\tLoss: 2809.2888\n",
      "Training Epoch: 3 [26320/49669]\tLoss: 3161.1907\n",
      "Training Epoch: 3 [26340/49669]\tLoss: 3090.4578\n",
      "Training Epoch: 3 [26360/49669]\tLoss: 3291.3801\n",
      "Training Epoch: 3 [26380/49669]\tLoss: 2726.7717\n",
      "Training Epoch: 3 [26400/49669]\tLoss: 3030.0898\n",
      "Training Epoch: 3 [26420/49669]\tLoss: 2756.3047\n",
      "Training Epoch: 3 [26440/49669]\tLoss: 2975.1353\n",
      "Training Epoch: 3 [26460/49669]\tLoss: 3029.4160\n",
      "Training Epoch: 3 [26480/49669]\tLoss: 2843.7883\n",
      "Training Epoch: 3 [26500/49669]\tLoss: 3127.0354\n",
      "Training Epoch: 3 [26520/49669]\tLoss: 2788.3003\n",
      "Training Epoch: 3 [26540/49669]\tLoss: 2832.3823\n",
      "Training Epoch: 3 [26560/49669]\tLoss: 2908.8928\n",
      "Training Epoch: 3 [26580/49669]\tLoss: 2846.4460\n",
      "Training Epoch: 3 [26600/49669]\tLoss: 3078.4309\n",
      "Training Epoch: 3 [26620/49669]\tLoss: 2534.2612\n",
      "Training Epoch: 3 [26640/49669]\tLoss: 2943.3018\n",
      "Training Epoch: 3 [26660/49669]\tLoss: 3091.2446\n",
      "Training Epoch: 3 [26680/49669]\tLoss: 2687.7336\n",
      "Training Epoch: 3 [26700/49669]\tLoss: 2797.2876\n",
      "Training Epoch: 3 [26720/49669]\tLoss: 2788.8030\n",
      "Training Epoch: 3 [26740/49669]\tLoss: 2661.2888\n",
      "Training Epoch: 3 [26760/49669]\tLoss: 2757.5793\n",
      "Training Epoch: 3 [26780/49669]\tLoss: 2644.7659\n",
      "Training Epoch: 3 [26800/49669]\tLoss: 2820.2026\n",
      "Training Epoch: 3 [26820/49669]\tLoss: 3010.0518\n",
      "Training Epoch: 3 [26840/49669]\tLoss: 3079.9924\n",
      "Training Epoch: 3 [26860/49669]\tLoss: 2873.0950\n",
      "Training Epoch: 3 [26880/49669]\tLoss: 2768.2625\n",
      "Training Epoch: 3 [26900/49669]\tLoss: 2826.8757\n",
      "Training Epoch: 3 [26920/49669]\tLoss: 2479.4338\n",
      "Training Epoch: 3 [26940/49669]\tLoss: 2825.7559\n",
      "Training Epoch: 3 [26960/49669]\tLoss: 2892.3879\n",
      "Training Epoch: 3 [26980/49669]\tLoss: 2764.1040\n",
      "Training Epoch: 3 [27000/49669]\tLoss: 3034.2312\n",
      "Training Epoch: 3 [27020/49669]\tLoss: 3000.8022\n",
      "Training Epoch: 3 [27040/49669]\tLoss: 2731.7344\n",
      "Training Epoch: 3 [27060/49669]\tLoss: 2870.1587\n",
      "Training Epoch: 3 [27080/49669]\tLoss: 2885.9460\n",
      "Training Epoch: 3 [27100/49669]\tLoss: 2740.8574\n",
      "Training Epoch: 3 [27120/49669]\tLoss: 2220.4915\n",
      "Training Epoch: 3 [27140/49669]\tLoss: 2557.4705\n",
      "Training Epoch: 3 [27160/49669]\tLoss: 3092.1418\n",
      "Training Epoch: 3 [27180/49669]\tLoss: 2758.7537\n",
      "Training Epoch: 3 [27200/49669]\tLoss: 2831.4626\n",
      "Training Epoch: 3 [27220/49669]\tLoss: 2588.6108\n",
      "Training Epoch: 3 [27240/49669]\tLoss: 2851.7568\n",
      "Training Epoch: 3 [27260/49669]\tLoss: 2652.7561\n",
      "Training Epoch: 3 [27280/49669]\tLoss: 2911.3364\n",
      "Training Epoch: 3 [27300/49669]\tLoss: 2830.5754\n",
      "Training Epoch: 3 [27320/49669]\tLoss: 3133.8750\n",
      "Training Epoch: 3 [27340/49669]\tLoss: 2949.4048\n",
      "Training Epoch: 3 [27360/49669]\tLoss: 3189.2197\n",
      "Training Epoch: 3 [27380/49669]\tLoss: 2555.2788\n",
      "Training Epoch: 3 [27400/49669]\tLoss: 3030.2886\n",
      "Training Epoch: 3 [27420/49669]\tLoss: 2446.1021\n",
      "Training Epoch: 3 [27440/49669]\tLoss: 2551.6929\n",
      "Training Epoch: 3 [27460/49669]\tLoss: 2855.5823\n",
      "Training Epoch: 3 [27480/49669]\tLoss: 2843.1572\n",
      "Training Epoch: 3 [27500/49669]\tLoss: 2712.8770\n",
      "Training Epoch: 3 [27520/49669]\tLoss: 2629.3218\n",
      "Training Epoch: 3 [27540/49669]\tLoss: 3118.4468\n",
      "Training Epoch: 3 [27560/49669]\tLoss: 3008.4419\n",
      "Training Epoch: 3 [27580/49669]\tLoss: 2802.9758\n",
      "Training Epoch: 3 [27600/49669]\tLoss: 3144.5029\n",
      "Training Epoch: 3 [27620/49669]\tLoss: 2836.7173\n",
      "Training Epoch: 3 [27640/49669]\tLoss: 2851.3337\n",
      "Training Epoch: 3 [27660/49669]\tLoss: 2861.2502\n",
      "Training Epoch: 3 [27680/49669]\tLoss: 2742.8489\n",
      "Training Epoch: 3 [27700/49669]\tLoss: 3088.7173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [27720/49669]\tLoss: 3071.0415\n",
      "Training Epoch: 3 [27740/49669]\tLoss: 3154.2043\n",
      "Training Epoch: 3 [27760/49669]\tLoss: 3185.9441\n",
      "Training Epoch: 3 [27780/49669]\tLoss: 3021.6377\n",
      "Training Epoch: 3 [27800/49669]\tLoss: 2716.6201\n",
      "Training Epoch: 3 [27820/49669]\tLoss: 2651.1868\n",
      "Training Epoch: 3 [27840/49669]\tLoss: 2951.2908\n",
      "Training Epoch: 3 [27860/49669]\tLoss: 2730.2754\n",
      "Training Epoch: 3 [27880/49669]\tLoss: 3051.8909\n",
      "Training Epoch: 3 [27900/49669]\tLoss: 3005.2507\n",
      "Training Epoch: 3 [27920/49669]\tLoss: 2890.4824\n",
      "Training Epoch: 3 [27940/49669]\tLoss: 2975.2930\n",
      "Training Epoch: 3 [27960/49669]\tLoss: 2800.3572\n",
      "Training Epoch: 3 [27980/49669]\tLoss: 2923.3772\n",
      "Training Epoch: 3 [28000/49669]\tLoss: 3052.0945\n",
      "Training Epoch: 3 [28020/49669]\tLoss: 2676.9609\n",
      "Training Epoch: 3 [28040/49669]\tLoss: 2963.4109\n",
      "Training Epoch: 3 [28060/49669]\tLoss: 2874.2046\n",
      "Training Epoch: 3 [28080/49669]\tLoss: 2822.2097\n",
      "Training Epoch: 3 [28100/49669]\tLoss: 2676.5049\n",
      "Training Epoch: 3 [28120/49669]\tLoss: 2931.4192\n",
      "Training Epoch: 3 [28140/49669]\tLoss: 3059.1777\n",
      "Training Epoch: 3 [28160/49669]\tLoss: 2866.1064\n",
      "Training Epoch: 3 [28180/49669]\tLoss: 2772.7908\n",
      "Training Epoch: 3 [28200/49669]\tLoss: 2857.2234\n",
      "Training Epoch: 3 [28220/49669]\tLoss: 2987.4802\n",
      "Training Epoch: 3 [28240/49669]\tLoss: 2849.4604\n",
      "Training Epoch: 3 [28260/49669]\tLoss: 2871.9949\n",
      "Training Epoch: 3 [28280/49669]\tLoss: 2586.4099\n",
      "Training Epoch: 3 [28300/49669]\tLoss: 2764.7878\n",
      "Training Epoch: 3 [28320/49669]\tLoss: 3015.6499\n",
      "Training Epoch: 3 [28340/49669]\tLoss: 2752.2432\n",
      "Training Epoch: 3 [28360/49669]\tLoss: 2826.3223\n",
      "Training Epoch: 3 [28380/49669]\tLoss: 2796.4341\n",
      "Training Epoch: 3 [28400/49669]\tLoss: 2711.3225\n",
      "Training Epoch: 3 [28420/49669]\tLoss: 2697.4812\n",
      "Training Epoch: 3 [28440/49669]\tLoss: 2752.6748\n",
      "Training Epoch: 3 [28460/49669]\tLoss: 2822.7766\n",
      "Training Epoch: 3 [28480/49669]\tLoss: 3104.4412\n",
      "Training Epoch: 3 [28500/49669]\tLoss: 2701.3862\n",
      "Training Epoch: 3 [28520/49669]\tLoss: 2813.0107\n",
      "Training Epoch: 3 [28540/49669]\tLoss: 2650.1196\n",
      "Training Epoch: 3 [28560/49669]\tLoss: 3034.8796\n",
      "Training Epoch: 3 [28580/49669]\tLoss: 2758.4246\n",
      "Training Epoch: 3 [28600/49669]\tLoss: 2800.1865\n",
      "Training Epoch: 3 [28620/49669]\tLoss: 2867.9790\n",
      "Training Epoch: 3 [28640/49669]\tLoss: 2973.4336\n",
      "Training Epoch: 3 [28660/49669]\tLoss: 2939.5769\n",
      "Training Epoch: 3 [28680/49669]\tLoss: 3109.9927\n",
      "Training Epoch: 3 [28700/49669]\tLoss: 3059.0293\n",
      "Training Epoch: 3 [28720/49669]\tLoss: 2970.1416\n",
      "Training Epoch: 3 [28740/49669]\tLoss: 2636.3606\n",
      "Training Epoch: 3 [28760/49669]\tLoss: 2944.3284\n",
      "Training Epoch: 3 [28780/49669]\tLoss: 2810.3657\n",
      "Training Epoch: 3 [28800/49669]\tLoss: 2853.1301\n",
      "Training Epoch: 3 [28820/49669]\tLoss: 2707.4951\n",
      "Training Epoch: 3 [28840/49669]\tLoss: 2580.9824\n",
      "Training Epoch: 3 [28860/49669]\tLoss: 2888.3154\n",
      "Training Epoch: 3 [28880/49669]\tLoss: 3184.6902\n",
      "Training Epoch: 3 [28900/49669]\tLoss: 2870.1008\n",
      "Training Epoch: 3 [28920/49669]\tLoss: 3082.4983\n",
      "Training Epoch: 3 [28940/49669]\tLoss: 2956.5090\n",
      "Training Epoch: 3 [28960/49669]\tLoss: 3098.2549\n",
      "Training Epoch: 3 [28980/49669]\tLoss: 2861.9924\n",
      "Training Epoch: 3 [29000/49669]\tLoss: 2828.4277\n",
      "Training Epoch: 3 [29020/49669]\tLoss: 2943.6809\n",
      "Training Epoch: 3 [29040/49669]\tLoss: 2771.3787\n",
      "Training Epoch: 3 [29060/49669]\tLoss: 2788.1926\n",
      "Training Epoch: 3 [29080/49669]\tLoss: 2644.3613\n",
      "Training Epoch: 3 [29100/49669]\tLoss: 2311.0232\n",
      "Training Epoch: 3 [29120/49669]\tLoss: 3120.9021\n",
      "Training Epoch: 3 [29140/49669]\tLoss: 2633.0066\n",
      "Training Epoch: 3 [29160/49669]\tLoss: 2919.7043\n",
      "Training Epoch: 3 [29180/49669]\tLoss: 2799.9714\n",
      "Training Epoch: 3 [29200/49669]\tLoss: 2555.4915\n",
      "Training Epoch: 3 [29220/49669]\tLoss: 3058.2480\n",
      "Training Epoch: 3 [29240/49669]\tLoss: 2798.9148\n",
      "Training Epoch: 3 [29260/49669]\tLoss: 2853.5940\n",
      "Training Epoch: 3 [29280/49669]\tLoss: 2755.6453\n",
      "Training Epoch: 3 [29300/49669]\tLoss: 2842.4609\n",
      "Training Epoch: 3 [29320/49669]\tLoss: 3135.7644\n",
      "Training Epoch: 3 [29340/49669]\tLoss: 2802.2087\n",
      "Training Epoch: 3 [29360/49669]\tLoss: 2825.7998\n",
      "Training Epoch: 3 [29380/49669]\tLoss: 3055.7156\n",
      "Training Epoch: 3 [29400/49669]\tLoss: 2854.6367\n",
      "Training Epoch: 3 [29420/49669]\tLoss: 2483.1787\n",
      "Training Epoch: 3 [29440/49669]\tLoss: 2659.5042\n",
      "Training Epoch: 3 [29460/49669]\tLoss: 2758.5110\n",
      "Training Epoch: 3 [29480/49669]\tLoss: 2909.5513\n",
      "Training Epoch: 3 [29500/49669]\tLoss: 3178.3545\n",
      "Training Epoch: 3 [29520/49669]\tLoss: 2584.9248\n",
      "Training Epoch: 3 [29540/49669]\tLoss: 2854.2256\n",
      "Training Epoch: 3 [29560/49669]\tLoss: 3157.7656\n",
      "Training Epoch: 3 [29580/49669]\tLoss: 3305.3972\n",
      "Training Epoch: 3 [29600/49669]\tLoss: 2791.6638\n",
      "Training Epoch: 3 [29620/49669]\tLoss: 2373.3738\n",
      "Training Epoch: 3 [29640/49669]\tLoss: 2964.8445\n",
      "Training Epoch: 3 [29660/49669]\tLoss: 2687.6748\n",
      "Training Epoch: 3 [29680/49669]\tLoss: 3190.8660\n",
      "Training Epoch: 3 [29700/49669]\tLoss: 2861.1875\n",
      "Training Epoch: 3 [29720/49669]\tLoss: 3190.2512\n",
      "Training Epoch: 3 [29740/49669]\tLoss: 2294.3904\n",
      "Training Epoch: 3 [29760/49669]\tLoss: 2868.2554\n",
      "Training Epoch: 3 [29780/49669]\tLoss: 2408.5686\n",
      "Training Epoch: 3 [29800/49669]\tLoss: 2665.4358\n",
      "Training Epoch: 3 [29820/49669]\tLoss: 2670.1401\n",
      "Training Epoch: 3 [29840/49669]\tLoss: 2708.3677\n",
      "Training Epoch: 3 [29860/49669]\tLoss: 2665.5093\n",
      "Training Epoch: 3 [29880/49669]\tLoss: 2811.9880\n",
      "Training Epoch: 3 [29900/49669]\tLoss: 2744.1636\n",
      "Training Epoch: 3 [29920/49669]\tLoss: 2950.0002\n",
      "Training Epoch: 3 [29940/49669]\tLoss: 2865.8787\n",
      "Training Epoch: 3 [29960/49669]\tLoss: 2765.4246\n",
      "Training Epoch: 3 [29980/49669]\tLoss: 2872.5415\n",
      "Training Epoch: 3 [30000/49669]\tLoss: 2780.6294\n",
      "Training Epoch: 3 [30020/49669]\tLoss: 2893.9680\n",
      "Training Epoch: 3 [30040/49669]\tLoss: 2817.2583\n",
      "Training Epoch: 3 [30060/49669]\tLoss: 2441.6606\n",
      "Training Epoch: 3 [30080/49669]\tLoss: 3056.6304\n",
      "Training Epoch: 3 [30100/49669]\tLoss: 2593.9709\n",
      "Training Epoch: 3 [30120/49669]\tLoss: 2838.9810\n",
      "Training Epoch: 3 [30140/49669]\tLoss: 2982.6287\n",
      "Training Epoch: 3 [30160/49669]\tLoss: 2934.1580\n",
      "Training Epoch: 3 [30180/49669]\tLoss: 2676.4990\n",
      "Training Epoch: 3 [30200/49669]\tLoss: 2915.7102\n",
      "Training Epoch: 3 [30220/49669]\tLoss: 2694.7949\n",
      "Training Epoch: 3 [30240/49669]\tLoss: 3143.2671\n",
      "Training Epoch: 3 [30260/49669]\tLoss: 3091.7380\n",
      "Training Epoch: 3 [30280/49669]\tLoss: 2784.7075\n",
      "Training Epoch: 3 [30300/49669]\tLoss: 2782.9099\n",
      "Training Epoch: 3 [30320/49669]\tLoss: 3078.5330\n",
      "Training Epoch: 3 [30340/49669]\tLoss: 2894.0203\n",
      "Training Epoch: 3 [30360/49669]\tLoss: 2381.5117\n",
      "Training Epoch: 3 [30380/49669]\tLoss: 2949.9424\n",
      "Training Epoch: 3 [30400/49669]\tLoss: 3112.1206\n",
      "Training Epoch: 3 [30420/49669]\tLoss: 2880.3137\n",
      "Training Epoch: 3 [30440/49669]\tLoss: 2864.1638\n",
      "Training Epoch: 3 [30460/49669]\tLoss: 2820.9749\n",
      "Training Epoch: 3 [30480/49669]\tLoss: 2396.9253\n",
      "Training Epoch: 3 [30500/49669]\tLoss: 2741.6365\n",
      "Training Epoch: 3 [30520/49669]\tLoss: 2853.2046\n",
      "Training Epoch: 3 [30540/49669]\tLoss: 2720.0061\n",
      "Training Epoch: 3 [30560/49669]\tLoss: 3345.8184\n",
      "Training Epoch: 3 [30580/49669]\tLoss: 2655.1604\n",
      "Training Epoch: 3 [30600/49669]\tLoss: 2762.8345\n",
      "Training Epoch: 3 [30620/49669]\tLoss: 2651.0869\n",
      "Training Epoch: 3 [30640/49669]\tLoss: 2808.7153\n",
      "Training Epoch: 3 [30660/49669]\tLoss: 2682.5708\n",
      "Training Epoch: 3 [30680/49669]\tLoss: 2772.8730\n",
      "Training Epoch: 3 [30700/49669]\tLoss: 2930.8528\n",
      "Training Epoch: 3 [30720/49669]\tLoss: 2840.5535\n",
      "Training Epoch: 3 [30740/49669]\tLoss: 2603.5554\n",
      "Training Epoch: 3 [30760/49669]\tLoss: 3092.2419\n",
      "Training Epoch: 3 [30780/49669]\tLoss: 2809.2217\n",
      "Training Epoch: 3 [30800/49669]\tLoss: 2814.1677\n",
      "Training Epoch: 3 [30820/49669]\tLoss: 3030.1833\n",
      "Training Epoch: 3 [30840/49669]\tLoss: 2815.3972\n",
      "Training Epoch: 3 [30860/49669]\tLoss: 2625.2947\n",
      "Training Epoch: 3 [30880/49669]\tLoss: 2571.0630\n",
      "Training Epoch: 3 [30900/49669]\tLoss: 2903.7747\n",
      "Training Epoch: 3 [30920/49669]\tLoss: 2649.9490\n",
      "Training Epoch: 3 [30940/49669]\tLoss: 2905.4111\n",
      "Training Epoch: 3 [30960/49669]\tLoss: 2577.3503\n",
      "Training Epoch: 3 [30980/49669]\tLoss: 2737.1294\n",
      "Training Epoch: 3 [31000/49669]\tLoss: 2996.8562\n",
      "Training Epoch: 3 [31020/49669]\tLoss: 2953.4395\n",
      "Training Epoch: 3 [31040/49669]\tLoss: 2670.1238\n",
      "Training Epoch: 3 [31060/49669]\tLoss: 2667.4438\n",
      "Training Epoch: 3 [31080/49669]\tLoss: 2991.3159\n",
      "Training Epoch: 3 [31100/49669]\tLoss: 2950.7893\n",
      "Training Epoch: 3 [31120/49669]\tLoss: 3151.3752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [31140/49669]\tLoss: 2742.4648\n",
      "Training Epoch: 3 [31160/49669]\tLoss: 2812.0354\n",
      "Training Epoch: 3 [31180/49669]\tLoss: 2794.8384\n",
      "Training Epoch: 3 [31200/49669]\tLoss: 2875.0503\n",
      "Training Epoch: 3 [31220/49669]\tLoss: 3088.1987\n",
      "Training Epoch: 3 [31240/49669]\tLoss: 2635.1379\n",
      "Training Epoch: 3 [31260/49669]\tLoss: 2701.1343\n",
      "Training Epoch: 3 [31280/49669]\tLoss: 2648.9507\n",
      "Training Epoch: 3 [31300/49669]\tLoss: 2585.7153\n",
      "Training Epoch: 3 [31320/49669]\tLoss: 2658.2117\n",
      "Training Epoch: 3 [31340/49669]\tLoss: 2847.0266\n",
      "Training Epoch: 3 [31360/49669]\tLoss: 2700.9124\n",
      "Training Epoch: 3 [31380/49669]\tLoss: 2921.9294\n",
      "Training Epoch: 3 [31400/49669]\tLoss: 2807.1008\n",
      "Training Epoch: 3 [31420/49669]\tLoss: 2798.4673\n",
      "Training Epoch: 3 [31440/49669]\tLoss: 2777.5774\n",
      "Training Epoch: 3 [31460/49669]\tLoss: 2829.5305\n",
      "Training Epoch: 3 [31480/49669]\tLoss: 2891.5708\n",
      "Training Epoch: 3 [31500/49669]\tLoss: 2507.0688\n",
      "Training Epoch: 3 [31520/49669]\tLoss: 3062.4683\n",
      "Training Epoch: 3 [31540/49669]\tLoss: 2315.5215\n",
      "Training Epoch: 3 [31560/49669]\tLoss: 2919.7505\n",
      "Training Epoch: 3 [31580/49669]\tLoss: 2650.5706\n",
      "Training Epoch: 3 [31600/49669]\tLoss: 3111.8479\n",
      "Training Epoch: 3 [31620/49669]\tLoss: 3013.4558\n",
      "Training Epoch: 3 [31640/49669]\tLoss: 3148.4663\n",
      "Training Epoch: 3 [31660/49669]\tLoss: 2835.6448\n",
      "Training Epoch: 3 [31680/49669]\tLoss: 2890.7559\n",
      "Training Epoch: 3 [31700/49669]\tLoss: 2762.7456\n",
      "Training Epoch: 3 [31720/49669]\tLoss: 2834.2378\n",
      "Training Epoch: 3 [31740/49669]\tLoss: 2926.2263\n",
      "Training Epoch: 3 [31760/49669]\tLoss: 2638.6880\n",
      "Training Epoch: 3 [31780/49669]\tLoss: 2939.4353\n",
      "Training Epoch: 3 [31800/49669]\tLoss: 2401.0911\n",
      "Training Epoch: 3 [31820/49669]\tLoss: 2859.3218\n",
      "Training Epoch: 3 [31840/49669]\tLoss: 2496.8108\n",
      "Training Epoch: 3 [31860/49669]\tLoss: 3153.5725\n",
      "Training Epoch: 3 [31880/49669]\tLoss: 3047.1401\n",
      "Training Epoch: 3 [31900/49669]\tLoss: 2726.2173\n",
      "Training Epoch: 3 [31920/49669]\tLoss: 2854.6843\n",
      "Training Epoch: 3 [31940/49669]\tLoss: 2722.9229\n",
      "Training Epoch: 3 [31960/49669]\tLoss: 2844.0813\n",
      "Training Epoch: 3 [31980/49669]\tLoss: 2905.0630\n",
      "Training Epoch: 3 [32000/49669]\tLoss: 2803.2766\n",
      "Training Epoch: 3 [32020/49669]\tLoss: 2894.2715\n",
      "Training Epoch: 3 [32040/49669]\tLoss: 2475.4658\n",
      "Training Epoch: 3 [32060/49669]\tLoss: 2741.3193\n",
      "Training Epoch: 3 [32080/49669]\tLoss: 2818.6943\n",
      "Training Epoch: 3 [32100/49669]\tLoss: 2447.3625\n",
      "Training Epoch: 3 [32120/49669]\tLoss: 2924.5789\n",
      "Training Epoch: 3 [32140/49669]\tLoss: 2819.9519\n",
      "Training Epoch: 3 [32160/49669]\tLoss: 2398.2122\n",
      "Training Epoch: 3 [32180/49669]\tLoss: 2792.2083\n",
      "Training Epoch: 3 [32200/49669]\tLoss: 2782.7158\n",
      "Training Epoch: 3 [32220/49669]\tLoss: 2803.8645\n",
      "Training Epoch: 3 [32240/49669]\tLoss: 2807.6331\n",
      "Training Epoch: 3 [32260/49669]\tLoss: 2984.4651\n",
      "Training Epoch: 3 [32280/49669]\tLoss: 2813.2769\n",
      "Training Epoch: 3 [32300/49669]\tLoss: 2492.7180\n",
      "Training Epoch: 3 [32320/49669]\tLoss: 2748.2156\n",
      "Training Epoch: 3 [32340/49669]\tLoss: 2616.8254\n",
      "Training Epoch: 3 [32360/49669]\tLoss: 2461.7729\n",
      "Training Epoch: 3 [32380/49669]\tLoss: 2803.0593\n",
      "Training Epoch: 3 [32400/49669]\tLoss: 2758.5276\n",
      "Training Epoch: 3 [32420/49669]\tLoss: 2815.3518\n",
      "Training Epoch: 3 [32440/49669]\tLoss: 2974.0732\n",
      "Training Epoch: 3 [32460/49669]\tLoss: 2939.7344\n",
      "Training Epoch: 3 [32480/49669]\tLoss: 3070.9094\n",
      "Training Epoch: 3 [32500/49669]\tLoss: 2794.8376\n",
      "Training Epoch: 3 [32520/49669]\tLoss: 2766.1104\n",
      "Training Epoch: 3 [32540/49669]\tLoss: 2803.3923\n",
      "Training Epoch: 3 [32560/49669]\tLoss: 2897.9822\n",
      "Training Epoch: 3 [32580/49669]\tLoss: 2765.5173\n",
      "Training Epoch: 3 [32600/49669]\tLoss: 2628.3735\n",
      "Training Epoch: 3 [32620/49669]\tLoss: 2591.0669\n",
      "Training Epoch: 3 [32640/49669]\tLoss: 2600.5068\n",
      "Training Epoch: 3 [32660/49669]\tLoss: 2711.0400\n",
      "Training Epoch: 3 [32680/49669]\tLoss: 2875.0017\n",
      "Training Epoch: 3 [32700/49669]\tLoss: 2581.8035\n",
      "Training Epoch: 3 [32720/49669]\tLoss: 2751.6646\n",
      "Training Epoch: 3 [32740/49669]\tLoss: 2895.6204\n",
      "Training Epoch: 3 [32760/49669]\tLoss: 3092.0034\n",
      "Training Epoch: 3 [32780/49669]\tLoss: 2952.6003\n",
      "Training Epoch: 3 [32800/49669]\tLoss: 2773.4932\n",
      "Training Epoch: 3 [32820/49669]\tLoss: 2678.1504\n",
      "Training Epoch: 3 [32840/49669]\tLoss: 2763.6140\n",
      "Training Epoch: 3 [32860/49669]\tLoss: 2594.0579\n",
      "Training Epoch: 3 [32880/49669]\tLoss: 2958.8657\n",
      "Training Epoch: 3 [32900/49669]\tLoss: 3074.2554\n",
      "Training Epoch: 3 [32920/49669]\tLoss: 3034.4829\n",
      "Training Epoch: 3 [32940/49669]\tLoss: 2691.8789\n",
      "Training Epoch: 3 [32960/49669]\tLoss: 3012.3777\n",
      "Training Epoch: 3 [32980/49669]\tLoss: 2913.9302\n",
      "Training Epoch: 3 [33000/49669]\tLoss: 2584.0283\n",
      "Training Epoch: 3 [33020/49669]\tLoss: 2629.1978\n",
      "Training Epoch: 3 [33040/49669]\tLoss: 2660.2537\n",
      "Training Epoch: 3 [33060/49669]\tLoss: 2999.4197\n",
      "Training Epoch: 3 [33080/49669]\tLoss: 2553.3696\n",
      "Training Epoch: 3 [33100/49669]\tLoss: 2623.0791\n",
      "Training Epoch: 3 [33120/49669]\tLoss: 2969.9463\n",
      "Training Epoch: 3 [33140/49669]\tLoss: 2493.1096\n",
      "Training Epoch: 3 [33160/49669]\tLoss: 2951.6853\n",
      "Training Epoch: 3 [33180/49669]\tLoss: 2712.5464\n",
      "Training Epoch: 3 [33200/49669]\tLoss: 2305.8689\n",
      "Training Epoch: 3 [33220/49669]\tLoss: 2790.1924\n",
      "Training Epoch: 3 [33240/49669]\tLoss: 2952.5071\n",
      "Training Epoch: 3 [33260/49669]\tLoss: 2750.8818\n",
      "Training Epoch: 3 [33280/49669]\tLoss: 2569.0159\n",
      "Training Epoch: 3 [33300/49669]\tLoss: 2621.4331\n",
      "Training Epoch: 3 [33320/49669]\tLoss: 2816.2234\n",
      "Training Epoch: 3 [33340/49669]\tLoss: 2525.6750\n",
      "Training Epoch: 3 [33360/49669]\tLoss: 2804.5105\n",
      "Training Epoch: 3 [33380/49669]\tLoss: 2827.6357\n",
      "Training Epoch: 3 [33400/49669]\tLoss: 2733.5684\n",
      "Training Epoch: 3 [33420/49669]\tLoss: 2503.1465\n",
      "Training Epoch: 3 [33440/49669]\tLoss: 2861.2678\n",
      "Training Epoch: 3 [33460/49669]\tLoss: 2724.3792\n",
      "Training Epoch: 3 [33480/49669]\tLoss: 2763.3872\n",
      "Training Epoch: 3 [33500/49669]\tLoss: 2403.4407\n",
      "Training Epoch: 3 [33520/49669]\tLoss: 3173.9944\n",
      "Training Epoch: 3 [33540/49669]\tLoss: 2698.4167\n",
      "Training Epoch: 3 [33560/49669]\tLoss: 2907.3325\n",
      "Training Epoch: 3 [33580/49669]\tLoss: 2616.4302\n",
      "Training Epoch: 3 [33600/49669]\tLoss: 2627.9949\n",
      "Training Epoch: 3 [33620/49669]\tLoss: 2872.8589\n",
      "Training Epoch: 3 [33640/49669]\tLoss: 2741.2407\n",
      "Training Epoch: 3 [33660/49669]\tLoss: 2615.8789\n",
      "Training Epoch: 3 [33680/49669]\tLoss: 2598.3191\n",
      "Training Epoch: 3 [33700/49669]\tLoss: 2639.3696\n",
      "Training Epoch: 3 [33720/49669]\tLoss: 2781.3857\n",
      "Training Epoch: 3 [33740/49669]\tLoss: 2758.5918\n",
      "Training Epoch: 3 [33760/49669]\tLoss: 2951.9258\n",
      "Training Epoch: 3 [33780/49669]\tLoss: 2622.6877\n",
      "Training Epoch: 3 [33800/49669]\tLoss: 2668.9233\n",
      "Training Epoch: 3 [33820/49669]\tLoss: 3033.7649\n",
      "Training Epoch: 3 [33840/49669]\tLoss: 2527.4482\n",
      "Training Epoch: 3 [33860/49669]\tLoss: 2624.8257\n",
      "Training Epoch: 3 [33880/49669]\tLoss: 3172.4390\n",
      "Training Epoch: 3 [33900/49669]\tLoss: 2334.3132\n",
      "Training Epoch: 3 [33920/49669]\tLoss: 2972.3115\n",
      "Training Epoch: 3 [33940/49669]\tLoss: 2878.3220\n",
      "Training Epoch: 3 [33960/49669]\tLoss: 2974.8040\n",
      "Training Epoch: 3 [33980/49669]\tLoss: 2568.3716\n",
      "Training Epoch: 3 [34000/49669]\tLoss: 2524.5012\n",
      "Training Epoch: 3 [34020/49669]\tLoss: 2959.7283\n",
      "Training Epoch: 3 [34040/49669]\tLoss: 2543.1470\n",
      "Training Epoch: 3 [34060/49669]\tLoss: 2745.4431\n",
      "Training Epoch: 3 [34080/49669]\tLoss: 2467.1621\n",
      "Training Epoch: 3 [34100/49669]\tLoss: 2738.6240\n",
      "Training Epoch: 3 [34120/49669]\tLoss: 2511.8513\n",
      "Training Epoch: 3 [34140/49669]\tLoss: 2587.7881\n",
      "Training Epoch: 3 [34160/49669]\tLoss: 2690.4199\n",
      "Training Epoch: 3 [34180/49669]\tLoss: 2586.4475\n",
      "Training Epoch: 3 [34200/49669]\tLoss: 2739.1377\n",
      "Training Epoch: 3 [34220/49669]\tLoss: 2772.5420\n",
      "Training Epoch: 3 [34240/49669]\tLoss: 2687.3789\n",
      "Training Epoch: 3 [34260/49669]\tLoss: 3073.0972\n",
      "Training Epoch: 3 [34280/49669]\tLoss: 2734.1892\n",
      "Training Epoch: 3 [34300/49669]\tLoss: 2779.7083\n",
      "Training Epoch: 3 [34320/49669]\tLoss: 2824.8501\n",
      "Training Epoch: 3 [34340/49669]\tLoss: 2588.5945\n",
      "Training Epoch: 3 [34360/49669]\tLoss: 2726.9119\n",
      "Training Epoch: 3 [34380/49669]\tLoss: 2589.8145\n",
      "Training Epoch: 3 [34400/49669]\tLoss: 2726.5190\n",
      "Training Epoch: 3 [34420/49669]\tLoss: 2646.6748\n",
      "Training Epoch: 3 [34440/49669]\tLoss: 2326.5591\n",
      "Training Epoch: 3 [34460/49669]\tLoss: 2763.2700\n",
      "Training Epoch: 3 [34480/49669]\tLoss: 2835.7598\n",
      "Training Epoch: 3 [34500/49669]\tLoss: 2748.9272\n",
      "Training Epoch: 3 [34520/49669]\tLoss: 2948.3655\n",
      "Training Epoch: 3 [34540/49669]\tLoss: 2539.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [34560/49669]\tLoss: 2910.5898\n",
      "Training Epoch: 3 [34580/49669]\tLoss: 2749.8909\n",
      "Training Epoch: 3 [34600/49669]\tLoss: 2749.2976\n",
      "Training Epoch: 3 [34620/49669]\tLoss: 2996.3652\n",
      "Training Epoch: 3 [34640/49669]\tLoss: 2743.7327\n",
      "Training Epoch: 3 [34660/49669]\tLoss: 2815.6006\n",
      "Training Epoch: 3 [34680/49669]\tLoss: 2603.9177\n",
      "Training Epoch: 3 [34700/49669]\tLoss: 2808.5107\n",
      "Training Epoch: 3 [34720/49669]\tLoss: 2531.9766\n",
      "Training Epoch: 3 [34740/49669]\tLoss: 3058.8748\n",
      "Training Epoch: 3 [34760/49669]\tLoss: 3095.7390\n",
      "Training Epoch: 3 [34780/49669]\tLoss: 2985.0828\n",
      "Training Epoch: 3 [34800/49669]\tLoss: 2845.6213\n",
      "Training Epoch: 3 [34820/49669]\tLoss: 2788.0422\n",
      "Training Epoch: 3 [34840/49669]\tLoss: 2999.3259\n",
      "Training Epoch: 3 [34860/49669]\tLoss: 2868.4016\n",
      "Training Epoch: 3 [34880/49669]\tLoss: 2693.6755\n",
      "Training Epoch: 3 [34900/49669]\tLoss: 2738.4851\n",
      "Training Epoch: 3 [34920/49669]\tLoss: 2723.4724\n",
      "Training Epoch: 3 [34940/49669]\tLoss: 2414.9324\n",
      "Training Epoch: 3 [34960/49669]\tLoss: 3045.2783\n",
      "Training Epoch: 3 [34980/49669]\tLoss: 2529.8872\n",
      "Training Epoch: 3 [35000/49669]\tLoss: 2935.5725\n",
      "Training Epoch: 3 [35020/49669]\tLoss: 2583.5876\n",
      "Training Epoch: 3 [35040/49669]\tLoss: 2675.1243\n",
      "Training Epoch: 3 [35060/49669]\tLoss: 2719.2144\n",
      "Training Epoch: 3 [35080/49669]\tLoss: 2783.5552\n",
      "Training Epoch: 3 [35100/49669]\tLoss: 2439.2390\n",
      "Training Epoch: 3 [35120/49669]\tLoss: 2789.6111\n",
      "Training Epoch: 3 [35140/49669]\tLoss: 2899.9067\n",
      "Training Epoch: 3 [35160/49669]\tLoss: 2832.9680\n",
      "Training Epoch: 3 [35180/49669]\tLoss: 2856.3655\n",
      "Training Epoch: 3 [35200/49669]\tLoss: 2353.7307\n",
      "Training Epoch: 3 [35220/49669]\tLoss: 2864.0066\n",
      "Training Epoch: 3 [35240/49669]\tLoss: 2951.1396\n",
      "Training Epoch: 3 [35260/49669]\tLoss: 2742.1516\n",
      "Training Epoch: 3 [35280/49669]\tLoss: 2783.0435\n",
      "Training Epoch: 3 [35300/49669]\tLoss: 2831.0156\n",
      "Training Epoch: 3 [35320/49669]\tLoss: 2740.4019\n",
      "Training Epoch: 3 [35340/49669]\tLoss: 2676.3208\n",
      "Training Epoch: 3 [35360/49669]\tLoss: 2827.7605\n",
      "Training Epoch: 3 [35380/49669]\tLoss: 2430.7788\n",
      "Training Epoch: 3 [35400/49669]\tLoss: 2761.9084\n",
      "Training Epoch: 3 [35420/49669]\tLoss: 2810.5835\n",
      "Training Epoch: 3 [35440/49669]\tLoss: 3086.8313\n",
      "Training Epoch: 3 [35460/49669]\tLoss: 2440.3457\n",
      "Training Epoch: 3 [35480/49669]\tLoss: 2696.7385\n",
      "Training Epoch: 3 [35500/49669]\tLoss: 3147.5125\n",
      "Training Epoch: 3 [35520/49669]\tLoss: 2790.3901\n",
      "Training Epoch: 3 [35540/49669]\tLoss: 2561.0781\n",
      "Training Epoch: 3 [35560/49669]\tLoss: 3078.6472\n",
      "Training Epoch: 3 [35580/49669]\tLoss: 2964.2207\n",
      "Training Epoch: 3 [35600/49669]\tLoss: 2695.7102\n",
      "Training Epoch: 3 [35620/49669]\tLoss: 2987.9924\n",
      "Training Epoch: 3 [35640/49669]\tLoss: 2857.2363\n",
      "Training Epoch: 3 [35660/49669]\tLoss: 2664.9761\n",
      "Training Epoch: 3 [35680/49669]\tLoss: 2624.4360\n",
      "Training Epoch: 3 [35700/49669]\tLoss: 2474.0591\n",
      "Training Epoch: 3 [35720/49669]\tLoss: 3021.5740\n",
      "Training Epoch: 3 [35740/49669]\tLoss: 2661.2542\n",
      "Training Epoch: 3 [35760/49669]\tLoss: 2717.7720\n",
      "Training Epoch: 3 [35780/49669]\tLoss: 2676.7642\n",
      "Training Epoch: 3 [35800/49669]\tLoss: 2389.9146\n",
      "Training Epoch: 3 [35820/49669]\tLoss: 3031.2212\n",
      "Training Epoch: 3 [35840/49669]\tLoss: 2809.4626\n",
      "Training Epoch: 3 [35860/49669]\tLoss: 2880.7227\n",
      "Training Epoch: 3 [35880/49669]\tLoss: 2688.8857\n",
      "Training Epoch: 3 [35900/49669]\tLoss: 2791.5105\n",
      "Training Epoch: 3 [35920/49669]\tLoss: 3137.8989\n",
      "Training Epoch: 3 [35940/49669]\tLoss: 2671.4875\n",
      "Training Epoch: 3 [35960/49669]\tLoss: 2749.3367\n",
      "Training Epoch: 3 [35980/49669]\tLoss: 2931.8701\n",
      "Training Epoch: 3 [36000/49669]\tLoss: 2563.7173\n",
      "Training Epoch: 3 [36020/49669]\tLoss: 2849.8953\n",
      "Training Epoch: 3 [36040/49669]\tLoss: 2509.0510\n",
      "Training Epoch: 3 [36060/49669]\tLoss: 3171.4971\n",
      "Training Epoch: 3 [36080/49669]\tLoss: 3114.1809\n",
      "Training Epoch: 3 [36100/49669]\tLoss: 2535.3281\n",
      "Training Epoch: 3 [36120/49669]\tLoss: 2605.2461\n",
      "Training Epoch: 3 [36140/49669]\tLoss: 2977.5618\n",
      "Training Epoch: 3 [36160/49669]\tLoss: 2832.2129\n",
      "Training Epoch: 3 [36180/49669]\tLoss: 2711.3469\n",
      "Training Epoch: 3 [36200/49669]\tLoss: 2743.4998\n",
      "Training Epoch: 3 [36220/49669]\tLoss: 2964.9346\n",
      "Training Epoch: 3 [36240/49669]\tLoss: 2813.9241\n",
      "Training Epoch: 3 [36260/49669]\tLoss: 2577.0774\n",
      "Training Epoch: 3 [36280/49669]\tLoss: 2807.1133\n",
      "Training Epoch: 3 [36300/49669]\tLoss: 2450.5737\n",
      "Training Epoch: 3 [36320/49669]\tLoss: 2861.2405\n",
      "Training Epoch: 3 [36340/49669]\tLoss: 2524.6016\n",
      "Training Epoch: 3 [36360/49669]\tLoss: 2760.3867\n",
      "Training Epoch: 3 [36380/49669]\tLoss: 2500.3647\n",
      "Training Epoch: 3 [36400/49669]\tLoss: 2371.7388\n",
      "Training Epoch: 3 [36420/49669]\tLoss: 2523.4211\n",
      "Training Epoch: 3 [36440/49669]\tLoss: 2809.2227\n",
      "Training Epoch: 3 [36460/49669]\tLoss: 2783.6687\n",
      "Training Epoch: 3 [36480/49669]\tLoss: 2732.1926\n",
      "Training Epoch: 3 [36500/49669]\tLoss: 2499.0581\n",
      "Training Epoch: 3 [36520/49669]\tLoss: 2606.6140\n",
      "Training Epoch: 3 [36540/49669]\tLoss: 2513.4565\n",
      "Training Epoch: 3 [36560/49669]\tLoss: 2617.1470\n",
      "Training Epoch: 3 [36580/49669]\tLoss: 2767.0259\n",
      "Training Epoch: 3 [36600/49669]\tLoss: 2291.7869\n",
      "Training Epoch: 3 [36620/49669]\tLoss: 2646.9646\n",
      "Training Epoch: 3 [36640/49669]\tLoss: 2242.5300\n",
      "Training Epoch: 3 [36660/49669]\tLoss: 2479.3035\n",
      "Training Epoch: 3 [36680/49669]\tLoss: 2306.6814\n",
      "Training Epoch: 3 [36700/49669]\tLoss: 2887.9958\n",
      "Training Epoch: 3 [36720/49669]\tLoss: 2839.4839\n",
      "Training Epoch: 3 [36740/49669]\tLoss: 2815.5593\n",
      "Training Epoch: 3 [36760/49669]\tLoss: 2601.2820\n",
      "Training Epoch: 3 [36780/49669]\tLoss: 2308.9807\n",
      "Training Epoch: 3 [36800/49669]\tLoss: 2565.9724\n",
      "Training Epoch: 3 [36820/49669]\tLoss: 2488.8650\n",
      "Training Epoch: 3 [36840/49669]\tLoss: 2659.2329\n",
      "Training Epoch: 3 [36860/49669]\tLoss: 2624.0027\n",
      "Training Epoch: 3 [36880/49669]\tLoss: 2588.6660\n",
      "Training Epoch: 3 [36900/49669]\tLoss: 3003.0649\n",
      "Training Epoch: 3 [36920/49669]\tLoss: 2655.9807\n",
      "Training Epoch: 3 [36940/49669]\tLoss: 2821.8052\n",
      "Training Epoch: 3 [36960/49669]\tLoss: 2719.8977\n",
      "Training Epoch: 3 [36980/49669]\tLoss: 2810.5615\n",
      "Training Epoch: 3 [37000/49669]\tLoss: 3004.9773\n",
      "Training Epoch: 3 [37020/49669]\tLoss: 2752.7769\n",
      "Training Epoch: 3 [37040/49669]\tLoss: 2615.8086\n",
      "Training Epoch: 3 [37060/49669]\tLoss: 3049.6357\n",
      "Training Epoch: 3 [37080/49669]\tLoss: 2730.3774\n",
      "Training Epoch: 3 [37100/49669]\tLoss: 2806.0625\n",
      "Training Epoch: 3 [37120/49669]\tLoss: 2828.1357\n",
      "Training Epoch: 3 [37140/49669]\tLoss: 2700.7927\n",
      "Training Epoch: 3 [37160/49669]\tLoss: 2546.6255\n",
      "Training Epoch: 3 [37180/49669]\tLoss: 2893.3359\n",
      "Training Epoch: 3 [37200/49669]\tLoss: 2699.5986\n",
      "Training Epoch: 3 [37220/49669]\tLoss: 2614.1577\n",
      "Training Epoch: 3 [37240/49669]\tLoss: 2674.2522\n",
      "Training Epoch: 3 [37260/49669]\tLoss: 2892.1851\n",
      "Training Epoch: 3 [37280/49669]\tLoss: 2242.9695\n",
      "Training Epoch: 3 [37300/49669]\tLoss: 2577.6111\n",
      "Training Epoch: 3 [37320/49669]\tLoss: 2521.1042\n",
      "Training Epoch: 3 [37340/49669]\tLoss: 2869.9194\n",
      "Training Epoch: 3 [37360/49669]\tLoss: 2535.7119\n",
      "Training Epoch: 3 [37380/49669]\tLoss: 2837.7288\n",
      "Training Epoch: 3 [37400/49669]\tLoss: 2834.1455\n",
      "Training Epoch: 3 [37420/49669]\tLoss: 2937.7708\n",
      "Training Epoch: 3 [37440/49669]\tLoss: 2681.5630\n",
      "Training Epoch: 3 [37460/49669]\tLoss: 2591.0645\n",
      "Training Epoch: 3 [37480/49669]\tLoss: 2818.1414\n",
      "Training Epoch: 3 [37500/49669]\tLoss: 2793.8735\n",
      "Training Epoch: 3 [37520/49669]\tLoss: 2299.3066\n",
      "Training Epoch: 3 [37540/49669]\tLoss: 2986.6638\n",
      "Training Epoch: 3 [37560/49669]\tLoss: 2903.8433\n",
      "Training Epoch: 3 [37580/49669]\tLoss: 3289.4512\n",
      "Training Epoch: 3 [37600/49669]\tLoss: 2496.6653\n",
      "Training Epoch: 3 [37620/49669]\tLoss: 2976.8945\n",
      "Training Epoch: 3 [37640/49669]\tLoss: 2797.5112\n",
      "Training Epoch: 3 [37660/49669]\tLoss: 2635.0459\n",
      "Training Epoch: 3 [37680/49669]\tLoss: 2832.3682\n",
      "Training Epoch: 3 [37700/49669]\tLoss: 2296.3403\n",
      "Training Epoch: 3 [37720/49669]\tLoss: 2656.9922\n",
      "Training Epoch: 3 [37740/49669]\tLoss: 2572.7446\n",
      "Training Epoch: 3 [37760/49669]\tLoss: 2698.9644\n",
      "Training Epoch: 3 [37780/49669]\tLoss: 2856.1079\n",
      "Training Epoch: 3 [37800/49669]\tLoss: 2857.7012\n",
      "Training Epoch: 3 [37820/49669]\tLoss: 2798.3787\n",
      "Training Epoch: 3 [37840/49669]\tLoss: 2165.8015\n",
      "Training Epoch: 3 [37860/49669]\tLoss: 2671.6970\n",
      "Training Epoch: 3 [37880/49669]\tLoss: 2694.8992\n",
      "Training Epoch: 3 [37900/49669]\tLoss: 2639.8992\n",
      "Training Epoch: 3 [37920/49669]\tLoss: 2667.9248\n",
      "Training Epoch: 3 [37940/49669]\tLoss: 2711.3938\n",
      "Training Epoch: 3 [37960/49669]\tLoss: 2241.2109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [37980/49669]\tLoss: 2410.7195\n",
      "Training Epoch: 3 [38000/49669]\tLoss: 2638.6172\n",
      "Training Epoch: 3 [38020/49669]\tLoss: 2698.6577\n",
      "Training Epoch: 3 [38040/49669]\tLoss: 2860.4392\n",
      "Training Epoch: 3 [38060/49669]\tLoss: 2308.4631\n",
      "Training Epoch: 3 [38080/49669]\tLoss: 2948.8806\n",
      "Training Epoch: 3 [38100/49669]\tLoss: 2851.6438\n",
      "Training Epoch: 3 [38120/49669]\tLoss: 2735.8240\n",
      "Training Epoch: 3 [38140/49669]\tLoss: 2343.6553\n",
      "Training Epoch: 3 [38160/49669]\tLoss: 2882.4541\n",
      "Training Epoch: 3 [38180/49669]\tLoss: 2592.9858\n",
      "Training Epoch: 3 [38200/49669]\tLoss: 2678.6396\n",
      "Training Epoch: 3 [38220/49669]\tLoss: 2592.5811\n",
      "Training Epoch: 3 [38240/49669]\tLoss: 2410.4326\n",
      "Training Epoch: 3 [38260/49669]\tLoss: 2583.2646\n",
      "Training Epoch: 3 [38280/49669]\tLoss: 2801.4329\n",
      "Training Epoch: 3 [38300/49669]\tLoss: 2652.4456\n",
      "Training Epoch: 3 [38320/49669]\tLoss: 2928.6484\n",
      "Training Epoch: 3 [38340/49669]\tLoss: 2781.6091\n",
      "Training Epoch: 3 [38360/49669]\tLoss: 2442.7737\n",
      "Training Epoch: 3 [38380/49669]\tLoss: 2854.8528\n",
      "Training Epoch: 3 [38400/49669]\tLoss: 2614.4753\n",
      "Training Epoch: 3 [38420/49669]\tLoss: 2694.3550\n",
      "Training Epoch: 3 [38440/49669]\tLoss: 2692.5962\n",
      "Training Epoch: 3 [38460/49669]\tLoss: 2596.6023\n",
      "Training Epoch: 3 [38480/49669]\tLoss: 2887.2488\n",
      "Training Epoch: 3 [38500/49669]\tLoss: 2534.3320\n",
      "Training Epoch: 3 [38520/49669]\tLoss: 2830.4924\n",
      "Training Epoch: 3 [38540/49669]\tLoss: 2514.7124\n",
      "Training Epoch: 3 [38560/49669]\tLoss: 2594.4570\n",
      "Training Epoch: 3 [38580/49669]\tLoss: 2639.4978\n",
      "Training Epoch: 3 [38600/49669]\tLoss: 2625.4373\n",
      "Training Epoch: 3 [38620/49669]\tLoss: 2743.7544\n",
      "Training Epoch: 3 [38640/49669]\tLoss: 2689.5005\n",
      "Training Epoch: 3 [38660/49669]\tLoss: 2741.6816\n",
      "Training Epoch: 3 [38680/49669]\tLoss: 2766.2886\n",
      "Training Epoch: 3 [38700/49669]\tLoss: 2233.9658\n",
      "Training Epoch: 3 [38720/49669]\tLoss: 2805.6338\n",
      "Training Epoch: 3 [38740/49669]\tLoss: 2659.8267\n",
      "Training Epoch: 3 [38760/49669]\tLoss: 2830.2812\n",
      "Training Epoch: 3 [38780/49669]\tLoss: 2796.6067\n",
      "Training Epoch: 3 [38800/49669]\tLoss: 2687.9312\n",
      "Training Epoch: 3 [38820/49669]\tLoss: 2489.8335\n",
      "Training Epoch: 3 [38840/49669]\tLoss: 2343.5701\n",
      "Training Epoch: 3 [38860/49669]\tLoss: 2728.2983\n",
      "Training Epoch: 3 [38880/49669]\tLoss: 2519.1411\n",
      "Training Epoch: 3 [38900/49669]\tLoss: 2400.4233\n",
      "Training Epoch: 3 [38920/49669]\tLoss: 2878.9946\n",
      "Training Epoch: 3 [38940/49669]\tLoss: 2895.6670\n",
      "Training Epoch: 3 [38960/49669]\tLoss: 2838.6785\n",
      "Training Epoch: 3 [38980/49669]\tLoss: 2414.5393\n",
      "Training Epoch: 3 [39000/49669]\tLoss: 2748.4429\n",
      "Training Epoch: 3 [39020/49669]\tLoss: 2287.1943\n",
      "Training Epoch: 3 [39040/49669]\tLoss: 2707.1501\n",
      "Training Epoch: 3 [39060/49669]\tLoss: 2448.1226\n",
      "Training Epoch: 3 [39080/49669]\tLoss: 2728.6477\n",
      "Training Epoch: 3 [39100/49669]\tLoss: 2794.4126\n",
      "Training Epoch: 3 [39120/49669]\tLoss: 2851.8489\n",
      "Training Epoch: 3 [39140/49669]\tLoss: 2429.3313\n",
      "Training Epoch: 3 [39160/49669]\tLoss: 2754.1565\n",
      "Training Epoch: 3 [39180/49669]\tLoss: 2367.7603\n",
      "Training Epoch: 3 [39200/49669]\tLoss: 2423.1235\n",
      "Training Epoch: 3 [39220/49669]\tLoss: 2767.4819\n",
      "Training Epoch: 3 [39240/49669]\tLoss: 2394.8701\n",
      "Training Epoch: 3 [39260/49669]\tLoss: 2511.0181\n",
      "Training Epoch: 3 [39280/49669]\tLoss: 2805.5063\n",
      "Training Epoch: 3 [39300/49669]\tLoss: 2665.9614\n",
      "Training Epoch: 3 [39320/49669]\tLoss: 2765.1580\n",
      "Training Epoch: 3 [39340/49669]\tLoss: 2846.8228\n",
      "Training Epoch: 3 [39360/49669]\tLoss: 2526.2507\n",
      "Training Epoch: 3 [39380/49669]\tLoss: 2611.8682\n",
      "Training Epoch: 3 [39400/49669]\tLoss: 2698.4956\n",
      "Training Epoch: 3 [39420/49669]\tLoss: 2725.1143\n",
      "Training Epoch: 3 [39440/49669]\tLoss: 2335.6250\n",
      "Training Epoch: 3 [39460/49669]\tLoss: 2943.7158\n",
      "Training Epoch: 3 [39480/49669]\tLoss: 2748.8787\n",
      "Training Epoch: 3 [39500/49669]\tLoss: 2773.1772\n",
      "Training Epoch: 3 [39520/49669]\tLoss: 2781.5039\n",
      "Training Epoch: 3 [39540/49669]\tLoss: 2641.1807\n",
      "Training Epoch: 3 [39560/49669]\tLoss: 2345.8301\n",
      "Training Epoch: 3 [39580/49669]\tLoss: 2910.9617\n",
      "Training Epoch: 3 [39600/49669]\tLoss: 2712.3389\n",
      "Training Epoch: 3 [39620/49669]\tLoss: 2752.9355\n",
      "Training Epoch: 3 [39640/49669]\tLoss: 2713.8337\n",
      "Training Epoch: 3 [39660/49669]\tLoss: 2939.5071\n",
      "Training Epoch: 3 [39680/49669]\tLoss: 2501.8711\n",
      "Training Epoch: 3 [39700/49669]\tLoss: 2398.0466\n",
      "Training Epoch: 3 [39720/49669]\tLoss: 2284.4912\n",
      "Training Epoch: 3 [39740/49669]\tLoss: 2802.8936\n",
      "Training Epoch: 3 [39760/49669]\tLoss: 2782.4399\n",
      "Training Epoch: 3 [39780/49669]\tLoss: 2804.3271\n",
      "Training Epoch: 3 [39800/49669]\tLoss: 2790.6423\n",
      "Training Epoch: 3 [39820/49669]\tLoss: 2642.2939\n",
      "Training Epoch: 3 [39840/49669]\tLoss: 2859.7009\n",
      "Training Epoch: 3 [39860/49669]\tLoss: 2522.7314\n",
      "Training Epoch: 3 [39880/49669]\tLoss: 2666.6179\n",
      "Training Epoch: 3 [39900/49669]\tLoss: 2624.8384\n",
      "Training Epoch: 3 [39920/49669]\tLoss: 2140.3638\n",
      "Training Epoch: 3 [39940/49669]\tLoss: 2804.6838\n",
      "Training Epoch: 3 [39960/49669]\tLoss: 3008.7922\n",
      "Training Epoch: 3 [39980/49669]\tLoss: 2537.7366\n",
      "Training Epoch: 3 [40000/49669]\tLoss: 2908.3882\n",
      "Training Epoch: 3 [40020/49669]\tLoss: 2995.5713\n",
      "Training Epoch: 3 [40040/49669]\tLoss: 2666.0974\n",
      "Training Epoch: 3 [40060/49669]\tLoss: 2696.0825\n",
      "Training Epoch: 3 [40080/49669]\tLoss: 2606.3604\n",
      "Training Epoch: 3 [40100/49669]\tLoss: 2822.3147\n",
      "Training Epoch: 3 [40120/49669]\tLoss: 2905.4800\n",
      "Training Epoch: 3 [40140/49669]\tLoss: 2772.0642\n",
      "Training Epoch: 3 [40160/49669]\tLoss: 2690.4658\n",
      "Training Epoch: 3 [40180/49669]\tLoss: 2836.7244\n",
      "Training Epoch: 3 [40200/49669]\tLoss: 2615.0776\n",
      "Training Epoch: 3 [40220/49669]\tLoss: 2878.5339\n",
      "Training Epoch: 3 [40240/49669]\tLoss: 2608.4463\n",
      "Training Epoch: 3 [40260/49669]\tLoss: 2590.0117\n",
      "Training Epoch: 3 [40280/49669]\tLoss: 2801.7446\n",
      "Training Epoch: 3 [40300/49669]\tLoss: 2984.4192\n",
      "Training Epoch: 3 [40320/49669]\tLoss: 2987.8210\n",
      "Training Epoch: 3 [40340/49669]\tLoss: 2445.3027\n",
      "Training Epoch: 3 [40360/49669]\tLoss: 2389.9978\n",
      "Training Epoch: 3 [40380/49669]\tLoss: 2835.3535\n",
      "Training Epoch: 3 [40400/49669]\tLoss: 3051.6494\n",
      "Training Epoch: 3 [40420/49669]\tLoss: 2526.9275\n",
      "Training Epoch: 3 [40440/49669]\tLoss: 2696.8391\n",
      "Training Epoch: 3 [40460/49669]\tLoss: 2808.5833\n",
      "Training Epoch: 3 [40480/49669]\tLoss: 2690.1753\n",
      "Training Epoch: 3 [40500/49669]\tLoss: 2406.8428\n",
      "Training Epoch: 3 [40520/49669]\tLoss: 2357.9666\n",
      "Training Epoch: 3 [40540/49669]\tLoss: 2673.4414\n",
      "Training Epoch: 3 [40560/49669]\tLoss: 2913.2803\n",
      "Training Epoch: 3 [40580/49669]\tLoss: 2525.1736\n",
      "Training Epoch: 3 [40600/49669]\tLoss: 2548.4910\n",
      "Training Epoch: 3 [40620/49669]\tLoss: 2624.5947\n",
      "Training Epoch: 3 [40640/49669]\tLoss: 2685.9717\n",
      "Training Epoch: 3 [40660/49669]\tLoss: 2615.6069\n",
      "Training Epoch: 3 [40680/49669]\tLoss: 2704.9187\n",
      "Training Epoch: 3 [40700/49669]\tLoss: 2889.2830\n",
      "Training Epoch: 3 [40720/49669]\tLoss: 2805.3638\n",
      "Training Epoch: 3 [40740/49669]\tLoss: 2641.3379\n",
      "Training Epoch: 3 [40760/49669]\tLoss: 2517.9897\n",
      "Training Epoch: 3 [40780/49669]\tLoss: 2332.0972\n",
      "Training Epoch: 3 [40800/49669]\tLoss: 2729.6143\n",
      "Training Epoch: 3 [40820/49669]\tLoss: 2512.2974\n",
      "Training Epoch: 3 [40840/49669]\tLoss: 2637.0903\n",
      "Training Epoch: 3 [40860/49669]\tLoss: 2870.6924\n",
      "Training Epoch: 3 [40880/49669]\tLoss: 2637.1194\n",
      "Training Epoch: 3 [40900/49669]\tLoss: 2698.8696\n",
      "Training Epoch: 3 [40920/49669]\tLoss: 2867.8442\n",
      "Training Epoch: 3 [40940/49669]\tLoss: 2726.5010\n",
      "Training Epoch: 3 [40960/49669]\tLoss: 2675.7659\n",
      "Training Epoch: 3 [40980/49669]\tLoss: 2664.1909\n",
      "Training Epoch: 3 [41000/49669]\tLoss: 2663.7292\n",
      "Training Epoch: 3 [41020/49669]\tLoss: 2434.6040\n",
      "Training Epoch: 3 [41040/49669]\tLoss: 2879.3933\n",
      "Training Epoch: 3 [41060/49669]\tLoss: 2416.2908\n",
      "Training Epoch: 3 [41080/49669]\tLoss: 2668.8103\n",
      "Training Epoch: 3 [41100/49669]\tLoss: 2824.4272\n",
      "Training Epoch: 3 [41120/49669]\tLoss: 2737.7983\n",
      "Training Epoch: 3 [41140/49669]\tLoss: 2819.6975\n",
      "Training Epoch: 3 [41160/49669]\tLoss: 2624.6411\n",
      "Training Epoch: 3 [41180/49669]\tLoss: 2918.9412\n",
      "Training Epoch: 3 [41200/49669]\tLoss: 2548.8037\n",
      "Training Epoch: 3 [41220/49669]\tLoss: 2390.7983\n",
      "Training Epoch: 3 [41240/49669]\tLoss: 2629.9946\n",
      "Training Epoch: 3 [41260/49669]\tLoss: 2709.7776\n",
      "Training Epoch: 3 [41280/49669]\tLoss: 2389.9585\n",
      "Training Epoch: 3 [41300/49669]\tLoss: 2633.1482\n",
      "Training Epoch: 3 [41320/49669]\tLoss: 2845.9561\n",
      "Training Epoch: 3 [41340/49669]\tLoss: 2811.5408\n",
      "Training Epoch: 3 [41360/49669]\tLoss: 2568.7546\n",
      "Training Epoch: 3 [41380/49669]\tLoss: 2838.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [41400/49669]\tLoss: 2465.6777\n",
      "Training Epoch: 3 [41420/49669]\tLoss: 2358.1501\n",
      "Training Epoch: 3 [41440/49669]\tLoss: 2959.3777\n",
      "Training Epoch: 3 [41460/49669]\tLoss: 2623.5566\n",
      "Training Epoch: 3 [41480/49669]\tLoss: 2827.9939\n",
      "Training Epoch: 3 [41500/49669]\tLoss: 2681.3066\n",
      "Training Epoch: 3 [41520/49669]\tLoss: 2655.2236\n",
      "Training Epoch: 3 [41540/49669]\tLoss: 2369.5078\n",
      "Training Epoch: 3 [41560/49669]\tLoss: 2379.4614\n",
      "Training Epoch: 3 [41580/49669]\tLoss: 2698.8115\n",
      "Training Epoch: 3 [41600/49669]\tLoss: 2245.9768\n",
      "Training Epoch: 3 [41620/49669]\tLoss: 2436.0049\n",
      "Training Epoch: 3 [41640/49669]\tLoss: 2834.8403\n",
      "Training Epoch: 3 [41660/49669]\tLoss: 2675.1057\n",
      "Training Epoch: 3 [41680/49669]\tLoss: 2892.3010\n",
      "Training Epoch: 3 [41700/49669]\tLoss: 2493.5186\n",
      "Training Epoch: 3 [41720/49669]\tLoss: 3130.6646\n",
      "Training Epoch: 3 [41740/49669]\tLoss: 2935.1492\n",
      "Training Epoch: 3 [41760/49669]\tLoss: 2355.1213\n",
      "Training Epoch: 3 [41780/49669]\tLoss: 2394.1699\n",
      "Training Epoch: 3 [41800/49669]\tLoss: 2879.4954\n",
      "Training Epoch: 3 [41820/49669]\tLoss: 2782.2849\n",
      "Training Epoch: 3 [41840/49669]\tLoss: 2648.5835\n",
      "Training Epoch: 3 [41860/49669]\tLoss: 2577.0151\n",
      "Training Epoch: 3 [41880/49669]\tLoss: 2574.2253\n",
      "Training Epoch: 3 [41900/49669]\tLoss: 2733.9504\n",
      "Training Epoch: 3 [41920/49669]\tLoss: 2372.0237\n",
      "Training Epoch: 3 [41940/49669]\tLoss: 2449.5532\n",
      "Training Epoch: 3 [41960/49669]\tLoss: 2505.1646\n",
      "Training Epoch: 3 [41980/49669]\tLoss: 2503.7551\n",
      "Training Epoch: 3 [42000/49669]\tLoss: 2611.0081\n",
      "Training Epoch: 3 [42020/49669]\tLoss: 2789.5696\n",
      "Training Epoch: 3 [42040/49669]\tLoss: 2749.7361\n",
      "Training Epoch: 3 [42060/49669]\tLoss: 2752.9541\n",
      "Training Epoch: 3 [42080/49669]\tLoss: 2811.7693\n",
      "Training Epoch: 3 [42100/49669]\tLoss: 2796.4778\n",
      "Training Epoch: 3 [42120/49669]\tLoss: 2898.9978\n",
      "Training Epoch: 3 [42140/49669]\tLoss: 2024.9874\n",
      "Training Epoch: 3 [42160/49669]\tLoss: 2558.3667\n",
      "Training Epoch: 3 [42180/49669]\tLoss: 2630.2085\n",
      "Training Epoch: 3 [42200/49669]\tLoss: 2796.1060\n",
      "Training Epoch: 3 [42220/49669]\tLoss: 2630.7466\n",
      "Training Epoch: 3 [42240/49669]\tLoss: 2777.2283\n",
      "Training Epoch: 3 [42260/49669]\tLoss: 2267.2925\n",
      "Training Epoch: 3 [42280/49669]\tLoss: 2855.3315\n",
      "Training Epoch: 3 [42300/49669]\tLoss: 2896.3499\n",
      "Training Epoch: 3 [42320/49669]\tLoss: 2735.6934\n",
      "Training Epoch: 3 [42340/49669]\tLoss: 3092.2021\n",
      "Training Epoch: 3 [42360/49669]\tLoss: 2559.0349\n",
      "Training Epoch: 3 [42380/49669]\tLoss: 2429.5935\n",
      "Training Epoch: 3 [42400/49669]\tLoss: 2587.8015\n",
      "Training Epoch: 3 [42420/49669]\tLoss: 2349.1567\n",
      "Training Epoch: 3 [42440/49669]\tLoss: 2453.2075\n",
      "Training Epoch: 3 [42460/49669]\tLoss: 2762.2502\n",
      "Training Epoch: 3 [42480/49669]\tLoss: 2490.7214\n",
      "Training Epoch: 3 [42500/49669]\tLoss: 2570.1990\n",
      "Training Epoch: 3 [42520/49669]\tLoss: 2774.0879\n",
      "Training Epoch: 3 [42540/49669]\tLoss: 2848.4084\n",
      "Training Epoch: 3 [42560/49669]\tLoss: 2522.0933\n",
      "Training Epoch: 3 [42580/49669]\tLoss: 2606.8911\n",
      "Training Epoch: 3 [42600/49669]\tLoss: 2459.2144\n",
      "Training Epoch: 3 [42620/49669]\tLoss: 2733.8257\n",
      "Training Epoch: 3 [42640/49669]\tLoss: 2562.2046\n",
      "Training Epoch: 3 [42660/49669]\tLoss: 2745.0715\n",
      "Training Epoch: 3 [42680/49669]\tLoss: 2820.0398\n",
      "Training Epoch: 3 [42700/49669]\tLoss: 2227.9834\n",
      "Training Epoch: 3 [42720/49669]\tLoss: 2601.6077\n",
      "Training Epoch: 3 [42740/49669]\tLoss: 2372.9797\n",
      "Training Epoch: 3 [42760/49669]\tLoss: 2573.5339\n",
      "Training Epoch: 3 [42780/49669]\tLoss: 2631.8660\n",
      "Training Epoch: 3 [42800/49669]\tLoss: 2761.4998\n",
      "Training Epoch: 3 [42820/49669]\tLoss: 2548.2698\n",
      "Training Epoch: 3 [42840/49669]\tLoss: 2630.1819\n",
      "Training Epoch: 3 [42860/49669]\tLoss: 2675.1350\n",
      "Training Epoch: 3 [42880/49669]\tLoss: 2850.5852\n",
      "Training Epoch: 3 [42900/49669]\tLoss: 2634.3499\n",
      "Training Epoch: 3 [42920/49669]\tLoss: 2800.5505\n",
      "Training Epoch: 3 [42940/49669]\tLoss: 2809.2654\n",
      "Training Epoch: 3 [42960/49669]\tLoss: 2417.7566\n",
      "Training Epoch: 3 [42980/49669]\tLoss: 2585.5068\n",
      "Training Epoch: 3 [43000/49669]\tLoss: 2752.7661\n",
      "Training Epoch: 3 [43020/49669]\tLoss: 2999.5210\n",
      "Training Epoch: 3 [43040/49669]\tLoss: 2738.8259\n",
      "Training Epoch: 3 [43060/49669]\tLoss: 2533.8894\n",
      "Training Epoch: 3 [43080/49669]\tLoss: 2829.2922\n",
      "Training Epoch: 3 [43100/49669]\tLoss: 2560.3962\n",
      "Training Epoch: 3 [43120/49669]\tLoss: 2718.8611\n",
      "Training Epoch: 3 [43140/49669]\tLoss: 2868.1841\n",
      "Training Epoch: 3 [43160/49669]\tLoss: 2666.2156\n",
      "Training Epoch: 3 [43180/49669]\tLoss: 2278.6047\n",
      "Training Epoch: 3 [43200/49669]\tLoss: 2473.6516\n",
      "Training Epoch: 3 [43220/49669]\tLoss: 2694.0703\n",
      "Training Epoch: 3 [43240/49669]\tLoss: 2739.9285\n",
      "Training Epoch: 3 [43260/49669]\tLoss: 2426.9053\n",
      "Training Epoch: 3 [43280/49669]\tLoss: 2377.6243\n",
      "Training Epoch: 3 [43300/49669]\tLoss: 2483.6050\n",
      "Training Epoch: 3 [43320/49669]\tLoss: 2861.7979\n",
      "Training Epoch: 3 [43340/49669]\tLoss: 2478.5632\n",
      "Training Epoch: 3 [43360/49669]\tLoss: 2652.2854\n",
      "Training Epoch: 3 [43380/49669]\tLoss: 2696.9673\n",
      "Training Epoch: 3 [43400/49669]\tLoss: 2639.0393\n",
      "Training Epoch: 3 [43420/49669]\tLoss: 2860.5940\n",
      "Training Epoch: 3 [43440/49669]\tLoss: 2588.3298\n",
      "Training Epoch: 3 [43460/49669]\tLoss: 2340.5520\n",
      "Training Epoch: 3 [43480/49669]\tLoss: 2865.5527\n",
      "Training Epoch: 3 [43500/49669]\tLoss: 2788.3936\n",
      "Training Epoch: 3 [43520/49669]\tLoss: 2786.2810\n",
      "Training Epoch: 3 [43540/49669]\tLoss: 2505.3091\n",
      "Training Epoch: 3 [43560/49669]\tLoss: 2774.3982\n",
      "Training Epoch: 3 [43580/49669]\tLoss: 2337.7822\n",
      "Training Epoch: 3 [43600/49669]\tLoss: 2757.7214\n",
      "Training Epoch: 3 [43620/49669]\tLoss: 2301.1443\n",
      "Training Epoch: 3 [43640/49669]\tLoss: 2601.6287\n",
      "Training Epoch: 3 [43660/49669]\tLoss: 2323.6775\n",
      "Training Epoch: 3 [43680/49669]\tLoss: 2683.1570\n",
      "Training Epoch: 3 [43700/49669]\tLoss: 2719.6843\n",
      "Training Epoch: 3 [43720/49669]\tLoss: 2680.7605\n",
      "Training Epoch: 3 [43740/49669]\tLoss: 2875.1572\n",
      "Training Epoch: 3 [43760/49669]\tLoss: 2472.6492\n",
      "Training Epoch: 3 [43780/49669]\tLoss: 2554.8918\n",
      "Training Epoch: 3 [43800/49669]\tLoss: 2180.4229\n",
      "Training Epoch: 3 [43820/49669]\tLoss: 2670.7097\n",
      "Training Epoch: 3 [43840/49669]\tLoss: 2468.3328\n",
      "Training Epoch: 3 [43860/49669]\tLoss: 2667.5027\n",
      "Training Epoch: 3 [43880/49669]\tLoss: 2688.8628\n",
      "Training Epoch: 3 [43900/49669]\tLoss: 2523.5723\n",
      "Training Epoch: 3 [43920/49669]\tLoss: 2659.9871\n",
      "Training Epoch: 3 [43940/49669]\tLoss: 2605.7205\n",
      "Training Epoch: 3 [43960/49669]\tLoss: 2614.6174\n",
      "Training Epoch: 3 [43980/49669]\tLoss: 2772.7263\n",
      "Training Epoch: 3 [44000/49669]\tLoss: 2644.4333\n",
      "Training Epoch: 3 [44020/49669]\tLoss: 2607.7483\n",
      "Training Epoch: 3 [44040/49669]\tLoss: 2772.3965\n",
      "Training Epoch: 3 [44060/49669]\tLoss: 2446.2134\n",
      "Training Epoch: 3 [44080/49669]\tLoss: 2684.2822\n",
      "Training Epoch: 3 [44100/49669]\tLoss: 2250.4482\n",
      "Training Epoch: 3 [44120/49669]\tLoss: 2921.9792\n",
      "Training Epoch: 3 [44140/49669]\tLoss: 2770.7156\n",
      "Training Epoch: 3 [44160/49669]\tLoss: 2978.0376\n",
      "Training Epoch: 3 [44180/49669]\tLoss: 2464.1567\n",
      "Training Epoch: 3 [44200/49669]\tLoss: 2653.4731\n",
      "Training Epoch: 3 [44220/49669]\tLoss: 2749.4031\n",
      "Training Epoch: 3 [44240/49669]\tLoss: 2516.6326\n",
      "Training Epoch: 3 [44260/49669]\tLoss: 2602.3047\n",
      "Training Epoch: 3 [44280/49669]\tLoss: 2724.6741\n",
      "Training Epoch: 3 [44300/49669]\tLoss: 2732.2566\n",
      "Training Epoch: 3 [44320/49669]\tLoss: 2444.8428\n",
      "Training Epoch: 3 [44340/49669]\tLoss: 2808.5916\n",
      "Training Epoch: 3 [44360/49669]\tLoss: 2578.5811\n",
      "Training Epoch: 3 [44380/49669]\tLoss: 2951.2390\n",
      "Training Epoch: 3 [44400/49669]\tLoss: 2615.0852\n",
      "Training Epoch: 3 [44420/49669]\tLoss: 2905.9885\n",
      "Training Epoch: 3 [44440/49669]\tLoss: 2345.4414\n",
      "Training Epoch: 3 [44460/49669]\tLoss: 2720.0066\n",
      "Training Epoch: 3 [44480/49669]\tLoss: 2529.5654\n",
      "Training Epoch: 3 [44500/49669]\tLoss: 2218.9392\n",
      "Training Epoch: 3 [44520/49669]\tLoss: 2662.8335\n",
      "Training Epoch: 3 [44540/49669]\tLoss: 2649.1145\n",
      "Training Epoch: 3 [44560/49669]\tLoss: 2688.6885\n",
      "Training Epoch: 3 [44580/49669]\tLoss: 2742.7058\n",
      "Training Epoch: 3 [44600/49669]\tLoss: 2698.6760\n",
      "Training Epoch: 3 [44620/49669]\tLoss: 2502.9565\n",
      "Training Epoch: 3 [44640/49669]\tLoss: 2592.5771\n",
      "Training Epoch: 3 [44660/49669]\tLoss: 2464.9788\n",
      "Training Epoch: 3 [44680/49669]\tLoss: 2527.2302\n",
      "Training Epoch: 3 [44700/49669]\tLoss: 2835.1758\n",
      "Training Epoch: 3 [44720/49669]\tLoss: 2254.8508\n",
      "Training Epoch: 3 [44740/49669]\tLoss: 2556.4519\n",
      "Training Epoch: 3 [44760/49669]\tLoss: 2382.4133\n",
      "Training Epoch: 3 [44780/49669]\tLoss: 2825.4668\n",
      "Training Epoch: 3 [44800/49669]\tLoss: 2430.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [44820/49669]\tLoss: 2494.2539\n",
      "Training Epoch: 3 [44840/49669]\tLoss: 2436.6287\n",
      "Training Epoch: 3 [44860/49669]\tLoss: 2633.5186\n",
      "Training Epoch: 3 [44880/49669]\tLoss: 2650.8643\n",
      "Training Epoch: 3 [44900/49669]\tLoss: 2616.3086\n",
      "Training Epoch: 3 [44920/49669]\tLoss: 2349.0530\n",
      "Training Epoch: 3 [44940/49669]\tLoss: 2514.3069\n",
      "Training Epoch: 3 [44960/49669]\tLoss: 2356.5520\n",
      "Training Epoch: 3 [44980/49669]\tLoss: 2542.3503\n",
      "Training Epoch: 3 [45000/49669]\tLoss: 2674.8203\n",
      "Training Epoch: 3 [45020/49669]\tLoss: 2479.8013\n",
      "Training Epoch: 3 [45040/49669]\tLoss: 2556.3538\n",
      "Training Epoch: 3 [45060/49669]\tLoss: 2706.9902\n",
      "Training Epoch: 3 [45080/49669]\tLoss: 2607.0266\n",
      "Training Epoch: 3 [45100/49669]\tLoss: 2506.3027\n",
      "Training Epoch: 3 [45120/49669]\tLoss: 2667.7073\n",
      "Training Epoch: 3 [45140/49669]\tLoss: 2563.3921\n",
      "Training Epoch: 3 [45160/49669]\tLoss: 2543.2866\n",
      "Training Epoch: 3 [45180/49669]\tLoss: 2287.9827\n",
      "Training Epoch: 3 [45200/49669]\tLoss: 2604.3638\n",
      "Training Epoch: 3 [45220/49669]\tLoss: 2561.6353\n",
      "Training Epoch: 3 [45240/49669]\tLoss: 2693.3711\n",
      "Training Epoch: 3 [45260/49669]\tLoss: 2561.4648\n",
      "Training Epoch: 3 [45280/49669]\tLoss: 2727.9631\n",
      "Training Epoch: 3 [45300/49669]\tLoss: 2545.1194\n",
      "Training Epoch: 3 [45320/49669]\tLoss: 2890.5356\n",
      "Training Epoch: 3 [45340/49669]\tLoss: 2473.8699\n",
      "Training Epoch: 3 [45360/49669]\tLoss: 2427.5515\n",
      "Training Epoch: 3 [45380/49669]\tLoss: 3057.1628\n",
      "Training Epoch: 3 [45400/49669]\tLoss: 2453.4241\n",
      "Training Epoch: 3 [45420/49669]\tLoss: 2790.1741\n",
      "Training Epoch: 3 [45440/49669]\tLoss: 2707.1345\n",
      "Training Epoch: 3 [45460/49669]\tLoss: 2483.3608\n",
      "Training Epoch: 3 [45480/49669]\tLoss: 2608.0391\n",
      "Training Epoch: 3 [45500/49669]\tLoss: 2814.7566\n",
      "Training Epoch: 3 [45520/49669]\tLoss: 2848.1541\n",
      "Training Epoch: 3 [45540/49669]\tLoss: 2678.2222\n",
      "Training Epoch: 3 [45560/49669]\tLoss: 2509.1707\n",
      "Training Epoch: 3 [45580/49669]\tLoss: 2628.0068\n",
      "Training Epoch: 3 [45600/49669]\tLoss: 2905.1348\n",
      "Training Epoch: 3 [45620/49669]\tLoss: 2554.6082\n",
      "Training Epoch: 3 [45640/49669]\tLoss: 3132.0686\n",
      "Training Epoch: 3 [45660/49669]\tLoss: 2592.4236\n",
      "Training Epoch: 3 [45680/49669]\tLoss: 2625.2837\n",
      "Training Epoch: 3 [45700/49669]\tLoss: 2602.3333\n",
      "Training Epoch: 3 [45720/49669]\tLoss: 2675.0295\n",
      "Training Epoch: 3 [45740/49669]\tLoss: 2655.8413\n",
      "Training Epoch: 3 [45760/49669]\tLoss: 2776.8333\n",
      "Training Epoch: 3 [45780/49669]\tLoss: 2370.9714\n",
      "Training Epoch: 3 [45800/49669]\tLoss: 2950.0837\n",
      "Training Epoch: 3 [45820/49669]\tLoss: 2836.5720\n",
      "Training Epoch: 3 [45840/49669]\tLoss: 2624.9678\n",
      "Training Epoch: 3 [45860/49669]\tLoss: 2892.7317\n",
      "Training Epoch: 3 [45880/49669]\tLoss: 2538.3020\n",
      "Training Epoch: 3 [45900/49669]\tLoss: 2643.4312\n",
      "Training Epoch: 3 [45920/49669]\tLoss: 2332.5107\n",
      "Training Epoch: 3 [45940/49669]\tLoss: 2912.0369\n",
      "Training Epoch: 3 [45960/49669]\tLoss: 2796.9001\n",
      "Training Epoch: 3 [45980/49669]\tLoss: 2789.5330\n",
      "Training Epoch: 3 [46000/49669]\tLoss: 2456.9080\n",
      "Training Epoch: 3 [46020/49669]\tLoss: 2648.0710\n",
      "Training Epoch: 3 [46040/49669]\tLoss: 2782.4226\n",
      "Training Epoch: 3 [46060/49669]\tLoss: 2519.2966\n",
      "Training Epoch: 3 [46080/49669]\tLoss: 2760.6743\n",
      "Training Epoch: 3 [46100/49669]\tLoss: 2470.9163\n",
      "Training Epoch: 3 [46120/49669]\tLoss: 2118.4973\n",
      "Training Epoch: 3 [46140/49669]\tLoss: 2679.8655\n",
      "Training Epoch: 3 [46160/49669]\tLoss: 2838.9995\n",
      "Training Epoch: 3 [46180/49669]\tLoss: 2693.6089\n",
      "Training Epoch: 3 [46200/49669]\tLoss: 2603.2039\n",
      "Training Epoch: 3 [46220/49669]\tLoss: 2742.3838\n",
      "Training Epoch: 3 [46240/49669]\tLoss: 2567.8303\n",
      "Training Epoch: 3 [46260/49669]\tLoss: 2737.6631\n",
      "Training Epoch: 3 [46280/49669]\tLoss: 2413.6936\n",
      "Training Epoch: 3 [46300/49669]\tLoss: 2945.0457\n",
      "Training Epoch: 3 [46320/49669]\tLoss: 2384.1340\n",
      "Training Epoch: 3 [46340/49669]\tLoss: 2464.8442\n",
      "Training Epoch: 3 [46360/49669]\tLoss: 2823.5046\n",
      "Training Epoch: 3 [46380/49669]\tLoss: 2773.6213\n",
      "Training Epoch: 3 [46400/49669]\tLoss: 2733.0911\n",
      "Training Epoch: 3 [46420/49669]\tLoss: 2802.7729\n",
      "Training Epoch: 3 [46440/49669]\tLoss: 2312.8276\n",
      "Training Epoch: 3 [46460/49669]\tLoss: 2385.5950\n",
      "Training Epoch: 3 [46480/49669]\tLoss: 2835.1565\n",
      "Training Epoch: 3 [46500/49669]\tLoss: 2654.5051\n",
      "Training Epoch: 3 [46520/49669]\tLoss: 2409.0564\n",
      "Training Epoch: 3 [46540/49669]\tLoss: 2296.0542\n",
      "Training Epoch: 3 [46560/49669]\tLoss: 3051.6160\n",
      "Training Epoch: 3 [46580/49669]\tLoss: 2470.9424\n",
      "Training Epoch: 3 [46600/49669]\tLoss: 2891.6348\n",
      "Training Epoch: 3 [46620/49669]\tLoss: 2628.5430\n",
      "Training Epoch: 3 [46640/49669]\tLoss: 2658.0078\n",
      "Training Epoch: 3 [46660/49669]\tLoss: 2714.5972\n",
      "Training Epoch: 3 [46680/49669]\tLoss: 2644.1067\n",
      "Training Epoch: 3 [46700/49669]\tLoss: 2792.6406\n",
      "Training Epoch: 3 [46720/49669]\tLoss: 2934.3984\n",
      "Training Epoch: 3 [46740/49669]\tLoss: 2430.7637\n",
      "Training Epoch: 3 [46760/49669]\tLoss: 2329.1428\n",
      "Training Epoch: 3 [46780/49669]\tLoss: 2672.6511\n",
      "Training Epoch: 3 [46800/49669]\tLoss: 2595.8240\n",
      "Training Epoch: 3 [46820/49669]\tLoss: 2536.5229\n",
      "Training Epoch: 3 [46840/49669]\tLoss: 2392.0938\n",
      "Training Epoch: 3 [46860/49669]\tLoss: 2730.1621\n",
      "Training Epoch: 3 [46880/49669]\tLoss: 2538.3701\n",
      "Training Epoch: 3 [46900/49669]\tLoss: 2836.3113\n",
      "Training Epoch: 3 [46920/49669]\tLoss: 2504.4626\n",
      "Training Epoch: 3 [46940/49669]\tLoss: 2478.9436\n",
      "Training Epoch: 3 [46960/49669]\tLoss: 2785.7917\n",
      "Training Epoch: 3 [46980/49669]\tLoss: 2529.6218\n",
      "Training Epoch: 3 [47000/49669]\tLoss: 2786.2397\n",
      "Training Epoch: 3 [47020/49669]\tLoss: 2710.6719\n",
      "Training Epoch: 3 [47040/49669]\tLoss: 2598.8257\n",
      "Training Epoch: 3 [47060/49669]\tLoss: 2364.7427\n",
      "Training Epoch: 3 [47080/49669]\tLoss: 2393.8777\n",
      "Training Epoch: 3 [47100/49669]\tLoss: 2525.3098\n",
      "Training Epoch: 3 [47120/49669]\tLoss: 2702.3730\n",
      "Training Epoch: 3 [47140/49669]\tLoss: 2601.7310\n",
      "Training Epoch: 3 [47160/49669]\tLoss: 2365.9849\n",
      "Training Epoch: 3 [47180/49669]\tLoss: 2579.7202\n",
      "Training Epoch: 3 [47200/49669]\tLoss: 2434.7935\n",
      "Training Epoch: 3 [47220/49669]\tLoss: 2844.6497\n",
      "Training Epoch: 3 [47240/49669]\tLoss: 2835.2051\n",
      "Training Epoch: 3 [47260/49669]\tLoss: 2801.7683\n",
      "Training Epoch: 3 [47280/49669]\tLoss: 3050.6506\n",
      "Training Epoch: 3 [47300/49669]\tLoss: 2447.8088\n",
      "Training Epoch: 3 [47320/49669]\tLoss: 2375.8379\n",
      "Training Epoch: 3 [47340/49669]\tLoss: 2667.9712\n",
      "Training Epoch: 3 [47360/49669]\tLoss: 2247.2354\n",
      "Training Epoch: 3 [47380/49669]\tLoss: 2499.0718\n",
      "Training Epoch: 3 [47400/49669]\tLoss: 2662.7971\n",
      "Training Epoch: 3 [47420/49669]\tLoss: 2833.4973\n",
      "Training Epoch: 3 [47440/49669]\tLoss: 2653.5276\n",
      "Training Epoch: 3 [47460/49669]\tLoss: 2851.9741\n",
      "Training Epoch: 3 [47480/49669]\tLoss: 2733.7544\n",
      "Training Epoch: 3 [47500/49669]\tLoss: 2426.3000\n",
      "Training Epoch: 3 [47520/49669]\tLoss: 2838.7668\n",
      "Training Epoch: 3 [47540/49669]\tLoss: 2642.1892\n",
      "Training Epoch: 3 [47560/49669]\tLoss: 2738.7048\n",
      "Training Epoch: 3 [47580/49669]\tLoss: 2695.4780\n",
      "Training Epoch: 3 [47600/49669]\tLoss: 2747.7520\n",
      "Training Epoch: 3 [47620/49669]\tLoss: 2732.7917\n",
      "Training Epoch: 3 [47640/49669]\tLoss: 2660.1423\n",
      "Training Epoch: 3 [47660/49669]\tLoss: 2888.5110\n",
      "Training Epoch: 3 [47680/49669]\tLoss: 2671.7188\n",
      "Training Epoch: 3 [47700/49669]\tLoss: 2777.4597\n",
      "Training Epoch: 3 [47720/49669]\tLoss: 2752.5562\n",
      "Training Epoch: 3 [47740/49669]\tLoss: 2565.6255\n",
      "Training Epoch: 3 [47760/49669]\tLoss: 2437.2495\n",
      "Training Epoch: 3 [47780/49669]\tLoss: 2504.5725\n",
      "Training Epoch: 3 [47800/49669]\tLoss: 2395.6682\n",
      "Training Epoch: 3 [47820/49669]\tLoss: 2641.8350\n",
      "Training Epoch: 3 [47840/49669]\tLoss: 2806.5525\n",
      "Training Epoch: 3 [47860/49669]\tLoss: 2416.4446\n",
      "Training Epoch: 3 [47880/49669]\tLoss: 2722.1755\n",
      "Training Epoch: 3 [47900/49669]\tLoss: 2802.1392\n",
      "Training Epoch: 3 [47920/49669]\tLoss: 2746.1443\n",
      "Training Epoch: 3 [47940/49669]\tLoss: 2452.7407\n",
      "Training Epoch: 3 [47960/49669]\tLoss: 2574.5818\n",
      "Training Epoch: 3 [47980/49669]\tLoss: 2644.7244\n",
      "Training Epoch: 3 [48000/49669]\tLoss: 2463.1982\n",
      "Training Epoch: 3 [48020/49669]\tLoss: 2706.2793\n",
      "Training Epoch: 3 [48040/49669]\tLoss: 2799.2156\n",
      "Training Epoch: 3 [48060/49669]\tLoss: 2475.1538\n",
      "Training Epoch: 3 [48080/49669]\tLoss: 2233.1926\n",
      "Training Epoch: 3 [48100/49669]\tLoss: 2786.5090\n",
      "Training Epoch: 3 [48120/49669]\tLoss: 2390.3640\n",
      "Training Epoch: 3 [48140/49669]\tLoss: 2616.4958\n",
      "Training Epoch: 3 [48160/49669]\tLoss: 2855.2327\n",
      "Training Epoch: 3 [48180/49669]\tLoss: 2679.2097\n",
      "Training Epoch: 3 [48200/49669]\tLoss: 2782.0166\n",
      "Training Epoch: 3 [48220/49669]\tLoss: 3014.5471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [48240/49669]\tLoss: 2634.4854\n",
      "Training Epoch: 3 [48260/49669]\tLoss: 2626.0544\n",
      "Training Epoch: 3 [48280/49669]\tLoss: 2550.2200\n",
      "Training Epoch: 3 [48300/49669]\tLoss: 2539.2473\n",
      "Training Epoch: 3 [48320/49669]\tLoss: 2323.9214\n",
      "Training Epoch: 3 [48340/49669]\tLoss: 2753.4797\n",
      "Training Epoch: 3 [48360/49669]\tLoss: 2251.4695\n",
      "Training Epoch: 3 [48380/49669]\tLoss: 2400.4050\n",
      "Training Epoch: 3 [48400/49669]\tLoss: 2441.5217\n",
      "Training Epoch: 3 [48420/49669]\tLoss: 2196.7837\n",
      "Training Epoch: 3 [48440/49669]\tLoss: 2610.5872\n",
      "Training Epoch: 3 [48460/49669]\tLoss: 3016.1094\n",
      "Training Epoch: 3 [48480/49669]\tLoss: 2684.3020\n",
      "Training Epoch: 3 [48500/49669]\tLoss: 2440.9434\n",
      "Training Epoch: 3 [48520/49669]\tLoss: 2566.2163\n",
      "Training Epoch: 3 [48540/49669]\tLoss: 2518.9143\n",
      "Training Epoch: 3 [48560/49669]\tLoss: 2571.9563\n",
      "Training Epoch: 3 [48580/49669]\tLoss: 2214.9092\n",
      "Training Epoch: 3 [48600/49669]\tLoss: 2405.6384\n",
      "Training Epoch: 3 [48620/49669]\tLoss: 2600.2424\n",
      "Training Epoch: 3 [48640/49669]\tLoss: 2884.3628\n",
      "Training Epoch: 3 [48660/49669]\tLoss: 2546.6218\n",
      "Training Epoch: 3 [48680/49669]\tLoss: 2954.4297\n",
      "Training Epoch: 3 [48700/49669]\tLoss: 2370.5422\n",
      "Training Epoch: 3 [48720/49669]\tLoss: 2693.6653\n",
      "Training Epoch: 3 [48740/49669]\tLoss: 2562.7622\n",
      "Training Epoch: 3 [48760/49669]\tLoss: 2730.4443\n",
      "Training Epoch: 3 [48780/49669]\tLoss: 2458.6675\n",
      "Training Epoch: 3 [48800/49669]\tLoss: 2617.3726\n",
      "Training Epoch: 3 [48820/49669]\tLoss: 2464.7888\n",
      "Training Epoch: 3 [48840/49669]\tLoss: 2460.5276\n",
      "Training Epoch: 3 [48860/49669]\tLoss: 2615.7463\n",
      "Training Epoch: 3 [48880/49669]\tLoss: 2843.7168\n",
      "Training Epoch: 3 [48900/49669]\tLoss: 2676.3159\n",
      "Training Epoch: 3 [48920/49669]\tLoss: 2722.7568\n",
      "Training Epoch: 3 [48940/49669]\tLoss: 2690.6873\n",
      "Training Epoch: 3 [48960/49669]\tLoss: 2486.3960\n",
      "Training Epoch: 3 [48980/49669]\tLoss: 2780.4834\n",
      "Training Epoch: 3 [49000/49669]\tLoss: 2493.8872\n",
      "Training Epoch: 3 [49020/49669]\tLoss: 2451.7402\n",
      "Training Epoch: 3 [49040/49669]\tLoss: 2746.8403\n",
      "Training Epoch: 3 [49060/49669]\tLoss: 2336.9678\n",
      "Training Epoch: 3 [49080/49669]\tLoss: 2703.5156\n",
      "Training Epoch: 3 [49100/49669]\tLoss: 2422.4429\n",
      "Training Epoch: 3 [49120/49669]\tLoss: 2471.5171\n",
      "Training Epoch: 3 [49140/49669]\tLoss: 2773.2456\n",
      "Training Epoch: 3 [49160/49669]\tLoss: 2560.7407\n",
      "Training Epoch: 3 [49180/49669]\tLoss: 2643.8232\n",
      "Training Epoch: 3 [49200/49669]\tLoss: 2419.9587\n",
      "Training Epoch: 3 [49220/49669]\tLoss: 2708.0818\n",
      "Training Epoch: 3 [49240/49669]\tLoss: 2232.3474\n",
      "Training Epoch: 3 [49260/49669]\tLoss: 2831.8867\n",
      "Training Epoch: 3 [49280/49669]\tLoss: 2477.4355\n",
      "Training Epoch: 3 [49300/49669]\tLoss: 2510.7881\n",
      "Training Epoch: 3 [49320/49669]\tLoss: 2491.1772\n",
      "Training Epoch: 3 [49340/49669]\tLoss: 2607.2368\n",
      "Training Epoch: 3 [49360/49669]\tLoss: 2443.6973\n",
      "Training Epoch: 3 [49380/49669]\tLoss: 2614.8813\n",
      "Training Epoch: 3 [49400/49669]\tLoss: 2424.9788\n",
      "Training Epoch: 3 [49420/49669]\tLoss: 2411.2407\n",
      "Training Epoch: 3 [49440/49669]\tLoss: 2639.4202\n",
      "Training Epoch: 3 [49460/49669]\tLoss: 2538.7866\n",
      "Training Epoch: 3 [49480/49669]\tLoss: 2707.0144\n",
      "Training Epoch: 3 [49500/49669]\tLoss: 2398.4744\n",
      "Training Epoch: 3 [49520/49669]\tLoss: 2640.0737\n",
      "Training Epoch: 3 [49540/49669]\tLoss: 2594.5293\n",
      "Training Epoch: 3 [49560/49669]\tLoss: 2587.7808\n",
      "Training Epoch: 3 [49580/49669]\tLoss: 2685.7070\n",
      "Training Epoch: 3 [49600/49669]\tLoss: 2610.7310\n",
      "Training Epoch: 3 [49620/49669]\tLoss: 2689.4106\n",
      "Training Epoch: 3 [49640/49669]\tLoss: 2464.4106\n",
      "Training Epoch: 3 [49660/49669]\tLoss: 2385.0166\n",
      "Training Epoch: 3 [49669/49669]\tLoss: 2218.0432\n",
      "Training Epoch: 3 [5519/5519]\tLoss: 2568.5117\n",
      "Training Epoch: 4 [20/49669]\tLoss: 2240.6909\n",
      "Training Epoch: 4 [40/49669]\tLoss: 2636.0471\n",
      "Training Epoch: 4 [60/49669]\tLoss: 2810.9702\n",
      "Training Epoch: 4 [80/49669]\tLoss: 2530.7349\n",
      "Training Epoch: 4 [100/49669]\tLoss: 2453.9666\n",
      "Training Epoch: 4 [120/49669]\tLoss: 2206.4556\n",
      "Training Epoch: 4 [140/49669]\tLoss: 2449.2258\n",
      "Training Epoch: 4 [160/49669]\tLoss: 2477.6443\n",
      "Training Epoch: 4 [180/49669]\tLoss: 2108.3992\n",
      "Training Epoch: 4 [200/49669]\tLoss: 2486.1404\n",
      "Training Epoch: 4 [220/49669]\tLoss: 2456.7849\n",
      "Training Epoch: 4 [240/49669]\tLoss: 2621.9429\n",
      "Training Epoch: 4 [260/49669]\tLoss: 2678.2061\n",
      "Training Epoch: 4 [280/49669]\tLoss: 2476.7886\n",
      "Training Epoch: 4 [300/49669]\tLoss: 2469.9080\n",
      "Training Epoch: 4 [320/49669]\tLoss: 2684.8960\n",
      "Training Epoch: 4 [340/49669]\tLoss: 2501.5654\n",
      "Training Epoch: 4 [360/49669]\tLoss: 2385.2168\n",
      "Training Epoch: 4 [380/49669]\tLoss: 2599.6519\n",
      "Training Epoch: 4 [400/49669]\tLoss: 2786.2766\n",
      "Training Epoch: 4 [420/49669]\tLoss: 3083.8684\n",
      "Training Epoch: 4 [440/49669]\tLoss: 2610.0095\n",
      "Training Epoch: 4 [460/49669]\tLoss: 2511.9326\n",
      "Training Epoch: 4 [480/49669]\tLoss: 2518.9888\n",
      "Training Epoch: 4 [500/49669]\tLoss: 2806.6035\n",
      "Training Epoch: 4 [520/49669]\tLoss: 2525.7429\n",
      "Training Epoch: 4 [540/49669]\tLoss: 2580.2107\n",
      "Training Epoch: 4 [560/49669]\tLoss: 2509.6033\n",
      "Training Epoch: 4 [580/49669]\tLoss: 2775.8860\n",
      "Training Epoch: 4 [600/49669]\tLoss: 2532.9541\n",
      "Training Epoch: 4 [620/49669]\tLoss: 2660.5818\n",
      "Training Epoch: 4 [640/49669]\tLoss: 2822.7664\n",
      "Training Epoch: 4 [660/49669]\tLoss: 2570.4316\n",
      "Training Epoch: 4 [680/49669]\tLoss: 2407.9907\n",
      "Training Epoch: 4 [700/49669]\tLoss: 2491.9194\n",
      "Training Epoch: 4 [720/49669]\tLoss: 2522.3560\n",
      "Training Epoch: 4 [740/49669]\tLoss: 2730.4270\n",
      "Training Epoch: 4 [760/49669]\tLoss: 2808.2690\n",
      "Training Epoch: 4 [780/49669]\tLoss: 2869.3015\n",
      "Training Epoch: 4 [800/49669]\tLoss: 2806.1184\n",
      "Training Epoch: 4 [820/49669]\tLoss: 2753.9785\n",
      "Training Epoch: 4 [840/49669]\tLoss: 2379.4009\n",
      "Training Epoch: 4 [860/49669]\tLoss: 2633.7942\n",
      "Training Epoch: 4 [880/49669]\tLoss: 2809.9785\n",
      "Training Epoch: 4 [900/49669]\tLoss: 2877.5496\n",
      "Training Epoch: 4 [920/49669]\tLoss: 2602.8232\n",
      "Training Epoch: 4 [940/49669]\tLoss: 2678.9583\n",
      "Training Epoch: 4 [960/49669]\tLoss: 2570.2014\n",
      "Training Epoch: 4 [980/49669]\tLoss: 2433.5732\n",
      "Training Epoch: 4 [1000/49669]\tLoss: 2474.1650\n",
      "Training Epoch: 4 [1020/49669]\tLoss: 2588.6790\n",
      "Training Epoch: 4 [1040/49669]\tLoss: 2698.1443\n",
      "Training Epoch: 4 [1060/49669]\tLoss: 2491.9702\n",
      "Training Epoch: 4 [1080/49669]\tLoss: 2570.5125\n",
      "Training Epoch: 4 [1100/49669]\tLoss: 2454.2732\n",
      "Training Epoch: 4 [1120/49669]\tLoss: 2889.9229\n",
      "Training Epoch: 4 [1140/49669]\tLoss: 2845.1472\n",
      "Training Epoch: 4 [1160/49669]\tLoss: 2811.8455\n",
      "Training Epoch: 4 [1180/49669]\tLoss: 2493.4895\n",
      "Training Epoch: 4 [1200/49669]\tLoss: 2806.8306\n",
      "Training Epoch: 4 [1220/49669]\tLoss: 2615.4075\n",
      "Training Epoch: 4 [1240/49669]\tLoss: 2165.7104\n",
      "Training Epoch: 4 [1260/49669]\tLoss: 2682.6694\n",
      "Training Epoch: 4 [1280/49669]\tLoss: 2914.2490\n",
      "Training Epoch: 4 [1300/49669]\tLoss: 2612.2917\n",
      "Training Epoch: 4 [1320/49669]\tLoss: 2533.5889\n",
      "Training Epoch: 4 [1340/49669]\tLoss: 2661.3704\n",
      "Training Epoch: 4 [1360/49669]\tLoss: 2176.6387\n",
      "Training Epoch: 4 [1380/49669]\tLoss: 2789.8845\n",
      "Training Epoch: 4 [1400/49669]\tLoss: 2375.0605\n",
      "Training Epoch: 4 [1420/49669]\tLoss: 2581.2202\n",
      "Training Epoch: 4 [1440/49669]\tLoss: 2558.4441\n",
      "Training Epoch: 4 [1460/49669]\tLoss: 2575.3701\n",
      "Training Epoch: 4 [1480/49669]\tLoss: 2493.8672\n",
      "Training Epoch: 4 [1500/49669]\tLoss: 2604.1641\n",
      "Training Epoch: 4 [1520/49669]\tLoss: 2453.3875\n",
      "Training Epoch: 4 [1540/49669]\tLoss: 2632.2517\n",
      "Training Epoch: 4 [1560/49669]\tLoss: 2696.9316\n",
      "Training Epoch: 4 [1580/49669]\tLoss: 2823.9934\n",
      "Training Epoch: 4 [1600/49669]\tLoss: 2356.4199\n",
      "Training Epoch: 4 [1620/49669]\tLoss: 2716.0308\n",
      "Training Epoch: 4 [1640/49669]\tLoss: 2749.6621\n",
      "Training Epoch: 4 [1660/49669]\tLoss: 2555.8142\n",
      "Training Epoch: 4 [1680/49669]\tLoss: 2770.2302\n",
      "Training Epoch: 4 [1700/49669]\tLoss: 2561.3665\n",
      "Training Epoch: 4 [1720/49669]\tLoss: 2625.6223\n",
      "Training Epoch: 4 [1740/49669]\tLoss: 2612.1487\n",
      "Training Epoch: 4 [1760/49669]\tLoss: 2403.7612\n",
      "Training Epoch: 4 [1780/49669]\tLoss: 2787.6833\n",
      "Training Epoch: 4 [1800/49669]\tLoss: 2138.9651\n",
      "Training Epoch: 4 [1820/49669]\tLoss: 2587.3542\n",
      "Training Epoch: 4 [1840/49669]\tLoss: 2744.0837\n",
      "Training Epoch: 4 [1860/49669]\tLoss: 2384.4639\n",
      "Training Epoch: 4 [1880/49669]\tLoss: 2616.8005\n",
      "Training Epoch: 4 [1900/49669]\tLoss: 2653.4985\n",
      "Training Epoch: 4 [1920/49669]\tLoss: 2307.4229\n",
      "Training Epoch: 4 [1940/49669]\tLoss: 2238.3159\n",
      "Training Epoch: 4 [1960/49669]\tLoss: 2523.8154\n",
      "Training Epoch: 4 [1980/49669]\tLoss: 2600.5232\n",
      "Training Epoch: 4 [2000/49669]\tLoss: 2688.7034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [2020/49669]\tLoss: 2539.1487\n",
      "Training Epoch: 4 [2040/49669]\tLoss: 2469.8738\n",
      "Training Epoch: 4 [2060/49669]\tLoss: 2591.1792\n",
      "Training Epoch: 4 [2080/49669]\tLoss: 2639.6892\n",
      "Training Epoch: 4 [2100/49669]\tLoss: 2808.9731\n",
      "Training Epoch: 4 [2120/49669]\tLoss: 2646.9719\n",
      "Training Epoch: 4 [2140/49669]\tLoss: 2643.2407\n",
      "Training Epoch: 4 [2160/49669]\tLoss: 2430.1323\n",
      "Training Epoch: 4 [2180/49669]\tLoss: 2590.6902\n",
      "Training Epoch: 4 [2200/49669]\tLoss: 2486.5369\n",
      "Training Epoch: 4 [2220/49669]\tLoss: 2527.0544\n",
      "Training Epoch: 4 [2240/49669]\tLoss: 2516.4666\n",
      "Training Epoch: 4 [2260/49669]\tLoss: 2529.0105\n",
      "Training Epoch: 4 [2280/49669]\tLoss: 2657.2654\n",
      "Training Epoch: 4 [2300/49669]\tLoss: 2398.5964\n",
      "Training Epoch: 4 [2320/49669]\tLoss: 2405.0425\n",
      "Training Epoch: 4 [2340/49669]\tLoss: 2470.9871\n",
      "Training Epoch: 4 [2360/49669]\tLoss: 2471.3489\n",
      "Training Epoch: 4 [2380/49669]\tLoss: 2604.7507\n",
      "Training Epoch: 4 [2400/49669]\tLoss: 2565.3682\n",
      "Training Epoch: 4 [2420/49669]\tLoss: 2876.6106\n",
      "Training Epoch: 4 [2440/49669]\tLoss: 2669.4280\n",
      "Training Epoch: 4 [2460/49669]\tLoss: 2277.1797\n",
      "Training Epoch: 4 [2480/49669]\tLoss: 2659.4219\n",
      "Training Epoch: 4 [2500/49669]\tLoss: 2689.9580\n",
      "Training Epoch: 4 [2520/49669]\tLoss: 2467.4326\n",
      "Training Epoch: 4 [2540/49669]\tLoss: 2796.3733\n",
      "Training Epoch: 4 [2560/49669]\tLoss: 2350.3882\n",
      "Training Epoch: 4 [2580/49669]\tLoss: 2672.7437\n",
      "Training Epoch: 4 [2600/49669]\tLoss: 2611.5405\n",
      "Training Epoch: 4 [2620/49669]\tLoss: 2505.2065\n",
      "Training Epoch: 4 [2640/49669]\tLoss: 2587.6560\n",
      "Training Epoch: 4 [2660/49669]\tLoss: 2654.3281\n",
      "Training Epoch: 4 [2680/49669]\tLoss: 2640.3105\n",
      "Training Epoch: 4 [2700/49669]\tLoss: 2752.6011\n",
      "Training Epoch: 4 [2720/49669]\tLoss: 2573.7910\n",
      "Training Epoch: 4 [2740/49669]\tLoss: 2669.8782\n",
      "Training Epoch: 4 [2760/49669]\tLoss: 2544.6550\n",
      "Training Epoch: 4 [2780/49669]\tLoss: 2776.3865\n",
      "Training Epoch: 4 [2800/49669]\tLoss: 2624.7703\n",
      "Training Epoch: 4 [2820/49669]\tLoss: 2712.1589\n",
      "Training Epoch: 4 [2840/49669]\tLoss: 2334.0562\n",
      "Training Epoch: 4 [2860/49669]\tLoss: 2673.7634\n",
      "Training Epoch: 4 [2880/49669]\tLoss: 2721.1401\n",
      "Training Epoch: 4 [2900/49669]\tLoss: 2835.4629\n",
      "Training Epoch: 4 [2920/49669]\tLoss: 2782.1445\n",
      "Training Epoch: 4 [2940/49669]\tLoss: 2780.3281\n",
      "Training Epoch: 4 [2960/49669]\tLoss: 2670.2212\n",
      "Training Epoch: 4 [2980/49669]\tLoss: 2575.9624\n",
      "Training Epoch: 4 [3000/49669]\tLoss: 2690.6260\n",
      "Training Epoch: 4 [3020/49669]\tLoss: 2672.2935\n",
      "Training Epoch: 4 [3040/49669]\tLoss: 2410.2559\n",
      "Training Epoch: 4 [3060/49669]\tLoss: 2637.1721\n",
      "Training Epoch: 4 [3080/49669]\tLoss: 2316.3921\n",
      "Training Epoch: 4 [3100/49669]\tLoss: 2638.9814\n",
      "Training Epoch: 4 [3120/49669]\tLoss: 2624.1838\n",
      "Training Epoch: 4 [3140/49669]\tLoss: 2305.0852\n",
      "Training Epoch: 4 [3160/49669]\tLoss: 2903.5076\n",
      "Training Epoch: 4 [3180/49669]\tLoss: 2677.1580\n",
      "Training Epoch: 4 [3200/49669]\tLoss: 2495.2009\n",
      "Training Epoch: 4 [3220/49669]\tLoss: 2551.8069\n",
      "Training Epoch: 4 [3240/49669]\tLoss: 2344.6946\n",
      "Training Epoch: 4 [3260/49669]\tLoss: 2535.2441\n",
      "Training Epoch: 4 [3280/49669]\tLoss: 2386.6558\n",
      "Training Epoch: 4 [3300/49669]\tLoss: 2548.6846\n",
      "Training Epoch: 4 [3320/49669]\tLoss: 2941.3423\n",
      "Training Epoch: 4 [3340/49669]\tLoss: 2363.5771\n",
      "Training Epoch: 4 [3360/49669]\tLoss: 2270.3455\n",
      "Training Epoch: 4 [3380/49669]\tLoss: 2658.8376\n",
      "Training Epoch: 4 [3400/49669]\tLoss: 2739.7546\n",
      "Training Epoch: 4 [3420/49669]\tLoss: 2624.8503\n",
      "Training Epoch: 4 [3440/49669]\tLoss: 2668.1902\n",
      "Training Epoch: 4 [3460/49669]\tLoss: 2500.6934\n",
      "Training Epoch: 4 [3480/49669]\tLoss: 2582.8406\n",
      "Training Epoch: 4 [3500/49669]\tLoss: 2068.9365\n",
      "Training Epoch: 4 [3520/49669]\tLoss: 2520.6445\n",
      "Training Epoch: 4 [3540/49669]\tLoss: 2497.9912\n",
      "Training Epoch: 4 [3560/49669]\tLoss: 2374.7209\n",
      "Training Epoch: 4 [3580/49669]\tLoss: 2246.7195\n",
      "Training Epoch: 4 [3600/49669]\tLoss: 2009.3723\n",
      "Training Epoch: 4 [3620/49669]\tLoss: 2567.6255\n",
      "Training Epoch: 4 [3640/49669]\tLoss: 2653.2390\n",
      "Training Epoch: 4 [3660/49669]\tLoss: 2373.8127\n",
      "Training Epoch: 4 [3680/49669]\tLoss: 2624.3416\n",
      "Training Epoch: 4 [3700/49669]\tLoss: 2364.3706\n",
      "Training Epoch: 4 [3720/49669]\tLoss: 2731.3396\n",
      "Training Epoch: 4 [3740/49669]\tLoss: 2200.0991\n",
      "Training Epoch: 4 [3760/49669]\tLoss: 2625.1199\n",
      "Training Epoch: 4 [3780/49669]\tLoss: 2419.7104\n",
      "Training Epoch: 4 [3800/49669]\tLoss: 2680.9985\n",
      "Training Epoch: 4 [3820/49669]\tLoss: 2430.3145\n",
      "Training Epoch: 4 [3840/49669]\tLoss: 2782.7485\n",
      "Training Epoch: 4 [3860/49669]\tLoss: 2423.1284\n",
      "Training Epoch: 4 [3880/49669]\tLoss: 2551.6731\n",
      "Training Epoch: 4 [3900/49669]\tLoss: 2527.4170\n",
      "Training Epoch: 4 [3920/49669]\tLoss: 2463.4561\n",
      "Training Epoch: 4 [3940/49669]\tLoss: 2481.9050\n",
      "Training Epoch: 4 [3960/49669]\tLoss: 2387.7241\n",
      "Training Epoch: 4 [3980/49669]\tLoss: 2386.7490\n",
      "Training Epoch: 4 [4000/49669]\tLoss: 2384.2542\n",
      "Training Epoch: 4 [4020/49669]\tLoss: 2339.1099\n",
      "Training Epoch: 4 [4040/49669]\tLoss: 2536.7598\n",
      "Training Epoch: 4 [4060/49669]\tLoss: 2443.0405\n",
      "Training Epoch: 4 [4080/49669]\tLoss: 2577.8101\n",
      "Training Epoch: 4 [4100/49669]\tLoss: 3089.6309\n",
      "Training Epoch: 4 [4120/49669]\tLoss: 2486.8230\n",
      "Training Epoch: 4 [4140/49669]\tLoss: 2584.6743\n",
      "Training Epoch: 4 [4160/49669]\tLoss: 2511.3833\n",
      "Training Epoch: 4 [4180/49669]\tLoss: 2738.5549\n",
      "Training Epoch: 4 [4200/49669]\tLoss: 2808.1047\n",
      "Training Epoch: 4 [4220/49669]\tLoss: 2451.1951\n",
      "Training Epoch: 4 [4240/49669]\tLoss: 2388.6375\n",
      "Training Epoch: 4 [4260/49669]\tLoss: 2322.7502\n",
      "Training Epoch: 4 [4280/49669]\tLoss: 2633.9885\n",
      "Training Epoch: 4 [4300/49669]\tLoss: 2515.7358\n",
      "Training Epoch: 4 [4320/49669]\tLoss: 2840.0405\n",
      "Training Epoch: 4 [4340/49669]\tLoss: 2402.0723\n",
      "Training Epoch: 4 [4360/49669]\tLoss: 2636.2231\n",
      "Training Epoch: 4 [4380/49669]\tLoss: 2627.3398\n",
      "Training Epoch: 4 [4400/49669]\tLoss: 2652.0320\n",
      "Training Epoch: 4 [4420/49669]\tLoss: 2463.7429\n",
      "Training Epoch: 4 [4440/49669]\tLoss: 2406.5007\n",
      "Training Epoch: 4 [4460/49669]\tLoss: 2224.3975\n",
      "Training Epoch: 4 [4480/49669]\tLoss: 2593.4143\n",
      "Training Epoch: 4 [4500/49669]\tLoss: 2465.3870\n",
      "Training Epoch: 4 [4520/49669]\tLoss: 2627.1362\n",
      "Training Epoch: 4 [4540/49669]\tLoss: 2734.4236\n",
      "Training Epoch: 4 [4560/49669]\tLoss: 2496.1755\n",
      "Training Epoch: 4 [4580/49669]\tLoss: 2513.1199\n",
      "Training Epoch: 4 [4600/49669]\tLoss: 2834.6826\n",
      "Training Epoch: 4 [4620/49669]\tLoss: 2697.5972\n",
      "Training Epoch: 4 [4640/49669]\tLoss: 2402.1572\n",
      "Training Epoch: 4 [4660/49669]\tLoss: 2302.9644\n",
      "Training Epoch: 4 [4680/49669]\tLoss: 2404.9763\n",
      "Training Epoch: 4 [4700/49669]\tLoss: 2440.6492\n",
      "Training Epoch: 4 [4720/49669]\tLoss: 2642.0339\n",
      "Training Epoch: 4 [4740/49669]\tLoss: 2546.0134\n",
      "Training Epoch: 4 [4760/49669]\tLoss: 2431.5693\n",
      "Training Epoch: 4 [4780/49669]\tLoss: 2210.4307\n",
      "Training Epoch: 4 [4800/49669]\tLoss: 2790.2158\n",
      "Training Epoch: 4 [4820/49669]\tLoss: 2936.1628\n",
      "Training Epoch: 4 [4840/49669]\tLoss: 2654.5596\n",
      "Training Epoch: 4 [4860/49669]\tLoss: 2664.6321\n",
      "Training Epoch: 4 [4880/49669]\tLoss: 2768.0854\n",
      "Training Epoch: 4 [4900/49669]\tLoss: 2741.2974\n",
      "Training Epoch: 4 [4920/49669]\tLoss: 2224.6477\n",
      "Training Epoch: 4 [4940/49669]\tLoss: 2700.4358\n",
      "Training Epoch: 4 [4960/49669]\tLoss: 2481.0042\n",
      "Training Epoch: 4 [4980/49669]\tLoss: 2582.4700\n",
      "Training Epoch: 4 [5000/49669]\tLoss: 2536.2751\n",
      "Training Epoch: 4 [5020/49669]\tLoss: 2435.4397\n",
      "Training Epoch: 4 [5040/49669]\tLoss: 2729.9038\n",
      "Training Epoch: 4 [5060/49669]\tLoss: 2421.9573\n",
      "Training Epoch: 4 [5080/49669]\tLoss: 2397.4231\n",
      "Training Epoch: 4 [5100/49669]\tLoss: 2823.6907\n",
      "Training Epoch: 4 [5120/49669]\tLoss: 2541.9990\n",
      "Training Epoch: 4 [5140/49669]\tLoss: 2574.7844\n",
      "Training Epoch: 4 [5160/49669]\tLoss: 2733.4233\n",
      "Training Epoch: 4 [5180/49669]\tLoss: 2524.8945\n",
      "Training Epoch: 4 [5200/49669]\tLoss: 2314.5444\n",
      "Training Epoch: 4 [5220/49669]\tLoss: 2402.7300\n",
      "Training Epoch: 4 [5240/49669]\tLoss: 2526.1541\n",
      "Training Epoch: 4 [5260/49669]\tLoss: 2756.9033\n",
      "Training Epoch: 4 [5280/49669]\tLoss: 2946.1797\n",
      "Training Epoch: 4 [5300/49669]\tLoss: 2621.2673\n",
      "Training Epoch: 4 [5320/49669]\tLoss: 2889.9719\n",
      "Training Epoch: 4 [5340/49669]\tLoss: 2503.6794\n",
      "Training Epoch: 4 [5360/49669]\tLoss: 2620.9602\n",
      "Training Epoch: 4 [5380/49669]\tLoss: 2541.3967\n",
      "Training Epoch: 4 [5400/49669]\tLoss: 2324.6697\n",
      "Training Epoch: 4 [5420/49669]\tLoss: 2756.8235\n",
      "Training Epoch: 4 [5440/49669]\tLoss: 2596.0044\n",
      "Training Epoch: 4 [5460/49669]\tLoss: 2798.7112\n",
      "Training Epoch: 4 [5480/49669]\tLoss: 2584.2117\n",
      "Training Epoch: 4 [5500/49669]\tLoss: 2529.8113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [5520/49669]\tLoss: 2514.9001\n",
      "Training Epoch: 4 [5540/49669]\tLoss: 2630.4604\n",
      "Training Epoch: 4 [5560/49669]\tLoss: 2848.5110\n",
      "Training Epoch: 4 [5580/49669]\tLoss: 2795.4778\n",
      "Training Epoch: 4 [5600/49669]\tLoss: 2567.5330\n",
      "Training Epoch: 4 [5620/49669]\tLoss: 2704.0364\n",
      "Training Epoch: 4 [5640/49669]\tLoss: 2443.0117\n",
      "Training Epoch: 4 [5660/49669]\tLoss: 2897.0171\n",
      "Training Epoch: 4 [5680/49669]\tLoss: 2648.8569\n",
      "Training Epoch: 4 [5700/49669]\tLoss: 3043.6943\n",
      "Training Epoch: 4 [5720/49669]\tLoss: 2281.1575\n",
      "Training Epoch: 4 [5740/49669]\tLoss: 2806.4114\n",
      "Training Epoch: 4 [5760/49669]\tLoss: 2666.7485\n",
      "Training Epoch: 4 [5780/49669]\tLoss: 2680.0010\n",
      "Training Epoch: 4 [5800/49669]\tLoss: 2577.6890\n",
      "Training Epoch: 4 [5820/49669]\tLoss: 2574.8447\n",
      "Training Epoch: 4 [5840/49669]\tLoss: 2601.0962\n",
      "Training Epoch: 4 [5860/49669]\tLoss: 2604.9309\n",
      "Training Epoch: 4 [5880/49669]\tLoss: 2194.6223\n",
      "Training Epoch: 4 [5900/49669]\tLoss: 2586.7554\n",
      "Training Epoch: 4 [5920/49669]\tLoss: 2741.9636\n",
      "Training Epoch: 4 [5940/49669]\tLoss: 2768.9966\n",
      "Training Epoch: 4 [5960/49669]\tLoss: 2605.1057\n",
      "Training Epoch: 4 [5980/49669]\tLoss: 2823.6199\n",
      "Training Epoch: 4 [6000/49669]\tLoss: 2303.5425\n",
      "Training Epoch: 4 [6020/49669]\tLoss: 2617.0601\n",
      "Training Epoch: 4 [6040/49669]\tLoss: 2759.6443\n",
      "Training Epoch: 4 [6060/49669]\tLoss: 2457.8679\n",
      "Training Epoch: 4 [6080/49669]\tLoss: 2686.3035\n",
      "Training Epoch: 4 [6100/49669]\tLoss: 2721.9170\n",
      "Training Epoch: 4 [6120/49669]\tLoss: 2673.7117\n",
      "Training Epoch: 4 [6140/49669]\tLoss: 2650.7349\n",
      "Training Epoch: 4 [6160/49669]\tLoss: 2405.2070\n",
      "Training Epoch: 4 [6180/49669]\tLoss: 2681.8923\n",
      "Training Epoch: 4 [6200/49669]\tLoss: 2778.5779\n",
      "Training Epoch: 4 [6220/49669]\tLoss: 2429.7517\n",
      "Training Epoch: 4 [6240/49669]\tLoss: 2488.3687\n",
      "Training Epoch: 4 [6260/49669]\tLoss: 2516.8083\n",
      "Training Epoch: 4 [6280/49669]\tLoss: 2333.9465\n",
      "Training Epoch: 4 [6300/49669]\tLoss: 2754.9934\n",
      "Training Epoch: 4 [6320/49669]\tLoss: 2554.3579\n",
      "Training Epoch: 4 [6340/49669]\tLoss: 2606.3718\n",
      "Training Epoch: 4 [6360/49669]\tLoss: 2103.9753\n",
      "Training Epoch: 4 [6380/49669]\tLoss: 2499.6892\n",
      "Training Epoch: 4 [6400/49669]\tLoss: 2452.0728\n",
      "Training Epoch: 4 [6420/49669]\tLoss: 2826.9275\n",
      "Training Epoch: 4 [6440/49669]\tLoss: 2519.1162\n",
      "Training Epoch: 4 [6460/49669]\tLoss: 2845.6521\n",
      "Training Epoch: 4 [6480/49669]\tLoss: 2427.2070\n",
      "Training Epoch: 4 [6500/49669]\tLoss: 2319.7124\n",
      "Training Epoch: 4 [6520/49669]\tLoss: 2177.0388\n",
      "Training Epoch: 4 [6540/49669]\tLoss: 2832.7754\n",
      "Training Epoch: 4 [6560/49669]\tLoss: 2413.6382\n",
      "Training Epoch: 4 [6580/49669]\tLoss: 2501.3279\n",
      "Training Epoch: 4 [6600/49669]\tLoss: 2446.7583\n",
      "Training Epoch: 4 [6620/49669]\tLoss: 2519.3999\n",
      "Training Epoch: 4 [6640/49669]\tLoss: 2660.9050\n",
      "Training Epoch: 4 [6660/49669]\tLoss: 2238.7312\n",
      "Training Epoch: 4 [6680/49669]\tLoss: 2533.0881\n",
      "Training Epoch: 4 [6700/49669]\tLoss: 2542.0886\n",
      "Training Epoch: 4 [6720/49669]\tLoss: 2045.8877\n",
      "Training Epoch: 4 [6740/49669]\tLoss: 2271.9192\n",
      "Training Epoch: 4 [6760/49669]\tLoss: 2220.9390\n",
      "Training Epoch: 4 [6780/49669]\tLoss: 2440.5747\n",
      "Training Epoch: 4 [6800/49669]\tLoss: 2629.4600\n",
      "Training Epoch: 4 [6820/49669]\tLoss: 2353.0920\n",
      "Training Epoch: 4 [6840/49669]\tLoss: 2647.8113\n",
      "Training Epoch: 4 [6860/49669]\tLoss: 2388.7039\n",
      "Training Epoch: 4 [6880/49669]\tLoss: 2496.1873\n",
      "Training Epoch: 4 [6900/49669]\tLoss: 2682.9771\n",
      "Training Epoch: 4 [6920/49669]\tLoss: 2699.4094\n",
      "Training Epoch: 4 [6940/49669]\tLoss: 2552.7205\n",
      "Training Epoch: 4 [6960/49669]\tLoss: 2619.1096\n",
      "Training Epoch: 4 [6980/49669]\tLoss: 2375.0059\n",
      "Training Epoch: 4 [7000/49669]\tLoss: 2654.1118\n",
      "Training Epoch: 4 [7020/49669]\tLoss: 2416.8372\n",
      "Training Epoch: 4 [7040/49669]\tLoss: 2489.5820\n",
      "Training Epoch: 4 [7060/49669]\tLoss: 2764.7610\n",
      "Training Epoch: 4 [7080/49669]\tLoss: 2524.6926\n",
      "Training Epoch: 4 [7100/49669]\tLoss: 2428.8591\n",
      "Training Epoch: 4 [7120/49669]\tLoss: 2490.6975\n",
      "Training Epoch: 4 [7140/49669]\tLoss: 2620.6697\n",
      "Training Epoch: 4 [7160/49669]\tLoss: 2271.3142\n",
      "Training Epoch: 4 [7180/49669]\tLoss: 2253.8359\n",
      "Training Epoch: 4 [7200/49669]\tLoss: 2648.9453\n",
      "Training Epoch: 4 [7220/49669]\tLoss: 2617.9895\n",
      "Training Epoch: 4 [7240/49669]\tLoss: 2785.5684\n",
      "Training Epoch: 4 [7260/49669]\tLoss: 2714.6787\n",
      "Training Epoch: 4 [7280/49669]\tLoss: 2516.7373\n",
      "Training Epoch: 4 [7300/49669]\tLoss: 2391.0725\n",
      "Training Epoch: 4 [7320/49669]\tLoss: 2664.1252\n",
      "Training Epoch: 4 [7340/49669]\tLoss: 2422.0745\n",
      "Training Epoch: 4 [7360/49669]\tLoss: 2933.8723\n",
      "Training Epoch: 4 [7380/49669]\tLoss: 2821.9917\n",
      "Training Epoch: 4 [7400/49669]\tLoss: 2751.1226\n",
      "Training Epoch: 4 [7420/49669]\tLoss: 2492.7891\n",
      "Training Epoch: 4 [7440/49669]\tLoss: 2681.2754\n",
      "Training Epoch: 4 [7460/49669]\tLoss: 2818.1062\n",
      "Training Epoch: 4 [7480/49669]\tLoss: 2401.4382\n",
      "Training Epoch: 4 [7500/49669]\tLoss: 2764.8489\n",
      "Training Epoch: 4 [7520/49669]\tLoss: 2346.8340\n",
      "Training Epoch: 4 [7540/49669]\tLoss: 2545.0024\n",
      "Training Epoch: 4 [7560/49669]\tLoss: 2354.0811\n",
      "Training Epoch: 4 [7580/49669]\tLoss: 2857.0916\n",
      "Training Epoch: 4 [7600/49669]\tLoss: 2452.1343\n",
      "Training Epoch: 4 [7620/49669]\tLoss: 2328.6484\n",
      "Training Epoch: 4 [7640/49669]\tLoss: 2721.4087\n",
      "Training Epoch: 4 [7660/49669]\tLoss: 2586.6680\n",
      "Training Epoch: 4 [7680/49669]\tLoss: 2492.1772\n",
      "Training Epoch: 4 [7700/49669]\tLoss: 2545.6455\n",
      "Training Epoch: 4 [7720/49669]\tLoss: 2599.2168\n",
      "Training Epoch: 4 [7740/49669]\tLoss: 2675.1228\n",
      "Training Epoch: 4 [7760/49669]\tLoss: 2343.7471\n",
      "Training Epoch: 4 [7780/49669]\tLoss: 2698.3928\n",
      "Training Epoch: 4 [7800/49669]\tLoss: 2509.6404\n",
      "Training Epoch: 4 [7820/49669]\tLoss: 2539.6187\n",
      "Training Epoch: 4 [7840/49669]\tLoss: 2474.8318\n",
      "Training Epoch: 4 [7860/49669]\tLoss: 2737.9014\n",
      "Training Epoch: 4 [7880/49669]\tLoss: 2730.4270\n",
      "Training Epoch: 4 [7900/49669]\tLoss: 2882.6577\n",
      "Training Epoch: 4 [7920/49669]\tLoss: 2471.6067\n",
      "Training Epoch: 4 [7940/49669]\tLoss: 2654.7891\n",
      "Training Epoch: 4 [7960/49669]\tLoss: 2176.1509\n",
      "Training Epoch: 4 [7980/49669]\tLoss: 2262.7837\n",
      "Training Epoch: 4 [8000/49669]\tLoss: 2555.0715\n",
      "Training Epoch: 4 [8020/49669]\tLoss: 2596.7749\n",
      "Training Epoch: 4 [8040/49669]\tLoss: 2319.5242\n",
      "Training Epoch: 4 [8060/49669]\tLoss: 2675.1006\n",
      "Training Epoch: 4 [8080/49669]\tLoss: 2407.9141\n",
      "Training Epoch: 4 [8100/49669]\tLoss: 2741.5762\n",
      "Training Epoch: 4 [8120/49669]\tLoss: 2863.2292\n",
      "Training Epoch: 4 [8140/49669]\tLoss: 2341.8523\n",
      "Training Epoch: 4 [8160/49669]\tLoss: 2832.0544\n",
      "Training Epoch: 4 [8180/49669]\tLoss: 2632.3296\n",
      "Training Epoch: 4 [8200/49669]\tLoss: 2664.8735\n",
      "Training Epoch: 4 [8220/49669]\tLoss: 2519.8552\n",
      "Training Epoch: 4 [8240/49669]\tLoss: 2086.5676\n",
      "Training Epoch: 4 [8260/49669]\tLoss: 2490.7754\n",
      "Training Epoch: 4 [8280/49669]\tLoss: 2722.0461\n",
      "Training Epoch: 4 [8300/49669]\tLoss: 2508.9592\n",
      "Training Epoch: 4 [8320/49669]\tLoss: 2652.8679\n",
      "Training Epoch: 4 [8340/49669]\tLoss: 2466.3809\n",
      "Training Epoch: 4 [8360/49669]\tLoss: 2916.9485\n",
      "Training Epoch: 4 [8380/49669]\tLoss: 2239.2456\n",
      "Training Epoch: 4 [8400/49669]\tLoss: 2566.3167\n",
      "Training Epoch: 4 [8420/49669]\tLoss: 2705.7566\n",
      "Training Epoch: 4 [8440/49669]\tLoss: 2904.4209\n",
      "Training Epoch: 4 [8460/49669]\tLoss: 2777.5444\n",
      "Training Epoch: 4 [8480/49669]\tLoss: 2127.0442\n",
      "Training Epoch: 4 [8500/49669]\tLoss: 2604.2458\n",
      "Training Epoch: 4 [8520/49669]\tLoss: 2375.8765\n",
      "Training Epoch: 4 [8540/49669]\tLoss: 2479.4641\n",
      "Training Epoch: 4 [8560/49669]\tLoss: 2728.7615\n",
      "Training Epoch: 4 [8580/49669]\tLoss: 2556.3665\n",
      "Training Epoch: 4 [8600/49669]\tLoss: 2617.9775\n",
      "Training Epoch: 4 [8620/49669]\tLoss: 2552.5635\n",
      "Training Epoch: 4 [8640/49669]\tLoss: 2370.7288\n",
      "Training Epoch: 4 [8660/49669]\tLoss: 2412.8052\n",
      "Training Epoch: 4 [8680/49669]\tLoss: 2516.6450\n",
      "Training Epoch: 4 [8700/49669]\tLoss: 2803.2805\n",
      "Training Epoch: 4 [8720/49669]\tLoss: 2611.2300\n",
      "Training Epoch: 4 [8740/49669]\tLoss: 2356.7874\n",
      "Training Epoch: 4 [8760/49669]\tLoss: 2581.9199\n",
      "Training Epoch: 4 [8780/49669]\tLoss: 2387.1575\n",
      "Training Epoch: 4 [8800/49669]\tLoss: 2538.4392\n",
      "Training Epoch: 4 [8820/49669]\tLoss: 2766.2148\n",
      "Training Epoch: 4 [8840/49669]\tLoss: 2496.2043\n",
      "Training Epoch: 4 [8860/49669]\tLoss: 2393.1602\n",
      "Training Epoch: 4 [8880/49669]\tLoss: 2591.1328\n",
      "Training Epoch: 4 [8900/49669]\tLoss: 2690.4534\n",
      "Training Epoch: 4 [8920/49669]\tLoss: 2423.9644\n",
      "Training Epoch: 4 [8940/49669]\tLoss: 2383.0559\n",
      "Training Epoch: 4 [8960/49669]\tLoss: 2548.5869\n",
      "Training Epoch: 4 [8980/49669]\tLoss: 2510.4519\n",
      "Training Epoch: 4 [9000/49669]\tLoss: 2276.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [9020/49669]\tLoss: 2426.9719\n",
      "Training Epoch: 4 [9040/49669]\tLoss: 2409.6592\n",
      "Training Epoch: 4 [9060/49669]\tLoss: 2794.5039\n",
      "Training Epoch: 4 [9080/49669]\tLoss: 2458.7739\n",
      "Training Epoch: 4 [9100/49669]\tLoss: 2565.4231\n",
      "Training Epoch: 4 [9120/49669]\tLoss: 2570.5325\n",
      "Training Epoch: 4 [9140/49669]\tLoss: 2687.2554\n",
      "Training Epoch: 4 [9160/49669]\tLoss: 2689.8330\n",
      "Training Epoch: 4 [9180/49669]\tLoss: 2520.8613\n",
      "Training Epoch: 4 [9200/49669]\tLoss: 2465.6174\n",
      "Training Epoch: 4 [9220/49669]\tLoss: 2229.4631\n",
      "Training Epoch: 4 [9240/49669]\tLoss: 2815.0632\n",
      "Training Epoch: 4 [9260/49669]\tLoss: 2366.1411\n",
      "Training Epoch: 4 [9280/49669]\tLoss: 2579.9746\n",
      "Training Epoch: 4 [9300/49669]\tLoss: 2540.1943\n",
      "Training Epoch: 4 [9320/49669]\tLoss: 2526.4028\n",
      "Training Epoch: 4 [9340/49669]\tLoss: 2703.2065\n",
      "Training Epoch: 4 [9360/49669]\tLoss: 2069.0654\n",
      "Training Epoch: 4 [9380/49669]\tLoss: 2524.0969\n",
      "Training Epoch: 4 [9400/49669]\tLoss: 2314.1431\n",
      "Training Epoch: 4 [9420/49669]\tLoss: 2634.6892\n",
      "Training Epoch: 4 [9440/49669]\tLoss: 2258.2258\n",
      "Training Epoch: 4 [9460/49669]\tLoss: 2473.9927\n",
      "Training Epoch: 4 [9480/49669]\tLoss: 2565.6174\n",
      "Training Epoch: 4 [9500/49669]\tLoss: 2760.3662\n",
      "Training Epoch: 4 [9520/49669]\tLoss: 2392.1245\n",
      "Training Epoch: 4 [9540/49669]\tLoss: 2337.6316\n",
      "Training Epoch: 4 [9560/49669]\tLoss: 2782.8445\n",
      "Training Epoch: 4 [9580/49669]\tLoss: 2182.6990\n",
      "Training Epoch: 4 [9600/49669]\tLoss: 2308.5820\n",
      "Training Epoch: 4 [9620/49669]\tLoss: 2470.0488\n",
      "Training Epoch: 4 [9640/49669]\tLoss: 2519.9072\n",
      "Training Epoch: 4 [9660/49669]\tLoss: 2721.4263\n",
      "Training Epoch: 4 [9680/49669]\tLoss: 2470.8804\n",
      "Training Epoch: 4 [9700/49669]\tLoss: 2736.0015\n",
      "Training Epoch: 4 [9720/49669]\tLoss: 2864.4155\n",
      "Training Epoch: 4 [9740/49669]\tLoss: 2384.6211\n",
      "Training Epoch: 4 [9760/49669]\tLoss: 2533.5273\n",
      "Training Epoch: 4 [9780/49669]\tLoss: 2639.8896\n",
      "Training Epoch: 4 [9800/49669]\tLoss: 2451.3989\n",
      "Training Epoch: 4 [9820/49669]\tLoss: 2449.3867\n",
      "Training Epoch: 4 [9840/49669]\tLoss: 2614.7952\n",
      "Training Epoch: 4 [9860/49669]\tLoss: 2330.0527\n",
      "Training Epoch: 4 [9880/49669]\tLoss: 2438.5144\n",
      "Training Epoch: 4 [9900/49669]\tLoss: 2575.4114\n",
      "Training Epoch: 4 [9920/49669]\tLoss: 2598.1284\n",
      "Training Epoch: 4 [9940/49669]\tLoss: 2397.0354\n",
      "Training Epoch: 4 [9960/49669]\tLoss: 2581.3435\n",
      "Training Epoch: 4 [9980/49669]\tLoss: 2608.9846\n",
      "Training Epoch: 4 [10000/49669]\tLoss: 2520.5452\n",
      "Training Epoch: 4 [10020/49669]\tLoss: 2608.0586\n",
      "Training Epoch: 4 [10040/49669]\tLoss: 2578.4412\n",
      "Training Epoch: 4 [10060/49669]\tLoss: 2723.0718\n",
      "Training Epoch: 4 [10080/49669]\tLoss: 2626.2488\n",
      "Training Epoch: 4 [10100/49669]\tLoss: 2333.8025\n",
      "Training Epoch: 4 [10120/49669]\tLoss: 2291.8652\n",
      "Training Epoch: 4 [10140/49669]\tLoss: 2664.7666\n",
      "Training Epoch: 4 [10160/49669]\tLoss: 2480.2373\n",
      "Training Epoch: 4 [10180/49669]\tLoss: 2392.1741\n",
      "Training Epoch: 4 [10200/49669]\tLoss: 2528.6409\n",
      "Training Epoch: 4 [10220/49669]\tLoss: 2686.8818\n",
      "Training Epoch: 4 [10240/49669]\tLoss: 2521.3896\n",
      "Training Epoch: 4 [10260/49669]\tLoss: 2592.3538\n",
      "Training Epoch: 4 [10280/49669]\tLoss: 2554.8025\n",
      "Training Epoch: 4 [10300/49669]\tLoss: 2225.8804\n",
      "Training Epoch: 4 [10320/49669]\tLoss: 2389.5278\n",
      "Training Epoch: 4 [10340/49669]\tLoss: 2539.0076\n",
      "Training Epoch: 4 [10360/49669]\tLoss: 2501.1494\n",
      "Training Epoch: 4 [10380/49669]\tLoss: 2373.8020\n",
      "Training Epoch: 4 [10400/49669]\tLoss: 2622.6208\n",
      "Training Epoch: 4 [10420/49669]\tLoss: 2363.4111\n",
      "Training Epoch: 4 [10440/49669]\tLoss: 2627.1914\n",
      "Training Epoch: 4 [10460/49669]\tLoss: 2410.6069\n",
      "Training Epoch: 4 [10480/49669]\tLoss: 2292.5403\n",
      "Training Epoch: 4 [10500/49669]\tLoss: 2678.1436\n",
      "Training Epoch: 4 [10520/49669]\tLoss: 2630.0271\n",
      "Training Epoch: 4 [10540/49669]\tLoss: 2488.1968\n",
      "Training Epoch: 4 [10560/49669]\tLoss: 2593.2354\n",
      "Training Epoch: 4 [10580/49669]\tLoss: 2426.9978\n",
      "Training Epoch: 4 [10600/49669]\tLoss: 2410.9243\n",
      "Training Epoch: 4 [10620/49669]\tLoss: 2150.6794\n",
      "Training Epoch: 4 [10640/49669]\tLoss: 2527.0823\n",
      "Training Epoch: 4 [10660/49669]\tLoss: 2403.5862\n",
      "Training Epoch: 4 [10680/49669]\tLoss: 2553.4495\n",
      "Training Epoch: 4 [10700/49669]\tLoss: 2387.5156\n",
      "Training Epoch: 4 [10720/49669]\tLoss: 2590.9172\n",
      "Training Epoch: 4 [10740/49669]\tLoss: 2663.7854\n",
      "Training Epoch: 4 [10760/49669]\tLoss: 2464.7637\n",
      "Training Epoch: 4 [10780/49669]\tLoss: 2701.4573\n",
      "Training Epoch: 4 [10800/49669]\tLoss: 2493.3223\n",
      "Training Epoch: 4 [10820/49669]\tLoss: 2437.4985\n",
      "Training Epoch: 4 [10840/49669]\tLoss: 2665.2170\n",
      "Training Epoch: 4 [10860/49669]\tLoss: 2652.1709\n",
      "Training Epoch: 4 [10880/49669]\tLoss: 2201.7261\n",
      "Training Epoch: 4 [10900/49669]\tLoss: 2520.3342\n",
      "Training Epoch: 4 [10920/49669]\tLoss: 2358.8262\n",
      "Training Epoch: 4 [10940/49669]\tLoss: 2739.1389\n",
      "Training Epoch: 4 [10960/49669]\tLoss: 2481.2441\n",
      "Training Epoch: 4 [10980/49669]\tLoss: 2292.9246\n",
      "Training Epoch: 4 [11000/49669]\tLoss: 2574.6375\n",
      "Training Epoch: 4 [11020/49669]\tLoss: 2843.5361\n",
      "Training Epoch: 4 [11040/49669]\tLoss: 2323.5898\n",
      "Training Epoch: 4 [11060/49669]\tLoss: 2486.0962\n",
      "Training Epoch: 4 [11080/49669]\tLoss: 2197.6367\n",
      "Training Epoch: 4 [11100/49669]\tLoss: 2400.4458\n",
      "Training Epoch: 4 [11120/49669]\tLoss: 2272.8728\n",
      "Training Epoch: 4 [11140/49669]\tLoss: 2492.7620\n",
      "Training Epoch: 4 [11160/49669]\tLoss: 2406.3042\n",
      "Training Epoch: 4 [11180/49669]\tLoss: 2394.4106\n",
      "Training Epoch: 4 [11200/49669]\tLoss: 2457.0173\n",
      "Training Epoch: 4 [11220/49669]\tLoss: 2348.2056\n",
      "Training Epoch: 4 [11240/49669]\tLoss: 2529.1174\n",
      "Training Epoch: 4 [11260/49669]\tLoss: 2709.6936\n",
      "Training Epoch: 4 [11280/49669]\tLoss: 2601.7612\n",
      "Training Epoch: 4 [11300/49669]\tLoss: 2208.3452\n",
      "Training Epoch: 4 [11320/49669]\tLoss: 2388.8687\n",
      "Training Epoch: 4 [11340/49669]\tLoss: 2591.6062\n",
      "Training Epoch: 4 [11360/49669]\tLoss: 2450.3743\n",
      "Training Epoch: 4 [11380/49669]\tLoss: 2482.8064\n",
      "Training Epoch: 4 [11400/49669]\tLoss: 2480.9746\n",
      "Training Epoch: 4 [11420/49669]\tLoss: 2141.9854\n",
      "Training Epoch: 4 [11440/49669]\tLoss: 2662.0911\n",
      "Training Epoch: 4 [11460/49669]\tLoss: 2770.5881\n",
      "Training Epoch: 4 [11480/49669]\tLoss: 2541.5522\n",
      "Training Epoch: 4 [11500/49669]\tLoss: 2237.8777\n",
      "Training Epoch: 4 [11520/49669]\tLoss: 2482.5408\n",
      "Training Epoch: 4 [11540/49669]\tLoss: 2595.6685\n",
      "Training Epoch: 4 [11560/49669]\tLoss: 2251.5083\n",
      "Training Epoch: 4 [11580/49669]\tLoss: 2545.7617\n",
      "Training Epoch: 4 [11600/49669]\tLoss: 2389.1191\n",
      "Training Epoch: 4 [11620/49669]\tLoss: 2734.9346\n",
      "Training Epoch: 4 [11640/49669]\tLoss: 2139.8577\n",
      "Training Epoch: 4 [11660/49669]\tLoss: 2483.4324\n",
      "Training Epoch: 4 [11680/49669]\tLoss: 2865.3513\n",
      "Training Epoch: 4 [11700/49669]\tLoss: 2394.6169\n",
      "Training Epoch: 4 [11720/49669]\tLoss: 2751.5588\n",
      "Training Epoch: 4 [11740/49669]\tLoss: 2291.8958\n",
      "Training Epoch: 4 [11760/49669]\tLoss: 2631.0835\n",
      "Training Epoch: 4 [11780/49669]\tLoss: 2596.9666\n",
      "Training Epoch: 4 [11800/49669]\tLoss: 2683.9954\n",
      "Training Epoch: 4 [11820/49669]\tLoss: 2121.3311\n",
      "Training Epoch: 4 [11840/49669]\tLoss: 2719.0559\n",
      "Training Epoch: 4 [11860/49669]\tLoss: 2733.5088\n",
      "Training Epoch: 4 [11880/49669]\tLoss: 2243.9441\n",
      "Training Epoch: 4 [11900/49669]\tLoss: 2339.8940\n",
      "Training Epoch: 4 [11920/49669]\tLoss: 2829.3206\n",
      "Training Epoch: 4 [11940/49669]\tLoss: 2550.0513\n",
      "Training Epoch: 4 [11960/49669]\tLoss: 2670.8899\n",
      "Training Epoch: 4 [11980/49669]\tLoss: 2286.7188\n",
      "Training Epoch: 4 [12000/49669]\tLoss: 2487.9280\n",
      "Training Epoch: 4 [12020/49669]\tLoss: 2590.8838\n",
      "Training Epoch: 4 [12040/49669]\tLoss: 2435.4084\n",
      "Training Epoch: 4 [12060/49669]\tLoss: 2645.0669\n",
      "Training Epoch: 4 [12080/49669]\tLoss: 2477.8284\n",
      "Training Epoch: 4 [12100/49669]\tLoss: 2549.1362\n",
      "Training Epoch: 4 [12120/49669]\tLoss: 2408.2900\n",
      "Training Epoch: 4 [12140/49669]\tLoss: 2321.8442\n",
      "Training Epoch: 4 [12160/49669]\tLoss: 2326.1416\n",
      "Training Epoch: 4 [12180/49669]\tLoss: 2409.6306\n",
      "Training Epoch: 4 [12200/49669]\tLoss: 2726.2703\n",
      "Training Epoch: 4 [12220/49669]\tLoss: 2468.3669\n",
      "Training Epoch: 4 [12240/49669]\tLoss: 2891.0913\n",
      "Training Epoch: 4 [12260/49669]\tLoss: 2663.8413\n",
      "Training Epoch: 4 [12280/49669]\tLoss: 2771.5598\n",
      "Training Epoch: 4 [12300/49669]\tLoss: 2503.1304\n",
      "Training Epoch: 4 [12320/49669]\tLoss: 2510.2029\n",
      "Training Epoch: 4 [12340/49669]\tLoss: 2629.3708\n",
      "Training Epoch: 4 [12360/49669]\tLoss: 2401.7454\n",
      "Training Epoch: 4 [12380/49669]\tLoss: 2377.7759\n",
      "Training Epoch: 4 [12400/49669]\tLoss: 2461.5925\n",
      "Training Epoch: 4 [12420/49669]\tLoss: 2439.1360\n",
      "Training Epoch: 4 [12440/49669]\tLoss: 2646.5376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [12460/49669]\tLoss: 2879.8826\n",
      "Training Epoch: 4 [12480/49669]\tLoss: 2499.7915\n",
      "Training Epoch: 4 [12500/49669]\tLoss: 2393.2312\n",
      "Training Epoch: 4 [12520/49669]\tLoss: 2441.0781\n",
      "Training Epoch: 4 [12540/49669]\tLoss: 2621.0706\n",
      "Training Epoch: 4 [12560/49669]\tLoss: 2451.7986\n",
      "Training Epoch: 4 [12580/49669]\tLoss: 2564.4316\n",
      "Training Epoch: 4 [12600/49669]\tLoss: 2695.3530\n",
      "Training Epoch: 4 [12620/49669]\tLoss: 2702.3518\n",
      "Training Epoch: 4 [12640/49669]\tLoss: 2760.1045\n",
      "Training Epoch: 4 [12660/49669]\tLoss: 2304.5369\n",
      "Training Epoch: 4 [12680/49669]\tLoss: 2493.5432\n",
      "Training Epoch: 4 [12700/49669]\tLoss: 2641.8398\n",
      "Training Epoch: 4 [12720/49669]\tLoss: 2296.9170\n",
      "Training Epoch: 4 [12740/49669]\tLoss: 2577.6697\n",
      "Training Epoch: 4 [12760/49669]\tLoss: 2597.1707\n",
      "Training Epoch: 4 [12780/49669]\tLoss: 2569.5972\n",
      "Training Epoch: 4 [12800/49669]\tLoss: 2463.9219\n",
      "Training Epoch: 4 [12820/49669]\tLoss: 2744.6292\n",
      "Training Epoch: 4 [12840/49669]\tLoss: 2680.9421\n",
      "Training Epoch: 4 [12860/49669]\tLoss: 2010.1826\n",
      "Training Epoch: 4 [12880/49669]\tLoss: 2636.9961\n",
      "Training Epoch: 4 [12900/49669]\tLoss: 2493.5718\n",
      "Training Epoch: 4 [12920/49669]\tLoss: 2039.1073\n",
      "Training Epoch: 4 [12940/49669]\tLoss: 2381.6204\n",
      "Training Epoch: 4 [12960/49669]\tLoss: 2517.6982\n",
      "Training Epoch: 4 [12980/49669]\tLoss: 2073.5925\n",
      "Training Epoch: 4 [13000/49669]\tLoss: 2275.8042\n",
      "Training Epoch: 4 [13020/49669]\tLoss: 2319.1621\n",
      "Training Epoch: 4 [13040/49669]\tLoss: 2506.1138\n",
      "Training Epoch: 4 [13060/49669]\tLoss: 2701.1758\n",
      "Training Epoch: 4 [13080/49669]\tLoss: 2445.0552\n",
      "Training Epoch: 4 [13100/49669]\tLoss: 2599.4680\n",
      "Training Epoch: 4 [13120/49669]\tLoss: 2587.0813\n",
      "Training Epoch: 4 [13140/49669]\tLoss: 2481.6875\n",
      "Training Epoch: 4 [13160/49669]\tLoss: 2876.5769\n",
      "Training Epoch: 4 [13180/49669]\tLoss: 2157.2646\n",
      "Training Epoch: 4 [13200/49669]\tLoss: 2316.4258\n",
      "Training Epoch: 4 [13220/49669]\tLoss: 2631.3728\n",
      "Training Epoch: 4 [13240/49669]\tLoss: 2443.6357\n",
      "Training Epoch: 4 [13260/49669]\tLoss: 2556.2793\n",
      "Training Epoch: 4 [13280/49669]\tLoss: 2316.4871\n",
      "Training Epoch: 4 [13300/49669]\tLoss: 2213.1338\n",
      "Training Epoch: 4 [13320/49669]\tLoss: 2441.3157\n",
      "Training Epoch: 4 [13340/49669]\tLoss: 2544.4514\n",
      "Training Epoch: 4 [13360/49669]\tLoss: 2332.2402\n",
      "Training Epoch: 4 [13380/49669]\tLoss: 2512.7124\n",
      "Training Epoch: 4 [13400/49669]\tLoss: 2325.4124\n",
      "Training Epoch: 4 [13420/49669]\tLoss: 2591.6138\n",
      "Training Epoch: 4 [13440/49669]\tLoss: 1915.2585\n",
      "Training Epoch: 4 [13460/49669]\tLoss: 2778.5742\n",
      "Training Epoch: 4 [13480/49669]\tLoss: 2432.5198\n",
      "Training Epoch: 4 [13500/49669]\tLoss: 2489.4607\n",
      "Training Epoch: 4 [13520/49669]\tLoss: 2345.8972\n",
      "Training Epoch: 4 [13540/49669]\tLoss: 2281.9607\n",
      "Training Epoch: 4 [13560/49669]\tLoss: 2376.1421\n",
      "Training Epoch: 4 [13580/49669]\tLoss: 2626.2278\n",
      "Training Epoch: 4 [13600/49669]\tLoss: 2292.0828\n",
      "Training Epoch: 4 [13620/49669]\tLoss: 2219.3479\n",
      "Training Epoch: 4 [13640/49669]\tLoss: 2530.3865\n",
      "Training Epoch: 4 [13660/49669]\tLoss: 2600.6399\n",
      "Training Epoch: 4 [13680/49669]\tLoss: 2728.8755\n",
      "Training Epoch: 4 [13700/49669]\tLoss: 2623.9282\n",
      "Training Epoch: 4 [13720/49669]\tLoss: 2438.9526\n",
      "Training Epoch: 4 [13740/49669]\tLoss: 2362.0012\n",
      "Training Epoch: 4 [13760/49669]\tLoss: 2313.6709\n",
      "Training Epoch: 4 [13780/49669]\tLoss: 2634.0671\n",
      "Training Epoch: 4 [13800/49669]\tLoss: 2654.3364\n",
      "Training Epoch: 4 [13820/49669]\tLoss: 2408.0935\n",
      "Training Epoch: 4 [13840/49669]\tLoss: 2329.3887\n",
      "Training Epoch: 4 [13860/49669]\tLoss: 2425.8923\n",
      "Training Epoch: 4 [13880/49669]\tLoss: 2482.2056\n",
      "Training Epoch: 4 [13900/49669]\tLoss: 2363.9463\n",
      "Training Epoch: 4 [13920/49669]\tLoss: 2458.8149\n",
      "Training Epoch: 4 [13940/49669]\tLoss: 2703.8469\n",
      "Training Epoch: 4 [13960/49669]\tLoss: 2509.4871\n",
      "Training Epoch: 4 [13980/49669]\tLoss: 2740.8599\n",
      "Training Epoch: 4 [14000/49669]\tLoss: 2630.2898\n",
      "Training Epoch: 4 [14020/49669]\tLoss: 2210.8020\n",
      "Training Epoch: 4 [14040/49669]\tLoss: 2594.2170\n",
      "Training Epoch: 4 [14060/49669]\tLoss: 2511.4177\n",
      "Training Epoch: 4 [14080/49669]\tLoss: 2615.6472\n",
      "Training Epoch: 4 [14100/49669]\tLoss: 2078.2622\n",
      "Training Epoch: 4 [14120/49669]\tLoss: 2827.1221\n",
      "Training Epoch: 4 [14140/49669]\tLoss: 2634.9102\n",
      "Training Epoch: 4 [14160/49669]\tLoss: 2448.0051\n",
      "Training Epoch: 4 [14180/49669]\tLoss: 2417.1426\n",
      "Training Epoch: 4 [14200/49669]\tLoss: 2479.6448\n",
      "Training Epoch: 4 [14220/49669]\tLoss: 2235.4116\n",
      "Training Epoch: 4 [14240/49669]\tLoss: 2393.8872\n",
      "Training Epoch: 4 [14260/49669]\tLoss: 2431.6953\n",
      "Training Epoch: 4 [14280/49669]\tLoss: 2584.6912\n",
      "Training Epoch: 4 [14300/49669]\tLoss: 2528.1299\n",
      "Training Epoch: 4 [14320/49669]\tLoss: 2553.5828\n",
      "Training Epoch: 4 [14340/49669]\tLoss: 2593.9722\n",
      "Training Epoch: 4 [14360/49669]\tLoss: 2294.5549\n",
      "Training Epoch: 4 [14380/49669]\tLoss: 2574.3296\n",
      "Training Epoch: 4 [14400/49669]\tLoss: 2644.0471\n",
      "Training Epoch: 4 [14420/49669]\tLoss: 2424.3538\n",
      "Training Epoch: 4 [14440/49669]\tLoss: 2392.3960\n",
      "Training Epoch: 4 [14460/49669]\tLoss: 2481.7063\n",
      "Training Epoch: 4 [14480/49669]\tLoss: 2513.9524\n",
      "Training Epoch: 4 [14500/49669]\tLoss: 2281.2463\n",
      "Training Epoch: 4 [14520/49669]\tLoss: 2633.2119\n",
      "Training Epoch: 4 [14540/49669]\tLoss: 2436.6074\n",
      "Training Epoch: 4 [14560/49669]\tLoss: 2456.4595\n",
      "Training Epoch: 4 [14580/49669]\tLoss: 2355.4917\n",
      "Training Epoch: 4 [14600/49669]\tLoss: 2587.1436\n",
      "Training Epoch: 4 [14620/49669]\tLoss: 2720.2639\n",
      "Training Epoch: 4 [14640/49669]\tLoss: 2417.0098\n",
      "Training Epoch: 4 [14660/49669]\tLoss: 2667.5881\n",
      "Training Epoch: 4 [14680/49669]\tLoss: 2310.3679\n",
      "Training Epoch: 4 [14700/49669]\tLoss: 2553.6982\n",
      "Training Epoch: 4 [14720/49669]\tLoss: 2322.9326\n",
      "Training Epoch: 4 [14740/49669]\tLoss: 2587.2646\n",
      "Training Epoch: 4 [14760/49669]\tLoss: 2583.5505\n",
      "Training Epoch: 4 [14780/49669]\tLoss: 2346.8076\n",
      "Training Epoch: 4 [14800/49669]\tLoss: 2588.8027\n",
      "Training Epoch: 4 [14820/49669]\tLoss: 2435.4155\n",
      "Training Epoch: 4 [14840/49669]\tLoss: 2580.2373\n",
      "Training Epoch: 4 [14860/49669]\tLoss: 2589.5847\n",
      "Training Epoch: 4 [14880/49669]\tLoss: 2552.9915\n",
      "Training Epoch: 4 [14900/49669]\tLoss: 2152.7666\n",
      "Training Epoch: 4 [14920/49669]\tLoss: 2585.2566\n",
      "Training Epoch: 4 [14940/49669]\tLoss: 2547.8313\n",
      "Training Epoch: 4 [14960/49669]\tLoss: 2547.9150\n",
      "Training Epoch: 4 [14980/49669]\tLoss: 2541.2732\n",
      "Training Epoch: 4 [15000/49669]\tLoss: 2572.3218\n",
      "Training Epoch: 4 [15020/49669]\tLoss: 2442.1714\n",
      "Training Epoch: 4 [15040/49669]\tLoss: 2093.3057\n",
      "Training Epoch: 4 [15060/49669]\tLoss: 2494.8525\n",
      "Training Epoch: 4 [15080/49669]\tLoss: 2314.5662\n",
      "Training Epoch: 4 [15100/49669]\tLoss: 2436.3586\n",
      "Training Epoch: 4 [15120/49669]\tLoss: 2432.9717\n",
      "Training Epoch: 4 [15140/49669]\tLoss: 2265.3560\n",
      "Training Epoch: 4 [15160/49669]\tLoss: 2536.9915\n",
      "Training Epoch: 4 [15180/49669]\tLoss: 2598.7805\n",
      "Training Epoch: 4 [15200/49669]\tLoss: 2538.2153\n",
      "Training Epoch: 4 [15220/49669]\tLoss: 2486.2964\n",
      "Training Epoch: 4 [15240/49669]\tLoss: 2445.0364\n",
      "Training Epoch: 4 [15260/49669]\tLoss: 2457.7341\n",
      "Training Epoch: 4 [15280/49669]\tLoss: 2632.6238\n",
      "Training Epoch: 4 [15300/49669]\tLoss: 2716.9373\n",
      "Training Epoch: 4 [15320/49669]\tLoss: 2325.7749\n",
      "Training Epoch: 4 [15340/49669]\tLoss: 2655.3811\n",
      "Training Epoch: 4 [15360/49669]\tLoss: 2489.1289\n",
      "Training Epoch: 4 [15380/49669]\tLoss: 2464.4082\n",
      "Training Epoch: 4 [15400/49669]\tLoss: 2187.4502\n",
      "Training Epoch: 4 [15420/49669]\tLoss: 2416.9050\n",
      "Training Epoch: 4 [15440/49669]\tLoss: 2646.5984\n",
      "Training Epoch: 4 [15460/49669]\tLoss: 2247.2778\n",
      "Training Epoch: 4 [15480/49669]\tLoss: 2574.6875\n",
      "Training Epoch: 4 [15500/49669]\tLoss: 2578.7888\n",
      "Training Epoch: 4 [15520/49669]\tLoss: 2694.7551\n",
      "Training Epoch: 4 [15540/49669]\tLoss: 2727.3616\n",
      "Training Epoch: 4 [15560/49669]\tLoss: 2332.5239\n",
      "Training Epoch: 4 [15580/49669]\tLoss: 2394.8821\n",
      "Training Epoch: 4 [15600/49669]\tLoss: 2108.2683\n",
      "Training Epoch: 4 [15620/49669]\tLoss: 2570.4519\n",
      "Training Epoch: 4 [15640/49669]\tLoss: 2530.6443\n",
      "Training Epoch: 4 [15660/49669]\tLoss: 2695.0474\n",
      "Training Epoch: 4 [15680/49669]\tLoss: 2612.0723\n",
      "Training Epoch: 4 [15700/49669]\tLoss: 2617.2266\n",
      "Training Epoch: 4 [15720/49669]\tLoss: 2332.0605\n",
      "Training Epoch: 4 [15740/49669]\tLoss: 2598.8596\n",
      "Training Epoch: 4 [15760/49669]\tLoss: 2471.3020\n",
      "Training Epoch: 4 [15780/49669]\tLoss: 2262.4939\n",
      "Training Epoch: 4 [15800/49669]\tLoss: 2386.6584\n",
      "Training Epoch: 4 [15820/49669]\tLoss: 2528.8098\n",
      "Training Epoch: 4 [15840/49669]\tLoss: 2545.3057\n",
      "Training Epoch: 4 [15860/49669]\tLoss: 2235.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [15880/49669]\tLoss: 2399.4045\n",
      "Training Epoch: 4 [15900/49669]\tLoss: 2814.6943\n",
      "Training Epoch: 4 [15920/49669]\tLoss: 2508.5586\n",
      "Training Epoch: 4 [15940/49669]\tLoss: 2627.3440\n",
      "Training Epoch: 4 [15960/49669]\tLoss: 2202.7903\n",
      "Training Epoch: 4 [15980/49669]\tLoss: 2701.8342\n",
      "Training Epoch: 4 [16000/49669]\tLoss: 2620.4617\n",
      "Training Epoch: 4 [16020/49669]\tLoss: 2263.4326\n",
      "Training Epoch: 4 [16040/49669]\tLoss: 2277.8313\n",
      "Training Epoch: 4 [16060/49669]\tLoss: 2742.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [39880/49669]\tLoss: 2258.8884\n",
      "Training Epoch: 5 [39900/49669]\tLoss: 2052.2622\n",
      "Training Epoch: 5 [39920/49669]\tLoss: 2395.2161\n",
      "Training Epoch: 5 [39940/49669]\tLoss: 2269.2053\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
