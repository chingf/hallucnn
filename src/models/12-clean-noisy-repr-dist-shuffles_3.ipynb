{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085eb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e30af",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be7bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "activations_dir = f'{engram_dir}activations_pnet/'\n",
    "pickles_dir = f'{engram_dir}pickles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104d0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2cd337",
   "metadata": {},
   "source": [
    "# Distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc3533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few distance metrics\n",
    "\n",
    "def row_rms(A, B):\n",
    "    \"\"\"\n",
    "    RMS across rows\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    \n",
    "    if len(A.shape) == 1:\n",
    "        stim = A - B\n",
    "        return np.sqrt(np.mean(stim * stim, axis = 0))\n",
    "    \n",
    "    rmses = []\n",
    "    for idx in range(A.shape[0]):\n",
    "        a = A[idx]\n",
    "        b = B[idx]\n",
    "        a, b = a.T, b.T\n",
    "        stim = (a - b)\n",
    "        out = np.sqrt(np.mean(stim * stim, axis = 0))\n",
    "        rmses.append(out)\n",
    "    return np.mean(rmses)\n",
    "\n",
    "def rms(A, B):\n",
    "    \"\"\"\n",
    "    RMS of flattened vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "        \n",
    "    stim = A - B\n",
    "    out = np.sqrt(np.mean(stim * stim))\n",
    "\n",
    "    return out\n",
    "\n",
    "def tanimoto_distance(A, B):\n",
    "    \"\"\"\n",
    "    Tanimoto distance of flattened vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "    \n",
    "    _out = np.dot(A, B)/(np.linalg.norm(A)**2 + np.linalg.norm(B)**2 - np.dot(A,B))\n",
    "    return _out\n",
    "    \n",
    "def cosine_similarity(A, B):\n",
    "    \"\"\"\n",
    "    Cosine similarity of flattened vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "    \n",
    "    if len(A.shape) == 1:\n",
    "        return np.dot(A, B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "    \n",
    "    out = []\n",
    "    for channel in range(n_channels):\n",
    "        a = A[channel]\n",
    "        b = B[channel]\n",
    "        _out = np.dot(a, b)/(np.linalg.norm(a)+np.linalg.norm(b)-np.dot(a,b))\n",
    "        if np.isnan(_out):\n",
    "            print(f'nan: {np.linalg.norm(a)}, {np.linalg.norm(b)}')\n",
    "        out.append(_out)\n",
    "\n",
    "    return np.mean(out)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "def pearsonr_sim(A, B):\n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "    pear, _ = pearsonr(A, B)\n",
    "    return pear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954794a4",
   "metadata": {},
   "source": [
    "# Function to collect correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179c44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_correlations(results, dist_func):\n",
    "    \n",
    "    engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "    checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "    tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "    activations_dir = f'{engram_dir}activations_pnet/'\n",
    "    pickles_dir = f'{engram_dir}pickles/'\n",
    "    DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    labels = np.array(results['label'])\n",
    "    idxs = np.arange(labels.size)\n",
    "    \n",
    "    corr_shuffle = []\n",
    "    timestep = []\n",
    "    layer = []\n",
    "    layers = ['conv1', 'conv2', 'conv3', 'conv4_W', 'fc6_W']\n",
    "    \n",
    "    n_timesteps = 5\n",
    "    for i in idxs:\n",
    "        mean_sub = {}\n",
    "        for t in range(n_timesteps):\n",
    "            for l in layers:\n",
    "                noisy_activ = results[f'{l}_{t}_activations'][i]\n",
    "                clean_activ = results[f'{l}_{t}_clean_activations'][np.random.choice(idxs)]\n",
    "                dist = dist_func(noisy_activ, clean_activ)\n",
    "                corr_shuffle.append(dist)\n",
    "                timestep.append(t)\n",
    "                layer.append(l)\n",
    "                \n",
    "    return corr_shuffle, timestep, layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e118447",
   "metadata": {},
   "source": [
    "# Background noise, incorrect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57144d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is bad practice! But the warnings are real annoying\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7cdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = 'shuffle_pearsonr'\n",
    "dist_func = pearsonr_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb89f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs = ['AudScene', 'Babble8Spkr']\n",
    "snrs = [3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bg in bgs:\n",
    "    for snr in snrs:\n",
    "        results_path = f'{activations_dir}{bg}_snr{int(snr)}.hdf5'\n",
    "        results = h5py.File(results_path, 'r')\n",
    "        corr_shuffle, timestep, layer = eval_correlations(\n",
    "            results, dist_func\n",
    "            )\n",
    "        with open(f'{pickles_dir}{file_prefix}_{bg}_snr{snr}.p', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'Corr Shuffle': corr_shuffle,\n",
    "                'Timestep': timestep,\n",
    "                'Layer': layer\n",
    "                }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa19b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
