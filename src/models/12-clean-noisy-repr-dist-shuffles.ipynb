{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085eb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e30af",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be7bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "activations_dir = f'{engram_dir}activations_pnet_all/'\n",
    "pickles_dir = f'{engram_dir}pickles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104d0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2cd337",
   "metadata": {},
   "source": [
    "# Distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8fb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc3533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few distance metrics\n",
    "\n",
    "def row_rms(A, B):\n",
    "    \"\"\"\n",
    "    RMS across rows\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    \n",
    "    if len(A.shape) == 1:\n",
    "        stim = A - B\n",
    "        return np.sqrt(np.mean(stim * stim, axis = 0))\n",
    "    \n",
    "    rmses = []\n",
    "    for idx in range(A.shape[0]):\n",
    "        a = A[idx]\n",
    "        b = B[idx]\n",
    "        a, b = a.T, b.T\n",
    "        stim = (a - b)\n",
    "        out = np.sqrt(np.mean(stim * stim, axis = 0))\n",
    "        rmses.append(out)\n",
    "    return np.mean(rmses)\n",
    "\n",
    "def rms(A, B):\n",
    "    \"\"\"\n",
    "    RMS of flattened vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "        \n",
    "    stim = A - B\n",
    "    out = np.sqrt(np.mean(stim * stim))\n",
    "\n",
    "    return out\n",
    "\n",
    "def tanimoto_distance(A, B):\n",
    "    \"\"\"\n",
    "    Tanimoto distance of flattened vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "    \n",
    "    _out = np.dot(A, B)/(np.linalg.norm(A)**2 + np.linalg.norm(B)**2 - np.dot(A,B))\n",
    "    return _out\n",
    "    \n",
    "def cosine_similarity(A, B):\n",
    "    \"\"\"\n",
    "    Cosine similarity of flattened vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "    \n",
    "    if len(A.shape) == 1:\n",
    "        return np.dot(A, B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "    \n",
    "    out = []\n",
    "    for channel in range(n_channels):\n",
    "        a = A[channel]\n",
    "        b = B[channel]\n",
    "        _out = np.dot(a, b)/(np.linalg.norm(a)+np.linalg.norm(b)-np.dot(a,b))\n",
    "        if np.isnan(_out):\n",
    "            print(f'nan: {np.linalg.norm(a)}, {np.linalg.norm(b)}')\n",
    "        out.append(_out)\n",
    "\n",
    "    return np.mean(out)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "def pearsonr_sim(A, B):\n",
    "    if torch.is_tensor(A):\n",
    "        A = A.numpy()\n",
    "    if torch.is_tensor(B):\n",
    "        B = B.numpy()\n",
    "    A = A.astype(float)\n",
    "    B = B.astype(float)\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "    pear, _ = pearsonr(A, B)\n",
    "    return pear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954794a4",
   "metadata": {},
   "source": [
    "# Function to collect correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179c44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_correlations(results, dist_func, undead_units):\n",
    "    labels = np.array(results['label'])\n",
    "    idxs = np.arange(labels.size)\n",
    "    \n",
    "    popln_shuffle = []\n",
    "    popln_shuffle_undead = []\n",
    "    popln_timestep = []\n",
    "    popln_layer = []\n",
    "    \n",
    "    unit_shuffle = []\n",
    "    unit_shuffle_alive = []\n",
    "    unit_timestep = []\n",
    "    unit_layer = []\n",
    "    layers = ['conv1', 'conv2', 'conv3', 'conv4_W', 'conv5_W', 'fc6_W']\n",
    "    \n",
    "    n_timesteps = 5\n",
    "    for t in range(n_timesteps):\n",
    "        for l in layers:\n",
    "            unit_noisy_response = []\n",
    "            unit_clean_response = []\n",
    "            undead_units_l = undead_units[l]\n",
    "            for i in idxs:\n",
    "                noisy_activ = results[f'{l}_{t}_activations'][i]\n",
    "                clean_activ = results[f'{l}_{t}_clean_activations'][np.random.choice(idxs)]\n",
    "                noisy_activ = noisy_activ.flatten()\n",
    "                clean_activ = clean_activ.flatten()\n",
    "                unit_noisy_response.append(noisy_activ)\n",
    "                unit_clean_response.append(clean_activ)\n",
    "\n",
    "                # Popln Corr\n",
    "                dist = dist_func(noisy_activ, clean_activ)\n",
    "                dist_undead = dist_func(\n",
    "                    noisy_activ[undead_units_l],\n",
    "                    clean_activ[undead_units_l]\n",
    "                    )\n",
    "                popln_shuffle.append(dist)\n",
    "                popln_shuffle_undead.append(dist_undead)\n",
    "                popln_timestep.append(t)\n",
    "                popln_layer.append(l)\n",
    "\n",
    "            # Popln Corr\n",
    "            unit_noisy_response = np.array(unit_noisy_response)\n",
    "            unit_clean_response = np.array(unit_clean_response)\n",
    "            for unit in np.arange(unit_noisy_response.shape[1]):               \n",
    "                dist = dist_func(\n",
    "                    unit_noisy_response[:,unit],\n",
    "                    unit_clean_response[:,unit]\n",
    "                    )\n",
    "                unit_shuffle.append(dist)\n",
    "                unit_shuffle_alive.append(undead_units_l[unit])\n",
    "                unit_timestep.append(t)\n",
    "                unit_layer.append(l)\n",
    "        \n",
    "    results = {\n",
    "        'popln_shuffle': popln_shuffle,\n",
    "        'popln_shuffle_undead': popln_shuffle_undead,\n",
    "        'popln_timestep': popln_timestep,\n",
    "        'popln_layer': popln_layer,\n",
    "        'unit_shuffle': unit_shuffle,\n",
    "        'unit_shuffle_alive': unit_shuffle_alive,\n",
    "        'unit_timestep': unit_timestep,\n",
    "        'unit_layer': unit_layer\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e118447",
   "metadata": {},
   "source": [
    "# Run and save shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57144d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is bad practice! But the warnings are real annoying\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7cdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = 'shuffle_pearsonr'\n",
    "dist_func = pearsonr_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb89f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs = ['AudScene'] #, 'AudScene', 'Babble8Spkr']\n",
    "snrs = [3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd24af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units_per_layer = {\n",
    "    'conv1': (96, 55, 134), 'conv2': (256, 14, 34),\n",
    "    'conv3': (512, 7, 17), 'conv4_W': (1024, 7, 17),\n",
    "    'conv5_W': (512, 7, 17), 'fc6_W': (4096,)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b329e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pickles_dir}dead_units.p', 'rb') as f:\n",
    "    undead_units = pickle.load(f)['undead_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee1914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bg in bgs:\n",
    "    for snr in snrs:\n",
    "        results_path = f'{activations_dir}{bg}_snr{int(snr)}.hdf5'\n",
    "        results = h5py.File(results_path, 'r')\n",
    "        results = eval_correlations(\n",
    "            results, dist_func, undead_units\n",
    "            )\n",
    "        with open(f'{pickles_dir}{file_prefix}_{bg}_snr{snr}.p', 'wb') as f:\n",
    "            pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a184a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
