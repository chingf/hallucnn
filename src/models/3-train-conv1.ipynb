{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "lr = 1E-4\n",
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'\n",
    "checkpoints_dir = f'{engram_dir}hcnn/checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}hcnn/tensorboard/'\n",
    "datafile = f'{engram_dir}seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_conv1 import PBranchedNetwork_Conv1SeparateHP\n",
    "PNetClass = PBranchedNetwork_Conv1SeparateHP\n",
    "pnet_name = 'conv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 21 11:45:56 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1F:00.0 Off |                  N/A |\r\n",
      "| 27%   25C    P8    18W / 250W |      3MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PNetClass(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBranchedNetwork_Conv1SeparateHP(\n",
       "  (backbone): BranchedNetwork(\n",
       "    (speech_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_W): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_W): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_W): FlattenPoolLayer()\n",
       "      (fc6_W): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_W): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=531, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (genre_branch): Sequential(\n",
       "      (conv1): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1, 96, kernel_size=(6, 14), stride=(3, 3), padding=(2, 6))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm1): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool1): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv2): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 2))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (rnorm2): LRNorm(\n",
       "        (block): LocalResponseNorm(5, alpha=0.005, beta=0.75, k=1.0)\n",
       "      )\n",
       "      (pool2): PoolLayer(\n",
       "        (block): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (conv3): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv4_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv5_G): ConvLayer(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (pool5_G): PoolLayer(\n",
       "        (block): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "      (pool5_flat_G): FlattenPoolLayer()\n",
       "      (fc6_G): FullyConnected(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fctop_G): FullyConnected(\n",
       "        (block): Linear(in_features=4096, out_features=42, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pcoder1): PCoderN(\n",
       "    (pmodule): Sequential(\n",
       "      (0): Upsample(scale_factor=(2.981818181818182, 2.985074626865672), mode=bilinear)\n",
       "      (1): ConvTranspose2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in pnet.named_parameters():\n",
    "    if 'genre_branch' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CleanSoundsDataset(datafile)\n",
    "n_train = int(len(full_dataset)*0.9)\n",
    "train_dataset = Subset(full_dataset, np.arange(n_train))\n",
    "eval_dataset = Subset(full_dataset, np.arange(n_train, len(full_dataset)))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, pnet_name + '-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"{pnet_name}\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [50/36450]\tLoss: 168287.4219\n",
      "Training Epoch: 1 [100/36450]\tLoss: 159777.7188\n",
      "Training Epoch: 1 [150/36450]\tLoss: 179423.4688\n",
      "Training Epoch: 1 [200/36450]\tLoss: 164653.5469\n",
      "Training Epoch: 1 [250/36450]\tLoss: 173087.4375\n",
      "Training Epoch: 1 [300/36450]\tLoss: 148132.1406\n",
      "Training Epoch: 1 [350/36450]\tLoss: 163822.8125\n",
      "Training Epoch: 1 [400/36450]\tLoss: 148009.9062\n",
      "Training Epoch: 1 [450/36450]\tLoss: 151031.3281\n",
      "Training Epoch: 1 [500/36450]\tLoss: 160897.9844\n",
      "Training Epoch: 1 [550/36450]\tLoss: 147012.3125\n",
      "Training Epoch: 1 [600/36450]\tLoss: 137536.6250\n",
      "Training Epoch: 1 [650/36450]\tLoss: 141199.2344\n",
      "Training Epoch: 1 [700/36450]\tLoss: 135750.0000\n",
      "Training Epoch: 1 [750/36450]\tLoss: 138057.7812\n",
      "Training Epoch: 1 [800/36450]\tLoss: 131865.4531\n",
      "Training Epoch: 1 [850/36450]\tLoss: 125987.0156\n",
      "Training Epoch: 1 [900/36450]\tLoss: 132285.9531\n",
      "Training Epoch: 1 [950/36450]\tLoss: 120428.0156\n",
      "Training Epoch: 1 [1000/36450]\tLoss: 128298.9219\n",
      "Training Epoch: 1 [1050/36450]\tLoss: 131673.4531\n",
      "Training Epoch: 1 [1100/36450]\tLoss: 124236.2109\n",
      "Training Epoch: 1 [1150/36450]\tLoss: 123408.5469\n",
      "Training Epoch: 1 [1200/36450]\tLoss: 120056.7109\n",
      "Training Epoch: 1 [1250/36450]\tLoss: 114289.3047\n",
      "Training Epoch: 1 [1300/36450]\tLoss: 119386.4922\n",
      "Training Epoch: 1 [1350/36450]\tLoss: 112619.5547\n",
      "Training Epoch: 1 [1400/36450]\tLoss: 99721.5781\n",
      "Training Epoch: 1 [1450/36450]\tLoss: 114545.2812\n",
      "Training Epoch: 1 [1500/36450]\tLoss: 97541.9688\n",
      "Training Epoch: 1 [1550/36450]\tLoss: 98629.4844\n",
      "Training Epoch: 1 [1600/36450]\tLoss: 100990.3672\n",
      "Training Epoch: 1 [1650/36450]\tLoss: 97953.2422\n",
      "Training Epoch: 1 [1700/36450]\tLoss: 98127.9062\n",
      "Training Epoch: 1 [1750/36450]\tLoss: 93832.1094\n",
      "Training Epoch: 1 [1800/36450]\tLoss: 88641.2969\n",
      "Training Epoch: 1 [1850/36450]\tLoss: 86135.0859\n",
      "Training Epoch: 1 [1900/36450]\tLoss: 86352.7344\n",
      "Training Epoch: 1 [1950/36450]\tLoss: 86649.3672\n",
      "Training Epoch: 1 [2000/36450]\tLoss: 90760.3984\n",
      "Training Epoch: 1 [2050/36450]\tLoss: 82021.6094\n",
      "Training Epoch: 1 [2100/36450]\tLoss: 81347.5938\n",
      "Training Epoch: 1 [2150/36450]\tLoss: 83294.2188\n",
      "Training Epoch: 1 [2200/36450]\tLoss: 79163.1406\n",
      "Training Epoch: 1 [2250/36450]\tLoss: 79800.9922\n",
      "Training Epoch: 1 [2300/36450]\tLoss: 73860.5312\n",
      "Training Epoch: 1 [2350/36450]\tLoss: 77474.1250\n",
      "Training Epoch: 1 [2400/36450]\tLoss: 73058.0703\n",
      "Training Epoch: 1 [2450/36450]\tLoss: 74037.6172\n",
      "Training Epoch: 1 [2500/36450]\tLoss: 68212.7109\n",
      "Training Epoch: 1 [2550/36450]\tLoss: 66100.7812\n",
      "Training Epoch: 1 [2600/36450]\tLoss: 66897.0156\n",
      "Training Epoch: 1 [2650/36450]\tLoss: 66048.5312\n",
      "Training Epoch: 1 [2700/36450]\tLoss: 64054.7539\n",
      "Training Epoch: 1 [2750/36450]\tLoss: 63431.3555\n",
      "Training Epoch: 1 [2800/36450]\tLoss: 59035.5664\n",
      "Training Epoch: 1 [2850/36450]\tLoss: 56589.1797\n",
      "Training Epoch: 1 [2900/36450]\tLoss: 58897.2500\n",
      "Training Epoch: 1 [2950/36450]\tLoss: 53711.8320\n",
      "Training Epoch: 1 [3000/36450]\tLoss: 57691.3242\n",
      "Training Epoch: 1 [3050/36450]\tLoss: 54259.5430\n",
      "Training Epoch: 1 [3100/36450]\tLoss: 55100.7539\n",
      "Training Epoch: 1 [3150/36450]\tLoss: 49759.0430\n",
      "Training Epoch: 1 [3200/36450]\tLoss: 50737.2891\n",
      "Training Epoch: 1 [3250/36450]\tLoss: 51620.0352\n",
      "Training Epoch: 1 [3300/36450]\tLoss: 48788.6094\n",
      "Training Epoch: 1 [3350/36450]\tLoss: 48945.8867\n",
      "Training Epoch: 1 [3400/36450]\tLoss: 47108.7578\n",
      "Training Epoch: 1 [3450/36450]\tLoss: 48992.6758\n",
      "Training Epoch: 1 [3500/36450]\tLoss: 41088.1328\n",
      "Training Epoch: 1 [3550/36450]\tLoss: 38259.1250\n",
      "Training Epoch: 1 [3600/36450]\tLoss: 44005.1680\n",
      "Training Epoch: 1 [3650/36450]\tLoss: 41523.8672\n",
      "Training Epoch: 1 [3700/36450]\tLoss: 38035.9258\n",
      "Training Epoch: 1 [3750/36450]\tLoss: 38504.2734\n",
      "Training Epoch: 1 [3800/36450]\tLoss: 38479.6328\n",
      "Training Epoch: 1 [3850/36450]\tLoss: 36744.9570\n",
      "Training Epoch: 1 [3900/36450]\tLoss: 35201.2344\n",
      "Training Epoch: 1 [3950/36450]\tLoss: 34928.0898\n",
      "Training Epoch: 1 [4000/36450]\tLoss: 36443.3555\n",
      "Training Epoch: 1 [4050/36450]\tLoss: 36459.7070\n",
      "Training Epoch: 1 [4100/36450]\tLoss: 37424.9102\n",
      "Training Epoch: 1 [4150/36450]\tLoss: 34546.5391\n",
      "Training Epoch: 1 [4200/36450]\tLoss: 30546.9414\n",
      "Training Epoch: 1 [4250/36450]\tLoss: 30434.2305\n",
      "Training Epoch: 1 [4300/36450]\tLoss: 29908.4590\n",
      "Training Epoch: 1 [4350/36450]\tLoss: 30752.0449\n",
      "Training Epoch: 1 [4400/36450]\tLoss: 29353.0449\n",
      "Training Epoch: 1 [4450/36450]\tLoss: 28143.2070\n",
      "Training Epoch: 1 [4500/36450]\tLoss: 28393.1875\n",
      "Training Epoch: 1 [4550/36450]\tLoss: 25701.3418\n",
      "Training Epoch: 1 [4600/36450]\tLoss: 27964.5645\n",
      "Training Epoch: 1 [4650/36450]\tLoss: 24734.5625\n",
      "Training Epoch: 1 [4700/36450]\tLoss: 25262.3711\n",
      "Training Epoch: 1 [4750/36450]\tLoss: 22983.1562\n",
      "Training Epoch: 1 [4800/36450]\tLoss: 23397.8809\n",
      "Training Epoch: 1 [4850/36450]\tLoss: 22973.2656\n",
      "Training Epoch: 1 [4900/36450]\tLoss: 22910.8574\n",
      "Training Epoch: 1 [4950/36450]\tLoss: 21765.8457\n",
      "Training Epoch: 1 [5000/36450]\tLoss: 20174.1758\n",
      "Training Epoch: 1 [5050/36450]\tLoss: 21273.7852\n",
      "Training Epoch: 1 [5100/36450]\tLoss: 19098.7637\n",
      "Training Epoch: 1 [5150/36450]\tLoss: 21056.7852\n",
      "Training Epoch: 1 [5200/36450]\tLoss: 18733.6152\n",
      "Training Epoch: 1 [5250/36450]\tLoss: 19224.4922\n",
      "Training Epoch: 1 [5300/36450]\tLoss: 19072.6914\n",
      "Training Epoch: 1 [5350/36450]\tLoss: 19392.8027\n",
      "Training Epoch: 1 [5400/36450]\tLoss: 19803.2480\n",
      "Training Epoch: 1 [5450/36450]\tLoss: 16997.3555\n",
      "Training Epoch: 1 [5500/36450]\tLoss: 16844.7754\n",
      "Training Epoch: 1 [5550/36450]\tLoss: 15864.9561\n",
      "Training Epoch: 1 [5600/36450]\tLoss: 16149.4160\n",
      "Training Epoch: 1 [5650/36450]\tLoss: 16490.9355\n",
      "Training Epoch: 1 [5700/36450]\tLoss: 16657.5879\n",
      "Training Epoch: 1 [5750/36450]\tLoss: 14953.2324\n",
      "Training Epoch: 1 [5800/36450]\tLoss: 14168.2021\n",
      "Training Epoch: 1 [5850/36450]\tLoss: 13926.6055\n",
      "Training Epoch: 1 [5900/36450]\tLoss: 13842.6484\n",
      "Training Epoch: 1 [5950/36450]\tLoss: 13971.0889\n",
      "Training Epoch: 1 [6000/36450]\tLoss: 13757.2402\n",
      "Training Epoch: 1 [6050/36450]\tLoss: 13013.6230\n",
      "Training Epoch: 1 [6100/36450]\tLoss: 12250.7666\n",
      "Training Epoch: 1 [6150/36450]\tLoss: 12202.3477\n",
      "Training Epoch: 1 [6200/36450]\tLoss: 12080.8535\n",
      "Training Epoch: 1 [6250/36450]\tLoss: 11914.5127\n",
      "Training Epoch: 1 [6300/36450]\tLoss: 11168.4346\n",
      "Training Epoch: 1 [6350/36450]\tLoss: 11359.9775\n",
      "Training Epoch: 1 [6400/36450]\tLoss: 10806.0527\n",
      "Training Epoch: 1 [6450/36450]\tLoss: 10717.0469\n",
      "Training Epoch: 1 [6500/36450]\tLoss: 10381.4238\n",
      "Training Epoch: 1 [6550/36450]\tLoss: 9960.9199\n",
      "Training Epoch: 1 [6600/36450]\tLoss: 10082.8447\n",
      "Training Epoch: 1 [6650/36450]\tLoss: 9547.3057\n",
      "Training Epoch: 1 [6700/36450]\tLoss: 9249.8330\n",
      "Training Epoch: 1 [6750/36450]\tLoss: 9307.2246\n",
      "Training Epoch: 1 [6800/36450]\tLoss: 9225.7646\n",
      "Training Epoch: 1 [6850/36450]\tLoss: 8571.4443\n",
      "Training Epoch: 1 [6900/36450]\tLoss: 8517.6113\n",
      "Training Epoch: 1 [6950/36450]\tLoss: 8341.5293\n",
      "Training Epoch: 1 [7000/36450]\tLoss: 7141.6885\n",
      "Training Epoch: 1 [7050/36450]\tLoss: 7516.5635\n",
      "Training Epoch: 1 [7100/36450]\tLoss: 7944.1597\n",
      "Training Epoch: 1 [7150/36450]\tLoss: 7891.5811\n",
      "Training Epoch: 1 [7200/36450]\tLoss: 7747.0215\n",
      "Training Epoch: 1 [7250/36450]\tLoss: 7173.2163\n",
      "Training Epoch: 1 [7300/36450]\tLoss: 7072.6714\n",
      "Training Epoch: 1 [7350/36450]\tLoss: 6979.1694\n",
      "Training Epoch: 1 [7400/36450]\tLoss: 6538.1079\n",
      "Training Epoch: 1 [7450/36450]\tLoss: 6787.0498\n",
      "Training Epoch: 1 [7500/36450]\tLoss: 6233.1548\n",
      "Training Epoch: 1 [7550/36450]\tLoss: 6115.9863\n",
      "Training Epoch: 1 [7600/36450]\tLoss: 6386.1963\n",
      "Training Epoch: 1 [7650/36450]\tLoss: 5866.6655\n",
      "Training Epoch: 1 [7700/36450]\tLoss: 6160.4072\n",
      "Training Epoch: 1 [7750/36450]\tLoss: 6206.2607\n",
      "Training Epoch: 1 [7800/36450]\tLoss: 5774.8706\n",
      "Training Epoch: 1 [7850/36450]\tLoss: 5141.8770\n",
      "Training Epoch: 1 [7900/36450]\tLoss: 5491.4297\n",
      "Training Epoch: 1 [7950/36450]\tLoss: 5434.2539\n",
      "Training Epoch: 1 [8000/36450]\tLoss: 5465.8472\n",
      "Training Epoch: 1 [8050/36450]\tLoss: 5278.9497\n",
      "Training Epoch: 1 [8100/36450]\tLoss: 5286.2178\n",
      "Training Epoch: 1 [8150/36450]\tLoss: 4860.7632\n",
      "Training Epoch: 1 [8200/36450]\tLoss: 4945.7109\n",
      "Training Epoch: 1 [8250/36450]\tLoss: 4724.4160\n",
      "Training Epoch: 1 [8300/36450]\tLoss: 4761.1538\n",
      "Training Epoch: 1 [8350/36450]\tLoss: 4408.9619\n",
      "Training Epoch: 1 [8400/36450]\tLoss: 4682.6060\n",
      "Training Epoch: 1 [8450/36450]\tLoss: 4452.5156\n",
      "Training Epoch: 1 [8500/36450]\tLoss: 4532.1084\n",
      "Training Epoch: 1 [8550/36450]\tLoss: 4283.2280\n",
      "Training Epoch: 1 [8600/36450]\tLoss: 4051.0608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [8650/36450]\tLoss: 4123.2686\n",
      "Training Epoch: 1 [8700/36450]\tLoss: 4065.0554\n",
      "Training Epoch: 1 [8750/36450]\tLoss: 4046.7588\n",
      "Training Epoch: 1 [8800/36450]\tLoss: 4258.7891\n",
      "Training Epoch: 1 [8850/36450]\tLoss: 3939.5586\n",
      "Training Epoch: 1 [8900/36450]\tLoss: 4022.9951\n",
      "Training Epoch: 1 [8950/36450]\tLoss: 3689.8604\n",
      "Training Epoch: 1 [9000/36450]\tLoss: 3787.5610\n",
      "Training Epoch: 1 [9050/36450]\tLoss: 3716.4819\n",
      "Training Epoch: 1 [9100/36450]\tLoss: 3557.8987\n",
      "Training Epoch: 1 [9150/36450]\tLoss: 3510.5942\n",
      "Training Epoch: 1 [9200/36450]\tLoss: 3514.7483\n",
      "Training Epoch: 1 [9250/36450]\tLoss: 3357.9426\n",
      "Training Epoch: 1 [9300/36450]\tLoss: 3454.7314\n",
      "Training Epoch: 1 [9350/36450]\tLoss: 3335.9893\n",
      "Training Epoch: 1 [9400/36450]\tLoss: 3304.8359\n",
      "Training Epoch: 1 [9450/36450]\tLoss: 3211.7666\n",
      "Training Epoch: 1 [9500/36450]\tLoss: 3233.5747\n",
      "Training Epoch: 1 [9550/36450]\tLoss: 3247.4304\n",
      "Training Epoch: 1 [9600/36450]\tLoss: 3205.2983\n",
      "Training Epoch: 1 [9650/36450]\tLoss: 3104.6108\n",
      "Training Epoch: 1 [9700/36450]\tLoss: 2956.7820\n",
      "Training Epoch: 1 [9750/36450]\tLoss: 3074.8242\n",
      "Training Epoch: 1 [9800/36450]\tLoss: 2915.1240\n",
      "Training Epoch: 1 [9850/36450]\tLoss: 2929.4299\n",
      "Training Epoch: 1 [9900/36450]\tLoss: 3013.6982\n",
      "Training Epoch: 1 [9950/36450]\tLoss: 2719.3894\n",
      "Training Epoch: 1 [10000/36450]\tLoss: 2859.3977\n",
      "Training Epoch: 1 [10050/36450]\tLoss: 2837.9011\n",
      "Training Epoch: 1 [10100/36450]\tLoss: 2828.0620\n",
      "Training Epoch: 1 [10150/36450]\tLoss: 2757.0691\n",
      "Training Epoch: 1 [10200/36450]\tLoss: 2622.3474\n",
      "Training Epoch: 1 [10250/36450]\tLoss: 2648.1353\n",
      "Training Epoch: 1 [10300/36450]\tLoss: 2773.0500\n",
      "Training Epoch: 1 [10350/36450]\tLoss: 2622.3840\n",
      "Training Epoch: 1 [10400/36450]\tLoss: 2652.9885\n",
      "Training Epoch: 1 [10450/36450]\tLoss: 2637.7942\n",
      "Training Epoch: 1 [10500/36450]\tLoss: 2589.8015\n",
      "Training Epoch: 1 [10550/36450]\tLoss: 2469.7915\n",
      "Training Epoch: 1 [10600/36450]\tLoss: 2478.5583\n",
      "Training Epoch: 1 [10650/36450]\tLoss: 2498.5386\n",
      "Training Epoch: 1 [10700/36450]\tLoss: 2580.1719\n",
      "Training Epoch: 1 [10750/36450]\tLoss: 2533.2842\n",
      "Training Epoch: 1 [10800/36450]\tLoss: 2559.2412\n",
      "Training Epoch: 1 [10850/36450]\tLoss: 2388.3518\n",
      "Training Epoch: 1 [10900/36450]\tLoss: 2466.9077\n",
      "Training Epoch: 1 [10950/36450]\tLoss: 2399.1316\n",
      "Training Epoch: 1 [11000/36450]\tLoss: 2254.8206\n",
      "Training Epoch: 1 [11050/36450]\tLoss: 2372.9319\n",
      "Training Epoch: 1 [11100/36450]\tLoss: 2285.7578\n",
      "Training Epoch: 1 [11150/36450]\tLoss: 2295.4736\n",
      "Training Epoch: 1 [11200/36450]\tLoss: 2290.2778\n",
      "Training Epoch: 1 [11250/36450]\tLoss: 2205.6589\n",
      "Training Epoch: 1 [11300/36450]\tLoss: 2390.0820\n",
      "Training Epoch: 1 [11350/36450]\tLoss: 2368.2175\n",
      "Training Epoch: 1 [11400/36450]\tLoss: 2212.8022\n",
      "Training Epoch: 1 [11450/36450]\tLoss: 2270.1348\n",
      "Training Epoch: 1 [11500/36450]\tLoss: 2151.7837\n",
      "Training Epoch: 1 [11550/36450]\tLoss: 2152.1450\n",
      "Training Epoch: 1 [11600/36450]\tLoss: 2169.9504\n",
      "Training Epoch: 1 [11650/36450]\tLoss: 2145.9387\n",
      "Training Epoch: 1 [11700/36450]\tLoss: 2258.7324\n",
      "Training Epoch: 1 [11750/36450]\tLoss: 2122.6934\n",
      "Training Epoch: 1 [11800/36450]\tLoss: 2091.4009\n",
      "Training Epoch: 1 [11850/36450]\tLoss: 2192.7209\n",
      "Training Epoch: 1 [11900/36450]\tLoss: 2107.6191\n",
      "Training Epoch: 1 [11950/36450]\tLoss: 2091.6304\n",
      "Training Epoch: 1 [12000/36450]\tLoss: 2011.5386\n",
      "Training Epoch: 1 [12050/36450]\tLoss: 2023.2317\n",
      "Training Epoch: 1 [12100/36450]\tLoss: 2052.1494\n",
      "Training Epoch: 1 [12150/36450]\tLoss: 2140.8452\n",
      "Training Epoch: 1 [12200/36450]\tLoss: 1991.0547\n",
      "Training Epoch: 1 [12250/36450]\tLoss: 1918.3495\n",
      "Training Epoch: 1 [12300/36450]\tLoss: 2075.7510\n",
      "Training Epoch: 1 [12350/36450]\tLoss: 1917.9592\n",
      "Training Epoch: 1 [12400/36450]\tLoss: 2040.5760\n",
      "Training Epoch: 1 [12450/36450]\tLoss: 2043.2670\n",
      "Training Epoch: 1 [12500/36450]\tLoss: 1984.9875\n",
      "Training Epoch: 1 [12550/36450]\tLoss: 1991.7411\n",
      "Training Epoch: 1 [12600/36450]\tLoss: 1946.8253\n",
      "Training Epoch: 1 [12650/36450]\tLoss: 2011.6788\n",
      "Training Epoch: 1 [12700/36450]\tLoss: 1959.8480\n",
      "Training Epoch: 1 [12750/36450]\tLoss: 1810.6954\n",
      "Training Epoch: 1 [12800/36450]\tLoss: 1884.2277\n",
      "Training Epoch: 1 [12850/36450]\tLoss: 1914.5691\n",
      "Training Epoch: 1 [12900/36450]\tLoss: 1834.2975\n",
      "Training Epoch: 1 [12950/36450]\tLoss: 1785.0331\n",
      "Training Epoch: 1 [13000/36450]\tLoss: 1906.2812\n",
      "Training Epoch: 1 [13050/36450]\tLoss: 1801.1527\n",
      "Training Epoch: 1 [13100/36450]\tLoss: 1783.8665\n",
      "Training Epoch: 1 [13150/36450]\tLoss: 1811.6212\n",
      "Training Epoch: 1 [13200/36450]\tLoss: 1841.2065\n",
      "Training Epoch: 1 [13250/36450]\tLoss: 1752.6992\n",
      "Training Epoch: 1 [13300/36450]\tLoss: 1794.5204\n",
      "Training Epoch: 1 [13350/36450]\tLoss: 1866.2308\n",
      "Training Epoch: 1 [13400/36450]\tLoss: 1727.9283\n",
      "Training Epoch: 1 [13450/36450]\tLoss: 1832.1493\n",
      "Training Epoch: 1 [13500/36450]\tLoss: 1781.4607\n",
      "Training Epoch: 1 [13550/36450]\tLoss: 1845.5990\n",
      "Training Epoch: 1 [13600/36450]\tLoss: 1758.0608\n",
      "Training Epoch: 1 [13650/36450]\tLoss: 1777.7115\n",
      "Training Epoch: 1 [13700/36450]\tLoss: 1733.3276\n",
      "Training Epoch: 1 [13750/36450]\tLoss: 1766.3110\n",
      "Training Epoch: 1 [13800/36450]\tLoss: 1652.5701\n",
      "Training Epoch: 1 [13850/36450]\tLoss: 1747.1245\n",
      "Training Epoch: 1 [13900/36450]\tLoss: 1720.3656\n",
      "Training Epoch: 1 [13950/36450]\tLoss: 1630.1300\n",
      "Training Epoch: 1 [14000/36450]\tLoss: 1733.3583\n",
      "Training Epoch: 1 [14050/36450]\tLoss: 1819.6786\n",
      "Training Epoch: 1 [14100/36450]\tLoss: 1707.6368\n",
      "Training Epoch: 1 [14150/36450]\tLoss: 1674.7349\n",
      "Training Epoch: 1 [14200/36450]\tLoss: 1627.1979\n",
      "Training Epoch: 1 [14250/36450]\tLoss: 1676.3357\n",
      "Training Epoch: 1 [14300/36450]\tLoss: 1604.2056\n",
      "Training Epoch: 1 [14350/36450]\tLoss: 1652.7944\n",
      "Training Epoch: 1 [14400/36450]\tLoss: 1642.8278\n",
      "Training Epoch: 1 [14450/36450]\tLoss: 1634.3352\n",
      "Training Epoch: 1 [14500/36450]\tLoss: 1655.8430\n",
      "Training Epoch: 1 [14550/36450]\tLoss: 1592.7235\n",
      "Training Epoch: 1 [14600/36450]\tLoss: 1642.3540\n",
      "Training Epoch: 1 [14650/36450]\tLoss: 1600.3615\n",
      "Training Epoch: 1 [14700/36450]\tLoss: 1493.6508\n",
      "Training Epoch: 1 [14750/36450]\tLoss: 1643.6991\n",
      "Training Epoch: 1 [14800/36450]\tLoss: 1488.1998\n",
      "Training Epoch: 1 [14850/36450]\tLoss: 1520.0710\n",
      "Training Epoch: 1 [14900/36450]\tLoss: 1601.9459\n",
      "Training Epoch: 1 [14950/36450]\tLoss: 1459.4835\n",
      "Training Epoch: 1 [15000/36450]\tLoss: 1561.2155\n",
      "Training Epoch: 1 [15050/36450]\tLoss: 1591.7719\n",
      "Training Epoch: 1 [15100/36450]\tLoss: 1551.7406\n",
      "Training Epoch: 1 [15150/36450]\tLoss: 1593.3521\n",
      "Training Epoch: 1 [15200/36450]\tLoss: 1489.7048\n",
      "Training Epoch: 1 [15250/36450]\tLoss: 1521.5953\n",
      "Training Epoch: 1 [15300/36450]\tLoss: 1520.2084\n",
      "Training Epoch: 1 [15350/36450]\tLoss: 1551.2667\n",
      "Training Epoch: 1 [15400/36450]\tLoss: 1497.4221\n",
      "Training Epoch: 1 [15450/36450]\tLoss: 1561.2303\n",
      "Training Epoch: 1 [15500/36450]\tLoss: 1472.8623\n",
      "Training Epoch: 1 [15550/36450]\tLoss: 1490.5085\n",
      "Training Epoch: 1 [15600/36450]\tLoss: 1467.2327\n",
      "Training Epoch: 1 [15650/36450]\tLoss: 1486.7319\n",
      "Training Epoch: 1 [15700/36450]\tLoss: 1532.3915\n",
      "Training Epoch: 1 [15750/36450]\tLoss: 1508.7531\n",
      "Training Epoch: 1 [15800/36450]\tLoss: 1341.5344\n",
      "Training Epoch: 1 [15850/36450]\tLoss: 1448.2478\n",
      "Training Epoch: 1 [15900/36450]\tLoss: 1520.8683\n",
      "Training Epoch: 1 [15950/36450]\tLoss: 1440.6082\n",
      "Training Epoch: 1 [16000/36450]\tLoss: 1440.0806\n",
      "Training Epoch: 1 [16050/36450]\tLoss: 1443.9634\n",
      "Training Epoch: 1 [16100/36450]\tLoss: 1461.4847\n",
      "Training Epoch: 1 [16150/36450]\tLoss: 1416.2092\n",
      "Training Epoch: 1 [16200/36450]\tLoss: 1416.3790\n",
      "Training Epoch: 1 [16250/36450]\tLoss: 1423.4915\n",
      "Training Epoch: 1 [16300/36450]\tLoss: 1435.2960\n",
      "Training Epoch: 1 [16350/36450]\tLoss: 1422.6252\n",
      "Training Epoch: 1 [16400/36450]\tLoss: 1425.1882\n",
      "Training Epoch: 1 [16450/36450]\tLoss: 1368.8298\n",
      "Training Epoch: 1 [16500/36450]\tLoss: 1434.9423\n",
      "Training Epoch: 1 [16550/36450]\tLoss: 1407.5861\n",
      "Training Epoch: 1 [16600/36450]\tLoss: 1339.5469\n",
      "Training Epoch: 1 [16650/36450]\tLoss: 1313.3079\n",
      "Training Epoch: 1 [16700/36450]\tLoss: 1463.2506\n",
      "Training Epoch: 1 [16750/36450]\tLoss: 1337.5728\n",
      "Training Epoch: 1 [16800/36450]\tLoss: 1310.6417\n",
      "Training Epoch: 1 [16850/36450]\tLoss: 1429.3848\n",
      "Training Epoch: 1 [16900/36450]\tLoss: 1357.8301\n",
      "Training Epoch: 1 [16950/36450]\tLoss: 1308.1902\n",
      "Training Epoch: 1 [17000/36450]\tLoss: 1346.6016\n",
      "Training Epoch: 1 [17050/36450]\tLoss: 1383.0227\n",
      "Training Epoch: 1 [17100/36450]\tLoss: 1345.9425\n",
      "Training Epoch: 1 [17150/36450]\tLoss: 1306.0162\n",
      "Training Epoch: 1 [17200/36450]\tLoss: 1338.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [17250/36450]\tLoss: 1337.2622\n",
      "Training Epoch: 1 [17300/36450]\tLoss: 1331.3682\n",
      "Training Epoch: 1 [17350/36450]\tLoss: 1311.2754\n",
      "Training Epoch: 1 [17400/36450]\tLoss: 1327.5210\n",
      "Training Epoch: 1 [17450/36450]\tLoss: 1333.0173\n",
      "Training Epoch: 1 [17500/36450]\tLoss: 1309.3784\n",
      "Training Epoch: 1 [17550/36450]\tLoss: 1259.2677\n",
      "Training Epoch: 1 [17600/36450]\tLoss: 1284.1460\n",
      "Training Epoch: 1 [17650/36450]\tLoss: 1346.9628\n",
      "Training Epoch: 1 [17700/36450]\tLoss: 1317.6759\n",
      "Training Epoch: 1 [17750/36450]\tLoss: 1220.9967\n",
      "Training Epoch: 1 [17800/36450]\tLoss: 1218.8387\n",
      "Training Epoch: 1 [17850/36450]\tLoss: 1263.6691\n",
      "Training Epoch: 1 [17900/36450]\tLoss: 1293.1213\n",
      "Training Epoch: 1 [17950/36450]\tLoss: 1296.8545\n",
      "Training Epoch: 1 [18000/36450]\tLoss: 1324.0638\n",
      "Training Epoch: 1 [18050/36450]\tLoss: 1226.8284\n",
      "Training Epoch: 1 [18100/36450]\tLoss: 1205.6410\n",
      "Training Epoch: 1 [18150/36450]\tLoss: 1187.1520\n",
      "Training Epoch: 1 [18200/36450]\tLoss: 1283.8835\n",
      "Training Epoch: 1 [18250/36450]\tLoss: 1250.6616\n",
      "Training Epoch: 1 [18300/36450]\tLoss: 1272.0748\n",
      "Training Epoch: 1 [18350/36450]\tLoss: 1262.8873\n",
      "Training Epoch: 1 [18400/36450]\tLoss: 1261.9603\n",
      "Training Epoch: 1 [18450/36450]\tLoss: 1243.6390\n",
      "Training Epoch: 1 [18500/36450]\tLoss: 1228.4900\n",
      "Training Epoch: 1 [18550/36450]\tLoss: 1219.4578\n",
      "Training Epoch: 1 [18600/36450]\tLoss: 1278.4369\n",
      "Training Epoch: 1 [18650/36450]\tLoss: 1209.0505\n",
      "Training Epoch: 1 [18700/36450]\tLoss: 1168.0387\n",
      "Training Epoch: 1 [18750/36450]\tLoss: 1186.3680\n",
      "Training Epoch: 1 [18800/36450]\tLoss: 1195.3665\n",
      "Training Epoch: 1 [18850/36450]\tLoss: 1139.0016\n",
      "Training Epoch: 1 [18900/36450]\tLoss: 1243.6813\n",
      "Training Epoch: 1 [18950/36450]\tLoss: 1137.8422\n",
      "Training Epoch: 1 [19000/36450]\tLoss: 1192.5525\n",
      "Training Epoch: 1 [19050/36450]\tLoss: 1267.1064\n",
      "Training Epoch: 1 [19100/36450]\tLoss: 1170.2303\n",
      "Training Epoch: 1 [19150/36450]\tLoss: 1206.9125\n",
      "Training Epoch: 1 [19200/36450]\tLoss: 1140.0961\n",
      "Training Epoch: 1 [19250/36450]\tLoss: 1115.0013\n",
      "Training Epoch: 1 [19300/36450]\tLoss: 1221.9907\n",
      "Training Epoch: 1 [19350/36450]\tLoss: 1165.9883\n",
      "Training Epoch: 1 [19400/36450]\tLoss: 1257.8734\n",
      "Training Epoch: 1 [19450/36450]\tLoss: 1150.2952\n",
      "Training Epoch: 1 [19500/36450]\tLoss: 1183.7549\n",
      "Training Epoch: 1 [19550/36450]\tLoss: 1187.7354\n",
      "Training Epoch: 1 [19600/36450]\tLoss: 1149.6318\n",
      "Training Epoch: 1 [19650/36450]\tLoss: 1199.9834\n",
      "Training Epoch: 1 [19700/36450]\tLoss: 1228.3896\n",
      "Training Epoch: 1 [19750/36450]\tLoss: 1114.8090\n",
      "Training Epoch: 1 [19800/36450]\tLoss: 1106.5511\n",
      "Training Epoch: 1 [19850/36450]\tLoss: 1230.3511\n",
      "Training Epoch: 1 [19900/36450]\tLoss: 1126.1411\n",
      "Training Epoch: 1 [19950/36450]\tLoss: 1202.2742\n",
      "Training Epoch: 1 [20000/36450]\tLoss: 1144.1549\n",
      "Training Epoch: 1 [20050/36450]\tLoss: 1112.6649\n",
      "Training Epoch: 1 [20100/36450]\tLoss: 1184.6080\n",
      "Training Epoch: 1 [20150/36450]\tLoss: 1086.4879\n",
      "Training Epoch: 1 [20200/36450]\tLoss: 1132.1594\n",
      "Training Epoch: 1 [20250/36450]\tLoss: 1136.6652\n",
      "Training Epoch: 1 [20300/36450]\tLoss: 1126.6731\n",
      "Training Epoch: 1 [20350/36450]\tLoss: 1060.9974\n",
      "Training Epoch: 1 [20400/36450]\tLoss: 1143.2880\n",
      "Training Epoch: 1 [20450/36450]\tLoss: 1118.3746\n",
      "Training Epoch: 1 [20500/36450]\tLoss: 1118.5757\n",
      "Training Epoch: 1 [20550/36450]\tLoss: 1104.0072\n",
      "Training Epoch: 1 [20600/36450]\tLoss: 1089.1788\n",
      "Training Epoch: 1 [20650/36450]\tLoss: 1076.4928\n",
      "Training Epoch: 1 [20700/36450]\tLoss: 1066.4880\n",
      "Training Epoch: 1 [20750/36450]\tLoss: 1078.6949\n",
      "Training Epoch: 1 [20800/36450]\tLoss: 1109.5693\n",
      "Training Epoch: 1 [20850/36450]\tLoss: 1097.7874\n",
      "Training Epoch: 1 [20900/36450]\tLoss: 1129.6761\n",
      "Training Epoch: 1 [20950/36450]\tLoss: 1040.4751\n",
      "Training Epoch: 1 [21000/36450]\tLoss: 1023.2913\n",
      "Training Epoch: 1 [21050/36450]\tLoss: 1117.6008\n",
      "Training Epoch: 1 [21100/36450]\tLoss: 1032.8186\n",
      "Training Epoch: 1 [21150/36450]\tLoss: 1069.7151\n",
      "Training Epoch: 1 [21200/36450]\tLoss: 1104.6321\n",
      "Training Epoch: 1 [21250/36450]\tLoss: 995.0239\n",
      "Training Epoch: 1 [21300/36450]\tLoss: 1055.8397\n",
      "Training Epoch: 1 [21350/36450]\tLoss: 1062.5341\n",
      "Training Epoch: 1 [21400/36450]\tLoss: 1112.1278\n",
      "Training Epoch: 1 [21450/36450]\tLoss: 1010.4507\n",
      "Training Epoch: 1 [21500/36450]\tLoss: 1147.6421\n",
      "Training Epoch: 1 [21550/36450]\tLoss: 1019.1123\n",
      "Training Epoch: 1 [21600/36450]\tLoss: 1074.8345\n",
      "Training Epoch: 1 [21650/36450]\tLoss: 1025.1984\n",
      "Training Epoch: 1 [21700/36450]\tLoss: 976.8417\n",
      "Training Epoch: 1 [21750/36450]\tLoss: 1091.8279\n",
      "Training Epoch: 1 [21800/36450]\tLoss: 1050.7260\n",
      "Training Epoch: 1 [21850/36450]\tLoss: 1068.2860\n",
      "Training Epoch: 1 [21900/36450]\tLoss: 1080.8618\n",
      "Training Epoch: 1 [21950/36450]\tLoss: 974.2670\n",
      "Training Epoch: 1 [22000/36450]\tLoss: 1005.4558\n",
      "Training Epoch: 1 [22050/36450]\tLoss: 1013.5970\n",
      "Training Epoch: 1 [22100/36450]\tLoss: 1006.9250\n",
      "Training Epoch: 1 [22150/36450]\tLoss: 979.6454\n",
      "Training Epoch: 1 [22200/36450]\tLoss: 1032.4680\n",
      "Training Epoch: 1 [22250/36450]\tLoss: 1038.3811\n",
      "Training Epoch: 1 [22300/36450]\tLoss: 1101.4290\n",
      "Training Epoch: 1 [22350/36450]\tLoss: 1015.3823\n",
      "Training Epoch: 1 [22400/36450]\tLoss: 1003.1257\n",
      "Training Epoch: 1 [22450/36450]\tLoss: 1049.2919\n",
      "Training Epoch: 1 [22500/36450]\tLoss: 968.2654\n",
      "Training Epoch: 1 [22550/36450]\tLoss: 961.4319\n",
      "Training Epoch: 1 [22600/36450]\tLoss: 1010.5743\n",
      "Training Epoch: 1 [22650/36450]\tLoss: 1124.1213\n",
      "Training Epoch: 1 [22700/36450]\tLoss: 941.7393\n",
      "Training Epoch: 1 [22750/36450]\tLoss: 1062.3138\n",
      "Training Epoch: 1 [22800/36450]\tLoss: 988.9323\n",
      "Training Epoch: 1 [22850/36450]\tLoss: 990.1343\n",
      "Training Epoch: 1 [22900/36450]\tLoss: 1005.9752\n",
      "Training Epoch: 1 [22950/36450]\tLoss: 937.7550\n",
      "Training Epoch: 1 [23000/36450]\tLoss: 986.7386\n",
      "Training Epoch: 1 [23050/36450]\tLoss: 1033.6234\n",
      "Training Epoch: 1 [23100/36450]\tLoss: 971.2834\n",
      "Training Epoch: 1 [23150/36450]\tLoss: 960.7023\n",
      "Training Epoch: 1 [23200/36450]\tLoss: 993.6938\n",
      "Training Epoch: 1 [23250/36450]\tLoss: 918.6148\n",
      "Training Epoch: 1 [23300/36450]\tLoss: 975.5850\n",
      "Training Epoch: 1 [23350/36450]\tLoss: 968.9731\n",
      "Training Epoch: 1 [23400/36450]\tLoss: 991.8036\n",
      "Training Epoch: 1 [23450/36450]\tLoss: 982.6313\n",
      "Training Epoch: 1 [23500/36450]\tLoss: 956.0671\n",
      "Training Epoch: 1 [23550/36450]\tLoss: 946.0607\n",
      "Training Epoch: 1 [23600/36450]\tLoss: 994.1259\n",
      "Training Epoch: 1 [23650/36450]\tLoss: 936.9913\n",
      "Training Epoch: 1 [23700/36450]\tLoss: 961.4648\n",
      "Training Epoch: 1 [23750/36450]\tLoss: 908.3679\n",
      "Training Epoch: 1 [23800/36450]\tLoss: 999.8434\n",
      "Training Epoch: 1 [23850/36450]\tLoss: 947.7697\n",
      "Training Epoch: 1 [23900/36450]\tLoss: 909.1904\n",
      "Training Epoch: 1 [23950/36450]\tLoss: 938.5339\n",
      "Training Epoch: 1 [24000/36450]\tLoss: 939.1707\n",
      "Training Epoch: 1 [24050/36450]\tLoss: 934.4465\n",
      "Training Epoch: 1 [24100/36450]\tLoss: 979.1829\n",
      "Training Epoch: 1 [24150/36450]\tLoss: 911.1134\n",
      "Training Epoch: 1 [24200/36450]\tLoss: 948.0567\n",
      "Training Epoch: 1 [24250/36450]\tLoss: 944.8008\n",
      "Training Epoch: 1 [24300/36450]\tLoss: 1023.2581\n",
      "Training Epoch: 1 [24350/36450]\tLoss: 874.3295\n",
      "Training Epoch: 1 [24400/36450]\tLoss: 947.4880\n",
      "Training Epoch: 1 [24450/36450]\tLoss: 965.0410\n",
      "Training Epoch: 1 [24500/36450]\tLoss: 917.4171\n",
      "Training Epoch: 1 [24550/36450]\tLoss: 928.8749\n",
      "Training Epoch: 1 [24600/36450]\tLoss: 981.5388\n",
      "Training Epoch: 1 [24650/36450]\tLoss: 1006.2009\n",
      "Training Epoch: 1 [24700/36450]\tLoss: 961.7769\n",
      "Training Epoch: 1 [24750/36450]\tLoss: 992.3485\n",
      "Training Epoch: 1 [24800/36450]\tLoss: 904.2618\n",
      "Training Epoch: 1 [24850/36450]\tLoss: 894.3882\n",
      "Training Epoch: 1 [24900/36450]\tLoss: 871.7651\n",
      "Training Epoch: 1 [24950/36450]\tLoss: 908.1849\n",
      "Training Epoch: 1 [25000/36450]\tLoss: 945.3116\n",
      "Training Epoch: 1 [25050/36450]\tLoss: 902.5883\n",
      "Training Epoch: 1 [25100/36450]\tLoss: 869.3475\n",
      "Training Epoch: 1 [25150/36450]\tLoss: 914.8583\n",
      "Training Epoch: 1 [25200/36450]\tLoss: 950.2380\n",
      "Training Epoch: 1 [25250/36450]\tLoss: 963.3283\n",
      "Training Epoch: 1 [25300/36450]\tLoss: 889.7174\n",
      "Training Epoch: 1 [25350/36450]\tLoss: 923.1951\n",
      "Training Epoch: 1 [25400/36450]\tLoss: 933.7187\n",
      "Training Epoch: 1 [25450/36450]\tLoss: 918.2693\n",
      "Training Epoch: 1 [25500/36450]\tLoss: 881.8955\n",
      "Training Epoch: 1 [25550/36450]\tLoss: 912.5580\n",
      "Training Epoch: 1 [25600/36450]\tLoss: 931.1448\n",
      "Training Epoch: 1 [25650/36450]\tLoss: 933.8749\n",
      "Training Epoch: 1 [25700/36450]\tLoss: 913.1565\n",
      "Training Epoch: 1 [25750/36450]\tLoss: 944.1804\n",
      "Training Epoch: 1 [25800/36450]\tLoss: 924.3124\n",
      "Training Epoch: 1 [25850/36450]\tLoss: 908.6052\n",
      "Training Epoch: 1 [25900/36450]\tLoss: 914.8780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [25950/36450]\tLoss: 926.3012\n",
      "Training Epoch: 1 [26000/36450]\tLoss: 982.1078\n",
      "Training Epoch: 1 [26050/36450]\tLoss: 927.6266\n",
      "Training Epoch: 1 [26100/36450]\tLoss: 924.4004\n",
      "Training Epoch: 1 [26150/36450]\tLoss: 836.8755\n",
      "Training Epoch: 1 [26200/36450]\tLoss: 851.0964\n",
      "Training Epoch: 1 [26250/36450]\tLoss: 881.3607\n",
      "Training Epoch: 1 [26300/36450]\tLoss: 951.1378\n",
      "Training Epoch: 1 [26350/36450]\tLoss: 872.4000\n",
      "Training Epoch: 1 [26400/36450]\tLoss: 886.5232\n",
      "Training Epoch: 1 [26450/36450]\tLoss: 831.1929\n",
      "Training Epoch: 1 [26500/36450]\tLoss: 912.8802\n",
      "Training Epoch: 1 [26550/36450]\tLoss: 847.7134\n",
      "Training Epoch: 1 [26600/36450]\tLoss: 886.5099\n",
      "Training Epoch: 1 [26650/36450]\tLoss: 887.0460\n",
      "Training Epoch: 1 [26700/36450]\tLoss: 998.7069\n",
      "Training Epoch: 1 [26750/36450]\tLoss: 879.7378\n",
      "Training Epoch: 1 [26800/36450]\tLoss: 889.5944\n",
      "Training Epoch: 1 [26850/36450]\tLoss: 959.5894\n",
      "Training Epoch: 1 [26900/36450]\tLoss: 917.0466\n",
      "Training Epoch: 1 [26950/36450]\tLoss: 867.4225\n",
      "Training Epoch: 1 [27000/36450]\tLoss: 909.3543\n",
      "Training Epoch: 1 [27050/36450]\tLoss: 869.7161\n",
      "Training Epoch: 1 [27100/36450]\tLoss: 942.0693\n",
      "Training Epoch: 1 [27150/36450]\tLoss: 871.0580\n",
      "Training Epoch: 1 [27200/36450]\tLoss: 878.4800\n",
      "Training Epoch: 1 [27250/36450]\tLoss: 903.6102\n",
      "Training Epoch: 1 [27300/36450]\tLoss: 806.5669\n",
      "Training Epoch: 1 [27350/36450]\tLoss: 909.9686\n",
      "Training Epoch: 1 [27400/36450]\tLoss: 891.1263\n",
      "Training Epoch: 1 [27450/36450]\tLoss: 878.1162\n",
      "Training Epoch: 1 [27500/36450]\tLoss: 937.1498\n",
      "Training Epoch: 1 [27550/36450]\tLoss: 947.2762\n",
      "Training Epoch: 1 [27600/36450]\tLoss: 816.0220\n",
      "Training Epoch: 1 [27650/36450]\tLoss: 894.8550\n",
      "Training Epoch: 1 [27700/36450]\tLoss: 925.4760\n",
      "Training Epoch: 1 [27750/36450]\tLoss: 891.1510\n",
      "Training Epoch: 1 [27800/36450]\tLoss: 876.1965\n",
      "Training Epoch: 1 [27850/36450]\tLoss: 853.7127\n",
      "Training Epoch: 1 [27900/36450]\tLoss: 833.0184\n",
      "Training Epoch: 1 [27950/36450]\tLoss: 890.2831\n",
      "Training Epoch: 1 [28000/36450]\tLoss: 812.7382\n",
      "Training Epoch: 1 [28050/36450]\tLoss: 802.0385\n",
      "Training Epoch: 1 [28100/36450]\tLoss: 862.4936\n",
      "Training Epoch: 1 [28150/36450]\tLoss: 900.7694\n",
      "Training Epoch: 1 [28200/36450]\tLoss: 837.1706\n",
      "Training Epoch: 1 [28250/36450]\tLoss: 812.6514\n",
      "Training Epoch: 1 [28300/36450]\tLoss: 908.1133\n",
      "Training Epoch: 1 [28350/36450]\tLoss: 854.3894\n",
      "Training Epoch: 1 [28400/36450]\tLoss: 857.7802\n",
      "Training Epoch: 1 [28450/36450]\tLoss: 835.3716\n",
      "Training Epoch: 1 [28500/36450]\tLoss: 820.7733\n",
      "Training Epoch: 1 [28550/36450]\tLoss: 909.8275\n",
      "Training Epoch: 1 [28600/36450]\tLoss: 837.9050\n",
      "Training Epoch: 1 [28650/36450]\tLoss: 849.0823\n",
      "Training Epoch: 1 [28700/36450]\tLoss: 821.7119\n",
      "Training Epoch: 1 [28750/36450]\tLoss: 902.3522\n",
      "Training Epoch: 1 [28800/36450]\tLoss: 814.3868\n",
      "Training Epoch: 1 [28850/36450]\tLoss: 898.1519\n",
      "Training Epoch: 1 [28900/36450]\tLoss: 815.1911\n",
      "Training Epoch: 1 [28950/36450]\tLoss: 925.5360\n",
      "Training Epoch: 1 [29000/36450]\tLoss: 819.4451\n",
      "Training Epoch: 1 [29050/36450]\tLoss: 835.0876\n",
      "Training Epoch: 1 [29100/36450]\tLoss: 851.9421\n",
      "Training Epoch: 1 [29150/36450]\tLoss: 793.1616\n",
      "Training Epoch: 1 [29200/36450]\tLoss: 854.1887\n",
      "Training Epoch: 1 [29250/36450]\tLoss: 815.8633\n",
      "Training Epoch: 1 [29300/36450]\tLoss: 858.9485\n",
      "Training Epoch: 1 [29350/36450]\tLoss: 809.1090\n",
      "Training Epoch: 1 [29400/36450]\tLoss: 804.2849\n",
      "Training Epoch: 1 [29450/36450]\tLoss: 791.0183\n",
      "Training Epoch: 1 [29500/36450]\tLoss: 931.7395\n",
      "Training Epoch: 1 [29550/36450]\tLoss: 832.1121\n",
      "Training Epoch: 1 [29600/36450]\tLoss: 859.9593\n",
      "Training Epoch: 1 [29650/36450]\tLoss: 796.8298\n",
      "Training Epoch: 1 [29700/36450]\tLoss: 866.5143\n",
      "Training Epoch: 1 [29750/36450]\tLoss: 857.4186\n",
      "Training Epoch: 1 [29800/36450]\tLoss: 878.5580\n",
      "Training Epoch: 1 [29850/36450]\tLoss: 852.2866\n",
      "Training Epoch: 1 [29900/36450]\tLoss: 802.6968\n",
      "Training Epoch: 1 [29950/36450]\tLoss: 919.8042\n",
      "Training Epoch: 1 [30000/36450]\tLoss: 790.6287\n",
      "Training Epoch: 1 [30050/36450]\tLoss: 840.0868\n",
      "Training Epoch: 1 [30100/36450]\tLoss: 847.4017\n",
      "Training Epoch: 1 [30150/36450]\tLoss: 807.0370\n",
      "Training Epoch: 1 [30200/36450]\tLoss: 860.7520\n",
      "Training Epoch: 1 [30250/36450]\tLoss: 811.4545\n",
      "Training Epoch: 1 [30300/36450]\tLoss: 807.7823\n",
      "Training Epoch: 1 [30350/36450]\tLoss: 851.6518\n",
      "Training Epoch: 1 [30400/36450]\tLoss: 796.0058\n",
      "Training Epoch: 1 [30450/36450]\tLoss: 897.9039\n",
      "Training Epoch: 1 [30500/36450]\tLoss: 833.5757\n",
      "Training Epoch: 1 [30550/36450]\tLoss: 838.9384\n",
      "Training Epoch: 1 [30600/36450]\tLoss: 864.0728\n",
      "Training Epoch: 1 [30650/36450]\tLoss: 878.9699\n",
      "Training Epoch: 1 [30700/36450]\tLoss: 845.0598\n",
      "Training Epoch: 1 [30750/36450]\tLoss: 810.2388\n",
      "Training Epoch: 1 [30800/36450]\tLoss: 767.9095\n",
      "Training Epoch: 1 [30850/36450]\tLoss: 913.1212\n",
      "Training Epoch: 1 [30900/36450]\tLoss: 838.3830\n",
      "Training Epoch: 1 [30950/36450]\tLoss: 843.6446\n",
      "Training Epoch: 1 [31000/36450]\tLoss: 843.9340\n",
      "Training Epoch: 1 [31050/36450]\tLoss: 831.3054\n",
      "Training Epoch: 1 [31100/36450]\tLoss: 902.9570\n",
      "Training Epoch: 1 [31150/36450]\tLoss: 764.4527\n",
      "Training Epoch: 1 [31200/36450]\tLoss: 803.9604\n",
      "Training Epoch: 1 [31250/36450]\tLoss: 823.4285\n",
      "Training Epoch: 1 [31300/36450]\tLoss: 794.3532\n",
      "Training Epoch: 1 [31350/36450]\tLoss: 846.3053\n",
      "Training Epoch: 1 [31400/36450]\tLoss: 812.7980\n",
      "Training Epoch: 1 [31450/36450]\tLoss: 796.3629\n",
      "Training Epoch: 1 [31500/36450]\tLoss: 830.1677\n",
      "Training Epoch: 1 [31550/36450]\tLoss: 833.1821\n",
      "Training Epoch: 1 [31600/36450]\tLoss: 866.5502\n",
      "Training Epoch: 1 [31650/36450]\tLoss: 784.8228\n",
      "Training Epoch: 1 [31700/36450]\tLoss: 844.3742\n",
      "Training Epoch: 1 [31750/36450]\tLoss: 819.7078\n",
      "Training Epoch: 1 [31800/36450]\tLoss: 869.6361\n",
      "Training Epoch: 1 [31850/36450]\tLoss: 836.6954\n",
      "Training Epoch: 1 [31900/36450]\tLoss: 898.6921\n",
      "Training Epoch: 1 [31950/36450]\tLoss: 847.7440\n",
      "Training Epoch: 1 [32000/36450]\tLoss: 813.2205\n",
      "Training Epoch: 1 [32050/36450]\tLoss: 854.1778\n",
      "Training Epoch: 1 [32100/36450]\tLoss: 821.7975\n",
      "Training Epoch: 1 [32150/36450]\tLoss: 777.8179\n",
      "Training Epoch: 1 [32200/36450]\tLoss: 836.4887\n",
      "Training Epoch: 1 [32250/36450]\tLoss: 783.0312\n",
      "Training Epoch: 1 [32300/36450]\tLoss: 839.5166\n",
      "Training Epoch: 1 [32350/36450]\tLoss: 767.6469\n",
      "Training Epoch: 1 [32400/36450]\tLoss: 866.5683\n",
      "Training Epoch: 1 [32450/36450]\tLoss: 791.7108\n",
      "Training Epoch: 1 [32500/36450]\tLoss: 816.3663\n",
      "Training Epoch: 1 [32550/36450]\tLoss: 800.3556\n",
      "Training Epoch: 1 [32600/36450]\tLoss: 763.0558\n",
      "Training Epoch: 1 [32650/36450]\tLoss: 811.7772\n",
      "Training Epoch: 1 [32700/36450]\tLoss: 802.5722\n",
      "Training Epoch: 1 [32750/36450]\tLoss: 854.2200\n",
      "Training Epoch: 1 [32800/36450]\tLoss: 863.5453\n",
      "Training Epoch: 1 [32850/36450]\tLoss: 804.5773\n",
      "Training Epoch: 1 [32900/36450]\tLoss: 781.9905\n",
      "Training Epoch: 1 [32950/36450]\tLoss: 834.0640\n",
      "Training Epoch: 1 [33000/36450]\tLoss: 782.2847\n",
      "Training Epoch: 1 [33050/36450]\tLoss: 829.4530\n",
      "Training Epoch: 1 [33100/36450]\tLoss: 858.7405\n",
      "Training Epoch: 1 [33150/36450]\tLoss: 781.5468\n",
      "Training Epoch: 1 [33200/36450]\tLoss: 789.3849\n",
      "Training Epoch: 1 [33250/36450]\tLoss: 757.0462\n",
      "Training Epoch: 1 [33300/36450]\tLoss: 812.2048\n",
      "Training Epoch: 1 [33350/36450]\tLoss: 800.7598\n",
      "Training Epoch: 1 [33400/36450]\tLoss: 787.3522\n",
      "Training Epoch: 1 [33450/36450]\tLoss: 855.3788\n",
      "Training Epoch: 1 [33500/36450]\tLoss: 798.2863\n",
      "Training Epoch: 1 [33550/36450]\tLoss: 780.1508\n",
      "Training Epoch: 1 [33600/36450]\tLoss: 733.0901\n",
      "Training Epoch: 1 [33650/36450]\tLoss: 748.2023\n",
      "Training Epoch: 1 [33700/36450]\tLoss: 827.8378\n",
      "Training Epoch: 1 [33750/36450]\tLoss: 775.9416\n",
      "Training Epoch: 1 [33800/36450]\tLoss: 781.7930\n",
      "Training Epoch: 1 [33850/36450]\tLoss: 788.7493\n",
      "Training Epoch: 1 [33900/36450]\tLoss: 816.9898\n",
      "Training Epoch: 1 [33950/36450]\tLoss: 781.3472\n",
      "Training Epoch: 1 [34000/36450]\tLoss: 828.8936\n",
      "Training Epoch: 1 [34050/36450]\tLoss: 736.3942\n",
      "Training Epoch: 1 [34100/36450]\tLoss: 835.6519\n",
      "Training Epoch: 1 [34150/36450]\tLoss: 813.0079\n",
      "Training Epoch: 1 [34200/36450]\tLoss: 778.8250\n",
      "Training Epoch: 1 [34250/36450]\tLoss: 867.9472\n",
      "Training Epoch: 1 [34300/36450]\tLoss: 793.4108\n",
      "Training Epoch: 1 [34350/36450]\tLoss: 763.8273\n",
      "Training Epoch: 1 [34400/36450]\tLoss: 841.9409\n",
      "Training Epoch: 1 [34450/36450]\tLoss: 749.1567\n",
      "Training Epoch: 1 [34500/36450]\tLoss: 778.6122\n",
      "Training Epoch: 1 [34550/36450]\tLoss: 845.6384\n",
      "Training Epoch: 1 [34600/36450]\tLoss: 859.6592\n",
      "Training Epoch: 1 [34650/36450]\tLoss: 817.5037\n",
      "Training Epoch: 1 [34700/36450]\tLoss: 782.0883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [34750/36450]\tLoss: 746.4789\n",
      "Training Epoch: 1 [34800/36450]\tLoss: 803.1672\n",
      "Training Epoch: 1 [34850/36450]\tLoss: 808.6339\n",
      "Training Epoch: 1 [34900/36450]\tLoss: 764.4031\n",
      "Training Epoch: 1 [34950/36450]\tLoss: 763.8154\n",
      "Training Epoch: 1 [35000/36450]\tLoss: 855.9781\n",
      "Training Epoch: 1 [35050/36450]\tLoss: 750.3104\n",
      "Training Epoch: 1 [35100/36450]\tLoss: 846.5773\n",
      "Training Epoch: 1 [35150/36450]\tLoss: 782.0018\n",
      "Training Epoch: 1 [35200/36450]\tLoss: 763.5951\n",
      "Training Epoch: 1 [35250/36450]\tLoss: 799.3811\n",
      "Training Epoch: 1 [35300/36450]\tLoss: 793.2510\n",
      "Training Epoch: 1 [35350/36450]\tLoss: 777.3450\n",
      "Training Epoch: 1 [35400/36450]\tLoss: 785.8243\n",
      "Training Epoch: 1 [35450/36450]\tLoss: 784.8521\n",
      "Training Epoch: 1 [35500/36450]\tLoss: 775.5897\n",
      "Training Epoch: 1 [35550/36450]\tLoss: 743.9166\n",
      "Training Epoch: 1 [35600/36450]\tLoss: 842.8394\n",
      "Training Epoch: 1 [35650/36450]\tLoss: 794.6246\n",
      "Training Epoch: 1 [35700/36450]\tLoss: 801.2769\n",
      "Training Epoch: 1 [35750/36450]\tLoss: 814.2369\n",
      "Training Epoch: 1 [35800/36450]\tLoss: 829.1446\n",
      "Training Epoch: 1 [35850/36450]\tLoss: 775.8239\n",
      "Training Epoch: 1 [35900/36450]\tLoss: 785.1513\n",
      "Training Epoch: 1 [35950/36450]\tLoss: 774.3088\n",
      "Training Epoch: 1 [36000/36450]\tLoss: 744.8344\n",
      "Training Epoch: 1 [36050/36450]\tLoss: 815.7867\n",
      "Training Epoch: 1 [36100/36450]\tLoss: 756.3541\n",
      "Training Epoch: 1 [36150/36450]\tLoss: 791.0449\n",
      "Training Epoch: 1 [36200/36450]\tLoss: 759.7497\n",
      "Training Epoch: 1 [36250/36450]\tLoss: 796.9802\n",
      "Training Epoch: 1 [36300/36450]\tLoss: 739.5267\n",
      "Training Epoch: 1 [36350/36450]\tLoss: 779.6187\n",
      "Training Epoch: 1 [36400/36450]\tLoss: 794.1790\n",
      "Training Epoch: 1 [36450/36450]\tLoss: 772.8694\n",
      "Training Epoch: 1 [4050/4050]\tLoss: 379.5244\n",
      "Training Epoch: 2 [50/36450]\tLoss: 737.5836\n",
      "Training Epoch: 2 [100/36450]\tLoss: 720.7411\n",
      "Training Epoch: 2 [150/36450]\tLoss: 767.6874\n",
      "Training Epoch: 2 [200/36450]\tLoss: 782.8267\n",
      "Training Epoch: 2 [250/36450]\tLoss: 817.5101\n",
      "Training Epoch: 2 [300/36450]\tLoss: 757.1216\n",
      "Training Epoch: 2 [350/36450]\tLoss: 769.0738\n",
      "Training Epoch: 2 [400/36450]\tLoss: 749.7116\n",
      "Training Epoch: 2 [450/36450]\tLoss: 845.4246\n",
      "Training Epoch: 2 [500/36450]\tLoss: 704.9022\n",
      "Training Epoch: 2 [550/36450]\tLoss: 806.5869\n",
      "Training Epoch: 2 [600/36450]\tLoss: 816.0615\n",
      "Training Epoch: 2 [650/36450]\tLoss: 841.4449\n",
      "Training Epoch: 2 [700/36450]\tLoss: 778.8387\n",
      "Training Epoch: 2 [750/36450]\tLoss: 731.8709\n",
      "Training Epoch: 2 [800/36450]\tLoss: 728.8662\n",
      "Training Epoch: 2 [850/36450]\tLoss: 804.7894\n",
      "Training Epoch: 2 [900/36450]\tLoss: 779.8196\n",
      "Training Epoch: 2 [950/36450]\tLoss: 800.5362\n",
      "Training Epoch: 2 [1000/36450]\tLoss: 761.0322\n",
      "Training Epoch: 2 [1050/36450]\tLoss: 768.1744\n",
      "Training Epoch: 2 [1100/36450]\tLoss: 746.0226\n",
      "Training Epoch: 2 [1150/36450]\tLoss: 780.9592\n",
      "Training Epoch: 2 [1200/36450]\tLoss: 713.0953\n",
      "Training Epoch: 2 [1250/36450]\tLoss: 767.0503\n",
      "Training Epoch: 2 [1300/36450]\tLoss: 824.6545\n",
      "Training Epoch: 2 [1350/36450]\tLoss: 789.1542\n",
      "Training Epoch: 2 [1400/36450]\tLoss: 778.8000\n",
      "Training Epoch: 2 [1450/36450]\tLoss: 747.1051\n",
      "Training Epoch: 2 [1500/36450]\tLoss: 808.8815\n",
      "Training Epoch: 2 [1550/36450]\tLoss: 794.7495\n",
      "Training Epoch: 2 [1600/36450]\tLoss: 744.9886\n",
      "Training Epoch: 2 [1650/36450]\tLoss: 725.8458\n",
      "Training Epoch: 2 [1700/36450]\tLoss: 726.5046\n",
      "Training Epoch: 2 [1750/36450]\tLoss: 795.8529\n",
      "Training Epoch: 2 [1800/36450]\tLoss: 802.2338\n",
      "Training Epoch: 2 [1850/36450]\tLoss: 782.2759\n",
      "Training Epoch: 2 [1900/36450]\tLoss: 780.8942\n",
      "Training Epoch: 2 [1950/36450]\tLoss: 767.2724\n",
      "Training Epoch: 2 [2000/36450]\tLoss: 721.8802\n",
      "Training Epoch: 2 [2050/36450]\tLoss: 761.0074\n",
      "Training Epoch: 2 [2100/36450]\tLoss: 736.2480\n",
      "Training Epoch: 2 [2150/36450]\tLoss: 769.9025\n",
      "Training Epoch: 2 [2200/36450]\tLoss: 766.4247\n",
      "Training Epoch: 2 [2250/36450]\tLoss: 751.8506\n",
      "Training Epoch: 2 [2300/36450]\tLoss: 797.2932\n",
      "Training Epoch: 2 [2350/36450]\tLoss: 831.3557\n",
      "Training Epoch: 2 [2400/36450]\tLoss: 795.3627\n",
      "Training Epoch: 2 [2450/36450]\tLoss: 762.6167\n",
      "Training Epoch: 2 [2500/36450]\tLoss: 758.7838\n",
      "Training Epoch: 2 [2550/36450]\tLoss: 776.5082\n",
      "Training Epoch: 2 [2600/36450]\tLoss: 825.2606\n",
      "Training Epoch: 2 [2650/36450]\tLoss: 724.7605\n",
      "Training Epoch: 2 [2700/36450]\tLoss: 705.5038\n",
      "Training Epoch: 2 [2750/36450]\tLoss: 796.1271\n",
      "Training Epoch: 2 [2800/36450]\tLoss: 805.3895\n",
      "Training Epoch: 2 [2850/36450]\tLoss: 750.8062\n",
      "Training Epoch: 2 [2900/36450]\tLoss: 761.2456\n",
      "Training Epoch: 2 [2950/36450]\tLoss: 753.8181\n",
      "Training Epoch: 2 [3000/36450]\tLoss: 754.1430\n",
      "Training Epoch: 2 [3050/36450]\tLoss: 774.1075\n",
      "Training Epoch: 2 [3100/36450]\tLoss: 729.4833\n",
      "Training Epoch: 2 [3150/36450]\tLoss: 711.7053\n",
      "Training Epoch: 2 [3200/36450]\tLoss: 811.6216\n",
      "Training Epoch: 2 [3250/36450]\tLoss: 765.8003\n",
      "Training Epoch: 2 [3300/36450]\tLoss: 751.9749\n",
      "Training Epoch: 2 [3350/36450]\tLoss: 764.7376\n",
      "Training Epoch: 2 [3400/36450]\tLoss: 775.1395\n",
      "Training Epoch: 2 [3450/36450]\tLoss: 779.6634\n",
      "Training Epoch: 2 [3500/36450]\tLoss: 746.4253\n",
      "Training Epoch: 2 [3550/36450]\tLoss: 702.0059\n",
      "Training Epoch: 2 [3600/36450]\tLoss: 717.9980\n",
      "Training Epoch: 2 [3650/36450]\tLoss: 783.3717\n",
      "Training Epoch: 2 [3700/36450]\tLoss: 739.5376\n",
      "Training Epoch: 2 [3750/36450]\tLoss: 802.1531\n",
      "Training Epoch: 2 [3800/36450]\tLoss: 786.8610\n",
      "Training Epoch: 2 [3850/36450]\tLoss: 780.7814\n",
      "Training Epoch: 2 [3900/36450]\tLoss: 767.6782\n",
      "Training Epoch: 2 [3950/36450]\tLoss: 698.4525\n",
      "Training Epoch: 2 [4000/36450]\tLoss: 721.8142\n",
      "Training Epoch: 2 [4050/36450]\tLoss: 790.8782\n",
      "Training Epoch: 2 [4100/36450]\tLoss: 753.1946\n",
      "Training Epoch: 2 [4150/36450]\tLoss: 747.3132\n",
      "Training Epoch: 2 [4200/36450]\tLoss: 721.8240\n",
      "Training Epoch: 2 [4250/36450]\tLoss: 795.3265\n",
      "Training Epoch: 2 [4300/36450]\tLoss: 749.1960\n",
      "Training Epoch: 2 [4350/36450]\tLoss: 761.4421\n",
      "Training Epoch: 2 [4400/36450]\tLoss: 792.7312\n",
      "Training Epoch: 2 [4450/36450]\tLoss: 695.2545\n",
      "Training Epoch: 2 [4500/36450]\tLoss: 720.2388\n",
      "Training Epoch: 2 [4550/36450]\tLoss: 746.4817\n",
      "Training Epoch: 2 [4600/36450]\tLoss: 699.2115\n",
      "Training Epoch: 2 [4650/36450]\tLoss: 817.5067\n",
      "Training Epoch: 2 [4700/36450]\tLoss: 815.6673\n",
      "Training Epoch: 2 [4750/36450]\tLoss: 742.6422\n",
      "Training Epoch: 2 [4800/36450]\tLoss: 694.1988\n",
      "Training Epoch: 2 [4850/36450]\tLoss: 809.1473\n",
      "Training Epoch: 2 [4900/36450]\tLoss: 730.3549\n",
      "Training Epoch: 2 [4950/36450]\tLoss: 719.9377\n",
      "Training Epoch: 2 [5000/36450]\tLoss: 793.8564\n",
      "Training Epoch: 2 [5050/36450]\tLoss: 718.3918\n",
      "Training Epoch: 2 [5100/36450]\tLoss: 743.8510\n",
      "Training Epoch: 2 [5150/36450]\tLoss: 716.3867\n",
      "Training Epoch: 2 [5200/36450]\tLoss: 742.8831\n",
      "Training Epoch: 2 [5250/36450]\tLoss: 761.8676\n",
      "Training Epoch: 2 [5300/36450]\tLoss: 779.3870\n",
      "Training Epoch: 2 [5350/36450]\tLoss: 760.3692\n",
      "Training Epoch: 2 [5400/36450]\tLoss: 741.0396\n",
      "Training Epoch: 2 [5450/36450]\tLoss: 722.3246\n",
      "Training Epoch: 2 [5500/36450]\tLoss: 713.6108\n",
      "Training Epoch: 2 [5550/36450]\tLoss: 728.2947\n",
      "Training Epoch: 2 [5600/36450]\tLoss: 771.2024\n",
      "Training Epoch: 2 [5650/36450]\tLoss: 745.8064\n",
      "Training Epoch: 2 [5700/36450]\tLoss: 775.7542\n",
      "Training Epoch: 2 [5750/36450]\tLoss: 774.4200\n",
      "Training Epoch: 2 [5800/36450]\tLoss: 785.1274\n",
      "Training Epoch: 2 [5850/36450]\tLoss: 733.0620\n",
      "Training Epoch: 2 [5900/36450]\tLoss: 725.2540\n",
      "Training Epoch: 2 [5950/36450]\tLoss: 738.3676\n",
      "Training Epoch: 2 [6000/36450]\tLoss: 808.5484\n",
      "Training Epoch: 2 [6050/36450]\tLoss: 736.7534\n",
      "Training Epoch: 2 [6100/36450]\tLoss: 754.3130\n",
      "Training Epoch: 2 [6150/36450]\tLoss: 698.4462\n",
      "Training Epoch: 2 [6200/36450]\tLoss: 767.4119\n",
      "Training Epoch: 2 [6250/36450]\tLoss: 731.5670\n",
      "Training Epoch: 2 [6300/36450]\tLoss: 729.2269\n",
      "Training Epoch: 2 [6350/36450]\tLoss: 806.2755\n",
      "Training Epoch: 2 [6400/36450]\tLoss: 692.7668\n",
      "Training Epoch: 2 [6450/36450]\tLoss: 731.2736\n",
      "Training Epoch: 2 [6500/36450]\tLoss: 738.0481\n",
      "Training Epoch: 2 [6550/36450]\tLoss: 762.1444\n",
      "Training Epoch: 2 [6600/36450]\tLoss: 701.4752\n",
      "Training Epoch: 2 [6650/36450]\tLoss: 766.0229\n",
      "Training Epoch: 2 [6700/36450]\tLoss: 720.7996\n",
      "Training Epoch: 2 [6750/36450]\tLoss: 683.4293\n",
      "Training Epoch: 2 [6800/36450]\tLoss: 768.3242\n",
      "Training Epoch: 2 [6850/36450]\tLoss: 784.4839\n",
      "Training Epoch: 2 [6900/36450]\tLoss: 803.9919\n",
      "Training Epoch: 2 [6950/36450]\tLoss: 741.1591\n",
      "Training Epoch: 2 [7000/36450]\tLoss: 753.8538\n",
      "Training Epoch: 2 [7050/36450]\tLoss: 788.3491\n",
      "Training Epoch: 2 [7100/36450]\tLoss: 747.5954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [7150/36450]\tLoss: 691.7972\n",
      "Training Epoch: 2 [7200/36450]\tLoss: 763.4047\n",
      "Training Epoch: 2 [7250/36450]\tLoss: 763.8364\n",
      "Training Epoch: 2 [7300/36450]\tLoss: 784.1341\n",
      "Training Epoch: 2 [7350/36450]\tLoss: 697.1221\n",
      "Training Epoch: 2 [7400/36450]\tLoss: 752.8517\n",
      "Training Epoch: 2 [7450/36450]\tLoss: 771.5412\n",
      "Training Epoch: 2 [7500/36450]\tLoss: 801.4147\n",
      "Training Epoch: 2 [7550/36450]\tLoss: 739.9390\n",
      "Training Epoch: 2 [7600/36450]\tLoss: 748.4344\n",
      "Training Epoch: 2 [7650/36450]\tLoss: 742.6821\n",
      "Training Epoch: 2 [7700/36450]\tLoss: 690.9766\n",
      "Training Epoch: 2 [7750/36450]\tLoss: 781.6192\n",
      "Training Epoch: 2 [7800/36450]\tLoss: 706.5283\n",
      "Training Epoch: 2 [7850/36450]\tLoss: 752.8760\n",
      "Training Epoch: 2 [7900/36450]\tLoss: 720.9376\n",
      "Training Epoch: 2 [7950/36450]\tLoss: 683.9084\n",
      "Training Epoch: 2 [8000/36450]\tLoss: 716.8273\n",
      "Training Epoch: 2 [8050/36450]\tLoss: 690.4471\n",
      "Training Epoch: 2 [8100/36450]\tLoss: 665.1628\n",
      "Training Epoch: 2 [8150/36450]\tLoss: 772.3748\n",
      "Training Epoch: 2 [8200/36450]\tLoss: 805.4302\n",
      "Training Epoch: 2 [8250/36450]\tLoss: 762.2088\n",
      "Training Epoch: 2 [8300/36450]\tLoss: 739.7236\n",
      "Training Epoch: 2 [8350/36450]\tLoss: 711.6085\n",
      "Training Epoch: 2 [8400/36450]\tLoss: 709.4979\n",
      "Training Epoch: 2 [8450/36450]\tLoss: 751.4396\n",
      "Training Epoch: 2 [8500/36450]\tLoss: 730.7206\n",
      "Training Epoch: 2 [8550/36450]\tLoss: 774.1718\n",
      "Training Epoch: 2 [8600/36450]\tLoss: 765.3529\n",
      "Training Epoch: 2 [8650/36450]\tLoss: 784.3264\n",
      "Training Epoch: 2 [8700/36450]\tLoss: 771.6527\n",
      "Training Epoch: 2 [8750/36450]\tLoss: 781.8076\n",
      "Training Epoch: 2 [8800/36450]\tLoss: 736.1844\n",
      "Training Epoch: 2 [8850/36450]\tLoss: 745.3130\n",
      "Training Epoch: 2 [8900/36450]\tLoss: 753.6924\n",
      "Training Epoch: 2 [8950/36450]\tLoss: 690.7629\n",
      "Training Epoch: 2 [9000/36450]\tLoss: 733.5934\n",
      "Training Epoch: 2 [9050/36450]\tLoss: 748.7156\n",
      "Training Epoch: 2 [9100/36450]\tLoss: 721.8673\n",
      "Training Epoch: 2 [9150/36450]\tLoss: 802.6500\n",
      "Training Epoch: 2 [9200/36450]\tLoss: 741.3890\n",
      "Training Epoch: 2 [9250/36450]\tLoss: 736.9563\n",
      "Training Epoch: 2 [9300/36450]\tLoss: 746.7637\n",
      "Training Epoch: 2 [9350/36450]\tLoss: 710.4831\n",
      "Training Epoch: 2 [9400/36450]\tLoss: 653.5811\n",
      "Training Epoch: 2 [9450/36450]\tLoss: 737.4835\n",
      "Training Epoch: 2 [9500/36450]\tLoss: 782.1956\n",
      "Training Epoch: 2 [9550/36450]\tLoss: 718.1830\n",
      "Training Epoch: 2 [9600/36450]\tLoss: 691.7092\n",
      "Training Epoch: 2 [9650/36450]\tLoss: 796.4263\n",
      "Training Epoch: 2 [9700/36450]\tLoss: 735.8871\n",
      "Training Epoch: 2 [9750/36450]\tLoss: 716.1388\n",
      "Training Epoch: 2 [9800/36450]\tLoss: 755.3770\n",
      "Training Epoch: 2 [9850/36450]\tLoss: 687.1379\n",
      "Training Epoch: 2 [9900/36450]\tLoss: 764.0151\n",
      "Training Epoch: 2 [9950/36450]\tLoss: 692.0467\n",
      "Training Epoch: 2 [10000/36450]\tLoss: 748.6157\n",
      "Training Epoch: 2 [10050/36450]\tLoss: 747.6396\n",
      "Training Epoch: 2 [10100/36450]\tLoss: 727.0687\n",
      "Training Epoch: 2 [10150/36450]\tLoss: 751.5201\n",
      "Training Epoch: 2 [10200/36450]\tLoss: 760.8933\n",
      "Training Epoch: 2 [10250/36450]\tLoss: 821.2857\n",
      "Training Epoch: 2 [10300/36450]\tLoss: 687.9075\n",
      "Training Epoch: 2 [10350/36450]\tLoss: 740.5997\n",
      "Training Epoch: 2 [10400/36450]\tLoss: 757.3550\n",
      "Training Epoch: 2 [10450/36450]\tLoss: 698.7408\n",
      "Training Epoch: 2 [10500/36450]\tLoss: 686.5482\n",
      "Training Epoch: 2 [10550/36450]\tLoss: 705.9516\n",
      "Training Epoch: 2 [10600/36450]\tLoss: 734.2772\n",
      "Training Epoch: 2 [10650/36450]\tLoss: 721.9075\n",
      "Training Epoch: 2 [10700/36450]\tLoss: 775.6118\n",
      "Training Epoch: 2 [10750/36450]\tLoss: 677.1025\n",
      "Training Epoch: 2 [10800/36450]\tLoss: 707.3465\n",
      "Training Epoch: 2 [10850/36450]\tLoss: 722.6912\n",
      "Training Epoch: 2 [10900/36450]\tLoss: 713.6060\n",
      "Training Epoch: 2 [10950/36450]\tLoss: 742.0519\n",
      "Training Epoch: 2 [11000/36450]\tLoss: 754.5670\n",
      "Training Epoch: 2 [11050/36450]\tLoss: 777.4508\n",
      "Training Epoch: 2 [11100/36450]\tLoss: 691.9042\n",
      "Training Epoch: 2 [11150/36450]\tLoss: 699.0867\n",
      "Training Epoch: 2 [11200/36450]\tLoss: 733.3887\n",
      "Training Epoch: 2 [11250/36450]\tLoss: 707.1661\n",
      "Training Epoch: 2 [11300/36450]\tLoss: 705.0048\n",
      "Training Epoch: 2 [11350/36450]\tLoss: 726.8326\n",
      "Training Epoch: 2 [11400/36450]\tLoss: 737.4102\n",
      "Training Epoch: 2 [11450/36450]\tLoss: 693.4632\n",
      "Training Epoch: 2 [11500/36450]\tLoss: 730.7266\n",
      "Training Epoch: 2 [11550/36450]\tLoss: 737.1222\n",
      "Training Epoch: 2 [11600/36450]\tLoss: 754.0143\n",
      "Training Epoch: 2 [11650/36450]\tLoss: 737.9620\n",
      "Training Epoch: 2 [11700/36450]\tLoss: 786.8846\n",
      "Training Epoch: 2 [11750/36450]\tLoss: 722.4759\n",
      "Training Epoch: 2 [11800/36450]\tLoss: 723.3858\n",
      "Training Epoch: 2 [11850/36450]\tLoss: 727.7329\n",
      "Training Epoch: 2 [11900/36450]\tLoss: 777.3990\n",
      "Training Epoch: 2 [11950/36450]\tLoss: 693.6790\n",
      "Training Epoch: 2 [12000/36450]\tLoss: 689.0762\n",
      "Training Epoch: 2 [12050/36450]\tLoss: 696.4874\n",
      "Training Epoch: 2 [12100/36450]\tLoss: 711.2875\n",
      "Training Epoch: 2 [12150/36450]\tLoss: 712.4780\n",
      "Training Epoch: 2 [12200/36450]\tLoss: 771.8403\n",
      "Training Epoch: 2 [12250/36450]\tLoss: 703.3226\n",
      "Training Epoch: 2 [12300/36450]\tLoss: 751.8961\n",
      "Training Epoch: 2 [12350/36450]\tLoss: 777.7374\n",
      "Training Epoch: 2 [12400/36450]\tLoss: 667.4108\n",
      "Training Epoch: 2 [12450/36450]\tLoss: 701.6040\n",
      "Training Epoch: 2 [12500/36450]\tLoss: 818.2614\n",
      "Training Epoch: 2 [12550/36450]\tLoss: 715.3463\n",
      "Training Epoch: 2 [12600/36450]\tLoss: 747.6059\n",
      "Training Epoch: 2 [12650/36450]\tLoss: 750.3691\n",
      "Training Epoch: 2 [12700/36450]\tLoss: 750.2093\n",
      "Training Epoch: 2 [12750/36450]\tLoss: 701.3713\n",
      "Training Epoch: 2 [12800/36450]\tLoss: 780.4409\n",
      "Training Epoch: 2 [12850/36450]\tLoss: 706.5016\n",
      "Training Epoch: 2 [12900/36450]\tLoss: 659.5438\n",
      "Training Epoch: 2 [12950/36450]\tLoss: 766.0330\n",
      "Training Epoch: 2 [13000/36450]\tLoss: 718.3275\n",
      "Training Epoch: 2 [13050/36450]\tLoss: 734.2271\n",
      "Training Epoch: 2 [13100/36450]\tLoss: 671.1230\n",
      "Training Epoch: 2 [13150/36450]\tLoss: 786.2407\n",
      "Training Epoch: 2 [13200/36450]\tLoss: 704.9699\n",
      "Training Epoch: 2 [13250/36450]\tLoss: 719.5573\n",
      "Training Epoch: 2 [13300/36450]\tLoss: 717.5539\n",
      "Training Epoch: 2 [13350/36450]\tLoss: 706.3157\n",
      "Training Epoch: 2 [13400/36450]\tLoss: 773.3394\n",
      "Training Epoch: 2 [13450/36450]\tLoss: 751.6617\n",
      "Training Epoch: 2 [13500/36450]\tLoss: 699.2923\n",
      "Training Epoch: 2 [13550/36450]\tLoss: 750.3673\n",
      "Training Epoch: 2 [13600/36450]\tLoss: 730.4260\n",
      "Training Epoch: 2 [13650/36450]\tLoss: 711.8042\n",
      "Training Epoch: 2 [13700/36450]\tLoss: 717.2361\n",
      "Training Epoch: 2 [13750/36450]\tLoss: 673.2720\n",
      "Training Epoch: 2 [13800/36450]\tLoss: 697.2493\n",
      "Training Epoch: 2 [13850/36450]\tLoss: 712.2032\n",
      "Training Epoch: 2 [13900/36450]\tLoss: 762.5929\n",
      "Training Epoch: 2 [13950/36450]\tLoss: 712.4441\n",
      "Training Epoch: 2 [14000/36450]\tLoss: 769.7633\n",
      "Training Epoch: 2 [14050/36450]\tLoss: 724.0727\n",
      "Training Epoch: 2 [14100/36450]\tLoss: 745.4124\n",
      "Training Epoch: 2 [14150/36450]\tLoss: 686.1951\n",
      "Training Epoch: 2 [14200/36450]\tLoss: 739.8524\n",
      "Training Epoch: 2 [14250/36450]\tLoss: 740.5192\n",
      "Training Epoch: 2 [14300/36450]\tLoss: 698.1777\n",
      "Training Epoch: 2 [14350/36450]\tLoss: 729.9825\n",
      "Training Epoch: 2 [14400/36450]\tLoss: 676.5245\n",
      "Training Epoch: 2 [14450/36450]\tLoss: 716.3080\n",
      "Training Epoch: 2 [14500/36450]\tLoss: 705.6774\n",
      "Training Epoch: 2 [14550/36450]\tLoss: 729.6905\n",
      "Training Epoch: 2 [14600/36450]\tLoss: 696.6116\n",
      "Training Epoch: 2 [14650/36450]\tLoss: 702.6320\n",
      "Training Epoch: 2 [14700/36450]\tLoss: 754.2491\n",
      "Training Epoch: 2 [14750/36450]\tLoss: 719.0436\n",
      "Training Epoch: 2 [14800/36450]\tLoss: 714.6857\n",
      "Training Epoch: 2 [14850/36450]\tLoss: 755.2064\n",
      "Training Epoch: 2 [14900/36450]\tLoss: 713.3495\n",
      "Training Epoch: 2 [14950/36450]\tLoss: 713.3740\n",
      "Training Epoch: 2 [15000/36450]\tLoss: 745.0964\n",
      "Training Epoch: 2 [15050/36450]\tLoss: 717.0314\n",
      "Training Epoch: 2 [15100/36450]\tLoss: 724.0219\n",
      "Training Epoch: 2 [15150/36450]\tLoss: 688.1036\n",
      "Training Epoch: 2 [15200/36450]\tLoss: 716.0140\n",
      "Training Epoch: 2 [15250/36450]\tLoss: 758.6002\n",
      "Training Epoch: 2 [15300/36450]\tLoss: 710.8093\n",
      "Training Epoch: 2 [15350/36450]\tLoss: 706.2693\n",
      "Training Epoch: 2 [15400/36450]\tLoss: 677.2779\n",
      "Training Epoch: 2 [15450/36450]\tLoss: 769.3464\n",
      "Training Epoch: 2 [15500/36450]\tLoss: 732.6219\n",
      "Training Epoch: 2 [15550/36450]\tLoss: 710.1279\n",
      "Training Epoch: 2 [15600/36450]\tLoss: 692.8690\n",
      "Training Epoch: 2 [15650/36450]\tLoss: 671.7606\n",
      "Training Epoch: 2 [15700/36450]\tLoss: 714.9483\n",
      "Training Epoch: 2 [15750/36450]\tLoss: 747.1794\n",
      "Training Epoch: 2 [15800/36450]\tLoss: 666.6108\n",
      "Training Epoch: 2 [15850/36450]\tLoss: 674.5599\n",
      "Training Epoch: 2 [15900/36450]\tLoss: 704.8920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [15950/36450]\tLoss: 671.1746\n",
      "Training Epoch: 2 [16000/36450]\tLoss: 729.7706\n",
      "Training Epoch: 2 [16050/36450]\tLoss: 746.9202\n",
      "Training Epoch: 2 [16100/36450]\tLoss: 694.8723\n",
      "Training Epoch: 2 [16150/36450]\tLoss: 722.4502\n",
      "Training Epoch: 2 [16200/36450]\tLoss: 714.3944\n",
      "Training Epoch: 2 [16250/36450]\tLoss: 695.5004\n",
      "Training Epoch: 2 [16300/36450]\tLoss: 742.0571\n",
      "Training Epoch: 2 [16350/36450]\tLoss: 694.7206\n",
      "Training Epoch: 2 [16400/36450]\tLoss: 722.8138\n",
      "Training Epoch: 2 [16450/36450]\tLoss: 690.7150\n",
      "Training Epoch: 2 [16500/36450]\tLoss: 745.3963\n",
      "Training Epoch: 2 [16550/36450]\tLoss: 665.3122\n",
      "Training Epoch: 2 [16600/36450]\tLoss: 758.5423\n",
      "Training Epoch: 2 [16650/36450]\tLoss: 661.5992\n",
      "Training Epoch: 2 [16700/36450]\tLoss: 721.3086\n",
      "Training Epoch: 2 [16750/36450]\tLoss: 733.4033\n",
      "Training Epoch: 2 [16800/36450]\tLoss: 703.9792\n",
      "Training Epoch: 2 [16850/36450]\tLoss: 645.8421\n",
      "Training Epoch: 2 [16900/36450]\tLoss: 691.7899\n",
      "Training Epoch: 2 [16950/36450]\tLoss: 697.5223\n",
      "Training Epoch: 2 [17000/36450]\tLoss: 660.1270\n",
      "Training Epoch: 2 [17050/36450]\tLoss: 741.9658\n",
      "Training Epoch: 2 [17100/36450]\tLoss: 697.9507\n",
      "Training Epoch: 2 [17150/36450]\tLoss: 745.9608\n",
      "Training Epoch: 2 [17200/36450]\tLoss: 693.1212\n",
      "Training Epoch: 2 [17250/36450]\tLoss: 707.0172\n",
      "Training Epoch: 2 [17300/36450]\tLoss: 670.2264\n",
      "Training Epoch: 2 [17350/36450]\tLoss: 683.1446\n",
      "Training Epoch: 2 [17400/36450]\tLoss: 749.0994\n",
      "Training Epoch: 2 [17450/36450]\tLoss: 660.5222\n",
      "Training Epoch: 2 [17500/36450]\tLoss: 740.6685\n",
      "Training Epoch: 2 [17550/36450]\tLoss: 713.2440\n",
      "Training Epoch: 2 [17600/36450]\tLoss: 699.0930\n",
      "Training Epoch: 2 [17650/36450]\tLoss: 715.1056\n",
      "Training Epoch: 2 [17700/36450]\tLoss: 688.4803\n",
      "Training Epoch: 2 [17750/36450]\tLoss: 723.9974\n",
      "Training Epoch: 2 [17800/36450]\tLoss: 711.1933\n",
      "Training Epoch: 2 [17850/36450]\tLoss: 775.2572\n",
      "Training Epoch: 2 [17900/36450]\tLoss: 679.4067\n",
      "Training Epoch: 2 [17950/36450]\tLoss: 705.3627\n",
      "Training Epoch: 2 [18000/36450]\tLoss: 665.6321\n",
      "Training Epoch: 2 [18050/36450]\tLoss: 679.1750\n",
      "Training Epoch: 2 [18100/36450]\tLoss: 686.8875\n",
      "Training Epoch: 2 [18150/36450]\tLoss: 748.3107\n",
      "Training Epoch: 2 [18200/36450]\tLoss: 663.2244\n",
      "Training Epoch: 2 [18250/36450]\tLoss: 717.1793\n",
      "Training Epoch: 2 [18300/36450]\tLoss: 723.9156\n",
      "Training Epoch: 2 [18350/36450]\tLoss: 718.8511\n",
      "Training Epoch: 2 [18400/36450]\tLoss: 734.7850\n",
      "Training Epoch: 2 [18450/36450]\tLoss: 685.0560\n",
      "Training Epoch: 2 [18500/36450]\tLoss: 749.1473\n",
      "Training Epoch: 2 [18550/36450]\tLoss: 756.6583\n",
      "Training Epoch: 2 [18600/36450]\tLoss: 707.9571\n",
      "Training Epoch: 2 [18650/36450]\tLoss: 713.2877\n",
      "Training Epoch: 2 [18700/36450]\tLoss: 703.7520\n",
      "Training Epoch: 2 [18750/36450]\tLoss: 693.5494\n",
      "Training Epoch: 2 [18800/36450]\tLoss: 716.3271\n",
      "Training Epoch: 2 [18850/36450]\tLoss: 689.0618\n",
      "Training Epoch: 2 [18900/36450]\tLoss: 695.9805\n",
      "Training Epoch: 2 [18950/36450]\tLoss: 712.5901\n",
      "Training Epoch: 2 [19000/36450]\tLoss: 692.9122\n",
      "Training Epoch: 2 [19050/36450]\tLoss: 700.0276\n",
      "Training Epoch: 2 [19100/36450]\tLoss: 740.7338\n",
      "Training Epoch: 2 [19150/36450]\tLoss: 753.4686\n",
      "Training Epoch: 2 [19200/36450]\tLoss: 720.3200\n",
      "Training Epoch: 2 [19250/36450]\tLoss: 673.7641\n",
      "Training Epoch: 2 [19300/36450]\tLoss: 668.5924\n",
      "Training Epoch: 2 [19350/36450]\tLoss: 718.9757\n",
      "Training Epoch: 2 [19400/36450]\tLoss: 747.2708\n",
      "Training Epoch: 2 [19450/36450]\tLoss: 714.0917\n",
      "Training Epoch: 2 [19500/36450]\tLoss: 686.1525\n",
      "Training Epoch: 2 [19550/36450]\tLoss: 691.8044\n",
      "Training Epoch: 2 [19600/36450]\tLoss: 699.2169\n",
      "Training Epoch: 2 [19650/36450]\tLoss: 738.4081\n",
      "Training Epoch: 2 [19700/36450]\tLoss: 664.8491\n",
      "Training Epoch: 2 [19750/36450]\tLoss: 715.3887\n",
      "Training Epoch: 2 [19800/36450]\tLoss: 659.2803\n",
      "Training Epoch: 2 [19850/36450]\tLoss: 695.3173\n",
      "Training Epoch: 2 [19900/36450]\tLoss: 753.2377\n",
      "Training Epoch: 2 [19950/36450]\tLoss: 652.4042\n",
      "Training Epoch: 2 [20000/36450]\tLoss: 687.5164\n",
      "Training Epoch: 2 [20050/36450]\tLoss: 704.8473\n",
      "Training Epoch: 2 [20100/36450]\tLoss: 692.8945\n",
      "Training Epoch: 2 [20150/36450]\tLoss: 747.8848\n",
      "Training Epoch: 2 [20200/36450]\tLoss: 753.1047\n",
      "Training Epoch: 2 [20250/36450]\tLoss: 696.5927\n",
      "Training Epoch: 2 [20300/36450]\tLoss: 675.3553\n",
      "Training Epoch: 2 [20350/36450]\tLoss: 744.2416\n",
      "Training Epoch: 2 [20400/36450]\tLoss: 684.0322\n",
      "Training Epoch: 2 [20450/36450]\tLoss: 705.8431\n",
      "Training Epoch: 2 [20500/36450]\tLoss: 766.3503\n",
      "Training Epoch: 2 [20550/36450]\tLoss: 724.4844\n",
      "Training Epoch: 2 [20600/36450]\tLoss: 689.7089\n",
      "Training Epoch: 2 [20650/36450]\tLoss: 671.0043\n",
      "Training Epoch: 2 [20700/36450]\tLoss: 710.7502\n",
      "Training Epoch: 2 [20750/36450]\tLoss: 681.8401\n",
      "Training Epoch: 2 [20800/36450]\tLoss: 687.3146\n",
      "Training Epoch: 2 [20850/36450]\tLoss: 740.2360\n",
      "Training Epoch: 2 [20900/36450]\tLoss: 744.3353\n",
      "Training Epoch: 2 [20950/36450]\tLoss: 737.3083\n",
      "Training Epoch: 2 [21000/36450]\tLoss: 708.1437\n",
      "Training Epoch: 2 [21050/36450]\tLoss: 717.9400\n",
      "Training Epoch: 2 [21100/36450]\tLoss: 685.1180\n",
      "Training Epoch: 2 [21150/36450]\tLoss: 641.7266\n",
      "Training Epoch: 2 [21200/36450]\tLoss: 756.3212\n",
      "Training Epoch: 2 [21250/36450]\tLoss: 673.1297\n",
      "Training Epoch: 2 [21300/36450]\tLoss: 683.4427\n",
      "Training Epoch: 2 [21350/36450]\tLoss: 669.2518\n",
      "Training Epoch: 2 [21400/36450]\tLoss: 679.2137\n",
      "Training Epoch: 2 [21450/36450]\tLoss: 735.9619\n",
      "Training Epoch: 2 [21500/36450]\tLoss: 711.7510\n",
      "Training Epoch: 2 [21550/36450]\tLoss: 681.3665\n",
      "Training Epoch: 2 [21600/36450]\tLoss: 663.8327\n",
      "Training Epoch: 2 [21650/36450]\tLoss: 713.0332\n",
      "Training Epoch: 2 [21700/36450]\tLoss: 718.1049\n",
      "Training Epoch: 2 [21750/36450]\tLoss: 671.1437\n",
      "Training Epoch: 2 [21800/36450]\tLoss: 732.5591\n",
      "Training Epoch: 2 [21850/36450]\tLoss: 687.3576\n",
      "Training Epoch: 2 [21900/36450]\tLoss: 685.2470\n",
      "Training Epoch: 2 [21950/36450]\tLoss: 689.7516\n",
      "Training Epoch: 2 [22000/36450]\tLoss: 738.3604\n",
      "Training Epoch: 2 [22050/36450]\tLoss: 698.4573\n",
      "Training Epoch: 2 [22100/36450]\tLoss: 746.9376\n",
      "Training Epoch: 2 [22150/36450]\tLoss: 698.3408\n",
      "Training Epoch: 2 [22200/36450]\tLoss: 667.5022\n",
      "Training Epoch: 2 [22250/36450]\tLoss: 649.7128\n",
      "Training Epoch: 2 [22300/36450]\tLoss: 717.1484\n",
      "Training Epoch: 2 [22350/36450]\tLoss: 791.1082\n",
      "Training Epoch: 2 [22400/36450]\tLoss: 683.2072\n",
      "Training Epoch: 2 [22450/36450]\tLoss: 650.4230\n",
      "Training Epoch: 2 [22500/36450]\tLoss: 682.8494\n",
      "Training Epoch: 2 [22550/36450]\tLoss: 681.4778\n",
      "Training Epoch: 2 [22600/36450]\tLoss: 714.8124\n",
      "Training Epoch: 2 [22650/36450]\tLoss: 652.9949\n",
      "Training Epoch: 2 [22700/36450]\tLoss: 703.7298\n",
      "Training Epoch: 2 [22750/36450]\tLoss: 745.0272\n",
      "Training Epoch: 2 [22800/36450]\tLoss: 700.6244\n",
      "Training Epoch: 2 [22850/36450]\tLoss: 716.0710\n",
      "Training Epoch: 2 [22900/36450]\tLoss: 726.5040\n",
      "Training Epoch: 2 [22950/36450]\tLoss: 638.1260\n",
      "Training Epoch: 2 [23000/36450]\tLoss: 655.4119\n",
      "Training Epoch: 2 [23050/36450]\tLoss: 709.0322\n",
      "Training Epoch: 2 [23100/36450]\tLoss: 724.8663\n",
      "Training Epoch: 2 [23150/36450]\tLoss: 650.7383\n",
      "Training Epoch: 2 [23200/36450]\tLoss: 739.3290\n",
      "Training Epoch: 2 [23250/36450]\tLoss: 692.0515\n",
      "Training Epoch: 2 [23300/36450]\tLoss: 684.8610\n",
      "Training Epoch: 2 [23350/36450]\tLoss: 675.4658\n",
      "Training Epoch: 2 [23400/36450]\tLoss: 670.3237\n",
      "Training Epoch: 2 [23450/36450]\tLoss: 653.7856\n",
      "Training Epoch: 2 [23500/36450]\tLoss: 708.3455\n",
      "Training Epoch: 2 [23550/36450]\tLoss: 727.9520\n",
      "Training Epoch: 2 [23600/36450]\tLoss: 710.8833\n",
      "Training Epoch: 2 [23650/36450]\tLoss: 771.7771\n",
      "Training Epoch: 2 [23700/36450]\tLoss: 689.1544\n",
      "Training Epoch: 2 [23750/36450]\tLoss: 731.1714\n",
      "Training Epoch: 2 [23800/36450]\tLoss: 743.5234\n",
      "Training Epoch: 2 [23850/36450]\tLoss: 685.4646\n",
      "Training Epoch: 2 [23900/36450]\tLoss: 659.7206\n",
      "Training Epoch: 2 [23950/36450]\tLoss: 698.5995\n",
      "Training Epoch: 2 [24000/36450]\tLoss: 667.3541\n",
      "Training Epoch: 2 [24050/36450]\tLoss: 642.3303\n",
      "Training Epoch: 2 [24100/36450]\tLoss: 649.5407\n",
      "Training Epoch: 2 [24150/36450]\tLoss: 676.1603\n",
      "Training Epoch: 2 [24200/36450]\tLoss: 679.0727\n",
      "Training Epoch: 2 [24250/36450]\tLoss: 694.3680\n",
      "Training Epoch: 2 [24300/36450]\tLoss: 669.1138\n",
      "Training Epoch: 2 [24350/36450]\tLoss: 668.1965\n",
      "Training Epoch: 2 [24400/36450]\tLoss: 682.9166\n",
      "Training Epoch: 2 [24450/36450]\tLoss: 696.1123\n",
      "Training Epoch: 2 [24500/36450]\tLoss: 730.8892\n",
      "Training Epoch: 2 [24550/36450]\tLoss: 662.8729\n",
      "Training Epoch: 2 [24600/36450]\tLoss: 706.4754\n",
      "Training Epoch: 2 [24650/36450]\tLoss: 624.3059\n",
      "Training Epoch: 2 [24700/36450]\tLoss: 656.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [24750/36450]\tLoss: 715.1647\n",
      "Training Epoch: 2 [24800/36450]\tLoss: 680.2169\n",
      "Training Epoch: 2 [24850/36450]\tLoss: 675.5620\n",
      "Training Epoch: 2 [24900/36450]\tLoss: 675.1974\n",
      "Training Epoch: 2 [24950/36450]\tLoss: 691.7653\n",
      "Training Epoch: 2 [25000/36450]\tLoss: 773.8466\n",
      "Training Epoch: 2 [25050/36450]\tLoss: 678.1747\n",
      "Training Epoch: 2 [25100/36450]\tLoss: 753.7874\n",
      "Training Epoch: 2 [25150/36450]\tLoss: 669.4763\n",
      "Training Epoch: 2 [25200/36450]\tLoss: 702.9120\n",
      "Training Epoch: 2 [25250/36450]\tLoss: 690.4830\n",
      "Training Epoch: 2 [25300/36450]\tLoss: 681.3601\n",
      "Training Epoch: 2 [25350/36450]\tLoss: 655.9238\n",
      "Training Epoch: 2 [25400/36450]\tLoss: 644.4474\n",
      "Training Epoch: 2 [25450/36450]\tLoss: 686.0731\n",
      "Training Epoch: 2 [25500/36450]\tLoss: 712.5533\n",
      "Training Epoch: 2 [25550/36450]\tLoss: 651.1875\n",
      "Training Epoch: 2 [25600/36450]\tLoss: 730.4465\n",
      "Training Epoch: 2 [25650/36450]\tLoss: 653.4936\n",
      "Training Epoch: 2 [25700/36450]\tLoss: 700.5345\n",
      "Training Epoch: 2 [25750/36450]\tLoss: 726.1206\n",
      "Training Epoch: 2 [25800/36450]\tLoss: 674.9254\n",
      "Training Epoch: 2 [25850/36450]\tLoss: 622.6772\n",
      "Training Epoch: 2 [25900/36450]\tLoss: 698.7736\n",
      "Training Epoch: 2 [25950/36450]\tLoss: 668.5269\n",
      "Training Epoch: 2 [26000/36450]\tLoss: 691.8768\n",
      "Training Epoch: 2 [26050/36450]\tLoss: 674.5438\n",
      "Training Epoch: 2 [26100/36450]\tLoss: 636.2105\n",
      "Training Epoch: 2 [26150/36450]\tLoss: 671.6107\n",
      "Training Epoch: 2 [26200/36450]\tLoss: 653.0236\n",
      "Training Epoch: 2 [26250/36450]\tLoss: 656.9760\n",
      "Training Epoch: 2 [26300/36450]\tLoss: 657.6428\n",
      "Training Epoch: 2 [26350/36450]\tLoss: 726.9891\n",
      "Training Epoch: 2 [26400/36450]\tLoss: 745.7303\n",
      "Training Epoch: 2 [26450/36450]\tLoss: 751.2086\n",
      "Training Epoch: 2 [26500/36450]\tLoss: 699.8422\n",
      "Training Epoch: 2 [26550/36450]\tLoss: 660.0562\n",
      "Training Epoch: 2 [26600/36450]\tLoss: 680.3951\n",
      "Training Epoch: 2 [26650/36450]\tLoss: 672.6566\n",
      "Training Epoch: 2 [26700/36450]\tLoss: 724.4477\n",
      "Training Epoch: 2 [26750/36450]\tLoss: 695.8636\n",
      "Training Epoch: 2 [26800/36450]\tLoss: 682.9357\n",
      "Training Epoch: 2 [26850/36450]\tLoss: 714.6170\n",
      "Training Epoch: 2 [26900/36450]\tLoss: 694.5921\n",
      "Training Epoch: 2 [26950/36450]\tLoss: 671.4446\n",
      "Training Epoch: 2 [27000/36450]\tLoss: 687.9154\n",
      "Training Epoch: 2 [27050/36450]\tLoss: 728.8311\n",
      "Training Epoch: 2 [27100/36450]\tLoss: 688.7444\n",
      "Training Epoch: 2 [27150/36450]\tLoss: 688.1939\n",
      "Training Epoch: 2 [27200/36450]\tLoss: 682.5552\n",
      "Training Epoch: 2 [27250/36450]\tLoss: 691.0424\n",
      "Training Epoch: 2 [27300/36450]\tLoss: 701.7513\n",
      "Training Epoch: 2 [27350/36450]\tLoss: 649.2909\n",
      "Training Epoch: 2 [27400/36450]\tLoss: 738.1246\n",
      "Training Epoch: 2 [27450/36450]\tLoss: 653.0826\n",
      "Training Epoch: 2 [27500/36450]\tLoss: 671.6357\n",
      "Training Epoch: 2 [27550/36450]\tLoss: 659.6686\n",
      "Training Epoch: 2 [27600/36450]\tLoss: 723.7575\n",
      "Training Epoch: 2 [27650/36450]\tLoss: 717.7597\n",
      "Training Epoch: 2 [27700/36450]\tLoss: 627.0220\n",
      "Training Epoch: 2 [27750/36450]\tLoss: 692.4253\n",
      "Training Epoch: 2 [27800/36450]\tLoss: 744.3452\n",
      "Training Epoch: 2 [27850/36450]\tLoss: 649.9634\n",
      "Training Epoch: 2 [27900/36450]\tLoss: 704.5103\n",
      "Training Epoch: 2 [27950/36450]\tLoss: 693.3548\n",
      "Training Epoch: 2 [28000/36450]\tLoss: 691.8244\n",
      "Training Epoch: 2 [28050/36450]\tLoss: 638.7029\n",
      "Training Epoch: 2 [28100/36450]\tLoss: 669.3944\n",
      "Training Epoch: 2 [28150/36450]\tLoss: 742.6833\n",
      "Training Epoch: 2 [28200/36450]\tLoss: 623.5050\n",
      "Training Epoch: 2 [28250/36450]\tLoss: 677.0541\n",
      "Training Epoch: 2 [28300/36450]\tLoss: 650.5645\n",
      "Training Epoch: 2 [28350/36450]\tLoss: 672.1192\n",
      "Training Epoch: 2 [28400/36450]\tLoss: 655.1004\n",
      "Training Epoch: 2 [28450/36450]\tLoss: 690.0383\n",
      "Training Epoch: 2 [28500/36450]\tLoss: 680.3619\n",
      "Training Epoch: 2 [28550/36450]\tLoss: 659.5498\n",
      "Training Epoch: 2 [28600/36450]\tLoss: 684.7238\n",
      "Training Epoch: 2 [28650/36450]\tLoss: 685.2451\n",
      "Training Epoch: 2 [28700/36450]\tLoss: 654.8130\n",
      "Training Epoch: 2 [28750/36450]\tLoss: 676.7852\n",
      "Training Epoch: 2 [28800/36450]\tLoss: 695.1731\n",
      "Training Epoch: 2 [28850/36450]\tLoss: 665.0630\n",
      "Training Epoch: 2 [28900/36450]\tLoss: 625.6370\n",
      "Training Epoch: 2 [28950/36450]\tLoss: 658.6904\n",
      "Training Epoch: 2 [29000/36450]\tLoss: 724.1361\n",
      "Training Epoch: 2 [29050/36450]\tLoss: 646.9256\n",
      "Training Epoch: 2 [29100/36450]\tLoss: 648.1460\n",
      "Training Epoch: 2 [29150/36450]\tLoss: 685.3170\n",
      "Training Epoch: 2 [29200/36450]\tLoss: 688.2003\n",
      "Training Epoch: 2 [29250/36450]\tLoss: 622.2758\n",
      "Training Epoch: 2 [29300/36450]\tLoss: 698.3845\n",
      "Training Epoch: 2 [29350/36450]\tLoss: 669.1425\n",
      "Training Epoch: 2 [29400/36450]\tLoss: 658.5569\n",
      "Training Epoch: 2 [29450/36450]\tLoss: 648.8715\n",
      "Training Epoch: 2 [29500/36450]\tLoss: 702.0836\n",
      "Training Epoch: 2 [29550/36450]\tLoss: 664.4196\n",
      "Training Epoch: 2 [29600/36450]\tLoss: 656.1108\n",
      "Training Epoch: 2 [29650/36450]\tLoss: 687.9438\n",
      "Training Epoch: 2 [29700/36450]\tLoss: 652.9197\n",
      "Training Epoch: 2 [29750/36450]\tLoss: 658.6370\n",
      "Training Epoch: 2 [29800/36450]\tLoss: 626.9443\n",
      "Training Epoch: 2 [29850/36450]\tLoss: 656.9564\n",
      "Training Epoch: 2 [29900/36450]\tLoss: 687.9818\n",
      "Training Epoch: 2 [29950/36450]\tLoss: 639.3151\n",
      "Training Epoch: 2 [30000/36450]\tLoss: 669.8774\n",
      "Training Epoch: 2 [30050/36450]\tLoss: 715.1628\n",
      "Training Epoch: 2 [30100/36450]\tLoss: 655.4760\n",
      "Training Epoch: 2 [30150/36450]\tLoss: 677.2618\n",
      "Training Epoch: 2 [30200/36450]\tLoss: 665.9492\n",
      "Training Epoch: 2 [30250/36450]\tLoss: 627.5113\n",
      "Training Epoch: 2 [30300/36450]\tLoss: 681.5275\n",
      "Training Epoch: 2 [30350/36450]\tLoss: 686.5839\n",
      "Training Epoch: 2 [30400/36450]\tLoss: 666.6019\n",
      "Training Epoch: 2 [30450/36450]\tLoss: 670.7523\n",
      "Training Epoch: 2 [30500/36450]\tLoss: 685.6392\n",
      "Training Epoch: 2 [30550/36450]\tLoss: 669.7772\n",
      "Training Epoch: 2 [30600/36450]\tLoss: 633.8038\n",
      "Training Epoch: 2 [30650/36450]\tLoss: 675.1989\n",
      "Training Epoch: 2 [30700/36450]\tLoss: 673.9223\n",
      "Training Epoch: 2 [30750/36450]\tLoss: 674.8777\n",
      "Training Epoch: 2 [30800/36450]\tLoss: 706.3916\n",
      "Training Epoch: 2 [30850/36450]\tLoss: 667.9955\n",
      "Training Epoch: 2 [30900/36450]\tLoss: 661.1177\n",
      "Training Epoch: 2 [30950/36450]\tLoss: 638.3637\n",
      "Training Epoch: 2 [31000/36450]\tLoss: 684.9462\n",
      "Training Epoch: 2 [31050/36450]\tLoss: 645.3447\n",
      "Training Epoch: 2 [31100/36450]\tLoss: 665.6810\n",
      "Training Epoch: 2 [31150/36450]\tLoss: 721.0128\n",
      "Training Epoch: 2 [31200/36450]\tLoss: 690.3389\n",
      "Training Epoch: 2 [31250/36450]\tLoss: 648.9471\n",
      "Training Epoch: 2 [31300/36450]\tLoss: 630.1982\n",
      "Training Epoch: 2 [31350/36450]\tLoss: 669.3708\n",
      "Training Epoch: 2 [31400/36450]\tLoss: 727.6119\n",
      "Training Epoch: 2 [31450/36450]\tLoss: 747.5258\n",
      "Training Epoch: 2 [31500/36450]\tLoss: 651.8754\n",
      "Training Epoch: 2 [31550/36450]\tLoss: 699.2803\n",
      "Training Epoch: 2 [31600/36450]\tLoss: 703.2238\n",
      "Training Epoch: 2 [31650/36450]\tLoss: 653.6166\n",
      "Training Epoch: 2 [31700/36450]\tLoss: 699.7094\n",
      "Training Epoch: 2 [31750/36450]\tLoss: 701.9084\n",
      "Training Epoch: 2 [31800/36450]\tLoss: 737.5038\n",
      "Training Epoch: 2 [31850/36450]\tLoss: 656.9630\n",
      "Training Epoch: 2 [31900/36450]\tLoss: 648.7595\n",
      "Training Epoch: 2 [31950/36450]\tLoss: 615.5556\n",
      "Training Epoch: 2 [32000/36450]\tLoss: 688.3305\n",
      "Training Epoch: 2 [32050/36450]\tLoss: 671.5604\n",
      "Training Epoch: 2 [32100/36450]\tLoss: 639.4767\n",
      "Training Epoch: 2 [32150/36450]\tLoss: 660.3932\n",
      "Training Epoch: 2 [32200/36450]\tLoss: 665.2997\n",
      "Training Epoch: 2 [32250/36450]\tLoss: 622.6011\n",
      "Training Epoch: 2 [32300/36450]\tLoss: 672.4307\n",
      "Training Epoch: 2 [32350/36450]\tLoss: 672.3091\n",
      "Training Epoch: 2 [32400/36450]\tLoss: 654.3020\n",
      "Training Epoch: 2 [32450/36450]\tLoss: 643.9996\n",
      "Training Epoch: 2 [32500/36450]\tLoss: 699.6391\n",
      "Training Epoch: 2 [32550/36450]\tLoss: 643.1757\n",
      "Training Epoch: 2 [32600/36450]\tLoss: 666.8576\n",
      "Training Epoch: 2 [32650/36450]\tLoss: 653.9838\n",
      "Training Epoch: 2 [32700/36450]\tLoss: 659.1544\n",
      "Training Epoch: 2 [32750/36450]\tLoss: 658.6126\n",
      "Training Epoch: 2 [32800/36450]\tLoss: 635.0964\n",
      "Training Epoch: 2 [32850/36450]\tLoss: 660.3629\n",
      "Training Epoch: 2 [32900/36450]\tLoss: 689.2902\n",
      "Training Epoch: 2 [32950/36450]\tLoss: 667.9918\n",
      "Training Epoch: 2 [33000/36450]\tLoss: 653.8257\n",
      "Training Epoch: 2 [33050/36450]\tLoss: 696.3105\n",
      "Training Epoch: 2 [33100/36450]\tLoss: 673.5980\n",
      "Training Epoch: 2 [33150/36450]\tLoss: 623.7168\n",
      "Training Epoch: 2 [33200/36450]\tLoss: 645.4973\n",
      "Training Epoch: 2 [33250/36450]\tLoss: 633.0791\n",
      "Training Epoch: 2 [33300/36450]\tLoss: 696.9738\n",
      "Training Epoch: 2 [33350/36450]\tLoss: 666.1332\n",
      "Training Epoch: 2 [33400/36450]\tLoss: 631.2221\n",
      "Training Epoch: 2 [33450/36450]\tLoss: 683.2573\n",
      "Training Epoch: 2 [33500/36450]\tLoss: 698.2784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [33550/36450]\tLoss: 669.0070\n",
      "Training Epoch: 2 [33600/36450]\tLoss: 671.7628\n",
      "Training Epoch: 2 [33650/36450]\tLoss: 647.9323\n",
      "Training Epoch: 2 [33700/36450]\tLoss: 658.2052\n",
      "Training Epoch: 2 [33750/36450]\tLoss: 631.9761\n",
      "Training Epoch: 2 [33800/36450]\tLoss: 677.2908\n",
      "Training Epoch: 2 [33850/36450]\tLoss: 614.5967\n",
      "Training Epoch: 2 [33900/36450]\tLoss: 689.5327\n",
      "Training Epoch: 2 [33950/36450]\tLoss: 678.5770\n",
      "Training Epoch: 2 [34000/36450]\tLoss: 673.9617\n",
      "Training Epoch: 2 [34050/36450]\tLoss: 643.0587\n",
      "Training Epoch: 2 [34100/36450]\tLoss: 645.6551\n",
      "Training Epoch: 2 [34150/36450]\tLoss: 677.3740\n",
      "Training Epoch: 2 [34200/36450]\tLoss: 703.1902\n",
      "Training Epoch: 2 [34250/36450]\tLoss: 696.4235\n",
      "Training Epoch: 2 [34300/36450]\tLoss: 712.3111\n",
      "Training Epoch: 2 [34350/36450]\tLoss: 688.4474\n",
      "Training Epoch: 2 [34400/36450]\tLoss: 676.0066\n",
      "Training Epoch: 2 [34450/36450]\tLoss: 659.0756\n",
      "Training Epoch: 2 [34500/36450]\tLoss: 639.5475\n",
      "Training Epoch: 2 [34550/36450]\tLoss: 666.1287\n",
      "Training Epoch: 2 [34600/36450]\tLoss: 651.9863\n",
      "Training Epoch: 2 [34650/36450]\tLoss: 698.0767\n",
      "Training Epoch: 2 [34700/36450]\tLoss: 712.8055\n",
      "Training Epoch: 2 [34750/36450]\tLoss: 637.5701\n",
      "Training Epoch: 2 [34800/36450]\tLoss: 685.0312\n",
      "Training Epoch: 2 [34850/36450]\tLoss: 653.3174\n",
      "Training Epoch: 2 [34900/36450]\tLoss: 639.6260\n",
      "Training Epoch: 2 [34950/36450]\tLoss: 662.5569\n",
      "Training Epoch: 2 [35000/36450]\tLoss: 675.2831\n",
      "Training Epoch: 2 [35050/36450]\tLoss: 594.2905\n",
      "Training Epoch: 2 [35100/36450]\tLoss: 651.0485\n",
      "Training Epoch: 2 [35150/36450]\tLoss: 615.7975\n",
      "Training Epoch: 2 [35200/36450]\tLoss: 729.3815\n",
      "Training Epoch: 2 [35250/36450]\tLoss: 664.2142\n",
      "Training Epoch: 2 [35300/36450]\tLoss: 667.8765\n",
      "Training Epoch: 2 [35350/36450]\tLoss: 685.0710\n",
      "Training Epoch: 2 [35400/36450]\tLoss: 659.0496\n",
      "Training Epoch: 2 [35450/36450]\tLoss: 670.7228\n",
      "Training Epoch: 2 [35500/36450]\tLoss: 649.4838\n",
      "Training Epoch: 2 [35550/36450]\tLoss: 655.5072\n",
      "Training Epoch: 2 [35600/36450]\tLoss: 659.5163\n",
      "Training Epoch: 2 [35650/36450]\tLoss: 615.7732\n",
      "Training Epoch: 2 [35700/36450]\tLoss: 726.9139\n",
      "Training Epoch: 2 [35750/36450]\tLoss: 660.4006\n",
      "Training Epoch: 2 [35800/36450]\tLoss: 651.8878\n",
      "Training Epoch: 2 [35850/36450]\tLoss: 657.4047\n",
      "Training Epoch: 2 [35900/36450]\tLoss: 638.3984\n",
      "Training Epoch: 2 [35950/36450]\tLoss: 695.2303\n",
      "Training Epoch: 2 [36000/36450]\tLoss: 661.7747\n",
      "Training Epoch: 2 [36050/36450]\tLoss: 685.0119\n",
      "Training Epoch: 2 [36100/36450]\tLoss: 655.8398\n",
      "Training Epoch: 2 [36150/36450]\tLoss: 647.5271\n",
      "Training Epoch: 2 [36200/36450]\tLoss: 641.5898\n",
      "Training Epoch: 2 [36250/36450]\tLoss: 626.2626\n",
      "Training Epoch: 2 [36300/36450]\tLoss: 674.1432\n",
      "Training Epoch: 2 [36350/36450]\tLoss: 751.2678\n",
      "Training Epoch: 2 [36400/36450]\tLoss: 669.7394\n",
      "Training Epoch: 2 [36450/36450]\tLoss: 652.7153\n",
      "Training Epoch: 2 [4050/4050]\tLoss: 313.0620\n",
      "Training Epoch: 3 [50/36450]\tLoss: 605.6052\n",
      "Training Epoch: 3 [100/36450]\tLoss: 643.8611\n",
      "Training Epoch: 3 [150/36450]\tLoss: 692.6530\n",
      "Training Epoch: 3 [200/36450]\tLoss: 612.1064\n",
      "Training Epoch: 3 [250/36450]\tLoss: 671.0035\n",
      "Training Epoch: 3 [300/36450]\tLoss: 619.5922\n",
      "Training Epoch: 3 [350/36450]\tLoss: 695.4011\n",
      "Training Epoch: 3 [400/36450]\tLoss: 612.6558\n",
      "Training Epoch: 3 [450/36450]\tLoss: 646.1368\n",
      "Training Epoch: 3 [500/36450]\tLoss: 644.5518\n",
      "Training Epoch: 3 [550/36450]\tLoss: 679.7208\n",
      "Training Epoch: 3 [600/36450]\tLoss: 644.2506\n",
      "Training Epoch: 3 [650/36450]\tLoss: 688.6631\n",
      "Training Epoch: 3 [700/36450]\tLoss: 643.4024\n",
      "Training Epoch: 3 [750/36450]\tLoss: 663.0123\n",
      "Training Epoch: 3 [800/36450]\tLoss: 602.9579\n",
      "Training Epoch: 3 [850/36450]\tLoss: 670.6532\n",
      "Training Epoch: 3 [900/36450]\tLoss: 613.1011\n",
      "Training Epoch: 3 [950/36450]\tLoss: 633.3741\n",
      "Training Epoch: 3 [1000/36450]\tLoss: 665.8773\n",
      "Training Epoch: 3 [1050/36450]\tLoss: 613.5276\n",
      "Training Epoch: 3 [1100/36450]\tLoss: 651.1828\n",
      "Training Epoch: 3 [1150/36450]\tLoss: 672.3695\n",
      "Training Epoch: 3 [1200/36450]\tLoss: 625.2786\n",
      "Training Epoch: 3 [1250/36450]\tLoss: 655.2554\n",
      "Training Epoch: 3 [1300/36450]\tLoss: 658.5234\n",
      "Training Epoch: 3 [1350/36450]\tLoss: 638.5473\n",
      "Training Epoch: 3 [1400/36450]\tLoss: 645.6632\n",
      "Training Epoch: 3 [1450/36450]\tLoss: 673.7219\n",
      "Training Epoch: 3 [1500/36450]\tLoss: 654.4429\n",
      "Training Epoch: 3 [1550/36450]\tLoss: 635.0516\n",
      "Training Epoch: 3 [1600/36450]\tLoss: 692.5679\n",
      "Training Epoch: 3 [1650/36450]\tLoss: 658.9740\n",
      "Training Epoch: 3 [1700/36450]\tLoss: 642.6005\n",
      "Training Epoch: 3 [1750/36450]\tLoss: 692.5526\n",
      "Training Epoch: 3 [1800/36450]\tLoss: 628.6371\n",
      "Training Epoch: 3 [1850/36450]\tLoss: 661.3700\n",
      "Training Epoch: 3 [1900/36450]\tLoss: 607.8275\n",
      "Training Epoch: 3 [1950/36450]\tLoss: 677.2182\n",
      "Training Epoch: 3 [2000/36450]\tLoss: 635.7169\n",
      "Training Epoch: 3 [2050/36450]\tLoss: 620.3300\n",
      "Training Epoch: 3 [2100/36450]\tLoss: 617.3875\n",
      "Training Epoch: 3 [2150/36450]\tLoss: 623.5244\n",
      "Training Epoch: 3 [2200/36450]\tLoss: 612.0647\n",
      "Training Epoch: 3 [2250/36450]\tLoss: 656.6850\n",
      "Training Epoch: 3 [2300/36450]\tLoss: 693.3325\n",
      "Training Epoch: 3 [2350/36450]\tLoss: 664.8996\n",
      "Training Epoch: 3 [2400/36450]\tLoss: 637.6329\n",
      "Training Epoch: 3 [2450/36450]\tLoss: 657.4763\n",
      "Training Epoch: 3 [2500/36450]\tLoss: 667.3820\n",
      "Training Epoch: 3 [2550/36450]\tLoss: 625.9339\n",
      "Training Epoch: 3 [2600/36450]\tLoss: 613.5123\n",
      "Training Epoch: 3 [2650/36450]\tLoss: 684.7545\n",
      "Training Epoch: 3 [2700/36450]\tLoss: 683.8535\n",
      "Training Epoch: 3 [2750/36450]\tLoss: 603.1567\n",
      "Training Epoch: 3 [2800/36450]\tLoss: 644.1794\n",
      "Training Epoch: 3 [2850/36450]\tLoss: 612.9513\n",
      "Training Epoch: 3 [2900/36450]\tLoss: 610.9686\n",
      "Training Epoch: 3 [2950/36450]\tLoss: 605.5745\n",
      "Training Epoch: 3 [3000/36450]\tLoss: 615.5624\n",
      "Training Epoch: 3 [3050/36450]\tLoss: 677.2026\n",
      "Training Epoch: 3 [3100/36450]\tLoss: 711.5570\n",
      "Training Epoch: 3 [3150/36450]\tLoss: 630.4259\n",
      "Training Epoch: 3 [3200/36450]\tLoss: 693.0969\n",
      "Training Epoch: 3 [3250/36450]\tLoss: 646.3962\n",
      "Training Epoch: 3 [3300/36450]\tLoss: 609.4483\n",
      "Training Epoch: 3 [3350/36450]\tLoss: 651.5848\n",
      "Training Epoch: 3 [3400/36450]\tLoss: 678.5689\n",
      "Training Epoch: 3 [3450/36450]\tLoss: 689.1033\n",
      "Training Epoch: 3 [3500/36450]\tLoss: 680.7405\n",
      "Training Epoch: 3 [3550/36450]\tLoss: 671.9615\n",
      "Training Epoch: 3 [3600/36450]\tLoss: 640.9147\n",
      "Training Epoch: 3 [3650/36450]\tLoss: 719.3376\n",
      "Training Epoch: 3 [3700/36450]\tLoss: 669.0522\n",
      "Training Epoch: 3 [3750/36450]\tLoss: 602.8629\n",
      "Training Epoch: 3 [3800/36450]\tLoss: 683.4943\n",
      "Training Epoch: 3 [3850/36450]\tLoss: 628.5106\n",
      "Training Epoch: 3 [3900/36450]\tLoss: 671.1522\n",
      "Training Epoch: 3 [3950/36450]\tLoss: 613.0628\n",
      "Training Epoch: 3 [4000/36450]\tLoss: 630.8580\n",
      "Training Epoch: 3 [4050/36450]\tLoss: 663.8297\n",
      "Training Epoch: 3 [4100/36450]\tLoss: 628.2921\n",
      "Training Epoch: 3 [4150/36450]\tLoss: 648.9118\n",
      "Training Epoch: 3 [4200/36450]\tLoss: 661.5515\n",
      "Training Epoch: 3 [4250/36450]\tLoss: 597.6746\n",
      "Training Epoch: 3 [4300/36450]\tLoss: 610.4301\n",
      "Training Epoch: 3 [4350/36450]\tLoss: 644.1425\n",
      "Training Epoch: 3 [4400/36450]\tLoss: 633.3013\n",
      "Training Epoch: 3 [4450/36450]\tLoss: 642.2434\n",
      "Training Epoch: 3 [4500/36450]\tLoss: 658.1827\n",
      "Training Epoch: 3 [4550/36450]\tLoss: 664.5824\n",
      "Training Epoch: 3 [4600/36450]\tLoss: 653.2803\n",
      "Training Epoch: 3 [4650/36450]\tLoss: 616.6896\n",
      "Training Epoch: 3 [4700/36450]\tLoss: 667.7861\n",
      "Training Epoch: 3 [4750/36450]\tLoss: 601.9731\n",
      "Training Epoch: 3 [4800/36450]\tLoss: 669.1134\n",
      "Training Epoch: 3 [4850/36450]\tLoss: 647.0988\n",
      "Training Epoch: 3 [4900/36450]\tLoss: 635.2324\n",
      "Training Epoch: 3 [4950/36450]\tLoss: 630.7435\n",
      "Training Epoch: 3 [5000/36450]\tLoss: 650.0201\n",
      "Training Epoch: 3 [5050/36450]\tLoss: 642.5903\n",
      "Training Epoch: 3 [5100/36450]\tLoss: 621.8273\n",
      "Training Epoch: 3 [5150/36450]\tLoss: 643.3848\n",
      "Training Epoch: 3 [5200/36450]\tLoss: 612.0025\n",
      "Training Epoch: 3 [5250/36450]\tLoss: 638.3757\n",
      "Training Epoch: 3 [5300/36450]\tLoss: 641.4741\n",
      "Training Epoch: 3 [5350/36450]\tLoss: 666.9850\n",
      "Training Epoch: 3 [5400/36450]\tLoss: 700.5948\n",
      "Training Epoch: 3 [5450/36450]\tLoss: 669.2680\n",
      "Training Epoch: 3 [5500/36450]\tLoss: 647.1646\n",
      "Training Epoch: 3 [5550/36450]\tLoss: 658.5148\n",
      "Training Epoch: 3 [5600/36450]\tLoss: 631.3149\n",
      "Training Epoch: 3 [5650/36450]\tLoss: 634.6398\n",
      "Training Epoch: 3 [5700/36450]\tLoss: 652.8058\n",
      "Training Epoch: 3 [5750/36450]\tLoss: 595.2540\n",
      "Training Epoch: 3 [5800/36450]\tLoss: 692.3242\n",
      "Training Epoch: 3 [5850/36450]\tLoss: 633.8845\n",
      "Training Epoch: 3 [5900/36450]\tLoss: 658.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [5950/36450]\tLoss: 612.7503\n",
      "Training Epoch: 3 [6000/36450]\tLoss: 646.4095\n",
      "Training Epoch: 3 [6050/36450]\tLoss: 590.3326\n",
      "Training Epoch: 3 [6100/36450]\tLoss: 643.8366\n",
      "Training Epoch: 3 [6150/36450]\tLoss: 603.6854\n",
      "Training Epoch: 3 [6200/36450]\tLoss: 623.8995\n",
      "Training Epoch: 3 [6250/36450]\tLoss: 656.3824\n",
      "Training Epoch: 3 [6300/36450]\tLoss: 643.3839\n",
      "Training Epoch: 3 [6350/36450]\tLoss: 620.7099\n",
      "Training Epoch: 3 [6400/36450]\tLoss: 688.2384\n",
      "Training Epoch: 3 [6450/36450]\tLoss: 611.4150\n",
      "Training Epoch: 3 [6500/36450]\tLoss: 650.6281\n",
      "Training Epoch: 3 [6550/36450]\tLoss: 636.3096\n",
      "Training Epoch: 3 [6600/36450]\tLoss: 612.9360\n",
      "Training Epoch: 3 [6650/36450]\tLoss: 649.5703\n",
      "Training Epoch: 3 [6700/36450]\tLoss: 644.6160\n",
      "Training Epoch: 3 [6750/36450]\tLoss: 647.2738\n",
      "Training Epoch: 3 [6800/36450]\tLoss: 639.2369\n",
      "Training Epoch: 3 [6850/36450]\tLoss: 611.3635\n",
      "Training Epoch: 3 [6900/36450]\tLoss: 606.8535\n",
      "Training Epoch: 3 [6950/36450]\tLoss: 664.0761\n",
      "Training Epoch: 3 [7000/36450]\tLoss: 682.1103\n",
      "Training Epoch: 3 [7050/36450]\tLoss: 633.4116\n",
      "Training Epoch: 3 [7100/36450]\tLoss: 631.8477\n",
      "Training Epoch: 3 [7150/36450]\tLoss: 605.5334\n",
      "Training Epoch: 3 [7200/36450]\tLoss: 637.8532\n",
      "Training Epoch: 3 [7250/36450]\tLoss: 640.7198\n",
      "Training Epoch: 3 [7300/36450]\tLoss: 634.7175\n",
      "Training Epoch: 3 [7350/36450]\tLoss: 648.0915\n",
      "Training Epoch: 3 [7400/36450]\tLoss: 631.3348\n",
      "Training Epoch: 3 [7450/36450]\tLoss: 641.6022\n",
      "Training Epoch: 3 [7500/36450]\tLoss: 664.2643\n",
      "Training Epoch: 3 [7550/36450]\tLoss: 664.9964\n",
      "Training Epoch: 3 [7600/36450]\tLoss: 617.6260\n",
      "Training Epoch: 3 [7650/36450]\tLoss: 645.1039\n",
      "Training Epoch: 3 [7700/36450]\tLoss: 667.8267\n",
      "Training Epoch: 3 [7750/36450]\tLoss: 658.3679\n",
      "Training Epoch: 3 [7800/36450]\tLoss: 580.2374\n",
      "Training Epoch: 3 [7850/36450]\tLoss: 595.1752\n",
      "Training Epoch: 3 [7900/36450]\tLoss: 638.7733\n",
      "Training Epoch: 3 [7950/36450]\tLoss: 683.0958\n",
      "Training Epoch: 3 [8000/36450]\tLoss: 618.6290\n",
      "Training Epoch: 3 [8050/36450]\tLoss: 639.4646\n",
      "Training Epoch: 3 [8100/36450]\tLoss: 606.7280\n",
      "Training Epoch: 3 [8150/36450]\tLoss: 568.7653\n",
      "Training Epoch: 3 [8200/36450]\tLoss: 636.3102\n",
      "Training Epoch: 3 [8250/36450]\tLoss: 614.4733\n",
      "Training Epoch: 3 [8300/36450]\tLoss: 601.7913\n",
      "Training Epoch: 3 [8350/36450]\tLoss: 625.3873\n",
      "Training Epoch: 3 [8400/36450]\tLoss: 608.6400\n",
      "Training Epoch: 3 [8450/36450]\tLoss: 633.0981\n",
      "Training Epoch: 3 [8500/36450]\tLoss: 617.0359\n",
      "Training Epoch: 3 [8550/36450]\tLoss: 590.4420\n",
      "Training Epoch: 3 [8600/36450]\tLoss: 663.6880\n",
      "Training Epoch: 3 [8650/36450]\tLoss: 652.0530\n",
      "Training Epoch: 3 [8700/36450]\tLoss: 617.9526\n",
      "Training Epoch: 3 [8750/36450]\tLoss: 586.2275\n",
      "Training Epoch: 3 [8800/36450]\tLoss: 617.3403\n",
      "Training Epoch: 3 [8850/36450]\tLoss: 642.4566\n",
      "Training Epoch: 3 [8900/36450]\tLoss: 637.6164\n",
      "Training Epoch: 3 [8950/36450]\tLoss: 640.1873\n",
      "Training Epoch: 3 [9000/36450]\tLoss: 671.6296\n",
      "Training Epoch: 3 [9050/36450]\tLoss: 653.2031\n",
      "Training Epoch: 3 [9100/36450]\tLoss: 647.2532\n",
      "Training Epoch: 3 [9150/36450]\tLoss: 626.3317\n",
      "Training Epoch: 3 [9200/36450]\tLoss: 643.2442\n",
      "Training Epoch: 3 [9250/36450]\tLoss: 652.7466\n",
      "Training Epoch: 3 [9300/36450]\tLoss: 588.1892\n",
      "Training Epoch: 3 [9350/36450]\tLoss: 612.8901\n",
      "Training Epoch: 3 [9400/36450]\tLoss: 662.8380\n",
      "Training Epoch: 3 [9450/36450]\tLoss: 634.9224\n",
      "Training Epoch: 3 [9500/36450]\tLoss: 580.2684\n",
      "Training Epoch: 3 [9550/36450]\tLoss: 619.0101\n",
      "Training Epoch: 3 [9600/36450]\tLoss: 605.9764\n",
      "Training Epoch: 3 [9650/36450]\tLoss: 626.8866\n",
      "Training Epoch: 3 [9700/36450]\tLoss: 647.7269\n",
      "Training Epoch: 3 [9750/36450]\tLoss: 668.3099\n",
      "Training Epoch: 3 [9800/36450]\tLoss: 652.7896\n",
      "Training Epoch: 3 [9850/36450]\tLoss: 625.6310\n",
      "Training Epoch: 3 [9900/36450]\tLoss: 618.9687\n",
      "Training Epoch: 3 [9950/36450]\tLoss: 617.6621\n",
      "Training Epoch: 3 [10000/36450]\tLoss: 598.0654\n",
      "Training Epoch: 3 [10050/36450]\tLoss: 680.6317\n",
      "Training Epoch: 3 [10100/36450]\tLoss: 597.0248\n",
      "Training Epoch: 3 [10150/36450]\tLoss: 601.4653\n",
      "Training Epoch: 3 [10200/36450]\tLoss: 659.5511\n",
      "Training Epoch: 3 [10250/36450]\tLoss: 599.7807\n",
      "Training Epoch: 3 [10300/36450]\tLoss: 654.5306\n",
      "Training Epoch: 3 [10350/36450]\tLoss: 641.9926\n",
      "Training Epoch: 3 [10400/36450]\tLoss: 632.2707\n",
      "Training Epoch: 3 [10450/36450]\tLoss: 633.3263\n",
      "Training Epoch: 3 [10500/36450]\tLoss: 633.1704\n",
      "Training Epoch: 3 [10550/36450]\tLoss: 600.4725\n",
      "Training Epoch: 3 [10600/36450]\tLoss: 628.4009\n",
      "Training Epoch: 3 [10650/36450]\tLoss: 616.9614\n",
      "Training Epoch: 3 [10700/36450]\tLoss: 652.1111\n",
      "Training Epoch: 3 [10750/36450]\tLoss: 629.9400\n",
      "Training Epoch: 3 [10800/36450]\tLoss: 687.9258\n",
      "Training Epoch: 3 [10850/36450]\tLoss: 628.9807\n",
      "Training Epoch: 3 [10900/36450]\tLoss: 643.2743\n",
      "Training Epoch: 3 [10950/36450]\tLoss: 650.8367\n",
      "Training Epoch: 3 [11000/36450]\tLoss: 593.1463\n",
      "Training Epoch: 3 [11050/36450]\tLoss: 645.6899\n",
      "Training Epoch: 3 [11100/36450]\tLoss: 585.4809\n",
      "Training Epoch: 3 [11150/36450]\tLoss: 674.6160\n",
      "Training Epoch: 3 [11200/36450]\tLoss: 610.2822\n",
      "Training Epoch: 3 [11250/36450]\tLoss: 653.8256\n",
      "Training Epoch: 3 [11300/36450]\tLoss: 609.7802\n",
      "Training Epoch: 3 [11350/36450]\tLoss: 600.6812\n",
      "Training Epoch: 3 [11400/36450]\tLoss: 592.6982\n",
      "Training Epoch: 3 [11450/36450]\tLoss: 639.0191\n",
      "Training Epoch: 3 [11500/36450]\tLoss: 562.8124\n",
      "Training Epoch: 3 [11550/36450]\tLoss: 698.9635\n",
      "Training Epoch: 3 [11600/36450]\tLoss: 626.6006\n",
      "Training Epoch: 3 [11650/36450]\tLoss: 613.1134\n",
      "Training Epoch: 3 [11700/36450]\tLoss: 586.2845\n",
      "Training Epoch: 3 [11750/36450]\tLoss: 596.2169\n",
      "Training Epoch: 3 [11800/36450]\tLoss: 625.7281\n",
      "Training Epoch: 3 [11850/36450]\tLoss: 657.3047\n",
      "Training Epoch: 3 [11900/36450]\tLoss: 627.6404\n",
      "Training Epoch: 3 [11950/36450]\tLoss: 641.4553\n",
      "Training Epoch: 3 [12000/36450]\tLoss: 663.6410\n",
      "Training Epoch: 3 [12050/36450]\tLoss: 624.1928\n",
      "Training Epoch: 3 [12100/36450]\tLoss: 650.5657\n",
      "Training Epoch: 3 [12150/36450]\tLoss: 634.3787\n",
      "Training Epoch: 3 [12200/36450]\tLoss: 619.4340\n",
      "Training Epoch: 3 [12250/36450]\tLoss: 578.6224\n",
      "Training Epoch: 3 [12300/36450]\tLoss: 624.8786\n",
      "Training Epoch: 3 [12350/36450]\tLoss: 593.9183\n",
      "Training Epoch: 3 [12400/36450]\tLoss: 601.0625\n",
      "Training Epoch: 3 [12450/36450]\tLoss: 632.7910\n",
      "Training Epoch: 3 [12500/36450]\tLoss: 580.6918\n",
      "Training Epoch: 3 [12550/36450]\tLoss: 627.2996\n",
      "Training Epoch: 3 [12600/36450]\tLoss: 696.8542\n",
      "Training Epoch: 3 [12650/36450]\tLoss: 609.4584\n",
      "Training Epoch: 3 [12700/36450]\tLoss: 627.2509\n",
      "Training Epoch: 3 [12750/36450]\tLoss: 634.0125\n",
      "Training Epoch: 3 [12800/36450]\tLoss: 659.3768\n",
      "Training Epoch: 3 [12850/36450]\tLoss: 705.7035\n",
      "Training Epoch: 3 [12900/36450]\tLoss: 614.3671\n",
      "Training Epoch: 3 [12950/36450]\tLoss: 594.5284\n",
      "Training Epoch: 3 [13000/36450]\tLoss: 633.2829\n",
      "Training Epoch: 3 [13050/36450]\tLoss: 627.8681\n",
      "Training Epoch: 3 [13100/36450]\tLoss: 589.2834\n",
      "Training Epoch: 3 [13150/36450]\tLoss: 616.8797\n",
      "Training Epoch: 3 [13200/36450]\tLoss: 614.1827\n",
      "Training Epoch: 3 [13250/36450]\tLoss: 652.1931\n",
      "Training Epoch: 3 [13300/36450]\tLoss: 598.5261\n",
      "Training Epoch: 3 [13350/36450]\tLoss: 594.8612\n",
      "Training Epoch: 3 [13400/36450]\tLoss: 597.2224\n",
      "Training Epoch: 3 [13450/36450]\tLoss: 655.6994\n",
      "Training Epoch: 3 [13500/36450]\tLoss: 623.2361\n",
      "Training Epoch: 3 [13550/36450]\tLoss: 658.4379\n",
      "Training Epoch: 3 [13600/36450]\tLoss: 595.7595\n",
      "Training Epoch: 3 [13650/36450]\tLoss: 584.2041\n",
      "Training Epoch: 3 [13700/36450]\tLoss: 661.7997\n",
      "Training Epoch: 3 [13750/36450]\tLoss: 658.6566\n",
      "Training Epoch: 3 [13800/36450]\tLoss: 664.9779\n",
      "Training Epoch: 3 [13850/36450]\tLoss: 576.1414\n",
      "Training Epoch: 3 [13900/36450]\tLoss: 584.5125\n",
      "Training Epoch: 3 [13950/36450]\tLoss: 601.8323\n",
      "Training Epoch: 3 [14000/36450]\tLoss: 602.2834\n",
      "Training Epoch: 3 [14050/36450]\tLoss: 678.6743\n",
      "Training Epoch: 3 [14100/36450]\tLoss: 571.1819\n",
      "Training Epoch: 3 [14150/36450]\tLoss: 626.7587\n",
      "Training Epoch: 3 [14200/36450]\tLoss: 600.0817\n",
      "Training Epoch: 3 [14250/36450]\tLoss: 623.0200\n",
      "Training Epoch: 3 [14300/36450]\tLoss: 596.7957\n",
      "Training Epoch: 3 [14350/36450]\tLoss: 636.3693\n",
      "Training Epoch: 3 [14400/36450]\tLoss: 615.9507\n",
      "Training Epoch: 3 [14450/36450]\tLoss: 665.5821\n",
      "Training Epoch: 3 [14500/36450]\tLoss: 584.8900\n",
      "Training Epoch: 3 [14550/36450]\tLoss: 621.1821\n",
      "Training Epoch: 3 [14600/36450]\tLoss: 616.5090\n",
      "Training Epoch: 3 [14650/36450]\tLoss: 626.9977\n",
      "Training Epoch: 3 [14700/36450]\tLoss: 639.8352\n",
      "Training Epoch: 3 [14750/36450]\tLoss: 637.4217\n",
      "Training Epoch: 3 [14800/36450]\tLoss: 630.1268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [14850/36450]\tLoss: 649.6882\n",
      "Training Epoch: 3 [14900/36450]\tLoss: 592.3326\n",
      "Training Epoch: 3 [14950/36450]\tLoss: 620.6467\n",
      "Training Epoch: 3 [15000/36450]\tLoss: 609.7225\n",
      "Training Epoch: 3 [15050/36450]\tLoss: 609.7914\n",
      "Training Epoch: 3 [15100/36450]\tLoss: 612.9830\n",
      "Training Epoch: 3 [15150/36450]\tLoss: 601.2396\n",
      "Training Epoch: 3 [15200/36450]\tLoss: 599.2824\n",
      "Training Epoch: 3 [15250/36450]\tLoss: 629.5974\n",
      "Training Epoch: 3 [15300/36450]\tLoss: 660.6217\n",
      "Training Epoch: 3 [15350/36450]\tLoss: 597.1235\n",
      "Training Epoch: 3 [15400/36450]\tLoss: 655.0621\n",
      "Training Epoch: 3 [15450/36450]\tLoss: 587.0954\n",
      "Training Epoch: 3 [15500/36450]\tLoss: 579.6671\n",
      "Training Epoch: 3 [15550/36450]\tLoss: 667.0392\n",
      "Training Epoch: 3 [15600/36450]\tLoss: 620.8943\n",
      "Training Epoch: 3 [15650/36450]\tLoss: 608.5153\n",
      "Training Epoch: 3 [15700/36450]\tLoss: 634.7496\n",
      "Training Epoch: 3 [15750/36450]\tLoss: 571.9882\n",
      "Training Epoch: 3 [15800/36450]\tLoss: 619.0661\n",
      "Training Epoch: 3 [15850/36450]\tLoss: 647.3394\n",
      "Training Epoch: 3 [15900/36450]\tLoss: 592.3939\n",
      "Training Epoch: 3 [15950/36450]\tLoss: 587.8668\n",
      "Training Epoch: 3 [16000/36450]\tLoss: 578.6805\n",
      "Training Epoch: 3 [16050/36450]\tLoss: 646.2113\n",
      "Training Epoch: 3 [16100/36450]\tLoss: 601.7036\n",
      "Training Epoch: 3 [16150/36450]\tLoss: 648.6076\n",
      "Training Epoch: 3 [16200/36450]\tLoss: 649.9562\n",
      "Training Epoch: 3 [16250/36450]\tLoss: 560.2346\n",
      "Training Epoch: 3 [16300/36450]\tLoss: 641.3149\n",
      "Training Epoch: 3 [16350/36450]\tLoss: 595.3377\n",
      "Training Epoch: 3 [16400/36450]\tLoss: 636.7838\n",
      "Training Epoch: 3 [16450/36450]\tLoss: 630.3704\n",
      "Training Epoch: 3 [16500/36450]\tLoss: 576.8818\n",
      "Training Epoch: 3 [16550/36450]\tLoss: 594.8119\n",
      "Training Epoch: 3 [16600/36450]\tLoss: 625.1603\n",
      "Training Epoch: 3 [16650/36450]\tLoss: 558.6436\n",
      "Training Epoch: 3 [16700/36450]\tLoss: 599.2899\n",
      "Training Epoch: 3 [16750/36450]\tLoss: 571.4418\n",
      "Training Epoch: 3 [16800/36450]\tLoss: 651.9880\n",
      "Training Epoch: 3 [16850/36450]\tLoss: 534.9327\n",
      "Training Epoch: 3 [16900/36450]\tLoss: 660.9430\n",
      "Training Epoch: 3 [16950/36450]\tLoss: 640.8922\n",
      "Training Epoch: 3 [17000/36450]\tLoss: 664.0001\n",
      "Training Epoch: 3 [17050/36450]\tLoss: 587.9574\n",
      "Training Epoch: 3 [17100/36450]\tLoss: 576.9438\n",
      "Training Epoch: 3 [17150/36450]\tLoss: 645.2927\n",
      "Training Epoch: 3 [17200/36450]\tLoss: 626.4606\n",
      "Training Epoch: 3 [17250/36450]\tLoss: 594.7728\n",
      "Training Epoch: 3 [17300/36450]\tLoss: 624.7738\n",
      "Training Epoch: 3 [17350/36450]\tLoss: 599.5707\n",
      "Training Epoch: 3 [17400/36450]\tLoss: 647.7432\n",
      "Training Epoch: 3 [17450/36450]\tLoss: 632.7012\n",
      "Training Epoch: 3 [17500/36450]\tLoss: 580.4194\n",
      "Training Epoch: 3 [17550/36450]\tLoss: 542.8797\n",
      "Training Epoch: 3 [17600/36450]\tLoss: 644.1254\n",
      "Training Epoch: 3 [17650/36450]\tLoss: 575.1005\n",
      "Training Epoch: 3 [17700/36450]\tLoss: 616.2778\n",
      "Training Epoch: 3 [17750/36450]\tLoss: 599.0613\n",
      "Training Epoch: 3 [17800/36450]\tLoss: 633.4330\n",
      "Training Epoch: 3 [17850/36450]\tLoss: 578.3980\n",
      "Training Epoch: 3 [17900/36450]\tLoss: 598.4516\n",
      "Training Epoch: 3 [17950/36450]\tLoss: 632.9822\n",
      "Training Epoch: 3 [18000/36450]\tLoss: 623.4525\n",
      "Training Epoch: 3 [18050/36450]\tLoss: 710.6331\n",
      "Training Epoch: 3 [18100/36450]\tLoss: 595.1746\n",
      "Training Epoch: 3 [18150/36450]\tLoss: 623.7125\n",
      "Training Epoch: 3 [18200/36450]\tLoss: 572.6718\n",
      "Training Epoch: 3 [18250/36450]\tLoss: 626.0667\n",
      "Training Epoch: 3 [18300/36450]\tLoss: 562.4627\n",
      "Training Epoch: 3 [18350/36450]\tLoss: 615.7108\n",
      "Training Epoch: 3 [18400/36450]\tLoss: 613.5472\n",
      "Training Epoch: 3 [18450/36450]\tLoss: 584.1295\n",
      "Training Epoch: 3 [18500/36450]\tLoss: 629.4600\n",
      "Training Epoch: 3 [18550/36450]\tLoss: 648.9055\n",
      "Training Epoch: 3 [18600/36450]\tLoss: 604.6388\n",
      "Training Epoch: 3 [18650/36450]\tLoss: 641.2808\n",
      "Training Epoch: 3 [18700/36450]\tLoss: 685.4161\n",
      "Training Epoch: 3 [18750/36450]\tLoss: 596.0402\n",
      "Training Epoch: 3 [18800/36450]\tLoss: 632.0116\n",
      "Training Epoch: 3 [18850/36450]\tLoss: 585.0703\n",
      "Training Epoch: 3 [18900/36450]\tLoss: 620.9312\n",
      "Training Epoch: 3 [18950/36450]\tLoss: 616.9911\n",
      "Training Epoch: 3 [19000/36450]\tLoss: 692.0123\n",
      "Training Epoch: 3 [19050/36450]\tLoss: 620.8016\n",
      "Training Epoch: 3 [19100/36450]\tLoss: 592.8425\n",
      "Training Epoch: 3 [19150/36450]\tLoss: 617.7070\n",
      "Training Epoch: 3 [19200/36450]\tLoss: 678.4485\n",
      "Training Epoch: 3 [19250/36450]\tLoss: 651.0909\n",
      "Training Epoch: 3 [19300/36450]\tLoss: 590.1168\n",
      "Training Epoch: 3 [19350/36450]\tLoss: 624.8522\n",
      "Training Epoch: 3 [19400/36450]\tLoss: 594.8232\n",
      "Training Epoch: 3 [19450/36450]\tLoss: 578.4940\n",
      "Training Epoch: 3 [19500/36450]\tLoss: 608.0240\n",
      "Training Epoch: 3 [19550/36450]\tLoss: 589.8604\n",
      "Training Epoch: 3 [19600/36450]\tLoss: 594.3779\n",
      "Training Epoch: 3 [19650/36450]\tLoss: 604.1998\n",
      "Training Epoch: 3 [19700/36450]\tLoss: 574.7364\n",
      "Training Epoch: 3 [19750/36450]\tLoss: 619.2949\n",
      "Training Epoch: 3 [19800/36450]\tLoss: 599.9811\n",
      "Training Epoch: 3 [19850/36450]\tLoss: 591.4491\n",
      "Training Epoch: 3 [19900/36450]\tLoss: 566.4972\n",
      "Training Epoch: 3 [19950/36450]\tLoss: 603.9698\n",
      "Training Epoch: 3 [20000/36450]\tLoss: 593.4614\n",
      "Training Epoch: 3 [20050/36450]\tLoss: 582.4178\n",
      "Training Epoch: 3 [20100/36450]\tLoss: 596.2930\n",
      "Training Epoch: 3 [20150/36450]\tLoss: 629.7251\n",
      "Training Epoch: 3 [20200/36450]\tLoss: 584.9937\n",
      "Training Epoch: 3 [20250/36450]\tLoss: 573.4480\n",
      "Training Epoch: 3 [20300/36450]\tLoss: 572.5541\n",
      "Training Epoch: 3 [20350/36450]\tLoss: 580.8926\n",
      "Training Epoch: 3 [20400/36450]\tLoss: 608.1074\n",
      "Training Epoch: 3 [20450/36450]\tLoss: 603.5165\n",
      "Training Epoch: 3 [20500/36450]\tLoss: 608.7269\n",
      "Training Epoch: 3 [20550/36450]\tLoss: 669.5093\n",
      "Training Epoch: 3 [20600/36450]\tLoss: 614.5840\n",
      "Training Epoch: 3 [20650/36450]\tLoss: 619.1794\n",
      "Training Epoch: 3 [20700/36450]\tLoss: 580.9099\n",
      "Training Epoch: 3 [20750/36450]\tLoss: 645.0340\n",
      "Training Epoch: 3 [20800/36450]\tLoss: 589.2144\n",
      "Training Epoch: 3 [20850/36450]\tLoss: 599.6617\n",
      "Training Epoch: 3 [20900/36450]\tLoss: 617.6517\n",
      "Training Epoch: 3 [20950/36450]\tLoss: 582.3569\n",
      "Training Epoch: 3 [21000/36450]\tLoss: 655.7491\n",
      "Training Epoch: 3 [21050/36450]\tLoss: 567.0711\n",
      "Training Epoch: 3 [21100/36450]\tLoss: 590.5898\n",
      "Training Epoch: 3 [21150/36450]\tLoss: 638.7032\n",
      "Training Epoch: 3 [21200/36450]\tLoss: 629.7305\n",
      "Training Epoch: 3 [21250/36450]\tLoss: 558.2637\n",
      "Training Epoch: 3 [21300/36450]\tLoss: 592.1110\n",
      "Training Epoch: 3 [21350/36450]\tLoss: 567.7256\n",
      "Training Epoch: 3 [21400/36450]\tLoss: 617.8530\n",
      "Training Epoch: 3 [21450/36450]\tLoss: 652.2859\n",
      "Training Epoch: 3 [21500/36450]\tLoss: 632.1793\n",
      "Training Epoch: 3 [21550/36450]\tLoss: 575.0566\n",
      "Training Epoch: 3 [21600/36450]\tLoss: 625.2271\n",
      "Training Epoch: 3 [21650/36450]\tLoss: 602.0540\n",
      "Training Epoch: 3 [21700/36450]\tLoss: 545.6478\n",
      "Training Epoch: 3 [21750/36450]\tLoss: 630.2433\n",
      "Training Epoch: 3 [21800/36450]\tLoss: 574.6595\n",
      "Training Epoch: 3 [21850/36450]\tLoss: 588.0179\n",
      "Training Epoch: 3 [21900/36450]\tLoss: 594.8520\n",
      "Training Epoch: 3 [21950/36450]\tLoss: 636.7302\n",
      "Training Epoch: 3 [22000/36450]\tLoss: 553.5717\n",
      "Training Epoch: 3 [22050/36450]\tLoss: 641.4713\n",
      "Training Epoch: 3 [22100/36450]\tLoss: 595.9828\n",
      "Training Epoch: 3 [22150/36450]\tLoss: 606.4012\n",
      "Training Epoch: 3 [22200/36450]\tLoss: 570.5585\n",
      "Training Epoch: 3 [22250/36450]\tLoss: 618.3595\n",
      "Training Epoch: 3 [22300/36450]\tLoss: 588.9416\n",
      "Training Epoch: 3 [22350/36450]\tLoss: 606.6772\n",
      "Training Epoch: 3 [22400/36450]\tLoss: 578.2118\n",
      "Training Epoch: 3 [22450/36450]\tLoss: 587.8623\n",
      "Training Epoch: 3 [22500/36450]\tLoss: 600.3314\n",
      "Training Epoch: 3 [22550/36450]\tLoss: 656.2299\n",
      "Training Epoch: 3 [22600/36450]\tLoss: 603.7520\n",
      "Training Epoch: 3 [22650/36450]\tLoss: 610.3857\n",
      "Training Epoch: 3 [22700/36450]\tLoss: 589.0199\n",
      "Training Epoch: 3 [22750/36450]\tLoss: 592.8862\n",
      "Training Epoch: 3 [22800/36450]\tLoss: 627.1971\n",
      "Training Epoch: 3 [22850/36450]\tLoss: 628.9905\n",
      "Training Epoch: 3 [22900/36450]\tLoss: 647.0663\n",
      "Training Epoch: 3 [22950/36450]\tLoss: 621.3665\n",
      "Training Epoch: 3 [23000/36450]\tLoss: 650.6467\n",
      "Training Epoch: 3 [23050/36450]\tLoss: 627.1534\n",
      "Training Epoch: 3 [23100/36450]\tLoss: 596.0297\n",
      "Training Epoch: 3 [23150/36450]\tLoss: 590.8242\n",
      "Training Epoch: 3 [23200/36450]\tLoss: 621.2318\n",
      "Training Epoch: 3 [23250/36450]\tLoss: 615.3308\n",
      "Training Epoch: 3 [23300/36450]\tLoss: 612.5652\n",
      "Training Epoch: 3 [23350/36450]\tLoss: 644.1324\n",
      "Training Epoch: 3 [23400/36450]\tLoss: 570.8852\n",
      "Training Epoch: 3 [23450/36450]\tLoss: 589.4531\n",
      "Training Epoch: 3 [23500/36450]\tLoss: 609.1705\n",
      "Training Epoch: 3 [23550/36450]\tLoss: 570.9137\n",
      "Training Epoch: 3 [23600/36450]\tLoss: 613.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [23650/36450]\tLoss: 599.0295\n",
      "Training Epoch: 3 [23700/36450]\tLoss: 608.2108\n",
      "Training Epoch: 3 [23750/36450]\tLoss: 574.5064\n",
      "Training Epoch: 3 [23800/36450]\tLoss: 587.9087\n",
      "Training Epoch: 3 [23850/36450]\tLoss: 598.7872\n",
      "Training Epoch: 3 [23900/36450]\tLoss: 566.8516\n",
      "Training Epoch: 3 [23950/36450]\tLoss: 592.6470\n",
      "Training Epoch: 3 [24000/36450]\tLoss: 584.7955\n",
      "Training Epoch: 3 [24050/36450]\tLoss: 613.6752\n",
      "Training Epoch: 3 [24100/36450]\tLoss: 555.9611\n",
      "Training Epoch: 3 [24150/36450]\tLoss: 625.6064\n",
      "Training Epoch: 3 [24200/36450]\tLoss: 551.3763\n",
      "Training Epoch: 3 [24250/36450]\tLoss: 543.1630\n",
      "Training Epoch: 3 [24300/36450]\tLoss: 596.1455\n",
      "Training Epoch: 3 [24350/36450]\tLoss: 616.3408\n",
      "Training Epoch: 3 [24400/36450]\tLoss: 665.9829\n",
      "Training Epoch: 3 [24450/36450]\tLoss: 591.4916\n",
      "Training Epoch: 3 [24500/36450]\tLoss: 606.9257\n",
      "Training Epoch: 3 [24550/36450]\tLoss: 543.2792\n",
      "Training Epoch: 3 [24600/36450]\tLoss: 649.6010\n",
      "Training Epoch: 3 [24650/36450]\tLoss: 548.1315\n",
      "Training Epoch: 3 [24700/36450]\tLoss: 591.6937\n",
      "Training Epoch: 3 [24750/36450]\tLoss: 640.7820\n",
      "Training Epoch: 3 [24800/36450]\tLoss: 575.8639\n",
      "Training Epoch: 3 [24850/36450]\tLoss: 587.9077\n",
      "Training Epoch: 3 [24900/36450]\tLoss: 570.9595\n",
      "Training Epoch: 3 [24950/36450]\tLoss: 630.5481\n",
      "Training Epoch: 3 [25000/36450]\tLoss: 569.1829\n",
      "Training Epoch: 3 [25050/36450]\tLoss: 577.4222\n",
      "Training Epoch: 3 [25100/36450]\tLoss: 608.6041\n",
      "Training Epoch: 3 [25150/36450]\tLoss: 600.6070\n",
      "Training Epoch: 3 [25200/36450]\tLoss: 638.6562\n",
      "Training Epoch: 3 [25250/36450]\tLoss: 612.6723\n",
      "Training Epoch: 3 [25300/36450]\tLoss: 554.5380\n",
      "Training Epoch: 3 [25350/36450]\tLoss: 565.8051\n",
      "Training Epoch: 3 [25400/36450]\tLoss: 543.4709\n",
      "Training Epoch: 3 [25450/36450]\tLoss: 611.9321\n",
      "Training Epoch: 3 [25500/36450]\tLoss: 627.5656\n",
      "Training Epoch: 3 [25550/36450]\tLoss: 587.7432\n",
      "Training Epoch: 3 [25600/36450]\tLoss: 577.9707\n",
      "Training Epoch: 3 [25650/36450]\tLoss: 565.4060\n",
      "Training Epoch: 3 [25700/36450]\tLoss: 593.2054\n",
      "Training Epoch: 3 [25750/36450]\tLoss: 659.9373\n",
      "Training Epoch: 3 [25800/36450]\tLoss: 611.3506\n",
      "Training Epoch: 3 [25850/36450]\tLoss: 634.1230\n",
      "Training Epoch: 3 [25900/36450]\tLoss: 553.2502\n",
      "Training Epoch: 3 [25950/36450]\tLoss: 639.0606\n",
      "Training Epoch: 3 [26000/36450]\tLoss: 596.4518\n",
      "Training Epoch: 3 [26050/36450]\tLoss: 608.1045\n",
      "Training Epoch: 3 [26100/36450]\tLoss: 595.4691\n",
      "Training Epoch: 3 [26150/36450]\tLoss: 566.9279\n",
      "Training Epoch: 3 [26200/36450]\tLoss: 509.9552\n",
      "Training Epoch: 3 [26250/36450]\tLoss: 592.8691\n",
      "Training Epoch: 3 [26300/36450]\tLoss: 607.7997\n",
      "Training Epoch: 3 [26350/36450]\tLoss: 585.2921\n",
      "Training Epoch: 3 [26400/36450]\tLoss: 616.1346\n",
      "Training Epoch: 3 [26450/36450]\tLoss: 563.5549\n",
      "Training Epoch: 3 [26500/36450]\tLoss: 630.2962\n",
      "Training Epoch: 3 [26550/36450]\tLoss: 619.6268\n",
      "Training Epoch: 3 [26600/36450]\tLoss: 572.4342\n",
      "Training Epoch: 3 [26650/36450]\tLoss: 634.7966\n",
      "Training Epoch: 3 [26700/36450]\tLoss: 626.7300\n",
      "Training Epoch: 3 [26750/36450]\tLoss: 567.3826\n",
      "Training Epoch: 3 [26800/36450]\tLoss: 592.7232\n",
      "Training Epoch: 3 [26850/36450]\tLoss: 567.8378\n",
      "Training Epoch: 3 [26900/36450]\tLoss: 590.8842\n",
      "Training Epoch: 3 [26950/36450]\tLoss: 579.0000\n",
      "Training Epoch: 3 [27000/36450]\tLoss: 601.2919\n",
      "Training Epoch: 3 [27050/36450]\tLoss: 619.4165\n",
      "Training Epoch: 3 [27100/36450]\tLoss: 615.2880\n",
      "Training Epoch: 3 [27150/36450]\tLoss: 591.9638\n",
      "Training Epoch: 3 [27200/36450]\tLoss: 581.0262\n",
      "Training Epoch: 3 [27250/36450]\tLoss: 609.0051\n",
      "Training Epoch: 3 [27300/36450]\tLoss: 598.2846\n",
      "Training Epoch: 3 [27350/36450]\tLoss: 593.4374\n",
      "Training Epoch: 3 [27400/36450]\tLoss: 617.0537\n",
      "Training Epoch: 3 [27450/36450]\tLoss: 567.7259\n",
      "Training Epoch: 3 [27500/36450]\tLoss: 602.7693\n",
      "Training Epoch: 3 [27550/36450]\tLoss: 590.8071\n",
      "Training Epoch: 3 [27600/36450]\tLoss: 593.8691\n",
      "Training Epoch: 3 [27650/36450]\tLoss: 594.9161\n",
      "Training Epoch: 3 [27700/36450]\tLoss: 577.6501\n",
      "Training Epoch: 3 [27750/36450]\tLoss: 566.3376\n",
      "Training Epoch: 3 [27800/36450]\tLoss: 540.8617\n",
      "Training Epoch: 3 [27850/36450]\tLoss: 611.6575\n",
      "Training Epoch: 3 [27900/36450]\tLoss: 605.6481\n",
      "Training Epoch: 3 [27950/36450]\tLoss: 597.9578\n",
      "Training Epoch: 3 [28000/36450]\tLoss: 581.6020\n",
      "Training Epoch: 3 [28050/36450]\tLoss: 587.4596\n",
      "Training Epoch: 3 [28100/36450]\tLoss: 506.8653\n",
      "Training Epoch: 3 [28150/36450]\tLoss: 574.5958\n",
      "Training Epoch: 3 [28200/36450]\tLoss: 582.8281\n",
      "Training Epoch: 3 [28250/36450]\tLoss: 574.5893\n",
      "Training Epoch: 3 [28300/36450]\tLoss: 579.6429\n",
      "Training Epoch: 3 [28350/36450]\tLoss: 602.5106\n",
      "Training Epoch: 3 [28400/36450]\tLoss: 617.5674\n",
      "Training Epoch: 3 [28450/36450]\tLoss: 561.9072\n",
      "Training Epoch: 3 [28500/36450]\tLoss: 577.2928\n",
      "Training Epoch: 3 [28550/36450]\tLoss: 559.6660\n",
      "Training Epoch: 3 [28600/36450]\tLoss: 591.4749\n",
      "Training Epoch: 3 [28650/36450]\tLoss: 571.0884\n",
      "Training Epoch: 3 [28700/36450]\tLoss: 608.9887\n",
      "Training Epoch: 3 [28750/36450]\tLoss: 604.4085\n",
      "Training Epoch: 3 [28800/36450]\tLoss: 588.5590\n",
      "Training Epoch: 3 [28850/36450]\tLoss: 558.6504\n",
      "Training Epoch: 3 [28900/36450]\tLoss: 589.0518\n",
      "Training Epoch: 3 [28950/36450]\tLoss: 606.7145\n",
      "Training Epoch: 3 [29000/36450]\tLoss: 625.8102\n",
      "Training Epoch: 3 [29050/36450]\tLoss: 589.5826\n",
      "Training Epoch: 3 [29100/36450]\tLoss: 604.7750\n",
      "Training Epoch: 3 [29150/36450]\tLoss: 577.7106\n",
      "Training Epoch: 3 [29200/36450]\tLoss: 591.2971\n",
      "Training Epoch: 3 [29250/36450]\tLoss: 573.6252\n",
      "Training Epoch: 3 [29300/36450]\tLoss: 605.1177\n",
      "Training Epoch: 3 [29350/36450]\tLoss: 627.7798\n",
      "Training Epoch: 3 [29400/36450]\tLoss: 653.3906\n",
      "Training Epoch: 3 [29450/36450]\tLoss: 546.6976\n",
      "Training Epoch: 3 [29500/36450]\tLoss: 563.0042\n",
      "Training Epoch: 3 [29550/36450]\tLoss: 570.9885\n",
      "Training Epoch: 3 [29600/36450]\tLoss: 589.5296\n",
      "Training Epoch: 3 [29650/36450]\tLoss: 565.2338\n",
      "Training Epoch: 3 [29700/36450]\tLoss: 576.6731\n",
      "Training Epoch: 3 [29750/36450]\tLoss: 560.6511\n",
      "Training Epoch: 3 [29800/36450]\tLoss: 608.5266\n",
      "Training Epoch: 3 [29850/36450]\tLoss: 579.7318\n",
      "Training Epoch: 3 [29900/36450]\tLoss: 581.3590\n",
      "Training Epoch: 3 [29950/36450]\tLoss: 618.3681\n",
      "Training Epoch: 3 [30000/36450]\tLoss: 586.6668\n",
      "Training Epoch: 3 [30050/36450]\tLoss: 577.1753\n",
      "Training Epoch: 3 [30100/36450]\tLoss: 607.8123\n",
      "Training Epoch: 3 [30150/36450]\tLoss: 563.5867\n",
      "Training Epoch: 3 [30200/36450]\tLoss: 569.4839\n",
      "Training Epoch: 3 [30250/36450]\tLoss: 555.1345\n",
      "Training Epoch: 3 [30300/36450]\tLoss: 580.0695\n",
      "Training Epoch: 3 [30350/36450]\tLoss: 578.3774\n",
      "Training Epoch: 3 [30400/36450]\tLoss: 581.3944\n",
      "Training Epoch: 3 [30450/36450]\tLoss: 586.7062\n",
      "Training Epoch: 3 [30500/36450]\tLoss: 532.5118\n",
      "Training Epoch: 3 [30550/36450]\tLoss: 530.9174\n",
      "Training Epoch: 3 [30600/36450]\tLoss: 595.8175\n",
      "Training Epoch: 3 [30650/36450]\tLoss: 610.2754\n",
      "Training Epoch: 3 [30700/36450]\tLoss: 572.4947\n",
      "Training Epoch: 3 [30750/36450]\tLoss: 601.0850\n",
      "Training Epoch: 3 [30800/36450]\tLoss: 609.9019\n",
      "Training Epoch: 3 [30850/36450]\tLoss: 581.2509\n",
      "Training Epoch: 3 [30900/36450]\tLoss: 580.9211\n",
      "Training Epoch: 3 [30950/36450]\tLoss: 558.4886\n",
      "Training Epoch: 3 [31000/36450]\tLoss: 637.1671\n",
      "Training Epoch: 3 [31050/36450]\tLoss: 548.5927\n",
      "Training Epoch: 3 [31100/36450]\tLoss: 553.0130\n",
      "Training Epoch: 3 [31150/36450]\tLoss: 559.3016\n",
      "Training Epoch: 3 [31200/36450]\tLoss: 566.6119\n",
      "Training Epoch: 3 [31250/36450]\tLoss: 584.8832\n",
      "Training Epoch: 3 [31300/36450]\tLoss: 585.9854\n",
      "Training Epoch: 3 [31350/36450]\tLoss: 560.2994\n",
      "Training Epoch: 3 [31400/36450]\tLoss: 616.4274\n",
      "Training Epoch: 3 [31450/36450]\tLoss: 529.5670\n",
      "Training Epoch: 3 [31500/36450]\tLoss: 549.9020\n",
      "Training Epoch: 3 [31550/36450]\tLoss: 596.2828\n",
      "Training Epoch: 3 [31600/36450]\tLoss: 650.7425\n",
      "Training Epoch: 3 [31650/36450]\tLoss: 552.5104\n",
      "Training Epoch: 3 [31700/36450]\tLoss: 630.2534\n",
      "Training Epoch: 3 [31750/36450]\tLoss: 609.7491\n",
      "Training Epoch: 3 [31800/36450]\tLoss: 578.8854\n",
      "Training Epoch: 3 [31850/36450]\tLoss: 587.0454\n",
      "Training Epoch: 3 [31900/36450]\tLoss: 583.0070\n",
      "Training Epoch: 3 [31950/36450]\tLoss: 600.8373\n",
      "Training Epoch: 3 [32000/36450]\tLoss: 616.9539\n",
      "Training Epoch: 3 [32050/36450]\tLoss: 533.7073\n",
      "Training Epoch: 3 [32100/36450]\tLoss: 561.9524\n",
      "Training Epoch: 3 [32150/36450]\tLoss: 579.0283\n",
      "Training Epoch: 3 [32200/36450]\tLoss: 632.6682\n",
      "Training Epoch: 3 [32250/36450]\tLoss: 623.4703\n",
      "Training Epoch: 3 [32300/36450]\tLoss: 614.7092\n",
      "Training Epoch: 3 [32350/36450]\tLoss: 601.1874\n",
      "Training Epoch: 3 [32400/36450]\tLoss: 578.3486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [32450/36450]\tLoss: 586.3676\n",
      "Training Epoch: 3 [32500/36450]\tLoss: 560.5509\n",
      "Training Epoch: 3 [32550/36450]\tLoss: 575.2612\n",
      "Training Epoch: 3 [32600/36450]\tLoss: 584.2888\n",
      "Training Epoch: 3 [32650/36450]\tLoss: 578.2030\n",
      "Training Epoch: 3 [32700/36450]\tLoss: 601.6864\n",
      "Training Epoch: 3 [32750/36450]\tLoss: 597.3870\n",
      "Training Epoch: 3 [32800/36450]\tLoss: 568.8140\n",
      "Training Epoch: 3 [32850/36450]\tLoss: 637.9033\n",
      "Training Epoch: 3 [32900/36450]\tLoss: 570.3207\n",
      "Training Epoch: 3 [32950/36450]\tLoss: 594.9857\n",
      "Training Epoch: 3 [33000/36450]\tLoss: 594.3549\n",
      "Training Epoch: 3 [33050/36450]\tLoss: 557.4948\n",
      "Training Epoch: 3 [33100/36450]\tLoss: 599.5021\n",
      "Training Epoch: 3 [33150/36450]\tLoss: 605.1627\n",
      "Training Epoch: 3 [33200/36450]\tLoss: 502.2153\n",
      "Training Epoch: 3 [33250/36450]\tLoss: 566.2689\n",
      "Training Epoch: 3 [33300/36450]\tLoss: 569.3997\n",
      "Training Epoch: 3 [33350/36450]\tLoss: 572.1613\n",
      "Training Epoch: 3 [33400/36450]\tLoss: 557.1617\n",
      "Training Epoch: 3 [33450/36450]\tLoss: 573.4629\n",
      "Training Epoch: 3 [33500/36450]\tLoss: 625.0494\n",
      "Training Epoch: 3 [33550/36450]\tLoss: 548.5526\n",
      "Training Epoch: 3 [33600/36450]\tLoss: 597.8160\n",
      "Training Epoch: 3 [33650/36450]\tLoss: 562.2119\n",
      "Training Epoch: 3 [33700/36450]\tLoss: 544.0809\n",
      "Training Epoch: 3 [33750/36450]\tLoss: 579.1400\n",
      "Training Epoch: 3 [33800/36450]\tLoss: 598.8102\n",
      "Training Epoch: 3 [33850/36450]\tLoss: 595.8972\n",
      "Training Epoch: 3 [33900/36450]\tLoss: 657.9156\n",
      "Training Epoch: 3 [33950/36450]\tLoss: 583.4333\n",
      "Training Epoch: 3 [34000/36450]\tLoss: 632.5267\n",
      "Training Epoch: 3 [34050/36450]\tLoss: 538.3094\n",
      "Training Epoch: 3 [34100/36450]\tLoss: 561.8240\n",
      "Training Epoch: 3 [34150/36450]\tLoss: 607.4962\n",
      "Training Epoch: 3 [34200/36450]\tLoss: 564.7845\n",
      "Training Epoch: 3 [34250/36450]\tLoss: 590.9973\n",
      "Training Epoch: 3 [34300/36450]\tLoss: 555.6318\n",
      "Training Epoch: 3 [34350/36450]\tLoss: 541.5797\n",
      "Training Epoch: 3 [34400/36450]\tLoss: 606.6146\n",
      "Training Epoch: 3 [34450/36450]\tLoss: 597.2001\n",
      "Training Epoch: 3 [34500/36450]\tLoss: 570.8428\n",
      "Training Epoch: 3 [34550/36450]\tLoss: 531.9160\n",
      "Training Epoch: 3 [34600/36450]\tLoss: 555.4352\n",
      "Training Epoch: 3 [34650/36450]\tLoss: 564.4836\n",
      "Training Epoch: 3 [34700/36450]\tLoss: 587.3185\n",
      "Training Epoch: 3 [34750/36450]\tLoss: 547.2751\n",
      "Training Epoch: 3 [34800/36450]\tLoss: 577.6426\n",
      "Training Epoch: 3 [34850/36450]\tLoss: 618.7280\n",
      "Training Epoch: 3 [34900/36450]\tLoss: 564.1382\n",
      "Training Epoch: 3 [34950/36450]\tLoss: 620.3279\n",
      "Training Epoch: 3 [35000/36450]\tLoss: 553.1743\n",
      "Training Epoch: 3 [35050/36450]\tLoss: 607.1374\n",
      "Training Epoch: 3 [35100/36450]\tLoss: 614.4661\n",
      "Training Epoch: 3 [35150/36450]\tLoss: 578.8824\n",
      "Training Epoch: 3 [35200/36450]\tLoss: 555.1345\n",
      "Training Epoch: 3 [35250/36450]\tLoss: 565.0446\n",
      "Training Epoch: 3 [35300/36450]\tLoss: 584.6317\n",
      "Training Epoch: 3 [35350/36450]\tLoss: 612.9388\n",
      "Training Epoch: 3 [35400/36450]\tLoss: 594.1055\n",
      "Training Epoch: 3 [35450/36450]\tLoss: 544.2213\n",
      "Training Epoch: 3 [35500/36450]\tLoss: 607.6289\n",
      "Training Epoch: 3 [35550/36450]\tLoss: 534.2299\n",
      "Training Epoch: 3 [35600/36450]\tLoss: 626.8200\n",
      "Training Epoch: 3 [35650/36450]\tLoss: 584.5275\n",
      "Training Epoch: 3 [35700/36450]\tLoss: 591.1256\n",
      "Training Epoch: 3 [35750/36450]\tLoss: 620.2418\n",
      "Training Epoch: 3 [35800/36450]\tLoss: 581.8398\n",
      "Training Epoch: 3 [35850/36450]\tLoss: 538.1742\n",
      "Training Epoch: 3 [35900/36450]\tLoss: 588.1161\n",
      "Training Epoch: 3 [35950/36450]\tLoss: 555.1517\n",
      "Training Epoch: 3 [36000/36450]\tLoss: 564.4261\n",
      "Training Epoch: 3 [36050/36450]\tLoss: 596.4963\n",
      "Training Epoch: 3 [36100/36450]\tLoss: 562.1765\n",
      "Training Epoch: 3 [36150/36450]\tLoss: 583.6904\n",
      "Training Epoch: 3 [36200/36450]\tLoss: 596.4637\n",
      "Training Epoch: 3 [36250/36450]\tLoss: 585.7314\n",
      "Training Epoch: 3 [36300/36450]\tLoss: 609.1511\n",
      "Training Epoch: 3 [36350/36450]\tLoss: 579.4458\n",
      "Training Epoch: 3 [36400/36450]\tLoss: 597.7643\n",
      "Training Epoch: 3 [36450/36450]\tLoss: 548.5779\n",
      "Training Epoch: 3 [4050/4050]\tLoss: 269.8743\n",
      "Training Epoch: 4 [50/36450]\tLoss: 589.2072\n",
      "Training Epoch: 4 [100/36450]\tLoss: 646.2292\n",
      "Training Epoch: 4 [150/36450]\tLoss: 550.9875\n",
      "Training Epoch: 4 [200/36450]\tLoss: 572.7464\n",
      "Training Epoch: 4 [250/36450]\tLoss: 600.5952\n",
      "Training Epoch: 4 [300/36450]\tLoss: 534.5313\n",
      "Training Epoch: 4 [350/36450]\tLoss: 620.7220\n",
      "Training Epoch: 4 [400/36450]\tLoss: 630.3193\n",
      "Training Epoch: 4 [450/36450]\tLoss: 560.8951\n",
      "Training Epoch: 4 [500/36450]\tLoss: 564.5314\n",
      "Training Epoch: 4 [550/36450]\tLoss: 571.2044\n",
      "Training Epoch: 4 [600/36450]\tLoss: 537.2893\n",
      "Training Epoch: 4 [650/36450]\tLoss: 610.2947\n",
      "Training Epoch: 4 [700/36450]\tLoss: 552.6244\n",
      "Training Epoch: 4 [750/36450]\tLoss: 598.2645\n",
      "Training Epoch: 4 [800/36450]\tLoss: 571.3800\n",
      "Training Epoch: 4 [850/36450]\tLoss: 563.6023\n",
      "Training Epoch: 4 [900/36450]\tLoss: 588.4256\n",
      "Training Epoch: 4 [950/36450]\tLoss: 586.5005\n",
      "Training Epoch: 4 [1000/36450]\tLoss: 588.4861\n",
      "Training Epoch: 4 [1050/36450]\tLoss: 568.0148\n",
      "Training Epoch: 4 [1100/36450]\tLoss: 567.5362\n",
      "Training Epoch: 4 [1150/36450]\tLoss: 555.6421\n",
      "Training Epoch: 4 [1200/36450]\tLoss: 568.9603\n",
      "Training Epoch: 4 [1250/36450]\tLoss: 579.2869\n",
      "Training Epoch: 4 [1300/36450]\tLoss: 541.4814\n",
      "Training Epoch: 4 [1350/36450]\tLoss: 622.8328\n",
      "Training Epoch: 4 [1400/36450]\tLoss: 532.9076\n",
      "Training Epoch: 4 [1450/36450]\tLoss: 603.5375\n",
      "Training Epoch: 4 [1500/36450]\tLoss: 551.3644\n",
      "Training Epoch: 4 [1550/36450]\tLoss: 568.1072\n",
      "Training Epoch: 4 [1600/36450]\tLoss: 578.6934\n",
      "Training Epoch: 4 [1650/36450]\tLoss: 592.7442\n",
      "Training Epoch: 4 [1700/36450]\tLoss: 584.1374\n",
      "Training Epoch: 4 [1750/36450]\tLoss: 569.2251\n",
      "Training Epoch: 4 [1800/36450]\tLoss: 529.0550\n",
      "Training Epoch: 4 [1850/36450]\tLoss: 552.6493\n",
      "Training Epoch: 4 [1900/36450]\tLoss: 530.6859\n",
      "Training Epoch: 4 [1950/36450]\tLoss: 549.0872\n",
      "Training Epoch: 4 [2000/36450]\tLoss: 536.8251\n",
      "Training Epoch: 4 [2050/36450]\tLoss: 570.2043\n",
      "Training Epoch: 4 [2100/36450]\tLoss: 608.7796\n",
      "Training Epoch: 4 [2150/36450]\tLoss: 562.5869\n",
      "Training Epoch: 4 [2200/36450]\tLoss: 550.7848\n",
      "Training Epoch: 4 [2250/36450]\tLoss: 557.7318\n",
      "Training Epoch: 4 [2300/36450]\tLoss: 578.7610\n",
      "Training Epoch: 4 [2350/36450]\tLoss: 585.6706\n",
      "Training Epoch: 4 [2400/36450]\tLoss: 546.6408\n",
      "Training Epoch: 4 [2450/36450]\tLoss: 563.1511\n",
      "Training Epoch: 4 [2500/36450]\tLoss: 616.4305\n",
      "Training Epoch: 4 [2550/36450]\tLoss: 583.9663\n",
      "Training Epoch: 4 [2600/36450]\tLoss: 549.2938\n",
      "Training Epoch: 4 [2650/36450]\tLoss: 549.4148\n",
      "Training Epoch: 4 [2700/36450]\tLoss: 534.1654\n",
      "Training Epoch: 4 [2750/36450]\tLoss: 572.5717\n",
      "Training Epoch: 4 [2800/36450]\tLoss: 535.4741\n",
      "Training Epoch: 4 [2850/36450]\tLoss: 563.3596\n",
      "Training Epoch: 4 [2900/36450]\tLoss: 570.3714\n",
      "Training Epoch: 4 [2950/36450]\tLoss: 570.2462\n",
      "Training Epoch: 4 [3000/36450]\tLoss: 551.2094\n",
      "Training Epoch: 4 [3050/36450]\tLoss: 538.9489\n",
      "Training Epoch: 4 [3100/36450]\tLoss: 549.8698\n",
      "Training Epoch: 4 [3150/36450]\tLoss: 580.8018\n",
      "Training Epoch: 4 [3200/36450]\tLoss: 550.7739\n",
      "Training Epoch: 4 [3250/36450]\tLoss: 544.0215\n",
      "Training Epoch: 4 [3300/36450]\tLoss: 575.3188\n",
      "Training Epoch: 4 [3350/36450]\tLoss: 605.9751\n",
      "Training Epoch: 4 [3400/36450]\tLoss: 565.0578\n",
      "Training Epoch: 4 [3450/36450]\tLoss: 548.4210\n",
      "Training Epoch: 4 [3500/36450]\tLoss: 507.1373\n",
      "Training Epoch: 4 [3550/36450]\tLoss: 565.8401\n",
      "Training Epoch: 4 [3600/36450]\tLoss: 545.9019\n",
      "Training Epoch: 4 [3650/36450]\tLoss: 566.3933\n",
      "Training Epoch: 4 [3700/36450]\tLoss: 570.3978\n",
      "Training Epoch: 4 [3750/36450]\tLoss: 551.6600\n",
      "Training Epoch: 4 [3800/36450]\tLoss: 569.7927\n",
      "Training Epoch: 4 [3850/36450]\tLoss: 572.5472\n",
      "Training Epoch: 4 [3900/36450]\tLoss: 538.2754\n",
      "Training Epoch: 4 [3950/36450]\tLoss: 539.2905\n",
      "Training Epoch: 4 [4000/36450]\tLoss: 599.1088\n",
      "Training Epoch: 4 [4050/36450]\tLoss: 582.1296\n",
      "Training Epoch: 4 [4100/36450]\tLoss: 573.6057\n",
      "Training Epoch: 4 [4150/36450]\tLoss: 536.6917\n",
      "Training Epoch: 4 [4200/36450]\tLoss: 522.0892\n",
      "Training Epoch: 4 [4250/36450]\tLoss: 556.5924\n",
      "Training Epoch: 4 [4300/36450]\tLoss: 639.4774\n",
      "Training Epoch: 4 [4350/36450]\tLoss: 566.7189\n",
      "Training Epoch: 4 [4400/36450]\tLoss: 521.1420\n",
      "Training Epoch: 4 [4450/36450]\tLoss: 562.4204\n",
      "Training Epoch: 4 [4500/36450]\tLoss: 587.5419\n",
      "Training Epoch: 4 [4550/36450]\tLoss: 562.0033\n",
      "Training Epoch: 4 [4600/36450]\tLoss: 556.0034\n",
      "Training Epoch: 4 [4650/36450]\tLoss: 510.7121\n",
      "Training Epoch: 4 [4700/36450]\tLoss: 583.0027\n",
      "Training Epoch: 4 [4750/36450]\tLoss: 595.3645\n",
      "Training Epoch: 4 [4800/36450]\tLoss: 577.9811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [4850/36450]\tLoss: 549.3212\n",
      "Training Epoch: 4 [4900/36450]\tLoss: 538.0317\n",
      "Training Epoch: 4 [4950/36450]\tLoss: 603.5884\n",
      "Training Epoch: 4 [5000/36450]\tLoss: 546.6028\n",
      "Training Epoch: 4 [5050/36450]\tLoss: 543.8677\n",
      "Training Epoch: 4 [5100/36450]\tLoss: 539.1689\n",
      "Training Epoch: 4 [5150/36450]\tLoss: 537.4986\n",
      "Training Epoch: 4 [5200/36450]\tLoss: 502.6562\n",
      "Training Epoch: 4 [5250/36450]\tLoss: 580.8885\n",
      "Training Epoch: 4 [5300/36450]\tLoss: 524.0299\n",
      "Training Epoch: 4 [5350/36450]\tLoss: 549.7158\n",
      "Training Epoch: 4 [5400/36450]\tLoss: 540.8253\n",
      "Training Epoch: 4 [5450/36450]\tLoss: 535.6432\n",
      "Training Epoch: 4 [5500/36450]\tLoss: 542.1769\n",
      "Training Epoch: 4 [5550/36450]\tLoss: 549.3896\n",
      "Training Epoch: 4 [5600/36450]\tLoss: 560.8851\n",
      "Training Epoch: 4 [5650/36450]\tLoss: 563.2260\n",
      "Training Epoch: 4 [5700/36450]\tLoss: 619.4739\n",
      "Training Epoch: 4 [5750/36450]\tLoss: 624.7223\n",
      "Training Epoch: 4 [5800/36450]\tLoss: 555.2300\n",
      "Training Epoch: 4 [5850/36450]\tLoss: 536.7325\n",
      "Training Epoch: 4 [5900/36450]\tLoss: 546.1627\n",
      "Training Epoch: 4 [5950/36450]\tLoss: 562.3271\n",
      "Training Epoch: 4 [6000/36450]\tLoss: 565.6071\n",
      "Training Epoch: 4 [6050/36450]\tLoss: 540.8362\n",
      "Training Epoch: 4 [6100/36450]\tLoss: 578.7155\n",
      "Training Epoch: 4 [6150/36450]\tLoss: 557.2136\n",
      "Training Epoch: 4 [6200/36450]\tLoss: 533.0755\n",
      "Training Epoch: 4 [6250/36450]\tLoss: 546.9232\n",
      "Training Epoch: 4 [6300/36450]\tLoss: 546.2387\n",
      "Training Epoch: 4 [6350/36450]\tLoss: 543.6124\n",
      "Training Epoch: 4 [6400/36450]\tLoss: 529.7184\n",
      "Training Epoch: 4 [6450/36450]\tLoss: 537.9930\n",
      "Training Epoch: 4 [6500/36450]\tLoss: 569.1427\n",
      "Training Epoch: 4 [6550/36450]\tLoss: 629.3268\n",
      "Training Epoch: 4 [6600/36450]\tLoss: 567.9567\n",
      "Training Epoch: 4 [6650/36450]\tLoss: 538.0146\n",
      "Training Epoch: 4 [6700/36450]\tLoss: 598.0133\n",
      "Training Epoch: 4 [6750/36450]\tLoss: 585.5506\n",
      "Training Epoch: 4 [6800/36450]\tLoss: 508.2004\n",
      "Training Epoch: 4 [6850/36450]\tLoss: 585.6505\n",
      "Training Epoch: 4 [6900/36450]\tLoss: 543.9280\n",
      "Training Epoch: 4 [6950/36450]\tLoss: 556.0518\n",
      "Training Epoch: 4 [7000/36450]\tLoss: 566.4088\n",
      "Training Epoch: 4 [7050/36450]\tLoss: 586.2847\n",
      "Training Epoch: 4 [7100/36450]\tLoss: 648.4382\n",
      "Training Epoch: 4 [7150/36450]\tLoss: 531.0312\n",
      "Training Epoch: 4 [7200/36450]\tLoss: 557.0474\n",
      "Training Epoch: 4 [7250/36450]\tLoss: 556.1467\n",
      "Training Epoch: 4 [7300/36450]\tLoss: 527.9485\n",
      "Training Epoch: 4 [7350/36450]\tLoss: 579.5770\n",
      "Training Epoch: 4 [7400/36450]\tLoss: 571.2693\n",
      "Training Epoch: 4 [7450/36450]\tLoss: 558.1332\n",
      "Training Epoch: 4 [7500/36450]\tLoss: 539.1130\n",
      "Training Epoch: 4 [7550/36450]\tLoss: 550.2272\n",
      "Training Epoch: 4 [7600/36450]\tLoss: 588.1846\n",
      "Training Epoch: 4 [7650/36450]\tLoss: 510.9678\n",
      "Training Epoch: 4 [7700/36450]\tLoss: 527.4410\n",
      "Training Epoch: 4 [7750/36450]\tLoss: 586.7931\n",
      "Training Epoch: 4 [7800/36450]\tLoss: 605.1310\n",
      "Training Epoch: 4 [7850/36450]\tLoss: 586.7391\n",
      "Training Epoch: 4 [7900/36450]\tLoss: 554.2909\n",
      "Training Epoch: 4 [7950/36450]\tLoss: 532.7415\n",
      "Training Epoch: 4 [8000/36450]\tLoss: 578.6101\n",
      "Training Epoch: 4 [8050/36450]\tLoss: 549.2714\n",
      "Training Epoch: 4 [8100/36450]\tLoss: 581.7275\n",
      "Training Epoch: 4 [8150/36450]\tLoss: 503.3519\n",
      "Training Epoch: 4 [8200/36450]\tLoss: 545.4571\n",
      "Training Epoch: 4 [8250/36450]\tLoss: 562.8947\n",
      "Training Epoch: 4 [8300/36450]\tLoss: 543.9773\n",
      "Training Epoch: 4 [8350/36450]\tLoss: 573.4352\n",
      "Training Epoch: 4 [8400/36450]\tLoss: 537.0066\n",
      "Training Epoch: 4 [8450/36450]\tLoss: 579.7799\n",
      "Training Epoch: 4 [8500/36450]\tLoss: 579.6417\n",
      "Training Epoch: 4 [8550/36450]\tLoss: 557.9854\n",
      "Training Epoch: 4 [8600/36450]\tLoss: 613.5731\n",
      "Training Epoch: 4 [8650/36450]\tLoss: 531.7818\n",
      "Training Epoch: 4 [8700/36450]\tLoss: 527.7253\n",
      "Training Epoch: 4 [8750/36450]\tLoss: 565.7256\n",
      "Training Epoch: 4 [8800/36450]\tLoss: 535.5451\n",
      "Training Epoch: 4 [8850/36450]\tLoss: 538.8333\n",
      "Training Epoch: 4 [8900/36450]\tLoss: 524.4065\n",
      "Training Epoch: 4 [8950/36450]\tLoss: 562.3780\n",
      "Training Epoch: 4 [9000/36450]\tLoss: 524.8522\n",
      "Training Epoch: 4 [9050/36450]\tLoss: 514.9557\n",
      "Training Epoch: 4 [9100/36450]\tLoss: 519.5778\n",
      "Training Epoch: 4 [9150/36450]\tLoss: 604.3256\n",
      "Training Epoch: 4 [9200/36450]\tLoss: 529.0923\n",
      "Training Epoch: 4 [9250/36450]\tLoss: 539.4828\n",
      "Training Epoch: 4 [9300/36450]\tLoss: 579.0950\n",
      "Training Epoch: 4 [9350/36450]\tLoss: 555.7039\n",
      "Training Epoch: 4 [9400/36450]\tLoss: 544.5303\n",
      "Training Epoch: 4 [9450/36450]\tLoss: 515.0031\n",
      "Training Epoch: 4 [9500/36450]\tLoss: 530.3091\n",
      "Training Epoch: 4 [9550/36450]\tLoss: 551.4693\n",
      "Training Epoch: 4 [9600/36450]\tLoss: 569.6317\n",
      "Training Epoch: 4 [9650/36450]\tLoss: 563.3086\n",
      "Training Epoch: 4 [9700/36450]\tLoss: 602.1862\n",
      "Training Epoch: 4 [9750/36450]\tLoss: 559.1245\n",
      "Training Epoch: 4 [9800/36450]\tLoss: 515.8271\n",
      "Training Epoch: 4 [9850/36450]\tLoss: 634.6517\n",
      "Training Epoch: 4 [9900/36450]\tLoss: 492.0552\n",
      "Training Epoch: 4 [9950/36450]\tLoss: 592.3875\n",
      "Training Epoch: 4 [10000/36450]\tLoss: 563.2607\n",
      "Training Epoch: 4 [10050/36450]\tLoss: 539.1956\n",
      "Training Epoch: 4 [10100/36450]\tLoss: 523.3367\n",
      "Training Epoch: 4 [10150/36450]\tLoss: 555.3726\n",
      "Training Epoch: 4 [10200/36450]\tLoss: 528.3024\n",
      "Training Epoch: 4 [10250/36450]\tLoss: 565.1971\n",
      "Training Epoch: 4 [10300/36450]\tLoss: 521.8616\n",
      "Training Epoch: 4 [10350/36450]\tLoss: 525.3751\n",
      "Training Epoch: 4 [10400/36450]\tLoss: 561.4873\n",
      "Training Epoch: 4 [10450/36450]\tLoss: 563.4812\n",
      "Training Epoch: 4 [10500/36450]\tLoss: 571.3251\n",
      "Training Epoch: 4 [10550/36450]\tLoss: 573.4951\n",
      "Training Epoch: 4 [10600/36450]\tLoss: 495.9795\n",
      "Training Epoch: 4 [10650/36450]\tLoss: 553.0673\n",
      "Training Epoch: 4 [10700/36450]\tLoss: 576.6199\n",
      "Training Epoch: 4 [10750/36450]\tLoss: 520.9614\n",
      "Training Epoch: 4 [10800/36450]\tLoss: 547.9078\n",
      "Training Epoch: 4 [10850/36450]\tLoss: 534.9785\n",
      "Training Epoch: 4 [10900/36450]\tLoss: 502.9116\n",
      "Training Epoch: 4 [10950/36450]\tLoss: 566.1321\n",
      "Training Epoch: 4 [11000/36450]\tLoss: 526.3513\n",
      "Training Epoch: 4 [11050/36450]\tLoss: 619.2939\n",
      "Training Epoch: 4 [11100/36450]\tLoss: 597.5218\n",
      "Training Epoch: 4 [11150/36450]\tLoss: 575.0314\n",
      "Training Epoch: 4 [11200/36450]\tLoss: 550.6467\n",
      "Training Epoch: 4 [11250/36450]\tLoss: 518.5625\n",
      "Training Epoch: 4 [11300/36450]\tLoss: 517.1614\n",
      "Training Epoch: 4 [11350/36450]\tLoss: 532.4653\n",
      "Training Epoch: 4 [11400/36450]\tLoss: 533.2465\n",
      "Training Epoch: 4 [11450/36450]\tLoss: 579.2738\n",
      "Training Epoch: 4 [11500/36450]\tLoss: 578.0547\n",
      "Training Epoch: 4 [11550/36450]\tLoss: 540.8144\n",
      "Training Epoch: 4 [11600/36450]\tLoss: 602.4116\n",
      "Training Epoch: 4 [11650/36450]\tLoss: 548.9555\n",
      "Training Epoch: 4 [11700/36450]\tLoss: 506.9940\n",
      "Training Epoch: 4 [11750/36450]\tLoss: 557.7698\n",
      "Training Epoch: 4 [11800/36450]\tLoss: 640.1313\n",
      "Training Epoch: 4 [11850/36450]\tLoss: 577.5875\n",
      "Training Epoch: 4 [11900/36450]\tLoss: 544.5483\n",
      "Training Epoch: 4 [11950/36450]\tLoss: 515.4124\n",
      "Training Epoch: 4 [12000/36450]\tLoss: 530.5367\n",
      "Training Epoch: 4 [12050/36450]\tLoss: 552.1489\n",
      "Training Epoch: 4 [12100/36450]\tLoss: 568.9961\n",
      "Training Epoch: 4 [12150/36450]\tLoss: 527.2557\n",
      "Training Epoch: 4 [12200/36450]\tLoss: 572.4877\n",
      "Training Epoch: 4 [12250/36450]\tLoss: 537.9530\n",
      "Training Epoch: 4 [12300/36450]\tLoss: 589.3731\n",
      "Training Epoch: 4 [12350/36450]\tLoss: 571.6118\n",
      "Training Epoch: 4 [12400/36450]\tLoss: 533.5536\n",
      "Training Epoch: 4 [12450/36450]\tLoss: 545.8616\n",
      "Training Epoch: 4 [12500/36450]\tLoss: 534.2226\n",
      "Training Epoch: 4 [12550/36450]\tLoss: 535.1774\n",
      "Training Epoch: 4 [12600/36450]\tLoss: 516.0342\n",
      "Training Epoch: 4 [12650/36450]\tLoss: 524.1982\n",
      "Training Epoch: 4 [12700/36450]\tLoss: 555.9136\n",
      "Training Epoch: 4 [12750/36450]\tLoss: 515.3190\n",
      "Training Epoch: 4 [12800/36450]\tLoss: 528.6727\n",
      "Training Epoch: 4 [12850/36450]\tLoss: 557.3408\n",
      "Training Epoch: 4 [12900/36450]\tLoss: 589.9850\n",
      "Training Epoch: 4 [12950/36450]\tLoss: 514.8431\n",
      "Training Epoch: 4 [13000/36450]\tLoss: 523.2988\n",
      "Training Epoch: 4 [13050/36450]\tLoss: 537.0463\n",
      "Training Epoch: 4 [13100/36450]\tLoss: 562.5062\n",
      "Training Epoch: 4 [13150/36450]\tLoss: 515.3885\n",
      "Training Epoch: 4 [13200/36450]\tLoss: 580.6436\n",
      "Training Epoch: 4 [13250/36450]\tLoss: 562.6219\n",
      "Training Epoch: 4 [13300/36450]\tLoss: 587.4958\n",
      "Training Epoch: 4 [13350/36450]\tLoss: 608.9741\n",
      "Training Epoch: 4 [13400/36450]\tLoss: 528.1605\n",
      "Training Epoch: 4 [13450/36450]\tLoss: 552.2819\n",
      "Training Epoch: 4 [13500/36450]\tLoss: 524.6437\n",
      "Training Epoch: 4 [13550/36450]\tLoss: 555.2747\n",
      "Training Epoch: 4 [13600/36450]\tLoss: 559.1093\n",
      "Training Epoch: 4 [13650/36450]\tLoss: 552.4537\n",
      "Training Epoch: 4 [13700/36450]\tLoss: 531.5978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [13750/36450]\tLoss: 603.3540\n",
      "Training Epoch: 4 [13800/36450]\tLoss: 582.8522\n",
      "Training Epoch: 4 [13850/36450]\tLoss: 557.0580\n",
      "Training Epoch: 4 [13900/36450]\tLoss: 507.8011\n",
      "Training Epoch: 4 [13950/36450]\tLoss: 535.7338\n",
      "Training Epoch: 4 [14000/36450]\tLoss: 598.2905\n",
      "Training Epoch: 4 [14050/36450]\tLoss: 573.9805\n",
      "Training Epoch: 4 [14100/36450]\tLoss: 492.0667\n",
      "Training Epoch: 4 [14150/36450]\tLoss: 532.4884\n",
      "Training Epoch: 4 [14200/36450]\tLoss: 554.8661\n",
      "Training Epoch: 4 [14250/36450]\tLoss: 551.6802\n",
      "Training Epoch: 4 [14300/36450]\tLoss: 537.9790\n",
      "Training Epoch: 4 [14350/36450]\tLoss: 558.2206\n",
      "Training Epoch: 4 [14400/36450]\tLoss: 546.9318\n",
      "Training Epoch: 4 [14450/36450]\tLoss: 531.5475\n",
      "Training Epoch: 4 [14500/36450]\tLoss: 501.9984\n",
      "Training Epoch: 4 [14550/36450]\tLoss: 539.0312\n",
      "Training Epoch: 4 [14600/36450]\tLoss: 566.3005\n",
      "Training Epoch: 4 [14650/36450]\tLoss: 556.1795\n",
      "Training Epoch: 4 [14700/36450]\tLoss: 535.7935\n",
      "Training Epoch: 4 [14750/36450]\tLoss: 555.8815\n",
      "Training Epoch: 4 [14800/36450]\tLoss: 536.3519\n",
      "Training Epoch: 4 [14850/36450]\tLoss: 496.5044\n",
      "Training Epoch: 4 [14900/36450]\tLoss: 532.0995\n",
      "Training Epoch: 4 [14950/36450]\tLoss: 575.1667\n",
      "Training Epoch: 4 [15000/36450]\tLoss: 522.9960\n",
      "Training Epoch: 4 [15050/36450]\tLoss: 487.4089\n",
      "Training Epoch: 4 [15100/36450]\tLoss: 577.0023\n",
      "Training Epoch: 4 [15150/36450]\tLoss: 566.8751\n",
      "Training Epoch: 4 [15200/36450]\tLoss: 547.4015\n",
      "Training Epoch: 4 [15250/36450]\tLoss: 547.9195\n",
      "Training Epoch: 4 [15300/36450]\tLoss: 521.5895\n",
      "Training Epoch: 4 [15350/36450]\tLoss: 547.7797\n",
      "Training Epoch: 4 [15400/36450]\tLoss: 499.3000\n",
      "Training Epoch: 4 [15450/36450]\tLoss: 542.2529\n",
      "Training Epoch: 4 [15500/36450]\tLoss: 526.7480\n",
      "Training Epoch: 4 [15550/36450]\tLoss: 551.9218\n",
      "Training Epoch: 4 [15600/36450]\tLoss: 517.3342\n",
      "Training Epoch: 4 [15650/36450]\tLoss: 509.5503\n",
      "Training Epoch: 4 [15700/36450]\tLoss: 517.9151\n",
      "Training Epoch: 4 [15750/36450]\tLoss: 549.8723\n",
      "Training Epoch: 4 [15800/36450]\tLoss: 536.6818\n",
      "Training Epoch: 4 [15850/36450]\tLoss: 523.8781\n",
      "Training Epoch: 4 [15900/36450]\tLoss: 477.8896\n",
      "Training Epoch: 4 [15950/36450]\tLoss: 528.5793\n",
      "Training Epoch: 4 [16000/36450]\tLoss: 569.1794\n",
      "Training Epoch: 4 [16050/36450]\tLoss: 564.3483\n",
      "Training Epoch: 4 [16100/36450]\tLoss: 547.8292\n",
      "Training Epoch: 4 [16150/36450]\tLoss: 547.0732\n",
      "Training Epoch: 4 [16200/36450]\tLoss: 521.9484\n",
      "Training Epoch: 4 [16250/36450]\tLoss: 525.2635\n",
      "Training Epoch: 4 [16300/36450]\tLoss: 549.8276\n",
      "Training Epoch: 4 [16350/36450]\tLoss: 540.7112\n",
      "Training Epoch: 4 [16400/36450]\tLoss: 574.4393\n",
      "Training Epoch: 4 [16450/36450]\tLoss: 491.0325\n",
      "Training Epoch: 4 [16500/36450]\tLoss: 526.3016\n",
      "Training Epoch: 4 [16550/36450]\tLoss: 544.8778\n",
      "Training Epoch: 4 [16600/36450]\tLoss: 563.1216\n",
      "Training Epoch: 4 [16650/36450]\tLoss: 521.6446\n",
      "Training Epoch: 4 [16700/36450]\tLoss: 538.2010\n",
      "Training Epoch: 4 [16750/36450]\tLoss: 517.4868\n",
      "Training Epoch: 4 [16800/36450]\tLoss: 553.7455\n",
      "Training Epoch: 4 [16850/36450]\tLoss: 531.1795\n",
      "Training Epoch: 4 [16900/36450]\tLoss: 513.0468\n",
      "Training Epoch: 4 [16950/36450]\tLoss: 518.9648\n",
      "Training Epoch: 4 [17000/36450]\tLoss: 581.5037\n",
      "Training Epoch: 4 [17050/36450]\tLoss: 543.3403\n",
      "Training Epoch: 4 [17100/36450]\tLoss: 521.8331\n",
      "Training Epoch: 4 [17150/36450]\tLoss: 546.3982\n",
      "Training Epoch: 4 [17200/36450]\tLoss: 605.3102\n",
      "Training Epoch: 4 [17250/36450]\tLoss: 510.8229\n",
      "Training Epoch: 4 [17300/36450]\tLoss: 594.8329\n",
      "Training Epoch: 4 [17350/36450]\tLoss: 577.3956\n",
      "Training Epoch: 4 [17400/36450]\tLoss: 559.4999\n",
      "Training Epoch: 4 [17450/36450]\tLoss: 548.0222\n",
      "Training Epoch: 4 [17500/36450]\tLoss: 529.9821\n",
      "Training Epoch: 4 [17550/36450]\tLoss: 519.1436\n",
      "Training Epoch: 4 [17600/36450]\tLoss: 504.9435\n",
      "Training Epoch: 4 [17650/36450]\tLoss: 569.0782\n",
      "Training Epoch: 4 [17700/36450]\tLoss: 565.0924\n",
      "Training Epoch: 4 [17750/36450]\tLoss: 527.1321\n",
      "Training Epoch: 4 [17800/36450]\tLoss: 535.7834\n",
      "Training Epoch: 4 [17850/36450]\tLoss: 577.7715\n",
      "Training Epoch: 4 [17900/36450]\tLoss: 538.7685\n",
      "Training Epoch: 4 [17950/36450]\tLoss: 581.2422\n",
      "Training Epoch: 4 [18000/36450]\tLoss: 519.5084\n",
      "Training Epoch: 4 [18050/36450]\tLoss: 503.6655\n",
      "Training Epoch: 4 [18100/36450]\tLoss: 516.2793\n",
      "Training Epoch: 4 [18150/36450]\tLoss: 512.7495\n",
      "Training Epoch: 4 [18200/36450]\tLoss: 533.7223\n",
      "Training Epoch: 4 [18250/36450]\tLoss: 536.2012\n",
      "Training Epoch: 4 [18300/36450]\tLoss: 566.3328\n",
      "Training Epoch: 4 [18350/36450]\tLoss: 533.4453\n",
      "Training Epoch: 4 [18400/36450]\tLoss: 551.3137\n",
      "Training Epoch: 4 [18450/36450]\tLoss: 524.5649\n",
      "Training Epoch: 4 [18500/36450]\tLoss: 537.5862\n",
      "Training Epoch: 4 [18550/36450]\tLoss: 558.1584\n",
      "Training Epoch: 4 [18600/36450]\tLoss: 537.6930\n",
      "Training Epoch: 4 [18650/36450]\tLoss: 539.0672\n",
      "Training Epoch: 4 [18700/36450]\tLoss: 487.7273\n",
      "Training Epoch: 4 [18750/36450]\tLoss: 571.0549\n",
      "Training Epoch: 4 [18800/36450]\tLoss: 532.2345\n",
      "Training Epoch: 4 [18850/36450]\tLoss: 546.8926\n",
      "Training Epoch: 4 [18900/36450]\tLoss: 546.7823\n",
      "Training Epoch: 4 [18950/36450]\tLoss: 554.4381\n",
      "Training Epoch: 4 [19000/36450]\tLoss: 584.0953\n",
      "Training Epoch: 4 [19050/36450]\tLoss: 604.6663\n",
      "Training Epoch: 4 [19100/36450]\tLoss: 519.1301\n",
      "Training Epoch: 4 [19150/36450]\tLoss: 508.9265\n",
      "Training Epoch: 4 [19200/36450]\tLoss: 522.2755\n",
      "Training Epoch: 4 [19250/36450]\tLoss: 491.8893\n",
      "Training Epoch: 4 [19300/36450]\tLoss: 487.9474\n",
      "Training Epoch: 4 [19350/36450]\tLoss: 499.6435\n",
      "Training Epoch: 4 [19400/36450]\tLoss: 547.6246\n",
      "Training Epoch: 4 [19450/36450]\tLoss: 560.8560\n",
      "Training Epoch: 4 [19500/36450]\tLoss: 498.6847\n",
      "Training Epoch: 4 [19550/36450]\tLoss: 625.3304\n",
      "Training Epoch: 4 [19600/36450]\tLoss: 518.9606\n",
      "Training Epoch: 4 [19650/36450]\tLoss: 524.6196\n",
      "Training Epoch: 4 [19700/36450]\tLoss: 535.6066\n",
      "Training Epoch: 4 [19750/36450]\tLoss: 546.9059\n",
      "Training Epoch: 4 [19800/36450]\tLoss: 534.1125\n",
      "Training Epoch: 4 [19850/36450]\tLoss: 536.2568\n",
      "Training Epoch: 4 [19900/36450]\tLoss: 491.7169\n",
      "Training Epoch: 4 [19950/36450]\tLoss: 555.7419\n",
      "Training Epoch: 4 [20000/36450]\tLoss: 489.2761\n",
      "Training Epoch: 4 [20050/36450]\tLoss: 526.0618\n",
      "Training Epoch: 4 [20100/36450]\tLoss: 505.0174\n",
      "Training Epoch: 4 [20150/36450]\tLoss: 501.4038\n",
      "Training Epoch: 4 [20200/36450]\tLoss: 534.4610\n",
      "Training Epoch: 4 [20250/36450]\tLoss: 524.1683\n",
      "Training Epoch: 4 [20300/36450]\tLoss: 547.1390\n",
      "Training Epoch: 4 [20350/36450]\tLoss: 509.1078\n",
      "Training Epoch: 4 [20400/36450]\tLoss: 510.8931\n",
      "Training Epoch: 4 [20450/36450]\tLoss: 481.8985\n",
      "Training Epoch: 4 [20500/36450]\tLoss: 494.9809\n",
      "Training Epoch: 4 [20550/36450]\tLoss: 515.3032\n",
      "Training Epoch: 4 [20600/36450]\tLoss: 501.6029\n",
      "Training Epoch: 4 [20650/36450]\tLoss: 503.2233\n",
      "Training Epoch: 4 [20700/36450]\tLoss: 582.2192\n",
      "Training Epoch: 4 [20750/36450]\tLoss: 582.5631\n",
      "Training Epoch: 4 [20800/36450]\tLoss: 474.1960\n",
      "Training Epoch: 4 [20850/36450]\tLoss: 566.5703\n",
      "Training Epoch: 4 [20900/36450]\tLoss: 526.2693\n",
      "Training Epoch: 4 [20950/36450]\tLoss: 507.4760\n",
      "Training Epoch: 4 [21000/36450]\tLoss: 547.8510\n",
      "Training Epoch: 4 [21050/36450]\tLoss: 521.0103\n",
      "Training Epoch: 4 [21100/36450]\tLoss: 506.1374\n",
      "Training Epoch: 4 [21150/36450]\tLoss: 512.2634\n",
      "Training Epoch: 4 [21200/36450]\tLoss: 485.3117\n",
      "Training Epoch: 4 [21250/36450]\tLoss: 509.8602\n",
      "Training Epoch: 4 [21300/36450]\tLoss: 524.0062\n",
      "Training Epoch: 4 [21350/36450]\tLoss: 538.6633\n",
      "Training Epoch: 4 [21400/36450]\tLoss: 539.7056\n",
      "Training Epoch: 4 [21450/36450]\tLoss: 526.8284\n",
      "Training Epoch: 4 [21500/36450]\tLoss: 521.1183\n",
      "Training Epoch: 4 [21550/36450]\tLoss: 518.0947\n",
      "Training Epoch: 4 [21600/36450]\tLoss: 560.8719\n",
      "Training Epoch: 4 [21650/36450]\tLoss: 537.3257\n",
      "Training Epoch: 4 [21700/36450]\tLoss: 540.3633\n",
      "Training Epoch: 4 [21750/36450]\tLoss: 557.9531\n",
      "Training Epoch: 4 [21800/36450]\tLoss: 479.9097\n",
      "Training Epoch: 4 [21850/36450]\tLoss: 581.7050\n",
      "Training Epoch: 4 [21900/36450]\tLoss: 519.9932\n",
      "Training Epoch: 4 [21950/36450]\tLoss: 558.2671\n",
      "Training Epoch: 4 [22000/36450]\tLoss: 517.2523\n",
      "Training Epoch: 4 [22050/36450]\tLoss: 515.1017\n",
      "Training Epoch: 4 [22100/36450]\tLoss: 535.3593\n",
      "Training Epoch: 4 [22150/36450]\tLoss: 496.6129\n",
      "Training Epoch: 4 [22200/36450]\tLoss: 555.7704\n",
      "Training Epoch: 4 [22250/36450]\tLoss: 502.9575\n",
      "Training Epoch: 4 [22300/36450]\tLoss: 548.9797\n",
      "Training Epoch: 4 [22350/36450]\tLoss: 495.9589\n",
      "Training Epoch: 4 [22400/36450]\tLoss: 512.7570\n",
      "Training Epoch: 4 [22450/36450]\tLoss: 537.2480\n",
      "Training Epoch: 4 [22500/36450]\tLoss: 513.3509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [22550/36450]\tLoss: 571.6969\n",
      "Training Epoch: 4 [22600/36450]\tLoss: 479.7019\n",
      "Training Epoch: 4 [22650/36450]\tLoss: 578.4493\n",
      "Training Epoch: 4 [22700/36450]\tLoss: 477.2248\n",
      "Training Epoch: 4 [22750/36450]\tLoss: 546.6171\n",
      "Training Epoch: 4 [22800/36450]\tLoss: 509.2281\n",
      "Training Epoch: 4 [22850/36450]\tLoss: 581.0145\n",
      "Training Epoch: 4 [22900/36450]\tLoss: 551.3455\n",
      "Training Epoch: 4 [22950/36450]\tLoss: 525.9250\n",
      "Training Epoch: 4 [23000/36450]\tLoss: 523.0561\n",
      "Training Epoch: 4 [23050/36450]\tLoss: 513.5774\n",
      "Training Epoch: 4 [23100/36450]\tLoss: 529.7548\n",
      "Training Epoch: 4 [23150/36450]\tLoss: 498.2423\n",
      "Training Epoch: 4 [23200/36450]\tLoss: 513.2899\n",
      "Training Epoch: 4 [23250/36450]\tLoss: 579.6160\n",
      "Training Epoch: 4 [23300/36450]\tLoss: 503.4202\n",
      "Training Epoch: 4 [23350/36450]\tLoss: 489.8573\n",
      "Training Epoch: 4 [23400/36450]\tLoss: 498.7819\n",
      "Training Epoch: 4 [23450/36450]\tLoss: 538.9719\n",
      "Training Epoch: 4 [23500/36450]\tLoss: 486.2584\n",
      "Training Epoch: 4 [23550/36450]\tLoss: 515.7981\n",
      "Training Epoch: 4 [23600/36450]\tLoss: 508.7951\n",
      "Training Epoch: 4 [23650/36450]\tLoss: 493.0747\n",
      "Training Epoch: 4 [23700/36450]\tLoss: 515.5556\n",
      "Training Epoch: 4 [23750/36450]\tLoss: 535.7415\n",
      "Training Epoch: 4 [23800/36450]\tLoss: 553.9542\n",
      "Training Epoch: 4 [23850/36450]\tLoss: 515.4221\n",
      "Training Epoch: 4 [23900/36450]\tLoss: 481.4778\n",
      "Training Epoch: 4 [23950/36450]\tLoss: 508.8377\n",
      "Training Epoch: 4 [24000/36450]\tLoss: 531.1368\n",
      "Training Epoch: 4 [24050/36450]\tLoss: 539.1588\n",
      "Training Epoch: 4 [24100/36450]\tLoss: 498.9546\n",
      "Training Epoch: 4 [24150/36450]\tLoss: 535.9454\n",
      "Training Epoch: 4 [24200/36450]\tLoss: 467.9337\n",
      "Training Epoch: 4 [24250/36450]\tLoss: 535.7173\n",
      "Training Epoch: 4 [24300/36450]\tLoss: 528.8153\n",
      "Training Epoch: 4 [24350/36450]\tLoss: 507.4999\n",
      "Training Epoch: 4 [24400/36450]\tLoss: 550.9296\n",
      "Training Epoch: 4 [24450/36450]\tLoss: 507.1441\n",
      "Training Epoch: 4 [24500/36450]\tLoss: 478.7970\n",
      "Training Epoch: 4 [24550/36450]\tLoss: 519.6094\n",
      "Training Epoch: 4 [24600/36450]\tLoss: 538.7122\n",
      "Training Epoch: 4 [24650/36450]\tLoss: 512.3403\n",
      "Training Epoch: 4 [24700/36450]\tLoss: 485.3357\n",
      "Training Epoch: 4 [24750/36450]\tLoss: 521.6714\n",
      "Training Epoch: 4 [24800/36450]\tLoss: 571.3334\n",
      "Training Epoch: 4 [24850/36450]\tLoss: 524.0308\n",
      "Training Epoch: 4 [24900/36450]\tLoss: 530.3361\n",
      "Training Epoch: 4 [24950/36450]\tLoss: 507.3181\n",
      "Training Epoch: 4 [25000/36450]\tLoss: 500.2668\n",
      "Training Epoch: 4 [25050/36450]\tLoss: 583.9164\n",
      "Training Epoch: 4 [25100/36450]\tLoss: 531.0916\n",
      "Training Epoch: 4 [25150/36450]\tLoss: 563.7969\n",
      "Training Epoch: 4 [25200/36450]\tLoss: 496.3015\n",
      "Training Epoch: 4 [25250/36450]\tLoss: 540.2029\n",
      "Training Epoch: 4 [25300/36450]\tLoss: 529.6458\n",
      "Training Epoch: 4 [25350/36450]\tLoss: 506.7428\n",
      "Training Epoch: 4 [25400/36450]\tLoss: 552.6729\n",
      "Training Epoch: 4 [25450/36450]\tLoss: 494.2332\n",
      "Training Epoch: 4 [25500/36450]\tLoss: 519.6359\n",
      "Training Epoch: 4 [25550/36450]\tLoss: 466.4275\n",
      "Training Epoch: 4 [25600/36450]\tLoss: 553.2915\n",
      "Training Epoch: 4 [25650/36450]\tLoss: 517.0522\n",
      "Training Epoch: 4 [25700/36450]\tLoss: 532.2351\n",
      "Training Epoch: 4 [25750/36450]\tLoss: 466.2281\n",
      "Training Epoch: 4 [25800/36450]\tLoss: 540.7684\n",
      "Training Epoch: 4 [25850/36450]\tLoss: 545.0748\n",
      "Training Epoch: 4 [25900/36450]\tLoss: 567.4019\n",
      "Training Epoch: 4 [25950/36450]\tLoss: 532.9573\n",
      "Training Epoch: 4 [26000/36450]\tLoss: 485.9140\n",
      "Training Epoch: 4 [26050/36450]\tLoss: 526.8178\n",
      "Training Epoch: 4 [26100/36450]\tLoss: 516.8914\n",
      "Training Epoch: 4 [26150/36450]\tLoss: 491.6534\n",
      "Training Epoch: 4 [26200/36450]\tLoss: 505.8456\n",
      "Training Epoch: 4 [26250/36450]\tLoss: 513.5596\n",
      "Training Epoch: 4 [26300/36450]\tLoss: 519.9083\n",
      "Training Epoch: 4 [26350/36450]\tLoss: 528.0491\n",
      "Training Epoch: 4 [26400/36450]\tLoss: 492.6533\n",
      "Training Epoch: 4 [26450/36450]\tLoss: 520.8727\n",
      "Training Epoch: 4 [26500/36450]\tLoss: 526.0884\n",
      "Training Epoch: 4 [26550/36450]\tLoss: 513.7139\n",
      "Training Epoch: 4 [26600/36450]\tLoss: 520.6082\n",
      "Training Epoch: 4 [26650/36450]\tLoss: 550.9785\n",
      "Training Epoch: 4 [26700/36450]\tLoss: 503.4878\n",
      "Training Epoch: 4 [26750/36450]\tLoss: 481.7921\n",
      "Training Epoch: 4 [26800/36450]\tLoss: 465.0883\n",
      "Training Epoch: 4 [26850/36450]\tLoss: 515.2529\n",
      "Training Epoch: 4 [26900/36450]\tLoss: 462.3067\n",
      "Training Epoch: 4 [26950/36450]\tLoss: 525.2038\n",
      "Training Epoch: 4 [27000/36450]\tLoss: 510.5194\n",
      "Training Epoch: 4 [27050/36450]\tLoss: 509.6141\n",
      "Training Epoch: 4 [27100/36450]\tLoss: 515.2912\n",
      "Training Epoch: 4 [27150/36450]\tLoss: 525.2368\n",
      "Training Epoch: 4 [27200/36450]\tLoss: 486.1892\n",
      "Training Epoch: 4 [27250/36450]\tLoss: 499.3204\n",
      "Training Epoch: 4 [27300/36450]\tLoss: 494.8896\n",
      "Training Epoch: 4 [27350/36450]\tLoss: 532.8337\n",
      "Training Epoch: 4 [27400/36450]\tLoss: 532.9240\n",
      "Training Epoch: 4 [27450/36450]\tLoss: 525.9748\n",
      "Training Epoch: 4 [27500/36450]\tLoss: 517.4431\n",
      "Training Epoch: 4 [27550/36450]\tLoss: 569.2384\n",
      "Training Epoch: 4 [27600/36450]\tLoss: 475.9528\n",
      "Training Epoch: 4 [27650/36450]\tLoss: 523.6099\n",
      "Training Epoch: 4 [27700/36450]\tLoss: 448.0603\n",
      "Training Epoch: 4 [27750/36450]\tLoss: 559.1622\n",
      "Training Epoch: 4 [27800/36450]\tLoss: 516.0880\n",
      "Training Epoch: 4 [27850/36450]\tLoss: 525.7430\n",
      "Training Epoch: 4 [27900/36450]\tLoss: 530.5281\n",
      "Training Epoch: 4 [27950/36450]\tLoss: 531.4660\n",
      "Training Epoch: 4 [28000/36450]\tLoss: 481.0598\n",
      "Training Epoch: 4 [28050/36450]\tLoss: 498.1206\n",
      "Training Epoch: 4 [28100/36450]\tLoss: 505.8108\n",
      "Training Epoch: 4 [28150/36450]\tLoss: 462.9373\n",
      "Training Epoch: 4 [28200/36450]\tLoss: 522.5759\n",
      "Training Epoch: 4 [28250/36450]\tLoss: 516.8878\n",
      "Training Epoch: 4 [28300/36450]\tLoss: 450.8086\n",
      "Training Epoch: 4 [28350/36450]\tLoss: 499.1754\n",
      "Training Epoch: 4 [28400/36450]\tLoss: 515.3090\n",
      "Training Epoch: 4 [28450/36450]\tLoss: 573.0738\n",
      "Training Epoch: 4 [28500/36450]\tLoss: 511.5028\n",
      "Training Epoch: 4 [28550/36450]\tLoss: 455.6152\n",
      "Training Epoch: 4 [28600/36450]\tLoss: 534.3027\n",
      "Training Epoch: 4 [28650/36450]\tLoss: 515.4496\n",
      "Training Epoch: 4 [28700/36450]\tLoss: 563.1337\n",
      "Training Epoch: 4 [28750/36450]\tLoss: 531.1494\n",
      "Training Epoch: 4 [28800/36450]\tLoss: 520.2397\n",
      "Training Epoch: 4 [28850/36450]\tLoss: 465.5611\n",
      "Training Epoch: 4 [28900/36450]\tLoss: 541.3371\n",
      "Training Epoch: 4 [28950/36450]\tLoss: 531.3336\n",
      "Training Epoch: 4 [29000/36450]\tLoss: 557.5820\n",
      "Training Epoch: 4 [29050/36450]\tLoss: 509.3795\n",
      "Training Epoch: 4 [29100/36450]\tLoss: 490.5002\n",
      "Training Epoch: 4 [29150/36450]\tLoss: 498.8830\n",
      "Training Epoch: 4 [29200/36450]\tLoss: 539.5626\n",
      "Training Epoch: 4 [29250/36450]\tLoss: 514.4657\n",
      "Training Epoch: 4 [29300/36450]\tLoss: 529.8972\n",
      "Training Epoch: 4 [29350/36450]\tLoss: 474.4205\n",
      "Training Epoch: 4 [29400/36450]\tLoss: 491.2809\n",
      "Training Epoch: 4 [29450/36450]\tLoss: 483.9710\n",
      "Training Epoch: 4 [29500/36450]\tLoss: 490.5734\n",
      "Training Epoch: 4 [29550/36450]\tLoss: 497.0662\n",
      "Training Epoch: 4 [29600/36450]\tLoss: 493.8749\n",
      "Training Epoch: 4 [29650/36450]\tLoss: 486.4921\n",
      "Training Epoch: 4 [29700/36450]\tLoss: 538.0126\n",
      "Training Epoch: 4 [29750/36450]\tLoss: 557.7143\n",
      "Training Epoch: 4 [29800/36450]\tLoss: 481.6413\n",
      "Training Epoch: 4 [29850/36450]\tLoss: 523.7004\n",
      "Training Epoch: 4 [29900/36450]\tLoss: 495.0847\n",
      "Training Epoch: 4 [29950/36450]\tLoss: 494.3848\n",
      "Training Epoch: 4 [30000/36450]\tLoss: 525.5061\n",
      "Training Epoch: 4 [30050/36450]\tLoss: 511.4692\n",
      "Training Epoch: 4 [30100/36450]\tLoss: 555.8093\n",
      "Training Epoch: 4 [30150/36450]\tLoss: 490.2312\n",
      "Training Epoch: 4 [30200/36450]\tLoss: 572.7715\n",
      "Training Epoch: 4 [30250/36450]\tLoss: 518.8986\n",
      "Training Epoch: 4 [30300/36450]\tLoss: 475.1384\n",
      "Training Epoch: 4 [30350/36450]\tLoss: 504.1307\n",
      "Training Epoch: 4 [30400/36450]\tLoss: 540.7844\n",
      "Training Epoch: 4 [30450/36450]\tLoss: 489.8671\n",
      "Training Epoch: 4 [30500/36450]\tLoss: 552.7391\n",
      "Training Epoch: 4 [30550/36450]\tLoss: 499.5645\n",
      "Training Epoch: 4 [30600/36450]\tLoss: 489.7235\n",
      "Training Epoch: 4 [30650/36450]\tLoss: 543.4568\n",
      "Training Epoch: 4 [30700/36450]\tLoss: 499.6469\n",
      "Training Epoch: 4 [30750/36450]\tLoss: 512.3004\n",
      "Training Epoch: 4 [30800/36450]\tLoss: 548.3286\n",
      "Training Epoch: 4 [30850/36450]\tLoss: 463.5565\n",
      "Training Epoch: 4 [30900/36450]\tLoss: 525.6752\n",
      "Training Epoch: 4 [30950/36450]\tLoss: 571.8716\n",
      "Training Epoch: 4 [31000/36450]\tLoss: 501.9138\n",
      "Training Epoch: 4 [31050/36450]\tLoss: 530.8668\n",
      "Training Epoch: 4 [31100/36450]\tLoss: 529.1059\n",
      "Training Epoch: 4 [31150/36450]\tLoss: 486.4382\n",
      "Training Epoch: 4 [31200/36450]\tLoss: 516.0339\n",
      "Training Epoch: 4 [31250/36450]\tLoss: 527.2788\n",
      "Training Epoch: 4 [31300/36450]\tLoss: 534.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [31350/36450]\tLoss: 487.7200\n",
      "Training Epoch: 4 [31400/36450]\tLoss: 551.9559\n",
      "Training Epoch: 4 [31450/36450]\tLoss: 548.8257\n",
      "Training Epoch: 4 [31500/36450]\tLoss: 491.1323\n",
      "Training Epoch: 4 [31550/36450]\tLoss: 525.6488\n",
      "Training Epoch: 4 [31600/36450]\tLoss: 542.0432\n",
      "Training Epoch: 4 [31650/36450]\tLoss: 485.1588\n",
      "Training Epoch: 4 [31700/36450]\tLoss: 470.6364\n",
      "Training Epoch: 4 [31750/36450]\tLoss: 490.9018\n",
      "Training Epoch: 4 [31800/36450]\tLoss: 497.4287\n",
      "Training Epoch: 4 [31850/36450]\tLoss: 460.9019\n",
      "Training Epoch: 4 [31900/36450]\tLoss: 527.2283\n",
      "Training Epoch: 4 [31950/36450]\tLoss: 511.5346\n",
      "Training Epoch: 4 [32000/36450]\tLoss: 505.1894\n",
      "Training Epoch: 4 [32050/36450]\tLoss: 468.0522\n",
      "Training Epoch: 4 [32100/36450]\tLoss: 519.0936\n",
      "Training Epoch: 4 [32150/36450]\tLoss: 521.1335\n",
      "Training Epoch: 4 [32200/36450]\tLoss: 478.1699\n",
      "Training Epoch: 4 [32250/36450]\tLoss: 455.8342\n",
      "Training Epoch: 4 [32300/36450]\tLoss: 498.5928\n",
      "Training Epoch: 4 [32350/36450]\tLoss: 491.7511\n",
      "Training Epoch: 4 [32400/36450]\tLoss: 498.0021\n",
      "Training Epoch: 4 [32450/36450]\tLoss: 486.5937\n",
      "Training Epoch: 4 [32500/36450]\tLoss: 518.7044\n",
      "Training Epoch: 4 [32550/36450]\tLoss: 499.9382\n",
      "Training Epoch: 4 [32600/36450]\tLoss: 501.6588\n",
      "Training Epoch: 4 [32650/36450]\tLoss: 472.8825\n",
      "Training Epoch: 4 [32700/36450]\tLoss: 543.7756\n",
      "Training Epoch: 4 [32750/36450]\tLoss: 480.5279\n",
      "Training Epoch: 4 [32800/36450]\tLoss: 503.8004\n",
      "Training Epoch: 4 [32850/36450]\tLoss: 498.5153\n",
      "Training Epoch: 4 [32900/36450]\tLoss: 464.5542\n",
      "Training Epoch: 4 [32950/36450]\tLoss: 507.1818\n",
      "Training Epoch: 4 [33000/36450]\tLoss: 497.5955\n",
      "Training Epoch: 4 [33050/36450]\tLoss: 536.0748\n",
      "Training Epoch: 4 [33100/36450]\tLoss: 531.2699\n",
      "Training Epoch: 4 [33150/36450]\tLoss: 498.0411\n",
      "Training Epoch: 4 [33200/36450]\tLoss: 508.0864\n",
      "Training Epoch: 4 [33250/36450]\tLoss: 508.5961\n",
      "Training Epoch: 4 [33300/36450]\tLoss: 502.5311\n",
      "Training Epoch: 4 [33350/36450]\tLoss: 488.4686\n",
      "Training Epoch: 4 [33400/36450]\tLoss: 536.1033\n",
      "Training Epoch: 4 [33450/36450]\tLoss: 501.5782\n",
      "Training Epoch: 4 [33500/36450]\tLoss: 510.1064\n",
      "Training Epoch: 4 [33550/36450]\tLoss: 512.8705\n",
      "Training Epoch: 4 [33600/36450]\tLoss: 517.2971\n",
      "Training Epoch: 4 [33650/36450]\tLoss: 499.5416\n",
      "Training Epoch: 4 [33700/36450]\tLoss: 495.7369\n",
      "Training Epoch: 4 [33750/36450]\tLoss: 515.7674\n",
      "Training Epoch: 4 [33800/36450]\tLoss: 522.2605\n",
      "Training Epoch: 4 [33850/36450]\tLoss: 488.4504\n",
      "Training Epoch: 4 [33900/36450]\tLoss: 508.9089\n",
      "Training Epoch: 4 [33950/36450]\tLoss: 491.4637\n",
      "Training Epoch: 4 [34000/36450]\tLoss: 494.0306\n",
      "Training Epoch: 4 [34050/36450]\tLoss: 498.3344\n",
      "Training Epoch: 4 [34100/36450]\tLoss: 539.7836\n",
      "Training Epoch: 4 [34150/36450]\tLoss: 476.8234\n",
      "Training Epoch: 4 [34200/36450]\tLoss: 486.9327\n",
      "Training Epoch: 4 [34250/36450]\tLoss: 522.9808\n",
      "Training Epoch: 4 [34300/36450]\tLoss: 492.0123\n",
      "Training Epoch: 4 [34350/36450]\tLoss: 501.9705\n",
      "Training Epoch: 4 [34400/36450]\tLoss: 481.9702\n",
      "Training Epoch: 4 [34450/36450]\tLoss: 518.6808\n",
      "Training Epoch: 4 [34500/36450]\tLoss: 488.9138\n",
      "Training Epoch: 4 [34550/36450]\tLoss: 492.9140\n",
      "Training Epoch: 4 [34600/36450]\tLoss: 501.1863\n",
      "Training Epoch: 4 [34650/36450]\tLoss: 487.9131\n",
      "Training Epoch: 4 [34700/36450]\tLoss: 515.3978\n",
      "Training Epoch: 4 [34750/36450]\tLoss: 465.8893\n",
      "Training Epoch: 4 [34800/36450]\tLoss: 502.6992\n",
      "Training Epoch: 4 [34850/36450]\tLoss: 554.1212\n",
      "Training Epoch: 4 [34900/36450]\tLoss: 497.8055\n",
      "Training Epoch: 4 [34950/36450]\tLoss: 462.6424\n",
      "Training Epoch: 4 [35000/36450]\tLoss: 527.2521\n",
      "Training Epoch: 4 [35050/36450]\tLoss: 524.0627\n",
      "Training Epoch: 4 [35100/36450]\tLoss: 510.2392\n",
      "Training Epoch: 4 [35150/36450]\tLoss: 488.1262\n",
      "Training Epoch: 4 [35200/36450]\tLoss: 517.6436\n",
      "Training Epoch: 4 [35250/36450]\tLoss: 565.7202\n",
      "Training Epoch: 4 [35300/36450]\tLoss: 528.8562\n",
      "Training Epoch: 4 [35350/36450]\tLoss: 552.0435\n",
      "Training Epoch: 4 [35400/36450]\tLoss: 486.6883\n",
      "Training Epoch: 4 [35450/36450]\tLoss: 521.1702\n",
      "Training Epoch: 4 [35500/36450]\tLoss: 544.6249\n",
      "Training Epoch: 4 [35550/36450]\tLoss: 515.9030\n",
      "Training Epoch: 4 [35600/36450]\tLoss: 534.5988\n",
      "Training Epoch: 4 [35650/36450]\tLoss: 531.0842\n",
      "Training Epoch: 4 [35700/36450]\tLoss: 476.9538\n",
      "Training Epoch: 4 [35750/36450]\tLoss: 504.6895\n",
      "Training Epoch: 4 [35800/36450]\tLoss: 500.3108\n",
      "Training Epoch: 4 [35850/36450]\tLoss: 528.2870\n",
      "Training Epoch: 4 [35900/36450]\tLoss: 515.9955\n",
      "Training Epoch: 4 [35950/36450]\tLoss: 544.2457\n",
      "Training Epoch: 4 [36000/36450]\tLoss: 524.7001\n",
      "Training Epoch: 4 [36050/36450]\tLoss: 532.2264\n",
      "Training Epoch: 4 [36100/36450]\tLoss: 527.7598\n",
      "Training Epoch: 4 [36150/36450]\tLoss: 484.7442\n",
      "Training Epoch: 4 [36200/36450]\tLoss: 439.4065\n",
      "Training Epoch: 4 [36250/36450]\tLoss: 478.5347\n",
      "Training Epoch: 4 [36300/36450]\tLoss: 537.9201\n",
      "Training Epoch: 4 [36350/36450]\tLoss: 544.6180\n",
      "Training Epoch: 4 [36400/36450]\tLoss: 488.2237\n",
      "Training Epoch: 4 [36450/36450]\tLoss: 474.6285\n",
      "Training Epoch: 4 [4050/4050]\tLoss: 234.0528\n",
      "Training Epoch: 5 [50/36450]\tLoss: 536.5713\n",
      "Training Epoch: 5 [100/36450]\tLoss: 540.4686\n",
      "Training Epoch: 5 [150/36450]\tLoss: 575.6431\n",
      "Training Epoch: 5 [200/36450]\tLoss: 518.5441\n",
      "Training Epoch: 5 [250/36450]\tLoss: 476.0322\n",
      "Training Epoch: 5 [300/36450]\tLoss: 508.4062\n",
      "Training Epoch: 5 [350/36450]\tLoss: 507.6615\n",
      "Training Epoch: 5 [400/36450]\tLoss: 503.7084\n",
      "Training Epoch: 5 [450/36450]\tLoss: 489.8687\n",
      "Training Epoch: 5 [500/36450]\tLoss: 534.4658\n",
      "Training Epoch: 5 [550/36450]\tLoss: 526.7616\n",
      "Training Epoch: 5 [600/36450]\tLoss: 503.2404\n",
      "Training Epoch: 5 [650/36450]\tLoss: 524.6740\n",
      "Training Epoch: 5 [700/36450]\tLoss: 511.0057\n",
      "Training Epoch: 5 [750/36450]\tLoss: 483.2469\n",
      "Training Epoch: 5 [800/36450]\tLoss: 516.4719\n",
      "Training Epoch: 5 [850/36450]\tLoss: 480.5775\n",
      "Training Epoch: 5 [900/36450]\tLoss: 465.2863\n",
      "Training Epoch: 5 [950/36450]\tLoss: 525.8134\n",
      "Training Epoch: 5 [1000/36450]\tLoss: 471.3372\n",
      "Training Epoch: 5 [1050/36450]\tLoss: 508.9324\n",
      "Training Epoch: 5 [1100/36450]\tLoss: 496.2920\n",
      "Training Epoch: 5 [1150/36450]\tLoss: 478.9811\n",
      "Training Epoch: 5 [1200/36450]\tLoss: 454.5281\n",
      "Training Epoch: 5 [1250/36450]\tLoss: 533.9992\n",
      "Training Epoch: 5 [1300/36450]\tLoss: 493.6781\n",
      "Training Epoch: 5 [1350/36450]\tLoss: 516.2337\n",
      "Training Epoch: 5 [1400/36450]\tLoss: 468.4376\n",
      "Training Epoch: 5 [1450/36450]\tLoss: 461.7566\n",
      "Training Epoch: 5 [1500/36450]\tLoss: 471.4398\n",
      "Training Epoch: 5 [1550/36450]\tLoss: 492.7112\n",
      "Training Epoch: 5 [1600/36450]\tLoss: 549.9272\n",
      "Training Epoch: 5 [1650/36450]\tLoss: 498.8871\n",
      "Training Epoch: 5 [1700/36450]\tLoss: 516.1212\n",
      "Training Epoch: 5 [1750/36450]\tLoss: 501.5638\n",
      "Training Epoch: 5 [1800/36450]\tLoss: 462.2437\n",
      "Training Epoch: 5 [1850/36450]\tLoss: 441.8915\n",
      "Training Epoch: 5 [1900/36450]\tLoss: 492.1169\n",
      "Training Epoch: 5 [1950/36450]\tLoss: 489.4382\n",
      "Training Epoch: 5 [2000/36450]\tLoss: 533.1208\n",
      "Training Epoch: 5 [2050/36450]\tLoss: 465.0314\n",
      "Training Epoch: 5 [2100/36450]\tLoss: 497.6943\n",
      "Training Epoch: 5 [2150/36450]\tLoss: 526.3233\n",
      "Training Epoch: 5 [2200/36450]\tLoss: 523.3264\n",
      "Training Epoch: 5 [2250/36450]\tLoss: 534.7195\n",
      "Training Epoch: 5 [2300/36450]\tLoss: 465.0981\n",
      "Training Epoch: 5 [2350/36450]\tLoss: 491.3879\n",
      "Training Epoch: 5 [2400/36450]\tLoss: 501.8224\n",
      "Training Epoch: 5 [2450/36450]\tLoss: 513.5455\n",
      "Training Epoch: 5 [2500/36450]\tLoss: 503.6954\n",
      "Training Epoch: 5 [2550/36450]\tLoss: 496.1048\n",
      "Training Epoch: 5 [2600/36450]\tLoss: 546.3427\n",
      "Training Epoch: 5 [2650/36450]\tLoss: 477.6222\n",
      "Training Epoch: 5 [2700/36450]\tLoss: 511.3938\n",
      "Training Epoch: 5 [2750/36450]\tLoss: 520.5141\n",
      "Training Epoch: 5 [2800/36450]\tLoss: 540.5339\n",
      "Training Epoch: 5 [2850/36450]\tLoss: 548.8351\n",
      "Training Epoch: 5 [2900/36450]\tLoss: 499.9379\n",
      "Training Epoch: 5 [2950/36450]\tLoss: 495.6400\n",
      "Training Epoch: 5 [3000/36450]\tLoss: 497.5161\n",
      "Training Epoch: 5 [3050/36450]\tLoss: 494.5610\n",
      "Training Epoch: 5 [3100/36450]\tLoss: 558.4069\n",
      "Training Epoch: 5 [3150/36450]\tLoss: 510.3817\n",
      "Training Epoch: 5 [3200/36450]\tLoss: 460.9624\n",
      "Training Epoch: 5 [3250/36450]\tLoss: 481.4790\n",
      "Training Epoch: 5 [3300/36450]\tLoss: 497.9668\n",
      "Training Epoch: 5 [3350/36450]\tLoss: 460.4237\n",
      "Training Epoch: 5 [3400/36450]\tLoss: 514.9099\n",
      "Training Epoch: 5 [3450/36450]\tLoss: 523.3130\n",
      "Training Epoch: 5 [3500/36450]\tLoss: 482.4262\n",
      "Training Epoch: 5 [3550/36450]\tLoss: 538.6708\n",
      "Training Epoch: 5 [3600/36450]\tLoss: 526.5190\n",
      "Training Epoch: 5 [3650/36450]\tLoss: 480.6310\n",
      "Training Epoch: 5 [3700/36450]\tLoss: 465.5246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [3750/36450]\tLoss: 503.9728\n",
      "Training Epoch: 5 [3800/36450]\tLoss: 462.1365\n",
      "Training Epoch: 5 [3850/36450]\tLoss: 510.2353\n",
      "Training Epoch: 5 [3900/36450]\tLoss: 492.9556\n",
      "Training Epoch: 5 [3950/36450]\tLoss: 494.4430\n",
      "Training Epoch: 5 [4000/36450]\tLoss: 561.4278\n",
      "Training Epoch: 5 [4050/36450]\tLoss: 460.7468\n",
      "Training Epoch: 5 [4100/36450]\tLoss: 459.0262\n",
      "Training Epoch: 5 [4150/36450]\tLoss: 548.5425\n",
      "Training Epoch: 5 [4200/36450]\tLoss: 478.6537\n",
      "Training Epoch: 5 [4250/36450]\tLoss: 454.2436\n",
      "Training Epoch: 5 [4300/36450]\tLoss: 506.6102\n",
      "Training Epoch: 5 [4350/36450]\tLoss: 478.8941\n",
      "Training Epoch: 5 [4400/36450]\tLoss: 542.2648\n",
      "Training Epoch: 5 [4450/36450]\tLoss: 487.9401\n",
      "Training Epoch: 5 [4500/36450]\tLoss: 469.4884\n",
      "Training Epoch: 5 [4550/36450]\tLoss: 483.9878\n",
      "Training Epoch: 5 [4600/36450]\tLoss: 489.4952\n",
      "Training Epoch: 5 [4650/36450]\tLoss: 501.7477\n",
      "Training Epoch: 5 [4700/36450]\tLoss: 466.8473\n",
      "Training Epoch: 5 [4750/36450]\tLoss: 485.7968\n",
      "Training Epoch: 5 [4800/36450]\tLoss: 561.5623\n",
      "Training Epoch: 5 [4850/36450]\tLoss: 491.1461\n",
      "Training Epoch: 5 [4900/36450]\tLoss: 483.2427\n",
      "Training Epoch: 5 [4950/36450]\tLoss: 531.3865\n",
      "Training Epoch: 5 [5000/36450]\tLoss: 474.9157\n",
      "Training Epoch: 5 [5050/36450]\tLoss: 512.2346\n",
      "Training Epoch: 5 [5100/36450]\tLoss: 495.1391\n",
      "Training Epoch: 5 [5150/36450]\tLoss: 500.9224\n",
      "Training Epoch: 5 [5200/36450]\tLoss: 553.8317\n",
      "Training Epoch: 5 [5250/36450]\tLoss: 473.1750\n",
      "Training Epoch: 5 [5300/36450]\tLoss: 481.9345\n",
      "Training Epoch: 5 [5350/36450]\tLoss: 469.8193\n",
      "Training Epoch: 5 [5400/36450]\tLoss: 553.0363\n",
      "Training Epoch: 5 [5450/36450]\tLoss: 447.5703\n",
      "Training Epoch: 5 [5500/36450]\tLoss: 461.6914\n",
      "Training Epoch: 5 [5550/36450]\tLoss: 471.2581\n",
      "Training Epoch: 5 [5600/36450]\tLoss: 528.5739\n",
      "Training Epoch: 5 [5650/36450]\tLoss: 537.3456\n",
      "Training Epoch: 5 [5700/36450]\tLoss: 468.0813\n",
      "Training Epoch: 5 [5750/36450]\tLoss: 433.0873\n",
      "Training Epoch: 5 [5800/36450]\tLoss: 484.5177\n",
      "Training Epoch: 5 [5850/36450]\tLoss: 497.4602\n",
      "Training Epoch: 5 [5900/36450]\tLoss: 503.5332\n",
      "Training Epoch: 5 [5950/36450]\tLoss: 494.3492\n",
      "Training Epoch: 5 [6000/36450]\tLoss: 519.5975\n",
      "Training Epoch: 5 [6050/36450]\tLoss: 481.6438\n",
      "Training Epoch: 5 [6100/36450]\tLoss: 513.5046\n",
      "Training Epoch: 5 [6150/36450]\tLoss: 447.4906\n",
      "Training Epoch: 5 [6200/36450]\tLoss: 468.7595\n",
      "Training Epoch: 5 [6250/36450]\tLoss: 487.0380\n",
      "Training Epoch: 5 [6300/36450]\tLoss: 434.4123\n",
      "Training Epoch: 5 [6350/36450]\tLoss: 538.9640\n",
      "Training Epoch: 5 [6400/36450]\tLoss: 472.8405\n",
      "Training Epoch: 5 [6450/36450]\tLoss: 481.6712\n",
      "Training Epoch: 5 [6500/36450]\tLoss: 459.2814\n",
      "Training Epoch: 5 [6550/36450]\tLoss: 507.8783\n",
      "Training Epoch: 5 [6600/36450]\tLoss: 522.1352\n",
      "Training Epoch: 5 [6650/36450]\tLoss: 530.9343\n",
      "Training Epoch: 5 [6700/36450]\tLoss: 496.8844\n",
      "Training Epoch: 5 [6750/36450]\tLoss: 494.2202\n",
      "Training Epoch: 5 [6800/36450]\tLoss: 519.5435\n",
      "Training Epoch: 5 [6850/36450]\tLoss: 492.3424\n",
      "Training Epoch: 5 [6900/36450]\tLoss: 511.5402\n",
      "Training Epoch: 5 [6950/36450]\tLoss: 454.2163\n",
      "Training Epoch: 5 [7000/36450]\tLoss: 516.0702\n",
      "Training Epoch: 5 [7050/36450]\tLoss: 468.6071\n",
      "Training Epoch: 5 [7100/36450]\tLoss: 433.8850\n",
      "Training Epoch: 5 [7150/36450]\tLoss: 505.3143\n",
      "Training Epoch: 5 [7200/36450]\tLoss: 537.8511\n",
      "Training Epoch: 5 [7250/36450]\tLoss: 526.5540\n",
      "Training Epoch: 5 [7300/36450]\tLoss: 483.8590\n",
      "Training Epoch: 5 [7350/36450]\tLoss: 509.7464\n",
      "Training Epoch: 5 [7400/36450]\tLoss: 519.3140\n",
      "Training Epoch: 5 [7450/36450]\tLoss: 466.2851\n",
      "Training Epoch: 5 [7500/36450]\tLoss: 604.1369\n",
      "Training Epoch: 5 [7550/36450]\tLoss: 477.2823\n",
      "Training Epoch: 5 [7600/36450]\tLoss: 532.7002\n",
      "Training Epoch: 5 [7650/36450]\tLoss: 488.9380\n",
      "Training Epoch: 5 [7700/36450]\tLoss: 470.4954\n",
      "Training Epoch: 5 [7750/36450]\tLoss: 482.5136\n",
      "Training Epoch: 5 [7800/36450]\tLoss: 503.5600\n",
      "Training Epoch: 5 [7850/36450]\tLoss: 478.6239\n",
      "Training Epoch: 5 [7900/36450]\tLoss: 490.6770\n",
      "Training Epoch: 5 [7950/36450]\tLoss: 496.1897\n",
      "Training Epoch: 5 [8000/36450]\tLoss: 471.6339\n",
      "Training Epoch: 5 [8050/36450]\tLoss: 442.3728\n",
      "Training Epoch: 5 [8100/36450]\tLoss: 478.4051\n",
      "Training Epoch: 5 [8150/36450]\tLoss: 474.3569\n",
      "Training Epoch: 5 [8200/36450]\tLoss: 546.6714\n",
      "Training Epoch: 5 [8250/36450]\tLoss: 516.8434\n",
      "Training Epoch: 5 [8300/36450]\tLoss: 484.0560\n",
      "Training Epoch: 5 [8350/36450]\tLoss: 450.0283\n",
      "Training Epoch: 5 [8400/36450]\tLoss: 444.8632\n",
      "Training Epoch: 5 [8450/36450]\tLoss: 492.0733\n",
      "Training Epoch: 5 [8500/36450]\tLoss: 474.7962\n",
      "Training Epoch: 5 [8550/36450]\tLoss: 476.4781\n",
      "Training Epoch: 5 [8600/36450]\tLoss: 475.8055\n",
      "Training Epoch: 5 [8650/36450]\tLoss: 480.8289\n",
      "Training Epoch: 5 [8700/36450]\tLoss: 504.4898\n",
      "Training Epoch: 5 [8750/36450]\tLoss: 497.2736\n",
      "Training Epoch: 5 [8800/36450]\tLoss: 463.4885\n",
      "Training Epoch: 5 [8850/36450]\tLoss: 479.2894\n",
      "Training Epoch: 5 [8900/36450]\tLoss: 469.9384\n",
      "Training Epoch: 5 [8950/36450]\tLoss: 484.8628\n",
      "Training Epoch: 5 [9000/36450]\tLoss: 516.7753\n",
      "Training Epoch: 5 [9050/36450]\tLoss: 458.8093\n",
      "Training Epoch: 5 [9100/36450]\tLoss: 475.4527\n",
      "Training Epoch: 5 [9150/36450]\tLoss: 448.9867\n",
      "Training Epoch: 5 [9200/36450]\tLoss: 461.5334\n",
      "Training Epoch: 5 [9250/36450]\tLoss: 475.2362\n",
      "Training Epoch: 5 [9300/36450]\tLoss: 511.9317\n",
      "Training Epoch: 5 [9350/36450]\tLoss: 441.3954\n",
      "Training Epoch: 5 [9400/36450]\tLoss: 496.2247\n",
      "Training Epoch: 5 [9450/36450]\tLoss: 442.6110\n",
      "Training Epoch: 5 [9500/36450]\tLoss: 478.4582\n",
      "Training Epoch: 5 [9550/36450]\tLoss: 494.6898\n",
      "Training Epoch: 5 [9600/36450]\tLoss: 493.9389\n",
      "Training Epoch: 5 [9650/36450]\tLoss: 472.8272\n",
      "Training Epoch: 5 [9700/36450]\tLoss: 509.7949\n",
      "Training Epoch: 5 [9750/36450]\tLoss: 479.5568\n",
      "Training Epoch: 5 [9800/36450]\tLoss: 469.2119\n",
      "Training Epoch: 5 [9850/36450]\tLoss: 487.8845\n",
      "Training Epoch: 5 [9900/36450]\tLoss: 508.3858\n",
      "Training Epoch: 5 [9950/36450]\tLoss: 450.3381\n",
      "Training Epoch: 5 [10000/36450]\tLoss: 482.8526\n",
      "Training Epoch: 5 [10050/36450]\tLoss: 511.5341\n",
      "Training Epoch: 5 [10100/36450]\tLoss: 506.4509\n",
      "Training Epoch: 5 [10150/36450]\tLoss: 496.2284\n",
      "Training Epoch: 5 [10200/36450]\tLoss: 474.2973\n",
      "Training Epoch: 5 [10250/36450]\tLoss: 468.9844\n",
      "Training Epoch: 5 [10300/36450]\tLoss: 447.9698\n",
      "Training Epoch: 5 [10350/36450]\tLoss: 492.8761\n",
      "Training Epoch: 5 [10400/36450]\tLoss: 508.4536\n",
      "Training Epoch: 5 [10450/36450]\tLoss: 508.9167\n",
      "Training Epoch: 5 [10500/36450]\tLoss: 530.4229\n",
      "Training Epoch: 5 [10550/36450]\tLoss: 468.3967\n",
      "Training Epoch: 5 [10600/36450]\tLoss: 458.2908\n",
      "Training Epoch: 5 [10650/36450]\tLoss: 498.5871\n",
      "Training Epoch: 5 [10700/36450]\tLoss: 494.0491\n",
      "Training Epoch: 5 [10750/36450]\tLoss: 497.3818\n",
      "Training Epoch: 5 [10800/36450]\tLoss: 451.6578\n",
      "Training Epoch: 5 [10850/36450]\tLoss: 487.6961\n",
      "Training Epoch: 5 [10900/36450]\tLoss: 488.7642\n",
      "Training Epoch: 5 [10950/36450]\tLoss: 481.4240\n",
      "Training Epoch: 5 [11000/36450]\tLoss: 492.0404\n",
      "Training Epoch: 5 [11050/36450]\tLoss: 504.4460\n",
      "Training Epoch: 5 [11100/36450]\tLoss: 472.3896\n",
      "Training Epoch: 5 [11150/36450]\tLoss: 466.4537\n",
      "Training Epoch: 5 [11200/36450]\tLoss: 445.5719\n",
      "Training Epoch: 5 [11250/36450]\tLoss: 528.4057\n",
      "Training Epoch: 5 [11300/36450]\tLoss: 461.9258\n",
      "Training Epoch: 5 [11350/36450]\tLoss: 498.9974\n",
      "Training Epoch: 5 [11400/36450]\tLoss: 468.4924\n",
      "Training Epoch: 5 [11450/36450]\tLoss: 478.7160\n",
      "Training Epoch: 5 [11500/36450]\tLoss: 509.2854\n",
      "Training Epoch: 5 [11550/36450]\tLoss: 482.6812\n",
      "Training Epoch: 5 [11600/36450]\tLoss: 442.0656\n",
      "Training Epoch: 5 [11650/36450]\tLoss: 470.5457\n",
      "Training Epoch: 5 [11700/36450]\tLoss: 506.4171\n",
      "Training Epoch: 5 [11750/36450]\tLoss: 515.0646\n",
      "Training Epoch: 5 [11800/36450]\tLoss: 481.7654\n",
      "Training Epoch: 5 [11850/36450]\tLoss: 521.0863\n",
      "Training Epoch: 5 [11900/36450]\tLoss: 488.0306\n",
      "Training Epoch: 5 [11950/36450]\tLoss: 492.5807\n",
      "Training Epoch: 5 [12000/36450]\tLoss: 474.6345\n",
      "Training Epoch: 5 [12050/36450]\tLoss: 491.7209\n",
      "Training Epoch: 5 [12100/36450]\tLoss: 561.4894\n",
      "Training Epoch: 5 [12150/36450]\tLoss: 423.4974\n",
      "Training Epoch: 5 [12200/36450]\tLoss: 463.9921\n",
      "Training Epoch: 5 [12250/36450]\tLoss: 486.6852\n",
      "Training Epoch: 5 [12300/36450]\tLoss: 490.6062\n",
      "Training Epoch: 5 [12350/36450]\tLoss: 501.3097\n",
      "Training Epoch: 5 [12400/36450]\tLoss: 485.8371\n",
      "Training Epoch: 5 [12450/36450]\tLoss: 537.0872\n",
      "Training Epoch: 5 [12500/36450]\tLoss: 540.6722\n",
      "Training Epoch: 5 [12550/36450]\tLoss: 485.4678\n",
      "Training Epoch: 5 [12600/36450]\tLoss: 487.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [12650/36450]\tLoss: 503.0556\n",
      "Training Epoch: 5 [12700/36450]\tLoss: 499.9274\n",
      "Training Epoch: 5 [12750/36450]\tLoss: 447.1376\n",
      "Training Epoch: 5 [12800/36450]\tLoss: 458.1008\n",
      "Training Epoch: 5 [12850/36450]\tLoss: 474.3469\n",
      "Training Epoch: 5 [12900/36450]\tLoss: 469.9955\n",
      "Training Epoch: 5 [12950/36450]\tLoss: 513.8745\n",
      "Training Epoch: 5 [13000/36450]\tLoss: 511.9817\n",
      "Training Epoch: 5 [13050/36450]\tLoss: 469.3125\n",
      "Training Epoch: 5 [13100/36450]\tLoss: 455.7034\n",
      "Training Epoch: 5 [13150/36450]\tLoss: 468.9980\n",
      "Training Epoch: 5 [13200/36450]\tLoss: 442.2413\n",
      "Training Epoch: 5 [13250/36450]\tLoss: 511.8927\n",
      "Training Epoch: 5 [13300/36450]\tLoss: 502.1420\n",
      "Training Epoch: 5 [13350/36450]\tLoss: 459.7285\n",
      "Training Epoch: 5 [13400/36450]\tLoss: 488.4333\n",
      "Training Epoch: 5 [13450/36450]\tLoss: 439.7680\n",
      "Training Epoch: 5 [13500/36450]\tLoss: 503.1779\n",
      "Training Epoch: 5 [13550/36450]\tLoss: 505.5757\n",
      "Training Epoch: 5 [13600/36450]\tLoss: 489.0772\n",
      "Training Epoch: 5 [13650/36450]\tLoss: 494.5739\n",
      "Training Epoch: 5 [13700/36450]\tLoss: 480.1418\n",
      "Training Epoch: 5 [13750/36450]\tLoss: 517.1775\n",
      "Training Epoch: 5 [13800/36450]\tLoss: 539.1049\n",
      "Training Epoch: 5 [13850/36450]\tLoss: 513.5630\n",
      "Training Epoch: 5 [13900/36450]\tLoss: 430.7555\n",
      "Training Epoch: 5 [13950/36450]\tLoss: 472.7635\n",
      "Training Epoch: 5 [14000/36450]\tLoss: 454.6142\n",
      "Training Epoch: 5 [14050/36450]\tLoss: 439.8100\n",
      "Training Epoch: 5 [14100/36450]\tLoss: 482.4331\n",
      "Training Epoch: 5 [14150/36450]\tLoss: 503.3460\n",
      "Training Epoch: 5 [14200/36450]\tLoss: 501.6598\n",
      "Training Epoch: 5 [14250/36450]\tLoss: 479.5262\n",
      "Training Epoch: 5 [14300/36450]\tLoss: 501.6125\n",
      "Training Epoch: 5 [14350/36450]\tLoss: 460.0877\n",
      "Training Epoch: 5 [14400/36450]\tLoss: 490.8405\n",
      "Training Epoch: 5 [14450/36450]\tLoss: 485.3556\n",
      "Training Epoch: 5 [14500/36450]\tLoss: 493.9932\n",
      "Training Epoch: 5 [14550/36450]\tLoss: 442.6449\n",
      "Training Epoch: 5 [14600/36450]\tLoss: 458.8276\n",
      "Training Epoch: 5 [14650/36450]\tLoss: 507.4986\n",
      "Training Epoch: 5 [14700/36450]\tLoss: 472.4205\n",
      "Training Epoch: 5 [14750/36450]\tLoss: 447.9291\n",
      "Training Epoch: 5 [14800/36450]\tLoss: 454.4498\n",
      "Training Epoch: 5 [14850/36450]\tLoss: 509.5854\n",
      "Training Epoch: 5 [14900/36450]\tLoss: 482.5085\n",
      "Training Epoch: 5 [14950/36450]\tLoss: 461.8881\n",
      "Training Epoch: 5 [15000/36450]\tLoss: 489.7335\n",
      "Training Epoch: 5 [15050/36450]\tLoss: 428.7883\n",
      "Training Epoch: 5 [15100/36450]\tLoss: 485.0488\n",
      "Training Epoch: 5 [15150/36450]\tLoss: 488.3011\n",
      "Training Epoch: 5 [15200/36450]\tLoss: 442.4181\n",
      "Training Epoch: 5 [15250/36450]\tLoss: 478.8941\n",
      "Training Epoch: 5 [15300/36450]\tLoss: 475.7814\n",
      "Training Epoch: 5 [15350/36450]\tLoss: 489.2032\n",
      "Training Epoch: 5 [15400/36450]\tLoss: 485.2566\n",
      "Training Epoch: 5 [15450/36450]\tLoss: 455.8986\n",
      "Training Epoch: 5 [15500/36450]\tLoss: 483.4222\n",
      "Training Epoch: 5 [15550/36450]\tLoss: 438.9616\n",
      "Training Epoch: 5 [15600/36450]\tLoss: 445.2350\n",
      "Training Epoch: 5 [15650/36450]\tLoss: 432.9084\n",
      "Training Epoch: 5 [15700/36450]\tLoss: 462.8287\n",
      "Training Epoch: 5 [15750/36450]\tLoss: 444.5353\n",
      "Training Epoch: 5 [15800/36450]\tLoss: 452.1881\n",
      "Training Epoch: 5 [15850/36450]\tLoss: 472.9505\n",
      "Training Epoch: 5 [15900/36450]\tLoss: 479.1597\n",
      "Training Epoch: 5 [15950/36450]\tLoss: 456.9381\n",
      "Training Epoch: 5 [16000/36450]\tLoss: 488.4326\n",
      "Training Epoch: 5 [16050/36450]\tLoss: 456.2002\n",
      "Training Epoch: 5 [16100/36450]\tLoss: 448.0416\n",
      "Training Epoch: 5 [16150/36450]\tLoss: 464.8001\n",
      "Training Epoch: 5 [16200/36450]\tLoss: 450.3681\n",
      "Training Epoch: 5 [16250/36450]\tLoss: 474.9870\n",
      "Training Epoch: 5 [16300/36450]\tLoss: 471.6940\n",
      "Training Epoch: 5 [16350/36450]\tLoss: 497.6124\n",
      "Training Epoch: 5 [16400/36450]\tLoss: 494.8455\n",
      "Training Epoch: 5 [16450/36450]\tLoss: 435.4336\n",
      "Training Epoch: 5 [16500/36450]\tLoss: 499.6618\n",
      "Training Epoch: 5 [16550/36450]\tLoss: 451.3632\n",
      "Training Epoch: 5 [16600/36450]\tLoss: 441.6692\n",
      "Training Epoch: 5 [16650/36450]\tLoss: 484.8360\n",
      "Training Epoch: 5 [16700/36450]\tLoss: 487.8829\n",
      "Training Epoch: 5 [16750/36450]\tLoss: 448.5566\n",
      "Training Epoch: 5 [16800/36450]\tLoss: 470.6888\n",
      "Training Epoch: 5 [16850/36450]\tLoss: 541.7834\n",
      "Training Epoch: 5 [16900/36450]\tLoss: 445.6539\n",
      "Training Epoch: 5 [16950/36450]\tLoss: 469.8945\n",
      "Training Epoch: 5 [17000/36450]\tLoss: 470.9483\n",
      "Training Epoch: 5 [17050/36450]\tLoss: 483.1178\n",
      "Training Epoch: 5 [17100/36450]\tLoss: 442.2110\n",
      "Training Epoch: 5 [17150/36450]\tLoss: 507.0665\n",
      "Training Epoch: 5 [17200/36450]\tLoss: 441.5465\n",
      "Training Epoch: 5 [17250/36450]\tLoss: 482.0458\n",
      "Training Epoch: 5 [17300/36450]\tLoss: 464.5046\n",
      "Training Epoch: 5 [17350/36450]\tLoss: 490.6279\n",
      "Training Epoch: 5 [17400/36450]\tLoss: 498.7795\n",
      "Training Epoch: 5 [17450/36450]\tLoss: 517.9225\n",
      "Training Epoch: 5 [17500/36450]\tLoss: 456.2833\n",
      "Training Epoch: 5 [17550/36450]\tLoss: 408.6277\n",
      "Training Epoch: 5 [17600/36450]\tLoss: 454.6928\n",
      "Training Epoch: 5 [17650/36450]\tLoss: 479.4841\n",
      "Training Epoch: 5 [17700/36450]\tLoss: 455.0849\n",
      "Training Epoch: 5 [17750/36450]\tLoss: 399.4062\n",
      "Training Epoch: 5 [17800/36450]\tLoss: 450.8168\n",
      "Training Epoch: 5 [17850/36450]\tLoss: 504.3539\n",
      "Training Epoch: 5 [17900/36450]\tLoss: 489.7248\n",
      "Training Epoch: 5 [17950/36450]\tLoss: 484.0284\n",
      "Training Epoch: 5 [18000/36450]\tLoss: 466.4019\n",
      "Training Epoch: 5 [18050/36450]\tLoss: 413.6243\n",
      "Training Epoch: 5 [18100/36450]\tLoss: 496.0785\n",
      "Training Epoch: 5 [18150/36450]\tLoss: 467.1825\n",
      "Training Epoch: 5 [18200/36450]\tLoss: 518.9539\n",
      "Training Epoch: 5 [18250/36450]\tLoss: 460.7431\n",
      "Training Epoch: 5 [18300/36450]\tLoss: 496.5781\n",
      "Training Epoch: 5 [18350/36450]\tLoss: 491.8934\n",
      "Training Epoch: 5 [18400/36450]\tLoss: 465.0575\n",
      "Training Epoch: 5 [18450/36450]\tLoss: 483.3256\n",
      "Training Epoch: 5 [18500/36450]\tLoss: 467.6953\n",
      "Training Epoch: 5 [18550/36450]\tLoss: 475.8929\n",
      "Training Epoch: 5 [18600/36450]\tLoss: 464.4319\n",
      "Training Epoch: 5 [18650/36450]\tLoss: 523.1474\n",
      "Training Epoch: 5 [18700/36450]\tLoss: 475.4297\n",
      "Training Epoch: 5 [18750/36450]\tLoss: 480.2632\n",
      "Training Epoch: 5 [18800/36450]\tLoss: 454.3199\n",
      "Training Epoch: 5 [18850/36450]\tLoss: 483.6105\n",
      "Training Epoch: 5 [18900/36450]\tLoss: 467.2490\n",
      "Training Epoch: 5 [18950/36450]\tLoss: 468.2609\n",
      "Training Epoch: 5 [19000/36450]\tLoss: 517.7754\n",
      "Training Epoch: 5 [19050/36450]\tLoss: 472.9465\n",
      "Training Epoch: 5 [19100/36450]\tLoss: 488.1276\n",
      "Training Epoch: 5 [19150/36450]\tLoss: 470.9181\n",
      "Training Epoch: 5 [19200/36450]\tLoss: 458.4910\n",
      "Training Epoch: 5 [19250/36450]\tLoss: 503.5584\n",
      "Training Epoch: 5 [19300/36450]\tLoss: 441.9311\n",
      "Training Epoch: 5 [19350/36450]\tLoss: 444.4269\n",
      "Training Epoch: 5 [19400/36450]\tLoss: 471.1168\n",
      "Training Epoch: 5 [19450/36450]\tLoss: 447.1996\n",
      "Training Epoch: 5 [19500/36450]\tLoss: 446.7998\n",
      "Training Epoch: 5 [19550/36450]\tLoss: 465.9842\n",
      "Training Epoch: 5 [19600/36450]\tLoss: 490.5160\n",
      "Training Epoch: 5 [19650/36450]\tLoss: 442.2647\n",
      "Training Epoch: 5 [19700/36450]\tLoss: 429.6595\n",
      "Training Epoch: 5 [19750/36450]\tLoss: 430.6941\n",
      "Training Epoch: 5 [19800/36450]\tLoss: 497.9861\n",
      "Training Epoch: 5 [19850/36450]\tLoss: 444.2939\n",
      "Training Epoch: 5 [19900/36450]\tLoss: 493.3823\n",
      "Training Epoch: 5 [19950/36450]\tLoss: 473.0452\n",
      "Training Epoch: 5 [20000/36450]\tLoss: 490.2438\n",
      "Training Epoch: 5 [20050/36450]\tLoss: 428.4319\n",
      "Training Epoch: 5 [20100/36450]\tLoss: 508.1530\n",
      "Training Epoch: 5 [20150/36450]\tLoss: 451.4317\n",
      "Training Epoch: 5 [20200/36450]\tLoss: 468.1167\n",
      "Training Epoch: 5 [20250/36450]\tLoss: 458.5176\n",
      "Training Epoch: 5 [20300/36450]\tLoss: 473.1701\n",
      "Training Epoch: 5 [20350/36450]\tLoss: 457.3745\n",
      "Training Epoch: 5 [20400/36450]\tLoss: 455.6434\n",
      "Training Epoch: 5 [20450/36450]\tLoss: 498.4472\n",
      "Training Epoch: 5 [20500/36450]\tLoss: 443.1643\n",
      "Training Epoch: 5 [20550/36450]\tLoss: 478.4565\n",
      "Training Epoch: 5 [20600/36450]\tLoss: 466.8861\n",
      "Training Epoch: 5 [20650/36450]\tLoss: 438.8360\n",
      "Training Epoch: 5 [20700/36450]\tLoss: 467.1862\n",
      "Training Epoch: 5 [20750/36450]\tLoss: 424.9702\n",
      "Training Epoch: 5 [20800/36450]\tLoss: 473.6491\n",
      "Training Epoch: 5 [20850/36450]\tLoss: 456.3291\n",
      "Training Epoch: 5 [20900/36450]\tLoss: 458.7054\n",
      "Training Epoch: 5 [20950/36450]\tLoss: 476.1509\n",
      "Training Epoch: 5 [21000/36450]\tLoss: 456.0635\n",
      "Training Epoch: 5 [21050/36450]\tLoss: 459.8202\n",
      "Training Epoch: 5 [21100/36450]\tLoss: 444.6985\n",
      "Training Epoch: 5 [21150/36450]\tLoss: 497.3235\n",
      "Training Epoch: 5 [21200/36450]\tLoss: 467.0826\n",
      "Training Epoch: 5 [21250/36450]\tLoss: 462.4660\n",
      "Training Epoch: 5 [21300/36450]\tLoss: 422.5521\n",
      "Training Epoch: 5 [21350/36450]\tLoss: 423.2964\n",
      "Training Epoch: 5 [21400/36450]\tLoss: 483.4583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [21450/36450]\tLoss: 461.1729\n",
      "Training Epoch: 5 [21500/36450]\tLoss: 453.2932\n",
      "Training Epoch: 5 [21550/36450]\tLoss: 460.5428\n",
      "Training Epoch: 5 [21600/36450]\tLoss: 427.5555\n",
      "Training Epoch: 5 [21650/36450]\tLoss: 430.9623\n",
      "Training Epoch: 5 [21700/36450]\tLoss: 462.9799\n",
      "Training Epoch: 5 [21750/36450]\tLoss: 474.8344\n",
      "Training Epoch: 5 [21800/36450]\tLoss: 445.1374\n",
      "Training Epoch: 5 [21850/36450]\tLoss: 469.2586\n",
      "Training Epoch: 5 [21900/36450]\tLoss: 421.0395\n",
      "Training Epoch: 5 [21950/36450]\tLoss: 461.4684\n",
      "Training Epoch: 5 [22000/36450]\tLoss: 441.8665\n",
      "Training Epoch: 5 [22050/36450]\tLoss: 459.3702\n",
      "Training Epoch: 5 [22100/36450]\tLoss: 442.1691\n",
      "Training Epoch: 5 [22150/36450]\tLoss: 472.4742\n",
      "Training Epoch: 5 [22200/36450]\tLoss: 475.2105\n",
      "Training Epoch: 5 [22250/36450]\tLoss: 441.2290\n",
      "Training Epoch: 5 [22300/36450]\tLoss: 469.0415\n",
      "Training Epoch: 5 [22350/36450]\tLoss: 494.0898\n",
      "Training Epoch: 5 [22400/36450]\tLoss: 461.1628\n",
      "Training Epoch: 5 [22450/36450]\tLoss: 484.4851\n",
      "Training Epoch: 5 [22500/36450]\tLoss: 423.9660\n",
      "Training Epoch: 5 [22550/36450]\tLoss: 435.8443\n",
      "Training Epoch: 5 [22600/36450]\tLoss: 472.1837\n",
      "Training Epoch: 5 [22650/36450]\tLoss: 444.1303\n",
      "Training Epoch: 5 [22700/36450]\tLoss: 441.0948\n",
      "Training Epoch: 5 [22750/36450]\tLoss: 461.9818\n",
      "Training Epoch: 5 [22800/36450]\tLoss: 440.9724\n",
      "Training Epoch: 5 [22850/36450]\tLoss: 428.7597\n",
      "Training Epoch: 5 [22900/36450]\tLoss: 512.2021\n",
      "Training Epoch: 5 [22950/36450]\tLoss: 499.1082\n",
      "Training Epoch: 5 [23000/36450]\tLoss: 439.9949\n",
      "Training Epoch: 5 [23050/36450]\tLoss: 404.3541\n",
      "Training Epoch: 5 [23100/36450]\tLoss: 441.4333\n",
      "Training Epoch: 5 [23150/36450]\tLoss: 439.5009\n",
      "Training Epoch: 5 [23200/36450]\tLoss: 425.8748\n",
      "Training Epoch: 5 [23250/36450]\tLoss: 496.9553\n",
      "Training Epoch: 5 [23300/36450]\tLoss: 479.3533\n",
      "Training Epoch: 5 [23350/36450]\tLoss: 444.6986\n",
      "Training Epoch: 5 [23400/36450]\tLoss: 419.6652\n",
      "Training Epoch: 5 [23450/36450]\tLoss: 472.8399\n",
      "Training Epoch: 5 [23500/36450]\tLoss: 431.5065\n",
      "Training Epoch: 5 [23550/36450]\tLoss: 473.1670\n",
      "Training Epoch: 5 [23600/36450]\tLoss: 486.0336\n",
      "Training Epoch: 5 [23650/36450]\tLoss: 431.7702\n",
      "Training Epoch: 5 [23700/36450]\tLoss: 453.4162\n",
      "Training Epoch: 5 [23750/36450]\tLoss: 469.0514\n",
      "Training Epoch: 5 [23800/36450]\tLoss: 465.4617\n",
      "Training Epoch: 5 [23850/36450]\tLoss: 469.4096\n",
      "Training Epoch: 5 [23900/36450]\tLoss: 463.0874\n",
      "Training Epoch: 5 [23950/36450]\tLoss: 511.3711\n",
      "Training Epoch: 5 [24000/36450]\tLoss: 412.8579\n",
      "Training Epoch: 5 [24050/36450]\tLoss: 452.8833\n",
      "Training Epoch: 5 [24100/36450]\tLoss: 469.8407\n",
      "Training Epoch: 5 [24150/36450]\tLoss: 455.0099\n",
      "Training Epoch: 5 [24200/36450]\tLoss: 470.1410\n",
      "Training Epoch: 5 [24250/36450]\tLoss: 461.6063\n",
      "Training Epoch: 5 [24300/36450]\tLoss: 498.8527\n",
      "Training Epoch: 5 [24350/36450]\tLoss: 423.4430\n",
      "Training Epoch: 5 [24400/36450]\tLoss: 482.9765\n",
      "Training Epoch: 5 [24450/36450]\tLoss: 445.8107\n",
      "Training Epoch: 5 [24500/36450]\tLoss: 514.3909\n",
      "Training Epoch: 5 [24550/36450]\tLoss: 492.4265\n",
      "Training Epoch: 5 [24600/36450]\tLoss: 428.0993\n",
      "Training Epoch: 5 [24650/36450]\tLoss: 463.6052\n",
      "Training Epoch: 5 [24700/36450]\tLoss: 484.2744\n",
      "Training Epoch: 5 [24750/36450]\tLoss: 470.8480\n",
      "Training Epoch: 5 [24800/36450]\tLoss: 517.4500\n",
      "Training Epoch: 5 [24850/36450]\tLoss: 521.6032\n",
      "Training Epoch: 5 [24900/36450]\tLoss: 488.6327\n",
      "Training Epoch: 5 [24950/36450]\tLoss: 491.0783\n",
      "Training Epoch: 5 [25000/36450]\tLoss: 441.2219\n",
      "Training Epoch: 5 [25050/36450]\tLoss: 422.4061\n",
      "Training Epoch: 5 [25100/36450]\tLoss: 430.8330\n",
      "Training Epoch: 5 [25150/36450]\tLoss: 498.3341\n",
      "Training Epoch: 5 [25200/36450]\tLoss: 454.9846\n",
      "Training Epoch: 5 [25250/36450]\tLoss: 538.5847\n",
      "Training Epoch: 5 [25300/36450]\tLoss: 442.3707\n",
      "Training Epoch: 5 [25350/36450]\tLoss: 479.0993\n",
      "Training Epoch: 5 [25400/36450]\tLoss: 416.6254\n",
      "Training Epoch: 5 [25450/36450]\tLoss: 461.5290\n",
      "Training Epoch: 5 [25500/36450]\tLoss: 461.9996\n",
      "Training Epoch: 5 [25550/36450]\tLoss: 475.5128\n",
      "Training Epoch: 5 [25600/36450]\tLoss: 497.6060\n",
      "Training Epoch: 5 [25650/36450]\tLoss: 490.2145\n",
      "Training Epoch: 5 [25700/36450]\tLoss: 468.7445\n",
      "Training Epoch: 5 [25750/36450]\tLoss: 471.6844\n",
      "Training Epoch: 5 [25800/36450]\tLoss: 410.9455\n",
      "Training Epoch: 5 [25850/36450]\tLoss: 457.8486\n",
      "Training Epoch: 5 [25900/36450]\tLoss: 500.1371\n",
      "Training Epoch: 5 [25950/36450]\tLoss: 486.4464\n",
      "Training Epoch: 5 [26000/36450]\tLoss: 472.8057\n",
      "Training Epoch: 5 [26050/36450]\tLoss: 420.2512\n",
      "Training Epoch: 5 [26100/36450]\tLoss: 435.5915\n",
      "Training Epoch: 5 [26150/36450]\tLoss: 457.9955\n",
      "Training Epoch: 5 [26200/36450]\tLoss: 467.1312\n",
      "Training Epoch: 5 [26250/36450]\tLoss: 427.1956\n",
      "Training Epoch: 5 [26300/36450]\tLoss: 489.2004\n",
      "Training Epoch: 5 [26350/36450]\tLoss: 463.4123\n",
      "Training Epoch: 5 [26400/36450]\tLoss: 466.5416\n",
      "Training Epoch: 5 [26450/36450]\tLoss: 448.4503\n",
      "Training Epoch: 5 [26500/36450]\tLoss: 437.4496\n",
      "Training Epoch: 5 [26550/36450]\tLoss: 477.1436\n",
      "Training Epoch: 5 [26600/36450]\tLoss: 463.9630\n",
      "Training Epoch: 5 [26650/36450]\tLoss: 470.6187\n",
      "Training Epoch: 5 [26700/36450]\tLoss: 445.3508\n",
      "Training Epoch: 5 [26750/36450]\tLoss: 445.0597\n",
      "Training Epoch: 5 [26800/36450]\tLoss: 517.0190\n",
      "Training Epoch: 5 [26850/36450]\tLoss: 485.9350\n",
      "Training Epoch: 5 [26900/36450]\tLoss: 493.0212\n",
      "Training Epoch: 5 [26950/36450]\tLoss: 440.1310\n",
      "Training Epoch: 5 [27000/36450]\tLoss: 471.7950\n",
      "Training Epoch: 5 [27050/36450]\tLoss: 449.6242\n",
      "Training Epoch: 5 [27100/36450]\tLoss: 458.6516\n",
      "Training Epoch: 5 [27150/36450]\tLoss: 434.6123\n",
      "Training Epoch: 5 [27200/36450]\tLoss: 470.3970\n",
      "Training Epoch: 5 [27250/36450]\tLoss: 463.4949\n",
      "Training Epoch: 5 [27300/36450]\tLoss: 511.8359\n",
      "Training Epoch: 5 [27350/36450]\tLoss: 431.8455\n",
      "Training Epoch: 5 [27400/36450]\tLoss: 512.5292\n",
      "Training Epoch: 5 [27450/36450]\tLoss: 434.8608\n",
      "Training Epoch: 5 [27500/36450]\tLoss: 447.4639\n",
      "Training Epoch: 5 [27550/36450]\tLoss: 467.0687\n",
      "Training Epoch: 5 [27600/36450]\tLoss: 440.9089\n",
      "Training Epoch: 5 [27650/36450]\tLoss: 441.2669\n",
      "Training Epoch: 5 [27700/36450]\tLoss: 465.9540\n",
      "Training Epoch: 5 [27750/36450]\tLoss: 425.6012\n",
      "Training Epoch: 5 [27800/36450]\tLoss: 466.1751\n",
      "Training Epoch: 5 [27850/36450]\tLoss: 464.8406\n",
      "Training Epoch: 5 [27900/36450]\tLoss: 475.5633\n",
      "Training Epoch: 5 [27950/36450]\tLoss: 458.0727\n",
      "Training Epoch: 5 [28000/36450]\tLoss: 452.7018\n",
      "Training Epoch: 5 [28050/36450]\tLoss: 473.1816\n",
      "Training Epoch: 5 [28100/36450]\tLoss: 443.9249\n",
      "Training Epoch: 5 [28150/36450]\tLoss: 447.9861\n",
      "Training Epoch: 5 [28200/36450]\tLoss: 428.5628\n",
      "Training Epoch: 5 [28250/36450]\tLoss: 450.8675\n",
      "Training Epoch: 5 [28300/36450]\tLoss: 439.8683\n",
      "Training Epoch: 5 [28350/36450]\tLoss: 457.8134\n",
      "Training Epoch: 5 [28400/36450]\tLoss: 465.9847\n",
      "Training Epoch: 5 [28450/36450]\tLoss: 413.0540\n",
      "Training Epoch: 5 [28500/36450]\tLoss: 434.3939\n",
      "Training Epoch: 5 [28550/36450]\tLoss: 455.0280\n",
      "Training Epoch: 5 [28600/36450]\tLoss: 498.1161\n",
      "Training Epoch: 5 [28650/36450]\tLoss: 464.3746\n",
      "Training Epoch: 5 [28700/36450]\tLoss: 432.7513\n",
      "Training Epoch: 5 [28750/36450]\tLoss: 466.8995\n",
      "Training Epoch: 5 [28800/36450]\tLoss: 413.6865\n",
      "Training Epoch: 5 [28850/36450]\tLoss: 437.2559\n",
      "Training Epoch: 5 [28900/36450]\tLoss: 464.9727\n",
      "Training Epoch: 5 [28950/36450]\tLoss: 455.9594\n",
      "Training Epoch: 5 [29000/36450]\tLoss: 523.4454\n",
      "Training Epoch: 5 [29050/36450]\tLoss: 501.0421\n",
      "Training Epoch: 5 [29100/36450]\tLoss: 447.4270\n",
      "Training Epoch: 5 [29150/36450]\tLoss: 520.3013\n",
      "Training Epoch: 5 [29200/36450]\tLoss: 441.8105\n",
      "Training Epoch: 5 [29250/36450]\tLoss: 428.5736\n",
      "Training Epoch: 5 [29300/36450]\tLoss: 400.8777\n",
      "Training Epoch: 5 [29350/36450]\tLoss: 457.2679\n",
      "Training Epoch: 5 [29400/36450]\tLoss: 441.8189\n",
      "Training Epoch: 5 [29450/36450]\tLoss: 470.4102\n",
      "Training Epoch: 5 [29500/36450]\tLoss: 406.8063\n",
      "Training Epoch: 5 [29550/36450]\tLoss: 433.6116\n",
      "Training Epoch: 5 [29600/36450]\tLoss: 445.3368\n",
      "Training Epoch: 5 [29650/36450]\tLoss: 449.5505\n",
      "Training Epoch: 5 [29700/36450]\tLoss: 453.0852\n",
      "Training Epoch: 5 [29750/36450]\tLoss: 417.8140\n",
      "Training Epoch: 5 [29800/36450]\tLoss: 475.5809\n",
      "Training Epoch: 5 [29850/36450]\tLoss: 468.2255\n",
      "Training Epoch: 5 [29900/36450]\tLoss: 469.0895\n",
      "Training Epoch: 5 [29950/36450]\tLoss: 414.0187\n",
      "Training Epoch: 5 [30000/36450]\tLoss: 450.2962\n",
      "Training Epoch: 5 [30050/36450]\tLoss: 500.4808\n",
      "Training Epoch: 5 [30100/36450]\tLoss: 434.5255\n",
      "Training Epoch: 5 [30150/36450]\tLoss: 473.4020\n",
      "Training Epoch: 5 [30200/36450]\tLoss: 445.7072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [30250/36450]\tLoss: 464.6711\n",
      "Training Epoch: 5 [30300/36450]\tLoss: 437.6425\n",
      "Training Epoch: 5 [30350/36450]\tLoss: 459.0228\n",
      "Training Epoch: 5 [30400/36450]\tLoss: 444.8516\n",
      "Training Epoch: 5 [30450/36450]\tLoss: 460.1548\n",
      "Training Epoch: 5 [30500/36450]\tLoss: 447.5588\n",
      "Training Epoch: 5 [30550/36450]\tLoss: 494.2242\n",
      "Training Epoch: 5 [30600/36450]\tLoss: 477.4679\n",
      "Training Epoch: 5 [30650/36450]\tLoss: 512.5944\n",
      "Training Epoch: 5 [30700/36450]\tLoss: 440.8765\n",
      "Training Epoch: 5 [30750/36450]\tLoss: 479.9438\n",
      "Training Epoch: 5 [30800/36450]\tLoss: 491.3422\n",
      "Training Epoch: 5 [30850/36450]\tLoss: 475.2365\n",
      "Training Epoch: 5 [30900/36450]\tLoss: 422.6395\n",
      "Training Epoch: 5 [30950/36450]\tLoss: 439.7020\n",
      "Training Epoch: 5 [31000/36450]\tLoss: 460.1163\n",
      "Training Epoch: 5 [31050/36450]\tLoss: 436.1303\n",
      "Training Epoch: 5 [31100/36450]\tLoss: 469.7207\n",
      "Training Epoch: 5 [31150/36450]\tLoss: 508.7414\n",
      "Training Epoch: 5 [31200/36450]\tLoss: 436.6713\n",
      "Training Epoch: 5 [31250/36450]\tLoss: 419.5189\n",
      "Training Epoch: 5 [31300/36450]\tLoss: 420.9903\n",
      "Training Epoch: 5 [31350/36450]\tLoss: 443.2233\n",
      "Training Epoch: 5 [31400/36450]\tLoss: 419.4219\n",
      "Training Epoch: 5 [31450/36450]\tLoss: 471.5551\n",
      "Training Epoch: 5 [31500/36450]\tLoss: 412.5474\n",
      "Training Epoch: 5 [31550/36450]\tLoss: 456.2160\n",
      "Training Epoch: 5 [31600/36450]\tLoss: 450.8895\n",
      "Training Epoch: 5 [31650/36450]\tLoss: 495.0225\n",
      "Training Epoch: 5 [31700/36450]\tLoss: 455.1302\n",
      "Training Epoch: 5 [31750/36450]\tLoss: 464.0477\n",
      "Training Epoch: 5 [31800/36450]\tLoss: 457.0674\n",
      "Training Epoch: 5 [31850/36450]\tLoss: 434.1282\n",
      "Training Epoch: 5 [31900/36450]\tLoss: 438.8968\n",
      "Training Epoch: 5 [31950/36450]\tLoss: 421.7245\n",
      "Training Epoch: 5 [32000/36450]\tLoss: 472.6932\n",
      "Training Epoch: 5 [32050/36450]\tLoss: 466.2173\n",
      "Training Epoch: 5 [32100/36450]\tLoss: 459.9477\n",
      "Training Epoch: 5 [32150/36450]\tLoss: 429.3961\n",
      "Training Epoch: 5 [32200/36450]\tLoss: 476.4563\n",
      "Training Epoch: 5 [32250/36450]\tLoss: 479.6595\n",
      "Training Epoch: 5 [32300/36450]\tLoss: 432.1110\n",
      "Training Epoch: 5 [32350/36450]\tLoss: 401.5562\n",
      "Training Epoch: 5 [32400/36450]\tLoss: 443.9343\n",
      "Training Epoch: 5 [32450/36450]\tLoss: 423.1285\n",
      "Training Epoch: 5 [32500/36450]\tLoss: 453.4594\n",
      "Training Epoch: 5 [32550/36450]\tLoss: 491.2251\n",
      "Training Epoch: 5 [32600/36450]\tLoss: 415.6114\n",
      "Training Epoch: 5 [32650/36450]\tLoss: 456.8944\n",
      "Training Epoch: 5 [32700/36450]\tLoss: 488.2825\n",
      "Training Epoch: 5 [32750/36450]\tLoss: 465.4509\n",
      "Training Epoch: 5 [32800/36450]\tLoss: 453.3081\n",
      "Training Epoch: 5 [32850/36450]\tLoss: 512.2852\n",
      "Training Epoch: 5 [32900/36450]\tLoss: 467.6758\n",
      "Training Epoch: 5 [32950/36450]\tLoss: 438.5955\n",
      "Training Epoch: 5 [33000/36450]\tLoss: 491.9126\n",
      "Training Epoch: 5 [33050/36450]\tLoss: 439.5027\n",
      "Training Epoch: 5 [33100/36450]\tLoss: 455.3874\n",
      "Training Epoch: 5 [33150/36450]\tLoss: 431.3102\n",
      "Training Epoch: 5 [33200/36450]\tLoss: 467.1413\n",
      "Training Epoch: 5 [33250/36450]\tLoss: 468.2345\n",
      "Training Epoch: 5 [33300/36450]\tLoss: 483.9627\n",
      "Training Epoch: 5 [33350/36450]\tLoss: 445.2253\n",
      "Training Epoch: 5 [33400/36450]\tLoss: 474.8874\n",
      "Training Epoch: 5 [33450/36450]\tLoss: 394.2304\n",
      "Training Epoch: 5 [33500/36450]\tLoss: 414.7790\n",
      "Training Epoch: 5 [33550/36450]\tLoss: 490.5772\n",
      "Training Epoch: 5 [33600/36450]\tLoss: 469.2772\n",
      "Training Epoch: 5 [33650/36450]\tLoss: 456.6218\n",
      "Training Epoch: 5 [33700/36450]\tLoss: 464.9524\n",
      "Training Epoch: 5 [33750/36450]\tLoss: 453.8145\n",
      "Training Epoch: 5 [33800/36450]\tLoss: 455.2876\n",
      "Training Epoch: 5 [33850/36450]\tLoss: 433.2186\n",
      "Training Epoch: 5 [33900/36450]\tLoss: 454.0646\n",
      "Training Epoch: 5 [33950/36450]\tLoss: 449.7023\n",
      "Training Epoch: 5 [34000/36450]\tLoss: 437.2661\n",
      "Training Epoch: 5 [34050/36450]\tLoss: 438.6536\n",
      "Training Epoch: 5 [34100/36450]\tLoss: 460.9624\n",
      "Training Epoch: 5 [34150/36450]\tLoss: 457.4759\n",
      "Training Epoch: 5 [34200/36450]\tLoss: 415.5002\n",
      "Training Epoch: 5 [34250/36450]\tLoss: 458.3757\n",
      "Training Epoch: 5 [34300/36450]\tLoss: 438.1387\n",
      "Training Epoch: 5 [34350/36450]\tLoss: 454.4593\n",
      "Training Epoch: 5 [34400/36450]\tLoss: 460.6806\n",
      "Training Epoch: 5 [34450/36450]\tLoss: 450.0831\n",
      "Training Epoch: 5 [34500/36450]\tLoss: 467.2577\n",
      "Training Epoch: 5 [34550/36450]\tLoss: 447.4729\n",
      "Training Epoch: 5 [34600/36450]\tLoss: 412.1792\n",
      "Training Epoch: 5 [34650/36450]\tLoss: 435.0268\n",
      "Training Epoch: 5 [34700/36450]\tLoss: 425.8709\n",
      "Training Epoch: 5 [34750/36450]\tLoss: 448.3554\n",
      "Training Epoch: 5 [34800/36450]\tLoss: 437.6223\n",
      "Training Epoch: 5 [34850/36450]\tLoss: 459.3667\n",
      "Training Epoch: 5 [34900/36450]\tLoss: 437.9447\n",
      "Training Epoch: 5 [34950/36450]\tLoss: 437.2754\n",
      "Training Epoch: 5 [35000/36450]\tLoss: 408.0444\n",
      "Training Epoch: 5 [35050/36450]\tLoss: 431.2274\n",
      "Training Epoch: 5 [35100/36450]\tLoss: 394.1868\n",
      "Training Epoch: 5 [35150/36450]\tLoss: 462.1600\n",
      "Training Epoch: 5 [35200/36450]\tLoss: 423.1400\n",
      "Training Epoch: 5 [35250/36450]\tLoss: 502.4920\n",
      "Training Epoch: 5 [35300/36450]\tLoss: 455.0222\n",
      "Training Epoch: 5 [35350/36450]\tLoss: 408.1705\n",
      "Training Epoch: 5 [35400/36450]\tLoss: 411.6091\n",
      "Training Epoch: 5 [35450/36450]\tLoss: 429.9155\n",
      "Training Epoch: 5 [35500/36450]\tLoss: 446.6305\n",
      "Training Epoch: 5 [35550/36450]\tLoss: 442.2758\n",
      "Training Epoch: 5 [35600/36450]\tLoss: 441.6484\n",
      "Training Epoch: 5 [35650/36450]\tLoss: 438.6627\n",
      "Training Epoch: 5 [35700/36450]\tLoss: 447.8784\n",
      "Training Epoch: 5 [35750/36450]\tLoss: 455.9453\n",
      "Training Epoch: 5 [35800/36450]\tLoss: 415.3484\n",
      "Training Epoch: 5 [35850/36450]\tLoss: 400.9116\n",
      "Training Epoch: 5 [35900/36450]\tLoss: 436.6527\n",
      "Training Epoch: 5 [35950/36450]\tLoss: 427.6205\n",
      "Training Epoch: 5 [36000/36450]\tLoss: 435.3943\n",
      "Training Epoch: 5 [36050/36450]\tLoss: 436.0533\n",
      "Training Epoch: 5 [36100/36450]\tLoss: 478.8397\n",
      "Training Epoch: 5 [36150/36450]\tLoss: 420.6305\n",
      "Training Epoch: 5 [36200/36450]\tLoss: 425.0420\n",
      "Training Epoch: 5 [36250/36450]\tLoss: 472.9806\n",
      "Training Epoch: 5 [36300/36450]\tLoss: 460.4966\n",
      "Training Epoch: 5 [36350/36450]\tLoss: 465.3924\n",
      "Training Epoch: 5 [36400/36450]\tLoss: 413.8494\n",
      "Training Epoch: 5 [36450/36450]\tLoss: 415.1400\n",
      "Training Epoch: 5 [4050/4050]\tLoss: 206.3408\n",
      "Training Epoch: 6 [50/36450]\tLoss: 463.5256\n",
      "Training Epoch: 6 [100/36450]\tLoss: 472.5607\n",
      "Training Epoch: 6 [150/36450]\tLoss: 471.6823\n",
      "Training Epoch: 6 [200/36450]\tLoss: 397.5852\n",
      "Training Epoch: 6 [250/36450]\tLoss: 462.5712\n",
      "Training Epoch: 6 [300/36450]\tLoss: 447.6336\n",
      "Training Epoch: 6 [350/36450]\tLoss: 440.5754\n",
      "Training Epoch: 6 [400/36450]\tLoss: 429.9444\n",
      "Training Epoch: 6 [450/36450]\tLoss: 429.6664\n",
      "Training Epoch: 6 [500/36450]\tLoss: 436.6425\n",
      "Training Epoch: 6 [550/36450]\tLoss: 417.8834\n",
      "Training Epoch: 6 [600/36450]\tLoss: 465.9972\n",
      "Training Epoch: 6 [650/36450]\tLoss: 426.2068\n",
      "Training Epoch: 6 [700/36450]\tLoss: 429.8216\n",
      "Training Epoch: 6 [750/36450]\tLoss: 429.9446\n",
      "Training Epoch: 6 [800/36450]\tLoss: 450.5570\n",
      "Training Epoch: 6 [850/36450]\tLoss: 438.4376\n",
      "Training Epoch: 6 [900/36450]\tLoss: 438.2995\n",
      "Training Epoch: 6 [950/36450]\tLoss: 441.1667\n",
      "Training Epoch: 6 [1000/36450]\tLoss: 432.0280\n",
      "Training Epoch: 6 [1050/36450]\tLoss: 456.8081\n",
      "Training Epoch: 6 [1100/36450]\tLoss: 447.7084\n",
      "Training Epoch: 6 [1150/36450]\tLoss: 426.7686\n",
      "Training Epoch: 6 [1200/36450]\tLoss: 461.2651\n",
      "Training Epoch: 6 [1250/36450]\tLoss: 455.7141\n",
      "Training Epoch: 6 [1300/36450]\tLoss: 417.8293\n",
      "Training Epoch: 6 [1350/36450]\tLoss: 468.1773\n",
      "Training Epoch: 6 [1400/36450]\tLoss: 423.6009\n",
      "Training Epoch: 6 [1450/36450]\tLoss: 462.1818\n",
      "Training Epoch: 6 [1500/36450]\tLoss: 441.4352\n",
      "Training Epoch: 6 [1550/36450]\tLoss: 409.0235\n",
      "Training Epoch: 6 [1600/36450]\tLoss: 449.1336\n",
      "Training Epoch: 6 [1650/36450]\tLoss: 440.4972\n",
      "Training Epoch: 6 [1700/36450]\tLoss: 447.5440\n",
      "Training Epoch: 6 [1750/36450]\tLoss: 449.0739\n",
      "Training Epoch: 6 [1800/36450]\tLoss: 449.1147\n",
      "Training Epoch: 6 [1850/36450]\tLoss: 439.0839\n",
      "Training Epoch: 6 [1900/36450]\tLoss: 421.9506\n",
      "Training Epoch: 6 [1950/36450]\tLoss: 438.4630\n",
      "Training Epoch: 6 [2000/36450]\tLoss: 438.7854\n",
      "Training Epoch: 6 [2050/36450]\tLoss: 462.8970\n",
      "Training Epoch: 6 [2100/36450]\tLoss: 446.6730\n",
      "Training Epoch: 6 [2150/36450]\tLoss: 461.8655\n",
      "Training Epoch: 6 [2200/36450]\tLoss: 408.3045\n",
      "Training Epoch: 6 [2250/36450]\tLoss: 402.2557\n",
      "Training Epoch: 6 [2300/36450]\tLoss: 462.1023\n",
      "Training Epoch: 6 [2350/36450]\tLoss: 437.1016\n",
      "Training Epoch: 6 [2400/36450]\tLoss: 417.8524\n",
      "Training Epoch: 6 [2450/36450]\tLoss: 415.1996\n",
      "Training Epoch: 6 [2500/36450]\tLoss: 426.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [2550/36450]\tLoss: 453.6287\n",
      "Training Epoch: 6 [2600/36450]\tLoss: 417.2899\n",
      "Training Epoch: 6 [2650/36450]\tLoss: 440.8862\n",
      "Training Epoch: 6 [2700/36450]\tLoss: 448.4848\n",
      "Training Epoch: 6 [2750/36450]\tLoss: 408.6734\n",
      "Training Epoch: 6 [2800/36450]\tLoss: 443.1174\n",
      "Training Epoch: 6 [2850/36450]\tLoss: 458.6912\n",
      "Training Epoch: 6 [2900/36450]\tLoss: 424.3484\n",
      "Training Epoch: 6 [2950/36450]\tLoss: 422.5819\n",
      "Training Epoch: 6 [3000/36450]\tLoss: 495.8237\n",
      "Training Epoch: 6 [3050/36450]\tLoss: 450.7480\n",
      "Training Epoch: 6 [3100/36450]\tLoss: 452.8221\n",
      "Training Epoch: 6 [3150/36450]\tLoss: 400.8737\n",
      "Training Epoch: 6 [3200/36450]\tLoss: 448.5619\n",
      "Training Epoch: 6 [3250/36450]\tLoss: 423.3221\n",
      "Training Epoch: 6 [3300/36450]\tLoss: 446.8468\n",
      "Training Epoch: 6 [3350/36450]\tLoss: 432.4272\n",
      "Training Epoch: 6 [3400/36450]\tLoss: 492.5894\n",
      "Training Epoch: 6 [3450/36450]\tLoss: 429.6422\n",
      "Training Epoch: 6 [3500/36450]\tLoss: 420.5717\n",
      "Training Epoch: 6 [3550/36450]\tLoss: 438.4216\n",
      "Training Epoch: 6 [3600/36450]\tLoss: 428.0999\n",
      "Training Epoch: 6 [3650/36450]\tLoss: 458.2210\n",
      "Training Epoch: 6 [3700/36450]\tLoss: 426.5955\n",
      "Training Epoch: 6 [3750/36450]\tLoss: 444.6104\n",
      "Training Epoch: 6 [3800/36450]\tLoss: 433.6992\n",
      "Training Epoch: 6 [3850/36450]\tLoss: 431.7156\n",
      "Training Epoch: 6 [3900/36450]\tLoss: 436.1295\n",
      "Training Epoch: 6 [3950/36450]\tLoss: 452.5553\n",
      "Training Epoch: 6 [4000/36450]\tLoss: 408.2984\n",
      "Training Epoch: 6 [4050/36450]\tLoss: 417.3648\n",
      "Training Epoch: 6 [4100/36450]\tLoss: 436.8222\n",
      "Training Epoch: 6 [4150/36450]\tLoss: 423.4132\n",
      "Training Epoch: 6 [4200/36450]\tLoss: 435.2067\n",
      "Training Epoch: 6 [4250/36450]\tLoss: 417.1021\n",
      "Training Epoch: 6 [4300/36450]\tLoss: 403.3741\n",
      "Training Epoch: 6 [4350/36450]\tLoss: 410.8754\n",
      "Training Epoch: 6 [4400/36450]\tLoss: 438.3643\n",
      "Training Epoch: 6 [4450/36450]\tLoss: 441.9607\n",
      "Training Epoch: 6 [4500/36450]\tLoss: 474.2287\n",
      "Training Epoch: 6 [4550/36450]\tLoss: 479.7805\n",
      "Training Epoch: 6 [4600/36450]\tLoss: 445.5595\n",
      "Training Epoch: 6 [4650/36450]\tLoss: 477.1993\n",
      "Training Epoch: 6 [4700/36450]\tLoss: 443.5199\n",
      "Training Epoch: 6 [4750/36450]\tLoss: 395.7180\n",
      "Training Epoch: 6 [4800/36450]\tLoss: 395.2003\n",
      "Training Epoch: 6 [4850/36450]\tLoss: 502.1581\n",
      "Training Epoch: 6 [4900/36450]\tLoss: 392.5254\n",
      "Training Epoch: 6 [4950/36450]\tLoss: 472.4032\n",
      "Training Epoch: 6 [5000/36450]\tLoss: 491.0570\n",
      "Training Epoch: 6 [5050/36450]\tLoss: 430.8397\n",
      "Training Epoch: 6 [5100/36450]\tLoss: 490.1390\n",
      "Training Epoch: 6 [5150/36450]\tLoss: 428.9878\n",
      "Training Epoch: 6 [5200/36450]\tLoss: 417.7393\n",
      "Training Epoch: 6 [5250/36450]\tLoss: 446.9844\n",
      "Training Epoch: 6 [5300/36450]\tLoss: 423.5229\n",
      "Training Epoch: 6 [5350/36450]\tLoss: 412.3768\n",
      "Training Epoch: 6 [5400/36450]\tLoss: 416.2248\n",
      "Training Epoch: 6 [5450/36450]\tLoss: 416.8294\n",
      "Training Epoch: 6 [5500/36450]\tLoss: 448.4630\n",
      "Training Epoch: 6 [5550/36450]\tLoss: 482.6319\n",
      "Training Epoch: 6 [5600/36450]\tLoss: 412.1789\n",
      "Training Epoch: 6 [5650/36450]\tLoss: 443.6314\n",
      "Training Epoch: 6 [5700/36450]\tLoss: 441.7399\n",
      "Training Epoch: 6 [5750/36450]\tLoss: 437.7773\n",
      "Training Epoch: 6 [5800/36450]\tLoss: 454.1813\n",
      "Training Epoch: 6 [5850/36450]\tLoss: 428.3183\n",
      "Training Epoch: 6 [5900/36450]\tLoss: 427.7684\n",
      "Training Epoch: 6 [5950/36450]\tLoss: 461.8601\n",
      "Training Epoch: 6 [6000/36450]\tLoss: 414.5473\n",
      "Training Epoch: 6 [6050/36450]\tLoss: 461.1714\n",
      "Training Epoch: 6 [6100/36450]\tLoss: 406.9237\n",
      "Training Epoch: 6 [6150/36450]\tLoss: 420.6041\n",
      "Training Epoch: 6 [6200/36450]\tLoss: 416.0561\n",
      "Training Epoch: 6 [6250/36450]\tLoss: 458.6696\n",
      "Training Epoch: 6 [6300/36450]\tLoss: 457.4501\n",
      "Training Epoch: 6 [6350/36450]\tLoss: 404.4433\n",
      "Training Epoch: 6 [6400/36450]\tLoss: 416.5513\n",
      "Training Epoch: 6 [6450/36450]\tLoss: 426.4705\n",
      "Training Epoch: 6 [6500/36450]\tLoss: 431.4756\n",
      "Training Epoch: 6 [6550/36450]\tLoss: 431.2110\n",
      "Training Epoch: 6 [6600/36450]\tLoss: 402.1495\n",
      "Training Epoch: 6 [6650/36450]\tLoss: 456.5566\n",
      "Training Epoch: 6 [6700/36450]\tLoss: 438.2692\n",
      "Training Epoch: 6 [6750/36450]\tLoss: 428.4601\n",
      "Training Epoch: 6 [6800/36450]\tLoss: 484.1224\n",
      "Training Epoch: 6 [6850/36450]\tLoss: 450.8294\n",
      "Training Epoch: 6 [6900/36450]\tLoss: 412.1496\n",
      "Training Epoch: 6 [6950/36450]\tLoss: 432.9731\n",
      "Training Epoch: 6 [7000/36450]\tLoss: 424.2056\n",
      "Training Epoch: 6 [7050/36450]\tLoss: 435.7814\n",
      "Training Epoch: 6 [7100/36450]\tLoss: 427.2246\n",
      "Training Epoch: 6 [7150/36450]\tLoss: 439.6935\n",
      "Training Epoch: 6 [7200/36450]\tLoss: 417.9668\n",
      "Training Epoch: 6 [7250/36450]\tLoss: 439.6122\n",
      "Training Epoch: 6 [7300/36450]\tLoss: 420.6182\n",
      "Training Epoch: 6 [7350/36450]\tLoss: 436.5603\n",
      "Training Epoch: 6 [7400/36450]\tLoss: 402.5728\n",
      "Training Epoch: 6 [7450/36450]\tLoss: 402.6023\n",
      "Training Epoch: 6 [7500/36450]\tLoss: 450.9588\n",
      "Training Epoch: 6 [7550/36450]\tLoss: 404.3021\n",
      "Training Epoch: 6 [7600/36450]\tLoss: 437.2177\n",
      "Training Epoch: 6 [7650/36450]\tLoss: 465.1212\n",
      "Training Epoch: 6 [7700/36450]\tLoss: 406.4855\n",
      "Training Epoch: 6 [7750/36450]\tLoss: 453.9478\n",
      "Training Epoch: 6 [7800/36450]\tLoss: 418.5804\n",
      "Training Epoch: 6 [7850/36450]\tLoss: 423.9836\n",
      "Training Epoch: 6 [7900/36450]\tLoss: 404.4463\n",
      "Training Epoch: 6 [7950/36450]\tLoss: 438.5468\n",
      "Training Epoch: 6 [8000/36450]\tLoss: 462.3559\n",
      "Training Epoch: 6 [8050/36450]\tLoss: 496.6480\n",
      "Training Epoch: 6 [8100/36450]\tLoss: 440.9642\n",
      "Training Epoch: 6 [8150/36450]\tLoss: 483.9186\n",
      "Training Epoch: 6 [8200/36450]\tLoss: 420.0909\n",
      "Training Epoch: 6 [8250/36450]\tLoss: 423.8067\n",
      "Training Epoch: 6 [8300/36450]\tLoss: 421.9961\n",
      "Training Epoch: 6 [8350/36450]\tLoss: 408.0985\n",
      "Training Epoch: 6 [8400/36450]\tLoss: 454.9994\n",
      "Training Epoch: 6 [8450/36450]\tLoss: 407.5519\n",
      "Training Epoch: 6 [8500/36450]\tLoss: 431.0027\n",
      "Training Epoch: 6 [8550/36450]\tLoss: 492.1772\n",
      "Training Epoch: 6 [8600/36450]\tLoss: 491.6268\n",
      "Training Epoch: 6 [8650/36450]\tLoss: 413.5511\n",
      "Training Epoch: 6 [8700/36450]\tLoss: 411.0195\n",
      "Training Epoch: 6 [8750/36450]\tLoss: 434.3324\n",
      "Training Epoch: 6 [8800/36450]\tLoss: 415.8552\n",
      "Training Epoch: 6 [8850/36450]\tLoss: 443.7995\n",
      "Training Epoch: 6 [8900/36450]\tLoss: 429.5601\n",
      "Training Epoch: 6 [8950/36450]\tLoss: 404.5843\n",
      "Training Epoch: 6 [9000/36450]\tLoss: 445.6957\n",
      "Training Epoch: 6 [9050/36450]\tLoss: 416.2406\n",
      "Training Epoch: 6 [9100/36450]\tLoss: 417.6836\n",
      "Training Epoch: 6 [9150/36450]\tLoss: 471.0716\n",
      "Training Epoch: 6 [9200/36450]\tLoss: 459.0516\n",
      "Training Epoch: 6 [9250/36450]\tLoss: 418.0942\n",
      "Training Epoch: 6 [9300/36450]\tLoss: 398.4709\n",
      "Training Epoch: 6 [9350/36450]\tLoss: 463.9932\n",
      "Training Epoch: 6 [9400/36450]\tLoss: 455.7097\n",
      "Training Epoch: 6 [9450/36450]\tLoss: 455.7671\n",
      "Training Epoch: 6 [9500/36450]\tLoss: 428.7910\n",
      "Training Epoch: 6 [9550/36450]\tLoss: 440.4887\n",
      "Training Epoch: 6 [9600/36450]\tLoss: 443.6340\n",
      "Training Epoch: 6 [9650/36450]\tLoss: 400.4923\n",
      "Training Epoch: 6 [9700/36450]\tLoss: 427.3058\n",
      "Training Epoch: 6 [9750/36450]\tLoss: 429.2751\n",
      "Training Epoch: 6 [9800/36450]\tLoss: 460.5619\n",
      "Training Epoch: 6 [9850/36450]\tLoss: 461.9639\n",
      "Training Epoch: 6 [9900/36450]\tLoss: 436.0521\n",
      "Training Epoch: 6 [9950/36450]\tLoss: 436.5136\n",
      "Training Epoch: 6 [10000/36450]\tLoss: 425.9057\n",
      "Training Epoch: 6 [10050/36450]\tLoss: 480.3942\n",
      "Training Epoch: 6 [10100/36450]\tLoss: 407.0410\n",
      "Training Epoch: 6 [10150/36450]\tLoss: 418.8874\n",
      "Training Epoch: 6 [10200/36450]\tLoss: 457.4189\n",
      "Training Epoch: 6 [10250/36450]\tLoss: 427.4485\n",
      "Training Epoch: 6 [10300/36450]\tLoss: 447.5770\n",
      "Training Epoch: 6 [10350/36450]\tLoss: 424.2293\n",
      "Training Epoch: 6 [10400/36450]\tLoss: 433.6000\n",
      "Training Epoch: 6 [10450/36450]\tLoss: 402.5032\n",
      "Training Epoch: 6 [10500/36450]\tLoss: 430.1085\n",
      "Training Epoch: 6 [10550/36450]\tLoss: 438.5210\n",
      "Training Epoch: 6 [10600/36450]\tLoss: 448.2837\n",
      "Training Epoch: 6 [10650/36450]\tLoss: 424.3263\n",
      "Training Epoch: 6 [10700/36450]\tLoss: 418.7251\n",
      "Training Epoch: 6 [10750/36450]\tLoss: 387.5251\n",
      "Training Epoch: 6 [10800/36450]\tLoss: 427.7726\n",
      "Training Epoch: 6 [10850/36450]\tLoss: 422.6218\n",
      "Training Epoch: 6 [10900/36450]\tLoss: 469.2489\n",
      "Training Epoch: 6 [10950/36450]\tLoss: 453.7750\n",
      "Training Epoch: 6 [11000/36450]\tLoss: 429.0312\n",
      "Training Epoch: 6 [11050/36450]\tLoss: 421.7545\n",
      "Training Epoch: 6 [11100/36450]\tLoss: 475.6947\n",
      "Training Epoch: 6 [11150/36450]\tLoss: 436.7976\n",
      "Training Epoch: 6 [11200/36450]\tLoss: 419.1103\n",
      "Training Epoch: 6 [11250/36450]\tLoss: 411.3037\n",
      "Training Epoch: 6 [11300/36450]\tLoss: 401.2178\n",
      "Training Epoch: 6 [11350/36450]\tLoss: 443.7696\n",
      "Training Epoch: 6 [11400/36450]\tLoss: 367.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [11450/36450]\tLoss: 419.1328\n",
      "Training Epoch: 6 [11500/36450]\tLoss: 409.7364\n",
      "Training Epoch: 6 [11550/36450]\tLoss: 422.0656\n",
      "Training Epoch: 6 [11600/36450]\tLoss: 464.3442\n",
      "Training Epoch: 6 [11650/36450]\tLoss: 412.9283\n",
      "Training Epoch: 6 [11700/36450]\tLoss: 425.0602\n",
      "Training Epoch: 6 [11750/36450]\tLoss: 468.6944\n",
      "Training Epoch: 6 [11800/36450]\tLoss: 447.9266\n",
      "Training Epoch: 6 [11850/36450]\tLoss: 416.5211\n",
      "Training Epoch: 6 [11900/36450]\tLoss: 434.8003\n",
      "Training Epoch: 6 [11950/36450]\tLoss: 414.6367\n",
      "Training Epoch: 6 [12000/36450]\tLoss: 419.1066\n",
      "Training Epoch: 6 [12050/36450]\tLoss: 382.7516\n",
      "Training Epoch: 6 [12100/36450]\tLoss: 422.2849\n",
      "Training Epoch: 6 [12150/36450]\tLoss: 444.3038\n",
      "Training Epoch: 6 [12200/36450]\tLoss: 427.3533\n",
      "Training Epoch: 6 [12250/36450]\tLoss: 400.0260\n",
      "Training Epoch: 6 [12300/36450]\tLoss: 405.3763\n",
      "Training Epoch: 6 [12350/36450]\tLoss: 449.8762\n",
      "Training Epoch: 6 [12400/36450]\tLoss: 463.8528\n",
      "Training Epoch: 6 [12450/36450]\tLoss: 427.5921\n",
      "Training Epoch: 6 [12500/36450]\tLoss: 443.2991\n",
      "Training Epoch: 6 [12550/36450]\tLoss: 456.9126\n",
      "Training Epoch: 6 [12600/36450]\tLoss: 422.5542\n",
      "Training Epoch: 6 [12650/36450]\tLoss: 442.1213\n",
      "Training Epoch: 6 [12700/36450]\tLoss: 460.9734\n",
      "Training Epoch: 6 [12750/36450]\tLoss: 424.0267\n",
      "Training Epoch: 6 [12800/36450]\tLoss: 434.5698\n",
      "Training Epoch: 6 [12850/36450]\tLoss: 438.5686\n",
      "Training Epoch: 6 [12900/36450]\tLoss: 435.5503\n",
      "Training Epoch: 6 [12950/36450]\tLoss: 442.2549\n",
      "Training Epoch: 6 [13000/36450]\tLoss: 431.0466\n",
      "Training Epoch: 6 [13050/36450]\tLoss: 435.1960\n",
      "Training Epoch: 6 [13100/36450]\tLoss: 437.6229\n",
      "Training Epoch: 6 [13150/36450]\tLoss: 460.7100\n",
      "Training Epoch: 6 [13200/36450]\tLoss: 446.4860\n",
      "Training Epoch: 6 [13250/36450]\tLoss: 412.4905\n",
      "Training Epoch: 6 [13300/36450]\tLoss: 413.5638\n",
      "Training Epoch: 6 [13350/36450]\tLoss: 432.3622\n",
      "Training Epoch: 6 [13400/36450]\tLoss: 394.1196\n",
      "Training Epoch: 6 [13450/36450]\tLoss: 413.5848\n",
      "Training Epoch: 6 [13500/36450]\tLoss: 481.5490\n",
      "Training Epoch: 6 [13550/36450]\tLoss: 408.8627\n",
      "Training Epoch: 6 [13600/36450]\tLoss: 411.8865\n",
      "Training Epoch: 6 [13650/36450]\tLoss: 423.0133\n",
      "Training Epoch: 6 [13700/36450]\tLoss: 391.8636\n",
      "Training Epoch: 6 [13750/36450]\tLoss: 403.3398\n",
      "Training Epoch: 6 [13800/36450]\tLoss: 437.1070\n",
      "Training Epoch: 6 [13850/36450]\tLoss: 398.4398\n",
      "Training Epoch: 6 [13900/36450]\tLoss: 440.6627\n",
      "Training Epoch: 6 [13950/36450]\tLoss: 468.4184\n",
      "Training Epoch: 6 [14000/36450]\tLoss: 405.3083\n",
      "Training Epoch: 6 [14050/36450]\tLoss: 435.0065\n",
      "Training Epoch: 6 [14100/36450]\tLoss: 411.8286\n",
      "Training Epoch: 6 [14150/36450]\tLoss: 456.6738\n",
      "Training Epoch: 6 [14200/36450]\tLoss: 474.6331\n",
      "Training Epoch: 6 [14250/36450]\tLoss: 479.8463\n",
      "Training Epoch: 6 [14300/36450]\tLoss: 434.0273\n",
      "Training Epoch: 6 [14350/36450]\tLoss: 472.1378\n",
      "Training Epoch: 6 [14400/36450]\tLoss: 422.9498\n",
      "Training Epoch: 6 [14450/36450]\tLoss: 439.5305\n",
      "Training Epoch: 6 [14500/36450]\tLoss: 459.4354\n",
      "Training Epoch: 6 [14550/36450]\tLoss: 435.9158\n",
      "Training Epoch: 6 [14600/36450]\tLoss: 434.3020\n",
      "Training Epoch: 6 [14650/36450]\tLoss: 395.4691\n",
      "Training Epoch: 6 [14700/36450]\tLoss: 420.8306\n",
      "Training Epoch: 6 [14750/36450]\tLoss: 444.0135\n",
      "Training Epoch: 6 [14800/36450]\tLoss: 448.0833\n",
      "Training Epoch: 6 [14850/36450]\tLoss: 430.5976\n",
      "Training Epoch: 6 [14900/36450]\tLoss: 421.0877\n",
      "Training Epoch: 6 [14950/36450]\tLoss: 420.0023\n",
      "Training Epoch: 6 [15000/36450]\tLoss: 453.8834\n",
      "Training Epoch: 6 [15050/36450]\tLoss: 419.6485\n",
      "Training Epoch: 6 [15100/36450]\tLoss: 481.9507\n",
      "Training Epoch: 6 [15150/36450]\tLoss: 390.9474\n",
      "Training Epoch: 6 [15200/36450]\tLoss: 405.1143\n",
      "Training Epoch: 6 [15250/36450]\tLoss: 394.1514\n",
      "Training Epoch: 6 [15300/36450]\tLoss: 459.0350\n",
      "Training Epoch: 6 [15350/36450]\tLoss: 439.4348\n",
      "Training Epoch: 6 [15400/36450]\tLoss: 439.6496\n",
      "Training Epoch: 6 [15450/36450]\tLoss: 431.6930\n",
      "Training Epoch: 6 [15500/36450]\tLoss: 457.0096\n",
      "Training Epoch: 6 [15550/36450]\tLoss: 426.8456\n",
      "Training Epoch: 6 [15600/36450]\tLoss: 449.4839\n",
      "Training Epoch: 6 [15650/36450]\tLoss: 428.5587\n",
      "Training Epoch: 6 [15700/36450]\tLoss: 467.7885\n",
      "Training Epoch: 6 [15750/36450]\tLoss: 402.4744\n",
      "Training Epoch: 6 [15800/36450]\tLoss: 470.8769\n",
      "Training Epoch: 6 [15850/36450]\tLoss: 425.1757\n",
      "Training Epoch: 6 [15900/36450]\tLoss: 428.6577\n",
      "Training Epoch: 6 [15950/36450]\tLoss: 418.1906\n",
      "Training Epoch: 6 [16000/36450]\tLoss: 405.2172\n",
      "Training Epoch: 6 [16050/36450]\tLoss: 378.7210\n",
      "Training Epoch: 6 [16100/36450]\tLoss: 430.1065\n",
      "Training Epoch: 6 [16150/36450]\tLoss: 401.5813\n",
      "Training Epoch: 6 [16200/36450]\tLoss: 433.4580\n",
      "Training Epoch: 6 [16250/36450]\tLoss: 414.9396\n",
      "Training Epoch: 6 [16300/36450]\tLoss: 488.2111\n",
      "Training Epoch: 6 [16350/36450]\tLoss: 394.7159\n",
      "Training Epoch: 6 [16400/36450]\tLoss: 415.0760\n",
      "Training Epoch: 6 [16450/36450]\tLoss: 429.0971\n",
      "Training Epoch: 6 [16500/36450]\tLoss: 452.2946\n",
      "Training Epoch: 6 [16550/36450]\tLoss: 468.6198\n",
      "Training Epoch: 6 [16600/36450]\tLoss: 418.0008\n",
      "Training Epoch: 6 [16650/36450]\tLoss: 421.9319\n",
      "Training Epoch: 6 [16700/36450]\tLoss: 399.0330\n",
      "Training Epoch: 6 [16750/36450]\tLoss: 443.7563\n",
      "Training Epoch: 6 [16800/36450]\tLoss: 448.4984\n",
      "Training Epoch: 6 [16850/36450]\tLoss: 395.0367\n",
      "Training Epoch: 6 [16900/36450]\tLoss: 429.2796\n",
      "Training Epoch: 6 [16950/36450]\tLoss: 486.3212\n",
      "Training Epoch: 6 [17000/36450]\tLoss: 431.7947\n",
      "Training Epoch: 6 [17050/36450]\tLoss: 379.4818\n",
      "Training Epoch: 6 [17100/36450]\tLoss: 492.7111\n",
      "Training Epoch: 6 [17150/36450]\tLoss: 426.7264\n",
      "Training Epoch: 6 [17200/36450]\tLoss: 409.1202\n",
      "Training Epoch: 6 [17250/36450]\tLoss: 434.3357\n",
      "Training Epoch: 6 [17300/36450]\tLoss: 404.2776\n",
      "Training Epoch: 6 [17350/36450]\tLoss: 424.0628\n",
      "Training Epoch: 6 [17400/36450]\tLoss: 410.3516\n",
      "Training Epoch: 6 [17450/36450]\tLoss: 455.8752\n",
      "Training Epoch: 6 [17500/36450]\tLoss: 413.0233\n",
      "Training Epoch: 6 [17550/36450]\tLoss: 441.3890\n",
      "Training Epoch: 6 [17600/36450]\tLoss: 460.0163\n",
      "Training Epoch: 6 [17650/36450]\tLoss: 440.1676\n",
      "Training Epoch: 6 [17700/36450]\tLoss: 413.4130\n",
      "Training Epoch: 6 [17750/36450]\tLoss: 446.2897\n",
      "Training Epoch: 6 [17800/36450]\tLoss: 408.8304\n",
      "Training Epoch: 6 [17850/36450]\tLoss: 402.6959\n",
      "Training Epoch: 6 [17900/36450]\tLoss: 427.3205\n",
      "Training Epoch: 6 [17950/36450]\tLoss: 455.4518\n",
      "Training Epoch: 6 [18000/36450]\tLoss: 394.2514\n",
      "Training Epoch: 6 [18050/36450]\tLoss: 402.9066\n",
      "Training Epoch: 6 [18100/36450]\tLoss: 435.2946\n",
      "Training Epoch: 6 [18150/36450]\tLoss: 417.7049\n",
      "Training Epoch: 6 [18200/36450]\tLoss: 421.8416\n",
      "Training Epoch: 6 [18250/36450]\tLoss: 445.7306\n",
      "Training Epoch: 6 [18300/36450]\tLoss: 438.8672\n",
      "Training Epoch: 6 [18350/36450]\tLoss: 459.9252\n",
      "Training Epoch: 6 [18400/36450]\tLoss: 400.0064\n",
      "Training Epoch: 6 [18450/36450]\tLoss: 406.5553\n",
      "Training Epoch: 6 [18500/36450]\tLoss: 416.6099\n",
      "Training Epoch: 6 [18550/36450]\tLoss: 470.5716\n",
      "Training Epoch: 6 [18600/36450]\tLoss: 402.4500\n",
      "Training Epoch: 6 [18650/36450]\tLoss: 398.8160\n",
      "Training Epoch: 6 [18700/36450]\tLoss: 434.7538\n",
      "Training Epoch: 6 [18750/36450]\tLoss: 416.8466\n",
      "Training Epoch: 6 [18800/36450]\tLoss: 456.5165\n",
      "Training Epoch: 6 [18850/36450]\tLoss: 397.6278\n",
      "Training Epoch: 6 [18900/36450]\tLoss: 421.4938\n",
      "Training Epoch: 6 [18950/36450]\tLoss: 405.9215\n",
      "Training Epoch: 6 [19000/36450]\tLoss: 404.8113\n",
      "Training Epoch: 6 [19050/36450]\tLoss: 419.2046\n",
      "Training Epoch: 6 [19100/36450]\tLoss: 428.5463\n",
      "Training Epoch: 6 [19150/36450]\tLoss: 411.1881\n",
      "Training Epoch: 6 [19200/36450]\tLoss: 411.3921\n",
      "Training Epoch: 6 [19250/36450]\tLoss: 398.6924\n",
      "Training Epoch: 6 [19300/36450]\tLoss: 404.4712\n",
      "Training Epoch: 6 [19350/36450]\tLoss: 411.5051\n",
      "Training Epoch: 6 [19400/36450]\tLoss: 456.5097\n",
      "Training Epoch: 6 [19450/36450]\tLoss: 443.7201\n",
      "Training Epoch: 6 [19500/36450]\tLoss: 431.0435\n",
      "Training Epoch: 6 [19550/36450]\tLoss: 442.1795\n",
      "Training Epoch: 6 [19600/36450]\tLoss: 420.7417\n",
      "Training Epoch: 6 [19650/36450]\tLoss: 445.1827\n",
      "Training Epoch: 6 [19700/36450]\tLoss: 439.6195\n",
      "Training Epoch: 6 [19750/36450]\tLoss: 430.6032\n",
      "Training Epoch: 6 [19800/36450]\tLoss: 452.1906\n",
      "Training Epoch: 6 [19850/36450]\tLoss: 439.7177\n",
      "Training Epoch: 6 [19900/36450]\tLoss: 414.6252\n",
      "Training Epoch: 6 [19950/36450]\tLoss: 374.7688\n",
      "Training Epoch: 6 [20000/36450]\tLoss: 452.4786\n",
      "Training Epoch: 6 [20050/36450]\tLoss: 471.9959\n",
      "Training Epoch: 6 [20100/36450]\tLoss: 475.3437\n",
      "Training Epoch: 6 [20150/36450]\tLoss: 422.6713\n",
      "Training Epoch: 6 [20200/36450]\tLoss: 408.3490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [20250/36450]\tLoss: 392.6855\n",
      "Training Epoch: 6 [20300/36450]\tLoss: 417.0544\n",
      "Training Epoch: 6 [20350/36450]\tLoss: 379.1357\n",
      "Training Epoch: 6 [20400/36450]\tLoss: 451.0346\n",
      "Training Epoch: 6 [20450/36450]\tLoss: 414.2039\n",
      "Training Epoch: 6 [20500/36450]\tLoss: 401.0368\n",
      "Training Epoch: 6 [20550/36450]\tLoss: 435.8230\n",
      "Training Epoch: 6 [20600/36450]\tLoss: 449.8896\n",
      "Training Epoch: 6 [20650/36450]\tLoss: 431.5140\n",
      "Training Epoch: 6 [20700/36450]\tLoss: 397.2414\n",
      "Training Epoch: 6 [20750/36450]\tLoss: 428.4016\n",
      "Training Epoch: 6 [20800/36450]\tLoss: 382.6086\n",
      "Training Epoch: 6 [20850/36450]\tLoss: 463.9469\n",
      "Training Epoch: 6 [20900/36450]\tLoss: 430.9799\n",
      "Training Epoch: 6 [20950/36450]\tLoss: 421.2302\n",
      "Training Epoch: 6 [21000/36450]\tLoss: 407.4564\n",
      "Training Epoch: 6 [21050/36450]\tLoss: 423.0901\n",
      "Training Epoch: 6 [21100/36450]\tLoss: 475.7414\n",
      "Training Epoch: 6 [21150/36450]\tLoss: 467.7596\n",
      "Training Epoch: 6 [21200/36450]\tLoss: 412.9507\n",
      "Training Epoch: 6 [21250/36450]\tLoss: 428.0723\n",
      "Training Epoch: 6 [21300/36450]\tLoss: 418.0817\n",
      "Training Epoch: 6 [21350/36450]\tLoss: 417.1039\n",
      "Training Epoch: 6 [21400/36450]\tLoss: 439.4819\n",
      "Training Epoch: 6 [21450/36450]\tLoss: 426.8344\n",
      "Training Epoch: 6 [21500/36450]\tLoss: 433.1901\n",
      "Training Epoch: 6 [21550/36450]\tLoss: 457.0547\n",
      "Training Epoch: 6 [21600/36450]\tLoss: 401.3152\n",
      "Training Epoch: 6 [21650/36450]\tLoss: 423.9237\n",
      "Training Epoch: 6 [21700/36450]\tLoss: 432.7507\n",
      "Training Epoch: 6 [21750/36450]\tLoss: 443.8513\n",
      "Training Epoch: 6 [21800/36450]\tLoss: 439.4499\n",
      "Training Epoch: 6 [21850/36450]\tLoss: 417.6482\n",
      "Training Epoch: 6 [21900/36450]\tLoss: 417.4559\n",
      "Training Epoch: 6 [21950/36450]\tLoss: 457.5554\n",
      "Training Epoch: 6 [22000/36450]\tLoss: 393.0061\n",
      "Training Epoch: 6 [22050/36450]\tLoss: 414.7493\n",
      "Training Epoch: 6 [22100/36450]\tLoss: 431.8923\n",
      "Training Epoch: 6 [22150/36450]\tLoss: 435.5451\n",
      "Training Epoch: 6 [22200/36450]\tLoss: 430.6178\n",
      "Training Epoch: 6 [22250/36450]\tLoss: 458.3336\n",
      "Training Epoch: 6 [22300/36450]\tLoss: 380.7362\n",
      "Training Epoch: 6 [22350/36450]\tLoss: 422.4571\n",
      "Training Epoch: 6 [22400/36450]\tLoss: 401.5608\n",
      "Training Epoch: 6 [22450/36450]\tLoss: 400.6199\n",
      "Training Epoch: 6 [22500/36450]\tLoss: 437.4010\n",
      "Training Epoch: 6 [22550/36450]\tLoss: 412.8440\n",
      "Training Epoch: 6 [22600/36450]\tLoss: 390.5951\n",
      "Training Epoch: 6 [22650/36450]\tLoss: 432.2611\n",
      "Training Epoch: 6 [22700/36450]\tLoss: 404.2682\n",
      "Training Epoch: 6 [22750/36450]\tLoss: 422.9995\n",
      "Training Epoch: 6 [22800/36450]\tLoss: 408.5037\n",
      "Training Epoch: 6 [22850/36450]\tLoss: 410.3081\n",
      "Training Epoch: 6 [22900/36450]\tLoss: 393.7116\n",
      "Training Epoch: 6 [22950/36450]\tLoss: 406.1615\n",
      "Training Epoch: 6 [23000/36450]\tLoss: 454.4561\n",
      "Training Epoch: 6 [23050/36450]\tLoss: 424.4059\n",
      "Training Epoch: 6 [23100/36450]\tLoss: 425.3668\n",
      "Training Epoch: 6 [23150/36450]\tLoss: 427.3816\n",
      "Training Epoch: 6 [23200/36450]\tLoss: 427.6727\n",
      "Training Epoch: 6 [23250/36450]\tLoss: 409.1848\n",
      "Training Epoch: 6 [23300/36450]\tLoss: 376.7498\n",
      "Training Epoch: 6 [23350/36450]\tLoss: 418.2993\n",
      "Training Epoch: 6 [23400/36450]\tLoss: 465.3376\n",
      "Training Epoch: 6 [23450/36450]\tLoss: 404.0086\n",
      "Training Epoch: 6 [23500/36450]\tLoss: 432.6972\n",
      "Training Epoch: 6 [23550/36450]\tLoss: 429.5522\n",
      "Training Epoch: 6 [23600/36450]\tLoss: 458.3784\n",
      "Training Epoch: 6 [23650/36450]\tLoss: 438.0898\n",
      "Training Epoch: 6 [23700/36450]\tLoss: 441.6804\n",
      "Training Epoch: 6 [23750/36450]\tLoss: 436.2377\n",
      "Training Epoch: 6 [23800/36450]\tLoss: 406.3014\n",
      "Training Epoch: 6 [23850/36450]\tLoss: 431.1342\n",
      "Training Epoch: 6 [23900/36450]\tLoss: 384.4646\n",
      "Training Epoch: 6 [23950/36450]\tLoss: 415.8764\n",
      "Training Epoch: 6 [24000/36450]\tLoss: 416.4262\n",
      "Training Epoch: 6 [24050/36450]\tLoss: 445.7420\n",
      "Training Epoch: 6 [24100/36450]\tLoss: 379.8852\n",
      "Training Epoch: 6 [24150/36450]\tLoss: 400.8191\n",
      "Training Epoch: 6 [24200/36450]\tLoss: 420.4284\n",
      "Training Epoch: 6 [24250/36450]\tLoss: 485.5313\n",
      "Training Epoch: 6 [24300/36450]\tLoss: 409.0666\n",
      "Training Epoch: 6 [24350/36450]\tLoss: 399.3080\n",
      "Training Epoch: 6 [24400/36450]\tLoss: 418.5435\n",
      "Training Epoch: 6 [24450/36450]\tLoss: 423.0585\n",
      "Training Epoch: 6 [24500/36450]\tLoss: 392.9801\n",
      "Training Epoch: 6 [24550/36450]\tLoss: 380.3937\n",
      "Training Epoch: 6 [24600/36450]\tLoss: 460.4144\n",
      "Training Epoch: 6 [24650/36450]\tLoss: 386.1674\n",
      "Training Epoch: 6 [24700/36450]\tLoss: 465.0938\n",
      "Training Epoch: 6 [24750/36450]\tLoss: 402.0604\n",
      "Training Epoch: 6 [24800/36450]\tLoss: 377.1010\n",
      "Training Epoch: 6 [24850/36450]\tLoss: 422.7681\n",
      "Training Epoch: 6 [24900/36450]\tLoss: 389.5447\n",
      "Training Epoch: 6 [24950/36450]\tLoss: 440.3491\n",
      "Training Epoch: 6 [25000/36450]\tLoss: 447.0480\n",
      "Training Epoch: 6 [25050/36450]\tLoss: 425.5398\n",
      "Training Epoch: 6 [25100/36450]\tLoss: 470.4499\n",
      "Training Epoch: 6 [25150/36450]\tLoss: 397.9297\n",
      "Training Epoch: 6 [25200/36450]\tLoss: 401.9512\n",
      "Training Epoch: 6 [25250/36450]\tLoss: 392.7304\n",
      "Training Epoch: 6 [25300/36450]\tLoss: 447.1875\n",
      "Training Epoch: 6 [25350/36450]\tLoss: 386.0635\n",
      "Training Epoch: 6 [25400/36450]\tLoss: 425.2977\n",
      "Training Epoch: 6 [25450/36450]\tLoss: 407.0381\n",
      "Training Epoch: 6 [25500/36450]\tLoss: 442.4581\n",
      "Training Epoch: 6 [25550/36450]\tLoss: 433.8179\n",
      "Training Epoch: 6 [25600/36450]\tLoss: 466.7022\n",
      "Training Epoch: 6 [25650/36450]\tLoss: 403.9211\n",
      "Training Epoch: 6 [25700/36450]\tLoss: 469.9608\n",
      "Training Epoch: 6 [25750/36450]\tLoss: 409.9199\n",
      "Training Epoch: 6 [25800/36450]\tLoss: 437.7307\n",
      "Training Epoch: 6 [25850/36450]\tLoss: 414.1508\n",
      "Training Epoch: 6 [25900/36450]\tLoss: 400.5179\n",
      "Training Epoch: 6 [25950/36450]\tLoss: 378.1570\n",
      "Training Epoch: 6 [26000/36450]\tLoss: 449.7576\n",
      "Training Epoch: 6 [26050/36450]\tLoss: 405.1953\n",
      "Training Epoch: 6 [26100/36450]\tLoss: 417.1660\n",
      "Training Epoch: 6 [26150/36450]\tLoss: 409.3932\n",
      "Training Epoch: 6 [26200/36450]\tLoss: 445.7294\n",
      "Training Epoch: 6 [26250/36450]\tLoss: 416.9090\n",
      "Training Epoch: 6 [26300/36450]\tLoss: 414.1400\n",
      "Training Epoch: 6 [26350/36450]\tLoss: 392.6004\n",
      "Training Epoch: 6 [26400/36450]\tLoss: 409.7250\n",
      "Training Epoch: 6 [26450/36450]\tLoss: 404.4257\n",
      "Training Epoch: 6 [26500/36450]\tLoss: 401.7026\n",
      "Training Epoch: 6 [26550/36450]\tLoss: 415.8976\n",
      "Training Epoch: 6 [26600/36450]\tLoss: 388.8529\n",
      "Training Epoch: 6 [26650/36450]\tLoss: 412.5338\n",
      "Training Epoch: 6 [26700/36450]\tLoss: 436.9549\n",
      "Training Epoch: 6 [26750/36450]\tLoss: 398.9695\n",
      "Training Epoch: 6 [26800/36450]\tLoss: 429.3340\n",
      "Training Epoch: 6 [26850/36450]\tLoss: 428.5682\n",
      "Training Epoch: 6 [26900/36450]\tLoss: 426.5480\n",
      "Training Epoch: 6 [26950/36450]\tLoss: 427.6644\n",
      "Training Epoch: 6 [27000/36450]\tLoss: 379.5815\n",
      "Training Epoch: 6 [27050/36450]\tLoss: 426.6924\n",
      "Training Epoch: 6 [27100/36450]\tLoss: 418.0732\n",
      "Training Epoch: 6 [27150/36450]\tLoss: 393.1499\n",
      "Training Epoch: 6 [27200/36450]\tLoss: 424.2697\n",
      "Training Epoch: 6 [27250/36450]\tLoss: 448.7142\n",
      "Training Epoch: 6 [27300/36450]\tLoss: 421.3600\n",
      "Training Epoch: 6 [27350/36450]\tLoss: 448.1133\n",
      "Training Epoch: 6 [27400/36450]\tLoss: 425.1305\n",
      "Training Epoch: 6 [27450/36450]\tLoss: 372.3364\n",
      "Training Epoch: 6 [27500/36450]\tLoss: 411.9042\n",
      "Training Epoch: 6 [27550/36450]\tLoss: 435.6277\n",
      "Training Epoch: 6 [27600/36450]\tLoss: 416.1938\n",
      "Training Epoch: 6 [27650/36450]\tLoss: 374.9543\n",
      "Training Epoch: 6 [27700/36450]\tLoss: 378.9273\n",
      "Training Epoch: 6 [27750/36450]\tLoss: 394.6034\n",
      "Training Epoch: 6 [27800/36450]\tLoss: 384.0037\n",
      "Training Epoch: 6 [27850/36450]\tLoss: 431.7527\n",
      "Training Epoch: 6 [27900/36450]\tLoss: 369.6039\n",
      "Training Epoch: 6 [27950/36450]\tLoss: 380.7079\n",
      "Training Epoch: 6 [28000/36450]\tLoss: 385.2138\n",
      "Training Epoch: 6 [28050/36450]\tLoss: 456.6999\n",
      "Training Epoch: 6 [28100/36450]\tLoss: 447.5555\n",
      "Training Epoch: 6 [28150/36450]\tLoss: 388.8667\n",
      "Training Epoch: 6 [28200/36450]\tLoss: 400.3920\n",
      "Training Epoch: 6 [28250/36450]\tLoss: 466.5567\n",
      "Training Epoch: 6 [28300/36450]\tLoss: 410.8484\n",
      "Training Epoch: 6 [28350/36450]\tLoss: 400.4203\n",
      "Training Epoch: 6 [28400/36450]\tLoss: 420.7583\n",
      "Training Epoch: 6 [28450/36450]\tLoss: 428.4718\n",
      "Training Epoch: 6 [28500/36450]\tLoss: 425.8256\n",
      "Training Epoch: 6 [28550/36450]\tLoss: 412.1212\n",
      "Training Epoch: 6 [28600/36450]\tLoss: 410.7075\n",
      "Training Epoch: 6 [28650/36450]\tLoss: 406.9618\n",
      "Training Epoch: 6 [28700/36450]\tLoss: 415.6653\n",
      "Training Epoch: 6 [28750/36450]\tLoss: 411.5388\n",
      "Training Epoch: 6 [28800/36450]\tLoss: 411.9025\n",
      "Training Epoch: 6 [28850/36450]\tLoss: 430.4705\n",
      "Training Epoch: 6 [28900/36450]\tLoss: 398.7130\n",
      "Training Epoch: 6 [28950/36450]\tLoss: 391.7216\n",
      "Training Epoch: 6 [29000/36450]\tLoss: 379.3465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [29050/36450]\tLoss: 414.2108\n",
      "Training Epoch: 6 [29100/36450]\tLoss: 371.8033\n",
      "Training Epoch: 6 [29150/36450]\tLoss: 441.5775\n",
      "Training Epoch: 6 [29200/36450]\tLoss: 415.3896\n",
      "Training Epoch: 6 [29250/36450]\tLoss: 370.4520\n",
      "Training Epoch: 6 [29300/36450]\tLoss: 465.8071\n",
      "Training Epoch: 6 [29350/36450]\tLoss: 437.1538\n",
      "Training Epoch: 6 [29400/36450]\tLoss: 376.1667\n",
      "Training Epoch: 6 [29450/36450]\tLoss: 422.1618\n",
      "Training Epoch: 6 [29500/36450]\tLoss: 402.6295\n",
      "Training Epoch: 6 [29550/36450]\tLoss: 428.9107\n",
      "Training Epoch: 6 [29600/36450]\tLoss: 406.2754\n",
      "Training Epoch: 6 [29650/36450]\tLoss: 418.5730\n",
      "Training Epoch: 6 [29700/36450]\tLoss: 423.3619\n",
      "Training Epoch: 6 [29750/36450]\tLoss: 464.9923\n",
      "Training Epoch: 6 [29800/36450]\tLoss: 391.2083\n",
      "Training Epoch: 6 [29850/36450]\tLoss: 392.1138\n",
      "Training Epoch: 6 [29900/36450]\tLoss: 453.8855\n",
      "Training Epoch: 6 [29950/36450]\tLoss: 440.0396\n",
      "Training Epoch: 6 [30000/36450]\tLoss: 439.5709\n",
      "Training Epoch: 6 [30050/36450]\tLoss: 421.5497\n",
      "Training Epoch: 6 [30100/36450]\tLoss: 430.1681\n",
      "Training Epoch: 6 [30150/36450]\tLoss: 414.2078\n",
      "Training Epoch: 6 [30200/36450]\tLoss: 399.4011\n",
      "Training Epoch: 6 [30250/36450]\tLoss: 415.8896\n",
      "Training Epoch: 6 [30300/36450]\tLoss: 415.3442\n",
      "Training Epoch: 6 [30350/36450]\tLoss: 435.7949\n",
      "Training Epoch: 6 [30400/36450]\tLoss: 400.9239\n",
      "Training Epoch: 6 [30450/36450]\tLoss: 369.5386\n",
      "Training Epoch: 6 [30500/36450]\tLoss: 385.2271\n",
      "Training Epoch: 6 [30550/36450]\tLoss: 447.8296\n",
      "Training Epoch: 6 [30600/36450]\tLoss: 429.7824\n",
      "Training Epoch: 6 [30650/36450]\tLoss: 410.6772\n",
      "Training Epoch: 6 [30700/36450]\tLoss: 404.5958\n",
      "Training Epoch: 6 [30750/36450]\tLoss: 388.9475\n",
      "Training Epoch: 6 [30800/36450]\tLoss: 451.7985\n",
      "Training Epoch: 6 [30850/36450]\tLoss: 369.2906\n",
      "Training Epoch: 6 [30900/36450]\tLoss: 399.8239\n",
      "Training Epoch: 6 [30950/36450]\tLoss: 362.8343\n",
      "Training Epoch: 6 [31000/36450]\tLoss: 396.0064\n",
      "Training Epoch: 6 [31050/36450]\tLoss: 452.1453\n",
      "Training Epoch: 6 [31100/36450]\tLoss: 408.9644\n",
      "Training Epoch: 6 [31150/36450]\tLoss: 444.8578\n",
      "Training Epoch: 6 [31200/36450]\tLoss: 416.8358\n",
      "Training Epoch: 6 [31250/36450]\tLoss: 409.8424\n",
      "Training Epoch: 6 [31300/36450]\tLoss: 392.5948\n",
      "Training Epoch: 6 [31350/36450]\tLoss: 392.2481\n",
      "Training Epoch: 6 [31400/36450]\tLoss: 398.4434\n",
      "Training Epoch: 6 [31450/36450]\tLoss: 404.0678\n",
      "Training Epoch: 6 [31500/36450]\tLoss: 432.8018\n",
      "Training Epoch: 6 [31550/36450]\tLoss: 399.3077\n",
      "Training Epoch: 6 [31600/36450]\tLoss: 449.0400\n",
      "Training Epoch: 6 [31650/36450]\tLoss: 374.9901\n",
      "Training Epoch: 6 [31700/36450]\tLoss: 442.3290\n",
      "Training Epoch: 6 [31750/36450]\tLoss: 437.9620\n",
      "Training Epoch: 6 [31800/36450]\tLoss: 393.5551\n",
      "Training Epoch: 6 [31850/36450]\tLoss: 392.9726\n",
      "Training Epoch: 6 [31900/36450]\tLoss: 458.0510\n",
      "Training Epoch: 6 [31950/36450]\tLoss: 453.8656\n",
      "Training Epoch: 6 [32000/36450]\tLoss: 385.8114\n",
      "Training Epoch: 6 [32050/36450]\tLoss: 378.4389\n",
      "Training Epoch: 6 [32100/36450]\tLoss: 375.5846\n",
      "Training Epoch: 6 [32150/36450]\tLoss: 445.8371\n",
      "Training Epoch: 6 [32200/36450]\tLoss: 395.1136\n",
      "Training Epoch: 6 [32250/36450]\tLoss: 403.8290\n",
      "Training Epoch: 6 [32300/36450]\tLoss: 382.5722\n",
      "Training Epoch: 6 [32350/36450]\tLoss: 432.2880\n",
      "Training Epoch: 6 [32400/36450]\tLoss: 440.6924\n",
      "Training Epoch: 6 [32450/36450]\tLoss: 410.1114\n",
      "Training Epoch: 6 [32500/36450]\tLoss: 460.6011\n",
      "Training Epoch: 6 [32550/36450]\tLoss: 398.1950\n",
      "Training Epoch: 6 [32600/36450]\tLoss: 416.6248\n",
      "Training Epoch: 6 [32650/36450]\tLoss: 377.9166\n",
      "Training Epoch: 6 [32700/36450]\tLoss: 374.9521\n",
      "Training Epoch: 6 [32750/36450]\tLoss: 416.3713\n",
      "Training Epoch: 6 [32800/36450]\tLoss: 357.2250\n",
      "Training Epoch: 6 [32850/36450]\tLoss: 399.8348\n",
      "Training Epoch: 6 [32900/36450]\tLoss: 379.8928\n",
      "Training Epoch: 6 [32950/36450]\tLoss: 426.7756\n",
      "Training Epoch: 6 [33000/36450]\tLoss: 438.5704\n",
      "Training Epoch: 6 [33050/36450]\tLoss: 408.1741\n",
      "Training Epoch: 6 [33100/36450]\tLoss: 366.9976\n",
      "Training Epoch: 6 [33150/36450]\tLoss: 407.9156\n",
      "Training Epoch: 6 [33200/36450]\tLoss: 441.5402\n",
      "Training Epoch: 6 [33250/36450]\tLoss: 423.3347\n",
      "Training Epoch: 6 [33300/36450]\tLoss: 408.7343\n",
      "Training Epoch: 6 [33350/36450]\tLoss: 381.2376\n",
      "Training Epoch: 6 [33400/36450]\tLoss: 421.2309\n",
      "Training Epoch: 6 [33450/36450]\tLoss: 381.3350\n",
      "Training Epoch: 6 [33500/36450]\tLoss: 434.8217\n",
      "Training Epoch: 6 [33550/36450]\tLoss: 408.6975\n",
      "Training Epoch: 6 [33600/36450]\tLoss: 349.4610\n",
      "Training Epoch: 6 [33650/36450]\tLoss: 452.5769\n",
      "Training Epoch: 6 [33700/36450]\tLoss: 438.9679\n",
      "Training Epoch: 6 [33750/36450]\tLoss: 425.1478\n",
      "Training Epoch: 6 [33800/36450]\tLoss: 425.4035\n",
      "Training Epoch: 6 [33850/36450]\tLoss: 392.6871\n",
      "Training Epoch: 6 [33900/36450]\tLoss: 451.2720\n",
      "Training Epoch: 6 [33950/36450]\tLoss: 391.4355\n",
      "Training Epoch: 6 [34000/36450]\tLoss: 394.3450\n",
      "Training Epoch: 6 [34050/36450]\tLoss: 392.0751\n",
      "Training Epoch: 6 [34100/36450]\tLoss: 403.1680\n",
      "Training Epoch: 6 [34150/36450]\tLoss: 394.9740\n",
      "Training Epoch: 6 [34200/36450]\tLoss: 426.7512\n",
      "Training Epoch: 6 [34250/36450]\tLoss: 425.0137\n",
      "Training Epoch: 6 [34300/36450]\tLoss: 438.3508\n",
      "Training Epoch: 6 [34350/36450]\tLoss: 430.8345\n",
      "Training Epoch: 6 [34400/36450]\tLoss: 420.2329\n",
      "Training Epoch: 6 [34450/36450]\tLoss: 435.2112\n",
      "Training Epoch: 6 [34500/36450]\tLoss: 447.7068\n",
      "Training Epoch: 6 [34550/36450]\tLoss: 418.2313\n",
      "Training Epoch: 6 [34600/36450]\tLoss: 411.6172\n",
      "Training Epoch: 6 [34650/36450]\tLoss: 358.2291\n",
      "Training Epoch: 6 [34700/36450]\tLoss: 408.7215\n",
      "Training Epoch: 6 [34750/36450]\tLoss: 411.9171\n",
      "Training Epoch: 6 [34800/36450]\tLoss: 403.4380\n",
      "Training Epoch: 6 [34850/36450]\tLoss: 422.1740\n",
      "Training Epoch: 6 [34900/36450]\tLoss: 388.1106\n",
      "Training Epoch: 6 [34950/36450]\tLoss: 456.8273\n",
      "Training Epoch: 6 [35000/36450]\tLoss: 438.6375\n",
      "Training Epoch: 6 [35050/36450]\tLoss: 371.6433\n",
      "Training Epoch: 6 [35100/36450]\tLoss: 437.5962\n",
      "Training Epoch: 6 [35150/36450]\tLoss: 419.9378\n",
      "Training Epoch: 6 [35200/36450]\tLoss: 368.8098\n",
      "Training Epoch: 6 [35250/36450]\tLoss: 438.2711\n",
      "Training Epoch: 6 [35300/36450]\tLoss: 406.6214\n",
      "Training Epoch: 6 [35350/36450]\tLoss: 375.8437\n",
      "Training Epoch: 6 [35400/36450]\tLoss: 401.8885\n",
      "Training Epoch: 6 [35450/36450]\tLoss: 395.7411\n",
      "Training Epoch: 6 [35500/36450]\tLoss: 441.5262\n",
      "Training Epoch: 6 [35550/36450]\tLoss: 421.6487\n",
      "Training Epoch: 6 [35600/36450]\tLoss: 399.1642\n",
      "Training Epoch: 6 [35650/36450]\tLoss: 367.9953\n",
      "Training Epoch: 6 [35700/36450]\tLoss: 423.6966\n",
      "Training Epoch: 6 [35750/36450]\tLoss: 378.5476\n",
      "Training Epoch: 6 [35800/36450]\tLoss: 428.7885\n",
      "Training Epoch: 6 [35850/36450]\tLoss: 392.1918\n",
      "Training Epoch: 6 [35900/36450]\tLoss: 373.0821\n",
      "Training Epoch: 6 [35950/36450]\tLoss: 402.5703\n",
      "Training Epoch: 6 [36000/36450]\tLoss: 437.4373\n",
      "Training Epoch: 6 [36050/36450]\tLoss: 390.3663\n",
      "Training Epoch: 6 [36100/36450]\tLoss: 401.9687\n",
      "Training Epoch: 6 [36150/36450]\tLoss: 408.8997\n",
      "Training Epoch: 6 [36200/36450]\tLoss: 414.7579\n",
      "Training Epoch: 6 [36250/36450]\tLoss: 425.3613\n",
      "Training Epoch: 6 [36300/36450]\tLoss: 406.4315\n",
      "Training Epoch: 6 [36350/36450]\tLoss: 399.8296\n",
      "Training Epoch: 6 [36400/36450]\tLoss: 437.2920\n",
      "Training Epoch: 6 [36450/36450]\tLoss: 392.8326\n",
      "Training Epoch: 6 [4050/4050]\tLoss: 187.0337\n",
      "Training Epoch: 7 [50/36450]\tLoss: 403.5636\n",
      "Training Epoch: 7 [100/36450]\tLoss: 376.4373\n",
      "Training Epoch: 7 [150/36450]\tLoss: 411.8820\n",
      "Training Epoch: 7 [200/36450]\tLoss: 387.7299\n",
      "Training Epoch: 7 [250/36450]\tLoss: 397.9133\n",
      "Training Epoch: 7 [300/36450]\tLoss: 388.4703\n",
      "Training Epoch: 7 [350/36450]\tLoss: 407.7877\n",
      "Training Epoch: 7 [400/36450]\tLoss: 379.1766\n",
      "Training Epoch: 7 [450/36450]\tLoss: 380.1709\n",
      "Training Epoch: 7 [500/36450]\tLoss: 382.6298\n",
      "Training Epoch: 7 [550/36450]\tLoss: 372.0381\n",
      "Training Epoch: 7 [600/36450]\tLoss: 381.7450\n",
      "Training Epoch: 7 [650/36450]\tLoss: 387.1136\n",
      "Training Epoch: 7 [700/36450]\tLoss: 394.6059\n",
      "Training Epoch: 7 [750/36450]\tLoss: 413.0372\n",
      "Training Epoch: 7 [800/36450]\tLoss: 405.3699\n",
      "Training Epoch: 7 [850/36450]\tLoss: 403.3299\n",
      "Training Epoch: 7 [900/36450]\tLoss: 432.7622\n",
      "Training Epoch: 7 [950/36450]\tLoss: 424.0285\n",
      "Training Epoch: 7 [1000/36450]\tLoss: 385.0327\n",
      "Training Epoch: 7 [1050/36450]\tLoss: 408.2825\n",
      "Training Epoch: 7 [1100/36450]\tLoss: 393.0726\n",
      "Training Epoch: 7 [1150/36450]\tLoss: 380.7627\n",
      "Training Epoch: 7 [1200/36450]\tLoss: 405.2851\n",
      "Training Epoch: 7 [1250/36450]\tLoss: 394.1382\n",
      "Training Epoch: 7 [1300/36450]\tLoss: 383.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [1350/36450]\tLoss: 384.7439\n",
      "Training Epoch: 7 [1400/36450]\tLoss: 399.5938\n",
      "Training Epoch: 7 [1450/36450]\tLoss: 403.0667\n",
      "Training Epoch: 7 [1500/36450]\tLoss: 372.4757\n",
      "Training Epoch: 7 [1550/36450]\tLoss: 441.1446\n",
      "Training Epoch: 7 [1600/36450]\tLoss: 370.3954\n",
      "Training Epoch: 7 [1650/36450]\tLoss: 416.9245\n",
      "Training Epoch: 7 [1700/36450]\tLoss: 426.6973\n",
      "Training Epoch: 7 [1750/36450]\tLoss: 422.1532\n",
      "Training Epoch: 7 [1800/36450]\tLoss: 417.5605\n",
      "Training Epoch: 7 [1850/36450]\tLoss: 372.5472\n",
      "Training Epoch: 7 [1900/36450]\tLoss: 397.6892\n",
      "Training Epoch: 7 [1950/36450]\tLoss: 406.4782\n",
      "Training Epoch: 7 [2000/36450]\tLoss: 422.5656\n",
      "Training Epoch: 7 [2050/36450]\tLoss: 424.6010\n",
      "Training Epoch: 7 [2100/36450]\tLoss: 397.4384\n",
      "Training Epoch: 7 [2150/36450]\tLoss: 373.5615\n",
      "Training Epoch: 7 [2200/36450]\tLoss: 465.1556\n",
      "Training Epoch: 7 [2250/36450]\tLoss: 387.2059\n",
      "Training Epoch: 7 [2300/36450]\tLoss: 395.3919\n",
      "Training Epoch: 7 [2350/36450]\tLoss: 407.4547\n",
      "Training Epoch: 7 [2400/36450]\tLoss: 445.7024\n",
      "Training Epoch: 7 [2450/36450]\tLoss: 436.5615\n",
      "Training Epoch: 7 [2500/36450]\tLoss: 376.3706\n",
      "Training Epoch: 7 [2550/36450]\tLoss: 404.9314\n",
      "Training Epoch: 7 [2600/36450]\tLoss: 385.9536\n",
      "Training Epoch: 7 [2650/36450]\tLoss: 378.6875\n",
      "Training Epoch: 7 [2700/36450]\tLoss: 375.4030\n",
      "Training Epoch: 7 [2750/36450]\tLoss: 408.3041\n",
      "Training Epoch: 7 [2800/36450]\tLoss: 411.8044\n",
      "Training Epoch: 7 [2850/36450]\tLoss: 373.0128\n",
      "Training Epoch: 7 [2900/36450]\tLoss: 396.4281\n",
      "Training Epoch: 7 [2950/36450]\tLoss: 429.6702\n",
      "Training Epoch: 7 [3000/36450]\tLoss: 409.1904\n",
      "Training Epoch: 7 [3050/36450]\tLoss: 380.3967\n",
      "Training Epoch: 7 [3100/36450]\tLoss: 411.4475\n",
      "Training Epoch: 7 [3150/36450]\tLoss: 373.7415\n",
      "Training Epoch: 7 [3200/36450]\tLoss: 373.4177\n",
      "Training Epoch: 7 [3250/36450]\tLoss: 365.9782\n",
      "Training Epoch: 7 [3300/36450]\tLoss: 386.3872\n",
      "Training Epoch: 7 [3350/36450]\tLoss: 418.9959\n",
      "Training Epoch: 7 [3400/36450]\tLoss: 422.3876\n",
      "Training Epoch: 7 [3450/36450]\tLoss: 387.8671\n",
      "Training Epoch: 7 [3500/36450]\tLoss: 419.5544\n",
      "Training Epoch: 7 [3550/36450]\tLoss: 426.3057\n",
      "Training Epoch: 7 [3600/36450]\tLoss: 418.2274\n",
      "Training Epoch: 7 [3650/36450]\tLoss: 429.7437\n",
      "Training Epoch: 7 [3700/36450]\tLoss: 401.8418\n",
      "Training Epoch: 7 [3750/36450]\tLoss: 407.8885\n",
      "Training Epoch: 7 [3800/36450]\tLoss: 394.9531\n",
      "Training Epoch: 7 [3850/36450]\tLoss: 386.9740\n",
      "Training Epoch: 7 [3900/36450]\tLoss: 420.9091\n",
      "Training Epoch: 7 [3950/36450]\tLoss: 368.4696\n",
      "Training Epoch: 7 [4000/36450]\tLoss: 393.6014\n",
      "Training Epoch: 7 [4050/36450]\tLoss: 369.4810\n",
      "Training Epoch: 7 [4100/36450]\tLoss: 417.1422\n",
      "Training Epoch: 7 [4150/36450]\tLoss: 426.2173\n",
      "Training Epoch: 7 [4200/36450]\tLoss: 392.8578\n",
      "Training Epoch: 7 [4250/36450]\tLoss: 399.6862\n",
      "Training Epoch: 7 [4300/36450]\tLoss: 414.0897\n",
      "Training Epoch: 7 [4350/36450]\tLoss: 358.5492\n",
      "Training Epoch: 7 [4400/36450]\tLoss: 422.8616\n",
      "Training Epoch: 7 [4450/36450]\tLoss: 401.7939\n",
      "Training Epoch: 7 [4500/36450]\tLoss: 454.6518\n",
      "Training Epoch: 7 [4550/36450]\tLoss: 436.2551\n",
      "Training Epoch: 7 [4600/36450]\tLoss: 429.7357\n",
      "Training Epoch: 7 [4650/36450]\tLoss: 368.7446\n",
      "Training Epoch: 7 [4700/36450]\tLoss: 443.4868\n",
      "Training Epoch: 7 [4750/36450]\tLoss: 393.1212\n",
      "Training Epoch: 7 [4800/36450]\tLoss: 425.9282\n",
      "Training Epoch: 7 [4850/36450]\tLoss: 415.4198\n",
      "Training Epoch: 7 [4900/36450]\tLoss: 425.4664\n",
      "Training Epoch: 7 [4950/36450]\tLoss: 414.0319\n",
      "Training Epoch: 7 [5000/36450]\tLoss: 365.5562\n",
      "Training Epoch: 7 [5050/36450]\tLoss: 384.3589\n",
      "Training Epoch: 7 [5100/36450]\tLoss: 438.3572\n",
      "Training Epoch: 7 [5150/36450]\tLoss: 418.0033\n",
      "Training Epoch: 7 [5200/36450]\tLoss: 386.0645\n",
      "Training Epoch: 7 [5250/36450]\tLoss: 415.1047\n",
      "Training Epoch: 7 [5300/36450]\tLoss: 352.4279\n",
      "Training Epoch: 7 [5350/36450]\tLoss: 417.2317\n",
      "Training Epoch: 7 [5400/36450]\tLoss: 401.1736\n",
      "Training Epoch: 7 [5450/36450]\tLoss: 420.3451\n",
      "Training Epoch: 7 [5500/36450]\tLoss: 383.4505\n",
      "Training Epoch: 7 [5550/36450]\tLoss: 410.7113\n",
      "Training Epoch: 7 [5600/36450]\tLoss: 457.6062\n",
      "Training Epoch: 7 [5650/36450]\tLoss: 424.9973\n",
      "Training Epoch: 7 [5700/36450]\tLoss: 373.5856\n",
      "Training Epoch: 7 [5750/36450]\tLoss: 421.5283\n",
      "Training Epoch: 7 [5800/36450]\tLoss: 406.7071\n",
      "Training Epoch: 7 [5850/36450]\tLoss: 430.6354\n",
      "Training Epoch: 7 [5900/36450]\tLoss: 380.4702\n",
      "Training Epoch: 7 [5950/36450]\tLoss: 434.0369\n",
      "Training Epoch: 7 [6000/36450]\tLoss: 392.7530\n",
      "Training Epoch: 7 [6050/36450]\tLoss: 375.6925\n",
      "Training Epoch: 7 [6100/36450]\tLoss: 373.0018\n",
      "Training Epoch: 7 [6150/36450]\tLoss: 397.2497\n",
      "Training Epoch: 7 [6200/36450]\tLoss: 389.7486\n",
      "Training Epoch: 7 [6250/36450]\tLoss: 454.9346\n",
      "Training Epoch: 7 [6300/36450]\tLoss: 400.3187\n",
      "Training Epoch: 7 [6350/36450]\tLoss: 424.7018\n",
      "Training Epoch: 7 [6400/36450]\tLoss: 416.6958\n",
      "Training Epoch: 7 [6450/36450]\tLoss: 394.0044\n",
      "Training Epoch: 7 [6500/36450]\tLoss: 391.9739\n",
      "Training Epoch: 7 [6550/36450]\tLoss: 383.7929\n",
      "Training Epoch: 7 [6600/36450]\tLoss: 376.3904\n",
      "Training Epoch: 7 [6650/36450]\tLoss: 418.0640\n",
      "Training Epoch: 7 [6700/36450]\tLoss: 390.0980\n",
      "Training Epoch: 7 [6750/36450]\tLoss: 391.4282\n",
      "Training Epoch: 7 [6800/36450]\tLoss: 409.8143\n",
      "Training Epoch: 7 [6850/36450]\tLoss: 388.5074\n",
      "Training Epoch: 7 [6900/36450]\tLoss: 387.7470\n",
      "Training Epoch: 7 [6950/36450]\tLoss: 378.1413\n",
      "Training Epoch: 7 [7000/36450]\tLoss: 449.2922\n",
      "Training Epoch: 7 [7050/36450]\tLoss: 388.0008\n",
      "Training Epoch: 7 [7100/36450]\tLoss: 376.5541\n",
      "Training Epoch: 7 [7150/36450]\tLoss: 457.1341\n",
      "Training Epoch: 7 [7200/36450]\tLoss: 398.4892\n",
      "Training Epoch: 7 [7250/36450]\tLoss: 372.8423\n",
      "Training Epoch: 7 [7300/36450]\tLoss: 383.8051\n",
      "Training Epoch: 7 [7350/36450]\tLoss: 447.1712\n",
      "Training Epoch: 7 [7400/36450]\tLoss: 391.6940\n",
      "Training Epoch: 7 [7450/36450]\tLoss: 360.7656\n",
      "Training Epoch: 7 [7500/36450]\tLoss: 406.2173\n",
      "Training Epoch: 7 [7550/36450]\tLoss: 373.8876\n",
      "Training Epoch: 7 [7600/36450]\tLoss: 475.0453\n",
      "Training Epoch: 7 [7650/36450]\tLoss: 388.2054\n",
      "Training Epoch: 7 [7700/36450]\tLoss: 353.8011\n",
      "Training Epoch: 7 [7750/36450]\tLoss: 392.5629\n",
      "Training Epoch: 7 [7800/36450]\tLoss: 424.1609\n",
      "Training Epoch: 7 [7850/36450]\tLoss: 387.4823\n",
      "Training Epoch: 7 [7900/36450]\tLoss: 340.9339\n",
      "Training Epoch: 7 [7950/36450]\tLoss: 391.1881\n",
      "Training Epoch: 7 [8000/36450]\tLoss: 386.0173\n",
      "Training Epoch: 7 [8050/36450]\tLoss: 389.7950\n",
      "Training Epoch: 7 [8100/36450]\tLoss: 406.6419\n",
      "Training Epoch: 7 [8150/36450]\tLoss: 436.9840\n",
      "Training Epoch: 7 [8200/36450]\tLoss: 401.6815\n",
      "Training Epoch: 7 [8250/36450]\tLoss: 365.7628\n",
      "Training Epoch: 7 [8300/36450]\tLoss: 403.2086\n",
      "Training Epoch: 7 [8350/36450]\tLoss: 445.6133\n",
      "Training Epoch: 7 [8400/36450]\tLoss: 363.3880\n",
      "Training Epoch: 7 [8450/36450]\tLoss: 427.5940\n",
      "Training Epoch: 7 [8500/36450]\tLoss: 395.3905\n",
      "Training Epoch: 7 [8550/36450]\tLoss: 397.0189\n",
      "Training Epoch: 7 [8600/36450]\tLoss: 407.5113\n",
      "Training Epoch: 7 [8650/36450]\tLoss: 374.5491\n",
      "Training Epoch: 7 [8700/36450]\tLoss: 399.7897\n",
      "Training Epoch: 7 [8750/36450]\tLoss: 444.5077\n",
      "Training Epoch: 7 [8800/36450]\tLoss: 419.1191\n",
      "Training Epoch: 7 [8850/36450]\tLoss: 370.3322\n",
      "Training Epoch: 7 [8900/36450]\tLoss: 448.3468\n",
      "Training Epoch: 7 [8950/36450]\tLoss: 373.4564\n",
      "Training Epoch: 7 [9000/36450]\tLoss: 368.8702\n",
      "Training Epoch: 7 [9050/36450]\tLoss: 380.7501\n",
      "Training Epoch: 7 [9100/36450]\tLoss: 399.4449\n",
      "Training Epoch: 7 [9150/36450]\tLoss: 405.2344\n",
      "Training Epoch: 7 [9200/36450]\tLoss: 415.2453\n",
      "Training Epoch: 7 [9250/36450]\tLoss: 359.0778\n",
      "Training Epoch: 7 [9300/36450]\tLoss: 413.8429\n",
      "Training Epoch: 7 [9350/36450]\tLoss: 417.6017\n",
      "Training Epoch: 7 [9400/36450]\tLoss: 340.4803\n",
      "Training Epoch: 7 [9450/36450]\tLoss: 371.7213\n",
      "Training Epoch: 7 [9500/36450]\tLoss: 386.5472\n",
      "Training Epoch: 7 [9550/36450]\tLoss: 390.3358\n",
      "Training Epoch: 7 [9600/36450]\tLoss: 385.9364\n",
      "Training Epoch: 7 [9650/36450]\tLoss: 383.0511\n",
      "Training Epoch: 7 [9700/36450]\tLoss: 340.3674\n",
      "Training Epoch: 7 [9750/36450]\tLoss: 421.2812\n",
      "Training Epoch: 7 [9800/36450]\tLoss: 399.2211\n",
      "Training Epoch: 7 [9850/36450]\tLoss: 379.2280\n",
      "Training Epoch: 7 [9900/36450]\tLoss: 371.1190\n",
      "Training Epoch: 7 [9950/36450]\tLoss: 376.0014\n",
      "Training Epoch: 7 [10000/36450]\tLoss: 362.4901\n",
      "Training Epoch: 7 [10050/36450]\tLoss: 376.5015\n",
      "Training Epoch: 7 [10100/36450]\tLoss: 400.7828\n",
      "Training Epoch: 7 [10150/36450]\tLoss: 409.6551\n",
      "Training Epoch: 7 [10200/36450]\tLoss: 391.5160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [10250/36450]\tLoss: 426.8150\n",
      "Training Epoch: 7 [10300/36450]\tLoss: 400.1472\n",
      "Training Epoch: 7 [10350/36450]\tLoss: 409.7119\n",
      "Training Epoch: 7 [10400/36450]\tLoss: 364.8125\n",
      "Training Epoch: 7 [10450/36450]\tLoss: 439.3177\n",
      "Training Epoch: 7 [10500/36450]\tLoss: 367.8342\n",
      "Training Epoch: 7 [10550/36450]\tLoss: 392.7538\n",
      "Training Epoch: 7 [10600/36450]\tLoss: 374.8449\n",
      "Training Epoch: 7 [10650/36450]\tLoss: 389.8110\n",
      "Training Epoch: 7 [10700/36450]\tLoss: 417.6556\n",
      "Training Epoch: 7 [10750/36450]\tLoss: 392.6730\n",
      "Training Epoch: 7 [10800/36450]\tLoss: 427.9626\n",
      "Training Epoch: 7 [10850/36450]\tLoss: 414.8117\n",
      "Training Epoch: 7 [10900/36450]\tLoss: 386.6237\n",
      "Training Epoch: 7 [10950/36450]\tLoss: 391.8396\n",
      "Training Epoch: 7 [11000/36450]\tLoss: 391.0821\n",
      "Training Epoch: 7 [11050/36450]\tLoss: 421.1372\n",
      "Training Epoch: 7 [11100/36450]\tLoss: 408.0454\n",
      "Training Epoch: 7 [11150/36450]\tLoss: 401.2641\n",
      "Training Epoch: 7 [11200/36450]\tLoss: 449.3638\n",
      "Training Epoch: 7 [11250/36450]\tLoss: 434.3468\n",
      "Training Epoch: 7 [11300/36450]\tLoss: 393.5398\n",
      "Training Epoch: 7 [11350/36450]\tLoss: 411.0433\n",
      "Training Epoch: 7 [11400/36450]\tLoss: 420.3387\n",
      "Training Epoch: 7 [11450/36450]\tLoss: 396.9447\n",
      "Training Epoch: 7 [11500/36450]\tLoss: 378.6392\n",
      "Training Epoch: 7 [11550/36450]\tLoss: 396.9630\n",
      "Training Epoch: 7 [11600/36450]\tLoss: 401.2125\n",
      "Training Epoch: 7 [11650/36450]\tLoss: 384.2042\n",
      "Training Epoch: 7 [11700/36450]\tLoss: 392.5173\n",
      "Training Epoch: 7 [11750/36450]\tLoss: 421.0438\n",
      "Training Epoch: 7 [11800/36450]\tLoss: 446.3763\n",
      "Training Epoch: 7 [11850/36450]\tLoss: 439.7842\n",
      "Training Epoch: 7 [11900/36450]\tLoss: 360.1475\n",
      "Training Epoch: 7 [11950/36450]\tLoss: 378.0585\n",
      "Training Epoch: 7 [12000/36450]\tLoss: 380.1924\n",
      "Training Epoch: 7 [12050/36450]\tLoss: 420.8061\n",
      "Training Epoch: 7 [12100/36450]\tLoss: 367.0637\n",
      "Training Epoch: 7 [12150/36450]\tLoss: 376.2977\n",
      "Training Epoch: 7 [12200/36450]\tLoss: 354.6937\n",
      "Training Epoch: 7 [12250/36450]\tLoss: 388.2081\n",
      "Training Epoch: 7 [12300/36450]\tLoss: 389.1181\n",
      "Training Epoch: 7 [12350/36450]\tLoss: 390.4222\n",
      "Training Epoch: 7 [12400/36450]\tLoss: 426.8032\n",
      "Training Epoch: 7 [12450/36450]\tLoss: 421.6131\n",
      "Training Epoch: 7 [12500/36450]\tLoss: 365.9424\n",
      "Training Epoch: 7 [12550/36450]\tLoss: 422.6269\n",
      "Training Epoch: 7 [12600/36450]\tLoss: 390.0811\n",
      "Training Epoch: 7 [12650/36450]\tLoss: 366.1712\n",
      "Training Epoch: 7 [12700/36450]\tLoss: 402.3412\n",
      "Training Epoch: 7 [12750/36450]\tLoss: 383.2185\n",
      "Training Epoch: 7 [12800/36450]\tLoss: 395.7113\n",
      "Training Epoch: 7 [12850/36450]\tLoss: 405.5640\n",
      "Training Epoch: 7 [12900/36450]\tLoss: 401.0621\n",
      "Training Epoch: 7 [12950/36450]\tLoss: 386.7070\n",
      "Training Epoch: 7 [13000/36450]\tLoss: 365.8581\n",
      "Training Epoch: 7 [13050/36450]\tLoss: 391.3855\n",
      "Training Epoch: 7 [13100/36450]\tLoss: 398.4811\n",
      "Training Epoch: 7 [13150/36450]\tLoss: 375.4815\n",
      "Training Epoch: 7 [13200/36450]\tLoss: 385.8076\n",
      "Training Epoch: 7 [13250/36450]\tLoss: 396.1967\n",
      "Training Epoch: 7 [13300/36450]\tLoss: 406.8075\n",
      "Training Epoch: 7 [13350/36450]\tLoss: 394.2516\n",
      "Training Epoch: 7 [13400/36450]\tLoss: 400.6875\n",
      "Training Epoch: 7 [13450/36450]\tLoss: 394.4888\n",
      "Training Epoch: 7 [13500/36450]\tLoss: 387.2156\n",
      "Training Epoch: 7 [13550/36450]\tLoss: 417.8224\n",
      "Training Epoch: 7 [13600/36450]\tLoss: 417.2359\n",
      "Training Epoch: 7 [13650/36450]\tLoss: 398.4294\n",
      "Training Epoch: 7 [13700/36450]\tLoss: 431.5343\n",
      "Training Epoch: 7 [13750/36450]\tLoss: 369.0395\n",
      "Training Epoch: 7 [13800/36450]\tLoss: 440.2690\n",
      "Training Epoch: 7 [13850/36450]\tLoss: 443.6942\n",
      "Training Epoch: 7 [13900/36450]\tLoss: 363.5658\n",
      "Training Epoch: 7 [13950/36450]\tLoss: 428.1019\n",
      "Training Epoch: 7 [14000/36450]\tLoss: 405.0982\n",
      "Training Epoch: 7 [14050/36450]\tLoss: 400.1114\n",
      "Training Epoch: 7 [14100/36450]\tLoss: 400.4453\n",
      "Training Epoch: 7 [14150/36450]\tLoss: 386.5447\n",
      "Training Epoch: 7 [14200/36450]\tLoss: 393.1177\n",
      "Training Epoch: 7 [14250/36450]\tLoss: 381.6583\n",
      "Training Epoch: 7 [14300/36450]\tLoss: 436.3995\n",
      "Training Epoch: 7 [14350/36450]\tLoss: 388.5288\n",
      "Training Epoch: 7 [14400/36450]\tLoss: 371.8847\n",
      "Training Epoch: 7 [14450/36450]\tLoss: 373.7235\n",
      "Training Epoch: 7 [14500/36450]\tLoss: 384.5880\n",
      "Training Epoch: 7 [14550/36450]\tLoss: 366.2002\n",
      "Training Epoch: 7 [14600/36450]\tLoss: 407.7953\n",
      "Training Epoch: 7 [14650/36450]\tLoss: 386.7392\n",
      "Training Epoch: 7 [14700/36450]\tLoss: 435.1485\n",
      "Training Epoch: 7 [14750/36450]\tLoss: 387.4190\n",
      "Training Epoch: 7 [14800/36450]\tLoss: 369.7679\n",
      "Training Epoch: 7 [14850/36450]\tLoss: 402.9077\n",
      "Training Epoch: 7 [14900/36450]\tLoss: 430.7974\n",
      "Training Epoch: 7 [14950/36450]\tLoss: 382.9197\n",
      "Training Epoch: 7 [15000/36450]\tLoss: 399.3172\n",
      "Training Epoch: 7 [15050/36450]\tLoss: 364.8448\n",
      "Training Epoch: 7 [15100/36450]\tLoss: 422.2209\n",
      "Training Epoch: 7 [15150/36450]\tLoss: 408.2875\n",
      "Training Epoch: 7 [15200/36450]\tLoss: 354.0051\n",
      "Training Epoch: 7 [15250/36450]\tLoss: 382.5506\n",
      "Training Epoch: 7 [15300/36450]\tLoss: 368.5361\n",
      "Training Epoch: 7 [15350/36450]\tLoss: 390.8998\n",
      "Training Epoch: 7 [15400/36450]\tLoss: 391.1566\n",
      "Training Epoch: 7 [15450/36450]\tLoss: 410.7285\n",
      "Training Epoch: 7 [15500/36450]\tLoss: 431.9869\n",
      "Training Epoch: 7 [15550/36450]\tLoss: 375.8756\n",
      "Training Epoch: 7 [15600/36450]\tLoss: 405.0442\n",
      "Training Epoch: 7 [15650/36450]\tLoss: 363.2318\n",
      "Training Epoch: 7 [15700/36450]\tLoss: 390.7954\n",
      "Training Epoch: 7 [15750/36450]\tLoss: 380.8315\n",
      "Training Epoch: 7 [15800/36450]\tLoss: 364.0659\n",
      "Training Epoch: 7 [15850/36450]\tLoss: 390.6658\n",
      "Training Epoch: 7 [15900/36450]\tLoss: 388.9955\n",
      "Training Epoch: 7 [15950/36450]\tLoss: 388.7345\n",
      "Training Epoch: 7 [16000/36450]\tLoss: 413.2265\n",
      "Training Epoch: 7 [16050/36450]\tLoss: 416.7847\n",
      "Training Epoch: 7 [16100/36450]\tLoss: 390.0962\n",
      "Training Epoch: 7 [16150/36450]\tLoss: 343.2859\n",
      "Training Epoch: 7 [16200/36450]\tLoss: 397.0242\n",
      "Training Epoch: 7 [16250/36450]\tLoss: 397.5841\n",
      "Training Epoch: 7 [16300/36450]\tLoss: 388.3632\n",
      "Training Epoch: 7 [16350/36450]\tLoss: 399.3627\n",
      "Training Epoch: 7 [16400/36450]\tLoss: 375.0477\n",
      "Training Epoch: 7 [16450/36450]\tLoss: 403.5229\n",
      "Training Epoch: 7 [16500/36450]\tLoss: 396.8856\n",
      "Training Epoch: 7 [16550/36450]\tLoss: 419.0469\n",
      "Training Epoch: 7 [16600/36450]\tLoss: 439.3851\n",
      "Training Epoch: 7 [16650/36450]\tLoss: 404.2822\n",
      "Training Epoch: 7 [16700/36450]\tLoss: 418.7766\n",
      "Training Epoch: 7 [16750/36450]\tLoss: 430.0522\n",
      "Training Epoch: 7 [16800/36450]\tLoss: 401.5674\n",
      "Training Epoch: 7 [16850/36450]\tLoss: 405.3979\n",
      "Training Epoch: 7 [16900/36450]\tLoss: 398.8492\n",
      "Training Epoch: 7 [16950/36450]\tLoss: 387.6484\n",
      "Training Epoch: 7 [17000/36450]\tLoss: 380.8619\n",
      "Training Epoch: 7 [17050/36450]\tLoss: 371.1462\n",
      "Training Epoch: 7 [17100/36450]\tLoss: 443.7527\n",
      "Training Epoch: 7 [17150/36450]\tLoss: 404.7734\n",
      "Training Epoch: 7 [17200/36450]\tLoss: 400.5590\n",
      "Training Epoch: 7 [17250/36450]\tLoss: 385.1750\n",
      "Training Epoch: 7 [17300/36450]\tLoss: 416.7861\n",
      "Training Epoch: 7 [17350/36450]\tLoss: 411.4404\n",
      "Training Epoch: 7 [17400/36450]\tLoss: 409.3565\n",
      "Training Epoch: 7 [17450/36450]\tLoss: 418.6505\n",
      "Training Epoch: 7 [17500/36450]\tLoss: 378.8722\n",
      "Training Epoch: 7 [17550/36450]\tLoss: 377.7615\n",
      "Training Epoch: 7 [17600/36450]\tLoss: 349.8525\n",
      "Training Epoch: 7 [17650/36450]\tLoss: 372.1349\n",
      "Training Epoch: 7 [17700/36450]\tLoss: 411.4742\n",
      "Training Epoch: 7 [17750/36450]\tLoss: 416.3536\n",
      "Training Epoch: 7 [17800/36450]\tLoss: 393.6721\n",
      "Training Epoch: 7 [17850/36450]\tLoss: 407.9733\n",
      "Training Epoch: 7 [17900/36450]\tLoss: 392.2187\n",
      "Training Epoch: 7 [17950/36450]\tLoss: 410.0865\n",
      "Training Epoch: 7 [18000/36450]\tLoss: 394.6966\n",
      "Training Epoch: 7 [18050/36450]\tLoss: 426.4050\n",
      "Training Epoch: 7 [18100/36450]\tLoss: 442.9668\n",
      "Training Epoch: 7 [18150/36450]\tLoss: 351.5707\n",
      "Training Epoch: 7 [18200/36450]\tLoss: 379.1725\n",
      "Training Epoch: 7 [18250/36450]\tLoss: 410.5758\n",
      "Training Epoch: 7 [18300/36450]\tLoss: 398.4966\n",
      "Training Epoch: 7 [18350/36450]\tLoss: 380.8715\n",
      "Training Epoch: 7 [18400/36450]\tLoss: 368.4289\n",
      "Training Epoch: 7 [18450/36450]\tLoss: 354.0640\n",
      "Training Epoch: 7 [18500/36450]\tLoss: 379.8608\n",
      "Training Epoch: 7 [18550/36450]\tLoss: 369.1462\n",
      "Training Epoch: 7 [18600/36450]\tLoss: 440.9785\n",
      "Training Epoch: 7 [18650/36450]\tLoss: 395.5950\n",
      "Training Epoch: 7 [18700/36450]\tLoss: 399.4475\n",
      "Training Epoch: 7 [18750/36450]\tLoss: 395.6659\n",
      "Training Epoch: 7 [18800/36450]\tLoss: 348.3286\n",
      "Training Epoch: 7 [18850/36450]\tLoss: 383.3224\n",
      "Training Epoch: 7 [18900/36450]\tLoss: 399.4964\n",
      "Training Epoch: 7 [18950/36450]\tLoss: 415.5833\n",
      "Training Epoch: 7 [19000/36450]\tLoss: 439.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [19050/36450]\tLoss: 365.1305\n",
      "Training Epoch: 7 [19100/36450]\tLoss: 380.4176\n",
      "Training Epoch: 7 [19150/36450]\tLoss: 389.6937\n",
      "Training Epoch: 7 [19200/36450]\tLoss: 374.8546\n",
      "Training Epoch: 7 [19250/36450]\tLoss: 430.2115\n",
      "Training Epoch: 7 [19300/36450]\tLoss: 351.1818\n",
      "Training Epoch: 7 [19350/36450]\tLoss: 398.5584\n",
      "Training Epoch: 7 [19400/36450]\tLoss: 438.8321\n",
      "Training Epoch: 7 [19450/36450]\tLoss: 375.6138\n",
      "Training Epoch: 7 [19500/36450]\tLoss: 331.4189\n",
      "Training Epoch: 7 [19550/36450]\tLoss: 390.3228\n",
      "Training Epoch: 7 [19600/36450]\tLoss: 410.7726\n",
      "Training Epoch: 7 [19650/36450]\tLoss: 420.0247\n",
      "Training Epoch: 7 [19700/36450]\tLoss: 407.2786\n",
      "Training Epoch: 7 [19750/36450]\tLoss: 395.6525\n",
      "Training Epoch: 7 [19800/36450]\tLoss: 409.8614\n",
      "Training Epoch: 7 [19850/36450]\tLoss: 337.5217\n",
      "Training Epoch: 7 [19900/36450]\tLoss: 389.3750\n",
      "Training Epoch: 7 [19950/36450]\tLoss: 396.0957\n",
      "Training Epoch: 7 [20000/36450]\tLoss: 410.2295\n",
      "Training Epoch: 7 [20050/36450]\tLoss: 409.6922\n",
      "Training Epoch: 7 [20100/36450]\tLoss: 408.0690\n",
      "Training Epoch: 7 [20150/36450]\tLoss: 404.3072\n",
      "Training Epoch: 7 [20200/36450]\tLoss: 399.4267\n",
      "Training Epoch: 7 [20250/36450]\tLoss: 380.1236\n",
      "Training Epoch: 7 [20300/36450]\tLoss: 376.8309\n",
      "Training Epoch: 7 [20350/36450]\tLoss: 391.0609\n",
      "Training Epoch: 7 [20400/36450]\tLoss: 389.2904\n",
      "Training Epoch: 7 [20450/36450]\tLoss: 374.9661\n",
      "Training Epoch: 7 [20500/36450]\tLoss: 399.3472\n",
      "Training Epoch: 7 [20550/36450]\tLoss: 366.1245\n",
      "Training Epoch: 7 [20600/36450]\tLoss: 405.8417\n",
      "Training Epoch: 7 [20650/36450]\tLoss: 392.9844\n",
      "Training Epoch: 7 [20700/36450]\tLoss: 399.6245\n",
      "Training Epoch: 7 [20750/36450]\tLoss: 392.1610\n",
      "Training Epoch: 7 [20800/36450]\tLoss: 385.7237\n",
      "Training Epoch: 7 [20850/36450]\tLoss: 431.2145\n",
      "Training Epoch: 7 [20900/36450]\tLoss: 394.8539\n",
      "Training Epoch: 7 [20950/36450]\tLoss: 373.9689\n",
      "Training Epoch: 7 [21000/36450]\tLoss: 405.4314\n",
      "Training Epoch: 7 [21050/36450]\tLoss: 411.6389\n",
      "Training Epoch: 7 [21100/36450]\tLoss: 390.0176\n",
      "Training Epoch: 7 [21150/36450]\tLoss: 398.4201\n",
      "Training Epoch: 7 [21200/36450]\tLoss: 381.7630\n",
      "Training Epoch: 7 [21250/36450]\tLoss: 394.0379\n",
      "Training Epoch: 7 [21300/36450]\tLoss: 401.9291\n",
      "Training Epoch: 7 [21350/36450]\tLoss: 378.4002\n",
      "Training Epoch: 7 [21400/36450]\tLoss: 404.3113\n",
      "Training Epoch: 7 [21450/36450]\tLoss: 375.4428\n",
      "Training Epoch: 7 [21500/36450]\tLoss: 376.5962\n",
      "Training Epoch: 7 [21550/36450]\tLoss: 373.5586\n",
      "Training Epoch: 7 [21600/36450]\tLoss: 374.8741\n",
      "Training Epoch: 7 [21650/36450]\tLoss: 384.9731\n",
      "Training Epoch: 7 [21700/36450]\tLoss: 383.3693\n",
      "Training Epoch: 7 [21750/36450]\tLoss: 396.6351\n",
      "Training Epoch: 7 [21800/36450]\tLoss: 415.7780\n",
      "Training Epoch: 7 [21850/36450]\tLoss: 369.3647\n",
      "Training Epoch: 7 [21900/36450]\tLoss: 391.5873\n",
      "Training Epoch: 7 [21950/36450]\tLoss: 407.5783\n",
      "Training Epoch: 7 [22000/36450]\tLoss: 422.5339\n",
      "Training Epoch: 7 [22050/36450]\tLoss: 431.4429\n",
      "Training Epoch: 7 [22100/36450]\tLoss: 389.5835\n",
      "Training Epoch: 7 [22150/36450]\tLoss: 373.2847\n",
      "Training Epoch: 7 [22200/36450]\tLoss: 398.7395\n",
      "Training Epoch: 7 [22250/36450]\tLoss: 359.4066\n",
      "Training Epoch: 7 [22300/36450]\tLoss: 358.2865\n",
      "Training Epoch: 7 [22350/36450]\tLoss: 366.2699\n",
      "Training Epoch: 7 [22400/36450]\tLoss: 439.6258\n",
      "Training Epoch: 7 [22450/36450]\tLoss: 382.1163\n",
      "Training Epoch: 7 [22500/36450]\tLoss: 368.2780\n",
      "Training Epoch: 7 [22550/36450]\tLoss: 367.4929\n",
      "Training Epoch: 7 [22600/36450]\tLoss: 362.5081\n",
      "Training Epoch: 7 [22650/36450]\tLoss: 357.7222\n",
      "Training Epoch: 7 [22700/36450]\tLoss: 415.0345\n",
      "Training Epoch: 7 [22750/36450]\tLoss: 420.5467\n",
      "Training Epoch: 7 [22800/36450]\tLoss: 388.6495\n",
      "Training Epoch: 7 [22850/36450]\tLoss: 414.5634\n",
      "Training Epoch: 7 [22900/36450]\tLoss: 371.7436\n",
      "Training Epoch: 7 [22950/36450]\tLoss: 396.9247\n",
      "Training Epoch: 7 [23000/36450]\tLoss: 391.3921\n",
      "Training Epoch: 7 [23050/36450]\tLoss: 409.1584\n",
      "Training Epoch: 7 [23100/36450]\tLoss: 372.5623\n",
      "Training Epoch: 7 [23150/36450]\tLoss: 414.4714\n",
      "Training Epoch: 7 [23200/36450]\tLoss: 371.7350\n",
      "Training Epoch: 7 [23250/36450]\tLoss: 383.5346\n",
      "Training Epoch: 7 [23300/36450]\tLoss: 373.3484\n",
      "Training Epoch: 7 [23350/36450]\tLoss: 383.4215\n",
      "Training Epoch: 7 [23400/36450]\tLoss: 354.6385\n",
      "Training Epoch: 7 [23450/36450]\tLoss: 342.0085\n",
      "Training Epoch: 7 [23500/36450]\tLoss: 344.2742\n",
      "Training Epoch: 7 [23550/36450]\tLoss: 372.6137\n",
      "Training Epoch: 7 [23600/36450]\tLoss: 371.5597\n",
      "Training Epoch: 7 [23650/36450]\tLoss: 439.1199\n",
      "Training Epoch: 7 [23700/36450]\tLoss: 390.6605\n",
      "Training Epoch: 7 [23750/36450]\tLoss: 372.4130\n",
      "Training Epoch: 7 [23800/36450]\tLoss: 409.2025\n",
      "Training Epoch: 7 [23850/36450]\tLoss: 392.6406\n",
      "Training Epoch: 7 [23900/36450]\tLoss: 359.0133\n",
      "Training Epoch: 7 [23950/36450]\tLoss: 408.7826\n",
      "Training Epoch: 7 [24000/36450]\tLoss: 369.0704\n",
      "Training Epoch: 7 [24050/36450]\tLoss: 394.5435\n",
      "Training Epoch: 7 [24100/36450]\tLoss: 428.5257\n",
      "Training Epoch: 7 [24150/36450]\tLoss: 385.0999\n",
      "Training Epoch: 7 [24200/36450]\tLoss: 416.5773\n",
      "Training Epoch: 7 [24250/36450]\tLoss: 423.2921\n",
      "Training Epoch: 7 [24300/36450]\tLoss: 441.4709\n",
      "Training Epoch: 7 [24350/36450]\tLoss: 378.8800\n",
      "Training Epoch: 7 [24400/36450]\tLoss: 426.7462\n",
      "Training Epoch: 7 [24450/36450]\tLoss: 383.8238\n",
      "Training Epoch: 7 [24500/36450]\tLoss: 404.2244\n",
      "Training Epoch: 7 [24550/36450]\tLoss: 386.9677\n",
      "Training Epoch: 7 [24600/36450]\tLoss: 402.6747\n",
      "Training Epoch: 7 [24650/36450]\tLoss: 368.7634\n",
      "Training Epoch: 7 [24700/36450]\tLoss: 429.3166\n",
      "Training Epoch: 7 [24750/36450]\tLoss: 383.8943\n",
      "Training Epoch: 7 [24800/36450]\tLoss: 386.9541\n",
      "Training Epoch: 7 [24850/36450]\tLoss: 382.3276\n",
      "Training Epoch: 7 [24900/36450]\tLoss: 366.3681\n",
      "Training Epoch: 7 [24950/36450]\tLoss: 383.7047\n",
      "Training Epoch: 7 [25000/36450]\tLoss: 366.4059\n",
      "Training Epoch: 7 [25050/36450]\tLoss: 401.1424\n",
      "Training Epoch: 7 [25100/36450]\tLoss: 367.9815\n",
      "Training Epoch: 7 [25150/36450]\tLoss: 356.8483\n",
      "Training Epoch: 7 [25200/36450]\tLoss: 394.7294\n",
      "Training Epoch: 7 [25250/36450]\tLoss: 403.1417\n",
      "Training Epoch: 7 [25300/36450]\tLoss: 358.5071\n",
      "Training Epoch: 7 [25350/36450]\tLoss: 342.6737\n",
      "Training Epoch: 7 [25400/36450]\tLoss: 426.0213\n",
      "Training Epoch: 7 [25450/36450]\tLoss: 409.6505\n",
      "Training Epoch: 7 [25500/36450]\tLoss: 339.9582\n",
      "Training Epoch: 7 [25550/36450]\tLoss: 409.3688\n",
      "Training Epoch: 7 [25600/36450]\tLoss: 395.0471\n",
      "Training Epoch: 7 [25650/36450]\tLoss: 385.7268\n",
      "Training Epoch: 7 [25700/36450]\tLoss: 372.9847\n",
      "Training Epoch: 7 [25750/36450]\tLoss: 382.1293\n",
      "Training Epoch: 7 [25800/36450]\tLoss: 353.6867\n",
      "Training Epoch: 7 [25850/36450]\tLoss: 408.7600\n",
      "Training Epoch: 7 [25900/36450]\tLoss: 405.6035\n",
      "Training Epoch: 7 [25950/36450]\tLoss: 364.9176\n",
      "Training Epoch: 7 [26000/36450]\tLoss: 401.9196\n",
      "Training Epoch: 7 [26050/36450]\tLoss: 386.3343\n",
      "Training Epoch: 7 [26100/36450]\tLoss: 368.7358\n",
      "Training Epoch: 7 [26150/36450]\tLoss: 367.0316\n",
      "Training Epoch: 7 [26200/36450]\tLoss: 418.1264\n",
      "Training Epoch: 7 [26250/36450]\tLoss: 422.7841\n",
      "Training Epoch: 7 [26300/36450]\tLoss: 402.6748\n",
      "Training Epoch: 7 [26350/36450]\tLoss: 407.5433\n",
      "Training Epoch: 7 [26400/36450]\tLoss: 374.1540\n",
      "Training Epoch: 7 [26450/36450]\tLoss: 399.6895\n",
      "Training Epoch: 7 [26500/36450]\tLoss: 410.9472\n",
      "Training Epoch: 7 [26550/36450]\tLoss: 414.7465\n",
      "Training Epoch: 7 [26600/36450]\tLoss: 397.7054\n",
      "Training Epoch: 7 [26650/36450]\tLoss: 404.9372\n",
      "Training Epoch: 7 [26700/36450]\tLoss: 377.5354\n",
      "Training Epoch: 7 [26750/36450]\tLoss: 420.7616\n",
      "Training Epoch: 7 [26800/36450]\tLoss: 411.2541\n",
      "Training Epoch: 7 [26850/36450]\tLoss: 354.0897\n",
      "Training Epoch: 7 [26900/36450]\tLoss: 383.3970\n",
      "Training Epoch: 7 [26950/36450]\tLoss: 446.1528\n",
      "Training Epoch: 7 [27000/36450]\tLoss: 383.3716\n",
      "Training Epoch: 7 [27050/36450]\tLoss: 394.8546\n",
      "Training Epoch: 7 [27100/36450]\tLoss: 407.3707\n",
      "Training Epoch: 7 [27150/36450]\tLoss: 382.9677\n",
      "Training Epoch: 7 [27200/36450]\tLoss: 449.5199\n",
      "Training Epoch: 7 [27250/36450]\tLoss: 387.5433\n",
      "Training Epoch: 7 [27300/36450]\tLoss: 370.4341\n",
      "Training Epoch: 7 [27350/36450]\tLoss: 353.8731\n",
      "Training Epoch: 7 [27400/36450]\tLoss: 384.5097\n",
      "Training Epoch: 7 [27450/36450]\tLoss: 376.5116\n",
      "Training Epoch: 7 [27500/36450]\tLoss: 367.6222\n",
      "Training Epoch: 7 [27550/36450]\tLoss: 398.3231\n",
      "Training Epoch: 7 [27600/36450]\tLoss: 407.1385\n",
      "Training Epoch: 7 [27650/36450]\tLoss: 393.1160\n",
      "Training Epoch: 7 [27700/36450]\tLoss: 361.2740\n",
      "Training Epoch: 7 [27750/36450]\tLoss: 411.8998\n",
      "Training Epoch: 7 [27800/36450]\tLoss: 367.0600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [27850/36450]\tLoss: 382.4641\n",
      "Training Epoch: 7 [27900/36450]\tLoss: 392.3110\n",
      "Training Epoch: 7 [27950/36450]\tLoss: 373.1003\n",
      "Training Epoch: 7 [28000/36450]\tLoss: 392.4602\n",
      "Training Epoch: 7 [28050/36450]\tLoss: 378.0197\n",
      "Training Epoch: 7 [28100/36450]\tLoss: 406.4625\n",
      "Training Epoch: 7 [28150/36450]\tLoss: 407.0957\n",
      "Training Epoch: 7 [28200/36450]\tLoss: 377.5704\n",
      "Training Epoch: 7 [28250/36450]\tLoss: 372.8524\n",
      "Training Epoch: 7 [28300/36450]\tLoss: 377.3410\n",
      "Training Epoch: 7 [28350/36450]\tLoss: 362.1485\n",
      "Training Epoch: 7 [28400/36450]\tLoss: 447.0735\n",
      "Training Epoch: 7 [28450/36450]\tLoss: 369.9254\n",
      "Training Epoch: 7 [28500/36450]\tLoss: 394.4918\n",
      "Training Epoch: 7 [28550/36450]\tLoss: 347.1379\n",
      "Training Epoch: 7 [28600/36450]\tLoss: 433.7709\n",
      "Training Epoch: 7 [28650/36450]\tLoss: 376.8324\n",
      "Training Epoch: 7 [28700/36450]\tLoss: 398.4989\n",
      "Training Epoch: 7 [28750/36450]\tLoss: 377.7794\n",
      "Training Epoch: 7 [28800/36450]\tLoss: 345.7816\n",
      "Training Epoch: 7 [28850/36450]\tLoss: 356.1077\n",
      "Training Epoch: 7 [28900/36450]\tLoss: 368.8817\n",
      "Training Epoch: 7 [28950/36450]\tLoss: 375.3435\n",
      "Training Epoch: 7 [29000/36450]\tLoss: 403.3841\n",
      "Training Epoch: 7 [29050/36450]\tLoss: 352.3184\n",
      "Training Epoch: 7 [29100/36450]\tLoss: 429.8106\n",
      "Training Epoch: 7 [29150/36450]\tLoss: 373.3610\n",
      "Training Epoch: 7 [29200/36450]\tLoss: 393.2630\n",
      "Training Epoch: 7 [29250/36450]\tLoss: 391.4459\n",
      "Training Epoch: 7 [29300/36450]\tLoss: 402.9310\n",
      "Training Epoch: 7 [29350/36450]\tLoss: 388.1945\n",
      "Training Epoch: 7 [29400/36450]\tLoss: 368.5876\n",
      "Training Epoch: 7 [29450/36450]\tLoss: 390.0049\n",
      "Training Epoch: 7 [29500/36450]\tLoss: 360.8998\n",
      "Training Epoch: 7 [29550/36450]\tLoss: 353.7428\n",
      "Training Epoch: 7 [29600/36450]\tLoss: 371.0726\n",
      "Training Epoch: 7 [29650/36450]\tLoss: 426.9058\n",
      "Training Epoch: 7 [29700/36450]\tLoss: 332.3392\n",
      "Training Epoch: 7 [29750/36450]\tLoss: 400.2210\n",
      "Training Epoch: 7 [29800/36450]\tLoss: 368.4756\n",
      "Training Epoch: 7 [29850/36450]\tLoss: 367.4649\n",
      "Training Epoch: 7 [29900/36450]\tLoss: 371.8174\n",
      "Training Epoch: 7 [29950/36450]\tLoss: 374.3140\n",
      "Training Epoch: 7 [30000/36450]\tLoss: 363.8477\n",
      "Training Epoch: 7 [30050/36450]\tLoss: 398.4357\n",
      "Training Epoch: 7 [30100/36450]\tLoss: 379.3389\n",
      "Training Epoch: 7 [30150/36450]\tLoss: 392.6238\n",
      "Training Epoch: 7 [30200/36450]\tLoss: 401.6358\n",
      "Training Epoch: 7 [30250/36450]\tLoss: 375.4722\n",
      "Training Epoch: 7 [30300/36450]\tLoss: 393.3564\n",
      "Training Epoch: 7 [30350/36450]\tLoss: 375.2046\n",
      "Training Epoch: 7 [30400/36450]\tLoss: 429.9829\n",
      "Training Epoch: 7 [30450/36450]\tLoss: 393.9969\n",
      "Training Epoch: 7 [30500/36450]\tLoss: 379.8455\n",
      "Training Epoch: 7 [30550/36450]\tLoss: 348.0987\n",
      "Training Epoch: 7 [30600/36450]\tLoss: 386.3978\n",
      "Training Epoch: 7 [30650/36450]\tLoss: 360.0279\n",
      "Training Epoch: 7 [30700/36450]\tLoss: 380.6227\n",
      "Training Epoch: 7 [30750/36450]\tLoss: 412.7159\n",
      "Training Epoch: 7 [30800/36450]\tLoss: 426.4803\n",
      "Training Epoch: 7 [30850/36450]\tLoss: 354.3193\n",
      "Training Epoch: 7 [30900/36450]\tLoss: 387.5306\n",
      "Training Epoch: 7 [30950/36450]\tLoss: 387.6775\n",
      "Training Epoch: 7 [31000/36450]\tLoss: 362.2050\n",
      "Training Epoch: 7 [31050/36450]\tLoss: 371.8029\n",
      "Training Epoch: 7 [31100/36450]\tLoss: 392.7000\n",
      "Training Epoch: 7 [31150/36450]\tLoss: 349.8837\n",
      "Training Epoch: 7 [31200/36450]\tLoss: 369.1935\n",
      "Training Epoch: 7 [31250/36450]\tLoss: 364.8552\n",
      "Training Epoch: 7 [31300/36450]\tLoss: 398.8109\n",
      "Training Epoch: 7 [31350/36450]\tLoss: 388.4901\n",
      "Training Epoch: 7 [31400/36450]\tLoss: 383.9778\n",
      "Training Epoch: 7 [31450/36450]\tLoss: 384.4956\n",
      "Training Epoch: 7 [31500/36450]\tLoss: 360.7007\n",
      "Training Epoch: 7 [31550/36450]\tLoss: 406.5643\n",
      "Training Epoch: 7 [31600/36450]\tLoss: 347.2727\n",
      "Training Epoch: 7 [31650/36450]\tLoss: 368.6891\n",
      "Training Epoch: 7 [31700/36450]\tLoss: 370.5442\n",
      "Training Epoch: 7 [31750/36450]\tLoss: 434.8735\n",
      "Training Epoch: 7 [31800/36450]\tLoss: 381.3268\n",
      "Training Epoch: 7 [31850/36450]\tLoss: 377.9830\n",
      "Training Epoch: 7 [31900/36450]\tLoss: 339.6490\n",
      "Training Epoch: 7 [31950/36450]\tLoss: 372.6350\n",
      "Training Epoch: 7 [32000/36450]\tLoss: 388.2342\n",
      "Training Epoch: 7 [32050/36450]\tLoss: 392.1310\n",
      "Training Epoch: 7 [32100/36450]\tLoss: 397.9664\n",
      "Training Epoch: 7 [32150/36450]\tLoss: 347.1570\n",
      "Training Epoch: 7 [32200/36450]\tLoss: 377.1415\n",
      "Training Epoch: 7 [32250/36450]\tLoss: 389.2096\n",
      "Training Epoch: 7 [32300/36450]\tLoss: 368.1864\n",
      "Training Epoch: 7 [32350/36450]\tLoss: 341.7816\n",
      "Training Epoch: 7 [32400/36450]\tLoss: 398.5218\n",
      "Training Epoch: 7 [32450/36450]\tLoss: 379.8763\n",
      "Training Epoch: 7 [32500/36450]\tLoss: 393.4299\n",
      "Training Epoch: 7 [32550/36450]\tLoss: 349.1137\n",
      "Training Epoch: 7 [32600/36450]\tLoss: 381.1019\n",
      "Training Epoch: 7 [32650/36450]\tLoss: 382.8714\n",
      "Training Epoch: 7 [32700/36450]\tLoss: 373.7430\n",
      "Training Epoch: 7 [32750/36450]\tLoss: 377.1665\n",
      "Training Epoch: 7 [32800/36450]\tLoss: 405.2850\n",
      "Training Epoch: 7 [32850/36450]\tLoss: 384.5019\n",
      "Training Epoch: 7 [32900/36450]\tLoss: 375.1090\n",
      "Training Epoch: 7 [32950/36450]\tLoss: 396.4074\n",
      "Training Epoch: 7 [33000/36450]\tLoss: 425.6401\n",
      "Training Epoch: 7 [33050/36450]\tLoss: 391.1404\n",
      "Training Epoch: 7 [33100/36450]\tLoss: 383.8965\n",
      "Training Epoch: 7 [33150/36450]\tLoss: 351.7721\n",
      "Training Epoch: 7 [33200/36450]\tLoss: 389.9312\n",
      "Training Epoch: 7 [33250/36450]\tLoss: 390.5549\n",
      "Training Epoch: 7 [33300/36450]\tLoss: 419.3177\n",
      "Training Epoch: 7 [33350/36450]\tLoss: 382.4767\n",
      "Training Epoch: 7 [33400/36450]\tLoss: 380.2422\n",
      "Training Epoch: 7 [33450/36450]\tLoss: 386.8673\n",
      "Training Epoch: 7 [33500/36450]\tLoss: 377.0136\n",
      "Training Epoch: 7 [33550/36450]\tLoss: 403.1870\n",
      "Training Epoch: 7 [33600/36450]\tLoss: 372.9224\n",
      "Training Epoch: 7 [33650/36450]\tLoss: 362.7355\n",
      "Training Epoch: 7 [33700/36450]\tLoss: 374.7951\n",
      "Training Epoch: 7 [33750/36450]\tLoss: 378.7067\n",
      "Training Epoch: 7 [33800/36450]\tLoss: 414.4485\n",
      "Training Epoch: 7 [33850/36450]\tLoss: 392.4804\n",
      "Training Epoch: 7 [33900/36450]\tLoss: 338.9841\n",
      "Training Epoch: 7 [33950/36450]\tLoss: 373.5675\n",
      "Training Epoch: 7 [34000/36450]\tLoss: 356.4052\n",
      "Training Epoch: 7 [34050/36450]\tLoss: 407.9817\n",
      "Training Epoch: 7 [34100/36450]\tLoss: 395.1214\n",
      "Training Epoch: 7 [34150/36450]\tLoss: 405.4345\n",
      "Training Epoch: 7 [34200/36450]\tLoss: 377.4552\n",
      "Training Epoch: 7 [34250/36450]\tLoss: 441.1727\n",
      "Training Epoch: 7 [34300/36450]\tLoss: 371.4092\n",
      "Training Epoch: 7 [34350/36450]\tLoss: 340.8415\n",
      "Training Epoch: 7 [34400/36450]\tLoss: 409.9797\n",
      "Training Epoch: 7 [34450/36450]\tLoss: 395.4580\n",
      "Training Epoch: 7 [34500/36450]\tLoss: 369.4017\n",
      "Training Epoch: 7 [34550/36450]\tLoss: 374.1897\n",
      "Training Epoch: 7 [34600/36450]\tLoss: 403.8212\n",
      "Training Epoch: 7 [34650/36450]\tLoss: 424.2850\n",
      "Training Epoch: 7 [34700/36450]\tLoss: 375.3497\n",
      "Training Epoch: 7 [34750/36450]\tLoss: 369.0499\n",
      "Training Epoch: 7 [34800/36450]\tLoss: 371.0010\n",
      "Training Epoch: 7 [34850/36450]\tLoss: 380.1913\n",
      "Training Epoch: 7 [34900/36450]\tLoss: 408.3426\n",
      "Training Epoch: 7 [34950/36450]\tLoss: 352.8079\n",
      "Training Epoch: 7 [35000/36450]\tLoss: 367.4590\n",
      "Training Epoch: 7 [35050/36450]\tLoss: 372.3455\n",
      "Training Epoch: 7 [35100/36450]\tLoss: 387.9166\n",
      "Training Epoch: 7 [35150/36450]\tLoss: 382.1195\n",
      "Training Epoch: 7 [35200/36450]\tLoss: 343.0766\n",
      "Training Epoch: 7 [35250/36450]\tLoss: 363.3484\n",
      "Training Epoch: 7 [35300/36450]\tLoss: 383.0591\n",
      "Training Epoch: 7 [35350/36450]\tLoss: 360.7696\n",
      "Training Epoch: 7 [35400/36450]\tLoss: 375.6345\n",
      "Training Epoch: 7 [35450/36450]\tLoss: 413.3837\n",
      "Training Epoch: 7 [35500/36450]\tLoss: 364.4179\n",
      "Training Epoch: 7 [35550/36450]\tLoss: 374.8040\n",
      "Training Epoch: 7 [35600/36450]\tLoss: 394.1823\n",
      "Training Epoch: 7 [35650/36450]\tLoss: 374.7076\n",
      "Training Epoch: 7 [35700/36450]\tLoss: 385.5875\n",
      "Training Epoch: 7 [35750/36450]\tLoss: 458.1671\n",
      "Training Epoch: 7 [35800/36450]\tLoss: 371.2661\n",
      "Training Epoch: 7 [35850/36450]\tLoss: 396.3592\n",
      "Training Epoch: 7 [35900/36450]\tLoss: 353.5279\n",
      "Training Epoch: 7 [35950/36450]\tLoss: 371.8352\n",
      "Training Epoch: 7 [36000/36450]\tLoss: 378.0735\n",
      "Training Epoch: 7 [36050/36450]\tLoss: 348.7323\n",
      "Training Epoch: 7 [36100/36450]\tLoss: 351.3780\n",
      "Training Epoch: 7 [36150/36450]\tLoss: 407.9586\n",
      "Training Epoch: 7 [36200/36450]\tLoss: 387.0377\n",
      "Training Epoch: 7 [36250/36450]\tLoss: 374.4842\n",
      "Training Epoch: 7 [36300/36450]\tLoss: 371.0477\n",
      "Training Epoch: 7 [36350/36450]\tLoss: 355.9931\n",
      "Training Epoch: 7 [36400/36450]\tLoss: 382.9889\n",
      "Training Epoch: 7 [36450/36450]\tLoss: 384.4642\n",
      "Training Epoch: 7 [4050/4050]\tLoss: 173.7220\n",
      "Training Epoch: 8 [50/36450]\tLoss: 351.5946\n",
      "Training Epoch: 8 [100/36450]\tLoss: 368.6049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [150/36450]\tLoss: 360.9014\n",
      "Training Epoch: 8 [200/36450]\tLoss: 360.3308\n",
      "Training Epoch: 8 [250/36450]\tLoss: 359.8363\n",
      "Training Epoch: 8 [300/36450]\tLoss: 360.4638\n",
      "Training Epoch: 8 [350/36450]\tLoss: 407.4501\n",
      "Training Epoch: 8 [400/36450]\tLoss: 355.9459\n",
      "Training Epoch: 8 [450/36450]\tLoss: 346.3855\n",
      "Training Epoch: 8 [500/36450]\tLoss: 366.3967\n",
      "Training Epoch: 8 [550/36450]\tLoss: 420.2977\n",
      "Training Epoch: 8 [600/36450]\tLoss: 378.9916\n",
      "Training Epoch: 8 [650/36450]\tLoss: 389.0998\n",
      "Training Epoch: 8 [700/36450]\tLoss: 390.4048\n",
      "Training Epoch: 8 [750/36450]\tLoss: 353.2825\n",
      "Training Epoch: 8 [800/36450]\tLoss: 391.6374\n",
      "Training Epoch: 8 [850/36450]\tLoss: 344.0870\n",
      "Training Epoch: 8 [900/36450]\tLoss: 363.8317\n",
      "Training Epoch: 8 [950/36450]\tLoss: 419.8562\n",
      "Training Epoch: 8 [1000/36450]\tLoss: 355.9916\n",
      "Training Epoch: 8 [1050/36450]\tLoss: 358.7548\n",
      "Training Epoch: 8 [1100/36450]\tLoss: 371.8988\n",
      "Training Epoch: 8 [1150/36450]\tLoss: 346.0331\n",
      "Training Epoch: 8 [1200/36450]\tLoss: 361.1793\n",
      "Training Epoch: 8 [1250/36450]\tLoss: 374.3552\n",
      "Training Epoch: 8 [1300/36450]\tLoss: 383.9174\n",
      "Training Epoch: 8 [1350/36450]\tLoss: 386.4997\n",
      "Training Epoch: 8 [1400/36450]\tLoss: 352.9064\n",
      "Training Epoch: 8 [1450/36450]\tLoss: 342.4347\n",
      "Training Epoch: 8 [1500/36450]\tLoss: 407.0185\n",
      "Training Epoch: 8 [1550/36450]\tLoss: 392.1436\n",
      "Training Epoch: 8 [1600/36450]\tLoss: 373.5431\n",
      "Training Epoch: 8 [1650/36450]\tLoss: 378.7188\n",
      "Training Epoch: 8 [1700/36450]\tLoss: 358.7246\n",
      "Training Epoch: 8 [1750/36450]\tLoss: 397.4660\n",
      "Training Epoch: 8 [1800/36450]\tLoss: 377.2589\n",
      "Training Epoch: 8 [1850/36450]\tLoss: 376.6238\n",
      "Training Epoch: 8 [1900/36450]\tLoss: 400.6356\n",
      "Training Epoch: 8 [1950/36450]\tLoss: 331.6956\n",
      "Training Epoch: 8 [2000/36450]\tLoss: 333.8103\n",
      "Training Epoch: 8 [2050/36450]\tLoss: 363.2304\n",
      "Training Epoch: 8 [2100/36450]\tLoss: 337.0844\n",
      "Training Epoch: 8 [2150/36450]\tLoss: 414.5370\n",
      "Training Epoch: 8 [2200/36450]\tLoss: 377.0708\n",
      "Training Epoch: 8 [2250/36450]\tLoss: 394.2124\n",
      "Training Epoch: 8 [2300/36450]\tLoss: 389.4153\n",
      "Training Epoch: 8 [2350/36450]\tLoss: 344.8487\n",
      "Training Epoch: 8 [2400/36450]\tLoss: 413.7668\n",
      "Training Epoch: 8 [2450/36450]\tLoss: 367.0797\n",
      "Training Epoch: 8 [2500/36450]\tLoss: 374.5593\n",
      "Training Epoch: 8 [2550/36450]\tLoss: 380.4660\n",
      "Training Epoch: 8 [2600/36450]\tLoss: 367.5151\n",
      "Training Epoch: 8 [2650/36450]\tLoss: 388.6157\n",
      "Training Epoch: 8 [2700/36450]\tLoss: 424.1711\n",
      "Training Epoch: 8 [2750/36450]\tLoss: 405.9980\n",
      "Training Epoch: 8 [2800/36450]\tLoss: 350.8626\n",
      "Training Epoch: 8 [2850/36450]\tLoss: 372.1642\n",
      "Training Epoch: 8 [2900/36450]\tLoss: 359.9023\n",
      "Training Epoch: 8 [2950/36450]\tLoss: 369.8008\n",
      "Training Epoch: 8 [3000/36450]\tLoss: 396.6545\n",
      "Training Epoch: 8 [3050/36450]\tLoss: 393.2780\n",
      "Training Epoch: 8 [3100/36450]\tLoss: 375.3613\n",
      "Training Epoch: 8 [3150/36450]\tLoss: 371.3862\n",
      "Training Epoch: 8 [3200/36450]\tLoss: 353.1011\n",
      "Training Epoch: 8 [3250/36450]\tLoss: 415.6241\n",
      "Training Epoch: 8 [3300/36450]\tLoss: 390.6688\n",
      "Training Epoch: 8 [3350/36450]\tLoss: 350.2877\n",
      "Training Epoch: 8 [3400/36450]\tLoss: 376.8600\n",
      "Training Epoch: 8 [3450/36450]\tLoss: 399.6709\n",
      "Training Epoch: 8 [3500/36450]\tLoss: 375.1726\n",
      "Training Epoch: 8 [3550/36450]\tLoss: 385.7638\n",
      "Training Epoch: 8 [3600/36450]\tLoss: 396.0561\n",
      "Training Epoch: 8 [3650/36450]\tLoss: 343.8033\n",
      "Training Epoch: 8 [3700/36450]\tLoss: 382.2843\n",
      "Training Epoch: 8 [3750/36450]\tLoss: 385.6205\n",
      "Training Epoch: 8 [3800/36450]\tLoss: 382.6970\n",
      "Training Epoch: 8 [3850/36450]\tLoss: 360.2191\n",
      "Training Epoch: 8 [3900/36450]\tLoss: 335.7132\n",
      "Training Epoch: 8 [3950/36450]\tLoss: 379.1114\n",
      "Training Epoch: 8 [4000/36450]\tLoss: 373.6086\n",
      "Training Epoch: 8 [4050/36450]\tLoss: 367.8266\n",
      "Training Epoch: 8 [4100/36450]\tLoss: 384.3688\n",
      "Training Epoch: 8 [4150/36450]\tLoss: 360.7172\n",
      "Training Epoch: 8 [4200/36450]\tLoss: 345.3288\n",
      "Training Epoch: 8 [4250/36450]\tLoss: 365.1966\n",
      "Training Epoch: 8 [4300/36450]\tLoss: 347.4789\n",
      "Training Epoch: 8 [4350/36450]\tLoss: 342.7223\n",
      "Training Epoch: 8 [4400/36450]\tLoss: 365.6603\n",
      "Training Epoch: 8 [4450/36450]\tLoss: 371.0320\n",
      "Training Epoch: 8 [4500/36450]\tLoss: 379.4807\n",
      "Training Epoch: 8 [4550/36450]\tLoss: 372.8455\n",
      "Training Epoch: 8 [4600/36450]\tLoss: 343.6869\n",
      "Training Epoch: 8 [4650/36450]\tLoss: 363.0255\n",
      "Training Epoch: 8 [4700/36450]\tLoss: 392.0844\n",
      "Training Epoch: 8 [4750/36450]\tLoss: 389.4171\n",
      "Training Epoch: 8 [4800/36450]\tLoss: 365.9067\n",
      "Training Epoch: 8 [4850/36450]\tLoss: 383.4300\n",
      "Training Epoch: 8 [4900/36450]\tLoss: 370.8773\n",
      "Training Epoch: 8 [4950/36450]\tLoss: 389.9266\n",
      "Training Epoch: 8 [5000/36450]\tLoss: 384.1566\n",
      "Training Epoch: 8 [5050/36450]\tLoss: 406.0344\n",
      "Training Epoch: 8 [5100/36450]\tLoss: 387.9168\n",
      "Training Epoch: 8 [5150/36450]\tLoss: 374.5005\n",
      "Training Epoch: 8 [5200/36450]\tLoss: 378.9451\n",
      "Training Epoch: 8 [5250/36450]\tLoss: 370.8758\n",
      "Training Epoch: 8 [5300/36450]\tLoss: 385.6590\n",
      "Training Epoch: 8 [5350/36450]\tLoss: 370.4468\n",
      "Training Epoch: 8 [5400/36450]\tLoss: 324.2299\n",
      "Training Epoch: 8 [5450/36450]\tLoss: 368.1893\n",
      "Training Epoch: 8 [5500/36450]\tLoss: 382.1455\n",
      "Training Epoch: 8 [5550/36450]\tLoss: 350.6143\n",
      "Training Epoch: 8 [5600/36450]\tLoss: 379.6277\n",
      "Training Epoch: 8 [5650/36450]\tLoss: 370.6974\n",
      "Training Epoch: 8 [5700/36450]\tLoss: 356.7116\n",
      "Training Epoch: 8 [5750/36450]\tLoss: 395.7435\n",
      "Training Epoch: 8 [5800/36450]\tLoss: 384.2505\n",
      "Training Epoch: 8 [5850/36450]\tLoss: 397.1820\n",
      "Training Epoch: 8 [5900/36450]\tLoss: 343.5652\n",
      "Training Epoch: 8 [5950/36450]\tLoss: 388.5920\n",
      "Training Epoch: 8 [6000/36450]\tLoss: 360.1627\n",
      "Training Epoch: 8 [6050/36450]\tLoss: 346.5233\n",
      "Training Epoch: 8 [6100/36450]\tLoss: 396.9060\n",
      "Training Epoch: 8 [6150/36450]\tLoss: 353.1178\n",
      "Training Epoch: 8 [6200/36450]\tLoss: 372.5650\n",
      "Training Epoch: 8 [6250/36450]\tLoss: 336.2564\n",
      "Training Epoch: 8 [6300/36450]\tLoss: 359.7310\n",
      "Training Epoch: 8 [6350/36450]\tLoss: 366.5322\n",
      "Training Epoch: 8 [6400/36450]\tLoss: 361.6895\n",
      "Training Epoch: 8 [6450/36450]\tLoss: 379.2732\n",
      "Training Epoch: 8 [6500/36450]\tLoss: 357.9110\n",
      "Training Epoch: 8 [6550/36450]\tLoss: 420.1898\n",
      "Training Epoch: 8 [6600/36450]\tLoss: 410.6949\n",
      "Training Epoch: 8 [6650/36450]\tLoss: 344.3288\n",
      "Training Epoch: 8 [6700/36450]\tLoss: 406.4457\n",
      "Training Epoch: 8 [6750/36450]\tLoss: 390.2919\n",
      "Training Epoch: 8 [6800/36450]\tLoss: 351.8517\n",
      "Training Epoch: 8 [6850/36450]\tLoss: 360.0856\n",
      "Training Epoch: 8 [6900/36450]\tLoss: 375.3029\n",
      "Training Epoch: 8 [6950/36450]\tLoss: 399.5424\n",
      "Training Epoch: 8 [7000/36450]\tLoss: 393.1509\n",
      "Training Epoch: 8 [7050/36450]\tLoss: 384.9583\n",
      "Training Epoch: 8 [7100/36450]\tLoss: 384.1351\n",
      "Training Epoch: 8 [7150/36450]\tLoss: 348.7666\n",
      "Training Epoch: 8 [7200/36450]\tLoss: 348.7694\n",
      "Training Epoch: 8 [7250/36450]\tLoss: 364.7440\n",
      "Training Epoch: 8 [7300/36450]\tLoss: 383.8389\n",
      "Training Epoch: 8 [7350/36450]\tLoss: 360.7527\n",
      "Training Epoch: 8 [7400/36450]\tLoss: 358.9588\n",
      "Training Epoch: 8 [7450/36450]\tLoss: 394.1180\n",
      "Training Epoch: 8 [7500/36450]\tLoss: 388.3960\n",
      "Training Epoch: 8 [7550/36450]\tLoss: 394.7177\n",
      "Training Epoch: 8 [7600/36450]\tLoss: 360.5366\n",
      "Training Epoch: 8 [7650/36450]\tLoss: 384.7773\n",
      "Training Epoch: 8 [7700/36450]\tLoss: 374.1590\n",
      "Training Epoch: 8 [7750/36450]\tLoss: 340.5702\n",
      "Training Epoch: 8 [7800/36450]\tLoss: 374.1072\n",
      "Training Epoch: 8 [7850/36450]\tLoss: 371.8643\n",
      "Training Epoch: 8 [7900/36450]\tLoss: 347.2035\n",
      "Training Epoch: 8 [7950/36450]\tLoss: 411.6315\n",
      "Training Epoch: 8 [8000/36450]\tLoss: 372.9975\n",
      "Training Epoch: 8 [8050/36450]\tLoss: 428.8764\n",
      "Training Epoch: 8 [8100/36450]\tLoss: 375.4028\n",
      "Training Epoch: 8 [8150/36450]\tLoss: 403.5416\n",
      "Training Epoch: 8 [8200/36450]\tLoss: 371.4591\n",
      "Training Epoch: 8 [8250/36450]\tLoss: 395.5093\n",
      "Training Epoch: 8 [8300/36450]\tLoss: 372.8927\n",
      "Training Epoch: 8 [8350/36450]\tLoss: 375.5352\n",
      "Training Epoch: 8 [8400/36450]\tLoss: 368.0085\n",
      "Training Epoch: 8 [8450/36450]\tLoss: 378.6162\n",
      "Training Epoch: 8 [8500/36450]\tLoss: 379.0897\n",
      "Training Epoch: 8 [8550/36450]\tLoss: 351.3359\n",
      "Training Epoch: 8 [8600/36450]\tLoss: 371.2589\n",
      "Training Epoch: 8 [8650/36450]\tLoss: 396.7516\n",
      "Training Epoch: 8 [8700/36450]\tLoss: 394.9068\n",
      "Training Epoch: 8 [8750/36450]\tLoss: 381.1863\n",
      "Training Epoch: 8 [8800/36450]\tLoss: 349.8875\n",
      "Training Epoch: 8 [8850/36450]\tLoss: 376.7524\n",
      "Training Epoch: 8 [8900/36450]\tLoss: 379.4969\n",
      "Training Epoch: 8 [8950/36450]\tLoss: 365.1791\n",
      "Training Epoch: 8 [9000/36450]\tLoss: 356.9195\n",
      "Training Epoch: 8 [9050/36450]\tLoss: 370.6937\n",
      "Training Epoch: 8 [9100/36450]\tLoss: 374.4146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [9150/36450]\tLoss: 377.9278\n",
      "Training Epoch: 8 [9200/36450]\tLoss: 378.3364\n",
      "Training Epoch: 8 [9250/36450]\tLoss: 376.2747\n",
      "Training Epoch: 8 [9300/36450]\tLoss: 396.3014\n",
      "Training Epoch: 8 [9350/36450]\tLoss: 361.9363\n",
      "Training Epoch: 8 [9400/36450]\tLoss: 397.4228\n",
      "Training Epoch: 8 [9450/36450]\tLoss: 376.2495\n",
      "Training Epoch: 8 [9500/36450]\tLoss: 339.8517\n",
      "Training Epoch: 8 [9550/36450]\tLoss: 380.1971\n",
      "Training Epoch: 8 [9600/36450]\tLoss: 373.4918\n",
      "Training Epoch: 8 [9650/36450]\tLoss: 341.3759\n",
      "Training Epoch: 8 [9700/36450]\tLoss: 367.5225\n",
      "Training Epoch: 8 [9750/36450]\tLoss: 409.6586\n",
      "Training Epoch: 8 [9800/36450]\tLoss: 365.0947\n",
      "Training Epoch: 8 [9850/36450]\tLoss: 329.9215\n",
      "Training Epoch: 8 [9900/36450]\tLoss: 356.6561\n",
      "Training Epoch: 8 [9950/36450]\tLoss: 350.6951\n",
      "Training Epoch: 8 [10000/36450]\tLoss: 409.2121\n",
      "Training Epoch: 8 [10050/36450]\tLoss: 366.1591\n",
      "Training Epoch: 8 [10100/36450]\tLoss: 396.6148\n",
      "Training Epoch: 8 [10150/36450]\tLoss: 357.6637\n",
      "Training Epoch: 8 [10200/36450]\tLoss: 383.0694\n",
      "Training Epoch: 8 [10250/36450]\tLoss: 369.2932\n",
      "Training Epoch: 8 [10300/36450]\tLoss: 353.2132\n",
      "Training Epoch: 8 [10350/36450]\tLoss: 375.8335\n",
      "Training Epoch: 8 [10400/36450]\tLoss: 380.5512\n",
      "Training Epoch: 8 [10450/36450]\tLoss: 398.4125\n",
      "Training Epoch: 8 [10500/36450]\tLoss: 347.8968\n",
      "Training Epoch: 8 [10550/36450]\tLoss: 384.4453\n",
      "Training Epoch: 8 [10600/36450]\tLoss: 359.1357\n",
      "Training Epoch: 8 [10650/36450]\tLoss: 399.7964\n",
      "Training Epoch: 8 [10700/36450]\tLoss: 379.5768\n",
      "Training Epoch: 8 [10750/36450]\tLoss: 370.0190\n",
      "Training Epoch: 8 [10800/36450]\tLoss: 430.7785\n",
      "Training Epoch: 8 [10850/36450]\tLoss: 378.4146\n",
      "Training Epoch: 8 [10900/36450]\tLoss: 374.8905\n",
      "Training Epoch: 8 [10950/36450]\tLoss: 332.9272\n",
      "Training Epoch: 8 [11000/36450]\tLoss: 347.3423\n",
      "Training Epoch: 8 [11050/36450]\tLoss: 353.6807\n",
      "Training Epoch: 8 [11100/36450]\tLoss: 454.4912\n",
      "Training Epoch: 8 [11150/36450]\tLoss: 413.8105\n",
      "Training Epoch: 8 [11200/36450]\tLoss: 377.3309\n",
      "Training Epoch: 8 [11250/36450]\tLoss: 368.8829\n",
      "Training Epoch: 8 [11300/36450]\tLoss: 356.0344\n",
      "Training Epoch: 8 [11350/36450]\tLoss: 358.2581\n",
      "Training Epoch: 8 [11400/36450]\tLoss: 368.5559\n",
      "Training Epoch: 8 [11450/36450]\tLoss: 401.4036\n",
      "Training Epoch: 8 [11500/36450]\tLoss: 368.1510\n",
      "Training Epoch: 8 [11550/36450]\tLoss: 382.5523\n",
      "Training Epoch: 8 [11600/36450]\tLoss: 386.7015\n",
      "Training Epoch: 8 [11650/36450]\tLoss: 361.4633\n",
      "Training Epoch: 8 [11700/36450]\tLoss: 358.4283\n",
      "Training Epoch: 8 [11750/36450]\tLoss: 362.5074\n",
      "Training Epoch: 8 [11800/36450]\tLoss: 396.4390\n",
      "Training Epoch: 8 [11850/36450]\tLoss: 382.2438\n",
      "Training Epoch: 8 [11900/36450]\tLoss: 357.7168\n",
      "Training Epoch: 8 [11950/36450]\tLoss: 337.3402\n",
      "Training Epoch: 8 [12000/36450]\tLoss: 396.3267\n",
      "Training Epoch: 8 [12050/36450]\tLoss: 380.7346\n",
      "Training Epoch: 8 [12100/36450]\tLoss: 364.9363\n",
      "Training Epoch: 8 [12150/36450]\tLoss: 381.4898\n",
      "Training Epoch: 8 [12200/36450]\tLoss: 364.8752\n",
      "Training Epoch: 8 [12250/36450]\tLoss: 369.9286\n",
      "Training Epoch: 8 [12300/36450]\tLoss: 421.5179\n",
      "Training Epoch: 8 [12350/36450]\tLoss: 404.2843\n",
      "Training Epoch: 8 [12400/36450]\tLoss: 353.5533\n",
      "Training Epoch: 8 [12450/36450]\tLoss: 364.1397\n",
      "Training Epoch: 8 [12500/36450]\tLoss: 355.9961\n",
      "Training Epoch: 8 [12550/36450]\tLoss: 354.0818\n",
      "Training Epoch: 8 [12600/36450]\tLoss: 398.9096\n",
      "Training Epoch: 8 [12650/36450]\tLoss: 409.9756\n",
      "Training Epoch: 8 [12700/36450]\tLoss: 350.6077\n",
      "Training Epoch: 8 [12750/36450]\tLoss: 417.0807\n",
      "Training Epoch: 8 [12800/36450]\tLoss: 355.0900\n",
      "Training Epoch: 8 [12850/36450]\tLoss: 372.3024\n",
      "Training Epoch: 8 [12900/36450]\tLoss: 330.2933\n",
      "Training Epoch: 8 [12950/36450]\tLoss: 363.0376\n",
      "Training Epoch: 8 [13000/36450]\tLoss: 341.6630\n",
      "Training Epoch: 8 [13050/36450]\tLoss: 368.1291\n",
      "Training Epoch: 8 [13100/36450]\tLoss: 385.5671\n",
      "Training Epoch: 8 [13150/36450]\tLoss: 348.5251\n",
      "Training Epoch: 8 [13200/36450]\tLoss: 340.2431\n",
      "Training Epoch: 8 [13250/36450]\tLoss: 347.7089\n",
      "Training Epoch: 8 [13300/36450]\tLoss: 412.2453\n",
      "Training Epoch: 8 [13350/36450]\tLoss: 360.5640\n",
      "Training Epoch: 8 [13400/36450]\tLoss: 369.1489\n",
      "Training Epoch: 8 [13450/36450]\tLoss: 381.2922\n",
      "Training Epoch: 8 [13500/36450]\tLoss: 344.4235\n",
      "Training Epoch: 8 [13550/36450]\tLoss: 363.4369\n",
      "Training Epoch: 8 [13600/36450]\tLoss: 413.0431\n",
      "Training Epoch: 8 [13650/36450]\tLoss: 381.5718\n",
      "Training Epoch: 8 [13700/36450]\tLoss: 353.2919\n",
      "Training Epoch: 8 [13750/36450]\tLoss: 376.0442\n",
      "Training Epoch: 8 [13800/36450]\tLoss: 377.8408\n",
      "Training Epoch: 8 [13850/36450]\tLoss: 398.7147\n",
      "Training Epoch: 8 [13900/36450]\tLoss: 410.7378\n",
      "Training Epoch: 8 [13950/36450]\tLoss: 390.3941\n",
      "Training Epoch: 8 [14000/36450]\tLoss: 368.4019\n",
      "Training Epoch: 8 [14050/36450]\tLoss: 364.6739\n",
      "Training Epoch: 8 [14100/36450]\tLoss: 396.8192\n",
      "Training Epoch: 8 [14150/36450]\tLoss: 353.5294\n",
      "Training Epoch: 8 [14200/36450]\tLoss: 364.2822\n",
      "Training Epoch: 8 [14250/36450]\tLoss: 348.6416\n",
      "Training Epoch: 8 [14300/36450]\tLoss: 356.7852\n",
      "Training Epoch: 8 [14350/36450]\tLoss: 368.0819\n",
      "Training Epoch: 8 [14400/36450]\tLoss: 387.0938\n",
      "Training Epoch: 8 [14450/36450]\tLoss: 370.4433\n",
      "Training Epoch: 8 [14500/36450]\tLoss: 368.7889\n",
      "Training Epoch: 8 [14550/36450]\tLoss: 358.6274\n",
      "Training Epoch: 8 [14600/36450]\tLoss: 379.9479\n",
      "Training Epoch: 8 [14650/36450]\tLoss: 355.9707\n",
      "Training Epoch: 8 [14700/36450]\tLoss: 411.1189\n",
      "Training Epoch: 8 [14750/36450]\tLoss: 395.3178\n",
      "Training Epoch: 8 [14800/36450]\tLoss: 354.1468\n",
      "Training Epoch: 8 [14850/36450]\tLoss: 381.9113\n",
      "Training Epoch: 8 [14900/36450]\tLoss: 378.6589\n",
      "Training Epoch: 8 [14950/36450]\tLoss: 364.4990\n",
      "Training Epoch: 8 [15000/36450]\tLoss: 381.5035\n",
      "Training Epoch: 8 [15050/36450]\tLoss: 391.4343\n",
      "Training Epoch: 8 [15100/36450]\tLoss: 394.3046\n",
      "Training Epoch: 8 [15150/36450]\tLoss: 349.8064\n",
      "Training Epoch: 8 [15200/36450]\tLoss: 368.3981\n",
      "Training Epoch: 8 [15250/36450]\tLoss: 370.5313\n",
      "Training Epoch: 8 [15300/36450]\tLoss: 333.2415\n",
      "Training Epoch: 8 [15350/36450]\tLoss: 394.4252\n",
      "Training Epoch: 8 [15400/36450]\tLoss: 382.4816\n",
      "Training Epoch: 8 [15450/36450]\tLoss: 356.1913\n",
      "Training Epoch: 8 [15500/36450]\tLoss: 362.6240\n",
      "Training Epoch: 8 [15550/36450]\tLoss: 371.0430\n",
      "Training Epoch: 8 [15600/36450]\tLoss: 404.0437\n",
      "Training Epoch: 8 [15650/36450]\tLoss: 372.1617\n",
      "Training Epoch: 8 [15700/36450]\tLoss: 372.0742\n",
      "Training Epoch: 8 [15750/36450]\tLoss: 402.7894\n",
      "Training Epoch: 8 [15800/36450]\tLoss: 394.9324\n",
      "Training Epoch: 8 [15850/36450]\tLoss: 364.4602\n",
      "Training Epoch: 8 [15900/36450]\tLoss: 377.4257\n",
      "Training Epoch: 8 [15950/36450]\tLoss: 404.8664\n",
      "Training Epoch: 8 [16000/36450]\tLoss: 390.7426\n",
      "Training Epoch: 8 [16050/36450]\tLoss: 384.1922\n",
      "Training Epoch: 8 [16100/36450]\tLoss: 410.4761\n",
      "Training Epoch: 8 [16150/36450]\tLoss: 377.5066\n",
      "Training Epoch: 8 [16200/36450]\tLoss: 376.5037\n",
      "Training Epoch: 8 [16250/36450]\tLoss: 323.1109\n",
      "Training Epoch: 8 [16300/36450]\tLoss: 340.2843\n",
      "Training Epoch: 8 [16350/36450]\tLoss: 378.3172\n",
      "Training Epoch: 8 [16400/36450]\tLoss: 410.0362\n",
      "Training Epoch: 8 [16450/36450]\tLoss: 336.4569\n",
      "Training Epoch: 8 [16500/36450]\tLoss: 406.3652\n",
      "Training Epoch: 8 [16550/36450]\tLoss: 386.1322\n",
      "Training Epoch: 8 [16600/36450]\tLoss: 390.6160\n",
      "Training Epoch: 8 [16650/36450]\tLoss: 370.6555\n",
      "Training Epoch: 8 [16700/36450]\tLoss: 386.6033\n",
      "Training Epoch: 8 [16750/36450]\tLoss: 356.6709\n",
      "Training Epoch: 8 [16800/36450]\tLoss: 329.4867\n",
      "Training Epoch: 8 [16850/36450]\tLoss: 361.6796\n",
      "Training Epoch: 8 [16900/36450]\tLoss: 367.7063\n",
      "Training Epoch: 8 [16950/36450]\tLoss: 380.2486\n",
      "Training Epoch: 8 [17000/36450]\tLoss: 377.4974\n",
      "Training Epoch: 8 [17050/36450]\tLoss: 353.5014\n",
      "Training Epoch: 8 [17100/36450]\tLoss: 373.4097\n",
      "Training Epoch: 8 [17150/36450]\tLoss: 369.3375\n",
      "Training Epoch: 8 [17200/36450]\tLoss: 364.1609\n",
      "Training Epoch: 8 [17250/36450]\tLoss: 383.0114\n",
      "Training Epoch: 8 [17300/36450]\tLoss: 368.3872\n",
      "Training Epoch: 8 [17350/36450]\tLoss: 363.9831\n",
      "Training Epoch: 8 [17400/36450]\tLoss: 367.1628\n",
      "Training Epoch: 8 [17450/36450]\tLoss: 341.2811\n",
      "Training Epoch: 8 [17500/36450]\tLoss: 408.2385\n",
      "Training Epoch: 8 [17550/36450]\tLoss: 412.2937\n",
      "Training Epoch: 8 [17600/36450]\tLoss: 379.7897\n",
      "Training Epoch: 8 [17650/36450]\tLoss: 340.3628\n",
      "Training Epoch: 8 [17700/36450]\tLoss: 326.2213\n",
      "Training Epoch: 8 [17750/36450]\tLoss: 357.7178\n",
      "Training Epoch: 8 [17800/36450]\tLoss: 367.8105\n",
      "Training Epoch: 8 [17850/36450]\tLoss: 361.1656\n",
      "Training Epoch: 8 [17900/36450]\tLoss: 375.9745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [17950/36450]\tLoss: 349.1034\n",
      "Training Epoch: 8 [18000/36450]\tLoss: 379.5248\n",
      "Training Epoch: 8 [18050/36450]\tLoss: 429.6830\n",
      "Training Epoch: 8 [18100/36450]\tLoss: 321.5656\n",
      "Training Epoch: 8 [18150/36450]\tLoss: 361.3284\n",
      "Training Epoch: 8 [18200/36450]\tLoss: 369.8040\n",
      "Training Epoch: 8 [18250/36450]\tLoss: 411.3943\n",
      "Training Epoch: 8 [18300/36450]\tLoss: 376.3753\n",
      "Training Epoch: 8 [18350/36450]\tLoss: 352.3208\n",
      "Training Epoch: 8 [18400/36450]\tLoss: 364.7570\n",
      "Training Epoch: 8 [18450/36450]\tLoss: 344.1204\n",
      "Training Epoch: 8 [18500/36450]\tLoss: 340.3481\n",
      "Training Epoch: 8 [18550/36450]\tLoss: 387.7151\n",
      "Training Epoch: 8 [18600/36450]\tLoss: 397.3371\n",
      "Training Epoch: 8 [18650/36450]\tLoss: 344.5251\n",
      "Training Epoch: 8 [18700/36450]\tLoss: 393.2369\n",
      "Training Epoch: 8 [18750/36450]\tLoss: 401.0393\n",
      "Training Epoch: 8 [18800/36450]\tLoss: 355.8474\n",
      "Training Epoch: 8 [18850/36450]\tLoss: 355.5159\n",
      "Training Epoch: 8 [18900/36450]\tLoss: 361.5785\n",
      "Training Epoch: 8 [18950/36450]\tLoss: 392.7904\n",
      "Training Epoch: 8 [19000/36450]\tLoss: 331.9416\n",
      "Training Epoch: 8 [19050/36450]\tLoss: 391.4423\n",
      "Training Epoch: 8 [19100/36450]\tLoss: 408.2554\n",
      "Training Epoch: 8 [19150/36450]\tLoss: 370.2241\n",
      "Training Epoch: 8 [19200/36450]\tLoss: 378.6640\n",
      "Training Epoch: 8 [19250/36450]\tLoss: 394.7895\n",
      "Training Epoch: 8 [19300/36450]\tLoss: 362.6932\n",
      "Training Epoch: 8 [19350/36450]\tLoss: 376.4325\n",
      "Training Epoch: 8 [19400/36450]\tLoss: 397.5084\n",
      "Training Epoch: 8 [19450/36450]\tLoss: 381.7432\n",
      "Training Epoch: 8 [19500/36450]\tLoss: 386.2600\n",
      "Training Epoch: 8 [19550/36450]\tLoss: 339.0977\n",
      "Training Epoch: 8 [19600/36450]\tLoss: 367.6555\n",
      "Training Epoch: 8 [19650/36450]\tLoss: 408.4835\n",
      "Training Epoch: 8 [19700/36450]\tLoss: 392.0334\n",
      "Training Epoch: 8 [19750/36450]\tLoss: 370.5582\n",
      "Training Epoch: 8 [19800/36450]\tLoss: 335.4389\n",
      "Training Epoch: 8 [19850/36450]\tLoss: 407.7960\n",
      "Training Epoch: 8 [19900/36450]\tLoss: 341.1414\n",
      "Training Epoch: 8 [19950/36450]\tLoss: 337.4026\n",
      "Training Epoch: 8 [20000/36450]\tLoss: 372.3107\n",
      "Training Epoch: 8 [20050/36450]\tLoss: 364.6652\n",
      "Training Epoch: 8 [20100/36450]\tLoss: 374.1797\n",
      "Training Epoch: 8 [20150/36450]\tLoss: 345.1929\n",
      "Training Epoch: 8 [20200/36450]\tLoss: 403.1590\n",
      "Training Epoch: 8 [20250/36450]\tLoss: 326.3788\n",
      "Training Epoch: 8 [20300/36450]\tLoss: 352.6837\n",
      "Training Epoch: 8 [20350/36450]\tLoss: 344.3699\n",
      "Training Epoch: 8 [20400/36450]\tLoss: 399.4122\n",
      "Training Epoch: 8 [20450/36450]\tLoss: 379.4951\n",
      "Training Epoch: 8 [20500/36450]\tLoss: 386.3704\n",
      "Training Epoch: 8 [20550/36450]\tLoss: 373.7199\n",
      "Training Epoch: 8 [20600/36450]\tLoss: 359.2479\n",
      "Training Epoch: 8 [20650/36450]\tLoss: 372.2717\n",
      "Training Epoch: 8 [20700/36450]\tLoss: 388.6081\n",
      "Training Epoch: 8 [20750/36450]\tLoss: 350.7279\n",
      "Training Epoch: 8 [20800/36450]\tLoss: 361.5909\n",
      "Training Epoch: 8 [20850/36450]\tLoss: 358.9926\n",
      "Training Epoch: 8 [20900/36450]\tLoss: 381.6092\n",
      "Training Epoch: 8 [20950/36450]\tLoss: 381.7110\n",
      "Training Epoch: 8 [21000/36450]\tLoss: 377.1515\n",
      "Training Epoch: 8 [21050/36450]\tLoss: 373.8239\n",
      "Training Epoch: 8 [21100/36450]\tLoss: 342.1208\n",
      "Training Epoch: 8 [21150/36450]\tLoss: 372.1538\n",
      "Training Epoch: 8 [21200/36450]\tLoss: 330.7675\n",
      "Training Epoch: 8 [21250/36450]\tLoss: 330.5216\n",
      "Training Epoch: 8 [21300/36450]\tLoss: 355.8607\n",
      "Training Epoch: 8 [21350/36450]\tLoss: 384.1458\n",
      "Training Epoch: 8 [21400/36450]\tLoss: 361.9814\n",
      "Training Epoch: 8 [21450/36450]\tLoss: 340.6868\n",
      "Training Epoch: 8 [21500/36450]\tLoss: 417.6599\n",
      "Training Epoch: 8 [21550/36450]\tLoss: 388.5965\n",
      "Training Epoch: 8 [21600/36450]\tLoss: 373.9206\n",
      "Training Epoch: 8 [21650/36450]\tLoss: 359.7447\n",
      "Training Epoch: 8 [21700/36450]\tLoss: 382.2335\n",
      "Training Epoch: 8 [21750/36450]\tLoss: 375.6539\n",
      "Training Epoch: 8 [21800/36450]\tLoss: 399.8375\n",
      "Training Epoch: 8 [21850/36450]\tLoss: 371.5946\n",
      "Training Epoch: 8 [21900/36450]\tLoss: 394.5430\n",
      "Training Epoch: 8 [21950/36450]\tLoss: 386.3345\n",
      "Training Epoch: 8 [22000/36450]\tLoss: 378.0572\n",
      "Training Epoch: 8 [22050/36450]\tLoss: 370.0719\n",
      "Training Epoch: 8 [22100/36450]\tLoss: 379.9821\n",
      "Training Epoch: 8 [22150/36450]\tLoss: 369.9518\n",
      "Training Epoch: 8 [22200/36450]\tLoss: 330.1302\n",
      "Training Epoch: 8 [22250/36450]\tLoss: 336.3714\n",
      "Training Epoch: 8 [22300/36450]\tLoss: 376.5547\n",
      "Training Epoch: 8 [22350/36450]\tLoss: 372.2549\n",
      "Training Epoch: 8 [22400/36450]\tLoss: 360.6606\n",
      "Training Epoch: 8 [22450/36450]\tLoss: 357.4131\n",
      "Training Epoch: 8 [22500/36450]\tLoss: 366.5768\n",
      "Training Epoch: 8 [22550/36450]\tLoss: 332.3426\n",
      "Training Epoch: 8 [22600/36450]\tLoss: 381.5857\n",
      "Training Epoch: 8 [22650/36450]\tLoss: 379.4940\n",
      "Training Epoch: 8 [22700/36450]\tLoss: 400.9613\n",
      "Training Epoch: 8 [22750/36450]\tLoss: 370.5094\n",
      "Training Epoch: 8 [22800/36450]\tLoss: 354.7631\n",
      "Training Epoch: 8 [22850/36450]\tLoss: 337.7552\n",
      "Training Epoch: 8 [22900/36450]\tLoss: 357.6105\n",
      "Training Epoch: 8 [22950/36450]\tLoss: 328.2917\n",
      "Training Epoch: 8 [23000/36450]\tLoss: 368.1554\n",
      "Training Epoch: 8 [23050/36450]\tLoss: 361.7785\n",
      "Training Epoch: 8 [23100/36450]\tLoss: 337.3770\n",
      "Training Epoch: 8 [23150/36450]\tLoss: 353.4388\n",
      "Training Epoch: 8 [23200/36450]\tLoss: 379.7656\n",
      "Training Epoch: 8 [23250/36450]\tLoss: 397.3502\n",
      "Training Epoch: 8 [23300/36450]\tLoss: 378.5127\n",
      "Training Epoch: 8 [23350/36450]\tLoss: 327.8809\n",
      "Training Epoch: 8 [23400/36450]\tLoss: 366.3341\n",
      "Training Epoch: 8 [23450/36450]\tLoss: 349.2244\n",
      "Training Epoch: 8 [23500/36450]\tLoss: 362.5517\n",
      "Training Epoch: 8 [23550/36450]\tLoss: 358.4120\n",
      "Training Epoch: 8 [23600/36450]\tLoss: 373.3250\n",
      "Training Epoch: 8 [23650/36450]\tLoss: 371.8188\n",
      "Training Epoch: 8 [23700/36450]\tLoss: 346.4185\n",
      "Training Epoch: 8 [23750/36450]\tLoss: 340.8870\n",
      "Training Epoch: 8 [23800/36450]\tLoss: 399.1439\n",
      "Training Epoch: 8 [23850/36450]\tLoss: 370.3091\n",
      "Training Epoch: 8 [23900/36450]\tLoss: 326.6372\n",
      "Training Epoch: 8 [23950/36450]\tLoss: 393.2206\n",
      "Training Epoch: 8 [24000/36450]\tLoss: 361.3028\n",
      "Training Epoch: 8 [24050/36450]\tLoss: 328.3095\n",
      "Training Epoch: 8 [24100/36450]\tLoss: 355.8225\n",
      "Training Epoch: 8 [24150/36450]\tLoss: 315.6237\n",
      "Training Epoch: 8 [24200/36450]\tLoss: 367.7677\n",
      "Training Epoch: 8 [24250/36450]\tLoss: 377.3347\n",
      "Training Epoch: 8 [24300/36450]\tLoss: 338.0754\n",
      "Training Epoch: 8 [24350/36450]\tLoss: 403.8043\n",
      "Training Epoch: 8 [24400/36450]\tLoss: 375.3915\n",
      "Training Epoch: 8 [24450/36450]\tLoss: 337.9006\n",
      "Training Epoch: 8 [24500/36450]\tLoss: 369.2494\n",
      "Training Epoch: 8 [24550/36450]\tLoss: 374.2098\n",
      "Training Epoch: 8 [24600/36450]\tLoss: 349.3742\n",
      "Training Epoch: 8 [24650/36450]\tLoss: 355.6259\n",
      "Training Epoch: 8 [24700/36450]\tLoss: 357.7966\n",
      "Training Epoch: 8 [24750/36450]\tLoss: 362.2039\n",
      "Training Epoch: 8 [24800/36450]\tLoss: 346.2449\n",
      "Training Epoch: 8 [24850/36450]\tLoss: 349.0586\n",
      "Training Epoch: 8 [24900/36450]\tLoss: 380.4324\n",
      "Training Epoch: 8 [24950/36450]\tLoss: 359.4384\n",
      "Training Epoch: 8 [25000/36450]\tLoss: 354.0984\n",
      "Training Epoch: 8 [25050/36450]\tLoss: 368.3900\n",
      "Training Epoch: 8 [25100/36450]\tLoss: 389.3711\n",
      "Training Epoch: 8 [25150/36450]\tLoss: 364.5055\n",
      "Training Epoch: 8 [25200/36450]\tLoss: 398.7057\n",
      "Training Epoch: 8 [25250/36450]\tLoss: 353.2650\n",
      "Training Epoch: 8 [25300/36450]\tLoss: 359.9857\n",
      "Training Epoch: 8 [25350/36450]\tLoss: 351.6882\n",
      "Training Epoch: 8 [25400/36450]\tLoss: 385.0225\n",
      "Training Epoch: 8 [25450/36450]\tLoss: 353.6262\n",
      "Training Epoch: 8 [25500/36450]\tLoss: 349.8157\n",
      "Training Epoch: 8 [25550/36450]\tLoss: 328.2095\n",
      "Training Epoch: 8 [25600/36450]\tLoss: 367.7448\n",
      "Training Epoch: 8 [25650/36450]\tLoss: 371.0784\n",
      "Training Epoch: 8 [25700/36450]\tLoss: 345.2124\n",
      "Training Epoch: 8 [25750/36450]\tLoss: 366.7047\n",
      "Training Epoch: 8 [25800/36450]\tLoss: 357.9854\n",
      "Training Epoch: 8 [25850/36450]\tLoss: 336.0067\n",
      "Training Epoch: 8 [25900/36450]\tLoss: 337.3172\n",
      "Training Epoch: 8 [25950/36450]\tLoss: 396.9823\n",
      "Training Epoch: 8 [26000/36450]\tLoss: 351.5963\n",
      "Training Epoch: 8 [26050/36450]\tLoss: 388.9981\n",
      "Training Epoch: 8 [26100/36450]\tLoss: 356.3301\n",
      "Training Epoch: 8 [26150/36450]\tLoss: 344.8394\n",
      "Training Epoch: 8 [26200/36450]\tLoss: 328.0191\n",
      "Training Epoch: 8 [26250/36450]\tLoss: 336.6483\n",
      "Training Epoch: 8 [26300/36450]\tLoss: 386.2647\n",
      "Training Epoch: 8 [26350/36450]\tLoss: 330.5942\n",
      "Training Epoch: 8 [26400/36450]\tLoss: 369.0086\n",
      "Training Epoch: 8 [26450/36450]\tLoss: 401.5885\n",
      "Training Epoch: 8 [26500/36450]\tLoss: 381.8117\n",
      "Training Epoch: 8 [26550/36450]\tLoss: 330.7910\n",
      "Training Epoch: 8 [26600/36450]\tLoss: 332.5069\n",
      "Training Epoch: 8 [26650/36450]\tLoss: 394.0540\n",
      "Training Epoch: 8 [26700/36450]\tLoss: 396.9524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [26750/36450]\tLoss: 367.8198\n",
      "Training Epoch: 8 [26800/36450]\tLoss: 343.4922\n",
      "Training Epoch: 8 [26850/36450]\tLoss: 375.9127\n",
      "Training Epoch: 8 [26900/36450]\tLoss: 370.9811\n",
      "Training Epoch: 8 [26950/36450]\tLoss: 373.3600\n",
      "Training Epoch: 8 [27000/36450]\tLoss: 344.0076\n",
      "Training Epoch: 8 [27050/36450]\tLoss: 377.3595\n",
      "Training Epoch: 8 [27100/36450]\tLoss: 387.1175\n",
      "Training Epoch: 8 [27150/36450]\tLoss: 379.5851\n",
      "Training Epoch: 8 [27200/36450]\tLoss: 383.0186\n",
      "Training Epoch: 8 [27250/36450]\tLoss: 330.4539\n",
      "Training Epoch: 8 [27300/36450]\tLoss: 361.1277\n",
      "Training Epoch: 8 [27350/36450]\tLoss: 366.4926\n",
      "Training Epoch: 8 [27400/36450]\tLoss: 362.8041\n",
      "Training Epoch: 8 [27450/36450]\tLoss: 344.3589\n",
      "Training Epoch: 8 [27500/36450]\tLoss: 343.8816\n",
      "Training Epoch: 8 [27550/36450]\tLoss: 369.5201\n",
      "Training Epoch: 8 [27600/36450]\tLoss: 353.1656\n",
      "Training Epoch: 8 [27650/36450]\tLoss: 354.6327\n",
      "Training Epoch: 8 [27700/36450]\tLoss: 387.7417\n",
      "Training Epoch: 8 [27750/36450]\tLoss: 351.8202\n",
      "Training Epoch: 8 [27800/36450]\tLoss: 379.7572\n",
      "Training Epoch: 8 [27850/36450]\tLoss: 389.0417\n",
      "Training Epoch: 8 [27900/36450]\tLoss: 384.7471\n",
      "Training Epoch: 8 [27950/36450]\tLoss: 385.0410\n",
      "Training Epoch: 8 [28000/36450]\tLoss: 346.4926\n",
      "Training Epoch: 8 [28050/36450]\tLoss: 390.0493\n",
      "Training Epoch: 8 [28100/36450]\tLoss: 352.9002\n",
      "Training Epoch: 8 [28150/36450]\tLoss: 354.7633\n",
      "Training Epoch: 8 [28200/36450]\tLoss: 341.6308\n",
      "Training Epoch: 8 [28250/36450]\tLoss: 379.2669\n",
      "Training Epoch: 8 [28300/36450]\tLoss: 351.6441\n",
      "Training Epoch: 8 [28350/36450]\tLoss: 358.3572\n",
      "Training Epoch: 8 [28400/36450]\tLoss: 340.2067\n",
      "Training Epoch: 8 [28450/36450]\tLoss: 388.5901\n",
      "Training Epoch: 8 [28500/36450]\tLoss: 352.4103\n",
      "Training Epoch: 8 [28550/36450]\tLoss: 344.6885\n",
      "Training Epoch: 8 [28600/36450]\tLoss: 365.5649\n",
      "Training Epoch: 8 [28650/36450]\tLoss: 391.1791\n",
      "Training Epoch: 8 [28700/36450]\tLoss: 335.6682\n",
      "Training Epoch: 8 [28750/36450]\tLoss: 399.7683\n",
      "Training Epoch: 8 [28800/36450]\tLoss: 387.0262\n",
      "Training Epoch: 8 [28850/36450]\tLoss: 365.5245\n",
      "Training Epoch: 8 [28900/36450]\tLoss: 364.3551\n",
      "Training Epoch: 8 [28950/36450]\tLoss: 378.3800\n",
      "Training Epoch: 8 [29000/36450]\tLoss: 353.1929\n",
      "Training Epoch: 8 [29050/36450]\tLoss: 420.4021\n",
      "Training Epoch: 8 [29100/36450]\tLoss: 362.2045\n",
      "Training Epoch: 8 [29150/36450]\tLoss: 326.6189\n",
      "Training Epoch: 8 [29200/36450]\tLoss: 361.6385\n",
      "Training Epoch: 8 [29250/36450]\tLoss: 329.0094\n",
      "Training Epoch: 8 [29300/36450]\tLoss: 353.3578\n",
      "Training Epoch: 8 [29350/36450]\tLoss: 355.0008\n",
      "Training Epoch: 8 [29400/36450]\tLoss: 326.5214\n",
      "Training Epoch: 8 [29450/36450]\tLoss: 361.3055\n",
      "Training Epoch: 8 [29500/36450]\tLoss: 395.8540\n",
      "Training Epoch: 8 [29550/36450]\tLoss: 390.0099\n",
      "Training Epoch: 8 [29600/36450]\tLoss: 421.4343\n",
      "Training Epoch: 8 [29650/36450]\tLoss: 359.1574\n",
      "Training Epoch: 8 [29700/36450]\tLoss: 361.1579\n",
      "Training Epoch: 8 [29750/36450]\tLoss: 350.1860\n",
      "Training Epoch: 8 [29800/36450]\tLoss: 363.5419\n",
      "Training Epoch: 8 [29850/36450]\tLoss: 368.6800\n",
      "Training Epoch: 8 [29900/36450]\tLoss: 355.1857\n",
      "Training Epoch: 8 [29950/36450]\tLoss: 353.4893\n",
      "Training Epoch: 8 [30000/36450]\tLoss: 322.9114\n",
      "Training Epoch: 8 [30050/36450]\tLoss: 410.8923\n",
      "Training Epoch: 8 [30100/36450]\tLoss: 362.5190\n",
      "Training Epoch: 8 [30150/36450]\tLoss: 384.5323\n",
      "Training Epoch: 8 [30200/36450]\tLoss: 363.0532\n",
      "Training Epoch: 8 [30250/36450]\tLoss: 342.1974\n",
      "Training Epoch: 8 [30300/36450]\tLoss: 340.0046\n",
      "Training Epoch: 8 [30350/36450]\tLoss: 359.3881\n",
      "Training Epoch: 8 [30400/36450]\tLoss: 368.1016\n",
      "Training Epoch: 8 [30450/36450]\tLoss: 341.0308\n",
      "Training Epoch: 8 [30500/36450]\tLoss: 365.8596\n",
      "Training Epoch: 8 [30550/36450]\tLoss: 404.0150\n",
      "Training Epoch: 8 [30600/36450]\tLoss: 380.8445\n",
      "Training Epoch: 8 [30650/36450]\tLoss: 358.2854\n",
      "Training Epoch: 8 [30700/36450]\tLoss: 348.1106\n",
      "Training Epoch: 8 [30750/36450]\tLoss: 380.9222\n",
      "Training Epoch: 8 [30800/36450]\tLoss: 366.8377\n",
      "Training Epoch: 8 [30850/36450]\tLoss: 353.2063\n",
      "Training Epoch: 8 [30900/36450]\tLoss: 388.6945\n",
      "Training Epoch: 8 [30950/36450]\tLoss: 384.0019\n",
      "Training Epoch: 8 [31000/36450]\tLoss: 350.7653\n",
      "Training Epoch: 8 [31050/36450]\tLoss: 354.0993\n",
      "Training Epoch: 8 [31100/36450]\tLoss: 361.0558\n",
      "Training Epoch: 8 [31150/36450]\tLoss: 339.8191\n",
      "Training Epoch: 8 [31200/36450]\tLoss: 360.0222\n",
      "Training Epoch: 8 [31250/36450]\tLoss: 371.5946\n",
      "Training Epoch: 8 [31300/36450]\tLoss: 353.1976\n",
      "Training Epoch: 8 [31350/36450]\tLoss: 368.4101\n",
      "Training Epoch: 8 [31400/36450]\tLoss: 358.9852\n",
      "Training Epoch: 8 [31450/36450]\tLoss: 370.6981\n",
      "Training Epoch: 8 [31500/36450]\tLoss: 348.4936\n",
      "Training Epoch: 8 [31550/36450]\tLoss: 392.8405\n",
      "Training Epoch: 8 [31600/36450]\tLoss: 393.4818\n",
      "Training Epoch: 8 [31650/36450]\tLoss: 362.2325\n",
      "Training Epoch: 8 [31700/36450]\tLoss: 341.3161\n",
      "Training Epoch: 8 [31750/36450]\tLoss: 396.1997\n",
      "Training Epoch: 8 [31800/36450]\tLoss: 305.5462\n",
      "Training Epoch: 8 [31850/36450]\tLoss: 414.5220\n",
      "Training Epoch: 8 [31900/36450]\tLoss: 375.2874\n",
      "Training Epoch: 8 [31950/36450]\tLoss: 404.0299\n",
      "Training Epoch: 8 [32000/36450]\tLoss: 333.5201\n",
      "Training Epoch: 8 [32050/36450]\tLoss: 362.4775\n",
      "Training Epoch: 8 [32100/36450]\tLoss: 316.3721\n",
      "Training Epoch: 8 [32150/36450]\tLoss: 374.7457\n",
      "Training Epoch: 8 [32200/36450]\tLoss: 365.7769\n",
      "Training Epoch: 8 [32250/36450]\tLoss: 350.1604\n",
      "Training Epoch: 8 [32300/36450]\tLoss: 342.6705\n",
      "Training Epoch: 8 [32350/36450]\tLoss: 364.9911\n",
      "Training Epoch: 8 [32400/36450]\tLoss: 335.5982\n",
      "Training Epoch: 8 [32450/36450]\tLoss: 354.4843\n",
      "Training Epoch: 8 [32500/36450]\tLoss: 384.8023\n",
      "Training Epoch: 8 [32550/36450]\tLoss: 384.5969\n",
      "Training Epoch: 8 [32600/36450]\tLoss: 350.6486\n",
      "Training Epoch: 8 [32650/36450]\tLoss: 370.8219\n",
      "Training Epoch: 8 [32700/36450]\tLoss: 314.0674\n",
      "Training Epoch: 8 [32750/36450]\tLoss: 355.2888\n",
      "Training Epoch: 8 [32800/36450]\tLoss: 366.4912\n",
      "Training Epoch: 8 [32850/36450]\tLoss: 325.9632\n",
      "Training Epoch: 8 [32900/36450]\tLoss: 321.9788\n",
      "Training Epoch: 8 [32950/36450]\tLoss: 387.3400\n",
      "Training Epoch: 8 [33000/36450]\tLoss: 335.1621\n",
      "Training Epoch: 8 [33050/36450]\tLoss: 372.0606\n",
      "Training Epoch: 8 [33100/36450]\tLoss: 353.2276\n",
      "Training Epoch: 8 [33150/36450]\tLoss: 355.7052\n",
      "Training Epoch: 8 [33200/36450]\tLoss: 336.7947\n",
      "Training Epoch: 8 [33250/36450]\tLoss: 340.9867\n",
      "Training Epoch: 8 [33300/36450]\tLoss: 369.9604\n",
      "Training Epoch: 8 [33350/36450]\tLoss: 362.2096\n",
      "Training Epoch: 8 [33400/36450]\tLoss: 346.0279\n",
      "Training Epoch: 8 [33450/36450]\tLoss: 368.1558\n",
      "Training Epoch: 8 [33500/36450]\tLoss: 358.1619\n",
      "Training Epoch: 8 [33550/36450]\tLoss: 348.0926\n",
      "Training Epoch: 8 [33600/36450]\tLoss: 378.2109\n",
      "Training Epoch: 8 [33650/36450]\tLoss: 388.1087\n",
      "Training Epoch: 8 [33700/36450]\tLoss: 319.0074\n",
      "Training Epoch: 8 [33750/36450]\tLoss: 382.9881\n",
      "Training Epoch: 8 [33800/36450]\tLoss: 349.1152\n",
      "Training Epoch: 8 [33850/36450]\tLoss: 361.0015\n",
      "Training Epoch: 8 [33900/36450]\tLoss: 370.6455\n",
      "Training Epoch: 8 [33950/36450]\tLoss: 366.0471\n",
      "Training Epoch: 8 [34000/36450]\tLoss: 337.8361\n",
      "Training Epoch: 8 [34050/36450]\tLoss: 350.1758\n",
      "Training Epoch: 8 [34100/36450]\tLoss: 325.4884\n",
      "Training Epoch: 8 [34150/36450]\tLoss: 364.7209\n",
      "Training Epoch: 8 [34200/36450]\tLoss: 344.9357\n",
      "Training Epoch: 8 [34250/36450]\tLoss: 370.0978\n",
      "Training Epoch: 8 [34300/36450]\tLoss: 352.8499\n",
      "Training Epoch: 8 [34350/36450]\tLoss: 352.2028\n",
      "Training Epoch: 8 [34400/36450]\tLoss: 360.8584\n",
      "Training Epoch: 8 [34450/36450]\tLoss: 384.6858\n",
      "Training Epoch: 8 [34500/36450]\tLoss: 345.5872\n",
      "Training Epoch: 8 [34550/36450]\tLoss: 383.7929\n",
      "Training Epoch: 8 [34600/36450]\tLoss: 380.2421\n",
      "Training Epoch: 8 [34650/36450]\tLoss: 345.6249\n",
      "Training Epoch: 8 [34700/36450]\tLoss: 372.6798\n",
      "Training Epoch: 8 [34750/36450]\tLoss: 325.7919\n",
      "Training Epoch: 8 [34800/36450]\tLoss: 416.6946\n",
      "Training Epoch: 8 [34850/36450]\tLoss: 390.4579\n",
      "Training Epoch: 8 [34900/36450]\tLoss: 359.3547\n",
      "Training Epoch: 8 [34950/36450]\tLoss: 360.4865\n",
      "Training Epoch: 8 [35000/36450]\tLoss: 366.6917\n",
      "Training Epoch: 8 [35050/36450]\tLoss: 326.0806\n",
      "Training Epoch: 8 [35100/36450]\tLoss: 335.6614\n",
      "Training Epoch: 8 [35150/36450]\tLoss: 400.0879\n",
      "Training Epoch: 8 [35200/36450]\tLoss: 344.9738\n",
      "Training Epoch: 8 [35250/36450]\tLoss: 363.6829\n",
      "Training Epoch: 8 [35300/36450]\tLoss: 334.5141\n",
      "Training Epoch: 8 [35350/36450]\tLoss: 401.1692\n",
      "Training Epoch: 8 [35400/36450]\tLoss: 320.1064\n",
      "Training Epoch: 8 [35450/36450]\tLoss: 360.9818\n",
      "Training Epoch: 8 [35500/36450]\tLoss: 350.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [35550/36450]\tLoss: 380.8297\n",
      "Training Epoch: 8 [35600/36450]\tLoss: 361.0313\n",
      "Training Epoch: 8 [35650/36450]\tLoss: 347.9636\n",
      "Training Epoch: 8 [35700/36450]\tLoss: 352.1636\n",
      "Training Epoch: 8 [35750/36450]\tLoss: 378.7878\n",
      "Training Epoch: 8 [35800/36450]\tLoss: 354.2153\n",
      "Training Epoch: 8 [35850/36450]\tLoss: 369.1340\n",
      "Training Epoch: 8 [35900/36450]\tLoss: 337.2881\n",
      "Training Epoch: 8 [35950/36450]\tLoss: 352.1578\n",
      "Training Epoch: 8 [36000/36450]\tLoss: 356.8338\n",
      "Training Epoch: 8 [36050/36450]\tLoss: 354.7770\n",
      "Training Epoch: 8 [36100/36450]\tLoss: 323.3962\n",
      "Training Epoch: 8 [36150/36450]\tLoss: 335.8556\n",
      "Training Epoch: 8 [36200/36450]\tLoss: 355.9166\n",
      "Training Epoch: 8 [36250/36450]\tLoss: 356.6859\n",
      "Training Epoch: 8 [36300/36450]\tLoss: 373.8029\n",
      "Training Epoch: 8 [36350/36450]\tLoss: 331.7831\n",
      "Training Epoch: 8 [36400/36450]\tLoss: 369.3821\n",
      "Training Epoch: 8 [36450/36450]\tLoss: 352.7587\n",
      "Training Epoch: 8 [4050/4050]\tLoss: 163.6308\n",
      "Training Epoch: 9 [50/36450]\tLoss: 340.6310\n",
      "Training Epoch: 9 [100/36450]\tLoss: 367.2683\n",
      "Training Epoch: 9 [150/36450]\tLoss: 349.5103\n",
      "Training Epoch: 9 [200/36450]\tLoss: 353.0705\n",
      "Training Epoch: 9 [250/36450]\tLoss: 375.3851\n",
      "Training Epoch: 9 [300/36450]\tLoss: 337.7604\n",
      "Training Epoch: 9 [350/36450]\tLoss: 359.9019\n",
      "Training Epoch: 9 [400/36450]\tLoss: 328.4294\n",
      "Training Epoch: 9 [450/36450]\tLoss: 336.0421\n",
      "Training Epoch: 9 [500/36450]\tLoss: 369.6375\n",
      "Training Epoch: 9 [550/36450]\tLoss: 352.1575\n",
      "Training Epoch: 9 [600/36450]\tLoss: 395.2294\n",
      "Training Epoch: 9 [650/36450]\tLoss: 360.2804\n",
      "Training Epoch: 9 [700/36450]\tLoss: 345.9196\n",
      "Training Epoch: 9 [750/36450]\tLoss: 365.9017\n",
      "Training Epoch: 9 [800/36450]\tLoss: 331.9604\n",
      "Training Epoch: 9 [850/36450]\tLoss: 376.9556\n",
      "Training Epoch: 9 [900/36450]\tLoss: 379.4388\n",
      "Training Epoch: 9 [950/36450]\tLoss: 344.2583\n",
      "Training Epoch: 9 [1000/36450]\tLoss: 355.2675\n",
      "Training Epoch: 9 [1050/36450]\tLoss: 378.0537\n",
      "Training Epoch: 9 [1100/36450]\tLoss: 368.2132\n",
      "Training Epoch: 9 [1150/36450]\tLoss: 364.5839\n",
      "Training Epoch: 9 [1200/36450]\tLoss: 358.7048\n",
      "Training Epoch: 9 [1250/36450]\tLoss: 350.7033\n",
      "Training Epoch: 9 [1300/36450]\tLoss: 330.7180\n",
      "Training Epoch: 9 [1350/36450]\tLoss: 371.1122\n",
      "Training Epoch: 9 [1400/36450]\tLoss: 340.8086\n",
      "Training Epoch: 9 [1450/36450]\tLoss: 379.0068\n",
      "Training Epoch: 9 [1500/36450]\tLoss: 335.0010\n",
      "Training Epoch: 9 [1550/36450]\tLoss: 382.0255\n",
      "Training Epoch: 9 [1600/36450]\tLoss: 343.4426\n",
      "Training Epoch: 9 [1650/36450]\tLoss: 345.0208\n",
      "Training Epoch: 9 [1700/36450]\tLoss: 373.3592\n",
      "Training Epoch: 9 [1750/36450]\tLoss: 334.0853\n",
      "Training Epoch: 9 [1800/36450]\tLoss: 368.3076\n",
      "Training Epoch: 9 [1850/36450]\tLoss: 367.8570\n",
      "Training Epoch: 9 [1900/36450]\tLoss: 351.7816\n",
      "Training Epoch: 9 [1950/36450]\tLoss: 360.8982\n",
      "Training Epoch: 9 [2000/36450]\tLoss: 350.1860\n",
      "Training Epoch: 9 [2050/36450]\tLoss: 326.2838\n",
      "Training Epoch: 9 [2100/36450]\tLoss: 342.5485\n",
      "Training Epoch: 9 [2150/36450]\tLoss: 365.2632\n",
      "Training Epoch: 9 [2200/36450]\tLoss: 377.3657\n",
      "Training Epoch: 9 [2250/36450]\tLoss: 350.3484\n",
      "Training Epoch: 9 [2300/36450]\tLoss: 369.7884\n",
      "Training Epoch: 9 [2350/36450]\tLoss: 354.5013\n",
      "Training Epoch: 9 [2400/36450]\tLoss: 341.5036\n",
      "Training Epoch: 9 [2450/36450]\tLoss: 382.6738\n",
      "Training Epoch: 9 [2500/36450]\tLoss: 373.7560\n",
      "Training Epoch: 9 [2550/36450]\tLoss: 311.6332\n",
      "Training Epoch: 9 [2600/36450]\tLoss: 328.9486\n",
      "Training Epoch: 9 [2650/36450]\tLoss: 352.0390\n",
      "Training Epoch: 9 [2700/36450]\tLoss: 331.8481\n",
      "Training Epoch: 9 [2750/36450]\tLoss: 366.7032\n",
      "Training Epoch: 9 [2800/36450]\tLoss: 361.3339\n",
      "Training Epoch: 9 [2850/36450]\tLoss: 370.2686\n",
      "Training Epoch: 9 [2900/36450]\tLoss: 360.8281\n",
      "Training Epoch: 9 [2950/36450]\tLoss: 379.0462\n",
      "Training Epoch: 9 [3000/36450]\tLoss: 344.2332\n",
      "Training Epoch: 9 [3050/36450]\tLoss: 347.7310\n",
      "Training Epoch: 9 [3100/36450]\tLoss: 371.1591\n",
      "Training Epoch: 9 [3150/36450]\tLoss: 361.4762\n",
      "Training Epoch: 9 [3200/36450]\tLoss: 353.1603\n",
      "Training Epoch: 9 [3250/36450]\tLoss: 336.4573\n",
      "Training Epoch: 9 [3300/36450]\tLoss: 369.5822\n",
      "Training Epoch: 9 [3350/36450]\tLoss: 342.6224\n",
      "Training Epoch: 9 [3400/36450]\tLoss: 351.5261\n",
      "Training Epoch: 9 [3450/36450]\tLoss: 348.3249\n",
      "Training Epoch: 9 [3500/36450]\tLoss: 354.5319\n",
      "Training Epoch: 9 [3550/36450]\tLoss: 342.6233\n",
      "Training Epoch: 9 [3600/36450]\tLoss: 369.1471\n",
      "Training Epoch: 9 [3650/36450]\tLoss: 374.8823\n",
      "Training Epoch: 9 [3700/36450]\tLoss: 361.7944\n",
      "Training Epoch: 9 [3750/36450]\tLoss: 355.2846\n",
      "Training Epoch: 9 [3800/36450]\tLoss: 374.6013\n",
      "Training Epoch: 9 [3850/36450]\tLoss: 345.1469\n",
      "Training Epoch: 9 [3900/36450]\tLoss: 389.7164\n",
      "Training Epoch: 9 [3950/36450]\tLoss: 335.7981\n",
      "Training Epoch: 9 [4000/36450]\tLoss: 326.0282\n",
      "Training Epoch: 9 [4050/36450]\tLoss: 353.1919\n",
      "Training Epoch: 9 [4100/36450]\tLoss: 370.9445\n",
      "Training Epoch: 9 [4150/36450]\tLoss: 373.8814\n",
      "Training Epoch: 9 [4200/36450]\tLoss: 355.1111\n",
      "Training Epoch: 9 [4250/36450]\tLoss: 375.4645\n",
      "Training Epoch: 9 [4300/36450]\tLoss: 345.0221\n",
      "Training Epoch: 9 [4350/36450]\tLoss: 337.1227\n",
      "Training Epoch: 9 [4400/36450]\tLoss: 339.6735\n",
      "Training Epoch: 9 [4450/36450]\tLoss: 350.0302\n",
      "Training Epoch: 9 [4500/36450]\tLoss: 371.0343\n",
      "Training Epoch: 9 [4550/36450]\tLoss: 357.8320\n",
      "Training Epoch: 9 [4600/36450]\tLoss: 355.3604\n",
      "Training Epoch: 9 [4650/36450]\tLoss: 353.2149\n",
      "Training Epoch: 9 [4700/36450]\tLoss: 367.3820\n",
      "Training Epoch: 9 [4750/36450]\tLoss: 355.7621\n",
      "Training Epoch: 9 [4800/36450]\tLoss: 353.1021\n",
      "Training Epoch: 9 [4850/36450]\tLoss: 324.9416\n",
      "Training Epoch: 9 [4900/36450]\tLoss: 349.8719\n",
      "Training Epoch: 9 [4950/36450]\tLoss: 344.7673\n",
      "Training Epoch: 9 [5000/36450]\tLoss: 437.1460\n",
      "Training Epoch: 9 [5050/36450]\tLoss: 353.0655\n",
      "Training Epoch: 9 [5100/36450]\tLoss: 341.2031\n",
      "Training Epoch: 9 [5150/36450]\tLoss: 366.5256\n",
      "Training Epoch: 9 [5200/36450]\tLoss: 369.2876\n",
      "Training Epoch: 9 [5250/36450]\tLoss: 352.8654\n",
      "Training Epoch: 9 [5300/36450]\tLoss: 358.7621\n",
      "Training Epoch: 9 [5350/36450]\tLoss: 341.9240\n",
      "Training Epoch: 9 [5400/36450]\tLoss: 389.8454\n",
      "Training Epoch: 9 [5450/36450]\tLoss: 360.8069\n",
      "Training Epoch: 9 [5500/36450]\tLoss: 360.1805\n",
      "Training Epoch: 9 [5550/36450]\tLoss: 368.9975\n",
      "Training Epoch: 9 [5600/36450]\tLoss: 386.2637\n",
      "Training Epoch: 9 [5650/36450]\tLoss: 343.0723\n",
      "Training Epoch: 9 [5700/36450]\tLoss: 367.5675\n",
      "Training Epoch: 9 [5750/36450]\tLoss: 391.9451\n",
      "Training Epoch: 9 [5800/36450]\tLoss: 355.4432\n",
      "Training Epoch: 9 [5850/36450]\tLoss: 322.4250\n",
      "Training Epoch: 9 [5900/36450]\tLoss: 344.7278\n",
      "Training Epoch: 9 [5950/36450]\tLoss: 311.7930\n",
      "Training Epoch: 9 [6000/36450]\tLoss: 319.8996\n",
      "Training Epoch: 9 [6050/36450]\tLoss: 344.0970\n",
      "Training Epoch: 9 [6100/36450]\tLoss: 368.1339\n",
      "Training Epoch: 9 [6150/36450]\tLoss: 388.4451\n",
      "Training Epoch: 9 [6200/36450]\tLoss: 343.3988\n",
      "Training Epoch: 9 [6250/36450]\tLoss: 320.3348\n",
      "Training Epoch: 9 [6300/36450]\tLoss: 375.1431\n",
      "Training Epoch: 9 [6350/36450]\tLoss: 399.1731\n",
      "Training Epoch: 9 [6400/36450]\tLoss: 387.3338\n",
      "Training Epoch: 9 [6450/36450]\tLoss: 378.2856\n",
      "Training Epoch: 9 [6500/36450]\tLoss: 353.5905\n",
      "Training Epoch: 9 [6550/36450]\tLoss: 315.4414\n",
      "Training Epoch: 9 [6600/36450]\tLoss: 330.3912\n",
      "Training Epoch: 9 [6650/36450]\tLoss: 347.9175\n",
      "Training Epoch: 9 [6700/36450]\tLoss: 340.8846\n",
      "Training Epoch: 9 [6750/36450]\tLoss: 363.0425\n",
      "Training Epoch: 9 [6800/36450]\tLoss: 375.2810\n",
      "Training Epoch: 9 [6850/36450]\tLoss: 391.6700\n",
      "Training Epoch: 9 [6900/36450]\tLoss: 358.7919\n",
      "Training Epoch: 9 [6950/36450]\tLoss: 350.3049\n",
      "Training Epoch: 9 [7000/36450]\tLoss: 356.0371\n",
      "Training Epoch: 9 [7050/36450]\tLoss: 339.1677\n",
      "Training Epoch: 9 [7100/36450]\tLoss: 375.6752\n",
      "Training Epoch: 9 [7150/36450]\tLoss: 345.0542\n",
      "Training Epoch: 9 [7200/36450]\tLoss: 374.3455\n",
      "Training Epoch: 9 [7250/36450]\tLoss: 356.7779\n",
      "Training Epoch: 9 [7300/36450]\tLoss: 321.8191\n",
      "Training Epoch: 9 [7350/36450]\tLoss: 385.3938\n",
      "Training Epoch: 9 [7400/36450]\tLoss: 348.0175\n",
      "Training Epoch: 9 [7450/36450]\tLoss: 348.0864\n",
      "Training Epoch: 9 [7500/36450]\tLoss: 365.9345\n",
      "Training Epoch: 9 [7550/36450]\tLoss: 352.7555\n",
      "Training Epoch: 9 [7600/36450]\tLoss: 350.3600\n",
      "Training Epoch: 9 [7650/36450]\tLoss: 339.8983\n",
      "Training Epoch: 9 [7700/36450]\tLoss: 357.4167\n",
      "Training Epoch: 9 [7750/36450]\tLoss: 332.0652\n",
      "Training Epoch: 9 [7800/36450]\tLoss: 362.1231\n",
      "Training Epoch: 9 [7850/36450]\tLoss: 326.8060\n",
      "Training Epoch: 9 [7900/36450]\tLoss: 345.0989\n",
      "Training Epoch: 9 [7950/36450]\tLoss: 373.3834\n",
      "Training Epoch: 9 [8000/36450]\tLoss: 359.2350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [8050/36450]\tLoss: 353.5472\n",
      "Training Epoch: 9 [8100/36450]\tLoss: 355.1361\n",
      "Training Epoch: 9 [8150/36450]\tLoss: 367.5594\n",
      "Training Epoch: 9 [8200/36450]\tLoss: 376.8632\n",
      "Training Epoch: 9 [8250/36450]\tLoss: 335.9279\n",
      "Training Epoch: 9 [8300/36450]\tLoss: 334.5414\n",
      "Training Epoch: 9 [8350/36450]\tLoss: 387.2309\n",
      "Training Epoch: 9 [8400/36450]\tLoss: 368.5381\n",
      "Training Epoch: 9 [8450/36450]\tLoss: 376.4660\n",
      "Training Epoch: 9 [8500/36450]\tLoss: 370.5032\n",
      "Training Epoch: 9 [8550/36450]\tLoss: 374.9797\n",
      "Training Epoch: 9 [8600/36450]\tLoss: 341.0122\n",
      "Training Epoch: 9 [8650/36450]\tLoss: 368.9959\n",
      "Training Epoch: 9 [8700/36450]\tLoss: 344.7373\n",
      "Training Epoch: 9 [8750/36450]\tLoss: 381.0357\n",
      "Training Epoch: 9 [8800/36450]\tLoss: 350.7997\n",
      "Training Epoch: 9 [8850/36450]\tLoss: 358.6536\n",
      "Training Epoch: 9 [8900/36450]\tLoss: 377.0323\n",
      "Training Epoch: 9 [8950/36450]\tLoss: 398.5573\n",
      "Training Epoch: 9 [9000/36450]\tLoss: 360.1878\n",
      "Training Epoch: 9 [9050/36450]\tLoss: 358.0352\n",
      "Training Epoch: 9 [9100/36450]\tLoss: 354.2943\n",
      "Training Epoch: 9 [9150/36450]\tLoss: 356.7760\n",
      "Training Epoch: 9 [9200/36450]\tLoss: 338.4693\n",
      "Training Epoch: 9 [9250/36450]\tLoss: 352.1628\n",
      "Training Epoch: 9 [9300/36450]\tLoss: 364.6627\n",
      "Training Epoch: 9 [9350/36450]\tLoss: 379.9041\n",
      "Training Epoch: 9 [9400/36450]\tLoss: 337.6716\n",
      "Training Epoch: 9 [9450/36450]\tLoss: 389.8730\n",
      "Training Epoch: 9 [9500/36450]\tLoss: 342.6083\n",
      "Training Epoch: 9 [9550/36450]\tLoss: 320.2509\n",
      "Training Epoch: 9 [9600/36450]\tLoss: 362.3515\n",
      "Training Epoch: 9 [9650/36450]\tLoss: 380.7833\n",
      "Training Epoch: 9 [9700/36450]\tLoss: 342.5463\n",
      "Training Epoch: 9 [9750/36450]\tLoss: 340.1755\n",
      "Training Epoch: 9 [9800/36450]\tLoss: 367.0460\n",
      "Training Epoch: 9 [9850/36450]\tLoss: 361.0613\n",
      "Training Epoch: 9 [9900/36450]\tLoss: 354.1147\n",
      "Training Epoch: 9 [9950/36450]\tLoss: 349.7853\n",
      "Training Epoch: 9 [10000/36450]\tLoss: 375.6202\n",
      "Training Epoch: 9 [10050/36450]\tLoss: 331.6261\n",
      "Training Epoch: 9 [10100/36450]\tLoss: 328.2748\n",
      "Training Epoch: 9 [10150/36450]\tLoss: 355.3174\n",
      "Training Epoch: 9 [10200/36450]\tLoss: 361.9944\n",
      "Training Epoch: 9 [10250/36450]\tLoss: 342.9463\n",
      "Training Epoch: 9 [10300/36450]\tLoss: 375.8435\n",
      "Training Epoch: 9 [10350/36450]\tLoss: 368.7830\n",
      "Training Epoch: 9 [10400/36450]\tLoss: 357.5752\n",
      "Training Epoch: 9 [10450/36450]\tLoss: 384.1791\n",
      "Training Epoch: 9 [10500/36450]\tLoss: 382.1823\n",
      "Training Epoch: 9 [10550/36450]\tLoss: 335.7208\n",
      "Training Epoch: 9 [10600/36450]\tLoss: 361.9903\n",
      "Training Epoch: 9 [10650/36450]\tLoss: 373.3333\n",
      "Training Epoch: 9 [10700/36450]\tLoss: 354.4618\n",
      "Training Epoch: 9 [10750/36450]\tLoss: 335.8247\n",
      "Training Epoch: 9 [10800/36450]\tLoss: 368.0343\n",
      "Training Epoch: 9 [10850/36450]\tLoss: 337.7383\n",
      "Training Epoch: 9 [10900/36450]\tLoss: 308.3940\n",
      "Training Epoch: 9 [10950/36450]\tLoss: 337.9291\n",
      "Training Epoch: 9 [11000/36450]\tLoss: 349.4805\n",
      "Training Epoch: 9 [11050/36450]\tLoss: 326.7701\n",
      "Training Epoch: 9 [11100/36450]\tLoss: 357.0417\n",
      "Training Epoch: 9 [11150/36450]\tLoss: 334.3674\n",
      "Training Epoch: 9 [11200/36450]\tLoss: 342.8422\n",
      "Training Epoch: 9 [11250/36450]\tLoss: 343.9673\n",
      "Training Epoch: 9 [11300/36450]\tLoss: 348.8408\n",
      "Training Epoch: 9 [11350/36450]\tLoss: 344.1378\n",
      "Training Epoch: 9 [11400/36450]\tLoss: 347.2180\n",
      "Training Epoch: 9 [11450/36450]\tLoss: 407.7981\n",
      "Training Epoch: 9 [11500/36450]\tLoss: 348.5144\n",
      "Training Epoch: 9 [11550/36450]\tLoss: 380.2182\n",
      "Training Epoch: 9 [11600/36450]\tLoss: 335.6868\n",
      "Training Epoch: 9 [11650/36450]\tLoss: 358.3241\n",
      "Training Epoch: 9 [11700/36450]\tLoss: 326.4423\n",
      "Training Epoch: 9 [11750/36450]\tLoss: 312.1803\n",
      "Training Epoch: 9 [11800/36450]\tLoss: 371.6579\n",
      "Training Epoch: 9 [11850/36450]\tLoss: 331.5385\n",
      "Training Epoch: 9 [11900/36450]\tLoss: 345.2715\n",
      "Training Epoch: 9 [11950/36450]\tLoss: 358.5022\n",
      "Training Epoch: 9 [12000/36450]\tLoss: 394.8897\n",
      "Training Epoch: 9 [12050/36450]\tLoss: 341.5363\n",
      "Training Epoch: 9 [12100/36450]\tLoss: 336.1720\n",
      "Training Epoch: 9 [12150/36450]\tLoss: 347.5203\n",
      "Training Epoch: 9 [12200/36450]\tLoss: 355.5684\n",
      "Training Epoch: 9 [12250/36450]\tLoss: 368.8765\n",
      "Training Epoch: 9 [12300/36450]\tLoss: 365.4254\n",
      "Training Epoch: 9 [12350/36450]\tLoss: 338.6844\n",
      "Training Epoch: 9 [12400/36450]\tLoss: 351.3756\n",
      "Training Epoch: 9 [12450/36450]\tLoss: 386.2717\n",
      "Training Epoch: 9 [12500/36450]\tLoss: 344.3250\n",
      "Training Epoch: 9 [12550/36450]\tLoss: 378.3748\n",
      "Training Epoch: 9 [12600/36450]\tLoss: 422.4085\n",
      "Training Epoch: 9 [12650/36450]\tLoss: 316.3079\n",
      "Training Epoch: 9 [12700/36450]\tLoss: 355.4782\n",
      "Training Epoch: 9 [12750/36450]\tLoss: 342.3425\n",
      "Training Epoch: 9 [12800/36450]\tLoss: 395.7632\n",
      "Training Epoch: 9 [12850/36450]\tLoss: 376.7937\n",
      "Training Epoch: 9 [12900/36450]\tLoss: 346.5211\n",
      "Training Epoch: 9 [12950/36450]\tLoss: 318.8745\n",
      "Training Epoch: 9 [13000/36450]\tLoss: 318.6741\n",
      "Training Epoch: 9 [13050/36450]\tLoss: 330.8693\n",
      "Training Epoch: 9 [13100/36450]\tLoss: 356.5720\n",
      "Training Epoch: 9 [13150/36450]\tLoss: 311.5704\n",
      "Training Epoch: 9 [13200/36450]\tLoss: 359.9143\n",
      "Training Epoch: 9 [13250/36450]\tLoss: 365.6368\n",
      "Training Epoch: 9 [13300/36450]\tLoss: 338.9632\n",
      "Training Epoch: 9 [13350/36450]\tLoss: 351.9292\n",
      "Training Epoch: 9 [13400/36450]\tLoss: 358.9750\n",
      "Training Epoch: 9 [13450/36450]\tLoss: 359.1830\n",
      "Training Epoch: 9 [13500/36450]\tLoss: 350.9062\n",
      "Training Epoch: 9 [13550/36450]\tLoss: 362.2507\n",
      "Training Epoch: 9 [13600/36450]\tLoss: 371.4194\n",
      "Training Epoch: 9 [13650/36450]\tLoss: 344.1744\n",
      "Training Epoch: 9 [13700/36450]\tLoss: 368.3135\n",
      "Training Epoch: 9 [13750/36450]\tLoss: 385.7065\n",
      "Training Epoch: 9 [13800/36450]\tLoss: 344.5647\n",
      "Training Epoch: 9 [13850/36450]\tLoss: 354.3241\n",
      "Training Epoch: 9 [13900/36450]\tLoss: 355.6119\n",
      "Training Epoch: 9 [13950/36450]\tLoss: 372.5186\n",
      "Training Epoch: 9 [14000/36450]\tLoss: 313.7380\n",
      "Training Epoch: 9 [14050/36450]\tLoss: 344.8024\n",
      "Training Epoch: 9 [14100/36450]\tLoss: 339.2140\n",
      "Training Epoch: 9 [14150/36450]\tLoss: 344.7114\n",
      "Training Epoch: 9 [14200/36450]\tLoss: 349.5859\n",
      "Training Epoch: 9 [14250/36450]\tLoss: 325.3025\n",
      "Training Epoch: 9 [14300/36450]\tLoss: 378.3098\n",
      "Training Epoch: 9 [14350/36450]\tLoss: 349.5565\n",
      "Training Epoch: 9 [14400/36450]\tLoss: 330.0947\n",
      "Training Epoch: 9 [14450/36450]\tLoss: 342.3821\n",
      "Training Epoch: 9 [14500/36450]\tLoss: 308.6766\n",
      "Training Epoch: 9 [14550/36450]\tLoss: 374.1870\n",
      "Training Epoch: 9 [14600/36450]\tLoss: 352.5677\n",
      "Training Epoch: 9 [14650/36450]\tLoss: 347.4135\n",
      "Training Epoch: 9 [14700/36450]\tLoss: 317.7421\n",
      "Training Epoch: 9 [14750/36450]\tLoss: 321.1774\n",
      "Training Epoch: 9 [14800/36450]\tLoss: 347.0709\n",
      "Training Epoch: 9 [14850/36450]\tLoss: 334.8117\n",
      "Training Epoch: 9 [14900/36450]\tLoss: 371.5039\n",
      "Training Epoch: 9 [14950/36450]\tLoss: 381.6074\n",
      "Training Epoch: 9 [15000/36450]\tLoss: 333.4219\n",
      "Training Epoch: 9 [15050/36450]\tLoss: 355.4572\n",
      "Training Epoch: 9 [15100/36450]\tLoss: 359.7245\n",
      "Training Epoch: 9 [15150/36450]\tLoss: 385.1174\n",
      "Training Epoch: 9 [15200/36450]\tLoss: 381.8338\n",
      "Training Epoch: 9 [15250/36450]\tLoss: 352.3119\n",
      "Training Epoch: 9 [15300/36450]\tLoss: 328.4481\n",
      "Training Epoch: 9 [15350/36450]\tLoss: 361.1443\n",
      "Training Epoch: 9 [15400/36450]\tLoss: 346.2624\n",
      "Training Epoch: 9 [15450/36450]\tLoss: 331.3224\n",
      "Training Epoch: 9 [15500/36450]\tLoss: 361.2368\n",
      "Training Epoch: 9 [15550/36450]\tLoss: 350.4547\n",
      "Training Epoch: 9 [15600/36450]\tLoss: 337.8094\n",
      "Training Epoch: 9 [15650/36450]\tLoss: 354.8058\n",
      "Training Epoch: 9 [15700/36450]\tLoss: 362.9218\n",
      "Training Epoch: 9 [15750/36450]\tLoss: 392.2216\n",
      "Training Epoch: 9 [15800/36450]\tLoss: 349.9067\n",
      "Training Epoch: 9 [15850/36450]\tLoss: 337.0310\n",
      "Training Epoch: 9 [15900/36450]\tLoss: 352.3723\n",
      "Training Epoch: 9 [15950/36450]\tLoss: 367.9800\n",
      "Training Epoch: 9 [16000/36450]\tLoss: 357.1809\n",
      "Training Epoch: 9 [16050/36450]\tLoss: 359.2274\n",
      "Training Epoch: 9 [16100/36450]\tLoss: 340.9972\n",
      "Training Epoch: 9 [16150/36450]\tLoss: 334.6891\n",
      "Training Epoch: 9 [16200/36450]\tLoss: 346.6399\n",
      "Training Epoch: 9 [16250/36450]\tLoss: 350.2516\n",
      "Training Epoch: 9 [16300/36450]\tLoss: 367.5905\n",
      "Training Epoch: 9 [16350/36450]\tLoss: 354.5289\n",
      "Training Epoch: 9 [16400/36450]\tLoss: 340.4214\n",
      "Training Epoch: 9 [16450/36450]\tLoss: 347.8972\n",
      "Training Epoch: 9 [16500/36450]\tLoss: 350.8685\n",
      "Training Epoch: 9 [16550/36450]\tLoss: 368.2465\n",
      "Training Epoch: 9 [16600/36450]\tLoss: 362.7768\n",
      "Training Epoch: 9 [16650/36450]\tLoss: 353.6773\n",
      "Training Epoch: 9 [16700/36450]\tLoss: 352.0219\n",
      "Training Epoch: 9 [16750/36450]\tLoss: 332.1513\n",
      "Training Epoch: 9 [16800/36450]\tLoss: 342.5018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [16850/36450]\tLoss: 344.6807\n",
      "Training Epoch: 9 [16900/36450]\tLoss: 297.9201\n",
      "Training Epoch: 9 [16950/36450]\tLoss: 316.5976\n",
      "Training Epoch: 9 [17000/36450]\tLoss: 355.4341\n",
      "Training Epoch: 9 [17050/36450]\tLoss: 354.2213\n",
      "Training Epoch: 9 [17100/36450]\tLoss: 334.4082\n",
      "Training Epoch: 9 [17150/36450]\tLoss: 364.9858\n",
      "Training Epoch: 9 [17200/36450]\tLoss: 355.5107\n",
      "Training Epoch: 9 [17250/36450]\tLoss: 323.1617\n",
      "Training Epoch: 9 [17300/36450]\tLoss: 372.5023\n",
      "Training Epoch: 9 [17350/36450]\tLoss: 363.2270\n",
      "Training Epoch: 9 [17400/36450]\tLoss: 340.1024\n",
      "Training Epoch: 9 [17450/36450]\tLoss: 390.1930\n",
      "Training Epoch: 9 [17500/36450]\tLoss: 309.9747\n",
      "Training Epoch: 9 [17550/36450]\tLoss: 335.0842\n",
      "Training Epoch: 9 [17600/36450]\tLoss: 360.5982\n",
      "Training Epoch: 9 [17650/36450]\tLoss: 332.1317\n",
      "Training Epoch: 9 [17700/36450]\tLoss: 371.0581\n",
      "Training Epoch: 9 [17750/36450]\tLoss: 327.7363\n",
      "Training Epoch: 9 [17800/36450]\tLoss: 342.6143\n",
      "Training Epoch: 9 [17850/36450]\tLoss: 315.7322\n",
      "Training Epoch: 9 [17900/36450]\tLoss: 380.9710\n",
      "Training Epoch: 9 [17950/36450]\tLoss: 330.1806\n",
      "Training Epoch: 9 [18000/36450]\tLoss: 371.9803\n",
      "Training Epoch: 9 [18050/36450]\tLoss: 343.9715\n",
      "Training Epoch: 9 [18100/36450]\tLoss: 335.2025\n",
      "Training Epoch: 9 [18150/36450]\tLoss: 325.7988\n",
      "Training Epoch: 9 [18200/36450]\tLoss: 374.3945\n",
      "Training Epoch: 9 [18250/36450]\tLoss: 365.7152\n",
      "Training Epoch: 9 [18300/36450]\tLoss: 355.1443\n",
      "Training Epoch: 9 [18350/36450]\tLoss: 348.5840\n",
      "Training Epoch: 9 [18400/36450]\tLoss: 370.7260\n",
      "Training Epoch: 9 [18450/36450]\tLoss: 329.1611\n",
      "Training Epoch: 9 [18500/36450]\tLoss: 335.4798\n",
      "Training Epoch: 9 [18550/36450]\tLoss: 311.9109\n",
      "Training Epoch: 9 [18600/36450]\tLoss: 338.6429\n",
      "Training Epoch: 9 [18650/36450]\tLoss: 378.0547\n",
      "Training Epoch: 9 [18700/36450]\tLoss: 301.7284\n",
      "Training Epoch: 9 [18750/36450]\tLoss: 349.0873\n",
      "Training Epoch: 9 [18800/36450]\tLoss: 346.5654\n",
      "Training Epoch: 9 [18850/36450]\tLoss: 348.4548\n",
      "Training Epoch: 9 [18900/36450]\tLoss: 353.8763\n",
      "Training Epoch: 9 [18950/36450]\tLoss: 334.1568\n",
      "Training Epoch: 9 [19000/36450]\tLoss: 357.8575\n",
      "Training Epoch: 9 [19050/36450]\tLoss: 331.8901\n",
      "Training Epoch: 9 [19100/36450]\tLoss: 326.9781\n",
      "Training Epoch: 9 [19150/36450]\tLoss: 328.7709\n",
      "Training Epoch: 9 [19200/36450]\tLoss: 337.6483\n",
      "Training Epoch: 9 [19250/36450]\tLoss: 332.9427\n",
      "Training Epoch: 9 [19300/36450]\tLoss: 361.5271\n",
      "Training Epoch: 9 [19350/36450]\tLoss: 338.6126\n",
      "Training Epoch: 9 [19400/36450]\tLoss: 322.5121\n",
      "Training Epoch: 9 [19450/36450]\tLoss: 316.7961\n",
      "Training Epoch: 9 [19500/36450]\tLoss: 340.0842\n",
      "Training Epoch: 9 [19550/36450]\tLoss: 354.0778\n",
      "Training Epoch: 9 [19600/36450]\tLoss: 407.4663\n",
      "Training Epoch: 9 [19650/36450]\tLoss: 348.1070\n",
      "Training Epoch: 9 [19700/36450]\tLoss: 342.9661\n",
      "Training Epoch: 9 [19750/36450]\tLoss: 333.3505\n",
      "Training Epoch: 9 [19800/36450]\tLoss: 342.8817\n",
      "Training Epoch: 9 [19850/36450]\tLoss: 324.7750\n",
      "Training Epoch: 9 [19900/36450]\tLoss: 359.9662\n",
      "Training Epoch: 9 [19950/36450]\tLoss: 354.3355\n",
      "Training Epoch: 9 [20000/36450]\tLoss: 353.7465\n",
      "Training Epoch: 9 [20050/36450]\tLoss: 370.5224\n",
      "Training Epoch: 9 [20100/36450]\tLoss: 352.9932\n",
      "Training Epoch: 9 [20150/36450]\tLoss: 325.5184\n",
      "Training Epoch: 9 [20200/36450]\tLoss: 374.0340\n",
      "Training Epoch: 9 [20250/36450]\tLoss: 370.0531\n",
      "Training Epoch: 9 [20300/36450]\tLoss: 336.3822\n",
      "Training Epoch: 9 [20350/36450]\tLoss: 349.7504\n",
      "Training Epoch: 9 [20400/36450]\tLoss: 310.5351\n",
      "Training Epoch: 9 [20450/36450]\tLoss: 325.0096\n",
      "Training Epoch: 9 [20500/36450]\tLoss: 332.9173\n",
      "Training Epoch: 9 [20550/36450]\tLoss: 370.3405\n",
      "Training Epoch: 9 [20600/36450]\tLoss: 380.5033\n",
      "Training Epoch: 9 [20650/36450]\tLoss: 311.4786\n",
      "Training Epoch: 9 [20700/36450]\tLoss: 336.1959\n",
      "Training Epoch: 9 [20750/36450]\tLoss: 351.4795\n",
      "Training Epoch: 9 [20800/36450]\tLoss: 334.4576\n",
      "Training Epoch: 9 [20850/36450]\tLoss: 344.1541\n",
      "Training Epoch: 9 [20900/36450]\tLoss: 362.4512\n",
      "Training Epoch: 9 [20950/36450]\tLoss: 353.5849\n",
      "Training Epoch: 9 [21000/36450]\tLoss: 321.0893\n",
      "Training Epoch: 9 [21050/36450]\tLoss: 326.1106\n",
      "Training Epoch: 9 [21100/36450]\tLoss: 370.1905\n",
      "Training Epoch: 9 [21150/36450]\tLoss: 336.2293\n",
      "Training Epoch: 9 [21200/36450]\tLoss: 389.7597\n",
      "Training Epoch: 9 [21250/36450]\tLoss: 366.0333\n",
      "Training Epoch: 9 [21300/36450]\tLoss: 320.5629\n",
      "Training Epoch: 9 [21350/36450]\tLoss: 341.0650\n",
      "Training Epoch: 9 [21400/36450]\tLoss: 353.3127\n",
      "Training Epoch: 9 [21450/36450]\tLoss: 374.0801\n",
      "Training Epoch: 9 [21500/36450]\tLoss: 330.9465\n",
      "Training Epoch: 9 [21550/36450]\tLoss: 386.2136\n",
      "Training Epoch: 9 [21600/36450]\tLoss: 368.8758\n",
      "Training Epoch: 9 [21650/36450]\tLoss: 328.9638\n",
      "Training Epoch: 9 [21700/36450]\tLoss: 366.2810\n",
      "Training Epoch: 9 [21750/36450]\tLoss: 348.5369\n",
      "Training Epoch: 9 [21800/36450]\tLoss: 396.4343\n",
      "Training Epoch: 9 [21850/36450]\tLoss: 350.2920\n",
      "Training Epoch: 9 [21900/36450]\tLoss: 382.6582\n",
      "Training Epoch: 9 [21950/36450]\tLoss: 341.0219\n",
      "Training Epoch: 9 [22000/36450]\tLoss: 328.9129\n",
      "Training Epoch: 9 [22050/36450]\tLoss: 343.7124\n",
      "Training Epoch: 9 [22100/36450]\tLoss: 362.0219\n",
      "Training Epoch: 9 [22150/36450]\tLoss: 311.1066\n",
      "Training Epoch: 9 [22200/36450]\tLoss: 344.1223\n",
      "Training Epoch: 9 [22250/36450]\tLoss: 319.3403\n",
      "Training Epoch: 9 [22300/36450]\tLoss: 301.3143\n",
      "Training Epoch: 9 [22350/36450]\tLoss: 332.1657\n",
      "Training Epoch: 9 [22400/36450]\tLoss: 354.4403\n",
      "Training Epoch: 9 [22450/36450]\tLoss: 349.7019\n",
      "Training Epoch: 9 [22500/36450]\tLoss: 353.1514\n",
      "Training Epoch: 9 [22550/36450]\tLoss: 350.5323\n",
      "Training Epoch: 9 [22600/36450]\tLoss: 291.1166\n",
      "Training Epoch: 9 [22650/36450]\tLoss: 342.5807\n",
      "Training Epoch: 9 [22700/36450]\tLoss: 332.9039\n",
      "Training Epoch: 9 [22750/36450]\tLoss: 395.8198\n",
      "Training Epoch: 9 [22800/36450]\tLoss: 329.9863\n",
      "Training Epoch: 9 [22850/36450]\tLoss: 313.7064\n",
      "Training Epoch: 9 [22900/36450]\tLoss: 348.9675\n",
      "Training Epoch: 9 [22950/36450]\tLoss: 355.1041\n",
      "Training Epoch: 9 [23000/36450]\tLoss: 322.5637\n",
      "Training Epoch: 9 [23050/36450]\tLoss: 343.6417\n",
      "Training Epoch: 9 [23100/36450]\tLoss: 404.2393\n",
      "Training Epoch: 9 [23150/36450]\tLoss: 352.0547\n",
      "Training Epoch: 9 [23200/36450]\tLoss: 377.4784\n",
      "Training Epoch: 9 [23250/36450]\tLoss: 385.4701\n",
      "Training Epoch: 9 [23300/36450]\tLoss: 358.1753\n",
      "Training Epoch: 9 [23350/36450]\tLoss: 349.0149\n",
      "Training Epoch: 9 [23400/36450]\tLoss: 340.8521\n",
      "Training Epoch: 9 [23450/36450]\tLoss: 390.5193\n",
      "Training Epoch: 9 [23500/36450]\tLoss: 349.4133\n",
      "Training Epoch: 9 [23550/36450]\tLoss: 342.8240\n",
      "Training Epoch: 9 [23600/36450]\tLoss: 318.1679\n",
      "Training Epoch: 9 [23650/36450]\tLoss: 358.4225\n",
      "Training Epoch: 9 [23700/36450]\tLoss: 334.7433\n",
      "Training Epoch: 9 [23750/36450]\tLoss: 362.1043\n",
      "Training Epoch: 9 [23800/36450]\tLoss: 385.7355\n",
      "Training Epoch: 9 [23850/36450]\tLoss: 324.0723\n",
      "Training Epoch: 9 [23900/36450]\tLoss: 380.4103\n",
      "Training Epoch: 9 [23950/36450]\tLoss: 334.7498\n",
      "Training Epoch: 9 [24000/36450]\tLoss: 317.9034\n",
      "Training Epoch: 9 [24050/36450]\tLoss: 395.6861\n",
      "Training Epoch: 9 [24100/36450]\tLoss: 335.4227\n",
      "Training Epoch: 9 [24150/36450]\tLoss: 360.0758\n",
      "Training Epoch: 9 [24200/36450]\tLoss: 372.5968\n",
      "Training Epoch: 9 [24250/36450]\tLoss: 321.3935\n",
      "Training Epoch: 9 [24300/36450]\tLoss: 323.5590\n",
      "Training Epoch: 9 [24350/36450]\tLoss: 316.7176\n",
      "Training Epoch: 9 [24400/36450]\tLoss: 345.7339\n",
      "Training Epoch: 9 [24450/36450]\tLoss: 345.1654\n",
      "Training Epoch: 9 [24500/36450]\tLoss: 372.3488\n",
      "Training Epoch: 9 [24550/36450]\tLoss: 335.8871\n",
      "Training Epoch: 9 [24600/36450]\tLoss: 348.9816\n",
      "Training Epoch: 9 [24650/36450]\tLoss: 334.4264\n",
      "Training Epoch: 9 [24700/36450]\tLoss: 341.1683\n",
      "Training Epoch: 9 [24750/36450]\tLoss: 354.6245\n",
      "Training Epoch: 9 [24800/36450]\tLoss: 362.1565\n",
      "Training Epoch: 9 [24850/36450]\tLoss: 339.5974\n",
      "Training Epoch: 9 [24900/36450]\tLoss: 345.4498\n",
      "Training Epoch: 9 [24950/36450]\tLoss: 355.0663\n",
      "Training Epoch: 9 [25000/36450]\tLoss: 340.9835\n",
      "Training Epoch: 9 [25050/36450]\tLoss: 382.8926\n",
      "Training Epoch: 9 [25100/36450]\tLoss: 345.5254\n",
      "Training Epoch: 9 [25150/36450]\tLoss: 326.8386\n",
      "Training Epoch: 9 [25200/36450]\tLoss: 362.9553\n",
      "Training Epoch: 9 [25250/36450]\tLoss: 344.7621\n",
      "Training Epoch: 9 [25300/36450]\tLoss: 359.8946\n",
      "Training Epoch: 9 [25350/36450]\tLoss: 338.5510\n",
      "Training Epoch: 9 [25400/36450]\tLoss: 349.1266\n",
      "Training Epoch: 9 [25450/36450]\tLoss: 359.7792\n",
      "Training Epoch: 9 [25500/36450]\tLoss: 347.1459\n",
      "Training Epoch: 9 [25550/36450]\tLoss: 347.4341\n",
      "Training Epoch: 9 [25600/36450]\tLoss: 338.1814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [25650/36450]\tLoss: 342.8751\n",
      "Training Epoch: 9 [25700/36450]\tLoss: 370.5941\n",
      "Training Epoch: 9 [25750/36450]\tLoss: 331.4068\n",
      "Training Epoch: 9 [25800/36450]\tLoss: 317.6865\n",
      "Training Epoch: 9 [25850/36450]\tLoss: 363.2369\n",
      "Training Epoch: 9 [25900/36450]\tLoss: 353.1353\n",
      "Training Epoch: 9 [25950/36450]\tLoss: 347.5164\n",
      "Training Epoch: 9 [26000/36450]\tLoss: 341.0340\n",
      "Training Epoch: 9 [26050/36450]\tLoss: 326.7566\n",
      "Training Epoch: 9 [26100/36450]\tLoss: 326.4932\n",
      "Training Epoch: 9 [26150/36450]\tLoss: 318.2413\n",
      "Training Epoch: 9 [26200/36450]\tLoss: 338.1338\n",
      "Training Epoch: 9 [26250/36450]\tLoss: 338.6182\n",
      "Training Epoch: 9 [26300/36450]\tLoss: 348.5243\n",
      "Training Epoch: 9 [26350/36450]\tLoss: 357.3837\n",
      "Training Epoch: 9 [26400/36450]\tLoss: 373.0986\n",
      "Training Epoch: 9 [26450/36450]\tLoss: 328.8445\n",
      "Training Epoch: 9 [26500/36450]\tLoss: 340.0941\n",
      "Training Epoch: 9 [26550/36450]\tLoss: 365.7397\n",
      "Training Epoch: 9 [26600/36450]\tLoss: 319.4572\n",
      "Training Epoch: 9 [26650/36450]\tLoss: 328.8683\n",
      "Training Epoch: 9 [26700/36450]\tLoss: 374.8380\n",
      "Training Epoch: 9 [26750/36450]\tLoss: 344.7528\n",
      "Training Epoch: 9 [26800/36450]\tLoss: 300.8674\n",
      "Training Epoch: 9 [26850/36450]\tLoss: 344.4636\n",
      "Training Epoch: 9 [26900/36450]\tLoss: 317.0266\n",
      "Training Epoch: 9 [26950/36450]\tLoss: 372.5062\n",
      "Training Epoch: 9 [27000/36450]\tLoss: 343.3478\n",
      "Training Epoch: 9 [27050/36450]\tLoss: 340.2505\n",
      "Training Epoch: 9 [27100/36450]\tLoss: 326.5206\n",
      "Training Epoch: 9 [27150/36450]\tLoss: 295.6869\n",
      "Training Epoch: 9 [27200/36450]\tLoss: 409.5694\n",
      "Training Epoch: 9 [27250/36450]\tLoss: 353.4494\n",
      "Training Epoch: 9 [27300/36450]\tLoss: 346.5692\n",
      "Training Epoch: 9 [27350/36450]\tLoss: 352.1429\n",
      "Training Epoch: 9 [27400/36450]\tLoss: 355.0363\n",
      "Training Epoch: 9 [27450/36450]\tLoss: 339.3770\n",
      "Training Epoch: 9 [27500/36450]\tLoss: 341.7630\n",
      "Training Epoch: 9 [27550/36450]\tLoss: 342.3602\n",
      "Training Epoch: 9 [27600/36450]\tLoss: 339.1670\n",
      "Training Epoch: 9 [27650/36450]\tLoss: 361.8175\n",
      "Training Epoch: 9 [27700/36450]\tLoss: 329.9193\n",
      "Training Epoch: 9 [27750/36450]\tLoss: 346.7501\n",
      "Training Epoch: 9 [27800/36450]\tLoss: 330.1939\n",
      "Training Epoch: 9 [27850/36450]\tLoss: 372.3575\n",
      "Training Epoch: 9 [27900/36450]\tLoss: 389.5163\n",
      "Training Epoch: 9 [27950/36450]\tLoss: 306.7766\n",
      "Training Epoch: 9 [28000/36450]\tLoss: 330.3460\n",
      "Training Epoch: 9 [28050/36450]\tLoss: 347.6221\n",
      "Training Epoch: 9 [28100/36450]\tLoss: 370.7794\n",
      "Training Epoch: 9 [28150/36450]\tLoss: 322.5016\n",
      "Training Epoch: 9 [28200/36450]\tLoss: 345.4637\n",
      "Training Epoch: 9 [28250/36450]\tLoss: 364.2507\n",
      "Training Epoch: 9 [28300/36450]\tLoss: 338.8776\n",
      "Training Epoch: 9 [28350/36450]\tLoss: 313.5837\n",
      "Training Epoch: 9 [28400/36450]\tLoss: 332.6214\n",
      "Training Epoch: 9 [28450/36450]\tLoss: 344.1569\n",
      "Training Epoch: 9 [28500/36450]\tLoss: 367.6455\n",
      "Training Epoch: 9 [28550/36450]\tLoss: 339.8922\n",
      "Training Epoch: 9 [28600/36450]\tLoss: 308.8345\n",
      "Training Epoch: 9 [28650/36450]\tLoss: 349.8557\n",
      "Training Epoch: 9 [28700/36450]\tLoss: 321.1962\n",
      "Training Epoch: 9 [28750/36450]\tLoss: 340.2833\n",
      "Training Epoch: 9 [28800/36450]\tLoss: 331.2157\n",
      "Training Epoch: 9 [28850/36450]\tLoss: 381.7086\n",
      "Training Epoch: 9 [28900/36450]\tLoss: 392.0995\n",
      "Training Epoch: 9 [28950/36450]\tLoss: 357.1786\n",
      "Training Epoch: 9 [29000/36450]\tLoss: 339.5339\n",
      "Training Epoch: 9 [29050/36450]\tLoss: 322.4451\n",
      "Training Epoch: 9 [29100/36450]\tLoss: 342.1691\n",
      "Training Epoch: 9 [29150/36450]\tLoss: 336.6019\n",
      "Training Epoch: 9 [29200/36450]\tLoss: 343.8521\n",
      "Training Epoch: 9 [29250/36450]\tLoss: 353.1668\n",
      "Training Epoch: 9 [29300/36450]\tLoss: 344.9708\n",
      "Training Epoch: 9 [29350/36450]\tLoss: 311.6310\n",
      "Training Epoch: 9 [29400/36450]\tLoss: 360.1434\n",
      "Training Epoch: 9 [29450/36450]\tLoss: 344.0534\n",
      "Training Epoch: 9 [29500/36450]\tLoss: 372.3171\n",
      "Training Epoch: 9 [29550/36450]\tLoss: 343.6035\n",
      "Training Epoch: 9 [29600/36450]\tLoss: 309.9357\n",
      "Training Epoch: 9 [29650/36450]\tLoss: 346.4518\n",
      "Training Epoch: 9 [29700/36450]\tLoss: 326.8959\n",
      "Training Epoch: 9 [29750/36450]\tLoss: 323.6898\n",
      "Training Epoch: 9 [29800/36450]\tLoss: 336.7512\n",
      "Training Epoch: 9 [29850/36450]\tLoss: 328.1577\n",
      "Training Epoch: 9 [29900/36450]\tLoss: 342.6400\n",
      "Training Epoch: 9 [29950/36450]\tLoss: 336.1565\n",
      "Training Epoch: 9 [30000/36450]\tLoss: 357.4892\n",
      "Training Epoch: 9 [30050/36450]\tLoss: 375.3270\n",
      "Training Epoch: 9 [30100/36450]\tLoss: 341.1541\n",
      "Training Epoch: 9 [30150/36450]\tLoss: 337.9960\n",
      "Training Epoch: 9 [30200/36450]\tLoss: 315.7288\n",
      "Training Epoch: 9 [30250/36450]\tLoss: 321.8778\n",
      "Training Epoch: 9 [30300/36450]\tLoss: 337.2666\n",
      "Training Epoch: 9 [30350/36450]\tLoss: 320.1400\n",
      "Training Epoch: 9 [30400/36450]\tLoss: 327.9909\n",
      "Training Epoch: 9 [30450/36450]\tLoss: 333.6266\n",
      "Training Epoch: 9 [30500/36450]\tLoss: 337.4411\n",
      "Training Epoch: 9 [30550/36450]\tLoss: 364.1250\n",
      "Training Epoch: 9 [30600/36450]\tLoss: 340.2910\n",
      "Training Epoch: 9 [30650/36450]\tLoss: 346.3362\n",
      "Training Epoch: 9 [30700/36450]\tLoss: 360.7132\n",
      "Training Epoch: 9 [30750/36450]\tLoss: 362.8189\n",
      "Training Epoch: 9 [30800/36450]\tLoss: 339.1161\n",
      "Training Epoch: 9 [30850/36450]\tLoss: 355.5302\n",
      "Training Epoch: 9 [30900/36450]\tLoss: 353.5361\n",
      "Training Epoch: 9 [30950/36450]\tLoss: 324.2477\n",
      "Training Epoch: 9 [31000/36450]\tLoss: 328.9531\n",
      "Training Epoch: 9 [31050/36450]\tLoss: 343.0003\n",
      "Training Epoch: 9 [31100/36450]\tLoss: 328.7964\n",
      "Training Epoch: 9 [31150/36450]\tLoss: 348.8123\n",
      "Training Epoch: 9 [31200/36450]\tLoss: 293.5493\n",
      "Training Epoch: 9 [31250/36450]\tLoss: 343.9108\n",
      "Training Epoch: 9 [31300/36450]\tLoss: 361.1516\n",
      "Training Epoch: 9 [31350/36450]\tLoss: 310.4285\n",
      "Training Epoch: 9 [31400/36450]\tLoss: 343.7440\n",
      "Training Epoch: 9 [31450/36450]\tLoss: 379.2480\n",
      "Training Epoch: 9 [31500/36450]\tLoss: 351.0568\n",
      "Training Epoch: 9 [31550/36450]\tLoss: 355.1944\n",
      "Training Epoch: 9 [31600/36450]\tLoss: 321.0146\n",
      "Training Epoch: 9 [31650/36450]\tLoss: 335.4012\n",
      "Training Epoch: 9 [31700/36450]\tLoss: 341.0799\n",
      "Training Epoch: 9 [31750/36450]\tLoss: 326.6342\n",
      "Training Epoch: 9 [31800/36450]\tLoss: 334.9510\n",
      "Training Epoch: 9 [31850/36450]\tLoss: 329.8936\n",
      "Training Epoch: 9 [31900/36450]\tLoss: 329.8715\n",
      "Training Epoch: 9 [31950/36450]\tLoss: 330.6227\n",
      "Training Epoch: 9 [32000/36450]\tLoss: 346.1584\n",
      "Training Epoch: 9 [32050/36450]\tLoss: 323.4232\n",
      "Training Epoch: 9 [32100/36450]\tLoss: 354.9242\n",
      "Training Epoch: 9 [32150/36450]\tLoss: 354.1026\n",
      "Training Epoch: 9 [32200/36450]\tLoss: 293.2650\n",
      "Training Epoch: 9 [32250/36450]\tLoss: 375.3343\n",
      "Training Epoch: 9 [32300/36450]\tLoss: 359.3571\n",
      "Training Epoch: 9 [32350/36450]\tLoss: 342.3866\n",
      "Training Epoch: 9 [32400/36450]\tLoss: 340.7198\n",
      "Training Epoch: 9 [32450/36450]\tLoss: 348.6131\n",
      "Training Epoch: 9 [32500/36450]\tLoss: 365.6107\n",
      "Training Epoch: 9 [32550/36450]\tLoss: 342.7611\n",
      "Training Epoch: 9 [32600/36450]\tLoss: 364.3000\n",
      "Training Epoch: 9 [32650/36450]\tLoss: 345.3633\n",
      "Training Epoch: 9 [32700/36450]\tLoss: 343.7728\n",
      "Training Epoch: 9 [32750/36450]\tLoss: 332.4584\n",
      "Training Epoch: 9 [32800/36450]\tLoss: 357.1562\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
