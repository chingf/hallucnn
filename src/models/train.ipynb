{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fe309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from pbranchednetwork_a1 import PBranchedNetwork_A1SeparateHP\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6565d",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c151af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "NUM_EPOCHS = 50\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "lr = 1E-4\n",
    "checkpoints_dir = '../../models/checkpoints/'\n",
    "tensorboard_dir = '../../models/tensorboard/'\n",
    "datafile = '../../data/seed_542_word_clean_random_order.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991aed74",
   "metadata": {},
   "source": [
    "# Load network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b219f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/Users/chingfang/Code/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    }
   ],
   "source": [
    "net = BranchedNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6155c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = PBranchedNetwork_A1SeparateHP(net, build_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5082319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet.eval()\n",
    "pnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params':getattr(pnet,f\"pcoder{x+1}\").pmodule.parameters(), 'lr':lr} for x in range(pnet.number_of_pcoders)],\n",
    "    weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a9cf0c",
   "metadata": {},
   "source": [
    "# Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47c07ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/hallucnn/src/data/CleanSoundsDataset.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  self.data = torch.tensor(f['data']).reshape((-1, 164, 400))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = eval_dataset = CleanSoundsDataset(datafile, subset=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3a3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-sampling: comment the following 4 lines to use the whole dataset\n",
    "# train_indices = np.random.permutation(len(train_dataset))[:5000]\n",
    "# eval_indices  = np.random.permutation(len(eval_dataset))[:500]\n",
    "# train_dataset = Subset(train_dataset, train_indices)\n",
    "# eval_dataset  = Subset(eval_dataset,  eval_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1427cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8fca0",
   "metadata": {},
   "source": [
    "# Set up checkpoints and tensorboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408a9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoints_dir, f\"pnet-a1\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, 'pnet-a1-{epoch}-{type}.pth')\n",
    "\n",
    "# summarywriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_path = os.path.join(tensorboard_dir, f\"pnet-a1\")\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "sumwriter = SummaryWriter(tensorboard_path, filename_suffix=f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b849b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3049371f98b248ca\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3049371f98b248ca\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2544ca9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/opt/anaconda3/envs/hcnn/lib/python3.9/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([256, 164, 400])) that is different to the input size (torch.Size([256, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n",
      "/Users/chingfang/opt/anaconda3/envs/hcnn/lib/python3.9/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "/Users/chingfang/opt/anaconda3/envs/hcnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([256, 164, 400])) that is different to the input size (torch.Size([256, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [256/1000]\tLoss: 0.0258\n",
      "Training Epoch: 1 [512/1000]\tLoss: 0.0149\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_pcoders(pnet, optimizer, loss_function, epoch, train_loader, DEVICE, sumwriter)\n",
    "    eval_pcoders(pnet, loss_function, epoch, eval_loader, DEVICE, sumwriter)\n",
    "\n",
    "    # track the reconstruction of a single evaluation image through epochs\n",
    "    sumwriter.add_image('Training Feedback Weights/sample input', denormalize_torch_images(pnet.input_mem[0], MEAN, STD), epoch)\n",
    "    sumwriter.add_image('Training Feedback Weights/sample reconstruction', denormalize_torch_images(pnet.pcoder1.prd[0], MEAN, STD), epoch)\n",
    "    \n",
    "    # save checkpoints every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(pnet.state_dict(), checkpoint_path.format(epoch=epoch, type='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bae9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
