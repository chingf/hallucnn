{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b84576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fbc7e",
   "metadata": {},
   "source": [
    "# Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfb56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total training epoches\n",
    "EPOCH = 15\n",
    "\n",
    "SAME_PARAM = False           # to use the same parameters for all pcoders or not\n",
    "FF_START = True             # to start from feedforward initialization\n",
    "MAX_TIMESTEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fae3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 2\n",
    "noise_types = ['AudScene', 'Babble8Spkr', 'SpeakerShapedNoise']\n",
    "snr_levels = [-9., -6., -3.,  0.,  3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c99b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path names\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Load network arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8efd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bdfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-50-regular.pth'\n",
    "fb_state_dict = torch.load(fb_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c46f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d783f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "        net, state_dict, build_graph, random_init,\n",
    "        ff_multiplier, fb_multiplier, er_multiplier,\n",
    "        same_param, device='cuda:0'):\n",
    "    \n",
    "    if same_param:\n",
    "        raise Exception('Not implemented!')\n",
    "    else:\n",
    "        pnet = PNetClass(\n",
    "            net, build_graph=build_graph, random_init=random_init,\n",
    "            ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier\n",
    "            )\n",
    "\n",
    "    pnet.load_state_dict(state_dict)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e13f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, epoch, dataloader, timesteps, loss_function, writer=None, tag='Clean'):\n",
    "    test_loss = np.zeros((timesteps+1,))\n",
    "    correct   = np.zeros((timesteps+1,))\n",
    "    for (images, labels) in dataloader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tt in range(timesteps+1):\n",
    "                if tt == 0:\n",
    "                    outputs, _ = net(images)\n",
    "                else:\n",
    "                    outputs, _ = net()\n",
    "                \n",
    "                loss = loss_function(outputs, labels)\n",
    "                test_loss[tt] += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct[tt] += preds.eq(labels).sum()\n",
    "\n",
    "    print()\n",
    "    for tt in range(timesteps+1):\n",
    "        test_loss[tt] /= len(dataloader.dataset)\n",
    "        correct[tt] /= len(dataloader.dataset)\n",
    "        print('Test set t = {:02d}: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "            tt,\n",
    "            test_loss[tt],\n",
    "            correct[tt]\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"{tag}Perf/Epoch#{epoch}\",\n",
    "                correct[tt], tt\n",
    "                )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d32cdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epoch, dataloader, timesteps, loss_function, optimizer, writer=None):\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "        net.reset()\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        ttloss = np.zeros((timesteps+1))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for tt in range(timesteps+1):\n",
    "            if tt == 0:\n",
    "                outputs, _ = net(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = loss.item()\n",
    "            else:\n",
    "                outputs, _ = net()\n",
    "                current_loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = current_loss.item()\n",
    "                loss += current_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.update_hyperparameters()\n",
    "            \n",
    "        print(f\"Training Epoch: {epoch} [{batch_index * 16 + len(images)}/{len(dataloader.dataset)}]\\tLoss: {loss.item():0.4f}\\tLR: {optimizer.param_groups[0]['lr']:0.6f}\")\n",
    "        for tt in range(timesteps+1):\n",
    "            print(f'{ttloss[tt]:0.4f}\\t', end='')\n",
    "        print()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"TrainingLoss/CE\", loss.item(),\n",
    "                (epoch-1)*len(dataloader) + batch_index\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68603d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_parameters(net, epoch, sumwriter, same_param=True):\n",
    "    if same_param:\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedforward\", getattr(net,f'ff_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedback\",    getattr(net,f'fb_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/error\",       getattr(net,f'errorm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/memory\",      getattr(net,f'mem_part').item(), epoch)\n",
    "\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedforward\", getattr(net,f'ffm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedback\",    getattr(net,f'fbm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/error\",       getattr(net,f'erm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/memory\",      1-getattr(net,f'ffm').item()-getattr(net,f'fbm').item(), epoch)\n",
    "    else:\n",
    "        for i in range(1, net.number_of_pcoders+1):\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedforward\", getattr(net,f'ffm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", 0, epoch)\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_error\", getattr(net,f'erm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item()-getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item(), epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb1fa7",
   "metadata": {},
   "source": [
    "# Main hyperparameter optimization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cf6488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(noise_type, snr_level):\n",
    "    # Load clean and noisy data\n",
    "    clean_ds_path = f'{engram_dir}training_dataset_random_order.hdf5'\n",
    "    clean_ds = CleanSoundsDataset(clean_ds_path)\n",
    "    clean_loader = torch.utils.data.DataLoader(\n",
    "        clean_ds,  batch_size=BATCH_SIZE,\n",
    "        shuffle=False, drop_last=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    noisy_ds = NoisyDataset(bg=noise_type, snr=snr_level)\n",
    "    noise_loader = torch.utils.data.DataLoader(\n",
    "        noisy_ds,  batch_size=BATCH_SIZE,\n",
    "        shuffle=True, drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    # Set up logs and network for training\n",
    "    net_dir = f'hyper_{noise_type}_snr{snr_level}'\n",
    "    if FF_START:\n",
    "        net_dir += '_FFstart'\n",
    "    if SAME_PARAM:\n",
    "        net_dir += '_shared'\n",
    "\n",
    "    sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "    net = BranchedNetwork() # Load original network\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet_fw = load_pnet( # Load FF PNet\n",
    "        net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "        ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    evaluate(\n",
    "        pnet_fw, 0, noise_loader, 1,\n",
    "        loss_function,\n",
    "        writer=sumwriter, tag='FeedForward')\n",
    "    del pnet_fw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load PNet for hyperparameter optimization\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = load_pnet(\n",
    "        net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "        ff_multiplier=0.33, fb_multiplier=0.33, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "\n",
    "    # Set up loss function and hyperparameters\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    hyperparams = [*pnet.get_hyperparameters()]\n",
    "    if SAME_PARAM:\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': hyperparams[:-1], 'lr':0.01},\n",
    "            {'params': hyperparams[-1:], 'lr':0.0001}], weight_decay=0.00001)\n",
    "    else:\n",
    "        fffbmem_hp = []\n",
    "        erm_hp = []\n",
    "        for pc in range(pnet.number_of_pcoders):\n",
    "            fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "            erm_hp.append(hyperparams[pc*4+3])\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': fffbmem_hp, 'lr':0.01},\n",
    "            {'params': erm_hp, 'lr':0.0001}], weight_decay=0.00001)\n",
    "\n",
    "    # Log initial hyperparameter and eval values\n",
    "    log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "    evaluate(\n",
    "        pnet, 0, noise_loader,\n",
    "        MAX_TIMESTEP, loss_function,\n",
    "        writer=sumwriter, tag='Noisy'\n",
    "        )\n",
    "\n",
    "    # Run epochs\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        train(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function, optimizer,\n",
    "            writer=sumwriter\n",
    "            )\n",
    "        log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "        hps = pnet.get_hyperparameters_values()\n",
    "        print(hps)\n",
    "\n",
    "        evaluate(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function,\n",
    "            writer=sumwriter, tag='Noisy'\n",
    "            )\n",
    "    # evaluate(\n",
    "    #     pnet, epoch, clean_loader,\n",
    "    #     timesteps=MAX_TIMESTEP, writer=sumwriter,\n",
    "    #     tag='Clean'\n",
    "    #     )\n",
    "    sumwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ba94982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.5143, Accuracy: 0.1812\n",
      "Test set t = 01: Average loss: 0.4853, Accuracy: 0.1758\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5163, Accuracy: 0.1812\n",
      "Test set t = 01: Average loss: 0.4876, Accuracy: 0.1758\n",
      "Test set t = 02: Average loss: 0.4772, Accuracy: 0.1829\n",
      "Test set t = 03: Average loss: 0.4891, Accuracy: 0.1723\n",
      "Test set t = 04: Average loss: 0.5168, Accuracy: 0.1421\n",
      "Test set t = 05: Average loss: 0.5555, Accuracy: 0.1208\n",
      "\n",
      "Training Epoch: 1 [10/563]\tLoss: 30.0502\tLR: 0.010000\n",
      "5.5910\t5.0671\t4.7328\t4.7108\t4.8560\t5.0926\t\n",
      "Training Epoch: 1 [26/563]\tLoss: 34.3668\tLR: 0.010000\n",
      "6.1533\t5.7779\t5.5205\t5.4747\t5.5966\t5.8439\t\n",
      "Training Epoch: 1 [42/563]\tLoss: 36.0857\tLR: 0.010000\n",
      "6.2206\t6.0012\t5.8223\t5.8077\t5.9562\t6.2776\t\n",
      "Training Epoch: 1 [58/563]\tLoss: 29.1049\tLR: 0.010000\n",
      "5.1213\t4.7311\t4.5545\t4.6771\t4.8815\t5.1394\t\n",
      "Training Epoch: 1 [74/563]\tLoss: 33.9697\tLR: 0.010000\n",
      "6.2441\t5.6931\t5.3953\t5.3634\t5.5321\t5.7417\t\n",
      "Training Epoch: 1 [90/563]\tLoss: 29.7530\tLR: 0.010000\n",
      "5.0797\t4.8384\t4.7470\t4.8096\t5.0140\t5.2642\t\n",
      "Training Epoch: 1 [106/563]\tLoss: 32.7381\tLR: 0.010000\n",
      "5.6028\t5.3562\t5.2366\t5.3218\t5.5133\t5.7075\t\n",
      "Training Epoch: 1 [122/563]\tLoss: 27.3403\tLR: 0.010000\n",
      "4.4134\t4.1955\t4.1593\t4.3823\t4.8366\t5.3531\t\n",
      "Training Epoch: 1 [138/563]\tLoss: 32.8125\tLR: 0.010000\n",
      "5.6122\t5.3231\t5.1756\t5.2284\t5.5081\t5.9651\t\n",
      "Training Epoch: 1 [154/563]\tLoss: 36.0139\tLR: 0.010000\n",
      "6.4052\t6.0650\t5.7916\t5.7220\t5.8389\t6.1912\t\n",
      "Training Epoch: 1 [170/563]\tLoss: 37.2057\tLR: 0.010000\n",
      "6.2383\t5.9547\t5.8645\t5.9549\t6.3069\t6.8865\t\n",
      "Training Epoch: 1 [186/563]\tLoss: 29.1710\tLR: 0.010000\n",
      "4.7245\t4.5614\t4.5607\t4.7443\t5.0700\t5.5101\t\n",
      "Training Epoch: 1 [202/563]\tLoss: 28.8490\tLR: 0.010000\n",
      "4.7293\t4.6059\t4.5943\t4.7075\t4.9454\t5.2665\t\n",
      "Training Epoch: 1 [218/563]\tLoss: 33.0403\tLR: 0.010000\n",
      "5.9370\t5.6543\t5.4061\t5.3395\t5.3407\t5.3626\t\n",
      "Training Epoch: 1 [234/563]\tLoss: 23.6791\tLR: 0.010000\n",
      "3.6831\t3.6290\t3.7097\t3.9119\t4.2199\t4.5255\t\n",
      "Training Epoch: 1 [250/563]\tLoss: 36.3001\tLR: 0.010000\n",
      "7.2491\t6.5300\t5.9285\t5.5997\t5.4744\t5.5184\t\n",
      "Training Epoch: 1 [266/563]\tLoss: 16.9909\tLR: 0.010000\n",
      "3.1248\t2.7920\t2.5852\t2.5991\t2.7896\t3.1001\t\n",
      "Training Epoch: 1 [282/563]\tLoss: 30.8638\tLR: 0.010000\n",
      "5.4625\t5.1040\t4.9153\t4.9387\t5.0983\t5.3450\t\n",
      "Training Epoch: 1 [298/563]\tLoss: 29.6941\tLR: 0.010000\n",
      "5.3171\t5.1219\t4.9513\t4.8354\t4.7549\t4.7134\t\n",
      "Training Epoch: 1 [314/563]\tLoss: 30.8387\tLR: 0.010000\n",
      "5.3300\t5.0509\t4.9165\t4.9551\t5.1379\t5.4482\t\n",
      "Training Epoch: 1 [330/563]\tLoss: 22.9963\tLR: 0.010000\n",
      "3.8007\t3.6425\t3.6194\t3.7319\t3.9599\t4.2418\t\n",
      "Training Epoch: 1 [346/563]\tLoss: 28.3859\tLR: 0.010000\n",
      "4.9530\t4.7038\t4.5676\t4.5904\t4.7023\t4.8688\t\n",
      "Training Epoch: 1 [362/563]\tLoss: 29.7044\tLR: 0.010000\n",
      "4.8008\t4.5599\t4.5809\t4.8160\t5.2374\t5.7094\t\n",
      "Training Epoch: 1 [378/563]\tLoss: 26.4446\tLR: 0.010000\n",
      "4.3674\t4.2289\t4.2167\t4.3111\t4.5164\t4.8040\t\n",
      "Training Epoch: 1 [394/563]\tLoss: 31.0123\tLR: 0.010000\n",
      "5.3814\t5.1378\t4.9827\t4.9842\t5.1306\t5.3956\t\n",
      "Training Epoch: 1 [410/563]\tLoss: 33.2644\tLR: 0.010000\n",
      "5.7695\t5.5041\t5.3601\t5.3753\t5.5167\t5.7388\t\n",
      "Training Epoch: 1 [426/563]\tLoss: 36.2303\tLR: 0.010000\n",
      "6.9775\t6.5588\t6.0983\t5.7588\t5.5364\t5.3005\t\n",
      "Training Epoch: 1 [442/563]\tLoss: 31.0568\tLR: 0.010000\n",
      "5.2919\t5.1198\t5.0655\t5.0943\t5.1784\t5.3070\t\n",
      "Training Epoch: 1 [458/563]\tLoss: 28.3429\tLR: 0.010000\n",
      "5.0862\t4.8617\t4.6486\t4.5459\t4.5617\t4.6389\t\n",
      "Training Epoch: 1 [474/563]\tLoss: 29.4845\tLR: 0.010000\n",
      "5.7674\t5.2748\t4.8244\t4.5517\t4.4871\t4.5792\t\n",
      "Training Epoch: 1 [490/563]\tLoss: 24.4984\tLR: 0.010000\n",
      "4.2219\t4.0089\t3.9067\t3.9506\t4.1226\t4.2877\t\n",
      "Training Epoch: 1 [506/563]\tLoss: 22.8922\tLR: 0.010000\n",
      "3.5529\t3.4815\t3.5542\t3.7811\t4.0971\t4.4254\t\n",
      "Training Epoch: 1 [522/563]\tLoss: 29.5245\tLR: 0.010000\n",
      "5.2124\t5.0019\t4.8018\t4.7183\t4.8073\t4.9828\t\n",
      "Training Epoch: 1 [538/563]\tLoss: 30.8771\tLR: 0.010000\n",
      "5.5857\t5.3461\t5.0705\t4.8948\t4.9129\t5.0672\t\n",
      "Training Epoch: 1 [554/563]\tLoss: 24.2437\tLR: 0.010000\n",
      "4.0600\t3.8655\t3.7739\t3.8599\t4.1524\t4.5320\t\n",
      "Training Epoch: 1 [570/563]\tLoss: 34.3365\tLR: 0.010000\n",
      "6.0679\t5.8414\t5.6466\t5.5609\t5.5725\t5.6473\t\n",
      "Training Epoch: 1 [586/563]\tLoss: 25.2131\tLR: 0.010000\n",
      "4.1337\t4.0834\t4.0831\t4.1501\t4.2727\t4.4901\t\n",
      "Training Epoch: 1 [602/563]\tLoss: 20.9469\tLR: 0.010000\n",
      "3.2107\t3.1987\t3.2858\t3.4916\t3.7520\t4.0081\t\n",
      "Training Epoch: 1 [618/563]\tLoss: 27.1286\tLR: 0.010000\n",
      "4.5431\t4.4765\t4.4115\t4.4270\t4.5444\t4.7261\t\n",
      "Training Epoch: 1 [634/563]\tLoss: 32.6590\tLR: 0.010000\n",
      "5.6363\t5.3598\t5.2317\t5.2722\t5.4439\t5.7151\t\n",
      "Training Epoch: 1 [650/563]\tLoss: 24.6371\tLR: 0.010000\n",
      "4.2368\t4.1632\t4.0946\t4.0310\t4.0200\t4.0916\t\n",
      "Training Epoch: 1 [666/563]\tLoss: 26.9469\tLR: 0.010000\n",
      "4.8009\t4.5311\t4.3406\t4.2943\t4.3788\t4.6011\t\n",
      "Training Epoch: 1 [682/563]\tLoss: 31.9345\tLR: 0.010000\n",
      "5.8681\t5.5592\t5.2558\t5.0881\t5.0516\t5.1117\t\n",
      "Training Epoch: 1 [698/563]\tLoss: 32.0952\tLR: 0.010000\n",
      "5.6888\t5.4349\t5.2436\t5.1718\t5.2115\t5.3446\t\n",
      "Training Epoch: 1 [714/563]\tLoss: 35.0916\tLR: 0.010000\n",
      "6.0418\t5.8664\t5.7479\t5.7327\t5.8001\t5.9027\t\n",
      "Training Epoch: 1 [730/563]\tLoss: 35.0056\tLR: 0.010000\n",
      "6.2064\t5.9855\t5.8008\t5.7036\t5.6713\t5.6379\t\n",
      "Training Epoch: 1 [746/563]\tLoss: 28.8267\tLR: 0.010000\n",
      "5.0282\t4.8703\t4.7537\t4.6734\t4.6965\t4.8046\t\n",
      "Training Epoch: 1 [762/563]\tLoss: 28.0743\tLR: 0.010000\n",
      "4.9351\t4.7186\t4.5799\t4.5425\t4.5915\t4.7067\t\n",
      "Training Epoch: 1 [778/563]\tLoss: 17.3281\tLR: 0.010000\n",
      "2.5486\t2.5907\t2.7178\t2.9035\t3.1572\t3.4103\t\n",
      "Training Epoch: 1 [794/563]\tLoss: 25.2728\tLR: 0.010000\n",
      "4.3177\t4.1503\t4.0539\t4.0818\t4.2257\t4.4435\t\n",
      "Training Epoch: 1 [810/563]\tLoss: 29.9629\tLR: 0.010000\n",
      "5.2419\t5.0489\t4.9000\t4.8502\t4.8960\t5.0259\t\n",
      "Training Epoch: 1 [826/563]\tLoss: 34.5249\tLR: 0.010000\n",
      "6.1798\t5.9784\t5.7392\t5.5688\t5.5222\t5.5366\t\n",
      "Training Epoch: 1 [842/563]\tLoss: 27.5299\tLR: 0.010000\n",
      "4.5682\t4.4819\t4.4556\t4.5223\t4.6548\t4.8473\t\n",
      "Training Epoch: 1 [858/563]\tLoss: 32.8504\tLR: 0.010000\n",
      "5.9485\t5.6715\t5.4128\t5.2753\t5.2454\t5.2969\t\n",
      "Training Epoch: 1 [874/563]\tLoss: 24.3432\tLR: 0.010000\n",
      "3.9356\t3.8732\t3.8991\t4.0174\t4.1928\t4.4252\t\n",
      "Training Epoch: 1 [890/563]\tLoss: 24.6798\tLR: 0.010000\n",
      "4.3992\t4.1908\t4.0109\t3.9509\t3.9901\t4.1378\t\n",
      "Training Epoch: 1 [899/563]\tLoss: 15.5162\tLR: 0.010000\n",
      "2.4302\t2.4895\t2.5405\t2.5770\t2.6655\t2.8134\t\n",
      "[0.35206323862075806, 0.21319961547851562, 0.4347371459007263, 0.009055787697434425, 0.3616902530193329, 0.23902437090873718, 0.39928537607192993, 0.009576830081641674, 0.2771238684654236, 0.2522335946559906, 0.4706425368785858, 0.006731577217578888, 0.325158029794693, 0.221040740609169, 0.453801229596138, 0.00680417800322175, 0.2076938897371292, 0.0, 0.7923061102628708, 0.012212713249027729]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5157, Accuracy: 0.1812\n",
      "Test set t = 01: Average loss: 0.4975, Accuracy: 0.1812\n",
      "Test set t = 02: Average loss: 0.4824, Accuracy: 0.1812\n",
      "Test set t = 03: Average loss: 0.4767, Accuracy: 0.1794\n",
      "Test set t = 04: Average loss: 0.4802, Accuracy: 0.1812\n",
      "Test set t = 05: Average loss: 0.4907, Accuracy: 0.1776\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([6, 164, 400])) that is different to the input size (torch.Size([6, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.6229, Accuracy: 0.0636\n",
      "Test set t = 01: Average loss: 0.5989, Accuracy: 0.0671\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6225, Accuracy: 0.0636\n",
      "Test set t = 01: Average loss: 0.5986, Accuracy: 0.0671\n",
      "Test set t = 02: Average loss: 0.5848, Accuracy: 0.0866\n",
      "Test set t = 03: Average loss: 0.5827, Accuracy: 0.0830\n",
      "Test set t = 04: Average loss: 0.5870, Accuracy: 0.0919\n",
      "Test set t = 05: Average loss: 0.5952, Accuracy: 0.0936\n",
      "\n",
      "Training Epoch: 1 [10/566]\tLoss: 33.5975\tLR: 0.010000\n",
      "5.7073\t5.5811\t5.5474\t5.5442\t5.5762\t5.6413\t\n",
      "Training Epoch: 1 [26/566]\tLoss: 31.5419\tLR: 0.010000\n",
      "5.3763\t5.2666\t5.1598\t5.1991\t5.2645\t5.2756\t\n",
      "Training Epoch: 1 [42/566]\tLoss: 38.8586\tLR: 0.010000\n",
      "6.8891\t6.6270\t6.4227\t6.3221\t6.2642\t6.3334\t\n",
      "Training Epoch: 1 [58/566]\tLoss: 40.4616\tLR: 0.010000\n",
      "7.2585\t6.9536\t6.6805\t6.5630\t6.5155\t6.4906\t\n",
      "Training Epoch: 1 [74/566]\tLoss: 37.5251\tLR: 0.010000\n",
      "6.9295\t6.4583\t6.0869\t5.9324\t5.9742\t6.1438\t\n",
      "Training Epoch: 1 [90/566]\tLoss: 36.2829\tLR: 0.010000\n",
      "6.3215\t6.1105\t5.9751\t5.9458\t5.9193\t6.0107\t\n",
      "Training Epoch: 1 [106/566]\tLoss: 41.5223\tLR: 0.010000\n",
      "7.8475\t7.2310\t6.6760\t6.4286\t6.5144\t6.8247\t\n",
      "Training Epoch: 1 [122/566]\tLoss: 36.6042\tLR: 0.010000\n",
      "6.5690\t6.1803\t5.9218\t5.9042\t5.9400\t6.0890\t\n",
      "Training Epoch: 1 [138/566]\tLoss: 34.4418\tLR: 0.010000\n",
      "6.1127\t5.7626\t5.5955\t5.6092\t5.6549\t5.7069\t\n",
      "Training Epoch: 1 [154/566]\tLoss: 36.3165\tLR: 0.010000\n",
      "6.1267\t6.0036\t5.9887\t6.0152\t6.0555\t6.1267\t\n",
      "Training Epoch: 1 [170/566]\tLoss: 28.5427\tLR: 0.010000\n",
      "4.5119\t4.5046\t4.6139\t4.8018\t4.9731\t5.1374\t\n",
      "Training Epoch: 1 [186/566]\tLoss: 32.8073\tLR: 0.010000\n",
      "5.9275\t5.6761\t5.4080\t5.3282\t5.3049\t5.1626\t\n",
      "Training Epoch: 1 [202/566]\tLoss: 31.5517\tLR: 0.010000\n",
      "5.0429\t5.0425\t5.1129\t5.2785\t5.4597\t5.6152\t\n",
      "Training Epoch: 1 [218/566]\tLoss: 37.3612\tLR: 0.010000\n",
      "6.6878\t6.3501\t6.1259\t6.0374\t6.0644\t6.0956\t\n",
      "Training Epoch: 1 [234/566]\tLoss: 34.3392\tLR: 0.010000\n",
      "6.3207\t5.9493\t5.6793\t5.5204\t5.4408\t5.4286\t\n",
      "Training Epoch: 1 [250/566]\tLoss: 37.5641\tLR: 0.010000\n",
      "6.7927\t6.4837\t6.2352\t6.0920\t5.9848\t5.9756\t\n",
      "Training Epoch: 1 [266/566]\tLoss: 34.1260\tLR: 0.010000\n",
      "5.8828\t5.7462\t5.6805\t5.6423\t5.5959\t5.5783\t\n",
      "Training Epoch: 1 [282/566]\tLoss: 35.2544\tLR: 0.010000\n",
      "6.1150\t6.0312\t5.9289\t5.7995\t5.7001\t5.6798\t\n",
      "Training Epoch: 1 [298/566]\tLoss: 32.8335\tLR: 0.010000\n",
      "5.6789\t5.5870\t5.5199\t5.4291\t5.3556\t5.2631\t\n",
      "Training Epoch: 1 [314/566]\tLoss: 30.6700\tLR: 0.010000\n",
      "5.1601\t4.9854\t4.9392\t5.0372\t5.1978\t5.3503\t\n",
      "Training Epoch: 1 [330/566]\tLoss: 31.5905\tLR: 0.010000\n",
      "5.1929\t5.1336\t5.1621\t5.2710\t5.4027\t5.4283\t\n",
      "Training Epoch: 1 [346/566]\tLoss: 37.2183\tLR: 0.010000\n",
      "6.5271\t6.2853\t6.1238\t6.0697\t6.0897\t6.1227\t\n",
      "Training Epoch: 1 [362/566]\tLoss: 36.6361\tLR: 0.010000\n",
      "6.6111\t6.1872\t5.9914\t5.9116\t5.9268\t6.0080\t\n",
      "Training Epoch: 1 [378/566]\tLoss: 38.4412\tLR: 0.010000\n",
      "6.6014\t6.3813\t6.2788\t6.2800\t6.4089\t6.4908\t\n",
      "Training Epoch: 1 [394/566]\tLoss: 39.5955\tLR: 0.010000\n",
      "7.0463\t6.6954\t6.4737\t6.3853\t6.4293\t6.5655\t\n",
      "Training Epoch: 1 [410/566]\tLoss: 41.4731\tLR: 0.010000\n",
      "7.3113\t6.9689\t6.7491\t6.7157\t6.7858\t6.9424\t\n",
      "Training Epoch: 1 [426/566]\tLoss: 32.8977\tLR: 0.010000\n",
      "5.7805\t5.4927\t5.3683\t5.3851\t5.4173\t5.4537\t\n",
      "Training Epoch: 1 [442/566]\tLoss: 34.7165\tLR: 0.010000\n",
      "5.9362\t5.7626\t5.6863\t5.7014\t5.7682\t5.8618\t\n",
      "Training Epoch: 1 [458/566]\tLoss: 36.8842\tLR: 0.010000\n",
      "6.9539\t6.4989\t6.1198\t5.8679\t5.7330\t5.7107\t\n",
      "Training Epoch: 1 [474/566]\tLoss: 35.1032\tLR: 0.010000\n",
      "5.9845\t5.8262\t5.7377\t5.7653\t5.8459\t5.9436\t\n",
      "Training Epoch: 1 [490/566]\tLoss: 34.9583\tLR: 0.010000\n",
      "6.3911\t6.0666\t5.8030\t5.6347\t5.5392\t5.5237\t\n",
      "Training Epoch: 1 [506/566]\tLoss: 36.1976\tLR: 0.010000\n",
      "6.2132\t6.0222\t5.9187\t5.8929\t5.9901\t6.1606\t\n",
      "Training Epoch: 1 [522/566]\tLoss: 36.6855\tLR: 0.010000\n",
      "6.5885\t6.3212\t6.0865\t5.9160\t5.8555\t5.9178\t\n",
      "Training Epoch: 1 [538/566]\tLoss: 31.2572\tLR: 0.010000\n",
      "5.3629\t5.2366\t5.1585\t5.1324\t5.1503\t5.2165\t\n",
      "Training Epoch: 1 [554/566]\tLoss: 35.5644\tLR: 0.010000\n",
      "6.2345\t6.0428\t5.8717\t5.7755\t5.8026\t5.8373\t\n",
      "Training Epoch: 1 [570/566]\tLoss: 39.5011\tLR: 0.010000\n",
      "7.1287\t6.7792\t6.5174\t6.3923\t6.3608\t6.3227\t\n",
      "Training Epoch: 1 [586/566]\tLoss: 36.0238\tLR: 0.010000\n",
      "6.0851\t5.8787\t5.8779\t5.9711\t6.0475\t6.1635\t\n",
      "Training Epoch: 1 [602/566]\tLoss: 35.8072\tLR: 0.010000\n",
      "5.8959\t5.8876\t5.9261\t5.9620\t6.0206\t6.1151\t\n",
      "Training Epoch: 1 [618/566]\tLoss: 31.2659\tLR: 0.010000\n",
      "4.8939\t4.9015\t5.0006\t5.2123\t5.5037\t5.7538\t\n",
      "Training Epoch: 1 [634/566]\tLoss: 36.1376\tLR: 0.010000\n",
      "6.2835\t6.0334\t5.8864\t5.9115\t5.9849\t6.0380\t\n",
      "Training Epoch: 1 [650/566]\tLoss: 38.3531\tLR: 0.010000\n",
      "6.8445\t6.5064\t6.2709\t6.2201\t6.2403\t6.2708\t\n",
      "Training Epoch: 1 [666/566]\tLoss: 37.4800\tLR: 0.010000\n",
      "6.7440\t6.4349\t6.1938\t6.0663\t6.0347\t6.0064\t\n",
      "Training Epoch: 1 [682/566]\tLoss: 33.9357\tLR: 0.010000\n",
      "5.7282\t5.5758\t5.5435\t5.5974\t5.6938\t5.7970\t\n",
      "Training Epoch: 1 [698/566]\tLoss: 35.7813\tLR: 0.010000\n",
      "6.2746\t6.0908\t5.9561\t5.8564\t5.8209\t5.7825\t\n",
      "Training Epoch: 1 [714/566]\tLoss: 37.9102\tLR: 0.010000\n",
      "6.8779\t6.5100\t6.2106\t6.0828\t6.0892\t6.1396\t\n",
      "Training Epoch: 1 [730/566]\tLoss: 31.0560\tLR: 0.010000\n",
      "4.9454\t4.9822\t5.0659\t5.1954\t5.3565\t5.5106\t\n",
      "Training Epoch: 1 [746/566]\tLoss: 36.9226\tLR: 0.010000\n",
      "6.6179\t6.2494\t6.0359\t5.9788\t5.9818\t6.0588\t\n",
      "Training Epoch: 1 [762/566]\tLoss: 33.0753\tLR: 0.010000\n",
      "5.3144\t5.2800\t5.3587\t5.5323\t5.7049\t5.8850\t\n",
      "Training Epoch: 1 [778/566]\tLoss: 36.8007\tLR: 0.010000\n",
      "6.5226\t6.2911\t6.1086\t6.0118\t5.9641\t5.9024\t\n",
      "Training Epoch: 1 [794/566]\tLoss: 36.3228\tLR: 0.010000\n",
      "6.1239\t5.9981\t5.9430\t5.9870\t6.0716\t6.1991\t\n",
      "Training Epoch: 1 [810/566]\tLoss: 34.3502\tLR: 0.010000\n",
      "5.9407\t5.7672\t5.6626\t5.6459\t5.6641\t5.6697\t\n",
      "Training Epoch: 1 [826/566]\tLoss: 34.8011\tLR: 0.010000\n",
      "6.3089\t5.9755\t5.7416\t5.6312\t5.5878\t5.5560\t\n",
      "Training Epoch: 1 [842/566]\tLoss: 33.2214\tLR: 0.010000\n",
      "5.7888\t5.5954\t5.4686\t5.4344\t5.4389\t5.4952\t\n",
      "Training Epoch: 1 [858/566]\tLoss: 35.4182\tLR: 0.010000\n",
      "6.0629\t5.8998\t5.8121\t5.8324\t5.8600\t5.9512\t\n",
      "Training Epoch: 1 [874/566]\tLoss: 37.7240\tLR: 0.010000\n",
      "7.0147\t6.6614\t6.2849\t6.0333\t5.8884\t5.8412\t\n",
      "Training Epoch: 1 [890/566]\tLoss: 31.8254\tLR: 0.010000\n",
      "5.5674\t5.3710\t5.2141\t5.1856\t5.2188\t5.2686\t\n",
      "Training Epoch: 1 [902/566]\tLoss: 30.9331\tLR: 0.010000\n",
      "4.6273\t4.8090\t5.0195\t5.2700\t5.5151\t5.6922\t\n",
      "[0.29067349433898926, 0.34282007813453674, 0.366506427526474, 0.011456404812633991, 0.36587727069854736, 0.2867208421230316, 0.347401887178421, 0.011864917352795601, 0.32944098114967346, 0.2828037738800049, 0.38775524497032166, 0.008226782083511353, 0.31641119718551636, 0.2799972593784332, 0.4035915434360504, 0.007118550129234791, 0.2590295076370239, 0.0, 0.7409704923629761, 0.010374649427831173]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6212, Accuracy: 0.0636\n",
      "Test set t = 01: Average loss: 0.6001, Accuracy: 0.0671\n",
      "Test set t = 02: Average loss: 0.5857, Accuracy: 0.0795\n",
      "Test set t = 03: Average loss: 0.5812, Accuracy: 0.0830\n",
      "Test set t = 04: Average loss: 0.5828, Accuracy: 0.0883\n",
      "Test set t = 05: Average loss: 0.5874, Accuracy: 0.0954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noise_type in noise_types:\n",
    "    for snr_level in snr_levels:\n",
    "        print(\"=====================\")\n",
    "        print(f'{noise_type}, for SNR {snr_level}')\n",
    "        print(\"=====================\")\n",
    "        train_and_eval(noise_type, snr_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc22e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
