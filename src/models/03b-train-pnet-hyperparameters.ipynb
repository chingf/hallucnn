{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b84576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fbc7e",
   "metadata": {},
   "source": [
    "# Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfb56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total training epoches\n",
    "EPOCH = 10\n",
    "\n",
    "SAME_PARAM = False           # to use the same parameters for all pcoders or not\n",
    "FF_START = True             # to start from feedforward initialization\n",
    "MAX_TIMESTEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fae3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 2\n",
    "NOISE_TYPE = 'AudScene'\n",
    "NOISE_SNR = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c99b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path names\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'\n",
    "\n",
    "net_dir = 'hyper_audscene_snr3'\n",
    "if FF_START:\n",
    "    net_dir += '_FFstart'\n",
    "if SAME_PARAM:\n",
    "    net_dir += '_shared'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Load network arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8efd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30bdfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-50-regular.pth'\n",
    "fb_state_dict = torch.load(fb_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434bd5b",
   "metadata": {},
   "source": [
    "# Load clean and noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf6488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ds_path = f'{engram_dir}training_dataset_random_order.hdf5'\n",
    "clean_ds = CleanSoundsDataset(clean_ds_path)\n",
    "clean_loader = torch.utils.data.DataLoader(\n",
    "    clean_ds,  batch_size=BATCH_SIZE,\n",
    "    shuffle=False, drop_last=False, num_workers=NUM_WORKERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65136a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_ds = NoisyDataset(bg=NOISE_TYPE, snr=NOISE_SNR)\n",
    "noise_loader = torch.utils.data.DataLoader(\n",
    "    noisy_ds,  batch_size=BATCH_SIZE,\n",
    "    shuffle=True, drop_last=False,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c46f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d783f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "        net, state_dict, build_graph, random_init,\n",
    "        ff_multiplier, fb_multiplier, er_multiplier,\n",
    "        same_param, device='cuda:0'):\n",
    "    \n",
    "    if same_param:\n",
    "        raise Exception('Not implemented!')\n",
    "    else:\n",
    "        pnet = PNetClass(\n",
    "            net, build_graph=build_graph, random_init=random_init,\n",
    "            ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier\n",
    "            )\n",
    "\n",
    "    pnet.load_state_dict(state_dict)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e13f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, epoch, dataloader, timesteps, writer=None, tag='Clean'):\n",
    "    test_loss = np.zeros((timesteps+1,))\n",
    "    correct   = np.zeros((timesteps+1,))\n",
    "    for (images, labels) in dataloader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tt in range(timesteps+1):\n",
    "                if tt == 0:\n",
    "                    outputs, _ = net(images)\n",
    "                else:\n",
    "                    outputs, _ = net()\n",
    "                \n",
    "                loss = loss_function(outputs, labels)\n",
    "                test_loss[tt] += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct[tt] += preds.eq(labels).sum()\n",
    "\n",
    "    print()\n",
    "    for tt in range(timesteps+1):\n",
    "        test_loss[tt] /= len(dataloader.dataset)\n",
    "        correct[tt] /= len(dataloader.dataset)\n",
    "        print('Test set t = {:02d}: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "            tt,\n",
    "            test_loss[tt],\n",
    "            correct[tt]\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"{tag}Perf/Epoch#{epoch}\",\n",
    "                correct[tt], tt\n",
    "                )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32cdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epoch, dataloader, timesteps, writer=None):\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "        net.reset()\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        ttloss = np.zeros((timesteps+1))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for tt in range(timesteps+1):\n",
    "            if tt == 0:\n",
    "                outputs, _ = net(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = loss.item()\n",
    "            else:\n",
    "                outputs, _ = net()\n",
    "                current_loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = current_loss.item()\n",
    "                loss += current_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.update_hyperparameters()\n",
    "            \n",
    "        print(f\"Training Epoch: {epoch} [{batch_index * 16 + len(images)}/{len(dataloader.dataset)}]\\tLoss: {loss.item():0.4f}\\tLR: {optimizer.param_groups[0]['lr']:0.6f}\")\n",
    "        for tt in range(timesteps+1):\n",
    "            print(f'{ttloss[tt]:0.4f}\\t', end='')\n",
    "        print()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"TrainingLoss/CE\", loss.item(),\n",
    "                (epoch-1)*len(dataloader) + batch_index\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68603d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_parameters(net, epoch, sumwriter, same_param=True):\n",
    "    if same_param:\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedforward\", getattr(net,f'ff_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedback\",    getattr(net,f'fb_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/error\",       getattr(net,f'errorm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/memory\",      getattr(net,f'mem_part').item(), epoch)\n",
    "\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedforward\", getattr(net,f'ffm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedback\",    getattr(net,f'fbm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/error\",       getattr(net,f'erm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/memory\",      1-getattr(net,f'ffm').item()-getattr(net,f'fbm').item(), epoch)\n",
    "    else:\n",
    "        for i in range(1, net.number_of_pcoders+1):\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedforward\", getattr(net,f'ffm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", 0, epoch)\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_error\", getattr(net,f'erm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item()-getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item(), epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe9384",
   "metadata": {},
   "source": [
    "# Set up logs and files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5ab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([10, 164, 400])) that is different to the input size (torch.Size([10, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([7, 164, 400])) that is different to the input size (torch.Size([7, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.3199, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2807, Accuracy: 0.5485\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "\n",
    "# Load original network\n",
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "\n",
    "# Load FF PNet\n",
    "pnet_fw = load_pnet(\n",
    "    net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "    ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "    same_param=SAME_PARAM, device='cuda:0'\n",
    "    )\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "evaluate(\n",
    "    pnet_fw, 0, noise_loader, timesteps=1,\n",
    "    writer=sumwriter, tag='FeedForward')\n",
    "del pnet_fw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa0d76",
   "metadata": {},
   "source": [
    "# Run training script for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a7c256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3173, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2786, Accuracy: 0.5485\n",
      "Test set t = 02: Average loss: 0.2525, Accuracy: 0.5397\n",
      "Test set t = 03: Average loss: 0.2559, Accuracy: 0.5115\n",
      "Test set t = 04: Average loss: 0.2961, Accuracy: 0.4356\n",
      "Test set t = 05: Average loss: 0.3597, Accuracy: 0.3122\n",
      "\n",
      "Training Epoch: 1 [10/567]\tLoss: 15.6441\tLR: 0.010000\n",
      "2.6925\t2.4166\t2.2594\t2.4088\t2.7410\t3.1259\t\n",
      "Training Epoch: 1 [26/567]\tLoss: 18.6096\tLR: 0.010000\n",
      "2.4786\t2.4531\t2.5728\t2.9338\t3.6694\t4.5019\t\n",
      "Training Epoch: 1 [42/567]\tLoss: 19.5718\tLR: 0.010000\n",
      "3.8044\t3.3485\t2.9770\t2.8534\t3.0675\t3.5210\t\n",
      "Training Epoch: 1 [58/567]\tLoss: 14.1749\tLR: 0.010000\n",
      "1.7344\t1.6771\t1.8429\t2.2919\t2.9300\t3.6986\t\n",
      "Training Epoch: 1 [74/567]\tLoss: 29.2312\tLR: 0.010000\n",
      "6.5995\t5.5979\t4.5505\t3.9996\t4.0304\t4.4533\t\n",
      "Training Epoch: 1 [90/567]\tLoss: 23.9452\tLR: 0.010000\n",
      "5.4958\t4.5412\t3.6797\t3.2100\t3.2820\t3.7365\t\n",
      "Training Epoch: 1 [106/567]\tLoss: 24.3836\tLR: 0.010000\n",
      "5.4104\t4.6742\t3.9541\t3.4352\t3.3657\t3.5440\t\n",
      "Training Epoch: 1 [122/567]\tLoss: 17.4182\tLR: 0.010000\n",
      "3.7950\t3.2435\t2.6332\t2.2998\t2.4423\t3.0046\t\n",
      "Training Epoch: 1 [138/567]\tLoss: 17.3975\tLR: 0.010000\n",
      "3.4240\t2.9219\t2.5578\t2.4418\t2.7497\t3.3022\t\n",
      "Training Epoch: 1 [154/567]\tLoss: 13.9114\tLR: 0.010000\n",
      "2.2754\t2.0991\t1.9949\t2.0655\t2.4304\t3.0462\t\n",
      "Training Epoch: 1 [170/567]\tLoss: 20.8773\tLR: 0.010000\n",
      "4.0258\t3.6410\t3.3453\t3.1620\t3.1996\t3.5036\t\n",
      "Training Epoch: 1 [186/567]\tLoss: 13.9590\tLR: 0.010000\n",
      "2.5321\t2.2149\t2.0621\t2.0966\t2.3204\t2.7328\t\n",
      "Training Epoch: 1 [202/567]\tLoss: 12.3127\tLR: 0.010000\n",
      "1.7083\t1.5445\t1.5118\t1.8355\t2.4699\t3.2428\t\n",
      "Training Epoch: 1 [218/567]\tLoss: 12.4726\tLR: 0.010000\n",
      "1.7339\t1.6722\t1.6351\t1.8602\t2.3950\t3.1763\t\n",
      "Training Epoch: 1 [234/567]\tLoss: 10.9125\tLR: 0.010000\n",
      "2.3888\t2.0154\t1.6773\t1.5009\t1.5358\t1.7944\t\n",
      "Training Epoch: 1 [250/567]\tLoss: 23.4621\tLR: 0.010000\n",
      "4.8342\t4.3119\t3.7530\t3.4310\t3.4499\t3.6821\t\n",
      "Training Epoch: 1 [266/567]\tLoss: 18.3232\tLR: 0.010000\n",
      "3.6563\t3.2072\t2.8914\t2.7334\t2.7636\t3.0712\t\n",
      "Training Epoch: 1 [282/567]\tLoss: 13.7848\tLR: 0.010000\n",
      "2.0157\t1.8812\t1.9062\t2.1236\t2.5872\t3.2709\t\n",
      "Training Epoch: 1 [298/567]\tLoss: 23.9075\tLR: 0.010000\n",
      "5.1265\t4.5539\t3.9372\t3.4871\t3.3390\t3.4637\t\n",
      "Training Epoch: 1 [314/567]\tLoss: 10.4590\tLR: 0.010000\n",
      "1.5924\t1.5303\t1.5292\t1.6452\t1.8965\t2.2654\t\n",
      "Training Epoch: 1 [330/567]\tLoss: 9.5617\tLR: 0.010000\n",
      "1.4783\t1.2672\t1.2231\t1.3802\t1.7958\t2.4170\t\n",
      "Training Epoch: 1 [346/567]\tLoss: 16.6689\tLR: 0.010000\n",
      "3.8841\t3.3921\t2.7796\t2.3452\t2.1455\t2.1224\t\n",
      "Training Epoch: 1 [362/567]\tLoss: 12.4885\tLR: 0.010000\n",
      "2.1916\t2.0530\t1.9252\t1.9016\t2.0544\t2.3628\t\n",
      "Training Epoch: 1 [378/567]\tLoss: 27.2041\tLR: 0.010000\n",
      "5.7660\t5.1583\t4.4821\t3.9796\t3.8218\t3.9964\t\n",
      "Training Epoch: 1 [394/567]\tLoss: 20.7244\tLR: 0.010000\n",
      "4.0353\t3.5636\t3.1558\t3.0755\t3.2922\t3.6020\t\n",
      "Training Epoch: 1 [410/567]\tLoss: 6.9599\tLR: 0.010000\n",
      "1.4334\t1.2606\t1.0804\t0.9688\t0.9836\t1.2331\t\n",
      "Training Epoch: 1 [426/567]\tLoss: 19.6184\tLR: 0.010000\n",
      "3.8177\t3.4869\t3.1719\t2.9269\t2.9426\t3.2724\t\n",
      "Training Epoch: 1 [442/567]\tLoss: 14.0996\tLR: 0.010000\n",
      "2.0358\t2.0128\t2.0565\t2.2667\t2.6175\t3.1102\t\n",
      "Training Epoch: 1 [458/567]\tLoss: 16.7069\tLR: 0.010000\n",
      "3.6728\t3.1467\t2.6330\t2.3466\t2.3324\t2.5755\t\n",
      "Training Epoch: 1 [474/567]\tLoss: 9.3418\tLR: 0.010000\n",
      "1.1364\t1.1615\t1.2642\t1.5031\t1.8842\t2.3925\t\n",
      "Training Epoch: 1 [490/567]\tLoss: 7.4237\tLR: 0.010000\n",
      "0.5996\t0.6718\t0.8298\t1.1448\t1.6984\t2.4793\t\n",
      "Training Epoch: 1 [506/567]\tLoss: 16.5555\tLR: 0.010000\n",
      "3.3476\t2.9324\t2.5798\t2.4009\t2.4872\t2.8076\t\n",
      "Training Epoch: 1 [522/567]\tLoss: 14.5467\tLR: 0.010000\n",
      "2.8739\t2.5365\t2.2216\t2.0878\t2.2205\t2.6064\t\n",
      "Training Epoch: 1 [538/567]\tLoss: 25.0082\tLR: 0.010000\n",
      "5.2841\t4.7270\t4.1064\t3.6720\t3.5387\t3.6800\t\n",
      "Training Epoch: 1 [554/567]\tLoss: 10.5805\tLR: 0.010000\n",
      "1.7581\t1.6125\t1.5536\t1.6289\t1.8273\t2.2001\t\n",
      "Training Epoch: 1 [570/567]\tLoss: 7.9781\tLR: 0.010000\n",
      "1.1257\t1.0817\t1.1358\t1.2871\t1.5107\t1.8371\t\n",
      "Training Epoch: 1 [586/567]\tLoss: 21.1791\tLR: 0.010000\n",
      "4.0259\t3.6153\t3.3265\t3.2491\t3.3725\t3.5898\t\n",
      "Training Epoch: 1 [602/567]\tLoss: 10.3958\tLR: 0.010000\n",
      "1.7112\t1.6181\t1.5859\t1.6224\t1.7793\t2.0788\t\n",
      "Training Epoch: 1 [618/567]\tLoss: 18.4908\tLR: 0.010000\n",
      "3.2405\t3.0704\t2.9638\t2.9333\t3.0336\t3.2492\t\n",
      "Training Epoch: 1 [634/567]\tLoss: 7.2613\tLR: 0.010000\n",
      "1.0069\t1.0049\t1.0165\t1.1137\t1.3650\t1.7544\t\n",
      "Training Epoch: 1 [650/567]\tLoss: 14.4526\tLR: 0.010000\n",
      "2.8148\t2.6145\t2.3736\t2.1942\t2.1396\t2.3159\t\n",
      "Training Epoch: 1 [666/567]\tLoss: 17.5773\tLR: 0.010000\n",
      "3.0627\t2.9513\t2.8118\t2.7676\t2.8587\t3.1252\t\n",
      "Training Epoch: 1 [682/567]\tLoss: 14.7555\tLR: 0.010000\n",
      "2.9897\t2.6800\t2.3964\t2.2271\t2.1826\t2.2797\t\n",
      "Training Epoch: 1 [698/567]\tLoss: 9.8689\tLR: 0.010000\n",
      "1.4927\t1.4674\t1.4996\t1.5962\t1.7808\t2.0323\t\n",
      "Training Epoch: 1 [714/567]\tLoss: 24.5215\tLR: 0.010000\n",
      "4.7228\t4.2722\t3.9183\t3.7821\t3.8202\t4.0059\t\n",
      "Training Epoch: 1 [730/567]\tLoss: 13.6134\tLR: 0.010000\n",
      "2.6306\t2.3706\t2.1572\t2.0913\t2.1176\t2.2461\t\n",
      "Training Epoch: 1 [746/567]\tLoss: 8.0718\tLR: 0.010000\n",
      "1.0123\t0.9717\t1.0339\t1.2913\t1.6811\t2.0815\t\n",
      "Training Epoch: 1 [762/567]\tLoss: 12.7769\tLR: 0.010000\n",
      "2.4353\t2.2516\t2.0503\t1.9266\t1.9507\t2.1624\t\n",
      "Training Epoch: 1 [778/567]\tLoss: 9.5620\tLR: 0.010000\n",
      "1.2163\t1.2635\t1.3941\t1.6065\t1.8902\t2.1914\t\n",
      "Training Epoch: 1 [794/567]\tLoss: 23.9816\tLR: 0.010000\n",
      "4.9248\t4.5542\t4.1007\t3.7144\t3.4123\t3.2752\t\n",
      "Training Epoch: 1 [810/567]\tLoss: 20.0508\tLR: 0.010000\n",
      "4.3531\t3.8220\t3.2918\t2.9394\t2.7940\t2.8504\t\n",
      "Training Epoch: 1 [826/567]\tLoss: 20.5893\tLR: 0.010000\n",
      "4.3155\t3.9421\t3.4863\t3.1385\t2.9155\t2.7913\t\n",
      "Training Epoch: 1 [842/567]\tLoss: 20.0925\tLR: 0.010000\n",
      "3.3051\t3.2041\t3.1604\t3.2323\t3.4319\t3.7587\t\n",
      "Training Epoch: 1 [858/567]\tLoss: 24.5489\tLR: 0.010000\n",
      "5.5684\t4.9395\t4.2073\t3.5908\t3.2045\t3.0384\t\n",
      "Training Epoch: 1 [874/567]\tLoss: 15.5127\tLR: 0.010000\n",
      "2.6487\t2.5743\t2.4858\t2.4754\t2.5595\t2.7689\t\n",
      "Training Epoch: 1 [890/567]\tLoss: 31.7914\tLR: 0.010000\n",
      "7.0199\t6.1662\t5.3389\t4.7267\t4.3545\t4.1852\t\n",
      "Training Epoch: 1 [903/567]\tLoss: 25.0602\tLR: 0.010000\n",
      "4.8360\t4.5129\t4.1824\t3.9366\t3.8101\t3.7822\t\n",
      "[0.3543008863925934, 0.23985876142978668, 0.40584035217761993, 0.01250030379742384, 0.3687695562839508, 0.22953006625175476, 0.40170037746429443, 0.010548185557126999, 0.3516121506690979, 0.22371122241020203, 0.4246766269207001, 0.005975975189357996, 0.32557547092437744, 0.2389962524175644, 0.43542827665805817, 0.007230091840028763, 0.2131120264530182, 0.0, 0.7868879735469818, 0.0083623006939888]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3179, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2934, Accuracy: 0.5556\n",
      "Test set t = 02: Average loss: 0.2683, Accuracy: 0.5485\n",
      "Test set t = 03: Average loss: 0.2532, Accuracy: 0.5397\n",
      "Test set t = 04: Average loss: 0.2513, Accuracy: 0.5291\n",
      "Test set t = 05: Average loss: 0.2624, Accuracy: 0.5079\n",
      "\n",
      "Training Epoch: 2 [10/567]\tLoss: 5.9210\tLR: 0.010000\n",
      "0.8206\t0.7912\t0.7875\t0.8911\t1.1453\t1.4853\t\n",
      "Training Epoch: 2 [26/567]\tLoss: 25.8784\tLR: 0.010000\n",
      "5.2102\t4.7938\t4.3639\t4.0008\t3.7715\t3.7381\t\n",
      "Training Epoch: 2 [42/567]\tLoss: 15.6471\tLR: 0.010000\n",
      "2.7254\t2.6050\t2.5066\t2.4855\t2.5689\t2.7557\t\n",
      "Training Epoch: 2 [58/567]\tLoss: 11.4879\tLR: 0.010000\n",
      "2.1698\t1.9993\t1.8190\t1.7500\t1.7849\t1.9649\t\n",
      "Training Epoch: 2 [74/567]\tLoss: 15.0677\tLR: 0.010000\n",
      "2.6109\t2.4438\t2.3329\t2.3621\t2.5309\t2.7871\t\n",
      "Training Epoch: 2 [90/567]\tLoss: 6.3984\tLR: 0.010000\n",
      "0.6222\t0.6845\t0.8477\t1.0950\t1.4008\t1.7481\t\n",
      "Training Epoch: 2 [106/567]\tLoss: 26.4041\tLR: 0.010000\n",
      "6.0509\t5.3629\t4.5511\t3.8603\t3.4081\t3.1708\t\n",
      "Training Epoch: 2 [122/567]\tLoss: 18.2142\tLR: 0.010000\n",
      "3.0905\t3.0260\t2.9575\t2.9451\t3.0170\t3.1780\t\n",
      "Training Epoch: 2 [138/567]\tLoss: 17.9911\tLR: 0.010000\n",
      "3.9901\t3.5291\t3.0231\t2.6502\t2.4242\t2.3745\t\n",
      "Training Epoch: 2 [154/567]\tLoss: 8.8025\tLR: 0.010000\n",
      "1.2047\t1.2187\t1.2888\t1.4325\t1.6618\t1.9960\t\n",
      "Training Epoch: 2 [170/567]\tLoss: 18.1509\tLR: 0.010000\n",
      "3.6961\t3.3459\t2.9865\t2.7680\t2.6721\t2.6823\t\n",
      "Training Epoch: 2 [186/567]\tLoss: 15.3268\tLR: 0.010000\n",
      "2.8232\t2.6418\t2.4427\t2.3438\t2.4185\t2.6568\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [202/567]\tLoss: 32.4499\tLR: 0.010000\n",
      "6.7204\t6.1929\t5.5751\t5.0159\t4.6092\t4.3364\t\n",
      "Training Epoch: 2 [218/567]\tLoss: 13.2903\tLR: 0.010000\n",
      "2.3873\t2.2416\t2.1141\t2.0839\t2.1566\t2.3068\t\n",
      "Training Epoch: 2 [234/567]\tLoss: 18.8558\tLR: 0.010000\n",
      "3.9015\t3.5164\t3.1312\t2.8675\t2.7300\t2.7092\t\n",
      "Training Epoch: 2 [250/567]\tLoss: 16.7343\tLR: 0.010000\n",
      "3.0398\t2.8921\t2.7122\t2.6228\t2.6707\t2.7967\t\n",
      "Training Epoch: 2 [266/567]\tLoss: 15.4668\tLR: 0.010000\n",
      "3.0659\t2.8170\t2.5577\t2.3807\t2.3024\t2.3431\t\n",
      "Training Epoch: 2 [282/567]\tLoss: 8.0748\tLR: 0.010000\n",
      "1.0585\t1.0271\t1.1037\t1.2926\t1.6046\t1.9883\t\n",
      "Training Epoch: 2 [298/567]\tLoss: 14.9866\tLR: 0.010000\n",
      "2.6323\t2.4960\t2.3877\t2.3796\t2.4525\t2.6384\t\n",
      "Training Epoch: 2 [314/567]\tLoss: 7.5732\tLR: 0.010000\n",
      "0.9877\t1.0143\t1.0846\t1.2297\t1.4682\t1.7887\t\n",
      "Training Epoch: 2 [330/567]\tLoss: 15.6200\tLR: 0.010000\n",
      "3.0035\t2.8171\t2.5793\t2.4059\t2.3559\t2.4582\t\n",
      "Training Epoch: 2 [346/567]\tLoss: 8.8130\tLR: 0.010000\n",
      "1.2427\t1.2493\t1.3151\t1.4476\t1.6461\t1.9121\t\n",
      "Training Epoch: 2 [362/567]\tLoss: 10.5795\tLR: 0.010000\n",
      "1.8444\t1.7126\t1.6131\t1.6237\t1.7591\t2.0266\t\n",
      "Training Epoch: 2 [378/567]\tLoss: 19.2643\tLR: 0.010000\n",
      "3.6585\t3.4303\t3.1800\t3.0112\t2.9605\t3.0238\t\n",
      "Training Epoch: 2 [394/567]\tLoss: 17.8605\tLR: 0.010000\n",
      "3.7205\t3.4264\t3.0646\t2.7489\t2.5170\t2.3830\t\n",
      "Training Epoch: 2 [410/567]\tLoss: 7.5969\tLR: 0.010000\n",
      "1.2654\t1.2193\t1.1764\t1.1950\t1.2803\t1.4604\t\n",
      "Training Epoch: 2 [426/567]\tLoss: 19.0250\tLR: 0.010000\n",
      "4.0405\t3.6553\t3.1981\t2.8307\t2.6497\t2.6507\t\n",
      "Training Epoch: 2 [442/567]\tLoss: 18.9738\tLR: 0.010000\n",
      "4.1274\t3.6333\t3.1371\t2.7966\t2.6328\t2.6466\t\n",
      "Training Epoch: 2 [458/567]\tLoss: 18.5873\tLR: 0.010000\n",
      "3.8748\t3.4912\t3.0727\t2.7821\t2.6760\t2.6906\t\n",
      "Training Epoch: 2 [474/567]\tLoss: 12.5861\tLR: 0.010000\n",
      "2.2567\t2.0881\t1.9615\t1.9519\t2.0546\t2.2732\t\n",
      "Training Epoch: 2 [490/567]\tLoss: 14.4025\tLR: 0.010000\n",
      "2.7743\t2.5280\t2.3084\t2.1924\t2.2251\t2.3743\t\n",
      "Training Epoch: 2 [506/567]\tLoss: 29.1334\tLR: 0.010000\n",
      "6.1800\t5.6116\t5.0076\t4.4789\t4.0613\t3.7940\t\n",
      "Training Epoch: 2 [522/567]\tLoss: 22.5244\tLR: 0.010000\n",
      "4.4260\t4.1301\t3.7964\t3.5062\t3.3517\t3.3140\t\n",
      "Training Epoch: 2 [538/567]\tLoss: 10.6930\tLR: 0.010000\n",
      "1.7512\t1.6332\t1.6342\t1.7098\t1.8685\t2.0961\t\n",
      "Training Epoch: 2 [554/567]\tLoss: 9.5872\tLR: 0.010000\n",
      "1.5854\t1.4837\t1.4412\t1.4864\t1.6515\t1.9390\t\n",
      "Training Epoch: 2 [570/567]\tLoss: 19.6875\tLR: 0.010000\n",
      "3.9982\t3.6093\t3.2177\t2.9871\t2.9101\t2.9651\t\n",
      "Training Epoch: 2 [586/567]\tLoss: 27.9704\tLR: 0.010000\n",
      "6.2336\t5.5818\t4.7826\t4.1378\t3.7083\t3.5263\t\n",
      "Training Epoch: 2 [602/567]\tLoss: 19.7221\tLR: 0.010000\n",
      "3.8774\t3.6426\t3.3174\t3.0836\t2.9263\t2.8749\t\n",
      "Training Epoch: 2 [618/567]\tLoss: 16.9761\tLR: 0.010000\n",
      "3.6151\t3.1963\t2.8278\t2.5382\t2.3960\t2.4027\t\n",
      "Training Epoch: 2 [634/567]\tLoss: 8.7174\tLR: 0.010000\n",
      "1.1284\t1.2099\t1.2963\t1.4356\t1.6669\t1.9803\t\n",
      "Training Epoch: 2 [650/567]\tLoss: 20.4822\tLR: 0.010000\n",
      "3.9757\t3.7083\t3.3884\t3.1667\t3.0908\t3.1522\t\n",
      "Training Epoch: 2 [666/567]\tLoss: 19.2580\tLR: 0.010000\n",
      "3.8599\t3.5496\t3.2116\t2.9668\t2.8302\t2.8398\t\n",
      "Training Epoch: 2 [682/567]\tLoss: 14.4962\tLR: 0.010000\n",
      "2.9988\t2.7942\t2.4647\t2.1849\t2.0244\t2.0292\t\n",
      "Training Epoch: 2 [698/567]\tLoss: 11.8602\tLR: 0.010000\n",
      "1.9008\t1.8647\t1.8579\t1.9187\t2.0537\t2.2644\t\n",
      "Training Epoch: 2 [714/567]\tLoss: 19.6931\tLR: 0.010000\n",
      "4.1366\t3.7493\t3.3036\t2.9627\t2.7743\t2.7666\t\n",
      "Training Epoch: 2 [730/567]\tLoss: 18.5878\tLR: 0.010000\n",
      "3.5391\t3.2858\t3.0320\t2.8835\t2.8745\t2.9729\t\n",
      "Training Epoch: 2 [746/567]\tLoss: 10.8738\tLR: 0.010000\n",
      "1.6235\t1.5812\t1.5831\t1.7170\t1.9887\t2.3803\t\n",
      "Training Epoch: 2 [762/567]\tLoss: 10.9083\tLR: 0.010000\n",
      "1.8767\t1.7852\t1.6840\t1.6686\t1.8015\t2.0923\t\n",
      "Training Epoch: 2 [778/567]\tLoss: 10.8277\tLR: 0.010000\n",
      "1.5550\t1.5381\t1.5964\t1.7522\t2.0219\t2.3641\t\n",
      "Training Epoch: 2 [794/567]\tLoss: 24.2506\tLR: 0.010000\n",
      "5.3912\t4.7586\t4.1044\t3.5810\t3.2665\t3.1488\t\n",
      "Training Epoch: 2 [810/567]\tLoss: 25.1334\tLR: 0.010000\n",
      "5.5795\t4.9811\t4.2984\t3.7315\t3.3607\t3.1822\t\n",
      "Training Epoch: 2 [826/567]\tLoss: 25.6261\tLR: 0.010000\n",
      "5.2381\t4.8067\t4.2954\t3.9382\t3.7118\t3.6360\t\n",
      "Training Epoch: 2 [842/567]\tLoss: 14.9239\tLR: 0.010000\n",
      "3.0980\t2.8183\t2.5331\t2.2923\t2.1256\t2.0565\t\n",
      "Training Epoch: 2 [858/567]\tLoss: 17.6453\tLR: 0.010000\n",
      "3.5194\t3.2605\t2.9317\t2.6865\t2.5957\t2.6514\t\n",
      "Training Epoch: 2 [874/567]\tLoss: 15.8874\tLR: 0.010000\n",
      "2.6496\t2.5067\t2.4666\t2.5677\t2.7366\t2.9602\t\n",
      "Training Epoch: 2 [890/567]\tLoss: 18.5474\tLR: 0.010000\n",
      "3.9265\t3.5260\t3.1063\t2.7967\t2.6276\t2.5642\t\n",
      "Training Epoch: 2 [903/567]\tLoss: 13.1222\tLR: 0.010000\n",
      "1.9517\t1.9494\t1.9883\t2.1504\t2.3867\t2.6957\t\n",
      "[0.3560144901275635, 0.2731771171092987, 0.3708083927631378, 0.012688608840107918, 0.3979710638523102, 0.24000319838523865, 0.36202573776245117, 0.011807048693299294, 0.37785807251930237, 0.22218003869056702, 0.3999618887901306, 0.004097155295312405, 0.3433506190776825, 0.24464182555675507, 0.41200755536556244, 0.006045765243470669, 0.20812730491161346, 0.0, 0.7918726950883865, 0.006992894224822521]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3191, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2935, Accuracy: 0.5573\n",
      "Test set t = 02: Average loss: 0.2678, Accuracy: 0.5485\n",
      "Test set t = 03: Average loss: 0.2526, Accuracy: 0.5414\n",
      "Test set t = 04: Average loss: 0.2509, Accuracy: 0.5291\n",
      "Test set t = 05: Average loss: 0.2620, Accuracy: 0.5062\n",
      "\n",
      "Training Epoch: 3 [10/567]\tLoss: 8.4306\tLR: 0.010000\n",
      "1.3317\t1.2540\t1.2371\t1.3217\t1.5105\t1.7756\t\n",
      "Training Epoch: 3 [26/567]\tLoss: 12.9097\tLR: 0.010000\n",
      "2.8109\t2.5010\t2.1506\t1.8919\t1.7701\t1.7853\t\n",
      "Training Epoch: 3 [42/567]\tLoss: 7.4050\tLR: 0.010000\n",
      "1.0317\t1.0223\t1.0531\t1.1648\t1.4004\t1.7326\t\n",
      "Training Epoch: 3 [58/567]\tLoss: 28.2945\tLR: 0.010000\n",
      "6.0402\t5.4401\t4.8074\t4.2704\t3.9259\t3.8105\t\n",
      "Training Epoch: 3 [74/567]\tLoss: 12.8235\tLR: 0.010000\n",
      "2.2516\t2.0897\t1.9616\t1.9621\t2.1182\t2.4403\t\n",
      "Training Epoch: 3 [90/567]\tLoss: 19.8340\tLR: 0.010000\n",
      "3.8995\t3.4943\t3.1881\t3.0318\t3.0392\t3.1810\t\n",
      "Training Epoch: 3 [106/567]\tLoss: 26.8456\tLR: 0.010000\n",
      "5.4040\t5.0193\t4.5393\t4.1481\t3.9048\t3.8302\t\n",
      "Training Epoch: 3 [122/567]\tLoss: 14.4785\tLR: 0.010000\n",
      "2.9490\t2.7133\t2.4089\t2.1827\t2.0879\t2.1366\t\n",
      "Training Epoch: 3 [138/567]\tLoss: 15.2216\tLR: 0.010000\n",
      "3.0184\t2.7819\t2.5330\t2.3501\t2.2608\t2.2774\t\n",
      "Training Epoch: 3 [154/567]\tLoss: 28.7111\tLR: 0.010000\n",
      "5.5861\t5.1719\t4.7718\t4.4757\t4.3537\t4.3519\t\n",
      "Training Epoch: 3 [170/567]\tLoss: 19.4430\tLR: 0.010000\n",
      "4.0397\t3.6133\t3.1942\t2.9040\t2.7982\t2.8936\t\n",
      "Training Epoch: 3 [186/567]\tLoss: 22.5987\tLR: 0.010000\n",
      "4.3095\t4.0036\t3.7162\t3.5415\t3.4772\t3.5506\t\n",
      "Training Epoch: 3 [202/567]\tLoss: 11.2408\tLR: 0.010000\n",
      "1.7131\t1.7552\t1.7755\t1.8406\t1.9657\t2.1906\t\n",
      "Training Epoch: 3 [218/567]\tLoss: 29.4342\tLR: 0.010000\n",
      "6.1988\t5.6032\t4.9766\t4.4803\t4.1564\t4.0188\t\n",
      "Training Epoch: 3 [234/567]\tLoss: 6.6033\tLR: 0.010000\n",
      "0.8871\t0.8534\t0.8492\t0.9955\t1.3070\t1.7111\t\n",
      "Training Epoch: 3 [250/567]\tLoss: 27.5121\tLR: 0.010000\n",
      "6.2566\t5.5047\t4.6656\t3.9973\t3.6031\t3.4849\t\n",
      "Training Epoch: 3 [266/567]\tLoss: 30.4496\tLR: 0.010000\n",
      "6.5079\t5.8469\t5.1282\t4.5853\t4.2587\t4.1227\t\n",
      "Training Epoch: 3 [282/567]\tLoss: 8.7731\tLR: 0.010000\n",
      "1.4504\t1.4082\t1.3784\t1.3934\t1.4798\t1.6629\t\n",
      "Training Epoch: 3 [298/567]\tLoss: 7.6548\tLR: 0.010000\n",
      "1.1020\t1.0778\t1.0884\t1.2047\t1.4398\t1.7422\t\n",
      "Training Epoch: 3 [314/567]\tLoss: 8.8839\tLR: 0.010000\n",
      "1.3382\t1.2579\t1.2426\t1.3680\t1.6434\t2.0338\t\n",
      "Training Epoch: 3 [330/567]\tLoss: 18.1625\tLR: 0.010000\n",
      "3.8619\t3.4211\t3.0120\t2.7168\t2.5665\t2.5842\t\n",
      "Training Epoch: 3 [346/567]\tLoss: 12.1909\tLR: 0.010000\n",
      "2.2365\t2.0255\t1.8798\t1.8579\t1.9673\t2.2240\t\n",
      "Training Epoch: 3 [362/567]\tLoss: 18.3526\tLR: 0.010000\n",
      "3.5800\t3.2802\t2.9636\t2.7821\t2.7828\t2.9640\t\n",
      "Training Epoch: 3 [378/567]\tLoss: 18.2350\tLR: 0.010000\n",
      "3.6572\t3.3581\t3.0063\t2.7825\t2.6881\t2.7428\t\n",
      "Training Epoch: 3 [394/567]\tLoss: 13.0417\tLR: 0.010000\n",
      "2.4784\t2.2536\t2.0597\t1.9663\t2.0300\t2.2537\t\n",
      "Training Epoch: 3 [410/567]\tLoss: 13.4153\tLR: 0.010000\n",
      "2.5167\t2.3439\t2.1566\t2.0692\t2.1149\t2.2140\t\n",
      "Training Epoch: 3 [426/567]\tLoss: 12.5873\tLR: 0.010000\n",
      "2.5798\t2.3157\t2.0374\t1.8620\t1.8503\t1.9422\t\n",
      "Training Epoch: 3 [442/567]\tLoss: 13.3998\tLR: 0.010000\n",
      "2.4022\t2.2265\t2.0895\t2.0649\t2.1915\t2.4251\t\n",
      "Training Epoch: 3 [458/567]\tLoss: 14.5570\tLR: 0.010000\n",
      "2.7384\t2.5774\t2.3545\t2.2258\t2.2440\t2.4169\t\n",
      "Training Epoch: 3 [474/567]\tLoss: 10.2733\tLR: 0.010000\n",
      "1.6961\t1.6437\t1.6009\t1.6188\t1.7512\t1.9626\t\n",
      "Training Epoch: 3 [490/567]\tLoss: 18.7339\tLR: 0.010000\n",
      "3.5511\t3.3117\t3.0795\t2.9346\t2.8784\t2.9787\t\n",
      "Training Epoch: 3 [506/567]\tLoss: 9.1081\tLR: 0.010000\n",
      "1.0705\t1.1479\t1.3007\t1.5284\t1.8426\t2.2179\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [522/567]\tLoss: 21.8353\tLR: 0.010000\n",
      "4.6471\t4.1985\t3.6969\t3.2901\t3.0523\t2.9502\t\n",
      "Training Epoch: 3 [538/567]\tLoss: 5.1670\tLR: 0.010000\n",
      "0.6672\t0.6666\t0.7050\t0.8155\t1.0114\t1.3012\t\n",
      "Training Epoch: 3 [554/567]\tLoss: 13.1157\tLR: 0.010000\n",
      "2.5038\t2.2727\t2.0801\t2.0031\t2.0628\t2.1932\t\n",
      "Training Epoch: 3 [570/567]\tLoss: 7.3844\tLR: 0.010000\n",
      "1.1937\t1.1894\t1.1770\t1.1866\t1.2561\t1.3816\t\n",
      "Training Epoch: 3 [586/567]\tLoss: 19.6589\tLR: 0.010000\n",
      "4.2143\t3.7437\t3.2552\t2.9360\t2.7670\t2.7427\t\n",
      "Training Epoch: 3 [602/567]\tLoss: 19.2584\tLR: 0.010000\n",
      "3.8686\t3.6028\t3.2633\t2.9775\t2.7979\t2.7484\t\n",
      "Training Epoch: 3 [618/567]\tLoss: 23.5781\tLR: 0.010000\n",
      "4.8502\t4.4851\t4.0381\t3.6281\t3.3593\t3.2173\t\n",
      "Training Epoch: 3 [634/567]\tLoss: 18.9232\tLR: 0.010000\n",
      "3.4542\t3.1764\t3.0561\t3.0434\t3.0741\t3.1190\t\n",
      "Training Epoch: 3 [650/567]\tLoss: 14.2267\tLR: 0.010000\n",
      "2.6237\t2.4256\t2.2247\t2.1743\t2.2810\t2.4973\t\n",
      "Training Epoch: 3 [666/567]\tLoss: 26.3749\tLR: 0.010000\n",
      "5.3027\t4.9127\t4.4409\t4.0616\t3.8514\t3.8055\t\n",
      "Training Epoch: 3 [682/567]\tLoss: 15.5490\tLR: 0.010000\n",
      "2.9842\t2.7848\t2.5729\t2.4317\t2.3498\t2.4256\t\n",
      "Training Epoch: 3 [698/567]\tLoss: 20.1037\tLR: 0.010000\n",
      "4.2413\t3.7679\t3.2968\t2.9998\t2.8899\t2.9080\t\n",
      "Training Epoch: 3 [714/567]\tLoss: 21.7515\tLR: 0.010000\n",
      "4.3345\t3.8905\t3.5249\t3.3192\t3.2949\t3.3874\t\n",
      "Training Epoch: 3 [730/567]\tLoss: 20.6284\tLR: 0.010000\n",
      "4.0399\t3.7479\t3.4403\t3.2273\t3.0965\t3.0766\t\n",
      "Training Epoch: 3 [746/567]\tLoss: 8.5789\tLR: 0.010000\n",
      "1.2634\t1.2385\t1.2733\t1.3998\t1.5827\t1.8211\t\n",
      "Training Epoch: 3 [762/567]\tLoss: 9.5057\tLR: 0.010000\n",
      "1.3784\t1.3376\t1.3629\t1.5256\t1.7843\t2.1169\t\n",
      "Training Epoch: 3 [778/567]\tLoss: 13.4558\tLR: 0.010000\n",
      "2.6347\t2.3672\t2.1446\t2.0412\t2.0549\t2.2132\t\n",
      "Training Epoch: 3 [794/567]\tLoss: 11.3513\tLR: 0.010000\n",
      "1.9048\t1.7970\t1.7417\t1.7705\t1.9215\t2.2158\t\n",
      "Training Epoch: 3 [810/567]\tLoss: 9.4978\tLR: 0.010000\n",
      "1.3811\t1.3593\t1.3973\t1.5320\t1.7561\t2.0719\t\n",
      "Training Epoch: 3 [826/567]\tLoss: 22.7209\tLR: 0.010000\n",
      "4.6157\t4.2744\t3.8787\t3.5342\t3.2890\t3.1289\t\n",
      "Training Epoch: 3 [842/567]\tLoss: 33.6366\tLR: 0.010000\n",
      "8.2943\t7.2881\t5.9825\t4.8097\t3.9093\t3.3527\t\n",
      "Training Epoch: 3 [858/567]\tLoss: 18.1937\tLR: 0.010000\n",
      "3.3117\t3.0659\t2.8936\t2.8761\t2.9475\t3.0989\t\n",
      "Training Epoch: 3 [874/567]\tLoss: 13.1512\tLR: 0.010000\n",
      "2.1938\t2.0842\t2.0175\t2.0594\t2.2477\t2.5486\t\n",
      "Training Epoch: 3 [890/567]\tLoss: 14.3662\tLR: 0.010000\n",
      "2.3343\t2.2361\t2.2169\t2.2998\t2.5065\t2.7726\t\n",
      "Training Epoch: 3 [903/567]\tLoss: 7.7365\tLR: 0.010000\n",
      "1.3055\t1.1997\t1.1667\t1.2080\t1.3255\t1.5310\t\n",
      "[0.3540492653846741, 0.3119581639766693, 0.3339925706386566, 0.012290743179619312, 0.4334416687488556, 0.24609428644180298, 0.32046404480934143, 0.01293505635112524, 0.41287103295326233, 0.21583938598632812, 0.37128958106040955, 0.0021403527352958918, 0.3541439473628998, 0.2565535306930542, 0.389302521944046, 0.00478498637676239, 0.19831866025924683, 0.0, 0.8016813397407532, 0.005497207399457693]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3182, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2922, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2668, Accuracy: 0.5503\n",
      "Test set t = 03: Average loss: 0.2515, Accuracy: 0.5414\n",
      "Test set t = 04: Average loss: 0.2493, Accuracy: 0.5309\n",
      "Test set t = 05: Average loss: 0.2593, Accuracy: 0.5115\n",
      "\n",
      "Training Epoch: 4 [10/567]\tLoss: 20.5592\tLR: 0.010000\n",
      "4.3295\t3.9193\t3.4344\t3.0763\t2.9058\t2.8940\t\n",
      "Training Epoch: 4 [26/567]\tLoss: 7.9308\tLR: 0.010000\n",
      "1.2143\t1.1114\t1.0750\t1.1945\t1.4732\t1.8623\t\n",
      "Training Epoch: 4 [42/567]\tLoss: 11.6467\tLR: 0.010000\n",
      "1.6629\t1.7298\t1.8351\t1.9643\t2.1211\t2.3334\t\n",
      "Training Epoch: 4 [58/567]\tLoss: 22.1131\tLR: 0.010000\n",
      "4.5578\t4.0856\t3.6155\t3.3179\t3.2198\t3.3164\t\n",
      "Training Epoch: 4 [74/567]\tLoss: 22.0360\tLR: 0.010000\n",
      "4.6668\t4.2803\t3.7889\t3.3382\t3.0366\t2.9251\t\n",
      "Training Epoch: 4 [90/567]\tLoss: 21.9654\tLR: 0.010000\n",
      "4.5220\t4.1637\t3.7438\t3.3709\t3.1144\t3.0506\t\n",
      "Training Epoch: 4 [106/567]\tLoss: 6.3466\tLR: 0.010000\n",
      "0.8710\t0.8728\t0.8912\t1.0130\t1.2207\t1.4779\t\n",
      "Training Epoch: 4 [122/567]\tLoss: 18.4563\tLR: 0.010000\n",
      "3.4621\t3.2374\t3.0244\t2.8989\t2.8552\t2.9783\t\n",
      "Training Epoch: 4 [138/567]\tLoss: 15.7113\tLR: 0.010000\n",
      "3.2598\t2.8948\t2.5616\t2.3476\t2.2913\t2.3563\t\n",
      "Training Epoch: 4 [154/567]\tLoss: 6.2209\tLR: 0.010000\n",
      "1.0758\t1.0178\t0.9641\t0.9511\t1.0088\t1.2033\t\n",
      "Training Epoch: 4 [170/567]\tLoss: 17.8271\tLR: 0.010000\n",
      "3.8481\t3.3922\t2.9289\t2.6118\t2.4869\t2.5593\t\n",
      "Training Epoch: 4 [186/567]\tLoss: 8.6077\tLR: 0.010000\n",
      "1.4731\t1.3967\t1.3417\t1.3441\t1.4344\t1.6177\t\n",
      "Training Epoch: 4 [202/567]\tLoss: 27.1108\tLR: 0.010000\n",
      "5.7907\t5.1595\t4.5682\t4.1277\t3.8177\t3.6471\t\n",
      "Training Epoch: 4 [218/567]\tLoss: 18.3835\tLR: 0.010000\n",
      "3.3581\t3.1212\t2.9466\t2.9077\t2.9772\t3.0727\t\n",
      "Training Epoch: 4 [234/567]\tLoss: 17.3537\tLR: 0.010000\n",
      "3.1465\t2.9019\t2.7450\t2.7038\t2.8102\t3.0463\t\n",
      "Training Epoch: 4 [250/567]\tLoss: 13.1802\tLR: 0.010000\n",
      "2.5820\t2.3379\t2.1313\t2.0284\t2.0206\t2.0801\t\n",
      "Training Epoch: 4 [266/567]\tLoss: 26.5662\tLR: 0.010000\n",
      "5.8203\t5.1424\t4.4586\t3.9480\t3.6571\t3.5399\t\n",
      "Training Epoch: 4 [282/567]\tLoss: 8.7258\tLR: 0.010000\n",
      "1.4274\t1.4029\t1.3948\t1.4265\t1.4939\t1.5803\t\n",
      "Training Epoch: 4 [298/567]\tLoss: 8.8749\tLR: 0.010000\n",
      "1.4498\t1.3273\t1.2931\t1.3811\t1.5819\t1.8417\t\n",
      "Training Epoch: 4 [314/567]\tLoss: 14.5862\tLR: 0.010000\n",
      "2.9522\t2.6717\t2.3598\t2.1880\t2.1431\t2.2715\t\n",
      "Training Epoch: 4 [330/567]\tLoss: 26.0008\tLR: 0.010000\n",
      "5.2775\t4.8943\t4.4539\t4.0428\t3.7468\t3.5855\t\n",
      "Training Epoch: 4 [346/567]\tLoss: 14.8860\tLR: 0.010000\n",
      "2.8808\t2.6611\t2.4543\t2.2926\t2.2383\t2.3590\t\n",
      "Training Epoch: 4 [362/567]\tLoss: 20.5676\tLR: 0.010000\n",
      "4.3517\t3.9064\t3.4320\t3.0892\t2.9074\t2.8809\t\n",
      "Training Epoch: 4 [378/567]\tLoss: 14.5376\tLR: 0.010000\n",
      "2.4602\t2.3468\t2.2551\t2.2953\t2.4474\t2.7328\t\n",
      "Training Epoch: 4 [394/567]\tLoss: 13.3961\tLR: 0.010000\n",
      "2.6465\t2.4180\t2.2030\t2.0658\t2.0055\t2.0573\t\n",
      "Training Epoch: 4 [410/567]\tLoss: 15.6567\tLR: 0.010000\n",
      "2.9963\t2.7355\t2.5190\t2.4374\t2.4394\t2.5291\t\n",
      "Training Epoch: 4 [426/567]\tLoss: 8.6948\tLR: 0.010000\n",
      "1.5678\t1.4240\t1.3162\t1.3177\t1.4270\t1.6421\t\n",
      "Training Epoch: 4 [442/567]\tLoss: 8.5256\tLR: 0.010000\n",
      "1.5519\t1.3708\t1.2617\t1.2597\t1.4065\t1.6751\t\n",
      "Training Epoch: 4 [458/567]\tLoss: 14.1085\tLR: 0.010000\n",
      "2.4654\t2.3609\t2.2707\t2.2569\t2.3199\t2.4346\t\n",
      "Training Epoch: 4 [474/567]\tLoss: 8.1777\tLR: 0.010000\n",
      "1.4012\t1.3523\t1.2611\t1.2262\t1.3477\t1.5892\t\n",
      "Training Epoch: 4 [490/567]\tLoss: 5.8462\tLR: 0.010000\n",
      "0.8035\t0.7873\t0.8216\t0.9342\t1.1139\t1.3857\t\n",
      "Training Epoch: 4 [506/567]\tLoss: 25.4509\tLR: 0.010000\n",
      "5.4632\t4.9225\t4.3087\t3.7956\t3.5100\t3.4508\t\n",
      "Training Epoch: 4 [522/567]\tLoss: 15.3381\tLR: 0.010000\n",
      "2.9556\t2.7231\t2.4623\t2.3221\t2.3675\t2.5074\t\n",
      "Training Epoch: 4 [538/567]\tLoss: 36.6403\tLR: 0.010000\n",
      "8.1204\t7.2599\t6.2990\t5.5108\t4.9215\t4.5287\t\n",
      "Training Epoch: 4 [554/567]\tLoss: 17.7175\tLR: 0.010000\n",
      "3.3541\t3.0501\t2.8170\t2.7240\t2.7852\t2.9873\t\n",
      "Training Epoch: 4 [570/567]\tLoss: 9.8829\tLR: 0.010000\n",
      "2.0096\t1.7534\t1.5306\t1.4283\t1.4902\t1.6709\t\n",
      "Training Epoch: 4 [586/567]\tLoss: 19.4305\tLR: 0.010000\n",
      "3.4823\t3.3092\t3.1345\t3.0837\t3.1304\t3.2905\t\n",
      "Training Epoch: 4 [602/567]\tLoss: 12.8951\tLR: 0.010000\n",
      "2.4409\t2.2336\t2.0386\t1.9620\t2.0183\t2.2017\t\n",
      "Training Epoch: 4 [618/567]\tLoss: 29.0823\tLR: 0.010000\n",
      "5.9319\t5.4500\t4.9079\t4.4854\t4.2281\t4.0791\t\n",
      "Training Epoch: 4 [634/567]\tLoss: 7.8621\tLR: 0.010000\n",
      "1.2650\t1.2386\t1.2290\t1.2556\t1.3607\t1.5132\t\n",
      "Training Epoch: 4 [650/567]\tLoss: 18.1172\tLR: 0.010000\n",
      "3.7267\t3.3700\t2.9924\t2.7184\t2.6218\t2.6879\t\n",
      "Training Epoch: 4 [666/567]\tLoss: 13.7152\tLR: 0.010000\n",
      "2.6927\t2.4663\t2.2597\t2.1090\t2.0552\t2.1322\t\n",
      "Training Epoch: 4 [682/567]\tLoss: 9.8394\tLR: 0.010000\n",
      "1.5171\t1.4664\t1.5025\t1.6045\t1.7656\t1.9833\t\n",
      "Training Epoch: 4 [698/567]\tLoss: 16.8091\tLR: 0.010000\n",
      "2.9429\t2.8080\t2.6843\t2.6725\t2.7551\t2.9465\t\n",
      "Training Epoch: 4 [714/567]\tLoss: 9.2427\tLR: 0.010000\n",
      "1.5398\t1.4169\t1.3915\t1.4701\t1.6172\t1.8072\t\n",
      "Training Epoch: 4 [730/567]\tLoss: 14.8195\tLR: 0.010000\n",
      "2.5815\t2.5152\t2.4239\t2.3906\t2.4019\t2.5062\t\n",
      "Training Epoch: 4 [746/567]\tLoss: 18.6787\tLR: 0.010000\n",
      "3.9110\t3.4624\t3.0189\t2.7503\t2.7154\t2.8207\t\n",
      "Training Epoch: 4 [762/567]\tLoss: 8.9180\tLR: 0.010000\n",
      "1.2780\t1.2578\t1.2995\t1.4311\t1.6710\t1.9807\t\n",
      "Training Epoch: 4 [778/567]\tLoss: 30.6570\tLR: 0.010000\n",
      "6.8168\t6.0299\t5.1709\t4.5162\t4.1471\t3.9761\t\n",
      "Training Epoch: 4 [794/567]\tLoss: 15.4052\tLR: 0.010000\n",
      "2.2558\t2.2524\t2.3534\t2.5475\t2.8320\t3.1642\t\n",
      "Training Epoch: 4 [810/567]\tLoss: 7.0703\tLR: 0.010000\n",
      "0.9606\t0.9598\t1.0093\t1.1472\t1.3741\t1.6193\t\n",
      "Training Epoch: 4 [826/567]\tLoss: 18.2833\tLR: 0.010000\n",
      "3.8617\t3.4301\t3.0032\t2.7265\t2.6219\t2.6399\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [842/567]\tLoss: 16.7439\tLR: 0.010000\n",
      "2.8508\t2.7176\t2.6523\t2.6785\t2.8138\t3.0309\t\n",
      "Training Epoch: 4 [858/567]\tLoss: 32.5206\tLR: 0.010000\n",
      "7.1336\t6.4337\t5.6091\t4.8592\t4.3692\t4.1159\t\n",
      "Training Epoch: 4 [874/567]\tLoss: 22.5295\tLR: 0.010000\n",
      "4.5484\t4.2251\t3.8129\t3.4756\t3.2714\t3.1962\t\n",
      "Training Epoch: 4 [890/567]\tLoss: 13.8323\tLR: 0.010000\n",
      "2.5727\t2.3808\t2.2274\t2.1474\t2.1801\t2.3240\t\n",
      "Training Epoch: 4 [903/567]\tLoss: 26.0788\tLR: 0.010000\n",
      "5.0866\t4.7583\t4.4158\t4.1109\t3.8890\t3.8183\t\n",
      "[0.3494298756122589, 0.35379189252853394, 0.29677823185920715, 0.012126980349421501, 0.4683662950992584, 0.2489951103925705, 0.2826385945081711, 0.013437588699162006, 0.44842904806137085, 0.20646457374095917, 0.34510637819767, 0.0003646443656180054, 0.3593849241733551, 0.2684766352176666, 0.37213844060897827, 0.0035985689610242844, 0.18999452888965607, 0.0, 0.8100054711103439, 0.004441494587808847]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3182, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2919, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2666, Accuracy: 0.5520\n",
      "Test set t = 03: Average loss: 0.2514, Accuracy: 0.5503\n",
      "Test set t = 04: Average loss: 0.2488, Accuracy: 0.5291\n",
      "Test set t = 05: Average loss: 0.2580, Accuracy: 0.5132\n",
      "\n",
      "Training Epoch: 5 [10/567]\tLoss: 7.3367\tLR: 0.010000\n",
      "0.9562\t0.9636\t1.0457\t1.2007\t1.4385\t1.7320\t\n",
      "Training Epoch: 5 [26/567]\tLoss: 10.1283\tLR: 0.010000\n",
      "1.9884\t1.8136\t1.6140\t1.5089\t1.5274\t1.6760\t\n",
      "Training Epoch: 5 [42/567]\tLoss: 32.0595\tLR: 0.010000\n",
      "7.0255\t6.1625\t5.3661\t4.7858\t4.4343\t4.2854\t\n",
      "Training Epoch: 5 [58/567]\tLoss: 16.1342\tLR: 0.010000\n",
      "2.8393\t2.6862\t2.5576\t2.5550\t2.6551\t2.8410\t\n",
      "Training Epoch: 5 [74/567]\tLoss: 28.7060\tLR: 0.010000\n",
      "6.2917\t5.6185\t4.9064\t4.3165\t3.9090\t3.6639\t\n",
      "Training Epoch: 5 [90/567]\tLoss: 15.9264\tLR: 0.010000\n",
      "2.7981\t2.7191\t2.6175\t2.5615\t2.5707\t2.6594\t\n",
      "Training Epoch: 5 [106/567]\tLoss: 6.0232\tLR: 0.010000\n",
      "0.7940\t0.7521\t0.7968\t0.9395\t1.1951\t1.5457\t\n",
      "Training Epoch: 5 [122/567]\tLoss: 17.6196\tLR: 0.010000\n",
      "3.3815\t3.1237\t2.8848\t2.7227\t2.6922\t2.8147\t\n",
      "Training Epoch: 5 [138/567]\tLoss: 16.6916\tLR: 0.010000\n",
      "3.1698\t2.9443\t2.7323\t2.6224\t2.5836\t2.6393\t\n",
      "Training Epoch: 5 [154/567]\tLoss: 17.3827\tLR: 0.010000\n",
      "3.7366\t3.3421\t2.9307\t2.6280\t2.4225\t2.3228\t\n",
      "Training Epoch: 5 [170/567]\tLoss: 7.8690\tLR: 0.010000\n",
      "0.9676\t0.9928\t1.0929\t1.2906\t1.5732\t1.9519\t\n",
      "Training Epoch: 5 [186/567]\tLoss: 7.0915\tLR: 0.010000\n",
      "0.8293\t0.8419\t0.9301\t1.1311\t1.4680\t1.8912\t\n",
      "Training Epoch: 5 [202/567]\tLoss: 17.4896\tLR: 0.010000\n",
      "3.3342\t3.0737\t2.8318\t2.7138\t2.7094\t2.8267\t\n",
      "Training Epoch: 5 [218/567]\tLoss: 9.6449\tLR: 0.010000\n",
      "1.5701\t1.4971\t1.4767\t1.5283\t1.6626\t1.9099\t\n",
      "Training Epoch: 5 [234/567]\tLoss: 23.8154\tLR: 0.010000\n",
      "4.6904\t4.2951\t3.9221\t3.6810\t3.5882\t3.6385\t\n",
      "Training Epoch: 5 [250/567]\tLoss: 12.8988\tLR: 0.010000\n",
      "1.9541\t1.9747\t2.0695\t2.1814\t2.2941\t2.4248\t\n",
      "Training Epoch: 5 [266/567]\tLoss: 14.8137\tLR: 0.010000\n",
      "3.0205\t2.7455\t2.4281\t2.2105\t2.1511\t2.2581\t\n",
      "Training Epoch: 5 [282/567]\tLoss: 23.3374\tLR: 0.010000\n",
      "4.9246\t4.4812\t4.0049\t3.5949\t3.2569\t3.0748\t\n",
      "Training Epoch: 5 [298/567]\tLoss: 9.9577\tLR: 0.010000\n",
      "1.5243\t1.4859\t1.4862\t1.5702\t1.7826\t2.1085\t\n",
      "Training Epoch: 5 [314/567]\tLoss: 13.5581\tLR: 0.010000\n",
      "2.7590\t2.4936\t2.2588\t2.0561\t1.9637\t2.0269\t\n",
      "Training Epoch: 5 [330/567]\tLoss: 12.9731\tLR: 0.010000\n",
      "2.3205\t2.1969\t2.0442\t1.9974\t2.1059\t2.3083\t\n",
      "Training Epoch: 5 [346/567]\tLoss: 21.1809\tLR: 0.010000\n",
      "4.2401\t3.8729\t3.4873\t3.2349\t3.1572\t3.1884\t\n",
      "Training Epoch: 5 [362/567]\tLoss: 20.1014\tLR: 0.010000\n",
      "3.5568\t3.2909\t3.1530\t3.1799\t3.3511\t3.5698\t\n",
      "Training Epoch: 5 [378/567]\tLoss: 10.9808\tLR: 0.010000\n",
      "1.6965\t1.6382\t1.6521\t1.7769\t1.9798\t2.2372\t\n",
      "Training Epoch: 5 [394/567]\tLoss: 11.8159\tLR: 0.010000\n",
      "1.8897\t1.8077\t1.8001\t1.9070\t2.0889\t2.3225\t\n",
      "Training Epoch: 5 [410/567]\tLoss: 11.3422\tLR: 0.010000\n",
      "1.9166\t1.8575\t1.8011\t1.8100\t1.9014\t2.0555\t\n",
      "Training Epoch: 5 [426/567]\tLoss: 14.0949\tLR: 0.010000\n",
      "2.6740\t2.5078\t2.3212\t2.1900\t2.1681\t2.2338\t\n",
      "Training Epoch: 5 [442/567]\tLoss: 11.1614\tLR: 0.010000\n",
      "2.6086\t2.2502\t1.8933\t1.6041\t1.4273\t1.3778\t\n",
      "Training Epoch: 5 [458/567]\tLoss: 11.0428\tLR: 0.010000\n",
      "1.8474\t1.7584\t1.7052\t1.7371\t1.8783\t2.1163\t\n",
      "Training Epoch: 5 [474/567]\tLoss: 22.1803\tLR: 0.010000\n",
      "4.6991\t4.2621\t3.7525\t3.3586\t3.1110\t2.9970\t\n",
      "Training Epoch: 5 [490/567]\tLoss: 22.4830\tLR: 0.010000\n",
      "5.4348\t4.7512\t3.9139\t3.2027\t2.7374\t2.4430\t\n",
      "Training Epoch: 5 [506/567]\tLoss: 14.8769\tLR: 0.010000\n",
      "2.8597\t2.6167\t2.3718\t2.2469\t2.2997\t2.4821\t\n",
      "Training Epoch: 5 [522/567]\tLoss: 20.5781\tLR: 0.010000\n",
      "4.0726\t3.7358\t3.4197\t3.1851\t3.0744\t3.0905\t\n",
      "Training Epoch: 5 [538/567]\tLoss: 23.1903\tLR: 0.010000\n",
      "4.6059\t4.2065\t3.7869\t3.5472\t3.4938\t3.5500\t\n",
      "Training Epoch: 5 [554/567]\tLoss: 12.4336\tLR: 0.010000\n",
      "2.2890\t2.1418\t1.9821\t1.9202\t1.9734\t2.1270\t\n",
      "Training Epoch: 5 [570/567]\tLoss: 24.3564\tLR: 0.010000\n",
      "4.9101\t4.4638\t4.0332\t3.7296\t3.6142\t3.6054\t\n",
      "Training Epoch: 5 [586/567]\tLoss: 9.5175\tLR: 0.010000\n",
      "1.3267\t1.3861\t1.4671\t1.5817\t1.7594\t1.9965\t\n",
      "Training Epoch: 5 [602/567]\tLoss: 16.3339\tLR: 0.010000\n",
      "3.3756\t3.0871\t2.7505\t2.4800\t2.3343\t2.3064\t\n",
      "Training Epoch: 5 [618/567]\tLoss: 10.6239\tLR: 0.010000\n",
      "1.8563\t1.6592\t1.5882\t1.6463\t1.8041\t2.0698\t\n",
      "Training Epoch: 5 [634/567]\tLoss: 19.9211\tLR: 0.010000\n",
      "3.7718\t3.5147\t3.2889\t3.1393\t3.0915\t3.1148\t\n",
      "Training Epoch: 5 [650/567]\tLoss: 21.2272\tLR: 0.010000\n",
      "4.5400\t4.1027\t3.6184\t3.2208\t2.9396\t2.8057\t\n",
      "Training Epoch: 5 [666/567]\tLoss: 16.7379\tLR: 0.010000\n",
      "3.4519\t3.1751\t2.8339\t2.5461\t2.3804\t2.3505\t\n",
      "Training Epoch: 5 [682/567]\tLoss: 10.8431\tLR: 0.010000\n",
      "2.0669\t1.8848\t1.7035\t1.6006\t1.6740\t1.9133\t\n",
      "Training Epoch: 5 [698/567]\tLoss: 18.7101\tLR: 0.010000\n",
      "3.8514\t3.4632\t3.0666\t2.7835\t2.7097\t2.8357\t\n",
      "Training Epoch: 5 [714/567]\tLoss: 28.2538\tLR: 0.010000\n",
      "6.2315\t5.5936\t4.8323\t4.2097\t3.7878\t3.5990\t\n",
      "Training Epoch: 5 [730/567]\tLoss: 21.7553\tLR: 0.010000\n",
      "4.6534\t4.1281\t3.6396\t3.2705\t3.0558\t3.0079\t\n",
      "Training Epoch: 5 [746/567]\tLoss: 18.9785\tLR: 0.010000\n",
      "3.8089\t3.4286\t3.0720\t2.8860\t2.8592\t2.9236\t\n",
      "Training Epoch: 5 [762/567]\tLoss: 21.9923\tLR: 0.010000\n",
      "4.6736\t4.1273\t3.6530\t3.3293\t3.1361\t3.0730\t\n",
      "Training Epoch: 5 [778/567]\tLoss: 11.2975\tLR: 0.010000\n",
      "1.6743\t1.6568\t1.7126\t1.8360\t2.0526\t2.3652\t\n",
      "Training Epoch: 5 [794/567]\tLoss: 24.2952\tLR: 0.010000\n",
      "4.9317\t4.5079\t4.1085\t3.7863\t3.5325\t3.4283\t\n",
      "Training Epoch: 5 [810/567]\tLoss: 14.4400\tLR: 0.010000\n",
      "2.7494\t2.5193\t2.3357\t2.2441\t2.2589\t2.3327\t\n",
      "Training Epoch: 5 [826/567]\tLoss: 5.9360\tLR: 0.010000\n",
      "0.7874\t0.7698\t0.8255\t0.9494\t1.1571\t1.4468\t\n",
      "Training Epoch: 5 [842/567]\tLoss: 22.3998\tLR: 0.010000\n",
      "4.9574\t4.4031\t3.8104\t3.3243\t3.0132\t2.8914\t\n",
      "Training Epoch: 5 [858/567]\tLoss: 14.5594\tLR: 0.010000\n",
      "2.6602\t2.5158\t2.3614\t2.2931\t2.3171\t2.4118\t\n",
      "Training Epoch: 5 [874/567]\tLoss: 22.3986\tLR: 0.010000\n",
      "4.9533\t4.3838\t3.7851\t3.3145\t3.0303\t2.9315\t\n",
      "Training Epoch: 5 [890/567]\tLoss: 12.2701\tLR: 0.010000\n",
      "2.0873\t1.9713\t1.8855\t1.9096\t2.0662\t2.3502\t\n",
      "Training Epoch: 5 [903/567]\tLoss: 9.4492\tLR: 0.010000\n",
      "1.5157\t1.4561\t1.4406\t1.5042\t1.6485\t1.8840\t\n",
      "[0.34004104137420654, 0.40000036358833313, 0.2599585950374603, 0.011110651306807995, 0.498360812664032, 0.2552473247051239, 0.24639186263084412, 0.013679706491529942, 0.4847489297389984, 0.2012850046157837, 0.3139660656452179, -0.0014380926731973886, 0.3703177869319916, 0.2818259000778198, 0.3478563129901886, 0.002533507067710161, 0.18981680274009705, 0.0, 0.810183197259903, 0.0032877582125365734]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3189, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2907, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2649, Accuracy: 0.5538\n",
      "Test set t = 03: Average loss: 0.2503, Accuracy: 0.5485\n",
      "Test set t = 04: Average loss: 0.2492, Accuracy: 0.5238\n",
      "Test set t = 05: Average loss: 0.2603, Accuracy: 0.5097\n",
      "\n",
      "Training Epoch: 6 [10/567]\tLoss: 7.7332\tLR: 0.010000\n",
      "1.0903\t1.0393\t1.0895\t1.2327\t1.4749\t1.8065\t\n",
      "Training Epoch: 6 [26/567]\tLoss: 22.1596\tLR: 0.010000\n",
      "4.9892\t4.3890\t3.7732\t3.2679\t2.9303\t2.8099\t\n",
      "Training Epoch: 6 [42/567]\tLoss: 25.9025\tLR: 0.010000\n",
      "5.8748\t5.1035\t4.3617\t3.8114\t3.4537\t3.2974\t\n",
      "Training Epoch: 6 [58/567]\tLoss: 26.0694\tLR: 0.010000\n",
      "5.6020\t4.9584\t4.3119\t3.8855\t3.6703\t3.6413\t\n",
      "Training Epoch: 6 [74/567]\tLoss: 9.6838\tLR: 0.010000\n",
      "1.3676\t1.3721\t1.4460\t1.6080\t1.8157\t2.0745\t\n",
      "Training Epoch: 6 [90/567]\tLoss: 21.6050\tLR: 0.010000\n",
      "4.6581\t4.0283\t3.5055\t3.1815\t3.0826\t3.1489\t\n",
      "Training Epoch: 6 [106/567]\tLoss: 11.6742\tLR: 0.010000\n",
      "2.0999\t1.9849\t1.8423\t1.7974\t1.8859\t2.0638\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [122/567]\tLoss: 23.6785\tLR: 0.010000\n",
      "4.9776\t4.5090\t3.9778\t3.5836\t3.3487\t3.2817\t\n",
      "Training Epoch: 6 [138/567]\tLoss: 16.3311\tLR: 0.010000\n",
      "2.9148\t2.6455\t2.5033\t2.5681\t2.7372\t2.9622\t\n",
      "Training Epoch: 6 [154/567]\tLoss: 9.1145\tLR: 0.010000\n",
      "1.4940\t1.3850\t1.3498\t1.4223\t1.5945\t1.8690\t\n",
      "Training Epoch: 6 [170/567]\tLoss: 8.0244\tLR: 0.010000\n",
      "1.3692\t1.2730\t1.2000\t1.2153\t1.3611\t1.6058\t\n",
      "Training Epoch: 6 [186/567]\tLoss: 33.0393\tLR: 0.010000\n",
      "7.3950\t6.5384\t5.6091\t4.8839\t4.4348\t4.1781\t\n",
      "Training Epoch: 6 [202/567]\tLoss: 20.2575\tLR: 0.010000\n",
      "3.9290\t3.5632\t3.3498\t3.1857\t3.1112\t3.1187\t\n",
      "Training Epoch: 6 [218/567]\tLoss: 22.1575\tLR: 0.010000\n",
      "4.3969\t4.0397\t3.6990\t3.4367\t3.2844\t3.3009\t\n",
      "Training Epoch: 6 [234/567]\tLoss: 8.9955\tLR: 0.010000\n",
      "1.7890\t1.5486\t1.3726\t1.3269\t1.3957\t1.5626\t\n",
      "Training Epoch: 6 [250/567]\tLoss: 8.7914\tLR: 0.010000\n",
      "1.2134\t1.1767\t1.2659\t1.4391\t1.6986\t1.9976\t\n",
      "Training Epoch: 6 [266/567]\tLoss: 10.2953\tLR: 0.010000\n",
      "1.5708\t1.5557\t1.6175\t1.7217\t1.8385\t1.9911\t\n",
      "Training Epoch: 6 [282/567]\tLoss: 18.1198\tLR: 0.010000\n",
      "3.2497\t3.0614\t2.9055\t2.8666\t2.9387\t3.0979\t\n",
      "Training Epoch: 6 [298/567]\tLoss: 13.8016\tLR: 0.010000\n",
      "2.9271\t2.5942\t2.2463\t2.0331\t1.9546\t2.0463\t\n",
      "Training Epoch: 6 [314/567]\tLoss: 17.5083\tLR: 0.010000\n",
      "3.6667\t3.2977\t2.9182\t2.6564\t2.5172\t2.4521\t\n",
      "Training Epoch: 6 [330/567]\tLoss: 39.8998\tLR: 0.010000\n",
      "9.0435\t7.9505\t6.7905\t5.8610\t5.2683\t4.9861\t\n",
      "Training Epoch: 6 [346/567]\tLoss: 18.7951\tLR: 0.010000\n",
      "3.8898\t3.4544\t3.0230\t2.7815\t2.7640\t2.8824\t\n",
      "Training Epoch: 6 [362/567]\tLoss: 17.9068\tLR: 0.010000\n",
      "3.2958\t3.0425\t2.8206\t2.7701\t2.8898\t3.0880\t\n",
      "Training Epoch: 6 [378/567]\tLoss: 31.2037\tLR: 0.010000\n",
      "6.7216\t6.0066\t5.2703\t4.7040\t4.3397\t4.1616\t\n",
      "Training Epoch: 6 [394/567]\tLoss: 20.4083\tLR: 0.010000\n",
      "3.4774\t3.3567\t3.2758\t3.2949\t3.4035\t3.6000\t\n",
      "Training Epoch: 6 [410/567]\tLoss: 7.3237\tLR: 0.010000\n",
      "0.9160\t0.9291\t0.9998\t1.1677\t1.4482\t1.8628\t\n",
      "Training Epoch: 6 [426/567]\tLoss: 9.0505\tLR: 0.010000\n",
      "1.9100\t1.6779\t1.4614\t1.3266\t1.2974\t1.3773\t\n",
      "Training Epoch: 6 [442/567]\tLoss: 15.9208\tLR: 0.010000\n",
      "2.8060\t2.6450\t2.5173\t2.4711\t2.6012\t2.8802\t\n",
      "Training Epoch: 6 [458/567]\tLoss: 23.4765\tLR: 0.010000\n",
      "6.0833\t5.2447\t4.2100\t3.2435\t2.5379\t2.1570\t\n",
      "Training Epoch: 6 [474/567]\tLoss: 23.5413\tLR: 0.010000\n",
      "4.7569\t4.2997\t3.8892\t3.6108\t3.4835\t3.5012\t\n",
      "Training Epoch: 6 [490/567]\tLoss: 23.3934\tLR: 0.010000\n",
      "4.6938\t4.2106\t3.7984\t3.5568\t3.5257\t3.6081\t\n",
      "Training Epoch: 6 [506/567]\tLoss: 16.4899\tLR: 0.010000\n",
      "2.6330\t2.5892\t2.5795\t2.6778\t2.8746\t3.1359\t\n",
      "Training Epoch: 6 [522/567]\tLoss: 22.2124\tLR: 0.010000\n",
      "4.9116\t4.2004\t3.6184\t3.2509\t3.1152\t3.1158\t\n",
      "Training Epoch: 6 [538/567]\tLoss: 17.1538\tLR: 0.010000\n",
      "3.2268\t2.9817\t2.7165\t2.6022\t2.6961\t2.9305\t\n",
      "Training Epoch: 6 [554/567]\tLoss: 11.8159\tLR: 0.010000\n",
      "2.0302\t1.8974\t1.8055\t1.8400\t1.9796\t2.2633\t\n",
      "Training Epoch: 6 [570/567]\tLoss: 12.8731\tLR: 0.010000\n",
      "2.4153\t2.2277\t2.0396\t1.9793\t2.0198\t2.1914\t\n",
      "Training Epoch: 6 [586/567]\tLoss: 17.1118\tLR: 0.010000\n",
      "3.0823\t2.9081\t2.7635\t2.6985\t2.7439\t2.9155\t\n",
      "Training Epoch: 6 [602/567]\tLoss: 15.8417\tLR: 0.010000\n",
      "2.6403\t2.5082\t2.4464\t2.5205\t2.7141\t3.0121\t\n",
      "Training Epoch: 6 [618/567]\tLoss: 4.4681\tLR: 0.010000\n",
      "0.2704\t0.3150\t0.4559\t0.7286\t1.1179\t1.5802\t\n",
      "Training Epoch: 6 [634/567]\tLoss: 20.8030\tLR: 0.010000\n",
      "4.5936\t3.9833\t3.4460\t3.0520\t2.8557\t2.8725\t\n",
      "Training Epoch: 6 [650/567]\tLoss: 8.4557\tLR: 0.010000\n",
      "1.5025\t1.4131\t1.3113\t1.2668\t1.3632\t1.5988\t\n",
      "Training Epoch: 6 [666/567]\tLoss: 18.1232\tLR: 0.010000\n",
      "3.4544\t3.1640\t2.9622\t2.8486\t2.8119\t2.8821\t\n",
      "Training Epoch: 6 [682/567]\tLoss: 11.3558\tLR: 0.010000\n",
      "2.1778\t1.9258\t1.7747\t1.7248\t1.7962\t1.9564\t\n",
      "Training Epoch: 6 [698/567]\tLoss: 2.6212\tLR: 0.010000\n",
      "0.1528\t0.1901\t0.2729\t0.4215\t0.6454\t0.9386\t\n",
      "Training Epoch: 6 [714/567]\tLoss: 17.2548\tLR: 0.010000\n",
      "3.0822\t2.9049\t2.7571\t2.7216\t2.8022\t2.9867\t\n",
      "Training Epoch: 6 [730/567]\tLoss: 9.6897\tLR: 0.010000\n",
      "1.3647\t1.3365\t1.3613\t1.5251\t1.8505\t2.2518\t\n",
      "Training Epoch: 6 [746/567]\tLoss: 9.5523\tLR: 0.010000\n",
      "1.4666\t1.4475\t1.4451\t1.5289\t1.7170\t1.9473\t\n",
      "Training Epoch: 6 [762/567]\tLoss: 13.9846\tLR: 0.010000\n",
      "2.4275\t2.2317\t2.1288\t2.1593\t2.3626\t2.6749\t\n",
      "Training Epoch: 6 [778/567]\tLoss: 7.8674\tLR: 0.010000\n",
      "0.9350\t1.0022\t1.1208\t1.3002\t1.5679\t1.9413\t\n",
      "Training Epoch: 6 [794/567]\tLoss: 13.4628\tLR: 0.010000\n",
      "2.5249\t2.3101\t2.1136\t2.0309\t2.1225\t2.3609\t\n",
      "Training Epoch: 6 [810/567]\tLoss: 22.9930\tLR: 0.010000\n",
      "4.8131\t4.3275\t3.8311\t3.4538\t3.2708\t3.2967\t\n",
      "Training Epoch: 6 [826/567]\tLoss: 9.9150\tLR: 0.010000\n",
      "1.6425\t1.5835\t1.5478\t1.5756\t1.6875\t1.8782\t\n",
      "Training Epoch: 6 [842/567]\tLoss: 7.9674\tLR: 0.010000\n",
      "1.1178\t1.0999\t1.1276\t1.2697\t1.5182\t1.8343\t\n",
      "Training Epoch: 6 [858/567]\tLoss: 27.4117\tLR: 0.010000\n",
      "5.9638\t5.2819\t4.6117\t4.1066\t3.7849\t3.6629\t\n",
      "Training Epoch: 6 [874/567]\tLoss: 15.0633\tLR: 0.010000\n",
      "2.9050\t2.6665\t2.4250\t2.2950\t2.3119\t2.4600\t\n",
      "Training Epoch: 6 [890/567]\tLoss: 6.2355\tLR: 0.010000\n",
      "0.7814\t0.8079\t0.8788\t1.0228\t1.2561\t1.4885\t\n",
      "Training Epoch: 6 [903/567]\tLoss: 20.6126\tLR: 0.010000\n",
      "4.8025\t4.2035\t3.5578\t3.0018\t2.6348\t2.4122\t\n",
      "[0.3414131700992584, 0.42482322454452515, 0.23376360535621643, 0.010225319303572178, 0.5400201678276062, 0.2434534728527069, 0.2165263593196869, 0.01388391200453043, 0.5245115756988525, 0.1869458258152008, 0.28854259848594666, -0.0033301671501249075, 0.3776099383831024, 0.29296040534973145, 0.32942965626716614, 0.0013931989669799805, 0.1792534589767456, 0.0, 0.8207465410232544, 0.0023196113761514425]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3177, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2900, Accuracy: 0.5573\n",
      "Test set t = 02: Average loss: 0.2648, Accuracy: 0.5556\n",
      "Test set t = 03: Average loss: 0.2500, Accuracy: 0.5485\n",
      "Test set t = 04: Average loss: 0.2477, Accuracy: 0.5291\n",
      "Test set t = 05: Average loss: 0.2564, Accuracy: 0.5150\n",
      "\n",
      "Training Epoch: 7 [10/567]\tLoss: 9.6787\tLR: 0.010000\n",
      "1.4396\t1.3659\t1.3545\t1.5045\t1.8128\t2.2014\t\n",
      "Training Epoch: 7 [26/567]\tLoss: 23.1400\tLR: 0.010000\n",
      "4.7407\t4.2114\t3.7370\t3.4718\t3.4508\t3.5283\t\n",
      "Training Epoch: 7 [42/567]\tLoss: 11.3591\tLR: 0.010000\n",
      "1.7923\t1.7599\t1.7637\t1.8382\t1.9888\t2.2160\t\n",
      "Training Epoch: 7 [58/567]\tLoss: 14.1913\tLR: 0.010000\n",
      "2.5873\t2.4353\t2.2944\t2.2252\t2.2471\t2.4020\t\n",
      "Training Epoch: 7 [74/567]\tLoss: 19.4002\tLR: 0.010000\n",
      "3.8760\t3.4825\t3.1588\t2.9800\t2.9322\t2.9708\t\n",
      "Training Epoch: 7 [90/567]\tLoss: 15.5971\tLR: 0.010000\n",
      "2.3945\t2.3421\t2.4071\t2.5693\t2.8065\t3.0776\t\n",
      "Training Epoch: 7 [106/567]\tLoss: 7.2932\tLR: 0.010000\n",
      "1.2395\t1.1420\t1.0927\t1.1443\t1.2625\t1.4122\t\n",
      "Training Epoch: 7 [122/567]\tLoss: 14.3856\tLR: 0.010000\n",
      "2.3808\t2.3366\t2.3152\t2.3368\t2.4361\t2.5802\t\n",
      "Training Epoch: 7 [138/567]\tLoss: 17.6412\tLR: 0.010000\n",
      "3.2547\t3.0319\t2.8489\t2.7784\t2.8040\t2.9235\t\n",
      "Training Epoch: 7 [154/567]\tLoss: 4.8877\tLR: 0.010000\n",
      "0.8877\t0.8129\t0.7368\t0.7152\t0.7839\t0.9512\t\n",
      "Training Epoch: 7 [170/567]\tLoss: 28.4503\tLR: 0.010000\n",
      "6.3001\t5.6491\t4.9408\t4.3022\t3.7823\t3.4759\t\n",
      "Training Epoch: 7 [186/567]\tLoss: 17.3120\tLR: 0.010000\n",
      "3.3218\t3.1070\t2.8773\t2.6886\t2.6262\t2.6911\t\n",
      "Training Epoch: 7 [202/567]\tLoss: 15.4012\tLR: 0.010000\n",
      "2.7929\t2.6083\t2.4702\t2.4420\t2.4872\t2.6006\t\n",
      "Training Epoch: 7 [218/567]\tLoss: 18.7722\tLR: 0.010000\n",
      "3.7068\t3.4036\t3.1235\t2.9270\t2.8169\t2.7945\t\n",
      "Training Epoch: 7 [234/567]\tLoss: 23.5409\tLR: 0.010000\n",
      "4.8677\t4.4199\t3.9858\t3.6397\t3.3748\t3.2530\t\n",
      "Training Epoch: 7 [250/567]\tLoss: 26.7993\tLR: 0.010000\n",
      "5.7748\t5.1966\t4.5443\t4.0184\t3.6998\t3.5654\t\n",
      "Training Epoch: 7 [266/567]\tLoss: 7.6780\tLR: 0.010000\n",
      "0.9221\t0.9953\t1.1258\t1.3119\t1.5365\t1.7864\t\n",
      "Training Epoch: 7 [282/567]\tLoss: 20.6041\tLR: 0.010000\n",
      "4.5123\t3.9614\t3.4354\t3.0518\t2.8431\t2.8001\t\n",
      "Training Epoch: 7 [298/567]\tLoss: 11.6733\tLR: 0.010000\n",
      "1.7835\t1.7600\t1.8040\t1.9180\t2.0869\t2.3209\t\n",
      "Training Epoch: 7 [314/567]\tLoss: 10.6650\tLR: 0.010000\n",
      "2.0193\t1.8039\t1.6509\t1.6041\t1.7047\t1.8820\t\n",
      "Training Epoch: 7 [330/567]\tLoss: 21.5720\tLR: 0.010000\n",
      "4.2752\t3.9613\t3.6328\t3.3540\t3.2053\t3.1436\t\n",
      "Training Epoch: 7 [346/567]\tLoss: 23.1199\tLR: 0.010000\n",
      "4.6639\t4.2201\t3.8302\t3.5609\t3.4173\t3.4276\t\n",
      "Training Epoch: 7 [362/567]\tLoss: 18.8224\tLR: 0.010000\n",
      "3.7105\t3.4107\t3.1159\t2.9206\t2.8258\t2.8389\t\n",
      "Training Epoch: 7 [378/567]\tLoss: 16.0629\tLR: 0.010000\n",
      "3.2523\t2.9756\t2.6930\t2.4698\t2.3436\t2.3285\t\n",
      "Training Epoch: 7 [394/567]\tLoss: 14.4648\tLR: 0.010000\n",
      "2.6430\t2.4218\t2.2647\t2.2545\t2.3574\t2.5234\t\n",
      "Training Epoch: 7 [410/567]\tLoss: 12.6599\tLR: 0.010000\n",
      "2.1129\t2.0400\t1.9860\t2.0183\t2.1523\t2.3505\t\n",
      "Training Epoch: 7 [426/567]\tLoss: 12.1070\tLR: 0.010000\n",
      "1.7959\t1.7627\t1.8219\t1.9684\t2.2226\t2.5355\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [442/567]\tLoss: 21.0196\tLR: 0.010000\n",
      "4.3577\t4.0045\t3.5989\t3.2564\t2.9998\t2.8023\t\n",
      "Training Epoch: 7 [458/567]\tLoss: 32.8525\tLR: 0.010000\n",
      "7.7095\t6.7181\t5.7205\t4.8455\t4.1599\t3.6989\t\n",
      "Training Epoch: 7 [474/567]\tLoss: 22.9620\tLR: 0.010000\n",
      "4.5197\t4.1058\t3.7347\t3.5214\t3.4936\t3.5867\t\n",
      "Training Epoch: 7 [490/567]\tLoss: 11.0449\tLR: 0.010000\n",
      "1.6396\t1.6367\t1.6863\t1.8185\t2.0115\t2.2522\t\n",
      "Training Epoch: 7 [506/567]\tLoss: 11.5072\tLR: 0.010000\n",
      "1.9837\t1.8914\t1.8457\t1.8456\t1.9107\t2.0301\t\n",
      "Training Epoch: 7 [522/567]\tLoss: 16.1328\tLR: 0.010000\n",
      "3.3116\t3.0226\t2.6817\t2.4367\t2.3327\t2.3475\t\n",
      "Training Epoch: 7 [538/567]\tLoss: 14.0621\tLR: 0.010000\n",
      "2.1862\t2.1571\t2.1876\t2.3002\t2.4887\t2.7424\t\n",
      "Training Epoch: 7 [554/567]\tLoss: 31.4773\tLR: 0.010000\n",
      "6.8432\t6.0740\t5.3234\t4.7362\t4.3617\t4.1387\t\n",
      "Training Epoch: 7 [570/567]\tLoss: 24.7553\tLR: 0.010000\n",
      "5.5042\t4.8624\t4.2222\t3.6887\t3.3327\t3.1452\t\n",
      "Training Epoch: 7 [586/567]\tLoss: 10.4458\tLR: 0.010000\n",
      "1.8275\t1.7243\t1.6414\t1.6434\t1.7312\t1.8781\t\n",
      "Training Epoch: 7 [602/567]\tLoss: 32.0946\tLR: 0.010000\n",
      "7.5420\t6.6348\t5.5771\t4.6507\t4.0082\t3.6819\t\n",
      "Training Epoch: 7 [618/567]\tLoss: 5.5485\tLR: 0.010000\n",
      "0.7714\t0.7681\t0.7988\t0.8915\t1.0409\t1.2777\t\n",
      "Training Epoch: 7 [634/567]\tLoss: 19.5341\tLR: 0.010000\n",
      "3.7392\t3.4210\t3.1719\t3.0591\t3.0373\t3.1056\t\n",
      "Training Epoch: 7 [650/567]\tLoss: 15.5768\tLR: 0.010000\n",
      "3.1202\t2.8804\t2.6061\t2.3879\t2.2925\t2.2896\t\n",
      "Training Epoch: 7 [666/567]\tLoss: 9.3285\tLR: 0.010000\n",
      "1.6161\t1.4555\t1.3556\t1.4006\t1.5984\t1.9023\t\n",
      "Training Epoch: 7 [682/567]\tLoss: 17.9683\tLR: 0.010000\n",
      "4.0110\t3.4948\t2.9930\t2.6317\t2.4352\t2.4025\t\n",
      "Training Epoch: 7 [698/567]\tLoss: 11.7593\tLR: 0.010000\n",
      "2.0741\t1.9213\t1.8319\t1.8483\t1.9575\t2.1262\t\n",
      "Training Epoch: 7 [714/567]\tLoss: 10.7667\tLR: 0.010000\n",
      "2.1433\t1.9289\t1.7306\t1.6014\t1.6201\t1.7425\t\n",
      "Training Epoch: 7 [730/567]\tLoss: 19.4854\tLR: 0.010000\n",
      "4.2450\t3.7341\t3.2529\t2.8831\t2.6837\t2.6866\t\n",
      "Training Epoch: 7 [746/567]\tLoss: 12.9596\tLR: 0.010000\n",
      "2.3453\t2.2082\t2.0667\t2.0214\t2.0813\t2.2367\t\n",
      "Training Epoch: 7 [762/567]\tLoss: 18.6695\tLR: 0.010000\n",
      "4.0586\t3.5679\t3.1421\t2.8112\t2.5903\t2.4995\t\n",
      "Training Epoch: 7 [778/567]\tLoss: 15.7160\tLR: 0.010000\n",
      "3.5055\t3.0553\t2.5428\t2.2017\t2.1181\t2.2927\t\n",
      "Training Epoch: 7 [794/567]\tLoss: 11.1196\tLR: 0.010000\n",
      "2.2181\t1.9488\t1.7377\t1.6650\t1.7105\t1.8395\t\n",
      "Training Epoch: 7 [810/567]\tLoss: 14.0766\tLR: 0.010000\n",
      "2.7957\t2.4623\t2.1875\t2.1074\t2.1749\t2.3488\t\n",
      "Training Epoch: 7 [826/567]\tLoss: 12.0930\tLR: 0.010000\n",
      "1.8587\t1.8568\t1.8886\t1.9869\t2.1536\t2.3484\t\n",
      "Training Epoch: 7 [842/567]\tLoss: 18.4104\tLR: 0.010000\n",
      "3.3859\t3.1422\t2.9603\t2.9135\t2.9469\t3.0617\t\n",
      "Training Epoch: 7 [858/567]\tLoss: 5.0018\tLR: 0.010000\n",
      "0.5760\t0.6197\t0.7068\t0.8307\t1.0056\t1.2631\t\n",
      "Training Epoch: 7 [874/567]\tLoss: 19.2226\tLR: 0.010000\n",
      "4.1197\t3.6159\t3.1447\t2.8620\t2.7357\t2.7446\t\n",
      "Training Epoch: 7 [890/567]\tLoss: 5.9559\tLR: 0.010000\n",
      "0.5829\t0.6462\t0.7806\t0.9912\t1.2833\t1.6717\t\n",
      "Training Epoch: 7 [903/567]\tLoss: 15.0943\tLR: 0.010000\n",
      "2.8674\t2.5727\t2.3500\t2.2760\t2.4046\t2.6236\t\n",
      "[0.32701894640922546, 0.4654202163219452, 0.20756083726882935, 0.008713536895811558, 0.561445951461792, 0.24348366260528564, 0.19507038593292236, 0.014145825058221817, 0.5563719272613525, 0.18053734302520752, 0.26309072971343994, -0.00473801651969552, 0.37995484471321106, 0.3138786852359772, 0.30616647005081177, 0.0007153686601668596, 0.18183200061321259, 0.0, 0.8181679993867874, 0.0012204819358885288]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3183, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2886, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2625, Accuracy: 0.5538\n",
      "Test set t = 03: Average loss: 0.2484, Accuracy: 0.5467\n",
      "Test set t = 04: Average loss: 0.2479, Accuracy: 0.5256\n",
      "Test set t = 05: Average loss: 0.2591, Accuracy: 0.5097\n",
      "\n",
      "Training Epoch: 8 [10/567]\tLoss: 11.3295\tLR: 0.010000\n",
      "1.9702\t1.8211\t1.7420\t1.7752\t1.9206\t2.1005\t\n",
      "Training Epoch: 8 [26/567]\tLoss: 3.8716\tLR: 0.010000\n",
      "0.5236\t0.5108\t0.5230\t0.5896\t0.7376\t0.9870\t\n",
      "Training Epoch: 8 [42/567]\tLoss: 18.1950\tLR: 0.010000\n",
      "3.6204\t3.2177\t2.8829\t2.7392\t2.7906\t2.9443\t\n",
      "Training Epoch: 8 [58/567]\tLoss: 14.4590\tLR: 0.010000\n",
      "2.8538\t2.5317\t2.2805\t2.1758\t2.2175\t2.3997\t\n",
      "Training Epoch: 8 [74/567]\tLoss: 12.6642\tLR: 0.010000\n",
      "2.3185\t2.1554\t2.0034\t1.9544\t2.0326\t2.1999\t\n",
      "Training Epoch: 8 [90/567]\tLoss: 10.9964\tLR: 0.010000\n",
      "2.1762\t1.9571\t1.7764\t1.6605\t1.6612\t1.7650\t\n",
      "Training Epoch: 8 [106/567]\tLoss: 35.7055\tLR: 0.010000\n",
      "7.9560\t7.0272\t6.0659\t5.3242\t4.8173\t4.5149\t\n",
      "Training Epoch: 8 [122/567]\tLoss: 11.0706\tLR: 0.010000\n",
      "1.9613\t1.7971\t1.6850\t1.7010\t1.8357\t2.0906\t\n",
      "Training Epoch: 8 [138/567]\tLoss: 15.9480\tLR: 0.010000\n",
      "2.6472\t2.5740\t2.5213\t2.5710\t2.7161\t2.9184\t\n",
      "Training Epoch: 8 [154/567]\tLoss: 7.1175\tLR: 0.010000\n",
      "1.2159\t1.1079\t1.0593\t1.0758\t1.2081\t1.4505\t\n",
      "Training Epoch: 8 [170/567]\tLoss: 11.1300\tLR: 0.010000\n",
      "1.7724\t1.7039\t1.7055\t1.7950\t1.9580\t2.1952\t\n",
      "Training Epoch: 8 [186/567]\tLoss: 26.7670\tLR: 0.010000\n",
      "5.5470\t5.0209\t4.5139\t4.1081\t3.8459\t3.7312\t\n",
      "Training Epoch: 8 [202/567]\tLoss: 8.2666\tLR: 0.010000\n",
      "1.0793\t1.0951\t1.1889\t1.3725\t1.6304\t1.9004\t\n",
      "Training Epoch: 8 [218/567]\tLoss: 20.8502\tLR: 0.010000\n",
      "4.0290\t3.6790\t3.3726\t3.2439\t3.2207\t3.3050\t\n",
      "Training Epoch: 8 [234/567]\tLoss: 17.0317\tLR: 0.010000\n",
      "2.8269\t2.7094\t2.6844\t2.7519\t2.9147\t3.1443\t\n",
      "Training Epoch: 8 [250/567]\tLoss: 25.5062\tLR: 0.010000\n",
      "5.4467\t4.8400\t4.2264\t3.8161\t3.6110\t3.5661\t\n",
      "Training Epoch: 8 [266/567]\tLoss: 19.2372\tLR: 0.010000\n",
      "3.6535\t3.2962\t3.0576\t2.9679\t3.0362\t3.2259\t\n",
      "Training Epoch: 8 [282/567]\tLoss: 27.0034\tLR: 0.010000\n",
      "6.1133\t5.3352\t4.5715\t3.9582\t3.5973\t3.4280\t\n",
      "Training Epoch: 8 [298/567]\tLoss: 37.6169\tLR: 0.010000\n",
      "8.8666\t7.6926\t6.4937\t5.4836\t4.7592\t4.3212\t\n",
      "Training Epoch: 8 [314/567]\tLoss: 9.5111\tLR: 0.010000\n",
      "1.5024\t1.4435\t1.4338\t1.5258\t1.6884\t1.9172\t\n",
      "Training Epoch: 8 [330/567]\tLoss: 26.6455\tLR: 0.010000\n",
      "5.8036\t5.1929\t4.5456\t4.0220\t3.6582\t3.4232\t\n",
      "Training Epoch: 8 [346/567]\tLoss: 17.8878\tLR: 0.010000\n",
      "3.3668\t3.0995\t2.8964\t2.8177\t2.8195\t2.8879\t\n",
      "Training Epoch: 8 [362/567]\tLoss: 12.1549\tLR: 0.010000\n",
      "1.9466\t1.8908\t1.8695\t1.9400\t2.1213\t2.3867\t\n",
      "Training Epoch: 8 [378/567]\tLoss: 13.0268\tLR: 0.010000\n",
      "2.3562\t2.1996\t2.0707\t2.0381\t2.1060\t2.2562\t\n",
      "Training Epoch: 8 [394/567]\tLoss: 8.8382\tLR: 0.010000\n",
      "1.6280\t1.4740\t1.3568\t1.3204\t1.4153\t1.6439\t\n",
      "Training Epoch: 8 [410/567]\tLoss: 26.0058\tLR: 0.010000\n",
      "5.1385\t4.7383\t4.3554\t4.0815\t3.8788\t3.8133\t\n",
      "Training Epoch: 8 [426/567]\tLoss: 13.6664\tLR: 0.010000\n",
      "2.3802\t2.2103\t2.1235\t2.1556\t2.3035\t2.4934\t\n",
      "Training Epoch: 8 [442/567]\tLoss: 14.4676\tLR: 0.010000\n",
      "3.0591\t2.7046\t2.3391\t2.1279\t2.0697\t2.1671\t\n",
      "Training Epoch: 8 [458/567]\tLoss: 17.6357\tLR: 0.010000\n",
      "3.6102\t3.2292\t2.8934\t2.6904\t2.6004\t2.6121\t\n",
      "Training Epoch: 8 [474/567]\tLoss: 22.1406\tLR: 0.010000\n",
      "4.8074\t4.2946\t3.7582\t3.3340\t3.0620\t2.8843\t\n",
      "Training Epoch: 8 [490/567]\tLoss: 8.1848\tLR: 0.010000\n",
      "1.0355\t1.0891\t1.1776\t1.3483\t1.5966\t1.9376\t\n",
      "Training Epoch: 8 [506/567]\tLoss: 19.3697\tLR: 0.010000\n",
      "4.0305\t3.5514\t3.1107\t2.8666\t2.8432\t2.9673\t\n",
      "Training Epoch: 8 [522/567]\tLoss: 22.8559\tLR: 0.010000\n",
      "4.8654\t4.3329\t3.8376\t3.4439\t3.2108\t3.1654\t\n",
      "Training Epoch: 8 [538/567]\tLoss: 9.1390\tLR: 0.010000\n",
      "1.2770\t1.3288\t1.4009\t1.5041\t1.6788\t1.9493\t\n",
      "Training Epoch: 8 [554/567]\tLoss: 19.6746\tLR: 0.010000\n",
      "4.4995\t3.9836\t3.3800\t2.8619\t2.5413\t2.4083\t\n",
      "Training Epoch: 8 [570/567]\tLoss: 33.1349\tLR: 0.010000\n",
      "7.3147\t6.4612\t5.6457\t4.9767\t4.4936\t4.2432\t\n",
      "Training Epoch: 8 [586/567]\tLoss: 10.3266\tLR: 0.010000\n",
      "1.9946\t1.8128\t1.6561\t1.5758\t1.5960\t1.6912\t\n",
      "Training Epoch: 8 [602/567]\tLoss: 6.1434\tLR: 0.010000\n",
      "1.0848\t0.9674\t0.8892\t0.8922\t1.0188\t1.2912\t\n",
      "Training Epoch: 8 [618/567]\tLoss: 3.5827\tLR: 0.010000\n",
      "0.4878\t0.4805\t0.4892\t0.5472\t0.6781\t0.8999\t\n",
      "Training Epoch: 8 [634/567]\tLoss: 15.1239\tLR: 0.010000\n",
      "3.1370\t2.8018\t2.4482\t2.2408\t2.1919\t2.3042\t\n",
      "Training Epoch: 8 [650/567]\tLoss: 16.3328\tLR: 0.010000\n",
      "3.3368\t2.9508\t2.6452\t2.4555\t2.4150\t2.5295\t\n",
      "Training Epoch: 8 [666/567]\tLoss: 11.4581\tLR: 0.010000\n",
      "2.1217\t1.9657\t1.8188\t1.7607\t1.8196\t1.9716\t\n",
      "Training Epoch: 8 [682/567]\tLoss: 18.0205\tLR: 0.010000\n",
      "3.8179\t3.3343\t2.8716\t2.6247\t2.6104\t2.7617\t\n",
      "Training Epoch: 8 [698/567]\tLoss: 14.7664\tLR: 0.010000\n",
      "2.7344\t2.4905\t2.2909\t2.2631\t2.3952\t2.5924\t\n",
      "Training Epoch: 8 [714/567]\tLoss: 13.6273\tLR: 0.010000\n",
      "2.4579\t2.2583\t2.1190\t2.1130\t2.2360\t2.4430\t\n",
      "Training Epoch: 8 [730/567]\tLoss: 15.7508\tLR: 0.010000\n",
      "2.5953\t2.4714\t2.4357\t2.5345\t2.7141\t2.9998\t\n",
      "Training Epoch: 8 [746/567]\tLoss: 20.7368\tLR: 0.010000\n",
      "4.0653\t3.6748\t3.3490\t3.1861\t3.1883\t3.2734\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [762/567]\tLoss: 31.9389\tLR: 0.010000\n",
      "7.2643\t6.3544\t5.4138\t4.7056\t4.2474\t3.9533\t\n",
      "Training Epoch: 8 [778/567]\tLoss: 14.7420\tLR: 0.010000\n",
      "2.5878\t2.4343\t2.3613\t2.3432\t2.4210\t2.5945\t\n",
      "Training Epoch: 8 [794/567]\tLoss: 7.7950\tLR: 0.010000\n",
      "1.0097\t1.0284\t1.0988\t1.2689\t1.5273\t1.8619\t\n",
      "Training Epoch: 8 [810/567]\tLoss: 10.4473\tLR: 0.010000\n",
      "1.6667\t1.6259\t1.6208\t1.7030\t1.8235\t2.0073\t\n",
      "Training Epoch: 8 [826/567]\tLoss: 11.0622\tLR: 0.010000\n",
      "1.9902\t1.8753\t1.7922\t1.7421\t1.7679\t1.8946\t\n",
      "Training Epoch: 8 [842/567]\tLoss: 11.4687\tLR: 0.010000\n",
      "1.5149\t1.5697\t1.7089\t1.9431\t2.2163\t2.5157\t\n",
      "Training Epoch: 8 [858/567]\tLoss: 19.1759\tLR: 0.010000\n",
      "3.9756\t3.5525\t3.1752\t2.9242\t2.7731\t2.7753\t\n",
      "Training Epoch: 8 [874/567]\tLoss: 16.6193\tLR: 0.010000\n",
      "3.4229\t3.0756\t2.7674\t2.5178\t2.4090\t2.4266\t\n",
      "Training Epoch: 8 [890/567]\tLoss: 12.9327\tLR: 0.010000\n",
      "2.0723\t2.0013\t1.9953\t2.0808\t2.2547\t2.5282\t\n",
      "Training Epoch: 8 [903/567]\tLoss: 8.7241\tLR: 0.010000\n",
      "1.5855\t1.4336\t1.3378\t1.3320\t1.4264\t1.6087\t\n",
      "[0.32534468173980713, 0.48521867394447327, 0.1894366443157196, 0.007157458923757076, 0.5865350961685181, 0.23540958762168884, 0.1780553162097931, 0.013724918477237225, 0.5873316526412964, 0.17140763998031616, 0.24126070737838745, -0.006421932019293308, 0.37785834074020386, 0.33380213379859924, 0.2883395254611969, -5.352497464627959e-05, 0.17624904215335846, 0.0, 0.8237509578466415, 0.00020179273269604892]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3178, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2880, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2621, Accuracy: 0.5556\n",
      "Test set t = 03: Average loss: 0.2481, Accuracy: 0.5485\n",
      "Test set t = 04: Average loss: 0.2472, Accuracy: 0.5256\n",
      "Test set t = 05: Average loss: 0.2575, Accuracy: 0.5168\n",
      "\n",
      "Training Epoch: 9 [10/567]\tLoss: 21.6828\tLR: 0.010000\n",
      "4.8340\t4.2947\t3.6580\t3.1615\t2.9023\t2.8323\t\n",
      "Training Epoch: 9 [26/567]\tLoss: 13.6983\tLR: 0.010000\n",
      "2.4739\t2.2649\t2.1254\t2.1077\t2.2407\t2.4857\t\n",
      "Training Epoch: 9 [42/567]\tLoss: 20.5440\tLR: 0.010000\n",
      "4.1639\t3.7624\t3.3678\t3.1087\t3.0255\t3.1157\t\n",
      "Training Epoch: 9 [58/567]\tLoss: 18.6583\tLR: 0.010000\n",
      "3.5301\t3.1717\t2.9273\t2.8971\t2.9738\t3.1584\t\n",
      "Training Epoch: 9 [74/567]\tLoss: 13.0096\tLR: 0.010000\n",
      "2.0409\t1.9816\t1.9981\t2.1159\t2.3164\t2.5567\t\n",
      "Training Epoch: 9 [90/567]\tLoss: 13.7227\tLR: 0.010000\n",
      "2.4993\t2.3537\t2.2173\t2.1340\t2.1862\t2.3322\t\n",
      "Training Epoch: 9 [106/567]\tLoss: 12.1861\tLR: 0.010000\n",
      "2.0017\t1.9085\t1.8684\t1.9354\t2.1129\t2.3591\t\n",
      "Training Epoch: 9 [122/567]\tLoss: 18.3672\tLR: 0.010000\n",
      "3.6307\t3.2774\t2.9929\t2.8345\t2.7967\t2.8349\t\n",
      "Training Epoch: 9 [138/567]\tLoss: 9.1323\tLR: 0.010000\n",
      "1.1081\t1.1693\t1.3157\t1.5361\t1.8375\t2.1655\t\n",
      "Training Epoch: 9 [154/567]\tLoss: 23.1972\tLR: 0.010000\n",
      "4.7951\t4.2952\t3.8339\t3.5424\t3.3711\t3.3595\t\n",
      "Training Epoch: 9 [170/567]\tLoss: 14.5886\tLR: 0.010000\n",
      "2.6693\t2.5141\t2.3632\t2.2936\t2.3161\t2.4323\t\n",
      "Training Epoch: 9 [186/567]\tLoss: 20.8151\tLR: 0.010000\n",
      "4.4570\t4.0098\t3.5487\t3.1581\t2.8870\t2.7546\t\n",
      "Training Epoch: 9 [202/567]\tLoss: 19.3986\tLR: 0.010000\n",
      "3.8171\t3.5083\t3.2213\t3.0150\t2.9176\t2.9193\t\n",
      "Training Epoch: 9 [218/567]\tLoss: 3.4869\tLR: 0.010000\n",
      "0.2854\t0.3180\t0.4139\t0.5743\t0.8117\t1.0836\t\n",
      "Training Epoch: 9 [234/567]\tLoss: 25.5569\tLR: 0.010000\n",
      "5.4352\t4.8096\t4.2334\t3.8551\t3.6411\t3.5825\t\n",
      "Training Epoch: 9 [250/567]\tLoss: 7.5341\tLR: 0.010000\n",
      "1.0369\t1.0269\t1.0638\t1.2118\t1.4573\t1.7373\t\n",
      "Training Epoch: 9 [266/567]\tLoss: 17.0945\tLR: 0.010000\n",
      "3.2018\t2.9303\t2.7312\t2.6523\t2.7151\t2.8638\t\n",
      "Training Epoch: 9 [282/567]\tLoss: 4.8464\tLR: 0.010000\n",
      "0.5412\t0.5838\t0.6757\t0.8144\t0.9999\t1.2314\t\n",
      "Training Epoch: 9 [298/567]\tLoss: 10.7741\tLR: 0.010000\n",
      "1.8392\t1.7435\t1.6904\t1.7032\t1.8139\t1.9838\t\n",
      "Training Epoch: 9 [314/567]\tLoss: 11.9319\tLR: 0.010000\n",
      "2.0294\t1.9263\t1.8742\t1.8944\t2.0131\t2.1945\t\n",
      "Training Epoch: 9 [330/567]\tLoss: 21.5795\tLR: 0.010000\n",
      "5.2313\t4.5020\t3.6998\t3.0419\t2.6428\t2.4617\t\n",
      "Training Epoch: 9 [346/567]\tLoss: 32.0124\tLR: 0.010000\n",
      "7.7859\t6.6564\t5.5220\t4.5834\t3.9320\t3.5327\t\n",
      "Training Epoch: 9 [362/567]\tLoss: 18.9543\tLR: 0.010000\n",
      "3.2056\t3.1175\t3.0738\t3.0941\t3.1695\t3.2938\t\n",
      "Training Epoch: 9 [378/567]\tLoss: 16.1624\tLR: 0.010000\n",
      "3.1935\t2.8840\t2.6154\t2.4711\t2.4643\t2.5342\t\n",
      "Training Epoch: 9 [394/567]\tLoss: 5.5296\tLR: 0.010000\n",
      "0.8321\t0.7806\t0.7740\t0.8551\t1.0237\t1.2641\t\n",
      "Training Epoch: 9 [410/567]\tLoss: 22.3596\tLR: 0.010000\n",
      "4.9431\t4.3438\t3.7552\t3.3231\t3.0489\t2.9455\t\n",
      "Training Epoch: 9 [426/567]\tLoss: 19.0352\tLR: 0.010000\n",
      "3.5302\t3.2883\t3.0939\t3.0100\t3.0244\t3.0885\t\n",
      "Training Epoch: 9 [442/567]\tLoss: 11.6886\tLR: 0.010000\n",
      "2.5288\t2.2072\t1.9064\t1.7073\t1.6369\t1.7019\t\n",
      "Training Epoch: 9 [458/567]\tLoss: 16.0819\tLR: 0.010000\n",
      "3.4777\t3.0710\t2.6566\t2.3759\t2.2348\t2.2658\t\n",
      "Training Epoch: 9 [474/567]\tLoss: 28.1424\tLR: 0.010000\n",
      "5.8958\t5.3307\t4.7684\t4.3142\t4.0011\t3.8322\t\n",
      "Training Epoch: 9 [490/567]\tLoss: 6.4266\tLR: 0.010000\n",
      "0.9617\t0.9629\t0.9875\t1.0481\t1.1574\t1.3092\t\n",
      "Training Epoch: 9 [506/567]\tLoss: 15.7983\tLR: 0.010000\n",
      "3.1117\t2.8304\t2.5893\t2.4358\t2.3905\t2.4405\t\n",
      "Training Epoch: 9 [522/567]\tLoss: 29.0646\tLR: 0.010000\n",
      "6.3387\t5.6819\t4.9869\t4.4037\t3.9690\t3.6844\t\n",
      "Training Epoch: 9 [538/567]\tLoss: 18.1578\tLR: 0.010000\n",
      "3.7197\t3.3372\t2.9788\t2.7623\t2.6771\t2.6828\t\n",
      "Training Epoch: 9 [554/567]\tLoss: 13.7425\tLR: 0.010000\n",
      "2.3601\t2.2881\t2.2592\t2.2331\t2.2588\t2.3432\t\n",
      "Training Epoch: 9 [570/567]\tLoss: 5.2585\tLR: 0.010000\n",
      "0.7695\t0.7222\t0.7217\t0.8036\t0.9851\t1.2564\t\n",
      "Training Epoch: 9 [586/567]\tLoss: 12.1857\tLR: 0.010000\n",
      "2.5055\t2.2941\t2.0087\t1.7802\t1.7403\t1.8569\t\n",
      "Training Epoch: 9 [602/567]\tLoss: 18.4313\tLR: 0.010000\n",
      "3.3778\t3.1374\t2.9908\t2.9402\t2.9416\t3.0435\t\n",
      "Training Epoch: 9 [618/567]\tLoss: 11.7529\tLR: 0.010000\n",
      "2.2074\t1.9752\t1.8275\t1.8046\t1.8803\t2.0579\t\n",
      "Training Epoch: 9 [634/567]\tLoss: 17.2795\tLR: 0.010000\n",
      "3.6381\t3.1878\t2.8468\t2.6162\t2.4921\t2.4985\t\n",
      "Training Epoch: 9 [650/567]\tLoss: 9.3415\tLR: 0.010000\n",
      "1.7110\t1.5835\t1.4772\t1.4439\t1.4989\t1.6270\t\n",
      "Training Epoch: 9 [666/567]\tLoss: 22.1269\tLR: 0.010000\n",
      "4.5373\t4.0796\t3.6381\t3.3483\t3.2344\t3.2892\t\n",
      "Training Epoch: 9 [682/567]\tLoss: 17.7304\tLR: 0.010000\n",
      "3.4372\t3.1972\t2.9135\t2.7272\t2.6728\t2.7826\t\n",
      "Training Epoch: 9 [698/567]\tLoss: 20.5993\tLR: 0.010000\n",
      "3.5535\t3.3658\t3.2719\t3.3224\t3.4616\t3.6243\t\n",
      "Training Epoch: 9 [714/567]\tLoss: 11.8658\tLR: 0.010000\n",
      "2.4690\t2.1660\t1.9221\t1.7708\t1.7216\t1.8162\t\n",
      "Training Epoch: 9 [730/567]\tLoss: 19.9389\tLR: 0.010000\n",
      "4.1684\t3.7216\t3.3554\t3.0896\t2.8794\t2.7245\t\n",
      "Training Epoch: 9 [746/567]\tLoss: 24.5240\tLR: 0.010000\n",
      "5.1491\t4.5281\t4.0007\t3.6733\t3.5656\t3.6071\t\n",
      "Training Epoch: 9 [762/567]\tLoss: 28.5978\tLR: 0.010000\n",
      "6.2163\t5.5054\t4.7936\t4.2972\t3.9727\t3.8127\t\n",
      "Training Epoch: 9 [778/567]\tLoss: 9.5344\tLR: 0.010000\n",
      "1.4559\t1.3718\t1.3759\t1.5045\t1.7490\t2.0773\t\n",
      "Training Epoch: 9 [794/567]\tLoss: 10.5727\tLR: 0.010000\n",
      "1.4299\t1.4350\t1.5015\t1.7127\t2.0414\t2.4521\t\n",
      "Training Epoch: 9 [810/567]\tLoss: 10.8037\tLR: 0.010000\n",
      "1.6542\t1.5747\t1.5979\t1.7280\t1.9595\t2.2894\t\n",
      "Training Epoch: 9 [826/567]\tLoss: 12.1116\tLR: 0.010000\n",
      "2.3748\t2.1124\t1.8835\t1.8173\t1.8750\t2.0486\t\n",
      "Training Epoch: 9 [842/567]\tLoss: 8.5783\tLR: 0.010000\n",
      "1.2086\t1.2187\t1.2774\t1.4048\t1.5955\t1.8732\t\n",
      "Training Epoch: 9 [858/567]\tLoss: 13.4743\tLR: 0.010000\n",
      "2.6027\t2.2879\t2.0651\t2.0122\t2.1388\t2.3676\t\n",
      "Training Epoch: 9 [874/567]\tLoss: 18.1971\tLR: 0.010000\n",
      "3.8085\t3.3966\t3.0087\t2.7558\t2.6191\t2.6085\t\n",
      "Training Epoch: 9 [890/567]\tLoss: 26.8236\tLR: 0.010000\n",
      "5.9185\t5.1984\t4.5195\t4.0008\t3.6816\t3.5049\t\n",
      "Training Epoch: 9 [903/567]\tLoss: 16.0876\tLR: 0.010000\n",
      "2.7813\t2.6664\t2.6052\t2.5886\t2.6665\t2.7796\t\n",
      "[0.3169921338558197, 0.5042538642883301, 0.17875400185585022, 0.006521634291857481, 0.605541467666626, 0.2246684581041336, 0.16979007422924042, 0.013673233799636364, 0.6180806159973145, 0.15990540385246277, 0.22201398015022278, -0.0076993973925709724, 0.38447338342666626, 0.345432311296463, 0.2700943052768707, -0.0008151783840730786, 0.1727464497089386, 0.0, 0.8272535502910614, -0.000474238651804626]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3185, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2883, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2622, Accuracy: 0.5573\n",
      "Test set t = 03: Average loss: 0.2483, Accuracy: 0.5467\n",
      "Test set t = 04: Average loss: 0.2472, Accuracy: 0.5309\n",
      "Test set t = 05: Average loss: 0.2571, Accuracy: 0.5203\n",
      "\n",
      "Training Epoch: 10 [10/567]\tLoss: 19.0796\tLR: 0.010000\n",
      "3.9341\t3.4971\t3.1106\t2.9046\t2.8060\t2.8272\t\n",
      "Training Epoch: 10 [26/567]\tLoss: 7.6277\tLR: 0.010000\n",
      "1.1789\t1.1494\t1.1581\t1.2194\t1.3586\t1.5633\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [42/567]\tLoss: 31.9217\tLR: 0.010000\n",
      "6.6724\t5.9229\t5.3081\t4.8861\t4.6266\t4.5057\t\n",
      "Training Epoch: 10 [58/567]\tLoss: 16.5814\tLR: 0.010000\n",
      "3.4764\t3.1210\t2.7242\t2.4724\t2.3661\t2.4213\t\n",
      "Training Epoch: 10 [74/567]\tLoss: 19.2602\tLR: 0.010000\n",
      "4.0364\t3.5621\t3.1377\t2.9071\t2.7959\t2.8211\t\n",
      "Training Epoch: 10 [90/567]\tLoss: 28.3567\tLR: 0.010000\n",
      "6.5535\t5.7170\t4.8779\t4.1757\t3.6785\t3.3541\t\n",
      "Training Epoch: 10 [106/567]\tLoss: 33.6577\tLR: 0.010000\n",
      "7.4906\t6.6256\t5.7074\t5.0190\t4.5405\t4.2746\t\n",
      "Training Epoch: 10 [122/567]\tLoss: 10.4049\tLR: 0.010000\n",
      "1.9087\t1.7643\t1.6173\t1.5745\t1.6666\t1.8735\t\n",
      "Training Epoch: 10 [138/567]\tLoss: 17.6404\tLR: 0.010000\n",
      "3.1366\t2.9428\t2.8489\t2.8327\t2.8916\t2.9878\t\n",
      "Training Epoch: 10 [154/567]\tLoss: 11.4782\tLR: 0.010000\n",
      "2.4179\t2.1189\t1.8381\t1.6727\t1.6681\t1.7624\t\n",
      "Training Epoch: 10 [170/567]\tLoss: 8.9545\tLR: 0.010000\n",
      "1.4574\t1.3921\t1.3769\t1.4273\t1.5495\t1.7514\t\n",
      "Training Epoch: 10 [186/567]\tLoss: 20.7433\tLR: 0.010000\n",
      "4.6400\t4.0335\t3.4502\t3.0150\t2.8227\t2.7818\t\n",
      "Training Epoch: 10 [202/567]\tLoss: 24.9573\tLR: 0.010000\n",
      "5.1430\t4.5494\t4.0836\t3.7990\t3.6821\t3.7003\t\n",
      "Training Epoch: 10 [218/567]\tLoss: 14.9597\tLR: 0.010000\n",
      "2.6582\t2.4673\t2.3160\t2.3016\t2.4535\t2.7630\t\n",
      "Training Epoch: 10 [234/567]\tLoss: 6.2113\tLR: 0.010000\n",
      "0.7443\t0.7702\t0.8447\t1.0042\t1.2654\t1.5825\t\n",
      "Training Epoch: 10 [250/567]\tLoss: 11.1252\tLR: 0.010000\n",
      "1.9600\t1.8086\t1.6818\t1.6753\t1.8644\t2.1352\t\n",
      "Training Epoch: 10 [266/567]\tLoss: 22.0955\tLR: 0.010000\n",
      "4.7018\t4.1126\t3.5741\t3.2836\t3.1962\t3.2272\t\n",
      "Training Epoch: 10 [282/567]\tLoss: 11.2024\tLR: 0.010000\n",
      "1.8226\t1.7189\t1.6934\t1.7729\t1.9680\t2.2266\t\n",
      "Training Epoch: 10 [298/567]\tLoss: 35.0343\tLR: 0.010000\n",
      "8.4162\t7.1777\t6.0171\t5.0806\t4.3822\t3.9604\t\n",
      "Training Epoch: 10 [314/567]\tLoss: 6.8763\tLR: 0.010000\n",
      "1.3077\t1.1793\t1.0433\t0.9926\t1.0789\t1.2744\t\n",
      "Training Epoch: 10 [330/567]\tLoss: 11.1623\tLR: 0.010000\n",
      "1.7744\t1.7038\t1.6869\t1.7830\t1.9746\t2.2396\t\n",
      "Training Epoch: 10 [346/567]\tLoss: 13.2038\tLR: 0.010000\n",
      "2.3464\t2.1253\t1.9909\t2.0468\t2.2209\t2.4735\t\n",
      "Training Epoch: 10 [362/567]\tLoss: 20.2390\tLR: 0.010000\n",
      "4.2921\t3.7495\t3.3256\t3.0488\t2.9137\t2.9093\t\n",
      "Training Epoch: 10 [378/567]\tLoss: 6.4952\tLR: 0.010000\n",
      "0.8430\t0.8548\t0.9384\t1.0709\t1.2602\t1.5280\t\n",
      "Training Epoch: 10 [394/567]\tLoss: 12.4807\tLR: 0.010000\n",
      "2.0809\t1.9257\t1.8895\t1.9822\t2.1738\t2.4285\t\n",
      "Training Epoch: 10 [410/567]\tLoss: 10.4475\tLR: 0.010000\n",
      "2.0506\t1.8191\t1.6319\t1.5631\t1.6108\t1.7720\t\n",
      "Training Epoch: 10 [426/567]\tLoss: 16.6625\tLR: 0.010000\n",
      "3.6092\t3.1207\t2.6312\t2.3708\t2.3842\t2.5464\t\n",
      "Training Epoch: 10 [442/567]\tLoss: 15.5600\tLR: 0.010000\n",
      "2.4907\t2.4375\t2.4251\t2.5249\t2.7242\t2.9575\t\n",
      "Training Epoch: 10 [458/567]\tLoss: 9.9476\tLR: 0.010000\n",
      "1.4471\t1.3996\t1.4668\t1.6470\t1.8649\t2.1221\t\n",
      "Training Epoch: 10 [474/567]\tLoss: 10.7941\tLR: 0.010000\n",
      "1.2729\t1.4060\t1.6107\t1.8701\t2.1626\t2.4717\t\n",
      "Training Epoch: 10 [490/567]\tLoss: 15.9689\tLR: 0.010000\n",
      "3.0357\t2.7482\t2.5430\t2.4686\t2.5195\t2.6539\t\n",
      "Training Epoch: 10 [506/567]\tLoss: 18.8930\tLR: 0.010000\n",
      "4.0007\t3.6293\t3.1954\t2.8570\t2.6417\t2.5689\t\n",
      "Training Epoch: 10 [522/567]\tLoss: 21.9447\tLR: 0.010000\n",
      "4.8137\t4.2155\t3.6598\t3.2526\t3.0263\t2.9768\t\n",
      "Training Epoch: 10 [538/567]\tLoss: 17.7201\tLR: 0.010000\n",
      "3.4200\t3.1151\t2.8642\t2.7333\t2.7380\t2.8495\t\n",
      "Training Epoch: 10 [554/567]\tLoss: 15.7560\tLR: 0.010000\n",
      "3.4109\t2.9827\t2.5884\t2.3486\t2.2133\t2.2121\t\n",
      "Training Epoch: 10 [570/567]\tLoss: 14.9913\tLR: 0.010000\n",
      "2.8922\t2.6203\t2.3909\t2.2699\t2.3023\t2.5158\t\n",
      "Training Epoch: 10 [586/567]\tLoss: 16.6816\tLR: 0.010000\n",
      "2.9563\t2.7640\t2.6551\t2.6488\t2.7471\t2.9104\t\n",
      "Training Epoch: 10 [602/567]\tLoss: 21.3667\tLR: 0.010000\n",
      "3.6965\t3.5476\t3.4431\t3.4202\t3.5247\t3.7346\t\n",
      "Training Epoch: 10 [618/567]\tLoss: 10.2589\tLR: 0.010000\n",
      "1.8131\t1.6639\t1.5882\t1.6007\t1.7143\t1.8788\t\n",
      "Training Epoch: 10 [634/567]\tLoss: 13.3531\tLR: 0.010000\n",
      "2.7337\t2.3586\t2.0577\t1.9602\t2.0266\t2.2162\t\n",
      "Training Epoch: 10 [650/567]\tLoss: 10.8844\tLR: 0.010000\n",
      "1.9690\t1.8189\t1.7075\t1.6724\t1.7563\t1.9602\t\n",
      "Training Epoch: 10 [666/567]\tLoss: 21.7249\tLR: 0.010000\n",
      "4.2546\t3.8639\t3.5177\t3.3410\t3.3157\t3.4320\t\n",
      "Training Epoch: 10 [682/567]\tLoss: 16.5526\tLR: 0.010000\n",
      "3.0379\t2.7600\t2.5783\t2.5572\t2.6920\t2.9271\t\n",
      "Training Epoch: 10 [698/567]\tLoss: 9.0713\tLR: 0.010000\n",
      "1.4020\t1.3536\t1.3528\t1.4436\t1.6328\t1.8863\t\n",
      "Training Epoch: 10 [714/567]\tLoss: 27.3644\tLR: 0.010000\n",
      "6.1426\t5.3914\t4.6529\t4.0813\t3.6704\t3.4258\t\n",
      "Training Epoch: 10 [730/567]\tLoss: 7.1796\tLR: 0.010000\n",
      "1.0384\t1.0355\t1.0775\t1.1585\t1.3219\t1.5478\t\n",
      "Training Epoch: 10 [746/567]\tLoss: 17.3935\tLR: 0.010000\n",
      "3.4119\t3.1012\t2.8370\t2.6747\t2.6268\t2.7418\t\n",
      "Training Epoch: 10 [762/567]\tLoss: 15.6292\tLR: 0.010000\n",
      "2.9351\t2.7107\t2.5003\t2.4079\t2.4712\t2.6040\t\n",
      "Training Epoch: 10 [778/567]\tLoss: 9.7471\tLR: 0.010000\n",
      "1.6710\t1.5405\t1.4731\t1.5097\t1.6584\t1.8943\t\n",
      "Training Epoch: 10 [794/567]\tLoss: 13.9104\tLR: 0.010000\n",
      "2.7093\t2.4335\t2.2121\t2.1005\t2.1368\t2.3181\t\n",
      "Training Epoch: 10 [810/567]\tLoss: 18.5202\tLR: 0.010000\n",
      "3.7325\t3.3559\t3.0206\t2.8199\t2.7626\t2.8287\t\n",
      "Training Epoch: 10 [826/567]\tLoss: 17.0876\tLR: 0.010000\n",
      "3.7829\t3.3297\t2.8834\t2.5338\t2.3258\t2.2320\t\n",
      "Training Epoch: 10 [842/567]\tLoss: 16.9715\tLR: 0.010000\n",
      "3.3600\t3.0540\t2.7558\t2.5914\t2.5626\t2.6477\t\n",
      "Training Epoch: 10 [858/567]\tLoss: 23.2702\tLR: 0.010000\n",
      "4.7858\t4.2782\t3.8211\t3.5394\t3.4225\t3.4231\t\n",
      "Training Epoch: 10 [874/567]\tLoss: 16.4971\tLR: 0.010000\n",
      "3.4688\t3.1248\t2.8070\t2.5388\t2.3283\t2.2294\t\n",
      "Training Epoch: 10 [890/567]\tLoss: 10.4408\tLR: 0.010000\n",
      "1.6436\t1.6255\t1.6294\t1.6927\t1.8254\t2.0242\t\n",
      "Training Epoch: 10 [903/567]\tLoss: 14.7762\tLR: 0.010000\n",
      "2.3774\t2.3514\t2.3564\t2.4246\t2.5523\t2.7142\t\n",
      "[0.31348809599876404, 0.5139621496200562, 0.1725497543811798, 0.005481532774865627, 0.6224265694618225, 0.212581068277359, 0.16499236226081848, 0.013216684572398663, 0.6418160200119019, 0.15159066021442413, 0.206593319773674, -0.009426181204617023, 0.38042375445365906, 0.3628375232219696, 0.25673872232437134, -0.0017658849246799946, 0.1657879650592804, 0.0, 0.8342120349407196, -0.001484726439230144]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3194, Accuracy: 0.5520\n",
      "Test set t = 01: Average loss: 0.2895, Accuracy: 0.5591\n",
      "Test set t = 02: Average loss: 0.2635, Accuracy: 0.5573\n",
      "Test set t = 03: Average loss: 0.2491, Accuracy: 0.5467\n",
      "Test set t = 04: Average loss: 0.2468, Accuracy: 0.5309\n",
      "Test set t = 05: Average loss: 0.2548, Accuracy: 0.5220\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d789bcaa0787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TIMESTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msumwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[0msumwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ff37c59e2e1f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(net, epoch, dataloader, timesteps, writer, tag)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mtest_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Load original network\n",
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "\n",
    "# Load PNet with hyperparameters\n",
    "pnet = load_pnet(\n",
    "    net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "    ff_multiplier=0.33, fb_multiplier=0.33, er_multiplier=0.0,\n",
    "    same_param=SAME_PARAM, device='cuda:0'\n",
    "    )\n",
    "\n",
    "# Set up loss function and hyperparameters\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "hyperparams = [*pnet.get_hyperparameters()]\n",
    "if SAME_PARAM:\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': hyperparams[:-1], 'lr':0.01},\n",
    "        {'params': hyperparams[-1:], 'lr':0.0001}], weight_decay=0.00001)\n",
    "else:\n",
    "    fffbmem_hp = []\n",
    "    erm_hp = []\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "        erm_hp.append(hyperparams[pc*4+3])\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': fffbmem_hp, 'lr':0.01},\n",
    "        {'params': erm_hp, 'lr':0.0001}], weight_decay=0.00001)\n",
    "\n",
    "# Log initial hyperparameter and eval values\n",
    "log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "hps = pnet.get_hyperparameters_values()\n",
    "print(hps)\n",
    "evaluate(\n",
    "    pnet, 0, noise_loader,\n",
    "    timesteps=MAX_TIMESTEP, writer=sumwriter,\n",
    "    tag='Noisy'\n",
    "    )\n",
    "\n",
    "# Run epochs\n",
    "for epoch in range(1, EPOCH+1):\n",
    "    train(\n",
    "        pnet, epoch, noise_loader,\n",
    "        timesteps=MAX_TIMESTEP, writer=sumwriter\n",
    "        )\n",
    "    log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "\n",
    "    evaluate(\n",
    "        pnet, epoch, noise_loader,\n",
    "        timesteps=MAX_TIMESTEP, writer=sumwriter,\n",
    "        tag='Noisy'\n",
    "        )\n",
    "evaluate(\n",
    "    pnet, epoch, clean_loader,\n",
    "    timesteps=MAX_TIMESTEP, writer=sumwriter,\n",
    "    tag='Clean'\n",
    "    )\n",
    "sumwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba94982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
