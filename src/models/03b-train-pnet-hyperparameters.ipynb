{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b84576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fbc7e",
   "metadata": {},
   "source": [
    "# Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfb56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total training epoches\n",
    "EPOCH = 15\n",
    "\n",
    "SAME_PARAM = False           # to use the same parameters for all pcoders or not\n",
    "FF_START = True             # to start from feedforward initialization\n",
    "MAX_TIMESTEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fae3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 2\n",
    "noise_types = ['Babble2Spkr'] #['AudScene', 'Babble8Spkr', 'SpeakerShapedNoise']\n",
    "snr_levels = [-9., -6., -3.,  0.,  3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c99b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path names\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Load network arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8efd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "pnet_name = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bdfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-50-regular.pth'\n",
    "fb_state_dict = torch.load(fb_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c46f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d783f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "        net, state_dict, build_graph, random_init,\n",
    "        ff_multiplier, fb_multiplier, er_multiplier,\n",
    "        same_param, device='cuda:0'):\n",
    "    \n",
    "    if same_param:\n",
    "        raise Exception('Not implemented!')\n",
    "    else:\n",
    "        pnet = PNetClass(\n",
    "            net, build_graph=build_graph, random_init=random_init,\n",
    "            ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier\n",
    "            )\n",
    "\n",
    "    pnet.load_state_dict(state_dict)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e13f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, epoch, dataloader, timesteps, loss_function, writer=None, tag='Clean'):\n",
    "    test_loss = np.zeros((timesteps+1,))\n",
    "    correct   = np.zeros((timesteps+1,))\n",
    "    for (images, labels) in dataloader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tt in range(timesteps+1):\n",
    "                if tt == 0:\n",
    "                    outputs, _ = net(images)\n",
    "                else:\n",
    "                    outputs, _ = net()\n",
    "                \n",
    "                loss = loss_function(outputs, labels)\n",
    "                test_loss[tt] += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct[tt] += preds.eq(labels).sum()\n",
    "\n",
    "    print()\n",
    "    for tt in range(timesteps+1):\n",
    "        test_loss[tt] /= len(dataloader.dataset)\n",
    "        correct[tt] /= len(dataloader.dataset)\n",
    "        print('Test set t = {:02d}: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "            tt,\n",
    "            test_loss[tt],\n",
    "            correct[tt]\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"{tag}Perf/Epoch#{epoch}\",\n",
    "                correct[tt], tt\n",
    "                )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32cdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epoch, dataloader, timesteps, loss_function, optimizer, writer=None):\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "        net.reset()\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        ttloss = np.zeros((timesteps+1))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for tt in range(timesteps+1):\n",
    "            if tt == 0:\n",
    "                outputs, _ = net(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = loss.item()\n",
    "            else:\n",
    "                outputs, _ = net()\n",
    "                current_loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = current_loss.item()\n",
    "                loss += current_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.update_hyperparameters()\n",
    "            \n",
    "        print(f\"Training Epoch: {epoch} [{batch_index * 16 + len(images)}/{len(dataloader.dataset)}]\\tLoss: {loss.item():0.4f}\\tLR: {optimizer.param_groups[0]['lr']:0.6f}\")\n",
    "        for tt in range(timesteps+1):\n",
    "            print(f'{ttloss[tt]:0.4f}\\t', end='')\n",
    "        print()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"TrainingLoss/CE\", loss.item(),\n",
    "                (epoch-1)*len(dataloader) + batch_index\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68603d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_parameters(net, epoch, sumwriter, same_param=True):\n",
    "    if same_param:\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedforward\", getattr(net,f'ff_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedback\",    getattr(net,f'fb_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/error\",       getattr(net,f'errorm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/memory\",      getattr(net,f'mem_part').item(), epoch)\n",
    "\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedforward\", getattr(net,f'ffm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedback\",    getattr(net,f'fbm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/error\",       getattr(net,f'erm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/memory\",      1-getattr(net,f'ffm').item()-getattr(net,f'fbm').item(), epoch)\n",
    "    else:\n",
    "        for i in range(1, net.number_of_pcoders+1):\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedforward\", getattr(net,f'ffm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", 0, epoch)\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_error\", getattr(net,f'erm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item()-getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item(), epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85955819",
   "metadata": {},
   "source": [
    "# Main hyperparameter optimization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf6488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(noise_type, snr_level):\n",
    "    # Load clean and noisy data\n",
    "    clean_ds_path = f'{engram_dir}training_dataset_random_order.hdf5'\n",
    "    clean_ds = CleanSoundsDataset(clean_ds_path)\n",
    "    clean_loader = torch.utils.data.DataLoader(\n",
    "        clean_ds,  batch_size=BATCH_SIZE,\n",
    "        shuffle=False, drop_last=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    noisy_ds = NoisyDataset(bg=noise_type, snr=snr_level)\n",
    "    noise_loader = torch.utils.data.DataLoader(\n",
    "        noisy_ds,  batch_size=BATCH_SIZE,\n",
    "        shuffle=True, drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    # Set up logs and network for training\n",
    "    net_dir = f'hyper_{noise_type}_snr{snr_level}'\n",
    "    if FF_START:\n",
    "        net_dir += '_FFstart'\n",
    "    if SAME_PARAM:\n",
    "        net_dir += '_shared'\n",
    "\n",
    "    sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "    net = BranchedNetwork() # Load original network\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet_fw = load_pnet( # Load FF PNet\n",
    "        net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "        ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    evaluate(\n",
    "        pnet_fw, 0, noise_loader, 1,\n",
    "        loss_function,\n",
    "        writer=sumwriter, tag='FeedForward')\n",
    "    del pnet_fw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load PNet for hyperparameter optimization\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = load_pnet(\n",
    "        net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "        ff_multiplier=0.33, fb_multiplier=0.33, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "\n",
    "    # Set up loss function and hyperparameters\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    hyperparams = [*pnet.get_hyperparameters()]\n",
    "    if SAME_PARAM:\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': hyperparams[:-1], 'lr':0.01},\n",
    "            {'params': hyperparams[-1:], 'lr':0.0001}], weight_decay=0.00001)\n",
    "    else:\n",
    "        fffbmem_hp = []\n",
    "        erm_hp = []\n",
    "        for pc in range(pnet.number_of_pcoders):\n",
    "            fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "            erm_hp.append(hyperparams[pc*4+3])\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': fffbmem_hp, 'lr':0.01},\n",
    "            {'params': erm_hp, 'lr':0.0001}], weight_decay=0.00001)\n",
    "\n",
    "    # Log initial hyperparameter and eval values\n",
    "    log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "    evaluate(\n",
    "        pnet, 0, noise_loader,\n",
    "        MAX_TIMESTEP, loss_function,\n",
    "        writer=sumwriter, tag='Noisy'\n",
    "        )\n",
    "\n",
    "    # Run epochs\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        train(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function, optimizer,\n",
    "            writer=sumwriter\n",
    "            )\n",
    "        log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "        hps = pnet.get_hyperparameters_values()\n",
    "        print(hps)\n",
    "\n",
    "        evaluate(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function,\n",
    "            writer=sumwriter, tag='Noisy'\n",
    "            )\n",
    "    # evaluate(\n",
    "    #     pnet, epoch, clean_loader,\n",
    "    #     timesteps=MAX_TIMESTEP, writer=sumwriter,\n",
    "    #     tag='Clean'\n",
    "    #     )\n",
    "    sumwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba94982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Babble2Spkr, for SNR -9.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([10, 164, 400])) that is different to the input size (torch.Size([10, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([9, 164, 400])) that is different to the input size (torch.Size([9, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.7686, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6871, Accuracy: 0.0703\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7686, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6872, Accuracy: 0.0703\n",
      "Test set t = 02: Average loss: 0.6241, Accuracy: 0.0721\n",
      "Test set t = 03: Average loss: 0.5920, Accuracy: 0.0773\n",
      "Test set t = 04: Average loss: 0.5866, Accuracy: 0.0844\n",
      "Test set t = 05: Average loss: 0.6041, Accuracy: 0.0844\n",
      "\n",
      "Training Epoch: 1 [10/569]\tLoss: 42.3507\tLR: 0.010000\n",
      "8.3115\t7.4629\t6.8658\t6.4816\t6.4753\t6.7535\t\n",
      "Training Epoch: 1 [26/569]\tLoss: 38.4413\tLR: 0.010000\n",
      "8.0770\t7.2235\t6.4415\t5.8244\t5.5063\t5.3686\t\n",
      "Training Epoch: 1 [42/569]\tLoss: 42.6062\tLR: 0.010000\n",
      "8.5138\t7.6022\t6.9552\t6.5760\t6.3951\t6.5640\t\n",
      "Training Epoch: 1 [58/569]\tLoss: 33.0611\tLR: 0.010000\n",
      "6.6789\t5.9609\t5.3928\t5.0175\t4.9069\t5.1041\t\n",
      "Training Epoch: 1 [74/569]\tLoss: 40.4247\tLR: 0.010000\n",
      "8.3084\t7.2208\t6.3611\t5.9673\t6.0300\t6.5372\t\n",
      "Training Epoch: 1 [90/569]\tLoss: 43.2227\tLR: 0.010000\n",
      "9.3444\t8.0700\t7.0063\t6.3575\t6.1055\t6.3389\t\n",
      "Training Epoch: 1 [106/569]\tLoss: 44.9373\tLR: 0.010000\n",
      "9.4225\t8.1580\t7.2128\t6.7614\t6.5980\t6.7847\t\n",
      "Training Epoch: 1 [122/569]\tLoss: 44.9355\tLR: 0.010000\n",
      "8.7667\t7.9955\t7.3476\t6.9964\t6.8435\t6.9858\t\n",
      "Training Epoch: 1 [138/569]\tLoss: 35.9222\tLR: 0.010000\n",
      "6.5254\t5.8983\t5.5584\t5.6529\t5.9306\t6.3566\t\n",
      "Training Epoch: 1 [154/569]\tLoss: 39.6439\tLR: 0.010000\n",
      "7.7908\t7.0572\t6.5106\t6.2104\t6.0493\t6.0255\t\n",
      "Training Epoch: 1 [170/569]\tLoss: 35.5783\tLR: 0.010000\n",
      "6.6953\t6.1258\t5.7810\t5.6353\t5.6645\t5.6764\t\n",
      "Training Epoch: 1 [186/569]\tLoss: 40.7334\tLR: 0.010000\n",
      "8.2705\t7.3617\t6.6783\t6.2999\t6.0768\t6.0462\t\n",
      "Training Epoch: 1 [202/569]\tLoss: 41.4751\tLR: 0.010000\n",
      "9.1386\t7.7604\t6.7546\t6.1383\t5.8465\t5.8367\t\n",
      "Training Epoch: 1 [218/569]\tLoss: 29.3482\tLR: 0.010000\n",
      "5.8580\t5.2352\t4.7929\t4.5648\t4.4773\t4.4200\t\n",
      "Training Epoch: 1 [234/569]\tLoss: 41.8374\tLR: 0.010000\n",
      "8.5886\t7.6285\t6.7277\t6.2844\t6.1874\t6.4208\t\n",
      "Training Epoch: 1 [250/569]\tLoss: 42.8972\tLR: 0.010000\n",
      "8.7471\t7.7377\t6.9442\t6.5322\t6.4027\t6.5332\t\n",
      "Training Epoch: 1 [266/569]\tLoss: 33.5806\tLR: 0.010000\n",
      "6.2937\t5.6426\t5.1141\t5.1206\t5.4215\t5.9882\t\n",
      "Training Epoch: 1 [282/569]\tLoss: 39.1149\tLR: 0.010000\n",
      "7.5554\t6.7713\t6.3273\t6.1197\t6.1102\t6.2310\t\n",
      "Training Epoch: 1 [298/569]\tLoss: 43.5528\tLR: 0.010000\n",
      "8.6926\t7.6196\t6.9940\t6.7022\t6.6874\t6.8570\t\n",
      "Training Epoch: 1 [314/569]\tLoss: 52.8806\tLR: 0.010000\n",
      "11.4780\t9.9366\t8.7077\t7.9072\t7.4700\t7.3811\t\n",
      "Training Epoch: 1 [330/569]\tLoss: 43.5126\tLR: 0.010000\n",
      "8.3024\t7.6186\t6.9850\t6.7770\t6.8198\t7.0099\t\n",
      "Training Epoch: 1 [346/569]\tLoss: 29.7624\tLR: 0.010000\n",
      "5.4437\t4.8258\t4.5638\t4.6727\t4.9446\t5.3116\t\n",
      "Training Epoch: 1 [362/569]\tLoss: 32.4795\tLR: 0.010000\n",
      "6.3879\t5.6551\t5.2198\t5.0128\t4.9947\t5.2092\t\n",
      "Training Epoch: 1 [378/569]\tLoss: 42.9151\tLR: 0.010000\n",
      "8.2794\t7.3864\t6.7839\t6.5119\t6.7209\t7.2327\t\n",
      "Training Epoch: 1 [394/569]\tLoss: 38.7316\tLR: 0.010000\n",
      "7.4383\t6.6764\t6.0805\t5.8678\t6.0505\t6.6182\t\n",
      "Training Epoch: 1 [410/569]\tLoss: 37.5813\tLR: 0.010000\n",
      "7.2639\t6.3061\t5.8758\t5.8265\t5.9542\t6.3548\t\n",
      "Training Epoch: 1 [426/569]\tLoss: 38.9093\tLR: 0.010000\n",
      "7.4215\t6.6878\t6.2101\t6.0775\t6.1147\t6.3976\t\n",
      "Training Epoch: 1 [442/569]\tLoss: 37.9526\tLR: 0.010000\n",
      "7.9779\t6.9232\t6.0815\t5.6769\t5.6431\t5.6500\t\n",
      "Training Epoch: 1 [458/569]\tLoss: 33.6712\tLR: 0.010000\n",
      "6.5120\t5.7147\t5.2460\t5.1611\t5.3646\t5.6728\t\n",
      "Training Epoch: 1 [474/569]\tLoss: 40.4623\tLR: 0.010000\n",
      "8.6996\t7.5534\t6.6406\t6.0353\t5.7593\t5.7741\t\n",
      "Training Epoch: 1 [490/569]\tLoss: 45.4082\tLR: 0.010000\n",
      "9.2079\t8.0886\t7.1433\t6.7847\t6.8522\t7.3314\t\n",
      "Training Epoch: 1 [506/569]\tLoss: 42.0975\tLR: 0.010000\n",
      "9.3032\t7.9552\t6.8123\t6.1441\t5.8395\t6.0433\t\n",
      "Training Epoch: 1 [522/569]\tLoss: 44.6538\tLR: 0.010000\n",
      "9.1077\t8.0691\t7.1172\t6.7280\t6.6852\t6.9465\t\n",
      "Training Epoch: 1 [538/569]\tLoss: 38.2217\tLR: 0.010000\n",
      "7.3787\t6.5301\t6.0342\t5.8663\t5.9975\t6.4149\t\n",
      "Training Epoch: 1 [554/569]\tLoss: 35.2824\tLR: 0.010000\n",
      "6.8903\t6.1054\t5.5814\t5.4305\t5.5726\t5.7022\t\n",
      "Training Epoch: 1 [570/569]\tLoss: 31.9354\tLR: 0.010000\n",
      "6.3584\t5.6688\t5.1417\t4.9414\t4.9154\t4.9097\t\n",
      "Training Epoch: 1 [586/569]\tLoss: 39.3032\tLR: 0.010000\n",
      "7.7443\t6.9056\t6.2978\t6.0086\t6.0388\t6.3083\t\n",
      "Training Epoch: 1 [602/569]\tLoss: 33.8503\tLR: 0.010000\n",
      "6.9543\t6.0767\t5.4622\t5.1142\t5.0772\t5.1657\t\n",
      "Training Epoch: 1 [618/569]\tLoss: 39.9950\tLR: 0.010000\n",
      "8.0495\t7.0358\t6.3299\t6.0567\t6.1005\t6.4228\t\n",
      "Training Epoch: 1 [634/569]\tLoss: 28.1737\tLR: 0.010000\n",
      "5.3308\t4.7765\t4.3662\t4.3203\t4.4948\t4.8851\t\n",
      "Training Epoch: 1 [650/569]\tLoss: 30.9121\tLR: 0.010000\n",
      "5.7738\t5.2210\t4.9033\t4.8630\t4.9639\t5.1872\t\n",
      "Training Epoch: 1 [666/569]\tLoss: 27.5341\tLR: 0.010000\n",
      "4.9886\t4.5472\t4.2779\t4.2911\t4.5415\t4.8879\t\n",
      "Training Epoch: 1 [682/569]\tLoss: 42.1020\tLR: 0.010000\n",
      "8.8131\t7.6998\t6.8399\t6.3792\t6.1535\t6.2166\t\n",
      "Training Epoch: 1 [698/569]\tLoss: 34.5193\tLR: 0.010000\n",
      "6.5892\t5.7712\t5.3718\t5.3011\t5.5252\t5.9607\t\n",
      "Training Epoch: 1 [714/569]\tLoss: 35.4922\tLR: 0.010000\n",
      "6.9598\t6.1709\t5.5960\t5.4117\t5.5391\t5.8148\t\n",
      "Training Epoch: 1 [730/569]\tLoss: 31.1798\tLR: 0.010000\n",
      "6.1544\t5.4470\t4.9147\t4.7665\t4.8776\t5.0197\t\n",
      "Training Epoch: 1 [746/569]\tLoss: 36.9075\tLR: 0.010000\n",
      "7.6393\t6.6248\t5.8852\t5.5620\t5.5188\t5.6775\t\n",
      "Training Epoch: 1 [762/569]\tLoss: 30.4582\tLR: 0.010000\n",
      "5.7532\t5.2624\t4.9997\t4.8839\t4.8100\t4.7491\t\n",
      "Training Epoch: 1 [778/569]\tLoss: 37.9631\tLR: 0.010000\n",
      "7.3727\t6.7681\t6.2239\t5.9421\t5.8364\t5.8199\t\n",
      "Training Epoch: 1 [794/569]\tLoss: 40.4282\tLR: 0.010000\n",
      "7.7831\t6.9443\t6.4044\t6.2674\t6.3901\t6.6389\t\n",
      "Training Epoch: 1 [810/569]\tLoss: 38.2971\tLR: 0.010000\n",
      "8.6746\t7.1543\t6.1184\t5.5390\t5.3857\t5.4251\t\n",
      "Training Epoch: 1 [826/569]\tLoss: 41.1411\tLR: 0.010000\n",
      "8.2885\t7.4052\t6.7299\t6.3707\t6.1667\t6.1800\t\n",
      "Training Epoch: 1 [842/569]\tLoss: 38.1633\tLR: 0.010000\n",
      "7.6314\t6.8376\t6.1668\t5.8282\t5.7616\t5.9377\t\n",
      "Training Epoch: 1 [858/569]\tLoss: 40.9977\tLR: 0.010000\n",
      "8.0892\t7.3275\t6.7098\t6.3425\t6.2014\t6.3274\t\n",
      "Training Epoch: 1 [874/569]\tLoss: 34.0663\tLR: 0.010000\n",
      "6.8017\t6.0606\t5.4141\t5.1456\t5.1649\t5.4794\t\n",
      "Training Epoch: 1 [890/569]\tLoss: 39.1861\tLR: 0.010000\n",
      "8.4205\t7.2217\t6.2676\t5.7884\t5.6614\t5.8265\t\n",
      "Training Epoch: 1 [905/569]\tLoss: 43.9116\tLR: 0.010000\n",
      "8.7676\t7.8159\t7.0487\t6.7727\t6.7525\t6.7542\t\n",
      "[0.28910985589027405, 0.3898114264011383, 0.32107871770858765, 0.012367469258606434, 0.43855711817741394, 0.24614861607551575, 0.3152942657470703, 0.01449163444340229, 0.3926227390766144, 0.24855291843414307, 0.35882434248924255, 0.006428836844861507, 0.3574124574661255, 0.24998030066490173, 0.3926072418689728, 0.005218259524554014, 0.34130457043647766, 0.0, 0.6586954295635223, 0.008687913417816162]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7688, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6768, Accuracy: 0.0703\n",
      "Test set t = 02: Average loss: 0.6102, Accuracy: 0.0721\n",
      "Test set t = 03: Average loss: 0.5810, Accuracy: 0.0773\n",
      "Test set t = 04: Average loss: 0.5781, Accuracy: 0.0826\n",
      "Test set t = 05: Average loss: 0.5923, Accuracy: 0.0879\n",
      "\n",
      "Training Epoch: 2 [10/569]\tLoss: 35.9099\tLR: 0.010000\n",
      "6.7426\t6.0161\t5.6641\t5.6184\t5.7637\t6.1050\t\n",
      "Training Epoch: 2 [26/569]\tLoss: 42.0126\tLR: 0.010000\n",
      "9.0118\t7.7064\t6.8080\t6.3376\t6.0951\t6.0535\t\n",
      "Training Epoch: 2 [42/569]\tLoss: 37.6913\tLR: 0.010000\n",
      "7.0086\t6.3864\t6.0479\t5.9133\t6.0458\t6.2893\t\n",
      "Training Epoch: 2 [58/569]\tLoss: 33.6024\tLR: 0.010000\n",
      "6.2706\t5.7277\t5.2791\t5.1878\t5.3762\t5.7609\t\n",
      "Training Epoch: 2 [74/569]\tLoss: 41.7404\tLR: 0.010000\n",
      "9.0951\t7.8529\t6.8129\t6.1626\t5.8689\t5.9479\t\n",
      "Training Epoch: 2 [90/569]\tLoss: 36.5715\tLR: 0.010000\n",
      "6.9982\t6.2303\t5.7923\t5.6900\t5.7882\t6.0726\t\n",
      "Training Epoch: 2 [106/569]\tLoss: 45.6468\tLR: 0.010000\n",
      "9.3906\t8.3898\t7.4645\t6.9776\t6.7393\t6.6851\t\n",
      "Training Epoch: 2 [122/569]\tLoss: 43.9629\tLR: 0.010000\n",
      "8.5314\t7.6435\t7.0318\t6.7357\t6.8252\t7.1954\t\n",
      "Training Epoch: 2 [138/569]\tLoss: 29.1780\tLR: 0.010000\n",
      "5.7345\t5.1374\t4.6529\t4.4473\t4.5000\t4.7060\t\n",
      "Training Epoch: 2 [154/569]\tLoss: 32.2077\tLR: 0.010000\n",
      "5.7870\t5.2016\t4.9117\t5.0507\t5.4393\t5.8174\t\n",
      "Training Epoch: 2 [170/569]\tLoss: 31.4552\tLR: 0.010000\n",
      "5.6871\t5.2088\t4.9276\t4.9966\t5.2119\t5.4232\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [186/569]\tLoss: 38.6514\tLR: 0.010000\n",
      "7.6604\t6.7020\t6.1146\t6.0222\t6.0428\t6.1094\t\n",
      "Training Epoch: 2 [202/569]\tLoss: 37.8525\tLR: 0.010000\n",
      "8.0200\t7.0953\t6.2285\t5.6626\t5.4492\t5.3969\t\n",
      "Training Epoch: 2 [218/569]\tLoss: 35.8558\tLR: 0.010000\n",
      "7.6931\t6.5617\t5.7497\t5.3514\t5.2136\t5.2863\t\n",
      "Training Epoch: 2 [234/569]\tLoss: 45.3071\tLR: 0.010000\n",
      "9.7139\t8.3165\t7.3148\t6.8025\t6.5436\t6.6157\t\n",
      "Training Epoch: 2 [250/569]\tLoss: 37.7404\tLR: 0.010000\n",
      "7.1287\t6.3921\t5.9733\t5.8833\t5.9983\t6.3647\t\n",
      "Training Epoch: 2 [266/569]\tLoss: 38.9778\tLR: 0.010000\n",
      "8.1234\t7.1202\t6.3578\t5.8972\t5.7100\t5.7692\t\n",
      "Training Epoch: 2 [282/569]\tLoss: 43.2525\tLR: 0.010000\n",
      "8.2765\t7.5442\t7.0083\t6.7812\t6.7515\t6.8907\t\n",
      "Training Epoch: 2 [298/569]\tLoss: 35.7470\tLR: 0.010000\n",
      "6.4148\t5.6884\t5.4735\t5.5800\t5.9124\t6.6778\t\n",
      "Training Epoch: 2 [314/569]\tLoss: 38.2355\tLR: 0.010000\n",
      "8.2416\t7.0528\t6.1667\t5.6784\t5.5394\t5.5566\t\n",
      "Training Epoch: 2 [330/569]\tLoss: 47.4261\tLR: 0.010000\n",
      "10.4968\t8.8383\t7.5615\t7.0042\t6.7613\t6.7640\t\n",
      "Training Epoch: 2 [346/569]\tLoss: 36.1947\tLR: 0.010000\n",
      "7.6720\t6.5948\t5.8033\t5.4071\t5.3218\t5.3957\t\n",
      "Training Epoch: 2 [362/569]\tLoss: 42.9360\tLR: 0.010000\n",
      "9.1871\t7.9920\t7.0888\t6.5326\t6.1718\t5.9637\t\n",
      "Training Epoch: 2 [378/569]\tLoss: 51.8544\tLR: 0.010000\n",
      "11.3985\t9.6959\t8.4632\t7.7124\t7.3484\t7.2360\t\n",
      "Training Epoch: 2 [394/569]\tLoss: 38.2011\tLR: 0.010000\n",
      "7.3049\t6.7076\t6.2554\t6.0605\t5.9549\t5.9178\t\n",
      "Training Epoch: 2 [410/569]\tLoss: 24.6068\tLR: 0.010000\n",
      "4.6618\t4.0090\t3.6783\t3.7912\t4.0983\t4.3681\t\n",
      "Training Epoch: 2 [426/569]\tLoss: 40.6083\tLR: 0.010000\n",
      "9.1252\t7.6152\t6.6053\t6.0560\t5.7099\t5.4967\t\n",
      "Training Epoch: 2 [442/569]\tLoss: 33.4987\tLR: 0.010000\n",
      "5.8460\t5.4010\t5.2809\t5.3635\t5.5849\t6.0224\t\n",
      "Training Epoch: 2 [458/569]\tLoss: 33.4058\tLR: 0.010000\n",
      "6.8075\t5.9428\t5.3949\t5.1339\t5.0613\t5.0654\t\n",
      "Training Epoch: 2 [474/569]\tLoss: 41.8113\tLR: 0.010000\n",
      "8.3426\t7.2942\t6.7058\t6.4412\t6.4154\t6.6123\t\n",
      "Training Epoch: 2 [490/569]\tLoss: 34.6791\tLR: 0.010000\n",
      "6.7740\t5.9454\t5.5032\t5.3428\t5.4083\t5.7054\t\n",
      "Training Epoch: 2 [506/569]\tLoss: 36.1044\tLR: 0.010000\n",
      "6.7830\t6.0409\t5.6767\t5.6443\t5.8525\t6.1071\t\n",
      "Training Epoch: 2 [522/569]\tLoss: 30.6316\tLR: 0.010000\n",
      "5.9985\t5.1909\t4.8739\t4.8352\t4.8491\t4.8840\t\n",
      "Training Epoch: 2 [538/569]\tLoss: 35.2008\tLR: 0.010000\n",
      "7.0051\t5.9555\t5.2940\t5.3695\t5.6428\t5.9339\t\n",
      "Training Epoch: 2 [554/569]\tLoss: 38.7877\tLR: 0.010000\n",
      "7.9992\t6.9109\t6.1373\t5.7965\t5.8352\t6.1086\t\n",
      "Training Epoch: 2 [570/569]\tLoss: 39.2751\tLR: 0.010000\n",
      "8.2886\t6.9285\t6.1464\t5.8252\t5.8354\t6.2509\t\n",
      "Training Epoch: 2 [586/569]\tLoss: 40.4645\tLR: 0.010000\n",
      "8.4150\t7.1217\t6.3033\t5.9425\t6.1090\t6.5730\t\n",
      "Training Epoch: 2 [602/569]\tLoss: 38.6806\tLR: 0.010000\n",
      "8.2899\t6.8551\t5.8436\t5.7026\t5.8650\t6.1243\t\n",
      "Training Epoch: 2 [618/569]\tLoss: 34.4124\tLR: 0.010000\n",
      "6.4801\t5.9853\t5.6774\t5.4535\t5.3667\t5.4495\t\n",
      "Training Epoch: 2 [634/569]\tLoss: 33.6402\tLR: 0.010000\n",
      "6.2721\t5.7083\t5.3625\t5.2794\t5.3700\t5.6479\t\n",
      "Training Epoch: 2 [650/569]\tLoss: 31.5507\tLR: 0.010000\n",
      "7.1017\t5.6123\t4.7327\t4.5544\t4.6674\t4.8822\t\n",
      "Training Epoch: 2 [666/569]\tLoss: 33.6560\tLR: 0.010000\n",
      "6.2217\t5.6292\t5.2973\t5.3054\t5.4599\t5.7426\t\n",
      "Training Epoch: 2 [682/569]\tLoss: 44.5478\tLR: 0.010000\n",
      "10.8451\t8.6766\t7.1408\t6.3213\t5.8832\t5.6807\t\n",
      "Training Epoch: 2 [698/569]\tLoss: 42.7804\tLR: 0.010000\n",
      "8.9372\t7.5593\t6.7439\t6.4538\t6.4557\t6.6305\t\n",
      "Training Epoch: 2 [714/569]\tLoss: 33.1395\tLR: 0.010000\n",
      "6.3470\t5.5572\t4.9845\t4.9704\t5.3313\t5.9491\t\n",
      "Training Epoch: 2 [730/569]\tLoss: 32.6730\tLR: 0.010000\n",
      "6.1640\t5.4888\t5.1270\t5.0582\t5.2633\t5.5716\t\n",
      "Training Epoch: 2 [746/569]\tLoss: 39.9143\tLR: 0.010000\n",
      "8.0918\t6.9958\t6.2657\t5.9756\t6.0376\t6.5477\t\n",
      "Training Epoch: 2 [762/569]\tLoss: 40.6788\tLR: 0.010000\n",
      "8.4016\t7.1010\t6.3137\t6.1284\t6.2175\t6.5166\t\n",
      "Training Epoch: 2 [778/569]\tLoss: 48.6412\tLR: 0.010000\n",
      "11.0021\t9.3019\t7.9934\t7.1879\t6.7352\t6.4207\t\n",
      "Training Epoch: 2 [794/569]\tLoss: 39.3211\tLR: 0.010000\n",
      "7.9175\t6.8922\t6.2057\t5.9433\t5.9562\t6.4063\t\n",
      "Training Epoch: 2 [810/569]\tLoss: 39.7291\tLR: 0.010000\n",
      "8.9314\t7.4532\t6.3322\t5.7628\t5.5798\t5.6697\t\n",
      "Training Epoch: 2 [826/569]\tLoss: 40.6846\tLR: 0.010000\n",
      "8.3812\t7.2686\t6.4865\t6.1048\t6.0495\t6.3941\t\n",
      "Training Epoch: 2 [842/569]\tLoss: 38.8124\tLR: 0.010000\n",
      "7.2673\t6.4131\t5.9694\t5.9416\t6.2936\t6.9273\t\n",
      "Training Epoch: 2 [858/569]\tLoss: 34.2732\tLR: 0.010000\n",
      "6.9852\t6.0239\t5.3679\t5.1428\t5.2787\t5.4747\t\n",
      "Training Epoch: 2 [874/569]\tLoss: 36.0435\tLR: 0.010000\n",
      "7.6767\t6.2863\t5.6252\t5.3920\t5.4106\t5.6528\t\n",
      "Training Epoch: 2 [890/569]\tLoss: 33.8068\tLR: 0.010000\n",
      "6.6368\t5.8451\t5.3738\t5.2375\t5.2729\t5.4406\t\n",
      "Training Epoch: 2 [905/569]\tLoss: 29.9385\tLR: 0.010000\n",
      "5.7184\t5.1298\t4.8138\t4.7661\t4.7620\t4.7484\t\n",
      "[0.3057307302951813, 0.4584267735481262, 0.2358424961566925, 0.013007071800529957, 0.5867652893066406, 0.18593348562717438, 0.227301225066185, 0.01831960491836071, 0.48983651399612427, 0.22611461579799652, 0.2840488702058792, 0.0030549780931323767, 0.39958035945892334, 0.22362542152404785, 0.3767942190170288, 0.0005314267473295331, 0.3681046664714813, 0.0, 0.6318953335285187, 0.005653613246977329]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7694, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6642, Accuracy: 0.0738\n",
      "Test set t = 02: Average loss: 0.5981, Accuracy: 0.0685\n",
      "Test set t = 03: Average loss: 0.5737, Accuracy: 0.0756\n",
      "Test set t = 04: Average loss: 0.5740, Accuracy: 0.0861\n",
      "Test set t = 05: Average loss: 0.5911, Accuracy: 0.0931\n",
      "\n",
      "Training Epoch: 3 [10/569]\tLoss: 37.6986\tLR: 0.010000\n",
      "7.6674\t6.5661\t5.9669\t5.8040\t5.7948\t5.8996\t\n",
      "Training Epoch: 3 [26/569]\tLoss: 44.6271\tLR: 0.010000\n",
      "9.4794\t8.1365\t7.2441\t6.7436\t6.4961\t6.5274\t\n",
      "Training Epoch: 3 [42/569]\tLoss: 35.5951\tLR: 0.010000\n",
      "6.6646\t5.8767\t5.5594\t5.4808\t5.6757\t6.3379\t\n",
      "Training Epoch: 3 [58/569]\tLoss: 37.0125\tLR: 0.010000\n",
      "7.3535\t6.5233\t5.8659\t5.6128\t5.6763\t5.9807\t\n",
      "Training Epoch: 3 [74/569]\tLoss: 36.5154\tLR: 0.010000\n",
      "7.0073\t6.0679\t5.6168\t5.6218\t5.9299\t6.2716\t\n",
      "Training Epoch: 3 [90/569]\tLoss: 36.5259\tLR: 0.010000\n",
      "8.1266\t6.8939\t5.9086\t5.3506\t5.1645\t5.0815\t\n",
      "Training Epoch: 3 [106/569]\tLoss: 36.4279\tLR: 0.010000\n",
      "6.4991\t5.8691\t5.6308\t5.7414\t6.0649\t6.6226\t\n",
      "Training Epoch: 3 [122/569]\tLoss: 39.2042\tLR: 0.010000\n",
      "7.8609\t6.9983\t6.4532\t6.1170\t5.9177\t5.8571\t\n",
      "Training Epoch: 3 [138/569]\tLoss: 31.6199\tLR: 0.010000\n",
      "5.9306\t5.3438\t4.8950\t4.8460\t5.1066\t5.4979\t\n",
      "Training Epoch: 3 [154/569]\tLoss: 34.5399\tLR: 0.010000\n",
      "6.9640\t6.0745\t5.4897\t5.3394\t5.3369\t5.3355\t\n",
      "Training Epoch: 3 [170/569]\tLoss: 35.9552\tLR: 0.010000\n",
      "7.5209\t6.3743\t5.6983\t5.4675\t5.4215\t5.4727\t\n",
      "Training Epoch: 3 [186/569]\tLoss: 35.8143\tLR: 0.010000\n",
      "6.9591\t6.0059\t5.5304\t5.5327\t5.7540\t6.0321\t\n",
      "Training Epoch: 3 [202/569]\tLoss: 40.6128\tLR: 0.010000\n",
      "8.5513\t7.1834\t6.3202\t6.0513\t6.1420\t6.3646\t\n",
      "Training Epoch: 3 [218/569]\tLoss: 38.8330\tLR: 0.010000\n",
      "7.7801\t6.8164\t6.2121\t5.9443\t5.9310\t6.1491\t\n",
      "Training Epoch: 3 [234/569]\tLoss: 41.6504\tLR: 0.010000\n",
      "9.2330\t7.7579\t6.6801\t6.0494\t5.8539\t6.0761\t\n",
      "Training Epoch: 3 [250/569]\tLoss: 39.0966\tLR: 0.010000\n",
      "7.9976\t6.9581\t6.2289\t5.9563\t5.9618\t5.9938\t\n",
      "Training Epoch: 3 [266/569]\tLoss: 40.7912\tLR: 0.010000\n",
      "8.4609\t7.3489\t6.6081\t6.2150\t6.0502\t6.1081\t\n",
      "Training Epoch: 3 [282/569]\tLoss: 37.2757\tLR: 0.010000\n",
      "7.9440\t6.7471\t5.9707\t5.6567\t5.5135\t5.4438\t\n",
      "Training Epoch: 3 [298/569]\tLoss: 36.7437\tLR: 0.010000\n",
      "7.0325\t6.2665\t5.8849\t5.7274\t5.7688\t6.0635\t\n",
      "Training Epoch: 3 [314/569]\tLoss: 41.7313\tLR: 0.010000\n",
      "9.0716\t7.6636\t6.7686\t6.2526\t6.0120\t5.9629\t\n",
      "Training Epoch: 3 [330/569]\tLoss: 25.5144\tLR: 0.010000\n",
      "4.5527\t4.1796\t4.0020\t4.0750\t4.2620\t4.4429\t\n",
      "Training Epoch: 3 [346/569]\tLoss: 37.3896\tLR: 0.010000\n",
      "7.7013\t6.6717\t5.9391\t5.6576\t5.6691\t5.7508\t\n",
      "Training Epoch: 3 [362/569]\tLoss: 32.6847\tLR: 0.010000\n",
      "6.9270\t5.7845\t5.0532\t4.8008\t4.9279\t5.1912\t\n",
      "Training Epoch: 3 [378/569]\tLoss: 46.9516\tLR: 0.010000\n",
      "9.7564\t8.3403\t7.4894\t7.1894\t7.0638\t7.1123\t\n",
      "Training Epoch: 3 [394/569]\tLoss: 43.3174\tLR: 0.010000\n",
      "8.9387\t7.6562\t6.9748\t6.6214\t6.5227\t6.6035\t\n",
      "Training Epoch: 3 [410/569]\tLoss: 32.9206\tLR: 0.010000\n",
      "6.2002\t5.4008\t5.0368\t5.1740\t5.4491\t5.6596\t\n",
      "Training Epoch: 3 [426/569]\tLoss: 36.3652\tLR: 0.010000\n",
      "7.3548\t6.2888\t5.7101\t5.5534\t5.6043\t5.8537\t\n",
      "Training Epoch: 3 [442/569]\tLoss: 38.1396\tLR: 0.010000\n",
      "7.6189\t6.6027\t6.0865\t5.9234\t5.8673\t6.0409\t\n",
      "Training Epoch: 3 [458/569]\tLoss: 30.5980\tLR: 0.010000\n",
      "5.6168\t5.0239\t4.8010\t4.8245\t5.0143\t5.3175\t\n",
      "Training Epoch: 3 [474/569]\tLoss: 37.9556\tLR: 0.010000\n",
      "7.1162\t6.2987\t5.9874\t6.0287\t6.1746\t6.3500\t\n",
      "Training Epoch: 3 [490/569]\tLoss: 33.5926\tLR: 0.010000\n",
      "6.2109\t5.7384\t5.4482\t5.3521\t5.3803\t5.4626\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [506/569]\tLoss: 40.1153\tLR: 0.010000\n",
      "8.5041\t7.1349\t6.2472\t6.0206\t6.0463\t6.1622\t\n",
      "Training Epoch: 3 [522/569]\tLoss: 35.7960\tLR: 0.010000\n",
      "7.5709\t6.3763\t5.7088\t5.3934\t5.3031\t5.4435\t\n",
      "Training Epoch: 3 [538/569]\tLoss: 37.0211\tLR: 0.010000\n",
      "7.4690\t6.4636\t5.9104\t5.6648\t5.6462\t5.8671\t\n",
      "Training Epoch: 3 [554/569]\tLoss: 37.2683\tLR: 0.010000\n",
      "7.2708\t6.4754\t6.0029\t5.8063\t5.7235\t5.9893\t\n",
      "Training Epoch: 3 [570/569]\tLoss: 37.3490\tLR: 0.010000\n",
      "7.6036\t6.6131\t5.9597\t5.7531\t5.6884\t5.7312\t\n",
      "Training Epoch: 3 [586/569]\tLoss: 43.9236\tLR: 0.010000\n",
      "10.1041\t8.2311\t7.0243\t6.4599\t6.1257\t5.9784\t\n",
      "Training Epoch: 3 [602/569]\tLoss: 35.7644\tLR: 0.010000\n",
      "7.8766\t6.4375\t5.5094\t5.1816\t5.2107\t5.5486\t\n",
      "Training Epoch: 3 [618/569]\tLoss: 38.6379\tLR: 0.010000\n",
      "7.9913\t6.8449\t6.1577\t5.8452\t5.8334\t5.9655\t\n",
      "Training Epoch: 3 [634/569]\tLoss: 36.8855\tLR: 0.010000\n",
      "7.1904\t6.2535\t5.7877\t5.7291\t5.8685\t6.0563\t\n",
      "Training Epoch: 3 [650/569]\tLoss: 39.2384\tLR: 0.010000\n",
      "9.3288\t7.4488\t6.2693\t5.6711\t5.4098\t5.1106\t\n",
      "Training Epoch: 3 [666/569]\tLoss: 32.5936\tLR: 0.010000\n",
      "6.0899\t5.3097\t5.0633\t5.1536\t5.3092\t5.6679\t\n",
      "Training Epoch: 3 [682/569]\tLoss: 43.5146\tLR: 0.010000\n",
      "9.6993\t8.1130\t6.9850\t6.4498\t6.1735\t6.0938\t\n",
      "Training Epoch: 3 [698/569]\tLoss: 35.3474\tLR: 0.010000\n",
      "7.0127\t6.0832\t5.6084\t5.4235\t5.5057\t5.7139\t\n",
      "Training Epoch: 3 [714/569]\tLoss: 44.5315\tLR: 0.010000\n",
      "9.1185\t7.8735\t6.9968\t6.6393\t6.7101\t7.1933\t\n",
      "Training Epoch: 3 [730/569]\tLoss: 33.6058\tLR: 0.010000\n",
      "6.7245\t5.8673\t5.3373\t5.2459\t5.2881\t5.1426\t\n",
      "Training Epoch: 3 [746/569]\tLoss: 33.5557\tLR: 0.010000\n",
      "6.0896\t5.3862\t5.1734\t5.3276\t5.5880\t5.9908\t\n",
      "Training Epoch: 3 [762/569]\tLoss: 36.1243\tLR: 0.010000\n",
      "6.7207\t6.1219\t5.8656\t5.7308\t5.7431\t5.9422\t\n",
      "Training Epoch: 3 [778/569]\tLoss: 42.0523\tLR: 0.010000\n",
      "8.8391\t7.5312\t6.7076\t6.2985\t6.2445\t6.4315\t\n",
      "Training Epoch: 3 [794/569]\tLoss: 32.7091\tLR: 0.010000\n",
      "6.2022\t5.3340\t4.9665\t5.0480\t5.3062\t5.8522\t\n",
      "Training Epoch: 3 [810/569]\tLoss: 35.8191\tLR: 0.010000\n",
      "7.2597\t6.2026\t5.6794\t5.5271\t5.5386\t5.6117\t\n",
      "Training Epoch: 3 [826/569]\tLoss: 37.0062\tLR: 0.010000\n",
      "8.4415\t6.8991\t5.8197\t5.3428\t5.2099\t5.2933\t\n",
      "Training Epoch: 3 [842/569]\tLoss: 42.5582\tLR: 0.010000\n",
      "8.9416\t7.6362\t6.6889\t6.3114\t6.3569\t6.6231\t\n",
      "Training Epoch: 3 [858/569]\tLoss: 36.7419\tLR: 0.010000\n",
      "8.1711\t6.6122\t5.7526\t5.4088\t5.3711\t5.4260\t\n",
      "Training Epoch: 3 [874/569]\tLoss: 40.6910\tLR: 0.010000\n",
      "9.0838\t7.5497\t6.4188\t5.9551\t5.8604\t5.8233\t\n",
      "Training Epoch: 3 [890/569]\tLoss: 36.5035\tLR: 0.010000\n",
      "7.8625\t6.5921\t5.7531\t5.4173\t5.3782\t5.5002\t\n",
      "Training Epoch: 3 [905/569]\tLoss: 40.5271\tLR: 0.010000\n",
      "8.2975\t7.1136\t6.4432\t6.2373\t6.1825\t6.2530\t\n",
      "[0.32998836040496826, 0.47569137811660767, 0.19432026147842407, 0.01127623300999403, 0.6762880682945251, 0.137046679854393, 0.18666525185108185, 0.021533513441681862, 0.5522409081459045, 0.22544662654399872, 0.22231246531009674, 0.0004766746424138546, 0.42429226636886597, 0.21681024134159088, 0.35889749228954315, -0.0037634135223925114, 0.38703417778015137, 0.0, 0.6129658222198486, 0.003010339569300413]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7690, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6550, Accuracy: 0.0738\n",
      "Test set t = 02: Average loss: 0.5911, Accuracy: 0.0703\n",
      "Test set t = 03: Average loss: 0.5707, Accuracy: 0.0808\n",
      "Test set t = 04: Average loss: 0.5735, Accuracy: 0.0861\n",
      "Test set t = 05: Average loss: 0.5925, Accuracy: 0.0914\n",
      "\n",
      "Training Epoch: 4 [10/569]\tLoss: 40.7615\tLR: 0.010000\n",
      "8.1029\t6.9335\t6.3329\t6.1932\t6.3906\t6.8084\t\n",
      "Training Epoch: 4 [26/569]\tLoss: 42.4921\tLR: 0.010000\n",
      "9.4740\t8.0236\t6.9426\t6.2553\t5.9165\t5.8802\t\n",
      "Training Epoch: 4 [42/569]\tLoss: 27.5135\tLR: 0.010000\n",
      "4.2941\t4.1117\t4.2277\t4.5900\t4.9883\t5.3017\t\n",
      "Training Epoch: 4 [58/569]\tLoss: 38.8198\tLR: 0.010000\n",
      "8.0957\t6.8907\t6.1138\t5.8536\t5.8169\t6.0491\t\n",
      "Training Epoch: 4 [74/569]\tLoss: 36.1035\tLR: 0.010000\n",
      "7.3246\t6.1653\t5.6343\t5.5902\t5.6488\t5.7402\t\n",
      "Training Epoch: 4 [90/569]\tLoss: 43.0943\tLR: 0.010000\n",
      "9.0476\t7.7979\t6.9311\t6.5079\t6.3941\t6.4156\t\n",
      "Training Epoch: 4 [106/569]\tLoss: 39.2923\tLR: 0.010000\n",
      "7.6802\t6.5151\t6.0066\t5.8917\t6.2266\t6.9720\t\n",
      "Training Epoch: 4 [122/569]\tLoss: 33.6002\tLR: 0.010000\n",
      "7.2759\t5.9759\t5.1875\t4.9972\t5.0766\t5.0869\t\n",
      "Training Epoch: 4 [138/569]\tLoss: 40.5834\tLR: 0.010000\n",
      "8.6197\t7.1649\t6.2031\t5.8024\t5.9974\t6.7960\t\n",
      "Training Epoch: 4 [154/569]\tLoss: 39.9880\tLR: 0.010000\n",
      "8.1293\t7.0489\t6.3929\t6.1860\t6.0823\t6.1486\t\n",
      "Training Epoch: 4 [170/569]\tLoss: 36.2112\tLR: 0.010000\n",
      "7.6688\t6.4352\t5.7042\t5.4330\t5.4230\t5.5471\t\n",
      "Training Epoch: 4 [186/569]\tLoss: 41.0965\tLR: 0.010000\n",
      "8.3546\t7.0674\t6.4108\t6.2084\t6.3347\t6.7206\t\n",
      "Training Epoch: 4 [202/569]\tLoss: 37.0589\tLR: 0.010000\n",
      "7.6613\t6.5597\t5.8797\t5.6568\t5.6184\t5.6830\t\n",
      "Training Epoch: 4 [218/569]\tLoss: 42.5475\tLR: 0.010000\n",
      "8.8960\t7.6831\t6.8625\t6.4479\t6.2607\t6.3974\t\n",
      "Training Epoch: 4 [234/569]\tLoss: 31.9444\tLR: 0.010000\n",
      "6.5117\t5.5979\t5.0711\t4.8785\t4.8990\t4.9863\t\n",
      "Training Epoch: 4 [250/569]\tLoss: 43.8281\tLR: 0.010000\n",
      "9.9836\t8.1760\t6.9329\t6.3998\t6.1701\t6.1657\t\n",
      "Training Epoch: 4 [266/569]\tLoss: 32.1225\tLR: 0.010000\n",
      "5.4481\t4.9550\t4.9259\t5.1592\t5.5604\t6.0739\t\n",
      "Training Epoch: 4 [282/569]\tLoss: 40.3461\tLR: 0.010000\n",
      "8.8175\t7.4143\t6.4856\t5.9442\t5.8256\t5.8587\t\n",
      "Training Epoch: 4 [298/569]\tLoss: 38.6570\tLR: 0.010000\n",
      "7.8675\t6.8451\t6.2544\t6.0092\t5.8624\t5.8184\t\n",
      "Training Epoch: 4 [314/569]\tLoss: 40.0745\tLR: 0.010000\n",
      "8.6901\t7.0812\t6.2575\t5.9273\t5.9194\t6.1990\t\n",
      "Training Epoch: 4 [330/569]\tLoss: 36.5735\tLR: 0.010000\n",
      "6.9932\t6.0823\t5.7075\t5.6862\t5.8925\t6.2119\t\n",
      "Training Epoch: 4 [346/569]\tLoss: 36.6053\tLR: 0.010000\n",
      "7.6960\t6.3428\t5.6172\t5.4919\t5.5664\t5.8909\t\n",
      "Training Epoch: 4 [362/569]\tLoss: 41.5236\tLR: 0.010000\n",
      "8.0115\t6.9338\t6.4631\t6.3560\t6.5883\t7.1709\t\n",
      "Training Epoch: 4 [378/569]\tLoss: 37.2614\tLR: 0.010000\n",
      "7.5312\t6.5190\t5.9009\t5.7199\t5.7178\t5.8727\t\n",
      "Training Epoch: 4 [394/569]\tLoss: 36.0114\tLR: 0.010000\n",
      "7.5460\t6.3145\t5.6615\t5.4321\t5.4304\t5.6270\t\n",
      "Training Epoch: 4 [410/569]\tLoss: 38.3362\tLR: 0.010000\n",
      "8.1468\t6.7425\t6.0750\t5.8395\t5.7393\t5.7930\t\n",
      "Training Epoch: 4 [426/569]\tLoss: 26.4156\tLR: 0.010000\n",
      "4.9599\t4.3621\t4.0664\t4.0887\t4.2302\t4.7082\t\n",
      "Training Epoch: 4 [442/569]\tLoss: 41.0017\tLR: 0.010000\n",
      "8.6064\t7.3362\t6.5564\t6.2194\t6.1031\t6.1801\t\n",
      "Training Epoch: 4 [458/569]\tLoss: 33.2493\tLR: 0.010000\n",
      "7.0069\t5.7894\t5.1100\t4.9835\t5.1055\t5.2541\t\n",
      "Training Epoch: 4 [474/569]\tLoss: 41.1586\tLR: 0.010000\n",
      "8.2005\t7.1791\t6.5409\t6.3495\t6.3819\t6.5067\t\n",
      "Training Epoch: 4 [490/569]\tLoss: 32.9636\tLR: 0.010000\n",
      "6.0354\t5.2951\t5.0640\t5.2148\t5.5074\t5.8469\t\n",
      "Training Epoch: 4 [506/569]\tLoss: 40.9676\tLR: 0.010000\n",
      "9.3297\t7.5963\t6.3937\t5.9645\t5.8539\t5.8294\t\n",
      "Training Epoch: 4 [522/569]\tLoss: 39.5841\tLR: 0.010000\n",
      "8.2615\t6.9860\t6.2667\t6.0277\t5.9755\t6.0668\t\n",
      "Training Epoch: 4 [538/569]\tLoss: 35.9335\tLR: 0.010000\n",
      "7.7124\t6.3603\t5.6564\t5.3790\t5.3665\t5.4588\t\n",
      "Training Epoch: 4 [554/569]\tLoss: 41.9930\tLR: 0.010000\n",
      "9.3981\t7.7806\t6.7052\t6.1618\t6.0023\t5.9450\t\n",
      "Training Epoch: 4 [570/569]\tLoss: 33.0387\tLR: 0.010000\n",
      "6.4724\t5.6323\t5.2024\t5.1825\t5.2522\t5.2971\t\n",
      "Training Epoch: 4 [586/569]\tLoss: 33.7992\tLR: 0.010000\n",
      "6.4980\t5.6500\t5.2609\t5.2625\t5.4504\t5.6773\t\n",
      "Training Epoch: 4 [602/569]\tLoss: 34.9003\tLR: 0.010000\n",
      "6.2892\t5.6384\t5.4939\t5.6193\t5.8271\t6.0324\t\n",
      "Training Epoch: 4 [618/569]\tLoss: 40.1270\tLR: 0.010000\n",
      "8.8901\t7.1871\t6.2148\t5.8564\t5.8819\t6.0966\t\n",
      "Training Epoch: 4 [634/569]\tLoss: 32.2317\tLR: 0.010000\n",
      "6.5859\t5.5296\t4.9962\t4.8579\t4.9452\t5.3169\t\n",
      "Training Epoch: 4 [650/569]\tLoss: 28.5351\tLR: 0.010000\n",
      "5.8066\t5.0284\t4.6158\t4.4602\t4.3767\t4.2473\t\n",
      "Training Epoch: 4 [666/569]\tLoss: 37.9048\tLR: 0.010000\n",
      "7.7693\t6.5236\t5.9225\t5.7856\t5.8941\t6.0096\t\n",
      "Training Epoch: 4 [682/569]\tLoss: 39.7575\tLR: 0.010000\n",
      "8.2482\t6.8767\t6.2713\t6.1503\t6.1001\t6.1109\t\n",
      "Training Epoch: 4 [698/569]\tLoss: 33.9266\tLR: 0.010000\n",
      "6.5056\t5.6609\t5.3173\t5.3307\t5.4462\t5.6659\t\n",
      "Training Epoch: 4 [714/569]\tLoss: 42.8212\tLR: 0.010000\n",
      "8.9305\t7.5312\t6.7753\t6.4755\t6.4811\t6.6274\t\n",
      "Training Epoch: 4 [730/569]\tLoss: 38.4511\tLR: 0.010000\n",
      "7.9140\t6.8694\t6.1834\t5.8307\t5.7701\t5.8835\t\n",
      "Training Epoch: 4 [746/569]\tLoss: 38.3353\tLR: 0.010000\n",
      "7.5618\t6.4995\t6.0151\t5.9381\t6.0033\t6.3174\t\n",
      "Training Epoch: 4 [762/569]\tLoss: 33.6927\tLR: 0.010000\n",
      "6.6336\t5.8113\t5.3367\t5.2316\t5.2299\t5.4496\t\n",
      "Training Epoch: 4 [778/569]\tLoss: 42.5826\tLR: 0.010000\n",
      "9.5335\t7.9435\t6.8287\t6.3125\t6.0551\t5.9094\t\n",
      "Training Epoch: 4 [794/569]\tLoss: 40.1448\tLR: 0.010000\n",
      "8.2580\t6.9750\t6.4207\t6.2632\t6.1804\t6.0475\t\n",
      "Training Epoch: 4 [810/569]\tLoss: 35.2745\tLR: 0.010000\n",
      "7.1189\t6.1158\t5.5443\t5.3815\t5.4327\t5.6811\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [826/569]\tLoss: 37.6606\tLR: 0.010000\n",
      "8.4716\t6.8318\t5.8769\t5.5291\t5.4852\t5.4660\t\n",
      "Training Epoch: 4 [842/569]\tLoss: 34.8969\tLR: 0.010000\n",
      "6.8449\t5.7875\t5.4395\t5.4526\t5.5598\t5.8126\t\n",
      "Training Epoch: 4 [858/569]\tLoss: 36.6480\tLR: 0.010000\n",
      "7.1494\t6.2969\t5.8826\t5.6971\t5.7410\t5.8809\t\n",
      "Training Epoch: 4 [874/569]\tLoss: 42.6253\tLR: 0.010000\n",
      "8.7744\t7.5262\t6.8407\t6.5543\t6.4198\t6.5099\t\n",
      "Training Epoch: 4 [890/569]\tLoss: 30.1223\tLR: 0.010000\n",
      "6.1244\t5.2035\t4.7131\t4.5646\t4.6610\t4.8558\t\n",
      "Training Epoch: 4 [905/569]\tLoss: 38.7426\tLR: 0.010000\n",
      "7.7480\t6.7662\t6.3221\t6.1668\t5.9851\t5.7544\t\n",
      "[0.3508007526397705, 0.4626449942588806, 0.18655425310134888, 0.009625709615647793, 0.7214038968086243, 0.10068801790475845, 0.17790808528661728, 0.02465018443763256, 0.5895542502403259, 0.23159539699554443, 0.17885035276412964, -0.0021783323027193546, 0.45389658212661743, 0.21364900469779968, 0.3324544131755829, -0.007944345474243164, 0.3981819450855255, 0.0, 0.6018180549144745, -0.00028762445435859263]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7691, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6503, Accuracy: 0.0738\n",
      "Test set t = 02: Average loss: 0.5882, Accuracy: 0.0685\n",
      "Test set t = 03: Average loss: 0.5702, Accuracy: 0.0808\n",
      "Test set t = 04: Average loss: 0.5733, Accuracy: 0.0826\n",
      "Test set t = 05: Average loss: 0.5902, Accuracy: 0.0896\n",
      "\n",
      "Training Epoch: 5 [10/569]\tLoss: 42.7681\tLR: 0.010000\n",
      "9.3896\t7.6850\t6.7822\t6.4025\t6.2495\t6.2593\t\n",
      "Training Epoch: 5 [26/569]\tLoss: 36.5852\tLR: 0.010000\n",
      "6.9665\t6.0954\t5.7356\t5.7737\t5.9126\t6.1014\t\n",
      "Training Epoch: 5 [42/569]\tLoss: 39.0555\tLR: 0.010000\n",
      "8.0132\t6.9960\t6.2972\t5.9365\t5.7837\t6.0289\t\n",
      "Training Epoch: 5 [58/569]\tLoss: 36.2137\tLR: 0.010000\n",
      "7.3508\t6.0128\t5.5429\t5.5794\t5.7767\t5.9511\t\n",
      "Training Epoch: 5 [74/569]\tLoss: 37.3728\tLR: 0.010000\n",
      "8.0085\t6.6400\t5.8243\t5.6176\t5.5916\t5.6907\t\n",
      "Training Epoch: 5 [90/569]\tLoss: 33.3824\tLR: 0.010000\n",
      "6.5559\t5.6019\t5.0475\t4.9723\t5.3218\t5.8830\t\n",
      "Training Epoch: 5 [106/569]\tLoss: 41.5309\tLR: 0.010000\n",
      "8.3345\t7.1280\t6.5123\t6.3778\t6.4560\t6.7222\t\n",
      "Training Epoch: 5 [122/569]\tLoss: 30.6638\tLR: 0.010000\n",
      "5.5165\t4.7851\t4.6418\t4.8872\t5.2364\t5.5968\t\n",
      "Training Epoch: 5 [138/569]\tLoss: 41.1024\tLR: 0.010000\n",
      "8.7661\t7.2561\t6.3961\t6.0465\t6.1317\t6.5058\t\n",
      "Training Epoch: 5 [154/569]\tLoss: 34.0441\tLR: 0.010000\n",
      "5.8269\t5.3999\t5.3734\t5.5543\t5.7441\t6.1455\t\n",
      "Training Epoch: 5 [170/569]\tLoss: 41.1006\tLR: 0.010000\n",
      "8.6164\t7.1869\t6.4435\t6.2450\t6.2429\t6.3659\t\n",
      "Training Epoch: 5 [186/569]\tLoss: 36.7737\tLR: 0.010000\n",
      "7.4277\t6.3217\t5.8484\t5.7381\t5.7057\t5.7320\t\n",
      "Training Epoch: 5 [202/569]\tLoss: 29.7219\tLR: 0.010000\n",
      "5.5158\t4.9957\t4.7547\t4.7004\t4.7839\t4.9714\t\n",
      "Training Epoch: 5 [218/569]\tLoss: 35.5157\tLR: 0.010000\n",
      "7.5067\t6.2921\t5.5955\t5.3654\t5.3453\t5.4107\t\n",
      "Training Epoch: 5 [234/569]\tLoss: 36.2099\tLR: 0.010000\n",
      "7.6653\t6.3309\t5.6774\t5.4994\t5.5144\t5.5225\t\n",
      "Training Epoch: 5 [250/569]\tLoss: 36.4616\tLR: 0.010000\n",
      "7.0777\t6.0361\t5.7133\t5.8124\t5.8989\t5.9232\t\n",
      "Training Epoch: 5 [266/569]\tLoss: 40.3150\tLR: 0.010000\n",
      "8.8633\t7.2734\t6.3460\t5.9100\t5.8676\t6.0547\t\n",
      "Training Epoch: 5 [282/569]\tLoss: 36.8722\tLR: 0.010000\n",
      "7.0185\t6.2116\t5.8803\t5.8073\t5.8721\t6.0824\t\n",
      "Training Epoch: 5 [298/569]\tLoss: 37.7538\tLR: 0.010000\n",
      "8.5034\t6.7379\t5.7589\t5.4753\t5.5041\t5.7742\t\n",
      "Training Epoch: 5 [314/569]\tLoss: 34.5605\tLR: 0.010000\n",
      "7.5342\t6.1645\t5.4325\t5.1957\t5.1471\t5.0866\t\n",
      "Training Epoch: 5 [330/569]\tLoss: 40.4906\tLR: 0.010000\n",
      "8.4216\t7.1839\t6.3956\t6.1062\t6.0611\t6.3222\t\n",
      "Training Epoch: 5 [346/569]\tLoss: 32.5755\tLR: 0.010000\n",
      "6.4921\t5.6502\t5.1984\t5.0114\t5.0328\t5.1907\t\n",
      "Training Epoch: 5 [362/569]\tLoss: 37.8258\tLR: 0.010000\n",
      "7.8262\t6.6088\t5.9659\t5.7454\t5.7974\t5.8820\t\n",
      "Training Epoch: 5 [378/569]\tLoss: 38.2749\tLR: 0.010000\n",
      "7.7315\t6.5457\t5.8819\t5.7023\t6.0008\t6.4127\t\n",
      "Training Epoch: 5 [394/569]\tLoss: 36.0870\tLR: 0.010000\n",
      "7.5062\t6.3612\t5.6465\t5.4052\t5.4954\t5.6725\t\n",
      "Training Epoch: 5 [410/569]\tLoss: 36.6499\tLR: 0.010000\n",
      "8.0148\t6.4266\t5.5865\t5.4085\t5.4692\t5.7443\t\n",
      "Training Epoch: 5 [426/569]\tLoss: 38.3696\tLR: 0.010000\n",
      "8.2000\t6.7692\t6.0391\t5.8193\t5.7586\t5.7834\t\n",
      "Training Epoch: 5 [442/569]\tLoss: 30.8344\tLR: 0.010000\n",
      "6.0578\t5.0634\t4.6993\t4.7217\t5.0183\t5.2739\t\n",
      "Training Epoch: 5 [458/569]\tLoss: 35.3296\tLR: 0.010000\n",
      "7.6244\t6.2430\t5.4731\t5.2698\t5.2874\t5.4318\t\n",
      "Training Epoch: 5 [474/569]\tLoss: 38.2052\tLR: 0.010000\n",
      "8.1678\t6.8545\t6.1664\t5.8336\t5.6416\t5.5412\t\n",
      "Training Epoch: 5 [490/569]\tLoss: 32.7532\tLR: 0.010000\n",
      "6.4547\t5.5446\t5.2420\t5.2062\t5.1885\t5.1172\t\n",
      "Training Epoch: 5 [506/569]\tLoss: 42.1006\tLR: 0.010000\n",
      "8.5174\t7.2212\t6.5729\t6.3631\t6.4819\t6.9440\t\n",
      "Training Epoch: 5 [522/569]\tLoss: 40.3656\tLR: 0.010000\n",
      "7.9431\t6.9364\t6.3994\t6.2358\t6.3152\t6.5357\t\n",
      "Training Epoch: 5 [538/569]\tLoss: 35.3634\tLR: 0.010000\n",
      "6.7023\t5.8907\t5.5906\t5.5597\t5.6846\t5.9355\t\n",
      "Training Epoch: 5 [554/569]\tLoss: 32.5421\tLR: 0.010000\n",
      "7.1044\t5.9513\t5.2736\t4.9563\t4.7603\t4.4962\t\n",
      "Training Epoch: 5 [570/569]\tLoss: 37.4673\tLR: 0.010000\n",
      "7.6938\t6.5998\t5.9860\t5.7403\t5.6552\t5.7923\t\n",
      "Training Epoch: 5 [586/569]\tLoss: 34.2861\tLR: 0.010000\n",
      "6.9772\t5.6784\t5.1756\t5.2403\t5.4891\t5.7256\t\n",
      "Training Epoch: 5 [602/569]\tLoss: 33.4947\tLR: 0.010000\n",
      "6.0516\t5.4276\t5.2584\t5.3593\t5.5476\t5.8502\t\n",
      "Training Epoch: 5 [618/569]\tLoss: 37.4154\tLR: 0.010000\n",
      "7.6552\t6.5668\t6.0100\t5.7284\t5.6239\t5.8311\t\n",
      "Training Epoch: 5 [634/569]\tLoss: 35.6882\tLR: 0.010000\n",
      "7.1113\t6.1922\t5.6935\t5.5386\t5.5436\t5.6090\t\n",
      "Training Epoch: 5 [650/569]\tLoss: 39.5594\tLR: 0.010000\n",
      "8.5824\t7.0992\t6.2658\t5.8866\t5.7725\t5.9529\t\n",
      "Training Epoch: 5 [666/569]\tLoss: 42.5085\tLR: 0.010000\n",
      "9.4136\t7.7944\t6.9063\t6.4193\t6.0816\t5.8932\t\n",
      "Training Epoch: 5 [682/569]\tLoss: 32.6852\tLR: 0.010000\n",
      "6.3498\t5.4732\t5.1263\t5.1680\t5.2573\t5.3104\t\n",
      "Training Epoch: 5 [698/569]\tLoss: 37.3822\tLR: 0.010000\n",
      "7.9752\t6.4693\t5.7695\t5.6682\t5.6985\t5.8016\t\n",
      "Training Epoch: 5 [714/569]\tLoss: 32.9369\tLR: 0.010000\n",
      "6.3547\t5.5618\t5.2002\t5.1709\t5.2350\t5.4144\t\n",
      "Training Epoch: 5 [730/569]\tLoss: 32.7955\tLR: 0.010000\n",
      "5.9831\t5.3901\t5.1743\t5.2289\t5.4013\t5.6177\t\n",
      "Training Epoch: 5 [746/569]\tLoss: 42.6287\tLR: 0.010000\n",
      "9.2178\t7.5580\t6.7141\t6.4420\t6.3359\t6.3609\t\n",
      "Training Epoch: 5 [762/569]\tLoss: 40.3291\tLR: 0.010000\n",
      "9.3366\t7.4287\t6.2865\t5.8275\t5.6889\t5.7608\t\n",
      "Training Epoch: 5 [778/569]\tLoss: 38.8389\tLR: 0.010000\n",
      "8.0687\t6.5942\t5.9766\t5.8644\t6.0099\t6.3251\t\n",
      "Training Epoch: 5 [794/569]\tLoss: 39.5815\tLR: 0.010000\n",
      "7.5386\t6.6428\t6.3279\t6.2602\t6.3182\t6.4939\t\n",
      "Training Epoch: 5 [810/569]\tLoss: 43.8895\tLR: 0.010000\n",
      "9.7223\t8.0289\t6.8813\t6.4843\t6.4025\t6.3703\t\n",
      "Training Epoch: 5 [826/569]\tLoss: 36.3480\tLR: 0.010000\n",
      "7.1995\t6.1047\t5.5881\t5.5336\t5.7646\t6.1575\t\n",
      "Training Epoch: 5 [842/569]\tLoss: 38.3867\tLR: 0.010000\n",
      "8.1800\t6.8015\t6.0254\t5.7219\t5.7520\t5.9058\t\n",
      "Training Epoch: 5 [858/569]\tLoss: 39.6659\tLR: 0.010000\n",
      "8.1778\t6.8222\t6.1645\t5.9914\t6.0815\t6.4285\t\n",
      "Training Epoch: 5 [874/569]\tLoss: 47.2546\tLR: 0.010000\n",
      "10.7700\t8.9362\t7.6835\t6.9137\t6.4997\t6.4514\t\n",
      "Training Epoch: 5 [890/569]\tLoss: 36.5527\tLR: 0.010000\n",
      "6.9828\t5.9809\t5.5894\t5.6596\t5.9747\t6.3654\t\n",
      "Training Epoch: 5 [905/569]\tLoss: 41.3026\tLR: 0.010000\n",
      "9.0755\t7.3625\t6.5266\t6.2266\t6.0932\t6.0182\t\n",
      "[0.3628671169281006, 0.44713348150253296, 0.18999940156936646, 0.00813122559338808, 0.7439544796943665, 0.07673675566911697, 0.17930876463651657, 0.027720382437109947, 0.6066089868545532, 0.2418193221092224, 0.15157169103622437, -0.004272973630577326, 0.4713480770587921, 0.22364242374897003, 0.30500949919223785, -0.01154988631606102, 0.41079002618789673, 0.0, 0.5892099738121033, -0.0033092207740992308]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7689, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6446, Accuracy: 0.0738\n",
      "Test set t = 02: Average loss: 0.5849, Accuracy: 0.0721\n",
      "Test set t = 03: Average loss: 0.5697, Accuracy: 0.0844\n",
      "Test set t = 04: Average loss: 0.5740, Accuracy: 0.0808\n",
      "Test set t = 05: Average loss: 0.5900, Accuracy: 0.0914\n",
      "\n",
      "Training Epoch: 6 [10/569]\tLoss: 36.1585\tLR: 0.010000\n",
      "7.7798\t6.4536\t5.7315\t5.4518\t5.3455\t5.3962\t\n",
      "Training Epoch: 6 [26/569]\tLoss: 42.9946\tLR: 0.010000\n",
      "9.4652\t7.9207\t7.0864\t6.5971\t6.1482\t5.7770\t\n",
      "Training Epoch: 6 [42/569]\tLoss: 39.1544\tLR: 0.010000\n",
      "8.2275\t6.8944\t6.2064\t5.9502\t5.8828\t5.9932\t\n",
      "Training Epoch: 6 [58/569]\tLoss: 35.9863\tLR: 0.010000\n",
      "7.4079\t6.1766\t5.5627\t5.4154\t5.5743\t5.8495\t\n",
      "Training Epoch: 6 [74/569]\tLoss: 38.5711\tLR: 0.010000\n",
      "7.8104\t6.7661\t6.1400\t5.9053\t5.8801\t6.0692\t\n",
      "Training Epoch: 6 [90/569]\tLoss: 35.0708\tLR: 0.010000\n",
      "7.5236\t6.1471\t5.5286\t5.4034\t5.3219\t5.1462\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [106/569]\tLoss: 33.1681\tLR: 0.010000\n",
      "7.3101\t5.7907\t5.1056\t4.8883\t4.9341\t5.1392\t\n",
      "Training Epoch: 6 [122/569]\tLoss: 31.7400\tLR: 0.010000\n",
      "6.3344\t5.0996\t4.6978\t4.8389\t5.1992\t5.5702\t\n",
      "Training Epoch: 6 [138/569]\tLoss: 34.7865\tLR: 0.010000\n",
      "7.4736\t5.9984\t5.3213\t5.1523\t5.2669\t5.5740\t\n",
      "Training Epoch: 6 [154/569]\tLoss: 29.7259\tLR: 0.010000\n",
      "5.7450\t4.9634\t4.8234\t4.8833\t4.7667\t4.5441\t\n",
      "Training Epoch: 6 [170/569]\tLoss: 35.5789\tLR: 0.010000\n",
      "7.5700\t6.2530\t5.5968\t5.3752\t5.3745\t5.4093\t\n",
      "Training Epoch: 6 [186/569]\tLoss: 33.5562\tLR: 0.010000\n",
      "6.2406\t5.3648\t5.0949\t5.2507\t5.5838\t6.0213\t\n",
      "Training Epoch: 6 [202/569]\tLoss: 29.1642\tLR: 0.010000\n",
      "5.3156\t4.7335\t4.5448\t4.6533\t4.8476\t5.0693\t\n",
      "Training Epoch: 6 [218/569]\tLoss: 30.1353\tLR: 0.010000\n",
      "5.0377\t4.4477\t4.4730\t4.8632\t5.3538\t5.9599\t\n",
      "Training Epoch: 6 [234/569]\tLoss: 37.2577\tLR: 0.010000\n",
      "7.9215\t6.4678\t5.8280\t5.5428\t5.6373\t5.8603\t\n",
      "Training Epoch: 6 [250/569]\tLoss: 39.3181\tLR: 0.010000\n",
      "8.7419\t7.2394\t6.2864\t5.8363\t5.6449\t5.5692\t\n",
      "Training Epoch: 6 [266/569]\tLoss: 38.7472\tLR: 0.010000\n",
      "7.3091\t6.4024\t6.0657\t6.0040\t6.1942\t6.7717\t\n",
      "Training Epoch: 6 [282/569]\tLoss: 38.9144\tLR: 0.010000\n",
      "8.3600\t6.8818\t6.1032\t5.8415\t5.8754\t5.8525\t\n",
      "Training Epoch: 6 [298/569]\tLoss: 30.9464\tLR: 0.010000\n",
      "5.3139\t4.9187\t4.9138\t5.0637\t5.2622\t5.4740\t\n",
      "Training Epoch: 6 [314/569]\tLoss: 34.4758\tLR: 0.010000\n",
      "6.9293\t5.6156\t5.2835\t5.4313\t5.5392\t5.6769\t\n",
      "Training Epoch: 6 [330/569]\tLoss: 43.4564\tLR: 0.010000\n",
      "9.4137\t7.6912\t6.8441\t6.5690\t6.4807\t6.4577\t\n",
      "Training Epoch: 6 [346/569]\tLoss: 35.2898\tLR: 0.010000\n",
      "6.7493\t5.8074\t5.5060\t5.5829\t5.7087\t5.9355\t\n",
      "Training Epoch: 6 [362/569]\tLoss: 35.6154\tLR: 0.010000\n",
      "8.0725\t6.3928\t5.5207\t5.2087\t5.2124\t5.2082\t\n",
      "Training Epoch: 6 [378/569]\tLoss: 41.2014\tLR: 0.010000\n",
      "8.7112\t7.2925\t6.5277\t6.3229\t6.1946\t6.1525\t\n",
      "Training Epoch: 6 [394/569]\tLoss: 42.8008\tLR: 0.010000\n",
      "9.7876\t7.7664\t6.6459\t6.2437\t6.1198\t6.2375\t\n",
      "Training Epoch: 6 [410/569]\tLoss: 41.0246\tLR: 0.010000\n",
      "8.3599\t7.1013\t6.4793\t6.2489\t6.3317\t6.5037\t\n",
      "Training Epoch: 6 [426/569]\tLoss: 43.2187\tLR: 0.010000\n",
      "9.0326\t7.5430\t6.7396\t6.5421\t6.5953\t6.7660\t\n",
      "Training Epoch: 6 [442/569]\tLoss: 39.9911\tLR: 0.010000\n",
      "8.5879\t6.8906\t6.1398\t5.9136\t6.0001\t6.4592\t\n",
      "Training Epoch: 6 [458/569]\tLoss: 36.6556\tLR: 0.010000\n",
      "7.3974\t6.1255\t5.6961\t5.7007\t5.7920\t5.9439\t\n",
      "Training Epoch: 6 [474/569]\tLoss: 43.9189\tLR: 0.010000\n",
      "9.4986\t7.6712\t6.8298\t6.5509\t6.5535\t6.8149\t\n",
      "Training Epoch: 6 [490/569]\tLoss: 38.5468\tLR: 0.010000\n",
      "8.4113\t6.7372\t5.8541\t5.7559\t5.8189\t5.9694\t\n",
      "Training Epoch: 6 [506/569]\tLoss: 27.5500\tLR: 0.010000\n",
      "5.2296\t4.5164\t4.2630\t4.2839\t4.4913\t4.7658\t\n",
      "Training Epoch: 6 [522/569]\tLoss: 45.4920\tLR: 0.010000\n",
      "10.0077\t8.1575\t7.2406\t6.8376\t6.6567\t6.5919\t\n",
      "Training Epoch: 6 [538/569]\tLoss: 38.1066\tLR: 0.010000\n",
      "7.6109\t6.5200\t6.0045\t5.8771\t5.9285\t6.1657\t\n",
      "Training Epoch: 6 [554/569]\tLoss: 37.5886\tLR: 0.010000\n",
      "7.2227\t6.0448\t5.6906\t5.8529\t6.1874\t6.5903\t\n",
      "Training Epoch: 6 [570/569]\tLoss: 38.7326\tLR: 0.010000\n",
      "7.5299\t6.4191\t6.0712\t6.0947\t6.1912\t6.4265\t\n",
      "Training Epoch: 6 [586/569]\tLoss: 36.6943\tLR: 0.010000\n",
      "8.2299\t6.6998\t5.8216\t5.3673\t5.2710\t5.3046\t\n",
      "Training Epoch: 6 [602/569]\tLoss: 35.8294\tLR: 0.010000\n",
      "6.4673\t5.7476\t5.6561\t5.7636\t5.9684\t6.2264\t\n",
      "Training Epoch: 6 [618/569]\tLoss: 41.7565\tLR: 0.010000\n",
      "9.4434\t7.4796\t6.4646\t6.0937\t6.1101\t6.1651\t\n",
      "Training Epoch: 6 [634/569]\tLoss: 34.1839\tLR: 0.010000\n",
      "6.5711\t5.7358\t5.4852\t5.4460\t5.4413\t5.5046\t\n",
      "Training Epoch: 6 [650/569]\tLoss: 31.3204\tLR: 0.010000\n",
      "6.1043\t5.2360\t4.9052\t4.9475\t5.0154\t5.1120\t\n",
      "Training Epoch: 6 [666/569]\tLoss: 35.7820\tLR: 0.010000\n",
      "7.9751\t6.2957\t5.4665\t5.2516\t5.3494\t5.4437\t\n",
      "Training Epoch: 6 [682/569]\tLoss: 38.4102\tLR: 0.010000\n",
      "7.3336\t6.4660\t6.1825\t6.1192\t6.0951\t6.2138\t\n",
      "Training Epoch: 6 [698/569]\tLoss: 27.5016\tLR: 0.010000\n",
      "4.8370\t4.3039\t4.2525\t4.4908\t4.6995\t4.9178\t\n",
      "Training Epoch: 6 [714/569]\tLoss: 34.6302\tLR: 0.010000\n",
      "7.0007\t5.8505\t5.2797\t5.2763\t5.4786\t5.7445\t\n",
      "Training Epoch: 6 [730/569]\tLoss: 32.7875\tLR: 0.010000\n",
      "6.3769\t5.4513\t4.9953\t5.0237\t5.2723\t5.6680\t\n",
      "Training Epoch: 6 [746/569]\tLoss: 29.3731\tLR: 0.010000\n",
      "5.3547\t4.6223\t4.4537\t4.6679\t4.9593\t5.3153\t\n",
      "Training Epoch: 6 [762/569]\tLoss: 47.8832\tLR: 0.010000\n",
      "10.5474\t8.5596\t7.4113\t6.9568\t7.0455\t7.3627\t\n",
      "Training Epoch: 6 [778/569]\tLoss: 36.7722\tLR: 0.010000\n",
      "7.5504\t6.3396\t5.8460\t5.7132\t5.6826\t5.6403\t\n",
      "Training Epoch: 6 [794/569]\tLoss: 40.5273\tLR: 0.010000\n",
      "8.4286\t7.0934\t6.4477\t6.2589\t6.1242\t6.1745\t\n",
      "Training Epoch: 6 [810/569]\tLoss: 46.0333\tLR: 0.010000\n",
      "9.7736\t8.1908\t7.3905\t7.0309\t6.8235\t6.8241\t\n",
      "Training Epoch: 6 [826/569]\tLoss: 36.8578\tLR: 0.010000\n",
      "7.6622\t6.4100\t5.7463\t5.6061\t5.6597\t5.7733\t\n",
      "Training Epoch: 6 [842/569]\tLoss: 38.2170\tLR: 0.010000\n",
      "6.8830\t6.1594\t5.9894\t6.0751\t6.3515\t6.7585\t\n",
      "Training Epoch: 6 [858/569]\tLoss: 37.7126\tLR: 0.010000\n",
      "7.9364\t6.6350\t5.8875\t5.6609\t5.6992\t5.8936\t\n",
      "Training Epoch: 6 [874/569]\tLoss: 37.6977\tLR: 0.010000\n",
      "8.5438\t6.8913\t6.0194\t5.6130\t5.3761\t5.2540\t\n",
      "Training Epoch: 6 [890/569]\tLoss: 48.2131\tLR: 0.010000\n",
      "10.7098\t8.7213\t7.6895\t7.2022\t6.9808\t6.9095\t\n",
      "Training Epoch: 6 [905/569]\tLoss: 39.9240\tLR: 0.010000\n",
      "8.4038\t6.9273\t6.1341\t5.9842\t6.1252\t6.3494\t\n",
      "[0.395358681678772, 0.414309024810791, 0.190332293510437, 0.006881744135171175, 0.7593445181846619, 0.05907119810581207, 0.18158428370952606, 0.0317203588783741, 0.6095108389854431, 0.25784000754356384, 0.13264915347099304, -0.006781591102480888, 0.4913145899772644, 0.2253691703081131, 0.2833162397146225, -0.01580624282360077, 0.4208950698375702, 0.0, 0.5791049301624298, -0.0066606709733605385]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7688, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6433, Accuracy: 0.0721\n",
      "Test set t = 02: Average loss: 0.5852, Accuracy: 0.0738\n",
      "Test set t = 03: Average loss: 0.5709, Accuracy: 0.0738\n",
      "Test set t = 04: Average loss: 0.5739, Accuracy: 0.0773\n",
      "Test set t = 05: Average loss: 0.5842, Accuracy: 0.0826\n",
      "\n",
      "Training Epoch: 7 [10/569]\tLoss: 44.8937\tLR: 0.010000\n",
      "9.9314\t8.1581\t7.1355\t6.7009\t6.5271\t6.4406\t\n",
      "Training Epoch: 7 [26/569]\tLoss: 33.2105\tLR: 0.010000\n",
      "6.6437\t5.5718\t5.2407\t5.2365\t5.2622\t5.2556\t\n",
      "Training Epoch: 7 [42/569]\tLoss: 31.0887\tLR: 0.010000\n",
      "5.4166\t4.8763\t4.9178\t5.0978\t5.3044\t5.4758\t\n",
      "Training Epoch: 7 [58/569]\tLoss: 38.6889\tLR: 0.010000\n",
      "8.2314\t6.8382\t6.1487\t5.8658\t5.8101\t5.7948\t\n",
      "Training Epoch: 7 [74/569]\tLoss: 43.2829\tLR: 0.010000\n",
      "9.6444\t7.8082\t6.8064\t6.3191\t6.2442\t6.4605\t\n",
      "Training Epoch: 7 [90/569]\tLoss: 33.8934\tLR: 0.010000\n",
      "7.2164\t5.8259\t5.2210\t5.0659\t5.1679\t5.3963\t\n",
      "Training Epoch: 7 [106/569]\tLoss: 33.2243\tLR: 0.010000\n",
      "7.2369\t5.6018\t4.9187\t4.9198\t5.1344\t5.4128\t\n",
      "Training Epoch: 7 [122/569]\tLoss: 43.3046\tLR: 0.010000\n",
      "8.8529\t7.5598\t6.9426\t6.6956\t6.6181\t6.6355\t\n",
      "Training Epoch: 7 [138/569]\tLoss: 31.5523\tLR: 0.010000\n",
      "5.8190\t5.1537\t4.9506\t4.9963\t5.1712\t5.4615\t\n",
      "Training Epoch: 7 [154/569]\tLoss: 46.4456\tLR: 0.010000\n",
      "11.0111\t8.5968\t7.2798\t6.6973\t6.4595\t6.4011\t\n",
      "Training Epoch: 7 [170/569]\tLoss: 34.2919\tLR: 0.010000\n",
      "7.2196\t5.9735\t5.3525\t5.1812\t5.2073\t5.3578\t\n",
      "Training Epoch: 7 [186/569]\tLoss: 39.3248\tLR: 0.010000\n",
      "9.0146\t6.9765\t6.0521\t5.7778\t5.7265\t5.7774\t\n",
      "Training Epoch: 7 [202/569]\tLoss: 41.9981\tLR: 0.010000\n",
      "8.7606\t7.3622\t6.6783\t6.4732\t6.3842\t6.3396\t\n",
      "Training Epoch: 7 [218/569]\tLoss: 36.2909\tLR: 0.010000\n",
      "6.6421\t5.7870\t5.5843\t5.7591\t6.0917\t6.4266\t\n",
      "Training Epoch: 7 [234/569]\tLoss: 37.9434\tLR: 0.010000\n",
      "8.1480\t6.7006\t5.9886\t5.7495\t5.6256\t5.7311\t\n",
      "Training Epoch: 7 [250/569]\tLoss: 39.1303\tLR: 0.010000\n",
      "7.9512\t6.6467\t6.1039\t6.0125\t6.1153\t6.3008\t\n",
      "Training Epoch: 7 [266/569]\tLoss: 39.4123\tLR: 0.010000\n",
      "7.7615\t6.6007\t6.1233\t6.1436\t6.2676\t6.5156\t\n",
      "Training Epoch: 7 [282/569]\tLoss: 37.5161\tLR: 0.010000\n",
      "8.2849\t6.6388\t5.8852\t5.5900\t5.5237\t5.5934\t\n",
      "Training Epoch: 7 [298/569]\tLoss: 38.9181\tLR: 0.010000\n",
      "8.6610\t7.1196\t6.3275\t5.8852\t5.5658\t5.3591\t\n",
      "Training Epoch: 7 [314/569]\tLoss: 33.4367\tLR: 0.010000\n",
      "6.7378\t5.5391\t5.2277\t5.2433\t5.3298\t5.3590\t\n",
      "Training Epoch: 7 [330/569]\tLoss: 39.7552\tLR: 0.010000\n",
      "8.9186\t7.0691\t6.2467\t5.8748\t5.7335\t5.9125\t\n",
      "Training Epoch: 7 [346/569]\tLoss: 40.0949\tLR: 0.010000\n",
      "8.1469\t6.8748\t6.3555\t6.1457\t6.1424\t6.4297\t\n",
      "Training Epoch: 7 [362/569]\tLoss: 35.9723\tLR: 0.010000\n",
      "6.7456\t5.8859\t5.7106\t5.7780\t5.8866\t5.9656\t\n",
      "Training Epoch: 7 [378/569]\tLoss: 41.9771\tLR: 0.010000\n",
      "9.0947\t7.1458\t6.4795\t6.3166\t6.4038\t6.5367\t\n",
      "Training Epoch: 7 [394/569]\tLoss: 39.8222\tLR: 0.010000\n",
      "8.6051\t7.1196\t6.3769\t5.9866\t5.8339\t5.9000\t\n",
      "Training Epoch: 7 [410/569]\tLoss: 32.1326\tLR: 0.010000\n",
      "6.5098\t5.3974\t4.9907\t4.9771\t5.0411\t5.2164\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [426/569]\tLoss: 39.5650\tLR: 0.010000\n",
      "8.1276\t6.8776\t6.2984\t6.0098\t6.0226\t6.2291\t\n",
      "Training Epoch: 7 [442/569]\tLoss: 36.7933\tLR: 0.010000\n",
      "8.2154\t6.4500\t5.6923\t5.5428\t5.4866\t5.4062\t\n",
      "Training Epoch: 7 [458/569]\tLoss: 36.5473\tLR: 0.010000\n",
      "6.6809\t5.7979\t5.5388\t5.7026\t6.1983\t6.6289\t\n",
      "Training Epoch: 7 [474/569]\tLoss: 32.0258\tLR: 0.010000\n",
      "6.8494\t5.5651\t5.0815\t4.9289\t4.8354\t4.7655\t\n",
      "Training Epoch: 7 [490/569]\tLoss: 32.2115\tLR: 0.010000\n",
      "6.0224\t5.3009\t5.0812\t5.1420\t5.2500\t5.4150\t\n",
      "Training Epoch: 7 [506/569]\tLoss: 32.8399\tLR: 0.010000\n",
      "6.2872\t5.2894\t5.1151\t5.2449\t5.4191\t5.4841\t\n",
      "Training Epoch: 7 [522/569]\tLoss: 40.9906\tLR: 0.010000\n",
      "9.0023\t7.1557\t6.2554\t5.9889\t6.1700\t6.4183\t\n",
      "Training Epoch: 7 [538/569]\tLoss: 29.0681\tLR: 0.010000\n",
      "5.8596\t4.6925\t4.3620\t4.5363\t4.7270\t4.8907\t\n",
      "Training Epoch: 7 [554/569]\tLoss: 38.3863\tLR: 0.010000\n",
      "7.3589\t6.3291\t5.9854\t6.0525\t6.2346\t6.4257\t\n",
      "Training Epoch: 7 [570/569]\tLoss: 36.5406\tLR: 0.010000\n",
      "6.9038\t6.0086\t5.8066\t5.8506\t5.9242\t6.0469\t\n",
      "Training Epoch: 7 [586/569]\tLoss: 34.6569\tLR: 0.010000\n",
      "6.8141\t5.7370\t5.4432\t5.4720\t5.5223\t5.6682\t\n",
      "Training Epoch: 7 [602/569]\tLoss: 42.7873\tLR: 0.010000\n",
      "9.8709\t7.8122\t6.6522\t6.2800\t6.1272\t6.0449\t\n",
      "Training Epoch: 7 [618/569]\tLoss: 37.0908\tLR: 0.010000\n",
      "7.5671\t6.3738\t5.7059\t5.6367\t5.7924\t6.0148\t\n",
      "Training Epoch: 7 [634/569]\tLoss: 41.2114\tLR: 0.010000\n",
      "9.1374\t7.4096\t6.4599\t6.1672\t6.0200\t6.0173\t\n",
      "Training Epoch: 7 [650/569]\tLoss: 34.0473\tLR: 0.010000\n",
      "7.0699\t5.7256\t5.2730\t5.2931\t5.3405\t5.3452\t\n",
      "Training Epoch: 7 [666/569]\tLoss: 34.2198\tLR: 0.010000\n",
      "6.6751\t5.7211\t5.4295\t5.4200\t5.4232\t5.5510\t\n",
      "Training Epoch: 7 [682/569]\tLoss: 35.3547\tLR: 0.010000\n",
      "7.6646\t6.0232\t5.3672\t5.3675\t5.4549\t5.4774\t\n",
      "Training Epoch: 7 [698/569]\tLoss: 42.2520\tLR: 0.010000\n",
      "8.8814\t7.3126\t6.5983\t6.4240\t6.4615\t6.5742\t\n",
      "Training Epoch: 7 [714/569]\tLoss: 26.5887\tLR: 0.010000\n",
      "4.1961\t3.8751\t4.0692\t4.5009\t4.8265\t5.1209\t\n",
      "Training Epoch: 7 [730/569]\tLoss: 45.6764\tLR: 0.010000\n",
      "9.6600\t7.8512\t6.9856\t6.8247\t7.0384\t7.3165\t\n",
      "Training Epoch: 7 [746/569]\tLoss: 33.1731\tLR: 0.010000\n",
      "6.3773\t5.3650\t5.1504\t5.2519\t5.4189\t5.6097\t\n",
      "Training Epoch: 7 [762/569]\tLoss: 35.3585\tLR: 0.010000\n",
      "6.9715\t5.7915\t5.3236\t5.4052\t5.7494\t6.1173\t\n",
      "Training Epoch: 7 [778/569]\tLoss: 35.5617\tLR: 0.010000\n",
      "7.4762\t6.2093\t5.5588\t5.4219\t5.4217\t5.4739\t\n",
      "Training Epoch: 7 [794/569]\tLoss: 33.3840\tLR: 0.010000\n",
      "5.6853\t5.2350\t5.1417\t5.3718\t5.8273\t6.1230\t\n",
      "Training Epoch: 7 [810/569]\tLoss: 34.3299\tLR: 0.010000\n",
      "6.7169\t5.8603\t5.4036\t5.3609\t5.4232\t5.5651\t\n",
      "Training Epoch: 7 [826/569]\tLoss: 43.3300\tLR: 0.010000\n",
      "9.6405\t7.6671\t6.7215\t6.4649\t6.4184\t6.4176\t\n",
      "Training Epoch: 7 [842/569]\tLoss: 36.8473\tLR: 0.010000\n",
      "7.2250\t6.1292\t5.7786\t5.7475\t5.8995\t6.0675\t\n",
      "Training Epoch: 7 [858/569]\tLoss: 39.1823\tLR: 0.010000\n",
      "9.0239\t6.8953\t5.9736\t5.7835\t5.7359\t5.7701\t\n",
      "Training Epoch: 7 [874/569]\tLoss: 34.2098\tLR: 0.010000\n",
      "7.1164\t5.9283\t5.4368\t5.2028\t5.2013\t5.3242\t\n",
      "Training Epoch: 7 [890/569]\tLoss: 41.3356\tLR: 0.010000\n",
      "9.0472\t7.2933\t6.4352\t6.1904\t6.1796\t6.1899\t\n",
      "Training Epoch: 7 [905/569]\tLoss: 34.8862\tLR: 0.010000\n",
      "6.0029\t5.3077\t5.3484\t5.7421\t6.0752\t6.4099\t\n",
      "[0.39913129806518555, 0.41124674677848816, 0.1896219551563263, 0.0050840531475842, 0.7684599161148071, 0.04793200269341469, 0.18360808119177818, 0.03583618998527527, 0.5986741781234741, 0.28348827362060547, 0.11783754825592041, -0.00944364070892334, 0.5113801956176758, 0.23799867928028107, 0.25062112510204315, -0.01952790468931198, 0.44529667496681213, 0.0, 0.5547033250331879, -0.009391799569129944]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7684, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6342, Accuracy: 0.0703\n",
      "Test set t = 02: Average loss: 0.5800, Accuracy: 0.0738\n",
      "Test set t = 03: Average loss: 0.5706, Accuracy: 0.0791\n",
      "Test set t = 04: Average loss: 0.5762, Accuracy: 0.0808\n",
      "Test set t = 05: Average loss: 0.5871, Accuracy: 0.0826\n",
      "\n",
      "Training Epoch: 8 [10/569]\tLoss: 39.3116\tLR: 0.010000\n",
      "7.7971\t6.6430\t6.2244\t6.1737\t6.2052\t6.2683\t\n",
      "Training Epoch: 8 [26/569]\tLoss: 38.3416\tLR: 0.010000\n",
      "7.9011\t6.6878\t6.1117\t5.9018\t5.8702\t5.8691\t\n",
      "Training Epoch: 8 [42/569]\tLoss: 38.3328\tLR: 0.010000\n",
      "8.8054\t6.9238\t5.8708\t5.5257\t5.5596\t5.6476\t\n",
      "Training Epoch: 8 [58/569]\tLoss: 34.5197\tLR: 0.010000\n",
      "6.6378\t5.7232\t5.4368\t5.4821\t5.5919\t5.6480\t\n",
      "Training Epoch: 8 [74/569]\tLoss: 37.4842\tLR: 0.010000\n",
      "8.0916\t6.6212\t6.0091\t5.7144\t5.5587\t5.4892\t\n",
      "Training Epoch: 8 [90/569]\tLoss: 33.3144\tLR: 0.010000\n",
      "6.0847\t5.3717\t5.2285\t5.3612\t5.5636\t5.7046\t\n",
      "Training Epoch: 8 [106/569]\tLoss: 29.1647\tLR: 0.010000\n",
      "6.1479\t5.1223\t4.6379\t4.4393\t4.3823\t4.4350\t\n",
      "Training Epoch: 8 [122/569]\tLoss: 31.7526\tLR: 0.010000\n",
      "6.7677\t5.3234\t4.7589\t4.7404\t5.0038\t5.1582\t\n",
      "Training Epoch: 8 [138/569]\tLoss: 31.7484\tLR: 0.010000\n",
      "6.1144\t5.1225\t4.8220\t4.9437\t5.2203\t5.5256\t\n",
      "Training Epoch: 8 [154/569]\tLoss: 31.0136\tLR: 0.010000\n",
      "5.9578\t5.0242\t4.8134\t4.9024\t5.0867\t5.2292\t\n",
      "Training Epoch: 8 [170/569]\tLoss: 41.4919\tLR: 0.010000\n",
      "8.9818\t7.2667\t6.5174\t6.3144\t6.2001\t6.2115\t\n",
      "Training Epoch: 8 [186/569]\tLoss: 34.9439\tLR: 0.010000\n",
      "7.1716\t5.9647\t5.4728\t5.3854\t5.4412\t5.5082\t\n",
      "Training Epoch: 8 [202/569]\tLoss: 29.0802\tLR: 0.010000\n",
      "4.6496\t4.3397\t4.5412\t4.8854\t5.1837\t5.4806\t\n",
      "Training Epoch: 8 [218/569]\tLoss: 38.2829\tLR: 0.010000\n",
      "7.4487\t6.5370\t6.0988\t5.9502\t6.0548\t6.1934\t\n",
      "Training Epoch: 8 [234/569]\tLoss: 32.5547\tLR: 0.010000\n",
      "5.9849\t5.2492\t5.0437\t5.2015\t5.4278\t5.6476\t\n",
      "Training Epoch: 8 [250/569]\tLoss: 39.3211\tLR: 0.010000\n",
      "9.0508\t7.0936\t6.1880\t5.7946\t5.6042\t5.5900\t\n",
      "Training Epoch: 8 [266/569]\tLoss: 33.6593\tLR: 0.010000\n",
      "7.3695\t5.9813\t5.3793\t5.0650\t4.9437\t4.9205\t\n",
      "Training Epoch: 8 [282/569]\tLoss: 41.8784\tLR: 0.010000\n",
      "9.7258\t7.4076\t6.4057\t6.1396\t6.0818\t6.1180\t\n",
      "Training Epoch: 8 [298/569]\tLoss: 45.4529\tLR: 0.010000\n",
      "9.6120\t7.9642\t7.2339\t6.9650\t6.8255\t6.8523\t\n",
      "Training Epoch: 8 [314/569]\tLoss: 39.8487\tLR: 0.010000\n",
      "8.4843\t6.9313\t6.2479\t6.1023\t6.0566\t6.0265\t\n",
      "Training Epoch: 8 [330/569]\tLoss: 44.0381\tLR: 0.010000\n",
      "9.9269\t7.8800\t6.9535\t6.5641\t6.3784\t6.3352\t\n",
      "Training Epoch: 8 [346/569]\tLoss: 37.2069\tLR: 0.010000\n",
      "8.1579\t6.5581\t5.7633\t5.5693\t5.5797\t5.5785\t\n",
      "Training Epoch: 8 [362/569]\tLoss: 37.6510\tLR: 0.010000\n",
      "8.1679\t6.4688\t5.7629\t5.6615\t5.7404\t5.8495\t\n",
      "Training Epoch: 8 [378/569]\tLoss: 35.4266\tLR: 0.010000\n",
      "6.7467\t5.7928\t5.5511\t5.6716\t5.7675\t5.8969\t\n",
      "Training Epoch: 8 [394/569]\tLoss: 38.5434\tLR: 0.010000\n",
      "7.8624\t6.6106\t6.0234\t5.9689\t6.0016\t6.0765\t\n",
      "Training Epoch: 8 [410/569]\tLoss: 38.7773\tLR: 0.010000\n",
      "8.0253\t6.7996\t6.1653\t5.9666\t5.9090\t5.9114\t\n",
      "Training Epoch: 8 [426/569]\tLoss: 37.2713\tLR: 0.010000\n",
      "7.9877\t6.3618\t5.8593\t5.7154\t5.6835\t5.6636\t\n",
      "Training Epoch: 8 [442/569]\tLoss: 39.8724\tLR: 0.010000\n",
      "8.8905\t6.9651\t6.1625\t5.9517\t5.9389\t5.9637\t\n",
      "Training Epoch: 8 [458/569]\tLoss: 29.5296\tLR: 0.010000\n",
      "5.0383\t4.4815\t4.6417\t4.9394\t5.1422\t5.2865\t\n",
      "Training Epoch: 8 [474/569]\tLoss: 41.5677\tLR: 0.010000\n",
      "8.4923\t7.1745\t6.6520\t6.5467\t6.4233\t6.2789\t\n",
      "Training Epoch: 8 [490/569]\tLoss: 42.0601\tLR: 0.010000\n",
      "9.4797\t7.5380\t6.6467\t6.2274\t6.0657\t6.1026\t\n",
      "Training Epoch: 8 [506/569]\tLoss: 36.5296\tLR: 0.010000\n",
      "7.4321\t6.1288\t5.6928\t5.6461\t5.7615\t5.8684\t\n",
      "Training Epoch: 8 [522/569]\tLoss: 31.4440\tLR: 0.010000\n",
      "5.0102\t4.9460\t5.1751\t5.3914\t5.4423\t5.4790\t\n",
      "Training Epoch: 8 [538/569]\tLoss: 40.5805\tLR: 0.010000\n",
      "7.3496\t6.6414\t6.5556\t6.5840\t6.6478\t6.8020\t\n",
      "Training Epoch: 8 [554/569]\tLoss: 39.6983\tLR: 0.010000\n",
      "8.8969\t7.0198\t6.0610\t5.8829\t5.8759\t5.9617\t\n",
      "Training Epoch: 8 [570/569]\tLoss: 41.3787\tLR: 0.010000\n",
      "9.2376\t7.1893\t6.2873\t6.1436\t6.2047\t6.3162\t\n",
      "Training Epoch: 8 [586/569]\tLoss: 39.7607\tLR: 0.010000\n",
      "8.8270\t6.9327\t6.0982\t5.9795\t5.9657\t5.9575\t\n",
      "Training Epoch: 8 [602/569]\tLoss: 35.7109\tLR: 0.010000\n",
      "6.3937\t5.4584\t5.3596\t5.7234\t6.2407\t6.5351\t\n",
      "Training Epoch: 8 [618/569]\tLoss: 34.8634\tLR: 0.010000\n",
      "6.8500\t5.7480\t5.3749\t5.4571\t5.6264\t5.8070\t\n",
      "Training Epoch: 8 [634/569]\tLoss: 36.5534\tLR: 0.010000\n",
      "7.0302\t5.9806\t5.7281\t5.8009\t5.9509\t6.0628\t\n",
      "Training Epoch: 8 [650/569]\tLoss: 36.1075\tLR: 0.010000\n",
      "6.7887\t5.8135\t5.5992\t5.7717\t5.9507\t6.1838\t\n",
      "Training Epoch: 8 [666/569]\tLoss: 36.7087\tLR: 0.010000\n",
      "7.5965\t6.1103\t5.6536\t5.7379\t5.7885\t5.8219\t\n",
      "Training Epoch: 8 [682/569]\tLoss: 41.0485\tLR: 0.010000\n",
      "8.2993\t6.8552\t6.2705\t6.1895\t6.5381\t6.8959\t\n",
      "Training Epoch: 8 [698/569]\tLoss: 42.2992\tLR: 0.010000\n",
      "9.8156\t7.6479\t6.5843\t6.1807\t6.0334\t6.0372\t\n",
      "Training Epoch: 8 [714/569]\tLoss: 33.3293\tLR: 0.010000\n",
      "6.5685\t5.3220\t5.1154\t5.2396\t5.4635\t5.6202\t\n",
      "Training Epoch: 8 [730/569]\tLoss: 34.1826\tLR: 0.010000\n",
      "7.2819\t5.7140\t5.1418\t5.1891\t5.3460\t5.5098\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [746/569]\tLoss: 38.4292\tLR: 0.010000\n",
      "7.5178\t6.3468\t6.0974\t6.1130\t6.1459\t6.2082\t\n",
      "Training Epoch: 8 [762/569]\tLoss: 33.2900\tLR: 0.010000\n",
      "6.7343\t5.4655\t5.1387\t5.1982\t5.3166\t5.4367\t\n",
      "Training Epoch: 8 [778/569]\tLoss: 36.0699\tLR: 0.010000\n",
      "7.7968\t6.2362\t5.5660\t5.3792\t5.4450\t5.6467\t\n",
      "Training Epoch: 8 [794/569]\tLoss: 36.7993\tLR: 0.010000\n",
      "7.8038\t6.4713\t5.8866\t5.6199\t5.5104\t5.5074\t\n",
      "Training Epoch: 8 [810/569]\tLoss: 32.5899\tLR: 0.010000\n",
      "6.7314\t5.4163\t4.9846\t5.0203\t5.1647\t5.2726\t\n",
      "Training Epoch: 8 [826/569]\tLoss: 45.5700\tLR: 0.010000\n",
      "10.7671\t8.2794\t6.9611\t6.5810\t6.4783\t6.5032\t\n",
      "Training Epoch: 8 [842/569]\tLoss: 42.2244\tLR: 0.010000\n",
      "9.3562\t7.5673\t6.6572\t6.2995\t6.1833\t6.1609\t\n",
      "Training Epoch: 8 [858/569]\tLoss: 32.3223\tLR: 0.010000\n",
      "6.7473\t5.3664\t4.9095\t4.8886\t5.1013\t5.3092\t\n",
      "Training Epoch: 8 [874/569]\tLoss: 38.5612\tLR: 0.010000\n",
      "8.1811\t6.6684\t6.0067\t5.8161\t5.8905\t5.9983\t\n",
      "Training Epoch: 8 [890/569]\tLoss: 37.0262\tLR: 0.010000\n",
      "8.1838\t6.3473\t5.7478\t5.6221\t5.5576\t5.5676\t\n",
      "Training Epoch: 8 [905/569]\tLoss: 38.4932\tLR: 0.010000\n",
      "6.6389\t6.1388\t6.1080\t6.2452\t6.5251\t6.8372\t\n",
      "[0.4189920723438263, 0.3940261900424957, 0.18698173761367798, 0.00401925016194582, 0.7828094363212585, 0.03852543607354164, 0.1786651276051998, 0.039977241307497025, 0.595094621181488, 0.29730063676834106, 0.1076047420501709, -0.012097367085516453, 0.5289455056190491, 0.24413713812828064, 0.2269173562526703, -0.02327161654829979, 0.4693775773048401, 0.0, 0.5306224226951599, -0.012193751521408558]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7689, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6287, Accuracy: 0.0685\n",
      "Test set t = 02: Average loss: 0.5781, Accuracy: 0.0738\n",
      "Test set t = 03: Average loss: 0.5716, Accuracy: 0.0756\n",
      "Test set t = 04: Average loss: 0.5775, Accuracy: 0.0773\n",
      "Test set t = 05: Average loss: 0.5861, Accuracy: 0.0773\n",
      "\n",
      "Training Epoch: 9 [10/569]\tLoss: 33.1030\tLR: 0.010000\n",
      "6.8925\t5.7525\t5.2724\t5.1428\t5.0409\t5.0019\t\n",
      "Training Epoch: 9 [26/569]\tLoss: 37.5133\tLR: 0.010000\n",
      "8.3825\t6.6016\t5.8541\t5.6214\t5.5372\t5.5165\t\n",
      "Training Epoch: 9 [42/569]\tLoss: 38.9529\tLR: 0.010000\n",
      "8.0206\t6.5977\t6.0954\t6.0724\t6.0713\t6.0955\t\n",
      "Training Epoch: 9 [58/569]\tLoss: 42.0147\tLR: 0.010000\n",
      "9.7181\t7.5343\t6.5561\t6.1672\t5.9761\t6.0629\t\n",
      "Training Epoch: 9 [74/569]\tLoss: 32.9070\tLR: 0.010000\n",
      "6.2166\t5.3179\t5.1039\t5.2520\t5.4359\t5.5808\t\n",
      "Training Epoch: 9 [90/569]\tLoss: 40.5504\tLR: 0.010000\n",
      "8.5862\t6.9083\t6.3617\t6.1933\t6.1942\t6.3067\t\n",
      "Training Epoch: 9 [106/569]\tLoss: 41.2285\tLR: 0.010000\n",
      "8.2237\t6.8322\t6.5133\t6.5471\t6.5633\t6.5488\t\n",
      "Training Epoch: 9 [122/569]\tLoss: 41.6137\tLR: 0.010000\n",
      "9.7338\t7.6542\t6.5705\t6.0553\t5.8480\t5.7518\t\n",
      "Training Epoch: 9 [138/569]\tLoss: 38.1629\tLR: 0.010000\n",
      "8.0104\t6.4627\t5.8569\t5.7712\t5.9381\t6.1236\t\n",
      "Training Epoch: 9 [154/569]\tLoss: 33.3035\tLR: 0.010000\n",
      "7.0594\t5.7084\t5.2817\t5.1655\t5.0667\t5.0219\t\n",
      "Training Epoch: 9 [170/569]\tLoss: 40.0523\tLR: 0.010000\n",
      "8.1499\t6.5520\t6.1010\t6.1437\t6.4078\t6.6979\t\n",
      "Training Epoch: 9 [186/569]\tLoss: 29.7011\tLR: 0.010000\n",
      "5.7252\t4.8109\t4.5429\t4.6353\t4.8947\t5.0921\t\n",
      "Training Epoch: 9 [202/569]\tLoss: 32.6897\tLR: 0.010000\n",
      "5.5465\t5.1209\t5.2868\t5.4954\t5.6096\t5.6305\t\n",
      "Training Epoch: 9 [218/569]\tLoss: 34.6752\tLR: 0.010000\n",
      "6.5023\t5.6321\t5.4641\t5.6046\t5.6990\t5.7731\t\n",
      "Training Epoch: 9 [234/569]\tLoss: 43.6581\tLR: 0.010000\n",
      "10.1618\t7.9362\t6.9265\t6.3762\t6.1676\t6.0898\t\n",
      "Training Epoch: 9 [250/569]\tLoss: 35.9380\tLR: 0.010000\n",
      "7.8149\t6.1823\t5.6318\t5.4740\t5.4057\t5.4294\t\n",
      "Training Epoch: 9 [266/569]\tLoss: 39.5338\tLR: 0.010000\n",
      "8.5631\t6.9579\t6.2016\t6.0148\t5.9176\t5.8788\t\n",
      "Training Epoch: 9 [282/569]\tLoss: 35.8456\tLR: 0.010000\n",
      "7.6665\t6.1194\t5.6462\t5.4448\t5.4361\t5.5326\t\n",
      "Training Epoch: 9 [298/569]\tLoss: 38.6581\tLR: 0.010000\n",
      "7.7163\t6.5079\t6.1819\t6.1065\t6.0783\t6.0670\t\n",
      "Training Epoch: 9 [314/569]\tLoss: 39.6776\tLR: 0.010000\n",
      "8.6551\t6.7456\t6.0053\t5.9360\t6.1026\t6.2331\t\n",
      "Training Epoch: 9 [330/569]\tLoss: 39.4164\tLR: 0.010000\n",
      "8.7902\t6.7013\t5.8647\t5.7359\t6.0285\t6.2958\t\n",
      "Training Epoch: 9 [346/569]\tLoss: 40.6805\tLR: 0.010000\n",
      "8.1293\t6.7486\t6.4473\t6.4089\t6.4353\t6.5112\t\n",
      "Training Epoch: 9 [362/569]\tLoss: 35.3372\tLR: 0.010000\n",
      "6.6919\t5.8024\t5.5220\t5.5704\t5.7958\t5.9546\t\n",
      "Training Epoch: 9 [378/569]\tLoss: 46.1891\tLR: 0.010000\n",
      "10.3560\t7.8778\t6.9982\t6.9143\t6.9817\t7.0611\t\n",
      "Training Epoch: 9 [394/569]\tLoss: 36.1948\tLR: 0.010000\n",
      "6.6059\t5.8702\t5.7107\t5.8168\t5.9841\t6.2071\t\n",
      "Training Epoch: 9 [410/569]\tLoss: 33.2346\tLR: 0.010000\n",
      "6.5083\t5.3041\t5.0803\t5.1959\t5.4491\t5.6969\t\n",
      "Training Epoch: 9 [426/569]\tLoss: 35.9007\tLR: 0.010000\n",
      "7.5099\t6.0692\t5.5769\t5.5240\t5.5876\t5.6330\t\n",
      "Training Epoch: 9 [442/569]\tLoss: 40.2102\tLR: 0.010000\n",
      "8.6598\t6.6457\t6.1008\t6.1545\t6.2990\t6.3505\t\n",
      "Training Epoch: 9 [458/569]\tLoss: 38.0094\tLR: 0.010000\n",
      "8.1634\t6.5099\t5.9415\t5.7636\t5.7498\t5.8812\t\n",
      "Training Epoch: 9 [474/569]\tLoss: 44.4936\tLR: 0.010000\n",
      "9.9618\t7.9267\t6.9454\t6.6466\t6.5604\t6.4528\t\n",
      "Training Epoch: 9 [490/569]\tLoss: 34.5725\tLR: 0.010000\n",
      "6.5886\t5.5735\t5.3438\t5.4720\t5.7129\t5.8817\t\n",
      "Training Epoch: 9 [506/569]\tLoss: 46.0324\tLR: 0.010000\n",
      "10.9688\t8.3377\t7.0534\t6.5842\t6.5199\t6.5685\t\n",
      "Training Epoch: 9 [522/569]\tLoss: 29.4874\tLR: 0.010000\n",
      "7.0770\t5.1705\t4.4161\t4.2128\t4.2706\t4.3404\t\n",
      "Training Epoch: 9 [538/569]\tLoss: 35.5199\tLR: 0.010000\n",
      "6.8559\t5.7123\t5.4844\t5.6508\t5.8479\t5.9686\t\n",
      "Training Epoch: 9 [554/569]\tLoss: 40.3364\tLR: 0.010000\n",
      "9.4647\t7.1608\t6.1081\t5.8328\t5.8406\t5.9294\t\n",
      "Training Epoch: 9 [570/569]\tLoss: 39.3302\tLR: 0.010000\n",
      "7.4975\t6.3684\t6.1420\t6.3357\t6.4618\t6.5247\t\n",
      "Training Epoch: 9 [586/569]\tLoss: 38.6017\tLR: 0.010000\n",
      "7.9076\t6.3033\t6.0022\t5.9864\t6.1067\t6.2955\t\n",
      "Training Epoch: 9 [602/569]\tLoss: 42.7368\tLR: 0.010000\n",
      "9.8381\t7.5517\t6.5662\t6.3377\t6.2311\t6.2121\t\n",
      "Training Epoch: 9 [618/569]\tLoss: 34.8452\tLR: 0.010000\n",
      "6.7369\t5.8193\t5.4144\t5.4700\t5.6168\t5.7878\t\n",
      "Training Epoch: 9 [634/569]\tLoss: 33.8301\tLR: 0.010000\n",
      "6.6529\t5.4825\t5.1871\t5.3648\t5.5168\t5.6261\t\n",
      "Training Epoch: 9 [650/569]\tLoss: 33.7356\tLR: 0.010000\n",
      "6.9487\t5.3445\t5.0814\t5.2961\t5.4692\t5.5957\t\n",
      "Training Epoch: 9 [666/569]\tLoss: 34.0690\tLR: 0.010000\n",
      "6.6527\t5.4840\t5.2831\t5.4370\t5.5535\t5.6587\t\n",
      "Training Epoch: 9 [682/569]\tLoss: 34.7949\tLR: 0.010000\n",
      "6.9889\t5.8102\t5.3978\t5.3994\t5.5182\t5.6804\t\n",
      "Training Epoch: 9 [698/569]\tLoss: 33.7667\tLR: 0.010000\n",
      "7.0647\t5.6198\t5.2725\t5.2296\t5.2562\t5.3239\t\n",
      "Training Epoch: 9 [714/569]\tLoss: 34.9593\tLR: 0.010000\n",
      "7.1895\t5.7213\t5.4038\t5.4930\t5.5517\t5.6000\t\n",
      "Training Epoch: 9 [730/569]\tLoss: 34.4370\tLR: 0.010000\n",
      "7.0929\t5.7846\t5.3649\t5.3431\t5.3765\t5.4751\t\n",
      "Training Epoch: 9 [746/569]\tLoss: 31.3725\tLR: 0.010000\n",
      "5.8105\t4.9781\t4.9209\t5.1022\t5.2319\t5.3290\t\n",
      "Training Epoch: 9 [762/569]\tLoss: 38.6069\tLR: 0.010000\n",
      "8.3987\t6.5816\t5.9538\t5.8340\t5.8889\t5.9498\t\n",
      "Training Epoch: 9 [778/569]\tLoss: 37.6465\tLR: 0.010000\n",
      "7.9992\t6.3970\t5.8663\t5.8111\t5.7547\t5.8182\t\n",
      "Training Epoch: 9 [794/569]\tLoss: 28.0445\tLR: 0.010000\n",
      "5.2512\t4.5450\t4.3942\t4.4662\t4.6197\t4.7683\t\n",
      "Training Epoch: 9 [810/569]\tLoss: 40.2305\tLR: 0.010000\n",
      "8.5394\t6.9213\t6.3365\t6.1878\t6.1418\t6.1036\t\n",
      "Training Epoch: 9 [826/569]\tLoss: 35.1845\tLR: 0.010000\n",
      "5.5622\t4.9676\t5.3282\t6.0129\t6.5367\t6.7768\t\n",
      "Training Epoch: 9 [842/569]\tLoss: 38.9380\tLR: 0.010000\n",
      "7.8665\t6.5451\t6.0574\t6.0521\t6.1614\t6.2554\t\n",
      "Training Epoch: 9 [858/569]\tLoss: 39.6566\tLR: 0.010000\n",
      "8.0195\t6.7108\t6.3085\t6.2400\t6.1880\t6.1898\t\n",
      "Training Epoch: 9 [874/569]\tLoss: 28.5340\tLR: 0.010000\n",
      "5.1139\t4.3788\t4.2851\t4.5716\t4.9418\t5.2428\t\n",
      "Training Epoch: 9 [890/569]\tLoss: 36.7823\tLR: 0.010000\n",
      "7.6262\t6.1747\t5.7267\t5.6872\t5.7488\t5.8188\t\n",
      "Training Epoch: 9 [905/569]\tLoss: 34.7514\tLR: 0.010000\n",
      "6.9975\t5.6877\t5.3561\t5.4279\t5.5850\t5.6973\t\n",
      "[0.4410082697868347, 0.3683694005012512, 0.19062232971191406, 0.003397202119231224, 0.7903218865394592, 0.03137011453509331, 0.17830799892544746, 0.04412032663822174, 0.5744624137878418, 0.3255230486392975, 0.10001453757286072, -0.015273715369403362, 0.5425606369972229, 0.2529872953891754, 0.20445206761360168, -0.027131134644150734, 0.4909077286720276, 0.0, 0.5090922713279724, -0.015065223909914494]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7688, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6240, Accuracy: 0.0685\n",
      "Test set t = 02: Average loss: 0.5770, Accuracy: 0.0756\n",
      "Test set t = 03: Average loss: 0.5726, Accuracy: 0.0703\n",
      "Test set t = 04: Average loss: 0.5777, Accuracy: 0.0721\n",
      "Test set t = 05: Average loss: 0.5837, Accuracy: 0.0703\n",
      "\n",
      "Training Epoch: 10 [10/569]\tLoss: 38.6413\tLR: 0.010000\n",
      "8.4732\t6.5352\t5.8627\t5.7983\t5.9213\t6.0506\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [26/569]\tLoss: 34.1041\tLR: 0.010000\n",
      "6.9375\t5.6798\t5.2579\t5.2645\t5.4292\t5.5351\t\n",
      "Training Epoch: 10 [42/569]\tLoss: 42.3104\tLR: 0.010000\n",
      "9.1942\t7.1890\t6.5183\t6.4462\t6.4700\t6.4926\t\n",
      "Training Epoch: 10 [58/569]\tLoss: 42.7572\tLR: 0.010000\n",
      "9.6718\t7.5827\t6.7680\t6.4602\t6.1995\t6.0750\t\n",
      "Training Epoch: 10 [74/569]\tLoss: 42.8540\tLR: 0.010000\n",
      "9.6315\t7.6218\t6.5713\t6.3078\t6.3217\t6.3999\t\n",
      "Training Epoch: 10 [90/569]\tLoss: 37.7393\tLR: 0.010000\n",
      "7.6117\t6.5362\t6.1500\t5.8928\t5.7762\t5.7723\t\n",
      "Training Epoch: 10 [106/569]\tLoss: 36.9614\tLR: 0.010000\n",
      "8.5982\t6.4833\t5.6811\t5.4861\t5.3792\t5.3334\t\n",
      "Training Epoch: 10 [122/569]\tLoss: 42.8422\tLR: 0.010000\n",
      "8.4376\t7.1944\t6.7848\t6.6560\t6.8055\t6.9637\t\n",
      "Training Epoch: 10 [138/569]\tLoss: 44.0235\tLR: 0.010000\n",
      "9.8325\t7.8534\t6.8968\t6.5167\t6.4507\t6.4733\t\n",
      "Training Epoch: 10 [154/569]\tLoss: 39.7193\tLR: 0.010000\n",
      "8.7292\t6.7652\t6.0494\t5.9770\t6.0433\t6.1553\t\n",
      "Training Epoch: 10 [170/569]\tLoss: 40.6983\tLR: 0.010000\n",
      "8.9686\t7.1746\t6.3144\t6.1197\t6.0933\t6.0276\t\n",
      "Training Epoch: 10 [186/569]\tLoss: 29.7118\tLR: 0.010000\n",
      "6.0780\t4.9068\t4.4442\t4.5113\t4.7745\t4.9971\t\n",
      "Training Epoch: 10 [202/569]\tLoss: 37.7972\tLR: 0.010000\n",
      "8.4789\t6.4777\t5.7022\t5.6106\t5.7277\t5.8001\t\n",
      "Training Epoch: 10 [218/569]\tLoss: 36.6174\tLR: 0.010000\n",
      "7.4817\t6.1349\t5.6817\t5.6213\t5.7748\t5.9230\t\n",
      "Training Epoch: 10 [234/569]\tLoss: 36.1352\tLR: 0.010000\n",
      "7.1989\t5.9620\t5.6307\t5.6682\t5.7796\t5.8958\t\n",
      "Training Epoch: 10 [250/569]\tLoss: 42.4346\tLR: 0.010000\n",
      "9.5775\t7.4058\t6.6393\t6.3802\t6.2442\t6.1876\t\n",
      "Training Epoch: 10 [266/569]\tLoss: 37.7338\tLR: 0.010000\n",
      "7.5176\t6.2564\t5.8819\t5.9509\t6.0427\t6.0843\t\n",
      "Training Epoch: 10 [282/569]\tLoss: 32.7313\tLR: 0.010000\n",
      "6.5959\t5.3553\t5.0717\t5.1032\t5.2370\t5.3682\t\n",
      "Training Epoch: 10 [298/569]\tLoss: 37.8525\tLR: 0.010000\n",
      "7.9556\t6.3211\t5.8669\t5.8501\t5.9143\t5.9445\t\n",
      "Training Epoch: 10 [314/569]\tLoss: 30.2352\tLR: 0.010000\n",
      "4.4297\t4.3522\t4.8826\t5.3347\t5.5560\t5.6800\t\n",
      "Training Epoch: 10 [330/569]\tLoss: 33.2549\tLR: 0.010000\n",
      "6.8921\t5.7166\t5.2449\t5.1249\t5.1205\t5.1559\t\n",
      "Training Epoch: 10 [346/569]\tLoss: 37.5744\tLR: 0.010000\n",
      "8.4339\t6.3234\t5.6332\t5.6252\t5.7222\t5.8365\t\n",
      "Training Epoch: 10 [362/569]\tLoss: 39.8750\tLR: 0.010000\n",
      "8.6369\t6.9560\t6.3374\t6.1045\t5.9481\t5.8921\t\n",
      "Training Epoch: 10 [378/569]\tLoss: 38.2256\tLR: 0.010000\n",
      "7.8877\t6.3720\t6.0059\t6.0340\t5.9868\t5.9391\t\n",
      "Training Epoch: 10 [394/569]\tLoss: 44.8868\tLR: 0.010000\n",
      "9.7213\t7.5493\t6.9804\t6.9671\t6.9058\t6.7628\t\n",
      "Training Epoch: 10 [410/569]\tLoss: 38.1813\tLR: 0.010000\n",
      "7.9989\t6.5455\t6.1251\t5.9109\t5.8149\t5.7860\t\n",
      "Training Epoch: 10 [426/569]\tLoss: 38.8215\tLR: 0.010000\n",
      "7.9409\t6.4775\t6.0709\t6.0562\t6.1056\t6.1705\t\n",
      "Training Epoch: 10 [442/569]\tLoss: 35.8348\tLR: 0.010000\n",
      "6.5844\t5.5517\t5.6023\t5.9068\t6.0602\t6.1294\t\n",
      "Training Epoch: 10 [458/569]\tLoss: 32.7623\tLR: 0.010000\n",
      "6.6419\t5.4021\t5.1102\t5.1201\t5.1932\t5.2949\t\n",
      "Training Epoch: 10 [474/569]\tLoss: 36.7275\tLR: 0.010000\n",
      "8.5875\t6.6258\t5.7335\t5.3781\t5.2233\t5.1793\t\n",
      "Training Epoch: 10 [490/569]\tLoss: 41.0258\tLR: 0.010000\n",
      "8.5091\t6.7818\t6.2664\t6.3340\t6.4972\t6.6374\t\n",
      "Training Epoch: 10 [506/569]\tLoss: 43.1254\tLR: 0.010000\n",
      "10.0232\t7.5569\t6.7387\t6.3623\t6.2518\t6.1926\t\n",
      "Training Epoch: 10 [522/569]\tLoss: 37.6002\tLR: 0.010000\n",
      "7.3443\t6.1498\t5.8949\t6.0292\t6.0916\t6.0903\t\n",
      "Training Epoch: 10 [538/569]\tLoss: 31.3252\tLR: 0.010000\n",
      "6.3019\t5.1137\t4.8939\t4.9487\t5.0092\t5.0579\t\n",
      "Training Epoch: 10 [554/569]\tLoss: 35.0228\tLR: 0.010000\n",
      "6.9876\t5.6272\t5.4408\t5.5716\t5.6776\t5.7180\t\n",
      "Training Epoch: 10 [570/569]\tLoss: 33.4862\tLR: 0.010000\n",
      "6.1455\t5.3474\t5.3141\t5.4214\t5.5734\t5.6844\t\n",
      "Training Epoch: 10 [586/569]\tLoss: 26.1714\tLR: 0.010000\n",
      "4.1249\t3.9323\t4.1486\t4.4461\t4.6625\t4.8570\t\n",
      "Training Epoch: 10 [602/569]\tLoss: 38.7293\tLR: 0.010000\n",
      "8.9081\t6.7754\t6.0319\t5.7829\t5.6444\t5.5866\t\n",
      "Training Epoch: 10 [618/569]\tLoss: 40.8972\tLR: 0.010000\n",
      "9.1906\t7.0761\t6.3013\t6.0740\t6.0949\t6.1602\t\n",
      "Training Epoch: 10 [634/569]\tLoss: 28.8257\tLR: 0.010000\n",
      "6.0478\t4.5078\t4.3107\t4.5190\t4.6694\t4.7710\t\n",
      "Training Epoch: 10 [650/569]\tLoss: 40.8276\tLR: 0.010000\n",
      "8.5827\t6.8129\t6.3346\t6.2627\t6.3516\t6.4831\t\n",
      "Training Epoch: 10 [666/569]\tLoss: 37.9251\tLR: 0.010000\n",
      "6.9929\t6.0513\t6.0833\t6.2386\t6.2841\t6.2749\t\n",
      "Training Epoch: 10 [682/569]\tLoss: 32.9721\tLR: 0.010000\n",
      "5.8826\t5.0431\t5.2658\t5.5209\t5.5859\t5.6738\t\n",
      "Training Epoch: 10 [698/569]\tLoss: 35.0043\tLR: 0.010000\n",
      "7.2454\t5.8125\t5.3203\t5.3737\t5.5802\t5.6721\t\n",
      "Training Epoch: 10 [714/569]\tLoss: 41.1215\tLR: 0.010000\n",
      "9.0828\t7.0403\t6.2797\t6.1263\t6.2321\t6.3601\t\n",
      "Training Epoch: 10 [730/569]\tLoss: 37.9005\tLR: 0.010000\n",
      "9.0296\t6.6345\t5.7572\t5.4683\t5.4712\t5.5397\t\n",
      "Training Epoch: 10 [746/569]\tLoss: 36.6614\tLR: 0.010000\n",
      "7.3953\t6.1357\t5.7127\t5.7401\t5.8244\t5.8532\t\n",
      "Training Epoch: 10 [762/569]\tLoss: 32.4782\tLR: 0.010000\n",
      "6.2706\t5.0625\t4.9876\t5.1796\t5.4169\t5.5609\t\n",
      "Training Epoch: 10 [778/569]\tLoss: 36.7837\tLR: 0.010000\n",
      "7.5090\t6.1650\t5.7455\t5.7547\t5.7683\t5.8412\t\n",
      "Training Epoch: 10 [794/569]\tLoss: 33.3932\tLR: 0.010000\n",
      "6.5058\t5.4218\t5.1865\t5.2090\t5.4272\t5.6429\t\n",
      "Training Epoch: 10 [810/569]\tLoss: 40.9508\tLR: 0.010000\n",
      "9.2077\t7.0342\t6.2547\t6.1196\t6.1372\t6.1974\t\n",
      "Training Epoch: 10 [826/569]\tLoss: 42.3405\tLR: 0.010000\n",
      "8.5603\t7.2015\t6.7380\t6.6203\t6.6007\t6.6197\t\n",
      "Training Epoch: 10 [842/569]\tLoss: 30.5334\tLR: 0.010000\n",
      "5.7614\t4.7658\t4.5771\t4.9281\t5.1800\t5.3209\t\n",
      "Training Epoch: 10 [858/569]\tLoss: 32.4664\tLR: 0.010000\n",
      "6.8807\t5.2940\t4.9926\t4.9988\t5.0936\t5.2068\t\n",
      "Training Epoch: 10 [874/569]\tLoss: 34.1566\tLR: 0.010000\n",
      "6.5889\t5.5622\t5.2687\t5.4593\t5.5976\t5.6799\t\n",
      "Training Epoch: 10 [890/569]\tLoss: 31.0407\tLR: 0.010000\n",
      "4.8747\t4.6403\t4.9607\t5.3466\t5.5633\t5.6551\t\n",
      "Training Epoch: 10 [905/569]\tLoss: 34.7285\tLR: 0.010000\n",
      "6.7264\t5.8806\t5.5720\t5.4878\t5.5031\t5.5585\t\n",
      "[0.4479764997959137, 0.36677318811416626, 0.18525031208992004, 0.0032469630241394043, 0.7968536019325256, 0.026468520984053612, 0.17667787708342075, 0.04851476848125458, 0.5701478123664856, 0.33669450879096985, 0.09315767884254456, -0.017691675573587418, 0.5573039054870605, 0.2589299976825714, 0.18376609683036804, -0.0305250845849514, 0.5160991549491882, 0.0, 0.48390084505081177, -0.017682218924164772]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7687, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6173, Accuracy: 0.0721\n",
      "Test set t = 02: Average loss: 0.5751, Accuracy: 0.0721\n",
      "Test set t = 03: Average loss: 0.5736, Accuracy: 0.0703\n",
      "Test set t = 04: Average loss: 0.5794, Accuracy: 0.0721\n",
      "Test set t = 05: Average loss: 0.5847, Accuracy: 0.0703\n",
      "\n",
      "Training Epoch: 11 [10/569]\tLoss: 44.2656\tLR: 0.010000\n",
      "9.4449\t7.4400\t6.9055\t6.8048\t6.8314\t6.8390\t\n",
      "Training Epoch: 11 [26/569]\tLoss: 40.4371\tLR: 0.010000\n",
      "8.9646\t6.8809\t6.1495\t6.0881\t6.1337\t6.2202\t\n",
      "Training Epoch: 11 [42/569]\tLoss: 33.6888\tLR: 0.010000\n",
      "6.9606\t5.7073\t5.2100\t5.1599\t5.2644\t5.3867\t\n",
      "Training Epoch: 11 [58/569]\tLoss: 35.8600\tLR: 0.010000\n",
      "7.4577\t6.0489\t5.6147\t5.6068\t5.5676\t5.5644\t\n",
      "Training Epoch: 11 [74/569]\tLoss: 36.8958\tLR: 0.010000\n",
      "7.9566\t6.3996\t5.7845\t5.5742\t5.5582\t5.6227\t\n",
      "Training Epoch: 11 [90/569]\tLoss: 41.0798\tLR: 0.010000\n",
      "8.3412\t7.0412\t6.5654\t6.3361\t6.3474\t6.4485\t\n",
      "Training Epoch: 11 [106/569]\tLoss: 34.6432\tLR: 0.010000\n",
      "6.4932\t5.4874\t5.4804\t5.6484\t5.7357\t5.7982\t\n",
      "Training Epoch: 11 [122/569]\tLoss: 38.6957\tLR: 0.010000\n",
      "8.2658\t6.6950\t6.1993\t5.9530\t5.8079\t5.7749\t\n",
      "Training Epoch: 11 [138/569]\tLoss: 30.5470\tLR: 0.010000\n",
      "4.9388\t4.4744\t4.8018\t5.1854\t5.4894\t5.6572\t\n",
      "Training Epoch: 11 [154/569]\tLoss: 36.0508\tLR: 0.010000\n",
      "8.8904\t6.2449\t5.3678\t5.1194\t5.1794\t5.2490\t\n",
      "Training Epoch: 11 [170/569]\tLoss: 28.9209\tLR: 0.010000\n",
      "5.3766\t4.6250\t4.5524\t4.6934\t4.8030\t4.8705\t\n",
      "Training Epoch: 11 [186/569]\tLoss: 34.3169\tLR: 0.010000\n",
      "7.1438\t5.6606\t5.3633\t5.3329\t5.3782\t5.4380\t\n",
      "Training Epoch: 11 [202/569]\tLoss: 35.3726\tLR: 0.010000\n",
      "7.7367\t5.9684\t5.3870\t5.3537\t5.4144\t5.5124\t\n",
      "Training Epoch: 11 [218/569]\tLoss: 32.5298\tLR: 0.010000\n",
      "7.0965\t5.3596\t4.8887\t4.9095\t5.0564\t5.2190\t\n",
      "Training Epoch: 11 [234/569]\tLoss: 42.5112\tLR: 0.010000\n",
      "8.7233\t7.2907\t6.8199\t6.6438\t6.5412\t6.4923\t\n",
      "Training Epoch: 11 [250/569]\tLoss: 44.6825\tLR: 0.010000\n",
      "9.6693\t7.7588\t7.0122\t6.8128\t6.7186\t6.7108\t\n",
      "Training Epoch: 11 [266/569]\tLoss: 37.0252\tLR: 0.010000\n",
      "7.6614\t6.2080\t5.8715\t5.7697\t5.7413\t5.7734\t\n",
      "Training Epoch: 11 [282/569]\tLoss: 36.3201\tLR: 0.010000\n",
      "7.9040\t6.2136\t5.6498\t5.5450\t5.5041\t5.5037\t\n",
      "Training Epoch: 11 [298/569]\tLoss: 38.1231\tLR: 0.010000\n",
      "7.1796\t6.0391\t5.8741\t6.1203\t6.4199\t6.4901\t\n",
      "Training Epoch: 11 [314/569]\tLoss: 36.2010\tLR: 0.010000\n",
      "7.1141\t6.0036\t5.7723\t5.7636\t5.7681\t5.7794\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [330/569]\tLoss: 40.0806\tLR: 0.010000\n",
      "7.0851\t6.2185\t6.3534\t6.6920\t6.8648\t6.8669\t\n",
      "Training Epoch: 11 [346/569]\tLoss: 32.7203\tLR: 0.010000\n",
      "7.2176\t5.5493\t4.9753\t4.8585\t5.0152\t5.1044\t\n",
      "Training Epoch: 11 [362/569]\tLoss: 37.1752\tLR: 0.010000\n",
      "7.1986\t5.9779\t5.7604\t5.9309\t6.1022\t6.2053\t\n",
      "Training Epoch: 11 [378/569]\tLoss: 41.3037\tLR: 0.010000\n",
      "9.5201\t7.1907\t6.2942\t6.1276\t6.1076\t6.0635\t\n",
      "Training Epoch: 11 [394/569]\tLoss: 35.0401\tLR: 0.010000\n",
      "6.6886\t5.5261\t5.4540\t5.6989\t5.8279\t5.8445\t\n",
      "Training Epoch: 11 [410/569]\tLoss: 33.3362\tLR: 0.010000\n",
      "6.2718\t5.4279\t5.2284\t5.3358\t5.4909\t5.5814\t\n",
      "Training Epoch: 11 [426/569]\tLoss: 34.5747\tLR: 0.010000\n",
      "7.2612\t5.6795\t5.2782\t5.3342\t5.4593\t5.5624\t\n",
      "Training Epoch: 11 [442/569]\tLoss: 38.8720\tLR: 0.010000\n",
      "9.0455\t6.4911\t5.7875\t5.7467\t5.8550\t5.9463\t\n",
      "Training Epoch: 11 [458/569]\tLoss: 38.9287\tLR: 0.010000\n",
      "7.4794\t6.2896\t6.1610\t6.2689\t6.3652\t6.3646\t\n",
      "Training Epoch: 11 [474/569]\tLoss: 35.2061\tLR: 0.010000\n",
      "7.2346\t5.8806\t5.4924\t5.5066\t5.5238\t5.5680\t\n",
      "Training Epoch: 11 [490/569]\tLoss: 40.0683\tLR: 0.010000\n",
      "8.4809\t6.8175\t6.2917\t6.1779\t6.1491\t6.1512\t\n",
      "Training Epoch: 11 [506/569]\tLoss: 33.3671\tLR: 0.010000\n",
      "6.3183\t5.3909\t5.2958\t5.4202\t5.4464\t5.4955\t\n",
      "Training Epoch: 11 [522/569]\tLoss: 36.1472\tLR: 0.010000\n",
      "7.7800\t6.0096\t5.5635\t5.5442\t5.6059\t5.6440\t\n",
      "Training Epoch: 11 [538/569]\tLoss: 38.3455\tLR: 0.010000\n",
      "7.6254\t6.2338\t5.8854\t6.0574\t6.2258\t6.3176\t\n",
      "Training Epoch: 11 [554/569]\tLoss: 29.5055\tLR: 0.010000\n",
      "5.9610\t4.3883\t4.2168\t4.6699\t5.0229\t5.2467\t\n",
      "Training Epoch: 11 [570/569]\tLoss: 42.5860\tLR: 0.010000\n",
      "10.0560\t7.5673\t6.4453\t6.2016\t6.1809\t6.1348\t\n",
      "Training Epoch: 11 [586/569]\tLoss: 35.4931\tLR: 0.010000\n",
      "5.8194\t5.6750\t5.8326\t5.9897\t6.0729\t6.1035\t\n",
      "Training Epoch: 11 [602/569]\tLoss: 33.9845\tLR: 0.010000\n",
      "7.3127\t5.9336\t5.3818\t5.0896\t5.0833\t5.1835\t\n",
      "Training Epoch: 11 [618/569]\tLoss: 35.7069\tLR: 0.010000\n",
      "7.2736\t5.8397\t5.4306\t5.5752\t5.7270\t5.8608\t\n",
      "Training Epoch: 11 [634/569]\tLoss: 38.6273\tLR: 0.010000\n",
      "8.1310\t6.6814\t6.0544\t5.9152\t5.9136\t5.9318\t\n",
      "Training Epoch: 11 [650/569]\tLoss: 42.4163\tLR: 0.010000\n",
      "9.7427\t7.4769\t6.5889\t6.2309\t6.1783\t6.1985\t\n",
      "Training Epoch: 11 [666/569]\tLoss: 36.6741\tLR: 0.010000\n",
      "7.6167\t6.0731\t5.6463\t5.6263\t5.8023\t5.9094\t\n",
      "Training Epoch: 11 [682/569]\tLoss: 38.6518\tLR: 0.010000\n",
      "8.5139\t6.5451\t5.9044\t5.8714\t5.8859\t5.9311\t\n",
      "Training Epoch: 11 [698/569]\tLoss: 37.7898\tLR: 0.010000\n",
      "7.4776\t6.1368\t5.9293\t6.0089\t6.1209\t6.1163\t\n",
      "Training Epoch: 11 [714/569]\tLoss: 39.5794\tLR: 0.010000\n",
      "8.8455\t6.9266\t6.2102\t5.9916\t5.8537\t5.7517\t\n",
      "Training Epoch: 11 [730/569]\tLoss: 35.1477\tLR: 0.010000\n",
      "8.2651\t6.0345\t5.3549\t5.2067\t5.1489\t5.1376\t\n",
      "Training Epoch: 11 [746/569]\tLoss: 36.6577\tLR: 0.010000\n",
      "7.2743\t5.9282\t5.7679\t5.8468\t5.9058\t5.9347\t\n",
      "Training Epoch: 11 [762/569]\tLoss: 32.4155\tLR: 0.010000\n",
      "6.2648\t5.1986\t5.0436\t5.1721\t5.3118\t5.4246\t\n",
      "Training Epoch: 11 [778/569]\tLoss: 40.0824\tLR: 0.010000\n",
      "8.7567\t6.6807\t6.2347\t6.1342\t6.1192\t6.1568\t\n",
      "Training Epoch: 11 [794/569]\tLoss: 35.7033\tLR: 0.010000\n",
      "6.8091\t5.7159\t5.6351\t5.7496\t5.8640\t5.9296\t\n",
      "Training Epoch: 11 [810/569]\tLoss: 37.7200\tLR: 0.010000\n",
      "8.3057\t6.3877\t5.7325\t5.6701\t5.7746\t5.8493\t\n",
      "Training Epoch: 11 [826/569]\tLoss: 36.8549\tLR: 0.010000\n",
      "6.9414\t5.9843\t5.8518\t5.9163\t6.0181\t6.1429\t\n",
      "Training Epoch: 11 [842/569]\tLoss: 41.7796\tLR: 0.010000\n",
      "9.5473\t7.3846\t6.5268\t6.2511\t6.0743\t5.9954\t\n",
      "Training Epoch: 11 [858/569]\tLoss: 38.1841\tLR: 0.010000\n",
      "8.5846\t6.3036\t5.8106\t5.7640\t5.8110\t5.9104\t\n",
      "Training Epoch: 11 [874/569]\tLoss: 34.7131\tLR: 0.010000\n",
      "7.4682\t5.8711\t5.3611\t5.3274\t5.3440\t5.3412\t\n",
      "Training Epoch: 11 [890/569]\tLoss: 37.3616\tLR: 0.010000\n",
      "7.1416\t6.2402\t6.0140\t6.0094\t5.9871\t5.9692\t\n",
      "Training Epoch: 11 [905/569]\tLoss: 33.6585\tLR: 0.010000\n",
      "6.7750\t5.4640\t5.1895\t5.3023\t5.4246\t5.5031\t\n",
      "[0.45552438497543335, 0.3646693527698517, 0.17980626225471497, 0.002209745580330491, 0.804142415523529, 0.02317664958536625, 0.1726809348911047, 0.052360426634550095, 0.5603883862495422, 0.35112830996513367, 0.0884833037853241, -0.020259510725736618, 0.5650097727775574, 0.26705968379974365, 0.16793054342269897, -0.03346596658229828, 0.5299379229545593, 0.0, 0.4700620770454407, -0.019756920635700226]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7686, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6133, Accuracy: 0.0685\n",
      "Test set t = 02: Average loss: 0.5744, Accuracy: 0.0703\n",
      "Test set t = 03: Average loss: 0.5743, Accuracy: 0.0721\n",
      "Test set t = 04: Average loss: 0.5801, Accuracy: 0.0738\n",
      "Test set t = 05: Average loss: 0.5847, Accuracy: 0.0703\n",
      "\n",
      "Training Epoch: 12 [10/569]\tLoss: 31.7311\tLR: 0.010000\n",
      "6.0220\t4.9923\t5.0116\t5.1540\t5.2375\t5.3137\t\n",
      "Training Epoch: 12 [26/569]\tLoss: 36.8959\tLR: 0.010000\n",
      "7.6087\t6.2044\t5.7885\t5.7113\t5.7631\t5.8199\t\n",
      "Training Epoch: 12 [42/569]\tLoss: 34.9205\tLR: 0.010000\n",
      "8.4305\t6.0526\t5.2092\t5.0541\t5.0419\t5.1322\t\n",
      "Training Epoch: 12 [58/569]\tLoss: 42.1494\tLR: 0.010000\n",
      "8.8654\t6.8513\t6.4133\t6.5946\t6.7116\t6.7131\t\n",
      "Training Epoch: 12 [74/569]\tLoss: 33.1222\tLR: 0.010000\n",
      "6.1322\t5.5125\t5.4617\t5.3443\t5.3121\t5.3595\t\n",
      "Training Epoch: 12 [90/569]\tLoss: 32.1128\tLR: 0.010000\n",
      "5.7139\t4.9753\t5.1537\t5.3858\t5.4276\t5.4565\t\n",
      "Training Epoch: 12 [106/569]\tLoss: 36.1135\tLR: 0.010000\n",
      "7.6729\t5.7854\t5.4648\t5.5945\t5.7513\t5.8446\t\n",
      "Training Epoch: 12 [122/569]\tLoss: 37.9093\tLR: 0.010000\n",
      "8.1813\t6.4729\t5.8197\t5.7411\t5.8082\t5.8862\t\n",
      "Training Epoch: 12 [138/569]\tLoss: 33.6973\tLR: 0.010000\n",
      "7.0394\t5.4831\t5.1332\t5.2276\t5.3628\t5.4511\t\n",
      "Training Epoch: 12 [154/569]\tLoss: 37.5284\tLR: 0.010000\n",
      "7.8449\t6.2705\t5.8283\t5.7954\t5.8558\t5.9334\t\n",
      "Training Epoch: 12 [170/569]\tLoss: 41.7238\tLR: 0.010000\n",
      "8.9024\t7.0415\t6.5186\t6.4460\t6.4314\t6.3839\t\n",
      "Training Epoch: 12 [186/569]\tLoss: 42.5702\tLR: 0.010000\n",
      "8.6963\t7.0975\t6.7165\t6.6802\t6.6938\t6.6858\t\n",
      "Training Epoch: 12 [202/569]\tLoss: 42.2059\tLR: 0.010000\n",
      "8.8679\t7.0047\t6.5823\t6.6055\t6.5878\t6.5576\t\n",
      "Training Epoch: 12 [218/569]\tLoss: 33.0645\tLR: 0.010000\n",
      "6.6780\t5.0940\t4.9958\t5.2760\t5.4634\t5.5572\t\n",
      "Training Epoch: 12 [234/569]\tLoss: 34.6855\tLR: 0.010000\n",
      "6.7263\t5.6569\t5.4133\t5.4637\t5.6489\t5.7765\t\n",
      "Training Epoch: 12 [250/569]\tLoss: 37.4927\tLR: 0.010000\n",
      "7.4176\t6.1398\t5.9411\t6.0145\t6.0034\t5.9764\t\n",
      "Training Epoch: 12 [266/569]\tLoss: 34.6497\tLR: 0.010000\n",
      "8.0286\t5.6913\t5.1164\t5.1460\t5.2718\t5.3955\t\n",
      "Training Epoch: 12 [282/569]\tLoss: 34.4624\tLR: 0.010000\n",
      "6.0687\t5.3295\t5.4175\t5.7004\t5.9071\t6.0392\t\n",
      "Training Epoch: 12 [298/569]\tLoss: 39.3194\tLR: 0.010000\n",
      "7.4681\t6.3856\t6.1429\t6.2423\t6.4819\t6.5986\t\n",
      "Training Epoch: 12 [314/569]\tLoss: 37.2631\tLR: 0.010000\n",
      "7.7803\t6.2049\t5.8856\t5.8323\t5.7872\t5.7729\t\n",
      "Training Epoch: 12 [330/569]\tLoss: 34.3244\tLR: 0.010000\n",
      "6.8671\t5.5903\t5.3924\t5.3963\t5.5073\t5.5708\t\n",
      "Training Epoch: 12 [346/569]\tLoss: 36.4994\tLR: 0.010000\n",
      "8.2082\t6.2555\t5.6174\t5.4634\t5.4489\t5.5059\t\n",
      "Training Epoch: 12 [362/569]\tLoss: 41.7171\tLR: 0.010000\n",
      "9.7777\t7.0946\t6.3059\t6.1274\t6.1698\t6.2416\t\n",
      "Training Epoch: 12 [378/569]\tLoss: 32.3517\tLR: 0.010000\n",
      "6.2673\t5.1724\t4.9546\t5.1522\t5.3471\t5.4582\t\n",
      "Training Epoch: 12 [394/569]\tLoss: 36.2730\tLR: 0.010000\n",
      "7.4851\t5.8188\t5.4240\t5.6208\t5.8837\t6.0405\t\n",
      "Training Epoch: 12 [410/569]\tLoss: 36.1504\tLR: 0.010000\n",
      "7.3765\t5.9148\t5.5513\t5.6391\t5.7992\t5.8696\t\n",
      "Training Epoch: 12 [426/569]\tLoss: 34.4574\tLR: 0.010000\n",
      "8.1932\t6.2385\t5.4381\t5.0269\t4.8187\t4.7420\t\n",
      "Training Epoch: 12 [442/569]\tLoss: 38.3874\tLR: 0.010000\n",
      "9.0119\t6.5928\t5.7442\t5.6023\t5.6703\t5.7660\t\n",
      "Training Epoch: 12 [458/569]\tLoss: 37.8994\tLR: 0.010000\n",
      "8.0115\t6.5501\t6.0793\t5.8815\t5.7169\t5.6602\t\n",
      "Training Epoch: 12 [474/569]\tLoss: 40.0570\tLR: 0.010000\n",
      "9.7908\t7.0645\t5.9943\t5.7154\t5.7202\t5.7718\t\n",
      "Training Epoch: 12 [490/569]\tLoss: 39.5120\tLR: 0.010000\n",
      "8.8299\t6.8364\t6.1121\t5.8800\t5.9020\t5.9516\t\n",
      "Training Epoch: 12 [506/569]\tLoss: 37.6322\tLR: 0.010000\n",
      "7.9271\t6.3002\t6.0169\t5.8807\t5.7882\t5.7191\t\n",
      "Training Epoch: 12 [522/569]\tLoss: 42.6591\tLR: 0.010000\n",
      "8.9583\t7.1700\t6.7291\t6.6516\t6.6108\t6.5392\t\n",
      "Training Epoch: 12 [538/569]\tLoss: 35.6036\tLR: 0.010000\n",
      "6.6813\t5.7867\t5.7377\t5.7533\t5.7989\t5.8456\t\n",
      "Training Epoch: 12 [554/569]\tLoss: 34.5636\tLR: 0.010000\n",
      "7.7554\t5.7045\t5.1794\t5.2376\t5.3097\t5.3770\t\n",
      "Training Epoch: 12 [570/569]\tLoss: 40.2272\tLR: 0.010000\n",
      "8.3169\t6.6069\t6.2642\t6.2613\t6.3587\t6.4193\t\n",
      "Training Epoch: 12 [586/569]\tLoss: 38.5244\tLR: 0.010000\n",
      "8.0171\t6.4502\t6.0374\t6.0014\t6.0128\t6.0055\t\n",
      "Training Epoch: 12 [602/569]\tLoss: 30.0134\tLR: 0.010000\n",
      "5.1485\t4.3191\t4.5461\t5.0372\t5.3978\t5.5647\t\n",
      "Training Epoch: 12 [618/569]\tLoss: 37.5881\tLR: 0.010000\n",
      "8.6732\t6.3297\t5.6811\t5.5599\t5.6258\t5.7185\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [634/569]\tLoss: 38.5934\tLR: 0.010000\n",
      "7.6864\t6.2387\t6.0233\t6.0741\t6.2268\t6.3440\t\n",
      "Training Epoch: 12 [650/569]\tLoss: 36.3478\tLR: 0.010000\n",
      "7.9282\t6.0913\t5.6431\t5.5890\t5.5567\t5.5395\t\n",
      "Training Epoch: 12 [666/569]\tLoss: 38.7698\tLR: 0.010000\n",
      "7.5980\t6.2740\t6.1871\t6.2545\t6.2390\t6.2171\t\n",
      "Training Epoch: 12 [682/569]\tLoss: 36.0586\tLR: 0.010000\n",
      "7.6750\t6.0529\t5.6392\t5.5560\t5.5713\t5.5643\t\n",
      "Training Epoch: 12 [698/569]\tLoss: 32.8209\tLR: 0.010000\n",
      "7.0053\t5.3844\t5.0688\t5.0399\t5.1422\t5.1802\t\n",
      "Training Epoch: 12 [714/569]\tLoss: 33.3597\tLR: 0.010000\n",
      "6.7704\t5.3015\t4.9828\t5.1803\t5.4677\t5.6571\t\n",
      "Training Epoch: 12 [730/569]\tLoss: 38.6372\tLR: 0.010000\n",
      "8.9543\t6.6048\t5.8812\t5.7495\t5.7203\t5.7271\t\n",
      "Training Epoch: 12 [746/569]\tLoss: 35.6554\tLR: 0.010000\n",
      "7.8987\t6.0734\t5.5371\t5.3735\t5.3626\t5.4101\t\n",
      "Training Epoch: 12 [762/569]\tLoss: 33.5226\tLR: 0.010000\n",
      "6.4853\t5.5662\t5.3504\t5.3273\t5.3716\t5.4219\t\n",
      "Training Epoch: 12 [778/569]\tLoss: 37.0932\tLR: 0.010000\n",
      "7.5056\t5.9953\t5.7535\t5.8792\t5.9549\t6.0049\t\n",
      "Training Epoch: 12 [794/569]\tLoss: 36.4309\tLR: 0.010000\n",
      "7.3274\t6.0483\t5.8533\t5.8246\t5.7126\t5.6648\t\n",
      "Training Epoch: 12 [810/569]\tLoss: 38.2699\tLR: 0.010000\n",
      "7.1571\t6.0081\t5.9196\t6.2334\t6.4314\t6.5203\t\n",
      "Training Epoch: 12 [826/569]\tLoss: 36.6298\tLR: 0.010000\n",
      "7.0019\t6.2209\t5.9606\t5.8695\t5.8119\t5.7650\t\n",
      "Training Epoch: 12 [842/569]\tLoss: 40.7933\tLR: 0.010000\n",
      "8.5717\t6.9265\t6.4133\t6.3287\t6.2887\t6.2644\t\n",
      "Training Epoch: 12 [858/569]\tLoss: 39.5923\tLR: 0.010000\n",
      "8.6558\t6.5422\t6.0002\t6.0338\t6.1362\t6.2241\t\n",
      "Training Epoch: 12 [874/569]\tLoss: 38.1616\tLR: 0.010000\n",
      "7.7737\t6.5238\t6.0251\t5.9699\t5.9473\t5.9216\t\n",
      "Training Epoch: 12 [890/569]\tLoss: 37.0001\tLR: 0.010000\n",
      "6.9711\t5.7976\t5.8030\t6.0323\t6.1723\t6.2238\t\n",
      "Training Epoch: 12 [905/569]\tLoss: 37.0788\tLR: 0.010000\n",
      "7.0269\t5.9290\t5.9495\t6.0756\t6.0727\t6.0251\t\n",
      "[0.4590469002723694, 0.362508624792099, 0.17844447493553162, 0.0023052296601235867, 0.8111657500267029, 0.02025008015334606, 0.16858416981995106, 0.056077223271131516, 0.5633962750434875, 0.3526683449745178, 0.08393537998199463, -0.022603143006563187, 0.5762979984283447, 0.27010759711265564, 0.15359440445899963, -0.03660475090146065, 0.5518259406089783, 0.0, 0.44817405939102173, -0.022176701575517654]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7689, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6084, Accuracy: 0.0685\n",
      "Test set t = 02: Average loss: 0.5739, Accuracy: 0.0703\n",
      "Test set t = 03: Average loss: 0.5754, Accuracy: 0.0685\n",
      "Test set t = 04: Average loss: 0.5813, Accuracy: 0.0738\n",
      "Test set t = 05: Average loss: 0.5854, Accuracy: 0.0703\n",
      "\n",
      "Training Epoch: 13 [10/569]\tLoss: 41.0570\tLR: 0.010000\n",
      "9.1579\t7.0845\t6.4367\t6.2024\t6.1065\t6.0688\t\n",
      "Training Epoch: 13 [26/569]\tLoss: 43.2164\tLR: 0.010000\n",
      "9.3208\t7.1194\t6.5091\t6.6385\t6.8182\t6.8104\t\n",
      "Training Epoch: 13 [42/569]\tLoss: 37.6780\tLR: 0.010000\n",
      "7.8654\t6.3689\t5.9403\t5.8679\t5.8186\t5.8169\t\n",
      "Training Epoch: 13 [58/569]\tLoss: 33.9841\tLR: 0.010000\n",
      "7.1310\t5.2746\t5.0879\t5.3627\t5.5267\t5.6012\t\n",
      "Training Epoch: 13 [74/569]\tLoss: 31.1035\tLR: 0.010000\n",
      "6.6179\t5.0011\t4.7472\t4.8498\t4.9303\t4.9571\t\n",
      "Training Epoch: 13 [90/569]\tLoss: 26.9742\tLR: 0.010000\n",
      "4.4817\t3.9484\t4.1941\t4.5712\t4.8250\t4.9538\t\n",
      "Training Epoch: 13 [106/569]\tLoss: 37.6945\tLR: 0.010000\n",
      "7.1670\t5.9017\t5.8016\t6.0849\t6.3445\t6.3948\t\n",
      "Training Epoch: 13 [122/569]\tLoss: 39.3328\tLR: 0.010000\n",
      "8.9136\t6.8299\t6.2007\t5.9104\t5.7611\t5.7171\t\n",
      "Training Epoch: 13 [138/569]\tLoss: 36.7050\tLR: 0.010000\n",
      "8.0889\t6.1130\t5.6869\t5.6333\t5.5920\t5.5910\t\n",
      "Training Epoch: 13 [154/569]\tLoss: 39.6799\tLR: 0.010000\n",
      "9.5575\t7.0563\t6.1353\t5.7580\t5.6076\t5.5652\t\n",
      "Training Epoch: 13 [170/569]\tLoss: 41.2186\tLR: 0.010000\n",
      "9.5830\t7.2174\t6.3719\t6.0651\t5.9834\t5.9979\t\n",
      "Training Epoch: 13 [186/569]\tLoss: 39.2618\tLR: 0.010000\n",
      "8.3391\t6.4654\t6.1604\t6.0559\t6.0950\t6.1460\t\n",
      "Training Epoch: 13 [202/569]\tLoss: 34.7958\tLR: 0.010000\n",
      "7.3169\t5.8636\t5.4024\t5.3394\t5.4044\t5.4692\t\n",
      "Training Epoch: 13 [218/569]\tLoss: 38.8437\tLR: 0.010000\n",
      "8.4083\t6.4881\t6.0228\t5.9662\t5.9776\t5.9807\t\n",
      "Training Epoch: 13 [234/569]\tLoss: 40.7621\tLR: 0.010000\n",
      "8.3614\t6.8186\t6.4690\t6.4065\t6.3616\t6.3450\t\n",
      "Training Epoch: 13 [250/569]\tLoss: 36.8233\tLR: 0.010000\n",
      "7.0270\t5.8595\t5.7354\t5.9114\t6.0951\t6.1949\t\n",
      "Training Epoch: 13 [266/569]\tLoss: 35.8943\tLR: 0.010000\n",
      "7.0873\t5.6775\t5.6212\t5.7621\t5.8585\t5.8877\t\n",
      "Training Epoch: 13 [282/569]\tLoss: 31.7132\tLR: 0.010000\n",
      "6.3380\t5.2193\t4.9832\t5.0651\t5.0463\t5.0612\t\n",
      "Training Epoch: 13 [298/569]\tLoss: 38.0191\tLR: 0.010000\n",
      "7.5085\t6.2816\t6.1211\t6.0343\t6.0360\t6.0375\t\n",
      "Training Epoch: 13 [314/569]\tLoss: 37.8497\tLR: 0.010000\n",
      "8.8887\t6.3803\t5.5680\t5.5557\t5.6734\t5.7836\t\n",
      "Training Epoch: 13 [330/569]\tLoss: 35.0543\tLR: 0.010000\n",
      "6.9123\t5.4715\t5.3744\t5.6660\t5.8097\t5.8204\t\n",
      "Training Epoch: 13 [346/569]\tLoss: 34.4411\tLR: 0.010000\n",
      "6.2216\t5.4207\t5.4824\t5.6925\t5.7746\t5.8492\t\n",
      "Training Epoch: 13 [362/569]\tLoss: 35.9101\tLR: 0.010000\n",
      "7.8416\t6.2556\t5.7206\t5.4592\t5.3405\t5.2926\t\n",
      "Training Epoch: 13 [378/569]\tLoss: 40.9189\tLR: 0.010000\n",
      "9.5483\t6.8201\t6.1240\t6.0489\t6.1569\t6.2207\t\n",
      "Training Epoch: 13 [394/569]\tLoss: 35.3077\tLR: 0.010000\n",
      "6.9978\t5.8408\t5.6585\t5.6098\t5.6024\t5.5984\t\n",
      "Training Epoch: 13 [410/569]\tLoss: 36.1826\tLR: 0.010000\n",
      "7.9104\t5.8322\t5.5532\t5.5767\t5.6088\t5.7013\t\n",
      "Training Epoch: 13 [426/569]\tLoss: 34.8489\tLR: 0.010000\n",
      "6.7744\t5.4896\t5.4312\t5.5818\t5.7316\t5.8403\t\n",
      "Training Epoch: 13 [442/569]\tLoss: 37.0180\tLR: 0.010000\n",
      "7.7242\t6.1143\t5.7384\t5.7551\t5.8325\t5.8535\t\n",
      "Training Epoch: 13 [458/569]\tLoss: 31.3531\tLR: 0.010000\n",
      "5.7844\t4.8392\t4.8115\t5.0993\t5.3572\t5.4615\t\n",
      "Training Epoch: 13 [474/569]\tLoss: 31.6919\tLR: 0.010000\n",
      "6.3692\t4.8004\t4.7510\t5.0732\t5.2995\t5.3987\t\n",
      "Training Epoch: 13 [490/569]\tLoss: 30.2685\tLR: 0.010000\n",
      "6.0276\t4.8407\t4.6495\t4.7751\t4.9536\t5.0219\t\n",
      "Training Epoch: 13 [506/569]\tLoss: 37.9632\tLR: 0.010000\n",
      "7.5937\t6.0465\t5.8552\t6.0484\t6.1715\t6.2479\t\n",
      "Training Epoch: 13 [522/569]\tLoss: 29.1192\tLR: 0.010000\n",
      "4.4176\t4.2465\t4.6665\t5.0862\t5.2984\t5.4040\t\n",
      "Training Epoch: 13 [538/569]\tLoss: 43.6718\tLR: 0.010000\n",
      "9.7608\t7.4545\t6.7468\t6.5686\t6.5735\t6.5675\t\n",
      "Training Epoch: 13 [554/569]\tLoss: 37.8642\tLR: 0.010000\n",
      "8.4555\t6.1684\t5.6590\t5.7687\t5.8640\t5.9487\t\n",
      "Training Epoch: 13 [570/569]\tLoss: 42.8129\tLR: 0.010000\n",
      "9.1734\t7.1272\t6.7722\t6.6017\t6.5827\t6.5558\t\n",
      "Training Epoch: 13 [586/569]\tLoss: 40.4781\tLR: 0.010000\n",
      "9.0213\t6.8861\t6.2425\t6.1310\t6.1071\t6.0901\t\n",
      "Training Epoch: 13 [602/569]\tLoss: 39.2307\tLR: 0.010000\n",
      "8.0914\t6.2185\t5.9473\t6.1605\t6.3600\t6.4529\t\n",
      "Training Epoch: 13 [618/569]\tLoss: 36.5536\tLR: 0.010000\n",
      "7.4257\t6.1966\t5.9058\t5.7242\t5.6571\t5.6442\t\n",
      "Training Epoch: 13 [634/569]\tLoss: 39.1574\tLR: 0.010000\n",
      "8.5588\t6.5657\t6.0411\t5.8951\t5.9962\t6.1005\t\n",
      "Training Epoch: 13 [650/569]\tLoss: 35.0551\tLR: 0.010000\n",
      "6.5537\t5.6058\t5.6138\t5.7458\t5.7778\t5.7583\t\n",
      "Training Epoch: 13 [666/569]\tLoss: 43.7049\tLR: 0.010000\n",
      "10.3997\t7.6251\t6.5659\t6.3679\t6.3459\t6.4004\t\n",
      "Training Epoch: 13 [682/569]\tLoss: 36.8136\tLR: 0.010000\n",
      "6.9918\t6.0082\t5.9302\t5.9353\t5.9598\t5.9883\t\n",
      "Training Epoch: 13 [698/569]\tLoss: 33.6189\tLR: 0.010000\n",
      "5.9727\t5.0568\t5.2582\t5.5948\t5.8120\t5.9243\t\n",
      "Training Epoch: 13 [714/569]\tLoss: 40.5488\tLR: 0.010000\n",
      "8.6367\t6.8540\t6.4466\t6.3199\t6.1785\t6.1131\t\n",
      "Training Epoch: 13 [730/569]\tLoss: 37.3482\tLR: 0.010000\n",
      "7.1418\t5.9582\t5.9534\t6.0666\t6.1029\t6.1253\t\n",
      "Training Epoch: 13 [746/569]\tLoss: 39.6187\tLR: 0.010000\n",
      "8.9052\t6.5882\t6.1330\t5.9732\t5.9965\t6.0226\t\n",
      "Training Epoch: 13 [762/569]\tLoss: 33.6382\tLR: 0.010000\n",
      "6.0587\t5.1956\t5.3035\t5.5858\t5.7318\t5.7629\t\n",
      "Training Epoch: 13 [778/569]\tLoss: 34.5802\tLR: 0.010000\n",
      "7.3030\t5.7355\t5.3715\t5.3256\t5.3717\t5.4728\t\n",
      "Training Epoch: 13 [794/569]\tLoss: 39.9121\tLR: 0.010000\n",
      "8.4916\t6.7563\t6.3300\t6.1743\t6.1036\t6.0563\t\n",
      "Training Epoch: 13 [810/569]\tLoss: 39.8410\tLR: 0.010000\n",
      "9.3150\t6.6932\t6.0086\t5.9473\t5.9271\t5.9499\t\n",
      "Training Epoch: 13 [826/569]\tLoss: 33.9985\tLR: 0.010000\n",
      "6.5418\t5.5170\t5.4124\t5.4404\t5.5108\t5.5761\t\n",
      "Training Epoch: 13 [842/569]\tLoss: 30.3826\tLR: 0.010000\n",
      "5.9568\t4.9763\t4.7787\t4.8228\t4.8981\t4.9499\t\n",
      "Training Epoch: 13 [858/569]\tLoss: 36.3495\tLR: 0.010000\n",
      "8.1741\t6.1489\t5.4708\t5.4204\t5.5174\t5.6177\t\n",
      "Training Epoch: 13 [874/569]\tLoss: 41.4946\tLR: 0.010000\n",
      "9.6208\t7.2859\t6.4546\t6.1124\t6.0254\t5.9955\t\n",
      "Training Epoch: 13 [890/569]\tLoss: 38.0320\tLR: 0.010000\n",
      "7.0723\t5.9898\t5.9411\t6.2399\t6.3651\t6.4238\t\n",
      "Training Epoch: 13 [905/569]\tLoss: 33.6482\tLR: 0.010000\n",
      "6.4741\t5.3886\t5.2401\t5.3829\t5.5396\t5.6229\t\n",
      "[0.466341495513916, 0.35429129004478455, 0.17936721444129944, 0.00269471388310194, 0.8166946768760681, 0.017447154968976974, 0.1658581681549549, 0.059745125472545624, 0.5679565072059631, 0.3519205152988434, 0.08012297749519348, -0.02480023168027401, 0.5938583016395569, 0.2641768455505371, 0.141964852809906, -0.03980642184615135, 0.5677490234375, 0.0, 0.4322509765625, -0.02427280880510807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.7690, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6058, Accuracy: 0.0685\n",
      "Test set t = 02: Average loss: 0.5738, Accuracy: 0.0685\n",
      "Test set t = 03: Average loss: 0.5761, Accuracy: 0.0721\n",
      "Test set t = 04: Average loss: 0.5815, Accuracy: 0.0650\n",
      "Test set t = 05: Average loss: 0.5850, Accuracy: 0.0650\n",
      "\n",
      "Training Epoch: 14 [10/569]\tLoss: 33.5799\tLR: 0.010000\n",
      "6.5358\t5.4216\t5.2484\t5.3174\t5.4710\t5.5857\t\n",
      "Training Epoch: 14 [26/569]\tLoss: 31.8511\tLR: 0.010000\n",
      "5.2944\t4.5666\t4.8013\t5.3682\t5.8222\t5.9983\t\n",
      "Training Epoch: 14 [42/569]\tLoss: 32.8550\tLR: 0.010000\n",
      "5.9320\t5.1509\t5.2352\t5.4131\t5.5267\t5.5972\t\n",
      "Training Epoch: 14 [58/569]\tLoss: 33.1254\tLR: 0.010000\n",
      "6.6244\t5.3915\t5.2689\t5.2586\t5.2808\t5.3011\t\n",
      "Training Epoch: 14 [74/569]\tLoss: 31.0564\tLR: 0.010000\n",
      "5.8842\t4.9051\t4.9217\t5.0523\t5.1240\t5.1691\t\n",
      "Training Epoch: 14 [90/569]\tLoss: 40.7172\tLR: 0.010000\n",
      "8.4281\t6.8078\t6.3985\t6.3781\t6.3556\t6.3491\t\n",
      "Training Epoch: 14 [106/569]\tLoss: 34.1462\tLR: 0.010000\n",
      "7.2524\t5.4904\t5.1787\t5.3056\t5.4132\t5.5058\t\n",
      "Training Epoch: 14 [122/569]\tLoss: 43.6946\tLR: 0.010000\n",
      "9.2665\t7.2709\t6.7688\t6.8749\t6.8247\t6.6888\t\n",
      "Training Epoch: 14 [138/569]\tLoss: 35.8508\tLR: 0.010000\n",
      "7.6951\t5.9066\t5.4614\t5.4822\t5.6147\t5.6907\t\n",
      "Training Epoch: 14 [154/569]\tLoss: 40.9111\tLR: 0.010000\n",
      "9.7660\t7.4446\t6.4139\t5.9220\t5.7061\t5.6586\t\n",
      "Training Epoch: 14 [170/569]\tLoss: 33.3419\tLR: 0.010000\n",
      "6.7491\t5.4429\t5.2591\t5.2391\t5.3040\t5.3477\t\n",
      "Training Epoch: 14 [186/569]\tLoss: 38.1433\tLR: 0.010000\n",
      "7.5728\t6.0799\t5.8623\t6.0851\t6.2449\t6.2983\t\n",
      "Training Epoch: 14 [202/569]\tLoss: 39.8054\tLR: 0.010000\n",
      "7.8135\t6.2513\t6.0482\t6.3835\t6.6129\t6.6960\t\n",
      "Training Epoch: 14 [218/569]\tLoss: 35.5906\tLR: 0.010000\n",
      "6.7992\t5.8253\t5.6656\t5.7054\t5.7847\t5.8104\t\n",
      "Training Epoch: 14 [234/569]\tLoss: 35.3368\tLR: 0.010000\n",
      "7.1620\t5.6502\t5.5045\t5.5991\t5.6817\t5.7393\t\n",
      "Training Epoch: 14 [250/569]\tLoss: 38.6480\tLR: 0.010000\n",
      "7.9743\t6.6634\t6.0945\t5.9530\t5.9749\t5.9879\t\n",
      "Training Epoch: 14 [266/569]\tLoss: 38.0935\tLR: 0.010000\n",
      "7.6728\t6.3064\t6.0231\t5.9897\t6.0350\t6.0665\t\n",
      "Training Epoch: 14 [282/569]\tLoss: 28.4110\tLR: 0.010000\n",
      "4.6792\t4.1466\t4.4524\t4.8565\t5.0854\t5.1910\t\n",
      "Training Epoch: 14 [298/569]\tLoss: 37.6557\tLR: 0.010000\n",
      "7.5083\t6.5206\t6.2426\t5.9410\t5.7553\t5.6878\t\n",
      "Training Epoch: 14 [314/569]\tLoss: 34.9233\tLR: 0.010000\n",
      "6.9005\t5.6648\t5.5210\t5.6250\t5.6135\t5.5984\t\n",
      "Training Epoch: 14 [330/569]\tLoss: 38.9217\tLR: 0.010000\n",
      "8.3086\t6.3883\t5.9442\t6.0305\t6.0997\t6.1504\t\n",
      "Training Epoch: 14 [346/569]\tLoss: 36.7029\tLR: 0.010000\n",
      "7.3209\t5.9248\t5.7530\t5.8688\t5.9120\t5.9233\t\n",
      "Training Epoch: 14 [362/569]\tLoss: 39.6219\tLR: 0.010000\n",
      "8.0618\t6.5347\t6.1854\t6.2356\t6.2869\t6.3177\t\n",
      "Training Epoch: 14 [378/569]\tLoss: 37.3667\tLR: 0.010000\n",
      "7.7179\t5.9803\t5.7965\t5.8561\t5.9869\t6.0290\t\n",
      "Training Epoch: 14 [394/569]\tLoss: 48.4066\tLR: 0.010000\n",
      "9.5959\t8.3013\t8.0021\t7.6832\t7.4826\t7.3415\t\n",
      "Training Epoch: 14 [410/569]\tLoss: 36.1487\tLR: 0.010000\n",
      "8.2572\t6.0797\t5.4624\t5.4417\t5.4499\t5.4578\t\n",
      "Training Epoch: 14 [426/569]\tLoss: 38.3166\tLR: 0.010000\n",
      "8.8968\t6.6092\t5.8790\t5.6835\t5.6152\t5.6330\t\n",
      "Training Epoch: 14 [442/569]\tLoss: 38.1032\tLR: 0.010000\n",
      "7.9526\t6.2880\t5.8334\t5.9159\t6.0187\t6.0946\t\n",
      "Training Epoch: 14 [458/569]\tLoss: 43.1743\tLR: 0.010000\n",
      "11.0911\t7.6345\t6.4870\t6.1113\t5.9245\t5.9259\t\n",
      "Training Epoch: 14 [474/569]\tLoss: 30.9215\tLR: 0.010000\n",
      "6.0810\t4.8242\t4.8118\t5.0092\t5.0799\t5.1155\t\n",
      "Training Epoch: 14 [490/569]\tLoss: 32.3465\tLR: 0.010000\n",
      "6.4625\t5.1748\t5.0415\t5.1291\t5.2432\t5.2954\t\n",
      "Training Epoch: 14 [506/569]\tLoss: 39.4708\tLR: 0.010000\n",
      "8.6748\t6.5528\t5.9675\t5.9751\t6.1092\t6.1914\t\n",
      "Training Epoch: 14 [522/569]\tLoss: 38.2033\tLR: 0.010000\n",
      "6.9057\t6.1022\t6.0870\t6.3026\t6.3944\t6.4113\t\n",
      "Training Epoch: 14 [538/569]\tLoss: 34.7643\tLR: 0.010000\n",
      "7.8306\t5.5419\t5.1325\t5.2442\t5.4547\t5.5604\t\n",
      "Training Epoch: 14 [554/569]\tLoss: 32.8486\tLR: 0.010000\n",
      "5.9215\t4.9395\t5.1372\t5.4682\t5.6596\t5.7227\t\n",
      "Training Epoch: 14 [570/569]\tLoss: 39.0695\tLR: 0.010000\n",
      "8.3431\t6.6319\t6.2817\t6.0114\t5.9112\t5.8902\t\n",
      "Training Epoch: 14 [586/569]\tLoss: 39.3214\tLR: 0.010000\n",
      "9.0281\t6.4924\t5.8866\t5.8967\t5.9660\t6.0516\t\n",
      "Training Epoch: 14 [602/569]\tLoss: 44.0734\tLR: 0.010000\n",
      "9.8843\t7.5040\t6.7809\t6.6396\t6.6307\t6.6338\t\n",
      "Training Epoch: 14 [618/569]\tLoss: 32.0455\tLR: 0.010000\n",
      "5.8533\t4.8324\t5.0789\t5.2915\t5.4596\t5.5299\t\n",
      "Training Epoch: 14 [634/569]\tLoss: 39.5319\tLR: 0.010000\n",
      "8.9061\t6.7335\t6.1381\t5.9748\t5.9221\t5.8573\t\n",
      "Training Epoch: 14 [650/569]\tLoss: 37.4097\tLR: 0.010000\n",
      "8.3846\t6.2238\t5.8271\t5.7136\t5.6335\t5.6271\t\n",
      "Training Epoch: 14 [666/569]\tLoss: 39.4967\tLR: 0.010000\n",
      "8.8792\t6.7104\t6.0920\t5.9835\t5.9190\t5.9127\t\n",
      "Training Epoch: 14 [682/569]\tLoss: 34.3284\tLR: 0.010000\n",
      "6.5844\t5.4447\t5.3557\t5.5264\t5.6568\t5.7603\t\n",
      "Training Epoch: 14 [698/569]\tLoss: 39.3391\tLR: 0.010000\n",
      "9.2742\t6.7954\t6.0785\t5.7964\t5.7099\t5.6847\t\n",
      "Training Epoch: 14 [714/569]\tLoss: 39.2768\tLR: 0.010000\n",
      "8.7549\t6.5902\t6.1039\t5.9545\t5.9356\t5.9377\t\n",
      "Training Epoch: 14 [730/569]\tLoss: 39.0579\tLR: 0.010000\n",
      "8.6433\t6.5618\t6.0163\t5.9422\t5.9291\t5.9653\t\n",
      "Training Epoch: 14 [746/569]\tLoss: 40.0409\tLR: 0.010000\n",
      "9.2603\t6.9566\t6.1608\t5.9299\t5.8730\t5.8603\t\n",
      "Training Epoch: 14 [762/569]\tLoss: 37.0015\tLR: 0.010000\n",
      "7.2520\t5.9698\t5.8984\t5.8984\t5.9699\t6.0128\t\n",
      "Training Epoch: 14 [778/569]\tLoss: 42.4771\tLR: 0.010000\n",
      "10.6855\t7.1704\t6.1561\t6.0903\t6.1677\t6.2072\t\n",
      "Training Epoch: 14 [794/569]\tLoss: 31.1495\tLR: 0.010000\n",
      "6.0425\t4.9189\t4.9275\t5.0126\t5.0893\t5.1587\t\n",
      "Training Epoch: 14 [810/569]\tLoss: 34.4999\tLR: 0.010000\n",
      "6.4639\t5.3954\t5.3985\t5.6633\t5.7684\t5.8104\t\n",
      "Training Epoch: 14 [826/569]\tLoss: 35.2757\tLR: 0.010000\n",
      "7.2717\t5.5475\t5.5270\t5.6182\t5.6458\t5.6654\t\n",
      "Training Epoch: 14 [842/569]\tLoss: 35.5277\tLR: 0.010000\n",
      "8.1906\t5.8633\t5.3755\t5.2938\t5.3648\t5.4397\t\n",
      "Training Epoch: 14 [858/569]\tLoss: 34.8072\tLR: 0.010000\n",
      "6.6071\t5.3434\t5.3724\t5.6588\t5.8626\t5.9630\t\n",
      "Training Epoch: 14 [874/569]\tLoss: 32.4893\tLR: 0.010000\n",
      "6.1919\t5.2174\t5.1953\t5.2128\t5.2874\t5.3846\t\n",
      "Training Epoch: 14 [890/569]\tLoss: 34.2322\tLR: 0.010000\n",
      "6.5244\t5.4907\t5.4282\t5.5417\t5.6194\t5.6280\t\n",
      "Training Epoch: 14 [905/569]\tLoss: 36.8405\tLR: 0.010000\n",
      "7.9087\t6.0869\t5.6928\t5.6796\t5.7173\t5.7553\t\n",
      "[0.47044241428375244, 0.3527451157569885, 0.17681246995925903, 0.002789232414215803, 0.8172221779823303, 0.015541437081992626, 0.16723638493567705, 0.06387773156166077, 0.5554249286651611, 0.3676464259624481, 0.07692864537239075, -0.027055561542510986, 0.5936406254768372, 0.274971067905426, 0.13138830661773682, -0.04293167218565941, 0.5881799459457397, 0.0, 0.41182005405426025, -0.026756063103675842]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7689, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.6010, Accuracy: 0.0685\n",
      "Test set t = 02: Average loss: 0.5736, Accuracy: 0.0685\n",
      "Test set t = 03: Average loss: 0.5770, Accuracy: 0.0668\n",
      "Test set t = 04: Average loss: 0.5821, Accuracy: 0.0703\n",
      "Test set t = 05: Average loss: 0.5850, Accuracy: 0.0685\n",
      "\n",
      "Training Epoch: 15 [10/569]\tLoss: 34.9656\tLR: 0.010000\n",
      "6.9268\t5.6085\t5.4283\t5.5565\t5.6775\t5.7680\t\n",
      "Training Epoch: 15 [26/569]\tLoss: 32.6582\tLR: 0.010000\n",
      "7.1943\t5.5392\t5.0370\t4.9533\t4.9537\t4.9808\t\n",
      "Training Epoch: 15 [42/569]\tLoss: 28.5042\tLR: 0.010000\n",
      "5.0764\t4.1514\t4.4856\t4.8071\t4.9372\t5.0465\t\n",
      "Training Epoch: 15 [58/569]\tLoss: 36.0439\tLR: 0.010000\n",
      "7.5631\t5.8327\t5.5908\t5.5898\t5.6769\t5.7906\t\n",
      "Training Epoch: 15 [74/569]\tLoss: 31.9978\tLR: 0.010000\n",
      "6.4302\t4.9223\t4.8580\t5.1046\t5.2909\t5.3917\t\n",
      "Training Epoch: 15 [90/569]\tLoss: 36.9040\tLR: 0.010000\n",
      "7.7230\t5.9866\t5.7321\t5.8070\t5.8251\t5.8304\t\n",
      "Training Epoch: 15 [106/569]\tLoss: 39.1981\tLR: 0.010000\n",
      "8.4671\t6.5833\t6.1121\t5.9978\t6.0158\t6.0219\t\n",
      "Training Epoch: 15 [122/569]\tLoss: 38.6882\tLR: 0.010000\n",
      "7.3330\t6.4174\t6.3593\t6.2301\t6.1754\t6.1730\t\n",
      "Training Epoch: 15 [138/569]\tLoss: 34.4360\tLR: 0.010000\n",
      "6.7189\t5.4971\t5.4187\t5.5393\t5.6167\t5.6453\t\n",
      "Training Epoch: 15 [154/569]\tLoss: 33.8576\tLR: 0.010000\n",
      "5.8098\t5.1955\t5.4528\t5.7589\t5.8182\t5.8222\t\n",
      "Training Epoch: 15 [170/569]\tLoss: 33.3055\tLR: 0.010000\n",
      "5.8230\t5.0469\t5.2791\t5.5617\t5.7571\t5.8377\t\n",
      "Training Epoch: 15 [186/569]\tLoss: 35.6089\tLR: 0.010000\n",
      "7.5491\t5.8544\t5.6254\t5.5512\t5.5183\t5.5105\t\n",
      "Training Epoch: 15 [202/569]\tLoss: 41.9772\tLR: 0.010000\n",
      "10.2959\t7.1904\t6.3431\t6.0814\t6.0382\t6.0282\t\n",
      "Training Epoch: 15 [218/569]\tLoss: 34.9749\tLR: 0.010000\n",
      "8.3372\t5.8851\t5.1806\t5.0920\t5.1771\t5.3028\t\n",
      "Training Epoch: 15 [234/569]\tLoss: 38.0519\tLR: 0.010000\n",
      "8.0842\t6.1656\t5.9119\t5.9498\t5.9766\t5.9639\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [250/569]\tLoss: 39.5288\tLR: 0.010000\n",
      "8.7039\t6.4956\t6.0566\t6.0548\t6.0927\t6.1251\t\n",
      "Training Epoch: 15 [266/569]\tLoss: 45.0369\tLR: 0.010000\n",
      "10.1001\t8.1412\t7.3782\t6.8299\t6.3954\t6.1921\t\n",
      "Training Epoch: 15 [282/569]\tLoss: 35.5580\tLR: 0.010000\n",
      "8.0912\t5.9557\t5.4717\t5.3717\t5.3457\t5.3220\t\n",
      "Training Epoch: 15 [298/569]\tLoss: 31.3554\tLR: 0.010000\n",
      "6.3194\t4.9846\t4.9320\t5.0227\t5.0379\t5.0588\t\n",
      "Training Epoch: 15 [314/569]\tLoss: 36.3202\tLR: 0.010000\n",
      "7.5403\t6.0418\t5.6491\t5.6624\t5.7061\t5.7205\t\n",
      "Training Epoch: 15 [330/569]\tLoss: 38.9179\tLR: 0.010000\n",
      "8.6411\t6.3958\t5.9320\t5.9975\t5.9980\t5.9536\t\n",
      "Training Epoch: 15 [346/569]\tLoss: 35.1006\tLR: 0.010000\n",
      "7.1211\t5.5516\t5.3602\t5.5451\t5.7014\t5.8211\t\n",
      "Training Epoch: 15 [362/569]\tLoss: 33.3902\tLR: 0.010000\n",
      "7.2329\t5.3420\t5.0461\t5.1444\t5.2635\t5.3613\t\n",
      "Training Epoch: 15 [378/569]\tLoss: 33.8551\tLR: 0.010000\n",
      "6.2810\t5.1353\t5.3581\t5.6222\t5.7058\t5.7526\t\n",
      "Training Epoch: 15 [394/569]\tLoss: 37.4491\tLR: 0.010000\n",
      "8.2333\t6.2987\t5.7617\t5.6590\t5.7161\t5.7803\t\n",
      "Training Epoch: 15 [410/569]\tLoss: 45.2073\tLR: 0.010000\n",
      "9.7907\t7.5234\t6.9983\t6.8883\t6.9708\t7.0357\t\n",
      "Training Epoch: 15 [426/569]\tLoss: 34.0249\tLR: 0.010000\n",
      "6.4593\t5.3792\t5.3052\t5.4945\t5.6669\t5.7197\t\n",
      "Training Epoch: 15 [442/569]\tLoss: 44.4571\tLR: 0.010000\n",
      "10.3611\t7.4725\t6.8271\t6.6693\t6.5944\t6.5327\t\n",
      "Training Epoch: 15 [458/569]\tLoss: 39.0066\tLR: 0.010000\n",
      "7.4735\t5.9811\t5.9039\t6.3722\t6.6366\t6.6393\t\n",
      "Training Epoch: 15 [474/569]\tLoss: 41.5013\tLR: 0.010000\n",
      "9.1293\t6.9014\t6.3867\t6.3774\t6.3699\t6.3367\t\n",
      "Training Epoch: 15 [490/569]\tLoss: 35.1966\tLR: 0.010000\n",
      "7.1881\t5.6325\t5.3651\t5.5028\t5.6931\t5.8149\t\n",
      "Training Epoch: 15 [506/569]\tLoss: 35.1540\tLR: 0.010000\n",
      "6.9987\t5.2287\t5.4162\t5.7073\t5.8720\t5.9310\t\n",
      "Training Epoch: 15 [522/569]\tLoss: 33.2486\tLR: 0.010000\n",
      "6.3371\t5.2853\t5.2724\t5.3668\t5.4583\t5.5286\t\n",
      "Training Epoch: 15 [538/569]\tLoss: 39.5957\tLR: 0.010000\n",
      "8.4388\t6.5759\t6.2297\t6.1813\t6.1211\t6.0489\t\n",
      "Training Epoch: 15 [554/569]\tLoss: 36.0821\tLR: 0.010000\n",
      "7.4872\t5.8547\t5.6820\t5.6973\t5.6830\t5.6778\t\n",
      "Training Epoch: 15 [570/569]\tLoss: 38.1232\tLR: 0.010000\n",
      "8.2416\t6.2833\t5.8276\t5.8724\t5.9423\t5.9560\t\n",
      "Training Epoch: 15 [586/569]\tLoss: 36.2729\tLR: 0.010000\n",
      "7.7827\t5.9199\t5.5283\t5.5738\t5.7062\t5.7620\t\n",
      "Training Epoch: 15 [602/569]\tLoss: 37.7608\tLR: 0.010000\n",
      "8.1503\t6.2610\t5.8309\t5.7905\t5.8455\t5.8825\t\n",
      "Training Epoch: 15 [618/569]\tLoss: 35.7947\tLR: 0.010000\n",
      "7.1509\t5.9606\t5.7446\t5.6563\t5.6384\t5.6440\t\n",
      "Training Epoch: 15 [634/569]\tLoss: 39.4243\tLR: 0.010000\n",
      "8.1718\t6.5027\t6.2431\t6.1491\t6.1639\t6.1937\t\n",
      "Training Epoch: 15 [650/569]\tLoss: 38.2913\tLR: 0.010000\n",
      "8.2530\t6.1540\t5.8755\t5.9412\t6.0082\t6.0592\t\n",
      "Training Epoch: 15 [666/569]\tLoss: 40.7215\tLR: 0.010000\n",
      "8.5049\t6.3926\t6.2947\t6.4942\t6.5219\t6.5132\t\n",
      "Training Epoch: 15 [682/569]\tLoss: 36.6729\tLR: 0.010000\n",
      "7.5985\t6.0616\t5.8462\t5.8128\t5.7008\t5.6530\t\n",
      "Training Epoch: 15 [698/569]\tLoss: 37.2102\tLR: 0.010000\n",
      "7.2949\t6.0271\t5.8770\t6.0057\t6.0209\t5.9845\t\n",
      "Training Epoch: 15 [714/569]\tLoss: 37.6336\tLR: 0.010000\n",
      "7.3721\t5.9657\t5.9681\t6.0538\t6.1188\t6.1551\t\n",
      "Training Epoch: 15 [730/569]\tLoss: 37.5981\tLR: 0.010000\n",
      "7.9896\t6.1787\t5.8088\t5.7635\t5.8958\t5.9618\t\n",
      "Training Epoch: 15 [746/569]\tLoss: 38.7515\tLR: 0.010000\n",
      "7.7473\t6.2551\t6.1197\t6.1617\t6.2138\t6.2538\t\n",
      "Training Epoch: 15 [762/569]\tLoss: 37.9527\tLR: 0.010000\n",
      "8.0453\t6.4422\t5.9848\t5.8595\t5.8059\t5.8150\t\n",
      "Training Epoch: 15 [778/569]\tLoss: 45.4872\tLR: 0.010000\n",
      "10.8653\t7.7220\t6.8679\t6.7270\t6.6613\t6.6437\t\n",
      "Training Epoch: 15 [794/569]\tLoss: 36.7100\tLR: 0.010000\n",
      "6.1490\t5.8153\t6.0676\t6.2070\t6.2364\t6.2348\t\n",
      "Training Epoch: 15 [810/569]\tLoss: 43.9321\tLR: 0.010000\n",
      "10.8687\t7.7572\t6.6843\t6.2841\t6.1860\t6.1517\t\n",
      "Training Epoch: 15 [826/569]\tLoss: 30.5644\tLR: 0.010000\n",
      "5.8954\t4.7793\t4.7599\t4.9104\t5.0657\t5.1537\t\n",
      "Training Epoch: 15 [842/569]\tLoss: 35.2592\tLR: 0.010000\n",
      "8.2301\t5.8646\t5.3388\t5.2429\t5.2652\t5.3176\t\n",
      "Training Epoch: 15 [858/569]\tLoss: 31.7585\tLR: 0.010000\n",
      "5.2092\t4.8298\t5.0204\t5.4213\t5.6096\t5.6682\t\n",
      "Training Epoch: 15 [874/569]\tLoss: 38.2550\tLR: 0.010000\n",
      "8.1222\t6.6536\t6.0646\t5.8301\t5.7963\t5.7883\t\n",
      "Training Epoch: 15 [890/569]\tLoss: 28.4361\tLR: 0.010000\n",
      "4.5691\t4.2663\t4.6665\t4.8796\t4.9926\t5.0619\t\n",
      "Training Epoch: 15 [905/569]\tLoss: 34.5183\tLR: 0.010000\n",
      "8.0674\t5.6121\t5.1115\t5.1754\t5.2599\t5.2921\t\n",
      "[0.4901473820209503, 0.33736005425453186, 0.17249256372451782, 0.002140298020094633, 0.8213030695915222, 0.013929353095591068, 0.16476757731288671, 0.06767050921916962, 0.5557732582092285, 0.36977577209472656, 0.07445096969604492, -0.029016884043812752, 0.5942725539207458, 0.28226539492607117, 0.12346205115318298, -0.04570498690009117, 0.6035208106040955, 0.0, 0.39647918939590454, -0.029125453904271126]\n",
      "\n",
      "Test set t = 00: Average loss: 0.7691, Accuracy: 0.0685\n",
      "Test set t = 01: Average loss: 0.5992, Accuracy: 0.0703\n",
      "Test set t = 02: Average loss: 0.5745, Accuracy: 0.0650\n",
      "Test set t = 03: Average loss: 0.5776, Accuracy: 0.0685\n",
      "Test set t = 04: Average loss: 0.5816, Accuracy: 0.0650\n",
      "Test set t = 05: Average loss: 0.5839, Accuracy: 0.0598\n",
      "\n",
      "=====================\n",
      "Babble2Spkr, for SNR -6.0\n",
      "=====================\n",
      "\n",
      "Test set t = 00: Average loss: 0.6740, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.6042, Accuracy: 0.1002\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6741, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.6044, Accuracy: 0.1002\n",
      "Test set t = 02: Average loss: 0.5519, Accuracy: 0.1037\n",
      "Test set t = 03: Average loss: 0.5318, Accuracy: 0.1037\n",
      "Test set t = 04: Average loss: 0.5382, Accuracy: 0.0861\n",
      "Test set t = 05: Average loss: 0.5645, Accuracy: 0.0984\n",
      "\n",
      "Training Epoch: 1 [10/569]\tLoss: 36.4673\tLR: 0.010000\n",
      "7.3750\t6.4436\t5.7014\t5.4474\t5.5226\t5.9774\t\n",
      "Training Epoch: 1 [26/569]\tLoss: 37.9805\tLR: 0.010000\n",
      "8.2948\t6.9837\t5.9732\t5.5164\t5.4784\t5.7339\t\n",
      "Training Epoch: 1 [42/569]\tLoss: 28.0866\tLR: 0.010000\n",
      "5.1625\t4.6369\t4.3291\t4.3631\t4.6406\t4.9544\t\n",
      "Training Epoch: 1 [58/569]\tLoss: 29.0448\tLR: 0.010000\n",
      "5.3810\t4.8365\t4.5249\t4.5259\t4.7126\t5.0639\t\n",
      "Training Epoch: 1 [74/569]\tLoss: 26.2114\tLR: 0.010000\n",
      "4.7174\t4.3938\t4.1484\t4.0749\t4.2758\t4.6011\t\n",
      "Training Epoch: 1 [90/569]\tLoss: 32.6114\tLR: 0.010000\n",
      "6.3553\t5.5518\t5.1148\t5.0247\t5.1495\t5.4152\t\n",
      "Training Epoch: 1 [106/569]\tLoss: 36.8592\tLR: 0.010000\n",
      "6.9503\t6.4404\t5.9187\t5.6430\t5.7898\t6.1170\t\n",
      "Training Epoch: 1 [122/569]\tLoss: 38.1856\tLR: 0.010000\n",
      "8.4658\t7.3052\t6.2062\t5.5640\t5.3338\t5.3105\t\n",
      "Training Epoch: 1 [138/569]\tLoss: 35.0705\tLR: 0.010000\n",
      "7.2210\t6.3097\t5.6231\t5.2696\t5.2217\t5.4254\t\n",
      "Training Epoch: 1 [154/569]\tLoss: 34.2154\tLR: 0.010000\n",
      "6.5102\t5.9854\t5.4475\t5.1950\t5.3366\t5.7407\t\n",
      "Training Epoch: 1 [170/569]\tLoss: 38.4437\tLR: 0.010000\n",
      "8.2150\t7.2441\t6.3392\t5.7773\t5.5202\t5.3479\t\n",
      "Training Epoch: 1 [186/569]\tLoss: 42.6283\tLR: 0.010000\n",
      "8.7483\t7.7720\t6.9794\t6.5222\t6.3042\t6.3021\t\n",
      "Training Epoch: 1 [202/569]\tLoss: 41.5699\tLR: 0.010000\n",
      "8.8936\t7.8387\t6.8669\t6.2030\t5.9009\t5.8669\t\n",
      "Training Epoch: 1 [218/569]\tLoss: 33.8367\tLR: 0.010000\n",
      "6.5194\t6.0129\t5.6082\t5.3562\t5.2108\t5.1293\t\n",
      "Training Epoch: 1 [234/569]\tLoss: 28.5748\tLR: 0.010000\n",
      "4.8715\t4.5951\t4.4830\t4.5842\t4.8506\t5.1904\t\n",
      "Training Epoch: 1 [250/569]\tLoss: 36.0437\tLR: 0.010000\n",
      "6.9390\t6.3099\t5.8801\t5.6267\t5.5776\t5.7104\t\n",
      "Training Epoch: 1 [266/569]\tLoss: 38.0082\tLR: 0.010000\n",
      "7.7744\t6.8506\t6.0790\t5.7449\t5.6828\t5.8764\t\n",
      "Training Epoch: 1 [282/569]\tLoss: 25.9379\tLR: 0.010000\n",
      "4.9591\t4.4220\t3.9459\t3.8674\t4.1875\t4.5561\t\n",
      "Training Epoch: 1 [298/569]\tLoss: 36.1480\tLR: 0.010000\n",
      "7.2187\t6.4612\t5.8580\t5.5090\t5.4236\t5.6775\t\n",
      "Training Epoch: 1 [314/569]\tLoss: 28.2105\tLR: 0.010000\n",
      "5.3218\t4.8030\t4.4209\t4.3139\t4.4991\t4.8518\t\n",
      "Training Epoch: 1 [330/569]\tLoss: 37.4598\tLR: 0.010000\n",
      "7.5795\t6.8704\t6.1504\t5.7429\t5.5943\t5.5221\t\n",
      "Training Epoch: 1 [346/569]\tLoss: 33.7515\tLR: 0.010000\n",
      "6.1651\t5.7025\t5.3518\t5.2612\t5.4722\t5.7988\t\n",
      "Training Epoch: 1 [362/569]\tLoss: 39.4849\tLR: 0.010000\n",
      "7.5972\t6.7262\t6.1334\t6.0323\t6.2621\t6.7337\t\n",
      "Training Epoch: 1 [378/569]\tLoss: 33.6173\tLR: 0.010000\n",
      "6.4574\t5.8594\t5.3729\t5.1710\t5.2639\t5.4927\t\n",
      "Training Epoch: 1 [394/569]\tLoss: 31.7933\tLR: 0.010000\n",
      "5.4324\t5.2818\t5.2146\t5.3063\t5.2598\t5.2983\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [410/569]\tLoss: 37.1957\tLR: 0.010000\n",
      "7.8796\t6.8546\t5.9507\t5.5164\t5.4303\t5.5642\t\n",
      "Training Epoch: 1 [426/569]\tLoss: 34.6828\tLR: 0.010000\n",
      "7.1578\t6.2033\t5.4833\t5.1767\t5.2721\t5.3897\t\n",
      "Training Epoch: 1 [442/569]\tLoss: 36.2927\tLR: 0.010000\n",
      "7.6919\t6.6462\t5.8187\t5.3714\t5.2883\t5.4763\t\n",
      "Training Epoch: 1 [458/569]\tLoss: 32.0021\tLR: 0.010000\n",
      "6.0654\t5.5785\t5.2026\t5.0301\t5.0192\t5.1064\t\n",
      "Training Epoch: 1 [474/569]\tLoss: 35.4667\tLR: 0.010000\n",
      "7.0073\t6.2168\t5.6884\t5.5059\t5.4744\t5.5740\t\n",
      "Training Epoch: 1 [490/569]\tLoss: 33.9775\tLR: 0.010000\n",
      "6.2881\t5.6637\t5.3113\t5.2819\t5.5263\t5.9062\t\n",
      "Training Epoch: 1 [506/569]\tLoss: 41.8289\tLR: 0.010000\n",
      "8.6541\t7.7318\t6.9115\t6.3716\t6.1069\t6.0529\t\n",
      "Training Epoch: 1 [522/569]\tLoss: 32.4760\tLR: 0.010000\n",
      "5.4689\t5.2035\t5.1666\t5.3351\t5.5207\t5.7813\t\n",
      "Training Epoch: 1 [538/569]\tLoss: 36.6542\tLR: 0.010000\n",
      "7.5390\t6.7994\t6.0813\t5.6202\t5.3527\t5.2618\t\n",
      "Training Epoch: 1 [554/569]\tLoss: 38.9962\tLR: 0.010000\n",
      "8.0143\t7.0510\t6.3409\t5.9687\t5.8079\t5.8135\t\n",
      "Training Epoch: 1 [570/569]\tLoss: 31.9744\tLR: 0.010000\n",
      "6.0681\t5.4576\t5.0849\t4.9718\t5.0821\t5.3100\t\n",
      "Training Epoch: 1 [586/569]\tLoss: 38.5386\tLR: 0.010000\n",
      "7.5025\t6.6676\t6.2011\t6.0028\t5.9884\t6.1762\t\n",
      "Training Epoch: 1 [602/569]\tLoss: 29.5110\tLR: 0.010000\n",
      "5.6049\t4.9977\t4.5752\t4.5150\t4.7125\t5.1056\t\n",
      "Training Epoch: 1 [618/569]\tLoss: 37.2552\tLR: 0.010000\n",
      "7.3222\t6.7250\t6.1901\t5.8218\t5.6386\t5.5575\t\n",
      "Training Epoch: 1 [634/569]\tLoss: 34.9409\tLR: 0.010000\n",
      "6.3139\t5.8423\t5.4664\t5.4485\t5.7299\t6.1399\t\n",
      "Training Epoch: 1 [650/569]\tLoss: 33.0716\tLR: 0.010000\n",
      "5.9865\t5.4279\t5.1511\t5.1652\t5.4724\t5.8684\t\n",
      "Training Epoch: 1 [666/569]\tLoss: 37.4402\tLR: 0.010000\n",
      "7.0185\t6.4642\t6.0774\t5.9522\t5.8759\t6.0519\t\n",
      "Training Epoch: 1 [682/569]\tLoss: 25.5367\tLR: 0.010000\n",
      "4.2771\t4.0588\t3.9386\t4.0854\t4.4045\t4.7723\t\n",
      "Training Epoch: 1 [698/569]\tLoss: 33.7903\tLR: 0.010000\n",
      "6.7878\t5.9422\t5.3715\t5.1220\t5.1536\t5.4131\t\n",
      "Training Epoch: 1 [714/569]\tLoss: 30.1372\tLR: 0.010000\n",
      "5.6044\t5.0829\t4.8712\t4.8403\t4.8269\t4.9115\t\n",
      "Training Epoch: 1 [730/569]\tLoss: 40.2684\tLR: 0.010000\n",
      "8.0758\t7.3039\t6.5598\t6.1769\t6.0793\t6.0727\t\n",
      "Training Epoch: 1 [746/569]\tLoss: 39.2335\tLR: 0.010000\n",
      "7.4099\t6.7796\t6.3292\t6.1727\t6.2038\t6.3383\t\n",
      "Training Epoch: 1 [762/569]\tLoss: 36.0773\tLR: 0.010000\n",
      "7.4845\t6.5063\t5.7323\t5.4337\t5.4044\t5.5160\t\n",
      "Training Epoch: 1 [778/569]\tLoss: 33.8513\tLR: 0.010000\n",
      "7.2250\t6.2673\t5.4465\t5.0198\t4.9116\t4.9813\t\n",
      "Training Epoch: 1 [794/569]\tLoss: 27.6406\tLR: 0.010000\n",
      "4.6485\t4.4316\t4.2671\t4.3352\t4.7306\t5.2276\t\n",
      "Training Epoch: 1 [810/569]\tLoss: 29.8866\tLR: 0.010000\n",
      "6.4198\t5.4471\t4.6436\t4.3701\t4.4316\t4.5744\t\n",
      "Training Epoch: 1 [826/569]\tLoss: 40.8027\tLR: 0.010000\n",
      "8.2801\t7.4310\t6.7233\t6.2747\t6.0583\t6.0355\t\n",
      "Training Epoch: 1 [842/569]\tLoss: 35.9956\tLR: 0.010000\n",
      "7.1419\t6.4774\t5.9153\t5.5380\t5.4313\t5.4917\t\n",
      "Training Epoch: 1 [858/569]\tLoss: 29.8848\tLR: 0.010000\n",
      "5.5249\t5.0367\t4.6997\t4.7421\t4.8538\t5.0277\t\n",
      "Training Epoch: 1 [874/569]\tLoss: 31.7441\tLR: 0.010000\n",
      "6.3321\t5.6334\t5.1232\t4.9069\t4.7963\t4.9521\t\n",
      "Training Epoch: 1 [890/569]\tLoss: 32.1887\tLR: 0.010000\n",
      "6.1296\t5.5888\t5.2180\t5.0543\t5.0393\t5.1588\t\n",
      "Training Epoch: 1 [905/569]\tLoss: 32.2977\tLR: 0.010000\n",
      "5.3183\t5.1488\t5.1192\t5.3049\t5.5511\t5.8555\t\n",
      "[0.2957804203033447, 0.36200574040412903, 0.34221383929252625, 0.011659451760351658, 0.42240041494369507, 0.24435590207576752, 0.3332436829805374, 0.014011753723025322, 0.39183467626571655, 0.233346089720726, 0.37481923401355743, 0.006347297690808773, 0.3322318494319916, 0.271314412355423, 0.39645373821258545, 0.005733714438974857, 0.30205807089805603, 0.0, 0.697941929101944, 0.009318086318671703]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6742, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.6039, Accuracy: 0.1002\n",
      "Test set t = 02: Average loss: 0.5503, Accuracy: 0.1037\n",
      "Test set t = 03: Average loss: 0.5283, Accuracy: 0.1037\n",
      "Test set t = 04: Average loss: 0.5296, Accuracy: 0.0931\n",
      "Test set t = 05: Average loss: 0.5443, Accuracy: 0.0914\n",
      "\n",
      "Training Epoch: 2 [10/569]\tLoss: 30.1725\tLR: 0.010000\n",
      "6.2071\t5.3683\t4.7405\t4.4724\t4.5304\t4.8539\t\n",
      "Training Epoch: 2 [26/569]\tLoss: 29.5809\tLR: 0.010000\n",
      "5.9485\t5.1810\t4.5951\t4.4417\t4.5928\t4.8219\t\n",
      "Training Epoch: 2 [42/569]\tLoss: 36.1585\tLR: 0.010000\n",
      "7.9797\t6.9101\t5.9408\t5.3256\t5.0488\t4.9534\t\n",
      "Training Epoch: 2 [58/569]\tLoss: 29.9128\tLR: 0.010000\n",
      "5.0333\t4.7554\t4.5755\t4.7054\t5.1228\t5.7203\t\n",
      "Training Epoch: 2 [74/569]\tLoss: 26.0105\tLR: 0.010000\n",
      "4.6374\t4.2715\t4.0568\t4.0096\t4.2638\t4.7715\t\n",
      "Training Epoch: 2 [90/569]\tLoss: 34.4460\tLR: 0.010000\n",
      "6.5418\t6.0029\t5.6344\t5.4889\t5.3937\t5.3843\t\n",
      "Training Epoch: 2 [106/569]\tLoss: 33.4866\tLR: 0.010000\n",
      "6.3153\t5.7844\t5.4198\t5.2120\t5.2872\t5.4679\t\n",
      "Training Epoch: 2 [122/569]\tLoss: 31.9098\tLR: 0.010000\n",
      "5.6212\t5.3052\t5.0587\t5.0701\t5.2724\t5.5823\t\n",
      "Training Epoch: 2 [138/569]\tLoss: 37.6896\tLR: 0.010000\n",
      "7.5863\t6.7819\t6.0976\t5.8178\t5.6799\t5.7261\t\n",
      "Training Epoch: 2 [154/569]\tLoss: 34.2085\tLR: 0.010000\n",
      "6.5959\t5.9471\t5.4997\t5.2900\t5.3296\t5.5463\t\n",
      "Training Epoch: 2 [170/569]\tLoss: 43.5813\tLR: 0.010000\n",
      "9.1292\t7.9610\t7.0429\t6.5860\t6.4373\t6.4249\t\n",
      "Training Epoch: 2 [186/569]\tLoss: 23.3764\tLR: 0.010000\n",
      "3.5799\t3.4280\t3.5375\t3.8856\t4.2659\t4.6795\t\n",
      "Training Epoch: 2 [202/569]\tLoss: 26.4650\tLR: 0.010000\n",
      "4.6509\t4.3513\t4.1455\t4.2268\t4.4203\t4.6701\t\n",
      "Training Epoch: 2 [218/569]\tLoss: 39.4358\tLR: 0.010000\n",
      "7.5129\t6.9435\t6.4223\t6.1886\t6.1403\t6.2282\t\n",
      "Training Epoch: 2 [234/569]\tLoss: 29.5924\tLR: 0.010000\n",
      "5.8410\t5.1674\t4.7112\t4.5492\t4.5730\t4.7506\t\n",
      "Training Epoch: 2 [250/569]\tLoss: 43.7948\tLR: 0.010000\n",
      "9.5840\t8.3666\t7.2318\t6.5050\t6.1531\t5.9542\t\n",
      "Training Epoch: 2 [266/569]\tLoss: 39.1389\tLR: 0.010000\n",
      "7.9106\t7.0900\t6.4137\t6.0522\t5.8346\t5.8377\t\n",
      "Training Epoch: 2 [282/569]\tLoss: 40.5889\tLR: 0.010000\n",
      "8.0312\t7.2807\t6.7031\t6.3332\t6.1521\t6.0886\t\n",
      "Training Epoch: 2 [298/569]\tLoss: 34.1747\tLR: 0.010000\n",
      "6.9197\t6.1745\t5.5661\t5.2256\t5.1257\t5.1631\t\n",
      "Training Epoch: 2 [314/569]\tLoss: 32.8423\tLR: 0.010000\n",
      "5.6103\t5.4003\t5.2605\t5.2462\t5.4371\t5.8879\t\n",
      "Training Epoch: 2 [330/569]\tLoss: 29.6657\tLR: 0.010000\n",
      "5.2848\t4.9440\t4.7134\t4.7331\t4.9033\t5.0871\t\n",
      "Training Epoch: 2 [346/569]\tLoss: 40.1712\tLR: 0.010000\n",
      "8.1741\t7.3298\t6.5797\t6.1590\t5.9776\t5.9511\t\n",
      "Training Epoch: 2 [362/569]\tLoss: 34.5308\tLR: 0.010000\n",
      "6.7584\t5.9632\t5.4863\t5.3386\t5.4361\t5.5482\t\n",
      "Training Epoch: 2 [378/569]\tLoss: 28.1647\tLR: 0.010000\n",
      "5.5595\t4.9139\t4.4399\t4.2707\t4.3637\t4.6172\t\n",
      "Training Epoch: 2 [394/569]\tLoss: 35.3280\tLR: 0.010000\n",
      "7.1743\t6.4216\t5.7905\t5.4015\t5.2760\t5.2642\t\n",
      "Training Epoch: 2 [410/569]\tLoss: 35.0295\tLR: 0.010000\n",
      "6.6047\t6.0334\t5.6288\t5.4711\t5.5636\t5.7280\t\n",
      "Training Epoch: 2 [426/569]\tLoss: 26.6460\tLR: 0.010000\n",
      "4.8048\t4.3777\t4.1825\t4.2226\t4.4262\t4.6322\t\n",
      "Training Epoch: 2 [442/569]\tLoss: 36.9682\tLR: 0.010000\n",
      "7.3191\t6.5598\t5.9980\t5.7295\t5.6731\t5.6888\t\n",
      "Training Epoch: 2 [458/569]\tLoss: 28.6504\tLR: 0.010000\n",
      "5.1197\t4.7131\t4.4814\t4.5415\t4.7699\t5.0247\t\n",
      "Training Epoch: 2 [474/569]\tLoss: 38.4515\tLR: 0.010000\n",
      "7.7153\t6.9580\t6.2765\t5.9177\t5.7602\t5.8239\t\n",
      "Training Epoch: 2 [490/569]\tLoss: 41.4866\tLR: 0.010000\n",
      "9.0064\t7.7880\t6.8016\t6.2523\t5.8974\t5.7410\t\n",
      "Training Epoch: 2 [506/569]\tLoss: 30.5420\tLR: 0.010000\n",
      "5.8393\t5.3028\t4.8536\t4.6770\t4.8020\t5.0673\t\n",
      "Training Epoch: 2 [522/569]\tLoss: 31.5615\tLR: 0.010000\n",
      "6.0625\t5.4480\t5.0283\t4.9148\t4.9981\t5.1098\t\n",
      "Training Epoch: 2 [538/569]\tLoss: 38.2369\tLR: 0.010000\n",
      "7.6292\t6.7763\t6.0896\t5.8086\t5.8634\t6.0697\t\n",
      "Training Epoch: 2 [554/569]\tLoss: 36.0552\tLR: 0.010000\n",
      "7.0951\t6.3978\t5.8774\t5.5922\t5.4688\t5.6238\t\n",
      "Training Epoch: 2 [570/569]\tLoss: 38.2873\tLR: 0.010000\n",
      "7.8351\t6.9076\t6.1450\t5.8246\t5.7783\t5.7968\t\n",
      "Training Epoch: 2 [586/569]\tLoss: 34.1711\tLR: 0.010000\n",
      "7.1982\t6.2648\t5.5172\t5.1150\t5.0251\t5.0508\t\n",
      "Training Epoch: 2 [602/569]\tLoss: 28.9075\tLR: 0.010000\n",
      "6.0471\t5.2786\t4.6453\t4.3235\t4.2554\t4.3576\t\n",
      "Training Epoch: 2 [618/569]\tLoss: 32.0990\tLR: 0.010000\n",
      "6.0955\t5.4002\t5.0414\t5.0406\t5.1530\t5.3684\t\n",
      "Training Epoch: 2 [634/569]\tLoss: 37.9867\tLR: 0.010000\n",
      "7.3104\t6.6526\t6.1507\t5.9270\t5.8865\t6.0595\t\n",
      "Training Epoch: 2 [650/569]\tLoss: 35.9453\tLR: 0.010000\n",
      "7.1014\t6.2873\t5.7381\t5.5418\t5.6041\t5.6727\t\n",
      "Training Epoch: 2 [666/569]\tLoss: 28.5368\tLR: 0.010000\n",
      "5.5473\t4.9348\t4.5431\t4.4434\t4.4944\t4.5737\t\n",
      "Training Epoch: 2 [682/569]\tLoss: 33.4665\tLR: 0.010000\n",
      "6.2530\t5.6846\t5.2372\t5.1209\t5.3590\t5.8118\t\n",
      "Training Epoch: 2 [698/569]\tLoss: 36.7984\tLR: 0.010000\n",
      "7.5286\t6.6789\t5.9381\t5.6086\t5.5138\t5.5304\t\n",
      "Training Epoch: 2 [714/569]\tLoss: 34.9107\tLR: 0.010000\n",
      "7.1295\t6.1551\t5.5388\t5.3052\t5.3364\t5.4456\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [730/569]\tLoss: 34.6053\tLR: 0.010000\n",
      "7.0020\t6.1097\t5.4672\t5.2336\t5.2834\t5.5095\t\n",
      "Training Epoch: 2 [746/569]\tLoss: 41.9999\tLR: 0.010000\n",
      "8.7828\t7.6339\t6.8020\t6.4034\t6.2369\t6.1409\t\n",
      "Training Epoch: 2 [762/569]\tLoss: 35.9619\tLR: 0.010000\n",
      "6.9875\t6.1505\t5.6865\t5.6046\t5.6603\t5.8724\t\n",
      "Training Epoch: 2 [778/569]\tLoss: 34.0665\tLR: 0.010000\n",
      "7.1600\t6.1282\t5.3991\t5.1037\t5.0573\t5.2181\t\n",
      "Training Epoch: 2 [794/569]\tLoss: 38.0069\tLR: 0.010000\n",
      "8.2928\t7.1155\t6.1810\t5.6736\t5.4243\t5.3197\t\n",
      "Training Epoch: 2 [810/569]\tLoss: 36.8718\tLR: 0.010000\n",
      "7.9075\t6.7498\t6.0021\t5.6145\t5.3743\t5.2236\t\n",
      "Training Epoch: 2 [826/569]\tLoss: 28.1322\tLR: 0.010000\n",
      "6.1549\t5.1321\t4.4190\t4.0671\t4.0866\t4.2726\t\n",
      "Training Epoch: 2 [842/569]\tLoss: 30.7855\tLR: 0.010000\n",
      "5.7478\t5.0707\t4.6893\t4.7087\t5.0196\t5.5494\t\n",
      "Training Epoch: 2 [858/569]\tLoss: 34.0913\tLR: 0.010000\n",
      "7.1639\t6.1098\t5.3527\t5.1080\t5.1274\t5.2295\t\n",
      "Training Epoch: 2 [874/569]\tLoss: 30.9443\tLR: 0.010000\n",
      "5.6276\t5.1804\t4.9379\t4.8912\t5.0389\t5.2684\t\n",
      "Training Epoch: 2 [890/569]\tLoss: 31.7551\tLR: 0.010000\n",
      "6.2983\t5.6659\t5.2567\t4.9639\t4.8061\t4.7640\t\n",
      "Training Epoch: 2 [905/569]\tLoss: 36.8867\tLR: 0.010000\n",
      "6.9802\t6.3818\t5.9919\t5.8395\t5.8192\t5.8741\t\n",
      "[0.28440365195274353, 0.4507748782634735, 0.26482146978378296, 0.012939777225255966, 0.54700767993927, 0.20134395360946655, 0.2516483664512634, 0.01771310716867447, 0.5063780546188354, 0.18980976939201355, 0.303812175989151, 0.003649249207228422, 0.3775154650211334, 0.2565699517726898, 0.36591458320617676, 0.0020757182501256466, 0.3166298568248749, 0.0, 0.6833701431751251, 0.009089135564863682]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6742, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5937, Accuracy: 0.1002\n",
      "Test set t = 02: Average loss: 0.5401, Accuracy: 0.1054\n",
      "Test set t = 03: Average loss: 0.5221, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5266, Accuracy: 0.0914\n",
      "Test set t = 05: Average loss: 0.5453, Accuracy: 0.0931\n",
      "\n",
      "Training Epoch: 3 [10/569]\tLoss: 31.4927\tLR: 0.010000\n",
      "5.9538\t5.3718\t4.9874\t4.8818\t4.9992\t5.2988\t\n",
      "Training Epoch: 3 [26/569]\tLoss: 27.7657\tLR: 0.010000\n",
      "5.7981\t4.9725\t4.4530\t4.2242\t4.1658\t4.1520\t\n",
      "Training Epoch: 3 [42/569]\tLoss: 42.1609\tLR: 0.010000\n",
      "8.7300\t7.5979\t6.7986\t6.4540\t6.2889\t6.2913\t\n",
      "Training Epoch: 3 [58/569]\tLoss: 33.4363\tLR: 0.010000\n",
      "6.2667\t5.6227\t5.2616\t5.2345\t5.4107\t5.6401\t\n",
      "Training Epoch: 3 [74/569]\tLoss: 40.1831\tLR: 0.010000\n",
      "8.6123\t7.2771\t6.3327\t5.9094\t5.8668\t6.1848\t\n",
      "Training Epoch: 3 [90/569]\tLoss: 37.4685\tLR: 0.010000\n",
      "8.2393\t7.0481\t6.1013\t5.5539\t5.2734\t5.2526\t\n",
      "Training Epoch: 3 [106/569]\tLoss: 31.5867\tLR: 0.010000\n",
      "6.4363\t5.6189\t5.0683\t4.8160\t4.7887\t4.8585\t\n",
      "Training Epoch: 3 [122/569]\tLoss: 38.2931\tLR: 0.010000\n",
      "7.9325\t6.7784\t5.9844\t5.7464\t5.8306\t6.0208\t\n",
      "Training Epoch: 3 [138/569]\tLoss: 31.3297\tLR: 0.010000\n",
      "5.7802\t5.2308\t4.8959\t4.9132\t5.1228\t5.3868\t\n",
      "Training Epoch: 3 [154/569]\tLoss: 31.8927\tLR: 0.010000\n",
      "5.9522\t5.3182\t4.9939\t5.0162\t5.1586\t5.4536\t\n",
      "Training Epoch: 3 [170/569]\tLoss: 36.1281\tLR: 0.010000\n",
      "7.2646\t6.3296\t5.7146\t5.5520\t5.6121\t5.6552\t\n",
      "Training Epoch: 3 [186/569]\tLoss: 39.4278\tLR: 0.010000\n",
      "8.9061\t7.3306\t6.1743\t5.6733\t5.5839\t5.7595\t\n",
      "Training Epoch: 3 [202/569]\tLoss: 36.7712\tLR: 0.010000\n",
      "7.3108\t6.3908\t5.8739\t5.6505\t5.6345\t5.9106\t\n",
      "Training Epoch: 3 [218/569]\tLoss: 33.0752\tLR: 0.010000\n",
      "5.9064\t5.5248\t5.3324\t5.2893\t5.3880\t5.6343\t\n",
      "Training Epoch: 3 [234/569]\tLoss: 29.5650\tLR: 0.010000\n",
      "5.6021\t4.8973\t4.5497\t4.5496\t4.7877\t5.1785\t\n",
      "Training Epoch: 3 [250/569]\tLoss: 27.7116\tLR: 0.010000\n",
      "5.3680\t4.7264\t4.2931\t4.2449\t4.3941\t4.6850\t\n",
      "Training Epoch: 3 [266/569]\tLoss: 31.8871\tLR: 0.010000\n",
      "6.2229\t5.3716\t4.7930\t4.7752\t5.1080\t5.6164\t\n",
      "Training Epoch: 3 [282/569]\tLoss: 34.0050\tLR: 0.010000\n",
      "6.6151\t5.6930\t5.2102\t5.1461\t5.4222\t5.9184\t\n",
      "Training Epoch: 3 [298/569]\tLoss: 32.2363\tLR: 0.010000\n",
      "6.6441\t5.7058\t5.1471\t4.9211\t4.8843\t4.9339\t\n",
      "Training Epoch: 3 [314/569]\tLoss: 34.7062\tLR: 0.010000\n",
      "6.7149\t5.8901\t5.4446\t5.3762\t5.4884\t5.7920\t\n",
      "Training Epoch: 3 [330/569]\tLoss: 40.5762\tLR: 0.010000\n",
      "8.9209\t7.4509\t6.4180\t5.9981\t5.8737\t5.9146\t\n",
      "Training Epoch: 3 [346/569]\tLoss: 36.4422\tLR: 0.010000\n",
      "7.1494\t6.4268\t5.9170\t5.6852\t5.5962\t5.6677\t\n",
      "Training Epoch: 3 [362/569]\tLoss: 35.8366\tLR: 0.010000\n",
      "7.0688\t6.3497\t5.7942\t5.5256\t5.4606\t5.6377\t\n",
      "Training Epoch: 3 [378/569]\tLoss: 29.8534\tLR: 0.010000\n",
      "5.5409\t4.8295\t4.5172\t4.5379\t4.9396\t5.4883\t\n",
      "Training Epoch: 3 [394/569]\tLoss: 37.3819\tLR: 0.010000\n",
      "7.5498\t6.6758\t6.0573\t5.7731\t5.6460\t5.6800\t\n",
      "Training Epoch: 3 [410/569]\tLoss: 31.6294\tLR: 0.010000\n",
      "6.2332\t5.4156\t4.9099\t4.8216\t4.9989\t5.2503\t\n",
      "Training Epoch: 3 [426/569]\tLoss: 35.4507\tLR: 0.010000\n",
      "7.3417\t6.3529\t5.6670\t5.3433\t5.3076\t5.4381\t\n",
      "Training Epoch: 3 [442/569]\tLoss: 38.4652\tLR: 0.010000\n",
      "7.8943\t6.9422\t6.2356\t5.8923\t5.8007\t5.7001\t\n",
      "Training Epoch: 3 [458/569]\tLoss: 32.6304\tLR: 0.010000\n",
      "6.3178\t5.6500\t5.2162\t5.0649\t5.0915\t5.2899\t\n",
      "Training Epoch: 3 [474/569]\tLoss: 28.8348\tLR: 0.010000\n",
      "5.7520\t4.9325\t4.4284\t4.3022\t4.5257\t4.8940\t\n",
      "Training Epoch: 3 [490/569]\tLoss: 25.8121\tLR: 0.010000\n",
      "5.0014\t4.5491\t4.2474\t4.0747\t3.9682\t3.9713\t\n",
      "Training Epoch: 3 [506/569]\tLoss: 34.2127\tLR: 0.010000\n",
      "6.8474\t6.0054\t5.4374\t5.2557\t5.2728\t5.3941\t\n",
      "Training Epoch: 3 [522/569]\tLoss: 35.3347\tLR: 0.010000\n",
      "6.8994\t6.2076\t5.7251\t5.5067\t5.4904\t5.5055\t\n",
      "Training Epoch: 3 [538/569]\tLoss: 31.5556\tLR: 0.010000\n",
      "5.9878\t5.3309\t4.9430\t4.9122\t5.0288\t5.3528\t\n",
      "Training Epoch: 3 [554/569]\tLoss: 32.3067\tLR: 0.010000\n",
      "5.7163\t5.2422\t5.0883\t5.2185\t5.4098\t5.6317\t\n",
      "Training Epoch: 3 [570/569]\tLoss: 29.8636\tLR: 0.010000\n",
      "5.6425\t5.0551\t4.7321\t4.5850\t4.6891\t5.1598\t\n",
      "Training Epoch: 3 [586/569]\tLoss: 30.1056\tLR: 0.010000\n",
      "5.3577\t5.0050\t4.8090\t4.8203\t4.9272\t5.1863\t\n",
      "Training Epoch: 3 [602/569]\tLoss: 30.3442\tLR: 0.010000\n",
      "5.7890\t5.1711\t4.7859\t4.7265\t4.7958\t5.0759\t\n",
      "Training Epoch: 3 [618/569]\tLoss: 31.6639\tLR: 0.010000\n",
      "5.8466\t5.2508\t5.0094\t5.0271\t5.1933\t5.3367\t\n",
      "Training Epoch: 3 [634/569]\tLoss: 37.4624\tLR: 0.010000\n",
      "7.6442\t6.6557\t5.9503\t5.6673\t5.6850\t5.8598\t\n",
      "Training Epoch: 3 [650/569]\tLoss: 33.2726\tLR: 0.010000\n",
      "7.0422\t6.0060\t5.2902\t4.9894\t4.9350\t5.0098\t\n",
      "Training Epoch: 3 [666/569]\tLoss: 37.5321\tLR: 0.010000\n",
      "7.2181\t6.5506\t6.0755\t5.9023\t5.8863\t5.8993\t\n",
      "Training Epoch: 3 [682/569]\tLoss: 31.1355\tLR: 0.010000\n",
      "5.4792\t5.1457\t4.9150\t4.9065\t5.1247\t5.5643\t\n",
      "Training Epoch: 3 [698/569]\tLoss: 44.8570\tLR: 0.010000\n",
      "10.5698\t8.8569\t7.3493\t6.4266\t5.9440\t5.7103\t\n",
      "Training Epoch: 3 [714/569]\tLoss: 42.2703\tLR: 0.010000\n",
      "9.5523\t8.0617\t6.9883\t6.2861\t5.8169\t5.5650\t\n",
      "Training Epoch: 3 [730/569]\tLoss: 33.5273\tLR: 0.010000\n",
      "6.8938\t5.9807\t5.3718\t5.1054\t5.0485\t5.1270\t\n",
      "Training Epoch: 3 [746/569]\tLoss: 36.6137\tLR: 0.010000\n",
      "6.9535\t6.2942\t5.9160\t5.8325\t5.8136\t5.8039\t\n",
      "Training Epoch: 3 [762/569]\tLoss: 33.7110\tLR: 0.010000\n",
      "6.4076\t5.6233\t5.2570\t5.3045\t5.4527\t5.6660\t\n",
      "Training Epoch: 3 [778/569]\tLoss: 36.5400\tLR: 0.010000\n",
      "8.3174\t6.8261\t5.7743\t5.3266\t5.1723\t5.1234\t\n",
      "Training Epoch: 3 [794/569]\tLoss: 26.4324\tLR: 0.010000\n",
      "5.0874\t4.5266\t4.0658\t3.9571\t4.1849\t4.6106\t\n",
      "Training Epoch: 3 [810/569]\tLoss: 35.9200\tLR: 0.010000\n",
      "6.9408\t6.1466\t5.7430\t5.6439\t5.6923\t5.7534\t\n",
      "Training Epoch: 3 [826/569]\tLoss: 31.1795\tLR: 0.010000\n",
      "5.7133\t5.2425\t4.9745\t4.9314\t5.0552\t5.2626\t\n",
      "Training Epoch: 3 [842/569]\tLoss: 31.4847\tLR: 0.010000\n",
      "5.5835\t5.1048\t4.8925\t5.0032\t5.3025\t5.5982\t\n",
      "Training Epoch: 3 [858/569]\tLoss: 33.0776\tLR: 0.010000\n",
      "6.4092\t5.6213\t5.2474\t5.1656\t5.2207\t5.4134\t\n",
      "Training Epoch: 3 [874/569]\tLoss: 29.7971\tLR: 0.010000\n",
      "5.4376\t4.9449\t4.7105\t4.7183\t4.8951\t5.0907\t\n",
      "Training Epoch: 3 [890/569]\tLoss: 32.4572\tLR: 0.010000\n",
      "6.1774\t5.4360\t5.1065\t5.0880\t5.2096\t5.4398\t\n",
      "Training Epoch: 3 [905/569]\tLoss: 36.0869\tLR: 0.010000\n",
      "6.9952\t6.1373\t5.6104\t5.5267\t5.7563\t6.0610\t\n",
      "[0.31185051798820496, 0.4711968004703522, 0.21695268154144287, 0.012236476875841618, 0.634028971195221, 0.161712184548378, 0.20425884425640106, 0.020495686680078506, 0.581985592842102, 0.16808469593524933, 0.24992971122264862, 0.0007744145696051419, 0.39813318848609924, 0.2539176046848297, 0.34794920682907104, -0.0013981865486130118, 0.3132382333278656, 0.0, 0.6867617666721344, 0.008280840702354908]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6740, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5910, Accuracy: 0.1002\n",
      "Test set t = 02: Average loss: 0.5384, Accuracy: 0.1072\n",
      "Test set t = 03: Average loss: 0.5208, Accuracy: 0.1142\n",
      "Test set t = 04: Average loss: 0.5243, Accuracy: 0.1037\n",
      "Test set t = 05: Average loss: 0.5406, Accuracy: 0.0861\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [10/569]\tLoss: 24.9081\tLR: 0.010000\n",
      "4.5617\t3.9614\t3.7195\t3.8175\t4.1815\t4.6665\t\n",
      "Training Epoch: 4 [26/569]\tLoss: 29.5848\tLR: 0.010000\n",
      "5.1841\t4.7544\t4.5829\t4.7053\t4.9773\t5.3810\t\n",
      "Training Epoch: 4 [42/569]\tLoss: 31.0373\tLR: 0.010000\n",
      "5.6889\t5.1138\t4.8711\t4.9185\t5.0795\t5.3655\t\n",
      "Training Epoch: 4 [58/569]\tLoss: 41.4353\tLR: 0.010000\n",
      "8.5626\t7.4268\t6.7002\t6.3370\t6.2106\t6.1982\t\n",
      "Training Epoch: 4 [74/569]\tLoss: 36.2818\tLR: 0.010000\n",
      "8.0196\t6.7738\t5.9110\t5.3856\t5.1535\t5.0383\t\n",
      "Training Epoch: 4 [90/569]\tLoss: 39.7542\tLR: 0.010000\n",
      "8.6836\t7.3878\t6.3535\t5.8447\t5.6746\t5.8100\t\n",
      "Training Epoch: 4 [106/569]\tLoss: 34.8604\tLR: 0.010000\n",
      "6.8258\t5.9970\t5.5366\t5.4378\t5.4730\t5.5902\t\n",
      "Training Epoch: 4 [122/569]\tLoss: 34.9891\tLR: 0.010000\n",
      "6.6236\t5.9672\t5.6136\t5.5106\t5.5839\t5.6901\t\n",
      "Training Epoch: 4 [138/569]\tLoss: 29.2284\tLR: 0.010000\n",
      "5.3011\t4.8633\t4.6135\t4.6417\t4.7998\t5.0090\t\n",
      "Training Epoch: 4 [154/569]\tLoss: 42.2411\tLR: 0.010000\n",
      "8.8986\t7.7804\t6.8379\t6.3697\t6.2031\t6.1515\t\n",
      "Training Epoch: 4 [170/569]\tLoss: 36.3759\tLR: 0.010000\n",
      "7.7580\t6.5873\t5.8028\t5.4298\t5.3454\t5.4527\t\n",
      "Training Epoch: 4 [186/569]\tLoss: 32.8531\tLR: 0.010000\n",
      "6.7987\t5.8796\t5.2379\t4.9583\t4.9133\t5.0652\t\n",
      "Training Epoch: 4 [202/569]\tLoss: 35.6344\tLR: 0.010000\n",
      "7.1833\t6.2543\t5.6634\t5.4617\t5.4803\t5.5915\t\n",
      "Training Epoch: 4 [218/569]\tLoss: 48.3797\tLR: 0.010000\n",
      "10.5251\t9.0119\t7.9425\t7.2819\t6.9084\t6.7099\t\n",
      "Training Epoch: 4 [234/569]\tLoss: 32.2715\tLR: 0.010000\n",
      "6.3549\t5.5491\t5.0432\t4.9092\t5.0394\t5.3757\t\n",
      "Training Epoch: 4 [250/569]\tLoss: 38.7052\tLR: 0.010000\n",
      "8.0940\t7.0361\t6.1462\t5.7184\t5.7299\t5.9806\t\n",
      "Training Epoch: 4 [266/569]\tLoss: 33.3071\tLR: 0.010000\n",
      "5.9818\t5.5236\t5.3306\t5.2975\t5.4104\t5.7633\t\n",
      "Training Epoch: 4 [282/569]\tLoss: 35.8493\tLR: 0.010000\n",
      "7.5350\t6.5108\t5.7456\t5.3702\t5.3072\t5.3805\t\n",
      "Training Epoch: 4 [298/569]\tLoss: 32.7670\tLR: 0.010000\n",
      "6.4468\t5.7108\t5.2415\t5.0946\t5.1077\t5.1656\t\n",
      "Training Epoch: 4 [314/569]\tLoss: 30.7863\tLR: 0.010000\n",
      "5.8292\t5.1895\t4.8333\t4.8000\t4.9618\t5.1724\t\n",
      "Training Epoch: 4 [330/569]\tLoss: 31.6979\tLR: 0.010000\n",
      "6.4211\t5.5483\t5.0457\t4.8534\t4.8708\t4.9586\t\n",
      "Training Epoch: 4 [346/569]\tLoss: 30.1975\tLR: 0.010000\n",
      "5.8726\t5.1411\t4.6588\t4.5809\t4.8120\t5.1321\t\n",
      "Training Epoch: 4 [362/569]\tLoss: 25.5760\tLR: 0.010000\n",
      "4.7867\t4.1851\t3.8888\t3.9707\t4.2145\t4.5303\t\n",
      "Training Epoch: 4 [378/569]\tLoss: 39.0657\tLR: 0.010000\n",
      "8.6642\t7.2929\t6.2517\t5.6954\t5.5148\t5.6468\t\n",
      "Training Epoch: 4 [394/569]\tLoss: 34.4034\tLR: 0.010000\n",
      "6.9184\t6.0500\t5.5625\t5.3437\t5.2417\t5.2871\t\n",
      "Training Epoch: 4 [410/569]\tLoss: 36.7946\tLR: 0.010000\n",
      "7.3690\t6.5708\t5.8997\t5.6004\t5.5499\t5.8048\t\n",
      "Training Epoch: 4 [426/569]\tLoss: 36.3261\tLR: 0.010000\n",
      "6.5079\t6.0404\t5.7800\t5.8333\t6.0046\t6.1600\t\n",
      "Training Epoch: 4 [442/569]\tLoss: 30.0216\tLR: 0.010000\n",
      "5.2380\t4.8238\t4.7301\t4.8098\t5.0291\t5.3909\t\n",
      "Training Epoch: 4 [458/569]\tLoss: 33.9977\tLR: 0.010000\n",
      "7.3603\t6.2457\t5.4689\t5.1277\t4.9762\t4.8188\t\n",
      "Training Epoch: 4 [474/569]\tLoss: 40.0757\tLR: 0.010000\n",
      "9.0595\t7.6155\t6.4013\t5.7726\t5.5735\t5.6534\t\n",
      "Training Epoch: 4 [490/569]\tLoss: 32.5865\tLR: 0.010000\n",
      "5.4405\t5.2377\t5.2549\t5.3682\t5.5137\t5.7716\t\n",
      "Training Epoch: 4 [506/569]\tLoss: 32.1325\tLR: 0.010000\n",
      "5.8418\t5.2936\t5.0551\t5.1014\t5.3057\t5.5349\t\n",
      "Training Epoch: 4 [522/569]\tLoss: 21.0228\tLR: 0.010000\n",
      "3.6022\t3.3153\t3.1850\t3.3428\t3.6222\t3.9553\t\n",
      "Training Epoch: 4 [538/569]\tLoss: 38.0262\tLR: 0.010000\n",
      "8.0205\t6.8382\t6.1096\t5.7539\t5.6442\t5.6598\t\n",
      "Training Epoch: 4 [554/569]\tLoss: 32.5323\tLR: 0.010000\n",
      "6.3814\t5.6572\t5.2461\t5.1159\t5.0605\t5.0712\t\n",
      "Training Epoch: 4 [570/569]\tLoss: 22.2385\tLR: 0.010000\n",
      "4.0149\t3.5374\t3.3013\t3.4180\t3.7392\t4.2276\t\n",
      "Training Epoch: 4 [586/569]\tLoss: 37.2610\tLR: 0.010000\n",
      "7.2378\t6.4969\t6.0287\t5.8350\t5.8008\t5.8618\t\n",
      "Training Epoch: 4 [602/569]\tLoss: 29.4693\tLR: 0.010000\n",
      "5.0190\t4.7140\t4.6898\t4.7866\t5.0105\t5.2494\t\n",
      "Training Epoch: 4 [618/569]\tLoss: 33.8077\tLR: 0.010000\n",
      "6.2591\t5.7274\t5.5310\t5.4955\t5.4333\t5.3614\t\n",
      "Training Epoch: 4 [634/569]\tLoss: 32.1772\tLR: 0.010000\n",
      "6.6867\t5.6304\t5.0100\t4.8014\t4.8886\t5.1602\t\n",
      "Training Epoch: 4 [650/569]\tLoss: 29.8097\tLR: 0.010000\n",
      "5.8336\t5.1411\t4.7141\t4.5596\t4.6930\t4.8682\t\n",
      "Training Epoch: 4 [666/569]\tLoss: 39.7828\tLR: 0.010000\n",
      "8.9275\t7.3614\t6.2775\t5.8060\t5.6556\t5.7548\t\n",
      "Training Epoch: 4 [682/569]\tLoss: 28.0126\tLR: 0.010000\n",
      "4.8966\t4.3886\t4.2967\t4.4713\t4.7933\t5.1661\t\n",
      "Training Epoch: 4 [698/569]\tLoss: 28.1739\tLR: 0.010000\n",
      "5.0447\t4.5986\t4.4357\t4.5628\t4.7264\t4.8056\t\n",
      "Training Epoch: 4 [714/569]\tLoss: 38.4172\tLR: 0.010000\n",
      "8.0278\t6.8830\t6.0909\t5.7810\t5.7465\t5.8882\t\n",
      "Training Epoch: 4 [730/569]\tLoss: 32.6808\tLR: 0.010000\n",
      "6.5445\t5.7308\t5.2277\t5.0188\t5.0333\t5.1257\t\n",
      "Training Epoch: 4 [746/569]\tLoss: 38.9016\tLR: 0.010000\n",
      "8.4405\t7.2234\t6.3275\t5.7991\t5.5820\t5.5292\t\n",
      "Training Epoch: 4 [762/569]\tLoss: 32.4657\tLR: 0.010000\n",
      "6.2584\t5.6190\t5.1626\t5.0381\t5.0835\t5.3041\t\n",
      "Training Epoch: 4 [778/569]\tLoss: 34.7354\tLR: 0.010000\n",
      "7.6439\t6.5615\t5.7755\t5.2846\t4.9121\t4.5579\t\n",
      "Training Epoch: 4 [794/569]\tLoss: 30.4721\tLR: 0.010000\n",
      "5.3982\t4.9456\t4.8380\t4.9402\t5.0850\t5.2651\t\n",
      "Training Epoch: 4 [810/569]\tLoss: 40.0002\tLR: 0.010000\n",
      "7.8738\t6.9844\t6.3822\t6.1481\t6.1407\t6.4710\t\n",
      "Training Epoch: 4 [826/569]\tLoss: 31.8554\tLR: 0.010000\n",
      "5.9862\t5.3219\t5.0417\t5.0370\t5.1426\t5.3259\t\n",
      "Training Epoch: 4 [842/569]\tLoss: 36.3934\tLR: 0.010000\n",
      "6.9578\t6.1567\t5.7379\t5.6544\t5.7719\t6.1148\t\n",
      "Training Epoch: 4 [858/569]\tLoss: 32.1902\tLR: 0.010000\n",
      "6.9126\t5.7220\t5.0515\t4.8304\t4.8032\t4.8705\t\n",
      "Training Epoch: 4 [874/569]\tLoss: 35.6897\tLR: 0.010000\n",
      "7.3671\t6.3289\t5.6557\t5.4220\t5.4240\t5.4920\t\n",
      "Training Epoch: 4 [890/569]\tLoss: 28.4990\tLR: 0.010000\n",
      "5.3480\t4.7765\t4.4657\t4.4364\t4.6225\t4.8499\t\n",
      "Training Epoch: 4 [905/569]\tLoss: 38.8666\tLR: 0.010000\n",
      "8.6861\t7.2319\t6.1413\t5.5699\t5.4700\t5.7675\t\n",
      "[0.3092552125453949, 0.49504172801971436, 0.19570305943489075, 0.010556912049651146, 0.6743029952049255, 0.13381098210811615, 0.1918860226869583, 0.022637955844402313, 0.6462233066558838, 0.14984764158725739, 0.20392905175685883, -0.0016197024378925562, 0.41604337096214294, 0.2545996606349945, 0.32935696840286255, -0.004583480302244425, 0.3082062602043152, 0.0, 0.6917937397956848, 0.00754733057692647]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6740, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5887, Accuracy: 0.1002\n",
      "Test set t = 02: Average loss: 0.5367, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5194, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5234, Accuracy: 0.1054\n",
      "Test set t = 05: Average loss: 0.5399, Accuracy: 0.0949\n",
      "\n",
      "Training Epoch: 5 [10/569]\tLoss: 36.0089\tLR: 0.010000\n",
      "7.0855\t6.1565\t5.6276\t5.5283\t5.6557\t5.9553\t\n",
      "Training Epoch: 5 [26/569]\tLoss: 35.5218\tLR: 0.010000\n",
      "7.5689\t6.4082\t5.6375\t5.3018\t5.2654\t5.3400\t\n",
      "Training Epoch: 5 [42/569]\tLoss: 34.4356\tLR: 0.010000\n",
      "6.9294\t6.0229\t5.4564\t5.2493\t5.3034\t5.4742\t\n",
      "Training Epoch: 5 [58/569]\tLoss: 27.6282\tLR: 0.010000\n",
      "4.7442\t4.4818\t4.4095\t4.5236\t4.6651\t4.8040\t\n",
      "Training Epoch: 5 [74/569]\tLoss: 30.4865\tLR: 0.010000\n",
      "5.2958\t4.9316\t4.8261\t4.9170\t5.1531\t5.3628\t\n",
      "Training Epoch: 5 [90/569]\tLoss: 34.5773\tLR: 0.010000\n",
      "6.6574\t5.8423\t5.4684\t5.3531\t5.5111\t5.7451\t\n",
      "Training Epoch: 5 [106/569]\tLoss: 40.7163\tLR: 0.010000\n",
      "8.4135\t7.3668\t6.6322\t6.1777\t6.0119\t6.1142\t\n",
      "Training Epoch: 5 [122/569]\tLoss: 32.1464\tLR: 0.010000\n",
      "6.4110\t5.5832\t5.0693\t4.9155\t5.0057\t5.1617\t\n",
      "Training Epoch: 5 [138/569]\tLoss: 34.2630\tLR: 0.010000\n",
      "6.9910\t6.0258\t5.3057\t5.1045\t5.2452\t5.5908\t\n",
      "Training Epoch: 5 [154/569]\tLoss: 36.4646\tLR: 0.010000\n",
      "7.1337\t6.2194\t5.7110\t5.5866\t5.7037\t6.1101\t\n",
      "Training Epoch: 5 [170/569]\tLoss: 32.1804\tLR: 0.010000\n",
      "6.5337\t5.6072\t5.0842\t4.9184\t4.9350\t5.1020\t\n",
      "Training Epoch: 5 [186/569]\tLoss: 33.8135\tLR: 0.010000\n",
      "6.0928\t5.6373\t5.4502\t5.4936\t5.5536\t5.5861\t\n",
      "Training Epoch: 5 [202/569]\tLoss: 33.1639\tLR: 0.010000\n",
      "6.7013\t5.8263\t5.2454\t5.0495\t5.0993\t5.2422\t\n",
      "Training Epoch: 5 [218/569]\tLoss: 31.8136\tLR: 0.010000\n",
      "5.9453\t5.4499\t5.1673\t5.0696\t5.0696\t5.1119\t\n",
      "Training Epoch: 5 [234/569]\tLoss: 37.1958\tLR: 0.010000\n",
      "8.0815\t6.8363\t5.9371\t5.5080\t5.3886\t5.4442\t\n",
      "Training Epoch: 5 [250/569]\tLoss: 28.0597\tLR: 0.010000\n",
      "5.1134\t4.5389\t4.2219\t4.3000\t4.6484\t5.2372\t\n",
      "Training Epoch: 5 [266/569]\tLoss: 33.6393\tLR: 0.010000\n",
      "6.2831\t5.7158\t5.3792\t5.3219\t5.4040\t5.5354\t\n",
      "Training Epoch: 5 [282/569]\tLoss: 35.9414\tLR: 0.010000\n",
      "7.0164\t6.3120\t5.8194\t5.6089\t5.5723\t5.6123\t\n",
      "Training Epoch: 5 [298/569]\tLoss: 42.1068\tLR: 0.010000\n",
      "9.1179\t7.8594\t6.9635\t6.3484\t5.9933\t5.8244\t\n",
      "Training Epoch: 5 [314/569]\tLoss: 36.4676\tLR: 0.010000\n",
      "7.3340\t6.3695\t5.7877\t5.5862\t5.6244\t5.7657\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [330/569]\tLoss: 32.1931\tLR: 0.010000\n",
      "5.9478\t5.4586\t5.2101\t5.1817\t5.1548\t5.2401\t\n",
      "Training Epoch: 5 [346/569]\tLoss: 38.5382\tLR: 0.010000\n",
      "7.6337\t6.7778\t6.2255\t5.9999\t5.9557\t5.9457\t\n",
      "Training Epoch: 5 [362/569]\tLoss: 35.5137\tLR: 0.010000\n",
      "8.1986\t6.7078\t5.6420\t5.0964\t4.9287\t4.9402\t\n",
      "Training Epoch: 5 [378/569]\tLoss: 31.3241\tLR: 0.010000\n",
      "5.5900\t5.1332\t4.9262\t4.9590\t5.1626\t5.5533\t\n",
      "Training Epoch: 5 [394/569]\tLoss: 27.6027\tLR: 0.010000\n",
      "4.9161\t4.4566\t4.3082\t4.4087\t4.6458\t4.8674\t\n",
      "Training Epoch: 5 [410/569]\tLoss: 37.6514\tLR: 0.010000\n",
      "7.9647\t6.8145\t6.0466\t5.6628\t5.5590\t5.6038\t\n",
      "Training Epoch: 5 [426/569]\tLoss: 30.2792\tLR: 0.010000\n",
      "6.3919\t5.3624\t4.6572\t4.4720\t4.5864\t4.8093\t\n",
      "Training Epoch: 5 [442/569]\tLoss: 29.0400\tLR: 0.010000\n",
      "5.2738\t4.6493\t4.4337\t4.5725\t4.8802\t5.2305\t\n",
      "Training Epoch: 5 [458/569]\tLoss: 34.9611\tLR: 0.010000\n",
      "6.7221\t5.8753\t5.4905\t5.4803\t5.6238\t5.7690\t\n",
      "Training Epoch: 5 [474/569]\tLoss: 42.5443\tLR: 0.010000\n",
      "9.1177\t7.7255\t6.8133\t6.3854\t6.2411\t6.2614\t\n",
      "Training Epoch: 5 [490/569]\tLoss: 27.2997\tLR: 0.010000\n",
      "5.6308\t4.7893\t4.2856\t4.1212\t4.1531\t4.3197\t\n",
      "Training Epoch: 5 [506/569]\tLoss: 31.6842\tLR: 0.010000\n",
      "6.7010\t5.6404\t5.0288\t4.7745\t4.7149\t4.8246\t\n",
      "Training Epoch: 5 [522/569]\tLoss: 34.1469\tLR: 0.010000\n",
      "7.0362\t6.0945\t5.5136\t5.2526\t5.1408\t5.1092\t\n",
      "Training Epoch: 5 [538/569]\tLoss: 32.5748\tLR: 0.010000\n",
      "6.1811\t5.3281\t4.9254\t4.9820\t5.2986\t5.8596\t\n",
      "Training Epoch: 5 [554/569]\tLoss: 37.2037\tLR: 0.010000\n",
      "7.3283\t6.4955\t5.9769\t5.7582\t5.7571\t5.8876\t\n",
      "Training Epoch: 5 [570/569]\tLoss: 30.0826\tLR: 0.010000\n",
      "5.7380\t5.1197\t4.7816\t4.7015\t4.7601\t4.9817\t\n",
      "Training Epoch: 5 [586/569]\tLoss: 40.4510\tLR: 0.010000\n",
      "8.7409\t7.5098\t6.5861\t6.0505\t5.7978\t5.7658\t\n",
      "Training Epoch: 5 [602/569]\tLoss: 32.1487\tLR: 0.010000\n",
      "6.6120\t5.7260\t5.1601\t4.9168\t4.8529\t4.8808\t\n",
      "Training Epoch: 5 [618/569]\tLoss: 35.6892\tLR: 0.010000\n",
      "6.7870\t6.0611\t5.7468\t5.6682\t5.6787\t5.7473\t\n",
      "Training Epoch: 5 [634/569]\tLoss: 30.6438\tLR: 0.010000\n",
      "6.0434\t5.3366\t4.8879\t4.7010\t4.7358\t4.9392\t\n",
      "Training Epoch: 5 [650/569]\tLoss: 34.9306\tLR: 0.010000\n",
      "7.8277\t6.5129\t5.6282\t5.1723\t4.9545\t4.8349\t\n",
      "Training Epoch: 5 [666/569]\tLoss: 35.6047\tLR: 0.010000\n",
      "6.9071\t6.1902\t5.7633\t5.5535\t5.5429\t5.6477\t\n",
      "Training Epoch: 5 [682/569]\tLoss: 35.5907\tLR: 0.010000\n",
      "7.5814\t6.5070\t5.7308\t5.3357\t5.2102\t5.2257\t\n",
      "Training Epoch: 5 [698/569]\tLoss: 27.5623\tLR: 0.010000\n",
      "4.5494\t4.1640\t4.1607\t4.4679\t4.8875\t5.3327\t\n",
      "Training Epoch: 5 [714/569]\tLoss: 28.0224\tLR: 0.010000\n",
      "5.3386\t4.6276\t4.2483\t4.2935\t4.5918\t4.9225\t\n",
      "Training Epoch: 5 [730/569]\tLoss: 32.1293\tLR: 0.010000\n",
      "6.3333\t5.4935\t5.0417\t4.9591\t5.0734\t5.2284\t\n",
      "Training Epoch: 5 [746/569]\tLoss: 34.7305\tLR: 0.010000\n",
      "7.8442\t6.4677\t5.5462\t5.0511\t4.8797\t4.9415\t\n",
      "Training Epoch: 5 [762/569]\tLoss: 29.0197\tLR: 0.010000\n",
      "5.2334\t4.6477\t4.4668\t4.5958\t4.8583\t5.2176\t\n",
      "Training Epoch: 5 [778/569]\tLoss: 32.1471\tLR: 0.010000\n",
      "6.2450\t5.4779\t5.0757\t5.0201\t5.1059\t5.2225\t\n",
      "Training Epoch: 5 [794/569]\tLoss: 33.0135\tLR: 0.010000\n",
      "6.5885\t5.7302\t5.1769\t5.0275\t5.1348\t5.3557\t\n",
      "Training Epoch: 5 [810/569]\tLoss: 34.8890\tLR: 0.010000\n",
      "7.6919\t6.4727\t5.6241\t5.1472\t4.9608\t4.9924\t\n",
      "Training Epoch: 5 [826/569]\tLoss: 31.2690\tLR: 0.010000\n",
      "5.8657\t5.2156\t4.9246\t4.9601\t5.0696\t5.2335\t\n",
      "Training Epoch: 5 [842/569]\tLoss: 30.5853\tLR: 0.010000\n",
      "6.0312\t5.3916\t4.8973\t4.7364\t4.7464\t4.7824\t\n",
      "Training Epoch: 5 [858/569]\tLoss: 36.6566\tLR: 0.010000\n",
      "6.9566\t6.2366\t5.7643\t5.6161\t5.7742\t6.3087\t\n",
      "Training Epoch: 5 [874/569]\tLoss: 42.0042\tLR: 0.010000\n",
      "8.9106\t7.5926\t6.7427\t6.3254\t6.1834\t6.2495\t\n",
      "Training Epoch: 5 [890/569]\tLoss: 32.6520\tLR: 0.010000\n",
      "6.6838\t5.7927\t5.2406\t5.0396\t4.9322\t4.9631\t\n",
      "Training Epoch: 5 [905/569]\tLoss: 35.2211\tLR: 0.010000\n",
      "6.9371\t6.0608\t5.5913\t5.4732\t5.5211\t5.6377\t\n",
      "[0.30944323539733887, 0.4941241145133972, 0.19643265008926392, 0.010048797354102135, 0.6925897002220154, 0.10960373282432556, 0.19780656695365906, 0.024226978421211243, 0.6883082389831543, 0.1382615864276886, 0.1734301745891571, -0.003428346710279584, 0.4202429950237274, 0.26843228936195374, 0.31132471561431885, -0.007503421977162361, 0.30883729457855225, 0.0, 0.6911627054214478, 0.006659742910414934]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6739, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5863, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5351, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5190, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5236, Accuracy: 0.1054\n",
      "Test set t = 05: Average loss: 0.5403, Accuracy: 0.0931\n",
      "\n",
      "Training Epoch: 6 [10/569]\tLoss: 31.3974\tLR: 0.010000\n",
      "5.8894\t5.1923\t4.9365\t4.9573\t5.0921\t5.3298\t\n",
      "Training Epoch: 6 [26/569]\tLoss: 36.4444\tLR: 0.010000\n",
      "7.0604\t6.1769\t5.7365\t5.6779\t5.7522\t6.0404\t\n",
      "Training Epoch: 6 [42/569]\tLoss: 34.0280\tLR: 0.010000\n",
      "6.4951\t5.7513\t5.3360\t5.2599\t5.4008\t5.7849\t\n",
      "Training Epoch: 6 [58/569]\tLoss: 27.4088\tLR: 0.010000\n",
      "5.0222\t4.5007\t4.2480\t4.2728\t4.5113\t4.8538\t\n",
      "Training Epoch: 6 [74/569]\tLoss: 31.5666\tLR: 0.010000\n",
      "5.9705\t5.2971\t4.9740\t4.9621\t5.1079\t5.2549\t\n",
      "Training Epoch: 6 [90/569]\tLoss: 33.9667\tLR: 0.010000\n",
      "7.2124\t6.1729\t5.3598\t5.0427\t5.0346\t5.1443\t\n",
      "Training Epoch: 6 [106/569]\tLoss: 27.3276\tLR: 0.010000\n",
      "4.6238\t4.3163\t4.2010\t4.3384\t4.6517\t5.1963\t\n",
      "Training Epoch: 6 [122/569]\tLoss: 28.1819\tLR: 0.010000\n",
      "5.3935\t4.8632\t4.4824\t4.3341\t4.3990\t4.7097\t\n",
      "Training Epoch: 6 [138/569]\tLoss: 36.8362\tLR: 0.010000\n",
      "7.4193\t6.4423\t5.8638\t5.6138\t5.6325\t5.8645\t\n",
      "Training Epoch: 6 [154/569]\tLoss: 32.7366\tLR: 0.010000\n",
      "6.1134\t5.5098\t5.1840\t5.1576\t5.2911\t5.4806\t\n",
      "Training Epoch: 6 [170/569]\tLoss: 26.3346\tLR: 0.010000\n",
      "4.5563\t4.1646\t4.1232\t4.2724\t4.5120\t4.7061\t\n",
      "Training Epoch: 6 [186/569]\tLoss: 36.9684\tLR: 0.010000\n",
      "7.6465\t6.6291\t5.9827\t5.6673\t5.5414\t5.5012\t\n",
      "Training Epoch: 6 [202/569]\tLoss: 34.5229\tLR: 0.010000\n",
      "6.9251\t6.0627\t5.5390\t5.2818\t5.3131\t5.4013\t\n",
      "Training Epoch: 6 [218/569]\tLoss: 33.3560\tLR: 0.010000\n",
      "6.4988\t5.7838\t5.3300\t5.2271\t5.2297\t5.2866\t\n",
      "Training Epoch: 6 [234/569]\tLoss: 40.2013\tLR: 0.010000\n",
      "9.0133\t7.5365\t6.4589\t5.8978\t5.6512\t5.6437\t\n",
      "Training Epoch: 6 [250/569]\tLoss: 38.2792\tLR: 0.010000\n",
      "7.7390\t6.7996\t6.2502\t5.9282\t5.7893\t5.7728\t\n",
      "Training Epoch: 6 [266/569]\tLoss: 30.6951\tLR: 0.010000\n",
      "6.1751\t5.3182\t4.8046\t4.6851\t4.7471\t4.9652\t\n",
      "Training Epoch: 6 [282/569]\tLoss: 28.6786\tLR: 0.010000\n",
      "5.6512\t4.8157\t4.4381\t4.4584\t4.6083\t4.7068\t\n",
      "Training Epoch: 6 [298/569]\tLoss: 35.4992\tLR: 0.010000\n",
      "6.8364\t6.0461\t5.6485\t5.5674\t5.6029\t5.7979\t\n",
      "Training Epoch: 6 [314/569]\tLoss: 39.1135\tLR: 0.010000\n",
      "8.7035\t7.2775\t6.2728\t5.7393\t5.5541\t5.5662\t\n",
      "Training Epoch: 6 [330/569]\tLoss: 29.4896\tLR: 0.010000\n",
      "6.1847\t5.3572\t4.6994\t4.3796\t4.3783\t4.4903\t\n",
      "Training Epoch: 6 [346/569]\tLoss: 35.8841\tLR: 0.010000\n",
      "7.2801\t6.3607\t5.8131\t5.5541\t5.4723\t5.4037\t\n",
      "Training Epoch: 6 [362/569]\tLoss: 42.9473\tLR: 0.010000\n",
      "9.2287\t7.9751\t7.0236\t6.4406\t6.1985\t6.0807\t\n",
      "Training Epoch: 6 [378/569]\tLoss: 37.6148\tLR: 0.010000\n",
      "7.5432\t6.7071\t6.1682\t5.8583\t5.6721\t5.6658\t\n",
      "Training Epoch: 6 [394/569]\tLoss: 30.8023\tLR: 0.010000\n",
      "5.7532\t5.2212\t4.9043\t4.8525\t4.9601\t5.1110\t\n",
      "Training Epoch: 6 [410/569]\tLoss: 29.4325\tLR: 0.010000\n",
      "5.4685\t4.7652\t4.5053\t4.6035\t4.9002\t5.1900\t\n",
      "Training Epoch: 6 [426/569]\tLoss: 35.4164\tLR: 0.010000\n",
      "7.2185\t6.2439\t5.6028\t5.3537\t5.3559\t5.6417\t\n",
      "Training Epoch: 6 [442/569]\tLoss: 34.2251\tLR: 0.010000\n",
      "6.5806\t5.9153\t5.5108\t5.3857\t5.3935\t5.4392\t\n",
      "Training Epoch: 6 [458/569]\tLoss: 30.9465\tLR: 0.010000\n",
      "6.1244\t5.2448\t4.8239\t4.7530\t4.8933\t5.1071\t\n",
      "Training Epoch: 6 [474/569]\tLoss: 35.7672\tLR: 0.010000\n",
      "7.1722\t6.3336\t5.7550\t5.5187\t5.4671\t5.5206\t\n",
      "Training Epoch: 6 [490/569]\tLoss: 31.2269\tLR: 0.010000\n",
      "5.9189\t5.2220\t4.8568\t4.8754\t5.0527\t5.3010\t\n",
      "Training Epoch: 6 [506/569]\tLoss: 38.3352\tLR: 0.010000\n",
      "8.2533\t6.9993\t6.1669\t5.7502\t5.5920\t5.5735\t\n",
      "Training Epoch: 6 [522/569]\tLoss: 30.2247\tLR: 0.010000\n",
      "6.4696\t5.5223\t4.8594\t4.5022\t4.3869\t4.4842\t\n",
      "Training Epoch: 6 [538/569]\tLoss: 25.6800\tLR: 0.010000\n",
      "4.3633\t4.0037\t3.9883\t4.1481\t4.3915\t4.7852\t\n",
      "Training Epoch: 6 [554/569]\tLoss: 40.4550\tLR: 0.010000\n",
      "8.8467\t7.4055\t6.4903\t6.0421\t5.8477\t5.8227\t\n",
      "Training Epoch: 6 [570/569]\tLoss: 28.0120\tLR: 0.010000\n",
      "5.3081\t4.6765\t4.2804\t4.2455\t4.5352\t4.9662\t\n",
      "Training Epoch: 6 [586/569]\tLoss: 33.2321\tLR: 0.010000\n",
      "6.6218\t5.7270\t5.1312\t5.0118\t5.2133\t5.5269\t\n",
      "Training Epoch: 6 [602/569]\tLoss: 37.5452\tLR: 0.010000\n",
      "6.5608\t6.0941\t5.9577\t6.0206\t6.2313\t6.6808\t\n",
      "Training Epoch: 6 [618/569]\tLoss: 31.3482\tLR: 0.010000\n",
      "6.4115\t5.4572\t4.8991\t4.8093\t4.8493\t4.9217\t\n",
      "Training Epoch: 6 [634/569]\tLoss: 38.7676\tLR: 0.010000\n",
      "8.2612\t7.0083\t6.2310\t5.8690\t5.7317\t5.6665\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [650/569]\tLoss: 32.3371\tLR: 0.010000\n",
      "5.9275\t5.3269\t5.1518\t5.1815\t5.3166\t5.4328\t\n",
      "Training Epoch: 6 [666/569]\tLoss: 33.2013\tLR: 0.010000\n",
      "6.0351\t5.4382\t5.1981\t5.2718\t5.4873\t5.7709\t\n",
      "Training Epoch: 6 [682/569]\tLoss: 28.9578\tLR: 0.010000\n",
      "5.5961\t4.8714\t4.5050\t4.4874\t4.6368\t4.8610\t\n",
      "Training Epoch: 6 [698/569]\tLoss: 32.9771\tLR: 0.010000\n",
      "6.6490\t5.6810\t5.1340\t5.0214\t5.0978\t5.3939\t\n",
      "Training Epoch: 6 [714/569]\tLoss: 35.1033\tLR: 0.010000\n",
      "7.2263\t6.2874\t5.6987\t5.3817\t5.2534\t5.2558\t\n",
      "Training Epoch: 6 [730/569]\tLoss: 28.8837\tLR: 0.010000\n",
      "5.4135\t4.8553\t4.5820\t4.5540\t4.6398\t4.8391\t\n",
      "Training Epoch: 6 [746/569]\tLoss: 38.1904\tLR: 0.010000\n",
      "7.7436\t6.7128\t6.1482\t5.9008\t5.8084\t5.8767\t\n",
      "Training Epoch: 6 [762/569]\tLoss: 38.3506\tLR: 0.010000\n",
      "7.7702\t6.7076\t6.0698\t5.8792\t5.9289\t5.9949\t\n",
      "Training Epoch: 6 [778/569]\tLoss: 31.0357\tLR: 0.010000\n",
      "7.1229\t5.6915\t4.7556\t4.3940\t4.4295\t4.6422\t\n",
      "Training Epoch: 6 [794/569]\tLoss: 30.2407\tLR: 0.010000\n",
      "5.7906\t5.0215\t4.6723\t4.6818\t4.8862\t5.1882\t\n",
      "Training Epoch: 6 [810/569]\tLoss: 37.4103\tLR: 0.010000\n",
      "7.8370\t6.8608\t6.1123\t5.6819\t5.4843\t5.4341\t\n",
      "Training Epoch: 6 [826/569]\tLoss: 29.0356\tLR: 0.010000\n",
      "5.6653\t5.0281\t4.6919\t4.5453\t4.5257\t4.5793\t\n",
      "Training Epoch: 6 [842/569]\tLoss: 40.4187\tLR: 0.010000\n",
      "8.2815\t7.1720\t6.4940\t6.1938\t6.1284\t6.1490\t\n",
      "Training Epoch: 6 [858/569]\tLoss: 31.2192\tLR: 0.010000\n",
      "5.9748\t5.3088\t4.9333\t4.8407\t5.0017\t5.1600\t\n",
      "Training Epoch: 6 [874/569]\tLoss: 35.3508\tLR: 0.010000\n",
      "7.0041\t6.2661\t5.7015\t5.4397\t5.4044\t5.5351\t\n",
      "Training Epoch: 6 [890/569]\tLoss: 38.3216\tLR: 0.010000\n",
      "8.4029\t7.1218\t6.2346\t5.7423\t5.4830\t5.3370\t\n",
      "Training Epoch: 6 [905/569]\tLoss: 41.2014\tLR: 0.010000\n",
      "8.7573\t7.4411\t6.6390\t6.2858\t6.0661\t6.0121\t\n",
      "[0.3169974982738495, 0.48783668875694275, 0.19516581296920776, 0.009207646362483501, 0.7056639790534973, 0.09003611654043198, 0.2042999044060707, 0.02625376544892788, 0.7251228094100952, 0.12438482791185379, 0.150492362678051, -0.00554939778521657, 0.4233322739601135, 0.2809714376926422, 0.29569628834724426, -0.010360841639339924, 0.30528387427330017, 0.0, 0.6947161257266998, 0.005901550408452749]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6742, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5860, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5351, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5190, Accuracy: 0.1142\n",
      "Test set t = 04: Average loss: 0.5228, Accuracy: 0.1072\n",
      "Test set t = 05: Average loss: 0.5380, Accuracy: 0.0984\n",
      "\n",
      "Training Epoch: 7 [10/569]\tLoss: 39.0087\tLR: 0.010000\n",
      "8.3710\t7.1647\t6.3509\t5.8467\t5.6332\t5.6421\t\n",
      "Training Epoch: 7 [26/569]\tLoss: 28.2204\tLR: 0.010000\n",
      "5.4560\t4.6749\t4.2746\t4.2884\t4.5881\t4.9383\t\n",
      "Training Epoch: 7 [42/569]\tLoss: 31.2325\tLR: 0.010000\n",
      "6.2906\t5.2655\t4.7814\t4.7900\t4.9383\t5.1668\t\n",
      "Training Epoch: 7 [58/569]\tLoss: 33.6540\tLR: 0.010000\n",
      "6.4631\t5.7283\t5.3206\t5.2612\t5.3437\t5.5371\t\n",
      "Training Epoch: 7 [74/569]\tLoss: 34.7483\tLR: 0.010000\n",
      "6.8163\t6.0145\t5.5353\t5.4366\t5.4489\t5.4968\t\n",
      "Training Epoch: 7 [90/569]\tLoss: 32.1251\tLR: 0.010000\n",
      "6.8388\t5.7229\t5.1160\t4.8603\t4.7807\t4.8063\t\n",
      "Training Epoch: 7 [106/569]\tLoss: 33.4813\tLR: 0.010000\n",
      "6.8877\t5.8279\t5.2697\t5.1063\t5.1728\t5.2170\t\n",
      "Training Epoch: 7 [122/569]\tLoss: 35.5967\tLR: 0.010000\n",
      "6.7849\t6.0534\t5.6946\t5.5635\t5.5871\t5.9133\t\n",
      "Training Epoch: 7 [138/569]\tLoss: 29.7932\tLR: 0.010000\n",
      "5.0514\t4.7128\t4.5987\t4.6893\t5.0840\t5.6570\t\n",
      "Training Epoch: 7 [154/569]\tLoss: 35.1579\tLR: 0.010000\n",
      "6.6873\t6.0055\t5.6357\t5.4960\t5.5589\t5.7745\t\n",
      "Training Epoch: 7 [170/569]\tLoss: 39.4189\tLR: 0.010000\n",
      "9.0446\t7.4154\t6.3630\t5.7535\t5.4769\t5.3655\t\n",
      "Training Epoch: 7 [186/569]\tLoss: 38.3863\tLR: 0.010000\n",
      "8.7154\t7.1265\t6.0544\t5.4963\t5.3600\t5.6337\t\n",
      "Training Epoch: 7 [202/569]\tLoss: 31.5515\tLR: 0.010000\n",
      "6.2435\t5.3668\t4.9575\t4.9160\t4.9814\t5.0863\t\n",
      "Training Epoch: 7 [218/569]\tLoss: 34.3280\tLR: 0.010000\n",
      "6.9990\t6.0332\t5.4618\t5.2623\t5.2437\t5.3279\t\n",
      "Training Epoch: 7 [234/569]\tLoss: 28.6961\tLR: 0.010000\n",
      "5.4629\t4.8965\t4.5977\t4.5357\t4.5482\t4.6552\t\n",
      "Training Epoch: 7 [250/569]\tLoss: 37.4025\tLR: 0.010000\n",
      "7.7412\t6.7134\t6.0991\t5.7508\t5.5682\t5.5299\t\n",
      "Training Epoch: 7 [266/569]\tLoss: 36.2590\tLR: 0.010000\n",
      "7.1813\t6.1764\t5.6912\t5.5836\t5.6373\t5.9893\t\n",
      "Training Epoch: 7 [282/569]\tLoss: 35.7326\tLR: 0.010000\n",
      "7.1120\t6.1109\t5.6155\t5.4771\t5.5887\t5.8283\t\n",
      "Training Epoch: 7 [298/569]\tLoss: 32.9749\tLR: 0.010000\n",
      "6.3350\t5.5199\t5.1601\t5.1699\t5.3152\t5.4748\t\n",
      "Training Epoch: 7 [314/569]\tLoss: 33.7000\tLR: 0.010000\n",
      "6.8703\t5.8269\t5.2515\t5.1400\t5.2422\t5.3691\t\n",
      "Training Epoch: 7 [330/569]\tLoss: 42.2627\tLR: 0.010000\n",
      "9.7470\t7.8437\t6.6590\t6.0976\t5.9341\t5.9812\t\n",
      "Training Epoch: 7 [346/569]\tLoss: 36.0765\tLR: 0.010000\n",
      "7.1634\t6.1924\t5.5782\t5.4467\t5.6392\t6.0566\t\n",
      "Training Epoch: 7 [362/569]\tLoss: 28.1793\tLR: 0.010000\n",
      "5.4815\t4.7754\t4.4671\t4.4191\t4.4793\t4.5569\t\n",
      "Training Epoch: 7 [378/569]\tLoss: 24.3709\tLR: 0.010000\n",
      "4.2082\t3.7766\t3.7012\t3.9077\t4.2424\t4.5348\t\n",
      "Training Epoch: 7 [394/569]\tLoss: 34.1808\tLR: 0.010000\n",
      "7.0200\t5.9065\t5.3001\t5.1341\t5.2884\t5.5317\t\n",
      "Training Epoch: 7 [410/569]\tLoss: 32.5798\tLR: 0.010000\n",
      "6.5458\t5.6251\t5.1194\t5.0151\t5.0981\t5.1762\t\n",
      "Training Epoch: 7 [426/569]\tLoss: 31.0810\tLR: 0.010000\n",
      "5.6849\t5.1355\t4.9161\t4.9732\t5.0869\t5.2845\t\n",
      "Training Epoch: 7 [442/569]\tLoss: 33.5781\tLR: 0.010000\n",
      "6.2936\t5.6531\t5.2899\t5.2864\t5.3983\t5.6567\t\n",
      "Training Epoch: 7 [458/569]\tLoss: 34.4302\tLR: 0.010000\n",
      "6.7624\t6.0597\t5.5430\t5.3154\t5.3269\t5.4228\t\n",
      "Training Epoch: 7 [474/569]\tLoss: 37.5110\tLR: 0.010000\n",
      "7.8252\t6.6408\t6.0119\t5.7279\t5.6227\t5.6824\t\n",
      "Training Epoch: 7 [490/569]\tLoss: 36.1003\tLR: 0.010000\n",
      "7.7811\t6.4941\t5.7046\t5.3901\t5.3177\t5.4127\t\n",
      "Training Epoch: 7 [506/569]\tLoss: 39.8332\tLR: 0.010000\n",
      "9.0809\t7.5088\t6.4573\t5.8270\t5.5012\t5.4579\t\n",
      "Training Epoch: 7 [522/569]\tLoss: 33.3638\tLR: 0.010000\n",
      "7.1227\t5.8483\t5.0620\t4.8465\t5.0389\t5.4454\t\n",
      "Training Epoch: 7 [538/569]\tLoss: 29.9410\tLR: 0.010000\n",
      "5.6566\t4.8992\t4.5750\t4.6476\t4.8698\t5.2928\t\n",
      "Training Epoch: 7 [554/569]\tLoss: 32.2284\tLR: 0.010000\n",
      "6.2508\t5.4931\t5.1258\t5.0460\t5.1275\t5.1852\t\n",
      "Training Epoch: 7 [570/569]\tLoss: 38.7083\tLR: 0.010000\n",
      "8.9060\t7.2310\t6.0542\t5.5299\t5.4344\t5.5528\t\n",
      "Training Epoch: 7 [586/569]\tLoss: 31.6170\tLR: 0.010000\n",
      "6.0474\t5.3187\t5.0230\t4.9692\t5.0504\t5.2083\t\n",
      "Training Epoch: 7 [602/569]\tLoss: 36.8050\tLR: 0.010000\n",
      "6.5296\t6.0638\t5.8646\t5.9078\t6.0861\t6.3532\t\n",
      "Training Epoch: 7 [618/569]\tLoss: 32.4920\tLR: 0.010000\n",
      "6.5875\t5.8298\t5.2820\t5.0164\t4.8939\t4.8824\t\n",
      "Training Epoch: 7 [634/569]\tLoss: 33.2913\tLR: 0.010000\n",
      "6.5847\t5.7624\t5.2868\t5.1852\t5.2158\t5.2564\t\n",
      "Training Epoch: 7 [650/569]\tLoss: 32.6840\tLR: 0.010000\n",
      "6.1613\t5.4205\t5.1082\t5.1131\t5.2919\t5.5891\t\n",
      "Training Epoch: 7 [666/569]\tLoss: 35.1518\tLR: 0.010000\n",
      "7.4838\t6.1630\t5.5053\t5.3198\t5.3143\t5.3656\t\n",
      "Training Epoch: 7 [682/569]\tLoss: 31.6932\tLR: 0.010000\n",
      "5.7434\t5.1789\t4.9218\t5.0493\t5.2708\t5.5291\t\n",
      "Training Epoch: 7 [698/569]\tLoss: 35.3286\tLR: 0.010000\n",
      "7.0639\t6.0697\t5.5280\t5.3774\t5.4808\t5.8088\t\n",
      "Training Epoch: 7 [714/569]\tLoss: 30.6708\tLR: 0.010000\n",
      "6.3692\t5.4199\t4.7682\t4.5830\t4.6686\t4.8619\t\n",
      "Training Epoch: 7 [730/569]\tLoss: 38.5617\tLR: 0.010000\n",
      "7.3937\t6.6387\t6.2678\t6.1315\t6.0563\t6.0736\t\n",
      "Training Epoch: 7 [746/569]\tLoss: 27.1258\tLR: 0.010000\n",
      "4.9111\t4.4269\t4.1885\t4.2686\t4.5160\t4.8146\t\n",
      "Training Epoch: 7 [762/569]\tLoss: 32.3068\tLR: 0.010000\n",
      "5.8396\t5.3000\t5.1175\t5.1802\t5.3546\t5.5148\t\n",
      "Training Epoch: 7 [778/569]\tLoss: 31.2394\tLR: 0.010000\n",
      "5.0419\t4.8691\t4.9066\t5.1331\t5.4445\t5.8442\t\n",
      "Training Epoch: 7 [794/569]\tLoss: 36.4080\tLR: 0.010000\n",
      "7.1034\t6.3746\t5.9480\t5.6699\t5.5624\t5.7497\t\n",
      "Training Epoch: 7 [810/569]\tLoss: 37.3095\tLR: 0.010000\n",
      "8.2248\t6.7470\t5.8597\t5.4987\t5.4347\t5.5446\t\n",
      "Training Epoch: 7 [826/569]\tLoss: 30.8981\tLR: 0.010000\n",
      "5.6860\t5.0853\t4.8933\t4.9210\t5.0485\t5.2640\t\n",
      "Training Epoch: 7 [842/569]\tLoss: 28.3422\tLR: 0.010000\n",
      "6.0633\t5.0694\t4.3333\t4.1014\t4.2464\t4.5285\t\n",
      "Training Epoch: 7 [858/569]\tLoss: 29.6621\tLR: 0.010000\n",
      "5.2731\t4.8417\t4.7384\t4.8096\t4.9358\t5.0635\t\n",
      "Training Epoch: 7 [874/569]\tLoss: 36.7404\tLR: 0.010000\n",
      "7.8917\t6.6190\t5.8589\t5.5166\t5.4151\t5.4391\t\n",
      "Training Epoch: 7 [890/569]\tLoss: 35.2611\tLR: 0.010000\n",
      "6.5328\t5.9393\t5.6144\t5.5680\t5.7064\t5.9002\t\n",
      "Training Epoch: 7 [905/569]\tLoss: 30.4636\tLR: 0.010000\n",
      "6.0249\t5.1987\t4.7684\t4.6940\t4.7998\t4.9777\t\n",
      "[0.32046520709991455, 0.48266664147377014, 0.1968681514263153, 0.008404657244682312, 0.7093238234519958, 0.07613827288150787, 0.21453790366649628, 0.02779472805559635, 0.7551969289779663, 0.11306595802307129, 0.1317371129989624, -0.007408577017486095, 0.42760786414146423, 0.29208704829216003, 0.28030508756637573, -0.013240637257695198, 0.30129069089889526, 0.0, 0.6987093091011047, 0.004603381734341383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.6737, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5854, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5350, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5188, Accuracy: 0.1160\n",
      "Test set t = 04: Average loss: 0.5221, Accuracy: 0.1072\n",
      "Test set t = 05: Average loss: 0.5362, Accuracy: 0.0984\n",
      "\n",
      "Training Epoch: 8 [10/569]\tLoss: 36.6847\tLR: 0.010000\n",
      "7.6533\t6.3443\t5.6963\t5.5375\t5.6297\t5.8236\t\n",
      "Training Epoch: 8 [26/569]\tLoss: 38.9977\tLR: 0.010000\n",
      "8.4140\t7.1118\t6.2322\t5.8073\t5.7114\t5.7210\t\n",
      "Training Epoch: 8 [42/569]\tLoss: 41.1387\tLR: 0.010000\n",
      "8.9732\t7.6109\t6.7027\t6.1950\t5.9224\t5.7345\t\n",
      "Training Epoch: 8 [58/569]\tLoss: 28.1744\tLR: 0.010000\n",
      "5.2286\t4.6844\t4.4772\t4.4595\t4.5584\t4.7662\t\n",
      "Training Epoch: 8 [74/569]\tLoss: 38.4745\tLR: 0.010000\n",
      "8.0644\t6.9234\t6.1863\t5.8329\t5.6977\t5.7697\t\n",
      "Training Epoch: 8 [90/569]\tLoss: 35.0405\tLR: 0.010000\n",
      "6.8380\t6.1192\t5.6745\t5.4873\t5.4484\t5.4730\t\n",
      "Training Epoch: 8 [106/569]\tLoss: 27.5616\tLR: 0.010000\n",
      "5.1924\t4.5294\t4.2445\t4.3105\t4.5262\t4.7587\t\n",
      "Training Epoch: 8 [122/569]\tLoss: 37.0406\tLR: 0.010000\n",
      "7.9438\t6.6870\t5.9278\t5.5455\t5.3916\t5.5448\t\n",
      "Training Epoch: 8 [138/569]\tLoss: 24.9799\tLR: 0.010000\n",
      "4.5226\t4.1013\t3.9306\t3.9807\t4.1396\t4.3049\t\n",
      "Training Epoch: 8 [154/569]\tLoss: 31.5131\tLR: 0.010000\n",
      "5.9758\t5.4243\t5.0954\t4.9547\t4.9814\t5.0816\t\n",
      "Training Epoch: 8 [170/569]\tLoss: 35.0892\tLR: 0.010000\n",
      "6.8050\t5.9809\t5.5897\t5.4824\t5.5383\t5.6928\t\n",
      "Training Epoch: 8 [186/569]\tLoss: 25.8890\tLR: 0.010000\n",
      "4.7800\t4.2206\t3.9686\t4.0180\t4.2991\t4.6027\t\n",
      "Training Epoch: 8 [202/569]\tLoss: 30.7202\tLR: 0.010000\n",
      "5.8694\t5.0752\t4.7364\t4.7912\t5.0054\t5.2425\t\n",
      "Training Epoch: 8 [218/569]\tLoss: 34.2675\tLR: 0.010000\n",
      "7.2579\t6.2020\t5.5491\t5.2109\t5.0441\t5.0035\t\n",
      "Training Epoch: 8 [234/569]\tLoss: 41.6291\tLR: 0.010000\n",
      "8.7128\t7.4477\t6.6892\t6.3334\t6.2086\t6.2374\t\n",
      "Training Epoch: 8 [250/569]\tLoss: 30.2959\tLR: 0.010000\n",
      "6.0900\t5.2052\t4.6767\t4.5574\t4.7309\t5.0357\t\n",
      "Training Epoch: 8 [266/569]\tLoss: 28.2473\tLR: 0.010000\n",
      "5.6981\t4.9450\t4.4457\t4.2481\t4.3429\t4.5675\t\n",
      "Training Epoch: 8 [282/569]\tLoss: 43.1275\tLR: 0.010000\n",
      "9.1459\t7.7811\t6.8881\t6.4595\t6.3924\t6.4605\t\n",
      "Training Epoch: 8 [298/569]\tLoss: 43.4750\tLR: 0.010000\n",
      "9.4547\t8.0034\t6.9924\t6.4761\t6.2722\t6.2763\t\n",
      "Training Epoch: 8 [314/569]\tLoss: 38.2870\tLR: 0.010000\n",
      "7.7567\t6.8221\t6.1252\t5.8364\t5.7932\t5.9534\t\n",
      "Training Epoch: 8 [330/569]\tLoss: 33.9428\tLR: 0.010000\n",
      "6.5538\t5.8887\t5.5605\t5.4079\t5.3009\t5.2311\t\n",
      "Training Epoch: 8 [346/569]\tLoss: 34.7586\tLR: 0.010000\n",
      "7.1184\t6.1588\t5.5388\t5.3070\t5.2758\t5.3597\t\n",
      "Training Epoch: 8 [362/569]\tLoss: 29.2330\tLR: 0.010000\n",
      "5.5468\t4.8693\t4.5713\t4.5877\t4.7400\t4.9179\t\n",
      "Training Epoch: 8 [378/569]\tLoss: 33.3801\tLR: 0.010000\n",
      "6.5234\t5.7806\t5.3463\t5.1982\t5.2123\t5.3193\t\n",
      "Training Epoch: 8 [394/569]\tLoss: 33.5077\tLR: 0.010000\n",
      "6.5453\t5.6476\t5.2042\t5.1584\t5.3075\t5.6446\t\n",
      "Training Epoch: 8 [410/569]\tLoss: 39.0460\tLR: 0.010000\n",
      "8.4852\t7.1616\t6.2564\t5.8218\t5.6768\t5.6441\t\n",
      "Training Epoch: 8 [426/569]\tLoss: 40.8391\tLR: 0.010000\n",
      "9.1084\t7.3718\t6.3862\t5.9943\t5.9006\t6.0777\t\n",
      "Training Epoch: 8 [442/569]\tLoss: 36.6066\tLR: 0.010000\n",
      "7.7940\t6.5401\t5.8523\t5.5496\t5.4274\t5.4431\t\n",
      "Training Epoch: 8 [458/569]\tLoss: 30.0323\tLR: 0.010000\n",
      "6.1008\t5.1962\t4.7641\t4.6410\t4.6381\t4.6922\t\n",
      "Training Epoch: 8 [474/569]\tLoss: 34.6993\tLR: 0.010000\n",
      "7.0063\t6.1413\t5.6294\t5.3960\t5.3036\t5.2229\t\n",
      "Training Epoch: 8 [490/569]\tLoss: 29.7752\tLR: 0.010000\n",
      "5.3070\t4.7548\t4.5674\t4.7283\t5.0257\t5.3920\t\n",
      "Training Epoch: 8 [506/569]\tLoss: 34.6078\tLR: 0.010000\n",
      "6.7051\t5.9204\t5.5510\t5.4671\t5.4487\t5.5157\t\n",
      "Training Epoch: 8 [522/569]\tLoss: 26.1834\tLR: 0.010000\n",
      "4.6643\t4.2832\t4.0850\t4.1611\t4.3720\t4.6178\t\n",
      "Training Epoch: 8 [538/569]\tLoss: 32.8618\tLR: 0.010000\n",
      "6.4898\t5.5067\t5.1102\t5.1012\t5.2264\t5.4276\t\n",
      "Training Epoch: 8 [554/569]\tLoss: 34.2366\tLR: 0.010000\n",
      "7.0671\t6.1868\t5.5586\t5.2341\t5.1142\t5.0759\t\n",
      "Training Epoch: 8 [570/569]\tLoss: 38.1322\tLR: 0.010000\n",
      "8.0561\t6.8116\t6.0500\t5.7686\t5.7029\t5.7431\t\n",
      "Training Epoch: 8 [586/569]\tLoss: 31.3738\tLR: 0.010000\n",
      "5.5646\t5.1352\t5.0110\t5.0624\t5.1771\t5.4236\t\n",
      "Training Epoch: 8 [602/569]\tLoss: 33.7247\tLR: 0.010000\n",
      "6.4325\t5.6890\t5.3565\t5.3056\t5.3868\t5.5543\t\n",
      "Training Epoch: 8 [618/569]\tLoss: 25.9515\tLR: 0.010000\n",
      "4.2675\t3.9509\t3.9487\t4.1886\t4.5912\t5.0046\t\n",
      "Training Epoch: 8 [634/569]\tLoss: 34.9803\tLR: 0.010000\n",
      "6.4897\t5.7454\t5.3941\t5.3794\t5.6844\t6.2874\t\n",
      "Training Epoch: 8 [650/569]\tLoss: 30.6989\tLR: 0.010000\n",
      "6.0980\t5.2303\t4.7849\t4.7482\t4.8518\t4.9858\t\n",
      "Training Epoch: 8 [666/569]\tLoss: 28.8694\tLR: 0.010000\n",
      "5.2377\t4.7811\t4.5704\t4.6061\t4.7079\t4.9662\t\n",
      "Training Epoch: 8 [682/569]\tLoss: 28.6642\tLR: 0.010000\n",
      "5.1732\t4.6642\t4.5050\t4.5971\t4.7909\t4.9339\t\n",
      "Training Epoch: 8 [698/569]\tLoss: 40.4880\tLR: 0.010000\n",
      "8.7294\t7.3031\t6.4148\t6.0578\t5.9805\t6.0025\t\n",
      "Training Epoch: 8 [714/569]\tLoss: 25.6505\tLR: 0.010000\n",
      "4.4459\t4.0903\t3.9803\t4.1310\t4.3858\t4.6172\t\n",
      "Training Epoch: 8 [730/569]\tLoss: 24.9469\tLR: 0.010000\n",
      "4.6995\t4.1300\t3.8663\t3.8277\t4.0348\t4.3886\t\n",
      "Training Epoch: 8 [746/569]\tLoss: 32.2542\tLR: 0.010000\n",
      "6.5780\t5.5150\t4.9772\t4.9076\t5.0178\t5.2585\t\n",
      "Training Epoch: 8 [762/569]\tLoss: 33.3467\tLR: 0.010000\n",
      "6.9327\t5.9682\t5.3370\t5.0788\t5.0091\t5.0208\t\n",
      "Training Epoch: 8 [778/569]\tLoss: 33.6187\tLR: 0.010000\n",
      "7.1154\t6.0630\t5.4204\t5.0889\t4.9712\t4.9600\t\n",
      "Training Epoch: 8 [794/569]\tLoss: 29.0884\tLR: 0.010000\n",
      "5.4975\t4.7923\t4.5177\t4.5836\t4.7696\t4.9276\t\n",
      "Training Epoch: 8 [810/569]\tLoss: 38.6568\tLR: 0.010000\n",
      "8.0682\t6.9098\t6.1984\t5.8626\t5.7778\t5.8400\t\n",
      "Training Epoch: 8 [826/569]\tLoss: 31.2049\tLR: 0.010000\n",
      "5.7334\t5.0793\t4.7887\t4.8131\t5.1367\t5.6537\t\n",
      "Training Epoch: 8 [842/569]\tLoss: 41.3895\tLR: 0.010000\n",
      "8.5943\t7.4927\t6.7973\t6.3965\t6.1291\t5.9797\t\n",
      "Training Epoch: 8 [858/569]\tLoss: 32.5824\tLR: 0.010000\n",
      "6.5812\t5.7932\t5.2818\t4.9863\t4.9162\t5.0237\t\n",
      "Training Epoch: 8 [874/569]\tLoss: 39.9082\tLR: 0.010000\n",
      "7.9728\t6.9739\t6.3968\t6.1991\t6.1642\t6.2012\t\n",
      "Training Epoch: 8 [890/569]\tLoss: 34.5561\tLR: 0.010000\n",
      "6.8358\t5.9986\t5.5016\t5.3363\t5.3644\t5.5195\t\n",
      "Training Epoch: 8 [905/569]\tLoss: 34.8588\tLR: 0.010000\n",
      "7.0458\t6.0670\t5.4816\t5.3076\t5.3661\t5.5907\t\n",
      "[0.34021979570388794, 0.46559566259384155, 0.1941845417022705, 0.0064996229484677315, 0.716951310634613, 0.06509392708539963, 0.21795476227998734, 0.03001565672457218, 0.7777337431907654, 0.10398860275745392, 0.1182776540517807, -0.009181720204651356, 0.43327972292900085, 0.30184900760650635, 0.2648712694644928, -0.01604665070772171, 0.29839175939559937, 0.0, 0.7016082406044006, 0.003664875403046608]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6740, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5863, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5362, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5194, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5212, Accuracy: 0.1054\n",
      "Test set t = 05: Average loss: 0.5330, Accuracy: 0.0931\n",
      "\n",
      "Training Epoch: 9 [10/569]\tLoss: 32.6137\tLR: 0.010000\n",
      "6.4188\t5.6392\t5.1669\t5.0599\t5.1128\t5.2159\t\n",
      "Training Epoch: 9 [26/569]\tLoss: 24.3432\tLR: 0.010000\n",
      "4.1826\t3.9674\t3.9628\t4.0343\t4.0991\t4.0970\t\n",
      "Training Epoch: 9 [42/569]\tLoss: 33.8351\tLR: 0.010000\n",
      "6.3668\t5.5993\t5.2550\t5.3157\t5.5563\t5.7420\t\n",
      "Training Epoch: 9 [58/569]\tLoss: 34.6107\tLR: 0.010000\n",
      "6.8856\t5.9389\t5.4917\t5.3597\t5.3942\t5.5408\t\n",
      "Training Epoch: 9 [74/569]\tLoss: 36.9674\tLR: 0.010000\n",
      "7.7712\t6.6873\t5.9627\t5.6001\t5.4553\t5.4908\t\n",
      "Training Epoch: 9 [90/569]\tLoss: 38.1321\tLR: 0.010000\n",
      "7.9988\t6.8244\t6.1216\t5.8025\t5.6916\t5.6932\t\n",
      "Training Epoch: 9 [106/569]\tLoss: 41.5332\tLR: 0.010000\n",
      "9.1646\t7.7106\t6.6588\t6.1527\t5.9447\t5.9019\t\n",
      "Training Epoch: 9 [122/569]\tLoss: 39.2800\tLR: 0.010000\n",
      "8.5415\t7.1085\t6.1406\t5.7565\t5.7251\t6.0079\t\n",
      "Training Epoch: 9 [138/569]\tLoss: 26.7089\tLR: 0.010000\n",
      "5.4098\t4.5888\t4.1226\t4.0057\t4.1627\t4.4194\t\n",
      "Training Epoch: 9 [154/569]\tLoss: 34.9149\tLR: 0.010000\n",
      "7.2678\t6.1554\t5.5089\t5.3258\t5.2941\t5.3629\t\n",
      "Training Epoch: 9 [170/569]\tLoss: 25.4112\tLR: 0.010000\n",
      "4.8112\t4.1411\t3.8607\t3.9247\t4.1714\t4.5020\t\n",
      "Training Epoch: 9 [186/569]\tLoss: 36.3362\tLR: 0.010000\n",
      "7.8205\t6.7169\t5.9341\t5.4411\t5.2492\t5.1744\t\n",
      "Training Epoch: 9 [202/569]\tLoss: 42.0560\tLR: 0.010000\n",
      "8.7520\t7.5957\t6.8941\t6.4485\t6.2495\t6.1163\t\n",
      "Training Epoch: 9 [218/569]\tLoss: 37.0776\tLR: 0.010000\n",
      "7.4879\t6.5616\t5.9958\t5.7141\t5.6109\t5.7073\t\n",
      "Training Epoch: 9 [234/569]\tLoss: 39.0718\tLR: 0.010000\n",
      "7.9785\t6.8778\t6.1872\t5.9456\t5.9656\t6.1171\t\n",
      "Training Epoch: 9 [250/569]\tLoss: 37.6804\tLR: 0.010000\n",
      "8.0337\t6.7986\t6.0061\t5.6283\t5.5499\t5.6637\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [266/569]\tLoss: 31.3875\tLR: 0.010000\n",
      "5.8221\t5.1654\t4.9441\t4.9658\t5.1589\t5.3313\t\n",
      "Training Epoch: 9 [282/569]\tLoss: 31.8857\tLR: 0.010000\n",
      "6.2947\t5.4815\t5.0490\t4.9247\t4.9811\t5.1548\t\n",
      "Training Epoch: 9 [298/569]\tLoss: 31.1698\tLR: 0.010000\n",
      "5.8878\t5.1595\t4.8527\t4.8525\t5.0631\t5.3542\t\n",
      "Training Epoch: 9 [314/569]\tLoss: 34.4617\tLR: 0.010000\n",
      "7.2230\t6.1156\t5.4713\t5.2530\t5.1707\t5.2281\t\n",
      "Training Epoch: 9 [330/569]\tLoss: 30.2260\tLR: 0.010000\n",
      "6.0450\t5.2447\t4.8561\t4.6905\t4.6640\t4.7257\t\n",
      "Training Epoch: 9 [346/569]\tLoss: 34.7107\tLR: 0.010000\n",
      "7.1288\t6.0557\t5.4834\t5.3165\t5.3229\t5.4035\t\n",
      "Training Epoch: 9 [362/569]\tLoss: 29.0573\tLR: 0.010000\n",
      "4.8608\t4.6075\t4.5324\t4.6862\t4.9723\t5.3982\t\n",
      "Training Epoch: 9 [378/569]\tLoss: 33.6332\tLR: 0.010000\n",
      "6.7983\t5.8829\t5.3699\t5.1596\t5.1698\t5.2527\t\n",
      "Training Epoch: 9 [394/569]\tLoss: 33.8227\tLR: 0.010000\n",
      "6.2756\t5.6885\t5.4132\t5.3789\t5.4537\t5.6129\t\n",
      "Training Epoch: 9 [410/569]\tLoss: 37.1209\tLR: 0.010000\n",
      "7.5578\t6.4806\t5.9343\t5.7263\t5.6799\t5.7420\t\n",
      "Training Epoch: 9 [426/569]\tLoss: 39.0235\tLR: 0.010000\n",
      "7.9889\t6.8782\t6.2896\t5.9919\t5.8733\t6.0016\t\n",
      "Training Epoch: 9 [442/569]\tLoss: 32.5857\tLR: 0.010000\n",
      "5.8611\t5.2685\t5.0697\t5.1636\t5.4047\t5.8181\t\n",
      "Training Epoch: 9 [458/569]\tLoss: 34.9076\tLR: 0.010000\n",
      "7.4006\t6.2722\t5.5906\t5.2626\t5.1854\t5.1962\t\n",
      "Training Epoch: 9 [474/569]\tLoss: 36.9761\tLR: 0.010000\n",
      "7.1785\t6.3505\t5.8848\t5.7375\t5.7946\t6.0302\t\n",
      "Training Epoch: 9 [490/569]\tLoss: 39.1872\tLR: 0.010000\n",
      "8.2902\t7.0359\t6.2584\t5.9365\t5.8412\t5.8251\t\n",
      "Training Epoch: 9 [506/569]\tLoss: 29.6141\tLR: 0.010000\n",
      "5.5049\t4.8845\t4.6173\t4.6557\t4.8681\t5.0836\t\n",
      "Training Epoch: 9 [522/569]\tLoss: 24.4956\tLR: 0.010000\n",
      "4.1609\t3.7259\t3.6894\t3.8891\t4.2649\t4.7653\t\n",
      "Training Epoch: 9 [538/569]\tLoss: 26.8878\tLR: 0.010000\n",
      "4.6817\t4.3341\t4.2396\t4.3559\t4.5488\t4.7276\t\n",
      "Training Epoch: 9 [554/569]\tLoss: 40.5964\tLR: 0.010000\n",
      "8.7359\t7.5178\t6.6419\t6.1401\t5.8443\t5.7164\t\n",
      "Training Epoch: 9 [570/569]\tLoss: 23.2164\tLR: 0.010000\n",
      "4.0333\t3.5952\t3.5091\t3.7268\t4.0195\t4.3326\t\n",
      "Training Epoch: 9 [586/569]\tLoss: 34.4222\tLR: 0.010000\n",
      "7.3368\t6.2430\t5.5687\t5.2083\t5.0539\t5.0116\t\n",
      "Training Epoch: 9 [602/569]\tLoss: 19.9253\tLR: 0.010000\n",
      "3.5551\t3.0832\t2.9350\t3.1126\t3.4343\t3.8051\t\n",
      "Training Epoch: 9 [618/569]\tLoss: 33.7548\tLR: 0.010000\n",
      "7.0377\t6.0406\t5.3959\t5.0649\t5.0592\t5.1565\t\n",
      "Training Epoch: 9 [634/569]\tLoss: 30.6288\tLR: 0.010000\n",
      "6.1387\t5.2449\t4.7380\t4.6576\t4.8317\t5.0180\t\n",
      "Training Epoch: 9 [650/569]\tLoss: 37.5286\tLR: 0.010000\n",
      "6.8182\t6.1371\t5.9399\t6.0337\t6.1907\t6.4090\t\n",
      "Training Epoch: 9 [666/569]\tLoss: 38.3590\tLR: 0.010000\n",
      "7.3841\t6.5625\t6.1032\t6.0472\t6.0964\t6.1655\t\n",
      "Training Epoch: 9 [682/569]\tLoss: 39.0127\tLR: 0.010000\n",
      "8.2677\t7.1318\t6.3985\t5.9860\t5.6856\t5.5432\t\n",
      "Training Epoch: 9 [698/569]\tLoss: 37.9006\tLR: 0.010000\n",
      "7.8937\t6.7785\t6.1142\t5.7654\t5.6640\t5.6848\t\n",
      "Training Epoch: 9 [714/569]\tLoss: 27.1010\tLR: 0.010000\n",
      "4.6279\t4.3404\t4.2933\t4.4110\t4.5989\t4.8296\t\n",
      "Training Epoch: 9 [730/569]\tLoss: 32.6902\tLR: 0.010000\n",
      "6.8674\t5.6618\t5.0256\t4.8817\t5.0266\t5.2271\t\n",
      "Training Epoch: 9 [746/569]\tLoss: 31.3239\tLR: 0.010000\n",
      "6.2977\t5.4638\t4.9748\t4.8077\t4.8498\t4.9299\t\n",
      "Training Epoch: 9 [762/569]\tLoss: 33.0681\tLR: 0.010000\n",
      "6.2529\t5.5795\t5.2540\t5.2348\t5.3083\t5.4386\t\n",
      "Training Epoch: 9 [778/569]\tLoss: 26.2449\tLR: 0.010000\n",
      "4.7184\t4.2416\t4.0635\t4.1541\t4.3938\t4.6735\t\n",
      "Training Epoch: 9 [794/569]\tLoss: 32.4901\tLR: 0.010000\n",
      "6.6039\t5.6823\t5.1536\t4.9276\t4.9582\t5.1645\t\n",
      "Training Epoch: 9 [810/569]\tLoss: 40.8306\tLR: 0.010000\n",
      "9.5964\t7.7266\t6.5297\t5.8887\t5.5914\t5.4978\t\n",
      "Training Epoch: 9 [826/569]\tLoss: 34.3863\tLR: 0.010000\n",
      "6.7403\t5.8676\t5.4134\t5.3558\t5.4439\t5.5653\t\n",
      "Training Epoch: 9 [842/569]\tLoss: 34.7315\tLR: 0.010000\n",
      "6.4586\t5.8106\t5.4507\t5.4328\t5.5962\t5.9826\t\n",
      "Training Epoch: 9 [858/569]\tLoss: 28.8198\tLR: 0.010000\n",
      "5.0125\t4.5999\t4.5292\t4.6787\t4.8702\t5.1294\t\n",
      "Training Epoch: 9 [874/569]\tLoss: 33.6575\tLR: 0.010000\n",
      "6.8497\t5.8423\t5.3420\t5.1681\t5.1891\t5.2664\t\n",
      "Training Epoch: 9 [890/569]\tLoss: 39.3914\tLR: 0.010000\n",
      "9.4125\t7.5729\t6.2757\t5.6001\t5.3013\t5.2289\t\n",
      "Training Epoch: 9 [905/569]\tLoss: 36.2003\tLR: 0.010000\n",
      "7.0503\t6.2453\t5.8556\t5.7498\t5.6663\t5.6329\t\n",
      "[0.3427877724170685, 0.46459275484085083, 0.1926194727420807, 0.004473847337067127, 0.715740978717804, 0.05507088452577591, 0.22918813675642014, 0.03275657072663307, 0.799990177154541, 0.09328826516866684, 0.10672155767679214, -0.011153765954077244, 0.4464125335216522, 0.30693528056144714, 0.24665218591690063, -0.019070202484726906, 0.3018791377544403, 0.0, 0.6981208622455597, 0.0027168814558535814]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6739, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5843, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5348, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5190, Accuracy: 0.1142\n",
      "Test set t = 04: Average loss: 0.5216, Accuracy: 0.1054\n",
      "Test set t = 05: Average loss: 0.5339, Accuracy: 0.0914\n",
      "\n",
      "Training Epoch: 10 [10/569]\tLoss: 35.4787\tLR: 0.010000\n",
      "6.7797\t6.0298\t5.6919\t5.6290\t5.6541\t5.6943\t\n",
      "Training Epoch: 10 [26/569]\tLoss: 34.6749\tLR: 0.010000\n",
      "6.6589\t6.0485\t5.6448\t5.4710\t5.4209\t5.4309\t\n",
      "Training Epoch: 10 [42/569]\tLoss: 28.7168\tLR: 0.010000\n",
      "5.8354\t5.0054\t4.5258\t4.3875\t4.4241\t4.5386\t\n",
      "Training Epoch: 10 [58/569]\tLoss: 36.5002\tLR: 0.010000\n",
      "7.1674\t6.2369\t5.7206\t5.6187\t5.7471\t6.0096\t\n",
      "Training Epoch: 10 [74/569]\tLoss: 33.8289\tLR: 0.010000\n",
      "6.7861\t5.8681\t5.4317\t5.2569\t5.2141\t5.2720\t\n",
      "Training Epoch: 10 [90/569]\tLoss: 26.9828\tLR: 0.010000\n",
      "5.3056\t4.5688\t4.1821\t4.1072\t4.2741\t4.5449\t\n",
      "Training Epoch: 10 [106/569]\tLoss: 36.9564\tLR: 0.010000\n",
      "7.3183\t6.3700\t5.8573\t5.7004\t5.7184\t5.9920\t\n",
      "Training Epoch: 10 [122/569]\tLoss: 35.2037\tLR: 0.010000\n",
      "7.2818\t6.2907\t5.5975\t5.2907\t5.3060\t5.4369\t\n",
      "Training Epoch: 10 [138/569]\tLoss: 33.6479\tLR: 0.010000\n",
      "7.0968\t5.9631\t5.3431\t5.1018\t5.0598\t5.0832\t\n",
      "Training Epoch: 10 [154/569]\tLoss: 27.9557\tLR: 0.010000\n",
      "5.3003\t4.6700\t4.3697\t4.3974\t4.5118\t4.7064\t\n",
      "Training Epoch: 10 [170/569]\tLoss: 37.7455\tLR: 0.010000\n",
      "7.6756\t6.6481\t6.0480\t5.8108\t5.7713\t5.7916\t\n",
      "Training Epoch: 10 [186/569]\tLoss: 40.0576\tLR: 0.010000\n",
      "8.6823\t7.3577\t6.4642\t5.9672\t5.8108\t5.7753\t\n",
      "Training Epoch: 10 [202/569]\tLoss: 31.6697\tLR: 0.010000\n",
      "5.8146\t5.1195\t4.9042\t5.0270\t5.2393\t5.5650\t\n",
      "Training Epoch: 10 [218/569]\tLoss: 38.6123\tLR: 0.010000\n",
      "8.0827\t6.9875\t6.2688\t5.9137\t5.7267\t5.6329\t\n",
      "Training Epoch: 10 [234/569]\tLoss: 28.0431\tLR: 0.010000\n",
      "4.9412\t4.4447\t4.3720\t4.5302\t4.7830\t4.9721\t\n",
      "Training Epoch: 10 [250/569]\tLoss: 28.4611\tLR: 0.010000\n",
      "6.1621\t5.0363\t4.3693\t4.1863\t4.2842\t4.4230\t\n",
      "Training Epoch: 10 [266/569]\tLoss: 34.2288\tLR: 0.010000\n",
      "7.3393\t6.0641\t5.4033\t5.1536\t5.0938\t5.1747\t\n",
      "Training Epoch: 10 [282/569]\tLoss: 38.8946\tLR: 0.010000\n",
      "7.8960\t6.8709\t6.2527\t5.9834\t5.9108\t5.9807\t\n",
      "Training Epoch: 10 [298/569]\tLoss: 32.5049\tLR: 0.010000\n",
      "6.5285\t5.4668\t4.9988\t4.9669\t5.1666\t5.3773\t\n",
      "Training Epoch: 10 [314/569]\tLoss: 33.7219\tLR: 0.010000\n",
      "6.7998\t5.8556\t5.2666\t5.1117\t5.1592\t5.5292\t\n",
      "Training Epoch: 10 [330/569]\tLoss: 31.7593\tLR: 0.010000\n",
      "5.8507\t5.2145\t4.9709\t5.0407\t5.2382\t5.4442\t\n",
      "Training Epoch: 10 [346/569]\tLoss: 36.7985\tLR: 0.010000\n",
      "8.0850\t6.6121\t5.7718\t5.4173\t5.4231\t5.4891\t\n",
      "Training Epoch: 10 [362/569]\tLoss: 38.2313\tLR: 0.010000\n",
      "8.4721\t7.0846\t6.1585\t5.6847\t5.4434\t5.3880\t\n",
      "Training Epoch: 10 [378/569]\tLoss: 24.7130\tLR: 0.010000\n",
      "4.1825\t3.7884\t3.7680\t4.0033\t4.3184\t4.6524\t\n",
      "Training Epoch: 10 [394/569]\tLoss: 36.3540\tLR: 0.010000\n",
      "7.5346\t6.3621\t5.7861\t5.5740\t5.5417\t5.5555\t\n",
      "Training Epoch: 10 [410/569]\tLoss: 23.2179\tLR: 0.010000\n",
      "4.3136\t3.8171\t3.6071\t3.6454\t3.8010\t4.0337\t\n",
      "Training Epoch: 10 [426/569]\tLoss: 36.2933\tLR: 0.010000\n",
      "7.5351\t6.3627\t5.7129\t5.5476\t5.5357\t5.5993\t\n",
      "Training Epoch: 10 [442/569]\tLoss: 30.1033\tLR: 0.010000\n",
      "5.7011\t4.9506\t4.7230\t4.7313\t4.8988\t5.0985\t\n",
      "Training Epoch: 10 [458/569]\tLoss: 35.8329\tLR: 0.010000\n",
      "6.8954\t6.0714\t5.7408\t5.6472\t5.6613\t5.8169\t\n",
      "Training Epoch: 10 [474/569]\tLoss: 28.4131\tLR: 0.010000\n",
      "5.3639\t4.7185\t4.4926\t4.5244\t4.6224\t4.6913\t\n",
      "Training Epoch: 10 [490/569]\tLoss: 38.6158\tLR: 0.010000\n",
      "7.8755\t6.7596\t6.1212\t5.8699\t5.9171\t6.0725\t\n",
      "Training Epoch: 10 [506/569]\tLoss: 32.5197\tLR: 0.010000\n",
      "5.9838\t5.3429\t5.1272\t5.0998\t5.2855\t5.6804\t\n",
      "Training Epoch: 10 [522/569]\tLoss: 31.7705\tLR: 0.010000\n",
      "5.7822\t5.1534\t5.0020\t5.1165\t5.2702\t5.4462\t\n",
      "Training Epoch: 10 [538/569]\tLoss: 38.7980\tLR: 0.010000\n",
      "7.6891\t6.8129\t6.2676\t5.9882\t5.9254\t6.1148\t\n",
      "Training Epoch: 10 [554/569]\tLoss: 34.5242\tLR: 0.010000\n",
      "6.6881\t5.9872\t5.5464\t5.4314\t5.4103\t5.4608\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [570/569]\tLoss: 31.3559\tLR: 0.010000\n",
      "6.1383\t5.3277\t4.9111\t4.9006\t4.9813\t5.0970\t\n",
      "Training Epoch: 10 [586/569]\tLoss: 32.8534\tLR: 0.010000\n",
      "6.0127\t5.3991\t5.1293\t5.1775\t5.3913\t5.7435\t\n",
      "Training Epoch: 10 [602/569]\tLoss: 33.1617\tLR: 0.010000\n",
      "5.9402\t5.5232\t5.3388\t5.3488\t5.4362\t5.5745\t\n",
      "Training Epoch: 10 [618/569]\tLoss: 39.9820\tLR: 0.010000\n",
      "8.6391\t7.3032\t6.4218\t5.9887\t5.8227\t5.8064\t\n",
      "Training Epoch: 10 [634/569]\tLoss: 37.7695\tLR: 0.010000\n",
      "7.4707\t6.5920\t6.1192\t5.9173\t5.8281\t5.8423\t\n",
      "Training Epoch: 10 [650/569]\tLoss: 28.4406\tLR: 0.010000\n",
      "5.6671\t4.9230\t4.4557\t4.3215\t4.4106\t4.6628\t\n",
      "Training Epoch: 10 [666/569]\tLoss: 39.6316\tLR: 0.010000\n",
      "8.5663\t7.2576\t6.4076\t5.9277\t5.7611\t5.7112\t\n",
      "Training Epoch: 10 [682/569]\tLoss: 30.8210\tLR: 0.010000\n",
      "6.2002\t5.3906\t4.8960\t4.7343\t4.7588\t4.8412\t\n",
      "Training Epoch: 10 [698/569]\tLoss: 35.0051\tLR: 0.010000\n",
      "6.3772\t5.8355\t5.6056\t5.6013\t5.6980\t5.8875\t\n",
      "Training Epoch: 10 [714/569]\tLoss: 39.4194\tLR: 0.010000\n",
      "8.4426\t7.1796\t6.3158\t5.9170\t5.7781\t5.7863\t\n",
      "Training Epoch: 10 [730/569]\tLoss: 44.7714\tLR: 0.010000\n",
      "10.6848\t8.5801\t7.1707\t6.3955\t6.0363\t5.9040\t\n",
      "Training Epoch: 10 [746/569]\tLoss: 36.7446\tLR: 0.010000\n",
      "8.0065\t6.6039\t5.7279\t5.4292\t5.4400\t5.5371\t\n",
      "Training Epoch: 10 [762/569]\tLoss: 33.9206\tLR: 0.010000\n",
      "6.9289\t5.9462\t5.4319\t5.2296\t5.1830\t5.2009\t\n",
      "Training Epoch: 10 [778/569]\tLoss: 34.5532\tLR: 0.010000\n",
      "6.9985\t5.9941\t5.4760\t5.3178\t5.3455\t5.4213\t\n",
      "Training Epoch: 10 [794/569]\tLoss: 34.8237\tLR: 0.010000\n",
      "7.1884\t6.2380\t5.5862\t5.3227\t5.2437\t5.2446\t\n",
      "Training Epoch: 10 [810/569]\tLoss: 37.5836\tLR: 0.010000\n",
      "7.6500\t6.6683\t6.1260\t5.8222\t5.6426\t5.6745\t\n",
      "Training Epoch: 10 [826/569]\tLoss: 26.2515\tLR: 0.010000\n",
      "5.3396\t4.4537\t4.0419\t4.0035\t4.1168\t4.2960\t\n",
      "Training Epoch: 10 [842/569]\tLoss: 26.4506\tLR: 0.010000\n",
      "5.1856\t4.3799\t4.0482\t4.0834\t4.2734\t4.4801\t\n",
      "Training Epoch: 10 [858/569]\tLoss: 23.4997\tLR: 0.010000\n",
      "4.0126\t3.6317\t3.5547\t3.7083\t4.0822\t4.5101\t\n",
      "Training Epoch: 10 [874/569]\tLoss: 31.9513\tLR: 0.010000\n",
      "6.0714\t5.3979\t5.1112\t5.0254\t5.0935\t5.2518\t\n",
      "Training Epoch: 10 [890/569]\tLoss: 29.1968\tLR: 0.010000\n",
      "5.4688\t4.7385\t4.5217\t4.5969\t4.8114\t5.0594\t\n",
      "Training Epoch: 10 [905/569]\tLoss: 37.4242\tLR: 0.010000\n",
      "7.1580\t6.3487\t6.0079\t5.9396\t5.9376\t6.0323\t\n",
      "[0.35388806462287903, 0.4599531888961792, 0.18615874648094177, 0.0030333716422319412, 0.7188231945037842, 0.04950396344065666, 0.23167284205555916, 0.035623688250780106, 0.8135773539543152, 0.08859757333993912, 0.0978250727057457, -0.01326774526387453, 0.4436149597167969, 0.3286466598510742, 0.2277383804321289, -0.021649319678544998, 0.3151698112487793, 0.0, 0.6848301887512207, 0.0016562424134463072]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6738, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5800, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5317, Accuracy: 0.1090\n",
      "Test set t = 03: Average loss: 0.5187, Accuracy: 0.1107\n",
      "Test set t = 04: Average loss: 0.5233, Accuracy: 0.1072\n",
      "Test set t = 05: Average loss: 0.5374, Accuracy: 0.0949\n",
      "\n",
      "Training Epoch: 11 [10/569]\tLoss: 35.5981\tLR: 0.010000\n",
      "7.2300\t6.1003\t5.4828\t5.3547\t5.5681\t5.8623\t\n",
      "Training Epoch: 11 [26/569]\tLoss: 33.2714\tLR: 0.010000\n",
      "6.0404\t5.5859\t5.4141\t5.3412\t5.3898\t5.5001\t\n",
      "Training Epoch: 11 [42/569]\tLoss: 33.0011\tLR: 0.010000\n",
      "6.9439\t5.8086\t5.1992\t5.0245\t4.9740\t5.0509\t\n",
      "Training Epoch: 11 [58/569]\tLoss: 29.8626\tLR: 0.010000\n",
      "5.1740\t4.7797\t4.6356\t4.8020\t5.0475\t5.4238\t\n",
      "Training Epoch: 11 [74/569]\tLoss: 37.0223\tLR: 0.010000\n",
      "8.2634\t6.8332\t5.9574\t5.5062\t5.2757\t5.1865\t\n",
      "Training Epoch: 11 [90/569]\tLoss: 28.4143\tLR: 0.010000\n",
      "5.2592\t4.6124\t4.3722\t4.4076\t4.6744\t5.0885\t\n",
      "Training Epoch: 11 [106/569]\tLoss: 25.9746\tLR: 0.010000\n",
      "4.8864\t4.2354\t3.9472\t4.0089\t4.3131\t4.5836\t\n",
      "Training Epoch: 11 [122/569]\tLoss: 34.0525\tLR: 0.010000\n",
      "6.0602\t5.5050\t5.3301\t5.4143\t5.6510\t6.0920\t\n",
      "Training Epoch: 11 [138/569]\tLoss: 30.3732\tLR: 0.010000\n",
      "6.0574\t5.1362\t4.6401\t4.6185\t4.8379\t5.0831\t\n",
      "Training Epoch: 11 [154/569]\tLoss: 31.6847\tLR: 0.010000\n",
      "6.2215\t5.3539\t4.9426\t4.8847\t5.0172\t5.2648\t\n",
      "Training Epoch: 11 [170/569]\tLoss: 40.8319\tLR: 0.010000\n",
      "8.7250\t7.4032\t6.6146\t6.2045\t5.9870\t5.8976\t\n",
      "Training Epoch: 11 [186/569]\tLoss: 24.4781\tLR: 0.010000\n",
      "4.2126\t3.8642\t3.8583\t3.9811\t4.1595\t4.4024\t\n",
      "Training Epoch: 11 [202/569]\tLoss: 33.8108\tLR: 0.010000\n",
      "5.9212\t5.4453\t5.3629\t5.5031\t5.6849\t5.8934\t\n",
      "Training Epoch: 11 [218/569]\tLoss: 37.3265\tLR: 0.010000\n",
      "7.6876\t6.7619\t6.1341\t5.7442\t5.5322\t5.4667\t\n",
      "Training Epoch: 11 [234/569]\tLoss: 33.9103\tLR: 0.010000\n",
      "6.5266\t5.7602\t5.4362\t5.3590\t5.3977\t5.4307\t\n",
      "Training Epoch: 11 [250/569]\tLoss: 30.4727\tLR: 0.010000\n",
      "5.7180\t5.0474\t4.7965\t4.8112\t4.9505\t5.1491\t\n",
      "Training Epoch: 11 [266/569]\tLoss: 29.6380\tLR: 0.010000\n",
      "5.6264\t5.0507\t4.7528\t4.6524\t4.7176\t4.8380\t\n",
      "Training Epoch: 11 [282/569]\tLoss: 37.5546\tLR: 0.010000\n",
      "8.0338\t6.8116\t6.0760\t5.6895\t5.4936\t5.4502\t\n",
      "Training Epoch: 11 [298/569]\tLoss: 39.3942\tLR: 0.010000\n",
      "8.4691\t7.2300\t6.3705\t5.9213\t5.7147\t5.6886\t\n",
      "Training Epoch: 11 [314/569]\tLoss: 35.7482\tLR: 0.010000\n",
      "7.2381\t6.2222\t5.6702\t5.5052\t5.5106\t5.6020\t\n",
      "Training Epoch: 11 [330/569]\tLoss: 28.3979\tLR: 0.010000\n",
      "4.9277\t4.5622\t4.5193\t4.6114\t4.8070\t4.9704\t\n",
      "Training Epoch: 11 [346/569]\tLoss: 31.7231\tLR: 0.010000\n",
      "6.6856\t5.6202\t5.0479\t4.8293\t4.7312\t4.8089\t\n",
      "Training Epoch: 11 [362/569]\tLoss: 36.1084\tLR: 0.010000\n",
      "7.0562\t6.2559\t5.7858\t5.6367\t5.6520\t5.7218\t\n",
      "Training Epoch: 11 [378/569]\tLoss: 37.6340\tLR: 0.010000\n",
      "7.2890\t6.4806\t6.0533\t5.9091\t5.9200\t5.9819\t\n",
      "Training Epoch: 11 [394/569]\tLoss: 35.6150\tLR: 0.010000\n",
      "7.4157\t6.2622\t5.6039\t5.3871\t5.4298\t5.5163\t\n",
      "Training Epoch: 11 [410/569]\tLoss: 33.4159\tLR: 0.010000\n",
      "6.5187\t5.7214\t5.3249\t5.2387\t5.2497\t5.3626\t\n",
      "Training Epoch: 11 [426/569]\tLoss: 34.9339\tLR: 0.010000\n",
      "7.0709\t6.0568\t5.5122\t5.3347\t5.3939\t5.5653\t\n",
      "Training Epoch: 11 [442/569]\tLoss: 29.2896\tLR: 0.010000\n",
      "5.3988\t4.8703\t4.5943\t4.5912\t4.7952\t5.0398\t\n",
      "Training Epoch: 11 [458/569]\tLoss: 28.7529\tLR: 0.010000\n",
      "5.5069\t4.8557\t4.5567\t4.5206\t4.6212\t4.6918\t\n",
      "Training Epoch: 11 [474/569]\tLoss: 41.2075\tLR: 0.010000\n",
      "8.5730\t7.2271\t6.4861\t6.1974\t6.2092\t6.5148\t\n",
      "Training Epoch: 11 [490/569]\tLoss: 31.3507\tLR: 0.010000\n",
      "6.3789\t5.4941\t4.9760\t4.8205\t4.8151\t4.8661\t\n",
      "Training Epoch: 11 [506/569]\tLoss: 36.1367\tLR: 0.010000\n",
      "8.1909\t6.6024\t5.6893\t5.2985\t5.1660\t5.1896\t\n",
      "Training Epoch: 11 [522/569]\tLoss: 37.1764\tLR: 0.010000\n",
      "7.7552\t6.5449\t5.8810\t5.6645\t5.6398\t5.6910\t\n",
      "Training Epoch: 11 [538/569]\tLoss: 29.7730\tLR: 0.010000\n",
      "5.8217\t5.1183\t4.7477\t4.6241\t4.6619\t4.7992\t\n",
      "Training Epoch: 11 [554/569]\tLoss: 37.5586\tLR: 0.010000\n",
      "8.2730\t6.8678\t6.0203\t5.6178\t5.4503\t5.3293\t\n",
      "Training Epoch: 11 [570/569]\tLoss: 31.3103\tLR: 0.010000\n",
      "6.0441\t5.1518\t4.8625\t4.9053\t5.0652\t5.2815\t\n",
      "Training Epoch: 11 [586/569]\tLoss: 28.5990\tLR: 0.010000\n",
      "5.4679\t4.7829\t4.4131\t4.4317\t4.6465\t4.8568\t\n",
      "Training Epoch: 11 [602/569]\tLoss: 31.3228\tLR: 0.010000\n",
      "6.1352\t5.3563\t4.9898\t4.9057\t4.9311\t5.0047\t\n",
      "Training Epoch: 11 [618/569]\tLoss: 36.0228\tLR: 0.010000\n",
      "7.0295\t6.2640\t5.8355\t5.6132\t5.5868\t5.6938\t\n",
      "Training Epoch: 11 [634/569]\tLoss: 42.1932\tLR: 0.010000\n",
      "9.3640\t7.6253\t6.6405\t6.2076\t6.1342\t6.2218\t\n",
      "Training Epoch: 11 [650/569]\tLoss: 33.7588\tLR: 0.010000\n",
      "6.6347\t5.7825\t5.3560\t5.2560\t5.3103\t5.4193\t\n",
      "Training Epoch: 11 [666/569]\tLoss: 39.4286\tLR: 0.010000\n",
      "9.2183\t7.4751\t6.3628\t5.7517\t5.3851\t5.2356\t\n",
      "Training Epoch: 11 [682/569]\tLoss: 31.8038\tLR: 0.010000\n",
      "6.1424\t5.3855\t5.0418\t5.0197\t5.0829\t5.1315\t\n",
      "Training Epoch: 11 [698/569]\tLoss: 30.4251\tLR: 0.010000\n",
      "5.3907\t4.9664\t4.8312\t4.9147\t5.0621\t5.2601\t\n",
      "Training Epoch: 11 [714/569]\tLoss: 29.0554\tLR: 0.010000\n",
      "6.1711\t5.1184\t4.4729\t4.3353\t4.3975\t4.5603\t\n",
      "Training Epoch: 11 [730/569]\tLoss: 28.5191\tLR: 0.010000\n",
      "5.6355\t4.7722\t4.4397\t4.4282\t4.5298\t4.7136\t\n",
      "Training Epoch: 11 [746/569]\tLoss: 36.5152\tLR: 0.010000\n",
      "7.1486\t6.2581\t5.8593\t5.7709\t5.7429\t5.7354\t\n",
      "Training Epoch: 11 [762/569]\tLoss: 37.2848\tLR: 0.010000\n",
      "7.6138\t6.5631\t5.9199\t5.7013\t5.6808\t5.8060\t\n",
      "Training Epoch: 11 [778/569]\tLoss: 41.3339\tLR: 0.010000\n",
      "8.7592\t7.5057\t6.7222\t6.2834\t6.0525\t6.0109\t\n",
      "Training Epoch: 11 [794/569]\tLoss: 35.1400\tLR: 0.010000\n",
      "6.7387\t5.8433\t5.4349\t5.4255\t5.6640\t6.0335\t\n",
      "Training Epoch: 11 [810/569]\tLoss: 43.7439\tLR: 0.010000\n",
      "9.4258\t7.8767\t6.9543\t6.5379\t6.3995\t6.5497\t\n",
      "Training Epoch: 11 [826/569]\tLoss: 32.9332\tLR: 0.010000\n",
      "6.6832\t5.7065\t5.1797\t5.0278\t5.0912\t5.2449\t\n",
      "Training Epoch: 11 [842/569]\tLoss: 26.7799\tLR: 0.010000\n",
      "5.7409\t4.7037\t4.1155\t3.9274\t4.0137\t4.2787\t\n",
      "Training Epoch: 11 [858/569]\tLoss: 26.0397\tLR: 0.010000\n",
      "5.0129\t4.2615\t3.9660\t4.0309\t4.2604\t4.5080\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [874/569]\tLoss: 35.0811\tLR: 0.010000\n",
      "6.8912\t5.9629\t5.5986\t5.4876\t5.5206\t5.6202\t\n",
      "Training Epoch: 11 [890/569]\tLoss: 35.0354\tLR: 0.010000\n",
      "7.3077\t6.1335\t5.4566\t5.2763\t5.2997\t5.5617\t\n",
      "Training Epoch: 11 [905/569]\tLoss: 31.2407\tLR: 0.010000\n",
      "5.6937\t5.1481\t4.9510\t5.0436\t5.1707\t5.2337\t\n",
      "[0.3570960760116577, 0.4599054753780365, 0.1829984486103058, 0.0012798053212463856, 0.7216330170631409, 0.04297565668821335, 0.23539132624864578, 0.0388406440615654, 0.8306601047515869, 0.07939977943897247, 0.08994011580944061, -0.015263509005308151, 0.45924004912376404, 0.3300413191318512, 0.21071863174438477, -0.024373017251491547, 0.3186728060245514, 0.0, 0.6813271939754486, 0.0007144135306589305]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6739, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5784, Accuracy: 0.1037\n",
      "Test set t = 02: Average loss: 0.5307, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5185, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5238, Accuracy: 0.1054\n",
      "Test set t = 05: Average loss: 0.5383, Accuracy: 0.0949\n",
      "\n",
      "Training Epoch: 12 [10/569]\tLoss: 27.4156\tLR: 0.010000\n",
      "4.7480\t4.3246\t4.2334\t4.3474\t4.6605\t5.1017\t\n",
      "Training Epoch: 12 [26/569]\tLoss: 35.4954\tLR: 0.010000\n",
      "6.4764\t5.8625\t5.6338\t5.6948\t5.8298\t5.9981\t\n",
      "Training Epoch: 12 [42/569]\tLoss: 32.5213\tLR: 0.010000\n",
      "6.6692\t5.6368\t5.1458\t5.0025\t5.0019\t5.0651\t\n",
      "Training Epoch: 12 [58/569]\tLoss: 37.5957\tLR: 0.010000\n",
      "8.0709\t6.8199\t6.1393\t5.7045\t5.4657\t5.3954\t\n",
      "Training Epoch: 12 [74/569]\tLoss: 28.6070\tLR: 0.010000\n",
      "6.0278\t5.0474\t4.4252\t4.2389\t4.3426\t4.5251\t\n",
      "Training Epoch: 12 [90/569]\tLoss: 29.0891\tLR: 0.010000\n",
      "5.5086\t4.8885\t4.6062\t4.5726\t4.7108\t4.8025\t\n",
      "Training Epoch: 12 [106/569]\tLoss: 31.2505\tLR: 0.010000\n",
      "5.6939\t5.0578\t4.7887\t4.9236\t5.2210\t5.5655\t\n",
      "Training Epoch: 12 [122/569]\tLoss: 32.0379\tLR: 0.010000\n",
      "5.7297\t5.3261\t5.1482\t5.1552\t5.2932\t5.3855\t\n",
      "Training Epoch: 12 [138/569]\tLoss: 38.8553\tLR: 0.010000\n",
      "8.5032\t7.0416\t6.1790\t5.7979\t5.6118\t5.7220\t\n",
      "Training Epoch: 12 [154/569]\tLoss: 35.8854\tLR: 0.010000\n",
      "7.4401\t6.3275\t5.6733\t5.4412\t5.4495\t5.5537\t\n",
      "Training Epoch: 12 [170/569]\tLoss: 32.1018\tLR: 0.010000\n",
      "5.9406\t5.3532\t5.1314\t5.1166\t5.2001\t5.3599\t\n",
      "Training Epoch: 12 [186/569]\tLoss: 29.7912\tLR: 0.010000\n",
      "6.4290\t5.3892\t4.7132\t4.4429\t4.3706\t4.4463\t\n",
      "Training Epoch: 12 [202/569]\tLoss: 29.7184\tLR: 0.010000\n",
      "5.4354\t4.8708\t4.6780\t4.7423\t4.9043\t5.0876\t\n",
      "Training Epoch: 12 [218/569]\tLoss: 36.0180\tLR: 0.010000\n",
      "7.5911\t6.3688\t5.6894\t5.4243\t5.4274\t5.5169\t\n",
      "Training Epoch: 12 [234/569]\tLoss: 33.3719\tLR: 0.010000\n",
      "6.4113\t5.7250\t5.3246\t5.2374\t5.2753\t5.3984\t\n",
      "Training Epoch: 12 [250/569]\tLoss: 35.9335\tLR: 0.010000\n",
      "7.1217\t6.1387\t5.7150\t5.5886\t5.6336\t5.7358\t\n",
      "Training Epoch: 12 [266/569]\tLoss: 40.4281\tLR: 0.010000\n",
      "8.6023\t7.2974\t6.4929\t6.1267\t5.9623\t5.9465\t\n",
      "Training Epoch: 12 [282/569]\tLoss: 21.9464\tLR: 0.010000\n",
      "3.0703\t3.0957\t3.3670\t3.7822\t4.1575\t4.4737\t\n",
      "Training Epoch: 12 [298/569]\tLoss: 34.2368\tLR: 0.010000\n",
      "6.6017\t5.7981\t5.3957\t5.3519\t5.4329\t5.6565\t\n",
      "Training Epoch: 12 [314/569]\tLoss: 35.7545\tLR: 0.010000\n",
      "7.4925\t6.3699\t5.7738\t5.4928\t5.3541\t5.2714\t\n",
      "Training Epoch: 12 [330/569]\tLoss: 30.4438\tLR: 0.010000\n",
      "6.2085\t5.3172\t4.7811\t4.6226\t4.6769\t4.8374\t\n",
      "Training Epoch: 12 [346/569]\tLoss: 34.8627\tLR: 0.010000\n",
      "6.6404\t5.8653\t5.5769\t5.5329\t5.5807\t5.6665\t\n",
      "Training Epoch: 12 [362/569]\tLoss: 39.0525\tLR: 0.010000\n",
      "8.3955\t7.0647\t6.2370\t5.8242\t5.7400\t5.7911\t\n",
      "Training Epoch: 12 [378/569]\tLoss: 41.0578\tLR: 0.010000\n",
      "8.6846\t7.4286\t6.6260\t6.2867\t6.0819\t5.9500\t\n",
      "Training Epoch: 12 [394/569]\tLoss: 40.8151\tLR: 0.010000\n",
      "8.6585\t7.3272\t6.5036\t6.1839\t6.0608\t6.0811\t\n",
      "Training Epoch: 12 [410/569]\tLoss: 32.5768\tLR: 0.010000\n",
      "6.8424\t5.7100\t5.1068\t4.9088\t4.9523\t5.0565\t\n",
      "Training Epoch: 12 [426/569]\tLoss: 41.1957\tLR: 0.010000\n",
      "8.6933\t7.2862\t6.5723\t6.2516\t6.1572\t6.2350\t\n",
      "Training Epoch: 12 [442/569]\tLoss: 37.3476\tLR: 0.010000\n",
      "7.9356\t6.7414\t6.0072\t5.6822\t5.5151\t5.4660\t\n",
      "Training Epoch: 12 [458/569]\tLoss: 37.8887\tLR: 0.010000\n",
      "8.3302\t6.7562\t5.9271\t5.6306\t5.5981\t5.6465\t\n",
      "Training Epoch: 12 [474/569]\tLoss: 32.0248\tLR: 0.010000\n",
      "6.3918\t5.5588\t5.1283\t4.9650\t4.9588\t5.0221\t\n",
      "Training Epoch: 12 [490/569]\tLoss: 26.2529\tLR: 0.010000\n",
      "4.7350\t4.1135\t4.0530\t4.2592\t4.4584\t4.6337\t\n",
      "Training Epoch: 12 [506/569]\tLoss: 30.1529\tLR: 0.010000\n",
      "5.7040\t4.9848\t4.7166\t4.7277\t4.8964\t5.1234\t\n",
      "Training Epoch: 12 [522/569]\tLoss: 33.3364\tLR: 0.010000\n",
      "6.1957\t5.3263\t5.0687\t5.1957\t5.5211\t6.0288\t\n",
      "Training Epoch: 12 [538/569]\tLoss: 27.3013\tLR: 0.010000\n",
      "5.5786\t4.5238\t4.1084\t4.1453\t4.3580\t4.5872\t\n",
      "Training Epoch: 12 [554/569]\tLoss: 33.8752\tLR: 0.010000\n",
      "6.4989\t5.6949\t5.4120\t5.3734\t5.4224\t5.4737\t\n",
      "Training Epoch: 12 [570/569]\tLoss: 32.9231\tLR: 0.010000\n",
      "7.0857\t5.8801\t5.1936\t4.9258\t4.8825\t4.9554\t\n",
      "Training Epoch: 12 [586/569]\tLoss: 41.3686\tLR: 0.010000\n",
      "9.2260\t7.5980\t6.5868\t6.1285\t5.9361\t5.8932\t\n",
      "Training Epoch: 12 [602/569]\tLoss: 33.6372\tLR: 0.010000\n",
      "6.8500\t5.7740\t5.2475\t5.1268\t5.1892\t5.4499\t\n",
      "Training Epoch: 12 [618/569]\tLoss: 36.9902\tLR: 0.010000\n",
      "7.5006\t6.2877\t5.7384\t5.6354\t5.7304\t6.0977\t\n",
      "Training Epoch: 12 [634/569]\tLoss: 34.5216\tLR: 0.010000\n",
      "7.0207\t6.0871\t5.5289\t5.2910\t5.2815\t5.3124\t\n",
      "Training Epoch: 12 [650/569]\tLoss: 29.6759\tLR: 0.010000\n",
      "5.5057\t4.8399\t4.5812\t4.6800\t4.9194\t5.1498\t\n",
      "Training Epoch: 12 [666/569]\tLoss: 37.0751\tLR: 0.010000\n",
      "7.2989\t6.3606\t5.9254\t5.7932\t5.7999\t5.8971\t\n",
      "Training Epoch: 12 [682/569]\tLoss: 32.8238\tLR: 0.010000\n",
      "7.4784\t5.9134\t5.0966\t4.8135\t4.7198\t4.8022\t\n",
      "Training Epoch: 12 [698/569]\tLoss: 33.9242\tLR: 0.010000\n",
      "6.5291\t5.6796\t5.3107\t5.2665\t5.4495\t5.6888\t\n",
      "Training Epoch: 12 [714/569]\tLoss: 35.6063\tLR: 0.010000\n",
      "7.2934\t6.3033\t5.7853\t5.5122\t5.3741\t5.3380\t\n",
      "Training Epoch: 12 [730/569]\tLoss: 36.3760\tLR: 0.010000\n",
      "7.1023\t6.2519\t5.8296\t5.6854\t5.7001\t5.8067\t\n",
      "Training Epoch: 12 [746/569]\tLoss: 37.5417\tLR: 0.010000\n",
      "7.9296\t6.7644\t6.0162\t5.6662\t5.5943\t5.5710\t\n",
      "Training Epoch: 12 [762/569]\tLoss: 27.1185\tLR: 0.010000\n",
      "4.7931\t4.2897\t4.2249\t4.4118\t4.6106\t4.7883\t\n",
      "Training Epoch: 12 [778/569]\tLoss: 36.7816\tLR: 0.010000\n",
      "7.9444\t6.5047\t5.7799\t5.5166\t5.4941\t5.5419\t\n",
      "Training Epoch: 12 [794/569]\tLoss: 29.9062\tLR: 0.010000\n",
      "5.7939\t4.9372\t4.6666\t4.6655\t4.8147\t5.0282\t\n",
      "Training Epoch: 12 [810/569]\tLoss: 33.0385\tLR: 0.010000\n",
      "6.4595\t5.5226\t5.1477\t5.1444\t5.2841\t5.4801\t\n",
      "Training Epoch: 12 [826/569]\tLoss: 27.3968\tLR: 0.010000\n",
      "5.1719\t4.4624\t4.1695\t4.2289\t4.4858\t4.8784\t\n",
      "Training Epoch: 12 [842/569]\tLoss: 36.1983\tLR: 0.010000\n",
      "7.1934\t6.1654\t5.7631\t5.6306\t5.6601\t5.7857\t\n",
      "Training Epoch: 12 [858/569]\tLoss: 28.0028\tLR: 0.010000\n",
      "5.1234\t4.5316\t4.2592\t4.3362\t4.5967\t5.1558\t\n",
      "Training Epoch: 12 [874/569]\tLoss: 37.2654\tLR: 0.010000\n",
      "7.8726\t6.5164\t5.7393\t5.5553\t5.6705\t5.9112\t\n",
      "Training Epoch: 12 [890/569]\tLoss: 34.7348\tLR: 0.010000\n",
      "6.9918\t6.0865\t5.6148\t5.3997\t5.2970\t5.3450\t\n",
      "Training Epoch: 12 [905/569]\tLoss: 24.1278\tLR: 0.010000\n",
      "4.0175\t3.7051\t3.7230\t3.9358\t4.2313\t4.5151\t\n",
      "[0.37339097261428833, 0.4469043016433716, 0.1797047257423401, -0.0005730965058319271, 0.7257964015007019, 0.037788599729537964, 0.23641499876976013, 0.04197241738438606, 0.8426232933998108, 0.07399976998567581, 0.0833769366145134, -0.017470700666308403, 0.4593808054924011, 0.34497204422950745, 0.19564715027809143, -0.027333885431289673, 0.3222719132900238, 0.0, 0.6777280867099762, -0.0005882689147256315]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6743, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5777, Accuracy: 0.1019\n",
      "Test set t = 02: Average loss: 0.5306, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5188, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5238, Accuracy: 0.1019\n",
      "Test set t = 05: Average loss: 0.5376, Accuracy: 0.0931\n",
      "\n",
      "Training Epoch: 13 [10/569]\tLoss: 36.6775\tLR: 0.010000\n",
      "7.8198\t6.5939\t5.8548\t5.5235\t5.4303\t5.4553\t\n",
      "Training Epoch: 13 [26/569]\tLoss: 33.6907\tLR: 0.010000\n",
      "7.0569\t5.9189\t5.2657\t5.0977\t5.1009\t5.2505\t\n",
      "Training Epoch: 13 [42/569]\tLoss: 35.0487\tLR: 0.010000\n",
      "7.2206\t6.1601\t5.6119\t5.3311\t5.3272\t5.3978\t\n",
      "Training Epoch: 13 [58/569]\tLoss: 33.7981\tLR: 0.010000\n",
      "7.1758\t6.0210\t5.2968\t5.0729\t5.0735\t5.1581\t\n",
      "Training Epoch: 13 [74/569]\tLoss: 35.9137\tLR: 0.010000\n",
      "7.5654\t6.3972\t5.7397\t5.4430\t5.3831\t5.3853\t\n",
      "Training Epoch: 13 [90/569]\tLoss: 34.8320\tLR: 0.010000\n",
      "7.8472\t6.3598\t5.4728\t5.0478\t4.9932\t5.1113\t\n",
      "Training Epoch: 13 [106/569]\tLoss: 33.5073\tLR: 0.010000\n",
      "6.9685\t5.9397\t5.3335\t5.0698\t5.0591\t5.1366\t\n",
      "Training Epoch: 13 [122/569]\tLoss: 28.3161\tLR: 0.010000\n",
      "4.9531\t4.5291\t4.4306\t4.5821\t4.7999\t5.0212\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13 [138/569]\tLoss: 30.0213\tLR: 0.010000\n",
      "5.0762\t4.7449\t4.8113\t4.9831\t5.1538\t5.2520\t\n",
      "Training Epoch: 13 [154/569]\tLoss: 30.7658\tLR: 0.010000\n",
      "6.0320\t5.2768\t4.9065\t4.7843\t4.8139\t4.9523\t\n",
      "Training Epoch: 13 [170/569]\tLoss: 36.0868\tLR: 0.010000\n",
      "7.2228\t6.0796\t5.5810\t5.5522\t5.6930\t5.9582\t\n",
      "Training Epoch: 13 [186/569]\tLoss: 34.0424\tLR: 0.010000\n",
      "6.5925\t5.7901\t5.4348\t5.3277\t5.3736\t5.5237\t\n",
      "Training Epoch: 13 [202/569]\tLoss: 26.6111\tLR: 0.010000\n",
      "4.8179\t4.2964\t4.2114\t4.2764\t4.4095\t4.5995\t\n",
      "Training Epoch: 13 [218/569]\tLoss: 36.6434\tLR: 0.010000\n",
      "7.9515\t6.4649\t5.7526\t5.4752\t5.4298\t5.5694\t\n",
      "Training Epoch: 13 [234/569]\tLoss: 42.2023\tLR: 0.010000\n",
      "9.8760\t7.8612\t6.6920\t6.1008\t5.8680\t5.8043\t\n",
      "Training Epoch: 13 [250/569]\tLoss: 31.9557\tLR: 0.010000\n",
      "6.6842\t5.5469\t4.9333\t4.8024\t4.8949\t5.0940\t\n",
      "Training Epoch: 13 [266/569]\tLoss: 30.5568\tLR: 0.010000\n",
      "5.8564\t5.1019\t4.8175\t4.8136\t4.9242\t5.0433\t\n",
      "Training Epoch: 13 [282/569]\tLoss: 37.2150\tLR: 0.010000\n",
      "7.3989\t6.3851\t5.9115\t5.7562\t5.7903\t5.9731\t\n",
      "Training Epoch: 13 [298/569]\tLoss: 32.0205\tLR: 0.010000\n",
      "5.8432\t5.3256\t5.1657\t5.1993\t5.2220\t5.2648\t\n",
      "Training Epoch: 13 [314/569]\tLoss: 37.5095\tLR: 0.010000\n",
      "7.5842\t6.6027\t6.0620\t5.8028\t5.7230\t5.7348\t\n",
      "Training Epoch: 13 [330/569]\tLoss: 26.8811\tLR: 0.010000\n",
      "4.8754\t4.3131\t4.1704\t4.3198\t4.4999\t4.7026\t\n",
      "Training Epoch: 13 [346/569]\tLoss: 30.3081\tLR: 0.010000\n",
      "6.4687\t5.3558\t4.7290\t4.5648\t4.5602\t4.6296\t\n",
      "Training Epoch: 13 [362/569]\tLoss: 40.1156\tLR: 0.010000\n",
      "9.1364\t7.3116\t6.2518\t5.8188\t5.7631\t5.8339\t\n",
      "Training Epoch: 13 [378/569]\tLoss: 34.4859\tLR: 0.010000\n",
      "6.6678\t5.8508\t5.4543\t5.3055\t5.3573\t5.8504\t\n",
      "Training Epoch: 13 [394/569]\tLoss: 32.0759\tLR: 0.010000\n",
      "5.9515\t5.1939\t4.9895\t5.1009\t5.2998\t5.5402\t\n",
      "Training Epoch: 13 [410/569]\tLoss: 37.0520\tLR: 0.010000\n",
      "7.6685\t6.3921\t5.8036\t5.6346\t5.7025\t5.8508\t\n",
      "Training Epoch: 13 [426/569]\tLoss: 35.9766\tLR: 0.010000\n",
      "7.7054\t6.4571\t5.7525\t5.4645\t5.3249\t5.2722\t\n",
      "Training Epoch: 13 [442/569]\tLoss: 39.2569\tLR: 0.010000\n",
      "8.0701\t7.0340\t6.3928\t6.0462\t5.8932\t5.8205\t\n",
      "Training Epoch: 13 [458/569]\tLoss: 23.1379\tLR: 0.010000\n",
      "4.5601\t3.7878\t3.4255\t3.5534\t3.7883\t4.0228\t\n",
      "Training Epoch: 13 [474/569]\tLoss: 40.9110\tLR: 0.010000\n",
      "8.1029\t7.1132\t6.5017\t6.2733\t6.2922\t6.6277\t\n",
      "Training Epoch: 13 [490/569]\tLoss: 33.4520\tLR: 0.010000\n",
      "7.1699\t5.9184\t5.1855\t4.9780\t5.0325\t5.1677\t\n",
      "Training Epoch: 13 [506/569]\tLoss: 39.4612\tLR: 0.010000\n",
      "8.6285\t7.1680\t6.3498\t5.9634\t5.7427\t5.6088\t\n",
      "Training Epoch: 13 [522/569]\tLoss: 30.2031\tLR: 0.010000\n",
      "5.4381\t4.9391\t4.8127\t4.8995\t4.9863\t5.1274\t\n",
      "Training Epoch: 13 [538/569]\tLoss: 28.8925\tLR: 0.010000\n",
      "5.0363\t4.6238\t4.5850\t4.7182\t4.8864\t5.0429\t\n",
      "Training Epoch: 13 [554/569]\tLoss: 32.0526\tLR: 0.010000\n",
      "6.2376\t5.3823\t5.0110\t4.9963\t5.1306\t5.2949\t\n",
      "Training Epoch: 13 [570/569]\tLoss: 34.8124\tLR: 0.010000\n",
      "6.7951\t5.8808\t5.4899\t5.4518\t5.5182\t5.6765\t\n",
      "Training Epoch: 13 [586/569]\tLoss: 32.9285\tLR: 0.010000\n",
      "6.7679\t5.6052\t5.1098\t5.0667\t5.1367\t5.2422\t\n",
      "Training Epoch: 13 [602/569]\tLoss: 37.2056\tLR: 0.010000\n",
      "8.1753\t6.7300\t5.9302\t5.5757\t5.4298\t5.3646\t\n",
      "Training Epoch: 13 [618/569]\tLoss: 27.3768\tLR: 0.010000\n",
      "4.8152\t4.3785\t4.2794\t4.4010\t4.6391\t4.8636\t\n",
      "Training Epoch: 13 [634/569]\tLoss: 33.1996\tLR: 0.010000\n",
      "5.7777\t5.3243\t5.1922\t5.3380\t5.6142\t5.9532\t\n",
      "Training Epoch: 13 [650/569]\tLoss: 28.9207\tLR: 0.010000\n",
      "5.1062\t4.6952\t4.5533\t4.6589\t4.8442\t5.0627\t\n",
      "Training Epoch: 13 [666/569]\tLoss: 33.9357\tLR: 0.010000\n",
      "7.3079\t6.0046\t5.2961\t5.0174\t5.0472\t5.2625\t\n",
      "Training Epoch: 13 [682/569]\tLoss: 26.6373\tLR: 0.010000\n",
      "4.7129\t4.2181\t4.1002\t4.2388\t4.5229\t4.8443\t\n",
      "Training Epoch: 13 [698/569]\tLoss: 27.1290\tLR: 0.010000\n",
      "4.9340\t4.4118\t4.1438\t4.2768\t4.5549\t4.8077\t\n",
      "Training Epoch: 13 [714/569]\tLoss: 37.3882\tLR: 0.010000\n",
      "7.4834\t6.4837\t6.0025\t5.8039\t5.7846\t5.8302\t\n",
      "Training Epoch: 13 [730/569]\tLoss: 34.0682\tLR: 0.010000\n",
      "7.0998\t5.9040\t5.3255\t5.1809\t5.2320\t5.3258\t\n",
      "Training Epoch: 13 [746/569]\tLoss: 31.4999\tLR: 0.010000\n",
      "5.6066\t5.0985\t5.0028\t5.0705\t5.2456\t5.4758\t\n",
      "Training Epoch: 13 [762/569]\tLoss: 31.7735\tLR: 0.010000\n",
      "6.8324\t5.6101\t4.9615\t4.7656\t4.7568\t4.8471\t\n",
      "Training Epoch: 13 [778/569]\tLoss: 29.3421\tLR: 0.010000\n",
      "5.5008\t4.9003\t4.6681\t4.6689\t4.7565\t4.8474\t\n",
      "Training Epoch: 13 [794/569]\tLoss: 32.9697\tLR: 0.010000\n",
      "5.5513\t5.3848\t5.3809\t5.4076\t5.5124\t5.7326\t\n",
      "Training Epoch: 13 [810/569]\tLoss: 39.2422\tLR: 0.010000\n",
      "7.5665\t6.7102\t6.2863\t6.1997\t6.2199\t6.2596\t\n",
      "Training Epoch: 13 [826/569]\tLoss: 32.5895\tLR: 0.010000\n",
      "6.9789\t5.8448\t5.1998\t4.9378\t4.8147\t4.8134\t\n",
      "Training Epoch: 13 [842/569]\tLoss: 39.8845\tLR: 0.010000\n",
      "8.0736\t7.1121\t6.4904\t6.1674\t6.0174\t6.0235\t\n",
      "Training Epoch: 13 [858/569]\tLoss: 32.0258\tLR: 0.010000\n",
      "6.5753\t5.5083\t5.0132\t4.9184\t4.9534\t5.0572\t\n",
      "Training Epoch: 13 [874/569]\tLoss: 42.4645\tLR: 0.010000\n",
      "9.2563\t7.7678\t6.8730\t6.4202\t6.1483\t5.9989\t\n",
      "Training Epoch: 13 [890/569]\tLoss: 34.9510\tLR: 0.010000\n",
      "7.0380\t5.9635\t5.4548\t5.3343\t5.4504\t5.7100\t\n",
      "Training Epoch: 13 [905/569]\tLoss: 30.3790\tLR: 0.010000\n",
      "6.2459\t5.2727\t4.8050\t4.6810\t4.6753\t4.6992\t\n",
      "[0.39572086930274963, 0.42639392614364624, 0.17788520455360413, -0.00282731419429183, 0.7292148470878601, 0.03292448818683624, 0.23786066472530365, 0.04517823085188866, 0.8539494276046753, 0.06783246248960495, 0.07821810990571976, -0.02003159560263157, 0.4669627249240875, 0.3504180908203125, 0.18261918425559998, -0.030423684045672417, 0.3113633096218109, 0.0, 0.6886366903781891, -0.0020723866764456034]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6741, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5809, Accuracy: 0.1019\n",
      "Test set t = 02: Average loss: 0.5334, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5193, Accuracy: 0.1125\n",
      "Test set t = 04: Average loss: 0.5213, Accuracy: 0.1037\n",
      "Test set t = 05: Average loss: 0.5316, Accuracy: 0.0949\n",
      "\n",
      "Training Epoch: 14 [10/569]\tLoss: 30.8649\tLR: 0.010000\n",
      "5.7135\t5.0458\t4.7757\t4.8426\t5.0816\t5.4057\t\n",
      "Training Epoch: 14 [26/569]\tLoss: 34.3307\tLR: 0.010000\n",
      "6.6071\t5.8589\t5.4634\t5.3784\t5.4555\t5.5674\t\n",
      "Training Epoch: 14 [42/569]\tLoss: 29.0612\tLR: 0.010000\n",
      "5.2743\t4.7695\t4.6100\t4.6761\t4.7961\t4.9351\t\n",
      "Training Epoch: 14 [58/569]\tLoss: 24.4291\tLR: 0.010000\n",
      "4.0505\t3.8760\t3.8676\t4.0179\t4.2029\t4.4142\t\n",
      "Training Epoch: 14 [74/569]\tLoss: 33.9725\tLR: 0.010000\n",
      "6.7997\t5.8398\t5.4171\t5.2892\t5.2909\t5.3358\t\n",
      "Training Epoch: 14 [90/569]\tLoss: 29.5205\tLR: 0.010000\n",
      "5.1386\t4.8396\t4.7352\t4.7937\t4.9435\t5.0700\t\n",
      "Training Epoch: 14 [106/569]\tLoss: 33.5409\tLR: 0.010000\n",
      "6.8806\t5.7904\t5.2775\t5.1421\t5.1665\t5.2837\t\n",
      "Training Epoch: 14 [122/569]\tLoss: 43.7012\tLR: 0.010000\n",
      "9.5339\t8.0412\t7.0972\t6.5599\t6.2917\t6.1772\t\n",
      "Training Epoch: 14 [138/569]\tLoss: 32.9086\tLR: 0.010000\n",
      "6.8335\t5.7051\t5.1383\t4.9732\t5.0462\t5.2122\t\n",
      "Training Epoch: 14 [154/569]\tLoss: 32.2332\tLR: 0.010000\n",
      "6.2293\t5.5703\t5.2079\t5.0824\t5.0410\t5.1023\t\n",
      "Training Epoch: 14 [170/569]\tLoss: 29.2768\tLR: 0.010000\n",
      "5.6507\t4.8791\t4.6165\t4.6108\t4.7217\t4.7980\t\n",
      "Training Epoch: 14 [186/569]\tLoss: 31.4032\tLR: 0.010000\n",
      "6.0140\t5.3497\t4.9897\t4.9082\t4.9858\t5.1559\t\n",
      "Training Epoch: 14 [202/569]\tLoss: 33.7897\tLR: 0.010000\n",
      "7.0091\t5.8734\t5.2722\t5.1257\t5.1966\t5.3128\t\n",
      "Training Epoch: 14 [218/569]\tLoss: 29.4882\tLR: 0.010000\n",
      "5.5250\t4.8662\t4.6175\t4.6610\t4.7910\t5.0274\t\n",
      "Training Epoch: 14 [234/569]\tLoss: 36.7733\tLR: 0.010000\n",
      "7.7727\t6.5353\t5.8481\t5.6017\t5.5165\t5.4990\t\n",
      "Training Epoch: 14 [250/569]\tLoss: 36.9539\tLR: 0.010000\n",
      "7.7461\t6.5374\t5.8085\t5.5878\t5.6030\t5.6710\t\n",
      "Training Epoch: 14 [266/569]\tLoss: 35.2275\tLR: 0.010000\n",
      "7.5321\t6.1684\t5.4931\t5.3141\t5.3169\t5.4029\t\n",
      "Training Epoch: 14 [282/569]\tLoss: 36.9427\tLR: 0.010000\n",
      "7.6758\t6.4110\t5.8098\t5.6360\t5.6691\t5.7410\t\n",
      "Training Epoch: 14 [298/569]\tLoss: 33.3250\tLR: 0.010000\n",
      "7.0456\t5.8893\t5.2196\t5.0181\t5.0311\t5.1212\t\n",
      "Training Epoch: 14 [314/569]\tLoss: 34.2635\tLR: 0.010000\n",
      "6.7063\t5.8183\t5.4400\t5.3431\t5.4334\t5.5225\t\n",
      "Training Epoch: 14 [330/569]\tLoss: 25.9400\tLR: 0.010000\n",
      "4.5670\t4.1908\t4.1157\t4.2206\t4.3582\t4.4878\t\n",
      "Training Epoch: 14 [346/569]\tLoss: 31.4870\tLR: 0.010000\n",
      "5.9204\t5.3152\t5.0668\t5.0362\t5.0559\t5.0926\t\n",
      "Training Epoch: 14 [362/569]\tLoss: 32.8625\tLR: 0.010000\n",
      "6.4183\t5.5457\t5.1714\t5.1208\t5.2025\t5.4038\t\n",
      "Training Epoch: 14 [378/569]\tLoss: 21.9011\tLR: 0.010000\n",
      "3.7825\t3.3970\t3.3723\t3.5508\t3.7997\t3.9990\t\n",
      "Training Epoch: 14 [394/569]\tLoss: 32.1217\tLR: 0.010000\n",
      "6.4903\t5.5352\t5.1122\t5.0081\t4.9939\t4.9820\t\n",
      "Training Epoch: 14 [410/569]\tLoss: 36.1951\tLR: 0.010000\n",
      "7.6274\t6.3848\t5.7379\t5.4540\t5.4278\t5.5633\t\n",
      "Training Epoch: 14 [426/569]\tLoss: 35.1394\tLR: 0.010000\n",
      "7.3950\t6.3470\t5.6655\t5.3300\t5.2174\t5.1845\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14 [442/569]\tLoss: 36.0229\tLR: 0.010000\n",
      "7.0591\t6.2290\t5.8601\t5.6963\t5.6009\t5.5776\t\n",
      "Training Epoch: 14 [458/569]\tLoss: 30.7041\tLR: 0.010000\n",
      "5.8533\t5.2109\t4.8864\t4.7950\t4.8874\t5.0711\t\n",
      "Training Epoch: 14 [474/569]\tLoss: 32.0377\tLR: 0.010000\n",
      "5.9615\t5.2510\t5.0294\t5.0963\t5.2565\t5.4429\t\n",
      "Training Epoch: 14 [490/569]\tLoss: 36.4421\tLR: 0.010000\n",
      "8.2347\t6.7976\t5.8526\t5.3487\t5.1369\t5.0716\t\n",
      "Training Epoch: 14 [506/569]\tLoss: 35.7237\tLR: 0.010000\n",
      "6.5862\t5.9598\t5.6713\t5.6735\t5.8182\t6.0148\t\n",
      "Training Epoch: 14 [522/569]\tLoss: 38.6944\tLR: 0.010000\n",
      "8.2492\t6.8911\t6.2528\t5.9146\t5.7335\t5.6532\t\n",
      "Training Epoch: 14 [538/569]\tLoss: 26.5883\tLR: 0.010000\n",
      "4.6089\t4.2576\t4.2508\t4.3509\t4.4660\t4.6541\t\n",
      "Training Epoch: 14 [554/569]\tLoss: 33.7663\tLR: 0.010000\n",
      "7.3265\t5.9711\t5.2169\t4.9626\t5.0377\t5.2514\t\n",
      "Training Epoch: 14 [570/569]\tLoss: 35.5084\tLR: 0.010000\n",
      "7.3670\t6.3635\t5.7489\t5.4094\t5.2878\t5.3317\t\n",
      "Training Epoch: 14 [586/569]\tLoss: 30.1308\tLR: 0.010000\n",
      "5.6558\t5.0670\t4.7816\t4.7616\t4.8700\t4.9948\t\n",
      "Training Epoch: 14 [602/569]\tLoss: 34.9288\tLR: 0.010000\n",
      "7.3683\t6.1786\t5.4870\t5.2455\t5.2841\t5.3653\t\n",
      "Training Epoch: 14 [618/569]\tLoss: 31.6595\tLR: 0.010000\n",
      "6.1797\t5.4149\t5.0339\t4.9250\t4.9843\t5.1218\t\n",
      "Training Epoch: 14 [634/569]\tLoss: 38.7680\tLR: 0.010000\n",
      "7.7219\t6.6575\t6.1232\t6.0042\t6.0902\t6.1710\t\n",
      "Training Epoch: 14 [650/569]\tLoss: 37.4364\tLR: 0.010000\n",
      "7.5432\t6.4397\t5.9855\t5.8242\t5.7916\t5.8521\t\n",
      "Training Epoch: 14 [666/569]\tLoss: 34.2080\tLR: 0.010000\n",
      "6.8882\t5.8487\t5.3791\t5.3164\t5.3426\t5.4330\t\n",
      "Training Epoch: 14 [682/569]\tLoss: 35.6886\tLR: 0.010000\n",
      "7.8609\t6.3972\t5.5834\t5.3083\t5.2578\t5.2811\t\n",
      "Training Epoch: 14 [698/569]\tLoss: 29.9829\tLR: 0.010000\n",
      "5.7415\t4.9666\t4.7299\t4.7259\t4.8307\t4.9883\t\n",
      "Training Epoch: 14 [714/569]\tLoss: 31.4621\tLR: 0.010000\n",
      "6.6441\t5.4326\t4.7871\t4.6786\t4.7765\t5.1432\t\n",
      "Training Epoch: 14 [730/569]\tLoss: 34.1945\tLR: 0.010000\n",
      "6.9071\t5.9044\t5.4210\t5.3127\t5.2898\t5.3594\t\n",
      "Training Epoch: 14 [746/569]\tLoss: 33.6830\tLR: 0.010000\n",
      "7.2394\t5.9201\t5.2526\t4.9764\t4.9864\t5.3081\t\n",
      "Training Epoch: 14 [762/569]\tLoss: 36.4613\tLR: 0.010000\n",
      "7.8179\t6.5365\t5.7500\t5.4423\t5.4011\t5.5135\t\n",
      "Training Epoch: 14 [778/569]\tLoss: 38.5243\tLR: 0.010000\n",
      "8.7444\t6.9933\t6.0279\t5.6729\t5.5187\t5.5671\t\n",
      "Training Epoch: 14 [794/569]\tLoss: 35.1616\tLR: 0.010000\n",
      "6.6245\t5.8361\t5.5803\t5.5909\t5.6780\t5.8518\t\n",
      "Training Epoch: 14 [810/569]\tLoss: 40.4361\tLR: 0.010000\n",
      "8.1319\t6.9207\t6.3098\t6.1728\t6.2748\t6.6260\t\n",
      "Training Epoch: 14 [826/569]\tLoss: 29.8817\tLR: 0.010000\n",
      "5.4829\t4.9078\t4.6903\t4.7250\t4.8694\t5.2065\t\n",
      "Training Epoch: 14 [842/569]\tLoss: 42.2739\tLR: 0.010000\n",
      "9.4682\t7.7546\t6.7329\t6.2453\t6.0707\t6.0021\t\n",
      "Training Epoch: 14 [858/569]\tLoss: 35.6202\tLR: 0.010000\n",
      "6.8842\t6.1165\t5.7582\t5.6434\t5.5961\t5.6219\t\n",
      "Training Epoch: 14 [874/569]\tLoss: 37.2260\tLR: 0.010000\n",
      "7.8258\t6.6557\t6.0132\t5.6600\t5.5334\t5.5379\t\n",
      "Training Epoch: 14 [890/569]\tLoss: 33.7712\tLR: 0.010000\n",
      "6.6047\t5.8805\t5.5344\t5.3255\t5.2180\t5.2081\t\n",
      "Training Epoch: 14 [905/569]\tLoss: 26.4322\tLR: 0.010000\n",
      "4.7963\t4.0696\t3.9001\t4.1356\t4.5057\t5.0250\t\n",
      "[0.3958689868450165, 0.43266040086746216, 0.17147061228752136, -0.005976528860628605, 0.7302728891372681, 0.030413128435611725, 0.2393139824271202, 0.04893231764435768, 0.861639678478241, 0.06454407423734665, 0.07381624728441238, -0.022072046995162964, 0.4755587577819824, 0.3598759174346924, 0.1645653247833252, -0.03279760479927063, 0.3284694254398346, 0.0, 0.6715305745601654, -0.0028014653362333775]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6742, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5755, Accuracy: 0.1019\n",
      "Test set t = 02: Average loss: 0.5296, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5188, Accuracy: 0.1107\n",
      "Test set t = 04: Average loss: 0.5237, Accuracy: 0.0967\n",
      "Test set t = 05: Average loss: 0.5369, Accuracy: 0.0967\n",
      "\n",
      "Training Epoch: 15 [10/569]\tLoss: 36.3387\tLR: 0.010000\n",
      "8.0391\t6.5817\t5.7737\t5.4175\t5.2799\t5.2467\t\n",
      "Training Epoch: 15 [26/569]\tLoss: 35.0613\tLR: 0.010000\n",
      "7.0161\t6.0285\t5.5365\t5.3925\t5.3864\t5.7013\t\n",
      "Training Epoch: 15 [42/569]\tLoss: 33.8606\tLR: 0.010000\n",
      "7.0014\t5.9195\t5.3384\t5.1717\t5.1905\t5.2391\t\n",
      "Training Epoch: 15 [58/569]\tLoss: 33.4976\tLR: 0.010000\n",
      "6.6212\t5.7131\t5.2784\t5.1909\t5.2722\t5.4218\t\n",
      "Training Epoch: 15 [74/569]\tLoss: 29.7183\tLR: 0.010000\n",
      "5.4650\t4.9177\t4.7227\t4.7899\t4.8898\t4.9332\t\n",
      "Training Epoch: 15 [90/569]\tLoss: 31.6891\tLR: 0.010000\n",
      "6.3800\t5.3449\t4.9142\t4.8440\t4.9741\t5.2320\t\n",
      "Training Epoch: 15 [106/569]\tLoss: 38.9304\tLR: 0.010000\n",
      "8.3833\t6.9819\t6.2086\t5.8621\t5.7285\t5.7658\t\n",
      "Training Epoch: 15 [122/569]\tLoss: 27.1562\tLR: 0.010000\n",
      "4.8967\t4.4229\t4.2555\t4.3372\t4.4944\t4.7494\t\n",
      "Training Epoch: 15 [138/569]\tLoss: 33.5622\tLR: 0.010000\n",
      "7.4850\t5.9061\t5.0837\t4.8900\t5.0129\t5.1845\t\n",
      "Training Epoch: 15 [154/569]\tLoss: 35.9068\tLR: 0.010000\n",
      "7.2758\t6.2509\t5.7772\t5.5910\t5.5232\t5.4887\t\n",
      "Training Epoch: 15 [170/569]\tLoss: 36.6524\tLR: 0.010000\n",
      "7.6937\t6.2690\t5.6889\t5.6091\t5.6754\t5.7163\t\n",
      "Training Epoch: 15 [186/569]\tLoss: 37.1928\tLR: 0.010000\n",
      "8.3565\t6.6658\t5.7523\t5.4699\t5.4165\t5.5319\t\n",
      "Training Epoch: 15 [202/569]\tLoss: 29.7454\tLR: 0.010000\n",
      "5.7378\t4.9467\t4.5816\t4.6073\t4.8174\t5.0547\t\n",
      "Training Epoch: 15 [218/569]\tLoss: 30.1543\tLR: 0.010000\n",
      "5.3014\t4.7541\t4.7098\t4.9104\t5.1422\t5.3364\t\n",
      "Training Epoch: 15 [234/569]\tLoss: 28.3407\tLR: 0.010000\n",
      "4.9816\t4.4252\t4.4336\t4.6449\t4.8286\t5.0269\t\n",
      "Training Epoch: 15 [250/569]\tLoss: 30.7193\tLR: 0.010000\n",
      "6.6612\t5.3940\t4.7611\t4.6201\t4.5880\t4.6949\t\n",
      "Training Epoch: 15 [266/569]\tLoss: 27.3083\tLR: 0.010000\n",
      "5.3936\t4.6196\t4.2338\t4.1830\t4.3263\t4.5521\t\n",
      "Training Epoch: 15 [282/569]\tLoss: 34.1264\tLR: 0.010000\n",
      "7.0436\t5.9788\t5.4354\t5.2046\t5.1809\t5.2831\t\n",
      "Training Epoch: 15 [298/569]\tLoss: 40.3855\tLR: 0.010000\n",
      "9.3213\t7.5173\t6.3505\t5.7707\t5.6093\t5.8164\t\n",
      "Training Epoch: 15 [314/569]\tLoss: 35.7578\tLR: 0.010000\n",
      "7.1597\t6.1900\t5.6702\t5.5356\t5.5645\t5.6379\t\n",
      "Training Epoch: 15 [330/569]\tLoss: 28.2139\tLR: 0.010000\n",
      "5.7910\t4.8172\t4.3644\t4.3041\t4.4128\t4.5244\t\n",
      "Training Epoch: 15 [346/569]\tLoss: 37.4465\tLR: 0.010000\n",
      "7.5892\t6.4755\t5.9493\t5.8021\t5.7785\t5.8519\t\n",
      "Training Epoch: 15 [362/569]\tLoss: 35.0566\tLR: 0.010000\n",
      "7.6920\t6.4618\t5.7606\t5.3193\t4.9911\t4.8318\t\n",
      "Training Epoch: 15 [378/569]\tLoss: 33.0644\tLR: 0.010000\n",
      "6.0809\t5.4187\t5.1684\t5.2278\t5.4259\t5.7427\t\n",
      "Training Epoch: 15 [394/569]\tLoss: 37.8443\tLR: 0.010000\n",
      "8.3616\t6.8117\t6.0000\t5.6571\t5.5243\t5.4896\t\n",
      "Training Epoch: 15 [410/569]\tLoss: 30.2735\tLR: 0.010000\n",
      "5.0863\t4.8029\t4.8407\t4.9831\t5.1708\t5.3898\t\n",
      "Training Epoch: 15 [426/569]\tLoss: 24.7405\tLR: 0.010000\n",
      "4.4523\t3.8439\t3.7481\t3.9241\t4.2400\t4.5321\t\n",
      "Training Epoch: 15 [442/569]\tLoss: 32.5453\tLR: 0.010000\n",
      "6.1146\t5.3904\t5.1321\t5.1671\t5.2984\t5.4428\t\n",
      "Training Epoch: 15 [458/569]\tLoss: 31.9274\tLR: 0.010000\n",
      "5.7813\t5.1731\t4.9877\t5.1189\t5.3424\t5.5240\t\n",
      "Training Epoch: 15 [474/569]\tLoss: 33.7666\tLR: 0.010000\n",
      "6.3274\t5.6637\t5.3744\t5.3496\t5.4426\t5.6089\t\n",
      "Training Epoch: 15 [490/569]\tLoss: 37.2844\tLR: 0.010000\n",
      "8.0487\t6.5979\t5.8662\t5.5904\t5.5641\t5.6170\t\n",
      "Training Epoch: 15 [506/569]\tLoss: 36.5835\tLR: 0.010000\n",
      "7.3377\t6.4233\t5.8937\t5.6498\t5.6287\t5.6503\t\n",
      "Training Epoch: 15 [522/569]\tLoss: 28.2837\tLR: 0.010000\n",
      "5.3790\t4.6784\t4.4150\t4.4318\t4.5824\t4.7971\t\n",
      "Training Epoch: 15 [538/569]\tLoss: 27.6826\tLR: 0.010000\n",
      "5.0166\t4.4478\t4.3599\t4.4783\t4.6039\t4.7760\t\n",
      "Training Epoch: 15 [554/569]\tLoss: 28.3084\tLR: 0.010000\n",
      "6.1273\t4.9840\t4.3855\t4.1947\t4.2355\t4.3814\t\n",
      "Training Epoch: 15 [570/569]\tLoss: 36.3772\tLR: 0.010000\n",
      "7.2816\t6.1563\t5.6579\t5.5777\t5.6866\t6.0171\t\n",
      "Training Epoch: 15 [586/569]\tLoss: 31.1824\tLR: 0.010000\n",
      "6.2172\t5.4414\t5.0454\t4.8826\t4.8093\t4.7865\t\n",
      "Training Epoch: 15 [602/569]\tLoss: 30.6739\tLR: 0.010000\n",
      "5.7767\t5.1099\t4.8173\t4.8174\t4.9567\t5.1958\t\n",
      "Training Epoch: 15 [618/569]\tLoss: 41.8354\tLR: 0.010000\n",
      "9.3318\t7.6795\t6.7085\t6.1910\t5.9733\t5.9513\t\n",
      "Training Epoch: 15 [634/569]\tLoss: 25.2993\tLR: 0.010000\n",
      "4.6340\t4.0190\t3.8987\t4.0444\t4.2592\t4.4439\t\n",
      "Training Epoch: 15 [650/569]\tLoss: 39.4179\tLR: 0.010000\n",
      "8.4056\t7.0165\t6.2674\t5.9610\t5.8612\t5.9063\t\n",
      "Training Epoch: 15 [666/569]\tLoss: 33.5651\tLR: 0.010000\n",
      "5.6634\t5.3633\t5.3632\t5.5182\t5.7260\t5.9311\t\n",
      "Training Epoch: 15 [682/569]\tLoss: 37.4258\tLR: 0.010000\n",
      "8.0968\t6.7490\t5.9295\t5.6029\t5.5254\t5.5223\t\n",
      "Training Epoch: 15 [698/569]\tLoss: 28.4787\tLR: 0.010000\n",
      "5.0263\t4.5694\t4.4792\t4.5721\t4.7434\t5.0882\t\n",
      "Training Epoch: 15 [714/569]\tLoss: 30.4866\tLR: 0.010000\n",
      "5.7329\t5.1261\t4.8133\t4.7501\t4.9172\t5.1469\t\n",
      "Training Epoch: 15 [730/569]\tLoss: 36.1819\tLR: 0.010000\n",
      "6.8610\t6.1030\t5.8301\t5.7500\t5.7626\t5.8752\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [746/569]\tLoss: 32.7083\tLR: 0.010000\n",
      "6.9592\t5.7491\t5.1182\t4.9508\t4.9354\t4.9956\t\n",
      "Training Epoch: 15 [762/569]\tLoss: 38.8250\tLR: 0.010000\n",
      "7.4723\t6.5122\t6.2174\t6.1626\t6.1957\t6.2649\t\n",
      "Training Epoch: 15 [778/569]\tLoss: 35.1154\tLR: 0.010000\n",
      "7.2239\t6.1013\t5.5475\t5.3831\t5.3903\t5.4694\t\n",
      "Training Epoch: 15 [794/569]\tLoss: 30.7146\tLR: 0.010000\n",
      "6.4848\t5.4287\t4.8609\t4.6468\t4.6084\t4.6850\t\n",
      "Training Epoch: 15 [810/569]\tLoss: 34.7908\tLR: 0.010000\n",
      "6.9685\t6.0060\t5.5580\t5.4146\t5.4022\t5.4414\t\n",
      "Training Epoch: 15 [826/569]\tLoss: 40.8602\tLR: 0.010000\n",
      "8.2676\t7.1571\t6.5887\t6.3452\t6.2541\t6.2475\t\n",
      "Training Epoch: 15 [842/569]\tLoss: 36.6547\tLR: 0.010000\n",
      "7.1603\t6.2681\t5.8764\t5.8197\t5.7661\t5.7641\t\n",
      "Training Epoch: 15 [858/569]\tLoss: 35.2991\tLR: 0.010000\n",
      "7.3161\t6.2501\t5.6267\t5.4031\t5.3526\t5.3505\t\n",
      "Training Epoch: 15 [874/569]\tLoss: 39.6291\tLR: 0.010000\n",
      "8.0373\t6.8830\t6.3477\t6.1400\t6.1114\t6.1096\t\n",
      "Training Epoch: 15 [890/569]\tLoss: 38.8335\tLR: 0.010000\n",
      "8.0054\t6.8213\t6.1897\t5.9516\t5.9037\t5.9617\t\n",
      "Training Epoch: 15 [905/569]\tLoss: 27.2557\tLR: 0.010000\n",
      "5.0159\t4.4005\t4.2632\t4.3814\t4.5305\t4.6642\t\n",
      "[0.41431093215942383, 0.4156148135662079, 0.1700742542743683, -0.008277812972664833, 0.7317900061607361, 0.026725441217422485, 0.24148455262184143, 0.053441472351551056, 0.867766797542572, 0.06191394105553627, 0.07031926140189171, -0.024635454639792442, 0.4739556908607483, 0.37464237213134766, 0.15140193700790405, -0.035883259028196335, 0.32401469349861145, 0.0, 0.6759853065013885, -0.004142456687986851]\n",
      "\n",
      "Test set t = 00: Average loss: 0.6741, Accuracy: 0.1037\n",
      "Test set t = 01: Average loss: 0.5767, Accuracy: 0.1019\n",
      "Test set t = 02: Average loss: 0.5308, Accuracy: 0.1107\n",
      "Test set t = 03: Average loss: 0.5191, Accuracy: 0.1107\n",
      "Test set t = 04: Average loss: 0.5224, Accuracy: 0.0949\n",
      "Test set t = 05: Average loss: 0.5335, Accuracy: 0.0967\n",
      "\n",
      "=====================\n",
      "Babble2Spkr, for SNR -3.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([2, 164, 400])) that is different to the input size (torch.Size([2, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.5634, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5129, Accuracy: 0.1762\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5666, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5151, Accuracy: 0.1762\n",
      "Test set t = 02: Average loss: 0.4783, Accuracy: 0.1851\n",
      "Test set t = 03: Average loss: 0.4723, Accuracy: 0.1886\n",
      "Test set t = 04: Average loss: 0.4924, Accuracy: 0.1495\n",
      "Test set t = 05: Average loss: 0.5279, Accuracy: 0.1281\n",
      "\n",
      "Training Epoch: 1 [10/562]\tLoss: 25.2613\tLR: 0.010000\n",
      "5.0635\t4.4915\t4.0304\t3.8210\t3.8609\t3.9940\t\n",
      "Training Epoch: 1 [26/562]\tLoss: 31.1607\tLR: 0.010000\n",
      "5.9580\t5.2744\t4.8442\t4.8074\t4.9860\t5.2907\t\n",
      "Training Epoch: 1 [42/562]\tLoss: 29.9635\tLR: 0.010000\n",
      "5.5951\t5.1692\t4.8118\t4.6546\t4.7350\t4.9978\t\n",
      "Training Epoch: 1 [58/562]\tLoss: 27.0996\tLR: 0.010000\n",
      "5.9966\t5.2029\t4.4506\t4.0216\t3.7865\t3.6414\t\n",
      "Training Epoch: 1 [74/562]\tLoss: 35.8357\tLR: 0.010000\n",
      "7.1390\t6.3702\t5.7606\t5.4334\t5.4554\t5.6771\t\n",
      "Training Epoch: 1 [90/562]\tLoss: 31.2986\tLR: 0.010000\n",
      "6.1304\t5.5650\t5.0318\t4.8054\t4.7897\t4.9762\t\n",
      "Training Epoch: 1 [106/562]\tLoss: 34.3162\tLR: 0.010000\n",
      "6.3554\t5.8873\t5.5430\t5.3613\t5.4420\t5.7272\t\n",
      "Training Epoch: 1 [122/562]\tLoss: 33.4747\tLR: 0.010000\n",
      "6.4153\t5.8313\t5.4077\t5.1811\t5.1964\t5.4429\t\n",
      "Training Epoch: 1 [138/562]\tLoss: 26.5923\tLR: 0.010000\n",
      "4.3869\t4.1882\t4.0559\t4.2349\t4.6629\t5.0636\t\n",
      "Training Epoch: 1 [154/562]\tLoss: 28.1884\tLR: 0.010000\n",
      "5.2281\t4.8108\t4.5273\t4.4743\t4.5464\t4.6015\t\n",
      "Training Epoch: 1 [170/562]\tLoss: 27.0930\tLR: 0.010000\n",
      "4.9327\t4.5475\t4.2248\t4.2265\t4.4300\t4.7316\t\n",
      "Training Epoch: 1 [186/562]\tLoss: 36.3934\tLR: 0.010000\n",
      "6.8301\t6.3519\t5.9502\t5.7313\t5.7141\t5.8158\t\n",
      "Training Epoch: 1 [202/562]\tLoss: 38.6580\tLR: 0.010000\n",
      "7.2951\t6.6463\t6.1969\t6.0851\t6.1524\t6.2823\t\n",
      "Training Epoch: 1 [218/562]\tLoss: 29.1823\tLR: 0.010000\n",
      "5.6269\t5.1147\t4.7765\t4.5804\t4.5189\t4.5650\t\n",
      "Training Epoch: 1 [234/562]\tLoss: 29.7217\tLR: 0.010000\n",
      "5.3261\t5.0041\t4.7663\t4.6890\t4.8104\t5.1259\t\n",
      "Training Epoch: 1 [250/562]\tLoss: 36.1352\tLR: 0.010000\n",
      "7.3740\t6.5506\t5.7974\t5.4546\t5.3662\t5.5925\t\n",
      "Training Epoch: 1 [266/562]\tLoss: 25.5424\tLR: 0.010000\n",
      "4.1959\t3.9205\t3.8327\t4.0745\t4.5370\t4.9819\t\n",
      "Training Epoch: 1 [282/562]\tLoss: 32.8056\tLR: 0.010000\n",
      "5.9277\t5.5118\t5.2215\t5.1766\t5.3559\t5.6122\t\n",
      "Training Epoch: 1 [298/562]\tLoss: 19.4907\tLR: 0.010000\n",
      "2.9096\t2.8978\t2.9978\t3.1916\t3.5340\t3.9600\t\n",
      "Training Epoch: 1 [314/562]\tLoss: 38.4281\tLR: 0.010000\n",
      "7.6435\t6.8622\t6.1997\t5.8077\t5.7642\t6.1508\t\n",
      "Training Epoch: 1 [330/562]\tLoss: 30.4510\tLR: 0.010000\n",
      "5.7793\t5.2505\t4.8458\t4.7502\t4.8427\t4.9825\t\n",
      "Training Epoch: 1 [346/562]\tLoss: 37.3201\tLR: 0.010000\n",
      "7.5959\t6.7070\t6.0109\t5.6481\t5.6252\t5.7330\t\n",
      "Training Epoch: 1 [362/562]\tLoss: 23.5446\tLR: 0.010000\n",
      "4.2217\t3.8437\t3.6142\t3.6345\t3.8818\t4.3488\t\n",
      "Training Epoch: 1 [378/562]\tLoss: 31.1609\tLR: 0.010000\n",
      "5.5836\t5.2629\t5.0288\t4.8824\t5.0110\t5.3924\t\n",
      "Training Epoch: 1 [394/562]\tLoss: 28.6053\tLR: 0.010000\n",
      "5.0990\t4.6967\t4.5022\t4.5918\t4.7298\t4.9857\t\n",
      "Training Epoch: 1 [410/562]\tLoss: 27.7537\tLR: 0.010000\n",
      "4.9074\t4.4746\t4.2365\t4.3138\t4.6712\t5.1502\t\n",
      "Training Epoch: 1 [426/562]\tLoss: 24.7467\tLR: 0.010000\n",
      "4.1872\t4.0084\t3.9109\t3.9493\t4.1766\t4.5142\t\n",
      "Training Epoch: 1 [442/562]\tLoss: 32.0449\tLR: 0.010000\n",
      "5.5759\t5.3467\t5.1870\t5.1904\t5.2830\t5.4618\t\n",
      "Training Epoch: 1 [458/562]\tLoss: 31.3535\tLR: 0.010000\n",
      "5.5732\t5.1411\t4.8898\t4.9399\t5.2286\t5.5808\t\n",
      "Training Epoch: 1 [474/562]\tLoss: 33.2917\tLR: 0.010000\n",
      "6.9864\t6.2561\t5.5130\t5.0360\t4.7839\t4.7164\t\n",
      "Training Epoch: 1 [490/562]\tLoss: 29.8838\tLR: 0.010000\n",
      "5.2692\t4.9748\t4.7641\t4.7440\t4.9270\t5.2048\t\n",
      "Training Epoch: 1 [506/562]\tLoss: 28.7282\tLR: 0.010000\n",
      "5.4382\t5.0157\t4.6571\t4.4721\t4.4987\t4.6463\t\n",
      "Training Epoch: 1 [522/562]\tLoss: 31.5688\tLR: 0.010000\n",
      "5.7476\t5.4676\t5.1910\t5.0146\t4.9897\t5.1584\t\n",
      "Training Epoch: 1 [538/562]\tLoss: 18.7238\tLR: 0.010000\n",
      "3.0316\t2.8417\t2.7592\t2.9544\t3.3422\t3.7946\t\n",
      "Training Epoch: 1 [554/562]\tLoss: 27.2690\tLR: 0.010000\n",
      "5.1854\t4.8581\t4.5240\t4.2609\t4.1843\t4.2563\t\n",
      "Training Epoch: 1 [570/562]\tLoss: 19.9111\tLR: 0.010000\n",
      "2.8799\t2.9068\t3.0215\t3.2657\t3.6847\t4.1525\t\n",
      "Training Epoch: 1 [586/562]\tLoss: 20.8497\tLR: 0.010000\n",
      "3.6473\t3.3944\t3.2310\t3.2444\t3.4757\t3.8569\t\n",
      "Training Epoch: 1 [602/562]\tLoss: 34.5410\tLR: 0.010000\n",
      "7.2180\t6.5285\t5.7610\t5.1888\t4.9579\t4.8868\t\n",
      "Training Epoch: 1 [618/562]\tLoss: 35.7354\tLR: 0.010000\n",
      "7.3839\t6.5508\t5.8204\t5.3984\t5.2618\t5.3202\t\n",
      "Training Epoch: 1 [634/562]\tLoss: 19.9206\tLR: 0.010000\n",
      "3.1428\t3.0502\t3.0467\t3.1829\t3.5204\t3.9777\t\n",
      "Training Epoch: 1 [650/562]\tLoss: 30.1150\tLR: 0.010000\n",
      "5.3504\t5.0380\t4.7759\t4.7881\t4.9861\t5.1765\t\n",
      "Training Epoch: 1 [666/562]\tLoss: 37.2429\tLR: 0.010000\n",
      "7.2204\t6.7280\t6.1614\t5.7926\t5.6500\t5.6906\t\n",
      "Training Epoch: 1 [682/562]\tLoss: 23.7911\tLR: 0.010000\n",
      "3.9215\t3.7529\t3.7178\t3.8269\t4.0976\t4.4743\t\n",
      "Training Epoch: 1 [698/562]\tLoss: 25.0690\tLR: 0.010000\n",
      "4.4496\t4.1769\t3.9999\t3.9863\t4.1319\t4.3243\t\n",
      "Training Epoch: 1 [714/562]\tLoss: 41.4565\tLR: 0.010000\n",
      "8.9257\t7.9610\t6.9611\t6.2311\t5.7922\t5.5854\t\n",
      "Training Epoch: 1 [730/562]\tLoss: 20.7430\tLR: 0.010000\n",
      "3.6214\t3.4376\t3.3365\t3.3197\t3.4150\t3.6129\t\n",
      "Training Epoch: 1 [746/562]\tLoss: 23.9594\tLR: 0.010000\n",
      "4.3530\t4.0815\t3.8938\t3.8176\t3.8531\t3.9604\t\n",
      "Training Epoch: 1 [762/562]\tLoss: 29.7997\tLR: 0.010000\n",
      "5.6436\t5.2792\t4.8969\t4.6583\t4.5969\t4.7248\t\n",
      "Training Epoch: 1 [778/562]\tLoss: 32.5139\tLR: 0.010000\n",
      "6.5910\t5.9590\t5.3440\t4.9536\t4.8427\t4.8236\t\n",
      "Training Epoch: 1 [794/562]\tLoss: 27.4459\tLR: 0.010000\n",
      "5.4445\t4.9173\t4.4649\t4.2172\t4.1562\t4.2457\t\n",
      "Training Epoch: 1 [810/562]\tLoss: 35.0181\tLR: 0.010000\n",
      "6.9164\t6.4288\t5.8396\t5.4079\t5.2045\t5.2211\t\n",
      "Training Epoch: 1 [826/562]\tLoss: 33.9373\tLR: 0.010000\n",
      "6.5548\t6.0542\t5.5862\t5.3168\t5.2288\t5.1965\t\n",
      "Training Epoch: 1 [842/562]\tLoss: 35.7559\tLR: 0.010000\n",
      "7.3259\t6.7549\t6.0531\t5.4727\t5.1547\t4.9946\t\n",
      "Training Epoch: 1 [858/562]\tLoss: 23.1677\tLR: 0.010000\n",
      "4.3098\t3.9882\t3.6883\t3.6179\t3.6922\t3.8713\t\n",
      "Training Epoch: 1 [874/562]\tLoss: 28.7268\tLR: 0.010000\n",
      "5.2218\t4.9111\t4.6383\t4.5287\t4.6301\t4.7967\t\n",
      "Training Epoch: 1 [890/562]\tLoss: 28.2454\tLR: 0.010000\n",
      "5.0274\t4.7881\t4.5958\t4.5101\t4.5889\t4.7350\t\n",
      "Training Epoch: 1 [898/562]\tLoss: 49.7151\tLR: 0.010000\n",
      "10.0337\t9.2943\t8.3944\t7.6724\t7.2775\t7.0429\t\n",
      "[0.3231821060180664, 0.32449328899383545, 0.35232460498809814, 0.012461754493415356, 0.4107045829296112, 0.2392883449792862, 0.3500070720911026, 0.013075504451990128, 0.37693750858306885, 0.22766698896884918, 0.39539550244808197, 0.006534317974001169, 0.32652604579925537, 0.251794695854187, 0.4216792583465576, 0.005994250997900963, 0.24401982128620148, 0.0, 0.7559801787137985, 0.006930298171937466]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5633, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5233, Accuracy: 0.1815\n",
      "Test set t = 02: Average loss: 0.4875, Accuracy: 0.1779\n",
      "Test set t = 03: Average loss: 0.4691, Accuracy: 0.1833\n",
      "Test set t = 04: Average loss: 0.4686, Accuracy: 0.1779\n",
      "Test set t = 05: Average loss: 0.4801, Accuracy: 0.1530\n",
      "\n",
      "Training Epoch: 2 [10/562]\tLoss: 25.9597\tLR: 0.010000\n",
      "4.9475\t4.5826\t4.1936\t4.0237\t4.0281\t4.1843\t\n",
      "Training Epoch: 2 [26/562]\tLoss: 22.9293\tLR: 0.010000\n",
      "4.2216\t3.9172\t3.6829\t3.6305\t3.6689\t3.8081\t\n",
      "Training Epoch: 2 [42/562]\tLoss: 32.6662\tLR: 0.010000\n",
      "6.3283\t5.8398\t5.3752\t5.0815\t5.0016\t5.0398\t\n",
      "Training Epoch: 2 [58/562]\tLoss: 32.8932\tLR: 0.010000\n",
      "6.3582\t5.8197\t5.3633\t5.1226\t5.0691\t5.1603\t\n",
      "Training Epoch: 2 [74/562]\tLoss: 31.2616\tLR: 0.010000\n",
      "6.1410\t5.5993\t5.0640\t4.7689\t4.7717\t4.9167\t\n",
      "Training Epoch: 2 [90/562]\tLoss: 23.7539\tLR: 0.010000\n",
      "4.0512\t3.8881\t3.8324\t3.8588\t3.9617\t4.1617\t\n",
      "Training Epoch: 2 [106/562]\tLoss: 22.3830\tLR: 0.010000\n",
      "3.8367\t3.6252\t3.4622\t3.5060\t3.7763\t4.1766\t\n",
      "Training Epoch: 2 [122/562]\tLoss: 33.3041\tLR: 0.010000\n",
      "6.8024\t6.1348\t5.5233\t5.1396\t4.8924\t4.8114\t\n",
      "Training Epoch: 2 [138/562]\tLoss: 17.2268\tLR: 0.010000\n",
      "2.5982\t2.5623\t2.6139\t2.7738\t3.1190\t3.5595\t\n",
      "Training Epoch: 2 [154/562]\tLoss: 29.5013\tLR: 0.010000\n",
      "5.3973\t5.0931\t4.8772\t4.7268\t4.6751\t4.7318\t\n",
      "Training Epoch: 2 [170/562]\tLoss: 39.8634\tLR: 0.010000\n",
      "8.3409\t7.5926\t6.7261\t6.0625\t5.6776\t5.4637\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [186/562]\tLoss: 41.9856\tLR: 0.010000\n",
      "9.1068\t8.1091\t7.0372\t6.2811\t5.8370\t5.6144\t\n",
      "Training Epoch: 2 [202/562]\tLoss: 33.7890\tLR: 0.010000\n",
      "6.5359\t5.9722\t5.5084\t5.2597\t5.2211\t5.2916\t\n",
      "Training Epoch: 2 [218/562]\tLoss: 29.0160\tLR: 0.010000\n",
      "5.0724\t4.8448\t4.6334\t4.6156\t4.8180\t5.0318\t\n",
      "Training Epoch: 2 [234/562]\tLoss: 31.0841\tLR: 0.010000\n",
      "5.6906\t5.3876\t5.0511\t4.9396\t4.9331\t5.0820\t\n",
      "Training Epoch: 2 [250/562]\tLoss: 33.0853\tLR: 0.010000\n",
      "6.4735\t5.8282\t5.2890\t5.0628\t5.1164\t5.3153\t\n",
      "Training Epoch: 2 [266/562]\tLoss: 26.3942\tLR: 0.010000\n",
      "4.9566\t4.6934\t4.3756\t4.1395\t4.0648\t4.1643\t\n",
      "Training Epoch: 2 [282/562]\tLoss: 29.2542\tLR: 0.010000\n",
      "5.6503\t5.2685\t4.8605\t4.5592\t4.4349\t4.4809\t\n",
      "Training Epoch: 2 [298/562]\tLoss: 23.4861\tLR: 0.010000\n",
      "4.3040\t3.9017\t3.6629\t3.7118\t3.8575\t4.0482\t\n",
      "Training Epoch: 2 [314/562]\tLoss: 22.2424\tLR: 0.010000\n",
      "3.8990\t3.5758\t3.3716\t3.4712\t3.7748\t4.1500\t\n",
      "Training Epoch: 2 [330/562]\tLoss: 33.8829\tLR: 0.010000\n",
      "6.3311\t5.9181\t5.5450\t5.3524\t5.3169\t5.4194\t\n",
      "Training Epoch: 2 [346/562]\tLoss: 31.3672\tLR: 0.010000\n",
      "5.5485\t5.2783\t5.0600\t5.0480\t5.1387\t5.2937\t\n",
      "Training Epoch: 2 [362/562]\tLoss: 35.6323\tLR: 0.010000\n",
      "7.2019\t6.6332\t6.0122\t5.5196\t5.2319\t5.0334\t\n",
      "Training Epoch: 2 [378/562]\tLoss: 31.5489\tLR: 0.010000\n",
      "6.5634\t5.7373\t5.0840\t4.7778\t4.6644\t4.7220\t\n",
      "Training Epoch: 2 [394/562]\tLoss: 38.2172\tLR: 0.010000\n",
      "7.7025\t6.9758\t6.2977\t5.8721\t5.7104\t5.6587\t\n",
      "Training Epoch: 2 [410/562]\tLoss: 27.3596\tLR: 0.010000\n",
      "5.1597\t4.6801\t4.3360\t4.2303\t4.3587\t4.5948\t\n",
      "Training Epoch: 2 [426/562]\tLoss: 25.4875\tLR: 0.010000\n",
      "4.3174\t4.0749\t3.9927\t4.1124\t4.3525\t4.6377\t\n",
      "Training Epoch: 2 [442/562]\tLoss: 27.8617\tLR: 0.010000\n",
      "5.1245\t4.7816\t4.4928\t4.3866\t4.4640\t4.6122\t\n",
      "Training Epoch: 2 [458/562]\tLoss: 28.9009\tLR: 0.010000\n",
      "5.0294\t4.8077\t4.7098\t4.6543\t4.7524\t4.9472\t\n",
      "Training Epoch: 2 [474/562]\tLoss: 28.3944\tLR: 0.010000\n",
      "5.8157\t5.3066\t4.7804\t4.3636\t4.0971\t4.0310\t\n",
      "Training Epoch: 2 [490/562]\tLoss: 24.8220\tLR: 0.010000\n",
      "4.5687\t4.1647\t3.9211\t3.8948\t4.0201\t4.2527\t\n",
      "Training Epoch: 2 [506/562]\tLoss: 35.0288\tLR: 0.010000\n",
      "6.5692\t6.0409\t5.6328\t5.4540\t5.5568\t5.7752\t\n",
      "Training Epoch: 2 [522/562]\tLoss: 28.7204\tLR: 0.010000\n",
      "5.2571\t4.9169\t4.5940\t4.4927\t4.6130\t4.8467\t\n",
      "Training Epoch: 2 [538/562]\tLoss: 36.3170\tLR: 0.010000\n",
      "7.0527\t6.4520\t5.8867\t5.6138\t5.5947\t5.7171\t\n",
      "Training Epoch: 2 [554/562]\tLoss: 36.0828\tLR: 0.010000\n",
      "7.3671\t6.6687\t5.9941\t5.4925\t5.2940\t5.2664\t\n",
      "Training Epoch: 2 [570/562]\tLoss: 31.1669\tLR: 0.010000\n",
      "5.6275\t5.3171\t5.0707\t4.9740\t5.0281\t5.1495\t\n",
      "Training Epoch: 2 [586/562]\tLoss: 32.5419\tLR: 0.010000\n",
      "5.9858\t5.6156\t5.3198\t5.1858\t5.1712\t5.2635\t\n",
      "Training Epoch: 2 [602/562]\tLoss: 27.2049\tLR: 0.010000\n",
      "5.0096\t4.6142\t4.3887\t4.3378\t4.3703\t4.4843\t\n",
      "Training Epoch: 2 [618/562]\tLoss: 28.8303\tLR: 0.010000\n",
      "5.0660\t4.8066\t4.6589\t4.6531\t4.7587\t4.8870\t\n",
      "Training Epoch: 2 [634/562]\tLoss: 23.8791\tLR: 0.010000\n",
      "4.4355\t4.0955\t3.8072\t3.7105\t3.8120\t4.0184\t\n",
      "Training Epoch: 2 [650/562]\tLoss: 24.2635\tLR: 0.010000\n",
      "4.1489\t3.8472\t3.7132\t3.8207\t4.1452\t4.5884\t\n",
      "Training Epoch: 2 [666/562]\tLoss: 29.2109\tLR: 0.010000\n",
      "5.5606\t5.0432\t4.6816\t4.5695\t4.6410\t4.7150\t\n",
      "Training Epoch: 2 [682/562]\tLoss: 30.9042\tLR: 0.010000\n",
      "5.6697\t5.3080\t5.0188\t4.8977\t4.9470\t5.0631\t\n",
      "Training Epoch: 2 [698/562]\tLoss: 36.2882\tLR: 0.010000\n",
      "7.4355\t6.6474\t5.9283\t5.5176\t5.3782\t5.3812\t\n",
      "Training Epoch: 2 [714/562]\tLoss: 25.9345\tLR: 0.010000\n",
      "4.4247\t4.2257\t4.1324\t4.1921\t4.3502\t4.6094\t\n",
      "Training Epoch: 2 [730/562]\tLoss: 30.1201\tLR: 0.010000\n",
      "5.6055\t5.1427\t4.8673\t4.7904\t4.7989\t4.9153\t\n",
      "Training Epoch: 2 [746/562]\tLoss: 35.4768\tLR: 0.010000\n",
      "7.2684\t6.5604\t5.8799\t5.4214\t5.1938\t5.1529\t\n",
      "Training Epoch: 2 [762/562]\tLoss: 28.3804\tLR: 0.010000\n",
      "5.2620\t4.8336\t4.5357\t4.4215\t4.5426\t4.7849\t\n",
      "Training Epoch: 2 [778/562]\tLoss: 27.6710\tLR: 0.010000\n",
      "5.3960\t4.9335\t4.5235\t4.2976\t4.2225\t4.2978\t\n",
      "Training Epoch: 2 [794/562]\tLoss: 27.2814\tLR: 0.010000\n",
      "5.3303\t4.9143\t4.4366\t4.1451\t4.1594\t4.2957\t\n",
      "Training Epoch: 2 [810/562]\tLoss: 29.3433\tLR: 0.010000\n",
      "5.3311\t4.9578\t4.6857\t4.6020\t4.7440\t5.0226\t\n",
      "Training Epoch: 2 [826/562]\tLoss: 23.0070\tLR: 0.010000\n",
      "3.6121\t3.4861\t3.5598\t3.7788\t4.1117\t4.4585\t\n",
      "Training Epoch: 2 [842/562]\tLoss: 20.7367\tLR: 0.010000\n",
      "3.0547\t2.9824\t3.1271\t3.4542\t3.8407\t4.2776\t\n",
      "Training Epoch: 2 [858/562]\tLoss: 27.6728\tLR: 0.010000\n",
      "5.8060\t5.0856\t4.4459\t4.0946\t4.0706\t4.1702\t\n",
      "Training Epoch: 2 [874/562]\tLoss: 30.5926\tLR: 0.010000\n",
      "6.0734\t5.5314\t5.0448\t4.7104\t4.5829\t4.6496\t\n",
      "Training Epoch: 2 [890/562]\tLoss: 34.4822\tLR: 0.010000\n",
      "6.5792\t6.0413\t5.6225\t5.4387\t5.3851\t5.4155\t\n",
      "Training Epoch: 2 [898/562]\tLoss: 16.9370\tLR: 0.010000\n",
      "3.3166\t2.9525\t2.6305\t2.5185\t2.6110\t2.9079\t\n",
      "[0.3076613247394562, 0.4024747312068939, 0.2898639440536499, 0.014151571318507195, 0.5098345279693604, 0.20543427765369415, 0.2847311943769455, 0.01572638377547264, 0.44685596227645874, 0.20481206476688385, 0.3483319729566574, 0.0051299468614161015, 0.35050100088119507, 0.25420865416526794, 0.395290344953537, 0.0028441043104976416, 0.2510669231414795, 0.0, 0.7489330768585205, 0.004305942915380001]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5670, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5215, Accuracy: 0.1815\n",
      "Test set t = 02: Average loss: 0.4840, Accuracy: 0.1851\n",
      "Test set t = 03: Average loss: 0.4667, Accuracy: 0.1815\n",
      "Test set t = 04: Average loss: 0.4680, Accuracy: 0.1833\n",
      "Test set t = 05: Average loss: 0.4806, Accuracy: 0.1655\n",
      "\n",
      "Training Epoch: 3 [10/562]\tLoss: 30.7815\tLR: 0.010000\n",
      "5.9454\t5.4645\t5.0934\t4.8390\t4.7177\t4.7215\t\n",
      "Training Epoch: 3 [26/562]\tLoss: 25.6681\tLR: 0.010000\n",
      "4.5288\t4.2755\t4.1238\t4.1188\t4.2325\t4.3887\t\n",
      "Training Epoch: 3 [42/562]\tLoss: 29.2515\tLR: 0.010000\n",
      "5.2480\t4.9042\t4.6824\t4.7007\t4.7714\t4.9448\t\n",
      "Training Epoch: 3 [58/562]\tLoss: 27.7739\tLR: 0.010000\n",
      "5.0220\t4.6906\t4.4616\t4.3909\t4.5099\t4.6989\t\n",
      "Training Epoch: 3 [74/562]\tLoss: 22.6447\tLR: 0.010000\n",
      "4.0971\t3.8533\t3.6478\t3.5908\t3.6577\t3.7980\t\n",
      "Training Epoch: 3 [90/562]\tLoss: 25.3630\tLR: 0.010000\n",
      "4.2889\t4.0912\t4.0045\t4.0886\t4.3088\t4.5811\t\n",
      "Training Epoch: 3 [106/562]\tLoss: 33.8549\tLR: 0.010000\n",
      "6.3452\t5.9375\t5.5836\t5.3626\t5.2959\t5.3300\t\n",
      "Training Epoch: 3 [122/562]\tLoss: 24.3416\tLR: 0.010000\n",
      "4.1198\t3.8689\t3.8051\t3.9248\t4.1448\t4.4782\t\n",
      "Training Epoch: 3 [138/562]\tLoss: 29.4872\tLR: 0.010000\n",
      "5.1997\t4.8805\t4.6803\t4.6641\t4.8884\t5.1742\t\n",
      "Training Epoch: 3 [154/562]\tLoss: 35.8172\tLR: 0.010000\n",
      "7.0195\t6.4535\t5.9451\t5.6221\t5.4294\t5.3476\t\n",
      "Training Epoch: 3 [170/562]\tLoss: 38.0061\tLR: 0.010000\n",
      "7.8346\t7.0206\t6.2630\t5.7677\t5.5944\t5.5259\t\n",
      "Training Epoch: 3 [186/562]\tLoss: 40.4723\tLR: 0.010000\n",
      "8.4630\t7.6417\t6.8045\t6.1361\t5.7934\t5.6337\t\n",
      "Training Epoch: 3 [202/562]\tLoss: 28.7442\tLR: 0.010000\n",
      "5.8798\t5.2631\t4.6873\t4.3962\t4.2708\t4.2468\t\n",
      "Training Epoch: 3 [218/562]\tLoss: 27.4116\tLR: 0.010000\n",
      "5.2303\t4.7885\t4.4514\t4.2828\t4.2727\t4.3859\t\n",
      "Training Epoch: 3 [234/562]\tLoss: 36.4721\tLR: 0.010000\n",
      "7.1591\t6.5092\t5.9691\t5.6913\t5.5751\t5.5682\t\n",
      "Training Epoch: 3 [250/562]\tLoss: 30.1600\tLR: 0.010000\n",
      "6.0291\t5.5510\t5.0548\t4.6573\t4.4520\t4.4156\t\n",
      "Training Epoch: 3 [266/562]\tLoss: 28.4588\tLR: 0.010000\n",
      "5.8188\t5.1352\t4.5653\t4.2835\t4.2268\t4.4292\t\n",
      "Training Epoch: 3 [282/562]\tLoss: 26.7184\tLR: 0.010000\n",
      "4.8230\t4.4531\t4.2370\t4.2558\t4.3870\t4.5626\t\n",
      "Training Epoch: 3 [298/562]\tLoss: 25.7097\tLR: 0.010000\n",
      "5.0101\t4.5356\t4.1351\t3.9107\t3.9477\t4.1706\t\n",
      "Training Epoch: 3 [314/562]\tLoss: 29.0923\tLR: 0.010000\n",
      "5.5968\t5.0609\t4.7027\t4.5402\t4.5426\t4.6490\t\n",
      "Training Epoch: 3 [330/562]\tLoss: 31.7400\tLR: 0.010000\n",
      "6.1178\t5.5695\t5.1218\t4.9230\t4.9502\t5.0576\t\n",
      "Training Epoch: 3 [346/562]\tLoss: 26.4744\tLR: 0.010000\n",
      "4.4429\t4.2960\t4.2139\t4.2609\t4.4748\t4.7860\t\n",
      "Training Epoch: 3 [362/562]\tLoss: 29.6597\tLR: 0.010000\n",
      "5.5842\t5.1013\t4.7457\t4.6100\t4.7011\t4.9175\t\n",
      "Training Epoch: 3 [378/562]\tLoss: 26.3582\tLR: 0.010000\n",
      "4.7969\t4.4688\t4.2635\t4.2199\t4.2574\t4.3517\t\n",
      "Training Epoch: 3 [394/562]\tLoss: 31.0467\tLR: 0.010000\n",
      "5.8673\t5.4241\t5.0607\t4.8976\t4.8986\t4.8983\t\n",
      "Training Epoch: 3 [410/562]\tLoss: 31.0024\tLR: 0.010000\n",
      "6.2196\t5.6963\t5.1888\t4.8191\t4.5886\t4.4900\t\n",
      "Training Epoch: 3 [426/562]\tLoss: 31.6865\tLR: 0.010000\n",
      "6.3462\t5.7312\t5.1983\t4.8988\t4.7472\t4.7648\t\n",
      "Training Epoch: 3 [442/562]\tLoss: 30.6101\tLR: 0.010000\n",
      "5.9170\t5.4804\t5.0498\t4.8038\t4.6705\t4.6886\t\n",
      "Training Epoch: 3 [458/562]\tLoss: 25.9414\tLR: 0.010000\n",
      "4.9229\t4.5182\t4.1959\t4.0676\t4.0537\t4.1832\t\n",
      "Training Epoch: 3 [474/562]\tLoss: 31.4251\tLR: 0.010000\n",
      "5.8560\t5.3376\t4.9963\t4.9533\t5.0484\t5.2335\t\n",
      "Training Epoch: 3 [490/562]\tLoss: 32.6751\tLR: 0.010000\n",
      "6.2459\t5.6833\t5.2773\t5.0926\t5.1222\t5.2538\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [506/562]\tLoss: 38.1977\tLR: 0.010000\n",
      "7.7119\t7.0128\t6.3926\t5.9172\t5.6298\t5.5334\t\n",
      "Training Epoch: 3 [522/562]\tLoss: 34.2243\tLR: 0.010000\n",
      "6.5837\t6.0534\t5.5843\t5.3476\t5.2924\t5.3629\t\n",
      "Training Epoch: 3 [538/562]\tLoss: 37.2210\tLR: 0.010000\n",
      "7.1584\t6.5345\t6.0839\t5.8410\t5.7772\t5.8259\t\n",
      "Training Epoch: 3 [554/562]\tLoss: 21.8311\tLR: 0.010000\n",
      "4.1293\t3.7480\t3.4445\t3.3241\t3.4777\t3.7075\t\n",
      "Training Epoch: 3 [570/562]\tLoss: 24.9129\tLR: 0.010000\n",
      "4.3153\t4.0959\t3.9523\t3.9748\t4.1669\t4.4076\t\n",
      "Training Epoch: 3 [586/562]\tLoss: 32.6112\tLR: 0.010000\n",
      "6.4986\t5.7707\t5.2432\t4.9648\t4.9764\t5.1575\t\n",
      "Training Epoch: 3 [602/562]\tLoss: 28.4527\tLR: 0.010000\n",
      "5.1351\t4.6915\t4.4385\t4.4768\t4.6979\t5.0130\t\n",
      "Training Epoch: 3 [618/562]\tLoss: 31.3570\tLR: 0.010000\n",
      "6.3325\t5.6705\t5.1068\t4.8014\t4.7080\t4.7378\t\n",
      "Training Epoch: 3 [634/562]\tLoss: 30.7776\tLR: 0.010000\n",
      "5.8760\t5.3809\t4.9905\t4.7885\t4.8130\t4.9288\t\n",
      "Training Epoch: 3 [650/562]\tLoss: 22.9541\tLR: 0.010000\n",
      "4.1922\t3.8576\t3.6452\t3.6029\t3.7246\t3.9317\t\n",
      "Training Epoch: 3 [666/562]\tLoss: 39.8645\tLR: 0.010000\n",
      "8.2029\t7.3865\t6.6311\t6.1276\t5.8190\t5.6973\t\n",
      "Training Epoch: 3 [682/562]\tLoss: 24.5445\tLR: 0.010000\n",
      "4.7299\t4.2432\t3.9051\t3.7725\t3.8615\t4.0323\t\n",
      "Training Epoch: 3 [698/562]\tLoss: 28.1854\tLR: 0.010000\n",
      "4.9861\t4.7404\t4.5106\t4.4793\t4.5922\t4.8768\t\n",
      "Training Epoch: 3 [714/562]\tLoss: 29.6438\tLR: 0.010000\n",
      "5.4390\t5.0512\t4.7143\t4.6201\t4.7734\t5.0458\t\n",
      "Training Epoch: 3 [730/562]\tLoss: 32.0643\tLR: 0.010000\n",
      "6.3449\t5.6702\t5.1360\t4.9403\t4.9400\t5.0330\t\n",
      "Training Epoch: 3 [746/562]\tLoss: 23.3486\tLR: 0.010000\n",
      "3.5501\t3.4724\t3.5506\t3.8426\t4.2594\t4.6734\t\n",
      "Training Epoch: 3 [762/562]\tLoss: 27.4328\tLR: 0.010000\n",
      "5.1222\t4.6925\t4.3640\t4.2559\t4.3690\t4.6291\t\n",
      "Training Epoch: 3 [778/562]\tLoss: 27.6776\tLR: 0.010000\n",
      "5.2376\t4.8245\t4.4264\t4.2864\t4.3625\t4.5401\t\n",
      "Training Epoch: 3 [794/562]\tLoss: 14.4750\tLR: 0.010000\n",
      "2.1720\t2.0506\t2.0638\t2.2755\t2.6673\t3.2459\t\n",
      "Training Epoch: 3 [810/562]\tLoss: 20.7583\tLR: 0.010000\n",
      "3.4079\t3.1373\t3.1046\t3.3265\t3.7145\t4.0675\t\n",
      "Training Epoch: 3 [826/562]\tLoss: 28.8749\tLR: 0.010000\n",
      "5.2707\t4.8600\t4.5762\t4.5664\t4.7024\t4.8991\t\n",
      "Training Epoch: 3 [842/562]\tLoss: 26.2102\tLR: 0.010000\n",
      "5.2369\t4.7071\t4.2461\t3.9674\t3.9386\t4.1141\t\n",
      "Training Epoch: 3 [858/562]\tLoss: 26.9985\tLR: 0.010000\n",
      "4.9006\t4.5434\t4.3164\t4.2734\t4.3611\t4.6036\t\n",
      "Training Epoch: 3 [874/562]\tLoss: 39.2882\tLR: 0.010000\n",
      "7.8586\t7.1254\t6.5220\t6.1107\t5.8975\t5.7741\t\n",
      "Training Epoch: 3 [890/562]\tLoss: 29.1839\tLR: 0.010000\n",
      "5.7111\t5.2352\t4.8017\t4.5554\t4.4424\t4.4381\t\n",
      "Training Epoch: 3 [898/562]\tLoss: 37.2526\tLR: 0.010000\n",
      "7.9405\t7.1849\t6.2693\t5.6117\t5.2421\t5.0041\t\n",
      "[0.308005690574646, 0.4545878469944, 0.23740646243095398, 0.01510590873658657, 0.5937533378601074, 0.17120423913002014, 0.23504242300987244, 0.017903689295053482, 0.5097451210021973, 0.18454620242118835, 0.3057086765766144, 0.004162242636084557, 0.3805968463420868, 0.247880220413208, 0.3715229332447052, -7.048037514323369e-05, 0.2479245662689209, 0.0, 0.7520754337310791, 0.0023326140362769365]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5702, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5213, Accuracy: 0.1833\n",
      "Test set t = 02: Average loss: 0.4836, Accuracy: 0.1851\n",
      "Test set t = 03: Average loss: 0.4668, Accuracy: 0.1815\n",
      "Test set t = 04: Average loss: 0.4678, Accuracy: 0.1851\n",
      "Test set t = 05: Average loss: 0.4799, Accuracy: 0.1637\n",
      "\n",
      "Training Epoch: 4 [10/562]\tLoss: 32.0314\tLR: 0.010000\n",
      "5.9386\t5.4732\t5.1507\t5.0560\t5.1259\t5.2869\t\n",
      "Training Epoch: 4 [26/562]\tLoss: 21.5565\tLR: 0.010000\n",
      "3.4873\t3.3307\t3.3335\t3.4876\t3.7811\t4.1363\t\n",
      "Training Epoch: 4 [42/562]\tLoss: 26.0504\tLR: 0.010000\n",
      "4.4357\t4.2833\t4.2228\t4.2719\t4.3491\t4.4877\t\n",
      "Training Epoch: 4 [58/562]\tLoss: 20.6438\tLR: 0.010000\n",
      "3.7888\t3.3816\t3.1481\t3.1528\t3.3682\t3.8043\t\n",
      "Training Epoch: 4 [74/562]\tLoss: 25.3808\tLR: 0.010000\n",
      "4.7842\t4.2928\t3.9940\t3.9378\t4.0542\t4.3179\t\n",
      "Training Epoch: 4 [90/562]\tLoss: 29.3582\tLR: 0.010000\n",
      "5.8999\t5.3400\t4.8276\t4.5123\t4.3829\t4.3954\t\n",
      "Training Epoch: 4 [106/562]\tLoss: 36.4155\tLR: 0.010000\n",
      "7.6900\t6.7720\t5.9884\t5.5357\t5.2792\t5.1503\t\n",
      "Training Epoch: 4 [122/562]\tLoss: 26.0964\tLR: 0.010000\n",
      "4.7153\t4.4299\t4.1274\t4.0850\t4.2524\t4.4864\t\n",
      "Training Epoch: 4 [138/562]\tLoss: 29.5698\tLR: 0.010000\n",
      "5.5297\t5.0739\t4.7621\t4.6553\t4.6739\t4.8749\t\n",
      "Training Epoch: 4 [154/562]\tLoss: 28.8635\tLR: 0.010000\n",
      "5.1306\t4.8755\t4.7003\t4.6335\t4.6828\t4.8409\t\n",
      "Training Epoch: 4 [170/562]\tLoss: 26.9768\tLR: 0.010000\n",
      "5.0661\t4.5903\t4.3020\t4.2346\t4.3230\t4.4607\t\n",
      "Training Epoch: 4 [186/562]\tLoss: 29.9252\tLR: 0.010000\n",
      "5.8313\t5.2828\t4.8433\t4.6459\t4.6400\t4.6819\t\n",
      "Training Epoch: 4 [202/562]\tLoss: 34.9274\tLR: 0.010000\n",
      "6.9169\t6.2860\t5.7794\t5.4423\t5.2737\t5.2291\t\n",
      "Training Epoch: 4 [218/562]\tLoss: 21.0912\tLR: 0.010000\n",
      "3.5420\t3.3880\t3.3242\t3.3730\t3.5735\t3.8905\t\n",
      "Training Epoch: 4 [234/562]\tLoss: 42.3610\tLR: 0.010000\n",
      "8.8667\t7.9949\t7.0994\t6.4600\t6.0546\t5.8856\t\n",
      "Training Epoch: 4 [250/562]\tLoss: 32.8500\tLR: 0.010000\n",
      "5.9542\t5.5793\t5.3249\t5.2579\t5.3023\t5.4314\t\n",
      "Training Epoch: 4 [266/562]\tLoss: 29.4667\tLR: 0.010000\n",
      "5.5032\t5.0145\t4.6831\t4.6210\t4.7183\t4.9266\t\n",
      "Training Epoch: 4 [282/562]\tLoss: 33.3615\tLR: 0.010000\n",
      "6.6120\t5.8326\t5.3332\t5.1342\t5.1756\t5.2740\t\n",
      "Training Epoch: 4 [298/562]\tLoss: 34.6231\tLR: 0.010000\n",
      "7.0875\t6.3057\t5.6886\t5.3139\t5.1256\t5.1017\t\n",
      "Training Epoch: 4 [314/562]\tLoss: 28.7102\tLR: 0.010000\n",
      "5.7465\t5.1369\t4.6492\t4.3777\t4.3249\t4.4749\t\n",
      "Training Epoch: 4 [330/562]\tLoss: 34.7993\tLR: 0.010000\n",
      "7.4769\t6.5678\t5.7189\t5.1958\t4.9426\t4.8973\t\n",
      "Training Epoch: 4 [346/562]\tLoss: 41.4090\tLR: 0.010000\n",
      "8.8741\t7.8306\t6.9007\t6.2328\t5.8556\t5.7152\t\n",
      "Training Epoch: 4 [362/562]\tLoss: 32.4676\tLR: 0.010000\n",
      "6.1255\t5.6374\t5.2423\t5.1385\t5.1431\t5.1807\t\n",
      "Training Epoch: 4 [378/562]\tLoss: 27.0532\tLR: 0.010000\n",
      "5.3946\t4.7346\t4.2587\t4.0828\t4.1819\t4.4005\t\n",
      "Training Epoch: 4 [394/562]\tLoss: 32.8146\tLR: 0.010000\n",
      "6.4718\t5.8768\t5.3807\t5.1019\t4.9692\t5.0142\t\n",
      "Training Epoch: 4 [410/562]\tLoss: 32.2695\tLR: 0.010000\n",
      "6.3987\t5.7855\t5.2792\t4.9985\t4.9272\t4.8804\t\n",
      "Training Epoch: 4 [426/562]\tLoss: 31.7416\tLR: 0.010000\n",
      "6.3262\t5.6519\t5.1795\t4.9215\t4.8291\t4.8334\t\n",
      "Training Epoch: 4 [442/562]\tLoss: 23.6548\tLR: 0.010000\n",
      "4.2257\t3.8952\t3.7268\t3.7323\t3.9035\t4.1712\t\n",
      "Training Epoch: 4 [458/562]\tLoss: 23.8826\tLR: 0.010000\n",
      "4.0895\t3.8922\t3.7898\t3.8214\t3.9859\t4.3038\t\n",
      "Training Epoch: 4 [474/562]\tLoss: 30.1839\tLR: 0.010000\n",
      "5.5066\t5.1766\t4.8609\t4.7846\t4.8393\t5.0159\t\n",
      "Training Epoch: 4 [490/562]\tLoss: 22.4540\tLR: 0.010000\n",
      "3.8910\t3.6025\t3.4483\t3.5369\t3.8100\t4.1653\t\n",
      "Training Epoch: 4 [506/562]\tLoss: 33.2646\tLR: 0.010000\n",
      "6.5133\t5.9109\t5.4093\t5.1107\t5.0878\t5.2326\t\n",
      "Training Epoch: 4 [522/562]\tLoss: 19.5079\tLR: 0.010000\n",
      "3.2607\t3.1399\t3.1150\t3.1795\t3.3315\t3.4812\t\n",
      "Training Epoch: 4 [538/562]\tLoss: 24.5686\tLR: 0.010000\n",
      "4.1712\t3.8154\t3.7083\t3.9077\t4.2701\t4.6958\t\n",
      "Training Epoch: 4 [554/562]\tLoss: 34.2417\tLR: 0.010000\n",
      "6.7305\t6.0415\t5.5841\t5.3447\t5.2507\t5.2901\t\n",
      "Training Epoch: 4 [570/562]\tLoss: 33.6963\tLR: 0.010000\n",
      "6.2780\t5.7466\t5.4238\t5.3610\t5.3830\t5.5038\t\n",
      "Training Epoch: 4 [586/562]\tLoss: 33.7509\tLR: 0.010000\n",
      "6.8603\t6.1615\t5.5487\t5.2186\t5.0342\t4.9277\t\n",
      "Training Epoch: 4 [602/562]\tLoss: 36.5252\tLR: 0.010000\n",
      "7.3004\t6.6367\t6.0197\t5.6363\t5.4722\t5.4599\t\n",
      "Training Epoch: 4 [618/562]\tLoss: 29.8209\tLR: 0.010000\n",
      "5.4032\t5.1064\t4.8597\t4.7508\t4.7828\t4.9180\t\n",
      "Training Epoch: 4 [634/562]\tLoss: 33.0544\tLR: 0.010000\n",
      "6.2034\t5.6874\t5.2940\t5.1488\t5.2617\t5.4592\t\n",
      "Training Epoch: 4 [650/562]\tLoss: 23.8043\tLR: 0.010000\n",
      "3.8358\t3.6300\t3.6425\t3.8492\t4.2150\t4.6319\t\n",
      "Training Epoch: 4 [666/562]\tLoss: 34.5208\tLR: 0.010000\n",
      "6.8945\t6.2232\t5.6570\t5.3412\t5.2055\t5.1994\t\n",
      "Training Epoch: 4 [682/562]\tLoss: 24.7148\tLR: 0.010000\n",
      "3.6337\t3.6434\t3.8600\t4.1662\t4.5243\t4.8873\t\n",
      "Training Epoch: 4 [698/562]\tLoss: 30.0436\tLR: 0.010000\n",
      "5.4013\t5.1222\t4.8824\t4.7821\t4.8538\t5.0019\t\n",
      "Training Epoch: 4 [714/562]\tLoss: 20.9104\tLR: 0.010000\n",
      "4.0372\t3.6470\t3.2782\t3.1080\t3.2485\t3.5915\t\n",
      "Training Epoch: 4 [730/562]\tLoss: 19.9759\tLR: 0.010000\n",
      "3.8767\t3.4962\t3.2451\t3.0941\t3.1003\t3.1635\t\n",
      "Training Epoch: 4 [746/562]\tLoss: 25.1511\tLR: 0.010000\n",
      "4.6438\t4.2553\t4.0189\t3.9494\t4.0547\t4.2290\t\n",
      "Training Epoch: 4 [762/562]\tLoss: 31.7083\tLR: 0.010000\n",
      "6.7498\t5.8362\t5.1205\t4.7517\t4.6169\t4.6334\t\n",
      "Training Epoch: 4 [778/562]\tLoss: 25.0973\tLR: 0.010000\n",
      "5.2487\t4.6142\t4.0442\t3.6843\t3.6412\t3.8647\t\n",
      "Training Epoch: 4 [794/562]\tLoss: 30.2002\tLR: 0.010000\n",
      "5.6166\t5.1334\t4.8410\t4.7805\t4.8590\t4.9697\t\n",
      "Training Epoch: 4 [810/562]\tLoss: 35.5055\tLR: 0.010000\n",
      "7.0911\t6.4490\t5.8735\t5.4942\t5.3259\t5.2718\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [826/562]\tLoss: 24.3050\tLR: 0.010000\n",
      "4.2385\t4.0296\t3.9218\t3.9320\t4.0251\t4.1579\t\n",
      "Training Epoch: 4 [842/562]\tLoss: 21.3182\tLR: 0.010000\n",
      "3.5960\t3.3855\t3.3223\t3.3954\t3.6340\t3.9849\t\n",
      "Training Epoch: 4 [858/562]\tLoss: 21.9941\tLR: 0.010000\n",
      "3.7439\t3.5091\t3.4476\t3.5422\t3.7597\t3.9916\t\n",
      "Training Epoch: 4 [874/562]\tLoss: 33.0234\tLR: 0.010000\n",
      "6.1131\t5.6895\t5.3683\t5.2648\t5.2631\t5.3246\t\n",
      "Training Epoch: 4 [890/562]\tLoss: 37.4232\tLR: 0.010000\n",
      "7.7065\t6.8482\t6.1697\t5.7537\t5.5346\t5.4104\t\n",
      "Training Epoch: 4 [898/562]\tLoss: 34.0593\tLR: 0.010000\n",
      "6.0595\t5.7631\t5.5219\t5.4191\t5.5407\t5.7551\t\n",
      "[0.30276355147361755, 0.4886853098869324, 0.20855113863945007, 0.016594456508755684, 0.6522101759910583, 0.14026924967765808, 0.20752057433128357, 0.019439322873950005, 0.5730249881744385, 0.16645103693008423, 0.2605239748954773, 0.003076213179156184, 0.39585742354393005, 0.2543908357620239, 0.349751740694046, -0.0025740645360201597, 0.2447969615459442, 0.0, 0.7552030384540558, 0.00019571346638258547]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5627, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5123, Accuracy: 0.1868\n",
      "Test set t = 02: Average loss: 0.4755, Accuracy: 0.1851\n",
      "Test set t = 03: Average loss: 0.4599, Accuracy: 0.1851\n",
      "Test set t = 04: Average loss: 0.4616, Accuracy: 0.1833\n",
      "Test set t = 05: Average loss: 0.4743, Accuracy: 0.1708\n",
      "\n",
      "Training Epoch: 5 [10/562]\tLoss: 34.3356\tLR: 0.010000\n",
      "6.9465\t6.1805\t5.5409\t5.2474\t5.1736\t5.2466\t\n",
      "Training Epoch: 5 [26/562]\tLoss: 23.1019\tLR: 0.010000\n",
      "4.1723\t3.9192\t3.7102\t3.6401\t3.7323\t3.9278\t\n",
      "Training Epoch: 5 [42/562]\tLoss: 33.5109\tLR: 0.010000\n",
      "6.8694\t6.0805\t5.4161\t5.0822\t5.0026\t5.0600\t\n",
      "Training Epoch: 5 [58/562]\tLoss: 29.6650\tLR: 0.010000\n",
      "5.5587\t5.1069\t4.8050\t4.6722\t4.6980\t4.8241\t\n",
      "Training Epoch: 5 [74/562]\tLoss: 34.1620\tLR: 0.010000\n",
      "7.0476\t6.2520\t5.5997\t5.1701\t5.0161\t5.0765\t\n",
      "Training Epoch: 5 [90/562]\tLoss: 33.8298\tLR: 0.010000\n",
      "6.7264\t6.1652\t5.6105\t5.2527\t5.0633\t5.0117\t\n",
      "Training Epoch: 5 [106/562]\tLoss: 32.0077\tLR: 0.010000\n",
      "6.0700\t5.5373\t5.1515\t4.9990\t5.0386\t5.2113\t\n",
      "Training Epoch: 5 [122/562]\tLoss: 36.4293\tLR: 0.010000\n",
      "6.9016\t6.3221\t5.9453\t5.8018\t5.7286\t5.7299\t\n",
      "Training Epoch: 5 [138/562]\tLoss: 31.5865\tLR: 0.010000\n",
      "5.5657\t5.2296\t5.0913\t5.1182\t5.2049\t5.3768\t\n",
      "Training Epoch: 5 [154/562]\tLoss: 17.1550\tLR: 0.010000\n",
      "3.1483\t2.8456\t2.6855\t2.6935\t2.8019\t2.9802\t\n",
      "Training Epoch: 5 [170/562]\tLoss: 24.8289\tLR: 0.010000\n",
      "4.6664\t4.2989\t4.0048\t3.8802\t3.9102\t4.0685\t\n",
      "Training Epoch: 5 [186/562]\tLoss: 34.6259\tLR: 0.010000\n",
      "7.0947\t6.2376\t5.6053\t5.2484\t5.1677\t5.2721\t\n",
      "Training Epoch: 5 [202/562]\tLoss: 22.9551\tLR: 0.010000\n",
      "4.2795\t3.8754\t3.6294\t3.5743\t3.7062\t3.8903\t\n",
      "Training Epoch: 5 [218/562]\tLoss: 25.2283\tLR: 0.010000\n",
      "4.2616\t4.0343\t3.9475\t4.0632\t4.3178\t4.6039\t\n",
      "Training Epoch: 5 [234/562]\tLoss: 20.9586\tLR: 0.010000\n",
      "3.6370\t3.3610\t3.2462\t3.3046\t3.5275\t3.8823\t\n",
      "Training Epoch: 5 [250/562]\tLoss: 32.3391\tLR: 0.010000\n",
      "5.7769\t5.4188\t5.1700\t5.1834\t5.3068\t5.4832\t\n",
      "Training Epoch: 5 [266/562]\tLoss: 27.0363\tLR: 0.010000\n",
      "5.0990\t4.6362\t4.3443\t4.2230\t4.2834\t4.4504\t\n",
      "Training Epoch: 5 [282/562]\tLoss: 20.9310\tLR: 0.010000\n",
      "3.4321\t3.2765\t3.2944\t3.4172\t3.6287\t3.8821\t\n",
      "Training Epoch: 5 [298/562]\tLoss: 25.7473\tLR: 0.010000\n",
      "4.6377\t4.2608\t4.0471\t4.0348\t4.2284\t4.5384\t\n",
      "Training Epoch: 5 [314/562]\tLoss: 28.3057\tLR: 0.010000\n",
      "5.4718\t4.9398\t4.5733\t4.4041\t4.3985\t4.5182\t\n",
      "Training Epoch: 5 [330/562]\tLoss: 27.4400\tLR: 0.010000\n",
      "4.8022\t4.4904\t4.3329\t4.3686\t4.5824\t4.8634\t\n",
      "Training Epoch: 5 [346/562]\tLoss: 22.8897\tLR: 0.010000\n",
      "4.2391\t3.8834\t3.6328\t3.5964\t3.6876\t3.8504\t\n",
      "Training Epoch: 5 [362/562]\tLoss: 33.6781\tLR: 0.010000\n",
      "6.2277\t5.7859\t5.5068\t5.3837\t5.3686\t5.4055\t\n",
      "Training Epoch: 5 [378/562]\tLoss: 25.1458\tLR: 0.010000\n",
      "4.5737\t4.2411\t4.0742\t3.9973\t4.0441\t4.2154\t\n",
      "Training Epoch: 5 [394/562]\tLoss: 29.8329\tLR: 0.010000\n",
      "5.7272\t5.1837\t4.8208\t4.6747\t4.6805\t4.7460\t\n",
      "Training Epoch: 5 [410/562]\tLoss: 34.5995\tLR: 0.010000\n",
      "6.9545\t6.2133\t5.6629\t5.3459\t5.2197\t5.2032\t\n",
      "Training Epoch: 5 [426/562]\tLoss: 24.0807\tLR: 0.010000\n",
      "4.5392\t4.0838\t3.8062\t3.7591\t3.8511\t4.0414\t\n",
      "Training Epoch: 5 [442/562]\tLoss: 25.0909\tLR: 0.010000\n",
      "4.6156\t4.2362\t4.0199\t3.9453\t4.0433\t4.2305\t\n",
      "Training Epoch: 5 [458/562]\tLoss: 20.4609\tLR: 0.010000\n",
      "3.3711\t3.2418\t3.1591\t3.2448\t3.5144\t3.9297\t\n",
      "Training Epoch: 5 [474/562]\tLoss: 32.6549\tLR: 0.010000\n",
      "6.3118\t5.6949\t5.2700\t5.0862\t5.1078\t5.1841\t\n",
      "Training Epoch: 5 [490/562]\tLoss: 28.3829\tLR: 0.010000\n",
      "5.9680\t5.3109\t4.6606\t4.2621\t4.0938\t4.0875\t\n",
      "Training Epoch: 5 [506/562]\tLoss: 33.0191\tLR: 0.010000\n",
      "6.6831\t6.0086\t5.4115\t5.0673\t4.9268\t4.9219\t\n",
      "Training Epoch: 5 [522/562]\tLoss: 26.7788\tLR: 0.010000\n",
      "5.4304\t4.8161\t4.3294\t4.0647\t4.0289\t4.1094\t\n",
      "Training Epoch: 5 [538/562]\tLoss: 36.2776\tLR: 0.010000\n",
      "7.6478\t6.7962\t6.0560\t5.5268\t5.1916\t5.0592\t\n",
      "Training Epoch: 5 [554/562]\tLoss: 26.4256\tLR: 0.010000\n",
      "5.2494\t4.6962\t4.2406\t4.0135\t4.0365\t4.1892\t\n",
      "Training Epoch: 5 [570/562]\tLoss: 29.2348\tLR: 0.010000\n",
      "5.7132\t5.2044\t4.7932\t4.5679\t4.4820\t4.4742\t\n",
      "Training Epoch: 5 [586/562]\tLoss: 35.4829\tLR: 0.010000\n",
      "6.6934\t6.2514\t5.8504\t5.6160\t5.5396\t5.5322\t\n",
      "Training Epoch: 5 [602/562]\tLoss: 29.9878\tLR: 0.010000\n",
      "5.9233\t5.3572\t4.8690\t4.6355\t4.5976\t4.6052\t\n",
      "Training Epoch: 5 [618/562]\tLoss: 34.0678\tLR: 0.010000\n",
      "6.7063\t6.0918\t5.5900\t5.3154\t5.1885\t5.1758\t\n",
      "Training Epoch: 5 [634/562]\tLoss: 28.7724\tLR: 0.010000\n",
      "5.3427\t4.9128\t4.6143\t4.5299\t4.5924\t4.7803\t\n",
      "Training Epoch: 5 [650/562]\tLoss: 33.3586\tLR: 0.010000\n",
      "6.6328\t6.0243\t5.5630\t5.2301\t5.0073\t4.9010\t\n",
      "Training Epoch: 5 [666/562]\tLoss: 34.2165\tLR: 0.010000\n",
      "6.9282\t6.2324\t5.6434\t5.2918\t5.0930\t5.0277\t\n",
      "Training Epoch: 5 [682/562]\tLoss: 31.9458\tLR: 0.010000\n",
      "6.7982\t5.9300\t5.1683\t4.7435\t4.6363\t4.6695\t\n",
      "Training Epoch: 5 [698/562]\tLoss: 23.3690\tLR: 0.010000\n",
      "3.5556\t3.4725\t3.6024\t3.8848\t4.2510\t4.6026\t\n",
      "Training Epoch: 5 [714/562]\tLoss: 35.9597\tLR: 0.010000\n",
      "6.9967\t6.3783\t5.9053\t5.6292\t5.5074\t5.5428\t\n",
      "Training Epoch: 5 [730/562]\tLoss: 31.1550\tLR: 0.010000\n",
      "6.2026\t5.5125\t5.0127\t4.7663\t4.7400\t4.9209\t\n",
      "Training Epoch: 5 [746/562]\tLoss: 26.4908\tLR: 0.010000\n",
      "4.6019\t4.3515\t4.1787\t4.2212\t4.4090\t4.7286\t\n",
      "Training Epoch: 5 [762/562]\tLoss: 29.5171\tLR: 0.010000\n",
      "5.4076\t5.0285\t4.8170\t4.7187\t4.7096\t4.8357\t\n",
      "Training Epoch: 5 [778/562]\tLoss: 35.4205\tLR: 0.010000\n",
      "7.4250\t6.4648\t5.7631\t5.3703\t5.2106\t5.1867\t\n",
      "Training Epoch: 5 [794/562]\tLoss: 30.1194\tLR: 0.010000\n",
      "5.3801\t5.0201\t4.8334\t4.8174\t4.9372\t5.1312\t\n",
      "Training Epoch: 5 [810/562]\tLoss: 31.7079\tLR: 0.010000\n",
      "5.9663\t5.5256\t5.2268\t5.0426\t4.9914\t4.9552\t\n",
      "Training Epoch: 5 [826/562]\tLoss: 29.0142\tLR: 0.010000\n",
      "5.3071\t4.8278\t4.5371\t4.4854\t4.7137\t5.1430\t\n",
      "Training Epoch: 5 [842/562]\tLoss: 27.2062\tLR: 0.010000\n",
      "4.8427\t4.4820\t4.3062\t4.3481\t4.5087\t4.7185\t\n",
      "Training Epoch: 5 [858/562]\tLoss: 33.8985\tLR: 0.010000\n",
      "6.3290\t5.8673\t5.5675\t5.4031\t5.3539\t5.3777\t\n",
      "Training Epoch: 5 [874/562]\tLoss: 24.6999\tLR: 0.010000\n",
      "4.5033\t4.1817\t4.0148\t3.9569\t3.9735\t4.0697\t\n",
      "Training Epoch: 5 [890/562]\tLoss: 29.7408\tLR: 0.010000\n",
      "5.6965\t5.1746\t4.7803\t4.6297\t4.6616\t4.7981\t\n",
      "Training Epoch: 5 [898/562]\tLoss: 22.1546\tLR: 0.010000\n",
      "5.0521\t4.2556\t3.5583\t3.2193\t2.9819\t3.0876\t\n",
      "[0.2963730990886688, 0.5054029822349548, 0.19822391867637634, 0.017813213169574738, 0.6832588911056519, 0.11562975496053696, 0.2011113539338112, 0.020482681691646576, 0.6244393587112427, 0.1498192399740219, 0.2257414013147354, 0.0025516210589557886, 0.4206617474555969, 0.2519111633300781, 0.32742708921432495, -0.005088621750473976, 0.24394342303276062, 0.0, 0.7560565769672394, -0.0013875897275283933]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5692, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5158, Accuracy: 0.1868\n",
      "Test set t = 02: Average loss: 0.4790, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4643, Accuracy: 0.1833\n",
      "Test set t = 04: Average loss: 0.4668, Accuracy: 0.1762\n",
      "Test set t = 05: Average loss: 0.4805, Accuracy: 0.1762\n",
      "\n",
      "Training Epoch: 6 [10/562]\tLoss: 25.7458\tLR: 0.010000\n",
      "4.3291\t4.0842\t4.0350\t4.1710\t4.3943\t4.7321\t\n",
      "Training Epoch: 6 [26/562]\tLoss: 24.3642\tLR: 0.010000\n",
      "4.1524\t3.8917\t3.8338\t3.9301\t4.1380\t4.4181\t\n",
      "Training Epoch: 6 [42/562]\tLoss: 34.1639\tLR: 0.010000\n",
      "6.7688\t6.1179\t5.5825\t5.3096\t5.1848\t5.2004\t\n",
      "Training Epoch: 6 [58/562]\tLoss: 33.4002\tLR: 0.010000\n",
      "6.5417\t5.9278\t5.4628\t5.1985\t5.1078\t5.1615\t\n",
      "Training Epoch: 6 [74/562]\tLoss: 30.7346\tLR: 0.010000\n",
      "6.5042\t5.6395\t4.9435\t4.5915\t4.5013\t4.5546\t\n",
      "Training Epoch: 6 [90/562]\tLoss: 32.1895\tLR: 0.010000\n",
      "6.3625\t5.6798\t5.1710\t4.9749\t4.9440\t5.0572\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [106/562]\tLoss: 33.5793\tLR: 0.010000\n",
      "6.3850\t5.8437\t5.4747\t5.2652\t5.2441\t5.3666\t\n",
      "Training Epoch: 6 [122/562]\tLoss: 30.4004\tLR: 0.010000\n",
      "6.1391\t5.5136\t4.9740\t4.6850\t4.5657\t4.5230\t\n",
      "Training Epoch: 6 [138/562]\tLoss: 29.5434\tLR: 0.010000\n",
      "5.2633\t4.8656\t4.6668\t4.6902\t4.8983\t5.1592\t\n",
      "Training Epoch: 6 [154/562]\tLoss: 30.3267\tLR: 0.010000\n",
      "6.0097\t5.4131\t4.9457\t4.6837\t4.6072\t4.6672\t\n",
      "Training Epoch: 6 [170/562]\tLoss: 30.8072\tLR: 0.010000\n",
      "5.9484\t5.3969\t4.9835\t4.8032\t4.7984\t4.8768\t\n",
      "Training Epoch: 6 [186/562]\tLoss: 25.4489\tLR: 0.010000\n",
      "4.7666\t4.3325\t4.0511\t3.9925\t4.0782\t4.2279\t\n",
      "Training Epoch: 6 [202/562]\tLoss: 28.8660\tLR: 0.010000\n",
      "5.9471\t5.2521\t4.6872\t4.3769\t4.2771\t4.3256\t\n",
      "Training Epoch: 6 [218/562]\tLoss: 33.7129\tLR: 0.010000\n",
      "6.7700\t5.9991\t5.4645\t5.2051\t5.1111\t5.1631\t\n",
      "Training Epoch: 6 [234/562]\tLoss: 35.9249\tLR: 0.010000\n",
      "7.8746\t6.7542\t5.8888\t5.3449\t5.0617\t5.0007\t\n",
      "Training Epoch: 6 [250/562]\tLoss: 29.8959\tLR: 0.010000\n",
      "5.7541\t5.1987\t4.7734\t4.6247\t4.6835\t4.8614\t\n",
      "Training Epoch: 6 [266/562]\tLoss: 34.4390\tLR: 0.010000\n",
      "6.5369\t5.9243\t5.5236\t5.4186\t5.4673\t5.5683\t\n",
      "Training Epoch: 6 [282/562]\tLoss: 22.1942\tLR: 0.010000\n",
      "3.9822\t3.6462\t3.5002\t3.5330\t3.6721\t3.8606\t\n",
      "Training Epoch: 6 [298/562]\tLoss: 29.8758\tLR: 0.010000\n",
      "5.9224\t5.1816\t4.8017\t4.6354\t4.6088\t4.7258\t\n",
      "Training Epoch: 6 [314/562]\tLoss: 34.6110\tLR: 0.010000\n",
      "7.1369\t6.2974\t5.6247\t5.2416\t5.1020\t5.2085\t\n",
      "Training Epoch: 6 [330/562]\tLoss: 37.3774\tLR: 0.010000\n",
      "7.8081\t6.9194\t6.1956\t5.7215\t5.4479\t5.2849\t\n",
      "Training Epoch: 6 [346/562]\tLoss: 33.0925\tLR: 0.010000\n",
      "6.6252\t5.9046\t5.3158\t5.0284\t5.0172\t5.2013\t\n",
      "Training Epoch: 6 [362/562]\tLoss: 25.1549\tLR: 0.010000\n",
      "4.3782\t4.1217\t3.9890\t4.0401\t4.2132\t4.4128\t\n",
      "Training Epoch: 6 [378/562]\tLoss: 24.9772\tLR: 0.010000\n",
      "4.1758\t3.9626\t3.9492\t4.1180\t4.3115\t4.4600\t\n",
      "Training Epoch: 6 [394/562]\tLoss: 25.6848\tLR: 0.010000\n",
      "5.5699\t4.7331\t4.1208\t3.7575\t3.7246\t3.7789\t\n",
      "Training Epoch: 6 [410/562]\tLoss: 26.0685\tLR: 0.010000\n",
      "4.9310\t4.4685\t4.0917\t3.9968\t4.1505\t4.4299\t\n",
      "Training Epoch: 6 [426/562]\tLoss: 32.9993\tLR: 0.010000\n",
      "6.3420\t5.7162\t5.3405\t5.1579\t5.1602\t5.2826\t\n",
      "Training Epoch: 6 [442/562]\tLoss: 26.7710\tLR: 0.010000\n",
      "4.8065\t4.4270\t4.2091\t4.2031\t4.4087\t4.7166\t\n",
      "Training Epoch: 6 [458/562]\tLoss: 27.4569\tLR: 0.010000\n",
      "5.0429\t4.5938\t4.4073\t4.3639\t4.4473\t4.6016\t\n",
      "Training Epoch: 6 [474/562]\tLoss: 36.9411\tLR: 0.010000\n",
      "7.6509\t6.7743\t6.0964\t5.6802\t5.4272\t5.3120\t\n",
      "Training Epoch: 6 [490/562]\tLoss: 42.1386\tLR: 0.010000\n",
      "9.2512\t8.0441\t6.9835\t6.2877\t5.8979\t5.6742\t\n",
      "Training Epoch: 6 [506/562]\tLoss: 32.5357\tLR: 0.010000\n",
      "6.0405\t5.5673\t5.2731\t5.1513\t5.1791\t5.3243\t\n",
      "Training Epoch: 6 [522/562]\tLoss: 18.2929\tLR: 0.010000\n",
      "2.8398\t2.7187\t2.7427\t2.9785\t3.3156\t3.6976\t\n",
      "Training Epoch: 6 [538/562]\tLoss: 42.7188\tLR: 0.010000\n",
      "9.1048\t7.9440\t7.0463\t6.4845\t6.1610\t5.9781\t\n",
      "Training Epoch: 6 [554/562]\tLoss: 23.3637\tLR: 0.010000\n",
      "4.1845\t3.8621\t3.6912\t3.7382\t3.8771\t4.0106\t\n",
      "Training Epoch: 6 [570/562]\tLoss: 32.5827\tLR: 0.010000\n",
      "6.1595\t5.6400\t5.2712\t5.1343\t5.1397\t5.2381\t\n",
      "Training Epoch: 6 [586/562]\tLoss: 33.7859\tLR: 0.010000\n",
      "6.5134\t5.8723\t5.4973\t5.3058\t5.2514\t5.3458\t\n",
      "Training Epoch: 6 [602/562]\tLoss: 35.6384\tLR: 0.010000\n",
      "6.8678\t6.2389\t5.7570\t5.5896\t5.5736\t5.6115\t\n",
      "Training Epoch: 6 [618/562]\tLoss: 25.2693\tLR: 0.010000\n",
      "4.5192\t4.1790\t3.9813\t4.0035\t4.1739\t4.4123\t\n",
      "Training Epoch: 6 [634/562]\tLoss: 26.5404\tLR: 0.010000\n",
      "4.4978\t4.2386\t4.1365\t4.2565\t4.5436\t4.8674\t\n",
      "Training Epoch: 6 [650/562]\tLoss: 30.2488\tLR: 0.010000\n",
      "5.8971\t5.2544\t4.8501\t4.6897\t4.7227\t4.8349\t\n",
      "Training Epoch: 6 [666/562]\tLoss: 22.0399\tLR: 0.010000\n",
      "3.6629\t3.4276\t3.3692\t3.5257\t3.8236\t4.2309\t\n",
      "Training Epoch: 6 [682/562]\tLoss: 28.7255\tLR: 0.010000\n",
      "5.3569\t4.8923\t4.5963\t4.5104\t4.5875\t4.7820\t\n",
      "Training Epoch: 6 [698/562]\tLoss: 25.3989\tLR: 0.010000\n",
      "5.0416\t4.4574\t4.0800\t3.8849\t3.9003\t4.0347\t\n",
      "Training Epoch: 6 [714/562]\tLoss: 34.2403\tLR: 0.010000\n",
      "6.7710\t6.0717\t5.5829\t5.3161\t5.2313\t5.2672\t\n",
      "Training Epoch: 6 [730/562]\tLoss: 20.8687\tLR: 0.010000\n",
      "3.3257\t3.1172\t3.1493\t3.3963\t3.7541\t4.1262\t\n",
      "Training Epoch: 6 [746/562]\tLoss: 29.9589\tLR: 0.010000\n",
      "5.6578\t5.1406\t4.8069\t4.6871\t4.7573\t4.9092\t\n",
      "Training Epoch: 6 [762/562]\tLoss: 27.2284\tLR: 0.010000\n",
      "4.9976\t4.5643\t4.3124\t4.2586\t4.4151\t4.6803\t\n",
      "Training Epoch: 6 [778/562]\tLoss: 30.2595\tLR: 0.010000\n",
      "5.7865\t5.2345\t4.8447\t4.6815\t4.7497\t4.9626\t\n",
      "Training Epoch: 6 [794/562]\tLoss: 30.4701\tLR: 0.010000\n",
      "5.7645\t5.3189\t4.9633\t4.7653\t4.7756\t4.8825\t\n",
      "Training Epoch: 6 [810/562]\tLoss: 28.9388\tLR: 0.010000\n",
      "5.5393\t4.9294\t4.5152\t4.4429\t4.6075\t4.9045\t\n",
      "Training Epoch: 6 [826/562]\tLoss: 25.3671\tLR: 0.010000\n",
      "4.8400\t4.3449\t4.0317\t3.9192\t4.0188\t4.2125\t\n",
      "Training Epoch: 6 [842/562]\tLoss: 18.9599\tLR: 0.010000\n",
      "2.7133\t2.6475\t2.8221\t3.1486\t3.5877\t4.0407\t\n",
      "Training Epoch: 6 [858/562]\tLoss: 25.9959\tLR: 0.010000\n",
      "4.3629\t4.1856\t4.1405\t4.2397\t4.4254\t4.6418\t\n",
      "Training Epoch: 6 [874/562]\tLoss: 21.2929\tLR: 0.010000\n",
      "3.7407\t3.4650\t3.3485\t3.3892\t3.5557\t3.7938\t\n",
      "Training Epoch: 6 [890/562]\tLoss: 19.7498\tLR: 0.010000\n",
      "3.1057\t3.0757\t3.1096\t3.2336\t3.4558\t3.7695\t\n",
      "Training Epoch: 6 [898/562]\tLoss: 21.0564\tLR: 0.010000\n",
      "3.4993\t3.3311\t3.3791\t3.4764\t3.6139\t3.7566\t\n",
      "[0.3035533130168915, 0.4998153746128082, 0.1966313123703003, 0.018497131764888763, 0.6960566639900208, 0.0975717082619667, 0.20637162774801254, 0.021747639402747154, 0.6593795418739319, 0.14002811908721924, 0.20059233903884888, 0.0016066732350736856, 0.4311573803424835, 0.2557019889354706, 0.3131406307220459, -0.007389162201434374, 0.24345213174819946, 0.0, 0.7565478682518005, -0.002975978422909975]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5678, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5141, Accuracy: 0.1868\n",
      "Test set t = 02: Average loss: 0.4772, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4623, Accuracy: 0.1868\n",
      "Test set t = 04: Average loss: 0.4644, Accuracy: 0.1779\n",
      "Test set t = 05: Average loss: 0.4774, Accuracy: 0.1744\n",
      "\n",
      "Training Epoch: 7 [10/562]\tLoss: 28.0821\tLR: 0.010000\n",
      "5.2832\t4.7482\t4.4657\t4.3896\t4.4838\t4.7115\t\n",
      "Training Epoch: 7 [26/562]\tLoss: 28.9070\tLR: 0.010000\n",
      "5.0966\t4.7566\t4.5889\t4.6137\t4.8130\t5.0381\t\n",
      "Training Epoch: 7 [42/562]\tLoss: 23.1915\tLR: 0.010000\n",
      "4.4870\t4.0086\t3.6976\t3.5758\t3.6153\t3.8073\t\n",
      "Training Epoch: 7 [58/562]\tLoss: 32.6060\tLR: 0.010000\n",
      "6.2571\t5.7339\t5.3264\t5.1216\t5.0530\t5.1140\t\n",
      "Training Epoch: 7 [74/562]\tLoss: 30.9891\tLR: 0.010000\n",
      "6.0063\t5.4279\t5.0217\t4.8257\t4.8148\t4.8926\t\n",
      "Training Epoch: 7 [90/562]\tLoss: 30.5146\tLR: 0.010000\n",
      "5.9375\t5.3679\t4.9783\t4.7929\t4.7157\t4.7222\t\n",
      "Training Epoch: 7 [106/562]\tLoss: 29.6061\tLR: 0.010000\n",
      "5.9170\t5.3682\t4.8544\t4.6014\t4.4678\t4.3972\t\n",
      "Training Epoch: 7 [122/562]\tLoss: 35.3304\tLR: 0.010000\n",
      "6.8190\t6.2165\t5.7700\t5.5376\t5.4737\t5.5137\t\n",
      "Training Epoch: 7 [138/562]\tLoss: 31.9354\tLR: 0.010000\n",
      "6.5152\t5.7811\t5.1999\t4.8590\t4.7637\t4.8165\t\n",
      "Training Epoch: 7 [154/562]\tLoss: 32.1337\tLR: 0.010000\n",
      "6.8691\t5.9393\t5.2272\t4.8090\t4.6336\t4.6556\t\n",
      "Training Epoch: 7 [170/562]\tLoss: 26.0660\tLR: 0.010000\n",
      "4.6014\t4.3393\t4.2107\t4.1958\t4.2837\t4.4351\t\n",
      "Training Epoch: 7 [186/562]\tLoss: 23.8477\tLR: 0.010000\n",
      "4.3874\t4.0541\t3.7900\t3.7108\t3.8137\t4.0918\t\n",
      "Training Epoch: 7 [202/562]\tLoss: 34.0011\tLR: 0.010000\n",
      "6.3455\t5.7799\t5.4631\t5.3671\t5.4518\t5.5937\t\n",
      "Training Epoch: 7 [218/562]\tLoss: 22.6571\tLR: 0.010000\n",
      "4.2258\t3.8806\t3.6613\t3.5855\t3.6084\t3.6955\t\n",
      "Training Epoch: 7 [234/562]\tLoss: 36.8080\tLR: 0.010000\n",
      "7.5413\t6.7237\t6.0632\t5.6347\t5.4498\t5.3954\t\n",
      "Training Epoch: 7 [250/562]\tLoss: 24.1580\tLR: 0.010000\n",
      "3.8738\t3.7528\t3.7896\t3.9490\t4.2300\t4.5627\t\n",
      "Training Epoch: 7 [266/562]\tLoss: 34.4421\tLR: 0.010000\n",
      "6.7088\t6.0493\t5.6157\t5.3805\t5.3111\t5.3767\t\n",
      "Training Epoch: 7 [282/562]\tLoss: 38.5975\tLR: 0.010000\n",
      "7.9677\t7.0900\t6.3795\t5.8994\t5.6548\t5.6061\t\n",
      "Training Epoch: 7 [298/562]\tLoss: 26.9572\tLR: 0.010000\n",
      "4.8508\t4.4401\t4.2629\t4.3043\t4.4536\t4.6455\t\n",
      "Training Epoch: 7 [314/562]\tLoss: 37.4047\tLR: 0.010000\n",
      "6.9713\t6.4456\t6.1077\t5.9510\t5.9291\t6.0000\t\n",
      "Training Epoch: 7 [330/562]\tLoss: 33.5667\tLR: 0.010000\n",
      "6.4003\t5.8548\t5.4464\t5.3011\t5.2548\t5.3094\t\n",
      "Training Epoch: 7 [346/562]\tLoss: 24.7724\tLR: 0.010000\n",
      "4.8859\t4.3583\t4.0000\t3.8031\t3.7947\t3.9304\t\n",
      "Training Epoch: 7 [362/562]\tLoss: 31.6951\tLR: 0.010000\n",
      "6.3755\t5.7525\t5.2461\t4.8965\t4.7167\t4.7078\t\n",
      "Training Epoch: 7 [378/562]\tLoss: 24.9520\tLR: 0.010000\n",
      "4.6584\t4.1501\t3.9029\t3.8898\t4.0375\t4.3133\t\n",
      "Training Epoch: 7 [394/562]\tLoss: 23.8945\tLR: 0.010000\n",
      "4.8909\t4.2692\t3.7936\t3.5944\t3.5992\t3.7472\t\n",
      "Training Epoch: 7 [410/562]\tLoss: 31.3576\tLR: 0.010000\n",
      "6.3080\t5.7068\t5.2099\t4.9098\t4.6932\t4.5299\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [426/562]\tLoss: 25.9830\tLR: 0.010000\n",
      "4.6949\t4.3219\t4.1634\t4.1388\t4.2412\t4.4228\t\n",
      "Training Epoch: 7 [442/562]\tLoss: 31.2072\tLR: 0.010000\n",
      "5.8858\t5.3486\t4.9982\t4.8943\t4.9546\t5.1258\t\n",
      "Training Epoch: 7 [458/562]\tLoss: 23.4216\tLR: 0.010000\n",
      "3.8862\t3.7050\t3.7065\t3.8315\t4.0384\t4.2541\t\n",
      "Training Epoch: 7 [474/562]\tLoss: 20.2550\tLR: 0.010000\n",
      "3.3271\t3.0909\t3.0404\t3.2223\t3.5760\t3.9984\t\n",
      "Training Epoch: 7 [490/562]\tLoss: 29.9923\tLR: 0.010000\n",
      "5.7479\t5.1727\t4.8309\t4.7022\t4.7147\t4.8238\t\n",
      "Training Epoch: 7 [506/562]\tLoss: 27.3461\tLR: 0.010000\n",
      "4.9905\t4.6544\t4.4357\t4.3623\t4.3998\t4.5035\t\n",
      "Training Epoch: 7 [522/562]\tLoss: 28.0211\tLR: 0.010000\n",
      "5.3624\t4.8757\t4.4722\t4.3063\t4.3890\t4.6155\t\n",
      "Training Epoch: 7 [538/562]\tLoss: 18.3349\tLR: 0.010000\n",
      "3.1804\t2.9584\t2.8504\t2.8842\t3.0977\t3.3638\t\n",
      "Training Epoch: 7 [554/562]\tLoss: 23.2455\tLR: 0.010000\n",
      "3.6818\t3.5926\t3.6384\t3.8139\t4.0831\t4.4357\t\n",
      "Training Epoch: 7 [570/562]\tLoss: 25.9143\tLR: 0.010000\n",
      "5.2044\t4.5898\t4.0986\t3.8843\t3.9518\t4.1853\t\n",
      "Training Epoch: 7 [586/562]\tLoss: 27.2955\tLR: 0.010000\n",
      "5.0493\t4.6747\t4.4179\t4.3286\t4.3515\t4.4735\t\n",
      "Training Epoch: 7 [602/562]\tLoss: 32.6962\tLR: 0.010000\n",
      "6.1923\t5.7075\t5.2945\t5.1242\t5.1323\t5.2454\t\n",
      "Training Epoch: 7 [618/562]\tLoss: 30.3452\tLR: 0.010000\n",
      "5.9302\t5.4379\t5.0214\t4.7566\t4.6429\t4.5562\t\n",
      "Training Epoch: 7 [634/562]\tLoss: 29.7742\tLR: 0.010000\n",
      "5.7949\t5.2824\t4.8677\t4.6219\t4.5700\t4.6372\t\n",
      "Training Epoch: 7 [650/562]\tLoss: 28.1470\tLR: 0.010000\n",
      "5.2790\t4.8667\t4.5596\t4.4377\t4.4493\t4.5547\t\n",
      "Training Epoch: 7 [666/562]\tLoss: 31.1212\tLR: 0.010000\n",
      "6.0861\t5.5495\t5.1042\t4.8407\t4.7547\t4.7859\t\n",
      "Training Epoch: 7 [682/562]\tLoss: 30.6775\tLR: 0.010000\n",
      "5.6846\t5.2521\t4.9899\t4.8840\t4.8931\t4.9738\t\n",
      "Training Epoch: 7 [698/562]\tLoss: 31.7864\tLR: 0.010000\n",
      "6.5103\t5.8139\t5.2480\t4.8764\t4.7078\t4.6300\t\n",
      "Training Epoch: 7 [714/562]\tLoss: 29.1223\tLR: 0.010000\n",
      "6.1074\t5.3938\t4.8260\t4.4533\t4.2201\t4.1218\t\n",
      "Training Epoch: 7 [730/562]\tLoss: 29.3635\tLR: 0.010000\n",
      "5.1814\t4.8654\t4.7186\t4.7213\t4.8264\t5.0504\t\n",
      "Training Epoch: 7 [746/562]\tLoss: 29.3116\tLR: 0.010000\n",
      "5.9638\t5.2710\t4.7453\t4.4543\t4.4114\t4.4659\t\n",
      "Training Epoch: 7 [762/562]\tLoss: 24.6168\tLR: 0.010000\n",
      "4.1707\t3.9588\t3.8931\t3.9901\t4.1846\t4.4194\t\n",
      "Training Epoch: 7 [778/562]\tLoss: 28.9671\tLR: 0.010000\n",
      "5.1535\t4.8326\t4.6816\t4.6769\t4.7442\t4.8782\t\n",
      "Training Epoch: 7 [794/562]\tLoss: 37.6057\tLR: 0.010000\n",
      "8.0652\t7.0848\t6.2333\t5.6704\t5.3462\t5.2058\t\n",
      "Training Epoch: 7 [810/562]\tLoss: 34.4297\tLR: 0.010000\n",
      "7.1776\t6.2715\t5.5818\t5.2220\t5.0826\t5.0942\t\n",
      "Training Epoch: 7 [826/562]\tLoss: 38.0431\tLR: 0.010000\n",
      "7.8133\t7.0140\t6.3382\t5.8575\t5.5834\t5.4367\t\n",
      "Training Epoch: 7 [842/562]\tLoss: 24.2142\tLR: 0.010000\n",
      "4.4028\t4.0378\t3.8597\t3.8471\t3.9398\t4.1270\t\n",
      "Training Epoch: 7 [858/562]\tLoss: 30.4545\tLR: 0.010000\n",
      "5.5041\t5.1965\t4.9976\t4.8823\t4.8770\t4.9971\t\n",
      "Training Epoch: 7 [874/562]\tLoss: 31.1055\tLR: 0.010000\n",
      "6.2334\t5.5769\t5.0834\t4.8160\t4.6829\t4.7129\t\n",
      "Training Epoch: 7 [890/562]\tLoss: 22.8537\tLR: 0.010000\n",
      "3.6084\t3.5091\t3.5630\t3.7601\t4.0484\t4.3648\t\n",
      "Training Epoch: 7 [898/562]\tLoss: 40.8006\tLR: 0.010000\n",
      "8.0842\t7.3663\t6.7936\t6.3639\t6.1305\t6.0621\t\n",
      "[0.3058614134788513, 0.497262179851532, 0.1968764066696167, 0.019412362948060036, 0.7002254724502563, 0.08371325582265854, 0.2160612717270851, 0.02260790765285492, 0.6866775155067444, 0.13170504570007324, 0.18161743879318237, 0.0013706594472751021, 0.43639642000198364, 0.26230016350746155, 0.3013034164905548, -0.009565673768520355, 0.23422299325466156, 0.0, 0.7657770067453384, -0.005000758916139603]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5662, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5136, Accuracy: 0.1868\n",
      "Test set t = 02: Average loss: 0.4778, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4624, Accuracy: 0.1851\n",
      "Test set t = 04: Average loss: 0.4630, Accuracy: 0.1762\n",
      "Test set t = 05: Average loss: 0.4743, Accuracy: 0.1744\n",
      "\n",
      "Training Epoch: 8 [10/562]\tLoss: 28.7674\tLR: 0.010000\n",
      "5.3415\t4.8393\t4.6455\t4.5825\t4.6332\t4.7254\t\n",
      "Training Epoch: 8 [26/562]\tLoss: 28.7989\tLR: 0.010000\n",
      "5.2892\t4.7909\t4.5730\t4.5669\t4.7032\t4.8757\t\n",
      "Training Epoch: 8 [42/562]\tLoss: 31.4579\tLR: 0.010000\n",
      "6.6729\t5.8557\t5.2031\t4.7256\t4.4899\t4.5108\t\n",
      "Training Epoch: 8 [58/562]\tLoss: 38.2050\tLR: 0.010000\n",
      "8.0751\t7.0422\t6.2490\t5.7925\t5.5561\t5.4901\t\n",
      "Training Epoch: 8 [74/562]\tLoss: 28.5968\tLR: 0.010000\n",
      "5.9516\t5.2635\t4.6538\t4.3222\t4.1894\t4.2162\t\n",
      "Training Epoch: 8 [90/562]\tLoss: 27.2936\tLR: 0.010000\n",
      "5.5341\t4.9227\t4.4278\t4.1458\t4.0864\t4.1767\t\n",
      "Training Epoch: 8 [106/562]\tLoss: 29.4653\tLR: 0.010000\n",
      "5.9778\t5.2246\t4.7191\t4.4909\t4.4837\t4.5692\t\n",
      "Training Epoch: 8 [122/562]\tLoss: 38.2118\tLR: 0.010000\n",
      "7.6371\t6.8461\t6.2671\t5.9465\t5.7818\t5.7332\t\n",
      "Training Epoch: 8 [138/562]\tLoss: 26.4578\tLR: 0.010000\n",
      "4.8877\t4.4665\t4.2256\t4.1576\t4.2470\t4.4734\t\n",
      "Training Epoch: 8 [154/562]\tLoss: 28.7007\tLR: 0.010000\n",
      "4.7927\t4.5867\t4.5993\t4.7329\t4.9221\t5.0669\t\n",
      "Training Epoch: 8 [170/562]\tLoss: 38.2365\tLR: 0.010000\n",
      "8.1293\t7.1212\t6.2617\t5.7849\t5.5281\t5.4113\t\n",
      "Training Epoch: 8 [186/562]\tLoss: 28.0688\tLR: 0.010000\n",
      "4.6615\t4.4531\t4.4393\t4.6165\t4.8368\t5.0615\t\n",
      "Training Epoch: 8 [202/562]\tLoss: 33.8288\tLR: 0.010000\n",
      "6.6794\t5.9656\t5.4624\t5.2651\t5.2136\t5.2426\t\n",
      "Training Epoch: 8 [218/562]\tLoss: 27.6732\tLR: 0.010000\n",
      "4.7537\t4.4903\t4.4026\t4.4508\t4.6835\t4.8924\t\n",
      "Training Epoch: 8 [234/562]\tLoss: 31.2502\tLR: 0.010000\n",
      "6.0183\t5.4756\t5.0673\t4.8699\t4.8427\t4.9764\t\n",
      "Training Epoch: 8 [250/562]\tLoss: 19.4296\tLR: 0.010000\n",
      "3.3377\t3.1288\t3.0425\t3.0948\t3.2735\t3.5523\t\n",
      "Training Epoch: 8 [266/562]\tLoss: 22.6731\tLR: 0.010000\n",
      "3.9925\t3.6409\t3.4938\t3.5497\t3.8190\t4.1772\t\n",
      "Training Epoch: 8 [282/562]\tLoss: 28.4225\tLR: 0.010000\n",
      "5.9756\t5.1614\t4.5837\t4.2416\t4.1618\t4.2983\t\n",
      "Training Epoch: 8 [298/562]\tLoss: 30.5743\tLR: 0.010000\n",
      "6.0082\t5.3703\t4.9301\t4.7442\t4.7189\t4.8026\t\n",
      "Training Epoch: 8 [314/562]\tLoss: 34.7338\tLR: 0.010000\n",
      "7.0178\t6.2451\t5.6711\t5.3457\t5.2175\t5.2367\t\n",
      "Training Epoch: 8 [330/562]\tLoss: 30.4040\tLR: 0.010000\n",
      "6.0904\t5.3903\t4.8958\t4.6619\t4.6104\t4.7551\t\n",
      "Training Epoch: 8 [346/562]\tLoss: 31.3992\tLR: 0.010000\n",
      "6.0898\t5.3611\t4.9635\t4.8726\t4.9675\t5.1447\t\n",
      "Training Epoch: 8 [362/562]\tLoss: 28.9363\tLR: 0.010000\n",
      "5.3426\t4.8951\t4.6805\t4.6293\t4.6522\t4.7365\t\n",
      "Training Epoch: 8 [378/562]\tLoss: 23.8229\tLR: 0.010000\n",
      "3.7876\t3.6341\t3.6915\t3.9222\t4.2289\t4.5584\t\n",
      "Training Epoch: 8 [394/562]\tLoss: 26.1833\tLR: 0.010000\n",
      "4.6181\t4.3380\t4.1658\t4.1739\t4.3493\t4.5382\t\n",
      "Training Epoch: 8 [410/562]\tLoss: 30.0921\tLR: 0.010000\n",
      "5.9435\t5.2722\t4.8594\t4.6785\t4.6489\t4.6896\t\n",
      "Training Epoch: 8 [426/562]\tLoss: 29.9949\tLR: 0.010000\n",
      "5.3803\t5.0057\t4.8412\t4.8315\t4.8932\t5.0430\t\n",
      "Training Epoch: 8 [442/562]\tLoss: 34.2945\tLR: 0.010000\n",
      "7.0246\t6.3339\t5.7013\t5.2783\t5.0359\t4.9203\t\n",
      "Training Epoch: 8 [458/562]\tLoss: 28.5609\tLR: 0.010000\n",
      "5.3101\t4.8981\t4.6759\t4.5897\t4.5540\t4.5330\t\n",
      "Training Epoch: 8 [474/562]\tLoss: 26.9871\tLR: 0.010000\n",
      "5.2683\t4.6391\t4.2577\t4.1505\t4.2305\t4.4410\t\n",
      "Training Epoch: 8 [490/562]\tLoss: 41.6965\tLR: 0.010000\n",
      "8.7296\t7.7078\t6.8481\t6.3451\t6.0923\t5.9734\t\n",
      "Training Epoch: 8 [506/562]\tLoss: 33.9321\tLR: 0.010000\n",
      "7.0061\t6.2027\t5.5506\t5.1498\t4.9885\t5.0345\t\n",
      "Training Epoch: 8 [522/562]\tLoss: 28.4431\tLR: 0.010000\n",
      "5.5707\t5.0704\t4.6355\t4.3871\t4.3465\t4.4330\t\n",
      "Training Epoch: 8 [538/562]\tLoss: 28.1818\tLR: 0.010000\n",
      "5.1800\t4.7180\t4.4803\t4.4729\t4.5800\t4.7506\t\n",
      "Training Epoch: 8 [554/562]\tLoss: 24.2313\tLR: 0.010000\n",
      "4.6431\t4.1720\t3.8518\t3.7445\t3.8042\t4.0157\t\n",
      "Training Epoch: 8 [570/562]\tLoss: 28.7324\tLR: 0.010000\n",
      "5.3299\t4.9157\t4.7036\t4.5806\t4.5631\t4.6396\t\n",
      "Training Epoch: 8 [586/562]\tLoss: 26.2954\tLR: 0.010000\n",
      "4.6602\t4.3269\t4.1929\t4.1923\t4.3419\t4.5812\t\n",
      "Training Epoch: 8 [602/562]\tLoss: 26.5553\tLR: 0.010000\n",
      "4.8237\t4.4373\t4.2423\t4.2191\t4.3139\t4.5190\t\n",
      "Training Epoch: 8 [618/562]\tLoss: 30.2472\tLR: 0.010000\n",
      "5.6546\t5.1600\t4.8168\t4.7082\t4.8400\t5.0676\t\n",
      "Training Epoch: 8 [634/562]\tLoss: 29.5048\tLR: 0.010000\n",
      "5.7282\t5.0686\t4.6301\t4.5110\t4.6391\t4.9278\t\n",
      "Training Epoch: 8 [650/562]\tLoss: 27.9659\tLR: 0.010000\n",
      "4.4583\t4.3177\t4.4081\t4.6416\t4.9267\t5.2134\t\n",
      "Training Epoch: 8 [666/562]\tLoss: 24.4381\tLR: 0.010000\n",
      "4.4817\t4.1235\t3.8518\t3.8046\t3.9662\t4.2102\t\n",
      "Training Epoch: 8 [682/562]\tLoss: 21.2474\tLR: 0.010000\n",
      "3.4353\t3.2509\t3.2492\t3.4594\t3.7801\t4.0724\t\n",
      "Training Epoch: 8 [698/562]\tLoss: 36.1613\tLR: 0.010000\n",
      "7.3015\t6.4625\t5.8801\t5.5773\t5.4765\t5.4634\t\n",
      "Training Epoch: 8 [714/562]\tLoss: 23.9025\tLR: 0.010000\n",
      "4.0280\t3.7490\t3.7098\t3.8714\t4.1140\t4.4302\t\n",
      "Training Epoch: 8 [730/562]\tLoss: 34.8830\tLR: 0.010000\n",
      "7.1268\t6.3440\t5.6969\t5.3049\t5.1817\t5.2287\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [746/562]\tLoss: 33.2687\tLR: 0.010000\n",
      "6.4116\t5.8138\t5.3892\t5.1988\t5.1909\t5.2645\t\n",
      "Training Epoch: 8 [762/562]\tLoss: 24.1224\tLR: 0.010000\n",
      "4.2045\t3.9351\t3.7687\t3.8471\t4.0478\t4.3193\t\n",
      "Training Epoch: 8 [778/562]\tLoss: 21.9349\tLR: 0.010000\n",
      "3.7928\t3.5445\t3.4422\t3.4966\t3.6942\t3.9645\t\n",
      "Training Epoch: 8 [794/562]\tLoss: 26.7359\tLR: 0.010000\n",
      "4.5634\t4.3131\t4.2357\t4.3277\t4.5144\t4.7815\t\n",
      "Training Epoch: 8 [810/562]\tLoss: 36.2483\tLR: 0.010000\n",
      "7.7649\t6.7446\t5.9716\t5.4798\t5.1980\t5.0894\t\n",
      "Training Epoch: 8 [826/562]\tLoss: 30.1367\tLR: 0.010000\n",
      "5.7872\t5.2468\t4.8901\t4.7174\t4.7176\t4.7775\t\n",
      "Training Epoch: 8 [842/562]\tLoss: 26.4467\tLR: 0.010000\n",
      "5.5542\t4.7875\t4.2161\t3.9509\t3.9148\t4.0232\t\n",
      "Training Epoch: 8 [858/562]\tLoss: 19.9227\tLR: 0.010000\n",
      "3.1352\t2.9269\t2.9727\t3.2470\t3.6183\t4.0224\t\n",
      "Training Epoch: 8 [874/562]\tLoss: 21.8271\tLR: 0.010000\n",
      "3.9481\t3.6568\t3.4784\t3.4259\t3.5337\t3.7842\t\n",
      "Training Epoch: 8 [890/562]\tLoss: 35.6824\tLR: 0.010000\n",
      "7.3330\t6.4344\t5.7925\t5.4495\t5.3307\t5.3423\t\n",
      "Training Epoch: 8 [898/562]\tLoss: 34.7553\tLR: 0.010000\n",
      "7.1283\t6.3753\t5.7377\t5.2582\t5.1311\t5.1247\t\n",
      "[0.3059779107570648, 0.49607083201408386, 0.19795125722885132, 0.01954205147922039, 0.703252375125885, 0.07225965708494186, 0.22448796778917313, 0.023548549041152, 0.709110677242279, 0.12609991431236267, 0.16478940844535828, 0.001017839414998889, 0.4400710165500641, 0.27409642934799194, 0.28583255410194397, -0.011714472435414791, 0.23551037907600403, 0.0, 0.764489620923996, -0.006624477915465832]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5690, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5149, Accuracy: 0.1851\n",
      "Test set t = 02: Average loss: 0.4793, Accuracy: 0.1851\n",
      "Test set t = 03: Average loss: 0.4648, Accuracy: 0.1904\n",
      "Test set t = 04: Average loss: 0.4661, Accuracy: 0.1797\n",
      "Test set t = 05: Average loss: 0.4775, Accuracy: 0.1744\n",
      "\n",
      "Training Epoch: 9 [10/562]\tLoss: 23.3784\tLR: 0.010000\n",
      "4.5680\t4.0629\t3.7084\t3.5837\t3.6342\t3.8212\t\n",
      "Training Epoch: 9 [26/562]\tLoss: 29.1384\tLR: 0.010000\n",
      "5.6504\t5.1749\t4.7791\t4.5306\t4.4663\t4.5370\t\n",
      "Training Epoch: 9 [42/562]\tLoss: 24.0422\tLR: 0.010000\n",
      "4.4709\t4.0474\t3.7812\t3.7548\t3.8788\t4.1091\t\n",
      "Training Epoch: 9 [58/562]\tLoss: 39.4309\tLR: 0.010000\n",
      "7.6427\t6.9252\t6.4430\t6.1796\t6.1049\t6.1354\t\n",
      "Training Epoch: 9 [74/562]\tLoss: 28.7060\tLR: 0.010000\n",
      "4.9510\t4.6719\t4.5656\t4.6690\t4.8267\t5.0219\t\n",
      "Training Epoch: 9 [90/562]\tLoss: 29.5932\tLR: 0.010000\n",
      "5.6028\t5.0101\t4.7156\t4.6431\t4.7320\t4.8896\t\n",
      "Training Epoch: 9 [106/562]\tLoss: 26.0574\tLR: 0.010000\n",
      "4.1113\t4.0428\t4.1208\t4.3077\t4.5843\t4.8905\t\n",
      "Training Epoch: 9 [122/562]\tLoss: 30.2768\tLR: 0.010000\n",
      "5.9413\t5.4182\t4.9862\t4.7184\t4.6186\t4.5941\t\n",
      "Training Epoch: 9 [138/562]\tLoss: 26.3563\tLR: 0.010000\n",
      "5.0179\t4.5223\t4.1920\t4.1100\t4.1909\t4.3230\t\n",
      "Training Epoch: 9 [154/562]\tLoss: 28.0416\tLR: 0.010000\n",
      "5.0384\t4.7175\t4.4857\t4.4486\t4.5606\t4.7908\t\n",
      "Training Epoch: 9 [170/562]\tLoss: 31.9127\tLR: 0.010000\n",
      "6.0180\t5.6237\t5.2525\t5.0161\t4.9666\t5.0357\t\n",
      "Training Epoch: 9 [186/562]\tLoss: 29.7754\tLR: 0.010000\n",
      "5.8040\t5.1585\t4.7986\t4.6444\t4.6240\t4.7458\t\n",
      "Training Epoch: 9 [202/562]\tLoss: 23.2999\tLR: 0.010000\n",
      "3.8562\t3.6536\t3.6315\t3.7631\t4.0392\t4.3562\t\n",
      "Training Epoch: 9 [218/562]\tLoss: 25.6473\tLR: 0.010000\n",
      "4.2308\t4.0524\t4.0593\t4.2163\t4.4241\t4.6644\t\n",
      "Training Epoch: 9 [234/562]\tLoss: 26.0690\tLR: 0.010000\n",
      "5.2266\t4.5872\t4.1537\t3.9969\t3.9919\t4.1127\t\n",
      "Training Epoch: 9 [250/562]\tLoss: 33.5398\tLR: 0.010000\n",
      "6.5662\t5.9580\t5.5313\t5.2491\t5.1296\t5.1056\t\n",
      "Training Epoch: 9 [266/562]\tLoss: 35.9134\tLR: 0.010000\n",
      "7.9509\t6.7721\t5.8930\t5.3422\t5.0461\t4.9090\t\n",
      "Training Epoch: 9 [282/562]\tLoss: 25.3181\tLR: 0.010000\n",
      "4.4678\t4.1875\t4.0341\t4.0567\t4.1865\t4.3855\t\n",
      "Training Epoch: 9 [298/562]\tLoss: 23.9324\tLR: 0.010000\n",
      "4.7673\t4.2004\t3.7756\t3.6204\t3.6954\t3.8732\t\n",
      "Training Epoch: 9 [314/562]\tLoss: 32.5154\tLR: 0.010000\n",
      "6.0398\t5.6112\t5.3433\t5.1764\t5.1367\t5.2080\t\n",
      "Training Epoch: 9 [330/562]\tLoss: 30.8348\tLR: 0.010000\n",
      "6.1000\t5.4913\t5.0396\t4.7918\t4.7084\t4.7038\t\n",
      "Training Epoch: 9 [346/562]\tLoss: 34.1058\tLR: 0.010000\n",
      "7.1285\t6.2738\t5.6085\t5.1968\t4.9921\t4.9061\t\n",
      "Training Epoch: 9 [362/562]\tLoss: 27.4564\tLR: 0.010000\n",
      "5.1092\t4.7018\t4.4292\t4.3336\t4.3725\t4.5102\t\n",
      "Training Epoch: 9 [378/562]\tLoss: 26.2337\tLR: 0.010000\n",
      "4.7591\t4.3400\t4.1585\t4.1877\t4.3276\t4.4607\t\n",
      "Training Epoch: 9 [394/562]\tLoss: 27.7660\tLR: 0.010000\n",
      "5.1155\t4.7280\t4.5031\t4.3988\t4.4343\t4.5864\t\n",
      "Training Epoch: 9 [410/562]\tLoss: 37.1911\tLR: 0.010000\n",
      "7.9160\t6.9067\t6.0967\t5.6167\t5.3834\t5.2716\t\n",
      "Training Epoch: 9 [426/562]\tLoss: 26.6987\tLR: 0.010000\n",
      "4.8259\t4.4513\t4.2933\t4.2891\t4.3629\t4.4764\t\n",
      "Training Epoch: 9 [442/562]\tLoss: 24.9394\tLR: 0.010000\n",
      "4.2983\t4.0818\t3.9996\t4.0670\t4.1588\t4.3340\t\n",
      "Training Epoch: 9 [458/562]\tLoss: 29.5831\tLR: 0.010000\n",
      "5.6225\t5.2121\t4.8845\t4.6710\t4.5732\t4.6198\t\n",
      "Training Epoch: 9 [474/562]\tLoss: 27.8869\tLR: 0.010000\n",
      "4.9246\t4.6331\t4.4925\t4.5075\t4.5978\t4.7314\t\n",
      "Training Epoch: 9 [490/562]\tLoss: 19.2692\tLR: 0.010000\n",
      "3.4045\t3.0939\t2.9615\t3.0272\t3.2251\t3.5569\t\n",
      "Training Epoch: 9 [506/562]\tLoss: 30.4985\tLR: 0.010000\n",
      "5.5816\t5.1890\t4.9454\t4.8365\t4.9068\t5.0392\t\n",
      "Training Epoch: 9 [522/562]\tLoss: 26.4840\tLR: 0.010000\n",
      "5.0181\t4.5345\t4.2333\t4.1551\t4.2311\t4.3120\t\n",
      "Training Epoch: 9 [538/562]\tLoss: 37.2722\tLR: 0.010000\n",
      "7.6838\t6.8003\t6.1430\t5.7182\t5.4993\t5.4275\t\n",
      "Training Epoch: 9 [554/562]\tLoss: 24.1009\tLR: 0.010000\n",
      "4.5837\t4.1525\t3.8312\t3.7183\t3.8166\t3.9986\t\n",
      "Training Epoch: 9 [570/562]\tLoss: 26.3911\tLR: 0.010000\n",
      "4.7426\t4.3232\t4.1551\t4.2078\t4.3639\t4.5985\t\n",
      "Training Epoch: 9 [586/562]\tLoss: 34.4700\tLR: 0.010000\n",
      "7.2259\t6.4396\t5.7497\t5.2840\t4.9629\t4.8080\t\n",
      "Training Epoch: 9 [602/562]\tLoss: 31.7678\tLR: 0.010000\n",
      "6.2562\t5.5824\t5.1383\t4.9285\t4.8811\t4.9812\t\n",
      "Training Epoch: 9 [618/562]\tLoss: 32.2078\tLR: 0.010000\n",
      "6.4267\t5.7802\t5.2854\t5.0038\t4.8716\t4.8402\t\n",
      "Training Epoch: 9 [634/562]\tLoss: 35.3804\tLR: 0.010000\n",
      "6.8331\t6.2362\t5.8450\t5.6018\t5.4648\t5.3996\t\n",
      "Training Epoch: 9 [650/562]\tLoss: 26.0013\tLR: 0.010000\n",
      "4.2772\t4.0768\t4.0746\t4.2262\t4.5085\t4.8379\t\n",
      "Training Epoch: 9 [666/562]\tLoss: 34.7589\tLR: 0.010000\n",
      "7.3415\t6.4217\t5.7321\t5.2889\t5.0351\t4.9395\t\n",
      "Training Epoch: 9 [682/562]\tLoss: 24.7297\tLR: 0.010000\n",
      "4.6208\t4.2475\t4.0022\t3.9123\t3.9205\t4.0265\t\n",
      "Training Epoch: 9 [698/562]\tLoss: 29.7122\tLR: 0.010000\n",
      "6.0389\t5.3730\t4.8498\t4.5435\t4.4372\t4.4697\t\n",
      "Training Epoch: 9 [714/562]\tLoss: 30.1764\tLR: 0.010000\n",
      "5.9086\t5.3455\t4.9246\t4.6929\t4.6366\t4.6683\t\n",
      "Training Epoch: 9 [730/562]\tLoss: 42.6026\tLR: 0.010000\n",
      "8.9827\t7.8827\t7.0317\t6.4744\t6.1733\t6.0578\t\n",
      "Training Epoch: 9 [746/562]\tLoss: 31.7392\tLR: 0.010000\n",
      "6.2715\t5.5764\t5.0968\t4.8593\t4.8734\t5.0619\t\n",
      "Training Epoch: 9 [762/562]\tLoss: 33.0634\tLR: 0.010000\n",
      "6.1341\t5.6050\t5.3186\t5.2437\t5.3200\t5.4419\t\n",
      "Training Epoch: 9 [778/562]\tLoss: 18.3175\tLR: 0.010000\n",
      "3.1680\t2.9380\t2.8339\t2.9114\t3.0949\t3.3714\t\n",
      "Training Epoch: 9 [794/562]\tLoss: 34.0490\tLR: 0.010000\n",
      "7.1181\t6.2287\t5.5448\t5.1200\t4.9956\t5.0419\t\n",
      "Training Epoch: 9 [810/562]\tLoss: 26.7499\tLR: 0.010000\n",
      "5.4525\t4.7771\t4.2465\t4.0385\t4.0311\t4.2042\t\n",
      "Training Epoch: 9 [826/562]\tLoss: 22.4279\tLR: 0.010000\n",
      "3.6897\t3.5564\t3.5729\t3.6546\t3.8441\t4.1102\t\n",
      "Training Epoch: 9 [842/562]\tLoss: 31.0582\tLR: 0.010000\n",
      "5.7780\t5.2932\t5.0050\t4.9300\t4.9776\t5.0743\t\n",
      "Training Epoch: 9 [858/562]\tLoss: 23.4259\tLR: 0.010000\n",
      "3.9766\t3.7372\t3.6804\t3.7755\t3.9837\t4.2725\t\n",
      "Training Epoch: 9 [874/562]\tLoss: 32.6061\tLR: 0.010000\n",
      "6.9293\t5.9674\t5.2718\t4.8845\t4.7463\t4.8068\t\n",
      "Training Epoch: 9 [890/562]\tLoss: 28.2225\tLR: 0.010000\n",
      "5.1254\t4.7205\t4.5074\t4.4597\t4.6132\t4.7961\t\n",
      "Training Epoch: 9 [898/562]\tLoss: 34.5357\tLR: 0.010000\n",
      "6.8799\t6.1793\t5.7404\t5.4707\t5.2354\t5.0300\t\n",
      "[0.3038419187068939, 0.4995349943637848, 0.1966230869293213, 0.019637105986475945, 0.7037373781204224, 0.06426515430212021, 0.23199746757745743, 0.024230845272541046, 0.7332451939582825, 0.11884807795286179, 0.14790672808885574, 0.0006116818985901773, 0.44746753573417664, 0.28666356205940247, 0.2658689022064209, -0.013855722732841969, 0.24046866595745087, 0.0, 0.7595313340425491, -0.008684488013386726]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5678, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5116, Accuracy: 0.1851\n",
      "Test set t = 02: Average loss: 0.4761, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4627, Accuracy: 0.1886\n",
      "Test set t = 04: Average loss: 0.4653, Accuracy: 0.1779\n",
      "Test set t = 05: Average loss: 0.4780, Accuracy: 0.1708\n",
      "\n",
      "Training Epoch: 10 [10/562]\tLoss: 25.8544\tLR: 0.010000\n",
      "4.6439\t4.3510\t4.1809\t4.1271\t4.1813\t4.3703\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [26/562]\tLoss: 27.5915\tLR: 0.010000\n",
      "4.9722\t4.5704\t4.3978\t4.3935\t4.5318\t4.7258\t\n",
      "Training Epoch: 10 [42/562]\tLoss: 30.5279\tLR: 0.010000\n",
      "6.2906\t5.4111\t4.8547\t4.6522\t4.6269\t4.6924\t\n",
      "Training Epoch: 10 [58/562]\tLoss: 27.2828\tLR: 0.010000\n",
      "4.5594\t4.2877\t4.2930\t4.4747\t4.7045\t4.9635\t\n",
      "Training Epoch: 10 [74/562]\tLoss: 32.4958\tLR: 0.010000\n",
      "6.2909\t5.5565\t5.1114\t5.0446\t5.1599\t5.3326\t\n",
      "Training Epoch: 10 [90/562]\tLoss: 28.9784\tLR: 0.010000\n",
      "5.3384\t4.9173\t4.6228\t4.5560\t4.6553\t4.8885\t\n",
      "Training Epoch: 10 [106/562]\tLoss: 30.7735\tLR: 0.010000\n",
      "5.9115\t5.3234\t4.9519\t4.8050\t4.8116\t4.9701\t\n",
      "Training Epoch: 10 [122/562]\tLoss: 27.0098\tLR: 0.010000\n",
      "4.7458\t4.4093\t4.2400\t4.3132\t4.5262\t4.7753\t\n",
      "Training Epoch: 10 [138/562]\tLoss: 30.4329\tLR: 0.010000\n",
      "5.6438\t5.2417\t4.9550\t4.8239\t4.8284\t4.9399\t\n",
      "Training Epoch: 10 [154/562]\tLoss: 42.6749\tLR: 0.010000\n",
      "9.2655\t8.0391\t7.0671\t6.3837\t6.0239\t5.8957\t\n",
      "Training Epoch: 10 [170/562]\tLoss: 31.8857\tLR: 0.010000\n",
      "5.9822\t5.4523\t5.1407\t5.0442\t5.0962\t5.1700\t\n",
      "Training Epoch: 10 [186/562]\tLoss: 35.6419\tLR: 0.010000\n",
      "7.3423\t6.4413\t5.7938\t5.4526\t5.3105\t5.3014\t\n",
      "Training Epoch: 10 [202/562]\tLoss: 35.0123\tLR: 0.010000\n",
      "7.0904\t6.3367\t5.7261\t5.3736\t5.2694\t5.2160\t\n",
      "Training Epoch: 10 [218/562]\tLoss: 19.8220\tLR: 0.010000\n",
      "2.9987\t2.9441\t3.0566\t3.2821\t3.5932\t3.9473\t\n",
      "Training Epoch: 10 [234/562]\tLoss: 32.8553\tLR: 0.010000\n",
      "6.6204\t5.8856\t5.3892\t5.1030\t4.9589\t4.8981\t\n",
      "Training Epoch: 10 [250/562]\tLoss: 28.4304\tLR: 0.010000\n",
      "5.3282\t4.8145\t4.5257\t4.4551\t4.5600\t4.7469\t\n",
      "Training Epoch: 10 [266/562]\tLoss: 38.2430\tLR: 0.010000\n",
      "8.4605\t7.1430\t6.1998\t5.6643\t5.4044\t5.3711\t\n",
      "Training Epoch: 10 [282/562]\tLoss: 36.6909\tLR: 0.010000\n",
      "7.5766\t6.6936\t6.0294\t5.5985\t5.4073\t5.3855\t\n",
      "Training Epoch: 10 [298/562]\tLoss: 23.3545\tLR: 0.010000\n",
      "4.9642\t4.2039\t3.6849\t3.4506\t3.4598\t3.5911\t\n",
      "Training Epoch: 10 [314/562]\tLoss: 20.3386\tLR: 0.010000\n",
      "4.0350\t3.4684\t3.1202\t3.0306\t3.1994\t3.4851\t\n",
      "Training Epoch: 10 [330/562]\tLoss: 29.3555\tLR: 0.010000\n",
      "5.6533\t5.1416\t4.7721\t4.5676\t4.5365\t4.6844\t\n",
      "Training Epoch: 10 [346/562]\tLoss: 25.6272\tLR: 0.010000\n",
      "4.8326\t4.3817\t4.1143\t3.9998\t4.0624\t4.2364\t\n",
      "Training Epoch: 10 [362/562]\tLoss: 45.5845\tLR: 0.010000\n",
      "10.3756\t8.8067\t7.5458\t6.7168\t6.2075\t5.9321\t\n",
      "Training Epoch: 10 [378/562]\tLoss: 29.5579\tLR: 0.010000\n",
      "5.8617\t5.1862\t4.7941\t4.5973\t4.5439\t4.5747\t\n",
      "Training Epoch: 10 [394/562]\tLoss: 27.1888\tLR: 0.010000\n",
      "5.3974\t4.7300\t4.3488\t4.1424\t4.1972\t4.3731\t\n",
      "Training Epoch: 10 [410/562]\tLoss: 24.7595\tLR: 0.010000\n",
      "4.0453\t3.8796\t3.8812\t4.0465\t4.3156\t4.5912\t\n",
      "Training Epoch: 10 [426/562]\tLoss: 32.4577\tLR: 0.010000\n",
      "7.5670\t6.3029\t5.3431\t4.6979\t4.3692\t4.1777\t\n",
      "Training Epoch: 10 [442/562]\tLoss: 31.0008\tLR: 0.010000\n",
      "5.1735\t5.0003\t4.9683\t5.0883\t5.2738\t5.4966\t\n",
      "Training Epoch: 10 [458/562]\tLoss: 29.5597\tLR: 0.010000\n",
      "5.2974\t4.8986\t4.7250\t4.7047\t4.8194\t5.1147\t\n",
      "Training Epoch: 10 [474/562]\tLoss: 34.3846\tLR: 0.010000\n",
      "7.2706\t6.1281\t5.4439\t5.1599\t5.1388\t5.2434\t\n",
      "Training Epoch: 10 [490/562]\tLoss: 34.6060\tLR: 0.010000\n",
      "6.6647\t6.0783\t5.6394\t5.4251\t5.3614\t5.4371\t\n",
      "Training Epoch: 10 [506/562]\tLoss: 28.7359\tLR: 0.010000\n",
      "5.3349\t4.8806\t4.6161\t4.5580\t4.6036\t4.7428\t\n",
      "Training Epoch: 10 [522/562]\tLoss: 30.0029\tLR: 0.010000\n",
      "5.2054\t4.8890\t4.7831\t4.8660\t5.0346\t5.2248\t\n",
      "Training Epoch: 10 [538/562]\tLoss: 27.4350\tLR: 0.010000\n",
      "5.4505\t4.7922\t4.3622\t4.2147\t4.2169\t4.3985\t\n",
      "Training Epoch: 10 [554/562]\tLoss: 24.8000\tLR: 0.010000\n",
      "4.2784\t3.9556\t3.8774\t3.9666\t4.2098\t4.5123\t\n",
      "Training Epoch: 10 [570/562]\tLoss: 21.7965\tLR: 0.010000\n",
      "3.9387\t3.5955\t3.4026\t3.4111\t3.5810\t3.8675\t\n",
      "Training Epoch: 10 [586/562]\tLoss: 26.9913\tLR: 0.010000\n",
      "4.7102\t4.3184\t4.1680\t4.3048\t4.5826\t4.9074\t\n",
      "Training Epoch: 10 [602/562]\tLoss: 34.5728\tLR: 0.010000\n",
      "7.0761\t6.2270\t5.6513\t5.3213\t5.1739\t5.1233\t\n",
      "Training Epoch: 10 [618/562]\tLoss: 35.8765\tLR: 0.010000\n",
      "7.3040\t6.3987\t5.7991\t5.5131\t5.4233\t5.4382\t\n",
      "Training Epoch: 10 [634/562]\tLoss: 27.1175\tLR: 0.010000\n",
      "5.0223\t4.5398\t4.2949\t4.2877\t4.4015\t4.5713\t\n",
      "Training Epoch: 10 [650/562]\tLoss: 37.9729\tLR: 0.010000\n",
      "7.5817\t6.7653\t6.1777\t5.8746\t5.7734\t5.8002\t\n",
      "Training Epoch: 10 [666/562]\tLoss: 32.9108\tLR: 0.010000\n",
      "6.7380\t5.9570\t5.3145\t5.0108\t4.9387\t4.9518\t\n",
      "Training Epoch: 10 [682/562]\tLoss: 18.8345\tLR: 0.010000\n",
      "2.7138\t2.6910\t2.8453\t3.1278\t3.5141\t3.9423\t\n",
      "Training Epoch: 10 [698/562]\tLoss: 23.0834\tLR: 0.010000\n",
      "4.2425\t3.8311\t3.6354\t3.6606\t3.7502\t3.9637\t\n",
      "Training Epoch: 10 [714/562]\tLoss: 25.5145\tLR: 0.010000\n",
      "4.8330\t4.3363\t4.0343\t3.9705\t4.0699\t4.2705\t\n",
      "Training Epoch: 10 [730/562]\tLoss: 28.5277\tLR: 0.010000\n",
      "5.0764\t4.7109\t4.5070\t4.5611\t4.7021\t4.9702\t\n",
      "Training Epoch: 10 [746/562]\tLoss: 28.9975\tLR: 0.010000\n",
      "5.4847\t4.9419\t4.6584\t4.5652\t4.6263\t4.7209\t\n",
      "Training Epoch: 10 [762/562]\tLoss: 31.5992\tLR: 0.010000\n",
      "6.1743\t5.4998\t5.1076\t4.9123\t4.9154\t4.9897\t\n",
      "Training Epoch: 10 [778/562]\tLoss: 24.4765\tLR: 0.010000\n",
      "4.0131\t3.8765\t3.9146\t4.0629\t4.2283\t4.3810\t\n",
      "Training Epoch: 10 [794/562]\tLoss: 25.1499\tLR: 0.010000\n",
      "4.6113\t4.1825\t3.9660\t3.9615\t4.1140\t4.3147\t\n",
      "Training Epoch: 10 [810/562]\tLoss: 28.2046\tLR: 0.010000\n",
      "5.0693\t4.6807\t4.4982\t4.4773\t4.6173\t4.8619\t\n",
      "Training Epoch: 10 [826/562]\tLoss: 21.1597\tLR: 0.010000\n",
      "3.6801\t3.4075\t3.2599\t3.3255\t3.5719\t3.9147\t\n",
      "Training Epoch: 10 [842/562]\tLoss: 24.7985\tLR: 0.010000\n",
      "4.1052\t3.8971\t3.8774\t4.0342\t4.3077\t4.5770\t\n",
      "Training Epoch: 10 [858/562]\tLoss: 24.1698\tLR: 0.010000\n",
      "4.2998\t4.0163\t3.8651\t3.8718\t3.9774\t4.1394\t\n",
      "Training Epoch: 10 [874/562]\tLoss: 29.1390\tLR: 0.010000\n",
      "5.1745\t4.8661\t4.7145\t4.6936\t4.7851\t4.9052\t\n",
      "Training Epoch: 10 [890/562]\tLoss: 24.0130\tLR: 0.010000\n",
      "4.6749\t4.0838\t3.7747\t3.7372\t3.8134\t3.9290\t\n",
      "Training Epoch: 10 [898/562]\tLoss: 19.7583\tLR: 0.010000\n",
      "3.3744\t3.2387\t3.2035\t3.1828\t3.2873\t3.4716\t\n",
      "[0.3101666271686554, 0.4931183457374573, 0.19671502709388733, 0.020334912464022636, 0.7018998265266418, 0.056447628885507584, 0.24165254458785057, 0.025262709707021713, 0.7508122324943542, 0.11289268732070923, 0.13629508018493652, 0.0003362849820405245, 0.45228391885757446, 0.2949013411998749, 0.25281473994255066, -0.0160504300147295, 0.23764534294605255, 0.0, 0.7623546570539474, -0.010333491489291191]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5613, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5067, Accuracy: 0.1851\n",
      "Test set t = 02: Average loss: 0.4726, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4601, Accuracy: 0.1886\n",
      "Test set t = 04: Average loss: 0.4632, Accuracy: 0.1833\n",
      "Test set t = 05: Average loss: 0.4758, Accuracy: 0.1708\n",
      "\n",
      "Training Epoch: 11 [10/562]\tLoss: 33.6735\tLR: 0.010000\n",
      "6.9509\t6.0249\t5.3985\t5.1302\t5.0575\t5.1114\t\n",
      "Training Epoch: 11 [26/562]\tLoss: 37.1025\tLR: 0.010000\n",
      "7.3702\t6.6531\t6.1283\t5.7799\t5.6304\t5.5407\t\n",
      "Training Epoch: 11 [42/562]\tLoss: 39.0157\tLR: 0.010000\n",
      "8.2473\t7.1775\t6.3833\t5.9121\t5.7066\t5.5889\t\n",
      "Training Epoch: 11 [58/562]\tLoss: 26.2847\tLR: 0.010000\n",
      "4.5624\t4.2304\t4.1395\t4.2590\t4.4532\t4.6404\t\n",
      "Training Epoch: 11 [74/562]\tLoss: 35.0710\tLR: 0.010000\n",
      "6.7089\t6.0966\t5.6887\t5.5087\t5.4819\t5.5861\t\n",
      "Training Epoch: 11 [90/562]\tLoss: 29.7049\tLR: 0.010000\n",
      "6.0148\t5.2652\t4.7693\t4.5366\t4.4924\t4.6266\t\n",
      "Training Epoch: 11 [106/562]\tLoss: 27.4037\tLR: 0.010000\n",
      "5.0639\t4.6139\t4.3599\t4.3084\t4.4240\t4.6336\t\n",
      "Training Epoch: 11 [122/562]\tLoss: 31.2348\tLR: 0.010000\n",
      "6.1094\t5.5129\t5.1112\t4.8917\t4.8065\t4.8030\t\n",
      "Training Epoch: 11 [138/562]\tLoss: 26.1695\tLR: 0.010000\n",
      "4.3834\t4.2183\t4.1931\t4.2869\t4.4476\t4.6402\t\n",
      "Training Epoch: 11 [154/562]\tLoss: 26.2542\tLR: 0.010000\n",
      "5.0123\t4.5631\t4.2074\t4.0529\t4.1213\t4.2971\t\n",
      "Training Epoch: 11 [170/562]\tLoss: 30.6600\tLR: 0.010000\n",
      "6.0908\t5.4290\t4.9972\t4.7398\t4.6726\t4.7306\t\n",
      "Training Epoch: 11 [186/562]\tLoss: 27.1734\tLR: 0.010000\n",
      "5.0601\t4.5627\t4.2563\t4.2335\t4.4134\t4.6473\t\n",
      "Training Epoch: 11 [202/562]\tLoss: 18.4167\tLR: 0.010000\n",
      "3.5521\t3.2070\t2.9449\t2.8607\t2.8771\t2.9750\t\n",
      "Training Epoch: 11 [218/562]\tLoss: 29.6448\tLR: 0.010000\n",
      "5.8616\t5.2396\t4.7930\t4.5832\t4.5482\t4.6192\t\n",
      "Training Epoch: 11 [234/562]\tLoss: 23.6321\tLR: 0.010000\n",
      "4.5335\t4.0625\t3.7630\t3.6636\t3.7175\t3.8921\t\n",
      "Training Epoch: 11 [250/562]\tLoss: 25.4918\tLR: 0.010000\n",
      "4.2643\t4.1075\t4.0697\t4.1603\t4.3353\t4.5547\t\n",
      "Training Epoch: 11 [266/562]\tLoss: 30.3039\tLR: 0.010000\n",
      "5.9850\t5.3261\t4.8813\t4.6606\t4.6722\t4.7787\t\n",
      "Training Epoch: 11 [282/562]\tLoss: 30.2212\tLR: 0.010000\n",
      "5.4390\t5.0480\t4.9034\t4.8617\t4.9179\t5.0513\t\n",
      "Training Epoch: 11 [298/562]\tLoss: 23.4837\tLR: 0.010000\n",
      "4.4345\t4.0918\t3.8362\t3.7149\t3.6764\t3.7300\t\n",
      "Training Epoch: 11 [314/562]\tLoss: 26.2171\tLR: 0.010000\n",
      "4.8065\t4.4234\t4.1922\t4.1537\t4.2574\t4.3839\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [330/562]\tLoss: 28.2493\tLR: 0.010000\n",
      "5.4546\t4.9073\t4.5377\t4.3803\t4.4052\t4.5643\t\n",
      "Training Epoch: 11 [346/562]\tLoss: 29.9344\tLR: 0.010000\n",
      "5.5525\t5.1123\t4.8340\t4.7427\t4.7841\t4.9088\t\n",
      "Training Epoch: 11 [362/562]\tLoss: 36.0881\tLR: 0.010000\n",
      "7.6804\t6.7123\t5.9795\t5.4674\t5.1830\t5.0654\t\n",
      "Training Epoch: 11 [378/562]\tLoss: 26.6744\tLR: 0.010000\n",
      "4.6509\t4.3494\t4.2121\t4.2604\t4.4689\t4.7326\t\n",
      "Training Epoch: 11 [394/562]\tLoss: 32.8761\tLR: 0.010000\n",
      "6.3767\t5.7669\t5.3388\t5.1174\t5.0936\t5.1827\t\n",
      "Training Epoch: 11 [410/562]\tLoss: 34.3877\tLR: 0.010000\n",
      "7.1156\t6.2946\t5.6673\t5.2734\t5.0493\t4.9875\t\n",
      "Training Epoch: 11 [426/562]\tLoss: 39.9452\tLR: 0.010000\n",
      "8.1821\t7.2188\t6.5452\t6.1667\t5.9704\t5.8619\t\n",
      "Training Epoch: 11 [442/562]\tLoss: 23.6907\tLR: 0.010000\n",
      "4.2574\t3.9110\t3.7636\t3.7451\t3.8843\t4.1293\t\n",
      "Training Epoch: 11 [458/562]\tLoss: 27.7114\tLR: 0.010000\n",
      "5.5416\t4.9334\t4.5200\t4.2978\t4.2056\t4.2130\t\n",
      "Training Epoch: 11 [474/562]\tLoss: 33.3553\tLR: 0.010000\n",
      "6.3672\t5.7859\t5.4560\t5.2663\t5.2023\t5.2775\t\n",
      "Training Epoch: 11 [490/562]\tLoss: 24.7581\tLR: 0.010000\n",
      "4.5123\t4.1607\t3.9719\t3.9299\t4.0261\t4.1572\t\n",
      "Training Epoch: 11 [506/562]\tLoss: 28.7483\tLR: 0.010000\n",
      "5.3525\t4.8506\t4.5750\t4.5131\t4.6044\t4.8528\t\n",
      "Training Epoch: 11 [522/562]\tLoss: 38.6942\tLR: 0.010000\n",
      "8.4729\t7.2929\t6.3597\t5.7749\t5.4645\t5.3293\t\n",
      "Training Epoch: 11 [538/562]\tLoss: 29.2021\tLR: 0.010000\n",
      "5.8845\t5.2199\t4.7536\t4.4920\t4.4042\t4.4478\t\n",
      "Training Epoch: 11 [554/562]\tLoss: 20.4488\tLR: 0.010000\n",
      "3.1141\t3.0307\t3.1525\t3.4043\t3.7196\t4.0277\t\n",
      "Training Epoch: 11 [570/562]\tLoss: 39.1262\tLR: 0.010000\n",
      "7.6797\t6.9060\t6.3802\t6.1290\t6.0133\t6.0180\t\n",
      "Training Epoch: 11 [586/562]\tLoss: 29.2596\tLR: 0.010000\n",
      "5.2678\t4.8964\t4.7361\t4.7267\t4.7638\t4.8689\t\n",
      "Training Epoch: 11 [602/562]\tLoss: 27.1295\tLR: 0.010000\n",
      "5.0849\t4.6136\t4.3257\t4.2449\t4.3304\t4.5299\t\n",
      "Training Epoch: 11 [618/562]\tLoss: 32.2434\tLR: 0.010000\n",
      "6.1938\t5.6258\t5.2631\t5.0606\t5.0093\t5.0908\t\n",
      "Training Epoch: 11 [634/562]\tLoss: 28.3725\tLR: 0.010000\n",
      "4.9099\t4.5408\t4.5001\t4.6174\t4.8093\t4.9949\t\n",
      "Training Epoch: 11 [650/562]\tLoss: 18.8128\tLR: 0.010000\n",
      "3.0757\t2.8868\t2.8493\t2.9877\t3.2865\t3.7267\t\n",
      "Training Epoch: 11 [666/562]\tLoss: 28.0128\tLR: 0.010000\n",
      "5.3264\t4.8795\t4.5934\t4.4088\t4.3542\t4.4504\t\n",
      "Training Epoch: 11 [682/562]\tLoss: 21.4575\tLR: 0.010000\n",
      "3.9103\t3.5527\t3.3598\t3.3874\t3.5295\t3.7178\t\n",
      "Training Epoch: 11 [698/562]\tLoss: 26.6052\tLR: 0.010000\n",
      "4.5690\t4.3393\t4.2706\t4.2752\t4.4175\t4.7336\t\n",
      "Training Epoch: 11 [714/562]\tLoss: 18.7900\tLR: 0.010000\n",
      "3.3948\t3.1163\t2.9593\t2.9609\t3.0742\t3.2845\t\n",
      "Training Epoch: 11 [730/562]\tLoss: 31.3948\tLR: 0.010000\n",
      "6.4923\t5.6523\t5.0626\t4.7810\t4.6869\t4.7196\t\n",
      "Training Epoch: 11 [746/562]\tLoss: 31.0684\tLR: 0.010000\n",
      "6.4911\t5.7161\t5.0480\t4.6743\t4.5419\t4.5972\t\n",
      "Training Epoch: 11 [762/562]\tLoss: 28.5547\tLR: 0.010000\n",
      "5.6087\t4.9927\t4.5891\t4.4273\t4.4357\t4.5011\t\n",
      "Training Epoch: 11 [778/562]\tLoss: 36.5220\tLR: 0.010000\n",
      "7.5922\t6.5876\t5.9231\t5.5640\t5.4219\t5.4333\t\n",
      "Training Epoch: 11 [794/562]\tLoss: 35.2711\tLR: 0.010000\n",
      "6.9209\t6.1522\t5.6698\t5.4914\t5.4939\t5.5430\t\n",
      "Training Epoch: 11 [810/562]\tLoss: 29.0222\tLR: 0.010000\n",
      "5.8564\t5.2506\t4.7723\t4.4995\t4.3514\t4.2919\t\n",
      "Training Epoch: 11 [826/562]\tLoss: 29.9328\tLR: 0.010000\n",
      "5.5718\t5.0942\t4.7931\t4.7293\t4.8029\t4.9416\t\n",
      "Training Epoch: 11 [842/562]\tLoss: 27.2930\tLR: 0.010000\n",
      "5.3528\t4.7596\t4.3567\t4.2050\t4.2227\t4.3961\t\n",
      "Training Epoch: 11 [858/562]\tLoss: 24.9379\tLR: 0.010000\n",
      "4.6960\t4.1955\t3.9775\t3.9311\t3.9945\t4.1434\t\n",
      "Training Epoch: 11 [874/562]\tLoss: 30.4925\tLR: 0.010000\n",
      "5.4580\t5.1236\t4.9299\t4.9029\t4.9781\t5.1000\t\n",
      "Training Epoch: 11 [890/562]\tLoss: 28.2943\tLR: 0.010000\n",
      "4.7199\t4.5933\t4.5620\t4.6406\t4.7857\t4.9928\t\n",
      "Training Epoch: 11 [898/562]\tLoss: 20.8918\tLR: 0.010000\n",
      "2.6450\t2.9280\t3.3095\t3.6579\t4.0144\t4.3370\t\n",
      "[0.31355488300323486, 0.4901634752750397, 0.19628164172172546, 0.02078906074166298, 0.7039846777915955, 0.04972395673394203, 0.2462913654744625, 0.026314683258533478, 0.7721002101898193, 0.10301938652992249, 0.12488040328025818, -0.00034885568311437964, 0.4683171510696411, 0.29212063550949097, 0.23956221342086792, -0.018576787784695625, 0.2310299575328827, 0.0, 0.7689700424671173, -0.012005810625851154]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5749, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5190, Accuracy: 0.1851\n",
      "Test set t = 02: Average loss: 0.4830, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4678, Accuracy: 0.1868\n",
      "Test set t = 04: Average loss: 0.4681, Accuracy: 0.1779\n",
      "Test set t = 05: Average loss: 0.4783, Accuracy: 0.1762\n",
      "\n",
      "Training Epoch: 12 [10/562]\tLoss: 30.5951\tLR: 0.010000\n",
      "5.7257\t5.1822\t4.8722\t4.8105\t4.9069\t5.0976\t\n",
      "Training Epoch: 12 [26/562]\tLoss: 22.2420\tLR: 0.010000\n",
      "3.9570\t3.6174\t3.4936\t3.5436\t3.7115\t3.9190\t\n",
      "Training Epoch: 12 [42/562]\tLoss: 22.8617\tLR: 0.010000\n",
      "4.3846\t3.9657\t3.6969\t3.5803\t3.5745\t3.6596\t\n",
      "Training Epoch: 12 [58/562]\tLoss: 25.9315\tLR: 0.010000\n",
      "4.7400\t4.4542\t4.2604\t4.1323\t4.1147\t4.2300\t\n",
      "Training Epoch: 12 [74/562]\tLoss: 30.8478\tLR: 0.010000\n",
      "6.0419\t5.4069\t5.0051\t4.8361\t4.7709\t4.7871\t\n",
      "Training Epoch: 12 [90/562]\tLoss: 26.8896\tLR: 0.010000\n",
      "5.2782\t4.7159\t4.3397\t4.1704\t4.1432\t4.2422\t\n",
      "Training Epoch: 12 [106/562]\tLoss: 28.3723\tLR: 0.010000\n",
      "5.7241\t5.0114\t4.5414\t4.3390\t4.3227\t4.4337\t\n",
      "Training Epoch: 12 [122/562]\tLoss: 27.0516\tLR: 0.010000\n",
      "4.6621\t4.4270\t4.3663\t4.4006\t4.5244\t4.6713\t\n",
      "Training Epoch: 12 [138/562]\tLoss: 29.4611\tLR: 0.010000\n",
      "5.6509\t5.1521\t4.7671\t4.6109\t4.6043\t4.6758\t\n",
      "Training Epoch: 12 [154/562]\tLoss: 31.3556\tLR: 0.010000\n",
      "5.9847\t5.4161\t5.0609\t4.9303\t4.9651\t4.9984\t\n",
      "Training Epoch: 12 [170/562]\tLoss: 28.0666\tLR: 0.010000\n",
      "4.8881\t4.6428\t4.5105\t4.5285\t4.6527\t4.8441\t\n",
      "Training Epoch: 12 [186/562]\tLoss: 24.5401\tLR: 0.010000\n",
      "3.9259\t3.8719\t3.9220\t4.0718\t4.2664\t4.4821\t\n",
      "Training Epoch: 12 [202/562]\tLoss: 28.9188\tLR: 0.010000\n",
      "5.6280\t5.0944\t4.7116\t4.4791\t4.4488\t4.5569\t\n",
      "Training Epoch: 12 [218/562]\tLoss: 31.1863\tLR: 0.010000\n",
      "6.1433\t5.4858\t5.0518\t4.8402\t4.8031\t4.8621\t\n",
      "Training Epoch: 12 [234/562]\tLoss: 24.3079\tLR: 0.010000\n",
      "4.5364\t4.0667\t3.7964\t3.7834\t3.9422\t4.1829\t\n",
      "Training Epoch: 12 [250/562]\tLoss: 26.9503\tLR: 0.010000\n",
      "4.9789\t4.6137\t4.3623\t4.2661\t4.3059\t4.4233\t\n",
      "Training Epoch: 12 [266/562]\tLoss: 20.9556\tLR: 0.010000\n",
      "4.1766\t3.6717\t3.3334\t3.1942\t3.2050\t3.3747\t\n",
      "Training Epoch: 12 [282/562]\tLoss: 30.4882\tLR: 0.010000\n",
      "6.1134\t5.4729\t4.9781\t4.7313\t4.6079\t4.5847\t\n",
      "Training Epoch: 12 [298/562]\tLoss: 34.7944\tLR: 0.010000\n",
      "7.3370\t6.3713\t5.6904\t5.2880\t5.0872\t5.0204\t\n",
      "Training Epoch: 12 [314/562]\tLoss: 28.5280\tLR: 0.010000\n",
      "5.2261\t4.8596\t4.6400\t4.5717\t4.5790\t4.6517\t\n",
      "Training Epoch: 12 [330/562]\tLoss: 31.4628\tLR: 0.010000\n",
      "6.0220\t5.4618\t5.1086\t4.9281\t4.9160\t5.0262\t\n",
      "Training Epoch: 12 [346/562]\tLoss: 23.0368\tLR: 0.010000\n",
      "4.6188\t4.1059\t3.7414\t3.5145\t3.4747\t3.5815\t\n",
      "Training Epoch: 12 [362/562]\tLoss: 33.2771\tLR: 0.010000\n",
      "6.3534\t5.7516\t5.3813\t5.2371\t5.2426\t5.3110\t\n",
      "Training Epoch: 12 [378/562]\tLoss: 36.9707\tLR: 0.010000\n",
      "7.8180\t6.8567\t6.1268\t5.6466\t5.3296\t5.1930\t\n",
      "Training Epoch: 12 [394/562]\tLoss: 24.2750\tLR: 0.010000\n",
      "4.3400\t4.0512\t3.9280\t3.9241\t3.9723\t4.0594\t\n",
      "Training Epoch: 12 [410/562]\tLoss: 26.9846\tLR: 0.010000\n",
      "4.9597\t4.5567\t4.3174\t4.2856\t4.3645\t4.5007\t\n",
      "Training Epoch: 12 [426/562]\tLoss: 29.2554\tLR: 0.010000\n",
      "5.3042\t4.8884\t4.7023\t4.6570\t4.7668\t4.9366\t\n",
      "Training Epoch: 12 [442/562]\tLoss: 26.4751\tLR: 0.010000\n",
      "5.5057\t4.8616\t4.3593\t4.0378\t3.8746\t3.8361\t\n",
      "Training Epoch: 12 [458/562]\tLoss: 35.4983\tLR: 0.010000\n",
      "6.7895\t6.1385\t5.7842\t5.5966\t5.5650\t5.6245\t\n",
      "Training Epoch: 12 [474/562]\tLoss: 29.5962\tLR: 0.010000\n",
      "5.4630\t5.0938\t4.8429\t4.7209\t4.7399\t4.7358\t\n",
      "Training Epoch: 12 [490/562]\tLoss: 29.6172\tLR: 0.010000\n",
      "5.9062\t5.2299\t4.7866\t4.5831\t4.5384\t4.5729\t\n",
      "Training Epoch: 12 [506/562]\tLoss: 30.6037\tLR: 0.010000\n",
      "5.9332\t5.3212\t4.9167\t4.7455\t4.7768\t4.9103\t\n",
      "Training Epoch: 12 [522/562]\tLoss: 23.6681\tLR: 0.010000\n",
      "4.5090\t4.0675\t3.7751\t3.6902\t3.7445\t3.8818\t\n",
      "Training Epoch: 12 [538/562]\tLoss: 28.1260\tLR: 0.010000\n",
      "4.9847\t4.6664\t4.5075\t4.5121\t4.6353\t4.8200\t\n",
      "Training Epoch: 12 [554/562]\tLoss: 21.6166\tLR: 0.010000\n",
      "3.5298\t3.4033\t3.4156\t3.5191\t3.7309\t4.0179\t\n",
      "Training Epoch: 12 [570/562]\tLoss: 29.3268\tLR: 0.010000\n",
      "6.0650\t5.3584\t4.8037\t4.4354\t4.3073\t4.3572\t\n",
      "Training Epoch: 12 [586/562]\tLoss: 28.3622\tLR: 0.010000\n",
      "5.2123\t4.8160\t4.5492\t4.4866\t4.5485\t4.7497\t\n",
      "Training Epoch: 12 [602/562]\tLoss: 33.1654\tLR: 0.010000\n",
      "6.4942\t5.8265\t5.3556\t5.1692\t5.1258\t5.1941\t\n",
      "Training Epoch: 12 [618/562]\tLoss: 32.0189\tLR: 0.010000\n",
      "6.0182\t5.4971\t5.1940\t5.0879\t5.1003\t5.1215\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [634/562]\tLoss: 31.6013\tLR: 0.010000\n",
      "6.4666\t5.6840\t5.1470\t4.8756\t4.7412\t4.6869\t\n",
      "Training Epoch: 12 [650/562]\tLoss: 29.3454\tLR: 0.010000\n",
      "5.3934\t5.0238\t4.7513\t4.6811\t4.7184\t4.7774\t\n",
      "Training Epoch: 12 [666/562]\tLoss: 33.0450\tLR: 0.010000\n",
      "6.0992\t5.6237\t5.3800\t5.2866\t5.2934\t5.3622\t\n",
      "Training Epoch: 12 [682/562]\tLoss: 27.7980\tLR: 0.010000\n",
      "5.3994\t4.8296\t4.4452\t4.2956\t4.3359\t4.4923\t\n",
      "Training Epoch: 12 [698/562]\tLoss: 29.0023\tLR: 0.010000\n",
      "5.3726\t4.9163\t4.6876\t4.5737\t4.6309\t4.8212\t\n",
      "Training Epoch: 12 [714/562]\tLoss: 31.0492\tLR: 0.010000\n",
      "6.3942\t5.6251\t5.0938\t4.7503\t4.5948\t4.5910\t\n",
      "Training Epoch: 12 [730/562]\tLoss: 31.2630\tLR: 0.010000\n",
      "5.8287\t5.3564\t5.0805\t4.9774\t4.9767\t5.0433\t\n",
      "Training Epoch: 12 [746/562]\tLoss: 27.2117\tLR: 0.010000\n",
      "5.2029\t4.6809\t4.3447\t4.2340\t4.3011\t4.4480\t\n",
      "Training Epoch: 12 [762/562]\tLoss: 30.8206\tLR: 0.010000\n",
      "6.3278\t5.5649\t4.9945\t4.7015\t4.6038\t4.6282\t\n",
      "Training Epoch: 12 [778/562]\tLoss: 22.1090\tLR: 0.010000\n",
      "3.3614\t3.3278\t3.4636\t3.7030\t3.9856\t4.2676\t\n",
      "Training Epoch: 12 [794/562]\tLoss: 31.1337\tLR: 0.010000\n",
      "6.1036\t5.3957\t4.9510\t4.7969\t4.8667\t5.0198\t\n",
      "Training Epoch: 12 [810/562]\tLoss: 34.0254\tLR: 0.010000\n",
      "6.9428\t6.1328\t5.5533\t5.2340\t5.0824\t5.0800\t\n",
      "Training Epoch: 12 [826/562]\tLoss: 33.0073\tLR: 0.010000\n",
      "6.4139\t5.7899\t5.3863\t5.1482\t5.1050\t5.1641\t\n",
      "Training Epoch: 12 [842/562]\tLoss: 35.0025\tLR: 0.010000\n",
      "6.0919\t5.7993\t5.6965\t5.7097\t5.7943\t5.9108\t\n",
      "Training Epoch: 12 [858/562]\tLoss: 34.4571\tLR: 0.010000\n",
      "7.4756\t6.5224\t5.7051\t5.1566\t4.8613\t4.7360\t\n",
      "Training Epoch: 12 [874/562]\tLoss: 31.3845\tLR: 0.010000\n",
      "6.6098\t5.7215\t5.0754\t4.7544\t4.6039\t4.6196\t\n",
      "Training Epoch: 12 [890/562]\tLoss: 32.5943\tLR: 0.010000\n",
      "6.2713\t5.6299\t5.2631\t5.0949\t5.1162\t5.2188\t\n",
      "Training Epoch: 12 [898/562]\tLoss: 27.8238\tLR: 0.010000\n",
      "4.9129\t4.5950\t4.5159\t4.5585\t4.6055\t4.6360\t\n",
      "[0.31798309087753296, 0.4871228039264679, 0.19489410519599915, 0.020901057869195938, 0.707409143447876, 0.04529894143342972, 0.2472919151186943, 0.027789128944277763, 0.7811921834945679, 0.10065864771604538, 0.11814916878938675, -0.0006024897447787225, 0.45924025774002075, 0.31106239557266235, 0.2296973466873169, -0.020419208332896233, 0.2346169501543045, 0.0, 0.7653830498456955, -0.013862330466508865]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5641, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5092, Accuracy: 0.1833\n",
      "Test set t = 02: Average loss: 0.4754, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4625, Accuracy: 0.1922\n",
      "Test set t = 04: Average loss: 0.4645, Accuracy: 0.1868\n",
      "Test set t = 05: Average loss: 0.4757, Accuracy: 0.1762\n",
      "\n",
      "Training Epoch: 13 [10/562]\tLoss: 30.2132\tLR: 0.010000\n",
      "5.8866\t5.2732\t4.9072\t4.7478\t4.6689\t4.7295\t\n",
      "Training Epoch: 13 [26/562]\tLoss: 35.5921\tLR: 0.010000\n",
      "7.5192\t6.6083\t5.8322\t5.3506\t5.1431\t5.1385\t\n",
      "Training Epoch: 13 [42/562]\tLoss: 24.3633\tLR: 0.010000\n",
      "4.1187\t3.9940\t3.9531\t3.9838\t4.0844\t4.2294\t\n",
      "Training Epoch: 13 [58/562]\tLoss: 36.6390\tLR: 0.010000\n",
      "8.1488\t6.8937\t5.9706\t5.4180\t5.1495\t5.0584\t\n",
      "Training Epoch: 13 [74/562]\tLoss: 34.7412\tLR: 0.010000\n",
      "7.1089\t6.3032\t5.7211\t5.3508\t5.1795\t5.0776\t\n",
      "Training Epoch: 13 [90/562]\tLoss: 32.5348\tLR: 0.010000\n",
      "5.8202\t5.3574\t5.1756\t5.1831\t5.3521\t5.6464\t\n",
      "Training Epoch: 13 [106/562]\tLoss: 36.0023\tLR: 0.010000\n",
      "7.2946\t6.4084\t5.8423\t5.5489\t5.4640\t5.4441\t\n",
      "Training Epoch: 13 [122/562]\tLoss: 26.5316\tLR: 0.010000\n",
      "4.8286\t4.3877\t4.1989\t4.2101\t4.3376\t4.5688\t\n",
      "Training Epoch: 13 [138/562]\tLoss: 32.0362\tLR: 0.010000\n",
      "5.7195\t5.3302\t5.1432\t5.1434\t5.2640\t5.4359\t\n",
      "Training Epoch: 13 [154/562]\tLoss: 44.1652\tLR: 0.010000\n",
      "9.5481\t8.2616\t7.2660\t6.6404\t6.3177\t6.1314\t\n",
      "Training Epoch: 13 [170/562]\tLoss: 32.6486\tLR: 0.010000\n",
      "6.2132\t5.6783\t5.2817\t5.1112\t5.1154\t5.2487\t\n",
      "Training Epoch: 13 [186/562]\tLoss: 29.7661\tLR: 0.010000\n",
      "5.6483\t5.0598\t4.7239\t4.6380\t4.7509\t4.9451\t\n",
      "Training Epoch: 13 [202/562]\tLoss: 26.6220\tLR: 0.010000\n",
      "5.0041\t4.4547\t4.1448\t4.1274\t4.3192\t4.5718\t\n",
      "Training Epoch: 13 [218/562]\tLoss: 29.0416\tLR: 0.010000\n",
      "5.6578\t5.0673\t4.7244\t4.5519\t4.5016\t4.5386\t\n",
      "Training Epoch: 13 [234/562]\tLoss: 20.2985\tLR: 0.010000\n",
      "3.6261\t3.2353\t3.1262\t3.1753\t3.3989\t3.7367\t\n",
      "Training Epoch: 13 [250/562]\tLoss: 27.1501\tLR: 0.010000\n",
      "5.2211\t4.7077\t4.3296\t4.2033\t4.2645\t4.4239\t\n",
      "Training Epoch: 13 [266/562]\tLoss: 33.2275\tLR: 0.010000\n",
      "6.0316\t5.5882\t5.3726\t5.3399\t5.3861\t5.5091\t\n",
      "Training Epoch: 13 [282/562]\tLoss: 28.9673\tLR: 0.010000\n",
      "4.7236\t4.5500\t4.6275\t4.7809\t5.0242\t5.2610\t\n",
      "Training Epoch: 13 [298/562]\tLoss: 33.6922\tLR: 0.010000\n",
      "6.9676\t6.1195\t5.5481\t5.1884\t4.9693\t4.8993\t\n",
      "Training Epoch: 13 [314/562]\tLoss: 28.5895\tLR: 0.010000\n",
      "5.7030\t5.0342\t4.6310\t4.4322\t4.3624\t4.4265\t\n",
      "Training Epoch: 13 [330/562]\tLoss: 24.6543\tLR: 0.010000\n",
      "4.1977\t4.0014\t3.8978\t3.9835\t4.1734\t4.4005\t\n",
      "Training Epoch: 13 [346/562]\tLoss: 33.5117\tLR: 0.010000\n",
      "6.4782\t5.8126\t5.4377\t5.2942\t5.2407\t5.2483\t\n",
      "Training Epoch: 13 [362/562]\tLoss: 22.3107\tLR: 0.010000\n",
      "3.6238\t3.4491\t3.4820\t3.6562\t3.9111\t4.1886\t\n",
      "Training Epoch: 13 [378/562]\tLoss: 30.8812\tLR: 0.010000\n",
      "6.1133\t5.3259\t4.8957\t4.7708\t4.8081\t4.9674\t\n",
      "Training Epoch: 13 [394/562]\tLoss: 26.3827\tLR: 0.010000\n",
      "4.9360\t4.4939\t4.2214\t4.1567\t4.2211\t4.3536\t\n",
      "Training Epoch: 13 [410/562]\tLoss: 20.7446\tLR: 0.010000\n",
      "3.6179\t3.2017\t3.0911\t3.2783\t3.6040\t3.9515\t\n",
      "Training Epoch: 13 [426/562]\tLoss: 28.1640\tLR: 0.010000\n",
      "5.6174\t4.8171\t4.4036\t4.3239\t4.4295\t4.5724\t\n",
      "Training Epoch: 13 [442/562]\tLoss: 25.1626\tLR: 0.010000\n",
      "5.1530\t4.4642\t4.0026\t3.8342\t3.8166\t3.8920\t\n",
      "Training Epoch: 13 [458/562]\tLoss: 27.0450\tLR: 0.010000\n",
      "5.0070\t4.5979\t4.4075\t4.3147\t4.3235\t4.3943\t\n",
      "Training Epoch: 13 [474/562]\tLoss: 21.5946\tLR: 0.010000\n",
      "4.1698\t3.6982\t3.4405\t3.3289\t3.4056\t3.5517\t\n",
      "Training Epoch: 13 [490/562]\tLoss: 24.0374\tLR: 0.010000\n",
      "4.6113\t3.9875\t3.6736\t3.6653\t3.8878\t4.2119\t\n",
      "Training Epoch: 13 [506/562]\tLoss: 21.6861\tLR: 0.010000\n",
      "4.1363\t3.7204\t3.4237\t3.3250\t3.4304\t3.6502\t\n",
      "Training Epoch: 13 [522/562]\tLoss: 31.8222\tLR: 0.010000\n",
      "6.3221\t5.6413\t5.1656\t4.9136\t4.8484\t4.9312\t\n",
      "Training Epoch: 13 [538/562]\tLoss: 35.9313\tLR: 0.010000\n",
      "6.6933\t6.1692\t5.8175\t5.7070\t5.7312\t5.8131\t\n",
      "Training Epoch: 13 [554/562]\tLoss: 38.6602\tLR: 0.010000\n",
      "8.0393\t7.0333\t6.2870\t5.9115\t5.7299\t5.6592\t\n",
      "Training Epoch: 13 [570/562]\tLoss: 28.0341\tLR: 0.010000\n",
      "5.1078\t4.6593\t4.4680\t4.4910\t4.5806\t4.7273\t\n",
      "Training Epoch: 13 [586/562]\tLoss: 32.3317\tLR: 0.010000\n",
      "6.4773\t5.7425\t5.2428\t4.9941\t4.9228\t4.9522\t\n",
      "Training Epoch: 13 [602/562]\tLoss: 24.7669\tLR: 0.010000\n",
      "4.2072\t3.9039\t3.8798\t4.0133\t4.2343\t4.5284\t\n",
      "Training Epoch: 13 [618/562]\tLoss: 28.7022\tLR: 0.010000\n",
      "5.5801\t5.0351\t4.6312\t4.4362\t4.4460\t4.5735\t\n",
      "Training Epoch: 13 [634/562]\tLoss: 26.8405\tLR: 0.010000\n",
      "5.0416\t4.5177\t4.2299\t4.1808\t4.3177\t4.5527\t\n",
      "Training Epoch: 13 [650/562]\tLoss: 23.8093\tLR: 0.010000\n",
      "4.0273\t3.8262\t3.7497\t3.8474\t4.0495\t4.3092\t\n",
      "Training Epoch: 13 [666/562]\tLoss: 28.6506\tLR: 0.010000\n",
      "5.0364\t4.7233\t4.6029\t4.6254\t4.7517\t4.9109\t\n",
      "Training Epoch: 13 [682/562]\tLoss: 27.6971\tLR: 0.010000\n",
      "5.5695\t4.8144\t4.3358\t4.1904\t4.2751\t4.5120\t\n",
      "Training Epoch: 13 [698/562]\tLoss: 33.1523\tLR: 0.010000\n",
      "6.5032\t5.8927\t5.4695\t5.1986\t5.0610\t5.0274\t\n",
      "Training Epoch: 13 [714/562]\tLoss: 20.7872\tLR: 0.010000\n",
      "3.1746\t3.0777\t3.1933\t3.4541\t3.7809\t4.1067\t\n",
      "Training Epoch: 13 [730/562]\tLoss: 28.9452\tLR: 0.010000\n",
      "5.5353\t4.9581\t4.6666\t4.5411\t4.5377\t4.7062\t\n",
      "Training Epoch: 13 [746/562]\tLoss: 31.3133\tLR: 0.010000\n",
      "6.2136\t5.5368\t5.0818\t4.8860\t4.8031\t4.7920\t\n",
      "Training Epoch: 13 [762/562]\tLoss: 33.3387\tLR: 0.010000\n",
      "6.5517\t5.8944\t5.4376\t5.1948\t5.1140\t5.1461\t\n",
      "Training Epoch: 13 [778/562]\tLoss: 28.1620\tLR: 0.010000\n",
      "5.6675\t4.9784\t4.5364\t4.3516\t4.2853\t4.3428\t\n",
      "Training Epoch: 13 [794/562]\tLoss: 29.9932\tLR: 0.010000\n",
      "5.8206\t5.2432\t4.8381\t4.6513\t4.6567\t4.7832\t\n",
      "Training Epoch: 13 [810/562]\tLoss: 29.2831\tLR: 0.010000\n",
      "5.6229\t5.1161\t4.7255\t4.5681\t4.5751\t4.6754\t\n",
      "Training Epoch: 13 [826/562]\tLoss: 45.6672\tLR: 0.010000\n",
      "9.6159\t8.5161\t7.5832\t6.9495\t6.5871\t6.4154\t\n",
      "Training Epoch: 13 [842/562]\tLoss: 29.3442\tLR: 0.010000\n",
      "5.4433\t4.9267\t4.7301\t4.6865\t4.7224\t4.8351\t\n",
      "Training Epoch: 13 [858/562]\tLoss: 18.1855\tLR: 0.010000\n",
      "3.4650\t3.1032\t2.8804\t2.8030\t2.8800\t3.0538\t\n",
      "Training Epoch: 13 [874/562]\tLoss: 27.7237\tLR: 0.010000\n",
      "5.2349\t4.5804\t4.2923\t4.3190\t4.5241\t4.7730\t\n",
      "Training Epoch: 13 [890/562]\tLoss: 23.1664\tLR: 0.010000\n",
      "3.7971\t3.6673\t3.6975\t3.8269\t4.0092\t4.1684\t\n",
      "Training Epoch: 13 [898/562]\tLoss: 13.9905\tLR: 0.010000\n",
      "2.3401\t2.1898\t2.0619\t2.1677\t2.3914\t2.8397\t\n",
      "[0.32177454233169556, 0.48360663652420044, 0.194618821144104, 0.02124088443815708, 0.7104570269584656, 0.04023706912994385, 0.24930590391159058, 0.029300235211849213, 0.7940601110458374, 0.09571048617362976, 0.11022940278053284, -0.001211340306326747, 0.4641517400741577, 0.3188547194004059, 0.2169935405254364, -0.022777339443564415, 0.23312018811702728, 0.0, 0.7668798118829727, -0.01532832719385624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [666/562]\tLoss: 27.0547\tLR: 0.010000\n",
      "4.4151\t4.1715\t4.1819\t4.4396\t4.7633\t5.0833\t\n",
      "Training Epoch: 15 [682/562]\tLoss: 26.6154\tLR: 0.010000\n",
      "4.5894\t4.2951\t4.2109\t4.3036\t4.4962\t4.7201\t\n",
      "Training Epoch: 15 [698/562]\tLoss: 26.2850\tLR: 0.010000\n",
      "4.3268\t4.1127\t4.1460\t4.3226\t4.5649\t4.8120\t\n",
      "Training Epoch: 15 [714/562]\tLoss: 26.3375\tLR: 0.010000\n",
      "5.4516\t4.6578\t4.1883\t3.9665\t3.9624\t4.1109\t\n",
      "Training Epoch: 15 [730/562]\tLoss: 28.7731\tLR: 0.010000\n",
      "5.2093\t4.7469\t4.5620\t4.5662\t4.7180\t4.9707\t\n",
      "Training Epoch: 15 [746/562]\tLoss: 28.0143\tLR: 0.010000\n",
      "5.2095\t4.6456\t4.4003\t4.4211\t4.5799\t4.7579\t\n",
      "Training Epoch: 15 [762/562]\tLoss: 32.6174\tLR: 0.010000\n",
      "6.4670\t5.7644\t5.2684\t5.0359\t4.9999\t5.0818\t\n",
      "Training Epoch: 15 [778/562]\tLoss: 23.6212\tLR: 0.010000\n",
      "3.9368\t3.7401\t3.7660\t3.8801\t4.0377\t4.2604\t\n",
      "Training Epoch: 15 [794/562]\tLoss: 25.9363\tLR: 0.010000\n",
      "4.4815\t4.1835\t4.1307\t4.2286\t4.3611\t4.5509\t\n",
      "Training Epoch: 15 [810/562]\tLoss: 30.3037\tLR: 0.010000\n",
      "5.8475\t5.2713\t4.9463\t4.7818\t4.7123\t4.7446\t\n",
      "Training Epoch: 15 [826/562]\tLoss: 37.7166\tLR: 0.010000\n",
      "7.7670\t6.8728\t6.1571\t5.7734\t5.5958\t5.5505\t\n",
      "Training Epoch: 15 [842/562]\tLoss: 26.1283\tLR: 0.010000\n",
      "4.5891\t4.2543\t4.1331\t4.1959\t4.3545\t4.6013\t\n",
      "Training Epoch: 15 [858/562]\tLoss: 31.2805\tLR: 0.010000\n",
      "5.6154\t5.2334\t5.0575\t5.0142\t5.1030\t5.2571\t\n",
      "Training Epoch: 15 [874/562]\tLoss: 25.2979\tLR: 0.010000\n",
      "4.5971\t4.1270\t3.9576\t4.0238\t4.2065\t4.3859\t\n",
      "Training Epoch: 15 [890/562]\tLoss: 20.8529\tLR: 0.010000\n",
      "3.5377\t3.3993\t3.3452\t3.3884\t3.5088\t3.6735\t\n",
      "Training Epoch: 15 [898/562]\tLoss: 26.0987\tLR: 0.010000\n",
      "4.2487\t4.0363\t4.0986\t4.3398\t4.5760\t4.7993\t\n",
      "[0.32842788100242615, 0.4799196422100067, 0.19165247678756714, 0.022304411977529526, 0.7102368474006653, 0.03225331753492355, 0.25750983506441116, 0.03283636271953583, 0.8213115334510803, 0.08254188299179077, 0.0961465835571289, -0.0021127888467162848, 0.4798023998737335, 0.32964566349983215, 0.19055193662643433, -0.02728748694062233, 0.23612286150455475, 0.0, 0.7638771384954453, -0.018602928146719933]\n",
      "\n",
      "Test set t = 00: Average loss: 0.5651, Accuracy: 0.1815\n",
      "Test set t = 01: Average loss: 0.5083, Accuracy: 0.1833\n",
      "Test set t = 02: Average loss: 0.4744, Accuracy: 0.1868\n",
      "Test set t = 03: Average loss: 0.4617, Accuracy: 0.1904\n",
      "Test set t = 04: Average loss: 0.4638, Accuracy: 0.1868\n",
      "Test set t = 05: Average loss: 0.4746, Accuracy: 0.1797\n",
      "\n",
      "=====================\n",
      "Babble2Spkr, for SNR 0.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([5, 164, 400])) that is different to the input size (torch.Size([5, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.3908, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3592, Accuracy: 0.3398\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3921, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3604, Accuracy: 0.3398\n",
      "Test set t = 02: Average loss: 0.3469, Accuracy: 0.3239\n",
      "Test set t = 03: Average loss: 0.3618, Accuracy: 0.2903\n",
      "Test set t = 04: Average loss: 0.4030, Accuracy: 0.2372\n",
      "Test set t = 05: Average loss: 0.4542, Accuracy: 0.1593\n",
      "\n",
      "Training Epoch: 1 [10/565]\tLoss: 27.8052\tLR: 0.010000\n",
      "4.3430\t4.2096\t4.2002\t4.4496\t4.9750\t5.6280\t\n",
      "Training Epoch: 1 [26/565]\tLoss: 22.4793\tLR: 0.010000\n",
      "3.7279\t3.5017\t3.4156\t3.5899\t3.9298\t4.3145\t\n",
      "Training Epoch: 1 [42/565]\tLoss: 24.5507\tLR: 0.010000\n",
      "4.3275\t3.8995\t3.7422\t3.8800\t4.1560\t4.5455\t\n",
      "Training Epoch: 1 [58/565]\tLoss: 31.5904\tLR: 0.010000\n",
      "5.8019\t5.3853\t5.0345\t4.9319\t5.0632\t5.3736\t\n",
      "Training Epoch: 1 [74/565]\tLoss: 20.9619\tLR: 0.010000\n",
      "3.9252\t3.5674\t3.2619\t3.1404\t3.3155\t3.7514\t\n",
      "Training Epoch: 1 [90/565]\tLoss: 11.9939\tLR: 0.010000\n",
      "1.0534\t1.1863\t1.4368\t1.9200\t2.7403\t3.6572\t\n",
      "Training Epoch: 1 [106/565]\tLoss: 13.5443\tLR: 0.010000\n",
      "1.9724\t1.8163\t1.8493\t2.1363\t2.5869\t3.1831\t\n",
      "Training Epoch: 1 [122/565]\tLoss: 23.0651\tLR: 0.010000\n",
      "3.8758\t3.4424\t3.2977\t3.5318\t4.1038\t4.8135\t\n",
      "Training Epoch: 1 [138/565]\tLoss: 20.5868\tLR: 0.010000\n",
      "3.6859\t3.3141\t3.0619\t3.0571\t3.4476\t4.0202\t\n",
      "Training Epoch: 1 [154/565]\tLoss: 17.8130\tLR: 0.010000\n",
      "2.7898\t2.7673\t2.7639\t2.8644\t3.1283\t3.4993\t\n",
      "Training Epoch: 1 [170/565]\tLoss: 25.7349\tLR: 0.010000\n",
      "4.5271\t4.1025\t3.8854\t4.0042\t4.3701\t4.8457\t\n",
      "Training Epoch: 1 [186/565]\tLoss: 32.8700\tLR: 0.010000\n",
      "7.0344\t6.1507\t5.3174\t4.8028\t4.7023\t4.8624\t\n",
      "Training Epoch: 1 [202/565]\tLoss: 27.3341\tLR: 0.010000\n",
      "4.5375\t4.3794\t4.3606\t4.3706\t4.5900\t5.0959\t\n",
      "Training Epoch: 1 [218/565]\tLoss: 27.1295\tLR: 0.010000\n",
      "5.1240\t4.7851\t4.4099\t4.2080\t4.2351\t4.3675\t\n",
      "Training Epoch: 1 [234/565]\tLoss: 21.1561\tLR: 0.010000\n",
      "3.1112\t3.1270\t3.2419\t3.5074\t3.8880\t4.2807\t\n",
      "Training Epoch: 1 [250/565]\tLoss: 19.2737\tLR: 0.010000\n",
      "3.0875\t2.9269\t2.8586\t3.0288\t3.4113\t3.9606\t\n",
      "Training Epoch: 1 [266/565]\tLoss: 16.3126\tLR: 0.010000\n",
      "2.3714\t2.4391\t2.5459\t2.7151\t2.9601\t3.2809\t\n",
      "Training Epoch: 1 [282/565]\tLoss: 18.6667\tLR: 0.010000\n",
      "3.1645\t2.9823\t2.8210\t2.8426\t3.1661\t3.6902\t\n",
      "Training Epoch: 1 [298/565]\tLoss: 23.2261\tLR: 0.010000\n",
      "4.1141\t3.8076\t3.6070\t3.5996\t3.8455\t4.2523\t\n",
      "Training Epoch: 1 [314/565]\tLoss: 23.6397\tLR: 0.010000\n",
      "4.6328\t4.1265\t3.7514\t3.5619\t3.6314\t3.9357\t\n",
      "Training Epoch: 1 [330/565]\tLoss: 19.3995\tLR: 0.010000\n",
      "3.2394\t3.0092\t2.8887\t3.0196\t3.3948\t3.8479\t\n",
      "Training Epoch: 1 [346/565]\tLoss: 22.6419\tLR: 0.010000\n",
      "3.8842\t3.5434\t3.4573\t3.6017\t3.8773\t4.2780\t\n",
      "Training Epoch: 1 [362/565]\tLoss: 21.8658\tLR: 0.010000\n",
      "4.0677\t3.8076\t3.6059\t3.4793\t3.3929\t3.5124\t\n",
      "Training Epoch: 1 [378/565]\tLoss: 21.9505\tLR: 0.010000\n",
      "3.6436\t3.5034\t3.4516\t3.5566\t3.7787\t4.0166\t\n",
      "Training Epoch: 1 [394/565]\tLoss: 21.4541\tLR: 0.010000\n",
      "3.4963\t3.2773\t3.2846\t3.4965\t3.8073\t4.0921\t\n",
      "Training Epoch: 1 [410/565]\tLoss: 22.9813\tLR: 0.010000\n",
      "4.0533\t3.7681\t3.6015\t3.6210\t3.8156\t4.1218\t\n",
      "Training Epoch: 1 [426/565]\tLoss: 24.7365\tLR: 0.010000\n",
      "4.3834\t4.1161\t3.9317\t3.9150\t4.0749\t4.3154\t\n",
      "Training Epoch: 1 [442/565]\tLoss: 22.5252\tLR: 0.010000\n",
      "4.1270\t3.9038\t3.6538\t3.5292\t3.5409\t3.7705\t\n",
      "Training Epoch: 1 [458/565]\tLoss: 18.3349\tLR: 0.010000\n",
      "2.9125\t2.7931\t2.8058\t2.9610\t3.2406\t3.6218\t\n",
      "Training Epoch: 1 [474/565]\tLoss: 20.5763\tLR: 0.010000\n",
      "3.2698\t3.2295\t3.2405\t3.3898\t3.6150\t3.8316\t\n",
      "Training Epoch: 1 [490/565]\tLoss: 21.5886\tLR: 0.010000\n",
      "3.9013\t3.6835\t3.4247\t3.3125\t3.4580\t3.8087\t\n",
      "Training Epoch: 1 [506/565]\tLoss: 21.4759\tLR: 0.010000\n",
      "3.7035\t3.4879\t3.3743\t3.4293\t3.6166\t3.8644\t\n",
      "Training Epoch: 1 [522/565]\tLoss: 14.2718\tLR: 0.010000\n",
      "2.4503\t2.3059\t2.2114\t2.2436\t2.3976\t2.6630\t\n",
      "Training Epoch: 1 [538/565]\tLoss: 22.5219\tLR: 0.010000\n",
      "4.2575\t3.8994\t3.5903\t3.4474\t3.5340\t3.7934\t\n",
      "Training Epoch: 1 [554/565]\tLoss: 17.4134\tLR: 0.010000\n",
      "2.3823\t2.5041\t2.6521\t2.8633\t3.2583\t3.7533\t\n",
      "Training Epoch: 1 [570/565]\tLoss: 23.7452\tLR: 0.010000\n",
      "4.0226\t3.9122\t3.8065\t3.8675\t3.9782\t4.1582\t\n",
      "Training Epoch: 1 [586/565]\tLoss: 14.3909\tLR: 0.010000\n",
      "2.5997\t2.3907\t2.2389\t2.2073\t2.3328\t2.6215\t\n",
      "Training Epoch: 1 [602/565]\tLoss: 23.2633\tLR: 0.010000\n",
      "3.7548\t3.7102\t3.7167\t3.8072\t4.0187\t4.2558\t\n",
      "Training Epoch: 1 [618/565]\tLoss: 26.2443\tLR: 0.010000\n",
      "4.8657\t4.5799\t4.3115\t4.1602\t4.1175\t4.2095\t\n",
      "Training Epoch: 1 [634/565]\tLoss: 23.0862\tLR: 0.010000\n",
      "4.5277\t4.1251\t3.8062\t3.5977\t3.5176\t3.5120\t\n",
      "Training Epoch: 1 [650/565]\tLoss: 20.6337\tLR: 0.010000\n",
      "3.7840\t3.6052\t3.4173\t3.2791\t3.2446\t3.3034\t\n",
      "Training Epoch: 1 [666/565]\tLoss: 23.2564\tLR: 0.010000\n",
      "3.9798\t3.8333\t3.7311\t3.7715\t3.8911\t4.0495\t\n",
      "Training Epoch: 1 [682/565]\tLoss: 23.1017\tLR: 0.010000\n",
      "4.2807\t4.0112\t3.7820\t3.6695\t3.6415\t3.7168\t\n",
      "Training Epoch: 1 [698/565]\tLoss: 24.9003\tLR: 0.010000\n",
      "4.4262\t4.2402\t4.0699\t3.9722\t4.0028\t4.1890\t\n",
      "Training Epoch: 1 [714/565]\tLoss: 32.8310\tLR: 0.010000\n",
      "6.7722\t6.1048\t5.4619\t5.0252\t4.7754\t4.6915\t\n",
      "Training Epoch: 1 [730/565]\tLoss: 17.0323\tLR: 0.010000\n",
      "2.8617\t2.7562\t2.6946\t2.7511\t2.8737\t3.0951\t\n",
      "Training Epoch: 1 [746/565]\tLoss: 19.7745\tLR: 0.010000\n",
      "3.8606\t3.5502\t3.2916\t3.1047\t2.9943\t2.9731\t\n",
      "Training Epoch: 1 [762/565]\tLoss: 18.9667\tLR: 0.010000\n",
      "3.2943\t3.1717\t3.0342\t3.0300\t3.1413\t3.2951\t\n",
      "Training Epoch: 1 [778/565]\tLoss: 23.4897\tLR: 0.010000\n",
      "4.3865\t4.1230\t3.8514\t3.6683\t3.6482\t3.8124\t\n",
      "Training Epoch: 1 [794/565]\tLoss: 24.4754\tLR: 0.010000\n",
      "4.1858\t3.9806\t3.8872\t3.9422\t4.1286\t4.3511\t\n",
      "Training Epoch: 1 [810/565]\tLoss: 19.4703\tLR: 0.010000\n",
      "3.3930\t3.2120\t3.1228\t3.1313\t3.2228\t3.3884\t\n",
      "Training Epoch: 1 [826/565]\tLoss: 25.6070\tLR: 0.010000\n",
      "4.8755\t4.6305\t4.2687\t4.0022\t3.8869\t3.9432\t\n",
      "Training Epoch: 1 [842/565]\tLoss: 17.9945\tLR: 0.010000\n",
      "3.5966\t3.3275\t2.9860\t2.7480\t2.6399\t2.6965\t\n",
      "Training Epoch: 1 [858/565]\tLoss: 34.2586\tLR: 0.010000\n",
      "7.0302\t6.4376\t5.7781\t5.2863\t4.9377\t4.7887\t\n",
      "Training Epoch: 1 [874/565]\tLoss: 13.8995\tLR: 0.010000\n",
      "2.2380\t2.1740\t2.1354\t2.2345\t2.4174\t2.7002\t\n",
      "Training Epoch: 1 [890/565]\tLoss: 17.0476\tLR: 0.010000\n",
      "2.8597\t2.7655\t2.7425\t2.7578\t2.8722\t3.0499\t\n",
      "Training Epoch: 1 [901/565]\tLoss: 28.6099\tLR: 0.010000\n",
      "5.5770\t5.0978\t4.6862\t4.4452\t4.3800\t4.4237\t\n",
      "[0.3464190661907196, 0.23655788600444794, 0.41702304780483246, 0.013232105411589146, 0.3670597970485687, 0.21800410747528076, 0.4149360954761505, 0.011483052745461464, 0.3526914119720459, 0.2153513878583908, 0.4319572001695633, 0.005882661323994398, 0.32077550888061523, 0.2263757586479187, 0.45284873247146606, 0.005749148316681385, 0.2090875804424286, 0.0, 0.7909124195575714, 0.007410413585603237]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3929, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3731, Accuracy: 0.3363\n",
      "Test set t = 02: Average loss: 0.3551, Accuracy: 0.3416\n",
      "Test set t = 03: Average loss: 0.3474, Accuracy: 0.3239\n",
      "Test set t = 04: Average loss: 0.3504, Accuracy: 0.3133\n",
      "Test set t = 05: Average loss: 0.3635, Accuracy: 0.3009\n",
      "\n",
      "Training Epoch: 2 [10/565]\tLoss: 26.9006\tLR: 0.010000\n",
      "5.1322\t4.7831\t4.4542\t4.2383\t4.1332\t4.1597\t\n",
      "Training Epoch: 2 [26/565]\tLoss: 12.0014\tLR: 0.010000\n",
      "1.8896\t1.8381\t1.8314\t1.9122\t2.1218\t2.4082\t\n",
      "Training Epoch: 2 [42/565]\tLoss: 24.2726\tLR: 0.010000\n",
      "4.8626\t4.5492\t4.1340\t3.7715\t3.5261\t3.4291\t\n",
      "Training Epoch: 2 [58/565]\tLoss: 22.7165\tLR: 0.010000\n",
      "4.2802\t4.0667\t3.7764\t3.5729\t3.4797\t3.5405\t\n",
      "Training Epoch: 2 [74/565]\tLoss: 26.8654\tLR: 0.010000\n",
      "5.2025\t4.7564\t4.3753\t4.1989\t4.1463\t4.1860\t\n",
      "Training Epoch: 2 [90/565]\tLoss: 16.0601\tLR: 0.010000\n",
      "2.9230\t2.7234\t2.4975\t2.4369\t2.5979\t2.8814\t\n",
      "Training Epoch: 2 [106/565]\tLoss: 24.6339\tLR: 0.010000\n",
      "4.4396\t4.1893\t3.9723\t3.9024\t3.9663\t4.1641\t\n",
      "Training Epoch: 2 [122/565]\tLoss: 22.7247\tLR: 0.010000\n",
      "3.9993\t3.8767\t3.7670\t3.6983\t3.6506\t3.7330\t\n",
      "Training Epoch: 2 [138/565]\tLoss: 21.3550\tLR: 0.010000\n",
      "3.7714\t3.5641\t3.4303\t3.4053\t3.4973\t3.6867\t\n",
      "Training Epoch: 2 [154/565]\tLoss: 26.5372\tLR: 0.010000\n",
      "4.9676\t4.7282\t4.4232\t4.2099\t4.1052\t4.1031\t\n",
      "Training Epoch: 2 [170/565]\tLoss: 23.8612\tLR: 0.010000\n",
      "4.5690\t4.2408\t3.9148\t3.7201\t3.6716\t3.7449\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [186/565]\tLoss: 18.1570\tLR: 0.010000\n",
      "3.0715\t2.9209\t2.8237\t2.8753\t3.0752\t3.3903\t\n",
      "Training Epoch: 2 [202/565]\tLoss: 25.1133\tLR: 0.010000\n",
      "4.4769\t4.3144\t4.1526\t4.0470\t4.0134\t4.1091\t\n",
      "Training Epoch: 2 [218/565]\tLoss: 31.9594\tLR: 0.010000\n",
      "5.8256\t5.6179\t5.3308\t5.1404\t5.0492\t4.9955\t\n",
      "Training Epoch: 2 [234/565]\tLoss: 24.4973\tLR: 0.010000\n",
      "4.5488\t4.2401\t3.9749\t3.8643\t3.8853\t3.9841\t\n",
      "Training Epoch: 2 [250/565]\tLoss: 22.4213\tLR: 0.010000\n",
      "4.1808\t3.9545\t3.7050\t3.5462\t3.5055\t3.5293\t\n",
      "Training Epoch: 2 [266/565]\tLoss: 16.4977\tLR: 0.010000\n",
      "2.8862\t2.7209\t2.6146\t2.6051\t2.7265\t2.9444\t\n",
      "Training Epoch: 2 [282/565]\tLoss: 19.6535\tLR: 0.010000\n",
      "3.3928\t3.2713\t3.1552\t3.1461\t3.2274\t3.4607\t\n",
      "Training Epoch: 2 [298/565]\tLoss: 18.7400\tLR: 0.010000\n",
      "3.4224\t3.2663\t3.0948\t2.9894\t2.9504\t3.0167\t\n",
      "Training Epoch: 2 [314/565]\tLoss: 26.0373\tLR: 0.010000\n",
      "4.4516\t4.3759\t4.2686\t4.2582\t4.2807\t4.4022\t\n",
      "Training Epoch: 2 [330/565]\tLoss: 23.1256\tLR: 0.010000\n",
      "3.9238\t3.8226\t3.7656\t3.7806\t3.8562\t3.9768\t\n",
      "Training Epoch: 2 [346/565]\tLoss: 24.6551\tLR: 0.010000\n",
      "4.5520\t4.3170\t4.0835\t3.9205\t3.8531\t3.9290\t\n",
      "Training Epoch: 2 [362/565]\tLoss: 19.4618\tLR: 0.010000\n",
      "3.5966\t3.3917\t3.1882\t3.0696\t3.0673\t3.1483\t\n",
      "Training Epoch: 2 [378/565]\tLoss: 23.5549\tLR: 0.010000\n",
      "4.2063\t4.0320\t3.8620\t3.7837\t3.8015\t3.8693\t\n",
      "Training Epoch: 2 [394/565]\tLoss: 24.9802\tLR: 0.010000\n",
      "4.3536\t4.2352\t4.1074\t4.0339\t4.0602\t4.1899\t\n",
      "Training Epoch: 2 [410/565]\tLoss: 23.5473\tLR: 0.010000\n",
      "4.4248\t4.0854\t3.7714\t3.6520\t3.7046\t3.9093\t\n",
      "Training Epoch: 2 [426/565]\tLoss: 20.0244\tLR: 0.010000\n",
      "3.7032\t3.5112\t3.3178\t3.1754\t3.1277\t3.1889\t\n",
      "Training Epoch: 2 [442/565]\tLoss: 15.1025\tLR: 0.010000\n",
      "2.5248\t2.4707\t2.4654\t2.4848\t2.5307\t2.6261\t\n",
      "Training Epoch: 2 [458/565]\tLoss: 18.8174\tLR: 0.010000\n",
      "3.2676\t3.1351\t3.0359\t3.0151\t3.0992\t3.2645\t\n",
      "Training Epoch: 2 [474/565]\tLoss: 18.1484\tLR: 0.010000\n",
      "3.0341\t2.9450\t2.9067\t2.9651\t3.0653\t3.2321\t\n",
      "Training Epoch: 2 [490/565]\tLoss: 19.3613\tLR: 0.010000\n",
      "3.7212\t3.4441\t3.1839\t3.0196\t2.9580\t3.0344\t\n",
      "Training Epoch: 2 [506/565]\tLoss: 15.8687\tLR: 0.010000\n",
      "2.6678\t2.6104\t2.5603\t2.5744\t2.6508\t2.8051\t\n",
      "Training Epoch: 2 [522/565]\tLoss: 28.3781\tLR: 0.010000\n",
      "5.5121\t5.1087\t4.7279\t4.4668\t4.3183\t4.2442\t\n",
      "Training Epoch: 2 [538/565]\tLoss: 27.1831\tLR: 0.010000\n",
      "5.3890\t5.0189\t4.5845\t4.2278\t4.0246\t3.9383\t\n",
      "Training Epoch: 2 [554/565]\tLoss: 18.5789\tLR: 0.010000\n",
      "2.9968\t2.9194\t2.9293\t3.0448\t3.2304\t3.4582\t\n",
      "Training Epoch: 2 [570/565]\tLoss: 16.6062\tLR: 0.010000\n",
      "2.5078\t2.5096\t2.5901\t2.7651\t2.9904\t3.2433\t\n",
      "Training Epoch: 2 [586/565]\tLoss: 20.9425\tLR: 0.010000\n",
      "3.6222\t3.4399\t3.3557\t3.3835\t3.4925\t3.6488\t\n",
      "Training Epoch: 2 [602/565]\tLoss: 15.6026\tLR: 0.010000\n",
      "2.5217\t2.4595\t2.4393\t2.5154\t2.6979\t2.9688\t\n",
      "Training Epoch: 2 [618/565]\tLoss: 21.3062\tLR: 0.010000\n",
      "3.7933\t3.5971\t3.4430\t3.3863\t3.4714\t3.6151\t\n",
      "Training Epoch: 2 [634/565]\tLoss: 25.5749\tLR: 0.010000\n",
      "5.0299\t4.6445\t4.2685\t3.9887\t3.8405\t3.8029\t\n",
      "Training Epoch: 2 [650/565]\tLoss: 30.6335\tLR: 0.010000\n",
      "6.0896\t5.6913\t5.2122\t4.7955\t4.4993\t4.3455\t\n",
      "Training Epoch: 2 [666/565]\tLoss: 23.1973\tLR: 0.010000\n",
      "4.0789\t3.8443\t3.7323\t3.7381\t3.8328\t3.9709\t\n",
      "Training Epoch: 2 [682/565]\tLoss: 19.5985\tLR: 0.010000\n",
      "3.4499\t3.2770\t3.1256\t3.1075\t3.2201\t3.4184\t\n",
      "Training Epoch: 2 [698/565]\tLoss: 12.7929\tLR: 0.010000\n",
      "2.0946\t2.0476\t2.0077\t2.0545\t2.1887\t2.3998\t\n",
      "Training Epoch: 2 [714/565]\tLoss: 15.4306\tLR: 0.010000\n",
      "2.9025\t2.6650\t2.4855\t2.4240\t2.4351\t2.5183\t\n",
      "Training Epoch: 2 [730/565]\tLoss: 16.7965\tLR: 0.010000\n",
      "2.6468\t2.6729\t2.7067\t2.7824\t2.8970\t3.0906\t\n",
      "Training Epoch: 2 [746/565]\tLoss: 20.7868\tLR: 0.010000\n",
      "3.7252\t3.5166\t3.3553\t3.3206\t3.3696\t3.4994\t\n",
      "Training Epoch: 2 [762/565]\tLoss: 16.8726\tLR: 0.010000\n",
      "3.0182\t2.8693\t2.7477\t2.7035\t2.7239\t2.8100\t\n",
      "Training Epoch: 2 [778/565]\tLoss: 16.7064\tLR: 0.010000\n",
      "2.4781\t2.5344\t2.6468\t2.7955\t2.9795\t3.2721\t\n",
      "Training Epoch: 2 [794/565]\tLoss: 23.2050\tLR: 0.010000\n",
      "4.0422\t3.8783\t3.7645\t3.7327\t3.8057\t3.9816\t\n",
      "Training Epoch: 2 [810/565]\tLoss: 15.9369\tLR: 0.010000\n",
      "2.9883\t2.8250\t2.6353\t2.5131\t2.4672\t2.5079\t\n",
      "Training Epoch: 2 [826/565]\tLoss: 26.0142\tLR: 0.010000\n",
      "4.7658\t4.5469\t4.2810\t4.1255\t4.1009\t4.1941\t\n",
      "Training Epoch: 2 [842/565]\tLoss: 16.5027\tLR: 0.010000\n",
      "2.8150\t2.7005\t2.6337\t2.6578\t2.7539\t2.9417\t\n",
      "Training Epoch: 2 [858/565]\tLoss: 38.6934\tLR: 0.010000\n",
      "7.3540\t6.9997\t6.5654\t6.1860\t5.8855\t5.7028\t\n",
      "Training Epoch: 2 [874/565]\tLoss: 25.1025\tLR: 0.010000\n",
      "4.5208\t4.3585\t4.1661\t4.0436\t3.9971\t4.0164\t\n",
      "Training Epoch: 2 [890/565]\tLoss: 18.9055\tLR: 0.010000\n",
      "3.5187\t3.3155\t3.1196\t2.9965\t2.9581\t2.9971\t\n",
      "Training Epoch: 2 [901/565]\tLoss: 17.5872\tLR: 0.010000\n",
      "2.4645\t2.4819\t2.6620\t2.9706\t3.3346\t3.6736\t\n",
      "[0.35163259506225586, 0.25749269127845764, 0.3908747136592865, 0.0140174999833107, 0.41027969121932983, 0.20097415149211884, 0.38874615728855133, 0.013585086911916733, 0.37863782048225403, 0.19928371906280518, 0.4220784604549408, 0.0034832737874239683, 0.33095279335975647, 0.21757394075393677, 0.45147326588630676, 0.0025166163686662912, 0.1876033991575241, 0.0, 0.8123966008424759, 0.005639048293232918]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3910, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3733, Accuracy: 0.3363\n",
      "Test set t = 02: Average loss: 0.3562, Accuracy: 0.3416\n",
      "Test set t = 03: Average loss: 0.3472, Accuracy: 0.3221\n",
      "Test set t = 04: Average loss: 0.3469, Accuracy: 0.3221\n",
      "Test set t = 05: Average loss: 0.3549, Accuracy: 0.3097\n",
      "\n",
      "Training Epoch: 3 [10/565]\tLoss: 30.6646\tLR: 0.010000\n",
      "5.6671\t5.4186\t5.1422\t4.9175\t4.8002\t4.7189\t\n",
      "Training Epoch: 3 [26/565]\tLoss: 17.6978\tLR: 0.010000\n",
      "3.0986\t2.9653\t2.8747\t2.8562\t2.8992\t3.0040\t\n",
      "Training Epoch: 3 [42/565]\tLoss: 20.7586\tLR: 0.010000\n",
      "3.5469\t3.4392\t3.3516\t3.3506\t3.4560\t3.6143\t\n",
      "Training Epoch: 3 [58/565]\tLoss: 17.9668\tLR: 0.010000\n",
      "3.4682\t3.2250\t2.9566\t2.7766\t2.7255\t2.8148\t\n",
      "Training Epoch: 3 [74/565]\tLoss: 33.7607\tLR: 0.010000\n",
      "6.5463\t6.1320\t5.6641\t5.3070\t5.0968\t5.0144\t\n",
      "Training Epoch: 3 [90/565]\tLoss: 15.9248\tLR: 0.010000\n",
      "2.3501\t2.3776\t2.4847\t2.6896\t2.9062\t3.1167\t\n",
      "Training Epoch: 3 [106/565]\tLoss: 17.5544\tLR: 0.010000\n",
      "2.7938\t2.8072\t2.8347\t2.8950\t3.0276\t3.1962\t\n",
      "Training Epoch: 3 [122/565]\tLoss: 19.1308\tLR: 0.010000\n",
      "3.2206\t3.1390\t3.0906\t3.1065\t3.2053\t3.3688\t\n",
      "Training Epoch: 3 [138/565]\tLoss: 14.7667\tLR: 0.010000\n",
      "2.2478\t2.2761\t2.3341\t2.4507\t2.6124\t2.8456\t\n",
      "Training Epoch: 3 [154/565]\tLoss: 22.7098\tLR: 0.010000\n",
      "3.6454\t3.6298\t3.6829\t3.7710\t3.9035\t4.0771\t\n",
      "Training Epoch: 3 [170/565]\tLoss: 16.9967\tLR: 0.010000\n",
      "3.4209\t3.1252\t2.8288\t2.6136\t2.5041\t2.5041\t\n",
      "Training Epoch: 3 [186/565]\tLoss: 17.0445\tLR: 0.010000\n",
      "2.7324\t2.7099\t2.6910\t2.7781\t2.9511\t3.1821\t\n",
      "Training Epoch: 3 [202/565]\tLoss: 24.1938\tLR: 0.010000\n",
      "4.6432\t4.3624\t4.0510\t3.8064\t3.6816\t3.6492\t\n",
      "Training Epoch: 3 [218/565]\tLoss: 23.3390\tLR: 0.010000\n",
      "4.2942\t4.0266\t3.8181\t3.7020\t3.7036\t3.7944\t\n",
      "Training Epoch: 3 [234/565]\tLoss: 26.0388\tLR: 0.010000\n",
      "4.6887\t4.5317\t4.3255\t4.1790\t4.1350\t4.1789\t\n",
      "Training Epoch: 3 [250/565]\tLoss: 20.2467\tLR: 0.010000\n",
      "3.8743\t3.6013\t3.3232\t3.1453\t3.1132\t3.1893\t\n",
      "Training Epoch: 3 [266/565]\tLoss: 19.1232\tLR: 0.010000\n",
      "3.1388\t3.1125\t3.1029\t3.1478\t3.2426\t3.3786\t\n",
      "Training Epoch: 3 [282/565]\tLoss: 18.3343\tLR: 0.010000\n",
      "3.2282\t3.0771\t2.9903\t2.9605\t2.9903\t3.0879\t\n",
      "Training Epoch: 3 [298/565]\tLoss: 25.3875\tLR: 0.010000\n",
      "4.7671\t4.5003\t4.2408\t4.0509\t3.9319\t3.8965\t\n",
      "Training Epoch: 3 [314/565]\tLoss: 23.3564\tLR: 0.010000\n",
      "4.2025\t4.0044\t3.8404\t3.7613\t3.7603\t3.7875\t\n",
      "Training Epoch: 3 [330/565]\tLoss: 11.9920\tLR: 0.010000\n",
      "1.7652\t1.7444\t1.7893\t1.9553\t2.2157\t2.5220\t\n",
      "Training Epoch: 3 [346/565]\tLoss: 26.0630\tLR: 0.010000\n",
      "4.8127\t4.5732\t4.3175\t4.1744\t4.1000\t4.0853\t\n",
      "Training Epoch: 3 [362/565]\tLoss: 21.0691\tLR: 0.010000\n",
      "3.6281\t3.5370\t3.4708\t3.4571\t3.4679\t3.5082\t\n",
      "Training Epoch: 3 [378/565]\tLoss: 13.9671\tLR: 0.010000\n",
      "2.3351\t2.2602\t2.2415\t2.2509\t2.3517\t2.5277\t\n",
      "Training Epoch: 3 [394/565]\tLoss: 18.1592\tLR: 0.010000\n",
      "3.5103\t3.2595\t3.0152\t2.8471\t2.7656\t2.7616\t\n",
      "Training Epoch: 3 [410/565]\tLoss: 23.0622\tLR: 0.010000\n",
      "3.9395\t3.8317\t3.7771\t3.7816\t3.8245\t3.9077\t\n",
      "Training Epoch: 3 [426/565]\tLoss: 24.4980\tLR: 0.010000\n",
      "4.7023\t4.4884\t4.1498\t3.8790\t3.6798\t3.5987\t\n",
      "Training Epoch: 3 [442/565]\tLoss: 21.1216\tLR: 0.010000\n",
      "3.6139\t3.5494\t3.4640\t3.4343\t3.4693\t3.5907\t\n",
      "Training Epoch: 3 [458/565]\tLoss: 23.0633\tLR: 0.010000\n",
      "4.0846\t3.9544\t3.8247\t3.7350\t3.7230\t3.7415\t\n",
      "Training Epoch: 3 [474/565]\tLoss: 26.9454\tLR: 0.010000\n",
      "4.7878\t4.6273\t4.4581\t4.3737\t4.3491\t4.3494\t\n",
      "Training Epoch: 3 [490/565]\tLoss: 18.6629\tLR: 0.010000\n",
      "3.1225\t3.0664\t3.0437\t3.0743\t3.1263\t3.2296\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [506/565]\tLoss: 16.4138\tLR: 0.010000\n",
      "2.8417\t2.7825\t2.7087\t2.6806\t2.6896\t2.7106\t\n",
      "Training Epoch: 3 [522/565]\tLoss: 25.4452\tLR: 0.010000\n",
      "4.7255\t4.4353\t4.2013\t4.0783\t4.0071\t3.9977\t\n",
      "Training Epoch: 3 [538/565]\tLoss: 20.8222\tLR: 0.010000\n",
      "3.9478\t3.7659\t3.5181\t3.3039\t3.1745\t3.1120\t\n",
      "Training Epoch: 3 [554/565]\tLoss: 27.9298\tLR: 0.010000\n",
      "5.2727\t4.9812\t4.6843\t4.4697\t4.3129\t4.2089\t\n",
      "Training Epoch: 3 [570/565]\tLoss: 16.9260\tLR: 0.010000\n",
      "2.9280\t2.8186\t2.7238\t2.7177\t2.7856\t2.9522\t\n",
      "Training Epoch: 3 [586/565]\tLoss: 27.8051\tLR: 0.010000\n",
      "5.3994\t5.0559\t4.6808\t4.3990\t4.1947\t4.0753\t\n",
      "Training Epoch: 3 [602/565]\tLoss: 18.4823\tLR: 0.010000\n",
      "3.4872\t3.2229\t3.0046\t2.8948\t2.8910\t2.9819\t\n",
      "Training Epoch: 3 [618/565]\tLoss: 23.3170\tLR: 0.010000\n",
      "4.2270\t4.0632\t3.8982\t3.7871\t3.7018\t3.6397\t\n",
      "Training Epoch: 3 [634/565]\tLoss: 21.4345\tLR: 0.010000\n",
      "4.1207\t3.8763\t3.5973\t3.3819\t3.2536\t3.2046\t\n",
      "Training Epoch: 3 [650/565]\tLoss: 28.3529\tLR: 0.010000\n",
      "5.5379\t5.2151\t4.8022\t4.4638\t4.2256\t4.1081\t\n",
      "Training Epoch: 3 [666/565]\tLoss: 17.8314\tLR: 0.010000\n",
      "3.3659\t3.1292\t2.8919\t2.7447\t2.7617\t2.9382\t\n",
      "Training Epoch: 3 [682/565]\tLoss: 25.5316\tLR: 0.010000\n",
      "4.5172\t4.4133\t4.2465\t4.1168\t4.0862\t4.1515\t\n",
      "Training Epoch: 3 [698/565]\tLoss: 27.1827\tLR: 0.010000\n",
      "5.0661\t4.8154\t4.5471\t4.3478\t4.2215\t4.1847\t\n",
      "Training Epoch: 3 [714/565]\tLoss: 18.9091\tLR: 0.010000\n",
      "3.4279\t3.2629\t3.0855\t3.0021\t3.0094\t3.1214\t\n",
      "Training Epoch: 3 [730/565]\tLoss: 12.8622\tLR: 0.010000\n",
      "1.9303\t1.9157\t1.9809\t2.1186\t2.3265\t2.5902\t\n",
      "Training Epoch: 3 [746/565]\tLoss: 33.8834\tLR: 0.010000\n",
      "6.2131\t5.9330\t5.6455\t5.4538\t5.3255\t5.3125\t\n",
      "Training Epoch: 3 [762/565]\tLoss: 11.7092\tLR: 0.010000\n",
      "1.5680\t1.6544\t1.7787\t1.9786\t2.2263\t2.5032\t\n",
      "Training Epoch: 3 [778/565]\tLoss: 26.3740\tLR: 0.010000\n",
      "5.1329\t4.8000\t4.3862\t4.1014\t3.9715\t3.9821\t\n",
      "Training Epoch: 3 [794/565]\tLoss: 30.9563\tLR: 0.010000\n",
      "6.4177\t5.9392\t5.3346\t4.7891\t4.3853\t4.0905\t\n",
      "Training Epoch: 3 [810/565]\tLoss: 22.4292\tLR: 0.010000\n",
      "3.9092\t3.8126\t3.6960\t3.6237\t3.6333\t3.7543\t\n",
      "Training Epoch: 3 [826/565]\tLoss: 20.3516\tLR: 0.010000\n",
      "3.5257\t3.3648\t3.2704\t3.2840\t3.3838\t3.5230\t\n",
      "Training Epoch: 3 [842/565]\tLoss: 23.2827\tLR: 0.010000\n",
      "4.3028\t4.1112\t3.8875\t3.6998\t3.6153\t3.6661\t\n",
      "Training Epoch: 3 [858/565]\tLoss: 14.3231\tLR: 0.010000\n",
      "2.5740\t2.4601\t2.3587\t2.2937\t2.2876\t2.3489\t\n",
      "Training Epoch: 3 [874/565]\tLoss: 16.0302\tLR: 0.010000\n",
      "2.9201\t2.7559\t2.6066\t2.5304\t2.5614\t2.6559\t\n",
      "Training Epoch: 3 [890/565]\tLoss: 25.8410\tLR: 0.010000\n",
      "4.8957\t4.5678\t4.2710\t4.0840\t3.9900\t4.0324\t\n",
      "Training Epoch: 3 [901/565]\tLoss: 15.2471\tLR: 0.010000\n",
      "2.4628\t2.4442\t2.4458\t2.4760\t2.6172\t2.8011\t\n",
      "[0.3361068665981293, 0.3111700415611267, 0.352723091840744, 0.01483822613954544, 0.4541094899177551, 0.19083450734615326, 0.3550560027360916, 0.015687644481658936, 0.4085249900817871, 0.19180428981781006, 0.39967072010040283, 0.0017299802275374532, 0.3368878960609436, 0.21962736546993256, 0.44348473846912384, -0.0004080623621121049, 0.1813259869813919, 0.0, 0.8186740130186081, 0.003926240839064121]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3930, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3746, Accuracy: 0.3363\n",
      "Test set t = 02: Average loss: 0.3572, Accuracy: 0.3434\n",
      "Test set t = 03: Average loss: 0.3478, Accuracy: 0.3239\n",
      "Test set t = 04: Average loss: 0.3469, Accuracy: 0.3239\n",
      "Test set t = 05: Average loss: 0.3542, Accuracy: 0.3115\n",
      "\n",
      "Training Epoch: 4 [10/565]\tLoss: 24.8834\tLR: 0.010000\n",
      "4.9921\t4.5783\t4.1627\t3.8588\t3.6769\t3.6147\t\n",
      "Training Epoch: 4 [26/565]\tLoss: 16.6348\tLR: 0.010000\n",
      "2.5707\t2.6035\t2.6427\t2.7507\t2.9185\t3.1487\t\n",
      "Training Epoch: 4 [42/565]\tLoss: 22.9110\tLR: 0.010000\n",
      "4.4494\t4.1612\t3.8648\t3.6266\t3.4585\t3.3505\t\n",
      "Training Epoch: 4 [58/565]\tLoss: 22.6100\tLR: 0.010000\n",
      "4.3126\t4.0187\t3.7283\t3.5658\t3.5023\t3.4824\t\n",
      "Training Epoch: 4 [74/565]\tLoss: 20.0911\tLR: 0.010000\n",
      "3.4691\t3.3840\t3.2701\t3.2276\t3.2846\t3.4557\t\n",
      "Training Epoch: 4 [90/565]\tLoss: 15.8970\tLR: 0.010000\n",
      "2.7373\t2.6517\t2.5727\t2.5631\t2.6230\t2.7492\t\n",
      "Training Epoch: 4 [106/565]\tLoss: 15.9766\tLR: 0.010000\n",
      "2.7165\t2.6161\t2.5421\t2.5658\t2.6652\t2.8710\t\n",
      "Training Epoch: 4 [122/565]\tLoss: 27.4647\tLR: 0.010000\n",
      "5.3142\t4.9138\t4.5423\t4.3165\t4.1931\t4.1848\t\n",
      "Training Epoch: 4 [138/565]\tLoss: 16.4158\tLR: 0.010000\n",
      "2.8907\t2.7953\t2.6914\t2.6420\t2.6759\t2.7206\t\n",
      "Training Epoch: 4 [154/565]\tLoss: 30.3840\tLR: 0.010000\n",
      "6.0446\t5.6328\t5.1541\t4.7626\t4.4788\t4.3111\t\n",
      "Training Epoch: 4 [170/565]\tLoss: 26.8860\tLR: 0.010000\n",
      "4.9960\t4.7712\t4.5007\t4.2925\t4.1710\t4.1545\t\n",
      "Training Epoch: 4 [186/565]\tLoss: 11.4300\tLR: 0.010000\n",
      "1.6558\t1.6794\t1.7504\t1.8971\t2.1010\t2.3464\t\n",
      "Training Epoch: 4 [202/565]\tLoss: 27.7552\tLR: 0.010000\n",
      "5.0375\t4.8091\t4.5916\t4.4608\t4.4035\t4.4528\t\n",
      "Training Epoch: 4 [218/565]\tLoss: 19.2872\tLR: 0.010000\n",
      "3.3608\t3.1762\t3.0879\t3.0959\t3.1978\t3.3687\t\n",
      "Training Epoch: 4 [234/565]\tLoss: 18.8345\tLR: 0.010000\n",
      "3.4729\t3.2592\t3.0731\t2.9932\t2.9895\t3.0464\t\n",
      "Training Epoch: 4 [250/565]\tLoss: 23.6837\tLR: 0.010000\n",
      "4.2277\t4.0005\t3.8254\t3.7944\t3.8614\t3.9743\t\n",
      "Training Epoch: 4 [266/565]\tLoss: 24.5266\tLR: 0.010000\n",
      "4.1837\t4.0216\t3.9224\t3.9577\t4.1137\t4.3276\t\n",
      "Training Epoch: 4 [282/565]\tLoss: 20.9137\tLR: 0.010000\n",
      "3.5007\t3.4294\t3.3943\t3.4143\t3.5125\t3.6624\t\n",
      "Training Epoch: 4 [298/565]\tLoss: 18.1824\tLR: 0.010000\n",
      "3.2678\t3.0985\t2.9547\t2.9000\t2.9280\t3.0333\t\n",
      "Training Epoch: 4 [314/565]\tLoss: 18.2753\tLR: 0.010000\n",
      "3.2072\t3.0718\t2.9719\t2.9385\t2.9725\t3.1135\t\n",
      "Training Epoch: 4 [330/565]\tLoss: 21.1592\tLR: 0.010000\n",
      "4.0169\t3.7604\t3.5180\t3.3414\t3.2607\t3.2619\t\n",
      "Training Epoch: 4 [346/565]\tLoss: 29.0785\tLR: 0.010000\n",
      "5.7615\t5.3415\t4.8912\t4.5255\t4.3181\t4.2407\t\n",
      "Training Epoch: 4 [362/565]\tLoss: 19.2998\tLR: 0.010000\n",
      "3.3687\t3.2331\t3.1281\t3.0973\t3.1584\t3.3142\t\n",
      "Training Epoch: 4 [378/565]\tLoss: 24.1283\tLR: 0.010000\n",
      "4.3058\t4.1581\t3.9919\t3.8830\t3.8547\t3.9349\t\n",
      "Training Epoch: 4 [394/565]\tLoss: 23.9796\tLR: 0.010000\n",
      "4.4212\t4.1804\t3.9848\t3.8642\t3.7834\t3.7456\t\n",
      "Training Epoch: 4 [410/565]\tLoss: 25.8927\tLR: 0.010000\n",
      "4.7486\t4.5106\t4.2559\t4.1252\t4.0998\t4.1526\t\n",
      "Training Epoch: 4 [426/565]\tLoss: 26.1524\tLR: 0.010000\n",
      "5.2540\t4.8787\t4.4397\t4.0804\t3.8044\t3.6952\t\n",
      "Training Epoch: 4 [442/565]\tLoss: 16.7020\tLR: 0.010000\n",
      "2.8719\t2.7602\t2.6715\t2.6722\t2.7610\t2.9653\t\n",
      "Training Epoch: 4 [458/565]\tLoss: 20.4289\tLR: 0.010000\n",
      "3.9359\t3.6458\t3.3704\t3.1717\t3.1159\t3.1893\t\n",
      "Training Epoch: 4 [474/565]\tLoss: 37.8364\tLR: 0.010000\n",
      "7.4846\t6.9987\t6.4660\t5.9982\t5.5919\t5.2970\t\n",
      "Training Epoch: 4 [490/565]\tLoss: 18.2186\tLR: 0.010000\n",
      "3.2992\t3.1267\t2.9753\t2.9075\t2.9128\t2.9971\t\n",
      "Training Epoch: 4 [506/565]\tLoss: 19.6004\tLR: 0.010000\n",
      "3.4565\t3.2699\t3.1588\t3.1636\t3.2191\t3.3326\t\n",
      "Training Epoch: 4 [522/565]\tLoss: 32.0233\tLR: 0.010000\n",
      "6.3005\t5.8768\t5.3998\t5.0169\t4.7684\t4.6610\t\n",
      "Training Epoch: 4 [538/565]\tLoss: 23.6744\tLR: 0.010000\n",
      "4.4392\t4.2643\t3.9672\t3.7408\t3.6241\t3.6388\t\n",
      "Training Epoch: 4 [554/565]\tLoss: 17.3140\tLR: 0.010000\n",
      "2.9746\t2.8534\t2.7715\t2.7740\t2.8742\t3.0663\t\n",
      "Training Epoch: 4 [570/565]\tLoss: 24.9466\tLR: 0.010000\n",
      "4.6526\t4.3506\t4.1093\t3.9789\t3.9311\t3.9240\t\n",
      "Training Epoch: 4 [586/565]\tLoss: 15.9341\tLR: 0.010000\n",
      "2.8405\t2.6643\t2.5261\t2.5148\t2.6047\t2.7837\t\n",
      "Training Epoch: 4 [602/565]\tLoss: 30.7350\tLR: 0.010000\n",
      "5.7172\t5.4029\t5.1288\t4.9228\t4.7846\t4.7787\t\n",
      "Training Epoch: 4 [618/565]\tLoss: 16.3344\tLR: 0.010000\n",
      "2.4894\t2.5503\t2.6100\t2.7239\t2.8836\t3.0772\t\n",
      "Training Epoch: 4 [634/565]\tLoss: 24.5272\tLR: 0.010000\n",
      "4.6232\t4.3278\t4.0426\t3.8618\t3.7985\t3.8734\t\n",
      "Training Epoch: 4 [650/565]\tLoss: 9.6230\tLR: 0.010000\n",
      "1.6165\t1.5664\t1.5464\t1.5401\t1.6033\t1.7503\t\n",
      "Training Epoch: 4 [666/565]\tLoss: 27.8639\tLR: 0.010000\n",
      "5.4328\t5.0890\t4.7001\t4.3849\t4.1639\t4.0931\t\n",
      "Training Epoch: 4 [682/565]\tLoss: 14.1766\tLR: 0.010000\n",
      "2.4288\t2.3651\t2.2830\t2.2774\t2.3386\t2.4838\t\n",
      "Training Epoch: 4 [698/565]\tLoss: 20.7028\tLR: 0.010000\n",
      "3.4194\t3.2900\t3.2647\t3.3734\t3.5717\t3.7836\t\n",
      "Training Epoch: 4 [714/565]\tLoss: 25.1646\tLR: 0.010000\n",
      "4.5003\t4.2396\t4.0866\t4.0386\t4.0827\t4.2167\t\n",
      "Training Epoch: 4 [730/565]\tLoss: 18.4759\tLR: 0.010000\n",
      "3.4066\t3.2021\t3.0101\t2.9153\t2.9275\t3.0142\t\n",
      "Training Epoch: 4 [746/565]\tLoss: 20.2806\tLR: 0.010000\n",
      "3.4122\t3.3481\t3.2729\t3.2772\t3.3743\t3.5960\t\n",
      "Training Epoch: 4 [762/565]\tLoss: 27.3287\tLR: 0.010000\n",
      "5.3898\t4.9629\t4.5445\t4.2623\t4.1019\t4.0673\t\n",
      "Training Epoch: 4 [778/565]\tLoss: 18.7973\tLR: 0.010000\n",
      "2.8700\t2.8764\t2.9614\t3.1210\t3.3565\t3.6120\t\n",
      "Training Epoch: 4 [794/565]\tLoss: 22.0267\tLR: 0.010000\n",
      "3.5891\t3.5329\t3.5193\t3.5936\t3.7668\t4.0250\t\n",
      "Training Epoch: 4 [810/565]\tLoss: 14.8096\tLR: 0.010000\n",
      "2.2927\t2.2595\t2.2951\t2.4161\t2.6241\t2.9221\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [826/565]\tLoss: 18.9640\tLR: 0.010000\n",
      "2.9742\t3.0137\t3.0613\t3.1454\t3.2883\t3.4811\t\n",
      "Training Epoch: 4 [842/565]\tLoss: 15.7344\tLR: 0.010000\n",
      "2.5839\t2.5141\t2.5029\t2.5780\t2.7025\t2.8530\t\n",
      "Training Epoch: 4 [858/565]\tLoss: 22.2506\tLR: 0.010000\n",
      "3.7448\t3.6263\t3.5658\t3.5984\t3.7552\t3.9601\t\n",
      "Training Epoch: 4 [874/565]\tLoss: 20.5012\tLR: 0.010000\n",
      "3.9199\t3.6277\t3.3634\t3.2156\t3.1560\t3.2186\t\n",
      "Training Epoch: 4 [890/565]\tLoss: 18.7595\tLR: 0.010000\n",
      "3.3874\t3.2082\t3.0507\t2.9627\t2.9924\t3.1581\t\n",
      "Training Epoch: 4 [901/565]\tLoss: 13.8475\tLR: 0.010000\n",
      "2.0509\t2.1027\t2.1840\t2.3061\t2.4925\t2.7114\t\n",
      "[0.3210489749908447, 0.37925025820732117, 0.2997007668018341, 0.015014969743788242, 0.5088819265365601, 0.18365001678466797, 0.307468056678772, 0.017325695604085922, 0.4493941366672516, 0.18789072334766388, 0.36271513998508453, 0.00012413285730872303, 0.344420462846756, 0.2256542295217514, 0.4299253076314926, -0.002962169237434864, 0.1773509979248047, 0.0, 0.8226490020751953, 0.001801265636458993]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3922, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3727, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3551, Accuracy: 0.3416\n",
      "Test set t = 03: Average loss: 0.3458, Accuracy: 0.3257\n",
      "Test set t = 04: Average loss: 0.3452, Accuracy: 0.3274\n",
      "Test set t = 05: Average loss: 0.3530, Accuracy: 0.3186\n",
      "\n",
      "Training Epoch: 5 [10/565]\tLoss: 16.4026\tLR: 0.010000\n",
      "2.8557\t2.7504\t2.6388\t2.6269\t2.6937\t2.8371\t\n",
      "Training Epoch: 5 [26/565]\tLoss: 23.8628\tLR: 0.010000\n",
      "4.5659\t4.2726\t3.9881\t3.7745\t3.6421\t3.6196\t\n",
      "Training Epoch: 5 [42/565]\tLoss: 29.7697\tLR: 0.010000\n",
      "5.7887\t5.4024\t4.9973\t4.6793\t4.4898\t4.4122\t\n",
      "Training Epoch: 5 [58/565]\tLoss: 24.8021\tLR: 0.010000\n",
      "4.3846\t4.2643\t4.1178\t3.9890\t3.9669\t4.0795\t\n",
      "Training Epoch: 5 [74/565]\tLoss: 17.1841\tLR: 0.010000\n",
      "3.0990\t2.9590\t2.8304\t2.7367\t2.7324\t2.8265\t\n",
      "Training Epoch: 5 [90/565]\tLoss: 21.6990\tLR: 0.010000\n",
      "3.9413\t3.8179\t3.6228\t3.4840\t3.4053\t3.4277\t\n",
      "Training Epoch: 5 [106/565]\tLoss: 32.6990\tLR: 0.010000\n",
      "6.2701\t5.8286\t5.4203\t5.1713\t5.0166\t4.9922\t\n",
      "Training Epoch: 5 [122/565]\tLoss: 24.9533\tLR: 0.010000\n",
      "4.7955\t4.4588\t4.1519\t3.9349\t3.8153\t3.7969\t\n",
      "Training Epoch: 5 [138/565]\tLoss: 22.9500\tLR: 0.010000\n",
      "3.9849\t3.8342\t3.7340\t3.7122\t3.7836\t3.9012\t\n",
      "Training Epoch: 5 [154/565]\tLoss: 26.1408\tLR: 0.010000\n",
      "5.1914\t4.7712\t4.3635\t4.0770\t3.9097\t3.8278\t\n",
      "Training Epoch: 5 [170/565]\tLoss: 18.6406\tLR: 0.010000\n",
      "3.0786\t3.0561\t3.0354\t3.0519\t3.1318\t3.2868\t\n",
      "Training Epoch: 5 [186/565]\tLoss: 17.6080\tLR: 0.010000\n",
      "3.2378\t3.0365\t2.8785\t2.7943\t2.7937\t2.8672\t\n",
      "Training Epoch: 5 [202/565]\tLoss: 21.4533\tLR: 0.010000\n",
      "3.8554\t3.6619\t3.4594\t3.3931\t3.4663\t3.6173\t\n",
      "Training Epoch: 5 [218/565]\tLoss: 21.9521\tLR: 0.010000\n",
      "4.0954\t3.8485\t3.6211\t3.4683\t3.4224\t3.4964\t\n",
      "Training Epoch: 5 [234/565]\tLoss: 20.7328\tLR: 0.010000\n",
      "3.7425\t3.5813\t3.4310\t3.3413\t3.3082\t3.3285\t\n",
      "Training Epoch: 5 [250/565]\tLoss: 22.4776\tLR: 0.010000\n",
      "3.9246\t3.7517\t3.6600\t3.6387\t3.6949\t3.8078\t\n",
      "Training Epoch: 5 [266/565]\tLoss: 18.4139\tLR: 0.010000\n",
      "3.2015\t3.1037\t2.9984\t2.9670\t3.0109\t3.1323\t\n",
      "Training Epoch: 5 [282/565]\tLoss: 19.6293\tLR: 0.010000\n",
      "3.4244\t3.2767\t3.1954\t3.1650\t3.2084\t3.3594\t\n",
      "Training Epoch: 5 [298/565]\tLoss: 23.2560\tLR: 0.010000\n",
      "4.2622\t4.0488\t3.8289\t3.7057\t3.6832\t3.7272\t\n",
      "Training Epoch: 5 [314/565]\tLoss: 17.3688\tLR: 0.010000\n",
      "3.2456\t3.0240\t2.8229\t2.7164\t2.7213\t2.8386\t\n",
      "Training Epoch: 5 [330/565]\tLoss: 16.0902\tLR: 0.010000\n",
      "2.9057\t2.7208\t2.5953\t2.5538\t2.5999\t2.7147\t\n",
      "Training Epoch: 5 [346/565]\tLoss: 23.4623\tLR: 0.010000\n",
      "4.3104\t4.1288\t3.9017\t3.7584\t3.6774\t3.6856\t\n",
      "Training Epoch: 5 [362/565]\tLoss: 20.5906\tLR: 0.010000\n",
      "3.5877\t3.4373\t3.3332\t3.3218\t3.3839\t3.5267\t\n",
      "Training Epoch: 5 [378/565]\tLoss: 19.7851\tLR: 0.010000\n",
      "3.1841\t3.1207\t3.1420\t3.2716\t3.4407\t3.6260\t\n",
      "Training Epoch: 5 [394/565]\tLoss: 11.0902\tLR: 0.010000\n",
      "1.6926\t1.6936\t1.7438\t1.8378\t1.9689\t2.1536\t\n",
      "Training Epoch: 5 [410/565]\tLoss: 18.4540\tLR: 0.010000\n",
      "3.3231\t3.1083\t2.9568\t2.9301\t2.9924\t3.1434\t\n",
      "Training Epoch: 5 [426/565]\tLoss: 22.3685\tLR: 0.010000\n",
      "4.2785\t4.0022\t3.7312\t3.5223\t3.4180\t3.4163\t\n",
      "Training Epoch: 5 [442/565]\tLoss: 27.0365\tLR: 0.010000\n",
      "5.0273\t4.6822\t4.4166\t4.2806\t4.2615\t4.3684\t\n",
      "Training Epoch: 5 [458/565]\tLoss: 16.5403\tLR: 0.010000\n",
      "3.1408\t2.9807\t2.7767\t2.6147\t2.5178\t2.5096\t\n",
      "Training Epoch: 5 [474/565]\tLoss: 20.3334\tLR: 0.010000\n",
      "3.7137\t3.5251\t3.3444\t3.2488\t3.2161\t3.2853\t\n",
      "Training Epoch: 5 [490/565]\tLoss: 23.0164\tLR: 0.010000\n",
      "3.9275\t3.8464\t3.7701\t3.7671\t3.8193\t3.8861\t\n",
      "Training Epoch: 5 [506/565]\tLoss: 20.1755\tLR: 0.010000\n",
      "3.9422\t3.7431\t3.4638\t3.2031\t2.9830\t2.8403\t\n",
      "Training Epoch: 5 [522/565]\tLoss: 27.6307\tLR: 0.010000\n",
      "5.3611\t5.0011\t4.6347\t4.3404\t4.1665\t4.1268\t\n",
      "Training Epoch: 5 [538/565]\tLoss: 25.3302\tLR: 0.010000\n",
      "4.9415\t4.5772\t4.2251\t3.9795\t3.8319\t3.7749\t\n",
      "Training Epoch: 5 [554/565]\tLoss: 17.2258\tLR: 0.010000\n",
      "2.6945\t2.6674\t2.7236\t2.8537\t3.0427\t3.2439\t\n",
      "Training Epoch: 5 [570/565]\tLoss: 14.3450\tLR: 0.010000\n",
      "2.3244\t2.2658\t2.2502\t2.3183\t2.4748\t2.7115\t\n",
      "Training Epoch: 5 [586/565]\tLoss: 15.1303\tLR: 0.010000\n",
      "2.3225\t2.3032\t2.3643\t2.5101\t2.6972\t2.9330\t\n",
      "Training Epoch: 5 [602/565]\tLoss: 22.7399\tLR: 0.010000\n",
      "3.8825\t3.7980\t3.7140\t3.6980\t3.7664\t3.8810\t\n",
      "Training Epoch: 5 [618/565]\tLoss: 18.9292\tLR: 0.010000\n",
      "3.2946\t3.2071\t3.1333\t3.0974\t3.0844\t3.1124\t\n",
      "Training Epoch: 5 [634/565]\tLoss: 22.9579\tLR: 0.010000\n",
      "4.2347\t4.0179\t3.8237\t3.6833\t3.6045\t3.5937\t\n",
      "Training Epoch: 5 [650/565]\tLoss: 16.0418\tLR: 0.010000\n",
      "2.3259\t2.3910\t2.5158\t2.6951\t2.9364\t3.1777\t\n",
      "Training Epoch: 5 [666/565]\tLoss: 28.1778\tLR: 0.010000\n",
      "5.2563\t4.9704\t4.6752\t4.5022\t4.4051\t4.3686\t\n",
      "Training Epoch: 5 [682/565]\tLoss: 11.2117\tLR: 0.010000\n",
      "1.8641\t1.8086\t1.7565\t1.7835\t1.9024\t2.0965\t\n",
      "Training Epoch: 5 [698/565]\tLoss: 19.3210\tLR: 0.010000\n",
      "3.4727\t3.3009\t3.1522\t3.1035\t3.1321\t3.1596\t\n",
      "Training Epoch: 5 [714/565]\tLoss: 17.1461\tLR: 0.010000\n",
      "2.9532\t2.8371\t2.7743\t2.7723\t2.8365\t2.9728\t\n",
      "Training Epoch: 5 [730/565]\tLoss: 27.0230\tLR: 0.010000\n",
      "5.1678\t4.8106\t4.4965\t4.2914\t4.1607\t4.0959\t\n",
      "Training Epoch: 5 [746/565]\tLoss: 22.7815\tLR: 0.010000\n",
      "4.0360\t3.8769\t3.7422\t3.6748\t3.6901\t3.7615\t\n",
      "Training Epoch: 5 [762/565]\tLoss: 26.5062\tLR: 0.010000\n",
      "4.9808\t4.7239\t4.4489\t4.2261\t4.0760\t4.0505\t\n",
      "Training Epoch: 5 [778/565]\tLoss: 36.7371\tLR: 0.010000\n",
      "7.2585\t6.7786\t6.2143\t5.7678\t5.4633\t5.2545\t\n",
      "Training Epoch: 5 [794/565]\tLoss: 23.7260\tLR: 0.010000\n",
      "4.4723\t4.2056\t3.9585\t3.7630\t3.6598\t3.6668\t\n",
      "Training Epoch: 5 [810/565]\tLoss: 30.1439\tLR: 0.010000\n",
      "5.6767\t5.4235\t5.1240\t4.8416\t4.6100\t4.4680\t\n",
      "Training Epoch: 5 [826/565]\tLoss: 20.2944\tLR: 0.010000\n",
      "3.5155\t3.4219\t3.3255\t3.2818\t3.3174\t3.4324\t\n",
      "Training Epoch: 5 [842/565]\tLoss: 13.3270\tLR: 0.010000\n",
      "2.3821\t2.2552\t2.1713\t2.1450\t2.1558\t2.2177\t\n",
      "Training Epoch: 5 [858/565]\tLoss: 24.7786\tLR: 0.010000\n",
      "4.4333\t4.2535\t4.0845\t3.9842\t3.9830\t4.0402\t\n",
      "Training Epoch: 5 [874/565]\tLoss: 17.5087\tLR: 0.010000\n",
      "3.1757\t3.0180\t2.8837\t2.7916\t2.7692\t2.8705\t\n",
      "Training Epoch: 5 [890/565]\tLoss: 24.1563\tLR: 0.010000\n",
      "4.4605\t4.2040\t3.9727\t3.8471\t3.8176\t3.8544\t\n",
      "Training Epoch: 5 [901/565]\tLoss: 13.0400\tLR: 0.010000\n",
      "1.7949\t1.8575\t1.9933\t2.2058\t2.4534\t2.7350\t\n",
      "[0.31766584515571594, 0.41383087635040283, 0.2685032784938812, 0.015432292595505714, 0.5533188581466675, 0.1698721945285797, 0.2768089473247528, 0.018036793917417526, 0.4872279167175293, 0.17587149143218994, 0.33690059185028076, -0.0013938839547336102, 0.35159674286842346, 0.22465965151786804, 0.4237436056137085, -0.005296860821545124, 0.16734836995601654, 0.0, 0.8326516300439835, 3.500652383081615e-05]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3898, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3709, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3539, Accuracy: 0.3398\n",
      "Test set t = 03: Average loss: 0.3446, Accuracy: 0.3292\n",
      "Test set t = 04: Average loss: 0.3432, Accuracy: 0.3274\n",
      "Test set t = 05: Average loss: 0.3497, Accuracy: 0.3204\n",
      "\n",
      "Training Epoch: 6 [10/565]\tLoss: 19.6631\tLR: 0.010000\n",
      "3.2675\t3.1815\t3.1602\t3.2248\t3.3224\t3.5068\t\n",
      "Training Epoch: 6 [26/565]\tLoss: 27.7433\tLR: 0.010000\n",
      "5.6381\t5.1937\t4.7080\t4.3237\t4.0261\t3.8537\t\n",
      "Training Epoch: 6 [42/565]\tLoss: 21.9750\tLR: 0.010000\n",
      "4.2328\t3.9231\t3.6357\t3.4486\t3.3701\t3.3646\t\n",
      "Training Epoch: 6 [58/565]\tLoss: 22.0975\tLR: 0.010000\n",
      "4.0394\t3.8599\t3.6620\t3.5409\t3.4879\t3.5075\t\n",
      "Training Epoch: 6 [74/565]\tLoss: 25.8295\tLR: 0.010000\n",
      "5.0286\t4.6834\t4.3363\t4.0755\t3.9030\t3.8025\t\n",
      "Training Epoch: 6 [90/565]\tLoss: 38.6969\tLR: 0.010000\n",
      "7.5820\t7.0850\t6.5409\t6.1052\t5.7935\t5.5904\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [106/565]\tLoss: 13.9905\tLR: 0.010000\n",
      "2.4680\t2.3104\t2.2258\t2.2100\t2.2829\t2.4933\t\n",
      "Training Epoch: 6 [122/565]\tLoss: 21.3655\tLR: 0.010000\n",
      "3.7116\t3.5999\t3.4886\t3.4343\t3.4909\t3.6400\t\n",
      "Training Epoch: 6 [138/565]\tLoss: 15.3833\tLR: 0.010000\n",
      "2.2026\t2.2661\t2.4100\t2.6104\t2.8237\t3.0706\t\n",
      "Training Epoch: 6 [154/565]\tLoss: 29.0703\tLR: 0.010000\n",
      "5.4174\t5.1366\t4.8552\t4.6542\t4.5229\t4.4840\t\n",
      "Training Epoch: 6 [170/565]\tLoss: 17.0017\tLR: 0.010000\n",
      "3.0323\t2.8591\t2.7538\t2.7099\t2.7559\t2.8908\t\n",
      "Training Epoch: 6 [186/565]\tLoss: 19.4541\tLR: 0.010000\n",
      "3.5326\t3.3795\t3.2067\t3.1128\t3.0963\t3.1263\t\n",
      "Training Epoch: 6 [202/565]\tLoss: 19.0302\tLR: 0.010000\n",
      "3.0633\t3.0451\t3.0644\t3.1556\t3.2756\t3.4262\t\n",
      "Training Epoch: 6 [218/565]\tLoss: 22.3523\tLR: 0.010000\n",
      "3.8459\t3.7608\t3.6672\t3.6336\t3.6652\t3.7796\t\n",
      "Training Epoch: 6 [234/565]\tLoss: 28.4470\tLR: 0.010000\n",
      "5.5354\t5.2031\t4.8331\t4.5111\t4.2507\t4.1136\t\n",
      "Training Epoch: 6 [250/565]\tLoss: 32.4800\tLR: 0.010000\n",
      "6.3635\t5.9046\t5.4664\t5.1287\t4.8896\t4.7272\t\n",
      "Training Epoch: 6 [266/565]\tLoss: 20.6080\tLR: 0.010000\n",
      "3.7285\t3.5352\t3.3694\t3.2962\t3.3053\t3.3734\t\n",
      "Training Epoch: 6 [282/565]\tLoss: 34.2234\tLR: 0.010000\n",
      "6.4734\t6.0919\t5.7065\t5.4282\t5.2642\t5.2593\t\n",
      "Training Epoch: 6 [298/565]\tLoss: 19.1111\tLR: 0.010000\n",
      "3.4989\t3.2663\t3.0963\t3.0325\t3.0529\t3.1641\t\n",
      "Training Epoch: 6 [314/565]\tLoss: 17.9627\tLR: 0.010000\n",
      "3.3438\t3.1336\t2.9482\t2.8441\t2.8264\t2.8667\t\n",
      "Training Epoch: 6 [330/565]\tLoss: 31.4062\tLR: 0.010000\n",
      "5.8995\t5.5171\t5.1963\t4.9924\t4.9060\t4.8948\t\n",
      "Training Epoch: 6 [346/565]\tLoss: 14.2813\tLR: 0.010000\n",
      "2.2234\t2.2022\t2.2285\t2.3293\t2.5248\t2.7731\t\n",
      "Training Epoch: 6 [362/565]\tLoss: 14.0159\tLR: 0.010000\n",
      "2.1702\t2.1426\t2.1980\t2.3189\t2.4898\t2.6964\t\n",
      "Training Epoch: 6 [378/565]\tLoss: 25.6776\tLR: 0.010000\n",
      "4.8155\t4.5374\t4.2725\t4.0901\t3.9874\t3.9747\t\n",
      "Training Epoch: 6 [394/565]\tLoss: 15.7271\tLR: 0.010000\n",
      "2.7209\t2.6383\t2.5751\t2.5426\t2.5755\t2.6748\t\n",
      "Training Epoch: 6 [410/565]\tLoss: 25.3937\tLR: 0.010000\n",
      "4.2850\t4.1403\t4.0862\t4.1566\t4.2761\t4.4495\t\n",
      "Training Epoch: 6 [426/565]\tLoss: 16.5867\tLR: 0.010000\n",
      "2.8670\t2.6925\t2.6044\t2.6347\t2.7815\t3.0065\t\n",
      "Training Epoch: 6 [442/565]\tLoss: 24.3243\tLR: 0.010000\n",
      "4.6490\t4.3288\t4.0438\t3.8406\t3.7375\t3.7246\t\n",
      "Training Epoch: 6 [458/565]\tLoss: 21.2584\tLR: 0.010000\n",
      "3.7169\t3.4750\t3.3933\t3.4181\t3.5380\t3.7171\t\n",
      "Training Epoch: 6 [474/565]\tLoss: 11.0615\tLR: 0.010000\n",
      "1.9315\t1.8451\t1.7645\t1.7415\t1.8110\t1.9677\t\n",
      "Training Epoch: 6 [490/565]\tLoss: 18.0642\tLR: 0.010000\n",
      "3.0829\t2.9141\t2.8434\t2.8894\t3.0602\t3.2741\t\n",
      "Training Epoch: 6 [506/565]\tLoss: 8.1305\tLR: 0.010000\n",
      "1.0658\t1.1221\t1.1830\t1.3027\t1.5481\t1.9087\t\n",
      "Training Epoch: 6 [522/565]\tLoss: 24.4548\tLR: 0.010000\n",
      "4.2826\t4.1137\t4.0048\t3.9691\t3.9962\t4.0884\t\n",
      "Training Epoch: 6 [538/565]\tLoss: 20.2993\tLR: 0.010000\n",
      "3.5594\t3.3864\t3.2755\t3.2499\t3.3161\t3.5120\t\n",
      "Training Epoch: 6 [554/565]\tLoss: 15.4621\tLR: 0.010000\n",
      "2.5741\t2.5172\t2.4919\t2.5144\t2.6043\t2.7601\t\n",
      "Training Epoch: 6 [570/565]\tLoss: 17.3442\tLR: 0.010000\n",
      "3.0010\t2.9171\t2.8301\t2.8059\t2.8411\t2.9490\t\n",
      "Training Epoch: 6 [586/565]\tLoss: 31.5857\tLR: 0.010000\n",
      "5.8974\t5.5759\t5.2581\t5.0407\t4.9332\t4.8804\t\n",
      "Training Epoch: 6 [602/565]\tLoss: 20.5262\tLR: 0.010000\n",
      "3.6072\t3.4666\t3.3453\t3.3023\t3.3487\t3.4560\t\n",
      "Training Epoch: 6 [618/565]\tLoss: 16.6087\tLR: 0.010000\n",
      "2.7765\t2.7062\t2.6598\t2.6932\t2.8028\t2.9702\t\n",
      "Training Epoch: 6 [634/565]\tLoss: 27.5970\tLR: 0.010000\n",
      "5.1082\t4.8434\t4.5807\t4.4092\t4.3283\t4.3273\t\n",
      "Training Epoch: 6 [650/565]\tLoss: 21.1228\tLR: 0.010000\n",
      "3.8362\t3.6333\t3.4588\t3.3862\t3.3800\t3.4283\t\n",
      "Training Epoch: 6 [666/565]\tLoss: 15.4430\tLR: 0.010000\n",
      "2.4731\t2.4651\t2.4818\t2.5599\t2.6643\t2.7988\t\n",
      "Training Epoch: 6 [682/565]\tLoss: 22.4302\tLR: 0.010000\n",
      "4.1996\t3.9825\t3.7586\t3.5933\t3.4736\t3.4226\t\n",
      "Training Epoch: 6 [698/565]\tLoss: 15.9794\tLR: 0.010000\n",
      "2.7660\t2.6638\t2.5836\t2.5736\t2.6297\t2.7627\t\n",
      "Training Epoch: 6 [714/565]\tLoss: 29.2774\tLR: 0.010000\n",
      "5.7346\t5.3206\t4.9062\t4.5932\t4.4035\t4.3193\t\n",
      "Training Epoch: 6 [730/565]\tLoss: 19.8309\tLR: 0.010000\n",
      "3.7378\t3.5256\t3.2715\t3.1189\t3.0608\t3.1164\t\n",
      "Training Epoch: 6 [746/565]\tLoss: 21.6136\tLR: 0.010000\n",
      "3.8808\t3.7052\t3.5467\t3.4652\t3.4517\t3.5641\t\n",
      "Training Epoch: 6 [762/565]\tLoss: 18.2421\tLR: 0.010000\n",
      "3.5913\t3.3092\t3.0169\t2.8254\t2.7334\t2.7659\t\n",
      "Training Epoch: 6 [778/565]\tLoss: 16.8445\tLR: 0.010000\n",
      "2.9395\t2.8694\t2.7858\t2.7271\t2.7197\t2.8030\t\n",
      "Training Epoch: 6 [794/565]\tLoss: 30.7902\tLR: 0.010000\n",
      "5.9967\t5.6247\t5.2362\t4.8856\t4.6076\t4.4394\t\n",
      "Training Epoch: 6 [810/565]\tLoss: 19.8563\tLR: 0.010000\n",
      "3.6252\t3.4411\t3.2819\t3.1780\t3.1488\t3.1813\t\n",
      "Training Epoch: 6 [826/565]\tLoss: 20.7425\tLR: 0.010000\n",
      "3.8959\t3.6104\t3.3910\t3.2812\t3.2624\t3.3017\t\n",
      "Training Epoch: 6 [842/565]\tLoss: 19.5166\tLR: 0.010000\n",
      "3.6589\t3.4418\t3.2169\t3.0525\t3.0310\t3.1156\t\n",
      "Training Epoch: 6 [858/565]\tLoss: 24.9019\tLR: 0.010000\n",
      "4.7812\t4.4923\t4.1750\t3.9304\t3.7786\t3.7444\t\n",
      "Training Epoch: 6 [874/565]\tLoss: 15.1736\tLR: 0.010000\n",
      "2.3233\t2.3022\t2.3610\t2.5019\t2.7060\t2.9793\t\n",
      "Training Epoch: 6 [890/565]\tLoss: 21.1076\tLR: 0.010000\n",
      "3.7244\t3.5454\t3.4276\t3.3981\t3.4543\t3.5577\t\n",
      "Training Epoch: 6 [901/565]\tLoss: 21.0403\tLR: 0.010000\n",
      "3.9177\t3.6934\t3.4917\t3.3465\t3.2840\t3.3071\t\n",
      "[0.304239422082901, 0.45776858925819397, 0.23799198865890503, 0.016336161643266678, 0.5982654690742493, 0.1539706289768219, 0.24776390194892883, 0.018761904910206795, 0.5263287425041199, 0.16627609729766846, 0.30739516019821167, -0.0025284939911216497, 0.35821080207824707, 0.2271091341972351, 0.4146800637245178, -0.00732642924413085, 0.16411171853542328, 0.0, 0.8358882814645767, -0.0009650039719417691]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3894, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3698, Accuracy: 0.3398\n",
      "Test set t = 02: Average loss: 0.3529, Accuracy: 0.3398\n",
      "Test set t = 03: Average loss: 0.3436, Accuracy: 0.3310\n",
      "Test set t = 04: Average loss: 0.3425, Accuracy: 0.3274\n",
      "Test set t = 05: Average loss: 0.3490, Accuracy: 0.3168\n",
      "\n",
      "Training Epoch: 7 [10/565]\tLoss: 22.3360\tLR: 0.010000\n",
      "4.0565\t3.8644\t3.6872\t3.5796\t3.5448\t3.6034\t\n",
      "Training Epoch: 7 [26/565]\tLoss: 20.5931\tLR: 0.010000\n",
      "3.3082\t3.3119\t3.3663\t3.4402\t3.5261\t3.6405\t\n",
      "Training Epoch: 7 [42/565]\tLoss: 26.9488\tLR: 0.010000\n",
      "5.3528\t4.9039\t4.5074\t4.2279\t4.0387\t3.9180\t\n",
      "Training Epoch: 7 [58/565]\tLoss: 21.8072\tLR: 0.010000\n",
      "4.1122\t3.8567\t3.6084\t3.4471\t3.3835\t3.3994\t\n",
      "Training Epoch: 7 [74/565]\tLoss: 16.0992\tLR: 0.010000\n",
      "2.9227\t2.7453\t2.5904\t2.5226\t2.5753\t2.7429\t\n",
      "Training Epoch: 7 [90/565]\tLoss: 23.9624\tLR: 0.010000\n",
      "4.2769\t4.0974\t3.9250\t3.8280\t3.8598\t3.9753\t\n",
      "Training Epoch: 7 [106/565]\tLoss: 16.1999\tLR: 0.010000\n",
      "2.9268\t2.7991\t2.6514\t2.5745\t2.5831\t2.6650\t\n",
      "Training Epoch: 7 [122/565]\tLoss: 14.2202\tLR: 0.010000\n",
      "2.6600\t2.4730\t2.3243\t2.2341\t2.2229\t2.3060\t\n",
      "Training Epoch: 7 [138/565]\tLoss: 25.1446\tLR: 0.010000\n",
      "4.8687\t4.4957\t4.1656\t3.9341\t3.8324\t3.8481\t\n",
      "Training Epoch: 7 [154/565]\tLoss: 31.4812\tLR: 0.010000\n",
      "5.9153\t5.6338\t5.2937\t5.0173\t4.8513\t4.7697\t\n",
      "Training Epoch: 7 [170/565]\tLoss: 21.0454\tLR: 0.010000\n",
      "4.0309\t3.7532\t3.4838\t3.3048\t3.2193\t3.2535\t\n",
      "Training Epoch: 7 [186/565]\tLoss: 29.2217\tLR: 0.010000\n",
      "5.5918\t5.2016\t4.8663\t4.6360\t4.4848\t4.4413\t\n",
      "Training Epoch: 7 [202/565]\tLoss: 21.6241\tLR: 0.010000\n",
      "3.7623\t3.6004\t3.4991\t3.4904\t3.5647\t3.7073\t\n",
      "Training Epoch: 7 [218/565]\tLoss: 31.3264\tLR: 0.010000\n",
      "6.2641\t5.7943\t5.2739\t4.8878\t4.6210\t4.4854\t\n",
      "Training Epoch: 7 [234/565]\tLoss: 22.6603\tLR: 0.010000\n",
      "4.0906\t3.8585\t3.6971\t3.6301\t3.6494\t3.7345\t\n",
      "Training Epoch: 7 [250/565]\tLoss: 16.1033\tLR: 0.010000\n",
      "2.6031\t2.5682\t2.5782\t2.6394\t2.7621\t2.9523\t\n",
      "Training Epoch: 7 [266/565]\tLoss: 17.4728\tLR: 0.010000\n",
      "2.9789\t2.9440\t2.8679\t2.8331\t2.8741\t2.9749\t\n",
      "Training Epoch: 7 [282/565]\tLoss: 11.0971\tLR: 0.010000\n",
      "1.6961\t1.6684\t1.6846\t1.7925\t1.9877\t2.2678\t\n",
      "Training Epoch: 7 [298/565]\tLoss: 13.9542\tLR: 0.010000\n",
      "1.9727\t2.0068\t2.1177\t2.3272\t2.6007\t2.9291\t\n",
      "Training Epoch: 7 [314/565]\tLoss: 27.2048\tLR: 0.010000\n",
      "4.9682\t4.6747\t4.4522\t4.3613\t4.3483\t4.4000\t\n",
      "Training Epoch: 7 [330/565]\tLoss: 30.1737\tLR: 0.010000\n",
      "6.3372\t5.7733\t5.1791\t4.6645\t4.2496\t3.9700\t\n",
      "Training Epoch: 7 [346/565]\tLoss: 13.1011\tLR: 0.010000\n",
      "2.5445\t2.3113\t2.1038\t1.9971\t2.0029\t2.1415\t\n",
      "Training Epoch: 7 [362/565]\tLoss: 28.3842\tLR: 0.010000\n",
      "5.3597\t5.0295\t4.7214\t4.5017\t4.3913\t4.3806\t\n",
      "Training Epoch: 7 [378/565]\tLoss: 23.9723\tLR: 0.010000\n",
      "4.0431\t3.9076\t3.8751\t3.9325\t4.0369\t4.1771\t\n",
      "Training Epoch: 7 [394/565]\tLoss: 26.7276\tLR: 0.010000\n",
      "5.1642\t4.8210\t4.4522\t4.2113\t4.0710\t4.0080\t\n",
      "Training Epoch: 7 [410/565]\tLoss: 19.7704\tLR: 0.010000\n",
      "3.7079\t3.4907\t3.2588\t3.1148\t3.0678\t3.1303\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [426/565]\tLoss: 21.6884\tLR: 0.010000\n",
      "3.7793\t3.5667\t3.4868\t3.4993\t3.6019\t3.7545\t\n",
      "Training Epoch: 7 [442/565]\tLoss: 23.2529\tLR: 0.010000\n",
      "4.2429\t4.0320\t3.8401\t3.7416\t3.6900\t3.7064\t\n",
      "Training Epoch: 7 [458/565]\tLoss: 15.1886\tLR: 0.010000\n",
      "2.1740\t2.1938\t2.3310\t2.5505\t2.8223\t3.1170\t\n",
      "Training Epoch: 7 [474/565]\tLoss: 19.6121\tLR: 0.010000\n",
      "3.3667\t3.2332\t3.1816\t3.1964\t3.2553\t3.3790\t\n",
      "Training Epoch: 7 [490/565]\tLoss: 26.4389\tLR: 0.010000\n",
      "4.8940\t4.6009\t4.3597\t4.1985\t4.1651\t4.2206\t\n",
      "Training Epoch: 7 [506/565]\tLoss: 17.6922\tLR: 0.010000\n",
      "3.1881\t2.9784\t2.8453\t2.8209\t2.8686\t2.9910\t\n",
      "Training Epoch: 7 [522/565]\tLoss: 24.4930\tLR: 0.010000\n",
      "4.4638\t4.2260\t4.0109\t3.8926\t3.8924\t4.0073\t\n",
      "Training Epoch: 7 [538/565]\tLoss: 16.2122\tLR: 0.010000\n",
      "2.7901\t2.6832\t2.6350\t2.6280\t2.6746\t2.8014\t\n",
      "Training Epoch: 7 [554/565]\tLoss: 14.7310\tLR: 0.010000\n",
      "2.1937\t2.2026\t2.2999\t2.4604\t2.6747\t2.8997\t\n",
      "Training Epoch: 7 [570/565]\tLoss: 17.6352\tLR: 0.010000\n",
      "3.1919\t3.0179\t2.8918\t2.8210\t2.8111\t2.9014\t\n",
      "Training Epoch: 7 [586/565]\tLoss: 27.1413\tLR: 0.010000\n",
      "5.5629\t5.0713\t4.5705\t4.1898\t3.9400\t3.8067\t\n",
      "Training Epoch: 7 [602/565]\tLoss: 13.2694\tLR: 0.010000\n",
      "2.0265\t2.0326\t2.0897\t2.1983\t2.3580\t2.5643\t\n",
      "Training Epoch: 7 [618/565]\tLoss: 17.2071\tLR: 0.010000\n",
      "2.7452\t2.6951\t2.7353\t2.8359\t2.9901\t3.2056\t\n",
      "Training Epoch: 7 [634/565]\tLoss: 20.4216\tLR: 0.010000\n",
      "3.5124\t3.4186\t3.3555\t3.3228\t3.3533\t3.4590\t\n",
      "Training Epoch: 7 [650/565]\tLoss: 19.4323\tLR: 0.010000\n",
      "3.6566\t3.4264\t3.2048\t3.0740\t3.0303\t3.0400\t\n",
      "Training Epoch: 7 [666/565]\tLoss: 28.6115\tLR: 0.010000\n",
      "5.5841\t5.2438\t4.8488\t4.5227\t4.2820\t4.1302\t\n",
      "Training Epoch: 7 [682/565]\tLoss: 18.9350\tLR: 0.010000\n",
      "3.0253\t2.9742\t2.9907\t3.0868\t3.2957\t3.5623\t\n",
      "Training Epoch: 7 [698/565]\tLoss: 18.0347\tLR: 0.010000\n",
      "3.3671\t3.1988\t3.0014\t2.8738\t2.7930\t2.8006\t\n",
      "Training Epoch: 7 [714/565]\tLoss: 18.3633\tLR: 0.010000\n",
      "3.3675\t3.1757\t3.0356\t2.9416\t2.9096\t2.9333\t\n",
      "Training Epoch: 7 [730/565]\tLoss: 24.7931\tLR: 0.010000\n",
      "4.5361\t4.2852\t4.0774\t3.9625\t3.9422\t3.9897\t\n",
      "Training Epoch: 7 [746/565]\tLoss: 19.9281\tLR: 0.010000\n",
      "3.7486\t3.5217\t3.3264\t3.1865\t3.0881\t3.0568\t\n",
      "Training Epoch: 7 [762/565]\tLoss: 17.8430\tLR: 0.010000\n",
      "2.9952\t2.8999\t2.8448\t2.8768\t3.0075\t3.2190\t\n",
      "Training Epoch: 7 [778/565]\tLoss: 15.7142\tLR: 0.010000\n",
      "2.6836\t2.6183\t2.5876\t2.5727\t2.5894\t2.6627\t\n",
      "Training Epoch: 7 [794/565]\tLoss: 30.7302\tLR: 0.010000\n",
      "5.5545\t5.3402\t5.1334\t4.9759\t4.8795\t4.8468\t\n",
      "Training Epoch: 7 [810/565]\tLoss: 30.3158\tLR: 0.010000\n",
      "5.9181\t5.4751\t5.0383\t4.7284\t4.5799\t4.5761\t\n",
      "Training Epoch: 7 [826/565]\tLoss: 17.9183\tLR: 0.010000\n",
      "3.3530\t3.1567\t2.9660\t2.8308\t2.7841\t2.8277\t\n",
      "Training Epoch: 7 [842/565]\tLoss: 24.4704\tLR: 0.010000\n",
      "4.2128\t4.1286\t4.0170\t3.9654\t4.0224\t4.1242\t\n",
      "Training Epoch: 7 [858/565]\tLoss: 28.4386\tLR: 0.010000\n",
      "5.4503\t5.1283\t4.7783\t4.5215\t4.3226\t4.2376\t\n",
      "Training Epoch: 7 [874/565]\tLoss: 18.9483\tLR: 0.010000\n",
      "3.0372\t3.0101\t3.0336\t3.1118\t3.2759\t3.4798\t\n",
      "Training Epoch: 7 [890/565]\tLoss: 16.7789\tLR: 0.010000\n",
      "3.0523\t2.8710\t2.7438\t2.6914\t2.6829\t2.7375\t\n",
      "Training Epoch: 7 [901/565]\tLoss: 22.1008\tLR: 0.010000\n",
      "4.3482\t3.9920\t3.6518\t3.4453\t3.3428\t3.3206\t\n",
      "[0.2976002097129822, 0.482025146484375, 0.22037464380264282, 0.016646400094032288, 0.6334218978881836, 0.13993830978870392, 0.2266397923231125, 0.01924947276711464, 0.5612741708755493, 0.1571664810180664, 0.2815593481063843, -0.003593134693801403, 0.36252573132514954, 0.2311737835407257, 0.40630048513412476, -0.00907677412033081, 0.16198287904262543, 0.0, 0.8380171209573746, -0.0024853830691426992]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3903, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3701, Accuracy: 0.3398\n",
      "Test set t = 02: Average loss: 0.3530, Accuracy: 0.3416\n",
      "Test set t = 03: Average loss: 0.3435, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3422, Accuracy: 0.3310\n",
      "Test set t = 05: Average loss: 0.3486, Accuracy: 0.3168\n",
      "\n",
      "Training Epoch: 8 [10/565]\tLoss: 19.8428\tLR: 0.010000\n",
      "3.4567\t3.2840\t3.1827\t3.1897\t3.2786\t3.4511\t\n",
      "Training Epoch: 8 [26/565]\tLoss: 12.0248\tLR: 0.010000\n",
      "2.1871\t2.0621\t1.9555\t1.9064\t1.9232\t1.9905\t\n",
      "Training Epoch: 8 [42/565]\tLoss: 20.6690\tLR: 0.010000\n",
      "3.7438\t3.4926\t3.3421\t3.3114\t3.3490\t3.4301\t\n",
      "Training Epoch: 8 [58/565]\tLoss: 19.2456\tLR: 0.010000\n",
      "3.5328\t3.3349\t3.1549\t3.0483\t3.0505\t3.1243\t\n",
      "Training Epoch: 8 [74/565]\tLoss: 24.3863\tLR: 0.010000\n",
      "4.2821\t4.1260\t4.0030\t3.9572\t3.9651\t4.0528\t\n",
      "Training Epoch: 8 [90/565]\tLoss: 24.9829\tLR: 0.010000\n",
      "4.5713\t4.4037\t4.1908\t4.0160\t3.9118\t3.8893\t\n",
      "Training Epoch: 8 [106/565]\tLoss: 14.3751\tLR: 0.010000\n",
      "2.2371\t2.2198\t2.2592\t2.3759\t2.5457\t2.7374\t\n",
      "Training Epoch: 8 [122/565]\tLoss: 17.1562\tLR: 0.010000\n",
      "2.9495\t2.8467\t2.7934\t2.7910\t2.8320\t2.9436\t\n",
      "Training Epoch: 8 [138/565]\tLoss: 24.9627\tLR: 0.010000\n",
      "4.7394\t4.5068\t4.2074\t3.9649\t3.8074\t3.7368\t\n",
      "Training Epoch: 8 [154/565]\tLoss: 20.1110\tLR: 0.010000\n",
      "3.6275\t3.4010\t3.2737\t3.2065\t3.2476\t3.3547\t\n",
      "Training Epoch: 8 [170/565]\tLoss: 27.4915\tLR: 0.010000\n",
      "4.7458\t4.6471\t4.5285\t4.4677\t4.5003\t4.6021\t\n",
      "Training Epoch: 8 [186/565]\tLoss: 17.2764\tLR: 0.010000\n",
      "3.0361\t2.9214\t2.8183\t2.7760\t2.8140\t2.9106\t\n",
      "Training Epoch: 8 [202/565]\tLoss: 23.5331\tLR: 0.010000\n",
      "4.6113\t4.2746\t3.9338\t3.6876\t3.5528\t3.4729\t\n",
      "Training Epoch: 8 [218/565]\tLoss: 21.4680\tLR: 0.010000\n",
      "3.8941\t3.7358\t3.5678\t3.4414\t3.4022\t3.4267\t\n",
      "Training Epoch: 8 [234/565]\tLoss: 25.9249\tLR: 0.010000\n",
      "4.6356\t4.4392\t4.2880\t4.1938\t4.1704\t4.1978\t\n",
      "Training Epoch: 8 [250/565]\tLoss: 19.3052\tLR: 0.010000\n",
      "3.6812\t3.4499\t3.2003\t3.0377\t2.9550\t2.9811\t\n",
      "Training Epoch: 8 [266/565]\tLoss: 24.9325\tLR: 0.010000\n",
      "4.7390\t4.4748\t4.2070\t3.9733\t3.8050\t3.7334\t\n",
      "Training Epoch: 8 [282/565]\tLoss: 19.4107\tLR: 0.010000\n",
      "3.2373\t3.1552\t3.1305\t3.1724\t3.2675\t3.4477\t\n",
      "Training Epoch: 8 [298/565]\tLoss: 25.0306\tLR: 0.010000\n",
      "4.7337\t4.4468\t4.1589\t3.9529\t3.8588\t3.8794\t\n",
      "Training Epoch: 8 [314/565]\tLoss: 18.1775\tLR: 0.010000\n",
      "3.0764\t2.9445\t2.8928\t2.9411\t3.0679\t3.2549\t\n",
      "Training Epoch: 8 [330/565]\tLoss: 20.7366\tLR: 0.010000\n",
      "3.9647\t3.6999\t3.4439\t3.2544\t3.1773\t3.1964\t\n",
      "Training Epoch: 8 [346/565]\tLoss: 20.9233\tLR: 0.010000\n",
      "3.6899\t3.5295\t3.4210\t3.3737\t3.4053\t3.5040\t\n",
      "Training Epoch: 8 [362/565]\tLoss: 27.1631\tLR: 0.010000\n",
      "5.2302\t4.8930\t4.5495\t4.2869\t4.1236\t4.0798\t\n",
      "Training Epoch: 8 [378/565]\tLoss: 18.0926\tLR: 0.010000\n",
      "3.5326\t3.2729\t3.0204\t2.8405\t2.7284\t2.6978\t\n",
      "Training Epoch: 8 [394/565]\tLoss: 28.7945\tLR: 0.010000\n",
      "5.4261\t5.1275\t4.8189\t4.5919\t4.4425\t4.3876\t\n",
      "Training Epoch: 8 [410/565]\tLoss: 27.1614\tLR: 0.010000\n",
      "4.7999\t4.6535\t4.5139\t4.4260\t4.3821\t4.3860\t\n",
      "Training Epoch: 8 [426/565]\tLoss: 25.0343\tLR: 0.010000\n",
      "4.9331\t4.4869\t4.1405\t3.9117\t3.7858\t3.7764\t\n",
      "Training Epoch: 8 [442/565]\tLoss: 18.0007\tLR: 0.010000\n",
      "3.0493\t2.9316\t2.8830\t2.9243\t3.0208\t3.1917\t\n",
      "Training Epoch: 8 [458/565]\tLoss: 19.2217\tLR: 0.010000\n",
      "3.4542\t3.2766\t3.1500\t3.0814\t3.0778\t3.1817\t\n",
      "Training Epoch: 8 [474/565]\tLoss: 34.1362\tLR: 0.010000\n",
      "6.5177\t6.1213\t5.7519\t5.4567\t5.2276\t5.0610\t\n",
      "Training Epoch: 8 [490/565]\tLoss: 23.9084\tLR: 0.010000\n",
      "4.6556\t4.2982\t3.9322\t3.7025\t3.6300\t3.6898\t\n",
      "Training Epoch: 8 [506/565]\tLoss: 22.0710\tLR: 0.010000\n",
      "3.9283\t3.6710\t3.5261\t3.5283\t3.6181\t3.7993\t\n",
      "Training Epoch: 8 [522/565]\tLoss: 20.2023\tLR: 0.010000\n",
      "3.6910\t3.4576\t3.2772\t3.1893\t3.2223\t3.3648\t\n",
      "Training Epoch: 8 [538/565]\tLoss: 22.5084\tLR: 0.010000\n",
      "3.9580\t3.7881\t3.6721\t3.6375\t3.6733\t3.7794\t\n",
      "Training Epoch: 8 [554/565]\tLoss: 21.0203\tLR: 0.010000\n",
      "3.8483\t3.6086\t3.4214\t3.3385\t3.3422\t3.4613\t\n",
      "Training Epoch: 8 [570/565]\tLoss: 23.5080\tLR: 0.010000\n",
      "4.3847\t4.1251\t3.8868\t3.7221\t3.6702\t3.7191\t\n",
      "Training Epoch: 8 [586/565]\tLoss: 30.1738\tLR: 0.010000\n",
      "5.5093\t5.2578\t5.0185\t4.8556\t4.7706\t4.7621\t\n",
      "Training Epoch: 8 [602/565]\tLoss: 14.0478\tLR: 0.010000\n",
      "2.1406\t2.1373\t2.2027\t2.3453\t2.5180\t2.7039\t\n",
      "Training Epoch: 8 [618/565]\tLoss: 18.5135\tLR: 0.010000\n",
      "3.3061\t3.0844\t2.9405\t2.9223\t3.0351\t3.2251\t\n",
      "Training Epoch: 8 [634/565]\tLoss: 13.2840\tLR: 0.010000\n",
      "2.4481\t2.3465\t2.2163\t2.1147\t2.0602\t2.0982\t\n",
      "Training Epoch: 8 [650/565]\tLoss: 16.6619\tLR: 0.010000\n",
      "3.2782\t2.9897\t2.7431\t2.5761\t2.5158\t2.5590\t\n",
      "Training Epoch: 8 [666/565]\tLoss: 19.3435\tLR: 0.010000\n",
      "3.1372\t3.0459\t3.0406\t3.1424\t3.3475\t3.6299\t\n",
      "Training Epoch: 8 [682/565]\tLoss: 19.7669\tLR: 0.010000\n",
      "3.6658\t3.4057\t3.2097\t3.1198\t3.1295\t3.2365\t\n",
      "Training Epoch: 8 [698/565]\tLoss: 23.6714\tLR: 0.010000\n",
      "4.5060\t4.2046\t3.9343\t3.7224\t3.6333\t3.6708\t\n",
      "Training Epoch: 8 [714/565]\tLoss: 26.3210\tLR: 0.010000\n",
      "4.8497\t4.5863\t4.3473\t4.2072\t4.1479\t4.1827\t\n",
      "Training Epoch: 8 [730/565]\tLoss: 29.2285\tLR: 0.010000\n",
      "5.5146\t5.1602\t4.8614\t4.6435\t4.5308\t4.5179\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [746/565]\tLoss: 21.8896\tLR: 0.010000\n",
      "3.8935\t3.7123\t3.5773\t3.5287\t3.5533\t3.6244\t\n",
      "Training Epoch: 8 [762/565]\tLoss: 14.9917\tLR: 0.010000\n",
      "2.3182\t2.3228\t2.3745\t2.4841\t2.6391\t2.8530\t\n",
      "Training Epoch: 8 [778/565]\tLoss: 19.7708\tLR: 0.010000\n",
      "3.9006\t3.5408\t3.1995\t3.0163\t3.0044\t3.1093\t\n",
      "Training Epoch: 8 [794/565]\tLoss: 20.2982\tLR: 0.010000\n",
      "3.6491\t3.4256\t3.2969\t3.2729\t3.2973\t3.3564\t\n",
      "Training Epoch: 8 [810/565]\tLoss: 17.0729\tLR: 0.010000\n",
      "2.9291\t2.8058\t2.7545\t2.7707\t2.8441\t2.9687\t\n",
      "Training Epoch: 8 [826/565]\tLoss: 20.6023\tLR: 0.010000\n",
      "3.5850\t3.4077\t3.3140\t3.3347\t3.4266\t3.5343\t\n",
      "Training Epoch: 8 [842/565]\tLoss: 16.1552\tLR: 0.010000\n",
      "2.7994\t2.6908\t2.6116\t2.5932\t2.6549\t2.8052\t\n",
      "Training Epoch: 8 [858/565]\tLoss: 29.9865\tLR: 0.010000\n",
      "5.5487\t5.2397\t4.9750\t4.7973\t4.7188\t4.7071\t\n",
      "Training Epoch: 8 [874/565]\tLoss: 21.5129\tLR: 0.010000\n",
      "3.9171\t3.7121\t3.5392\t3.4469\t3.4308\t3.4667\t\n",
      "Training Epoch: 8 [890/565]\tLoss: 16.1601\tLR: 0.010000\n",
      "2.9636\t2.7719\t2.6266\t2.5496\t2.5752\t2.6732\t\n",
      "Training Epoch: 8 [901/565]\tLoss: 9.5299\tLR: 0.010000\n",
      "1.4435\t1.4563\t1.4679\t1.5364\t1.6837\t1.9421\t\n",
      "[0.2825697064399719, 0.5099746584892273, 0.20745563507080078, 0.016951588913798332, 0.6599186062812805, 0.12812605500221252, 0.21195533871650696, 0.019544802606105804, 0.5960625410079956, 0.15073341131210327, 0.2532040476799011, -0.004504520446062088, 0.3660394251346588, 0.2372153252363205, 0.3967452496290207, -0.01074861641973257, 0.16205234825611115, 0.0, 0.8379476517438889, -0.003751183394342661]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3889, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3682, Accuracy: 0.3398\n",
      "Test set t = 02: Average loss: 0.3511, Accuracy: 0.3398\n",
      "Test set t = 03: Average loss: 0.3421, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3415, Accuracy: 0.3310\n",
      "Test set t = 05: Average loss: 0.3488, Accuracy: 0.3204\n",
      "\n",
      "Training Epoch: 9 [10/565]\tLoss: 18.8568\tLR: 0.010000\n",
      "3.6679\t3.4028\t3.1353\t2.9390\t2.8556\t2.8561\t\n",
      "Training Epoch: 9 [26/565]\tLoss: 13.6590\tLR: 0.010000\n",
      "2.3659\t2.2660\t2.1804\t2.1752\t2.2471\t2.4244\t\n",
      "Training Epoch: 9 [42/565]\tLoss: 25.6205\tLR: 0.010000\n",
      "4.9033\t4.5902\t4.2823\t4.0408\t3.9176\t3.8862\t\n",
      "Training Epoch: 9 [58/565]\tLoss: 21.3292\tLR: 0.010000\n",
      "3.9358\t3.7539\t3.5400\t3.4019\t3.3481\t3.3494\t\n",
      "Training Epoch: 9 [74/565]\tLoss: 20.9517\tLR: 0.010000\n",
      "3.6562\t3.5046\t3.4056\t3.3932\t3.4331\t3.5590\t\n",
      "Training Epoch: 9 [90/565]\tLoss: 27.2178\tLR: 0.010000\n",
      "4.8747\t4.6701\t4.5040\t4.4150\t4.3710\t4.3830\t\n",
      "Training Epoch: 9 [106/565]\tLoss: 13.8641\tLR: 0.010000\n",
      "2.3275\t2.2263\t2.2003\t2.2529\t2.3468\t2.5104\t\n",
      "Training Epoch: 9 [122/565]\tLoss: 15.8768\tLR: 0.010000\n",
      "2.7057\t2.5957\t2.5300\t2.5441\t2.6549\t2.8463\t\n",
      "Training Epoch: 9 [138/565]\tLoss: 8.8760\tLR: 0.010000\n",
      "1.3561\t1.3196\t1.3442\t1.4297\t1.5897\t1.8366\t\n",
      "Training Epoch: 9 [154/565]\tLoss: 30.0086\tLR: 0.010000\n",
      "5.6683\t5.3395\t5.0359\t4.7942\t4.6260\t4.5447\t\n",
      "Training Epoch: 9 [170/565]\tLoss: 19.6495\tLR: 0.010000\n",
      "3.6224\t3.3912\t3.2273\t3.1399\t3.1157\t3.1530\t\n",
      "Training Epoch: 9 [186/565]\tLoss: 32.2127\tLR: 0.010000\n",
      "6.4237\t5.9113\t5.4134\t5.0403\t4.7729\t4.6513\t\n",
      "Training Epoch: 9 [202/565]\tLoss: 22.3512\tLR: 0.010000\n",
      "3.8245\t3.7210\t3.6487\t3.6385\t3.6940\t3.8245\t\n",
      "Training Epoch: 9 [218/565]\tLoss: 15.5641\tLR: 0.010000\n",
      "2.5243\t2.4562\t2.4618\t2.5365\t2.6841\t2.9013\t\n",
      "Training Epoch: 9 [234/565]\tLoss: 19.3064\tLR: 0.010000\n",
      "3.2954\t3.1973\t3.1237\t3.1149\t3.2011\t3.3741\t\n",
      "Training Epoch: 9 [250/565]\tLoss: 23.3439\tLR: 0.010000\n",
      "4.4830\t4.1824\t3.9059\t3.7075\t3.5709\t3.4942\t\n",
      "Training Epoch: 9 [266/565]\tLoss: 14.5820\tLR: 0.010000\n",
      "2.3775\t2.3652\t2.3516\t2.3849\t2.4805\t2.6222\t\n",
      "Training Epoch: 9 [282/565]\tLoss: 17.7901\tLR: 0.010000\n",
      "3.4955\t3.2055\t2.9427\t2.7667\t2.6814\t2.6983\t\n",
      "Training Epoch: 9 [298/565]\tLoss: 29.2872\tLR: 0.010000\n",
      "5.6746\t5.2773\t4.9105\t4.6227\t4.4468\t4.3553\t\n",
      "Training Epoch: 9 [314/565]\tLoss: 11.0915\tLR: 0.010000\n",
      "1.9680\t1.8408\t1.7463\t1.7360\t1.8163\t1.9841\t\n",
      "Training Epoch: 9 [330/565]\tLoss: 22.3974\tLR: 0.010000\n",
      "3.8714\t3.7595\t3.6698\t3.6573\t3.6820\t3.7575\t\n",
      "Training Epoch: 9 [346/565]\tLoss: 24.3786\tLR: 0.010000\n",
      "4.7534\t4.4026\t4.0870\t3.8423\t3.6872\t3.6060\t\n",
      "Training Epoch: 9 [362/565]\tLoss: 22.3443\tLR: 0.010000\n",
      "4.1344\t3.9231\t3.7026\t3.5484\t3.5006\t3.5350\t\n",
      "Training Epoch: 9 [378/565]\tLoss: 14.3033\tLR: 0.010000\n",
      "2.3062\t2.2709\t2.2683\t2.3158\t2.4629\t2.6793\t\n",
      "Training Epoch: 9 [394/565]\tLoss: 29.8275\tLR: 0.010000\n",
      "5.6734\t5.2974\t4.9524\t4.7475\t4.6171\t4.5396\t\n",
      "Training Epoch: 9 [410/565]\tLoss: 13.4813\tLR: 0.010000\n",
      "1.9603\t1.9890\t2.0977\t2.2689\t2.4619\t2.7036\t\n",
      "Training Epoch: 9 [426/565]\tLoss: 20.7742\tLR: 0.010000\n",
      "3.7696\t3.5867\t3.4396\t3.3413\t3.3018\t3.3351\t\n",
      "Training Epoch: 9 [442/565]\tLoss: 12.6721\tLR: 0.010000\n",
      "2.0280\t1.9956\t2.0125\t2.0736\t2.1919\t2.3705\t\n",
      "Training Epoch: 9 [458/565]\tLoss: 33.2366\tLR: 0.010000\n",
      "6.4960\t6.0358\t5.5955\t5.2629\t5.0075\t4.8389\t\n",
      "Training Epoch: 9 [474/565]\tLoss: 14.2271\tLR: 0.010000\n",
      "2.1421\t2.1838\t2.2575\t2.3701\t2.5322\t2.7414\t\n",
      "Training Epoch: 9 [490/565]\tLoss: 30.8140\tLR: 0.010000\n",
      "5.9388\t5.5423\t5.1573\t4.8827\t4.7036\t4.5895\t\n",
      "Training Epoch: 9 [506/565]\tLoss: 29.9178\tLR: 0.010000\n",
      "5.4960\t5.1857\t4.9317\t4.7892\t4.7460\t4.7691\t\n",
      "Training Epoch: 9 [522/565]\tLoss: 32.9873\tLR: 0.010000\n",
      "6.4292\t5.9388\t5.4921\t5.1753\t5.0044\t4.9475\t\n",
      "Training Epoch: 9 [538/565]\tLoss: 27.1946\tLR: 0.010000\n",
      "5.3555\t4.9751\t4.6027\t4.2723\t4.0461\t3.9428\t\n",
      "Training Epoch: 9 [554/565]\tLoss: 19.5997\tLR: 0.010000\n",
      "3.2198\t3.1619\t3.1509\t3.2093\t3.3368\t3.5210\t\n",
      "Training Epoch: 9 [570/565]\tLoss: 15.5754\tLR: 0.010000\n",
      "2.6822\t2.5301\t2.4673\t2.5064\t2.6211\t2.7684\t\n",
      "Training Epoch: 9 [586/565]\tLoss: 20.2286\tLR: 0.010000\n",
      "3.4221\t3.3185\t3.2615\t3.2818\t3.3939\t3.5510\t\n",
      "Training Epoch: 9 [602/565]\tLoss: 23.5648\tLR: 0.010000\n",
      "4.4449\t4.1833\t3.9192\t3.7344\t3.6414\t3.6417\t\n",
      "Training Epoch: 9 [618/565]\tLoss: 18.2433\tLR: 0.010000\n",
      "3.0165\t2.9630\t2.9380\t2.9780\t3.0827\t3.2651\t\n",
      "Training Epoch: 9 [634/565]\tLoss: 20.8247\tLR: 0.010000\n",
      "3.5957\t3.4253\t3.3329\t3.3614\t3.4717\t3.6377\t\n",
      "Training Epoch: 9 [650/565]\tLoss: 15.9327\tLR: 0.010000\n",
      "2.8394\t2.6788\t2.5685\t2.5187\t2.5757\t2.7515\t\n",
      "Training Epoch: 9 [666/565]\tLoss: 21.5301\tLR: 0.010000\n",
      "3.9703\t3.7554\t3.5651\t3.4455\t3.3895\t3.4044\t\n",
      "Training Epoch: 9 [682/565]\tLoss: 17.9328\tLR: 0.010000\n",
      "3.3992\t3.1531\t2.9437\t2.8225\t2.7889\t2.8255\t\n",
      "Training Epoch: 9 [698/565]\tLoss: 27.3872\tLR: 0.010000\n",
      "5.0447\t4.7481\t4.5152\t4.3838\t4.3411\t4.3544\t\n",
      "Training Epoch: 9 [714/565]\tLoss: 28.8549\tLR: 0.010000\n",
      "5.4019\t5.0632\t4.8003\t4.6179\t4.5026\t4.4692\t\n",
      "Training Epoch: 9 [730/565]\tLoss: 27.2969\tLR: 0.010000\n",
      "5.0246\t4.7437\t4.5196\t4.3804\t4.3191\t4.3094\t\n",
      "Training Epoch: 9 [746/565]\tLoss: 33.7498\tLR: 0.010000\n",
      "6.3743\t6.0087\t5.6567\t5.3841\t5.2030\t5.1229\t\n",
      "Training Epoch: 9 [762/565]\tLoss: 20.5998\tLR: 0.010000\n",
      "3.6612\t3.5358\t3.4042\t3.3255\t3.3157\t3.3574\t\n",
      "Training Epoch: 9 [778/565]\tLoss: 19.6323\tLR: 0.010000\n",
      "3.4057\t3.2783\t3.1813\t3.1589\t3.2279\t3.3803\t\n",
      "Training Epoch: 9 [794/565]\tLoss: 25.7042\tLR: 0.010000\n",
      "5.0245\t4.6912\t4.3244\t4.0314\t3.8580\t3.7747\t\n",
      "Training Epoch: 9 [810/565]\tLoss: 11.7157\tLR: 0.010000\n",
      "1.8054\t1.7578\t1.7985\t1.9138\t2.1089\t2.3313\t\n",
      "Training Epoch: 9 [826/565]\tLoss: 24.6174\tLR: 0.010000\n",
      "4.8017\t4.4562\t4.1177\t3.8671\t3.7114\t3.6633\t\n",
      "Training Epoch: 9 [842/565]\tLoss: 17.8814\tLR: 0.010000\n",
      "3.2899\t3.1041\t2.9517\t2.8451\t2.8164\t2.8742\t\n",
      "Training Epoch: 9 [858/565]\tLoss: 17.2821\tLR: 0.010000\n",
      "2.8024\t2.7658\t2.7793\t2.8502\t2.9683\t3.1162\t\n",
      "Training Epoch: 9 [874/565]\tLoss: 14.3089\tLR: 0.010000\n",
      "2.2997\t2.2524\t2.2517\t2.3222\t2.4710\t2.7120\t\n",
      "Training Epoch: 9 [890/565]\tLoss: 18.8594\tLR: 0.010000\n",
      "3.4398\t3.2356\t3.0778\t3.0121\t3.0094\t3.0847\t\n",
      "Training Epoch: 9 [901/565]\tLoss: 29.8533\tLR: 0.010000\n",
      "5.7856\t5.3992\t5.0292\t4.7021\t4.5117\t4.4256\t\n",
      "[0.2768765687942505, 0.5203253626823425, 0.20279806852340698, 0.017278287559747696, 0.6803615689277649, 0.11543449014425278, 0.20420394092798233, 0.02005992829799652, 0.6219286322593689, 0.14264602959156036, 0.23542533814907074, -0.005377535242587328, 0.37084925174713135, 0.23696638643741608, 0.3921843618154526, -0.012668246403336525, 0.16004902124404907, 0.0, 0.8399509787559509, -0.00507117947563529]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3889, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3680, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3510, Accuracy: 0.3398\n",
      "Test set t = 03: Average loss: 0.3419, Accuracy: 0.3363\n",
      "Test set t = 04: Average loss: 0.3413, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3485, Accuracy: 0.3204\n",
      "\n",
      "Training Epoch: 10 [10/565]\tLoss: 34.0420\tLR: 0.010000\n",
      "6.8883\t6.3668\t5.7912\t5.3185\t4.9624\t4.7149\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [26/565]\tLoss: 22.1331\tLR: 0.010000\n",
      "3.6817\t3.6157\t3.5779\t3.6181\t3.7502\t3.8896\t\n",
      "Training Epoch: 10 [42/565]\tLoss: 22.8423\tLR: 0.010000\n",
      "4.0582\t3.8644\t3.7349\t3.6969\t3.7163\t3.7716\t\n",
      "Training Epoch: 10 [58/565]\tLoss: 11.2259\tLR: 0.010000\n",
      "1.8507\t1.7859\t1.7722\t1.8031\t1.9095\t2.1044\t\n",
      "Training Epoch: 10 [74/565]\tLoss: 18.4785\tLR: 0.010000\n",
      "3.5760\t3.2399\t2.9896\t2.8602\t2.8459\t2.9667\t\n",
      "Training Epoch: 10 [90/565]\tLoss: 18.6762\tLR: 0.010000\n",
      "3.4316\t3.2200\t3.0693\t2.9958\t2.9569\t3.0027\t\n",
      "Training Epoch: 10 [106/565]\tLoss: 18.7219\tLR: 0.010000\n",
      "3.1416\t3.0415\t3.0091\t3.0505\t3.1585\t3.3206\t\n",
      "Training Epoch: 10 [122/565]\tLoss: 18.2377\tLR: 0.010000\n",
      "3.2024\t3.0499\t2.9734\t2.9523\t2.9885\t3.0713\t\n",
      "Training Epoch: 10 [138/565]\tLoss: 12.9072\tLR: 0.010000\n",
      "2.2259\t2.1242\t2.0608\t2.0647\t2.1471\t2.2845\t\n",
      "Training Epoch: 10 [154/565]\tLoss: 29.1939\tLR: 0.010000\n",
      "5.3435\t5.0270\t4.7802\t4.6421\t4.6503\t4.7509\t\n",
      "Training Epoch: 10 [170/565]\tLoss: 25.2301\tLR: 0.010000\n",
      "4.4239\t4.2484\t4.1324\t4.0779\t4.1134\t4.2340\t\n",
      "Training Epoch: 10 [186/565]\tLoss: 23.6947\tLR: 0.010000\n",
      "4.8829\t4.4386\t3.9936\t3.6584\t3.4251\t3.2962\t\n",
      "Training Epoch: 10 [202/565]\tLoss: 28.9401\tLR: 0.010000\n",
      "5.4166\t5.0691\t4.7833\t4.6083\t4.5251\t4.5376\t\n",
      "Training Epoch: 10 [218/565]\tLoss: 16.2373\tLR: 0.010000\n",
      "2.8625\t2.7400\t2.6464\t2.6106\t2.6479\t2.7300\t\n",
      "Training Epoch: 10 [234/565]\tLoss: 20.7341\tLR: 0.010000\n",
      "4.0146\t3.7329\t3.4811\t3.2761\t3.1307\t3.0987\t\n",
      "Training Epoch: 10 [250/565]\tLoss: 14.5350\tLR: 0.010000\n",
      "2.3914\t2.3182\t2.3037\t2.3615\t2.4847\t2.6755\t\n",
      "Training Epoch: 10 [266/565]\tLoss: 32.4864\tLR: 0.010000\n",
      "6.4741\t6.0372\t5.5152\t5.0907\t4.7897\t4.5795\t\n",
      "Training Epoch: 10 [282/565]\tLoss: 23.9033\tLR: 0.010000\n",
      "4.2809\t4.1184\t3.9645\t3.8690\t3.8258\t3.8447\t\n",
      "Training Epoch: 10 [298/565]\tLoss: 18.1387\tLR: 0.010000\n",
      "3.1566\t3.0158\t2.9342\t2.9134\t2.9791\t3.1395\t\n",
      "Training Epoch: 10 [314/565]\tLoss: 15.8029\tLR: 0.010000\n",
      "2.6540\t2.5446\t2.5161\t2.5520\t2.6659\t2.8703\t\n",
      "Training Epoch: 10 [330/565]\tLoss: 25.9270\tLR: 0.010000\n",
      "4.9211\t4.5829\t4.2961\t4.1047\t4.0073\t4.0149\t\n",
      "Training Epoch: 10 [346/565]\tLoss: 26.9446\tLR: 0.010000\n",
      "5.3921\t4.9584\t4.5464\t4.2131\t3.9829\t3.8518\t\n",
      "Training Epoch: 10 [362/565]\tLoss: 23.5540\tLR: 0.010000\n",
      "4.2705\t4.0340\t3.8467\t3.7461\t3.7773\t3.8794\t\n",
      "Training Epoch: 10 [378/565]\tLoss: 21.5165\tLR: 0.010000\n",
      "4.0215\t3.7603\t3.5517\t3.4337\t3.3685\t3.3809\t\n",
      "Training Epoch: 10 [394/565]\tLoss: 18.9282\tLR: 0.010000\n",
      "3.1650\t3.0582\t3.0359\t3.0701\t3.1817\t3.4174\t\n",
      "Training Epoch: 10 [410/565]\tLoss: 22.3920\tLR: 0.010000\n",
      "4.2485\t4.0206\t3.7635\t3.5540\t3.4108\t3.3947\t\n",
      "Training Epoch: 10 [426/565]\tLoss: 26.3706\tLR: 0.010000\n",
      "4.9550\t4.6377\t4.3832\t4.2103\t4.1175\t4.0669\t\n",
      "Training Epoch: 10 [442/565]\tLoss: 17.9902\tLR: 0.010000\n",
      "2.9557\t2.8719\t2.8397\t2.8992\t3.0742\t3.3496\t\n",
      "Training Epoch: 10 [458/565]\tLoss: 17.6173\tLR: 0.010000\n",
      "3.1597\t2.9981\t2.8600\t2.7974\t2.8312\t2.9709\t\n",
      "Training Epoch: 10 [474/565]\tLoss: 20.8770\tLR: 0.010000\n",
      "3.7844\t3.6015\t3.4377\t3.3457\t3.3348\t3.3729\t\n",
      "Training Epoch: 10 [490/565]\tLoss: 25.5076\tLR: 0.010000\n",
      "4.8524\t4.5147\t4.2029\t4.0187\t3.9378\t3.9810\t\n",
      "Training Epoch: 10 [506/565]\tLoss: 21.2190\tLR: 0.010000\n",
      "3.9882\t3.7396\t3.5241\t3.3745\t3.3036\t3.2891\t\n",
      "Training Epoch: 10 [522/565]\tLoss: 17.0387\tLR: 0.010000\n",
      "2.8922\t2.7823\t2.7513\t2.7868\t2.8582\t2.9679\t\n",
      "Training Epoch: 10 [538/565]\tLoss: 17.5399\tLR: 0.010000\n",
      "3.2402\t3.0240\t2.8516\t2.7646\t2.7753\t2.8843\t\n",
      "Training Epoch: 10 [554/565]\tLoss: 27.1228\tLR: 0.010000\n",
      "5.5082\t4.9971\t4.5092\t4.1659\t3.9856\t3.9568\t\n",
      "Training Epoch: 10 [570/565]\tLoss: 18.6429\tLR: 0.010000\n",
      "3.1517\t3.0522\t2.9983\t3.0540\t3.1316\t3.2550\t\n",
      "Training Epoch: 10 [586/565]\tLoss: 12.2070\tLR: 0.010000\n",
      "2.0326\t1.9493\t1.9148\t1.9630\t2.0881\t2.2593\t\n",
      "Training Epoch: 10 [602/565]\tLoss: 24.9225\tLR: 0.010000\n",
      "4.8706\t4.4581\t4.0958\t3.8696\t3.8000\t3.8285\t\n",
      "Training Epoch: 10 [618/565]\tLoss: 12.9909\tLR: 0.010000\n",
      "1.9417\t1.9728\t2.0416\t2.1555\t2.3219\t2.5574\t\n",
      "Training Epoch: 10 [634/565]\tLoss: 31.0023\tLR: 0.010000\n",
      "6.0592\t5.6107\t5.1774\t4.8712\t4.6770\t4.6068\t\n",
      "Training Epoch: 10 [650/565]\tLoss: 18.3717\tLR: 0.010000\n",
      "3.1752\t3.0549\t2.9634\t2.9469\t3.0286\t3.2027\t\n",
      "Training Epoch: 10 [666/565]\tLoss: 26.4196\tLR: 0.010000\n",
      "4.8149\t4.5760\t4.3721\t4.2389\t4.1991\t4.2187\t\n",
      "Training Epoch: 10 [682/565]\tLoss: 20.2988\tLR: 0.010000\n",
      "3.2831\t3.1797\t3.1947\t3.3336\t3.5312\t3.7765\t\n",
      "Training Epoch: 10 [698/565]\tLoss: 17.3825\tLR: 0.010000\n",
      "2.7640\t2.7565\t2.7917\t2.8763\t2.9978\t3.1962\t\n",
      "Training Epoch: 10 [714/565]\tLoss: 30.2766\tLR: 0.010000\n",
      "5.5726\t5.2777\t5.0195\t4.8536\t4.7829\t4.7702\t\n",
      "Training Epoch: 10 [730/565]\tLoss: 20.0603\tLR: 0.010000\n",
      "3.2705\t3.2150\t3.2228\t3.3067\t3.4423\t3.6030\t\n",
      "Training Epoch: 10 [746/565]\tLoss: 27.6334\tLR: 0.010000\n",
      "4.9958\t4.8179\t4.6170\t4.4631\t4.3842\t4.3553\t\n",
      "Training Epoch: 10 [762/565]\tLoss: 23.8086\tLR: 0.010000\n",
      "4.3066\t4.0644\t3.8861\t3.8054\t3.8269\t3.9193\t\n",
      "Training Epoch: 10 [778/565]\tLoss: 26.1085\tLR: 0.010000\n",
      "4.9085\t4.5478\t4.2917\t4.1383\t4.0910\t4.1313\t\n",
      "Training Epoch: 10 [794/565]\tLoss: 13.2667\tLR: 0.010000\n",
      "2.0863\t2.0376\t2.0502\t2.1438\t2.3445\t2.6043\t\n",
      "Training Epoch: 10 [810/565]\tLoss: 13.8945\tLR: 0.010000\n",
      "2.5514\t2.3554\t2.2256\t2.1842\t2.2220\t2.3559\t\n",
      "Training Epoch: 10 [826/565]\tLoss: 18.0061\tLR: 0.010000\n",
      "3.0961\t2.9929\t2.9124\t2.9096\t2.9791\t3.1159\t\n",
      "Training Epoch: 10 [842/565]\tLoss: 19.6456\tLR: 0.010000\n",
      "3.7529\t3.4995\t3.2473\t3.0866\t3.0206\t3.0386\t\n",
      "Training Epoch: 10 [858/565]\tLoss: 20.1434\tLR: 0.010000\n",
      "3.4419\t3.3169\t3.2792\t3.2917\t3.3491\t3.4647\t\n",
      "Training Epoch: 10 [874/565]\tLoss: 20.8424\tLR: 0.010000\n",
      "3.9342\t3.6715\t3.4386\t3.2724\t3.2400\t3.2858\t\n",
      "Training Epoch: 10 [890/565]\tLoss: 20.5870\tLR: 0.010000\n",
      "3.9334\t3.6303\t3.3737\t3.2250\t3.1758\t3.2488\t\n",
      "Training Epoch: 10 [901/565]\tLoss: 25.2969\tLR: 0.010000\n",
      "4.2162\t4.1038\t4.0839\t4.1415\t4.2837\t4.4677\t\n",
      "[0.27561941742897034, 0.5197319388389587, 0.20464864373207092, 0.017858358100056648, 0.6986702680587769, 0.10381761193275452, 0.19751212000846863, 0.02037043683230877, 0.6477203965187073, 0.13525627553462982, 0.2170233279466629, -0.0064822095446288586, 0.370342493057251, 0.2432052046060562, 0.3864523023366928, -0.014226775616407394, 0.15758149325847626, 0.0, 0.8424185067415237, -0.005592023022472858]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3914, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3704, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3533, Accuracy: 0.3381\n",
      "Test set t = 03: Average loss: 0.3440, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3428, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3491, Accuracy: 0.3257\n",
      "\n",
      "Training Epoch: 11 [10/565]\tLoss: 11.7222\tLR: 0.010000\n",
      "1.8791\t1.8479\t1.8402\t1.8996\t2.0288\t2.2267\t\n",
      "Training Epoch: 11 [26/565]\tLoss: 14.4348\tLR: 0.010000\n",
      "2.2977\t2.2541\t2.2609\t2.3532\t2.5171\t2.7517\t\n",
      "Training Epoch: 11 [42/565]\tLoss: 23.1575\tLR: 0.010000\n",
      "4.5518\t4.1968\t3.8526\t3.6141\t3.4783\t3.4640\t\n",
      "Training Epoch: 11 [58/565]\tLoss: 23.4219\tLR: 0.010000\n",
      "4.4877\t4.1882\t3.9162\t3.7221\t3.5825\t3.5252\t\n",
      "Training Epoch: 11 [74/565]\tLoss: 21.6676\tLR: 0.010000\n",
      "4.1297\t3.8740\t3.6208\t3.4133\t3.3223\t3.3075\t\n",
      "Training Epoch: 11 [90/565]\tLoss: 18.0809\tLR: 0.010000\n",
      "3.2496\t3.0845\t2.9449\t2.8625\t2.8998\t3.0396\t\n",
      "Training Epoch: 11 [106/565]\tLoss: 28.3603\tLR: 0.010000\n",
      "5.5051\t5.0873\t4.7325\t4.4794\t4.3209\t4.2352\t\n",
      "Training Epoch: 11 [122/565]\tLoss: 23.5823\tLR: 0.010000\n",
      "4.3731\t4.0989\t3.9028\t3.7672\t3.7133\t3.7270\t\n",
      "Training Epoch: 11 [138/565]\tLoss: 23.2754\tLR: 0.010000\n",
      "4.1301\t3.9344\t3.8256\t3.7784\t3.7771\t3.8298\t\n",
      "Training Epoch: 11 [154/565]\tLoss: 14.8755\tLR: 0.010000\n",
      "2.8320\t2.6402\t2.4611\t2.3383\t2.2808\t2.3232\t\n",
      "Training Epoch: 11 [170/565]\tLoss: 17.7789\tLR: 0.010000\n",
      "3.0913\t2.9911\t2.9221\t2.8926\t2.9142\t2.9676\t\n",
      "Training Epoch: 11 [186/565]\tLoss: 18.2520\tLR: 0.010000\n",
      "3.0907\t3.0160\t2.9620\t2.9594\t3.0330\t3.1909\t\n",
      "Training Epoch: 11 [202/565]\tLoss: 19.6265\tLR: 0.010000\n",
      "3.8165\t3.5387\t3.2832\t3.0967\t2.9771\t2.9143\t\n",
      "Training Epoch: 11 [218/565]\tLoss: 25.1846\tLR: 0.010000\n",
      "4.6891\t4.3810\t4.1244\t3.9705\t3.9601\t4.0595\t\n",
      "Training Epoch: 11 [234/565]\tLoss: 21.8938\tLR: 0.010000\n",
      "4.0531\t3.8411\t3.6353\t3.5001\t3.4300\t3.4341\t\n",
      "Training Epoch: 11 [250/565]\tLoss: 20.5941\tLR: 0.010000\n",
      "3.8789\t3.6393\t3.4131\t3.2499\t3.1859\t3.2269\t\n",
      "Training Epoch: 11 [266/565]\tLoss: 29.3924\tLR: 0.010000\n",
      "5.5050\t5.1612\t4.8764\t4.7070\t4.5911\t4.5518\t\n",
      "Training Epoch: 11 [282/565]\tLoss: 14.2528\tLR: 0.010000\n",
      "2.5860\t2.4276\t2.3084\t2.2603\t2.2910\t2.3796\t\n",
      "Training Epoch: 11 [298/565]\tLoss: 19.0499\tLR: 0.010000\n",
      "3.2850\t3.1828\t3.1126\t3.0998\t3.1369\t3.2328\t\n",
      "Training Epoch: 11 [314/565]\tLoss: 20.9910\tLR: 0.010000\n",
      "3.7522\t3.5986\t3.4424\t3.3718\t3.3848\t3.4412\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [330/565]\tLoss: 18.3795\tLR: 0.010000\n",
      "3.2231\t3.0708\t2.9667\t2.9371\t3.0044\t3.1775\t\n",
      "Training Epoch: 11 [346/565]\tLoss: 25.1513\tLR: 0.010000\n",
      "4.7445\t4.3833\t4.1122\t3.9662\t3.9424\t4.0026\t\n",
      "Training Epoch: 11 [362/565]\tLoss: 20.2755\tLR: 0.010000\n",
      "3.6901\t3.5123\t3.3398\t3.2431\t3.2165\t3.2736\t\n",
      "Training Epoch: 11 [378/565]\tLoss: 16.0681\tLR: 0.010000\n",
      "3.1272\t2.8901\t2.6848\t2.5364\t2.4341\t2.3954\t\n",
      "Training Epoch: 11 [394/565]\tLoss: 18.8952\tLR: 0.010000\n",
      "3.4886\t3.2980\t3.1177\t2.9958\t2.9675\t3.0276\t\n",
      "Training Epoch: 11 [410/565]\tLoss: 14.5498\tLR: 0.010000\n",
      "2.4513\t2.3552\t2.3120\t2.3422\t2.4615\t2.6275\t\n",
      "Training Epoch: 11 [426/565]\tLoss: 29.0066\tLR: 0.010000\n",
      "5.3045\t4.9645\t4.7573\t4.6631\t4.6325\t4.6846\t\n",
      "Training Epoch: 11 [442/565]\tLoss: 21.5429\tLR: 0.010000\n",
      "3.7719\t3.5825\t3.4670\t3.4606\t3.5477\t3.7132\t\n",
      "Training Epoch: 11 [458/565]\tLoss: 19.8102\tLR: 0.010000\n",
      "3.3543\t3.2464\t3.2083\t3.2448\t3.3233\t3.4331\t\n",
      "Training Epoch: 11 [474/565]\tLoss: 25.9022\tLR: 0.010000\n",
      "4.7833\t4.4868\t4.2529\t4.1252\t4.1031\t4.1508\t\n",
      "Training Epoch: 11 [490/565]\tLoss: 15.9906\tLR: 0.010000\n",
      "2.7389\t2.6220\t2.5809\t2.5992\t2.6673\t2.7823\t\n",
      "Training Epoch: 11 [506/565]\tLoss: 24.8919\tLR: 0.010000\n",
      "4.7991\t4.4783\t4.1444\t3.9097\t3.7975\t3.7629\t\n",
      "Training Epoch: 11 [522/565]\tLoss: 23.5710\tLR: 0.010000\n",
      "4.2299\t4.0146\t3.8496\t3.7924\t3.8106\t3.8739\t\n",
      "Training Epoch: 11 [538/565]\tLoss: 26.7008\tLR: 0.010000\n",
      "5.0286\t4.6994\t4.4294\t4.2365\t4.1625\t4.1445\t\n",
      "Training Epoch: 11 [554/565]\tLoss: 17.8662\tLR: 0.010000\n",
      "2.8843\t2.8318\t2.8563\t2.9372\t3.0721\t3.2846\t\n",
      "Training Epoch: 11 [570/565]\tLoss: 21.5251\tLR: 0.010000\n",
      "4.0021\t3.7325\t3.5222\t3.4102\t3.3969\t3.4612\t\n",
      "Training Epoch: 11 [586/565]\tLoss: 23.9063\tLR: 0.010000\n",
      "4.2779\t4.0658\t3.9333\t3.8674\t3.8579\t3.9041\t\n",
      "Training Epoch: 11 [602/565]\tLoss: 20.1850\tLR: 0.010000\n",
      "3.1367\t3.1212\t3.1880\t3.3428\t3.5662\t3.8301\t\n",
      "Training Epoch: 11 [618/565]\tLoss: 18.6808\tLR: 0.010000\n",
      "3.2972\t3.2075\t3.0931\t3.0087\t2.9962\t3.0780\t\n",
      "Training Epoch: 11 [634/565]\tLoss: 18.1815\tLR: 0.010000\n",
      "3.2580\t3.0661\t2.9469\t2.9142\t2.9495\t3.0469\t\n",
      "Training Epoch: 11 [650/565]\tLoss: 27.6072\tLR: 0.010000\n",
      "5.4185\t5.0761\t4.7012\t4.3508\t4.0919\t3.9688\t\n",
      "Training Epoch: 11 [666/565]\tLoss: 25.7534\tLR: 0.010000\n",
      "4.6369\t4.4434\t4.2779\t4.1570\t4.1054\t4.1327\t\n",
      "Training Epoch: 11 [682/565]\tLoss: 21.8929\tLR: 0.010000\n",
      "3.8829\t3.7074\t3.5727\t3.5326\t3.5711\t3.6262\t\n",
      "Training Epoch: 11 [698/565]\tLoss: 30.5633\tLR: 0.010000\n",
      "6.0097\t5.5613\t5.1441\t4.8140\t4.5832\t4.4510\t\n",
      "Training Epoch: 11 [714/565]\tLoss: 14.2998\tLR: 0.010000\n",
      "2.5959\t2.4600\t2.3201\t2.2672\t2.2855\t2.3712\t\n",
      "Training Epoch: 11 [730/565]\tLoss: 22.0903\tLR: 0.010000\n",
      "4.0992\t3.8647\t3.6556\t3.5234\t3.4619\t3.4855\t\n",
      "Training Epoch: 11 [746/565]\tLoss: 23.5445\tLR: 0.010000\n",
      "4.2720\t4.0592\t3.8852\t3.7775\t3.7471\t3.8036\t\n",
      "Training Epoch: 11 [762/565]\tLoss: 15.3823\tLR: 0.010000\n",
      "2.8011\t2.6383\t2.4971\t2.4293\t2.4526\t2.5638\t\n",
      "Training Epoch: 11 [778/565]\tLoss: 19.4001\tLR: 0.010000\n",
      "3.3114\t3.2454\t3.1935\t3.1636\t3.1912\t3.2950\t\n",
      "Training Epoch: 11 [794/565]\tLoss: 24.1430\tLR: 0.010000\n",
      "4.6562\t4.3520\t4.0350\t3.7910\t3.6655\t3.6432\t\n",
      "Training Epoch: 11 [810/565]\tLoss: 26.5852\tLR: 0.010000\n",
      "5.0261\t4.6939\t4.3917\t4.1974\t4.1268\t4.1493\t\n",
      "Training Epoch: 11 [826/565]\tLoss: 19.9737\tLR: 0.010000\n",
      "3.3824\t3.3043\t3.2513\t3.2465\t3.3307\t3.4584\t\n",
      "Training Epoch: 11 [842/565]\tLoss: 21.2035\tLR: 0.010000\n",
      "3.6334\t3.5194\t3.4774\t3.4918\t3.5137\t3.5678\t\n",
      "Training Epoch: 11 [858/565]\tLoss: 16.5949\tLR: 0.010000\n",
      "2.7030\t2.6585\t2.6504\t2.7357\t2.8415\t3.0059\t\n",
      "Training Epoch: 11 [874/565]\tLoss: 24.4030\tLR: 0.010000\n",
      "4.2965\t4.1090\t3.9900\t3.9410\t3.9758\t4.0907\t\n",
      "Training Epoch: 11 [890/565]\tLoss: 21.7825\tLR: 0.010000\n",
      "4.1749\t3.8549\t3.5765\t3.4249\t3.3651\t3.3862\t\n",
      "Training Epoch: 11 [901/565]\tLoss: 39.0823\tLR: 0.010000\n",
      "7.3756\t6.9737\t6.5569\t6.2628\t6.0424\t5.8709\t\n",
      "[0.2681078612804413, 0.5252417922019958, 0.20665034651756287, 0.018863070756196976, 0.705231785774231, 0.09565674513578415, 0.1991114690899849, 0.020653443410992622, 0.6669086813926697, 0.1299837827682495, 0.2031075358390808, -0.007068829610943794, 0.3713224232196808, 0.24639664590358734, 0.3822809308767319, -0.015769394114613533, 0.1559499204158783, 0.0, 0.8440500795841217, -0.0066967010498046875]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3923, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3710, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3538, Accuracy: 0.3363\n",
      "Test set t = 03: Average loss: 0.3444, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3430, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3492, Accuracy: 0.3257\n",
      "\n",
      "Training Epoch: 12 [10/565]\tLoss: 16.8345\tLR: 0.010000\n",
      "2.8232\t2.6730\t2.6366\t2.7191\t2.8824\t3.1002\t\n",
      "Training Epoch: 12 [26/565]\tLoss: 21.4464\tLR: 0.010000\n",
      "3.6987\t3.5713\t3.4809\t3.4700\t3.5458\t3.6796\t\n",
      "Training Epoch: 12 [42/565]\tLoss: 26.0267\tLR: 0.010000\n",
      "4.9204\t4.5809\t4.3159\t4.1419\t4.0514\t4.0161\t\n",
      "Training Epoch: 12 [58/565]\tLoss: 21.3396\tLR: 0.010000\n",
      "3.8148\t3.6132\t3.4742\t3.4314\t3.4539\t3.5522\t\n",
      "Training Epoch: 12 [74/565]\tLoss: 22.6699\tLR: 0.010000\n",
      "4.2782\t3.9782\t3.7239\t3.5625\t3.5284\t3.5987\t\n",
      "Training Epoch: 12 [90/565]\tLoss: 26.5904\tLR: 0.010000\n",
      "5.3532\t4.8888\t4.4797\t4.1444\t3.9155\t3.8088\t\n",
      "Training Epoch: 12 [106/565]\tLoss: 17.5887\tLR: 0.010000\n",
      "3.0818\t2.9407\t2.8410\t2.8145\t2.8890\t3.0216\t\n",
      "Training Epoch: 12 [122/565]\tLoss: 16.9390\tLR: 0.010000\n",
      "3.0203\t2.9002\t2.7898\t2.7198\t2.7176\t2.7911\t\n",
      "Training Epoch: 12 [138/565]\tLoss: 12.3392\tLR: 0.010000\n",
      "2.1971\t2.0400\t1.9388\t1.9319\t2.0267\t2.2048\t\n",
      "Training Epoch: 12 [154/565]\tLoss: 17.4580\tLR: 0.010000\n",
      "3.1234\t2.9819\t2.8534\t2.7954\t2.8080\t2.8960\t\n",
      "Training Epoch: 12 [170/565]\tLoss: 22.1909\tLR: 0.010000\n",
      "4.4154\t4.0081\t3.6586\t3.4391\t3.3361\t3.3337\t\n",
      "Training Epoch: 12 [186/565]\tLoss: 18.6088\tLR: 0.010000\n",
      "3.1588\t3.0705\t3.0106\t3.0057\t3.0940\t3.2691\t\n",
      "Training Epoch: 12 [202/565]\tLoss: 22.2749\tLR: 0.010000\n",
      "3.9342\t3.7857\t3.6560\t3.6050\t3.6127\t3.6813\t\n",
      "Training Epoch: 12 [218/565]\tLoss: 19.4621\tLR: 0.010000\n",
      "3.4399\t3.2708\t3.1536\t3.1249\t3.1833\t3.2895\t\n",
      "Training Epoch: 12 [234/565]\tLoss: 16.6981\tLR: 0.010000\n",
      "2.9046\t2.8152\t2.7377\t2.7039\t2.7165\t2.8202\t\n",
      "Training Epoch: 12 [250/565]\tLoss: 15.0296\tLR: 0.010000\n",
      "2.5653\t2.4591\t2.3773\t2.4004\t2.5230\t2.7045\t\n",
      "Training Epoch: 12 [266/565]\tLoss: 23.3413\tLR: 0.010000\n",
      "4.2635\t4.0605\t3.8726\t3.7559\t3.6992\t3.6896\t\n",
      "Training Epoch: 12 [282/565]\tLoss: 31.2510\tLR: 0.010000\n",
      "5.8267\t5.5262\t5.2328\t5.0103\t4.8599\t4.7952\t\n",
      "Training Epoch: 12 [298/565]\tLoss: 24.1938\tLR: 0.010000\n",
      "4.2138\t4.0537\t3.9302\t3.8979\t3.9669\t4.1313\t\n",
      "Training Epoch: 12 [314/565]\tLoss: 15.6636\tLR: 0.010000\n",
      "2.6283\t2.5524\t2.5190\t2.5530\t2.6371\t2.7739\t\n",
      "Training Epoch: 12 [330/565]\tLoss: 21.1111\tLR: 0.010000\n",
      "3.9465\t3.6958\t3.4775\t3.3276\t3.2949\t3.3688\t\n",
      "Training Epoch: 12 [346/565]\tLoss: 18.4958\tLR: 0.010000\n",
      "3.5143\t3.2408\t3.0233\t2.8910\t2.8746\t2.9519\t\n",
      "Training Epoch: 12 [362/565]\tLoss: 24.7431\tLR: 0.010000\n",
      "4.7766\t4.4431\t4.1497\t3.9145\t3.7602\t3.6989\t\n",
      "Training Epoch: 12 [378/565]\tLoss: 29.8384\tLR: 0.010000\n",
      "5.7706\t5.3523\t4.9916\t4.7300\t4.5440\t4.4500\t\n",
      "Training Epoch: 12 [394/565]\tLoss: 23.6988\tLR: 0.010000\n",
      "4.0738\t3.9964\t3.9252\t3.8837\t3.8848\t3.9349\t\n",
      "Training Epoch: 12 [410/565]\tLoss: 22.8532\tLR: 0.010000\n",
      "4.0207\t3.7997\t3.6834\t3.6934\t3.7709\t3.8850\t\n",
      "Training Epoch: 12 [426/565]\tLoss: 21.2893\tLR: 0.010000\n",
      "3.9338\t3.6862\t3.4960\t3.3787\t3.3614\t3.4331\t\n",
      "Training Epoch: 12 [442/565]\tLoss: 19.5403\tLR: 0.010000\n",
      "3.2518\t3.1806\t3.1479\t3.1772\t3.2998\t3.4830\t\n",
      "Training Epoch: 12 [458/565]\tLoss: 28.1780\tLR: 0.010000\n",
      "5.5958\t5.1755\t4.7456\t4.4064\t4.1854\t4.0694\t\n",
      "Training Epoch: 12 [474/565]\tLoss: 12.9869\tLR: 0.010000\n",
      "2.1395\t2.0787\t2.0570\t2.1033\t2.2192\t2.3892\t\n",
      "Training Epoch: 12 [490/565]\tLoss: 31.4632\tLR: 0.010000\n",
      "6.0042\t5.5858\t5.2246\t4.9848\t4.8601\t4.8036\t\n",
      "Training Epoch: 12 [506/565]\tLoss: 12.5124\tLR: 0.010000\n",
      "2.0697\t1.9622\t1.9420\t2.0189\t2.1565\t2.3631\t\n",
      "Training Epoch: 12 [522/565]\tLoss: 24.9079\tLR: 0.010000\n",
      "4.2578\t4.1508\t4.0815\t4.0800\t4.1238\t4.2140\t\n",
      "Training Epoch: 12 [538/565]\tLoss: 24.4825\tLR: 0.010000\n",
      "4.3808\t4.2057\t4.0456\t3.9418\t3.9157\t3.9930\t\n",
      "Training Epoch: 12 [554/565]\tLoss: 14.4344\tLR: 0.010000\n",
      "2.2815\t2.2394\t2.2644\t2.3753\t2.5343\t2.7395\t\n",
      "Training Epoch: 12 [570/565]\tLoss: 22.3828\tLR: 0.010000\n",
      "4.0047\t3.8067\t3.6779\t3.6101\t3.6060\t3.6774\t\n",
      "Training Epoch: 12 [586/565]\tLoss: 26.9133\tLR: 0.010000\n",
      "5.2322\t4.8580\t4.5086\t4.2439\t4.0754\t3.9953\t\n",
      "Training Epoch: 12 [602/565]\tLoss: 16.2255\tLR: 0.010000\n",
      "2.7437\t2.6546\t2.6039\t2.6330\t2.7316\t2.8587\t\n",
      "Training Epoch: 12 [618/565]\tLoss: 10.1079\tLR: 0.010000\n",
      "1.7826\t1.6799\t1.6049\t1.5896\t1.6496\t1.8013\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [634/565]\tLoss: 27.8018\tLR: 0.010000\n",
      "5.5312\t5.0634\t4.6304\t4.3424\t4.1605\t4.0738\t\n",
      "Training Epoch: 12 [650/565]\tLoss: 30.4296\tLR: 0.010000\n",
      "5.8165\t5.4721\t5.1204\t4.8315\t4.6419\t4.5472\t\n",
      "Training Epoch: 12 [666/565]\tLoss: 26.2493\tLR: 0.010000\n",
      "4.6910\t4.4560\t4.2869\t4.2198\t4.2495\t4.3462\t\n",
      "Training Epoch: 12 [682/565]\tLoss: 14.8322\tLR: 0.010000\n",
      "2.8474\t2.6779\t2.5087\t2.3563\t2.2338\t2.2081\t\n",
      "Training Epoch: 12 [698/565]\tLoss: 31.4827\tLR: 0.010000\n",
      "6.2851\t5.8094\t5.3183\t4.9309\t4.6635\t4.4754\t\n",
      "Training Epoch: 12 [714/565]\tLoss: 15.7991\tLR: 0.010000\n",
      "2.8696\t2.6801\t2.5675\t2.5149\t2.5324\t2.6347\t\n",
      "Training Epoch: 12 [730/565]\tLoss: 11.4510\tLR: 0.010000\n",
      "1.8713\t1.8035\t1.7890\t1.8445\t1.9689\t2.1737\t\n",
      "Training Epoch: 12 [746/565]\tLoss: 25.2344\tLR: 0.010000\n",
      "4.8712\t4.5167\t4.2104\t3.9805\t3.8496\t3.8060\t\n",
      "Training Epoch: 12 [762/565]\tLoss: 24.0807\tLR: 0.010000\n",
      "4.6849\t4.3503\t4.0259\t3.7881\t3.6473\t3.5843\t\n",
      "Training Epoch: 12 [778/565]\tLoss: 21.8372\tLR: 0.010000\n",
      "3.9475\t3.7416\t3.6063\t3.5158\t3.4925\t3.5336\t\n",
      "Training Epoch: 12 [794/565]\tLoss: 26.5487\tLR: 0.010000\n",
      "4.8899\t4.5905\t4.3674\t4.2644\t4.2176\t4.2190\t\n",
      "Training Epoch: 12 [810/565]\tLoss: 19.6687\tLR: 0.010000\n",
      "3.3361\t3.2229\t3.1658\t3.1878\t3.2854\t3.4708\t\n",
      "Training Epoch: 12 [826/565]\tLoss: 19.0889\tLR: 0.010000\n",
      "3.4341\t3.2992\t3.1528\t3.0574\t3.0425\t3.1029\t\n",
      "Training Epoch: 12 [842/565]\tLoss: 18.2466\tLR: 0.010000\n",
      "3.1345\t3.0196\t2.9736\t2.9773\t3.0228\t3.1188\t\n",
      "Training Epoch: 12 [858/565]\tLoss: 22.7177\tLR: 0.010000\n",
      "3.9585\t3.7846\t3.6782\t3.6646\t3.7436\t3.8881\t\n",
      "Training Epoch: 12 [874/565]\tLoss: 14.4695\tLR: 0.010000\n",
      "2.2635\t2.2671\t2.2918\t2.3670\t2.5286\t2.7515\t\n",
      "Training Epoch: 12 [890/565]\tLoss: 25.9956\tLR: 0.010000\n",
      "4.6197\t4.4073\t4.2724\t4.2135\t4.2057\t4.2770\t\n",
      "Training Epoch: 12 [901/565]\tLoss: 30.1406\tLR: 0.010000\n",
      "5.7376\t5.2963\t4.9978\t4.7859\t4.6688\t4.6542\t\n",
      "[0.25939953327178955, 0.5314251780509949, 0.20917528867721558, 0.019780751317739487, 0.7168536186218262, 0.08817801624536514, 0.19496836513280869, 0.020785456523299217, 0.6889090538024902, 0.12418004125356674, 0.18691090494394302, -0.007925479672849178, 0.37565848231315613, 0.25337764620780945, 0.3709638714790344, -0.017270196229219437, 0.15972432494163513, 0.0, 0.8402756750583649, -0.007488531526178122]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3941, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3717, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3541, Accuracy: 0.3381\n",
      "Test set t = 03: Average loss: 0.3449, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3443, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3515, Accuracy: 0.3239\n",
      "\n",
      "Training Epoch: 13 [10/565]\tLoss: 18.9391\tLR: 0.010000\n",
      "3.2667\t3.1283\t3.0418\t3.0473\t3.1350\t3.3199\t\n",
      "Training Epoch: 13 [26/565]\tLoss: 15.4217\tLR: 0.010000\n",
      "2.4960\t2.4562\t2.4590\t2.5307\t2.6558\t2.8240\t\n",
      "Training Epoch: 13 [42/565]\tLoss: 23.1667\tLR: 0.010000\n",
      "4.1457\t3.9496\t3.7939\t3.7241\t3.7393\t3.8141\t\n",
      "Training Epoch: 13 [58/565]\tLoss: 23.6125\tLR: 0.010000\n",
      "4.3347\t4.1053\t3.9154\t3.7722\t3.7203\t3.7646\t\n",
      "Training Epoch: 13 [74/565]\tLoss: 18.6809\tLR: 0.010000\n",
      "3.4819\t3.2848\t3.1086\t2.9653\t2.9023\t2.9381\t\n",
      "Training Epoch: 13 [90/565]\tLoss: 21.3037\tLR: 0.010000\n",
      "3.8854\t3.6404\t3.4709\t3.4042\t3.4105\t3.4923\t\n",
      "Training Epoch: 13 [106/565]\tLoss: 27.5612\tLR: 0.010000\n",
      "5.3750\t4.9636\t4.6159\t4.3597\t4.1814\t4.0656\t\n",
      "Training Epoch: 13 [122/565]\tLoss: 16.1667\tLR: 0.010000\n",
      "2.8642\t2.7281\t2.6334\t2.6001\t2.6240\t2.7169\t\n",
      "Training Epoch: 13 [138/565]\tLoss: 16.1082\tLR: 0.010000\n",
      "2.6687\t2.5795\t2.5696\t2.6214\t2.7414\t2.9276\t\n",
      "Training Epoch: 13 [154/565]\tLoss: 26.7423\tLR: 0.010000\n",
      "4.8020\t4.5699\t4.3872\t4.3030\t4.3102\t4.3701\t\n",
      "Training Epoch: 13 [170/565]\tLoss: 23.7625\tLR: 0.010000\n",
      "4.5394\t4.1849\t3.8947\t3.7324\t3.6884\t3.7227\t\n",
      "Training Epoch: 13 [186/565]\tLoss: 15.7344\tLR: 0.010000\n",
      "2.5833\t2.5205\t2.5143\t2.5800\t2.7007\t2.8356\t\n",
      "Training Epoch: 13 [202/565]\tLoss: 18.3260\tLR: 0.010000\n",
      "3.0339\t2.9216\t2.9024\t2.9819\t3.1419\t3.3443\t\n",
      "Training Epoch: 13 [218/565]\tLoss: 21.9418\tLR: 0.010000\n",
      "3.9905\t3.7624\t3.5988\t3.5163\t3.5098\t3.5641\t\n",
      "Training Epoch: 13 [234/565]\tLoss: 21.1705\tLR: 0.010000\n",
      "3.7638\t3.5419\t3.3934\t3.3804\t3.4621\t3.6289\t\n",
      "Training Epoch: 13 [250/565]\tLoss: 18.0970\tLR: 0.010000\n",
      "3.2773\t3.0800\t2.9258\t2.8562\t2.9034\t3.0544\t\n",
      "Training Epoch: 13 [266/565]\tLoss: 27.5345\tLR: 0.010000\n",
      "5.5143\t5.0416\t4.6133\t4.2858\t4.0768\t4.0026\t\n",
      "Training Epoch: 13 [282/565]\tLoss: 14.2527\tLR: 0.010000\n",
      "2.5484\t2.4174\t2.2874\t2.2383\t2.2927\t2.4684\t\n",
      "Training Epoch: 13 [298/565]\tLoss: 21.4348\tLR: 0.010000\n",
      "3.9759\t3.7154\t3.5401\t3.4316\t3.3798\t3.3921\t\n",
      "Training Epoch: 13 [314/565]\tLoss: 17.9257\tLR: 0.010000\n",
      "3.1782\t3.0078\t2.8961\t2.8733\t2.9234\t3.0470\t\n",
      "Training Epoch: 13 [330/565]\tLoss: 16.0255\tLR: 0.010000\n",
      "2.7880\t2.6613\t2.5697\t2.5673\t2.6575\t2.7817\t\n",
      "Training Epoch: 13 [346/565]\tLoss: 25.9380\tLR: 0.010000\n",
      "4.7795\t4.5080\t4.2735\t4.1261\t4.0895\t4.1613\t\n",
      "Training Epoch: 13 [362/565]\tLoss: 19.4096\tLR: 0.010000\n",
      "3.4353\t3.2810\t3.1870\t3.1294\t3.1481\t3.2288\t\n",
      "Training Epoch: 13 [378/565]\tLoss: 22.0559\tLR: 0.010000\n",
      "4.4884\t4.0721\t3.6909\t3.4191\t3.2386\t3.1469\t\n",
      "Training Epoch: 13 [394/565]\tLoss: 14.8726\tLR: 0.010000\n",
      "2.3538\t2.3198\t2.3514\t2.4319\t2.5908\t2.8249\t\n",
      "Training Epoch: 13 [410/565]\tLoss: 23.3294\tLR: 0.010000\n",
      "4.7764\t4.3893\t3.9494\t3.5890\t3.3639\t3.2614\t\n",
      "Training Epoch: 13 [426/565]\tLoss: 29.4405\tLR: 0.010000\n",
      "5.6393\t5.2207\t4.8654\t4.6535\t4.5349\t4.5268\t\n",
      "Training Epoch: 13 [442/565]\tLoss: 24.1500\tLR: 0.010000\n",
      "4.0501\t3.9592\t3.9304\t3.9545\t4.0410\t4.2147\t\n",
      "Training Epoch: 13 [458/565]\tLoss: 17.4446\tLR: 0.010000\n",
      "3.1302\t2.9472\t2.8108\t2.7712\t2.8299\t2.9553\t\n",
      "Training Epoch: 13 [474/565]\tLoss: 16.9688\tLR: 0.010000\n",
      "3.0706\t2.8435\t2.6949\t2.6542\t2.7504\t2.9553\t\n",
      "Training Epoch: 13 [490/565]\tLoss: 21.8799\tLR: 0.010000\n",
      "4.1328\t3.9022\t3.6446\t3.4656\t3.3720\t3.3627\t\n",
      "Training Epoch: 13 [506/565]\tLoss: 22.4338\tLR: 0.010000\n",
      "3.9000\t3.7359\t3.6327\t3.6321\t3.7059\t3.8272\t\n",
      "Training Epoch: 13 [522/565]\tLoss: 17.0308\tLR: 0.010000\n",
      "2.8400\t2.7739\t2.7549\t2.7732\t2.8640\t3.0248\t\n",
      "Training Epoch: 13 [538/565]\tLoss: 21.6810\tLR: 0.010000\n",
      "3.8913\t3.7089\t3.5742\t3.5129\t3.4951\t3.4986\t\n",
      "Training Epoch: 13 [554/565]\tLoss: 23.6254\tLR: 0.010000\n",
      "4.5359\t4.2140\t3.9302\t3.7240\t3.6280\t3.5933\t\n",
      "Training Epoch: 13 [570/565]\tLoss: 27.1962\tLR: 0.010000\n",
      "5.0213\t4.7605\t4.5119\t4.3507\t4.2856\t4.2663\t\n",
      "Training Epoch: 13 [586/565]\tLoss: 16.7353\tLR: 0.010000\n",
      "2.9195\t2.7574\t2.6690\t2.6721\t2.7721\t2.9452\t\n",
      "Training Epoch: 13 [602/565]\tLoss: 15.9658\tLR: 0.010000\n",
      "2.7377\t2.6563\t2.5940\t2.5806\t2.6460\t2.7512\t\n",
      "Training Epoch: 13 [618/565]\tLoss: 27.0323\tLR: 0.010000\n",
      "5.2013\t4.8668\t4.5637\t4.2996\t4.1125\t3.9885\t\n",
      "Training Epoch: 13 [634/565]\tLoss: 25.6405\tLR: 0.010000\n",
      "4.9571\t4.6243\t4.3150\t4.0775\t3.8850\t3.7816\t\n",
      "Training Epoch: 13 [650/565]\tLoss: 26.9166\tLR: 0.010000\n",
      "4.9868\t4.7299\t4.4790\t4.2959\t4.2135\t4.2113\t\n",
      "Training Epoch: 13 [666/565]\tLoss: 20.9100\tLR: 0.010000\n",
      "3.7538\t3.5681\t3.4371\t3.3649\t3.3666\t3.4195\t\n",
      "Training Epoch: 13 [682/565]\tLoss: 32.4475\tLR: 0.010000\n",
      "6.2248\t5.8347\t5.4715\t5.1804\t4.9361\t4.8001\t\n",
      "Training Epoch: 13 [698/565]\tLoss: 22.2024\tLR: 0.010000\n",
      "3.7758\t3.6721\t3.6354\t3.6386\t3.6804\t3.8000\t\n",
      "Training Epoch: 13 [714/565]\tLoss: 24.4770\tLR: 0.010000\n",
      "4.2997\t4.1433\t4.0162\t3.9821\t3.9843\t4.0514\t\n",
      "Training Epoch: 13 [730/565]\tLoss: 14.5350\tLR: 0.010000\n",
      "2.3624\t2.3361\t2.3151\t2.3508\t2.4788\t2.6919\t\n",
      "Training Epoch: 13 [746/565]\tLoss: 17.3598\tLR: 0.010000\n",
      "2.8065\t2.7632\t2.7778\t2.8439\t2.9821\t3.1863\t\n",
      "Training Epoch: 13 [762/565]\tLoss: 27.4772\tLR: 0.010000\n",
      "5.1041\t4.8304\t4.5867\t4.4273\t4.2997\t4.2289\t\n",
      "Training Epoch: 13 [778/565]\tLoss: 18.7861\tLR: 0.010000\n",
      "3.2609\t3.1191\t3.0467\t3.0456\t3.1058\t3.2079\t\n",
      "Training Epoch: 13 [794/565]\tLoss: 16.2061\tLR: 0.010000\n",
      "2.9477\t2.7533\t2.5997\t2.5519\t2.6110\t2.7424\t\n",
      "Training Epoch: 13 [810/565]\tLoss: 21.7656\tLR: 0.010000\n",
      "3.9734\t3.7675\t3.6006\t3.5060\t3.4533\t3.4647\t\n",
      "Training Epoch: 13 [826/565]\tLoss: 18.0868\tLR: 0.010000\n",
      "3.1968\t3.0584\t2.9554\t2.9086\t2.9313\t3.0362\t\n",
      "Training Epoch: 13 [842/565]\tLoss: 22.5467\tLR: 0.010000\n",
      "4.5275\t4.1570\t3.8121\t3.5134\t3.3162\t3.2204\t\n",
      "Training Epoch: 13 [858/565]\tLoss: 25.7795\tLR: 0.010000\n",
      "4.7001\t4.4400\t4.2432\t4.1426\t4.1169\t4.1367\t\n",
      "Training Epoch: 13 [874/565]\tLoss: 24.2721\tLR: 0.010000\n",
      "4.9343\t4.4584\t4.0213\t3.7418\t3.5891\t3.5273\t\n",
      "Training Epoch: 13 [890/565]\tLoss: 21.2127\tLR: 0.010000\n",
      "3.7379\t3.5933\t3.4921\t3.4311\t3.4413\t3.5171\t\n",
      "Training Epoch: 13 [901/565]\tLoss: 26.3926\tLR: 0.010000\n",
      "4.7937\t4.5703\t4.3904\t4.2591\t4.1840\t4.1951\t\n",
      "[0.25693196058273315, 0.5276468992233276, 0.2154211401939392, 0.02008122391998768, 0.7226532101631165, 0.07988385856151581, 0.19746293127536774, 0.0209009051322937, 0.7059029340744019, 0.11941146105527878, 0.17468560487031937, -0.008613898418843746, 0.3783665597438812, 0.2539246678352356, 0.3677087724208832, -0.018798572942614555, 0.1570052057504654, 0.0, 0.8429947942495346, -0.008458446711301804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.3926, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3707, Accuracy: 0.3416\n",
      "Test set t = 02: Average loss: 0.3536, Accuracy: 0.3381\n",
      "Test set t = 03: Average loss: 0.3446, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3440, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3509, Accuracy: 0.3239\n",
      "\n",
      "Training Epoch: 14 [10/565]\tLoss: 22.7067\tLR: 0.010000\n",
      "4.1279\t3.8587\t3.6946\t3.6431\t3.6576\t3.7248\t\n",
      "Training Epoch: 14 [26/565]\tLoss: 24.1540\tLR: 0.010000\n",
      "4.4393\t4.2351\t4.0241\t3.8677\t3.7867\t3.8011\t\n",
      "Training Epoch: 14 [42/565]\tLoss: 20.6670\tLR: 0.010000\n",
      "3.3041\t3.2658\t3.3032\t3.4204\t3.5850\t3.7886\t\n",
      "Training Epoch: 14 [58/565]\tLoss: 10.9776\tLR: 0.010000\n",
      "1.6589\t1.6462\t1.6737\t1.7590\t1.9601\t2.2797\t\n",
      "Training Epoch: 14 [74/565]\tLoss: 15.9513\tLR: 0.010000\n",
      "2.7394\t2.6242\t2.5510\t2.5607\t2.6471\t2.8289\t\n",
      "Training Epoch: 14 [90/565]\tLoss: 16.2673\tLR: 0.010000\n",
      "2.7602\t2.6390\t2.5747\t2.6006\t2.7417\t2.9511\t\n",
      "Training Epoch: 14 [106/565]\tLoss: 19.3583\tLR: 0.010000\n",
      "3.2269\t3.1338\t3.1002\t3.1465\t3.2737\t3.4772\t\n",
      "Training Epoch: 14 [122/565]\tLoss: 19.1488\tLR: 0.010000\n",
      "3.4555\t3.2698\t3.1177\t3.0434\t3.0740\t3.1884\t\n",
      "Training Epoch: 14 [138/565]\tLoss: 9.2616\tLR: 0.010000\n",
      "1.4368\t1.4001\t1.3932\t1.4655\t1.6491\t1.9168\t\n",
      "Training Epoch: 14 [154/565]\tLoss: 25.9571\tLR: 0.010000\n",
      "5.1234\t4.7372\t4.3597\t4.0734\t3.8896\t3.7738\t\n",
      "Training Epoch: 14 [170/565]\tLoss: 19.6760\tLR: 0.010000\n",
      "3.6924\t3.4347\t3.2236\t3.0885\t3.0679\t3.1688\t\n",
      "Training Epoch: 14 [186/565]\tLoss: 27.1565\tLR: 0.010000\n",
      "5.8901\t5.3016\t4.7095\t4.1688\t3.7188\t3.3676\t\n",
      "Training Epoch: 14 [202/565]\tLoss: 10.3903\tLR: 0.010000\n",
      "1.4852\t1.5308\t1.6106\t1.7357\t1.9050\t2.1230\t\n",
      "Training Epoch: 14 [218/565]\tLoss: 18.0578\tLR: 0.010000\n",
      "3.3374\t3.1415\t2.9658\t2.8518\t2.8394\t2.9220\t\n",
      "Training Epoch: 14 [234/565]\tLoss: 20.2795\tLR: 0.010000\n",
      "3.4211\t3.3276\t3.2988\t3.3210\t3.3997\t3.5114\t\n",
      "Training Epoch: 14 [250/565]\tLoss: 16.4591\tLR: 0.010000\n",
      "2.7164\t2.6375\t2.6114\t2.6712\t2.8095\t3.0131\t\n",
      "Training Epoch: 14 [266/565]\tLoss: 24.3943\tLR: 0.010000\n",
      "4.4788\t4.2209\t4.0148\t3.9092\t3.8758\t3.8948\t\n",
      "Training Epoch: 14 [282/565]\tLoss: 16.1041\tLR: 0.010000\n",
      "2.6131\t2.6011\t2.6116\t2.6614\t2.7392\t2.8776\t\n",
      "Training Epoch: 14 [298/565]\tLoss: 22.4857\tLR: 0.010000\n",
      "4.1193\t3.8760\t3.6973\t3.5891\t3.5699\t3.6341\t\n",
      "Training Epoch: 14 [314/565]\tLoss: 22.1997\tLR: 0.010000\n",
      "4.1891\t3.8749\t3.6547\t3.5290\t3.4769\t3.4751\t\n",
      "Training Epoch: 14 [330/565]\tLoss: 35.6144\tLR: 0.010000\n",
      "7.0074\t6.4578\t5.9742\t5.6174\t5.3533\t5.2042\t\n",
      "Training Epoch: 14 [346/565]\tLoss: 23.9236\tLR: 0.010000\n",
      "4.6281\t4.3052\t3.9914\t3.7652\t3.6314\t3.6023\t\n",
      "Training Epoch: 14 [362/565]\tLoss: 18.7762\tLR: 0.010000\n",
      "3.3976\t3.2455\t3.1007\t3.0170\t2.9838\t3.0317\t\n",
      "Training Epoch: 14 [378/565]\tLoss: 24.6180\tLR: 0.010000\n",
      "4.5271\t4.2485\t4.0634\t3.9657\t3.9162\t3.8972\t\n",
      "Training Epoch: 14 [394/565]\tLoss: 23.0457\tLR: 0.010000\n",
      "4.1347\t3.9133\t3.7677\t3.7002\t3.7184\t3.8114\t\n",
      "Training Epoch: 14 [410/565]\tLoss: 17.1685\tLR: 0.010000\n",
      "3.2457\t3.0154\t2.8385\t2.7083\t2.6628\t2.6978\t\n",
      "Training Epoch: 14 [426/565]\tLoss: 19.2668\tLR: 0.010000\n",
      "3.1510\t3.0961\t3.1018\t3.1768\t3.2986\t3.4425\t\n",
      "Training Epoch: 14 [442/565]\tLoss: 25.9895\tLR: 0.010000\n",
      "5.0928\t4.6951\t4.3238\t4.0862\t3.9345\t3.8571\t\n",
      "Training Epoch: 14 [458/565]\tLoss: 21.3703\tLR: 0.010000\n",
      "4.0395\t3.8109\t3.5802\t3.3895\t3.2742\t3.2760\t\n",
      "Training Epoch: 14 [474/565]\tLoss: 14.5033\tLR: 0.010000\n",
      "2.2857\t2.2352\t2.2494\t2.3560\t2.5624\t2.8145\t\n",
      "Training Epoch: 14 [490/565]\tLoss: 27.4709\tLR: 0.010000\n",
      "5.2690\t4.9098\t4.6122\t4.3652\t4.1970\t4.1177\t\n",
      "Training Epoch: 14 [506/565]\tLoss: 21.5406\tLR: 0.010000\n",
      "3.7145\t3.5643\t3.4759\t3.4951\t3.5852\t3.7056\t\n",
      "Training Epoch: 14 [522/565]\tLoss: 25.5817\tLR: 0.010000\n",
      "4.8402\t4.5663\t4.2859\t4.0682\t3.9300\t3.8912\t\n",
      "Training Epoch: 14 [538/565]\tLoss: 30.2686\tLR: 0.010000\n",
      "5.7010\t5.3262\t5.0158\t4.8299\t4.7194\t4.6763\t\n",
      "Training Epoch: 14 [554/565]\tLoss: 21.9182\tLR: 0.010000\n",
      "4.1116\t3.8572\t3.6337\t3.4771\t3.4057\t3.4329\t\n",
      "Training Epoch: 14 [570/565]\tLoss: 27.2786\tLR: 0.010000\n",
      "4.8482\t4.6341\t4.5036\t4.4366\t4.4135\t4.4425\t\n",
      "Training Epoch: 14 [586/565]\tLoss: 10.0951\tLR: 0.010000\n",
      "1.6282\t1.5461\t1.5457\t1.6158\t1.7661\t1.9932\t\n",
      "Training Epoch: 14 [602/565]\tLoss: 26.9465\tLR: 0.010000\n",
      "4.8339\t4.6099\t4.4278\t4.3520\t4.3504\t4.3726\t\n",
      "Training Epoch: 14 [618/565]\tLoss: 24.8908\tLR: 0.010000\n",
      "4.5361\t4.3100\t4.1052\t3.9770\t3.9530\t4.0095\t\n",
      "Training Epoch: 14 [634/565]\tLoss: 29.2774\tLR: 0.010000\n",
      "5.4321\t5.1294\t4.8524\t4.6867\t4.5965\t4.5802\t\n",
      "Training Epoch: 14 [650/565]\tLoss: 23.5734\tLR: 0.010000\n",
      "4.4789\t4.2135\t3.9278\t3.7132\t3.6114\t3.6286\t\n",
      "Training Epoch: 14 [666/565]\tLoss: 26.1522\tLR: 0.010000\n",
      "4.8502\t4.5573\t4.3163\t4.1585\t4.1201\t4.1497\t\n",
      "Training Epoch: 14 [682/565]\tLoss: 24.2713\tLR: 0.010000\n",
      "4.8928\t4.4890\t4.1114\t3.7885\t3.5548\t3.4348\t\n",
      "Training Epoch: 14 [698/565]\tLoss: 24.4279\tLR: 0.010000\n",
      "4.5435\t4.2629\t4.0289\t3.8749\t3.8327\t3.8850\t\n",
      "Training Epoch: 14 [714/565]\tLoss: 25.3262\tLR: 0.010000\n",
      "4.2087\t4.1140\t4.0977\t4.1717\t4.2858\t4.4484\t\n",
      "Training Epoch: 14 [730/565]\tLoss: 24.4382\tLR: 0.010000\n",
      "4.6763\t4.3137\t4.0341\t3.8546\t3.7788\t3.7807\t\n",
      "Training Epoch: 14 [746/565]\tLoss: 9.4079\tLR: 0.010000\n",
      "1.1820\t1.2609\t1.3958\t1.5910\t1.8478\t2.1303\t\n",
      "Training Epoch: 14 [762/565]\tLoss: 24.6662\tLR: 0.010000\n",
      "4.5865\t4.2867\t4.0367\t3.9227\t3.8907\t3.9429\t\n",
      "Training Epoch: 14 [778/565]\tLoss: 25.4043\tLR: 0.010000\n",
      "4.9815\t4.5737\t4.2452\t3.9868\t3.8438\t3.7733\t\n",
      "Training Epoch: 14 [794/565]\tLoss: 19.3164\tLR: 0.010000\n",
      "3.7948\t3.4793\t3.2068\t3.0092\t2.9169\t2.9094\t\n",
      "Training Epoch: 14 [810/565]\tLoss: 14.7154\tLR: 0.010000\n",
      "2.5616\t2.4660\t2.3898\t2.3710\t2.4021\t2.5248\t\n",
      "Training Epoch: 14 [826/565]\tLoss: 27.5458\tLR: 0.010000\n",
      "5.2219\t4.8778\t4.5666\t4.3472\t4.2568\t4.2755\t\n",
      "Training Epoch: 14 [842/565]\tLoss: 12.2024\tLR: 0.010000\n",
      "2.1394\t2.0305\t1.9475\t1.9362\t2.0129\t2.1359\t\n",
      "Training Epoch: 14 [858/565]\tLoss: 24.4130\tLR: 0.010000\n",
      "4.4187\t4.1545\t3.9898\t3.9302\t3.9436\t3.9761\t\n",
      "Training Epoch: 14 [874/565]\tLoss: 13.1093\tLR: 0.010000\n",
      "2.0001\t1.9821\t2.0368\t2.1618\t2.3434\t2.5851\t\n",
      "Training Epoch: 14 [890/565]\tLoss: 22.0865\tLR: 0.010000\n",
      "3.8744\t3.7370\t3.6444\t3.6090\t3.5878\t3.6339\t\n",
      "Training Epoch: 14 [901/565]\tLoss: 25.1428\tLR: 0.010000\n",
      "4.6234\t4.3109\t4.0932\t3.9882\t4.0066\t4.1206\t\n",
      "[0.24796509742736816, 0.5325693488121033, 0.21946555376052856, 0.020808571949601173, 0.7266634702682495, 0.07298919558525085, 0.20034733414649963, 0.020978661254048347, 0.7257850766181946, 0.11351214349269867, 0.16070277988910675, -0.00929240882396698, 0.3811449706554413, 0.2611502408981323, 0.3577047884464264, -0.02029549330472946, 0.1607947200536728, 0.0, 0.8392052799463272, -0.00935432966798544]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3908, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3682, Accuracy: 0.3398\n",
      "Test set t = 02: Average loss: 0.3511, Accuracy: 0.3381\n",
      "Test set t = 03: Average loss: 0.3427, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3432, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3515, Accuracy: 0.3221\n",
      "\n",
      "Training Epoch: 15 [10/565]\tLoss: 15.2699\tLR: 0.010000\n",
      "2.7923\t2.6148\t2.4675\t2.4008\t2.4385\t2.5560\t\n",
      "Training Epoch: 15 [26/565]\tLoss: 20.5976\tLR: 0.010000\n",
      "3.7738\t3.5305\t3.3500\t3.2659\t3.2845\t3.3928\t\n",
      "Training Epoch: 15 [42/565]\tLoss: 14.1160\tLR: 0.010000\n",
      "2.3519\t2.2511\t2.2103\t2.2737\t2.4089\t2.6201\t\n",
      "Training Epoch: 15 [58/565]\tLoss: 22.6779\tLR: 0.010000\n",
      "4.4637\t4.0664\t3.7441\t3.5415\t3.4366\t3.4256\t\n",
      "Training Epoch: 15 [74/565]\tLoss: 14.8138\tLR: 0.010000\n",
      "2.6502\t2.4723\t2.3692\t2.3503\t2.4132\t2.5587\t\n",
      "Training Epoch: 15 [90/565]\tLoss: 20.7877\tLR: 0.010000\n",
      "3.8991\t3.6161\t3.4042\t3.2983\t3.2688\t3.3013\t\n",
      "Training Epoch: 15 [106/565]\tLoss: 18.1743\tLR: 0.010000\n",
      "3.3007\t3.1482\t3.0088\t2.9113\t2.8885\t2.9168\t\n",
      "Training Epoch: 15 [122/565]\tLoss: 19.1494\tLR: 0.010000\n",
      "3.2920\t3.1385\t3.0396\t3.0545\t3.1976\t3.4272\t\n",
      "Training Epoch: 15 [138/565]\tLoss: 37.3270\tLR: 0.010000\n",
      "7.4295\t6.8705\t6.3111\t5.8596\t5.5315\t5.3248\t\n",
      "Training Epoch: 15 [154/565]\tLoss: 27.8852\tLR: 0.010000\n",
      "5.1848\t4.8379\t4.5960\t4.4546\t4.4053\t4.4066\t\n",
      "Training Epoch: 15 [170/565]\tLoss: 14.3129\tLR: 0.010000\n",
      "2.4395\t2.3229\t2.2757\t2.3068\t2.4015\t2.5665\t\n",
      "Training Epoch: 15 [186/565]\tLoss: 20.4842\tLR: 0.010000\n",
      "3.4719\t3.3783\t3.3223\t3.3244\t3.4120\t3.5752\t\n",
      "Training Epoch: 15 [202/565]\tLoss: 18.0609\tLR: 0.010000\n",
      "3.4032\t3.1217\t2.9179\t2.8275\t2.8395\t2.9511\t\n",
      "Training Epoch: 15 [218/565]\tLoss: 26.7230\tLR: 0.010000\n",
      "4.8052\t4.5786\t4.4024\t4.2865\t4.2866\t4.3637\t\n",
      "Training Epoch: 15 [234/565]\tLoss: 15.8527\tLR: 0.010000\n",
      "2.5357\t2.5020\t2.5215\t2.6024\t2.7418\t2.9493\t\n",
      "Training Epoch: 15 [250/565]\tLoss: 28.1971\tLR: 0.010000\n",
      "5.2908\t4.9678\t4.6975\t4.4994\t4.3884\t4.3533\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [266/565]\tLoss: 17.2046\tLR: 0.010000\n",
      "3.1123\t2.9331\t2.7914\t2.7329\t2.7562\t2.8786\t\n",
      "Training Epoch: 15 [282/565]\tLoss: 21.8318\tLR: 0.010000\n",
      "3.9029\t3.6801\t3.5425\t3.5072\t3.5503\t3.6487\t\n",
      "Training Epoch: 15 [298/565]\tLoss: 16.6621\tLR: 0.010000\n",
      "2.9543\t2.7717\t2.6773\t2.6754\t2.7336\t2.8498\t\n",
      "Training Epoch: 15 [314/565]\tLoss: 21.0040\tLR: 0.010000\n",
      "3.5682\t3.4998\t3.4517\t3.4494\t3.4786\t3.5563\t\n",
      "Training Epoch: 15 [330/565]\tLoss: 13.3116\tLR: 0.010000\n",
      "1.8782\t1.9449\t2.0707\t2.2319\t2.4527\t2.7332\t\n",
      "Training Epoch: 15 [346/565]\tLoss: 20.7740\tLR: 0.010000\n",
      "3.6127\t3.4816\t3.3945\t3.3699\t3.4095\t3.5058\t\n",
      "Training Epoch: 15 [362/565]\tLoss: 24.9466\tLR: 0.010000\n",
      "4.6247\t4.3725\t4.1361\t3.9839\t3.9179\t3.9115\t\n",
      "Training Epoch: 15 [378/565]\tLoss: 23.4080\tLR: 0.010000\n",
      "4.3765\t4.1050\t3.8794\t3.7514\t3.6591\t3.6365\t\n",
      "Training Epoch: 15 [394/565]\tLoss: 27.9424\tLR: 0.010000\n",
      "5.4040\t5.0099\t4.6571\t4.4024\t4.2575\t4.2113\t\n",
      "Training Epoch: 15 [410/565]\tLoss: 13.4768\tLR: 0.010000\n",
      "2.1718\t2.1445\t2.1459\t2.2055\t2.3165\t2.4926\t\n",
      "Training Epoch: 15 [426/565]\tLoss: 23.6424\tLR: 0.010000\n",
      "4.0133\t3.8677\t3.8155\t3.8511\t3.9592\t4.1354\t\n",
      "Training Epoch: 15 [442/565]\tLoss: 17.9350\tLR: 0.010000\n",
      "3.2016\t3.0600\t2.9489\t2.8768\t2.8755\t2.9724\t\n",
      "Training Epoch: 15 [458/565]\tLoss: 25.0146\tLR: 0.010000\n",
      "4.9871\t4.5297\t4.1384\t3.8891\t3.7570\t3.7133\t\n",
      "Training Epoch: 15 [474/565]\tLoss: 19.1383\tLR: 0.010000\n",
      "3.4238\t3.2352\t3.1091\t3.0667\t3.0986\t3.2050\t\n",
      "Training Epoch: 15 [490/565]\tLoss: 15.0381\tLR: 0.010000\n",
      "2.4452\t2.3697\t2.3783\t2.4398\t2.5965\t2.8086\t\n",
      "Training Epoch: 15 [506/565]\tLoss: 15.2976\tLR: 0.010000\n",
      "2.3856\t2.3763\t2.4345\t2.5360\t2.6809\t2.8844\t\n",
      "Training Epoch: 15 [522/565]\tLoss: 20.8777\tLR: 0.010000\n",
      "3.9614\t3.6838\t3.4560\t3.2959\t3.2323\t3.2483\t\n",
      "Training Epoch: 15 [538/565]\tLoss: 16.4711\tLR: 0.010000\n",
      "2.9202\t2.7691\t2.6906\t2.6622\t2.6811\t2.7479\t\n",
      "Training Epoch: 15 [554/565]\tLoss: 24.3988\tLR: 0.010000\n",
      "4.3196\t4.1741\t4.0408\t3.9557\t3.9249\t3.9837\t\n",
      "Training Epoch: 15 [570/565]\tLoss: 14.8054\tLR: 0.010000\n",
      "2.4910\t2.4677\t2.4256\t2.4170\t2.4621\t2.5419\t\n",
      "Training Epoch: 15 [586/565]\tLoss: 27.2222\tLR: 0.010000\n",
      "5.0827\t4.7904\t4.5212\t4.3355\t4.2461\t4.2464\t\n",
      "Training Epoch: 15 [602/565]\tLoss: 22.6345\tLR: 0.010000\n",
      "4.3856\t4.0494\t3.7438\t3.5286\t3.4437\t3.4833\t\n",
      "Training Epoch: 15 [618/565]\tLoss: 24.6830\tLR: 0.010000\n",
      "4.4940\t4.2781\t4.0826\t3.9579\t3.9160\t3.9543\t\n",
      "Training Epoch: 15 [634/565]\tLoss: 29.0845\tLR: 0.010000\n",
      "5.6127\t5.2267\t4.8986\t4.6205\t4.4242\t4.3017\t\n",
      "Training Epoch: 15 [650/565]\tLoss: 25.1287\tLR: 0.010000\n",
      "4.6341\t4.3629\t4.1495\t4.0203\t3.9734\t3.9886\t\n",
      "Training Epoch: 15 [666/565]\tLoss: 18.0166\tLR: 0.010000\n",
      "3.0665\t2.9473\t2.8843\t2.8941\t3.0136\t3.2108\t\n",
      "Training Epoch: 15 [682/565]\tLoss: 27.5790\tLR: 0.010000\n",
      "5.0090\t4.7759\t4.5806\t4.4470\t4.3818\t4.3845\t\n",
      "Training Epoch: 15 [698/565]\tLoss: 30.4022\tLR: 0.010000\n",
      "5.9497\t5.4960\t5.0961\t4.8087\t4.5940\t4.4578\t\n",
      "Training Epoch: 15 [714/565]\tLoss: 22.3101\tLR: 0.010000\n",
      "3.9738\t3.7891\t3.6733\t3.6004\t3.6057\t3.6678\t\n",
      "Training Epoch: 15 [730/565]\tLoss: 26.1786\tLR: 0.010000\n",
      "4.9800\t4.6095\t4.3331\t4.1632\t4.0775\t4.0152\t\n",
      "Training Epoch: 15 [746/565]\tLoss: 19.8244\tLR: 0.010000\n",
      "3.5398\t3.3927\t3.2413\t3.1767\t3.1917\t3.2822\t\n",
      "Training Epoch: 15 [762/565]\tLoss: 23.3501\tLR: 0.010000\n",
      "4.2353\t3.9618\t3.8094\t3.7629\t3.7678\t3.8130\t\n",
      "Training Epoch: 15 [778/565]\tLoss: 25.9697\tLR: 0.010000\n",
      "4.8000\t4.5523\t4.3177\t4.1619\t4.0769\t4.0609\t\n",
      "Training Epoch: 15 [794/565]\tLoss: 27.4834\tLR: 0.010000\n",
      "5.1440\t4.8345\t4.5524\t4.3731\t4.2918\t4.2876\t\n",
      "Training Epoch: 15 [810/565]\tLoss: 24.9769\tLR: 0.010000\n",
      "4.9821\t4.5937\t4.2097\t3.9089\t3.7002\t3.5823\t\n",
      "Training Epoch: 15 [826/565]\tLoss: 16.7592\tLR: 0.010000\n",
      "3.0892\t2.8701\t2.7022\t2.6443\t2.6688\t2.7846\t\n",
      "Training Epoch: 15 [842/565]\tLoss: 27.3885\tLR: 0.010000\n",
      "5.4722\t5.0242\t4.5976\t4.2570\t4.0566\t3.9809\t\n",
      "Training Epoch: 15 [858/565]\tLoss: 15.8293\tLR: 0.010000\n",
      "2.7484\t2.6286\t2.5613\t2.5478\t2.6042\t2.7390\t\n",
      "Training Epoch: 15 [874/565]\tLoss: 16.2265\tLR: 0.010000\n",
      "2.7399\t2.6564\t2.6196\t2.6387\t2.7132\t2.8586\t\n",
      "Training Epoch: 15 [890/565]\tLoss: 23.0638\tLR: 0.010000\n",
      "4.2055\t3.9845\t3.8003\t3.7086\t3.6807\t3.6844\t\n",
      "Training Epoch: 15 [901/565]\tLoss: 6.0607\tLR: 0.010000\n",
      "0.7597\t0.7956\t0.8571\t0.9714\t1.1856\t1.4913\t\n",
      "[0.2430223524570465, 0.5313321948051453, 0.22564545273780823, 0.0217735655605793, 0.7280300259590149, 0.06768276542425156, 0.20428720861673355, 0.020777517929673195, 0.738155722618103, 0.11098669469356537, 0.1508575826883316, -0.009769958443939686, 0.382965087890625, 0.26467475295066833, 0.35236015915870667, -0.02190285176038742, 0.15585483610630035, 0.0, 0.8441451638936996, -0.010067291557788849]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3935, Accuracy: 0.3381\n",
      "Test set t = 01: Average loss: 0.3710, Accuracy: 0.3398\n",
      "Test set t = 02: Average loss: 0.3536, Accuracy: 0.3381\n",
      "Test set t = 03: Average loss: 0.3445, Accuracy: 0.3345\n",
      "Test set t = 04: Average loss: 0.3439, Accuracy: 0.3327\n",
      "Test set t = 05: Average loss: 0.3509, Accuracy: 0.3239\n",
      "\n",
      "=====================\n",
      "Babble2Spkr, for SNR 3.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py:260: UserWarning: Using a target size (torch.Size([6, 164, 400])) that is different to the input size (torch.Size([6, 1, 164, 400])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.prediction_error  = nn.functional.mse_loss(self.prd, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.3353, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3031, Accuracy: 0.4541\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3368, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3042, Accuracy: 0.4541\n",
      "Test set t = 02: Average loss: 0.2891, Accuracy: 0.4293\n",
      "Test set t = 03: Average loss: 0.3043, Accuracy: 0.4081\n",
      "Test set t = 04: Average loss: 0.3497, Accuracy: 0.3233\n",
      "Test set t = 05: Average loss: 0.4104, Accuracy: 0.2102\n",
      "\n",
      "Training Epoch: 1 [10/566]\tLoss: 16.5824\tLR: 0.010000\n",
      "2.9449\t2.5755\t2.4051\t2.4656\t2.7741\t3.4172\t\n",
      "Training Epoch: 1 [26/566]\tLoss: 17.8285\tLR: 0.010000\n",
      "2.8770\t2.6337\t2.5517\t2.7158\t3.2236\t3.8267\t\n",
      "Training Epoch: 1 [42/566]\tLoss: 15.8141\tLR: 0.010000\n",
      "2.2424\t2.2691\t2.3015\t2.4733\t2.9483\t3.5796\t\n",
      "Training Epoch: 1 [58/566]\tLoss: 21.9234\tLR: 0.010000\n",
      "3.3229\t3.1221\t3.1172\t3.4757\t4.1343\t4.7513\t\n",
      "Training Epoch: 1 [74/566]\tLoss: 20.7997\tLR: 0.010000\n",
      "3.9388\t3.3893\t3.0056\t2.9586\t3.4019\t4.1054\t\n",
      "Training Epoch: 1 [90/566]\tLoss: 24.2526\tLR: 0.010000\n",
      "5.0308\t4.3343\t3.7107\t3.4890\t3.7206\t3.9672\t\n",
      "Training Epoch: 1 [106/566]\tLoss: 18.2798\tLR: 0.010000\n",
      "2.5142\t2.4712\t2.5989\t2.9509\t3.4947\t4.2498\t\n",
      "Training Epoch: 1 [122/566]\tLoss: 18.0579\tLR: 0.010000\n",
      "2.5295\t2.4608\t2.5342\t2.7844\t3.4245\t4.3246\t\n",
      "Training Epoch: 1 [138/566]\tLoss: 17.4004\tLR: 0.010000\n",
      "3.5328\t3.0542\t2.5685\t2.3820\t2.6748\t3.1882\t\n",
      "Training Epoch: 1 [154/566]\tLoss: 23.6641\tLR: 0.010000\n",
      "4.8397\t4.2310\t3.6781\t3.4687\t3.6007\t3.8459\t\n",
      "Training Epoch: 1 [170/566]\tLoss: 23.7433\tLR: 0.010000\n",
      "4.7985\t4.2945\t3.7254\t3.4396\t3.5447\t3.9406\t\n",
      "Training Epoch: 1 [186/566]\tLoss: 22.5038\tLR: 0.010000\n",
      "4.0067\t3.6331\t3.3920\t3.4578\t3.7352\t4.2790\t\n",
      "Training Epoch: 1 [202/566]\tLoss: 21.4471\tLR: 0.010000\n",
      "3.7315\t3.3786\t3.3045\t3.4647\t3.6951\t3.8726\t\n",
      "Training Epoch: 1 [218/566]\tLoss: 17.2113\tLR: 0.010000\n",
      "2.4033\t2.2963\t2.3584\t2.7810\t3.3682\t4.0043\t\n",
      "Training Epoch: 1 [234/566]\tLoss: 27.3132\tLR: 0.010000\n",
      "5.4441\t4.9207\t4.4435\t4.1628\t4.0927\t4.2494\t\n",
      "Training Epoch: 1 [250/566]\tLoss: 22.2629\tLR: 0.010000\n",
      "3.6727\t3.5347\t3.5563\t3.6360\t3.7885\t4.0749\t\n",
      "Training Epoch: 1 [266/566]\tLoss: 13.8302\tLR: 0.010000\n",
      "2.6482\t2.2910\t2.0337\t2.0208\t2.2236\t2.6129\t\n",
      "Training Epoch: 1 [282/566]\tLoss: 15.3270\tLR: 0.010000\n",
      "2.3625\t2.2312\t2.3076\t2.4793\t2.7665\t3.1798\t\n",
      "Training Epoch: 1 [298/566]\tLoss: 15.1469\tLR: 0.010000\n",
      "2.4863\t2.3529\t2.2637\t2.3303\t2.6017\t3.1120\t\n",
      "Training Epoch: 1 [314/566]\tLoss: 10.0383\tLR: 0.010000\n",
      "1.1392\t1.1133\t1.1993\t1.5219\t2.1227\t2.9420\t\n",
      "Training Epoch: 1 [330/566]\tLoss: 21.0685\tLR: 0.010000\n",
      "3.6293\t3.3435\t3.2227\t3.2907\t3.5803\t4.0021\t\n",
      "Training Epoch: 1 [346/566]\tLoss: 15.6413\tLR: 0.010000\n",
      "2.3132\t2.1688\t2.2060\t2.5067\t2.9312\t3.5154\t\n",
      "Training Epoch: 1 [362/566]\tLoss: 15.6483\tLR: 0.010000\n",
      "2.9659\t2.6630\t2.4400\t2.3722\t2.4609\t2.7463\t\n",
      "Training Epoch: 1 [378/566]\tLoss: 19.4705\tLR: 0.010000\n",
      "3.5560\t3.3221\t3.0633\t2.9662\t3.1103\t3.4526\t\n",
      "Training Epoch: 1 [394/566]\tLoss: 14.3680\tLR: 0.010000\n",
      "1.7117\t1.7969\t2.0281\t2.4002\t2.9200\t3.5111\t\n",
      "Training Epoch: 1 [410/566]\tLoss: 22.7403\tLR: 0.010000\n",
      "4.8185\t4.2917\t3.8196\t3.4744\t3.2167\t3.1192\t\n",
      "Training Epoch: 1 [426/566]\tLoss: 19.8146\tLR: 0.010000\n",
      "3.8514\t3.5545\t3.2617\t3.0818\t3.0040\t3.0612\t\n",
      "Training Epoch: 1 [442/566]\tLoss: 19.5827\tLR: 0.010000\n",
      "3.7448\t3.4026\t3.1000\t2.9777\t3.0703\t3.2873\t\n",
      "Training Epoch: 1 [458/566]\tLoss: 26.7687\tLR: 0.010000\n",
      "5.1850\t4.6968\t4.3081\t4.1611\t4.1394\t4.2783\t\n",
      "Training Epoch: 1 [474/566]\tLoss: 21.7470\tLR: 0.010000\n",
      "4.0966\t3.8087\t3.4950\t3.3305\t3.3790\t3.6372\t\n",
      "Training Epoch: 1 [490/566]\tLoss: 22.9303\tLR: 0.010000\n",
      "4.5105\t4.1116\t3.7527\t3.5273\t3.4754\t3.5528\t\n",
      "Training Epoch: 1 [506/566]\tLoss: 20.4244\tLR: 0.010000\n",
      "3.5507\t3.3536\t3.2362\t3.2435\t3.3900\t3.6503\t\n",
      "Training Epoch: 1 [522/566]\tLoss: 18.6132\tLR: 0.010000\n",
      "3.1387\t3.0154\t2.9248\t2.9714\t3.1396\t3.4232\t\n",
      "Training Epoch: 1 [538/566]\tLoss: 20.2725\tLR: 0.010000\n",
      "3.7519\t3.4688\t3.1982\t3.1005\t3.2415\t3.5116\t\n",
      "Training Epoch: 1 [554/566]\tLoss: 18.7600\tLR: 0.010000\n",
      "2.8759\t2.8096\t2.9227\t3.1185\t3.3723\t3.6610\t\n",
      "Training Epoch: 1 [570/566]\tLoss: 12.6752\tLR: 0.010000\n",
      "2.1650\t2.0244\t1.9011\t1.9064\t2.1211\t2.5573\t\n",
      "Training Epoch: 1 [586/566]\tLoss: 22.3389\tLR: 0.010000\n",
      "3.9640\t3.7498\t3.5571\t3.5244\t3.6348\t3.9089\t\n",
      "Training Epoch: 1 [602/566]\tLoss: 19.3889\tLR: 0.010000\n",
      "2.9810\t2.8917\t2.9232\t3.1844\t3.5261\t3.8824\t\n",
      "Training Epoch: 1 [618/566]\tLoss: 29.4832\tLR: 0.010000\n",
      "6.0854\t5.5790\t5.0132\t4.5539\t4.2240\t4.0277\t\n",
      "Training Epoch: 1 [634/566]\tLoss: 24.1449\tLR: 0.010000\n",
      "4.8567\t4.4770\t4.0595\t3.7429\t3.5310\t3.4777\t\n",
      "Training Epoch: 1 [650/566]\tLoss: 18.3645\tLR: 0.010000\n",
      "3.4715\t3.2152\t2.9369\t2.8040\t2.8652\t3.0717\t\n",
      "Training Epoch: 1 [666/566]\tLoss: 12.5521\tLR: 0.010000\n",
      "1.7406\t1.7803\t1.8432\t2.0404\t2.3758\t2.7717\t\n",
      "Training Epoch: 1 [682/566]\tLoss: 11.1076\tLR: 0.010000\n",
      "1.7272\t1.6931\t1.7040\t1.7795\t1.9509\t2.2529\t\n",
      "Training Epoch: 1 [698/566]\tLoss: 22.1185\tLR: 0.010000\n",
      "4.7884\t4.2570\t3.6494\t3.2322\t3.0904\t3.1010\t\n",
      "Training Epoch: 1 [714/566]\tLoss: 17.4502\tLR: 0.010000\n",
      "2.9993\t2.9235\t2.8376\t2.8007\t2.8532\t3.0358\t\n",
      "Training Epoch: 1 [730/566]\tLoss: 16.8043\tLR: 0.010000\n",
      "2.7269\t2.5664\t2.5818\t2.7043\t2.9470\t3.2781\t\n",
      "Training Epoch: 1 [746/566]\tLoss: 15.7432\tLR: 0.010000\n",
      "2.9780\t2.7747\t2.5711\t2.4401\t2.3952\t2.5841\t\n",
      "Training Epoch: 1 [762/566]\tLoss: 7.2276\tLR: 0.010000\n",
      "0.9814\t0.9409\t0.9627\t1.0993\t1.3972\t1.8461\t\n",
      "Training Epoch: 1 [778/566]\tLoss: 18.7896\tLR: 0.010000\n",
      "3.2477\t3.1019\t2.9738\t2.9586\t3.0871\t3.4206\t\n",
      "Training Epoch: 1 [794/566]\tLoss: 25.5154\tLR: 0.010000\n",
      "5.2555\t4.7551\t4.2361\t3.9048\t3.7165\t3.6474\t\n",
      "Training Epoch: 1 [810/566]\tLoss: 7.0714\tLR: 0.010000\n",
      "1.0449\t0.9945\t1.0035\t1.1012\t1.3111\t1.6162\t\n",
      "Training Epoch: 1 [826/566]\tLoss: 19.7981\tLR: 0.010000\n",
      "3.8079\t3.5530\t3.2665\t3.0876\t3.0074\t3.0756\t\n",
      "Training Epoch: 1 [842/566]\tLoss: 22.3479\tLR: 0.010000\n",
      "4.1422\t3.8131\t3.5453\t3.4802\t3.5987\t3.7685\t\n",
      "Training Epoch: 1 [858/566]\tLoss: 15.1840\tLR: 0.010000\n",
      "2.6888\t2.5495\t2.3692\t2.3503\t2.4930\t2.7332\t\n",
      "Training Epoch: 1 [874/566]\tLoss: 12.1810\tLR: 0.010000\n",
      "2.1262\t2.0071\t1.9083\t1.9287\t2.0226\t2.1881\t\n",
      "Training Epoch: 1 [890/566]\tLoss: 15.2866\tLR: 0.010000\n",
      "2.8192\t2.5863\t2.4178\t2.3521\t2.4227\t2.6885\t\n",
      "Training Epoch: 1 [902/566]\tLoss: 22.1013\tLR: 0.010000\n",
      "3.9152\t3.6971\t3.5673\t3.5465\t3.5983\t3.7768\t\n",
      "[0.3448841869831085, 0.22743472456932068, 0.4276810884475708, 0.01297686155885458, 0.35818472504615784, 0.2166001945734024, 0.42521508038043976, 0.0103120943531394, 0.3532131612300873, 0.21532821655273438, 0.43145862221717834, 0.006320425309240818, 0.2993212640285492, 0.23871749639511108, 0.4619612395763397, 0.0069291433319449425, 0.2106793373823166, 0.0, 0.7893206626176834, 0.008130848407745361]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3362, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3160, Accuracy: 0.4488\n",
      "Test set t = 02: Average loss: 0.2972, Accuracy: 0.4505\n",
      "Test set t = 03: Average loss: 0.2891, Accuracy: 0.4276\n",
      "Test set t = 04: Average loss: 0.2931, Accuracy: 0.4187\n",
      "Test set t = 05: Average loss: 0.3085, Accuracy: 0.3993\n",
      "\n",
      "Training Epoch: 2 [10/566]\tLoss: 17.9882\tLR: 0.010000\n",
      "3.2609\t3.0470\t2.8408\t2.8111\t2.9118\t3.1167\t\n",
      "Training Epoch: 2 [26/566]\tLoss: 16.1399\tLR: 0.010000\n",
      "2.8448\t2.7400\t2.6263\t2.5713\t2.6064\t2.7512\t\n",
      "Training Epoch: 2 [42/566]\tLoss: 11.0843\tLR: 0.010000\n",
      "1.6720\t1.6858\t1.7195\t1.7812\t1.9530\t2.2727\t\n",
      "Training Epoch: 2 [58/566]\tLoss: 23.5869\tLR: 0.010000\n",
      "4.2044\t4.0035\t3.8450\t3.7943\t3.8265\t3.9132\t\n",
      "Training Epoch: 2 [74/566]\tLoss: 24.2290\tLR: 0.010000\n",
      "4.8053\t4.4385\t4.0473\t3.7537\t3.6135\t3.5708\t\n",
      "Training Epoch: 2 [90/566]\tLoss: 12.7905\tLR: 0.010000\n",
      "2.4272\t2.2400\t2.0861\t1.9840\t1.9697\t2.0834\t\n",
      "Training Epoch: 2 [106/566]\tLoss: 26.5347\tLR: 0.010000\n",
      "5.0779\t4.7540\t4.3904\t4.1901\t4.0885\t4.0338\t\n",
      "Training Epoch: 2 [122/566]\tLoss: 13.7220\tLR: 0.010000\n",
      "2.1429\t2.1049\t2.1346\t2.2408\t2.4156\t2.6832\t\n",
      "Training Epoch: 2 [138/566]\tLoss: 11.5110\tLR: 0.010000\n",
      "1.8747\t1.7656\t1.6898\t1.8025\t2.0395\t2.3389\t\n",
      "Training Epoch: 2 [154/566]\tLoss: 13.4895\tLR: 0.010000\n",
      "2.4439\t2.3029\t2.1419\t2.0845\t2.1685\t2.3479\t\n",
      "Training Epoch: 2 [170/566]\tLoss: 19.5156\tLR: 0.010000\n",
      "3.7839\t3.5063\t3.1884\t2.9944\t2.9655\t3.0771\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [186/566]\tLoss: 21.8828\tLR: 0.010000\n",
      "3.9003\t3.6981\t3.5132\t3.4852\t3.5609\t3.7251\t\n",
      "Training Epoch: 2 [202/566]\tLoss: 18.1761\tLR: 0.010000\n",
      "3.0927\t2.9661\t2.8932\t2.9243\t3.0398\t3.2601\t\n",
      "Training Epoch: 2 [218/566]\tLoss: 14.9583\tLR: 0.010000\n",
      "2.8218\t2.6449\t2.4582\t2.3148\t2.3007\t2.4179\t\n",
      "Training Epoch: 2 [234/566]\tLoss: 25.1124\tLR: 0.010000\n",
      "4.6872\t4.4563\t4.1597\t3.9775\t3.9150\t3.9167\t\n",
      "Training Epoch: 2 [250/566]\tLoss: 8.4951\tLR: 0.010000\n",
      "1.2588\t1.2426\t1.2635\t1.3566\t1.5435\t1.8302\t\n",
      "Training Epoch: 2 [266/566]\tLoss: 19.9822\tLR: 0.010000\n",
      "3.5178\t3.3578\t3.2205\t3.1951\t3.2521\t3.4390\t\n",
      "Training Epoch: 2 [282/566]\tLoss: 28.8563\tLR: 0.010000\n",
      "6.0584\t5.5217\t4.8959\t4.3947\t4.0785\t3.9072\t\n",
      "Training Epoch: 2 [298/566]\tLoss: 20.5948\tLR: 0.010000\n",
      "3.6602\t3.4316\t3.2728\t3.2537\t3.3739\t3.6026\t\n",
      "Training Epoch: 2 [314/566]\tLoss: 19.7733\tLR: 0.010000\n",
      "3.4072\t3.2685\t3.2067\t3.2109\t3.2684\t3.4118\t\n",
      "Training Epoch: 2 [330/566]\tLoss: 21.2960\tLR: 0.010000\n",
      "3.7376\t3.6028\t3.4449\t3.3832\t3.4610\t3.6666\t\n",
      "Training Epoch: 2 [346/566]\tLoss: 17.5836\tLR: 0.010000\n",
      "2.9830\t2.8874\t2.8048\t2.7848\t2.9427\t3.1809\t\n",
      "Training Epoch: 2 [362/566]\tLoss: 18.6102\tLR: 0.010000\n",
      "3.5502\t3.3290\t3.1098\t2.9408\t2.8374\t2.8429\t\n",
      "Training Epoch: 2 [378/566]\tLoss: 12.4685\tLR: 0.010000\n",
      "2.2315\t2.1531\t2.0342\t1.9520\t1.9956\t2.1020\t\n",
      "Training Epoch: 2 [394/566]\tLoss: 15.3488\tLR: 0.010000\n",
      "2.6640\t2.4705\t2.3749\t2.4095\t2.5925\t2.8374\t\n",
      "Training Epoch: 2 [410/566]\tLoss: 21.0792\tLR: 0.010000\n",
      "3.9199\t3.6908\t3.4491\t3.3129\t3.3080\t3.3986\t\n",
      "Training Epoch: 2 [426/566]\tLoss: 26.8173\tLR: 0.010000\n",
      "5.5615\t5.1269\t4.6123\t4.1522\t3.7788\t3.5856\t\n",
      "Training Epoch: 2 [442/566]\tLoss: 12.9545\tLR: 0.010000\n",
      "2.2310\t2.1633\t2.0716\t2.0442\t2.1327\t2.3117\t\n",
      "Training Epoch: 2 [458/566]\tLoss: 24.0214\tLR: 0.010000\n",
      "4.7181\t4.3359\t3.9734\t3.7320\t3.6144\t3.6475\t\n",
      "Training Epoch: 2 [474/566]\tLoss: 12.4012\tLR: 0.010000\n",
      "2.1145\t2.0735\t2.0036\t1.9737\t2.0272\t2.2087\t\n",
      "Training Epoch: 2 [490/566]\tLoss: 19.5763\tLR: 0.010000\n",
      "3.5989\t3.4448\t3.2444\t3.1053\t3.0708\t3.1122\t\n",
      "Training Epoch: 2 [506/566]\tLoss: 18.4590\tLR: 0.010000\n",
      "3.6441\t3.3456\t3.0370\t2.8415\t2.7676\t2.8233\t\n",
      "Training Epoch: 2 [522/566]\tLoss: 19.1275\tLR: 0.010000\n",
      "3.7934\t3.5330\t3.2196\t2.9624\t2.8040\t2.8151\t\n",
      "Training Epoch: 2 [538/566]\tLoss: 27.6514\tLR: 0.010000\n",
      "5.2218\t4.9214\t4.6366\t4.4225\t4.2603\t4.1890\t\n",
      "Training Epoch: 2 [554/566]\tLoss: 15.8086\tLR: 0.010000\n",
      "2.7319\t2.6528\t2.5636\t2.5896\t2.6151\t2.6555\t\n",
      "Training Epoch: 2 [570/566]\tLoss: 8.6945\tLR: 0.010000\n",
      "1.4993\t1.3971\t1.3159\t1.3415\t1.4710\t1.6695\t\n",
      "Training Epoch: 2 [586/566]\tLoss: 14.1850\tLR: 0.010000\n",
      "2.3074\t2.2810\t2.2738\t2.3262\t2.4272\t2.5694\t\n",
      "Training Epoch: 2 [602/566]\tLoss: 20.1996\tLR: 0.010000\n",
      "3.6410\t3.4656\t3.3050\t3.2281\t3.2453\t3.3147\t\n",
      "Training Epoch: 2 [618/566]\tLoss: 23.1424\tLR: 0.010000\n",
      "4.7533\t4.4207\t3.9458\t3.5511\t3.2995\t3.1721\t\n",
      "Training Epoch: 2 [634/566]\tLoss: 13.2096\tLR: 0.010000\n",
      "2.5176\t2.3490\t2.1557\t2.0419\t2.0291\t2.1164\t\n",
      "Training Epoch: 2 [650/566]\tLoss: 17.6166\tLR: 0.010000\n",
      "3.4651\t3.2303\t2.9563\t2.7595\t2.6265\t2.5790\t\n",
      "Training Epoch: 2 [666/566]\tLoss: 21.6458\tLR: 0.010000\n",
      "4.4735\t4.1616\t3.7290\t3.3434\t3.0524\t2.8858\t\n",
      "Training Epoch: 2 [682/566]\tLoss: 20.3607\tLR: 0.010000\n",
      "3.6766\t3.4722\t3.2994\t3.2623\t3.2892\t3.3610\t\n",
      "Training Epoch: 2 [698/566]\tLoss: 11.1559\tLR: 0.010000\n",
      "1.8735\t1.7915\t1.7430\t1.7629\t1.8817\t2.1033\t\n",
      "Training Epoch: 2 [714/566]\tLoss: 16.6838\tLR: 0.010000\n",
      "2.8398\t2.6909\t2.6228\t2.6632\t2.8332\t3.0340\t\n",
      "Training Epoch: 2 [730/566]\tLoss: 15.4476\tLR: 0.010000\n",
      "2.5377\t2.4917\t2.4669\t2.5084\t2.6347\t2.8082\t\n",
      "Training Epoch: 2 [746/566]\tLoss: 21.4561\tLR: 0.010000\n",
      "4.0418\t3.7365\t3.4983\t3.3920\t3.3750\t3.4124\t\n",
      "Training Epoch: 2 [762/566]\tLoss: 18.7465\tLR: 0.010000\n",
      "3.0329\t2.8769\t2.8574\t3.0346\t3.3002\t3.6445\t\n",
      "Training Epoch: 2 [778/566]\tLoss: 24.6781\tLR: 0.010000\n",
      "4.4046\t4.1991\t4.0346\t3.9729\t4.0023\t4.0645\t\n",
      "Training Epoch: 2 [794/566]\tLoss: 23.3802\tLR: 0.010000\n",
      "4.3462\t4.1848\t3.9245\t3.7163\t3.6027\t3.6057\t\n",
      "Training Epoch: 2 [810/566]\tLoss: 23.3456\tLR: 0.010000\n",
      "4.4195\t4.1821\t3.9407\t3.7407\t3.5591\t3.5034\t\n",
      "Training Epoch: 2 [826/566]\tLoss: 19.5993\tLR: 0.010000\n",
      "3.4279\t3.3537\t3.2680\t3.1620\t3.1520\t3.2357\t\n",
      "Training Epoch: 2 [842/566]\tLoss: 13.9477\tLR: 0.010000\n",
      "2.8352\t2.5405\t2.2949\t2.1310\t2.0612\t2.0848\t\n",
      "Training Epoch: 2 [858/566]\tLoss: 15.8720\tLR: 0.010000\n",
      "2.9536\t2.7979\t2.6077\t2.4827\t2.4535\t2.5767\t\n",
      "Training Epoch: 2 [874/566]\tLoss: 11.8357\tLR: 0.010000\n",
      "1.9607\t1.8658\t1.8214\t1.8710\t2.0259\t2.2909\t\n",
      "Training Epoch: 2 [890/566]\tLoss: 15.4397\tLR: 0.010000\n",
      "2.7536\t2.6195\t2.5169\t2.4753\t2.4920\t2.5825\t\n",
      "Training Epoch: 2 [902/566]\tLoss: 16.5879\tLR: 0.010000\n",
      "2.8481\t2.7549\t2.6896\t2.6947\t2.7631\t2.8375\t\n",
      "[0.35403725504875183, 0.23054997622966766, 0.4154127687215805, 0.014109215699136257, 0.3950236141681671, 0.19609595835208893, 0.40888042747974396, 0.011217827908694744, 0.38257578015327454, 0.19361045956611633, 0.42381376028060913, 0.004363817162811756, 0.3027679920196533, 0.23066116869449615, 0.4665708392858505, 0.0054521323181688786, 0.19177985191345215, 0.0, 0.8082201480865479, 0.006291602738201618]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3370, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3186, Accuracy: 0.4505\n",
      "Test set t = 02: Average loss: 0.3003, Accuracy: 0.4523\n",
      "Test set t = 03: Average loss: 0.2903, Accuracy: 0.4382\n",
      "Test set t = 04: Average loss: 0.2900, Accuracy: 0.4223\n",
      "Test set t = 05: Average loss: 0.2989, Accuracy: 0.4117\n",
      "\n",
      "Training Epoch: 3 [10/566]\tLoss: 16.7898\tLR: 0.010000\n",
      "3.0556\t2.8768\t2.6954\t2.6382\t2.6939\t2.8299\t\n",
      "Training Epoch: 3 [26/566]\tLoss: 11.6256\tLR: 0.010000\n",
      "2.0844\t1.9723\t1.8724\t1.8190\t1.8609\t2.0166\t\n",
      "Training Epoch: 3 [42/566]\tLoss: 10.8682\tLR: 0.010000\n",
      "1.6341\t1.6528\t1.6903\t1.7591\t1.9208\t2.2111\t\n",
      "Training Epoch: 3 [58/566]\tLoss: 23.8172\tLR: 0.010000\n",
      "4.4911\t4.1908\t3.9187\t3.7768\t3.7040\t3.7358\t\n",
      "Training Epoch: 3 [74/566]\tLoss: 15.0746\tLR: 0.010000\n",
      "2.7320\t2.5920\t2.4679\t2.3962\t2.3989\t2.4876\t\n",
      "Training Epoch: 3 [90/566]\tLoss: 15.0549\tLR: 0.010000\n",
      "2.4628\t2.4451\t2.4435\t2.4406\t2.5381\t2.7249\t\n",
      "Training Epoch: 3 [106/566]\tLoss: 12.3886\tLR: 0.010000\n",
      "2.1225\t2.0429\t1.9652\t1.9569\t2.0530\t2.2481\t\n",
      "Training Epoch: 3 [122/566]\tLoss: 33.7227\tLR: 0.010000\n",
      "6.7900\t6.3337\t5.7094\t5.2223\t4.9236\t4.7437\t\n",
      "Training Epoch: 3 [138/566]\tLoss: 9.3137\tLR: 0.010000\n",
      "1.2938\t1.2648\t1.3513\t1.5454\t1.7995\t2.0588\t\n",
      "Training Epoch: 3 [154/566]\tLoss: 9.2004\tLR: 0.010000\n",
      "1.6047\t1.5333\t1.4844\t1.4643\t1.5061\t1.6076\t\n",
      "Training Epoch: 3 [170/566]\tLoss: 20.6423\tLR: 0.010000\n",
      "3.8944\t3.6455\t3.4228\t3.2672\t3.1965\t3.2159\t\n",
      "Training Epoch: 3 [186/566]\tLoss: 22.9410\tLR: 0.010000\n",
      "4.4516\t4.1439\t3.8314\t3.6096\t3.4703\t3.4342\t\n",
      "Training Epoch: 3 [202/566]\tLoss: 22.7395\tLR: 0.010000\n",
      "4.1929\t4.0129\t3.8024\t3.6460\t3.5572\t3.5280\t\n",
      "Training Epoch: 3 [218/566]\tLoss: 17.3108\tLR: 0.010000\n",
      "2.8896\t2.7639\t2.7371\t2.8226\t2.9653\t3.1323\t\n",
      "Training Epoch: 3 [234/566]\tLoss: 25.3333\tLR: 0.010000\n",
      "4.8913\t4.5688\t4.2269\t4.0032\t3.8499\t3.7933\t\n",
      "Training Epoch: 3 [250/566]\tLoss: 11.6857\tLR: 0.010000\n",
      "2.0177\t1.9653\t1.9095\t1.8851\t1.9085\t1.9997\t\n",
      "Training Epoch: 3 [266/566]\tLoss: 27.2490\tLR: 0.010000\n",
      "5.4212\t5.0780\t4.6358\t4.2478\t3.9965\t3.8697\t\n",
      "Training Epoch: 3 [282/566]\tLoss: 18.1831\tLR: 0.010000\n",
      "3.5044\t3.2757\t3.0264\t2.8468\t2.7557\t2.7740\t\n",
      "Training Epoch: 3 [298/566]\tLoss: 32.4500\tLR: 0.010000\n",
      "6.4173\t6.0557\t5.5491\t5.1150\t4.7829\t4.5300\t\n",
      "Training Epoch: 3 [314/566]\tLoss: 23.3038\tLR: 0.010000\n",
      "4.6509\t4.3281\t3.9475\t3.6379\t3.4144\t3.3250\t\n",
      "Training Epoch: 3 [330/566]\tLoss: 14.9890\tLR: 0.010000\n",
      "2.6381\t2.5835\t2.4751\t2.4126\t2.3993\t2.4803\t\n",
      "Training Epoch: 3 [346/566]\tLoss: 13.0433\tLR: 0.010000\n",
      "2.1472\t2.0912\t2.0579\t2.0957\t2.2263\t2.4250\t\n",
      "Training Epoch: 3 [362/566]\tLoss: 21.0152\tLR: 0.010000\n",
      "4.1838\t3.8464\t3.4879\t3.2360\t3.1204\t3.1408\t\n",
      "Training Epoch: 3 [378/566]\tLoss: 19.9681\tLR: 0.010000\n",
      "4.0190\t3.6834\t3.3170\t3.0707\t2.9462\t2.9318\t\n",
      "Training Epoch: 3 [394/566]\tLoss: 17.9849\tLR: 0.010000\n",
      "3.2498\t3.1444\t3.0184\t2.9143\t2.8173\t2.8407\t\n",
      "Training Epoch: 3 [410/566]\tLoss: 23.2716\tLR: 0.010000\n",
      "4.7369\t4.3420\t3.9051\t3.5766\t3.3861\t3.3249\t\n",
      "Training Epoch: 3 [426/566]\tLoss: 12.3296\tLR: 0.010000\n",
      "1.8623\t1.8372\t1.8944\t2.0069\t2.2321\t2.4967\t\n",
      "Training Epoch: 3 [442/566]\tLoss: 7.9684\tLR: 0.010000\n",
      "1.2417\t1.2229\t1.2398\t1.2962\t1.4074\t1.5605\t\n",
      "Training Epoch: 3 [458/566]\tLoss: 25.1943\tLR: 0.010000\n",
      "5.1023\t4.7350\t4.2677\t3.8847\t3.6440\t3.5606\t\n",
      "Training Epoch: 3 [474/566]\tLoss: 8.9471\tLR: 0.010000\n",
      "1.3827\t1.3678\t1.3922\t1.4537\t1.5756\t1.7752\t\n",
      "Training Epoch: 3 [490/566]\tLoss: 37.4948\tLR: 0.010000\n",
      "7.5272\t6.8929\t6.2661\t5.8170\t5.5646\t5.4269\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [506/566]\tLoss: 15.1363\tLR: 0.010000\n",
      "2.6410\t2.5142\t2.4201\t2.4325\t2.5044\t2.6239\t\n",
      "Training Epoch: 3 [522/566]\tLoss: 13.4954\tLR: 0.010000\n",
      "2.2224\t2.1466\t2.1212\t2.1668\t2.3022\t2.5362\t\n",
      "Training Epoch: 3 [538/566]\tLoss: 16.6505\tLR: 0.010000\n",
      "2.8943\t2.7790\t2.6740\t2.6752\t2.7421\t2.8859\t\n",
      "Training Epoch: 3 [554/566]\tLoss: 20.7149\tLR: 0.010000\n",
      "4.0525\t3.7263\t3.3919\t3.2067\t3.1522\t3.1853\t\n",
      "Training Epoch: 3 [570/566]\tLoss: 21.4378\tLR: 0.010000\n",
      "4.0170\t3.7929\t3.5807\t3.4336\t3.3241\t3.2895\t\n",
      "Training Epoch: 3 [586/566]\tLoss: 11.3529\tLR: 0.010000\n",
      "1.7021\t1.7291\t1.7713\t1.8649\t2.0272\t2.2583\t\n",
      "Training Epoch: 3 [602/566]\tLoss: 15.6919\tLR: 0.010000\n",
      "2.7761\t2.6260\t2.5017\t2.4699\t2.5754\t2.7427\t\n",
      "Training Epoch: 3 [618/566]\tLoss: 10.9663\tLR: 0.010000\n",
      "1.4469\t1.4776\t1.6117\t1.8439\t2.1376\t2.4485\t\n",
      "Training Epoch: 3 [634/566]\tLoss: 8.1368\tLR: 0.010000\n",
      "1.2758\t1.2568\t1.2403\t1.2945\t1.4179\t1.6516\t\n",
      "Training Epoch: 3 [650/566]\tLoss: 18.9472\tLR: 0.010000\n",
      "3.6351\t3.4185\t3.1580\t2.9495\t2.8833\t2.9029\t\n",
      "Training Epoch: 3 [666/566]\tLoss: 24.5810\tLR: 0.010000\n",
      "4.2508\t4.1555\t4.0027\t3.9523\t4.0266\t4.1931\t\n",
      "Training Epoch: 3 [682/566]\tLoss: 22.8228\tLR: 0.010000\n",
      "4.3649\t4.0928\t3.8110\t3.5771\t3.4579\t3.5190\t\n",
      "Training Epoch: 3 [698/566]\tLoss: 13.5077\tLR: 0.010000\n",
      "2.0409\t2.0588\t2.1545\t2.2641\t2.3947\t2.5948\t\n",
      "Training Epoch: 3 [714/566]\tLoss: 12.5111\tLR: 0.010000\n",
      "2.0366\t1.9695\t1.9473\t2.0175\t2.1619\t2.3783\t\n",
      "Training Epoch: 3 [730/566]\tLoss: 32.4653\tLR: 0.010000\n",
      "6.8410\t6.2031\t5.4698\t4.9216\t4.5928\t4.4369\t\n",
      "Training Epoch: 3 [746/566]\tLoss: 19.6831\tLR: 0.010000\n",
      "3.5376\t3.4001\t3.2145\t3.1448\t3.1639\t3.2222\t\n",
      "Training Epoch: 3 [762/566]\tLoss: 17.4322\tLR: 0.010000\n",
      "3.1796\t2.9899\t2.8457\t2.7830\t2.7922\t2.8418\t\n",
      "Training Epoch: 3 [778/566]\tLoss: 20.9751\tLR: 0.010000\n",
      "4.0955\t3.7767\t3.4681\t3.2395\t3.1511\t3.2442\t\n",
      "Training Epoch: 3 [794/566]\tLoss: 26.8983\tLR: 0.010000\n",
      "4.9965\t4.7431\t4.4512\t4.2707\t4.1958\t4.2411\t\n",
      "Training Epoch: 3 [810/566]\tLoss: 18.6189\tLR: 0.010000\n",
      "3.2927\t3.1175\t3.0148\t2.9847\t3.0302\t3.1789\t\n",
      "Training Epoch: 3 [826/566]\tLoss: 14.7713\tLR: 0.010000\n",
      "2.7290\t2.5830\t2.4044\t2.3211\t2.3283\t2.4056\t\n",
      "Training Epoch: 3 [842/566]\tLoss: 14.7088\tLR: 0.010000\n",
      "2.8485\t2.6175\t2.4040\t2.2855\t2.2511\t2.3022\t\n",
      "Training Epoch: 3 [858/566]\tLoss: 18.6776\tLR: 0.010000\n",
      "3.2754\t3.1166\t2.9743\t2.9420\t3.0640\t3.3052\t\n",
      "Training Epoch: 3 [874/566]\tLoss: 14.7109\tLR: 0.010000\n",
      "2.5401\t2.4230\t2.3442\t2.3557\t2.4424\t2.6055\t\n",
      "Training Epoch: 3 [890/566]\tLoss: 14.3851\tLR: 0.010000\n",
      "2.2624\t2.2782\t2.3004\t2.3618\t2.4919\t2.6903\t\n",
      "Training Epoch: 3 [902/566]\tLoss: 15.0816\tLR: 0.010000\n",
      "2.4418\t2.2930\t2.3002\t2.4681\t2.6735\t2.9051\t\n",
      "[0.35036608576774597, 0.2603428363800049, 0.38929107785224915, 0.01504435669630766, 0.4325093626976013, 0.18911974132061005, 0.37837089598178864, 0.01228239107877016, 0.41026267409324646, 0.18408411741256714, 0.4056532084941864, 0.0026450862642377615, 0.29869967699050903, 0.24078215658664703, 0.46051816642284393, 0.004223095718771219, 0.18383659422397614, 0.0, 0.8161634057760239, 0.004181113559752703]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3349, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3168, Accuracy: 0.4488\n",
      "Test set t = 02: Average loss: 0.2988, Accuracy: 0.4541\n",
      "Test set t = 03: Average loss: 0.2888, Accuracy: 0.4329\n",
      "Test set t = 04: Average loss: 0.2878, Accuracy: 0.4258\n",
      "Test set t = 05: Average loss: 0.2958, Accuracy: 0.4223\n",
      "\n",
      "Training Epoch: 4 [10/566]\tLoss: 14.9013\tLR: 0.010000\n",
      "2.4350\t2.4096\t2.4087\t2.4403\t2.5221\t2.6856\t\n",
      "Training Epoch: 4 [26/566]\tLoss: 18.1268\tLR: 0.010000\n",
      "3.1528\t3.0919\t3.0010\t2.9431\t2.9363\t3.0018\t\n",
      "Training Epoch: 4 [42/566]\tLoss: 14.1403\tLR: 0.010000\n",
      "2.4815\t2.3585\t2.2889\t2.2694\t2.3049\t2.4372\t\n",
      "Training Epoch: 4 [58/566]\tLoss: 13.7047\tLR: 0.010000\n",
      "2.4079\t2.3387\t2.2407\t2.1894\t2.2122\t2.3158\t\n",
      "Training Epoch: 4 [74/566]\tLoss: 9.5417\tLR: 0.010000\n",
      "1.3154\t1.3886\t1.4583\t1.5747\t1.7743\t2.0304\t\n",
      "Training Epoch: 4 [90/566]\tLoss: 10.2276\tLR: 0.010000\n",
      "1.6456\t1.5955\t1.6026\t1.6663\t1.7834\t1.9343\t\n",
      "Training Epoch: 4 [106/566]\tLoss: 30.5033\tLR: 0.010000\n",
      "6.1678\t5.6888\t5.2057\t4.7806\t4.4402\t4.2201\t\n",
      "Training Epoch: 4 [122/566]\tLoss: 16.9622\tLR: 0.010000\n",
      "3.3842\t3.1640\t2.8633\t2.6029\t2.4705\t2.4773\t\n",
      "Training Epoch: 4 [138/566]\tLoss: 18.2018\tLR: 0.010000\n",
      "3.0667\t2.9715\t2.8986\t2.9429\t3.0728\t3.2493\t\n",
      "Training Epoch: 4 [154/566]\tLoss: 32.5785\tLR: 0.010000\n",
      "7.0511\t6.4313\t5.6536\t4.9731\t4.4326\t4.0368\t\n",
      "Training Epoch: 4 [170/566]\tLoss: 20.1189\tLR: 0.010000\n",
      "3.6785\t3.5153\t3.3137\t3.1980\t3.1667\t3.2467\t\n",
      "Training Epoch: 4 [186/566]\tLoss: 21.0261\tLR: 0.010000\n",
      "3.7982\t3.6277\t3.4644\t3.3830\t3.3449\t3.4079\t\n",
      "Training Epoch: 4 [202/566]\tLoss: 22.8325\tLR: 0.010000\n",
      "4.6003\t4.2438\t3.8303\t3.5320\t3.3568\t3.2692\t\n",
      "Training Epoch: 4 [218/566]\tLoss: 18.7433\tLR: 0.010000\n",
      "3.2234\t3.1372\t3.0699\t3.0447\t3.0758\t3.1923\t\n",
      "Training Epoch: 4 [234/566]\tLoss: 16.2814\tLR: 0.010000\n",
      "2.9098\t2.7521\t2.6212\t2.5906\t2.6436\t2.7641\t\n",
      "Training Epoch: 4 [250/566]\tLoss: 22.4421\tLR: 0.010000\n",
      "4.3560\t4.0083\t3.6984\t3.5202\t3.4385\t3.4206\t\n",
      "Training Epoch: 4 [266/566]\tLoss: 18.3906\tLR: 0.010000\n",
      "3.5777\t3.3093\t3.0475\t2.8613\t2.7866\t2.8082\t\n",
      "Training Epoch: 4 [282/566]\tLoss: 16.4461\tLR: 0.010000\n",
      "2.8793\t2.8012\t2.6875\t2.6288\t2.6734\t2.7759\t\n",
      "Training Epoch: 4 [298/566]\tLoss: 15.9743\tLR: 0.010000\n",
      "2.6283\t2.5797\t2.5542\t2.5902\t2.7011\t2.9208\t\n",
      "Training Epoch: 4 [314/566]\tLoss: 18.0475\tLR: 0.010000\n",
      "2.8318\t2.8626\t2.9194\t3.0105\t3.1346\t3.2885\t\n",
      "Training Epoch: 4 [330/566]\tLoss: 18.0808\tLR: 0.010000\n",
      "3.4097\t3.1337\t2.9095\t2.8200\t2.8426\t2.9652\t\n",
      "Training Epoch: 4 [346/566]\tLoss: 15.1791\tLR: 0.010000\n",
      "2.6529\t2.5297\t2.4366\t2.4269\t2.4985\t2.6345\t\n",
      "Training Epoch: 4 [362/566]\tLoss: 20.4241\tLR: 0.010000\n",
      "3.9981\t3.7311\t3.4074\t3.1577\t3.0475\t3.0822\t\n",
      "Training Epoch: 4 [378/566]\tLoss: 20.8999\tLR: 0.010000\n",
      "3.6928\t3.5760\t3.4447\t3.3568\t3.3551\t3.4745\t\n",
      "Training Epoch: 4 [394/566]\tLoss: 13.3679\tLR: 0.010000\n",
      "2.1840\t2.1360\t2.0877\t2.1448\t2.2959\t2.5195\t\n",
      "Training Epoch: 4 [410/566]\tLoss: 25.2360\tLR: 0.010000\n",
      "5.2759\t4.8145\t4.3214\t3.8967\t3.5603\t3.3673\t\n",
      "Training Epoch: 4 [426/566]\tLoss: 21.9368\tLR: 0.010000\n",
      "3.9650\t3.7849\t3.5945\t3.5046\t3.5103\t3.5776\t\n",
      "Training Epoch: 4 [442/566]\tLoss: 19.6031\tLR: 0.010000\n",
      "3.6853\t3.4951\t3.2883\t3.1367\t3.0119\t2.9858\t\n",
      "Training Epoch: 4 [458/566]\tLoss: 17.4815\tLR: 0.010000\n",
      "3.0030\t2.8605\t2.7837\t2.7987\t2.9192\t3.1164\t\n",
      "Training Epoch: 4 [474/566]\tLoss: 19.4253\tLR: 0.010000\n",
      "3.7436\t3.4785\t3.2211\t3.0389\t2.9706\t2.9727\t\n",
      "Training Epoch: 4 [490/566]\tLoss: 9.1888\tLR: 0.010000\n",
      "1.6576\t1.5142\t1.4050\t1.4064\t1.5146\t1.6909\t\n",
      "Training Epoch: 4 [506/566]\tLoss: 18.3543\tLR: 0.010000\n",
      "3.5728\t3.3331\t3.0873\t2.8845\t2.7502\t2.7264\t\n",
      "Training Epoch: 4 [522/566]\tLoss: 25.3853\tLR: 0.010000\n",
      "5.0475\t4.6683\t4.2632\t3.9488\t3.7661\t3.6914\t\n",
      "Training Epoch: 4 [538/566]\tLoss: 20.4523\tLR: 0.010000\n",
      "3.9392\t3.6451\t3.3695\t3.2084\t3.1342\t3.1559\t\n",
      "Training Epoch: 4 [554/566]\tLoss: 26.9192\tLR: 0.010000\n",
      "5.2850\t4.9574\t4.5480\t4.2171\t4.0034\t3.9082\t\n",
      "Training Epoch: 4 [570/566]\tLoss: 15.5464\tLR: 0.010000\n",
      "2.5271\t2.4430\t2.4376\t2.5461\t2.7102\t2.8824\t\n",
      "Training Epoch: 4 [586/566]\tLoss: 10.4645\tLR: 0.010000\n",
      "1.6848\t1.6437\t1.6530\t1.7097\t1.8101\t1.9633\t\n",
      "Training Epoch: 4 [602/566]\tLoss: 11.4947\tLR: 0.010000\n",
      "2.0979\t1.9851\t1.8708\t1.8169\t1.8251\t1.8988\t\n",
      "Training Epoch: 4 [618/566]\tLoss: 24.9674\tLR: 0.010000\n",
      "4.8017\t4.4998\t4.1775\t3.9373\t3.7848\t3.7663\t\n",
      "Training Epoch: 4 [634/566]\tLoss: 19.2713\tLR: 0.010000\n",
      "3.5560\t3.3744\t3.1909\t3.0633\t3.0164\t3.0704\t\n",
      "Training Epoch: 4 [650/566]\tLoss: 22.6813\tLR: 0.010000\n",
      "4.8107\t4.3888\t3.8722\t3.4290\t3.1455\t3.0351\t\n",
      "Training Epoch: 4 [666/566]\tLoss: 18.4074\tLR: 0.010000\n",
      "3.5932\t3.3519\t3.0353\t2.8244\t2.7576\t2.8449\t\n",
      "Training Epoch: 4 [682/566]\tLoss: 26.4991\tLR: 0.010000\n",
      "4.9707\t4.6213\t4.3381\t4.2037\t4.1720\t4.1933\t\n",
      "Training Epoch: 4 [698/566]\tLoss: 11.8462\tLR: 0.010000\n",
      "1.7330\t1.7521\t1.8194\t1.9505\t2.1537\t2.4375\t\n",
      "Training Epoch: 4 [714/566]\tLoss: 17.5852\tLR: 0.010000\n",
      "3.1272\t3.0088\t2.9044\t2.8344\t2.8261\t2.8843\t\n",
      "Training Epoch: 4 [730/566]\tLoss: 17.5777\tLR: 0.010000\n",
      "3.1727\t2.9929\t2.8289\t2.7800\t2.8213\t2.9818\t\n",
      "Training Epoch: 4 [746/566]\tLoss: 19.8414\tLR: 0.010000\n",
      "3.5700\t3.4860\t3.3270\t3.2202\t3.1397\t3.0985\t\n",
      "Training Epoch: 4 [762/566]\tLoss: 5.8635\tLR: 0.010000\n",
      "0.7442\t0.7219\t0.7705\t0.9135\t1.1850\t1.5283\t\n",
      "Training Epoch: 4 [778/566]\tLoss: 11.4049\tLR: 0.010000\n",
      "2.0234\t1.8605\t1.7388\t1.7515\t1.8981\t2.1327\t\n",
      "Training Epoch: 4 [794/566]\tLoss: 16.9201\tLR: 0.010000\n",
      "2.9893\t2.8347\t2.7199\t2.6927\t2.7760\t2.9075\t\n",
      "Training Epoch: 4 [810/566]\tLoss: 13.0211\tLR: 0.010000\n",
      "2.1844\t2.1144\t2.0786\t2.0965\t2.1861\t2.3611\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [826/566]\tLoss: 30.7011\tLR: 0.010000\n",
      "6.1380\t5.6964\t5.2024\t4.8136\t4.5124\t4.3383\t\n",
      "Training Epoch: 4 [842/566]\tLoss: 27.0925\tLR: 0.010000\n",
      "5.0787\t4.7380\t4.4663\t4.3133\t4.2593\t4.2368\t\n",
      "Training Epoch: 4 [858/566]\tLoss: 9.4309\tLR: 0.010000\n",
      "1.4584\t1.4492\t1.4593\t1.5252\t1.6594\t1.8794\t\n",
      "Training Epoch: 4 [874/566]\tLoss: 14.6094\tLR: 0.010000\n",
      "2.4364\t2.3923\t2.3653\t2.3811\t2.4293\t2.6050\t\n",
      "Training Epoch: 4 [890/566]\tLoss: 13.5835\tLR: 0.010000\n",
      "2.2986\t2.2026\t2.1320\t2.1526\t2.2991\t2.4985\t\n",
      "Training Epoch: 4 [902/566]\tLoss: 14.4098\tLR: 0.010000\n",
      "2.3547\t2.2999\t2.2953\t2.3539\t2.4642\t2.6418\t\n",
      "[0.3430171012878418, 0.3030661642551422, 0.353916734457016, 0.015571605414152145, 0.47730109095573425, 0.1828608512878418, 0.33983805775642395, 0.013044939376413822, 0.44345977902412415, 0.17312078177928925, 0.3834194391965866, 0.0011003982508555055, 0.29994767904281616, 0.24763824045658112, 0.4524140805006027, 0.0032329170498996973, 0.17906539142131805, 0.0, 0.820934608578682, 0.0024531832896173]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3366, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3180, Accuracy: 0.4488\n",
      "Test set t = 02: Average loss: 0.2999, Accuracy: 0.4558\n",
      "Test set t = 03: Average loss: 0.2897, Accuracy: 0.4364\n",
      "Test set t = 04: Average loss: 0.2884, Accuracy: 0.4311\n",
      "Test set t = 05: Average loss: 0.2960, Accuracy: 0.4240\n",
      "\n",
      "Training Epoch: 5 [10/566]\tLoss: 10.1615\tLR: 0.010000\n",
      "1.9116\t1.7882\t1.6416\t1.5385\t1.5680\t1.7137\t\n",
      "Training Epoch: 5 [26/566]\tLoss: 12.7449\tLR: 0.010000\n",
      "2.3078\t2.1593\t2.0296\t2.0028\t2.0609\t2.1845\t\n",
      "Training Epoch: 5 [42/566]\tLoss: 16.5245\tLR: 0.010000\n",
      "2.6958\t2.6214\t2.5970\t2.6868\t2.8594\t3.0642\t\n",
      "Training Epoch: 5 [58/566]\tLoss: 5.8614\tLR: 0.010000\n",
      "0.7214\t0.7529\t0.8197\t0.9497\t1.1522\t1.4655\t\n",
      "Training Epoch: 5 [74/566]\tLoss: 16.2137\tLR: 0.010000\n",
      "2.7578\t2.6927\t2.6145\t2.5997\t2.6961\t2.8529\t\n",
      "Training Epoch: 5 [90/566]\tLoss: 22.8194\tLR: 0.010000\n",
      "4.5848\t4.2056\t3.7974\t3.5129\t3.3668\t3.3519\t\n",
      "Training Epoch: 5 [106/566]\tLoss: 16.9883\tLR: 0.010000\n",
      "3.1720\t2.9816\t2.8097\t2.6919\t2.6527\t2.6805\t\n",
      "Training Epoch: 5 [122/566]\tLoss: 19.3029\tLR: 0.010000\n",
      "3.3407\t3.2002\t3.1171\t3.1220\t3.1896\t3.3333\t\n",
      "Training Epoch: 5 [138/566]\tLoss: 19.5137\tLR: 0.010000\n",
      "3.7242\t3.4625\t3.2149\t3.0785\t3.0084\t3.0252\t\n",
      "Training Epoch: 5 [154/566]\tLoss: 13.4651\tLR: 0.010000\n",
      "2.3939\t2.2032\t2.1136\t2.1156\t2.2243\t2.4145\t\n",
      "Training Epoch: 5 [170/566]\tLoss: 14.2656\tLR: 0.010000\n",
      "2.5340\t2.4378\t2.3300\t2.2720\t2.2761\t2.4157\t\n",
      "Training Epoch: 5 [186/566]\tLoss: 16.5772\tLR: 0.010000\n",
      "3.0143\t2.8217\t2.6879\t2.6415\t2.6527\t2.7590\t\n",
      "Training Epoch: 5 [202/566]\tLoss: 26.3704\tLR: 0.010000\n",
      "4.9752\t4.6906\t4.4022\t4.2048\t4.0841\t4.0136\t\n",
      "Training Epoch: 5 [218/566]\tLoss: 14.1877\tLR: 0.010000\n",
      "2.3816\t2.2902\t2.2526\t2.2903\t2.3975\t2.5756\t\n",
      "Training Epoch: 5 [234/566]\tLoss: 22.7109\tLR: 0.010000\n",
      "4.1792\t3.9914\t3.7938\t3.6500\t3.5684\t3.5281\t\n",
      "Training Epoch: 5 [250/566]\tLoss: 26.5214\tLR: 0.010000\n",
      "5.1511\t4.8332\t4.4608\t4.1783\t3.9819\t3.9161\t\n",
      "Training Epoch: 5 [266/566]\tLoss: 14.5265\tLR: 0.010000\n",
      "2.4335\t2.3723\t2.3417\t2.3653\t2.4439\t2.5697\t\n",
      "Training Epoch: 5 [282/566]\tLoss: 22.5147\tLR: 0.010000\n",
      "4.3644\t4.1224\t3.8175\t3.5573\t3.3648\t3.2883\t\n",
      "Training Epoch: 5 [298/566]\tLoss: 29.8273\tLR: 0.010000\n",
      "5.8649\t5.4944\t5.0570\t4.6990\t4.4475\t4.2644\t\n",
      "Training Epoch: 5 [314/566]\tLoss: 27.1852\tLR: 0.010000\n",
      "5.1967\t4.9239\t4.5678\t4.3005\t4.1367\t4.0597\t\n",
      "Training Epoch: 5 [330/566]\tLoss: 24.5684\tLR: 0.010000\n",
      "4.4770\t4.2897\t4.0641\t3.9165\t3.8739\t3.9472\t\n",
      "Training Epoch: 5 [346/566]\tLoss: 15.9567\tLR: 0.010000\n",
      "2.6547\t2.5553\t2.5348\t2.6073\t2.7215\t2.8831\t\n",
      "Training Epoch: 5 [362/566]\tLoss: 14.6858\tLR: 0.010000\n",
      "2.4672\t2.4189\t2.3916\t2.4060\t2.4591\t2.5429\t\n",
      "Training Epoch: 5 [378/566]\tLoss: 20.9042\tLR: 0.010000\n",
      "4.1690\t3.8900\t3.5387\t3.2608\t3.0687\t2.9770\t\n",
      "Training Epoch: 5 [394/566]\tLoss: 10.0706\tLR: 0.010000\n",
      "1.5275\t1.5966\t1.6431\t1.6778\t1.7465\t1.8791\t\n",
      "Training Epoch: 5 [410/566]\tLoss: 25.1056\tLR: 0.010000\n",
      "4.8165\t4.4989\t4.1835\t3.9520\t3.8467\t3.8080\t\n",
      "Training Epoch: 5 [426/566]\tLoss: 23.8133\tLR: 0.010000\n",
      "4.7221\t4.3790\t3.9891\t3.6929\t3.5321\t3.4981\t\n",
      "Training Epoch: 5 [442/566]\tLoss: 19.1738\tLR: 0.010000\n",
      "3.1945\t3.1085\t3.0767\t3.1422\t3.2510\t3.4008\t\n",
      "Training Epoch: 5 [458/566]\tLoss: 16.7945\tLR: 0.010000\n",
      "2.8844\t2.7840\t2.7282\t2.7189\t2.7767\t2.9022\t\n",
      "Training Epoch: 5 [474/566]\tLoss: 12.4761\tLR: 0.010000\n",
      "2.3175\t2.1489\t2.0164\t1.9436\t1.9693\t2.0805\t\n",
      "Training Epoch: 5 [490/566]\tLoss: 11.7322\tLR: 0.010000\n",
      "2.1528\t1.9960\t1.8633\t1.8349\t1.8791\t2.0061\t\n",
      "Training Epoch: 5 [506/566]\tLoss: 24.6460\tLR: 0.010000\n",
      "5.0821\t4.7406\t4.2529\t3.8020\t3.4621\t3.3063\t\n",
      "Training Epoch: 5 [522/566]\tLoss: 14.7549\tLR: 0.010000\n",
      "2.7976\t2.5937\t2.3988\t2.3201\t2.2998\t2.3449\t\n",
      "Training Epoch: 5 [538/566]\tLoss: 29.7197\tLR: 0.010000\n",
      "5.4083\t5.1499\t4.9393\t4.8059\t4.7198\t4.6964\t\n",
      "Training Epoch: 5 [554/566]\tLoss: 25.1593\tLR: 0.010000\n",
      "5.0326\t4.6882\t4.2875\t3.9443\t3.6842\t3.5224\t\n",
      "Training Epoch: 5 [570/566]\tLoss: 18.4695\tLR: 0.010000\n",
      "2.9557\t2.9009\t2.9066\t3.0086\t3.2286\t3.4690\t\n",
      "Training Epoch: 5 [586/566]\tLoss: 15.0065\tLR: 0.010000\n",
      "2.6088\t2.4596\t2.3745\t2.3797\t2.4865\t2.6974\t\n",
      "Training Epoch: 5 [602/566]\tLoss: 14.0690\tLR: 0.010000\n",
      "2.5587\t2.3821\t2.2463\t2.2143\t2.2692\t2.3984\t\n",
      "Training Epoch: 5 [618/566]\tLoss: 8.6923\tLR: 0.010000\n",
      "1.4265\t1.3728\t1.3499\t1.3724\t1.4788\t1.6918\t\n",
      "Training Epoch: 5 [634/566]\tLoss: 20.1709\tLR: 0.010000\n",
      "3.6936\t3.5137\t3.3336\t3.2212\t3.1862\t3.2225\t\n",
      "Training Epoch: 5 [650/566]\tLoss: 28.7165\tLR: 0.010000\n",
      "6.0028\t5.4697\t4.8900\t4.4210\t4.0741\t3.8589\t\n",
      "Training Epoch: 5 [666/566]\tLoss: 16.2757\tLR: 0.010000\n",
      "2.7382\t2.6547\t2.6407\t2.6706\t2.7347\t2.8368\t\n",
      "Training Epoch: 5 [682/566]\tLoss: 15.2769\tLR: 0.010000\n",
      "2.6992\t2.6130\t2.4974\t2.4552\t2.4719\t2.5402\t\n",
      "Training Epoch: 5 [698/566]\tLoss: 20.9520\tLR: 0.010000\n",
      "4.1438\t3.8505\t3.5376\t3.2847\t3.1041\t3.0313\t\n",
      "Training Epoch: 5 [714/566]\tLoss: 17.0892\tLR: 0.010000\n",
      "3.0145\t2.8544\t2.7396\t2.7112\t2.8016\t2.9678\t\n",
      "Training Epoch: 5 [730/566]\tLoss: 11.0489\tLR: 0.010000\n",
      "1.9275\t1.8270\t1.7424\t1.7443\t1.8228\t1.9849\t\n",
      "Training Epoch: 5 [746/566]\tLoss: 12.2433\tLR: 0.010000\n",
      "1.8472\t1.8760\t1.9296\t2.0298\t2.1814\t2.3793\t\n",
      "Training Epoch: 5 [762/566]\tLoss: 44.4516\tLR: 0.010000\n",
      "9.6568\t8.7844\t7.7149\t6.7846\t6.0364\t5.4746\t\n",
      "Training Epoch: 5 [778/566]\tLoss: 16.7188\tLR: 0.010000\n",
      "3.2537\t2.9677\t2.7173\t2.5810\t2.5550\t2.6442\t\n",
      "Training Epoch: 5 [794/566]\tLoss: 15.7430\tLR: 0.010000\n",
      "3.2140\t2.9303\t2.6584\t2.4371\t2.2634\t2.2398\t\n",
      "Training Epoch: 5 [810/566]\tLoss: 17.7691\tLR: 0.010000\n",
      "2.7918\t2.8196\t2.8423\t2.9239\t3.0805\t3.3111\t\n",
      "Training Epoch: 5 [826/566]\tLoss: 9.4438\tLR: 0.010000\n",
      "1.4691\t1.4546\t1.4661\t1.5310\t1.6710\t1.8519\t\n",
      "Training Epoch: 5 [842/566]\tLoss: 16.3617\tLR: 0.010000\n",
      "2.8438\t2.7413\t2.6552\t2.6307\t2.6802\t2.8105\t\n",
      "Training Epoch: 5 [858/566]\tLoss: 11.4125\tLR: 0.010000\n",
      "2.0473\t1.9531\t1.8414\t1.7894\t1.8273\t1.9539\t\n",
      "Training Epoch: 5 [874/566]\tLoss: 9.9807\tLR: 0.010000\n",
      "1.4331\t1.4401\t1.5011\t1.6475\t1.8545\t2.1043\t\n",
      "Training Epoch: 5 [890/566]\tLoss: 23.1591\tLR: 0.010000\n",
      "4.3676\t4.1068\t3.8304\t3.6541\t3.5904\t3.6098\t\n",
      "Training Epoch: 5 [902/566]\tLoss: 9.7908\tLR: 0.010000\n",
      "1.6841\t1.5730\t1.5294\t1.5507\t1.6429\t1.8107\t\n",
      "[0.3266851603984833, 0.35891807079315186, 0.31439676880836487, 0.015831412747502327, 0.5247038006782532, 0.17515558004379272, 0.3001406192779541, 0.014333825558423996, 0.4886263608932495, 0.15933233499526978, 0.3520413041114807, -0.00012889747449662536, 0.3070509433746338, 0.2518656551837921, 0.4410834014415741, 0.002207570243626833, 0.17654700577259064, 0.0, 0.8234529942274094, 0.0014620959991589189]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3358, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3163, Accuracy: 0.4488\n",
      "Test set t = 02: Average loss: 0.2980, Accuracy: 0.4523\n",
      "Test set t = 03: Average loss: 0.2881, Accuracy: 0.4382\n",
      "Test set t = 04: Average loss: 0.2871, Accuracy: 0.4346\n",
      "Test set t = 05: Average loss: 0.2950, Accuracy: 0.4258\n",
      "\n",
      "Training Epoch: 6 [10/566]\tLoss: 16.6203\tLR: 0.010000\n",
      "2.9193\t2.8165\t2.7263\t2.6707\t2.6968\t2.7908\t\n",
      "Training Epoch: 6 [26/566]\tLoss: 13.0982\tLR: 0.010000\n",
      "2.0043\t1.9642\t2.0065\t2.1411\t2.3592\t2.6230\t\n",
      "Training Epoch: 6 [42/566]\tLoss: 16.8274\tLR: 0.010000\n",
      "2.7036\t2.7119\t2.7119\t2.7625\t2.8810\t3.0564\t\n",
      "Training Epoch: 6 [58/566]\tLoss: 23.7711\tLR: 0.010000\n",
      "4.9205\t4.5552\t4.1073\t3.6914\t3.3425\t3.1541\t\n",
      "Training Epoch: 6 [74/566]\tLoss: 22.6549\tLR: 0.010000\n",
      "4.3319\t4.0574\t3.7884\t3.5829\t3.4650\t3.4293\t\n",
      "Training Epoch: 6 [90/566]\tLoss: 22.4379\tLR: 0.010000\n",
      "4.1303\t3.9377\t3.7419\t3.5843\t3.5213\t3.5225\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [106/566]\tLoss: 17.4778\tLR: 0.010000\n",
      "3.2940\t3.0567\t2.8786\t2.7726\t2.7282\t2.7478\t\n",
      "Training Epoch: 6 [122/566]\tLoss: 11.8549\tLR: 0.010000\n",
      "2.1078\t1.9877\t1.9078\t1.8857\t1.9192\t2.0466\t\n",
      "Training Epoch: 6 [138/566]\tLoss: 23.6396\tLR: 0.010000\n",
      "4.2023\t4.0545\t3.8804\t3.8111\t3.8168\t3.8744\t\n",
      "Training Epoch: 6 [154/566]\tLoss: 21.9127\tLR: 0.010000\n",
      "4.1144\t3.8388\t3.6203\t3.4802\t3.4241\t3.4349\t\n",
      "Training Epoch: 6 [170/566]\tLoss: 25.7692\tLR: 0.010000\n",
      "4.9807\t4.6069\t4.2619\t4.0264\t3.9456\t3.9477\t\n",
      "Training Epoch: 6 [186/566]\tLoss: 19.7797\tLR: 0.010000\n",
      "3.4177\t3.3195\t3.2496\t3.2224\t3.2439\t3.3265\t\n",
      "Training Epoch: 6 [202/566]\tLoss: 14.3932\tLR: 0.010000\n",
      "2.6514\t2.4677\t2.2699\t2.2020\t2.2810\t2.5211\t\n",
      "Training Epoch: 6 [218/566]\tLoss: 13.7594\tLR: 0.010000\n",
      "2.3897\t2.2828\t2.2251\t2.2211\t2.2545\t2.3863\t\n",
      "Training Epoch: 6 [234/566]\tLoss: 18.4088\tLR: 0.010000\n",
      "3.2587\t3.0606\t2.9481\t2.9278\t3.0248\t3.1889\t\n",
      "Training Epoch: 6 [250/566]\tLoss: 13.0866\tLR: 0.010000\n",
      "2.3915\t2.2292\t2.1113\t2.0643\t2.0843\t2.2061\t\n",
      "Training Epoch: 6 [266/566]\tLoss: 26.0271\tLR: 0.010000\n",
      "4.9582\t4.6396\t4.3455\t4.1493\t4.0091\t3.9254\t\n",
      "Training Epoch: 6 [282/566]\tLoss: 23.2342\tLR: 0.010000\n",
      "5.0156\t4.5088\t3.9248\t3.4623\t3.1925\t3.1301\t\n",
      "Training Epoch: 6 [298/566]\tLoss: 31.8401\tLR: 0.010000\n",
      "6.6750\t6.0358\t5.3567\t4.8487\t4.5395\t4.3844\t\n",
      "Training Epoch: 6 [314/566]\tLoss: 23.8658\tLR: 0.010000\n",
      "4.8222\t4.4020\t3.9752\t3.6695\t3.5242\t3.4727\t\n",
      "Training Epoch: 6 [330/566]\tLoss: 15.0059\tLR: 0.010000\n",
      "2.8128\t2.6274\t2.4517\t2.3776\t2.3594\t2.3770\t\n",
      "Training Epoch: 6 [346/566]\tLoss: 11.5265\tLR: 0.010000\n",
      "1.8080\t1.7913\t1.7905\t1.8792\t2.0347\t2.2227\t\n",
      "Training Epoch: 6 [362/566]\tLoss: 13.6031\tLR: 0.010000\n",
      "2.5176\t2.3045\t2.1496\t2.0986\t2.1656\t2.3672\t\n",
      "Training Epoch: 6 [378/566]\tLoss: 16.3048\tLR: 0.010000\n",
      "3.0157\t2.8113\t2.6517\t2.5714\t2.5653\t2.6895\t\n",
      "Training Epoch: 6 [394/566]\tLoss: 14.9313\tLR: 0.010000\n",
      "2.5546\t2.4678\t2.4123\t2.4017\t2.4791\t2.6158\t\n",
      "Training Epoch: 6 [410/566]\tLoss: 14.9773\tLR: 0.010000\n",
      "2.3378\t2.3344\t2.3554\t2.4672\t2.6215\t2.8610\t\n",
      "Training Epoch: 6 [426/566]\tLoss: 20.2821\tLR: 0.010000\n",
      "4.1432\t3.8377\t3.4554\t3.1292\t2.9090\t2.8076\t\n",
      "Training Epoch: 6 [442/566]\tLoss: 19.1222\tLR: 0.010000\n",
      "3.3242\t3.2043\t3.0928\t3.0691\t3.1365\t3.2953\t\n",
      "Training Epoch: 6 [458/566]\tLoss: 15.1300\tLR: 0.010000\n",
      "2.7429\t2.5926\t2.4635\t2.3937\t2.4111\t2.5263\t\n",
      "Training Epoch: 6 [474/566]\tLoss: 7.3543\tLR: 0.010000\n",
      "1.0220\t1.0292\t1.0875\t1.2185\t1.3892\t1.6078\t\n",
      "Training Epoch: 6 [490/566]\tLoss: 17.1735\tLR: 0.010000\n",
      "3.2014\t2.9748\t2.7806\t2.7029\t2.7017\t2.8122\t\n",
      "Training Epoch: 6 [506/566]\tLoss: 20.1092\tLR: 0.010000\n",
      "3.7672\t3.5612\t3.3519\t3.2018\t3.1208\t3.1064\t\n",
      "Training Epoch: 6 [522/566]\tLoss: 16.3457\tLR: 0.010000\n",
      "3.1967\t2.9013\t2.6698\t2.5063\t2.4772\t2.5944\t\n",
      "Training Epoch: 6 [538/566]\tLoss: 15.2615\tLR: 0.010000\n",
      "2.5287\t2.4614\t2.4216\t2.4698\t2.5892\t2.7908\t\n",
      "Training Epoch: 6 [554/566]\tLoss: 9.1295\tLR: 0.010000\n",
      "1.5350\t1.5012\t1.4556\t1.4657\t1.5209\t1.6511\t\n",
      "Training Epoch: 6 [570/566]\tLoss: 13.6897\tLR: 0.010000\n",
      "2.2484\t2.2115\t2.1852\t2.2258\t2.3331\t2.4856\t\n",
      "Training Epoch: 6 [586/566]\tLoss: 23.9371\tLR: 0.010000\n",
      "4.0220\t3.9313\t3.8955\t3.9197\t4.0249\t4.1437\t\n",
      "Training Epoch: 6 [602/566]\tLoss: 25.6286\tLR: 0.010000\n",
      "5.0547\t4.6725\t4.2595\t3.9872\t3.8473\t3.8075\t\n",
      "Training Epoch: 6 [618/566]\tLoss: 17.6787\tLR: 0.010000\n",
      "3.1162\t3.0111\t2.8883\t2.8364\t2.8606\t2.9661\t\n",
      "Training Epoch: 6 [634/566]\tLoss: 19.4296\tLR: 0.010000\n",
      "3.5780\t3.4077\t3.2287\t3.1013\t3.0238\t3.0900\t\n",
      "Training Epoch: 6 [650/566]\tLoss: 15.3405\tLR: 0.010000\n",
      "2.7079\t2.6200\t2.5214\t2.4667\t2.4827\t2.5417\t\n",
      "Training Epoch: 6 [666/566]\tLoss: 9.7639\tLR: 0.010000\n",
      "1.8420\t1.7017\t1.5608\t1.4857\t1.5232\t1.6505\t\n",
      "Training Epoch: 6 [682/566]\tLoss: 21.7404\tLR: 0.010000\n",
      "4.1935\t3.8425\t3.5442\t3.4101\t3.3648\t3.3853\t\n",
      "Training Epoch: 6 [698/566]\tLoss: 18.6595\tLR: 0.010000\n",
      "3.5033\t3.2852\t3.0753\t2.9332\t2.8865\t2.9759\t\n",
      "Training Epoch: 6 [714/566]\tLoss: 18.3967\tLR: 0.010000\n",
      "3.5454\t3.3604\t3.1013\t2.8799\t2.7592\t2.7504\t\n",
      "Training Epoch: 6 [730/566]\tLoss: 34.7393\tLR: 0.010000\n",
      "7.1356\t6.4756\t5.8310\t5.3635\t5.0502\t4.8835\t\n",
      "Training Epoch: 6 [746/566]\tLoss: 15.2658\tLR: 0.010000\n",
      "2.3755\t2.3533\t2.4084\t2.5484\t2.7073\t2.8728\t\n",
      "Training Epoch: 6 [762/566]\tLoss: 20.8560\tLR: 0.010000\n",
      "3.9248\t3.7134\t3.4855\t3.2874\t3.2125\t3.2325\t\n",
      "Training Epoch: 6 [778/566]\tLoss: 20.8446\tLR: 0.010000\n",
      "4.1871\t3.8734\t3.4934\t3.2178\t3.0457\t3.0272\t\n",
      "Training Epoch: 6 [794/566]\tLoss: 16.3442\tLR: 0.010000\n",
      "3.3544\t3.0670\t2.7443\t2.5034\t2.3659\t2.3092\t\n",
      "Training Epoch: 6 [810/566]\tLoss: 23.1448\tLR: 0.010000\n",
      "4.3084\t4.0729\t3.8528\t3.6879\t3.6120\t3.6108\t\n",
      "Training Epoch: 6 [826/566]\tLoss: 16.7987\tLR: 0.010000\n",
      "3.1778\t2.9417\t2.7282\t2.6317\t2.6274\t2.6919\t\n",
      "Training Epoch: 6 [842/566]\tLoss: 18.9073\tLR: 0.010000\n",
      "3.1387\t3.0687\t3.0639\t3.1180\t3.1984\t3.3195\t\n",
      "Training Epoch: 6 [858/566]\tLoss: 4.2967\tLR: 0.010000\n",
      "0.6298\t0.5796\t0.5904\t0.6556\t0.7929\t1.0485\t\n",
      "Training Epoch: 6 [874/566]\tLoss: 15.3394\tLR: 0.010000\n",
      "2.4306\t2.3548\t2.3818\t2.5062\t2.7126\t2.9534\t\n",
      "Training Epoch: 6 [890/566]\tLoss: 16.9576\tLR: 0.010000\n",
      "3.1422\t2.9580\t2.7610\t2.6537\t2.6551\t2.7877\t\n",
      "Training Epoch: 6 [902/566]\tLoss: 16.3551\tLR: 0.010000\n",
      "2.8330\t2.6601\t2.5636\t2.5971\t2.7354\t2.9659\t\n",
      "[0.32229647040367126, 0.3985922634601593, 0.27911126613616943, 0.016639897599816322, 0.5722838044166565, 0.163929283618927, 0.2637869119644165, 0.01502107921987772, 0.5303551554679871, 0.14694958925247192, 0.322695255279541, -0.0010337464045733213, 0.3152945041656494, 0.25525832176208496, 0.4294471740722656, 0.0018127457005903125, 0.17285886406898499, 0.0, 0.827141135931015, 0.00042742519872263074]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3346, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3146, Accuracy: 0.4488\n",
      "Test set t = 02: Average loss: 0.2965, Accuracy: 0.4488\n",
      "Test set t = 03: Average loss: 0.2865, Accuracy: 0.4382\n",
      "Test set t = 04: Average loss: 0.2854, Accuracy: 0.4364\n",
      "Test set t = 05: Average loss: 0.2931, Accuracy: 0.4293\n",
      "\n",
      "Training Epoch: 7 [10/566]\tLoss: 12.8641\tLR: 0.010000\n",
      "2.5242\t2.3421\t2.1179\t1.9771\t1.9248\t1.9780\t\n",
      "Training Epoch: 7 [26/566]\tLoss: 17.1998\tLR: 0.010000\n",
      "3.1605\t2.9853\t2.8030\t2.7073\t2.7174\t2.8264\t\n",
      "Training Epoch: 7 [42/566]\tLoss: 17.1634\tLR: 0.010000\n",
      "3.0726\t2.8598\t2.7182\t2.6995\t2.8099\t3.0034\t\n",
      "Training Epoch: 7 [58/566]\tLoss: 25.6113\tLR: 0.010000\n",
      "5.0992\t4.6748\t4.2741\t3.9830\t3.8159\t3.7643\t\n",
      "Training Epoch: 7 [74/566]\tLoss: 9.1409\tLR: 0.010000\n",
      "1.5840\t1.5207\t1.4586\t1.4368\t1.4985\t1.6423\t\n",
      "Training Epoch: 7 [90/566]\tLoss: 15.9749\tLR: 0.010000\n",
      "3.0673\t2.8343\t2.6012\t2.4538\t2.4425\t2.5759\t\n",
      "Training Epoch: 7 [106/566]\tLoss: 15.9966\tLR: 0.010000\n",
      "2.7819\t2.6846\t2.5789\t2.5563\t2.6289\t2.7661\t\n",
      "Training Epoch: 7 [122/566]\tLoss: 18.0009\tLR: 0.010000\n",
      "3.1362\t3.0488\t2.9515\t2.8967\t2.9318\t3.0358\t\n",
      "Training Epoch: 7 [138/566]\tLoss: 28.9345\tLR: 0.010000\n",
      "6.2241\t5.6223\t4.9632\t4.4205\t4.0005\t3.7038\t\n",
      "Training Epoch: 7 [154/566]\tLoss: 11.1055\tLR: 0.010000\n",
      "1.8511\t1.7641\t1.7311\t1.7728\t1.8851\t2.1014\t\n",
      "Training Epoch: 7 [170/566]\tLoss: 15.7751\tLR: 0.010000\n",
      "2.9342\t2.7433\t2.5678\t2.4705\t2.4823\t2.5770\t\n",
      "Training Epoch: 7 [186/566]\tLoss: 14.4221\tLR: 0.010000\n",
      "2.3048\t2.2567\t2.2983\t2.3860\t2.4941\t2.6822\t\n",
      "Training Epoch: 7 [202/566]\tLoss: 17.4399\tLR: 0.010000\n",
      "3.1342\t2.9507\t2.8039\t2.7616\t2.8251\t2.9643\t\n",
      "Training Epoch: 7 [218/566]\tLoss: 20.6483\tLR: 0.010000\n",
      "3.7405\t3.6004\t3.4189\t3.3003\t3.2651\t3.3232\t\n",
      "Training Epoch: 7 [234/566]\tLoss: 17.4496\tLR: 0.010000\n",
      "3.0526\t2.8961\t2.8173\t2.8263\t2.8960\t2.9613\t\n",
      "Training Epoch: 7 [250/566]\tLoss: 16.3694\tLR: 0.010000\n",
      "2.9981\t2.8467\t2.6741\t2.5767\t2.5844\t2.6893\t\n",
      "Training Epoch: 7 [266/566]\tLoss: 31.0221\tLR: 0.010000\n",
      "6.2796\t5.8403\t5.3329\t4.8670\t4.4931\t4.2091\t\n",
      "Training Epoch: 7 [282/566]\tLoss: 12.8573\tLR: 0.010000\n",
      "2.2087\t2.1097\t2.0595\t2.0471\t2.1344\t2.2980\t\n",
      "Training Epoch: 7 [298/566]\tLoss: 15.3316\tLR: 0.010000\n",
      "2.6499\t2.5293\t2.4411\t2.4321\t2.5492\t2.7301\t\n",
      "Training Epoch: 7 [314/566]\tLoss: 18.8867\tLR: 0.010000\n",
      "3.6676\t3.4339\t3.1958\t2.9980\t2.8370\t2.7543\t\n",
      "Training Epoch: 7 [330/566]\tLoss: 24.0600\tLR: 0.010000\n",
      "4.7562\t4.4113\t4.0825\t3.8303\t3.5812\t3.3985\t\n",
      "Training Epoch: 7 [346/566]\tLoss: 21.8271\tLR: 0.010000\n",
      "4.1836\t3.8838\t3.5958\t3.4184\t3.3629\t3.3826\t\n",
      "Training Epoch: 7 [362/566]\tLoss: 21.8079\tLR: 0.010000\n",
      "3.8107\t3.7445\t3.6284\t3.5419\t3.5092\t3.5732\t\n",
      "Training Epoch: 7 [378/566]\tLoss: 21.2224\tLR: 0.010000\n",
      "4.0140\t3.7410\t3.4889\t3.3128\t3.2821\t3.3836\t\n",
      "Training Epoch: 7 [394/566]\tLoss: 12.6029\tLR: 0.010000\n",
      "2.2925\t2.1533\t2.0063\t1.9719\t2.0280\t2.1509\t\n",
      "Training Epoch: 7 [410/566]\tLoss: 25.4292\tLR: 0.010000\n",
      "4.9965\t4.5991\t4.2234\t3.9681\t3.8338\t3.8084\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [426/566]\tLoss: 19.9296\tLR: 0.010000\n",
      "3.4245\t3.3399\t3.2673\t3.2462\t3.2890\t3.3626\t\n",
      "Training Epoch: 7 [442/566]\tLoss: 7.5339\tLR: 0.010000\n",
      "1.0719\t1.0942\t1.1343\t1.2224\t1.3881\t1.6229\t\n",
      "Training Epoch: 7 [458/566]\tLoss: 19.8964\tLR: 0.010000\n",
      "3.6538\t3.4202\t3.2365\t3.1451\t3.1642\t3.2767\t\n",
      "Training Epoch: 7 [474/566]\tLoss: 12.6595\tLR: 0.010000\n",
      "2.2362\t2.1171\t2.0156\t1.9979\t2.0680\t2.2247\t\n",
      "Training Epoch: 7 [490/566]\tLoss: 19.8616\tLR: 0.010000\n",
      "3.8401\t3.5783\t3.3012\t3.1211\t3.0185\t3.0025\t\n",
      "Training Epoch: 7 [506/566]\tLoss: 21.6138\tLR: 0.010000\n",
      "3.9524\t3.7455\t3.5831\t3.4747\t3.4116\t3.4465\t\n",
      "Training Epoch: 7 [522/566]\tLoss: 13.9185\tLR: 0.010000\n",
      "2.3290\t2.2334\t2.1818\t2.2263\t2.3612\t2.5869\t\n",
      "Training Epoch: 7 [538/566]\tLoss: 15.3345\tLR: 0.010000\n",
      "2.7635\t2.5896\t2.4834\t2.4413\t2.4811\t2.5757\t\n",
      "Training Epoch: 7 [554/566]\tLoss: 24.4075\tLR: 0.010000\n",
      "4.1869\t4.0810\t3.9945\t3.9831\t4.0251\t4.1370\t\n",
      "Training Epoch: 7 [570/566]\tLoss: 15.6633\tLR: 0.010000\n",
      "2.7886\t2.6226\t2.5025\t2.4917\t2.5619\t2.6961\t\n",
      "Training Epoch: 7 [586/566]\tLoss: 12.3124\tLR: 0.010000\n",
      "2.0670\t1.9030\t1.8630\t1.9621\t2.1498\t2.3674\t\n",
      "Training Epoch: 7 [602/566]\tLoss: 18.2052\tLR: 0.010000\n",
      "3.2720\t3.0947\t2.9630\t2.9167\t2.9372\t3.0215\t\n",
      "Training Epoch: 7 [618/566]\tLoss: 9.9055\tLR: 0.010000\n",
      "1.7547\t1.7017\t1.6266\t1.5753\t1.5712\t1.6759\t\n",
      "Training Epoch: 7 [634/566]\tLoss: 26.9273\tLR: 0.010000\n",
      "4.8497\t4.6029\t4.4212\t4.3550\t4.3319\t4.3664\t\n",
      "Training Epoch: 7 [650/566]\tLoss: 24.8046\tLR: 0.010000\n",
      "4.6020\t4.3533\t4.1141\t3.9526\t3.8778\t3.9048\t\n",
      "Training Epoch: 7 [666/566]\tLoss: 6.7422\tLR: 0.010000\n",
      "0.9707\t0.9557\t1.0009\t1.1031\t1.2602\t1.4516\t\n",
      "Training Epoch: 7 [682/566]\tLoss: 28.4955\tLR: 0.010000\n",
      "6.0547\t5.4337\t4.7978\t4.3155\t4.0037\t3.8901\t\n",
      "Training Epoch: 7 [698/566]\tLoss: 11.1904\tLR: 0.010000\n",
      "2.0821\t1.9905\t1.8723\t1.7814\t1.7334\t1.7307\t\n",
      "Training Epoch: 7 [714/566]\tLoss: 19.7547\tLR: 0.010000\n",
      "4.0150\t3.6629\t3.3129\t3.0584\t2.8833\t2.8223\t\n",
      "Training Epoch: 7 [730/566]\tLoss: 14.6752\tLR: 0.010000\n",
      "2.4661\t2.3991\t2.3657\t2.3868\t2.4691\t2.5883\t\n",
      "Training Epoch: 7 [746/566]\tLoss: 19.0197\tLR: 0.010000\n",
      "3.6169\t3.3641\t3.1367\t2.9861\t2.9313\t2.9845\t\n",
      "Training Epoch: 7 [762/566]\tLoss: 31.9675\tLR: 0.010000\n",
      "6.7117\t6.0989\t5.4467\t4.9037\t4.5224\t4.2841\t\n",
      "Training Epoch: 7 [778/566]\tLoss: 9.8756\tLR: 0.010000\n",
      "1.2928\t1.3521\t1.4629\t1.6398\t1.9029\t2.2252\t\n",
      "Training Epoch: 7 [794/566]\tLoss: 19.4197\tLR: 0.010000\n",
      "3.5293\t3.3584\t3.1599\t3.0445\t3.0778\t3.2499\t\n",
      "Training Epoch: 7 [810/566]\tLoss: 18.3723\tLR: 0.010000\n",
      "3.5025\t3.2172\t2.9784\t2.8627\t2.8583\t2.9532\t\n",
      "Training Epoch: 7 [826/566]\tLoss: 7.4223\tLR: 0.010000\n",
      "1.0664\t1.0480\t1.0913\t1.2043\t1.3797\t1.6326\t\n",
      "Training Epoch: 7 [842/566]\tLoss: 20.0473\tLR: 0.010000\n",
      "3.5357\t3.3845\t3.2637\t3.2076\t3.2631\t3.3927\t\n",
      "Training Epoch: 7 [858/566]\tLoss: 20.5850\tLR: 0.010000\n",
      "3.9837\t3.6611\t3.4129\t3.2402\t3.1286\t3.1585\t\n",
      "Training Epoch: 7 [874/566]\tLoss: 18.3829\tLR: 0.010000\n",
      "3.4184\t3.1865\t2.9878\t2.8973\t2.9040\t2.9889\t\n",
      "Training Epoch: 7 [890/566]\tLoss: 18.5032\tLR: 0.010000\n",
      "3.2368\t3.0906\t3.0177\t3.0049\t3.0164\t3.1368\t\n",
      "Training Epoch: 7 [902/566]\tLoss: 26.4150\tLR: 0.010000\n",
      "5.5220\t5.0177\t4.5183\t4.0873\t3.7524\t3.5173\t\n",
      "[0.3088754117488861, 0.44131603837013245, 0.24980854988098145, 0.01710885763168335, 0.616576075553894, 0.15129384398460388, 0.23213008046150208, 0.015497046522796154, 0.5719082355499268, 0.13501548767089844, 0.2930762767791748, -0.0018810806795954704, 0.3234291076660156, 0.25795504450798035, 0.41861584782600403, 0.00127546361181885, 0.16690677404403687, 0.0, 0.8330932259559631, -0.0004983288235962391]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3381, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3176, Accuracy: 0.4523\n",
      "Test set t = 02: Average loss: 0.2993, Accuracy: 0.4505\n",
      "Test set t = 03: Average loss: 0.2890, Accuracy: 0.4399\n",
      "Test set t = 04: Average loss: 0.2873, Accuracy: 0.4364\n",
      "Test set t = 05: Average loss: 0.2942, Accuracy: 0.4346\n",
      "\n",
      "Training Epoch: 8 [10/566]\tLoss: 9.4352\tLR: 0.010000\n",
      "1.3184\t1.3292\t1.4238\t1.5572\t1.7735\t2.0331\t\n",
      "Training Epoch: 8 [26/566]\tLoss: 23.6373\tLR: 0.010000\n",
      "4.5541\t4.1819\t3.9026\t3.7326\t3.6310\t3.6350\t\n",
      "Training Epoch: 8 [42/566]\tLoss: 18.3503\tLR: 0.010000\n",
      "3.5102\t3.2860\t3.0515\t2.8891\t2.8156\t2.7979\t\n",
      "Training Epoch: 8 [58/566]\tLoss: 9.7602\tLR: 0.010000\n",
      "1.6658\t1.5942\t1.5330\t1.5378\t1.6264\t1.8031\t\n",
      "Training Epoch: 8 [74/566]\tLoss: 32.1673\tLR: 0.010000\n",
      "6.5243\t5.9747\t5.4361\t5.0025\t4.7105\t4.5192\t\n",
      "Training Epoch: 8 [90/566]\tLoss: 10.6806\tLR: 0.010000\n",
      "1.4947\t1.5348\t1.6282\t1.7788\t1.9975\t2.2467\t\n",
      "Training Epoch: 8 [106/566]\tLoss: 21.3319\tLR: 0.010000\n",
      "3.9044\t3.6806\t3.4783\t3.3961\t3.3917\t3.4809\t\n",
      "Training Epoch: 8 [122/566]\tLoss: 30.3250\tLR: 0.010000\n",
      "6.3538\t5.8036\t5.2291\t4.7256\t4.2684\t3.9445\t\n",
      "Training Epoch: 8 [138/566]\tLoss: 6.2335\tLR: 0.010000\n",
      "0.7229\t0.7852\t0.9007\t1.0569\t1.2594\t1.5084\t\n",
      "Training Epoch: 8 [154/566]\tLoss: 10.6089\tLR: 0.010000\n",
      "1.7099\t1.6360\t1.6032\t1.6648\t1.8574\t2.1375\t\n",
      "Training Epoch: 8 [170/566]\tLoss: 7.5760\tLR: 0.010000\n",
      "1.2629\t1.1967\t1.1715\t1.1891\t1.2871\t1.4686\t\n",
      "Training Epoch: 8 [186/566]\tLoss: 23.8316\tLR: 0.010000\n",
      "4.8698\t4.4067\t3.9815\t3.6774\t3.4791\t3.4170\t\n",
      "Training Epoch: 8 [202/566]\tLoss: 12.8887\tLR: 0.010000\n",
      "2.4186\t2.1966\t2.0525\t2.0018\t2.0451\t2.1741\t\n",
      "Training Epoch: 8 [218/566]\tLoss: 13.5800\tLR: 0.010000\n",
      "2.0934\t2.0966\t2.1486\t2.2474\t2.3949\t2.5991\t\n",
      "Training Epoch: 8 [234/566]\tLoss: 28.7957\tLR: 0.010000\n",
      "5.7771\t5.3014\t4.8460\t4.4997\t4.2536\t4.1179\t\n",
      "Training Epoch: 8 [250/566]\tLoss: 11.2648\tLR: 0.010000\n",
      "1.9635\t1.7893\t1.6974\t1.7436\t1.9102\t2.1608\t\n",
      "Training Epoch: 8 [266/566]\tLoss: 9.8780\tLR: 0.010000\n",
      "1.4494\t1.4354\t1.5009\t1.6223\t1.8105\t2.0595\t\n",
      "Training Epoch: 8 [282/566]\tLoss: 12.8839\tLR: 0.010000\n",
      "2.3465\t2.2227\t2.0860\t2.0183\t2.0481\t2.1624\t\n",
      "Training Epoch: 8 [298/566]\tLoss: 22.8331\tLR: 0.010000\n",
      "4.4584\t4.1830\t3.8472\t3.5649\t3.3948\t3.3848\t\n",
      "Training Epoch: 8 [314/566]\tLoss: 16.5537\tLR: 0.010000\n",
      "2.7357\t2.6223\t2.5743\t2.6515\t2.8583\t3.1116\t\n",
      "Training Epoch: 8 [330/566]\tLoss: 18.2038\tLR: 0.010000\n",
      "3.2060\t3.0506\t2.9610\t2.9344\t2.9780\t3.0739\t\n",
      "Training Epoch: 8 [346/566]\tLoss: 22.7184\tLR: 0.010000\n",
      "4.2472\t4.0368\t3.7701\t3.6112\t3.5380\t3.5151\t\n",
      "Training Epoch: 8 [362/566]\tLoss: 14.5245\tLR: 0.010000\n",
      "2.3068\t2.2560\t2.2784\t2.4008\t2.5546\t2.7279\t\n",
      "Training Epoch: 8 [378/566]\tLoss: 21.2714\tLR: 0.010000\n",
      "3.8328\t3.6680\t3.5000\t3.3973\t3.3867\t3.4866\t\n",
      "Training Epoch: 8 [394/566]\tLoss: 23.8561\tLR: 0.010000\n",
      "4.5010\t4.2147\t3.9483\t3.7813\t3.7035\t3.7073\t\n",
      "Training Epoch: 8 [410/566]\tLoss: 14.1208\tLR: 0.010000\n",
      "2.2118\t2.2271\t2.2808\t2.3532\t2.4436\t2.6043\t\n",
      "Training Epoch: 8 [426/566]\tLoss: 16.6714\tLR: 0.010000\n",
      "2.8613\t2.7595\t2.6838\t2.6971\t2.7738\t2.8959\t\n",
      "Training Epoch: 8 [442/566]\tLoss: 13.8109\tLR: 0.010000\n",
      "2.3520\t2.2515\t2.2042\t2.2338\t2.3175\t2.4519\t\n",
      "Training Epoch: 8 [458/566]\tLoss: 21.1210\tLR: 0.010000\n",
      "3.8968\t3.6480\t3.4607\t3.3664\t3.3701\t3.3789\t\n",
      "Training Epoch: 8 [474/566]\tLoss: 24.0360\tLR: 0.010000\n",
      "5.2854\t4.7318\t4.1353\t3.6262\t3.2364\t3.0209\t\n",
      "Training Epoch: 8 [490/566]\tLoss: 22.3262\tLR: 0.010000\n",
      "4.1918\t3.9404\t3.7089\t3.5447\t3.4668\t3.4735\t\n",
      "Training Epoch: 8 [506/566]\tLoss: 28.3755\tLR: 0.010000\n",
      "5.7226\t5.2673\t4.7804\t4.3990\t4.1541\t4.0520\t\n",
      "Training Epoch: 8 [522/566]\tLoss: 26.1305\tLR: 0.010000\n",
      "4.4491\t4.3253\t4.2739\t4.3087\t4.3569\t4.4165\t\n",
      "Training Epoch: 8 [538/566]\tLoss: 9.3176\tLR: 0.010000\n",
      "1.6212\t1.5241\t1.4824\t1.4871\t1.5453\t1.6575\t\n",
      "Training Epoch: 8 [554/566]\tLoss: 25.8311\tLR: 0.010000\n",
      "5.2897\t4.8604\t4.3776\t3.9821\t3.7240\t3.5972\t\n",
      "Training Epoch: 8 [570/566]\tLoss: 17.4178\tLR: 0.010000\n",
      "3.1414\t2.9734\t2.8129\t2.7467\t2.8038\t2.9397\t\n",
      "Training Epoch: 8 [586/566]\tLoss: 19.1652\tLR: 0.010000\n",
      "3.9979\t3.5905\t3.1953\t2.8988\t2.7493\t2.7333\t\n",
      "Training Epoch: 8 [602/566]\tLoss: 34.2262\tLR: 0.010000\n",
      "7.0309\t6.4495\t5.8260\t5.3062\t4.9202\t4.6934\t\n",
      "Training Epoch: 8 [618/566]\tLoss: 19.5810\tLR: 0.010000\n",
      "3.8637\t3.5234\t3.2365\t3.0400\t2.9438\t2.9736\t\n",
      "Training Epoch: 8 [634/566]\tLoss: 20.4643\tLR: 0.010000\n",
      "3.8602\t3.6095\t3.3930\t3.2449\t3.1640\t3.1927\t\n",
      "Training Epoch: 8 [650/566]\tLoss: 18.4275\tLR: 0.010000\n",
      "3.8560\t3.4880\t3.0820\t2.7855\t2.6277\t2.5883\t\n",
      "Training Epoch: 8 [666/566]\tLoss: 17.0743\tLR: 0.010000\n",
      "3.0812\t2.9048\t2.7710\t2.7193\t2.7546\t2.8434\t\n",
      "Training Epoch: 8 [682/566]\tLoss: 13.5565\tLR: 0.010000\n",
      "2.0914\t2.0544\t2.0932\t2.2207\t2.4221\t2.6748\t\n",
      "Training Epoch: 8 [698/566]\tLoss: 12.7986\tLR: 0.010000\n",
      "2.3040\t2.1741\t2.0713\t2.0426\t2.0678\t2.1388\t\n",
      "Training Epoch: 8 [714/566]\tLoss: 19.1913\tLR: 0.010000\n",
      "3.3625\t3.2850\t3.1820\t3.1272\t3.1005\t3.1341\t\n",
      "Training Epoch: 8 [730/566]\tLoss: 15.5734\tLR: 0.010000\n",
      "2.5974\t2.5273\t2.4890\t2.5183\t2.6257\t2.8159\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [746/566]\tLoss: 23.0381\tLR: 0.010000\n",
      "4.5183\t4.2221\t3.8935\t3.6258\t3.4401\t3.3382\t\n",
      "Training Epoch: 8 [762/566]\tLoss: 13.4788\tLR: 0.010000\n",
      "2.1891\t2.1261\t2.0899\t2.1494\t2.3322\t2.5920\t\n",
      "Training Epoch: 8 [778/566]\tLoss: 8.9181\tLR: 0.010000\n",
      "1.3258\t1.3412\t1.4053\t1.4920\t1.5943\t1.7595\t\n",
      "Training Epoch: 8 [794/566]\tLoss: 24.8925\tLR: 0.010000\n",
      "4.8794\t4.5321\t4.1815\t3.8886\t3.7245\t3.6865\t\n",
      "Training Epoch: 8 [810/566]\tLoss: 19.8943\tLR: 0.010000\n",
      "3.7646\t3.5702\t3.3097\t3.1177\t3.0373\t3.0949\t\n",
      "Training Epoch: 8 [826/566]\tLoss: 10.5883\tLR: 0.010000\n",
      "2.0570\t1.8566\t1.6915\t1.6320\t1.6332\t1.7181\t\n",
      "Training Epoch: 8 [842/566]\tLoss: 23.8289\tLR: 0.010000\n",
      "4.5975\t4.2388\t3.9692\t3.7793\t3.6500\t3.5941\t\n",
      "Training Epoch: 8 [858/566]\tLoss: 18.4480\tLR: 0.010000\n",
      "3.6311\t3.3502\t3.0675\t2.8561\t2.7518\t2.7913\t\n",
      "Training Epoch: 8 [874/566]\tLoss: 15.1630\tLR: 0.010000\n",
      "2.5675\t2.4992\t2.4248\t2.4141\t2.5201\t2.7372\t\n",
      "Training Epoch: 8 [890/566]\tLoss: 17.1191\tLR: 0.010000\n",
      "2.7539\t2.7075\t2.7144\t2.7943\t2.9612\t3.1878\t\n",
      "Training Epoch: 8 [902/566]\tLoss: 6.1537\tLR: 0.010000\n",
      "0.8876\t0.8578\t0.8837\t0.9747\t1.1618\t1.3881\t\n",
      "[0.29797878861427307, 0.47429656982421875, 0.22772464156150818, 0.017421770840883255, 0.6502059102058411, 0.14160414040088654, 0.2081899493932724, 0.015810800716280937, 0.6078330278396606, 0.12685631215572357, 0.2653106600046158, -0.002622197847813368, 0.3288215696811676, 0.26863887906074524, 0.40253955125808716, 0.0008400479564443231, 0.17008757591247559, 0.0, 0.8299124240875244, -0.0010083212982863188]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3350, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3135, Accuracy: 0.4523\n",
      "Test set t = 02: Average loss: 0.2953, Accuracy: 0.4488\n",
      "Test set t = 03: Average loss: 0.2858, Accuracy: 0.4452\n",
      "Test set t = 04: Average loss: 0.2857, Accuracy: 0.4399\n",
      "Test set t = 05: Average loss: 0.2946, Accuracy: 0.4329\n",
      "\n",
      "Training Epoch: 9 [10/566]\tLoss: 20.3816\tLR: 0.010000\n",
      "3.6304\t3.4701\t3.3295\t3.2755\t3.3027\t3.3733\t\n",
      "Training Epoch: 9 [26/566]\tLoss: 32.4606\tLR: 0.010000\n",
      "6.5155\t6.0040\t5.4812\t5.0847\t4.7977\t4.5775\t\n",
      "Training Epoch: 9 [42/566]\tLoss: 16.3287\tLR: 0.010000\n",
      "2.8427\t2.7119\t2.6395\t2.6338\t2.6944\t2.8064\t\n",
      "Training Epoch: 9 [58/566]\tLoss: 16.5016\tLR: 0.010000\n",
      "3.1120\t2.8612\t2.6710\t2.5640\t2.5889\t2.7044\t\n",
      "Training Epoch: 9 [74/566]\tLoss: 19.5190\tLR: 0.010000\n",
      "3.7603\t3.4568\t3.2025\t3.0490\t2.9829\t3.0675\t\n",
      "Training Epoch: 9 [90/566]\tLoss: 20.8523\tLR: 0.010000\n",
      "3.7794\t3.5951\t3.4328\t3.3342\t3.3150\t3.3958\t\n",
      "Training Epoch: 9 [106/566]\tLoss: 23.5458\tLR: 0.010000\n",
      "4.6908\t4.2749\t3.9134\t3.6504\t3.5047\t3.5116\t\n",
      "Training Epoch: 9 [122/566]\tLoss: 13.0064\tLR: 0.010000\n",
      "2.4190\t2.2438\t2.0817\t2.0162\t2.0558\t2.1898\t\n",
      "Training Epoch: 9 [138/566]\tLoss: 18.3388\tLR: 0.010000\n",
      "3.5309\t3.2611\t3.0220\t2.8675\t2.8045\t2.8529\t\n",
      "Training Epoch: 9 [154/566]\tLoss: 20.0678\tLR: 0.010000\n",
      "3.6462\t3.4593\t3.2723\t3.1789\t3.2128\t3.2982\t\n",
      "Training Epoch: 9 [170/566]\tLoss: 16.9173\tLR: 0.010000\n",
      "2.9151\t2.7996\t2.6973\t2.7056\t2.8119\t2.9878\t\n",
      "Training Epoch: 9 [186/566]\tLoss: 24.6701\tLR: 0.010000\n",
      "4.8677\t4.4903\t4.1477\t3.8940\t3.7004\t3.5700\t\n",
      "Training Epoch: 9 [202/566]\tLoss: 9.7737\tLR: 0.010000\n",
      "1.6689\t1.5895\t1.5573\t1.5679\t1.6326\t1.7575\t\n",
      "Training Epoch: 9 [218/566]\tLoss: 17.2914\tLR: 0.010000\n",
      "3.0927\t2.8710\t2.7520\t2.7550\t2.8381\t2.9827\t\n",
      "Training Epoch: 9 [234/566]\tLoss: 11.3109\tLR: 0.010000\n",
      "1.6423\t1.6514\t1.7207\t1.8769\t2.0839\t2.3358\t\n",
      "Training Epoch: 9 [250/566]\tLoss: 21.2512\tLR: 0.010000\n",
      "4.3460\t3.9575\t3.5781\t3.2722\t3.0850\t3.0124\t\n",
      "Training Epoch: 9 [266/566]\tLoss: 26.2168\tLR: 0.010000\n",
      "5.8345\t5.1609\t4.4723\t3.8950\t3.5227\t3.3315\t\n",
      "Training Epoch: 9 [282/566]\tLoss: 20.6197\tLR: 0.010000\n",
      "3.7921\t3.5563\t3.3863\t3.2991\t3.2605\t3.3253\t\n",
      "Training Epoch: 9 [298/566]\tLoss: 15.1477\tLR: 0.010000\n",
      "2.8512\t2.6338\t2.4452\t2.3591\t2.3724\t2.4862\t\n",
      "Training Epoch: 9 [314/566]\tLoss: 17.2654\tLR: 0.010000\n",
      "2.9105\t2.7842\t2.7189\t2.7655\t2.9213\t3.1649\t\n",
      "Training Epoch: 9 [330/566]\tLoss: 24.1883\tLR: 0.010000\n",
      "4.9662\t4.5191\t4.0923\t3.7419\t3.4915\t3.3773\t\n",
      "Training Epoch: 9 [346/566]\tLoss: 13.9913\tLR: 0.010000\n",
      "2.1839\t2.1676\t2.1912\t2.2921\t2.4678\t2.6887\t\n",
      "Training Epoch: 9 [362/566]\tLoss: 9.1565\tLR: 0.010000\n",
      "1.5866\t1.4868\t1.4112\t1.4194\t1.5322\t1.7202\t\n",
      "Training Epoch: 9 [378/566]\tLoss: 18.1033\tLR: 0.010000\n",
      "3.0118\t2.9242\t2.8862\t2.9437\t3.0686\t3.2688\t\n",
      "Training Epoch: 9 [394/566]\tLoss: 23.5599\tLR: 0.010000\n",
      "4.2247\t4.0505\t3.8994\t3.8079\t3.7718\t3.8057\t\n",
      "Training Epoch: 9 [410/566]\tLoss: 11.0563\tLR: 0.010000\n",
      "1.7082\t1.6783\t1.7030\t1.8199\t1.9769\t2.1700\t\n",
      "Training Epoch: 9 [426/566]\tLoss: 23.8060\tLR: 0.010000\n",
      "4.9980\t4.4965\t4.0104\t3.6398\t3.3923\t3.2690\t\n",
      "Training Epoch: 9 [442/566]\tLoss: 12.7131\tLR: 0.010000\n",
      "2.1081\t2.0502\t2.0321\t2.0641\t2.1572\t2.3014\t\n",
      "Training Epoch: 9 [458/566]\tLoss: 23.5150\tLR: 0.010000\n",
      "4.5097\t4.1641\t3.8663\t3.6957\t3.6265\t3.6527\t\n",
      "Training Epoch: 9 [474/566]\tLoss: 15.8483\tLR: 0.010000\n",
      "3.0090\t2.8184\t2.6325\t2.4867\t2.4165\t2.4852\t\n",
      "Training Epoch: 9 [490/566]\tLoss: 17.8037\tLR: 0.010000\n",
      "3.4193\t3.1761\t2.9239\t2.7704\t2.7277\t2.7863\t\n",
      "Training Epoch: 9 [506/566]\tLoss: 24.2164\tLR: 0.010000\n",
      "4.4746\t4.2443\t4.0494\t3.9003\t3.7906\t3.7571\t\n",
      "Training Epoch: 9 [522/566]\tLoss: 10.8849\tLR: 0.010000\n",
      "1.6965\t1.5893\t1.6041\t1.7450\t1.9812\t2.2688\t\n",
      "Training Epoch: 9 [538/566]\tLoss: 39.0784\tLR: 0.010000\n",
      "8.2459\t7.4532\t6.6782\t6.0250\t5.5187\t5.1574\t\n",
      "Training Epoch: 9 [554/566]\tLoss: 19.8013\tLR: 0.010000\n",
      "3.5807\t3.3799\t3.2358\t3.1758\t3.1695\t3.2596\t\n",
      "Training Epoch: 9 [570/566]\tLoss: 9.1652\tLR: 0.010000\n",
      "1.4418\t1.3929\t1.3867\t1.4613\t1.6231\t1.8593\t\n",
      "Training Epoch: 9 [586/566]\tLoss: 16.5403\tLR: 0.010000\n",
      "2.9380\t2.7699\t2.6732\t2.6489\t2.7012\t2.8092\t\n",
      "Training Epoch: 9 [602/566]\tLoss: 18.7919\tLR: 0.010000\n",
      "3.5957\t3.2855\t3.0696\t2.9300\t2.9024\t3.0087\t\n",
      "Training Epoch: 9 [618/566]\tLoss: 18.9458\tLR: 0.010000\n",
      "3.3834\t3.1977\t3.0840\t3.0415\t3.0607\t3.1785\t\n",
      "Training Epoch: 9 [634/566]\tLoss: 27.0962\tLR: 0.010000\n",
      "4.9079\t4.6820\t4.4784\t4.3650\t4.3252\t4.3378\t\n",
      "Training Epoch: 9 [650/566]\tLoss: 19.5973\tLR: 0.010000\n",
      "3.4527\t3.2560\t3.0947\t3.0609\t3.2293\t3.5037\t\n",
      "Training Epoch: 9 [666/566]\tLoss: 15.6560\tLR: 0.010000\n",
      "2.7707\t2.6320\t2.5242\t2.4757\t2.5349\t2.7184\t\n",
      "Training Epoch: 9 [682/566]\tLoss: 17.2314\tLR: 0.010000\n",
      "2.6908\t2.6810\t2.7159\t2.8164\t3.0230\t3.3042\t\n",
      "Training Epoch: 9 [698/566]\tLoss: 11.7437\tLR: 0.010000\n",
      "1.8347\t1.7478\t1.7713\t1.9116\t2.1201\t2.3581\t\n",
      "Training Epoch: 9 [714/566]\tLoss: 15.4523\tLR: 0.010000\n",
      "3.0122\t2.8036\t2.5559\t2.4107\t2.3321\t2.3377\t\n",
      "Training Epoch: 9 [730/566]\tLoss: 21.1169\tLR: 0.010000\n",
      "4.2233\t3.8412\t3.4859\t3.2438\t3.1440\t3.1787\t\n",
      "Training Epoch: 9 [746/566]\tLoss: 13.2254\tLR: 0.010000\n",
      "2.2932\t2.2177\t2.1599\t2.1187\t2.1469\t2.2891\t\n",
      "Training Epoch: 9 [762/566]\tLoss: 14.2040\tLR: 0.010000\n",
      "2.2555\t2.2523\t2.2404\t2.3026\t2.4615\t2.6918\t\n",
      "Training Epoch: 9 [778/566]\tLoss: 14.1738\tLR: 0.010000\n",
      "2.6599\t2.4484\t2.2999\t2.2212\t2.2189\t2.3255\t\n",
      "Training Epoch: 9 [794/566]\tLoss: 14.4134\tLR: 0.010000\n",
      "2.6432\t2.4793\t2.3369\t2.2636\t2.3050\t2.3854\t\n",
      "Training Epoch: 9 [810/566]\tLoss: 23.9583\tLR: 0.010000\n",
      "5.0177\t4.5071\t4.0000\t3.6335\t3.4349\t3.3650\t\n",
      "Training Epoch: 9 [826/566]\tLoss: 8.9908\tLR: 0.010000\n",
      "1.2286\t1.2197\t1.3023\t1.4781\t1.7247\t2.0374\t\n",
      "Training Epoch: 9 [842/566]\tLoss: 19.8568\tLR: 0.010000\n",
      "3.4128\t3.3010\t3.2149\t3.2141\t3.2878\t3.4262\t\n",
      "Training Epoch: 9 [858/566]\tLoss: 17.7366\tLR: 0.010000\n",
      "3.7927\t3.3703\t2.9738\t2.6808\t2.4844\t2.4345\t\n",
      "Training Epoch: 9 [874/566]\tLoss: 6.6559\tLR: 0.010000\n",
      "1.1198\t1.0812\t1.0558\t1.0524\t1.1124\t1.2343\t\n",
      "Training Epoch: 9 [890/566]\tLoss: 15.2365\tLR: 0.010000\n",
      "2.7333\t2.5917\t2.4625\t2.3930\t2.4398\t2.6162\t\n",
      "Training Epoch: 9 [902/566]\tLoss: 18.5398\tLR: 0.010000\n",
      "3.5558\t3.3310\t3.0823\t2.8846\t2.8154\t2.8708\t\n",
      "[0.2935512661933899, 0.4928566813468933, 0.2135920524597168, 0.01742684841156006, 0.6865584850311279, 0.1294814944267273, 0.18396002054214478, 0.01612284779548645, 0.6454480886459351, 0.11751539260149002, 0.23703651875257492, -0.0036397441290318966, 0.33193573355674744, 0.27309954166412354, 0.39496472477912903, 0.00019930943381041288, 0.1612323373556137, 0.0, 0.8387676626443863, -0.001758637255989015]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3356, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3145, Accuracy: 0.4523\n",
      "Test set t = 02: Average loss: 0.2963, Accuracy: 0.4470\n",
      "Test set t = 03: Average loss: 0.2861, Accuracy: 0.4488\n",
      "Test set t = 04: Average loss: 0.2845, Accuracy: 0.4399\n",
      "Test set t = 05: Average loss: 0.2912, Accuracy: 0.4382\n",
      "\n",
      "Training Epoch: 10 [10/566]\tLoss: 16.9126\tLR: 0.010000\n",
      "2.9205\t2.7663\t2.6787\t2.6999\t2.8287\t3.0186\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 [26/566]\tLoss: 13.5932\tLR: 0.010000\n",
      "2.6775\t2.4948\t2.3062\t2.1389\t2.0143\t1.9616\t\n",
      "Training Epoch: 10 [42/566]\tLoss: 24.6552\tLR: 0.010000\n",
      "4.7007\t4.4084\t4.1299\t3.9128\t3.7736\t3.7299\t\n",
      "Training Epoch: 10 [58/566]\tLoss: 7.1974\tLR: 0.010000\n",
      "0.9524\t0.9760\t1.0651\t1.2064\t1.3903\t1.6072\t\n",
      "Training Epoch: 10 [74/566]\tLoss: 24.8155\tLR: 0.010000\n",
      "5.2825\t4.7560\t4.2043\t3.7729\t3.4683\t3.3315\t\n",
      "Training Epoch: 10 [90/566]\tLoss: 12.7720\tLR: 0.010000\n",
      "2.0899\t2.0233\t2.0109\t2.0723\t2.1981\t2.3775\t\n",
      "Training Epoch: 10 [106/566]\tLoss: 12.9177\tLR: 0.010000\n",
      "2.2064\t2.1355\t2.0815\t2.0655\t2.1322\t2.2966\t\n",
      "Training Epoch: 10 [122/566]\tLoss: 15.7425\tLR: 0.010000\n",
      "2.8364\t2.6640\t2.5334\t2.4885\t2.5406\t2.6796\t\n",
      "Training Epoch: 10 [138/566]\tLoss: 15.6954\tLR: 0.010000\n",
      "2.5993\t2.5408\t2.4928\t2.5211\t2.6586\t2.8828\t\n",
      "Training Epoch: 10 [154/566]\tLoss: 16.9909\tLR: 0.010000\n",
      "2.8905\t2.7860\t2.7191\t2.7332\t2.8424\t3.0197\t\n",
      "Training Epoch: 10 [170/566]\tLoss: 16.8562\tLR: 0.010000\n",
      "2.8486\t2.7473\t2.6958\t2.7181\t2.8306\t3.0157\t\n",
      "Training Epoch: 10 [186/566]\tLoss: 20.5471\tLR: 0.010000\n",
      "4.0279\t3.7129\t3.4148\t3.2242\t3.1099\t3.0575\t\n",
      "Training Epoch: 10 [202/566]\tLoss: 22.6302\tLR: 0.010000\n",
      "4.6008\t4.2047\t3.8083\t3.5207\t3.3163\t3.1794\t\n",
      "Training Epoch: 10 [218/566]\tLoss: 9.6278\tLR: 0.010000\n",
      "1.4676\t1.4817\t1.5179\t1.5796\t1.7036\t1.8774\t\n",
      "Training Epoch: 10 [234/566]\tLoss: 23.9406\tLR: 0.010000\n",
      "4.5355\t4.2089\t3.9331\t3.7884\t3.7256\t3.7491\t\n",
      "Training Epoch: 10 [250/566]\tLoss: 19.5161\tLR: 0.010000\n",
      "3.4706\t3.3224\t3.2138\t3.1498\t3.1395\t3.2200\t\n",
      "Training Epoch: 10 [266/566]\tLoss: 19.1705\tLR: 0.010000\n",
      "3.2880\t3.1744\t3.0988\t3.1114\t3.1773\t3.3205\t\n",
      "Training Epoch: 10 [282/566]\tLoss: 18.3369\tLR: 0.010000\n",
      "3.5237\t3.3062\t3.0568\t2.8701\t2.7816\t2.7984\t\n",
      "Training Epoch: 10 [298/566]\tLoss: 15.8147\tLR: 0.010000\n",
      "2.8198\t2.6465\t2.5216\t2.5020\t2.5863\t2.7385\t\n",
      "Training Epoch: 10 [314/566]\tLoss: 16.6690\tLR: 0.010000\n",
      "2.8853\t2.7820\t2.6803\t2.6676\t2.7528\t2.9010\t\n",
      "Training Epoch: 10 [330/566]\tLoss: 13.6949\tLR: 0.010000\n",
      "2.8277\t2.5474\t2.2854\t2.0935\t1.9813\t1.9596\t\n",
      "Training Epoch: 10 [346/566]\tLoss: 11.7537\tLR: 0.010000\n",
      "1.7078\t1.7186\t1.7980\t1.9561\t2.1684\t2.4050\t\n",
      "Training Epoch: 10 [362/566]\tLoss: 21.7688\tLR: 0.010000\n",
      "3.8810\t3.6957\t3.5934\t3.5452\t3.5247\t3.5288\t\n",
      "Training Epoch: 10 [378/566]\tLoss: 20.5208\tLR: 0.010000\n",
      "3.7360\t3.5422\t3.3539\t3.2693\t3.2782\t3.3411\t\n",
      "Training Epoch: 10 [394/566]\tLoss: 22.2029\tLR: 0.010000\n",
      "4.2419\t3.9947\t3.7354\t3.5283\t3.3732\t3.3294\t\n",
      "Training Epoch: 10 [410/566]\tLoss: 23.0642\tLR: 0.010000\n",
      "4.1263\t3.9117\t3.7675\t3.6979\t3.7346\t3.8261\t\n",
      "Training Epoch: 10 [426/566]\tLoss: 16.7195\tLR: 0.010000\n",
      "2.7720\t2.7075\t2.6975\t2.7441\t2.8280\t2.9704\t\n",
      "Training Epoch: 10 [442/566]\tLoss: 20.6078\tLR: 0.010000\n",
      "3.9372\t3.6932\t3.4381\t3.2501\t3.1508\t3.1384\t\n",
      "Training Epoch: 10 [458/566]\tLoss: 14.9264\tLR: 0.010000\n",
      "2.7800\t2.5830\t2.4427\t2.3669\t2.3483\t2.4056\t\n",
      "Training Epoch: 10 [474/566]\tLoss: 19.1397\tLR: 0.010000\n",
      "3.2202\t3.1340\t3.1034\t3.1319\t3.2074\t3.3428\t\n",
      "Training Epoch: 10 [490/566]\tLoss: 9.4152\tLR: 0.010000\n",
      "1.7422\t1.6507\t1.5629\t1.5045\t1.4659\t1.4890\t\n",
      "Training Epoch: 10 [506/566]\tLoss: 13.0700\tLR: 0.010000\n",
      "2.1444\t2.1146\t2.1010\t2.1360\t2.2152\t2.3588\t\n",
      "Training Epoch: 10 [522/566]\tLoss: 15.6164\tLR: 0.010000\n",
      "3.0120\t2.7550\t2.5513\t2.4363\t2.4117\t2.4501\t\n",
      "Training Epoch: 10 [538/566]\tLoss: 25.1884\tLR: 0.010000\n",
      "4.4429\t4.3071\t4.1735\t4.1050\t4.0797\t4.0803\t\n",
      "Training Epoch: 10 [554/566]\tLoss: 10.1671\tLR: 0.010000\n",
      "1.7918\t1.6942\t1.6320\t1.6299\t1.6670\t1.7523\t\n",
      "Training Epoch: 10 [570/566]\tLoss: 22.7792\tLR: 0.010000\n",
      "4.5052\t4.1524\t3.7890\t3.5441\t3.4116\t3.3769\t\n",
      "Training Epoch: 10 [586/566]\tLoss: 23.0824\tLR: 0.010000\n",
      "4.9778\t4.4775\t3.9516\t3.5148\t3.1838\t2.9769\t\n",
      "Training Epoch: 10 [602/566]\tLoss: 24.0812\tLR: 0.010000\n",
      "4.7726\t4.3786\t4.0175\t3.7437\t3.5952\t3.5736\t\n",
      "Training Epoch: 10 [618/566]\tLoss: 21.7668\tLR: 0.010000\n",
      "4.3716\t4.0197\t3.6683\t3.4057\t3.2080\t3.0936\t\n",
      "Training Epoch: 10 [634/566]\tLoss: 26.8481\tLR: 0.010000\n",
      "5.2421\t4.7982\t4.4547\t4.2389\t4.0912\t4.0231\t\n",
      "Training Epoch: 10 [650/566]\tLoss: 18.8402\tLR: 0.010000\n",
      "3.5682\t3.3655\t3.1533\t2.9882\t2.8952\t2.8698\t\n",
      "Training Epoch: 10 [666/566]\tLoss: 12.9672\tLR: 0.010000\n",
      "2.2743\t2.1330\t2.0677\t2.0686\t2.1345\t2.2891\t\n",
      "Training Epoch: 10 [682/566]\tLoss: 17.7765\tLR: 0.010000\n",
      "3.3606\t3.1170\t2.9284\t2.8057\t2.7666\t2.7981\t\n",
      "Training Epoch: 10 [698/566]\tLoss: 19.1364\tLR: 0.010000\n",
      "3.5364\t3.3425\t3.1728\t3.0488\t2.9967\t3.0393\t\n",
      "Training Epoch: 10 [714/566]\tLoss: 12.7096\tLR: 0.010000\n",
      "2.2259\t2.1080\t2.0294\t2.0146\t2.0931\t2.2386\t\n",
      "Training Epoch: 10 [730/566]\tLoss: 23.7606\tLR: 0.010000\n",
      "4.9452\t4.4578\t4.0138\t3.6738\t3.4168\t3.2530\t\n",
      "Training Epoch: 10 [746/566]\tLoss: 26.1330\tLR: 0.010000\n",
      "5.1051\t4.7409\t4.3972\t4.1331\t3.9418\t3.8148\t\n",
      "Training Epoch: 10 [762/566]\tLoss: 18.3821\tLR: 0.010000\n",
      "3.6536\t3.3727\t3.0837\t2.8492\t2.7235\t2.6993\t\n",
      "Training Epoch: 10 [778/566]\tLoss: 21.9046\tLR: 0.010000\n",
      "4.4276\t4.0697\t3.6854\t3.3877\t3.1971\t3.1371\t\n",
      "Training Epoch: 10 [794/566]\tLoss: 16.0211\tLR: 0.010000\n",
      "2.7199\t2.6025\t2.5539\t2.5861\t2.6916\t2.8671\t\n",
      "Training Epoch: 10 [810/566]\tLoss: 10.6557\tLR: 0.010000\n",
      "1.4988\t1.5306\t1.5940\t1.7506\t1.9943\t2.2876\t\n",
      "Training Epoch: 10 [826/566]\tLoss: 19.6142\tLR: 0.010000\n",
      "3.7097\t3.4410\t3.2117\t3.0535\t3.0475\t3.1509\t\n",
      "Training Epoch: 10 [842/566]\tLoss: 13.1062\tLR: 0.010000\n",
      "2.3368\t2.2262\t2.1296\t2.0754\t2.1045\t2.2337\t\n",
      "Training Epoch: 10 [858/566]\tLoss: 20.9225\tLR: 0.010000\n",
      "4.3518\t3.8996\t3.5051\t3.2059\t3.0241\t2.9360\t\n",
      "Training Epoch: 10 [874/566]\tLoss: 22.6071\tLR: 0.010000\n",
      "4.4410\t4.1052\t3.7688\t3.4948\t3.3814\t3.4159\t\n",
      "Training Epoch: 10 [890/566]\tLoss: 9.2835\tLR: 0.010000\n",
      "1.3505\t1.3561\t1.4106\t1.5311\t1.7098\t1.9256\t\n",
      "Training Epoch: 10 [902/566]\tLoss: 25.3034\tLR: 0.010000\n",
      "4.5853\t4.3896\t4.1862\t4.0645\t4.0315\t4.0464\t\n",
      "[0.2783150374889374, 0.5156399607658386, 0.206045001745224, 0.017917213961482048, 0.7040531039237976, 0.12117551267147064, 0.17477138340473175, 0.01604250818490982, 0.6756955981254578, 0.1109582930803299, 0.21334610879421234, -0.004369187634438276, 0.33973851799964905, 0.2789708971977234, 0.38129058480262756, -0.0002065549633698538, 0.16230949759483337, 0.0, 0.8376905024051666, -0.002426095074042678]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3354, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3135, Accuracy: 0.4523\n",
      "Test set t = 02: Average loss: 0.2954, Accuracy: 0.4470\n",
      "Test set t = 03: Average loss: 0.2856, Accuracy: 0.4505\n",
      "Test set t = 04: Average loss: 0.2848, Accuracy: 0.4382\n",
      "Test set t = 05: Average loss: 0.2927, Accuracy: 0.4382\n",
      "\n",
      "Training Epoch: 11 [10/566]\tLoss: 20.9016\tLR: 0.010000\n",
      "3.9508\t3.6904\t3.4359\t3.2731\t3.2378\t3.3136\t\n",
      "Training Epoch: 11 [26/566]\tLoss: 20.6200\tLR: 0.010000\n",
      "3.8262\t3.5993\t3.3840\t3.2461\t3.2349\t3.3295\t\n",
      "Training Epoch: 11 [42/566]\tLoss: 26.8103\tLR: 0.010000\n",
      "5.1885\t4.7785\t4.4381\t4.2099\t4.1014\t4.0940\t\n",
      "Training Epoch: 11 [58/566]\tLoss: 14.7738\tLR: 0.010000\n",
      "2.3973\t2.3486\t2.3485\t2.4015\t2.5370\t2.7409\t\n",
      "Training Epoch: 11 [74/566]\tLoss: 18.9626\tLR: 0.010000\n",
      "3.7139\t3.4152\t3.1002\t2.9126\t2.8731\t2.9476\t\n",
      "Training Epoch: 11 [90/566]\tLoss: 13.7930\tLR: 0.010000\n",
      "2.2037\t2.1954\t2.1992\t2.2522\t2.3683\t2.5742\t\n",
      "Training Epoch: 11 [106/566]\tLoss: 14.4147\tLR: 0.010000\n",
      "2.4709\t2.3470\t2.2749\t2.2991\t2.4084\t2.6144\t\n",
      "Training Epoch: 11 [122/566]\tLoss: 16.6793\tLR: 0.010000\n",
      "3.1266\t2.9311\t2.7373\t2.6253\t2.6069\t2.6521\t\n",
      "Training Epoch: 11 [138/566]\tLoss: 23.5814\tLR: 0.010000\n",
      "4.1135\t3.9352\t3.8001\t3.7975\t3.8884\t4.0467\t\n",
      "Training Epoch: 11 [154/566]\tLoss: 9.4584\tLR: 0.010000\n",
      "1.5263\t1.5048\t1.4763\t1.5252\t1.6258\t1.8001\t\n",
      "Training Epoch: 11 [170/566]\tLoss: 41.3477\tLR: 0.010000\n",
      "8.7861\t7.9195\t7.0323\t6.3251\t5.8132\t5.4717\t\n",
      "Training Epoch: 11 [186/566]\tLoss: 23.7032\tLR: 0.010000\n",
      "4.3210\t4.1001\t3.9011\t3.7748\t3.7713\t3.8351\t\n",
      "Training Epoch: 11 [202/566]\tLoss: 15.8491\tLR: 0.010000\n",
      "2.6733\t2.5531\t2.5001\t2.5433\t2.6808\t2.8986\t\n",
      "Training Epoch: 11 [218/566]\tLoss: 20.3276\tLR: 0.010000\n",
      "3.8141\t3.5499\t3.3409\t3.2101\t3.1751\t3.2375\t\n",
      "Training Epoch: 11 [234/566]\tLoss: 12.4805\tLR: 0.010000\n",
      "2.2949\t2.1050\t1.9794\t1.9554\t2.0151\t2.1307\t\n",
      "Training Epoch: 11 [250/566]\tLoss: 15.0485\tLR: 0.010000\n",
      "2.8790\t2.6489\t2.4560\t2.3324\t2.3206\t2.4116\t\n",
      "Training Epoch: 11 [266/566]\tLoss: 14.4876\tLR: 0.010000\n",
      "2.3904\t2.3506\t2.3330\t2.3613\t2.4527\t2.5996\t\n",
      "Training Epoch: 11 [282/566]\tLoss: 20.2323\tLR: 0.010000\n",
      "3.7254\t3.5185\t3.3293\t3.2131\t3.1925\t3.2536\t\n",
      "Training Epoch: 11 [298/566]\tLoss: 16.5565\tLR: 0.010000\n",
      "2.7003\t2.6279\t2.6255\t2.6897\t2.8477\t3.0654\t\n",
      "Training Epoch: 11 [314/566]\tLoss: 20.4761\tLR: 0.010000\n",
      "3.9424\t3.6152\t3.3753\t3.2290\t3.1562\t3.1582\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11 [330/566]\tLoss: 15.8005\tLR: 0.010000\n",
      "3.1586\t2.8616\t2.5906\t2.4233\t2.3683\t2.3982\t\n",
      "Training Epoch: 11 [346/566]\tLoss: 24.3839\tLR: 0.010000\n",
      "5.0635\t4.5756\t4.1342\t3.7864\t3.5017\t3.3225\t\n",
      "Training Epoch: 11 [362/566]\tLoss: 21.6881\tLR: 0.010000\n",
      "4.5877\t4.1455\t3.6819\t3.3097\t3.0471\t2.9161\t\n",
      "Training Epoch: 11 [378/566]\tLoss: 13.8264\tLR: 0.010000\n",
      "2.5871\t2.4562\t2.3143\t2.1976\t2.1342\t2.1371\t\n",
      "Training Epoch: 11 [394/566]\tLoss: 17.4165\tLR: 0.010000\n",
      "3.2963\t3.0218\t2.7981\t2.7028\t2.7269\t2.8705\t\n",
      "Training Epoch: 11 [410/566]\tLoss: 20.7552\tLR: 0.010000\n",
      "3.7224\t3.5380\t3.4126\t3.3510\t3.3387\t3.3926\t\n",
      "Training Epoch: 11 [426/566]\tLoss: 15.5386\tLR: 0.010000\n",
      "2.5461\t2.4641\t2.4574\t2.5455\t2.6750\t2.8505\t\n",
      "Training Epoch: 11 [442/566]\tLoss: 17.0188\tLR: 0.010000\n",
      "3.1466\t2.9639\t2.8280\t2.7146\t2.6668\t2.6989\t\n",
      "Training Epoch: 11 [458/566]\tLoss: 22.9040\tLR: 0.010000\n",
      "4.3708\t4.0397\t3.7836\t3.6071\t3.5322\t3.5705\t\n",
      "Training Epoch: 11 [474/566]\tLoss: 5.7234\tLR: 0.010000\n",
      "0.7317\t0.7576\t0.8255\t0.9465\t1.1241\t1.3380\t\n",
      "Training Epoch: 11 [490/566]\tLoss: 18.6618\tLR: 0.010000\n",
      "3.4657\t3.2151\t3.0238\t2.9371\t2.9553\t3.0647\t\n",
      "Training Epoch: 11 [506/566]\tLoss: 17.6950\tLR: 0.010000\n",
      "3.4997\t3.2383\t2.9398\t2.7332\t2.6355\t2.6485\t\n",
      "Training Epoch: 11 [522/566]\tLoss: 18.3722\tLR: 0.010000\n",
      "3.3665\t3.1617\t3.0067\t2.9156\t2.9042\t3.0174\t\n",
      "Training Epoch: 11 [538/566]\tLoss: 17.7866\tLR: 0.010000\n",
      "3.4438\t3.1176\t2.8832\t2.7663\t2.7446\t2.8311\t\n",
      "Training Epoch: 11 [554/566]\tLoss: 19.8386\tLR: 0.010000\n",
      "3.5682\t3.3798\t3.2299\t3.1405\t3.1958\t3.3243\t\n",
      "Training Epoch: 11 [570/566]\tLoss: 14.9769\tLR: 0.010000\n",
      "2.3356\t2.3067\t2.3585\t2.4788\t2.6419\t2.8553\t\n",
      "Training Epoch: 11 [586/566]\tLoss: 19.2659\tLR: 0.010000\n",
      "3.4577\t3.2559\t3.1427\t3.1065\t3.1159\t3.1872\t\n",
      "Training Epoch: 11 [602/566]\tLoss: 25.4004\tLR: 0.010000\n",
      "4.8477\t4.5542\t4.2555\t4.0161\t3.8781\t3.8488\t\n",
      "Training Epoch: 11 [618/566]\tLoss: 16.9661\tLR: 0.010000\n",
      "3.5274\t3.2106\t2.8735\t2.6022\t2.4176\t2.3346\t\n",
      "Training Epoch: 11 [634/566]\tLoss: 16.0233\tLR: 0.010000\n",
      "3.0215\t2.8023\t2.6390\t2.5360\t2.4932\t2.5313\t\n",
      "Training Epoch: 11 [650/566]\tLoss: 19.0598\tLR: 0.010000\n",
      "3.3796\t3.1654\t3.0267\t3.0103\t3.1269\t3.3508\t\n",
      "Training Epoch: 11 [666/566]\tLoss: 20.8447\tLR: 0.010000\n",
      "3.9137\t3.6284\t3.3942\t3.2794\t3.2649\t3.3641\t\n",
      "Training Epoch: 11 [682/566]\tLoss: 21.4172\tLR: 0.010000\n",
      "4.1275\t3.7477\t3.4711\t3.3281\t3.3248\t3.4180\t\n",
      "Training Epoch: 11 [698/566]\tLoss: 20.3388\tLR: 0.010000\n",
      "3.5515\t3.4180\t3.3266\t3.2923\t3.3242\t3.4261\t\n",
      "Training Epoch: 11 [714/566]\tLoss: 21.9661\tLR: 0.010000\n",
      "4.3224\t3.9431\t3.6334\t3.4256\t3.3258\t3.3158\t\n",
      "Training Epoch: 11 [730/566]\tLoss: 11.9202\tLR: 0.010000\n",
      "2.0085\t1.8460\t1.7925\t1.9013\t2.0768\t2.2950\t\n",
      "Training Epoch: 11 [746/566]\tLoss: 8.3968\tLR: 0.010000\n",
      "1.4706\t1.3824\t1.3114\t1.3030\t1.3922\t1.5373\t\n",
      "Training Epoch: 11 [762/566]\tLoss: 6.9328\tLR: 0.010000\n",
      "1.1002\t1.0630\t1.0591\t1.1056\t1.2139\t1.3910\t\n",
      "Training Epoch: 11 [778/566]\tLoss: 11.0523\tLR: 0.010000\n",
      "1.9787\t1.8574\t1.7680\t1.7411\t1.7899\t1.9172\t\n",
      "Training Epoch: 11 [794/566]\tLoss: 19.3508\tLR: 0.010000\n",
      "3.7538\t3.4456\t3.1872\t3.0071\t2.9397\t3.0174\t\n",
      "Training Epoch: 11 [810/566]\tLoss: 32.9618\tLR: 0.010000\n",
      "6.8781\t6.2768\t5.6366\t5.1008\t4.6763\t4.3933\t\n",
      "Training Epoch: 11 [826/566]\tLoss: 17.7693\tLR: 0.010000\n",
      "3.4488\t3.1693\t2.9129\t2.7596\t2.7145\t2.7641\t\n",
      "Training Epoch: 11 [842/566]\tLoss: 9.2222\tLR: 0.010000\n",
      "1.3006\t1.3151\t1.3785\t1.5064\t1.7164\t2.0051\t\n",
      "Training Epoch: 11 [858/566]\tLoss: 12.5166\tLR: 0.010000\n",
      "2.2088\t2.1191\t2.0663\t2.0259\t2.0109\t2.0857\t\n",
      "Training Epoch: 11 [874/566]\tLoss: 14.8721\tLR: 0.010000\n",
      "2.8640\t2.5994\t2.3833\t2.2794\t2.2984\t2.4476\t\n",
      "Training Epoch: 11 [890/566]\tLoss: 21.5934\tLR: 0.010000\n",
      "3.7990\t3.6211\t3.5310\t3.5215\t3.5418\t3.5790\t\n",
      "Training Epoch: 11 [902/566]\tLoss: 13.2100\tLR: 0.010000\n",
      "2.0292\t2.0058\t2.0464\t2.1901\t2.3658\t2.5728\t\n",
      "[0.27203771471977234, 0.5188348293304443, 0.20912745594978333, 0.01887180097401142, 0.7183021903038025, 0.1104484423995018, 0.1712493672966957, 0.015947401523590088, 0.701113760471344, 0.10464930534362793, 0.19423693418502808, -0.005005479324609041, 0.3427285850048065, 0.28776171803474426, 0.3695096969604492, -0.0006072123069316149, 0.16036877036094666, 0.0, 0.8396312296390533, -0.0028141415677964687]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3364, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3143, Accuracy: 0.4505\n",
      "Test set t = 02: Average loss: 0.2960, Accuracy: 0.4470\n",
      "Test set t = 03: Average loss: 0.2861, Accuracy: 0.4523\n",
      "Test set t = 04: Average loss: 0.2850, Accuracy: 0.4417\n",
      "Test set t = 05: Average loss: 0.2926, Accuracy: 0.4417\n",
      "\n",
      "Training Epoch: 12 [10/566]\tLoss: 15.8816\tLR: 0.010000\n",
      "3.3643\t3.0311\t2.6866\t2.4026\t2.2291\t2.1680\t\n",
      "Training Epoch: 12 [26/566]\tLoss: 15.3401\tLR: 0.010000\n",
      "2.9931\t2.7495\t2.5166\t2.3616\t2.3245\t2.3949\t\n",
      "Training Epoch: 12 [42/566]\tLoss: 24.0902\tLR: 0.010000\n",
      "4.1770\t4.0622\t3.9905\t3.9514\t3.9205\t3.9886\t\n",
      "Training Epoch: 12 [58/566]\tLoss: 15.9622\tLR: 0.010000\n",
      "2.6871\t2.5831\t2.5606\t2.5962\t2.6762\t2.8590\t\n",
      "Training Epoch: 12 [74/566]\tLoss: 12.2277\tLR: 0.010000\n",
      "1.9640\t1.9327\t1.9275\t1.9767\t2.1123\t2.3144\t\n",
      "Training Epoch: 12 [90/566]\tLoss: 10.3916\tLR: 0.010000\n",
      "1.5448\t1.5199\t1.5813\t1.7155\t1.8969\t2.1333\t\n",
      "Training Epoch: 12 [106/566]\tLoss: 30.4090\tLR: 0.010000\n",
      "5.8589\t5.4867\t5.1210\t4.8359\t4.6194\t4.4871\t\n",
      "Training Epoch: 12 [122/566]\tLoss: 17.7488\tLR: 0.010000\n",
      "3.6734\t3.2882\t2.9350\t2.6936\t2.5754\t2.5832\t\n",
      "Training Epoch: 12 [138/566]\tLoss: 14.0402\tLR: 0.010000\n",
      "2.4543\t2.3223\t2.2331\t2.2265\t2.3160\t2.4881\t\n",
      "Training Epoch: 12 [154/566]\tLoss: 16.9549\tLR: 0.010000\n",
      "3.0969\t2.9324\t2.7778\t2.6907\t2.6904\t2.7668\t\n",
      "Training Epoch: 12 [170/566]\tLoss: 22.0739\tLR: 0.010000\n",
      "4.0736\t3.8201\t3.6044\t3.5116\t3.5043\t3.5599\t\n",
      "Training Epoch: 12 [186/566]\tLoss: 20.2890\tLR: 0.010000\n",
      "3.9767\t3.7081\t3.4009\t3.1620\t3.0274\t3.0139\t\n",
      "Training Epoch: 12 [202/566]\tLoss: 21.3549\tLR: 0.010000\n",
      "3.9868\t3.7063\t3.5137\t3.3960\t3.3513\t3.4010\t\n",
      "Training Epoch: 12 [218/566]\tLoss: 19.0566\tLR: 0.010000\n",
      "3.9459\t3.5832\t3.2267\t2.9188\t2.7103\t2.6717\t\n",
      "Training Epoch: 12 [234/566]\tLoss: 11.3467\tLR: 0.010000\n",
      "1.9794\t1.9157\t1.8518\t1.8374\t1.8507\t1.9117\t\n",
      "Training Epoch: 12 [250/566]\tLoss: 22.6362\tLR: 0.010000\n",
      "4.7130\t4.2668\t3.8026\t3.4617\t3.2559\t3.1362\t\n",
      "Training Epoch: 12 [266/566]\tLoss: 27.1949\tLR: 0.010000\n",
      "5.3359\t4.8585\t4.4938\t4.2593\t4.1492\t4.0982\t\n",
      "Training Epoch: 12 [282/566]\tLoss: 6.4467\tLR: 0.010000\n",
      "0.9166\t0.9069\t0.9367\t1.0319\t1.2051\t1.4496\t\n",
      "Training Epoch: 12 [298/566]\tLoss: 17.4745\tLR: 0.010000\n",
      "3.4079\t3.1569\t2.9025\t2.6985\t2.6290\t2.6796\t\n",
      "Training Epoch: 12 [314/566]\tLoss: 21.8258\tLR: 0.010000\n",
      "4.4709\t4.0449\t3.6381\t3.3330\t3.1823\t3.1565\t\n",
      "Training Epoch: 12 [330/566]\tLoss: 9.3276\tLR: 0.010000\n",
      "1.3250\t1.3319\t1.3820\t1.5408\t1.7493\t1.9986\t\n",
      "Training Epoch: 12 [346/566]\tLoss: 22.6497\tLR: 0.010000\n",
      "4.5872\t4.1847\t3.8021\t3.4898\t3.3043\t3.2816\t\n",
      "Training Epoch: 12 [362/566]\tLoss: 29.4809\tLR: 0.010000\n",
      "5.7934\t5.3438\t4.9323\t4.6094\t4.4281\t4.3738\t\n",
      "Training Epoch: 12 [378/566]\tLoss: 16.7201\tLR: 0.010000\n",
      "2.8487\t2.7194\t2.6709\t2.7166\t2.8152\t2.9493\t\n",
      "Training Epoch: 12 [394/566]\tLoss: 17.9056\tLR: 0.010000\n",
      "3.0979\t2.9684\t2.9146\t2.9147\t2.9583\t3.0516\t\n",
      "Training Epoch: 12 [410/566]\tLoss: 18.3030\tLR: 0.010000\n",
      "3.4085\t3.1872\t3.0176\t2.9283\t2.8776\t2.8838\t\n",
      "Training Epoch: 12 [426/566]\tLoss: 12.2648\tLR: 0.010000\n",
      "2.3917\t2.1975\t2.0020\t1.8795\t1.8563\t1.9378\t\n",
      "Training Epoch: 12 [442/566]\tLoss: 24.4340\tLR: 0.010000\n",
      "4.9031\t4.5227\t4.0868\t3.7712\t3.5942\t3.5561\t\n",
      "Training Epoch: 12 [458/566]\tLoss: 19.2230\tLR: 0.010000\n",
      "3.4984\t3.2883\t3.1291\t3.0582\t3.0787\t3.1703\t\n",
      "Training Epoch: 12 [474/566]\tLoss: 8.0454\tLR: 0.010000\n",
      "1.1755\t1.1655\t1.2075\t1.3138\t1.4756\t1.7076\t\n",
      "Training Epoch: 12 [490/566]\tLoss: 30.5429\tLR: 0.010000\n",
      "6.0861\t5.5712\t5.1389\t4.8182\t4.5618\t4.3667\t\n",
      "Training Epoch: 12 [506/566]\tLoss: 19.4514\tLR: 0.010000\n",
      "3.4493\t3.2239\t3.1128\t3.1204\t3.1938\t3.3513\t\n",
      "Training Epoch: 12 [522/566]\tLoss: 13.2685\tLR: 0.010000\n",
      "2.2287\t2.1370\t2.1026\t2.1379\t2.2438\t2.4185\t\n",
      "Training Epoch: 12 [538/566]\tLoss: 9.2675\tLR: 0.010000\n",
      "1.4735\t1.4331\t1.4284\t1.4911\t1.6197\t1.8217\t\n",
      "Training Epoch: 12 [554/566]\tLoss: 18.8688\tLR: 0.010000\n",
      "3.4226\t3.2057\t3.0252\t2.9724\t3.0519\t3.1909\t\n",
      "Training Epoch: 12 [570/566]\tLoss: 6.9055\tLR: 0.010000\n",
      "1.0088\t1.0223\t1.0244\t1.0884\t1.2543\t1.5073\t\n",
      "Training Epoch: 12 [586/566]\tLoss: 24.2223\tLR: 0.010000\n",
      "4.8238\t4.4418\t4.0796\t3.8005\t3.5921\t3.4846\t\n",
      "Training Epoch: 12 [602/566]\tLoss: 14.0003\tLR: 0.010000\n",
      "2.0925\t2.0875\t2.1601\t2.3055\t2.5420\t2.8127\t\n",
      "Training Epoch: 12 [618/566]\tLoss: 10.2489\tLR: 0.010000\n",
      "1.5304\t1.5033\t1.5512\t1.6691\t1.8771\t2.1179\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12 [634/566]\tLoss: 21.9546\tLR: 0.010000\n",
      "3.9943\t3.7632\t3.5792\t3.4992\t3.5051\t3.6137\t\n",
      "Training Epoch: 12 [650/566]\tLoss: 18.4837\tLR: 0.010000\n",
      "3.3047\t3.1045\t2.9805\t2.9802\t3.0231\t3.0906\t\n",
      "Training Epoch: 12 [666/566]\tLoss: 17.0977\tLR: 0.010000\n",
      "2.9993\t2.8314\t2.7451\t2.7405\t2.8075\t2.9740\t\n",
      "Training Epoch: 12 [682/566]\tLoss: 19.7442\tLR: 0.010000\n",
      "3.9754\t3.6174\t3.2824\t3.0442\t2.9211\t2.9037\t\n",
      "Training Epoch: 12 [698/566]\tLoss: 21.9394\tLR: 0.010000\n",
      "3.8774\t3.6791\t3.5647\t3.5150\t3.5701\t3.7332\t\n",
      "Training Epoch: 12 [714/566]\tLoss: 22.3010\tLR: 0.010000\n",
      "4.6396\t4.1498\t3.7161\t3.4004\t3.2152\t3.1798\t\n",
      "Training Epoch: 12 [730/566]\tLoss: 20.6215\tLR: 0.010000\n",
      "3.9229\t3.6437\t3.4094\t3.2580\t3.1927\t3.1947\t\n",
      "Training Epoch: 12 [746/566]\tLoss: 25.8581\tLR: 0.010000\n",
      "5.0884\t4.7064\t4.3457\t4.0661\t3.8767\t3.7748\t\n",
      "Training Epoch: 12 [762/566]\tLoss: 8.4639\tLR: 0.010000\n",
      "1.1575\t1.1822\t1.2575\t1.3928\t1.6042\t1.8696\t\n",
      "Training Epoch: 12 [778/566]\tLoss: 22.1446\tLR: 0.010000\n",
      "4.3041\t3.9871\t3.7167\t3.5007\t3.3502\t3.2858\t\n",
      "Training Epoch: 12 [794/566]\tLoss: 17.4010\tLR: 0.010000\n",
      "3.0186\t2.8995\t2.8127\t2.7911\t2.8636\t3.0154\t\n",
      "Training Epoch: 12 [810/566]\tLoss: 17.6725\tLR: 0.010000\n",
      "3.3276\t3.1062\t2.9241\t2.8082\t2.7611\t2.7454\t\n",
      "Training Epoch: 12 [826/566]\tLoss: 17.3643\tLR: 0.010000\n",
      "3.0023\t2.8714\t2.7879\t2.7988\t2.8834\t3.0205\t\n",
      "Training Epoch: 12 [842/566]\tLoss: 21.6069\tLR: 0.010000\n",
      "4.3527\t3.9807\t3.6036\t3.3437\t3.1964\t3.1298\t\n",
      "Training Epoch: 12 [858/566]\tLoss: 16.9744\tLR: 0.010000\n",
      "3.1019\t2.9268\t2.7864\t2.6973\t2.6851\t2.7769\t\n",
      "Training Epoch: 12 [874/566]\tLoss: 15.4754\tLR: 0.010000\n",
      "2.6287\t2.5398\t2.4771\t2.5057\t2.5933\t2.7308\t\n",
      "Training Epoch: 12 [890/566]\tLoss: 19.6318\tLR: 0.010000\n",
      "3.7777\t3.5081\t3.2593\t3.0787\t2.9972\t3.0107\t\n",
      "Training Epoch: 12 [902/566]\tLoss: 9.5077\tLR: 0.010000\n",
      "1.4571\t1.4120\t1.4359\t1.5442\t1.7205\t1.9381\t\n",
      "[0.2738114595413208, 0.5144419074058533, 0.21174663305282593, 0.019127486273646355, 0.7332299947738647, 0.10101839154958725, 0.165751613676548, 0.015885628759860992, 0.7205102443695068, 0.09823932498693466, 0.1812504306435585, -0.005731409415602684, 0.3436219394207001, 0.29390114545822144, 0.3624769151210785, -0.00115252286195755, 0.1559258997440338, 0.0, 0.8440741002559662, -0.003633987857028842]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3361, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3144, Accuracy: 0.4505\n",
      "Test set t = 02: Average loss: 0.2965, Accuracy: 0.4470\n",
      "Test set t = 03: Average loss: 0.2865, Accuracy: 0.4523\n",
      "Test set t = 04: Average loss: 0.2848, Accuracy: 0.4452\n",
      "Test set t = 05: Average loss: 0.2913, Accuracy: 0.4452\n",
      "\n",
      "Training Epoch: 13 [10/566]\tLoss: 13.3816\tLR: 0.010000\n",
      "2.2085\t2.1650\t2.1471\t2.1891\t2.2757\t2.3961\t\n",
      "Training Epoch: 13 [26/566]\tLoss: 19.1368\tLR: 0.010000\n",
      "3.4296\t3.2829\t3.1425\t3.0640\t3.0676\t3.1503\t\n",
      "Training Epoch: 13 [42/566]\tLoss: 14.0119\tLR: 0.010000\n",
      "2.2085\t2.1692\t2.1821\t2.2913\t2.4598\t2.7009\t\n",
      "Training Epoch: 13 [58/566]\tLoss: 27.4206\tLR: 0.010000\n",
      "5.0544\t4.7508\t4.5176\t4.3961\t4.3483\t4.3534\t\n",
      "Training Epoch: 13 [74/566]\tLoss: 15.8327\tLR: 0.010000\n",
      "2.9327\t2.7428\t2.5896\t2.5077\t2.4948\t2.5651\t\n",
      "Training Epoch: 13 [90/566]\tLoss: 25.0527\tLR: 0.010000\n",
      "5.3197\t4.7857\t4.2438\t3.8068\t3.5077\t3.3890\t\n",
      "Training Epoch: 13 [106/566]\tLoss: 25.6640\tLR: 0.010000\n",
      "5.2302\t4.7433\t4.2831\t3.9572\t3.7666\t3.6837\t\n",
      "Training Epoch: 13 [122/566]\tLoss: 24.6419\tLR: 0.010000\n",
      "4.6431\t4.3329\t4.0790\t3.9029\t3.8288\t3.8552\t\n",
      "Training Epoch: 13 [138/566]\tLoss: 22.1659\tLR: 0.010000\n",
      "4.3467\t4.0142\t3.7083\t3.4746\t3.3414\t3.2806\t\n",
      "Training Epoch: 13 [154/566]\tLoss: 25.8073\tLR: 0.010000\n",
      "4.9250\t4.5757\t4.2747\t4.0666\t3.9744\t3.9909\t\n",
      "Training Epoch: 13 [170/566]\tLoss: 9.6017\tLR: 0.010000\n",
      "1.4789\t1.4312\t1.4568\t1.5576\t1.7249\t1.9523\t\n",
      "Training Epoch: 13 [186/566]\tLoss: 15.9689\tLR: 0.010000\n",
      "2.6102\t2.5482\t2.5320\t2.6099\t2.7420\t2.9266\t\n",
      "Training Epoch: 13 [202/566]\tLoss: 12.2855\tLR: 0.010000\n",
      "2.1125\t1.9875\t1.8981\t1.9230\t2.0689\t2.2955\t\n",
      "Training Epoch: 13 [218/566]\tLoss: 8.9991\tLR: 0.010000\n",
      "1.4230\t1.3844\t1.4097\t1.4714\t1.5767\t1.7338\t\n",
      "Training Epoch: 13 [234/566]\tLoss: 23.9905\tLR: 0.010000\n",
      "5.0206\t4.5442\t4.0574\t3.6743\t3.4040\t3.2900\t\n",
      "Training Epoch: 13 [250/566]\tLoss: 12.4921\tLR: 0.010000\n",
      "2.5713\t2.3044\t2.0556\t1.8854\t1.8181\t1.8574\t\n",
      "Training Epoch: 13 [266/566]\tLoss: 12.2532\tLR: 0.010000\n",
      "2.3879\t2.2236\t2.0365\t1.8989\t1.8548\t1.8516\t\n",
      "Training Epoch: 13 [282/566]\tLoss: 9.8042\tLR: 0.010000\n",
      "1.3717\t1.4032\t1.5013\t1.6557\t1.8244\t2.0479\t\n",
      "Training Epoch: 13 [298/566]\tLoss: 28.0663\tLR: 0.010000\n",
      "5.8735\t5.2852\t4.7506\t4.3209\t4.0199\t3.8162\t\n",
      "Training Epoch: 13 [314/566]\tLoss: 17.8916\tLR: 0.010000\n",
      "3.0524\t2.9680\t2.9159\t2.9346\t2.9778\t3.0430\t\n",
      "Training Epoch: 13 [330/566]\tLoss: 17.1966\tLR: 0.010000\n",
      "3.1756\t2.9835\t2.7948\t2.7096\t2.7127\t2.8203\t\n",
      "Training Epoch: 13 [346/566]\tLoss: 6.9609\tLR: 0.010000\n",
      "1.1688\t1.1205\t1.0895\t1.0994\t1.1738\t1.3089\t\n",
      "Training Epoch: 13 [362/566]\tLoss: 17.4307\tLR: 0.010000\n",
      "3.0173\t2.9005\t2.8311\t2.8206\t2.8680\t2.9931\t\n",
      "Training Epoch: 13 [378/566]\tLoss: 15.8997\tLR: 0.010000\n",
      "2.8033\t2.6314\t2.5499\t2.5483\t2.6032\t2.7637\t\n",
      "Training Epoch: 13 [394/566]\tLoss: 11.9753\tLR: 0.010000\n",
      "1.8669\t1.8199\t1.8507\t1.9660\t2.1307\t2.3410\t\n",
      "Training Epoch: 13 [410/566]\tLoss: 22.4514\tLR: 0.010000\n",
      "4.4145\t4.0686\t3.7195\t3.4872\t3.3881\t3.3736\t\n",
      "Training Epoch: 13 [426/566]\tLoss: 8.6401\tLR: 0.010000\n",
      "1.6943\t1.5550\t1.4124\t1.3207\t1.2901\t1.3676\t\n",
      "Training Epoch: 13 [442/566]\tLoss: 25.2617\tLR: 0.010000\n",
      "4.9847\t4.5258\t4.1971\t3.9538\t3.8226\t3.7776\t\n",
      "Training Epoch: 13 [458/566]\tLoss: 15.7870\tLR: 0.010000\n",
      "2.6521\t2.5531\t2.5091\t2.5313\t2.6642\t2.8772\t\n",
      "Training Epoch: 13 [474/566]\tLoss: 12.2388\tLR: 0.010000\n",
      "2.3694\t2.1957\t2.0235\t1.8888\t1.8585\t1.9030\t\n",
      "Training Epoch: 13 [490/566]\tLoss: 18.1596\tLR: 0.010000\n",
      "3.3688\t3.1470\t2.9613\t2.8604\t2.8673\t2.9548\t\n",
      "Training Epoch: 13 [506/566]\tLoss: 22.1975\tLR: 0.010000\n",
      "4.1577\t3.8481\t3.6047\t3.4854\t3.5045\t3.5971\t\n",
      "Training Epoch: 13 [522/566]\tLoss: 19.1103\tLR: 0.010000\n",
      "3.2603\t3.1399\t3.0932\t3.1086\t3.1828\t3.3255\t\n",
      "Training Epoch: 13 [538/566]\tLoss: 11.9845\tLR: 0.010000\n",
      "1.8584\t1.8309\t1.8832\t1.9694\t2.1267\t2.3159\t\n",
      "Training Epoch: 13 [554/566]\tLoss: 19.1260\tLR: 0.010000\n",
      "3.4547\t3.2856\t3.1368\t3.0483\t3.0464\t3.1543\t\n",
      "Training Epoch: 13 [570/566]\tLoss: 26.6678\tLR: 0.010000\n",
      "4.9940\t4.7153\t4.4461\t4.2530\t4.1549\t4.1045\t\n",
      "Training Epoch: 13 [586/566]\tLoss: 11.7934\tLR: 0.010000\n",
      "1.9421\t1.8699\t1.8590\t1.9167\t2.0309\t2.1747\t\n",
      "Training Epoch: 13 [602/566]\tLoss: 38.0017\tLR: 0.010000\n",
      "7.5075\t6.9206\t6.4199\t6.0338\t5.6912\t5.4286\t\n",
      "Training Epoch: 13 [618/566]\tLoss: 14.8339\tLR: 0.010000\n",
      "2.6189\t2.5061\t2.4263\t2.3843\t2.4020\t2.4963\t\n",
      "Training Epoch: 13 [634/566]\tLoss: 25.9347\tLR: 0.010000\n",
      "4.5756\t4.3573\t4.2311\t4.1995\t4.2329\t4.3382\t\n",
      "Training Epoch: 13 [650/566]\tLoss: 26.1139\tLR: 0.010000\n",
      "5.2899\t4.8180\t4.3928\t4.0554\t3.8309\t3.7269\t\n",
      "Training Epoch: 13 [666/566]\tLoss: 12.0506\tLR: 0.010000\n",
      "2.0295\t1.9154\t1.8561\t1.8955\t2.0548\t2.2993\t\n",
      "Training Epoch: 13 [682/566]\tLoss: 35.2324\tLR: 0.010000\n",
      "7.1935\t6.6153\t6.0210\t5.5066\t5.0923\t4.8037\t\n",
      "Training Epoch: 13 [698/566]\tLoss: 17.8170\tLR: 0.010000\n",
      "2.8362\t2.8375\t2.8362\t2.9348\t3.0971\t3.2752\t\n",
      "Training Epoch: 13 [714/566]\tLoss: 12.1438\tLR: 0.010000\n",
      "2.3945\t2.1830\t1.9979\t1.8846\t1.8335\t1.8504\t\n",
      "Training Epoch: 13 [730/566]\tLoss: 17.1433\tLR: 0.010000\n",
      "3.1624\t2.9850\t2.8120\t2.6959\t2.6893\t2.7987\t\n",
      "Training Epoch: 13 [746/566]\tLoss: 7.5364\tLR: 0.010000\n",
      "1.0538\t1.0508\t1.1140\t1.2383\t1.4138\t1.6656\t\n",
      "Training Epoch: 13 [762/566]\tLoss: 15.6189\tLR: 0.010000\n",
      "3.0746\t2.8265\t2.5883\t2.4124\t2.3392\t2.3780\t\n",
      "Training Epoch: 13 [778/566]\tLoss: 19.9835\tLR: 0.010000\n",
      "4.0965\t3.7110\t3.3499\t3.0683\t2.9009\t2.8569\t\n",
      "Training Epoch: 13 [794/566]\tLoss: 14.5854\tLR: 0.010000\n",
      "2.5386\t2.4202\t2.3512\t2.3413\t2.4003\t2.5338\t\n",
      "Training Epoch: 13 [810/566]\tLoss: 13.5989\tLR: 0.010000\n",
      "2.6677\t2.4279\t2.2263\t2.0949\t2.0523\t2.1298\t\n",
      "Training Epoch: 13 [826/566]\tLoss: 17.0579\tLR: 0.010000\n",
      "3.2014\t3.0053\t2.8055\t2.6878\t2.6534\t2.7044\t\n",
      "Training Epoch: 13 [842/566]\tLoss: 17.0747\tLR: 0.010000\n",
      "3.2143\t3.0287\t2.8150\t2.6829\t2.6407\t2.6931\t\n",
      "Training Epoch: 13 [858/566]\tLoss: 23.2638\tLR: 0.010000\n",
      "4.4979\t4.1536\t3.8635\t3.6728\t3.5552\t3.5209\t\n",
      "Training Epoch: 13 [874/566]\tLoss: 15.4092\tLR: 0.010000\n",
      "2.8725\t2.6852\t2.5406\t2.4730\t2.4221\t2.4157\t\n",
      "Training Epoch: 13 [890/566]\tLoss: 20.7224\tLR: 0.010000\n",
      "3.9059\t3.6245\t3.3945\t3.2633\t3.2356\t3.2986\t\n",
      "Training Epoch: 13 [902/566]\tLoss: 10.7260\tLR: 0.010000\n",
      "1.6640\t1.6701\t1.7141\t1.7883\t1.8812\t2.0083\t\n",
      "[0.2650798261165619, 0.5205859541893005, 0.21433421969413757, 0.01983039081096649, 0.7394762635231018, 0.09384428709745407, 0.16667944937944412, 0.015644045546650887, 0.7401581406593323, 0.09246835857629776, 0.16737350076436996, -0.006130646914243698, 0.3476741313934326, 0.30063796043395996, 0.3516879081726074, -0.0012837373651564121, 0.15480594336986542, 0.0, 0.8451940566301346, -0.004020004067569971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.3352, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3133, Accuracy: 0.4505\n",
      "Test set t = 02: Average loss: 0.2954, Accuracy: 0.4488\n",
      "Test set t = 03: Average loss: 0.2855, Accuracy: 0.4505\n",
      "Test set t = 04: Average loss: 0.2839, Accuracy: 0.4452\n",
      "Test set t = 05: Average loss: 0.2906, Accuracy: 0.4452\n",
      "\n",
      "Training Epoch: 14 [10/566]\tLoss: 10.4356\tLR: 0.010000\n",
      "1.7481\t1.7144\t1.6650\t1.6570\t1.7371\t1.9140\t\n",
      "Training Epoch: 14 [26/566]\tLoss: 15.1325\tLR: 0.010000\n",
      "2.4274\t2.3806\t2.3947\t2.4757\t2.6162\t2.8379\t\n",
      "Training Epoch: 14 [42/566]\tLoss: 26.8183\tLR: 0.010000\n",
      "5.0985\t4.7733\t4.4884\t4.2742\t4.1270\t4.0570\t\n",
      "Training Epoch: 14 [58/566]\tLoss: 25.5911\tLR: 0.010000\n",
      "5.0961\t4.7123\t4.3239\t4.0114\t3.7768\t3.6706\t\n",
      "Training Epoch: 14 [74/566]\tLoss: 17.6981\tLR: 0.010000\n",
      "3.2189\t3.0100\t2.8745\t2.8194\t2.8419\t2.9334\t\n",
      "Training Epoch: 14 [90/566]\tLoss: 13.2899\tLR: 0.010000\n",
      "2.1205\t2.0576\t2.0660\t2.1688\t2.3326\t2.5443\t\n",
      "Training Epoch: 14 [106/566]\tLoss: 10.2897\tLR: 0.010000\n",
      "1.9164\t1.7743\t1.6512\t1.5888\t1.6165\t1.7426\t\n",
      "Training Epoch: 14 [122/566]\tLoss: 14.9428\tLR: 0.010000\n",
      "2.6842\t2.5915\t2.4527\t2.3670\t2.3825\t2.4649\t\n",
      "Training Epoch: 14 [138/566]\tLoss: 20.4899\tLR: 0.010000\n",
      "3.9234\t3.6721\t3.4288\t3.2402\t3.1191\t3.1063\t\n",
      "Training Epoch: 14 [154/566]\tLoss: 15.4168\tLR: 0.010000\n",
      "2.9064\t2.6751\t2.5036\t2.4232\t2.4156\t2.4928\t\n",
      "Training Epoch: 14 [170/566]\tLoss: 19.9823\tLR: 0.010000\n",
      "3.5713\t3.4050\t3.2737\t3.2033\t3.2126\t3.3165\t\n",
      "Training Epoch: 14 [186/566]\tLoss: 13.7258\tLR: 0.010000\n",
      "2.2596\t2.1923\t2.1637\t2.2167\t2.3450\t2.5485\t\n",
      "Training Epoch: 14 [202/566]\tLoss: 23.9949\tLR: 0.010000\n",
      "4.7845\t4.3962\t4.0437\t3.7599\t3.5538\t3.4568\t\n",
      "Training Epoch: 14 [218/566]\tLoss: 11.5760\tLR: 0.010000\n",
      "2.0698\t1.9459\t1.8588\t1.8330\t1.8711\t1.9975\t\n",
      "Training Epoch: 14 [234/566]\tLoss: 20.3394\tLR: 0.010000\n",
      "4.2951\t3.8854\t3.4724\t3.1241\t2.8720\t2.6903\t\n",
      "Training Epoch: 14 [250/566]\tLoss: 10.2747\tLR: 0.010000\n",
      "1.4401\t1.4483\t1.5527\t1.7282\t1.9297\t2.1758\t\n",
      "Training Epoch: 14 [266/566]\tLoss: 22.3392\tLR: 0.010000\n",
      "4.4956\t4.1397\t3.7924\t3.4923\t3.2654\t3.1538\t\n",
      "Training Epoch: 14 [282/566]\tLoss: 25.5253\tLR: 0.010000\n",
      "4.8399\t4.5673\t4.2869\t4.0742\t3.9102\t3.8469\t\n",
      "Training Epoch: 14 [298/566]\tLoss: 19.1049\tLR: 0.010000\n",
      "3.8674\t3.4836\t3.1409\t2.9277\t2.8330\t2.8523\t\n",
      "Training Epoch: 14 [314/566]\tLoss: 17.2923\tLR: 0.010000\n",
      "3.2566\t3.0452\t2.8349\t2.7209\t2.6934\t2.7413\t\n",
      "Training Epoch: 14 [330/566]\tLoss: 20.9384\tLR: 0.010000\n",
      "4.0122\t3.7088\t3.4511\t3.2771\t3.2180\t3.2712\t\n",
      "Training Epoch: 14 [346/566]\tLoss: 12.7406\tLR: 0.010000\n",
      "2.1698\t2.0506\t2.0031\t2.0453\t2.1519\t2.3199\t\n",
      "Training Epoch: 14 [362/566]\tLoss: 15.3778\tLR: 0.010000\n",
      "2.4659\t2.4166\t2.4378\t2.5355\t2.6763\t2.8457\t\n",
      "Training Epoch: 14 [378/566]\tLoss: 13.4186\tLR: 0.010000\n",
      "2.5766\t2.3593\t2.1825\t2.0842\t2.0680\t2.1481\t\n",
      "Training Epoch: 14 [394/566]\tLoss: 15.6984\tLR: 0.010000\n",
      "2.7977\t2.6619\t2.5591\t2.5125\t2.5415\t2.6257\t\n",
      "Training Epoch: 14 [410/566]\tLoss: 14.1009\tLR: 0.010000\n",
      "2.7295\t2.4597\t2.2696\t2.1661\t2.1681\t2.3079\t\n",
      "Training Epoch: 14 [426/566]\tLoss: 20.2674\tLR: 0.010000\n",
      "3.6619\t3.4643\t3.2949\t3.2216\t3.2485\t3.3761\t\n",
      "Training Epoch: 14 [442/566]\tLoss: 14.0989\tLR: 0.010000\n",
      "2.1480\t2.1150\t2.1671\t2.3179\t2.5404\t2.8105\t\n",
      "Training Epoch: 14 [458/566]\tLoss: 28.4416\tLR: 0.010000\n",
      "5.7050\t5.2746\t4.8317\t4.4417\t4.1789\t4.0097\t\n",
      "Training Epoch: 14 [474/566]\tLoss: 19.2178\tLR: 0.010000\n",
      "3.5142\t3.3148\t3.1633\t3.0719\t3.0490\t3.1045\t\n",
      "Training Epoch: 14 [490/566]\tLoss: 29.1260\tLR: 0.010000\n",
      "6.2153\t5.5625\t4.9390\t4.4431\t4.0944\t3.8717\t\n",
      "Training Epoch: 14 [506/566]\tLoss: 21.3427\tLR: 0.010000\n",
      "3.9753\t3.7235\t3.5050\t3.3663\t3.3395\t3.4330\t\n",
      "Training Epoch: 14 [522/566]\tLoss: 25.2959\tLR: 0.010000\n",
      "4.8311\t4.4730\t4.2207\t4.0303\t3.8915\t3.8492\t\n",
      "Training Epoch: 14 [538/566]\tLoss: 23.5080\tLR: 0.010000\n",
      "4.6619\t4.2565\t3.8939\t3.6614\t3.5346\t3.4998\t\n",
      "Training Epoch: 14 [554/566]\tLoss: 13.9280\tLR: 0.010000\n",
      "2.2743\t2.1684\t2.1599\t2.2475\t2.4299\t2.6479\t\n",
      "Training Epoch: 14 [570/566]\tLoss: 23.3183\tLR: 0.010000\n",
      "4.2599\t4.0099\t3.8363\t3.7377\t3.7041\t3.7704\t\n",
      "Training Epoch: 14 [586/566]\tLoss: 7.6032\tLR: 0.010000\n",
      "1.1060\t1.0750\t1.0955\t1.2098\t1.4213\t1.6956\t\n",
      "Training Epoch: 14 [602/566]\tLoss: 14.3017\tLR: 0.010000\n",
      "2.5869\t2.4054\t2.2660\t2.2078\t2.3153\t2.5204\t\n",
      "Training Epoch: 14 [618/566]\tLoss: 37.2965\tLR: 0.010000\n",
      "7.8716\t7.1163\t6.3527\t5.7375\t5.2694\t4.9490\t\n",
      "Training Epoch: 14 [634/566]\tLoss: 14.1472\tLR: 0.010000\n",
      "2.3479\t2.2800\t2.2517\t2.3008\t2.4074\t2.5595\t\n",
      "Training Epoch: 14 [650/566]\tLoss: 5.4475\tLR: 0.010000\n",
      "0.7089\t0.7244\t0.7764\t0.8798\t1.0592\t1.2988\t\n",
      "Training Epoch: 14 [666/566]\tLoss: 21.8389\tLR: 0.010000\n",
      "4.3134\t3.9537\t3.6349\t3.4136\t3.2807\t3.2425\t\n",
      "Training Epoch: 14 [682/566]\tLoss: 22.6304\tLR: 0.010000\n",
      "4.5877\t4.1771\t3.7861\t3.4766\t3.3254\t3.2775\t\n",
      "Training Epoch: 14 [698/566]\tLoss: 12.4624\tLR: 0.010000\n",
      "2.0847\t2.0010\t1.9774\t2.0400\t2.1238\t2.2355\t\n",
      "Training Epoch: 14 [714/566]\tLoss: 11.6561\tLR: 0.010000\n",
      "1.8970\t1.8130\t1.7679\t1.8406\t2.0440\t2.2936\t\n",
      "Training Epoch: 14 [730/566]\tLoss: 17.2053\tLR: 0.010000\n",
      "3.1592\t2.9599\t2.8172\t2.7250\t2.7212\t2.8228\t\n",
      "Training Epoch: 14 [746/566]\tLoss: 19.1620\tLR: 0.010000\n",
      "3.7155\t3.4168\t3.1698\t3.0052\t2.9221\t2.9326\t\n",
      "Training Epoch: 14 [762/566]\tLoss: 20.0095\tLR: 0.010000\n",
      "3.5727\t3.3509\t3.2274\t3.2080\t3.2552\t3.3953\t\n",
      "Training Epoch: 14 [778/566]\tLoss: 20.3410\tLR: 0.010000\n",
      "3.6021\t3.4283\t3.3251\t3.2966\t3.3139\t3.3749\t\n",
      "Training Epoch: 14 [794/566]\tLoss: 11.0199\tLR: 0.010000\n",
      "1.8144\t1.7561\t1.7320\t1.7723\t1.8884\t2.0566\t\n",
      "Training Epoch: 14 [810/566]\tLoss: 22.5718\tLR: 0.010000\n",
      "4.5285\t4.1176\t3.7339\t3.4905\t3.3620\t3.3392\t\n",
      "Training Epoch: 14 [826/566]\tLoss: 13.8025\tLR: 0.010000\n",
      "2.0350\t2.0707\t2.1629\t2.3239\t2.5052\t2.7048\t\n",
      "Training Epoch: 14 [842/566]\tLoss: 18.6337\tLR: 0.010000\n",
      "3.6396\t3.3048\t3.0556\t2.8952\t2.8452\t2.8934\t\n",
      "Training Epoch: 14 [858/566]\tLoss: 13.2935\tLR: 0.010000\n",
      "2.2478\t2.1155\t2.0602\t2.1221\t2.2724\t2.4754\t\n",
      "Training Epoch: 14 [874/566]\tLoss: 22.7478\tLR: 0.010000\n",
      "4.6752\t4.2737\t3.8385\t3.5046\t3.2773\t3.1785\t\n",
      "Training Epoch: 14 [890/566]\tLoss: 10.7504\tLR: 0.010000\n",
      "1.6666\t1.6513\t1.6806\t1.7564\t1.9009\t2.0945\t\n",
      "Training Epoch: 14 [902/566]\tLoss: 26.0033\tLR: 0.010000\n",
      "4.8898\t4.5184\t4.2475\t4.1104\t4.0866\t4.1506\t\n",
      "[0.2545490860939026, 0.5217105746269226, 0.2237403392791748, 0.02035006880760193, 0.7443529963493347, 0.08597220480442047, 0.1696747988462448, 0.015530120581388474, 0.7587451338768005, 0.08655065298080444, 0.15470421314239502, -0.006801432929933071, 0.35283681750297546, 0.3090471625328064, 0.33811601996421814, -0.0017572569195181131, 0.1554878056049347, 0.0, 0.8445121943950653, -0.004850117024034262]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3354, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3130, Accuracy: 0.4505\n",
      "Test set t = 02: Average loss: 0.2950, Accuracy: 0.4488\n",
      "Test set t = 03: Average loss: 0.2853, Accuracy: 0.4541\n",
      "Test set t = 04: Average loss: 0.2842, Accuracy: 0.4488\n",
      "Test set t = 05: Average loss: 0.2915, Accuracy: 0.4452\n",
      "\n",
      "Training Epoch: 15 [10/566]\tLoss: 18.2125\tLR: 0.010000\n",
      "3.2978\t3.0852\t2.9288\t2.8741\t2.9336\t3.0931\t\n",
      "Training Epoch: 15 [26/566]\tLoss: 23.8977\tLR: 0.010000\n",
      "4.6260\t4.2969\t4.0085\t3.7847\t3.6329\t3.5487\t\n",
      "Training Epoch: 15 [42/566]\tLoss: 25.9446\tLR: 0.010000\n",
      "5.1033\t4.6999\t4.3562\t4.0965\t3.8995\t3.7892\t\n",
      "Training Epoch: 15 [58/566]\tLoss: 17.2451\tLR: 0.010000\n",
      "3.3002\t3.0449\t2.8097\t2.6666\t2.6607\t2.7630\t\n",
      "Training Epoch: 15 [74/566]\tLoss: 13.6473\tLR: 0.010000\n",
      "2.4835\t2.3072\t2.1753\t2.1459\t2.2046\t2.3308\t\n",
      "Training Epoch: 15 [90/566]\tLoss: 14.0643\tLR: 0.010000\n",
      "2.4015\t2.2755\t2.2075\t2.2312\t2.3628\t2.5858\t\n",
      "Training Epoch: 15 [106/566]\tLoss: 15.0744\tLR: 0.010000\n",
      "2.9418\t2.7040\t2.4953\t2.3402\t2.2869\t2.3063\t\n",
      "Training Epoch: 15 [122/566]\tLoss: 17.8498\tLR: 0.010000\n",
      "3.2908\t3.0575\t2.8946\t2.8374\t2.8412\t2.9283\t\n",
      "Training Epoch: 15 [138/566]\tLoss: 23.3832\tLR: 0.010000\n",
      "4.7167\t4.2587\t3.8780\t3.6122\t3.4693\t3.4482\t\n",
      "Training Epoch: 15 [154/566]\tLoss: 21.0431\tLR: 0.010000\n",
      "3.8809\t3.6468\t3.4570\t3.3431\t3.3334\t3.3818\t\n",
      "Training Epoch: 15 [170/566]\tLoss: 27.5349\tLR: 0.010000\n",
      "5.6950\t5.1721\t4.6481\t4.2527\t3.9610\t3.8059\t\n",
      "Training Epoch: 15 [186/566]\tLoss: 28.2682\tLR: 0.010000\n",
      "5.7401\t5.2104\t4.7279\t4.3886\t4.1558\t4.0452\t\n",
      "Training Epoch: 15 [202/566]\tLoss: 13.6156\tLR: 0.010000\n",
      "2.3335\t2.2199\t2.1769\t2.1996\t2.2741\t2.4117\t\n",
      "Training Epoch: 15 [218/566]\tLoss: 14.1374\tLR: 0.010000\n",
      "2.8976\t2.6062\t2.3347\t2.1490\t2.0679\t2.0820\t\n",
      "Training Epoch: 15 [234/566]\tLoss: 22.3495\tLR: 0.010000\n",
      "4.6099\t4.1849\t3.7731\t3.4454\t3.2174\t3.1188\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15 [250/566]\tLoss: 21.8957\tLR: 0.010000\n",
      "3.9623\t3.7570\t3.5915\t3.5129\t3.5123\t3.5597\t\n",
      "Training Epoch: 15 [266/566]\tLoss: 8.3357\tLR: 0.010000\n",
      "1.2739\t1.2301\t1.2134\t1.3119\t1.5108\t1.7955\t\n",
      "Training Epoch: 15 [282/566]\tLoss: 5.2270\tLR: 0.010000\n",
      "0.7782\t0.7499\t0.7611\t0.8292\t0.9659\t1.1427\t\n",
      "Training Epoch: 15 [298/566]\tLoss: 15.2902\tLR: 0.010000\n",
      "2.6431\t2.5237\t2.4423\t2.4534\t2.5451\t2.6828\t\n",
      "Training Epoch: 15 [314/566]\tLoss: 22.1617\tLR: 0.010000\n",
      "3.8154\t3.6585\t3.5714\t3.5973\t3.6994\t3.8197\t\n",
      "Training Epoch: 15 [330/566]\tLoss: 12.3860\tLR: 0.010000\n",
      "1.9355\t1.9228\t1.9706\t2.0404\t2.1619\t2.3548\t\n",
      "Training Epoch: 15 [346/566]\tLoss: 9.8507\tLR: 0.010000\n",
      "1.5427\t1.5214\t1.5457\t1.6098\t1.7219\t1.9092\t\n",
      "Training Epoch: 15 [362/566]\tLoss: 18.8717\tLR: 0.010000\n",
      "3.6039\t3.3007\t3.0747\t2.9604\t2.9402\t2.9918\t\n",
      "Training Epoch: 15 [378/566]\tLoss: 19.6500\tLR: 0.010000\n",
      "3.6669\t3.4070\t3.2021\t3.0881\t3.0875\t3.1985\t\n",
      "Training Epoch: 15 [394/566]\tLoss: 12.1170\tLR: 0.010000\n",
      "2.0391\t1.9449\t1.9139\t1.9461\t2.0555\t2.2174\t\n",
      "Training Epoch: 15 [410/566]\tLoss: 14.6139\tLR: 0.010000\n",
      "2.6903\t2.5078\t2.3784\t2.3127\t2.3128\t2.4118\t\n",
      "Training Epoch: 15 [426/566]\tLoss: 11.8023\tLR: 0.010000\n",
      "1.7450\t1.7106\t1.7798\t1.9497\t2.1680\t2.4493\t\n",
      "Training Epoch: 15 [442/566]\tLoss: 17.6872\tLR: 0.010000\n",
      "3.2816\t3.0430\t2.8911\t2.8121\t2.8153\t2.8441\t\n",
      "Training Epoch: 15 [458/566]\tLoss: 14.0759\tLR: 0.010000\n",
      "2.3340\t2.2244\t2.1983\t2.2751\t2.4219\t2.6223\t\n",
      "Training Epoch: 15 [474/566]\tLoss: 18.7982\tLR: 0.010000\n",
      "3.1930\t3.1022\t3.0163\t3.0171\t3.1214\t3.3482\t\n",
      "Training Epoch: 15 [490/566]\tLoss: 21.3113\tLR: 0.010000\n",
      "4.4962\t4.0427\t3.5966\t3.2535\t3.0153\t2.9070\t\n",
      "Training Epoch: 15 [506/566]\tLoss: 14.2693\tLR: 0.010000\n",
      "2.3937\t2.2871\t2.2402\t2.2960\t2.4319\t2.6203\t\n",
      "Training Epoch: 15 [522/566]\tLoss: 33.2089\tLR: 0.010000\n",
      "6.8651\t6.2529\t5.6827\t5.1909\t4.7716\t4.4457\t\n",
      "Training Epoch: 15 [538/566]\tLoss: 21.5896\tLR: 0.010000\n",
      "4.5341\t4.0352\t3.5935\t3.2789\t3.0950\t3.0529\t\n",
      "Training Epoch: 15 [554/566]\tLoss: 14.0657\tLR: 0.010000\n",
      "2.3105\t2.2681\t2.2399\t2.2902\t2.3927\t2.5644\t\n",
      "Training Epoch: 15 [570/566]\tLoss: 10.3067\tLR: 0.010000\n",
      "1.8268\t1.7065\t1.6331\t1.6294\t1.6878\t1.8230\t\n",
      "Training Epoch: 15 [586/566]\tLoss: 20.3523\tLR: 0.010000\n",
      "3.5956\t3.4303\t3.2987\t3.2703\t3.3194\t3.4380\t\n",
      "Training Epoch: 15 [602/566]\tLoss: 19.5129\tLR: 0.010000\n",
      "4.1135\t3.6878\t3.2681\t2.9451\t2.7658\t2.7326\t\n",
      "Training Epoch: 15 [618/566]\tLoss: 20.2540\tLR: 0.010000\n",
      "3.7191\t3.4525\t3.2841\t3.2225\t3.2393\t3.3364\t\n",
      "Training Epoch: 15 [634/566]\tLoss: 19.7010\tLR: 0.010000\n",
      "3.6497\t3.4459\t3.2662\t3.1421\t3.0869\t3.1103\t\n",
      "Training Epoch: 15 [650/566]\tLoss: 24.1468\tLR: 0.010000\n",
      "4.2275\t4.0442\t3.9343\t3.9189\t3.9748\t4.0471\t\n",
      "Training Epoch: 15 [666/566]\tLoss: 25.8765\tLR: 0.010000\n",
      "4.7867\t4.5327\t4.3146\t4.1559\t4.0557\t4.0308\t\n",
      "Training Epoch: 15 [682/566]\tLoss: 17.1433\tLR: 0.010000\n",
      "3.0257\t2.8603\t2.7870\t2.7843\t2.8096\t2.8764\t\n",
      "Training Epoch: 15 [698/566]\tLoss: 19.0262\tLR: 0.010000\n",
      "3.8275\t3.5448\t3.2166\t2.9585\t2.7899\t2.6888\t\n",
      "Training Epoch: 15 [714/566]\tLoss: 12.7252\tLR: 0.010000\n",
      "2.1361\t2.0584\t2.0131\t2.0369\t2.1443\t2.3365\t\n",
      "Training Epoch: 15 [730/566]\tLoss: 23.2013\tLR: 0.010000\n",
      "4.2930\t4.0221\t3.8064\t3.6989\t3.6609\t3.7198\t\n",
      "Training Epoch: 15 [746/566]\tLoss: 6.7849\tLR: 0.010000\n",
      "0.8816\t0.9055\t0.9795\t1.1089\t1.3142\t1.5952\t\n",
      "Training Epoch: 15 [762/566]\tLoss: 23.5322\tLR: 0.010000\n",
      "4.4700\t4.1717\t3.9128\t3.7171\t3.6163\t3.6443\t\n",
      "Training Epoch: 15 [778/566]\tLoss: 9.9564\tLR: 0.010000\n",
      "1.6726\t1.5803\t1.5434\t1.5708\t1.6934\t1.8958\t\n",
      "Training Epoch: 15 [794/566]\tLoss: 24.9931\tLR: 0.010000\n",
      "4.8743\t4.5460\t4.2089\t3.9150\t3.7440\t3.7049\t\n",
      "Training Epoch: 15 [810/566]\tLoss: 14.0919\tLR: 0.010000\n",
      "2.4366\t2.3591\t2.2862\t2.2555\t2.3143\t2.4401\t\n",
      "Training Epoch: 15 [826/566]\tLoss: 23.5849\tLR: 0.010000\n",
      "4.8969\t4.4510\t4.0010\t3.6289\t3.3726\t3.2344\t\n",
      "Training Epoch: 15 [842/566]\tLoss: 17.7899\tLR: 0.010000\n",
      "3.2077\t3.0648\t2.9379\t2.8497\t2.8348\t2.8952\t\n",
      "Training Epoch: 15 [858/566]\tLoss: 20.3557\tLR: 0.010000\n",
      "4.0079\t3.6973\t3.4107\t3.1967\t3.0517\t2.9914\t\n",
      "Training Epoch: 15 [874/566]\tLoss: 14.0875\tLR: 0.010000\n",
      "2.0528\t2.0747\t2.1586\t2.3193\t2.5698\t2.9124\t\n",
      "Training Epoch: 15 [890/566]\tLoss: 15.9229\tLR: 0.010000\n",
      "3.0900\t2.8165\t2.5944\t2.4724\t2.4381\t2.5114\t\n",
      "Training Epoch: 15 [902/566]\tLoss: 17.4937\tLR: 0.010000\n",
      "3.1638\t2.9328\t2.8206\t2.7993\t2.8369\t2.9403\t\n",
      "[0.24862031638622284, 0.5245051980018616, 0.2268744856119156, 0.02116345427930355, 0.7493222951889038, 0.07853611558675766, 0.17214158922433853, 0.015393869951367378, 0.7742550373077393, 0.08105257153511047, 0.14469239115715027, -0.007125783711671829, 0.35767748951911926, 0.3123437166213989, 0.3299787938594818, -0.0022305550519376993, 0.1523735225200653, 0.0, 0.8476264774799347, -0.00536835053935647]\n",
      "\n",
      "Test set t = 00: Average loss: 0.3363, Accuracy: 0.4523\n",
      "Test set t = 01: Average loss: 0.3141, Accuracy: 0.4505\n",
      "Test set t = 02: Average loss: 0.2962, Accuracy: 0.4488\n",
      "Test set t = 03: Average loss: 0.2865, Accuracy: 0.4470\n",
      "Test set t = 04: Average loss: 0.2850, Accuracy: 0.4488\n",
      "Test set t = 05: Average loss: 0.2917, Accuracy: 0.4505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noise_type in noise_types:\n",
    "    for snr_level in snr_levels:\n",
    "        print(\"=====================\")\n",
    "        print(f'{noise_type}, for SNR {snr_level}')\n",
    "        print(\"=====================\")\n",
    "        train_and_eval(noise_type, snr_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556f63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
