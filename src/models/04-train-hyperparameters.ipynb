{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b84576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork\n",
    "from data.CleanSoundsDataset import CleanSoundsDataset\n",
    "from data.NoisyDataset import NoisyDataset, FullNoisyDataset, LargeNoisyDataset\n",
    "from data.MergedNoisyDataset import MergedNoisyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fbc7e",
   "metadata": {},
   "source": [
    "# Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb90b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main args\n",
    "SAME_PARAM = False           # to use the same parameters for all pcoders or not\n",
    "noise_types = ['Merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fae3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "snr_levels = [None]\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfb56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other training params\n",
    "EPOCH = 15\n",
    "FF_START = True             # to start from feedforward initialization\n",
    "MAX_TIMESTEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c99b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path names\n",
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn/'\n",
    "checkpoints_dir = f'{engram_dir}checkpoints/'\n",
    "tensorboard_dir = f'{engram_dir}tensorboard/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544952f",
   "metadata": {},
   "source": [
    "# Load network arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c7bb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAME_PARAM:\n",
    "    from pbranchednetwork_shared import PBranchedNetwork_SharedSameHP\n",
    "    PNetClass = PBranchedNetwork_SharedSameHP\n",
    "    pnet_name = 'all_noNulls'\n",
    "    fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-shared-50-regular.pth'\n",
    "else:\n",
    "    from pbranchednetwork_all import PBranchedNetwork_AllSeparateHP\n",
    "    PNetClass = PBranchedNetwork_AllSeparateHP\n",
    "    pnet_name = 'all_noNulls'\n",
    "    fb_state_dict_path = f'{checkpoints_dir}{pnet_name}/{pnet_name}-50-regular.pth'\n",
    "fb_state_dict = torch.load(fb_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443c46f",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d783f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "        net, state_dict, build_graph, random_init,\n",
    "        ff_multiplier, fb_multiplier, er_multiplier,\n",
    "        same_param, device='cuda:0'):\n",
    "    \n",
    "    pnet = PNetClass(\n",
    "        net, build_graph=build_graph, random_init=random_init,\n",
    "        ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier\n",
    "        )\n",
    "\n",
    "    pnet.load_state_dict(state_dict)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e13f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, epoch, dataloader, timesteps, loss_function, writer=None, tag='Clean'):\n",
    "    test_loss = np.zeros((timesteps+1,))\n",
    "    correct   = np.zeros((timesteps+1,))\n",
    "    for (images, labels) in dataloader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tt in range(timesteps+1):\n",
    "                if tt == 0:\n",
    "                    outputs, _ = net(images)\n",
    "                else:\n",
    "                    outputs, _ = net()\n",
    "                \n",
    "                loss = loss_function(outputs, labels)\n",
    "                test_loss[tt] += loss.item()\n",
    "                _, preds = outputs.max(1)\n",
    "                correct[tt] += preds.eq(labels).sum()\n",
    "\n",
    "    print()\n",
    "    for tt in range(timesteps+1):\n",
    "        test_loss[tt] /= len(dataloader.dataset)\n",
    "        correct[tt] /= len(dataloader.dataset)\n",
    "        print('Test set t = {:02d}: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "            tt,\n",
    "            test_loss[tt],\n",
    "            correct[tt]\n",
    "        ))\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"{tag}Perf/Epoch#{epoch}\",\n",
    "                correct[tt], tt\n",
    "                )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32cdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epoch, dataloader, timesteps, loss_function, optimizer, writer=None):\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "        net.reset()\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        ttloss = np.zeros((timesteps+1))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for tt in range(timesteps+1):\n",
    "            if tt == 0:\n",
    "                outputs, _ = net(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = loss.item()\n",
    "            else:\n",
    "                outputs, _ = net()\n",
    "                current_loss = loss_function(outputs, labels)\n",
    "                ttloss[tt] = current_loss.item()\n",
    "                loss += current_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.update_hyperparameters()\n",
    "            \n",
    "        print(f\"Training Epoch: {epoch} [{batch_index * BATCH_SIZE + len(images)}/{len(dataloader.dataset)}]\\tLoss: {loss.item():0.4f}\\tLR: {optimizer.param_groups[0]['lr']:0.6f}\")\n",
    "        for tt in range(timesteps+1):\n",
    "            print(f'{ttloss[tt]:0.4f}\\t', end='')\n",
    "        print()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                f\"TrainingLoss/CE\", loss.item(),\n",
    "                (epoch-1)*len(dataloader) + batch_index\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68603d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyper_parameters(net, epoch, sumwriter, same_param=True):\n",
    "    if same_param:\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedforward\", getattr(net,f'ff_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/feedback\",    getattr(net,f'fb_part').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/error\",       getattr(net,f'errorm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"HyperparamRaw/memory\",      getattr(net,f'mem_part').item(), epoch)\n",
    "\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedforward\", getattr(net,f'ffm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/feedback\",    getattr(net,f'fbm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/error\",       getattr(net,f'erm').item(), epoch)\n",
    "        sumwriter.add_scalar(f\"Hyperparam/memory\",      1-getattr(net,f'ffm').item()-getattr(net,f'fbm').item(), epoch)\n",
    "    else:\n",
    "        for i in range(1, net.number_of_pcoders+1):\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedforward\", getattr(net,f'ffm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_feedback\", 0, epoch)\n",
    "            sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_error\", getattr(net,f'erm{i}').item(), epoch)\n",
    "            if i < net.number_of_pcoders:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item()-getattr(net,f'fbm{i}').item(), epoch)\n",
    "            else:\n",
    "                sumwriter.add_scalar(f\"Hyperparam/pcoder{i}_memory\",      1-getattr(net,f'ffm{i}').item(), epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85955819",
   "metadata": {},
   "source": [
    "# Main hyperparameter optimization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cf6488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(noise_type, snr_level):\n",
    "    # Load clean and noisy data\n",
    "#     clean_ds_path = f'{engram_dir}training_dataset_random_order.hdf5'\n",
    "#     clean_ds = CleanSoundsDataset(clean_ds_path)\n",
    "#     clean_loader = torch.utils.data.DataLoader(\n",
    "#         clean_ds,  batch_size=BATCH_SIZE,\n",
    "#         shuffle=False, drop_last=False, num_workers=NUM_WORKERS\n",
    "#         )\n",
    "\n",
    "    if noise_type == 'Merged':\n",
    "        noisy_ds = MergedNoisyDataset(subset=0.9, train=True)\n",
    "        noise_loader = torch.utils.data.DataLoader(\n",
    "            noisy_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "        eval_ds = MergedNoisyDataset(subset=0.9, train=False)\n",
    "        eval_loader = torch.utils.data.DataLoader(\n",
    "            eval_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "    else:\n",
    "        noisy_ds = NoisyDataset(bg=noise_type, snr=snr_level)\n",
    "        noise_loader = torch.utils.data.DataLoader(\n",
    "            noisy_ds,  batch_size=BATCH_SIZE,\n",
    "            shuffle=True, drop_last=False,\n",
    "            num_workers=NUM_WORKERS\n",
    "            )\n",
    "        eval_loader = noise_loader\n",
    "\n",
    "    # Set up logs and network for training\n",
    "    net_dir = f'hyper_{noise_type}_snr{snr_level}'\n",
    "    if FF_START:\n",
    "        net_dir += '_FFstart'\n",
    "    if SAME_PARAM:\n",
    "        net_dir += '_shared'\n",
    "\n",
    "    sumwriter = SummaryWriter(f'{tensorboard_dir}{net_dir}')\n",
    "    net = BranchedNetwork() # Load original network\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet_fw = load_pnet( # Load FF PNet\n",
    "        net, fb_state_dict, build_graph=False, random_init=(not FF_START),\n",
    "        ff_multiplier=1.0, fb_multiplier=0.0, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    evaluate(\n",
    "        pnet_fw, 0, eval_loader, 1,\n",
    "        loss_function,\n",
    "        writer=sumwriter, tag='FeedForward')\n",
    "    del pnet_fw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load PNet for hyperparameter optimization\n",
    "    net = BranchedNetwork()\n",
    "    net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))\n",
    "    pnet = load_pnet(\n",
    "        net, fb_state_dict, build_graph=True, random_init=(not FF_START),\n",
    "        ff_multiplier=0.33, fb_multiplier=0.33, er_multiplier=0.0,\n",
    "        same_param=SAME_PARAM, device='cuda:0'\n",
    "        )\n",
    "\n",
    "    # Set up loss function and hyperparameters\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    hyperparams = [*pnet.get_hyperparameters()]\n",
    "    if SAME_PARAM:\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': hyperparams[:-1], 'lr':0.01},\n",
    "            {'params': hyperparams[-1:], 'lr':0.0001}], weight_decay=0.00001)\n",
    "    else:\n",
    "        fffbmem_hp = []\n",
    "        erm_hp = []\n",
    "        for pc in range(pnet.number_of_pcoders):\n",
    "            fffbmem_hp.extend(hyperparams[pc*4:pc*4+3])\n",
    "            erm_hp.append(hyperparams[pc*4+3])\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': fffbmem_hp, 'lr':0.01},\n",
    "            {'params': erm_hp, 'lr':0.0001}], weight_decay=0.00001)\n",
    "\n",
    "    # Log initial hyperparameter and eval values\n",
    "    log_hyper_parameters(pnet, 0, sumwriter, same_param=SAME_PARAM)\n",
    "    hps = pnet.get_hyperparameters_values()\n",
    "    print(hps)\n",
    "    evaluate(\n",
    "        pnet, 0, eval_loader,\n",
    "        MAX_TIMESTEP, loss_function,\n",
    "        writer=sumwriter, tag='Noisy'\n",
    "        )\n",
    "\n",
    "    # Run epochs\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        train(\n",
    "            pnet, epoch, noise_loader,\n",
    "            MAX_TIMESTEP, loss_function, optimizer,\n",
    "            writer=sumwriter\n",
    "            )\n",
    "        log_hyper_parameters(pnet, epoch, sumwriter, same_param=SAME_PARAM)\n",
    "        hps = pnet.get_hyperparameters_values()\n",
    "        print(hps)\n",
    "\n",
    "        evaluate(\n",
    "            pnet, epoch, eval_loader,\n",
    "            MAX_TIMESTEP, loss_function,\n",
    "            writer=sumwriter, tag='Noisy'\n",
    "            )\n",
    "    # evaluate(\n",
    "    #     pnet, epoch, clean_loader,\n",
    "    #     timesteps=MAX_TIMESTEP, writer=sumwriter,\n",
    "    #     tag='Clean'\n",
    "    #     )\n",
    "    sumwriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ba94982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Merged, for SNR None\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n",
      "/home/es3773/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set t = 00: Average loss: 0.2486, Accuracy: 0.5334\n",
      "Test set t = 01: Average loss: 0.2458, Accuracy: 0.5339\n",
      "\n",
      "[0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.30000001192092896, 0.3999999761581421, 0.009999999776482582, 0.30000001192092896, 0.0, 0.699999988079071, 0.009999999776482582]\n",
      "\n",
      "Test set t = 00: Average loss: 0.2486, Accuracy: 0.5334\n",
      "Test set t = 01: Average loss: 0.2458, Accuracy: 0.5339\n",
      "Test set t = 02: Average loss: 0.2472, Accuracy: 0.5275\n",
      "Test set t = 03: Average loss: 0.2538, Accuracy: 0.5198\n",
      "Test set t = 04: Average loss: 0.2651, Accuracy: 0.5069\n",
      "Test set t = 05: Average loss: 0.2806, Accuracy: 0.4901\n",
      "\n",
      "Training Epoch: 1 [10/67482]\tLoss: 20.0702\tLR: 0.010000\n",
      "3.4022\t3.3479\t3.3174\t3.3045\t3.3261\t3.3720\t\n",
      "Training Epoch: 1 [20/67482]\tLoss: 13.2718\tLR: 0.010000\n",
      "2.1616\t2.0720\t2.0677\t2.1496\t2.2962\t2.5248\t\n",
      "Training Epoch: 1 [30/67482]\tLoss: 13.3335\tLR: 0.010000\n",
      "2.1263\t2.1035\t2.1343\t2.1934\t2.3067\t2.4693\t\n",
      "Training Epoch: 1 [40/67482]\tLoss: 13.5960\tLR: 0.010000\n",
      "2.3342\t2.2136\t2.1690\t2.1898\t2.2762\t2.4132\t\n",
      "Training Epoch: 1 [50/67482]\tLoss: 18.3165\tLR: 0.010000\n",
      "3.0992\t3.0874\t3.0434\t3.0120\t3.0139\t3.0606\t\n",
      "Training Epoch: 1 [60/67482]\tLoss: 13.1110\tLR: 0.010000\n",
      "2.1617\t2.1227\t2.1171\t2.1553\t2.2370\t2.3173\t\n",
      "Training Epoch: 1 [70/67482]\tLoss: 17.0051\tLR: 0.010000\n",
      "2.6336\t2.7018\t2.7827\t2.8608\t2.9564\t3.0698\t\n",
      "Training Epoch: 1 [80/67482]\tLoss: 18.7240\tLR: 0.010000\n",
      "2.9872\t2.9905\t3.0235\t3.0977\t3.2277\t3.3973\t\n",
      "Training Epoch: 1 [90/67482]\tLoss: 18.7872\tLR: 0.010000\n",
      "2.9537\t3.0141\t3.0761\t3.1417\t3.2414\t3.3603\t\n",
      "Training Epoch: 1 [100/67482]\tLoss: 8.0411\tLR: 0.010000\n",
      "1.2101\t1.2012\t1.2321\t1.3217\t1.4515\t1.6246\t\n",
      "Training Epoch: 1 [110/67482]\tLoss: 21.7716\tLR: 0.010000\n",
      "3.8041\t3.6846\t3.6011\t3.5467\t3.5565\t3.5785\t\n",
      "Training Epoch: 1 [120/67482]\tLoss: 10.5703\tLR: 0.010000\n",
      "1.7541\t1.7549\t1.7489\t1.7535\t1.7680\t1.7907\t\n",
      "Training Epoch: 1 [130/67482]\tLoss: 12.1854\tLR: 0.010000\n",
      "1.9033\t1.8946\t1.9462\t2.0146\t2.1370\t2.2897\t\n",
      "Training Epoch: 1 [140/67482]\tLoss: 16.5551\tLR: 0.010000\n",
      "2.7384\t2.6846\t2.6903\t2.7421\t2.8026\t2.8971\t\n",
      "Training Epoch: 1 [150/67482]\tLoss: 17.6241\tLR: 0.010000\n",
      "2.9392\t2.9125\t2.9054\t2.9147\t2.9502\t3.0021\t\n",
      "Training Epoch: 1 [160/67482]\tLoss: 11.5933\tLR: 0.010000\n",
      "1.8851\t1.9052\t1.9099\t1.9279\t1.9554\t2.0097\t\n",
      "Training Epoch: 1 [170/67482]\tLoss: 8.5854\tLR: 0.010000\n",
      "1.1161\t1.1938\t1.3177\t1.4766\t1.6467\t1.8345\t\n",
      "Training Epoch: 1 [180/67482]\tLoss: 12.0631\tLR: 0.010000\n",
      "2.0333\t1.9919\t1.9591\t1.9712\t2.0214\t2.0862\t\n",
      "Training Epoch: 1 [190/67482]\tLoss: 20.5106\tLR: 0.010000\n",
      "3.2843\t3.3331\t3.3680\t3.4345\t3.5122\t3.5786\t\n",
      "Training Epoch: 1 [200/67482]\tLoss: 22.9365\tLR: 0.010000\n",
      "3.8487\t3.8201\t3.8015\t3.7836\t3.8086\t3.8740\t\n",
      "Training Epoch: 1 [210/67482]\tLoss: 19.1203\tLR: 0.010000\n",
      "3.2337\t3.1791\t3.1488\t3.1474\t3.1742\t3.2371\t\n",
      "Training Epoch: 1 [220/67482]\tLoss: 11.5895\tLR: 0.010000\n",
      "1.7635\t1.7853\t1.8527\t1.9459\t2.0570\t2.1850\t\n",
      "Training Epoch: 1 [230/67482]\tLoss: 22.4879\tLR: 0.010000\n",
      "3.7875\t3.7020\t3.6585\t3.6870\t3.7728\t3.8802\t\n",
      "Training Epoch: 1 [240/67482]\tLoss: 19.3870\tLR: 0.010000\n",
      "3.1050\t3.1430\t3.1983\t3.2537\t3.3017\t3.3853\t\n",
      "Training Epoch: 1 [250/67482]\tLoss: 8.4016\tLR: 0.010000\n",
      "1.3207\t1.3181\t1.3348\t1.3863\t1.4660\t1.5757\t\n",
      "Training Epoch: 1 [260/67482]\tLoss: 15.3569\tLR: 0.010000\n",
      "2.3889\t2.3972\t2.4661\t2.5769\t2.7066\t2.8212\t\n",
      "Training Epoch: 1 [270/67482]\tLoss: 22.7548\tLR: 0.010000\n",
      "3.8755\t3.7913\t3.7365\t3.7359\t3.7720\t3.8436\t\n",
      "Training Epoch: 1 [280/67482]\tLoss: 10.8971\tLR: 0.010000\n",
      "1.6902\t1.7123\t1.7490\t1.8169\t1.9072\t2.0215\t\n",
      "Training Epoch: 1 [290/67482]\tLoss: 12.9302\tLR: 0.010000\n",
      "2.3094\t2.1807\t2.0845\t2.0626\t2.1069\t2.1861\t\n",
      "Training Epoch: 1 [300/67482]\tLoss: 14.1713\tLR: 0.010000\n",
      "2.6127\t2.5123\t2.3911\t2.2883\t2.2081\t2.1589\t\n",
      "Training Epoch: 1 [310/67482]\tLoss: 15.8302\tLR: 0.010000\n",
      "2.5699\t2.5804\t2.6010\t2.6444\t2.6917\t2.7428\t\n",
      "Training Epoch: 1 [320/67482]\tLoss: 18.0315\tLR: 0.010000\n",
      "2.9469\t2.9592\t2.9654\t2.9962\t3.0433\t3.1204\t\n",
      "Training Epoch: 1 [330/67482]\tLoss: 15.3859\tLR: 0.010000\n",
      "2.6833\t2.6228\t2.5445\t2.5129\t2.5034\t2.5189\t\n",
      "Training Epoch: 1 [340/67482]\tLoss: 4.6778\tLR: 0.010000\n",
      "0.6412\t0.6533\t0.7212\t0.7960\t0.8848\t0.9812\t\n",
      "Training Epoch: 1 [350/67482]\tLoss: 10.7784\tLR: 0.010000\n",
      "1.8073\t1.7805\t1.7725\t1.7730\t1.8004\t1.8448\t\n",
      "Training Epoch: 1 [360/67482]\tLoss: 19.3149\tLR: 0.010000\n",
      "3.1986\t3.2051\t3.1836\t3.1970\t3.2407\t3.2900\t\n",
      "Training Epoch: 1 [370/67482]\tLoss: 16.3878\tLR: 0.010000\n",
      "2.7593\t2.7469\t2.7231\t2.7167\t2.7180\t2.7238\t\n",
      "Training Epoch: 1 [380/67482]\tLoss: 13.4372\tLR: 0.010000\n",
      "2.4024\t2.3213\t2.2208\t2.1705\t2.1564\t2.1658\t\n",
      "Training Epoch: 1 [390/67482]\tLoss: 8.5815\tLR: 0.010000\n",
      "1.4294\t1.4092\t1.3953\t1.4057\t1.4414\t1.5005\t\n",
      "Training Epoch: 1 [400/67482]\tLoss: 10.0492\tLR: 0.010000\n",
      "1.6827\t1.6714\t1.6596\t1.6588\t1.6729\t1.7038\t\n",
      "Training Epoch: 1 [410/67482]\tLoss: 16.1214\tLR: 0.010000\n",
      "2.7467\t2.6726\t2.6440\t2.6470\t2.6768\t2.7343\t\n",
      "Training Epoch: 1 [420/67482]\tLoss: 12.1828\tLR: 0.010000\n",
      "1.9760\t1.9665\t1.9835\t2.0255\t2.0762\t2.1551\t\n",
      "Training Epoch: 1 [430/67482]\tLoss: 5.3152\tLR: 0.010000\n",
      "0.8313\t0.8388\t0.8563\t0.8879\t0.9265\t0.9745\t\n",
      "Training Epoch: 1 [440/67482]\tLoss: 13.2893\tLR: 0.010000\n",
      "2.1973\t2.2007\t2.1842\t2.1839\t2.2257\t2.2975\t\n",
      "Training Epoch: 1 [450/67482]\tLoss: 20.9608\tLR: 0.010000\n",
      "3.6090\t3.5524\t3.4996\t3.4585\t3.4305\t3.4107\t\n",
      "Training Epoch: 1 [460/67482]\tLoss: 13.6191\tLR: 0.010000\n",
      "2.1447\t2.1770\t2.2311\t2.2948\t2.3532\t2.4183\t\n",
      "Training Epoch: 1 [470/67482]\tLoss: 19.4191\tLR: 0.010000\n",
      "3.4255\t3.3264\t3.2328\t3.1716\t3.1415\t3.1213\t\n",
      "Training Epoch: 1 [480/67482]\tLoss: 7.6880\tLR: 0.010000\n",
      "1.2215\t1.2434\t1.2635\t1.2834\t1.3161\t1.3599\t\n",
      "Training Epoch: 1 [490/67482]\tLoss: 17.4542\tLR: 0.010000\n",
      "2.9285\t2.9186\t2.9059\t2.9033\t2.8979\t2.9000\t\n",
      "Training Epoch: 1 [500/67482]\tLoss: 11.7675\tLR: 0.010000\n",
      "1.8161\t1.8624\t1.9186\t1.9949\t2.0555\t2.1201\t\n",
      "Training Epoch: 1 [510/67482]\tLoss: 20.9476\tLR: 0.010000\n",
      "3.4999\t3.4658\t3.4558\t3.4645\t3.5049\t3.5568\t\n",
      "Training Epoch: 1 [520/67482]\tLoss: 14.2030\tLR: 0.010000\n",
      "2.1249\t2.2032\t2.3161\t2.4100\t2.5145\t2.6344\t\n",
      "Training Epoch: 1 [530/67482]\tLoss: 7.9998\tLR: 0.010000\n",
      "1.2914\t1.2476\t1.2619\t1.3076\t1.3897\t1.5016\t\n",
      "Training Epoch: 1 [540/67482]\tLoss: 22.6576\tLR: 0.010000\n",
      "3.8906\t3.8408\t3.7805\t3.7341\t3.7104\t3.7012\t\n",
      "Training Epoch: 1 [550/67482]\tLoss: 12.7555\tLR: 0.010000\n",
      "2.1059\t2.0979\t2.0930\t2.1076\t2.1457\t2.2054\t\n",
      "Training Epoch: 1 [560/67482]\tLoss: 13.9995\tLR: 0.010000\n",
      "2.2546\t2.2464\t2.2708\t2.3231\t2.4010\t2.5036\t\n",
      "Training Epoch: 1 [570/67482]\tLoss: 10.8932\tLR: 0.010000\n",
      "1.8671\t1.8512\t1.8288\t1.7974\t1.7748\t1.7739\t\n",
      "Training Epoch: 1 [580/67482]\tLoss: 13.6192\tLR: 0.010000\n",
      "2.3299\t2.2939\t2.2566\t2.2306\t2.2366\t2.2716\t\n",
      "Training Epoch: 1 [590/67482]\tLoss: 15.3736\tLR: 0.010000\n",
      "2.5327\t2.5351\t2.5264\t2.5496\t2.5905\t2.6393\t\n",
      "Training Epoch: 1 [600/67482]\tLoss: 17.1967\tLR: 0.010000\n",
      "2.9770\t2.8919\t2.8354\t2.8058\t2.8193\t2.8673\t\n",
      "Training Epoch: 1 [610/67482]\tLoss: 31.7155\tLR: 0.010000\n",
      "5.4615\t5.3949\t5.3107\t5.2440\t5.1767\t5.1277\t\n",
      "Training Epoch: 1 [620/67482]\tLoss: 22.0657\tLR: 0.010000\n",
      "3.6504\t3.6371\t3.6465\t3.6609\t3.7041\t3.7667\t\n",
      "Training Epoch: 1 [630/67482]\tLoss: 11.9481\tLR: 0.010000\n",
      "1.9439\t1.9487\t1.9654\t1.9972\t2.0302\t2.0626\t\n",
      "Training Epoch: 1 [640/67482]\tLoss: 9.9726\tLR: 0.010000\n",
      "1.7252\t1.6726\t1.6390\t1.6241\t1.6394\t1.6723\t\n",
      "Training Epoch: 1 [650/67482]\tLoss: 21.5508\tLR: 0.010000\n",
      "3.5485\t3.5527\t3.5635\t3.5917\t3.6261\t3.6684\t\n",
      "Training Epoch: 1 [660/67482]\tLoss: 7.2083\tLR: 0.010000\n",
      "1.0036\t1.0691\t1.1439\t1.2327\t1.3286\t1.4304\t\n",
      "Training Epoch: 1 [670/67482]\tLoss: 13.6984\tLR: 0.010000\n",
      "2.3033\t2.2578\t2.2438\t2.2591\t2.2958\t2.3386\t\n",
      "Training Epoch: 1 [680/67482]\tLoss: 14.5994\tLR: 0.010000\n",
      "2.5174\t2.4760\t2.4231\t2.3944\t2.3848\t2.4037\t\n",
      "Training Epoch: 1 [690/67482]\tLoss: 16.8818\tLR: 0.010000\n",
      "2.8528\t2.8461\t2.8323\t2.8143\t2.7890\t2.7474\t\n",
      "Training Epoch: 1 [700/67482]\tLoss: 21.3762\tLR: 0.010000\n",
      "3.6610\t3.6229\t3.5771\t3.5300\t3.5010\t3.4842\t\n",
      "Training Epoch: 1 [710/67482]\tLoss: 14.9094\tLR: 0.010000\n",
      "2.5084\t2.4876\t2.4695\t2.4666\t2.4801\t2.4972\t\n",
      "Training Epoch: 1 [720/67482]\tLoss: 15.7256\tLR: 0.010000\n",
      "2.5114\t2.5306\t2.5690\t2.6339\t2.7022\t2.7785\t\n",
      "Training Epoch: 1 [730/67482]\tLoss: 9.5343\tLR: 0.010000\n",
      "1.5279\t1.5265\t1.5570\t1.5984\t1.6404\t1.6841\t\n",
      "Training Epoch: 1 [740/67482]\tLoss: 19.8090\tLR: 0.010000\n",
      "3.2089\t3.2130\t3.2438\t3.2967\t3.3734\t3.4732\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [750/67482]\tLoss: 12.6181\tLR: 0.010000\n",
      "1.9573\t2.0002\t2.0570\t2.1221\t2.1926\t2.2889\t\n",
      "Training Epoch: 1 [760/67482]\tLoss: 18.7988\tLR: 0.010000\n",
      "3.2288\t3.1888\t3.1422\t3.1059\t3.0757\t3.0574\t\n",
      "Training Epoch: 1 [770/67482]\tLoss: 29.0253\tLR: 0.010000\n",
      "5.0700\t4.9658\t4.8582\t4.7644\t4.7035\t4.6634\t\n",
      "Training Epoch: 1 [780/67482]\tLoss: 18.0230\tLR: 0.010000\n",
      "2.9432\t2.9431\t2.9630\t3.0047\t3.0536\t3.1154\t\n",
      "Training Epoch: 1 [790/67482]\tLoss: 21.8598\tLR: 0.010000\n",
      "3.8153\t3.7408\t3.6575\t3.5888\t3.5457\t3.5117\t\n",
      "Training Epoch: 1 [800/67482]\tLoss: 9.9412\tLR: 0.010000\n",
      "1.7320\t1.6972\t1.6724\t1.6441\t1.6122\t1.5832\t\n",
      "Training Epoch: 1 [810/67482]\tLoss: 8.9990\tLR: 0.010000\n",
      "1.4255\t1.4484\t1.4804\t1.5130\t1.5474\t1.5842\t\n",
      "Training Epoch: 1 [820/67482]\tLoss: 20.6061\tLR: 0.010000\n",
      "3.6741\t3.5552\t3.4451\t3.3604\t3.3030\t3.2681\t\n",
      "Training Epoch: 1 [830/67482]\tLoss: 16.1995\tLR: 0.010000\n",
      "2.6656\t2.6526\t2.6549\t2.6846\t2.7324\t2.8095\t\n",
      "Training Epoch: 1 [840/67482]\tLoss: 27.9691\tLR: 0.010000\n",
      "4.9111\t4.8371\t4.7187\t4.5996\t4.4953\t4.4075\t\n",
      "Training Epoch: 1 [850/67482]\tLoss: 13.1603\tLR: 0.010000\n",
      "2.3349\t2.2672\t2.2144\t2.1645\t2.1132\t2.0660\t\n",
      "Training Epoch: 1 [860/67482]\tLoss: 9.5347\tLR: 0.010000\n",
      "1.4799\t1.5041\t1.5441\t1.5943\t1.6640\t1.7482\t\n",
      "Training Epoch: 1 [870/67482]\tLoss: 12.2555\tLR: 0.010000\n",
      "2.0454\t2.0321\t2.0300\t2.0334\t2.0497\t2.0650\t\n",
      "Training Epoch: 1 [880/67482]\tLoss: 8.9736\tLR: 0.010000\n",
      "1.4162\t1.4411\t1.4703\t1.5058\t1.5507\t1.5895\t\n",
      "Training Epoch: 1 [890/67482]\tLoss: 9.0689\tLR: 0.010000\n",
      "1.6409\t1.5859\t1.5191\t1.4687\t1.4345\t1.4198\t\n",
      "Training Epoch: 1 [900/67482]\tLoss: 12.4049\tLR: 0.010000\n",
      "2.0385\t2.0369\t2.0460\t2.0561\t2.0845\t2.1429\t\n",
      "Training Epoch: 1 [910/67482]\tLoss: 20.6379\tLR: 0.010000\n",
      "3.4600\t3.4629\t3.4471\t3.4324\t3.4214\t3.4141\t\n",
      "Training Epoch: 1 [920/67482]\tLoss: 15.7623\tLR: 0.010000\n",
      "2.6759\t2.6514\t2.6226\t2.6022\t2.5991\t2.6110\t\n",
      "Training Epoch: 1 [930/67482]\tLoss: 18.6346\tLR: 0.010000\n",
      "3.1806\t3.1445\t3.1013\t3.0728\t3.0660\t3.0695\t\n",
      "Training Epoch: 1 [940/67482]\tLoss: 21.5967\tLR: 0.010000\n",
      "3.7109\t3.6819\t3.6273\t3.5647\t3.5169\t3.4950\t\n",
      "Training Epoch: 1 [950/67482]\tLoss: 11.9398\tLR: 0.010000\n",
      "2.0539\t2.0258\t1.9852\t1.9620\t1.9534\t1.9596\t\n",
      "Training Epoch: 1 [960/67482]\tLoss: 14.5339\tLR: 0.010000\n",
      "2.3832\t2.3959\t2.4140\t2.4243\t2.4448\t2.4718\t\n",
      "Training Epoch: 1 [970/67482]\tLoss: 15.7317\tLR: 0.010000\n",
      "2.6941\t2.6678\t2.6238\t2.5944\t2.5774\t2.5742\t\n",
      "Training Epoch: 1 [980/67482]\tLoss: 18.3244\tLR: 0.010000\n",
      "3.1981\t3.1377\t3.0595\t2.9989\t2.9628\t2.9674\t\n",
      "Training Epoch: 1 [990/67482]\tLoss: 18.8934\tLR: 0.010000\n",
      "3.1830\t3.1609\t3.1466\t3.1349\t3.1302\t3.1377\t\n",
      "Training Epoch: 1 [1000/67482]\tLoss: 13.1371\tLR: 0.010000\n",
      "2.1264\t2.1417\t2.1633\t2.1906\t2.2304\t2.2847\t\n",
      "Training Epoch: 1 [1010/67482]\tLoss: 6.5117\tLR: 0.010000\n",
      "1.1125\t1.0874\t1.0637\t1.0607\t1.0782\t1.1092\t\n",
      "Training Epoch: 1 [1020/67482]\tLoss: 11.1398\tLR: 0.010000\n",
      "1.8193\t1.8178\t1.8290\t1.8486\t1.8874\t1.9377\t\n",
      "Training Epoch: 1 [1030/67482]\tLoss: 20.3020\tLR: 0.010000\n",
      "3.3410\t3.3474\t3.3591\t3.3810\t3.4149\t3.4585\t\n",
      "Training Epoch: 1 [1040/67482]\tLoss: 15.2106\tLR: 0.010000\n",
      "2.5593\t2.5663\t2.5517\t2.5317\t2.5079\t2.4938\t\n",
      "Training Epoch: 1 [1050/67482]\tLoss: 9.0330\tLR: 0.010000\n",
      "1.5381\t1.5073\t1.4916\t1.4907\t1.4951\t1.5101\t\n",
      "Training Epoch: 1 [1060/67482]\tLoss: 10.5533\tLR: 0.010000\n",
      "1.6845\t1.6901\t1.7214\t1.7675\t1.8156\t1.8741\t\n",
      "Training Epoch: 1 [1070/67482]\tLoss: 13.3195\tLR: 0.010000\n",
      "2.2016\t2.2089\t2.2181\t2.2321\t2.2324\t2.2265\t\n",
      "Training Epoch: 1 [1080/67482]\tLoss: 13.9676\tLR: 0.010000\n",
      "2.3480\t2.3329\t2.3226\t2.3191\t2.3216\t2.3235\t\n",
      "Training Epoch: 1 [1090/67482]\tLoss: 19.6913\tLR: 0.010000\n",
      "3.3845\t3.3291\t3.2601\t3.2320\t3.2273\t3.2582\t\n",
      "Training Epoch: 1 [1100/67482]\tLoss: 14.2788\tLR: 0.010000\n",
      "2.3598\t2.3612\t2.3685\t2.3822\t2.3969\t2.4101\t\n",
      "Training Epoch: 1 [1110/67482]\tLoss: 14.6405\tLR: 0.010000\n",
      "2.3780\t2.3917\t2.4186\t2.4477\t2.4806\t2.5238\t\n",
      "Training Epoch: 1 [1120/67482]\tLoss: 9.3178\tLR: 0.010000\n",
      "1.6150\t1.5564\t1.5295\t1.5211\t1.5346\t1.5611\t\n",
      "Training Epoch: 1 [1130/67482]\tLoss: 14.5949\tLR: 0.010000\n",
      "2.4457\t2.4333\t2.4247\t2.4269\t2.4262\t2.4381\t\n",
      "Training Epoch: 1 [1140/67482]\tLoss: 10.4681\tLR: 0.010000\n",
      "1.7399\t1.7338\t1.7295\t1.7310\t1.7512\t1.7827\t\n",
      "Training Epoch: 1 [1150/67482]\tLoss: 20.0439\tLR: 0.010000\n",
      "3.3577\t3.3303\t3.3221\t3.3231\t3.3410\t3.3696\t\n",
      "Training Epoch: 1 [1160/67482]\tLoss: 9.1339\tLR: 0.010000\n",
      "1.5124\t1.5026\t1.4983\t1.5080\t1.5369\t1.5757\t\n",
      "Training Epoch: 1 [1170/67482]\tLoss: 13.0414\tLR: 0.010000\n",
      "2.2389\t2.2120\t2.1702\t2.1503\t2.1377\t2.1323\t\n",
      "Training Epoch: 1 [1180/67482]\tLoss: 13.6685\tLR: 0.010000\n",
      "2.3523\t2.3334\t2.3043\t2.2610\t2.2247\t2.1930\t\n",
      "Training Epoch: 1 [1190/67482]\tLoss: 18.1935\tLR: 0.010000\n",
      "3.1106\t3.0991\t3.0634\t3.0130\t2.9703\t2.9370\t\n",
      "Training Epoch: 1 [1200/67482]\tLoss: 15.3438\tLR: 0.010000\n",
      "2.4938\t2.4909\t2.5088\t2.5525\t2.6158\t2.6819\t\n",
      "Training Epoch: 1 [1210/67482]\tLoss: 18.2090\tLR: 0.010000\n",
      "3.1491\t3.0855\t3.0345\t2.9947\t2.9757\t2.9695\t\n",
      "Training Epoch: 1 [1220/67482]\tLoss: 12.4800\tLR: 0.010000\n",
      "2.0988\t2.0804\t2.0753\t2.0699\t2.0680\t2.0877\t\n",
      "Training Epoch: 1 [1230/67482]\tLoss: 14.0938\tLR: 0.010000\n",
      "2.2688\t2.2537\t2.2724\t2.3356\t2.4277\t2.5356\t\n",
      "Training Epoch: 1 [1240/67482]\tLoss: 5.0747\tLR: 0.010000\n",
      "0.8399\t0.8407\t0.8401\t0.8419\t0.8536\t0.8584\t\n",
      "Training Epoch: 1 [1250/67482]\tLoss: 33.0887\tLR: 0.010000\n",
      "5.6005\t5.5817\t5.5444\t5.4870\t5.4522\t5.4230\t\n",
      "Training Epoch: 1 [1260/67482]\tLoss: 18.9517\tLR: 0.010000\n",
      "3.3283\t3.2460\t3.1697\t3.1049\t3.0649\t3.0379\t\n",
      "Training Epoch: 1 [1270/67482]\tLoss: 7.0352\tLR: 0.010000\n",
      "1.1445\t1.1529\t1.1644\t1.1776\t1.1902\t1.2056\t\n",
      "Training Epoch: 1 [1280/67482]\tLoss: 7.5977\tLR: 0.010000\n",
      "1.2131\t1.2202\t1.2349\t1.2684\t1.3068\t1.3543\t\n",
      "Training Epoch: 1 [1290/67482]\tLoss: 20.8275\tLR: 0.010000\n",
      "3.5340\t3.5003\t3.4705\t3.4511\t3.4361\t3.4355\t\n",
      "Training Epoch: 1 [1300/67482]\tLoss: 8.6594\tLR: 0.010000\n",
      "1.4872\t1.4683\t1.4482\t1.4297\t1.4156\t1.4105\t\n",
      "Training Epoch: 1 [1310/67482]\tLoss: 20.6589\tLR: 0.010000\n",
      "3.5300\t3.4975\t3.4611\t3.4228\t3.3859\t3.3616\t\n",
      "Training Epoch: 1 [1320/67482]\tLoss: 18.4670\tLR: 0.010000\n",
      "3.0917\t3.1120\t3.1117\t3.0866\t3.0530\t3.0120\t\n",
      "Training Epoch: 1 [1330/67482]\tLoss: 13.6569\tLR: 0.010000\n",
      "2.2506\t2.2395\t2.2411\t2.2667\t2.3045\t2.3545\t\n",
      "Training Epoch: 1 [1340/67482]\tLoss: 17.5026\tLR: 0.010000\n",
      "2.8915\t2.9071\t2.9147\t2.9235\t2.9317\t2.9339\t\n",
      "Training Epoch: 1 [1350/67482]\tLoss: 14.4716\tLR: 0.010000\n",
      "2.4129\t2.3990\t2.3991\t2.4066\t2.4188\t2.4351\t\n",
      "Training Epoch: 1 [1360/67482]\tLoss: 10.4950\tLR: 0.010000\n",
      "1.6973\t1.6830\t1.6956\t1.7387\t1.8009\t1.8795\t\n",
      "Training Epoch: 1 [1370/67482]\tLoss: 10.1312\tLR: 0.010000\n",
      "1.5916\t1.6189\t1.6559\t1.6995\t1.7527\t1.8126\t\n",
      "Training Epoch: 1 [1380/67482]\tLoss: 16.2341\tLR: 0.010000\n",
      "2.6368\t2.6653\t2.6920\t2.7208\t2.7447\t2.7745\t\n",
      "Training Epoch: 1 [1390/67482]\tLoss: 12.1287\tLR: 0.010000\n",
      "2.0905\t2.0468\t2.0106\t1.9912\t1.9923\t1.9973\t\n",
      "Training Epoch: 1 [1400/67482]\tLoss: 17.2995\tLR: 0.010000\n",
      "2.7580\t2.7876\t2.8290\t2.8899\t2.9700\t3.0650\t\n",
      "Training Epoch: 1 [1410/67482]\tLoss: 14.2611\tLR: 0.010000\n",
      "2.3392\t2.3461\t2.3502\t2.3671\t2.4072\t2.4514\t\n",
      "Training Epoch: 1 [1420/67482]\tLoss: 18.0711\tLR: 0.010000\n",
      "3.1040\t3.0834\t3.0430\t2.9834\t2.9403\t2.9170\t\n",
      "Training Epoch: 1 [1430/67482]\tLoss: 21.0616\tLR: 0.010000\n",
      "3.7985\t3.6990\t3.5582\t3.4302\t3.3273\t3.2485\t\n",
      "Training Epoch: 1 [1440/67482]\tLoss: 12.7532\tLR: 0.010000\n",
      "2.1555\t2.1428\t2.1219\t2.1107\t2.1092\t2.1131\t\n",
      "Training Epoch: 1 [1450/67482]\tLoss: 22.1164\tLR: 0.010000\n",
      "3.9367\t3.8464\t3.7326\t3.6238\t3.5261\t3.4508\t\n",
      "Training Epoch: 1 [1460/67482]\tLoss: 19.7641\tLR: 0.010000\n",
      "3.2789\t3.2749\t3.2752\t3.2866\t3.3131\t3.3354\t\n",
      "Training Epoch: 1 [1470/67482]\tLoss: 15.4261\tLR: 0.010000\n",
      "2.5921\t2.5745\t2.5609\t2.5568\t2.5643\t2.5776\t\n",
      "Training Epoch: 1 [1480/67482]\tLoss: 8.9292\tLR: 0.010000\n",
      "1.5560\t1.5264\t1.4933\t1.4614\t1.4468\t1.4452\t\n",
      "Training Epoch: 1 [1490/67482]\tLoss: 3.7546\tLR: 0.010000\n",
      "0.6151\t0.6107\t0.6158\t0.6243\t0.6352\t0.6534\t\n",
      "Training Epoch: 1 [1500/67482]\tLoss: 5.8822\tLR: 0.010000\n",
      "0.9901\t0.9706\t0.9602\t0.9676\t0.9852\t1.0085\t\n",
      "Training Epoch: 1 [1510/67482]\tLoss: 21.7687\tLR: 0.010000\n",
      "3.5692\t3.6005\t3.6269\t3.6344\t3.6540\t3.6838\t\n",
      "Training Epoch: 1 [1520/67482]\tLoss: 19.1177\tLR: 0.010000\n",
      "3.3110\t3.2520\t3.1881\t3.1454\t3.1155\t3.1056\t\n",
      "Training Epoch: 1 [1530/67482]\tLoss: 15.7606\tLR: 0.010000\n",
      "2.7824\t2.7183\t2.6474\t2.5875\t2.5346\t2.4904\t\n",
      "Training Epoch: 1 [1540/67482]\tLoss: 11.6623\tLR: 0.010000\n",
      "2.0167\t1.9856\t1.9522\t1.9201\t1.9008\t1.8869\t\n",
      "Training Epoch: 1 [1550/67482]\tLoss: 17.2183\tLR: 0.010000\n",
      "2.9006\t2.8890\t2.8708\t2.8545\t2.8513\t2.8521\t\n",
      "Training Epoch: 1 [1560/67482]\tLoss: 15.7689\tLR: 0.010000\n",
      "2.5173\t2.5459\t2.6009\t2.6541\t2.7034\t2.7472\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [1570/67482]\tLoss: 7.5142\tLR: 0.010000\n",
      "1.2551\t1.2496\t1.2493\t1.2426\t1.2484\t1.2692\t\n",
      "Training Epoch: 1 [1580/67482]\tLoss: 21.0257\tLR: 0.010000\n",
      "3.3507\t3.3915\t3.4596\t3.5322\t3.6058\t3.6858\t\n",
      "Training Epoch: 1 [1590/67482]\tLoss: 10.6563\tLR: 0.010000\n",
      "1.9468\t1.8517\t1.7634\t1.7097\t1.6914\t1.6935\t\n",
      "Training Epoch: 1 [1600/67482]\tLoss: 12.6422\tLR: 0.010000\n",
      "2.1151\t2.1089\t2.1090\t2.1041\t2.0981\t2.1070\t\n",
      "Training Epoch: 1 [1610/67482]\tLoss: 18.1171\tLR: 0.010000\n",
      "3.1299\t3.0893\t3.0368\t2.9918\t2.9500\t2.9192\t\n",
      "Training Epoch: 1 [1620/67482]\tLoss: 19.2151\tLR: 0.010000\n",
      "3.2117\t3.1948\t3.1909\t3.1938\t3.2067\t3.2172\t\n",
      "Training Epoch: 1 [1630/67482]\tLoss: 7.1461\tLR: 0.010000\n",
      "1.2481\t1.2069\t1.1754\t1.1629\t1.1661\t1.1867\t\n",
      "Training Epoch: 1 [1640/67482]\tLoss: 3.9977\tLR: 0.010000\n",
      "0.6494\t0.6520\t0.6570\t0.6661\t0.6795\t0.6937\t\n",
      "Training Epoch: 1 [1650/67482]\tLoss: 10.0742\tLR: 0.010000\n",
      "1.6857\t1.6546\t1.6404\t1.6532\t1.6902\t1.7502\t\n",
      "Training Epoch: 1 [1660/67482]\tLoss: 7.5649\tLR: 0.010000\n",
      "1.2513\t1.2342\t1.2309\t1.2484\t1.2828\t1.3173\t\n",
      "Training Epoch: 1 [1670/67482]\tLoss: 21.8277\tLR: 0.010000\n",
      "3.7555\t3.6929\t3.6486\t3.6099\t3.5763\t3.5444\t\n",
      "Training Epoch: 1 [1680/67482]\tLoss: 9.5872\tLR: 0.010000\n",
      "1.5744\t1.5753\t1.5843\t1.5984\t1.6171\t1.6377\t\n",
      "Training Epoch: 1 [1690/67482]\tLoss: 17.1839\tLR: 0.010000\n",
      "3.0153\t2.9426\t2.8715\t2.8181\t2.7814\t2.7550\t\n",
      "Training Epoch: 1 [1700/67482]\tLoss: 11.9236\tLR: 0.010000\n",
      "1.9618\t1.9633\t1.9667\t1.9790\t2.0072\t2.0455\t\n",
      "Training Epoch: 1 [1710/67482]\tLoss: 20.8859\tLR: 0.010000\n",
      "3.5739\t3.5429\t3.5019\t3.4674\t3.4215\t3.3784\t\n",
      "Training Epoch: 1 [1720/67482]\tLoss: 14.6081\tLR: 0.010000\n",
      "2.3703\t2.3773\t2.4012\t2.4397\t2.4870\t2.5326\t\n",
      "Training Epoch: 1 [1730/67482]\tLoss: 10.5027\tLR: 0.010000\n",
      "1.7358\t1.7308\t1.7382\t1.7487\t1.7625\t1.7867\t\n",
      "Training Epoch: 1 [1740/67482]\tLoss: 14.2512\tLR: 0.010000\n",
      "2.3768\t2.3668\t2.3566\t2.3716\t2.3830\t2.3963\t\n",
      "Training Epoch: 1 [1750/67482]\tLoss: 17.0564\tLR: 0.010000\n",
      "2.8197\t2.8041\t2.8008\t2.8254\t2.8710\t2.9354\t\n",
      "Training Epoch: 1 [1760/67482]\tLoss: 10.1981\tLR: 0.010000\n",
      "1.8780\t1.7841\t1.6984\t1.6409\t1.6064\t1.5903\t\n",
      "Training Epoch: 1 [1770/67482]\tLoss: 9.1352\tLR: 0.010000\n",
      "1.4566\t1.4740\t1.5056\t1.5352\t1.5655\t1.5982\t\n",
      "Training Epoch: 1 [1780/67482]\tLoss: 13.9530\tLR: 0.010000\n",
      "2.3591\t2.3310\t2.3176\t2.3101\t2.3099\t2.3252\t\n",
      "Training Epoch: 1 [1790/67482]\tLoss: 12.1867\tLR: 0.010000\n",
      "2.0126\t2.0016\t2.0099\t2.0288\t2.0538\t2.0799\t\n",
      "Training Epoch: 1 [1800/67482]\tLoss: 27.8993\tLR: 0.010000\n",
      "4.8263\t4.7535\t4.6692\t4.5964\t4.5457\t4.5082\t\n",
      "Training Epoch: 1 [1810/67482]\tLoss: 6.3557\tLR: 0.010000\n",
      "1.0082\t1.0163\t1.0361\t1.0636\t1.0987\t1.1329\t\n",
      "Training Epoch: 1 [1820/67482]\tLoss: 15.0068\tLR: 0.010000\n",
      "2.5108\t2.5023\t2.4973\t2.4900\t2.4937\t2.5127\t\n",
      "Training Epoch: 1 [1830/67482]\tLoss: 12.0422\tLR: 0.010000\n",
      "1.9266\t1.9484\t1.9768\t2.0175\t2.0606\t2.1123\t\n",
      "Training Epoch: 1 [1840/67482]\tLoss: 16.3612\tLR: 0.010000\n",
      "2.7211\t2.7270\t2.7243\t2.7274\t2.7268\t2.7347\t\n",
      "Training Epoch: 1 [1850/67482]\tLoss: 14.2570\tLR: 0.010000\n",
      "2.4423\t2.4279\t2.3955\t2.3699\t2.3288\t2.2928\t\n",
      "Training Epoch: 1 [1860/67482]\tLoss: 18.8790\tLR: 0.010000\n",
      "3.3327\t3.2409\t3.1523\t3.0871\t3.0486\t3.0174\t\n",
      "Training Epoch: 1 [1870/67482]\tLoss: 15.7223\tLR: 0.010000\n",
      "2.7012\t2.6841\t2.6364\t2.5949\t2.5627\t2.5431\t\n",
      "Training Epoch: 1 [1880/67482]\tLoss: 16.9448\tLR: 0.010000\n",
      "2.8985\t2.8597\t2.8282\t2.8027\t2.7825\t2.7733\t\n",
      "Training Epoch: 1 [1890/67482]\tLoss: 17.5947\tLR: 0.010000\n",
      "2.9837\t2.9540\t2.9293\t2.9135\t2.9081\t2.9060\t\n",
      "Training Epoch: 1 [1900/67482]\tLoss: 12.9321\tLR: 0.010000\n",
      "2.2053\t2.1832\t2.1619\t2.1385\t2.1245\t2.1186\t\n",
      "Training Epoch: 1 [1910/67482]\tLoss: 10.8493\tLR: 0.010000\n",
      "1.7560\t1.7738\t1.7963\t1.8180\t1.8392\t1.8661\t\n",
      "Training Epoch: 1 [1920/67482]\tLoss: 9.2398\tLR: 0.010000\n",
      "1.4209\t1.4584\t1.5090\t1.5601\t1.6158\t1.6757\t\n",
      "Training Epoch: 1 [1930/67482]\tLoss: 19.3172\tLR: 0.010000\n",
      "3.3135\t3.2772\t3.2345\t3.2012\t3.1608\t3.1301\t\n",
      "Training Epoch: 1 [1940/67482]\tLoss: 16.0593\tLR: 0.010000\n",
      "2.6793\t2.6795\t2.6721\t2.6698\t2.6718\t2.6866\t\n",
      "Training Epoch: 1 [1950/67482]\tLoss: 6.4905\tLR: 0.010000\n",
      "0.9831\t1.0120\t1.0517\t1.0970\t1.1472\t1.1994\t\n",
      "Training Epoch: 1 [1960/67482]\tLoss: 19.3013\tLR: 0.010000\n",
      "3.3220\t3.2726\t3.2266\t3.1974\t3.1608\t3.1219\t\n",
      "Training Epoch: 1 [1970/67482]\tLoss: 15.6667\tLR: 0.010000\n",
      "2.6394\t2.6286\t2.6110\t2.6011\t2.5937\t2.5929\t\n",
      "Training Epoch: 1 [1980/67482]\tLoss: 23.0849\tLR: 0.010000\n",
      "3.7314\t3.7642\t3.8180\t3.8759\t3.9281\t3.9672\t\n",
      "Training Epoch: 1 [1990/67482]\tLoss: 11.7990\tLR: 0.010000\n",
      "1.9243\t1.9278\t1.9554\t1.9837\t1.9962\t2.0115\t\n",
      "Training Epoch: 1 [2000/67482]\tLoss: 16.9631\tLR: 0.010000\n",
      "2.7126\t2.7599\t2.8073\t2.8567\t2.8955\t2.9312\t\n",
      "Training Epoch: 1 [2010/67482]\tLoss: 11.9568\tLR: 0.010000\n",
      "2.0002\t1.9866\t1.9749\t1.9769\t1.9956\t2.0226\t\n",
      "Training Epoch: 1 [2020/67482]\tLoss: 13.4543\tLR: 0.010000\n",
      "2.2124\t2.2196\t2.2346\t2.2558\t2.2670\t2.2649\t\n",
      "Training Epoch: 1 [2030/67482]\tLoss: 9.9047\tLR: 0.010000\n",
      "1.7514\t1.7118\t1.6606\t1.6205\t1.5921\t1.5683\t\n",
      "Training Epoch: 1 [2040/67482]\tLoss: 22.7260\tLR: 0.010000\n",
      "3.7914\t3.7829\t3.7754\t3.7737\t3.7904\t3.8123\t\n",
      "Training Epoch: 1 [2050/67482]\tLoss: 13.5236\tLR: 0.010000\n",
      "2.2901\t2.2679\t2.2505\t2.2415\t2.2362\t2.2373\t\n",
      "Training Epoch: 1 [2060/67482]\tLoss: 6.4338\tLR: 0.010000\n",
      "1.0596\t1.0513\t1.0523\t1.0647\t1.0883\t1.1178\t\n",
      "Training Epoch: 1 [2070/67482]\tLoss: 11.6132\tLR: 0.010000\n",
      "1.9482\t1.9276\t1.9215\t1.9229\t1.9324\t1.9605\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ba168df1af5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{noise_type}, for SNR {snr_level}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=====================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-a67bbe1c04af>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(noise_type, snr_level)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mMAX_TIMESTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msumwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             )\n\u001b[1;32m     98\u001b[0m         \u001b[0mlog_hyper_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msame_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAME_PARAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1c034493122f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epoch, dataloader, timesteps, loss_function, optimizer, writer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mttloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/predify/networks/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/es3773/hallucnn/src/models/networks_2022.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, _input)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_encoder_representations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mspeech_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/es3773/hallucnn/src/models/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, _input)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [left, right, top, bot]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/issa/users/es3773/hallucnn/src/models/pbranchednetwork_all.py\u001b[0m in \u001b[0;36mfw_hook2\u001b[0;34m(m, m_in, m_out)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCoderN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfw_hook2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfbm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech_branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_hook2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/predify/modules/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ff, fb, target, build_graph, ffm, fbm, erm)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_error\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hcnn/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for noise_type in noise_types:\n",
    "    for snr_level in snr_levels:\n",
    "        print(\"=====================\")\n",
    "        print(f'{noise_type}, for SNR {snr_level}')\n",
    "        print(\"=====================\")\n",
    "        train_and_eval(noise_type, snr_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b31d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
