{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tested for `networks_2022`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "sys.path.append(root)\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from predify.utils.training import train_pcoders, eval_pcoders\n",
    "\n",
    "from networks_2022 import BranchedNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/issa-locker/users/Erica/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:78: UserWarning: Inconsistent tf pad calculation in ConvLayer.\n",
      "  warnings.warn('Inconsistent tf pad calculation in ConvLayer.')\n",
      "/share/issa/users/es3773/temp-hallucnn/src/models/layers.py:173: UserWarning: Inconsistent tf pad calculation: 0, 1\n",
      "  warnings.warn(f'Inconsistent tf pad calculation: {pad_left}, {pad_right}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BranchedNetwork()\n",
    "net.load_state_dict(torch.load(f'{engram_dir}networks_2022_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats = []\n",
    "\n",
    "with h5py.File(f\"{engram_dir}PsychophysicsWord2017W_not_resampled.hdf5\", 'r') as f_in:\n",
    "    for ind in range(len(f_in['data'])):\n",
    "        word, genre = net(torch.tensor(np.reshape( f_in['data'][ind],(164,400))))\n",
    "        \n",
    "        y_hat = int(torch.argmax(word))\n",
    "        y_hats.append(y_hat)\n",
    "        if len(y_hats) > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229, 312, 525, 3, 393, 5, 6, 7, 8, 9, 10, 11, 12, 13, 126, 108, 15, 298, 17, 46]\n"
     ]
    }
   ],
   "source": [
    "# Torch network outputs\n",
    "print(y_hats[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to TF labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_y_hats = np.array([229, 312, 525, 3, 393, 5, 6, 7, 8, 9, 10, 11, 12, 13, 126, 108, 15, 298, 17, 46, 152, 388, 278, 459, 256, 23, 14, 25, 26, 27, 129, 29, 30, 31, 63, 337, 34, 216, 36, 37, 38, 241, 40, 41, 183, 42, 393, 240, 45, 196, 46, 487, 48, 129, 142, 51, 269, 519, 54, 55, 284, 57, 65, 58, 239, 59, 376, 62, 63, 158, 345, 66, 67, 68, 69, 70, 394, 72, 494, 74, 109, 75, 48, 78, 343, 455, 487, 413, 82, 109, 84, 496, 243, 490, 9, 24, 89, 0, 110, 259])\n",
    "tf_y_hats_no_Dropout = [229, 312, 525, 3, 393, 5, 6, 7, 8, 9, 10, 11, 12, 13, 126, 108, 15, 298, 17, 46, 152, 388, 278, 459, 256, 23, 14, 25, 26, 27, 129, 29, 30, 31, 63, 337, 34, 216, 36, 37, 38, 241, 40, 41, 183, 42, 393, 240, 45, 196, 46, 487, 48, 129, 142, 51, 269, 519, 54, 55, 284, 57, 65, 58, 239, 59, 376, 62, 63, 158, 345, 66, 67, 68, 69, 70, 394, 72, 494, 74, 109, 75, 48, 78, 343, 455, 487, 413, 82, 109, 84, 496, 243, 490, 9, 24, 89, 0, 110, 259]\n",
    "print(tf_y_hats[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(np.where(tf_y_hats == np.array(y_hats))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:1339/notebooks/JupyterNotebooks/thesis/analysesUsingNAC/net7_tf_2020w-naturalistic_heldOutSpkr.ipynb\n",
    "\n",
    "# Think this is WSJ only -- 531 words in train set rather tha 587"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to True labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_metadata = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_key = np.load(f\"{engram_dir}PsychophysicsWord2017W_999c6fc475be1e82e114ab9865aa5459e4fd329d.__META_key.npy\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for word in f_metadata['word']:\n",
    "    idx = np.argwhere(f_key == word)\n",
    "    if len(idx) == 0:\n",
    "        labels.append(-1)\n",
    "    else:\n",
    "        labels.append(idx.item())\n",
    "labels = np.array(labels)\n",
    "labels += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/es3773/.local/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    }
   ],
   "source": [
    "y_hats = []\n",
    "corrects = []\n",
    "\n",
    "with h5py.File(f\"{engram_dir}PsychophysicsWord2017W_not_resampled.hdf5\", 'r') as f_in:\n",
    "    for ind in range(len(f_in['data'])):\n",
    "        word, genre = net(torch.tensor(np.reshape( f_in['data'][ind],(164,400))))\n",
    "        \n",
    "        y_hat = int(torch.argmax(word))\n",
    "        y_hats.append(y_hat)\n",
    "        corrects.append(y_hat == labels[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{engram_dir}PsychophysicsWord2017W_net_performance.p\", 'wb') as f:\n",
    "    results = {\n",
    "        'net_pred': np.array(y_hats),\n",
    "        'net_mistakes': np.logical_not(corrects)\n",
    "    }\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
